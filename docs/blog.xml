<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Hirofumi Shiba</title>
<link>https://162348.github.io/blog.html</link>
<atom:link href="https://162348.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>A Blog by a Bayesian Computation Researcher</description>
<image>
<url>https://162348.github.io/profile.jpg</url>
<title>Hirofumi Shiba</title>
<link>https://162348.github.io/blog.html</link>
</image>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Mon, 17 Jun 2024 15:00:00 GMT</lastBuildDate>
<item>
  <title>R による記号微分入門</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Computation/calculus.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<div class="callout callout-style-simple callout-tip callout-titled" title="calculus リンク集">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
calculus リンク集
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://cran.r-project.org/web/packages/calculus/index.html">CRAN パッケージページ</a></li>
<li><a href="https://github.com/eguidotti/calculus">GitHub</a></li>
<li><a href="https://www.r-package.org/">r-package.org</a></li>
<li>論文 <span class="citation" data-cites="Guidotti2022">(Guidotti, 2022)</span></li>
</ul>
</div>
</div>
<section id="概要" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="概要">概要</h2>
<p>R パッケージ<code>calculus</code>は，記号微分と数値微分・積分をシームレスに繋いだ機能を提供するパッケージである．</p>
<p>数値微積分は<code>Rcpp</code>パッケージと<code>cubature</code>パッケージを通じて<code>C++</code>バックエンドを用いて提供する．記号微分は外部の記号計算ソフトウェアに依らない実装を提供している．</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">install.packages</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"calculus"</span>)</span></code></pre></div>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="例">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
例
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Einstein の縮約記法</li>
<li>Levi-Civita 記号の計算</li>
<li>一般化 Kronecker デルタ</li>
<li>Taylor 展開</li>
<li>高階導関数</li>
<li>多変数 Hermite 多項式</li>
<li>常微分方程式</li>
<li>微分作用素</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="関連パッケージとの違い">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
関連パッケージとの違い
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://cran.r-project.org/web/packages/numDeriv/index.html"><code>numDeriv</code></a>は数値微分による Jacobian, Hessian 計算を提供するが，それ以上の高階の微分ができず，テンソル値関数の微分もできない．</li>
<li><a href="https://cran.r-project.org/web/packages/tensorA/index.html"><code>tensorA</code></a>は Einstein の縮約記法を提供するが，２階のテンソルまでに限る．</li>
<li><a href="https://cran.r-project.org/web/packages/mpoly/index.html"><code>mpoly</code></a>は１変数の Hermite 多項式を提供するが，多変数には対応していない．</li>
<li><a href="https://cran.r-project.org/web/packages/pracma/index.html"><code>pracma</code></a>は１変数の Taylor 展開を提供するが，多変数には対応していない．</li>
<li><a href="https://cran.r-project.org/web/packages/cubature/index.html"><code>cubature</code></a>は多変数関数の数値積分に対応しているが，その他の（直交）座標には対応していない．</li>
</ul>
<p>特に R は記号計算が苦手である．外部の記号計算ソフトウェアに繋ぐことは試みがあるかもしれないが，R 独自のパッケージ内で扱うシステムは従来なかった．</p>
<ul>
<li><a href="https://cran.r-project.org/web/packages/Ryacas/index.html"><code>Ryacas</code></a>は<a href="http://www.yacas.org/index.html"><code>Yacas</code></a>という外部の記号計算ソフトウェアへのインターフェースを提供する．</li>
<li><a href="https://cran.r-project.org/web/packages/caracas/index.html"><code>caracas</code></a>は R-Python インターフェース<a href="https://rstudio.github.io/reticulate/"><code>reticulate</code></a>を通じて<a href="https://www.sympy.org/en/index.html"><code>SymPy</code></a>という Python の記号計算ライブラリへのインターフェースを提供する．</li>
</ul>
<p>これにより，R の，新たな統計推測手法を実装し論文として公開するために用いる言語という性質を活かすことが，パッケージ<code>calculus</code>の目的にある <span class="citation" data-cites="Guidotti2022">(Guidotti, 2022, p. 4)</span>．</p>
</div>
</div>
</div>
<p>確率過程に対する漸近展開公式では，1000 を超える連立常微分方程式系を解き，その解を通じて多変数 Hermite 多項式の和を計算する必要があり，その際に YUIMA パッケージにおいて記号微分を実行する際に用いられている：</p>
<div class="article-card-container">
  <div class="article-card">
    <a href="https://162348.github.io/posts/2024/Computation/YUIMA.html" target="_blank">
      <img src="https://162348.github.io/posts/2024/Computation/YUIMA_files/figure-html/unnamed-chunk-5-1.png" alt="Article Image" class="article-image">
      <div class="article-content">
        <h3 class="article-title anchored">YUIMA 入門</h3>
        <p class="article-description">確率微分方程式のシミュレーションと推測のためのパッケージ`yuima`の構造と使い方をまとめます．</p>
      </div>
    </a>
  </div>
</div>
</section>
<section id="基本" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="基本"><span class="header-section-number">1</span> 基本</h2>
<section id="概観" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="概観"><span class="header-section-number">1.1</span> 概観</h3>
<p>ベクトル，行列，テンソルはいずれも配列 <code>array</code> として実装されている．特に，ベクトルと行列はテンソルの特殊な場合と理解できるように，実装上も，統一的に Einstein の縮約記法が適用可能である．</p>
<p>すべての関数（＝数学的演算）は，数値バージョンと記号演算バージョンのディスパッチとして実装される．</p>
</section>
<section id="演算" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="演算"><span class="header-section-number">1.2</span> 演算</h3>
<p><code>numeric</code>, <code>complex</code>, <code>character</code>, <code>expression</code> のいずれかの型を持つ，同次元の配列に対して定義されている．</p>
<p>例えば１次元の<code>character</code>に対しては：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(calculus)</span>
<span id="cb2-2">(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a + b"</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%prod%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>i) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%sum%</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%prod%</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"c"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%diff%</span> (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">expression</span>(d <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> e) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%div%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "((a + b) * (0+1i)) - ((d + e) / 3)"</code></pre>
</div>
</div>
<p>ここで，<code>1i</code>, <code>0</code>, <code>expression(d+e)</code>,<code>3</code>はいずれも<code>character</code>に変換されてから実行されている：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">typeof</span>(.Last.value)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "list"</code></pre>
</div>
</div>
<p>この段階では<code>+0</code>などを省くのみで，評価や簡約化はされない．</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="() の消去">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
() の消去
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%0A(a+b)(c+d)%0A"> を <img src="https://latex.codecogs.com/png.latex?%0Aa+bc+d%0A"> と解釈してしまうことなどを防ぐため，すべての変数は<code>()</code>で囲まれる．</p>
<p>この挙動は<code>options(calculus.auto.wrap=FALSE)</code>で無効にできる．</p>
</div>
</div>
</div>
</section>
<section id="評価" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="評価"><span class="header-section-number">1.3</span> 評価</h3>
<p>関数<code>evaluate</code>が提供されている．</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">x <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">array</span>(letters[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>], <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dim=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb6-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">evaluate</span>(x, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">a=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">b=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">c=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">d=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">e=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6</code></pre>
</div>
</div>
<p>関数<code>evaluate</code>はベクトル化されている：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">var <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">a=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">b=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">c=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">d=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">e=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)</span>
<span id="cb8-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">evaluate</span>(x, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span>var)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    2    3    4    5    6
[2,]    3    3    4    5    6    7</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="R Base の eval 関数との違い">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
R Base の eval 関数との違い
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><code>eval</code>関数では，１つの変数の代入しか行えない．</p>
<p>複数与えられた場合でも，最後の１つのみ評価した結果が返される：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">var_list <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">a=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">b=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">c=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">d=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">e=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb10-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">eval</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">parse</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">text=</span>x), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">envir=</span>var_list)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6</code></pre>
</div>
</div>
<p><code>evaluate</code>関数と同様の結果を得るには，成分ごとに繰り返し適用する必要がある．例えば次のように：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">apply</span>(x, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(expr) <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">eval</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">parse</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">text=</span>expr), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">envir=</span>var_list))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="微積分" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="微積分"><span class="header-section-number">2</span> 微積分</h2>
<section id="記号微分" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="記号微分"><span class="header-section-number">2.1</span> 記号微分</h3>
<p>関数<code>derivative</code>は関数を表す文字列<code>f</code>と，微分する変数名を表す<code>var</code>の２つの引数を取る．</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">derivative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sin(x)"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "cos(x)"</code></pre>
</div>
</div>
<p>多変数も同様．高階微分は引数<code>order</code>で指定する： <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%7D%7B%5Cpartial%20x%7D%5Cfrac%7B%5Cpartial%20%5E2%7D%7B%5Cpartial%20y%5E2%7Dy%5E2%5Csin(x)=2%5Ccos(x)%0A"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">derivative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y^2 * sin(x)"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">order =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "2 * cos(x)"</code></pre>
</div>
</div>
</section>
<section id="比較" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="比較"><span class="header-section-number">2.2</span> 比較</h3>
<section id="基本的な構文の違い" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="基本的な構文の違い"><span class="header-section-number">2.2.1</span> 基本的な構文の違い</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%7D%7B%5Cpartial%20x%7D%5Csin(x)%5Cbigg%7C_%7Bx_0%7D%0A"></p>
<p>は次のように計算できる：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1">sym <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">derivative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sin(x)"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb18-2">num <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">derivative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(x) <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sin</span>(x), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span></code></pre></div>
</div>
<div class="cell">
<details class="code-fold">
<summary>出力</summary>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1">result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(</span>
<span id="cb19-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Method =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Symbolic"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Numeric"</span>),</span>
<span id="cb19-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Value =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(sym, num)</span>
<span id="cb19-4">)</span>
<span id="cb19-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(result)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>    Method Value
1 Symbolic     1
2  Numeric     1</code></pre>
</div>
</div>
</section>
<section id="階微分での比較" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="階微分での比較"><span class="header-section-number">2.2.2</span> ４階微分での比較</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20%5E4%7D%7B%5Cpartial%20x%5E4%7D%5Csin(x)%5Cbigg%7C_%7Bx=0%7D%0A"> は次のように計算できる：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1">sym <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">derivative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sin(x)"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">order=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb21-2">num <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">derivative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">f=</span><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(x) <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sin</span>(x), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">var=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">order=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span></code></pre></div>
</div>
<div class="cell">
<details class="code-fold">
<summary>出力</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1">result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(</span>
<span id="cb22-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Method =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Symbolic"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Numeric"</span>),</span>
<span id="cb22-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Value =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(sym, num)</span>
<span id="cb22-4">)</span>
<span id="cb22-5"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(result)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>    Method         Value
1 Symbolic  0.000000e+00
2  Numeric -9.767766e-12</code></pre>
</div>
</div>
</section>
<section id="多変数での比較" class="level4" data-number="2.2.3">




</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">2.2.3 多変数での比較</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Guidotti2022" class="csl-entry">
Guidotti, E. (2022). Calculus: High-dimensional numerical and symbolic calculus in r. <em>Journal of Statistical Software</em>, <em>104</em>(5), 1–37. <a href="https://doi.org/10.18637/jss.v104.i05">https://doi.org/10.18637/jss.v104.i05</a>
</div>
</div></section></div> ]]></description>
  <category>R</category>
  <category>YUIMA</category>
  <guid>https://162348.github.io/posts/2024/Computation/calculus.html</guid>
  <pubDate>Mon, 17 Jun 2024 15:00:00 GMT</pubDate>
</item>
<item>
  <title>総合研究大学院大学５年一貫博士課程のすすめ</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Life/SOKENDAI.html</link>
  <description><![CDATA[ 





<!--







-->
<div class="article-card-container">
<blockquote class="twitter-tweet blockquote"><p lang="ja" dir="ltr">総研大と鎌谷研の学生募集ですわ〜！ <a href="https://t.co/kv27wI4wtV">https://t.co/kv27wI4wtV</a></p>— 総合研究大学院大学お嬢様部 (@sokenojou) <a href="https://twitter.com/sokenojou/status/1798237358514643257?ref_src=twsrc%5Etfw">June 5, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
</div>
<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<div class="callout callout-style-simple callout-caution">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>筆者は <strong>2023 年度入学</strong> であり，その場合についてのみ述べます．</p>
<p>特に，本稿で紹介している 2023 年度の制度から変更がある可能性がありますから，本格的に進学を検討する場合は，記事中に適宜付しました「チェックすべきリンク集」のリンク先もご確認ください．</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled" title="チェックすべきリンク集">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
チェックすべきリンク集
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.soken.ac.jp/">総研大の HP</a></li>
<li><a href="https://www.ism.ac.jp/senkou/">統計科学コースの HP</a></li>
<li><a href="https://www.soken.ac.jp/campuslife/handbook/">総研大の学生便覧</a></li>
</ul>
</div>
</div>
<section id="統数研での５年一貫博士課程" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="統数研での５年一貫博士課程"><span class="header-section-number">1</span> 統数研での５年一貫博士課程</h2>
<section id="総合研究大学院大学について" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="総合研究大学院大学について"><span class="header-section-number">1.1</span> 総合研究大学院大学について</h3>
<p>統計数理研究所において研究教育を受けたい場合，大学院生としては</p>
<div class="callout callout-style-simple callout-caution no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li>総合研究大学院大学
<ul>
<li>先端学術院先端学術専攻
<ul>
<li><a href="https://www.ism.ac.jp/senkou/">統計科学コース</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<p>に所属することになります．</p>
<p>上位概念から解説して参ります．</p>
<ol type="1">
<li><p><u>総合研究大学院大学（略して <strong>総研大</strong>）</u></p>
<p>学部を持たない国立（大学院）大学です．<sup>1</sup> 1988 年の開学以来 35 年の歴史を持ちます．</p></li>
<li><p><u>先端学術院先端学術専攻</u></p>
<p>総合研究大学院大学はもともと複数の専攻に分かれてしましたが，１つの「先端学術院先端学術専攻」に統合され，20 個のコースに分かれている，という扱いになりました．このことにより，定員に関する融通がより効くようになっており，統計科学コースは定員２名ですが，2024 年度のように受験者数２名でも合格者が０名のことも十分に起こり得ます．</p></li>
<li><p><u>統計科学コース</u></p>
<p>全 20 コースの一覧は <a href="https://www.soken.ac.jp/prog/">こちら</a> からご覧になれます．それぞれのコースが <strong>基盤機関</strong> と呼ばれる研究所と対応しており，実際の教育研究はその研究所で行われます．<sup>2</sup> 統数研の他には，国立天文台，宇宙科学研究所（JAXA の一部），国立民俗学博物館，国立情報学研究所などが基盤機関としてあります．</p></li>
</ol>
</section>
<section id="学生数について" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="学生数について"><span class="header-section-number">1.2</span> 学生数について</h3>
<div class="callout callout-style-default callout-caution callout-titled" title="定員について">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定員について
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>５年一貫博士課程の定員は２名ですが，2021 年度から2023 年度は１名（筆者），2024 年度は０名でした．</p>
<p>一方で，３年次編入の定員が６名で，毎年ほとんど６名の方が入学されます．例年，そのうちの <img src="https://latex.codecogs.com/png.latex?2/3"> が社会人です．</p>
</div>
</div>
</div>
<p>2024 年４月１日の学生総数は <img src="https://latex.codecogs.com/png.latex?34"> 名で，うち社会人が <img src="https://latex.codecogs.com/png.latex?24"> 名になります．それ以外の学生を <strong>フルタイム学生</strong> と呼ぶ伝統があります．</p>
<p>社会人学生の方は基本的にリモートでの研究指導を受けることになりますから，研究所には半年に一回来れれば良い方だという人も多いです．</p>
<p>そのため，フルタイム学生は，例え研究所に来てもほとんど学生に会えないことが多いです．ましてや同期の存在は期待できないでしょう．</p>
</section>
<section id="sec-teachers" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="sec-teachers"><span class="header-section-number">1.3</span> 教員陣について</h3>
<div class="callout callout-style-simple callout-tip callout-titled" title="チェックすべきリンク集">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
チェックすべきリンク集
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.ism.ac.jp/ism_info_j/theme_j.html">統数研の教員と専門一覧</a></li>
<li><a href="https://www.ism.ac.jp/senkou/subject/teacher.html">統数研の教員の主な教育内容</a></li>
<li><a href="https://cplan-public.soken.ac.jp/public/web/Syllabus/WebSyllabusKensaku/UI/WSL_SyllabusKensaku.aspx">総研大のシラバス</a></li>
</ul>
</div>
</div>
<p>統数研には授業を開講している専任教員だけで 81 人います．</p>
<p>また，上掲の <a href="https://www.ism.ac.jp/ism_info_j/theme_j.html">統数研の教員と専門一覧</a> から分かる通り，凡そ統計に関連のある分野が理論・応用を分けずに網羅されており，それもスター研究者が揃っています．</p>
<p>統計と言ってもあまりにその関連分野が広いため，筆者は統数研の真の裾の広さは，入学後に <a href="https://www.ism.ac.jp/meetings/seminar.html">統計数理セミナー</a> を通じて，やっと気づきました．</p>
<div class="callout callout-style-default callout-caution callout-titled" title="統計数理セミナーについて">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
統計数理セミナーについて
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>統数研では <a href="https://www.ism.ac.jp/meetings/seminar.html">統計数理セミナー</a> と呼ばれるセミナーが毎週水曜日 16:00~17:20 に開催されており，全ての教員は毎年１回は発表することになっています．</p>
<p>統計数理セミナーは学生は出席報告とレポート提出をすることで単位を取得することも出来ます．そこで多くの分野の最新の研究に触れることになり，必ずや新たな発見があることでしょう．</p>
<p>特に，数学と数理統計の一部しか知らずに入学した筆者にとっては，実際の統計モデリングの研究（地震データ，天文データ，統数研が実施している <a href="https://www.ism.ac.jp/kokuminsei/">国民性調査</a> のサーベイデータ）から，機械学習の理論研究からマテリアルズインフォマティクスへの応用まで，無限次元の情報幾何から MCMC まで，思いつく順番に挙げたまでですが，多岐に渡る研究分野の存在と，その分野でのスターが揃っていることを，入学してしばらくしてようやく理解されてきました．</p>
<p>統計数理セミナーは２年目ですが，何周しても飽きないように思われます．</p>
</div>
</div>
</div>
</section>
<section id="授業について" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="授業について"><span class="header-section-number">1.4</span> 授業について</h3>
<p>ここまでご説明しました通り，統数研では教員が学生数の３倍近くいるため，授業は大変充実しています．しかしその分，多くの授業では教員と１対１になることが想定されます．</p>
<p>ですが実際は，先生がメーリングリストで追加の参加者を呼びかけたり，学生側が他の学生を誘い合わせたりするため，１対１になることは少ないです．<sup>3</sup></p>
<p>開講が珍しい授業ですと，教員陣も学生として参加していることも珍しくありません．</p>
<p>また，社会人学生が <img src="https://latex.codecogs.com/png.latex?2/3"> を占めている以上，リモートやハイブリッドでの開講や日程調整については極めて柔軟に対応されます．</p>
<p>筆者は統数研から遠い片道２時間弱の場所に住んでいるため，なるべく対面授業の日は１日にまとめたく，日程を調整していただきました．</p>
</section>
<section id="sec-library" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="sec-library"><span class="header-section-number">1.5</span> 図書室について</h3>
<p>ここで特筆したいのは，<a href="https://www.ism.ac.jp/library/index_j.html">図書室</a> の存在です．</p>
<p>統数研の歴史は極めて長く，戦前の 1944 年に設立され，2004 年度までは文部省直轄の研究機関でした．</p>
<p>そのこともあってか，統計と数学に関する膨大な文献が揃っており，筆者は今のところ参照したい書籍が統数研図書室で見当たらなかったことはほとんどありません．</p>
<p>貴重な文献も，「文部省統計数理研究所」の印と共に２冊ほど所蔵されている，という経験が何度もあります．最新の書籍も寄贈や同研究所の先生方がすでに買っているかなどのことも多く，よく揃っています．</p>
<p>なお，電子リソース（論文や e-book）も，統数研内のネットワークからアクセスすることで殆どが閲覧できます．自宅にいても，<code>ssh</code> を通じた所内ネットワークへのリモートアクセスで随時論文や電子リソースにアクセスすることができます．</p>
</section>
<section id="sec-support" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="sec-support"><span class="header-section-number">1.6</span> 金銭支援について</h3>
<div class="callout callout-style-simple callout-tip callout-titled" title="チェックすべきリンク集">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
チェックすべきリンク集
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.ism.ac.jp/senkou/life/scholarship.html">統数研のページ「奨学金等」</a></li>
<li><a href="https://www.soken.ac.jp/campuslife/tuition/">総研大のページ「学費・各種経済支援」</a></li>
</ul>
</div>
</div>
<ol type="1">
<li><p>リサーチ・アシスタント</p>
<p>５年の間いつでも，申請をすることで RA としての雇用を希望することができます．月額 10 万円弱の給与が７月から３月まで支給されます．１年ごとの更新です．<sup>4</sup></p></li>
<li><p><a href="https://www.soken.ac.jp/campuslife/tuition/sp_researcher_ch/">総研大特別研究員</a></p>
<p>博士後期課程では，特別研究員制度へ応募することができます．すなわち，学振の特別研究員（DC1, DC2）に加えて，総研大独自の特別研究員制度もあるということです．</p></li>
</ol>
<p><a href="https://www.soken.ac.jp/campuslife/tuition/exemption/">授業料免除</a> や <a href="https://www.soken.ac.jp/campuslife/tuition/jasso/">奨学金</a> を除いて，統計科学コース独自のものは基本この２つです．</p>
<p>他にも，次の研究支援があります：</p>
<ul>
<li><p>研究環境整備費</p>
<p>初年度に 20 万円を上限に，PC やモニターなどの研究環境を整えるために，学生に予算が配分されます．</p></li>
<li><p><a href="https://www.soken.ac.jp/education/dispatch/sokendai_studentdispatchprogram/">SOKENDAI 研究派遣プログラム</a></p>
<p>３ヶ月前後の海外での研究滞在を支援するプログラムです．</p></li>
</ul>
</section>
<section id="修士号について" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="修士号について"><span class="header-section-number">1.7</span> 修士号について</h3>
<p>五年一貫博士課程では，基本的には博士（統計科学）の学位のみが授与されます．<sup>5</sup></p>
<p>しかし，特例として，２年以上在籍し，特定の要件を満たした場合は，中途退学と同時に修士の学位を得ることもできます <span class="citation" data-cites="総合研究大学院大学学生便覧2023">(学生便覧, 2023, p. 5)</span>．</p>
</section>
</section>
<section id="学生生活私の場合" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="学生生活私の場合"><span class="header-section-number">2</span> 学生生活（私の場合）</h2>
<p>以上が統数研で学生生活をするにあたっての，基本的な環境です．いかが思われたでしょうか？</p>
<p>私にとっては現状，この上なく理想的な環境だと感じています．主な理由は次の通りです：</p>
<div class="callout callout-style-simple callout-important no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ol type="1">
<li>図書室が充実していて，殆どの文献をすぐに参照することができる 1.5</li>
<li>計算機準備予算も付いたので，大きなモニターと高性能な計算機で研究・勉強ができる 1.6</li>
<li>先生方との距離が近く，研究に関する相談もしやすい 1.3</li>
<li>事務の方との距離が近く，込み入った相談や提出物の忘れなども，時には先回りをして親切に対応していただける 2.2</li>
<li>およそ統計に関連する分野のエキスパートが，単一の機関に揃っている 2.4</li>
</ol>
</div>
</div>
</div>
<p>というように，研究のための環境はこれ以上ないほどのものだと言って過言でないでしょう．</p>
<section id="sec-warning" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sec-warning"><span class="header-section-number">2.1</span> 【注意喚起】同期が居ないことについて</h3>
<p>ただし，通常の大学院生活とは全く違うものになることは確かです．最大の違いは，研究室の同期・先輩が居ない代わりに，教員・事務の先生方との距離が近い，ということです．<sup>6</sup></p>
<p>最初から「大人」としての振る舞いが求められる，とでも言うべきでしょうか．</p>
<p>換言すれば，学生は研究所内では極めて少数派ですので，先生方も事務の方々も，学生が何で困っているのか，本当に想像がつかない場合が多いのです．さらに言えば，あなたが学生であることも言わなきゃわからないことが多いです．</p>
<p>ですから，少しでもわからないことや不安なことがあれば，はっきりと「自分は学生である」ということと「こういうことに困っている」と言語化してコミュニケーションを取らないと，困った状況に陥りやすいことが，最大の注意点でしょう．</p>
<p>例えば研究や資料準備などがうまく行っていなくて後ろめたい気持ちがあり，コミュニケーションを避けがちな状態では尚更で，普通の大学院生活よりも，「時すでに遅し」という状況に陥りやすいと言えるかも知れません．</p>
<p>この点から，「基本的には，特別な理由がない限り，５年一貫博士課程は勧めない」という立場の先生方も多いです．</p>
<p>研究室内に溜まった暗黙のノウハウ，のようなものもありませんから，奨学金申請，授業料免除申請，研究費の使い方，申請書の書き方，ポスターの作り方などは，積極的に先生方と事務の方々に，時には，「すみません，一から何をすべきか分からないかもしれません」と「迷惑」をかけていく姿勢が必要になることもあるかもしれません．</p>
<p>その点，研究所外でのつながりも重要です．</p>
<div class="callout callout-style-default callout-caution callout-titled" title="さらに踏み込んだアドバイス">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
さらに踏み込んだアドバイス
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>「迷惑」と書きましたのは，実際は迷惑ではないからです．自分で調べて解決しようとしても，すでに慣習やルールがあるかもしれませんから，自己流にやって手遅れになるよりかは，わからないことはわからないと聞くのが「正解」で想定されています．</p>
<p>自分からアクションを起こさなきゃいけないのは，５年一貫制博士課程の入学者が毎年０か１かですので，先生方も事務の方も本当に忘れている場合の方が多いです．その意味で，学生はマイノリティですから，「自分は学生としてここにいる」という気持ちを強く持って，研究生活を歩むのが大事だと思います．</p>
<p>社会人学生が大半を占めるように，実際，学生数が少ないため一人当たりのリソースが多く，社会人の方が共同研究を遂行するには最高の環境でしょう．しかし，学生のみなさんも，以上の理由からみすみすこの環境を逃すにはもったいないと考えます．</p>
<p>そこで，本記事を用意したのでした．</p>
</div>
</div>
</div>
</section>
<section id="sec-life-support" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="sec-life-support"><span class="header-section-number">2.2</span> 学生生活の支援</h3>
<p>しかし，先生方も事務の方も，困っている学生を助けたいと常に配慮してくださいますし，上述の通り，支援制度と研究環境は大変充実しています．</p>
<p>また，とりわけ，大学院担当の事務の方々は，学生の背景も覚えていてくださり，申請書を書く際にいくつかわからない項目や揃わない書類があった際は親身になって相談に乗ってくださいましたし，筆者のミスで出席連絡メールが不着であったときは「司馬くん出席してるよね？もしかして送信先ミスってない？」と気づいてくださったりもしました．</p>
<p>ですから，良い環境で研究したいと主体的に思える方は，ぜひ統数研の５年一貫博士課程を検討してみて欲しい，その気持ちで筆を取っております．</p>
<p>ただし，健康診断も職員と同一扱いです．めっちゃすごいところでの検診になります．</p>
</section>
<section id="研究室の様子" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="研究室の様子"><span class="header-section-number">2.3</span> 研究室の様子</h3>
<p>先ほど第 2.1 節で「同期がいない」と強調しました．５年一貫博士課程の場合は，通例あなたひとりになりますから，同期はいないと思って良いでしょう．</p>
<p>一方で，研究室には先輩がいる可能性があります．次に述べます通り，学生がいる研究室が集中しているためです．</p>
<p>筆者の入学時，５年一貫博士課程生は筆者を入れて４名，３年次編入学生が33名でした．</p>
<div class="callout callout-style-default callout-important no-icon callout-titled" title="学生が３名以上居る研究室（2023 年度当時）">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
学生が３名以上居る研究室（2023 年度当時）
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_13">野間研究室</a> ５名</li>
<li><a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_07">日野研究室</a> ５名</li>
<li><a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_28">二宮研究室</a> ５名</li>
<li><a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_30">藤澤研究室</a> ４名</li>
<li><a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_39">持橋研究室</a> ４名</li>
</ul>
<!--
* 上野研究室 ２名
* 中野研究室 ２名
* 福水研究室 ２名
* 吉田研究室 １名
* 庄研究室 １名
* 松井研究室 １名
* 南研究室 １名
* 山下研究室 １名
* 金藤研究室 １名
* 志村研究室 １名
* 鎌谷研究室 １名
-->
</div>
</div>
<p></p><div id="tweet-78877"></div><script>tweet={"url":"https:\/\/twitter.com\/daiti_m\/status\/1801219144446320867","author_name":"Daichi Mochihashi","author_url":"https:\/\/twitter.com\/daiti_m","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"ja\" dir=\"ltr\"\u003E総研大統計科学コース(統数研)の現役学生の司馬さんによる紹介、とても良い記事だと思いました。\u003Cbr\u003E同期が少ないのは国立研究所全体に共通ですが、研究室によっては(私の研究室も含めて)、学生が複数いることもあります。機械学習に関しては研究室を越えた交流があるかと思います。 \u003Ca href=\"https:\/\/t.co\/gSrGCg3kv4\"\u003Ehttps:\/\/t.co\/gSrGCg3kv4\u003C\/a\u003E\u003C\/p\u003E&mdash; Daichi Mochihashi (@daiti_m) \u003Ca href=\"https:\/\/twitter.com\/daiti_m\/status\/1801219144446320867?ref_src=twsrc%5Etfw\"\u003EJune 13, 2024\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-78877").innerHTML = tweet["html"];</script><p></p>
</section>
<section id="sec-lectures" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="sec-lectures"><span class="header-section-number">2.4</span> 授業について</h3>
<div class="callout callout-style-simple callout-tip callout-titled" title="チェックすべきリンク集">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
チェックすべきリンク集
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://www.ism.ac.jp/senkou/subject/">統数研ページ「担当教員・授業科目」</a></li>
<li><a href="https://www.ism.ac.jp/senkou/about/doctoral_thesis.html">統数研ページ「終了要件」</a></li>
</ul>
</div>
</div>
<p>５年一貫博士課程の卒業要件では，必修授業（研究指導）を除いて 20 単位の取得が必要です <span class="citation" data-cites="総合研究大学院大学学生便覧2023">(学生便覧, 2023, p. 22)</span>．</p>
<p>私は修士の１年の間に（必修を除いて）12 単位を取得しましたので，あと４授業分を４年で消化すれば良いことになります．</p>
<p>その 12 単位は１年の間に取るべきだと考えた入門的なオムニバス講義でしたから，単位取得や成績の確保に追われることなく，自分の学びたいことをじっくり学ぶことができます．<sup>7</sup></p>
<div class="callout callout-style-default callout-caution callout-titled" title="筆者が１年目にとった授業">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
筆者が１年目にとった授業
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>統計科学コースには準必修と呼ぶべき「取らなきゃいけない授業」はありませんが，修士のうちに履修すべきとされる基本的な内容を扱う <a href="https://www.ism.ac.jp/senkou/subject/subject1.html"><strong>基礎科目</strong></a> と呼ばれる授業が全部で８つあります．</p>
<p>そのうち，次の３つを筆者は履修しました．</p>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="多変量解析">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
多変量解析
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>疫学と医療統計の <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_13">野間先生</a> や，環境統計の <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_12">金藤先生</a>，調査の <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_23">前田先生</a> と <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_21">朴先生</a>，金融データの <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_15">山下先生</a>，差分プライバシーの <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_14">南先生</a> と，各分野で多変量解析を実際に使って研究なさっている先生方によるオムニバス講義です．</p>
<p>何よりも，各分野によって多変量解析への姿勢が少しずつ違い，教科書で読むだけでは無味乾燥であった多変量解析が極めて身近で手触りを感じました．</p>
<p>以下はシラバスの抜粋です．</p>
<blockquote class="blockquote">
<p>下記の構成で授業を行う。（）内は主な担当教員である。</p>
</blockquote>
<ol type="1">
<li>ガイダンス（多変量データ解析概観）（野間）</li>
<li>多変量データの取得と合成変量モデルによる解析（前田）</li>
<li>多変量の推測統計の初歩（多変量正規分布，多変量T2検定など）（金藤）</li>
<li>カテゴリカルデータの解析（分割表の解析）（野間）</li>
<li>多変量データのための回帰分析・重回帰分析 (1)（南）</li>
<li>多変量データのための回帰分析・重回帰分析 (2)（南）</li>
<li>離散変数と定性データの分類・統計モデル (1) （山下）</li>
<li>離散変数と定性データの分類・統計モデル (2)（山下）</li>
<li>一般化線型モデル(1) 一般論，推測手法（金藤）</li>
<li>一般化線形モデル(2) ロジスティック回帰，ポアソン回帰（野間）</li>
<li>主成分分析と関連手法（朴）</li>
<li>因子分析・共分散構造分析（朴）</li>
<li>正準相関分析と数量化の方法（前田）</li>
<li>クラスター分析・MDS（前田）</li>
<li>まとめ・発展的話題（山下・南）</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="計算推論基礎">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
計算推論基礎
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>指導教員の <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_03">鎌谷先生</a> による MCMC の授業だけでなく，データ同化を研究する <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_06">中野先生</a> から粒子フィルターについて，統計物理学出身の <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_33">坂田先生</a> からグラフィカルモデルについて，自然言語処理のエキスパートである <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_39">持橋先生</a> からノンパラメトリックベイズについて学べた授業です．</p>
<p>どの内容に対しても先生方の背景がピッタリであることにお気づきでしょうか．</p>
<p>計算推論の手法は数理的に高度ですが，どの分野から生まれた手法で，どのような問題に適用されているかをセットで学ぶことで，稀有な直感が醸成されます．</p>
<p>指定の教科書を読むだけでは得られない肌感を得るのに最適な授業で，出席する前と後での理解度の差が歴然としていました．</p>
<p>以下はシラバスの抜粋です．</p>
<ol type="1">
<li>イントロダクション</li>
<li>乱数の使い方：ブートストラップ法，モンテカルロ法</li>
<li>ＭＣＭＣ入門</li>
<li>ベイズ統計入門</li>
<li>階層ベイズモデルとベイズ平滑化</li>
<li>状態空間モデル，逐次ベイズ推定，粒子フィルタ</li>
<li>粒子フィルタの応用と発展</li>
<li>グラフィカルモデルの基礎</li>
<li>有向グラフと無向グラフ</li>
<li>様々なグラフィカルモデル</li>
<li>確率伝搬法とその周辺</li>
<li>混合ガウス分布とK-means法</li>
<li>EMアルゴリズム</li>
<li>変分ベイズEMアルゴリズムとその応用</li>
<li>Variational Autoencoder (VAE)とその応用</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="統計的機械学習基礎">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
統計的機械学習基礎
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>日本のカーネル法・深層学習研究の第一線を走っていらっしゃる <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_29">福水先生</a> から直接深層学習について学べたことが大変印象深かったものでした．</p>
<p>それだけでなく，マテリアルズインフォマティクスで機械学習がどう活かされるかを <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_16">吉田先生</a> と <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_19">Wu 先生</a> 先生から学び，学習理論を <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_07">日野先生</a> から学び，Gauss 過程法を <a href="https://www.ism.ac.jp/senkou/subject/teacher.html#page_08">松井先生</a> から学ぶことが出来る授業です．</p>
<p>以下はシラバスの抜粋です．</p>
<ol type="1">
<li>総論（機械学習とは？　教師あり学習、教師無し学習、モデル選択）</li>
<li>カーネル法　（カーネル法の基礎と方法）</li>
<li>サポートベクターマシン I　（SVMの基礎）</li>
<li>サポートベクターマシン　II　（SVMの基礎と発展）</li>
<li>アンサンブル法（ランダムフォレスト、バギング、ブースティングなど）</li>
<li>深層学習 I　（深層学習の基礎）</li>
<li>深層学習 II 　（深層学習の発展）</li>
<li>ガウス過程I（ガウス過程の概念と基礎）</li>
<li>ガウス過程II（カウス過程の方法）</li>
<li>様々な問題設定 I（転移学習、半教師付き学習）</li>
<li>様々な問題設定II （強化学習）</li>
<li>様々な問題設定III （能動学習、実験計画、応用的話題）</li>
<li>統計的学習理論 I　（訓練誤差と汎化誤差の理論）</li>
<li>統計的学習理論 II　（訓練誤差と汎化誤差の理論）</li>
<li>まとめ</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
<p>また，統数研が毎年開催している公開講座も，学生ならば無料で参加ができます．</p>
<!--

## 受験に当たって

### 定員

### 入試の基本情報

::: {.callout-tip appearance="simple" title="チェックすべきリンク集"}
* [統数研のページ](https://www.ism.ac.jp/senkou/admission/examination.html)
* [総研大のページ](https://www.soken.ac.jp/admission/application_info/stat/index.html)
:::

### 過去問と解答

a

-->
</section>
</section>



<div id="quarto-appendix" class="default"><section id="終わりに" class="level2 appendix" data-number="3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">3</span> 終わりに</h2><div class="quarto-appendix-contents">

<p>ページ下部のコメントからもぜひお気軽にお問い合わせください．</p>



</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-総合研究大学院大学学生便覧2023" class="csl-entry">
学生便覧. (2023). <em>総合研究大学院大学</em>. <a href="https://www.soken.ac.jp/campuslife/handbook/">https://www.soken.ac.jp/campuslife/handbook/</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>なお，博士課程のみの国立大学院大学としては，日本初であるという（<a href="https://ja.wikipedia.org/wiki/%E7%B7%8F%E5%90%88%E7%A0%94%E7%A9%B6%E5%A4%A7%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6">Wikipedia</a>）．↩︎</p></li>
<li id="fn2"><p>総研大本部は葉山にキャンパスありますが，そこに通うことは，統合進化科学研究センターに対応する統合進化科学コースを除いてありません．↩︎</p></li>
<li id="fn3"><p>筆者もまだ経験していませんが，受講者が２人だけということは３回ほど経験しています．↩︎</p></li>
<li id="fn4"><p>収入の要件はあります．また，指導教員が別の予算を持っている場合は，その予算から追加で（二重に）雇用される場合もあります．↩︎</p></li>
<li id="fn5"><p>場合によっては博士（学術）も授与されます．↩︎</p></li>
<li id="fn6"><p>現在，統数研では，複数名の学生がいる教員も５名ほど居ます．ですが，それでも同期は基本的におらず，先輩もすぐに卒業してしまったり，社会人であり殆ど連絡を取る機会がない，という場合が多いです．同期・先輩の存在を期待して進学するのは危険でしょう．↩︎</p></li>
<li id="fn7"><p>成績が気になるなら，計 20 単位を５年で消化することにすれば，１学期に１授業ずつ取ることも出来ますからね．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Opinion</category>
  <category>Life</category>
  <guid>https://162348.github.io/posts/2024/Life/SOKENDAI.html</guid>
  <pubDate>Fri, 24 May 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Life/Images/博士（統計科学）.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>新時代の MCMC を迎えるために</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Computation/MCMC.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../static/Posters/ISM-OH2024.pdf"><img src="https://162348.github.io/static/Posters/ISM-OH2024.jpg" class="img-fluid figure-img" style="width:50.0%" alt="「新時代の MCMC を迎えるために」統数研オープンハウス（タップで PDF 閲覧）"></a></p>
<figcaption>「新時代の MCMC を迎えるために」統数研オープンハウス（タップで PDF 閲覧）</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>以下は，5月24日 10:30~12:30 （コアタイム：10:30~11:10）に行われた <a href="../../../static/Sessions.html#sec-ISM-openhouse2024">2024年度統数研オープンハウス</a> ポスターセッション（掲載 No.&nbsp;E1）に於て発表されたポスターに関する解説記事です．</p>
</div>
</div>
</div>
<section id="導入" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="導入"><span class="header-section-number">1</span> 導入</h2>
<section id="mcmc-小史" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="mcmc-小史"><span class="header-section-number">1.1</span> MCMC 小史</h3>
<p>現状，HMC (Hamiltonian Monte Carlo) という約 40 年前に提案された MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．<sup>1</sup></p>
<p>この手法はもともと <span class="citation" data-cites="Duane+1987">(Duane et al., 1987)</span> が場の量子論に特化した Monte Carlo 法として提案したものであったところを，<span class="citation" data-cites="Neal1994">(Neal, 1994)</span> が一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．</p>
<p>ということで，HMC は，オリジナルの MCMC が物理学者 <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> に由るように，物理学において着想された MCMC 手法であったのです．</p>
<p><strong>そのHMC が，提案から 40 年目を迎える前に，更なる効率的な手法によって代替されようとしています</strong>．</p>
<p>そのきっかけ <span class="citation" data-cites="Peters-deWith2012">(Peters &amp; de&nbsp;With, 2012)</span> も，やはり，物理学（正確には物質科学）からの着想でした．</p>
</section>
<section id="mcmc-とは何か" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="mcmc-とは何か"><span class="header-section-number">1.2</span> MCMC とは何か？</h3>
<p>MCMC とは，確率変数をシミュレーションする際に用いられる汎用的アルゴリズムです．</p>
<p>一様分布や正規分布などの名前がついた分布ではない場合，どのようにすればその分布に従う確率変数をシミュレーションできるのか？は，古くからの問題でした．</p>
<p>実際，「MCMC では空間を探索するマルコフ連鎖を構成し，その足跡を辿るとちょうど確率変数のシミュレーションになっている」と種明かしを聞いても，「なぜそのような回りくどい方法を使うのか？」「もっと良い方法はないのか？」と思っても当然でしょう．</p>
<p>ですが，MCMC を，発明された経緯を辿り，物理学の問題意識から見てみると，実は極めて自然な発想に思えてくるかもしれません．</p>
<p>以降，MCMC の起源である物理系のシミュレーション（第 2 節）を例に取り，分子動力学法（第 2.1 節），Metropolis 法（第 2.2 節）を復習します．</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-md-output-1.png" class="img-fluid figure-img"></p>
<figcaption>分子動力学法の出力（第 2.1 節）<sup>2</sup></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-mh-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Metropolis 法の出力（第 2.2 節）</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>これを基礎として，近年提案された非対称な MCMC 手法（第 3 章），そして最新の連続時間 MCMC 手法（第 4 章）を紹介します．</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-lmh-output-1.png" class="img-fluid figure-img"></p>
<figcaption>非対称 MCMC 法（Lifted Metropolis 法）の出力（第 3 章）</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-pdmp-output-1.png" class="img-fluid figure-img"></p>
<figcaption>連続時間 MCMC 法（Zig-Zag サンプラー）の出力（第 4 章）</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sec-figs" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="sec-figs"><span class="header-section-number">1.3</span> 自己相関・軌跡の一覧</h3>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-mh-auto-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Metropolis 法の自己相関関数（第 2.2 節）</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-lmh-auto-output-1.png" class="img-fluid figure-img"></p>
<figcaption>非対称 MCMC 法（Lifted Metropolis 法）の自己相関関数（第 3 章）</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-pdmp-auto-output-1.png" class="img-fluid figure-img"></p>
<figcaption>連続時間 MCMC 法（Zig-Zag サンプラー）の自己相関関数（第 4 章）</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-1-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Metropolis 法の軌跡（第 2.2 節）</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-4-output-1.png" class="img-fluid figure-img"></p>
<figcaption>非対称 MCMC 法（Lifted Metropolis 法）の軌跡（第 3 章）</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-6-output-1.png" class="img-fluid figure-img"></p>
<figcaption>連続時間 MCMC 法（Zig-Zag サンプラー）の軌跡（第 4 章）</figcaption>
</figure>
</div>
</div>
</div>
</div>
<!--

### Comparison

::: {layout-ncol=3}
![Autocorrelation of the random walk Metropolis algorithm from section [-@sec-MH]](../../../docs/posts/2024/Computation/MCMC_files/figure-html/fig-mh-auto-output-1.png)

![Autocorrelation of the lifted Metropolis algorithm from chapter [-@sec-LMH]](../../../docs/posts/2024/Computation/MCMC_files/figure-html/fig-lmh-auto-output-1.png)

![Autocorrelation of the Zig-Zag sampler from chapter [-@sec-PDMP]](../../../docs/posts/2024/Computation/MCMC_files/figure-html/fig-pdmp-auto-output-1.png)
:::

::: {layout-ncol=3}
![Trajectory of the random walk Metropolis algorithm from section [-@sec-MH]](../../../docs/posts/2024/Computation/MCMC_files/figure-html/fig-1-output-1.png)

![Trajectory of the lifted Metropolis algorithm from chapter [-@sec-LMH]](../../../docs/posts/2024/Computation/MCMC_files/figure-html/fig-4-output-1.png)

![Trajectory of the Zig-Zag sampler from chapter [-@sec-PDMP]](../../../docs/posts/2024/Computation/MCMC_files/figure-html/fig-6-output-1.png)
:::

-->
</section>
</section>
<section id="sec-origin" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-origin"><span class="header-section-number">2</span> MCMC の起源</h2>
<div class="callout callout-style-default callout-caution callout-titled" title="よりみち：どうして MCMC が必要だったのか？">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
よりみち：どうして MCMC が必要だったのか？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> では，温度 <img src="https://latex.codecogs.com/png.latex?T"> 一定の条件下で <img src="https://latex.codecogs.com/png.latex?N"> 粒子系をシミュレートし，任意の物理量 <img src="https://latex.codecogs.com/png.latex?F"> に対してその相空間上の平均 <img src="https://latex.codecogs.com/png.latex?%0A%5Clangle%20F%5Crangle=%5Cfrac%7B%5Cint%20Fe%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7Ddp%7D%7B%5Cint%20e%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7Ddp%7D%0A"> を効率的に計算する汎用アルゴリズムが提案された．これが現在では Metropolis 法と呼ばれている．<sup>3</sup></p>
<p><span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> では <img src="https://latex.codecogs.com/png.latex?N"> が数百になる場合を考えており（時代を感じるスケール感），当然愚直な数値積分は現代の計算機でも実行可能ではない．そこで Monte Carlo 法を考えることになるが，当時 Monte Carlo 法といえば，一様乱数を用いた計算法の全般を指し，具体的には <img src="https://latex.codecogs.com/png.latex?%5Clangle%20F%5Crangle"> を重点サンプリング推定量 <img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BF%7D=%5Cfrac%7B%5Csum_%7Bn=1%7D%5ENF(%5Comega)e%5E%7B-%5Cfrac%7BE(%5Comega)%7D%7BkT%7D%7D%7D%7B%5Csum_%7Bn=1%7D%5ENe%5E%7B-%5Cfrac%7BE(%5Comega)%7D%7BkT%7D%7D%7D%0A"> で推定することを指した．<sup>4</sup></p>
<p>しかしこれでは，高エネルギーな状態・低エネルギーな状態を全く区別せず，状態 <img src="https://latex.codecogs.com/png.latex?%5Comega%5Cin%5COmega"> を完全に一様に生成するため，その分だけ非効率である．</p>
<p>これを低減することが出来れば Monte Carlo 法の更なる効率改善に繋がる．こうして，Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7BZ%7De%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7D"> から直接的サンプリングする方法が模索されたのである．</p>
</div>
</div>
</div>
<p><span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> では，エネルギー <img src="https://latex.codecogs.com/png.latex?E"> を持つ系の Boltzmann-Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7BZ%7De%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7D"> から直接サンプリングする方法が探求されました．</p>
<p>ここでは簡単のため，１粒子が次のようなポテンシャル <img src="https://latex.codecogs.com/png.latex?%0AU(x)=%5Cfrac%7Bx%5E2%7D%7B2%7D+%5Cfrac%7Bx%5E4%7D%7B4%7D%0A"> に従って運動する場合を考えましょう：</p>
<div id="fig-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/Files/potential.svg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;1: ポテンシャル <img src="https://latex.codecogs.com/png.latex?U"> のプロット
</figcaption>
</figure>
</div>
<p>このポテンシャルに関する Boltzmann-Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi%5C,%5Cpropto%5C,e%5E%7B-%5Cbeta%20U%7D"> は次のような形になります：<sup>5</sup></p>
<div id="fig-3" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/Files/Gibbs.svg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;2: ポテンシャル <img src="https://latex.codecogs.com/png.latex?U"> が定める Botlzmann-Gibbs 分布のプロット
</figcaption>
</figure>
</div>
<!--
```
\begin{tikzpicture}
\begin{axis}[
    axis lines = middle,
    axis line style={->},
    xlabel = $x$,
    ylabel = {$e^{-\beta U(x)}$},
    xlabel style={at={(ticklabel* cs:1)}, anchor=north west},
    ylabel style={at={(ticklabel* cs:1)}, anchor=south west},
    xmin=-1.2, xmax=1.2,
    ymin=0, ymax=1,
    xtick distance=1,
    ytick=\empty,
    grid=both,
    grid style={gray!30, dashed},
    minor tick num=1,
    width=10cm,
    height=8cm,
    legend pos=north east,
    legend cell align=left,
    legend style={fill=white, fill opacity=0.8, draw opacity=1, text opacity=1, font=\small},
]
\addplot[domain=-2:2, samples=100, smooth, thick, minty] {exp(-x^2 - x^4)};
\addlegendentry{$y = e^{-\frac{U(x)}{kT}}$}
\end{axis}
\end{tikzpicture}
```
-->
<p>２つのプロットを見比べると，低エネルギー状態ほど出現確率が高く，エネルギーが上がるにつれて急激に出現確率が下がることがわかります．以降，<img src="https://latex.codecogs.com/png.latex?%5Cbeta=1"> としましょう．<sup>6</sup></p>
<section id="sec-MD" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sec-MD"><span class="header-section-number">2.1</span> 分子動力学法</h3>
<p>統計力学によれば，<img src="https://latex.codecogs.com/png.latex?%5Cbeta=1"> で定まる温度とポテンシャル <img src="https://latex.codecogs.com/png.latex?U"> を持つ Boltzmann-Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-U%7D"> は，温度 <img src="https://latex.codecogs.com/png.latex?T=%5Cfrac%7B1%7D%7Bk_B%5Cbeta%7D"> を持つ熱浴に接している力学系を，長時間シミュレーションして時間平均を取ることでサンプリングできるはずです．</p>
<p>このように，力学に基づいて物理過程を数値シミュレーションをすることを通じてサンプリングを達成する方法を <a href="https://ja.wikipedia.org/wiki/%E5%88%86%E5%AD%90%E5%8B%95%E5%8A%9B%E5%AD%A6%E6%B3%95"><strong>分子動力学法</strong></a> といいます．</p>
<p>これを実際にやってみます．図 1 で定めたポテンシャルを持つ粒子を考えます．<sup>7</sup></p>
<p>続いてこれを温度 <img src="https://latex.codecogs.com/png.latex?T=%5Cfrac%7B1%7D%7Bk_B%5Cbeta%7D"> を持つ熱浴と相互作用させます．例えば，ポテンシャル 1 の <img src="https://latex.codecogs.com/png.latex?x=0"> の位置に半透性の壁を置き，確率 <img src="https://latex.codecogs.com/png.latex?1/2"> でこの温度 <img src="https://latex.codecogs.com/png.latex?T"> の壁の粒子と弾性衝突するとします．（残りの確率 <img src="https://latex.codecogs.com/png.latex?1/2"> では衝突せずに通過する）．</p>
<p>壁の粒子の速度は Maxwell の境界条件から与えられるものとすれば，次のようにして粒子の位置 <img src="https://latex.codecogs.com/png.latex?x"> がシミュレートできます：<sup>8</sup></p>
<div id="0dda1a40" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2024</span>)</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> U(x):</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> pi(x):</span>
<span id="cb1-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.exp(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>U(x))</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> md(num_samples, initial_state, initial_velocity, timestep<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>):</span>
<span id="cb1-12">    samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [initial_state]</span>
<span id="cb1-13">    current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> initial_state</span>
<span id="cb1-14">    current_velocity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> initial_velocity</span>
<span id="cb1-15">    current_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-16"></span>
<span id="cb1-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb1-18">        proposed_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> current_velocity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> timestep</span>
<span id="cb1-19">        current_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> timestep</span>
<span id="cb1-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> proposed_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>:</span>
<span id="cb1-21">            current_velocity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.sign(current_velocity) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.log(np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-7</span>))</span>
<span id="cb1-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb1-23">            current_velocity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_velocity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ( current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span> ) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> timestep</span>
<span id="cb1-24">            current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> proposed_state</span>
<span id="cb1-25">        samples.append(current_state)</span>
<span id="cb1-26"></span>
<span id="cb1-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.array(samples)</span>
<span id="cb1-28"></span>
<span id="cb1-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># サンプル数と初期条件を固定</span></span>
<span id="cb1-30">num_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb1-31">initial_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb1-32">initial_velocity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span></span>
<span id="cb1-33"></span>
<span id="cb1-34">samples_MD <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> md(num_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, initial_state, initial_velocity, timestep<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span></code></pre></div>
</details>
</div>
<div id="cell-fig-MD" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-md" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-md-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-md-output-1.png" width="310" height="263" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-md-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;3: 分子動力学法からのサンプル
</figcaption>
</figure>
</div>
</div>
</div>
<p>この方法は極めて収束が遅く，イテレーション数を <img src="https://latex.codecogs.com/png.latex?10%5E6"> 以上に取らないと目標分布 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-U%7D"> の良い近似とならないことを思い知りました（上図も <img src="https://latex.codecogs.com/png.latex?10%5E6"> サンプルで生成しています）．なお，以降の MCMC 法ではいずれもイテレーション数は一桁少ない <img src="https://latex.codecogs.com/png.latex?10%5E5"> としています．</p>
<div id="fc556262" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/cell-5-output-1.png" width="329" height="287" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>たしかに，目標分布 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-U%7D"> に収束しそうですね．</p>
</section>
<section id="sec-MH" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="sec-MH"><span class="header-section-number">2.2</span> Metropolis 法</h3>
<p>もちろん，分布 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-U%7D"> をサンプリングするために，必ずしも背景にある物理過程まで戻ってシミュレーションをする必要はありません．</p>
<p>そこで，シミュレーションは簡単なランダムウォークで行い，その結果を適切に修正することで目標分布に収束させる方法が <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> で考えられました．</p>
<p><span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> の手法は，現在では random walk Metropolis-Hastings 法と呼ばれます．</p>
<p>この背後の物理現象から離陸する一歩が，分子動力学法と MCMC 法とを分けるものでした．</p>
<div id="c0c7617a" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> metropolis(num_samples, initial_state, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb2-2">    samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [initial_state]</span>
<span id="cb2-3">    current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> initial_state</span>
<span id="cb2-4"></span>
<span id="cb2-5">    accept <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-6"></span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb2-8">        proposed_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.uniform(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-9">        acceptance_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pi(proposed_state) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pi(current_state)</span>
<span id="cb2-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> acceptance_ratio:</span>
<span id="cb2-11">            current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> proposed_state</span>
<span id="cb2-12">            accept.append(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-13">        samples.append(current_state)</span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> verbose:</span>
<span id="cb2-16">        rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(accept) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> num_samples</span>
<span id="cb2-17">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'acceptance rate : </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rate<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb2-18"></span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.array(samples)</span></code></pre></div>
</details>
</div>
<div id="cell-fig-MH" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div id="fig-mh" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mh-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-mh-output-1.png" width="310" height="266" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mh-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;4: Metropolis 法からのサンプル
</figcaption>
</figure>
</div>
</div>
</div>
<p>サンプル数は分子動力学法の <img src="https://latex.codecogs.com/png.latex?1/10"> であるにも拘らず，目標分布 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-U%7D"> の良い近似を得ています．</p>
<p>一般に，MCMC からのサンプルの質の良さは，<a href="https://ja.wikipedia.org/wiki/%E8%87%AA%E5%B7%B1%E7%9B%B8%E9%96%A2">自己相関関数</a> を見ることで評価できます．<sup>9</sup></p>
<p>Metropolis 法の自己相関関数を計算してみると，横軸の Lag が大きくなればなるほど Autocorrelation の値は小さくなっています．</p>
<div id="cell-fig-MH-auto" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-mh-auto" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mh-auto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-mh-auto-output-1.png" width="328" height="302" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mh-auto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;5: Metropolis 法の自己相関関数
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-1" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-1-output-1.png" width="324" height="302" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;6: Metropolis 法の軌跡
</figcaption>
</figure>
</div>
</div>
</div>
<p>上図は Metropolis 法で構成される Markov 連鎖の軌跡を表しています．行ったり来たりしているのがわかります．棄却率は５割弱です．<sup>10</sup></p>
</section>
<section id="統計学への応用" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="統計学への応用"><span class="header-section-number">2.3</span> 統計学への応用</h3>
<p>こうして MCMC が発明されれば，すぐにイノベーションとして理解されたかというとそうではありませんでした．</p>
<p>この Metropolis の手法が極めて賢いシミュレーション手法であることは一目瞭然でも，一般の確率分布からのサンプリングに使える汎用アルゴリズムになっているという抽象的な観点が得られるまでには時間を要しました．これを成し遂げたのが <span class="citation" data-cites="Hastings1970">(Hastings, 1970)</span> でした．<sup>11</sup></p>
<p>さらに，Hastings のこの結果も見過ごされたと言って良いでしょう．真にMCMC を統計学界隈に広め，現代におけるベイズ統計学の興隆の契機となったのは階層モデリングにおける Gibbs サンプリングの有用性を強調した <span class="citation" data-cites="Gelfand-Smith1990">(Gelfand &amp; Smith, 1990)</span> だと言われます．<sup>12</sup></p>
<p>当時，代替手法としては複雑な数値アルゴリズムしかなかったベイズ統計学において，MCMC は汎用的で実装も容易であることが周知され，ベイズ統計学が普及するきっかけとなりました．</p>
</section>
</section>
<section id="sec-LMH" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-LMH"><span class="header-section-number">3</span> 非対称化への試み</h2>
<section id="対称性という制約" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="対称性という制約"><span class="header-section-number">3.1</span> 対称性という制約</h3>
<p>ここでもう一度 Metropolis 法の軌跡 図&nbsp;6 を見てみましょう．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-1-output-1.png" class="img-fluid figure-img" style="width:3.19444in"></p>
<figcaption>図&nbsp;6 Metropolis 法の軌跡</figcaption>
</figure>
</div>
<p>最初の 50 サンプルしか表示していませんから，運が悪いとうまく見つからないかもしれませんが，「一度歩んだルートを，その後すぐに逆に戻ってしまう」という事象が発生しやすいことが観察できますでしょうか？</p>
<p>これを <strong>対称性</strong> (reversibility) または <strong>可逆性</strong> と言います．</p>
<p>Metropolis 法は構成上，この対称性を持つことが必要ですが，対称であるが故に一箇所に長時間とどまってしまうことが多くなります．</p>
<p>その結果，対象分布が複雑で多峰性を持つ場合は，もっといろんなモード（峰）からもサンプリングをしてほしいのに，長時間１つの峰から離れられずにいることがあります．</p>
<p>コーヒーに砂糖を溶かすことを考えてみましょう．砂糖の粒が拡散するのに任せておくと，最終的には均一に溶けるでしょうが，莫大な時間がかかります．スプーンで混ぜるなどして，砂糖が元の場所にとどまらずに移動し続けるようにすれば，はるかに速く平衡状態に到達できるでしょう．</p>
<p>これが <strong>非対称化</strong> のアイデアです．数ある Metropolis 法の改良の方向の中でも，この対称性を破るという試みは特に注目されてきました．</p>
</section>
<section id="リフティング" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="リフティング"><span class="header-section-number">3.2</span> リフティング</h3>
<p>Metropolis 法を非対称化するアプローチに，<strong>リフティング</strong> <span class="citation" data-cites="Chen+1999">(Chen et al., 1999)</span> と呼ばれる方法があります．</p>
<p>これは，元々の状態空間を２つの「モード」<img src="https://latex.codecogs.com/png.latex?+1"> と <img src="https://latex.codecogs.com/png.latex?-1"> に分裂させ，<img src="https://latex.codecogs.com/png.latex?+1"> のモードではひたすら右側に，<img src="https://latex.codecogs.com/png.latex?-1"> のモードではひたすら左側に移動するようする方法です．</p>
<p>２つのモード <img src="https://latex.codecogs.com/png.latex?+1,-1"> の間を遷移する確率を調整することで，最終的な不変分布は変わらないようにすることができます．</p>
<p>こうすることで，対称性を破り，一度「この方向に行く！」と決めたら行き続けるようにしながら，収束先は変わらないように変更することが出来るのです．</p>
<p>実際に Metropolis 法に適用した Lifted Metropolis-Hastings 法 <span class="citation" data-cites="Turitsyn+2011">(Turitsyn et al., 2011)</span> を実装してみましょう：</p>
<div id="e6bb3df8" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> lifted_metropolis(num_samples, initial_state, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb3-2">    samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [initial_state]</span>
<span id="cb3-3">    current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> initial_state</span>
<span id="cb3-4">    lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-5">    accept <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-6"></span>
<span id="cb3-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb3-8">        delta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.uniform(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-9">        proposed_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> delta</span>
<span id="cb3-10">        acceptance_ratio <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pi(proposed_state) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pi(current_state)</span>
<span id="cb3-11"></span>
<span id="cb3-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> acceptance_ratio:</span>
<span id="cb3-13">            current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> proposed_state</span>
<span id="cb3-14">            accept.append(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb3-16">            lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> lifting_variable</span>
<span id="cb3-17"></span>
<span id="cb3-18">        samples.append(current_state)</span>
<span id="cb3-19">    </span>
<span id="cb3-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> verbose:</span>
<span id="cb3-21">        rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(accept) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> num_samples</span>
<span id="cb3-22">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'acceptance rate : </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rate<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-23"></span>
<span id="cb3-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.array(samples)</span></code></pre></div>
</details>
</div>
<p>新しく追加されたリフティング変数 <img src="https://latex.codecogs.com/png.latex?%5Csigma%5Cin%5C%7B%5Cpm1%5C%7D"> に依存して，<img src="https://latex.codecogs.com/png.latex?%5Csigma=+1"> の場合には右方向に，<img src="https://latex.codecogs.com/png.latex?%5Csigma=-1"> の場合は左方向にしか提案を出さない MH 法と見れます．</p>
<div id="cell-fig-LMH" class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<div id="fig-lmh" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lmh-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-lmh-output-1.png" width="310" height="267" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lmh-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;7: 非対称 Metropolis 法からのサンプル
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-LMH-auto" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div id="fig-lmh-auto" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lmh-auto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-lmh-auto-output-1.png" width="340" height="302" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lmh-auto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;8: 非対称 Metropolis 法の自己相関関数
</figcaption>
</figure>
</div>
</div>
</div>
<p>自己相関関数を見ると，Metropolis 法よりも急速に減衰していることがわかります．むしろ，過減衰のように自己相関関数が負になっていることもあります．</p>
<p>これは，一度「この方向に行く！」と決めたら行き続けるように設計したために，正の値が出たしばらくあとは負の値が，負の値が出たしばらくあとは正の値が出やすいようになってしまっているためです．</p>
<p>したがってこれは１次元の分布を考えていることに起因するため，殊更問題とすべきではないでしょう．</p>
<div id="cell-fig-4" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div id="fig-4" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-4-output-1.png" width="324" height="302" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;9: 非対称 Metropolis 法の軌跡
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="リフティングの有用性" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="リフティングの有用性"><span class="header-section-number">3.3</span> リフティングの有用性</h3>
<p>今回のような単純なポテンシャル <img src="https://latex.codecogs.com/png.latex?U"> （図&nbsp;1） だけでなく，統計力学における磁性体のモデルである Curie-Weiss モデルのハミルトニアン</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH_n(x)=-%5Cfrac%7Bd%5Cbeta%7D%7B2n%7D%5Csum_%7Bi,j=1%7D%5Enx_ix_j-h%5Csum_%7Bi=1%7D%5Enx_i,%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ah%5Cin%5Cmathbb%7BR%7D,x%5Cin%5C%7B%5Cpm1%5C%7D%5En,%5Cquad%20n,d=1,2,%5Ccdots%0A"></p>
<p>が定める Boltzmann-Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-H%7D"> に対する Lifted Metropolis-Hastings も，単純な Metropolis-Hastings 法よりも効率的であることが知られています．<sup>13</sup></p>
<p>具体的には，モデルのパラメータ数 <img src="https://latex.codecogs.com/png.latex?n"> に対して，緩和時間を <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D"> のオーダーだけ改善することが，<span class="citation" data-cites="Turitsyn+2011">(Turitsyn et al., 2011)</span> では数値実験で，<span class="citation" data-cites="Bierkens-Roberts2017">(Bierkens &amp; Roberts, 2017)</span> では理論的に検証されているのです．</p>
<!--

### HMC

Hamiltonian Monte Carlo 法も一種のリフティングと見ることができ，

-->
</section>
</section>
<section id="sec-PDMP" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-PDMP"><span class="header-section-number">4</span> 新たな MCMC</h2>
<!--

こうして MCMC は物理学者から物質科学者，そして統計学者から機械学習家まで，多くの人が幅広く用いる手法になりました．

その結果，多くの同一の手法が違う名前で呼ばれていることも多く，現状の最先端ではどのようなことが起こっているのか見極めるのが困難です．

ここでは，上述のすべての分野に渡って共通して起こりつつある大きな地殻変動を紹介します．キーワードは **連続時間 MCMC** です．^[[@Fearnhead+2018-PDMC] から取った用語です．コンピュータシミュレーションである以上，結局は離散化するのですが，粒子の動きは（従来の Metropolis-Hastings 法のような）Markov 連鎖であるというより，連続時間確率過程のような動きをする手法群であることには間違いありません．]

-->
<section id="背後の物理現象からの更なる離陸" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="背後の物理現象からの更なる離陸"><span class="header-section-number">4.1</span> 背後の物理現象からの更なる離陸</h3>
<p>第 2 章で，分子動力学法（第 2.1 節）から，提案分布を背後の物理現象とは全く関係ないランダムウォークとすることで，Metropolis 法（第 2.2 節）は一気に効率的なサンプリング法となったことを見ました．</p>
<p>しかし，Metropolis 法はまだ思考が物理に引っ張られているのかも知れません．平衡統計力学において，ミクロの状態は等価で，ミクロなダイナミクスは可逆と考えられます．その前提が，知らず知らずのうちにまだ埋め込まれたままだと言えるでしょう．</p>
<p>そこで，スプーンでかき混ぜるように，遷移を非対称にすることで，より効率的なサンプリング法となることを前章 3 で見ました．</p>
<p>ここでは，さらに暗黙の思い込みから解き放たれようとします．それは，<strong>シミュレーションするにあたって，必ずしも離散時間ステップに囚われる必要はない</strong> ということです．</p>
<p>もう一度，Lifted Metropolis-Hastings 法の軌跡を見てみましょう：</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/docs/posts/2024/Computation/MCMC_files/figure-html/fig-4-output-1.png" class="img-fluid figure-img" style="width:3.19444in"></p>
<figcaption>図&nbsp;9 非対称 Metropolis 法の軌跡</figcaption>
</figure>
</div>
<p>この軌跡から得られる情報のほとんどは，「どこで折り返したか？」です．</p>
<p>ですから，この軌跡をシミュレーションするにあたって，一歩一歩採択-棄却のステップを繰り返す必要はなく，「どこで折り返すか？」を先に計算できてしまえば，あとは好きなステップサイズで切り出してサンプルとすれば良いのです．</p>
<p>実は，「折り返す地点だけを効率的に計算する」ことが可能であり，それが <strong>連続時間 MCMC</strong> のアイデアです．</p>
</section>
<section id="連続時間-mcmc" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="連続時間-mcmc"><span class="header-section-number">4.2</span> 連続時間 MCMC</h3>
<p>Lifted Metropolis-Hastings の適切な連続時間極限 <img src="https://latex.codecogs.com/png.latex?%5CDelta%20t%5Cto0"> を考えることで，「折り返す」という事象（が起こった回数）は Poisson 過程に従うことが導けます．</p>
<p>すると，「折り返す」事象が起こるまでの待ち時間 (interarrival time) は指数分布に従うことがわかります．これに基づいて，「折り返す」事象が起こる時刻を計算し，そこまでの軌跡を直線で補間すれば，Lifted Metropolis-Hastings 法（の連続時間極限）の軌跡が模倣できることになります．</p>
<p>最終的に得られる過程は，ランダムな時刻に「折り返す」事象が起こり，その間は確定的な動き（等速直線運動）をするというもので，このような過程を <strong>区分確定的 Markov 過程</strong> (PDMP: Piecewise Deterministic Markov Process) と呼びます．</p>
<p>このような PDMP は，Lifted Metropolis-Hastings 以外にも種々の MCMC 法の極限から見つかっており，その中でも特に有名なのが次の <strong>Zig-Zag sampler</strong> です：</p>
<div id="e92a1c43" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> math</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> zigzag(num_samples, initial_state, step<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb4-4">    samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [initial_state]</span>
<span id="cb4-5">    trajectory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [initial_state]</span>
<span id="cb4-6">    current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> initial_state</span>
<span id="cb4-7">    lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb4-8">    t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb4-9"></span>
<span id="cb4-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> num_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> step:</span>
<span id="cb4-11">        state_event <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.sqrt(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.sqrt( <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.log(np.random.rand()) ))</span>
<span id="cb4-12">        t_event <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(state_event <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_state)</span>
<span id="cb4-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> np.arange(np.ceil(t<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>step)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>step, np.ceil(t_event<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>step)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>step, step):</span>
<span id="cb4-14">              samples.append(current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> t))</span>
<span id="cb4-15">        current_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> state_event</span>
<span id="cb4-16">        trajectory.append(current_state)</span>
<span id="cb4-17">        lifting_variable <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> lifting_variable</span>
<span id="cb4-18">        t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t_event</span>
<span id="cb4-19"></span>
<span id="cb4-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.array(samples), np.array(trajectory)</span></code></pre></div>
</details>
</div>
<!--
自己相関関数こんなに spiky になるっけ？
いや，多分この設定だと zig-zag は普通に悪いんだろうな．
棄却率が高くて７割くらいある LMH に負けるのが「正しい」のかも知れない．

そして１次元なのに等間隔でサンプリングしているから，自己相関関数は sine curve に（むしろ）なるべきなのか！
-->
<div id="cell-fig-PDMP" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">samples_zigzag, trajectory_zigzag <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> zigzag(num_samples, initial_state, step<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb5-2">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb5-3">plt.hist(samples_zigzag, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>minty)</span>
<span id="cb5-4">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pdmp" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pdmp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-pdmp-output-1.png" width="310" height="263" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pdmp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;10: Zig-Zag サンプラーからのサンプル
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-PDMP-auto" class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<div id="fig-pdmp-auto" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pdmp-auto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-pdmp-auto-output-1.png" width="340" height="302" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pdmp-auto-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;11: Zig-Zag サンプラーの自己相関関数
</figcaption>
</figure>
</div>
</div>
</div>
<p>自己相関関数は，Lifted Metropolis-Hastings 法と同様に急激に下がって負の値に突き抜けたあとは，少し振動が残っているのがわかります．</p>
<p>全３サンプラーの比較は第 1.3 節をご覧ください．</p>
<p>次の軌跡を見て分かる通り，モードである <img src="https://latex.codecogs.com/png.latex?x=0"> を中心に激しく往復するので，直後のサンプルとは負の相関が出やすいようです．</p>
<div id="cell-fig-6" class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<div id="fig-6" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/fig-6-output-1.png" width="328" height="302" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;12: Zig-Zag サンプラーの軌跡
</figcaption>
</figure>
</div>
</div>
</div>
<p>連続時間極限 <img src="https://latex.codecogs.com/png.latex?%5CDelta%20t%5Cto0"> をとっているということは，「極めて小さいステップサイズでの random walk Metropolis 法（第 2.2 節）」に相当します．従って，一度折り返したら，原点 <img src="https://latex.codecogs.com/png.latex?x=0"> を超えるまでは絶対に棄却されません．</p>
<p>そのため，このように往復するような軌跡が得られます．</p>
</section>
<section id="sec-PDMP-advantages" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="sec-PDMP-advantages"><span class="header-section-number">4.3</span> 連続時間 MCMC の美点</h3>
<p>前節では，必ずしも PDMP 法である Zig-Zag sampler が，Lifted Metropolis-Hastings 法より，自己相関関数の観点で良いとは言い切れないことを見ました．</p>
<p>しかし，今回の設定は１次元という特殊な条件下であることを考慮に入れる必要があります．</p>
<p>１次元なので Zig-Zag サンプラーは行き来することしか出来ていませんが，<sup>14</sup> ２次元以上，特に高次元の場合は，Zig-Zag サンプラーは極めて効率的に状態空間を探索できることが期待されます．</p>
<p>例えば，標準正規分布に対する２次元での軌跡は次の通りです：</p>
<div id="fig-5" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://162348.github.io/posts/2024/Computation/Files/ZigZag_2d.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;13: Zig-Zag Sampler in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2">
</figcaption>
</figure>
</div>
<p>そして何より，軌道が効率的な空間の探索に有利であるだけでなく，正確なサブサンプリングを取り入れることが可能です．すなわち，ほとんどの他手法と違って，<strong>バイアスを導入することなく</strong>，データの一部のみを用いてアルゴリズムを走らせることができます．</p>
<p>したがって，従来の MCMC 法が採択-棄却のステップにおいて尤度を評価する必要があり，データサイズ <img src="https://latex.codecogs.com/png.latex?n"> に対して <img src="https://latex.codecogs.com/png.latex?O(n)"> の計算量を要するのに対して，<img src="https://latex.codecogs.com/png.latex?O(n)"> に比例する焼き入れのステップを除けば，<img src="https://latex.codecogs.com/png.latex?O(1)"> の複雑でほとんど i.i.d. なサンプルを得ることができます <span class="citation" data-cites="Bierkens+2019">(Bierkens et al., 2019)</span>．</p>
<!-- 

### 筆者の目標：新時代のサンプラーの開発

情報通信機器の発達によりデータが複雑で大規模化する現代では，モデルも同様に大規模で複雑化していく必要があります．OpenAI の ChatGPT や Sora，Anthropic の Claude-3 などの **基盤モデル** はその象徴と言えるでしょう．

筆者は，その中で **新時代の MCMC** の開発を目標としています．

高次元空間上の複雑な分布からも効率的にサンプリングできる MCMC 手法が開発された際には，多くの人が自分のノートパソコンで気軽にできるベイズ統計分析の幅が大きく広がることでしょう．

それこそ，ニューラルネットワークの表現力をフルに活用するだけでなく，ベイズ手法の強みも併せて，小規模データでも鮮やかな分析が簡単に出来るようになるかもしれません．

そのような世界線こそ，AI 技術の民主化と呼ぶにふさわしい，来るべき未来だと筆者は信じています．

また，基盤モデルの Bayes 的な理解を進めることも，実は壮大ながらも，筆者の最終的な目標の一つであります．

-->
</section>
</section>




<div id="quarto-appendix" class="default"><section id="終わりに" class="level2 appendix" data-number="5"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">5</span> 終わりに</h2><div class="quarto-appendix-contents">

<p>こうみると，MCMC は物理学の問題意識から生まれた手法でありながら，背後の物理現象を模倣することから離れていくことで，計算手法としての効率を高めていく一途を辿っていることがわかります．</p>
<p>そう見ると，新時代の大規模データと大規模モデルが課す MCMC の次なる脱皮は，連続時間 MCMC で間違いないような気がしてくるのですが……．まだ筆者にははっきりとは見えてきません．</p>
<p>また本稿では１つの流れしか取り上げておらず，例えば HMC とその非対称化がどのような位置づけにあるかもまだ考慮中です．</p>
<p>統計力学，統計学，機械学習が交差するなんとも面白いテーマです．</p>


</div></section><section id="執筆のきっかけ" class="level3 appendix" data-number="5.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">5.1</span> 執筆のきっかけ</h2><div class="quarto-appendix-contents">

<p>本ポスター，そして本解説記事の執筆のきっかけは，<a href="../../../posts/2024/Particles/PF.html">MLSS でのポスター発表</a> で連続時間 MCMC のことが機械学習の界隈では全く知られていないことを知ったことと，情報統計力学の研究集会に出席し，連続時間 MCMC が物理学の方からも研究されていることを知ったことでした．</p>
<p>統計界隈では（本稿で解説しました通り） PDMP や連続時間 MCMC と呼ばれる手法は，物理学界隈では event-based simulation，rejection-free と呼ばれる手法群の１つとして活発に研究されていました．</p>
<p>全く同じ問題を解こうとしているのに，用語法が全く異なることに驚かされました．</p>
<p>２つの分野の相互理解と知見の交換が進むことを目指し，これからも研究していきたいと考えます．</p>
</div></section><section id="アルゴリズムのステップサイズについて" class="level3 appendix" data-number="5.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">5.2</span> アルゴリズムのステップサイズについて</h2><div class="quarto-appendix-contents">

<p>コードからわかります通り，提案分布のスケールは次のようになっています：</p>
<div class="table-responsive-sm">
<table class="table-striped table-hover table">
<caption>３手法のスケーリング</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">手法</th>
<th style="text-align: center;">提案分布</th>
<th style="text-align: center;">平均移動距離<sup>15</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">MH</td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BU%7D(%5B-2,2%5D)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?1"></td>
</tr>
<tr class="even">
<td style="text-align: center;">LMH</td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BU%7D(%5B0,2%5D)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?1"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Zig-Zag</td>
<td style="text-align: center;">なし</td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?2"></td>
</tr>
</tbody>
</table>
</div>
<p>ですので，平均した隣接（提案）サンプル間距離について，Zig-Zag サンプラーはズルをしているともいえます．</p>
<p>しかし，必ずしもアンフェアな比較をしていたわけではありません．</p>
<p>もし，３手法で計算量を揃えるならば，Zig-Zag サンプラーにとっての１回のループは方向転換をするまでであり，２つの方向転換の間の平均距離はだいたい <img src="https://latex.codecogs.com/png.latex?1.93"> になります．</p>
<div id="3042cdc3" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">diffs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.diff(trajectory_zigzag)</span>
<span id="cb6-2">abs_diffs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(diffs)</span>
<span id="cb6-3">mean_abs_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(abs_diffs)</span>
<span id="cb6-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(mean_abs_diff)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1.9232887144203465</code></pre>
</div>
</div>
<p>MH, LMH については，ほとんど最適なスケーリングになるように調節してありますが，Zig-Zag サンプラーにおいては最適なスケーリングという概念は存在しません．あらかじめ 図&nbsp;12 の軌跡があり，どれくらい距離を空けてサンプルとするか，という問題しかありません．</p>
<p>ここでは，アルゴリズムのループ１回が１回のサンプリングになるように，３つの手法を揃えて比較の根拠としました．</p>
<p>なお，このように，Zig-Zag サンプラーなどの連続時間 MCMC と，従来の離散時間 MCMC の直接の性能比較は，微妙な問題が多く，筆者もまだ十分に説明できる準備はありません．</p>
<p>しかし，次年度以降のオープンハウスで，より詳細な解説を行う予定です．</p>
<div id="940996f5" class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Computation/MCMC_files/figure-html/cell-19-output-1.png" width="574" height="278" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Zig-Zag サンプラーのサンプリングステップを <img src="https://latex.codecogs.com/png.latex?1"> や <img src="https://latex.codecogs.com/png.latex?1.5"> にすると，たしかに自己相関は悪化します．</p>
</div></section><section id="参考文献" class="level2 appendix" data-number="6"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">6</span> 参考文献</h2><div class="quarto-appendix-contents">

<p>本稿では，用いたコードの導出を一切触れませんでした．これについては，文献 <span class="citation" data-cites="Tartero-Krauth2023">(Tartero &amp; Krauth, 2023)</span> をご参照ください．非調和振動子を系にとり，正準集団とみなすことで，分子動力学法，メトロポリス法からそのリフティングまで，種々のサンプラーを同じ題材で比較するアイデアをもらいました．こんなにわかりやすい解析ができるのかと心底驚きました．</p>
<p>続いて，Metropolis-Hastings 法 → 非対称 MCMC → 連続時間 MCMC という発展の過程を，背後の物理過程の模倣からの離陸という視点で統一的に捉えることが出来るということは，<span class="citation" data-cites="Turitsyn+2011">(Turitsyn et al., 2011)</span> の魅力的なイントロダクションで気付かされました．本文献はリフティングによる MCMC の非可逆化を抽象的に定式化して数値実験で検証したものであり，「ねじれ詳細釣り合い条件」を導入した点で，アイデアの宝庫といえます．</p>
<p>リフティングによる Metropolis 法の非対称化について，<span class="citation" data-cites="酒井佑士2017">(酒井佑士, 2017)</span> は貴重な日本語文献です．当該文献の第３章（の第２節）にここで紹介した内容が詳しくまとめられています．</p>
<p>第 4.3 節で紹介しましたように，Zig-Zag サンプラーを用いたサンプリングではそのスケーラビリティが魅力です．この点については，Zig-Zag サンプラーが提案された論文 <span class="citation" data-cites="Bierkens+2019">(Bierkens et al., 2019)</span> でも，前面に押し出して解説されています．</p>



</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Bierkens+2019" class="csl-entry">
Bierkens, J., Fearnhead, P., &amp; Roberts, G. (2019). <span class="nocase">The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data</span>. <em>The Annals of Statistics</em>, <em>47</em>(3), 1288–1320. <a href="https://doi.org/10.1214/18-AOS1715">https://doi.org/10.1214/18-AOS1715</a>
</div>
<div id="ref-Bierkens-Roberts2017" class="csl-entry">
Bierkens, J., &amp; Roberts, G. (2017). <span class="nocase">A Piecewise Deterministic Scaling Limit of Lifted Metropolis-Hastings in the Curie-Weiss Model</span>. <em>The Annals of Applied Probability</em>, <em>27</em>(2), 846–882. <a href="http://www.jstor.org/stable/44249153">http://www.jstor.org/stable/44249153</a>
</div>
<div id="ref-Chen+1999" class="csl-entry">
Chen, F., Lovász, L., &amp; Pak, I. (1999). Lifting markov chains to speed up mixing. <em>Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing</em>, 275–281. <a href="https://doi.org/10.1145/301250.301315">https://doi.org/10.1145/301250.301315</a>
</div>
<div id="ref-Duane+1987" class="csl-entry">
Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). Hybrid monte carlo. <em>Physics Letters B</em>, <em>195</em>(2), 216–222. <a href="https://doi.org/10.1016/0370-2693(87)91197-X">https://doi.org/10.1016/0370-2693(87)91197-X</a>
</div>
<div id="ref-Gelfand-Smith1990" class="csl-entry">
Gelfand, A. E., &amp; Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal densities. <em>Journal of the American Statistical Association</em>, <em>85</em>(410), 398–409. <a href="https://doi.org/10.2307/2289776">https://doi.org/10.2307/2289776</a>
</div>
<div id="ref-Gelman+1996" class="csl-entry">
Gelman, A., Roberts, G. O., &amp; Gilks, W. R. (1996). <span>Efficient Metropolis Jumping Rules</span>. In <em><span class="nocase">Bayesian Statistics 5: Proceedings of the Fifth Valencia International Meeting</span></em>. Oxford University Press. <a href="https://doi.org/10.1093/oso/9780198523567.003.0038">https://doi.org/10.1093/oso/9780198523567.003.0038</a>
</div>
<div id="ref-Hastings1970" class="csl-entry">
Hastings, W. K. (1970). Monte carlo sampling methods using markov chains and their applications. <em>Biometrika</em>, <em>57</em>(1), 97–109. <a href="https://www.jstor.org/stable/2334940">https://www.jstor.org/stable/2334940</a>
</div>
<div id="ref-Martin+2023-history" class="csl-entry">
Martin, G. M., Fraizier, D. T., &amp; Robert, C. P. (2023). Computing bayes: From then ‘til now. <em>Statistical Science</em>, <em>Advanced Publication</em>, 1–17.
</div>
<div id="ref-Metropolis+1953" class="csl-entry">
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a>
</div>
<div id="ref-Neal1994" class="csl-entry">
Neal, R. M. (1994). An improved acceptance procedure for the hybrid monte carlo algorithm. <em>Journal of Computational Physics</em>, <em>111</em>(1), 194–203. <a href="https://doi.org/10.1006/jcph.1994.1054">https://doi.org/10.1006/jcph.1994.1054</a>
</div>
<div id="ref-Neal2011-HMC" class="csl-entry">
Neal, R. M. (2011). (S. Brooks, A. Gelman, G. Jones, &amp; X.-L. Meng, Eds.; pp. 113–162). Chapman; Hall/CRC. <a href="https://www.taylorfrancis.com/chapters/edit/10.1201/b10905-6/mcmc-using-hamiltonian-dynamics-radford-neal">https://www.taylorfrancis.com/chapters/edit/10.1201/b10905-6/mcmc-using-hamiltonian-dynamics-radford-neal</a>
</div>
<div id="ref-Peters-deWith2012" class="csl-entry">
Peters, E. A. J. F., &amp; de&nbsp;With, G. (2012). Rejection-free monte carlo sampling for general potentials. <em>Physical Review E</em>, <em>85</em>(2). <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026703">https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026703</a>
</div>
<div id="ref-Robert-Casella2011" class="csl-entry">
Robert, C., &amp; Casella, G. (2011). A short history of markov chain monte carlo: Subjective recollections from incomplete data. <em>Statistical Science</em>, <em>26</em>(1), 102–115. <a href="http://www.jstor.org/stable/23059158">http://www.jstor.org/stable/23059158</a>
</div>
<div id="ref-Roberts+1997" class="csl-entry">
Roberts, G. O., Gelman, A., &amp; Gilks, W. R. (1997). <span class="nocase">Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms</span>. <em>The Annals of Applied Probability</em>, <em>7</em>(1), 110–120. <a href="http://www.jstor.org/stable/2245134">http://www.jstor.org/stable/2245134</a>
</div>
<div id="ref-Tartero-Krauth2023" class="csl-entry">
Tartero, G., &amp; Krauth, W. (2023). <em>Concepts in monte carlo sampling</em>. <a href="https://arxiv.org/abs/2309.03136">https://arxiv.org/abs/2309.03136</a>
</div>
<div id="ref-Turitsyn+2011" class="csl-entry">
Turitsyn, K. S., Chertkov, M., &amp; Vucelja, M. (2011). <span class="nocase">Irreversible Monte Carlo algorithms for Efficient Sampling</span>. <em>Physica D-Nonlinear Phenomena</em>, <em>240</em>(5-Apr), 410–414. <a href="https://doi.org/10.1016/j.physd.2010.10.003">https://doi.org/10.1016/j.physd.2010.10.003</a>
</div>
<div id="ref-Vasdekis-Roberts2022" class="csl-entry">
Vasdekis, G., &amp; Roberts, G. O. (2022). A note on the polynomial ergodicity of the one-dimensional zig-zag process. <em>Journal of Applied Probability</em>, <em>59</em>(3), 895–903. <a href="https://doi.org/10.1017/jpr.2021.97">https://doi.org/10.1017/jpr.2021.97</a>
</div>
<div id="ref-酒井佑士2017" class="csl-entry">
酒井佑士. (2017). <em>マルコフ連鎖モンテカルロ法における詳細つり合い条件の破れの効果と応用</em> [PhD thesis, 東京大学]. <a href="https://repository.dl.itc.u-tokyo.ac.jp/records/50422">https://repository.dl.itc.u-tokyo.ac.jp/records/50422</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Hamiltonian Monte Carlo の名称は <span class="citation" data-cites="Neal2011-HMC">(Neal, 2011)</span> からで，元々は Hybrid Monte Carlo と呼ばれていました．分子動力学法 (Molecular Dynamics) と MCMC のハイブリッド，という意味でした．Stan で実装されている MCMC アルゴリズムについては <a href="https://mc-stan.org/docs/reference-manual/mcmc.html">こちら</a> を参照．↩︎</p></li>
<li id="fn2"><p>特に収束が遅く，他の手法と比べてイテレーション数を 10　倍にしています．↩︎</p></li>
<li id="fn3"><p>統計学界隈では <span class="citation" data-cites="Hastings1970">(Hastings, 1970)</span> を入れて，Metropolis-Hastings 法とも呼ばれる．↩︎</p></li>
<li id="fn4"><p>ただし，配置 <img src="https://latex.codecogs.com/png.latex?%5Comega%5Cin%5COmega"> は空間内にランダム（一様）に粒子 <img src="https://latex.codecogs.com/png.latex?N"> 個を配置することで生成することとする．↩︎</p></li>
<li id="fn5"><p>１粒子系なので相互作用はなく，<img src="https://latex.codecogs.com/png.latex?E=U">．↩︎</p></li>
<li id="fn6"><p><img src="https://latex.codecogs.com/png.latex?%5Cbeta=1"> と約束することは，系の温度を <img src="https://latex.codecogs.com/png.latex?T=k_B%5E%7B-1%7D"> に固定することにあたります．↩︎</p></li>
<li id="fn7"><p>図 1 で定めたポテンシャルを持つ力学系には，代表的なものは（非調和）振動子や，あるいは <img src="https://latex.codecogs.com/png.latex?U"> の形をした谷を行ったり来たりするボールを考えても構いません．↩︎</p></li>
<li id="fn8"><p>詳しい議論は <span class="citation" data-cites="Tartero-Krauth2023">(Tartero &amp; Krauth, 2023)</span> をご参照ください．大変教育的な入門です．↩︎</p></li>
<li id="fn9"><p>自己相関関数が大きいほど，その Markov 連鎖を用いて構成した Monte Carlo 推定量の漸近分散が大きくなります．加えて，自己相関関数の裾が重すぎると，例えエルゴード性を持っており大数の法則が成り立とうとも，中心極限定理が成り立たなくなります．換言すれば，<img src="https://latex.codecogs.com/png.latex?n%5E%7B-1/2%7D"> よりも遅い収束レートになってしまいます．↩︎</p></li>
<li id="fn10"><p>一般には，ランダムウォーク MH 法において，採択率を <img src="https://latex.codecogs.com/png.latex?0.2%5Cle%5Calpha%5Cle0.4"> 前後に抑えるのが良いとされています <span class="citation" data-cites="Roberts+1997">(Roberts et al., 1997)</span>．これは状態空間の次元が無限に漸近する設定下での，漸近論的な結果ですが，低次元の場合でも極めて良い指標になることが <span class="citation" data-cites="Gelman+1996">(Gelman et al., 1996)</span> で実証されています．また今回も，１次元であるにも拘らず，たしかに棄却率が半分を越さないほうが，自己相関が小さくなる傾向が確認されました．しかし今回は対象分布の裾が極めて軽いので，あまり大きなムーブは要らず，ステップサイズの最大値を <img src="https://latex.codecogs.com/png.latex?2">，採択率は <img src="https://latex.codecogs.com/png.latex?0.5"> 近くにしました．他の手法，LMH と Zig-Zag もステップサイズの最大値が <img src="https://latex.codecogs.com/png.latex?2"> になるように統一しました．↩︎</p></li>
<li id="fn11"><p>“While <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> proposed the use of MCMC sampling to compute particular integrals in statistical mechanics, <strong>it was the Hastings paper that elevated the concept to a general one, and introduced it to the broader statistics community</strong>.” <span class="citation" data-cites="Martin+2023-history">(Martin et al., 2023, p. 7)</span> 3.5節．↩︎</p></li>
<li id="fn12"><p><span class="citation" data-cites="Martin+2023-history">(Martin et al., 2023, p. 8)</span> 4節，<span class="citation" data-cites="Robert-Casella2011">(Robert &amp; Casella, 2011, p. 102)</span> など．↩︎</p></li>
<li id="fn13"><p>ただし，<img src="https://latex.codecogs.com/png.latex?e%5E%7B-H%7D"> が多峰性を示す低温領域では，LMH の方が効率的であるというはっきりとした理論保証はまだありません．↩︎</p></li>
<li id="fn14"><p>今回は対象分布の減衰が極めて激しかったために差が現れにくかったのだと考えられます．Zig-Zag サンプラーは１次元でも，（広い設定の下で）（そして特に目標分布の裾が重いときに）ランダムウォーク・メトロポリス法や Metripolis-adjusted Langevin algorithm よりも速い収束レートを持ちます <span class="citation" data-cites="Vasdekis-Roberts2022">(Vasdekis &amp; Roberts, 2022)</span>，↩︎</p></li>
<li id="fn15"><p>提案分布の．実際の軌跡の１ステップでの移動距離とは違う．↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-copyright"><h2 class="anchored quarto-appendix-heading">Copyright</h2><div class="quarto-appendix-contents"><div>Copyright Hirofumi Shiba 2024. All Rights Reserved</div></div></section></div> ]]></description>
  <category>MCMC</category>
  <category>Simulation</category>
  <category>Poster</category>
  <guid>https://162348.github.io/posts/2024/Computation/MCMC.html</guid>
  <pubDate>Thu, 23 May 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/static/Posters/ISM-OH2024.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>R によるベイズ混合モデリング入門</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Computation/brms.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<div class="callout callout-style-simple callout-tip callout-titled" title="brms: Bayesian Regression Models using 'Stan' リンク集">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
brms: Bayesian Regression Models using ‘Stan’ リンク集
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><a href="https://cran.r-project.org/web/packages/brms/">r-project</a></li>
<li><a href="https://paul-buerkner.github.io/brms/">Documentation</a></li>
<li><a href="https://github.com/paul-buerkner/brms">GitHub</a></li>
<li><a href="https://discourse.mc-stan.org/">discourse</a></li>
<li><span class="citation" data-cites="Burkner2017">(Bürkner, 2017)</span></li>
<li><span class="citation" data-cites="Burkner2018">(Bürkner, 2018)</span></li>
<li><span class="citation" data-cites="Burkner2021">(Bürkner, 2021)</span></li>
</ul>
</div>
</div>
<p>ダウンロードは：<sup>1</sup></p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">install.packages</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"brms"</span>)</span></code></pre></div>
<section id="sec-example" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-example"><span class="header-section-number">1</span> 例：カウントデータのモデリング</h2>
<p>Documentation で紹介されている，<a href="https://search.r-project.org/CRAN/refmans/dhglm/html/epilepsy.html"><code>Epilepsy Seizures Data</code></a> <span class="citation" data-cites="Leppik+1987">(Leppik et al., 1987)</span>，<span class="citation" data-cites="Thall-Vail1990">(Thall &amp; Vail, 1990)</span> を用いた <a href="https://paul-buerkner.github.io/brms/">例</a> を実行してみる：</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(brms)</span>
<span id="cb2-2">fit1 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(count <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> zAge <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> zBase <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Trt <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>patient),</span>
<span id="cb2-3">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> epilepsy, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">poisson</span>())</span></code></pre></div>
<p>てんかん (epilepsy) 患者の発作回数<code>count</code>を被説明変数とし，処置の効果を表す説明変数<code>Trt</code>と患者毎のランダムな切片項<code>(1|patient)</code>と<code>zAge</code>,<code>zBase</code>への依存構造を調べたい．被説明変数<code>count</code>のモデルとしては，Poisson 分布族を用いる．</p>
<div class="callout callout-style-simple callout-note callout-titled" title="説明変数">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
説明変数
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><code>zAge</code>：標準化された年齢</li>
<li><code>zBase</code>：ベースの発作回数</li>
<li><code>Trt</code>：治療の有無を表す２値変数</li>
<li><code>(1|patient)</code>：患者ごとに異なるとした切片項</li>
</ul>
<p><code>zBase * Trt</code>という記法は，この２つの交互作用もモデルに含めることを意味する．</p>
<p>また，59 人の患者に関して，４回の入院時の発作回数を記録した，全 236 データからなる．<code>patient</code>が患者を識別する ID であり，<code>(1|patient)</code>は患者ごとのランダム効果ということになる．</p>
<p>従って本モデルは<code>zAge</code>, <code>zBase</code>, <code>Trt</code>, <code>Trt*zBase</code>という固定効果，<code>(1|patient)</code>というランダム効果を取り入れた混合効果モデルということになり，回帰式は次の通りである： <img src="https://latex.codecogs.com/png.latex?%0Ay_%7Bit%7D%20=%20%5Cbeta_1%20%5Ccdot%5Ctexttt%7BzAge%7D_i+%20%5Cbeta_2%20%5Ccdot%20%5Ctexttt%7BzBase%7D_i%20+%20%5Cbeta_3%20%5Ccdot%20%5Ctexttt%7BTrt%7D_i%0A"> <img src="https://latex.codecogs.com/png.latex?%0A+%20%5Cbeta_4%20%5Ccdot%20(%5Ctexttt%7BzBase%7D_i%20%5Ccdot%20%5Ctexttt%7BTrt%7D_i)%20+%20%5Calpha_i%20+%5Cepsilon_%7Bit%7D.%0A"> ただし，<img src="https://latex.codecogs.com/png.latex?%5Ctexttt%7Bcount%7D_%7Bit%7D"> の Poisson 母数を <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bit%7D"> として，<img src="https://latex.codecogs.com/png.latex?y_%7Bit%7D:=%5Clog(%5Clambda_%7Bit%7D)"> とした．</p>
<p>ベースの発作回数が高いほど，治療効果が高い／低いのではないか？という仮説を検証する，<code>zBase*Trt</code>を曝露因子としたモデルである．</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled" title="全データ">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
全データ
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">epilepsy</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Age Base Trt patient visit count obs        zAge        zBase
1    31   11   0       1     1     5   1  0.42499501 -0.757172825
2    30   11   0       2     1     3   2  0.26528351 -0.757172825
3    25    6   0       3     1     2   3 -0.53327400 -0.944403322
4    36    8   0       4     1     4   4  1.22355252 -0.869511123
5    22   66   0       5     1     7   5 -1.01240850  1.302362646
6    29   27   0       6     1     5   6  0.10557201 -0.158035233
7    31   12   0       7     1     6   7  0.42499501 -0.719726725
8    42   52   0       8     1    40   8  2.18182153  0.778117253
9    37   23   0       9     1     5   9  1.38326402 -0.307819631
10   28   10   0      10     1    14  10 -0.05413949 -0.794618924
11   36   52   0      11     1    26  11  1.22355252  0.778117253
12   24   33   0      12     1    12  12 -0.69298550  0.066641363
13   23   18   0      13     1     4  13 -0.85269700 -0.495050128
14   36   42   0      14     1     7  14  1.22355252  0.403656259
15   26   87   0      15     1    16  15 -0.37356249  2.088730734
16   26   50   0      16     1    11  16 -0.37356249  0.703225054
17   28   18   0      17     1     0  17 -0.05413949 -0.495050128
18   31  111   0      18     1    37  18  0.42499501  2.987437121
19   32   18   0      19     1     3  19  0.58470651 -0.495050128
20   21   20   0      20     1     3  20 -1.17212000 -0.420157930
21   29   12   0      21     1     3  21  0.10557201 -0.719726725
22   21    9   0      22     1     3  22 -1.17212000 -0.832065024
23   32   17   0      23     1     2  23  0.58470651 -0.532496228
24   25   28   0      24     1     8  24 -0.53327400 -0.120589134
25   30   55   0      25     1    18  25  0.26528351  0.890455552
26   40    9   0      26     1     2  26  1.86239852 -0.832065024
27   19   10   0      27     1     3  27 -1.49154300 -0.794618924
28   22   47   0      28     1    13  28 -1.01240850  0.590886756
29   18   76   1      29     1    11  29 -1.65125450  1.676823640
30   32   38   1      30     1     8  30  0.58470651  0.253871861
31   20   19   1      31     1     0  31 -1.33183150 -0.457604029
32   30   10   1      32     1     3  32  0.26528351 -0.794618924
33   18   19   1      33     1     2  33 -1.65125450 -0.457604029
34   24   24   1      34     1     4  34 -0.69298550 -0.270373532
35   30   31   1      35     1    22  35  0.26528351 -0.008250835
36   35   14   1      36     1     5  36  1.06384102 -0.644834526
37   27   11   1      37     1     2  37 -0.21385099 -0.757172825
38   20   67   1      38     1     3  38 -1.33183150  1.339808745
39   22   41   1      39     1     4  39 -1.01240850  0.366210159
40   28    7   1      40     1     2  40 -0.05413949 -0.906957223
41   23   22   1      41     1     0  41 -0.85269700 -0.345265731
42   40   13   1      42     1     5  42  1.86239852 -0.682280626
43   33   46   1      43     1    11  43  0.74441801  0.553440656
44   21   36   1      44     1    10  44 -1.17212000  0.178979662
45   35   38   1      45     1    19  45  1.06384102  0.253871861
46   25    7   1      46     1     1  46 -0.53327400 -0.906957223
47   26   36   1      47     1     6  47 -0.37356249  0.178979662
48   25   11   1      48     1     2  48 -0.53327400 -0.757172825
49   22  151   1      49     1   102  49 -1.01240850  4.485281100
50   32   22   1      50     1     4  50  0.58470651 -0.345265731
51   25   41   1      51     1     8  51 -0.53327400  0.366210159
52   35   32   1      52     1     1  52  1.06384102  0.029195264
53   21   56   1      53     1    18  53 -1.17212000  0.927901651
54   41   24   1      54     1     6  54  2.02211002 -0.270373532
55   32   16   1      55     1     3  55  0.58470651 -0.569942327
56   26   22   1      56     1     1  56 -0.37356249 -0.345265731
57   21   25   1      57     1     2  57 -1.17212000 -0.232927432
58   36   13   1      58     1     0  58  1.22355252 -0.682280626
59   37   12   1      59     1     1  59  1.38326402 -0.719726725
60   31   11   0       1     2     3  60  0.42499501 -0.757172825
61   30   11   0       2     2     5  61  0.26528351 -0.757172825
62   25    6   0       3     2     4  62 -0.53327400 -0.944403322
63   36    8   0       4     2     4  63  1.22355252 -0.869511123
64   22   66   0       5     2    18  64 -1.01240850  1.302362646
65   29   27   0       6     2     2  65  0.10557201 -0.158035233
66   31   12   0       7     2     4  66  0.42499501 -0.719726725
67   42   52   0       8     2    20  67  2.18182153  0.778117253
68   37   23   0       9     2     6  68  1.38326402 -0.307819631
69   28   10   0      10     2    13  69 -0.05413949 -0.794618924
70   36   52   0      11     2    12  70  1.22355252  0.778117253
71   24   33   0      12     2     6  71 -0.69298550  0.066641363
72   23   18   0      13     2     4  72 -0.85269700 -0.495050128
73   36   42   0      14     2     9  73  1.22355252  0.403656259
74   26   87   0      15     2    24  74 -0.37356249  2.088730734
75   26   50   0      16     2     0  75 -0.37356249  0.703225054
76   28   18   0      17     2     0  76 -0.05413949 -0.495050128
77   31  111   0      18     2    29  77  0.42499501  2.987437121
78   32   18   0      19     2     5  78  0.58470651 -0.495050128
79   21   20   0      20     2     0  79 -1.17212000 -0.420157930
80   29   12   0      21     2     4  80  0.10557201 -0.719726725
81   21    9   0      22     2     4  81 -1.17212000 -0.832065024
82   32   17   0      23     2     3  82  0.58470651 -0.532496228
83   25   28   0      24     2    12  83 -0.53327400 -0.120589134
84   30   55   0      25     2    24  84  0.26528351  0.890455552
85   40    9   0      26     2     1  85  1.86239852 -0.832065024
86   19   10   0      27     2     1  86 -1.49154300 -0.794618924
87   22   47   0      28     2    15  87 -1.01240850  0.590886756
88   18   76   1      29     2    14  88 -1.65125450  1.676823640
89   32   38   1      30     2     7  89  0.58470651  0.253871861
90   20   19   1      31     2     4  90 -1.33183150 -0.457604029
91   30   10   1      32     2     6  91  0.26528351 -0.794618924
92   18   19   1      33     2     6  92 -1.65125450 -0.457604029
93   24   24   1      34     2     3  93 -0.69298550 -0.270373532
94   30   31   1      35     2    17  94  0.26528351 -0.008250835
95   35   14   1      36     2     4  95  1.06384102 -0.644834526
96   27   11   1      37     2     4  96 -0.21385099 -0.757172825
97   20   67   1      38     2     7  97 -1.33183150  1.339808745
98   22   41   1      39     2    18  98 -1.01240850  0.366210159
99   28    7   1      40     2     1  99 -0.05413949 -0.906957223
100  23   22   1      41     2     2 100 -0.85269700 -0.345265731
101  40   13   1      42     2     4 101  1.86239852 -0.682280626
102  33   46   1      43     2    14 102  0.74441801  0.553440656
103  21   36   1      44     2     5 103 -1.17212000  0.178979662
104  35   38   1      45     2     7 104  1.06384102  0.253871861
105  25    7   1      46     2     1 105 -0.53327400 -0.906957223
106  26   36   1      47     2    10 106 -0.37356249  0.178979662
107  25   11   1      48     2     1 107 -0.53327400 -0.757172825
108  22  151   1      49     2    65 108 -1.01240850  4.485281100
109  32   22   1      50     2     3 109  0.58470651 -0.345265731
110  25   41   1      51     2     6 110 -0.53327400  0.366210159
111  35   32   1      52     2     3 111  1.06384102  0.029195264
112  21   56   1      53     2    11 112 -1.17212000  0.927901651
113  41   24   1      54     2     3 113  2.02211002 -0.270373532
114  32   16   1      55     2     5 114  0.58470651 -0.569942327
115  26   22   1      56     2    23 115 -0.37356249 -0.345265731
116  21   25   1      57     2     3 116 -1.17212000 -0.232927432
117  36   13   1      58     2     0 117  1.22355252 -0.682280626
118  37   12   1      59     2     4 118  1.38326402 -0.719726725
119  31   11   0       1     3     3 119  0.42499501 -0.757172825
120  30   11   0       2     3     3 120  0.26528351 -0.757172825
121  25    6   0       3     3     0 121 -0.53327400 -0.944403322
122  36    8   0       4     3     1 122  1.22355252 -0.869511123
123  22   66   0       5     3     9 123 -1.01240850  1.302362646
124  29   27   0       6     3     8 124  0.10557201 -0.158035233
125  31   12   0       7     3     0 125  0.42499501 -0.719726725
126  42   52   0       8     3    21 126  2.18182153  0.778117253
127  37   23   0       9     3     6 127  1.38326402 -0.307819631
128  28   10   0      10     3     6 128 -0.05413949 -0.794618924
129  36   52   0      11     3     6 129  1.22355252  0.778117253
130  24   33   0      12     3     8 130 -0.69298550  0.066641363
131  23   18   0      13     3     6 131 -0.85269700 -0.495050128
132  36   42   0      14     3    12 132  1.22355252  0.403656259
133  26   87   0      15     3    10 133 -0.37356249  2.088730734
134  26   50   0      16     3     0 134 -0.37356249  0.703225054
135  28   18   0      17     3     3 135 -0.05413949 -0.495050128
136  31  111   0      18     3    28 136  0.42499501  2.987437121
137  32   18   0      19     3     2 137  0.58470651 -0.495050128
138  21   20   0      20     3     6 138 -1.17212000 -0.420157930
139  29   12   0      21     3     3 139  0.10557201 -0.719726725
140  21    9   0      22     3     3 140 -1.17212000 -0.832065024
141  32   17   0      23     3     3 141  0.58470651 -0.532496228
142  25   28   0      24     3     2 142 -0.53327400 -0.120589134
143  30   55   0      25     3    76 143  0.26528351  0.890455552
144  40    9   0      26     3     2 144  1.86239852 -0.832065024
145  19   10   0      27     3     4 145 -1.49154300 -0.794618924
146  22   47   0      28     3    13 146 -1.01240850  0.590886756
147  18   76   1      29     3     9 147 -1.65125450  1.676823640
148  32   38   1      30     3     9 148  0.58470651  0.253871861
149  20   19   1      31     3     3 149 -1.33183150 -0.457604029
150  30   10   1      32     3     1 150  0.26528351 -0.794618924
151  18   19   1      33     3     7 151 -1.65125450 -0.457604029
152  24   24   1      34     3     1 152 -0.69298550 -0.270373532
153  30   31   1      35     3    19 153  0.26528351 -0.008250835
154  35   14   1      36     3     7 154  1.06384102 -0.644834526
155  27   11   1      37     3     0 155 -0.21385099 -0.757172825
156  20   67   1      38     3     7 156 -1.33183150  1.339808745
157  22   41   1      39     3     2 157 -1.01240850  0.366210159
158  28    7   1      40     3     1 158 -0.05413949 -0.906957223
159  23   22   1      41     3     4 159 -0.85269700 -0.345265731
160  40   13   1      42     3     0 160  1.86239852 -0.682280626
161  33   46   1      43     3    25 161  0.74441801  0.553440656
162  21   36   1      44     3     3 162 -1.17212000  0.178979662
163  35   38   1      45     3     6 163  1.06384102  0.253871861
164  25    7   1      46     3     2 164 -0.53327400 -0.906957223
165  26   36   1      47     3     8 165 -0.37356249  0.178979662
166  25   11   1      48     3     0 166 -0.53327400 -0.757172825
167  22  151   1      49     3    72 167 -1.01240850  4.485281100
168  32   22   1      50     3     2 168  0.58470651 -0.345265731
169  25   41   1      51     3     5 169 -0.53327400  0.366210159
170  35   32   1      52     3     1 170  1.06384102  0.029195264
171  21   56   1      53     3    28 171 -1.17212000  0.927901651
172  41   24   1      54     3     4 172  2.02211002 -0.270373532
173  32   16   1      55     3     4 173  0.58470651 -0.569942327
174  26   22   1      56     3    19 174 -0.37356249 -0.345265731
175  21   25   1      57     3     0 175 -1.17212000 -0.232927432
176  36   13   1      58     3     0 176  1.22355252 -0.682280626
177  37   12   1      59     3     3 177  1.38326402 -0.719726725
178  31   11   0       1     4     3 178  0.42499501 -0.757172825
179  30   11   0       2     4     3 179  0.26528351 -0.757172825
180  25    6   0       3     4     5 180 -0.53327400 -0.944403322
181  36    8   0       4     4     4 181  1.22355252 -0.869511123
182  22   66   0       5     4    21 182 -1.01240850  1.302362646
183  29   27   0       6     4     7 183  0.10557201 -0.158035233
184  31   12   0       7     4     2 184  0.42499501 -0.719726725
185  42   52   0       8     4    12 185  2.18182153  0.778117253
186  37   23   0       9     4     5 186  1.38326402 -0.307819631
187  28   10   0      10     4     0 187 -0.05413949 -0.794618924
188  36   52   0      11     4    22 188  1.22355252  0.778117253
189  24   33   0      12     4     4 189 -0.69298550  0.066641363
190  23   18   0      13     4     2 190 -0.85269700 -0.495050128
191  36   42   0      14     4    14 191  1.22355252  0.403656259
192  26   87   0      15     4     9 192 -0.37356249  2.088730734
193  26   50   0      16     4     5 193 -0.37356249  0.703225054
194  28   18   0      17     4     3 194 -0.05413949 -0.495050128
195  31  111   0      18     4    29 195  0.42499501  2.987437121
196  32   18   0      19     4     5 196  0.58470651 -0.495050128
197  21   20   0      20     4     7 197 -1.17212000 -0.420157930
198  29   12   0      21     4     4 198  0.10557201 -0.719726725
199  21    9   0      22     4     4 199 -1.17212000 -0.832065024
200  32   17   0      23     4     5 200  0.58470651 -0.532496228
201  25   28   0      24     4     8 201 -0.53327400 -0.120589134
202  30   55   0      25     4    25 202  0.26528351  0.890455552
203  40    9   0      26     4     1 203  1.86239852 -0.832065024
204  19   10   0      27     4     2 204 -1.49154300 -0.794618924
205  22   47   0      28     4    12 205 -1.01240850  0.590886756
206  18   76   1      29     4     8 206 -1.65125450  1.676823640
207  32   38   1      30     4     4 207  0.58470651  0.253871861
208  20   19   1      31     4     0 208 -1.33183150 -0.457604029
209  30   10   1      32     4     3 209  0.26528351 -0.794618924
210  18   19   1      33     4     4 210 -1.65125450 -0.457604029
211  24   24   1      34     4     3 211 -0.69298550 -0.270373532
212  30   31   1      35     4    16 212  0.26528351 -0.008250835
213  35   14   1      36     4     4 213  1.06384102 -0.644834526
214  27   11   1      37     4     4 214 -0.21385099 -0.757172825
215  20   67   1      38     4     7 215 -1.33183150  1.339808745
216  22   41   1      39     4     5 216 -1.01240850  0.366210159
217  28    7   1      40     4     0 217 -0.05413949 -0.906957223
218  23   22   1      41     4     0 218 -0.85269700 -0.345265731
219  40   13   1      42     4     3 219  1.86239852 -0.682280626
220  33   46   1      43     4    15 220  0.74441801  0.553440656
221  21   36   1      44     4     8 221 -1.17212000  0.178979662
222  35   38   1      45     4     7 222  1.06384102  0.253871861
223  25    7   1      46     4     3 223 -0.53327400 -0.906957223
224  26   36   1      47     4     8 224 -0.37356249  0.178979662
225  25   11   1      48     4     0 225 -0.53327400 -0.757172825
226  22  151   1      49     4    63 226 -1.01240850  4.485281100
227  32   22   1      50     4     4 227  0.58470651 -0.345265731
228  25   41   1      51     4     7 228 -0.53327400  0.366210159
229  35   32   1      52     4     5 229  1.06384102  0.029195264
230  21   56   1      53     4    13 230 -1.17212000  0.927901651
231  41   24   1      54     4     0 231  2.02211002 -0.270373532
232  32   16   1      55     4     3 232  0.58470651 -0.569942327
233  26   22   1      56     4     8 233 -0.37356249 -0.345265731
234  21   25   1      57     4     1 234 -1.17212000 -0.232927432
235  36   13   1      58     4     0 235  1.22355252 -0.682280626
236  37   12   1      59     4     2 236  1.38326402 -0.719726725</code></pre>
</div>
</div>
</div>
</div>
</div>
<section id="モデルの推定とプロット" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="モデルの推定とプロット"><span class="header-section-number">1.1</span> モデルの推定とプロット</h3>
<div class="callout callout-style-default callout-caution callout-titled" title="フィッティングの出力">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
フィッティングの出力
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(brms)</span>
<span id="cb5-2">fit1 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(count <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> zAge <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> zBase <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Trt <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>patient),</span>
<span id="cb5-3">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> epilepsy, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">poisson</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Compiling Stan program...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Start sampling</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 5.1e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 1.039 seconds (Warm-up)
Chain 1:                0.712 seconds (Sampling)
Chain 1:                1.751 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1.2e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 1.102 seconds (Warm-up)
Chain 2:                0.763 seconds (Sampling)
Chain 2:                1.865 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1.6e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 1.02 seconds (Warm-up)
Chain 3:                0.79 seconds (Sampling)
Chain 3:                1.81 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 1.2e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 1.008 seconds (Warm-up)
Chain 4:                0.74 seconds (Sampling)
Chain 4:                1.748 seconds (Total)
Chain 4: </code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summary</span>(fit1)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> Family: poisson 
  Links: mu = log 
Formula: count ~ zAge + zBase * Trt + (1 | patient) 
   Data: epilepsy (Number of observations: 236) 
  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
         total post-warmup draws = 4000

Multilevel Hyperparameters:
~patient (Number of levels: 59) 
              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)     0.58      0.07     0.46     0.74 1.00      719     1600

Regression Coefficients:
           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept      1.77      0.12     1.54     2.00 1.00      690     1215
zAge           0.10      0.09    -0.09     0.28 1.00      618     1043
zBase          0.70      0.12     0.47     0.94 1.01      675     1415
Trt1          -0.27      0.16    -0.59     0.06 1.00      724     1395
zBase:Trt1     0.05      0.16    -0.27     0.37 1.00      758     1479

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
<p>基本的な解析の前提がまず出力され，推定結果はグループ毎（今回は患者毎）の変数（今回は <img src="https://latex.codecogs.com/png.latex?%5Calpha_i">）から表示される．</p>
<p>後半に固定効果の係数，すなわち回帰係数の推定結果が表示される．</p>
<p>治療効果<code>Trt</code>の係数は負で，平均的に処置効果はある可能性があるが，95% 信頼区間は <img src="https://latex.codecogs.com/png.latex?0"> を跨いでいるという意味で，有意とは言えない．また，交差項<code>zBase*Trt</code>の係数は小さく，交互効果の存在を示す証拠はないと思われる．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BR%7D"> が１より大きい場合，MCMC が収束していない可能性を意味する <span class="citation" data-cites="Vehtari+2021">(Vehtari et al., 2021)</span>．通説には <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BR%7D%5Cle1.1"> などの基準がある．</p>
<p>変数を指定して，事後分布と MCMC の軌跡をプロットできる：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot</span>(fit1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">variable =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b_Trt1"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b_zBase"</span>))</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Computation/brms_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>より詳しく見るには<code>conditional_effects</code>関数を用いることもできる．交差項の効果はほとんどないことがわかる：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">plot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">conditional_effects</span>(fit1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">effects =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"zBase:Trt"</span>))</span></code></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Computation/brms_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="モデルによる予測" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="モデルによる予測"><span class="header-section-number">1.2</span> モデルによる予測</h3>
<p>fit したモデル <code>fit1</code> を用いて，平均年齢と平均ベースレートを持つ患者に対する治療効果を予測する：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">newdata <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Trt =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">zAge =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">zBase =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb13-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">predict</span>(fit1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> newdata, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">re_formula =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Estimate Est.Error Q2.5 Q97.5
[1,]  5.93450  2.522360    2    11
[2,]  4.54775  2.183898    1     9</code></pre>
</div>
</div>
<p>関数<a href="https://paul-buerkner.github.io/brms/reference/predict.brmsfit.html"><code>predict()</code></a>は事後予測分布からのサンプリングを行う．一方で，関数<a href="https://paul-buerkner.github.io/brms/reference/fitted.brmsfit.html"><code>fitted()</code></a>は平均を返す．</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fitted</span>(fit1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> newdata, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">re_formula =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Estimate Est.Error     Q2.5    Q97.5
[1,] 5.939461 0.7104193 4.648686 7.423907
[2,] 4.519774 0.5232029 3.604023 5.638578</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled" title="予測の出力">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
予測の出力
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>従って，もう１度ずつ実行すると，<code>predict</code>では値が変わるが，<code>fitted</code>では同じ値が出力される．</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">predict</span>(fit1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> newdata, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">re_formula =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Estimate Est.Error Q2.5 Q97.5
[1,]  5.93175  2.544250    2    11
[2,]  4.48125  2.199286    1     9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fitted</span>(fit1, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">newdata =</span> newdata, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">re_formula =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Estimate Est.Error     Q2.5    Q97.5
[1,] 5.939461 0.7104193 4.648686 7.423907
[2,] 4.519774 0.5232029 3.604023 5.638578</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-model-comparison" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="sec-model-comparison"><span class="header-section-number">1.3</span> モデルの比較</h3>
<p>モデル<code>fit1</code>で行った Poisson 回帰分析は，個々の観測が独立であるという仮定の上に成り立っている（第 3.1.3 節）．</p>
<p>この仮定が破れているとき，Poisson 分布の性質 <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BE%7D%5BX%5D=%5Cmathrm%7BV%7D%5BX%5D=%5Clambda%5Cqquad%20(X%5Csim%5Cmathrm%7BPois%7D(%5Clambda))%0A"> からの離反として現れ，この現象は <strong>過分散</strong>（overdispersion）とも呼ばれる．</p>
<section id="観測レベルランダム効果" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="観測レベルランダム効果"><span class="header-section-number">1.3.1</span> 観測レベルランダム効果</h4>
<p>ということで，Poisson 分布族ではなく，分散が平均よりも大きいような別の分布族を用いて，フィット度合いを比較してみることを考えたい．</p>
<p>そこで，追加の変動をモデルに追加するべく，モデル<code>fit1</code>に観測ごとの切片項 <img src="https://latex.codecogs.com/png.latex?%5Ceta_%7Bit%7D"> を追加してみる（この手法は観測レベルランダム効果と呼ばれる．第 2.6 節参照）．</p>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1">fit2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(count <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> zAge <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> zBase <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Trt <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>patient) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>obs),</span>
<span id="cb21-2">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> epilepsy, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">poisson</span>())</span></code></pre></div>
<div class="callout callout-style-default callout-caution callout-titled" title="フィッティングの出力">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
フィッティングの出力
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1">fit2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(count <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> zAge <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> zBase <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Trt <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>patient) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>obs),</span>
<span id="cb22-2">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> epilepsy, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">poisson</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Compiling Stan program...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Start sampling</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 7.3e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 1.976 seconds (Warm-up)
Chain 1:                1.156 seconds (Sampling)
Chain 1:                3.132 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1.8e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 2.089 seconds (Warm-up)
Chain 2:                1.839 seconds (Sampling)
Chain 2:                3.928 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1.8e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 1.888 seconds (Warm-up)
Chain 3:                1.158 seconds (Sampling)
Chain 3:                3.046 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 2.3e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 1.96 seconds (Warm-up)
Chain 4:                1.613 seconds (Sampling)
Chain 4:                3.573 seconds (Total)
Chain 4: </code></pre>
</div>
</div>
</div>
</div>
</div>
<p>こうして得た２つのモデル<code>fit1</code>,<code>fit2</code>を比較する．</p>
<p>LLO (Leave-One-Out) cross-validation が関数<code>loo</code>によって実行できる：</p>
<div class="sourceCode" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">loo</span>(fit1, fit2)</span></code></pre></div>
<div class="callout callout-style-default callout-caution callout-titled" title="LOO-CV の結果">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LOO-CV の結果
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">loo</span>(fit1, fit2)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Found 6 observations with a pareto_k &gt; 0.7 in model 'fit1'. We
recommend to set 'moment_match = TRUE' in order to perform moment matching for
problematic observations.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Found 55 observations with a pareto_k &gt; 0.7 in model 'fit2'. We
recommend to set 'moment_match = TRUE' in order to perform moment matching for
problematic observations.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Output of model 'fit1':

Computed from 4000 by 236 log-likelihood matrix.

         Estimate   SE
elpd_loo   -673.3 37.4
p_loo        96.0 15.4
looic      1346.6 74.8
------
MCSE of elpd_loo is NA.
MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.3]).

Pareto k diagnostic values:
                         Count Pct.    Min. ESS
(-Inf, 0.7]   (good)     230   97.5%   144     
   (0.7, 1]   (bad)        4    1.7%   &lt;NA&gt;    
   (1, Inf)   (very bad)   2    0.8%   &lt;NA&gt;    
See help('pareto-k-diagnostic') for details.

Output of model 'fit2':

Computed from 4000 by 236 log-likelihood matrix.

         Estimate   SE
elpd_loo   -595.0 14.0
p_loo       107.7  7.1
looic      1190.1 27.9
------
MCSE of elpd_loo is NA.
MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).

Pareto k diagnostic values:
                         Count Pct.    Min. ESS
(-Inf, 0.7]   (good)     181   76.7%   65      
   (0.7, 1]   (bad)       50   21.2%   &lt;NA&gt;    
   (1, Inf)   (very bad)   5    2.1%   &lt;NA&gt;    
See help('pareto-k-diagnostic') for details.

Model comparisons:
     elpd_diff se_diff
fit2   0.0       0.0  
fit1 -78.3      27.9  </code></pre>
</div>
</div>
</div>
</div>
</div>
<p><code>elpd_diff</code> は expected log posterior density の差異を表す．<code>fit2</code>の方が大きく当てはまりが良いことが見て取れる．</p>
<p>また，WAIC (Watanabe-Akaike Information Criterion) も実装されている：</p>
<div class="sourceCode" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">waic</span>(fit1))</span></code></pre></div>
<div class="callout callout-style-default callout-caution callout-titled" title="WAIC の結果">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
WAIC の結果
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">waic</span>(fit1))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: 
52 (22.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Computed from 4000 by 236 log-likelihood matrix.

          Estimate   SE
elpd_waic   -668.5 36.5
p_waic        91.2 14.5
waic        1337.0 73.1

52 (22.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. </code></pre>
</div>
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">waic</span>(fit2))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: 
64 (27.1%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Computed from 4000 by 236 log-likelihood matrix.

          Estimate   SE
elpd_waic   -572.5 12.0
p_waic        85.1  5.2
waic        1144.9 24.1

64 (27.1%) p_waic estimates greater than 0.4. We recommend trying loo instead. </code></pre>
</div>
</div>
</div>
</div>
</div>
<p>他にも，<code>reloo</code>, <code>kfold</code> などの関数もある．</p>
<div class="callout callout-style-default callout-caution callout-titled" title="他の関数一覧">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
他の関数一覧
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">methods</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"brmsfit"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] add_criterion           add_ic                  as_draws_array         
 [4] as_draws_df             as_draws_list           as_draws_matrix        
 [7] as_draws_rvars          as_draws                as.array               
[10] as.data.frame           as.matrix               as.mcmc                
[13] autocor                 bayes_factor            bayes_R2               
[16] bridge_sampler          coef                    conditional_effects    
[19] conditional_smooths     control_params          default_prior          
[22] expose_functions        family                  fitted                 
[25] fixef                   formula                 getCall                
[28] hypothesis              kfold                   log_lik                
[31] log_posterior           logLik                  loo_compare            
[34] loo_linpred             loo_model_weights       loo_moment_match       
[37] loo_predict             loo_predictive_interval loo_R2                 
[40] loo_subsample           loo                     LOO                    
[43] marginal_effects        marginal_smooths        mcmc_plot              
[46] model_weights           model.frame             nchains                
[49] ndraws                  neff_ratio              ngrps                  
[52] niterations             nobs                    nsamples               
[55] nuts_params             nvariables              pairs                  
[58] parnames                plot                    post_prob              
[61] posterior_average       posterior_epred         posterior_interval     
[64] posterior_linpred       posterior_predict       posterior_samples      
[67] posterior_smooths       posterior_summary       pp_average             
[70] pp_check                pp_mixture              predict                
[73] predictive_error        predictive_interval     prepare_predictions    
[76] print                   prior_draws             prior_summary          
[79] psis                    ranef                   reloo                  
[82] residuals               restructure             rhat                   
[85] stancode                standata                stanplot               
[88] summary                 update                  VarCorr                
[91] variables               vcov                    waic                   
[94] WAIC                   
see '?methods' for accessing help and source code</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="患者内の相関構造のモデリング" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="患者内の相関構造のモデリング"><span class="header-section-number">1.3.2</span> 患者内の相関構造のモデリング</h4>
<p>また，<code>fit1</code>において，同一患者の異なる訪問の間には全く相関がないと仮定されており，これは全く非現実的な仮定をおいてしまっていると言える．</p>
<p>患者内の相関構造は，<code>brm()</code>関数の<code>autocor</code>引数で指定できる（第 3.1.3.2 節）．</p>
<p>例えば，全く構造を仮定しない場合は，<a href="http://paul-buerkner.github.io/brms/reference/unstr.html"><code>unstr</code></a>を指定する：</p>
<div class="sourceCode" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1">fit3 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(count <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> zAge <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> zBase <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Trt <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>patient),</span>
<span id="cb40-2">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autocor =</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">unstr</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">time=</span>visit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">gr=</span>patient),</span>
<span id="cb40-3">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> epilepsy, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">poisson</span>())</span></code></pre></div>
<div class="callout callout-style-default callout-caution callout-titled" title="フィッティングの出力">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
フィッティングの出力
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1">fit3 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">brm</span>(count <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> zAge <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> zBase <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> Trt <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span>patient),</span>
<span id="cb41-2">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">autocor =</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">unstr</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">time=</span>visit, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">gr=</span>patient),</span>
<span id="cb41-3">            <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">data =</span> epilepsy, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">poisson</span>())</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Argument 'autocor' should be specified within the 'formula' argument.
See ?brmsformula for help.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Compiling Stan program...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Start sampling</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000117 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.17 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 4.071 seconds (Warm-up)
Chain 1:                2.597 seconds (Sampling)
Chain 1:                6.668 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 3.4e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 3.856 seconds (Warm-up)
Chain 2:                2.163 seconds (Sampling)
Chain 2:                6.019 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 3.4e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 3.833 seconds (Warm-up)
Chain 3:                2.182 seconds (Sampling)
Chain 3:                6.015 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 6.8e-05 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.68 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 3.859 seconds (Warm-up)
Chain 4:                2.367 seconds (Sampling)
Chain 4:                6.226 seconds (Total)
Chain 4: </code></pre>
</div>
</div>
</div>
</div>
</div>
<p>このモデルも<code>fit1</code>より遥かに当てはまりが良く，<code>fit2</code>とほとんど同じ当てはまりの良さが見られる：</p>
<div class="callout callout-style-default callout-caution callout-titled" title="LOO-CV の結果">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LOO-CV の結果
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">loo</span>(fit2,fit3)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Found 55 observations with a pareto_k &gt; 0.7 in model 'fit2'. We
recommend to set 'moment_match = TRUE' in order to perform moment matching for
problematic observations.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Found 64 observations with a pareto_k &gt; 0.7 in model 'fit3'. We
recommend to set 'moment_match = TRUE' in order to perform moment matching for
problematic observations.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Output of model 'fit2':

Computed from 4000 by 236 log-likelihood matrix.

         Estimate   SE
elpd_loo   -595.0 14.0
p_loo       107.7  7.1
looic      1190.1 27.9
------
MCSE of elpd_loo is NA.
MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).

Pareto k diagnostic values:
                         Count Pct.    Min. ESS
(-Inf, 0.7]   (good)     181   76.7%   65      
   (0.7, 1]   (bad)       50   21.2%   &lt;NA&gt;    
   (1, Inf)   (very bad)   5    2.1%   &lt;NA&gt;    
See help('pareto-k-diagnostic') for details.

Output of model 'fit3':

Computed from 4000 by 236 log-likelihood matrix.

         Estimate   SE
elpd_loo   -602.5 15.0
p_loo       113.4  8.1
looic      1204.9 30.0
------
MCSE of elpd_loo is NA.
MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.5]).

Pareto k diagnostic values:
                         Count Pct.    Min. ESS
(-Inf, 0.7]   (good)     172   72.9%   122     
   (0.7, 1]   (bad)       56   23.7%   &lt;NA&gt;    
   (1, Inf)   (very bad)   8    3.4%   &lt;NA&gt;    
See help('pareto-k-diagnostic') for details.

Model comparisons:
     elpd_diff se_diff
fit2  0.0       0.0   
fit3 -7.4       2.6   </code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="その他の例" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="その他の例"><span class="header-section-number">1.4</span> その他の例</h3>
<p>Sebastian Weber らにより，<a href="https://opensource.nibr.com/bamdd/">新薬の治験における実際の解析事例をまとめたウェブサイト</a> が公開されている．<sup>2</sup></p>
<p>特に，<a href="https://opensource.nibr.com/bamdd/src/02h_mmrm.html#brms-implementation">13 章</a>で，同様の経時的繰り返し観測データを扱っているが，ここではカウントデータではなく連続な応用変数が扱われている．</p>
</section>
</section>
<section id="混合効果モデルに関する補足" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="混合効果モデルに関する補足"><span class="header-section-number">2</span> 混合効果モデルに関する補足</h2>
<div class="callout callout-style-simple callout-important no-icon callout-titled" title="概要">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
概要
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>ランダム効果モデルとは，グループ毎に異なる切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> を追加し，これにも誤差を仮定してモデルに入れて得る階層モデルである．</li>
<li>しかし，<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> が（ユニットレベルの）説明変数 <img src="https://latex.codecogs.com/png.latex?x_i"> と相関を持つ場合，推定量の一致性が失われる．これを回避するために，<img src="https://latex.codecogs.com/png.latex?x_i"> の係数 <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> にのみ関心がある場合は，固定効果モデルが用いられることも多い．</li>
<li>だが，簡単なトリック（<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の説明変数に <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Bx%7D_s"> を追加すること）で，推定量の一致性を回復することができる．</li>
<li>このトリックを取り入れたランダム効果モデルは，<img src="https://latex.codecogs.com/png.latex?x_i"> と <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> に相関がない場合は固定効果モデルと等価な <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> の推定量を与え，相関がある場合でも，<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> を一致推定し，各変動切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の構造にも洞察を与えてくれる．</li>
</ul>
</div>
</div>
<section id="ランダム効果のモデル" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="ランダム効果のモデル"><span class="header-section-number">2.1</span> ランダム効果のモデル</h3>
<p>ランダム効果は，変動する切片項と呼んだ方がわかりやすい <span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007)</span> と言われるように，サブグループ毎に異なる切片を加えたモデルである．</p>
<p>従って，ユニットレベルの回帰式を書き下すと，グループ選択関数 <img src="https://latex.codecogs.com/png.latex?s:%5Bn%5D%5Cto%5BS%5D%5C;(S%5Cle%20n)"> を通じて， <span id="eq-stage-1"><img src="https://latex.codecogs.com/png.latex?%0Ay_i=%5Calpha_%7Bs%5Bi%5D%7D+%5Cbeta%20x_i+%5Cepsilon_i,%5Cqquad%20i%5Cin%5Bn%5D,%0A%5Ctag%7B1%7D"></span> というようになる．</p>
<p>これは，確率変数 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の平均を <img src="https://latex.codecogs.com/png.latex?%5Calpha_0"> とすると，グループレベルの回帰式 <span id="eq-stage-2"><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_s=%5Calpha_0+%5Ceta_s,%5Cqquad%20s%5Cin%5BS%5D%0A%5Ctag%7B2%7D"></span> が背後にある階層モデルだとみなすこともできる．</p>
</section>
<section id="説明変数との相関の問題" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="説明変数との相関の問題"><span class="header-section-number">2.2</span> 説明変数との相関の問題</h3>
<section id="問題の所在" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="問題の所在"><span class="header-section-number">2.2.1</span> 問題の所在</h4>
<p>ランダム効果では，ユニットレベルの説明変数 <img src="https://latex.codecogs.com/png.latex?x_i"> と変動切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> が相関を持たないという仮定が Gauss-Markov の定理の仮定に等価になるため，これが違反されると推定量の不偏性・一致性が約束されず，推定量の分散も大きくなる．<sup>3</sup></p>
<p>実際，ランダム効果モデルの階層構造を，式&nbsp;2 を 式&nbsp;1 に代入することで一つの式にまとめると <span id="eq-RF"><img src="https://latex.codecogs.com/png.latex?%0Ay_i=%5Calpha_0+%5Cbeta%20x_i+%5Cunderbrace%7B%5Cepsilon_i'%7D_%7B%5Cepsilon_i+%5Ceta_%7Bs%5Bi%5D%7D%7D%0A%5Ctag%7B3%7D"></span> を得る．<sup>4</sup> <img src="https://latex.codecogs.com/png.latex?x_i"> と <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の相関をモデルに含めていない場合，<img src="https://latex.codecogs.com/png.latex?x_i"> と <img src="https://latex.codecogs.com/png.latex?%5Ceta_s"> が相関を持ってしまい，結果として 式&nbsp;3 では説明変数と誤差 <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i'"> に相関が生じてしまう．<sup>5</sup></p>
</section>
<section id="業界の現状" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="業界の現状"><span class="header-section-number">2.2.2</span> 業界の現状</h4>
<p>そのため，ランダム効果モデルは避けられる傾向にあり，切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D%5Cequiv%5Calpha_0"> は変動しないとし，グループレベルの効果を無視してモデリングすることも多い： <img src="https://latex.codecogs.com/png.latex?%0Ay_i=%5Calpha_0+%5Cbeta%20x_i+%5Cepsilon_i.%0A"> このことを complete pooling model と呼び，ランダム効果モデルを partial pooling model と対比させることがある．<sup>6</sup></p>
<p>実際，これ以上の仮定を置かず，ランダム効果は局外母数として一般化推定方程式の方法（第 2.4.1 節）によれば，<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> の不偏推定が可能である．</p>
<p>リンク関数 <img src="https://latex.codecogs.com/png.latex?g"> を通じた非線型モデル <img src="https://latex.codecogs.com/png.latex?%0Ag(%5Coperatorname%7BE%7D%5By_i%7Cx_i%5D)=%5Cbeta%20x_i%0A"> であっても，指数型分布族を仮定すれば，<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> の一致推定が可能である．</p>
<p>このような場合は，marginal model や population-average model とも呼ばれる <span class="citation" data-cites="Gardiner+2009">(Gardiner et al., 2009, p. 228)</span>．</p>
</section>
<section id="sec-fixed-effects-model" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="sec-fixed-effects-model"><span class="header-section-number">2.2.3</span> 固定効果モデルという解決</h4>
<p>問題を起こさずに，しかしながらグループレベルの効果をモデリングしたい場合， <img src="https://latex.codecogs.com/png.latex?%0Ay_i=%5Calpha_%7Bs%5Bi%5D%7D%5E%7B%5Ctext%7Bunmodeled%7D%7D+%5Cbeta%20x_i+%5Cepsilon_i%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_s%5E%7B%5Ctext%7Bunmodeled%7D%7D%5Csim%5Cmathrm%7BN%7D(%5Calpha_0,%5Cinfty)%0A"> として，グループ毎に変動する切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D%5E%7B%5Ctext%7Bunmodeled%7D%7D"> を許すが，この変数自体にモデルは仮定しない．</p>
<p>このようなモデルは，グループ毎に別々の回帰分析を実行し，<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> の値はこれらのグループの間で適切に重みづけて最終的な推定値としているに等しい．</p>
<p>すなわち，グループの数だけ，グループへの所属を表す２値変数 <img src="https://latex.codecogs.com/png.latex?1_%7Bs%5Bi%5D=s%7D"> を導入し，<img src="https://latex.codecogs.com/png.latex?S"> 個の項 <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bs=1%7D%5ES1_%7Bs%5Bi%5D=s%7D%5Calpha_%7Bs%5Bi%5D%7D%5E%7B%5Ctext%7Bunmodeled%7D%7D"> を説明変数に加えて回帰分析を行うことに等しい．</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="名前">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
名前
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007)</span> は unmodeled varying intercept と呼んでいる．</li>
<li><span class="citation" data-cites="Hansen2022">(Hansen, 2022)</span> をはじめ，計量経済学では fixed effects model と呼ばれる．</li>
<li>least squares dummy variable regression とも呼べる．<sup>7</sup></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="利点">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
利点
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?x_i"> と <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> が相関を持ち得る場合も，固定効果モデルでは関係がない．<sup>8</sup></p>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="問題点">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
問題点
</div>
</div>
<div class="callout-body-container callout-body">
<p>異なるグループのデータが相互作用する機構がランダム効果モデルに比べて貧しい．</p>
<p>（正しく特定された）ランダム効果モデルの方は，外れ値グループが存在しても，<img src="https://latex.codecogs.com/png.latex?%5Ceta_s"> を通じて緩やかに情報が伝達され，<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> の値は平均へ縮小されて推定されるが，固定効果モデルではそのような頑健性を持たない．<sup>9</sup></p>
</div>
</div>
<p>固定効果モデルは <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> （のみ）に関心がある場合，<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> と <img src="https://latex.codecogs.com/png.latex?x_i"> の相関の存在に対してロバストな推定法として有用であり，その理由で計量経済学（特に線型パネルデータ）では主流の推定手法となっている．<sup>10</sup> 実際，<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> と <img src="https://latex.codecogs.com/png.latex?x_i"> が無相関であるとき，<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> に関しては等価な推定量を与える．</p>
<blockquote class="blockquote">
<p>Current econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estmimator for linear panel data models. <span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 624)</span></p>
</blockquote>
<p>逆に言えば，固定効果モデルは <img src="https://latex.codecogs.com/png.latex?x_i"> と <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の構造のモデリングを放棄したモデリング法であり，各 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の値にも興味がある場合，および <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> のグループ毎の値も考えたい場合にはやはり <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の誤差と相関構造もモデルに取り入れたランダム効果モデルを用いたい，ということになる．</p>
</section>
</section>
<section id="ランダム効果モデルにおける相関のモデリング" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="ランダム効果モデルにおける相関のモデリング"><span class="header-section-number">2.3</span> ランダム効果モデルにおける相関のモデリング</h3>
<p><img src="https://latex.codecogs.com/png.latex?x_i"> と <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> との相関は，欠落変数が存在するため，と考えることができる．</p>
<p>そして，この相関は，説明変数の平均 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Bx%7D_s"> を変動する切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_s"> の説明変数として追加することで除去できる：<sup>11</sup></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_i=%5Calpha_%7Bs%5Bi%5D%7D+%5Cbeta%20x_i+%5Cepsilon_i%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_s=%5Calpha_0+%5Calpha_1%5Coverline%7Bx%7D_s+%5Ceta_s%0A"></p>
<p>これにより，Gauss-Markov の仮定（外生性）が回復される．</p>
<p><span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, pp. 7–9)</span> にシミュレーションによる検証が掲載されている．</p>
<blockquote class="blockquote">
<p>Practitioners can get around this problem by taking advantage of the multilevel structure of their regression equation. <span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, p. 12)</span></p>
</blockquote>
</section>
<section id="混合効果モデル" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="混合効果モデル"><span class="header-section-number">2.4</span> 混合効果モデル</h3>
<p>以上，解説してきた「ランダム効果モデル」であるが，混合効果モデルとも呼ばれる．<sup>12</sup></p>
<p>何を言っているのかわからないかもしれないが，式 式&nbsp;1 <img src="https://latex.codecogs.com/png.latex?%0Ay_i=%5Calpha_%7Bs%5Bi%5D%7D+%5Cbeta%20x_i+%5Cepsilon_i,%5Cqquad%20i%5Cin%5Bn%5D,%0A"> において，<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> がランダム効果であるが，回帰係数 <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> を固定効果とも呼ぶのである．</p>
<p>現代的には，必要ならば <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> を確率変数とみなしても良いだろうが，慣習的にそう呼ぶため，これに従わざるを得ない，というのが <span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 625)</span> などを見る限り共通了解であるようである．</p>
<p>これが計量経済学における固定効果モデル（第 2.2.3 節）の名前の由来である．<sup>13</sup> 固定効果モデルは，たしかに（ユニットレベルでの回帰係数という意味での）「固定効果」を表す変数しか含んでいない（少なくとも見た目上は）．</p>
<p>そこで，式 式&nbsp;1 自体は，固定効果と変量効果の両方を含んだ <strong>混合（効果）モデル</strong> というのである．</p>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="名前">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
名前
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="citation" data-cites="Chung+2013">(Chung et al., 2013)</span> によると</p>
<ul>
<li>線型混合モデル (linear mixed models) <span class="citation" data-cites="Kincaid2005">(Kincaid, 2005)</span></li>
<li>階層モデル (hierarchical models)</li>
<li>マルチレベル線型モデル (multilevel linear models)</li>
<li>混合効果モデル (mixed-effects models) <span class="citation" data-cites="Chung+2015">(Chung et al., 2015)</span></li>
<li>ランダム効果モデル (random effects model) <span class="citation" data-cites="Hubbard+2010">(Hubbard et al., 2010)</span> （え？）</li>
<li>分散成分モデル (variance component model)<sup>14</sup></li>
</ul>
<p>などと呼ばれる．</p>
<p>ただし，ランダム効果モデルと呼んでしまうことも多い．<span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007)</span> のアブストラクトなど．</p>
</div>
</div>
<section id="sec-GEE" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="sec-GEE"><span class="header-section-number">2.4.1</span> GEE との違い</h4>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="一般化推定方程式 (GEE: Generalized Estimating Equation) との違い">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
一般化推定方程式 (GEE: Generalized Estimating Equation) との違い
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><u>回帰式が違う</u></p>
<p>線型の場合の GEE は <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20Y_%7Bit%7D=%5Calpha+%5Cbeta_1x_%7B1,i,t%7D+%5Ccdots+%5Cbeta_px_%7Bp,i,t%7D%0A%20%20%20"> とも表され，ランダムな切片項というものは見当たらない．その代わり，グループ間の影響は相関係数行列としてモデル化を行う．ランダム効果モデルでは，この相関構造を，ランダムな切片項を追加し，その回帰式も立てることでモデルに取り込む．</p></li>
<li><p><u>推定目標が違う</u></p>
<p>GEE は population average model でよく用いられる <span class="citation" data-cites="Hubbard+2010">(Hubbard et al., 2010)</span> ように，あくまで応答 <img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D"> の平均の不偏推定が目標であり，共分散構造はいわば局外母数である．一方，混合効果モデルは，その階層モデルとしての性質の通り，平均構造と分散構造のいずれも推定対象として扱う志向性がある．</p></li>
<li><p><u>推定方法が違う</u></p>
<p>混合効果モデルは主に最尤法により推定される <span class="citation" data-cites="Hubbard+2010">(Hubbard et al., 2010)</span>．GEE はモーメント法により推定され，最尤法ベースではないため，完全にランダムとは言えない欠測がある場合は弱く，IPW などの方法が用いられる．</p></li>
</ol>
</div>
</div>
<p>GEE にとって相関構造は局外母数であり，正確な特定は目的に含まれない．この意味で GEE の相関係数⾏列におく仮定は「間違えていてもよい便宜的な仮定」であるため，作業相関係数行列 (working correlation coefficient matrix) とも呼ばれる．相関構造を誤特定していても，平均構造は一致推定が可能であり，ロバストである．両方の特定に成功した場合はセミパラメトリック有効性が達成される．</p>
<p>一方で，混合効果モデルは，階層モデルとして，平均構造と分散構造のいずれにも明示的な仮定をおくため，片方（例えば共分散構造）の特定を間違えていた場合，もう片方の解釈性が失われる，というリスクがあると論じることができる．特に <span class="citation" data-cites="Hubbard+2010">(Hubbard et al., 2010)</span> に見られる論調である．</p>
<p>しかし，子供の身長の成長曲線の描画が主な研究目標である場合など，ユニットの平均効果ではなく各個人に注目したい場合には，（特に変動係数を取り入れた）混合効果モデルの方が適していることになる <span class="citation" data-cites="Gardiner+2009">(Gardiner et al., 2009)</span>．実際，モデルの特定に成功していれば，いずれのパラメータも最尤推定されるため，一致性を持つ．</p>
<p>従って，モデル選択において用いられる基準も違う．GEE における作業相関係数行列と説明変数の選択には QIC (Quasi-likelihood Information Criterion) が，混合効果モデルには AIC や BIC （または cAIC や mAIC <span class="citation" data-cites="Vaida-Blanchard2005">(Vaida &amp; Blanchard, 2005)</span>）が用いられる <span class="citation" data-cites="Gardiner+2009">(Gardiner et al., 2009, p. 228)</span>．</p>
<!--

本データを扱った論文 [@Thall-Vail1990] では，[@Liang-Zeger1986] の一般化推定方程式の枠組みに則り，共分散の構造にどのようなパラメトリック分布を仮定するのが良いかが，漸近論の観点から議論されている．

-->
</section>
<section id="ベイズ混合効果モデルという光" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="ベイズ混合効果モデルという光"><span class="header-section-number">2.4.2</span> ベイズ混合効果モデルという光……？</h4>
<p>しかし，結局ベイズ統計学の立場からは，２つの違いはほとんど重要ではなく，混合効果モデルを推定した後に，周辺化をして平均構造に関する marginal estimator を構成すれば，GEE の代用になっているのではないか？</p>
<p>計算機の性能と，計算統計手法の発展が目まぐるしい現代にて，過去の議論を踏襲しすぎることは，問題の本質を誤るということもあるのだろう．</p>
<p>ということで，以上議論したグループレベル構造を持ったデータに対する２階の階層モデルを，本稿では「混合効果モデル」と呼ぶことにする．</p>
<p>この節はこれで終わり．</p>
</section>
</section>
<section id="sec-group-level-variance-estimation" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="sec-group-level-variance-estimation"><span class="header-section-number">2.5</span> グループレベル分散の推定</h3>
<p>混合効果モデル（階層モデル）の推定において，特にグループ数 <img src="https://latex.codecogs.com/png.latex?S"> が小さい場合，グループレベルの変動切片項 <img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Bs%5Bi%5D%7D"> の共分散行列 <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BV%7D%5B%5Ceta_s%5D"> の推定が不安定になるという問題点が古くからの問題である <span class="citation" data-cites="Harville1977">(Harville, 1977)</span>．<sup>15</sup></p>
<p>この <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BV%7D%5B%5Ceta_s%5D"> は何の仮定も置かれておらず，グループ間の相関構造のモデリングを一手に引き受けている．EM アルゴリズムが提案されたばかりの頃 <span class="citation" data-cites="Laird-Ware1982">(Laird &amp; Ware, 1982)</span> では，共分散構造にパラメトリックな仮定をおいていたが，現代ではこれを取り去った最尤推定法・ベイズ推定法が主流である．</p>
<p>しかし，最尤推定法と，一定の事前分布を仮定したベイズ MAP 推定法では，推定された共分散行列が退化してしまうことがある．これは Wishart 事前分布を仮定することでこれが回避される <span class="citation" data-cites="Chung+2015">(Chung et al., 2015)</span>．<sup>16</sup> これは最尤法の文脈では，penalized likelihood と等価になる <span class="citation" data-cites="Chung+2013">(Chung et al., 2013)</span>．</p>
<p>モデルのサイズによっては，完全なベイズ推定を実行することが難しく，一部は等価な頻度論的な方法や近似を用いることもある．その際，最適化ソルバーの収束を速めるために，共分散構造に（データや計画とは無関係に）パラメトリックモデルを仮定してしまうこともある <span class="citation" data-cites="Kincaid2005">(Kincaid, 2005)</span>．</p>
</section>
<section id="sec-subsec-count-data" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="sec-subsec-count-data"><span class="header-section-number">2.6</span> カウントデータのモデリング</h3>
<p>カウントデータの基本は Poisson 分布であろうが，過分散を考慮するために，負の二項分布でモデリングすることもできる．これは例えば，マーケティングにおいて，顧客の購買回数をモデル化する際に用いられる <span class="citation" data-cites="森岡-今西16-確率思考の戦略論">(森岡毅，今西聖貴, 2016)</span>．</p>
<p>この行為は，Poisson 分布の Gamma 分布による混合分布族を用いた，混合モデリングを行っているとみなせる：</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="命題">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
命題
</div>
</div>
<div class="callout-body-container callout-body">
<p>Poisson 分布 <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BPois%7D(%5Ctheta)"> の <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BGamma%7D(%5Calpha,%5Cnu)">-混合は負の二項分布 <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BNB%7D%5Cleft(%5Cnu,%5Cfrac%7B%5Calpha%7D%7B%5Calpha+1%7D%5Cright)"> になる．</p>
<p>ただし，負の二項分布 <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BNB%7D(%5Cnu,p)"> は，次の確率質量関数 <img src="https://latex.codecogs.com/png.latex?p(x;%5Cnu,p)"> が定める <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BN%7D"> 上の確率分布である： <img src="https://latex.codecogs.com/png.latex?%0Ap(x;%5Cnu,p)=%5Cbegin%7Bpmatrix%7Dx+%5Cnu-1%5C%5Cx%5Cend%7Bpmatrix%7Dp%5E%5Cnu(1-p)%5Ex.%0A"></p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>確率分布の変換則より，次のように計算できる：</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20p(x)&amp;=%5Cint_%7B%5Cmathbb%7BR%7D_+%7D%5Cfrac%7B%5Ctheta%5Ex%7D%7Bx!%7De%5E%7B-%5Ctheta%7D%5Cfrac%7B1%7D%7B%5CGamma(%5Cnu)%7D%5Calpha%5E%5Cnu%5Ctheta%5E%7B%5Cnu-1%7De%5E%7B-%5Calpha%5Ctheta%7Dd%5Ctheta%5C%5C%0A%20%20&amp;=%5Cfrac%7B%5Calpha%5E%5Cnu%7D%7Bx!%5CGamma(%5Cnu)%7D%5Cint_%7B%5Cmathbb%7BR%7D_+%7D%5Ctheta%5E%7Bx+%5Cnu-1%7De%5E%7B-(%5Calpha+1)%5Ctheta%7Dd%5Ctheta%5C%5C%0A%20%20&amp;=%5Cfrac%7B%5Calpha%5E%5Cnu%7D%7Bx!%5CGamma(%5Cnu)%7D%5Cfrac%7B%5CGamma(x+%5Cnu)%7D%7B(%5Calpha+1)%5E%7Bx+%5Cnu%7D%7D%5C%5C%0A%20%20&amp;=%5Cbegin%7Bpmatrix%7D%5Cnu+x-1%5C%5Cx%5Cend%7Bpmatrix%7D%5Cleft(%5Cfrac%7B1%7D%7B%5Calpha+1%7D%5Cright)%5Ex%5Cleft(%5Cfrac%7B%5Calpha%7D%7B%5Calpha+1%7D%5Cright)%5E%5Cnu.%0A%5Cend%7Balign*%7D"></p>
<p>この最右辺は，たしかに負の二項分布の質量関数である．</p>
<p>この証明方法と，Gamma 分布については次の記事を参照：</p>
<div class="article-card-container">
  <div class="article-card">
    <a href="https://162348.github.io/posts/2023/Probability/Beta-Gamma.html" target="_blank">
      <img src="https://162348.github.io/posts/2023/Probability/Beta-Gamma_files/figure-html/cell-4-output-1.png" alt="Article Image" class="article-image">
      <div class="article-content">
        <h3 class="article-title anchored">確率測度の変換則</h3>
        <p class="article-description">Gamma 分布とBeta 分布を例に</p>
      </div>
    </a>
  </div>
</div>
</div>
</div>
</div>
<p>Poisson 回帰</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%20y_%7Bit%7D%20&amp;%20%5Csim%20%5Coperatorname%7BPois%7D(%5Clambda_%7Bs%5Bi%5D%7D)%20%5C%5C%20%5Clog(%5Clambda_%7Bs%5Bi%5D%7D)%20&amp;%20=%20%5Calpha_i%20+%20%5Ceta_%7Bit%7D%20%5C%5C%20%5Ceta_%7Bit%7D%20&amp;%20%5Csim%20%5Coperatorname%7BN%7D(0,%20%5Csigma).%20%5Cend%7Balign*%7D%0A"></p>
<p>を考えると，各 <img src="https://latex.codecogs.com/png.latex?y_%7Bit%7D"> を，（グループ毎に条件付ければ）Poisson 分布の対数正規分布による混合分布を用いてモデル化していることにあたる．</p>
<p>この，Poisson-対数正規分布族は，<span class="citation" data-cites="Bulmer1974">(Bulmer, 1974)</span> により生物種の個体数分布のモデリングで，過分散を説明するために用いられている．</p>
<p>すなわち，第 1 節のモデルの比較 1.3 で扱った，<strong>観測レベルランダム効果</strong> (OLRE: Observation-level Random Effects) の方法は，観測毎に <img src="https://latex.codecogs.com/png.latex?%5Ceta_%7Bit%7D"> というランダム切片項を追加するだけで，本質的には混合モデリングを実施するという，いわばハックのような使い方である．<sup>17</sup></p>
<p>今回はモデル比較の結果が良かったため，本格的に対数正規混合を実施してみるのも良いかもしれない．</p>
</section>
</section>
<section id="brmsの実装" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="brmsの実装"><span class="header-section-number">3</span> <code>brms</code>の実装</h2>
<p><a href="https://paul-buerkner.github.io/brms/reference/brm.html"><code>brm</code> 関数</a>（コードは <a href="https://github.com/paul-buerkner/brms/blob/master/R/brm.R">こちら</a>）の実装を調べる．</p>
<div class="callout callout-style-simple callout-important no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><a href="https://github.com/paul-buerkner/brms/blob/master/R/brm.R#L436"><code>brms</code></a></li>
</ul>
<p>Stan コードを扱っている関数は <a href="https://github.com/paul-buerkner/brms/blob/d42adcd22f5af441870038b1d78ad4d9408f344f/R/stancode.R"><code>.stancode()</code></a> であった．最終的に，<a href="https://github.com/paul-buerkner/brms/blob/d42adcd22f5af441870038b1d78ad4d9408f344f/R/backends.R#L67"><code>.compile_model_rstan()</code></a> と <a href="https://github.com/paul-buerkner/brms/blob/d42adcd22f5af441870038b1d78ad4d9408f344f/R/backends.R#L145"><code>.fit_model_rstan()</code></a> が呼ばれるようになっている．</p>
<ul>
<li><a href="https://github.com/paul-buerkner/brms/blob/d42adcd22f5af441870038b1d78ad4d9408f344f/R/standata.R#L109"><code>.standata</code></a></li>
</ul>
</div>
</div>
</div>
<section id="モデリング機能" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="モデリング機能"><span class="header-section-number">3.1</span> モデリング機能</h3>
<section id="事前分布" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="事前分布"><span class="header-section-number">3.1.1</span> 事前分布</h4>
<p><a href="https://paul-buerkner.github.io/brms/reference/brm.html"><code>brm</code>関数</a> では，デフォルトでは無情報事前分布が用いられる．</p>
<blockquote class="blockquote">
<p>Default priors are chosen to be non or very weakly informative so that their influence on the results will be negligible and you usually don’t have to worry about them. However, after getting more familiar with Bayesian statistics, I recommend you to start thinking about reasonable informative priors for your model parameters: Nearly always, there is at least some prior information available that can be used to improve your inference.<br><a href="https://paul-buerkner.github.io/brms/reference/brm.html">brm(): Fit Bayesian Generalized (Non-)Linear Multivariate Multilevel Models</a></p>
</blockquote>
</section>
<section id="回帰式" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="回帰式"><span class="header-section-number">3.1.2</span> 回帰式</h4>
<p><code>brm()</code>関数の第一引数は<code>brmsformula</code>オブジェクトを取る．</p>
<p><a href="http://paul-buerkner.github.io/brms/reference/brmsformula.html"><code>brmsformula()</code>関数</a> は，R の<code>formula</code>オブジェクトを通じて，階層モデルを定義できるようになっている（実装は<a href="../../../posts/2024/Computation/R3.html">リスト</a>）．</p>
</section>
<section id="sec-ubsubsec-covariance-structure" class="level4" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="sec-ubsubsec-covariance-structure"><span class="header-section-number">3.1.3</span> 共分散構造</h4>
<p>共分散構造は２つの観点から，<code>brmsformula</code>オブジェクトから自動的に指定される．</p>
<p>１つ目がグルーピング構造（共分散行列のブロック構造）であり，これは<a href="https://paul-buerkner.github.io/brms/reference/gr.html"><code>gr</code>関数</a> が使用される．２つ目がグループ内の相関構造であり，これは<code>brm()</code>関数の<code>autocor</code>引数を用いる．</p>
<section id="gr関数" class="level5" data-number="3.1.3.1">
<h5 data-number="3.1.3.1" class="anchored" data-anchor-id="gr関数"><span class="header-section-number">3.1.3.1</span> <code>gr</code>関数</h5>
<p>この関数は<code>brm</code>関数の第一引数として与えられたモデル定義式から，暗黙のうちに内部で呼び出される．</p>
<p>例えば，回帰式に<code>(1|patient)</code>が含まれていた場合，<code>gr(patient)</code>が呼び出される．</p>
<p>共分散構造におく仮定について，重要なデフォルト設定が２つある：</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>グループ間の相関構造は想定されている：<code>cor=True</code>．</p></li>
<li><p>一方で，グループ内の相関構造は想定されておらず，独立とされている．具体的に指定したい場合は引数<code>cov</code>を用いる．</p>
<blockquote class="blockquote">
<p>By default, levels of the same grouping factor are modeled as independent of each other.<br><a href="https://paul-buerkner.github.io/brms/reference/gr.html">gr(): Set up basic grouping terms in brms</a></p>
</blockquote></li>
</ul>
<p>すなわち，<img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BV%7D%5B%5Ceta_s%5D"> には一才仮定が置かれておらず（第 2.5 節），一方で <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cepsilon_%7Bit%7D%5C%7D_%7Bt=1%7D%5ET"> は互いに独立とされている．</p>
</div>
</div>
</div>
<p>また，この二階層目の分布族（第 1 節での <img src="https://latex.codecogs.com/png.latex?%5Calpha_i"> と <img src="https://latex.codecogs.com/png.latex?%5Ceta_%7Bit%7D">）は，分散共分散行列 <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BV%7D%5B%5Ceta_s%5D"> を持った正規分布がデフォルトで，現状他の分布族は指定できないでいる．</p>
<blockquote class="blockquote">
<p>dist: Name of the distribution of the group-level effects. Currently “gaussian” is the only option.<br><a href="https://paul-buerkner.github.io/brms/reference/gr.html">gr(): Set up basic grouping terms in brms</a></p>
</blockquote>
</section>
<section id="sec-autocor-argument" class="level5" data-number="3.1.3.2">
<h5 data-number="3.1.3.2" class="anchored" data-anchor-id="sec-autocor-argument"><span class="header-section-number">3.1.3.2</span> <code>autocor</code>引数</h5>
<p><code>brm()</code>関数には，<a href="http://paul-buerkner.github.io/brms/reference/autocor-terms.html"><code>autocor</code>引数</a> が用意されている．</p>
<p><code>gr()</code>のデフォルト値では独立とされていたグループ内の相関構造を，具体的に指定するのに用いられる．</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><code>unstr</code>：一才の仮定を置かない．</li>
<li><code>AR</code>：一次の自己相関構造．</li>
</ul>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="モデリング実装" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="モデリング実装"><span class="header-section-number">3.2</span> モデリング実装</h3>
<section id="回帰式-1" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="回帰式-1"><span class="header-section-number">3.2.1</span> 回帰式</h4>
<p><code>brm</code>関数に与えられた<code>formula</code>は，<code>validate_formula</code>関数に渡される．</p>
<p>この関数は S3 のメソッドのディスパッチを用いて実装されており，<code>brmsformula</code>オブジェクトに対しては，<code>validate_formula.brmsformula</code>関数が呼び出される．</p>
<p>ここでは<code>autocor</code>引数が引かれている場合，出力の<code>formula</code>属性に追加される：<sup>18</sup></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1">fit3<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>formula</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>count ~ zAge + zBase * Trt + (1 | patient) 
autocor ~ unstr(time = visit, gr = patient)</code></pre>
</div>
</div>
</section>
</section>
<section id="推論エンジン" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="推論エンジン"><span class="header-section-number">3.3</span> 推論エンジン</h3>
<p><a href="https://paul-buerkner.github.io/brms/reference/brm.html"><code>brm</code>関数</a> は，Stan による MCMC サンプリングを通じて，事後分布を計算する．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Bafumi-Gelman2007" class="csl-entry">
Bafumi, J., &amp; Gelman, A. (2007). <span class="nocase">Fitting Multilevel Models When Predictors and Group Effects Correlate</span>. <em>SSRN</em>. <a href="https://dx.doi.org/10.2139/ssrn.1010095">https://dx.doi.org/10.2139/ssrn.1010095</a>
</div>
<div id="ref-Bulmer1974" class="csl-entry">
Bulmer, M. G. (1974). On fitting the poisson lognormal distribution to species-abundance data. <em>Biometrics</em>, <em>30</em>(1), 101–110. <a href="http://www.jstor.org/stable/2529621">http://www.jstor.org/stable/2529621</a>
</div>
<div id="ref-Burkner2017" class="csl-entry">
Bürkner, P.-C. (2017). Brms: An r package for bayesian multilevel models using stan. <em>Journal of Statistical Software</em>, <em>80</em>(1), 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>
</div>
<div id="ref-Burkner2018" class="csl-entry">
Bürkner, P.-C. (2018). <span class="nocase">Advanced Bayesian Multilevel Modeling with the R Package brms</span>. <em><span>The R Journal</span></em>, <em>10</em>(1), 395–411. <a href="https://doi.org/10.32614/RJ-2018-017">https://doi.org/10.32614/RJ-2018-017</a>
</div>
<div id="ref-Burkner2021" class="csl-entry">
Bürkner, P.-C. (2021). Bayesian item response modeling in r with brms and stan. <em>Journal of Statistical Software</em>, <em>100</em>(5), 1–54. <a href="https://doi.org/10.18637/jss.v100.i05">https://doi.org/10.18637/jss.v100.i05</a>
</div>
<div id="ref-Chung+2015" class="csl-entry">
Chung, Y., Gelman, A., Rabe-Hesketh, S., Liu, J., &amp; Dorie, V. (2015). Weakly informative prior for point estimation of covariance matrices in hierarchical models. <em>Journal of Educational and Behavioral Statistics</em>, <em>40</em>(2), 136–157. <a href="https://doi.org/10.3102/1076998615570945">https://doi.org/10.3102/1076998615570945</a>
</div>
<div id="ref-Chung+2013" class="csl-entry">
Chung, Y., Rabe-Hesketh, S., Dorie, V., Gelman, A., &amp; Liu, J. (2013). A nondegenerate penalized likelihood estimator for variance parameters in multilevel models. <em>Psychometrika</em>, <em>78</em>(4), 685–709. <a href="https://doi.org/10.1007/s11336-013-9328-2">https://doi.org/10.1007/s11336-013-9328-2</a>
</div>
<div id="ref-Cunningham21-Mixtape" class="csl-entry">
Cunningham, S. (2021). <em>Causal inference : The mixtape</em>. Yale University Press.
</div>
<div id="ref-Gardiner+2009" class="csl-entry">
Gardiner, J. C., Luo, Z., &amp; Roman, L. A. (2009). Fixed effects, random effects and GEE: What are the differences? <em>Statistics in Medicine</em>, <em>28</em>(2), 221–239. https://doi.org/<a href="https://doi.org/10.1002/sim.3478">https://doi.org/10.1002/sim.3478</a>
</div>
<div id="ref-Hansen2022" class="csl-entry">
Hansen, B. E. (2022). <em>Econometrics</em>. Princeton University Press.
</div>
<div id="ref-Harville1977" class="csl-entry">
Harville, D. A. (1977). Maximum likelihood approaches to variance component estimation and to related problems. <em>Journal of the American Statistical Association</em>, <em>72</em>(358), 320–338. <a href="https://doi.org/10.1080/01621459.1977.10480998">https://doi.org/10.1080/01621459.1977.10480998</a>
</div>
<div id="ref-Hubbard+2010" class="csl-entry">
Hubbard, A. E., Ahern, J., Fleischer, N. L., Laan, M. V. der, Lippman, S. A., Jewell, N., Bruckner, T., &amp; Satariano, W. A. (2010). To GEE or not to GEE: Comparing population average and mixed models for estimating the associations between neighborhood risk factors and health. <em>Epidemiology</em>, <em>21</em>(4). <a href="https://journals.lww.com/epidem/fulltext/2010/07000/to_gee_or_not_to_gee__comparing_population_average.7.aspx">https://journals.lww.com/epidem/fulltext/2010/07000/to_gee_or_not_to_gee__comparing_population_average.7.aspx</a>
</div>
<div id="ref-Kincaid2005" class="csl-entry">
Kincaid, C. D. (2005). <span class="nocase">Guidelines for Selecting the Covariance Structure in Mixed Model Analysis</span>. <em>SAS User Group International</em>, <em>30</em>, 198–130. <a href="https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/198-30.pdf">https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/198-30.pdf</a>
</div>
<div id="ref-Laird-Ware1982" class="csl-entry">
Laird, N. M., &amp; Ware, J. H. (1982). Random-effects models for longitudinal data. <em>Biometrics</em>, <em>38</em>(4), 963–974. <a href="http://www.jstor.org/stable/2529876">http://www.jstor.org/stable/2529876</a>
</div>
<div id="ref-Leppik+1987" class="csl-entry">
Leppik, I. E., Dreifuss, F. E., Porter, R., Bowman, T., Santilli, N., Jacobs, M., Crosby, C., Cloyd, J., Stackman, J., Graves, N., Sutula, T., Welty, T., Vickery, J., Brundage, R., Gates, J., Gumnit, R. J., &amp; Gutierrez, A. (1987). A controlled study of progabide in partial seizures. <em>Neurology</em>, <em>37</em>(6), 963–963. <a href="https://doi.org/10.1212/WNL.37.6.963">https://doi.org/10.1212/WNL.37.6.963</a>
</div>
<div id="ref-Thall-Vail1990" class="csl-entry">
Thall, P. F., &amp; Vail, S. C. (1990). Some covariance models for longitudinal count data with overdispersion. <em>Biometrics</em>, <em>46</em>(3), 657–671. <a href="http://www.jstor.org/stable/2532086">http://www.jstor.org/stable/2532086</a>
</div>
<div id="ref-Vaida-Blanchard2005" class="csl-entry">
Vaida, F., &amp; Blanchard, S. (2005). Conditional akaike information for mixed-effects models. <em>Biometrika</em>, <em>92</em>(2), 351–370. <a href="http://www.jstor.org/stable/20441193">http://www.jstor.org/stable/20441193</a>
</div>
<div id="ref-Vehtari+2021" class="csl-entry">
Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2021). <span class="nocase">Rank-Normalization, Folding, and Localization: An Improved <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BR%7D"> for Assessing Convergence of MCMC (with Discussion)</span>. <em>Bayesian Analysis</em>, <em>16</em>(2), 667–718. <a href="https://doi.org/10.1214/20-BA1221">https://doi.org/10.1214/20-BA1221</a>
</div>
<div id="ref-森岡-今西16-確率思考の戦略論" class="csl-entry">
森岡毅，今西聖貴. (2016). <em>確率思考の戦略論―ＵＳＪでも実証された数学マーケティングの力</em>. 角川書店.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>R を最新バージョン 4.3.1 → 4.4.0 にアップデートしなければインストールに失敗したことに注意．↩︎</p></li>
<li id="fn2"><p>Statistical Modeling, Causal Inference, and Social Science における <a href="https://statmodeling.stat.columbia.edu/2024/05/08/bamdd/">こちらのエントリ</a> も参照．↩︎</p></li>
<li id="fn3"><p><span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 333)</span> 第12.3節，<span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, p. 3)</span>, <span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 604)</span>，<span class="citation" data-cites="Gardiner+2009">(Gardiner et al., 2009, p. 228)</span>．↩︎</p></li>
<li id="fn4"><p>このような誤差項の構造 <img src="https://latex.codecogs.com/png.latex?e_%7Bit%7D=%5Calpha_i+%5Cepsilon_%7Bit%7D"> を一元誤差成分モデル (one-way error component model) ともいう <span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 600)</span>．↩︎</p></li>
<li id="fn5"><p>この，説明変数と誤差の間に相関があることを，計量経済学では <strong>内生性</strong> (endogeneity) という．↩︎</p></li>
<li id="fn6"><p><span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, p. 5)</span>．↩︎</p></li>
<li id="fn7"><p><span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, p. 5)</span>，<span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 609)</span> 17.11節 など．狭義では，fixed effects model は within transformation を行い，グループ間の影響を引いたあとに回帰を実行する……という手続きを指すこともあるが，２つは等価な結果を生む．詳しくは <span class="citation" data-cites="Cunningham21-Mixtape">(Cunningham, 2021)</span> なども参照．↩︎</p></li>
<li id="fn8"><p><span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 624)</span> 17.25節．↩︎</p></li>
<li id="fn9"><p><span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, pp. 4–5)</span>．↩︎</p></li>
<li id="fn10"><p><span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 624)</span>，<span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, p. 6)</span>．↩︎</p></li>
<li id="fn11"><p><span class="citation" data-cites="Bafumi-Gelman2007">(Bafumi &amp; Gelman, 2007, p. 6)</span>．↩︎</p></li>
<li id="fn12"><p><span class="citation" data-cites="Hubbard+2010">(Hubbard et al., 2010)</span> では両方の名前で呼んでいる．↩︎</p></li>
<li id="fn13"><p><span class="citation" data-cites="Hansen2022">(Hansen, 2022, p. 625)</span> 17.25節．疫学・生物統計学では，実験計画法でしか「固定効果」「変量効果モデル」とは言わない，という認識であることも筆者は聞いたことがある．↩︎</p></li>
<li id="fn14"><p><img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BV%7D%5B%5Ceta_s%5D"> はブロック行列の構造を持つためこう呼ばれるs．↩︎</p></li>
<li id="fn15"><p><span class="citation" data-cites="Laird-Ware1982">(Laird &amp; Ware, 1982)</span>，<span class="citation" data-cites="Chung+2013">(Chung et al., 2013)</span>，<span class="citation" data-cites="Chung+2015">(Chung et al., 2015)</span>，<a href="https://statmodeling.stat.columbia.edu/2023/06/02/blme-bayesian-linear-mixed-effects-models/">Statistical Modeling, Causal Inference, and Social Science ブログ 6/2/2023</a>．↩︎</p></li>
<li id="fn16"><p>逆 Wishart ではないらしい <span class="citation" data-cites="Chung+2015">(Chung et al., 2015)</span>．↩︎</p></li>
<li id="fn17"><p><a href="https://solomonkurz.netlify.app/blog/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/#negative-binomial-counts">Solomon Kurtz (2021)</a> による解説，<a href="https://rpubs.com/INBOstats/OLRE">RPubs</a> も参照．↩︎</p></li>
<li id="fn18"><p><a href="https://github.com/paul-buerkner/brms/blob/deb56d02d0f897422a4d1d5a43d18e99400f80a0/R/brmsformula.R#L1363">Line 1363</a>．↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a></div></div></section></div> ]]></description>
  <category>MCMC</category>
  <category>Stan</category>
  <category>R</category>
  <guid>https://162348.github.io/posts/2024/Computation/brms.html</guid>
  <pubDate>Sat, 11 May 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Computation/brms_files/figure-html/unnamed-chunk-4-1.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>志学・応用数学</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Life/AppliedMath.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="はじめに" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="はじめに"><span class="header-section-number">1</span> はじめに</h2>
<p>現在，AI，専門的には機械学習と呼ばれる分野の研究と社会実装が急速に進んでいる．</p>
<p>機械学習は，元々は情報理論と計算機科学に源流を持つが，ロボティクス，コンピュータビジョン，認知科学，統計力学，統計学を巻き込んで，かつてないほど学際的な研究テーマとして雪だるま式に成長している．</p>
<p>特に，トランスフォーマーや状態空間モデルなどを取り入れた大規模な深層学習モデルは学術・産業の両面で大きな成功を見ている．同時に，大自由度モデルとしての深層学習モデルは，平均場理論をはじめとした統計力学の手法を通じての理論解析も進んでいる．</p>
<p>Tensor Programs <span class="citation" data-cites="Yang2019">(Yang, 2019)</span> など，実際に応用に貢献できる理論も急速に発達してきている．この応用と理論の関係は，今後さらに密接になっていくだろう．</p>
<p>高次元統計学，情報統計力学などがキーワードとなっていくだろう．しかし筆者は，ここに新たなキーワードを提案したい．それは <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> である．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Life/Images/ModernAppliedMath.svg" class="img-fluid figure-img"></p>
<figcaption>現代の応用数学では，応用分野が増えただけでなく，応用分野間で相互に知見の交換がある．数学は「共通言語」としての役割も期待されていくだろう．</figcaption>
</figure>
</div>
</section>
<section id="mathcalpe-とは" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="mathcalpe-とは"><span class="header-section-number">2</span> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> とは</h2>
<p>可測空間 <img src="https://latex.codecogs.com/png.latex?(E,%5Cmathcal%7BE%7D)"> に対して，その上の確率測度の全体からなる空間を <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> と表そう．有限な測度の全体 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5E1(E)"> は Banach 空間をなし，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> はその閉凸集合である．</p>
<p>統計モデルとは，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> の適当に切り出した部分集合をいう．例えば指数分布族のなす部分集合 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D%5Csubset%5Cmathcal%7BP%7D(E)"> などに注目し，その上の微分幾何的な構造を調べる分野は情報幾何学と呼ばれる．</p>
<p>このように統計学で <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> は重要な研究対象であるが，ベイズ統計学ではあらゆる不確実性の表現に確率分布を用いるため，特に重要な役割を果たす．事前分布を条件付けて事後分布を求める手続きは，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の写像を与えており，いわば <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の（適切な）移動がベイズ推論といっても過言でないのである．</p>
<p>統計力学において，ランダムな系（時々刻々と物理量の確率分布が移り変わっていく系）は，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上でみれば決定論的な系として解析できる．例えば，平衡分布とは <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の力学系として見たときの不動点である．このような不動点は複数あり，実際，熱力学極限において等価な予測を与えるモデルは複数存在する．そのうち，計算しやすいものを選ぶのが統計力学の立場であり，統計学でいうモデル選択と対応する．このような統計的な問題を，古典的な推測統計の問題と対比して <strong>順問題</strong> と呼ぶこととする．<sup>1</sup></p>
<div class="article-card-container">
  <div class="article-card">
    <a href="https://162348.github.io/posts/2024/Nature/StatisticalMechanics2.html" target="_blank">
      <img src="https://162348.github.io/posts/2024/Nature/canon.svg" alt="Article Image" class="article-image">
      <div class="article-content">
        <h3 class="article-title anchored">数学者のための統計力学２</h3>
        <p class="article-description">小正準集団・正準集団・大正準集団</p>
      </div>
    </a>
  </div>
</div>
<p>機械学習では，統計的な逆問題を解くという問題意識を古典的な統計学と共有しており，特に <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上に真のデータ分布 <img src="https://latex.codecogs.com/png.latex?P_0%5Cin%5Cmathcal%7BP%7D(E)"> を特定することを目標とする．しかし古典的なデータ解析の手法でこれを達成するのではなく，様々な尺度（汎函数 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)%5E2%5Cto%5Cmathbb%7BR%7D">）を通じて，<img src="https://latex.codecogs.com/png.latex?P_0"> に最も近い分布を選び出すという原則を採る．この <strong>乖離度</strong> (divergence) と呼ばれる汎函数を最適化することにより，データ解析の手続きを自動化することが，機械学習に通底する精神である．</p>
</section>
<section id="物理学と機械学習の双対性" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="物理学と機械学習の双対性"><span class="header-section-number">3</span> 物理学と機械学習の双対性</h2>
<p>ここで筆者は，この <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> を中心に据えてこれらの分野を見渡してみると，物理学と機械学習の間に双対的な関係があると考える．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Life/Images/AppliedMath.svg" class="img-fluid figure-img"></p>
<figcaption>左：数学における幾何と解析の双対性．右：これからの応用数学における物理的アプローチと機械学習的アプローチの双対性．</figcaption>
</figure>
</div>
<section id="幾何学と解析学は双対である" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="幾何学と解析学は双対である"><span class="header-section-number">3.1</span> 幾何学と解析学は双対である</h3>
<p>数学者 <a href="https://en.wikipedia.org/wiki/Mikhael_Gromov_(mathematician)">Mikhael Gromov</a> によると，空間 <img src="https://latex.codecogs.com/png.latex?X"> の解析学とは <img src="https://latex.codecogs.com/png.latex?X"> <strong>上の</strong> 関数の研究で，<img src="https://latex.codecogs.com/png.latex?X"> の幾何学とは <img src="https://latex.codecogs.com/png.latex?X"> <strong>への</strong> 関数の研究である <span class="citation" data-cites="深谷賢治1997">(深谷賢治, 1997, p. 11)</span>．<sup>2</sup></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://www.math.kyoto-u.ac.jp/~fukaya/shzuok.pdf"><img src="https://162348.github.io/posts/2024/Kernels/Images/Gromov.png" class="img-fluid figure-img" alt="Gromov による幾何と解析の解釈（深谷賢治講義録より）"></a></p>
<figcaption>Gromov による幾何と解析の解釈（深谷賢治講義録より）</figcaption>
</figure>
</div>
<p>だからといって何だというわけではないが，幾何と解析の，未知の対象 <img src="https://latex.codecogs.com/png.latex?X"> へのアプローチの違いを上手く捉えた，大変簡潔な定義である．</p>
</section>
<section id="物理学は" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="物理学は"><span class="header-section-number">3.2</span> 物理学は</h3>
<p>いわば統計の <strong>順問題</strong> を解いてきたと言える．</p>
<p>（平衡）統計力学は系の統計的ふるまいを記述するために，正準集団を代表としていくつかの等価なモデルを用意しており，<sup>3</sup> 場合に応じて計算しやすいものを用いるというモデル選択の立場に立っていると言える．<sup>4</sup></p>
<p>系の確率的なダイナミクスを予測するために <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> や <span class="citation" data-cites="Glauber1963">(Glauber, 1963)</span> の方法など，現在 MCMC として知られる重要なシミュレーション法が開発された．</p>
<p>つまり，現代ではベイズ統計学では事後分布からの推論手法として用いられる MCMC は，物理学においては「実際の系よりも高速に平衡に至るように設計された Markov ダイナミクス」として発明されたものであり，平衡分布の統計的な性質を調べようとするサンプリング法として開発されたということである．</p>
<p>MCMC の近年の発展はこちらの記事も参照：</p>
<div class="article-card-container">
  <div class="article-card">
    <a href="https://162348.github.io/posts/2024/Computation/MCMC.html" target="_blank">
      <img src="https://162348.github.io/static/Posters/ISM-OH2024.jpg" alt="Article Image" class="article-image">
      <div class="article-content">
        <h3 class="article-title anchored">新時代の MCMC を迎えるために</h3>
        <p class="article-description">連続時間アルゴリズムへの進化</p>
      </div>
    </a>
  </div>
</div>
<!--

Markov 連鎖のエルゴード性の研究は，統計力学の発展と両輪であった．Markov 性はなんといってもランダムな自然現象の基本的なモデルであり，これを通じて確率的なダイナミクスを調べるという方法は強力な計算科学の手法として受け入れられた．

統計力学は順問題的である側面があり，コレを逆問題に応用したのがベイズ統計学であると捉えられる．

加えて，SDE と拡散過程は熱方程式をはじめとする PDE の幾何学的な解釈を与える．

-->
</section>
<section id="機械学習では" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="機械学習では"><span class="header-section-number">3.3</span> 機械学習では，</h3>
<p>基本的には統計的な逆問題，すなわちデータから背後の確率分布を推定する問題を解く．</p>
<p>だが，特に統計的な手続きを <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の力学系を通じて自動化しようという志向が古典的な統計学と異なる．</p>
<p>ベイズ手法を例にとれば，推論を MCMC などのダイナミクスに任せるのではなく，KL-乖離度を最小にする最適化によって実行する（変分推論という）．</p>
<p>変分推論だけでなく，多くの物理学的な原理が，何かしらの汎函数の最小化問題として変分法的に理解されるのと同様，<sup>5</sup> 機械学習の多くのアルゴリズムは <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の汎函数（特に KL-乖離度）の最適化問題として理解される．</p>
</section>
</section>
<section id="双対性の未来" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="双対性の未来"><span class="header-section-number">4</span> 双対性の未来</h2>
<p>こうして，<strong><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> への写像</strong> を志向する物理学と，<strong><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> からの写像</strong> を志向する機械学習の間に，基本的な立場の違いを見るのである．</p>
<p>今までも，そしておそらくこれからも，逆問題（統計的推論）においては，変分法と最適化によるアプローチの方が計算が速い．従って，たとえベイズ推論が必要な場面でも，MCMC よりは変分推論アルゴリズムを実装に用いることが多い．</p>
<p>しかし，MCMC をはじめとしたサンプリング法には，物理学的な美点がある．Ising 模型などの統計力学のモデルの平衡分布の様子を調べるのが MCMC の第一義的な用途であったように，機械学習モデルでも事後分布の様子を直接観察できる貴重な手段になるという点である．</p>
<p>すなわち，計算機シミュレーションは物理学においては実験を補間する「第三の手法」として位置付けられるかもしれないが，機械学習においては「唯一の実験手段」としての立場を確立するかもしれない．</p>
<div class="article-card-container">
  <div class="article-card">
    <a href="https://162348.github.io/posts/2024/Computation/AboutSimulation.html" target="_blank">
      <img src="https://162348.github.io/posts/2024/Computation/Files/computation.png" alt="Article Image" class="article-image">
      <div class="article-content">
        <h3 class="article-title anchored">計算とは何か</h3>
        <p class="article-description">数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションにより計算を実行する手段である．</p>
      </div>
    </a>
  </div>
</div>
<p>物理学において，変分原理とその他の理論との間に相補的な関係があるように，統計においても，変分推論と MCMC，果てには（本稿でいう）機械学習的なアプローチと物理学的なアプローチとは相補的な関係を果たしていくと考えられる．</p>
</section>
<section id="これからの応用数学" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="これからの応用数学"><span class="header-section-number">5</span> これからの応用数学</h2>
<p>そのためには，統計的な営みを <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上での数学的現象として捉え直し，物理学と機械学習の互いに双対的な２つの観点からの意味を比較考慮することが重要である．</p>
<p>そうすれば，ベイズ模型の物理的な意味を解析する場面と，高速なデータ解析を実行したい場面とに，明瞭な架け橋が渡されるのである．</p>
<p>統計的推論というダイナミクスを，変分原理の眼から捉える，情報の物理学になるかもしれない．変分原理に，かつてなかったベイズ推論としての意味や事前分布が明確化されることで，種々の推論アルゴリズムを統一的に理解することができるかもしれない．</p>
<p>このような観点から，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> という共通言語を中心に据えて，物理学と機械学習の２つのアプローチを融和させる，新たな応用数学を建てたい．</p>
<div class="table-responsive-sm">
<table class="table-striped table-hover table">
<caption>これからの統計数学と２つのアプローチ</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">物理的アプローチ</th>
<th style="text-align: center;">機械学習的アプローチ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">数学的内容</td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の幾何</td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上の解析</td>
</tr>
<tr class="even">
<td style="text-align: center;">統計的関心</td>
<td style="text-align: center;">順問題</td>
<td style="text-align: center;">逆問題</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ベイズ推論エンジン</td>
<td style="text-align: center;">MCMC</td>
<td style="text-align: center;">変分推論</td>
</tr>
<tr class="even">
<td style="text-align: center;">解析者</td>
<td style="text-align: center;">人間</td>
<td style="text-align: center;">計算機</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="例" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="例"><span class="header-section-number">6</span> 例</h2>
<p>現時点で筆者が集めている例は次のとおりである：</p>
<section id="機械学習とはmathcalpx-上の力学系である" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="機械学習とはmathcalpx-上の力学系である"><span class="header-section-number">6.1</span> 機械学習とは，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(X)"> 上の力学系である</h3>
<p>機械学習の過程が，相互情報量の最小化の過程として特徴付けられたのは <span class="citation" data-cites="Ackley+1985">(Ackley et al., 1985)</span> 以来であるようである．</p>
</section>
<section id="smc-テンパリング" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="smc-テンパリング"><span class="header-section-number">6.2</span> SMC テンパリング</h3>
<p>例えば，MCMC よりも多峰性に強いサンプリング法として期待されている <a href="../../../posts/Surveys/SMCSamplers.html">テンパリング SMC 法</a><sup>6</sup> は，最適化の観点からは，KL ダイバージェンスを <a href="https://en.wikipedia.org/wiki/Mirror_descent">鏡映降下法</a> によって最小化した際の離散力学系と等価になると報告されている <span class="citation" data-cites="Chopin+2023">(Chopin et al., 2023)</span>．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Ackley+1985" class="csl-entry">
Ackley, D. H., Hinton, G. E., &amp; Sejnowski, T. J. (1985). A learning algorithm for boltzmann machines. <em>Cognitive Science</em>, <em>9</em>(1), 147–169. https://doi.org/<a href="https://doi.org/10.1016/S0364-0213(85)80012-4">https://doi.org/10.1016/S0364-0213(85)80012-4</a>
</div>
<div id="ref-Chopin+2023" class="csl-entry">
Chopin, N., Crucinio, F. R., &amp; Korba, A. (2023). <em>A connection between tempering and entropic mirror descent</em>. <a href="https://arxiv.org/abs/2310.11914">https://arxiv.org/abs/2310.11914</a>
</div>
<div id="ref-Chopin-Papaspiliopoulos20-SMC" class="csl-entry">
Chopin, N., &amp; Papaspiliopoulos, O. (2020). <em>An introduction to sequential monte carlo</em>. Springer Cham.
</div>
<div id="ref-Glauber1963" class="csl-entry">
Glauber, R. J. (1963). <span class="nocase">Time‐Dependent Statistics of the Ising Model</span>. <em>Journal of Mathematical Physics</em>, <em>4</em>(2), 294–307. <a href="https://doi.org/10.1063/1.1703954">https://doi.org/10.1063/1.1703954</a>
</div>
<div id="ref-Metropolis+1953" class="csl-entry">
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a>
</div>
<div id="ref-Yang2019" class="csl-entry">
Yang, G. (2019). Wide feedforward or recurrent neural networks of any architecture are gaussian processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, &amp; R. Garnett (Eds.), <em>Advances in neural information processing systems</em> (Vol. 32). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf</a>
</div>
<div id="ref-戸田+2011" class="csl-entry">
戸田盛和, 斎藤信彦, 久保亮五, &amp; 橋爪夏樹. (2011). <em>統計物理学</em> (新装版, Vol. 5). 岩波書店. <a href="https://www.iwanami.co.jp/book/b259545.html">https://www.iwanami.co.jp/book/b259545.html</a>
</div>
<div id="ref-樺島祥介2002" class="csl-entry">
樺島祥介. (2002). <em>学習と情報の平均場理論</em> (Vol. 2). 岩波書店. <a href="https://www.iwanami.co.jp/book/b476277.html">https://www.iwanami.co.jp/book/b476277.html</a>
</div>
<div id="ref-深谷賢治1997" class="csl-entry">
深谷賢治. (1997). <em>「位相的場の理論」 集中講義ノート</em>. <a href="https://www.math.kyoto-u.ac.jp/~fukaya/">https://www.math.kyoto-u.ac.jp/~fukaya/</a>
</div>
<div id="ref-田崎晴明2008II" class="csl-entry">
田崎晴明. (2008). <em>統計力学II</em> (Vol. 38). 培風館.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="樺島祥介2002">(樺島祥介, 2002, p. 4)</span> に倣った．ここでは，「効率的な平均計算法の開発は統計力学ではエルゴード理論と同じくらい重要な意味を持つ」と，このような計算の問題を順問題と呼んでいる．そして，次のように続ける：「実のところ，歴史的に統計学は観測データからそれを生成した統計モデルを推定することで事実を説明する <strong>逆問題</strong> を中心に発展してきたという経緯があり，<strong>順問題</strong> 的な難しさが広く意識されはじめたのは計算機が発達し大規模な統計モデルが盛んに用いられるようになった比較的最近のことである．」↩︎</p></li>
<li id="fn2"><p>同講義録 <span class="citation" data-cites="深谷賢治1997">(深谷賢治, 1997, p. 12)</span> にて，平面幾何とは，例えば円とは <img src="https://latex.codecogs.com/png.latex?S%5E1%5Chookrightarrow%5Cmathbb%7BR%7D%5E2"> なる写像を調べるもの，<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E2"> 上の解析とは，<img src="https://latex.codecogs.com/png.latex?f(x,y)"> などの２変数関数を調べる，という例を挙げている．↩︎</p></li>
<li id="fn3"><p>主な目標が熱力学極限にあり，この極限において等価な予測を与えるモデルとして等価である，という意味である <span class="citation" data-cites="田崎晴明2008II">(田崎晴明, 2008, p. 333)</span> など．↩︎</p></li>
<li id="fn4"><p><span class="citation" data-cites="樺島祥介2002">(樺島祥介, 2002, pp. 4–5)</span>，<span class="citation" data-cites="戸田+2011">(戸田盛和 et al., 2011, p. 34)</span> など．↩︎</p></li>
<li id="fn5"><p>幾何光学における Fermat の原理から懐胎されていたアイデアであり，解析力学の最小作用の原理に代表され，同様の原理は量子力学，電磁気学など多くの分野で通用する．↩︎</p></li>
<li id="fn6"><p><span class="citation" data-cites="Chopin-Papaspiliopoulos20-SMC">(Chopin &amp; Papaspiliopoulos, 2020, p. 28)</span> など．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Opinion</category>
  <category>Life</category>
  <guid>https://162348.github.io/posts/2024/Life/AppliedMath.html</guid>
  <pubDate>Thu, 09 May 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Life/Images/AppliedMath.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Review/Roberts-Tweedie1996.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="概要" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="概要">概要</h2>
<section id="研究の立ち位置" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="研究の立ち位置">研究の立ち位置</h3>
<p>MALA は <span class="citation" data-cites="Besag1994">(Besag, 1994)</span> で提案され，<span class="citation" data-cites="Roberts-Tweedie1996">(Roberts &amp; Tweedie, 1996a)</span> で指数エルゴード性の必要十分条件が示されている．後の研究 <span class="citation" data-cites="Roberts-Rosenthal1998">(Roberts &amp; Rosenthal, 1998)</span> で最適スケーリングが論じられる．</p>
<div class="article-card-container">
  <div class="article-card">
    <a href="https://162348.github.io/posts/2024/Review/Roberts-Rosenthal1998.html" target="_blank">
      <img src="https://162348.github.io/posts/2024/Review/Roberts-Rosenthal1998.svg" alt="Article Image" class="article-image">
      <div class="article-content">
        <h3 class="article-title anchored">Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions</h3>
        <p class="article-description">Roberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの．</p>
      </div>
    </a>
  </div>
</div>
<p><span class="citation" data-cites="Roberts-Tweedie1996">(Roberts &amp; Tweedie, 1996a)</span> は <span class="citation" data-cites="Roberts-Tweedie1996MH">(Roberts &amp; Tweedie, 1996b)</span> とセットである．後者はランダムウォーク MH に対象を限定して，指数エルゴード性がどのような含意を持つかを検証している（積率も指数収束するための条件，中心極限定理など）．</p>
</section>
<section id="mala-とは" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="mala-とは">MALA とは</h3>
<p>MALA は，提案核を対象分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi"> から Langevin 拡散の形で構成する，problem-specific な Metropolis-Hastings 法であり，ランダムウォーク MH よりも速く収束することが期待される．<span class="citation" data-cites="Corenflos-Finke2024">(Corenflos &amp; Finke, 2024)</span> でも state-of-the-art と呼ばれているサンプリング法である．</p>
<p><span class="citation" data-cites="Fearnhead+2018-PDMC">(Fearnhead et al., 2018)</span> において，MALA は BPS と比較されている．モデルは AR(1) を用いており，低次元ではほとんど変わらないが，高次元では BPS の方が自己相関時間が５倍良いという結論が得られている．</p>
<p>MALA の他に，<span class="citation" data-cites="Neal1994">(Neal, 1994)</span> などの hybrid-Monte Carlo アルゴリズムも MALA と「提案核をよく考えている」という点で関係が深いが，本論文では扱わない．</p>
</section>
<section id="mala-のエルゴード性について" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="mala-のエルゴード性について">MALA のエルゴード性について</h3>
<p>Langevin 拡散 <img src="https://latex.codecogs.com/png.latex?%0AdL_t=dB_t+%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5Clog%5Cpi(L_t)dt%0A"> は，例えば１次元では，尾部確率が指数減衰するならば，指数エルゴード性を持つ．つまり，<img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%5C,%5Cpropto%5C,e%5E%7B-%5Cgamma%5Clvert%20x%5Crvert%5E%5Cbeta%7D%5C;(%5Cbeta%5Cge1)"> など．</p>
<p>このように，確率分布の裾の重さに影響されるという現象が普遍的である．</p>
<p>続いて，Langevin 拡散の離散化が，元の Langevin 拡散に収束するかどうかも別問題である．</p>
<p>最後に，Langevin 拡散の離散化を Metropolis 法によって修正したアルゴリズムである MALA が指数収束する条件を示す．</p>
</section>
</section>
<section id="導入" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="導入"><span class="header-section-number">1</span> 導入</h2>
<section id="mcmc-手法の指数エルゴード性について" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="mcmc-手法の指数エルゴード性について"><span class="header-section-number">1.1</span> MCMC 手法の指数エルゴード性について</h3>
<p><span class="citation" data-cites="Gelfand-Smith1990">(Gelfand &amp; Smith, 1990)</span> 以来，MH 法の利用が爆発している</p>
<blockquote class="blockquote">
<p>There has recently been a real explosion in the use of the Hastings and Metropolis algorithms</p>
</blockquote>
<p>乱歩 MH などは対象分布に依らずに実装できる汎用アルゴリズムであるが，対象分布の情報を取り入れて収束を高速にすることが考えられる．その例が MALA である．</p>
<p>Langevin 動力学は極めて優秀なモデリング手法であるが，シミュレーションのための離散化が入ることで，収束は遅くなるどころかそもそも収束性が失われることさえある．</p>
<p><span class="citation" data-cites="Neal1994">(Neal, 1994)</span> を引用しながら，hybrid Monte Carlo が Langevin アルゴリズムに似ていると触れながら，話が対称性と非対称性の話に変わる．</p>
<blockquote class="blockquote">
<p>However, it is worth remarking that often methods induced by non-reversible methods can be shown to converge more quickly than their reversible counterparts (see, for example, <span class="citation" data-cites="Sheu1991">(Sheu, 1991)</span>).</p>
</blockquote>
<p>しかし <span class="citation" data-cites="Sheu1991">(Sheu, 1991)</span> は <img src="https://latex.codecogs.com/png.latex?P_t"> の評価を上下からしているのみで，非対称性どころか収束の議論は全くしていない．議論に飛躍があるか，引用が間違っているかが考えられる．</p>
</section>
<section id="langevin-拡散について" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="langevin-拡散について"><span class="header-section-number">1.2</span> Langevin 拡散について</h3>
<p>本論文が研究する MH 法は，提案を Langevin 拡散により取り入れたものである．</p>
<p>密度 <img src="https://latex.codecogs.com/png.latex?%5Cpi"> が可微分であるとき，<img src="https://latex.codecogs.com/png.latex?%5Cnabla%5Clog%5Cpi"> が考えられる．これに対して，Langevin 拡散 <img src="https://latex.codecogs.com/png.latex?%5C%7BL_t%5C%7D%5Csubset%5Cmathbb%7BR%7D%5Ek"> とは <img src="https://latex.codecogs.com/png.latex?%0AdL_t=dW_t+%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5Clog%5Cpi(L_t)dt%0A"> と定義される．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cpi"> が十分滑らかであるとき，これはエルゴード性を持つ： <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CP%5Et_L(x,-)-%5Cpi%5C%7C_%5Cmathrm%7BTV%7D%5Cto0.%0A"></p>
<p>本論文では，この収束が指数レートで起こる場合，そして同様の積率収束も起こる場合に興味がある．</p>
</section>
<section id="指数収束する例" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="指数収束する例"><span class="header-section-number">1.3</span> 指数収束する例</h3>
<section id="次元クラス-mathcalebetagamma" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="次元クラス-mathcalebetagamma"><span class="header-section-number">1.3.1</span> １次元クラス <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D(%5Cbeta,%5Cgamma)"></h4>
<p><img src="https://latex.codecogs.com/png.latex?k=1"> とする．<img src="https://latex.codecogs.com/png.latex?%5Cpi%5Cin%5Cmathcal%7BE%7D(%5Cbeta,%5Cgamma)%5Csubset%20C%5E%5Cinfty(%5Cmathbb%7BR%7D)"> であるとは，可微分であり，加えてある <img src="https://latex.codecogs.com/png.latex?x_0%5Cin%5Cmathbb%7BR%7D"> と <img src="https://latex.codecogs.com/png.latex?%5Cgamma,%5Cbeta%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%5C,%5Cpropto%5C,e%5E%7B-%5Cgamma%5Clvert%20x%5Crvert%5E%5Cbeta%7D,%5Cquad%5Clvert%20x%5Crvert%5Cge%20x_0,%0A"> が成り立つことをいう．このとき， <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%5Clog%5Cpi(x)=-%5Cgamma%5Cbeta%5Cmathop%7B%5Cmathrm%7Bsgn%7D%7D(x)%5Clvert%20x%5Crvert%5E%7B%5Cbeta-1%7D,%5Cquad%5Clvert%20x%5Crvert%3Ex_0.%0A"></p>
<p>2.3 節にて，指数収束するための必要十分条件は <img src="https://latex.codecogs.com/png.latex?%5Cbeta%5Cge1"> であることを見る．この結論は乱歩 MH アルゴリズムと全く同様である <span class="citation" data-cites="Mengersen-Robert1996">(Mengersen &amp; Robert, 1996, p. 定理3.5)</span>．</p>
</section>
<section id="多次元指数分布族-mathcalp_m" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="多次元指数分布族-mathcalp_m"><span class="header-section-number">1.3.2</span> 多次元指数分布族 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D_m"></h4>
<p><span class="citation" data-cites="Roberts-Tweedie1996MH">(Roberts &amp; Tweedie, 1996b)</span> で乱歩 MH 法について考えたように，多次元の統計モデルとしては指数分布族を考えることとする．特に，次の場合を考える： <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(x)%5C,%5Cpropto%5C,e%5E%7B-p(x)%7D%5Cquad%20%5Clvert%20x%5Crvert%5Cge%20x_0,%0A"> ただし <img src="https://latex.codecogs.com/png.latex?p"> は <img src="https://latex.codecogs.com/png.latex?m"> 次の多項式で，次の表示を持つ： <img src="https://latex.codecogs.com/png.latex?%0Ap=p_m+q_%7Bm-1%7D.%0A"> <img src="https://latex.codecogs.com/png.latex?%5Cpi%5Cin%5Cmathcal%7BP%7D_m%5Csubset%20C%5E%5Cinfty(%5Cmathbb%7BR%7D%5Ek)"> とは，<img src="https://latex.codecogs.com/png.latex?p_m%5Cxrightarrow%7B%5Clvert%20x%5Crvert%5Cto%5Cinfty%7D%5Cinfty"> を満たすことをいう．特に，<img src="https://latex.codecogs.com/png.latex?m%5Cge2"> であることに注意．</p>
</section>
</section>
<section id="langevin-拡散-l_t-の離散近似" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="langevin-拡散-l_t-の離散近似"><span class="header-section-number">1.4</span> Langevin 拡散 <img src="https://latex.codecogs.com/png.latex?L_t"> の離散近似</h3>
<p>実際の実装で <img src="https://latex.codecogs.com/png.latex?L_t"> をそのまま用いることは出来ず，これを離散化することが必要である．</p>
<section id="ula-unadjusted-langevin-algorithm" class="level4" data-number="1.4.1">
<h4 data-number="1.4.1" class="anchored" data-anchor-id="ula-unadjusted-langevin-algorithm"><span class="header-section-number">1.4.1</span> ULA (unadjusted Langevin algorithm)</h4>
<blockquote class="blockquote">
<p>The unadjusted Langevin algorithm (ULA) is a discrete-time Markov chain <img src="https://latex.codecogs.com/png.latex?U_n"> which is the natural discretization of ordinary Langevin diffusion <img src="https://latex.codecogs.com/png.latex?L_t">.</p>
</blockquote>
<p><span class="citation" data-cites="Parisi1980">(Parisi, 1980)</span>, <span class="citation" data-cites="Grenander-Miller1994">(Grenander &amp; Miller, 1994)</span> で考えられたものである．</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU_n%5Csim%5Cmathrm%7BN%7D_k%5Cleft(U_%7Bn-1%7D+%5Cfrac%7Bh%7D%7B2%7D%5Cnabla%5Clog%5Cpi(U_%7Bn-1%7D),hI_k%5Cright)%0A"> によって構成できる． <span class="citation" data-cites="Besag1994">(Besag, 1994)</span> が指摘したように，元の <img src="https://latex.codecogs.com/png.latex?L_t"> と似た（しかしステップサイズ <img src="https://latex.codecogs.com/png.latex?h"> に依存する変換を受けた）不変分布を持つ．</p>
<div class="callout callout-style-default callout-caution callout-titled" title="例">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
例
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Cpi=%5Cmathrm%7BN%7D_1(0,1)"> であり，ステップサイズを <img src="https://latex.codecogs.com/png.latex?h=2"> とすると，<img src="https://latex.codecogs.com/png.latex?%5Cnabla%5Clog%5Cpi(x)">=-x$ であるから， <img src="https://latex.codecogs.com/png.latex?%0AU_%7Bn-1%7D+%5Cfrac%7Bh%7D%7B2%7D%5Cnabla%5Clog%5Cpi(U_%7Bn-1%7D)=U_%7Bn-1%7D-%5Cfrac%7B2%7D%7B2%7D%5Cfrac%7BU_%7Bn-1%7D%7D%7B1%7D=0%0A"> より，<img src="https://latex.codecogs.com/png.latex?U_n%5Csim%5Cmathrm%7BN%7D_1(0,2)"> となる．</p>
</div>
</div>
</div>
</section>
<section id="mala-metropolis-adjusted-langevin-algorithm" class="level4" data-number="1.4.2">
<h4 data-number="1.4.2" class="anchored" data-anchor-id="mala-metropolis-adjusted-langevin-algorithm"><span class="header-section-number">1.4.2</span> MALA (Metropolis-Adjusted Langevin Algorithm)</h4>
<p><span class="citation" data-cites="Besag1994">(Besag, 1994)</span> に従い，修正を加える．<img src="https://latex.codecogs.com/png.latex?U_n"> を提案として，Metropolis-Hastings 法を実行するのである．</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aq(M_%7Bn-1%7D,-):=%5Cmathrm%7BN%7D_k%5Cleft(M_%7Bn-1%7D+%5Cfrac%7B1%7D%7B2%7Dh%5Cnabla%5Clog%5Cpi(M_%7Bn-1%7D),hI_k%5Cright)%0A"> を提案核とし， <img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(M_%7Bn-1%7D,U_n):=1%5Cland%5Cfrac%7B%5Cpi(U_n)q(U_n,M_%7Bn-1%7D)%7D%7B%5Cpi(M_%7Bn-1%7D)q(M_%7Bn-1%7D,U_n)%7D%0A"> を採択確率とする．</p>
<p>この場合，必ずエルゴード性が成り立つ．<span class="citation" data-cites="Roberts-Tweedie1996MH">(Roberts &amp; Tweedie, 1996b)</span> の結論と同様，<img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約かつ周期的だからである．<sup>1</sup> 特に，殆ど至る所の <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathbb%7BR%7D%5Ek"> について， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CP%5En_M(x,-)-%5Cpi%5C%7C_%5Cmathrm%7BTV%7D%5Cto0.%0A"> また，本論文が提示する指数収束の条件の下では，任意の <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathbb%7BR%7D%5Ek"> で起こる．</p>
<p>ULA が推移的であるとき，MALA も指数エルゴード性を持たないことが判る．だが，これは尾部確率が指数よりも重い場合にしか起こらない．</p>
</section>
<section id="malta-metropolis-adjusted-langevin-truncated-algorithm" class="level4" data-number="1.4.3">
<h4 data-number="1.4.3" class="anchored" data-anchor-id="malta-metropolis-adjusted-langevin-truncated-algorithm"><span class="header-section-number">1.4.3</span> MALTA (Metropolis-Adjusted Langevin Truncated Algorithm)</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0AT_n%5Csim%5Cmathrm%7BN%7D_k%5Cbiggr(M_%7Bn-1%7D+R(M_%7Bn-1%7D),hI_k%5Cbiggl)%0A"> <img src="https://latex.codecogs.com/png.latex?%0AR(x):=%5Cfrac%7BD%5Cnabla%5Clog%5Cpi(x)%7D%7B2(D%5Clor%5Clvert%5Cnabla%5Clog%5Cpi(x)%5Crvert)%7D.%0A"> を提案とし，MH 法を適用する．</p>
<p>MALTA は大変ロバストで，指数収束が起こりやすい．</p>
</section>
</section>
</section>
<section id="langevin-拡散アルゴリズムの指数収束" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="langevin-拡散アルゴリズムの指数収束"><span class="header-section-number">2</span> Langevin 拡散アルゴリズムの指数収束</h2>
<section id="一般の収束結果" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="一般の収束結果"><span class="header-section-number">2.1</span> 一般の収束結果</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0AdL_t=dW_t+%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5Clog%5Cpi(L_t)dt%0A"></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="定理2.1（Langevin 拡散のエルゴード性）">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理2.1（Langevin 拡散のエルゴード性）
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Cnabla%5Clog%5Cpi"> は <img src="https://latex.codecogs.com/png.latex?C%5E1(%5Cmathbb%7BR%7D%5Ek)">-級で，<sup>2</sup> ある <img src="https://latex.codecogs.com/png.latex?N,a,b%3E0"> が存在して <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%5Clog%5Cpi(x)%5Ccdot%20x%5Cle%20a%5Clvert%20x%5Crvert%5E2+b,%5Cqquad%5Clvert%20x%5Crvert%3EN,%0A"> を満たすとする．このとき，Langevin 拡散 <img src="https://latex.codecogs.com/png.latex?%5C%7BL_t%5C%7D"> について次が成り立つ：</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?L"> は爆発的でなく，<img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約で，周期的で，強 Feller であり，任意のコンパクト集合は小集合である．</li>
<li><img src="https://latex.codecogs.com/png.latex?L"> は <img src="https://latex.codecogs.com/png.latex?%5Cpi">-不変であり，任意の <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathbb%7BR%7D%5Ek"> について <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CP%5Et_L(x,-)-%5Cpi%5C%7C_%5Cmathrm%7BTV%7D%5Cto0.%0A"></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Clvert%20L_t%5Crvert"> と OU 過程を比較することで，<img src="https://latex.codecogs.com/png.latex?%5Clvert%20L_t%5Crvert"> が爆発しないことが判る．</p>
<p>続いて，ドリフト係数が局所有界であるため，<span class="citation" data-cites="Bhattacharya1978">(Bhattacharya, 1978)</span> と同様の議論により強 Feller である．加えて，<img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約であることも判る．<sup>3</sup></p>
<blockquote class="blockquote">
<p>Theorem 2.1. <em>If, in addition to the hypothesis (A), <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D(-)"> and <img src="https://latex.codecogs.com/png.latex?b_i(-)"> are bounded on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ek">, then for each <img src="https://latex.codecogs.com/png.latex?x"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ek"> there exists a unique probability measure <img src="https://latex.codecogs.com/png.latex?P_x"> on <img src="https://latex.codecogs.com/png.latex?(%5COmega',%5Cmathcal%7BM%7D')"> such that (i) <img src="https://latex.codecogs.com/png.latex?P_x(X_0=x)=1">, (ii) for every bounded real <img src="https://latex.codecogs.com/png.latex?f"> on <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ek"> having bounded continuous first and second order derivatives, the process <img src="https://latex.codecogs.com/png.latex?f(X_t)-%5Cint%5Et_0Lf(X_s)ds"> is a martingale under <img src="https://latex.codecogs.com/png.latex?P_x">. Further, (a) <img src="https://latex.codecogs.com/png.latex?X"> is strong Markov and strong Feller, and (b) support of <img src="https://latex.codecogs.com/png.latex?P_x"> is <img src="https://latex.codecogs.com/png.latex?%5COmega_x':=%5Cleft%5C%7B%5Comega%5Cin%5COmega%5Cmid%5Comega(0)=x%5Cright%5C%7D">. <span class="citation" data-cites="Bhattacharya1978">(Bhattacharya, 1978)</span></em></p>
</blockquote>
<p>任意の <img src="https://latex.codecogs.com/png.latex?%5Cell_k">-正集合 <img src="https://latex.codecogs.com/png.latex?A%5Csubset%5Cmathbb%7BR%7D%5Ek"> に対して， <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%5Cwidetilde%7BA%7D:=%5Cleft%5C%7B%5Comega%5Cin%5COmega_x'%5Cmid%20%5Comega(t)%5Cin%20A%5Cright%5C%7D%0A%20%20%20"> は <img src="https://latex.codecogs.com/png.latex?P_x">-正集合である．実際，<img src="https://latex.codecogs.com/png.latex?A"> が <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5Ek"> 上の開集合を含むために <img src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7BA%7D"> は <img src="https://latex.codecogs.com/png.latex?%5COmega_x'"> 上の開集合を含むので，<img src="https://latex.codecogs.com/png.latex?P_x">-零であったら <img src="https://latex.codecogs.com/png.latex?P_x"> の台が <img src="https://latex.codecogs.com/png.latex?%5COmega_x'"> 全体であることに矛盾する．これより，<img src="https://latex.codecogs.com/png.latex?L_t"> は <img src="https://latex.codecogs.com/png.latex?A"> に到達可能である．よって <img src="https://latex.codecogs.com/png.latex?L_t"> は <img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約．</p>
<p>（強）Feller かつ <img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約ならば，任意のコンパクト集合は小集合である <span class="citation" data-cites="Tweedie1994">(Tweedie, 1994, p. 定理5.1)</span>．<sup>4</sup></p>
<p>非周期性は，任意のスケルトンも <img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約であることから従う．<sup>5</sup></p></li>
<li><p><span class="citation" data-cites="Ikeda-Watanabe1981">(Ikeda &amp; Watanabe, 1981, p. 5.4節)</span> により，<img src="https://latex.codecogs.com/png.latex?%5Cpi"> は <img src="https://latex.codecogs.com/png.latex?L"> の不変分布であることが，生成作用素 <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20A_Lf(x)=%5Cleft(%5Cfrac%7B1%7D%7B2%7D%5Cnabla%5Clog%5Cpi(x)%5Cright)%5Ccdot%5Cnabla%20f(x)+%5Cfrac%7B1%7D%7B2%7D%5Cmathop%7B%7D%5C!%5Cmathbin%5Cbigtriangleup%20f(x)%0A%20%20%20"> <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20f%5Cin%20C%5E2(%5Cmathbb%7BR%7D%5Ek)%0A%20%20%20"> に関して，<img src="https://latex.codecogs.com/png.latex?(%5Cpi%7CA_Lf)=0%5C;(f%5Cin%20C%5E2(%5Cmathbb%7BR%7D%5Ek))"> を満たすために従う．<img src="https://latex.codecogs.com/png.latex?C%5E2(%5Cmathbb%7BR%7D%5Ek)"> は，爆発しない拡散過程の生成作用素の核をなす．<sup>6</sup></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cell_k">-既約な Makrov 過程であって不変確率分布を持つから，再帰的ではある．<span class="citation" data-cites="Meyn-Tweedie2009">(S. Meyn &amp; Tweedie, 2009, p. 172)</span>定理8.0.1．これは，<img src="https://latex.codecogs.com/png.latex?%5Cpsi">-既約な Markov 連鎖は再帰的であるか推移的であるかの２択だからである<span class="citation" data-cites="Tweedie1994">(Tweedie, 1994, p. 定理2.3)</span>．</p>
<p><strong>また連続な過程であるから，これは Harris 再帰性を意味する．</strong></p>
<p>不変確率分布を持つ非周期的な Harris 再帰的な Markov 連鎖であるから，全変動収束が従う <span class="citation" data-cites="Meyn-Tweedie1993">(S. P. Meyn &amp; Tweedie, 1993, p. 定理6.1)</span>．</p>
<blockquote class="blockquote">
<p><em>Proposition 6.1</em>. Suppose that <img src="https://latex.codecogs.com/png.latex?%5CPhi"> is positive Harris recurrent, and that some skeleton chain is irreducible. If <img src="https://latex.codecogs.com/png.latex?C"> is petite, then there exists a constant <img src="https://latex.codecogs.com/png.latex?T%3E0"> and a non-trivial measure <img src="https://latex.codecogs.com/png.latex?%5Cmu"> such that <img src="https://latex.codecogs.com/png.latex?P%5Es(x,-)%5Cge%5Cmu(-)%5C;(s%5Cge%20T,x%5Cin%20C)">. <span class="citation" data-cites="Meyn-Tweedie1993">(S. P. Meyn &amp; Tweedie, 1993)</span>_</p>
</blockquote></li>
</ol>
</div>
</div>
</div>
</section>
<section id="l_t-の指数エルゴード性" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="l_t-の指数エルゴード性"><span class="header-section-number">2.2</span> <img src="https://latex.codecogs.com/png.latex?L_t"> の指数エルゴード性</h3>
<p><img src="https://latex.codecogs.com/png.latex?V:%5Cmathbb%7BR%7D%5Ek%5Cto%5B1,%5Cinfty)"> に対して，<strong><img src="https://latex.codecogs.com/png.latex?V">-一様エルゴード的</strong> であるとは，任意の <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathbb%7BR%7D%5Ek"> に対して，ある <img src="https://latex.codecogs.com/png.latex?R%3E0,%5Crho%5Cin(0,1)"> が存在して <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CP%5Et_L(x,-)-%5Cpi%5C%7C_V%5Cle%20V(x)R%5Crho%5Et,%5Cqquad%20t%5Cge0,%0A"> が成り立つことをいう．ただし， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CA%5C%7C_V:=%5Csup_%7B%5Clvert%20f%5Crvert%5Cle%20V%7D%5Cint_%7B%5Cmathbb%7BR%7D%5Ek%7Df(x)A(dx).%0A"></p>
<p><span class="citation" data-cites="Meyn-Tweedie2009">(S. Meyn &amp; Tweedie, 2009, pp. 336–337)</span> 第 14 章にいう <img src="https://latex.codecogs.com/png.latex?f">-エルゴード性に当たる概念であることに注意．<img src="https://latex.codecogs.com/png.latex?V">-一様エルゴード性は第 16 章で別の意味に使われている．</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="定理2.2（Langevin 拡散の指数エルゴード性）">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理2.2（Langevin 拡散の指数エルゴード性）
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Cnabla%5Clog%5Cpi"> は <img src="https://latex.codecogs.com/png.latex?C%5E1(%5Cmathbb%7BR%7D%5Ek)">-級で，ある <img src="https://latex.codecogs.com/png.latex?N,a,b%3E0"> が存在して <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla%5Clog%5Cpi(x)%5Ccdot%20x%5Cle%20a%5Clvert%20x%5Crvert%5E2+b,%5Cqquad%5Clvert%20x%5Crvert%3EN,%0A"> を満たすとする．このとき，Langevin 拡散 <img src="https://latex.codecogs.com/png.latex?%5C%7BL_t%5C%7D"> について次が成り立つ：</p>
<ol type="1">
<li>任意の <img src="https://latex.codecogs.com/png.latex?V%5Cin%20C%5E2(%5Cmathbb%7BR%7D%5Ek)"> であって，<img src="https://latex.codecogs.com/png.latex?V%5Cge1"> かつ，ある <img src="https://latex.codecogs.com/png.latex?b,c%3E0"> と非空コンパクト集合 <img src="https://latex.codecogs.com/png.latex?C%5Coverset%7B%5Ctextrm%7Bcpt%7D%7D%7B%5Csubset%7D%5Cmathbb%7BR%7D%5Ek"> とについて <img src="https://latex.codecogs.com/png.latex?%0AA_LV%5Cle-cV+b1_C%0A"> を満たすものについて，<img src="https://latex.codecogs.com/png.latex?V">-一様エルゴード的である．</li>
<li><img src="https://latex.codecogs.com/png.latex?L"> が指数エルゴード的ならば，ある非空コンパクト集合 <img src="https://latex.codecogs.com/png.latex?C%5Coverset%7B%5Ctextrm%7Bcpt%7D%7D%7B%5Csubset%7D%5Cmathbb%7BR%7D%5Ek"> について，ある <img src="https://latex.codecogs.com/png.latex?%5Ckappa%3E1,%5Cdelta%3E0"> が存在して，任意のスタート地点 <img src="https://latex.codecogs.com/png.latex?x%5Cin%20%5Cmathbb%7BR%7D%5Ek"> について， <img src="https://latex.codecogs.com/png.latex?%0A%5Csup_%7Bx%5Cin%20C%7D%5Coperatorname%7BE%7D%5B%5Ckappa%5E%7B%5Ctau_C%5E%5Cdelta%7D%5D%3C%5Cinfty,%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Ctau%5E%5Cdelta_C:=%5Cinf%5Cleft%5C%7Bt%5Cge%5Cdelta%5Cmid%20L_t%5Cin%20C%5Cright%5C%7D.%0A"></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>非空コンパクト集合 <img src="https://latex.codecogs.com/png.latex?C%5Coverset%7B%5Ctextrm%7Bcpt%7D%7D%7B%5Csubset%7D%5Cmathbb%7BR%7D%5Ek"> は小集合であるから，これは成り立つ．<span class="citation" data-cites="Meyn-Tweedie2009">(S. Meyn &amp; Tweedie, 2009, pp. 336–337)</span> 定理 14.0.1 が Markov 連鎖に関して与えている消息である．</p></li>
<li><p>任意の <img src="https://latex.codecogs.com/png.latex?%5Cdelta%3E0"> について，<img src="https://latex.codecogs.com/png.latex?%5Cdelta">-スケルトンも幾何エルゴード的である．この <img src="https://latex.codecogs.com/png.latex?%5Cdelta">-スケルトンに関する <img src="https://latex.codecogs.com/png.latex?C"> への到着時刻は <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E%5Cdelta_C"> よりも必ず大きくなる．</p>
<p><img src="https://latex.codecogs.com/png.latex?L_t"> は強 Feller であるから，<img src="https://latex.codecogs.com/png.latex?C"> は <img src="https://latex.codecogs.com/png.latex?%5Cdelta">-スケルトンに関しても小集合である．あとは，Markov 連鎖に関する幾何エルゴード定理より従う．</p></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="定理2.3（Langevin 拡散の指数エルゴード性）">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理2.3（Langevin 拡散の指数エルゴード性）
</div>
</div>
<div class="callout-body-container callout-body">
<p>ある <img src="https://latex.codecogs.com/png.latex?S%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%5Clvert%20x%5Crvert%5Cge%20S"> 上で <img src="https://latex.codecogs.com/png.latex?%5Cpi"> は有界であるとする．加えて，ある <img src="https://latex.codecogs.com/png.latex?d%5Cin(0,1)"> が存在して <img src="https://latex.codecogs.com/png.latex?%0A%5Climinf_%7B%5Clvert%20x%5Crvert%5Cto%5Cinfty%7D(1-d)%5Clvert%5Cnabla%5Clog%5Cpi(x)%5Crvert%5E2+%5Cnabla%5E2%5Clog%5Cpi(x)%3E0%0A"> を満たすならば，<img src="https://latex.codecogs.com/png.latex?%5C%7BL_t%5C%7D"> は指数エルゴード的である．</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>上の定理の 1. のドリフト条件を示せば良い．<img src="https://latex.codecogs.com/png.latex?V=%5Cpi%5E%7B-d%7D%5C;(0%3Cd%3C1)"> と取れば良い．このとき， <img src="https://latex.codecogs.com/png.latex?%0A2A_LV=V%5Cbiggr(%5Clvert%5Cnabla%5Clog%5Cpi%5Crvert%5E2(d%5E2-d)-d%5Cnabla%5E2%5Clog%5Cpi%5Cbiggl).%0A"></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="定理2.4（Langevin 拡散が指数エルゴード性を失うとき）">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理2.4（Langevin 拡散が指数エルゴード性を失うとき）
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Clvert%5Cnabla%5Clog%5Cpi(x)%5Crvert%5Cto0"> ならば，<img src="https://latex.codecogs.com/png.latex?%5C%7BL_t%5C%7D"> は指数エルゴード的でない．</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>上の定理の 2. と矛盾させれば良い．幾何エルゴード的だと仮定し，コンパクトな Kendall 集合 <img src="https://latex.codecogs.com/png.latex?C%5Coverset%7B%5Ctextrm%7Bcpt%7D%7D%7B%5Csubset%7D%5Cmathbb%7BR%7D%5Ek"> が存在するとする： <img src="https://latex.codecogs.com/png.latex?%0A%5Csup_%7Bx%5Cin%20C%7D%5Coperatorname%7BE%7D%5B%5Ckappa%5E%7B%5Ctau%5E%5Cdelta_C%7D%5D%3C%5Cinfty.%0A"> ある <img src="https://latex.codecogs.com/png.latex?R%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%0A%5Clvert%5Cnabla%5Clog%5Cpi(x)%5Crvert%5Cle2(%5Clog%5Ckappa)%5E%7B1/2%7D%5Cquad(%5Clvert%20x%5Crvert%5Cge%20R).%0A"> このとき，<img src="https://latex.codecogs.com/png.latex?S%5Cge%5Csup_%7Bx%5Cin%20C%7D%5Clvert%20x%5Crvert%5Clor%20R"> を取り，<img src="https://latex.codecogs.com/png.latex?%5Clvert%20y%5Crvert%5Cge2S"> を満たす地点 <img src="https://latex.codecogs.com/png.latex?y%5Cin%5Cmathbb%7BR%7D%5Ek"> からスタートしたとすると，<img src="https://latex.codecogs.com/png.latex?Z_t:=%5Clvert%20L_t%5Crvert"> は，<img src="https://latex.codecogs.com/png.latex?a(x)%3E-(%5Clog%5Ckappa)%5E%7B1/2%7D%5C;(%5Clvert%20x%5Crvert%3ES)"> を満たす係数について <img src="https://latex.codecogs.com/png.latex?%0AdZ_t=dW_t+a(L_t)dt%0A"> を満たす．<img src="https://latex.codecogs.com/png.latex?B_t:=W_t-(%5Clog%5Ckappa)%5E%7B1/2%7Dt"> とし，それぞれの過程の <img src="https://latex.codecogs.com/png.latex?S"> への到着時刻 <img src="https://latex.codecogs.com/png.latex?%5Csigma(Z),%5Csigma(B)"> を考えると，<img src="https://latex.codecogs.com/png.latex?%5Csigma(Z)%5Cge%5Csigma(B)%5C;%5C;%5Ctext%7Ba.s.%7D"> が成り立つ．このとき， <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%5Coperatorname%7BP%7D_y%5B%5Ctau_C%3Et%5D&amp;%5Cge%5Coperatorname%7BP%7D_y%5B%5Csigma(Z)%3Et%5D%5C%5C%0A%20%20&amp;%5Cge%5Coperatorname%7BP%7D_y%5B%5Csigma(B)%3Et%5D%5C%5C%0A%20%20&amp;%5Cge%5CPhi%5Cleft(%5Cfrac%7B-S+t(%5Clog%5Ckappa)%5E%7B1/2%7D%7D%7B%5Csqrt%7Bt%7D%7D%5Cright)-e%5E%7B2S(%5Clog%5Ckappa)%5E%7B1/2%7D%7D%5CPhi%5Cleft(%5Cfrac%7B-S-t(%5Clog%5Ckappa)%5E%7B1/2%7D%7D%7B%5Csqrt%7Bt%7D%7D%5Cright).%0A%5Cend%7Balign*%7D"> 最後の式は Bachelier-Lévy の公式による．<img src="https://latex.codecogs.com/png.latex?%5Csigma(B)"> の密度を <img src="https://latex.codecogs.com/png.latex?f"> とすると， <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Clog%20f(t)%7D%7Bt%7D%5Cto-%5Cfrac%7B%5Clog%5Ckappa%7D%7B2%7D.%0A"> これは矛盾を起こすらしい．</p>
</div>
</div>
</div>
</section>
<section id="指数収束する例-1" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="指数収束する例-1"><span class="header-section-number">2.3</span> 指数収束する例</h3>
<section id="次元クラス-mathcalebetagamma-1" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="次元クラス-mathcalebetagamma-1"><span class="header-section-number">2.3.1</span> １次元クラス <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D(%5Cbeta,%5Cgamma)"></h4>
</section>
<section id="多次元指数分布族-mathcalp_m-1" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="多次元指数分布族-mathcalp_m-1"><span class="header-section-number">2.3.2</span> 多次元指数分布族 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D_m"></h4>
</section>
</section>
</section>
<section id="ula-アルゴリズム" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="ula-アルゴリズム"><span class="header-section-number">3</span> ULA アルゴリズム</h2>
</section>
<section id="mala-アルゴリズム" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="mala-アルゴリズム"><span class="header-section-number">4</span> MALA アルゴリズム</h2>
</section>
<section id="結論" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="結論"><span class="header-section-number">5</span> 結論</h2>
</section>
<section id="references" class="level2" data-number="6">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">6 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Besag1994" class="csl-entry">
Besag, J. E. (1994). Comments on <span>“representations of knowledge in complex systems”</span> by u. Grenander and MI miller. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>56</em>(4), 591–592. <a href="https://www.jstor.org/stable/2346184">https://www.jstor.org/stable/2346184</a>
</div>
<div id="ref-Bhattacharya1978" class="csl-entry">
Bhattacharya, R. N. (1978). <span class="nocase">Criteria for Recurrence and Existence of Invariant Measures for Multidimensional Diffusions</span>. <em>The Annals of Probability</em>, <em>6</em>(4), 541–553. <a href="https://doi.org/10.1214/aop/1176995476">https://doi.org/10.1214/aop/1176995476</a>
</div>
<div id="ref-Corenflos-Finke2024" class="csl-entry">
Corenflos, A., &amp; Finke, A. (2024). <em>Particle-MALA and particle-mGRAD: Gradient-based MCMC methods for high-dimensional state-space models</em>. <a href="https://arxiv.org/abs/2401.14868">https://arxiv.org/abs/2401.14868</a>
</div>
<div id="ref-Fearnhead+2018-PDMC" class="csl-entry">
Fearnhead, P., Bierkens, J., Pollock, M., &amp; Roberts, G. O. (2018). Piecewise deterministic markov processes for continuous-time monte carlo. <em>Statistical Science</em>, <em>33</em>(3), 386–412. <a href="https://www.jstor.org/stable/26771007">https://www.jstor.org/stable/26771007</a>
</div>
<div id="ref-Gelfand-Smith1990" class="csl-entry">
Gelfand, A. E., &amp; Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal densities. <em>Journal of the American Statistical Association</em>, <em>85</em>(410), 398–409. <a href="https://doi.org/10.2307/2289776">https://doi.org/10.2307/2289776</a>
</div>
<div id="ref-Grenander-Miller1994" class="csl-entry">
Grenander, U., &amp; Miller, M. I. (1994). Representations of knowledge in complex systems. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>56</em>(4), 549–603. <a href="http://www.jstor.org/stable/2346184">http://www.jstor.org/stable/2346184</a>
</div>
<div id="ref-Ikeda-Watanabe1981" class="csl-entry">
Ikeda, N., &amp; Watanabe, S. (1981). <em>Stochastic differential equations and diffusion processes</em> (Vol. 24). Elsevier. <a href="https://doi.org/10.1016/S0924-6509(08)70217-4">https://doi.org/10.1016/S0924-6509(08)70217-4</a>
</div>
<div id="ref-Mengersen-Robert1996" class="csl-entry">
Mengersen, K. L., &amp; Robert, C. P. (1996). Testing for mixtures: A bayesian entropic approach. <em>Bayesian Statistics 5: Proceedings of the Fifth Valencia International Meetings</em>, 255–276. <a href="https://academic.oup.com/book/54042/chapter-abstract/422209682">https://academic.oup.com/book/54042/chapter-abstract/422209682</a>
</div>
<div id="ref-Meyn-Tweedie1993" class="csl-entry">
Meyn, S. P., &amp; Tweedie, R. L. (1993). Stability of markovian processes II: Continuous-time processes and sampled chains. <em>Advances in Applied Probability</em>, <em>25</em>(3), 487–517. <a href="http://www.jstor.org/stable/1427521">http://www.jstor.org/stable/1427521</a>
</div>
<div id="ref-Meyn-Tweedie2009" class="csl-entry">
Meyn, S., &amp; Tweedie, R. L. (2009). <em>Markov chains and stochastic stability</em>. Cambridge University Press. <a href="https://www.cambridge.org/core/books/markov-chains-and-stochastic-stability/E2B82BFB409CD2F7D67AFC5390C565EC">https://www.cambridge.org/core/books/markov-chains-and-stochastic-stability/E2B82BFB409CD2F7D67AFC5390C565EC</a>
</div>
<div id="ref-Neal1994" class="csl-entry">
Neal, R. M. (1994). An improved acceptance procedure for the hybrid monte carlo algorithm. <em>Journal of Computational Physics</em>, <em>111</em>(1), 194–203. <a href="https://doi.org/10.1006/jcph.1994.1054">https://doi.org/10.1006/jcph.1994.1054</a>
</div>
<div id="ref-Parisi1980" class="csl-entry">
Parisi, G. (1980). A sequence of approximated solutions to the s-k model for spin glasses. <em>Journal of Physics A: Mathematical and General</em>, <em>13</em>(4), L115. <a href="https://doi.org/10.1088/0305-4470/13/4/009">https://doi.org/10.1088/0305-4470/13/4/009</a>
</div>
<div id="ref-Roberts-Rosenthal1998" class="csl-entry">
Roberts, G. O., &amp; Rosenthal, J. S. (1998). Optimal scaling of discrete approximations to langevin diffusions. <em>Journal of the Royal Statistical Society. Series B (Statistical Methodology)</em>, <em>60</em>(1), 255–268. <a href="http://www.jstor.org/stable/2985986">http://www.jstor.org/stable/2985986</a>
</div>
<div id="ref-Roberts-Tweedie1996" class="csl-entry">
Roberts, G. O., &amp; Tweedie, R. L. (1996a). Exponential convergence of langevin distributions and their discrete approximations. <em>Bernoulli</em>, <em>2</em>(4), 341–363. <a href="https://www.jstor.org/stable/3318418">https://www.jstor.org/stable/3318418</a>
</div>
<div id="ref-Roberts-Tweedie1996MH" class="csl-entry">
Roberts, G. O., &amp; Tweedie, R. L. (1996b). Geometric convergence and central limit theorems for multidimensional hastings and metropolis algorithms. <em>Biometrika</em>, <em>83</em>(1), 95–110. <a href="http://www.jstor.org/stable/2337435">http://www.jstor.org/stable/2337435</a>
</div>
<div id="ref-Sheu1991" class="csl-entry">
Sheu, S.-J. (1991). Some estimates of the transition density of a nondegenerate diffusion markov process. <em>The Annals of Probability</em>, <em>19</em>(2), 538–561. <a href="http://www.jstor.org/stable/2244362">http://www.jstor.org/stable/2244362</a>
</div>
<div id="ref-Tweedie1994" class="csl-entry">
Tweedie, R. L. (1994). Topological conditions enabling use of harris methods in discrete and continuous time. <em>Acta Applicandae Mathematica</em>, <em>34</em>(1), 175–188. <a href="https://doi.org/10.1007/BF00994264">https://doi.org/10.1007/BF00994264</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Doob の定理から，で良くないか？↩︎</p></li>
<li id="fn2"><p><img src="https://latex.codecogs.com/png.latex?%5Clog%5Cpi"> が <img src="https://latex.codecogs.com/png.latex?C%5E1">-級，の間違いでは？↩︎</p></li>
<li id="fn3"><p>これは，Langevin 拡散の分布は <img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7B%5Comega%5Cin%20C(%5Cmathbb%7BR%7D_+;%5Cmathbb%7BR%7D%5Ek)%5Cmid%5Comega(0)=x%5Cright%5C%7D"> 全体を台に持つから，<img src="https://latex.codecogs.com/png.latex?%5Cell_k"> と同値な分布を持つことになることが<span class="citation" data-cites="Bhattacharya1978">(Bhattacharya, 1978)</span> 定理2.1で触れられている．すると，<img src="https://latex.codecogs.com/png.latex?%5Cell_k">-非零集合は必ず到達可能である．↩︎</p></li>
<li id="fn4"><p><span class="citation" data-cites="Meyn-Tweedie2009">(S. Meyn &amp; Tweedie, 2009, p. 124)</span> 第６章がこのテーマを扱っている．定理6.0.1の(iii)は <img src="https://latex.codecogs.com/png.latex?%5Cpsi">-既約な Feller Markov 過程が <img src="https://latex.codecogs.com/png.latex?(%5Cmathrm%7Bsupp%7D%5C;%5Cpsi)%5E%5Ccirc%5Cne%5Cemptyset"> であるとき，<img src="https://latex.codecogs.com/png.latex?T">-連鎖であること，(ii)は <img src="https://latex.codecogs.com/png.latex?%5Cpsi">-既約な <img src="https://latex.codecogs.com/png.latex?T">-連鎖は任意のコンパクト集合を petite にすることを述べている．<span class="citation" data-cites="Tweedie1994">(Tweedie, 1994, p. 定理5.1)</span> は連続過程について述べているが結局「<span class="citation" data-cites="Meyn-Tweedie2009">(S. Meyn &amp; Tweedie, 2009)</span>と同様に示せる」としか言っていない．最後に，既約かつ非周期的ならば，任意の petite 集合は小さい．↩︎</p></li>
<li id="fn5"><p><img src="https://latex.codecogs.com/png.latex?%5Cell_k"> と <img src="https://latex.codecogs.com/png.latex?L_t"> の分布が同値であるから，任意の <img src="https://latex.codecogs.com/png.latex?%5Cell_k">-正集合には，任意のスケルトンが到達可能であるはずである．このことは周期性を破る．↩︎</p></li>
<li id="fn6"><p>分布を定める (distribution-determining class)という言い方をしている．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Review</category>
  <guid>https://162348.github.io/posts/2024/Review/Roberts-Tweedie1996.html</guid>
  <pubDate>Mon, 22 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Review/Roberts-Tweedie1996.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>Duane+ (1987) Hybrid Monte Carlo</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Review/Duane+1987.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="概要と背景" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="概要と背景"><span class="header-section-number">1</span> 概要と背景</h2>
<section id="hmc-とは" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="hmc-とは"><span class="header-section-number">1.1</span> HMC とは</h3>
<p>hybrid Monte Carlo とは，MD と MC（＝Metropolis 法） の融合を指す．</p>
<p>すなわち，粒子を動かして，これを棄却手続きによって正準集団を作る MCMC 法を指すが，粒子の動かし方＝提案核を運動論から構成するというのである．<sup>1</sup></p>
<p>そもそも著者は <span class="citation" data-cites="Duane1985">(Duane, 1985)</span> において，場の量子論をシミュレーションするための “hybrid algorithm” を提案していた．これは第三の量子化と呼ばれる <span class="citation" data-cites="Parisi-Wu1981">(Parisi &amp; Wu, 1981)</span> の確率過程量子化に基づく Langevin algorithm と 小正準法（量子系に対する MD 法）のいいとこ取りをする確率的アルゴリズムである．</p>
<p>ここからさらに Monte Carlo を導入し，「MD を提案分布にする」という発想の転換がある．正確には，任意の Hamilton 力学系を提案分布にとっても，Metropolis 法が使えるということを示唆したのである．この Hamilton 力学系を正確に取ると，これは hybrid algorithm になる（古典系に対しては MD に一致するだろう）が，必ずしも正確に取る必要はないのである．</p>
</section>
<section id="概要" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="概要"><span class="header-section-number">1.2</span> 概要</h3>
<p><span class="citation" data-cites="Duane+1987">(Duane et al., 1987)</span> は格子上の場の理論における数値シミュレーション法として提案している．格子ゲージ理論は量子色力学で扱われる模型である．</p>
<ul>
<li>大きなステップサイズを用いても離散化誤差がない</li>
<li>フェルミオン自由度を含む量子色力学系のシミュレーションに有効</li>
</ul>
<p>である点が abstract で触れられている．</p>
</section>
<section id="導入" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="導入"><span class="header-section-number">1.3</span> 導入</h3>
<p>フェルミオン自由度がある系では，“Grassmann nature of the fermions” を除去するためにまず積分をして有効作用のみを取り出し，残りのボゾンのみを考えるが，このときに非常に遠距離な（非局所的な）相互作用になってしまう．</p>
<p>従来法には次の２つがある：</p>
<ol type="1">
<li>exact / entire Monte Carlo：ボゾンの局所的なアップデートは系の全体の状態をシミュレーションしないとわからないから，nested Monte Carlo ともいうべきサブルーチンを回す必要がある．pseudofermion を導入して，有効作用の変化を効率的に計算し，これを用いて元々のボゾン場をアップデートする．要は棄却のステップが大変に高価ということだろうか？</li>
<li>運動方程式の計算：MD に対応する方法である．小さいステップサイズで系全体を運動方程式に沿ってアップデートしていくことで，非局所的な有効作用というものは考えなくて済む．しかし，運動方程式の決定論的計算に伴う truncation error が導入される．</li>
</ol>
<p>後者の計算効率性と，前者の正確性を両取りすることを考える．</p>
</section>
</section>
<section id="本論" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="本論"><span class="header-section-number">2</span> 本論</h2>
<p>HMC は結局完全に Metropolis 法 <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> の枠内であり，詳細釣り合い条件を満たしにいくことを考える．ただし，この枠組みの中で最も優秀な方法を考える，というのである．</p>
<p>採択関数は <img src="https://latex.codecogs.com/png.latex?%0A%5Calpha(x,y)=1%5Cland%5Cfrac%7B%5Cpi(y)q(y,x)%7D%7B%5Cpi(x)q(x,y)%7D%0A"> で与えられるから，<sup>2</sup> <img src="https://latex.codecogs.com/png.latex?q(x,y)"> の計算が速いだけでなく，<img src="https://latex.codecogs.com/png.latex?q(y,x)"> も得られやすい理論的に都合の良い提案核 <img src="https://latex.codecogs.com/png.latex?Q"> を探すことを考える．</p>
<section id="先行研究" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="先行研究"><span class="header-section-number">2.1</span> 先行研究</h3>
<p>この考えは molecular dynamics と Langevin algorithm をハイブリッドするアルゴリズム <span class="citation" data-cites="Duane1985">(Duane, 1985)</span>, <span class="citation" data-cites="Duane-Kogut1986">(Duane &amp; Kogut, 1986)</span> に基づく．</p>
<p>場の理論において，<a href="https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E9%81%8E%E7%A8%8B%E9%87%8F%E5%AD%90%E5%8C%96">確率過程量子化</a> (stochastic quantization) <span class="citation" data-cites="Parisi-Wu1981">(Parisi &amp; Wu, 1981)</span> に基づく Langevin 方程式の方法と，小正準集団の方法（MD に近い，QCD の熱力学のシミュレーションにも使われる）という２つの方法が，特に力学的フェルミオンを含んだ系の数値シミュレーションにおいて魅力的な代替理論になっている．</p>
<p>この２つのシミュレーションのいいとこ取りをする hybrid algorithm が <span class="citation" data-cites="Duane1985">(Duane, 1985)</span> で提案されており，<span class="citation" data-cites="Duane-Kogut1986">(Duane &amp; Kogut, 1986)</span> で理論的な解析が進められた．これは，確率 <img src="https://latex.codecogs.com/png.latex?p%5CDelta"> によって，Langevin 法を用いるか，小正準法を決めるというアルゴリズムである．これは系をある熱浴に接続するという物理的な解釈を持つ．加えて，このアルゴリズムの軌跡は，ある古典的な運動方程式に従った奇跡ともみなせる．</p>
</section>
<section id="アイデア" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="アイデア"><span class="header-section-number">2.2</span> アイデア</h3>
<p>Parisi-Wu の確率過程量子化では，仮想的な時間 <img src="https://latex.codecogs.com/png.latex?t"> を導入して，場の量 <img src="https://latex.codecogs.com/png.latex?%5Cphi_i(x)"> が Langevin 方程式に従って発展するとする．こうして定まる確率過程が <img src="https://latex.codecogs.com/png.latex?t%5Cto%5Cinfty"> の極限で場の量子論を与えるというのである．</p>
<p>これに倣い，仮想的な時間パラメータ <img src="https://latex.codecogs.com/png.latex?%5Ctau"> と Hamilton 力学系を導入する．ここで補助変数として，共役運動量 <img src="https://latex.codecogs.com/png.latex?%5Cpi(t)"> が導入される．</p>
<p>もし Hamiltonian <img src="https://latex.codecogs.com/png.latex?H"> を正確に対象の系と同様に取れば，採択率は <img src="https://latex.codecogs.com/png.latex?1"> になり，これが hybrid algorithm <span class="citation" data-cites="Duane1985">(Duane, 1985)</span> に他ならない．しかし，ずらしても良いのである．</p>
</section>
<section id="検証" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="検証"><span class="header-section-number">2.3</span> 検証</h3>
<p>詳細釣り合い条件を満たすことを示している．詳細釣り合い条件が満たされる主な理由は Hamilton 力学系が可逆であることによる．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Duane1985" class="csl-entry">
Duane, S. (1985). Stochastic quantization versus the microcanonical ensemble: Getting the best of both worlds. <em>Nuclear Physics B</em>, <em>257</em>, 652–662. <a href="https://doi.org/10.1016/0550-3213(85)90369-4">https://doi.org/10.1016/0550-3213(85)90369-4</a>
</div>
<div id="ref-Duane+1987" class="csl-entry">
Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). Hybrid monte carlo. <em>Physics Letters B</em>, <em>195</em>(2), 216–222. <a href="https://doi.org/10.1016/0370-2693(87)91197-X">https://doi.org/10.1016/0370-2693(87)91197-X</a>
</div>
<div id="ref-Duane-Kogut1986" class="csl-entry">
Duane, S., &amp; Kogut, J. B. (1986). The theory of hybrid stochastic algorithms. <em>Nuclear Physics B</em>, <em>275</em>(3), 398–420. <a href="https://doi.org/10.1016/0550-3213(86)90606-1">https://doi.org/10.1016/0550-3213(86)90606-1</a>
</div>
<div id="ref-Metropolis+1953" class="csl-entry">
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a>
</div>
<div id="ref-Neal1996" class="csl-entry">
Neal, R. M. (1996). <em>Bayesian learning for neural networks</em> (Vol. 118). Springer New York. <a href="https://link.springer.com/book/10.1007/978-1-4612-0745-0">https://link.springer.com/book/10.1007/978-1-4612-0745-0</a>
</div>
<div id="ref-Parisi-Wu1981" class="csl-entry">
Parisi, G., &amp; Wu, Y. (1981). PERTURBATION THEORY WITHOUT GAUGE FIXING. <em>Scientia Sinica</em>, <em>24</em>(4), 483–483. <a href="https://www.sciengine.com/Math%20A0/doi/10.1360/ya1981-24-4-483">https://www.sciengine.com/Math%20A0/doi/10.1360/ya1981-24-4-483</a>
</div>
<div id="ref-鎌谷20-モンテカルロ" class="csl-entry">
鎌谷研吾. (2020). <em>モンテカルロ統計計算</em>. 講談社.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>“a form of the Metropolis algorithm in which candidate states are found by means of dynamical simulation.” <span class="citation" data-cites="Neal1996">(Neal, 1996, p. 55)</span>↩︎</p></li>
<li id="fn2"><p><span class="citation" data-cites="鎌谷20-モンテカルロ">(鎌谷研吾, 2020)</span> より．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Review</category>
  <guid>https://162348.github.io/posts/2024/Review/Duane+1987.html</guid>
  <pubDate>Wed, 17 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Review/Duane+1987.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>Tartero and Krauth (2023) Concepts in Monte Carlo Sampling</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Review/Tartero-Krauth2023.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="背景" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="背景"><span class="header-section-number">1</span> 背景</h2>
<p>Krauth は <span class="citation" data-cites="Bernard+2009">(Bernard et al., 2009)</span> において event-chain Monte Carlo アルゴリズムを提案した．</p>
<p>その後この手法は一般の連続系に適用できる形に拡張され，連続スピン系などにも適用されている <span class="citation" data-cites="酒井佑士2017">(酒井佑士, 2017, p. 24)</span>．</p>
</section>
<section id="本論" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="本論"><span class="header-section-number">2</span> 本論</h2>
<p>１粒子が次の one-dimensional anharmonic potential <img src="https://latex.codecogs.com/png.latex?%0AU_%7B24%7D(x)=%5Cfrac%7Bx%5E2%7D%7B2%7D+%5Cfrac%7Bx%5E4%7D%7B4%7D%0A"> に従って運動する場合を考える．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Computation/Files/potential.svg" class="img-fluid figure-img"></p>
<figcaption>ポテンシャル <img src="https://latex.codecogs.com/png.latex?U"> のプロット</figcaption>
</figure>
</div>
<p>このポテンシャルに関する Boltzmann-Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B24%7D"> は次の通り：</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Computation/Files/Gibbs.svg" class="img-fluid figure-img"></p>
<figcaption>ポテンシャル <img src="https://latex.codecogs.com/png.latex?U"> が定める Botlzmann-Gibbs 分布のプロット</figcaption>
</figure>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ(%5Cbeta)=%5Cint%5E%5Cinfty_%7B-%5Cinfty%7D%5Cpi_%7B24%7D(x)%5C,dx=%5Cfrac%7Be%5E%7B%5Cfrac%7B%5Cbeta%7D%7B8%7D%7D%7D%7B%5Csqrt%7B2%7D%7DK_%7B1/4%7D%5Cleft(%5Cfrac%7B%5Cbeta%7D%7B8%7D%5Cright)%0A"></p>
<p>１粒子非調和振動子は，孤立系としては決定論的な力学系であるが，熱浴に接続すると区分確定的な系になる <span class="citation" data-cites="Davis1984">(Davis, 1984)</span>．この系からは，Newton 力学と熱浴との相互作用の MD モデリングによって，<img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B24%7D"> からサンプリングすることができる．</p>
<p>この系を「熱浴に接続する」とは，速度を交換出来る仕組みを導入すれば良い．例えば振動中心 <img src="https://latex.codecogs.com/png.latex?x=0"> に半透性の弾性的な物体（thermostat）を固定し，振動子が <img src="https://latex.codecogs.com/png.latex?x=0"> を通る度に確率 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D"> で弾性衝突して速度を交換する系などとして考えられる．この半透性で弾性的な物体は無視できる幅で振動しながら熱浴と接続されており，温度が一定に保たれているとする．<sup>1</sup></p>
<p>この系を十分に放置すると，粒子の位置 <img src="https://latex.codecogs.com/png.latex?x"> は Boltzmann-Gibbs 分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B24%7D%5C,%5Cpropto%5C,e%5E%7B-%5Cbeta%20U_%7B24%7D%7D"> に従う．</p>
<ol type="1">
<li>これを，粒子の位置 <img src="https://latex.codecogs.com/png.latex?x"> を力学に基づいて追跡することで <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B24%7D"> からサンプリングすることも考えられる．これを MD 法という．</li>
<li>一方で Monte Carlo 法によりサンプリングすることが出来る．ここでは Gauss 分布の方が裾が重いので，これを提案分布とした棄却法によりサンプリングできる．積分を実行する場合も重点サンプリング法の考え方で実行できる．</li>
<li><span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> などではこの提案分布と対象分布の距離が離れすぎていることが問題なのであった．そこで Markov 連鎖を用いるのである．Metropolis 法とは，正方形の範囲への一様ランダムウォークから，採択-棄却のフィルターを通じて Markov 核を構成する普遍的手続きだと言える．</li>
<li>目標分布が因子分解可能であるとき，別のフィルター factorized Metropolis filter を通じても詳細釣り合い条件から Markov 核が構成できる．これは独立な乱数を生成して，全員可決したときに採択する，というよりスピーディーな棄却手続きが可能で consensus と呼ばれている．</li>
<li><span class="citation" data-cites="Chen+1999">(Chen et al., 1999)</span>, <span class="citation" data-cites="Diaconis+2000">(Diaconis et al., 2000)</span> によって詳細釣り合い条件を破る方法 lifting が提案された．これは補助変数法により，同じ方向に進み続けるように設計された Markov 連鎖である．<sup>2</sup></li>
<li>lifting を通じて，アルゴリズムを連続時間ベースにできる．これは，いちいち細かいステップサイズで「提案」するのではなく，次に棄却される位置と時刻をサンプリングすれば良い，というのである．event-driven にすることで連続時間ベースのシミュレーションが可能になるのである．</li>
<li>この event-driven なバージョンでも，factorized filter と consensus を応用できる．</li>
<li><img src="https://latex.codecogs.com/png.latex?U"> の評価が高価である場合，これよりも採択率が下がるような bounding potential <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BU%7D"> を用いることができる．これをフィルターを狭めるという感覚で thining <span class="citation" data-cites="Lewis-Shedler1979">(Lewis &amp; Shedler, 1979)</span> という．棄却された場合に，本格的に <img src="https://latex.codecogs.com/png.latex?U"> を評価する．</li>
</ol>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Bernard+2009" class="csl-entry">
Bernard, E. P., Krauth, W., &amp; Wilson, D. B. (2009). Event-chain monte carlo algorithms for hard-sphere systems. <em>Phys. Rev. E</em>, <em>80</em>, 056704. <a href="https://doi.org/10.1103/PhysRevE.80.056704">https://doi.org/10.1103/PhysRevE.80.056704</a>
</div>
<div id="ref-Chen+1999" class="csl-entry">
Chen, F., Lovász, L., &amp; Pak, I. (1999). Lifting markov chains to speed up mixing. <em>Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing</em>, 275–281. <a href="https://doi.org/10.1145/301250.301315">https://doi.org/10.1145/301250.301315</a>
</div>
<div id="ref-Davis1984" class="csl-entry">
Davis, M. H. A. (1984). Piecewise-deterministic markov processes: A general class of non-diffusion stochastic models. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>46</em>(3), 353–388. <a href="https://www.jstor.org/stable/2345677">https://www.jstor.org/stable/2345677</a>
</div>
<div id="ref-Diaconis+2000" class="csl-entry">
Diaconis, P., Holmes, S., &amp; Neal, R. M. (2000). Analysis of a nonreversible markov chain sampler. <em>The Annals of Applied Probability</em>, <em>10</em>(3), 726–752. <a href="http://www.jstor.org/stable/2667319">http://www.jstor.org/stable/2667319</a>
</div>
<div id="ref-Lewis-Shedler1979" class="csl-entry">
Lewis, P. A. W., &amp; Shedler, G. S. (1979). Simulation of nonhomogeneous poisson processes by thinning. <em>Naval Research Logistics Quarterly</em>, <em>26</em>(3), 403–413. <a href="https://doi.org/10.1002/nav.3800260304">https://doi.org/10.1002/nav.3800260304</a>
</div>
<div id="ref-Metropolis+1953" class="csl-entry">
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a>
</div>
<div id="ref-Turitsyn+2011" class="csl-entry">
Turitsyn, K. S., Chertkov, M., &amp; Vucelja, M. (2011). <span class="nocase">Irreversible Monte Carlo algorithms for Efficient Sampling</span>. <em>Physica D-Nonlinear Phenomena</em>, <em>240</em>(5-Apr), 410–414. <a href="https://doi.org/10.1016/j.physd.2010.10.003">https://doi.org/10.1016/j.physd.2010.10.003</a>
</div>
<div id="ref-酒井佑士2017" class="csl-entry">
酒井佑士. (2017). <em>マルコフ連鎖モンテカルロ法における詳細つり合い条件の破れの効果と応用</em> [PhD thesis, 東京大学]. <a href="https://repository.dl.itc.u-tokyo.ac.jp/records/50422">https://repository.dl.itc.u-tokyo.ac.jp/records/50422</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>このとき，thermostat は Maxwell 分布に従うわけではない点に注意．熱浴内の粒子と違って，thermostat は <img src="https://latex.codecogs.com/png.latex?x=0"> に固定されているため（例えば無視できる幅で振動しているとする），Maxwell boundary condition と呼ばれる分布に従い，比較的に簡単にサンプリングできる．↩︎</p></li>
<li id="fn2"><p>元の状態空間を拡張した上で詳細釣り合い条件の破れを導入する手法を総称して lifting と呼ぶ <span class="citation" data-cites="酒井佑士2017">(酒井佑士, 2017)</span>．特に２値空間との席をとってリフティングをする手法は <span class="citation" data-cites="Turitsyn+2011">(Turitsyn et al., 2011)</span> による方法で，<span class="citation" data-cites="酒井佑士2017">(酒井佑士, 2017, pp. 24–24)</span> で詳細な解析が与えられている．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Review</category>
  <guid>https://162348.github.io/posts/2024/Review/Tartero-Krauth2023.html</guid>
  <pubDate>Wed, 17 Apr 2024 15:00:00 GMT</pubDate>
</item>
<item>
  <title>Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Review/Metropolis+1953.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="概要" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="概要"><span class="header-section-number">1</span> 概要</h2>
<p>相互作用する分子系からなる物質の状態方程式などの性質を調べるために使える汎用手法（ fast computing machine にぴったり！）を提案する．この手法は配置空間上の，提案分布を正確にした「修正版 Monte Carlo 積分法」だと捉えられる．MANIAC を用いて，２次元剛体球の場合のシミュレーション結果を付した．その結果を，自由体積状態方程式と，４項ビリアル係数展開と比較した．</p>
</section>
<section id="背景" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="背景"><span class="header-section-number">2</span> 背景</h2>
<p>むしろ cell method などの計算手法との類似で捉えており，<span class="citation" data-cites="Hastings1970">(Hastings, 1970)</span> に取り上げられるまで全く別個の，広く抽象的に応用されるポテンシャルを持つ技術であるという抽象的な観点とは離れた泥臭い探索を感じる．</p>
<p>まだ Markov 連鎖という単語も使われていない．</p>
<p>話の展開は，分子系のシミュレーションの問題を，Gibbs 分布に関する積分計算の問題に帰着し，Monte Carlo 法を用いるところまで還元するが，そこで不適当な提案分布による重点サンプリングを実行するよりかは，系の Gibbs 分布を直接シミュレーションすれば良い，というものである．</p>
<p>よってここでいう修正 Monte Carlo 法とは，Monte Carlo 法（ここでは重点サンプリングと同義，（一様）乱数を用いた数値計算，くらいの意味）による乱数生成過程を，少しだけ MD 法の考え方を取り入れて，より Gibbs 分布に則した乱数生成をする，くらいの提案である．</p>
<p>後世では次のようにも説明されている始末である：</p>
<blockquote class="blockquote">
<p>The Monte Carlo method addresses the sampling problem more abstractly than molecular dynamics, as it samples (obtains samples <img src="https://latex.codecogs.com/png.latex?x"> from) the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B24%7D(x)"> without simulating a physical process. <span class="citation" data-cites="Tartero-Krauth2023">(Tartero &amp; Krauth, 2023)</span></p>
</blockquote>
<p>もはや，愚直な Monte Carlo 法が MD 法で，運動方程式を解かない MD 法が Monte Carlo 法，という具合である．事実，気体の状態方程式は，気体分子の運動の力学には無関係に成り立つので，必ずしも正しい力学に従ったシミュレーションが必要というわけではないのである．<sup>1</sup></p>
<p>なお，Monte Carlo 法の提案者として <a href="https://en.wikipedia.org/wiki/Joseph_Edward_Mayer">Joseph Edward Mayer</a> と <a href="https://en.wikipedia.org/wiki/Stanis%C5%82aw_Ulam">Stanisław Ulam</a> の名前を挙げている．</p>
<p>ここで <a href="https://en.wikipedia.org/wiki/Berni_Alder">Berni Alder</a> の名前が上がっており，やはり MCMC の開発は MD と極めて深い関係にあることが伺える．</p>
<blockquote class="blockquote">
<p>This method has been proposed independently by J. E. Mayer and by S. Ulam. Mayer suggested the method as a tool to deal with the problem of the liquid state, while Ulam proposed it as a procedure of general usefulness. B. Alder, J. Kirkwood, S. Frankel, and V. Lewinson discussed an application very similar to ours.</p>
</blockquote>
<section id="monte-carlo-法の起源について" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="monte-carlo-法の起源について"><span class="header-section-number">2.1</span> Monte Carlo 法の起源について</h3>
<p>John von Neumann は Edward Teller と同郷であり，戦時中の ENIAC の開発にも関わっていたことから，偉い人たちを説得して最初の ENIAC のテストに熱核融合反応の計算問題を用いることにした．</p>
<p>計算可能なモデルの構築をしたのが Metropolis である．これが完成し，実際にテストが行われたのは戦後の 1946 年であったが．</p>
<p>そのお披露目会に居合わせた Stanislaw Ulam が，統計的サンプリング技術を電子計算機で復活させることを提案し，Johnny がすぐさまその重要性を理解した．これがモンテカルロ法の始まりとなった，という <span class="citation" data-cites="Metropolis1987">(Metropolis, 1987)</span>．</p>
<p>Metropolis は Ulam が着想を得た理由として，数学的な背景を持っていたために，統計的サンプリング技術が計算の難しさのために歴史に埋没したことを知っており，ENIAC のポテンシャルを見てこれと関連づけることに成功したのではないかと示唆している．</p>
<p>そして Johnny の熱の入り用が周りも刺激した．1947 年には統計的サンプリングの中でも特に中性子の拡散問題を取り上げて当時の Los Alamos の理論部リーダーであった Robert Richtmyer に手紙を送った <span class="citation" data-cites="Eckhardt1987">(Eckhardt, 1987)</span>．こうして周りを巻き込んで大ごとになっていった．Monte Carlo 法という命名も 1947 年だったという</p>
<blockquote class="blockquote">
<p>It was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo.”</p>
</blockquote>
<blockquote class="blockquote">
<p>On a less grand scale these events brought about a renascence of a mathematical technique known to the old guard as statistical sampling; in its new surroundings and owing to its nature, there was no denying its new name of the Monte Carlo method. <span class="citation" data-cites="Metropolis1987">(Metropolis, 1987)</span></p>
</blockquote>
<p>特に戦時中の関心もあり，核分裂時の <strong>中性子の拡散</strong> のシミュレーションが問題であった．Monte Carlo 法と呼んでいるが本質的に MD 法チックであり，ここの中性子の散乱・吸収・分裂の系譜をシミュレートすることで全体の統計的性質がわかる，というだけの話であった．</p>
<p>その後 1952 年には後続機の MANIAC が開発され，nucleaer cascade と状態方程式も射程に入った．</p>
<p>この状態方程式を取り扱う際に <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> がさらに効率的な「モンテカルロ法」を発明したのである．これは Gibbs 分布を直接シミュレーションできるというブレイクスルーであり，「一般の確率分布からサンプリングできる」という今の理解とは大きく異なる文脈の中で発見されたと言うべきである．</p>
</section>
<section id="monte-carlo-法とはなんだろうか" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="monte-carlo-法とはなんだろうか"><span class="header-section-number">2.2</span> Monte Carlo 法とはなんだろうか</h3>
<p>思うに，「ランダムな方法を使って計算する」というのは外道に思えるかもしれない．</p>
<p>だが，実はランダムな系の <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(E)"> 上のダイナミクスの決定論的な計算になっているのかもしれない．</p>
<p>そう思わせるだけの透徹性が測度論にはある．</p>
<p>ただ，<span class="citation" data-cites="Metropolis1987">(Metropolis, 1987)</span> は Monte Carlo 法を <strong>実験数学</strong> (experimental mathematics) と呼んでおり，極めて物理学的な見方で評している：</p>
<blockquote class="blockquote">
<p>At long last, mathematics achieved a certain parity–the twofold aspect of experiment and theory–that all other sciences enjoy.</p>
</blockquote>
</section>
<section id="他のコメント" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="他のコメント"><span class="header-section-number">2.3</span> 他のコメント</h3>
<blockquote class="blockquote">
<p>Note that <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> move one particle at a time, rather than moving all of them together, which makes the initial algorithm appear a primitive kind of Gibbs sampler! <span class="citation" data-cites="Robert-Casella2011">(Robert &amp; Casella, 2011)</span></p>
</blockquote>
</section>
</section>
<section id="本論" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="本論"><span class="header-section-number">3</span> 本論</h2>
<section id="設定" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="設定"><span class="header-section-number">3.1</span> 設定</h3>
<p>古典統計を仮定し，２体間相互作用のみを考え，ポテンシャルは球対称であるとする（流体力学では通常の仮定である）．だが，温度や密度には全く仮定を置かない．<sup>2</sup></p>
<p>実際の計算のために，粒子数 <img src="https://latex.codecogs.com/png.latex?N"> は several hundred に取る．そして正方形の中にいれ，境界条件を最小化するために同様の系が２次元に無限に連なっているとする．２つの粒子 <img src="https://latex.codecogs.com/png.latex?A"> の他の粒子 <img src="https://latex.codecogs.com/png.latex?B"> との最短距離を <img src="https://latex.codecogs.com/png.latex?d_%7BA,B%7D"> とし，これのみが粒子 <img src="https://latex.codecogs.com/png.latex?A"> にかかる主な力になるとする．</p>
<p>仮に <img src="https://latex.codecogs.com/png.latex?N=1"> だとしたら，これは <a href="https://ja.wikipedia.org/wiki/Particle-in-Cell%E6%B3%95">cell method</a> と呼ばれるモデルでもある．こうして粒子を増やすことで，単一相のシステムに対するより良いモデルになるだろうが，二相以上のシステムには限界がある．</p>
<p>以上の仮定から，系のエネルギーが次のように与えられる： <img src="https://latex.codecogs.com/png.latex?%0AE=%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%5Cne%20j%5Cin%5BN%5D%7DV(d_%7Bij%7D).%0A"></p>
<p>この系の平衡状態の性質を計算するには，Gibbs の正準分布を利用し，計算したい物理量 <img src="https://latex.codecogs.com/png.latex?F"> に対して <img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BF%7D=%5Cfrac%7B%5Cint%20Fe%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7Dd%5E%7B2N%7Dpd%5E%7B2N%7Dq%7D%7B%5Cint%20e%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7Dd%5E%7B2N%7Dpd%5E%7B2N%7Dq%7D%0A"> を計算すれば良い．ただし，<img src="https://latex.codecogs.com/png.latex?d%5E%7B2n%7Dpd%5E%7B2n%7Dq"> は <img src="https://latex.codecogs.com/png.latex?4N"> 次元相空間上の体積要素である．</p>
<p>加えて，ここではポテンシャル <img src="https://latex.codecogs.com/png.latex?V"> は位置のみの引数としているから，<img src="https://latex.codecogs.com/png.latex?2N"> 次元上でのみ計算すれば良い．</p>
<p>このような数百次元上での積分を数値的方法で実行するのは明らかに実行可能でないから，Monte Carlo 法に頼らざるを得ない．と言っても，決定論的な点で値を計算する代わりに，ランダムに点をうつ，というだけの違いではある．</p>
<p>最も簡単な実装としては，ランダムに <img src="https://latex.codecogs.com/png.latex?N"> 粒子を配置してエネルギーを計算し（重点荷重），これにウェイトをつけて足していくということが考えられる（重点サンプリングだ！）．しかし，高エネルギーの配置もたくさん生成してしまうから，これによる効率の低減が避けられない（提案分布が悪いのだ！）．</p>
<p>そこで我々は <strong>modified Monte Carlo method</strong> を考える．<strong>そもそも確率 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-%5Cfrac%7BE%7D%7BkT%7D%7D"> からサンプルを生成し，荷重を一様にすることを目指す</strong>．</p>
</section>
<section id="アルゴリズムの記述" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="アルゴリズムの記述"><span class="header-section-number">3.2</span> アルゴリズムの記述</h3>
<p>これは次のようにする．まず適当に初期分布を決める（格子点上に <img src="https://latex.codecogs.com/png.latex?N"> 粒子を配置するなど）．そしてこれをアップデートしていく： <img src="https://latex.codecogs.com/png.latex?%0AX%5Cmapsto%20X+%5Calpha%5Cxi_1%0A"> <img src="https://latex.codecogs.com/png.latex?%0AY%5Cmapsto%20Y+%5Calpha%5Cxi_2%0A"> <img src="https://latex.codecogs.com/png.latex?%5Calpha%3E0"> は一度にどれくらい動かすかを調節するパラメータであり，<img src="https://latex.codecogs.com/png.latex?%5Cxi_1,%5Cxi_2%5Cin(0,1)"> は一様乱数とする．<sup>3</sup></p>
<p>すなわち，<img src="https://latex.codecogs.com/png.latex?(X,Y)"> を中心とした一辺 <img src="https://latex.codecogs.com/png.latex?2%5Calpha"> の正方形の中で，新たな位置をランダムに決めるのである．<sup>4</sup></p>
<p>この動きによるエネルギーの変化量 <img src="https://latex.codecogs.com/png.latex?%5CDelta%20E"> を計算し，<img src="https://latex.codecogs.com/png.latex?%5CDelta%20E%3C0"> ならばこれを実行するが，<img src="https://latex.codecogs.com/png.latex?%5CDelta%20E%3E0"> ならば確率 <img src="https://latex.codecogs.com/png.latex?e%5E%7B-%5Cfrac%7B%5CDelta%20E%7D%7BkT%7D%7D"> によって採択する．</p>
<p>仮に棄却されたとしても，そのポジションから新たな Monte Carlo 標本を取り，最終的に <img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BF%7D=%5Cfrac%7B1%7D%7BM%7D%5Csum_%7Bj=1%7D%5EMF_j%0A"> を推定量とする．</p>
</section>
<section id="アルゴリズムの有効性の検証" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="アルゴリズムの有効性の検証"><span class="header-section-number">3.3</span> アルゴリズムの有効性の検証</h3>
<p>まずこの系はエルゴードであると主張しているが，その論証は「任意の粒子が任意の位置に行くポテンシャルがあるため，この手法はエルゴード的である」で終わっている．エルゴードという単語を「任意の状態からもう一つの任意の状態に遷移可能である」という意味で使っている．これは現代的には既約性という．<sup>5</sup></p>
<p>続いて，この系をたくさんコピーしてアンサンブルを考えたとき，状態 <img src="https://latex.codecogs.com/png.latex?r"> にいるアンサンブルの数 <img src="https://latex.codecogs.com/png.latex?%5Cnu_r"> は <img src="https://latex.codecogs.com/png.latex?%0A%5Cnu_r%5C,%5Cpropto%5C,e%5E%7B-%5Cfrac%7BE_r%7D%7BkT%7D%7D%0A"> を満たすことを示したい．その論証は，上の比率から崩れていたら，平衡に至る方向へ移動が起こるということを具体的に議論している．</p>
<p>以上の２点から，提案されたアルゴリズムは正準分布に収束することの根拠としている．</p>
<p>このアンサンブルによる考え方は極めて直感的に訴える．実際，この語彙を用いて，棄却された場合は元々の状態を Monte Carlo サンプルとしてダブルカウントすべきであることを説明している．これをしなければ，低エネルギー状態のアンサンブルの数を不当に低く評価してしまう，という説明である．</p>
<p>ただし，<strong>このアンサンブルによる考え方は自然に我々の思考を詳細釣り合い条件に絞っている</strong>．</p>
</section>
<section id="附言" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="附言"><span class="header-section-number">3.4</span> 附言</h3>
<p>収束の速さについて注意喚起しているのみで，ステップサイズ <img src="https://latex.codecogs.com/png.latex?%5Calpha%3E0"> は大きすぎても棄却率が高まり，小さすぎても攪拌が遅くなるということ以外具体的なことは触れていない．</p>
</section>
</section>
<section id="実験" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="実験"><span class="header-section-number">4</span> 実験</h2>
<blockquote class="blockquote">
<p>In the case of two-dimensional rigid spheres, runs made with 56 particles and with 224 particles agreed within statistical error. For a computing time of a few hours with presently available electronic computers, it seems possible to obtain the pressure for a given volume and temperature to an accuracy of a few percent. In the case of two-dimensional rigid spheres our results are in agreement with the free volume approximation for <img src="https://latex.codecogs.com/png.latex?A/A_0%3C%201.8"> and with a five-term virial expansion for <img src="https://latex.codecogs.com/png.latex?A/A_0%3E%202.5">. There is no indication of a phase transition.</p>
</blockquote>
<p>16 step を焼き入れとし，48-64 いてレーションを実行するのに，MANIAC で 4-5時間かかったという．</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Eckhardt1987" class="csl-entry">
Eckhardt, R. (1987). <span class="nocase">Stan Ulam, John von Neumann, and the Monte Carlo Method</span>. <em>Los Alamos Science Special Issue</em>, <em>15</em>, 131–143. <a href="http://www-star.st-and.ac.uk/~kw25/teaching/mcrt/MC_history_3.pdf">http://www-star.st-and.ac.uk/~kw25/teaching/mcrt/MC_history_3.pdf</a>
</div>
<div id="ref-Hastings1970" class="csl-entry">
Hastings, W. K. (1970). Monte carlo sampling methods using markov chains and their applications. <em>Biometrika</em>, <em>57</em>(1), 97–109. <a href="https://www.jstor.org/stable/2334940">https://www.jstor.org/stable/2334940</a>
</div>
<div id="ref-Metropolis1987" class="csl-entry">
Metropolis, N. (1987). The beginning of the monte carlo method. <em>Los Alamos Science Special Issue</em>, <em>15</em>, 125–130. <a href="https://library.lanl.gov/cgi-bin/getfile?00326866.pdf">https://library.lanl.gov/cgi-bin/getfile?00326866.pdf</a>
</div>
<div id="ref-Metropolis+1953" class="csl-entry">
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a>
</div>
<div id="ref-Robert-Casella2011" class="csl-entry">
Robert, C., &amp; Casella, G. (2011). A short history of markov chain monte carlo: Subjective recollections from incomplete data. <em>Statistical Science</em>, <em>26</em>(1), 102–115. <a href="http://www.jstor.org/stable/23059158">http://www.jstor.org/stable/23059158</a>
</div>
<div id="ref-Tartero-Krauth2023" class="csl-entry">
Tartero, G., &amp; Krauth, W. (2023). <em>Concepts in monte carlo sampling</em>. <a href="https://arxiv.org/abs/2309.03136">https://arxiv.org/abs/2309.03136</a>
</div>
<div id="ref-戸田+2011" class="csl-entry">
戸田盛和, 斎藤信彦, 久保亮五, &amp; 橋爪夏樹. (2011). <em>統計物理学</em> (新装版, Vol. 5). 岩波書店. <a href="https://www.iwanami.co.jp/book/b259545.html">https://www.iwanami.co.jp/book/b259545.html</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="戸田+2011">(戸田盛和 et al., 2011, p. 14)</span>↩︎</p></li>
<li id="fn2"><p>さらに，Lennard-Jones ポテンシャルについての２次元のケースも考えており，次のレポートで報告される予定であるという．↩︎</p></li>
<li id="fn3"><p>ここで乱数生成法に関して注記されている．これは <strong>middle square process</strong> で生成する，としている．↩︎</p></li>
<li id="fn4"><p>なお，periodic assumption をしているため（長方形の系の外には同様の系が無数に並んでいるとしたため），境界の外に出ようとした場合は衝突するのではなく，反対側の辺から入ってくるものとする．↩︎</p></li>
<li id="fn5"><p><span class="citation" data-cites="Robert-Casella2011">(Robert &amp; Casella, 2011)</span> も指摘している．だが，<span class="citation" data-cites="戸田+2011">(戸田盛和 et al., 2011, p. 5)</span> にも同じ意味で「エルゴード的」という単語を使っている記述がある．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Review</category>
  <guid>https://162348.github.io/posts/2024/Review/Metropolis+1953.html</guid>
  <pubDate>Wed, 17 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Review/Metropolis+1953.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>数学者のための統計力学２</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Nature/StatisticalMechanics2.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<p>殆どの力学的現象は，相空間 <img src="https://latex.codecogs.com/png.latex?%5COmega%5Csubset%5Cmathbb%7BR%7D%5E%7B6N%7D"> と呼ばれる <a href="https://ncatlab.org/nlab/show/symplectic+manifold">シンプレクティック多様体</a> 上の Hamiltonian flow <img src="https://latex.codecogs.com/png.latex?(S_t)"> として理解される．統計力学では，その上に確率測度 <img src="https://latex.codecogs.com/png.latex?P"> を導入して運動を理解する．組 <img src="https://latex.codecogs.com/png.latex?(%5COmega,P)"> を統計集団という．</p>
<p>例えば，<img src="https://latex.codecogs.com/png.latex?N">-粒子系 <img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N%7D"> の統計的な振る舞いを最もよく記述する確率分布 <img src="https://latex.codecogs.com/png.latex?P%5Cin%5Cmathcal%7BP%7D(%5COmega_%7B%5CLambda,N%7D)"> を推定することを考える．</p>
<p>系が平衡状態にない場合は分子の衝突を考える必要があり，現在でも理論は発展中であるが，すでに平衡状態に至ったとみなせる場合には，この確率分布 <img src="https://latex.codecogs.com/png.latex?P"> というのは殆どわかる．</p>
<p>仮に初期分布 <img src="https://latex.codecogs.com/png.latex?P_0%5Cin%5Cmathcal%7BP%7D(%5COmega_%7B%5CLambda,N%7D)"> を持っていた場合，分布は Hamilton 流の押し出しにより <img src="https://latex.codecogs.com/png.latex?(S_t)_*P_0"> というように <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(%5COmega_%7B%5CLambda,N%7D)"> 上の力学系として発展していく．<sup>1</sup></p>
<p>従って，平衡分布は必ず <img src="https://latex.codecogs.com/png.latex?S_t"> の不動点，不変確率分布である必要があるが，これを満たす分布は複数存在する．そのうち特に意味のあるものが，小正準集団 <img src="https://latex.codecogs.com/png.latex?(%5COmega_%7B%5CLambda,N,E%7D,P%5E%7B%5Ctext%7Bmicrocanon%7D%7D_%7B%5CLambda,N,E%7D)">，正準集団 <img src="https://latex.codecogs.com/png.latex?(%5COmega_%7B%5CLambda,N%7D,P%5E%7B%5Ctext%7Bcanon%7D%7D_%7B%5CLambda,N,%5Cbeta%7D)">，大正準集団 <img src="https://latex.codecogs.com/png.latex?(%5COmega_%5CLambda,P%5E%7B%5Ctext%7Bgrandcanon%7D%7D_%7B%5CLambda,%5Cbeta,%5Cmu%7D)"> などである．「小」「大」などの接頭辞は分布の台の大きさとも思える．</p>
<p><strong>確率的な系も <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(%5COmega_%7B%5CLambda,N%7D)"> 上で見れば決定論的な発展をするのが数理の妙である</strong>．</p>
<p>これら３つの分布はどれもそれだけで完全なものではなく，熱力学極限において真に物理的に意味のある量になると理解する．</p>
<section id="小正準分布" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="小正準分布"><span class="header-section-number">1</span> 小正準分布</h2>
<p><a href="../../../posts/2024/Nature/StatisticalMechanics1.html#sec-microcanonical">小正準測度</a> <img src="https://latex.codecogs.com/png.latex?%5Cnu_%7B%5CLambda,N,E%7D"> を正規化したものも <img src="https://latex.codecogs.com/png.latex?S_t"> の不変確率分布である： <img src="https://latex.codecogs.com/png.latex?%0AP%5E%7B%5Ctext%7Bmicrocanon%7D%7D_%7B%5CLambda,N,E%7D(A):=%5Cfrac%7B%5Cnu_%7B%5CLambda,N,E%7D(A)%7D%7B%5Cnu_%7B%5CLambda,N,E%7D(%5COmega_%7B%5CLambda,N,E%7D)%7D%0A"></p>
<p>改めて，<img src="https://latex.codecogs.com/png.latex?P%5E%7B%5Ctext%7Bmicrocanon%7D%7D_%7B%5CLambda,N,E%7D"> は等エネルギー面 <img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E%7D"> 上にしか台を持たないことに注意．そのため，<img src="https://latex.codecogs.com/png.latex?P%5E%7B%5Ctext%7Bmicrocanon%7D%7D_%7B%5CLambda,N,E%7D"> は <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7B6N%7D"> 上の Lebesgue 測度 <img src="https://latex.codecogs.com/png.latex?%5Cell_%7B6N%7D"> に関しては絶対連続ではない．<sup>2</sup></p>
<blockquote class="blockquote">
<p>このようにエネルギーが一定であるような力学系の集団を<strong>ミクロカノニカル集団</strong>，この集団の分布，すなわちそのエネルギーに対応する全ての微視状態へ，等しい確率を持って存在するような分布を<strong>ミクロカノニカル分布</strong>という．<span class="citation" data-cites="久保亮五2003">(久保亮五, 2003, p. 27)</span></p>
</blockquote>
<p>孤立系など，エネルギー一定の系は Hamiltonian の等位集合上で運動をする．その上の小正準測度 / Gelfand-Leray 測度は，相空間上の自然な体積要素から誘導される．<sup>3</sup> 従って小正準分布は運動に関して不変ではあるが，だからと言ってどのようなマクロ系に対しても小正準分布が平衡分布になると約束する法則は何もない．孤立したマクロ系（エネルギー一定以外に制約がない場合）が小正準分布に従うことは，等重率の仮定の具体的な表現と見れる，一種のモデルの仮定である．<sup>4</sup></p>
</section>
<section id="gibbs-の正準分布" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="gibbs-の正準分布"><span class="header-section-number">2</span> Gibbs の正準分布</h2>
<p>系のエネルギーを指定するのではなく，逆温度を指定することで，相空間上で密度 <img src="https://latex.codecogs.com/png.latex?%0Ap%5E%7B%5Ctext%7Bcanon%7D%7D_%7B%5CLambda,N,%5Cbeta%7D(Q,V)=%5Cfrac%7B1%7D%7BZ_%7B%5CLambda,N,%5Cbeta%7D%7De%5E%7B-%5Cbeta%20H_%7B%5CLambda,N%7D(Q,V)%7D%0A"> を持つ不変確率分布を得る．<sup>5</sup> これを <strong>正準分布</strong> (canonical distribution) という．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%3E0"> を <strong>逆温度</strong>，<img src="https://latex.codecogs.com/png.latex?Z_%7B%5CLambda,N,%5Cbeta%7D"> を <strong>分配関数</strong> (partition function / statistical sum) という．</p>
<p>古典気体の <img src="https://latex.codecogs.com/png.latex?H"> において位置に依存する項と速度に依存する項とが分かれているため，<img src="https://latex.codecogs.com/png.latex?P%5E%7B%5Ctext%7Bcanon%7D%7D_%7B%5CLambda,N,%5Cbeta%7D"> において位置と速度は独立であり，速度 <img src="https://latex.codecogs.com/png.latex?v_1,%5Ccdots,v_N"> も互いに独立でそれぞれは Gauss 分布に従う．これを <a href="https://ja.wikipedia.org/wiki/%E3%83%9E%E3%82%AF%E3%82%B9%E3%82%A6%E3%82%A7%E3%83%AB%E5%88%86%E5%B8%83"><strong>Maxwell の法則</strong></a> という．</p>
<p>また特に，Gibbs 測度の配置空間上での周辺分布は，configuration gas の Gibbs 測度に一致する．この意味で，古典気体の統計力学的性質は，configuration gas に帰着すると言える．</p>
</section>
<section id="つの関係と熱力学極限" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="つの関係と熱力学極限"><span class="header-section-number">3</span> ２つの関係と熱力学極限</h2>
<p>大雑把に捉えれば，エネルギー一定の力学系（例えば孤立系）の全ての状態の上の分布がミクロカノニカル分布であり，このエネルギーも動かした場合（例えば孤立系の部分系），各エネルギー上の分布はカノニカル分布になる．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5CLambda_0%5Csubset%5CLambda"> を部分領域として，この領域に <img src="https://latex.codecogs.com/png.latex?s"> 個の粒子が存在する状態 <img src="https://latex.codecogs.com/png.latex?%0A%5COmega_%7B%5CLambda,N%7D%5E%7B%5CLambda_0,s%7D:=%5Cleft%5C%7B(Q,V)%5Cin%5COmega_%7BV,N%7D%5Cmid%5Clvert%20Q%5Ccap%5CLambda_0%5Crvert=s%5Cright%5C%7D%0A"> を考え，この部分領域への小正準分布 <img src="https://latex.codecogs.com/png.latex?P%5E%7B%5Ctext%7Bmicrocanon%7D%7D_%7B%5CLambda,N,E%7D"> の制限を考えると，ある極限に関して正準分布になる <span class="citation" data-cites="Minlos2000">(Minlos, 2000)</span>．これがいわば <strong>熱浴</strong> や thermostat の概念である．</p>
<p>すなわち，ある領域 <img src="https://latex.codecogs.com/png.latex?%5CLambda_0"> に <img src="https://latex.codecogs.com/png.latex?s"> 個の粒子が存在する限り値が変わらないような量 <img src="https://latex.codecogs.com/png.latex?F:%5COmega_%7B%5CLambda,N%7D%5Cto%5Cmathbb%7BR%7D"> は，小正準平均も正準平均も「ほとんど」変わらないことになる．この極限の正確な意味は <img src="https://latex.codecogs.com/png.latex?%0A%5CLambda%5Cnearrow%5Cmathbb%7BR%7D%5Ev,N%5Cto%5Cinfty,%5Cfrac%7BN%7D%7B%5Clvert%5CLambda%5Crvert%7D%5Cto%5Crho%5Cin%5Cmathbb%7BR%7D,%5Cfrac%7BE_i%7D%7B%5Clvert%5CLambda_i%5Crvert%7D%5Cto%20e%5Cin%5Cmathbb%7BR%7D,%0A"> というものであり，これを <strong>熱力学極限</strong> という（第 5 節）．</p>
<blockquote class="blockquote">
<p>一般に極めて多数の自由度を持つ力学系 B と，問題の対象たる一つの力学系 A（上の例では <img src="https://latex.codecogs.com/png.latex?s"> 個の粒子からなる系）とが結合しているときに，系 A の一つの微視状態の実現確率を与える．これはそのような条件の下に，前節に述べた等重率の仮定に代わるものである． 物理的な言葉に引き直せば，結局，熱容量の大きい物体と熱平衡を保つ任意の力学系の統計的分布を表す集団がカノニカル集団である．<span class="citation" data-cites="久保亮五2003">(久保亮五, 2003, p. 30)</span></p>
</blockquote>
</section>
<section id="大正準集団" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="大正準集団"><span class="header-section-number">4</span> 大正準集団</h2>
<p>大正準集団は，領域 <img src="https://latex.codecogs.com/png.latex?%5CLambda%5Csubset%5Cmathbb%7BR%7D%5E3"> に粒子数 <img src="https://latex.codecogs.com/png.latex?N%5Cin%5Cmathbb%7BN%7D"> を定めずに存在する互いに区別できない粒子の系を考えることになる（エネルギーの交換だけでなく粒子も交換する部分系など）： <img src="https://latex.codecogs.com/png.latex?%0A%5COmega_%5CLambda=C_%5CLambda%5E%7B(0)%7D%5Ccup%20C_%5CLambda%5E%7B(1)%7D%5Ccup%5Ccdots%5Ccup%20C_%5Clambda%5E%7B(N)%7D%5Ccup%5Ccdots%0A"></p>
<p>この <img src="https://latex.codecogs.com/png.latex?%5COmega_%5CLambda"> 上に，各 <img src="https://latex.codecogs.com/png.latex?C_%5CLambda%5E%7B(N)%7D"> 上での Lebesgue 測度が誘導する測度 (Lebesgue-Poisson measure) <img src="https://latex.codecogs.com/png.latex?%0A%5Cmu_%5CLambda%5E%7B(N)%7D(A):=%5Cfrac%7B%5Cell_%7B3N%7D(A)%7D%7BN!%7D%0A"> の貼り合わせとして定義される測度を正規化したものを <img src="https://latex.codecogs.com/png.latex?%5Cmu"> とし，これを体積の代わりとする．</p>
<p>これを基底測度として密度 <img src="https://latex.codecogs.com/png.latex?%0Ap%5E%7B%5Ctext%7Bgrandcanon%7D%7D_%7B%5CLambda,%5Cbeta,%5Cmu%7D(c):=%5Cfrac%7B1%7D%7B%5CXi(%5CLambda,%5Cbeta,%5Cmu)%7De%5E%7B-%5Cbeta(H_%5CLambda(c)+%5Cmu%20N(c))%7D%0A"> が定める分布を <strong>大正準分布</strong> という．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%3E0"> は逆温度であるが，<img src="https://latex.codecogs.com/png.latex?%5Cmu%5Cin%5Cmathbb%7BR%7D"> は化学ポテンシャルである．</p>
<p>小正準集団が正準集団になる際に，エネルギー一定の制約は解放されて，代わりに新たなパラメータ <img src="https://latex.codecogs.com/png.latex?%5Cbeta%3E0"> を得た．ここからさらに粒子数一定の制約を解放し，代わりに新たなパラメータ <img src="https://latex.codecogs.com/png.latex?%5Cmu%5Cin%5Cmathbb%7BR%7D"> を得たものが大正準集団である．この順に解析が容易になる．</p>
<p>大正準分布を粒子数一定の条件で条件付けて得る <img src="https://latex.codecogs.com/png.latex?C_%5CLambda%5E%7B(N)%7D"> 上の分布は正準分布に一致する．</p>
</section>
<section id="sec-thermodynamic-limit" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-thermodynamic-limit"><span class="header-section-number">5</span> 熱力学極限</h2>
<p><img src="https://latex.codecogs.com/png.latex?F_B:%5COmega_%5CLambda%5Cto%5Cmathbb%7BR%7D"> が <strong>局所変数</strong> であるとは， <img src="https://latex.codecogs.com/png.latex?%0AF_B(c)=F_B(c%5Ccap%20B)%5Cquad(c%5Cin%5COmega_%5CLambda)%0A"> を満たすことをいう．</p>
<p>熱力学極限 <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7B%5CLambda%5Cnearrow%5Cmathbb%7BR%7D%5E3%7D%5Clangle%20F_B%5Crangle_%7B%5CLambda,%5Cbeta,%5Cmu%7D=%5Clangle%20F_B%5Crangle_%7B%5Cinfty,%5Cbeta,%5Cmu%7D%0A"> は，ある空間上のある積分と捉えられる．これを <strong>極限 Gibbs 測度</strong> (limit Gibbs distribution) という．</p>
<p>この極限 Gibbs 測度が複数存在したり，良い性質が失われたりする現象を <strong>相転移</strong> といい，その際の <img src="https://latex.codecogs.com/png.latex?(%5Cbeta,%5Cmu)"> を特異点という．</p>
<p>熱力学では，系の変化が極めて緩慢であるために，一瞬一瞬において平衡状態が保たれているとみなせる物理的過程が扱われるため，大正準集団では <img src="https://latex.codecogs.com/png.latex?T=%5Cbeta%5E%7B-1%7D,%5Cmu">，正準集団では <img src="https://latex.codecogs.com/png.latex?T,%5Crho">，小正準集団では <img src="https://latex.codecogs.com/png.latex?%5Crho,e"> などのマクロ変量によって記述できる理論になっている．</p>
<p>そして熱力学極限において，これら３集団は等価であるから，これらはパラメータ変換の問題でしかなく，さらにこのパラメータ空間上に別の関数も導入される．特にエントロピー <img src="https://latex.codecogs.com/png.latex?s(e,%5Crho)">，Helmholz の自由エネルギー（密度） <img src="https://latex.codecogs.com/png.latex?f(%5Cbeta,%5Crho)">，圧力 <img src="https://latex.codecogs.com/png.latex?p(%5Cbeta,%5Cmu)"> などであり，これらは互いに Legendre 変換により関係し合っている．</p>
<p>これらの熱力学的関数も，特定の統計力学的な量の熱力学極限として理解できる．</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As(%5Crho,e)=%5Clim_%7B%5Csubstack%7B%5CLambda%5Cnearrow%5Cmathbb%7BZ%7D%5Ev%5C%5C%5Cfrac%7BN%7D%7B%5Clvert%5CLambda%5Crvert%7D%5Cto%5Crho%5C%5C%5Cfrac%7BE%7D%7B%5CLambda%7D%5Cto%20e%7D%7D%5Cfrac%7B%5Clog%20Q%5E%7B%5Ctext%7Bindistin%7D%7D(%5CLambda,E,N)%7D%7B%5Clvert%5CLambda%5Crvert%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0Af(%5Cbeta,%5Crho)=-%5Cfrac%7B1%7D%7B%5Cbeta%7D%5Clim_%7B%5Csubstack%7B%5CLambda%5Cnearrow%5Cmathbb%7BZ%7D%5E3%5C%5C%5Cfrac%7BN%7D%7B%5Clvert%5CLambda%5Crvert%7D%5Cto%5Crho%7D%7D%5Cfrac%7B%5Clog%20Z%5E%7B%5Ctext%7Bindistin%7D%7D(%5CLambda,N,%5Cbeta)%7D%7B%5Clvert%5CLambda%5Crvert%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ap(%5Cbeta,%5Cmu)=%5Cfrac%7B1%7D%7B%5Cbeta%7D%5Clim_%7BN%5Cnearrow%5Cmathbb%7BZ%7D%5E3%7D%5Cfrac%7B%5Clog%5CXi(%5CLambda,%5Cmu,%5Cbeta)%7D%7B%5Clvert%5CLambda%5Crvert%7D%0A"></p>
<p>ただし，<img src="https://latex.codecogs.com/png.latex?Q%5E%7B%5Ctext%7Bindistin%7D%7D,Z%5E%7B%5Ctext%7Bindistin%7D%7D,%5CXi"> はそれぞれ，（粒子が互いに区別がつかない場合の）小正準集団，正準集団，大正準集団の正規化定数とした．</p>
</section>



<div id="quarto-appendix" class="default"><section id="参考文献" class="level2 appendix" data-number="6"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">6</span> 参考文献</h2><div class="quarto-appendix-contents">

<p><span class="citation" data-cites="Minlos2000">(Minlos, 2000)</span>, <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003)</span>, <span class="citation" data-cites="Baxter1982">(Baxter, 1982)</span></p>



</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Baxter1982" class="csl-entry">
Baxter, R. J. (1982). <em>Exactly solved models in statistical mechanics</em>. Academic Press. <a href="https://physics.anu.edu.au/research/ftp/mpg/baxter_book.php">https://physics.anu.edu.au/research/ftp/mpg/baxter_book.php</a>
</div>
<div id="ref-Khinchin1949" class="csl-entry">
Khinchin, A. (1949). <em><a href="">Mathematical foundations of statistical mechanics</a></em>. Courier Corporation.
</div>
<div id="ref-Minlos2000" class="csl-entry">
Minlos, R. A. (2000). <em>Introduction to mathematical statistical physics</em> (Vol. 19). American Mathematical Society. <a href="https://doi.org/10.1090/ulect/019">https://doi.org/10.1090/ulect/019</a>
</div>
<div id="ref-久保亮五2003" class="csl-entry">
久保亮五. (2003). <em>新装版統計力学</em>. 共立出版. <a href="https://www.kyoritsu-pub.co.jp/book/b10011230.html">https://www.kyoritsu-pub.co.jp/book/b10011230.html</a>
</div>
<div id="ref-戸田+2011" class="csl-entry">
戸田盛和, 斎藤信彦, 久保亮五, &amp; 橋爪夏樹. (2011). <em>統計物理学</em> (新装版, Vol. 5). 岩波書店. <a href="https://www.iwanami.co.jp/book/b259545.html">https://www.iwanami.co.jp/book/b259545.html</a>
</div>
<div id="ref-田崎晴明2008" class="csl-entry">
田崎晴明. (2008). <em>統計力学I</em> (Vol. 37). 培風館. <a href="https://www.gakushuin.ac.jp/~881791/statbook/">https://www.gakushuin.ac.jp/~881791/statbook/</a>
</div>
<div id="ref-砂田利一2004" class="csl-entry">
砂田利一. (2004). <em>数学から見た統計学と熱力学</em> (Vol. 4). 岩波書店. <a href="https://www.iwanami.co.jp/book/b476285.html">https://www.iwanami.co.jp/book/b476285.html</a>
</div>
<div id="ref-西森秀稔2003" class="csl-entry">
西森秀稔. (2003). <em>スピングラスと連想記憶</em>. 岩波書店. <a href="https://www.iwanami.co.jp/book/b476276.html">https://www.iwanami.co.jp/book/b476276.html</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>運動は相空間の自然な体積形式 (Liouville 形式ともいう) を不変に保つという Liouville の定理 <span class="citation" data-cites="Khinchin1949">(Khinchin, 1949)</span> は量子論でも成り立つ．<span class="citation" data-cites="戸田+2011">(戸田盛和 et al., 2011, p. 23)</span> これらの性質を抽象した形で，古典・量子力学は，統計力学に等重率の仮定を課す．↩︎</p></li>
<li id="fn2"><p>ただし，<img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E%7D"> 上に制限された体積測度に関しては，密度 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%5Clvert%5Cmathop%7B%5Cmathrm%7B%5Cmathrm%7Bgrad%7D%7D%7DH%5Crvert%7D"> を持つ <span class="citation" data-cites="Khinchin1949">(Khinchin, 1949)</span>．↩︎</p></li>
<li id="fn3"><p>これを Liouville 形式だけでなく Liouville 測度ということもあるようである <span class="citation" data-cites="砂田利一2004">(砂田利一, 2004)</span>．↩︎</p></li>
<li id="fn4"><p>等重率の仮定は量子論の観点から「マクロな量子系では，ある平衡状態に対応する許容される量子状態の全てが区別できない」と説明されることも多い．いずれにしろ極めて非自明な主張であるが，これを認めてみると良い理論を得る．実際 <span class="citation" data-cites="田崎晴明2008">(田崎晴明, 2008, p. 89)</span> では等重率の仮定を「戦略」「等重率の原理に基づく確率モデル」と説明している．いわば，統計モデルの１つであり，モデル選択の観点に立っているのである．↩︎</p></li>
<li id="fn5"><p><img src="https://latex.codecogs.com/png.latex?S_t"> は体積を保存し Hamiltonian も保存するため，これは明らかに <img src="https://latex.codecogs.com/png.latex?S_t"> の不変確率分布である．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Nature</category>
  <guid>https://162348.github.io/posts/2024/Nature/StatisticalMechanics2.html</guid>
  <pubDate>Sat, 06 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Nature/canon.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>数学者のための統計力学１</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Nature/StatisticalMechanics1.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>古典粒子系を例にとって，統計力学における基本的な用語を確認する．</p>
<p>統計力学といえども場面設定は力学であり，その形式はハミルトン形式が利用される．</p>
<p>しかし，その運動方程式を解析的に分析するのではなく，相空間 <img src="https://latex.codecogs.com/png.latex?%5COmega"> 上の（有界）測度に注目して，確率統計学，果てには計算統計学を利用して展開していく．</p>
<p>ちょうど，統計学ではここのサンプル <img src="https://latex.codecogs.com/png.latex?%5Comega%5Cin%5COmega"> よりも全体的な振る舞い <img src="https://latex.codecogs.com/png.latex?P%5Cin%5Cmathcal%7BP%7D(%5COmega)"> や統計量 <img src="https://latex.codecogs.com/png.latex?%5COmega%5Cto%5Cmathbb%7BR%7D"> の平均値や分散などの統計的な振る舞いに興味があるのと同じことである．</p>
</div>
</div>
</div>
<section id="古典粒子系" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="古典粒子系"><span class="header-section-number">1</span> 古典粒子系</h2>
<section id="多様体-omega_lambdan" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="多様体-omega_lambdan"><span class="header-section-number">1.1</span> 多様体 <img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N%7D"></h3>
<p>有界集合 <img src="https://latex.codecogs.com/png.latex?%5CLambda%5Csubset%5Cmathbb%7BR%7D%5E3"> に囚われた古典的な <img src="https://latex.codecogs.com/png.latex?N"> 粒子系を考えると，位置と速度によって各粒子は記述できるから， <img src="https://latex.codecogs.com/png.latex?%0A%5COmega_%7B%5CLambda,N%7D:=(%5CLambda%5Ctimes%5Cmathbb%7BR%7D%5E3)%5EN%0A"> 内の点によって系が記述できる．これを <strong>相空間</strong> (phase space) という．<sup>1</sup></p>
</section>
<section id="関数-h_lambdan" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="関数-h_lambdan"><span class="header-section-number">1.2</span> 関数 <img src="https://latex.codecogs.com/png.latex?H_%7B%5CLambda,N%7D"></h3>
<p>この上に <strong>エネルギー</strong> が定まるが，これは <strong>ハミルトニアン</strong> ともいう：<sup>2</sup> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20H_%7B%5CLambda,N%7D(Q,V)&amp;=%5Csum_%7Bi=1%7D%5EN%5Cfrac%7Bmv_i%5E2%7D%7B2%7D+%5Csum_%7Bi%3Cj%5Cin%5BN%5D%7DU(q_i-q_j)%5C%5C%0A%20%20%20%20&amp;%5Cqquad%5Cqquad+%5Csum_%7Bi=1%7D%5ENV_b(q_i)%0A%5Cend%7Balign*%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0AQ=(q_1,%5Ccdots,q_N)%5Cin%5CLambda%5E%7B3N%7D%0A"></p>
<p>多くの場合，ポテンシャル関数 <img src="https://latex.codecogs.com/png.latex?U"> はコンパクト台を持つ，すなわち，<strong>相互作用半径</strong> (radius of interaction) <img src="https://latex.codecogs.com/png.latex?R"> をもつとする：<img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bsupp%7D%5C;U%5Csubset%20U_R(0)">．<sup>3</sup></p>
<p>経験則的に，まずは Lennard-Jones ポテンシャル <img src="https://latex.codecogs.com/png.latex?%0AU(r)=%5Cfrac%7BA%7D%7Br%5E%7B12%7D%7D-%5Cfrac%7BB%7D%7Br%5E6%7D%5Cquad%20A,B%3E0%0A"> を用いてモデリングする場合が多い．</p>
<p>系の時間発展は変換の族 <img src="https://latex.codecogs.com/png.latex?%5C%7BS_t%5C%7D%5Csubset%5Cmathrm%7BAut%7D(%5COmega_%7B%5CLambda,N%7D)"> （<strong>動力学</strong> dynamics という）によって記述され， 動力学は運動方程式から導出される． <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bd%20q_i%7D%7Bd%20t%7D=v_i%0A"> <img src="https://latex.codecogs.com/png.latex?%0Am%5Cfrac%7Bd%20v_i%7D%7Bd%20t%7D=-%5Csum_%7Bi%5Cne%20j%7D%5Cnabla%20U(q_j-q_i)-%5Cnabla%20V_b(q_i)%0A"></p>
</section>
<section id="sec-microcanonical" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="sec-microcanonical"><span class="header-section-number">1.3</span> 部分多様体 <img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E%7D"> と小正準分布</h3>
<p>この動力学 <img src="https://latex.codecogs.com/png.latex?%5C%7BS_t%5C%7D"> は体積とエネルギーを保存量にもつ．すなわち，各 <img src="https://latex.codecogs.com/png.latex?S_t"> は保測的で，等エネルギー集合 <img src="https://latex.codecogs.com/png.latex?%0A%5COmega_%7B%5CLambda,N,E%7D:=%5Cleft%5C%7B(Q,%5CLambda)%5Cin%5COmega_%7B%5CLambda,N%7D%5Cmid%20H_%7B%5CLambda,N%7D(Q,%5CLambda)=E%5Cright%5C%7D%0A"> を不変部分集合にもつ．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E%7D"> 上の測度 <img src="https://latex.codecogs.com/png.latex?%0A%5Cnu_%7B%5CLambda,N,E%7D(B):=%5Clim_%7B%5CDelta%20E%5Cto0%7D%5Cfrac%7B%5Cell(%5CDelta%20B)%7D%7B%5CDelta%20E%7D%0A"> は <img src="https://latex.codecogs.com/png.latex?S_t"> によって保存される．<sup>4</sup> これを <strong>小正準分布</strong> (microcanonical measures) または Gelfand-Leray measures という．</p>
<p>ただし，<img src="https://latex.codecogs.com/png.latex?%5CDelta%20B"> は <img src="https://latex.codecogs.com/png.latex?x%5Cin%20B"> から始まった <img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E%7D"> の法線（面）で，<img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E+%5CDelta%20E%7D"> との交点で終わる線分の <img src="https://latex.codecogs.com/png.latex?x%5Cin%20B"> に関する合併である．</p>
<p>他にも，等位集合が <img src="https://latex.codecogs.com/png.latex?S_t"> によって保存される関数 <img src="https://latex.codecogs.com/png.latex?I_i:%5COmega_%7B%5CLambda,N%7D%5Cto%5Cmathbb%7BR%7D"> は存在し得て，その場合は全ての合併 <img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E,I_1,%5Ccdots,I_k%7D"> 上に小正準分布が遺伝する．</p>
</section>
<section id="エルゴード仮説" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="エルゴード仮説"><span class="header-section-number">1.4</span> エルゴード仮説</h3>
<p>Boltzman のエルゴード仮説は，他の積分 <img src="https://latex.codecogs.com/png.latex?I_i"> が存在しない場合，<img src="https://latex.codecogs.com/png.latex?%5COmega_%7B%5CLambda,N,E%7D"> 上の <img src="https://latex.codecogs.com/png.latex?S_t">-不変で <img src="https://latex.codecogs.com/png.latex?%5Cnu_%7B%5CLambda,N,E%7D">-絶対連続な測度は，<img src="https://latex.codecogs.com/png.latex?%5Cnu_%7B%5CLambda,N,E%7D"> の定数倍に限る，というものである．</p>
<p>これにより，特定の系が運動 <img src="https://latex.codecogs.com/png.latex?S_t"> によって平衡状態に至った際，相空間上を旅する際に特定の状態が現れる頻度分布は，必ず小正準分布に一致することが帰結される．</p>
</section>
<section id="configuration-gas" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="configuration-gas"><span class="header-section-number">1.5</span> configuration gas</h3>
<p>もし粒子が全て動いていないならば，相空間は <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7B3N%7D=:%5COmega_%7B%5CLambda,N%7D%5E%7B%5Ctext%7Bconf%7D%7D"> で良い．</p>
<p>これを <strong>configuration gas</strong> といい，古典粒子系の更なる理想化に当たる．</p>
</section>
</section>
<section id="格子模型" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="格子模型"><span class="header-section-number">2</span> 格子模型</h2>
<p>一部は実際の物理系の良いモデルとなっているが，より複雑なモデルの統計物理学的性質の良い第一近似としても用いられる．</p>
<p>加えて，このような離散的なグラフとしての表現を通じて，機械学習や情報科学との関わりを持つ．そのような分野は情報統計力学と呼ばれる <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003)</span>．<sup>5</sup></p>
<section id="格子気体" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="格子気体"><span class="header-section-number">2.1</span> 格子気体</h3>
<p>configuration gas から更なるモデルの簡略化を考える．</p>
<p>粒子の位置は必ず格子点上にあるとすれば，配置空間はさらに <img src="https://latex.codecogs.com/png.latex?%5CLambda%5Csubset%5Cmathbb%7BZ%7D%5E3"> に対して <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20&amp;%5COmega_%7B%5CLambda,N%7D=%5C%5C%0A%20%20%20%20&amp;%5Cleft%5C%7BQ=(q_1,%5Ccdots,q_N)%5Cin%5CLambda%5EN%5Cmid%20q_i%5Cne%20q_j%5C;(i%5Cne%20j)%5Cright%5C%7D%0A%5Cend%7Balign*%7D%0A"> と簡略化される．</p>
<p>仮に外場もないとすると，ハミルトニアンは単にポテンシャルの和 <img src="https://latex.codecogs.com/png.latex?%0AH(Q)=%5Csum_%7Bi%3Cj%7DU(q_i-q_j)%0A"> となる．</p>
</section>
<section id="スピン系" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="スピン系"><span class="header-section-number">2.2</span> スピン系</h3>
<p>有限集合 <img src="https://latex.codecogs.com/png.latex?%5CLambda%5Csubset%5Cmathbb%7BZ%7D%5E3"> が粒子で満ちているとすると，配置空間は縮退する．</p>
<p>その際のスピン系の相空間は，関数の集合 <img src="https://latex.codecogs.com/png.latex?%0A%5COmega_%5CLambda:=S%5E%5CLambda%0A"> になる．</p>
<p><img src="https://latex.codecogs.com/png.latex?S=%5Cpartial%20U_1(0)%5Csubset%5Cmathbb%7BR%7D%5E3"> の場合を <strong>平面回転子</strong> (planar rotator) モデルという．<sup>6</sup> <img src="https://latex.codecogs.com/png.latex?S=%5C%7B%5Cpm1%5C%7D"> としても（Ising spin），<sup>7</sup> 物理系のモデルとして磁性体の第一近似として使える模型になる．</p>
<p>ハミルトニアンは，相互作用と外場の和として <img src="https://latex.codecogs.com/png.latex?%0AH_%5CLambda(%5Csigma)=%5Csum_%7Bx%5Cne%20y%5Cin%5CLambda%7DU(x-y)%5Csigma(x)%5Csigma(y)+h%5Csum_%7Bx%5Cin%5CLambda%7D%5Csigma(x)%0A"> と与えられる．</p>
<section id="ising-模型" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="ising-模型"><span class="header-section-number">2.2.1</span> Ising 模型</h4>
<p>関数 <img src="https://latex.codecogs.com/png.latex?U"> が <img src="https://latex.codecogs.com/png.latex?U_1(0)%5Ccap%5Cmathbb%7BZ%7D%5E3"> を台に持つ場合を（狭義の） <strong>Ising 模型</strong> といい，<img src="https://latex.codecogs.com/png.latex?U"> は <img src="https://latex.codecogs.com/png.latex?J"> でも表す：<sup>8</sup> <img src="https://latex.codecogs.com/png.latex?%0AH_%5CLambda(%5Csigma)=-J_%7Bx,y%7D%5Csum_%7B%5Clvert%20x-y%5Crvert=1%7D%5Csigma(x)%5Csigma(y)-h%5Csum_%7Bx%5Cin%5CLambda%7D%5Csigma(x)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?J_%7Bx,y%7D%3E0"> である場合，低温秩序相は強磁性体のモデルになっている．<img src="https://latex.codecogs.com/png.latex?J_%7Bx,y%7D%5Cequiv%20J"> という定値性の仮定もよく置かれる．</p>
</section>
<section id="curie-weiss-模型" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="curie-weiss-模型"><span class="header-section-number">2.2.2</span> Curie-Weiss 模型</h4>
<p>この最近傍 Ising 模型に，Weiss 近似という平均場近似を施し，Curie-Weiss Hamiltonian <img src="https://latex.codecogs.com/png.latex?%0AH_%7B%5CLambda%7D(%5Csigma)=-%5Cfrac%7BdJ%7D%7B%5Clvert%5CGamma%5Crvert%7D%5Csum_%7Bx%5Cne%20y%5Cin%5CLambda%7D%5Csigma(x)%5Csigma(y)-h%5Csum_%7Bx%5Cin%5CLambda%7D%5Csigma(x)%0A"> を用いる模型を <a href="../../../posts/2024/Computation/PGM2.html#sec-Curie-Weiss">Curie-Weiss 模型</a> という．</p>
<p>相互作用がもはや最近傍同士ではなくなっている．これを <strong>無限レンジ模型</strong> ともいう <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003, p. 24)</span>．</p>
<p>この式からは，各スピンが，具体的な他のスピンと相互作用するというより，全ての他スピンからなる平均場（<strong>有効磁場</strong>）と相互作用していると読める．<sup>9</sup></p>
<p>このような平均場近似を施していても，低温の強磁性相と高温の常磁性相が別れることが観察される <span class="citation" data-cites="Friedli-Velenik2017">(Friedli &amp; Velenik, 2017, p. 62)</span>．</p>
</section>
</section>
<section id="スピングラス" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="スピングラス"><span class="header-section-number">2.3</span> スピングラス</h3>
<p><img src="https://latex.codecogs.com/png.latex?J_%7Bx,y%7D"> の符号がバラバラである場合，これをスピングラスの模型という．<sup>10</sup></p>
<p>このような模型では，低温秩序相が消えて，スピンがバラバラである状態（スピングラス相）も安定たり得ることがわかっている．</p>
<p>特に，安定な状態が複数存在し，温度を少し変えるだけで全く性質の異なる別の状態へ系が移ることもよくある．自由エネルギーが強い多峰性を示すのである．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Nature/SG.png" class="img-fluid figure-img"></p>
<figcaption><a href="https://ocw.u-tokyo.ac.jp/lecture_files/issp_02/1/notes/ja/takayama-lecture.pdf">最終講義 スピングラスと計算物性物理</a> p.28</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution callout-titled" title="スピングラスの例">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
スピングラスの例
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>CuMn などはスピン間の相互作用が，RKKY (Ruderman-Kittel-Kazuya-Yoshida) 相互作用 <img src="https://latex.codecogs.com/png.latex?%0AJ%5C,%5Cpropto%5C,%5Cfrac%7B%5Ccos(2k_Fr_%7B12%7D)%7D%7Br_%7B12%7D%5E3%7D%0A"> により表され，これは符号が分子の <img src="https://latex.codecogs.com/png.latex?%5Ccos"> により正にも負にもなり得る．</p>
<p>AuFe もスピングラスである <span class="citation" data-cites="Cannella-Mydosh1972">(Cannella &amp; Mydosh, 1972)</span>．<sup>11</sup></p>
</div>
</div>
</div>
<section id="edwards-anderson-模型-edwards-anderson1975" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="edwards-anderson-模型-edwards-anderson1975"><span class="header-section-number">2.3.1</span> Edwards-Anderson 模型 <span class="citation" data-cites="Edwards-Anderson1975">(Edwards &amp; Anderson, 1975)</span></h4>
<p><img src="https://latex.codecogs.com/png.latex?(J_%7Bx,y%7D)_%7B%5Clvert%20x-y%5Crvert=1%7D"> を，グラフのエッジの集合上に定義された独立な Gauss 確率場とし，外場を考えないものを，<strong>Edwards-Anderson 模型</strong> という： <img src="https://latex.codecogs.com/png.latex?%0AH(%5Csigma)=-%5Csum_%7B%5Clvert%20x-y%5Crvert=1%7DJ_%7Bx,y%7D%5Csigma(x)%5Csigma(y).%0A"></p>
<p>確率変数 <img src="https://latex.codecogs.com/png.latex?J_%7Bx,y%7D%5Csim%5Cmathrm%7BN%7D_1(0,N)"> の平均（配位平均）と，アンサンブル平均という２つの平均を扱う必要がある点で極めて難しい模型となっている．</p>
<p>この模型において自由エネルギーを計算するために，分配関数の対数の平均を，分配関数の積率によって計算する <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BE%7D%5B%5Clog%20Z%5D=%5Clim_%7Bn%5Cto%5Cinfty%7D%5Cfrac%7B%5Coperatorname%7BE%7D%5BZ%5En%5D-1%7D%7Bn%7D%0A"> という関係式を用いた．これを <strong>レプリカ法</strong> という．ただし，期待値は <img src="https://latex.codecogs.com/png.latex?J_%7Bx,y%7D"> に関するもので，アンサンブル平均 <img src="https://latex.codecogs.com/png.latex?%5Clangle-%5Crangle"> とは関係ないことに注意．</p>
<p>現状，特に Talagrand はレプリカ法の数学的妥当性について極めて懐疑的であるが，多くは数値実験により検証されており，何らかの本質を捉えていることは間違いない．<sup>12</sup></p>
</section>
<section id="sherrington-kirkpatrick-模型-sherrington-kirkpatrick1975" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="sherrington-kirkpatrick-模型-sherrington-kirkpatrick1975"><span class="header-section-number">2.3.2</span> Sherrington-Kirkpatrick 模型 <span class="citation" data-cites="Sherrington-Kirkpatrick1975">(Sherrington &amp; Kirkpatrick, 1975)</span></h4>
<p>EA 模型を無限レンジにすることで，平均場近似が厳密解を与えるようにし，熱力学極限 <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7BN%5Cto%5Cinfty%7D%5Cfrac%7B%5Coperatorname%7BE%7D%5B%5Clog%20Z_N%5D%7D%7BN%7D%0A"> を与えることでこれを解いたものである．その際にもレプリカ法が用いられた．</p>
<p>著者のうちの Scott Kirkpatrick は <a href="../../../posts/Surveys/SMCSamplers.html#sec-SA">擬似アニーリング</a> <span class="citation" data-cites="Kirkpartick+1983">(Kirkpartick et al., 1983)</span> の開発者でもある．</p>
<p>しかしこの解（レプリカ対称な解）は初め低温域では破綻を起こすとされていた．<span class="citation" data-cites="Parisi1980">(Parisi, 1980)</span> がこの問題を解決し，任意の温度 <img src="https://latex.codecogs.com/png.latex?T%3E0"> での厳密解（レプリカ対称性破れ解）が得られた．これは Parisi ansatz と呼ばれる．<sup>13</sup> この解は計算機シミュレーションと高い精度で一致し，常磁性相と強磁性相に加えて，スピングラス相を示す．</p>
<p>Parisi はこの業績で 2021 年にノーベル物理学賞を受賞した．その３番目に多く引用されている論文 <span class="citation" data-cites="Marinari-Parisi1992">(Marinari &amp; Parisi, 1992)</span> は <a href="../../../posts/Surveys/SMCSamplers.html#sec-ST"><strong>擬似テンパリング</strong></a> の提案論文である．</p>
</section>
<section id="thouless-anderson-plamer-方程式-thouless1977-と近似メッセージ伝播-bolthausen2014" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="thouless-anderson-plamer-方程式-thouless1977-と近似メッセージ伝播-bolthausen2014"><span class="header-section-number">2.3.3</span> Thouless-Anderson-Plamer 方程式 <span class="citation" data-cites="Thouless+1977">(Thouless et al., 1977)</span> と近似メッセージ伝播 <span class="citation" data-cites="Bolthausen2014">(Bolthausen, 2014)</span></h4>
<p>一方で，SK 模型に対して高温摂動展開により自由エネルギーを与えるアプローチもある．</p>
<p>特に，熱力学極限に向かって漸近的に成り立つ次の方程式を TAP 方程式という： <img src="https://latex.codecogs.com/png.latex?%0A%5Clangle%5Csigma_i%5Crangle%5Capprox%5Ctanh%5Cleft(%5Cfrac%7B%5Cbeta%7D%7B%5Csqrt%7BN%7D%7D%5Csum_%7Bj%5Cne%20i%7DJ_%7Bij%7D%5Clangle%5Csigma_j%5Crangle+h-%5Cbeta%5E2(1-q)%5Clangle%5Csigma_i%5Crangle%5Cright)%0A"></p>
<p>これの数学からの証明も近年試みられている <span class="citation" data-cites="Talagrand2003">(Talagrand, 2003)</span>, <span class="citation" data-cites="Chatterjee2010">(Chatterjee, 2010)</span>．しかし，厳密な証明は高温に限られ，低温域では解の一意性が失われるのが困難を窺わせる．２つの層の分離面としては Almeida-Thouless 線が提案されている．</p>
<p>しかし <span class="citation" data-cites="Bolthausen2014">(Bolthausen, 2014)</span> は <strong>近似メッセージパッシング</strong> に基づいて，この TAP 方程式の解を与えるアルゴリズムを提案した．このアルゴリズムは，高次元統計学において <img src="https://latex.codecogs.com/png.latex?M">-推定量を計算するのにも応用されている <span class="citation" data-cites="Donoho-Montanari2016">(Donoho &amp; Montanari, 2016)</span>．高次元漸近論は計算科学の進歩とともにあるのである．</p>
</section>
<section id="hopfield-模型-hopfield1982" class="level4" data-number="2.3.4">
<h4 data-number="2.3.4" class="anchored" data-anchor-id="hopfield-模型-hopfield1982"><span class="header-section-number">2.3.4</span> Hopfield 模型 <span class="citation" data-cites="Hopfield1982">(Hopfield, 1982)</span></h4>
<p>のちにスピングラスの理論は <strong>連想記憶</strong> にも応用され，広く情報処理の問題を統計力学の技法によって研究する情報統計力学という新たな分野が開拓された．</p>
<p><a href="../../../posts/2024/Kernels/Deep.html#sec-Hopfield">連想記憶のニューラルネットワーク</a> は無限レンジ，すなわち全結合のニューラルネットワークで，素子 <img src="https://latex.codecogs.com/png.latex?%5C%7BS_i%5C%7D_%7Bi=1%7D%5EN%5Csubset%5Cmathrm%7BMap%7D(T;%5C%7B%5Cpm1%5C%7D)"> からなるとき， <img src="https://latex.codecogs.com/png.latex?%0AS_i(t+%5CDelta%20t)=%5Cmathop%7B%5Cmathrm%7Bsgn%7D%7D%5Cleft(%5Csum_%7Bj%5Cne%20i%7DJ_%7Bij%7D%5Cfrac%7BS_j(t)+1%7D%7B2%7D-%5Ctheta_i%5Cright)%0A"> という規則で運動する．<img src="https://latex.codecogs.com/png.latex?J_%7Bij%7D,%5Ctheta_i"> がモデルパラメータである．</p>
<p><img src="https://latex.codecogs.com/png.latex?J_%7Bij%7D"> をうまく「学習」できた際には，一部の初期値について，この運動の収束先として画像が「連想」出来る．記憶しておけるのである．<sup>14</sup></p>
<p>実は，<img src="https://latex.codecogs.com/png.latex?p"> 個のパターン <img src="https://latex.codecogs.com/png.latex?(%5Cxi%5E%5Cmu_i)_%7Bi=1%7D%5EN%5Cin%5COmega%5C;(%5Cmu=1,%5Ccdots,p)"> を記憶させるには， <img src="https://latex.codecogs.com/png.latex?%0AJ_%7Bij%7D=1_%7B%5Cleft%5C%7Bi%5Cne%20j%5Cright%5C%7D%7D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7B%5Cmu=1%7D%5Ep%5Cxi_i%5E%5Cmu%5Cxi_j%5E%5Cmu%0A"> と設定すると良いことが知られており，これを <strong>Hebb 則</strong> <span class="citation" data-cites="Hebb1949">(Hebb, 1949)</span> という．</p>
<p>連想記憶がうまくいくためには，互いの直交性 <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7BN%7D(%5Cxi%5E%5Cmu%7C%5Cxi%5E%5Cnu)=%5Cdelta_%7B%5Cmu,%5Cnu%7D+O%5Cleft(%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7D%5Cright)%0A"> が重要であることも知られている．</p>
<p>実は，Hebb 則によるパラメータを備えた Hopfield 模型は，結合が対称である <img src="https://latex.codecogs.com/png.latex?J_%7Bij%7D=J_%7Bji%7D"> とき，次の Hamiltonian を減少させる方向に運動する： <img src="https://latex.codecogs.com/png.latex?%0AH=-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%5Cne%20j%7DJ_%7Bij%7DS_iS_j%0A"></p>
<p>ここで係数 <img src="https://latex.codecogs.com/png.latex?J_%7Bij%7D"> はデータ <img src="https://latex.codecogs.com/png.latex?(%5Cxi%5E%5Cmu)_%7B%5Cmu=1%7D%5Ep"> から決まっているという意味では，確率変数であることに注意．</p>
<p>この模型を統計力学的に解析すると，Hopfield 模型は，想起相だけでなく，常磁性相ももち，その間にスピングラス相がある．これは <img src="https://latex.codecogs.com/png.latex?%5Calpha=%5Cfrac%7Bp%7D%7BN%7D"> を大きくすると到達することができる <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003, p. 57)</span>．</p>
<p>素子数一定の状況下で，覚えるパターン数を増やしすぎると，ある瞬間に相転移を起こして何も覚えなくなるのである．</p>
</section>
</section>
<section id="因子グラフ" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="因子グラフ"><span class="header-section-number">2.4</span> 因子グラフ</h3>
<p>特に近距離相互作用のみを仮定している場面では，ハミルトニアン <img src="https://latex.codecogs.com/png.latex?H"> やその他の物理量の <strong>局所性</strong> が目立った（コンパクト台を持つ関数になっている）．</p>
<p>そのこともあり，物理系はグラフィカルモデル（Bayesian networks, Markov networks）としての表現と親和性があり，特に <a href="../../../posts/2024/Computation/PGM1.html#sec-Factor-Graph"><strong>因子グラフ</strong></a> も重要な形式として用いられる <span class="citation" data-cites="Mezard-Montanari2009">(Mézard &amp; Montanari, 2009, p. 100)</span>．</p>
</section>
</section>



<div id="quarto-appendix" class="default"><section id="参考文献" class="level2 appendix" data-number="3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">3</span> 参考文献</h2><div class="quarto-appendix-contents">

<p><span class="citation" data-cites="Minlos2000">(Minlos, 2000)</span>, <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003)</span>, <span class="citation" data-cites="Altieri-Baity-Jesi2024">(Altieri &amp; Baity-Jesi, 2024)</span>, <span class="citation" data-cites="Chatterjee2023">(Chatterjee, 2023)</span>, <span class="citation" data-cites="Panchenko2012">(Panchenko, 2012)</span>, <span class="citation" data-cites="Talagrand2003">(Talagrand, 2003)</span>, <span class="citation" data-cites="Bolthausen2014">(Bolthausen, 2014)</span></p>



</div></section><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Altieri-Baity-Jesi2024" class="csl-entry">
Altieri, A., &amp; Baity-Jesi, M. (2024). An introduction to the theory of spin glasses. In <em>Encyclopedia of condensed matter physics</em> (pp. 361–370). Elsevier. <a href="https://doi.org/10.1016/b978-0-323-90800-9.00249-3">https://doi.org/10.1016/b978-0-323-90800-9.00249-3</a>
</div>
<div id="ref-Baxter1982" class="csl-entry">
Baxter, R. J. (1982). <em>Exactly solved models in statistical mechanics</em>. Academic Press. <a href="https://physics.anu.edu.au/research/ftp/mpg/baxter_book.php">https://physics.anu.edu.au/research/ftp/mpg/baxter_book.php</a>
</div>
<div id="ref-Bolthausen2014" class="csl-entry">
Bolthausen, E. (2014). An iterative construction of solutions of the TAP equations for the sherrington–kirkpatrick model. <em>Communications in Mathematical Physics</em>, <em>325</em>(1), 333–366. <a href="https://doi.org/10.1007/s00220-013-1862-3">https://doi.org/10.1007/s00220-013-1862-3</a>
</div>
<div id="ref-Cannella-Mydosh1972" class="csl-entry">
Cannella, V., &amp; Mydosh, J. A. (1972). Magnetic ordering in gold-iron alloys. <em>Phys. Rev. B</em>, <em>6</em>, 4220–4237. <a href="https://doi.org/10.1103/PhysRevB.6.4220">https://doi.org/10.1103/PhysRevB.6.4220</a>
</div>
<div id="ref-Chatterjee2010" class="csl-entry">
Chatterjee, S. (2010). Spin glasses and stein’s method. <em>Probability Theory and Related Fields</em>, <em>148</em>(3), 567–600. <a href="https://doi.org/10.1007/s00440-009-0240-8">https://doi.org/10.1007/s00440-009-0240-8</a>
</div>
<div id="ref-Chatterjee2023" class="csl-entry">
Chatterjee, S. (2023). <em>Spin glass phase at zero temperature in the edwards-anderson model</em>. <a href="https://arxiv.org/abs/2301.04112">https://arxiv.org/abs/2301.04112</a>
</div>
<div id="ref-Donoho-Montanari2016" class="csl-entry">
Donoho, D., &amp; Montanari, A. (2016). High dimensional robust m-estimation: Asymptotic variance via approximate message passing. <em>Probability Theory and Related Fields</em>, <em>166</em>(3), 935–969. <a href="https://doi.org/10.1007/s00440-015-0675-z">https://doi.org/10.1007/s00440-015-0675-z</a>
</div>
<div id="ref-Edwards-Anderson1975" class="csl-entry">
Edwards, S. F., &amp; Anderson, P. W. (1975). Theory of spin glasses. <em>Journal of Physics F: Metal Physics</em>, <em>5</em>(5), 965. <a href="https://doi.org/10.1088/0305-4608/5/5/017">https://doi.org/10.1088/0305-4608/5/5/017</a>
</div>
<div id="ref-Friedli-Velenik2017" class="csl-entry">
Friedli, S., &amp; Velenik, Y. (2017). <em>Statistical mechanics of lattice systems</em>. Cambridge University Press.
</div>
<div id="ref-Hebb1949" class="csl-entry">
Hebb, D. O. (1949). <em>The organization of behavior: A neuropsychological theory</em>. John Wiley &amp; Sons, Chapman; Hall. <a href="https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb">https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb</a>
</div>
<div id="ref-Hopfield1982" class="csl-entry">
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. <em>Proceedings of the National Academy of Science</em>, <em>79</em>(8), 2554–2558. <a href="https://www.pnas.org/doi/10.1073/pnas.79.8.2554">https://www.pnas.org/doi/10.1073/pnas.79.8.2554</a>
</div>
<div id="ref-Kirkpartick+1983" class="csl-entry">
Kirkpartick, S., Gelatt, C. D., &amp; Vecchi, M. P. (1983). Optimization by simulated annealing. <em>Science</em>, <em>220</em>(4598), 671–680.
</div>
<div id="ref-Marinari-Parisi1992" class="csl-entry">
Marinari, E., &amp; Parisi, G. (1992). Simulated tempering: A new monte carlo scheme. <em>Europhysics Letters</em>, <em>19</em>(6), 451–458.
</div>
<div id="ref-Mezard-Montanari2009" class="csl-entry">
Mézard, M., &amp; Montanari, A. (2009). <em>Information, physics, and computation</em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780198570837.001.0001">https://doi.org/10.1093/acprof:oso/9780198570837.001.0001</a>
</div>
<div id="ref-Minlos2000" class="csl-entry">
Minlos, R. A. (2000). <em>Introduction to mathematical statistical physics</em> (Vol. 19). American Mathematical Society. <a href="https://doi.org/10.1090/ulect/019">https://doi.org/10.1090/ulect/019</a>
</div>
<div id="ref-Panchenko2012" class="csl-entry">
Panchenko, D. (2012). The sherrington-kirkpatrick model: An overview. <em>Journal of Statistical Physics</em>, <em>149</em>(2), 362–383. <a href="https://doi.org/10.1007/s10955-012-0586-7">https://doi.org/10.1007/s10955-012-0586-7</a>
</div>
<div id="ref-Parisi1980" class="csl-entry">
Parisi, G. (1980). A sequence of approximated solutions to the s-k model for spin glasses. <em>Journal of Physics A: Mathematical and General</em>, <em>13</em>(4), L115. <a href="https://doi.org/10.1088/0305-4470/13/4/009">https://doi.org/10.1088/0305-4470/13/4/009</a>
</div>
<div id="ref-Petersen1983" class="csl-entry">
Petersen, K. E. (1983). <em>Ergodic theory</em> (Vol. 2). Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511608728">https://doi.org/10.1017/CBO9780511608728</a>
</div>
<div id="ref-Sherrington-Kirkpatrick1975" class="csl-entry">
Sherrington, D., &amp; Kirkpatrick, S. (1975). Solvable model of a spin-glass. <em>Phys. Rev. Lett.</em>, <em>35</em>, 1792–1796. <a href="https://doi.org/10.1103/PhysRevLett.35.1792">https://doi.org/10.1103/PhysRevLett.35.1792</a>
</div>
<div id="ref-Talagrand2003" class="csl-entry">
Talagrand, M. (2003). <em>Spin glasses: A challenge for mathematicians: Cavity and mean field models</em>. Springer Berlin, Heidelberg. <a href="https://link.springer.com/book/9783540003564">https://link.springer.com/book/9783540003564</a>
</div>
<div id="ref-Thouless+1977" class="csl-entry">
Thouless, D. J., Anderson, P. W., &amp; Palmer, R. G. (1977). Solution of ’solvable model of a spin glass’. <em>Philosophical Magazine</em>, <em>35</em>(3), 593–601. <a href="https://doi.org/10.1080/14786437708235992">https://doi.org/10.1080/14786437708235992</a>
</div>
<div id="ref-田中利幸2007" class="csl-entry">
田中利幸. (2007). レプリカ法における解析接続について(情報物理学の数学的構造). <em>数理解析研究所講究録</em>, <em>1532</em>, 118–129. <a href="http://hdl.handle.net/2433/58952">http://hdl.handle.net/2433/58952</a>
</div>
<div id="ref-西森秀稔2003" class="csl-entry">
西森秀稔. (2003). <em>スピングラスと連想記憶</em>. 岩波書店. <a href="https://www.iwanami.co.jp/book/b476276.html">https://www.iwanami.co.jp/book/b476276.html</a>
</div>
<div id="ref-都福仁1977" class="csl-entry">
都福仁. (1977). スピングラス. <em>日本物理学会誌</em>, <em>32</em>(6), 463–473. <a href="https://doi.org/10.11316/butsuri1946.32.463">https://doi.org/10.11316/butsuri1946.32.463</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>すなわち，<img src="https://latex.codecogs.com/png.latex?%5CLambda"> 上の線型束である．↩︎</p></li>
<li id="fn2"><p><img src="https://latex.codecogs.com/png.latex?V"> は境界ポテンシャルといい，外場との相互作用がある場合に現れる．↩︎</p></li>
<li id="fn3"><p>これは粒子間相互作用が short-range であると仮定しているためである．重力やクーロン力を考えている訳ではない．↩︎</p></li>
<li id="fn4"><p><span class="citation" data-cites="Petersen1983">(Petersen, 1983, p. 6)</span> 命題2.2．↩︎</p></li>
<li id="fn5"><p><a href="https://www.ism.ac.jp/ism_info_j/labo/project/156.html">統数研プロジェクト紹介</a> も参照．↩︎</p></li>
<li id="fn6"><p><a href="https://ja.wikipedia.org/wiki/%E5%8F%A4%E5%85%B8%E3%83%8F%E3%82%A4%E3%82%BC%E3%83%B3%E3%83%99%E3%83%AB%E3%82%AF%E6%A8%A1%E5%9E%8B">古典 Heisenberg 模型</a> はその例である．↩︎</p></li>
<li id="fn7"><p><span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003, p. 4)</span>．情報統計力学では，スピンをビットやニューロンの状態に見立てる．↩︎</p></li>
<li id="fn8"><p><span class="citation" data-cites="Baxter1982">(Baxter, 1982, p. 21)</span> では nearest-neighbour Ising model と呼んでいる．↩︎</p></li>
<li id="fn9"><p>無限レンジの仮定をおくと，平均場近似は近似でなくなる，という論理的依存関係がある <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003, p. 26)</span>．↩︎</p></li>
<li id="fn10"><p><span class="citation" data-cites="Mezard-Montanari2009">(Mézard &amp; Montanari, 2009, p. 168)</span> <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003, p. 16)</span> など．スピングラスはもともと B. Coles が希薄合金の磁性を表現するために造語したが，現在は「全くランダムな状態でスピンが凍結した状態」という意味で使われるようになている <span class="citation" data-cites="都福仁1977">(都福仁, 1977)</span>．↩︎</p></li>
<li id="fn11"><p>Mydosh による講演が，髙山一氏のスピングラス研究の発端となったという（<a href="https://ocw.u-tokyo.ac.jp/lecture_files/issp_02/1/notes/ja/takayama-lecture.pdf">最終講義</a>）．↩︎</p></li>
<li id="fn12"><p><span class="citation" data-cites="田中利幸2007">(田中利幸, 2007)</span>↩︎</p></li>
<li id="fn13"><p><span class="citation" data-cites="Panchenko2012">(Panchenko, 2012)</span> も参照．↩︎</p></li>
<li id="fn14"><p>パターンを記憶させることを「埋め込む」ともいう <span class="citation" data-cites="西森秀稔2003">(西森秀稔, 2003, p. 33)</span>．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Nature</category>
  <category>Deep</category>
  <guid>https://162348.github.io/posts/2024/Nature/StatisticalMechanics1.html</guid>
  <pubDate>Sat, 06 Apr 2024 15:00:00 GMT</pubDate>
</item>
<item>
  <title>計算とは何か</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Computation/AboutSimulation.html</link>
  <description><![CDATA[ 





<section id="今後の世界においてシミュレーションが果たす役割はどこまで拡大するのだろうか" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="今後の世界においてシミュレーションが果たす役割はどこまで拡大するのだろうか"><span class="header-section-number">1</span> 今後の世界においてシミュレーションが果たす役割はどこまで拡大するのだろうか？</h2>
<p>２つの方向で「シミュレーション」へのアプローチが急速に進みつつある．</p>
<section id="sec-physical-simulation" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="sec-physical-simulation"><span class="header-section-number">1.1</span> 物理学的シミュレーション</h3>
<p>従来物理学においては，実験と理論が相補的な関係にあった．</p>
<blockquote class="blockquote">
<p>If it disagrees with experiment, it’s wrong. In that simple statement, is the key to science. <span class="citation" data-cites="Feynman1964">(Feynman, 1964)</span></p>
</blockquote>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/EYPapE-3FRw?si=YmbUd1k8NCZo0s4k" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>しかし今後は，計算機シミュレーションと Monte Carlo 法が新時代の科学の第三の要素になるのかも知れない．</p>
<blockquote class="blockquote">
<p>These are some of the reasons why computer simulation has recently emerged as a third way in science besides the experimental and theoretical approach. <span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, p. 2)</span></p>
</blockquote>
<p>素粒子論など，実験が出来ないために理論の検証がまたれている場面では，計算機シミュレーションが有望な手法になる．また半導体や製薬業界など，実験が高くつく産業では急速に計算機シミュレーションによる代替が進んでいる．</p>
<p>一方で，多体問題などの解析的な手法では刃が立たない場面では，数値計算が唯一の理論的進歩を与えてくれる手法になる．<strong>数値実験</strong> という言葉に，「実験」の語が含まれている理由である．</p>
<blockquote class="blockquote">
<p>The rapid development of parallel computing systems made it possible to recreate and predict physical processes on computers. <span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, Preface)</span></p>
</blockquote>
<p>しかし，これには新たな数理の発展が必要である．実験結果には統計的な解釈を加えないと無意味であったように，シミュレーションも新たな統計学・機械学習と共に科学に資される必要がある．</p>
<blockquote class="blockquote">
<p>Experimental practice rests on a long (occasionally blemished) tradition; computer simulation, because of its novelty, is still somewhat more haphazard, but methodologies are gradually evolving. <strong>The output of any simulation should be treated by the same statistical methods used in the analysis of experiments</strong>. <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, pp. 2–3)</span></p>
</blockquote>
<p>筆者は学部入学直後に物理学実験という全理系学生必修の講義を経験し，その初回講義は不確実性の扱いについてであった．実際に講堂に集まった全学生でサイコロをふり，記録をとり，大数の法則を体験するという極めて印象深い授業であった．</p>
<p>未来の物理学実験の授業は，第２回授業ではコンピュータシミュレーションを扱うようになるのかも知れない．</p>
</section>
<section id="意味論的シミュレーション" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="意味論的シミュレーション"><span class="header-section-number">1.2</span> 意味論的シミュレーション</h3>
<p>大規模言語モデル（LLM）は，物理学的なシミュレーションと相補的な世界モデルを提供しつつある．</p>
</section>
</section>
<section id="計算とシミュレーションの違いとは何だろうか" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="計算とシミュレーションの違いとは何だろうか"><span class="header-section-number">2</span> 計算とシミュレーションの違いとは何だろうか？</h2>
<p>第 1.1 節で指摘した通り，シミュレーションとは本来は計算とは関係がなく，ある物理的な過程を別の物理的な過程によって模倣する行為である．</p>
<blockquote class="blockquote">
<p>What distinguishes computer simulation in general from other forms of computation, if such a distinction can be made, is the manner in which the computer is used: instead of merely performing a calculation, <strong>the computer becomes the virtual laboratory in which a system is studied - a numerical experiment</strong>. <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, p. 3)</span></p>
</blockquote>
<p>LLM では文字が，数値実験では数字が表象となっているに過ぎない．ここで文字や数字は，人間が人間の理解のために採用している形式である．VAE の中間層に特殊な形式を形式を矯正しているに過ぎない．</p>
<p>すると，ここを最適な形式によって相互接続し，end-to-end にすることで更なる効率化を図ろうとする論理が考えられるが，これが機械学習の悲願なのかも知れない．</p>
<section id="buffon-の針と-monte-carlo-積分法" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="buffon-の針と-monte-carlo-積分法"><span class="header-section-number">2.1</span> Buffon の針と Monte Carlo 積分法</h3>
<p>LLM が計算によりシミュレーションを実行する手法であるとしたら，Monte Carlo 積分法は，シミュレーションによって計算を実行する手法である．</p>
<p><span class="citation" data-cites="Liu2004">(Liu, 2004 Preface)</span> で Monte Carlo computation の最も初源的なアイデアは Buffon の針にあると指摘されている．</p>
<blockquote class="blockquote">
<p>The idea of simulating random processes so as to help evaluate certain quantities of interest is now an essential part of scientific computing. <span class="citation" data-cites="Liu2004">(Liu, 2004)</span></p>
</blockquote>
<p>となると，Bayesian computation とは，本質的にシミュレーションによるコンピューテーションへの反逆なのかもしれない．</p>
<div class="table-responsive-sm">
<table class="table-striped table-hover table">
<caption>Contrast of the two main approachs to Machine Learning</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Bayesian</th>
<th style="text-align: center;">Frequentist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Inference is<sup>1</sup></td>
<td style="text-align: center;">Marginalization</td>
<td style="text-align: center;">Approximation</td>
</tr>
<tr class="even">
<td style="text-align: center;">Mathematical Idea</td>
<td style="text-align: center;">Integration</td>
<td style="text-align: center;">Differentiation<sup>2</sup></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Computational Idea<sup>3</sup></td>
<td style="text-align: center;">Integration</td>
<td style="text-align: center;">Optimization</td>
</tr>
<tr class="even">
<td style="text-align: center;">Computational Solution</td>
<td style="text-align: center;">Simulation</td>
<td style="text-align: center;">Computation</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">Uncertainty Quantification</td>
<td style="text-align: center;">Recovery of True Value</td>
</tr>
<tr class="even">
<td style="text-align: center;">Emphasis</td>
<td style="text-align: center;">Modelling</td>
<td style="text-align: center;">Inference</td>
</tr>
</tbody>
</table>
</div>
<p>重要な観察として，</p>
<ul>
<li>決定論的な数値計算法の方が収束が速い．グリッド法などは <img src="https://latex.codecogs.com/png.latex?O(n%5E%7B-1%7D)">．</li>
<li>しかし，シミュレーションベースの方法の方がスケールする．Monte Carlo 積分法にしか太刀打ちできない領域は大きい上に，スケールする MCMC の最も重要な性質は，不偏推定量を通じてサブサンプルのみによる実行が可能である点である．</li>
</ul>
<p>という２点があり，Bayesian / Frequentist 双対性の背後に隠れているものと似ている．</p>
</section>
<section id="最適化-シミュレーションの双対性" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="最適化-シミュレーションの双対性"><span class="header-section-number">2.2</span> 最適化-シミュレーションの双対性</h3>
<p>積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．</p>
<p>また，最適化問題は simulated annealing <span class="citation" data-cites="Kirkpartick+1983">(Kirkpartick et al., 1983)</span> を通じてサンプリング問題としても解ける．</p>
</section>
</section>
<section id="bayesian-computation-に賭ける思い" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="bayesian-computation-に賭ける思い"><span class="header-section-number">3</span> Bayesian Computation に賭ける思い</h2>
<p>もしかしたら，MCMC はシミュレーションで，SMC はコンピューテーションであるというべきなのかも知れない．前者の活躍の場は，誕生から革新まで，ずっと物理学と物質科学にイニシアティブがあった．後者は防衛目的をはじめとした computer vision の文脈から生まれ，現在も機械学習など計算機方面の応用と親和性が高い．</p>
<p>２つは粒子 (particle) というキーワードでつながり合っており，２大ベイズ計算手法として双璧をなしている．</p>
<p>この２つのサンプリング手法はきっと相補的な役割を果たしながら，これからも大きく発展して固有の立ち位置を占めることになるだろう．僕をどのような旅に導いてくれるのだろうか．いずれにしろ，たくさんの人と関わることが出来ると思うと，非常にワクワクするし，良いテーマに出会ったなという感謝の気持ちでいっぱいになる．</p>
<p>この２つの世界樹の上から，自然科学と社会科学，計算機と人間，数学と言葉，どれもバランスよく考え続けることができたらと願っている．</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Feynman1964" class="csl-entry">
Feynman, R. P. (1964). <em>Seeking new laws</em>. <a href="https://www.feynmanlectures.caltech.edu/messenger.html">https://www.feynmanlectures.caltech.edu/messenger.html</a>
</div>
<div id="ref-Ghahramani2015" class="csl-entry">
Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. <em>Nature</em>, <em>521</em>, 452–459. <a href="https://www.nature.com/articles/nature14541">https://www.nature.com/articles/nature14541</a>
</div>
<div id="ref-Griebel+2007" class="csl-entry">
Griebel, M., Zumbusch, G., &amp; Knapek, S. (2007). <em>Numerical simulation in molecular dynamics: Numerics, algorithms, parallelization, applications</em> (Vol. 5). Springer Belin, Heidelberg. <a href="https://link.springer.com/book/10.1007/978-3-540-68095-6">https://link.springer.com/book/10.1007/978-3-540-68095-6</a>
</div>
<div id="ref-Kirkpartick+1983" class="csl-entry">
Kirkpartick, S., Gelatt, C. D., &amp; Vecchi, M. P. (1983). Optimization by simulated annealing. <em>Science</em>, <em>220</em>(4598), 671–680.
</div>
<div id="ref-Liu2004" class="csl-entry">
Liu, J. S. (2004). <em>Monte carlo strategies in scientific computing</em>. Springer New York. <a href="https://doi.org/10.1007/978-0-387-76371-2">https://doi.org/10.1007/978-0-387-76371-2</a>
</div>
<div id="ref-Rapaport2004" class="csl-entry">
Rapaport, D. C. (2004). <em>The art of molecular dynamics simulation</em> (2nd ed.). Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511816581">https://doi.org/10.1017/CBO9780511816581</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>この違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 454)</span>．↩︎</p></li>
<li id="fn2"><p>or, variation.↩︎</p></li>
<li id="fn3"><p>“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 454)</span>．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Computation</category>
  <category>Simulation</category>
  <category>Opinion</category>
  <guid>https://162348.github.io/posts/2024/Computation/AboutSimulation.html</guid>
  <pubDate>Fri, 05 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Computation/Files/computation.png" medium="image" type="image/png" height="146" width="144"/>
</item>
<item>
  <title>Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Review/Peters-deWith2012.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="概要" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="概要"><span class="header-section-number">1</span> 概要</h2>
<p>統計用語でいう Bouncy Particle Sampler を，Metropolis-Hastings 法の連続時間極限として初めて提案した論文であるが，これが <span class="citation" data-cites="Bouchard-Cote+2018-BPS">(Bouchard-Côté et al., 2018)</span> に発見されるには６年の時間を要した．</p>
<p>これを正準分布からのサンプリング法として導入した <span class="citation" data-cites="Peters-deWith2012">(Peters &amp; de&nbsp;With, 2012)</span> ではこの手法を event-driven rejection-free Monte Carlo 法と呼ぶ．連続なポテンシャル <img src="https://latex.codecogs.com/png.latex?E"> を用いることが可能なことが特徴．event-driven アプローチだけでなく，event-chain アプローチでの実装も扱った．Lennard-Jones ポテンシャルを持った流体のシミュレーションで検証した．</p>
<section id="備考" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="備考"><span class="header-section-number">1.1</span> 備考</h3>
<ul>
<li>ここでは本質的には連続時間極限をとっているが，直接議論されているのはポテンシャルの細分化の連続極限である．
<ul>
<li><img src="https://latex.codecogs.com/png.latex?U(x(t))"> があったとき，<img src="https://latex.codecogs.com/png.latex?%5CDelta%20U%5Cto0"> の極限を考えているために，実質的に <img src="https://latex.codecogs.com/png.latex?%5CDelta%20t%5Cto0"> を議論している．</li>
</ul></li>
<li>多粒子系のなすカノニカル分布からのサンプリングを考えているため，分布が積の形に分解できることを前提としている．</li>
<li>SEC <span class="citation" data-cites="Bernard+2009">(Bernard et al., 2009)</span> からの改良と読める．</li>
</ul>
</section>
</section>
<section id="背景" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="背景"><span class="header-section-number">2</span> 背景</h2>
<section id="time-driven-から-event-driven-へ" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="time-driven-から-event-driven-へ"><span class="header-section-number">2.1</span> time-driven から event-driven へ</h3>
<p>（古典）統計力学に従って粒子系をシミュレーションする際には，MD (molecular dynamics) と MC (Monte Carlo methods) の組み合わせ <span class="citation" data-cites="Duane+1987">(Duane et al., 1987)</span> を用いることになるが，現状いずれも Metropolis <span class="citation" data-cites="Metropolis+1953">(Metropolis et al., 1953)</span> の採択-棄却の枠組みで実行されるのが最も一般的である．</p>
<p>Metropolis の方法だと，</p>
<ul>
<li>凝縮系では MD 法としてのタイムステップは小さく取る必要があると同時に，他の粒子と重なってしまいやすく，棄却率が高くなる．</li>
<li>希薄系ではタイムスケールは分子の衝突過程に依存するので，シミュレーション時間のほとんどは無駄に使う必要が出る．</li>
</ul>
<p>という難点がある．</p>
<p>同様にして，time-driven MD も剛体球などの撃力相互作用を及ぼしあう系ではうまく働かないため，明らかに event-driven な方がいい．これは最初の MD 法 <span class="citation" data-cites="Alder-Wainwright1959">(Alder &amp; Wainwright, 1959)</span> で取られた通りの作戦である．</p>
<p>いずれの場合も event-driven な手法を用いることで効率化される可能性がある．</p>
</section>
<section id="ed-md-の改良" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="ed-md-の改良"><span class="header-section-number">2.2</span> ED-MD の改良</h3>
<p>かといって event-driven MD にも難点がある．</p>
<p>ED-MD はポテンシャルが単関数である場合にまでしか一般化できない．単関数でないと，いつイベントが起こるかが予測できないのである．</p>
<p>しかし本手法は，イベントを Poisson 過程でシミュレーションすることにしたので，一般のポテンシャルに適用できる（ Section&nbsp;2.6 も参照）．</p>
<p>また，衝突のモデリングも event-chain 手法 <span class="citation" data-cites="Bernard+2009">(Bernard et al., 2009)</span>, <span class="citation" data-cites="Bernard-Krauth2011">(Bernard &amp; Krauth, 2011)</span> 同様，選択の余地があり，SEC のように衝突時の動きを被衝突粒子が模倣するでも良いし，普通にビリヤードのように Newton 力学的衝突をモデリングするのでも良い．</p>
</section>
<section id="他の手法との比較" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="他の手法との比較"><span class="header-section-number">2.3</span> 他の手法との比較</h3>
<p>アルゴリズム的には kinetic / dynamic MC <span class="citation" data-cites="Fichthorn-Weinberg1991">(Fichthorn &amp; Weinberg, 1991)</span> <img src="https://latex.codecogs.com/png.latex?n">-fold way MC simulation <span class="citation" data-cites="Bortz+1975">(Bortz et al., 1975)</span> に似て Poisson 過程のシミュレーションに帰着する．</p>
<p>だが本手法は，イベントをシミュレーションしたのちに，そのイベントの間の粒子系の動きは線形に補間される．</p>
</section>
<section id="分子動力学" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="分子動力学"><span class="header-section-number">2.4</span> 分子動力学</h3>
<p>分子動力学法 (MD: Molecular Dynamics simulation) は，系のミクロなダイナミクスを実験的に調べる代わりに，シミュレーションによって必要条件を絞り込んでいく構成的な・計算機集約的なアプローチである．</p>
<p>特に，多体系の運動方程式を計算機を用いて数値的に解くことを指す <span class="citation" data-cites="Alder-Wainwright1959">(Alder &amp; Wainwright, 1959)</span>．それ故，物理や化学はもちろん，生化学，物質科学，さらには工学分野にまで幅広く用いられるシミュレーション法になる．</p>
<blockquote class="blockquote">
<p>the inescapable conclusion is that MD will — if it hasn’t already — become an indispensable part of the theorist’s toolbox. <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, p. ix)</span></p>
</blockquote>
<p>従来は実験と理論が相補的な関係にあったが，まるで基盤モデルが強化学習のための効率的な世界モデルになるように，計算機シミュレーションと Monte Carlo 法が新時代の科学の第三の要素になるのかも知れない．<sup>1</sup></p>
<p>アイデア自体は当然極めてナイーブであり，Laplace の悪魔や気体分子運動論の時代から大変に意識されたが，その最初の非自明な適用はデジタルコンピュータ完成のあとであった <span class="citation" data-cites="Alder-Wainwright1958">(Alder &amp; Wainwright, 1958)</span>, <span class="citation" data-cites="Alder-Wainwright1959">(Alder &amp; Wainwright, 1959)</span>．</p>
<p>彼らの関心は，ポロニウムなどの剛体球 (hard-sphere) の系とみれる物質の温度に依存した相転移現象にあった <span class="citation" data-cites="Alder-Wainwright1957">(Alder &amp; Wainwright, 1957)</span>．<span class="citation" data-cites="Alder-Wainwright1959">(Alder &amp; Wainwright, 1959)</span> では撃力相互作用 (impulsive interaction) が問題であったので，event-driven なシミュレーション (ED-MD scheme) が取られていたことは <span class="citation" data-cites="Peters-deWith2012">(Peters &amp; de&nbsp;With, 2012)</span> でも言及されている．当時の計算機では，500個の粒子を扱うことが限界であったことが興味深い．</p>
<p>その他に，粒子に内部構造がない場合は time-driven な Newton 力学のシミュレーションで良いかもしれないが，剛体で体積を持つ場合は Euler 方程式，内部構造を持つ場合は Langrange 方程式のシミュレーションも伴うかもしれない <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, p. 4)</span>．</p>
<p>通常の平衡状態の（エネルギー・体積・粒子数一定の）分子系は小正準集団に対応するが，特定の温度条件下での物性を考えたい場合は，運動方程式を修正してシミュレーションする方法もある．この方法の欠点は，物性を再現できても，個々の分子の軌道を正しく模倣しているわけではないという点であるが，そもそもそのカオス的な振る舞いから，正しい軌道を得ることは諦めることが多い．このこともあり，MD 法で用いられる数値積分法は比較的低次元で軽量なものでも十分なのである <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, p. 4)</span>．<sup>2</sup></p>
<section id="他手法の可能性" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="他手法の可能性"><span class="header-section-number">2.4.1</span> 他手法の可能性</h4>
<p>セルオートマトンや格子ボルツマン法などの格子ベースの手法の方が計算量は安価であるが，表現力に劣ることになる．</p>
<p>Lanvegin 方程式に基づいた Brownian dynamics などのさらに連続時間ベースの手法もある．</p>
</section>
</section>
<section id="統計力学で用いる分子動力学法" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="統計力学で用いる分子動力学法"><span class="header-section-number">2.5</span> 統計力学で用いる分子動力学法</h3>
<p>統計力学で用いられる Monte Carlo 法と molecular dynamics には極めて深い繋がりがある．</p>
<section id="布置平均を取る方法" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="布置平均を取る方法"><span class="header-section-number">2.5.1</span> 布置平均を取る方法</h4>
<p>一度 MD の文脈を取り去り，純粋に統計力学的な量を Monte Carlo 法によって計算したいとする．</p>
<p>やりたいことは抽象的には，その統計的集団の Boltzmann 分布から系をサンプリングできれば良い <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 184)</span>．</p>
<p>典型的には MCMC によって行うこととなるが，結局その提案は MD によって示唆されたものの方が効率が良くなる．</p>
<blockquote class="blockquote">
<p>The advantage of the MD proposal is that the resulting MCMC moves follow the dynamics of the target distribution more closely. <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 184)</span></p>
</blockquote>
<p>これは結局，背後の物理学的な本質を捉えているためとも言える</p>
<blockquote class="blockquote">
<p>A major advantage of molecular dynamics simulation in physical systems is its reliance on basic physics principles (e.g.&nbsp;Newton’s equation), which has been shown by nature to work well. <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 189)</span></p>
</blockquote>
</section>
<section id="時間平均を取る方法" class="level4" data-number="2.5.2">
<h4 data-number="2.5.2" class="anchored" data-anchor-id="時間平均を取る方法"><span class="header-section-number">2.5.2</span> 時間平均を取る方法</h4>
<p>統計力学において，マクロ物理量はアンサンブル平均として計算され，エルゴード仮説の下では１つの系の時間発展を追うことで計算できることになる．</p>
<p>これに MD を用いるとすると，自然と Monte Carlo シミュレーションのサブルーチンとして MD を用いることになるが，タイムステップが系の特徴的な事象をよく捉えるように注意して取る必要がある．特に，系のハミルトニアンが変化しすぎないように離散化誤差を抑える必要がある <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 184)</span>．</p>
<p>特に MD シミュレーション自体が（温度と粒子数が一定な）小正準集団に対応することになる <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, p. 6)</span>．</p>
<blockquote class="blockquote">
<p><strong>混合モンテカルロ／分子動力学法</strong>：モンテカルロ法を用いた構造分布の生成において，試行に用いる構造をニュートンの運動方程式など決定論的手法に従って用意する手法．分子凝集系などの複雑な系に対して，効率的に構造空間探索を行えると期待される．<span class="citation" data-cites="用語解説2022">(栗﨑 &amp; 田中, 2022)</span></p>
</blockquote>
</section>
<section id="optimal-scaling-の問題" class="level4" data-number="2.5.3">
<h4 data-number="2.5.3" class="anchored" data-anchor-id="optimal-scaling-の問題"><span class="header-section-number">2.5.3</span> Optimal scaling の問題</h4>
<p>ここで，optimal scaling と並行な，<strong>タイムステップ選択の問題</strong> がやはり中核となることが確認された．</p>
<blockquote class="blockquote">
<p>how to choose a good step size has always been an art in the field. <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 183)</span></p>
</blockquote>
<p>特に，凝縮系において系を放置し過ぎると，すぐにハミルトニアンが保存されなくなってしまうため，MD を用いた Monte Carlo 法では極めてタイムステップを細かくすることが必須であることが一番の問題である．</p>
<blockquote class="blockquote">
<p>a main problem with MD simulation is the stringent requirement of a small time-step size. <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 189)</span><sup>3</sup></p>
</blockquote>
<p>かといって，MD から遊離した MCMC を実行して性能を上げるためには，何かしらの形で遷移核に分布の形状を通知したいところであるが，これは MD 法なしには難しいということになる．<sup>4</sup></p>
<p>明らかに分布が特殊な形状をしているのに，ランダムウォークがその方向を見つけるまで待つのでは効率が悪い．</p>
</section>
<section id="hamiltonian-monte-carlo-法" class="level4" data-number="2.5.4">
<h4 data-number="2.5.4" class="anchored" data-anchor-id="hamiltonian-monte-carlo-法"><span class="header-section-number">2.5.4</span> Hamiltonian Monte Carlo 法</h4>
<p>そこで，完全な MD シミュレーションは行わず，Hamiltonian からの知識を部分的に提案核に取り入れるというアイデアが HMC (hybrid Monte Carlo) <span class="citation" data-cites="Duane+1987">(Duane et al., 1987)</span> として示された．<sup>5</sup></p>
<p>後に，統計力学的な背景を持たないサンプリング問題についても，Hamilton 系から示唆された提案分布を用いる方が MCMC として性能が良いことが発見された．これが HMC (Hamiltonian Monte Carlo) 法 <span class="citation" data-cites="Neal1996">(Neal, 1996)</span> である．</p>
</section>
</section>
<section id="sec-effectiveness-of-potentials" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="sec-effectiveness-of-potentials"><span class="header-section-number">2.6</span> ポテンシャルの役割</h3>
<p>例え量子系のシミュレーションであろうとも，Born-Oppenheimer などの近似を重ねて，原子に働く有効ポテンシャルにのみ注目することにより，古典系のシミュレーションと同様の方法で実行することができ，多くのマクロ的な性質を再現することが出来ることが，<span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, pp. 17–17)</span> に詳細に説明されている．</p>
<p>実際の系での粒子間相互作用を決定する際，純粋に Schrödinger 方程式などから理論的に導出するだけでなく，解析的に記述されたポテンシャルをフィッティングしてみてそれが理論や実験に合致するかどうかを見る統計的な方法もとられる <span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, p. 27)</span>．このようなモデル選択の立場に立つのが良い <span class="citation" data-cites="戸田+2011">(戸田盛和 et al., 2011, p. 34)</span>．</p>
<p>多くの場合ポテンシャルは，粒子間距離や角度，座標などの変数が入っており，これらのパラメータを推定することで探されるが，当然このステップは難しいものである：</p>
<blockquote class="blockquote">
<p>The construction of good potentials is still a form of art and requires much skill, work, and intuition. <span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, p. 28)</span></p>
</blockquote>
<p>Morse ポテンシャルや Lennard-Jones ポテンシャルがその代表例である．いわば，これらのポテンシャルも模型なのである．半導体のモデリングなどでは，さらに複雑なポテンシャルが必要になる <span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, p. 30)</span>．</p>
<p>このように，多体問題をポテンシャル関数によって解く手法は 1980 年代からである <span class="citation" data-cites="Griebel+2007">(Griebel et al., 2007, p. 30)</span>．ポテンシャル関数の形によって、適した数値計算アルゴリズムが異なる。特に、長距離力の高速計算のためには、ポテンシャルの性質を巧みに利用したアルゴリズムが必要である。</p>
<section id="システム同定としての物理学" class="level4" data-number="2.6.1">
<h4 data-number="2.6.1" class="anchored" data-anchor-id="システム同定としての物理学"><span class="header-section-number">2.6.1</span> システム同定としての物理学</h4>
<blockquote class="blockquote">
<p>What do we mean by “understanding” something? We can imagine that this complicated array of moving things which constitutes “the world” is something like a great chess game being played by the gods, and we are observers of the game. <span class="citation" data-cites="Fenyman+1963">(Feynman et al., 1964)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>今まで見てきたように，状態空間モデルは運動学習過程を統一的に説明するモデルであり，最適推定や最適制御の定式化に欠かせない．一方，状態空間モデルに含まれる行列の値は，運動方程式などといった物理的要請から決定できることもあるが，運動適応の学習係数などは未知のパラメータであることが多い．従って，与えられた実験データから状態空間モデルのパラメタを決定する必要がある．このパラメタ推定は制御理論において <strong>システム同定</strong> と呼ばれる．冒頭の引用のように，ファインマンは数学や物理を自然界のシステム同定とみなした．<span class="citation" data-cites="田中宏和2019">(田中宏和, 2019, p. 173)</span></p>
</blockquote>
</section>
</section>
</section>
<section id="本論" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="本論"><span class="header-section-number">3</span> 本論</h2>
<p>Metropolis scheme のように提案と棄却によって詳細釣り合い条件を満たすのではなく，棄却するところを衝突にすることによって詳細釣り合い条件を達成することを考える．</p>
<p>この状態から連続極限を考えていく．</p>
<p>まず，<img src="https://latex.codecogs.com/png.latex?U"> が階段関数であるとし，<img src="https://latex.codecogs.com/png.latex?q(x,y)"> が提案されたとすると，登った段数の分だけ独立な採択-棄却を実行することで，1回の採択-棄却の手続きを代用できる（ <span class="citation" data-cites="Tartero-Krauth2023">Tartero &amp; Krauth (2023)</span> の consensus 方式）．すなわち，最終的な採択確率は <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20P_%7B%5Ctext%7Bno-coll%7D%7D(x,y)&amp;=%5Cprod_i1%5Cland%20e%5E%7B-%5Cbeta%5CDelta%20U_i%7D%5C%5C%0A%20%20%20%20&amp;=%5Cexp%5Cleft(-%5Cbeta%5Csum_%7Bi%7D(%5CDelta%20U_i)_+%5Cright)%0A%5Cend%7Balign*%7D%0A"></p>
<p>こうして，連続なポテンシャルを単関数近似して，同様の手続きを実行するアイデアが考えられるが，ここではより洗練された手法を考える．</p>
<p>ポテンシャルのステップ <img src="https://latex.codecogs.com/png.latex?%5CDelta%20U"> が <img src="https://latex.codecogs.com/png.latex?0"> に近づくという連続極限を取ることで，「どの時点まで衝突せずに直進できるか」を計算することに帰着する．例えば時刻 <img src="https://latex.codecogs.com/png.latex?%5Bs,s_0%5D"> の間に衝突しない確率は <img src="https://latex.codecogs.com/png.latex?%0AP_%7B%5Ctext%7Bno-coll%7D%7D(s)=%5Cexp%5Cleft(-%5Cbeta%5Cint_s%5E%7Bs_0%7D(D_t%20U(x(t)))_+dt%5Cright)%0A"> となる．</p>
<p>実際に，衝突する時刻を求めるには，採択-棄却手続きのための一様変数 <img src="https://latex.codecogs.com/png.latex?u%5Csim%5Cmathrm%7BU%7D(%5B0,1%5D)"> を取り，これに対して <img src="https://latex.codecogs.com/png.latex?u=P_%7B%5Ctext%7Bno-coll%7D%7D(s)"> を満たす時刻 <img src="https://latex.codecogs.com/png.latex?s"> を計算すれば良いのである： <img src="https://latex.codecogs.com/png.latex?%0A-k_BT%5Clog%20u=%5Cint%5Es_%7Bs_0%7D(D_tU(x(t)))_+dt.%0A"></p>
<p>衝突のモデルが，例えば新しい速度 <img src="https://latex.codecogs.com/png.latex?v"> を「対称」に決めるようなものであったならば，この手法は対称な手法で詳細釣り合い条件を満たす．ここら辺も evnet-chain Monte Carlo <span class="citation" data-cites="Bernard+2009">(Bernard et al., 2009)</span> に似ている．</p>
</section>
<section id="多粒子系での実装" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="多粒子系での実装"><span class="header-section-number">4</span> 多粒子系での実装</h2>
<p>ポテンシャル <img src="https://latex.codecogs.com/png.latex?U=%5Csum_%7B%5Calpha%5Cin%5CLambda%7DU_%5Calpha"> を持つ多粒子系を考え，衝突規則とその実装の例を提示している．</p>
<p>このとき，粒子の速度 <img src="https://latex.codecogs.com/png.latex?v"> を何かしらの分布に従って refresh するとしているが，これは ECMC <span class="citation" data-cites="Bernard+2009">(Bernard et al., 2009)</span> の名残だろう．</p>
<p>実際，Appendix にて，動力学の不変分布が Boltzmann-Gibbs 分布になっていることが示されており，refresh は必要ないことが注記されている．</p>
<p>また，ここで例示されているアルゴリズムは対称な MCMC を定めるとしているのも懸念点の１つである．</p>
</section>
<section id="議論" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="議論"><span class="header-section-number">5</span> 議論</h2>
<p>３粒子以上が関与するポテンシャルに関しても自然に拡張でき，これが SEC <span class="citation" data-cites="Bernard+2009">(Bernard et al., 2009)</span> にはない美点である．</p>
<p>MD よりも効率的であることは理論的には示していないが，実験的にはそう予想される．</p>
<blockquote class="blockquote">
<p>It remains to be seen if the application of the method is suited for niche applications only or if it can rival with MD and Metropolis-MC for general purpose molecular simulations.</p>
</blockquote>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Alder-Wainwright1957" class="csl-entry">
Alder, B. J., &amp; Wainwright, T. E. (1957). <span class="nocase">Phase Transition for a Hard Sphere System</span>. <em>The Journal of Chemical Physics</em>, <em>27</em>(5), 1208–1209. <a href="https://doi.org/10.1063/1.1743957">https://doi.org/10.1063/1.1743957</a>
</div>
<div id="ref-Alder-Wainwright1958" class="csl-entry">
Alder, B. J., &amp; Wainwright, T. E. (1958). <a href="">Molecular dynamics by electronic computers</a>. In I. Prigogine (Ed.), <em>Proceedings of the international symposium on transport processes in statistical mechanics. Held in brussels, august 27-31, 1956</em> (p. 97).
</div>
<div id="ref-Alder-Wainwright1959" class="csl-entry">
Alder, B. J., &amp; Wainwright, T. E. (1959). <span class="nocase">Studies in Molecular Dynamics. I. General Method</span>. <em>The Journal of Chemical Physics</em>, <em>31</em>(2), 459–466. <a href="https://doi.org/10.1063/1.1730376">https://doi.org/10.1063/1.1730376</a>
</div>
<div id="ref-Bernard-Krauth2011" class="csl-entry">
Bernard, E. P., &amp; Krauth, W. (2011). Two-step melting in two dimensions: First-order liquid-hexatic transition. <em>Phys. Rev. Lett.</em>, <em>107</em>, 155704. <a href="https://doi.org/10.1103/PhysRevLett.107.155704">https://doi.org/10.1103/PhysRevLett.107.155704</a>
</div>
<div id="ref-Bernard+2009" class="csl-entry">
Bernard, E. P., Krauth, W., &amp; Wilson, D. B. (2009). Event-chain monte carlo algorithms for hard-sphere systems. <em>Phys. Rev. E</em>, <em>80</em>, 056704. <a href="https://doi.org/10.1103/PhysRevE.80.056704">https://doi.org/10.1103/PhysRevE.80.056704</a>
</div>
<div id="ref-Bortz+1975" class="csl-entry">
Bortz, A. B., Kalos, M. H., &amp; Lebowitz, J. L. (1975). A new algorithm for monte carlo simulation of ising spin systems. <em>Journal of Computational Physics</em>, <em>17</em>(1), 10–18. https://doi.org/<a href="https://doi.org/10.1016/0021-9991(75)90060-1">https://doi.org/10.1016/0021-9991(75)90060-1</a>
</div>
<div id="ref-Bouchard-Cote+2018-BPS" class="csl-entry">
Bouchard-Côté, A., Vollmer, S. J., &amp; Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. <em>Journal of the American Statistical Association</em>, <em>113</em>(522), 855–867. <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1294075">https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1294075</a>
</div>
<div id="ref-Duane+1987" class="csl-entry">
Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). Hybrid monte carlo. <em>Physics Letters B</em>, <em>195</em>(2), 216–222. <a href="https://doi.org/10.1016/0370-2693(87)91197-X">https://doi.org/10.1016/0370-2693(87)91197-X</a>
</div>
<div id="ref-Fenyman+1963" class="csl-entry">
Feynman, R. P., Leighton, R. B., &amp; Sands, M. (1964). <em>The feynman lectures of physics: Vol. I</em>. Addison-Wesley. <a href="https://www.feynmanlectures.caltech.edu/I_01.html">https://www.feynmanlectures.caltech.edu/I_01.html</a>
</div>
<div id="ref-Fichthorn-Weinberg1991" class="csl-entry">
Fichthorn, K. A., &amp; Weinberg, W. H. (1991). <span class="nocase">Theoretical foundations of dynamical Monte Carlo simulations</span>. <em>The Journal of Chemical Physics</em>, <em>95</em>(2), 1090–1096. <a href="https://doi.org/10.1063/1.461138">https://doi.org/10.1063/1.461138</a>
</div>
<div id="ref-Griebel+2007" class="csl-entry">
Griebel, M., Zumbusch, G., &amp; Knapek, S. (2007). <em>Numerical simulation in molecular dynamics: Numerics, algorithms, parallelization, applications</em> (Vol. 5). Springer Belin, Heidelberg. <a href="https://link.springer.com/book/10.1007/978-3-540-68095-6">https://link.springer.com/book/10.1007/978-3-540-68095-6</a>
</div>
<div id="ref-Liu2004" class="csl-entry">
Liu, J. S. (2004). <em>Monte carlo strategies in scientific computing</em>. Springer New York. <a href="https://doi.org/10.1007/978-0-387-76371-2">https://doi.org/10.1007/978-0-387-76371-2</a>
</div>
<div id="ref-Metropolis+1953" class="csl-entry">
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a>
</div>
<div id="ref-Neal1996" class="csl-entry">
Neal, R. M. (1996). <em>Bayesian learning for neural networks</em> (Vol. 118). Springer New York. <a href="https://link.springer.com/book/10.1007/978-1-4612-0745-0">https://link.springer.com/book/10.1007/978-1-4612-0745-0</a>
</div>
<div id="ref-Nemeth-Fearnhead2021" class="csl-entry">
Nemeth, C., &amp; Fearnhead, P. (2021). Stochastic gradient markov chain monte carlo. <em>Journal of the American Statistical Association</em>, <em>116</em>(533), 433–450. <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1847120">https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1847120</a>
</div>
<div id="ref-Peters-deWith2012" class="csl-entry">
Peters, E. A. J. F., &amp; de&nbsp;With, G. (2012). Rejection-free monte carlo sampling for general potentials. <em>Physical Review E</em>, <em>85</em>(2). <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026703">https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026703</a>
</div>
<div id="ref-Rapaport2004" class="csl-entry">
Rapaport, D. C. (2004). <em>The art of molecular dynamics simulation</em> (2nd ed.). Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511816581">https://doi.org/10.1017/CBO9780511816581</a>
</div>
<div id="ref-Tartero-Krauth2023" class="csl-entry">
Tartero, G., &amp; Krauth, W. (2023). <em>Concepts in monte carlo sampling</em>. <a href="https://arxiv.org/abs/2309.03136">https://arxiv.org/abs/2309.03136</a>
</div>
<div id="ref-戸田+2011" class="csl-entry">
戸田盛和, 斎藤信彦, 久保亮五, &amp; 橋爪夏樹. (2011). <em>統計物理学</em> (新装版, Vol. 5). 岩波書店. <a href="https://www.iwanami.co.jp/book/b259545.html">https://www.iwanami.co.jp/book/b259545.html</a>
</div>
<div id="ref-用語解説2022" class="csl-entry">
栗﨑, &amp; 田中. (2022). 用語解説. <em>生物物理</em>, <em>62</em>(4), 250–250. <a href="https://doi.org/10.2142/biophys.62.250">https://doi.org/10.2142/biophys.62.250</a>
</div>
<div id="ref-田中宏和2019" class="csl-entry">
田中宏和. (2019). <em>計算論的神経科学：脳の運動制御・感覚処理機構の理論的理解へ</em>. 森北出版. <a href="https://www.morikita.co.jp/books/mid/085161">https://www.morikita.co.jp/books/mid/085161</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>“What distinguishes computer simulation in general from other forms of computation, if such a distinction can be made, is the manner in which the computer is used: instead of merely performing a calculation, the computer becomes the virtual laboratory in which a system is studied - a numerical experiment.” <span class="citation" data-cites="Rapaport2004">(Rapaport, 2004, p. 3)</span>．↩︎</p></li>
<li id="fn2"><p>内部構造がある分子をシミュレーションする場合はよりやわらかい相互作用を考える必要があり，その際は積分は高次元になる上に，内部の運動が高速であるためにタイムステップも細かくする必要が出てくる．さらに拘束条件が存在する場合は，積分法よりも高い精度で取り扱う必要があり，特別な注意を要する．↩︎</p></li>
<li id="fn3"><p>“For example, the protein folding process takes about <img src="https://latex.codecogs.com/png.latex?10%5E%7B-3%7D"> seconds in nature. A proper MD simulation of such a process needs a step size of order <img src="https://latex.codecogs.com/png.latex?10%5E%7B-12%7D"> and will take about <img src="https://latex.codecogs.com/png.latex?10%5E6"> days using a current computer.”↩︎</p></li>
<li id="fn4"><p>For example, if the system of interest consists of closely packed particles, a random proposal for moving a particle is most likely rejected because the proposed new position has been partially occupied by others. <span class="citation" data-cites="Liu2004">(Liu, 2004, p. 189)</span>↩︎</p></li>
<li id="fn5"><p>特に格子場の理論において，Fermion 自由度が存在する場合のシミュレーションを問題として扱っていた．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Review</category>
  <guid>https://162348.github.io/posts/2024/Review/Peters-deWith2012.html</guid>
  <pubDate>Fri, 05 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Review/Peters-deWith2012.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Review/Butkovsky-Veretennikov2013.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="概要" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="概要"><span class="header-section-number">1</span> 概要</h2>
<ul>
<li>マルチンゲールによる導出</li>
<li>Lyapunov 型の条件が簡単に十分条件を与えるような，再帰時刻による取り扱い</li>
</ul>
<p><span class="citation" data-cites="Kulik2018">(Kulik, 2018, pp. 45–45)</span> では，安定性の定理の証明に，マルチンゲールを用いた議論を用いている．エルゴード性を持つ Markov 連鎖は必ず</p>
<ol type="1">
<li>局所的な集合上で良い攪拌性を持ち，</li>
<li>その他の点に行ってしまった場合でも，「十分早く」その局所的な集合に戻ってくる</li>
</ol>
<p>という２つのモードを持つ．これを別々に解析する見通しの良い議論を与えてくれるのがマルチンゲールによる議論であるとしている <span class="citation" data-cites="Kulik2018">(Kulik, 2018, p. 71)</span> が，似たような議論をしているのが本論文 <span class="citation" data-cites="Butkovsky-Veretennikov2013">(Butkovsky &amp; Veretennikov, 2013)</span> である．<sup>1</sup></p>
</section>
<section id="背景" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="背景"><span class="header-section-number">2</span> 背景</h2>
<ul>
<li><p>一様エルゴード性を <strong>strongly ergodic</strong> とも呼んでいる： <img src="https://latex.codecogs.com/png.latex?%0A%5Csup_%7Bx%5Cin%20E%7D%5C%7CP%5En(x,-)-%5Cpi%5C%7C_%5Cmathrm%7BTV%7D%5Cle%20Ce%5E%7B-%5Clambda%20n%7D.%0A"></p></li>
<li><p>一方で，各点 <img src="https://latex.codecogs.com/png.latex?x%5Cin%20E"> で <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CP%5En(x,-)-%5Cpi%5C%7C_%5Cmathrm%7BTV%7D%5Cto0%0A"> が成り立つことを <strong>weakly ergodic</strong> と呼んでいる．</p></li>
</ul>
<p>本論文では，<img src="https://latex.codecogs.com/png.latex?%5Clambda"> を推定する <span class="citation" data-cites="Diaconis-Stroock1991">(Diaconis &amp; Stroock, 1991)</span> 理論を，weakly ergodic の場合と非対称な場合に拡張する．</p>
<p>すると，<span class="citation" data-cites="Diaconis-Stroock1991">(Diaconis &amp; Stroock, 1991)</span> 理論では遷移確率核 <img src="https://latex.codecogs.com/png.latex?P"> のスペクトルギャップであった <img src="https://latex.codecogs.com/png.latex?%5Clambda"> は，一般の設定の下でもある一般化した半群生成作用素のスペクトル半径に関係することがわかった．</p>
<section id="diaconis-stroock1991-理論" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="diaconis-stroock1991-理論"><span class="header-section-number">2.1</span> <span class="citation" data-cites="Diaconis-Stroock1991">(Diaconis &amp; Stroock, 1991)</span> 理論</h3>
<p>一様エルゴード性の収束速度 <img src="https://latex.codecogs.com/png.latex?%5Clambda"> を定量化するアプローチの１つ．</p>
<div class="callout callout-style-default callout-tip callout-titled" title="定理">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理
</div>
</div>
<div class="callout-body-container callout-body">
<p>有限状態空間 <img src="https://latex.codecogs.com/png.latex?E"> 上の <img src="https://latex.codecogs.com/png.latex?P">-一様 Markov 連鎖は，既約かつ対称ならば， <img src="https://latex.codecogs.com/png.latex?%0A%5Clambda%3C%5Clog%5Coperatorname%7BGap%7D(P)%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BGap%7D(P):=%5Cmax%5Cleft%5C%7B%5Clvert%5Clambda%5Crvert%5Cin%5Cmathbb%7BR%7D_+%5Cmid%201%3E%5Clambda%5Cin%5Cmathrm%7BSp%7D(P)%5Cright%5C%7D%0A"></p>
</div>
</div>
<ul>
<li>対称ならば <img src="https://latex.codecogs.com/png.latex?P=P%5E*">．</li>
<li>スペクトルギャップは一般の正作用素に定義できる．</li>
</ul>
</section>
<section id="vaserstein1969-による最適カップリングの構成" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="vaserstein1969-による最適カップリングの構成"><span class="header-section-number">2.2</span> <span class="citation" data-cites="Vaserstein1969">(Vaserstein, 1969)</span> による最適カップリングの構成</h3>
<p>これを一般化したという．</p>
<p>最適な Markov カップリングよりも，カップリング確率が高いカップリングがあるらしい（しかし Markov にならない）．<a href="../../../posts/2024/Process/Coupling.html">この稿</a> に書いた．</p>
</section>
<section id="lyapunov-型の条件" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="lyapunov-型の条件"><span class="header-section-number">2.3</span> Lyapunov 型の条件</h3>
<p><span class="citation" data-cites="Douc+2004">(Douc et al., 2004)</span>, <span class="citation" data-cites="Kalashinikov1973">(Kalashinikov, 1973)</span>, <span class="citation" data-cites="Lamperti1960">(Lamperti, 1960)</span>, <span class="citation" data-cites="Rosenthal2002">(Rosenthal, 2002)</span>, <span class="citation" data-cites="Tweedie1981">(Tweedie, 1981)</span> による．</p>
</section>
</section>
<section id="本論" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="本論"><span class="header-section-number">3</span> 本論</h2>
<section id="カップリング補題" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="カップリング補題"><span class="header-section-number">3.1</span> カップリング補題</h3>
<p>本論に入る前に，次の結果を準備している：</p>
<div class="callout callout-style-default callout-tip callout-titled" title="補題（カップリング不等式の評価）^[[@Butkovsky-Veretennikov2013 pp.3521-22] 補題2.2]">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
補題（カップリング不等式の評価）<sup>2</sup>
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?X_1,X_2"> を確率核 <img src="https://latex.codecogs.com/png.latex?P"> を持つ Markov 連鎖，<img src="https://latex.codecogs.com/png.latex?Z=(X_1,X_2)"> をその最適 Markov カップリングを <img src="https://latex.codecogs.com/png.latex?%0AX_n%5E1=:%5Cxi_n1_%7B%5Cleft%5C%7B%5Czeta_n=0%5Cright%5C%7D%7D+%5Ceta_n%5E11_%7B%5Cleft%5C%7B%5Czeta_n=1%5Cright%5C%7D%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0AX_n%5E2=:%5Cxi_n1_%7B%5Cleft%5C%7B%5Czeta_n=0%5Cright%5C%7D%7D+%5Ceta_n%5E21_%7B%5Cleft%5C%7B%5Czeta_n=1%5Cright%5C%7D%7D%0A"> とする．このとき， <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BP%7D%5BX_n%5E1%5Cne%20X_n%5E2%5D%5Cle%5Cbiggr(1-p_0%5Cbiggl)%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bk=0%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta_k%5E1,%5Ceta_k%5E2)%5Cbiggl)%5Cright%5D%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ap(x_1,x_2):=1-%5Cfrac%7B1%7D%7B2%7D%5C%7CP(x_1,-)-P(x_2,-)%5C%7C_%5Cmathrm%7BTV%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ap_0:=1-%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Coperatorname%7BP%7D%5E%7BX_0%5E1%7D-%5Coperatorname%7BP%7D%5E%7BX_0%5E2%7D%5C%7C_%5Cmathrm%7BTV%7D=%5Coperatorname%7BP%7D%5BX_0%5E1=X_0%5E2%5D%0A"> が成り立つ．</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>基本的な考え方は <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Coperatorname%7BP%7D%5BX%5E1_n%5Cne%20X_n%5E2%5D&amp;%5Cle%5Coperatorname%7BP%7D%5B%5Czeta_0=1,%5Czeta_1=1,%5Ccdots,%5Czeta_%7Bn%7D=1%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5B1_%7B%5Cleft%5C%7B%5Czeta_0=1%5Cright%5C%7D%7D1_%7B%5Cleft%5C%7B%5Czeta_1=1%5Cright%5C%7D%7D%5Ccdots1_%7B%5Cleft%5C%7B%5Czeta_%7Bn%7D=1%5Cright%5C%7D%7D%5D%0A%5Cend%7Balign*%7D%0A"> である．これは，<img src="https://latex.codecogs.com/png.latex?X_n%5E1%5Cne%20X_n%5E2"> ならば <img src="https://latex.codecogs.com/png.latex?%5Czeta_n=1"> が必要であるため， <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cleft%5C%7BX_n%5E1%5Cne%20X_n%5E2%5Cright%5C%7D&amp;%5Csubset%5Cleft%5C%7B%5Czeta_n=1%5Cright%5C%7D%5C%5C%0A%20%20%20%20&amp;=%5Cleft%5C%7B%5Czeta_0=1,%5Czeta_1=1,%5Ccdots,%5Czeta_%7Bn%7D=1%5Cright%5C%7D%0A%5Cend%7Balign*%7D%0A"> であるが，逆は必ずしも成り立たないためである．</p>
<p>これに，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_n:=%5Csigma%5BX_n%5E1,X_n%5E2%5D"> について， <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20&amp;%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k%7D%5En1_%7B%5Cleft%5C%7B%5Czeta_i=1%5Cright%5C%7D%7D%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k-1%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta%5E1_i,%5Ceta_i%5E2)%5Cbiggl)%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D1_%7B%5Cleft%5C%7B%5Czeta_%7Bk-1%7D=1%5Cright%5C%7D%7D%0A%5Cend%7Balign*%7D%0A"> の <img src="https://latex.codecogs.com/png.latex?k=1"> の場合を併せて結論を得る．この等式自体は降下法により示す．</p>
<p><img src="https://latex.codecogs.com/png.latex?k=n"> の場合の式 <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BE%7D%5B1_%7B%5Cleft%5C%7B%5Czeta_n=1%5Cright%5C%7D%7D%5C,%7C%5C,%5Cmathcal%7BF%7D_%7Bn-1%7D%5D=%5Cbiggr(1-p(%5Ceta_%7Bn-1%7D%5E1,%5Ceta_%7Bn-1%7D%5E2)%5Cbiggl)1_%7B%5Cleft%5C%7B%5Czeta_%7Bn-1%7D=1%5Cright%5C%7D%7D%0A"> は明らかである．<img src="https://latex.codecogs.com/png.latex?k%3Cn"> の場合，帰納法の仮定と，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_%7Bk-1%7D"> の下で <img src="https://latex.codecogs.com/png.latex?%5Czeta_k"> と <img src="https://latex.codecogs.com/png.latex?(%5Ceta_k%5E1,%5Ceta_k%5E2)"> は条件付き独立であるから，次のように式変形できる： <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20&amp;%5Cquad%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k%7D%5En1_%7B%5Cleft%5C%7B%5Czeta_i=1%5Cright%5C%7D%7D%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5Cleft%5B1_%7B%5Cleft%5C%7B%5Czeta_k=1%5Cright%5C%7D%7D%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k+1%7D%5En1_%7B%5Cleft%5C%7B%5Czeta_i=1%5Cright%5C%7D%7D%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_k%5Cright%5D%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5Cleft%5B1_%7B%5Cleft%5C%7B%5Czeta_k=1%5Cright%5C%7D%7D%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta%5E1_i,%5Ceta%5E2_i)%5Cbiggl)%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_k%5Cright%5D%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5B1_%7B%5Cleft%5C%7B%5Czeta_k=1%5Cright%5C%7D%7D%5C,%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5D%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta%5E1_i,%5Ceta%5E2_i)%5Cbiggl)%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=1_%7B%5Cleft%5C%7B%5Czeta_%7Bk-1%7D=1%5Cright%5C%7D%7D%5Cbiggr(1-p(%5Ceta%5E1_%7Bk-1%7D,%5Ceta%5E2_%7Bk-1%7D)%5Cbiggl)%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta%5E1_i,%5Ceta%5E2_i)%5Cbiggl)%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=1_%7B%5Cleft%5C%7B%5Czeta_%7Bk-1%7D=1%5Cright%5C%7D%7D%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=k-1%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta%5E1_i,%5Ceta%5E2_i)%5Cbiggl)%5C,%5Cmiddle%7C%5C,%5Cmathcal%7BF%7D_%7Bk-1%7D%5Cright%5D%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
</div>
</div>
</div>
<p>この補題により，確率核 <img src="https://latex.codecogs.com/png.latex?P"> を共有する２つの Markov 過程 <img src="https://latex.codecogs.com/png.latex?(X_n%5E1),(X_n%5E2)"> が与えられたとき，これらのカップリング <img src="https://latex.codecogs.com/png.latex?(%5Cwidetilde%7BX%7D%5E1_n),(%5Cwidetilde%7BX%7D%5E2_n)"> を同一の確率空間 <img src="https://latex.codecogs.com/png.latex?(%5COmega,%5Cmathcal%7BF%7D,%5Coperatorname%7BP%7D)"> 上に構成し，<img src="https://latex.codecogs.com/png.latex?(%5Cwidetilde%7BX%7D_n%5E1)%5Coverset%7B%5Ctext%7Bd%7D%7D%7B=%7D(X_n%5E1),(%5Cwidetilde%7BX%7D_n%5E2)%5Coverset%7B%5Ctext%7Bd%7D%7D%7B=%7D(X_n%5E2)"> であるが，全変動距離を <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BP%7D"> によって評価できるようになる．</p>
</section>
<section id="スペクトルギャップによる一様エルゴード速度" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="スペクトルギャップによる一様エルゴード速度"><span class="header-section-number">3.2</span> スペクトルギャップによる一様エルゴード速度</h3>
<p>次の積分作用素 <img src="https://latex.codecogs.com/png.latex?A:%5Cmathcal%7BL%7D_b(E%5E2)%5Cto%5Cmathcal%7BL%7D_b(E%5E2)"> を考える： <img src="https://latex.codecogs.com/png.latex?%0AAf(x):=%5Cbiggr(1-p(x)%5Cbiggl)%5Coperatorname%7BE%7D%5Bf(%5Ceta_1)%5C,%7C%5C,%5Ceta_0=x%5D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Ceta_i=%5Cbegin%7Bpmatrix%7D%5Ceta_i%5E1%5C%5C%5Ceta_i%5E2%5Cend%7Bpmatrix%7D,%5Cquad%20x=%5Cbegin%7Bpmatrix%7Dx%5E1%5C%5Cx%5E2%5Cend%7Bpmatrix%7D.%0A"> このスペクトル半径 <img src="https://latex.codecogs.com/png.latex?%0Ar(A):=%5Climsup_%7Bn%5Cto%5Cinfty%7D%5Csqrt%5Bn%5D%7B%5C%7CA%5En%5C%7C%7D%0A"> が一様エルゴード性を引き起こすのである．</p>
<div class="callout callout-style-default callout-caution callout-titled" title="証明の概略">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明の概略
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>なお，この定義式は，後述の <img src="https://latex.codecogs.com/png.latex?r(A)%5Cle1"> と併せると，任意の <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3E0"> に対して，ある <img src="https://latex.codecogs.com/png.latex?C%3E0"> が存在して，<img src="https://latex.codecogs.com/png.latex?r(A)%5Cle%20r(A)%5E%7B1-%5Cepsilon%7D"> であるから， <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5C%7CA%5En%5C%7C&amp;%5Cle%20C%5Cleft(r(A)%5E%7B1-%5Cepsilon%7D%5Cright)%5En%5C%5C%0A%20%20%20%20&amp;=Ce%5E%7Bn(1-%5Cepsilon)%5Clog%20r(A)%7D%5C%5C%0A%20%20%20%20&amp;=Ce%5E%7B-n(1-%5Cepsilon)%5Clog%5Clvert%20r(A)%5Crvert%7D%0A%5Cend%7Balign*%7D%0A"> を含意することに注意．</p>
<p>実は次の定理の証明は，次の不等式を導いているのみである： <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7B2%7D%5C%7CP%5En(x,-)-P%5En(y,-)%5C%7C_%5Cmathrm%7BTV%7D%5Cle%5C%7CA%5E%7Bn%7D%5C%7C.%0A"></p>
<p>この２式より，直ちに証明が完成する．</p>
</div>
</div>
</div>
<p>ただし，作用素ノルム <img src="https://latex.codecogs.com/png.latex?%5C%7CA%5En%5C%7C"> は任意の <img src="https://latex.codecogs.com/png.latex?%5C%7C1%5C%7C=1"> を満たす関数ノルムに関して構成して良い．<img src="https://latex.codecogs.com/png.latex?C_b(E%5E2)"> を考えることも有用である．</p>
<p>いずれにしろ， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7CA%5C%7C_%5Cinfty=1-%5Cinf_%7Bx%5Cin%20E%5E2%7Dp(x)%5Cle1%0A"> という条件式は変わらない．作用素ノルムの劣乗法性から <img src="https://latex.codecogs.com/png.latex?%0Ar(A)=%5Climsup_%7Bn%5Cto%5Cinfty%7D%5Csqrt%5Bn%5D%7B%5C%7CA%5En%5C%7C_%5Cinfty%7D%5Cle%5C%7CA%5C%7C_%5Cinfty%5Cle1%0A"> であることに注意．</p>
<div class="callout callout-style-default callout-tip callout-titled" title="定理^[[@Butkovsky-Veretennikov2013] 定理2.1．]">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理<sup>3</sup>
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?A:%5Cmathcal%7BL%7D_b(E%5E2)%5Cto%5Cmathcal%7BL%7D_b(E%5E2)"> は有界作用素であり，<img src="https://latex.codecogs.com/png.latex?r(A)%5Cle1"> を満たす．</li>
<li><img src="https://latex.codecogs.com/png.latex?r(A)%3C1"> ならば，<img src="https://latex.codecogs.com/png.latex?X"> はただ一つの不変確率測度を持ち，任意の <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3E0"> に対して，初期分布 <img src="https://latex.codecogs.com/png.latex?X_0%5E1,X_0%5E2"> に依らないある <img src="https://latex.codecogs.com/png.latex?C%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Coperatorname%7BP%7D%5E%7BX_n%5E1%7D-%5Coperatorname%7BP%7D%5E%7BX_n%5E2%7D%5C%7C_%5Cmathrm%7BTV%7D%5Cle%20C(1-p_0)e%5E%7B-n%5Clvert%5Clog%20r(A)%5Crvert(1-%5Cepsilon)%7D.%0A"></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>一般に，任意の <img src="https://latex.codecogs.com/png.latex?f%5Cin%5Cmathcal%7BL%7D_b(E%5E2)"> に対して，<img src="https://latex.codecogs.com/png.latex?(%5Ceta_k)"> のMarkov性より， <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Coperatorname%7BE%7D%5Cleft%5Bf(%5Ceta_n)%5Cprod_%7Bi=0%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta_i)%5Cbiggl)%5Cright%5D&amp;=%5Coperatorname%7BE%7D%5Cleft%5B%5Coperatorname%7BE%7D%5Cleft%5Bf(%5Ceta_n)%5Cprod_%7Bi=0%7D%5E%7Bn-1%7D%5Cbiggr(1-p(%5Ceta_i)%5Cbiggl)%5C,%5Cmiddle%7C%5C,%5Ceta_0,%5Ccdots,%5Ceta_%7Bn-1%7D%5Cright%5D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=0%7D%5E%7Bn-2%7D%5Cbiggr(1-p(%5Ceta_i)%5Cbiggl)%5Ccdot%5Cbiggr(1-p(%5Ceta_%7Bn-1%7D)%5Cbiggl)%5Coperatorname%7BE%7D%5Bf(%5Ceta_n)%5C,%7C%5C,%5Ceta_%7Bn-1%7D%5D%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=%5Coperatorname%7BE%7D%5Cleft%5B%5Cprod_%7Bi=0%7D%5E%7Bn-2%7D%5Cbiggr(1-p(%5Ceta_i)%5Cbiggl)%5Ccdot%20Af(%5Ceta_%7Bn-1%7D)%5Cright%5D=%5Coperatorname%7BE%7D%5BA%5Enf(%5Ceta_0)%5D.%0A%5Cend%7Balign*%7D%0A"> これより，<img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7B%5Ceta_n=0%5Cright%5C%7D%5Csupset%5Cleft%5C%7B%5Cprod_%7Bk=0%7D%5E%7Bn-1%7D%5Cbiggr(1-p(Z_k)%5Cbiggl)=0%5Cright%5C%7D"> に注意すれば，次の評価を得る： <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5C%7CP%5En(x,-)-P%5En(y,-)%5C%7C_%5Cmathrm%7BTV%7D&amp;%5Cle2%5Coperatorname%7BP%7D_%7B(x,y)%7D%5EQ%5BX_n%5Cne%20Y_n%5D%5Cle2%5Coperatorname%7BE%7D_%7B(x,y)%7D%5E%7BQ_%5Cperp%7D%5Cleft%5B%5Cprod_%7Bk=0%7D%5E%7Bn-1%7D%5Cbiggr(1-p(Z_k)%5Cbiggl)%5Cright%5D%5C%5C%0A%20%20%20%20&amp;=2%5Coperatorname%7BE%7D_%7B(x,y)%7D%5E%7BQ_%5Cperp%7D%5Cleft%5B%5Cdelta_1(%5Ceta_n)%5Cprod_%7Bk=0%7D%5E%7Bn-1%7D%5Cbiggr(1-p(Z_k)%5Cbiggl)%5Cright%5D=2%5Coperatorname%7BE%7D_%7B(x,y)%7D%5E%7BQ_%5Cperp%7D%5BA%5E%7Bn-1%7D%5Cdelta_1(%5Ceta_1)%5D(1-p(%5Ceta_0))%5C%5C%0A%20%20%20%20&amp;%5Cle2%5Cdelta_%7Bx,y%7D%5C%7CA%5E%7Bn-1%7D%5C%7C_%5Cinfty%0A%5Cend%7Balign*%7D%0A"> よって，<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%5C%7CP%5En(x,-)-P%5En(y,-)%5C%7C"> はちょうど作用素ノルム <img src="https://latex.codecogs.com/png.latex?%5C%7CA%5E%7Bn-1%7D%5C%7C"> を評価する問題に帰着する．</p>
</div>
</div>
</div>
</section>
<section id="再帰性と非一様エルゴード速度" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="再帰性と非一様エルゴード速度"><span class="header-section-number">3.3</span> 再帰性と非一様エルゴード速度</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5C%7CA%5C%7C_%5Cinfty=1-%5Cinf_%7Bx%5Cin%20E%5E2%7Dp(x)=1%0A"> の場合に当たるものであるが， <img src="https://latex.codecogs.com/png.latex?%0AK(%5Cepsilon):=%5Cleft%5C%7Bx%5Cin%20E%5E2%5Cmid%20p(x)%5Cge%5Cepsilon%5Cright%5C%7D%0A"> に無限回再帰するとき，その頻度に依存して，指数的か多項式的か決まる．<sup>4</sup> この頻度は，次の帰着時間 <img src="https://latex.codecogs.com/png.latex?%5Ctau%5EB,T%5EB"> の積率条件で記述される： <img src="https://latex.codecogs.com/png.latex?%0A%5Ctau%5EB:=%5Cinf%5Cleft%5C%7Bn%5Cge%201%5Cmid%5Ceta_n%5Cin%20B%5Cright%5C%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0AT%5EB:=%5Cinf%5Cleft%5C%7Bn%5Cge%201%5Cmid(X_n%5E1,X_n%5E2)%5Cin%20B%5Cright%5C%7D%0A"> ただし，<img src="https://latex.codecogs.com/png.latex?B%5Cin%5Cmathcal%7BE%7D%5E%7B%5Cotimes2%7D">．</p>
<div class="callout callout-style-default callout-tip callout-titled" title="命題2.1">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
命題2.1
</div>
</div>
<div class="callout-body-container callout-body">
<p>ある <img src="https://latex.codecogs.com/png.latex?%5Cepsilon,%5Clambda,M%3E0"> と <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BE%7D%5E%7B%5Cotimes2%7D%5Cni%20B%5Csubset%20K(%5Cepsilon)"> について，次が成り立つとする：</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?Q:=%5Coperatorname%7BE%7D%5Be%5E%7B%5Clambda%5Ctau%5EB%7D%5D%3C%5Cinfty">．</li>
<li>任意の <img src="https://latex.codecogs.com/png.latex?x%5Cin%20B"> について，<img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D_x%5Be%5E%7B%5Clambda%5Ctau%5EB%7D%5D%5Cle%20M">．</li>
</ol>
<p>このとき，<img src="https://latex.codecogs.com/png.latex?X"> はただ一つの不変確率測度 <img src="https://latex.codecogs.com/png.latex?%5Cpi"> をもち，またある初期分布 <img src="https://latex.codecogs.com/png.latex?(X_0%5E1,X_0%5E2)"> に依らない <img src="https://latex.codecogs.com/png.latex?C%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Coperatorname%7BP%7D%5E%7BX_n%5E1%7D-%5Coperatorname%7BP%7D%5E%7BX_n%5E2%7D%5C%7C_%5Cmathrm%7BTV%7D%5Cle%20CQe%5E%7B-n%5Ctheta%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta:=%5Cfrac%7B%5Clvert%5Clog(1-%5Cepsilon)%5Crvert%5Clambda%7D%7B%5Clog%20M+%5Clvert%5Clog(1-%5Cepsilon)%5Crvert%7D%0A"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="定理2.2">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理2.2
</div>
</div>
<div class="callout-body-container callout-body">
<p>ある <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3E0,%5Clambda%3E0,M%3E0,%5Cmathcal%7BE%7D%5E%7B%5Cotimes2%7D%5Cni%20B%5Csubset%20K(%5Cepsilon)"> について，次が成り立つとする：</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?Q_2:=%5Coperatorname%7BE%7D%5Be%5E%7B%5Clambda%20T%5EB%7D%5D%3C%5Cinfty">．</li>
<li>任意の <img src="https://latex.codecogs.com/png.latex?x%5Cin%20B%5Csetminus%20K(1)"> について，<img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D_x%5Be%5E%7B%5Clambda%20T%5EB%7D%5D%3CM">．</li>
</ol>
<p>このとき，過程 <img src="https://latex.codecogs.com/png.latex?X"> はただ一つの不変確率分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi"> をもち，ある初期分布 <img src="https://latex.codecogs.com/png.latex?(X_0%5E1,X_0%5E2)"> に依存しない定数 <img src="https://latex.codecogs.com/png.latex?C%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Coperatorname%7BP%7D%5E%7BX_n%5E1%7D-%5Coperatorname%7BP%7D%5E%7BX_n%5E2%7D%5C%7C_%5Cmathrm%7BTV%7D%5Cle%20CQ_2e%5E%7B-n%5Ctheta_1%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_1:=%5Cfrac%7B%5Clvert%5Clog(1-%5Cepsilon)%5Crvert%5Clambda%7D%7B%5Clog%20M+3%5Clvert%5Clog(1-%5Cepsilon)%5Crvert%7D.%0A"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="定理2.3">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定理2.3
</div>
</div>
<div class="callout-body-container callout-body">
<p>ある <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3E0,%5Clambda%5Cge1,M%3E0,%5Cmathcal%7BE%7D%5E%7B%5Cotimes2%7D%5Cni%20B%5Csubset%20K(%5Cepsilon)"> について，次が成り立つとする：</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?Q_3:=%5Coperatorname%7BE%7D%5B(T%5EB)%5E%5Clambda%5D%3C%5Cinfty">．</li>
<li>任意の <img src="https://latex.codecogs.com/png.latex?x%5Cin%20B%5Csetminus%20K(1)"> に対して，<img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BE%7D_x%5B(T%5EB)%5E%5Clambda%5D%3CM">．</li>
</ol>
<p>このとき，過程 <img src="https://latex.codecogs.com/png.latex?X"> はただ一つの不変確率分布 <img src="https://latex.codecogs.com/png.latex?%5Cpi"> をもち，任意の <img src="https://latex.codecogs.com/png.latex?%5Clambda_1%5Cin(0,%5Clambda)"> に対して，ある初期分布 <img src="https://latex.codecogs.com/png.latex?(X_0%5E1,X_0%5E2)"> に依存しない定数 <img src="https://latex.codecogs.com/png.latex?C%3E0"> が存在して， <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Coperatorname%7BP%7D%5E%7BX_n%5E1%7D-%5Coperatorname%7BP%7D%5E%7BX_n%5E2%7D%5C%7C_%5Cmathrm%7BTV%7D%5Cle%20CQ_3n%5E%7B-%5Clambda_1%7D%0A"></p>
</div>
</div>
</section>
</section>
<section id="例" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="例"><span class="header-section-number">4</span> 例</h2>
<p>あ</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Butkovsky-Veretennikov2013" class="csl-entry">
Butkovsky, O. A., &amp; Veretennikov, A. Yu. (2013). On asymptotics for vaserstein coupling of markov chains. <em>Stochastic Processes and Their Applications</em>, <em>123</em>(9), 3518–3541. <a href="https://doi.org/10.1016/j.spa.2013.04.016">https://doi.org/10.1016/j.spa.2013.04.016</a>
</div>
<div id="ref-Diaconis-Stroock1991" class="csl-entry">
Diaconis, P., &amp; Stroock, D. (1991). <span class="nocase">Geometric Bounds for Eigenvalues of Markov Chains</span>. <em>The Annals of Applied Probability</em>, <em>1</em>(1), 36–61. <a href="https://doi.org/10.1214/aoap/1177005980">https://doi.org/10.1214/aoap/1177005980</a>
</div>
<div id="ref-Douc+2004" class="csl-entry">
Douc, R., Moulines, E., &amp; Rosenthal, J. S. (2004). <span class="nocase">Quantitative bounds on convergence of time-inhomogeneous Markov chains</span>. <em>The Annals of Applied Probability</em>, <em>14</em>(4), 1643–1665. <a href="https://doi.org/10.1214/105051604000000620">https://doi.org/10.1214/105051604000000620</a>
</div>
<div id="ref-Durmus+2016" class="csl-entry">
Durmus, A., Fort, G., &amp; Moulines, Éric. (2016). <span class="nocase">Subgeometric rates of convergence in Wasserstein distance for Markov chains</span>. <em>Annales de l’Institut Henri Poincar<span>é</span>, Probabilit<span>é</span>s Et Statistiques</em>, <em>52</em>(4), 1799–1822. <a href="https://doi.org/10.1214/15-AIHP699">https://doi.org/10.1214/15-AIHP699</a>
</div>
<div id="ref-Kalashinikov1973" class="csl-entry">
Kalashinikov, V. V. (1973). <a href="">The property of <img src="https://latex.codecogs.com/png.latex?%5Cgamma">-reflexivity for markov sequences</a>. <em>Soviet Mathematics Doklady</em>, <em>14</em>, 1869–1873.
</div>
<div id="ref-Kulik2018" class="csl-entry">
Kulik, A. (2018). <em>Ergodic behavior of markov processes: With applications to limit theorems</em> (Vol. 67). De Gruyter: Berlin, Boston. <a href="https://doi.org/10.1515/9783110458930">https://doi.org/10.1515/9783110458930</a>
</div>
<div id="ref-Lamperti1960" class="csl-entry">
Lamperti, J. (1960). Criteria for the recurrence or transience of stochastic process. i. <em>Journal of Mathematical Analysis and Applications</em>, <em>1</em>(3), 314–330. https://doi.org/<a href="https://doi.org/10.1016/0022-247X(60)90005-6">https://doi.org/10.1016/0022-247X(60)90005-6</a>
</div>
<div id="ref-Rosenthal2002" class="csl-entry">
Rosenthal, J. (2002). <span class="nocase">Quantitative Convergence Rates of Markov Chains: A Simple Account</span>. <em>Electronic Communications in Probability</em>, <em>7</em>(none), 123–128. <a href="https://doi.org/10.1214/ECP.v7-1054">https://doi.org/10.1214/ECP.v7-1054</a>
</div>
<div id="ref-Tweedie1981" class="csl-entry">
Tweedie, R. L. (1981). Criteria for ergodicity, exponential ergodicity and strong ergodicity of markov processes. <em>Journal of Applied Probability</em>, <em>18</em>(1), 122–130. <a href="http://www.jstor.org/stable/3213172">http://www.jstor.org/stable/3213172</a>
</div>
<div id="ref-Vaserstein1969" class="csl-entry">
Vaserstein, L. N. (1969). Markov processes on countable product spaces describing large systems of automata. <em>Problemy Peredachi Informatsii</em>, <em>5</em>(3), 64–72. <a href="https://www.mathnet.ru/php/archive.phtml?wshow=paper&amp;jrnid=ppi&amp;paperid=1811&amp;option_lang=eng">https://www.mathnet.ru/php/archive.phtml?wshow=paper&amp;jrnid=ppi&amp;paperid=1811&amp;option_lang=eng</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>それと <span class="citation" data-cites="Durmus+2016">(Durmus et al., 2016)</span> を挙げている．↩︎</p></li>
<li id="fn2"><p><span class="citation" data-cites="Butkovsky-Veretennikov2013">(Butkovsky &amp; Veretennikov, 2013, pp. 3521–3522)</span> 補題2.2↩︎</p></li>
<li id="fn3"><p><span class="citation" data-cites="Butkovsky-Veretennikov2013">(Butkovsky &amp; Veretennikov, 2013)</span> 定理2.1．↩︎</p></li>
<li id="fn4"><p>前節から判る通り，局所Dobrushin条件を満たす集合（small set ともいう）上では常に指数収束である．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Review</category>
  <category>Kernel</category>
  <guid>https://162348.github.io/posts/2024/Review/Butkovsky-Veretennikov2013.html</guid>
  <pubDate>Wed, 03 Apr 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Review/Butkovsky-Veretennikov2013.svg" medium="image" type="image/svg+xml"/>
</item>
<item>
  <title>これからはじめるベイズ機械学習</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/AI/BAI.html</link>
  <description><![CDATA[ 





<p>現在，産業界における “AI” というと専ら，いくつかの限られた巨大 IT 企業が，巨大ニューラルネットワークを最尤推定で学習させ，これを基盤モデルとして公開し，我々一般庶民はそれを有効活用して下流タスクを安価に解くことだけ考えるという営みを指す．</p>
<p>その産業や生活への破壊的な影響を憂慮しながらも，雨乞いをする日々である．</p>
<div id="fig-Altman" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Altman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="Images/SamAltman.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Altman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
図&nbsp;1: 3月19日時点，GPT-5 にも Sora にもアクセス権を持たない我々 v.s. <a href="https://youtu.be/jvqFAi7vkBc?si=hwF_LJAs7XE3bNTR&amp;t=2695">Lex Fridman Podcast</a> に出演した Sam Altman
</figcaption>
</figure>
</div>
<p>AI はそんなものではない．AI はこれにかぎるものではない．</p>
<p>AI が真に我々の友となり，我々の日常をほんとうに豊かにするは，AI の進歩だけが必要なのではなく，<strong>人間との協業が得意になる必要がある</strong>．</p>
<p>そのための第一歩はすでに明らかである．<strong>不確実性の定量化</strong> である．</p>
<p>つまり，「その AI には何が出来て何が出来ないか」「AI の出力がいつ信頼にたるもので，いつ人間の介入が必要であるのか」がわかりやすい形で伝わるコミュニケーション様式をそなえている必要があるのである．<sup>1</sup></p>
<hr>
<p>筆者の知る限り，ここにある全てのナラティブは現時点では全く広く語られているものではなく，筆者も最初の１年の研究生活を通じて朧げながら見えて来たばかりのものである．</p>
<p><strong>不確実性の定量化は，機械学習モデルを民主化し，我々の民芸に取り込むための重要な一歩である</strong>（のではないだろうか？）．</p>
<p>本稿はこの発見を共有するために書いた．筆者の反芻不足から，冗長な部分も多いだろうが，少しでも，琴線に触れるものがあれば幸いである．<sup>2</sup></p>
<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="ベイズ機械学習のすすめ" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="ベイズ機械学習のすすめ"><span class="header-section-number">1</span> ベイズ機械学習のすすめ</h2>
<p>我々が AI をより信頼するためには，何が必要だろうか？</p>
<p>筆者の考えでは，信頼への第一歩は <strong>不確実性の定量化</strong> が出来るようになることのはずである．</p>
<p>そしてそのためには <strong>ベイズ機械学習</strong> (Bayesian Machine Learning) の発展による本質的解決が必要不可欠である．本稿はこの点を説明するために執筆されたものである．</p>
<p>筆者に言わせれば，ベイズ機械学習が，今後数年間で AI が経験すべき進展の方向である．この山を越えれば，今まででさえ思っても見なかった未来がひらけてくるだろう．</p>
<blockquote class="blockquote">
<p>Although considerable challenges remain, the coming decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework. <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 452)</span></p>
</blockquote>
<section id="ベイズとは何か" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="ベイズとは何か"><span class="header-section-number">1.1</span> ベイズとは何か？</h3>
<p>機械学習において，確率論的なモデリングに基づいたアプローチを <strong>ベイズ機械学習</strong> ともいう．典型的には，モデルの全変数上の結合分布をモデリングし，ベイズ規則によりパラメータのベイズ推定を行う，という手続きからなる．そのため，<strong>確率論的アプローチ</strong> や <strong>モデルベースアプローチ</strong> も同義語として用いられる．<sup>3</sup></p>
<p>一方で，<strong>頻度論的</strong> という言葉は，よく非ベイズ的アプローチを示す接頭辞として用いられる．典型的には，損失関数を設定し，これを最小化するパラメータを探索することによって実行される．</p>
<p>この２つのアプローチは互いに対照的であり，統計学の始まりから基本的な二項対立の図式をなしてきた．</p>
<div class="table-responsive-sm">
<table class="table-striped table-hover table">
<caption>Contrast of the two main approachs to Machine Learning</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Bayesian</th>
<th style="text-align: center;">Frequentist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Inference is<sup>4</sup></td>
<td style="text-align: center;">Marginalization</td>
<td style="text-align: center;">Approximation</td>
</tr>
<tr class="even">
<td style="text-align: center;">Computational Idea<sup>5</sup></td>
<td style="text-align: center;">Integration</td>
<td style="text-align: center;">Optimization</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">Uncertainty Quantification</td>
<td style="text-align: center;">Recovery of True Value</td>
</tr>
<tr class="even">
<td style="text-align: center;">Emphasis</td>
<td style="text-align: center;">Modelling</td>
<td style="text-align: center;">Inference</td>
</tr>
</tbody>
</table>
</div>
<p>しかし，機械学習の時代においては，互いの弱みを補間し合う形で発展していくと筆者は考える．特に，現状の<u><strong>推論偏重でモデリング軽視の風潮が，重要な実世界応用の多くを阻んでしまっている</strong></u>．機械学習の世界樹は実は２本あるのである．</p>
</section>
<section id="ベイズと頻度論との違い" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="ベイズと頻度論との違い"><span class="header-section-number">1.2</span> ベイズと頻度論との違い</h3>
<p>ベイズと頻度論では，確率の解釈も異なるかも知れないが，数学的枠組みとしてはベイズの方が一般的な枠組みであり，また手続き上は，モデリングを重視するか，推論を重視するかの違いでしかない．</p>
<p>実際，殆どの場合，頻度論的手法はある特定の事前分布を持ったベイズ手法とみなせ，逆も然りである．</p>
<p>データから推論を行うには，何らかの仮定が必ず必要であり，それを明示的にモデルに組み込むのがベイズで，推論アルゴリズムにより自動化する精神を持つのが頻度論的手法である．</p>
<p>その結果，優秀な推論アルゴリズムが日夜驚異的なスピードで提案され，今や機械学習手法は教師あり学習・教師なし学習・強化学習の全てで目覚ましい発展を見た．</p>
<p>しかし，ベイズと頻度論の２つの柱のバランスを欠いた発展はここまでである．今や，頻度論的な手法を採用した際に，自分たちがどのような仮定を置いたのか全く明瞭な知識を欠いてしまっている．一方で，現実のビッグで複雑なデータを扱うためには，もはや確率的なモデリングを避けては通れない．<sup>6</sup></p>
<p>極めて本質的で強大な敵に対面しつつあるのである．</p>
<p>だが，現状の病理は明らかであり，頻度論とベイズの手法の間に対応をつけ，足並みを揃えることで次の前進が約束されてる．この意味で，２つの世界樹が必要なのである．</p>
<p>さらに，ベイズ推論は帰納的推論の確率論的拡張と見れるため，エージェントの合理的な学習と意思決定の最良のモデル（の一つ）と信じられている．<sup>7</sup></p>
<p>したがって，<u><strong>ベイズ流解釈により手法を理解し，最適化流解釈により手法を実装する</strong></u>．これがあるべき機械学習の未来であると筆者は考える．</p>
</section>
<section id="つの世界樹" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="つの世界樹"><span class="header-section-number">1.3</span> ２つの世界樹</h3>
<p>今こそ，この２つの手法は根底では繋がっていることをよく周知し，この２つの視座を往来しながら適材適所に使うことが大事だと筆者は考える．</p>
<p>しかしそのためには，ベイズ機械学習の発展が遅れている現状を鑑みて，ベイズの手法のより一層の発展と理解の深化が必要である．<sup>8</sup></p>
<p>本章「ベイズ機械学習のすすめ」は，ベイズの手法の特に肝心と思われる３つの側面を指摘して終わる．以下３章を通じて，</p>
<ol type="1">
<li><p>第 2 節 <u><strong>ベイズは不確実性を定量化する</strong></u></p>
<p>Bayes の方が不確実性の定量化が得意であるため，そのような応用先では頻度論的な手法よりも，Bayes バージョンの手法を用いることが出来ると便利である．</p></li>
<li><p>第 3 節 <u><strong>ベイズは分布という共通言語を与える</strong></u></p>
<p>Bayes による統一的な扱いが理論的に有用である場面が増えている．その際に，Bayes による理論解析と最適化による実際の推論という適材適所の協業が未来の方向であるかも知れない．</p></li>
<li><p>第 4 節 <u><strong>ベイズは理解を促進する</strong></u></p>
<p>ベイズの手法が敬遠されていた理由も，換言すれば，「事前分布」という得体の知れないものを通じて，理論的深淵と直結するためである．ベイズ手法の研究が理論的な解明を要請する．だからこそ，数学者の魂を持った者がこの途を通ることは人類に大きく資すると筆者は考える．</p></li>
</ol>
</section>
</section>
<section id="sec-uncertainty-quantification" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-uncertainty-quantification"><span class="header-section-number">2</span> ベイズは不確実性を定量化する</h2>
<section id="sec-need-for-uncertainty-quantification" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sec-need-for-uncertainty-quantification"><span class="header-section-number">2.1</span> 不確実性の定量化の必要性</h3>
<p>機械学習と統計学が単なる道具ではなく，人間のより大きなシステムの一環を単独で担う場面が増えてきた．例えば，</p>
<ul>
<li>金融・経営・政策決定などの分野で，意思決定に繋げるデータ解析をするとき</li>
<li>科学において，発見や仮説を検証するためのデータ解析をするとき</li>
<li>ロボットや自動車などの自動化をし，社会に実装するとき<sup>9</sup></li>
<li>医療診断や裁判などの場面で，専門家を補助するシステムを作るとき</li>
</ul>
<p>これらのいずれの例でも，<u><strong>システムの一部を担うにあたって，不確実性を定量化しておくことが欠かせない</strong></u>．その出力を用いるのが人間である場合も勿論，別の機械学習モデルである場合は尚更である．</p>
<p>つまり，人間社会で優秀であるだけでなくホウレンソウと信頼獲得も重要であるように，機械学習モデルも性能の高さと正確さだけでなく，いつその結果を信頼して良いのかを「どの程度」という指標と共に知らせてくれることが信頼関係の基本となるだろう．</p>
<p>実際，殆どの場面で，データから高い確証度で言えることと，そうではないことでは全く違う意味を持つ．それぞれの場面での例には，次のようなものがあるだろう：</p>
<ul>
<li>データから高い確証度で言えることと，意思決定者による采配が必要な部分を分離できない限り，意思決定プロセスの一部として組み込むことが難しく，結局機械学習手法が全く採用されないということもあり得る．</li>
<li>結果の再現可能性が科学の基本的な要請である以上，その結果の不確実性を実験結果に付記することは基本的な科学的態度である．後述（第 2.3.1 節）するが，<img src="https://latex.codecogs.com/png.latex?p">-値や信頼区間などの統計量は<u><strong>これに応えるものではない</strong></u>．</li>
<li>ロボットや自動車の自動化 AI システムは，いくつかのモデルを組み合わせて作ることになるだろう．個々が十分な性能を持っていても，小さな誤差が累積してシステムとしての性能を著しく低下させることがある．これを防ぐために，統一した方法での不確実性の取り扱いが必要である．</li>
<li>個々人の権利と法益が衝突する場面にも AI が利用されより良い生活が実現されるには，法的な解釈可能性が担保される必要があることが，実は大きな難関として我々を待っている．その第一歩は，不確実性の可視化になるだろう．<sup>10</sup></li>
</ul>
<p>以上の内容は，結果の <strong>解釈可能性</strong> でも全く同じことが言えるだろう．</p>
</section>
<section id="信頼のおける-ai-システム" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="信頼のおける-ai-システム"><span class="header-section-number">2.2</span> 信頼のおける AI システム</h3>
<p>上述の点をまとめると，機械学習手法と人間社会がよりよく共生していくには AI の <strong>信頼性</strong> (trustworthyness) が必要とされているのである．不確実性の定量化と解釈可能性は，AI が人間社会で信頼を獲得するにあたって根本的な要素になるだろう．</p>
<p>現状の手法の延長でこの信頼性の問題は扱えず，新たな手法が必要とされている．Bayesian approach や probabilistic approach と呼ばれている試みは，まさにこれに応えるものであり，近年急速に発展している．</p>
</section>
<section id="sec-Bayes-for-uncertainty-quatification" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="sec-Bayes-for-uncertainty-quatification"><span class="header-section-number">2.3</span> 不確実性を扱うには Bayes が必要である</h3>
<p>実装は頻度論的な手法の方が簡単で高速であることが多いが，不確実性の定量化には向かない．</p>
<p>このような場面では，頻度論的手法を頻度論的に改善する，という方向は筋が悪いと思われる．<strong>このようなときこそ，もう一つの世界樹であるベイズの方法を用いるべきである</strong>．</p>
<p>これを，科学における再現性の危機を例にとって確認したい．<sup>11</sup></p>
<section id="sec-replication-crisis" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="sec-replication-crisis"><span class="header-section-number">2.3.1</span> 再現性の危機</h4>
<p>多くの実験科学では不確実性の定量化が必要不可欠である <span class="citation" data-cites="Krzywinski-Altman2013">(Krzywinski &amp; Altman, 2013)</span>．</p>
<blockquote class="blockquote">
<p>It is necessary and true that all of the things we say in science, all of the conclusions, are uncertain … <span class="citation" data-cites="Feynman1998">(Feynman, 1998)</span></p>
</blockquote>
<p><a href="https://ja.wikipedia.org/wiki/%E5%86%8D%E7%8F%BE%E6%80%A7%E3%81%AE%E5%8D%B1%E6%A9%9F"><strong>再現性の危機</strong></a> (replication crisis) とは，多くの実験において報告されている統計的有意性が，再現実験において得られないことが多いという問題を指し，2010年代の初めから多くの科学分野において問題として取り上げられてきた．<sup>12</sup></p>
<p>その理由は明白である．<strong>信頼区間は集合値の推定量であるため，「分散」が十分大きいならば，データセットを変えて何回も計算することでいずれは非自明なものを得ることが出来るのである</strong>．そのため，信頼区間や <img src="https://latex.codecogs.com/png.latex?P">-値を報告するだけでは，結果の信頼性については何も保証されないのである．</p>
<p>その結果多くの科学分野では <strong>Bayes 統計学による不確実性の定量化に移行しつつある</strong> <span class="citation" data-cites="Herzog-Ostwald2013">(Herzog &amp; Ostwald, 2013)</span>, <span class="citation" data-cites="Trafimow-Marks2015">(Trafimow &amp; Marks, 2015)</span>, <span class="citation" data-cites="Nuzzo2014">(Nuzzo, 2014)</span>．</p>
<p>信頼区間と信用区間の違いに注目して，その違いを解説する．</p>
</section>
<section id="信頼区間と信用区間" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="信頼区間と信用区間"><span class="header-section-number">2.3.2</span> 信頼区間と信用区間</h4>
<p>「95 % の信頼区間」と言ったとき，「95 % の確率で真の値がその範囲に含まれるような区間」だと思いがちであるが，これはどちらかというと信用区間の説明であり，<strong>信頼区間は計算するごとに値が変わってしまう確率変数である</strong> ことを見落としがちである．<sup>13</sup></p>
<p>つまり，信頼区間は頻度論的な概念であり，「真の値」がまず存在し，区間自体が変動し，95 % の確率で被覆するというのである．今回見ている信頼区間が，別のデータセットで計算した場合にどう変わるかについては全く未知である．</p>
<p>このことは，信頼区間は「真のパラメータの値」で条件づけて得るものであるが，信用区間はデータによって条件づけて得るものであるという点で違う，とまとめられる．この２つの混同は「何で条件づけているか？」を意識することで回避することができる．<sup>14</sup></p>
<p>誤解を恐れず言うならば，再現性の危機とは，信頼区間というサイコロの出目によって科学が踊らされていたということに他ならない <span class="citation" data-cites="Nuzzo2014">(Nuzzo, 2014)</span>．<sup>15</sup></p>
</section>
<section id="なぜベイズを用いれば良いのか" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="なぜベイズを用いれば良いのか"><span class="header-section-number">2.3.3</span> なぜベイズを用いれば良いのか？</h4>
<p>これは，信頼区間や <img src="https://latex.codecogs.com/png.latex?P">-値などの頻度論的な手法は，しばしば尤度原理に違反するためである．<sup>16</sup></p>
<p>換言すれば，何らかのモデルと事前分布に関するベイズ手法と等価である，すなわち，Bayesianly justifiable <span class="citation" data-cites="Rubin1984">(Rubin, 1984)</span> とみなせない手法は，何らかの意味でデータを十分に反映できていない可能性が高くなる．</p>
<p>従って，ベイズの手法が原理的に最も適切である場面が多い．一方でその計算の困難さや，全てのステップをモデリング段階に組み込む点を回避するために，種々の頻度論的な実装は考え得て，頻度論的な手法はそのような運用においては健全であるとの指標にもなる．<sup>17</sup></p>
<p>Bayes により手法を理解し，頻度論的に手法を実装することが，あるべき姿勢であると思われる．</p>
<blockquote class="blockquote">
<p>The applied statistician should be Bayesian in principle and calibrated to the real world in practice. <span class="citation" data-cites="Rubin1984">(Rubin, 1984)</span></p>
</blockquote>
</section>
</section>
<section id="ベイズ深層学習という夢" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="ベイズ深層学習という夢"><span class="header-section-number">2.4</span> ベイズ深層学習という夢</h3>
<p>深層モデルはその性能の高さから，最も実世界応用が期待されるモデルであるが，パラメータが極めて多いため，特にベイズ化することが難しいと言われている．</p>
<p>例えば，ハルシネーション (hallucination) として，LLM が「事実に基づかない」情報を生成してしまうことが問題とされているが，これも不確実性の定量化の問題に他ならない．<sup>18</sup></p>
<p>その他の場面でも，不確実性の定量化には conformal prediction などの事後的な手法が試みられている．<sup>19</sup> これらはどのようなブラックボックスに対しても適用可能である一方で，対症療法というべきものであり，ベイズ流の解釈をすることで直接的に事後分布を求めるという根本的な解決にも，もっと注力されるべきである．</p>
<p>ベイズによる不確実性の定量化は，自然であるだけでなく，より有用な不確実性の定量化を与えるものだと予想している．<sup>20</sup></p>
<p>加えて，事前分布を変えることで，種々の帰納バイアスを加えるという「プロンプトエンジニアリング」ならぬ「プライヤーエンジニアリング」の理論が樹立できるかもしれない．すでに，公平性，同変性，スパース性，共変量シフトへの頑健性などを達成するための事前分布が考えられている．<sup>21</sup></p>
</section>
<section id="分野全体の動向" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="分野全体の動向"><span class="header-section-number">2.5</span> 分野全体の動向</h3>
<p>現状の機械学習モデルと実応用との乖離は，他の側面でも生じている．</p>
<p>まず，訓練データが実際の運用環境を十分に反映できていないということは極めて頻繁に起こるだろう．この現象を <strong>分布シフト</strong> といい，機械学種モデルの予測性能だけで無く，分布外汎化 (out-of-distribution generalization) 能力も重視するという潮流が生じている．</p>
<p>さらに，一度訓練したモデルを，分布シフト自体が移り変わっていく環境で，微調整のみによって繰り返し使い続けるという使用を想定した <strong>継続学習</strong> (continual learning) という考え方もある．<sup>22</sup></p>
<p>章を変えて別の角度から議論を続けよう．</p>
</section>
</section>
<section id="sec-distributional-representation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-distributional-representation"><span class="header-section-number">3</span> ベイズは分布という共通言語を与える</h2>
<section id="継続学習という発想" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="継続学習という発想"><span class="header-section-number">3.1</span> 継続学習という発想</h3>
<p>継続学習は，機械学習モデルをより動的で実際的な環境でも使えるようにするための新たな枠組みである．そこまで，教師あり学習モデルがすでに実用的な性能を獲得したということでもある．</p>
<p>つまり，単に「教師あり」「教師なし」の１タスクを解く営みは爛熟しつつあり，機械学習の理論と応用の最先端は，より深い森に分け入りつつあるのである．</p>
<p>ここにおいて，ベイズ流の接近が統一的な取り扱いを与えるという美点が，さらに重要でもはや必要不可欠な役割を果たすものと思われる．</p>
<section id="ベイズ推論が与える統一的枠組み" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="ベイズ推論が与える統一的枠組み"><span class="header-section-number">3.1.1</span> ベイズ推論が与える統一的枠組み</h4>
<p>ベイズ推論とは，<strong>事前分布</strong> というものを設定して，これをデータによって更新するという営みである（その更新規則は Bayes の公式が与える）．</p>
<p>事前分布をどう設定すれば良いか？の問題は，ベイズ推論の初期からの問題であった．極めて自由度が高いことが，逆にベイズ推論が実際のデータ解析の場面において敬遠される一因ともなっていた．</p>
</section>
<section id="ベイズと最適化との協業" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="ベイズと最適化との協業"><span class="header-section-number">3.1.2</span> ベイズと最適化との協業</h4>
<p>しかし，継続学習が当たり前になった社会において，全てのパラメータ値を事前分布と事後分布とみなし，全ての学習過程をベイズの公式という統一的な方法で更新すると捉えられることは，極めて大きな利点になり得る．</p>
<p>というのも，継続学習においては，学習を繰り返すうちに過去に学んだ内容を忘れ去ってしまうという <strong>壊滅的忘却</strong> (catastrophically forgetting) が最大の困難である．</p>
<p>理論的には，分布のベイズ更新の繰り返しとして見る方が極めて見通しが良い．一方で，事後分布の近似が十分でない場合，実際にベイズ更新を行うことは性能に悪影響を与える．</p>
<p>そこで，理論解析や設計をベイズの観点から行い，実際の推論は最適化ベースで行うという適材適所により，壊滅的忘却を緩和できる可能性がある <span class="citation" data-cites="Farquhar-Gal2019">(Farquhar &amp; Gal, 2019)</span>．</p>
</section>
</section>
<section id="例強化学習への分布によるアプローチ" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="例強化学習への分布によるアプローチ"><span class="header-section-number">3.2</span> 例：強化学習への分布によるアプローチ</h3>
<blockquote class="blockquote">
<p>we believe the value distribution has a central role to play in reinforcement learning. <span class="citation" data-cites="Bellemare+2017">(Bellemare et al., 2017)</span></p>
</blockquote>
</section>
</section>
<section id="sec-inductive-bias" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-inductive-bias"><span class="header-section-number">4</span> ベイズは理解を促進する</h2>
<p>我々はもはや機械学習を通じて，自分たちが何をやっているのかわかっていない．この愚かさを AI に継がせてはならない．</p>
<section id="なぜベイズ法の発展が遅れたか" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="なぜベイズ法の発展が遅れたか"><span class="header-section-number">4.1</span> なぜベイズ法の発展が遅れたか？</h3>
<p>ベイズ法の採用は，自分たちが何をやっているかへの理解と解釈可能性を刺激するという側面がある．</p>
<p>その理由は簡単である．ベイズ推論は，モデルとその上の事前分布を定めれば，あとはベイズ更新規則をどう計算するかの問題となり，近似手法は様々あれど，<strong>もはや推論手法に選択の余地はない</strong>．</p>
<p>換言すれば，その分解析者がモデルと事前分布の特定を全てこなす必要があるのであり，<strong>解析者に確率モデリングへの理解を強要する</strong>ところがある．</p>
<p>しかしこれは「面倒なことは全てアルゴリズムにやってほしい」という精神とは対立するため，ベイズの美点であると同時に，ベイズの発展を阻害してきた遠因の一つでもあった．</p>
<p>これを指して「事前分布の選択に恣意性が入る」という通り文句がよく使われるが，<u>実際は，頻度論的手法における「どのような目的関数をどのように最適化すれば良いか？」という恣意性に変換されているのみであり，問題を先送りにして，「ベイズ法 対 頻度論的手法」という虚構の対立を作り上げているのみである</u>．</p>
<p>機械学習のポテンシャルが具現化したいまこそ，この困難に立ち向かう必要があるが，この問題は最適化や頻度論的な立場から見るより，ベイズの立場から見た方が，理論的な見通しが良いようである（第 4.4 節）．</p>
</section>
<section id="帰納バイアスの明確化の必要性" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="帰納バイアスの明確化の必要性"><span class="header-section-number">4.2</span> 帰納バイアスの明確化の必要性</h3>
<p>機械学習の真の理解のためには，各モデルの帰納バイアスを明確化する必要がある．</p>
<section id="帰納バイアスとは何か" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="帰納バイアスとは何か"><span class="header-section-number">4.2.1</span> 帰納バイアスとは何か？</h4>
<p>現状の AI システムは大量のラベル付きデータが必要であり，多くの現実的に有用なタスクでこのような教師データが用意できるわけではない．</p>
<p>一方で，人間は遥かに少ないデータから効率的に学習することができる．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/Images/model_sizes.png" class="img-fluid figure-img"></p>
<figcaption>Number of Training Tokens <a href="https://babylm.github.io/">BabyLM Challenge</a></figcaption>
</figure>
</div>
<p>その違いは，進化が我々生物に授けた <strong>帰納バイアス</strong> にあると考えられている．</p>
<p>我々には遺伝的に継がれている生まれ持った学習特性があり，より効率的に学習出来るのかも知れない．</p>
<p>事実，一度事前学習をした LLM は，極めて少ないデータにより新しいタスクを学習することができるがわかりつつある <span class="citation" data-cites="Zhou+2023">(Zhou et al., 2023)</span>．<a href="../../../posts/2024/Kernels/Deep2.html#sec-foundation-model">LLM の事後調整に関する稿</a> も参照．</p>
</section>
<section id="事前分布に向き合わずにやり過ごしてきた" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="事前分布に向き合わずにやり過ごしてきた"><span class="header-section-number">4.2.2</span> 事前分布に向き合わずにやり過ごしてきた</h4>
<p>現状，多くの機械学習手法は確率的な方法を取っていない．これは事前分布を明示せずに（ひょっとしたら明後日の方向に向かって）行われる Bayes 学習手法であるとみなせる．</p>
<p>現状の機械学習の成功は，事前分布に関する知識なしに到達されたものであり，それ故の限界がある．例えば，現状のままではモデルにどのような帰納バイアスが組み込まれているか不明瞭である．<sup>23</sup></p>
</section>
<section id="帰納バイアスに対するベイズ的視点" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="帰納バイアスに対するベイズ的視点"><span class="header-section-number">4.2.3</span> 帰納バイアスに対するベイズ的視点</h4>
<p>データの空間 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D"> 上の任意のモデル <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D"> の周辺尤度 <img src="https://latex.codecogs.com/png.latex?p(x%7C%5Cmathcal%7BM%7D)"> は，<sup>24</sup> ベイズ流には事後確率として捉えられ，全てのデータ <img src="https://latex.codecogs.com/png.latex?x%5Cin%5Cmathcal%7BX%7D"> 上に有限な測度を定める．<sup>25</sup></p>
<p>よって，<strong>全てのモデルは，あるデータを得意とするならば他のデータについては不得意であることを免れない</strong>．これは no free lunch 定理と呼ばれる定理の一群により推測されており，分類問題などの簡単なタスクを除いて完全な形式的表現はまだ持たない作業仮設である．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/AI/Images/mackay.png" class="img-fluid figure-img"></p>
<figcaption>A Probabilistic Perspective of Genelization <span class="citation" data-cites="Wilson-Izmailov2020">(Wilson &amp; Izmailov, 2020)</span></figcaption>
</figure>
</div>
<p>例えば，<a href="../../../posts/2024/Kernels/Deep2.html#sec-fine-tuning">基盤モデル</a> とは，インターネット上のデータから最大限人間の言語というものに関する帰納バイアスを取り込んだ，パラメータ上の初期設定であると見れる．</p>
<p>これは，あるパラメータ空間上の理想的な事前分布からのサンプリングであるかも知れない．それ故，種々の下流タスクに対して，小さなモデル変更のみにより適応することが出来る．</p>
<p>大規模言語モデルの能力創発現象は，帰納バイアスを十分取り込むことにより自然に解かれるタスクであったのかもしれない．</p>
</section>
<section id="worst-case-analysis-からの脱皮" class="level4" data-number="4.2.4">
<h4 data-number="4.2.4" class="anchored" data-anchor-id="worst-case-analysis-からの脱皮"><span class="header-section-number">4.2.4</span> worst-case analysis からの脱皮</h4>
<p>帰納バイアスを明確にせず，やり過ごしてきたつけが，特に学習理論においても現れている．</p>
<p>現状の統計的学習理論は全て，worst-case analysis であるが，実用上は全くそうではない．「動くモデル」には暗黙の帰納バイアスが入っており，これに明るくなる必要があるのである．</p>
<p>2024 年に生きる我々は，worst-case analysis からの脱皮を迫られている．</p>
</section>
</section>
<section id="数学者の哲学" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="数学者の哲学"><span class="header-section-number">4.3</span> 数学者の哲学</h3>
<p>Bayes の見方は，機械学習モデルを底流する数理的枠組みになっている．仮に次の <a href="https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%BC%E3%83%B3%E3%83%80%E3%83%BC%E3%82%B9%E3%83%BB%E3%83%9E%E3%83%83%E3%82%AF%E3%83%AC%E3%83%BC%E3%83%B3">Mac Lane</a> の言葉が数学者のあるべき態度の１つであるとするならば，この意味での数学者には Bayes の立場から機械学習を研究することを特におすすめする．</p>
<blockquote class="blockquote">
<p>However, I persisted in the position that <strong>as mathematicians we must know whereof we speak</strong>, be it a homotopy group or an adjoint functor. <span class="citation" data-cites="MacLane1983">(Mac&nbsp;Lane, 1983, p. 55)</span></p>
</blockquote>
<p>数理統計学に始まり，数学者の統計や機械学習分野への参入は，推論手法の解析が想像されるかも知れない．</p>
<p>しかし，真の数学的理解は，手法の数学的な機械仕掛けを紐解くだけでなく，それぞれの手法がモデルとしてどのような仮定の下で成り立っているかを，モデリングの観点から理解することにもあると筆者には思われる．</p>
<p>現状，後者の視点が大変に不足しており，数理的な知識に支えられた大局観というものがない．<u><strong>個々の数学的な道具に捉われず，大局的な構造を捉える数理的枠組みが必要である</strong></u>．</p>
<p>これに応えるのがベイズの枠組みであると筆者は信じる．</p>
<p>推論とモデリングという双対的な営みは深い数理的な構造を持っていることが明らかになりつつある．この大局的構造の解明と理論構築には，ベイズの観点から光を照らしてくれるような，<strong>Mac Lane の意味での数学者的な魂</strong>が必要とされているのである．</p>
<section id="bayes-の数学" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="bayes-の数学"><span class="header-section-number">4.3.1</span> Bayes の数学</h4>
<p>Bayes 流の解釈では，どんなにモデルが複雑で巨大になろうとも，推論とは積分に他ならない．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/AI/Images/Bayes.svg" class="img-fluid figure-img"></p>
<figcaption>Bayes’ Theorem (density form)</figcaption>
</figure>
</div>
<p>全ての（尤度原理に則った）推論は，事後分布の関数としてなされる（べきである）．</p>
<p>実際の実装は，その近似として実行される（べきである）．</p>
<p>よって，実装とモデリングの段階を明確に分離する枠組みを提供している上に，極めて普遍的な枠組みである．</p>
<p>というのも，Bayes 流のモデリングは，<a href="../../../posts/2023/Probability/MarkovCategory.html">Markov 圏</a> 上の図式と見ることができ（第 5.2 節），普遍的である上に，数学的にも最も直接的で直感的な表現であると思われる．</p>
<p>圏として持つ代数的性質は，モデルの結合・分解が自由に出来るということに繋がり，<a href="https://ja.wikipedia.org/wiki/%E3%83%A2%E3%82%B8%E3%83%A5%E3%83%BC%E3%83%AB">モジュール性</a> が高いということになる．</p>
<blockquote class="blockquote">
<p>I basically know of two principles for treating complicated systems in simple ways: the ﬁrst is the <strong>principle of modularity</strong> and the second is the <strong>principle of abstraction</strong>. I am an apologist for computational probability in machine learning because I believe that <strong>probability theory implements these two principles in deep and intriguing ways</strong> — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. Michael I. Jordan excerpted from <span class="citation" data-cites="Frey1998">(Frey, 1998)</span></p>
</blockquote>
<p>分布を明示的に用いた <a href="../../../static/Notations.html#sec-kernels"><strong>確率核</strong></a> を通じてのモデリングは，なぜだか数学的に極めて自然なアプローチを提供してくれるようである．</p>
</section>
<section id="ベイズの代数幾何解析" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="ベイズの代数幾何解析"><span class="header-section-number">4.3.2</span> ベイズの代数・幾何・解析</h4>
<p>上述したように，ベイズのモデリング法と学習規則は本質的に代数的なところがある．</p>
<p>加えて，分布を基本言語とするために，ベイズ推論においては空間 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(%5Cmathcal%7BX%7D)%5Csubset%5Cmathcal%7BM%7D%5E1(%5Cmathcal%7BX%7D)"> が極めて基本的な役割を果たす．</p>
<p>サンプリングは <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(%5Cmathcal%7BX%7D)"> 上の幾何学に関係が深く，情報幾何学や最適輸送などの発展が見られている．</p>
<p>一方で最適化は <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(%5Cmathcal%7BX%7D)"> 上の解析学に関係が深く，古くから機械学習分野では <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D(%5Cmathcal%7BX%7D)"> 上の様々な汎函数が <strong>ダイバージェンス</strong> の名前で考察されており，その勾配流として種々の最適化手法が理解できる．</p>
</section>
<section id="bayes-に繋げる数学" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="bayes-に繋げる数学"><span class="header-section-number">4.3.3</span> Bayes に繋げる数学</h4>
<p>通常の頻度論的手法は，うまくいくことが先であり，理論が後付けされる．そしてその理論もどこか ad-hoc というべきであり，worst-case で漸近論的である．</p>
<p>これらに Bayes 的な解釈を与えることで，暗黙のうちにどのような仮定を課しているモデリング手法に相等するのか明確にされる．特に，非漸近論的な知見を与えてくれる数少ないの道の一つである．</p>
</section>
</section>
<section id="sec-Bayesian-rule" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="sec-Bayesian-rule"><span class="header-section-number">4.4</span> ベイズ推論とみる美点</h3>
<p>ベイズ推論自体への理解だけでなく，種々の頻度論的手法を（特定の環境下での）ベイズ推論の近似として理解することは，新たなアルゴリズムの開発に有用であるという合意が形成されつつあるようである．<sup>26</sup></p>
<p>最適化に基づく手法の計算効率性は，正確なベイズ推論に勝る場面も多い．ここで注意すべきは，ベイズ推論の実行が肝要であり，その実装は最適化に依ろうと，積分近似に依ろうと大した違いではないのである．</p>
<p>「ベイズ推論は多くの最尤法に基づく手法よりも，自然な正則化がなされるために過学習の問題がない．」と説明されるが<u><strong>本来は逆である</strong></u>．多くの最適化に基づく手法は，目的関数の選択に恣意性があり，その選択を誤り続けているために過学習という問題が生じている，という方が，後世の教科書に載る表現なのではないかと筆者は考えている．</p>
<p>そこで，種々の既存手法のベイズ流の解釈を探究することは，より良い推論アルゴリズムの開発に資すると考えられている．</p>
<p>この方向の近年の発展をいくつか紹介したい．</p>
<section id="ベイズ学習規則" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="ベイズ学習規則"><span class="header-section-number">4.4.1</span> ベイズ学習規則</h4>
<p>現状の機械学習は，統計学，連続最適化，計算機科学の知識を総動員して開発された種々の推論手法によって支えられている．</p>
<p>その性能は驚異的なスピードで向上しているが，それぞれの手法がどのような仮定をモデリングの段階で課しているかが不明瞭であり，どの手法を使うべきかの統一的な枠組みは得られていない．</p>
<p>この現状の抜本的な改善が，それぞれの手法のベイズ流の解釈を探究することで得られると考えられる．</p>
<p>その枠組みの一つが <strong>ベイズ学習規則</strong> <span class="citation" data-cites="Khan-Rue2023">(Khan &amp; Rue, 2023)</span> である．</p>
<p><span class="citation" data-cites="Khan-Rue2023">(Khan &amp; Rue, 2023, p. 4)</span> では，ベイズ流の解釈を持つ種々の手法が他より優れている理由として，目的関数に現れるエントロピー項が <strong>自然勾配</strong> の概念を通じて自然な正則化を与えることが，ベイズ学習規則という新たな理論的枠組みの中で示されている．</p>
</section>
<section id="例強化学習" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="例強化学習"><span class="header-section-number">4.4.2</span> 例：強化学習</h4>
<p>強化学習でも，モデルベースのアプローチが取り入れられつつあり <span class="citation" data-cites="Deisenroth-Rasmussen2011">(Deisenroth &amp; Rasmussen, 2011)</span>，さらに学習と制御をベイズ推論と見ることが，アルゴリズムの設計において有用であることが提唱されつつある：</p>
<blockquote class="blockquote">
<p>Crucially, in the framework of PGMs, it is sufficient to write down the model and pose the question, and the objectives for learning and inference emerge automatically. <span class="citation" data-cites="Levine2018">(Levine, 2018)</span></p>
</blockquote>
</section>
</section>
</section>
<section id="bayes-機械学習の例" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="bayes-機械学習の例"><span class="header-section-number">5</span> Bayes 機械学習の例</h2>
<div class="callout callout-style-default callout-important no-icon callout-titled" title="要約">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
要約
</div>
</div>
<div class="callout-body-container callout-body">
<p>深層学習モデルにより教師あり学習は十分に発展し，多くの訓練データが得られる場面では驚異的な性能を発揮するようになった．</p>
<p>この発展は，モデリングの仮定に捉われずに純粋にアルゴリズムの開発に集中することが出来るという頻度論的な枠組みの利点を有効活用する形で達成された．</p>
<p>しかし，殆どの実世界応用では，不確実性のモデリングが必要不可欠である．この点を後回しにして性能を追求することで得た栄華である．だからこそ，極めて高い性能を誇るモデルを，実世界応用の場面で有効活用する手段を我々はまだ知らないのである．</p>
<p>その鍵はベイズにある．安全性，信頼性，柔軟性……．これらの21世紀の社会の要請に応えるためには，ベイズ機械学習手法の発展と，既存の手法のベイズ流の理解とが追いつくことが，第一歩である．</p>
</div>
</div>
<p>既存の深層学習モデルは，「教師あり学習」という枠組みや，画像の分類タスクや自然言語処理のタスクなど，広く周知された問題設定とデータセットが存在する．</p>
<p>一方で，ベイズ機械学習における対応物はまだ十分に周知されていないようである．</p>
<p>ベイズ機械学習では「損失を最小化する」という枠組みの中でなるべく性能の良い推論手法を探す，というわかりやすい枠組みがある訳ではないようである．</p>
<p>そこで，本章ではベイズ機械学習の近年の発展を概観することを試みる．</p>
<section id="bayes-深層学習" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="bayes-深層学習"><span class="header-section-number">5.1</span> Bayes 深層学習</h3>
<p>ニューラルネットワークモデルは，隠れ素子数が無限大になる極限において，Gauss 過程モデルに漸近することが知られている <span class="citation" data-cites="Neal1996">(Neal, 1996)</span>．Gauss 過程とはノンパラメトリックなベイズ機械学習手法の代表である．この対応を通じて，深層学習のベイズ流の解釈が進められている．</p>
<p>この稿の執筆後，本稿をまとめるかのようなアブストラクトを持ったポジションペーパー <span class="citation" data-cites="Papamarkou+2024">(Papamarkou et al., 2024)</span> が公開された</p>
<blockquote class="blockquote">
<p>In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. <span class="citation" data-cites="Papamarkou+2024">(Papamarkou et al., 2024)</span></p>
</blockquote>
<p>深層学習をベイズ化することで，上にあげた</p>
<ul>
<li>不確実性の自然な定量化</li>
<li>継続学習への柔軟な接続</li>
<li>科学的営みの促進</li>
</ul>
<p>などが目指せる．特に，現状の大規模な基盤モデルをベイズ化する悲願を真っ向から論じている．</p>
</section>
<section id="sec-graphical-model" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="sec-graphical-model"><span class="header-section-number">5.2</span> 確率的グラフィカルモデル</h3>
<p>歴史的に，（確率的）モデリングは，主に（確率的）グラフィカルモデルを通じて機械学習の分野に導入された．</p>
<p>そのため，20世紀に入ったばかりの頃は，Bayes 機械学習の唯一の例は確率的グラフィカルモデルなのであった．<sup>27</sup></p>
<p>だが，確率的グラフィカルモデルは，極めて普遍的で，従来の因果推論・階層モデル・欠測モデル・潜在変数モデル・構造方程式モデルなどの発展を包含する統一的な枠組みであることをより広く認識すべきである．</p>
<section id="ベイジアンネットワーク" class="level4" data-number="5.2.1">
<h4 data-number="5.2.1" class="anchored" data-anchor-id="ベイジアンネットワーク"><span class="header-section-number">5.2.1</span> ベイジアンネットワーク</h4>
<p>Bayesian Network は Markov 圏上の図式であり，方向関係のある変数間の関係をモデリングする最も直接的な方法である．</p>
</section>
<section id="構造的因果モデル" class="level4" data-number="5.2.2">
<h4 data-number="5.2.2" class="anchored" data-anchor-id="構造的因果モデル"><span class="header-section-number">5.2.2</span> 構造的因果モデル</h4>
</section>
<section id="階層モデル" class="level4" data-number="5.2.3">
<h4 data-number="5.2.3" class="anchored" data-anchor-id="階層モデル"><span class="header-section-number">5.2.3</span> 階層モデル</h4>
<p>階層モデルとは，ベイズの枠組みでは，観測変数・潜在変数の区別なく，モデルを自由に結合出来る点を利用したモデリング手法である．</p>
</section>
<section id="モデルの属人化" class="level4" data-number="5.2.4">
<h4 data-number="5.2.4" class="anchored" data-anchor-id="モデルの属人化"><span class="header-section-number">5.2.4</span> モデルの属人化</h4>
<p>大きなデータも，属人化医療や推薦システムなど多くの文脈では小さなデータの寄せ集めであり，そうでなくともその構造を正しく捉え，全ての不確実性を取り入れた柔軟なモデリングをすることで，さらに密接な形で社会に取り入れることができる．<sup>28</sup></p>
</section>
</section>
<section id="確率的プログラミング" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="確率的プログラミング"><span class="header-section-number">5.3</span> 確率的プログラミング</h3>
<section id="アルゴリズムのプログラミングからモデルのプログラミングへ" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="アルゴリズムのプログラミングからモデルのプログラミングへ"><span class="header-section-number">5.3.1</span> アルゴリズムのプログラミングから，モデルのプログラミングへ</h4>
<p>ベイズ流の解釈では，解析者の恣意的な選択はモデリングの段階に集中しており，モデルが決定すれば推論手法は自動的に従う．</p>
<p>このパラダイムでは，推論手法は背後に隠し，解析者はモデリングに集中するための新たなプログラミング言語があっても良いはずである．</p>
<p>このような言語を <strong>確率的プログラミング</strong> (Probabilistic Programming) 言語と呼ぶ．</p>
</section>
<section id="確率的プログラミングはグラフィカルモデルの拡張である" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="確率的プログラミングはグラフィカルモデルの拡張である"><span class="header-section-number">5.3.2</span> 確率的プログラミングはグラフィカルモデルの拡張である</h4>
<p>確率的グラフィカルモデルをどのようにプログラムに落とし込むかというと，確率核をシミュレーターとして実装するのである．</p>
<p>逆に，シミュレーションが可能な限りどのようなモデルも実装できるので，確率的グラフィカルモデルの真の拡張であると言える．<sup>29</sup></p>
</section>
<section id="simulation-based-inference" class="level4" data-number="5.3.3">
<h4 data-number="5.3.3" class="anchored" data-anchor-id="simulation-based-inference"><span class="header-section-number">5.3.3</span> Simulation-based Inference</h4>
<p>上述の通り，シミュレーターがあればモデルが定義でき，モデルがあれば推論ができる．さらに，棄却法，重点サンプリング法，MCMC，SMC などの Monte Carlo 法のレパートリーにより，殆どあらゆるシミュレーションと推論が統一的に実行できる．これが Bayes 推論の強みである．</p>
</section>
</section>
<section id="bayes-最適化" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="bayes-最適化"><span class="header-section-number">5.4</span> Bayes 最適化</h3>
<p>ベイズはシステムの一部として自然に組み込まれると論じたが（第 2.1 節），現状その最先端をいくのがベイズ最適化の分野である．</p>
<p>ベイズ最適化は最も簡単な形では，未知の関数 <img src="https://latex.codecogs.com/png.latex?f:X%5Cto%5Cmathbb%7BR%7D"> の最大値点を求める問題を，<a href="https://en.wikipedia.org/wiki/Sequential_analysis">逐次意思決定問題</a> として解く手法である．</p>
<p>ベイズ数値計算 <span class="citation" data-cites="Hagen1991">(O’Hagan, 1991)</span> の現代的な再解釈とも捉えられる．<sup>30</sup></p>
<p>この際，未知の関数 <img src="https://latex.codecogs.com/png.latex?f"> を Gauss 過程などでモデリングし，不確実性の高い点からサンプル <img src="https://latex.codecogs.com/png.latex?f(x_1),f(x_2),%5Ccdots"> を取って最も効率の良い方法で最大化していくことを目指す．</p>
<p>ベイズ最適化は多腕バンディット問題と関係が深く，２つの問題は共に一方向のエージェント・環境相互作用しか仮定していないという形での強化学習への入り口である．</p>
</section>
<section id="確率的データ圧縮" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="確率的データ圧縮"><span class="header-section-number">5.5</span> 確率的データ圧縮</h3>
<p>殆どの（可逆）データ圧縮アルゴリズムは，シンボルの列に対する確率的モデリングと等価である．<sup>31</sup> そしてモデルの予測精度が良いほど，データの圧縮率は高い．</p>
<p>したがって，より良いベイズ（ノンパラメトリック）モデルの開発と，より幅広いデータに対するデータ圧縮技術の発展とは両輪である．<sup>32</sup></p>
</section>
<section id="モデルの自動発見" class="level3" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="モデルの自動発見"><span class="header-section-number">5.6</span> モデルの自動発見</h3>
<p>機械学習の精神の一つに，データからの知識獲得をなるべく自動化したいというものがある．</p>
<p>ベイズの方から，統計解析自体を自動化する Automatic Statistician <span class="citation" data-cites="Lloyd+2014">(Lloyd et al., 2014)</span> という試みがある．これはデータを説明するモデルを自動発見し，結果を自然言語でまとめてくれる上に，モデルに含まれる不確実性に関しても報告してくれる．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Amershi+2019" class="csl-entry">
Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., Suh, J., Iqbal, S., Bennett, P. N., Inkpen, K., Teevan, J., Kikin-Gil, R., &amp; Horvitz, E. (2019). Guidelines for human-AI interaction. <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, 1–13. <a href="https://doi.org/10.1145/3290605.3300233">https://doi.org/10.1145/3290605.3300233</a>
</div>
<div id="ref-Angrist-Pischke2010" class="csl-entry">
Angrist, J. D., &amp; Pischke, J.-S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. <em>The Journal of Economic Perspectives</em>, <em>24</em>(2), 3–30. <a href="http://www.jstor.org/stable/25703496">http://www.jstor.org/stable/25703496</a>
</div>
<div id="ref-Bensal+2019" class="csl-entry">
Bansal, G., Nushi, B., Kamar, E., Weld, D. S., Lasecki, W. S., &amp; Horvitz, E. (2019). Updates in human-AI teams: Understanding and addressing the performance/compatibility tradeoff. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>33</em>(01), 2429–2437. <a href="https://doi.org/10.1609/aaai.v33i01.33012429">https://doi.org/10.1609/aaai.v33i01.33012429</a>
</div>
<div id="ref-Bellemare+2017" class="csl-entry">
Bellemare, M. G., Dabney, W., &amp; Munos, R. (2017). A distributional perspective on reinforcement learning. In D. Precup &amp; Y. W. Teh (Eds.), <em>Proceedings of the 34th international conference on machine learning</em> (Vol. 70, pp. 449–458). PMLR. <a href="https://proceedings.mlr.press/v70/bellemare17a.html">https://proceedings.mlr.press/v70/bellemare17a.html</a>
</div>
<div id="ref-Broderick+2023" class="csl-entry">
Broderick, T., Gelman, A., Meager, R., Smith, A. L., &amp; Zheng, T. (2023). Toward a taxonomy of trust for probabilistic machine learning. <em>Science Advances</em>, <em>9</em>(7), eabn3999. <a href="https://doi.org/10.1126/sciadv.abn3999">https://doi.org/10.1126/sciadv.abn3999</a>
</div>
<div id="ref-Chen+2023" class="csl-entry">
Chen, J., Monica, J., Chao, W.-L., &amp; Campbell, M. (2023). <em>Probabilistic uncertainty quantification of prediction models with application to visual localization</em>. <a href="https://arxiv.org/abs/2305.20044">https://arxiv.org/abs/2305.20044</a>
</div>
<div id="ref-Deisenroth-Rasmussen2011" class="csl-entry">
Deisenroth, M. P., &amp; Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient approach to policy search. <em>Proceedings of the 28th International Conference on International Conference on Machine Learning</em>, 465–472.
</div>
<div id="ref-Efron1986" class="csl-entry">
Efron, B. (1986). Why isn’t everyone a bayesian? <em>The American Statistician</em>, <em>40</em>(1), 1–5. <a href="http://www.jstor.org/stable/2683105">http://www.jstor.org/stable/2683105</a>
</div>
<div id="ref-Farquhar-Gal2019" class="csl-entry">
Farquhar, S., &amp; Gal, Y. (2019). <em>A unifying bayesian view of continual learning</em>. <a href="https://arxiv.org/abs/1902.06494">https://arxiv.org/abs/1902.06494</a>
</div>
<div id="ref-Feynman1998" class="csl-entry">
Feynman, R. P. (1998). <em><a href="">The meaning of it all: Thoughts of a citizen scientist</a></em>. Addison-Wesley.
</div>
<div id="ref-Frey1998" class="csl-entry">
Frey, B. J. (1998). <em>Graphical models for machine learning and digital communication</em>. The MIT Press. <a href="https://mitpress.mit.edu/9780262062022/graphical-models-for-machine-learning-and-digital-communication/">https://mitpress.mit.edu/9780262062022/graphical-models-for-machine-learning-and-digital-communication/</a>
</div>
<div id="ref-Gal-Ghahramani2016" class="csl-entry">
Gal, Y., &amp; Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In M. F. Balcan &amp; K. Q. Weinberger (Eds.), <em>Proceedings of the 33rd international conference on machine learning</em> (Vol. 48, pp. 1050–1059). PMLR. <a href="https://proceedings.mlr.press/v48/gal16.html">https://proceedings.mlr.press/v48/gal16.html</a>
</div>
<div id="ref-Ghahramani2015" class="csl-entry">
Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. <em>Nature</em>, <em>521</em>, 452–459. <a href="https://www.nature.com/articles/nature14541">https://www.nature.com/articles/nature14541</a>
</div>
<div id="ref-Herzog-Ostwald2013" class="csl-entry">
Herzog, S., &amp; Ostwald, D. (2013). Sometimes bayesian statistics are better. <em>Nature</em>, <em>494</em>(7435), 35–35. <a href="https://doi.org/10.1038/494035b">https://doi.org/10.1038/494035b</a>
</div>
<div id="ref-Khan-Rue2023" class="csl-entry">
Khan, M. E., &amp; Rue, H. (2023). The bayesian learning rule. <em>Journal of Machine Learning Research</em>, <em>24</em>(281), 1–46. <a href="http://jmlr.org/papers/v24/22-0291.html">http://jmlr.org/papers/v24/22-0291.html</a>
</div>
<div id="ref-Krzywinski-Altman2013" class="csl-entry">
Krzywinski, M., &amp; Altman, N. (2013). Importance of being uncertain. <em>Nature Methods</em>, <em>10</em>(9), 809–810. <a href="https://doi.org/10.1038/nmeth.2613">https://doi.org/10.1038/nmeth.2613</a>
</div>
<div id="ref-Levine2018" class="csl-entry">
Levine, S. (2018). <em>Reinforcement learning and control as probabilistic inference: Tutorial and review</em>. <a href="https://arxiv.org/abs/1805.00909">https://arxiv.org/abs/1805.00909</a>
</div>
<div id="ref-Lloyd+2014" class="csl-entry">
Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum, J. B., &amp; Ghahramani, Z. (2014). Automatic construction and natural-language description of nonparametric regression models. <em>Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</em>, 1242–1250.
</div>
<div id="ref-MacLane1983" class="csl-entry">
Mac&nbsp;Lane, S. (1983). The health of mathematics. <em>The Mathematical Intelligencer</em>, <em>5</em>(4), 53–56. <a href="https://doi.org/10.1007/BF03026510">https://doi.org/10.1007/BF03026510</a>
</div>
<div id="ref-Mohri-Hashimoto2024" class="csl-entry">
Mohri, C., &amp; Hashimoto, T. (2024). <em>Language models with conformal factuality guarantees</em>. <a href="https://arxiv.org/abs/2402.10978">https://arxiv.org/abs/2402.10978</a>
</div>
<div id="ref-Murphy2022" class="csl-entry">
Murphy, K. P. (2022). <em>Probabilistic machine learning: An introduction</em>. MIT Press. <a href="https://probml.github.io/pml-book/book1.html">https://probml.github.io/pml-book/book1.html</a>
</div>
<div id="ref-Neal1996" class="csl-entry">
Neal, R. M. (1996). <em>Bayesian learning for neural networks</em> (Vol. 118). Springer New York. <a href="https://link.springer.com/book/10.1007/978-1-4612-0745-0">https://link.springer.com/book/10.1007/978-1-4612-0745-0</a>
</div>
<div id="ref-Neal-Hinton1998" class="csl-entry">
Neal, R. M., &amp; Hinton, G. E. (1998). <em>Learning in graphical models</em> (M. I. Jordan, Ed.; pp. 355–368). Springer Dordrecht. <a href="https://link.springer.com/chapter/10.1007/978-94-011-5014-9_12">https://link.springer.com/chapter/10.1007/978-94-011-5014-9_12</a>
</div>
<div id="ref-Novello+2024" class="csl-entry">
Novello, P., Dalmau, J., &amp; Andeol, L. (2024). <em>Out-of-distribution detection should use conformal prediction (and vice-versa?)</em>. <a href="https://arxiv.org/abs/2403.11532">https://arxiv.org/abs/2403.11532</a>
</div>
<div id="ref-Nuzzo2014" class="csl-entry">
Nuzzo, R. (2014). Scientific method: Statistical errors. <em>Nature</em>, <em>506</em>(7487), 150–152. <a href="https://doi.org/10.1038/506150a">https://doi.org/10.1038/506150a</a>
</div>
<div id="ref-Hagen1991" class="csl-entry">
O’Hagan, A. (1991). Bayes–hermite quadrature. <em>Journal of Statistical Planning and Inference</em>, <em>29</em>(3), 245–260. https://doi.org/<a href="https://doi.org/10.1016/0378-3758(91)90002-V">https://doi.org/10.1016/0378-3758(91)90002-V</a>
</div>
<div id="ref-Papamarkou+2024" class="csl-entry">
Papamarkou, T., Skoularidou, M., Palla, K., Aitchison, L., Arbel, J., Dunson, D., Filippone, M., Fortuin, V., Hennig, P., Lobato, J. M. H., Hubin, A., Immer, A., Karaletsos, T., Khan, M. E., Kristiadi, A., Li, Y., Mandt, S., Nemeth, C., Osborne, M. A., … Zhang, R. (2024). <em>Position paper: Bayesian deep learning in the age of large-scale AI</em>. <a href="https://arxiv.org/abs/2402.00809">https://arxiv.org/abs/2402.00809</a>
</div>
<div id="ref-Rubin1984" class="csl-entry">
Rubin, D. B. (1984). <span class="nocase">Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician</span>. <em>The Annals of Statistics</em>, <em>12</em>(4), 1151–1172. <a href="https://doi.org/10.1214/aos/1176346785">https://doi.org/10.1214/aos/1176346785</a>
</div>
<div id="ref-Steinruecken+2015" class="csl-entry">
Steinruecken, C., Ghahramani, Z., &amp; MacKay, D. (2015). Improving PPM with dynamic parameter updates. <em>2015 Data Compression Conference</em>, 193–202. <a href="https://doi.org/10.1109/DCC.2015.77">https://doi.org/10.1109/DCC.2015.77</a>
</div>
<div id="ref-Trafimow-Marks2015" class="csl-entry">
Trafimow, D., &amp; Marks, M. (2015). Editorial. <em>Basic and Applied Social Psychology</em>, <em>37</em>(1), 1–2. <a href="https://doi.org/10.1080/01973533.2015.1012991">https://doi.org/10.1080/01973533.2015.1012991</a>
</div>
<div id="ref-Wamg+2024" class="csl-entry">
Wang, L., Zhang, X., Su, H., &amp; Zhu, J. (2024). A comprehensive survey of continual learning: Theory, method and application. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 1–20. <a href="https://doi.org/10.1109/TPAMI.2024.3367329">https://doi.org/10.1109/TPAMI.2024.3367329</a>
</div>
<div id="ref-Wilson-Izmailov2020" class="csl-entry">
Wilson, A. G., &amp; Izmailov, P. (2020). Bayesian deep learning and a probabilistic perspective of generalization. <em>Proceedings of the 34th International Conference on Neural Information Processing Systems</em>. <a href="https://proceedings.neurips.cc/paper/2020/hash/322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/322f62469c5e3c7dc3e58f5a4d1ea399-Abstract.html</a>
</div>
<div id="ref-Zhou+2023" class="csl-entry">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., YU, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., &amp; Levy, O. (2023). <span>LIMA</span>: Less is more for alignment. <em>Thirty-Seventh Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=KBMOKmX2he">https://openreview.net/forum?id=KBMOKmX2he</a>
</div>
<div id="ref-平石-中村2022" class="csl-entry">
平石界, &amp; 中村大輝. (2022). 心理学における再現性危機の10年―危機は克服されたのか，克服され得るのか―. <em>科学哲学</em>, <em>54</em>(2), 27–50. <a href="https://www.jstage.jst.go.jp/article/jpssj/54/2/54_27/_article/-char/ja">https://www.jstage.jst.go.jp/article/jpssj/54/2/54_27/_article/-char/ja</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>これは <a href="https://www.microsoft.com/en-us/research/project/guidelines-for-human-ai-interaction/">Human-AI interaction におけるガイドライン</a> <span class="citation" data-cites="Amershi+2019">(Amershi et al., 2019)</span>, <span class="citation" data-cites="Bensal+2019">(Bansal et al., 2019)</span> でも明確にされている点である．この方向への試みの代表がベイズ機械学習，というわけではないが，筆者はベイズ機械学習の興隆は信頼のおける AI システムの構築にための極めて盤石な土台になるだろうと論じる．↩︎</p></li>
<li id="fn2"><p>本稿執筆後に，ほとんど同じ論調を，深層学習や基盤モデルを中心に，遥かに明瞭に述べた論文 <span class="citation" data-cites="Papamarkou+2024">(Papamarkou et al., 2024)</span> を見つけたので，賢明な読者はぜひこちらを参考にしていただきたい．↩︎</p></li>
<li id="fn3"><p><span class="citation" data-cites="Broderick+2023">(Broderick et al., 2023, p. 2)</span> など．↩︎</p></li>
<li id="fn4"><p>この違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 454)</span>．↩︎</p></li>
<li id="fn5"><p>“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 454)</span>．このように，その用いる手法も鮮やかに対照的に見えるが，積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．↩︎</p></li>
<li id="fn6"><p><span class="citation" data-cites="Broderick+2023">(Broderick et al., 2023)</span> が極めて説得的にこの点を指摘している．↩︎</p></li>
<li id="fn7"><p>合理的な信念の度合い (degree of belief) は確率の公理を満たす必要がある，という主張は <a href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox の名前でも呼ばれる</a>．この点から，Bayes の定理は，帰納的推論の確率論的な拡張だとも捉えられる．“This justifies the use of subjective Bayesian probabilistic representations in artificial intelligence.” “Probabilistic modelling also has some conceptual advantages over alternatives because it is a normative theory for learning in artificially intelligent systems.” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 453)</span>．↩︎</p></li>
<li id="fn8"><p>現状，日本にてベイズ機械学習を専業として研究を進めている人は <a href="https://emtiyaz.github.io/">Emtiyaz Khan</a> に限ると思われる．<span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 452)</span> でも “Probabilistic approaches have only recently become a mainstream approach to artificial intellifence, robotics, and machine learning.” と述べられている．↩︎</p></li>
<li id="fn9"><p>“The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars.” <span class="citation" data-cites="Chen+2023">(Chen et al., 2023)</span>．↩︎</p></li>
<li id="fn10"><p>モデルの予測結果に不確実性の定量化が伴われていたならば，モデルを信用出来ない場面で意思決定者がこれを信用したため責任があるのか，使用者には非難可能性がないのか，モデル設計者に過失があったと言えるのかの議論に，足場を与えることが出来るだろう．↩︎</p></li>
<li id="fn11"><p><span class="citation" data-cites="Gal-Ghahramani2016">(Gal &amp; Ghahramani, 2016)</span> も参照．↩︎</p></li>
<li id="fn12"><p>心理学においては「再現性問題が大きく注目される大きな契機となった「超能力論文」が出版されたのが 2011 年である」 <span class="citation" data-cites="平石-中村2022">(平石界 &amp; 中村大輝, 2022)</span> ようである．計量経済学における <strong>信頼性革命</strong> <span class="citation" data-cites="Angrist-Pischke2010">(Angrist &amp; Pischke, 2010)</span> は，再現性の危機の，もう一つの革新的な解決法である．↩︎</p></li>
<li id="fn13"><p>「それでは，信頼区間は不確実性の正しい定量化を与えないではないか！」ということになるが，その通りなのである．<img src="https://latex.codecogs.com/png.latex?P">-値を計算する過程とは，帰無仮説で条件付けているだけであり，データの関数でもある．<img src="https://latex.codecogs.com/png.latex?P">-値の確率変数としての分散が大きいほど，何回か同じ実験を繰り返せばすぐに小さな <img src="https://latex.codecogs.com/png.latex?P">-値が得られることになる．これは <a href="../../../posts/2023/数理法務/法律家のための統計数理2.html#sec-Bayes-problem"><strong>基準確率の誤謬</strong></a> と似ている．↩︎</p></li>
<li id="fn14"><p>“Confidence intervals suffer from an inverse inference problem that is not very different from that suffered by the NHSTP. In the NHSTP, the problem is in traversing the distance from the probability of the finding, given the null hypothesis, to the probability of the null hypothesis, given the finding.” <span class="citation" data-cites="Trafimow-Marks2015">(Trafimow &amp; Marks, 2015)</span>↩︎</p></li>
<li id="fn15"><p><span class="citation" data-cites="Nuzzo2014">(Nuzzo, 2014)</span> には，Fisher が最初に用いてから，Neyman-Pearson 理論がこれを排除したものの，コミュニティが <img src="https://latex.codecogs.com/png.latex?P">-値を誤解して都合の良いように利用するようになるまでに至った歴史が説明されている．↩︎</p></li>
<li id="fn16"><p><span class="citation" data-cites="Murphy2022">(Murphy, 2022, p. 201)</span> の議論も参照．↩︎</p></li>
<li id="fn17"><p><span class="citation" data-cites="Efron1986">(Efron, 1986)</span> も示唆深い．↩︎</p></li>
<li id="fn18"><p><span class="citation" data-cites="Mohri-Hashimoto2024">(Mohri &amp; Hashimoto, 2024)</span>, <span class="citation" data-cites="Papamarkou+2024">(Papamarkou et al., 2024, p. 3)</span> 2.1節 なども指摘している．↩︎</p></li>
<li id="fn19"><p><span class="citation" data-cites="Novello+2024">(Novello et al., 2024)</span> では out-of-distribution detection, <span class="citation" data-cites="Mohri-Hashimoto2024">(Mohri &amp; Hashimoto, 2024)</span> は LLM の hallucination への応用．↩︎</p></li>
<li id="fn20"><p>「筆者は，conformal prediction などの post-hoc な手法は，便利かも知れないが，「信頼区間」や「<img src="https://latex.codecogs.com/png.latex?P">-値」のような側面（第 2.3 節）も併せ持つのではないかと危惧しながら見ている．」と当初は書いていたが，どうもそう簡単な話ではないようである．<span class="citation" data-cites="Papamarkou+2024">(Papamarkou et al., 2024)</span> を読んで思った．↩︎</p></li>
<li id="fn21"><p><span class="citation" data-cites="Papamarkou+2024">(Papamarkou et al., 2024, p. 5)</span> 3.4節．↩︎</p></li>
<li id="fn22"><p><span class="citation" data-cites="Wamg+2024">(Wang et al., 2024)</span> が最新のサーベイであるようだ．↩︎</p></li>
<li id="fn23"><p>Philipp Hennig <a href="https://youtu.be/TTo2kjrAuTo?si=QD_pqMkdLOl52OsR&amp;t=3703"><em>Probabilistic ML - Lecture 1 - Introduction</em></a> “Statistical Learning Theory is about Bayesian Reasoning when you don’t say out aloud what the prior is.”↩︎</p></li>
<li id="fn24"><p>これを <a href="https://en.wikipedia.org/wiki/Marginal_likelihood"><strong>証拠</strong></a> (model evidence) ともいう．↩︎</p></li>
<li id="fn25"><p>事前分布として非有限な測度を用いた場合など，例外もある．↩︎</p></li>
<li id="fn26"><p>“most conventional optimization-based machine-learning approaches have probabilistic analogues that handle uncertainty in a more principled manner.” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 458)</span>．↩︎</p></li>
<li id="fn27"><p><span class="citation" data-cites="Neal-Hinton1998">(Neal &amp; Hinton, 1998)</span> など．↩︎</p></li>
<li id="fn28"><p><span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 458)</span> はこれを <strong>モデルの属人化</strong> (personalization of models) と呼んでいる．↩︎</p></li>
<li id="fn29"><p><span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 453)</span> “probabilistic programming offers an elegant way of generalizing graphical models, allowing a much richer representation of models.”．↩︎</p></li>
<li id="fn30"><p>“More generally, Bayesian optimization is a special case of Bayesian numerical computation, which is re-emerging as a very active area of research, and includes topics such as solving ordinary differential equations and numerical integration.” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 456)</span>．↩︎</p></li>
<li id="fn31"><p>“All commonly used lossless data compression algorithms (for example, <code>gzip</code>) can be viewed as probabilistic models of sequences of symbols.” <span class="citation" data-cites="Ghahramani2015">(Ghahramani, 2015, p. 456)</span>．↩︎</p></li>
<li id="fn32"><p><span class="citation" data-cites="Steinruecken+2015">(Steinruecken et al., 2015)</span> は記号列に対するノンパラメトリックモデルを改良することで，データ圧縮アルゴリズム <code>PPM</code> を改良した良い例である．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Bayesian</category>
  <category>AI</category>
  <category>Opinion</category>
  <guid>https://162348.github.io/posts/2024/AI/BAI.html</guid>
  <pubDate>Tue, 19 Mar 2024 15:00:00 GMT</pubDate>
</item>
<item>
  <title>半導体入門</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/AI/Semiconductor.html</link>
  <description><![CDATA[ 





<iframe class="slide-deck" src="Semiconductor_slides.html"></iframe>
<p><a href="../../../posts/2024/AI/Semiconductor_slides.html">スライドを全画面で開くにはこちら</a></p>
<section id="半導体入門" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="半導体入門"><span class="header-section-number">1</span> 半導体入門</h2>
<section id="半導体とは" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="半導体とは"><span class="header-section-number">1.1</span> 半導体とは？</h3>
<p>価電子帯と伝導帯の間の <a href="https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%B3%E3%83%89%E3%82%AE%E3%83%A3%E3%83%83%E3%83%97"><strong>禁制帯</strong></a> (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を <a href="https://ja.wikipedia.org/wiki/%E5%8D%8A%E5%B0%8E%E4%BD%93"><strong>半導体</strong></a> という．<sup>1</sup></p>
<p>このような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．<sup>2</sup> この <a href="https://ja.wikipedia.org/wiki/%E6%AD%A3%E5%AD%94">正孔</a> (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが <span class="citation" data-cites="Heisenberg1931">(Heisenberg, 1931)</span> である．</p>
</section>
<section id="半導体発見の歴史" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="半導体発見の歴史"><span class="header-section-number">1.2</span> 半導体発見の歴史</h3>
<p><span class="citation" data-cites="Faraday1833">(Faraday, 1833)</span> は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag<sub>2</sub>S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．</p>
<p><span class="citation" data-cites="Braun1874">(Braun, 1874)</span> は <a href="https://ja.wikipedia.org/wiki/%E6%96%B9%E9%89%9B%E9%89%B1">方鉛鉱</a> PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，<sup>3</sup> その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える <span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012, p. 1)</span>．<sup>4</sup></p>
<p>Braun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．</p>
<p><span class="citation" data-cites="Round1907">(Round, 1907)</span> はダイオードが電界を印加することで発行することがある (electroluminescence) ことを発見した．</p>
</section>
<section id="基本用語のまとめ" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="基本用語のまとめ"><span class="header-section-number">1.3</span> 基本用語のまとめ</h3>
<p>半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した <strong>ウエハ (wafer)</strong> 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/AI/Images/chip.png" class="img-fluid figure-img"></p>
<figcaption><a href="https://www.hitachi-hightech.com/jp/ja/knowledge/semiconductor/room/about/ic.html">チップの断面構造</a></figcaption>
</figure>
</div>
<p>一つのウエハから多数のチップが作成され，その各単位を <a href="https://www.hitachi-hightech.com/jp/ja/knowledge/semiconductor/room/words.html#die"><strong>ダイ (die)</strong></a> ともいう．</p>
<p>しかし，普段我々が目にする <a href="https://www.hitachi-hightech.com/jp/ja/knowledge/semiconductor/room/about/ic.html">IC チップ</a> は <a href="https://www.toppan.com/ja/electronics/package/semicon/"><strong>パッケージ</strong></a> されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．</p>
<p>32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．<sup>5</sup></p>
</section>
<section id="集積回路が出来るまで" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="集積回路が出来るまで"><span class="header-section-number">1.4</span> 集積回路が出来るまで</h3>
<p>まず回路を設計し，原版（マスター）を作る．これを <strong>フォトマスク (photomask)</strong> または <strong>レティクル (reticle)</strong> という．</p>
<p>これをウエハに転写するには，<a href="https://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A9%E3%83%88%E3%83%AA%E3%82%BD%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3">フォトリソグラフィ</a> (Photolithography) を用いる．シリコンウエハの形成は，<a href="https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A7%E3%82%AF%E3%83%A9%E3%83%AB%E3%82%B9%E3%82%AD%E3%83%BC%E6%B3%95">Czochralski 法</a> <span class="citation" data-cites="Czochralski1918">(Czochralski, 1918)</span> による．<sup>6</sup></p>
<p>リソグラフィ自体は 1798 年からあり，<a href="https://ja.wikipedia.org/wiki/%E3%83%8B%E3%82%BB%E3%83%95%E3%82%A9%E3%83%BC%E3%83%AB%E3%83%BB%E3%83%8B%E3%82%A8%E3%83%97%E3%82%B9">Niépce</a> が <a href="https://ja.wikipedia.org/wiki/%E6%AD%B4%E9%9D%92">歴青</a> が感光剤の役割を果たすことを発見し，カメラの発明と同時に発見された．</p>
<p>エッチングに耐性のある感光剤を使えば，半導体デバイスの製造に応用できると気づいたのは <span class="citation" data-cites="Andrus1957">(Andrus, 1957)</span> である．この技術は半導体製造コストの 35 %を占めており，半導体市場の急成長はほとんどこの技術の進歩と両輪であると言う者も多い．<sup>7</sup></p>
<p>シリコン表面に酸化被膜を形成することで不純物原子の移動を阻止できることは <span class="citation" data-cites="Frosch-Derick1957">(Frosch &amp; Derick, 1957)</span> が発見した．</p>
<p>以上の技術を用いて，最初の IC は <span class="citation" data-cites="Kilby1959">(Kilby, 1959)</span> が作った．<a href="https://en.wikipedia.org/wiki/Jack_Kilby">Jack Kilby</a> はその後 2000 年にノーベル物理学賞を受賞する．</p>
<p>しかし真に大量生産可能にし，半導体産業を大きくしたのは <span class="citation" data-cites="Noyce1959">(Noyce, 1959)</span> の発明であった．これは，現在主流の製法の基である <a href="https://ja.wikipedia.org/wiki/%E3%83%97%E3%83%AC%E3%83%BC%E3%83%8A%E3%83%BC_%E3%83%97%E3%83%AD%E3%82%BB%E3%82%B9">Planar Process</a> <span class="citation" data-cites="Hoerni1960">(Hoerni, 1960)</span> で作られた．</p>
<p>実際に，どのように半導体チップを製造するかについては次節 2 で詳しく解説する．</p>
</section>
</section>
<section id="sec-manufacturing" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-manufacturing"><span class="header-section-number">2</span> 半導体の製造</h2>
<p>半導体の製造段階は，典型的には次の３つに大別される．</p>
<ol type="1">
<li>設計
<ol type="1">
<li>回路・レイアウト作成 (design)</li>
<li>フォトマスク (photomask) 作成</li>
</ol></li>
<li>前工程：次の３工程を繰り返ことで何層もの膜を積層する
<ol type="1">
<li>成膜 (deposition)</li>
<li>パターン転写 (exposure)</li>
<li>食刻 (etching)</li>
</ol></li>
<li>後工程
<ol type="1">
<li>角切り (dicing)</li>
<li>封入 (packaging)</li>
<li>品質検査 (inspection)</li>
</ol></li>
</ol>
<p><a href="https://www.hitachi-hightech.com/jp/ja/knowledge/semiconductor/room/manufacturing/process.html">日立の解説</a> も参照．</p>
<section id="設計工程" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="設計工程"><span class="header-section-number">2.1</span> 設計工程</h3>
<p>18に分類されることもある．</p>
<section id="回路レイアウト作成" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="回路レイアウト作成"><span class="header-section-number">2.1.1</span> 回路・レイアウト作成</h4>
<p><a href="https://ja.wikipedia.org/wiki/EDA_(%E5%8D%8A%E5%B0%8E%E4%BD%93)">EDA (Eleotron Design Automation)</a> を用いて設計を行う．</p>
</section>
</section>
<section id="前工程" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="前工程"><span class="header-section-number">2.2</span> 前工程</h3>
<section id="基本３工程" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="基本３工程"><span class="header-section-number">2.2.1</span> 基本３工程</h4>
<p>成膜，パターン転写，食刻の３工程を繰り返すことでウエハ上に構造を作っていくのであるが，その目的は大きく５つに分類出来る．</p>
<ol type="1">
<li><p>素子分離領域形成</p>
<p>酸化被膜により，素子間の絶縁を形成する．</p></li>
<li><p>well 形成</p>
<p>トランジスタの基盤となる領域に，食刻の代わりにイオンの添加する．</p></li>
<li><p>トランジスタ形成</p>
<p>ウエハ基盤上にトランジスタ素子を形成する．</p></li>
<li><p>電極形成</p>
<p>シリコン基盤上のトランジスタに届くように，すでに形成された層に穴 (contact hole) をあけ，導体を埋め込む．</p></li>
<li><p>配線層形成</p>
<p>基本３工程を繰り返すことで，トランジスタ層上を分厚い配線層で覆う．<sup>8</sup></p></li>
</ol>
</section>
<section id="異物検査と洗浄" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="異物検査と洗浄"><span class="header-section-number">2.2.2</span> 異物検査と洗浄</h4>
<p>ほとんどの工程間に，異物検査と洗浄の工程が必要になる．</p>
<p><a href="https://www.hitachi-hightech.com/jp/ja/products/semiconductor-manufacturing/cd-sem/inspection-solution/ls.html">日立の製品例</a>，<a href="https://www.hitachi-hightech.com/jp/ja/products/semiconductor-manufacturing/cd-sem/inspection-solution/is.html">ウェーハ欠陥検査</a></p>
</section>
<section id="表面酸化" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="表面酸化"><span class="header-section-number">2.2.3</span> 表面酸化</h4>
<p>熱酸化法では，酸素や高温のスチームを当てることで，表面に SiO<sub>2</sub> の酸化被膜を形成し，絶縁体として用いる．</p>
</section>
<section id="成膜" class="level4" data-number="2.2.4">
<h4 data-number="2.2.4" class="anchored" data-anchor-id="成膜"><span class="header-section-number">2.2.4</span> 成膜</h4>
</section>
<section id="パターン転写" class="level4" data-number="2.2.5">
<h4 data-number="2.2.5" class="anchored" data-anchor-id="パターン転写"><span class="header-section-number">2.2.5</span> パターン転写</h4>
<p>フォトマスク上から紫外線を当てることで，フォトレジストを感光させる．次の工程で感光部分のみを食刻することで，パターン該当部分のみに酸化被膜を残すことが出来る．</p>
</section>
<section id="食刻" class="level4" data-number="2.2.6">
<h4 data-number="2.2.6" class="anchored" data-anchor-id="食刻"><span class="header-section-number">2.2.6</span> 食刻</h4>
<p>現像後は，寸法計測を行う (ADI: After Development Inspection)．これは走査性電子顕微鏡 (SEM: Scanning Electron Microscope) である <a href="https://www.hitachi-hightech.com/jp/ja/knowledge/semiconductor/room/manufacturing/cd-sem.html">CD-SEM</a> などを用いて行う（<a href="https://www.hitachi-hightech.com/jp/ja/products/semiconductor-manufacturing/cd-sem/metrology-solution/">日立の製品例</a>）</p>
<p>これにより，正しくパターンが転写されていることが確認されたのち，食刻を行い，再び寸法計測を行う（AEI: After Etch Inspection などと呼び分ける）．</p>
<p>最後に，残ったフォトレジストはオゾンやプラズマにより灰化 (ashing) により除去する．</p>
<p>コンダクターエンチング（<a href="https://www.hitachi-hightech.com/jp/ja/products/semiconductor-manufacturing/dry-etch-systems/conductor/">日立の例</a>）</p>
</section>
<section id="イオン添加" class="level4" data-number="2.2.7">
<h4 data-number="2.2.7" class="anchored" data-anchor-id="イオン添加"><span class="header-section-number">2.2.7</span> イオン添加</h4>
<p>イオンを注入するとアモルファスとなるため，一度再加熱をして (annealing) 再結晶化する．</p>
</section>
</section>
<section id="後工程" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="後工程"><span class="header-section-number">2.3</span> 後工程</h3>
</section>
</section>
<section id="半導体デバイス" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="半導体デバイス"><span class="header-section-number">3</span> 半導体デバイス</h2>
<section id="構成要素" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="構成要素"><span class="header-section-number">3.1</span> 構成要素</h3>
<p>整流作用を示す接合には，ショットキー接合と pn 接合がある．</p>
</section>
<section id="双極から-mos-へ" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="双極から-mos-へ"><span class="header-section-number">3.2</span> 双極から MOS へ</h3>
<p>MOS (Metal-Oxide-Semiconductor) の３層構造を用いたトランジスタが採用される前は，双極トランジスタ (bipolar transistor) <span class="citation" data-cites="Shockley1949">(Shockley, 1949)</span> を用いた <a href="https://ja.wikipedia.org/wiki/Transistor-transistor_logic">TTL (Transistor-Transistor Logic)</a> 回路が主流であった．</p>
<p>しかしこれは，トランジスタの間の絶縁が難しく，密度を上げることが難しかった．そのような中で MOSFET (Metal-Oxide-Silicon Field-Effect Transistor) が開発された <span class="citation" data-cites="Kahng-Atalla1960">(Kahng &amp; Atalla, 1960)</span>．MOSFET 技術は現代の半導体市場の 95% に関連する．<sup>9</sup></p>
<p>MOSFET は自己絶縁構造を持つため，これ以上の絶縁処理を必要とせず，双極トランジスタの 10 %の面積で済んだ．</p>
</section>
<section id="cmos-complementary-metal-oxide-semiconductor" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="cmos-complementary-metal-oxide-semiconductor"><span class="header-section-number">3.3</span> CMOS (Complementary Metal-Oxide-Semiconductor)</h3>
<p>MOS が初め採用した設計論理のは <a href="https://ja.wikipedia.org/wiki/NMOS%E3%83%AD%E3%82%B8%E3%83%83%E3%82%AF">NMOS</a> や <a href="https://ja.wikipedia.org/wiki/PMOS%E3%83%AD%E3%82%B8%E3%83%83%E3%82%AF">PMOS</a> であったが，常に直流電流を消費する必要があった．</p>
<p>しかし現代では，P 型と N 型の MOSFET を相補的に用いる CMOS が集積回路における支配的な技術である．</p>
<p>その秘訣は消費電力の少なさにあり，トランジスタの <img src="https://latex.codecogs.com/png.latex?0,1"> の切り替えの際に生じる電力（動的エネルギー）のみが消費される．<sup>10</sup></p>
<blockquote class="blockquote">
<p>CMOS技術は、当初アメリカの半導体業界では、当時より高性能だったNMOSを優先して見過ごされていた。しかし、CMOSは低消費電力であることから日本の半導体メーカーにいち早く採用され、さらに進化し、日本の半導体産業の隆盛につながった。<a href="https://ja.wikipedia.org/wiki/CMOS">CMOS</a></p>
</blockquote>
</section>
<section id="現代に汎在する半導体" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="現代に汎在する半導体"><span class="header-section-number">3.4</span> 現代に汎在する半導体</h3>
<p>計算機を構成する要素は多いが，Moore の法則により，多くが同一のチップの上に載ってしまい，不可視化が進んでいる．<sup>11</sup></p>
<section id="プロセッサ" class="level4" data-number="3.4.1">
<h4 data-number="3.4.1" class="anchored" data-anchor-id="プロセッサ"><span class="header-section-number">3.4.1</span> プロセッサ</h4>
<p>コンピュータの CPU と言ったときに，１枚のチップを意味するようになったのは 1971 年の <a href="https://ja.wikipedia.org/wiki/Intel_4004">Intel 4004</a> が初めてである．<sup>12</sup></p>
<p>3mm × 4mm のチップ上に 2300 の MOSFET を備え，大きな机ほどの CPU を備えた IBM コンピュータに匹敵する処理能力を持っていた．<sup>13</sup></p>
</section>
<section id="半導体メモリ" class="level4" data-number="3.4.2">
<h4 data-number="3.4.2" class="anchored" data-anchor-id="半導体メモリ"><span class="header-section-number">3.4.2</span> 半導体メモリ</h4>
<p>現代でメモリといえば <a href="https://ja.wikipedia.org/wiki/Random_Access_Memory">RAM (Random Access Memory)</a> を指す．本来はアクセスする順番に制約があった SAM (Sequential Access Memory) に対して作られた言葉であったが，現代では ROM (Read Only Memory) との対義語として理解されることが多いようである．</p>
<p><a href="https://ja.wikipedia.org/wiki/Static_Random_Access_Memory">SRAM (Static RAM)</a> は半導体メモリの一種であり，DRAM (Dynamic RAM) と比べて高速である．１ビットあたり６から８のトランジスタを使用したフリップフロップ回路により情報を記憶するため，定期的なリフレッシュが不要で高速な読み書きが可能であるが，集積率を上げることが出来ず，大容量メモリには向かない．<sup>14</sup></p>
<p><a href="https://ja.wikipedia.org/wiki/Dynamic_Random_Access_Memory">DRAM (Dynamic RAM)</a> <span class="citation" data-cites="Dennard1967">(Dennard, 1967)</span> はチップ内にコンデンサを備えており，１つのコンデンサで１ビットを表現する．これを読み出すのに１つのトランジスタ <a href="https://ja.wikipedia.org/wiki/MOSFET">MOSFET</a> を使うのみであるから，SRAM に比べて安価であるが，電荷は時間と共に散逸するため，定期的にリフレッシュする必要があり，消費電力は大きい．<sup>15</sup></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/AI/Images/DRAM.png" class="img-fluid figure-img"></p>
<figcaption>From <span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 380)</span></figcaption>
</figure>
</div>
<p>DRAM と SRAM はいずれも揮発性である．不揮発性の半導体メモリには <a href="https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%A9%E3%83%83%E3%82%B7%E3%83%A5%E3%83%A1%E3%83%A2%E3%83%AA">フィラッシュメモリ</a> がある．これには <a href="https://ja.wikipedia.org/wiki/%E6%B5%AE%E9%81%8A%E3%82%B2%E3%83%BC%E3%83%88MOSFET">蜉蝣ゲートMOSFET</a> という素子で捉えた電子により情報を記憶することで不揮発性を実現している．</p>
<p>フラッシュメモリには NAND 型と NOR 型があり，前者が主流である．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/AI/Images/Memory_Revenue.png" class="img-fluid figure-img"></p>
<figcaption>Memory Revenue May, 2022. Source: <a href="https://omdia.tech.informa.com/pr/2023/mar/omdia-2022-a-record-year-for-semiconductors-that-feels-anything-but">Omedia</a></figcaption>
</figure>
</div>
</section>
<section id="graphics-processing-unit" class="level4" data-number="3.4.3">
<h4 data-number="3.4.3" class="anchored" data-anchor-id="graphics-processing-unit"><span class="header-section-number">3.4.3</span> Graphics Processing Unit</h4>
<p>描画を扱うチップは従来 VGA コントローラーと呼ばれていたが，1999 年には１つのチップで描画タスクの殆どをこなせるようになり，特に <a href="https://ja.wikipedia.org/wiki/NVIDIA_GeForce">NVIDIA GeForce</a> 256 は GPU という名称で売り出された．</p>
<p>こうして GPU は元来の 3D グラフィクスに特化した存在から，徐々に CPU を補完する多様なタスクに柔軟に対応できるように，プログラム可能で，大量のコアを持って並列計算可能なものに進化していった．近年の CPU はマルチコアのものが多いが，現在の GPU は 1000 コアを超えるものも多い．</p>
</section>
<section id="language-processing-unit" class="level4" data-number="3.4.4">
<h4 data-number="3.4.4" class="anchored" data-anchor-id="language-processing-unit"><span class="header-section-number">3.4.4</span> Language Processing Unit</h4>
<p><a href="https://wow.groq.com/lpu-inference-engine/">LPU</a> は，Google で <a href="https://ja.wikipedia.org/wiki/%E3%83%86%E3%83%B3%E3%82%BD%E3%83%AB%E3%83%BB%E3%83%97%E3%83%AD%E3%82%BB%E3%83%83%E3%82%B7%E3%83%B3%E3%82%B0%E3%83%BB%E3%83%A6%E3%83%8B%E3%83%83%E3%83%88">TSU (Tensor Processing Unit)</a> のプロジェクトに初期から従事していたエンジニア <a href="https://wow.groq.com/jonathan-ross-every-word-matters/">Jonathan Ross</a> が 2016 年に創業したスタートアップ Groq の <a href="https://wow.groq.com/lpu-inference-engine/">登録商標</a> である．</p>
<p>Groq が <a href="https://www.prnewswire.com/news-releases/groq-selects-samsung-foundry-to-bring-next-gen-lpu-to-the-ai-acceleration-market-301900464.html">Samsung と協力して</a> 実現した LPU <span class="citation" data-cites="Abts+2022">(Abts et al., 2022)</span> は <a href="https://ja.wikipedia.org/wiki/EDRAM">eDRAM</a> を持つ <a href="https://ja.wikipedia.org/wiki/ASIC">ASIC (Application Specific Integrated Circuit)</a> であり，メモリのバンド幅と計算密度を増やし，逐次処理に特化することで特に言語処理に特化している．</p>
<p>2/20/2024 に <a href="https://groq.com/">デモ</a> を公開した．</p>
<p>LPU は推論に，GPU は学習に特化しており，相補的な役割を演じながら AI の進化を支えていく可能性がある．</p>
</section>
</section>
</section>
<section id="物理" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="物理"><span class="header-section-number">4</span> 物理</h2>
<section id="単体の半導体" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="単体の半導体"><span class="header-section-number">4.1</span> 単体の半導体</h3>
</section>
</section>
<section id="文献レビュー" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="文献レビュー"><span class="header-section-number">5</span> 文献レビュー</h2>
<p><span class="citation" data-cites="SemiconductorHandbook2023">(Rudan et al., 2023)</span> は辞典として使える．<span class="citation" data-cites="VanRossum2005">(Van Rossum, 2005)</span> は凝縮系物理学のハンドブック内でのエントリ．</p>
<section id="産業" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="産業"><span class="header-section-number">5.1</span> 産業</h3>
<p><span class="citation" data-cites="Miller2022">(Miller, 2022)</span> は Tufts 大学の <a href="https://www.christophermiller.net/">Chris Miller</a> による書籍．</p>
<p>和書としては，<span class="citation" data-cites="菊池正典2023">(菊池正典, 2023)</span> は <a href="https://ja.wikipedia.org/wiki/%E6%97%A5%E6%9C%AC%E9%9B%BB%E6%B0%97">NEC</a> 社員による半導体業界の解説書．</p>
</section>
<section id="教科書" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="教科書"><span class="header-section-number">5.2</span> 教科書</h3>
<p><span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012)</span> は浮遊ゲート MOSFET の発明者でもある <a href="https://ja.wikipedia.org/wiki/%E3%82%B5%E3%82%A4%E3%83%A2%E3%83%B3%E3%83%BB%E3%82%B8%E3%82%A3%E3%83%BC">Simon Min Sze（施敏）</a>の著作で，半導体分野で最も多く引用される教科書とされている．第２版なら <a href="https://www.kinokuniya.co.jp/f/dsg-01-9784782855508">和訳</a> もある．</p>
<p><span class="citation" data-cites="May-Spanos2006">(May &amp; Spanos, 2006)</span> は California 大学 Davis 校の現学長 <a href="https://en.wikipedia.org/wiki/Gary_S._May">Gary May</a> と California 大学 Berkeley 校の <a href="https://en.wikipedia.org/wiki/Costas_Spanos">Costas Spanos</a> による書籍．</p>
<p><span class="citation" data-cites="May-Sze2003">(May &amp; Sze, 2003)</span> もある．さらに発展的なものは <span class="citation" data-cites="Pierret2003">(Pierret, 2003)</span>．</p>
</section>
<section id="理論" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="理論"><span class="header-section-number">5.3</span> 理論</h3>
<p><span class="citation" data-cites="Kittel2018">(Kittel, 2018)</span> が固体物理学の標準的な入門書とされている．<span class="citation" data-cites="Boer-Pohl2018">(Böer &amp; Pohl, 2018)</span> が特に半導体物理学の専門書になる．<span class="citation" data-cites="Huebener2019">(Huebener, 2019)</span> は前２つの橋渡しの役割をするが，重点は超伝導にある．</p>
</section>
<section id="その他" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="その他"><span class="header-section-number">5.4</span> その他</h3>
<p><span class="citation" data-cites="Richard2023">(Richard, 2023)</span> は SignalFire という VC に所属する著者による書．</p>
<p><span class="citation" data-cites="Lau2021">(Lau, 2021)</span> は <a href="https://ja.wikipedia.org/wiki/ASM%E3%82%A4%E3%83%B3%E3%82%BF%E3%83%BC%E3%83%8A%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB">AMS Pacific Technology</a> の <a href="https://www.semiconchina.org/en/870">技術アドバイザー</a> による書籍．</p>
<p><span class="citation" data-cites="Evstigneev2022">(Evstigneev, 2022)</span> はカナダ <a href="https://ja.wikipedia.org/wiki/%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%95%E3%82%A1%E3%83%B3%E3%83%89%E3%83%A9%E3%83%B3%E3%83%89%E3%83%A1%E3%83%A2%E3%83%AA%E3%82%A2%E3%83%AB%E5%A4%A7%E5%AD%A6">Memorial 大学</a> の <a href="https://www.physics.mun.ca/~mevstigneev/">凝縮系物理学者</a> が書いた．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Abts+2022" class="csl-entry">
Abts, D., Kimmell, G., Ling, A., Kim, J., Boyd, M., Bitar, A., Parmar, S., Ahmed, I., DiCecco, R., Han, D., Thompson, J., Bye, M., Hwang, J., Fowers, J., Lillian, P., Murthy, A., Mehtabuddin, E., Tekur, C., Sohmers, T., … Ross, J. (2022). A software-defined tensor streaming multiprocessor for large-scale machine learning. <em>Proceedings of the 49th Annual International Symposium on Computer Architecture</em>, 567–580. <a href="https://doi.org/10.1145/3470496.3527405">https://doi.org/10.1145/3470496.3527405</a>
</div>
<div id="ref-Andrus1957" class="csl-entry">
Andrus, J. (1957). <em>Fabrication of semiconductor devices</em>. U.S. Patent 3,122,817A. <a href="https://patents.google.com/patent/US3122817A/en">https://patents.google.com/patent/US3122817A/en</a>
</div>
<div id="ref-Boer-Pohl2018" class="csl-entry">
Böer, K. W., &amp; Pohl, U. W. (2018). <em>Semiconductor physics</em>. Springer Cham. <a href="https://link.springer.com/referencework/10.1007/978-3-319-69150-3">https://link.springer.com/referencework/10.1007/978-3-319-69150-3</a>
</div>
<div id="ref-Braun1874" class="csl-entry">
Braun, F. (1874). <a href="">Über die stromleitung durch schwefelmetalic</a>. <em>Annalen Der Physik and Chemie</em>, <em>153</em>(4), 556–563.
</div>
<div id="ref-Bridgman1925" class="csl-entry">
Bridgman, P. W. (1925). Certain physical properties of single crystals of tungsten, antimony, bismuth, tellurium, cadmium, zinc, and tin. <em>Proceedings of the American Academy of Arts and Sciences</em>, <em>60</em>(6), 305–383. <a href="https://www.jstor.org/stable/25130058">https://www.jstor.org/stable/25130058</a>
</div>
<div id="ref-Czochralski1918" class="csl-entry">
Czochralski, J. (1918). Ein neues verfahren zur messung der kristallisationsgeschwindigkeit der metalle. <em>Zeitschrift Für Physikalische Chemie</em>, <em>92U</em>(1), 219–221. <a href="https://doi.org/10.1515/zpch-1918-9212">https://doi.org/10.1515/zpch-1918-9212</a>
</div>
<div id="ref-Dennard1967" class="csl-entry">
Dennard, R. H. (1967). <em>Field effect transistor memory</em>. U.S. Patent 3,387,286A. <a href="https://patents.google.com/patent/US3387286A/en">https://patents.google.com/patent/US3387286A/en</a>
</div>
<div id="ref-Evstigneev2022" class="csl-entry">
Evstigneev, M. (2022). <em>Introduction to semiconductor physics and devices</em>. Springer Cham. <a href="https://link.springer.com/book/10.1007/978-3-031-08458-4">https://link.springer.com/book/10.1007/978-3-031-08458-4</a>
</div>
<div id="ref-Faraday1833" class="csl-entry">
Faraday, M. (1833). Experimental researches in electricity. Third series. <em>Philosophical Transactions of the Royal Society of London</em>, <em>123</em>, 23–54. <a href="https://www.jstor.org/stable/107985">https://www.jstor.org/stable/107985</a>
</div>
<div id="ref-Frosch-Derick1957" class="csl-entry">
Frosch, C. J., &amp; Derick, L. (1957). Surface protection and selective masking during diffusion in silicon. <em>Journal of the Electrochemical Society</em>, <em>104</em>(9), 547. <a href="https://iopscience.iop.org/article/10.1149/1.2428650">https://iopscience.iop.org/article/10.1149/1.2428650</a>
</div>
<div id="ref-Heisenberg1931" class="csl-entry">
Heisenberg, W. (1931). Zum paulischen ausschließungsprinzip. <em>Annalen Der Physik</em>, <em>402</em>(7), 888–904. <a href="https://onlinelibrary.wiley.com/doi/10.1002/andp.19314020710">https://onlinelibrary.wiley.com/doi/10.1002/andp.19314020710</a>
</div>
<div id="ref-Hoerni1960" class="csl-entry">
Hoerni, J. A. (1960). Planar silicon diodes and transistors. <em>1960 International Electron Devices Meeting</em>. <a href="https://ieeexplore.ieee.org/document/1472833">https://ieeexplore.ieee.org/document/1472833</a>
</div>
<div id="ref-Hoff+1996" class="csl-entry">
Hoff, M. E., Faggin, F., Mazor, S., &amp; Shima, M. (1996). The history of the 4004. <em>IEEE Micro</em>, <em>16</em>(6), 10–20. <a href="https://ieeexplore.ieee.org/document/546561">https://ieeexplore.ieee.org/document/546561</a>
</div>
<div id="ref-Huebener2019" class="csl-entry">
Huebener, R. P. (2019). <em>Conductors, semiconductors, superconductors: An introduction to solid-state physics</em> (3rd ed.). Springer Cham. <a href="https://link-springer-com.utokyo.idm.oclc.org/book/10.1007/978-3-030-31420-0">https://link-springer-com.utokyo.idm.oclc.org/book/10.1007/978-3-030-31420-0</a>
</div>
<div id="ref-Kahng-Atalla1960" class="csl-entry">
Kahng, D., &amp; Atalla, M. M. (1960). Silicon-silicon dioxide surface device. <em>IRE Device Reserach Conference, Pittsburgh</em>. <a href="https://www.worldscientific.com/doi/10.1142/9789814503464_0076">https://www.worldscientific.com/doi/10.1142/9789814503464_0076</a>
</div>
<div id="ref-Kilby1959" class="csl-entry">
Kilby, J. S. (1959). <em>Miniaturized electronic circuits</em>. U.S. Patent 3,138,743A. <a href="https://patents.google.com/patent/US3138743A/en">https://patents.google.com/patent/US3138743A/en</a>
</div>
<div id="ref-Kittel2018" class="csl-entry">
Kittel, C. (2018). <em>Introduction to solid state physics</em> (8th ed.). John Wiley. <a href="https://www.wiley.com/en-ie/Kittel%27s+Introduction+to+Solid+State+Physics%2C+Global+Edition%2C+8th+Edition-p-9781119454168">https://www.wiley.com/en-ie/Kittel%27s+Introduction+to+Solid+State+Physics%2C+Global+Edition%2C+8th+Edition-p-9781119454168</a>
</div>
<div id="ref-Lau2021" class="csl-entry">
Lau, J. H. (2021). <em>Semiconductor advanced packaging</em>. Springer Singapore. <a href="https://link.springer.com/book/10.1007/978-981-16-1376-0">https://link.springer.com/book/10.1007/978-981-16-1376-0</a>
</div>
<div id="ref-Madelung1978" class="csl-entry">
Madelung, O. (1978). <em>Introduction to solid-state theory</em> (Vol. 2). Springer Berlin, Heidelberg. <a href="https://link.springer.com/book/10.1007/978-3-642-61885-7">https://link.springer.com/book/10.1007/978-3-642-61885-7</a>
</div>
<div id="ref-May-Spanos2006" class="csl-entry">
May, G. S., &amp; Spanos, C. J. (2006). <em>Fundamentals of semiconductor manufacturing and process control</em>. John Wiley &amp; Sons. <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/0471790281">https://onlinelibrary.wiley.com/doi/book/10.1002/0471790281</a>
</div>
<div id="ref-May-Sze2003" class="csl-entry">
May, G. S., &amp; Sze, S. M. (2003). <em>Fundamentals of semiconductor fabrication</em>. John Wiley &amp; Sons. <a href="https://www.wiley.com/en-us/Fundamentals+of+Semiconductor+Fabrication-p-9780471232797">https://www.wiley.com/en-us/Fundamentals+of+Semiconductor+Fabrication-p-9780471232797</a>
</div>
<div id="ref-Miller2022" class="csl-entry">
Miller, C. (2022). <em>Chip war: The fight for the world’s most critical technology</em>. Scribner. <a href="https://www.simonandschuster.com/books/Chip-War/Chris-Miller/9781982172008">https://www.simonandschuster.com/books/Chip-War/Chris-Miller/9781982172008</a>
</div>
<div id="ref-Noyce1959" class="csl-entry">
Noyce, R. N. (1959). <em>Semiconductor device-and-lead structure</em>. U.S. Patent 2,981,877A. <a href="https://patents.google.com/patent/US2981877A/en">https://patents.google.com/patent/US2981877A/en</a>
</div>
<div id="ref-Patterson-Hennessy2014" class="csl-entry">
Patterson, D. A., &amp; Hennessy, J. L. (2014). <em><a href="">Computer organization and design: The hardware/software interface</a></em> (5th ed.). Elsevier.
</div>
<div id="ref-Pierret2003" class="csl-entry">
Pierret, R. F. (2003). <em>Advanced semiconductor fundamentals: Vol. VI</em> (2nd ed.). Pearson. <a href="https://www.pearson.com/en-us/subject-catalog/p/advanced-semiconductor-fundamentals/P200000003285/9780130617927">https://www.pearson.com/en-us/subject-catalog/p/advanced-semiconductor-fundamentals/P200000003285/9780130617927</a>
</div>
<div id="ref-Richard2023" class="csl-entry">
Richard, C. (2023). <em>Understanding semiconductors: A technical guide for non-technical people</em>. Apress Berkeley. <a href="https://link.springer.com/book/10.1007/978-1-4842-8847-4">https://link.springer.com/book/10.1007/978-1-4842-8847-4</a>
</div>
<div id="ref-Round1907" class="csl-entry">
Round, H. J. (1907). A note on carborundum. <em>Electrical World</em>, <em>19</em>, 309–310. <a href="https://www.worldscientific.com/doi/abs/10.1142/9789814503464_0116">https://www.worldscientific.com/doi/abs/10.1142/9789814503464_0116</a>
</div>
<div id="ref-SemiconductorHandbook2023" class="csl-entry">
Rudan, M., Brunetti, R., &amp; Reggiani, S. (Eds.). (2023). <em>Springer handbook of semiconductor devices</em>. Springer Cham. <a href="https://link.springer.com/book/10.1007/978-3-030-79827-7">https://link.springer.com/book/10.1007/978-3-030-79827-7</a>
</div>
<div id="ref-Shockley1949" class="csl-entry">
Shockley, W. (1949). The theory of <img src="https://latex.codecogs.com/png.latex?p">-<img src="https://latex.codecogs.com/png.latex?n"> junction in semiconductors and <img src="https://latex.codecogs.com/png.latex?p">-<img src="https://latex.codecogs.com/png.latex?n"> hunction transistors. <em>The Bell System Technical Journal</em>, <em>28</em>(3), 435–489. <a href="https://ieeexplore.ieee.org/document/6773080">https://ieeexplore.ieee.org/document/6773080</a>
</div>
<div id="ref-Sze-Lee2012" class="csl-entry">
Sze, S. M., &amp; Lee, M.-K. (2012). <em>Semiconductor devices: Physics and technology</em> (3rd ed.). John Wiley &amp; Sons. <a href="https://www.wiley.com/en-us/Semiconductor+Devices:+Physics+and+Technology,+3rd+Edition-p-9780470537947">https://www.wiley.com/en-us/Semiconductor+Devices:+Physics+and+Technology,+3rd+Edition-p-9780470537947</a>
</div>
<div id="ref-VanRossum2005" class="csl-entry">
Van Rossum, M. (2005). Integrated circuits. In T. Chakraborty (Ed.), <em>Encyclopedia of condensed matter physics (second edition)</em> (Second Edition, pp. 748–756). Academic Press. <a href="https://doi.org/10.1016/B978-0-323-90800-9.00292-4">https://doi.org/10.1016/B978-0-323-90800-9.00292-4</a>
</div>
<div id="ref-Welker1952" class="csl-entry">
Welker, H. (1952). Über neue halbleitende verbindungen. <em>Zeitschrift Für Naturforschung A</em>, <em>7</em>(11), 744–749. <a href="https://doi.org/10.1515/zna-1952-1110">https://doi.org/10.1515/zna-1952-1110</a>
</div>
<div id="ref-菊池正典2023" class="csl-entry">
菊池正典. (2023). <em>半導体産業のすべて：世界の先端企業から日本メーカーの展望まで</em>. ダイヤモンド社. <a href="https://www.diamond.co.jp/book/9784478117118.html">https://www.diamond.co.jp/book/9784478117118.html</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="Boer-Pohl2018">(Böer &amp; Pohl, 2018, p. 4)</span>, <span class="citation" data-cites="Huebener2019">(Huebener, 2019, p. 73)</span> Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う <span class="citation" data-cites="Madelung1978">(Madelung, 1978, p. 17)</span>．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である <span class="citation" data-cites="Madelung1978">(Madelung, 1978, p. 211)</span>．↩︎</p></li>
<li id="fn2"><p><span class="citation" data-cites="Huebener2019">(Huebener, 2019, p. 75)</span>．↩︎</p></li>
<li id="fn3"><p><span class="citation" data-cites="Huebener2019">(Huebener, 2019, p. 73)</span> 特に伝導体と半導体の境界部分で強く見られた．↩︎</p></li>
<li id="fn4"><p>現代では，このような接合を金属-半導体接合 (metal-semiconductor contact) または <a href="https://ja.wikipedia.org/wiki/%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%E3%82%AD%E3%83%BC%E6%8E%A5%E5%90%88">Schottky 接合</a> といい，Ohmic 接合と対比する．↩︎</p></li>
<li id="fn5"><p><span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 27)</span> も参照．↩︎</p></li>
<li id="fn6"><p>一方で GaAs の形成は Bridgman 法 <span class="citation" data-cites="Bridgman1925">(Bridgman, 1925)</span> による．最も，この化合物が半導体であると発見されたのは <span class="citation" data-cites="Welker1952">(Welker, 1952)</span> になってようやくのことである．<span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012, p. 6)</span> も参照．↩︎</p></li>
<li id="fn7"><p><span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012, p. 6)</span> など．↩︎</p></li>
<li id="fn8"><p>今日の IC ではトランジスタ層は１層のみで，絶縁層で仕切ることで２〜８層の金属導体の配線層をその上に設ける． <span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 26)</span>．↩︎</p></li>
<li id="fn9"><p><span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012, p. 4)</span> など．↩︎</p></li>
<li id="fn10"><p><span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 41)</span>．↩︎</p></li>
<li id="fn11"><p><span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 379)</span>．↩︎</p></li>
<li id="fn12"><p><span class="citation" data-cites="Hoff+1996">(Hoff et al., 1996)</span> が開発者自ら歴史を振り返っている．当時は Intel も出来て３年しか経っていない新興企業であった．↩︎</p></li>
<li id="fn13"><p><span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012, p. 8)</span> など．↩︎</p></li>
<li id="fn14"><p><span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 379)</span>．↩︎</p></li>
<li id="fn15"><p><span class="citation" data-cites="Sze-Lee2012">(Sze &amp; Lee, 2012, p. 8)</span>，<span class="citation" data-cites="Patterson-Hennessy2014">(Patterson &amp; Hennessy, 2014, p. 379)</span> 参照．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Nature</category>
  <category>Survey</category>
  <guid>https://162348.github.io/posts/2024/AI/Semiconductor.html</guid>
  <pubDate>Sun, 25 Feb 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/AI/Images/chipwar.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A Recent Development of Particle Methods</title>
  <dc:creator>Hirofumi Shiba</dc:creator>
  <link>https://162348.github.io/posts/2024/Particles/PF.html</link>
  <description><![CDATA[ 





<div class="hidden">
$$
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../static/Posters/MLSS2024.pdf"><img src="https://162348.github.io/static/Posters/MLSS2024.jpg" class="img-fluid figure-img" width="200" alt="A Recent Development of Particle Filter. Tap the image to view PDF"></a></p>
<figcaption>A Recent Development of Particle Filter. Tap the image to view PDF</figcaption>
</figure>
</div>
<p>The following is a detailed version of the poster presented at <a href="../../../static/Sessions.html#sec-MLSS2024">MLSS2024</a>, S3-41, March 8 (Fri) 18:00-19:30.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<p>Compared to their discrete-time counterparts, <strong>Monte Carlo methods based on continuous-time processes</strong> exhibit superior <strong>computational efficiency</strong> and <strong>mixing rates</strong>, making them more suitable for high-dimensional applications.</p>
</div>
</div>
<section id="what-is-particle-filter" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="what-is-particle-filter"><span class="header-section-number">1</span> What Is Particle Filter?</h2>
<p><em>Particle filters</em>, also known as <em>Sequential Monte Carlo</em> methods (SMCs), were invented in <span class="citation" data-cites="Kitagawa1993">(Kitagawa, 1993)</span> and <span class="citation" data-cites="Gordon+1993">(Gordon et al., 1993)</span> independently as an simulation-based algorithm which performs filtering in non-Gaussian and non-linear state space models, overcoming the weeknesses of then-standard Kalman-based filtering methods.<sup>1</sup></p>
<p>In particle-based approaches, a filtering distribution is approximated by a cloud of weighted samples, hence giving rise to the term ‘particle filter’. The samples are propagated to approximate the next distribution, leading to efficient sequential estimation in dynamic settings.</p>
<p>Recent developments have highlithgted the capability of particle filters as general-purpose samplers, extending their applicability beyond the traditional realm of temporal graphical models to a broader range of statistical inference problems. This versatility has earned them the alternative name ‘SMC’, a term reminiscent of ‘MCMC’. This poster trys to be another contribution in this direction.</p>
</section>
<section id="mcmc-vs.-smc" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="mcmc-vs.-smc"><span class="header-section-number">2</span> MCMC vs.&nbsp;SMC</h2>
<p>PDMPs (Piecewise Deterministic Markov Processes) <span class="citation" data-cites="Davis1984">(Davis, 1984)</span>, a type of continuous-time Markov processes with jumps as their only random components, play a complementary role to diffusion processes in stochastic modelling.<sup>2</sup></p>
<p>In <span class="citation" data-cites="Peters-deWith2012">(Peters &amp; de&nbsp;With, 2012)</span>, a PDMP was identified through the continuous limit of the MCMC, Metropolis-Hastings algorithm. The PDMP was further investigated and termed <em>Bouncy Particle Sampler</em> (BPS) in <span class="citation" data-cites="Bouchard-Cote+2018-BPS">(Bouchard-Côté et al., 2018)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(RZigZag)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ggplot2)</span>
<span id="cb1-3">V <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nrow=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-4">mu <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-5">x0 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-6">result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">BPSGaussian</span>(V, mu, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n_iter =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x0 =</span> x0)</span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-8">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_path</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,], <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,]), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-9">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,], <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,]), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-10">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">title=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bouncy Particle Sampler"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-11">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_void</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb1-12">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">text=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">size=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">axis.title=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plot.title=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>))</span></code></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Particles/PF_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Also, other types of continuous-time MCMC algorithms have been developed, such as the <em>Zig-Zag</em> sampler <span class="citation" data-cites="Bierkens+2019">(Bierkens et al., 2019)</span>:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">V <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">matrix</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">nrow=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-2">mu <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-3">result <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ZigZagGaussian</span>(V, mu, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb2-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-5">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_path</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,], <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,]), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-6">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,], <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y=</span>result<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>Positions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,]), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-7">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">labs</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">title=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Zig-Zag Sampler"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-8">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_void</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-9">   <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">text=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">size=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">axis.title=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">plot.title=</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">element_text</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#2F579C"</span>))</span></code></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Particles/PF_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Enpirical evidence suggests that continuous-time MCMCs are more efficient than their discrete-time counterparts.</p>
<blockquote class="blockquote">
<p>Interestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as <strong>they need only access a small sub-set of data points at each iteration</strong>, and yet are still guaranteed to target the true posterior distribution. <span class="citation" data-cites="Fearnhead+2018-PDMC">(Fearnhead et al., 2018)</span></p>
</blockquote>
</section>
<section id="inquiry-for-continuous-time-smc" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="inquiry-for-continuous-time-smc"><span class="header-section-number">3</span> Inquiry for Continuous-time SMC</h2>
<p>Despite the success of continuous-time MCMC, the continuous-time limit of SMC has not been fully explored. The continuous-time limit of SMC is expected to be a jump process, which is similar to PDMP, but is more diffusion-like.</p>
<p>MCMC has now taken a step ahead; it is time for SMC to explore its continuous-time limit!</p>
<section id="a-generic-particle-filter-an-algorithmic-description" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="a-generic-particle-filter-an-algorithmic-description"><span class="header-section-number">3.1</span> A Generic Particle Filter: An Algorithmic Description</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Particles/PF.svg" class="img-fluid figure-img"></p>
<figcaption>Procedure of a generic step of a particle filter at time <img src="https://latex.codecogs.com/png.latex?t"></figcaption>
</figure>
</div>
<ol type="1">
<li><p><strong>Resampling Step</strong></p>
<p>Particles with high weights are duplicated, and those with the lowest weights are discarded.</p></li>
<li><p><strong>Movement Step</strong></p>
<p>Subsequently, a MCMC move is executed from the resampled particles.</p></li>
</ol>
<p>The <em>resampling step</em> is the key difference from sequential importance sampling methods. Particle filters incorporate a resampling step to occasionally reset the weights of the samples, while maintaining the overall distribution they represent, in order to prevent the effective number of particles participating in the estimation from becoming too small–a situation also called <em>weight degeneracy</em>.</p>
</section>
<section id="a-necessary-condition-resampling-stability" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="a-necessary-condition-resampling-stability"><span class="header-section-number">3.2</span> A Necessary Condition: Resampling Stability</h3>
<p>In order to have a time-step <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cto0"> limit, resampling events must occur with (at most linearly) decreasing frequency as <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cto0">.</p>
<p>Only the most efficient resampling schemes satisfy this property.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Particles/box2_filtering_512.svg" class="img-fluid figure-img"></p>
<figcaption>Root mean squared errors of marginal likelihood estimates <span class="citation" data-cites="Chopin+2022">(Chopin et al., 2022)</span></figcaption>
</figure>
</div>
</section>
</section>
<section id="the-continuous-time-limit-process" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="the-continuous-time-limit-process"><span class="header-section-number">4</span> The Continuous-time Limit Process</h2>
<p>The continuous-time limit process, if it exists, is characterized by a <strong>Feller-Dynkin process</strong>, whose infinitesimal generator is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cmathcal%7BL%7Df(x)&amp;=%5Csum_%7Bn=1%7D%5EN%5Csum_%7Bi=1%7D%5Edb_i(x%5En)%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%5En_i%7D(x)%5C%5C%0A%20%20%20%20&amp;%5C;%5C;+%5Csum_%7Bn=1%7D%5EN%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi,j=1%7D%5Ed(%5Csigma%5Csigma%5E%5Ctop)_%7Bij%7D(x%5En)%5Cfrac%7B%5Cpartial%20%5E2f%7D%7B%5Cpartial%20x%5En_i%5Cpartial%20x%5En_j%7D(x)%5C%5C%0A%20%20%20%20&amp;%5C;%5C;+%5Csum_%7Ba%5Cne1:N%7D%5Coverline%7B%5Ciota%7D(V(x),a)%5Cbiggr(f(x%5E%7Ba(1:N)%7D)-f(x%5E%7B1:N%7D)%5Cbiggl)%0A%5Cend%7Balign*%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A(f%5Cin%20C_c%5E2(%5Cmathbb%7BR%7D%5E%7BdN%7D),x%5Cin%5Cmathbb%7BR%7D%5E%7BdN%7D,x%5En%5Cin%5Cmathbb%7BR%7D%5Ed)%0A"></p>
<p>when the latent process <img src="https://latex.codecogs.com/png.latex?(X_t)"> is an <strong>Itô process</strong> given by the generator:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20Lf(x)&amp;=%5Csum_%7Bi=1%7D%5Edb_i(x)%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x_i%7D(x)%5C%5C%0A%20%20%20%20&amp;%5C;%5C;+%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi,j=1%7D%5Ed(%5Csigma%5Csigma%5E%5Ctop)_%7Bij%7D(x)%5Cfrac%7B%5Cpartial%20%5E2f%7D%7B%5Cpartial%20x_i%5Cpartial%20x_j%7D(x)%0A%5Cend%7Balign*%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A(f%5Cin%20C_c%5E2(%5Cmathbb%7BR%7D%5Ed),x%5Cin%5Cmathbb%7BR%7D%5Ed)%0A"></p>
<p>For details, please consult <span class="citation" data-cites="Chopin+2022">(Chopin et al., 2022, p. 3206)</span>, Theorem 19.</p>
</section>
<section id="conclusions" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">5</span> Conclusions</h2>
<div class="callout callout-style-default callout-important callout-titled" title="Summaries">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summaries
</div>
</div>
<div class="callout-body-container callout-body">
<p>SMC with efficient resampling schemes possess a continuous-time limit <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cto0">, which turns out to be a Feller-Dynkin process, a diffusion process with jumps, when <img src="https://latex.codecogs.com/png.latex?(X_t)"> is a diffusion.</p>
</div>
</div>
</section>
<section id="forthcoming-research" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="forthcoming-research"><span class="header-section-number">6</span> Forthcoming Research</h2>
<div class="callout callout-style-default callout-important callout-titled" title="Ultimate Purpose">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ultimate Purpose
</div>
</div>
<div class="callout-body-container callout-body">
<p>How can we leverage the knowledge of the continuous-time limit process to design efficient Sequential Monte Carlo (SMC) samplers capable of sampling from posterior distributions of diffusions?</p>
</div>
</div>
<ul>
<li>What are the <strong>properties of this limit jump process</strong>, and how do they change with modifications to the underlying latent process?</li>
<li>How does the <strong>timing of resampling</strong> affect overall efficiency? Can insights be gained from the perspective of continuous-time limits?</li>
<li>Does the continuous-time limit process improve SMC efficiency when used for <strong>particle propagation</strong>?</li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Bierkens+2019" class="csl-entry">
Bierkens, J., Fearnhead, P., &amp; Roberts, G. (2019). <span class="nocase">The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data</span>. <em>The Annals of Statistics</em>, <em>47</em>(3), 1288–1320. <a href="https://doi.org/10.1214/18-AOS1715">https://doi.org/10.1214/18-AOS1715</a>
</div>
<div id="ref-Bouchard-Cote+2018-BPS" class="csl-entry">
Bouchard-Côté, A., Vollmer, S. J., &amp; Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. <em>Journal of the American Statistical Association</em>, <em>113</em>(522), 855–867. <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1294075">https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1294075</a>
</div>
<div id="ref-Chopin+2022" class="csl-entry">
Chopin, N., Singh, S. S., Soto, T., &amp; Vihola, M. (2022). <span class="nocase">On Resampling Schemes for Particle Filter with Weakly Informative Observations</span>. <em>The Annals of Statistics</em>, <em>50</em>(6), 3197–3222. <a href="https://doi.org/10.1214/22-AOS2222">https://doi.org/10.1214/22-AOS2222</a>
</div>
<div id="ref-Davis1984" class="csl-entry">
Davis, M. H. A. (1984). Piecewise-deterministic markov processes: A general class of non-diffusion stochastic models. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>46</em>(3), 353–388. <a href="https://www.jstor.org/stable/2345677">https://www.jstor.org/stable/2345677</a>
</div>
<div id="ref-Fearnhead+2018-PDMC" class="csl-entry">
Fearnhead, P., Bierkens, J., Pollock, M., &amp; Roberts, G. O. (2018). Piecewise deterministic markov processes for continuous-time monte carlo. <em>Statistical Science</em>, <em>33</em>(3), 386–412. <a href="https://www.jstor.org/stable/26771007">https://www.jstor.org/stable/26771007</a>
</div>
<div id="ref-Gordon+1993" class="csl-entry">
Gordon, N. G., Salmond, D. J., &amp; Smith, A. F. M. (1993). Novel approach to nonlinear/non-gaussian bayesian state estimation. <em>IEE Proceedings-F</em>, <em>140</em>(2), 107–113.
</div>
<div id="ref-Kitagawa1993" class="csl-entry">
Kitagawa, G. (1993). A monte carlo filtering and smoothing method for non-gaussian nonlinear state space methods. <em>Proceedings of the 2nd u.s.-Japan Joint Seminar on Statistical Time Series Analysis. Honolulu, Hawaii, January</em>, 25–29. <a href="https://www.ism.ac.jp/~kitagawa/1993_US-Japan.pdf">https://www.ism.ac.jp/~kitagawa/1993_US-Japan.pdf</a>
</div>
<div id="ref-Murphy2023" class="csl-entry">
Murphy, K. P. (2023). <em>Probabilistic machine learning: Advanced topics</em>. MIT Press. <a href="http://probml.github.io/book2">http://probml.github.io/book2</a>
</div>
<div id="ref-Peters-deWith2012" class="csl-entry">
Peters, E. A. J. F., &amp; de&nbsp;With, G. (2012). Rejection-free monte carlo sampling for general potentials. <em>Physical Review E</em>, <em>85</em>(2). <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026703">https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026703</a>
</div>
<div id="ref-Theodoridis2020" class="csl-entry">
Theodoridis, S. (2020). <em>Machine learning: A bayesian and optimization perspective</em> (2nd ed.). Academic Press. <a href="https://doi.org/10.1016/C2019-0-03772-7">https://doi.org/10.1016/C2019-0-03772-7</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Good references are <span class="citation" data-cites="Murphy2023">(Murphy, 2023)</span> Chapter 13, and <span class="citation" data-cites="Theodoridis2020">(Theodoridis, 2020, p. 881)</span> Section 17.4.↩︎</p></li>
<li id="fn2"><p><span class="citation" data-cites="Fearnhead+2018-PDMC">(Fearnhead et al., 2018)</span> is a great introduction to this topic.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Particles</category>
  <category>Computation</category>
  <category>Poster</category>
  <guid>https://162348.github.io/posts/2024/Particles/PF.html</guid>
  <pubDate>Sat, 24 Feb 2024 15:00:00 GMT</pubDate>
</item>
<item>
  <title>数学者のための深層学習２</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Kernels/Deep2.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="トランスフォーマー" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</h2>
<section id="名前の由来と背景" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</h3>
<p>トランスフォーマー <span class="citation" data-cites="Vaswani+2017">(Vaswani et al., 2017)</span> は，注意 (attension) という機構を通じて，時系列データの依存関係を効率的に学習することの出来るモデルである．この「変換器」という名前は，後述の内部表現ベクトル <img src="https://latex.codecogs.com/png.latex?Y"> を，入力 <img src="https://latex.codecogs.com/png.latex?X"> から次元を変えずにより良いものに「変換する」というところから名前が付けられている．</p>
<p>初めは自然言語処理（特に機械翻訳）の文脈で導入されたデコーダーとエンコーダーの組からなるモデルであるが，そのエンコーダー部分だけで言語，画像，動画などあらゆる系列データのモデリング全体で抜群の性能を発揮する上に，これら複数ドメインのデータを組み合わせてモデリングすることもできる（第 4 節）．</p>
<p>さらに，トランスフォーマーはアーキテクチャとして（CNN や RNN などに比べると）シンプルであり，大規模なデータセットで大規模なモデルを訓練することが出来るスケーラビリティが魅力である．また，モデルの大きさに対して性能が単調に改善するというスケーリング則 <span class="citation" data-cites="Hestness+2017">(Hestness et al., 2017)</span>, <span class="citation" data-cites="Kaplan+2020">(Kaplan et al., 2020)</span> が成り立つことが示されており，大規模な資源を投下して大規模なモデルを作る経営判断も下しやすかった．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/Images/SimplePowerLaws.png" class="img-fluid figure-img"></p>
<figcaption>Scaling Laws <span class="citation" data-cites="Kaplan+2020">(Kaplan et al., 2020)</span></figcaption>
</figure>
</div>
<p>その後すぐに，一度大規模なモデルを訓練してしまえば，少しの修正を施すのみで種々の下流タスクに適用することが可能であることが発覚した．これを <strong>基盤モデル</strong> という（第 3.2 節）．</p>
</section>
<section id="注意機構" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="注意機構"><span class="header-section-number">1.2</span> 注意機構</h3>
<p>トランスフォーマーの核はその注意機構にある．とはいっても，注意機構自体はトランスフォーマー以前から存在した技術である．</p>
<p>元々機械翻訳に用いられていたエンコーダー・デコーダー型の RNN の性能を向上させる機構として提案された <span class="citation" data-cites="Bahdanau+2015">(Bahdanau et al., 2015)</span>．その後，<span class="citation" data-cites="Vaswani+2017">(Vaswani et al., 2017)</span> の <em>Attention is All You Need</em> とは，<strong>注意機構のみが重要で，RNN としての構造（や画像では畳み込みの構造）を排してシンプルにした方が更に性能が向上する</strong>，という報告である．</p>
<p>時系列データの解析では，そして自然言語処理ではとりわけ，文脈というものが重要である．しかし文脈は長期の依存関係になることもしばしばあり，従来の RNN ではこのモデリングに苦労していた (bottleneck problem)．</p>
<p>注意機構は，遠く離れた２つのトークンも直接に相互作用を持つアーキテクチャになっており，この点を抜本的に解決したものである．その結果，元の RNN のアーキテクチャも不要とするくらいのモデリング能力を，自然言語のみでなく，画像や動画に対しても示したのである．</p>
<p>注意機構は自己注意と交差注意に分けられる．</p>
<section id="枠組み" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="枠組み"><span class="header-section-number">1.2.1</span> 枠組み</h4>
<p>トランスフォーマーに入力する系列を <img src="https://latex.codecogs.com/png.latex?%5C%7Bx%5En%5C%7D_%7Bn=1%7D%5EN%5Csubset%5Cmathbb%7BR%7D%5ED"> で表す．生のデータをそのままモデルに入れるわけではないので，別の言葉で呼び変える．</p>
<p>慣習として，特に言語データの場合は各 <img src="https://latex.codecogs.com/png.latex?x%5En"> を <strong>トークン</strong> (token) という．画像では <strong>パッチ</strong> (patch) ともいう．</p>
<p>以降，<img src="https://latex.codecogs.com/png.latex?X:=(x%5En)_%7Bn=1%7D%5EN%5Cin%20M_%7BND%7D(%5Cmathbb%7BR%7D)"> とも表す．</p>
</section>
<section id="自己注意機構のプロトタイプ" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="自己注意機構のプロトタイプ"><span class="header-section-number">1.2.2</span> 自己注意機構のプロトタイプ</h4>
<p>自己注意機構とは，<img src="https://latex.codecogs.com/png.latex?Y=AX"> によって定まる <img src="https://latex.codecogs.com/png.latex?M_%7BND%7D(%5Cmathbb%7BR%7D)"> 上の線型変換 <img src="https://latex.codecogs.com/png.latex?X%5Cmapsto%20Y"> のことである： <span id="eq-1"><img src="https://latex.codecogs.com/png.latex?%0Ay%5En=%5Csum_%7Bm=1%7D%5EN%20a%5En_mx%5Em,%0A%5Ctag%7B1%7D"></span> <span id="eq-2"><img src="https://latex.codecogs.com/png.latex?%0Aa%5En_m=%5Cfrac%7Be%5E%7B(x%5En)%5E%5Ctop%20x%5Em%7D%7D%7B%5Csum_%7Bk=1%7D%5ENe%5E%7B(x%5En)%5E%5Ctop%20x%5Ek%7D%7D.%0A%5Ctag%7B2%7D"></span> ここで，<img src="https://latex.codecogs.com/png.latex?A=(a%5En_m)_%7Bn,m%5Cin%5BN%5D%7D%5Cin%20M_N(%5Cmathbb%7BR%7D)"> は <a href="https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E8%A1%8C%E5%88%97">確率行列</a> をなし，その成分を <strong>注意荷重</strong> (attention weight) という．</p>
<p>この変換において，同じ <img src="https://latex.codecogs.com/png.latex?x%5Em"> の値を，３回別々の意味で使われていることに注意する：</p>
<ul>
<li>式&nbsp;1 における <img src="https://latex.codecogs.com/png.latex?x%5Em"> は，新たな表現 <img src="https://latex.codecogs.com/png.latex?y%5En"> を作るためのプロトタイプにような働きをしている．これを <strong>値</strong> (value) という．</li>
<li>式&nbsp;2 において，内積が用いられており，<img src="https://latex.codecogs.com/png.latex?x%5En"> と <img src="https://latex.codecogs.com/png.latex?x%5Em"> の類似度が測られている．
<ul>
<li><img src="https://latex.codecogs.com/png.latex?x%5Em"> を，<img src="https://latex.codecogs.com/png.latex?x%5Em"> が提供出来る情報を要約した量としての働きをし，<strong>鍵</strong> (key) という．</li>
<li><img src="https://latex.codecogs.com/png.latex?x%5En"> は，<img src="https://latex.codecogs.com/png.latex?x%5En"> と関連すべき情報を要求する役割を果たし，<strong>クエリ</strong> (query) という．</li>
</ul></li>
<li>最終的に，鍵とクエリの類似度・マッチ度を，<a href="https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0">ソフトマックス関数</a> を通じて確率分布として表現し，値の空間 <img src="https://latex.codecogs.com/png.latex?%5C%7Bx%5Em%5C%7D_%7Bm=1%7D%5EN"> 上の確率質量関数 <img src="https://latex.codecogs.com/png.latex?%5C%7Ba%5En_m%5C%7D_%7Bm=1%7D%5EN"> を得ている．これに関して <strong>平均する</strong> ことで，鍵 <img src="https://latex.codecogs.com/png.latex?y%5En"> を得る．</li>
</ul>
</section>
<section id="内積による自己注意機構" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="内積による自己注意機構"><span class="header-section-number">1.2.3</span> 内積による自己注意機構</h4>
<p>３つの別々の役割を果たしている以上，それぞれ固有の表現を持っていても良いはずである．そこで，値，鍵，クエリに，それぞれにニューラルネットワーク <img src="https://latex.codecogs.com/png.latex?W_%7B(%5CLambda)%7D%5Cin%20M_%7BDD_%7B(%5CLambda)%7D%7D(%5Cmathbb%7BR%7D)%5C;(%5CLambda%5Cin%5C%7BV,K,Q%5C%7D)"> を与えて固有の表現 <img src="https://latex.codecogs.com/png.latex?%0Ax_%7B(%5CLambda)%7D%5En:=XW_%7B(%5CLambda)%7D%0A"> を持たせ，この <img src="https://latex.codecogs.com/png.latex?W_%7B(%5CLambda)%7D"> を誤差逆伝播法により同時に学習することとする．</p>
<p>こうして得るのが，<strong>内積による自己注意機構</strong> (dot-product self-attention mechanism) である．このとき，<img src="https://latex.codecogs.com/png.latex?D_%7B(K)%7D=D_%7B(Q)%7D"> は必要だが，<img src="https://latex.codecogs.com/png.latex?y%5En%5Cin%5Cmathbb%7BR%7D%5E%7BD_%7B(V)%7D%7D"> は，元の次元 <img src="https://latex.codecogs.com/png.latex?D"> と異なっても良いことに注意．</p>
<p>最後に，ソフトマックス関数の適用において，勾配消失を回避するために，次元 <img src="https://latex.codecogs.com/png.latex?D_%7B(K)%7D"> に応じたスケーリングを介して <img src="https://latex.codecogs.com/png.latex?%0Aa%5En_m=%5Cfrac%7Be%5E%7B%5Cfrac%7B%5Cleft(x%5En_%7B(Q)%7D%5Cright)%5E%5Ctop%20x%5Em_%7B(K)%7D%7D%7B%5Csqrt%7BD_K%7D%7D%7D%7D%7B%5Csum_%7Bk=1%7D%5ENe%5E%7B%5Cfrac%7B%5Cleft(x%5En_%7B(Q)%7D%5Cright)%5E%5Ctop%20x%5Ek_%7B(K)%7D%7D%7B%5Csqrt%7BD_K%7D%7D%7D%7D%0A"> とする．これを最終的な <strong>自己注意機構</strong> (scaled dot-product self-attention mechanism) という．</p>
</section>
<section id="交差注意" class="level4" data-number="1.2.4">
<h4 data-number="1.2.4" class="anchored" data-anchor-id="交差注意"><span class="header-section-number">1.2.4</span> 交差注意</h4>
<p>デコーダーとエンコーダーの接続部に用いられる <strong>交差注意</strong> (cross attention) については，ここでは触れない．</p>
</section>
<section id="マスキング" class="level4" data-number="1.2.5">
<h4 data-number="1.2.5" class="anchored" data-anchor-id="マスキング"><span class="header-section-number">1.2.5</span> マスキング</h4>
<p>実際に学習するとき，注意荷重 <img src="https://latex.codecogs.com/png.latex?A"> は上三角部分が <img src="https://latex.codecogs.com/png.latex?-%5Cinfty"> になったものを用いる．</p>
<p>これは，次のトークンを予測するにあたって，そのトークンより後のトークンを見ないようにするためである．</p>
</section>
</section>
<section id="トランスフォーマーの全体" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</h3>
<p>注意機構に加えて，次の３要素を含め，典型的には 20 から 24 層を成した深層ニューラルネットワークがトランスフォーマーの全てである．<sup>1</sup></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/Images/ModalNet-21.png" class="img-fluid figure-img"></p>
<figcaption>Transformer Architecture <span class="citation" data-cites="Vaswani+2017">(Vaswani et al., 2017)</span></figcaption>
</figure>
</div>
<section id="多頭注意" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="多頭注意"><span class="header-section-number">1.3.1</span> 多頭注意</h4>
<p>以上の自己注意機構を１単位として，これを複数独立に訓練し，最終的にはこれらの線型結合を採用する仕組みを <strong>多頭注意</strong> (multi-head attention) という．</p>
<p>これにより，種々の文脈をより頑健に読み取ることが出来るようである．</p>
</section>
<section id="残差結合と正規化" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="残差結合と正規化"><span class="header-section-number">1.3.2</span> 残差結合と正規化</h4>
<p>更に勾配消失を回避するために，<a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">残差結合</a> を導入し，訓練の高速化のために正規化 <span class="citation" data-cites="Ba+2016">(Ba et al., 2016)</span> が導入される．</p>
<p>そして，モデルを大規模化していくには，この「多頭注意＋残差結合と正規化」のブロックを積み重ねる．</p>
</section>
<section id="sec-FF" class="level4" data-number="1.3.3">
<h4 data-number="1.3.3" class="anchored" data-anchor-id="sec-FF"><span class="header-section-number">1.3.3</span> 多層パーセプトロン</h4>
<p>注意機構は線型性が高いため，多頭注意の層の間に，通常の Feedforward ネットワークもスタックして，ネットワークの表現能力を保つ工夫もされる．</p>
</section>
<section id="正規化レイヤーについての補足" class="level4" data-number="1.3.4">
<h4 data-number="1.3.4" class="anchored" data-anchor-id="正規化レイヤーについての補足"><span class="header-section-number">1.3.4</span> 正規化レイヤーについての補足</h4>
<p>レイヤー正則化 (layer normalization) <span class="citation" data-cites="Ba+2016">(Ba et al., 2016)</span> は，バッチ正規化 (batch normalization) <span class="citation" data-cites="Ioffe-Szegedy2015">(Ioffe &amp; Szegedy, 2015)</span> が RNN にも適するようにした修正として提案された．</p>
<p>バッチ正規化は，ニューラルネットワークの内部層の学習が，手前の層のパラメータが時事刻々と変化するために安定した学習が出来ないという <strong>内部共変量シフト</strong> (internal covariate shift) にあると突き止め，これをモデルアーキテクチャに正規化層を取り入れることで解決するものである．</p>
<p>正規化層は，ニューラルネットワークへの入力を，平均が零で分散が <img src="https://latex.codecogs.com/png.latex?1"> になるように変換する．元々，ニューラルネットワークの入力を正規化してから学習させることで学習が効率化されることは知られていた <span class="citation" data-cites="LeCun+2012">(LeCun et al., 2012)</span> が，バッチ正規化は，これをバッチごとに，かつ，モデルの内部にも取り込んだものである．</p>
<p>バッチ正規化は精度の上昇と訓練の加速をもたらす．これはバッチ正規化により大きな学習率で訓練しても活性化が発散せず，これにより訓練時間の短縮と，局所解に囚われにくく汎化性能の向上がもたらされているようである <span class="citation" data-cites="Bjorck+2018">(Bjorck et al., 2018)</span>．</p>
</section>
</section>
<section id="なぜトランスフォーマーはうまく行くのか" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="なぜトランスフォーマーはうまく行くのか"><span class="header-section-number">1.4</span> なぜトランスフォーマーはうまく行くのか？</h3>
<p>注意機構は全体として線型変換になっている．これをカーネル法などを用いて非線型にする試みは多くあるが，これは成功していない．<sup>2</sup></p>
<p>その代わり，トランスフォーマーのパラメータ数のほとんどは FF 層（ 節&nbsp;1.3.3 ）によるものであり，この層が大きな表現能力を持っていることが，トランスフォーマーの性能を支えていると考えられている．<sup>3</sup></p>
<p><strong>注意機構は，遠く離れた２つのトークンを直接相互作用可能にすることに妙がある</strong>．実際，注意機構は，荷重行列を入力から学習するような，荷重平均プーリング (weighted mean pooling) ともみなせる．</p>
</section>
</section>
<section id="言語トランスフォーマー" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</h2>
<p>トランスフォーマーの訓練は，後述するように事前学習と事後調整からなる．事後調整は 節&nbsp;3 で述べる．ここでは，事前学習を，言語を例に取って説明する．</p>
<p>トランスフォーマーの事前学習とは <a href="https://ja.wikipedia.org/wiki/%E3%83%AC%E3%83%88%E3%83%AD%E3%83%8B%E3%83%A0">レトロニム</a> であり，トークン（≒単語）上の確率分布をモデリングをすることに他ならない．</p>
<p>古典的には <img src="https://latex.codecogs.com/png.latex?n">-gram 節&nbsp;2.2.1 などのモデルが用いられていたが，これをニューラルネットワークによって作ることはトランスフォーマー以前から試みられていた <span class="citation" data-cites="Bengio+2000">(Bengio et al., 2000)</span>．</p>
<p>その後，トランスフォーマーの登場まで，これには RNN 節&nbsp;2.2.2 が主に用いられていた．しかし，RNN は長い系列に対しては勾配消失とボトルネック問題が起こりやすく，また，訓練の並列化が難しいという問題があった．</p>
<section id="言語の取り扱い" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</h3>
<section id="単語の分散表現" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="単語の分散表現"><span class="header-section-number">2.1.1</span> 単語の分散表現</h4>
<p>言語をそのまま扱うのではなく，トークン <img src="https://latex.codecogs.com/png.latex?x%5En%5Cin%5Cmathbb%7BR%7D%5ED"> の形に符号化する必要がある．</p>
<p>言語には他にも改行や数式，コンピューターコードがあるが，まずは単語の表現を考える．</p>
<p>単語を Euclid 空間内に埋め込んだものを <strong>分散表現</strong> (distributed representation) という．これを２層のニューラルネットワークで行う技術が <code>word2vec</code> である <span class="citation" data-cites="Mikolov2013">(Mikolov et al., 2013)</span>．</p>
<p>その訓練法には２つあり，窓の幅を <img src="https://latex.codecogs.com/png.latex?M=5"> などとすると，</p>
<ul>
<li>CBOW (Continuous Bag of Words)：前後 <img src="https://latex.codecogs.com/png.latex?M"> 語のみを見せて，中央の語を予測する．</li>
<li>Continuous Skip-gram：中央の語を見せて，前後 <img src="https://latex.codecogs.com/png.latex?M"> 語を予測する．</li>
</ul>
<p>という，いずれも教師なしの方法によって学習される．</p>
</section>
<section id="トークン化" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="トークン化"><span class="header-section-number">2.1.2</span> トークン化</h4>
<p>バイトペア符号化 (BPE: Byte Pair Encoding) <span class="citation" data-cites="Sennrich+2016">(Sennrich et al., 2016)</span> は，データ圧縮の手法であるが，単語に限らず種々のデータを含んだ文字列を符号化するのに用いられる．</p>
</section>
<section id="位置情報符号化" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="位置情報符号化"><span class="header-section-number">2.1.3</span> 位置情報符号化</h4>
<p>トランスフォーマーはそのままではトークンの順番を考慮しないため，トークンの順番の情報も符号化時に含める必要がある．これを <strong>位置情報符号化</strong> (positional encoding) という <span class="citation" data-cites="Dufter+2021">(Dufter et al., 2021)</span>．</p>
<p>このようにして，位置情報はトランスフォーマーのモデル構造を修正して組み込むのではなく，符号化の段階で組み込み，トランスフォーマーはそのまま使うのである．</p>
<p>これは，位置情報をトークンと同じ空間に埋め込んだ表現 <img src="https://latex.codecogs.com/png.latex?r%5En"> を学習し， <img src="https://latex.codecogs.com/png.latex?%0A%5Cwidetilde%7Bx%7D%5En:=x%5En+r%5En%0A"> を新たな符号とする．<sup>4</sup></p>
</section>
</section>
<section id="従来の言語モデル" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</h3>
<p>文章をトークン列 <img src="https://latex.codecogs.com/png.latex?%5C%7Bx%5En%5C%7D_%7Bn=1%7D%5EN%5Csubset%5Cmathbb%7BR%7D%5ED"> に置き換えたあとに，この上の結合分布 <img src="https://latex.codecogs.com/png.latex?p(x%5E1,%5Ccdots,x%5EN)"> をモデリングすることが，<strong>言語モデル</strong> の目標である．</p>
<section id="sec-n-gram" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="sec-n-gram"><span class="header-section-number">2.2.1</span> <img src="https://latex.codecogs.com/png.latex?n">-gram</h4>
<p><img src="https://latex.codecogs.com/png.latex?n%5Cge1"> とし，<img src="https://latex.codecogs.com/png.latex?x_i=0%5C;(i%5Cle0)"> として， <img src="https://latex.codecogs.com/png.latex?%0Ap(x_1,%5Ccdots,x_N)=%5Cprod_%7Bi=1%7D%5ENp_%7B%5Ctheta_i%7D(x_i%7Cx_%7Bi-n%7D,%5Ccdots,x_%7Bi-1%7D)%0A"> という形で <img src="https://latex.codecogs.com/png.latex?p"> をモデリングする．</p>
<p>これを <img src="https://latex.codecogs.com/png.latex?n">-gram モデルと呼ぶが，文章の長さ <img src="https://latex.codecogs.com/png.latex?N"> が大きくなると，必要なパラメータ <img src="https://latex.codecogs.com/png.latex?%5Ctheta_n"> の数が増加する．</p>
<p>これに対処する方法としては，<a href="../../../posts/2023/Surveys/SSM.html">隠れ Markov モデル</a> を用いることが考えられる．</p>
<p>ニューラルネットワーク <span class="citation" data-cites="Bengio+2000">(Bengio et al., 2000)</span> を用いることも出来る．しかし，依存の長さ <img src="https://latex.codecogs.com/png.latex?n%5Cge1"> が固定されていることはやはり問題である．</p>
</section>
<section id="sec-RNN" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="sec-RNN"><span class="header-section-number">2.2.2</span> RNN</h4>
<p>長さ制限のない長期的な依存関係を Feedback network によって表現することが，<span class="citation" data-cites="Mikolov+2010">(Mikolov et al., 2010)</span> によって試みられた．</p>
<p>これは通常のニューラルネットワーク (FFN: Feedforward Network と呼ばれる) に，出力の一部を次の入力に使うという回帰的な流れを追加することで，隠れ Markov モデルのように次に持ち越される内部状態を持つことを可能にしたモデルである．</p>
<p>しかしこれは学習が困難であることと，結局長期的な依存関係は効率的に学習されないという２つの問題があった．</p>
<p>誤差の逆伝播を時間に対しても逆方向に繰り返す必要がある (Backpropagation through time) ので，長い系列に対しては逆伝播しなければいけない距離が長く，勾配消失・爆発が起こりやすい．これは長期的な依存関係を学習しにくいということももたらす．<sup>5</sup> また，並列化も難しく，大規模なモデルの学習は難しい．</p>
<p>これに対処するために，モデルの構造を変えて過去の情報を流用しやすくする方法も種々提案された．LSTM (Long short-term memory) <span class="citation" data-cites="Hochreiter-Schmidhuber1997">(Hochreiter &amp; Schmidhuber, 1997)</span> や GRU (Gated Recurrent Unit) <span class="citation" data-cites="Cho+2014">(Cho et al., 2014)</span> などがその例である．</p>
</section>
</section>
<section id="トランスフォーマーによる言語モデルとその訓練" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="トランスフォーマーによる言語モデルとその訓練"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデルとその訓練</h3>
<p>トランスフォーマーによる言語モデルの最大の美点は，自己教師あり学習による言語モデルの学習が可能である点である．これによりインターネットに蓄積していた大量のデータが利用可能になる．</p>
<p>パラメータを自己教師あり学習により初期化することで，言語モデルの性能が大幅に改善できることは <span class="citation" data-cites="Dai-Le2015">(Dai &amp; Le, 2015)</span> が LSTM 入りの RRN における実験を通じて最初に指摘したようである．</p>
<p>ラベルデータを必要とするならば，これは本当の意味でスケーラブルではなかったであろう．</p>
<ol type="1">
<li>BERT (Bidirectional Encoder Representations from Transformers) <span class="citation" data-cites="Devlin+2019">(Devlin et al., 2019)</span> は，双方向エンコーダーである．</li>
<li>GPT (Generative Pre-trained Transformer) <span class="citation" data-cites="Radford+2018">(Radford et al., 2018)</span> は，単方向デコーダーである．</li>
<li>BART (Bidirectional and Auto-Regressive Transformer) <span class="citation" data-cites="Lewis+2020-BART">(M. Lewis et al., 2020)</span> は，双方向エンコーダーと単方向デコーダーの両方を持つ．</li>
</ol>
<section id="デコーダーのみの言語モデル" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="デコーダーのみの言語モデル"><span class="header-section-number">2.3.1</span> デコーダーのみの言語モデル</h4>
<p>GPT などの生成モデルは，デコーダー部分のトランスフォーマーの機能を主に用いている．</p>
<p>これはまず，</p>
<ol type="1">
<li>トークン列 <img src="https://latex.codecogs.com/png.latex?(x%5En)_%7Bn=1%7D%5E%7BN-1%7D"> を入力し，条件付き分布 <img src="https://latex.codecogs.com/png.latex?p(x%5EN%7Cx_1,%5Ccdots,x%5E%7BN-1%7D)"> を得る．</li>
<li>分布 <img src="https://latex.codecogs.com/png.latex?p(x%5EN%7Cx_1,%5Ccdots,x%5E%7BN-1%7D)"> からサンプリングをする．</li>
</ol>
<p>の２段階で行われる．こうして <img src="https://latex.codecogs.com/png.latex?(x%5En)_%7Bn=1%7D%5EN"> を得たら，次は <img src="https://latex.codecogs.com/png.latex?x%5E%7BN+1%7D"> を生成し，文章が終わるまでこれを続けることで，最終的な生成を完遂する．</p>
<section id="条件付き分布の表現" class="level5" data-number="2.3.1.1">
<h5 data-number="2.3.1.1" class="anchored" data-anchor-id="条件付き分布の表現"><span class="header-section-number">2.3.1.1</span> 条件付き分布の表現</h5>
<p>大規模なデータセットの上で，文章を途中まで読み，次のトークンを推測する，という自己教師あり学習を行うことで，トークン上の条件付き分布を学習する．</p>
<p>この際に，先のトークンの情報は使わないように，注意機構を工夫 (masked / causal attention) して訓練する．</p>
</section>
<section id="条件付き分布からのサンプリング" class="level5" data-number="2.3.1.2">
<h5 data-number="2.3.1.2" class="anchored" data-anchor-id="条件付き分布からのサンプリング"><span class="header-section-number">2.3.1.2</span> 条件付き分布からのサンプリング</h5>
<p>仮に最も確率の高いトークンを毎回選択する場合，出力は決定論的であり，同じ表現を繰り返すことが多くみられる．</p>
<p>実は，より人間らしい表現は，確率の低いトークンもかなら頻繁に採用される <span class="citation" data-cites="Holtzman+2020">(Holtzman et al., 2020)</span>．</p>
<p>かと言って，純粋なサンプリングをしたのでは，文章全体から見て意味をなさない場合も多い．</p>
<p>これを解決したのが top-<img src="https://latex.codecogs.com/png.latex?p"> sampling / nucleus sampling <span class="citation" data-cites="Holtzman+2020">(Holtzman et al., 2020)</span> である．</p>
<p><a href="https://github.com/openai/gpt-2-output-dataset/issues/5">GPT-2 にも実装されている</a> ようである．</p>
</section>
</section>
<section id="エンコーダーのみの言語モデル" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="エンコーダーのみの言語モデル"><span class="header-section-number">2.3.2</span> エンコーダーのみの言語モデル</h4>
<p>BERT (bidirectional encoder representations from transformers) <span class="citation" data-cites="Devlin+2019">(Devlin et al., 2019)</span> などの言語理解モデルは，エンコーダ部分のトランスフォーマーの機能を主に用いている．その結果，生成は出来ない．</p>
<p>訓練は，データセットから単語を確率的に脱落させ，これを補完するように訓練する．結果として，文章の前後両方 (bidirectional) の文脈を考慮するようになるのである．</p>
<p>実際に使う際は，例えば感情の判別などでは，文章の冒頭に <code>[class]</code> などの特殊なトークンを置き，これをエンコーダーに通してトークンが何に置き換わるかを見ることで，判別を実行することができる．</p>
</section>
<section id="エンコーダーデコーダーの言語モデル" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="エンコーダーデコーダーの言語モデル"><span class="header-section-number">2.3.3</span> エンコーダー・デコーダーの言語モデル</h4>
<p>トランスフォーマーは原論文 <span class="citation" data-cites="Vaswani+2017">(Vaswani et al., 2017)</span> では，エンコーダーとデコーダーがセットになったモデルとして提案された．</p>
<p>これは機械翻訳を念頭に置いていたため，RNN の構造を引き継いだ形で提案されたためである．この場合，次のようにしてモデルは使われる</p>
<ol type="1">
<li>入力 <img src="https://latex.codecogs.com/png.latex?X"> をエンコーダーに通し，内部表現 <img src="https://latex.codecogs.com/png.latex?Z"> を得る．</li>
<li>この内部表現 <img src="https://latex.codecogs.com/png.latex?Z"> を元に，デコードした結果 <img src="https://latex.codecogs.com/png.latex?Y"> を出力する．</li>
<li>唯一，<img src="https://latex.codecogs.com/png.latex?Z"> をデコーダーに渡す部分での注意機構層では，鍵と値としては <img src="https://latex.codecogs.com/png.latex?Z"> を使うが，クエリとしては <img src="https://latex.codecogs.com/png.latex?Y"> を使う．</li>
</ol>
<p>３の機構を <strong>エンコーダー・デコーダーの注意機構</strong> (encoder-decoder / corss attention mechanism) といい，これによって <img src="https://latex.codecogs.com/png.latex?Z"> と <img src="https://latex.codecogs.com/png.latex?Y"> のトークンの間の類似度をモデルに取り入れる．</p>
</section>
</section>
<section id="近年の進展" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="近年の進展"><span class="header-section-number">2.4</span> 近年の進展</h3>
<section id="sec-in-context-learning" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="sec-in-context-learning"><span class="header-section-number">2.4.1</span> 分布外データに対するロバスト性</h4>
<p>GPT-4 などの大規模言語モデルが，コンテクスト内学習 (in-context learning) が可能であることが，興味深い現象として解析されている．</p>
<p>この文脈内学習とは，<strong>パラメータをそのタスクに対して事後調整した訳でもないのに優れた性能を見せること</strong> を指す <span class="citation" data-cites="Garg+2022">(Garg et al., 2022)</span>．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/Images/ff_1.png" class="img-fluid figure-img"></p>
<figcaption>An Example of In-context Learning <span class="citation" data-cites="Bubeck+2023">(Bubeck et al., 2023)</span></figcaption>
</figure>
</div>
<p>これの理論的な解明が進んでいる．</p>
<p>トランスフォーマーの注意機構をデータが通過する過程が，勾配降下を通じて局所モデルを学習する過程と等価になっている見方が指摘されている <span class="citation" data-cites="vanOswald+2023">(von&nbsp;Oswald et al., 2023)</span>．すなわち，トランスフォーマーを通過すること自体が，汎用的な学習そのものになっている <span class="citation" data-cites="Akyurek+2023">(Akyürek et al., 2023)</span>, <span class="citation" data-cites="Bai+2023">(Bai et al., 2023)</span>, <span class="citation" data-cites="Guo+2024">(Guo et al., 2024)</span>, <span class="citation" data-cites="Kim-Suzuki2024">(Kim &amp; Suzuki, 2024)</span>．</p>
</section>
<section id="sec-SSM" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="sec-SSM"><span class="header-section-number">2.4.2</span> 状態空間モデルによる依存構造モデリング</h4>
<p>トランスフォーマーの注意機構はデータ内の長期的な依存関係をモデリングできる点が革新的なのであった．</p>
<p>しかし，そのシークエンスの長さに対して計算複雑性が非線型に増加してしまう点を改良すべく，種々の代替的なアーキテクチャが試みられており，中でも状態空間モデルを中間層に用いるモデルが注目されている．</p>
<p>しかし，まだまだ長期的な依存関係の処理を苦手とするため，言語においては注意機構ほど性能が出ず，トランスフォーマーに比べて並列計算が難しいために実行速度が遅くなる，という問題がある．</p>
<p>しかし，これらは解決可能で，トランスフォーマーの性能を凌駕する可能性があるとされている <span class="citation" data-cites="Gu+2022">(Gu et al., 2022)</span>, <span class="citation" data-cites="Fu+2023">(Fu et al., 2023)</span>．まず，状態空間モデルのパラメータを入力から決めることで依存関係のモデリングを豊かにし（ <strong>選択的状態空間モデル</strong> (Selective SSM)），並列可能なアルゴリズムも提案されている．</p>
<p>実際に，選択的状態空間モデルを注意機構と多層パーセプトロン層の代わりに取り入れた Mamba <span class="citation" data-cites="Gu-Dao2024">(Gu &amp; Dao, 2024)</span> は同じサイズのトランスフォーマーの性能を凌駕する．</p>
</section>
</section>
</section>
<section id="sec-fine-tuning" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-fine-tuning"><span class="header-section-number">3</span> 基盤モデル</h2>
<p>大規模なトランスフォーマーを，インターネットに蓄積していた大量のデータを用いて訓練することにより得るモデルは，チャットボットや感情分析，要約など種々の下流タスクに少しの事後調整を施すだけで抜群の性能を発揮することが発見された．</p>
<p>これを <strong>基盤モデル</strong> という．</p>
<section id="大規模言語モデル" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="大規模言語モデル"><span class="header-section-number">3.1</span> 大規模言語モデル</h3>
<section id="名前の由来と背景-1" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="名前の由来と背景-1"><span class="header-section-number">3.1.1</span> 名前の由来と背景</h4>
<p>自然言語処理にトランスフォーマーを応用した例は大きな成功を見ている．GPT <span class="citation" data-cites="Radford+2018">(Radford et al., 2018)</span>, <a href="https://openai.com/research/gpt-2-1-5b-release">GPT-2</a> <span class="citation" data-cites="Radford+2019">(Radford et al., 2019)</span>, GPT-3 <span class="citation" data-cites="Brown+2020">(Brown et al., 2020)</span>, <a href="https://openai.com/research/gpt-4">GPT-4</a> <span class="citation" data-cites="OpenAI2023">(OpenAI, 2023b)</span> のシリーズはその代表であり，特に GPT-4 はその文脈内学習能力（第 2.4.1 節）の高さから AGI の実現に向けた重要な一歩とも評されている <span class="citation" data-cites="Bubeck+2023">(Bubeck et al., 2023)</span>．</p>
<p>その成功は，アーキテクチャとして優れているという点よりもむしろ，並列化が可能であり GPU などの計算資源を効率的に使える <span class="citation" data-cites="Weng-Brockman2022">(Weng &amp; Brockman, 2022)</span> という点にあり，アーキテクチャの改良よりも計算資源の増強が最終的に大きな進歩をもたらすという側面が大きい，という認識が優勢になっている <span class="citation" data-cites="Sutton2019">(R. Sutton, 2019)</span>．これはスケーリング則として理論的にも理解が試みられている <span class="citation" data-cites="Kaplan+2020">(Kaplan et al., 2020)</span>．</p>
<p>この観点から，トランスフォーマーを用いた事前学習済みの言語モデルが，種々のタスクをほとんど例示なし (few-shot / zero-shot) で解ける能力を創発する程度に大きい場合，その規模が意味を持つことを強調して，<strong>大規模言語モデル</strong> (LLM: Large Language Model) とも呼ぶ <span class="citation" data-cites="Zhao+2023">(Zhao et al., 2023)</span>．</p>
</section>
<section id="最適なモデルサイズ" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="最適なモデルサイズ"><span class="header-section-number">3.1.2</span> 最適なモデルサイズ</h4>
<p><strong>従来の LLM は，訓練データに対してモデルが大規模すぎる</strong> 可能性があることが <span class="citation" data-cites="Hoffmann+2022">(Hoffmann et al., 2022)</span> で指摘された．</p>
<table class="table-striped table-hover table">
<caption>Size of LLMs</caption>
<colgroup>
<col style="width: 50%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">Training Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LaMDA <span class="citation" data-cites="Thoppilan+2022">(Thoppilan et al., 2022)</span></td>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">168B</td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT-3 <span class="citation" data-cites="Brown+2020">(Brown et al., 2020)</span></td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Jurassic <span class="citation" data-cites="Lieber+2021">(Lieber et al., 2021)</span></td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="even">
<td style="text-align: center;">Gopher <span class="citation" data-cites="Rae+2021">(Rae et al., 2021)</span></td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Chinchilla</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">1.4T</td>
</tr>
</tbody>
</table>
<p>最適なパラメータ-学習データサイズの比を考慮して設計された Chinchilla <span class="citation" data-cites="Hoffmann+2022">(Hoffmann et al., 2022)</span> は，モデルのサイズは最も小さいにも拘らず，種々の下流タスクに対して，他のモデルを凌駕することが <span class="citation" data-cites="Hoffmann+2022">(Hoffmann et al., 2022)</span> で報告されている．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/Images/model_sizes.png" class="img-fluid figure-img"></p>
<figcaption>Number of Training Tokens <a href="https://babylm.github.io/">BabyLM Challenge</a></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="種々の LLM とマルチモーダル化">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
種々の LLM とマルチモーダル化
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Google の <a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(Lepikhin et al., 2021)</span>，<a href="https://japan.googleblog.com/2023/05/palm-2.html">PaML</a> <span class="citation" data-cites="Chowdhery+2022">(Chowdhery et al., 2022)</span>，M4 <span class="citation" data-cites="Aharoni+2019">(Aharoni et al., 2019)</span>，Google Brain の Switch Transformers <span class="citation" data-cites="Fedus+2022">(Fedus et al., 2022)</span>，Google DeepMind の <a href="https://deepmind.google/discover/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval/">Gopher</a> <span class="citation" data-cites="Rae+2021">(Rae et al., 2021)</span> などがある．</p>
<p>最近のものでは，</p>
<ul>
<li>Google の <a href="https://blog.google/technology/ai/lamda/">LaMDA</a> <span class="citation" data-cites="Thoppilan+2022">(Thoppilan et al., 2022)</span> は会話に特化した LLM である．</li>
<li>Google から 12/6/2023 に Gemini <span class="citation" data-cites="Geminiteam+2023">(Team et al., 2023)</span> が発表され，<a href="https://japan.googleblog.com/2024/02/gemini-15.html">2/16/2024</a> には Gemini 1.5 が発表された．
<ul>
<li>これに伴い，Bard と Duet AI はいずれも Gemini に名称変更された．</li>
<li><a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(Lepikhin et al., 2021)</span> 同様，トランスフォーマーに加えて，新しいアーキテクチャである Sparsely-Gated MoE <span class="citation" data-cites="Shazeer+2017">(Shazeer et al., 2017)</span> が用いられている．これはモデルのパラメータを分割し（それぞれを専門家 expert という），１つの入力にはその一部分しか使わないようにすることでメモリを節約し並列化を可能にする手法である．</li>
<li>文書から高精度にテキストを抽出する LMDX (Language Model-based Document Information Extraction and Localization) <span class="citation" data-cites="Perot+2023">(Perot et al., 2023)</span> も用いられている．</li>
</ul></li>
<li>OpenAI から 9/5/2023 に GPT-4V <span class="citation" data-cites="OpenAI2023-GPT4V">(OpenAI, 2023c)</span> が発表され，ChatGPT にも実装された．
<ul>
<li>Microsoft の研究者も，GPT-4V の出来ること関する考察 <span class="citation" data-cites="Yang+2023">(Yang et al., 2023)</span> を発表している．</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="訓練の並列化" class="level4" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="訓練の並列化"><span class="header-section-number">3.1.3</span> 訓練の並列化</h4>
<p>ここまで大規模なモデルだと，訓練時の GPU の並行計算を適切に計画することが肝心になる．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/Images/parallelism.png" class="img-fluid figure-img"></p>
<figcaption>four types of parallelism <span class="citation" data-cites="Weng-Brockman2022">(Weng &amp; Brockman, 2022)</span></figcaption>
</figure>
</div>
<p><a href="https://ja.wikipedia.org/wiki/ByteDance">ByteDance</a> の MegaScale <span class="citation" data-cites="Jiang+2024">(Jiang et al., 2024)</span> は，12,288 の GPU を用いながら，55.2 % の Model FLOPs Utilization を引き出した．</p>
<p>更なる LLM の訓練の効率化には，GPU による並列計算におけるボトルネックである <a href="https://ja.wikipedia.org/wiki/%E3%83%A1%E3%83%A2%E3%83%AA%E5%B8%AF%E5%9F%9F%E5%B9%85">メモリ帯域幅</a> を克服するために，分散型訓練手法を採用することが提案されている．</p>
</section>
</section>
<section id="sec-foundation-model" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</h3>
<p>GPT の P とは Pre-trained である．自己教師あり学習によって <a href="../../../posts/2024/Kernels/Deep.html#sec-pretraining-using-AE">事前学習</a> をしたあと，その後のタスクに応じて，教師あり学習によって <strong>事後調整</strong> (fine-tune) を行う．<sup>6</sup></p>
<p>事後調整では，目的関数に KL 乖離度を入れるなどして，元のモデルから遠く離れすぎないように工夫されている．</p>
<p>事後調整を行う前の大規模言語モデルのことを，種々の応用や下流タスク (downstream task) の基礎となるモデルであることと，そのものでは未完成であることとを強調して，<strong>基盤モデル</strong> (foundation model) とも呼ばれる <span class="citation" data-cites="Bommasani+2021">(Bommasani et al., 2021)</span>．</p>
<p>事後調整では，モデルの全体では規模が大きすぎるため，出力層の後に新しいニューラルネットを付加したり，最後の数層のみを追加で教師あり学習をしたりする方法が一般的である．または，LoRA (Low-Rank Adaptation) <span class="citation" data-cites="Hu+2021">(E. J. Hu et al., 2021)</span> では，トランスフォーマーの各層に新たな層を挿入し，これを学習する．</p>
<p>これは，事後調整に有効な内的次元は実際には小さく <span class="citation" data-cites="Aghajanyan+2021">(Aghajanyan et al., 2021)</span>，これに有効にアクセスし，効率的な事後調整を行うことが出来るという．さらに <span class="citation" data-cites="Zhou+2023">(Zhou et al., 2023)</span> によると，事後調整に必要なラベル付きデータは，量よりも質が重要であり，LLaMA 5.1 に対しても多くて 1000 データで十分であるようである．</p>
<p>事後調整には，他にも，ChatGPT のようなサービスを展開するために必要なユーザー体験の改善を目的としたものも含まれる．これは <strong>アラインメント</strong> とも呼ばれ，強化学習が用いられることが多い．実際，GPT-4 では <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(Christiano et al., 2017)</span> が用いられている <span class="citation" data-cites="OpenAI2023">(OpenAI, 2023b, p. 2)</span>．</p>
<p>これについては第 3.5 節で改めて論じる．．</p>
</section>
<section id="プロンプトエンジニアリング" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</h3>
<p>基盤モデルには世界と人間に対する膨大な知識が含まれているが，使い方によって大きく性能が変わる．正しい条件付けを行うことで，内部に存在する知識をうまく引き出すことができる．これを大規模言語モデルでは <strong>prompt engineering</strong> <span class="citation" data-cites="Liu+2023">(Liu et al., 2023)</span> という．プロンプトの送り方によって性能がどう変わるかを調べる新たな分野である．</p>
<p>その結果，プロンプト内で新たなタスクを定義するだけで，またはいくつか例を与えるだけで，これが解けてしまうこともわかっており，これを zero-shot または few-shot learning という．</p>
</section>
<section id="rag" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="rag"><span class="header-section-number">3.4</span> RAG</h3>
<p>LLM は世界に関する正確な知識を持っており，知識ベースとしての利用も期待されている <span class="citation" data-cites="Petroni+2019">(Petroni et al., 2019)</span>．<sup>7</sup> しかし，知識を正確に，そして信頼出来る形で引き出すことが難しいのであった．</p>
<p>特に，出典を示すことや，最新の知識のアップデートなどが難題として待っている．</p>
<p>そこで，LLM に（自由に外部情報を探索できるという意味で）ノンパラメトリックな知識ベースを接続することで解決するのが RAG (Retrieval-Augmented Generation) モデル <span class="citation" data-cites="Lewis+2020">(P. Lewis et al., 2020)</span> である．</p>
<p>DPR (Dense Passage Retriever) <span class="citation" data-cites="Karpukhin+2020">(Karpukhin et al., 2020)</span> は文書を密に符号化する手法を開発し，これを用いて文書検索をすることで Q&amp;A タスクを効率的に解く手法を提案した．このような文書の符号化器は <strong>検索器</strong> (retriever) と呼ばれる．</p>
<p>RAG <span class="citation" data-cites="Lewis+2020">(P. Lewis et al., 2020)</span> はこの検索器を BART <span class="citation" data-cites="Lewis+2020-BART">(M. Lewis et al., 2020)</span> に接続した．</p>
<p>REALM (Retrieval-Augmented Language Model) <span class="citation" data-cites="Guu+2020">(Guu et al., 2020)</span> も同時期に提案されている．</p>
<p>Meta での研究 <span class="citation" data-cites="Yasunaga+2023">(Yasunaga et al., 2023)</span> はこの検索器を Text-to-Image トランスフォーマー である CM3 <span class="citation" data-cites="Aghajanyan+2022">(Aghajanyan et al., 2022)</span> と結合することで，初めて言語と画像の両方を扱える RAG モデル RA-CM3 (retrieval-augmented CM3) を構成した．</p>
<p><a href="https://openai.com/research/webgpt">WebGPT</a> <span class="citation" data-cites="Nakano+2022">(Nakano et al., 2022)</span> は，RAG や REAML が文書検索をしているところを，Web 検索を実行できるようにした GPT-3 <span class="citation" data-cites="Brown+2020">(Brown et al., 2020)</span> の事後調整である．</p>
</section>
<section id="sec-alignment" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-alignment"><span class="header-section-number">3.5</span> アラインメント</h3>
<p>LLM などの機械学習モデルを訓練する際の目的関数は，そのままでは人間社会が要請するものとずれがあることが多い．これを修正するような試みを <strong>アラインメント</strong> (alignment) という．</p>
<blockquote class="blockquote">
<p>For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, <strong>these models are not aligned with their users</strong>. <span class="citation" data-cites="Ouyang+2022">(Ouyang et al., 2022)</span></p>
</blockquote>
<p>加えて，人間の選好は，0-1 損失関数で表現できるものではないことが多い．そこで，強化学習を用いることが考えられた．しかし，一々人間がフィードバックを与える方法はスケーラビリティに深刻な問題があるため，「人間の選好」をモデリングするニューラルネットワークを <strong>代理モデル</strong> (surrogate model) として構築することも考える．</p>
<p>その代表的な手法が <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(Christiano et al., 2017)</span> である．</p>
<p><a href="https://openai.com/research/instruction-following">InstructGPT</a> <span class="citation" data-cites="Ouyang+2022">(Ouyang et al., 2022)</span> は OpenAI API を通じて寄せられたフィードバックを用いて，<a href="https://openai.com/research/openai-baselines-ppo">PPO</a> (Proximal Policy Optimization) アルゴリズム <span class="citation" data-cites="Schulman+2017">(Schulman et al., 2017)</span> による強化学習により事後調整をしたものである．</p>
<p>近接ポリシー最適化 (PPO: Proximal Policy Optimization) アルゴリズム <span class="citation" data-cites="Schulman+2017">(Schulman et al., 2017)</span> は 信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) <span class="citation" data-cites="Schulman+2015">(Schulman et al., 2015)</span> の洗練化として提案されたもので，現在の RLHF においても最も広く使われている手法である <span class="citation" data-cites="Zheng+2023">(Zheng et al., 2023)</span>．</p>
<p>InstructGPT が ChatGPT の前身となっている．</p>
</section>
</section>
<section id="sec-multimodal-transformer" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</h2>
<p>トランスフォーマーは自然言語処理の文脈で開発されたが，画像や動画，音声 <span class="citation" data-cites="Radford+2023">(Radford et al., 2023a)</span>，さらにはプログラミング言語 <span class="citation" data-cites="Chen+2021">(Chen et al., 2021)</span> にも適用されている．</p>
<p>動画はまだしも画像には，直感的には時系列構造がないように思えるが，トランスフォーマーはもはや汎用のニューラルネットワークアーキテクチャとして使用できることが解りつつある．</p>
<p>それぞれの応用分野で <strong>モデルの構造は殆ど差異がなく</strong>，トークン化の手法などに差異があるのみのように見受けられる．<sup>8</sup></p>
<section id="画像認識トランスフォーマー-vit" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="画像認識トランスフォーマー-vit"><span class="header-section-number">4.1</span> 画像認識トランスフォーマー (ViT)</h3>
<p>画像の分類問題を解くためのエンコーダ・トランスフォーマーは ViT (Vision Transformer) <span class="citation" data-cites="Dosovitskiy+2021">(Dosovitskiy et al., 2021)</span> と呼ばれており，ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) では未だ <a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">ResNet</a> 系のモデルが優勢であった 2021 年に，これを超える性能を示した．</p>
<p>実はモデルは殆どトランスフォーマーそのままであり，肝要であったのは画像をトークン化である．ピクセルをそのまま用いるのではなく，ある程度大きなピクセルの集合である <strong>パッチ</strong> (patch) を用いることで，計算量を下げる．<span class="citation" data-cites="Dosovitskiy+2021">(Dosovitskiy et al., 2021)</span> では <img src="https://latex.codecogs.com/png.latex?16%5Ctimes16"> サイズなどが採用された．</p>
<p>一方で，画像を恣意的に系列化しているため，幾何学的な構造は１から学ぶ必要があり，最初からモデルに組み込まれている <a href="../../../posts/2024/Kernels/Deep.html#sec-CNN">CNN</a> よりは一般に多くの訓練データを必要とする．だが，これにより帰納バイアスが弱いということでもある．<sup>9</sup></p>
<p>ViT はその後，動画も扱える ViViT <span class="citation" data-cites="Arnab+2021">(Arnab et al., 2021)</span>, あらゆるアスペクト比に対応する NaViT (Native Resolution ViT) <span class="citation" data-cites="Dehghani+2023">(Dehghani et al., 2023)</span> などの拡張が続いた．</p>
<p>DeepMind の <a href="https://deepmind.google/discover/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation/">Perceiver</a> <span class="citation" data-cites="Jaegle+2021">(Jaegle et al., 2021)</span> は画像，動画，音声のいずれのメディアの分類問題にも対応可能な，非対称な注意機構を持つトランスフォーマーである．</p>
</section>
<section id="画像生成トランスフォーマー" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</h3>
<p>一方でデコーダートランスフォーマーを用いて，画像の生成モデリングを行った最初の例は <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(Chen et al., 2020)</span> である．</p>
<p>なお，自己回帰的な生成モデルを通じて画像の生成を試みることは，CNN <span class="citation" data-cites="vandenOord+2016b">(van&nbsp;den&nbsp;Oord, Kalchbrenner, Vinyals, et al., 2016)</span> や RNN <span class="citation" data-cites="vandenOord+2016">(van&nbsp;den&nbsp;Oord, Kalchbrenner, &amp; Kavukcuoglu, 2016)</span> でも行われていた．</p>
<p>この際に判明したことには，画像の分類タスクでは連続表現が役に立っても，生成タスクでは高い解像度を持った画像の生成が難しく，離散表現が有効であることが知られている．しかしこれではデータ量が増えてしまうため，画像の <a href="../../../posts/2024/Computation/VI.html#sec-history">ベクトル量子化</a> が行われることが多い．</p>
<p>そこで，<a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(Chen et al., 2020)</span> でも <img src="https://latex.codecogs.com/png.latex?K">-平均法によるクラスタリングが行われており，さらに <a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いたデータ圧縮も行われている．</p>
<p><a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(Chen et al., 2020)</span> では最終的に各ピクセルを one-hot 表現にまで落とし込み，これを GPT-2 モデル <span class="citation" data-cites="Radford+2019">(Radford et al., 2019)</span> につなげている．</p>
<p><a href="https://muse-model.github.io/">MUSE</a> <span class="citation" data-cites="Chang+2023">(Chang et al., 2023)</span> も，トランスフォーマーを用いた画像生成モデルの例である．</p>
<section id="拡散モデルとの邂逅" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="拡散モデルとの邂逅"><span class="header-section-number">4.2.1</span> 拡散モデルとの邂逅</h4>
<p><a href="../../../posts/2024/Kernels/Deep5.html#sec-idea">潜在拡散モデル</a> で <a href="../../../posts/2024/Kernels/Deep.html#sec-U-net">U-Net</a> <span class="citation" data-cites="Ronneberger+2015">(Ronneberger et al., 2015)</span> を用いていたところをトランスフォーマーに置換した <strong>拡散トランスフォーマー</strong> (DiT: Diffusion Transformer) <span class="citation" data-cites="Peebles-Xie2023">(Peebles &amp; Xie, 2023)</span> が発表された．</p>
<p>その後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) <span class="citation" data-cites="Ma+2024">(Ma et al., 2024)</span> が発表された．</p>
</section>
</section>
<section id="text-to-image-トランスフォーマー" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</h3>
<p>この分野は <span class="citation" data-cites="Reed+2016">(Reed et al., 2016)</span> 以来，初めは GAN によるアプローチが試みられていた．</p>
<p>GPT-3 <span class="citation" data-cites="Brown+2020">(Brown et al., 2020)</span> と <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(Chen et al., 2020)</span> とは殆ど同じモデルを用いている．これらを組み合わせたデコーダー型のトランスフォーマーが <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(Ramesh et al., 2021)</span> である．<sup>10</sup></p>
<p>トークン化して仕舞えば，言語も画像も等価に扱えるというのである．Google の <a href="https://sites.research.google/parti/">Parti</a> <span class="citation" data-cites="Yu+2022">(J. Yu et al., 2022)</span> も同様のアプローチである．</p>
<p>Meta の CM3 <span class="citation" data-cites="Aghajanyan+2022">(Aghajanyan et al., 2022)</span> と <a href="https://ai.meta.com/blog/generative-ai-text-images-cm3leon/">CM3leon</a> <span class="citation" data-cites="Yu+2023">(L. Yu et al., 2023)</span> は画像と言語を両方含んだ HTML ドキュメントから学習している．</p>
<p>Google DeepMind の <a href="https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/">Flamingo</a> <span class="citation" data-cites="Alayrac+2022">(Alayrac et al., 2022)</span> は画像から言語を生成する．</p>
</section>
<section id="image-to-text-トランスフォーマー" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="image-to-text-トランスフォーマー"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/clip">CLIP</a> (Contrastive Language-Image Pre-training) <span class="citation" data-cites="Radford+2021">(Radford et al., 2021)</span> は画像の表現学習をする視覚モデルである．これは <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(Ramesh et al., 2021)</span> と同時に開発された重要な構成要素である．</p>
<p>一方で <a href="https://openai.com/dall-e-2">DALL-E2</a> <span class="citation" data-cites="Ramesh+2022">(Ramesh et al., 2022)</span> では，CLIP により画像を潜在空間にエンコードし，拡散モデルによってデコードする．</p>
<p><a href="https://openai.com/research/dall-e-3-system-card">DALL-E3</a> <span class="citation" data-cites="OpenAI2023DallE3">(OpenAI, 2023a)</span> もその改良である．</p>
</section>
<section id="動画生成トランスフォーマー" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="動画生成トランスフォーマー"><span class="header-section-number">4.5</span> 動画生成トランスフォーマー</h3>
<p>動画を画像の連続と見てトランスフォーマーを応用するアプローチは Latte (Latent Diffusion Transformer) <span class="citation" data-cites="Rakhimov+2020">(Rakhimov et al., 2020)</span> に始まる．</p>
<p><a href="https://wilson1yan.github.io/videogpt/index.html">VideoGPT</a> <span class="citation" data-cites="Yan+2021">(Yan et al., 2021)</span> では動画を 3D の CNN でデータ圧縮，VQ-VAE で量子化して離散的な潜在表現を得た後，GPT と殆ど似たトランスフォーマーに通して学習する．</p>
<p><a href="https://wayve.ai/company/">Wayve</a> の <a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(A. Hu et al., 2023)</span> も同様の手法で動画を生成しているが，その動画を用いて自動運転の強化学習に応用する点が画期的である．</p>
<p>OpenAI は 2/15/2024 に <a href="https://openai.com/sora">Sora</a> <span class="citation" data-cites="Brooks+2024">(Brooks et al., 2024)</span> を発表した．これも <a href="../../../posts/2024/Kernels/Deep5.html#sec-idea">潜在拡散モデル</a> <span class="citation" data-cites="Rombach+2022">(Rombach et al., 2022)</span> 同様，自己符号化器による動画の潜在表現を得た上でパッチに分割し，この上で拡散トランスフォーマー <span class="citation" data-cites="Peebles-Xie2023">(Peebles &amp; Xie, 2023)</span> の学習を行う．</p>
</section>
<section id="世界モデルとしてのトランスフォーマー" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</h3>
<p>トランスフォーマーを世界モデルとして用いて，シミュレーションを行い動画を生成し，これをモデルベースの強化学習 <span class="citation" data-cites="Sutton-Barto2018">(R. S. Sutton &amp; Barto, 2018)</span> の材料とすることが広く提案されている．これは learning in imagination <span class="citation" data-cites="Racaniere+2017">(Racanière et al., 2017)</span> と呼ばれる．<sup>11</sup></p>
<p>IRIS (Imagination with auto-Regression over an Inner Speech) <span class="citation" data-cites="Micheli+2023">(Micheli et al., 2023)</span> はこれに初めてトランスフォーマーを用いた世界モデルから動画生成をした．</p>
<p><a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(A. Hu et al., 2023)</span> は自動運転に特化した世界モデルを，トランスフォーマーを用いて構築している．</p>
<p>他にも，動画生成を強化学習に応用する例としては，OpenAI による <a href="https://openai.com/research/vpt">VPT (Video Pre-Training)</a> <span class="citation" data-cites="Baker+2022">(Baker et al., 2022)</span> がある．</p>
</section>
<section id="音声生成トランスフォーマー" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="音声生成トランスフォーマー"><span class="header-section-number">4.7</span> 音声生成トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/jukebox">Jukebox</a> <span class="citation" data-cites="Dhariwal+2020">(Dhariwal et al., 2020)</span> は，<a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いて音声データを圧縮・量子化し，トランスフォーマーに通したものである．</p>
<p>このトランスフォーマーは <a href="https://openai.com/research/sparse-transformer">Sparse Transformer</a> <span class="citation" data-cites="Child+2019">(Child et al., 2019)</span> という，注意機構の計算効率を改良したモデルを用いている．</p>
</section>
<section id="text-to-speech-トランスフォーマー" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</h3>
<p>Microsoft Research の <a href="https://www.microsoft.com/en-us/research/project/vall-e-x/">VALL-E</a> <span class="citation" data-cites="Wang+2023">(Wang et al., 2023)</span> は，音声データをベクトル量子化によって言語データと全く同等に扱うことで，トランスフォーマーを用いて音声生成を行っている．</p>
</section>
<section id="speach-to-text-トランスフォーマー" class="level3" data-number="4.9">
<h3 data-number="4.9" class="anchored" data-anchor-id="speach-to-text-トランスフォーマー"><span class="header-section-number">4.9</span> Speach to Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/whisper">Whisper</a> <span class="citation" data-cites="Radford+2023-Whisper">(Radford et al., 2023b)</span> は encoder-decoder 型のトランスフォーマーを用いている．</p>
</section>
</section>
<section id="近年の動向" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="近年の動向"><span class="header-section-number">5</span> 近年の動向</h2>
<section id="sec-LLaMA" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="sec-LLaMA"><span class="header-section-number">5.1</span> LLaMA の一般公開とその影響</h3>
<p>Meta AI が <a href="https://about.fb.com/ja/news/2023/07/meta-and-microsoft-introduce-the-next-generation-of-llama/">7/18/2023</a> に LLM <a href="https://llama.meta.com/">LLaMA</a> <span class="citation" data-cites="Touvron+2023">(Touvron et al., 2023)</span> を公開した．そして API を通じて利用する形ではなく，そのモデルのウェイトが公開されたため，Stanford 大学の <a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> など，モデルの改良と研究が促進されている．</p>
<p>特に事後調整のための公開データセットの整備が進んでおり，<a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> では Self-Instruct <span class="citation" data-cites="Wang+2023">(Wang et al., 2023)</span> による効率的な alignment 技術が採用されている．</p>
<p>産業界でも影響は大きい．<a href="https://elyza.ai/">ELYZA</a> は <a href="https://elyza.ai/news/2023/12/27/130%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E5%95%86%E7%94%A8%E5%88%A9%E7%94%A8%E5%8F%AF%E8%83%BD%E3%81%AA%E6%97%A5%E6%9C%AC%E8%AA%9Ellmelyza-j">12/27/2023</a> に日本語に特化した LLM である ELYZA-japanese-Llama-2-13b を公開している．<a href="https://stockmark.co.jp/">Stockmark</a> も <a href="https://stockmark.co.jp/news/20231027">10/27/2023</a> に Stockmark-13b を公開している．</p>
<p>いずれも，開発費と開発時間が大幅に圧縮されたという．<sup>12</sup></p>
<p>IBM は <a href="https://jp.newsroom.ibm.com/2023-09-12-Blog-Building-AI-for-business-IBM-Granite-foundation-models">9/12/2023</a> に LLM Granite を発表している．加えて，プラットフォーム <code>watsonx</code> も提供しており，その上で RAG など独自の事後調整を可能にしている．</p>
<p>IBM と Meta の２社が発起人となり，<a href="https://www.ibm.com/blogs/solutions/jp-ja/the-ai-alliance/">12/5/2023</a> に AI Alliance が発足し，オープンイノベーションを推進している．</p>
<p><a href="https://ja.stability.ai/stable-diffusion">Stable Diffusion</a> <span class="citation" data-cites="Rombach+2022">(Rombach et al., 2022)</span> もソースコードとウェイトが <a href="https://github.com/Stability-AI/stablediffusion">一般公開</a> されている．</p>
</section>
<section id="llm-の経済的影響" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="llm-の経済的影響"><span class="header-section-number">5.2</span> LLM の経済的影響</h3>
<p><span class="citation" data-cites="Tamkin+2021">(Tamkin et al., 2021)</span> は早い段階での OpenAI と Stanford 大学 <a href="https://hai.stanford.edu/">HAI</a> (Human-centered AI) との対談録である．</p>
<p>OpenAI はコード生成能力の経済的な影響を重要なアジェンダとしている <span class="citation" data-cites="Manning+2022">(Manning et al., 2022)</span>．</p>
<p>Open AI の <a href="https://openai.com/blog/openai-codex">Codex</a> <span class="citation" data-cites="Chen+2021">(Chen et al., 2021)</span> はプログラム言語を扱うトランスフォーマーであり，<a href="https://github.com/features/copilot">GitHub Copilot</a> の元となっている．これが社会に与える影響も，新たな評価フレームワークと共に提案されている <span class="citation" data-cites="Khlaaf+2022">(Khlaaf et al., 2022)</span>．</p>
<p>LLM の労働市場へのインパクトも推定している <span class="citation" data-cites="Eloundou+2023">(Eloundou et al., 2023)</span>．これによると，アメリカの労働者の 80% が，LLM の導入により少なくとも仕事の 10% に影響が生じるとしている．さらに全体の 20% は仕事の半分以上が影響を受けるとしている．</p>
</section>
<section id="世界モデルとしての基盤モデル" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="世界モデルとしての基盤モデル"><span class="header-section-number">5.3</span> 世界モデルとしての基盤モデル</h3>
<section id="社会行動シミュレーターとしての-llm" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="社会行動シミュレーターとしての-llm"><span class="header-section-number">5.3.1</span> 社会行動シミュレーターとしての LLM</h4>
<p>社会的なシミュレーションを LLM 内で行うことで，社会科学やビジネスの場面での意思決定を支援することが期待されている．</p>
<p>LLM は人間の心の理論を理解し，その心情・意図を（ある程度）シミュレートすることが出来るようである <span class="citation" data-cites="Andreas2022">(Andreas, 2022)</span>．</p>
<p>LLM でのシミュレーションを通じて，社会科学的な知識を引き出そうとする試みもある <span class="citation" data-cites="Leng-Yuan2023">(Leng &amp; Yuan, 2023)</span>．</p>
</section>
</section>
<section id="llm-と経済安全保障" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="llm-と経済安全保障"><span class="header-section-number">5.4</span> LLM と経済安全保障</h3>
<section id="幻覚の防止" class="level4" data-number="5.4.1">
<h4 data-number="5.4.1" class="anchored" data-anchor-id="幻覚の防止"><span class="header-section-number">5.4.1</span> 幻覚の防止</h4>
<p>LLM が事実と異なる物語を生成することを <strong>幻覚</strong> (hallucination) と呼び，一部の応用では問題になることがある．</p>
<p>これを解決するにあたって，<strong>等角推測</strong> (conformal prediction) と組み合わせ，出力の不確実性を評価することで幻覚を防止する手法が提案されている <span class="citation" data-cites="Mohri-Hashimoto2024">(Mohri &amp; Hashimoto, 2024)</span>．</p>
<p>一般に意思決定の場面において AI を活用するには，不確実性の定量化が必要不可欠である．</p>
<p>GPT-3 を Bayesian にし，自身の確証度合いを言表するように事後調整する研究が OpenAI で行われている <span class="citation" data-cites="Lin+2022">(Lin et al., 2022)</span>．</p>
</section>
<section id="ウォーターマーク" class="level4" data-number="5.4.2">
<h4 data-number="5.4.2" class="anchored" data-anchor-id="ウォーターマーク"><span class="header-section-number">5.4.2</span> ウォーターマーク</h4>
<p>ウォーターマークを開発することで，LLM から出力された文章であることを高確率で検出できるようにする方法が，統計的仮説検定の技術を応用して提案されている <span class="citation" data-cites="Kuditipudi+2023">(Kuditipudi et al., 2023)</span>．</p>
</section>
<section id="偽情報対策" class="level4" data-number="5.4.3">
<h4 data-number="5.4.3" class="anchored" data-anchor-id="偽情報対策"><span class="header-section-number">5.4.3</span> 偽情報対策</h4>
<p>生成 AI は，一国の政府が特定のプロパガンダを流布するための効果的な手段として選ばれることになる．その際の考え得る使用例と，それに対する対策が考えられてる <span class="citation" data-cites="Goldstein+2023">(Goldstein et al., 2023)</span>．</p>
</section>
<section id="開発規制" class="level4" data-number="5.4.4">
<h4 data-number="5.4.4" class="anchored" data-anchor-id="開発規制"><span class="header-section-number">5.4.4</span> 開発規制</h4>
<p><span class="citation" data-cites="Anderljung+2023">(Anderljung et al., 2023)</span> は先端的な AI を <a href="https://www.cnas.org/publications/commentary/frontier-ai-regulation-managing-emerging-risks-to-public-safety">Frontier AI</a> と呼び，これの開発過程におけるあるべき規制を模索している．監督当局に執行権を付与することやフロンティアAIモデルのライセンス制度などが議論されている．</p>
<p><span class="citation" data-cites="Shoker+2023">(Shoker et al., 2023)</span> は LLM と国家安全保障との関係を議論している．信頼構築措置 (CBMs: Confidence-Building Measures) とは，国家間の敵意を減少させることで，衝突のリスクを減らす措置の全般をいう．元々は冷戦時代に提案された概念であるが，これを LLM 開発に適用することが具体的に提案されている．</p>
</section>
<section id="生物学的脅威" class="level4" data-number="5.4.5">
<h4 data-number="5.4.5" class="anchored" data-anchor-id="生物学的脅威"><span class="header-section-number">5.4.5</span> 生物学的脅威</h4>
<p>LLM の登場により個人がエンパワーメントを受けており，生物学的脅威を作る障壁が低下していることは間違いない．</p>
<p><span class="citation" data-cites="Patwardhan+2024">(Patwardhan et al., 2024)</span> では，生物学的リスクに焦点を当てて，AI による安全リスク評価の手法と事前警鐘システムを模索している．この研究では，LLM によりリスクが増加するという統計的に有意義な証拠は得られていないが，この方面の研究の草分けとなっている．</p>
</section>
</section>
<section id="アラインメント問題" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="アラインメント問題"><span class="header-section-number">5.5</span> アラインメント問題</h3>
<p>DALL-E2 では訓練前の緩和策も取られている <span class="citation" data-cites="Nichol2022">(Nichol, 2022)</span>．</p>
<section id="プログラムの支援" class="level4" data-number="5.5.1">
<h4 data-number="5.5.1" class="anchored" data-anchor-id="プログラムの支援"><span class="header-section-number">5.5.1</span> プログラムの支援</h4>
<p>遺伝的プログラムの改良の過程を模倣できる <span class="citation" data-cites="Lehman+2024">(Lehman et al., 2024)</span> として，ソフトウェア開発やロボット開発分野での，プログラムの漸次的改良への応用が考えられている．</p>
<p>困難なタスクに対して AI がアシストするという研究もある <span class="citation" data-cites="Saunders+2022">(Saunders et al., 2022)</span>．これは最終的に，AI のアラインメントにおいても重要な技術になるとしている．</p>
</section>
<section id="超アラインメント問題" class="level4" data-number="5.5.2">
<h4 data-number="5.5.2" class="anchored" data-anchor-id="超アラインメント問題"><span class="header-section-number">5.5.2</span> 超アラインメント問題</h4>
<p>これは，OpenAI のアラインメント研究が次の３本の柱であることが背景にある <span class="citation" data-cites="Leike+2022">(Leike et al., 2022)</span></p>
<ol type="1">
<li>人間のフィードバックによる AI の強化学習</li>
<li>AI の支援を通じて人間のフィードバックを正確にする</li>
<li>AI を通じてアラインメントの研究を促進する</li>
</ol>
<p>の３つである．</p>
<p>第３の柱として，GPT-4 <span class="citation" data-cites="OpenAI2023">(OpenAI, 2023b)</span> によるシミュレーションを通じて，特定のニューロンがどのような出力に対応しているかを解明する手法を提案している <span class="citation" data-cites="Leike+2023">(Leike et al., 2023)</span>．これにより，人間が直接調べる行為が自動化され，アラインメントの研究が効率化され，スケーラブルな手法になるということである．</p>
<p>さらに，将来的なアラインメントは RLHF では出来なくなっていく．人智を超えた超知能 (superintelligence) をアラインメントすることを，超アラインメントと呼び，OpenAI は 2023 年の暮れに <a href="https://openai.com/blog/introducing-superalignment">超アラインメントチーム</a> を創設し，人間が「弱い監督者」となってしまった状況でもどのように超アラインメントを実行すれば良いかを研究するとしている．</p>
</section>
<section id="ai-エージェントへの道" class="level4" data-number="5.5.3">
<h4 data-number="5.5.3" class="anchored" data-anchor-id="ai-エージェントへの道"><span class="header-section-number">5.5.3</span> AI エージェントへの道</h4>
<p>OpenAI は能動的 AI システム (Agentic AI system) の構築に向けて，安全な運用と責任のある管理を目指す白書 <span class="citation" data-cites="Shavit+2023">(Shavit et al., 2023)</span> を発表した．</p>



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Aghajanyan+2021" class="csl-entry">
Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic dimensionality explains the effectiveness of language model fine-tuning. <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</em>, <em>1</em>, 7319–7328. <a href="https://aclanthology.org/2021.acl-long.568/">https://aclanthology.org/2021.acl-long.568/</a>
</div>
<div id="ref-Aghajanyan+2022" class="csl-entry">
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., &amp; Zettlemoyer, L. (2022). <em>CM3: A causal masked multimodal model of the internet</em>. <a href="https://arxiv.org/abs/2201.07520">https://arxiv.org/abs/2201.07520</a>
</div>
<div id="ref-Aharoni+2019" class="csl-entry">
Aharoni, R., Johnson, M., &amp; Firat, O. (2019). Massively multilingual neural machine translation. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.), <em>Proceedings of the 2019 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</em> (pp. 3874–3884). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1388">https://doi.org/10.18653/v1/N19-1388</a>
</div>
<div id="ref-Akyurek+2023" class="csl-entry">
Akyürek, E., Schuurmans, D., Andreas, J., Ma, T., &amp; Zhou, D. (2023). ​​What learning algorithm is in-context learning? Investigations with linear models. <em>The Eleventh International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=0g0X4H8yN4I">https://openreview.net/forum?id=0g0X4H8yN4I</a>
</div>
<div id="ref-Alayrac+2022" class="csl-entry">
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., … Simonyan, K. (2022). <em>Flamingo: A visual language model for few-shot learning</em>. <a href="https://arxiv.org/abs/2204.14198">https://arxiv.org/abs/2204.14198</a>
</div>
<div id="ref-Anderljung+2023" class="csl-entry">
Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., … Wolf, K. (2023). <em>Frontier AI regulation: Managing emerging risks to public safety</em>. OpenAI. <a href="https://openai.com/research/frontier-ai-regulation">https://openai.com/research/frontier-ai-regulation</a>
</div>
<div id="ref-Andreas2022" class="csl-entry">
Andreas, J. (2022). <em>Language models as agent models</em>. <a href="https://arxiv.org/abs/2212.01681">https://arxiv.org/abs/2212.01681</a>
</div>
<div id="ref-Arnab+2021" class="csl-entry">
Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., &amp; Schmid, C. (2021). ViViT: A video vision transformer. <em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 6816–6826. <a href="https://doi.org/10.1109/ICCV48922.2021.00676">https://doi.org/10.1109/ICCV48922.2021.00676</a>
</div>
<div id="ref-Ba+2016" class="csl-entry">
Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016). <em>Layer normalization</em>. <a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>
</div>
<div id="ref-Bahdanau+2015" class="csl-entry">
Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). <em>Neural machine translation by jointly learning to align and translate</em>. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>
</div>
<div id="ref-Bai+2023" class="csl-entry">
Bai, Y., Chen, F., Wang, H., Xiong, C., &amp; Mei, S. (2023). Transformers as statisticians: Provable in-context learning with in-context algorithm selection. <em>Workshop on Efficient Systems for Foundation Models @ ICML2023</em>. <a href="https://openreview.net/forum?id=vlCG5HKEkI">https://openreview.net/forum?id=vlCG5HKEkI</a>
</div>
<div id="ref-Baker+2022" class="csl-entry">
Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., &amp; Clune, J. (2022). <em>Video PreTraining (VPT): Learning to act by watching unlabeled online videos</em>. <a href="https://arxiv.org/abs/2206.11795">https://arxiv.org/abs/2206.11795</a>
</div>
<div id="ref-Bengio+2000" class="csl-entry">
Bengio, Y., Ducharme, R., &amp; Vincent, P. (2000). A neural probabilistic language model. <em>Advances in Neural Information Processing Systems</em>, <em>13</em>. <a href="https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html">https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html</a>
</div>
<div id="ref-Bjorck+2018" class="csl-entry">
Bjorck, N., Gomes, C. P., Selman, B., &amp; Weinberger, K. Q. (2018). Understanding batch normalization. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>. <a href="https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html">https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html</a>
</div>
<div id="ref-Bommasani+2021" class="csl-entry">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., Arx, S. von, Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K. A., Davis, J., Demszky, D., … Liang, P. (2021). On the opportunities and risks of foundation models. <em>ArXiv</em>. <a href="https://crfm.stanford.edu/report.html">https://crfm.stanford.edu/report.html</a>
</div>
<div id="ref-Brooks+2024" class="csl-entry">
Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C. W. Y., Wang, R., &amp; Ramesh, A. (2024). <em>Video generation models as world simulators</em>. OpenAI. <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a>
</div>
<div id="ref-Brown+2020" class="csl-entry">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., … Amodei, D. (2020). Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 1877–1901. <a href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>
</div>
<div id="ref-Bubeck+2023" class="csl-entry">
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., &amp; Zhang, Y. (2023). <em>Sparks of artificial general intelligence: Early experiments with GPT-4</em>. <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>
</div>
<div id="ref-Chang+2023" class="csl-entry">
Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K. P., Freeman, W. T., Rubinstein, M., Li, Y., &amp; Krishnan, D. (2023). Muse: Text-to-image generation via masked generative transformers. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, &amp; J. Scarlett (Eds.), <em>Proceedings of the 40th international conference on machine learning</em> (Vol. 202, pp. 4055–4075). PMLR. <a href="https://proceedings.mlr.press/v202/chang23b.html">https://proceedings.mlr.press/v202/chang23b.html</a>
</div>
<div id="ref-Chen+2020" class="csl-entry">
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., &amp; Sutskever, I. (2020). Generative pretraining from pixels. <em>Proceedings of the 37th International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v119/chen20s.html">https://proceedings.mlr.press/v119/chen20s.html</a>
</div>
<div id="ref-Chen+2021" class="csl-entry">
Chen, M., Tworek, J., Jun, H., Yuan, Q., Oliveira Pinto, H. P. de, Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). <em>Evaluating large language models trained on code</em>. <a href="https://arxiv.org/abs/2107.03374">https://arxiv.org/abs/2107.03374</a>
</div>
<div id="ref-Child+2019" class="csl-entry">
Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). <em>Generating long sequences with sparse transformers</em>. <a href="https://arxiv.org/abs/1904.10509">https://arxiv.org/abs/1904.10509</a>
</div>
<div id="ref-Cho+2014" class="csl-entry">
Cho, K., Merriënboer, B. van, Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using <span>RNN</span> encoder<span>–</span>decoder for statistical machine translation. In A. Moschitti, B. Pang, &amp; W. Daelemans (Eds.), <em>Proceedings of the 2014 conference on empirical methods in natural language processing (<span>EMNLP</span>)</em> (pp. 1724–1734). Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1179">https://doi.org/10.3115/v1/D14-1179</a>
</div>
<div id="ref-Chowdhery+2022" class="csl-entry">
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). <em>PaLM: Scaling language modeling with pathways</em>. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>
</div>
<div id="ref-Christiano+2017" class="csl-entry">
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>
</div>
<div id="ref-Dai-Le2015" class="csl-entry">
Dai, A. M., &amp; Le, Q. V. (2015). <em>Semi-supervised sequence learning</em>. <a href="https://arxiv.org/abs/1511.01432">https://arxiv.org/abs/1511.01432</a>
</div>
<div id="ref-Dehghani+2023" class="csl-entry">
Dehghani, M., Mustafa, B., Djolonga, J., Heek, J., Minderer, M., Caron, M., Steiner, A. P., Puigcerver, J., Geirhos, R., Alabdulmohsin, I., Oliver, A., Padlewski, P., Gritsenko, A. A., Lucic, M., &amp; Houlsby, N. (2023). Patch n<span>’</span> pack: NaViT, a vision transformer for any aspect ratio and resolution. <em>Thirty-Seventh Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=VpGFHmI7e5">https://openreview.net/forum?id=VpGFHmI7e5</a>
</div>
<div id="ref-Devlin+2019" class="csl-entry">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, <em>1</em>, 4171–4186. <a href="https://aclanthology.org/N19-1423/">https://aclanthology.org/N19-1423/</a>
</div>
<div id="ref-Dhariwal+2020" class="csl-entry">
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., &amp; Sutskever, I. (2020). <em>Jukebox: A generative model for music</em>. <a href="https://arxiv.org/abs/2005.00341">https://arxiv.org/abs/2005.00341</a>
</div>
<div id="ref-Dosovitskiy+2021" class="csl-entry">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp; Houlsby, N. (2021). An image is worth 16×16 words: Transformers for image recognition at scale. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a>
</div>
<div id="ref-Dufter+2021" class="csl-entry">
Dufter, P., Schmitt, M., &amp; Schütze, H. (2021). <em>Position information in transformers: An overview</em>. <a href="https://arxiv.org/abs/2102.11090">https://arxiv.org/abs/2102.11090</a>
</div>
<div id="ref-Eloundou+2023" class="csl-entry">
Eloundou, T., Manning, S., Mishkin, P., &amp; Rock, D. (2023). <em>GPTs are GPTs: An early look at the labor market impact potential of large language models</em>. <a href="https://openai.com/research/gpts-are-gpts">https://openai.com/research/gpts-are-gpts</a>
</div>
<div id="ref-Fedus+2022" class="csl-entry">
Fedus, W., Zoph, B., &amp; Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. <em>The Journal of Machine Learning Research</em>, <em>23</em>(120), 1–39. <a href="https://jmlr.org/papers/v23/21-0998.html">https://jmlr.org/papers/v23/21-0998.html</a>
</div>
<div id="ref-Fu+2023" class="csl-entry">
Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., &amp; Re, C. (2023). Hungry hungry hippos: Towards language modeling with state space models. <em>The Eleventh International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=COZDy0WYGg">https://openreview.net/forum?id=COZDy0WYGg</a>
</div>
<div id="ref-Garg+2022" class="csl-entry">
Garg, S., Tsipras, D., Liang, P., &amp; Valiant, G. (2022). What can transformers learn in-context? A case study of simple function classes. In A. H. Oh, A. Agarwal, D. Belgrave, &amp; K. Cho (Eds.), <em>Advances in neural information processing systems</em>. <a href="https://openreview.net/forum?id=flNZJ2eOet">https://openreview.net/forum?id=flNZJ2eOet</a>
</div>
<div id="ref-Goldstein+2023" class="csl-entry">
Goldstein, J. A., Sastry, G., Musser, M., DiResta, R., Gentzel, M., &amp; Sedova, K. (2023). <em>Generative language models and automated influence operations: Emerging threats and potential mitigations</em>. <a href="https://openai.com/research/forecasting-misuse">https://openai.com/research/forecasting-misuse</a>
</div>
<div id="ref-Gu-Dao2024" class="csl-entry">
Gu, A., &amp; Dao, T. (2024). <em>Mamba: Linear-time sequence modeling with selective state spaces</em>. <a href="https://openreview.net/forum?id=AL1fq05o7H">https://openreview.net/forum?id=AL1fq05o7H</a>
</div>
<div id="ref-Gu+2022" class="csl-entry">
Gu, A., Goel, K., &amp; Re, C. (2022). Efficiently modeling long sequences with structured state spaces. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=uYLFoz1vlAC">https://openreview.net/forum?id=uYLFoz1vlAC</a>
</div>
<div id="ref-Guo+2024" class="csl-entry">
Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., &amp; Bai, Y. (2024). How do transformers learn in-context beyond simple functions? A case study on learning with representations. <em>The Twelfth International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=ikwEDva1JZ">https://openreview.net/forum?id=ikwEDva1JZ</a>
</div>
<div id="ref-Guu+2020" class="csl-entry">
Guu, K., Lee, K., Tung, Z., Pasupat, P., &amp; Chang, M.-W. (2020). REALM: Retrieval-augmented language model pre-training. <em>Proceedings of the 37th International Conference on Machine Learning</em>.
</div>
<div id="ref-Ha-Schmidthuber2018" class="csl-entry">
Ha, D., &amp; Schmidhuber, J. (2018). Recurrent world models facilitate policy evaluation. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>. <a href="https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html">https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html</a>
</div>
<div id="ref-Hafner+2021" class="csl-entry">
Hafner, D., Lillicrap, T. P., Norouzi, M., &amp; Ba, J. (2021). Mastering atari with discrete world models. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=0oabwyZbOu">https://openreview.net/forum?id=0oabwyZbOu</a>
</div>
<div id="ref-Hashimoto2024" class="csl-entry">
Hashimoto, T. (2024). <em>Large language models</em>. Lecture at MLSS2024.
</div>
<div id="ref-Hestness+2017" class="csl-entry">
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, Md. M. A., Yang, Y., &amp; Zhou, Y. (2017). <em>Deep learning scaling is predictable, empirically</em>. <a href="https://arxiv.org/abs/1712.00409">https://arxiv.org/abs/1712.00409</a>
</div>
<div id="ref-Hochreiter-Schmidhuber1997" class="csl-entry">
Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-time memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780. <a href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext</a>
</div>
<div id="ref-Hoffmann+2022" class="csl-entry">
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Las Casas, D. de, Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). <em>Training compute-optimal large language models</em>. <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>
</div>
<div id="ref-Holtzman+2020" class="csl-entry">
Holtzman, A., Buys, J., Du, L., Forbes, M., &amp; Choi, Y. (2020). The curious case of neural text degeneration. <em>International Conference on Learning Representations</em>. <a href="https://arxiv.org/abs/1904.09751">https://arxiv.org/abs/1904.09751</a>
</div>
<div id="ref-Hu+2023" class="csl-entry">
Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., &amp; Corrado, G. (2023). <em>GAIA-1: A generative world model for autonomous driving</em>. <a href="https://arxiv.org/abs/2309.17080">https://arxiv.org/abs/2309.17080</a>
</div>
<div id="ref-Hu+2021" class="csl-entry">
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). <em>LoRA: Low-rank adaptation of large language models</em>. <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>
</div>
<div id="ref-Ioffe-Szegedy2015" class="csl-entry">
Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. <em>Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37</em>, 448–456. <a href="https://dl.acm.org/doi/10.5555/3045118.3045167">https://dl.acm.org/doi/10.5555/3045118.3045167</a>
</div>
<div id="ref-Jaegle+2021" class="csl-entry">
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., &amp; Carreira, J. (2021). Perceiver: General perception with iterative attention. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 4651–4664). PMLR. <a href="https://proceedings.mlr.press/v139/jaegle21a.html">https://proceedings.mlr.press/v139/jaegle21a.html</a>
</div>
<div id="ref-Jiang+2024" class="csl-entry">
Jiang, Z., Lin, H., Zhong, Y., Huang, Q., Chen, Y., Zhang, Z., Peng, Y., Li, X., Xie, C., Nong, S., Jia, Y., He, S., Chen, H., Bai, Z., Hou, Q., Yan, S., Zhou, D., Sheng, Y., Jiang, Z., … Liu, X. (2024). <em>MegaScale: Scaling large language model training to more than 10,000 GPUs</em>. <a href="https://arxiv.org/abs/2402.15627">https://arxiv.org/abs/2402.15627</a>
</div>
<div id="ref-Kaiser+2020" class="csl-entry">
Kaiser, Ł., Babaeizadeh, M., Miłos, P., Osiński, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., &amp; Michalewski, H. (2020). Model based reinforcement learning for atari. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=S1xCPJHtDB">https://openreview.net/forum?id=S1xCPJHtDB</a>
</div>
<div id="ref-Kaplan+2020" class="csl-entry">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020). <em>Scaling laws for neural language models</em>. <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a>
</div>
<div id="ref-Karpukhin+2020" class="csl-entry">
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., &amp; Yih, W. (2020). Dense passage retrieval for open-domain question answering. In B. Webber, T. Cohn, Y. He, &amp; Y. Liu (Eds.), <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 6769–6781). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.550">https://doi.org/10.18653/v1/2020.emnlp-main.550</a>
</div>
<div id="ref-Khlaaf+2022" class="csl-entry">
Khlaaf, H., Mishkin, P., Achiam, J., Krueger, G., &amp; Brundage, M. (2022). <em>A hazard analysis framework for code synthesis large language models</em>. <a href="https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models">https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models</a>
</div>
<div id="ref-Kim-Suzuki2024" class="csl-entry">
Kim, J., &amp; Suzuki, T. (2024). <em>Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape</em>. <a href="https://arxiv.org/abs/2402.01258">https://arxiv.org/abs/2402.01258</a>
</div>
<div id="ref-Kuditipudi+2023" class="csl-entry">
Kuditipudi, R., Thickstun, J., Hashimoto, T., &amp; Liang, P. (2023). <em>Robust distortion-free watermarks for language models</em>. <a href="https://arxiv.org/abs/2402.10978">https://arxiv.org/abs/2402.10978</a>
</div>
<div id="ref-LeCun+2012" class="csl-entry">
LeCun, Y. A., Bottou, L., Orr, G. B., &amp; Müller, K.-R. (2012). <em>Neural networks: Tricks of the trade</em> (G. Montavon, G. B. Orr, &amp; K.-R. Müller, Eds.; 2nd ed., pp. 9–48). Springer Berlin, Heidelberg. <a href="https://link.springer.com/book/10.1007/978-3-642-35289-8">https://link.springer.com/book/10.1007/978-3-642-35289-8</a>
</div>
<div id="ref-Lehman+2024" class="csl-entry">
Lehman, J., Jain, J. G. S., Ndousse, K., Yah, C., &amp; Stanley, K. O. (2024). <em>Handbook of evolutionary machine learning</em> (W. Banzhaf, P. Machado, &amp; M. Zhang, Eds.; pp. 331–366). Springer Singapore. <a href="https://link.springer.com/chapter/10.1007/978-981-99-3814-8_11">https://link.springer.com/chapter/10.1007/978-981-99-3814-8_11</a>
</div>
<div id="ref-Leike+2022" class="csl-entry">
Leike, J., Schulman, J., &amp; Wu, J. (2022). <em>Our approach to alignment research</em>. OpenAI. <a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a>
</div>
<div id="ref-Leike+2023" class="csl-entry">
Leike, J., Wu, J., Bills, S., Saunders, W., Gao, L., Tillman, H., &amp; Mossing, D. (2023). <em>Language models can explain neurons in language models</em>. OpenAI. <a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models">https://openai.com/research/language-models-can-explain-neurons-in-language-models</a>
</div>
<div id="ref-Leng-Yuan2023" class="csl-entry">
Leng, Y., &amp; Yuan, Y. (2023). <em>Do LLM agents exhibit social behavior?</em> <a href="https://arxiv.org/abs/2312.15198">https://arxiv.org/abs/2312.15198</a>
</div>
<div id="ref-Lepikhin+2021" class="csl-entry">
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., &amp; Chen, Z. (2021). GShard: Scaling giant models with conditional computation and automatic sharding. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=qrwe7XHTmYb">https://openreview.net/forum?id=qrwe7XHTmYb</a>
</div>
<div id="ref-Lewis+2020-BART" class="csl-entry">
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., &amp; Zettlemoyer, L. (2020). <span>BART</span>: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In D. Jurafsky, J. Chai, N. Schluter, &amp; J. Tetreault (Eds.), <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em> (pp. 7871–7880). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a>
</div>
<div id="ref-Lewis+2020" class="csl-entry">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 9459–9474. <a href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>
</div>
<div id="ref-Lieber+2021" class="csl-entry">
Lieber, O., Sharir, O., Lenz, B., &amp; Shoham, Y. (2021). <em>Jurrassic-1: Technical details and evaluation</em>. AI21. <a href="https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1">https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1</a>
</div>
<div id="ref-Lin+2022" class="csl-entry">
Lin, S., Hilton, J., &amp; Evans, O. (2022). Teaching models to express their uncertainty in words. <em>Transactions on Machine Learning Research</em>. <a href="https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words">https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words</a>
</div>
<div id="ref-Liu+2023" class="csl-entry">
Liu, X., Gong, C., &amp; liu, qiang. (2023). Flow straight and fast: Learning to generate and transfer data with rectified flow. <em>The Eleventh International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=XVjTT1nw5z">https://openreview.net/forum?id=XVjTT1nw5z</a>
</div>
<div id="ref-Ma+2024" class="csl-entry">
Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., &amp; Xie, S. (2024). <em>SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers</em>. <a href="https://arxiv.org/abs/2401.08740">https://arxiv.org/abs/2401.08740</a>
</div>
<div id="ref-Manning+2022" class="csl-entry">
Manning, S., Mishkin, P., Hadfield, G., Eloundou, T., &amp; Eisne, E. (2022). <em>A research agenda for assessing the economic impacts of code generation models</em>. OpenAI. <a href="https://openai.com/research/economic-impacts">https://openai.com/research/economic-impacts</a>
</div>
<div id="ref-Marcus2020" class="csl-entry">
Marcus, G. (2020). <em>The next decade in AI: Four steps towards robust artificial intelligence</em>. <a href="https://arxiv.org/abs/2002.06177">https://arxiv.org/abs/2002.06177</a>
</div>
<div id="ref-Micheli+2023" class="csl-entry">
Micheli, V., Alonso, E., &amp; Fleuret, F. (2023). Transformers are sample-efficient world models. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=vhFu1Acb0xb">https://openreview.net/forum?id=vhFu1Acb0xb</a>
</div>
<div id="ref-Mikolov2013" class="csl-entry">
Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Efficient estimation of word representations in vector space</em>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>
</div>
<div id="ref-Mikolov+2010" class="csl-entry">
Mikolov, T., Kopecky, J., Burget, L., C̆ernocky, J., &amp; Khudanpur, S. (2010). Recurrent neural network based language model. <em>Proceedings of Interspeech</em>. <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf</a>
</div>
<div id="ref-Mohri-Hashimoto2024" class="csl-entry">
Mohri, C., &amp; Hashimoto, T. (2024). <em>Language models with conformal factuality guarantees</em>. <a href="https://arxiv.org/abs/2402.10978">https://arxiv.org/abs/2402.10978</a>
</div>
<div id="ref-Nakano+2022" class="csl-entry">
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., &amp; Schulman, J. (2022). <em>WebGPT: Browser-assisted question-answering with human feedback</em>. <a href="https://arxiv.org/abs/2112.09332">https://arxiv.org/abs/2112.09332</a>
</div>
<div id="ref-Nichol2022" class="csl-entry">
Nichol, A. (2022). <em>DALL-E2 pre-training mitigations</em>. OpenAI. <a href="https://openai.com/research/dall-e-2-pre-training-mitigations">https://openai.com/research/dall-e-2-pre-training-mitigations</a>
</div>
<div id="ref-OpenAI2023DallE3" class="csl-entry">
OpenAI. (2023a). <em>DALL-E3 system card</em>. OpenAI. <a href="https://openai.com/research/dall-e-3-system-card">https://openai.com/research/dall-e-3-system-card</a>
</div>
<div id="ref-OpenAI2023" class="csl-entry">
OpenAI. (2023b). <em>GPT-4 technical report</em>. <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a>
</div>
<div id="ref-OpenAI2023-GPT4V" class="csl-entry">
OpenAI. (2023c). <em>GPT-4V(ision) system card</em>. OpenAI. <a href="https://openai.com/research/gpt-4v-system-card">https://openai.com/research/gpt-4v-system-card</a>
</div>
<div id="ref-Ouyang+2022" class="csl-entry">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, &amp; A. Oh (Eds.), <em>Advances in neural information processing systems</em> (Vol. 35, pp. 27730–27744). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf</a>
</div>
<div id="ref-Patwardhan+2024" class="csl-entry">
Patwardhan, T., Liu, K., Markov, T., Chowdhury, N., Leet, D., Cone, N., Maltbie, C., Huizinga, J., Wainwright, C., Jackson, S. (Froggi), Adler, S., Casagrande, R., &amp; Mandry, A. (2024). <em>Building an early warning system for LLM-aided biological threat creation</em>. OpenAI. <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</a>
</div>
<div id="ref-Peebles-Xie2023" class="csl-entry">
Peebles, W., &amp; Xie, S. (2023). Scalable diffusion models with transformers. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 4195–4205. <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html</a>
</div>
<div id="ref-Perot+2023" class="csl-entry">
Perot, V., Kang, K., Luisier, F., Su, G., Sun, X., Boppana, R. S., Wang, Z., Mu, J., Zhang, H., &amp; Hua, N. (2023). <em>LMDX: Language model-based document information extraction and localization</em>. <a href="https://arxiv.org/abs/2309.10952">https://arxiv.org/abs/2309.10952</a>
</div>
<div id="ref-Petroni+2019" class="csl-entry">
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., &amp; Miller, A. (2019). Language models as knowledge bases? In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em> (pp. 2463–2473). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1250">https://doi.org/10.18653/v1/D19-1250</a>
</div>
<div id="ref-Racaniere+2017" class="csl-entry">
Racanière, S., Weber, T., Reichert, D. P., Buesing, L., Guez, A., Rezende, D., Badia, A. P., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia, P., Hassabis, D., Silver, D., &amp; Wierstra, D. (2017). Imagination-augmented agents for deep reinforcement learning. <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 5694–5705. <a href="https://dl.acm.org/doi/10.5555/3295222.3295320">https://dl.acm.org/doi/10.5555/3295222.3295320</a>
</div>
<div id="ref-Radford+2021" class="csl-entry">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8748–8763). PMLR. <a href="https://proceedings.mlr.press/v139/radford21a.html">https://proceedings.mlr.press/v139/radford21a.html</a>
</div>
<div id="ref-Radford+2023" class="csl-entry">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2023a). Robust speech recognition via large-scale weak supervision. <em>Proceedings of the 40th International Conference on Machine Learning</em>. <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">https://dl.acm.org/doi/10.5555/3618408.3619590</a>
</div>
<div id="ref-Radford+2023-Whisper" class="csl-entry">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2023b). Robust speech recognition via large-scale weak supervision. <em>Proceedings of the 40th International Conference on Machine Learning</em>. <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">https://dl.acm.org/doi/10.5555/3618408.3619590</a>
</div>
<div id="ref-Radford+2018" class="csl-entry">
Radford, A., narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <em>Improving language understanding with unsupervised learning</em>. OpenAI. <a href="https://openai.com/research/language-unsupervised">https://openai.com/research/language-unsupervised</a>
</div>
<div id="ref-Radford+2019" class="csl-entry">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <em>Language models are unsupervised multitask learners</em>. <a href="https://github.com/openai/gpt-2?tab=readme-ov-file">https://github.com/openai/gpt-2?tab=readme-ov-file</a>
</div>
<div id="ref-Rae+2021" class="csl-entry">
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks, L. A., Rauh, M., Huang, P.-S., … Irving, G. (2021). <em>Scaling language models: Methods, analysis &amp; insights from training gopher</em>. Google DeepMind. <a href="https://arxiv.org/abs/2112.11446">https://arxiv.org/abs/2112.11446</a>
</div>
<div id="ref-Raffel+2020" class="csl-entry">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. <em>J. Mach. Learn. Res.</em>, <em>21</em>(1).
</div>
<div id="ref-Rakhimov+2020" class="csl-entry">
Rakhimov, R., Volkhonskiy, D., Artemov, A., Zorin, D., &amp; Burnaev, E. (2020). <em>Latent video transformer</em>. <a href="https://arxiv.org/abs/2006.10704">https://arxiv.org/abs/2006.10704</a>
</div>
<div id="ref-Ramesh+2022" class="csl-entry">
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). <em>Hierarchical text-conditional image generation with CLIP latents</em>. <a href="https://arxiv.org/abs/2204.06125">https://arxiv.org/abs/2204.06125</a>
</div>
<div id="ref-Ramesh+2021" class="csl-entry">
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. (2021). Zero-shot text-to-image generation. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8821–8831). PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>
</div>
<div id="ref-Reed+2016" class="csl-entry">
Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., &amp; Lee, H. (2016). Generative adversarial text to image synthesis. In M. F. Balcan &amp; K. Q. Weinberger (Eds.), <em>Proceedings of the 33rd international conference on machine learning</em> (Vol. 48, pp. 1060–1069). PMLR. <a href="https://proceedings.mlr.press/v48/reed16.html">https://proceedings.mlr.press/v48/reed16.html</a>
</div>
<div id="ref-Roberts+2020" class="csl-entry">
Roberts, A., Raffel, C., &amp; Shazeer, N. (2020). How much knowledge can you pack into the parameters of a language model? In B. Webber, T. Cohn, Y. He, &amp; Y. Liu (Eds.), <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 5418–5426). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.437">https://doi.org/10.18653/v1/2020.emnlp-main.437</a>
</div>
<div id="ref-Rombach+2022" class="csl-entry">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-resolution image systhesis with latent diffusion models. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10684–10695. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html</a>
</div>
<div id="ref-Ronneberger+2015" class="csl-entry">
Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In N. Navab, J. Hornegger, W. M. Wells, &amp; A. F. Frangi (Eds.), <em>Medical image computing and computer-assisted intervention – MICCAI 2015</em> (pp. 234–241). Springer International Publishing. <a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28">https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28</a>
</div>
<div id="ref-Saunders+2022" class="csl-entry">
Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., &amp; Leike, J. (2022). <em>Self-critiquing models for assisting human evaluators</em>. <a href="https://openai.com/research/critiques">https://openai.com/research/critiques</a>
</div>
<div id="ref-Schulman+2015" class="csl-entry">
Schulman, J., Levine, S., Moritz, P., Jordan, M. I., &amp; Abbeel, P. (2015). <em>Trust region policy optimization</em>. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a>
</div>
<div id="ref-Schulman+2017" class="csl-entry">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). <em>Proximal policy optimization algorithms</em>. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>
</div>
<div id="ref-Sennrich+2016" class="csl-entry">
Sennrich, R., Haddow, B., &amp; Birch, A. (2016). Neural machine translation of rare words with subword units. <em>Proceedings of the 54th Annual Meetings of the Association for Computational Linguistics</em>, <em>1</em>, 1715–1725. <a href="https://aclanthology.org/P16-1162/">https://aclanthology.org/P16-1162/</a>
</div>
<div id="ref-Shavit+2023" class="csl-entry">
Shavit, Y., Agarwal, S., &amp; Brundage, M. (2023). <em>Practices for governing agentic AI systems</em>. OpenAI. <a href="https://openai.com/research/practices-for-governing-agentic-ai-systems">https://openai.com/research/practices-for-governing-agentic-ai-systems</a>
</div>
<div id="ref-Shazeer+2017" class="csl-entry">
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., &amp; Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=B1ckMDqlg">https://openreview.net/forum?id=B1ckMDqlg</a>
</div>
<div id="ref-Shoker+2023" class="csl-entry">
Shoker, S., Reddie, A., Barrington, S., Booth, R., Brundage, M., Chahal, H., Depp, M., Drexel, B., Gupta, R., Favaro, M., Hecla, J., Hickey, A., Konaev, M., Kumar, K., Lambert, N., Lohn, A., O’Keefe, C., Rajani, N., Sellitto, M., … Young, J. (2023). <em>Confidence-building measures for artificial intelligence: Workshop proceedings</em>. <a href="https://openai.com/research/confidence-building-measures-for-artificial-intelligence">https://openai.com/research/confidence-building-measures-for-artificial-intelligence</a>
</div>
<div id="ref-Sutton2019" class="csl-entry">
Sutton, R. (2019). <em>The bitter lesson</em>. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>
</div>
<div id="ref-Sutton-Barto2018" class="csl-entry">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (2nd ed.). MIT Press. <a href="https://mitpress.mit.edu/9780262352703/reinforcement-learning/">https://mitpress.mit.edu/9780262352703/reinforcement-learning/</a>
</div>
<div id="ref-Tamkin+2021" class="csl-entry">
Tamkin, A., Brundage, M., Clark, J., &amp; Ganguli, D. (2021). <em>Understanding the capabilities, limitations, and societal impact of large language models</em>. <a href="https://arxiv.org/abs/2102.02503">https://arxiv.org/abs/2102.02503</a>
</div>
<div id="ref-Geminiteam+2023" class="csl-entry">
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). <em>Gemini: A family of highly capable multimodal models</em>. <a href="https://arxiv.org/abs/2312.11805">https://arxiv.org/abs/2312.11805</a>
</div>
<div id="ref-Thoppilan+2022" class="csl-entry">
Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., … Le, Q. (2022). <em>LaMDA: Language models for dialog applications</em>. <a href="https://arxiv.org/abs/2201.08239">https://arxiv.org/abs/2201.08239</a>
</div>
<div id="ref-Touvron+2023" class="csl-entry">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). <em>LLaMA: Open and efficient foundation language models</em>. <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a>
</div>
<div id="ref-vandenOord+2016" class="csl-entry">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., &amp; Kavukcuoglu, K. (2016). Pixel recurrent neural networks. <em>Proceedings of the 33rd International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v48/oord16.html">https://proceedings.mlr.press/v48/oord16.html</a>
</div>
<div id="ref-vandenOord+2016b" class="csl-entry">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &amp; Kavukcuoglu, K. (2016). Conditional image generation with PixelCNN decoders. <em>Proceedings of the 30th International Conference on Neural Information Processing Systems</em>, 4797–4805. <a href="https://dl.acm.org/doi/10.5555/3157382.3157633">https://dl.acm.org/doi/10.5555/3157382.3157633</a>
</div>
<div id="ref-Vaswani+2017" class="csl-entry">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>
</div>
<div id="ref-vanOswald+2023" class="csl-entry">
von&nbsp;Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., &amp; Vladymyrov, M. (2023). <em>Transformers learn in-context by gradient descent</em>. 35151–35174. <a href="https://research.google/pubs/transformers-learn-in-context-by-gradient-descent/">https://research.google/pubs/transformers-learn-in-context-by-gradient-descent/</a>
</div>
<div id="ref-Wang+2023" class="csl-entry">
Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., &amp; Yu, P. S. (2023). Generalizing to unseen domains: A survey on domain generalization. <em>IEEE Transactions on Knowledge &amp;Amp; Data Engineering</em>, <em>35</em>(08), 8052–8072. <a href="https://doi.org/10.1109/TKDE.2022.3178128">https://doi.org/10.1109/TKDE.2022.3178128</a>
</div>
<div id="ref-Weng-Brockman2022" class="csl-entry">
Weng, L., &amp; Brockman, G. (2022). <em>Techniques for training large neural networks</em>. OpenAI. <a href="https://openai.com/research/techniques-for-training-large-neural-networks">https://openai.com/research/techniques-for-training-large-neural-networks</a>
</div>
<div id="ref-Yan+2021" class="csl-entry">
Yan, W., Zhang, Y., Abbeel, P., &amp; Srinivas, A. (2021). <em>VideoGPT: Video generation using VQ-VAE and transformers</em>. <a href="https://arxiv.org/abs/2104.10157">https://arxiv.org/abs/2104.10157</a>
</div>
<div id="ref-Yang+2023" class="csl-entry">
Yang, R., Srivastava, P., &amp; Mandt, S. (2023). Diffusion probabilistic modeling for video generation. <em>Entropy</em>, <em>25</em>(10). <a href="https://www.mdpi.com/1099-4300/25/10/1469">https://www.mdpi.com/1099-4300/25/10/1469</a>
</div>
<div id="ref-Yasunaga+2023" class="csl-entry">
Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &amp; Yih, W.-T. (2023). Retrieval-augmented multimodal language modeling. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, &amp; J. Scarlett (Eds.), <em>Proceedings of the 40th international conference on machine learning</em> (Vol. 202, pp. 39755–39769). PMLR. <a href="https://proceedings.mlr.press/v202/yasunaga23a.html">https://proceedings.mlr.press/v202/yasunaga23a.html</a>
</div>
<div id="ref-Yu+2022" class="csl-entry">
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., &amp; Wu, Y. (2022). Scaling autoregressive models for content-rich text-to-image generation. <em>Transactions on Machine Learning Research</em>. <a href="https://openreview.net/forum?id=AFDcYJKhND">https://openreview.net/forum?id=AFDcYJKhND</a>
</div>
<div id="ref-Yu+2023" class="csl-entry">
Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., Ross, C., Polyak, A., Howes, R., Sharma, V., Xu, P., Tamoyan, H., Ashual, O., Singer, U., Li, S.-W., … Aghajanyan, A. (2023). <em>Scaling autoregressive multi-modal models: Pretraining and instruction tuning</em>. <a href="https://arxiv.org/abs/2309.02591">https://arxiv.org/abs/2309.02591</a>
</div>
<div id="ref-Zhao+2023" class="csl-entry">
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). <em>A survey of large language models</em>. <a href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a>
</div>
<div id="ref-Zheng+2023" class="csl-entry">
Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., Xiong, L., Chen, L., Xi, Z., Xu, N., Lai, W., Zhu, M., Chang, C., Yin, Z., Weng, R., … Huang, X. (2023). <em>Secrets of RLHF in large language models part i: PPO</em>. <a href="https://arxiv.org/abs/2307.04964">https://arxiv.org/abs/2307.04964</a>
</div>
<div id="ref-Zhou+2023" class="csl-entry">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., YU, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., &amp; Levy, O. (2023). <span>LIMA</span>: Less is more for alignment. <em>Thirty-Seventh Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=KBMOKmX2he">https://openreview.net/forum?id=KBMOKmX2he</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>層の数などの表現は <span class="citation" data-cites="Hashimoto2024">(Hashimoto, 2024)</span> から．↩︎</p></li>
<li id="fn2"><p><span class="citation" data-cites="Hashimoto2024">(Hashimoto, 2024)</span> で聞きました．↩︎</p></li>
<li id="fn3"><p><span class="citation" data-cites="Hashimoto2024">(Hashimoto, 2024)</span> で聞きました．↩︎</p></li>
<li id="fn4"><p>もちろん結合することも考えられているが，加算を行うことが現状の多数派であるようである <span class="citation" data-cites="Hashimoto2024">(Hashimoto, 2024)</span>．↩︎</p></li>
<li id="fn5"><p>勾配の爆発に対しては gradient clipping などの対症療法が用いられる．↩︎</p></li>
<li id="fn6"><p>後述のアラインメントも事後調整の一つであるが，これと区別して，<strong>教師ありの事後調整</strong> (supervised fine-tuning) とも呼ばれる．↩︎</p></li>
<li id="fn7"><p>パラメトリックな知識ベースとしての利用については <span class="citation" data-cites="Raffel+2020">(Raffel et al., 2020)</span>，<span class="citation" data-cites="Roberts+2020">(Roberts et al., 2020)</span> など．一方で <span class="citation" data-cites="Marcus2020">(Marcus, 2020)</span> などは，hallucination などの欠点を補う形で，古典的な知識ベースと連結したハイブリット型での使用を提案している．↩︎</p></li>
<li id="fn8"><p>モデルの比較は <span class="citation" data-cites="Raffel+2020">(Raffel et al., 2020)</span> などが行っている．↩︎</p></li>
<li id="fn9"><p>トークン化に小規模な CNN を用いてデータ圧縮を行うこともある．↩︎</p></li>
<li id="fn10"><p>すなわち，DALL-E は GPT-3 のマルチモーダルな実装である <span class="citation" data-cites="Tamkin+2021">(Tamkin et al., 2021, p. 4)</span>．↩︎</p></li>
<li id="fn11"><p><span class="citation" data-cites="Ha-Schmidthuber2018">(Ha &amp; Schmidhuber, 2018)</span> は RNN により世界モデルを構築している．<span class="citation" data-cites="Kaiser+2020">(Kaiser et al., 2020)</span> は動画から Atari を学習している．<span class="citation" data-cites="Hafner+2021">(Hafner et al., 2021)</span> はさらに性能が良い．↩︎</p></li>
<li id="fn12"><p><a href="https://www.nikkei.com/article/DGXZQOUC268J80W3A221C2000000/">日経新聞 (2/19/2024)</a>↩︎</p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-copyright"><h2 class="anchored quarto-appendix-heading">Copyright</h2><div class="quarto-appendix-contents"><div>CC BY-NC-ND</div></div></section></div> ]]></description>
  <category>Deep</category>
  <guid>https://162348.github.io/posts/2024/Kernels/Deep2.html</guid>
  <pubDate>Mon, 19 Feb 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Kernels/Transformer.png" medium="image" type="image/png" height="214" width="144"/>
</item>
<item>
  <title>数学者のための深層学習４</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Kernels/Deep4.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="確率的勾配降下法による変分ベイズ-sgvb" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="確率的勾配降下法による変分ベイズ-sgvb"><span class="header-section-number">1</span> 確率的勾配降下法による変分ベイズ (SGVB)</h2>
<p>変分自己符号化器 (Variational Auto-encoder) の説明に入る前に，変分ベイズ法において勾配を用いた最適化を実行するための汎用手法である <strong>SGVB (Stochastic Gradient Variational Bayes)</strong> について説明する．</p>
<p>VAE は元々この SGVB という要素技術とセットで提案された <span class="citation" data-cites="Kingma-Welling2014">(Kingma &amp; Welling, 2014)</span>．</p>
<p>変分近似をする分布族 <img src="https://latex.codecogs.com/png.latex?%5C%7Bq_%5Cphi%5C%7D"> としてニューラルネットを用いた場合が，VAE であり，広く SGVB は，一般の連続な潜在変数を持った（有向）グラフィカルモデルに適用できる．</p>
<section id="sgvb-のアイデア勾配の-monte-carlo-推定" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="sgvb-のアイデア勾配の-monte-carlo-推定"><span class="header-section-number">1.1</span> SGVB のアイデア：勾配の Monte Carlo 推定</h3>
<p><a href="../../../posts/2024/Kernels/Deep3.html">GAN</a> 同様，生成モデリングは，潜在空間 <img src="https://latex.codecogs.com/png.latex?Z"> で条件付けた際の分布 <img src="https://latex.codecogs.com/png.latex?p(x%7Cz)"> をモデリングすることに等しい．GAN は <img src="https://latex.codecogs.com/png.latex?p(x%7Cz)"> を明示的に評価することを回避することで複雑な生成モデリングを達成していた．</p>
<p>一方で，（周辺）尤度の評価を完全に回避せずとも，<a href="../../../posts/2024/Computation/VI3.html">変分 Bayes 法</a> によるアプローチが可能である．<img src="https://latex.codecogs.com/png.latex?p(x%7Cz)"> に分布族 <img src="https://latex.codecogs.com/png.latex?q_%5Cphi(x%7Cz)"> を導入し，真の分布 <img src="https://latex.codecogs.com/png.latex?p"> との KL-距離を最小にする <img src="https://latex.codecogs.com/png.latex?%5Cphi%5Cin%5CPhi"> を選ぶのである．</p>
<p>変分 Bayes ではこれを解析的に実行する必要があった．そのため，分布族 <img src="https://latex.codecogs.com/png.latex?%5C%7Bq_%5Cphi%5C%7D"> を指数分布族や共役分布族に限るか，平均場近似を用いるか，などの強い仮定が必要で，これが複雑な生成モデリングを妨げていた．<sup>1</sup></p>
<p>そこで，一般の分布族 <img src="https://latex.codecogs.com/png.latex?%5C%7Bq_%5Cphi%5C%7D"> に対して勾配情報を用いた最適化が実施できるように，変分下界 <img src="https://latex.codecogs.com/png.latex?F(p_%5Ctheta,q_%5Cphi)"> 対する Monte Carlo 推定量を開発するのである．これが SGVB 推定量である．</p>
</section>
<section id="変分下界の復習" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="変分下界の復習"><span class="header-section-number">1.2</span> 変分下界の復習</h3>
<p>データ <img src="https://latex.codecogs.com/png.latex?X"> の生成過程に，モデル <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(z)p_%5Ctheta(x%7Cz)"> を考える．これがニューラルネットワークによるモデルであるとすると，周辺尤度 <img src="https://latex.codecogs.com/png.latex?%0Ap_%5Ctheta(x)=%5Cint_%5Cmathcal%7BZ%7Dp_%5Ctheta(z)p_%5Ctheta(x%7Cz)%5C,dz%0A"> や事後分布 <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(z%7Cx)"> の評価は容易でない．</p>
<p>そこで，<img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(z%7Cx)"> に対して，認識モデル <img src="https://latex.codecogs.com/png.latex?%5C%7Bq_%5Cphi(z%7Cx)%5C%7D_%7B%5Cphi%5Cin%5CPhi%7D"> を導入する．VAE 2 では，これもニューラルネットワークとし，<img src="https://latex.codecogs.com/png.latex?(%5Ctheta,%5Cphi)%5Cin%5CTheta%5Ctimes%5CPhi"> を同時に SGD により学習することを考える．</p>
<p>潜在変数 <img src="https://latex.codecogs.com/png.latex?Z"> を情報源と見て，<img src="https://latex.codecogs.com/png.latex?q_%5Ctheta(z%7Cx)"> を <strong>符号化器</strong> (encoder) と呼び，<img src="https://latex.codecogs.com/png.latex?p_%5Ctheta(x%7Cz)"> を <strong>復号器</strong> (decoder) とも呼ぶ．</p>
<p>このとき，対数周辺尤度の変分下界は次のように表せるのであった：<sup>2</sup> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Clog%20p_%5Ctheta(x)&amp;=%5Clog%5Cint_%5Cmathcal%7BZ%7Dp_%5Ctheta(x,z)%5C,dz%5C%5C%0A%20%20%20%20&amp;=%5Clog%5Cint_%5Cmathcal%7BZ%7Dq_%5Cphi(z)%5Cfrac%7Bp_%5Ctheta(x,z)%7D%7Bq_%5Cphi(z)%7D%5C,dz%5C%5C%0A%20%20%20%20&amp;%5Cge%5Cint_%5Cmathcal%7BZ%7Dq_%5Cphi(z)%5Clog%5Cfrac%7Bq_%5Ctheta(x%7Cz)p_%5Ctheta(z)%7D%7Bq_%5Cphi(z)%7D%5C,dz%5C%5C%0A%20%20%20%20&amp;=-%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q_%5Cphi,p_%5Ctheta)+%5Cint_%5Cmathcal%7BZ%7Dq_%5Cphi(z)%5Clog%20p_%5Ctheta(x%7Cz)%5C,dz%5C%5C%0A%20%20%20%20&amp;=:F(%5Ctheta,%5Cphi;x)%0A%5Cend%7Balign*%7D%0A"></p>
<p>この <img src="https://latex.codecogs.com/png.latex?F"> を <img src="https://latex.codecogs.com/png.latex?%5Ctheta,%5Cphi"> に関して逐次的に最大化するのが変分 Bayes である．これを実行するために <img src="https://latex.codecogs.com/png.latex?q_%5Cphi"> に平均場近似などをするのが旧来手法であるが，これ以上の近似をせずとも，<img src="https://latex.codecogs.com/png.latex?F"> の勾配の推定量を用いて，<img src="https://latex.codecogs.com/png.latex?p_%5Ctheta,q_%5Cphi"> を同時に学習することが出来るというのである．</p>
</section>
<section id="sgvb-推定量" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="sgvb-推定量"><span class="header-section-number">1.3</span> SGVB 推定量</h3>
<p>例えば <img src="https://latex.codecogs.com/png.latex?F"> を <img src="https://latex.codecogs.com/png.latex?%5Cphi"> に関して勾配情報から最大化する際に，勾配 <img src="https://latex.codecogs.com/png.latex?D_%5Cphi%20F"> の Monte Carlo 推定量が利用できる．しかし，単に <img src="https://latex.codecogs.com/png.latex?q_%5Cphi(z%7Cx)"> からのサンプルを用いた crude Monte Carlo では，この推定量の分散は非常に大きい <span class="citation" data-cites="Paisley+2012">(Paisley et al., 2012)</span>．</p>
<p>これを <strong>重点サンプリングの考え方により解決した</strong> のが <img src="https://latex.codecogs.com/png.latex?D_%5Cphi%20F,D_%5Ctheta%20F"> に対する SGVB 推定量である．<sup>3</sup> <span class="citation" data-cites="Kingma-Welling2014">(Kingma &amp; Welling, 2014)</span> では reparameterization trick と呼んでいる．</p>
<p>ある分布 <img src="https://latex.codecogs.com/png.latex?P%5Cin%5Cmathcal%7BP%7D(E)"> と可微分同相 <img src="https://latex.codecogs.com/png.latex?g_%5Cphi:E%5Ctimes%5Cmathcal%7BX%7D%5Cto%5Cmathcal%7BZ%7D"> であって <img src="https://latex.codecogs.com/png.latex?%0Ag_%5Cphi(%5Cepsilon,x)%5Csim%20q_%5Cphi(z,x)%5Cquad(%5Cepsilon%5Csim%20P)%0A"> を満たすものを見つけることができて，この <img src="https://latex.codecogs.com/png.latex?P"> を提案分布とする重点サンプリング推定量 <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Coperatorname%7BE%7D_%7Bq_%5Cphi%7D%5Bf(Z)%5D&amp;=%5Coperatorname%7BE%7D_%7BP%7D%5Bf(g_%5Cphi(%5Cepsilon,x))%5D%5C%5C%0A%20%20%20%20&amp;%5Csimeq%5Cfrac%7B1%7D%7BM%7D%5Csum_%7Bi=1%7D%5EMf(g_%5Cphi(%5Cepsilon%5Ei,x))%0A%5Cend%7Balign*%7D%0A"> により，Monte Carlo 推定量の分散を減らすことができる．<img src="https://latex.codecogs.com/png.latex?f=F"> と取ることで SGVB 推定量を得る．</p>
<p>さらに，<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BZ%7D"> 上のモデル <img src="https://latex.codecogs.com/png.latex?q_%5Cphi(z),p_%5Ctheta(z)"> とが <img src="https://latex.codecogs.com/png.latex?d">-次元の正規分布であった場合， <img src="https://latex.codecogs.com/png.latex?%0A-%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q_%5Cphi,p_%5Ctheta)=%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bj=1%7D%5Ed%5Cbiggr(1+%5Clog(%5Csigma_j%5E2)-%5Cmu_j%5E2-%5Csigma_j%5E2%5Cbiggl)%0A"> と解析的に解けるので，結局 Monte Carlo 近似が必要なのは，再構成誤差を表す <img src="https://latex.codecogs.com/png.latex?%0A%5Cint_%5Cmathcal%7BZ%7Dq_%5Cphi(z)%5Clog%20p_%5Ctheta(x%7Cz)%5C,dz%0A"> の部分だけである．</p>
<p>このような理由で，<img src="https://latex.codecogs.com/png.latex?q_%5Cphi(z),p_%5Ctheta(z)"> は典型的には正規分布としてモデリングされる．</p>
</section>
</section>
<section id="sec-VAE" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-VAE"><span class="header-section-number">2</span> VAE <span class="citation" data-cites="Kingma-Welling2014">(Kingma &amp; Welling, 2014)</span></h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://162348.github.io/posts/2024/Kernels/VAE.png" class="img-fluid figure-img"></p>
<figcaption>Samples from a VQ-VAE Taken from Figure 6 <span class="citation" data-cites="Razavi+2019">(Razavi et al., 2019, p. 8)</span></figcaption>
</figure>
</div>
<section id="導入" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="導入"><span class="header-section-number">2.1</span> 導入</h3>
<p>Variational Auto-encoder <span class="citation" data-cites="Kingma-Welling2014">(Kingma &amp; Welling, 2014)</span>, <span class="citation" data-cites="Rezende+2014">(Rezende et al., 2014)</span> も GAN と同じく，深層生成モデル <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta"> にもう１つの深層ニューラルネットワーク <img src="https://latex.codecogs.com/png.latex?q_%5Cphi"> を対置するが，このニューラルネット <img src="https://latex.codecogs.com/png.latex?q_%5Cphi"> は GAN のように判別をするのではなく，近似推論によってデータ生成源を再構成しようとする <strong>認識モデル</strong> (recognition model) である．<img src="https://latex.codecogs.com/png.latex?q_%5Cphi"> はエンコーダーとも呼ばれる．</p>
<p>この深層生成モデル <img src="https://latex.codecogs.com/png.latex?p_%5Ctheta"> と近似推論器 <img src="https://latex.codecogs.com/png.latex?q_%5Cphi"> とを，同時に確率勾配降下法によって学習する <span class="citation" data-cites="Kingma-Welling2019">(Kingma &amp; Welling, 2019)</span>．</p>
<p>VAE のエンコーダー <img src="https://latex.codecogs.com/png.latex?q_%5Cphi"> は動画データの圧縮表現の学習 <span class="citation" data-cites="Brooks+2024">(Brooks et al., 2024)</span> など，その他の生成モデルの構成要素としても用いられる．</p>
</section>
<section id="sec-VQ-VAE" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="sec-VQ-VAE"><span class="header-section-number">2.2</span> VQ-VAE による画像の量子化</h3>
<p>VAE は画像データの生成にも応用されており，その際のデータ圧縮の技術（連続データである画像を離散化するので，<strong>ベクトル量子化</strong> と呼ばれる）だけが取り出され，DALL-E <span class="citation" data-cites="Ramesh+2021">(Ramesh et al., 2021)</span> など，モデルの構成要素としても利用されている．</p>
<section id="vq-vae" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="vq-vae"><span class="header-section-number">2.2.1</span> VQ-VAE</h4>
<p>VQ-VAE <span class="citation" data-cites="vandenOord+2017">(van&nbsp;den&nbsp;Oord et al., 2017)</span>, <span class="citation" data-cites="Razavi+2019">(Razavi et al., 2019)</span> は，自己符号化器の中間表現に <a href="../../../posts/2024/Computation/VI.html#sec-history">ベクトル量子化</a> を施し，JPEG <span class="citation" data-cites="Wallace1992">(Wallace, 1992)</span> のような画像データの圧縮を行うことで，不要な情報のモデリングを回避している．</p>
<p>実際，元データの 30 分の 1 以下のサイズで学習を行い，最終的にデコーダーを用いて殆ど歪みなく再構成できるという．</p>
<p>GAN は元データのうち，尤度が低い部分が無視され，サンプルの多様性が失われがちであったが，VQ-VAE はこの問題を解決している．また，GAN にはないようなモデル評価の指標が複数提案されている．</p>
</section>
<section id="連続緩和" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="連続緩和"><span class="header-section-number">2.2.2</span> 連続緩和</h4>
<p>質的変数のサンプリングにおいて，Gumbel 分布を提案分布として重点サンプリングを行うことが有効である．この reparametrization trick を Gumbel Max Trick <span class="citation" data-cites="Jang+2017">(Jang et al., 2017)</span> という．</p>
<p>Concrete (Continuous Relaxatino of Discrete) <span class="citation" data-cites="Maddison+2017">(Maddison et al., 2017)</span> はこれを連続分布に拡張し，reparametrization trick に応用したものである．</p>
<p>これらの手法は VAE や <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(Ramesh et al., 2021)</span> の訓練にも応用されている．</p>
</section>
<section id="codebook-collapse" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="codebook-collapse"><span class="header-section-number">2.2.3</span> Codebook collapse</h4>
<p>VQ-VAE は符号帳 (codebook) に冗長性が生まれ，符号帳の一部が使われなくなるという問題がある．これを解決するためには，符号帳への対応を softmax 関数を用いて軟化することが dVAE <span class="citation" data-cites="Ramesh+2021">(Ramesh et al., 2021)</span> として考えられている．</p>
<p>しかしこの dVAE も codebook collapse から完全に解放されるわけではない．これは softmax 関数の性質によると考えられ，実際，Dirichlet 事前分布を導入した Bayes モデルによって緩和される <span class="citation" data-cites="Baykal+2023">(Baykal et al., 2023)</span>．</p>
<p>このような技術を <strong>エビデンス付き深層学習</strong> (EDL: Evidential Deep Learning) <span class="citation" data-cites="Sensoy+2018">(Sensoy et al., 2018)</span>, <span class="citation" data-cites="Amini+2020">(Amini et al., 2020)</span> という．<sup>4</sup></p>
</section>
</section>
<section id="wasserstein-vae-tolstikhin2018" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="wasserstein-vae-tolstikhin2018"><span class="header-section-number">2.3</span> Wasserstein VAE <span class="citation" data-cites="Tolstikhin+2018">(Tolstikhin et al., 2018)</span></h3>
<p>VAE は GAN よりも画像生成時の解像度が劣るという問題がある．</p>
<p>これを，目的関数を Wasserstein 距離に基づいて再定式化することで解決できるというのが Wasserstein Auto-encoder <span class="citation" data-cites="Tolstikhin+2018">(Tolstikhin et al., 2018)</span> である．</p>
</section>
<section id="beta-vae-higgins2017" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="beta-vae-higgins2017"><span class="header-section-number">2.4</span> beta-VAE <span class="citation" data-cites="Higgins+2017">(Higgins et al., 2017)</span></h3>
<p>VAE を画像の因子表現学習に用いる際に，解釈可能性を担保する教師なし学習手法である．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Amini+2020" class="csl-entry">
Amini, A., Schwarting, W., Soleimany, A., &amp; Rus, D. (2020). <em>Deep evidential regression</em>. <a href="https://arxiv.org/abs/1910.02600">https://arxiv.org/abs/1910.02600</a>
</div>
<div id="ref-Baykal+2023" class="csl-entry">
Baykal, G., Kandemir, M., &amp; Unal, G. (2023). <em>EdVAE: Mitigating codebook collapse with evidential discrete variational autoencoders</em>. <a href="https://arxiv.org/abs/2310.05718">https://arxiv.org/abs/2310.05718</a>
</div>
<div id="ref-Brooks+2024" class="csl-entry">
Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C. W. Y., Wang, R., &amp; Ramesh, A. (2024). <em>Video generation models as world simulators</em>. OpenAI. <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a>
</div>
<div id="ref-Geyer1996" class="csl-entry">
Geyer, C. (1996). <em>Markov chain monte carlo in practice</em> (W. R. Gilks, S. Richardson, &amp; D. Spiegelhalter, Eds.; pp. 241–258). Chapman; Hall. <a href="https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson">https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson</a>
</div>
<div id="ref-Higgins+2017" class="csl-entry">
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., &amp; Lerchner, A. (2017). Beta-<span>VAE</span>: Learning basic visual concepts with a constrained variational framework. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=Sy2fzU9gl">https://openreview.net/forum?id=Sy2fzU9gl</a>
</div>
<div id="ref-Jang+2017" class="csl-entry">
Jang, E., Gu, S., &amp; Poole, B. (2017). Categorical reparameterization with gumbel-softmax. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=rkE3y85ee">https://openreview.net/forum?id=rkE3y85ee</a>
</div>
<div id="ref-Kingma-Welling2014" class="csl-entry">
Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational bayes. <em>International Conference on Learning Representations</em>, <em>2</em>. <a href="https://openreview.net/forum?id=33X9fd2-9FyZd">https://openreview.net/forum?id=33X9fd2-9FyZd</a>
</div>
<div id="ref-Kingma-Welling2019" class="csl-entry">
Kingma, D. P., &amp; Welling, M. (2019). An introduction to variational autoencoders. <em>Foundations and Treands in Machine Learning</em>, <em>12</em>(4), 307–392. <a href="https://www.nowpublishers.com/article/Details/MAL-056">https://www.nowpublishers.com/article/Details/MAL-056</a>
</div>
<div id="ref-Maddison+2017" class="csl-entry">
Maddison, C. J., Mnih, A., &amp; Teh, Y. W. (2017). The concrete distribution: A continuous relaxation of discrete random variables. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=S1jE5L5gl">https://openreview.net/forum?id=S1jE5L5gl</a>
</div>
<div id="ref-Paisley+2012" class="csl-entry">
Paisley, J., Blei, D. M., &amp; Jordan, M. I. (2012). Variational bayesian inference with stochastic search. <em>Proceedings of the 29th International Conference on Machine Learning</em>, 1363–1370. <a href="https://dl.acm.org/doi/10.5555/3042573.3042748">https://dl.acm.org/doi/10.5555/3042573.3042748</a>
</div>
<div id="ref-Ramesh+2021" class="csl-entry">
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. (2021). Zero-shot text-to-image generation. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8821–8831). PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>
</div>
<div id="ref-Razavi+2019" class="csl-entry">
Razavi, A., van&nbsp;den&nbsp;Oord, A., &amp; Vinyals, O. (2019). Generating diverse high-fidelity images with VQ-VAE-2. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>. <a href="https://papers.nips.cc/paper_files/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html">https://papers.nips.cc/paper_files/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html</a>
</div>
<div id="ref-Rezende+2014" class="csl-entry">
Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Approximate inference in deep generative models. <em>Proceedings of the 31st International Conference on Machine Learning</em>, <em>32</em>, 1278–1286. <a href="https://proceedings.mlr.press/v32/rezende14.html">https://proceedings.mlr.press/v32/rezende14.html</a>
</div>
<div id="ref-Robert-Casella2004" class="csl-entry">
Robert, C. P., &amp; Casella, G. (2004). <em>Monte carlo statistical methods</em> (2nd ed.). Springer New York.
</div>
<div id="ref-Sensoy+2018" class="csl-entry">
Sensoy, M., Kaplan, L., &amp; Kandemir, M. (2018). Evidential deep learning to quantify classification uncertainty. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, &amp; R. Garnett (Eds.), <em>Advances in neural information processing systems</em> (Vol. 31). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf</a>
</div>
<div id="ref-Tolstikhin+2018" class="csl-entry">
Tolstikhin, I., Bousquet, O., Gelly, S., &amp; Schoelkopf, B. (2018). Wasserstein auto-encoders. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=HkL7n1-0b">https://openreview.net/forum?id=HkL7n1-0b</a>
</div>
<div id="ref-vandenOord+2017" class="csl-entry">
van&nbsp;den&nbsp;Oord, A., Vinyals, O., &amp; Kavukcuoglu, K. (2017). Neural discrete representation learning. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html</a>
</div>
<div id="ref-Wallace1992" class="csl-entry">
Wallace, G. K. (1992). The JPEG still picture compression standard. <em>IEEE Transactions on Consumer Electronics</em>, <em>38</em>, 1. <a href="https://ieeexplore.ieee.org/document/125072">https://ieeexplore.ieee.org/document/125072</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>なお，平均場近似も極めて困難な解析的計算を必要とする <span class="citation" data-cites="Kingma-Welling2014">(Kingma &amp; Welling, 2014)</span>．↩︎</p></li>
<li id="fn2"><p><a href="../../../posts/2024/Computation/VI3.html#sec-ELBO">前稿</a> も参照．↩︎</p></li>
<li id="fn3"><p>最適化の文脈において，目的関数の評価が困難であるとき，Monte Carlo 推定量でこれを代替する際，重点サンプリングを用いると良いことは従来提案されている <span class="citation" data-cites="Geyer1996">(Geyer, 1996)</span>．<span class="citation" data-cites="Robert-Casella2004">(Robert &amp; Casella, 2004, p. 203)</span> も参照．↩︎</p></li>
<li id="fn4"><p><a href="https://deepsquare.jp/2020/12/deep-evidential-regression/">Present Square 記事</a>，<a href="https://gigazine.net/news/20201130-neural-network-trust/">GIGAZINE 記事</a> もある．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Deep</category>
  <guid>https://162348.github.io/posts/2024/Kernels/Deep4.html</guid>
  <pubDate>Sat, 17 Feb 2024 15:00:00 GMT</pubDate>
</item>
<item>
  <title>変分推論３</title>
  <dc:creator>司馬博文 </dc:creator>
  <link>https://162348.github.io/posts/2024/Computation/VI3.html</link>
  <description><![CDATA[ 





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
$$ %%% 汎用コード列
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="導入" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="導入"><span class="header-section-number">1</span> 導入</h2>
<section id="変分-bayes-推論の立ち位置" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="変分-bayes-推論の立ち位置"><span class="header-section-number">1.1</span> 変分 Bayes 推論の立ち位置</h3>
<p>Bayes 推論を実行するにあたって，サンプリング法は exact な方法であると言われる．これは十分な計算量を等価することで，任意の精度で事後分布を近似できるためである．</p>
<p>この性質は肝要であるが，真に厳密な近似を得ることよりも，ある程度の誤差を許容しながらも計算のコストを下げる方が重要である場面も多い．これを叶えてくれる，極めて自然な決定論的な近似手法が，変分推論である．</p>
<p>Bayes 事後分布に簡単な分布族を想定し，その中で KL 距離の意味で最も近い分布を，変分最適化によって探すのである．</p>
<p>どれくらい実行し続けていれば欲しい精度が出るのか分かりにくい MCMC よりも，KL 距離という（実は訳のわかっていない）尺度が大切ということにして，これが目に見えて減少していく方がアルゴリズムとして達成感があるというのである．</p>
</section>
<section id="変分法の歴史" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="変分法の歴史"><span class="header-section-number">1.2</span> 変分法の歴史</h3>
<p>変分法とは，関数空間上での微分法をいう．</p>
<p>変分法自体は，多くの応用先に古くから用いられている．統計学 <span class="citation" data-cites="Rustagi1976">(Rustagi, 1976)</span>，統計力学 <span class="citation" data-cites="Parisi1988">(Parisi, 1988)</span>，量子力学 <span class="citation" data-cites="Sakurai1985">(Sakurai, 1985)</span>，有限要素解析 <span class="citation" data-cites="Schwarz1988">(Schwarz, 1988)</span>, <span class="citation" data-cites="Bathe1996">(Bathe, 1996)</span> などの教科書で触れられている．最大エントロピー法 <span class="citation" data-cites="Kapur1989">(Kapur, 1989)</span>，最小作用の原理 <span class="citation" data-cites="Fenyman+1964">(Feynman et al., 1964)</span> も変分法の例である．</p>
<p>いずれの場面でも，変分法は困難な問題を，自由度を分解する (decoupling of the degrees of freedom) ことで，簡単な問題に分解する方法として用いられている <span class="citation" data-cites="Jordan+1999">(Jordan et al., 1999, p. 198)</span>．典型的には，変分パラメータ (variational parameter) という追加の変数を導入する手続きを伴う．</p>
</section>
</section>
<section id="sec-VB" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-VB"><span class="header-section-number">2</span> 変分 Bayes のアルゴリズム</h2>
<p>潜在変数を持つグラフィカルモデルの文脈では，EM アルゴリズムのような点推定によるパラメータ推定では汎化性能が伸びず，事後分布を導出したいが，その計算は困難である．これを打開すべく提案されたのが変分 Bayes 推定である <span class="citation" data-cites="Attias1999">(Attias, 1999)</span>．</p>
<section id="sec-ELBO" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sec-ELBO"><span class="header-section-number">2.1</span> アルゴリズムの前提</h3>
<p>変分 EM アルゴリズムは，<img src="https://latex.codecogs.com/png.latex?%5Ctheta"> の事前分布としてデルタ分布を置いていた場合の変分 Bayes アルゴリズムとみなせる．<sup>1</sup></p>
<p>モデルのパラメータや潜在変数を全て含めて <img src="https://latex.codecogs.com/png.latex?z"> とし，尤度が <img src="https://latex.codecogs.com/png.latex?%0Ap(x)=%5Cint_%5Cmathcal%7BZ%7Dp(x,z)%5C,dz%0A"> と与えられている解する．このとき，対数尤度の下界は <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Clog%20p(x)&amp;=%5Clog%5Cint_%5Cmathcal%7BZ%7Dp(x,z)%5C,dz%5C%5C%0A%20%20%20%20&amp;%5Cge%5Cint_%5Cmathcal%7BZ%7Dq(z%7Cx)%5Clog%5Cfrac%7Bp(x,z)%7D%7Bq(z%7Cx)%7D%5C,dz%5C%5C%0A%20%20%20%20&amp;=%5Cint_%5Cmathcal%7BZ%7Dq(z%7Cx)%5Clog%5Cfrac%7Bp(z%7Cx)p(x)%7D%7Bq(z%7Cx)%7D%5C,dz%5C%5C%0A%20%20%20%20&amp;=%5Clog%20p(x)-%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q,p)=:F(q).%0A%5Cend%7Balign*%7D%0A"> と表せるのであった．<sup>2</sup></p>
<p>この <img src="https://latex.codecogs.com/png.latex?F"> を変分下界または <a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">ELBO</a> (Evidence Lower Bound) という．<img src="https://latex.codecogs.com/png.latex?F"> を変分自由エネルギーともいう．</p>
<p>無制約下では，<img src="https://latex.codecogs.com/png.latex?F"> は <img src="https://latex.codecogs.com/png.latex?q=p"> のときに最大となる．これが EM アルゴリズムの E-ステップなのであった．しかし，このステップは難しい場面も多い．</p>
<p>そのような場合，まず <img src="https://latex.codecogs.com/png.latex?p"> にニューラルネットワークなどのパラメトリックモデルをおき，そのパラメータを KL-距離を最適化することで求めることが考えられる．こうしてパラメトリックな変分 Bayes アルゴリズムを得る．</p>
</section>
<section id="平均場近似" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="平均場近似"><span class="header-section-number">2.2</span> 平均場近似</h3>
<p>関数形ではなく，次のような仮定をおくことでも，変分 Bayes アルゴリズムが得られる．</p>
<p><span id="eq-4"><img src="https://latex.codecogs.com/png.latex?%0Aq(z%7Cx)=q(z_1%7Cx)q(z_2%7Cx)%0A%5Ctag%7B1%7D"></span></p>
<p>と仮定すると <img src="https://latex.codecogs.com/png.latex?%0AF(q)=%5Cint_%5Cmathcal%7BZ%7Dq(z_1%7Cx)q(z_2%7Cx)%5Clog%5Cfrac%7Bp(x,z)%7D%7Bq(z_1%7Cx)q(z_2%7Cx)%7D%5C,dz%0A"> の表示を得る．このような仮定は平均場近似とも呼ばれる．<sup>3</sup></p>
<p>実は，この表示ならば，<img src="https://latex.codecogs.com/png.latex?q(z_1%7Cx)"> と <img src="https://latex.codecogs.com/png.latex?q(z_2%7Cx)"> について逐次的に最大化していくための解析的な公式が求まる．</p>
<p>さらに，<img src="https://latex.codecogs.com/png.latex?F"> は各因子 <img src="https://latex.codecogs.com/png.latex?q(z_1%7Cx),q(z_2%7Cx)"> に関して凸になるので，こうして得るアルゴリズムの収束も保証される．<sup>4</sup></p>
<section id="vb-e-ステップ" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="vb-e-ステップ"><span class="header-section-number">2.2.1</span> VB-<img src="https://latex.codecogs.com/png.latex?E"> ステップ</h4>
<p><img src="https://latex.codecogs.com/png.latex?F"> の <img src="https://latex.codecogs.com/png.latex?q(z_1%7Cx)"> に関する最大値は <img src="https://latex.codecogs.com/png.latex?%0Aq(z_1)%5Cpropto%20e%5E%7B(q(z_2)dz_2%5C,%7C%5Clog%20p(x,z_2%7Cz_1))%7D%0A"> が与える．</p>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>まず，<img src="https://latex.codecogs.com/png.latex?q(z_1%7Cx)"> について最大化することを考える．<img src="https://latex.codecogs.com/png.latex?F"> の <img src="https://latex.codecogs.com/png.latex?q(z_1%7Cx)"> に関する最大化は， <img src="https://latex.codecogs.com/png.latex?%0AL:=F(q)+%5Clambda%5Cleft(%5Cint_%5Cmathcal%7BZ%7Dq(z_1)%5C,dz_1-1%5Cright)%0A"> の <img src="https://latex.codecogs.com/png.latex?%5Clambda"> との同時最大化と同値である．これが <a href="https://ja.wikipedia.org/wiki/%E3%83%A9%E3%82%B0%E3%83%A9%E3%83%B3%E3%82%B8%E3%83%A5%E3%81%AE%E6%9C%AA%E5%AE%9A%E4%B9%97%E6%95%B0%E6%B3%95">Lagrange の未定乗数法</a> である．</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20%5Cfrac%7B%5Cdelta%20L%7D%7B%5Cdelta%20q(z_1%7Cx)%7D&amp;(q(z_2%7Cx)dz_2%7C%5Clog%20p(x,z_1%7Cz_2))-%5Clog%20q(z_1)+%5Clambda+%5Cmathrm%7Bconst.%7D%0A%5Cend%7Balign*%7D%0A"> と計算できるから，これは <img src="https://latex.codecogs.com/png.latex?%0Aq(z_1)%5Cpropto%20e%5E%7B(q(z_2)dz_2%5C,%7C%5Clog%20p(x,z_2%7Cz_1))%7D%0A"> にて最大化される．これが変分 Bayes アルゴリズムの <img src="https://latex.codecogs.com/png.latex?E">-ステップである．</p>
<p><span class="citation" data-cites="Bishop2006">(Bishop, 2006, p. 465)</span> はまた違った議論を提供している．</p>
</div>
</div>
</div>
<p><img src="https://latex.codecogs.com/png.latex?q(z_2)=%5Cdelta(%5Cvarphi)"> であるとき， <span id="eq-5"><img src="https://latex.codecogs.com/png.latex?%0Aq(z_1)%5Cpropto%20p(x,z_1%7C%5Cvarphi)%5Cpropto%20p(z_1%7Cx,%5Cvarphi)%0A%5Ctag%7B2%7D"></span> であることに注意．</p>
</section>
<section id="vb-m-ステップ" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="vb-m-ステップ"><span class="header-section-number">2.2.2</span> VB-<img src="https://latex.codecogs.com/png.latex?M"> ステップ</h4>
<p>全く同様にして， <img src="https://latex.codecogs.com/png.latex?%0Aq(z_2)%5Cpropto%20p(z_2)e%5E%7B(q(z_1)dz_2%5C,%7C%5Clog%20p(x,z_1%7Cz_2))%7D%0A"> で最大化される．</p>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">

</div>
</div>
</div>
</section>
<section id="sec-VB-regularization" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="sec-VB-regularization"><span class="header-section-number">2.2.3</span> 自動正則化</h4>
<p>またこの枠組みは，その他のベイズ的な手法と同様，過学習を防ぐ正則化が暗黙のうちに盛り込まれているともみなせる．<sup>5</sup></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%20%20%20%20F(q)&amp;=%5Cint_%5Cmathcal%7BZ%7Dq(z_1%7Cx)q(z_2%7Cx)%5Clog%5Cfrac%7Bp(x,z)%7D%7Bq(z_1%7Cx)q(z_2%7Cx)%7D%5C,dz%5C%5C%0A%20%20%20%20&amp;=%5Cint_%5Cmathcal%7BZ%7Dq(z_1%7Cx)q(z_2%7Cx)%5Clog%5Cfrac%7Bp(x,z_1%7Cz_2)p(z_2)%7D%7Bq(z_1%7Cx)q(z_2%7Cx)%7D%5C,dz%5C%5C%0A%20%20%20%20&amp;=%5Cint_%5Cmathcal%7BZ%7Dq(z_1%7Cx)q(z_2%7Cx)%5Clog%5Cfrac%7Bp(x,z_1%7Cz_2)%7D%7Bq(z_1%7Cx)%7Ddz%5C%5C%0A%20%20%20%20&amp;%5Cqquad-%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q(-%7Cx),p(-)).%0A%5Cend%7Balign*%7D%0A"></p>
</section>
</section>
<section id="sec-VB-problem" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="sec-VB-problem"><span class="header-section-number">2.3</span> 平均場近似の問題点</h3>
<p>いわば <img src="https://latex.codecogs.com/png.latex?q"> の全ての周辺分布を「独立」だと解釈しているため，実際には変数間に強い相関があった際に，分散を過小評価する嫌いがある．</p>
</section>
</section>
<section id="sec-EP" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-EP"><span class="header-section-number">3</span> 期待値伝播法</h2>
<section id="導入-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="導入-1"><span class="header-section-number">3.1</span> 導入</h3>
<p>節&nbsp;2 では <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q,p)"> を最小化する <img src="https://latex.codecogs.com/png.latex?q"> を探索したが，逆に <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(p,q)"> を最小化すると考えるのが期待値伝播法 (EP: Expectation Propagation) <span class="citation" data-cites="Minka2001b">(Minka, 2001a)</span>, <span class="citation" data-cites="Minka2001">(Minka, 2001b)</span> である．</p>
<p>なお，<img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(p,q)"> を，MCMC によって推定した勾配を用いて確率的勾配降下法によって最小化する手法も提案されている： Markovian Score Climbing <span class="citation" data-cites="Naesseth+2020">(Naesseth et al., 2020)</span>, Joint Stochastic Approximation <span class="citation" data-cites="Ou-Song2020">(Ou &amp; Song, 2020)</span> とこれらを包含する Markov Chain Score Ascent <span class="citation" data-cites="Kim+2022">(Kim et al., 2022)</span> など．</p>
</section>
<section id="sec-alpha-divergence" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-alpha-divergence"><span class="header-section-number">3.2</span> <img src="https://latex.codecogs.com/png.latex?%5Calpha">-乖離度</h3>
<p>期待値伝播法と変分 Bayes 推論との振る舞いの違いは，<img src="https://latex.codecogs.com/png.latex?%5Calpha">-乖離度の振る舞いの変化によって理解できる．</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="定義（$\alpha$-divergence）[@Amari1985 p.85], [@Cichocki+2008 p.1434]">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定義（<img src="https://latex.codecogs.com/png.latex?%5Calpha">-divergence）<span class="citation" data-cites="Amari1985">(Amari, 1985, p. 85)</span>, <span class="citation" data-cites="Cichocki+2008">(Cichocki et al., 2008, p. 1434)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p><img src="https://latex.codecogs.com/png.latex?%5Calpha%5Cin%5Cmathbb%7BR%7D%5Csetminus%5C%7B%5Cpm1%5C%7D"> に関して， <img src="https://latex.codecogs.com/png.latex?%0AD_%5Calpha(p,q):=%5Cfrac%7B4%7D%7B1-%5Calpha%5E2%7D%5Cleft(1-%5Cint_%5Cmathcal%7BX%7Dp(x)%5E%7B%5Cfrac%7B1+%5Calpha%7D%7B2%7D%7Dq(x)%5E%7B%5Cfrac%7B1-%5Calpha%7D%7B2%7D%7D%5C,dx%5Cright)%0A"> を <strong><img src="https://latex.codecogs.com/png.latex?%5Calpha">-乖離度</strong> という．<sup>6</sup></p>
</div>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%5Calpha%5Cto1"> の極限では <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(p,q)"> に収束し，<img src="https://latex.codecogs.com/png.latex?%5Calpha%5Cto-1"> の極限では <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q,p)"> に収束する．<sup>7</sup> いわば，この２つの量を補間する量である．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Calpha=0"> の場合を，Hellinger 距離（の自乗）という．</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Calpha%5Cle-1"> の場合は <img src="https://latex.codecogs.com/png.latex?D_%5Calpha"> は <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bq%7D%7Bp%7D"> を含むため，<img src="https://latex.codecogs.com/png.latex?p"> が零ならば <img src="https://latex.codecogs.com/png.latex?q"> も零になるようになる：<img src="https://latex.codecogs.com/png.latex?q%5Cll%20p">．実際，変分近似は分散を過小評価しがちである 節&nbsp;2.3．</p>
<p>一方で <img src="https://latex.codecogs.com/png.latex?%5Calpha%5Cge1"> の場合は <img src="https://latex.codecogs.com/png.latex?D_%5Calpha"> は <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp%7D%7Bq%7D"> を含むため，<img src="https://latex.codecogs.com/png.latex?p"> の台を <img src="https://latex.codecogs.com/png.latex?q"> の台が含むようになる．</p>
<p>こうして EP は，変分 Bayes よりも，複数の峰がある分布を平均したように，裾の広い近似を与えるという対照的な性質を持つ．</p>
</section>
<section id="power-ep" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="power-ep"><span class="header-section-number">3.3</span> Power-EP</h3>
<p>一般の <img src="https://latex.codecogs.com/png.latex?%5Calpha">-乖離度を最小化する手法が Power-EP <span class="citation" data-cites="Minka2004">(Minka, 2004)</span> である．</p>
<p>多くのメッセージ伝播アルゴリズムもこの枠組みで導出できる <span class="citation" data-cites="Minka2005">(Minka, 2005)</span>．<sup>8</sup></p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2">
<div id="ref-Amari1985" class="csl-entry">
Amari, S. (1985). <em>Differential-geometrical methods in statistics</em> (Vol. 28). Springer New York. <a href="https://link.springer.com/book/10.1007/978-1-4612-5056-2">https://link.springer.com/book/10.1007/978-1-4612-5056-2</a>
</div>
<div id="ref-Attias1999" class="csl-entry">
Attias, H. (1999). Inferring parameters and structure of latent variable models by variational bayes. <em>Proceedings of the Fifteenth Conference in Artificial Intelligence</em>, 21–30. <a href="https://dl.acm.org/doi/10.5555/2073796.2073799">https://dl.acm.org/doi/10.5555/2073796.2073799</a>
</div>
<div id="ref-Bathe1996" class="csl-entry">
Bathe, K.-J. (1996). <em><a href="">Finite element procedures</a></em>. Prentice-Hall.
</div>
<div id="ref-Bishop2006" class="csl-entry">
Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. Springer New York. <a href="https://link.springer.com/book/9780387310732">https://link.springer.com/book/9780387310732</a>
</div>
<div id="ref-Cichocki+2008" class="csl-entry">
Cichocki, A., Lee, H., Kim, Y.-D., &amp; Choi, S. (2008). Non-negative matrix factorization with <img src="https://latex.codecogs.com/png.latex?%5Calpha">-divergence. <em>Pattern Recognition Letters</em>, <em>29</em>(9), 1433–1440. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508000767">https://www.sciencedirect.com/science/article/abs/pii/S0167865508000767</a>
</div>
<div id="ref-Fenyman+1964" class="csl-entry">
Feynman, R. P., Leighton, R. B., &amp; Sands, M. (1964). <em>The feynman lectures of physics: Vol. II</em>. Addison-Wesley. <a href="https://www.feynmanlectures.caltech.edu/II_19.html">https://www.feynmanlectures.caltech.edu/II_19.html</a>
</div>
<div id="ref-Jordan+1999" class="csl-entry">
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>, 183–233. <a href="https://link.springer.com/article/10.1023/A:1007665907178">https://link.springer.com/article/10.1023/A:1007665907178</a>
</div>
<div id="ref-Kapur1989" class="csl-entry">
Kapur, J. N. (1989). <em><a href="">Maximum-entropy models in science and engineering</a></em> (2nd ed.). Wiley.
</div>
<div id="ref-Khan-Rue2023" class="csl-entry">
Khan, M. E., &amp; Rue, H. (2023). The bayesian learning rule. <em>Journal of Machine Learning Research</em>, <em>24</em>(281), 1–46. <a href="http://jmlr.org/papers/v24/22-0291.html">http://jmlr.org/papers/v24/22-0291.html</a>
</div>
<div id="ref-Kim+2022" class="csl-entry">
Kim, K., Oh, J., Gardner, J., Dieng, A. B., &amp; Kim, H. (2022). Markov chain score ascent: A unifying framework of variational inference with markovian gradients. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, &amp; A. Oh (Eds.), <em>Advances in neural information processing systems</em> (Vol. 35, pp. 34802–34816). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/e0fbc0f2e35e58aeffe5524a69ba90e5-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2022/file/e0fbc0f2e35e58aeffe5524a69ba90e5-Paper-Conference.pdf</a>
</div>
<div id="ref-Minka2001b" class="csl-entry">
Minka, T. P. (2001a). <em>A family of approximation algorithms for bayesian inference</em> [PhD thesis]. MIT.
</div>
<div id="ref-Minka2001" class="csl-entry">
Minka, T. P. (2001b). Expectation propagation for approximate bayesian inference. <em>Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence</em>, 362–369. <a href="https://dl.acm.org/doi/10.5555/2074022.2074067">https://dl.acm.org/doi/10.5555/2074022.2074067</a>
</div>
<div id="ref-Minka2004" class="csl-entry">
Minka, T. P. (2004). <em>Power EP</em>. Microsoft Research Cambridge. <a href="https://www.microsoft.com/en-us/research/publication/power-ep/">https://www.microsoft.com/en-us/research/publication/power-ep/</a>
</div>
<div id="ref-Minka2005" class="csl-entry">
Minka, T. P. (2005). <em>Divergence measures and message passing</em>. Microsoft Research Cambridge. <a href="https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/">https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/</a>
</div>
<div id="ref-Naesseth+2020" class="csl-entry">
Naesseth, C., Lindsten, F., &amp; Blei, D. (2020). Markovian score climbing: Variational inference with KL(pq). In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, &amp; H. Lin (Eds.), <em>Advances in neural information processing systems</em> (Vol. 33, pp. 15499–15510). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf</a>
</div>
<div id="ref-Ou-Song2020" class="csl-entry">
Ou, Z., &amp; Song, Y. (2020). Joint stochastic approximation and its application to learning discrete latent variable models. In J. Peters &amp; D. Sontag (Eds.), <em>Proceedings of the 36th conference on uncertainty in artificial intelligence (UAI)</em> (Vol. 124, pp. 929–938). PMLR. <a href="https://proceedings.mlr.press/v124/ou20a.html">https://proceedings.mlr.press/v124/ou20a.html</a>
</div>
<div id="ref-Parisi1988" class="csl-entry">
Parisi, G. (1988). <em><a href="">Statistical field theory</a></em>. Addison-Wesley.
</div>
<div id="ref-Rustagi1976" class="csl-entry">
Rustagi, J. (1976). <em><a href="">Variational methods in statistics</a></em>. Academic Press.
</div>
<div id="ref-Sakurai1985" class="csl-entry">
Sakurai, J. (1985). <em><a href="">Modern quantum mechanics</a></em>. Addison-Wesley.
</div>
<div id="ref-Schwarz1988" class="csl-entry">
Schwarz, H. R. (1988). <em><a href="">Finite element methods</a></em>. Academic Press.
</div>
<div id="ref-Wainwright-Jordan2008" class="csl-entry">
Wainwright, M. J., &amp; Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. <em>Foundations and Trends in Machine Learning</em>, <em>1</em>(1-2), 1–305. <a href="https://www.nowpublishers.com/article/Details/MAL-001">https://www.nowpublishers.com/article/Details/MAL-001</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="Wainwright-Jordan2008">(Wainwright &amp; Jordan, 2008, p. 164)</span> も参照．↩︎</p></li>
<li id="fn2"><p>EM アルゴリズムに関する <a href="../../../posts/2024/Computation/VI2.html#sec-EM">前稿</a> も参照．↩︎</p></li>
<li id="fn3"><p><span class="citation" data-cites="Bishop2006">(Bishop, 2006, p. 465)</span>．↩︎</p></li>
<li id="fn4"><p><span class="citation" data-cites="Bishop2006">(Bishop, 2006, p. 466)</span>．↩︎</p></li>
<li id="fn5"><p>その理由に関する洞察は，エントロピー項 <img src="https://latex.codecogs.com/png.latex?H(q)"> が大きな役割を果たしているようである．<span class="citation" data-cites="Khan-Rue2023">(Khan &amp; Rue, 2023)</span> なども示唆的である．↩︎</p></li>
<li id="fn6"><p>関連する乖離度に，<a href="https://en.wikipedia.org/wiki/Rényi_entropy#Rényi_divergence">Rényi の <img src="https://latex.codecogs.com/png.latex?%5Calpha">-乖離度</a> がある．↩︎</p></li>
<li id="fn7"><p>前者 <img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(p,q)"> を <img src="https://latex.codecogs.com/png.latex?q"> に関して exclusive と言い，<img src="https://latex.codecogs.com/png.latex?%5Cmathop%7B%5Cmathrm%7BKL%7D%7D(q,p)"> は <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bsupp%7D%5C;(q)%5Csubset%5Cmathrm%7Bsupp%7D%5C;(p)"> を満たすため inclusive ともいう．<span class="citation" data-cites="Kim+2022">(Kim et al., 2022)</span> など．↩︎</p></li>
<li id="fn8"><p><span class="citation" data-cites="Bishop2006">(Bishop, 2006, p. 517)</span> も参照．↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Bayesian</category>
  <category>Computation</category>
  <category>Python</category>
  <guid>https://162348.github.io/posts/2024/Computation/VI3.html</guid>
  <pubDate>Sun, 11 Feb 2024 15:00:00 GMT</pubDate>
  <media:content url="https://162348.github.io/posts/2024/Computation/VI3.svg" medium="image" type="image/svg+xml"/>
</item>
</channel>
</rss>
