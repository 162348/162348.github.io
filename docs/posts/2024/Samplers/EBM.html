<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="司馬博文">

<title>Hirofumi Shiba - エネルギーベースモデル</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/profile.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-lst-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-title','listing-image','listing-date','listing-subtitle',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-lst-listing'] = new List('listing-lst-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Zen+Kurenaido&amp;display=swap" rel="stylesheet">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&amp;display=swap" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../assets/styles.css">
<meta property="og:title" content="Hirofumi Shiba - エネルギーベースモデル">
<meta property="og:description" content="確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．">
<meta property="og:image" content="https://162348.github.io/posts/2024/Samplers/Files/NCL/thumb_checkerboard.gif">
<meta property="og:site_name" content="Hirofumi Shiba">
<meta name="twitter:title" content="Hirofumi Shiba - エネルギーベースモデル">
<meta name="twitter:description" content="確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．">
<meta name="twitter:image" content="https://162348.github.io/posts/2024/Samplers/Files/NCL/thumb_checkerboard.gif">
<meta name="twitter:creator" content="@ano2math5">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Sessions.html"> 
<span class="menu-text">Sessions</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Japanese.html"> 
<span class="menu-text">自己紹介</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
    <a href="https://github.com/162348/162348.github.io/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">エネルギーベースモデル</h1>
            <p class="subtitle lead">深層生成モデル５</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Deep</div>
                <div class="quarto-category">Nature</div>
                <div class="quarto-category">Sampling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>司馬博文 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">3/30/2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">8/01/2024</p>
      </div>
    </div>
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">概要</div>
      確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目次</h2>
   
  <ul>
  <li><a href="#エネルギーベースモデル" id="toc-エネルギーベースモデル" class="nav-link active" data-scroll-target="#エネルギーベースモデル"><span class="header-section-number">1</span> エネルギーベースモデル</a>
  <ul class="collapse">
  <li><a href="#はじめに" id="toc-はじめに" class="nav-link" data-scroll-target="#はじめに"><span class="header-section-number">1.1</span> はじめに</a></li>
  <li><a href="#例" id="toc-例" class="nav-link" data-scroll-target="#例"><span class="header-section-number">1.2</span> 例</a></li>
  <li><a href="#分配関数" id="toc-分配関数" class="nav-link" data-scroll-target="#分配関数"><span class="header-section-number">1.3</span> 分配関数</a></li>
  <li><a href="#エネルギー" id="toc-エネルギー" class="nav-link" data-scroll-target="#エネルギー"><span class="header-section-number">1.4</span> エネルギー</a></li>
  <li><a href="#応用" id="toc-応用" class="nav-link" data-scroll-target="#応用"><span class="header-section-number">1.5</span> 応用</a></li>
  <li><a href="#推定手法" id="toc-推定手法" class="nav-link" data-scroll-target="#推定手法"><span class="header-section-number">1.6</span> 推定手法</a></li>
  </ul></li>
  <li><a href="#sec-ML" id="toc-sec-ML" class="nav-link" data-scroll-target="#sec-ML"><span class="header-section-number">2</span> 最尤推定</a>
  <ul class="collapse">
  <li><a href="#はじめに-1" id="toc-はじめに-1" class="nav-link" data-scroll-target="#はじめに-1"><span class="header-section-number">2.1</span> はじめに</a></li>
  <li><a href="#確率勾配-langevin-動力学-sgld-welling-teh2011" id="toc-確率勾配-langevin-動力学-sgld-welling-teh2011" class="nav-link" data-scroll-target="#確率勾配-langevin-動力学-sgld-welling-teh2011"><span class="header-section-number">2.2</span> 確率勾配 Langevin 動力学 (SGLD) <span class="citation" data-cites="Welling-Teh2011">(Welling and Teh, 2011)</span></a></li>
  <li><a href="#sec-CD" id="toc-sec-CD" class="nav-link" data-scroll-target="#sec-CD"><span class="header-section-number">2.3</span> 対照分離度 (CD)</a></li>
  </ul></li>
  <li><a href="#sec-SM" id="toc-sec-SM" class="nav-link" data-scroll-target="#sec-SM"><span class="header-section-number">3</span> スコアマッチング <span class="citation" data-cites="Hyvarinen2005">(SM, Hyvärinen, 2005)</span></a>
  <ul class="collapse">
  <li><a href="#はじめに-2" id="toc-はじめに-2" class="nav-link" data-scroll-target="#はじめに-2"><span class="header-section-number">3.1</span> はじめに</a></li>
  <li><a href="#sec-Hyvarinen2005" id="toc-sec-Hyvarinen2005" class="nav-link" data-scroll-target="#sec-Hyvarinen2005"><span class="header-section-number">3.2</span> 部分積分 <span class="citation" data-cites="Hyvarinen2005">(Hyvärinen, 2005)</span></a></li>
  <li><a href="#sec-RSM" id="toc-sec-RSM" class="nav-link" data-scroll-target="#sec-RSM"><span class="header-section-number">3.3</span> 正則化スコアマッチング (RSM) <span class="citation" data-cites="Kingma-LeCun2010">(Kingma and LeCun, 2010)</span></a></li>
  <li><a href="#sec-DSM" id="toc-sec-DSM" class="nav-link" data-scroll-target="#sec-DSM"><span class="header-section-number">3.4</span> Denoising スコアマッチング (DSM) <span class="citation" data-cites="Vincent2011">(Vincent, 2011)</span></a></li>
  <li><a href="#sec-SSM" id="toc-sec-SSM" class="nav-link" data-scroll-target="#sec-SSM"><span class="header-section-number">3.5</span> スライススコアマッチング (SSM) <span class="citation" data-cites="Song+2019">(Song et al., 2019)</span></a></li>
  <li><a href="#sm-目的関数の理論" id="toc-sm-目的関数の理論" class="nav-link" data-scroll-target="#sm-目的関数の理論"><span class="header-section-number">3.6</span> SM 目的関数の理論</a></li>
  </ul></li>
  <li><a href="#sec-NCE" id="toc-sec-NCE" class="nav-link" data-scroll-target="#sec-NCE"><span class="header-section-number">4</span> ノイズ対照学習 <span class="citation" data-cites="Gutmann-Hyvarinen2010">(NCE, M. Gutmann and Hyvärinen, 2010)</span></a>
  <ul class="collapse">
  <li><a href="#はじめに-3" id="toc-はじめに-3" class="nav-link" data-scroll-target="#はじめに-3"><span class="header-section-number">4.1</span> はじめに</a></li>
  <li><a href="#ノイズ-p_n-の選び方" id="toc-ノイズ-p_n-の選び方" class="nav-link" data-scroll-target="#ノイズ-p_n-の選び方"><span class="header-section-number">4.2</span> ノイズ <span class="math inline">\(p_n\)</span> の選び方</a></li>
  <li><a href="#スコアマッチングによる解釈" id="toc-スコアマッチングによる解釈" class="nav-link" data-scroll-target="#スコアマッチングによる解釈"><span class="header-section-number">4.3</span> スコアマッチングによる解釈</a></li>
  <li><a href="#応用-1" id="toc-応用-1" class="nav-link" data-scroll-target="#応用-1"><span class="header-section-number">4.4</span> 応用</a></li>
  <li><a href="#sec-FCE" id="toc-sec-FCE" class="nav-link" data-scroll-target="#sec-FCE"><span class="header-section-number">4.5</span> フロー対照推定 <span class="citation" data-cites="Gao+2020">(FCE, Gao et al., 2020)</span></a></li>
  </ul></li>
  <li><a href="#その他の訓練方法" id="toc-その他の訓練方法" class="nav-link" data-scroll-target="#その他の訓練方法"><span class="header-section-number">5</span> その他の訓練方法</a>
  <ul class="collapse">
  <li><a href="#stein-乖離度" id="toc-stein-乖離度" class="nav-link" data-scroll-target="#stein-乖離度"><span class="header-section-number">5.1</span> Stein 乖離度</a></li>
  <li><a href="#敵対的な訓練" id="toc-敵対的な訓練" class="nav-link" data-scroll-target="#敵対的な訓練"><span class="header-section-number">5.2</span> 敵対的な訓練</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
<p>$$</p>
<p>$$</p>
</div>
<section id="関連ページ" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="関連ページ">関連ページ</h3>
<div id="listing-lst-listing" class="listing quarto-float quarto-listing quarto-listing-container-grid">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="Deep,Python" data-listing-date-sort="1722524400000" data-listing-file-modified-sort="1724761473773" data-listing-date-modified-sort="1724252400000" data-listing-reading-time-sort="4" data-listing-word-count-sort="784">
<a href="../../../posts/2024/Samplers/EBM1.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img src="../../../posts/2024/Samplers/Files/swiss_roll.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
スコアマッチング
</h5>
<div class="card-subtitle listing-subtitle">
JAX によるハンズオン
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-08-02
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="1" data-categories="Deep,Sampling,Python" data-listing-date-sort="1722610800000" data-listing-file-modified-sort="1724239848322" data-listing-date-modified-sort="1724166000000" data-listing-reading-time-sort="11" data-listing-word-count-sort="2117">
<a href="../../../posts/2024/Samplers/EBM2.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img src="../../../posts/2024/Samplers/Files/NCL/thumb_8gaussians.gif" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
エネルギーベースモデルのノイズ対照学習
</h5>
<div class="card-subtitle listing-subtitle">
<code>PyTorch</code> によるハンズオン
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-08-03
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="2" data-categories="Deep,Sampling,P(X)" data-listing-date-sort="1707836400000" data-listing-file-modified-sort="1724764651955" data-listing-date-modified-sort="1724598000000" data-listing-reading-time-sort="4" data-listing-word-count-sort="758">
<a href="../../../posts/2024/Samplers/NF1.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img src="../../../posts/2024/Samplers/Files/linear/output.gif" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
ニューラル常微分方程式
</h5>
<div class="card-subtitle listing-subtitle">
シミュレーションなしの拡散モデルとしての連続正規化流
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-14
</div>
</div>
</div>
</div>
</a>
</div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div>
</section>
<section id="エネルギーベースモデル" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="エネルギーベースモデル"><span class="header-section-number">1</span> エネルギーベースモデル</h2>
<section id="はじめに" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="はじめに"><span class="header-section-number">1.1</span> はじめに</h3>
<p>エネルギーベースのモデル (EBM: Energy-based Model) とは，密度が <span class="math display">\[
p(x,z)\,\propto\,e^{-H(x,z)}
\]</span> の形で与えられるモデルをいう．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>これは統計物理学的な系との対応が意識されているように，入力 <span class="math inline">\(z\)</span> と出力 <span class="math inline">\(x\)</span> の整合性 <span class="math inline">\(k(x,z)\)</span> をエネルギー <span class="math inline">\(H(x,z)\)</span> の言葉で与えているモデルであると見ることができ，このエネルギー <span class="math inline">\(H\)</span> をニューラルネットワークによってモデル化するのである．</p>
<p>すると EBM の最尤推定とは，訓練データ <span class="math inline">\(\{x_i\}_{i=1}^n\)</span> に対して最も低いエネルギーを割り当てるエネルギー関数 <span class="math inline">\(H_\theta\)</span> を探すという基底状態探索の問題に対応する <span class="citation" data-cites="LeCun+2007">(<a href="#ref-LeCun+2007" role="doc-biblioref">LeCun et al., 2007</a>)</span>．</p>
<p>これで EBM の概要は終わりであるが，このような極めて一般的な設定で有用な一般論が得られることは驚くべきことである．</p>
<!-- 回帰や分類などの古典的なタスクだけでなく，ほとんどの確率的モデルもこの手続きから理解することができ，この場合は EBM が非確率的な／最適化ベースの推論手法を提供するフレームワークとして働くことになる [@LeCun+2007 p.192]． -->
</section>
<section id="例" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="例"><span class="header-section-number">1.2</span> 例</h3>
<section id="有向グラフを-ebm-とみなす" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="有向グラフを-ebm-とみなす"><span class="header-section-number">1.2.1</span> 有向グラフを EBM とみなす</h4>
<p><a href="../../../posts/2024/Kernels/Deep4.html">VAE</a> や <a href="../../../posts/2024/Samplers/NF.html">正規化流</a> などの生成モデルや，<a href="../../../posts/2024/Kernels/Deep2.html">トランスフォーマー</a> などの自己回帰モデルも EBM とみなせる．</p>
<p>特にこれらの生成モデルを事後調整しようと思うと，自然に EBM の構造が出てくる．例えば，LLM における RLHF などの事後調整は，生成モデル <span class="math inline">\(p(x|z)\)</span> とある目的関数 <span class="math inline">\(\phi\)</span> に関して <span class="math display">\[
p'(x|z):=\frac{1}{Z(z)}p(x|z)\phi(z)
\]</span> によって分布を調整する営みであると見ると，やはり正規化定数を除いた分布族が定まる <span class="citation" data-cites="Zhao+2024">(<a href="#ref-Zhao+2024" role="doc-biblioref">Zhao et al., 2024</a>)</span>．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</section>
<section id="無向グラフ" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="無向グラフ"><span class="header-section-number">1.2.2</span> 無向グラフ</h4>
<p>Markov 確率場などの無向グラフィカルモデルで定義される分布族や，正規化定数を除いて定義された確率分布族は，自然に EBM とみなせる．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="citation" data-cites="Hinton-Teh2001">(<a href="#ref-Hinton-Teh2001" role="doc-biblioref">Hinton and Teh, 2001</a>)</span> で論じられている通り，音声や画像などのデータに対する独立成分分析では，DAG などの有向グラフによるモデリングでは正確な推論ができないため，無向グラフによるより良いモデリング法が <strong>積スパートモデル</strong> (PoE: Product of Experts) などを通じて探求されていた．</p>
</section>
<section id="boltzmann-機械" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="boltzmann-機械"><span class="header-section-number">1.2.3</span> Boltzmann 機械</h4>
<p>例えば Ising 模型の一種でもある <a href="../../../posts/2024/Kernels/Deep.html#sec-Hopfield">Hopfield ネットワーク</a> <span class="citation" data-cites="Hopfield1982">(<a href="#ref-Hopfield1982" role="doc-biblioref">Hopfield, 1982</a>)</span> と Boltzmann 機械 <span class="citation" data-cites="Ackley+1985">(<a href="#ref-Ackley+1985" role="doc-biblioref">Ackley et al., 1985</a>)</span> は最も有名な EBM の１つである．<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>しかしこの場合，分配関数 <span class="math inline">\(Z_\theta\)</span> の計算が難しく（ほとんどの統計力学模型の中心問題である），EBM を訓練することが難しかった．</p>
<p>このようなモデルに対する最初の実用的な訓練手法（というより SGD の目的関数）は，Boltzmann 機械に対して与えられた Contrastive Divergence <span class="citation" data-cites="Hinton2002">(<a href="#ref-Hinton2002" role="doc-biblioref">Hinton, 2002</a>)</span> であった．</p>
<p>これ以降正規化定数が不明である模型も効率的に訓練可能であることが周知され，ICA に応用した論文 <span class="citation" data-cites="Teh+2003">(<a href="#ref-Teh+2003" role="doc-biblioref">Teh et al., 2003</a>)</span> で “Energy-based Model” の名前が造語された．</p>
</section>
<section id="深層化" class="level4" data-number="1.2.4">
<h4 data-number="1.2.4" class="anchored" data-anchor-id="深層化"><span class="header-section-number">1.2.4</span> 深層化</h4>
<p>制約付き Boltzmann 機械に対する greedy, layer-by-layer の事前学習を取り入れることで，深層化しても訓練が効率的に行われるようになった <span class="citation" data-cites="Hinton+2006">(<a href="#ref-Hinton+2006" role="doc-biblioref">Hinton et al., 2006</a>)</span>, <span class="citation" data-cites="Hinton-Salakhutdinov2006">(<a href="#ref-Hinton-Salakhutdinov2006" role="doc-biblioref">Hinton and Salakhutdinov, 2006</a>)</span>．</p>
<p>しかしこの方法により訓練されたモデルは深層信念機械というべきものであり，一般的な Boltmzmann 機械とは違った．深層 Boltzmann 機械の訓練法は <span class="citation" data-cites="Salakhutdinov-Hinton2009">(<a href="#ref-Salakhutdinov-Hinton2009" role="doc-biblioref">Salakhutdinov and Hinton, 2009</a>)</span> で確立された．</p>
<p>画像生成の分野においても，生成 CNN (generative ConvNet) <span class="citation" data-cites="Xie+2016">(<a href="#ref-Xie+2016" role="doc-biblioref">Xie et al., 2016</a>)</span> 以降，<span class="math inline">\(H_\theta\)</span> のモデリングには深層の <a href="../../../posts/2024/Kernels/Deep.html#sec-CNN">CNN</a> が用いられるようになっていった．</p>
</section>
</section>
<section id="分配関数" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="分配関数"><span class="header-section-number">1.3</span> 分配関数</h3>
<p>エネルギーベースモデルでは，分布は正規化定数を除いて定まっており， <span class="math display">\[
Z_\theta:=\int e^{-H(x,\theta)}dx
\]</span> も既知ではないとする．この点が EBM の表現力を支える自由度の高さであるが，<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> EBM の訓練にあたっては <span class="math inline">\(Z_\theta\)</span> の推定というタスクが増える．</p>
<p>これが <a href="../../../posts/2024/Kernels/Deep2.html">自己回帰モデル</a> や <a href="../../../posts/2024/Samplers/NF.html">フローベースモデル</a> との最大の違いである．</p>
<p>また，<a href="../../../posts/2024/Kernels/Deep3.html">GAN</a> や <a href="../../../posts/2024/Kernels/Deep4.html">VAE</a> も含めた生成モデルでは，生成のタスクが念頭にあるためにモデルを有向グラフによって表現するが，EBM はより一般の状況も考える広いクラスだと言える．</p>
<p><span class="math inline">\(Z_\theta\)</span> の推定という追加のタスクには，典型的には MCMC が用いられ，最尤推定 <a href="#sec-ML" class="quarto-xref">2</a> が完遂させられるが，近年スコアマッチング（第 <a href="#sec-SM" class="quarto-xref">3</a> 節）や <a href="../../../posts/2024/Kernels/NCL.html">ノイズ対照学習</a> などの新たな手法が提案されている．</p>
</section>
<section id="エネルギー" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="エネルギー"><span class="header-section-number">1.4</span> エネルギー</h3>
<p>仮に，２つの学習済みモデル <span class="math inline">\(p_1(x),p_2(x)\)</span> が EBM の形で得られており，それぞれのエネルギーが <span class="math inline">\(H_1,H_2\)</span> で与えられるとする．この際，<span class="math inline">\(H:=H_1+H_2\)</span> をエネルギーにもつ EBM は <span class="math inline">\(p\,\propto\,p_1p_2\)</span> であり，このモデルは <span class="math inline">\(p_1\)</span> でも <span class="math inline">\(p_2\)</span> でも高確率であるような <span class="math inline">\(x\)</span> を高く評価することになる．</p>
<p>このようにして得るモデル <span class="math inline">\(p\)</span> を <strong>積スパートモデル</strong> (PoE: Product of Experts) <span class="citation" data-cites="Hinton2002">(<a href="#ref-Hinton2002" role="doc-biblioref">Hinton, 2002</a>)</span> という．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="citation" data-cites="Hinton2002">(<a href="#ref-Hinton2002" role="doc-biblioref">Hinton, 2002</a>)</span> は引き続き，対照分離度 (contrast divergence) <a href="#sec-CD" class="quarto-xref">2.3</a> による訓練法を提案している．</p>
<p>例えば <span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 840</a>)</span> では，タンパク質構造の生成モデルを作りたいとして，<span class="math inline">\(p_1\)</span> を「常温で安定であるタンパク質」の生成モデル，<span class="math inline">\(p_2\)</span> を「COVID-19 のスパイクタンパク質に結合するタンパク質」の生成モデルとして説明している．</p>
<p>従ってこの <span class="math inline">\(H\)</span> は，データの好ましさを表すパラメータと考えられ，他モデルへの移転にも使えることが期待される．<span class="math inline">\(H\)</span> は，contrast function, value function, 負の対数尤度などとも呼ぶ <span class="citation" data-cites="LeCun+2007">(<a href="#ref-LeCun+2007" role="doc-biblioref">LeCun et al., 2007, p. 193</a>)</span>．</p>
</section>
<section id="応用" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="応用"><span class="header-section-number">1.5</span> 応用</h3>
<p>２次元の密度推定に対して，FCE (Flow Contrastive Estimation, 第 <a href="#sec-FCE" class="quarto-xref">4.5</a> 節) により学習させた EBM は，<a href="../../../posts/2024/Samplers/NF.html">正規化流</a> よりも遥かに小さいネットワークサイズで高い性能を示した <span class="citation" data-cites="Gao+2020">(<a href="#ref-Gao+2020" role="doc-biblioref">Gao et al., 2020</a>)</span>．</p>
</section>
<section id="推定手法" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="推定手法"><span class="header-section-number">1.6</span> 推定手法</h3>
<p>Contrastive Divergence <a href="#sec-CD" class="quarto-xref">2.3</a> は漸近的に消えないバイアスを持つ <span class="citation" data-cites="Perpinan-Hinton2005">(<a href="#ref-Perpinan-Hinton2005" role="doc-biblioref">Carreira-Perpiñán and Hinton, 2005</a>)</span> が，SM <a href="#sec-SM" class="quarto-xref">3</a> と NCE <a href="#sec-NCE" class="quarto-xref">4</a> は一致推定量である．</p>
<section id="cd" class="level4" data-number="1.6.1">
<h4 data-number="1.6.1" class="anchored" data-anchor-id="cd"><span class="header-section-number">1.6.1</span> CD</h4>
<p>CD <a href="#sec-CD" class="quarto-xref">2.3</a> は最尤推定を SGD によって実行するために建てられた代理目標である．</p>
<p>近似が入っているために消えないバイアスがある．</p>
</section>
<section id="スコアマッチング" class="level4" data-number="1.6.2">
<h4 data-number="1.6.2" class="anchored" data-anchor-id="スコアマッチング"><span class="header-section-number">1.6.2</span> スコアマッチング</h4>
<p>スコアマッチング（第 <a href="#sec-SM" class="quarto-xref">3</a> 節）のアイデアは，データ分布の密度 <span class="math inline">\(p(x)\)</span> とモデル <span class="math inline">\(p_\theta(x)\)</span> とのスコア関数をマッチングさせることを狙う： <span class="math display">\[
\nabla_x\log p(x)=\nabla_x\log p_\theta(x).
\]</span> このとき，両辺を積分すると，正規化条件から <span class="math inline">\(p(x)=p_\theta(x)\)</span> が従う．</p>
<p>この考え方は EBM だけでなく，尤度の評価は困難でも対数尤度の評価は可能である文脈で普遍的に有効である．</p>
<p>例えば <a href="../../../posts/2024/Kernels/Deep4.html#sec-AE">自己符号化器</a> においても用いられるアイデアである <span class="citation" data-cites="Vincent2011">(<a href="#ref-Vincent2011" role="doc-biblioref">Vincent, 2011</a>)</span>, <span class="citation" data-cites="Swersku+2011">(<a href="#ref-Swersku+2011" role="doc-biblioref">Swersky et al., 2011</a>)</span>．</p>
</section>
<section id="nce" class="level4" data-number="1.6.3">
<h4 data-number="1.6.3" class="anchored" data-anchor-id="nce"><span class="header-section-number">1.6.3</span> NCE</h4>
<p>NCE <a href="#sec-NCE" class="quarto-xref">4</a> は，表現学習や深層距離学習で用いられる <a href="../../../posts/2024/Kernels/Kernel.html#sec-deep-metric-learning">対照学習</a> を，特にノイズと対照させることで最尤推定に応用したものである．</p>
</section>
</section>
</section>
<section id="sec-ML" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-ML"><span class="header-section-number">2</span> 最尤推定</h2>
<section id="はじめに-1" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="はじめに-1"><span class="header-section-number">2.1</span> はじめに</h3>
<p>EBM は，データの分布との KL 乖離度，または等価なことだが対数尤度の期待値 <span class="math display">\[
\operatorname{E}[\log p_\theta]
\]</span> を最小化することによって学習することが考えられる <span class="citation" data-cites="LeCun+2007">(<a href="#ref-LeCun+2007" role="doc-biblioref">LeCun et al., 2007</a>)</span>．</p>
<p>しかし尤度の評価は正規化定数 <span class="math inline">\(Z_\theta:=\int e^{-H_\theta(x)}\,dx\)</span> の評価が必要であるため，一般の設定では実行できないが，勾配 <span id="eq-gradient"><span class="math display">\[
\nabla_\theta\log p_\theta(x)=-\nabla_\theta H_\theta(x)-\nabla_\theta\log Z_\theta
\tag{1}\]</span></span> は近似できる．<span class="math inline">\(\nabla_\theta H_\theta(x)\)</span> はニューラルネットワークの自動微分で計算することができ，２項目は <span class="math display">\[
\nabla_\theta\log Z_\theta=-(p_\theta|\nabla_\theta H_\theta)
\]</span> の関係を用いて，<span class="math inline">\(p_\theta\)</span> からのサンプルを用いた Monte Carlo 推定量で評価できる <span class="citation" data-cites="Younes1999">(<a href="#ref-Younes1999" role="doc-biblioref">Younes, 1999</a>)</span>．</p>
<p>正規化定数の不明な密度 <span class="math inline">\(p_\theta\)</span> からのサンプリングといえば，MCMC と SMC である．この Monte Carlo 近似を通じて，確率的勾配降下法によって最尤推定が実行できる．</p>
</section>
<section id="確率勾配-langevin-動力学-sgld-welling-teh2011" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="確率勾配-langevin-動力学-sgld-welling-teh2011"><span class="header-section-number">2.2</span> 確率勾配 Langevin 動力学 (SGLD) <span class="citation" data-cites="Welling-Teh2011">(<a href="#ref-Welling-Teh2011" role="doc-biblioref">Welling and Teh, 2011</a>)</span></h3>
<p>この際の Monte Carlo 推定には，多少バイアスがあっても高速に収束してくれる MCMC が欲しい．そこで提案されたのが <strong>確率勾配 Langevin 動力学</strong> である．</p>
<p>これは <span class="citation" data-cites="Hyvarinen2005">(<a href="#ref-Hyvarinen2005" role="doc-biblioref">Hyvärinen, 2005</a>)</span> のスコア関数<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span id="eq-score"><span class="math display">\[
\nabla_x\log p_\theta(x)=-\nabla_x H_\theta(x)
\tag{2}\]</span></span> の値から情報を得て，<span class="math inline">\(x\)</span> の空間上で効率的な Markov 連鎖のダイナミクスを構成する方法である．</p>
<p><span class="math inline">\(H_\theta\)</span> の勾配が急であればあるほど高速に収束するが，<a href="../../../posts/2024/Slides/ZigZagSampler.html">Zig-Zag サンプラー</a> などの PDMP 手法の方が高速に収束する．</p>
</section>
<section id="sec-CD" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="sec-CD"><span class="header-section-number">2.3</span> 対照分離度 (CD)</h3>
<p>SGD の各ステップに Monte Carlo 推定が必要であるが，毎度 MCMC を十分な数だけ回して，十分に分散が小さい勾配の推定量を得る必要はない．</p>
<p>このように，系統的に MCMC を打ち切って，手早く計算された勾配の推定量を通じて SGD により最尤推定を行う方法 (short-run MCMC) の代表的なものに，対照分離度 (CD: Contrastive Divergence) <span class="citation" data-cites="Hinton2002">(<a href="#ref-Hinton2002" role="doc-biblioref">Hinton, 2002</a>)</span> がある．</p>
<p>CD による訓練では，バッチごとに提供された訓練データ <span class="math inline">\(x_n\)</span> を開始地点として，一定の回数 <span class="math inline">\(T\)</span> だけ MCMC を回す．多くの場合 <span class="math inline">\(T=1\)</span> でさえある (CD-1 algorithm)．</p>
<section id="rbm-での-cd-の例" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="rbm-での-cd-の例"><span class="header-section-number">2.3.1</span> RBM での CD の例</h4>
<p><span class="citation" data-cites="Hinton2002">(<a href="#ref-Hinton2002" role="doc-biblioref">Hinton, 2002</a>)</span> は Gibbs サンプリングが可能な潜在変数を持つ EBM モデルである制限付き Boltzmann マシン (RBM) について CD を提案した．</p>
<p>特に簡単な binary RBM は，次のエネルギーが定める Markov 確率場である： <span class="math display">\[
H_w(x,z)=\sum_{d\in[D],k\in[K]}x_dz_kW_{dk}+\sum_{d=1}^Dx_db_d+\sum_{k=1}^KZ_kx_k.
\]</span> 観測 <span class="math inline">\(x_d\)</span> を入力して学習させたとき，<span class="math inline">\(z_k\)</span> も似たような値だった場合，それが強化されるように <span class="math inline">\(W_{dk}\)</span> が更新されるというように，Hebb 則に則った学習が行われる．</p>
<p>このとき， <span class="math display">\[
\frac{\partial }{\partial w_{dk}} H_w(x,z)=z_dz_k
\]</span> より，対数尤度の勾配 (<a href="#eq-gradient" class="quarto-xref">1</a>) の期待値は <span class="math display">\[
\operatorname{E}[\nabla_w\log p_w(x)]=-\operatorname{E}[xz^\top]-(p_\theta(x)|xz^\top).
\]</span></p>
<p>第一項はデータ <span class="math inline">\(x_n\)</span> に対して <span class="math inline">\(x_n\operatorname{E}[z|x_n,W]^\top\)</span> によって，第二項は <span class="math inline">\(x_n\)</span> を初期値として <span class="math inline">\(T\)</span> 回 MCMC を回して得られたサンプル <span class="math inline">\(x_n'\)</span> を用いて <span class="math inline">\(x_n'\operatorname{E}[z|x_n',W]^\top\)</span> によって推定される．</p>
<p>この手続きは，<span class="math inline">\(p_0\)</span> をデータ <span class="math inline">\(\{x_i\}_{i=1}^n\)</span> の分布として， <span class="math display">\[\newcommand{\CD}{\operatorname{CD}}
\CD_T:=\operatorname{KL}(p_0,p_\infty)-\operatorname{KL}(p_T,p_\infty)
\]</span> を最小化することに相当している．</p>
<!-- すなわち，データ点から初めて，MCMC を $T$ ステップ実行することに相当する行為を，$\CD_T$ の最小化として捉え直し，確率的勾配降下法によって学習する． -->
</section>
<section id="pcd効率的な不偏推定を目指して" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="pcd効率的な不偏推定を目指して"><span class="header-section-number">2.3.2</span> PCD：効率的な不偏推定を目指して</h4>
<p>データ点を取り替えるごとに <span class="math inline">\(p_0\)</span> を取り替えるのではなく，<span class="math inline">\(p_0\)</span> を以前の MCMC の終わり値から定めた場合の CD の変形を <strong>PCD (Persistent Contrastive Divergence)</strong> <span class="citation" data-cites="Tieleman2008">(<a href="#ref-Tieleman2008" role="doc-biblioref">Tieleman, 2008</a>)</span>, <span class="citation" data-cites="Tieleman-Hinton2009">(<a href="#ref-Tieleman-Hinton2009" role="doc-biblioref">Tieleman and Hinton, 2009</a>)</span> という．</p>
<p>確かにバッチごとに <span class="math inline">\(\theta\)</span> がアップデートされるため，MCMC の目標分布 <span class="math inline">\(p_\theta\)</span> は取り替える必要があるから，CD-<span class="math inline">\(T\)</span> のように毎回新たな MCMC を回す必要があるように思えるかもしれない．しかし，<span class="math inline">\(\theta\)</span> の更新は総じて大変小さなものであるとすると，真のモデル分布 <span class="math inline">\(p_\theta\)</span> からはずれていくかもしれないが，１つの収束した MCMC からサンプリングし続けた方が良い可能性がある．</p>
<p>更に，完全に同じ MCMC を走らせ続けるというところから，リサンプリングを取り入れて <span class="math inline">\(\theta\)</span> のアップデートに<u>収束性を保ちながら</u>対応することで，GAN に匹敵する性能と，分布の峰を正確に再現できるという GAN にはない美点を獲得できるという <span class="citation" data-cites="Du-Mordatch2019">(<a href="#ref-Du-Mordatch2019" role="doc-biblioref">Du and Mordatch, 2019</a>)</span>．</p>
</section>
<section id="mcmc-による生成" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="mcmc-による生成"><span class="header-section-number">2.3.3</span> MCMC による生成</h4>
<p>一方で，正しい <span class="math inline">\(p_\theta\)</span> によく収束する short-run MCMC が CD 法により訓練できたならば，これは効率的な生成モデルとなるかもしれない．</p>
<p><span class="citation" data-cites="Nijkaml+2019">(<a href="#ref-Nijkaml+2019" role="doc-biblioref">Nijkamp et al., 2019</a>)</span> は，MCMC が EBM モデルに対置されている analysis by synthesis スキーム <span class="citation" data-cites="Grenander-Miller1994">(<a href="#ref-Grenander-Miller1994" role="doc-biblioref">Grenander and Miller, 1994</a>)</span> と見て，この short-run MCMC をよく学ぶことに特化したアプローチ Short-Run MCMC as Generator or Flow Model を提案した．</p>
<p>このアプローチでは，MCMC は毎回同じ初期分布（ノイズの分布）からスタートさせ，<span class="math inline">\(T\)</span> の値も固定する．このようなスキームで学習された EBM は全く良い性能を持たないが，EBM から返ってくる Hyvärinen スコアを持った勾配 MCMC 法は，生成モデルとして良い性能を持つという <span class="citation" data-cites="Nijkaml+2019">(<a href="#ref-Nijkaml+2019" role="doc-biblioref">Nijkamp et al., 2019</a>)</span>．</p>
</section>
<section id="安定した-cd-訓練に向けて" class="level4" data-number="2.3.4">
<h4 data-number="2.3.4" class="anchored" data-anchor-id="安定した-cd-訓練に向けて"><span class="header-section-number">2.3.4</span> 安定した CD 訓練に向けて</h4>
<p><span class="citation" data-cites="Du+2021">(<a href="#ref-Du+2021" role="doc-biblioref">Du et al., 2021</a>)</span> は，この MCMC に情報を与える Hyvärinen スコアの変化が CD 訓練の重要な要素であり，これを正確に扱うことがを安定化させることを報告した．</p>
<p>特にこれは，従来 CD フレームワークの目的関数が捨象していた「第三項」 <span class="math display">\[
\frac{\partial q_\theta}{\partial \theta}\frac{\partial \operatorname{KL}(q_\theta,p_\theta)}{\partial q_\theta}
\]</span> を目的関数に含めることとして捉えられる <span class="citation" data-cites="Du+2021">(<a href="#ref-Du+2021" role="doc-biblioref">Du et al., 2021</a>)</span>．ただし，<span class="math inline">\(q_\theta\)</span> は真のデータ分布を初期分布として <span class="math inline">\(T\)</span> ステップ MCMC を実行して得られる分布とした．</p>
</section>
<section id="adversarial-cd" class="level4" data-number="2.3.5">
<h4 data-number="2.3.5" class="anchored" data-anchor-id="adversarial-cd"><span class="header-section-number">2.3.5</span> adversarial CD</h4>
<p>尤度 <span class="math inline">\(p_\theta\)</span> の評価を迂回するため，<a href="../../../posts/2024/Kernels/Deep3.html">GAN</a> にヒントを得た <span class="citation" data-cites="Finn+2016">(<a href="#ref-Finn+2016" role="doc-biblioref">Finn et al., 2016</a>)</span>，２つのネットワークを対置させて行う敵対的な学習も考えられている <span class="citation" data-cites="Kim-Bengio2016">(<a href="#ref-Kim-Bengio2016" role="doc-biblioref">Kim and Bengio, 2016</a>)</span>．</p>
</section>
</section>
</section>
<section id="sec-SM" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-SM"><span class="header-section-number">3</span> スコアマッチング <span class="citation" data-cites="Hyvarinen2005">(SM, <a href="#ref-Hyvarinen2005" role="doc-biblioref">Hyvärinen, 2005</a>)</span></h2>
<section id="はじめに-2" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="はじめに-2"><span class="header-section-number">3.1</span> はじめに</h3>
<p>EBM のスコア関数 (<a href="#eq-score" class="quarto-xref">2</a>) を <span class="math display">\[
s_\theta(x):=\nabla_x\log p_\theta(x)=-\nabla_xH_\theta(x)\tag{2}
\]</span> と定め，データ分布 <span class="math inline">\(p\)</span> との Fisher 乖離度を最小化するスキームが <span class="citation" data-cites="Hyvarinen2005">(<a href="#ref-Hyvarinen2005" role="doc-biblioref">Hyvärinen, 2005</a>)</span> の <strong>スコアマッチング</strong> 目的関数である： <span id="eq-SM-objective"><span class="math display">\[
D_F(p,p_\theta)=\frac{\|s-s_\theta\|_{L^2(p)}^2}{2}=\operatorname{E}\left[\frac{1}{2}\biggl|\nabla_x\log p(X)-\nabla_x\log p_\theta(X)\biggr|^2\right].
\tag{3}\]</span></span></p>
<p>この際，データ分布のスコア <span class="math inline">\(s(x)=\nabla_x\log p(x)\)</span> の計算を回避することが焦点になる．</p>
</section>
<section id="sec-Hyvarinen2005" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-Hyvarinen2005"><span class="header-section-number">3.2</span> 部分積分 <span class="citation" data-cites="Hyvarinen2005">(<a href="#ref-Hyvarinen2005" role="doc-biblioref">Hyvärinen, 2005</a>)</span></h3>
<p>次が成り立つことがあり得る： <span class="math display">\[\begin{align*}
    D_F(p,p_\theta)&amp;=\operatorname{E}\left[\frac{1}{2}\lvert s_\theta(X)\rvert^2+\operatorname{div}(s_\theta(X))\right]+(\theta\;\text{に依らない定数})\\
    &amp;=\operatorname{E}\left[\frac{1}{2}\lvert\nabla H_\theta(X)\rvert^2-\mathop{}\!\mathbin\bigtriangleup H_\theta(X)\right]+(\theta\;\text{に依らない定数})
\end{align*}\]</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled" title="証明">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
証明
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align*}
    D_F(p,p_\theta)&amp;=\operatorname{E}\left[\frac{1}{2}\biggl|\nabla_x\log p(x)-\nabla_x\log p_\theta(x)\biggr|^2\right]\\
    &amp;=\frac{1}{2}\int_{\mathbb{R}^d}\sum_{i=1}^d\left(\frac{1}{p(x)}\frac{\partial p}{\partial x_i}(x)-s_\theta(x)_i\right)^2p(x)\,dx\\
    &amp;=\frac{1}{2}\int_{\mathbb{R}^d}\frac{\lvert Dp\rvert^2}{p}\,dx-\sum_{i=1}^d\int_{\mathbb{R}^d}s_\theta(x)_i\frac{\partial p}{\partial x_i}(x)+\frac{1}{2}\int_{\mathbb{R}^d}\lvert s_\theta(x)\rvert^2p(x)\,dx\\
    &amp;=\mathrm{const.}+\int_{\mathbb{R}^d}\operatorname{Tr}(Ds_\theta)p\,dx+\operatorname{E}\left[\frac{1}{2}\lvert s_\theta(x)\rvert^2\right]\\
    &amp;=\operatorname{E}\left[\frac{1}{2}\lvert s_\theta(x)\rvert^2+\operatorname{div}(s_\theta(x))\right]+\mathrm{const.}
\end{align*}\]</span></p>
</div>
</div>
</div>
<p>この右辺はデータのスコアを含まないので，<span class="math inline">\(p_\theta\)</span> の２階微分が計算可能ならば計算できるが，データの次元 <span class="math inline">\(d\)</span> に関するスケールは悪い．</p>
<p>加えて，<span class="math inline">\(D_F(p,p_\theta)\)</span> をここから <span class="math inline">\(\theta\)</span> に関して微分することが困難である．独立成分分析モデルを除いて，<span class="citation" data-cites="Koster-Hyvarinen2007">(<a href="#ref-Koster-Hyvarinen2007" role="doc-biblioref">Köster and Hyvärinen, 2007</a>)</span>, <span class="citation" data-cites="Koster+2009">(<a href="#ref-Koster+2009" role="doc-biblioref">Köster et al., 2009</a>)</span> などのニューラルネットワークモデルにも応用されたが，画像などの実データに直接適用することは困難であった．特に，<span class="math inline">\(H_\theta\)</span> の勾配と Laplacian を解析的に計算してから実装していた．</p>
</section>
<section id="sec-RSM" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="sec-RSM"><span class="header-section-number">3.3</span> 正則化スコアマッチング (RSM) <span class="citation" data-cites="Kingma-LeCun2010">(<a href="#ref-Kingma-LeCun2010" role="doc-biblioref">Kingma and LeCun, 2010</a>)</span></h3>
<p>画像データなどの量子化されたデータの密度 <span class="math inline">\(p(x)\)</span> は可微分ではない上に，有限な台を持ってしまうためスコア <span class="math inline">\(s\)</span> は well-defined ではない．</p>
<p>このような量子化されたデータ <span class="math inline">\(x\)</span> に対して，Gauss ノイズを加えたもの <span class="math display">\[
\widetilde{x}=x+\epsilon,\qquad\epsilon\sim\mathrm{N}(0,\sigma^2I)
\]</span> は連続なデータに変貌する（Gauss 核が軟化子として働く）．</p>
<p>この <span class="math inline">\(\epsilon\)</span> だけ摂動されたデータの分布 <span class="math inline">\(\widetilde{p}\)</span> に対して，スコアマッチング目的関数 (<a href="#eq-SM-objective" class="quarto-xref">3</a>) は <span class="math display">\[\begin{align*}
    D_F(\widetilde{p},p_\theta)&amp;=\operatorname{E}\left[\frac{1}{2}\lvert\nabla H_\theta\rvert^2-\mathop{}\!\mathbin\bigtriangleup H_\theta+\frac{\sigma^2}{2}\|D^2H_\theta\|^2_2\right]+O(\epsilon^2)\\
    &amp;=D_F(p,p_\theta)+\frac{\sigma^2}{2}\operatorname{E}\biggl[\|D^2H_\theta\|_2^2\biggr]+O(\epsilon^2)
\end{align*}\]</span> と表せる <span class="citation" data-cites="Kingma-LeCun2010">(<a href="#ref-Kingma-LeCun2010" role="doc-biblioref">Kingma and LeCun, 2010</a>)</span>．</p>
<p>この <span class="math inline">\(D_F\)</span> は，Hessian <span class="math inline">\(D^2H\)</span> の非対角項を対角項で近似し，結局は元の目的関数に対して正則化項 <span class="math display">\[
\lambda\mathop{}\!\mathbin\bigtriangleup H_\theta,\qquad\lambda\approx\frac{\sigma^2}{2}
\]</span> を加えたもので近似できる．</p>
<p>これを目的関数の用いる方法が <strong>正則化スコアマッチング</strong> (RSM: Regularized Score Matching) <span class="citation" data-cites="Kingma-LeCun2010">(<a href="#ref-Kingma-LeCun2010" role="doc-biblioref">Kingma and LeCun, 2010</a>)</span> である．</p>
<p>さらに <span class="citation" data-cites="Kingma-LeCun2010">(<a href="#ref-Kingma-LeCun2010" role="doc-biblioref">Kingma and LeCun, 2010</a>)</span> は，従来のように，解析的に微分が計算可能な <span class="math inline">\(H_\theta\)</span> を採用するのではなく，誤差逆伝播を２回行う Double Backpropagation <span class="citation" data-cites="Drucker-LeCun1992">(<a href="#ref-Drucker-LeCun1992" role="doc-biblioref">Drucker and Le Cun, 1992</a>)</span> によってこの目的関数の勾配 <span class="math inline">\(\frac{d D_F(p,p_\theta)}{d \theta}\)</span> を自動微分で計算する方法を提案した．</p>
</section>
<section id="sec-DSM" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="sec-DSM"><span class="header-section-number">3.4</span> Denoising スコアマッチング (DSM) <span class="citation" data-cites="Vincent2011">(<a href="#ref-Vincent2011" role="doc-biblioref">Vincent, 2011</a>)</span></h3>
<p>RSM の場合と違い，次のような表示もできる <span class="citation" data-cites="Vincent2011">(<a href="#ref-Vincent2011" role="doc-biblioref">Vincent, 2011</a>)</span>： <span class="math display">\[
D_F(\widetilde{p},p_\theta)=\frac{1}{2}\operatorname{E}\left[\left\|s_\theta(\widetilde{X})-\frac{X-\widetilde{X}}{\sigma^2}\right\|^2_2\right]+\mathrm{const.}
\]</span></p>
<p>すなわち，ノイズ消去方向のベクトル <span class="math inline">\(\frac{x-\widetilde{x}}{\sigma^2}\)</span> に一致するようにモデルのスコア <span class="math inline">\(s_\theta\)</span> を学習することが考えられる．</p>
<p>こうすることで，<span class="math inline">\(D^2H_\theta\)</span> の計算を回避することができる．</p>
<p>ただし，RSM と DSM に共通することであるが，あくまで <span class="math inline">\(D_F(\widetilde{p},p_\theta)\)</span> はノイズの入ったデータ分布を学習してしまうのであり，<span class="math inline">\(\sigma\to0\)</span> が志向されるが，<span class="math inline">\(\sigma\)</span> が小さいほど <span class="math inline">\(D_F(\widetilde{p},p_\theta)\)</span> に関する推定は不安定になる．<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
</section>
<section id="sec-SSM" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-SSM"><span class="header-section-number">3.5</span> スライススコアマッチング (SSM) <span class="citation" data-cites="Song+2019">(<a href="#ref-Song+2019" role="doc-biblioref">Song et al., 2019</a>)</span></h3>
<p>SSM (Sliced Score Matching) <span class="citation" data-cites="Song+2019">(<a href="#ref-Song+2019" role="doc-biblioref">Song et al., 2019</a>)</span> ではスコアマッチング目的関数 (<a href="#eq-SM-objective" class="quarto-xref">3</a>) 自体を，sliced Fisher 乖離度 <span class="math display">\[\begin{align*}
    D_{SF}(p,p_\theta)&amp;=\operatorname{E}\left[\frac{1}{2}\biggr(V^\top s(X)-V^\top s_\theta(X)\biggl)^2\right]\\
    &amp;=\operatorname{E}\left[\frac{1}{2}\sum_{i=1}^d\left(\frac{\partial H_\theta(X)}{\partial x_i}V_i\right)^2+\sum_{i,j=1}^d\frac{\partial ^2H_\theta(X)}{\partial x_i\partial x_j}V_iV_j\right]+\mathrm{const.}
\end{align*}\]</span> で Monte Carlo 近似する．ただし，<span class="math inline">\(V\)</span> はランダムな <span class="math inline">\(\mathbb{R}^d\)</span> 上のベクトルである．<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>やはり <span class="math inline">\(D^2H_\theta\)</span> が登場しているように思えるが， <span class="math display">\[
\sum_{i,j=1}^d\frac{\partial ^2H_\theta}{\partial x_i\partial x_j}V_iV_j=\sum_{i=1}^d\frac{\partial }{\partial x_i}V_i\sum_{j=1}^d\frac{\partial H_\theta}{\partial x_j}V_j=\sum_{i=1}^d\frac{\partial }{\partial x_i}V_i(DH_\theta|V)
\]</span> の表示により，一度内積 <span class="math inline">\((DH_\theta|V)\)</span> を計算してしまえば，以降，この項は自動微分を通じて <span class="math inline">\(O(d)\)</span> のオーダーで計算できる．</p>
<p>なお，<span class="math inline">\(V\sim\mathrm{N}_d(0,I_d)\)</span> と取った際の目的関数 <span class="math inline">\(D_{SF}\)</span> は，元の Fisher 乖離度による目的関数に対して，<a href="../../../posts/2024/Probability/Trace.html">Hutchinson の跡推定量</a>（<a href="../../../posts/2024/Samplers/NF.html#sec-Hutchinson">正規化流でも出てきた</a>）により Jacobian <span class="math inline">\(Ds_\theta\)</span> を推定したものとも同一視できる．</p>
<p>SSM の最大の美点は，ノイズが印加された分布 <span class="math inline">\(\widetilde{p}\)</span> を学習してしまう DSM と違って真の分布 <span class="math inline">\(p\)</span> を学習できることである．一方で，自動微分の分だけ，4倍ほど計算量が必要になる．</p>
</section>
<section id="sm-目的関数の理論" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="sm-目的関数の理論"><span class="header-section-number">3.6</span> SM 目的関数の理論</h3>
<p>スコアマッチングの目的関数 <span class="math inline">\(D_F(p,p_\theta)\)</span> は，Contrastive Divergence のある極限に一致する <span class="citation" data-cites="Hyvarinen2007">(<a href="#ref-Hyvarinen2007" role="doc-biblioref">Hyvarinen, 2007</a>)</span>．</p>
<p>ステップサイズ <span class="math inline">\(\epsilon&gt;0\)</span> の Langevin Monte Carlo 法により <span class="math inline">\(p_\theta\)</span> からサンプリングした場合の対数尤度の期待値は <span class="math display">\[
\operatorname{E}[\nabla_\theta\log p_\theta]=\frac{\epsilon^2}{2}\nabla_\theta D_F(p,p_\theta)+o(\epsilon^2)
\]</span> と Monte Carlo 近似されるという <span class="citation" data-cites="Hyvarinen2007">(<a href="#ref-Hyvarinen2007" role="doc-biblioref">Hyvarinen, 2007</a>)</span>．</p>
<p>ここで de Bruijin の関係 <span class="math display">\[
\frac{d }{d t}\operatorname{KL}(\widetilde{p}_t,\widetilde{p}_{\theta,t})=-\frac{1}{2}D_F(\widetilde{p}_t,\widetilde{p}_{\theta,t})
\]</span> に似た消息が生じている <span class="citation" data-cites="Lyu2009">(<a href="#ref-Lyu2009" role="doc-biblioref">Lyu, 2009</a>)</span>．なお，<span class="math inline">\(\widetilde{p}\)</span> とは，<span class="math inline">\(p,p_\theta\)</span> の分散 <span class="math inline">\(t^2\)</span> を持った Gaussian i.i.d. ノイズによる摂動とする．</p>
</section>
</section>
<section id="sec-NCE" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-NCE"><span class="header-section-number">4</span> ノイズ対照学習 <span class="citation" data-cites="Gutmann-Hyvarinen2010">(NCE, <a href="#ref-Gutmann-Hyvarinen2010" role="doc-biblioref">M. Gutmann and Hyvärinen, 2010</a>)</span></h2>
<section id="はじめに-3" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="はじめに-3"><span class="header-section-number">4.1</span> はじめに</h3>
<p>既知の密度 <span class="math inline">\(p_n\)</span> と対置させ，これをノイズとしてデータ分布 <span class="math inline">\(p\)</span> との識別を繰り返すことで学習を行う．</p>
<p><span class="math inline">\(Y\sim\mathrm{Ber}\left(\frac{\nu}{1+\nu}\right)\)</span> に関する混合を <span class="math display">\[
\widetilde{X}:=YX+(1-Y)N,\qquad N\sim p_n,X\sim p,
\]</span> と定め，これに対して混合分布族 <span class="math display">\[
p_{n,\theta}:=\frac{1}{1+\nu}p_n+\frac{\nu}{1+\nu}p_\theta
\]</span> を KL-乖離度の意味でマッチさせることを目指す．</p>
</section>
<section id="ノイズ-p_n-の選び方" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="ノイズ-p_n-の選び方"><span class="header-section-number">4.2</span> ノイズ <span class="math inline">\(p_n\)</span> の選び方</h3>
<p>重点サンプリング法の提案分布のように，<span class="math inline">\(p_n\)</span> は真のデータ分布 <span class="math inline">\(p\)</span> に近ければ近いほどよい <span class="citation" data-cites="Gutmann-Hirayama2011">(<a href="#ref-Gutmann-Hirayama2011" role="doc-biblioref">M. U. Gutmann and Hirayama, 2011</a>)</span>．</p>
<p>従って多くの方法では，<span class="math inline">\(p_n\)</span> を適応的に選ぶことが考えられる．</p>
</section>
<section id="スコアマッチングによる解釈" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="スコアマッチングによる解釈"><span class="header-section-number">4.3</span> スコアマッチングによる解釈</h3>
<p><span class="math inline">\(p_n\)</span> を <span class="math inline">\(p\)</span> の摂動 <span class="math display">\[
p_n(x):=p(x-v)
\]</span> とした場合，<span class="math inline">\(\|v\|_2\to0\)</span> の極限において，<span class="math inline">\(V\)</span> に関するスライススコアマッチングの目的関数 <a href="#sec-SSM" class="quarto-xref">3.5</a> に一致する <span class="citation" data-cites="Gutmann-Hirayama2011">(<a href="#ref-Gutmann-Hirayama2011" role="doc-biblioref">M. U. Gutmann and Hirayama, 2011</a>)</span>, <span class="citation" data-cites="Song+2019">(<a href="#ref-Song+2019" role="doc-biblioref">Song et al., 2019</a>)</span>．</p>
</section>
<section id="応用-1" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="応用-1"><span class="header-section-number">4.4</span> 応用</h3>
<p>ノイズ対照学習は，<a href="../../../posts/2024/Kernels/Kernel.html#sec-metric-learning">距離学習</a>や<a href="../../../posts/2024/Kernels/Manifold.html#sec-embedding">埋め込み</a>，特に word2vec <span class="citation" data-cites="Mikolov2013">(<a href="#ref-Mikolov2013" role="doc-biblioref">Mikolov, Chen, et al., 2013</a>)</span>, <span class="citation" data-cites="Mikolov2013b">(<a href="#ref-Mikolov2013b" role="doc-biblioref">Mikolov, Sutskever, et al., 2013</a>)</span> などの言語の埋め込みに応用される．</p>
</section>
<section id="sec-FCE" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="sec-FCE"><span class="header-section-number">4.5</span> フロー対照推定 <span class="citation" data-cites="Gao+2020">(FCE, <a href="#ref-Gao+2020" role="doc-biblioref">Gao et al., 2020</a>)</span></h3>
<p>ノイズ対照推定のノイズ分布を，<a href="../../../posts/2024/Samplers/NF.html">正規化流</a> を用いて敵対的に高難易度にしていく．この状況で，EBM とフローベースモデルを同時に学習することを考える手法である．</p>
</section>
</section>
<section id="その他の訓練方法" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="その他の訓練方法"><span class="header-section-number">5</span> その他の訓練方法</h2>
<section id="stein-乖離度" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="stein-乖離度"><span class="header-section-number">5.1</span> Stein 乖離度</h3>
<p>Stein 乖離度は，Fisher 乖離度の１つの <span class="math inline">\(s_\theta\)</span> を動かして得るダイバージェンスである： <span class="math display">\[
D_S(p,p_\theta):=\sup_{f\in\mathcal{F}}\operatorname{E}\left[s_\theta(X)^\top f(X)+\operatorname{div}(f(X))\right].
\]</span></p>
<p>Fisher 乖離度の難点は発散項の計算が <span class="math inline">\(O(d^2)\)</span> の計算量を持ってしまうことであった．Stein 乖離度はこれをカーネル法の方法で迂回することができる．</p>
<p><span class="math inline">\(\mathcal{F}\)</span> がある RKHS の単位閉球であった場合，発散項は定数になる <span class="citation" data-cites="Chwialkowski+2016">(<a href="#ref-Chwialkowski+2016" role="doc-biblioref">Chwialkowski et al., 2016</a>)</span>, <span class="citation" data-cites="Liu+2016">(<a href="#ref-Liu+2016" role="doc-biblioref">Q. Liu et al., 2016</a>)</span>．</p>
</section>
<section id="敵対的な訓練" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="敵対的な訓練"><span class="header-section-number">5.2</span> 敵対的な訓練</h3>
<p><span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023</a>)</span> 24.5.3 も参照．</p>
</section>
</section>



<div id="quarto-appendix" class="default"><section id="文献" class="level2 appendix" data-number="6"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">6</span> 文献</h2><div class="quarto-appendix-contents">

<p>Energy-based model は独立成分分析の研究 <span class="citation" data-cites="Teh+2003">(<a href="#ref-Teh+2003" role="doc-biblioref">Teh et al., 2003</a>)</span> において命名され，スコアマッチングの <span class="citation" data-cites="Hyvarinen2005">(<a href="#ref-Hyvarinen2005" role="doc-biblioref">Hyvärinen, 2005</a>)</span> も非線型独立成分分析の大家である．</p>
<p><a href="https://github.com/yataobian/awesome-ebm"><code>Awesome-EBM</code></a> レポジトリは，種々の EBM のリストを与えている．</p>
<p>系統的なイントロダクションには <span class="citation" data-cites="Bishop-Bishop2024">(<a href="#ref-Bishop-Bishop2024" role="doc-biblioref">Bishop and Bishop, 2024</a>)</span> 14.3節，<span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023</a>)</span> 24章が良い．</p>
<p>訓練方法について <span class="citation" data-cites="Song-Kingma2021">(<a href="#ref-Song-Kingma2021" role="doc-biblioref">Song and Kingma, 2021</a>)</span> が詳細なサーベイを与えている．上の２つの教科書の記述も多くはこのサーベイに基づいている．</p>
<p><span class="citation" data-cites="Gao+2020">(<a href="#ref-Gao+2020" role="doc-biblioref">Gao et al., 2020</a>)</span></p>




</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Ackley+1985" class="csl-entry" role="listitem">
Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). <a href="https://doi.org/10.1016/S0364-0213(85)80012-4">A learning algorithm for boltzmann machines</a>. <em>Cognitive Science</em>, <em>9</em>(1), 147–169.
</div>
<div id="ref-Bishop-Bishop2024" class="csl-entry" role="listitem">
Bishop, C. M., and Bishop, H. (2024). <em><a href="https://link.springer.com/book/10.1007/978-3-031-45468-4">Deep learning: Foundations and concepts</a></em>. Springer Cham.
</div>
<div id="ref-Carbone2024" class="csl-entry" role="listitem">
Carbone, D. (2024). <a href="https://arxiv.org/abs/2406.13661">Hitchhiker’s guide on energy-based models: A comprehensive review on the relation with other generative models, sampling and statistical physics</a>.
</div>
<div id="ref-Perpinan-Hinton2005" class="csl-entry" role="listitem">
Carreira-Perpiñán, M. Á., and Hinton, G. (2005). <a href="https://proceedings.mlr.press/r5/carreira-perpinan05a.html">On contrastive divergence learning</a>. In R. G. Cowell and Z. Ghahramani, editors, <em>Proceedings of the tenth international workshop on artificial intelligence and statistics</em>,Vol. R5, pages 33–40. PMLR.
</div>
<div id="ref-Chewi2024" class="csl-entry" role="listitem">
Chewi, S. (2024). <em><a href="https://chewisinho.github.io/">Log-concave sampling</a></em>.
</div>
<div id="ref-Chwialkowski+2016" class="csl-entry" role="listitem">
Chwialkowski, K., Strathmann, H., and Gretton, A. (2016). <a href="https://proceedings.mlr.press/v48/chwialkowski16.html">A kernel test of goodness of fit</a>. In M. F. Balcan and K. Q. Weinberger, editors, <em>Proceedings of the 33rd international conference on machine learning</em>,Vol. 48, pages 2606–2615. New York, New York, USA: PMLR.
</div>
<div id="ref-Drucker-LeCun1992" class="csl-entry" role="listitem">
Drucker, H., and Le Cun, Y. (1992). <a href="https://doi.org/10.1109/72.165600">Improving generalization performance using double backpropagation</a>. <em>IEEE Transactions on Neural Networks</em>, <em>3</em>(6), 991–997.
</div>
<div id="ref-Du+2021" class="csl-entry" role="listitem">
Du, Y., Li, S., Tenenbaum, J., and Mordatch, I. (2021). <a href="https://proceedings.mlr.press/v139/du21b.html">Improved contrastive divergence training of energy-based models</a>. In M. Meila and T. Zhang, editors, <em>Proceedings of the 38th international conference on machine learning</em>,Vol. 139, pages 2837–2848. PMLR.
</div>
<div id="ref-Du-Mordatch2019" class="csl-entry" role="listitem">
Du, Y., and Mordatch, I. (2019). <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf"><span class="nocase">Implicit Generation and Modeling with Energy Based Models</span></a>. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in neural information processing systems</em>,Vol. 32. Curran Associates, Inc.
</div>
<div id="ref-Finn+2016" class="csl-entry" role="listitem">
Finn, C., Christiano, P., Abbeel, P., and Levine, S. (2016). <a href="https://arxiv.org/abs/1611.03852">A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models</a>.
</div>
<div id="ref-Friedli-Velenik2017" class="csl-entry" role="listitem">
Friedli, S., and Velenik, Y. (2017). <em>Statistical mechanics of lattice systems</em>. Cambridge University Press.
</div>
<div id="ref-Gao+2020" class="csl-entry" role="listitem">
Gao, R., Nijkamp, E., Kingma, D. P., Xu, Z., Dai, A. M., and Wu, Y. N. (2020). <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Gao_Flow_Contrastive_Estimation_of_Energy-Based_Models_CVPR_2020_paper">Flow contrastive estimation of energy-based models</a>. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>.
</div>
<div id="ref-Grenander-Miller1994" class="csl-entry" role="listitem">
Grenander, U., and Miller, M. I. (1994). <a href="http://www.jstor.org/stable/2346184">Representations of knowledge in complex systems</a>. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>56</em>(4), 549–603.
</div>
<div id="ref-Gutmann-Hirayama2011" class="csl-entry" role="listitem">
Gutmann, M. U., and Hirayama, J. (2011). Bregman divergence as general framework to estimate unnormalized statistical models. In <em>Proceedings of the twenty-seventh conference on uncertainty in artificial intelligence</em>, pages 283–290. Arlington, Virginia, USA: AUAI Press.
</div>
<div id="ref-Gutmann-Hyvarinen2010" class="csl-entry" role="listitem">
Gutmann, M., and Hyvärinen, A. (2010). <a href="https://proceedings.mlr.press/v9/gutmann10a.html">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a>. In Y. W. Teh and M. Titterington, editors, <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em>,Vol. 9, pages 297–304. Chia Laguna Resort, Sardinia, Italy: PMLR.
</div>
<div id="ref-Hinton2002" class="csl-entry" role="listitem">
Hinton, G. E. (2002). <a href="https://doi.org/10.1162/089976602760128018"><span class="nocase">Training Products of Experts by Minimizing Contrastive Divergence</span></a>. <em>Neural Computation</em>, <em>14</em>(8), 1771–1800.
</div>
<div id="ref-Hinton+2006" class="csl-entry" role="listitem">
Hinton, G. E., Esindero, S., and Teh, Y.-W. (2006). <a href="https://direct.mit.edu/neco/article/18/7/1527/7065/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets">A fast learning algorithm for deep belief nets</a>. <em>Naural Computation</em>, <em>18</em>(7), 1527–1554.
</div>
<div id="ref-Hinton-Salakhutdinov2006" class="csl-entry" role="listitem">
Hinton, G. E., and Salakhutdinov, R. R. (2006). <a href="https://www.science.org/doi/10.1126/science.1127647">Reducing the dimensionality of data with neural networks</a>. <em>Science</em>, <em>313</em>(5786), 504–507.
</div>
<div id="ref-Hinton-Teh2001" class="csl-entry" role="listitem">
Hinton, G. E., and Teh, Y.-W. (2001). <a href="https://dl.acm.org/doi/abs/10.5555/2074022.2074051">Discovering multiple constraints that are frequently approximately satisfied</a>. In <em>Proceedings of the seventeenth conference on uncertainty in artificial intelligence</em>, pages 227–234. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
</div>
<div id="ref-Hopfield1982" class="csl-entry" role="listitem">
Hopfield, J. J. (1982). <a href="https://www.pnas.org/doi/10.1073/pnas.79.8.2554">Neural networks and physical systems with emergent collective computational abilities</a>. <em>Proceedings of the National Academy of Science</em>, <em>79</em>(8), 2554–2558.
</div>
<div id="ref-Hyvarinen2007" class="csl-entry" role="listitem">
Hyvarinen, A. (2007). <a href="https://doi.org/10.1109/TNN.2007.895819">Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables</a>. <em>IEEE Transactions on Neural Networks</em>, <em>18</em>(5), 1529–1531.
</div>
<div id="ref-Hyvarinen2005" class="csl-entry" role="listitem">
Hyvärinen, A. (2005). <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(24), 695–709.
</div>
<div id="ref-Jacobs+1991" class="csl-entry" role="listitem">
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). <a href="https://doi.org/10.1162/neco.1991.3.1.79">Adaptive mixtures of local experts</a>. <em>Neural Computation</em>, <em>3</em>(1), 79–87.
</div>
<div id="ref-Kim-Bengio2016" class="csl-entry" role="listitem">
Kim, T., and Bengio, Y. (2016). <a href="https://openreview.net/forum?id=BNYAGZZj5S7PwR1riXzA">Deep directed generative models with energy-based probability estimation</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Kingma-LeCun2010" class="csl-entry" role="listitem">
Kingma, D. P., and LeCun, Y. (2010). <a href="https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf">Regularized estimation of image statistics by score matching</a>. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, <em>Advances in neural information processing systems</em>,Vol. 23. Curran Associates, Inc.
</div>
<div id="ref-Koller-Friedman2009" class="csl-entry" role="listitem">
Koller, D., and Friedman, N. (2009). <em><a href="https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/">Probabilistic graphical models</a></em>. MIT Press.
</div>
<div id="ref-Koster-Hyvarinen2007" class="csl-entry" role="listitem">
Köster, U., and Hyvärinen, A. (2007). A two-layer ICA-like model estimated by score matching. In J. M. de Sá, L. A. Alexandre, W. Duch, and D. Mandic, editors, <em>Artificial neural networks – ICANN 2007</em>, pages 798–807. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-Koster+2009" class="csl-entry" role="listitem">
Köster, U., Lindgren, J. T., and Hyvärinen, A. (2009). Estimating markov random field potentials for natural images. In T. Adali, C. Jutten, J. M. T. Romano, and A. K. Barros, editors, <em>Independent component analysis and signal separation</em>, pages 515–522. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-LeCun+2007" class="csl-entry" role="listitem">
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M. A., and Huang, F. J. (2007). <a href="https://ieeexplore.ieee.org/document/6270201">Predicting structured data</a>. In G. Baklr, T. Hofmann, B. Schölkopf, A. J. Smola, B. Taskar, and S. V. N. Vishwanathan, editors, pages 191–246. The MIT Press.
</div>
<div id="ref-Liu2004" class="csl-entry" role="listitem">
Liu, J. S. (2004). <em><a href="https://doi.org/10.1007/978-0-387-76371-2">Monte carlo strategies in scientific computing</a></em>. Springer New York.
</div>
<div id="ref-Liu+2016" class="csl-entry" role="listitem">
Liu, Q., Lee, J., and Jordan, M. (2016). <a href="https://proceedings.mlr.press/v48/liub16.html">A kernelized stein discrepancy for goodness-of-fit tests</a>. In M. F. Balcan and K. Q. Weinberger, editors, <em>Proceedings of the 33rd international conference on machine learning</em>,Vol. 48, pages 276–284. New York, New York, USA: PMLR.
</div>
<div id="ref-Lyu2009" class="csl-entry" role="listitem">
Lyu, S. (2009). Interpretation and generalization of score matching. In <em>Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</em>, pages 359–366. Arlington, Virginia, USA: AUAI Press.
</div>
<div id="ref-Mezard-Montanari2009" class="csl-entry" role="listitem">
Mézard, M., and Montanari, A. (2009). <em><a href="https://doi.org/10.1093/acprof:oso/9780198570837.001.0001">Information, physics, and computation</a></em>. Oxford University Press.
</div>
<div id="ref-Mikolov2013" class="csl-entry" role="listitem">
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). <em><a href="https://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a></em>.
</div>
<div id="ref-Mikolov2013b" class="csl-entry" role="listitem">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). <a href="https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed representations of words and phrases and their compositionality</a>. In C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, <em>Advances in neural information processing systems</em>,Vol. 26. Curran Associates, Inc.
</div>
<div id="ref-Murphy2023" class="csl-entry" role="listitem">
Murphy, K. P. (2023). <em><a href="http://probml.github.io/book2">Probabilistic machine learning: Advanced topics</a></em>. MIT Press.
</div>
<div id="ref-Nijkaml+2019" class="csl-entry" role="listitem">
Nijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y. N. (2019). <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf">Learning non-convergent non-persistent short-run MCMC toward energy-based model</a>. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett, editors, <em>Advances in neural information processing systems</em>,Vol. 32. Curran Associates, Inc.
</div>
<div id="ref-Salakhutdinov-Hinton2009" class="csl-entry" role="listitem">
Salakhutdinov, R., and Hinton, G. (2009). <a href="https://proceedings.mlr.press/v5/salakhutdinov09a.html">Deep boltzmann machines</a>. In <em>Proceedings of the twelth international conference on artificial intelligence and statistics</em>,Vol. 5, pages 448–455.
</div>
<div id="ref-Song+2019" class="csl-entry" role="listitem">
Song, Y., Garg, S., Shi, J., and Ermon, S. (2019). <a href="https://arxiv.org/abs/1905.07088"><span class="nocase">Sliced Score Matching: A Scalable Approach to Density and Score Estimation</span></a>. In.
</div>
<div id="ref-Song-Kingma2021" class="csl-entry" role="listitem">
Song, Y., and Kingma, D. P. (2021). <a href="https://arxiv.org/abs/2101.03288">How to train your energy-based models</a>.
</div>
<div id="ref-Swersku+2011" class="csl-entry" role="listitem">
Swersky, K., Ranzato, M., Buchman, D., Marlin, B. M., and Freitas, N. (2011). <a href="https://dl.acm.org/doi/10.5555/3104482.3104633">On autoencoders and score matching for energy based models</a>. In <em>Proceedings of the 28th international conference on international conference on machine learning</em>, pages 1201–1208. Madison, WI, USA: Omnipress.
</div>
<div id="ref-Teh+2003" class="csl-entry" role="listitem">
Teh, Y. W., Welling, M., Osindero, S., and Hinton, G. E. (2003). <a href="https://www.jmlr.org/papers/v4/teh03a.html">Energy-based models for sparse overcomplete representations</a>. <em>Journal of Machine Learning Research</em>, <em>4</em>(null), 1235–1260.
</div>
<div id="ref-Tieleman2008" class="csl-entry" role="listitem">
Tieleman, T. (2008). <a href="https://doi.org/10.1145/1390156.1390290">Training restricted boltzmann machines using approximations to the likelihood gradient</a>. In <em>Proceedings of the 25th international conference on machine learning</em>, pages 1064–1071. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Tieleman-Hinton2009" class="csl-entry" role="listitem">
Tieleman, T., and Hinton, G. (2009). <a href="https://doi.org/10.1145/1553374.1553506">Using fast weights to improve persistent contrastive divergence</a>. In <em>Proceedings of the 26th annual international conference on machine learning</em>, pages 1033–1040. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Vincent2011" class="csl-entry" role="listitem">
Vincent, P. (2011). <a href="https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising">A connection between score matching and denoising autoencoders</a>. <em>Neural Computation</em>, <em>23</em>(7), 1661–1674.
</div>
<div id="ref-Welling-Teh2011" class="csl-entry" role="listitem">
Welling, M., and Teh, Y. W. (2011). <a href="https://dl.acm.org/doi/10.5555/3104482.3104568">Bayesian learning via stochastic gradient langevin dynamics</a>. In <em>Proceedings of the 28th international conference on international conference on machine learning</em>, pages 681–688. Madison, WI, USA: Omnipress.
</div>
<div id="ref-Xie+2016" class="csl-entry" role="listitem">
Xie, J., Lu, Y., Zhu, S.-C., and Wu, Y. (2016). <a href="https://proceedings.mlr.press/v48/xiec16.html">A theory of generative ConvNet</a>. In M. F. Balcan and K. Q. Weinberger, editors, <em>Proceedings of the 33rd international conference on machine learning</em>,Vol. 48, pages 2635–2644. New York, New York, USA: PMLR.
</div>
<div id="ref-Younes1999" class="csl-entry" role="listitem">
Younes, L. (1999). <a href="https://doi.org/10.1080/17442509908834179">On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates</a>. <em>Stochastics and Stochastic Reports</em>, <em>65</em>(3-4), 177–228. doi: 10.1080/17442509908834179.
</div>
<div id="ref-Zhao+2024" class="csl-entry" role="listitem">
Zhao, S., Brekelmans, R., Makhzani, A., and Grosse, R. (2024). <a href="https://arxiv.org/abs/2404.17546">Probabilistic inference in language models via twisted sequential monte carlo</a>. In.
</div>
<div id="ref-田崎晴明2008" class="csl-entry" role="listitem">
田崎晴明. (2008). <em><a href="https://www.gakushuin.ac.jp/~881791/statbook/">統計力学I</a></em>,Vol. 37. 培風館.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>non-normalized probabilistic model ともいう <span class="citation" data-cites="Song-Kingma2021">(<a href="#ref-Song-Kingma2021" role="doc-biblioref">Song and Kingma, 2021</a>)</span>．この形の分布族を <strong>正準分布</strong> または <strong>Gibbs 分布</strong> <span class="citation" data-cites="Koller-Friedman2009">(<a href="#ref-Koller-Friedman2009" role="doc-biblioref">Koller and Friedman, 2009, p. 108</a>)</span>, <span class="citation" data-cites="Friedli-Velenik2017">(<a href="#ref-Friedli-Velenik2017" role="doc-biblioref">Friedli and Velenik, 2017, p. 25</a>)</span>，または <strong>Boltzmann 分布</strong> <span class="citation" data-cites="Kim-Bengio2016">(<a href="#ref-Kim-Bengio2016" role="doc-biblioref">Kim and Bengio, 2016</a>)</span>, <span class="citation" data-cites="Mezard-Montanari2009">(<a href="#ref-Mezard-Montanari2009" role="doc-biblioref">Mézard and Montanari, 2009, p. 23</a>)</span>, <span class="citation" data-cites="Chewi2024">(<a href="#ref-Chewi2024" role="doc-biblioref">Chewi, 2024</a>)</span> ともいう．物理の用語では <span class="math inline">\(e^{H(z)}\)</span> を Boltzmann 因子と呼ぶのみであるようである <span class="citation" data-cites="田崎晴明2008">(<a href="#ref-田崎晴明2008" role="doc-biblioref">田崎晴明, 2008, p. 107</a>)</span>．<span class="citation" data-cites="Liu2004">(<a href="#ref-Liu2004" role="doc-biblioref">J. S. Liu, 2004, p. 7</a>)</span> ではどちらも掲載している．正準集団は，NVT 一定集団ともいう．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>今回の表示は特に SMC と相性が良く，<span class="citation" data-cites="Zhao+2024">(<a href="#ref-Zhao+2024" role="doc-biblioref">Zhao et al., 2024</a>)</span> では <span class="math inline">\(p'(x|z)\)</span> を段階的に近似する列を通じた twisted SMC という手法を提案している．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>だが，正規化定数 <span class="math inline">\(Z\)</span> が評価できないという前提で EBM の理論は進むため，有向グラフで定義される VAE などのモデルを EBM とみる積極的な理由はない．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>なお，2つの Hamiltonian <span class="math inline">\(H\)</span> の形は同じで温度が違うのみである．加えて，用途も違う：Hopfield ネットワークは連想記憶のモデル，Boltzmann 機械はデータ分布の（生成）モデリングに用いられる <span class="citation" data-cites="Carbone2024">(<a href="#ref-Carbone2024" role="doc-biblioref">Carbone, 2024, p. 4</a>)</span>．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>例えば <span class="math inline">\(H\)</span> のモデリングには <a href="../../../posts/2024/Kernels/GNN.html">GNN</a>，CNN などが自由に使える．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>MoE (Mixture of Expert) <span class="citation" data-cites="Jacobs+1991">(<a href="#ref-Jacobs+1991" role="doc-biblioref">Jacobs et al., 1991</a>)</span> と並んで使う用語である．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>通常，スコア関数といったとき，微分はパラメータ <span class="math inline">\(\theta\)</span> について取ることに注意．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><span class="math inline">\(D_F(\widetilde{p},p_\theta)\)</span> を近似する Monte Carlo 推定量の分散が大きくなる．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><span class="citation" data-cites="Song+2019">(<a href="#ref-Song+2019" role="doc-biblioref">Song et al., 2019</a>)</span> では <span class="math inline">\(V\sim\mathrm{N}_d(0,I_d)\)</span> とすることで Monte Carlo 近似による追加の誤差を回避している．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/162348\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="162348/162348.github.io" data-repo-id="R_kgDOKlfKYQ" data-category="Announcements" data-category-id="DIC_kwDOKlfKYc4CgDmb" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://162348.github.io/">
<p>Hirofumi Shiba</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/162348/162348.github.io/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ano2math5">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:shiba.hirofumi@ism.ac.jp">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>