<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="司馬博文">
<meta name="dcterms.date" content="2024-02-20">

<title>Hirofumi Shiba - 数学者のための深層学習２</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../profile.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Hirofumi Shiba - 数学者のための深層学習２">
<meta property="og:description" content="A Blog by a Bayesian Computation Researcher">
<meta property="og:image" content="https://162348.github.io/posts/2024/Kernels/Transformer.png">
<meta property="og:site-name" content="Hirofumi Shiba">
<meta property="og:image:height" content="582">
<meta property="og:image:width" content="391">
<meta name="twitter:title" content="Hirofumi Shiba - 数学者のための深層学習２">
<meta name="twitter:description" content="トランスフォーマーを概説する．">
<meta name="twitter:image" content="https://162348.github.io/posts/2024/Kernels/Transformer.png">
<meta name="twitter:image-height" content="582">
<meta name="twitter:image-width" content="391">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../recent.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Sessions.html" rel="" target="">
 <span class="menu-text">Sessions</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Japanese.html" rel="" target="">
 <span class="menu-text">日本語</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">数学者のための深層学習２</h1>
            <p class="subtitle lead">トランスフォーマー</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Kernel</div>
                <div class="quarto-category">Math Notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>司馬博文 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 20, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="abstract-title">概要</div>
      トランスフォーマーを概説する．
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#トランスフォーマー" id="toc-トランスフォーマー" class="nav-link active" data-scroll-target="#トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#名前の由来と背景" id="toc-名前の由来と背景" class="nav-link" data-scroll-target="#名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</a></li>
  <li><a href="#注意機構" id="toc-注意機構" class="nav-link" data-scroll-target="#注意機構"><span class="header-section-number">1.2</span> 注意機構</a></li>
  <li><a href="#トランスフォーマーの全体" id="toc-トランスフォーマーの全体" class="nav-link" data-scroll-target="#トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</a></li>
  <li><a href="#なぜトランスフォーマーはうまく行くのか" id="toc-なぜトランスフォーマーはうまく行くのか" class="nav-link" data-scroll-target="#なぜトランスフォーマーはうまく行くのか"><span class="header-section-number">1.4</span> なぜトランスフォーマーはうまく行くのか？</a></li>
  </ul></li>
  <li><a href="#言語トランスフォーマー" id="toc-言語トランスフォーマー" class="nav-link" data-scroll-target="#言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#言語の取り扱い" id="toc-言語の取り扱い" class="nav-link" data-scroll-target="#言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</a></li>
  <li><a href="#従来の言語モデル" id="toc-従来の言語モデル" class="nav-link" data-scroll-target="#従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</a></li>
  <li><a href="#トランスフォーマーによる言語モデルとその訓練" id="toc-トランスフォーマーによる言語モデルとその訓練" class="nav-link" data-scroll-target="#トランスフォーマーによる言語モデルとその訓練"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデルとその訓練</a></li>
  <li><a href="#近年の進展" id="toc-近年の進展" class="nav-link" data-scroll-target="#近年の進展"><span class="header-section-number">2.4</span> 近年の進展</a></li>
  </ul></li>
  <li><a href="#sec-fine-tuning" id="toc-sec-fine-tuning" class="nav-link" data-scroll-target="#sec-fine-tuning"><span class="header-section-number">3</span> 基盤モデル</a>
  <ul class="collapse">
  <li><a href="#大規模言語モデル" id="toc-大規模言語モデル" class="nav-link" data-scroll-target="#大規模言語モデル"><span class="header-section-number">3.1</span> 大規模言語モデル</a></li>
  <li><a href="#sec-foundation-model" id="toc-sec-foundation-model" class="nav-link" data-scroll-target="#sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</a></li>
  <li><a href="#プロンプトエンジニアリング" id="toc-プロンプトエンジニアリング" class="nav-link" data-scroll-target="#プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</a></li>
  <li><a href="#rag" id="toc-rag" class="nav-link" data-scroll-target="#rag"><span class="header-section-number">3.4</span> RAG</a></li>
  <li><a href="#sec-alignment" id="toc-sec-alignment" class="nav-link" data-scroll-target="#sec-alignment"><span class="header-section-number">3.5</span> アラインメント</a></li>
  </ul></li>
  <li><a href="#sec-multimodal-transformer" id="toc-sec-multimodal-transformer" class="nav-link" data-scroll-target="#sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#画像認識トランスフォーマー-vit" id="toc-画像認識トランスフォーマー-vit" class="nav-link" data-scroll-target="#画像認識トランスフォーマー-vit"><span class="header-section-number">4.1</span> 画像認識トランスフォーマー (ViT)</a></li>
  <li><a href="#画像生成トランスフォーマー" id="toc-画像生成トランスフォーマー" class="nav-link" data-scroll-target="#画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</a></li>
  <li><a href="#text-to-image-トランスフォーマー" id="toc-text-to-image-トランスフォーマー" class="nav-link" data-scroll-target="#text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</a></li>
  <li><a href="#image-to-text-トランスフォーマー" id="toc-image-to-text-トランスフォーマー" class="nav-link" data-scroll-target="#image-to-text-トランスフォーマー"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</a></li>
  <li><a href="#動画生成トランスフォーマー" id="toc-動画生成トランスフォーマー" class="nav-link" data-scroll-target="#動画生成トランスフォーマー"><span class="header-section-number">4.5</span> 動画生成トランスフォーマー</a></li>
  <li><a href="#世界モデルとしてのトランスフォーマー" id="toc-世界モデルとしてのトランスフォーマー" class="nav-link" data-scroll-target="#世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</a></li>
  <li><a href="#音声生成トランスフォーマー" id="toc-音声生成トランスフォーマー" class="nav-link" data-scroll-target="#音声生成トランスフォーマー"><span class="header-section-number">4.7</span> 音声生成トランスフォーマー</a></li>
  <li><a href="#text-to-speech-トランスフォーマー" id="toc-text-to-speech-トランスフォーマー" class="nav-link" data-scroll-target="#text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</a></li>
  <li><a href="#speach-to-text-トランスフォーマー" id="toc-speach-to-text-トランスフォーマー" class="nav-link" data-scroll-target="#speach-to-text-トランスフォーマー"><span class="header-section-number">4.9</span> Speach to Text トランスフォーマー</a></li>
  </ul></li>
  <li><a href="#近年の動向" id="toc-近年の動向" class="nav-link" data-scroll-target="#近年の動向"><span class="header-section-number">5</span> 近年の動向</a>
  <ul class="collapse">
  <li><a href="#llama-の一般公開とその影響" id="toc-llama-の一般公開とその影響" class="nav-link" data-scroll-target="#llama-の一般公開とその影響"><span class="header-section-number">5.1</span> LLaMA の一般公開とその影響</a></li>
  <li><a href="#llm-の経済的影響" id="toc-llm-の経済的影響" class="nav-link" data-scroll-target="#llm-の経済的影響"><span class="header-section-number">5.2</span> LLM の経済的影響</a></li>
  <li><a href="#種々の下流タスクの発見" id="toc-種々の下流タスクの発見" class="nav-link" data-scroll-target="#種々の下流タスクの発見"><span class="header-section-number">5.3</span> 種々の下流タスクの発見</a></li>
  <li><a href="#llm-と経済安全保障" id="toc-llm-と経済安全保障" class="nav-link" data-scroll-target="#llm-と経済安全保障"><span class="header-section-number">5.4</span> LLM と経済安全保障</a></li>
  <li><a href="#アラインメント" id="toc-アラインメント" class="nav-link" data-scroll-target="#アラインメント"><span class="header-section-number">5.5</span> アラインメント</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="hidden">
$$
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="トランスフォーマー" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</h2>
<section id="名前の由来と背景" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</h3>
<p>トランスフォーマー <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> は，注意 (attension) という機構を通じて，時系列データの依存関係を効率的に学習することの出来るモデルである．この「変換器」という名前は，後述の内部表現ベクトル <span class="math inline">\(Y\)</span> を，入力 <span class="math inline">\(X\)</span> から次元を変えずにより良いものに「変換する」というところから名前が付けられている．</p>
<p>初めは自然言語処理（特に機械翻訳）の文脈で導入されたデコーダーとエンコーダーの組からなるモデルであるが，そのエンコーダー部分だけで言語，画像，動画などあらゆる系列データのモデリング全体で抜群の性能を発揮する上に，これら複数ドメインのデータを組み合わせてモデリングすることもできる（第 <a href="#sec-multimodal-transformer">4</a> 節）．</p>
<p>さらに，トランスフォーマーはアーキテクチャとして（CNN や RNN などに比べると）シンプルであり，大規模なデータセットで大規模なモデルを訓練することが出来るスケーラビリティが魅力である．また，モデルの大きさに対して性能が単調に改善するというスケーリング則 <span class="citation" data-cites="Hestness+2017">(<a href="#ref-Hestness+2017" role="doc-biblioref">Hestness et al., 2017</a>)</span>, <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span> が成り立つことが示されており，大規模な資源を投下して大規模なモデルを作る経営判断も下しやすかった．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/SimplePowerLaws.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Scaling Laws <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span></figcaption>
</figure>
</div>
<p>その後すぐに，一度大規模なモデルを訓練してしまえば，少しの修正を施すのみで種々の下流タスクに適用することが可能であることが発覚した．これを <strong>基盤モデル</strong> という（第 <a href="#sec-foundation-model">3.2</a> 節）．</p>
</section>
<section id="注意機構" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="注意機構"><span class="header-section-number">1.2</span> 注意機構</h3>
<p>トランスフォーマーの核はその注意機構にある．とはいっても，注意機構自体はトランスフォーマー以前から存在した技術である．</p>
<p>元々機械翻訳に用いられていたエンコーダー・デコーダー型の RNN の性能を向上させる機構として提案された <span class="citation" data-cites="Bahdanau+2015">(<a href="#ref-Bahdanau+2015" role="doc-biblioref">Bahdanau et al., 2015</a>)</span>．その後，<span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> の <em>Attention is All You Need</em> とは，<strong>注意機構のみが重要で，RNN としての構造（や画像では畳み込みの構造）を排してシンプルにした方が更に性能が向上する</strong>，という報告である．</p>
<p>時系列データの解析では，そして自然言語処理ではとりわけ，文脈というものが重要である．しかし文脈は長期の依存関係になることもしばしばあり，従来の RNN ではこのモデリングに苦労していた (bottleneck problem)．</p>
<p>注意機構は，遠く離れた２つのトークンも直接に相互作用を持つアーキテクチャになっており，この点を抜本的に解決したものである．その結果，元の RNN のアーキテクチャも不要とするくらいのモデリング能力を，自然言語のみでなく，画像や動画に対しても示したのである．</p>
<p>注意機構は自己注意と交差注意に分けられる．</p>
<section id="枠組み" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="枠組み"><span class="header-section-number">1.2.1</span> 枠組み</h4>
<p>トランスフォーマーに入力する系列を <span class="math inline">\(\{x^n\}_{n=1}^N\subset\mathbb{R}^D\)</span> で表す．生のデータをそのままモデルに入れるわけではないので，別の言葉で呼び変える．</p>
<p>慣習として，特に言語データの場合は各 <span class="math inline">\(x^n\)</span> を <strong>トークン</strong> (token) という．画像では <strong>パッチ</strong> (patch) ともいう．</p>
<p>以降，<span class="math inline">\(X:=(x^n)_{n=1}^N\in M_{ND}(\mathbb{R})\)</span> とも表す．</p>
</section>
<section id="自己注意機構のプロトタイプ" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="自己注意機構のプロトタイプ"><span class="header-section-number">1.2.2</span> 自己注意機構のプロトタイプ</h4>
<p>自己注意機構とは，<span class="math inline">\(Y=AX\)</span> によって定まる <span class="math inline">\(M_{ND}(\mathbb{R})\)</span> 上の線型変換 <span class="math inline">\(X\mapsto Y\)</span> のことである： <span id="eq-1"><span class="math display">\[
y^n=\sum_{m=1}^N a^n_mx^m,
\tag{1}\]</span></span> <span id="eq-2"><span class="math display">\[
a^n_m=\frac{e^{(x^n)^\top x^m}}{\sum_{k=1}^Ne^{(x^n)^\top x^k}}.
\tag{2}\]</span></span> ここで，<span class="math inline">\(A=(a^n_m)_{n,m\in[N]}\in M_N(\mathbb{R})\)</span> は <a href="https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E8%A1%8C%E5%88%97">確率行列</a> をなし，その成分を <strong>注意荷重</strong> (attention weight) という．</p>
<p>この変換において，同じ <span class="math inline">\(x^m\)</span> の値を，３回別々の意味で使われていることに注意する：</p>
<ul>
<li><a href="#eq-1">式&nbsp;1</a> における <span class="math inline">\(x^m\)</span> は，新たな表現 <span class="math inline">\(y^n\)</span> を作るためのプロトタイプにような働きをしている．これを <strong>値</strong> (value) という．</li>
<li><a href="#eq-2">式&nbsp;2</a> において，内積が用いられており，<span class="math inline">\(x^n\)</span> と <span class="math inline">\(x^m\)</span> の類似度が測られている．
<ul>
<li><span class="math inline">\(x^m\)</span> を，<span class="math inline">\(x^m\)</span> が提供出来る情報を要約した量としての働きをし，<strong>鍵</strong> (key) という．</li>
<li><span class="math inline">\(x^n\)</span> は，<span class="math inline">\(x^n\)</span> と関連すべき情報を要求する役割を果たし，<strong>クエリ</strong> (query) という．</li>
</ul></li>
<li>最終的に，鍵とクエリの類似度・マッチ度を，<a href="https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0">ソフトマックス関数</a> を通じて確率分布として表現し，値の空間 <span class="math inline">\(\{x^m\}_{m=1}^N\)</span> 上の確率質量関数 <span class="math inline">\(\{a^n_m\}_{m=1}^N\)</span> を得ている．これに関して <strong>平均する</strong> ことで，鍵 <span class="math inline">\(y^n\)</span> を得る．</li>
</ul>
</section>
<section id="内積による自己注意機構" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="内積による自己注意機構"><span class="header-section-number">1.2.3</span> 内積による自己注意機構</h4>
<p>３つの別々の役割を果たしている以上，それぞれ固有の表現を持っていても良いはずである．そこで，値，鍵，クエリに，それぞれにニューラルネットワーク <span class="math inline">\(W_{(\Lambda)}\in M_{DD_{(\Lambda)}}(\mathbb{R})\;(\Lambda\in\{V,K,Q\})\)</span> を与えて固有の表現 <span class="math display">\[
x_{(\Lambda)}^n:=XW_{(\Lambda)}
\]</span> を持たせ，この <span class="math inline">\(W_{(\Lambda)}\)</span> を誤差逆伝播法により同時に学習することとする．</p>
<p>こうして得るのが，<strong>内積による自己注意機構</strong> (dot-product self-attention mechanism) である．このとき，<span class="math inline">\(D_{(K)}=D_{(Q)}\)</span> は必要だが，<span class="math inline">\(y^n\in\mathbb{R}^{D_{(V)}}\)</span> は，元の次元 <span class="math inline">\(D\)</span> と異なっても良いことに注意．</p>
<p>最後に，ソフトマックス関数の適用において，勾配消失を回避するために，次元 <span class="math inline">\(D_{(K)}\)</span> に応じたスケーリングを介して <span class="math display">\[
a^n_m=\frac{e^{\frac{\left(x^n_{(Q)}\right)^\top x^m_{(K)}}{\sqrt{D_K}}}}{\sum_{k=1}^Ne^{\frac{\left(x^n_{(Q)}\right)^\top x^k_{(K)}}{\sqrt{D_K}}}}
\]</span> とする．これを最終的な <strong>自己注意機構</strong> (scaled dot-product self-attention mechanism) という．</p>
</section>
<section id="交差注意" class="level4" data-number="1.2.4">
<h4 data-number="1.2.4" class="anchored" data-anchor-id="交差注意"><span class="header-section-number">1.2.4</span> 交差注意</h4>
<p>デコーダーとエンコーダーの接続部に用いられる <strong>交差注意</strong> (cross attention) については，ここでは触れない．</p>
</section>
<section id="マスキング" class="level4" data-number="1.2.5">
<h4 data-number="1.2.5" class="anchored" data-anchor-id="マスキング"><span class="header-section-number">1.2.5</span> マスキング</h4>
<p>実際に学習するとき，注意荷重 <span class="math inline">\(A\)</span> は上三角部分が <span class="math inline">\(-\infty\)</span> になったものを用いる．</p>
<p>これは，次のトークンを予測するにあたって，そのトークンより後のトークンを見ないようにするためである．</p>
</section>
</section>
<section id="トランスフォーマーの全体" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</h3>
<p>注意機構に加えて，次の３要素を含め，典型的には 20 から 24 層を成した深層ニューラルネットワークがトランスフォーマーの全てである．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/ModalNet-21.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Transformer Architecture <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span></figcaption>
</figure>
</div>
<section id="多頭注意" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="多頭注意"><span class="header-section-number">1.3.1</span> 多頭注意</h4>
<p>以上の自己注意機構を１単位として，これを複数独立に訓練し，最終的にはこれらの線型結合を採用する仕組みを <strong>多頭注意</strong> (multi-head attention) という．</p>
<p>これにより，種々の文脈をより頑健に読み取ることが出来るようである．</p>
</section>
<section id="残差結合と正規化" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="残差結合と正規化"><span class="header-section-number">1.3.2</span> 残差結合と正規化</h4>
<p>更に勾配消失を回避するために，<a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">残差結合</a> を導入し，訓練の高速化のために正規化 <span class="citation" data-cites="Ba+2016">(<a href="#ref-Ba+2016" role="doc-biblioref">Ba et al., 2016</a>)</span> が導入される．</p>
<p>そして，モデルを大規模化していくには，この「多頭注意＋残差結合と正規化」のブロックを積み重ねる．</p>
</section>
<section id="sec-FF" class="level4" data-number="1.3.3">
<h4 data-number="1.3.3" class="anchored" data-anchor-id="sec-FF"><span class="header-section-number">1.3.3</span> 多層パーセプトロン</h4>
<p>注意機構は線型性が高いため，多頭注意の層の間に，通常の Feedforward ネットワークもスタックして，ネットワークの表現能力を保つ工夫もされる．</p>
</section>
<section id="正規化レイヤーについての補足" class="level4" data-number="1.3.4">
<h4 data-number="1.3.4" class="anchored" data-anchor-id="正規化レイヤーについての補足"><span class="header-section-number">1.3.4</span> 正規化レイヤーについての補足</h4>
<p>レイヤー正則化 (layer normalization) <span class="citation" data-cites="Ba+2016">(<a href="#ref-Ba+2016" role="doc-biblioref">Ba et al., 2016</a>)</span> は，バッチ正規化 (batch normalization) <span class="citation" data-cites="Ioffe-Szegedy2015">(<a href="#ref-Ioffe-Szegedy2015" role="doc-biblioref">Ioffe &amp; Szegedy, 2015</a>)</span> が RNN にも適するようにした修正として提案された．</p>
<p>バッチ正規化は，ニューラルネットワークの内部層の学習が，手前の層のパラメータが時事刻々と変化するために安定した学習が出来ないという <strong>内部共変量シフト</strong> (internal covariate shift) にあると突き止め，これをモデルアーキテクチャに正規化層を取り入れることで解決するものである．</p>
<p>正規化層は，ニューラルネットワークへの入力を，平均が零で分散が <span class="math inline">\(1\)</span> になるように変換する．元々，ニューラルネットワークの入力を正規化してから学習させることで学習が効率化されることは知られていた <span class="citation" data-cites="LeCun+2012">(<a href="#ref-LeCun+2012" role="doc-biblioref">LeCun et al., 2012</a>)</span> が，バッチ正規化は，これをバッチごとに，かつ，モデルの内部にも取り込んだものである．</p>
<p>バッチ正規化は精度の上昇と訓練の加速をもたらす．これはバッチ正規化により大きな学習率で訓練しても活性化が発散せず，これにより訓練時間の短縮と，局所解に囚われにくく汎化性能の向上がもたらされているようである <span class="citation" data-cites="Bjorck+2018">(<a href="#ref-Bjorck+2018" role="doc-biblioref">Bjorck et al., 2018</a>)</span>．</p>
</section>
</section>
<section id="なぜトランスフォーマーはうまく行くのか" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="なぜトランスフォーマーはうまく行くのか"><span class="header-section-number">1.4</span> なぜトランスフォーマーはうまく行くのか？</h3>
<p>注意機構は全体として線型変換になっている．これをカーネル法などを用いて非線型にする試みは多くあるが，これは成功していない．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>その代わり，トランスフォーマーのパラメータ数のほとんどは FF 層（ <a href="#sec-FF">節&nbsp;1.3.3</a> ）によるものであり，この層が大きな表現能力を持っていることが，トランスフォーマーの性能を支えていると考えられている．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><strong>注意機構は，遠く離れた２つのトークンを直接相互作用可能にすることに妙がある</strong>．実際，注意機構は，荷重行列を入力から学習するような，荷重平均プーリング (weighted mean pooling) ともみなせる．</p>
</section>
</section>
<section id="言語トランスフォーマー" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</h2>
<p>トランスフォーマーの訓練は，後述するように事前学習と事後調整からなる．事後調整は <a href="#sec-fine-tuning">節&nbsp;3</a> で述べる．ここでは，事前学習を，言語を例に取って説明する．</p>
<p>トランスフォーマーの事前学習とは <a href="https://ja.wikipedia.org/wiki/%E3%83%AC%E3%83%88%E3%83%AD%E3%83%8B%E3%83%A0">レトロニム</a> であり，トークン（≒単語）上の確率分布をモデリングをすることに他ならない．</p>
<p>古典的には <span class="math inline">\(n\)</span>-gram <a href="#sec-n-gram">節&nbsp;2.2.1</a> などのモデルが用いられていたが，これをニューラルネットワークによって作ることはトランスフォーマー以前から試みられていた <span class="citation" data-cites="Bengio+2000">(<a href="#ref-Bengio+2000" role="doc-biblioref">Bengio et al., 2000</a>)</span>．</p>
<p>その後，トランスフォーマーの登場まで，これには RNN <a href="#sec-RNN">節&nbsp;2.2.2</a> が主に用いられていた．しかし，RNN は長い系列に対しては勾配消失とボトルネック問題が起こりやすく，また，訓練の並列化が難しいという問題があった．</p>
<section id="言語の取り扱い" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</h3>
<section id="単語の分散表現" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="単語の分散表現"><span class="header-section-number">2.1.1</span> 単語の分散表現</h4>
<p>言語をそのまま扱うのではなく，トークン <span class="math inline">\(x^n\in\mathbb{R}^D\)</span> の形に符号化する必要がある．</p>
<p>言語には他にも改行や数式，コンピューターコードがあるが，まずは単語の表現を考える．</p>
<p>単語を Euclid 空間内に埋め込んだものを <strong>分散表現</strong> (distributed representation) という．これを２層のニューラルネットワークで行う技術が <code>word2vec</code> である <span class="citation" data-cites="Mikolov2013">(<a href="#ref-Mikolov2013" role="doc-biblioref">Mikolov et al., 2013</a>)</span>．</p>
<p>その訓練法には２つあり，窓の幅を <span class="math inline">\(M=5\)</span> などとすると，</p>
<ul>
<li>CBOW (Continuous Bag of Words)：前後 <span class="math inline">\(M\)</span> 語のみを見せて，中央の語を予測する．</li>
<li>Continuous Skip-gram：中央の語を見せて，前後 <span class="math inline">\(M\)</span> 語を予測する．</li>
</ul>
<p>という，いずれも教師なしの方法によって学習される．</p>
</section>
<section id="トークン化" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="トークン化"><span class="header-section-number">2.1.2</span> トークン化</h4>
<p>バイトペア符号化 (BPE: Byte Pair Encoding) <span class="citation" data-cites="Sennrich+2016">(<a href="#ref-Sennrich+2016" role="doc-biblioref">Sennrich et al., 2016</a>)</span> は，データ圧縮の手法であるが，単語に限らず種々のデータを含んだ文字列を符号化するのに用いられる．</p>
</section>
<section id="位置情報符号化" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="位置情報符号化"><span class="header-section-number">2.1.3</span> 位置情報符号化</h4>
<p>トランスフォーマーはそのままではトークンの順番を考慮しないため，トークンの順番の情報も符号化時に含める必要がある．これを <strong>位置情報符号化</strong> (positional encoding) という <span class="citation" data-cites="Dufter+2021">(<a href="#ref-Dufter+2021" role="doc-biblioref">Dufter et al., 2021</a>)</span>．</p>
<p>このようにして，位置情報はトランスフォーマーのモデル構造を修正して組み込むのではなく，符号化の段階で組み込み，トランスフォーマーはそのまま使うのである．</p>
<p>これは，位置情報をトークンと同じ空間に埋め込んだ表現 <span class="math inline">\(r^n\)</span> を学習し， <span class="math display">\[
\widetilde{x}^n:=x^n+r^n
\]</span> を新たな符号とする．<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
</section>
</section>
<section id="従来の言語モデル" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</h3>
<p>文章をトークン列 <span class="math inline">\(\{x^n\}_{n=1}^N\subset\mathbb{R}^D\)</span> に置き換えたあとに，この上の結合分布 <span class="math inline">\(p(x^1,\cdots,x^N)\)</span> をモデリングすることが，<strong>言語モデル</strong> の目標である．</p>
<section id="sec-n-gram" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="sec-n-gram"><span class="header-section-number">2.2.1</span> <span class="math inline">\(n\)</span>-gram</h4>
<p><span class="math inline">\(n\ge1\)</span> とし，<span class="math inline">\(x_i=0\;(i\le0)\)</span> として， <span class="math display">\[
p(x_1,\cdots,x_N)=\prod_{i=1}^Np_{\theta_i}(x_i|x_{i-n},\cdots,x_{i-1})
\]</span> という形で <span class="math inline">\(p\)</span> をモデリングする．</p>
<p>これを <span class="math inline">\(n\)</span>-gram モデルと呼ぶが，文章の長さ <span class="math inline">\(N\)</span> が大きくなると，必要なパラメータ <span class="math inline">\(\theta_n\)</span> の数が増加する．</p>
<p>これに対処する方法としては，<a href="../../../posts/2023/Surveys/SSM.html">隠れ Markov モデル</a> を用いることが考えられる．</p>
<p>ニューラルネットワーク <span class="citation" data-cites="Bengio+2000">(<a href="#ref-Bengio+2000" role="doc-biblioref">Bengio et al., 2000</a>)</span> を用いることも出来る．しかし，依存の長さ <span class="math inline">\(n\ge1\)</span> が固定されていることはやはり問題である．</p>
</section>
<section id="sec-RNN" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="sec-RNN"><span class="header-section-number">2.2.2</span> RNN</h4>
<p>長さ制限のない長期的な依存関係を Feedback network によって表現することが，<span class="citation" data-cites="Mikolov+2010">(<a href="#ref-Mikolov+2010" role="doc-biblioref">Mikolov et al., 2010</a>)</span> によって試みられた．</p>
<p>これは通常のニューラルネットワーク (FFN: Feedforward Network と呼ばれる) に，出力の一部を次の入力に使うという回帰的な流れを追加することで，隠れ Markov モデルのように次に持ち越される内部状態を持つことを可能にしたモデルである．</p>
<p>しかしこれは学習が困難であることと，結局長期的な依存関係は効率的に学習されないという２つの問題があった．</p>
<p>誤差の逆伝播を時間に対しても逆方向に繰り返す必要がある (Backpropagation through time) ので，長い系列に対しては逆伝播しなければいけない距離が長く，勾配消失・爆発が起こりやすい．これは長期的な依存関係を学習しにくいということももたらす．<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> また，並列化も難しく，大規模なモデルの学習は難しい．</p>
<p>これに対処するために，モデルの構造を変えて過去の情報を流用しやすくする方法も種々提案された．LSTM (Long short-term memory) <span class="citation" data-cites="Hochreiter-Schmidhuber1997">(<a href="#ref-Hochreiter-Schmidhuber1997" role="doc-biblioref">Hochreiter &amp; Schmidhuber, 1997</a>)</span> や GRU (Gated Recurrent Unit) <span class="citation" data-cites="Cho+2014">(<a href="#ref-Cho+2014" role="doc-biblioref">Cho et al., 2014</a>)</span> などがその例である．</p>
</section>
</section>
<section id="トランスフォーマーによる言語モデルとその訓練" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="トランスフォーマーによる言語モデルとその訓練"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデルとその訓練</h3>
<p>トランスフォーマーによる言語モデルの最大の美点は，自己教師あり学習による言語モデルの学習が可能である点である．これによりインターネットに蓄積していた大量のデータが利用可能になる．</p>
<p>パラメータを自己教師あり学習により初期化することで，言語モデルの性能が大幅に改善できることは <span class="citation" data-cites="Dai-Le2015">(<a href="#ref-Dai-Le2015" role="doc-biblioref">Dai &amp; Le, 2015</a>)</span> が LSTM 入りの RRN における実験を通じて最初に指摘したようである．</p>
<p>ラベルデータを必要とするならば，これは本当の意味でスケーラブルではなかったであろう．</p>
<ol type="1">
<li>BERT (Bidirectional Encoder Representations from Transformers) <span class="citation" data-cites="Devlin+2019">(<a href="#ref-Devlin+2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> は，双方向エンコーダーである．</li>
<li>GPT (Generative Pre-trained Transformer) <span class="citation" data-cites="Radford+2018">(<a href="#ref-Radford+2018" role="doc-biblioref">Radford et al., 2018</a>)</span> は，単方向デコーダーである．</li>
<li>BART (Bidirectional and Auto-Regressive Transformer) <span class="citation" data-cites="Lewis+2020-BART">(<a href="#ref-Lewis+2020-BART" role="doc-biblioref">M. Lewis et al., 2020</a>)</span> は，双方向エンコーダーと単方向デコーダーの両方を持つ．</li>
</ol>
<section id="デコーダーのみの言語モデル" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="デコーダーのみの言語モデル"><span class="header-section-number">2.3.1</span> デコーダーのみの言語モデル</h4>
<p>GPT などの生成モデルは，デコーダー部分のトランスフォーマーの機能を主に用いている．</p>
<p>これはまず，</p>
<ol type="1">
<li>トークン列 <span class="math inline">\((x^n)_{n=1}^{N-1}\)</span> を入力し，条件付き分布 <span class="math inline">\(p(x^N|x_1,\cdots,x^{N-1})\)</span> を得る．</li>
<li>分布 <span class="math inline">\(p(x^N|x_1,\cdots,x^{N-1})\)</span> からサンプリングをする．</li>
</ol>
<p>の２段階で行われる．こうして <span class="math inline">\((x^n)_{n=1}^N\)</span> を得たら，次は <span class="math inline">\(x^{N+1}\)</span> を生成し，文章が終わるまでこれを続けることで，最終的な生成を完遂する．</p>
<section id="条件付き分布の表現" class="level5" data-number="2.3.1.1">
<h5 data-number="2.3.1.1" class="anchored" data-anchor-id="条件付き分布の表現"><span class="header-section-number">2.3.1.1</span> 条件付き分布の表現</h5>
<p>大規模なデータセットの上で，文章を途中まで読み，次のトークンを推測する，という自己教師あり学習を行うことで，トークン上の条件付き分布を学習する．</p>
<p>この際に，先のトークンの情報は使わないように，注意機構を工夫 (masked / causal attention) して訓練する．</p>
</section>
<section id="条件付き分布からのサンプリング" class="level5" data-number="2.3.1.2">
<h5 data-number="2.3.1.2" class="anchored" data-anchor-id="条件付き分布からのサンプリング"><span class="header-section-number">2.3.1.2</span> 条件付き分布からのサンプリング</h5>
<p>仮に最も確率の高いトークンを毎回選択する場合，出力は決定論的であり，同じ表現を繰り返すことが多くみられる．</p>
<p>実は，より人間らしい表現は，確率の低いトークンもかなら頻繁に採用される <span class="citation" data-cites="Holtzman+2020">(<a href="#ref-Holtzman+2020" role="doc-biblioref">Holtzman et al., 2020</a>)</span>．</p>
<p>かと言って，純粋なサンプリングをしたのでは，文章全体から見て意味をなさない場合も多い．</p>
<p>これを解決したのが top-<span class="math inline">\(p\)</span> sampling / nucleus sampling <span class="citation" data-cites="Holtzman+2020">(<a href="#ref-Holtzman+2020" role="doc-biblioref">Holtzman et al., 2020</a>)</span> である．</p>
<p><a href="https://github.com/openai/gpt-2-output-dataset/issues/5">GPT-2 にも実装されている</a> ようである．</p>
</section>
</section>
<section id="エンコーダーのみの言語モデル" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="エンコーダーのみの言語モデル"><span class="header-section-number">2.3.2</span> エンコーダーのみの言語モデル</h4>
<p>BERT (bidirectional encoder representations from transformers) <span class="citation" data-cites="Devlin+2019">(<a href="#ref-Devlin+2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> などの言語理解モデルは，エンコーダ部分のトランスフォーマーの機能を主に用いている．その結果，生成は出来ない．</p>
<p>訓練は，データセットから単語を確率的に脱落させ，これを補完するように訓練する．結果として，文章の前後両方 (bidirectional) の文脈を考慮するようになるのである．</p>
<p>実際に使う際は，例えば感情の判別などでは，文章の冒頭に <code>[class]</code> などの特殊なトークンを置き，これをエンコーダーに通してトークンが何に置き換わるかを見ることで，判別を実行することができる．</p>
</section>
<section id="エンコーダーデコーダーの言語モデル" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="エンコーダーデコーダーの言語モデル"><span class="header-section-number">2.3.3</span> エンコーダー・デコーダーの言語モデル</h4>
<p>トランスフォーマーは原論文 <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> では，エンコーダーとデコーダーがセットになったモデルとして提案された．</p>
<p>これは機械翻訳を念頭に置いていたため，RNN の構造を引き継いだ形で提案されたためである．この場合，次のようにしてモデルは使われる</p>
<ol type="1">
<li>入力 <span class="math inline">\(X\)</span> をエンコーダーに通し，内部表現 <span class="math inline">\(Z\)</span> を得る．</li>
<li>この内部表現 <span class="math inline">\(Z\)</span> を元に，デコードした結果 <span class="math inline">\(Y\)</span> を出力する．</li>
<li>唯一，<span class="math inline">\(Z\)</span> をデコーダーに渡す部分での注意機構層では，鍵と値としては <span class="math inline">\(Z\)</span> を使うが，クエリとしては <span class="math inline">\(Y\)</span> を使う．</li>
</ol>
<p>３の機構を <strong>エンコーダー・デコーダーの注意機構</strong> (encoder-decoder / corss attention mechanism) といい，これによって <span class="math inline">\(Z\)</span> と <span class="math inline">\(Y\)</span> のトークンの間の類似度をモデルに取り入れる．</p>
</section>
</section>
<section id="近年の進展" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="近年の進展"><span class="header-section-number">2.4</span> 近年の進展</h3>
<section id="分布外のデータに対するロバスト性" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="分布外のデータに対するロバスト性"><span class="header-section-number">2.4.1</span> 分布外のデータに対するロバスト性</h4>
<p>GPT-4 などの大規模言語モデルが，コンテクスト内学習 (in-context learning) が可能であることが，興味深い現象として解析されている．</p>
<p>in-context learning とは，<strong>パラメータをそのタスクに対して事後調整した訳でもないのに優れた性能を見せること</strong> を指す．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/ff_1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An Example of In-context Learning <span class="citation" data-cites="Bubeck+2023">(<a href="#ref-Bubeck+2023" role="doc-biblioref">Bubeck et al., 2023</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="状態空間モデル" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="状態空間モデル"><span class="header-section-number">2.4.2</span> 状態空間モデル</h4>
<p>状態空間モデルを中間層に用いるモデルが注目されている．</p>
<p>しかし，特に長期的な依存関係の処理を苦手とするため，言語においては注意機構ほど性能が出ず，トランスフォーマーに比べて並列計算が難しいために実行速度が遅くなる，という問題があるが，これらは解決可能で，トランスフォーマーの性能を凌駕する可能性があるとされている <span class="citation" data-cites="Gu+2022">(<a href="#ref-Gu+2022" role="doc-biblioref">Gu et al., 2022</a>)</span>, <span class="citation" data-cites="Fu+2023">(<a href="#ref-Fu+2023" role="doc-biblioref">Fu et al., 2023</a>)</span>．</p>
</section>
</section>
</section>
<section id="sec-fine-tuning" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-fine-tuning"><span class="header-section-number">3</span> 基盤モデル</h2>
<p>大規模なトランスフォーマーを，インターネットに蓄積していた大量のデータを用いて訓練することにより得るモデルは，チャットボットや感情分析，要約など種々の下流タスクに少しの事後調整を施すだけで抜群の性能を発揮することが発見された．</p>
<p>これを <strong>基盤モデル</strong> という．</p>
<section id="大規模言語モデル" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="大規模言語モデル"><span class="header-section-number">3.1</span> 大規模言語モデル</h3>
<section id="名前の由来と背景-1" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="名前の由来と背景-1"><span class="header-section-number">3.1.1</span> 名前の由来と背景</h4>
<p>自然言語処理にトランスフォーマーを応用した例は大きな成功を見ている．GPT <span class="citation" data-cites="Radford+2018">(<a href="#ref-Radford+2018" role="doc-biblioref">Radford et al., 2018</a>)</span>, <a href="https://openai.com/research/gpt-2-1-5b-release">GPT-2</a> <span class="citation" data-cites="Radford+2019">(<a href="#ref-Radford+2019" role="doc-biblioref">Radford et al., 2019</a>)</span>, GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span>, <a href="https://openai.com/research/gpt-4">GPT-4</a> <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023b</a>)</span> のシリーズはその代表であり，特に GPT-4 は AGI の実現に向けた重要な一歩とも評されている <span class="citation" data-cites="Bubeck+2023">(<a href="#ref-Bubeck+2023" role="doc-biblioref">Bubeck et al., 2023</a>)</span>．</p>
<p>その成功は，言語モデルとして優れているという点よりもむしろ，並列化が可能であり GPU などの計算資源を効率的に使える <span class="citation" data-cites="Weng-Brockman2022">(<a href="#ref-Weng-Brockman2022" role="doc-biblioref">Weng &amp; Brockman, 2022</a>)</span> という点にあり，モデルの改良よりも計算資源の増強が最終的に大きな進歩をもたらすという側面が大きい，という認識が優勢になっている <span class="citation" data-cites="Sutton2019">(<a href="#ref-Sutton2019" role="doc-biblioref">R. Sutton, 2019</a>)</span>．これはスケーリング則として理論的にも理解が試みられている <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span>．</p>
<p>この観点から，トランスフォーマーを用いた事前学習済みの言語モデルが，種々のタスクをほとんど例示なし (few-shot / zero-shot) で解ける能力を創発する程度に大きい場合，その規模が意味を持つことを強調して，<strong>大規模言語モデル</strong> (LLM: Large Language Model) とも呼ぶ <span class="citation" data-cites="Zhao+2023">(<a href="#ref-Zhao+2023" role="doc-biblioref">Zhao et al., 2023</a>)</span>．</p>
</section>
<section id="chinchilla-hoffmann2022" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="chinchilla-hoffmann2022"><span class="header-section-number">3.1.2</span> Chinchilla <span class="citation" data-cites="Hoffmann+2022">(<a href="#ref-Hoffmann+2022" role="doc-biblioref">Hoffmann et al., 2022</a>)</span></h4>
<table class="table-striped table-hover table">
<caption>Size of LLMs</caption>
<colgroup>
<col style="width: 50%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">Training Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LaMDA <span class="citation" data-cites="Thoppilan+2022">(<a href="#ref-Thoppilan+2022" role="doc-biblioref">Thoppilan et al., 2022</a>)</span></td>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">168B</td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span></td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Jurassic <span class="citation" data-cites="Lieber+2021">(<a href="#ref-Lieber+2021" role="doc-biblioref">Lieber et al., 2021</a>)</span></td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="even">
<td style="text-align: center;">Gopher <span class="citation" data-cites="Rae+2021">(<a href="#ref-Rae+2021" role="doc-biblioref">Rae et al., 2021</a>)</span></td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Chinchilla</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">1.4T</td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/model_sizes.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Number of Training Tokens <a href="https://babylm.github.io/">BabyLM Challenge</a></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="種々の LLM とマルチモーダル化">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
種々の LLM とマルチモーダル化
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Google の <a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(<a href="#ref-Lepikhin+2021" role="doc-biblioref">Lepikhin et al., 2021</a>)</span>，<a href="https://japan.googleblog.com/2023/05/palm-2.html">PaML</a> <span class="citation" data-cites="Chowdhery+2022">(<a href="#ref-Chowdhery+2022" role="doc-biblioref">Chowdhery et al., 2022</a>)</span>，M4 <span class="citation" data-cites="Aharoni+2019">(<a href="#ref-Aharoni+2019" role="doc-biblioref">Aharoni et al., 2019</a>)</span>，Google Brain の Switch Transformers <span class="citation" data-cites="Fedus+2022">(<a href="#ref-Fedus+2022" role="doc-biblioref">Fedus et al., 2022</a>)</span>，Google DeepMind の <a href="https://deepmind.google/discover/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval/">Gopher</a> <span class="citation" data-cites="Rae+2021">(<a href="#ref-Rae+2021" role="doc-biblioref">Rae et al., 2021</a>)</span> などがある．</p>
<p>最近のものでは，</p>
<ul>
<li>Google の <a href="https://blog.google/technology/ai/lamda/">LaMDA</a> <span class="citation" data-cites="Thoppilan+2022">(<a href="#ref-Thoppilan+2022" role="doc-biblioref">Thoppilan et al., 2022</a>)</span> は会話に特化した LLM である．</li>
<li>Google から 12/6/2023 に Gemini <span class="citation" data-cites="Geminiteam+2023">(<a href="#ref-Geminiteam+2023" role="doc-biblioref">Team et al., 2023</a>)</span> が発表され，<a href="https://japan.googleblog.com/2024/02/gemini-15.html">2/16/2024</a> には Gemini 1.5 が発表された．
<ul>
<li>これに伴い，Bard と Duet AI はいずれも Gemini に名称変更された．</li>
<li><a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(<a href="#ref-Lepikhin+2021" role="doc-biblioref">Lepikhin et al., 2021</a>)</span> 同様，トランスフォーマーに加えて，新しいアーキテクチャである Sparsely-Gated MoE <span class="citation" data-cites="Shazeer+2017">(<a href="#ref-Shazeer+2017" role="doc-biblioref">Shazeer et al., 2017</a>)</span> が用いられている．これはモデルのパラメータを分割し（それぞれを専門家 expert という），１つの入力にはその一部分しか使わないようにすることでメモリを節約し並列化を可能にする手法である．</li>
<li>文書から高精度にテキストを抽出する LMDX (Language Model-based Document Information Extraction and Localization) <span class="citation" data-cites="Perot+2023">(<a href="#ref-Perot+2023" role="doc-biblioref">Perot et al., 2023</a>)</span> も用いられている．</li>
</ul></li>
<li>OpenAI から 9/5/2023 に GPT-4V <span class="citation" data-cites="OpenAI2023-GPT4V">(<a href="#ref-OpenAI2023-GPT4V" role="doc-biblioref">OpenAI, 2023c</a>)</span> が発表され，ChatGPT にも実装された．
<ul>
<li>Microsoft の研究者も，GPT-4V の出来ること関する考察 <span class="citation" data-cites="Yang+2023">(<a href="#ref-Yang+2023" role="doc-biblioref">Yang et al., 2023</a>)</span> を発表している．</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
</section>
<section id="sec-foundation-model" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</h3>
<p>GPT の P とは Pre-trained である．自己教師あり学習によって <a href="../../../posts/2024/Kernels/Deep.html#sec-pretraining-using-AE">事前学習</a> をしたあと，さらに教師あり学習によって微調整を行う．</p>
<p>この微調整は <strong>事後調整</strong> (fine-tune) の一種であり，転移学習の一種ともみなせる．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> この意味でも，さらに使い方の意味でも，種々の応用や下流タスク (downstream task) の基礎となるモデルであることと，そのものでは未完成であることとを強調して，<strong>基盤モデル</strong> (foundation model) とも呼ばれる <span class="citation" data-cites="Bommasani+2021">(<a href="#ref-Bommasani+2021" role="doc-biblioref">Bommasani et al., 2021</a>)</span>．</p>
<p>事後調整では，モデルの全体では規模が大きすぎるため，出力層の後に新しいニューラルネットを付加したり，最後の数層のみを追加で教師あり学習をしたりする方法が一般的である．または，LoRA (Low-Rank Adaptation) <span class="citation" data-cites="Hu+2021">(<a href="#ref-Hu+2021" role="doc-biblioref">E. J. Hu et al., 2021</a>)</span> では，トランスフォーマーの各層に新たな層を挿入し，これを学習する．</p>
<p>これは，事後調整に有効な内的次元は実際には小さく <span class="citation" data-cites="Aghajanyan+2021">(<a href="#ref-Aghajanyan+2021" role="doc-biblioref">Aghajanyan et al., 2021</a>)</span>，これに有効にアクセスし，効率的な事後調整を行うことが出来るという．</p>
<p>事後調整には，他にも，ChatGPT のようなサービスを展開するために必要なユーザー体験の改善を目的としたものも含まれる．GPT-4 では <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(<a href="#ref-Christiano+2017" role="doc-biblioref">Christiano et al., 2017</a>)</span> が用いられている <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023b, p. 2</a>)</span>（第 <a href="#sec-alignment">3.5</a> 節）．</p>
</section>
<section id="プロンプトエンジニアリング" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</h3>
<p>基盤モデルは使い方によって大きく性能が変わる．特に，prompt engineering <span class="citation" data-cites="Liu+2023">(<a href="#ref-Liu+2023" role="doc-biblioref">Liu et al., 2023</a>)</span> は，プロンプトの送り方によって性能がどう変わるかを調べる新たな分野である．</p>
<p>その結果，プロンプト内で新たなタスクを定義するだけで，これが解けてしまうこともわかっており，これを few-shot learning という．これも大規模言語モデルの大きな特徴である．</p>
</section>
<section id="rag" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="rag"><span class="header-section-number">3.4</span> RAG</h3>
<p>LLM は世界に関する正確な知識も持っており，知識ベースとしての利用も期待されている <span class="citation" data-cites="Petroni+2019">(<a href="#ref-Petroni+2019" role="doc-biblioref">Petroni et al., 2019</a>)</span>．<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> しかし，知識を正確に・信頼出来る形で引き出すことが難しい．特に，出典を示すことや，最新の知識のアップデートなどが難題として待っている．</p>
<p>そこで，LLM に（自由に外部情報を探索できるという意味で）ノンパラメトリックな知識ベースを接続することで解決するのが RAG (Retrieval-Augmented Generation) モデル <span class="citation" data-cites="Lewis+2020">(<a href="#ref-Lewis+2020" role="doc-biblioref">P. Lewis et al., 2020</a>)</span> である．</p>
<p>DPR (Dense Passage Retriever) <span class="citation" data-cites="Karpukhin+2020">(<a href="#ref-Karpukhin+2020" role="doc-biblioref">Karpukhin et al., 2020</a>)</span> は文書を密に符号化する手法を開発し，これを用いて文書検索をすることで Q&amp;A タスクを効率的に解く手法を提案した．このような文書の符号化器は <strong>検索器</strong> (retriever) と呼ばれる．</p>
<p>RAG <span class="citation" data-cites="Lewis+2020">(<a href="#ref-Lewis+2020" role="doc-biblioref">P. Lewis et al., 2020</a>)</span> はこの検索器を BART <span class="citation" data-cites="Lewis+2020-BART">(<a href="#ref-Lewis+2020-BART" role="doc-biblioref">M. Lewis et al., 2020</a>)</span> に接続した．</p>
<p>REALM (Retrieval-Augmented Language Model) <span class="citation" data-cites="Guu+2020">(<a href="#ref-Guu+2020" role="doc-biblioref">Guu et al., 2020</a>)</span> も同時期に提案されている．</p>
<p>Meta での研究 <span class="citation" data-cites="Yasunaga+2023">(<a href="#ref-Yasunaga+2023" role="doc-biblioref">Yasunaga et al., 2023</a>)</span> はこの検索器を Text-to-Image トランスフォーマー である CM3 <span class="citation" data-cites="Aghajanyan+2022">(<a href="#ref-Aghajanyan+2022" role="doc-biblioref">Aghajanyan et al., 2022</a>)</span> と結合することで，初めて言語と画像の両方を扱える RAG モデル RA-CM3 (retrieval-augmented CM3) を構成した．</p>
<p><a href="https://openai.com/research/webgpt">WebGPT</a> <span class="citation" data-cites="Nakano+2022">(<a href="#ref-Nakano+2022" role="doc-biblioref">Nakano et al., 2022</a>)</span> は，RAG や REAML が文書検索をしているところを，Web 検索を実行できるようにした GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span> の事後調整である．</p>
</section>
<section id="sec-alignment" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-alignment"><span class="header-section-number">3.5</span> アラインメント</h3>
<p>LLM などの機械学習モデルを訓練する際の目的関数は，そのままでは人間社会が要請するものとずれがあることが多い．これを修正するような試みを <strong>アラインメント</strong> (alignment) という．</p>
<blockquote class="blockquote">
<p>For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, <strong>these models are not aligned with their users</strong>. <span class="citation" data-cites="Ouyang+2022">(<a href="#ref-Ouyang+2022" role="doc-biblioref">Ouyang et al., 2022</a>)</span></p>
</blockquote>
<p>代表的な手法が <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(<a href="#ref-Christiano+2017" role="doc-biblioref">Christiano et al., 2017</a>)</span> である．</p>
<p><a href="https://openai.com/research/instruction-following">InstructGPT</a> <span class="citation" data-cites="Ouyang+2022">(<a href="#ref-Ouyang+2022" role="doc-biblioref">Ouyang et al., 2022</a>)</span> は OpenAI API を通じて寄せられたフィードバックを用いて，<a href="https://openai.com/research/openai-baselines-ppo">PPO</a> (Proximal Policy Optimization) アルゴリズム <span class="citation" data-cites="Schulman+2017">(<a href="#ref-Schulman+2017" role="doc-biblioref">Schulman et al., 2017</a>)</span> による強化学習により事後調整をしたものである．</p>
<p>近接ポリシー最適化 (PPO: Proximal Policy Optimization) アルゴリズム <span class="citation" data-cites="Schulman+2017">(<a href="#ref-Schulman+2017" role="doc-biblioref">Schulman et al., 2017</a>)</span> は 信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) <span class="citation" data-cites="Schulman+2015">(<a href="#ref-Schulman+2015" role="doc-biblioref">Schulman et al., 2015</a>)</span> の洗練化として提案されたもので，現在の RLHF においても最も広く使われている手法である <span class="citation" data-cites="Zheng+2023">(<a href="#ref-Zheng+2023" role="doc-biblioref">Zheng et al., 2023</a>)</span>．</p>
<p>InstructGPT が ChatGPT の前身となっている．</p>
</section>
</section>
<section id="sec-multimodal-transformer" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</h2>
<p>トランスフォーマーは自然言語処理の文脈で開発されたが，画像や動画，音声 <span class="citation" data-cites="Radford+2023">(<a href="#ref-Radford+2023" role="doc-biblioref">Radford et al., 2023a</a>)</span>，さらにはプログラミング言語 <span class="citation" data-cites="Chen+2021">(<a href="#ref-Chen+2021" role="doc-biblioref">Chen et al., 2021</a>)</span> にも適用されている．</p>
<p>動画はまだしも画像には，直感的には時系列構造がないように思えるが，トランスフォーマーはもはや汎用のニューラルネットワークアーキテクチャとして使用できることが解りつつある．</p>
<p>それぞれの応用分野で <strong>モデルの構造は殆ど差異がなく</strong>，トークン化の手法などに差異があるのみのように見受けられる．<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<section id="画像認識トランスフォーマー-vit" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="画像認識トランスフォーマー-vit"><span class="header-section-number">4.1</span> 画像認識トランスフォーマー (ViT)</h3>
<p>画像の分類問題を解くためのエンコーダ・トランスフォーマーは ViT (Vision Transformer) <span class="citation" data-cites="Dosovitskiy+2021">(<a href="#ref-Dosovitskiy+2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span> と呼ばれており，ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) では未だ <a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">ResNet</a> 系のモデルが優勢であった 2021 年に，これを超える性能を示した．</p>
<p>実はモデルは殆どトランスフォーマーそのままであり，肝要であったのは画像をトークン化である．ピクセルをそのまま用いるのではなく，ある程度大きなピクセルの集合である <strong>パッチ</strong> (patch) を用いることで，計算量を下げる．<span class="citation" data-cites="Dosovitskiy+2021">(<a href="#ref-Dosovitskiy+2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span> では <span class="math inline">\(16\times16\)</span> サイズなどが採用された．</p>
<p>一方で，画像を恣意的に系列化しているため，幾何学的な構造は１から学ぶ必要があり，最初からモデルに組み込まれている <a href="../../../posts/2024/Kernels/Deep.html#sec-CNN">CNN</a> よりは一般に多くの訓練データを必要とする．だが，これにより帰納バイアスが弱いということでもある．<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>ViT はその後，動画も扱える ViViT <span class="citation" data-cites="Arnab+2021">(<a href="#ref-Arnab+2021" role="doc-biblioref">Arnab et al., 2021</a>)</span>, あらゆるアスペクト比に対応する NaViT (Native Resolution ViT) <span class="citation" data-cites="Dehghani+2023">(<a href="#ref-Dehghani+2023" role="doc-biblioref">Dehghani et al., 2023</a>)</span> などの拡張が続いた．</p>
<p>DeepMind の <a href="https://deepmind.google/discover/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation/">Perceiver</a> <span class="citation" data-cites="Jaegle+2021">(<a href="#ref-Jaegle+2021" role="doc-biblioref">Jaegle et al., 2021</a>)</span> は画像，動画，音声のいずれのメディアの分類問題にも対応可能な，非対称な注意機構を持つトランスフォーマーである．</p>
</section>
<section id="画像生成トランスフォーマー" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</h3>
<p>一方でデコーダートランスフォーマーを用いて，画像の生成モデリングを行った最初の例は <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> である．</p>
<p>なお，自己回帰的な生成モデルを通じて画像の生成を試みることは，CNN <span class="citation" data-cites="vandenOord+2016b">(<a href="#ref-vandenOord+2016b" role="doc-biblioref">van&nbsp;den&nbsp;Oord, Kalchbrenner, Vinyals, et al., 2016</a>)</span> や RNN <span class="citation" data-cites="vandenOord+2016">(<a href="#ref-vandenOord+2016" role="doc-biblioref">van&nbsp;den&nbsp;Oord, Kalchbrenner, &amp; Kavukcuoglu, 2016</a>)</span> でも行われていた．</p>
<p>この際に判明したことには，画像の分類タスクでは連続表現が役に立っても，生成タスクでは高い解像度を持った画像の生成が難しく，離散表現が有効であることが知られている．しかしこれではデータ量が増えてしまうため，画像の <a href="../../../posts/2024/Computation/VI.html#sec-history">ベクトル量子化</a> が行われることが多い．</p>
<p>そこで，<a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> でも <span class="math inline">\(K\)</span>-平均法によるクラスタリングが行われており，さらに <a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いたデータ圧縮も行われている．</p>
<p><a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> では最終的に各ピクセルを one-hot 表現にまで落とし込み，これを GPT-2 モデル <span class="citation" data-cites="Radford+2019">(<a href="#ref-Radford+2019" role="doc-biblioref">Radford et al., 2019</a>)</span> につなげている．</p>
<p><a href="https://muse-model.github.io/">MUSE</a> <span class="citation" data-cites="Chang+2023">(<a href="#ref-Chang+2023" role="doc-biblioref">Chang et al., 2023</a>)</span> も，トランスフォーマーを用いた画像生成モデルの例である．</p>
<section id="拡散モデルとの邂逅" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="拡散モデルとの邂逅"><span class="header-section-number">4.2.1</span> 拡散モデルとの邂逅</h4>
<p><a href="../../../posts/2024/Kernels/Deep5.html#sec-idea">潜在拡散モデル</a> で <a href="../../../posts/2024/Kernels/Deep.html#sec-U-net">U-Net</a> <span class="citation" data-cites="Ronneberger+2015">(<a href="#ref-Ronneberger+2015" role="doc-biblioref">Ronneberger et al., 2015</a>)</span> を用いていたところをトランスフォーマーに置換した <strong>拡散トランスフォーマー</strong> (DiT: Diffusion Transformer) <span class="citation" data-cites="Peebles-Xie2023">(<a href="#ref-Peebles-Xie2023" role="doc-biblioref">Peebles &amp; Xie, 2023</a>)</span> が発表された．</p>
<p>その後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) <span class="citation" data-cites="Ma+2024">(<a href="#ref-Ma+2024" role="doc-biblioref">Ma et al., 2024</a>)</span> が発表された．</p>
</section>
</section>
<section id="text-to-image-トランスフォーマー" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</h3>
<p>この分野は <span class="citation" data-cites="Reed+2016">(<a href="#ref-Reed+2016" role="doc-biblioref">Reed et al., 2016</a>)</span> 以来，初めは GAN によるアプローチが試みられていた．</p>
<p>GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span> と <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> とは殆ど同じモデルを用いている．これらを組み合わせたデコーダー型のトランスフォーマーが <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(<a href="#ref-Ramesh+2021" role="doc-biblioref">Ramesh et al., 2021</a>)</span> である．<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>トークン化して仕舞えば，言語も画像も等価に扱えるというのである．Google の <a href="https://sites.research.google/parti/">Parti</a> <span class="citation" data-cites="Yu+2022">(<a href="#ref-Yu+2022" role="doc-biblioref">J. Yu et al., 2022</a>)</span> も同様のアプローチである．</p>
<p>Meta の CM3 <span class="citation" data-cites="Aghajanyan+2022">(<a href="#ref-Aghajanyan+2022" role="doc-biblioref">Aghajanyan et al., 2022</a>)</span> と <a href="https://ai.meta.com/blog/generative-ai-text-images-cm3leon/">CM3leon</a> <span class="citation" data-cites="Yu+2023">(<a href="#ref-Yu+2023" role="doc-biblioref">L. Yu et al., 2023</a>)</span> は画像と言語を両方含んだ HTML ドキュメントから学習している．</p>
<p>Google DeepMind の <a href="https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/">Flamingo</a> <span class="citation" data-cites="Alayrac+2022">(<a href="#ref-Alayrac+2022" role="doc-biblioref">Alayrac et al., 2022</a>)</span> は画像から言語を生成する．</p>
</section>
<section id="image-to-text-トランスフォーマー" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="image-to-text-トランスフォーマー"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/clip">CLIP</a> (Contrastive Language-Image Pre-training) <span class="citation" data-cites="Radford+2021">(<a href="#ref-Radford+2021" role="doc-biblioref">Radford et al., 2021</a>)</span> は画像の表現学習をする視覚モデルである．これは <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(<a href="#ref-Ramesh+2021" role="doc-biblioref">Ramesh et al., 2021</a>)</span> と同時に開発された重要な構成要素である．</p>
<p>一方で <a href="https://openai.com/dall-e-2">DALL-E2</a> <span class="citation" data-cites="Ramesh+2022">(<a href="#ref-Ramesh+2022" role="doc-biblioref">Ramesh et al., 2022</a>)</span> では，CLIP により画像を潜在空間にエンコードし，拡散モデルによってデコードする．</p>
<p><a href="https://openai.com/research/dall-e-3-system-card">DALL-E3</a> <span class="citation" data-cites="OpenAI2023DallE3">(<a href="#ref-OpenAI2023DallE3" role="doc-biblioref">OpenAI, 2023a</a>)</span> もその改良である．</p>
</section>
<section id="動画生成トランスフォーマー" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="動画生成トランスフォーマー"><span class="header-section-number">4.5</span> 動画生成トランスフォーマー</h3>
<p>動画を画像の連続と見てトランスフォーマーを応用するアプローチは Latte (Latent Diffusion Transformer) <span class="citation" data-cites="Rakhimov+2020">(<a href="#ref-Rakhimov+2020" role="doc-biblioref">Rakhimov et al., 2020</a>)</span> に始まる．</p>
<p><a href="https://wilson1yan.github.io/videogpt/index.html">VideoGPT</a> <span class="citation" data-cites="Yan+2021">(<a href="#ref-Yan+2021" role="doc-biblioref">Yan et al., 2021</a>)</span> では動画を 3D の CNN でデータ圧縮，VQ-VAE で量子化して離散的な潜在表現を得た後，GPT と殆ど似たトランスフォーマーに通して学習する．</p>
<p><a href="https://wayve.ai/company/">Wayve</a> の <a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(<a href="#ref-Hu+2023" role="doc-biblioref">A. Hu et al., 2023</a>)</span> も同様の手法で動画を生成しているが，その動画を用いて自動運転の強化学習に応用する点が画期的である．</p>
<p>OpenAI は 2/15/2024 に <a href="https://openai.com/sora">Sora</a> <span class="citation" data-cites="Brooks+2024">(<a href="#ref-Brooks+2024" role="doc-biblioref">Brooks et al., 2024</a>)</span> を発表した．これも <a href="../../../posts/2024/Kernels/Deep5.html#sec-idea">潜在拡散モデル</a> <span class="citation" data-cites="Rombach+2022">(<a href="#ref-Rombach+2022" role="doc-biblioref">Rombach et al., 2022</a>)</span> 同様，自己符号化器による動画の潜在表現を得た上でパッチに分割し，この上で拡散トランスフォーマー <span class="citation" data-cites="Peebles-Xie2023">(<a href="#ref-Peebles-Xie2023" role="doc-biblioref">Peebles &amp; Xie, 2023</a>)</span> の学習を行う．</p>
</section>
<section id="世界モデルとしてのトランスフォーマー" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</h3>
<p>トランスフォーマーを世界モデルとして用いて，シミュレーションを行い動画を生成し，これをモデルベースの強化学習 <span class="citation" data-cites="Sutton-Barto2018">(<a href="#ref-Sutton-Barto2018" role="doc-biblioref">R. S. Sutton &amp; Barto, 2018</a>)</span> の材料とすることが広く提案されている．これは learning in imagination <span class="citation" data-cites="Racaniere+2017">(<a href="#ref-Racaniere+2017" role="doc-biblioref">Racanière et al., 2017</a>)</span> と呼ばれる．<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>IRIS (Imagination with auto-Regression over an Inner Speech) <span class="citation" data-cites="Micheli+2023">(<a href="#ref-Micheli+2023" role="doc-biblioref">Micheli et al., 2023</a>)</span> はこれに初めてトランスフォーマーを用いた世界モデルから動画生成をした．</p>
<p><a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(<a href="#ref-Hu+2023" role="doc-biblioref">A. Hu et al., 2023</a>)</span> は自動運転に特化した世界モデルを，トランスフォーマーを用いて構築している．</p>
<p>他にも，動画生成を強化学習に応用する例としては，OpenAI による <a href="https://openai.com/research/vpt">VPT (Video Pre-Training)</a> <span class="citation" data-cites="Baker+2022">(<a href="#ref-Baker+2022" role="doc-biblioref">Baker et al., 2022</a>)</span> がある．</p>
</section>
<section id="音声生成トランスフォーマー" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="音声生成トランスフォーマー"><span class="header-section-number">4.7</span> 音声生成トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/jukebox">Jukebox</a> <span class="citation" data-cites="Dhariwal+2020">(<a href="#ref-Dhariwal+2020" role="doc-biblioref">Dhariwal et al., 2020</a>)</span> は，<a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いて音声データを圧縮・量子化し，トランスフォーマーに通したものである．</p>
<p>このトランスフォーマーは <a href="https://openai.com/research/sparse-transformer">Sparse Transformer</a> <span class="citation" data-cites="Child+2019">(<a href="#ref-Child+2019" role="doc-biblioref">Child et al., 2019</a>)</span> という，注意機構の計算効率を改良したモデルを用いている．</p>
</section>
<section id="text-to-speech-トランスフォーマー" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</h3>
<p>Microsoft Research の <a href="https://www.microsoft.com/en-us/research/project/vall-e-x/">VALL-E</a> <span class="citation" data-cites="Wang+2023">(<a href="#ref-Wang+2023" role="doc-biblioref">Wang et al., 2023</a>)</span> は，音声データをベクトル量子化によって言語データと全く同等に扱うことで，トランスフォーマーを用いて音声生成を行っている．</p>
</section>
<section id="speach-to-text-トランスフォーマー" class="level3" data-number="4.9">
<h3 data-number="4.9" class="anchored" data-anchor-id="speach-to-text-トランスフォーマー"><span class="header-section-number">4.9</span> Speach to Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/whisper">Whisper</a> <span class="citation" data-cites="Radford+2023-Whisper">(<a href="#ref-Radford+2023-Whisper" role="doc-biblioref">Radford et al., 2023b</a>)</span> は encoder-decoder 型のトランスフォーマーを用いている．</p>
</section>
</section>
<section id="近年の動向" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="近年の動向"><span class="header-section-number">5</span> 近年の動向</h2>
<section id="llama-の一般公開とその影響" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="llama-の一般公開とその影響"><span class="header-section-number">5.1</span> LLaMA の一般公開とその影響</h3>
<p>Meta AI が <a href="https://about.fb.com/ja/news/2023/07/meta-and-microsoft-introduce-the-next-generation-of-llama/">7/18/2023</a> に LLM <a href="https://llama.meta.com/">LLaMA</a> <span class="citation" data-cites="Touvron+2023">(<a href="#ref-Touvron+2023" role="doc-biblioref">Touvron et al., 2023</a>)</span> を公開した．そして API を通じて利用する形ではなく，そのモデルのウェイトが公開されたため，Stanford 大学の <a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> など，モデルの改良と研究が促進されている．</p>
<p>特に公開事後調整データセットの整備が進んでおり，<a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> では Self-Instruct <span class="citation" data-cites="Wang+2023">(<a href="#ref-Wang+2023" role="doc-biblioref">Wang et al., 2023</a>)</span> による効率的な alignment 技術が採用されている．</p>
<p>産業界でも影響は大きい．<a href="https://elyza.ai/">ELYZA</a> は <a href="https://elyza.ai/news/2023/12/27/130%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E5%95%86%E7%94%A8%E5%88%A9%E7%94%A8%E5%8F%AF%E8%83%BD%E3%81%AA%E6%97%A5%E6%9C%AC%E8%AA%9Ellmelyza-j">12/27/2023</a> に日本語に特化した LLM である ELYZA-japanese-Llama-2-13b を公開している．<a href="https://stockmark.co.jp/">Stockmark</a> も <a href="https://stockmark.co.jp/news/20231027">10/27/2023</a> に Stockmark-13b を公開している．</p>
<p>いずれも，開発費と開発時間が大幅に圧縮されたという．<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p>IBM は <a href="https://jp.newsroom.ibm.com/2023-09-12-Blog-Building-AI-for-business-IBM-Granite-foundation-models">9/12/2023</a> に LLM Granite を発表している．加えて，プラットフォーム <code>watsonx</code> も提供しており，その上で RAG など独自の事後調整を可能にしている．</p>
<p>IBM と Meta の２社が発起人となり，<a href="https://www.ibm.com/blogs/solutions/jp-ja/the-ai-alliance/">12/5/2023</a> に AI Alliance が発足し，オープンイノベーションを推進している．</p>
<p><a href="https://ja.stability.ai/stable-diffusion">Stable Diffusion</a> <span class="citation" data-cites="Rombach+2022">(<a href="#ref-Rombach+2022" role="doc-biblioref">Rombach et al., 2022</a>)</span> もソースコードとウェイトが <a href="https://github.com/Stability-AI/stablediffusion">一般公開</a> されている．</p>
</section>
<section id="llm-の経済的影響" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="llm-の経済的影響"><span class="header-section-number">5.2</span> LLM の経済的影響</h3>
<p><span class="citation" data-cites="Tamkin+2021">(<a href="#ref-Tamkin+2021" role="doc-biblioref">Tamkin et al., 2021</a>)</span> は早い段階での OpenAI と Stanford 大学 <a href="https://hai.stanford.edu/">HAI</a> (Human-centered AI) との対談録である．</p>
<p>OpenAI はコード生成能力の経済的な影響を重要なアジェンダとしている <span class="citation" data-cites="Manning+2022">(<a href="#ref-Manning+2022" role="doc-biblioref">Manning et al., 2022</a>)</span>．</p>
<p>Open AI の <a href="https://openai.com/blog/openai-codex">Codex</a> <span class="citation" data-cites="Chen+2021">(<a href="#ref-Chen+2021" role="doc-biblioref">Chen et al., 2021</a>)</span> はプログラム言語を扱うトランスフォーマーであり，<a href="https://github.com/features/copilot">GitHub Copilot</a> の元となっている．これが社会に与える影響も，新たな評価フレームワークと共に提案されている <span class="citation" data-cites="Khlaaf+2022">(<a href="#ref-Khlaaf+2022" role="doc-biblioref">Khlaaf et al., 2022</a>)</span>．</p>
<p>LLM の労働市場へのインパクトも推定している <span class="citation" data-cites="Eloundou+2023">(<a href="#ref-Eloundou+2023" role="doc-biblioref">Eloundou et al., 2023</a>)</span>．これによると，アメリカの労働者の 80% が，LLM の導入により少なくとも仕事の 10% に影響が生じるとしている．さらに全体の 20% は仕事の半分以上が影響を受けるとしている．</p>
</section>
<section id="種々の下流タスクの発見" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="種々の下流タスクの発見"><span class="header-section-number">5.3</span> 種々の下流タスクの発見</h3>
<section id="bayesian-ai" class="level4" data-number="5.3.1">
<h4 data-number="5.3.1" class="anchored" data-anchor-id="bayesian-ai"><span class="header-section-number">5.3.1</span> Bayesian AI</h4>
<p>意思決定の場面において AI を活用するには，不確実性の定量化が必要不可欠である．</p>
<p>GPT-3 を Bayesian にし，自身の確証度合いを言表するように事後調整する研究が OpenAI で行われている <span class="citation" data-cites="Lin+2022">(<a href="#ref-Lin+2022" role="doc-biblioref">Lin et al., 2022</a>)</span>．</p>
</section>
<section id="社会行動シミュレーターとしての-llm" class="level4" data-number="5.3.2">
<h4 data-number="5.3.2" class="anchored" data-anchor-id="社会行動シミュレーターとしての-llm"><span class="header-section-number">5.3.2</span> 社会行動シミュレーターとしての LLM</h4>
<p>社会的なシミュレーションを LLM 内で行うことで，社会科学やビジネスの場面での意思決定を支援することが期待されている．</p>
<p>LLM は人間の心の理論を理解し，その心情・意図を（ある程度）シミュレートすることが出来るようである <span class="citation" data-cites="Andreas2022">(<a href="#ref-Andreas2022" role="doc-biblioref">Andreas, 2022</a>)</span>．</p>
<p>LLM でのシミュレーションを通じて，社会科学的な知識を引き出そうとする試みもある <span class="citation" data-cites="Leng-Yuan2023">(<a href="#ref-Leng-Yuan2023" role="doc-biblioref">Leng &amp; Yuan, 2023</a>)</span>．</p>
</section>
</section>
<section id="llm-と経済安全保障" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="llm-と経済安全保障"><span class="header-section-number">5.4</span> LLM と経済安全保障</h3>
<section id="偽情報対策" class="level4" data-number="5.4.1">
<h4 data-number="5.4.1" class="anchored" data-anchor-id="偽情報対策"><span class="header-section-number">5.4.1</span> 偽情報対策</h4>
<p>生成 AI は，一国の政府が特定のプロパガンダを流布するための効果的な手段として選ばれることになる．その際の考え得る使用例と，それに対する対策が考えられてる <span class="citation" data-cites="Goldstein+2023">(<a href="#ref-Goldstein+2023" role="doc-biblioref">Goldstein et al., 2023</a>)</span>．</p>
</section>
<section id="開発規制" class="level4" data-number="5.4.2">
<h4 data-number="5.4.2" class="anchored" data-anchor-id="開発規制"><span class="header-section-number">5.4.2</span> 開発規制</h4>
<p><span class="citation" data-cites="Anderljung+2023">(<a href="#ref-Anderljung+2023" role="doc-biblioref">Anderljung et al., 2023</a>)</span> は先端的な AI を <a href="https://www.cnas.org/publications/commentary/frontier-ai-regulation-managing-emerging-risks-to-public-safety">Frontier AI</a> と呼び，これの開発過程におけるあるべき規制を模索している．監督当局に執行権を付与することやフロンティアAIモデルのライセンス制度などが議論されている．</p>
<p><span class="citation" data-cites="Shoker+2023">(<a href="#ref-Shoker+2023" role="doc-biblioref">Shoker et al., 2023</a>)</span> は LLM と国家安全保障との関係を議論している．信頼構築措置 (CBMs: Confidence-Building Measures) とは，国家間の敵意を減少させることで，衝突のリスクを減らす措置の全般をいう．元々は冷戦時代に提案された概念であるが，これを LLM 開発に適用することが具体的に提案されている．</p>
</section>
<section id="生物学的脅威" class="level4" data-number="5.4.3">
<h4 data-number="5.4.3" class="anchored" data-anchor-id="生物学的脅威"><span class="header-section-number">5.4.3</span> 生物学的脅威</h4>
<p>LLM の登場により個人がエンパワーメントを受けており，生物学的脅威を作る障壁が低下していることは間違いない．</p>
<p><span class="citation" data-cites="Patwardhan+2024">(<a href="#ref-Patwardhan+2024" role="doc-biblioref">Patwardhan et al., 2024</a>)</span> では，生物学的リスクに焦点を当てて，AI による安全リスク評価の手法と事前警鐘システムを模索している．この研究では，LLM によりリスクが増加するという統計的に有意義な証拠は得られていないが，この方面の研究の草分けとなっている．</p>
</section>
</section>
<section id="アラインメント" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="アラインメント"><span class="header-section-number">5.5</span> アラインメント</h3>
<p>DALL-E2 では訓練前の緩和策も取られている <span class="citation" data-cites="Nichol2022">(<a href="#ref-Nichol2022" role="doc-biblioref">Nichol, 2022</a>)</span>．</p>
<section id="プログラムの支援" class="level4" data-number="5.5.1">
<h4 data-number="5.5.1" class="anchored" data-anchor-id="プログラムの支援"><span class="header-section-number">5.5.1</span> プログラムの支援</h4>
<p>遺伝的プログラムの改良の過程を模倣できる <span class="citation" data-cites="Lehman+2024">(<a href="#ref-Lehman+2024" role="doc-biblioref">Lehman et al., 2024</a>)</span> として，ソフトウェア開発やロボット開発分野での，プログラムの漸次的改良への応用が考えられている．</p>
<p>困難なタスクに対して AI がアシストするという研究もある <span class="citation" data-cites="Saunders+2022">(<a href="#ref-Saunders+2022" role="doc-biblioref">Saunders et al., 2022</a>)</span>．これは最終的に，AI のアラインメントにおいても重要な技術になるとしている．</p>
</section>
<section id="超アラインメント問題" class="level4" data-number="5.5.2">
<h4 data-number="5.5.2" class="anchored" data-anchor-id="超アラインメント問題"><span class="header-section-number">5.5.2</span> 超アラインメント問題</h4>
<p>これは，OpenAI のアラインメント研究が次の３本の柱であることが背景にある <span class="citation" data-cites="Leike+2022">(<a href="#ref-Leike+2022" role="doc-biblioref">Leike et al., 2022</a>)</span></p>
<ol type="1">
<li>人間のフィードバックによる AI の強化学習</li>
<li>AI の支援を通じて人間のフィードバックを正確にする</li>
<li>AI を通じてアラインメントの研究を促進する</li>
</ol>
<p>の３つである．</p>
<p>第３の柱として，GPT-4 <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023b</a>)</span> によるシミュレーションを通じて，特定のニューロンがどのような出力に対応しているかを解明する手法を提案している <span class="citation" data-cites="Leike+2023">(<a href="#ref-Leike+2023" role="doc-biblioref">Leike et al., 2023</a>)</span>．これにより，人間が直接調べる行為が自動化され，アラインメントの研究が効率化され，スケーラブルな手法になるということである．</p>
<p>さらに，将来的なアラインメントは RLHF では出来なくなっていく．人智を超えた超知能 (superintelligence) をアラインメントすることを，超アラインメントと呼び，OpenAI は 2023 年の暮れに <a href="https://openai.com/blog/introducing-superalignment">超アラインメントチーム</a> を創設し，人間が「弱い監督者」となってしまった状況でもどのように超アラインメントを実行すれば良いかを研究するとしている．</p>
</section>
<section id="ai-エージェントへの道" class="level4" data-number="5.5.3">
<h4 data-number="5.5.3" class="anchored" data-anchor-id="ai-エージェントへの道"><span class="header-section-number">5.5.3</span> AI エージェントへの道</h4>
<p>OpenAI は能動的 AI システム (Agentic AI system) の構築に向けて，安全な運用と責任のある管理を目指す白書 <span class="citation" data-cites="Shavit+2023">(<a href="#ref-Shavit+2023" role="doc-biblioref">Shavit et al., 2023</a>)</span> を発表した．</p>



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-Aghajanyan+2021" class="csl-entry" role="listitem">
Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic dimensionality explains the effectiveness of language model fine-tuning. <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</em>, <em>1</em>, 7319–7328. <a href="https://aclanthology.org/2021.acl-long.568/">https://aclanthology.org/2021.acl-long.568/</a>
</div>
<div id="ref-Aghajanyan+2022" class="csl-entry" role="listitem">
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., &amp; Zettlemoyer, L. (2022). <em>CM3: A causal masked multimodal model of the internet</em>. <a href="https://arxiv.org/abs/2201.07520">https://arxiv.org/abs/2201.07520</a>
</div>
<div id="ref-Aharoni+2019" class="csl-entry" role="listitem">
Aharoni, R., Johnson, M., &amp; Firat, O. (2019). Massively multilingual neural machine translation. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.), <em>Proceedings of the 2019 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</em> (pp. 3874–3884). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1388">https://doi.org/10.18653/v1/N19-1388</a>
</div>
<div id="ref-Alayrac+2022" class="csl-entry" role="listitem">
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., … Simonyan, K. (2022). <em>Flamingo: A visual language model for few-shot learning</em>. <a href="https://arxiv.org/abs/2204.14198">https://arxiv.org/abs/2204.14198</a>
</div>
<div id="ref-Anderljung+2023" class="csl-entry" role="listitem">
Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O’Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., … Wolf, K. (2023). <em>Frontier AI regulation: Managing emerging risks to public safety</em>. OpenAI. <a href="https://openai.com/research/frontier-ai-regulation">https://openai.com/research/frontier-ai-regulation</a>
</div>
<div id="ref-Andreas2022" class="csl-entry" role="listitem">
Andreas, J. (2022). <em>Language models as agent models</em>. <a href="https://arxiv.org/abs/2212.01681">https://arxiv.org/abs/2212.01681</a>
</div>
<div id="ref-Arnab+2021" class="csl-entry" role="listitem">
Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., &amp; Schmid, C. (2021). ViViT: A video vision transformer. <em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 6816–6826. <a href="https://doi.org/10.1109/ICCV48922.2021.00676">https://doi.org/10.1109/ICCV48922.2021.00676</a>
</div>
<div id="ref-Ba+2016" class="csl-entry" role="listitem">
Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016). <em>Layer normalization</em>. <a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>
</div>
<div id="ref-Bahdanau+2015" class="csl-entry" role="listitem">
Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). <em>Neural machine translation by jointly learning to align and translate</em>. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>
</div>
<div id="ref-Baker+2022" class="csl-entry" role="listitem">
Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., &amp; Clune, J. (2022). <em>Video PreTraining (VPT): Learning to act by watching unlabeled online videos</em>. <a href="https://arxiv.org/abs/2206.11795">https://arxiv.org/abs/2206.11795</a>
</div>
<div id="ref-Bengio+2000" class="csl-entry" role="listitem">
Bengio, Y., Ducharme, R., &amp; Vincent, P. (2000). A neural probabilistic language model. <em>Advances in Neural Information Processing Systems</em>, <em>13</em>. <a href="https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html">https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html</a>
</div>
<div id="ref-Bjorck+2018" class="csl-entry" role="listitem">
Bjorck, N., Gomes, C. P., Selman, B., &amp; Weinberger, K. Q. (2018). Understanding batch normalization. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>. <a href="https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html">https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html</a>
</div>
<div id="ref-Bommasani+2021" class="csl-entry" role="listitem">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., Arx, S. von, Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K. A., Davis, J., Demszky, D., … Liang, P. (2021). On the opportunities and risks of foundation models. <em>ArXiv</em>. <a href="https://crfm.stanford.edu/report.html">https://crfm.stanford.edu/report.html</a>
</div>
<div id="ref-Brooks+2024" class="csl-entry" role="listitem">
Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C. W. Y., Wang, R., &amp; Ramesh, A. (2024). <em>Video generation models as world simulators</em>. OpenAI. <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a>
</div>
<div id="ref-Brown+2020" class="csl-entry" role="listitem">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., … Amodei, D. (2020). Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 1877–1901. <a href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>
</div>
<div id="ref-Bubeck+2023" class="csl-entry" role="listitem">
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., &amp; Zhang, Y. (2023). <em>Sparks of artificial general intelligence: Early experiments with GPT-4</em>. <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>
</div>
<div id="ref-Chang+2023" class="csl-entry" role="listitem">
Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K. P., Freeman, W. T., Rubinstein, M., Li, Y., &amp; Krishnan, D. (2023). Muse: Text-to-image generation via masked generative transformers. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, &amp; J. Scarlett (Eds.), <em>Proceedings of the 40th international conference on machine learning</em> (Vol. 202, pp. 4055–4075). PMLR. <a href="https://proceedings.mlr.press/v202/chang23b.html">https://proceedings.mlr.press/v202/chang23b.html</a>
</div>
<div id="ref-Chen+2020" class="csl-entry" role="listitem">
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., &amp; Sutskever, I. (2020). Generative pretraining from pixels. <em>Proceedings of the 37th International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v119/chen20s.html">https://proceedings.mlr.press/v119/chen20s.html</a>
</div>
<div id="ref-Chen+2021" class="csl-entry" role="listitem">
Chen, M., Tworek, J., Jun, H., Yuan, Q., Oliveira Pinto, H. P. de, Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). <em>Evaluating large language models trained on code</em>. <a href="https://arxiv.org/abs/2107.03374">https://arxiv.org/abs/2107.03374</a>
</div>
<div id="ref-Child+2019" class="csl-entry" role="listitem">
Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). <em>Generating long sequences with sparse transformers</em>. <a href="https://arxiv.org/abs/1904.10509">https://arxiv.org/abs/1904.10509</a>
</div>
<div id="ref-Cho+2014" class="csl-entry" role="listitem">
Cho, K., Merriënboer, B. van, Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using <span>RNN</span> encoder<span>–</span>decoder for statistical machine translation. In A. Moschitti, B. Pang, &amp; W. Daelemans (Eds.), <em>Proceedings of the 2014 conference on empirical methods in natural language processing (<span>EMNLP</span>)</em> (pp. 1724–1734). Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1179">https://doi.org/10.3115/v1/D14-1179</a>
</div>
<div id="ref-Chowdhery+2022" class="csl-entry" role="listitem">
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). <em>PaLM: Scaling language modeling with pathways</em>. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>
</div>
<div id="ref-Christiano+2017" class="csl-entry" role="listitem">
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>
</div>
<div id="ref-Dai-Le2015" class="csl-entry" role="listitem">
Dai, A. M., &amp; Le, Q. V. (2015). <em>Semi-supervised sequence learning</em>. <a href="https://arxiv.org/abs/1511.01432">https://arxiv.org/abs/1511.01432</a>
</div>
<div id="ref-Dehghani+2023" class="csl-entry" role="listitem">
Dehghani, M., Mustafa, B., Djolonga, J., Heek, J., Minderer, M., Caron, M., Steiner, A. P., Puigcerver, J., Geirhos, R., Alabdulmohsin, I., Oliver, A., Padlewski, P., Gritsenko, A. A., Lucic, M., &amp; Houlsby, N. (2023). Patch n<span>’</span> pack: NaViT, a vision transformer for any aspect ratio and resolution. <em>Thirty-Seventh Conference on Neural Information Processing Systems</em>. <a href="https://openreview.net/forum?id=VpGFHmI7e5">https://openreview.net/forum?id=VpGFHmI7e5</a>
</div>
<div id="ref-Devlin+2019" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, <em>1</em>, 4171–4186. <a href="https://aclanthology.org/N19-1423/">https://aclanthology.org/N19-1423/</a>
</div>
<div id="ref-Dhariwal+2020" class="csl-entry" role="listitem">
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., &amp; Sutskever, I. (2020). <em>Jukebox: A generative model for music</em>. <a href="https://arxiv.org/abs/2005.00341">https://arxiv.org/abs/2005.00341</a>
</div>
<div id="ref-Dosovitskiy+2021" class="csl-entry" role="listitem">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp; Houlsby, N. (2021). An image is worth 16×16 words: Transformers for image recognition at scale. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a>
</div>
<div id="ref-Dufter+2021" class="csl-entry" role="listitem">
Dufter, P., Schmitt, M., &amp; Schütze, H. (2021). <em>Position information in transformers: An overview</em>. <a href="https://arxiv.org/abs/2102.11090">https://arxiv.org/abs/2102.11090</a>
</div>
<div id="ref-Eloundou+2023" class="csl-entry" role="listitem">
Eloundou, T., Manning, S., Mishkin, P., &amp; Rock, D. (2023). <em>GPTs are GPTs: An early look at the labor market impact potential of large language models</em>. <a href="https://openai.com/research/gpts-are-gpts">https://openai.com/research/gpts-are-gpts</a>
</div>
<div id="ref-Fedus+2022" class="csl-entry" role="listitem">
Fedus, W., Zoph, B., &amp; Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. <em>The Journal of Machine Learning Research</em>, <em>23</em>(120), 1–39. <a href="https://jmlr.org/papers/v23/21-0998.html">https://jmlr.org/papers/v23/21-0998.html</a>
</div>
<div id="ref-Fu+2023" class="csl-entry" role="listitem">
Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., &amp; Re, C. (2023). Hungry hungry hippos: Towards language modeling with state space models. <em>The Eleventh International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=COZDy0WYGg">https://openreview.net/forum?id=COZDy0WYGg</a>
</div>
<div id="ref-Goldstein+2023" class="csl-entry" role="listitem">
Goldstein, J. A., Sastry, G., Musser, M., DiResta, R., Gentzel, M., &amp; Sedova, K. (2023). <em>Generative language models and automated influence operations: Emerging threats and potential mitigations</em>. <a href="https://openai.com/research/forecasting-misuse">https://openai.com/research/forecasting-misuse</a>
</div>
<div id="ref-Gu+2022" class="csl-entry" role="listitem">
Gu, A., Goel, K., &amp; Re, C. (2022). Efficiently modeling long sequences with structured state spaces. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=uYLFoz1vlAC">https://openreview.net/forum?id=uYLFoz1vlAC</a>
</div>
<div id="ref-Guu+2020" class="csl-entry" role="listitem">
Guu, K., Lee, K., Tung, Z., Pasupat, P., &amp; Chang, M.-W. (2020). REALM: Retrieval-augmented language model pre-training. <em>Proceedings of the 37th International Conference on Machine Learning</em>.
</div>
<div id="ref-Ha-Schmidthuber2018" class="csl-entry" role="listitem">
Ha, D., &amp; Schmidhuber, J. (2018). Recurrent world models facilitate policy evaluation. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>. <a href="https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html">https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html</a>
</div>
<div id="ref-Hafner+2021" class="csl-entry" role="listitem">
Hafner, D., Lillicrap, T. P., Norouzi, M., &amp; Ba, J. (2021). Mastering atari with discrete world models. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=0oabwyZbOu">https://openreview.net/forum?id=0oabwyZbOu</a>
</div>
<div id="ref-Hashimoto2024" class="csl-entry" role="listitem">
Hashimoto, T. (2024). <em>Large language models</em>. Lecture at MLSS2024.
</div>
<div id="ref-Hestness+2017" class="csl-entry" role="listitem">
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, Md. M. A., Yang, Y., &amp; Zhou, Y. (2017). <em>Deep learning scaling is predictable, empirically</em>. <a href="https://arxiv.org/abs/1712.00409">https://arxiv.org/abs/1712.00409</a>
</div>
<div id="ref-Hochreiter-Schmidhuber1997" class="csl-entry" role="listitem">
Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-time memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780. <a href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext</a>
</div>
<div id="ref-Hoffmann+2022" class="csl-entry" role="listitem">
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Las Casas, D. de, Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. van den, Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). <em>Training compute-optimal large language models</em>. <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>
</div>
<div id="ref-Holtzman+2020" class="csl-entry" role="listitem">
Holtzman, A., Buys, J., Du, L., Forbes, M., &amp; Choi, Y. (2020). The curious case of neural text degeneration. <em>International Conference on Learning Representation</em>. <a href="https://arxiv.org/abs/1904.09751">https://arxiv.org/abs/1904.09751</a>
</div>
<div id="ref-Hu+2023" class="csl-entry" role="listitem">
Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., &amp; Corrado, G. (2023). <em>GAIA-1: A generative world model for autonomous driving</em>. <a href="https://arxiv.org/abs/2309.17080">https://arxiv.org/abs/2309.17080</a>
</div>
<div id="ref-Hu+2021" class="csl-entry" role="listitem">
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). <em>LoRA: Low-rank adaptation of large language models</em>. <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>
</div>
<div id="ref-Ioffe-Szegedy2015" class="csl-entry" role="listitem">
Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. <em>Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37</em>, 448–456. <a href="https://dl.acm.org/doi/10.5555/3045118.3045167">https://dl.acm.org/doi/10.5555/3045118.3045167</a>
</div>
<div id="ref-Jaegle+2021" class="csl-entry" role="listitem">
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., &amp; Carreira, J. (2021). Perceiver: General perception with iterative attention. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 4651–4664). PMLR. <a href="https://proceedings.mlr.press/v139/jaegle21a.html">https://proceedings.mlr.press/v139/jaegle21a.html</a>
</div>
<div id="ref-Kaiser+2020" class="csl-entry" role="listitem">
Kaiser, Ł., Babaeizadeh, M., Miłos, P., Osiński, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., &amp; Michalewski, H. (2020). Model based reinforcement learning for atari. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=S1xCPJHtDB">https://openreview.net/forum?id=S1xCPJHtDB</a>
</div>
<div id="ref-Kaplan+2020" class="csl-entry" role="listitem">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020). <em>Scaling laws for neural language models</em>. <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a>
</div>
<div id="ref-Karpukhin+2020" class="csl-entry" role="listitem">
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., &amp; Yih, W. (2020). Dense passage retrieval for open-domain question answering. In B. Webber, T. Cohn, Y. He, &amp; Y. Liu (Eds.), <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 6769–6781). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.550">https://doi.org/10.18653/v1/2020.emnlp-main.550</a>
</div>
<div id="ref-Khlaaf+2022" class="csl-entry" role="listitem">
Khlaaf, H., Mishkin, P., Achiam, J., Krueger, G., &amp; Brundage, M. (2022). <em>A hazard analysis framework for code synthesis large language models</em>. <a href="https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models">https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models</a>
</div>
<div id="ref-LeCun+2012" class="csl-entry" role="listitem">
LeCun, Y. A., Bottou, L., Orr, G. B., &amp; Müller, K.-R. (2012). <em>Neural networks: Tricks of the trade</em> (G. Montavon, G. B. Orr, &amp; K.-R. Müller, Eds.; 2nd ed., pp. 9–48). Springer Berlin, Heidelberg. <a href="https://link.springer.com/book/10.1007/978-3-642-35289-8">https://link.springer.com/book/10.1007/978-3-642-35289-8</a>
</div>
<div id="ref-Lehman+2024" class="csl-entry" role="listitem">
Lehman, J., Jain, J. G. S., Ndousse, K., Yah, C., &amp; Stanley, K. O. (2024). <em>Handbook of evolutionary machine learning</em> (W. Banzhaf, P. Machado, &amp; M. Zhang, Eds.; pp. 331–366). Springer Singapore. <a href="https://link.springer.com/chapter/10.1007/978-981-99-3814-8_11">https://link.springer.com/chapter/10.1007/978-981-99-3814-8_11</a>
</div>
<div id="ref-Leike+2022" class="csl-entry" role="listitem">
Leike, J., Schulman, J., &amp; Wu, J. (2022). <em>Our approach to alignment research</em>. OpenAI. <a href="https://openai.com/blog/our-approach-to-alignment-research">https://openai.com/blog/our-approach-to-alignment-research</a>
</div>
<div id="ref-Leike+2023" class="csl-entry" role="listitem">
Leike, J., Wu, J., Bills, S., Saunders, W., Gao, L., Tillman, H., &amp; Mossing, D. (2023). <em>Language models can explain neurons in language models</em>. OpenAI. <a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models">https://openai.com/research/language-models-can-explain-neurons-in-language-models</a>
</div>
<div id="ref-Leng-Yuan2023" class="csl-entry" role="listitem">
Leng, Y., &amp; Yuan, Y. (2023). <em>Do LLM agents exhibit social behavior?</em> <a href="https://arxiv.org/abs/2312.15198">https://arxiv.org/abs/2312.15198</a>
</div>
<div id="ref-Lepikhin+2021" class="csl-entry" role="listitem">
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., &amp; Chen, Z. (2021). GShard: Scaling giant models with conditional computation and automatic sharding. <em>International Conference on Learning Representation</em>. <a href="https://openreview.net/forum?id=qrwe7XHTmYb">https://openreview.net/forum?id=qrwe7XHTmYb</a>
</div>
<div id="ref-Lewis+2020-BART" class="csl-entry" role="listitem">
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., &amp; Zettlemoyer, L. (2020). <span>BART</span>: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In D. Jurafsky, J. Chai, N. Schluter, &amp; J. Tetreault (Eds.), <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em> (pp. 7871–7880). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a>
</div>
<div id="ref-Lewis+2020" class="csl-entry" role="listitem">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 9459–9474. <a href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>
</div>
<div id="ref-Lieber+2021" class="csl-entry" role="listitem">
Lieber, O., Sharir, O., Lenz, B., &amp; Shoham, Y. (2021). <em>Jurrassic-1: Technical details and evaluation</em>. AI21. <a href="https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1">https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1</a>
</div>
<div id="ref-Lin+2022" class="csl-entry" role="listitem">
Lin, S., Hilton, J., &amp; Evans, O. (2022). Teaching models to express their uncertainty in words. <em>Transactions on Machine Learning Research</em>. <a href="https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words">https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words</a>
</div>
<div id="ref-Liu+2023" class="csl-entry" role="listitem">
Liu, X., Gong, C., &amp; liu, qiang. (2023). Flow straight and fast: Learning to generate and transfer data with rectified flow. <em>The Eleventh International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=XVjTT1nw5z">https://openreview.net/forum?id=XVjTT1nw5z</a>
</div>
<div id="ref-Ma+2024" class="csl-entry" role="listitem">
Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., &amp; Xie, S. (2024). <em>SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers</em>. <a href="https://arxiv.org/abs/2401.08740">https://arxiv.org/abs/2401.08740</a>
</div>
<div id="ref-Manning+2022" class="csl-entry" role="listitem">
Manning, S., Mishkin, P., Hadfield, G., Eloundou, T., &amp; Eisne, E. (2022). <em>A research agenda for assessing the economic impacts of code generation models</em>. OpenAI. <a href="https://openai.com/research/economic-impacts">https://openai.com/research/economic-impacts</a>
</div>
<div id="ref-Marcus2020" class="csl-entry" role="listitem">
Marcus, G. (2020). <em>The next decade in AI: Four steps towards robust artificial intelligence</em>. <a href="https://arxiv.org/abs/2002.06177">https://arxiv.org/abs/2002.06177</a>
</div>
<div id="ref-Micheli+2023" class="csl-entry" role="listitem">
Micheli, V., Alonso, E., &amp; Fleuret, F. (2023). Transformers are sample-efficient world models. <em>International Conference on Learning Representation</em>. <a href="https://openreview.net/forum?id=vhFu1Acb0xb">https://openreview.net/forum?id=vhFu1Acb0xb</a>
</div>
<div id="ref-Mikolov2013" class="csl-entry" role="listitem">
Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Efficient estimation of word representations in vector space</em>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>
</div>
<div id="ref-Mikolov+2010" class="csl-entry" role="listitem">
Mikolov, T., Kopecky, J., Burget, L., C̆ernocky, J., &amp; Khudanpur, S. (2010). Recurrent neural network based language model. <em>Proceedings of Interspeech</em>. <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf</a>
</div>
<div id="ref-Nakano+2022" class="csl-entry" role="listitem">
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., &amp; Schulman, J. (2022). <em>WebGPT: Browser-assisted question-answering with human feedback</em>. <a href="https://arxiv.org/abs/2112.09332">https://arxiv.org/abs/2112.09332</a>
</div>
<div id="ref-Nichol2022" class="csl-entry" role="listitem">
Nichol, A. (2022). <em>DALL-E2 pre-training mitigations</em>. OpenAI. <a href="https://openai.com/research/dall-e-2-pre-training-mitigations">https://openai.com/research/dall-e-2-pre-training-mitigations</a>
</div>
<div id="ref-OpenAI2023DallE3" class="csl-entry" role="listitem">
OpenAI. (2023a). <em>DALL-E3 system card</em>. OpenAI. <a href="https://openai.com/research/dall-e-3-system-card">https://openai.com/research/dall-e-3-system-card</a>
</div>
<div id="ref-OpenAI2023" class="csl-entry" role="listitem">
OpenAI. (2023b). <em>GPT-4 technical report</em>. <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a>
</div>
<div id="ref-OpenAI2023-GPT4V" class="csl-entry" role="listitem">
OpenAI. (2023c). <em>GPT-4V(ision) system card</em>. OpenAI. <a href="https://openai.com/research/gpt-4v-system-card">https://openai.com/research/gpt-4v-system-card</a>
</div>
<div id="ref-Ouyang+2022" class="csl-entry" role="listitem">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, &amp; A. Oh (Eds.), <em>Advances in neural information processing systems</em> (Vol. 35, pp. 27730–27744). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf</a>
</div>
<div id="ref-Patwardhan+2024" class="csl-entry" role="listitem">
Patwardhan, T., Liu, K., Markov, T., Chowdhury, N., Leet, D., Cone, N., Maltbie, C., Huizinga, J., Wainwright, C., Jackson, S. (Froggi), Adler, S., Casagrande, R., &amp; Mandry, A. (2024). <em>Building an early warning system for LLM-aided biological threat creation</em>. OpenAI. <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</a>
</div>
<div id="ref-Peebles-Xie2023" class="csl-entry" role="listitem">
Peebles, W., &amp; Xie, S. (2023). Scalable diffusion models with transformers. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 4195–4205. <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html</a>
</div>
<div id="ref-Perot+2023" class="csl-entry" role="listitem">
Perot, V., Kang, K., Luisier, F., Su, G., Sun, X., Boppana, R. S., Wang, Z., Mu, J., Zhang, H., &amp; Hua, N. (2023). <em>LMDX: Language model-based document information extraction and localization</em>. <a href="https://arxiv.org/abs/2309.10952">https://arxiv.org/abs/2309.10952</a>
</div>
<div id="ref-Petroni+2019" class="csl-entry" role="listitem">
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., &amp; Miller, A. (2019). Language models as knowledge bases? In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em> (pp. 2463–2473). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1250">https://doi.org/10.18653/v1/D19-1250</a>
</div>
<div id="ref-Racaniere+2017" class="csl-entry" role="listitem">
Racanière, S., Weber, T., Reichert, D. P., Buesing, L., Guez, A., Rezende, D., Badia, A. P., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia, P., Hassabis, D., Silver, D., &amp; Wierstra, D. (2017). Imagination-augmented agents for deep reinforcement learning. <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 5694–5705. <a href="https://dl.acm.org/doi/10.5555/3295222.3295320">https://dl.acm.org/doi/10.5555/3295222.3295320</a>
</div>
<div id="ref-Radford+2021" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8748–8763). PMLR. <a href="https://proceedings.mlr.press/v139/radford21a.html">https://proceedings.mlr.press/v139/radford21a.html</a>
</div>
<div id="ref-Radford+2023" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2023a). Robust speech recognition via large-scale weak supervision. <em>Proceedings of the 40th International Conference on Machine Learning</em>. <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">https://dl.acm.org/doi/10.5555/3618408.3619590</a>
</div>
<div id="ref-Radford+2023-Whisper" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2023b). Robust speech recognition via large-scale weak supervision. <em>Proceedings of the 40th International Conference on Machine Learning</em>. <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">https://dl.acm.org/doi/10.5555/3618408.3619590</a>
</div>
<div id="ref-Radford+2018" class="csl-entry" role="listitem">
Radford, A., narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <em>Improving language understanding with unsupervised learning</em>. OpenAI. <a href="https://openai.com/research/language-unsupervised">https://openai.com/research/language-unsupervised</a>
</div>
<div id="ref-Radford+2019" class="csl-entry" role="listitem">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <em>Language models are unsupervised multitask learners</em>. <a href="https://github.com/openai/gpt-2?tab=readme-ov-file">https://github.com/openai/gpt-2?tab=readme-ov-file</a>
</div>
<div id="ref-Rae+2021" class="csl-entry" role="listitem">
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks, L. A., Rauh, M., Huang, P.-S., … Irving, G. (2021). <em>Scaling language models: Methods, analysis &amp; insights from training gopher</em>. Google DeepMind. <a href="https://arxiv.org/abs/2112.11446">https://arxiv.org/abs/2112.11446</a>
</div>
<div id="ref-Raffel+2020" class="csl-entry" role="listitem">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. <em>J. Mach. Learn. Res.</em>, <em>21</em>(1).
</div>
<div id="ref-Rakhimov+2020" class="csl-entry" role="listitem">
Rakhimov, R., Volkhonskiy, D., Artemov, A., Zorin, D., &amp; Burnaev, E. (2020). <em>Latent video transformer</em>. <a href="https://arxiv.org/abs/2006.10704">https://arxiv.org/abs/2006.10704</a>
</div>
<div id="ref-Ramesh+2022" class="csl-entry" role="listitem">
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). <em>Hierarchical text-conditional image generation with CLIP latents</em>. <a href="https://arxiv.org/abs/2204.06125">https://arxiv.org/abs/2204.06125</a>
</div>
<div id="ref-Ramesh+2021" class="csl-entry" role="listitem">
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. (2021). Zero-shot text-to-image generation. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8821–8831). PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>
</div>
<div id="ref-Reed+2016" class="csl-entry" role="listitem">
Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., &amp; Lee, H. (2016). Generative adversarial text to image synthesis. In M. F. Balcan &amp; K. Q. Weinberger (Eds.), <em>Proceedings of the 33rd international conference on machine learning</em> (Vol. 48, pp. 1060–1069). PMLR. <a href="https://proceedings.mlr.press/v48/reed16.html">https://proceedings.mlr.press/v48/reed16.html</a>
</div>
<div id="ref-Roberts+2020" class="csl-entry" role="listitem">
Roberts, A., Raffel, C., &amp; Shazeer, N. (2020). How much knowledge can you pack into the parameters of a language model? In B. Webber, T. Cohn, Y. He, &amp; Y. Liu (Eds.), <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 5418–5426). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.437">https://doi.org/10.18653/v1/2020.emnlp-main.437</a>
</div>
<div id="ref-Rombach+2022" class="csl-entry" role="listitem">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-resolution image systhesis with latent diffusion models. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10684–10695. <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html</a>
</div>
<div id="ref-Ronneberger+2015" class="csl-entry" role="listitem">
Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In N. Navab, J. Hornegger, W. M. Wells, &amp; A. F. Frangi (Eds.), <em>Medical image computing and computer-assisted intervention – MICCAI 2015</em> (pp. 234–241). Springer International Publishing. <a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28">https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28</a>
</div>
<div id="ref-Saunders+2022" class="csl-entry" role="listitem">
Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., &amp; Leike, J. (2022). <em>Self-critiquing models for assisting human evaluators</em>. <a href="https://openai.com/research/critiques">https://openai.com/research/critiques</a>
</div>
<div id="ref-Schulman+2015" class="csl-entry" role="listitem">
Schulman, J., Levine, S., Moritz, P., Jordan, M. I., &amp; Abbeel, P. (2015). <em>Trust region policy optimization</em>. <a href="https://arxiv.org/abs/1502.05477">https://arxiv.org/abs/1502.05477</a>
</div>
<div id="ref-Schulman+2017" class="csl-entry" role="listitem">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). <em>Proximal policy optimization algorithms</em>. <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>
</div>
<div id="ref-Sennrich+2016" class="csl-entry" role="listitem">
Sennrich, R., Haddow, B., &amp; Birch, A. (2016). Neural machine translation of rare words with subword units. <em>Proceedings of the 54th Annual Meetings of the Association for Computational Linguistics</em>, <em>1</em>, 1715–1725. <a href="https://aclanthology.org/P16-1162/">https://aclanthology.org/P16-1162/</a>
</div>
<div id="ref-Shavit+2023" class="csl-entry" role="listitem">
Shavit, Y., Agarwal, S., &amp; Brundage, M. (2023). <em>Practices for governing agentic AI systems</em>. OpenAI. <a href="https://openai.com/research/practices-for-governing-agentic-ai-systems">https://openai.com/research/practices-for-governing-agentic-ai-systems</a>
</div>
<div id="ref-Shazeer+2017" class="csl-entry" role="listitem">
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., &amp; Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. <em>International Conference on Learning Representation</em>. <a href="https://openreview.net/forum?id=B1ckMDqlg">https://openreview.net/forum?id=B1ckMDqlg</a>
</div>
<div id="ref-Shoker+2023" class="csl-entry" role="listitem">
Shoker, S., Reddie, A., Barrington, S., Booth, R., Brundage, M., Chahal, H., Depp, M., Drexel, B., Gupta, R., Favaro, M., Hecla, J., Hickey, A., Konaev, M., Kumar, K., Lambert, N., Lohn, A., O’Keefe, C., Rajani, N., Sellitto, M., … Young, J. (2023). <em>Confidence-building measures for artificial intelligence: Workshop proceedings</em>. <a href="https://openai.com/research/confidence-building-measures-for-artificial-intelligence">https://openai.com/research/confidence-building-measures-for-artificial-intelligence</a>
</div>
<div id="ref-Sutton2019" class="csl-entry" role="listitem">
Sutton, R. (2019). <em>The bitter lesson</em>. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>
</div>
<div id="ref-Sutton-Barto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (2nd ed.). MIT Press. <a href="https://mitpress.mit.edu/9780262352703/reinforcement-learning/">https://mitpress.mit.edu/9780262352703/reinforcement-learning/</a>
</div>
<div id="ref-Tamkin+2021" class="csl-entry" role="listitem">
Tamkin, A., Brundage, M., Clark, J., &amp; Ganguli, D. (2021). <em>Understanding the capabilities, limitations, and societal impact of large language models</em>. <a href="https://arxiv.org/abs/2102.02503">https://arxiv.org/abs/2102.02503</a>
</div>
<div id="ref-Geminiteam+2023" class="csl-entry" role="listitem">
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., … Vinyals, O. (2023). <em>Gemini: A family of highly capable multimodal models</em>. <a href="https://arxiv.org/abs/2312.11805">https://arxiv.org/abs/2312.11805</a>
</div>
<div id="ref-Thoppilan+2022" class="csl-entry" role="listitem">
Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., … Le, Q. (2022). <em>LaMDA: Language models for dialog applications</em>. <a href="https://arxiv.org/abs/2201.08239">https://arxiv.org/abs/2201.08239</a>
</div>
<div id="ref-Touvron+2023" class="csl-entry" role="listitem">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). <em>LLaMA: Open and efficient foundation language models</em>. <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a>
</div>
<div id="ref-vandenOord+2016" class="csl-entry" role="listitem">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., &amp; Kavukcuoglu, K. (2016). Pixel recurrent neural networks. <em>Proceedings of the 33rd International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v48/oord16.html">https://proceedings.mlr.press/v48/oord16.html</a>
</div>
<div id="ref-vandenOord+2016b" class="csl-entry" role="listitem">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &amp; Kavukcuoglu, K. (2016). Conditional image generation with PixelCNN decoders. <em>Proceedings of the 30th International Conference on Neural Information Processing Systems</em>, 4797–4805. <a href="https://dl.acm.org/doi/10.5555/3157382.3157633">https://dl.acm.org/doi/10.5555/3157382.3157633</a>
</div>
<div id="ref-Vaswani+2017" class="csl-entry" role="listitem">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>
</div>
<div id="ref-Wang+2023" class="csl-entry" role="listitem">
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., &amp; Wei, F. (2023). <em>Neural codec language models are zero-shot text to speech synthesizers</em>. <a href="https://arxiv.org/abs/2301.02111">https://arxiv.org/abs/2301.02111</a>
</div>
<div id="ref-Weng-Brockman2022" class="csl-entry" role="listitem">
Weng, L., &amp; Brockman, G. (2022). <em>Techniques for training large neural networks</em>. OpenAI. <a href="https://openai.com/research/techniques-for-training-large-neural-networks">https://openai.com/research/techniques-for-training-large-neural-networks</a>
</div>
<div id="ref-Yan+2021" class="csl-entry" role="listitem">
Yan, W., Zhang, Y., Abbeel, P., &amp; Srinivas, A. (2021). <em>VideoGPT: Video generation using VQ-VAE and transformers</em>. <a href="https://arxiv.org/abs/2104.10157">https://arxiv.org/abs/2104.10157</a>
</div>
<div id="ref-Yang+2023" class="csl-entry" role="listitem">
Yang, R., Srivastava, P., &amp; Mandt, S. (2023). Diffusion probabilistic modeling for video generation. <em>Entropy</em>, <em>25</em>(10). <a href="https://www.mdpi.com/1099-4300/25/10/1469">https://www.mdpi.com/1099-4300/25/10/1469</a>
</div>
<div id="ref-Yasunaga+2023" class="csl-entry" role="listitem">
Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &amp; Yih, W.-T. (2023). Retrieval-augmented multimodal language modeling. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, &amp; J. Scarlett (Eds.), <em>Proceedings of the 40th international conference on machine learning</em> (Vol. 202, pp. 39755–39769). PMLR. <a href="https://proceedings.mlr.press/v202/yasunaga23a.html">https://proceedings.mlr.press/v202/yasunaga23a.html</a>
</div>
<div id="ref-Yu+2022" class="csl-entry" role="listitem">
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., &amp; Wu, Y. (2022). Scaling autoregressive models for content-rich text-to-image generation. <em>Transactions on Machine Learning Research</em>. <a href="https://openreview.net/forum?id=AFDcYJKhND">https://openreview.net/forum?id=AFDcYJKhND</a>
</div>
<div id="ref-Yu+2023" class="csl-entry" role="listitem">
Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., Ross, C., Polyak, A., Howes, R., Sharma, V., Xu, P., Tamoyan, H., Ashual, O., Singer, U., Li, S.-W., … Aghajanyan, A. (2023). <em>Scaling autoregressive multi-modal models: Pretraining and instruction tuning</em>. <a href="https://arxiv.org/abs/2309.02591">https://arxiv.org/abs/2309.02591</a>
</div>
<div id="ref-Zhao+2023" class="csl-entry" role="listitem">
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). <em>A survey of large language models</em>. <a href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a>
</div>
<div id="ref-Zheng+2023" class="csl-entry" role="listitem">
Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., Xiong, L., Chen, L., Xi, Z., Xu, N., Lai, W., Zhu, M., Chang, C., Yin, Z., Weng, R., … Huang, X. (2023). <em>Secrets of RLHF in large language models part i: PPO</em>. <a href="https://arxiv.org/abs/2307.04964">https://arxiv.org/abs/2307.04964</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>層の数などの表現は <span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span> から．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span> で聞きました．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span> で聞きました．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>もちろん結合することも考えられているが，加算を行うことが現状の多数派であるようである <span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span>．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>勾配の爆発に対しては gradient clipping などの対症療法が用いられる．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>後述のアラインメントも事後調整の一つであるが，これと区別して，<strong>教師ありの事後調整</strong> (supervised fine-tuning) とも呼ばれる．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>パラメトリックな知識ベースとしての利用については <span class="citation" data-cites="Raffel+2020">(<a href="#ref-Raffel+2020" role="doc-biblioref">Raffel et al., 2020</a>)</span>，<span class="citation" data-cites="Roberts+2020">(<a href="#ref-Roberts+2020" role="doc-biblioref">Roberts et al., 2020</a>)</span> など．一方で <span class="citation" data-cites="Marcus2020">(<a href="#ref-Marcus2020" role="doc-biblioref">Marcus, 2020</a>)</span> などは，hallucination などの欠点を補う形で，古典的な知識ベースと連結したハイブリット型での使用を提案している．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>モデルの比較は <span class="citation" data-cites="Raffel+2020">(<a href="#ref-Raffel+2020" role="doc-biblioref">Raffel et al., 2020</a>)</span> などが行っている．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>トークン化に小規模な CNN を用いてデータ圧縮を行うこともある．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>すなわち，DALL-E は GPT-3 のマルチモーダルな実装である <span class="citation" data-cites="Tamkin+2021">(<a href="#ref-Tamkin+2021" role="doc-biblioref">Tamkin et al., 2021, p. 4</a>)</span>．<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><span class="citation" data-cites="Ha-Schmidthuber2018">(<a href="#ref-Ha-Schmidthuber2018" role="doc-biblioref">Ha &amp; Schmidhuber, 2018</a>)</span> は RNN により世界モデルを構築している．<span class="citation" data-cites="Kaiser+2020">(<a href="#ref-Kaiser+2020" role="doc-biblioref">Kaiser et al., 2020</a>)</span> は動画から Atari を学習している．<span class="citation" data-cites="Hafner+2021">(<a href="#ref-Hafner+2021" role="doc-biblioref">Hafner et al., 2021</a>)</span> はさらに性能が良い．<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><a href="https://www.nikkei.com/article/DGXZQOUC268J80W3A221C2000000/">日経新聞 (2/19/2024)</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="quarto-dev/quarto-web" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>