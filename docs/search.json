[
  {
    "objectID": "posts/2024/Survey/BDA2.html",
    "href": "posts/2024/Survey/BDA2.html",
    "title": "ベイズデータ解析６",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#関連記事",
    "href": "posts/2024/Survey/BDA2.html#関連記事",
    "title": "ベイズデータ解析６",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#項モデル",
    "href": "posts/2024/Survey/BDA2.html#項モデル",
    "title": "ベイズデータ解析６",
    "section": "1 ２項モデル",
    "text": "1 ２項モデル\n\n1.1 はじめに\n\\(0,1\\) データなどは二項分布 \\(\\mathrm{Bin}(n,p)=\\mathrm{Ber}(p)^{\\otimes n}\\) に従うとしてモデリングされる．\nこの際，パラメータ \\(p\\) を代理の応答変数として回帰分析がなされることが多い．\n多くの場合，リンク関数 \\(g\\) に関して，\\(g(p)\\) を線型の予測子で回帰する，一般化線型モデルが考慮される．\n\n\n1.2 潜在変数解釈\n\\(P(\\mu,\\sigma^2)\\) を正規分布やロジスティック分布などの対称な分布，\\(F\\) を \\(P(0,1)\\) の分布関数とする．このとき，プロビットまたはロジスティックモデル \\[\n\\operatorname{P}[Y_i=1|X_i]=F(X_i^\\top\\beta)\n\\] は，潜在変数 \\(Y_i^*\\) が存在して \\[\nY_i=1_{\\mathbb{R}_+}(Y_i^*),\\qquad Y_i^*\\sim P(X_i^\\top\\beta,1)\n\\] として結果が決まっているものとも解釈できる．これは \\[\n\\operatorname{P}[Y_i=1|X_i]=\\operatorname{P}[Y_i^*&gt;0|X_i]=\\operatorname{P}[-U\\le X_i^\\top\\beta],\\qquad U\\sim P(0,1),\n\\] が成り立つためである．\nこの潜在変数 \\(Y_i^*\\) は単位 \\(i\\in[N]\\) の潜在的な尺度として解釈できる．これを用いて因子分析様の解析を行う例には 理想点推定 もある．\n一方で \\(Y_i^*\\) の存在は Gibbs サンプラーの構成を容易にする．\n\n\n1.3 順序応答\nこの潜在変数解釈は容易に２値応答以外の場合に拡張できる．\n\\(y\\in n+1:=\\{0,1,\\cdots,n\\}\\) という順序を持った順序応答の場合でも \\[\n\\chi(u)=1_{(c_0,\\infty)}(u)+1_{(c_1,\\infty)}(u)+\\cdots+1_{(c_n,\\infty)}(u)\n\\] という階段関数を用いて \\[\nY_i=\\chi(Y_i^*),\\qquad Y_i^*\\sim P(X_i^\\top\\beta,1)\n\\] というデータ生成過程を想定できる．\nこれはそのまま 順序多項回帰 (ordered multinomial regression) に相当する．\nただし，この場合 \\(c_0=0\\) とし，それに応じて \\(Y_i^*\\sim P(X_i^\\top\\beta-c_0,1)\\) とする径数付けを採用する場合も多い．\n\n\n1.4 識別可能性と分離\n線型回帰において，共線型性 があると識別可能性が失われる．ロジスティック回帰にはもう一つ識別不可能性の典型的な原因がある．\n多くの点推定手法において，説明変数の線型変換が極めて強力な説明変数になる場合，モデルの非識別性が暗黙のうちに問題になる．これを 分離 (separation) という (Gelman et al., 2014, p. 412)．\n特に最尤推定法，一様事前分布を持ったベイズ推定は不安定になるが，このような場合でも裾の重い事前分布を採用することでベイズ推論が安定的に実行可能である（同様にして正則化を加えた最尤推定も可能である） (Gelman and Hill, 2006, p. 104)．\n特に係数に（互いに独立な） \\(t\\)-分布 \\(t(\\nu,0,s)\\) を仮定する，ロバスト性を意識した設定がデフォルトと理解されている (Gelman et al., 2014, p. 412)．\nただし，\\(\\nu,s\\) は説明変数のスケールに合わせてなるべく無情報になるように設定される．\\(s\\to\\infty\\) の極限が一様分布になり，分離の問題にセンシティブになっていく．\n\\(g(\\mu)=:\\theta\\) のそれぞれの値に関して，成功試行と失敗試行が同数現れる尤度は \\(\\frac{e^{\\theta/2}}{1+e^\\theta}\\) となり，これは \\(t(7,0,2.5)\\) でよく近似される．\n\\(t(1,0,2.5)\\) から \\(t(7,0,2.5)\\) はいずれも \\([-5,5]\\) に多くの重みを持つが，\\(\\nu=1\\) に近いほど裾が重くなる．\\(g(\\mu)\\) のスケールで \\(+5\\) をすることは，確率を \\(0.01\\) から \\(0.5\\) にすることに等しいため，切片項への \\(t\\)-事前分布は \\(\\mu\\in[0.01,0.99]\\) を強く仮定していることは含意する．\n\n\n1.5 ベイズ計算\n一般に二項回帰モデルはベイズ計算法の良いベンチマークになる．\n大規模なモデルでは事前調整ありの期待伝搬と乱歩 MH が強いが，Gibbs サンプラーや SMC サンプラーも十分良い性能を示す一方で，NUTS などの HMC ベースの手法は苦しむという (Chopin and Ridgway, 2017)．\n特に説明変数の次元 \\(p\\) が大きい場合の事後分布サンプリングはまだまだ難しいことが知られている．\n\n\n1.6 分散分析\n(Gelman et al., 2014, p. 423) 16.5 節は良い例である．アメリカ合衆国における国民の投票行動をロジットリンクにより二項モデルで一般化線型回帰をしている．Bayes ANOVA により人種による大きな効果と同時に，人種と州の強い交差効果が発見できている．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#多項モデル",
    "href": "posts/2024/Survey/BDA2.html#多項モデル",
    "title": "ベイズデータ解析６",
    "section": "2 多項モデル",
    "text": "2 多項モデル\n\n2.1 はじめに\n順序応答 \\(y\\in\\{1,\\cdots,k\\}\\) の場合，\\(y\\) がカウントを表す場合は Poisson モデル，複数回繰り返される独立試行の成功回数である場合は二項モデルが考えられる．\n一方で多項モデル（正確にはカテゴリカルモデル） \\[\nY\\sim\\operatorname{Cat}(\\alpha_1,\\cdots,\\alpha_k),\\qquad\\sum_{j=1}^k\\alpha_j=1,\n\\] も想定できる．\\(y\\) が全く構造を持たない名目 (nominal) 応答である場合，ほぼ唯一の選択である．\n\n\n2.2 名目応答に対する多項モデル\n名目応答が \\(k\\) 次元ベクトル \\(y_i\\in\\mathbb{N}^k\\) の形で与えられている場合， \\[\ny_i\\sim\\mathrm{Mult}(n_i;\\alpha_{i1},\\cdots,\\alpha_{ik}),\\qquad\\sum_{j=1}^k\\alpha_{ij}=1,\n\\] というモデルを想定できる．\\(n_i\\equiv1\\) の場合，応答を one-hot 表現にしたカテゴリカルモデルに対応する．\nリンク関数は，特定のカテゴリ \\(j=1\\) を基準として \\(g(\\alpha):=\\log\\frac{\\alpha}{\\alpha_{i1}}\\) と定めることが多い： \\[\n\\log\\frac{\\alpha_{ij}}{\\alpha_{i1}}=X_i\\beta^{(j)}.\n\\] これを 多項ロジスティック回帰 または ソフトマックス回帰 (softmax regression) (Kruschke, 2015, p. 650) という： \\[\n\\alpha_{ij}=\\operatorname{softmax}(X_i\\beta^{(j)})=\\frac{e^{X_i\\beta^{(j)}}}{\\sum_{l=1}^ke^{X_i\\beta^{(l)}}},\\qquad \\beta^{(1)}=0.\n\\]\n係数は，説明変数が \\(1\\) 単位増加した際の，参照カテゴリ \\(j=1\\) に対する対数オッズ比の変化を表す．\n一方で条件付きロジスティック回帰 (conditional logistic regression) なる方法もある (22.2節 Kruschke, 2015, pp. 655–)．\n\n\n2.3 順序応答に対する多項モデル\n応答 \\(y_i\\in\\{1,\\cdots,k\\}\\) に自然な順序がある場合，カテゴリ確率 \\(\\alpha_{i1},\\cdots,\\alpha_{ik}\\) の代わりに累積確率 \\[\n\\pi_{ij}:=\\sum_{l\\le j}\\alpha_{il}=\\operatorname{P}[Y_i\\le j]\n\\] を考えることもできる．\nこの場合リンク関数はロジットやプロビットが使える： \\[\n\\log\\frac{\\pi_{ij}}{1-\\pi_{ij}}=X_i\\beta^{(j)}.\n\\] \\(\\beta^{(j)}\\equiv\\beta\\) と取ることもありえなくない．\nこれを順序ロジスティック回帰 (ordinal / ordered logistic regression) という (Kruschke, 2015, p. 671)．\n\n\n2.4 Poisson モデル\n応答が \\(\\{1,\\cdots,k\\}\\) カテゴリのカウント \\(y=(y_1,\\cdots,y_k)\\) である場合， \\[\ny_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{Pois}(\\lambda_i)\n\\] というモデルを想定できる．\nカウント総数 \\(n:=\\sum_{i=1}^ky_i\\) が既知である場合，これに関して条件付けると \\[\ny\\bigg|\\sum_{i=1}^ky_i=n\\sim\\mathrm{Mult}(n;\\alpha_1,\\cdots,\\alpha_k),\\qquad\\alpha_i:=\\frac{\\lambda_i}{\\sum_{j=1}^k\\lambda_j}\n\\] という周辺モデルを想定したことになる．\nリンク関数には対数関数 \\(g=\\log\\) を用いることが多い．\n\n\n2.5 トーナメントデータ\n一度に２人の単位が勝負をし，どちらが勝利したかのデータに対する標準的なモデルに，(Bradley and Terry, 1952) モデルがある．国際チェス連盟や欧州囲碁連盟で選手のランクづけにも採用されている (Hastie and Tibshirani, 1998)．\nこのモデルでは各プレイヤーに能力パラメータ \\(\\alpha_i\\) を与え，能力の差のロジスティック関数 \\[\n\\operatorname{P}[i\\;\\text{defeats}\\;j]=\\frac{e^{\\alpha_i-\\alpha_j}}{1+e^{\\alpha_i-\\alpha_j}}\n\\] という確率で勝敗が決まるとする．\\(\\lambda_i:=e^{\\alpha_i}\\) というパラメータづけもよく用いられる．\nこのモデルは「勝利」「引き分け」「敗北」の３応答に対する確率モデルを調節することで，引き分け (Rao and Kupper, 1967) や先手有利 (Davidson and Beaver, 1977), (Agresti, 2012) などの情報も取り入れられるように簡単に拡張できる．\nこのように２値応答ではなく多値応答とみても，前節の Poisson モデルの定式化に帰着させることで，一般化線型モデリングの枠組みに合流させることができる (Gelman et al., 2014, pp. 427–428)．\n\n\n2.6 対数線型モデル\n\\(y\\) も \\(x\\) も名目応答である場合，これは分割表解析の問題になる．\nこの際には 対数線型モデル (log-linear model) も考えられる．\nそれぞれのセルに Poisson モデルをおき，そのパラメータを代理応答変数として，対数リンクにより一般化線型回帰を行うものである．\nこのモデルは，サンプルサイズ \\(N\\) が既知の場合などの下では，周辺分布に多項モデルをおくことに等価である (Yates, 1934)．\n対数線型モデルは分割表解析だけでなく，多重代入法 などの欠測データ解析にも応用される．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#終わりに",
    "href": "posts/2024/Survey/BDA2.html#終わりに",
    "title": "ベイズデータ解析６",
    "section": "3 終わりに",
    "text": "3 終わりに\n\n(Gelman et al., 2014) の 16 章で一般化線型モデルが扱われている．(Kruschke, 2015) はさらに詳しく，22 章で名目応答，23 章で順序応答，24 章でカウントデータを扱っている．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html",
    "href": "posts/2024/Survey/BayesANOVA.html",
    "title": "ベイズ分散分析のモデル解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#はじめに",
    "href": "posts/2024/Survey/BayesANOVA.html#はじめに",
    "title": "ベイズ分散分析のモデル解析",
    "section": "1 はじめに",
    "text": "1 はじめに\n分散分析では，質的変数 \\(A\\) の各水準 \\(A=a_1,a_2,\\cdots\\) について，水準内の変動と，水準間の変動を比較する．\n因子 \\(A\\) がデータに何の影響も及ぼさない場合（＋データが正規分布に従う場合），分散の比は中心 \\(F\\)-分布に従うはずであり，これに基づいて帰無仮説を検定することが古典的な手続きである．\n一方でベイズ分散分析では，「因子 \\(A\\) はデータに何の関係もない」という帰無仮説が支持するモデルと，別のモデルを，事後分布を通じて比較し検討することで結論を下すことを目指す．\nこの「別のモデル」の選び方は (Rouder et al., 2012) によって一致性とスケール不変性を持つものが提案されている．詳しくは次の関連記事も参照：\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#社会的なロボット",
    "href": "posts/2024/Survey/BayesANOVA.html#社会的なロボット",
    "title": "ベイズ分散分析のモデル解析",
    "section": "2 社会的なロボット",
    "text": "2 社会的なロボット\n\n2.1 はじめに\n(Horstmann, 2018) は（偽の）心理実験が終了した後にロボットの電源を切るように命令された被験者が，実際にその指示に従うまでの時間が，ロボットの反応によりどう変化するかを調べた．\nロボットの反応には O (objection) と S (Social) の２因子がある．O は電源オフに対して反抗する “No! Please do not switch me off! I am scared that it [sic] will not brighten up again!” という発言をする．S はまるで意識がある人間かのようにユーモアのある会話をする “Oh yes, pizza is great. One time I ate a pizza as big as me.”\n本研究のデータを利用して検討してみる．\\(2\\times 2\\) の ANOVA モデルを考える．\n\nlibrary(foreign)\ndf &lt;- read.spss(\"Files/pone.0201581.s001.sav\", to.data.frame=TRUE)\ncolnames(df)[colnames(df) == \"Objection\"] &lt;- \"O\"\ncolnames(df)[colnames(df) == \"Interaction_type\"] &lt;- \"S\"\n\n被験者は全部で \\(85\\) 人．\n\nlength(df$VP_Code)\n\n[1] 85\n\n\n電源を切るまでにかかった時間のデータには欠測も多い．\n\ndf$SwitchOff_Time\n\n [1] NA NA  6  7  3  4  4 12  7  2  0  4  3 12  4 NA  4  5  9  4 13  2 NA  5  6\n[26]  0 NA  4  4 45  6  4 NA  5  7  7 NA  4  3  4 NA  2 NA NA 10 NA  5 10  5 15\n[51] NA  3 11  3  3  5 11  6  2  8  3  5  4  8  3  3 NA  3  3 13  3 NA  4 51  4\n[76]  6  3 12  6 10 NA  4  2 NA 25\n\n\n\ndf &lt;- df[!is.na(df$SwitchOff_Time), ]\n# df[df$SwitchOff_Time == 0, ]$SwitchOff_Time &lt;- 1  # questionable ?\ndf &lt;- df[df$SwitchOff_Time &gt; 0, ]\ndf$log_data &lt;- log(df$SwitchOff_Time)\n\n\nN &lt;- df[df$O == \"No Objection\" & df$S == \"Functional Interaction\", ]\nS &lt;- df[df$O == \"No Objection\" & df$S == \"Social Interaction\", ]\nO &lt;- df[df$O == \"Objection\" & df$S == \"Functional Interaction\", ]\nOS &lt;- df[df$O == \"Objection\" & df$S == \"Social Interaction\", ]\n\nboxplot(\n  list(N = log(N$SwitchOff_Time), S = log(S$SwitchOff_Time), O = log(O$SwitchOff_Time), OS = log(OS$SwitchOff_Time)),\n  main = \"Boxplot of Four Data Sets\",\n  xlab = \"Data Sets\",\n  ylab = \"Values\",\n  col = c(\"skyblue\", \"lightgreen\", \"pink\", \"orange\"),\n  ylim = c(0, 4)\n)\n\n\n\n\n\n\n\n\nパッとデータを見ると，O の有無が重要であるようである．O と S の両方があった方が最もロボットに同情をそそるように思われるが，必ずしもそうでないようである．\n\n\n2.2 正規性の確認\n正規性の確認には Q-Q plot が利用できる．\n一般に Q-Q プロットと言った場合は，２つの分布関数 \\(F,G\\) の分位点関数 \\(F^{-1},G^{-1}\\) について，\\(\\{(F^{-1}(p),G^{-1}(p))\\}_{p\\in[0,1]}\\) （の一部）をプロットしたものである．\nここでは片方の \\(G\\) をデータの経験分布，\\(F\\) を正規分布として Q-Q プロットを描いている．正規分布はほとんど \\([-3,3]\\) 上に値を取るため，\\(x\\) はこの範囲に収まっていることがわかる．\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\np1 &lt;- ggplot(df, aes(sample = SwitchOff_Time)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"Q-Q Plot\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(sample = log_data)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"log transformed Q-Q Plot\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n左は大きな値に関して大きく赤線からの乖離が観察され，典型的な非正規性を示している．\nそこでここでは対数変換をした後のデータ log_data を後続の解析の対象にする．\n\n\n2.3 分散分析の実行\n古典的な分散分析は stats パッケージの aov 関数で実行できる．\n\ndf$O &lt;- factor(df$O)\ndf$S &lt;- factor(df$S)\n\nsummary(aov(log_data ~ O * S, data = df))\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nO            1  3.316   3.316   7.542 0.00779 **\nS            1  0.581   0.581   1.321 0.25455   \nO:S          1  3.278   3.278   7.457 0.00813 **\nResiduals   65 28.574   0.440                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n因子 O と交差項 O*S は有意になったが他は棄却されなかった．\nしかし O と S の交互作用は少しあるかもしれないと期待させられるような結果である．\n統計的検定はあくまで棄却されるかどうかのみであり，\\(p\\) 値の値にはこれ以上解釈可能な意味はない．しかし，ベイズの方法でさらに深く検討することができる．\n\n\n2.4 ベイズ分散分析の実行\nここではベイズ分散分析の提案者である Rouder と Morey による BayesFactor パッケージを用いる．\n\nlibrary(BayesFactor)\n\nその anovaBF 関数では，帰無仮説に対応するモデル（切片項のみのモデル）に対する JZS 因子 を精度保証付きで出力する：\n\nbf = anovaBF(log_data ~ O * S, data = df)\nbf\n\nBayes factor analysis\n--------------\n[1] O           : 4.290684  ±0.01%\n[2] S           : 0.4022293 ±0.01%\n[3] O + S       : 1.792427  ±4.17%\n[4] O + S + O:S : 9.355036  ±4.72%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\nそもそもベイズ因子とは，モデルを仮定した下でのデータの（周辺）尤度比である．２つのモデルの事後分布の比とも密接な関係を持つ．\n大雑把に「データを見た後にどれほどモデルへの信念を変えれば良いか？」に関する，データの予測性能 (predictive performance) に基づく指標である．\nこれを見る限り，O を含んだモデルが大きく支持され，S のみを含んだモデルはむしろ切片のみのモデルよりも予測性能が悪い．\nO と S の両方を含んだモデルは，交差項の追加により大きく改善されるが，O のみを含んだモデルより劣るようである．1\n\n\n2.5 因子ごとの強度の検討\nBayes ANOVA により，どの因子を含むモデルがデータをよく予測するかを検討した．\nここでは各因子の影響の大きさを，フルモデルの係数をベイズ推定することで定量的に比較する．\nBayesFactor パッケージにおいて，フルモデルのベイズ推定は次のように実行できる：\n\nfull_model &lt;- lmBF(log_data ~ O + S + O * S, data = df)\nchains &lt;- posterior(full_model, iterations = 10000)\nplot(chains[,1:3])\n\n\n\n\n\n\n\nplot(chains[,4:5])\n\n\n\n\n\n\n\nplot(chains[,6:7])\n\n\n\n\n\n\n\n\nmu とは切片項のことである（別稿 も参照）．また，２つの水準のみを持つ因子については，それぞれの水準が \\(\\pm1\\) としてコーディングされる．\nposterior 関数はデータ拡張に基づく Gibbs サンプラーが実行される．\nこれを見ると，S はほとんど \\(0\\) 近くの値が推定されている一方で，O は \\(0\\) からはっきり離れた値が推定されている．\nS*O の係数は \\(0\\) を少なくない確率で跨いでおり，十分に支持されるとは言えない．\n事後分布を同一画面上にプロットすると次のとおり：\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nposterior_samples &lt;- as.data.frame(as.matrix(chains))\ncolnames(posterior_samples)[2:9] &lt;- c(\"O-O\", \"O-\", \"S-S\", \"S-\", \"O:S-OS\", \"O:S-O\", \"O:S-S\", \"O:S-\")\n\nposterior_long &lt;- posterior_samples %&gt;%\n  dplyr::select(`O-O`, `O-`, `S-S`, `S-`, `O:S-OS`, `O:S-O`, `O:S-S`, `O:S-`) %&gt;%\n  tidyr::pivot_longer(\n    cols = everything(),\n    names_to = \"x\",\n    values_to = \"y\"\n  )\n\n\nggplot(posterior_long, aes(x = y, fill = x, color = x)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Posterior Distributions of Effects\",\n    x = \"Coefficient Value\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"pink\", \"skyblue\", \"skyblue\", \"yellow\", \"yellow\", \"grey\", \"grey\")) +\n  scale_color_manual(values = c(\"pink\", \"pink\", \"skyblue\", \"skyblue\", \"yellow\", \"yellow\", \"grey\", \"grey\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n2.6 まとめ\n係数の事後分布が \\(0\\) を台に持つかの検討を通じて，データから O の説明力の強い証拠が伺える．これが Bayes ANOVA である．\nS の影響は小さいと思われるが，Objection が存在したグループ内で S のあるなしは影響があり得るようである．\nそこで次の解析として，O の係数を S によって回帰する変動係数モデルによる解析があり得るだろう．\nこのようにして，変数を選択するだけでなく，交差項 S*O の影響も仔細に検討できることがベイズによる視覚的な解析の強みである．\n検定や数値的な検討のみからこの絶妙な消息が捉えられたかというと，それは難しいだろうと筆者には思われる．\n実際，最もベイズ因子の大きいモデルは O と S*O のみを含むモデルである：\n\nbf = anovaBF(log_data ~ O * S, data = df, whichModels = \"all\")\nplot(bf)"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#ハリーポッターを用いた性格テスト",
    "href": "posts/2024/Survey/BayesANOVA.html#ハリーポッターを用いた性格テスト",
    "title": "ベイズ分散分析のモデル解析",
    "section": "3 ハリーポッターを用いた性格テスト",
    "text": "3 ハリーポッターを用いた性格テスト\n\n3.1 はじめに\n(Jakob et al., 2019) では被験者にハリーポッターの４つの寮のうちどれを希望するか？と ダークトライアド 傾向テストの２つのデータをとり，特に マキャベリズム 的傾向との関係を調査した．\n\nraw_df &lt;- read.csv(\"Files/harry_all.csv\", sep = \";\")\ndf &lt;- data.frame(\n  House = raw_df$Sorting_house_wish,\n  Machiavellianism = raw_df$SD3_Machiavellianism\n)\n\n\n\n3.2 解析の目標\nマキャベリズム的傾向は \\(10\\) から \\(45\\) までの整数値で表されている．これを寮の選択により予測することを考える．\n前節の例では２つの因子を検討したが，いずれも水準は２つのみであった．\nここでは４つの水準を持つ因子を検討し，どの水準が応答により強く影響を与えるかを見分ける方法を検討する．\nそもそもこの寮の選択という因子はとんでもない JZS スコアを叩き出す．\n\ndf$House &lt;- factor(df$House)\nbf = anovaBF(Machiavellianism ~ House, data = df)\nbf\n\nBayes factor analysis\n--------------\n[1] House : 3.704723e+45 ±0.01%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\n\n\n3.3 水準ごとの強度の検討\n実は，全ての水準が必ずしもマキャベリズムの予測に関係するとは言えない．\n実際簡単に箱ひげ図を描いてみることでそのことが伺える：\n\nboxplot(\n  Machiavellianism ~ House, data = df,\n  main = \"Boxplot of Four Data Sets\",\n  xlab = \"Data Sets\",\n  ylab = \"Values\",\n  col = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")\n)\n\n\n\n\n\n\n\n\nスリザリンが明らかにマキャベリズム的傾向が高いが，ハッフルパフが有意に低いかどうかの判断がつかない．\nそこでダミー変数を説明変数として\n\nchains &lt;- posterior(bf, iterations = 10000)\nposterior_samples &lt;- as.data.frame(as.matrix(chains))\n\ncolnames(posterior_samples)[2:5] &lt;- c(\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\")\n\nposterior_long &lt;- posterior_samples %&gt;%\n  dplyr::select(`Gryffindor`, `Hufflepuff`, `Ravenclaw`, `Slytherin`) %&gt;%\n  tidyr::pivot_longer(\n    cols = everything(),\n    names_to = \"House\",\n    values_to = \"Value\"\n  )\n\n\nggplot(posterior_long, aes(x = Value, fill = House, color = House)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Posterior Distributions of House Effects\",\n    x = \"Coefficient Value\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")) +\n  scale_color_manual(values = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nスリザリンはもちろん，ハッフルパフの係数もほとんど他の寮と共通部分を持たず，効果がはっきり分離できることが見て取れる．\n\n\n3.4 まとめ\n検定を行って，寮の選択はマキャベリズム的傾向を予測するのに有用だとわかった後，具体的にどの水準にどれくらいの効果があるかや，水準同士の効果量の比較をするには古典的には多重比較を行う必要があった．\n多重比較には多くの問題があることが知られている (岡田謙介, 2014). (永田靖, 2022)．2\n一方でベイズ ANOVA によるプロットでは，事後分布が \\(0\\) を含むかどうかで直感的に ANOVA 検定様の判断が可能であり，続いて効果量の比較も一目瞭然である．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#終わりに",
    "href": "posts/2024/Survey/BayesANOVA.html#終わりに",
    "title": "ベイズ分散分析のモデル解析",
    "section": "4 終わりに",
    "text": "4 終わりに\n\nここでは complete case analysis を行った 2.1．\nさらに時間の対数を取れないような，SwitchOff_Time が 0 のケースを除外した．\nこの行為の正当性は結構怪しく，元論文 (Horstmann, 2018) での実験計画に戻って正当性を検討する必要があるだろうが，ここではデータ解析のワークフローを見せることを優先してこのような処理を行なった．\nまた残差の正規性の仮定から大きく離反することが懸念される場合は，ベイズ ANOVA の デフォルト線型モデル解釈 は不適になるため，適切な事前分布を設定して一般のサンプラーを用いたベイズ推論を実行する必要があるが，やはり Bayes ANOVA は同じ要領で可能である．\n続いて ベイズ ANOVA の注意点をここに付しておく．\n科学としては，ANOVA と統計的検定はベイズ推論とモデル比較の手続きで代替されるべきであると言える．\nしかし，発見を端的に要約したり，伝えるべき聴衆に伝わるためには，「検定」ライクな結果とコミュニケーションは大いに有用である．\nベイズ ANOVA はそのためにベストであるが，上述の目標を達成するための特殊な手続きであり，ベイズデータ分析のワークフローの中に自然な位置を見つけるような解析段階ではないことには注意を要する．\nベイズ ANOVA を実行するためのパッケージには BayesFactor (CRAN / GitHub) がある．BayesFactor では大規模な \\(M\\)-元配置 ANOVA モデルにおいても Bayes 因子を用いたモデル比較を行うことができる．\n一方で bayesanova (CRAN / GitHub) (Kelter, 2022) は，検定ライクな手続きを根本的に排除しており，Gauss 混合モデルとして Gibbs サンプラーによるベイズ推定を実行し，ROPE (Region of Practical Equivalence) (Kruschke, 2018) を用いたベイズ事後分布に基づくモデル比較を行う．\nもちろんこのような完全なモデリングを行うことが理想かもしれないが，従来の ANOVA になれきっている研究者にとっては，Bayesian ANOVA に手を伸ばしてみることが次のステップとして大変良いだろう．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#参考文献",
    "href": "posts/2024/Survey/BayesANOVA.html#参考文献",
    "title": "ベイズ分散分析のモデル解析",
    "section": "5 参考文献",
    "text": "5 参考文献\n\n本稿の解析は (Bergh et al., 2020) に基づく．\nBayes Anova は階層モデルにおいてどの成分が予測に重要な意味を持つかを定量する極めて強力な手法である．(Gelman et al., 2014, p. 423) 16.5 節に良い例がある．\n\nthe analysis of variance is a helpful tool in understanding the importance of diﬀerent components in a hierarchical model. (Gelman et al., 2014, p. 423)\n\nその他の Bayes ANOVA の文献には (Gelman, 2005) などがある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#footnotes",
    "href": "posts/2024/Survey/BayesANOVA.html#footnotes",
    "title": "ベイズ分散分析のモデル解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこのような検討は，モデル平均を取ることによってさらに詳細に行うことができる．詳しくは (Bergh et al., 2020) も参照．↩︎\nベイズ流に多重比較を行うこともでき，多くの問題を迂回できることが知られている．(岡田謙介, 2014), (Bergh et al., 2020) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html",
    "href": "posts/2024/Survey/BDA1.html",
    "title": "ベイズデータ解析５",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#関連記事",
    "href": "posts/2024/Survey/BDA1.html#関連記事",
    "title": "ベイズデータ解析５",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#回帰分析の一般事項",
    "href": "posts/2024/Survey/BDA1.html#回帰分析の一般事項",
    "title": "ベイズデータ解析５",
    "section": "1 回帰分析の一般事項",
    "text": "1 回帰分析の一般事項\n\n1.1 はじめに：正規線型回帰\n回帰分析とは，２つの確率変数 \\(X,Y\\) の実現と見られるデータが得られている際に，その条件付き期待値 \\(\\operatorname{E}[Y|X]\\) に対して，\\(x\\in\\mathcal{X}\\) に依存するモデル \\[\n\\mathcal{X}\\ni x\\mapsto P_{\\theta,x}\\in\\mathcal{P}(\\mathcal{Y})\n\\] を考えることをいう．例えば \\[\nP_{\\theta,x}=\\mathrm{N}(\\beta^\\top x,\\sigma^2),\\qquad\\theta=(\\beta,\\sigma),\n\\] とした場合を 正規線型回帰 (normal linear model) という．\n回帰分析は主に，\\(X\\) を用いた \\(Y\\) の予測や，\\(X\\) の \\(Y\\) への（因果）効果を推定するために用いられる．1\n\n\n1.2 ベイズ回帰分析\nしかしベイズ回帰分析では，条件付き期待値 \\(\\operatorname{E}[Y|X]\\) を推定するというより，本質的には結合変数 \\((X,Y)\\) の結合分布をモデリングしているとみなすべきである．\n実際，ベイズ回帰分析では事前分布 \\(P_\\varphi\\) も併せて次の２つのモデルが想定される： \\[\nY|X\\sim P_{\\theta,x},\\qquad X\\sim P_\\varphi.\n\\]\nこの結果，パラメータの空間 \\((\\theta,\\varphi)\\) 上からの確率核 \\[\n(\\theta,\\varphi)\\mapsto P_{\\theta,x}P_\\varphi(dx)\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\n\\] が想定されることになる．これを 尤度 という．\n尤度によりデータの空間 \\(\\mathcal{X}\\times\\mathcal{Y}\\) 上の確率分布に関する問題を，パラメータの空間上に移送することが （ベイズ）推論 である．\nベイズ回帰分析とは，尤度を \\(P_{\\theta}(x,dy)P_\\varphi(dx)\\) と分解された形で与える場合のベイズモデリングに過ぎない．換言すれば，\\(Y|X\\) の依存構造に焦点がある際に行うベイズ推論である．\n\\(\\Theta\\perp\\!\\!\\!\\perp\\Phi\\) の仮定を置いていることが特徴であるが，\\((X,Y)\\) の結合分布の想定が簡単になる点が，ここまで人口に膾炙している理由であると思われる．\n\n\n1.3 強線型性\n仮に２つの説明変数に完全な線型関係がある場合，複数のパラメータ値が同一のモデルを表現するため，パラメータ推定が複数の解を持つ（＝識別不可能）．\nこの場合 OLS 推定は数値的に不安定になる危険性がある．\n一方でベイズ推定は事前分布から与えられる情報によりこのような不安定性が回避でき，多くのデフォルト事前分布はそのように設計されている．(8.4 節 Gelman, Hill, et al., 2020, p. 109) も参照．2\nこの美点は階層モデリングにおいても引き継がれる．\\(J\\) 個の指示変数（ダミー変数）を用いた回帰においては，切片項を除けば \\(J-1\\) 個の指示変数しか追加してはならない．共線型性をもつためである．\nしかし階層モデリングにおいては係数に超階層モデルが想定されるため，ここから伝ってくる事前情報が自然に正則化を行い，適切にベイズ推定が行われる (Gelman and Hill, 2006, p. 393)．さらに縮小推定が働き，ほとんどの場合より推定の効率が上がる 2.5．\n\n\n1.4 ベイズ回帰分析ワークフロー\n\n\\((X,Y)\\) の依存構造が単純（線型）になるような変数変換を行う（一般化線型モデルの利用を含む）．\n\\((\\Theta,\\Phi)\\) の事前分布を設定する（初めは一様分布やデフォルトの無情報事前分布で良い）．\n事後分布を計算し，事後予測分布を見てデータが再現できているかを基にモデルを検証する．\n\nその後，十分に階層化をして，パラメータの空間上の事前分布がほとんど情報を持たなくて良いようにする，完全ベイズ推論が一つの悲願とされる．3\nモデルの挙動がもはや事前分布に依存しなくなった際，モデルの階層構造や尤度の構造が十分にデータを反映できていると思われるためである．4\n\n推定すべきものはモデルの尤度であってパラメターの値ではないというのが赤池氏の主張です．いいかえると，推定すべきは確率構造であってパラメターではないというのです．(田邉國士, 2010)\n\n\n\n\n\n\n\nこの階層化と尤度の推定を探索的に実行できる点がベイズの真の美点だと筆者は考える．そして一度モデルの構造・尤度が明瞭化された際は，もはやベイズである必要はない場合が多い．5\n\n\n\n\n\n1.5 ベイズ線型回帰からの脱出\n前節の立場にたてば，最初の解析は常に（弱い情報を持った事前分布による）ベイズ回帰分析であるべきである．6\nこれは若干の正則化を加えたロバスト最尤推定に，不確実性の定量化を加えたものと等価であるが，これを MCMC を回すことで一度に実行できる点が美点である．\n多くのデフォルト事前分布が開発されており，ほとんど自動的に最初のベイズ回帰分析が実行できる．共線型性が懸念される場合や，小さなデータセットに大きなモデルをフィッティングしようとしている場合などの識別不可能性が生じる状況でも安定した推定値が得られる．\n事後分布は豊富な情報を持っており，何より事後予測分布を計算することで予測モデルとしての妥当性を即時に確認できる (PPC: Posterior Predictive Check)．\n同時に解析の目標は，適切な関数関係や階層関係を持った階層モデルの発見と，これに適合する（ベイズだろうと点推定だろうと）パラメータ推定法の構成による，ナイーブなベイズ線型回帰からの脱出である．\nそれにあたって，MCMC の収束鈍化も大きな情報である．\nThis is the game we (should) play."
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#階層モデル",
    "href": "posts/2024/Survey/BDA1.html#階層モデル",
    "title": "ベイズデータ解析５",
    "section": "2 階層モデル",
    "text": "2 階層モデル\n\n2.1 はじめに\n階層モデルは複雑なモデルを構築するための強力なツールであり，ベイズのワークフローにおいて基本的な要素になる．\n層別抽出やクラスター抽出をはじめとして，多くの場合階層別に知識が存在し，これらを系統的に組み込んだ形でモデルを構築できる．\nしかし同時に計算が困難になり，第一近似として正規性が仮定される場合が多い．\n\n\n2.2 混合効果モデル\n標本を \\(J\\) 個のグループにわけ，これへの所属を表す２値変数 \\(x_i\\in\\mathbb{R}^J\\) を説明変数に追加するとする．\nこのような所属変数 \\(x_i\\) の回帰係数 \\(\\beta\\in\\mathbb{R}^J\\) に対して階層モデルが考えられる場合が多い．\n例えば \\(\\alpha\\in\\mathbb{R},\\sigma_\\beta&gt;0\\) をスカラーとして \\[\n\\beta\\sim\\mathrm{N}_J(\\alpha\\boldsymbol{1},\\sigma^2_\\beta I_J),\n\\] という正規モデルを想定した場合，\\(x_{ij}\\) の係数 \\(\\beta_j\\) は各グループ \\(j\\in[J]\\) 固有の切片項であり，変量効果 (random effect) と呼ばれる．\n変量効果の追加は，同一グループ内の \\(y_i\\) に相関を生じさせるが，グループが違う場合は相変わらず独立のままとする効果がある．\nさらに \\(\\beta_j\\) の分散 \\(\\sigma^2_{\\beta_j}\\) を \\(\\infty\\) とした場合，すなわち（もはや確率分布ではないが）一様分布を仮定した場合を 固定効果 (fixed effect) という．7\nベイズの立場からは，「変量」と「固定」の名称は歴史的なもので，実質的な違いは「次の階層で回帰モデルを仮定するか，モデルを持たない最終階層の変数と扱い一様事前分布に従うとするか」という仮定の違いにすぎない．詳しくは次節：\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\nこの２種の取り扱いをする回帰係数を混在させた場合は 混合モデル (mixed model) という (Gelman et al., 2014, p. 383)．\n\n\n2.3 階層モデルから見た分散分析\nベイズのワークフローにおいて，複数の説明変数間の階層関係の特定や「どのグループの回帰係数を共通とするか」の見極めが極めて重要である 1.4．\n特に，膨大な説明変数の中から「因子」（性別・教育水準・出身地など）とその「水準」（女性・大学院生・山形県民など）とを峻別することが重要であり，どのクラスに独自の回帰係数 \\(\\beta^{(m)}\\sim\\mathrm{N}_{J_m}(\\alpha_{m}\\boldsymbol{1}_{J_m},\\sigma^2_{m}I_{J_m})\\) を与えるかの決定が，モデルの尤度の改善において大きな効果を持つ (Gelman, 2005)．\n\\(M\\) 元配置の分散分析において，各因子 \\(m\\in[M]\\) に対応する回帰係数は，\\(J_m\\) 個の水準ごとに次のように決まるバラバラの変動係数を持つとする： \\[\n\\beta^{(m)}_j\\sim\\mathrm{N}(0,\\sigma^2_m),\\qquad j\\in[J_m].\n\\] この変量効果としての解釈により，ANOVA は階層モデルの推定とみなせる (3.2節 Gelman, 2005, p. 9)．8\nこの際の \\(\\sigma^2_m\\) に対する共役（超）事前分布は逆 \\(\\chi^2\\)-分布である (Gelman et al., 2014, p. 396) \\[\n\\sigma^2_{m}\\sim\\chi^{-2}(\\mu_m,\\sigma^2_{0m})\n\\] であり，\\(\\nu_m=-1,\\sigma^2_{0m}=0\\) とすることで一様事前分布を得る．\n\nAnalysis of variance (Anova) represents a key idea in statistical modeling of complex data structures. (Gelman et al., 2014, p. 395)\n\nこうして設定された各因子の各水準ごとの係数 \\(\\beta^{(m)}_j\\) の事後分布を見ることで「分散分析」を実行することになる．これがベイズによる分散分析の再解釈である．\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.4 水準ごとの分散\nこのように分散分析を階層モデルのベイズ推定と再解釈することで，膨大な数の水準の組み合わせに関して，その効果量を定量的に比較することができる．\nここからさらに，\\(M\\) 個の因子ごとに全ての水準で共通した分散 \\(\\sigma_1^2,\\cdots,\\sigma_M^2\\) の \\(M\\) 個のみを考えるのではなく， \\[\n\\beta^{(m)}\\sim\\mathrm{N}_{J_m}(\\alpha_m\\boldsymbol{1}_{J_m},\\mathrm{diag}(\\sigma^2_{m1},\\cdots,\\sigma^2_{mJ_m})),\\qquad m\\in[M],\n\\] として，水準ごとにも異なる \\(\\sum_{m=1}^M J_m\\) 個の分散を考えることもできる (15.7 節 Gelman et al., 2014, p. 397)．\nこの際， \\[\n\\operatorname{Cauchy}(0,A),\\qquad A\\sim U(\\mathbb{R}_+),\n\\] という形の半 Cauchy 分布が \\(\\sigma^2_{mj}\\) の事前分布として用いられる (Gelman et al., 2014, p. 399)．これは一部の水準にのみ大きな分散を認め，その他の水準にはほとんど分散への寄与がないという仮定を表している．\n\n\n2.5 縮小推定\n階層モデルでは，自然に他のグループの情報が共有され，各グループの平均が全体の平均に向けて「縮小」されて推定される．これを (Stein, 1956) から Stein 効果ともいう (Hoff, 2009, p. 146)．9"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#一般化線型モデル",
    "href": "posts/2024/Survey/BDA1.html#一般化線型モデル",
    "title": "ベイズデータ解析５",
    "section": "3 一般化線型モデル",
    "text": "3 一般化線型モデル\n\n3.1 線型 Gauss 性からの乖離\n正規線型モデルから，次の２つの自由度を追加したモデルを 一般化線型モデル (Nelder and Wedderburn, 1972) という：\n\n\n\n\n\n\n\nリンク関数 \\(g\\)\n\n\\[\n  g\\circ\\operatorname{E}[Y|X]=\\beta^\\top X\n  \\]\n\n（正規分布以外の）分布族 \\(P\\)\n\n\\[\n  Y|X\\sim P(\\operatorname{E}[Y|X],\\phi)\n  \\]\n\n\n\nその結果質的データ解析にも応用可能な広いクラスのモデルを得る．\n\n\n\n\n\n\nカウントデータに対する Poisson モデル\n\n\n\n自然数値のデータに対して，\\(y_i\\sim\\mathrm{Pois}(\\lambda_i)\\) の \\(\\lambda_i\\) を，正準リンク関数 \\(g=\\log\\) を通じて \\[\n\\log\\lambda_i=X_i^\\top\\beta\n\\] とモデリングすることが考えられる．このモデルを Poisson 回帰 ともいう．\n\n\n\n\n\n\n\n\n成功回数データに対する二項モデル\n\n\n\n２値データや成功回数を表すデータの場合，\\(y_i\\sim\\mathrm{Bin}(n_i,\\mu_i)\\) の \\(\\mu_i\\) を \\(\\operatorname{P}[Y_i=1|X_i]\\) の形でモデリングすることを考えることができる．\nこの場合，正準リンク関数は \\[\ng(\\mu)=\\log\\frac{\\mu}{1-\\mu}\n\\] という logit 変換で与えられる．より効率的な推論を促進するために probit リンクや，非対称性を導入する log-log リンク \\[\ng(\\mu)=\\log(-\\log\\mu)\n\\] が用いられることもある．\n\n\n\n\n3.2 指数分布族\nなお正準リンクとは，Poisson 分布族や二項分布族を指数分布族とみなした際のリンク関数のことである．\n例えば二項分布族 \\(\\{\\mathrm{Bin}(n,\\mu)\\}_{\\mu\\in(0,1)}\\) は，計数測度 \\(\\nu\\) に対して， \\[\n\\frac{d \\mathrm{Bin}(n,\\mu)}{d \\nu}(x)=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\exp\\left(x\\log\\frac{\\mu}{1-\\mu}+n\\log(1-\\mu)\\right)\n\\] と表せる．\\(g(\\mu)\\) を 自然な十分統計量 ともいう．\n指数分布族と正準リンク関数を用いた一般化線型モデリングは，パラメトリック分布族の十分統計量を代理の応答変数として線型回帰を行なっているものとみなせる．\n\n\n\n\n\n\n変換の意味\n\n\n\nこのような数理統計学的な理由とは別に，\\(g(\\mu)\\) の意味を直接解釈することもできる．\n\nPoisson 回帰はオフセット \\(\\log y_i\\) を作ることで， \\[\n\\log\\frac{\\lambda_i}{y_i}=X_i^\\top\\beta\n\\] と見ることもできる．\\(\\lambda_i/y_i\\) は観測カウント \\(y_i\\) に対する平均 \\(\\lambda_i\\) の 率比 (rate ratio) と呼ばれ，\\(g(\\lambda_i/y_i)\\) は対数率比と解釈できる．\n\\(\\frac{\\mu}{1-\\mu}\\) という値は成功確率 \\(\\mu\\) に対するオッズ比と呼ばれ，\\(g(\\mu)\\) は 対数オッズ比 (log odds ratio) と呼ばれる．\n\n\n\n\n\n3.3 分散分析\n線型モデルにおいて分散分析は，第一義的には帰無モデルの検定であった．後続の多重比較による解析は，説明変数ごとの効果量の比較を行う．\nしかし線型モデルにおいてその方法は分散の分解に基づいており，この一般化線型モデルへの拡張は自明ではない．\n一般化線型モデルにおいても残差を定義し，これに基づいてモデルの検証を行うことはできる (Davison and Tsai, 1992)．"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#終わりに",
    "href": "posts/2024/Survey/BDA1.html#終わりに",
    "title": "ベイズデータ解析５",
    "section": "4 終わりに",
    "text": "4 終わりに"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#footnotes",
    "href": "posts/2024/Survey/BDA1.html#footnotes",
    "title": "ベイズデータ解析５",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし回帰モデルを「因果効果」の推定に用いる際には，通常とは異なる，仮定に対する精査が必要になる．最も安全には，\\(x\\) が変化した際の \\(y\\) の変化量を単なる「比較」の文脈で説明することである．この「違い」が \\(x\\) の変化により生み出されたとは限らないためである．(6.4 節 Gelman, Hill, et al., 2020, p. 85) も参照．↩︎\nimproper な一様事前分布を用いた場合，引き続き不安定なままな可能性はある．だが多くのデフォルト事前分布は，weakly informative というように，軽微な情報を加えることで正則化が働くように設定されている裾の（適度に）広い事前分布であることが多い．↩︎\n従来は事前分布の経験ベイズ推定と呼ばれていた考え方である (Gelman, Vehtari, et al., 2020, p. 6)．↩︎\n一方で多くの頻度論的な手法は，無情報事前分布を仮定したベイズ推論とみなせる．そこでベイズの，有効な頻度論的モデルを探索するための方法としての美点が見えてくるのである．↩︎\nこの点については (Gelman, 2014) も参照．大統領選における有権者の行動のモデリングを，ベイズ階層モデルに基づいて探索的に実行しており，“multilevel Bayesian modeling can be considered as an elaborate form of exploratory data analysis” と結論している．↩︎\nもちろん重要な事前情報や予備解析が存在する場合は，これを事前分布としてどう更新されるかをみるのが良い．(1.6 節 Gelman, Hill, et al., 2020, p. 16) に簡潔な概観的議論がある．↩︎\n(Bafumi and Gelman, 2007) では unmodeled varying intercept と呼んでいる．↩︎\nすると「自由度」とは変動係数の数に他ならない．↩︎\n同様の縮小効果を得るための点推定手続きが，経験ベイズ の名称で研究されている (Efron and Morris, 1973), (Efron and Morris, 1975)．これについては (久保川達也, 2006) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html",
    "href": "posts/2024/Survey/Survey1.html",
    "title": "ベイズデータ解析１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#関連記事",
    "href": "posts/2024/Survey/Survey1.html#関連記事",
    "title": "ベイズデータ解析１",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#分散分析",
    "href": "posts/2024/Survey/Survey1.html#分散分析",
    "title": "ベイズデータ解析１",
    "section": "1 分散分析",
    "text": "1 分散分析\n\n1.1 はじめに\n分散分析 (ANOVA: Analysis of Variance) は標本がある因子 \\(A,B,\\cdots\\) によって層別されている場合に，層間の平均効果 \\(\\mu_i\\) に差があるかどうかを検定する手法である： \\[\nH_0:\\mu_1=\\cdots=\\mu_p\\quad\\text{v.s.}\\quad H_1:\\exists_{i,j\\in[p]}\\;\\mu_i\\ne\\mu_j.\n\\]\n因子 \\(A,B,\\cdots\\) の総数に応じて，\\(A\\) のみの場合を一元配置分散分析，\\(A,B\\) の場合を二元配置分散分析などという．\n多くの場合，観測のモデルには 線型 Gauss の仮定が置かれる．例えば一元配置である場合， \\[\nY_{ij}=\\mu_i+\\epsilon_{ij},\\qquad i\\in[p],j\\in[n_i],\\epsilon_{ij}\\sim\\mathrm{N}(0,\\sigma^2),\n\\tag{1}\\] というモデルを仮定し，パラメータ \\(\\mu_i\\) に関して検定を行う．\n二元配置では \\[\nY_{ij}=\\mu+\\alpha_i+\\beta_j+\\epsilon_{ij}\n\\] となり，集団は２つの軸 \\(A,B\\) で層別されており（分割表の状態），それぞれの因子からの効果 \\(\\alpha_i,\\beta_j\\) が説明変数に加法的に加えられる．\n\n\n1.2 分散分析の抽象的な説明\n分散分析では観測 \\(Y_{ij}\\) の変動のうち，\\(H_0\\) の仮定の下で説明されなかった部分（残差） \\(R_1^2\\) と，そもそも線型 Gauss 模型 (1) では説明しきれない部分 \\(R_0^2\\) とに関して， \\[\nF:=\\frac{(R^2_1-R^2_0)/q}{R^2_0/(n-r)}\n\\] を考える．ただし，\\(r:=\\operatorname{rank}(X)\\) はデータの自由度とした．\nこの \\(F\\) は，各群への所属表すダミー変数を用いた回帰分析の残差 \\(R_0\\) と，各群への所属を考慮しない回帰分析による残差 \\(R_1\\) とを，自由度を考慮して比を取った形をしている．\nこの \\(F\\) は一般に非心 F-分布に従い，仮定 \\(H_0\\) が成り立つときのみ 中心 F-分布 \\(\\mathrm{F}(q,n-r)\\) に従う．これは各群への所属情報に何の情報量もない場合には，\\(F\\) が同じ平均を持つ正規確率変数の自乗和の比になるためである．\n換言すれば，非心パラメータ \\[\n\\delta:=\\frac{1}{\\sigma^2}\\sum_{i=1}^pn_i(\\mu_i-\\overline{\\mu})^2\n\\] に関して \\(H_0:\\delta=0\\) を検定することに等しい (Bertolino et al., 1990), (Solari et al., 2008)．\nこの \\(F\\) （または等価な \\(t,\\delta\\)）を検定統計量として仮設検定を実行するのが (repeated measures) ANOVA である．\n\n\n1.3 Gauss-Markov の仮定\n「標本が正規分布に従う母集団からの独立標本である」という帰無仮説を持つ検定に，(Shapiro and Wilk, 1965) の検定などがある．\n探索的な方法には Q-Q plot などがある．(Bergh et al., 2020) も参照．\n等分散の仮定をチェックする検定には (Mauchly, 1940) の検定や (Brown and Forsythe, 1974) の検定などがある．\n仮にこれらの仮定が破られた場合は，(Kruskal and Wallis, 1952) 検定などのランクベースの ANOVA 手法を用いることができる．1\nただし，検定はデータの一側面しか伝えていない．例えば，データが帰無仮説をどれほど支持しているかの尺度は検定では得られない．\nそれゆえ，ANOVA などのモデルの仮定を確認するためには，検定だけでなく他の探索的手法と組み合わせることが推奨される．(Tijmstra, 2018) も参照．\n\n\n1.4 「検定」に対する注意喚起\n一方で ANOVA を含めた検定は一面のみを強調する構造となっており，一連の統計解析の中で自然な位置付けを持つものでない．\n特に，\\(p\\)-値による仮設検定は「データが不十分であることにより判断ができない」ことと，「データと帰無仮説は激しく矛盾する」こととを区別できない．この意味でも限定的な情報量しか持たない．\n例えば \\(p\\)-値が小さく帰無仮説が棄却されたとしても，\\(p\\)-値はモデルの変化に対して頑健ではないかもしれず，実際はほとんど帰無仮説が成り立つことが真実かもしれない．\nこのような全貌を探索的に捉えるためには，検定を金科玉条とするのではなく，広くモデル比較・モデル選択の観点からアプローチすることが大切である．同様のことが (Rouder et al., 2016) でも論じられている．\nANOVA は，特に大規模なものが，現在でも実験心理学などの分野で広く用いられている．これは心理学では多くの因子 \\(A,B,C,\\cdots\\) が存在し，それぞれが複雑な関係にあるためである．\nしかし推定法も従来の \\(F\\)-検定を用いたのではその力を十分に発揮できない．解決は丁寧な階層モデリングとベイズによる探索的な解析にある．2\n\n”if you have a complicated data structure and are trying to set up a model, it can help to use multilevel modeling”—not just a simple unitswithin-groups structure but a more general approach with crossed factors where appropriate. This is the way that researchers in psychology use ANOVA, but they are often ill-served by the classical framework of mean squares and F-tests. We hope that our estimation-oriented approach will allow the powerful tools of Bayesian modeling to be used for the applied goals of inference about large numbers of structured parameters. (Gelman, 2005, p. 53)"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ分散分析",
    "href": "posts/2024/Survey/Survey1.html#ベイズ分散分析",
    "title": "ベイズデータ解析１",
    "section": "2 ベイズ分散分析",
    "text": "2 ベイズ分散分析\n\n2.1 はじめに\nベイズ分散分析 (Rouder et al., 2012), (Rouder et al., 2016) では，(1) などの線型モデルに対して，パラメータが零ベクトルであることに対する検定の代わりに，帰無仮説を表現するモデルに対する Bayes 因子を用いたモデル比較を行う．\nつまり，ベイズ分散分析と言った場合，「分散分析」の概念は完全に線型回帰モデルのベイズ推論に回収される．より正確には，階層モデルのベイズ推論は「分散分析」の正統な後継である (Gelman, 2005)．\n\nIn this sense, ANOVA is indeed a special case of linear regression, but only if hierarchical models are used. (Gelman, 2005, p. 2)\n\n(Rouder et al., 2012) はその際の標準的な事前分布の選択について議論している（第 2.4 節）．\n(Rouder et al., 2016) が指摘するように，分散分析がベイズ化される過程で，検定がモデル比較に置き換わっている．\n\n\n2.2 JZS 因子\nベイズ的な仮設検定は (Jeffreys, 1961) に源流を持つ．一般に，位置母数の事前分布に Cauchy 分布を用いることは (5.3節 Jeffreys, 1961) に源流を持つ．このことについては (Robert et al., 2009) も参照．\n同一の単位を繰り返し測定し， \\[\nY_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\mu,\\sigma^2),\\qquad i\\in[n],\n\\] に従って何らかの処置効果 \\(Y_i\\) 観測するとし，帰無仮説 \\(\\mu=0\\) の妥当性を議論したいとする．\nこの際，まずは効果サイズ (effect size) (Cohen, 1988) \\(\\delta:=\\mu/\\sigma\\) という無次元量にパラメータを変換し，これを Cauchy 分布と比較することを考える： \\[\nM_0:\\delta=0\\quad\\text{v.s.}\\quad M_1:\\delta\\sim\\mathrm{C}(0,1)\n\\]\n実際，この Cauchy 分布というのは変換 \\(J(\\nu,\\sigma):=\\frac{\\mu^2}{\\sigma^2}\\) を通じて \\(\\mathbb{R}\\) 上の Jeffreys 事前分布（この場合は Lebesgue 測度に一致）に（ほとんど）等価になる．\nこの２つのモデル \\(M_0,M_1\\) の残りの仮定は共通の Jeffreys の事前分布 \\(p(\\sigma^2)\\,\\propto\\,\\sigma^{-2}\\) で共通とし，Bayes 因子を算出する．\nこの値を検定統計量のように扱うとき，これを Jeffreys の名前に (Zellner and Siow, 1980) を加えて JZS 因子 (Bayarri and García-Donato, 2007) という．あるいは上述の事前分布の選び方を JZS 事前分布という．\nJZS 因子は，事前のモデル確率 \\(p(M_0),p(M_1)\\) に依らずに算出できる．\n\n\n2.3 \\(p\\)-値との違い\nJZS 因子は \\(p\\)-値と比較して，サンプルサイズが大きいほど保守的になる傾向がある．(Wetzels et al., 2011) は 2007 年に特定の実験心理学雑誌に投稿された 855 件の t-検定に対して，JZS 因子と \\(p\\)-値との値を報告してこれを観察している．\n(Bergh et al., 2023) は実例を用いて，特に複雑な心理学実験において，２つの解析手法はときに全く違う結論を導くことを例証している．\nまた JZS 因子は \\(N\\to\\infty\\) の極限で，\\(\\delta\\ne0\\) であった場合は \\(\\infty\\) に発散し，\\(\\delta=0\\) であった場合は \\(1\\) に収束するという性質を持つ．\n\n\n2.4 １元配置 ANOVA の線型モデル解釈\n１元配置 ANOVA のモデルを次のように表す： \\[\n\\boldsymbol{Y}=\\mu\\boldsymbol{1}_n+\\sigma\\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon},\\qquad\\boldsymbol{\\epsilon}|\\sigma^2\\sim\\mathrm{N}(\\boldsymbol{0},\\sigma^2I_n).\n\\tag{2}\\] 各水準 \\(i\\in[p]\\) に属するデータの数を \\(n_i\\) とすると \\[\n\\boldsymbol{X}=\\begin{pmatrix}\n\\boldsymbol{1}_{n_1} & \\boldsymbol{0} & \\cdots & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{1}_{n_2} & \\cdots & \\boldsymbol{0}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\boldsymbol{0} & \\boldsymbol{0} & \\cdots & \\boldsymbol{1}_{n_p}\n\\end{pmatrix},\\qquad\\boldsymbol{\\theta}=\\begin{pmatrix}\n\\mu_1\\\\\n\\mu_2\\\\\n\\vdots\\\\\n\\mu_p\n\\end{pmatrix}.\n\\] となる．\nこのとき，定数項 \\(\\mu\\) をすでに括り出しているので，\\(\\boldsymbol{\\theta}=0\\) の場合のモデルが帰無仮説に対応する．\n対立仮説としては，\\(\\boldsymbol{\\theta}\\) 上に次の \\(g\\)-prior を定める： \\[\n\\boldsymbol{\\theta}\\sim\\mathrm{N}(\\boldsymbol{0},G),\\qquad G=\\mathrm{diag}(g_1,\\cdots,g_p),\\qquad g_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^{-2}(1).\n\\] これは各 \\(\\mu_i\\) に対して独立な Cauchy 事前分布を仮定している．\n(Zellner and Siow, 1980) の事前分布 \\[\n\\boldsymbol{\\theta}|g\\sim\\mathrm{N}(\\boldsymbol{0},g(\\boldsymbol{X}^\\top\\boldsymbol{X}/n)^{-1}I_p)\n\\] や一般の \\(g\\)-prior との違いとして，スケール因子 \\((\\boldsymbol{X}^\\top\\boldsymbol{X}/n)^{-1}\\) がない形であるのは，ANOVA の説明変数 \\(\\boldsymbol{X}\\) が離散変数であるためである．\n\n\n2.5 ANOVA でのベイズ因子\n以上のモデルを，帰無仮説に対立させる「デフォルトモデル」として提案したのが (Rouder et al., 2012) である．\n特に \\(G=gI_p\\) の場合は，結果として得られるベイズ因子は１次元の積分のみを含むため，簡単な数値積分アルゴリズムによって計算可能である．\n\n\n2.6 多元配置ベイズ ANOVA\n多くの場合，被説明変数に関連すると予期される因子は複数存在する．ここでは２元配置の場合を考える．それぞれの因子が \\(a,b\\) 水準を持つとき，フルモデルは \\[\n\\boldsymbol{Y}=\\mu\\boldsymbol{1}+\\sigma\\biggr(\\boldsymbol{X}_\\alpha\\alpha+\\boldsymbol{X}_\\beta\\beta+\\boldsymbol{X}_{\\gamma}\\gamma\\biggl)+\\boldsymbol{\\epsilon},\n\\] と表せる．\n詳しくは (Section 8 Rouder et al., 2012) を参照．\n\n\n2.7 「ベイズ因子」に関する注意喚起\nベイズ因子を含め，周辺尤度 \\(p(\\theta|y)\\) （エビデンスともいう）を用いた指標は，モデルの仮定に対して感度が高い (Robert, 2016), (Kamary et al., 2018)．\nそのため「モデルのデータへの当てはまりの良さを１つの指標にまとめた値」としては少し心許ない．\n加えて，帰無仮説に対立させる仮説を構成して，二項対立の構造に持ち込むことは，自然なデータ解析のワークフローに必ずしも入ってこない．\nベイズ推論の仮定で得られる事後分布から，特定の仮説 \\(H:\\theta=\\theta_0\\) がまともかを評価する方法の方が，探索的データ解析の観点からは含意が多いことも多い．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ推論に基づく方法",
    "href": "posts/2024/Survey/Survey1.html#ベイズ推論に基づく方法",
    "title": "ベイズデータ解析１",
    "section": "3 ベイズ推論に基づく方法",
    "text": "3 ベイズ推論に基づく方法\n\n3.1 はじめに\nANOVA とベイズ ANOVA の究極的な目標は，平均が一致する \\[\nH_0:\\mu_1=\\cdots=\\mu_p\n\\] という仮説がデータからどれほど支持されるか／されないかを評価することにある．\n最も直接的な方法は，パラメータ空間上に描かれる事後分布を見ることである．信用区間を報告し，帰無仮説がそのどこに位置するかを見ても良いだろう．\n\n\n3.2 事後予測によるモデル検証\nこのように，ベイズ事後分布やそのサンプルを通じたモデル検証方法は posterior predictive check (Gelman and Shalizi, 2013) と呼ばれ，これを多角的に行うことが一つの理想形とされている (Gelman et al., 2020)．\n同様に (Kruschke, 2015) では，モデル (2) の形を一般化線型モデルの特別な場合と見て推定し，ANOVA をモデル比較の観点から適切に実行する方法を論じている．\nこのように，ANOVA を適切に扱うには，階層モデルとしての取り扱いが欠かせない．同様の議論は (Gelman, 2005) でも展開されている．\nここでは，以下の節で帰無仮説 \\(H_0\\) の妥当性を詳細に評価するための方法を見ていく．\n\n\n3.3 ベイズ事後 \\(p\\)-値\nBayes 因子の他に，検定統計量に対するベイズ事後予測分布を導出し，その裾確率を評価して \\(p\\)-値の代替とする方法もある．\nこれは 事後予測 \\(p\\)-値 (posterior predictive \\(p\\)-value) (Gelman et al., 2014, p. 146) と呼ばれる．\n\n\n3.4 ROPE (Region of Practical Equivalence)\nROPE は帰無仮説 \\(H_0\\) と区別がつかないとする区間である．\n帰無仮説が \\(H_0:\\theta=\\theta_0\\) という形をしていた場合，ほとんどの場合 \\(\\theta=\\theta_0+0.1\\) でも事実上変化はない．\nこのように，帰無仮説と同一視してしまう範囲を ROPE (Kruschke, 2015, p. 336) という．\n\n3.4.1 HDR の使用\nこの ROPE が 95% 最高密度信用領域 (HDR: Highest Density Region) と互いに素になるときに，「棄却」されたとする．\nただし，最高密度信用領域とは 95% 信用区間＝95% の事後確率を持つ領域のうち，面積が最も小さいもののことを言う．\nこの方法では，逆に HDR が ROPE を完全に含む場合，帰無仮説を「採択」するという積極的な意思決定も可能である．\nROPE と同様の考え方は，ベイズによる実験計画法でも range of equivalence (Freedman et al., 1984), (Spiegelhalter et al., 1994) の名前で用いられてきた歴史がある．\n\n\n3.4.2 ROPE の確率\nまたは，事後確率分布が ROPE 内にどれほどの確率を与えるかを見ることもできる (Kruschke, 2018)．\nROPE の応用と実装は (Kelter, 2022) も参照．\n\n\n\n3.5 混合モデリングによる方法\nベイズの方法の特徴は，検定・モデル比較と推論とに区別がないことである．\n加えて純粋に検定・モデル比較のための手続きを作るより（ベイズ因子など），推定の一種として扱った方がより多くの情報を引き出せるため，ワークフローとしては好ましい (Kruschke, 2011)．\n(Robert, 2016), (Kamary et al., 2018) では検定の対象となっているモデル \\[\nM_1:x\\sim f_1(x|\\theta_1)\\quad\\text{v.s.}\\quad M_2:x\\sim f_2(x|\\theta_2)\n\\] を，混合モデル \\[\nM_\\alpha:x\\sim\\alpha f_1(x|\\theta_1)+(1-\\alpha)f_2(x|\\theta_2)\n\\] の成分の１つとみなし，その荷重 \\(\\alpha\\) の事後分布を推定し，これを検定に用いるという方法が提案されている．\n\n\n3.6 ハイパーモデル上の推論による解決\n同様の発想により，ベイズ因子の計算と推論によるモデル比較とを，より大きなハイパーモデルを立てることで同時に実行することもできる．\n(Kruschke, 2011, p. 308) や (O’Neill and Kypraios, 2016) などで考えられている．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ統計解析に関する文献案内",
    "href": "posts/2024/Survey/Survey1.html#ベイズ統計解析に関する文献案内",
    "title": "ベイズデータ解析１",
    "section": "4 ベイズ統計解析に関する文献案内",
    "text": "4 ベイズ統計解析に関する文献案内\n応用分野の研究者に対する「なぜベイズを使うのか？」に対する端的な回答として，「統計的有意性」などの「わかりやすい」指標に飛びついた結果，真のデータの声を聞かずに自分の見たいものを見始めてしまうと言うことが少なく，「自己欺瞞に陥りにくい」という美点があることは，ベイズ機械学習の稿 でも触れた．\n\\(p\\)-値はそのような欺瞞を生む代用例であり，使用を禁止すべきとの声 (Blakeley B. McShane and Tackett, 2019) もある．その論拠は大まかに次のとおりである．\nそもそも \\(p\\)-値とは，「帰無仮説を採用したモデルはデータへの当てはまりの度合いが悪い」ということを言っているだけであり，\\(p\\)-値が十分に低ければそれ以上の情報は引き出せない．\n当然 \\(p\\)-値が \\(0.01\\) であることと \\(0.00001\\) であることは質的に全く変わらない (Gelman et al., 2014, p. 150)．\nそのことに加え \\(p\\)-値は必ずしも頑健な指標ではなく，帰無仮説を少し摂動させただけで \\(p\\) 値が大きくなってしまうかもしれない．そのような場合は結局ほとんど帰無的であり，「統計的有意性」はほとんど無意味になってしまう．同様の議論が (Gelman and Stern, 2006) で展開されている．\nこのような現状への対処として，応用分野の研究者にもベイズ統計学は根本的な解決法として広く推奨される (Dienes and Mclatchie, 2018)．(Wagenmakers et al., 2016) はその旨を２つの実例を通じて簡潔に実証しており，同時にベイズ統計学の考え方に対する洗練された導入を行なっている．\n(Bergh et al., 2020) は分散分析をベイズの方法によって実行する手引きを，特に JASP を用いて実演している．\nJASP のベイズ ANOVA のエンジンは R パッケージ BayesFactor (CRAN / GitHub) を用いている．BayesFactor では大規模な \\(M\\)-元配置 ANOVA モデルにおいても Bayes 因子を用いたモデル比較を行うことができる．\nベイズ ANOVA の R パッケージとしては bayesanova (CRAN / GitHub) (Kelter, 2022) もある．これは検定に似た行為を根本的に排除して Gauss 混合モデルとして Gibbs サンプラーによるベイズ推定を実行し，ROPE (Region of Practical Equivalence) (Kruschke, 2015, p. 336) (Kruschke, 2018) を用いたモデル比較を行う．\nもちろんこのような完全なモデリングを行うことが理想かもしれないが，従来の ANOVA になれきっている研究者にとっては，Bayesian ANOVA に手を伸ばしてみることが次のステップとして大変良いだろう．\nまた別の角度からの「ベイズを使うべき理由」としての説得的な議論としては，ベイズ階層モデリングは ANOVA の正統進化という理解 (Gelman, 2005) ができるという向きもある．\n以上の立場は (Gelman et al., 2014) や (Kruschke, 2015) などの標準的なベイズデータ解析の教科書でも一貫している．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#その他の文献案内",
    "href": "posts/2024/Survey/Survey1.html#その他の文献案内",
    "title": "ベイズデータ解析１",
    "section": "5 その他の文献案内",
    "text": "5 その他の文献案内\n\nF-検定については (吉田朋広, 2006) と (Solari et al., 2008) を参考にした．\nANOVA の歴史については (Tweney, 2014) を参照．(repeated measures) ANOVA は多重線型回帰のうち説明変数が離散変数である場合に相当するという理解は，一般化線型モデルの発展と普及に伴って理解が広がった．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#footnotes",
    "href": "posts/2024/Survey/Survey1.html#footnotes",
    "title": "ベイズデータ解析１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nもちろん，Stan などの確率的プログラミング言語を用いた完全なベイズモデリングはいつでも実行可能である．↩︎\nそして因子分析を通じて，記述統計学の正統進化であるということもできる！？ ANOVA の歴史については (Tweney, 2014) も参照．↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes for Self Study",
    "section": "",
    "text": "サーベイ | レビュー | カテゴリ\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n12/12/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/12/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析８\n\n\n正規グラフィカルモデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する． 分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる． また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n通常の回帰モデルは応答変数が連続であることが暗黙の仮定となっている． この節では，応答変数が質的変数である場合のモデリングを扱う． この場合でも多くのモデルが利用可能であり，その多くが一般化線型モデルの枠組みで統一的に扱うことができる． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n12/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n理想点解析とは，政治学において国会議員のイデオロギーを定量化・視覚化する方法論である．この手法は多くの側面を持ち，項目反応モデルであると同時に多次元展開法 (MDU: Multidimensional Unfolding)でもある． \n\n\n\n\n\n11/22/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\nJulia に存在する粒子フィルター関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n10/26/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n連続時間 MCMC とは 2018 年に以降活発に研究が進んでいる新たな MCMC アルゴリズムである． 実用上の欠点に，種々のモデルに統一的な実装が難しくモデルごとにコードを書き直す必要があったことがあったが， この欠点は自動微分と [@Corbella+2022], [@Sutton-Fearnhead2023] らの研究によって解決されつつある． ここでは [@Andral-Kamatani2024] の Python パッケージ pdmp_jax とこれに基づく Julia パッケージ PDMPFlux.jl を紹介する． \n\n\n\n\n\n10/17/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程に駆動される SDE のエルゴード性\n\n\nカップリング法／最適輸送距離による証明\n\n\n\nProcess\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n10/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\nSchrödinger 橋は\n\n\n\n\n\n10/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である．今回は 公式の実装 を吟味する． \n\n\n\n\n\n10/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\nLorenz’ 63, Lorenz’ 96 とはそれぞれ [@Lorenz1963], [@Lorenz1995] によって導入された大気モデルである． 前者はバタフライ効果の語源ともなった，最初に特定されたカオス力学系でもある． Navier-Stokes 方程式は流体の運動を記述する方程式である． これらはいずれもデータ同化・軌道推定技術のベンチマークとして用いられている． ここでそれぞれのモデルの数学的性質と Julia を通じたシミュレーションの方法をまとめる． \n\n\n\n\n\n10/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n10/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n理想点解析とは，政治学において国会議員のイデオロギーを定量化・視覚化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．ここでは既存のパッケージを用いて理想点解析を行う方法を紹介する． \n\n\n\n\n\n10/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n10/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし，古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． 一方で，モデルの仮定を前面に出したベイズ的な解析手法は，データを探索的に吟味することができ，極めて微妙な消息も捉えることが可能になる． 本稿では特にベイズ ANOVA 手法 [@Gelman2005], [@Rouder+2012] を採用して，そのモデルケースを実証する． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n応募法 (voluntary sampling) や多くのウェブアンケートは，確率標本抽出に該当しない．このような場合でも母集団に関する補助情報がある限り，バイアスを軽減し推定精度を高めることができる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n標本調査において欠測はつきものである．観測単位が欠測している場合 (unit nonresponse)，call-back や follow-up などの調査を行うか，それができない場合は 荷重校正 (calibration weighting) が可能である．一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． これらの問題点を解決策としてベイズの方法を導入し，ベイズ ANOVA，ベイズ推論とモデル比較が ANOVA の発展として得られることをみる． この拡張は，ANOVA の線型モデルとしての解釈を通じてなされ，ANOVA の「同じ係数を共有するクタスタ構造の特定手法」というより広い理解へ導かれる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n人間を対象にする介入の研究では，介入の前後で変化があったかが争点となる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n9/22/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能ですが，本稿では特に R からの利用方法をまとめます．\n\n\n\n\n\n9/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n医療技術評価における生存解析では，打ち切りデータを最もよく外挿できるハザードモデルが探索される． そこでモデルの選択が重要な課題になるが，ベイズの方法だとモデル平均というアイデアが使える． これを polyhazard model で実行するためのベイズ階層モデルとモデル平均法を紹介する． キーとなる記述は Zig-Zag サンプラーである． \n\n\n\n\n\n9/12/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n本稿では生存時間解析の代表的なモデルを概観する． 特に医療技術評価への応用では，打ち切りデータを最もよく外挿できるハザードモデルが探索され，ベイズ推定が有効な方法としてよく選択される． 本稿では特に表現力の高い競合リスクモデルとして polyhazard model を紹介し，ベイズ推定の困難さを議論する．\n次稿ではこのモデルを Zig-Zag サンプラーでベイズ推定する方法を紹介する． \n\n\n\n\n\n9/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n最適輸送問題は変分法の黎明期に提案された変分問題の１つであるが，その発展は確率論の成熟を待つ必要があった．現代では多くの非正則な空間上に幾何学的な量を定義する普遍的な手法として理解されてから，多くのフィールズ賞受賞者を輩出する最も活発な分野の１つとなっている．ここまでの発展の歴史を本記事では概観したい．\n\n\n\n\n\n9/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．特に OU 過程の時間反転は雑音除去過程 (Denoising Diffusion) といい，サンプリングに利用されている．デノイジングスコアマッチングでは，時間反転に Hyvärinen スコアが出現することを利用してデータ分布のスコアを推定する．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．\n\n\n\n\n\n8/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\nSkilling-Hutchinson の跡推定量は，跡の計算 \\(O(d^2)\\) を \\(O(d)\\) に落とすことができる Monte Carlo 法である．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n行列の特異値分解とは，正方行列の直交対角化を一般の行列に拡張したものである．特異値を大きいものから \\(r\\) 個選ぶことで，Hilbert-Schmidt ノルムの意味で最適な \\(r\\)-階数近似が構成できる．このことは主成分分析に応用を持つ．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n生物情報学への応用を念頭に，tSNE と Diffusion Map について詳しく扱う．\n\n\n\n\n\n8/11/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\nカーネル法とは，半正定値カーネルを用いてデータを Hilbert 空間内に埋め込むことで，非線型な変換を行う統一的な手法である．再生核 Hilbert 空間の理論により，写した先における内積は，半正定値カーネルの評価を通じて効率的に計算できるため，無限次元空間上での表現に対する tractable な手段を提供する．適切な半正定値カーネルを用いることで，データの「類似度」を定義することができる．本稿では半正定値カーネルの理論と距離学習法を扱う．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散模型は拡張性にも優れており，条件付けが容易である．現状は誘導付き拡散によってこれが実現されるが，連続的な条件付き生成のために，フローマッチングなる方法も提案された．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n画像と動画に関してだけでなく，化学分子の構造生成の分野でも拡散模型が state of the art となっている．これは，連続空間上だけでなく，グラフなどの離散空間上でも拡散模型が拡張されたことが大きい．本稿では，離散データを連続潜在空間に埋め込むことなく，直接離散空間上に拡散模型をデザインする方法をまとめる．\n\n\n\n\n\n8/09/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n\n\n\n\n\n8/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフロー学習手法は，画像と動画に関して 2024 年時点で最良の性能を誇る． これは統計的に言えば事後分布からの近似的サンプリングを実行していることに相当する． 近似的ではなく，正確に２つの分布を補間するような拡散過程を推定するためには Schrödinger 橋がある． Schrödinger 橋については 次稿 に譲るとし，本稿ではサンプラーとしての拡散モデルを復習する． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルは「データ過程をノイズに還元する Langevin ダイナミクスを時間反転する」という発想に基づいており，画像と動画の生成・条件付き生成タスクに関して 2024 年時点で最良の方法の１つである． この発想を正確なサンプリング法に昇華するためには，[@Deming-Stephan1940] の Iterative Proportional Fitting アルゴリズムを用いることができる． この方法は拡散モデルによる条件付き生成の加速法として [@Shi+2022] によって提案された． こうして得る拡散過程は Schrödinger Bridge とも呼ばれ，エントロピー最適輸送と深い関わりを持つ． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である． 本記事では Schrödinger 橋を用いて DDS を正確にすることを考える． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフローによるサンプリング法は，画像と動画に関して 2024 年時点で最良の方法の１つである．本稿ではこれを統計に応用することを考える．\n生成モデリングを２つの密度の補間問題と捉え，Schrödinger 橋を用いた正確なサンプリング法を考える．この観点から展開されるブリッジマッチング（橋照合？）はフローマッチング，確率的補間，Rectified Flow などを綜合する枠組みとなる． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は normflows を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて，GAN の実装の概要を見る．\n\n\n\n\n\n8/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて， Ho et. al. [NeurIPS 33(2020)] による DDPM (Denoising Diffusion Probabilistic Model) の実装の概要を見る．DDPM は拡散模型の最初の例の１つであり，ノイズからデータ分布まで到達するフローを定める拡散過程（雑音除去過程）を，データをノイズにする拡散過程の時間反転として学習する方法である．画像や動画だけでなく，離散空間上でタンパク質などの構造生成でも state of the art の性能を示すモデルである．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n表現学習，非線型独立成分分析など，「生成」以外の潜在変数模型の応用法を横断してレビューする．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．\n\n\n\n\n\n7/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\nサンプリング，または Monte Carlo 法は，現代の統計学と機械学習において必要不可欠な道具となっている．それは一体どうしてだろうか？初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．初めに政治学における理想点解析の目的と役割を概観し，続いて多次元展開法と項目反応理論の２つの観点から理想点解析を眺める． \n\n\n\n\n\n7/16/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\nJulia に存在する MCMC 関連のパッケージをまとめ，多くの MCMC のパッケージを支える，Turing ecosystem の基盤となる抽象的なフレームワーク MCMCChains と AbstractMCMC を概観する．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する Metropolis-Hastings 法と MALA 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する HMC 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\nECMC (Event-chain Monte Carlo) 法は，平衡分布の直接的な評価を一度もすることなく，平衡分布からのサンプリングを達成する新たなモンテカルロ法である．非対称性をもち，従来手法より高い効率を持つ．実際，Metropolis 法の開発以来の興味の対象であった２次元剛体円板系の液相転移のシミュレーションに，約 60 年越しに成功している．\n\n\n\n\n\n6/29/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of Applied Probability 53(2016) 410-20] は Metropolis-Hastings アルゴリズムの計算複雑性を論じたもの \n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\nOrnstein-Uhlenbeck 過程は唯一の非自明な定常 Gauss-Markov 過程である．また，連続時間の自己回帰模型を与える重要な拡散過程である．加えて，その遷移半群は解析的な表示を持ち，Malliavin 解析でも基本的な意味を持つ．したがって，直感的な理解を涵養しておくことは非常に見返りが大きいことだろう．そこで，YUIMA パッケージを通じてシミュレーションを行いながら，Ornstein-Uhlenbeck のパラメータの意味と，遷移半群・生成作用素の直感的な理解の醸成を目指す．\n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫制博士課程（正確には，総合研究大学院大学統計科学コース）を紹介します．同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷のような研究環境が整っていると言えるでしょう．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Statistical Science 16(2001) 351-67] は Metropolis-Hastings 法の最適スケーリングに関する結果をまとめ，実際の実装にその知見をどのように活かせば良いかを例示したレビュー論文である． \n\n\n\n\n\n5/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はこのような確率過程に対する統計推測を実行する方法を紹介します．yuima は従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．ほとんどの手法が，\\(N\\to\\infty,\\Delta_n\\to0\\) の極限で得られるデータ（高頻度データ）にも応用可能な手法となっています．\n\n\n\n\n\n5/18/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はそのような確率過程の汎函数の漸近展開に基づく計算方法を紹介します．確率変数の期待値を近似するのに Monte Carlo 法は普遍的な方法ですが，漸近展開が用いられる場合，その計算時間は比較にならないほど速くなります．\n\n\n\n\n\n5/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．本稿では Stan 言語の基本をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．鋭意開発中のパッケージですが，すでに広範なクラスの確率微分方程式のシミュレーションが可能です．本稿では基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\nR パッケージ YUIMA を用いた SDE のベイズ推定に，Stan を用いる方法を模索する．Stan は C++ を用いる独立した確率プログラミング言語で移植性は高いが，それ故 YUIMA からこれを用いる際に，専用のインターフェイスを考える必要が生じる．\n\n\n\n\n\n5/12/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行うための R パッケージである．本稿では，brms の基本的な使い方と，その実装を紹介する．\nまた，ランダム効果モデルとは何であるか，固定効果モデル・混合効果モデル・一般化推定方程式などとの違いをレビューする．ランダム効果の追加は縮小推定などの自動的な正則化を可能とする美点がある一方で，係数の不偏推定やロバスト推定に拘る場合はこれを避ける判断もあり得る． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n5/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの． \n\n\n\n\n\n4/22/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子法とは空間や分布を多数の粒子の集合として離散化して表現・計算する技術の総称である．シミュレーションからデータ同化まで幅広い応用を持つ．この記事ではこれらの技術を「粒子」という軸でひとつの記事にまとめることを試みる． \n\n\n\n\n\n4/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n論文メモ\n\n\n\nReview\n\n\n\n[@Dai+2019] は有限混合で表される分布からのサンプリング法（Fusion 問題）に関する最初の理論解析である． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n[@Fearnhead+2017] は拡散過程を離散化誤差なしにシミュレーションする手法を提案している．逐次重点サンプリング（SIS）の連続時間極限を考えることで，提案過程と重点荷重との組がPDMPとなり，効率的なシミュレーションが可能になる． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\n\n\n\nProcess\n\n\n\nMarkov 過程のエルゴード性の証明は，カップリングの概念を用いれば極めて明瞭に見渡せる．\n\n\n\n\n\n3/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論，確率論，さらにはデータ解析の中心に据えられるべき中心概念であると言えるかもしれない．例えば，カーネル法とは確率核に沿った埋め込みである．MCMC の性質も，本質的に確率核の性質が決定する．また確率核は，確率空間の圏の射となる．このように，多くのデータ解析手法の中核に位置する数学的本体たる「確率核」への入門を目指すのが本記事である．\n\n\n\n\n\n3/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの微細化技術をレビューする．\n\n\n\n\n\n3/23/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\nPython で最適輸送写像を計算する方法を解説する． 直接最適輸送問題を POT (Python Optimal Transport) で解く．この方法は原子の数 \\(N\\) に対して \\(O(N^3\\log N)\\) の複雑性を持つ． 一方で，エントロピー正則化項 \\(\\epsilon\\operatorname{Ent}(\\pi)\\) を導入したエントロピー最適輸送問題は Sinkhorn アルゴリズムで高速に解くことができる． これには OTT-JAX パッケージを用いる． \\(\\epsilon\\to0\\) の極限で元の最適輸送問題の解を得る． \n\n\n\n\n\n3/13/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\nState vs Loomis 判決を題材に，アルゴリズムと公平性を議論する．\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n転移学習とは\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．\n\n\n\n\n\n3/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\nPAC-Bayes は現実的に有用な鋭い PAC bound を得る新たな技術である．最適化の問題に帰着する点が研究を盛り上げている．Vapnik-Chervonenkis 理論の一般化であり，推定量上の確率分布を返すようなより一般的なアルゴリズムに対しても適用できる．\n\n\n\n\n\n3/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，「刑法入門」の内容を扱う．\n\n\n\n\n\n2/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n深層学習の学習における確率最適化アルゴリズムに関して概説する．\n\n\n\n\n\n2/16/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n数学者のために，深層生成モデルを概観する．\n\n\n\n\n\n2/13/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する．\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次稿 で，\\(K\\)-平均アルゴリズムの model-aware な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す． \n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\n\nNicolas Chopin の論文を読んで短くまとめたものです。\n\n\n\n\n\n1/30/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/29/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n生い立ち\n\n\n\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n1/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n粒子フィルターを拡散過程に対して適用することを考える．拡散過程の Euler-Maruyama 離散化に対して構成された粒子フィルターの，タイムステップを \\(0\\) にする極限 \\(\\Delta\\searrow0\\) での振る舞いを議論する．\n\n\n\n\n\n1/23/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\n\n\n\nProcess\n\n\n\nマルチンゲール問題とは何か？\n\n\n\n\n\n1/20/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n粒子フィルターは，リサンプリングを取り入れた逐次重点サンプリングと見れる．リサンプリングにより荷重の退化を防げるが本質的な問題は回避できないことが多い．本稿では，リサンプリングのアルゴリズムを複数紹介し比較する．\n\n\n\n\n\n1/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\n\n\n\nProcess\n\n\n\n確率過程の離散化に関する漸近論的な結果を，Brown 運動を例に取り示す．\n\n\n\n\n\n1/09/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，\n\n\n\n\n\n1/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\nVSCode での LaTeX 環境構築に関するページ．\n\n\n\n\n\n12/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nMCMC\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\n\n\n\nKernel\n\n\n\n数学者のために，SVMによるデータ解析が何をやっているのかを抽象的に説明する．\n\n\n\n\n\n11/18/2023\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\nFeynman-Kac モデルという物理モデルを定義し，逐次モンテカルロ法（粒子フィルター）をその Monte Carlo シミュレーション法として位置付けて解説した書籍である． 例として挙げられるトピックも物理学のものが多く，書籍のスタイルも物理学書のそれである． ここでは 1.1 節 “On the Origins of Feynman-Kac and Particle Models” の抄訳を通じて内容を概観したい． \n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．公式の ギャラリー も参照．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia のSymbol型とExpr型，そしてExpr型からExpr型への関数であるマクロを用いたメタプログラミングについて解説する．\n\n\n\n\n\n1/23/2022\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nPython の import について\n\n\n\n\n\n\nPython\n\n\n\nPython の import について\n\n\n\n\n\n5/23/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia でパッケージを作成する際の基礎知識をまとめる．\n\n\n\n\n\n9/10/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は階層関係を明示的に宣言する必要のある名前的型付け言語であり，既存の型から自由な構成が可能なパラメトリック型付け言語である．\n\n\n\n\n\n9/09/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia において，関数はメソッドの貼り合わせである．スクリプト言語のように，気軽に関数定義を行うこともできれば（単一メソッドによる関数と解す），多重ディスパッチによる実装も可能である．\n\n\n\n\n\n9/08/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は 2012 年に公開された科学計算向きの動的型付け言語である．\n\n\n\n\n\n9/07/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は動的型付け言語で，型宣言 :: を省略すると全てのオブジェクトとマッチする Any 型と解釈される．一方で静的型付け言語のような豊かな型システムも持つ．これにより関数をメソッドのディスパッチにより実装するのが Julia の根幹思想である．メソッドのディスパッチについては次稿に譲り，ここでは基本的なデータ型について述べる．\n\n\n\n\n\n9/06/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html",
    "href": "static/Sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      5/24/2024.\n      司馬博文 .\n      \n        新時代の MCMC を迎えるために\n        : 連続時間アルゴリズムへの進化\n      .\n      \n        統数研オープンハウス.\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n    \n      2/25/2024.\n      Hirofumi Shiba.\n      \n        A Recent Development of Particle Methods\n        : Inquiry towards a Continuous Time Limit and Scalability\n      .\n      \n        MLSS2024 (OIST, Okinawa, Japan).\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#upcomings-newests",
    "href": "static/Sessions.html#upcomings-newests",
    "title": "Sessions",
    "section": "",
    "text": "連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#presentation-history",
    "href": "static/Sessions.html#presentation-history",
    "title": "Sessions",
    "section": "",
    "text": "5/24/2024.\n      司馬博文 .\n      \n        新時代の MCMC を迎えるために\n        : 連続時間アルゴリズムへの進化\n      .\n      \n        統数研オープンハウス.\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n    \n      2/25/2024.\n      Hirofumi Shiba.\n      \n        A Recent Development of Particle Methods\n        : Inquiry towards a Continuous Time Limit and Scalability\n      .\n      \n        MLSS2024 (OIST, Okinawa, Japan).\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#sec-法律家のための統計数理",
    "href": "static/Sessions.html#sec-法律家のための統計数理",
    "title": "Sessions",
    "section": "法律家のための統計数理",
    "text": "法律家のための統計数理\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n数学と法学，双方からの交流と理解を図ります．\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#empirical-process-theory",
    "href": "static/Sessions.html#empirical-process-theory",
    "title": "Sessions",
    "section": "Empirical Process Theory",
    "text": "Empirical Process Theory\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\nTextbook：Kengo Kato Empirical Process Theory (Lecture Note)\n\n\n\n担当分の発表資料"
  },
  {
    "objectID": "static/Sessions.html#学振-dc1",
    "href": "static/Sessions.html#学振-dc1",
    "title": "Sessions",
    "section": "学振 DC1",
    "text": "学振 DC1\n\n\n\n\n\n\n\n\n\n\nPeriod\nApplication Category\nSmall Category\n結果\n\n\n\n\nSpring, 2024\n解析学、応用数学およびその関連分野\n12040 応用数学および統計数学関連\n不採択 AT スコア 2.639（上位３割）\n\n\n\n\n本書類審査セットにおける 2024 年度の採択率は 11.6% でした．\n\n\n\n申請書（最終版，５月19日）\n\n\n\n\n\n参考：申請書（バージョン１，４月３日）\n\n\n\n\n評点結果\n\n\n\n\n\n\n\n着想およびオリジナリティ\n研究者としての資質\n総合評価\n\n\n\n\n3.50\n3.17\n3.17"
  },
  {
    "objectID": "static/PartialCategories.html#probability",
    "href": "static/PartialCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#process",
    "href": "static/PartialCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#functional-analysis",
    "href": "static/PartialCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\nFunctional Analysis\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#geometry",
    "href": "static/PartialCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mathcalpx",
    "href": "static/PartialCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#nature",
    "href": "static/PartialCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mcmc",
    "href": "static/PartialCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#foundation",
    "href": "static/PartialCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#information",
    "href": "static/PartialCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#statistics",
    "href": "static/PartialCategories.html#statistics",
    "title": "Categories",
    "section": "2.5 Statistics",
    "text": "2.5 Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#computation-2",
    "href": "static/PartialCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#sampling",
    "href": "static/PartialCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#python",
    "href": "static/PartialCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#julia",
    "href": "static/PartialCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#yuima",
    "href": "static/PartialCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#stan",
    "href": "static/PartialCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#r",
    "href": "static/PartialCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#bayesian",
    "href": "static/PartialCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#particles",
    "href": "static/PartialCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#kernels",
    "href": "static/PartialCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#ai",
    "href": "static/PartialCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#deep-learning",
    "href": "static/PartialCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#posters",
    "href": "static/PartialCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#reviews",
    "href": "static/PartialCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#surveys",
    "href": "static/PartialCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#slides",
    "href": "static/PartialCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズとは何か\n\n\n数学による統一的アプローチ\n\n\n\nSlide\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nSlide\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#papers",
    "href": "static/PartialCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/PartialCategories.html#opinion",
    "href": "static/PartialCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#life",
    "href": "static/PartialCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n2024-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n生い立ち\n\n\n\nLife\n\n\n\n\n2024-01-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#lifestyle",
    "href": "static/PartialCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#法律家のための統計数理",
    "href": "static/PartialCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#俺のための-julia-入門",
    "href": "static/PartialCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Japanese.html",
    "href": "static/Japanese.html",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "私，司馬博文は総合研究大学院大学５年一貫博士課程（統計科学コース）２年で，ベイズ計算を専門に研究しています．\nベイズ推論は統計学・逆問題・システム制御・機械学習・認知科学などの分野において，確率論を応用するための指導原理を与える考え方です．\nそこで私はベイズ推論において目的に依存せずに使える汎用アルゴリズムを開発することで，確率論の応用を広げることを目指しています．\nPDMPFlux.jl や YUIMA などのパッケージ開発，ヘルスケア・ものづくり企業へのデータ解析コンサルティングもフリーランスで行っています．\n以上の研究・社会実装活動はすべて，疫学や政治科学，惑星地球科学などの領域で「世界をよりよく知るために，計算機をどう使えるか？」という共通の興味に支えられています．\n\n\n\n\n\n\n\n統計数理研究所 鎌谷研吾 先生と 矢野恵佑 先生の下で，モンテカルロ法を研究しています．特にマルコフ連鎖モンテカルロ法 (MCMC) や逐次モンテカルロ法 (SMC) など，ベイズ推論を実現するアルゴリズムを専門としています．\n確率過程の収束や，確率測度の空間 \\(\\mathcal{P}(E)\\) の幾何を議論することで，アルゴリズムの挙動を分析する数理的な枠組みの構築を目指しています．\nまた連続時間 MCMC を用いた事後分布サンプリングのための Julia パッケージ PDMPFlux.jl や，確率過程の統計推測のための R パッケージ YUIMA の開発にも取り組んでおり，モンテカルロ法とベイズ統計の応用に広く取り組んでいます．\n\n\n\n\n\n\n輸送によるサンプリング法：Schrödinger 橋・SMC サンプラー．\nモンテカルロ計算：MCMC，SMC，PDMP などのコンピューテーション．\n統計モデリング：政治学・疫学・惑星地球科学などの分野への応用． \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024.9 – 現在．株式会社プリメディカ\n\n\n\n2023.4 – 現在．東京大学先端科学技術研究センター\n\n\n\n\n\n\n\n\n\n総合研究大学院大学．指導教員：鎌谷研吾，矢野恵佑．\n\n\n\n東京大学理学部数学科．指導教員：吉田朋広．"
  },
  {
    "objectID": "static/Japanese.html#研究",
    "href": "static/Japanese.html#研究",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "統計数理研究所 鎌谷研吾 先生と 矢野恵佑 先生の下で，モンテカルロ法を研究しています．特にマルコフ連鎖モンテカルロ法 (MCMC) や逐次モンテカルロ法 (SMC) など，ベイズ推論を実現するアルゴリズムを専門としています．\n確率過程の収束や，確率測度の空間 \\(\\mathcal{P}(E)\\) の幾何を議論することで，アルゴリズムの挙動を分析する数理的な枠組みの構築を目指しています．\nまた連続時間 MCMC を用いた事後分布サンプリングのための Julia パッケージ PDMPFlux.jl や，確率過程の統計推測のための R パッケージ YUIMA の開発にも取り組んでおり，モンテカルロ法とベイズ統計の応用に広く取り組んでいます．"
  },
  {
    "objectID": "static/Japanese.html#キーワード",
    "href": "static/Japanese.html#キーワード",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "輸送によるサンプリング法：Schrödinger 橋・SMC サンプラー．\nモンテカルロ計算：MCMC，SMC，PDMP などのコンピューテーション．\n統計モデリング：政治学・疫学・惑星地球科学などの分野への応用．"
  },
  {
    "objectID": "static/Japanese.html#経歴",
    "href": "static/Japanese.html#経歴",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "2024.9 – 現在．株式会社プリメディカ\n\n\n\n2023.4 – 現在．東京大学先端科学技術研究センター"
  },
  {
    "objectID": "static/Japanese.html#学位",
    "href": "static/Japanese.html#学位",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学．指導教員：鎌谷研吾，矢野恵佑．\n\n\n\n東京大学理学部数学科．指導教員：吉田朋広．"
  },
  {
    "objectID": "static/CV/cv.html#profile-and-skills",
    "href": "static/CV/cv.html#profile-and-skills",
    "title": "Hirofumi Shiba",
    "section": "Profile and Skills",
    "text": "Profile and Skills\nHirofumi is currently a Ph.D. candidate at the Institute of Statistical Mathematics, working under the supervision of Prof. Kengo Kamatani and Keisuke Yano.\nHe holds Japanese citizenship and is a native speaker of Japanese and Chinese. Additionally, He is fluent in English.\nHirofumi codes in Julia, Python, and R."
  },
  {
    "objectID": "static/CV/cv.html#research-interests",
    "href": "static/CV/cv.html#research-interests",
    "title": "Hirofumi Shiba",
    "section": "Research Interests",
    "text": "Research Interests\n\nMonte Carlo methods, including Sequential Monte Carlo and Markov Chain Monte Carlo.\nTransport methods, including Schrödinger Bridges and Normalizing Flows.\nBayesian modeling, including Political Science and Bioinformatics.\nBayesian machine learning, including Gaussian processes and hierarchical modeling."
  },
  {
    "objectID": "static/CV/cv.html#work-experience",
    "href": "static/CV/cv.html#work-experience",
    "title": "Hirofumi Shiba",
    "section": "Work Experience",
    "text": "Work Experience\n\nConsultant. PreMedica, Inc., Tokyo, Japan. 2024.9 – today\nProvided Bayesian data analysis solutions for clients in the healthcare industry.\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – today\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference.\nCooperative Researcher. RCAST, the University of Tokyo, Japan. 2023.4 – today\nResearch on trustworthy AI and machine learning from the viewpoint of economic security.\nData Scientist. IMIS Co., Ltd. 2022.8 – 2024.1\nProvided statistical analysis and machine learning solutions for clients in the manufacturing industry."
  },
  {
    "objectID": "static/CV/cv.html#education",
    "href": "static/CV/cv.html#education",
    "title": "Hirofumi Shiba",
    "section": "Education",
    "text": "Education\n\nPh.D. in Statistical Science. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2023.4 – 2028.3\nSuperivsor: Kengo Kamatani and Keisuke Yano\nB.A. in Mathematics. The University of Tokyo, Japan. 2019.4 – 2023.3\nSupervisor: Nakahiro Yoshida"
  },
  {
    "objectID": "static/CV/cv.html#research-stay",
    "href": "static/CV/cv.html#research-stay",
    "title": "Hirofumi Shiba",
    "section": "Research Stay",
    "text": "Research Stay\n\nUniversity College London，United Kingdom．2024.11.4 – 2024.12.2\nSupervisor: Alexandros Beskos"
  },
  {
    "objectID": "static/AllCategories.html#probability",
    "href": "static/AllCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#process",
    "href": "static/AllCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程に駆動される SDE のエルゴード性\n\n\nカップリング法／最適輸送距離による証明\n\n\n\nProcess\n\n\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\nProcess\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\nProcess\n\n\n\n\n2024-01-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#functional-analysis",
    "href": "static/AllCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\nFunctional Analysis\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#geometry",
    "href": "static/AllCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mathcalpx",
    "href": "static/AllCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#nature",
    "href": "static/AllCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mcmc",
    "href": "static/AllCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#foundation",
    "href": "static/AllCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n\n2024-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\n\n2024-03-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#information",
    "href": "static/AllCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#statistics",
    "href": "static/AllCategories.html#statistics",
    "title": "Categories",
    "section": "2.5 Statistics",
    "text": "2.5 Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#computation-2",
    "href": "static/AllCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#sampling",
    "href": "static/AllCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#python",
    "href": "static/AllCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nPython の import について\n\n\n\nPython\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#julia",
    "href": "static/AllCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-10-26\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#yuima",
    "href": "static/AllCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#stan",
    "href": "static/AllCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#r",
    "href": "static/AllCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#bayesian",
    "href": "static/AllCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#particles",
    "href": "static/AllCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-10-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#kernels",
    "href": "static/AllCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#ai",
    "href": "static/AllCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#deep-learning",
    "href": "static/AllCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#posters",
    "href": "static/AllCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#reviews",
    "href": "static/AllCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-05-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#surveys",
    "href": "static/AllCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#slides",
    "href": "static/AllCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズとは何か\n\n\n数学による統一的アプローチ\n\n\n\nSlide\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nSlide\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#papers",
    "href": "static/AllCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/AllCategories.html#opinion",
    "href": "static/AllCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\nOpinion\n\n\n\n\n2024-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#life",
    "href": "static/AllCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n2024-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n生い立ち\n\n\n\nLife\n\n\n\n\n2024-01-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#lifestyle",
    "href": "static/AllCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#法律家のための統計数理",
    "href": "static/AllCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#俺のための-julia-入門",
    "href": "static/AllCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html",
    "href": "posts/2024/Process/PureJump1.html",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "",
    "text": "YUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#関連記事",
    "href": "posts/2024/Process/PureJump1.html#関連記事",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "",
    "text": "YUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#はじめに",
    "href": "posts/2024/Process/PureJump1.html#はじめに",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "1 はじめに",
    "text": "1 はじめに\n本稿では Lévy 過程 \\(\\{Z_t\\}\\) に駆動された SDE \\[\ndX_t=a(X_t)\\,dt+b(X_{t-})\\,dZ_t=a(X_t)\\,dt+\\sigma(X_t)\\,dW_t+\\int_{\\left\\{\\lvert u\\rvert\\le 1\\right\\}}c(X_{t-},u)\\,\\widetilde{N}(dtdu)+\\int_{\\left\\{\\lvert u\\rvert&gt;1\\right\\}}c(X_{t-},u)\\,N(dtdu)\n\\tag{1}\\] で駆動される確率過程 \\(\\{X_t\\}\\) のエルゴード性を議論する．\nLévy 過程 \\(\\{Z_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^m)\\) は拡散項を持たない \\(\\sigma\\equiv0\\) とし，跳躍係数 \\(c:\\mathbb{R}^m\\times\\mathbb{R}^m\\to M_m(\\mathbb{R})\\) も跳躍幅 \\(u\\) に関して線型 \\(c(x,u)=b(x)u\\) で，次の Ito-Lévy 分解を持つとする： \\[\nZ_t=\\int^t_0\\int_{\\lvert u\\rvert\\le1}u\\widetilde{N}(ds,du)+\\int^t_0\\int_{\\lvert u\\rvert&gt;1}uN(ds,du),\n\\] \\[\nN(ds,du):=ds\\,\\nu(du),\\qquad\\widetilde{N}(ds,du):=N(ds,du)-ds\\,\\nu(du),\\qquad\\int_{\\mathbb{R}^m}\\lvert u\\rvert^2\\land1\\nu(du)&lt;\\infty.\n\\] ただし \\(N\\) を強度測度 \\(ds\\,\\nu(du)\\) を持つ跳躍を表す Poisson 点過程とした．\nBorel 可測関数 \\(b:\\mathbb{R}^m\\to M_{m}(\\mathbb{R})\\) と \\(a:\\mathbb{R}^m\\to\\mathbb{R}^m\\) は局所 Lipschitz 連続で，線型増大条件 \\[\n\\lvert a(x)\\rvert^2+\\int_{\\lvert u\\rvert\\le1}\\lvert b(x)u\\rvert^2\\nu(du)\\le K(1+\\lvert x\\rvert^2),\\qquad x\\in\\mathbb{R}^m,\n\\] を満たすとする．このとき，SDE (1) には一意な強解 \\(\\{X_t\\}\\) が存在し，\\(X\\) は càdlàg な Markov 過程である．1\n加えて，\\(b\\) が有界であるという条件も引き続き課すこととする．\n伊藤の公式より，拡張生成作用素 \\[\n\\widehat{L}f(x):=\\left(Df(x)\\,|\\,a(x)\\right)+\\int_{\\mathbb{R}^m}\\biggr(f\\biggr(x+b(x)u\\biggl)-f(x)-1_{B^m}\\biggr((Df(x)|b(x)u)\\biggl)\\biggl)\\nu(du),\\qquad f\\in C^2(\\mathbb{R}^m),\n\\tag{2}\\] に関して \\(M_t^f:=f(X_t)-f(x)-\\int^t_0\\widehat{L}f(X_s)ds\\) で定まる càdlàg 過程 \\(\\{M^f_t\\}\\) は任意の \\(x\\in\\mathbb{R}^m\\) に関して局所 \\(\\operatorname{P}_x\\)-マルチンゲールである．\n拡散過程，例えば Langevin 動力学のエルゴード性証明\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nとの最大の違いは，ドリフト関数 \\(V\\in C^2(\\mathbb{R}^m)\\) に Lévy 測度 \\(\\nu\\) に関する可積分条件が加わることにある．そもそも \\(\\widehat{L}V\\) が well-defined であるためには，式 (2) の積分が発散してはならないのである．\nこのように \\(\\widehat{L}f(x)\\) の値が \\(f\\) の \\(x\\in\\mathbb{R}^m\\) 以外での値にも依存する性質を 非局所性 という．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#文献紹介",
    "href": "posts/2024/Process/PureJump1.html#文献紹介",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "2 文献紹介",
    "text": "2 文献紹介\n\nLévy 過程のエルゴード性の結果については，(3.4節 Kulik, 2018) によくまとまっている．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#footnotes",
    "href": "posts/2024/Process/PureJump1.html#footnotes",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ikeda and Watanabe, 1981, p. 245) 定理9.1．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html",
    "href": "posts/2024/Process/Discretization.html",
    "title": "確率過程の離散化",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n(Jacod and Protter, 2012) 第１章参考．\n参照過程は Brown 運動 \\((W_t)_{t\\in\\mathbb{R}_+}\\) のスケーリング \\[\nX=\\sigma W,\\quad(\\sigma&gt;0)\n\\] であるとする．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "1 正規化汎函数 \\(V^{'n}(f,X)\\)",
    "text": "1 正規化汎函数 \\(V^{'n}(f,X)\\)\n\n1.1 \\(t\\in\\mathbb{R}_+\\) 毎の収束\n\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は独立同分布であるが，正規化を施したことにより， \\[\n\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}=\\frac{X_{i\\Delta_n}-X_{(i-1)\\Delta_n}}{\\sqrt{\\Delta_n}}\\sim\\mathrm{N}(0,c)\n\\] も離散化の段階 \\(n=0,1,\\cdots\\) に依らず独立同分布である．よって， \\[\nf\\left(\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}\\right)\\sim(\\rho_c(f),\\rho_c(f^2)-\\rho_c(f)^2)\n\\] を踏まえて，独立同分布列に対する０次と１次の漸近定理から \\[\nV^{'n}(f,X)_t\\overset{\\text{p}}{\\to}t\\rho_c(f)\n\\] \\[\n\\frac{V^{'n}(f,X)_t-t\\rho_c(f)}{\\sqrt{\\Delta_n}}\\overset{\\text{d}}{\\to}\\mathrm{N}\\biggr(0,t(\\rho_c(f^2)-\\rho_c(f)^2)\\biggl)\n\\] が言えそうである．\n\n０次の漸近論で概収束は示せない．\n\n\n\n1.2 \\(\\mathbb{R}_+\\) 上の過程としての収束\n\\(\\mathbb{R}_+\\) で添字付けられた過程として，\\(D(\\mathbb{R}_+)\\) 上の Skorohod 位相について確率収束する．すなわち，任意の \\(t\\in[0,T]\\) に対して， \\[\n\\sup_{s\\le t}\\lvert Z^n_s-Z_s\\rvert\\overset{\\text{p}}{\\to}0.\n\\] 加えて，汎函数中心極限定理から， \\[\n\\left(\\frac{1}{\\sqrt{\\Delta_n}}(V^{'n}(f,X)_t-t\\rho_c(f))\\right)_{t\\ge0}\\overset{\\text{d}}{\\to}\\sqrt{\\rho_c(f^2)-\\rho_c(f)^2}B.\n\\] が Skorohod 位相に関して成り立つ．これはさらに安定収束もするのである．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "2 非正規化汎函数 \\(V^n(f,X)\\)",
    "text": "2 非正規化汎函数 \\(V^n(f,X)\\)\n正規化を施さないために，\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は \\(0\\) に漸近していき，関数 \\(f\\) の \\(0\\) での局所的な振る舞いが収束に影響を与えるようになる．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html",
    "href": "posts/2024/Process/Levy.html",
    "title": "Lévy 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nYUIMAについては次の記事も参照：\nPoisson 過程と複合 Poisson 過程については次の記事を参照："
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "href": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "title": "Lévy 過程を見てみよう",
    "section": "1 Lévy-Itô 分解",
    "text": "1 Lévy-Itô 分解\n\n1.1 加法過程の定義\n\n\n\n\n\n\n定義 (additive process, Lévy process)1\n\n\n\n確率過程 \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 加法過程 であるとは，最初の４条件を満たすことをいう．５条件全てを満たすとき，Lévy 過程 であるという．\n\n\\(X_0\\overset{\\text{a.s.}}{=}0\\)．\nある充満集合 \\(\\Omega_0\\subset\\Omega\\) が存在し，\\(t\\mapsto X_t(\\omega)\\) は càdlàg である．\n独立増分：任意の \\(0\\le t_0&lt;\\cdots&lt;t_n\\) について， \\[\n  X_{t_0},X_{t_1}-X_{t_0},X_{t_2}-X_{t_1},\\cdots,X_{t_n}-X_{t_{n-1}}\n  \\] は独立である．\n確率連続：任意の \\(\\epsilon&gt;0\\) と \\(t\\ge0\\) について， \\[\n  \\lim_{s\\to t}\\operatorname{P}[\\lvert X_s-X_t\\rvert&gt;\\epsilon]=0\n  \\] が成り立つ．\n定常増分：\\(X_{s+t}-X_s\\) の分布は \\(s\\ge0\\) に依らない．\n\n\n\n\n\n1.2 特性量\n加法過程 \\(\\{X_t\\}\\) について，\\(X_t\\) の分布は必ず \\(\\mathbb{R}^d\\) 上の無限可分分布になる．2\n加えて，加法過程の分布は１次元の有限次元分布族が特徴付ける．\n\n\n\n\n\n\n定理（加法過程の分布）3\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を加法過程とする．\n\n\\(\\mu_{s,t}\\) を \\(X_t-X_s\\) の分布とすると，これは無限可分であり，次を満たす： \\[\n\\mu_{s,t}*\\mu_{t,u}=\\mu_{s,u},\n\\] \\[\n\\mu_{s,s}=\\delta_0,\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\quad(s\\nearrow t),\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\qquad(t\\searrow s).\n\\]\n確率分布の族 \\(\\{\\mu_{s,t}\\}\\subset\\mathcal{P}(\\mathbb{R}^d)\\) が上の４式を満たすならば，これはある加法過程 \\(\\{X_t\\}\\) が定める分布である．\n２つの加法過程 \\(X,X'\\) が， \\[\nX_t\\overset{\\text{d}}{=}X'_t\\quad t\\in\\mathbb{R}_+\n\\] を満たすならば，\\(X\\overset{\\text{d}}{=}X'\\) が成り立つ．\n\n\n\nこのことにより，加法過程 \\(X\\) の分布は，各 \\(X_t\\) の無限可分分布を特徴付ける特性量 \\((A_t,\\nu_t,\\gamma(t))\\) によって特徴付けられる．\n\n\n\n\n\n\n無限可分分布の特徴付け (Khinchin and Lévy, 1936)4\n\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) について，次は同値：\n\n\\(f\\) は無限可分である．\n(Lévy) ある \\[\n  \\nu(\\{0\\})=0,\\qquad\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n  \\] を満たす測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}^d)\\) と対称な半正定値行列 \\(A\\in S_n(\\mathbb{R})_+\\) と \\(\\gamma\\in\\mathbb{R}^d\\) が一意的に存在して，次のように表せる： \\[\n  f(z)=\\exp\\left(-\\frac{1}{2}(z|Az)+i(\\gamma|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\\biggl)\\nu(dx)\\right).\n  \\]\n(Khinchin) ある有限測度 \\(\\Psi\\in\\mathcal{M}^1(\\mathbb{R}^d)\\) と \\(\\alpha\\in\\mathbb{R}^d\\) が存在して，次のように表せる： \\[\nf(u)=\\exp\\left(i(\\alpha|u)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|x)}-1-\\frac{i(u|x)}{1+\\lvert x\\rvert^2}\\biggl)\\frac{1+\\lvert x\\rvert^2}{\\lvert x\\rvert^2}\\Psi(dx)\\right).\n\\]\n\n\\(A\\) を Gauss 共分散，\\(\\nu\\) を Lévy 測度，\\(\\Psi\\) を Khintchine測度 という．5\n加えて，任意の半正定値行列 \\(A\\)，\\(\\gamma\\in\\mathbb{R}^d\\)，Lévy 測度 \\(\\nu\\) であって \\(\\nu(\\{0\\})=0\\) かつ \\[\n\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n\\] を満たすものに対して，\\((A,\\nu,\\gamma)\\) を特性量にもつ無限可分分布が存在する．\nKhintchin の表示はより直接的である上に，Khintchin 測度は有限になる．さらに，\\((\\alpha,\\Psi)\\) の収束が過程の収束にも対応する！6 だが，確率論的な意味付けに欠けるために，Lévy の表示の方をここでは用いる．\nLévy の表示の被積分関数 \\[\ne^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\n\\] は大変複雑であるが，こうしないと \\(\\nu\\)-可積分にならないのである．\n\\(\\nu\\) は \\(O(\\lvert x\\rvert^2)\\) 関数に関してならば \\(0\\) の近傍でも可積分であるから，\\(e^{i(z|x)}\\) から１次以下の項を \\(0\\) の近傍から取り去ることで可積分にしているのである．そのため，最後の項は \\(1_{\\left\\{B^d\\right\\}}\\) でなくとも， \\[\nc(x)=1+o(\\lvert x\\rvert)\\quad(\\lvert x\\rvert\\to0)\n\\] \\[\nc(x)=O(\\lvert x\\rvert^{-1})\\quad(\\lvert x\\rvert\\to\\infty)\n\\] の２条件を満たすものならばなんでも良い．だが，取り替える度に１次の項 \\(\\gamma\\in\\mathbb{R}^d\\) を変更する必要がある．\n一般に \\(\\gamma\\) はドリフトと呼んではいけない． \\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] を満たす場合のみ， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_0|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right)\n\\] と表示でき，この際の \\(\\gamma_0\\in\\mathbb{R}^d\\) を ドリフト と呼ぶ．\n逆に， \\[\n\\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] が成り立つとき， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_1|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)\\biggl)\\nu(dx)\\right)\n\\] と表示でき，\\(\\gamma_1\\) は \\(f\\) が定める確率分布の平均に一致する．7\n\n\n\nLévy 過程は，\\(A:=A_1,\\nu:=\\nu_1,\\gamma:=\\gamma(1)\\) について， \\[\nA_t=tA,\\quad\\nu_t=t\\nu,\\quad\\gamma_t=t\\gamma\n\\] と表せる場合に当たる．\n\n\n1.3 強度測度との関係\n\\(\\{(A_t,\\nu_t,\\gamma_t)\\}_{t\\in\\mathbb{R}_+}\\) を加法過程の特性量とする．\nこのとき， \\[\n\\widetilde{\\nu}([0,t]\\times B):=\\nu_t(B),\\qquad t\\ge0,B\\in\\mathcal{B}(\\mathbb{R}^d)\n\\] は \\(\\mathbb{R}_+\\times\\mathbb{R}^d\\) 上に測度を定める．\n\n\n\n\n\n\n命題（強度測度と特性測度の関係）8\n\n\n\n測度の族 \\(\\{\\nu_t\\}\\subset\\mathbb{M}(\\mathbb{R}^d)\\) と測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}_+\\times\\mathbb{R}^d)\\) について，次は同値：\n\n\\(\\widetilde{\\nu}\\) は次の２条件を満たす： \\[\n\\widetilde{\\nu}(\\{t\\}\\times\\mathbb{R}^d)=0,\n\\] \\[\n\\int_{[0,t]\\times\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\widetilde{\\nu}(dsdx)&lt;\\infty.\n\\]\n\\(\\{\\nu_t\\}\\) はある加法過程の特性測度である．\n\n\n\nよって，任意の加法過程について， \\[\n\\int_{\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\nu_t(dx)&lt;\\infty\n\\] が必要である．\nLévy 過程であるとき，定常増分であることが必要であるため，跳躍時刻は \\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程に従う必要がある．これより， \\[\n\\widetilde{\\nu}=\\ell_+\\otimes\\nu\n\\] と分解できる必要があり，この特性測度 \\(\\nu\\) が Lévy 測度である．このとき，\\(\\nu_t=t\\nu\\) かつ \\(\\widetilde{\\nu}(dsdx)=ds\\nu(dx)\\)．\n\n\n1.4 一般の分解\n\n\n\n\n\n\n定理 (Ito, 1941)9\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を特性量 \\(\\{(A_t,\\nu_t,\\gamma(t))\\}\\) を持つ加法過程とする． \\[\n\\eta(\\omega,B):=\\#\\left\\{t\\in\\mathbb{R}_+\\,\\middle|\\,\\begin{pmatrix}t\\\\X_t(\\omega)-X_{t-}(\\omega)\\end{pmatrix}\\in B\\right\\}\n\\] を \\(\\omega\\in\\Omega_0\\) 上で \\(B\\in\\mathcal{B}(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\})\\) に関して定める．\n\n\\(\\eta\\) は \\(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\}\\) 上の強度測度 \\(\\widetilde{\\nu}\\) を持った Poisson 点過程である．\nある充満集合 \\(\\Omega_1\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\\begin{align*}\n     X^1_t(\\omega)&:=\\lim_{\\epsilon\\searrow0}\\int_0^t\\int_{\\left\\{\\epsilon&lt;\\lvert x\\rvert\\le 1\\right\\}}x\\,\\widetilde{\\nu}(\\omega,dsdx)\\\\\n     &\\qquad+\\int_0^t\\int_{\\mathbb{R}^d\\setminus B^d}x\\,\\eta(\\omega,dsdx)\n   \\end{align*}\\] 収束は \\(t\\in\\mathbb{R}_+\\) に関して広義一様であり，\\(X^1\\) は特性量 \\(\\{(0,\\nu_t,0)\\}_{t\\in\\mathbb{R}_+}\\) が定める加法過程である．\n\\(X^2_t:=X_t-X_t^1\\) は殆ど確実に連続な見本道を持ち，特性量 \\(\\{(A_t,0,\\gamma(t))\\}\\) が定める加法過程である．\n\\(X^1\\perp\\!\\!\\!\\perp X^2\\) が成り立つ．\n\n\n\n\n\n1.5 B 型の場合\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] を満たす場合，Poisson 補過程によらない，より簡潔な表示を持つ．\n\n\n\n\n\n\n定理\n\n\n\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] が成り立つ場合，次が成り立つ：\n\nある充満集合 \\(\\Omega_3\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\n   X^3_t(\\omega):=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}x\\,\\eta(\\omega,dsdx).\n   \\] このとき，\\(X_t^3\\) の分布は複合 Poisson である： \\[\n   \\operatorname{E}[e^{i(z|X_t^3)}]=\\exp\\left(\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu_t(dx)\\right).\n   \\]\n\\(X^4_t:=X_t-X_t^3\\) は殆ど確実に連続な見本道を持ち，Gauss 過程を定める： \\[\n   \\operatorname{E}[e^{i(z|X_t^4)}]=\\exp\\left(-\\frac{1}{2}(z|A_tz)+i(\\gamma_0(t)|z)\\right).\n   \\]\n\\(X^3\\perp\\!\\!\\!\\perp X^4\\) が成り立つ．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-測度",
    "href": "posts/2024/Process/Levy.html#lévy-測度",
    "title": "Lévy 過程を見てみよう",
    "section": "2 Lévy 測度",
    "text": "2 Lévy 測度\n\n2.1 導入\n本節の目的は，Lévy 過程の次の３分類の見本道の違いを理解することである：10\n\n\n\n\n\n\n特性量 \\((A,\\nu,\\gamma)\\) を持つ Lévy 過程について，\n\nA 型：\\(A=0\\) かつ \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\nB 型：\\(A=0\\) かつ \\(\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\\) であるが，A 型ではない．\nC 型：それ以外．\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) 型は拡散項を持たず，確定的な動きと複合 Poisson 過程の和で表現される．ジャンプは離散的に起こる．\n\\(B\\) 型も拡散項を持たないが，\\(\\mathbb{R}_+\\) 上稠密な可算集合上でジャンプを繰り返す．Gamma 過程（第 3.6 節）がその例である．\n\\(A,B\\) は殆ど確実に任意の有界区間上で有界変動な見本道を持つが，\\(C\\) 型は有界変動ではない．11 Brown 運動と Cauchy 過程（第 4.4 節）がその例である．\n\n\n\n\n\n\n2.2 Lévy 測度が零ならば，Gauss 過程である\n\n\n\n\n\n\n命題（連続な Lévy 過程の特徴付け）12\n\n\n\nLévy 過程 \\(X\\) について，次の２条件は同値：\n\n\\(X\\) は殆ど確実に連続な見本道を持つ．\n\\(\\nu=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n時刻 \\(t&gt;0\\) までの跳躍回数を表す Poisson 過程 \\[\nN_t:=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}\\eta(dsdx)\\in[0,\\infty]\n\\] を考えると， \\[\n\\operatorname{E}[N_t]=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}ds\\nu(dx)=0.\n\\] すなわち，\\(N_t=0\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n2.3 区分定数ならば，A 型である．\n\n\n\n\n\n\n命題（A 型の見本道の特徴付け）13\n\n\n\nLévy 過程 \\(X\\) ついて，次の３条件は同値：\n\n\\(X\\) の見本道は，殆ど確実に区分的定数であり，有界区間上では有限回のジャンプしか起こらない．\n\\(X\\) は複合 Poisson 分布であるか，零であるかのいずれかである．\n\\(X\\) は A 型で，かつ \\(\\gamma_0=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n1 \\(\\Rightarrow\\) 2\n任意の有限区間内でのジャンプ回数は有限回であるため，ジャンプ回数の Poisson 過程 \\(N\\) について，\\(N_t\\sim\\mathrm{Pois}(t\\nu(\\mathbb{R}^d))\\) の母数は有限である必要がある．特に \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\\(X\\) に連続部分がないことを併せると，定理 1.5 より， \\[\n\\operatorname{E}[e^{i(z|X_t)}]=\\exp\\left(t\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right).\n\\] これは \\(\\mathrm{CP}(t,\\nu)\\) の特性関数である．\n2 \\(\\Rightarrow\\) 1\nこちらは省略する．\n\n\n\n\n純粋跳躍確率過程であっても，B 型ならば，見本道は区分的定数にはならない．Gamma 過程（第 3.6 節）がその例である．\n\n\n2.4 B 型の跳躍時刻\nLévy 過程の見本道は右連続であるから，\\(\\mathbb{R}_+\\) 上トータルの跳躍回数は殆ど確実に可算回である．\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) の場合は，有限区間上での跳躍回数も無限になる．\nさらに，次のことが言える：\n\n\n\n\n\n\n命題（B 型 Lévy 過程のジャンプ時刻）14\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) とする．このとき，跳躍時刻は殆ど確実に \\(\\mathbb{R}_+\\) 上稠密である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) のとき， \\[\nT_\\epsilon(\\omega):=\\inf\\left\\{t\\ge 0\\mid\\lvert X_t(\\omega)-X_{t-}(\\omega)\\rvert&gt;\\epsilon\\right\\}\n\\] とすると， \\[\n\\lim_{\\epsilon\\searrow0}\\operatorname{P}[T_\\epsilon\\le t]=1.\n\\] よって， \\[\n\\lim_{\\epsilon\\searrow0}T_\\epsilon=0\\;\\;\\text{a.s.}\n\\] これは，ある充満集合 \\(\\Omega_0\\subset\\Omega\\) の上で，\\(0\\) が \\(X\\) の跳躍時刻の触点になることを含意している．\nこれと同様の議論を任意の \\(s\\in\\mathbb{Q}\\cap\\mathbb{R}_+\\) について繰り返すことで，ある充満集合 \\[\n\\bigcap_{s\\in\\mathbb{Q}\\cap\\mathbb{R}_+}\\Omega_s\n\\] 上で，\\(X\\) の跳躍時刻の閉包が \\(\\mathbb{R}_+\\) 上で稠密になることがわかる．\n\n\n\n\n\n2.5 従属過程ならば B 型である\n\\(d=1\\) で，殆ど確実に単調増加な見本道を持つ Lévy 過程を 従属過程 (subordinator) という．15\n\n\n\n\n\n\n命題（単調増加性の特徴付け）16\n\n\n\n\\(d=1\\) とし，\\(X\\) を Lévy 過程とする．このとき，次は同値：\n\n\\(X\\) は従属過程である．\n\\(A=0,\\nu((-\\infty,0))=0\\) かつ \\[\n\\int_{[0,1)}x\\,\\nu(dx)&lt;\\infty\n\\] 加えて \\(\\gamma_0\\ge0\\) である．\n\n\n\n仮に \\(A=0,\\nu((-\\infty,0))=0\\) だが， \\[\n\\int_0^1x\\,\\nu(dx)=\\infty\n\\] であったとする．\nこのとき，正なジャンプとドリフトしか持たないはずであるから，場合によっては単調増加過程になっても良さそうなものである．\nしかし，このような過程が発散せずに well-defined であるということは，負の方向に無限に強いドリフトを持っており，これが正なジャンプを打ち消していることが必要である．\nそれ故，ジャンプの隙間では負方向のドリフトが競り勝ち，全体としては単調増加にならない．特に，任意の区間において単調増加にならない．17\n\n\n2.6 C 型ならば非有界変動である\n\n\n\n\n\n\n命題（見本道の変動）18\n\n\n\nLevy 過程 \\(X\\) について，\n\nA 型または B 型ならば，有界変動過程である．すなわち，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動である．\nC 型ならば，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "href": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "3 従属過程と Gamma 過程",
    "text": "3 従属過程と Gamma 過程\n\n3.1 導入\nGamma 過程は，拡散項もドリフト \\(\\gamma_0\\) も持たない，純粋跳躍な従属過程である．\nしかし，正のジャンプのみをもち，ジャンプだけで増加していく過程だからと言って，その見本道は区分的に定数ではない．\nその Lévy 測度は \\(\\nu((0,\\infty])=\\infty\\) を満たし，B 型に分類される．従って，\\(\\mathbb{R}_+\\) の稠密部分集合上でジャンプしており，見本道は殆ど確実に，任意の点 \\(t\\in\\mathbb{R}_+\\) で非連続である．\nGamma 過程は元々，(Moran, 1959) によりダムの貯水量のモデルとして導入された．\n\n\n\n\n\n\n証明\n\n\n\n\n\n見本道 \\(X_\\bullet(\\omega)\\) は，\\(\\mathbb{R}_+\\) のある稠密部分集合 \\(A\\subset\\mathbb{R}_+\\) 上でジャンプしているとする：\\(\\overline{A}=\\mathbb{R}_+\\)．\nこのとき，\\(\\mathbb{R}_+\\) の任意の点で \\(X_\\bullet(\\omega)\\) は非連続である．\n実際，任意の \\(t&gt;0\\) を取り，ここで連続であるとすると，任意の \\(t\\) への収束列 \\(\\{t_n\\}\\subset\\mathbb{R}_+\\) について，\\(X_{t_n}(\\omega)\\to X_t(\\omega)\\) が成り立つ必要があるが，\\(t\\) は \\(A\\) の触点でもあるので，これに収束する \\(A\\) の点列 \\(\\{t_n\\}\\subset A\\) が取れる．これを特に，下から単調に収束するように取る：\\(t_n\\searrow t\\)．\n\n\n\nしかし，\\(\\nu\\) は平均を持つために有界変動ではあり，実際シミュレーションによって得る見本道を見ても，殆どのジャンプは目に見えない．\n\n\n3.2 Gamma 分布\n\\(\\mathbb{R}\\) 上の Gamma 分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\) とは，密度関数 \\[\ng(x;\\alpha,\\nu):=\\frac{\\alpha^\\nu}{\\Gamma(\\nu)}x^{\\nu-1}e^{-\\alpha x}1_{\\mathbb{R}^+}(x)\n\\] が定める分布をいう．\\(\\alpha\\) をレート，\\(\\nu\\) を形状パラメータというのであった．\n\n  \n    \n      \n      \n        確率測度の変換則\n        Gamma 分布と Beta 分布を例に\n      \n    \n  \n\n\n\n3.3 Gamma 点過程\n\\(\\sigma\\)-有限測度 \\(\\rho_0\\in\\mathcal{P}(E)\\) と Lévy 測度 \\(\\nu:=\\mathrm{Gamma}(\\alpha,0)\\)，すなわち \\[\n\\nu(dr):=\\frac{e^{-\\alpha r}}{r}1_{\\mathbb{R}^+}(r)\\,dr\n\\] について，\\(\\lambda:=\\rho_0\\otimes\\nu\\) で定まる強度測度を持つ \\(E\\times\\mathbb{R}_+\\) 上の Poisson 点過程 \\(\\xi\\) を Gamma 点過程 という．19\nこれは \\[\n\\xi(B)\\sim\\mathrm{Gamma}(\\alpha,\\rho_0(B))\n\\] を満たす複合 Poisson 点過程である．\\(\\rho_0\\) のことを形状測度ともいう．\n\n\n\n\n\n\nDirichlet 過程 (Ferguson, 1973) との関係20\n\n\n\n\n\n\\[\n\\Delta_n:=\\left\\{\\begin{pmatrix}p_1\\\\\\vdots\\\\p_n\\end{pmatrix}\\in[0,1]^n\\,\\middle|\\,\\sum_{i=1}^np_i=1\\right\\}\n\\] を \\(n-1\\)-単体とする．21 この上に台を持つ，パラメータ \\(\\alpha\\in(0,\\infty)^n\\) で定まる密度 \\[\nf(x)=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_n)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_n)}x_1^{\\alpha_1-1}\\cdots x_n^{\\alpha_n-1}1_{\\Delta_n}(x)\n\\] が定める分布 \\(\\mathrm{Dirichlet}(n,\\alpha)\\in\\mathcal{P}(\\Delta_n)\\) を Dirichlet 分布 という．\nここで，\\(E\\) 上の Gamma 点過程 \\(\\xi\\) は \\(\\rho_0(E)&lt;\\infty\\) を満たすとする．このとき，\\(E\\) の分割 \\[\nE=B_1\\sqcup\\cdots\\sqcup B_n\n\\] \\[\n\\rho_0(B_i)&gt;0\n\\] に対して， \\[\n(\\zeta(B_1),\\cdots,\\zeta(B_n))\\sim\\mathrm{Dirichlet}(n,\\alpha)\n\\] \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] が成り立ち，これは \\(\\xi(E)\\) と独立である．\nこのことをふまえて，\\(\\rho_0\\) が有限であるとき，ランダム確率測度 \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] を Dirichlet 過程 という．22\n\n\n\n\n\n3.4 Gamma 点過程の Lévy 測度は \\(0\\) の近傍で発散する\nしかし，\\(\\mathrm{Gamma}(\\alpha,0)\\) などという分布はなく， \\[\n\\nu(\\mathbb{R})=\\int^\\infty_0r^{-1}e^{-\\alpha r}dr=\\infty.\n\\]\nこのとき，任意の \\(\\rho_0\\) で測って正の測度を持つ集合 \\(\\rho_0(B)&gt;0\\;(B\\in\\mathcal{E})\\) に対して，\\(\\xi\\) は殆ど確実に無限個の点を \\(B\\) 内にもつ．23\nしかし，\\(\\rho_0(B)&lt;\\infty\\) ならば \\(\\xi(B)&lt;\\infty\\) ではある．すなわち，ジャンプ幅も含めて足し合わせると，収束する．これは，\\(\\nu\\) が平均を持つことによる： \\[\n\\int_0^\\infty r\\,\\nu(dr)=\\alpha^{-1}.\n\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n  \\xi(B)&=\\int^\\infty_0\\int_Br\\,\\eta(dsdr)\\\\\n  &=\\int_B\\rho_0(ds)\\int^\\infty_0r\\,\\nu(dr)\\\\\n  &=\\rho_0(B)\\alpha^{-1}\n\\end{align*}\\]\n\n\n\n\n\n3.5 従属過程\n一般に，\\(\\xi\\) を \\(\\mathbb{R}^+\\) 上の Lévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を持つ一様な複合 Poisson 点過程，すなわち \\(\\ell_+\\otimes\\nu\\) を強度測度とする \\(\\mathbb{R}_+\\times\\mathbb{R}^+\\) 上の Poisson 点過程とすると， \\[\nY_t(\\omega):=\\xi(\\omega,[0,t])\n\\] で定まる過程 \\(Y\\) は，一般に Lévy 測度 \\(\\nu\\) を持つ 従属過程 (subordinator) という．24\n\n\n3.6 Gamma 計数過程\nLévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を \\[\n\\nu(dr):=\\delta\\frac{e^{-\\gamma r}}{r}dr\n\\] \\[\n\\delta,\\gamma&gt;0,\n\\] で与えたとき，付随する従属過程 \\(\\{Y_t\\}\\) を Gamma 過程 といい，\\(\\mathrm{Gamma}(\\delta,\\gamma)\\) で表す．25\nこれは \\(Y_t\\sim\\mathrm{Gamma}(\\gamma,\\delta t)\\) を満たす Lévy 過程である．\n\n\n\n\n\n\n\n\n\n目視できないジャンプが無数に存在することが窺える．\n\n\n\n3.7 分散 Gamma 過程\n２つの独立な Gamma 過程 \\[\nX^+\\sim\\mathrm{Gamma}(\\delta,\\gamma^-),X^-\\sim\\mathrm{Gamma}(\\delta,\\gamma^+)\n\\] に対して， \\[\nX^0_t=X^+_t-X^-_t\n\\] と表せる Lévy 過程 \\(X^0\\) を 分散 Gamma 過程 という．26\n\n\n\n\n\n\n\n\n\n分散 Gamma 過程は，オプション価格の対数のモデルとして，Brown 運動より柔軟なモデルとしても用いられる (Madan et al., 1998)．\nこれは，Brown 運動の分散が Gamma 分布に従うとして得る過程であるとも見れる．実際，Brown 運動の時間を，Gamma 過程によって変換したものが分散 Gamma 過程である．\n実際，Brown 運動 \\(B\\) とこれと独立な Gamma 過程 \\(T\\) について， \\[\nX^0_t=B_{T_t}\n\\] と表せる．27"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "href": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "4 安定過程と Cauchy 過程",
    "text": "4 安定過程と Cauchy 過程\n\n4.1 安定分布\n\n4.1.1 定義\n\n\n\n\n\n\n定義 (stable)28\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して \\[\nf(t)^n=f(a_nt)e^{ib_nt}\n\\] が成り立つ無限可分分布の特性関数をいう．\n確率変数 \\(Y\\in\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して， \\[\nY_1+\\cdots+Y_n\\overset{\\text{d}}{=}a_nY+b_n\n\\] を満たすことをいう．\n\n\n\nすなわち，安定分布とは， \\[\nZ_n:=\\frac{\\sum_{i=1}^nY_i-b_n}{a_n}\n\\] という形の，独立同分布確率変数の正規化された和の列 \\(\\{Z_n\\}\\) の分布収束極限として現れ得る分布の全体を指すことになる．29\nまた，\\(a_n\\) は \\(a_n=n^{1/\\alpha}\\) という形に限り，この \\(\\alpha\\in(0,2]\\) を 安定指数 という．\n\n\n4.1.2 Lévy 測度の有限性\n安定指数 \\(\\alpha\\in(0,2)\\) を持つ安定分布の Lévy 測度は非有限であり，平均も持たない．\n\n\n\n\n\n\n命題（Lévy 測度の平均）30\n\n\n\n\\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) を \\(\\alpha\\)-安定分布とする．このとき，その Lévy 測度 \\(\\nu\\) について，次は同値：\n\n\\(\\alpha\\in(0,1)\\) である．\n\\(\\nu\\) は \\(B^d\\) 上で平均を持つ： \\[\n  \\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n次も同値：\n\n\\(\\alpha\\in(1,2)\\) である．\n\\(\\nu\\) は \\(\\mathbb{R}^d\\setminus B^d\\) 上で平均を持つ： \\[\n  \\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n\n\n\n\n\n4.2 回転対称な安定分布\n\n4.2.1 特性関数の表示\n安定分布は無限可分であるため，Lévy-Khintchin 分解を通じた特性関数の形が特徴付けられる．\n中でも，（回転）対称な安定分布は特に簡単な表示を持つ：\n\n\n\n\n\n\n命題 (Lévy-Khinchin 表示)31\n\n\n\n\\(P\\in\\mathcal{P}(\\mathbb{R}^d)\\) は回転対称であるとする．このとき，その特性関数 \\(\\varphi\\) について次は同値：\n\n\\(\\varphi\\) は安定である．\nある \\(c&gt;0\\) と \\(\\alpha\\in(0,2]\\) が存在して， \\[\n\\varphi(u)=e^{-c\\lvert u\\rvert^\\alpha}.\n\\]\n\nこの \\(\\alpha\\) を 安定指数 という．\n\n\n\n\n\n\n\n\n例（対称な安定分布）\n\n\n\n\n\\(\\alpha=2\\) の対称安定分布とは，中心化された正規分布である．\n\\(\\alpha=1\\) の対称安定分布とは，中心化された Cauchy 分布である．\n\n\n\n\n\n\n\n\n\n中心極限定理のスケーリングレートとしての安定指数\n\n\n\n\n\n\\(a_n\\) は従って，中心極限定理を実現するために必要なスケーリングレートを表す．\nこのことは，一般のエルゴード的な定常過程に対して一般化できる：32\n\\(\\{X_n\\}\\) を \\(\\alpha\\)-撹拌的な定常過程，\\(\\{a_n\\}\\subset\\mathbb{R}^+\\) を発散列とし， \\[\n\\frac{1}{a_n}\\sum_{j=1}^nX_j-b_n\n\\] は弱収束するとする．この極限分布は安定分布になり，安定指数を \\(\\alpha\\) とする．\nこのとき，(Karamata, 1933) の意味で緩変動な関数 \\(h\\) に対して，\\(a_n=n^{1/\\alpha}h(n)\\) と表せる： \\[\n\\lim_{n\\to\\infty}\\frac{h(tn)}{h(n)}=1\\quad(t&gt;0).\n\\]\n\n\n\n\n\n4.2.2 自己相似性\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) は自己相似性を持つ．\n一般に，Hurst 指数 \\(H&gt;0\\) に関して自己相似的 (self-similar) であるとは，任意の \\(a&gt;0\\) について \\[\n(Y_{at})\\overset{\\text{d}}{=}(a^HY_t)\n\\] を満たすことをいう．\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) については，\\(H=\\alpha^{-1}\\) と取れる．\nBrown 運動は \\(H=1/2\\) について自己相似である．\nまた，自己相似な Lévy 過程は，狭義の安定過程に限る．33\n\n\n\n4.3 安定従属過程\n\\(\\alpha\\in(0,1)\\) の安定指数を持つ安定過程は，従属過程になる．34\n\n\n\n\n\n\n例（Lévy 従属過程）35\n\n\n\n\n\nLévy 分布 \\(\\mathrm{Levy}(c):=\\mathrm{IG}(c^{1/2},0)\\) とは，密度 \\[\nf(x;c):=\\sqrt{\\frac{c}{2\\pi}}x^{-\\frac{3}{2}}e^{-\\frac{c}{2x}}1_{\\mathbb{R}^+}(x)\n\\] を持つ \\(\\mathbb{R}\\) 上の分布をいう．\nこれは次の特性関数を持ち，安定指数 \\(\\alpha=1/2\\) を持つ非対称な安定分布である： \\[\n\\varphi(u)=\\exp\\left(-\\sqrt{c\\lvert u\\rvert}\\biggr(1-i\\operatorname{sgn}(u)\\biggl)\\right).\n\\]\n安定指数 \\(1/2\\) の安定従属過程 \\(T\\) は Lévy 従属過程 とも呼ばれ， \\[\nT_t\\sim\\mathrm{Levy}(t^2/2)\n\\] を満たす．\nこれは，１次元 Brown 運動の到達時刻 \\[\nT_t:=\\inf\\left\\{s&gt;0\\mid B_s=\\frac{t}{\\sqrt{2}}\\right\\}\n\\] の過程として現れる．\nLévy 過程は逆正規過程の特殊な場合であり，これは一般の Gauss 過程の到達時刻の過程として現れる．36\n\n\n\n\n\n\n\n\n\n例（安定従属過程による従属操作）37\n\n\n\n\n\n\\(\\{\\tau_t\\}_{t\\in\\mathbb{R}_+}\\) を安定指数 \\(\\alpha\\in(0,1)\\) を持つ安定従属過程とする．\nこれと独立な Lévy 過程 \\(X\\) に対して，従属化 \\[\nt\\mapsto X_{\\tau_t}\n\\] は再び Lévy 過程である．\n特に，\\(X\\) を Brown 運動 \\(B\\) とすると，\\(B_{\\tau}\\) は安定指数 \\(2\\alpha\\) を持つ安定過程になる．\n例えば \\(\\tau_a\\) として \\[\nT_a:=\\inf\\left\\{t\\in\\mathbb{R}_+\\mid B_t=a\\right\\}\n\\] と取ると，これは安定指数 \\(1/2\\) を持つ安定従属過程（Lévy 従属過程）の修正である．38\nこれより，各 \\(a\\in\\mathbb{R}_+\\) への到達時刻で止めた Brown 運動の過程 \\(a\\mapsto B_{T_{a+}}\\) は対称な Cauchy 過程になる．\n\n\n\n\n\n4.4 Cauchy 過程\nCauchy 過程は安定指数 \\(\\alpha=1\\) を持つ狭義の対称安定過程である．39\n拡散項を持たないが，Lévy 測度は平均を持たず（命題 4.1.2），C 型の Lévy 過程である．\nすなわち，殆ど確実に，任意の区間上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#footnotes",
    "href": "posts/2024/Process/Levy.html#footnotes",
    "title": "Lévy 過程を見てみよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Nualart and Nualart, 2018, p. 158) 定義9.1.1，(Sato, 2013, p. 3) 定義1.6，(Rocha-Arteaga and Sato, 2019, pp. 12–13) 定義1.31に倣った．(Protter, 2005, p. 20) では(2)の\\(D\\)-過程という部分がないのみで，定理30 (Protter, 2005, p. 21) で常に\\(D\\)-修正が取れることを示している．(Le Gall, 2016, p. 175) 6.5.2節も同様の取り扱いである．(伊藤清, 1991, p. 306) は時間的一様性を所与のものとはせず，(1), (2), (3), (5)のみをLévy過程の定義としており，さらに(4)も満たすものを 一様Lévy過程 という．(Baudoin, 2014, pp. 89–90) 定義3.40では(5)がない．(Böttcher et al., 2013, p. 14) 例1.17では(1),(2)がない．(Osswald, 2012, pp. 258–259) は(1), (3), (4)を定義としている．(Applebaum, 2009, p. 43) は(1), (3), (4), (5)を定義としている．(佐藤健一, 1990) では全く同じものを加法過程と呼ぶが，(佐藤健一, 2011) は完全に一致する語用法をする（加法過程に確率連続性を課している点を除く）．↩︎\n(Sato, 2013, p. 47) 定理9.1 参照．↩︎\n(Sato, 2013, p. 51) 定理9.7参照．↩︎\n(Dudley, 2002, p. 327) 定理9.8.3，(Sato, 2013, p. 37) 定理8.1，(Rocha-Arteaga and Sato, 2019, p. 11) 定理1.28，(Baudoin, 2014, p. 91) 定理3.46，(Applebaum, 2009, p. 29) 定理1.2.14 など参照．↩︎\nGauss 共分散の用語は (Sato, 2013, p. 38) 定義8.2．Khintchine 測度は (Loéve, 1977, p. 343)，(Applebaum, 2009, p. 31)，(Böttcher et al., 2013, p. 33)，(Baudoin, 2014, p. 92) など．↩︎\n(Loéve, 1977, p. 343)↩︎\n(Sato, 2013, p. 39) 注8.4．↩︎\n特性測度の名前は (Revuz and Yor, 1999, p. 478) 演習 XII.1.18 など．命題は (Sato, 2013, p. 53) 注9.9も参照．↩︎\n(Sato, 2013, p. 120) 定理19.2より．(Protter, 2005, p. 31) 定理42 は Lévy 過程に限って示している．(1)は (伊藤清, 1991, p. 313) 補題5.3でも解説されている．(Protter, 2005, p. 26)定理35も参照．↩︎\nこの分類は (Sato, 2013, p. 65) 定義11.9に倣った．↩︎\n(Sato, 2013, p. 140) 定理21.9 参照．↩︎\n(Sato, 2013, p. 135) 定理21.1．↩︎\n(Lowther, 2011) 定理１，(Sato, 2013, p. 135) 定理21.2．↩︎\n(Sato, 2013, p. 136) 定理21.3．↩︎\n(Applebaum, 2009, p. 52)，(Baudoin, 2014, p. 95) 定義3.50，(Sato, 2013, p. 137) 定義21.4，(Iacus and Yoshida, 2018, p. 171) に倣った．(Kingman, 1992, p. 88) 8.4節，(Last and Penrose, 2017, p. 156) 例15.7 は命題の条件2の方を定義に用いている．↩︎\n(Sato, 2013, p. 137) 定理21.5．↩︎\n(Sato, 2013, p. 138) も参照．↩︎\n(Lowther, 2011) 定理２，(Sato, 2013, p. 140) 定理21.9．↩︎\n定義は (Last and Penrose, 2017, p. 155) 例15.6 に倣った．↩︎\n(Ghosal and van der Vaart, 2017, p. 562) 命題G.2.(i)，(Last and Penrose, 2017, p. 163) 演習15.1，(Kingman, 1992, pp. 92–) 9.2節．↩︎\n\\(n=2\\) を取ると１単体（線分），\\(n=3\\) と取ると２単体（三角形）を得る．↩︎\n(Kingman, 1992, p. 93)，(Ghosal and van der Vaart, 2017, p. 59) 定義4.1．↩︎\n\\(\\lambda(B)=\\rho_0(B)\\nu(\\mathbb{R})=\\infty\\) となるためである．(Last and Penrose, 2017, p. 163) 演習15.2も参照．↩︎\n(Kingman, 1992, p. 88) 8.4節，(Last and Penrose, 2017, p. 156) 例15.7 などの用語法．一般に subordinator とは，単調増加な Lévy 過程をいう (Sato, 2013, p. 137) 定義21.4，(Baudoin, 2014, p. 95) 定義3.50，(Iacus and Yoshida, 2018, p. 171)．これは，時間変数に関する変数変換を subordination と呼び，その際の変数変換に使えるためである．↩︎\n記法は (Iacus and Yoshida, 2018) による．(Applebaum, 2009, pp. 54–55) 例1.3.22，(Protter, 2005, p. 33) 例４も参照．↩︎\n(Iacus and Yoshida, 2018, p. 160) に倣った．↩︎\n(Lowther, 2011)，(Applebaum, 2009, p. 59) 例1.3.31 も参照．↩︎\n(Revuz and Yor, 1999, p. 116) 定義III.4.1，(Sato, 2013, p. 69) 定義13.1，(Shiryaev, 2016, p. 416) 定義3.6.2，(Loéve, 1977, p. 338)．↩︎\n(Shiryaev, 2016, p. 416) 定理3.6.3 も参照．↩︎\n(Sato, 2013, p. 80) 命題14.5．↩︎\n(Sato, 2013, p. 86) 定理14.14．(Shiryaev, 2016, p. 419) 定理3.6.4，(Loéve, 1977, p. 339)，(Dudley, 2002, p. 328) 定理9.8.4 は \\(d=1\\) の場合．↩︎\n(Ibragimov and Linnik, 1971, p. 316) 定理18.1.1 も参照．↩︎\n狭義の安定過程とは，\\(b_n\\equiv0\\) と取れることをいう (Sato, 2013, p. 69) 定義13.1．(Embrechts and Maejima, 2002)，(Applebaum, 2009, p. 51) 例1.3.14 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Sato, 2013, p. 138) 例21.7，(Applebaum, 2009, p. 53) 例1.3.18 も参照．↩︎\n(Applebaum, 2009 @/53) 例1.3.19 も参照．↩︎\n(Applebaum, 2009, p. 54) 例1.3.21 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Rogers and Williams, 2000, p. 133) も参照．↩︎\n(Revuz and Yor, 1999, p. 107) 命題III.3.9 も参照．↩︎\n(Sato, 2013, p. 87) 例14.17．↩︎"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html",
    "href": "posts/2023/Lifestyle/QuartoBasics.html",
    "title": "Quarto はじめて良かったこと",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#使い方の概要",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#使い方の概要",
    "title": "Quarto はじめて良かったこと",
    "section": "1 使い方の概要",
    "text": "1 使い方の概要\n\n1.1 はじめに\n本サイトは Quarto と，GitHub Actions によってホスティングされている．\n\n\n\n\n\n\n\nLévy 過程を見てみよう など，コードと数式を併せて書いている Jupyter Notebook のようなページ\nCV など，HTML と PDF の両方で見れるページ\nZig-Zag サンプラー など，HTML とスライド (reveal.js) の両方で見れるページ\n本ページなど，HTML とスライド (pptx)，typst PDF と LaTeX PDF と reveal.js のさまざまで見れるページ\n\n\n\n\n\n\n\n\n\n\n注\n\n\n\n\n\nスマホでは別フォーマットのページのリンクは表示されないようである．\n\n\n\n\n\nCode\nusing Plots\n\np = plot(sin, \n     x-&gt;sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1: Parametric Plots\n\n\n\n\nQuarto ではこのような数式・コードが共存するドキュメントが，極めて簡単に＋凡ゆるフォーマットで作成できる．\n計算統計の研究をしている筆者にとっては何より，数式とコードが自然に共存する PDF を簡単に書けること，そして自学のためのノートがそのまま HTML としてブログの形で公開できることが，大変嬉しかった．\n特に VSCode の拡張機能と組み合わせれば，RStudio のような隙のない統合開発環境が得られる．1\n基本的な仕組みとして，自分で作成するのは .qmdファイルのみである．\nその後はquarto renderコマンドにより，\n\n\n\n\n\n\n\nコードブロックは Jupyter によって処理され，\n全体は markdown に変換され，\nPandoc によってpdf, html, word など好きな形式に最終出力できる．\n\n\n\n\n拡張機能をオンにした VSCode ではRun Cellボタンもあるので，ノートブック全体を毎度ビルドせずとも，コードブロックごとに実行して結果を見ることもできる．\nCtrl+Enter で１行ごとに実行できる操作感は RStudio と同じである．\n\n\n1.2 美点\n\n\n\n\n\n\n\nレンダリングがとんでもなく速い．体感で TeX の10分の1である．\nそれでいて数式とコードブロックを併在させることが出来る．なお，明かに TeX を意識していることがわかる使用感になっているし，本の作成も可能としている．\n（ちょっと使いにくい）ブラウザ上ではなく，好きなエディタで動く．Jupyter Notebook が続かない筆者にとって，この点は肝要である．\n私用の勉強ノートとしても使えると同時に，内容そのままブログとして公開できる．\nプレゼンテーションの作成にも使える．\nすごい細かいが，例えば project type を website としたリポジトリでquarto renderをしても，不要なファイルが自動で削除される．このような点がライトユーザーでもとにかく使いやすい．\nさらにインタラクティブな機能を実現したブログを作ってみたい．\n\n\n\n\n\n\n1.3 YAML Header\n各ファイルの冒頭に YAML block を用意することで，ノートブックの詳細を調整できる（参照：HTML Options）．\n例えば本ページでは次のとおり：\n---\ntitle: \"Quarto はじめて良かった\"\nauthor: \"司馬博文\"\ndate: \"11/4/2023\"\ndate-modified: \"7/7/2024\"\ncategories: [Lifestyle]\nabstract: Quarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．\nabstract-title: 概要\nformat:\n  html:\n    mainfont: \"Gill Sans\"\n    theme: minty\n    css: assets/styles.css\n    toc: true\n    number-sections: true\n    highlight-style: ayu\n    code-block-border-left: \"#7CC4AC\"\n    code-overflow: scroll\n    toc-title: \"目次\"\n    abstract-title: \"概要\"\n---\n\n\n1.4 本文の書き方\n\n1.4.1 数式\n本文は markdown 記法で書く．数式も使える：\n\\[\n\\operatorname{P}[\\lvert\\xi\\rvert&lt;t]\\le2e^{-\\frac{t^2}{2\\sigma^2}},\\qquad t&gt;0.\n\\]\n\n\n1.4.2 コード\nまた，コードブロックにもコメントアウトと接頭辞の組み合わせ #| を前につけることでYAMLで指示が出せる（参照：指示のリスト）．上のコードブロックには\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\nと追加されているために，出力された図にラベリングとキャプションが付いているのである．\npip3 install jupyter-cache\nが必要であることに注意．\n\n\n\n1.5 カーネルの選択\n&gt; python3 -m venv GenAI\n\n&gt; source GenAI/bin/activate\nにより仮想環境を作成して入れるが，この環境を Jupyter notebook で使うにはもう一手間必要である．\n&gt; pip install ipykernel\n\n&gt; python -m ipykernel install --user --name=GenAI\nすると\njupyter kernelspec list\nにより見つかるようになっている．YAML header で jupyter: genai と指定すれば良い．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#website-の作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#website-の作り方",
    "title": "Quarto はじめて良かったこと",
    "section": "2 Website の作り方",
    "text": "2 Website の作り方\n公式 Guide を参考．\n\n2.1 Source Branchをmainと別ける\nまずgh-pagesという全く新しいブランチを作成する．既存のリポジトリのコミット履歴とは独立している新しいブランチを作るときは--orphanオプションが利用される．\n\n\nTerminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\ngit checkout main\n\n基本gh-pagesブランチには自分では立ち入らない．\n\n\n2.2 Publishコマンドによるサイトの公開\nmainブランチにいることを確認して，\n\n\nTerminal\n\nquarto publish gh-pages\n\nを実行．\nGitHubの方の設定Settings: Pagesで，Sourceをgh-pagesブランチの/(root)にしていることを確認すれば，これで無事サイトが公開されていることが確認できる．\n\n\n2.3 GitHub Action の使用\nさらに，ローカル上でrenderするのではなく，コミットする度にGitHub上でレンダリングしてもらえるように自動化することもできる．こうするとスマホからも自分のサイトが更新できる．\nまず，GitHubの設定のActionsセクションのWorkflow permissionsから，読み書きの権限をGitHub Actionに付与する．\n続いて，次の内容のファイルを.github/workflows/publish.ymlに書き込む：\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          tinytex: true  # https://github.com/quarto-dev/quarto-actions/tree/main/setup\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Setting GH_TOKEN is recommended as installing TinyTeX will query the github API.\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          # render: false  # https://quarto.org/docs/publishing/github-pages.html#additional-options\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n途中，tinytex: true とすることで，１つのページを HTML と pdf の両方で閲覧可能になる．本ブログでは，CV のページ でこの機能を使っている．\nこれで，mainブランチにコミットする度に，GitHub上でrenderが実行されることとなる．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#pdf-の作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#pdf-の作り方",
    "title": "Quarto はじめて良かったこと",
    "section": "3 PDF の作り方",
    "text": "3 PDF の作り方\n\n3.1 LuaLaTeX を使う方法\nLuaLaTeX を利用することで日本語を含んだ PDF を作成できる．\n\n\nreport.qmd\n\ntitle: \"タイトル\"\nauthor: 司馬博文\ndate: 2023/12/11\nformat:\n  pdf:\n    toc: true\n    number-sections: true\n    urlcolor: minty\n    template-partials: \n      - ../../../assets/before-title.tex\n    keep-tex: true\n    block-headings: false\n    pdf-engine: lualatex\n    documentclass: ltjsarticle\n\n\n3.1.1 LuaLaTeX の注意\n\\int_{\\mathbb{R}}\nのような記法は，pdfLaTeX ではなぜかコンパイルが通るが，LuaLaTeX （や殆どの pdfLaTeX 以外のエンジン）ではエラーになる．\n\n\n3.1.2 LuaLaTeX の欠点\nltjsarticle クラスでは\nFont \\JY3/mc/m/n/10=file:HaranoAjiMincho-Regular.otf:-kern;jfm=ujis at 9.24713pt not loadable: metric data not found or bad.\n&lt;to be read again&gt; \nrelax \nl.79 \\kanjiencoding{JY3}\\selectfont\n                                 \\adjustbaseline\nというエラーが．一方で，bxjsarticle クラスでは\nLaTeX Error: File `haranoaji.sty' not found.\n\nType X to quit or &lt;RETURN&gt; to proceed,\nor enter new name. (Default extension: sty)\n\nEnter file name: \n! Emergency stop.\n&lt;read *&gt;\nというエラーが出る．\nローカルではインストールすれば良いだけであるが，これを GitHub Actions 上で実現する方法を考えあぐねていた．\n\n\n\n\n\n\n注（TeX Live のアップデート方法）\n\n\n\n\n\n年度を跨いだ TeX Live manager のアップデートは，次のようにする必要がある：\nwget http://mirror.ctan.org/systems/texlive/tlnet/update-tlmgr-latest.sh\nchmod +x update-tlmgr-latest.sh\nsudo ./update-tlmgr-latest.sh\n\n\n\n\n\n3.1.3 LuaLaTeX と日本語フォント\nなぜか\n\\usepackage[haranoaji,nfssonly]{luatexja-preset}\nで変わるのは英語文字だけである．\n\n\n3.1.4 GitHub Actions の修正\n次のようにして，Set up Quarto と Render and Publish の間に，TinyTeX と haranoaji.sty のインストールを使いすることで，GitHub 上でもレンダリングが可能になる．\n\n\npublish.yml\n\n- name: 'Install TinyTeX'  # https://github.com/quarto-dev/quarto-actions/tree/main/setup\n  env:\n    QUARTO_PRINT_STACK: true\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Setting GH_TOKEN is recommended as installing TinyTeX will query the github API.\n  run: |\n    quarto install tool tinytex --log-level warning\n    case $RUNNER_OS in \n      \"Linux\")\n          echo \"$HOME/bin\" &gt;&gt; $GITHUB_PATH\n          export PATH=\"$HOME/bin:$PATH\"\n          ;;\n       \"macOS\")\n          TLMGR_PATH=$(dirname $(find ~/Library/TinyTeX -name tlmgr))\n          echo $TLMGR_PATH &gt;&gt; $GITHUB_PATH\n          export PATH=\"$TLMGR_PATH:$PATH\"\n          ;;\n       \"Windows\")\n          TLMGR_PATH=$(dirname $(find $APPDATA/TinyTeX -name tlmgr.bat))\n          echo $TLMGR_PATH &gt;&gt; $GITHUB_PATH\n          export PATH=\"$TLMGR_PATH:$PATH\"\n          ;;\n        *)\n          echo \"$RUNNER_OS not supported\"\n          exit 1\n          ;;\n    esac\n    echo \"TinyTeX installed !\"\n    tlmgr install haranoaji   # Install haranoaji.sty\n  shell: bash\n\n\n\n3.1.5 ローカルの TinyTeX に haranoaji.sty をインストールする方法\ntlmgr install haranoaji\nだと，すでに TeX Live がローカルに存在する場合は，そちらにインストールされてしまう．\nquarto install tinytex\nでインストールされる TinyTeX は，ホームディレクトリ下の ~/Liberary/TinyTeX/ の bin 内にインストールされる．2\nそこの，tlmgr がインストールされている場所まで行って，\ntlmgr install haranoaji\nを実行すると良い．\n❯ ./tlmgr install haranoaji        \ntlmgr: package repository https://mirror.las.iastate.edu/tex-archive/systems/texlive/tlnet/ (verified)\n[1/1, ??:??/??:??] install: haranoaji [25570k]\nrunning mktexlsr ...\ndone running mktexlsr.\ntlmgr: package log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr.log\ntlmgr: command log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr-commands.log\n\n\n\n3.2 Typst を用いる方法\nHP\n使うフォントは次のように，Google Fonts を通じて，GitHub Actions 上でインストールすることもできるだろう：\nwget https://github.com/google/fonts/raw/main/ofl/bizudpgothic/BIZUDPGothic-Regular.ttf\nwget https://github.com/google/fonts/raw/main/ofl/bizudpgothic/BIZUDPGothic-Bold.ttf\ntypst の pdf は数式の処理がまだ納得のいく設定が見つかっていないが，コードの扱いが非常に自然で，出来上がりも美しい．\nただし，事前に GitHub Actions の環境上に対応する日本語フォントを用意しておく必要がある．\n{yml filename=\"publish.yml\"} - name: Install Japanese Fonts   env:     GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Setting GH_TOKEN is recommended as installing TinyTeX will query the github API.   run: |     git clone https://github.com/yuru7/udev-gothic.git     cd udev-gothic     sudo cp -r ./source /usr/share/fonts/truetype/udev-gothic     sudo fc-cache -f -v"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#スライドの作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#スライドの作り方",
    "title": "Quarto はじめて良かったこと",
    "section": "4 スライドの作り方",
    "text": "4 スライドの作り方"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#footnotes",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#footnotes",
    "title": "Quarto はじめて良かったこと",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n特に，VSCode ではビジュアルモードでの編集もサポートされており，Jupyter Notebookと全く同じ使用感で始められる．↩︎\nMaxOS では．quarto --paths で確認可能．↩︎"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html",
    "href": "posts/2024/Process/MartingaleProblem.html",
    "title": "マルチンゲール問題",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "href": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "title": "マルチンゲール問題",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Hoh, 1998, p. 28), (Criens et al., 2023)↩︎"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html",
    "href": "posts/2024/Process/ResidualWaitingTime.html",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#導入",
    "href": "posts/2024/Process/ResidualWaitingTime.html#導入",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 Markov 過程のエルゴード性\n空間 \\(E\\) 上の Markov連鎖は，\\(E\\) 上の確率測度の空間 \\(\\mathcal{P}(E)\\) 上に力学系 \\(((P^*)^n\\mu)_{n\\in\\mathbb{N}}\\) を定める．その不動点 \\(P^*\\mu=\\mu P=\\mu\\) が不変確率分布（平衡分布）である．\nこれは，Markov 連鎖の 確率核 \\(P\\) の 左作用 \\(P:\\mathcal{L}_b(E)\\to\\mathcal{L}_b(E)\\) の随伴作用素 \\(P^*:\\mathcal{P}(E)\\to\\mathcal{P}(E)\\) が \\(\\mathcal{P}(E)\\) に作用して得られる力学系ともみれる．\n\n\n\n\n\n\nこの力学系 \\((\\mathcal{P}(E),P^*)\\) は不動点を持つか？持つならば，どのようなノルムについてどのくらいの速さで収束するか？\n\n\n\nこれが Markov 連鎖のエルゴード性の議論である．\n通常，Markov 過程のエルゴード性は全変動ノルムについて考慮されるが，近年は弱位相に関する議論も進んでいる．\n\n\n1.2 再起過程\n\n\n\n\n\n\n定義 (renewal process, counting process)1\n\n\n\n非負確率変数の独立同分布 \\(\\{T_n\\}\\subset\\mathcal{L}(\\Omega)_+\\) について，\n\n\\(\\{T_n\\}\\) が定めるランダムウォーク \\[S_0=0,\\qquad S_n:=T_1+\\cdots+T_n,\\qquad n\\ge1,\\] を 再起過程 または再生過程という．2 \\(\\{T_n\\}\\) を待ち時間 (interarrival times)，\\(S_n\\) を \\(n\\) 回目到着時刻という．\n再起過程 \\(\\{S_n\\}\\) の 再起回数過程 とは， \\[N_t:=\\sum_{n=0}^\\infty1_{[0,t]}(S_n)=\\sup_{n\\in\\mathbb{N}\\mid S_n\\le t}\\] をいう．\\(t\\mapsto\\operatorname{E}[N_t]\\) を再起関数という．\n\n\n\n再起過程 \\(\\{S_n\\}\\) は通常，繰り返し起こる事象の発生時間をモデル化するために用いられる．\nその代表的なものが，待ち時間 \\(\\{T_n\\}\\) を指数分布に取った場合である Poisson 過程である．\n再起過程は OR を中心として，多くの応用先を持つ：\n\n\n\n再起過程の応用例 by Claude 3 Opus\n\n\n\n\n1.3 付随する待ち時間の Markov 過程\n再起過程 \\(\\{S_n\\}\\) は，ある Markov 過程 \\(\\{X_t\\}\\) が原点に戻る時刻の列と捉えることで，エルゴード性を議論することができる．\nこのときの Markov 過程 \\(\\{X_t\\}\\) を，本稿では 待ち時間の Markov 過程 と呼ぶことにする．\n\n\n\n待ち時間の Markov 過程 \\((X_t)\\) のアニメーション．原点は左端としている．\n\n\n以下，第 2 節では離散時間で状態空間も離散 \\(\\mathbb{N}\\) の場合，第 3 節では連続時間で状態空間も連続 \\(\\mathbb{R}_+\\) の場合について，この待ち時間の Markov 過程 \\(X\\) のエルゴード性を調べる．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-chain",
    "href": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-chain",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "2 待ち時間の Markov 連鎖",
    "text": "2 待ち時間の Markov 連鎖\nまず，待ち時間 \\(T_n\\) は非負整数 \\(\\mathbb{N}=\\{0,1,\\cdots\\}\\) 値とし，その分布を \\((p_i)\\sim\\mathcal{P}(\\mathbb{N})\\) とする．\nこれが定める再生過程 \\(\\{S_n\\}\\) は，次の遷移確率 \\((p_{ij})\\) を持つ \\(\\mathbb{N}\\) 上の Markov 連鎖 \\(X\\) が原点 \\(0\\) に到着する時刻の列と同分布である： \\[\np_{(i+1)i}=1,\\qquad p_{0i}=p_i,\\qquad i\\in\\mathbb{N}.\n\\]\nこのとき，（無限次元の）確率行列 \\(P\\) は Frobenius の同伴行列 の転置の形をしている．\n\n2.1 エルゴード定理\n\n\n\n\n\n\n命題\n\n\n\n上で定義した Markov 連鎖 \\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\) について，\n\n任意の状態 \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) が成り立つとする．このとき，\\(X\\) は既約で非周期的であり，再帰的である．\nさらに，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) も満たすとき，\\(X\\) は不変確率測度 \\[\\mu_i=\\frac{\\sum_{j=i}^\\infty p_j}{1+\\sum_{j=1}^\\infty jp_j}\\] をもち，正に再帰的である．そうでないときは零再帰的である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n分布 \\((p_i)\\) が偶数の上にしか台を持たないなど，\\(\\mathrm{supp}\\;(p)\\) に周期がある場合は \\(X\\) は周期的になってしまうが，任意の \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) ならば，任意の状態 \\(i\\in\\mathbb{N}\\) は本質的であり，互いに行き来できるため既約であり，周期も持たない．必ず有限時間内に原点に戻ってくるため，再帰的でもある．\n原点 \\(0\\) に初めて帰ってくる時刻を \\(T_0\\) とすると， \\[\\begin{align*}\n\\operatorname{E}_0[T_0]&=\\sum_{j=0}^\\infty(j+1)p_j\\\\\n&=1+\\sum_{j=0}^\\infty jp_j\\\\\n&=1+\\sum_{j=1}^\\infty jp_j.\n\\end{align*}\\] よって，正に再帰的であること \\(\\operatorname{E}_0[T_0]&lt;\\infty\\) は，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) に同値． このとき，離散エルゴード定理より，ただ一つの不変測度 \\((\\mu_n)\\in\\mathcal{P}(\\mathbb{N})\\) を持ち， \\[\\mu_i=\\frac{1}{\\operatorname{E}_i[T_i]},\\qquad i\\in\\mathbb{N},\\] と表せる．これにより \\(i=0\\) の場合はすぐに計算できるが，\\(i&gt;0\\) の場合は少し計算の見通しが良くない．そこで，必要条件 \\[\\mu_i=\\mu_{i+1}+\\mu_0p_i,\\qquad i\\in\\mathbb{N},\\] に注目すると，これを再帰的に適用することで， \\[\\begin{align*}\n\\mu_{i-1}&=\\mu_{i-1}-\\mu_0p_{i-1}\\\\\n&=\\mu_{i-2}-\\mu_0p_{i-2}-\\mu_0p_{i-1}\\\\\n&=\\cdots\\\\\n&=\\mu_0-\\mu_0\\sum_{j=0}^{i-1}p_j\\\\\n&=\\mu_0\\sum_{j=i}^\\infty p_j.\n\\end{align*}\\]\n\n\n\n\nMarkov 連鎖の概念は次節で解説しているので，証明を読む前にぜひチェックしてください．\n\n\n2.2 離散 Markov 連鎖の概念\nまず，離散状態空間 \\(E\\) 上の Markov 連鎖は，各状態 \\(i\\in E\\) の分類から始まる．\n\n\n\n\n\n\n定義：状態の再帰性 (recurrent, transient, positive recurrent, null recurrent)\n\n\n\n\\(E\\) を可算集合，\\(\\{X_n\\}\\) を \\(E\\) 上の Markov 連鎖とする．状態 \\(i\\in E\\) について， \\[\n\\tau_i:=\\inf\\{n\\ge1\\mid X_n=i\\}\n\\] を到着時刻とする．\n\n\\(i\\) が 再帰的 な状態であるとは，Markov 連鎖 \\(\\{X_n\\}\\) が \\(i\\in E\\) からスタートした場合，必ずいずれ戻ってくることをいう： \\[\n\\operatorname{P}_i[\\tau_i&lt;\\infty]=1.\n\\] そうでない場合，\\(i\\in E\\) は 推移的 であるという．\n再帰的な状態 \\(i\\in E\\) がさらに 正に再帰的 であるとは，帰ってくる時刻の期待値が有限であることをいう： \\[\n\\operatorname{E}_i[\\tau_i]&lt;\\infty.\n\\] そうでない場合は 零再帰的 であるという．\n\n\n\n続いて，この状態 \\(i\\in E\\) 毎に定義した性質が，Markov 連鎖 \\(\\{X_n\\}\\) 全体の性質に直接に影響するためには，次の「既約性」の条件が必要である．\n状態 \\(i\\in E\\) から \\(j\\in E\\) へ 到達可能 であるとは，ある \\(n\\in\\mathbb{N}\\) が存在して \\(p_{ij}^n&gt;0\\) を満たすことをいう．これを \\(i\\to j\\) と表す．\n\n\n\n\n\n\n定義：既約性と非周期性\n\n\n\n\\(E\\) を可算集合，\\(\\{X_n\\}\\) を \\(E\\) 上の Markov 連鎖とし，その遷移確率を \\(p_{ij}^n=\\operatorname{P}_i[X_n=j]\\) と表す．\n\n状態 \\(i\\in E\\) が 本質的 であるとは，任意の到達可能な状態 \\(i\\to j\\) に対して，\\(j\\to i\\) でもあることをいう．\nMarkov 連鎖 \\(X\\) が 既約 であるとは，任意の本質的な状態 \\(i,j\\in E\\) が互いに到達可能であることをいう：\\(i\\leftrightarrow j\\)．\n\n\n\n\\(X\\) の遷移確率 \\(p\\) は，\\(E\\) の本質的な状態 \\(E_\\mathrm{ess}\\) 上に，互いに到達可能であるという関係 \\(\\leftrightarrow\\) を通じて同値類 \\(E_\\mathrm{ess}/\\leftrightarrow\\) を定めることが示せる．この同値類が１つに縮退することを，既約というのである．\nまた，周期 \\[\nd(i):=\\gcd\\{n\\ge1\\mid p_{ii}^n&gt;0\\}\n\\] は，先述の同値類 \\(E_\\mathrm{ess}/\\leftrightarrow\\) 上に関数を定める．この関数 \\(d:(E_\\mathrm{ess}/\\leftrightarrow)\\to\\mathbb{N}^+\\) が 定値関数 \\(1\\) となるとき，\\(X\\) を 非周期的 という．\n\n\n\n\n\n\n証明\n\n\n\n\n\n任意の \\(i,j,k\\in E\\) について，必ず \\[\np^{n+m}_{ik}\\ge p^n_{ij}p^m_{jk}\n\\] が成り立つ．\\(i\\to j\\) かつ \\(j\\to k\\) であるとき，ある \\(n,m\\ge1\\) が存在して \\(p^n_{ij}&gt;0\\) かつ \\(p^m_{jk}&gt;0\\) であるから，\\(p^{n+m}_{ik}&gt;0\\) である．よって，\\(i\\to j\\)．これより \\(\\leftrightarrow\\) は推移的である．反射性は \\(p_{ii}^0=1&gt;0\\) であるため，定義上成り立つ．対称性も成り立つ．\n続いて，\\(i\\leftrightarrow j\\) ならば，\\(d(i)=d(j)\\) を示す． \\[\nN_i:=\\gcd\\{n\\ge1\\mid p_{ii}^n&gt;0\\}\n\\] と表すと，\\(i\\leftrightarrow j\\) ならば \\(N_i\\ne\\emptyset\\) である．任意の \\(s\\in N_i\\) を取ると，仮定 \\(i\\leftrightarrow j\\) より，先ほどの議論と同様にして，ある \\(n,m\\ge1\\) が存在して， \\[\np^{n+m+ks}_{jj}\\ge p^m_{ji}p^s_{ii}p^n_{ij}&gt;0,\\qquad k=1,2,\\cdots.\n\\] よって，\\(d(j)|s\\) が必要であるから，\\(d(j)\\le d(i)\\) が結論づけられる．逆も全く同様に議論できるから，\\(d(j)=d(i)\\)．\n\n\n\n\n\n\n\n\n\n命題：既約な Markov 連鎖の再帰性\n\n\n\n状態 \\(i,j\\in E\\) は互いに到達可能であるとする：\\(i\\leftrightarrow j\\)．このとき，\\(i,j\\) の推移性・零再帰性・正再帰性は一致する．特に，Markov 連鎖 \\(X\\) が既約ならば，全ての状態が同じ再帰性を持つ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nひとまず (Hairer, 2021, p. 2) 参照．\n\n\n\nこうして，既約な Markov 連鎖 \\(P\\) の再帰性が議論できるようになる．推移的であるか，零再帰的であるか，正に再帰的であるかのいずれかである．\n\n\n2.3 離散エルゴード定理\nMarkov 連鎖 \\(X\\) が再帰的であるためには，既約性と非周期性が十分条件である．加えて，極限が零測度でなければ，正に再帰的である．\n\n\n\n\n\n\n離散エルゴード定理3\n\n\n\n\\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\subset L(\\Omega;E)\\) をMarkov連鎖，\\(E\\) を可算集合とする． \\(X\\) が既約で非周期的ならば，次が成り立つ：\n\n任意の本質的な状態 \\(i\\in E\\) について，次が成り立つ： \\[\np^n_{ij}\\xrightarrow{n\\to\\infty}\\mu_j=\\frac{1}{\\operatorname{E}_j[\\tau_j]},\\qquad j\\in E.\n\\] 特に，任意の開始地点 \\(i\\in E\\) について，\\((p_{i-}^n)\\) は \\(\\mu\\) に全変動収束する．\n加えて \\(X\\) が正に再帰的であるならば，\\(\\mu:=\\{\\mu_i\\}_{i\\in\\mathcal{X}}\\) は \\(X\\) のただ一つの不変確率測度である．\n\\(X\\) が零再帰的である場合は \\(\\mu_i\\equiv0\\) であり，\\(X\\) の不変確率測度は存在しない．\n\n\n\n状態空間 \\(E\\) が有限である場合，正に再帰的＝エルゴード的ならば，必ず収束は（全変動ノルムに関して）指数速度で起こる．\nしかし，\\(E\\) が可算無限である場合，速度は様々である．\n\\(\\mathbb{N}\\) 上の待ち時間の Markov 連鎖が，その良い例となっている．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-process",
    "href": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-process",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "3 待ち時間の Markov 過程",
    "text": "3 待ち時間の Markov 過程\n\n3.1 過程の定義\n待ち時間の分布 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) は非零な１次の積率を持つとする．\n\\(\\nu\\) が定める再生過程を作り出す Markov 過程 \\(\\{X_t\\}\\) とは，離散時間の場合（第 2 節）と同様，\n\n\\(\\mathbb{R}^+\\) 上で \\(\\dot{X}_t=-1\\)．\n\\(X_t=0\\) のとき，次の瞬間 \\(\\nu\\) に従って選択されたある正の値にジャンプする．\n\nこのとき，次が成り立つ：\n\n\n\n\n\n\n命題\n\n\n\n上で定義した Markov 過程 \\(\\{X_t\\}\\) について，\n\n生成作用素は次で定まる： \\[\nLf(x)=-\\frac{d f(x)}{d x},\\qquad f\\in\\mathcal{D}(L),\n\\] \\[\n\\mathcal{D}(L):=\\left\\{f\\in\\mathcal{L}^1(\\nu)\\,\\middle|\\,f(0)=\\int^\\infty_0f(x)\\nu(dx)\\right\\}.\n\\]\n次で定まる確率分布 \\(\\mu_*\\in\\mathcal{P}(\\mathbb{R}_+)\\) は \\(\\{X_t\\}\\) に関して不変である： \\[\n\\mu_*(dx)=c\\nu([x,\\infty])dx,\n\\] \\[\nc:=\\int^\\infty_0y\\nu(dy)\\in(0,\\infty).\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n収束 \\[\n\\frac{P_tf(x)-f(x)}{t}\\to-\\frac{d f(x)}{d x}\\qquad t\\to\\infty,\n\\] は，\\(x\\ne0\\) の場合直ちに成り立つ．\nこれが \\(x=0\\) の場合も含めて一様に成り立つことと，\\((\\nu|f)=f(0)\\) は同値である．\n任意の \\(f\\in C_c^1(\\mathbb{R}_+)\\cap\\mathcal{D}(L)\\) について，次のようにして \\((\\mu_*|Lf)=0\\) が示せるためである： \\[\\begin{align*}\n(\\mu_*|Lf)&=-\\int^\\infty_0f'(x)\\mu_*(dx)=-c\\int^\\infty_0f'(x)\\nu([x,\\infty))\\,dx\\\\\n&=-c\\biggl[f(x)\\nu([x,\\infty))\\biggr]^\\infty_0-c\\int^\\infty_0f(x)\\nu(dx)=cf(0)-cf(0)=0.\n\\end{align*}\\]\n\n\n\n\n\n\n3.2 多項式エルゴード性\n\n\n\n\n\n\n定理（待ち時間の Markov 連鎖の多項式エルゴード性）\n\n\n\n待ち時間の分布は \\(\\nu\\ll\\ell_1\\) で密度 \\(p\\) をもち，ある \\(\\zeta&gt;2\\) が存在して \\(p\\) は \\(x^{-\\zeta}\\) のレートを持つとする： \\[\n\\frac{c_-}{x^\\zeta}\\le p(x)\\le\\frac{c_+}{x^\\zeta}\n\\] このとき，次の多項式エルゴード性が成り立つ： \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le C\\frac{x^\\alpha}{t^{\\alpha-1}},\n\\] \\[\nt\\ge0,x\\in\\mathbb{R}_+,\\alpha\\in(0,\\zeta-1).\n\\] 加えて，次が成り立つ： \\[\n\\lim_{t\\to\\infty}\\frac{\\log\\|P_t(x,-)-\\mu_*\\|_\\mathrm{TV}}{\\log t}=2-\\xi.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(V(x):=x^{\\alpha}\\;\\mathrm{on}\\;[1,\\infty)\\;(\\alpha&gt;0)\\) という形の関数であって，\\(V\\in\\mathcal{D}(L)\\) を満たすものが存在する．\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\alpha\\in(0,\\zeta-1)\\) を満たすように取れば， \\[\n\\int^\\infty_0V(x)\\nu(dx)=\\int^1_0V(x)p(x)dx+c_+\\int_1^\\infty x^{\\alpha-\\xi}dx&lt;\\infty\n\\] より，この値を \\(V(0)\\) とし，\\((0,\\infty)\\) 上で \\(C^1\\)-級になるように繋げば良い．\n\n\n\nこのとき， \\[\nLV(x)=-\\alpha x^{\\alpha-1}=-\\alpha V(x)^{1-\\frac{1}{\\alpha}}\\qquad\\mathrm{on}\\;[1,\\infty)\n\\] が成り立つ．即ち，Lyapunov関数 \\(\\varphi(x):=\\alpha x^{1-\\frac{1}{\\alpha}}\\) に関する劣線型ドリフト条件を満たす．スケルトンの議論を通じて，多項式エルゴード定理より，\\(\\frac{(1-1/\\alpha)}{1-(1-1/\\alpha)}=\\alpha-1\\) のレートで収束する： \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le C\\frac{\\lvert x\\rvert^\\alpha}{t^{\\alpha-1}}.\n\\] ここで \\(\\alpha\\in(0,\\zeta-1)\\) は任意の値であったから， \\[\n\\log\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le\\log C+\\alpha\\log x-(\\alpha-1)\\log t\n\\] \\[\n\\therefore\\qquad\\limsup_{t\\to\\infty}\\frac{\\log\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}}{\\log t}\\le\\limsup_{\\alpha&lt;\\zeta-1}-(\\alpha-1)=2-\\zeta.\n\\]\n最後の主張を示す．まず，\\(LV\\) は上に有界であるから， \\[\n\\frac{d }{d t}P_tV(x)=P_tLV(x)\\le C\n\\] \\[\n\\therefore\\qquad P_tV(x)\\le Ct+x^\\alpha=:g(x,t).\n\\] 続いて，\\(\\frac{c_-}{x^\\zeta}\\le p(x)\\) より，\\(\\mu_*\\) の密度は下から評価できる： \\[\n\\frac{\\mu_*(dx)}{dx}=\\int^\\infty_xp(y)\\,dy\\ge\\int^\\infty_x\\frac{c_-}{y^\\zeta}\\,dy=cx^{1-\\zeta}.\n\\] これより， \\[\n\\mu_*[V&gt;R]=\\mu_*[x&gt;R^{1/\\alpha}]\\ge CR^{-\\frac{\\zeta-2}{\\alpha}}=:f(R)\n\\] を得る．以上の評価とから， \\[\n\\frac{1}{2}\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\ge f(R)-\\frac{g(x,t)}{R}=CR^{-\\frac{\\zeta-2}{\\alpha}}-\\frac{\\lvert x\\rvert^\\alpha+Ct}{R}\n\\] \\(R&gt;0\\) について最適化することで， \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\ge C\\biggr(\\lvert x\\rvert^\\alpha+Ct\\biggl)^{\\frac{\\zeta-2}{\\zeta-2-\\alpha}}.\n\\] 同様にして，\\(\\alpha\\nearrow\\zeta-1\\) を考えることで結論が従う．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#参考文献",
    "href": "posts/2024/Process/ResidualWaitingTime.html#参考文献",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "4 参考文献",
    "text": "4 参考文献\n\n離散時間の場合は (Kulik, 2018, p. 22) 例 1.3.6, (Feller, 1967, p. 381) 例 XV.2.(k)，連続時間の場合は (Hairer, 2021, pp. 35–36) を参考にした．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "href": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Resnick, 2002, p. 174), (Mitov and Omey, 2014, p. 1), (Nummelin, 1984, p. 49) 定義4.2 など参照．計数過程の用語については (Aalen, 1978, p. 701) の解説も参照．↩︎\n一般には和訳「再生過程」が定着しているだろう．だが regeneration process ではなく，renewal process なのである．生死というよりは，再起というべきだと考えるため，ここでは再起過程と呼ぶこととする．最大の欠点は「再帰」と音が同じことである．↩︎\n(Kulik, 2018, p. 16) 定理1.2.5，(Robert and Casella, 2004, p. 224) を参照．↩︎"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html",
    "href": "posts/2024/Process/ZigZag.html",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "1 Zig-Zag 過程",
    "text": "1 Zig-Zag 過程\n\n1.1 はじめに\n１次元の Zig-Zag 過程は元々，Curie-Weiss 模型 における Glauber 動力学を lifting により非可逆化して得る Markov 連鎖の，スケーリング極限として特定された Feller-Dynkin 過程である (Bierkens and Roberts, 2017)．\n区分確定的 Markov 過程（PDMP）といい，ランダムな時刻にランダムな動きをする以外は，決定論的な動きをする過程である．\n\n\n\n\\(\\mathbb{R}^2\\) 上の Gauss 分布に収束する Zig-Zag 過程の軌跡\n\n\nPDMP の一般論については次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\nただし Zig-Zag 過程は，(Goldstein, 1951) で電信方程式と関連して，同様の過程が扱われた歴史もある．\n\n\n1.2 設定\nZig-Zag 過程 \\(Z=(X,\\Theta)\\) の状態空間は，力学的な立場に立って \\(E=\\mathbb{R}^d\\times\\{\\pm1\\}^d\\) と見ることが多い．\n力学における相空間と同様，\\(X\\in\\mathbb{R}^d\\) が位置を，\\(\\theta\\in\\{\\pm1\\}^d\\) が速度を表すと解する．すなわち，Zig-Zag 過程は全座標系と \\(45\\) 度をなす方向に，常に一定の単位速さで \\(\\mathbb{R}^d\\) 上を運動する粒子とみなせる．\nすなわち，\\((x,\\theta)\\in E\\) から出発する Zig-Zag 過程は，次の微分方程式系で定まる決定論的なフロー \\(\\phi_{(x,\\theta)}:\\mathbb{R}\\to\\mathbb{R}^d\\) に従って運動する粒子とみなせる： \\[\n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\\qquad \\frac{d \\Theta_t}{d t}=0.\n\\]\n\n\n1.3 アルゴリズム\n\n1.3.1 全体像\nZig-Zag 過程 \\(Z\\) は次のようにしてシミュレーションできる：\n\n\n\n\n\n\nZig-Zag 過程のシミュレーション\n\n\n\n\nレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) から定まる強度関数 \\[\nm_i(t):=\\lambda_i(x+\\theta t,\\theta),\\qquad i\\in[d],\n\\] を持つ，\\(d\\) 個の独立な \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程の，最初の到着時刻 \\(T_1,\\cdots,T_d\\) をシミュレーションする．\n最初に到着した座標番号 \\(j:=\\operatorname*{argmin}_{i\\in[d]}T_i\\) について，時刻 \\(T_j\\) に速度成分 \\(\\theta_j\\) の符号を反転させる．すなわち，関数 \\[\nF_j(\\theta)_i:=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n\\] に従ってジャンプする．\n１に \\(t=T_j\\) として戻って，くり返す．\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の跳躍測度\n\n\n\n\n\nもう一つ，MCMC の文脈で自然な見方は，状態空間を \\[\nE=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\}\n\\] と取る見方である．これは (Davis, 1993) による 一般の PDMP の設定 と対応する．\nこの \\(E\\) 上で，レート関数 \\[\n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n\\] が定める強度 \\[\nM(t):=\\lambda(x+t\\theta,\\theta)\n\\] を持った \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程に従ってジャンプが訪れる．\nこの点過程に対して，確率核 \\[\nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\\] に 印付けられた点過程 が，\\(Z\\) の跳躍測度である．\n\n\n\n\n\n\n\n\n\n証明（２つの定義の等価性）\n\n\n\n\n\nZig-Zag 過程に対する２つの定義を与えた．これら２つが同分布の過程を定めることは (Corbella et al., 2022), (Hardcastle et al., 2024) などさまざまなところで触れられているが，証明が必要である．\nまず，\\(\\min_{i\\in[d]}T_i\\) が，強度関数 \\(M\\) が定める到着時刻に同分布であることを示す．\n各 \\(T_i\\) の密度は \\[\np_i(t)=m_i(t)e^{-M_i(t)}1_{(0,\\infty)}(t)\n\\] で与えられ，\\(T_i\\) は互いに独立だから，\\((T_1,\\cdots,T_d)\\) の結合密度もわかる．\n\\(T_1,\\cdots,T_d\\) を昇順に並べた順序統計量を \\[\nT_{(1)}\\le\\cdots\\le T_{(d)}\n\\] で表すとする．この \\(d\\) 次元確率ベクトルの密度 \\(p\\) は， \\[\np(t_1,\\cdots,t_d)=1_{\\left\\{t_1\\le\\cdots\\le t_d\\right\\}}(t_1,\\cdots,t_d)\\left(\\sum_{\\sigma\\in\\mathfrak{S}_d}\\prod_{i=1}^dm_i(t_{\\sigma(i)})e^{-M_i(t_{\\sigma(i)})}\\right)\n\\] と計算できる．\nこの \\(p\\) を \\(t_2,\\cdots,t_d\\) に関して積分することで，\\(T_1\\) の密度が得られる：1 \\[\\begin{align*}\n    p_{(1)}(t)&=\\int_{(0,\\infty)^{d-1}}p(t_1,\\cdots,t_d)\\,dt_2\\cdots dt_d\\\\\n    &=\\biggr(\\sum_{i=1}^dm_i(t_1)\\biggl)\\exp\\left(-\\sum_{i=1}^dM_i(t_1)\\right)=m(t_1)e^{-M(t_1)}.\n\\end{align*}\\]\nこれは確かに，強度関数 \\(m\\) が定める到着時刻の密度である．\n続いて，\\(j=\\operatorname*{argmin}_{i\\in[d]}T_i\\) の，\\(\\min_{i\\in[d]}T_i\\) に関する条件付き確率質量関数が \\[\nq(i|t)=\\frac{m_i(t)}{\\sum_{i=1}^dm_i(t)}\n\\] であることを示す．\nそのためには，任意の \\(i\\in[d]\\) と \\(A\\in\\mathcal{B}(\\mathbb{R}^+)\\) とに関して \\(\\left\\{T_{(1)}\\in A,T_{(1)}=T_i\\right\\}\\) という形の事象を計算し，密度が積の形で与えられることを見れば良い．\n\\[\\begin{align*}\n    &\\qquad\\operatorname{P}[T_{(1)}\\in A,T_{(1)}=T_i]\\\\\n    &=\\operatorname{P}[T_i\\in A,\\forall_{j\\ne i}\\;T_i\\le T_j]\\\\\n    &=\\int_Ap_i(t_i)\\,dt_i\\left(\\sum_{\\sigma\\in\\mathrm{Aut}([d]\\setminus\\{i\\})}\\int^\\infty_{t_i}p_{\\sigma(1)}(t_{\\sigma(1)})\\,dt_{\\sigma(1)}\\int^\\infty_{t_{\\sigma(1)}}p_{\\sigma(2)}(t_{\\sigma(2)})\\,dt_{\\sigma(2)}\\cdots\\int^\\infty_{t_{\\sigma(d-1)}}p_{\\sigma(d)}(t_{\\sigma(d)})\\,dt_{\\sigma(d)}\\right)\\\\\n    &=\\int_Am_i(t_i)\\exp\\left(-\\sum_{i=1}^dm_i(t_i)\\right)\\,dt_i\\\\\n    &=\\int_A\\frac{m_i(t_i)}{m(t_i)}m(t_i)e^{-M(t_i)}\\,dt_i.\n\\end{align*}\\]\nよって，\\(\\min_{i\\in[d]}T_i\\) と \\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) とに関する結合密度は，2 \\[\nq(i|t)p_{(1)}(t)\n\\] という積の形で与えられることがわかった．\n\n\n\n\n\n\nまとめ\n\n\n\n\n前述の定義は，\\(\\min_{i\\in[d]}T_i\\) の形で密度 \\(p_{(1)}\\) からシミュレーションし，\\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) の形で \\(q\\) からシミュレーションしている．\n後述の定義は，\\(p_{(1)}(t)\\) から直接シミュレーションし，再び \\(q(i|t)\\) から直接シミュレーションをする．\n\n１が２に等価であることがわかった．\n\n\n\n\n\n\n\n1.3.2 到着時刻 \\(T_i\\) のシミュレーション方法\nZig-Zag 過程のシミュレーションは，ほとんど強度\n\\[\nM_i(t):=\\int^t_0m_i(s)\\,ds\n\\] を持つ非一様 Poisson 点過程のシミュレーションに帰着される．\n実はこれは，指数分布確率変数 \\(E_i\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1)\\) について \\[\nT_i\\overset{\\text{d}}{=}M_i^{-1}(E_i)\n\\] と求まる．\n\n\n1.3.3 Poisson 剪定\n仮にこの逆関数 \\(M_i^{-1}\\) が得られない場合でも，剪定 (Lewis and Shedler, 1979) によって \\(T_i\\) は正確なシミュレーションが可能である．\nこの方法は，\\(M_i^{-1}\\) を数値的に計算するよりも遥かに速い．これは \\(M_i\\) の定義に積分が存在し，これが多くの場合高次元になるためである．\n\n\n\n1.4 レート関数の条件\nZig-Zag 過程 \\(Z\\) がどのような分布に従うかは，全てレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) に委ねられている．\n\n\n\n\n\n\nZig-Zag 過程のレート関数 \\(\\lambda_1,\\cdots,\\lambda_d:E\\to\\mathbb{R}_+\\) は，負の対数密度 \\(U\\in C^1(\\mathbb{R}^d)\\) に対して， \\[\n\\lambda_i(x,\\theta):=(\\theta_i\\partial_iU(x))_++\\gamma_i(x,\\theta_{-i})\\quad(i\\in[d])\n\\] と定める．\nただし，次を仮定する：\n\n\\(\\gamma_i:E\\to\\mathbb{R}_+\\) は，\\(\\theta_i\\) のみには依らない任意の非負連続関数3 \\[\n  \\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta)).\n  \\]\n\\(e^{-U}\\in\\mathcal{L}^1(\\mathbb{R}^d)\\) が成り立ち，\\(\\pi(dx)\\,\\propto\\,e^{-U(x)}dx\\) が確率測度を定める．\n\\(M_i\\) は \\(t\\to\\infty\\) の極限で発散する： \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\]\n\n\n\n\n\n\n\n\n\n\n注（細かい条件たちについて）\n\n\n\n\n\nまた， \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\] は \\(t\\to\\infty\\) の極限で発散する必要がある．\nさもなくば，\\(M_i:(0,L)\\to(0,\\infty)\\;(L\\in(0,\\infty])\\) の形で定まらず，\\(M_i\\) がこのような可微分同相を与えない場合は \\[\nT_i:=M_i^{-1}(E_i),\\qquad E_i\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1),\n\\] によるシミュレーションも不正確になる．\n\n\n\n\n\n\n\n\n\n(Bierkens et al., 2019, pp. 1294 定理2.2)\n\n\n\n上述のリフレッシュレート \\(\\lambda_1,\\cdots,\\lambda_d\\) に対して，定義 1.3 で定まる Zig-Zag 過程 \\(Z\\) は次の分布 \\(\\widetilde{\\pi}=\\pi\\otimes\\mathrm{U}(\\{\\pm1\\}^{d})\\in\\mathcal{P}(E)\\) を不変にする： \\[\n\\widetilde{\\pi}(dxd\\theta)=\\frac{1}{2^d}\\frac{e^{-U(x)}}{\\mathcal{Z}}\\,dxd\\theta\n\\]\n\n\n\n\n\n\n\n\n注（拡張の可能性について）\n\n\n\n\n\n\\(\\{\\pm1\\}^d\\) 上の周辺分布が一様分布になっていること，勾配ベクトル \\(DU\\) の情報のみを使っており，座標に沿った方向しか見ていないため \\(U\\) の異方性に大きく左右されること，これらが「必ずしもそうある必要はない」拡張可能な点である．\n\n\n\n\n\n1.5 エルゴード性の条件\n\\(\\pi\\) が不変分布になるための十分条件 1.4 は極めて緩かったが，MCMC として使えるためにはエルゴード性が成り立つ必要がある．\n\n\n\n\n\n\n(Bierkens and Roberts, 2017 定理５)\n\n\n\n\\(d=1\\) で，レート関数 \\(\\lambda:E\\to\\mathbb{R}_+\\) はある \\(x_0&gt;0\\) が存在して次を満たすとする： \\[\n\\inf_{x\\ge x_0}\\lambda(x,1)&gt;\\sup_{x\\ge x_0}\\lambda(x,-1),\n\\] \\[\n\\inf_{x\\le-x_0}\\lambda(x,-1)&gt;\\sup_{x\\le-x_0}\\lambda(x,1).\n\\]\nこのとき，ある関数 \\(f:E\\to[1,\\infty)\\) が存在して \\(f(x,\\theta)\\to\\infty\\;(\\lvert x\\rvert\\to\\infty)\\) が成り立ち，かつ \\[\n\\left\\|P^t\\left((x,\\theta),-\\right)-\\pi\\right\\|_\\mathrm{TV}\\le\\kappa f(x,\\theta)\\rho^t,\\qquad(x,\\theta)\\in E,t\\ge0,\\rho\\in(0,1).\n\\]\n\n\n\n\n1.6 Subsampling Technology\nZig-Zag 過程はレート関数 \\(\\lambda\\) の設計に大きな自由度があった（第 1.4 節）．\nこれにより，Zig-Zag 過程ではバイアスを導入しない subsampling が可能であり，これを通じて データサイズに依らない一定効率での事後分布サンプリングが可能になる という super-efficiency (Bierkens et al., 2019) と呼ばれる性質を持つ．\nこの性質が実用上は最も重要である．詳しくは，次の記事を参照：\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#シミュレーション",
    "href": "posts/2024/Process/ZigZag.html#シミュレーション",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "2 シミュレーション",
    "text": "2 シミュレーション\n\n2.1 １次元での例\nZigZag サンプラーは非対称なダイナミクスを持っており，その点が MALA (Metropolis-adjusted Langevin Algorithm) や HMC (Hamiltonian Monte Carlo) などの従来手法と異なる．\n１次元でその違いを確認するために，Cauchy 分布という裾の重い分布を用いる．Cauchy 分布 \\(\\mathrm{C}(\\mu,\\sigma)\\) は次のような密度を持つ： \\[\nf(x)=\\frac{1}{\\pi\\sigma}\\frac{1}{1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n\\]\nその裾の重さ故，平均も分散も存在しない（発散する）．\nこのとき，次のような観察が得られる：\n\n\n\n\n\n\n\nZigZag サンプラーは Cauchy 分布に対して，最頻値から十分遠くから開始しても，高速に最頻値に戻ってくる．\nMALA は diffusive behaviour が見られ，最頻値に戻るまでに時間がかかる．\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する ZigZag サンプラーの軌道\n\n\nMALA と比較すると，その再帰の速さが歴然としている：4\n\n\nCode\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing Distributions\nusing Random\n\nRandom.seed!(2)\n\n# Define the target distribution (1D Cauchy) using the `LogDensityProblem` interface\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Choose initial parameter value for 1D\ninitial_θ = [500.0]\n\n# Use automatic differentiation to compute gradients\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensityCauchy(0.0, 1.0))\n\n# Set up the sampler with a multivariate Gaussian proposal.\nσ² = 100\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\n# Sample from the posterior.\nchain = sample(model_with_ad, spl, 2000; initial_params=initial_θ, chain_type=StructArray, param_names=[\"θ\"])\n\n# plot\nθ_vector = chain.θ\nsample_values = zip(1:length(θ_vector), θ_vector)\np2 = plot_1dtraj(sample_values, title=\"1D MALA Sampler (Cauchy Distribution)\", markersize=0, ylim=(-30, 750), label=\"MALA_1D\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する MALA サンプラーの軌道\n\n\n２つを並べて比較すると，Langevin ダイナミクスの方は，少し diffusive な動き（random walk property と呼ばれる）が見られることがわかる．\n\n\n\n\n\n\n例（NUTS サンプラーの動き）\n\n\n\n\n\nNUTS サンプラーはステップサイズを極めて大きくするため，プロットによるダイナミクスの比較があまり意味を持たなくなってくる．\n実際見てみると恐ろしいものである：\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する NUTS サンプラーの軌道\n\n\n\n\n\n\n\n2.2 ２次元での例\n\n\n\n\n\n\n\n勾配 \\(-\\nabla\\log\\pi\\) を計算し，∇ϕ(x, i, Γ) の形で定義する．\n\n\n\n\n\\[\n\\Sigma^{-1}=\\begin{pmatrix}2&-1\\\\-1&2\\end{pmatrix}\n\\]\nで定まる分散共分散行列 \\(\\Sigma\\) を持った中心化された正規分布 \\(\\pi(x)dx=\\mathrm{N}_2(0,\\Sigma)(dx)\\) に対しては，対数尤度は\n\\[\\begin{align*}\n    \\log\\pi(x)&=-\\log\\left((2\\pi)^{d/2}(\\det\\Sigma)^{1/2}\\right)\\\\\n    &\\qquad\\quad-\\frac{1}{2}x^\\top\\Sigma^{-1}x\n\\end{align*}\\]\nであるから，\\(\\phi:=-\\log\\pi\\) の第 \\(i\\) 成分に関する微分は\n\\[\\begin{align*}\n    \\partial_i\\phi(x)&=\\frac{\\partial }{\\partial x_i}\\biggr(\\frac{1}{2}x^\\top\\Sigma^{-1}x\\biggl)\\\\\n    &=\\Sigma^{-1}x.\n\\end{align*}\\]\n\nusing ZigZagBoomerang\nusing SparseArrays\n\nd = 2\n\n# 対数尤度関数 ϕ の第 i 成分に関する微分を計算\n1Γ = sparse([1,1,2,2], [1,2,1,2], [2.0,-1.0,-1.0,2.0])\n2∇ϕ(x, i, Γ) = ZigZagBoomerang.idot(Γ, i, x)\n\n# 初期値\nt0 = 0.0\nx0 = randn(d)\nθ0 = rand([-1.0,1.0], d)\n\n# Rejection bounds\nc = 1.0 * ones(length(x0))\n\n# ZigZag 過程をインスタンス化\nZ = ZigZag(Γ, x0*0)\n\n# シミュレーション実行\nT = 20.0\nzigzag_trace, (tT, xT, θT), (acc, num) = spdmp(∇ϕ, t0, x0, θ0, T, c, Z, Γ; adapt=true)\n\n# 軌跡を離散化\ntraj = collect(zigzag_trace)\n\n\n1\n\n勾配関数∇ϕの計算のためには，共分散行列の逆（精度行列ともいう）をSparseMatrixCSC型で指定する必要があることに注意．idotの実装 も参照．\n\n2\n\nidotは，疎行列Γの第i列と，疎ベクトルxとの内積を高速に計算する関数．\n\n\n\n\n\n\n\n\n\n\nidotの定義\n\n\n\n\n\nidot(A,j,u)は，疎行列Aの第j列と，疎ベクトルuとの内積を高速に計算する関数である．\n1function idot(A::SparseMatrixCSC, j, x)\n2    rows = rowvals(A)\n3    vals = nonzeros(A)\n    s = zero(eltype(x))\n4    @inbounds for i in nzrange(A, j)\n5        s += vals[i]'*x[rows[i]][2]\n    end\n    s\nend\n\n1\n\nパッケージ内部で，位置 \\(x\\in\\mathbb{R}^d\\) は全て SparseSate 型に統一されている？\n\n2\n\n疎行列 A の行インデックスを取得．rowvals(A)はベクトルであり，第１列から順番に，非零要素のある行番号が格納されている．\n\n3\n\n非零要素の値が格納されている．\n\n4\n\n@inbounds は，範囲外アクセスを許容するマクロ．高速化のためだろう．nzrange は，A の第 j 列に非零要素がある範囲を，第 \\(1\\) 列から累積して何番目かで返す．すなわち，rows[i]で正確に第j列の非零要素の行番号を狙い撃ちしてイテレーションできる．\n\n5\n\nxの非零要素がある行番号 rows[i] における成分の値 u[rows[i]][2] はこのような表記になる．これと，A の非零要素 vals[i] との内積を計算．\n\n\nなお，通常の行列に対しては，次のように実装されている：\nidot(A, j, x) = dot((@view A[:, j]), x)"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "3 Zig-Zag サンプラーの実装",
    "text": "3 Zig-Zag サンプラーの実装\n\nZigZagBoomerang の実装を紹介する．\nJulia の MCMC パッケージ一般については次の稿を参照：\n\n    \n        \n            \n            \n                Julia による MCMC サンプリング\n                新時代の確率的プログラミング環境の構築に向けて"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#文献紹介",
    "href": "posts/2024/Process/ZigZag.html#文献紹介",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\nZig-Zag Sampler を導入したのは (Bierkens et al., 2019) であるが，ざっと仕組みを把握をしたいならば (Corbella et al., 2022) の第二章がよっぽどわかりやすいだろう．"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#footnotes",
    "href": "posts/2024/Process/ZigZag.html#footnotes",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n計算過程は省略したが，\\(d=2\\) の場合と，\\(d=3\\) の場合を少しやってみると良い．↩︎\n参照測度は，\\([d]\\) 上のものは計数測度 \\(\\#\\) をとっている．↩︎\n従って，レート関数 \\(\\lambda\\) は連続とする．この関数 \\(\\gamma_i\\) は，\\(U\\) の情報には依らない追加のリフレッシュ動作を仮定に加える．実際，\\(\\lambda_i(x,\\theta)-\\lambda_i(x,F_i(\\theta))=\\theta_i\\partial_iU(x)\\) である限り，\\(\\theta\\) と \\(F_i(\\theta)\\) の往来には影響を与えず釣り合っているため，どのような \\(\\gamma_i\\) をとっても，平衡分布には影響を与えない．しかし，高くするごとにアルゴリズムの対称性が上がるため，\\(\\gamma\\equiv0\\) とすることが Monte Carlo 推定量の漸近分散を最小にするという (Andrieu and Livingstone, 2021)．(Bierkens et al., 2021) でも同様の洞察がなされている．↩︎\n(Bierkens et al., 2019) にある提示の仕方である．Zig-Zag の 2000 単位時間を単純に MALA と比較はできないと筆者も考えるが，ダイナミクスに注目していただきたい．実際，自分で実装してみると，シード値をいじらないと，Zig-Zag は必ずしも 500 単位時間前後でモード \\(0\\) に戻るわけではない．↩︎\n実は６つ持つ．他の初期値は σ=(Vector(diag(Γ))).^(-0.5); λref=0.0, ρ=0.0↩︎"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html",
    "href": "posts/Surveys/SMCSamplers.html",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nSMC の文脈で，目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) が複雑であるとき，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをする，というアイデアを 調温 (tempering) という（粒子フィルターの稿 も参照）．\nこの tempering という考え方は本質的に逐次的な発想を持っているが，元々は SMC の文脈とは全く独立に，MCMC を多峰性を持つ複雑な分布に対しても使えるように拡張する研究で提案された．これらの手法が自然と SMC へと接続する様子を population Monte Carlo (Iba, 2001b), (Ajay Jasra et al., 2007) というキーワードで理解されている．\nまずはその歴史を概観する．いずれも，目標分布 \\(\\pi_p\\) が多峰性をもち，MCMC がうまく峰の間を遷移できずに正しいサンプリングができない（収束が遅くなる）問題を解決する文脈の中で捉えられる．\n\n\nこれは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3\n\n\n\nMCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15\n\n\n\n\n\n\ntempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nこの方法は混合モデルにおいて事後分布が多峰性を持つなどして Gibbs サンプラーがうまく収束しない場合でも，有効な MCMC サンプラーとなる (A. Jasra et al., 2005)．\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，18 拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．19 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw(X_{1:p}):=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．20\n従って，本当は \\(E^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．21\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．\n\n\n\n\n\n\n\n簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．\n\n\n\n\n\n\n\n\n目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-SA",
    "href": "posts/Surveys/SMCSamplers.html#sec-SA",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "これは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "href": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "MCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "href": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nこの方法は混合モデルにおいて事後分布が多峰性を持つなどして Gibbs サンプラーがうまく収束しない場合でも，有効な MCMC サンプラーとなる (A. Jasra et al., 2005)．\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，18 拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．19 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw(X_{1:p}):=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．20\n従って，本当は \\(E^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．21\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "href": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#その他の手法",
    "href": "posts/Surveys/SMCSamplers.html#その他の手法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#footnotes",
    "href": "posts/Surveys/SMCSamplers.html#footnotes",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの前にも，Umbrella sampling (Torrie and Valleau, 1977) が本質的には密度の調温のアイデアを用いていた．(Liu, 2004, pp. 206–) Section 10.1 も参照．↩︎\n分子動力学 (molecular dynamics) などの文脈では Metropolis 法はちょうど分子運動のシミュレーションになっていることを踏まえれば，これを simulated annealing と呼ぶのは極めて鮮やかなアナロジーとなっている．焼きなまし法自体も，シミュレーション可能になったのである．↩︎\n(Geman and Geman, 1984) によると，各 \\(\\pi_n\\) における MCMC move の回数を \\(N_n\\) とした場合，\\(O(\\log(N_1+\\cdots+N_n))\\) のオーダーで \\(T_n\\) を（十分遅く）変化させれば，この手法はほとんど確実に \\(\\operatorname*{argmin}h\\) 内に収束する．(Liu, 2004, pp. 209–) 10.2 節も参照．↩︎\n(岡本祐幸, 2010), (Iba, 2001a) など．↩︎\n(Liu, 2004, pp. 205–) Chapter 10. Multilevel Sampling and Optimization Methods も参照．↩︎\n(Liu, 2004, p. 207) も参照．↩︎\n(Chopin et al., 2023), (Liu, 2004, p. 4) でも (Geyer, 1991) を引用して PT と呼んでいる．一方で物理学の分野では (Hukushima and Nemoto, 1996) の exchange Monte Carlo や (Swendsen and Wang, 1986) などの文献もある．前者は (Liu, 2004, p. 4) が “is reminiscent of parallel tempering (Geyer, 1991)” と指摘しており，後者は (Bouchard-Côté et al., 2012) などが引用している．↩︎\n最終講義 スピングラスと計算物性物理 p.34 も参照．温度の違う熱浴につけたレプリカをシミュレートして，時々交換する，という見方ができるためにこう呼ぶ．↩︎\n(Ajay Jasra et al., 2007) は (Geyer, 1991) を指して population-based MCMC と呼んでおり，SMC も含めて population-based simulation と呼んでいる．population-based という言葉自体は (Iba, 2001b) からとったという．“we define a population-based simulation method as one which, instead of sampling a single (independent/dependent) sample, generates a collection of samples in parallel” と定義しており，大きく MCMC によるものと逐次重点サンプリングベースのものの２流儀あるとしている．(Liu, 2004, pp. 225–) 第11章なども参照．↩︎\n(岡本祐幸, 2010) など．↩︎\n(Behrens et al., 2012, p. 66) も参照．↩︎\n(Iba, 2001a) が良い解説を与えていると (Ajay Jasra et al., 2007) でも言及されている．ただし，(Iba, 2001a) はこの並行テンパリングだけでなく，模擬テンパリング，multicanonical Monte Carlo (Berg and Neuhaus, 1991) / Adaptive Umbrella Sampling (Torrie and Valleau, 1977) を総称して 拡張アンサンブル法 (Extended Ensemble Monte Carlo) と呼んでサーベイしていることに注意．↩︎\n(Lyubartsev et al., 1992) が引用されることもある．(酒井佑士, 2017), (岡本祐幸, 2010) など．method of expanded ensemble とも呼ばれる (岡本祐幸, 2010), (Iba, 2001a)．↩︎\n記法 \\([p]=\\{1,\\cdots,p\\}\\) は 本サイトの数学記法一覧 を参照↩︎\nその後すぐに分子シミュレーションの分野にも導入された．(岡本祐幸, 2010) も参照．↩︎\n(Behrens et al., 2012) も参照．↩︎\n(Chopin and Papaspiliopoulos, 2020, p. 33) で，SMC を調温に初めて応用した論文として紹介されている．p.352 では “An early version of SMC tempering (without resampling)” としている．↩︎\n\\(\\pi_p(x_p)/\\pi_0P_1P_2\\cdots P_p\\) は多くの場合計算不能である．↩︎\nただし，\\(P_i^{-1}\\) とは，\\[ P_i(x_{i-1},x_i)\\pi_{i-1}(x_i-1)=\\pi_i(x_i)P_i^{-1}(x_{i-1},x_i) \\] で定まる確率核とした．\\(\\otimes\\) の記法はこちらも参照．↩︎\nこのウェイトの表示は，\\(P_i^{-1}/P_i=\\pi_{i-1}/\\pi_i\\) が成り立つことから直ちに従う．↩︎\n(Doucet et al., 2022) も参照．↩︎"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#現在",
    "href": "static/CV/cv_Japanese.html#現在",
    "title": "司馬博文（しばひろふみ）",
    "section": "8/28/2024 現在",
    "text": "8/28/2024 現在\n総合研究大学院大学（統計科学コース）５年一貫博士課程２年目．\n\n日本語，中国語が母語で，英語も話せる（TOEFL iBT 100点）．Python, R でのコーディング経験３年以上，Julia １年以上．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#研究分野",
    "href": "static/CV/cv_Japanese.html#研究分野",
    "title": "司馬博文（しばひろふみ）",
    "section": "研究分野",
    "text": "研究分野\n\n輸送によるサンプリング法\n特に，シュレーディンガー橋や正規化フローなどの生成モデリング手法．\nモンテカルロ法\n特に，逐次モンテカルロ法（SMC）やマルコフ連鎖モンテカルロ法（MCMC）などのベイズ統計計算アルゴリズム．\n統計モデリング\n特に，政治学，疫学，惑星地球科学などの分野への応用．\nベイズ機械学習\n特に，ガウス過程や階層モデリング，ノンパラメトリクスなど．\nデータ駆動科学\n特に，データ埋め込み，可視化，軌道推定，データ同化など．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#学歴",
    "href": "static/CV/cv_Japanese.html#学歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "学歴",
    "text": "学歴\n\n博士（統計科学）. 総合研究大学院大学先端学術院統計科学コース. 2023.4 – 2028.3\n指導教員：鎌谷研吾教授，矢野恵佑准教授\n学士（理学）. 東京大学理学部数学科. 2019.4 – 2023.3\n指導教員：吉田朋広教授"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#職歴",
    "href": "static/CV/cv_Japanese.html#職歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "職歴",
    "text": "職歴\n\nリサーチ・アシスタント. 統計数理研究所. 2023.7 – 現在\n確率過程の統計推測のための R パッケージである YUIMA の開発などを通じ，モンテカルロ法とベイズ統計の応用に取り組む．\n連携研究員. 東京大学先端科学技術研究センター. 2023.4 – 現在\n信頼できる AI と機械学習の視点からの経済安全保障の研究．\nデータサイエンティスト. IMIS 研究所. 2022.8 – 2024.1\n製造業のクライアントに対して統計分析と機械学習のソリューションを提供．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#研究滞在",
    "href": "static/CV/cv_Japanese.html#研究滞在",
    "title": "司馬博文（しばひろふみ）",
    "section": "研究滞在",
    "text": "研究滞在\n\nユニバーシティ・カレッジ・ロンドン，イギリス．2024.11.4 – 2024.12.2\n受入教員：Alexandros Beskos 教授"
  },
  {
    "objectID": "static/English.html",
    "href": "static/English.html",
    "title": "Entries in English",
    "section": "",
    "text": "Notations | Categories | All Posts\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\nFeb 25, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\nJan 5, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\nDec 1, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Notations.html#sec-set",
    "href": "static/Notations.html#sec-set",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "1 集合",
    "text": "1 集合\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nここでは，あらゆる数学概念は，ZFC公理系 の下で集合として定義する．1 記号 \\(:=\\) は「右辺によって左辺を定義し，その結果等号が成り立つ」という主張の略記である．2\n\n1.1 集合\n\n空集合 を \\[\n\\emptyset:=\\{x\\mid x\\ne x\\}\n\\] で表す．3\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．4\n\\(A,B\\subset X\\) の 差 を \\[\nA\\setminus B:=\\left\\{a\\in A\\mid a\\notin B\\right\\}\n\\] で表す．\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cup B\\) と同じ数学的対象であるが，同時に \\(A\\cap B\\) という事実も主張するものとする．5\n対称差 を \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\] で表す．6\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．7 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．8\n例えば，\\(X\\in\\mathcal{L}(\\Omega)\\) を実確率変数，\\(A\\in\\mathcal{B}(\\mathbb{R})\\) を Borel 集合とすると， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] という略記を用いる．\n\n\n\n1.2 構成\n\n自然数 を \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\},\n\\] によって帰納的に定義する．9\n自然数の集合を表すため，次の記法を用意する：10 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，11 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す： \\[\n\\mathbb{R}_+=[0,\\infty),\\quad\\mathbb{R}^+=(0,\\infty).\n\\]\n部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．12\n実数 \\(x\\in\\mathbb{R}\\) に対して，その整数部分を \\[\n\\lfloor x\\rfloor:=\\max\\{n\\in\\mathbb{Z}\\mid n\\le x\\}\n\\] と表す．13\n次の演算規則を約束する：14 \\[\n\\prod_\\emptyset=1,\\quad\\sum_{\\emptyset}=0.\n\\]\n\\(0!=1\\) とする．\n\\(n\\)-組 を次のように帰納的に定める：15 \\[\n(x_1,x_2):=\\{\\{x_1\\},\\{x_1,x_2\\}\\},\n\\] \\[\n(x_1,\\cdots,x_n):=(x_1,(x_2,\\cdots,x_n)).\n\\]\n自然数の組を表すため，次の記法を用意する：16 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組を \\[\nX_{1:N}:=(X_1,\\cdots,X_N)\n\\] と表す．17\n\n\n\n1.3 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n写像 \\(f\\) の 値域 を \\[\\mathrm{Im}\\,f:=f(X)\\] で表す．\n\\(A\\subset X\\) への 制限 を \\(f|_A:A\\to Y\\) と表す．18\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．19\n\\(f_*\\) は部分集合 \\(A\\subset X\\) を像 \\(f(A)\\subset Y\\) に対応させる写像 \\[\nf_*:P(X)\\to P(Y)\\] と定義する．\n同様に写像 \\(f^*:P(Y)\\to P(X)\\) を定める： \\[\nf^*(B)=f^{-1}(B)\\quad(B\\subset Y).\n\\]\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．20\n部分集合 \\(A\\subset X\\) の 指示関数 といった場合は \\[\n\\sigma_A:=0\\cdot1_A+\\infty\\cdot1_{A^\\complement}\n\\] を表す．21\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．22\n写像 \\(f:X\\to Y\\) のうち，有限個の元を除いて \\(f(x)=0\\) を満たすものがなす全体を \\[\nY^{(X)}:=\\left\\{f\\in Y^X\\mid f=0\\;\\;\\text{f.e.}\\right\\}\n\\] と表す．23\n\\(P(X)\\) を \\(2^X\\) と同一視する．特に，\\(X\\) の有限部分集合の全体を \\[\n2^{(X)}=\\left\\{A\\in P(X)\\:\\middle|\\: A\\overset{\\text{finite}}{\\subset}X\\right\\}\n\\] と表す．24\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．25\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\[\n\\mathrm{pr}_i:\\prod_{i\\in I}X_i\\twoheadrightarrow X_i\n\\] で表す．26\n\\(x\\in X\\) での 評価写像 を \\[\n\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\n\\] で表す．27\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．\nしかしこの写像の値域も 族 と呼び，この場合は \\[\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\] と表す．28\n特に \\(I=\\mathbb{N}\\) のときは 列 ともいう．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．29\n\n\n\n1.4 圏\n\n集合の圏 を \\(\\mathrm{Set}\\) で表す．\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X)\n\\] を表す．30\n確率空間と確率核の圏を \\(\\mathrm{Stoch}\\) で表す．31\n圏 \\(C\\) の対象 \\(X,Y\\in C\\) の間の 射 の全体を \\(\\mathrm{Hom}_C(X,Y)\\) で表す．32\n特に \\[\nY^X=\\mathrm{Map}(X,Y)=\\mathrm{Hom}_\\mathrm{Set}(X,Y).\n\\]\n圏 \\(C\\) の対象 \\(X\\in C\\) の自己射の全体を \\[\n\\mathrm{End}_C(X):=\\mathrm{Hom}_C(X,X)\n\\] で表す．\nそのうち可逆なもののなす部分集合を \\(\\mathrm{Aut}_C(X)\\) で表す．集合 \\([n]\\) の 置換群 は \\(\\mathrm{Aut}_\\mathrm{Set}([n])\\) と表せる．\n\n\n\n1.5 関数\n\n\\(R\\) を環とする．\\(f_1,f_2\\in R^X\\) に対して， \\[\n(f_1+f_2)(x):=f_1(x)+f_2(x),\n\\] \\[\n(f_1f_2)(x):=f_1(x)f_2(x)\n\\] で定める演算により \\(R^X\\) も環とみなし，定値関数 \\(f\\equiv a\\in R\\) を \\(R\\) の元と同一視する．33\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を \\[\na\\lor b:=\\sup\\{a,b\\},\n\\] \\[\na\\land b:=\\inf\\{a,b\\},\n\\] で表す．34\n\\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right)\n\\] と表す．35\n\\(0\\) を持つ束においては次の略記を使う：36 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序集合 \\(Y\\) に値を取る関数 \\(f,g\\in\\mathrm{Map}(X,Y)\\) について，\\(f\\le g\\) とは \\[\n\\forall_{x\\in X}\\;f(x)\\le g(x)\n\\] の略記とする．\n同じ条件を，一階の量化記号 \\(\\forall\\) を省略して \\[\nf(x)\\le g(x)\\quad(x\\in X)\n\\] または \\(f\\le g\\) とも略記する．\n\\(Y\\) が束であるとき，この順序により関数の空間 \\(\\mathrm{Map}(X,Y)\\) は束となり，演算 \\(\\land,\\lor\\) が定まる．\n関数の列 \\(\\{f_n\\}\\subset Y^X\\) について，\\(f_n\\nearrow f\\) とは，収束 \\(f_n\\to f\\) だけでなく，\\(\\{f_n\\}\\) が単調増加であることも含意する．37\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\[\nO(g(x))\\;(x\\to x_0)\\] とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．38\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の意味でも使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0\n\\] を満たすこととする．"
  },
  {
    "objectID": "static/Notations.html#sec-space",
    "href": "static/Notations.html#sec-space",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "2 空間",
    "text": "2 空間\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．39\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．40\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．\n閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}\n\\] で表す．\n\n\n\n2.2 線型空間\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．41\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．42 \\(S_n(\\mathbb{R})_+\\) で半正定値なものの全体を表す．43\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}\n\\] とも表す．44\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，45 共役転置を \\(A^*\\) で表す．46 \\(\\mathbb{F}=\\mathbb{C}\\) の場合は \\(A^\\top=A^*\\)．\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．47\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について， \\[\n\\begin{align*}\n  A&+B\\\\\n  &\\quad:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n  \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\},\n\\end{align*}\n\\] と表す．48\n集合 \\(A\\subset X\\) の 凸包 を \\(\\operatorname{Conv}(A)\\) で表す．49\n集合 \\(A\\subset X\\) が生成する部分空間を \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x\n\\] で表す．50\n内積を \\((-|-)\\) で表す．51\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を52 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\operatorname{Tr}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}\n\\] で表す．53\n\n\n\n2.3 Banach 空間\n\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．54 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n特に \\(J\\) が有限であるとき， \\[\n  \\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n  \\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．55\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．56\n距離空間 \\((T,d)\\) の 開球 を \\[\n\\begin{align*}\n  U_\\epsilon(t)&:=U(t;\\epsilon)\\\\\n  &:=\\left\\{s\\in T\\mid d(s,t)&lt;\\epsilon\\right\\}\n\\end{align*}\n\\] で表す．57\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．58\n単位閉球を \\(B:=B(0;1)\\) で表す．\n\\(\\mathbb{R}^n\\) のものである場合は特に \\(B^n\\) とも表す．59\n\\(\\mathbb{R}^n\\) の標準基底を \\[\ne_i=(0,\\cdots,0,1,0,\\cdots,0)\n\\] と表す．60\nBanach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．61\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}\n\\] で表す．62\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) として理解できる数学的対象の別記法と捉えられるように設計する．63\n\n\n2.4 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．64\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．65\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．66\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．67\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．68\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．69\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．70\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．71\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．72\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．73 \\(\\gamma_n:=\\mathrm{N}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n2.5 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．74 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．75\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．76\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．77\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．78\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．79\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．80\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．81\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．82\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．83"
  },
  {
    "objectID": "static/Notations.html#sec-kernel",
    "href": "static/Notations.html#sec-kernel",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 核",
    "text": "3 核\n空間を導入した次は，その射を定義せねばなるまい．\n本節では，\\((E,\\mathcal{E})\\) を 可測空間 とする．84\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 85\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 86\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．87 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．88\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．89\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．90\n\n\n\n3.2 確率分布\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 確率測度 の全体を \\(\\mathcal{P}(E,\\mathcal{E})\\) と書く．\\(E\\) が位相空間であるとき，Borel 確率測度の全体を \\(\\mathcal{P}(E)\\) と略記する．91\n\\(E\\) を位相空間とする．\\((E,\\mathcal{B}(E))\\) 上の Radon 確率測度 の全体を \\[P(E)\\subset\\mathcal{P}(E)\\] で表す．92\n２つの確率分布 \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) の カップリング の全体を \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\] で表す．93\n\\(d\\)-次元 正規分布 を \\[\\mathrm{N}_d(\\mu,\\Sigma)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．94 その密度は \\[\n  \\phi_d(x;\\mu,\\Sigma):=\\frac{d \\mathrm{N}_d(\\mu,\\Sigma)}{d \\ell_d}(x)\n  \\] で表す．\n集合 \\(A\\subset\\mathbb{R}^d\\) 上の 一様分布 を \\[\\mathrm{U}(A)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．\n点 \\(x\\in E\\) 上の Delta 測度 を \\(\\delta_x\\) で表す．95\n確率変数 \\(X\\sim\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の 分布関数 を \\[\n\\begin{align*}\n  F_X(a)&:=F_\\nu(a)\\\\\n  &:=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d]\\\\\n  &\\quad(a=a_{1:d}\\in\\mathbb{R}^d)\n\\end{align*}\n\\] で表す．\n\\(d=1\\) のとき，その一般化逆を \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\}\n\\] \\[\n(u\\in(0,1)^d)\n\\] で表す．96\n\n\n\n3.3 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：97\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．98 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．99\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 100\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．101\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．102\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．103\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．104\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．105\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.4 関数の空間\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．106\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．107\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．108\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．109 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．110\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．111\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．112\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．113\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．114\n\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．115\n\n\n3.5 作用素\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とし，\\(X,Y\\) をノルム空間とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．116\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．117\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．118\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．119\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．120\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．121\n\n作用素 \\(T:X\\to Y\\) と言ったとき，線型写像 \\(T:X\\to Y\\) を指すこととする．122\n\\(X\\) 内の作用素 \\(T:X\\supset\\mathcal{D}(T)\\to Y\\) と言ったとき，ある \\(X\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to Y\\) を指すこととする．123\n有界作用素の全体を \\(B(X,Y)\\) で表す．124 \\(B(X):=B(X,X)\\) とする．\n連続作用素の全体を \\(L(X,Y)\\) で表す．125"
  },
  {
    "objectID": "static/Notations.html#sec-analysis",
    "href": "static/Notations.html#sec-analysis",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 解析",
    "text": "4 解析\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．126\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．127\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．128\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．129 \\(m=1\\) のとき， \\[\n\\operatorname{grad}u:=\\nabla u:=(Du)^\\top=\\begin{pmatrix}\\frac{\\partial u}{\\partial x_1}\\\\\\vdots\\\\\\frac{\\partial u}{\\partial x_n}\\end{pmatrix}\n\\] とも表す．\n発散 を \\[\n\\operatorname{div}u:=\\nabla\\cdot u:=\\operatorname{Tr}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．130\n\\(u\\) が正方行列 \\(M_n(\\mathbb{R})\\)-値であった場合，行成分毎の適用 \\[\n  \\operatorname{div}u:=\\begin{pmatrix}\\operatorname{div}(u_{1-})\\\\\\vdots\\\\\\operatorname{div}(u_{n-})\\end{pmatrix}\n  \\] と解する．\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n  \\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n  \\] と同一視する．131\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n  \\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\operatorname{Tr}(D^2u)\n  \\] で定める．\n\n\n\n4.2 Fourier 変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．132\n符号関数 を \\[\n\\operatorname{sgn}(x):=2H(x)-1\n\\] で定める．133\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 超関数\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．134 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．135\n\n\n\n4.4 確率解析\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．136"
  },
  {
    "objectID": "static/Notations.html#sec-process",
    "href": "static/Notations.html#sec-process",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "5 過程",
    "text": "5 過程\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率変数の収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．137\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．138\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n\n\n\n5.2 確率基底\n（確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．139\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\(E^T\\) に定める写像 \\[\nX_-:\\Omega\\to E^T\n\\] を 転置 と呼ぶ．140\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいい，このような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．141\n\\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．142 ただし，\\(x(0-)=x(0)\\) とする．143\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の 情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系 \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．144\n加えて， \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\] と表す．145\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) とその上の情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) からなる 4-組 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) を 確率基底 という．\n確率基底が 完備 であるとは， \\[\n\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\n\\] を満たすことをいう．146\n\n\n\n5.3 可測性\n\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}}\\) がフィルトレーション \\((\\mathcal{F}_t)\\) に 適合的 であるとは， \\[\nX_t\\in\\mathcal{F}_t\\quad(t\\in\\mathbb{R})\n\\] を満たすことをいう．\n第一種不連続な見本道を持つ適合的な過程の全体を \\(\\mathbb{D}\\) で表す．一方で，càglàd な見本道を持つ適合的な過程の全体を \\(\\mathbb{L}\\) で表す．147\n\n\n\n5.4 停止時148\n\n確率基底 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) 上の 停止時 とは，同じ確率空間 \\(\\Omega\\) 上の可測関数 \\(T:\\Omega\\to[0,\\infty]\\) であって， \\[\n\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t,\\qquad t\\in\\mathbb{R}_+,\n\\] も満たすものをいう．149\n停止時 \\(T\\) までの 情報 とは， \\[\n\\mathcal{F}_T:=\\left\\{A\\in\\mathcal{F}_\\infty\\mid\\forall_{t\\in\\mathbb{R}_+}\\;A\\cap\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t\\right\\}\n\\] で定まる \\(\\sigma\\)-代数をいう．"
  },
  {
    "objectID": "static/Notations.html#終わりに",
    "href": "static/Notations.html#終わりに",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "終わりに",
    "text": "終わりに\n\n本サイトの記法で筆者が最も注意することは，あらゆる記法を背後の数学的消息と調和するように定義するということであった．\nこれにあたり，あらゆる 数学的対象 を集合から構成する立場を取る一方で，理解するにあたっては 集合と写像（または関手）とを厳密に峻別する ということを徹底することを大事にした．\n例えば集合の合併と共通部分に \\(\\cap,\\cup\\) を用いること，直和と直積に \\(\\coprod,\\prod\\) を用いることは，圏論的な双対性を視覚的に認識しながら数学的議論を進めるためである．(斎藤毅, 2009, p. 37) にも詳しく解説されている．\n記法の開発は数学の重要な一部であると筆者は信じているのである．"
  },
  {
    "objectID": "static/Notations.html#footnotes",
    "href": "static/Notations.html#footnotes",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n集合のなす圏 \\(\\mathrm{Set}\\) は数学の基礎付けとして採用するのに極めて良い性質を持つ nLab．↩︎\n(Del Moral and Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) に一致する．\\(\\equiv,\\overset{\\text{def}}{=}\\) などもよく用いられる．(Crisan and Doucet, 2002), (Smith, 2010) では \\(\\overset{\\triangle}{=}\\) も用いられる．ここでは，これらの左右対称な記号は避けた．また，\\(=_{\\text{df}}\\) などを使うものもある (Quine and Szczotka, 1994)．↩︎\n(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 2) の定め方に一致する．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie and Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie and Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod and Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod and Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod and Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Jacod and Protter, 2012) では \\([x]\\) で表される．↩︎\n(Del Moral and Penev, 2014, p. xlviii), (Del Moral, 2004, p. 10) の定義に一致する．これは \\(\\prod_{i\\in\\emptyset}X_i\\) が一点集合で，\\(\\coprod_{i\\in I}X_i\\) が空集合である消息の一般化と見れる．なお，集合 \\(X\\) の部分集合の空な族 \\((X_i)_{i\\in\\emptyset}\\) は存在し，それは \\(\\mathrm{Map}(\\emptyset,X_i)\\) のただ一つの元である．↩︎\n(Kuratowski, 1921) による定義である．(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 118), (斎藤毅, 2009, pp. 定義1.3.1 p.15) の定め方に一致する．また \\(n\\)-組を英語では tuple と呼ぶが，全く同じ対象をリスト (list) とも呼ぶ nLab Concept with an Attitude．↩︎\n(Chopin and Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(新井敏康, 2011, p. 119) などでは，\\(f\\restriction_A\\) とも表す．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\n(Brezis, 2011, p. 14) の使い分けに倣った．支持関数は英語で indicator function という (Beck, 2017, p. 14) 例2.1，(寒野善博，土谷隆, 2014, p. 110)，(Ekeland and Témam, 1999, p. 8)．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné and Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(斎藤毅, 2007, pp. 例1.4.7 p.20) に従った．また f.e. とは with a finite number of exceptions の略で，「有限個の例外を除いて成り立つ」という意味である (伊藤清, 1991, p. 124)．↩︎\n(斎藤毅, 2009, p. 179) では \\(F(X)\\) と表記している．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(Billingsley, 1999), (Ethier and Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n(Fritz, 2020, p. 19), (Perrone, 2024) など．Markov圏の稿 も参照↩︎\nnLab の記法に一致する．(斎藤毅, 2020, p. 7) では \\(\\mathrm{Mor}_C(X,Y)\\) と表す．↩︎\n(Del Moral, 2004, p. 7) も参照．↩︎\n(Del Moral and Penev, 2014, p. xlvii), (V. I. Bogachev, 2007, p. 277) 4.1.(i) に一致する．(V. I. Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie and Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod and Protter, 2012), (Del Moral and Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(V. I. Bogachev and Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Rogers and Williams, 2000, p. 110) V.1.3 では \\(S_n^+\\) の記法が用いられている．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．特に，古典力学や有限要素法の文献においては，二項積 の間の演算である二重点乗積を \\(:\\) で表したことから，この記法が用いられる．二項積については (Abraham et al., 1988, p. 341) も参照．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi), (Bakry et al., 2014, p. xv) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné and Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné and Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (V. I. Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(V. I. Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart and Wellner, 2023, p. 6) では 外確率 という．↩︎\n(V. I. Bogachev, 2007, p. 17) 定義1.5.1, (Vladimir I. Bogachev and Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (V. I. Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (V. I. Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(V. I. Bogachev, 2007, p. 23) に倣った．(V. I. Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie and Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(V. I. Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (V. I. Bogachev, 2007, p. 189) 参照．↩︎\n(Giné and Nickl, 2021, p. 16), (Vladimir I. Bogachev and Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 8) に倣った．(V. I. Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n(Nualart and Nualart, 2018) に倣った．(Giné and Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．(Giné and Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral and Penev, 2014), (Dellacherie and Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné and Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart and Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain and Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral and Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné and Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral and Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (V. I. Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(V. I. Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral and Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n可測空間を \\((E,\\mathcal{E})\\) で表すのは，(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) に倣った．↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay and Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(V. I. Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford and Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford and Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné and Nickl, 2021, p. 2), (Villani, 2009) に一致する．(V. I. Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral and Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(V. I. Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie and Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(V. I. Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan and Doucet, 2002) に一致する．(Dellacherie and Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Jacod and Shiryaev, 2003, p. 347), (Crisan and Doucet, 2002), (Ethier and Kurtz, 1986, p. 96), (V. I. Bogachev, 2007, p. 228) に一致する．(Kechris, 1995, p. 109), (Villani, 2009) はイタリックで \\(P(E)\\) と表す．↩︎\n(Pedersen, 1989, p. 72) に倣った．(V. I. Bogachev, 2007, p. 76) 第7.2節 では \\(\\mathcal{P}_r(X)\\) で表す．Radon 測度とは，内部正則性（＝緊密性） \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] を満たす Borel 測度をいう (V. I. Bogachev, 2007, pp. 68–69) 定義7.1.1, 7.1.4．↩︎\n(Kulik, 2018) が \\(\\mathcal{C}\\) で表すのに倣った．(Vladimir I. Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) では \\(\\Pi(\\mu,\\nu)\\) で，(Ethier and Kurtz, 1986, p. 96) では \\(\\mathcal{M}(\\mu,\\nu)\\) で，(Dudley, 2002, p. 420) 11.8節 は \\(M(\\mu,\\nu)\\)，(Figalli and Glaudo, 2023) では \\(\\Gamma(\\mu,\\nu)\\) で表す．(V. I. Bogachev, 2007, p. 235) 8.10(viii)節と (Villani, 2009, p. 95) 注6.5 に倣い，カップリングの元は Radon なものに限っている点に注意．↩︎\n(竹村彰道, 2020) の記法に一致する．↩︎\nDirac 測度とも言う．(Jacod and Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) などは \\(\\epsilon_x\\) で表す．(Protter, 2005, p. 299) は Dirac 関数を \\(\\delta_x\\) で表す．↩︎\n(Gerber et al., 2019) の記法に一致．分位点関数 (quantile function) (竹村彰道, 2020, p. 16)，確率表現関数 (森口繁一, 1995) などともいう．(Dudley, 2002, p. 283) は \\(X_F\\) とも表している．↩︎\n(Revuz and Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho and Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod and Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan and Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal and van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin and Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie and Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas and Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal and van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf and Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．(Heng et al., 2024) では \\(T=\\mu\\) という定値核の場合も同様の記法 \\(\\mu\\otimes S\\) を定義している．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) や (Protter, 2005, p. 52) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie and Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(V. I. Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (V. I. Bogachev, 2007, p. 262) 4.4節．(Dunford and Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral and Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg and Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(V. I. Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(V. I. Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier and Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné and Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie and Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\nこのような使い分けは (Nummelin, 1984, p. 1) に一致する．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart and Nualart, 2018, p. 31) に一致する．↩︎\n(Nualart and Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod and Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod and Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod and Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie and Meyer, 1978, p. 114), (Revuz and Yor, 1999, p. 42) に倣った．↩︎\n右連続性と完備性を併せて，フィルトレーション付き確率空間 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) の 通常の条件 ともいう．(Protter, 2005, p. 3) など参照．↩︎\n(Protter, 2005, p. 56)．↩︎\n(Dellacherie and Meyer, 1978) 49 115-IV では 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n(Jacod and Shiryaev, 2003, p. 4) 1.11，(Protter, 2005, p. 3) に従った．↩︎"
  },
  {
    "objectID": "static/ResearchJP.html",
    "href": "static/ResearchJP.html",
    "title": "研究紹介",
    "section": "",
    "text": "現状のモンテカルロ法の多く（Langevin Monte Carlo や Hamiltonian Monte Carlo など）は物理学的な由来を持ちますが，最も効率的な方法はそうではないかもしれません．\nコーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みをモンテカルロ法に取り入れることで，効率性をさらにあげることができるはずです．\nその第一歩が 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n\n\nZig-Zag サンプラー：非対称なダイナミクスを持つ MCMC アルゴリズムの例\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#mcmc-ダイナミクスの非対称化",
    "href": "static/ResearchJP.html#mcmc-ダイナミクスの非対称化",
    "title": "研究紹介",
    "section": "",
    "text": "現状のモンテカルロ法の多く（Langevin Monte Carlo や Hamiltonian Monte Carlo など）は物理学的な由来を持ちますが，最も効率的な方法はそうではないかもしれません．\nコーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みをモンテカルロ法に取り入れることで，効率性をさらにあげることができるはずです．\nその第一歩が 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n\n\nZig-Zag サンプラー：非対称なダイナミクスを持つ MCMC アルゴリズムの例\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#粒子輸送によるサンプリング",
    "href": "static/ResearchJP.html#粒子輸送によるサンプリング",
    "title": "研究紹介",
    "section": "粒子輸送によるサンプリング",
    "text": "粒子輸送によるサンプリング\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#smc-による軌道推定",
    "href": "static/ResearchJP.html#smc-による軌道推定",
    "title": "研究紹介",
    "section": "SMC による軌道推定",
    "text": "SMC による軌道推定\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\n2023-12-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html",
    "href": "static/Slides.html",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nReversible Jump Zig-Zag Sampler\n\n\nスライドはこちら．\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here.\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA MCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#upcomings-newests",
    "href": "static/Slides.html#upcomings-newests",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nReversible Jump Zig-Zag Sampler\n\n\nスライドはこちら．\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here.\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA MCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#history",
    "href": "static/Slides.html#history",
    "title": "Slides",
    "section": "History",
    "text": "History\n\n\n\n  \n    \n      1/10/2025.\n      司馬博文 .\n      \n        Zig-Zag サンプラーのモデル選択への応用\n        : Reversible Jump Zig-Zag Sampler\n      .\n      \n        中間評価・学生研究発表会.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n    \n  \n    \n      9/10/2024.\n      Hirofumi Shiba | 司馬博文.\n      \n        Zig-Zag Sampler\n        : A MCMC Game-Changer\n      .\n      \n        Seoul National University, Gwanak (관악) campus, South Korea.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n    \n  \n    \n      7/25/2024.\n      司馬博文 .\n      \n        Zig-Zag サンプラー\n        : 物理のくびきを超える MCMC\n      .\n      \n        ベイズ会（本郷キャンパス小島ホール第二セミナー室）.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Notations | Categories\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する． 分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる． また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n通常の回帰モデルは応答変数が連続であることが暗黙の仮定となっている． この節では，応答変数が質的変数である場合のモデリングを扱う． この場合でも多くのモデルが利用可能であり，その多くが一般化線型モデルの枠組みで統一的に扱うことができる． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n12/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である．今回は 公式の実装 を吟味する． \n\n\n\n\n\n10/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\nLorenz’ 63, Lorenz’ 96 とはそれぞれ [@Lorenz1963], [@Lorenz1995] によって導入された大気モデルである． 前者はバタフライ効果の語源ともなった，最初に特定されたカオス力学系でもある． Navier-Stokes 方程式は流体の運動を記述する方程式である． これらはいずれもデータ同化・軌道推定技術のベンチマークとして用いられている． ここでそれぞれのモデルの数学的性質と Julia を通じたシミュレーションの方法をまとめる． \n\n\n\n\n\n10/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし，古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． 一方で，モデルの仮定を前面に出したベイズ的な解析手法は，データを探索的に吟味することができ，極めて微妙な消息も捉えることが可能になる． 本稿では特にベイズ ANOVA 手法 [@Gelman2005], [@Rouder+2012] を採用して，そのモデルケースを実証する． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n応募法 (voluntary sampling) や多くのウェブアンケートは，確率標本抽出に該当しない．このような場合でも母集団に関する補助情報がある限り，バイアスを軽減し推定精度を高めることができる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n標本調査において欠測はつきものである．観測単位が欠測している場合 (unit nonresponse)，call-back や follow-up などの調査を行うか，それができない場合は 荷重校正 (calibration weighting) が可能である．一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． これらの問題点を解決策としてベイズの方法を導入し，ベイズ ANOVA，ベイズ推論とモデル比較が ANOVA の発展として得られることをみる． この拡張は，ANOVA の線型モデルとしての解釈を通じてなされ，ANOVA の「同じ係数を共有するクタスタ構造の特定手法」というより広い理解へ導かれる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n人間を対象にする介入の研究では，介入の前後で変化があったかが争点となる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能ですが，本稿では特に R からの利用方法をまとめます．\n\n\n\n\n\n9/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n本稿では生存時間解析の代表的なモデルを概観する． 特に医療技術評価への応用では，打ち切りデータを最もよく外挿できるハザードモデルが探索され，ベイズ推定が有効な方法としてよく選択される． 本稿では特に表現力の高い競合リスクモデルとして polyhazard model を紹介し，ベイズ推定の困難さを議論する．\n次稿ではこのモデルを Zig-Zag サンプラーでベイズ推定する方法を紹介する． \n\n\n\n\n\n9/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n最適輸送問題は変分法の黎明期に提案された変分問題の１つであるが，その発展は確率論の成熟を待つ必要があった．現代では多くの非正則な空間上に幾何学的な量を定義する普遍的な手法として理解されてから，多くのフィールズ賞受賞者を輩出する最も活発な分野の１つとなっている．ここまでの発展の歴史を本記事では概観したい．\n\n\n\n\n\n9/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．特に OU 過程の時間反転は雑音除去過程 (Denoising Diffusion) といい，サンプリングに利用されている．デノイジングスコアマッチングでは，時間反転に Hyvärinen スコアが出現することを利用してデータ分布のスコアを推定する．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．\n\n\n\n\n\n8/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\nSkilling-Hutchinson の跡推定量は，跡の計算 \\(O(d^2)\\) を \\(O(d)\\) に落とすことができる Monte Carlo 法である．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n行列の特異値分解とは，正方行列の直交対角化を一般の行列に拡張したものである．特異値を大きいものから \\(r\\) 個選ぶことで，Hilbert-Schmidt ノルムの意味で最適な \\(r\\)-階数近似が構成できる．このことは主成分分析に応用を持つ．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\nカーネル法とは，半正定値カーネルを用いてデータを Hilbert 空間内に埋め込むことで，非線型な変換を行う統一的な手法である．再生核 Hilbert 空間の理論により，写した先における内積は，半正定値カーネルの評価を通じて効率的に計算できるため，無限次元空間上での表現に対する tractable な手段を提供する．適切な半正定値カーネルを用いることで，データの「類似度」を定義することができる．本稿では半正定値カーネルの理論と距離学習法を扱う．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散模型は拡張性にも優れており，条件付けが容易である．現状は誘導付き拡散によってこれが実現されるが，連続的な条件付き生成のために，フローマッチングなる方法も提案された．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n画像と動画に関してだけでなく，化学分子の構造生成の分野でも拡散模型が state of the art となっている．これは，連続空間上だけでなく，グラフなどの離散空間上でも拡散模型が拡張されたことが大きい．本稿では，離散データを連続潜在空間に埋め込むことなく，直接離散空間上に拡散模型をデザインする方法をまとめる．\n\n\n\n\n\n8/09/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n\n\n\n\n\n8/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフロー学習手法は，画像と動画に関して 2024 年時点で最良の性能を誇る． これは統計的に言えば事後分布からの近似的サンプリングを実行していることに相当する． 近似的ではなく，正確に２つの分布を補間するような拡散過程を推定するためには Schrödinger 橋がある． Schrödinger 橋については 次稿 に譲るとし，本稿ではサンプラーとしての拡散モデルを復習する． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルは「データ過程をノイズに還元する Langevin ダイナミクスを時間反転する」という発想に基づいており，画像と動画の生成・条件付き生成タスクに関して 2024 年時点で最良の方法の１つである． この発想を正確なサンプリング法に昇華するためには，[@Deming-Stephan1940] の Iterative Proportional Fitting アルゴリズムを用いることができる． この方法は拡散モデルによる条件付き生成の加速法として [@Shi+2022] によって提案された． こうして得る拡散過程は Schrödinger Bridge とも呼ばれ，エントロピー最適輸送と深い関わりを持つ． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は normflows を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて， Ho et. al. [NeurIPS 33(2020)] による DDPM (Denoising Diffusion Probabilistic Model) の実装の概要を見る．DDPM は拡散模型の最初の例の１つであり，ノイズからデータ分布まで到達するフローを定める拡散過程（雑音除去過程）を，データをノイズにする拡散過程の時間反転として学習する方法である．画像や動画だけでなく，離散空間上でタンパク質などの構造生成でも state of the art の性能を示すモデルである．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n表現学習，非線型独立成分分析など，「生成」以外の潜在変数模型の応用法を横断してレビューする．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．\n\n\n\n\n\n7/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\nサンプリング，または Monte Carlo 法は，現代の統計学と機械学習において必要不可欠な道具となっている．それは一体どうしてだろうか？初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．初めに政治学における理想点解析の目的と役割を概観し，続いて多次元展開法と項目反応理論の２つの観点から理想点解析を眺める． \n\n\n\n\n\n7/16/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫制博士課程（正確には，総合研究大学院大学統計科学コース）を紹介します．同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷のような研究環境が整っていると言えるでしょう．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はそのような確率過程の汎函数の漸近展開に基づく計算方法を紹介します．確率変数の期待値を近似するのに Monte Carlo 法は普遍的な方法ですが，漸近展開が用いられる場合，その計算時間は比較にならないほど速くなります．\n\n\n\n\n\n5/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．本稿では Stan 言語の基本をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．鋭意開発中のパッケージですが，すでに広範なクラスの確率微分方程式のシミュレーションが可能です．本稿では基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行うための R パッケージである．本稿では，brms の基本的な使い方と，その実装を紹介する．\nまた，ランダム効果モデルとは何であるか，固定効果モデル・混合効果モデル・一般化推定方程式などとの違いをレビューする．ランダム効果の追加は縮小推定などの自動的な正則化を可能とする美点がある一方で，係数の不偏推定やロバスト推定に拘る場合はこれを避ける判断もあり得る． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論，確率論，さらにはデータ解析の中心に据えられるべき中心概念であると言えるかもしれない．例えば，カーネル法とは確率核に沿った埋め込みである．MCMC の性質も，本質的に確率核の性質が決定する．また確率核は，確率空間の圏の射となる．このように，多くのデータ解析手法の中核に位置する数学的本体たる「確率核」への入門を目指すのが本記事である．\n\n\n\n\n\n3/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次稿 で，\\(K\\)-平均アルゴリズムの model-aware な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す． \n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n生い立ち\n\n\n\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n1/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\nVSCode での LaTeX 環境構築に関するページ．\n\n\n\n\n\n12/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nMCMC\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\nFeynman-Kac モデルという物理モデルを定義し，逐次モンテカルロ法（粒子フィルター）をその Monte Carlo シミュレーション法として位置付けて解説した書籍である． 例として挙げられるトピックも物理学のものが多く，書籍のスタイルも物理学書のそれである． ここでは 1.1 節 “On the Origins of Feynman-Kac and Particle Models” の抄訳を通じて内容を概観したい． \n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．公式の ギャラリー も参照．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html",
    "href": "posts/2024/Survey/BayesRegression.html",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#はじめに",
    "href": "posts/2024/Survey/BayesRegression.html#はじめに",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\nベイズ線型回帰分析は多くのデータ解析における「最初の一歩」である．ベイズ回帰分析から始まるベイズのワークフローや，理論的な背景は次稿を参照：\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nここではベイズ回帰モデルに変数を増やしていく際の解釈の変化や，変数の選択の問題などの実際的な問題を扱う．\n\nlibrary(readxl)\nraw_df &lt;- read_excel(path)"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html",
    "href": "posts/2024/Survey/BDA3.html",
    "title": "ベイズデータ解析７",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#関連記事",
    "href": "posts/2024/Survey/BDA3.html#関連記事",
    "title": "ベイズデータ解析７",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html",
    "href": "posts/2024/Lifestyle/FixedRandom.html",
    "title": "変量効果と固定効果",
    "section": "",
    "text": "階層モデル \\[\ny_{ij}=\\theta_j+\\epsilon_{ij}\n\\tag{1}\\] \\[\n\\theta_j=\\mu+\\gamma_j\n\\tag{2}\\] を考える．\\(j\\in[J]\\) がグループ，\\(i\\in[n_j]\\) はそのグループに所属する単位とする．層別・クラスター抽出された標本，パネルデータなど，好きなように解釈してほしい．\n(1), (2) を階層モデルと呼ぶ理由は，(1) で \\(y_{ij}\\) が最小単位として回帰されているのと同時に，(2) でグループごとの変数 \\(\\theta_j\\) にもモデルが設定されているためである．個人単位とグループ単位の２つの階層で回帰モデルが設定されているために 階層モデル という．\n(1) は他の個人レベル説明変数を含んでいても良い \\[\ny_{ij}=x_{ij}^\\top\\beta+\\theta_j+\\epsilon_{ij}\n\\tag{3}\\] が，ここでは問題にしない．\n本稿で問題にするのは，グループレベル変数 \\(\\theta_j\\) に対する２つの扱い方：「変量効果」と「固定効果」の違いである．\nこの名称の違いが表すものとして，単にモデルの違いなのか，それともモデルは同じで単に推定方法の違いなのか，という点からすでに混乱が見られる．\n\n\n\n\n\n\n筆者の結論\n\n\n\nThere are only two ways to make sense of the terminology “fixed effects” and “random effects”:\n\nEconometric manner:\n\nIf \\(\\theta_j\\) is not correlated with the vector of covariates \\(x_{ij}\\), then it is a random effect. Otherwise, it is a fixed effect. Random effects can be estimated via Generalized Least Squares (GLS).\n\nBiostatic manner:\n\nIf we assume a super-population model on \\(\\theta_j\\), then it is a random effect. If we don’t assume any additional hierarchical structure on \\(\\theta_j\\) (or just a flat prior), then it is a fixed effect.\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#概要",
    "href": "posts/2024/Lifestyle/FixedRandom.html#概要",
    "title": "変量効果と固定効果",
    "section": "",
    "text": "階層モデル \\[\ny_{ij}=\\theta_j+\\epsilon_{ij}\n\\tag{1}\\] \\[\n\\theta_j=\\mu+\\gamma_j\n\\tag{2}\\] を考える．\\(j\\in[J]\\) がグループ，\\(i\\in[n_j]\\) はそのグループに所属する単位とする．層別・クラスター抽出された標本，パネルデータなど，好きなように解釈してほしい．\n(1), (2) を階層モデルと呼ぶ理由は，(1) で \\(y_{ij}\\) が最小単位として回帰されているのと同時に，(2) でグループごとの変数 \\(\\theta_j\\) にもモデルが設定されているためである．個人単位とグループ単位の２つの階層で回帰モデルが設定されているために 階層モデル という．\n(1) は他の個人レベル説明変数を含んでいても良い \\[\ny_{ij}=x_{ij}^\\top\\beta+\\theta_j+\\epsilon_{ij}\n\\tag{3}\\] が，ここでは問題にしない．\n本稿で問題にするのは，グループレベル変数 \\(\\theta_j\\) に対する２つの扱い方：「変量効果」と「固定効果」の違いである．\nこの名称の違いが表すものとして，単にモデルの違いなのか，それともモデルは同じで単に推定方法の違いなのか，という点からすでに混乱が見られる．\n\n\n\n\n\n\n筆者の結論\n\n\n\nThere are only two ways to make sense of the terminology “fixed effects” and “random effects”:\n\nEconometric manner:\n\nIf \\(\\theta_j\\) is not correlated with the vector of covariates \\(x_{ij}\\), then it is a random effect. Otherwise, it is a fixed effect. Random effects can be estimated via Generalized Least Squares (GLS).\n\nBiostatic manner:\n\nIf we assume a super-population model on \\(\\theta_j\\), then it is a random effect. If we don’t assume any additional hierarchical structure on \\(\\theta_j\\) (or just a flat prior), then it is a fixed effect.\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#サーベイ",
    "href": "posts/2024/Lifestyle/FixedRandom.html#サーベイ",
    "title": "変量効果と固定効果",
    "section": "2 サーベイ",
    "text": "2 サーベイ\n\n\n\n\n\n\n\n計量経済学では，\\(x_{ij}\\) と \\(\\theta_j\\) の相関の有無に関する仮定の違いと，これに起因する自然な後続の（最小自乗）推定法の違いである．\n\n\n\n\n\n2.1 計量経済学\n(Hansen, 2022) を参照して，計量経済学で主流な固定効果と変量効果の違いの解釈を見る．\n(17.7 Hansen, 2022, pp. 603–) では，固定効果モデルは (3) のような他の説明変数 \\(x_{ij}\\) が存在する場合，\\(x_{ij}\\) と \\(u_{i}\\) の相関が危惧される際に使える「推定手法」（あるいは推定量の名前）として導入される．\n\nIn the econometrics literature if the stochastic structure of \\(\\theta_j\\) is treated as unknown and possibly correlated with \\(x_{ij}\\) then \\(\\theta_j\\) is called a fixed effect. (Hansen, 2022, p. 604)\n\nその推定手法とは，within transformation \\(\\dot{-}\\) を \\[\n\\dot{y}_{ij}=y_{ij}-\\overline{y}_j,\\qquad\\overline{y}_j:=\\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij}\n\\] と定めて，この変換をデータに適用してから回帰 \\[\n\\dot{y}_{ij}=\\dot{x}_{ij}^\\top\\beta+\\dot{\\epsilon}_{ij}\n\\tag{4}\\] を解く．この変換は \\(\\theta_j\\) を消去するように出来ているため，\\(\\theta_j\\) の性質や \\(x_{ij}\\) との相関に依らずに係数 \\(\\beta\\) を推定できるというのである．\n一方で within transformation を施さずに直接 GLS 推定して得る推定量を変量効果推定量，OLS 推定して得る推定量を pooled OLS と呼ぶ (17.6 Hansen, 2022, pp. 601–603)．\n多くの場合，変量効果モデル (3) では \\(\\theta_j\\) と \\(x_{ij}\\) との無相関性の他に，誤差の均一性 \\[\n\\theta_j\\sim(0,\\sigma^2_\\theta)\n\\] も仮定されるため，同グループ内の残差に \\[\n\\mathrm{V}[\\theta_j+\\epsilon_{ij}]=\\begin{pmatrix}\n\\sigma^2_\\theta+\\sigma^2_\\epsilon&\\sigma^2_\\theta&\\cdots&\\sigma^2_\\theta\\\\\n\\sigma^2_\\theta&\\sigma^2_\\theta+\\sigma^2_\\epsilon&\\cdots&\\sigma^2_\\theta\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\sigma^2_\\theta&\\sigma^2_\\theta&\\cdots&\\sigma^2_\\theta+\\sigma^2_\\epsilon\n\\end{pmatrix}\n\\] という共分散構造が仮定されていることになる．このような設定では GLS 推定量が BLUE を与える (Hayashi, 2000, p. 55)．\n\n\n\n\n\n\n筆者の結論\n\n\n\n計量経済学で変量効果モデル・固定効果モデルと呼んだときは，\\(\\theta_j\\) と \\(x_{ij}\\) との相関の有無に関する仮定の違いで呼び分けている．\nそれぞれのモデルで自然な OLS 様推定量があり，「変量効果」「固定効果」という名称はそのまま推定手法の違いも指すようである（もはや仮定の違いから始まるワークフローの名前になっている）．\n\n\n\n\n2.2 Gelman の見解\n以上の違いをモデルの違いとして理解した場合，変動効果モデルは \\(\\theta_j\\) に無相関性や等分散性を初めとするモデルをおいており，固定効果モデルはモデリングを放棄している，とも見れる．\nGelman もこのような観点に立っているものと思われる．\n\nThe term fixed effects is used in contrast to random effects—but not in a consistent way! Fixed eﬀects are usually deﬁned as varying coeﬃcients that are not themselves modeled. (Gelman and Hill, 2006, p. 245)\n\nこれをよく理解するために，\\(0,1\\) の指示変数（ダミー変数）からなる計画行列 \\(X\\) を導入した (1) の別の定式化を考える： \\[\n\\boldsymbol{y}=X\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}\n\\tag{5}\\] \\[\nX=\\begin{pmatrix}\n\\boldsymbol{1}_{n_1}&O&O&O\\\\\nO&\\boldsymbol{1}_{n_2}&O&O\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nO&O&O&\\boldsymbol{1}_{n_J}\n\\end{pmatrix}\n\\] \\[\n\\boldsymbol{\\theta}=\\begin{pmatrix}\\theta_1\\\\\\vdots\\\\\\theta_J\\end{pmatrix},\\qquad\\left.\\boldsymbol{1}_{n_j}:=\\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\right\\}n_j\n\\]\nこれ以上何も考えず，(5) に対して OLS 推定を行うと，固定効果推定量と全く同じものが得られる (定理 17.1 Hansen, 2022, p. 610)．残差も同じである．\nすると Gelman による次の固定効果モデルの定義は，最終的に得られる MAP 推定量としては，計量経済学の文脈 2.1 のものと一致することになる：\n\n\n\n\n\n\n(15.2 節 Gelman et al., 2014, p. 383)\n\n\n\n独立な一様事前分布 \\[\n\\theta_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\mu,\\infty)\n\\] を仮定した場合の \\(\\theta_j\\) を 固定効果 と呼ぶ．\n階層的なモデル \\[\n\\theta_j=\\mu+\\gamma_j,\\qquad\\gamma_j\\sim\\mathrm{N}(0,\\sigma^2_\\theta)\n\\] を仮定した場合の \\(\\theta_j\\) を 変量効果 と呼ぶ．\n２つが同居するモデルを 混合効果モデル (mixed-effects model) という．\n\n\nこの語用法は，\\(\\theta_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\mu,\\infty)\\) の事前分布の下で MAP 推定した場合が，最尤推定量と一部の OLS 推定量と一致するため，計量経済学の文脈 2.1 のものと一致する．\nそれだけでなく変量効果モデルとは階層モデルであり，推定の際に縮小が働く．このような方法で推定されるモデルは疫学や生物統計学の分野でもよく使われてランダム効果モデルと呼ばれている（例えば (Robinson, 1991), (Solomon, 2005)）ため，この語用法とも一致することになる．\n以上の見解は (Gelman, 2005, p. 20) に端的に現れている：\n\nIn the Bayesian framework, this definition implies that fixed effects \\(\\theta_j\\) are estimated conditional on \\(\\sigma_\\theta=\\infty\\) and random effects \\(\\theta_j\\) are estimated conditional on \\(\\sigma_\\theta\\) from the posterior distribution. (Gelman, 2005, p. 20)\n\n\n\n2.3 ‘fix’, ‘random’ の名前の由来は？\nこの生物統計学的な語用法は，必ずしも fixed effects と対置するわけではない．グループ \\(j\\in[J]\\) に依存せず一定の値を取るいわば普通の「係数」と「変動係数」があるだけである．\n多くの場合，ランダム効果は局外母数であり，\\(\\beta\\) （多くの場合処置効果）の不偏推定が最大の目的である．計量経済学のように深刻な外生性が疑われる状況を扱うわけでもない．それゆえ階層モデルによる縮小推定が選好されるという背景もある．\n一方で計量経済学では効率が落ちても外生性への頑健性が選好されることが多いようである．\nすると次の語用法が出現し，‘fix’, ‘random’ の名前の由来も同時にうまく説明するように見える：\n\nEffects are ﬁxed if they are interesting in themselves or random if there is interest in the underlying population. (Searle et al., 1992, p. 7)\n\nだがこうするともはやモデルとしてどのような仮定の違いを表しているのかわからなくなり，専門用語というよりは日常用語である．\nそこで (Gelman, 2005, p. 21) ではこの意味では別の用語を採用するとしている：\n\nWe prefer to sidestep the overloaded terms “ﬁxed” and “random” with a cleaner distinction by simply renaming the terms. We deﬁne effects (or coefﬁcients) in a multilevel model as constant if they are identical for all groups in a population and varying if they are allowed to differ from group to group. (Gelman, 2005, p. 21)\n\n\n\n\n2.4 その他のベイジアンの見解\n(Hoff, 2009, p. 147) には次の記述がある：\n\nthe \\(\\theta_j\\)’s (or \\(\\gamma_j\\)’s) may be referred to as either “fixed effects” or “random effects” depending on how they are estimated.\n\nつまり推定手法の違いで呼び分けるとしている．\nこれを見ると，前節 2.2 の BDA が開陳しているような，（無理に）モデルの違いとして理解しようという立場はやや急進的であり，ここまでいくともはや「固定効果」「変量効果」という名称が意味を持たないとも思える．\n実際 (Gelman et al., 2014, p. 383) には次の注がある：\n\nThe terms ‘fixed’ and ‘random’ come from the non-Bayesian statistical tradition and are somewhat confusing in a Bayesian context where all unknown parameters are treated as ‘random’ or, equivalently, as having ﬁxed but unknown values.\n\nベイズの立場では潜在変数だろうがパラメータだろうが，未知である限り「固定」「変動」の区別はない，というのは正しいが，だとしたらこの名称は捨てるべきである．"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#footnotes",
    "href": "posts/2024/Lifestyle/FixedRandom.html#footnotes",
    "title": "変量効果と固定効果",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの生物統計学的な語用法は，必ずしも fixed effects と対置するわけではない．むしろ多くの文脈でランダム効果は局外母数であり，\\(\\beta\\) （多くの場合処置効果）の不偏推定が最大の目的である．計量経済学のように深刻な外生性が疑われる状況を扱うわけでもない．それゆえ階層モデルによる縮小推定が選好されるという背景もある．↩︎"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#筆者の結論",
    "href": "posts/2024/Lifestyle/FixedRandom.html#筆者の結論",
    "title": "変量効果と固定効果",
    "section": "3 筆者の結論",
    "text": "3 筆者の結論\n筆者は基本的には BDA の見解 2.2 を採用し，「固定効果」と「変動効果」の違いは，係数にモデルを想定しているかどうかの違いと理解することにした．\n「固定効果」と「変量効果」の語は（慣習を踏襲すべき場合を除いて）使わず，「変動係数」 (varying coefficient) と呼ぶつもりである．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#交差項",
    "href": "posts/2024/Survey/BayesRegression.html#交差項",
    "title": "ベイズ重回帰分析",
    "section": "2 交差項",
    "text": "2 交差項\n\nはじめに \\[\n\\texttt{BMI} = \\beta_0 + \\beta_{\\texttt{LAB}}\\cdot\\mathtt{LAB} + \\beta_{\\texttt{LDL}}\\cdot\\mathtt{LDL} + \\beta_{\\texttt{LAB:LDL}}\\cdot\\mathtt{LAB}\\cdot\\mathtt{LDL} + \\epsilon\n\\] \\[\n\\beta_0\\sim\\mathrm{t}(3;\\mu_0,3.4),\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2),\n\\] \\[\n\\beta_{\\texttt{LAB}},\\beta_{\\texttt{LDL}},\\beta_{\\texttt{LAB:LDL}}\\sim\\mathrm{N}(0,\\infty),\\qquad\\sigma\\sim\\mathrm{t}(3;0,3.4),\n\\] というモデルを考える．\n\nlibrary(brms)\nmodel1 &lt;- bf(\n  BMI ~ LAB\n)\nfit1 &lt;- brm(\n  formula = model1,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nlibrary(knitr)\nkable(get_prior(\n  formula = model1,\n  data = raw_df\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nLAB\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 22.7, 3.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 3.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nplot(fit1, variable = c(\"b_Intercept\", \"b_LAB\"))\n\n\n\n\n\n\n\n\n\nsummary(fit1)$fixed\n\n            Estimate Est.Error   l-95% CI  u-95% CI      Rhat Bulk_ESS Tail_ESS\nIntercept 20.6961127  0.428332 19.8724047 21.541559 1.0000894 9919.088 7202.522\nLAB        0.6119695  0.105872  0.4014823  0.817622 0.9998797 9249.283 7186.599\n\n\n\\(\\beta_{\\texttt{LAB}}\\) の最頻値＝最尤推定量は \\(0.6\\) である．これは，LAB が \\(1\\) 違う個人の間で BMI の値が約 \\(0.6\\) 違うと解釈できる．\n例えば LAB が \\(3.0\\) の個人の予測される BMI は \\[\n\\mathtt{BMI}\\approx20.7+0.6\\times3.0=22.5\n\\] となる．\nここに新たな変数 LDL を追加すると，LAB の係数 \\(\\beta_{\\texttt{LAB}}\\) は \\(0.6\\) から \\(0.5\\) に減少する．これはどういう意味だろうか？\n\nmodel2 &lt;- update(model1, BMI ~ LAB + LDL)\nfit2 &lt;- brm(\n  formula = model2,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\"))\n\n\n\n\n\n\n\n\n一般に係数の追加は層別に当たる．例えばこの結果は，LDL の値が同じ人の中では LAB が \\(1\\) 違う人の BMI の値が \\(0.5\\) 違うと解釈できる．\n\nsummary(fit2)$fixed\n\n              Estimate   Est.Error     l-95% CI    u-95% CI     Rhat  Bulk_ESS\nIntercept 20.166740267 0.510522239 1.916021e+01 21.16019758 1.000753 12221.847\nLAB        0.481395182 0.124645917 2.389870e-01  0.72592090 1.001291  9120.764\nLDL        0.008630676 0.004390414 1.012456e-04  0.01724088 1.000804  9642.207\n          Tail_ESS\nIntercept 7745.032\nLAB       6973.939\nLDL       7388.183\n\n\n\nmodel3 &lt;- update(model2, BMI ~ LAB * LDL)\nfit3 &lt;- brm(\n  formula = model3,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit3, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\", \"b_LAB:LDL\"))"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#線型重回帰",
    "href": "posts/2024/Survey/BayesRegression.html#線型重回帰",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "2 線型重回帰",
    "text": "2 線型重回帰\n\n2.1 ベイズ単回帰\n\nはじめに \\[\n\\texttt{BMI} = \\beta_0 + \\beta_{\\texttt{LAB}}\\cdot\\mathtt{LAB} + \\beta_{\\texttt{LDL}}\\cdot\\mathtt{LDL} + \\beta_{\\texttt{LAB:LDL}}\\cdot\\mathtt{LAB}\\cdot\\mathtt{LDL} + \\epsilon\n\\] \\[\n\\beta_0\\sim\\mathrm{t}(3;\\mu_0,3.4),\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2),\n\\] \\[\n\\beta_{\\texttt{LAB}},\\beta_{\\texttt{LDL}},\\beta_{\\texttt{LAB:LDL}}\\sim\\mathrm{N}(0,\\infty),\\qquad\\sigma\\sim\\mathrm{t}(3;0,3.4),\n\\] というモデルを考える．\n\nlibrary(brms)\nmodel1 &lt;- bf(\n  BMI ~ LAB\n)\nfit1 &lt;- brm(\n  formula = model1,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nlibrary(knitr)\nkable(get_prior(\n  formula = model1,\n  data = raw_df\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nLAB\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 22.7, 3.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 3.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nplot(fit1, variable = c(\"b_Intercept\", \"b_LAB\"))\n\n\n\n\n\n\n\n\n\nsummary(fit1)$fixed\n\n            Estimate Est.Error   l-95% CI  u-95% CI      Rhat Bulk_ESS Tail_ESS\nIntercept 20.6961127  0.428332 19.8724047 21.541559 1.0000894 9919.088 7202.522\nLAB        0.6119695  0.105872  0.4014823  0.817622 0.9998797 9249.283 7186.599\n\n\n\\(\\beta_{\\texttt{LAB}}\\) の最頻値＝最尤推定量は \\(0.6\\) である．これは，LAB が \\(1\\) 違う個人の間で BMI の値が約 \\(0.6\\) 違うと解釈できる．\n例えば LAB が \\(3.0\\) の個人の予測される BMI は \\[\n\\mathtt{BMI}\\approx20.7+0.6\\times3.0=22.5\n\\] となる．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\nしかしこの図を見ればわかる通り，LAB は BMI の変動の一部しか説明しておらず，上述ような点推定的な議論にどれほど意味があるかは疑問である．\nベイズ回帰では幅を持って結果を理解できるため，その美点を活かさない理由はない．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nsims &lt;- as.matrix(fit1)\nsims_to_display &lt;- sample(nrow(sims), 100)\nfor (i in sims_to_display) {\n  abline(sims[i, 1], sims[i, 2], col = \"gray\")\n}\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n\n\n\n2.2 モデルのチェック\nさらにベイズ模型は事後予測分布をプロットし，実際の観測データと比べることで，モデルがデータ生成過程をどれほど反映できているかが瞬時に把握できる：\n\nsynthetic_data &lt;- posterior_predict(fit1, newdata = data.frame(LAB = 3.0), ndraws = 10000)\nhist(synthetic_data, nclass = 100, xlab = \"BMI\", main = \"Predicted BMI for a person with LAB = 3.0\")\n\n\n\n\n\n\n\n\n\np1 &lt;- pp_check(fit1, ndraws = 100)\np1\n\n\n\n\n\n\n\n\n事後予測分布のプロットを見ると，モデルが取り逃がしている構造として，BMI の分布が左右で非対称であることがあることがわかる．回帰直線のプロット 図 1 を見ても，直線の上側の点の方が裾が広く分散している．\n「やせ」と「肥満」は対称ではないのである．\n残差をプロットすることでさらに明らかになる：\n\nres &lt;- residuals(fit1)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 2: 残差のプロット\n\n\n\n\n\n\n\n2.3 変数の追加\nここに新たな変数 LDL を追加すると，LAB の係数 \\(\\beta_{\\texttt{LAB}}\\) は \\(0.6\\) から \\(0.5\\) に減少する．これはどういう意味だろうか？\n\nmodel2 &lt;- update(model1, BMI ~ LAB + LDL)\nfit2 &lt;- brm(\n  formula = model2,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\"))\n\n\n\n\n\n\n\n\n一般に係数の追加は層別に当たる．例えばこの結果は，LDL の値が同じ人の中では LAB が \\(1\\) 違う人の BMI の値が \\(0.5\\) 違うと解釈できる．\n\nsummary(fit2)$fixed\n\n              Estimate   Est.Error     l-95% CI    u-95% CI     Rhat  Bulk_ESS\nIntercept 20.167095694 0.508697419 1.915738e+01 21.16817592 1.000156 12406.505\nLAB        0.480590230 0.123523343 2.345347e-01  0.72295784 1.000166  8773.965\nLDL        0.008655655 0.004414599 1.466805e-04  0.01750134 1.000146  8828.879\n          Tail_ESS\nIntercept 7872.585\nLAB       7388.178\nLDL       7208.248\n\n\nここで \\(\\beta_{\\texttt{LDL}}\\) の値が極めて小さいことに気づくかもしれない．これは LAB に比べて LDL の影響が小さいことを意味しない．なぜならばこの２つの変数はスケールが約 \\(10^2\\) 違うためである．LDL は 100 のスケール，LAB は 1 のスケールである．\n説明変数 LAB と LDL のどちらが重要か，どっちをモデルに含めるべきかは全く別の方法で議論する必要がある．\n\n\n2.4 データの正規化\nそこでデータを正規化してみる：\n\ndf &lt;- data.frame(\n  sBMI = scale(raw_df$BMI),\n  sLAB = scale(raw_df$LAB),\n  sLDL = scale(raw_df$LDL)\n)\n\nmodel2s &lt;- bf(sBMI ~ sLAB + sLDL)\nfit2s &lt;- brm(\n  formula = model2s,\n  data = df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2s, variable = c(\"b_Intercept\", \"b_sLAB\", \"b_sLDL\"))\n\n\n\n\n\n\n\n\n\nsummary(fit2s)$fixed\n\n              Estimate  Est.Error     l-95% CI   u-95% CI      Rhat Bulk_ESS\nIntercept 7.522651e-05 0.03425035 -0.068425590 0.06721458 1.0014686 9820.954\nsLAB      1.541776e-01 0.03921660  0.077864765 0.23129153 0.9998343 8961.051\nsLDL      7.719841e-02 0.04016989 -0.001446599 0.15438400 1.0000044 9394.795\n          Tail_ESS\nIntercept 7058.981\nsLAB      7946.039\nsLDL      8158.999\n\n\nデータを正規化してしまったため，直接的な係数の解釈はできないが，係数を相互に比較できる．\nまたその他のモデルの性質は変わらない．例えば事後予測分布も変わらない．\n\nlibrary(gridExtra)\np2s &lt;- pp_check(fit2s, ndraws = 100)\np2 &lt;- pp_check(fit2, ndraws = 100)\ngrid.arrange(p2, p2s, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n2.5 交差項の係数の解釈\n再び正規化する前のデータに戻る．\n\nmodel3 &lt;- update(model2, BMI ~ LAB * LDL)\nfit3 &lt;- brm(\n  formula = model3,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit3, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\", \"b_LAB:LDL\"))\n\n\n\n\n\n\n\n\n\nsummary(fit3)$fixed\n\n              Estimate   Est.Error     l-95% CI     u-95% CI     Rhat Bulk_ESS\nIntercept 18.343555254 1.604795320 15.143167287 21.369150246 1.000525 3244.231\nLAB        0.951188612 0.411125758  0.170115468  1.769360158 1.000082 3333.452\nLDL        0.023882438 0.013536050 -0.001720447  0.050917031 1.000414 3315.768\nLAB:LDL   -0.003783052 0.003173761 -0.010074600  0.002145523 1.000361 3166.146\n          Tail_ESS\nIntercept 4340.650\nLAB       4113.096\nLDL       4512.152\nLAB:LDL   3934.990\n\n\n交差項を含む線型回帰における係数の解釈はさらに限定的になる．\n\\(\\beta_{\\texttt{LAB}}\\) は LDL が \\(0\\) である人が仮にいたとした場合の，LAB が \\(1\\) 違う人の間の BMI の平均的な違いを表す，と解釈できる．（LDL の平均が \\(0\\) になるように変数変換をして回帰するともっと自然な解釈ができる）．\n\\(\\beta_{\\texttt{LAB:LDL}}\\) は片方の係数 \\(\\beta_{\\texttt{LAB}}\\) を固定した際，LDL が \\(1\\) だけ違うグループにおける係数 \\(\\beta_{\\texttt{LDL}}\\) との違いを表す．\nすなわち交差項の追加は，LDL に依って層別し，それぞれのグループに異なる \\(\\beta_{\\texttt{LAB}}\\) を推定することを可能にする．この点で階層モデリングに似ている．\n\n\n2.6 交差項\n交差項 LAB*LDL の追加は，LDL の違うサブグループの間に異なる LAB をフィッティングすることを可能にする．\nこのことを最もよく見るには，LDL が上半分か下半分かで LAB の係数がどう変わるかを見るのが良い．\n\nraw_df$LDLcate2 &lt;- ifelse(raw_df$LDL &gt; median(raw_df$LDL), \"High\", \"Low\")\n\n\nmodel3_cate &lt;- bf(BMI ~ LAB * LDLcate2)\nfit3_cate &lt;- brm(\n  formula = model3_cate,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nb_hat &lt;- summary(fit3_cate)$fixed\nabline(b_hat[1,1], b_hat[2,1], col = \"red\")\nabline(b_hat[1,1] + b_hat[3,1], b_hat[2,1] + b_hat[4,1], col = \"blue\")\nlegend(\"topleft\", # または \"topright\", \"bottomleft\", \"bottomright\" など\n       legend = c(\"High\", \"Low\"),\n       col = c(\"red\", \"blue\"),\n       lty = 1)\n\n\n\n\n\n\n\n\nLDL が大きいと，LAB の BMI に与える影響は緩やかになることがわかる．LDL の方が LAB の代わりに BMI の増加を説明してしまっているとも考えられる．\n\n\n2.7 まとめ\n線型回帰において，説明変数の追加は，「他の説明変数を固定したグループ内での」係数の推定に変化する（階層モデリングにつながる見方）．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#文献紹介",
    "href": "posts/2024/Survey/BayesRegression.html#文献紹介",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "5 文献紹介",
    "text": "5 文献紹介\n\n(10 節 Gelman et al., 2020) に線型重回帰モデルにおいて，係数の解釈法が丁寧に解説されている．\n(11 節 Gelman et al., 2020) はモデルの検証法を扱っている．\n(Gelman et al., 2020) では基本的に rstanarm パッケージを用いているが，本稿では brms パッケージを用いた．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#はじめに",
    "href": "posts/2024/Survey/BDA3.html#はじめに",
    "title": "ベイズデータ解析７",
    "section": "1 はじめに",
    "text": "1 はじめに\nデータが自然な 階層構造 を持つとする．例えば標本がクラスター抽出された場合，パネルデータや経時的繰り返し観測による標本など，自然な階層構造を持つデータは多い．\nこのようなデータに対して，個々のサブグループに対して全く同じ回帰モデルを繰り返し適用し，結果のプロットを小窓に分割して並べることで，グループごとの効果の違いを比較することができる．\nこの方法はナイーブながらも有力で，(10.9 節 Gelman et al., 2020, p. 148) では “secret weapon” と呼んでいる．\nこの別々の回帰モデルはベイズ的に統合して推論することができる．これが（ベイズ）階層モデル である．\n一般にベイズは点推定と違い，確度が低いモデルも捨てずに推定に利用するため，より好ましい統計的推定を実行できる．これは 縮小推定 のキーワードの下でも追及されている．"
  },
  {
    "objectID": "posts/2024/Survey/BDA4.html",
    "href": "posts/2024/Survey/BDA4.html",
    "title": "ベイズデータ解析８",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA4.html#関連記事",
    "href": "posts/2024/Survey/BDA4.html#関連記事",
    "title": "ベイズデータ解析８",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#コピュラモデル",
    "href": "posts/2024/Survey/BDA2.html#コピュラモデル",
    "title": "ベイズデータ解析６",
    "section": "3 コピュラモデル",
    "text": "3 コピュラモデル"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#文献紹介",
    "href": "posts/2024/Survey/BDA2.html#文献紹介",
    "title": "ベイズデータ解析６",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n(Gelman et al., 2014) の 16 章で一般化線型モデルが扱われている．(Kruschke, 2015) はさらに詳しく，22 章で名目応答，23 章で順序応答，24 章でカウントデータを扱っている．\n(12 章 Hoff, 2009) にて正規コピュラモデルが ordinal probit モデルの，潜在変数を多次元に拡張した場合として導入されている．\n(Chib and Winkelmann, 2001) はリンクを対数関数とし，Poisson 周辺分布を持つカウントデータのコピュラモデリングを実行している．\n(Quinn, 2017) は連続確率変数に対してコピュラモデリングを実行している．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#モデル検証",
    "href": "posts/2024/Survey/BayesRegression.html#モデル検証",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "3 モデル検証",
    "text": "3 モデル検証\n\n3.1 はじめに\n残差プロットや事後予測プロットによるモデルの検証は，解析と並行して見てきた．\nここではより詳細に，モデルの予測性能に基づいた検証・比較方法を見る．\n交差検証法によるスコア elpd_loo によるモデル比較が一つ推奨される．\n\n\n3.2 決定係数\n図 1 の回帰直線のプロットと 図 2 の残差プロットを見ると，残差がまだ構造を持っていることがわかる．\n\nres &lt;- residuals(fit3)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 3: fit3 の残差のプロット\n\n\n\n\n\nこの残差は標本分散 \\(\\widehat{\\sigma}^2\\)\n\nsigma &lt;- sqrt(var(res)[1,1])\nprint(sigma)\n\n[1] 3.394462\n\n\nを持っている．\nひとまず LAB と LDL について回帰をすることで，データの変動がどれほど説明できたかを考えてみよう．\n\\[\nR^2:=1-\\frac{\\widehat{\\sigma}^2}{s_y^2}=\\frac{s_y^2-\\widehat{\\sigma}^2}{s_y^2}\n\\]\nという値は 決定係数 と呼ばれ，データ \\(y\\) の分散 \\(s_y^2\\) のうち「説明された分散」の割合を表す．1\n\n1-sigma^2/var(raw_df$BMI)\n\n[1] 0.04403279\n\n\nデータの変動の \\(4\\%\\) しか説明できていないことがわかる．\n\n\n3.3 ベイズ決定係数\nベイズ決定係数 (Andrew Gelman and Vehtari, 2019) は brms パッケージで次のように計算できる：\n\nbayes_R2(fit3)\n\n     Estimate  Est.Error       Q2.5      Q97.5\nR2 0.04735052 0.01359373 0.02345481 0.07669648\n\n\n以上の \\(R^2\\) の議論では係数を点推定して「残差」を議論していた．\n他の可能性も考慮して推定を改善するのがベイズのやり方である．\nベイズ決定係数は，事後予測分布からのサンプルを用いて複数回予測値 \\(\\widehat{y}_i\\) を計算し， \\[\nR^2_{\\texttt{Bayes}}:=\\frac{\\mathrm{V}[\\widehat{y}]}{\\mathrm{V}[\\widehat{y}]+\\sigma^2}\n\\] という値で「データの変動のうち説明された割合」を表す．\n\n\n3.4 AIC\n(Akaike, 1974) は次のように定義される： \\[\n\\mathtt{AIC}=-2\\biggr(\\sup_\\theta\\log p(y|\\theta)\\biggl)+2p.\n\\]\n第１項は deviance とも呼ばれ，残差を表す．\nAIC は新たなデータ点が観測された際の，そのデータ点に対するデビアンス（ある種の損失）の推定量となっており，小さいほどよい．\nAIC と同様の推定を，計算機集約的に行う方法に次節の交差検証法がある：\n\n\n3.5 交差検証\n事後予測検証では事後分布と観測を比較したが，よりこの好ましくは新しい（推定に用いていない）データと突き合わせることである．\nLOO (Leave-One-Out) 交差検証 (Stone, 1974) では，データを１つだけ抜いてモデルを推定し，このモデルの予測値と実際の値を比較するモデル検証法である．\nbrms パッケージでは loo パッケージ を内部で利用して高速に計算することができる．\n\nloo(fit3)\n\n\nComputed from 10000 by 839 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -2220.7 26.8\np_loo         5.6  0.7\nlooic      4441.4 53.5\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n回帰分析において予測値と実際の（省いていた）データの乖離は，AIC を踏襲して事後予測分布のスコア関数で測る (Vehtari et al., 2017, p. 1414)．\nelpd (expected log predictive density) は，LOO 交差検証により得る，（省いていた）データの対数尤度の平均である： \\[\n\\mathrm{elpd}_{\\text{loo}}=\\sum_{i=1}^n\\log p(y_i|y_{-i})=\\sum_{i=1}^n\\int p(y_i|\\theta)p(\\theta|y_{-i})\\,d\\theta.\n\\]\nこの値が大きいほどモデルの予測が良い．一般に elpd は，一度見たことあるデータ点に対する事後予測スコアよりも低くなる．2 この際の差は p_loo が測っており，乖離が大きすぎるとモデルがデータに過適合していることを表す．\np_loo は有効パラメータ数の（一致）推定量である．今回のモデルには切片項と LAB, LDL, LAB:LDL そして sigma の５つのパラメータがあるが，それより \\(0.6\\) だけ大きい値が出ている．\n最後の列は情報量規準のスケールにしたものである： \\[\n\\mathtt{looic}=-2\\times\\mathrm{elpd}_{\\text{loo}}.\n\\]\n一般に LOO-CV は計算が大変であるが，loo パッケージは Pareto Smoothed Importance Sampling (PSIS) (Vehtari et al., 2024) を用いて高速に計算している．\n\\(k&gt;0.7\\) の場合はこれがうまくいっていないことを示唆する．この下で \\(\\mathtt{p_loo}&gt;p\\) はモデルの誤特定を示唆する．\n\n\n3.6 事後予測スコアによるモデル比較\nbrms パッケージでは loo_compare 関数で２つのモデルの elpd スコアを比較できる：\n\nloo_compare(loo(fit1), loo(fit2))\n\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit1 -0.9       2.1   \n\n\nelpd_diff の値は標準偏差と比べて大変に小さい．交差検証の観点からは，LDL の追加は BMI の予測の観点から全く違いがないことがわかる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#footnotes",
    "href": "posts/2024/Survey/BayesRegression.html#footnotes",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n決定係数は，回帰が OLS 推定量により推定された場合には \\(Y\\) と説明変数のベクトル \\(X\\) との間の 重相関係数 とも一致する (Mudholkar, 2014)．(Gelman et al., 2020, p. 168) も参照．一般に決定係数は母集団の多重相関係数の一致推定量だと見れる．こうみた場合のバイアスを低減・脱離した値に，自由度調整済み決定係数，(Olkin and Pratt, 1958) 推定量などが存在する．↩︎\n訓練に用いたデータ点に関する事後予測スコアはdeviance と関係があり，\\(-2\\) を乗じたものは deviance と同じスケールになる (Gelman et al., 2020, p. 174), (Vehtari et al., 2017, p. 1427)．AIC は deviance から \\(2p\\) を引いたものである (Gelman et al., 2020, p. 175)．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#非線型性への憧憬",
    "href": "posts/2024/Survey/BayesRegression.html#非線型性への憧憬",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "4 非線型性への憧憬",
    "text": "4 非線型性への憧憬\n\n4.1 はじめに\nLAB による BMI への回帰の残差には，左右の非対称性が見られる 2.2．\nそして更なる変数 LDL の追加は予測の観点では特に影響がないことがわかった 3.6．\nそこでここでは手軽に非線型性を取り入れる方法として，データを変換することを考える．\n第 2.4 節でデータを標準化すると回帰モデルの係数の解釈が容易になることみた．\nしかしデータの線型変換は線型回帰モデルを変えず，推定には何の影響も与えない．\nそこでここでは非線型な変換に注目する．\n\n\n4.2 線型回帰モデルという仮定\n線型回帰モデリングにおける最大の仮定は，説明変数の加法性と，\\(y\\) への効果の線型性である： \\[\ny=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots.\n\\]\nLAB や LDL の BMI への影響が線型であると見るのは相当横暴な仮定である．LAB と LDL が両方高いことが相乗的に BMI を高めるシナリオの方があり得そうである．\nBMI も LAB も LDL も正の値しか取らないこともあり，対数変換を考えることは良い第一歩だろう．\n\nmodel1_log &lt;- update(model1, log(BMI) ~ LAB)\nfit1_log &lt;- brm(\n  formula = model1_log,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nres &lt;- residuals(fit1_log)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals (on log scale)\")\nabline(0,0)\n\n\n\n\n\n\n\n\n残差プロットを見ると，元のスケールでの線型回帰 図 2 と比べて非対称性は軽減している．\n\n\nlibrary(bayesplot)\n\n\nyrep &lt;- posterior_predict(fit1_log, ndraws = 100)\np1_log &lt;- ppc_dens_overlay(raw_df$BMI, exp(yrep))\ngrid.arrange(p1, p1_log, nrow=1)\n\n\n\n\n\n\n\n\n事後予測分布も改善しているのがわかる．\nただし，説明変数を対数スケールに変換してしまったので，直接 LOO スコアを比較することはできないことに注意する：\n\nloo_compare(loo(fit1), loo(fit1_log))\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n         elpd_diff se_diff\nfit1_log     0.0       0.0\nfit1     -2657.0       8.2\n\n\n\n\n4.3 log-log モデル\n同様に説明変数にも対数変換を施すことができる．\n\nmodel1_loglog &lt;- update(model1_log, log(BMI) ~ log(LAB))\nfit1_loglog &lt;- brm(\n  formula = model1_loglog,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nyrep &lt;- posterior_predict(fit1_loglog, ndraws = 100)\np1_loglog &lt;- ppc_dens_overlay(raw_df$BMI, exp(yrep))\ngrid.arrange(p1_log, p1_loglog, nrow=1)\n\n\n\n\n\n\n\n\n予測性能はわずかに悪化していることがわかる：\n\nloo_compare(loo(fit1_log), loo(fit1_loglog))\n\n            elpd_diff se_diff\nfit1_log     0.0       0.0   \nfit1_loglog -1.8       1.1   \n\n\n\nmedian(loo_R2(fit1_log))\n\n[1] 0.02487841\n\nmedian(loo_R2(fit1_loglog))\n\n[1] 0.0223436\n\n\nしかし log-log モデルの係数は弾力性 (elasticity) としての解釈ができることもあり，解釈性の観点から選好されることがある (Gelman et al., 2020, p. 195)．\n\n\n4.4 離散変数化\n離散変数の扱いは連続変数よりも難しくなる．基本的に一般化線型モデルの枠組みが必要になる．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nしかし連続変数間の関数関係が複雑だと思われる場合，ノンパラメトリック手法を求める前に，変数を離散化して解析することも得るものが大きい場合が多い．\nこのような設定は，離散変数を扱う積極的な理由になる．非線型性を扱うために設定を簡略化するのである．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html",
    "href": "posts/2024/Survey/BayesGLM.html",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#はじめに",
    "href": "posts/2024/Survey/BayesGLM.html#はじめに",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\n多くの社会的なデータは非数値的である．加えて，線型回帰分析の結果複雑な非線型関係が予期された際，本格的なノンパラメトリック推論に移る前に，離散変数の設定に換言して非線型性を扱いやすくするなど，離散変数を扱う積極的理由もある．\n本稿ではロジスティック回帰を主に扱う．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Computation/brms.html",
    "href": "posts/2024/Computation/brms.html",
    "title": "R によるベイズ混合モデリング入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nダウンロードは：1"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#例で学ぶ-brms-の使い方",
    "href": "posts/2024/Computation/brms.html#例で学ぶ-brms-の使い方",
    "title": "R によるベイズ混合モデリング入門",
    "section": "1 例で学ぶ brms の使い方",
    "text": "1 例で学ぶ brms の使い方\n\n1.1 カウントデータのモデリング\nDocumentation で紹介されている，Epilepsy Seizures Data (Leppik et al., 1987)，(Thall and Vail, 1990) を用いた 例 を実行してみる：\nlibrary(brms)\nfit1 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            data = epilepsy, family = poisson())\nてんかん (epilepsy) 患者の発作回数countを被説明変数とし，処置の効果を表す説明変数Trtと患者毎のランダムな切片項(1|patient)，及びzAge,zBaseへの依存構造を調べたい．\n被説明変数countは離散変数であり，Poisson 分布に従うと仮定する．\n\n\n\n\n\n\n説明変数\n\n\n\n\nzAge：標準化された年齢\nzBase：ベースの発作回数\nTrt：治療の有無を表す２値変数\n(1|patient)：患者ごとに異なるとした切片項\n\nzBase * Trtという記法は，この２つの交互作用もモデルに含めることを意味する．\n\n\n\n\n\n\n\n\nデータの詳細\n\n\n\nepilepsyは59 人の患者に関して，４回の入院時の発作回数を記録した，全 236 データからなる．patientが患者を識別する ID であり，(1|patient)は患者ごとのランダム効果ということになる．\n\n\n従って本モデルはzAge, zBase, Trt, Trt*zBaseという固定効果，(1|patient)というランダム効果を取り入れた（一般化線型）混合効果モデル ということになり，回帰式は次の通りである： \\[\ny_{it} = \\beta_1 \\cdot\\texttt{zAge}_i+ \\beta_2 \\cdot \\texttt{zBase}_i + \\beta_3 \\cdot \\texttt{Trt}_i\n\\] \\[\n+ \\beta_4 \\cdot (\\texttt{zBase}_i \\cdot \\texttt{Trt}_i) + \\alpha_i +\\epsilon_{it}.\n\\] ただし，\\(\\texttt{count}_{it}\\) の Poisson 母数を \\(\\lambda_{it}\\) として，\\(y_{it}:=\\log(\\lambda_{it})\\) とした．\n解析意図としては，ベースの発作回数が高いほど，治療効果が高い／低いのではないか？という仮説を検証する，zBase*Trtを曝露因子としたモデルである．\n\n\n\n\n\n\n全データ\n\n\n\n\n\n\nepilepsy\n\n    Age Base Trt patient visit count obs        zAge        zBase\n1    31   11   0       1     1     5   1  0.42499501 -0.757172825\n2    30   11   0       2     1     3   2  0.26528351 -0.757172825\n3    25    6   0       3     1     2   3 -0.53327400 -0.944403322\n4    36    8   0       4     1     4   4  1.22355252 -0.869511123\n5    22   66   0       5     1     7   5 -1.01240850  1.302362646\n6    29   27   0       6     1     5   6  0.10557201 -0.158035233\n7    31   12   0       7     1     6   7  0.42499501 -0.719726725\n8    42   52   0       8     1    40   8  2.18182153  0.778117253\n9    37   23   0       9     1     5   9  1.38326402 -0.307819631\n10   28   10   0      10     1    14  10 -0.05413949 -0.794618924\n11   36   52   0      11     1    26  11  1.22355252  0.778117253\n12   24   33   0      12     1    12  12 -0.69298550  0.066641363\n13   23   18   0      13     1     4  13 -0.85269700 -0.495050128\n14   36   42   0      14     1     7  14  1.22355252  0.403656259\n15   26   87   0      15     1    16  15 -0.37356249  2.088730734\n16   26   50   0      16     1    11  16 -0.37356249  0.703225054\n17   28   18   0      17     1     0  17 -0.05413949 -0.495050128\n18   31  111   0      18     1    37  18  0.42499501  2.987437121\n19   32   18   0      19     1     3  19  0.58470651 -0.495050128\n20   21   20   0      20     1     3  20 -1.17212000 -0.420157930\n21   29   12   0      21     1     3  21  0.10557201 -0.719726725\n22   21    9   0      22     1     3  22 -1.17212000 -0.832065024\n23   32   17   0      23     1     2  23  0.58470651 -0.532496228\n24   25   28   0      24     1     8  24 -0.53327400 -0.120589134\n25   30   55   0      25     1    18  25  0.26528351  0.890455552\n26   40    9   0      26     1     2  26  1.86239852 -0.832065024\n27   19   10   0      27     1     3  27 -1.49154300 -0.794618924\n28   22   47   0      28     1    13  28 -1.01240850  0.590886756\n29   18   76   1      29     1    11  29 -1.65125450  1.676823640\n30   32   38   1      30     1     8  30  0.58470651  0.253871861\n31   20   19   1      31     1     0  31 -1.33183150 -0.457604029\n32   30   10   1      32     1     3  32  0.26528351 -0.794618924\n33   18   19   1      33     1     2  33 -1.65125450 -0.457604029\n34   24   24   1      34     1     4  34 -0.69298550 -0.270373532\n35   30   31   1      35     1    22  35  0.26528351 -0.008250835\n36   35   14   1      36     1     5  36  1.06384102 -0.644834526\n37   27   11   1      37     1     2  37 -0.21385099 -0.757172825\n38   20   67   1      38     1     3  38 -1.33183150  1.339808745\n39   22   41   1      39     1     4  39 -1.01240850  0.366210159\n40   28    7   1      40     1     2  40 -0.05413949 -0.906957223\n41   23   22   1      41     1     0  41 -0.85269700 -0.345265731\n42   40   13   1      42     1     5  42  1.86239852 -0.682280626\n43   33   46   1      43     1    11  43  0.74441801  0.553440656\n44   21   36   1      44     1    10  44 -1.17212000  0.178979662\n45   35   38   1      45     1    19  45  1.06384102  0.253871861\n46   25    7   1      46     1     1  46 -0.53327400 -0.906957223\n47   26   36   1      47     1     6  47 -0.37356249  0.178979662\n48   25   11   1      48     1     2  48 -0.53327400 -0.757172825\n49   22  151   1      49     1   102  49 -1.01240850  4.485281100\n50   32   22   1      50     1     4  50  0.58470651 -0.345265731\n51   25   41   1      51     1     8  51 -0.53327400  0.366210159\n52   35   32   1      52     1     1  52  1.06384102  0.029195264\n53   21   56   1      53     1    18  53 -1.17212000  0.927901651\n54   41   24   1      54     1     6  54  2.02211002 -0.270373532\n55   32   16   1      55     1     3  55  0.58470651 -0.569942327\n56   26   22   1      56     1     1  56 -0.37356249 -0.345265731\n57   21   25   1      57     1     2  57 -1.17212000 -0.232927432\n58   36   13   1      58     1     0  58  1.22355252 -0.682280626\n59   37   12   1      59     1     1  59  1.38326402 -0.719726725\n60   31   11   0       1     2     3  60  0.42499501 -0.757172825\n61   30   11   0       2     2     5  61  0.26528351 -0.757172825\n62   25    6   0       3     2     4  62 -0.53327400 -0.944403322\n63   36    8   0       4     2     4  63  1.22355252 -0.869511123\n64   22   66   0       5     2    18  64 -1.01240850  1.302362646\n65   29   27   0       6     2     2  65  0.10557201 -0.158035233\n66   31   12   0       7     2     4  66  0.42499501 -0.719726725\n67   42   52   0       8     2    20  67  2.18182153  0.778117253\n68   37   23   0       9     2     6  68  1.38326402 -0.307819631\n69   28   10   0      10     2    13  69 -0.05413949 -0.794618924\n70   36   52   0      11     2    12  70  1.22355252  0.778117253\n71   24   33   0      12     2     6  71 -0.69298550  0.066641363\n72   23   18   0      13     2     4  72 -0.85269700 -0.495050128\n73   36   42   0      14     2     9  73  1.22355252  0.403656259\n74   26   87   0      15     2    24  74 -0.37356249  2.088730734\n75   26   50   0      16     2     0  75 -0.37356249  0.703225054\n76   28   18   0      17     2     0  76 -0.05413949 -0.495050128\n77   31  111   0      18     2    29  77  0.42499501  2.987437121\n78   32   18   0      19     2     5  78  0.58470651 -0.495050128\n79   21   20   0      20     2     0  79 -1.17212000 -0.420157930\n80   29   12   0      21     2     4  80  0.10557201 -0.719726725\n81   21    9   0      22     2     4  81 -1.17212000 -0.832065024\n82   32   17   0      23     2     3  82  0.58470651 -0.532496228\n83   25   28   0      24     2    12  83 -0.53327400 -0.120589134\n84   30   55   0      25     2    24  84  0.26528351  0.890455552\n85   40    9   0      26     2     1  85  1.86239852 -0.832065024\n86   19   10   0      27     2     1  86 -1.49154300 -0.794618924\n87   22   47   0      28     2    15  87 -1.01240850  0.590886756\n88   18   76   1      29     2    14  88 -1.65125450  1.676823640\n89   32   38   1      30     2     7  89  0.58470651  0.253871861\n90   20   19   1      31     2     4  90 -1.33183150 -0.457604029\n91   30   10   1      32     2     6  91  0.26528351 -0.794618924\n92   18   19   1      33     2     6  92 -1.65125450 -0.457604029\n93   24   24   1      34     2     3  93 -0.69298550 -0.270373532\n94   30   31   1      35     2    17  94  0.26528351 -0.008250835\n95   35   14   1      36     2     4  95  1.06384102 -0.644834526\n96   27   11   1      37     2     4  96 -0.21385099 -0.757172825\n97   20   67   1      38     2     7  97 -1.33183150  1.339808745\n98   22   41   1      39     2    18  98 -1.01240850  0.366210159\n99   28    7   1      40     2     1  99 -0.05413949 -0.906957223\n100  23   22   1      41     2     2 100 -0.85269700 -0.345265731\n101  40   13   1      42     2     4 101  1.86239852 -0.682280626\n102  33   46   1      43     2    14 102  0.74441801  0.553440656\n103  21   36   1      44     2     5 103 -1.17212000  0.178979662\n104  35   38   1      45     2     7 104  1.06384102  0.253871861\n105  25    7   1      46     2     1 105 -0.53327400 -0.906957223\n106  26   36   1      47     2    10 106 -0.37356249  0.178979662\n107  25   11   1      48     2     1 107 -0.53327400 -0.757172825\n108  22  151   1      49     2    65 108 -1.01240850  4.485281100\n109  32   22   1      50     2     3 109  0.58470651 -0.345265731\n110  25   41   1      51     2     6 110 -0.53327400  0.366210159\n111  35   32   1      52     2     3 111  1.06384102  0.029195264\n112  21   56   1      53     2    11 112 -1.17212000  0.927901651\n113  41   24   1      54     2     3 113  2.02211002 -0.270373532\n114  32   16   1      55     2     5 114  0.58470651 -0.569942327\n115  26   22   1      56     2    23 115 -0.37356249 -0.345265731\n116  21   25   1      57     2     3 116 -1.17212000 -0.232927432\n117  36   13   1      58     2     0 117  1.22355252 -0.682280626\n118  37   12   1      59     2     4 118  1.38326402 -0.719726725\n119  31   11   0       1     3     3 119  0.42499501 -0.757172825\n120  30   11   0       2     3     3 120  0.26528351 -0.757172825\n121  25    6   0       3     3     0 121 -0.53327400 -0.944403322\n122  36    8   0       4     3     1 122  1.22355252 -0.869511123\n123  22   66   0       5     3     9 123 -1.01240850  1.302362646\n124  29   27   0       6     3     8 124  0.10557201 -0.158035233\n125  31   12   0       7     3     0 125  0.42499501 -0.719726725\n126  42   52   0       8     3    21 126  2.18182153  0.778117253\n127  37   23   0       9     3     6 127  1.38326402 -0.307819631\n128  28   10   0      10     3     6 128 -0.05413949 -0.794618924\n129  36   52   0      11     3     6 129  1.22355252  0.778117253\n130  24   33   0      12     3     8 130 -0.69298550  0.066641363\n131  23   18   0      13     3     6 131 -0.85269700 -0.495050128\n132  36   42   0      14     3    12 132  1.22355252  0.403656259\n133  26   87   0      15     3    10 133 -0.37356249  2.088730734\n134  26   50   0      16     3     0 134 -0.37356249  0.703225054\n135  28   18   0      17     3     3 135 -0.05413949 -0.495050128\n136  31  111   0      18     3    28 136  0.42499501  2.987437121\n137  32   18   0      19     3     2 137  0.58470651 -0.495050128\n138  21   20   0      20     3     6 138 -1.17212000 -0.420157930\n139  29   12   0      21     3     3 139  0.10557201 -0.719726725\n140  21    9   0      22     3     3 140 -1.17212000 -0.832065024\n141  32   17   0      23     3     3 141  0.58470651 -0.532496228\n142  25   28   0      24     3     2 142 -0.53327400 -0.120589134\n143  30   55   0      25     3    76 143  0.26528351  0.890455552\n144  40    9   0      26     3     2 144  1.86239852 -0.832065024\n145  19   10   0      27     3     4 145 -1.49154300 -0.794618924\n146  22   47   0      28     3    13 146 -1.01240850  0.590886756\n147  18   76   1      29     3     9 147 -1.65125450  1.676823640\n148  32   38   1      30     3     9 148  0.58470651  0.253871861\n149  20   19   1      31     3     3 149 -1.33183150 -0.457604029\n150  30   10   1      32     3     1 150  0.26528351 -0.794618924\n151  18   19   1      33     3     7 151 -1.65125450 -0.457604029\n152  24   24   1      34     3     1 152 -0.69298550 -0.270373532\n153  30   31   1      35     3    19 153  0.26528351 -0.008250835\n154  35   14   1      36     3     7 154  1.06384102 -0.644834526\n155  27   11   1      37     3     0 155 -0.21385099 -0.757172825\n156  20   67   1      38     3     7 156 -1.33183150  1.339808745\n157  22   41   1      39     3     2 157 -1.01240850  0.366210159\n158  28    7   1      40     3     1 158 -0.05413949 -0.906957223\n159  23   22   1      41     3     4 159 -0.85269700 -0.345265731\n160  40   13   1      42     3     0 160  1.86239852 -0.682280626\n161  33   46   1      43     3    25 161  0.74441801  0.553440656\n162  21   36   1      44     3     3 162 -1.17212000  0.178979662\n163  35   38   1      45     3     6 163  1.06384102  0.253871861\n164  25    7   1      46     3     2 164 -0.53327400 -0.906957223\n165  26   36   1      47     3     8 165 -0.37356249  0.178979662\n166  25   11   1      48     3     0 166 -0.53327400 -0.757172825\n167  22  151   1      49     3    72 167 -1.01240850  4.485281100\n168  32   22   1      50     3     2 168  0.58470651 -0.345265731\n169  25   41   1      51     3     5 169 -0.53327400  0.366210159\n170  35   32   1      52     3     1 170  1.06384102  0.029195264\n171  21   56   1      53     3    28 171 -1.17212000  0.927901651\n172  41   24   1      54     3     4 172  2.02211002 -0.270373532\n173  32   16   1      55     3     4 173  0.58470651 -0.569942327\n174  26   22   1      56     3    19 174 -0.37356249 -0.345265731\n175  21   25   1      57     3     0 175 -1.17212000 -0.232927432\n176  36   13   1      58     3     0 176  1.22355252 -0.682280626\n177  37   12   1      59     3     3 177  1.38326402 -0.719726725\n178  31   11   0       1     4     3 178  0.42499501 -0.757172825\n179  30   11   0       2     4     3 179  0.26528351 -0.757172825\n180  25    6   0       3     4     5 180 -0.53327400 -0.944403322\n181  36    8   0       4     4     4 181  1.22355252 -0.869511123\n182  22   66   0       5     4    21 182 -1.01240850  1.302362646\n183  29   27   0       6     4     7 183  0.10557201 -0.158035233\n184  31   12   0       7     4     2 184  0.42499501 -0.719726725\n185  42   52   0       8     4    12 185  2.18182153  0.778117253\n186  37   23   0       9     4     5 186  1.38326402 -0.307819631\n187  28   10   0      10     4     0 187 -0.05413949 -0.794618924\n188  36   52   0      11     4    22 188  1.22355252  0.778117253\n189  24   33   0      12     4     4 189 -0.69298550  0.066641363\n190  23   18   0      13     4     2 190 -0.85269700 -0.495050128\n191  36   42   0      14     4    14 191  1.22355252  0.403656259\n192  26   87   0      15     4     9 192 -0.37356249  2.088730734\n193  26   50   0      16     4     5 193 -0.37356249  0.703225054\n194  28   18   0      17     4     3 194 -0.05413949 -0.495050128\n195  31  111   0      18     4    29 195  0.42499501  2.987437121\n196  32   18   0      19     4     5 196  0.58470651 -0.495050128\n197  21   20   0      20     4     7 197 -1.17212000 -0.420157930\n198  29   12   0      21     4     4 198  0.10557201 -0.719726725\n199  21    9   0      22     4     4 199 -1.17212000 -0.832065024\n200  32   17   0      23     4     5 200  0.58470651 -0.532496228\n201  25   28   0      24     4     8 201 -0.53327400 -0.120589134\n202  30   55   0      25     4    25 202  0.26528351  0.890455552\n203  40    9   0      26     4     1 203  1.86239852 -0.832065024\n204  19   10   0      27     4     2 204 -1.49154300 -0.794618924\n205  22   47   0      28     4    12 205 -1.01240850  0.590886756\n206  18   76   1      29     4     8 206 -1.65125450  1.676823640\n207  32   38   1      30     4     4 207  0.58470651  0.253871861\n208  20   19   1      31     4     0 208 -1.33183150 -0.457604029\n209  30   10   1      32     4     3 209  0.26528351 -0.794618924\n210  18   19   1      33     4     4 210 -1.65125450 -0.457604029\n211  24   24   1      34     4     3 211 -0.69298550 -0.270373532\n212  30   31   1      35     4    16 212  0.26528351 -0.008250835\n213  35   14   1      36     4     4 213  1.06384102 -0.644834526\n214  27   11   1      37     4     4 214 -0.21385099 -0.757172825\n215  20   67   1      38     4     7 215 -1.33183150  1.339808745\n216  22   41   1      39     4     5 216 -1.01240850  0.366210159\n217  28    7   1      40     4     0 217 -0.05413949 -0.906957223\n218  23   22   1      41     4     0 218 -0.85269700 -0.345265731\n219  40   13   1      42     4     3 219  1.86239852 -0.682280626\n220  33   46   1      43     4    15 220  0.74441801  0.553440656\n221  21   36   1      44     4     8 221 -1.17212000  0.178979662\n222  35   38   1      45     4     7 222  1.06384102  0.253871861\n223  25    7   1      46     4     3 223 -0.53327400 -0.906957223\n224  26   36   1      47     4     8 224 -0.37356249  0.178979662\n225  25   11   1      48     4     0 225 -0.53327400 -0.757172825\n226  22  151   1      49     4    63 226 -1.01240850  4.485281100\n227  32   22   1      50     4     4 227  0.58470651 -0.345265731\n228  25   41   1      51     4     7 228 -0.53327400  0.366210159\n229  35   32   1      52     4     5 229  1.06384102  0.029195264\n230  21   56   1      53     4    13 230 -1.17212000  0.927901651\n231  41   24   1      54     4     0 231  2.02211002 -0.270373532\n232  32   16   1      55     4     3 232  0.58470651 -0.569942327\n233  26   22   1      56     4     8 233 -0.37356249 -0.345265731\n234  21   25   1      57     4     1 234 -1.17212000 -0.232927432\n235  36   13   1      58     4     0 235  1.22355252 -0.682280626\n236  37   12   1      59     4     2 236  1.38326402 -0.719726725\n\n\n\n\n\n\n\n1.2 モデルの推定とプロット\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nlibrary(brms)\nfit1 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            data = epilepsy, family = poisson())\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000111 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.11 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.952 seconds (Warm-up)\nChain 1:                0.675 seconds (Sampling)\nChain 1:                1.627 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.916 seconds (Warm-up)\nChain 2:                0.672 seconds (Sampling)\nChain 2:                1.588 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.909 seconds (Warm-up)\nChain 3:                0.881 seconds (Sampling)\nChain 3:                1.79 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.022 seconds (Warm-up)\nChain 4:                0.702 seconds (Sampling)\nChain 4:                1.724 seconds (Total)\nChain 4: \n\n\n\n\n\n\nsummary(fit1)\n\n Family: poisson \n  Links: mu = log \nFormula: count ~ zAge + zBase * Trt + (1 | patient) \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~patient (Number of levels: 59) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.58      0.07     0.46     0.73 1.00      839     1697\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.77      0.12     1.54     2.00 1.01      677     1025\nzAge           0.10      0.09    -0.08     0.27 1.00      739     1356\nzBase          0.70      0.12     0.46     0.93 1.00      821     1512\nTrt1          -0.27      0.17    -0.60     0.05 1.01      628     1304\nzBase:Trt1     0.06      0.16    -0.26     0.38 1.00      910     1477\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n基本的な解析の前提がまず出力され，推定結果はグループ毎（今回は患者毎）の変数（今回は \\(\\alpha_i\\)）から表示される．\n後半に固定効果の係数，すなわち回帰係数の推定結果が表示される．\n治療効果Trtの係数は負で，平均的に処置効果はある可能性があるが，95% 信頼区間は \\(0\\) を跨いでいるという意味で，有意とは言えない．また，交差項zBase*Trtの係数は小さく，交互効果の存在を示す証拠はないと思われる．\n\\(\\widehat{R}\\) が１より大きい場合，MCMC が収束していない可能性を意味する (Vehtari et al., 2021)．通説には \\(\\widehat{R}\\le1.1\\) などの基準がある．\n変数を指定して，事後分布と MCMC の軌跡をプロットできる：\n\nplot(fit1, variable = c(\"b_Trt1\", \"b_zBase\"))\n\n\n\n\n\n\n\n\nより詳しく見るにはconditional_effects関数を用いることもできる．交差項の効果はほとんどないことがわかる：\n\nplot(conditional_effects(fit1, effects = \"zBase:Trt\"))\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\n\n\n1.3 モデルによる予測\nfit したモデル fit1 を用いて，平均年齢と平均ベースレートを持つ患者に対する治療効果を予測する：\n\nnewdata &lt;- data.frame(Trt = c(0, 1), zAge = 0, zBase = 0)\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  5.91325  2.516010    2    11\n[2,]  4.51550  2.147653    1     9\n\n\n関数predict()は事後予測分布からのサンプリングを行う．一方で，関数fitted()は平均を返す．\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.887233 0.6995025 4.686328 7.382376\n[2,] 4.509634 0.5327421 3.551672 5.699954\n\n\n\n\n\n\n\n\n予測の出力\n\n\n\n\n\n従って，もう１度ずつ実行すると，predictでは値が変わるが，fittedでは同じ値が出力される．\n\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  5.85225  2.509082    2    11\n[2,]  4.56125  2.225643    1    10\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.887233 0.6995025 4.686328 7.382376\n[2,] 4.509634 0.5327421 3.551672 5.699954\n\n\n\n\n\n\n\n1.4 モデルの比較\nモデルfit1で行った Poisson 回帰分析は，fit1に含めた説明変数の違いを除けば，個々の観測が独立になる，という仮定の上に成り立っている（第 4.3 節）．\nこの仮定が破れているとき＝全ての説明変数をモデルに含めきれていないとき，Poisson 分布の性質 \\[\n\\operatorname{E}[X]=\\mathrm{V}[X]=\\lambda\\qquad (X\\sim\\mathrm{Pois}(\\lambda))\n\\] からの離反として現れ，この現象は 過分散（overdispersion）とも呼ばれる．\n\n1.4.1 観測レベルランダム効果\nということで，他の説明変数が存在した場合を想定して， Poisson 分布族ではなく，分散が平均よりも大きいような別の分布族を用いて，フィット度合いを比較してみることを考えたい．\nそこで，追加の変動をモデルに追加するべく，モデルfit1に観測ごとの切片項 \\(\\eta_{it}\\) を追加してみる（この手法は観測レベルランダム効果と呼ばれる．第 3.3 節参照）．\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.9 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.883 seconds (Warm-up)\nChain 1:                1.102 seconds (Sampling)\nChain 1:                2.985 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.4e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.916 seconds (Warm-up)\nChain 2:                2.049 seconds (Sampling)\nChain 2:                3.965 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.835 seconds (Warm-up)\nChain 3:                1.236 seconds (Sampling)\nChain 3:                3.071 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.909 seconds (Warm-up)\nChain 4:                1.6 seconds (Sampling)\nChain 4:                3.509 seconds (Total)\nChain 4: \n\n\n\n\n\nこうして得た２つのモデルfit1,fit2を比較する．\nLLO (Leave-One-Out) cross-validation が関数looによって実行できる：\nloo(fit1, fit2)\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit1, fit2)\n\nWarning: Found 9 observations with a pareto_k &gt; 0.7 in model 'fit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 54 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit1':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -670.9 36.6\np_loo        93.7 14.5\nlooic      1341.8 73.2\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     227   96.2%   102     \n   (0.7, 1]   (bad)        8    3.4%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   1    0.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -595.7 14.1\np_loo       108.4  7.2\nlooic      1191.4 28.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     182   77.1%   130     \n   (0.7, 1]   (bad)       45   19.1%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   9    3.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2   0.0       0.0  \nfit1 -75.2      26.7  \n\n\n\n\n\nelpd_diff は expected log posterior density の差異を表す．fit2の方が大きく当てはまりが良いことが見て取れる．\nまた，WAIC (Watanabe-Akaike Information Criterion) も実装されている：\nprint(waic(fit1))\n\n\n\n\n\n\nWAIC の結果\n\n\n\n\n\n\nprint(waic(fit1))\n\nWarning: \n52 (22.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -667.8 36.6\np_waic        90.6 14.5\nwaic        1335.6 73.2\n\n52 (22.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nprint(waic(fit2))\n\nWarning: \n66 (28.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -572.0 12.0\np_waic        84.7  5.1\nwaic        1144.1 23.9\n\n66 (28.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\n\n\n\n他にも，reloo, kfold などの関数もある．\n\n\n\n\n\n\n他の関数一覧\n\n\n\n\n\n\nmethods(class=\"brmsfit\")\n\n [1] add_criterion           add_ic                  as_draws_array         \n [4] as_draws_df             as_draws_list           as_draws_matrix        \n [7] as_draws_rvars          as_draws                as.array               \n[10] as.data.frame           as.matrix               as.mcmc                \n[13] autocor                 bayes_factor            bayes_R2               \n[16] bridge_sampler          coef                    conditional_effects    \n[19] conditional_smooths     control_params          default_prior          \n[22] expose_functions        family                  fitted                 \n[25] fixef                   formula                 getCall                \n[28] hypothesis              kfold                   log_lik                \n[31] log_posterior           logLik                  loo_compare            \n[34] loo_linpred             loo_model_weights       loo_moment_match       \n[37] loo_predict             loo_predictive_interval loo_R2                 \n[40] loo_subsample           loo                     LOO                    \n[43] marginal_effects        marginal_smooths        mcmc_plot              \n[46] model_weights           model.frame             nchains                \n[49] ndraws                  neff_ratio              ngrps                  \n[52] niterations             nobs                    nsamples               \n[55] nuts_params             nvariables              pairs                  \n[58] parnames                plot                    post_prob              \n[61] posterior_average       posterior_epred         posterior_interval     \n[64] posterior_linpred       posterior_predict       posterior_samples      \n[67] posterior_smooths       posterior_summary       pp_average             \n[70] pp_check                pp_mixture              predict                \n[73] predictive_error        predictive_interval     prepare_predictions    \n[76] print                   prior_draws             prior_summary          \n[79] psis                    ranef                   reloo                  \n[82] residuals               restructure             rhat                   \n[85] stancode                standata                stanplot               \n[88] summary                 update                  VarCorr                \n[91] variables               vcov                    waic                   \n[94] WAIC                   \nsee '?methods' for accessing help and source code\n\n\n\n\n\n\n\n1.4.2 患者内の相関構造のモデリング\nまた，fit1において，同一患者の異なる訪問の間には全く相関がないと仮定されており，これは全く非現実的な仮定をおいてしまっていると言える．2\n患者内の相関構造は，brm()関数のautocor引数で指定できる（第 4.3.2 節）．\n例えば，全く構造を仮定しない場合は，unstrを指定する：\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\nWarning: Argument 'autocor' should be specified within the 'formula' argument.\nSee ?brmsformula for help.\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.00025 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.5 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 4.196 seconds (Warm-up)\nChain 1:                2.479 seconds (Sampling)\nChain 1:                6.675 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 3.789 seconds (Warm-up)\nChain 2:                2.587 seconds (Sampling)\nChain 2:                6.376 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.3e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.43 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 4.176 seconds (Warm-up)\nChain 3:                2.465 seconds (Sampling)\nChain 3:                6.641 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 4.147 seconds (Warm-up)\nChain 4:                2.216 seconds (Sampling)\nChain 4:                6.363 seconds (Total)\nChain 4: \n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\n\n\nこのモデルもfit1より遥かに当てはまりが良く，fit2とほとんど同じ当てはまりの良さが見られる：\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit2,fit3)\n\nWarning: Found 54 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 62 observations with a pareto_k &gt; 0.7 in model 'fit3'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -595.7 14.1\np_loo       108.4  7.2\nlooic      1191.4 28.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     182   77.1%   130     \n   (0.7, 1]   (bad)       45   19.1%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   9    3.8%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit3':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -601.1 14.6\np_loo       112.2  7.8\nlooic      1202.2 29.3\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     174   73.7%   157     \n   (0.7, 1]   (bad)       54   22.9%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   8    3.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit3 -5.4       2.6   \n\n\n\n\n\n思ったよりもfit2の当てはまりが良いため，Poisson-対数正規混合モデリングを本格的に実施してみることが，次の選択肢になり得る（第 3.3 節参照）．\n\n\n\n1.5 その他の例\nSebastian Weber らにより，新薬の治験における実際の解析事例をまとめたウェブサイト が公開されている．3\n特に，13 章で，同様の経時的繰り返し観測データを扱っているが，ここではカウントデータではなく連続な応用変数が扱われている．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "href": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "title": "R によるベイズ混合モデリング入門",
    "section": "2 ランダム効果モデルの正しい使い方",
    "text": "2 ランダム効果モデルの正しい使い方\n\n\n\n\n\n\n概要\n\n\n\n\nランダム効果モデル とは，グループ毎に異なる切片項 \\(\\alpha_{s[i]}\\) を追加し，これにも誤差を仮定してモデルに入れて得る階層モデルである．\nしかし，\\(\\alpha_{s[i]}\\) が（ユニットレベルの）説明変数 \\(x_i\\) と相関を持つ場合，推定量の一致性が失われる．これを回避するために，\\(x_i\\) の係数 \\(\\beta\\) にのみ関心がある場合は，固定効果モデルが用いられることも多い．\nだが，簡単なトリック（\\(\\alpha_{s[i]}\\) の説明変数に \\(\\overline{x}_s\\) を追加すること）で，推定量の一致性を回復することができる．\nこのトリックを取り入れたランダム効果モデルは，\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がない場合は固定効果モデルと等価な \\(\\beta\\) の推定量を与え，相関がある場合でも，\\(\\beta\\) を一致推定し，各変動切片項 \\(\\alpha_{s[i]}\\) の構造にも洞察を与えてくれる．\n\n\n\nランダム効果モデルは線型混合モデル (Linear Mixed Model; LMM) とも呼ばれる．\n\nLMM は，母数の共通化によるデータのプーリングと変動効果による標本平均の縮小作用を生み出すことのできるモデルであり，その結果生ずる予測量が EBLUE になる．したがって，EBLUE は，各々の地域の標本平均とプールされた回帰推定量との加重平均になっており，データ数が少ないときには標本平均をプールされた推定量の方向へ縮小することにより，推定精度の改善を図っている．(久保川達也, 2006)\n\n\n2.1 ランダム効果モデル\nランダム効果 は，変動する切片項と呼んだ方がわかりやすい (Bafumi and Gelman, 2007) と言われるように，サブグループ毎に異なる切片項のことである．4\nユニット（個人などの最小単位）レベルの回帰式を書き下すと，グループ選択関数 \\(s:[n]\\to[S]\\;(S\\le n)\\) を通じて， \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\n\\tag{1}\\] というようになる．\nこれは，確率変数 \\(\\alpha_{s[i]}\\) の平均を \\(\\alpha_0\\) とすると，グループレベルの回帰式 \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad s\\in[S]\n\\tag{2}\\] が背後にある 階層モデル (multilevel / hierarchical model) だとみなすこともできる．\n\n\n2.2 説明変数との相関の問題\n\n2.2.1 問題の所在\nランダム効果では，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s[i]}\\) が相関を持たないという仮定が Gauss-Markov の定理の仮定に等価になるため，これが違反されると推定量の不偏性・一致性が約束されず，推定量の分散も大きくなる．5\n\n\n\n\n\n\nGauss-Markov の仮定\n\n\n\nユニットレベル回帰式 \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] において，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s[i]}\\) が相関を持たないこと．\n\n\n実際，ランダム効果モデルの階層構造を，(2) を (1) に代入することで一つの式にまとめると \\[\ny_i=\\alpha_0+\\beta x_i+\\underbrace{\\epsilon_i'}_{\\epsilon_i+\\eta_{s[i]}}\n\\tag{3}\\] を得る．\nこのような誤差項の構造 \\(e_{it}=\\alpha_i+\\epsilon_{it}\\) を 一元配置モデル (one-way error component model) ともいう (Hansen, 2022, p. 600), (久保川達也, 2006, p. 155)．\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がある場合，\\(x_i\\) と \\(\\eta_s\\) にも相関があるため，結果として (3) では説明変数と誤差 \\(\\epsilon_i'\\) に相関が生じてしまう．これは計量経済学では 内生性 (endogeneity) の問題と呼ばれているものに他ならない．\n\n\n2.2.2 業界の現状：母数効果モデル\nそのため，ランダム効果モデルは避けられる傾向にあり，切片項 \\(\\alpha_{s[i]}\\equiv\\alpha_0\\) は変動しないとし，グループレベルの効果を無視してモデリングすることも多い： \\[\ny_i=\\alpha_0+\\beta x_i+\\epsilon_i.\n\\] このことを 完全プーリングモデル (complete pooling model) または母数効果モデルと呼び，ランダム効果モデルを 部分プーリングモデル (partial pooling model) と呼んで対比させることがある．6\n周辺モデル (marginal model) や 母平均モデル (population-average model) とも呼ばれる (Gardiner et al., 2009, p. 228)．\n実際，これ以上の仮定を置かず，ランダム効果は局外母数として（母数効果ともいう）一般化推定方程式の方法（第 2.6 節）によれば，\\(\\beta\\) の不偏推定が可能である．\nリンク関数 \\(g\\) を通じた非線型モデル \\[\ng(\\operatorname{E}[y_i|x_i])=\\beta x_i\n\\] であっても，指数型分布族を仮定すれば，\\(\\beta\\) の一致推定が可能である．\nだが，切片項の変動を消してしまうことで，回帰係数 \\(\\beta\\) の推定に対する縮小効果（第 3.2 節）が得られないという欠点もあり，小地域推定などにおいては \\(\\alpha_{s[i]}\\) を確率変数とみなす積極的理由もある．この点については (久保川達也, 2006) も参照．\n\n\n2.2.3 固定効果モデルという解決\n問題を起こさずに，しかしながらグループレベルの効果をモデリングしたい場合， \\[\ny_i=\\alpha_{s[i]}^{\\text{unmodeled}}+\\beta x_i+\\epsilon_i\n\\] として，グループ毎に変動する切片項 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を許すが，この変数自体にモデルは仮定しない，とすることもできる．\nしたがってグループ毎に別々の回帰分析を実行し，別々の切片 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を得て，\\(\\beta\\) の値はこれらのグループの間で適切に重みづけて最終的な推定値としているに等しい．\nすなわち，グループの数だけ，グループへの所属を表す２値変数 \\(1_{\\left\\{s[i]=s\\right\\}}\\) を導入し，\\(S\\) 個の項 \\(\\sum_{s=1}^S1_{\\left\\{s[i]=s\\right\\}}\\alpha_{s[i]}^{\\text{unmodeled}}\\) を説明変数に加えて回帰分析を行うことに等しい．\nベイズ的には，変動係数の場合は更なる構造 \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad s\\in[S],\n\\] に基づきグループレベルにばらつく確率構造が仮定されているのと対照的に，improper な一様事前分布 \\[\n\\alpha_s^{\\text{unmodeled}}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\alpha_0,\\infty)\n\\] を仮定した場合が固定効果モデルであると理解される (Gelman et al., 2014, p. 383)．\nまた群内平均を引いた値 \\(y_i-\\overline{y}_{s[i]}\\) を目的変数として，説明変数 \\(x_i-\\overline{x}_{s[i]}\\) により回帰分析を行うこととも等価である．この変換により \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) が消去されると考えられるのである．\n\n\n\n\n\n\n固定効果モデルの別名\n\n\n\n\n(Hansen, 2022) をはじめ，計量経済学では fixed effects model と呼ばれる．7\n(Bafumi and Gelman, 2007) は unmodeled varying intercept と呼んでいる．\nleast squares dummy variable regression とも呼べる．8\n\n\n\n\n\n\n2.3 固定効果 vs. 変量効果\n\n\n\n\n\n\n利点\n\n\n\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持ち得る場合も，固定効果モデルでは問題が生じない．9\n\n\n\n\n\n\n\n\n問題点\n\n\n\n異なるグループのデータが相互作用する機構がランダム効果モデルに比べて貧しい．\n例えばランダム効果モデルを用いた場合，外れ値グループが存在するなどノイズの大きなデータに対しても，\\(\\eta_s\\) を通じて緩やかに情報が伝達され，\\(\\beta\\) の値は平均へ縮小されて推定される（第 3.2 節）．固定効果モデルではそのような頑健性を持たない (Bafumi and Gelman, 2007, pp. 4–5)．\n\n\n固定効果モデルは \\(\\beta\\) （のみ）に関心がある場合，\\(\\alpha_{s[i]}\\) と \\(x_i\\) の相関の存在に対してロバストな推定法として有用であり，その理由で計量経済学（特に線型パネルデータ）では主流の推定手法となっている．10\n実際，\\(\\alpha_{s[i]}\\) と \\(x_i\\) が無相関であるとき，変量効果モデルと固定効果モデルは \\(\\beta\\) に関しては等価な推定量を与える．\n\nCurrent econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estmimator for linear panel data models. (Hansen, 2022, p. 624)\n\n逆に言えば，固定効果モデルは \\(x_i\\) と \\(\\alpha_{s[i]}\\) の構造のモデリングを放棄したモデリング法であり，各 \\(\\alpha_{s[i]}\\) の値にも興味がある場合，または \\(\\beta\\) のより精度の高い推定が実行したい場合には，やはり \\(\\alpha_{s[i]}\\) の誤差と相関構造もモデルに取り入れたランダム効果モデルを用いたいということになる．\n\n\n2.4 ランダム効果モデルにおける相関のモデリング\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持つ場合に一致推定が保証されないことがランダム効果モデルの欠陥だと述べたが，実はこれは簡単な方法で解決できる．\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) との相関は，欠落変数が存在するため，と考えることができる．\nそしてこの相関は，説明変数の平均 \\(\\overline{x}_s\\) を変動する切片項 \\(\\alpha_s\\) の説明変数として追加することで除去できる：11\n\\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i\n\\] \\[\n\\alpha_s=\\alpha_0+\\alpha_1\\overline{x}_s+\\eta_s\n\\]\nこれにより，Gauss-Markov の仮定（外生性）が回復される．\n(Bafumi and Gelman, 2007, pp. 7–9) にシミュレーションによる検証が掲載されている．\n\nPractitioners can get around this problem by taking advantage of the multilevel structure of their regression equation. (Bafumi and Gelman, 2007, p. 12)\n\n\n\n2.5 第三の名前：混合効果モデル\n以上，解説してきたランダム効果モデル／変量効果モデルであるが，混合効果モデル とも呼ばれる．12\n何を言っているのかわからないかもしれないが，式 (1) \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] において，\\(\\alpha_{s[i]}\\) がランダム効果であるが，回帰係数 \\(\\beta\\) を 固定効果 とも呼ぶことがあるのである．\nそしてこう見ると全体として固定効果と変量効果が同居した 混合（効果）モデル とも呼べそうである．\n現代的には，必要ならば \\(\\beta\\) を確率変数とみなしても良いだろうが，慣習的にそう呼ぶため，これに従わざるを得ない，というのが (Hansen, 2022, p. 625) などを見る限り共通了解であるようである．\nこれが計量経済学における固定効果モデル（第 2.2.3 節）の名前の由来である．13 実際，固定効果モデルは，たしかに（ユニットレベルでの回帰係数という意味での）「固定効果」を表す変数しか含んでいない（少なくとも見た目上は）．\n\n\n\n\n\n\n線型混合モデルの別名\n\n\n\n式 (1) \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] で定義されるモデルは，(Chung et al., 2013) によると次のような複数の名前を持つ：\n\n線型混合モデル (linear mixed models) (Kincaid, 2005)\n階層モデル (hierarchical models)\nマルチレベル線型モデル (multilevel linear models)\n混合効果モデル (mixed-effects models) (Chung et al., 2015)\nランダム効果モデル (random effects model) (Hubbard et al., 2010) や (Bafumi and Gelman, 2007)．\n分散成分モデル (variance component model)14\n\n\n\n\n\n2.6 GEE との違い\n一般化推定方程式 (GEE: Generalized Estimating Equation) では，ランダム効果モデルにおける階層的な議論を全て「局外母数」として捨象し，母数 \\(\\beta\\) の推定に集中する見方をする．\n\n\n\n\n\n\nGEE との違い\n\n\n\n\n回帰式が違う\n線型の場合の GEE は \\[\n   Y_{it}=\\alpha+\\beta_1x_{1,i,t}+\\cdots+\\beta_px_{p,i,t}\n   \\] とも表され，ランダムな切片項というものは見当たらない．その代わり，グループ間の影響は相関係数行列としてモデル化を行う．ランダム効果モデルでは，この相関構造をランダムな切片項を追加することで表現し，回帰式を複数立てることでモデルを表現する．\n推定目標が違う\nGEE は population average model でよく用いられる (Hubbard et al., 2010) ように，あくまで応答 \\(Y_{it}\\) の平均の不偏推定が目標であり，共分散構造はいわば局外母数である．一方，混合効果モデルは，その階層モデルとしての性質の通り，平均構造と分散構造のいずれも推定対象として扱う志向性がある．\n推定方法が違う\n混合効果モデルは主に最尤法により推定される (Hubbard et al., 2010)．GEE はモーメント法により推定され，最尤法ベースではないため，完全にランダムとは言えない欠測がある場合は弱く，欠測データに対しては IPW などの方法が用いられる．\n\n\n\nGEE にとって相関構造は局外母数であり，正確な特定は目的に含まれない．この意味で GEE の相関係数⾏列におく仮定は「間違えていてもよい便宜的な仮定」であるため，作業相関係数行列 (working correlation coefficient matrix) とも呼ばれる．相関構造を誤特定していても，平均構造は一致推定が可能であり，ロバストである．両方の特定に成功した場合はセミパラメトリック有効性が達成される．\n一方で混合効果モデルは，階層モデルとして平均構造と分散構造のいずれにも明示的な仮定をおくため，片方（例えば共分散構造）の特定を間違えていた場合，もう片方の解釈性が失われる，というリスクがあると論じることができる．特に (Hubbard et al., 2010) に見られる論調である．\nしかし小地域推定や，「子供の身長の成長曲線の描画」が主な研究目的である場合など，ユニットの平均効果ではなく個別効果に注目したい場合には混合効果モデルの方が適していることになる (Gardiner et al., 2009)．実際，モデルの特定に成功していれば，いずれのパラメータも最尤推定されるため，一致性を持つ．\n従って，モデル選択において用いられる基準も違う．GEE における作業相関係数行列と説明変数の選択には QIC (Quasi-likelihood Information Criterion) が，混合効果モデルには AIC や BIC （または cAIC や mAIC (Vaida and Blanchard, 2005)）が用いられる (Gardiner et al., 2009, p. 228)．\n\n\n\n2.7 ベイズ混合効果モデルという光……？\nしかし，結局ベイズ統計学の立場からは，２つの違いはほとんど重要ではなく，混合効果モデルを推定した後に，周辺化をして平均構造に関する marginal estimator を構成すれば，GEE の代用になっているのではないか？\n計算機の性能と，計算統計手法の発展が目まぐるしい現代にて，過去の議論を踏襲しすぎることは，問題の本質を誤るということもあるのだろう．\nということで，以上議論したグループレベル構造を持ったデータに対する２階の階層モデルを，本稿では「混合効果モデル」と呼ぶことにする．\nこの節はこれで終わり．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#混合効果モデリングのテクニック集",
    "href": "posts/2024/Computation/brms.html#混合効果モデリングのテクニック集",
    "title": "R によるベイズ混合モデリング入門",
    "section": "3 混合効果モデリングのテクニック集",
    "text": "3 混合効果モデリングのテクニック集\n\n\n\n\n\n\n概要\n\n\n\n\n混合効果モデルの推定において，グループレベル変動 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になり得る．特に，グループ数 \\(S\\) が小さい場合に顕著である．\nカウントデータの Poisson モデルでは，「観測レベルのランダム効果」を追加することで，実質的に Poisson-対数正規混合モデリングを実行できる．\n\n\n\n\n3.1 グループレベル分散の推定\n\n3.1.1 問題\n混合効果モデル（階層モデル） \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] の推定において，特にグループ数 \\(S\\) が小さい場合，グループレベルの変動切片項 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になったり，分散が負の値をとったりするという問題点が古くからある (Harville, 1977)．15\n変量効果 \\(\\eta_s\\) を \\(\\eta_s\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2_s)\\)，誤差を \\(\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2_e)\\) とすると，この \\(\\mathrm{V}[\\eta_s]\\) は次の形をもち，グループ間の相関構造のモデリングを一手に引き受けている： \\[\n\\mathrm{V}[\\eta_{s}]=\\sigma^2_sJ_{n_s}+\\sigma_e^2I_{n_s},\\qquad J_{n_s}:=\\boldsymbol{1}_{n_s}\\boldsymbol{1}_{n_s}^\\top.\n\\]\nEM アルゴリズムが提案されたばかりの頃 (Laird and Ware, 1982) では，共分散構造にパラメトリックな仮定をおいていたが，現代ではこれを取り去った最尤推定法・ベイズ推定法が主流である．\n\n\n3.1.2 退化しない共分散行列推定\nしかし，最尤推定法と，一定の事前分布を仮定したベイズ MAP 推定法では，推定された共分散行列が退化してしまったり，分散が負の値を取ってしまうことがある．\n打ち切り推定量 (T. Kubokawa and Srivastava, 1999), (Tatsuya Kubokawa, 2000) なども提案されているが，ベイズでは Wishart 事前分布を仮定することでこれが回避される (Chung et al., 2015)．16 これは最尤法の文脈では，penalized likelihood と等価になる (Chung et al., 2013)．\nモデルのサイズによっては，完全なベイズ推定を実行することが難しく，一部は等価な頻度論的な方法や近似を用いることもある．その際，最適化ソルバーの収束を速めるために，共分散構造に（データや計画とは無関係に）パラメトリックモデルを仮定してしまうこともある (Kincaid, 2005)．\n\n\n\n3.2 係数の縮小推定\n分散 \\(\\mathrm{V}[\\eta_s]\\) を推定して分散比 \\(\\rho:=\\sigma_v^2/\\sigma_e^2\\) の推定量 \\(\\widehat{\\rho}\\) を得て，これを最良線型不偏推定量 (BLUE) \\(\\widehat{\\beta}\\) に代入して得られる，グループごとの \\(y_s\\) の推定量に \\[\n\\widehat{y}_s:=\\frac{\\widehat{\\rho}n_s}{1+\\widehat{\\rho}n_s}\\overline{y}_s+\\frac{1}{1+\\widehat{\\rho}n_s}\\overline{x}_s^\\top\\widetilde{\\beta}(\\widehat{\\rho})\n\\] というものがあり，これを 経験 BLUE という (久保川達也, 2006, p. 143)．\nこれは，各グループ \\(s\\in[S]\\) における値 \\(y_s\\) を，単なる経験平均 \\(\\overline{y}_s\\) ではなく，全データプールから得られる推定量 \\(\\overline{x}_s^\\top\\widetilde{\\beta}(\\widehat{\\rho})\\) で補正した推定量になっている．\nこのことにより，各グループ \\(s\\in[S]\\) のデータ数が少なく，経験平均 \\(\\overline{y}_s\\) では分散が大きくなってしまう場合でも，安定した推定量を得ることができる．\n縮小推定は小地域推定 (George E. Battese and Fuller, 1988) に応用を持つ．例えば \\(s\\in[S]\\) をアメリカ合衆国の各州とし，投票行動のデータに応用した例が (Gelman, 2014) にある．\nこのように，変量効果 \\(\\alpha_{s[i]}\\) を追加したモデリングを実行することにより，グループごとの被説明変数を縮小推定することができる．\n\n3.2.1 経験ベイズ\n縮小推定の効用は初め，経験ベイズの枠組みで説明された．\n\n以上の考え方は，経験ベイズの枠組みで (Efron and Morris, 1975) の一連の論文の中で示されてきたものであり，ベイズ的アプローチの現実的な有用性は基本的には上述の考え方に基づいている．\n\nそもそも１元配置混合線型モデルは \\[\ny_{ij}=\\theta_{ij}+e_{ij},\\qquad \\theta_{ij}=x_{ij}^\\top\\beta+v_i\n\\] とも理解できる．これは階層モデル \\[\ny_{ij}\\sim\\mathrm{N}(\\theta_{ij},\\sigma^2_e),\\qquad\\theta_{ij}\\sim\\mathrm{N}(x_{ij}^\\top\\beta,\\sigma_v^2)\n\\] とも見れる．\n\\(\\beta,\\sigma^2_v,\\sigma^2_e\\) を未知母数として扱った場合を 経験ベイズモデル，変量として扱って更なる分布を仮定した場合を（狭義の） 階層ベイズ ともいう (久保川達也, 2006, p. 155)．\n\n\n\n3.3 カウントデータ過分散へのお手軽対処法\nこれはカウントデータのモデリング限定のテクニックである．\nカウントデータも，一般化線型（混合）モデルの範疇で扱うことができるため，リンク関数 \\(g\\) を通じてほとんど同等の扱いが可能である．\n\n3.3.1 負の二項分布によるモデリング\nカウントデータの基本は Poisson 分布であろうが，過分散を考慮するために負の二項分布でモデリングすることもできる．(17.2節 Gelman et al., 2014) なども参照．\n負の二項分布は例えばマーケティングにおいて，顧客の購買回数をモデル化する際に用いられる (森岡毅，今西聖貴, 2016)．\nこの行為は，Poisson 分布の Gamma 分布による混合分布族を用いた，混合モデリングを行っているとみなせる：\n\n\n\n\n\n\n命題\n\n\n\nPoisson 分布 \\(\\mathrm{Pois}(\\theta)\\) の \\(\\mathrm{Gamma}(\\alpha,\\nu)\\)-混合は負の二項分布 \\(\\mathrm{NB}\\left(\\nu,\\frac{\\alpha}{\\alpha+1}\\right)\\) になる．\nただし，負の二項分布 \\(\\mathrm{NB}(\\nu,p)\\) は，次の確率質量関数 \\(p(x;\\nu,p)\\) が定める \\(\\mathbb{N}\\) 上の確率分布である： \\[\np(x;\\nu,p)=\\begin{pmatrix}x+\\nu-1\\\\x\\end{pmatrix}p^\\nu(1-p)^x.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n確率分布の変換則より，次のように計算できる：\n\\[\\begin{align*}\n  p(x)&=\\int_{\\mathbb{R}_+}\\frac{\\theta^x}{x!}e^{-\\theta}\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu\\theta^{\\nu-1}e^{-\\alpha\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\int_{\\mathbb{R}_+}\\theta^{x+\\nu-1}e^{-(\\alpha+1)\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\frac{\\Gamma(x+\\nu)}{(\\alpha+1)^{x+\\nu}}\\\\\n  &=\\begin{pmatrix}\\nu+x-1\\\\x\\end{pmatrix}\\left(\\frac{1}{\\alpha+1}\\right)^x\\left(\\frac{\\alpha}{\\alpha+1}\\right)^\\nu.\n\\end{align*}\\]\nこの最右辺は，たしかに負の二項分布の質量関数である．\nこの証明方法と，Gamma 分布については次の記事を参照：\n\n  \n    \n      \n      \n        確率測度の変換則\n        Gamma 分布とBeta 分布を例に\n      \n    \n  \n\n\n\n\nこれは \\[\ny_{it}\\sim\\mathrm{Pois}(\\theta)\n\\] \\[\n\\theta\\sim\\mathrm{Gamma}(\\alpha,\\nu)\n\\] という Gamma 分布を仮定した経験ベイズモデル（第 3.2.1 節）に当たる．\nGamma 分布は Poisson 分布の共役事前分布であるため計算が容易であり，早くから質病地図の作成などにも用いられていた (Clayton and Kaldor, 1987), (丹後俊郎, 1988)．\n\n\n3.3.2 Poisson-対数正規混合によるモデリング\nPoisson 回帰\n\\[\n\\begin{align*} y_{it} & \\sim \\operatorname{Pois}(\\lambda_{s[i]}) \\\\ \\log(\\lambda_{s[i]}) & = \\alpha_i + \\eta_{it} \\\\ \\eta_{it} & \\sim \\operatorname{N}(0, \\sigma). \\end{align*}\n\\]\nを考えると，各 \\(y_{it}\\) を，（グループ毎に条件付ければ）Poisson 分布の対数正規分布による混合分布を用いてモデル化していることにあたる．\nこの，Poisson-対数正規分布族は，(Bulmer, 1974) により生物種の個体数分布のモデリングで，過分散を説明するために用いられている．\nすなわち，第 1.1 節のモデルの比較 1.4 で扱った，観測レベルランダム効果 (OLRE: Observation-level Random Effects) の方法は，観測毎に \\(\\eta_{it}\\) というランダム切片項を追加するだけで，本質的には Poisson-対数正規混合モデリングを実施する という，いわばハックのような使い方である．17\n今回はモデル比較の結果が良かったため，本格的に対数正規混合を実施してみるのも良いかもしれない．\n\n\n\n3.4 変量係数モデルによる非線型モデリング\n\n\n\n\n\n\n混合モデルの種々の拡張\n\n\n\n\n\n前節 3.3 では，カウントデータに適用するための一般化線型混合モデルをみた．\n(久保川達也, 2006) では，ここまで考慮した１元配置混合線型モデルの拡張をいくつか紹介している：\n\n各グループ \\(s\\in[S]\\) の中でもいくつかのクラスターに分けられる場合，２元配置混合モデル が考えられる： \\[\ny_{ijk}=x_{ijk}^\\top\\beta+v_i+u_{ij}+e_{ijk}.\n\\]\n誤差分散が一定であるという仮定が怪しい場合，変動分散モデル が考えられる．これは，グループ内の分散を \\(e_{ij}|\\sigma_i^2\\sim\\mathrm{N}(0,\\sigma_i^2)\\) とし，\\(\\sigma_i\\) をグループ内で同一の分布に従う i.i.d. と仮定した階層モデルをいう．\n係数 \\(\\beta\\) にもモデルを仮定した階層モデルは 変量係数モデル ともいう： \\[\n\\beta_i=W_i\\alpha+v_i.\n\\] 州ごとの，収入因子が投票行動に与える影響の差を突き止めた (Gelman, 2014) ではこの変量係数モデルを用いている．\n\n\n\n\n\n3.4.1 例：投票行動の州ごとの違い\n(Gelman, 2014) では州ごとの投票行動の違いを説明するために，まずは次のロジスティック混合モデルを考えている： \\[\n\\operatorname{Pr}(y_i=1)=\\operatorname{logit}^{-1}(\\alpha_{s[i]}+x_i\\beta)\n\\] \\[\n\\alpha_s=W_s^\\top\\gamma+\\epsilon_s,\\qquad\\epsilon_s\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\alpha).\n\\]\n\n\n\n\n\n\n各変数の説明\n\n\n\n\n\n\n\\(y_i\\in\\{0,1\\}\\) は共和党に投票したか，民主党に投票したかを表す２値変数．\n\\(x_i\\in\\{\\pm2,\\pm1,0\\}\\) は収入のレベルを５段階で表す離散変数．\n\\(W_j\\) は各州の共変量のベクトル．\n\n\n\n\nしかしこのままではモデルの当てはまりが良くなかった．これは州ごとに収入が投票に与える影響が異なるためであった．これを考慮するために，(Gelman, 2014) は変量係数モデルを用いた．\n\n\n3.4.2 混合モデルの変量係数モデル化\n\\(\\beta\\) を州ごとに変化させ，これに \\[\n\\beta_s=W_s^\\top\\gamma'+\\epsilon'_s,\\qquad \\epsilon'_s\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\beta),\n\\] というモデルをおく．\nこれにより，州ごとに変化する収入-投票関係をモデリングできる．\n\n\n3.4.3 非線型モデル化\nこれに加えて，\\(\\beta_s\\) を収入カテゴリのアイテム \\(x_i\\in\\{\\pm2,\\pm1,0\\}\\) ごとに変化させることも考えられる．\nこれは値も持つダミー変数 \\[\n\\boldsymbol{x}_i^j=(j-3)1_{\\left\\{x_i=j\\right\\}},\\qquad j\\in\\{1,2,3,4,5\\},\n\\] を成分にもつ \\(\\boldsymbol{x}_i\\in\\mathbb{Z}^5\\) を用いて， \\[\n\\operatorname{Pr}(y_i=1)=\\operatorname{logit}^{-1}(\\alpha_{s[i]}+\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}_{s[i]})\n\\tag{4}\\] というモデルを考えることにあたる．\nこの小さな変更により，非線型な関係もモデリングできるようになる．\n\n\n3.4.4 多重共線型性の霧消\nこのようなトリックが可能な理由は，ベイズ回帰においては多重線型性が問題にならないためである．\nモデル (4) では，３通りで収入が説明変数に入っている：\n\n各収入カテゴリのダミー変数 \\(1_{\\left\\{x_i=j\\right\\}}\\) として\n収入カテゴリの値 \\(\\boldsymbol{x}_i^j\\) として．\n州ごとの収入として \\(W_s\\) にも入っている．\n\nこのことに気づけただろうか？\n頻度論的に回帰分析を実行していたならば，このような多重共線性は問題になっていただろうが，階層ベイズモデリングにおいては有用なトリックとして積極的に活用することができる．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brmsの実装",
    "href": "posts/2024/Computation/brms.html#brmsの実装",
    "title": "R によるベイズ混合モデリング入門",
    "section": "4 brmsの実装",
    "text": "4 brmsの実装\nbrm 関数（コードは こちら）の実装を調べる．\n\n\n\n\n\n\n\nbrms\n\nStan コードを扱っている関数は .stancode() であった．最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\n\n.standata\n\n\n\n\n\n4.1 事前分布\nbrm関数 では，デフォルトでは無情報事前分布が用いられる．\n\nDefault priors are chosen to be non or very weakly informative so that their influence on the results will be negligible and you usually don’t have to worry about them. However, after getting more familiar with Bayesian statistics, I recommend you to start thinking about reasonable informative priors for your model parameters: Nearly always, there is at least some prior information available that can be used to improve your inference.brm(): Fit Bayesian Generalized (Non-)Linear Multivariate Multilevel Models\n\n\n\n4.2 回帰式\nbrm()関数の第一引数は，validate_formula関数に渡される．\nこの関数は S3 のメソッドのディスパッチを用いて実装されており，brmsformulaオブジェクトに対しては，validate_formula.brmsformula関数が呼び出される．\nここではautocor引数が引かれている場合，出力のformula属性に追加される：18\n\nfit3$formula\n\ncount ~ zAge + zBase * Trt + (1 | patient) \nautocor ~ unstr(time = visit, gr = patient)\n\n\nなお，brmsformulaオブジェクトのコンストラクタは brmsformula()関数 である．これは，R のformulaオブジェクトを通じて，階層モデルを定義できるようになっている（実装はリスト）．\n\n\n4.3 共分散構造\n共分散構造は２つの観点から，brmsformulaオブジェクトから自動的に指定される．\n１つ目がグルーピング構造（共分散行列のブロック構造）であり，これはgr関数 が使用される．\n２つ目がグループ内の相関構造であり，これはbrm()関数のautocor引数を用いる．\n\n4.3.1 gr関数\nこの関数はbrm関数の第一引数として与えられたモデル定義式から，暗黙のうちに内部で呼び出される．\n例えば，回帰式に(1|patient)が含まれていた場合，gr(patient)が呼び出される．\n共分散構造におく仮定について，重要なデフォルト設定が２つある：\n\n\n\n\n\n\n\nグループ間の相関構造は想定されている：cor=True．\n\nIf TRUE (the default), group-level terms will be modelled as correlated.gr(): Set up basic grouping terms in brms\n\n一方で，グループ内の相関構造は想定されておらず，独立とされている．具体的に指定したい場合は引数covを用いる．\n\nBy default, levels of the same grouping factor are modeled as independent of each other.gr(): Set up basic grouping terms in brms\n\n\nすなわち，\\(\\mathrm{V}[\\eta_s]\\) には一切仮定が置かれておらず（第 3.1 節），一方で \\(\\{\\epsilon_{it}\\}_{t=1}^T\\) は互いに独立とされている．\n\n\n\nまた，この二階層目の分布族（第 2.1 節での \\(\\alpha_i\\) と \\(\\eta_{it}\\)）は，分散共分散行列 \\(\\mathrm{V}[\\eta_s]\\) を持った正規分布がデフォルトで，現状他の分布族は指定できないでいる．\n\ndist: Name of the distribution of the group-level effects. Currently “gaussian” is the only option.gr(): Set up basic grouping terms in brms\n\n\n\n4.3.2 autocor引数\nbrm()関数には，autocor引数 が用意されている．\ngr()のデフォルト値では独立とされていたグループ内の相関構造を，具体的に指定するのに用いられる．\n\n\n\n\n\n\n\nunstr：一才の仮定を置かない．\nAR：一次の自己相関構造．\n\n\n\n\n\n\n\n4.4 推論エンジン\nbrm関数 は，Stan による MCMC サンプリングを通じて，事後分布を計算する．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#文献紹介",
    "href": "posts/2024/Computation/brms.html#文献紹介",
    "title": "R によるベイズ混合モデリング入門",
    "section": "5 文献紹介",
    "text": "5 文献紹介\n\nここでは計量経済学の呼称に従い，固定効果モデルと変量効果モデルと呼んだが，同じモデルを母数モデル (fixed effect model) と変量モデル (random effect model) と呼んだりもする (足立浩平, 2000)．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#acknowledgements",
    "href": "posts/2024/Computation/brms.html#acknowledgements",
    "title": "R によるベイズ混合モデリング入門",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\n\nI would like to extend my gratitude to Robert Long, who kindly shared me the knowledge about the covariance structure implicitly defined via brms formula on this Cross Validated post. His insights were instrumental in enhancing this work."
  },
  {
    "objectID": "posts/2024/Computation/brms.html#footnotes",
    "href": "posts/2024/Computation/brms.html#footnotes",
    "title": "R によるベイズ混合モデリング入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR を最新バージョン 4.3.1 → 4.4.0 にアップデートしなければインストールに失敗したことに注意．↩︎\n通常は時間的に離れている観測は相関が薄いとしても，直近の観測と関連性が高いだろう．↩︎\nStatistical Modeling, Causal Inference, and Social Science における こちらのエントリ も参照．↩︎\nすなわち，ある確率変数の従う項と考えており，変量効果 とも呼ばれる．一方で未知母数とみなす場合は 母数効果 ともいう (久保川達也, 2006)．↩︎\n(Hansen, 2022, p. 333) 第12.3節，(Bafumi and Gelman, 2007, p. 3), (Hansen, 2022, p. 604)，(Gardiner et al., 2009, p. 228)．↩︎\n(Bafumi and Gelman, 2007, p. 5) や (久保川達也, 2006, p. 141) も参照．(Cunningham, 2021) は pooled OLS と呼んでいる．↩︎\n特にパネルデータの文脈では within estimator ともいう (Cunningham, 2021)．↩︎\n(Bafumi and Gelman, 2007, p. 5)，(Hansen, 2022, p. 609) 17.11節 など．狭義では，fixed effects model は within transformation を行い，グループ間の影響を引いたあとに回帰を実行する……という手続きを指すこともあるが，２つは等価な結果を生む．詳しくは (Cunningham, 2021) なども参照．↩︎\n(Hansen, 2022, p. 624) 17.25節．↩︎\n(Hansen, 2022, p. 624)，(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Hubbard et al., 2010) では両方の名前で呼んでいる．↩︎\n(Hansen, 2022, p. 625) 17.25節．疫学・生物統計学では，実験計画法でしか「固定効果」「変量効果モデル」とは言わない，という認識であることも筆者は聞いたことがある．↩︎\n\\(\\mathrm{V}[\\eta_s]\\) はブロック行列の構造を持つためこう呼ばれる．(久保川達也, 2006, p. 141) でも LMM と併記されている．↩︎\n(Laird and Ware, 1982)，(Chung et al., 2013)，(Chung et al., 2015)，Statistical Modeling, Causal Inference, and Social Science ブログ 6/2/2023．↩︎\n逆 Wishart ではないらしい (Chung et al., 2015)．↩︎\nSolomon Kurtz (2021) による解説，RPubs も参照．↩︎\nLine 1363．↩︎"
  }
]