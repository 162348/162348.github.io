[
  {
    "objectID": "static/AllCategories.html",
    "href": "static/AllCategories.html",
    "title": "Categories",
    "section": "",
    "text": "Mathematics\n\n\nProbability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSimulation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nProcess\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 連鎖\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nSimulation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\nProcess\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\nProcess\n\n\n\n\n2024-01-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nFunctional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nApplied Mathematics\n\n\n\\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOptimization\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nOptimization\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\nOptimization\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nNature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nBayesian Machine Learning\n\n\nBayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル３\n\n\nグラフィカルモデルの推論\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2024-02-17\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nParticles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装：リサンプリング編\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nKernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析２\n\n\n理論編\n\n\n\nKernel\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\nOptimization\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nBayesian Computation\n\n\nComputation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論４\n\n\n主成分分析\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-03-01\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル３\n\n\nグラフィカルモデルの推論\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2024-02-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSimulation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間への進化\n\n\n\nMCMC\n\n\nSimulation\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nSimulation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSimulation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nPython\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論４\n\n\n主成分分析\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-03-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装：リサンプリング編\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nJulia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n俺のための Julia 入門０\n\n\n数値計算への新たな接近\n\n\n\nJuliaLang\n\n\nLifestyle\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門１\n\n\nJulia による確率的プログラミング\n\n\n\nJuliaLang\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nArtificial Intelligence\n\nAI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nDeep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n数学者のための深層学習７\n\n\nエネルギーベースモデル\n\n\n\nDeep\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習５\n\n\n拡散モデル\n\n\n\nDeep\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習６\n\n\n正規化流\n\n\n\nDeep\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nFoundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n\n2024-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\n\n2024-03-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nOthers\n\n\nReview\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\nReview\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\nReview\n\n\n\n\n2023-10-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSurveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nOpinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nLife\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nLifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n俺のための Julia 入門０\n\n\n数値計算への新たな接近\n\n\n\nJuliaLang\n\n\nLifestyle\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n「穴あき式」の考え方\n\n\n\nLifestyle\n\n\n\n\n2024-01-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\nGit覚書\n\n\n\nLifestyle\n\n\n\n\n2023-11-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nSeries\n\n\n法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Categories.html",
    "href": "static/Categories.html",
    "title": "Categories",
    "section": "",
    "text": "Mathematics\n\n\nProbability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nComputation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nProcess\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nComputation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nFunctional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nApplied Mathematics\n\n\n\\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOptimization\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nOptimization\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\nOptimization\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nNature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nBayesian Machine Learning\n\n\nBayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nParticles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nKernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nBayesian Computation\n\n\nComputation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nComputation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nComputation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nPython\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nJulia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\n\nArtificial Intelligence\n\nAI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nDeep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n数学者のための深層学習７\n\n\nエネルギーベースモデル\n\n\n\nDeep\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習５\n\n\n拡散モデル\n\n\n\nDeep\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習６\n\n\n正規化流\n\n\n\nDeep\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nFoundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nOthers\n\n\nReview\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\nReview\n\n\n\n\n2023-10-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSurveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nLife\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nLifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nSeries\n\n\n法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "",
    "text": "Hirofumi Shiba is a Ph.D. candidate at the Institute of Statistical Mathematics (ISM) in Japan, specializing in Bayesian Computation under Prof. Kengo Kamatani.\nHe is particularly focused on devising more efficient and scalable Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) algorithms by leveraging the insights from their continuous-time limits.\nHis ultimate goal is to create sampling methods that remain effective for large-scale and complex models, which is crucial for pushing the application of Bayesian machine learning methods in real-world scenarios."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "1 Education",
    "text": "1 Education\n\nPh.D. in Statistical Science\nGraduate University for Advanced Studies, SOKENDAI, Tokyo, Japan\nApr. 2023 - Mar. 2028 (expected)\nB.A. in Mathematics\nUniversity of Tokyo, Japan\nApr. 2019 - Mar. 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "2 Experience",
    "text": "2 Experience\n\nResearch Assistant at the Institute of Statistical Mathematics\nJuly 2023 - present\nCooperative Researcher at the RCAST, the University of Tokyo\nApr. 2023 - present"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "3 Biography",
    "text": "3 Biography\nHe was born in Yokohama Japan in May 1999. He received his Bachelor of Science degree in Mathematics from the Univ. of Tokyo in 2023, where he was mentored by Prof. Nakahiro Yoshida.\nExpanding his study on statistical inference for stochastic processes, he is now further specializing in sampling methods and interacting particle methods under the supervision of Prof. Kengo Kamatani.\nHe speaks, reads, and writes in Japanese, Mandarin Chinese, and English.\nHe also contributes to the YUIMA package, an open-source projectaiming to simulate and infer multidimensional stochastic differential equations.\n\nAdditionally, he is a cooperative researcher at RCAST, the University of Tokyo, where he is engaged in a project that aims to build trustworthy AI systems and a robust semiconductor ecosystem from a policy perspective, incorporating uncertainty quantification as a key component."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Interests",
    "text": "Interests\nI am deeply fascinated by\n\nProbabilistic Modelling, a domain dedicated to developing machine learning models that can simultaneously capture both structure and uncertainty in data in a principled manner, as well as\nBayesian Computation, which focuses on developing and improving computational methods for Bayesian inference and learning.\n\nMy current interest lies in understanding and acquiring unified perspectives on sampling and optimization as a flow on the space of probability measures, \\(\\mathcal{P}(X)\\).\nAt my core, I am an applied mathematician, driven by a desire to foster mutual understanding among humans, nature, and computers, utilizing mathematics as a common language. As we are still in the process of learning, it is remarkable that (probability) measures and (Markov) kernels have proven to be unreasonably effective in describing the world models of all three aforementioned entities.\n\nThe true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as in poetry. – Bertrand Russell (1959) My Philosophical Development"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html",
    "href": "posts/Surveys/SMCSamplers.html",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\nSMC の文脈で，目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) が複雑であるとき，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをする，というアイデアを 調温 (tempering) という（粒子フィルターの稿 も参照）．\nこの tempering という考え方は本質的に逐次的な発想を持っているが，元々は SMC の文脈とは全く独立に，MCMC を多峰性を持つ複雑な分布に対しても使えるように拡張する研究で提案された．これらの手法が自然と SMC へと接続する様子を population Monte Carlo (Iba, 2001b), (Jasra et al., 2007) というキーワードで理解されている．\nまずはその歴史を概観する．いずれも，目標分布 \\(\\pi_p\\) が多峰性をもち，MCMC がうまく峰の間を遷移できずに正しいサンプリングができない（収束が遅くなる）問題を解決する文脈の中で捉えられる．\n\n\nこれは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，凝縮系物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\mathop{\\mathrm{arg\\,min}}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3\n\n\n\nMCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ (岡本祐幸, 2010)．\nmultilevel sampling とも呼ばれる．4\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Jasra et al., 2007)．\n\n\n(Torrie & Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．5\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering6 または exchange Monte Carlo (Hukushima & Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，7 さらには population-based MCMC8 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita & Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や擬似テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．9\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．10\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．11 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang & Wong, 2000), (Liang & Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または擬似アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 擬似テンパリング (simulated tempering) (Marinari & Parisi, 1992) である．12\n擬似アニーリングでは温度は下がる一方であったのが，擬似テンパリングでは温度もある周辺分布に従って遷移する．擬似アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，擬似テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n擬似テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．13 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer & Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg & Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．14\n\n\n\n\n\n\ntempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．15\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．16 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nテンパリング遷移の後半のアルゴリズムを発展させた形である．\n\n\n\nこちらは擬似テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．\n\n\n\n\n\n\n\n\n\n目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe & Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#mc3-並行テンパリング",
    "href": "posts/Surveys/SMCSamplers.html#mc3-並行テンパリング",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "まず最初の発想は，目標分布 \\(\\pi_p\\) が多峰性をもち，MCMC がうまく峰を見つけられず，収束が遅くなる問題を解決する中で生まれた．\nそこで，峰の間で遷移する動きを，不変分布を変えないように MCMC に加えることで，収束性が改善できないかと考えられた．\n峰を全て特定し，正しいステップサイズを選択するために，複数の MCMC を同時に走らせる MC3 (Metropolis-Coupled MCMC) という手法が (Geyer, 1991) により提案された．\nこれは \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として独立な MCMC を実行し，時折 Metropolis 核の提案に従って不変分布を変えないようにそれらの位置を交換するという手法である．\nこの手法は parallel tempering1 または レプリカ交換法，さらには population-based MCMC2 とも呼ばれる．\nしかしながら，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまう．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#擬似テンパリング",
    "href": "posts/Surveys/SMCSamplers.html#擬似テンパリング",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "焼きなまし法（または擬似アニーリング） (Kirkpartick et al., 1983) の改良として最適化の文脈で提案されたのが 焼き戻し法，または 擬似テンパリング (simulated tempering) (Marinari & Parisi, 1992) である．\nこれは状態空間を \\(E\\times [p]\\) に拡大し，4 その上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer & Thompson, 1995)．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#テンパリング遷移-neal1996",
    "href": "posts/Surveys/SMCSamplers.html#テンパリング遷移-neal1996",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．10\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#焼きなまし重点サンプリング-neal2001",
    "href": "posts/Surveys/SMCSamplers.html#焼きなまし重点サンプリング-neal2001",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "ここで初めて SMC の文脈にもテンパリングが輸入された．14 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nテンパリング遷移の後半のアルゴリズムを発展させた形である．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#重点テンパリング-gramacy2010",
    "href": "posts/Surveys/SMCSamplers.html#重点テンパリング-gramacy2010",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "こちらは擬似テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#多峰性の最適化に基づく対処",
    "href": "posts/Surveys/SMCSamplers.html#多峰性の最適化に基づく対処",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe & Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#footnotes",
    "href": "posts/Surveys/SMCSamplers.html#footnotes",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの前にも，Umbrella sampling (Torrie & Valleau, 1977) が本質的には密度の調温のアイデアを用いていた．(Liu, 2004, pp. 206–206) Section 10.1 も参照．↩︎\n分子動力学 (molecular dynamics) などの文脈では Metropolis 法はちょうど分子運動のシミュレーションになっていることを踏まえれば，これを simulated annealing と呼ぶのは極めて鮮やかなアナロジーとなっている．焼きなまし法自体も，シミュレーション可能になったのである．↩︎\n(Geman & Geman, 1984) によると，各 \\(\\pi_n\\) における MCMC move の回数を \\(N_n\\) とした場合，\\(O(\\log(N_1+\\cdots+N_n))\\) のオーダーで \\(T_n\\) を（十分遅く）変化させれば，この手法はほとんど確実に \\(\\mathop{\\mathrm{arg\\,min}}h\\) 内に収束する．(Liu, 2004, pp. 209–209) 10.2 節も参照．↩︎\n(Liu, 2004, pp. 205–205) Chapter 10. Multilevel Sampling and Optimization Methods も参照．↩︎\n(Liu, 2004, p. 207) も参照．↩︎\n(Chopin et al., 2023), (Liu, 2004, p. 4) でも (Geyer, 1991) を引用して PT と呼んでいる．一方で物理学の分野では (Hukushima & Nemoto, 1996) の exchange Monte Carlo や (Swendsen & Wang, 1986) などの文献もある．前者は (Liu, 2004, p. 4) が “is reminiscent of parallel tempering (Geyer, 1991)” と指摘しており，後者は (Bouchard-Côté et al., 2012) などが引用している．↩︎\n最終講義 スピングラスと計算物性物理 p.34 も参照．温度の違う熱浴につけたレプリカをシミュレートして，時々交換する，という見方ができるためにこう呼ぶ．↩︎\n(Jasra et al., 2007) は (Geyer, 1991) も指して population-based と呼んでおり，この言葉自体は (Iba, 2001b) からとったという．“we define a population-based simulation method as one which, instead of sampling a single (independent/dependent) sample, generates a collection of samples in parallel” と定義しており，大きく MCMC によるものと逐次重点サンプリングベースのものの２流儀あるとしている．(Liu, 2004, pp. 225–225) 第11章なども参照．↩︎\n(岡本祐幸, 2010) など．↩︎\n(Behrens et al., 2012, p. 66) も参照．↩︎\n(Iba, 2001a) が良い解説を与えていると (Jasra et al., 2007) でも言及されている．ただし，(Iba, 2001a) はこの並行テンパリングだけでなく，擬似テンパリング，multicanonical Monte Carlo (Berg & Neuhaus, 1991) / Adaptive Umbrella Sampling (Torrie & Valleau, 1977) を総称して 拡張アンサンブル法 (Extended Ensemble Monte Carlo) と呼んでサーベイしていることに注意．↩︎\n(Lyubartsev et al., 1992) が引用されることもある．(酒井佑士, 2017), (岡本祐幸, 2010) など．method of expanded ensemble とも呼ばれる (岡本祐幸, 2010), (Iba, 2001a)．↩︎\n記法 \\([p]=\\{1,\\cdots,p\\}\\) は 本サイトの数学記法一覧 を参照↩︎\nその後すぐに分子シミュレーションの分野にも導入された．(岡本祐幸, 2010) も参照．↩︎\n(Behrens et al., 2012) も参照．↩︎\n(Chopin & Papaspiliopoulos, 2020, p. 33) で，SMC を調温に初めて応用した論文として紹介されている．p.352 では “An early version of SMC tempering (without resampling)” としている．↩︎"
  },
  {
    "objectID": "posts/2023/FunctionalAnalysis/BoundedMeasure.html",
    "href": "posts/2023/FunctionalAnalysis/BoundedMeasure.html",
    "title": "「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\\(B\\) をBanach空間または拡張実数 \\([-\\infty,\\infty]\\) とする．可測空間 \\((S,\\Sigma)\\) 上の \\(B\\)-値 \\(\\sigma\\)-加法的関数 \\(\\mu:\\Sigma\\to B\\) を考える．\nこの設定の下で，\\(B=[-\\infty,\\infty]\\) のときに有限（＝\\(\\mathbb{R}\\)-値）であること，有界であること，有界変動である（全変動が有限）ことの３概念がどのように関係するかを見る．"
  },
  {
    "objectID": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#sec-1",
    "href": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#sec-1",
    "title": "「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures",
    "section": "1 Jordan分解からの理解",
    "text": "1 Jordan分解からの理解\nJordan分解より，符号付き測度 \\(\\mu:\\Sigma\\to[-\\infty,\\infty]\\) が関数として有界であることと，全変動が有限（＝\\(\\mathbb{R}\\)-値）になること（有界変動であること）とは同値になる．\n\n\n\n\n\n\n定理 (Hahn, 1921)\n\n\n\n\\((S,\\Sigma)\\) を可測空間，\\(\\mu:\\Sigma\\to[-\\infty,\\infty]\\) を符号付き測度とする．このとき，ある可測集合 \\(E_0\\in\\Sigma\\) が存在して，\\(\\mu:E_0\\cap\\Sigma\\to[0,\\infty]\\) は非負で，\\(\\mu:E_0^\\complement\\cap\\Sigma\\to[-\\infty,0]\\) は非正である．\n\n\n証明は (藤田宏，吉田耕作, 1991, p. 385) 定理7.2，(Dunford & Schwartz, 1958, p. 129) 定理III.4.10, (Dudley, 2002, pp. 178–179) 定理5.6.1 など．(Bogachev & Smolyanov, 2020, p. 72) 定理2.6.1, (Lang, 1993, p. 203) 系VII.3.6 も同様だが，\\(\\mathbb{R}\\)-値の場合に限って証明を与えている．\n\n\n\n\n\n\n系5 （Jordan分解）\n\n\n\n\\((S,\\Sigma)\\) を可測空間，\\(\\mu:\\Sigma\\to[-\\infty,\\infty]\\) を符号付き測度とする．このとき， \\[\n\\mu^+(E):=\\mu(E_0\\cap E),\n\\] \\[\n\\mu^-(E):=-\\mu(E_0^\\complement\\cap E)\n\\] はHahn分解 \\(E_0\\in\\Sigma\\) の取り方に依らずに定まる測度となる．\n\n\n\n系2 (Hahn) \\(\\Phi(E)\\) が最大値 \\(+\\infty\\) をとれば，\\(\\Phi(E)\\) の最小値は \\(&gt;-\\infty\\)．また \\(\\Phi(E)\\) が最小値 \\(-\\infty\\) をとれば \\(\\Phi(E)\\) の最大値は \\(&lt;+\\infty\\)．とくに \\(\\Phi(E)\\) が有限であるならば（すなわち有限値しかとらないならば），\\(\\Phi\\) の値域 \\(\\{\\Phi(E)\\mid E\\in\\mathcal{M}\\}\\) は有界である．(藤田宏，吉田耕作, 1991, p. 389)"
  },
  {
    "objectID": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#有界変動性と有界性",
    "href": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#有界変動性と有界性",
    "title": "「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures",
    "section": "2 有界変動性と有界性",
    "text": "2 有界変動性と有界性\n前節では有界性（値域が \\(\\mathbb{R}\\) の有界集合になること）を証明したが，符号付き測度ではこれは有界変動性（全変動が有限＝有界）と同値である．\n\n\n\n\n\n\n定理（全変動）\n\n\n\n\\((S,\\Sigma)\\) を可測空間，\\(\\mu:\\Sigma\\to[-\\infty,\\infty]\\) を符号付き測度とする．このとき，次が成り立つ： \\[\n\\begin{align*}\n    &\\nu^+(E)+\\nu^-(E)\\\\\n    &=\\sup\\left\\{\\sum_{k=1}^n\\lvert\\nu(E_k)\\rvert\\in[0,\\infty]\\:\\middle|\\:\\substack{n\\in\\mathbb{N}^+,\\{E_k\\}\\subset\\mathcal{E}\\;\\text{は}\\\\E\\;\\text{の分割}}\\right\\}.\n\\end{align*}\n\\] これを \\(\\nu\\) の 全変動 と呼び，\\(\\lvert\\nu\\rvert\\) で表す．\n\n\n\n\n\n\n\n\n例6 （全変動）\n\n\n\n\\((S,\\Sigma,\\nu)\\) を測度空間とする．\n\n\\(\\lvert\\nu\\rvert=\\nu\\) である．特に，この場合有界測度と有界変動測度の概念は一致する．\n\\(f\\in L^1(\\nu)\\) について， \\[\nF(E):=\\int_Ef(x)\\,\\nu(dx),\\qquad E\\in\\Sigma,\n\\] は \\(\\sigma\\)-加法的関数である．この全変動は \\[\\lvert F\\rvert(E)=\\int_E\\lvert f(x)\\rvert\\,\\nu(dx).\\]\n全変動 \\(\\lvert\\nu\\rvert:\\Sigma\\to[0,\\infty]\\) は \\[\\|\\nu(E)\\|\\le\\lambda(E),\\qquad E\\in\\Sigma,\\] を満たす \\(\\sigma\\)-加法的関数 \\(\\lambda\\) のうち最小のものである．"
  },
  {
    "objectID": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#sec-2",
    "href": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#sec-2",
    "title": "「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures",
    "section": "3 直接の証明",
    "text": "3 直接の証明\n\n\n\n\n\n\n命題7 （有限ならば有界）\n\n\n\n\\((S,\\Sigma)\\) を可測空間，\\(\\mu:\\Sigma\\to[-\\infty,\\infty)\\) を \\(\\sigma\\)-加法的な集合関数とする．このとき，\\(\\mu\\) は上に有界である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n仮に \\(\\mu\\) は上に有界ではないと仮定して，矛盾を導く．可測集合 \\(E_1\\in\\Sigma\\) が非有界集合であるとは， \\[\\sup_{E\\in\\Sigma}\\mu(E\\cap E_1)=+\\infty\\] が成り立つこととすると，仮定より，少なくとも全体集合 \\(S\\) は非有界である．ここで，\n\n任意の非有界集合は，任意に大きな測度を持つ非有界部分集合を持つ．\nある非有界集合 \\(F\\in\\Sigma\\) が存在して，ある \\(N\\in\\mathbb{N}\\) よりも大きな測度を持つ \\(F\\) の非有界部分集合は存在しない．\n\nの２つの場合に分けられる．\n\nこのとき，減少列 \\(\\{E_n\\}\\subset\\Sigma\\) であって \\(\\mu(E_n)\\ge n\\;(n\\in\\mathbb{N}^+)\\) を満たすものが取れる．このとき，\\(\\sigma\\)-加法性から \\[\n\\begin{align*}\n&\\mu\\left(\\bigcap_{i=1}^\\infty E_i\\right)+\\sum_{i=n}^\\infty\\mu(E_i\\setminus E_{i+1})\\\\\n&\\qquad=\\mu(E_n)\n\\end{align*}\n\\] が成り立つが，仮定より \\(\\mu(E_n)&lt;\\infty\\) だから，左辺の第二項の無限和は任意の \\(n\\in\\mathbb{N}^+\\) について収束することがわかる．よって，\\(n\\to\\infty\\) の極限を考えることで右辺は発散するから，左辺も第一項が発散している必要がある： \\[\\mu\\left(\\bigcap_{i=1}^\\infty E_i\\right)=\\lim_{n\\to\\infty}\\mu(E_n)=\\infty.\\] これは \\(\\bigcap_{i=1}^\\infty E_i\\in\\Sigma\\) に矛盾．\n条件を満たす \\(F\\in\\Sigma\\) を取り，ある可測部分集合 \\(F_1\\subset F\\) は \\(\\mu(F_1)=\\mu(F_1\\cap F)&gt;N\\) を満たすとする．すると \\(F_1\\) は有界である必要があるが，\\(F\\) は非有界としたから，\\(F\\setminus F_1\\) が非有界である必要がある．よって可測部分集合 \\(A_1\\subset F\\setminus F_1\\) で \\(\\mu(A_1)\\ge1\\) を満たすものが取れる．すると \\(F_2:=F_1\\cup A_1\\) も \\(\\mu(F_2)\\ge\\mu(F_1)&gt;N\\) より，やはり有界である必要がある．これを繰り返すことで， \\[\\mu\\left(\\bigcup_{i=1}^\\infty A_i\\right)=\\infty\\] を満たす \\(\\{A_i\\}_{i\\in\\mathbb{N}^+}\\subset\\Sigma\\) が見つかってしまう．\n\n\n\n\n\n\n\n\n\n系（ベクトル値測度も有限）\n\n\n\n\\(B\\) をBanach空間，\\(\\mu:\\Sigma\\to B\\) を \\(\\sigma\\)-加法的関数とする． このとき，\\(\\mu\\) の値域は有界である． しかし，\\(\\lvert\\mu\\rvert\\) が有限とは限らないことに注意．"
  },
  {
    "objectID": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#sec-3",
    "href": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#sec-3",
    "title": "「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures",
    "section": "4 一般のベクトル値測度の場合",
    "text": "4 一般のベクトル値測度の場合\n一般の Banach 空間値の \\(\\sigma\\)-加法的集合関数について，値域が有限次元であるならば自動的に有界になることが示せる．\n\n\n\n\n\n\n命題8 （有限ならば有界）\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間，\\(B\\) をBanach空間，\\(\\nu:\\mathcal{E}\\to B\\) を可算加法的集合関数とする．\n\n全変動 \\(\\lvert\\nu\\rvert:\\mathcal{E}\\to[0,\\infty]\\) も測度である．\n\\(B\\) が有限次元ならば，\\(\\lvert\\nu\\rvert\\) は有限である．特に，\\(\\mu\\) は有界である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n仮に \\(\\lvert\\nu\\rvert(E)=\\infty\\) と仮定して矛盾を導く．\\(B=\\mathbb{R}\\) として示せば，あとは成分ごとに考えることで一般次元の場合も同様である．\n\\(A_1:=E\\) から始まる減少列を定める．全変動の定義から，ある部分集合 \\(B\\in\\mathcal{E},B\\subset A_1\\) が存在して， \\[\\lvert\\nu(B)\\rvert\\ge\\lvert\\nu(A_1)\\rvert+2\\] を満たす．\\(\\lvert\\nu\\rvert(B)=\\infty\\) のとき \\(A_2:=B\\) とし，\\(\\lvert\\nu\\rvert(B)&lt;\\infty\\) のとき\\(A_2:=A_1\\setminus B\\) とすると， \\[\\lvert\\nu\\rvert(A_1\\setminus B)=\\lvert\\nu\\rvert(A_1)-\\lvert\\nu\\rvert(B)=\\infty.\\] このとき，三角不等式から，どちらの場合も \\[\\lvert\\nu(A_2)\\rvert\\ge\\lvert\\nu(B)\\rvert-\\lvert\\nu(A_1)\\rvert\\ge2.\\] これを繰り返すことで，\\(\\lvert\\nu(A_n)\\rvert\\ge n\\;(n\\in\\mathbb{N}^+)\\) を満たす減少列 \\(\\{A_n\\}_{n=1}^\\infty\\) を得る．するとこの極限 \\(A:=\\bigcap_{n=1}^\\infty A_n\\) の測度は発散するが，これは \\(\\nu\\) が \\(B\\)-値であることに矛盾する．よって，\\(\\lvert\\nu\\rvert\\) は有限である．\n\n\n\n\n\n\n\n\n反例9 （無限次元の場合）\n\n\n\n\\(B=l^2\\) など無限次元の場合，\\(\\nu\\) が \\(B\\)-値であっても，その全変動 \\(\\lvert\\nu\\rvert\\) は発散し得る． これは \\(E\\) 上のノルムが同値になるとは限らないためである．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(X=\\mathbb{R}_+\\) とし，その上の（\\([n-1,n]\\) 上の）測度を，Lebesgue 測度 \\(\\ell\\) を通じて \\[\n\\nu_n(A):=\\frac{1}{n}\\ell(A\\cap[n-1,n]),\n\\] \\[\nn\\in\\mathbb{N}^+,A\\in\\mathcal{B}(\\mathbb{R}_+)_{\\ell},\n\\] と定め，\\(\\nu(A)_n:=\\nu_n(A)\\) とすると，確かに \\(\\nu(A)\\in l^2\\)： \\[\n\\begin{align*}\n    \\|\\nu(A)\\|_2&=\\sqrt{\\sum_{n=1}^\\infty\\biggr(\\frac{\\ell(A\\cap[n-1,n])}{n}\\biggl)^2}\\\\\n    &\\le\\sqrt{\\sum_{n=1}^\\infty\\frac{1}{n^2}}&lt;\\infty.\n\\end{align*}\n\\] 一方で，全変動は発散する： \\[\n\\begin{align*}\n    \\lvert\\nu\\rvert(\\mathbb{R}_+)&=\\sum_{n=1}^\\infty\\lvert\\nu_n([n-1,n])\\rvert\\\\\n    &=\\sum_{n=1}^\\infty\\frac{1}{n}=\\infty.\n\\end{align*}\n\\]\n\n\nでは，ベクトル値測度の有界性は何と同値であるかというと，有界半変動性 (bounded semivariation) という有界変動性よりも弱い概念と同値になる．(Diestel & Uhl Jr., 1977) 参照．"
  },
  {
    "objectID": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#footnotes",
    "href": "posts/2023/FunctionalAnalysis/BoundedMeasure.html#footnotes",
    "title": "「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nここで符号付き測度とは，\\(\\sigma\\)-加法的な集合関数 \\(\\mu:\\Sigma\\to[-\\infty,\\infty]\\) であって，値域には \\(\\infty,-\\infty\\) のいずれか一方しか含まれないもの，としている．数学記法一覧 も参照．↩︎\n(Giesy, 1970)↩︎\n(Dudley, 2002, p. 181) 命題5.6.4．↩︎\n(Dunford & Schwartz, 1958, p. 127) III.4.5．IIII.10.2 p.319 も参照．↩︎\n(Dunford & Schwartz, 1958, p. 130) では，有限なJordan測度に関連する分解をJordan分解，有限とは限らない測度に関する分解をHahn分解と呼び分けているが，現代では空間の分解をHahn，測度の分解をJordanと呼び分けることが主流であるようである．(Halmos, 1950) も後者．↩︎\n(1), (3)は (Dunford & Schwartz, 1958, p. 97)．(2)は (藤田宏，吉田耕作, 1991, p. 389) 例7.1，(Rudin, 1987, p. 125) 6.13．↩︎\n(Dunford & Schwartz, 1958, p. 127) 補題III.4.4 参照．↩︎\n(Lang, 1993, pp. 定理3.2 p.197)．↩︎\n(Lang, 1993, p. 198). (Birkhoff, 1935) も参照．↩︎"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html",
    "href": "posts/2023/Surveys/BayesianComp.html",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "",
    "text": "History of Bayesian Computation (Martin et al., 2023, p. 4)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズまでの統計学の黎明",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズまでの統計学の黎明",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.1 ベイズまでの統計学の黎明",
    "text": "1.1 ベイズまでの統計学の黎明\n統計学の黎明を要請したものは，社会への不安であった．筆者に言わせれば，この社会への不安を直視したのがドイツ，数で解決しようとしたのがイギリスで，解決への筋道を確率論で基礎づけたのがフランスである．\n\nプロシヤにおける国勢学，イギリスにおける政治算術，フランスにおける古典確率論–統計学はこれら３つの異った厳選を持つと言われる (増山元三郎, 1950, p. 6)．\n\n17世紀初頭から何度も流行を繰り返し，遂に1665年にはロンドンの人口の1/4を死に至らしめた ペストの大流行 は恐怖の対象であった．パンデミックは現代でも恐怖の対象であるが，当時はその全貌の把握が難しく，これが第一に切望された．月の運行による健康被害，国王の統治が疫病を引き起こす，などの俗見が流布していた時代である．しかし，「数」という解決手段は極めて功を奏した．\n数による解決が他でもないイギリスから生まれたことは，Francis Bacon 1561-1626 と Thomas Hobbes 1588-1679 に象徴される自然科学の風土，「Aristotelesの三段論法を通じて，経験的に因果関係を発見することで，我々は自然を理解できる」という希望が当時のイギリスには存在したことが挙げられる．\n\n海へ行け，きっと獲物があるぞという先輩が Bacon であった．漁獲法一般の講義をする先生が，例えば後世の J. S. Mill の帰納論理学に相当するのである．統計学を作った漁師たちは，Mill 先生の帰納法の論理学の講義などは，上の空で聞いた．そして各自の漁獲法を自らの浜で覚えたのである． (北川敏男, 1949, p. 12)\n\n\n新大陸の発見，東洋への海路の開拓により，世界商業の中心は砂漠の商隊と地中海の商人とを介して栄えていたイタリアから，スペイン・ポルトガル・オランダを経て16世紀の終り頃には，すでにイギリスに移っていたのである．ここに興隆の一路を辿る市民社会・殷盛を極める海上貿易・繁栄する英都の商業・封建制の崩壊を示す Cromwell 革命 (1649) 後のイギリス社会に，市民科学としての政治算術が起ったことは敢えて異とするに足りないであろう (増山元三郎, 1950, p. 10)．\n\n\n1.1.1 John Grauntの死亡表\nペスト流行の激しさの判定に寄与する人口状況を，最初に数によって理解しようとしたのが John Graunt 1620-1674 であった．\n当時の英国王立理学協会1 は，封建的な諸関係の崩壊解消と同時に，商品生産・貨幣による売買の全面支配によって貨幣的表現が富の大部分に侵入したことにより新たに誕生した市民階級が勢力を占めており，Graunt もこのような商人階級の出身であった．\nそのような身分の Graunt が英国王の推薦を受けて王立協会員の名誉を勝ち取った論文 (Graunt, 1662) は，ギルド発行の死亡統計 Bills of Mortality と教会に蓄積していた統計資料2 から統計的な処理を通じて世界初の「死亡表」を作成し，次の内容を初めて結論づけた．\n\n36%の幼児は６歳未満で死亡する．\n洗礼数をみると，男女比は16:15くらいである．\n都市の死亡率は地方より高い．\nLondonの城外では死亡率は３倍である．\n\n加えてLondonの世帯数を3通りの方法で推算し，世帯数は5万であろうと結論づけた．なお，当時の俗見ではLondon人口は100万と言われていた．\nその後このような「生命表」は精緻化の一途を辿り，イギリスのギルド的な共助制度の土壌の上で，生命保険の成立という実を結んだ．\n\n\n1.1.2 統計学への期待と希望\nこのイギリスの数を使った解決は，政治算術学派 と呼ばれ，海外への輸出が進んだ．\nドイツの牧師 Johann Peter Süβmilch 1707-1767 は Graunt に倣って，教会に蓄積していた統計資料を用い，出生率の性別比が長期的には女性1,000対男性1,050に収束することを発見した．\n中でも特に，「たくさんのデータを集めると何かが見えてくる」ことに大きな希望を持ち，Graunt が教会の資料に注目したことを Columbus の新大陸発見になぞらえている．そう，歴史上最初の統計分析は，教会の資料によるものであったのである．\n\n若し我々が家を一軒一軒数えていくならば，ある家では娘だけに，またある家では息子だけに，あるいはそうでなくとも，非常に不釣り合いな両者の配合にでくわすであろう．小さな社会や村落でも秩序的なものを認めることは，容易ではない．（中略）．かかる場合に，誰が，能く規則と秩序とに想達し得るだろう．所で，教会の記録はこの秩序の確認のための大きな手段である．それは教会用及び世俗用のためにすでに数世紀前から取られ，とくに宗教改革後はかなり正確にとられてきた．誰がそれを利用したか？その発見はアメリカ発見と同時に可能であったのだ．（中略）それをGrauntがなし得たのである． –Süβmilch (1741) 『神の秩序』 (Göttliche Ordnung) 訳文は (北川敏男, 1949) より．\n\nこのように Süβmilch は男児の出生率の方が高いことを神の存在証明と見なしたのであった．この宗教的な外被を取り去るには，確率論の登場をまたねばならなかったが，これにはさらにフランスの学派が合流するのを待つ必要があり，それには100年を要したのであった（ 節 1.7 も参照）．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズが取り組んだ問題",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズが取り組んだ問題",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.2 ベイズが取り組んだ問題",
    "text": "1.2 ベイズが取り組んだ問題\nというわけで，\\(i=1,\\cdots,n\\) 番目の世帯の新生児が，男児である \\(y_i=1\\) か女児である \\(y_i=0\\) かのデータなどから，人口・疫病・国家動態に役立つ知識を引き出すことが当時の重要な問題意識であることをわかっていただけただろう．\nイギリスの牧師 Thomas Bayes 1701-1761 は，より抽象的な設定で統計的推定の問題を研究していた．Bayes は就中，次のような区間推定の問題を考えていた．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\n\n\nベイズが取り組んだ問題（現代語訳）\n\n\n\n2値のデータ \\(Y_i\\in\\{0,1\\}\\) は，ある未知の「成功率」 \\(\\theta\\in(0,1)\\) に従って， \\[\nY_i=\\begin{cases}\n1&\\text{確率 }\\theta\\text{ で}\\\\\n0&\\text{残りの確率} 1-\\theta\\text{ で}\n\\end{cases}\n\\] という値を取るとする．3 このようなデータの独立観測標本 \\(\\boldsymbol{y}:=(y_1,\\cdots,y_n)^\\top\\) から．神のみぞ知る，このデータ \\(\\boldsymbol{y}\\) を生み出した真の成功率 \\(\\theta\\) が，区間 \\((a,b)\\subset(0,1)\\) に入っているという確率 \\(\\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]\\) をどう見積もれば良いか？\n\n\nここでは引き続き \\(Y_i\\) は性別で，\\(\\theta\\) は男児が生まれる確率 \\(\\theta=\\operatorname{P}[Y_i=1]\\) だと解釈する．Bayes 自身は「ある未知の位置に白線が引かれたテーブル上にボールを \\(n\\) 個転がし，それぞれの領域に幾つのボールが入ったかの情報のみから，白線の位置を推定する」という表現によって問題を定式化した (Bayes, 1763)．これは後世ではビリヤード台の問題とも呼ばれた．こちらのサイトも参照．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズのアイデア",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズのアイデア",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.3 ベイズのアイデア",
    "text": "1.3 ベイズのアイデア\n彼の発想は極めてシンプルであり，次の3段階によって推定を試みた：\n\n事前分布 \\(p(\\theta)\\) と呼ばれる，最初の \\(\\theta\\in(0,1)\\) に対する予想 \\(p(\\theta)\\) を自由に表現する．4\n事前分布のデータ \\(\\boldsymbol{y}\\) を観測した下での条件付き分布 \\[p(\\theta|\\boldsymbol{y}):=\\frac{p(\\theta,\\boldsymbol{y})}{p(\\boldsymbol{y})}\\] を計算する．これを事後分布という．\nこの事後分布 \\(p(\\theta|\\boldsymbol{y})\\) の形から区間推定を実行する．\n\nこの 3.の部分は，Bayes が特に区間推定に拘ったためのものであり，点推定でも良ければ次期予測でも良い．推定対象 3.を目的に応じて自由に入れ替えても，1.と 2.の部分が同じように動作するということ，これがベイズ統計学の枠組みである．\nそれだけに事後分布というものが表現力に富んでいるのである．また，以下の例で納得していただけるかもしれないが，ベイズ統計学の手続きは「眼前のデータは，事前の信念を変えるのにどれほど説得的であるか？」という観点からも見れ，定量的であると同時に定性的な判断も可能にする． 節 3.6 で紹介するように，この特徴は意思決定への応用おいても重要である．\n\n\n\n\n\n\n問題に対する (Bayes, 1763) の解決\n\n\n\n\nまず，事前分布 \\(p(\\theta)\\) を設定する．Bayes は前述のビリヤードの問題を考えていたこともあり， 「\\(\\theta\\in(0,1)\\) は全く予想がつかない」「どんな \\(\\theta\\) も同様にあり得る」という立場を取った．横軸を \\(\\theta\\in(0,1)\\) の値，縦軸を「主観的にあり得ると思う度合い」として図で表すと次の通りである：\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the range for x-axis\nx = np.linspace(0, 1, 1000)\n\n# Uniform distribution density function is constant\ny = np.ones_like(x)\n\n# Plot the graph\nplt.figure(figsize=(3, 2)) # Size suitable for a smartphone screen\nplt.plot(x, y, label='Uniform Distribution (0,1)', color=(0.35, 0.71, 0.73, 1))\nplt.fill_between(x, y, color=(0.35, 0.71, 0.73, 0.3))\nplt.xlabel('x')\nplt.ylim(0, 1.5)\nplt.ylabel('Density')\nplt.title('Posterior Distribution')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nこの図が表す \\((0,1)\\) 上の確率分布を一様分布という．このように，一様分布とは「どのような \\(\\theta\\) の値も同様に確からしい」という予想の表現である．\n\n次に，データ \\(\\boldsymbol{y}=(y_1,\\cdots,y_n)^\\top\\) が観測された後の条件付き分布 \\(p(\\theta|\\boldsymbol{y})\\) を計算することで，本データ \\(\\boldsymbol{y}\\) が事前の信念 \\(p(\\theta)\\) をどのように変えてしまうかを観る．簡単な確率論の結果として，条件付き分布は次の公式によって計算できる（講義ノートも参照）：5 \\[\np(\\theta|\\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|\\theta)p(\\theta)}{\\int_\\Theta p(\\boldsymbol{y}|\\theta)p(\\theta)\\,d\\theta}\n\\tag{1}\\]\n例えば 日本の2021年の出生児性別のデータ を用いると次のようになる．\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# パラメータの設定\nn = 811622\nmale = 415903\nfemale = n - male\n\n# ベータ分布のPDFを計算\nx = np.linspace(0, 1, 1000)\ny = beta.pdf(x, 1+male, 1+female)\n\n# プロット\nplt.figure(figsize=(3, 2))\nplt.plot(x, y, label=f'Beta({1+male}, {1+female})', color=(0.35, 0.71, 0.73, 1))\nplt.fill_between(x, y, color=(0.35, 0.71, 0.73, 0.3))\nplt.xlabel('p')\nplt.xlim(0.4, 0.6)\nplt.ylabel('Probability Density')\nplt.title('Bayesian Posterior Distribution')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nこうして極めて鋭い事後分布が出来た．事前に設定した分布 \\(p(\\theta)\\) は極めて平坦な一様分布であったのに，それをデータで条件付けた \\(p(\\theta|\\boldsymbol{y})\\) には極めて鋭いスパイクが現れたのである．式 1 を認めるならば，この図は「男児の方が女児よりも生まれる確率が高い」ことの証拠として，極めて説得的ではないだろうか？\n\nでは区間推定の例として，\\((a,b)=(0.5,1.0)\\) として，「男児の方が女児よりも多い確率」を推定しよう．これは次を計算することになる： \\[\n\\begin{align*}\n&\\operatorname{P}\\left[\\frac{1}{2}&lt;\\theta&lt;1\\right]\\\\\n&=\\int^1_{\\frac{1}{2}}p(\\boldsymbol{y}|\\theta)\\,d\\theta.\n\\end{align*}\n\\]\n\n\n\nCode\nprint(sum(y[500:600])/1000)\n\n\n1.003097768300707\n\n\nもはや丸め誤差により \\(1\\) を越してしまっている．ほとんど確実に「男児の方が生まれる確率が高い」と結論づけて良いだろう．\n\n\nこの (Bayes, 1763) が実行したように，事後分布 \\(p(\\theta|\\boldsymbol{y})\\) をみて \\(\\theta\\) に関する推論をする，という立場からの統計的営み全体をベイズ統計学．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-fundamental-problem-of-Bayes",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-fundamental-problem-of-Bayes",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.4 ベイズ統計学の基本問題",
    "text": "1.4 ベイズ統計学の基本問題\n事後分布 \\(p(\\theta|\\boldsymbol{y})\\) を導く際に用いた条件付き確率の公式である 式 1 \\[\np(\\theta|\\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|\\theta)p(\\theta)}{\\int_\\Theta p(\\boldsymbol{y}|\\theta)p(\\theta)\\,d\\theta}\\quad\\text{(1)}\n\\] は Bayesの公式 と呼ばれるようになった．今回の場合では，Pythonコードをご覧になった方はわかったかもしれないが，事後分布は \\[\np(\\theta|\\boldsymbol{y})=\\frac{\\theta^m(1-\\theta)^{n-m}}{B(m+1,n-m+1)}\n\\] となり，これはパラメータの空間 \\((0,1)\\) 上の Beta分布 と呼ばれるものである．\n現代のベイズ統計学の多くの統計量は，ある可積分関数 \\(g:\\Theta\\to\\mathcal{X}\\) を用いて \\[\n\\operatorname{E}[g(\\theta)|\\boldsymbol{y}]=\\int_{\\Theta}g(\\theta)p(\\theta|\\boldsymbol{y})\\,d\\theta\n\\tag{2}\\] と表される．先ほどの Bayes の区間推定の例では \\(g=1_{(a,b)}\\) と取った場合に当たる．6 実は，この積分は，この最も簡単と思われる \\(p(\\theta),p(\\boldsymbol{y}|\\theta)\\) の設定でも，殆ど計算できないのである．\n鮮やかな解決法を提示したかと思えば，結局実行出来ないのでは全く本末転倒である！そのこともあってか，論文 (Bayes, 1763) は実は Bayes の死後に Richard Price によって投稿されたものであり，生前に自ら投稿・発表した訳ではなかった．7 当然，発表当時は全く注目を受けなかった (Stigler, 1990)．\n\nHence, despite the analytical availability of \\(p(\\theta|\\boldsymbol{y})\\) via (2)–“Bayes’ rule” as it is now known-—the quantity that was of interest to Bayes needed to be estimated, or computed. The quest for a computational solution to a Bayesian problem was thus born. (Martin et al., 2023, p. 2)\n\n\n\n\n\n\n\nまとめ：ベイズ統計学の基本問題\n\n\n\nベイズの提示した統一的な統計推測の枠組み\n\n推定したい値 \\(\\theta\\) の空間上に事前分布 \\(p(\\theta)\\) を設定する．\n事前分布 \\(p(\\theta)\\) のデータ \\(\\boldsymbol{y}\\) に関する条件付き分布として事後分布 \\(p(\\theta|\\boldsymbol{y})\\) を得る．\n\nは非常に自然で，特に確率分布 \\(p(\\theta),p(\\theta|\\boldsymbol{y})\\) を簡単に視覚化できる現代では「データ \\(\\boldsymbol{y}\\) は，パラメータ \\(\\theta\\) に対する事前の信念をどれほど変えるに値するか？」を定量的にも定性的にも実感出来るという美点がある．\nしかしながら，モデル \\(p(\\theta),p(\\boldsymbol{y}|\\theta)\\) の設定をいくら簡単にしても根本的に計算が困難で実行不可能なのである．これを解決する分野をベイズ計算という．ベイズの論文 (Bayes, 1763) でも，計算法の開発が約半分を占めた．8 このように，Bayes統計学は当初からBayes計算の問題を懐胎していたのである．\n\nIn short, the implementation of all forms of Bayesian analysis relies heavily on numerical computation. (Martin et al., 2023, p. 2)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-Laplace",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-Laplace",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.5 Laplaceの近似",
    "text": "1.5 Laplaceの近似\nフランスの数学者 Laplace は25歳時の初めての統計に関する著作 (Laplace, 1774) を発表した．この中で，Bayes が解こうとしたものと全く同じ\n\\[\\begin{align*}\n    &\\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]\\\\\n    &\\quad=\\frac{\\int^b_a\\theta^m(1-\\theta)^{n-m}\\,d\\theta}{B(m+1,n-m+1)}\n\\end{align*} \\tag{3}\\]\nという積分計算の問題を，被積分関数を \\[\nf(\\theta):=\\frac{\\log p(\\theta|\\boldsymbol{y})}{n}\n\\] を用いて指数関数の形に表すことで解いた：9 \\[\n\\begin{align*}\n    \\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]&=\\int^b_ap(\\theta|\\boldsymbol{y})\\,d\\theta\\\\\n    &=\\int^b_ae^{nf(\\theta)}\\,d\\theta\n\\end{align*}\n\\] この形に変形することがどのように役立つかは，次の定理が説明してくれる：\n\n\n\n\n\n\n定理（Laplace近似）\n\n\n\n10 関数 \\(f:[a,b]\\to\\mathbb{R}\\) はただ一つの最大値を \\(x_0\\in(a,b)\\) で取り，\\(f''(x_0)&gt;0\\) を満たすとする．このとき \\(n\\to\\infty\\) の極限について， \\[\n\\int^b_ae^{nf(x)}\\,dx\\sim\\sqrt{-\\frac{2\\pi}{nf''(x_0)}}e^{nf(x_0)}\n\\]\n\n\nこれを \\(f\\) の二次近似について適用することで，あらゆる確率分布 \\(p(\\theta|\\boldsymbol{y})\\,d\\theta\\) に関する積分 式 3 を，その正規近似に関する積分で近似できるのである．\nこの手法は現在のBayes計算手法のアイデアの源泉であり続けている (Rue et al., 2009), (MacKay, 2003)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズ統計学の長く苦しい時代",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズ統計学の長く苦しい時代",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.6 ベイズ統計学の長く苦しい時代",
    "text": "1.6 ベイズ統計学の長く苦しい時代\n「ベイズの枠組みは理念的に好ましかろうと，実際には実行不可能である」というベイズ統計学の基本問題は，Laplace が普遍的な近似計算法を開発したこと（ 節 1.5 ）を除いて，次の進展を見るには計算機の発明と普及を待つ必要があった．その間実に2世紀超えである．\nまた，Laplace の近似手法は普遍的であり，Bayes の最初の設定のような簡単な設定の \\(p(\\theta|\\boldsymbol{y}),g\\) に限らずとも使えるという，ベイズ統計学に大きく資する特徴も備えていたが，パラメータ \\(\\theta\\in(0,1)\\) の次元が1ではなくなると途端に使えなくなるという欠点がある．\n\n（前略）ベイズ統計学の有用性は以前から理解されていたが，この問題の抜本的な解決は1980年代まで待たざるを得なかった．それ以前は，ベイズの定理自体は18世紀に早々に発見されたにもかかわらず，長い間，確率の解釈，事前分布の設定，事後分布の計算の困難さのために哲学的議論に終始し，実用化にはほど遠かったのである．実用化の扉の鍵となったのは，一つは計算機の急速な発達，もう一つは計算集約的な画期的アルゴリズムの提案である． (樋口知之, 2014, p. 17)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-France",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-France",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.7 フランスでの確率論の歴史",
    "text": "1.7 フランスでの確率論の歴史\nこのように，Laplace が，ベイズ統計学暗黒の時代の中で唯一の小さな前進を生んだ．それだけでなく，Laplace は後の1812年に当時の確率論最大の集大成と言える大著『確率論の解析理論』を産んでおり，これが他でもないフランスから生まれたことにも相応の理由があった．\nまず第一に，賭博の流行により，確率というものの理解と征服が嘱望された．\n\nその（確率論の）発展の動きを与えたものは，交易を賭ける商業資本家が占星術よりも確実な指導をこの学術に求めるという様な社会が基盤となって存在したことである．例えば，17世紀中葉のPascalとFermatの間の往復文書に取り扱われたカード遊びの数学的問題が，広く人々の関心を呼び起こした事情の裏には，至富の途を確実に求める商人たちの渇望が学問が外の世界にあったことを忘れてはならない． (北川敏男, 1949)\n\n第二に，統計的現象を神学的な畏怖の対象と見るのではなく，自然科学による自然の理解と征服の文脈の最先端として理解する土壌がフランスにあったことが指摘できる．\n\n17, 18世紀の啓蒙的合理主義は，偶然的な事象に対しても数学的な取り扱いを行うことに特別の興味を持った．思想的にはこの時代精神こと確率論を発展させた最大の動力であった．その駆使する数学解析の多彩と合理主義の徹底とに於て，Laplace の大著はよくこの時代を代表するものと言うべきであろう．\n\n当時の財務総監 Jacques Turgot を通じてパリ造幣局の監査官も務めた Nicolas de Condorcet 1743-94 は Laplace の確率論を積極的に社会分析に応用した．「社会数学」と呼んだこの運動は社会学の源流ともみなされる． 当然後進も Laplace に続いた．ベルギーの数学者 Adolphe Quetelet 1796-74 は Laplace の確率論を社会に応用することを目指し「社会物理学」なる分野を創始し，BMIの別名「ケトレー指数」にも名を残している．11\n\n古典確率論の一応の完成は典雅に見えるであろう．だが人は，確率論のもった政治的，社会的意義を忘れてはならない．理知を一切の尺度として「代数学の炉火によって倫理学及び政治学を照さん」(Condorcet) という時代精神，神の啓示に代らんとする確率論，それはフランス革命の思想的基礎に連関することを見失ってはならないのである． (北川敏男, 1949)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#quetelet-による綜合",
    "href": "posts/2023/Surveys/BayesianComp.html#quetelet-による綜合",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.8 Quetelet による綜合",
    "text": "1.8 Quetelet による綜合\nQuetelet は応用統計学の祖とも呼ばれる．\n\n［政治算術学派から人口理論 (Malthus, 1798) をはじめとする数多くの統計的法則の発見など，多くの成果があったにも拘らず，］18世紀の後半は理論的成果の観点からは全く空白の時代であった．吾々はその原因を方法論が進歩しなかった点に見出しうる．先験的対数法則を中枢とする古典確率論の方法がこれと合流するに至るまで，統計学の理論的分野は足踏みを続けざるを得なかったのである．後述 Quetelet の手による，この合流の着手は，応に近代的意味における統計学の発足を示すものであるとともに，記述統計学の定礎を意味するものでなければならなかった．(増山元三郎, 1950, p. 12)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-MCMC",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-MCMC",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.1 マルコフ連鎖によるモンテカルロ法の発明",
    "text": "2.1 マルコフ連鎖によるモンテカルロ法の発明\n乱数のシミュレーションを用いた確率的なアルゴリズムをモンテカルロ法と総称する．これは Metropolis が同僚 Ulam のポーカー好きから，モナコの首都 Monte Carlo にちなんで名付けたものである (Metropolis & Ulam, 1949)．このようなアルゴリズムが最初に生まれたのが，第二次世界大戦中の Los Alamos研究所 で進行中だった原爆開発計画である Manhattan計画 においてである．\n当時の問題は，原子爆弾着火時における Schödinger 作用素の基底状態のエネルギーを計算することにあった．抽象的には，\\(p\\) を \\(N\\) 個の粒子が従う Boltzmann 分布として，積分 式 2 を計算することにあった：\n\\[\n\\operatorname{E}[g(\\boldsymbol{\\theta})]=\\int_\\Theta g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\\quad\\text{(2)}\n\\]\nただし，\n\n積分領域 \\(\\Theta\\) が \\(2N\\) 次元というとてつもない高次元空間上であること\n分布 \\(p\\) は定数倍を除いてしか計算できない\n\nという，2つの大きな制約があった．1.のために通常の数値積分法が使えず，また 2.により \\(p\\) からの直接の乱数シミュレーションが出来ないので，\\(p\\) からの乱数 \\(X_1,\\cdots,X_M\\) を十分多く生成することで積分 式 2 を \\[\n\\frac{1}{M}\\sum_{i=1}^Mg(X_i)\n\\] によって近似するという通常の Monte Carlo 積分法を実行することも出来ない．そこで，Metropolis ら当時の Los Alamos に集まった物理学者たちは新しい方法を考える必要があった．\n最終的な解決 (Metropolis et al., 1953) は，Monte Carlo 法の中でもとりわけ画期的な発想によるものであった．それは，Markov 連鎖を用いるということである．Markov連鎖とは（ある一定の条件を満たす）確率過程のクラスであり，\\(p\\) から直接のシミュレーションが出来ない状況でも，\\(p\\) に収束するようなMarkov 連鎖を構成することは可能だったのである．\n制約 1.と 2.は広く物理学とベイズ統計学の至る所で見られる障壁であり，これをものともしない汎用アルゴリズムの発明は極めて大きなブレイクスルーであった．(Dongarra & Sullivan, 2000) は Metropolis アルゴリズムを理学・工学分野に20世紀最大の影響を与えたアルゴリズムの1つとしている．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-importance-sampling",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-importance-sampling",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.2 重点サンプリング法の発明",
    "text": "2.2 重点サンプリング法の発明\n実は Manhattan 計画に最中に，もう一つのサンプリング技法が生まれていた．厚い壁で中性子線とガンマ線がどのように吸収されるかに取り組んでいたグループにて，Herman Kahn らが中心となり，式 2 の分布 \\(p\\) に関する積分が \\[\n\\begin{align*}\n    \\operatorname{E}[g(\\boldsymbol{\\theta})]&=\\int_\\Theta g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\\\\\n    &=\\int_\\Theta\\frac{g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p^*(\\boldsymbol{\\theta})}p^*(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\n\\end{align*}\n\\] という式変形により，別の分布 \\(p^*\\) からのサンプリングを通じて計算できる，という技法が利用された．彼らはこれに重点サンプリング法という名前をつけた．これは Gerald Goertzel による命名である可能性が高い (Andral, 2022)．\nなお，当時は \\(p\\) からのサンプリングを回避できるという点よりも，\\(p^*\\) をうまく選ぶことにより元々の \\(p\\) を用いた Monte Carlo 積分法を適用するよりも近似の精度をあげることが出来るという点の方が注目された (Hammersley & Handscomb, 1964)．\n前節の Metropolis 法がMCMCの先駆けであるとしたら，この2つの美点を持った重点サンプリング法は，SMC（粒子フィルター） の先駆けであった．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#mcmcの普及とギブスサンプラー",
    "href": "posts/2023/Surveys/BayesianComp.html#mcmcの普及とギブスサンプラー",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.3 MCMCの普及とギブスサンプラー",
    "text": "2.3 MCMCの普及とギブスサンプラー\nMetropolis 法の発明から，すぐにMCMCの画期性が広く認識された訳ではなかった．特に，元々物理学の文脈で発明されたこともあり，統計学の文脈への応用が始まるには (Hastings, 1970) の仕事を待つ必要があった．\nしかし1970年代とはマイクロプロセッサが開発されたばかりの時代であり，12 MCMCが実際の統計解析の現場で採用可能な計算手法になるとは（そもそも現代のように小型なコンピュータを個人が所有するようになるとは）夢にも思われなかった時代であったが，ここからたったの20年で現代人の生活とベイズ統計学は大きく変わることになる．\n\n各人が安価に高性能なコンピュータを所有するようになった．\n高次元分布からのサンプリングを可能にするアルゴリズムが発見された．\n\nの2点が最後に加わることで，MCMCがベイズ計算法不動の金科玉条となった．\nこの 2.は計算機の性能の問題だけでなく，統計的画像処理 の分野から Gibbsサンプラー という新たなアルゴリズムが生まれた (Geman & Geman, 1984) ことによって実現された．13 これは，パラメータが \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)^\\top\\) と表されるとき，適切に定めた初期値 \\(\\theta_2^{(0)}\\) から初めて，条件付き分布からのサンプリング \\[\n\\theta_1^{(i)}\\sim p_1(\\theta_1^{(i)}|\\theta_2^{(i-1)},\\boldsymbol{y}),\n\\] \\[\n\\theta_2^{(i)}\\sim p_2(\\theta_2^{(i)}|\\theta_1^{(i)},\\boldsymbol{y}),\n\\] を繰り返すことで，最終的に \\(\\boldsymbol{\\theta}^{(i)}:=(\\theta_1^{(i)},\\theta_2^{(i)})^\\top\\) は全体として \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) に従うように収束する，という技法である．\nGibbs 法により，パラメータ \\(\\boldsymbol{\\theta}\\) の次元が大きく，直接のサンプリングが難しい場合や，条件付き分布の系はわかっているが結合分布がわからない場合14 でも，\\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots)\\) というように低次元変数の結合と理解することで，あるいは補助変数を追加してわざと問題を高次元化してでもそのような状況をうまく作り出すことで (Tanner & Wong, 1987) ，部分的な低次元サンプリングから組み上げることが出来るようになった．さらにその後も，このアイデアが (Roberts & Rosenthal, 1999) のスライスサンプラーにつながっている．\nこの点をはっきり強調して示し，ベイズ統計学がすでに実行可能なものになっており，ベイズ統計学の基本問題（ 節 1.4 ）もすでに過去の遺物となっているということを，統計学界隈に広く知らしめたのが (Gelfand & Smith, 1990) であった．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#擬似周辺尤度法",
    "href": "posts/2023/Surveys/BayesianComp.html#擬似周辺尤度法",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.1 擬似周辺尤度法",
    "text": "3.1 擬似周辺尤度法\n実は尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) が解析的に得られない場合や計算が極めて困難になる場合でも，この不偏推定量があればMCMCを実行して事後分布を得るのに十分である (Andrieu & Roberts, 2009)．この尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) の不偏推定量を得るのに粒子フィルターを用いた場合を，特に粒子MCMCという (Andrieu et al., 2010)．\nこのときの不偏推定量の性能が最終的な Monte Carlo 推定量に影響する．不偏推定量の分散を改善するには，サブルーチンである粒子フィルターの反復数を増やす必要がある．すると本体であるMCMCの反復数とのトレードオフが生じる．こうしてアルゴリズムの最適な調整が課題になる．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#高次元問題に対処するmcmc",
    "href": "posts/2023/Surveys/BayesianComp.html#高次元問題に対処するmcmc",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.2 高次元問題に対処するMCMC",
    "text": "3.2 高次元問題に対処するMCMC\nほとんどのMCMC手法は，データサイズやモデルのパラメータサイズの増加に対して，計算負荷が飛躍的に上昇する次元の呪いに苦しむ．これを克服する手法はscalabilityの名の下に盛んに研究されている (鎌谷研吾, 2021, p. 394)．\n\n対象分布の探索を効率よく行う手法として，HMC (Hamiltonian Monte Carlo) 法が提案された (Neal, 2011)．他にも NUTS (No U-Turn Sampling) (Hoffman & Gelman, 2014), Metropolis-Adjusted Langevin Algorithm (Roberts & Tweedie, 1996), Stochastic Gradient MCMC (Nemeth & Fearnhead, 2021), PDMP (区分的確定なMCMC) (Bierkens et al., 2018), (Fearnhead et al., 2018) とジグザグサンプラー (Bierkens et al., 2019) などがある，\nより良い提案分布の選択法について，MH法の最適スケーリング法，適応的サンプリング，焼き戻しなどの手法がある．\n並列計算による効率化の方向性には，並列MCMC，完全サンプリングなどの手法がある．\n他の分散低減法に，Rao-Blackwell化 (Casella & Robert, 1996)，操作変数法などがある．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#近似ベイズ手法",
    "href": "posts/2023/Surveys/BayesianComp.html#近似ベイズ手法",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.3 近似ベイズ手法",
    "text": "3.3 近似ベイズ手法\n上述までの手法はいずれもシミュレーションを十分多く行えば（理論的には）任意の精度で正しい値を得ることができるが，15 その適用範囲やスケーラビリティが課題なのであった．そこで同時に，最初からある許容精度を定めた下での近似を実行することとし，代わりにより広い適用可能性と計算速度を得るための手法も探求されている．これを近似ベイズ法という．\n一つのアプローチはシミュレーションによる方法である．これにはABC (Approximation Bayesian Computation) (Tavaré et al., 1997) と BSL (Bayesian synthetic likelihood) (Price et al., 2018) の2つの手法があるが，いずれもデータ生成過程（モデル）の複雑性と高次元性という２つの障壁が併存したときでも使える手法である．ABCではまず事後分布 \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) をある低次元な要約統計量 \\(S:\\mathcal{Y}\\to\\mathbb{R}^d\\) を用いて \\(p(\\boldsymbol{\\theta}|S(\\boldsymbol{y}))\\) で近似し，さらに尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) を直接評価することは回避し，シミュレーションのみを用いて \\(p(\\boldsymbol{\\theta}|S(\\boldsymbol{y}))\\) を推定する．BSLはさらに尤度 \\(p(S(\\boldsymbol{y})|\\boldsymbol{\\theta})\\) にパラメトリックな仮定をおく．\n第二に最適化による方法がある．変分ベイズ手法とは，これは大きなパラメトリックモデル \\(\\{q^*(\\boldsymbol{\\theta})\\}\\) の中から \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) に最も近いものを選ぶ手法である．一方で INLA (integrated nested Laplace approximation) とは，Laplaceの近似（ 節 1.5 ）に最適化を組み合わせて高次元の問題にも対応する．\nABCでは逐次モンテカルロ法も大きな役割を果たしており，ABC-SMC (Sisson et al., 2007)，ABCフィルタリング (Jasra et al., 2012)，更には変分Bayes法への応用 (Tran et al., 2017) なども進んでいる．\n変分Bayesの枠組みでは，モデルの誤想定に頑健な手法の開発も試みられている (Wang & Blei, 2019)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-BayesianModeling",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-BayesianModeling",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.4 ベイズ統計モデリングが理論モデルの実証に役立つ",
    "text": "3.4 ベイズ統計モデリングが理論モデルの実証に役立つ\nベイズモデリングの有用性は，（上述のベイズ計算の問題を除けば）どんなに複雑で大規模なモデルでも，統一的な思想と方法で対応できる点にある．\n\nメカニズムを明示的に表現した数理社会学の数理モデルを，論理的に飛躍することなくダイレクトに統計モデルへと接続できるベイズ統計モデリングは，理論モデルベースの実証研究と相性のよい，たいへん便利な方法と言えるだろう． (浜田宏, 2022, p. 137)\n\nMCMCの開発とパッケージへの実装，そして安価で高性能な計算機が普及してからというもの，ベイズ統計学の興隆は目覚ましく，現在ではベイズ統計学は統計学に関する論文の1割強を占め，諸科学分野全体に浸透しつつある．経済学・心理学への応用は早かったのに比べて，政治科学・社会科学への応用は遅れ気味であり，社会学での使用はまだ稀であると言える (Lynch & Bartlett, 2019)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズによる逆問題",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズによる逆問題",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.5 ベイズによる逆問題",
    "text": "3.5 ベイズによる逆問題\nベイズ統計学の枠組みは，逆問題の文脈においても有用である．逆問題とは，観測データ \\(\\boldsymbol{y}\\) が与えられたときに，そのデータを説明するようなモデルのパラメータ \\(\\boldsymbol{\\theta}\\) を推定する問題である．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-BayesianCausalInference",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-BayesianCausalInference",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.6 ベイズによる因果推論",
    "text": "3.6 ベイズによる因果推論\n前節に挙げたベイズモデリングの美点は因果推論の文脈でも全く同様である．特に因果推論の問題では推定対象が複雑であることが多いが，このような場合でも全く同じ枠組みを提供してくれるのがベイズである．頻度論的接近では設定に応じた個別具体的な議論がベイズ計算の問題に帰着する点が利点として働くことは多いようである (Li et al., 2023)．\n実際，ベイズノンパラメトリック手法は2016年の大西洋因果推論カンファレンスのコンペティションで大きな成功を見ている (Dorie et al., 2019)．加えて強い理論的な保証も得られつつあり (Ray & van der Vaart, 2020)，これにより因果推論分野で大きな注目を集めている (Linero & Antonelli, 2023), (Daniels et al., 2023)．\n加えて，「あらゆる種の不確実性に対する統一的な定量化を与える」というベイズの性質は，因果推論から意思決定までの接続を地続きにし，例えば属人化医療などの現場でのダイナミックな意思決定に活用できることが期待される．\n\n不確実性を定量化するのに、ベイズ計算では必ず『確率』を使います。一般の人から見たら、統計で確率を使うのは当たり前と思うでしょうが、じつは他の統計手法ではそうでもなく、さまざまな解釈が生まれてしまう。定量化にはすべて統一的に確率を使うベイズ計算は、非常にシンプルなので、最終的にすべての統計はベイズに行き着くしかないと思っています．鎌谷研吾\n\nしかし，ベイズの方法が因果推論の分野で普及するための障壁は，近づきやすさにあると議論できる (Li et al., 2023)．従来の頻度論的な因果推論手法の成功には，潜在反応モデルの特定を殆どしなくて良いこと（モデルフリー），実装が簡単であることが少なからず寄与しているとすれば，ベイズ的接近もこれに当たるものを提供できるようになる必要があるだろう．Stan言語 (Carpenter et al., 2017) はこの方向への大きな試みである．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズ学習",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズ学習",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.7 ベイズ学習",
    "text": "3.7 ベイズ学習\n機械学習の手法を用いてベイズ推論を実行する営みをベイズ学習，または単に「機械学習への確率論的アプローチ」と言ってベイズの枠組みを暗に指す場合も多い (Murphy, 2022), (Ghahramani, 2013)．\n古典的な統計手法と同様，多くの既存の（頻度論的）手法にはベイズ手法の対応物が存在する．ベイズの方法だと推定の確信度合いもセットで定量化され，頻度論的対応物よりも得られる情報が多い一方で，計算は既存手法よりも難しいことが多いという構造は，機械学習においても変わらない．\n実際，現存のニューラルネットワークの訓練法を超えるベイズ計算法が今後提案されるとは考えにくいが，その最適化する所の目的関数が例えば正則化項付きの平均自乗誤差である場合は，ある正規事前分布と正規尤度に対するMAP推定量に対応する (Seitz 2022)．畢竟，多くの既存手法も「ベイズ学習を非ベイズ的な方法で実行している」と捉えられるのである（逆も然り）．\n中でもベイズ学習を採用するのが良い場面としては，モデルの大きさに対して学習に使えるデータの数が少ない場合や，モデルに事前情報を組み込みたい場合16 ，さらには医療・政策への応用など意思決定に繋げるために不確実性の定量化が肝要な場面などがあり得る．\n実際，ベイジアン・ニューラルネットワークでは計算の困難ささえ乗り越えれば，複数の適切なモデルに対し，事後分布によって平均を取って最終的なモデルとすることで，過学習を防止し (Mackay, 1995)，大きな性能改善を得ることができる (Wilson & Izmailov, 2020)．17"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#世紀の統計学とベイズの役割",
    "href": "posts/2023/Surveys/BayesianComp.html#世紀の統計学とベイズの役割",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.8 21世紀の統計学とベイズの役割",
    "text": "3.8 21世紀の統計学とベイズの役割\nこのように，21世紀に入ってからベイズの成功は目まぐるしく，この傾向はさらに進むと思われる．これは統計計算の手法の進化によって達成された．今後とも統計計算の手法は，シミュレーション・変分法・最適化の垣根を超えて多様化の一途を辿るだろう (Green et al., 2015, p. 857)．\nその中でも筆者は，ベイズ手法が提供する事後分布として得られる不確実性の表現・視覚化が，計算機・自然・人間の間のよきインターフェイスとなっていくことを願っている．18\n\nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice-—appropriate frequency calculations help to define such a tie. (Rubin, 1984)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#footnotes",
    "href": "posts/2023/Surveys/BayesianComp.html#footnotes",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n正式名称をThe Royal Society for the Improvement of Natural Knowledge by Experimentという↩︎\n当局の人間に死亡を報告する義務は全くなかった。その代わり、それぞれの教区では2人かそれ以上の死体を調査し、死因を決定する義務を負う調査員を任命していた。「調査員」は死亡を報告する毎に遺族より少額の手数料を徴収する資格が与えられていたので、教区では任命しなければ貧困のため救貧税による支援が必要となりそうな人間を割り当てていた。（Wikipediaページより）↩︎\nこれは統計的モデルとしてBernoulli分布 \\(Y_i|\\theta\\overset{\\text{iid}}{\\sim}\\mathrm{Ber}(\\theta)\\) を仮定するということである．↩︎\nパラメータ \\(\\theta\\) は「男児が生まれる確率」であるが，これ自体にも事前分布という「確率」\\(p(\\theta)\\) を導入することに戸惑う読者も居るだろう．しかし，これがベイズ統計学の特徴である．「男児が生まれる確率 \\(\\theta\\)」だろうとなんだろうと，「わからない」「不確実性がある」と主観的に感じるあらゆる対象に，確率分布を導入して事後分布を得ることで推論を実行する，これがベイズ統計学の枠組みの普遍性であり，無差別性であり，有用性を支えている．↩︎\n各 \\(\\theta\\) の下で目の前のデータ \\(y_1,\\cdots,y_n\\) が生成される確率 \\(p(\\boldsymbol{y}|\\theta)\\) が低いということは，「その \\(\\theta\\) から生成されたデータである確率は低い」という逆の発想ができる．そこで \\(p(\\boldsymbol{y}|\\theta)\\) という条件付き確率を尤度ともいう．今回は \\(p(\\boldsymbol{y}|\\theta)=\\theta^{\\sum_{i=1}^ny_i}(1-\\theta)^{\\sum_{i=1}^n(1-y_i)}\\) である．↩︎\nさらに，\\(g(\\theta)=\\theta^p\\) と取った場合，事後積率という統計量になる．等に \\(p=1\\) の場合が事後平均である．↩︎\nなお，1763に出版されたものはPriceによる補遺も付いた短縮版であり，全文は1974年に出版された．(Stigler, 1990)↩︎\npp.376-403 がBayesの論文の本論の内容であり pp.399-403 で計算法を３つのルールにまとめているが，その導出部は一部「長すぎるから掲載を省略する」とされている．↩︎\n一方で，Bayesの逆確率の問題への言及自体は，Laplaceの後年の1781年の著作Mémoire sur les probabilitésへのCondorcetによる序文で初めて登場する (Martin et al., 2023, p. 5)．↩︎\nnCatLab 参照．↩︎\n(安藤洋美, 1995) も参照．↩︎\n1970年にインテルが世界初の DRAMである Intel 1103 を発売した．Wikipediaページ参照．↩︎\n物理学では Heat Bath 法と呼ばれ古くから同様のアルゴリズムが存在したが，統計学界隈では現在でも Gibbs サンプラーと呼ばれる．↩︎\n統計的画像処理など，Markov 確率場 によってモデリングされる対象においてはよくある状況である．↩︎\nこの性質を指して，approximateの対義語としてexactという形容詞で表現される．↩︎\nfunctional Bayes (Sun et al., 2019) という手法では，希望する入力と出力の組を事前に用意するのみで，適切な事前分布を提案してくれる枠組みである．↩︎\n(Wilson & Izmailov, 2020) によると，二重降下現象も見られない．↩︎\n推定結果に自信がないときはそう表明してくれる機械は親しみやすい．↩︎"
  },
  {
    "objectID": "posts/2023/DigitalNature/Internet.html",
    "href": "posts/2023/DigitalNature/Internet.html",
    "title": "インターネットとは AS 間が BGP で相互接続された裏路地である",
    "section": "",
    "text": "登氏による総務省「西日本横断サイバーセキュリティ・グランプリ」講演資料 登大遊 (2023) 秘密のNTT電話局，フレッツ光，およびインターネット入門(1) という文献を，引用を交えながら，筆者が理解した事項を驚いた事項を紹介する．どんな内容が書いてあるかを概観してもらい，読者にも自分で興味のある箇所をぜひ読んでいただきたい．"
  },
  {
    "objectID": "posts/2023/DigitalNature/Internet.html#as-autonomous-system-とは何か",
    "href": "posts/2023/DigitalNature/Internet.html#as-autonomous-system-とは何か",
    "title": "インターネットとは AS 間が BGP で相互接続された裏路地である",
    "section": "1 AS (Autonomous System) とは何か？",
    "text": "1 AS (Autonomous System) とは何か？\n\nインターネットという連合体を形成する主体 1 つ 1 つを、「AS (Autonomous System: 自律システム) 」 という。自律システムとは、独立した領主・領土というような意味である。インターネット上の概念における主権を持っていて、他の主権者によって決して干渉されない。AS は、免許番号のような形で、整数の番号を持っている。これは AS 番号と呼ばれる。AS は、インターネット上の土地のような、「IP アドレス」というものを持っている。IP アドレスの種類としては、IPv4 と IPv6 とがある。バージョン 4 と 6 という意味である。西暦 1970 年代に成立した IPv4 アドレスというアドレス空間は、今となってはとても希少である。 (登大遊, 2023, p. 32)\n\n登氏はASがインターネット空間の主権者の最大単位だと表現している．筆者は現実空間における法人と自然人を混ぜて1つの概念とした，インターネット空間上の抽象的存在と理解している．例として，ソフトイーサ社がASとして他のどのASと接続されているかを，「米国 Hurricane Electric 社 (激安 ISP) の BGP Toolkit のページ 」（登さんの語彙）で確認出来る．\n\n自宅のコンピュータから、一度いずれかの AS の中にアクセスしたら (これは電話会社によって提供される。)、裏路地を通って世界中のすべての AS にアクセスできる。裏路地は、主体 (AS) がそれぞれ大変適当に提供し合っている。裏路地の責任主体は、極めて怪しい。裏路地を通るとき、色々な AS の土地を勝手に通過させてもらうことができる。何ら契約関係がなくても、通っていって良いのである。ただし、裏路地にはぬかるんでいる所があり、不快なこともあるが、無料なのでまあいいや、ということになる。この裏路地の存在が、インターネットの画期的な点である。 (登大遊, 2023, p. 37)\n\nつまり，NTTやその販売代理店が，光フレッツというFTTH(Fiber To The Home)サービスを通じて提供してくれるのは，光ファイバーによる物理通信環境と，ASとの最初の接続を提供してくれる，というわけである．しかしインターネットはすべての主体が相対的であり，自らがAS割り当てを受けて，最寄のASとの通信を確立させたら（筆者はどうやれば良いのかまだわからないが），自分もインターネット空間においてISPと全く変わらない主体になる，というわけであるようだ．\n光フレッツが電話会社から提供されているばかりに，一消費者としてはついつい強権的な存在を想定して，「インターネットは電話会社が接続させてくれるもの」と思いがちであるが，「インターネットのめんどくさいことを全部やってくれるスターターキット」以上の意味はないようである．この点が筆者にとって衝撃的であった．"
  },
  {
    "objectID": "posts/2023/DigitalNature/Internet.html#bgp-border-gateway-protocol-とは何か",
    "href": "posts/2023/DigitalNature/Internet.html#bgp-border-gateway-protocol-とは何か",
    "title": "インターネットとは AS 間が BGP で相互接続された裏路地である",
    "section": "2 BGP (Border Gateway Protocol) とは何か",
    "text": "2 BGP (Border Gateway Protocol) とは何か\nインターネット空間に存在する唯一のルールともいうべき，P2Pにおける通信規約である．登氏は「国境接続儀礼」と訳している．\n\nBGP を少し悪用すると、勝手に他の主体の IP アドレスを使うと宣言して、その IP アドレス宛の通信を全世界から全部引っ張り込むこともできてしまう。セキュリティ的に大変危険であるが、これはなかなか防げないので、問題になっている。 AS は国際社会における最上位の主権者であり、BGP は国際社会における事実上の法 (国際法) である。ある AS が BGP の法に違反したからといって取り締まられることがない (より上位の主体がないため)。ただし、「あの AS は法を遵守していない危険な AS だ。」 (例: Google の Public DNS サーバーの IP アドレス 8.8.8.8 を他の AS が勝手に名乗った) ということで、すぐに世界中に風評が伝わり、他のすべての AS から村八分にされることで、事実上隔離され、危険は回避される。インターネットにおける IP アドレスに基づく通信というのは、このように、大変にいい加減な仕組みである。 このあたりのインターネット基礎中の基礎を知らずに、日本警察のサイバー犯罪対策課などは、IP アドレスについては、Whois 台帳 (どの IP アドレスがどの組織に割当てられているかを管理する台帳) の記載を信用していて、事件に使われた IP アドレスが分かれば確実に通信者 (少なくともプロバイダ) が特定できるなどと誤解していて、これを疑うことをしない。Whois 台帳と、実際に誰が IP アドレスを使っていたかは、全く無関係である。少し自ら BGP をやってみればすぐに分かることである。ある土地で殺人事件が起きたときに、登記簿を見て、土地の所有者にお前が犯人だと言うようなものである。土地の登記簿と、その事件があったときに土地に誰がいたのかは、無関係である。ちなみに、日本の警察がサイバー、サイバーといっておきながら、サイバー空間の根本部分の基礎知識が分かっていないことは、無理もないことである。警察組織の中に通信技術やインターネット技術といったものの内側の知識習得や試行錯誤を行なう環境がこれまで存在しなかったからである。しかし、これからは真剣に勉強する意欲があるようなので、未来は明るい。 (登大遊, 2023, pp. 37–38)"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html",
    "href": "posts/2023/Life/ChangedMyLife.html",
    "title": "俺の人生を変えたものTop5",
    "section": "",
    "text": "９月の京都学会→台湾研修（３週間）から帰ってきて，最初に手をつけたのが家の片付けであった．不要な本は全て売り（段ボール5箱），粗大ゴミを8000円分捨て，古着も処分した．特に，半ば物置と化していた和室をリフォームし，自分の書斎として使えるようにした．結果，11月を迎えた今，8月までの生活に比べて，\n\n3時に寝て1時に起きるのもザラにあった生活が，12時には寝て10時には活動を始めているようになった．特に，午前中からだらけることなく研究に従事することができるようになった．\n1日2食（昼に当たる時間と晩）でも胃もたれしたり，冷たいものを飲み過ぎて戻してしまうことがしばしばあった生活が，1日3食がっつり食べるようになった．それで居て胃もたれもなく，冷たい飲み物が苦手ということも無くなった．\n急に立ち上がったり，電車から降りて階段を登ったりするタイミングで目の前が見えなくなるほどの立ちくらみがすることがよくあった（週の2,3回ほど）が，今では1度もなければ前兆もない．\n以前は週1回ランニングに出れば良い方だったが，今では外出しない日は殆どランニングに出ている．\n結果，「やられっぱなしにはならず耐える」ことが人生の中心になっていたが，いまでは「自分の手で人生を変えていける」という感覚を得ることが出来ている．\n\n正直これほどの変化が起こるとは思わなかったし，すでに２週間近く全くブレずに持続している．自分のこれまでの人生から見ても，これからを思っても，これほど効果覿面な投資もなかったと思うので，10月に導入して良かったものベスト5を書きおこうと思う．"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#何が起こったか",
    "href": "posts/2023/Life/ChangedMyLife.html#何が起こったか",
    "title": "俺の人生を変えたものTop5",
    "section": "",
    "text": "９月の京都学会→台湾研修（３週間）から帰ってきて，最初に手をつけたのが家の片付けであった．不要な本は全て売り（段ボール5箱），粗大ゴミを8000円分捨て，古着も処分した．特に，半ば物置と化していた和室をリフォームし，自分の書斎として使えるようにした．結果，11月を迎えた今，8月までの生活に比べて，\n\n3時に寝て1時に起きるのもザラにあった生活が，12時には寝て10時には活動を始めているようになった．特に，午前中からだらけることなく研究に従事することができるようになった．\n1日2食（昼に当たる時間と晩）でも胃もたれしたり，冷たいものを飲み過ぎて戻してしまうことがしばしばあった生活が，1日3食がっつり食べるようになった．それで居て胃もたれもなく，冷たい飲み物が苦手ということも無くなった．\n急に立ち上がったり，電車から降りて階段を登ったりするタイミングで目の前が見えなくなるほどの立ちくらみがすることがよくあった（週の2,3回ほど）が，今では1度もなければ前兆もない．\n以前は週1回ランニングに出れば良い方だったが，今では外出しない日は殆どランニングに出ている．\n結果，「やられっぱなしにはならず耐える」ことが人生の中心になっていたが，いまでは「自分の手で人生を変えていける」という感覚を得ることが出来ている．\n\n正直これほどの変化が起こるとは思わなかったし，すでに２週間近く全くブレずに持続している．自分のこれまでの人生から見ても，これからを思っても，これほど効果覿面な投資もなかったと思うので，10月に導入して良かったものベスト5を書きおこうと思う．"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#スタンディングデスクの導入",
    "href": "posts/2023/Life/ChangedMyLife.html#スタンディングデスクの導入",
    "title": "俺の人生を変えたものTop5",
    "section": "１．スタンディングデスクの導入1",
    "text": "１．スタンディングデスクの導入1\n\n\n\n\nStanding Desk\n\n\nこれが俺の生活を根底から変えてしまった．\n１週間ほどかけて物置状態の和室にスタンディングデスクを導入し，書斎として使えるようにした．すると，朝ご飯から研究が，晩御飯から読書が，シームレスに繋がるようになった．特に何もせずにだらだらしていた時間がまるごと消えてしまった．外から家に帰ってきて，風呂に入るわけでもなくソファに座ってだらだらしているようなことも減った．\nあとから思えば，朝起きて研究に取りかかれない理由のうち殆どの部分が「日当たりのない部屋で」「座るのが嫌だ」の２つの事項に帰することが出来たのだ．私の部屋は北向きで窓はあれど殆ど陽は入らず，一方で和室はリビングに繋がっており，大きな窓から朝日が差し込んでくる．午前中に自宅で勉強する行為は人生全体で見て殆ど初めてのことだったが，心の底から幸せだと感じた．\nまた，睡眠の改善は，後述の睡眠グッズの影響も大きいだろうが，朝日当たりの良い場所で研究・読書をすることで，午前中から太陽の光を浴びるようになったことによる影響も大きいだろうと思われる．\nさらに，筆者はオンライン授業を聞くのが極めて苦手で，全く集中できない上に他のことをがっつりやることも出来ない，大きなストレス源であったが，スタンディングデスクであると自然な形で聴くことができる．\nこのような例もあるのだ．「ダラダラしてしまうのは自分が臆病だからだ」とか，「朝に弱いのだ」などと早合点せず，もっと早くスタンディングデスクを導入して，自宅内にも２箇所勉強できる場所を用意しておけば良かったと今では思う．疲れたら立って／座ってみるだけでギアが変わるように集中力が持続する．スタンディングデスクにステッパーを組み合わせて，歩きながら作業できるようにすると雑務にもストレスが溜まらない．"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#段ハンガーラック",
    "href": "posts/2023/Life/ChangedMyLife.html#段ハンガーラック",
    "title": "俺の人生を変えたものTop5",
    "section": "２．２段ハンガーラック2",
    "text": "２．２段ハンガーラック2\n【ポール径25mm】 エリソン ハンガーラック ワードローブ [ホワイト] 幅110cm 3段 幅111×奥行41×高さ220cm EHE11213WH [EHE11183WH ADD-P45WH HP-110WH]| スチールラック・メタル製ラック通販のルミナスクラブ\n和室には他に本棚，ベッドと，この２段ハンガーラックが用意してある．縦方向に長い(220cmある)ために場所を取らないが，多くの衣服を収納できるし，何より取り出しやすい．これで散らかしがちだった服を一箇所に整理することが出来た．"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#高反発マットレス",
    "href": "posts/2023/Life/ChangedMyLife.html#高反発マットレス",
    "title": "俺の人生を変えたものTop5",
    "section": "３．高反発マットレス3",
    "text": "３．高反発マットレス3\n\nこれも全く予想外だった．マットレスを変えることが睡眠に影響を与えるとも思っていなかったし，「痩せている場合は低反発」というネット上の文句がやけに腑に落ちる部分もあったため，「高反発で本当に良かったのか？」と買ってからも逡巡していたが，「体圧分散マットレス」であれば高反発だろうと身体を痛めることはない．さらに筆者の場合は，高反発マットレスで寝起きした方が，身体が疲れていないと感じる．"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#iotシーリングライト",
    "href": "posts/2023/Life/ChangedMyLife.html#iotシーリングライト",
    "title": "俺の人生を変えたものTop5",
    "section": "４．IoTシーリングライト4",
    "text": "４．IoTシーリングライト4\n【調光調色 スマホ操作やタイマーが便利】6畳 LEDシーリングライト フラヴィア リモート リモコン付き IoT スマホで操作 おしゃれ 照明器具 リビング用 居間用 ダイニング用 食卓用 電気 寝室 一人暮らし シンプル 声で操作 子供部屋 間接照明 電灯-おしゃれ照明・ライトのBeauBelle（ボーベル）\n部屋のライトが音を出すようになったことがきっかけで買い替えたが，このライトは色の調整も可能でありながら，「朝９時に点灯させる」といったようなスケジューリングも可能である．\n実際明るくなったことで起きることはなかったが，起きた場合に二度寝することが減った．それも，不快感も特に強くなく，勝手に身体にエネルギーが起きてくれるのである．\nまた，集中する際は白色光で，寝る前は暖色で光の強さも弱めていくことで，自然な眠気を誘うこともできる．"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#日13時間は水以外口に入れない時間を作る",
    "href": "posts/2023/Life/ChangedMyLife.html#日13時間は水以外口に入れない時間を作る",
    "title": "俺の人生を変えたものTop5",
    "section": "５．1日13時間は水以外口に入れない時間を作る",
    "text": "５．1日13時間は水以外口に入れない時間を作る\nこれが今回唯一の自助努力となったが，最大の気づきでもあった．夜寝る前にラーメンや甘いものをつい食べがちになっていたが，その代わりに日中の摂取カロリーを増やし，（翌朝10時に朝ご飯を食べるとするならば）９時以降は水以外，口に何も入れないようにする．大事なのはカロリーを取らない点である．人体は，口腔内にカロリーを検出するだけで，それに合わせて胃も消化の必要を見越して胃液を分泌し，調和を図るようになっている．5\n思い返せば，小学校低学年までは胃腸が弱く，体調を崩した際は必ず嘔吐を繰り返した．その際に学んだ絶対の規則は「一度吐いたら６時間は何も食べないし何も飲まない」を徹底することであった．疲れさせてしまったら休ませることが人体の鉄則である．\n筆者の場合は夜はしっかり半日以上胃腸を休ませることにより，日中にフルパワーで活動させることができ，夜中にお腹が空くということは減ったのであった．\n正直、いまの自分は、昔の自分がなりたかった自分の最新バージョンに他ならない。己に恥じない毎日を過ごしたいと思う。"
  },
  {
    "objectID": "posts/2023/Life/ChangedMyLife.html#footnotes",
    "href": "posts/2023/Life/ChangedMyLife.html#footnotes",
    "title": "俺の人生を変えたものTop5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFlexiSpot 電動昇降式デスクEF1↩︎\n【ポール径25mm】 エリソン ハンガーラック ワードローブ [ホワイト] 幅110cm 3段 幅111×奥行41×高さ220cm EHE11213WH [EHE11183WH ADD-P45WH HP-110WH] ↩︎\nGOKUMIN 高反発マットレス↩︎\nBeauBelle IoT シーリングライト Flavia bbs-095t↩︎\n関連する話はこのブログなどにも紹介されている．↩︎"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html",
    "href": "posts/2023/Particles/ParticleFilter.html",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "",
    "text": "Nicolas Chopin による逐次モンテカルロ法のための Python パッケージ particles の実装を参考に，NumPy, SciPy のみを用いて1から粒子フィルターを実装することで，その仕組みを理解することを目指す．"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#リサンプリングの実装",
    "href": "posts/2023/Particles/ParticleFilter.html#リサンプリングの実装",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "1 リサンプリングの実装",
    "text": "1 リサンプリングの実装\nまずリサンプリング法を実装する．今回は系統的リサンプリング法 (Carpenter et al., 1999) を用いることとする．1\n\n1.1 システマティックリサンプリング\n系統的リサンプリングとは，粒子数 \\(N\\) から \\(M\\) 個のサンプルを復元抽出する方法であって，次の2段階からなる．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n各区間 \\(\\left[\\frac{n-1}{M},\\frac{n}{M}\\right]\\subset[0,1]\\;(n\\in[M])\\) での変動の由来をただ一つのサンプル \\(U\\sim\\mathrm{U}([0,1])\\) から取ってしまい， \\[\nU^{(n)}:=\\frac{n-1+U}{N}\\quad(n\\in[M])\n\\] と乱数列を定める．\n正規化荷重 \\(\\{w^{(n)}\\}_{n=1}^N\\) が定める累積和 \\[\nF(n):=\\sum_{i=1}^nw^{(i)}\n\\] に対して，この一般化逆関数 \\(F^-:[0,1]\\to[N]\\) を通じて，\\(A^{n}:=F^{-1}(U^{(n)})\\;(n\\in[M])\\) をサンプルとする．\n\nこれにより，粒子の添字の対応 \\((1,\\cdots,N)\\mapsto A^{1:M}\\) が得られる．\n\n\nCode\nimport numpy as np\nfrom numba import jit\n\n@jit(nopython=True)\ndef inverse_cdf(su, W):\n    \"\"\"Inverse CDF algorithm for a finite distribution.\n    su: (M,) ndarray of sorted uniform variables\n    W: (N,) ndarray of normalized weights\"\"\"\n    j = 0\n    s = W[0]\n    M = su.shape[0]\n    A = np.empty(M, dtype=np.int64)\n    for n in range(M):\n        while su[n] &gt; s:\n            j += 1\n            s += W[j]\n        A[n] = j\n    return A\n\ndef systematic(W, M):\n    \"\"\"Systematic resampling\n    W: (N,) ndarray of normalized weights\n    M : number of resampled points\"\"\"\n    su = (random.rand(1) + np.arange(M)) / M\n    return inverse_cdf(su, W)\n\n\n\n\n1.2 荷重を保持するWeightsクラス\n次に，SMC に繋げるために，粒子の荷重を保持するためのクラスを定義する．粒子の荷重は極めて小さくなり得るため，対数によって保持する．このクラス内の属性として，正規化荷重もESSも得られるようにする：\n\nlw：正規化されていない荷重を対数で保持\nw：正規化された荷重\nESS：有効サンプル数2\n\nこれらの属性を __init__(lw) 内で計算する．加えて add(delta) メソッドで，incremental weightsを乗じるルーチンを用意する．\n\n\nCode\nclass Weights:\n    \"\"\"A class to hold the N weights of the particles\"\"\"\n    def __init__(self, lw=None):\n        self.lw = lw  # t=0で呼ばれた際はNoneである\n        if lw is not None:\n            self.lw[np.isnan(self.lw)] = -np.inf  # 欠損値処理\n            m = self.lw.max()\n            w = np.exp(self.lw - m)  # 大きすぎる値にならないように\n            s = w.sum()\n            self.W = w / s  # 正規化荷重\n            self.ESS = 1.0 / np.sum(self.W ** 2)\n            self.log_mean = m + np.log(s / self.N)\n    \n    @property\n    def N(self):\n        \"\"\"Number of particles\"\"\"\n        return 0 if self.lw is None else self.lw.shape[0]\n\n    def add(self, delta):\n        \"\"\"Add increment weights delta to the log weights\"\"\"\n        if self.lw is None:\n            return self.__class__(lw=delta)\n        else:\n            return self.__class__(lw=self.lw + delta)\n\n\n初期化は \\[\nW^i=\\frac{e^{\\log w^i-m}}{\\sum_{j=1}^Ne^{\\log w^j-m}}\n\\] \\[\nm:=\\log\\left(\\max_{i\\in[N]}w^i\\right)\n\\] に基づいて計算されている．log_mean は \\[\n\\begin{align*}\n    &\\log\\left(\\max_{i\\in[N]}w^i\\right)+\\log\\left(\\frac{\\sum_{j=1}^Ne^{\\log w^j-m}}{N}\\right)\\\\\n    &=\\log\\left(\\frac{1}{N}\\sum_{j=1}^Nw^j\\right)\n\\end{align*}\n\\] という値である．"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#粒子の情報保持particlehistoryクラス",
    "href": "posts/2023/Particles/ParticleFilter.html#粒子の情報保持particlehistoryクラス",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "2 粒子の情報保持：ParticleHistoryクラス",
    "text": "2 粒子の情報保持：ParticleHistoryクラス\n\n2.1 情報を収集するCollectorクラス\nSMCの結果をプロットするために，各時間で粒子の標本統計量を SMC クラス（ 節 3 ）から適宜抜き出して保存しておくためのクラス Summaries を作成する．抜き出すためのメソッドを Collector クラスの継承クラスとして定義する．\n\n\nCode\nclass Collector:\n    \"\"\"Base class for collectors\"\"\"\n    def __init__(self, **kwargs):\n        self.summary = []\n\n    def collect(self, smc):\n        self.summary.append(self.fetch(smc))\n\nclass ESSs(Collector):\n    summary_name = \"ESSs\"\n    def fetch(self, smc):\n        return smc.wgts.ESS\n\nclass LogLts(Collector):\n    summary_name = \"LogLts\"\n    def fetch(self, smc):\n        return smc.logLt\n\nclass Rs_flags(Collector):\n    summary_name = \"Rs_flags\"\n    def fetch(self, smc):\n        return smc.rs_flag\n\nclass Moments(Collector):\n    \"\"\"Collects empirical moments of the particles\"\"\"\n    summary_name = \"Moments\"\n    def fetch(self, smc):\n        m = np.average(smc.X, weights=smc.wgts.W, axis=0)\n        m2 = np.average(smc.X ** 2, weights=smc.wgts.W, axis=0)\n        v = m2 - m ** 2\n        return {\"mean\": m, \"var\": v}\n\ndefault_collector_cls = [ESSs, LogLts, Rs_flags]\n\n\n\n\n2.2 標本統計量を保持するSummariesクラス\nこのクラスはデフォルトで用意されている default_collector_cls に加えて，cols引数で指定されたメソッドを追加し，collect() メソッドが呼ばれるとこれらを集めて属性として保持する．\n\n\nCode\nclass Summaries:\n    \"\"\"A class to hold the summaries of the SMC algorithm\"\"\"\n    def __init__(self, cols):\n        self._collectors = [cls() for cls in default_collector_cls]\n        if cols is not None:\n            self._collectors.extend(col() for col in cols)\n        for col in self._collectors:\n            setattr(self, col.summary_name, col.summary)\n\n    def collect(self, smc):\n        for col in self._collectors:\n            col.collect(smc)\n\n\n\n\n2.3 ヒストリを保持するParticleHistoryクラス\ndequeオブジェクト としてヒストリを格納するためのクラスParticleHistory実装する．これにより直前 \\(k\\) ステップの情報だけを保持出来るように作れるが，今回はプロットのために全履歴を保持する．\n\n\nCode\nclass ParticleHistory:\n    \"\"\"History of the particles\n    Full history that keeps all the particle systems based on lists.\n    \"\"\"\n    def __init__(self, fk):\n        self.X, self.A, self.wgts = [], [], []\n        self.fk = fk\n\n    def save(self, smc):\n        self.X.append(smc.X)\n        self.A.append(smc.A)\n        self.wgts.append(smc.wgts)\n\n\n\n\nCode\ndef generate_hist_obj(option, smc):\n    if option is True:\n        return ParticleHistory(smc.fk)\n    else:\n        return None"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#sec-SMC",
    "href": "posts/2023/Particles/ParticleFilter.html#sec-SMC",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "3 実行部分：SMCクラス",
    "text": "3 実行部分：SMCクラス\nこのクラスがやるべきことは多い．Feynman-Kacモデル fk（ 節 4.1 で後述），粒子数 N，リサンプリング法 resampling を引数に取り，粒子フィルターを実行する．\n最も大事なこととして，本クラスはイテレータとして定義し，__next__ メソッドを実装する．そして run() メソッドで __next__ を終了するまで繰り返し呼び出すことでイテレータプロトコルを実行する．\n__next__メソッドでは，次のような処理を行う：\n\n終了フラッグ fk.done(self) が立っているかどうかを確認する．\n\\(t=0\\) の場合，最初の粒子を初期分布 \\(M_0\\) から \\(N\\) 個サンプリングする．\n\\(t&gt;0\\) の場合は，リサンプリングと粒子移動を行う．これは resample_move() メソッドで行う．\n\nリサンプリングフラッグ fk.time_to_resample(self) が立っている場合にリサンプリングを systematic メソッド（ 節 1.1 ）により行う．これにより，移動（変異）する粒子 \\(A^{1:N}_t\\) を確定させる．\n確率核 \\(M_t(X_{t-1}^{A_t^{1:N}},-)\\) に従って，粒子 \\(X_t^{1:N}\\) をサンプリングする．\n\n粒子の荷重を更新する．これは reweight_particles() メソッドで行う．\ncompute_summariesメソッドを呼び出して，粒子の標本統計量を Summaries クラスに，ヒストリを Particle History クラスに追記する．\n時刻 \\(t\\) を進めて 3.に戻る．\n\n\n\nCode\nclass SMC:\n    \"\"\"Metaclass for SMC algorithms\"\"\"\n\n    def __init__(\n        self,\n        fk=None,\n        N=100,\n        resampling=\"systematic\",\n        ESSrmin=0.5,\n        store_history=False,\n        collect=None,\n    ):\n\n        self.fk = fk\n        self.N = N\n        self.resampling = resampling\n        self.ESSrmin = ESSrmin\n\n        # initialisation\n        self.t = 0\n        self.rs_flag = False  # no resampling at time 0, by construction\n        self.logLt = 0.0\n        self.wgts = Weights()\n        self.X, self.Xp, self.A = None, None, None\n\n        self.summaries = Summaries(collect)\n        self.hist = generate_hist_obj(store_history, self)\n\n    def generate_particles(self):\n        \"\"\"Generate particles at time t=0\"\"\"\n        self.X = self.fk.M0(self.N)\n    \n    def reset_weights(self):\n        \"\"\"Reset weights to uniform after a resamping step\"\"\"\n        self.wgts = Weights()\n    \n    def resample_move(self):\n        \"\"\"Adaptively resample and move particles at time t\"\"\"\n        self.rs_flag = self.fk.time_to_resample(self)\n        if self.rs_flag:\n            self.A  = systematic(self.wgts.W, M=self.N)\n            self.Xp = self.X[self.A]\n            self.reset_weights()\n        else:\n            self.A = np.arange(self.N)\n            self.Xp = self.X\n        self.X = self.fk.M(self.t, self.Xp)\n\n    def reweight_particles(self):\n        \"\"\"Reweight particles at time t\"\"\"\n        self.wgts = self.wgts.add(self.fk.logG(self.t, self.Xp, self.X))\n\n    def compute_summaries(self):\n        \"\"\"Compute summaries at time t\"\"\"\n        if self.t &gt; 0:  # なぜかこれを前におかないとUnboundLocalErrorが出る\n            prec_log_mean_w = self.log_mean_w\n        self.log_mean_w = self.wgts.log_mean\n        if self.t == 0 or self.rs_flag:\n            self.loglt = self.log_mean_w\n        else:\n            self.loglt = self.log_mean_w - prec_log_mean_w\n        self.logLt += self.loglt\n\n        self.hist.save(self)\n        self.summaries.collect(self)\n\n    def __next__(self):\n        \"\"\"One step of the SMC algorithm\"\"\"\n        if self.fk.done(self):\n            raise StopIteration\n        if self.t == 0:\n            self.generate_particles()\n        else:\n            self.resample_move()\n        self.reweight_particles()\n        self.compute_summaries()\n        self.t += 1\n\n    def __iter__(self):\n        return self\n\n    def run(self):\n        \"\"\"Run the SMC algorithm until completion\"\"\"\n        for _ in self:\n            pass"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#粒子フィルタの実行東京の年別気温データ",
    "href": "posts/2023/Particles/ParticleFilter.html#粒子フィルタの実行東京の年別気温データ",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "4 粒子フィルタの実行：東京の年別気温データ",
    "text": "4 粒子フィルタの実行：東京の年別気温データ\n\n4.1 Feynman-Kacモデルの枠組み\nparticle パッケージの抽象クラス FeynmanKac は次のメソッドを持つ．3\n\nM0(N): 初期分布 \\(M_0\\) から \\(N\\) 個のサンプルを生成する．\nM(t, xp): カーネル \\(M_t(x_{t-1}|-)\\) から \\(X_t\\) をサイズ xp.shape[0] で生成する．\nlogG(t, xp, x): ポテンシャル \\(G_t(x_{t-1},x_t)\\) の対数を返す．\n\n加えて，粒子フィルターの実行時に必要なフラグも用意する．\n\ntime_to_resample(smc): smc オブジェクトを引数に取り，その属性 smc.aux.ESS, smc.ESSrmin からリサンプリングが必要かどうかを判定する．\ndone(smc): smc オブジェクトを引数に取り，その属性 smc.t, smc.T からアルゴリズムを終了すべきかどうかを判定する．\n\nparticle パッケージを使うときは FeynmanKac クラスを継承して用いることになるが，ここでは自分で定義していく．\n\n\n4.2 使用するデータ\n気象庁が HP にて公開している1876年から2022年までの計147年分の東京の年別気温データを用いる．\n\n\nCode\nimport pandas as pd\n\ndata = pd.read_csv(\"TemperatureDataAtTokyo.csv\")\nprint(data.describe())\n\n\n                年度         日平均         日最高        日最低          最高          最低\ncount   147.000000  147.000000  147.000000  147.00000  147.000000  147.000000\nmean   1949.000000   14.963946   19.337415   11.12517   35.098639   -4.317687\nstd      42.579338    1.132396    0.875794    1.46614    1.674956    2.439366\nmin    1876.000000   12.900000   17.500000    8.30000   31.600000   -9.200000\n25%    1912.500000   14.000000   18.700000    9.90000   34.000000   -6.150000\n50%    1949.000000   14.800000   19.300000   10.80000   34.900000   -4.700000\n75%    1985.500000   15.800000   19.900000   12.30000   36.200000   -2.250000\nmax    2022.000000   17.300000   21.300000   13.90000   39.500000    0.900000\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(3.5, 3))\n\nplt.title(\"temperature in Tokyo\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature (Celsius)\")\n\nplt.scatter(data['年度'], data['日平均'], s=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.3 気温の1次のトレンドモデル\n気温の観測値 \\(\\{y_k\\}\\) に対して，1次元の線型Gauss状態空間モデル \\[\n\\begin{cases}\nx_k=x_{k-1}+v_k,\\\\\ny_k=x_k+w_k.\n\\end{cases}\n\\tag{1}\\] \\[\nv_k\\overset{\\text{iid}}{\\sim}\\mathrm{N}(0,Q^2),\\quad w_k\\overset{\\text{iid}}{\\sim}\\mathrm{N}(0,R^2),\n\\] を想定する．このモデルを1次のトレンドモデルという (北川, 2005, p. 第11章)．\nこれをSMCメソッド（ 節 3 ）に渡せるように実装するには次のようにする：\n\n\nCode\nfrom numpy import random\nfrom scipy import stats\n\nclass Bootstrap:\n    \"\"\"Abstract base class for Feynman-Kac models derived from State Space Model (1).\n    \"\"\"\n\n    def __init__(self, data, T, R, Q):\n        self.data = data\n        self.T = T\n        self.R = R\n        self.Q = Q\n    \n    def M0(self, N):\n        \"\"\"Sample N times from initial distribution M0 of the FK model\"\"\"\n        return random.normal(loc=13.6, scale=self.Q, size=N)\n    \n    def M(self, t, xp):  # xp: resampled previous state\n        \"\"\"Sample Xt from kernel Mt conditioned on Xt-1=xp\"\"\"\n        return random.normal(loc=xp, scale=self.Q, size=xp.shape[0])\n    \n    def logG(self, t, xp, x):  # x: current state\n        \"\"\"Evaluate the log potential Gt(xt-1,xt)\"\"\"\n        return stats.norm.logpdf(self.data[t], loc=x, scale=self.R)\n    \n    def time_to_resample(self, smc):\n        \"\"\"Return True if resampling is needed\"\"\"\n        return smc.wgts.ESS &lt; smc.N * smc.ESSrmin\n    \n    def done(self, smc):\n        \"\"\"Return True if the algorithm is done\"\"\"\n        return smc.t &gt;= self.T\n\n\n\n\n4.4 \\((R,Q)=(0.2,0.1)\\) の場合\n仮に \\((R,Q)=(0.2,0.1)\\) としてみる．すなわち，システムノイズ \\(Q^2=0.1\\) が小さく，観測ノイズ \\(R^2=0.4\\) はそれよりは大きいとしている．\n\n\nCode\nmodel1 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.2, Q=0.1)\nPF1 = SMC(fk=model1, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF1.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF1.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n\n\n\n図 1: (R,Q)=(0.2,0.1) の場合の粒子フィルターの実行結果\n\n\n\n\n\n少し揺らぎながらも，トレンドとして気温が上昇していく様子が見られる．\n\n\n4.5 \\((R,Q)=(0.7,0.1)\\) の場合\n濾波して得たトレンドの揺らぎが少し大きいと思われたため，観測誤差はもう少し大きいものとして \\((R,Q)=(0.7,0.1)\\) としてみる．\n\n\nCode\nmodel4 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.7, Q=0.1)\nPF4 = SMC(fk=model4, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF4.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF4.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.7,0.1) の場合の粒子フィルターの実行結果\n\n\n\n\nこうしてトレンドとして少しばかり直線的なものが得られた．やはり上昇トレンドが見られる．\n\n\n4.6 \\((R,Q)=(0.2,0.01)\\) の場合\n\\(Q^2=10^{-4}\\) としてシステムノイズは極めて小さいと想定してみる．「トレンドは殆ど変化しない」という仮定を置いたことになる．\n\n\nCode\nmodel2 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.2, Q=0.01)\nPF2 = SMC(fk=model2, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF2.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF2.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,0.01) の場合の粒子フィルターの実行結果\n\n\n\n\nあまり良い当てはまりを見せないため，この気温の時系列を全てが観測誤差によるものだと理解するのは妥当ではないと考えられる．\n\n\n4.7 \\((R,Q)=(0.2,1)\\) の場合\n逆にシステムノイズを極めて大きい値 \\(Q^2=1\\) と設定する．トレンドは年別の揺らぎが大きいと想定したことになる．\n\n\nCode\nmodel3 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.2, Q=1.0)\nPF3 = SMC(fk=model3, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF3.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF3.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,1) の場合の粒子フィルターの実行結果\n\n\n\n\nとんでもない過適応を見せて，全てをトレンドとして説明してしまっており，これもまた妥当ではないと考えられる．\n\n\n4.8 カルマンフィルタとの比較\n線型Gaussモデルを想定しているため，粒子フィルターは \\(N\\to\\infty\\) の極限で最適フィルターであるカルマンフィルターに一致するはずである．そこで，pykalman パッケージを用いてこれを実装する．\\((R,Q)=(0.2,0.1)\\) とする．\n\n\nCode\nfrom pykalman import KalmanFilter\nKF1 = KalmanFilter(initial_state_mean=13.6, initial_state_covariance=0.1,\n                   transition_matrices=1, observation_matrices=1,\n                   transition_covariance=0.1, observation_covariance=0.2, n_dim_state=1, n_dim_obs=1)\nKF1 = KF1.em(data['日平均'], n_iter=5)  # EMアルゴリズムの過適応回避のため\n(filtered_state_means, filtered_state_covariances) = KF1.filter(data['日平均'])\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot(filtered_state_means, label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,0.1) の場合のKalmanフィルターの実行結果\n\n\n\n\nたしかに 図 1 と極めて似通った結果になっている．\n\n\n4.9 カルマン平滑化の結果\n\n\nCode\n(smoothed_state_means, smoothed_state_covariances) = KF1.smooth(data['日平均'])\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot(smoothed_state_means, label='smoothed temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,0.1) の場合のKalman平滑化の実行結果\n\n\n\n\nより滑らかなトレンドが得られている．"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#footnotes",
    "href": "posts/2023/Particles/ParticleFilter.html#footnotes",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n実はこのリサンプリング法は (Kitagawa, 1996) の付録で原型が（全く決定論的なアルゴリズムとして）提案されている↩︎\n有効サンプル数の定義については (Chopin & Papaspiliopoulos, 2020) 参照．↩︎\nFeynman-Kacモデルなどの用語については (Chopin & Papaspiliopoulos, 2020) 参照．↩︎"
  },
  {
    "objectID": "posts/2023/Lifestyle/書き起こし.html",
    "href": "posts/2023/Lifestyle/書き起こし.html",
    "title": "Whispter API を通じて日本語音声を書き起こす方法",
    "section": "",
    "text": "Whispter"
  },
  {
    "objectID": "posts/2023/Lifestyle/書き起こし.html#whispter-のダウンロード",
    "href": "posts/2023/Lifestyle/書き起こし.html#whispter-のダウンロード",
    "title": "Whispter API を通じて日本語音声を書き起こす方法",
    "section": "1 Whispter のダウンロード",
    "text": "1 Whispter のダウンロード\nまず Whisper を OpenAI の GitHub からダウンロードします．\npip install git+https://github.com/openai/whisper.git\nさらに，内部で ffmpeg が必要になるので，これもダウンロードしておく必要があります．MacOS の場合は次のコマンドでインストールできます．1\nbrew install ffmpeg\nローカル環境ではなくとも，Google Colaboratory を用いてブラウザ上で実行することもできます．その場合の詳しいやり方は，こちらのサイト が参考になります．"
  },
  {
    "objectID": "posts/2023/Lifestyle/書き起こし.html#ファイルの分割",
    "href": "posts/2023/Lifestyle/書き起こし.html#ファイルの分割",
    "title": "Whispter API を通じて日本語音声を書き起こす方法",
    "section": "2 ファイルの分割",
    "text": "2 ファイルの分割\nWhispter はどんなに大きな音声ファイルを渡しても 25MB 時点までしか書き起こしてくれません．そのため，ファイルを分割して Whispter に渡すこととします．次のコードは大きなファイルを分割するための関数を定義しています． duration=240 で，何秒間でファイルを区切るかを指定します．筆者の経験上 240 秒（４分）がうまくいきます．\n\n2.1 .wav ファイルの場合\nimport wave\n\ndef split_wav_file(filename, duration=240):\n    # WAVファイルを開く\n    with wave.open(filename, 'rb') as wav:\n        # パラメータの取得\n        n_channels, sampwidth, framerate, n_frames, comptype, compname = wav.getparams()\n\n        # 5分間のフレーム数を計算\n        frames_per_split = framerate * duration * n_channels * sampwidth\n\n        # 全フレームを読み込み\n        frames = wav.readframes(n_frames)\n\n        # 分割してファイルに書き込む\n        for i in range(0, len(frames), frames_per_split):\n            # 新しいファイル名\n            new_file = f'split_{i // frames_per_split}.wav'\n\n            # 新しいファイルを書き込む\n            with wave.open(new_file, 'wb') as new_wav:\n                new_wav.setparams((n_channels, sampwidth, framerate, frames_per_split // (n_channels * sampwidth), comptype, compname))\n                new_wav.writeframes(frames[i:i+frames_per_split])\nこうして定義した関数を次のように用いると， split_n.wav という名前で，複数のファイルに分割してくれます．\nsplit_wav_file('［あなたの手元のファイル名］.wav')\n\n\n2.2 .mp3 ファイルの場合\nfrom pydub import AudioSegment\n\ndef split_audio_file(filename, duration=240000):  # durationはミリ秒単位\n    # ファイル形式を拡張子から判断\n    extension = filename.split('.')[-1].lower()\n    \n    # AudioSegmentを使用してオーディオファイルを読み込む\n    if extension == \"wav\":\n        audio = AudioSegment.from_wav(filename)\n    elif extension == \"mp3\":\n        audio = AudioSegment.from_mp3(filename)\n    else:\n        raise ValueError(\"Unsupported file format\")\n    \n    # 分割してファイルに書き込む\n    start = 0\n    part = 1\n    while start &lt; len(audio):\n        # 新しいファイル名\n        new_file = f'split_{part}.{extension}'\n        # セグメントを切り出して保存\n        segment = audio[start:start+duration]\n        segment.export(new_file, format=extension)\n        \n        start += duration\n        part += 1\nただし，目的となるファイルが存在するディレクトリで\nsplit_audio_file(\"［あなたの手元のファイル名］.mp3\")\nと使うようにしてください．"
  },
  {
    "objectID": "posts/2023/Lifestyle/書き起こし.html#書き起こし",
    "href": "posts/2023/Lifestyle/書き起こし.html#書き起こし",
    "title": "Whispter API を通じて日本語音声を書き起こす方法",
    "section": "3 書き起こし",
    "text": "3 書き起こし\n続いて，細かく分けたファイル split_n.wav たちを順に Whisper に渡して書き起こしてもらい，結果を１つのテキストファイル 書き起こし.txt にまとめてもらいます．\nimport whisper\n\n# モデルのロード\nmodel = whisper.load_model(\"large\")  # やっぱ精度が違います\n\n# ファイルのリスト\nfiles = [f\"split_{i}.wav\" for i in range(27)]  # split_0.wav から split_26.wav まで\n\n# 結果を格納するための空の文字列\ntranscription = \"\"\n\n# 各ファイルを順番に処理\nfor file in files:\n    # ファイルを書き起こし\n    result = model.transcribe(file, language='ja')\n    transcription += result[\"text\"] + \"\\n\\n\"\n\n# 書き起こし結果をテキストファイルに書き込む\nwith open(\"書き起こし.txt\", \"w\", encoding=\"utf-8\") as text_file:\n    text_file.write(transcription)\nここでは最大のモデル large を用いています．その場合，結構な時間がかかります．"
  },
  {
    "objectID": "posts/2023/Lifestyle/書き起こし.html#footnotes",
    "href": "posts/2023/Lifestyle/書き起こし.html#footnotes",
    "title": "Whispter API を通じて日本語音声を書き起こす方法",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nそれ以外の OS の場合は こちら の README.md にやり方が書いてあります．↩︎"
  },
  {
    "objectID": "posts/2023/Lifestyle/LaTeXwithVSCode.html",
    "href": "posts/2023/Lifestyle/LaTeXwithVSCode.html",
    "title": "VSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode",
    "section": "",
    "text": "Please consult the following links (Japanese):\n\nVSCode で最高の LaTeX 環境を作る\nVSCodeでのLaTeXの環境構築\n\n\n\n\nAs for me (Mac mini, and MacBook Pro), I use the following .latexmkrc file to compile LaTeX documents, through VSCode’s LaTeX Workshop extension.\n\n\n.latexmkrc\n\n#!/usr/bin/env perl\n\n$pdf_mode = 3;\n$latex            = 'uplatex %O -kanji=utf8 -no-guess-input-enc -interaction=nonstopmode -file-line-error %S -synctex=1';\n$bibtex           = 'upbibtex %O %B';\n$dvipdf           = 'dvipdfmx %O -o %D %S';\n$makeindex        = 'mendex %O -o %D %S';\n\n$pvc_view_file_via_temporary = 0;\n$pdf_previewer               = 'open -ga /Applications/Skim.app';\n\nPlace .latexmkrc in your home directory (~/.latexmkrc).\n\n\n\nIn order to make a pdf file from a LaTeX file, you need to set the following in your settings.json file.\n\n\nsettings.json\n\n\n{\n    \"latex-workshop.latex.recipes\": [\n        {\n            \"name\": \"upLaTeX\",\n            \"tools\": [\n                \"latexmk\"\n            ]\n        },\n    ],\n\n    \"latex-workshop.latex.tools\": [\n        {\n            \"name\": \"latexmk\",\n            \"command\": \"latexmk\",\n            \"args\": [\n                \"-silent\",\n                \"-outdir=%OUTDIR%\",\n                \"%DOC%\"\n            ],\n        },\n    ],\n}"
  },
  {
    "objectID": "posts/2023/Lifestyle/LaTeXwithVSCode.html#starting-guide",
    "href": "posts/2023/Lifestyle/LaTeXwithVSCode.html#starting-guide",
    "title": "VSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode",
    "section": "",
    "text": "Please consult the following links (Japanese):\n\nVSCode で最高の LaTeX 環境を作る\nVSCodeでのLaTeXの環境構築\n\n\n\n\nAs for me (Mac mini, and MacBook Pro), I use the following .latexmkrc file to compile LaTeX documents, through VSCode’s LaTeX Workshop extension.\n\n\n.latexmkrc\n\n#!/usr/bin/env perl\n\n$pdf_mode = 3;\n$latex            = 'uplatex %O -kanji=utf8 -no-guess-input-enc -interaction=nonstopmode -file-line-error %S -synctex=1';\n$bibtex           = 'upbibtex %O %B';\n$dvipdf           = 'dvipdfmx %O -o %D %S';\n$makeindex        = 'mendex %O -o %D %S';\n\n$pvc_view_file_via_temporary = 0;\n$pdf_previewer               = 'open -ga /Applications/Skim.app';\n\nPlace .latexmkrc in your home directory (~/.latexmkrc).\n\n\n\nIn order to make a pdf file from a LaTeX file, you need to set the following in your settings.json file.\n\n\nsettings.json\n\n\n{\n    \"latex-workshop.latex.recipes\": [\n        {\n            \"name\": \"upLaTeX\",\n            \"tools\": [\n                \"latexmk\"\n            ]\n        },\n    ],\n\n    \"latex-workshop.latex.tools\": [\n        {\n            \"name\": \"latexmk\",\n            \"command\": \"latexmk\",\n            \"args\": [\n                \"-silent\",\n                \"-outdir=%OUTDIR%\",\n                \"%DOC%\"\n            ],\n        },\n    ],\n}"
  },
  {
    "objectID": "posts/2023/Lifestyle/LaTeXwithVSCode.html#tips",
    "href": "posts/2023/Lifestyle/LaTeXwithVSCode.html#tips",
    "title": "VSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode",
    "section": "2 Tips",
    "text": "2 Tips\n\n2.1 Intellisense\nWhen editing *.bib files, typing @ triggers intellisense to suggest snippets.\nThese snippet suggestions are controlled by the file bibtex-entries.json, and three variables concerning formatting:\n\nlatex-workshop.bibtex-format.tab\nlatex-workshop.bibtex-format.surround\nlatex-workshop.bibtex-format.case\n\nYou can customize the variable latex-workshop.intellisense.bibtexJSON.replace to modify the default fields included in the snippet auto-completion.\n\n\nsettings.json\n\n{\n    \"latex-workshop.intellisense.bibtexJSON.replace\": {\n        \"article\" : [\"author\", \"year\", \"title\", \"journal\", \"volume\", \"number\", \"pages\", \"url\"]\n    },\n    \"latex-workshop.bibtex-format.tab\": \"4 spaces\",\n}"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2013.html",
    "href": "posts/2023/Articles/DelMoral2013.html",
    "title": "書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration",
    "section": "",
    "text": "book cover"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2013.html#書籍紹介-del-moral-2013-mean-field-simulation-for-monte-carlo-integration",
    "href": "posts/2023/Articles/DelMoral2013.html#書籍紹介-del-moral-2013-mean-field-simulation-for-monte-carlo-integration",
    "title": "書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration",
    "section": "書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration",
    "text": "書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\nリンクの在処は分かりにくいですが，前文と一部の内容が著者のHPからご覧になれます．\nMean Field Simulation for Monte Carlo Integration | Pierre Del Moral |\n「平均場粒子モデルは非線型発展方程式を確率的に線型化する技法である」とはどういうことか？前文を読むだけでストーリーがあらかた掴めます（が長いです）．"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2013.html#preface",
    "href": "posts/2023/Articles/DelMoral2013.html#preface",
    "title": "書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration",
    "section": "Preface",
    "text": "Preface\n\nMonte Carlo Integration\n本書は平均場シミュレーションモデルのモンテカルロ積分への応用と理論的基礎を扱う．\nここ30年，このトピックが純粋・応用確率論，ベイズ推論，統計的機械学習，情報理論，理論化学，量子物理，金融数学，信号処理，リスク解析，工学，計算機科学の，最も活発な接点の1つとなっている．\nモンテカルロシミュレーションの源は， Metropolis and Ulam (1949) The Monte Carlo Method であり，MetropolisがUlamのポーカー付きから，モナコの首都にちなんで名付けた．\nモンテカルロ積分の最初の応用はロスアラモス国立研究所でのManhattan計画にて，原子爆弾着火のモデルにおいてSchödinger作用素の基底状態のエネルギーを計算するために用いられたものだ．\nMonte Carlo積分の理論，MCMCとSMC，そして平均場IPS (Interacting Particle Systems)とは，いずれも複雑な確率分布からサンプリングするために用いられる．この文脈では，ランダムサンプルは積分の計算のために用いられる．他の状況で，確率的なアルゴリズムは，逆問題，大域的最適化，事後分布計算，非線形推定問題，統計的学習問題など，複雑な推定問題を解くのにも使われる．\n最も有名なMCMCアルゴリズムはMetropolis-Hastings法だろう． Metropolis and Ulam (1949) The Monte Carlo Method によると，\n\nthe Monte Carlo method is, “essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.”\n\nまさにそういうように，「確率測度の空間上の任意の発展モデルは，いつでもMarkov過程のランダムな状態の分布だと解釈できる」ことを強調しておきたい．この観察は，Markov過程と関連する線型発展モデルとの研究ではよく知られていることである．\nさらに面白いことには，Markov過程のランダムな状態の分布と，非線形に相互作用することを許すならば，非線型な発展モデルもMarkov過程の分布とみれる．そのMarkov連鎖のランダムな状態は複雑な確率分布のフローに従い，しばしば解析的な解が存在しない（計算不可能）であることもある．この文脈で，Monte Carlo法と平均場法は，このような複雑系をシミュレーションしたり分析するにあたって，シンプルで安価な方法を提供してくれる．\nこの2点の観察が，本書で展開される平均場粒子理論の大事な到達点である．\n\n\nMean Field Simulation\n平均場IPSの研究は1960年代の Henry McKean の流体力学における非線型楕円型偏微分方程式のMarkov解釈の研究から始まった．\nこの初期の研究から1990年代の半ばにかけて，複数の研究者がこの分野を開拓した．主な内容は非線型Markov連鎖モデルの存在に関連するマルチンゲール問題を解く問題，連続時間IPSモデル（McKean-Vlasov拡散，反応拡散方程式，Boltzmann型相関ジャンプ過程など）のカオスの伝播の記述などであった．古典的な応用は基本的に流体力学，化学，凝縮系物理に制限されていた．\nしかし1990年代の中盤から，平均場IPS手法を，情報理論，工学，計算物理学，統計的機械学習理論におけるMonte Carloシミュレーションへの応用が爆発した．この洗練された，個体群タイプのIPSアルゴリズムは，並列化や分散化された計算環境にも向いていた．その結果，ここ数年来，安価な計算資源の普及に後押しされて，これらの計算機集約的なツールは大きく知名度を上げた．この発展的なMonte Carlo積分法は，決定論的な関数射影アルゴリズムやグリッドを用いるアルゴリズムが低次元空間における線型モデルにしか使えない弱みを補完する手法を提供している．\n古典的なMCMC法と違って，平均場IPS法の大きな美点は，正確性を司るパラメータは，何か固定された目的分布とも，予備動作時間とも関係なく，ただ粒子数 \\(N\\) のみに依存する，という点である．つまり，IPS算譜を動かす計算機の並列計算力などのみに依存して，正確性を発揮することが出来るということである．\n過去20年，非線型フィルタリング問題や複雑なBayes事後分布を計算したり，遺伝的手法で最適化問題を解いたりする中で，新たなクラスの平均場PISサンプラーが発明された．こうして，古典的な流体力学モデルから，種々の科学分野で見られる様々な非線型問題に，射程を広げつつある．\n非線型フィルタリング問題への応用は，乱流についての流体力学や，天気予報の問題で生じる．最近では空間的点過程への応用が進んでいるが，これは生態系モデリング，生物学，疫学，地震，物質科学，待ち行列理論，天文学など種々の分野で見つかる．\nさらに最近では金融への応用も盛んである．粒子法の稀事象解釈を通じて，信頼のおけるポートフォリオが同時にデフォルトを起こす確率のシミュレーションを行っている研究が Carmona, Fouque and Vestal (2009) にある．\nさらに最近には，ジャンプ拡散過程による価格モデルで，ジャンプ時刻やジャンプサイズなどの潜在変数をフィルタリングする研究もある．粒子法は，確率的最適化アルゴリズムの構築にも使えるが，これも金融数学の分野で応用が進みつつある．これは複数の最小値点が存在する場合でも使える手法として Ben Hamida and Cont (2005) が開拓している．\nさらに分岐するIPSは，直接生物学や自然淘汰理論のモデルとして応用が進んでいる．\nこの文脈で，平均場ゲーム理論にも触れねばならないだろう．ここで「流体粒子」に当たるものはエージェント（または会社）とみなされ，ある報酬関数に対して最適な行動を取るように，社会経済的な環境で競争をしていく様子をモデリングする．\nKolokoltsovにより，大量のエージェントを備えるゲーム理論の生物学・経済学・ファイナンスへの応用が進んでいる．平均場ゲームのHamilton-Jacobi非線型方程式を解くための有限差分法の研究も同時に進んでいる．\n\n\nA Need for Inter-diciplinary Research\nこの平均場シミュレーション理論には多くの分野の研究者が参入していることは，その文献の多さが証明している．しかし，これらの異なる分野の間のコミュニケーションは非常に難しいことで，実際まだまだ伸び代がある．実際，平均場Feynman-Kacモデルは多くの別の名前で知られている．\n\nIn physics, engineering sciences, as well as in Bayesian statistical analysis, the same interacting jump mean ﬁeld model is known under several lively buzzwords; to name a few: pruning [403, 549], branching selection [170, 286, 484, 569], rejuvenation [8, 134, 275, 336, 490], condensation [339], look-ahead and pilot exploration resampling [263, 403, 405], Resampled Monte Carlo and RMC methods [556], subset simulation [20, 21, 22, 396, 399], Rao-Blackwellized particle filters [280, 444, 457], spawning [138], cloning [310, 500, 501, 502], go-with-the-winner [7, 310], resampling [324, 522, 408], rejection and weighting [403], survival of the fittest [138], splitting [121, 124, 282], bootstrapping [43, 289, 290, 409], replenish [316, 408], enrichment [54, 336, 262, 374], and many other botanical names.\n\n一方で，多くの応用的な研究が，数学的な側面については盲目に突き進んでいるという現状もある．結果として，多くの応用的な研究で平均場IPSモデルが自然言語的に直感的に提示され，全くパフォーマンス解析もロバスト性や安定性に対する言及もなく使われている．\n逆に数学的な側面の研究についても，多くの未解決問題が存在する．数理統計楽や確率論の研究者は，現在の多くの応用研究で進行中の研究を注視する必要がある．より豊かな応用数学と応用科学のためには，学際的な交流が欠かせないと考える．\nそのためには，統一的な数学的基盤，共通言語というのが欠かせないだろう．この本は，Monte Carlo積分に対する平均場シミュレーションの技術に対して最新の取り扱いを統一的に記述することで，複数の分野を橋渡しするためにある！この本が確率論研究者，応用統計家，生物学者，統計物理学者，計算機科学者が，互いの障壁を乗り越える一助になることを願っている．\n古典的なMonte Carlo法やシミュレーション法本は数え切れないにも拘らず，平均場シミュレーション理論を扱った書籍は少ない．\n本書は Del Moral (2004) と確率論セミナー (2000)，そしてさらに新しいサーベイ (2012) の続編とみなすことができる．本書は，Feynmna-Kacモデルだけでなく，McKean-Vlasovモデルや分岐相関ジャンプ過程などの種々のIPSアルゴリズムに応用可能な平均場理論も提供する．\n\n\nUse and Interpretations of Mean Field Models\n特に強調したいことは，本書で扱うほとんどの平均場IPSアルゴリズムは数学的には全く等価であり，ただ解釈の仕方が，その応用分野に依って異なるのみである．\n流体力学と計算物理学において，平均場粒子モデルはマクロ物理量がミクロ変数の分布と相互作用しながら発展していく系のモデルになる．例えば期待，マクロ流体モデルや，分子系である．中心的なアイデアは，2次の摂動項を無視することで，分布の空間上の閉じた非線型発展方程式に還元することである．このモデルの平均場極限は，（多くの場合微分／積分方程式の言葉で）物理量の発展を記述する．\n計算生物学や個体群動態学では，生誕・死亡や競争選択の過程によって，平均場遺伝的粒子モデルが構成される．このモデルの平均場極限はしばしば「無限人口モデル (infinite population model)」と呼ばれる．\n計算機科学では，平均場遺伝的IPSアルゴリズムは複雑な最適化問題を解くための確率的探索手法として使われる．この場合のモデルの平均場極限は，ある種の適合度を表すポテンシャル関数に付随するBoltzmann-Gibbs測度によって与えられる．\n信号処理と機械学習理論において，平均場IPSモデルは逐次Monte Carloサンプラーとも呼ばれる．名前の通り，このモデルは，複雑性が増していく確率分布の列から逐次的にサンプリングをするために用いられる．状態空間は，稀事象シミュレーションではexcursion space，逐次重点サンプリングでは遷移状態空間，フィルタリングと平滑化問題では見本道の空間である．信号処理の分野においては，この手法は「粒子フィルター」とも呼ばれる．このモデルにおいて，平均場極限は，ある事象について条件づけた際の確率過程の条件付き分布の発展方程式系になる．線型Gaussモデルにおいて，最適フィルターは，平均と分散がKalmanフィルターによって逐次的に与えられるような条件付きGauss分布になる．この設定において，その発展方程式はMcKean-Vlasov拡散モデルとみなせる！この平均場モデルは，気象予測とデータ同化の分野で用いられているアンサンブルKalmanフィルターに一致するのである．\n物理学と分子化学において，平均場IPS発展モデルは多体Schödinger発展方程式の基底状態のエネルギーの推定に用いられる．この設定において，「粒子」と呼ぶと物理的対象と混同してしまうため，”walker”（探索者）と呼ばれる．この確率的な発展モデルは QMC (Quantum Monte Carlo) または DMC (Diffusion Monte Carlo) 法と呼ばれる．この手法は多体系の状態空間上での経路積分を近似するために設計される．このモデルの平均場極限は正規化されたSchrödinger方程式になる．したがって，このモデルの非線型半群の長期的な振る舞いは，Schrödinger作用素の最大固有値と基底状態のエネルギーに関連を持つのである．\n確率論において，平均場IPSモデルは2通りの解釈を持つ．1つ目の見方として，粒子系は，目的の発展方程式の解を，逐次的に経験測度の空間へ射影しているとみれる．より正確に，平均場IPSモデルの定める経験測度は，この削減された有限次元状態空間上のMarkov過程として発展していく．従来のMCMC法は単一の確率過程の長期的な振る舞いに基づいて設計されていたのと対照的に，平均場IPSのMarkov過程は \\(N\\) 個の状態空間上の積上で発展する．この意味で「平均場粒子モデルは非線型発展方程式を確率的に線型化する技法である」と言える．2つ目の見方は，分布の空間上の非線型発展方程式の新たな確率的摂動論という見方である．粒子の集団の局所的なサンプリングの推移は，現在の粒子の経験測度に依存するので，局所的なサンプリング誤差を系に導入する．粒子はこの摂動を持ちながら非線型発展方程式に従っている，と見れるのである．\n\n\nA Unifying Theoretical Framework\n本書の大半は，「平均場理論の離散世代と分布空間上の非線型発展方程式への応用」を扱う．ほとんどのモデルは，連続時間における測度値過程を，離散時間で近似することで生じる．物理学，金融，生物学分野での連続時間モデルの重要性を鑑みて，本書の多くの部分は離散時間測度値過程とその連続時間の場合（特に線型・非線型微分・積分方程式）との関連も取り扱っている．\n古典的なMCMC法の平衡への収束に関する解析で用いられている数学と，我々が用いる数学とは大きく異なる．加えて，従来のMonte Carloサンプラーと対照的に，平均場IPSモデルは統計的に独立な粒子を取り扱っている訳ではないから，従来の大数の法則の知識を直接適用して相関粒子系によるサンプラーの解析に用いることは出来ない．\n直近に発展した，連続時間の相関粒子系の解析は，「カオスの伝播」という性質と漸近理論に基づいており，エルゴード的な性質や指数集中性については全く判っていなかった．\n筆者の知る限り，離散世代平均場粒子モデル，時間パラメータに対する一様な定量的推定とその非線型フィルタリング問題への応用とについての最初の研究は，Del Moral (1996), Del Moral (1998) である．その後この研究は更なる発展を遂げ，多くの応用を持った．\n離散世代平均場モデルの収束解析を進めるにあたって，次の数学理論を使うことになるだろう：分布空間上の非線型半群，相関を持つ経験過程理論，指数集中不等式，時間上限 \\(T\\) に関する一様収束推定，汎函数揺動定理．さらにこれらの複合を用いることが日常茶飯事である．例えば，一様指数集中不等式は，後ろ向き非線型半群と，分布空間上の1次のTaylor展開， \\(L^m\\)-平均誤差推定，Orliczノルム解析とLaplace近似を用いる．\n時間上限 \\(T\\) の一様定量的 \\(L^m\\)-平均誤差バウンドと一様集中不等式とは，非線型半群の極限的な安定性に関する振る舞いに依存する．このような，平均場粒子モデルの長期的な振る舞いと，測度のフローの極限的な安定性とが関連しているというタイプの結果は新しいものではない．\n離散世代遺伝的粒子モデルの解析は，系統樹モデル，部分的に観測された分岐過程，粒子自由エネルギー，後ろ向きMarkov粒子モデルなど，他の数学モデルとも深い繋がりを持っている．本書の大部分は，半群理論と確率的摂動理論とを組み合わせて，種々の相関粒子系の収束を示す，という議論を抽象的に行う．\nいま，抽象的で一般的な非線型発展方程式の解析で苦しんでいる人の苦悩は，本書を読めば，すぐに解決されるかもしれない．というのも，平均場相関粒子近似は，すぐさまに強力なMonte Carloシミュレーション法を与えるからである．このタイプの叙述も，McKean-Vlasov拡散モデルと，Feynman-Kac分布フローと，空間分岐発展モデルなどとについて行った．\n本書は，現状強力な道具となっているカオスの伝播の性質や，Berry-Esseen定理や，漸近的な大偏差原理については触れない．これらは Del Moral (2004) を参照のこと．\n\n\nA Contents Guide\n本書の中心的なテーマは平均場シミュレーション理論の，分布空間上の非線型発展方程式への応用である．\n初めの第1章と第2章は概観を提供する．この2つの章は読み飛ばすべきではない．\n理論の基礎はMarkov過程である．線型だろうと非線型だろうと，発展方程式の解析においてMarkov過程は中心的な役割を果たす．分布の空間上の発展モデルは常にランダムな状態を持つMarkov過程の分布として解釈できる．この同一視により，Markov過程の理論が，分布値の方程式を，そのMarkov過程からランダムにサンプリングすることで解く方法を示唆する．この抽象的な理論を，種々の応用例で解説しているのが第1章である．\n第2章はMcKean-Vlasov拡散モデル，Feynman-Kacモデルを，種々の応用の中で見ていく．\n第3章はFeynman-Kacモデルを導入し，応用を見る．\n第4章で，Feynman-Kacモデルの4つの等価な解釈を見る．それは分岐過程による解釈とそれが導く遺伝的アルゴリズム(GA)，逐次Monte Carlo法を導く解釈(SMC)，相関を持つMCMCサンプラーを導く解釈(i-MCMC)，そして最後に平均場相関粒子系としての解釈である(IPS)．\n数学的な観点からは，いずれの解釈も全く等価である．しかしながら，それぞれの解釈は異なるアルゴリズムを導く．\nさらに，McKean-Vlasov拡散モデルも，平均場Feynman-Kacモデルと結びつくということを強調しておきたい．この種の平均場IPSフィルタリングモデルは乱流流体力学や気象予測問題における非線型フィルタリング問題を解くのに使われている．\n第5章で，離散世代Feynman-Kacモデルとその連続時間バージョンとの関係を述べる．\n第6章で，\n本書の第II部は，平均場IPS理論を種々の科学分野への応用を扱う．"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html",
    "href": "posts/2023/Processes/BranchingProcesses.html",
    "title": "分岐過程",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html#離散分岐過程",
    "href": "posts/2023/Processes/BranchingProcesses.html#離散分岐過程",
    "title": "分岐過程",
    "section": "1 離散分岐過程",
    "text": "1 離散分岐過程\n\n1.1 定義\n\n\n\n\n\n\n定義1 ：Bienaymé-Galton-Watson 過程 (Bienaymé, 1845), (Watson & Galton, 1875)\n\n\n\n\\(Y_1,Y_2,\\cdots\\) を \\(\\mathbb{Z}\\)-値確率変数の列とし，\\(\\mu\\in\\mathcal{P}(\\mathbb{Z})\\) を初期分布とする．これに対して \\[\nX_0\\sim\\mu,\\quad X_{n+1}:=1_{\\left\\{X_n\\ge 1\\right\\}}\\sum_{j=T_{n-1}+1}^{T_{n-1}+X_n}Y_j,\n\\] \\[\nT_{-1}:=0,\\quad T_n:=\\sum_{j=0}^nX_j\\quad(n\\in\\mathbb{N}),\n\\] と定める．この過程 \\(\\{X_n\\}_{n=0}^\\infty\\) を 分岐過程 といい，分布を \\(\\operatorname{P}_\\mu\\) と表す．\n特に \\(\\{Y_n\\}_{n=1}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{N})\\) が非負で独立同分布である場合，過程 \\(\\{X_n\\}_{n=0}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{N})\\) を Bienaymé-Galton-Watson 分岐過程 という．\n\n\n\\(X_n\\) を第 \\(n\\) 世代の個体数と解釈してみよう．\\(T_n\\) は第 \\(n\\) 世代までを含め，存在し得た全個体数を意味することとなる．\n第 \\(n\\) 世代を構成する \\(X_n\\) 個体がそれぞれ \\(Y_{T_{n-1}+1},\\cdots,Y_{T_{n-1}+X_n}=Y_{T_n}\\) 人の家族を遺して消滅し，これらの子孫が第 \\(n+1\\) 世代 \\(X_{n+1}\\) の構成員となる．\n\\(X_n\\le0\\) となった場合は以降も常に \\(X_m=0\\;(m\\ge n)\\) が成り立ち，\\(0\\) が吸収点となる．\n\\(X_n,Y_n\\) に負の整数値も許す場合は，2つの種族の個体数の差を考える場合などと解釈できる．\n\n\n1.2 乱歩への埋め込み\n初期分布を \\(\\mu=\\delta_1\\) とすると，\\(T_0=X_0=1\\;\\;\\text{a.s.}\\)．すると，任意の \\(m\\in\\mathbb{N}^+\\) について，事象 \\(\\left\\{X_m\\ge 1\\right\\}\\) の上では， \\[\nT_{n}=1+\\sum_{j=1}^{T_{n-1}}Y_j\\quad(0\\le n\\le m+1)\n\\tag{1}\\] \\[\n\\begin{align*}\n    X_{n}&=T_{n}-T_{n-1}\\\\\n    &=1+\\sum_{j=1}^{T_{n-1}}Y_j-T_{n-1}\\\\\n    &=1+\\sum_{j=1}^{T_{n-1}}\\widetilde{Y}_j\\quad(0\\le n\\le m+1)\n\\end{align*}\n\\tag{2}\\] \\[\n\\widetilde{Y}_j:=Y_j-1\\quad(j\\in\\mathbb{N}^+)\n\\] が成り立つ．\nよって，絶滅しないという事象 \\[\n\\bigcap_{m\\in\\mathbb{N}^+}\\left\\{X_m\\ge 1\\right\\}=\\left\\{\\lim_{n\\to\\infty}T_n=\\infty\\right\\}\n\\] の上では，\\(X_0=1\\;\\;\\text{a.s.}\\) から始まる乱歩 (random walk) \\[\nX_n=1+\\widetilde{S}_{T_{n-1}}\\quad(n\\in\\mathbb{N})\n\\] \\[\n\\widetilde{S}_n:=\\sum_{j=1}^n\\widetilde{Y}_j\\quad(n\\in\\mathbb{N})\n\\] と理解できる．\n\n\n1.3 Markov性\n\n\n\n\n\n\n命題2 （Markov性）\n\n\n\n\\(\\{X_n\\}_{n=0}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{Z})\\) を，初期分布 \\(\\mu=\\delta_1\\) と確率変数列 \\(\\{Y_n\\}_{n=1}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{Z})\\) が定める分岐過程，\\(\\mathcal{F}_n:=\\sigma(X_k|k\\in n+1)\\) を \\((X_n)\\) が定める filtration とする．\n\n任意の \\(n\\in\\mathbb{N}^+\\) について \\(\\operatorname{E}_{\\delta_1}[Y_{n+1}|Y_1,\\cdots,Y_n]=a\\) ならば， \\[\n\\operatorname{E}_{\\delta_1}[X_{n+1}|\\mathcal{F}_n]=aX_n^+,\\quad n\\in\\mathbb{N}.\n\\]\n加えて \\(\\mathrm{V}_{\\delta_1}[Y_{n+1}|Y_1,\\cdots,Y_n]=\\sigma^2\\;(n\\in\\mathbb{N}^+)\\) ならば， \\[\n\\mathrm{V}_{\\delta_1}[X_{n+1}|\\mathcal{F}_n]=\\sigma^2X_n^+,\\quad n\\in\\mathbb{N}.\n\\]\nさらに \\(\\{Y_n\\}_{n=1}^\\infty\\) が独立であるならば，\\(\\{(X_n,T_{n-1})\\}_{n=1}^\\infty\\) は Markov 連鎖であるが，\\(\\{X_n\\}_{n=0}^\\infty\\) は Markov 連鎖であるとは限らない．\nさらに \\(\\{Y_n\\}_{n=1}^\\infty\\) が同分布であるならば，\\(\\{X_n\\}_{n=0}^\\infty\\) と \\(\\{(X_n,T_{n-1})\\}_{n=1}^\\infty\\) はいずれも時間的に一様な Markov 連鎖である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n任意の \\(\\{r_k\\}_{k=1}^n\\subset\\mathbb{Z}\\) について，等式 \\[\n\\operatorname{E}_{\\delta_1}[X_{n+1}|X_1=r_1,\\cdots,X_n=r_n]=ar_n^+\n\\] を示せば良い．\\(r_n\\le0\\) の場合は直ちに従うから，\\(r_n\\ge1\\) として考える．この下では変形 式 1 を用いることが出来るから， \\[\n\\begin{align*}\n&\\left\\{\\forall_{j\\in[n]}\\;X_j=r_j\\right\\}\\\\\n&=\\left\\{\\forall_{j\\in[n-1]}\\;T_j=i_j,X_n=r_n\\right\\}\\\\\n&=\\biggl\\{\\forall_{j\\in[n-1]}\\;\\sum_{k=1}^{i_{j-1}}Y_k=i_j-1,\\\\\n&\\qquad\\qquad\\qquad\\sum_{j=i_{n-2}+1}^{i_{n-1}}Y_j=r_n\\biggr\\}\\\\\n&=:B_n\n\\end{align*}\n\\] \\[\ni_j:=1+r_1+\\cdots+r_j,\\quad j\\in[n-1],\n\\] が成り立つから，仮定より \\[\n\\operatorname{E}_{\\delta_1}[Y_{j}1_{B_n}]=a\\operatorname{P}_{\\delta_1}[B_n]\\quad(j\\ge i_{n-1}+1)\n\\] であることに注意して， \\[\n\\begin{align*}\n&\\operatorname{E}_{\\delta_1}[X_{n+1}|X_1=r_1,\\cdots,X_n=r_n]\\\\\n&\\qquad\\cdot\\operatorname{P}_{\\delta_1}[X_1=r_1,\\cdots,X_n=r_n]\\\\\n&=\\operatorname{E}_{\\delta_1}[X_{n+1}1_{\\left\\{X_1=r_1,\\cdots,X_n=r_n\\right\\}}]\\\\\n&=\\operatorname{E}_{\\delta_1}\\left[1_{B_n}\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}Y_j\\right]\\\\\n&=ar_n\\operatorname{P}_{\\delta_1}[B_n]\n\\end{align*}\n\\] を得る．両辺を \\(\\operatorname{P}_{\\delta_1}[B_n]\\) で割ると，初めの式を得る．3\n同様にして， \\[\n\\begin{align*}\n&\\mathrm{V}_{\\delta_1}[X_{n+1}|X_1=r_1,\\cdots,X_n=r_n]\\\\\n&=\\operatorname{E}_{\\delta_1}[X_{n+1}^2|B_n]-\\operatorname{E}_{\\delta_1}[X_{n+1}|B_n]^2\\\\\n&=\\operatorname{E}_{\\delta_1}\\left[\\left(\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}Y_j\\right)^2\\:\\middle|\\:B_n\\right]\\\\\n&\\qquad\\qquad-\\operatorname{E}_{\\delta_1}\\left[\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}Y_j\\:\\middle|\\:B_n\\right]^2\\\\\n&=\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}\\biggr(\\operatorname{E}_{\\delta_1}[Y_j^2|B_n]-\\operatorname{E}_{\\delta_1}[Y_j|B_n]^2\\biggl)\\\\\n&=\\sigma^2r_n^+.\n\\end{align*}\n\\] ただし，途中の式変形で，任意の \\(i_{n-1}&lt;j&lt;k\\le i_{n}\\) について \\[\n\\begin{align*}\n\\operatorname{E}_{\\delta_1}[Y_jY_k|B_n]&=\\operatorname{E}_{\\delta_1}\\biggl[Y_j\\operatorname{E}[Y_k|B_n,Y_j]\\:\\bigg|\\:B_n\\biggr]\\\\\n&=\\operatorname{E}_{\\delta_1}[Y_ja|B_n]\\\\\n&=a^2=\\operatorname{E}_{\\delta_1}[Y_j|B_n]\\operatorname{E}_{\\delta_1}[Y_k|B_n]\n\\end{align*}\n\\] が成り立つことを用いた．\n\\((Y_k)_{k=1}^\\infty\\) が独立であるとき，\\((X_k)_{k=0}^\\infty\\) も独立である．これより，任意の時点 \\(m_1&lt;\\cdots&lt;m_k&lt;n\\in\\mathbb{N}\\) について，\\(T_{m_k-1}\\) が与えられた下で，\\(X_k\\;(k\\ge m_k)\\) と \\(\\mathcal{F}_{m_k-1}\\) とは独立である．よって特に，\\((X_{m_k},T_{m_k-1})\\) が与えられた下で，\\((X_n,T_{n-1})\\) と \\((X_{m_i},T_{m_i-1})\\;(i&lt;k)\\) とは独立である．よって，条件付き独立性の性質 より \\[\n\\begin{align*}\n&\\operatorname{P}_{\\delta_1}\\biggl[(X_{n},T_{n-1})=(r_{n},s_{n-1})\\:\\bigg|\\:\\\\\n&\\qquad\\qquad\\forall_{i\\in[k]}\\;(X_{m_i},T_{m_i-1})=(r_{m_i},s_{m_i-1})\\biggr]\\\\\n&=\\operatorname{P}_{\\delta_1}\\biggl[(X_n,T_{n-1})=(r_n,s_{n-1})\\:\\bigg|\\:\\\\\n&\\qquad\\qquad(X_{m_k},T_{m_k-1})=(r_{m_k},s_{m_k-1})\\biggr]\n\\end{align*}\n\\] が成り立つ．\n\n\n\n\n\n\n\n1.4 生存確率\n\n\n\n\n\n\n生存確率\n\n\n\n\\(\\{Z_n\\}_{n=0}^\\infty\\) を BGW 過程で，\\(\\operatorname{P}[Y_1\\in 2]&lt;1\\) とする．このとき，\\(\\mu:=\\operatorname{E}[Y_1]\\in[0,\\infty]\\) について， 4\n\n\\(\\mu\\le1\\) ならば， \\[\n\\operatorname{P}_1[\\forall_{n\\in\\mathbb{N}}\\;Z_n\\ge1]=0.\n\\]\n\\(\\mu&gt;1\\) ならば，ある \\(q\\in[0,1)\\) が存在して \\[\n\\operatorname{P}_1[\\forall_{n\\in\\mathbb{N}}\\;Z_n\\ge1]=1-q&gt;0\n\\] を満たし，\\(q\\in[0,1)\\) は \\(Y_1\\) の確率母関数 \\[\ng(z):=\\sum_{k=0}^\\infty\\operatorname{P}[Y_1=k]z^k\n\\] に関する方程式 \\(g(z)=z\\) の \\([0,1)\\) 上でのただ一つの解である．\n\n\n\nすなわち，「未来永劫絶滅しない」確率が正であるかどうかは \\(Y_1\\) の期待値 \\(\\mu\\) のみに依存するが，その確率 \\(1-q\\) は \\(Y_1\\) の分布によって定まる． 5\n\n\n\n\n\n\n証明"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html#連続分岐過程",
    "href": "posts/2023/Processes/BranchingProcesses.html#連続分岐過程",
    "title": "分岐過程",
    "section": "2 連続分岐過程",
    "text": "2 連続分岐過程\n\n2.1 定義\n\n\n\n\n\n\n定義6 ：連続分岐過程\n\n\n\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}_+)\\) が（連続時間）分岐過程 であるとは， \\[\nQ_t(x,-)*Q_t(y,-)=Q_t(x+y,-)\n\\] を満たす確率核の半群 \\(\\{Q_t\\}_{t\\in\\mathbb{R}_+}\\subset B(\\mathcal{L}_b(\\mathbb{R}_+))\\) が定めるMarkov過程であることをいう．\n\n\nたしかに \\(Q_t(0,-)=\\delta_0\\;(t\\in\\mathbb{R}_+)\\) を満たす．\n\n\n2.2 分岐性\n\n\n\n\n\n\n命題7 （分岐性）\n\n\n\n\\(\\{X_t\\},\\{X'_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}_+)\\) を遷移半群 \\(\\{Q_t\\}\\subset B(\\mathcal{L}_b(\\mathbb{R}_+))\\) を共通とする，互いに独立な分岐過程とする． このとき，\\(\\{X_t+X'_t\\}\\) もやはり \\(\\{Q_t\\}\\) を遷移半群に持つ分岐過程である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(X_0,X_0'\\) の分布をそれぞれ \\(\\nu,\\nu'\\in P(\\mathbb{R})\\) とすると， \\(X_0\\perp\\!\\!\\!\\perp X_0'\\) より， \\(X_0+X_0'\\sim\\nu*\\nu'\\) である． すると，任意の \\(B\\in\\mathcal{B}(\\mathbb{R}_+)\\) に対して， \\[\\begin{align*}\n&\\operatorname{P}[X_t+X_t'\\in B]\\\\\n&=\\operatorname{E}[1_{\\left\\{X_t+X_t'\\in B\\right\\}}]\\\\\n&=\\operatorname{E}\\biggl[\\operatorname{E}[1_{\\left\\{X_t\\in B-x'\\right\\}}|X_t'=x']\\biggr]\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(x,B-x')\\nu(dx)\\operatorname{P}^{X_t'}(dx')\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(x,B-x')\\nu(dx)Q_t(y,dx')\\nu'(dy)\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(x+y,B)\\nu(dx)\\nu'(dy)\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(z,B)(\\nu*\\nu')(dz).\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n補題8 （畳み込み測度に関する積分）\n\n\n\n\\(f\\in L(\\mathbb{R}^d,\\nu_1*\\nu_2)\\) について， \\[\n\\int_{\\mathbb{R}^d}f(z)(\\nu_1*\\nu_2)(dz)=\\int_{\\mathbb{R}^d}f(x+y)\\nu_1(dx)\\nu_2(dy).\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nまず \\(f=1_B\\;(B\\in\\mathcal{B}(\\mathbb{R}^d))\\) という定義関数の場合は，定義からすぐに従う．一般の \\(f\\) については単関数近似による．\n\n\n\n\n\n2.3 Feller性\n\n\n\n\n\n\n命題9\n\n\n\n分岐過程 \\(\\{X_t\\}\\) の遷移半群 \\(\\{Q_t\\}\\) は\n\n任意 の\\(x&gt;0,t&gt;0\\) について，\\(Q_t(x,\\{0\\})&lt;1\\)．\n\\(Q_t(x,-)\\overset{\\text{d}}{\\to}\\delta_x(-)\\;(t\\to\\infty)\\)．\n\nを満たすとする．このとき，\\((Q_t)\\) はFeller半群である．\n\n\n\n\n\n\n\n\n証明"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html#footnotes",
    "href": "posts/2023/Processes/BranchingProcesses.html#footnotes",
    "title": "分岐過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの一般的な定義は (Quine & Szczotka, 1994) を参考にした．歴史は (Kendall, 1975), (Heyde & Seneta, 1977) が詳しい．(Bienaymé, 1845) が初めに考察をしたが，注目されなかった．(Watson & Galton, 1875) は注目されたが，数学的な議論には誤りが含まれていた．↩︎\n(Quine & Szczotka, 1994, p. 1208) 命題1．↩︎\n条件付き期待値のアトムの上での性質 も参照．↩︎\n(Schinazi, 2014, p. 19) 第2章定理1.1，(Bhattacharya & Waymire, 2021, p. 114) 定理9.1 など．↩︎\nこの \\(\\mu\\) を 基本再生産数 と呼ぶこともある Wikipedia．↩︎\n(Le Gall, 2016, p. 177) を参考にした．↩︎\n(Le Gall, 2016, p. 177) は branching property と呼んでいる．↩︎\n(Applebaum, 2009, p. 22) 命題1.2.2．↩︎\n(Le Gall, 2016, p. 177) 命題6.22．↩︎"
  },
  {
    "objectID": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html",
    "href": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html",
    "title": "条件付き正規分布からのシミュレーション法",
    "section": "",
    "text": "(Doucet, 2010) の内容に基づき，証明を与えながら，条件付き Gauss 分布の特定と，効率的なシミュレーション法を議論し，線型Gauss 状態空間モデルのフィルタリング（特に Ensemble Kalman filter）に応用する．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#sec-1",
    "href": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#sec-1",
    "title": "条件付き正規分布からのシミュレーション法",
    "section": "1 正規確率変数同士の条件付き分布",
    "text": "1 正規確率変数同士の条件付き分布\n\n\n\n\n\n\n命題：正規確率変数同士の条件付き分布\n\n\n\n\\[Z=(X,Y)\\sim\\mathop{\\mathrm{N}}_n(m,\\Sigma)\\] \\[m=\\begin{pmatrix}m_x\\\\m_y\\end{pmatrix},\\qquad\\Sigma=\\begin{pmatrix}\\Sigma_{xx}&\\Sigma_{xy}\\\\\\Sigma_{xy}^\\top&\\Sigma_{yy}\\end{pmatrix}\\] で，共分散行列は正則 \\(\\Sigma\\in\\mathrm{GL}_n(\\mathbb{R})\\) とする．このとき， \\[X|Y=y\\sim\\mathop{\\mathrm{N}}_{n_x}(m_{x|y},\\Sigma_{x|y}),\\] \\[m_{x|y}=m_x+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-m_y),\\] \\[\\Sigma_{x|y}=\\Sigma_{xx}-\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{xy}^\\top.\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(Z\\)　の密度 \\[\n\\begin{align*}\n    &\\frac{1}{(2\\pi)^{n/2}(\\det\\Sigma)^{1/2}}\\\\\n    &\\quad\\times\\exp\\left(-\\frac{1}{2}(z-m)^\\top\\Sigma^{-1}(z-m)\\right)\n\\end{align*}\n\\] を，\\(Y\\) の密度との積で表した際，残った因子が \\(X|Y\\) の密度となる． \\(\\exp\\) の中身に注目する．Schur 補行列を \\(S:=\\Sigma_{xx}-\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{xy}^\\top\\) とすると， \\[\n\\Sigma^{-1}=\\begin{pmatrix}S^{-1}&-S^{-1}T\\\\-T^\\top S^{-1}&\\Sigma_{yy}^{-1}+T^\\top S^{-1}T\\end{pmatrix},\n\\] \\[\nT:=\\Sigma_{xy}\\Sigma_{yy}^{-1},\n\\] であるから， \\(\\eta:=T(y-m_y)=\\Sigma_{xx}\\Sigma_{yy}^{-1}(y-m_y)\\) とおくと， \\[\n\\begin{align*}\n    &\\quad(z-m)^\\top\\Sigma^{-1}(z-m)\\\\\n    &=(x-m_x)^\\top S^{-1}(x-m_x)\\\\\n    &\\qquad\\quad-(y-m_y)^\\top T^\\top S^{-1}(x-m_x)\\\\\n    &\\qquad\\quad-(x-m_x)^\\top S^{-1}T(y-m_y)\\\\\n    &\\qquad\\quad+(y-m_y)^\\top T^\\top S^{-1}T(y-m_y)\\\\\n    &\\qquad\\quad+(y-m_y)^\\top\\Sigma_{yy}^{-1}(y-m_y)\\\\\n    &=\\biggr((x-m_x)^\\top-\\eta^\\top\\biggl)S^{-1}(x-m_x)\\\\\n    &\\qquad\\quad-\\biggr((x-m_x)^\\top+\\eta^\\top\\biggl)S^{-1}\\eta\\\\\n    &\\qquad\\quad+(y-m_y)^\\top\\Sigma_{yy}^{-1}(y-m_y)\\\\\n    &=\\biggr((x-m_x)^\\top-\\eta^\\top\\biggl)S^{-1}\\biggr((x-m_x)-\\eta\\biggl)\\\\\n    &\\qquad\\qquad+(y-m_y)^\\top\\Sigma_{yy}^{-1}(y-m_y).\n\\end{align*}\n\\] 以上より，平均は \\(m_{x|y}=m_x+\\eta\\) で，共分散行列は \\(\\Sigma_{x|y}=S\\) ．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#sec-2",
    "href": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#sec-2",
    "title": "条件付き正規分布からのシミュレーション法",
    "section": "2 条件付き分布からのシミュレーション",
    "text": "2 条件付き分布からのシミュレーション\n\n\n\n\n\n\n条件付き分布からのシミュレーション\n\n\n\n条件付き確率変数 \\(X|Y=y\\) のシミュレーションは，条件付き共分散行列 \\(\\Sigma_{x|y}\\) の Cholesky 分解 \\(\\Sigma_{x|y}=\\sqrt{\\Sigma_{x|y}}\\left(\\sqrt{\\Sigma_{x|y}}\\right)^\\top\\) を用いて， \\[\\overline{X}=m_{x|y}+\\sqrt{\\Sigma_{x|y}}U,\\] \\[U\\sim\\mathop{\\mathrm{N}}_{n_x}(0,I_{n_x})\\] によって行うのも直接的だが， \\(n_x\\) の次元が大きすぎる場合，Cholesky 分解の計算がネックとなる．そのような場合は， \\[\\overline{X}=X+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-Y),\\] \\[Z=\\begin{pmatrix}X\\\\Y\\end{pmatrix}\\sim\\mathop{\\mathrm{N}}_n(m,\\Sigma),\\] というアルゴリズムを用いることが出来る (Hoffman & Ribak, 1991)．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\overline{X}\\) は Gauss 確率変数の線型変換だからやはり Gauss である．よって，平均と分散が \\(X|Y\\) に一致することを示せば良い． 次のように \\(\\overline{X}\\) を書き換えることが出来る： \\[\\begin{align*}\n    \\overline{X}&=X+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-Y)\\\\\n    &=X+\\biggr(m_x+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-m_y)\\biggl)\\\\\n    &\\qquad+\\biggr(m_x+\\Sigma_{xy}\\Sigma_{yy}^{-1}(Y-m_y)\\biggl)\\\\\n    &=X+m_{x|y}-\\operatorname{E}[X|Y]\n\\end{align*}\\] これより，\\(\\operatorname{E}[\\overline{X}|Y]=m_{x|y}\\)．よって， \\[\\operatorname{E}[\\overline{X}]=\\operatorname{E}[\\operatorname{E}[\\overline{X}|Y]]=m_{x|y}.\\] 続いて， \\[\n\\begin{align*}\n    \\mathrm{V}[\\overline{X}|Y]&=\\mathrm{V}[X-\\operatorname{E}[X|Y]|Y]\\\\\n    &=\\operatorname{E}[(X-\\operatorname{E}[X|Y])^2|Y]\\\\\n    &=\\mathrm{V}[X|Y]=\\Sigma_{x|y}\n\\end{align*}\n\\] より，全分散の公式から \\[\n\\begin{align*}\n\\mathrm{V}[\\overline{X}]&=\\operatorname{E}[\\mathrm{V}[\\overline{X}|Y]]+\\underbrace{\\mathrm{V}[\\operatorname{E}[\\overline{X}|Y]]}_{=0}\\\\\n&=\\Sigma_{x|y}.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#応用-ensemble-kalman-filter",
    "href": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#応用-ensemble-kalman-filter",
    "title": "条件付き正規分布からのシミュレーション法",
    "section": "3 応用 Ensemble Kalman filter",
    "text": "3 応用 Ensemble Kalman filter\nまた，Ensemble Kalman filter はこの手法の応用と理解することができ，この手法の別の応用として FFBS (Forward Filtering Backward Sampling) アルゴリズムを代替するサンプリングアルゴリズムを得ることが出来ることも論じている．\n線型Gaussな状態空間モデル \\[\n\\begin{cases}\nX_n=A_nX_{n-1}+a_n+W_n&n\\ge 1,\\\\\nY_n=B_nX_n+b_n+V_n,&n\\ge0.\n\\end{cases}\n\\] \\[\nW_n\\sim\\mathop{\\mathrm{N}}_p(0,R^w_n),\\quad V_n\\sim\\mathop{\\mathrm{N}}_q(0,R_n^v),\n\\]\nの最適な一段階予測推定量 \\[\n\\eta_n:=\\mathcal{L}[X_n|(Y_0,\\cdots,Y_{n-1})]\n\\] も，フィルタリング推定量 \\[\n\\widehat{\\eta}_n:=\\mathcal{L}[X_n|(Y_0,\\cdots,Y_n)]\n\\] も Gauss 確率変数で，平均と分散は 節 1 の命題の繰り返し適用によって計算できる．これを Kalman filter という．1\n\n3.1 EnKF\nしかし，状態空間（\\(X_n\\) の値域）の次元が大きすぎる場合，節 2 で述べた理由と同様の理由で，Kalman gain の行列計算が実行不可能になる．\nこのステップを，粒子平均によって代替する粒子法が Ensemble Kalman filter であり，前述の障碍が典型的に生じてきた地球科学・海洋科学の分野で発展してきた (Evensen, 1994)．この方法では， 節 2 のサンプリングトリックを用いて，再帰的にフィルタリング分布と予測分布を近似していく．\n\n\n3.2 FFBS\nまた，線型 Gauss 状態空間モデルのハイパーパラメータの推定が必要な場合などでは，Feynman-Kac 分布 \\(p(x_{0:n}|y_{1:n})\\) からのサンプリングが必要になる．\n典型的には FFBS (Foward Filtering Backward Sampling) などの方法が知られている．これは \\(p(x_{0:n}|y_{1:n})\\) がある Markov 連鎖の見本道の分布に一致することに基づき，その後ろ向き核による分解から，\n\n前向きにフィルタリング分布と予測分布を計算する再帰的アルゴリズムを実行する．\n２つの分布から後ろ向き核を計算する．\n後ろ向き核を用いて， \\(X_n\\sim p(x_n|y_{1:n})\\) を後ろ向きにサンプリングしていく．\n\nと実行する方法である．2\n一方で， 節 2 のテクニックで次のようにしてサンプリングすることもできる．\n\n前向きに \\(\\operatorname{E}[X_{0:n}|Y_{1:n}], \\operatorname{E}[X_{0:n}|Y_{1:n}=y_{1:n}]\\) を計算する．\n次をサンプリングする： \\[\n\\begin{align*}\n\\overline{X}_{0:n}&:=\\operatorname{E}[X_{0:n}|Y_{1:n}=y_{1:n}]\\\\\n&\\qquad+X_{0:n}-\\operatorname{E}[X_{0:n}|Y_{1:n}]\n\\end{align*}\n\\]\n\n(Durbin & Koopman, 2002) では，多くの場合 \\(R^w_n\\) のランクが低いことに注目して， \\(\\operatorname{E}[W_{1:n}|Y_{1:n}]\\) を計算して， \\(p(x_0,w_{1:n}|y_{1:n})\\) からサンプリングすることを提唱している．\n\n\n3.3 その他\n(Doucet, 2010) は他にも，時空間統計 (Cressie, 1993) と機械学習 (Rasmussen & Williams, 2006) などで生じる Gauss 過程への応用で役に立ち得るのではないかと示唆している．\nCard"
  },
  {
    "objectID": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#footnotes",
    "href": "posts/2023/Probability/条件付き正規分布からのシミュレーション法.html#footnotes",
    "title": "条件付き正規分布からのシミュレーション法",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Del Moral & Penev, 2014) p.280 など参照．↩︎\n(Chopin & Papaspiliopoulos, 2020) 5.4.4節 p.63 など参照．↩︎"
  },
  {
    "objectID": "posts/2023/Probability/独立性.html",
    "href": "posts/2023/Probability/独立性.html",
    "title": "正規標本の標本平均と標本分散が独立であることの証明",
    "section": "",
    "text": "次の命題の証明を与える．\nなお，この性質は正規分布を特徴付ける (Kawata & Sakamoto, 1949)．"
  },
  {
    "objectID": "posts/2023/Probability/独立性.html#sec-1",
    "href": "posts/2023/Probability/独立性.html#sec-1",
    "title": "正規標本の標本平均と標本分散が独立であることの証明",
    "section": "1 Helmert変換による証明",
    "text": "1 Helmert変換による証明\n最も直接的で，示唆も深い． (竹村彰道, 2020) 第4.3節 pp.69-70 も参照．\n\n\n\n\n\n\n定義\n\n\n\n次のように定まる行列 \\(\\mathbb{H}\\in M_{N+1}(\\mathbb{R})\\) を Helmert行列 という．2 最初の行を \\[\\mathbb{H}_{1,j}:=\\frac{1}{\\sqrt{N+1}},\\qquad 1\\le j\\le N+1,\\] とし，それ以下の行を \\[\\mathbb{H}_{i,j}:=\\overline{\\mathbb{H}}_{i,j}:=\\begin{cases}\n\\frac{1}{\\sqrt{i(i-1)}}&1\\le j&lt;i,\\\\\n-\\frac{i-1}{\\sqrt{i(i-1)}}&j=i,\\\\\n0&i&lt;j\\le N+1.\n\\end{cases}\\] と定める．このとき， \\[\\mathbb{H}=\\begin{pmatrix}\\frac{\\boldsymbol{1}_{N+1}^\\top}{\\sqrt{N+1}}\\\\\\overline{\\mathbb{H}}\\end{pmatrix}\\] と表せる．3\n\n\n\n\n\n\n\n\n補題\n\n\n\nHelmert行列 \\(\\mathbb{H}\\in M_{N+1}(\\mathbb{R})\\) とその部分行列 \\(\\overline{\\mathbb{H}}\\in M_{N,N+1}(\\mathbb{R})\\) について，\n\n直交行列である．\n次を満たす：\\[\\overline{\\mathbb{H}}^\\top\\overline{\\mathbb{H}}=I-\\frac{1}{N+1}J=\\epsilon.\\]\n\nただし，次のように定めた： \\[\n\\epsilon:=I_{N+1}-\\frac{J_{N+1}}{N+1},\\qquad J_{N+1}:=\\boldsymbol{1}_{N+1}\\boldsymbol{1}_{N+1}^\\top.\n\\]\n\n\\(x,y\\in\\mathbb{R}^{N+1}\\) に対して， \\(x^\\top\\epsilon y=(x-\\overline{x}\\boldsymbol{1}_{N+1})^\\top(y-\\overline{y}\\boldsymbol{1}_{N+1})\\)\n\n\n\n\n\n\n\n\n\n証明（補題）\n\n\n\n\\(\\mathbb{H}\\) を具体的に書けば，\n\\[\n\\mathbb{H}:=\\begin{pmatrix}\\frac{1}{\\sqrt{N+1}}&\\frac{1}{\\sqrt{N+1}}&\\frac{1}{\\sqrt{N+1}}&\\frac{1}{\\sqrt{N+1}}&\\cdots&\\cdots&\\frac{1}{\\sqrt{N+1}}\\\\\n\\frac{1}{\\sqrt{2}}&-\\frac{1}{\\sqrt{2}}&0&0&\\cdots&\\cdots&0\\\\\n\\frac{1}{\\sqrt{6}}&\\frac{1}{\\sqrt{6}}&-\\frac{2}{\\sqrt{6}}&0&\\cdots&\\cdots&0\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\ddots&\\vdots\\\\\n\\frac{1}{\\sqrt{k(k-1)}}&\\cdots&\\frac{1}{\\sqrt{k(k-1)}}&\\frac{1-k}{\\sqrt{k(k-1)}}&0&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots&\\vdots&\\ddots&\\ddots&\\vdots\\\\\n\\frac{1}{\\sqrt{N(N+1)}}&\\cdots&\\cdots&\\cdots&\\cdots&\\frac{1}{\\sqrt{N(N+1)}}&\\frac{N}{\\sqrt{N(N+1)}}\n\\end{pmatrix}\n\\]\n\n\\(\\mathbb{H}\\) の任意の行は正規化されており，異なる行の間の内積は必ず零になることはすぐに判る．よって， \\(\\mathbb{H}\\mathbb{H}^\\top=I\\)．列についても同様であることが， \\[\n\\frac{1}{N+1}+\\sum_{k=1}^N\\frac{1}{k(k+1)}=1\n\\] に注意すれば同様に判る．よって，\n\n\\[\n\\mathbb{H}\\mathbb{H}^\\top=I=\\mathbb{H}^\\top\\mathbb{H}=\\frac{1}{N+1}J+\\overline{\\mathbb{H}}^\\top\\overline{\\mathbb{H}}.\n\\]\n\n1.の最後の等式から従う．なお，1.の最後の等式は次のように判る：\n\n\\[\n\\left(\\frac{\\boldsymbol{1}_{N+1}}{\\sqrt{N+1}}\\;\\overline{\\mathbb{H}}^\\top\\right)\\begin{pmatrix}\\frac{\\boldsymbol{1}_{N+1}^\\top}{\\sqrt{N+1}}\\\\\\overline{\\mathbb{H}}\\end{pmatrix}=\\frac{\\boldsymbol{1}_{N+1}\\boldsymbol{1}_{N+1}^\\top}{N+1}+\\overline{\\mathbb{H}}^\\top\\overline{\\mathbb{H}}.\n\\]\n\n\\(\\epsilon=I_{N+1}-\\frac{\\boldsymbol{1}_{N+1}\\boldsymbol{1}_{N+1}^\\top}{N+1}\\) を具体的に書けば \\[\n\\epsilon=\\begin{pmatrix}\n\\frac{N}{N+1}&-\\frac{1}{N+1}&-\\frac{1}{N+1}&\\cdots&-\\frac{1}{N+1}\\\\\n-\\frac{1}{N+1}&\\frac{N}{N+1}&-\\frac{1}{N+1}&\\cdots&-\\frac{1}{N+1}\\\\\n\\vdots&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n-\\frac{1}{N+1}&\\cdots&\\cdots&-\\frac{1}{N+1}&\\frac{N}{N+1}\n\\end{pmatrix}\n\\]\n\nとなるから， \\[\n\\begin{align*}\nx^\\top\\epsilon y&=\\frac{1}{N+1}(x_1\\;\\cdots\\;x_{N+1})\\begin{pmatrix}(N+1)y_1-\\sum_{i=1}^{N+1}y_i\\\\\\vdots\\\\(N+1)y_{N+1}-\\sum_{i=1}^{N+1}y_i\\end{pmatrix}\\\\\n&=\\frac{1}{N+1}\\left((N+1)\\sum_{i=1}^{N+1}x_iy_i-\\left(\\sum_{i=1}^{N+1}x_i\\right)\\left(\\sum_{i=1}^{N+1}y_i\\right)\\right)\\\\\n&=\\sum_{i=1}^{N+1}x_iy_i-(N+1)\\overline{x}\\cdot\\overline{y}\\\\\n&=\\sum_{i=1}^{N+1}(x_iy_i-\\overline{x}\\overline{y})\\\\\n&=\\sum_{i=1}^{N+1}(x_i-\\overline{x})(y_i-\\overline{y})\\\\\n&=(x-\\boldsymbol{1}_{N+1}\\overline{x})^\\top(y-\\boldsymbol{1}_{N+1}\\overline{y}).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(\\mu=0,\\sigma^2=1\\) と仮定して示せば， 一般の \\(X_i\\) に対しても \\(\\frac{X_i-\\mu}{\\sigma}\\sim\\mathop{\\mathrm{N}}(0,1)\\) に対して同様の議論をすることで一般の場合の結果も得る．\n\\[X_{1:N+1}:=\\begin{pmatrix}X_1\\\\\\vdots\\\\X_{N+1}\\end{pmatrix}\\sim\\mathop{\\mathrm{N}}_{N+1}(0,I_{N+1})\\]\nに対して， \\(Y_{1:N+1}:=\\mathbb{H}X_{1:N+1}\\) と定めると， \\(\\mathbb{H}\\) は直交行列だからやはり \\(Y\\sim\\mathop{\\mathrm{N}}_{N+1}(0,I_{N+1})\\)．加えて，\\(\\mathbb{H}\\) の構成から \\[\nY_0=\\frac{\\boldsymbol{1}_{N+1}^\\top}{\\sqrt{N+1}}X_{1:N+1}=\\sqrt{N+1}\\cdot\\overline{X}\n\\] が成り立っている．\n補題の2.と3.から， \\(Y_{2:N+1}=\\overline{\\mathbb{H}}X_{1:N+1}\\) に注意して， \\[\n\\begin{align*}\n\\|Y_{2:N+1}\\|^2&=(\\overline{\\mathbb{H}}X_{1:N+1})^\\top(\\overline{\\mathbb{H}}X_{1:N+1})\\\\\n&=X_{1:N+1}^\\top(\\overline{\\mathbb{H}}^\\top\\overline{\\mathbb{H}})X_{1:N+1}\\\\\n&=X_{1:N+1}^\\top\\epsilon X_{1:N+1}=\\|X_{1:N+1}-\\overline{X}\\boldsymbol{1}_{N+1}\\|^2\\\\\n&=\\sum_{i=1}^{N+1}(X_i^2-\\overline{X}^2)\\\\\n&=\\sum_{i=1}^{N+1}(X_i-\\overline{X})^2=NU^2.\n\\end{align*}\n\\]\n以上より， \\(\\overline{X}\\) は \\(Y_1\\) のみの関数で， \\(S^2,U^2\\) は \\(Y_{2:N+1}\\) のみの関数であるから，互いに独立である．"
  },
  {
    "objectID": "posts/2023/Probability/独立性.html#basuの定理による証明",
    "href": "posts/2023/Probability/独立性.html#basuの定理による証明",
    "title": "正規標本の標本平均と標本分散が独立であることの証明",
    "section": "2 Basuの定理による証明",
    "text": "2 Basuの定理による証明\n\n\n\n\n\n\n(Basu, 1955)\n\n\n\n\\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を分布族， \\((\\mathcal{X},\\mathcal{A}),(\\mathcal{T},\\mathcal{B}),(\\mathrm{V},\\mathcal{C})\\) を可測空間とする． \\(T:\\mathcal{X}\\to\\mathcal{T}\\) を \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) の完備十分統計量，統計量 \\(V:\\mathcal{X}\\to\\mathrm{V}\\) の分布 \\(P^V_\\theta\\) は \\(\\theta\\) に依らないとする．4 このとき，任意の \\(\\theta\\in\\Theta\\) に対して，\\(T\\) と \\(V\\) は独立である： \\[P_\\theta[T\\in A,V\\in B]=P_\\theta[T\\in A]P_\\theta[V\\in B]\\qquad(A\\in\\mathcal{B},B\\in\\mathcal{C},\\theta\\in\\Theta)\\]\n\n\n\n\n\n\n\n\n証明（Basuの定理）\n\n\n\n仮定より，\\(p_B:=P_\\theta[V\\in B]\\in\\mathbb{R},q_B(T):=P_\\theta[V\\in B|T]:\\mathcal{X}\\to\\mathbb{R}\\) は \\(\\theta\\in\\Theta\\) に依らない． これに対して，条件付き期待値の性質から \\[p_B=E_\\theta[1_B(V)]=E_\\theta[E_\\theta[1_B(V)|T]]=E_\\theta[q_B(T)]\\] であるから，\\(E_\\theta[p_B-q_B(T)]=0\\) が従う． 完備性から，\\(P_\\theta[p_B=q_B(T)]=1\\)．よって，任意の \\(\\theta\\in\\Theta\\) について， \\[\\begin{align*}\n    P_\\theta[T\\in A,V\\in B]&=E_\\theta[1_A(T)1_B(V)]\\\\\n    &=E_\\theta[1_A(T)E_\\theta[1_B(V)|T]]\\\\\n    &=E_\\theta[1_A(T)q_B(T)]\\\\\n    &=E_\\theta[1_A(T)p_B]\\\\\n    &=E_\\theta[1_A(T)]p_B\\\\\n    &=P_\\theta[T\\in A]P_\\theta[V\\in B].\n\\end{align*}\\]\n\n\nこれを用いて，次のように証明できる．\n\n\n\n\n\n\n証明\n\n\n\n\n標本平均は平均の完備十分統計量である\n標本分散は平均の補助統計量である\n\nの2点を示せば，Basuの定理から，標本平均と標本分散は独立である：\\(\\overline{X}\\perp\\!\\!\\!\\perp S^2\\)． 同様にして，標本平均と不偏分散も独立である．\n\n分布族 \\(\\{\\mathop{\\mathrm{N}}(\\mu,\\sigma^2)^{\\otimes n}\\}_{\\mu\\in\\mathbb{R}}\\) は指数型であり， 統計量 \\[T_1(x):=\\sum_{i\\in[n]}x_i=n\\overline{X}\\] は \\(\\mu\\) の完備十分統計量である．\n標本分散の分布は \\[S^2:=\\frac{1}{n}\\sum_{i\\in[n]}(X_i-\\overline{X})^2\\sim\\chi^2(n-1)\\] より，パラメータ\\(\\mu\\in\\mathbb{R}\\)に依らない．"
  },
  {
    "objectID": "posts/2023/Probability/独立性.html#fisher-cochranの定理の考え方",
    "href": "posts/2023/Probability/独立性.html#fisher-cochranの定理の考え方",
    "title": "正規標本の標本平均と標本分散が独立であることの証明",
    "section": "3 Fisher-Cochranの定理の考え方",
    "text": "3 Fisher-Cochranの定理の考え方\n総合研究大学院大学統計科学コース2021年8月実施の入試問題の第三問にて，本命題を背景とした問題が出題された．このアプローチは 節 1 の証明法を別の角度から見れる．5\n\n\n\n\n\n\n補題\n\n\n\n\\[X_{1:n}=\\begin{pmatrix}X_1\\\\\\vdots\\\\X_n\\end{pmatrix},\\qquad X_i\\overset{\\text{iid}}{\\sim}\\mathop{\\mathrm{N}}(\\mu,\\sigma^2),\\] をGauss確率ベクトル，\\(B\\in M_{mn}(\\mathbb{R}),A\\in M_n(\\mathbb{R})\\) を対称行列とする．\\(BA=O_{m,n}\\) のとき，2つの確率変数 \\(BX_{1:n}\\) と \\(X^\\top_{1:n} AX_{1:n}\\) とは独立になる．\n\n\n\n\n\n\n\n\n証明（補題）\n\n\n\n\\(A\\) は対称行列だから，ある直交行列 \\(U\\in \\mathrm{O}_n(\\mathbb{R}),U^\\top U=I_n\\) を用いて，\\(U^\\top DU=A\\) と対角化出来る．ただし， \\(D:=\\mathrm{diag}(a_1,\\cdots,a_r)\\in M_n(\\mathbb{R}),r:=\\mathop{\\mathrm{\\mathrm{rank}}}A\\) は対角行列とした． よって，\\(Y_{1:n}:=UX_{1:n}\\) と定めると，これは再び成分が互いに独立な正規確率変数のベクトル \\(Y_{1:n}\\sim\\mathop{\\mathrm{N}}_n(\\mu U1_n,\\sigma^2I_n)\\) で， \\[X_{1:n}^\\top AX_{1:n}=(UX_{1:n})^\\top D(UX_{1:n})=a_1Y_1^2+\\cdots+a_rY_r^2,\\] と表せる．\n次に，\\(BA=0\\) より，\\(\\mathrm{Im}\\,A\\subset\\mathrm{Ker}\\;B\\)，従って双方の直交補空間を考えると \\(\\mathrm{Im}\\,B\\subset\\mathrm{Ker}\\;A\\) でもあるから，\\(BX_{1:n}\\) は \\(y_{r+1},\\cdots,y_{n}\\) のみによって表せる確率変数のベクトルである（使わないものも許す）． よって，\\(BX_{1:n}\\) と \\(X_{1:n}^\\top AX_{1:n}\\) は独立．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(m=1,B:=\\frac{1}{N+1}1_{N+1}^\\top\\) と \\[A:=N\\epsilon=\\frac{N}{N+1}\\begin{pmatrix}\nN&-1&-1&\\cdots&-1\\\\\n-1&N&-1&\\cdots&-1\\\\\n\\vdots&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n-1&\\cdots&\\cdots&-1&N\n\\end{pmatrix}\\in M_{N+1}(\\mathbb{R})\\] と定めると，\\(BA=O\\) であり，同時に \\(\\overline{X}=BX_{1:N+1}\\) かつ \\(U^2=X^\\top_{1:N} AX_{1:N}\\) である．"
  },
  {
    "objectID": "posts/2023/Probability/独立性.html#footnotes",
    "href": "posts/2023/Probability/独立性.html#footnotes",
    "title": "正規標本の標本平均と標本分散が独立であることの証明",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(U^2\\) は不偏分散と呼ばれる統計量である．代わりに標本分散 \\(S^2:=\\frac{1}{N+1}\\sum_{i=1}^{N+1}(X_i-\\overline{X})^2\\) を考えても同様の主張 \\(\\overline{X}\\perp\\!\\!\\!\\perp S^2\\) を得る．↩︎\n(Del Moral & Horton, 2023) も参照．↩︎\n\\(\\boldsymbol{1}_{N+1}\\) は \\(1\\) のみを成分に持つ \\(\\mathbb{R}^{N+1}\\) の元， \\(\\overline{\\mathbb{H}}\\) は行列 \\(\\overline{\\mathbb{H}}:=(\\overline{\\mathbb{H}}_{i,j})_{2\\le i\\le N+1,1\\le i\\le N+1}\\) とした．↩︎\nこのような性質を満たす統計量 \\(V\\) を分布族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) の補助統計量という．↩︎\n過去9年分の入試問題の解答はこちらから↩︎"
  },
  {
    "objectID": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html",
    "href": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html",
    "title": "数学者のためのカーネル法概観",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#導入",
    "href": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#導入",
    "title": "数学者のためのカーネル法概観",
    "section": "1 導入",
    "text": "1 導入\nカーネル法はデータ解析の観点から見ると，非線型な前処理を施すことで，その後の線型なデータ解析の性能を向上させるための手法であると言える．\nこれはニューラルネットワークと比較すると，カーネル関数を適応的に決められない一方で，可算無限個の基底を用いた無限次元の特徴空間上の表現を得ることが出来るという相違点がある．\n\n1.1 データの非線型変換としてのカーネル法\n線型手法が使えないときに，変数変換を挟むことで線型分離可能な問題に還元するという手法は広く見られる．\n\n1.1.1 Box-Cox 変換\n例えば，データが正規分布から大きく違っている場合，Box-Cox 変換 (Box & Cox, 1964)\n\\[\nx \\longmapsto x^{(\\lambda)} = \\begin{cases}\n\\displaystyle \\frac{x^\\lambda - 1}{\\lambda} & \\lambda \\neq 0\\\\\n\\log x & \\lambda = 0\\end{cases}\n\\]\nを通じて正規分布に近づけることが出来る．\\(\\lambda\\in\\mathbb{R}\\) をハイパーパラメータとしてデータから調整出来る．\nBox-Cox 変換は特定のリンク関数 \\(G\\) に関する一般化線型モデルと見れる．1\n\n\n1.1.2 カーネル法\nBox-Cox 変換はカーネル法もこのような非線形変換を見つけてくるための統一的な方法論である．\nカーネル法によってデータを非線型変換をして得る空間は 特徴空間 と呼ばれ，この上で従来の線型なデータ解析を施すだけで，全体としては非線型な手法の完成である．\n\n\n\n1.2 カーネル法の歴史2\n1992年，当時は2023年に生きる我々と全く同じく，ニューラルネットワーク（以降NN）のブームのさなかにあった（当時は第二期ブーム）．このブームを一度終わらせたのが，(Boser et al., 1992) によるカーネル法であった．\n当時は数学の範疇から出たことがなかったカーネルの概念を用いて，SVMを非線形化したKernel SVMという手法を提案したのである．以降，非線形問題を扱えるモデルとして，NNを凌ぐ勢いで発展し，NNがvanishing gradientsという障壁にぶつかったこともあり，2000年から2006年を「深層学習の冬」とまで言わしめた．\n\n\n1.3 カーネル法の課題\n2006年というのは，(Hinton et al., 2006), (Hinton & Salakhutdinov, 2006) の年である．この自己符号化器の発表がきっかけになり，DNNの訓練がますます効率的になり，一方でKernel SVMは次の2つの障壁に直面しており，現在のDNN最強の時代を我々は見ている．3\n\n種々のタスクに対して，最適なカーネルが何かがわかっていない．\nデータ数が多すぎると実行不可能になる．\n\nカーネル法のトリックは単純であるから，個々の問題に即したカーネルの選び方，または適応的にカーネルを定めるアルゴリズムのデザインが，今後の課題である．"
  },
  {
    "objectID": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#カーネル法の数理",
    "href": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#カーネル法の数理",
    "title": "数学者のためのカーネル法概観",
    "section": "2 カーネル法の数理",
    "text": "2 カーネル法の数理\n\n2.1 カーネル法の骨格\nカーネルと言ったとき，数学的には「（実数値の）半正定値対称関数」を指す．\n\n\n\n\n\n\n定義：（実数値の）正定値カーネル\n\n\n\n関数 \\(k:\\Omega\\times\\Omega\\to\\mathbb{R}\\) が正定値カーネルであるとは，次の２条件を満たすことをいう：\n\n対称性：\\(k(x,y)=k(y,x)\\)．\n正値性：任意有限個の点 \\(x_1,\\cdots,x_n\\in\\Omega\\) に対し，行列 \\(\\bigl(k(x_i,x_j)\\bigr)^n_{i,j=1}\\) は半正定値：4 \\[\n  \\sum_{i,j=1}^nc_ic_jk(x_i,x_j)\\ge0,\\qquad c_i\\in\\mathbb{R}.\n\\]\n\n\n\n\n\n\n\n\n\n例：\\(\\Omega=\\mathbb{R}^d\\) 上の正定値カーネル\n\n\n\n\nEuclid内積：\\(k(x,y)=x^\\top y\\)．\nGaussカーネル：\\(k_G(x,y)=\\exp\\left(-\\frac{1}{2\\sigma^2}\\|x-y\\|^2\\right)\\;(\\sigma&gt;0)\\)．\nLaplaceカーネル：\\(k_L(x,y)=\\exp\\left(-\\alpha\\sum_{a=1}^d|x_a-y_a|\\right)\\;(\\alpha&gt;0)\\)．\n多項式カーネル：\\(k_P(x,y)=(c+x^\\top y)^d\\;(c\\ge0,d\\in\\mathbb{N})\\)．\n\n\n\nこの「正定値カーネル」の概念は，次の意味で，「内積」と同一視できる．「内積」と同一視できるという意味で，「類似度の測り方」に対応する．\n\n\n\n\n\n\n定理：Moore-Aronszajn 1950\n\n\n\n任意の集合 \\(\\Omega\\) 上の正定値カーネル \\(k\\) に対して，\\(\\Omega\\) 上の関数からなるHilbert空間 \\(H_k\\) であって，以下を満たすものが一意に定まる：5\n\n\\(k(-,x)\\in H_k\\;(\\forall_{x\\in\\Omega})\\)．\n（再生性）\\((f\\,|\\,k(-,x))_{H_k}=f(x)\\)．\n\nこのHilbert空間 \\(H_k\\) を数学では \\(k\\)-再生核Hilbert空間といい，データ解析では \\(k\\)-特徴空間という．\n\n\nここで，数学概念について，少し突飛に思えるかもしれないが，次の名前をつける．\n\n\n\n\n\n\n定義\n\n\n\n正定値カーネル \\(k:\\Omega\\to\\mathbb{R}\\) について，\n\n\\(x\\mapsto k(-,x)\\) という対応 \\(\\Phi:\\Omega\\to H_k\\) を特徴写像という．\n\\(\\Phi\\) は「内積を保つ」が，この性質をカーネルトリックという： \\[\n\\left(\\Phi(x)\\,\\middle|\\,\\Phi(y)\\right)_{H_k}=k(x,y).\n\\]\n\n\n\n「特徴写像」は，データ \\(x,y\\in\\Omega\\) を正定値カーネルが測る「類似度」を変えないように，しかしながら全く違う空間内の点 \\(\\Phi(x),\\Phi(y)\\in H_k\\) に写している．「類似度」が変わっていないことを「カーネルトリック」と呼ぶ．\nこの「トリック」は少し 米田埋め込み に似ている．データ \\(x\\in\\Omega\\) の他のデータとの類似度の全体 \\(k(-,x):\\Omega\\to\\mathbb{R}\\) は，そのデータを特徴づけるのである \\(k(-,x)=\\Phi(x)\\in H_k\\)．\nまた，関数のなすHilbert空間 \\(H\\subset\\mathbb{R}^\\Omega\\) が， \\(\\{\\mathrm{ev}_x\\}_{x\\in\\Omega}\\subset H^*\\) を満たすならば，\\(H\\) は再生核を持つ．このような関数空間 \\(H\\) の内積の構造を \\(\\Omega\\) にも導入したいとき， \\(k\\) を通じてすれば良いということになる． \\(k\\) は違う \\(H_k\\) と \\(\\Omega\\) を対応づけ，正しい \\(k\\) を選ぶと，データ \\(\\{x_1,\\cdots,x_n\\}\\subset\\Omega\\) のうちなる「特徴」を暴き出せるかもしれない．\n\n\n2.2 カーネル法の強み\nこうして見たように，カーネル法は\n\n非線型な情報，特に高次モーメントの扱いができる．\nデータの次元 \\(X_i\\in\\mathbb{R}^p\\) に依らない．が，データ数 \\(N\\) に依存し，次元の呪いを受ける．\nデータの形式にも依らない．ベクトルでなくとも，グラフでも，行列でも，分布でも良い．"
  },
  {
    "objectID": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#種々のデータ解析のカーネル化",
    "href": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#種々のデータ解析のカーネル化",
    "title": "数学者のためのカーネル法概観",
    "section": "3 種々のデータ解析のカーネル化",
    "text": "3 種々のデータ解析のカーネル化\n\n3.1 データ解析のやり方\n\nカーネル \\(k\\) を用意する．すると，特徴空間 \\(H_k\\) が定まるが，これは一般に関数空間であり，無限次元である．\\(H_k\\) の元を「特徴ベクトル」と言ったりするのに，その正体は関数である．\nカーネルトリック（≒再生性）が，「特徴ベクトル同士の内積」だけを計算可能にする．\n\n要は計算できることは内積だけなのである！しかし，特徴写像や特徴ベクトルの表示を陽に使わずとも，内積だけで実行可能な線型データ解析は，実に多いのである．\n\n\n3.2 例：Ridge回帰\nRidge回帰は，次の最適化によって線型回帰係数 \\(a\\in\\mathbb{R}^p\\) を推定するロバスト手法である：\n\\[\n  \\min_{a\\in\\mathbb{R}^p}\\frac{1}{n}\\sum_{i=1}^n(Y_i-a^\\top X_i)^2+\\lambda\\|a\\|^2.\n\\]\nこれを「カーネル化する」とは，「特徴空間で実行する」ということである． \\(X_i\\) の代わりに \\(\\Phi(X_i)\\) 上で，Euclid内積 \\(a^\\top X_i\\) の代わりに特徴空間の内積 \\((f|\\Phi(X_i))_{H_k}\\) で実行するということである：\n\\[\n  \\min_{f\\in H_k}\\frac{1}{n}\\sum_{i=1}^n\\biggl(Y_i-(f|\\Phi(X_i))_{H_k}\\biggr)+\\lambda\\|f\\|^2_H\n\\]\n実はこの式は次と等価：\n\\[\n  \\min_{f\\in H_k}\\frac{1}{n}\\sum_{i=1}^n\\biggl(Y_i-f(X_i)\\biggr)+\\lambda\\|f\\|^2_H\n\\]\nたしかに，\\(f\\in H\\) は一般の関数であり，非線形な回帰を行なっていることになる！\nしかし，最後にこの最適化問題をどう解くか？という問題が残り，無限次元空間 \\(H\\) 上での最適化の理論が必要になるかといえばそうではなく，\n\\[\n  f=f_\\Phi:=\\sum_{i=1}^nc_i\\Phi(X_i)\n\\]\nという形のみで解を探せば良いことが判る．これは \\(f=f_\\Phi\\oplus f_\\perp\\) という直交分解を考えることで従う．つまり，最適化はデータ点 \\(\\Phi(X_1),\\cdots,\\Phi(X_n)\\) の張る有限次元部分空間上のみで考えれば良い．この事実にはRepresenter定理という仰々しい名前がついている．\nすると目的関数は\n\\[\n\\frac{1}{N}\\sum_{i=1}^N\\biggl(Y_i-\\sum_{j=1}^Nc_jk(X_i,X_j)\\biggr)^2+\\lambda\\underbrace{\\sum_{i,j=1}^Nc_ic_jk(X_i,X_j)}_{=c^\\top Kc}\n\\]\nとなり，これを解くと，カーネルRidge回帰の解は\n\\[\n\\widehat{f}(x)=\\boldsymbol{k}(x)^\\top(K+n\\lambda_nI_n)^{-1}\\boldsymbol{Y},\n\\]\n\\[\n\\boldsymbol{k}(x)=\\begin{pmatrix}k(x,X_1)\\\\\\vdots\\\\k(x,X_N)\\end{pmatrix},\\boldsymbol{Y}=\\begin{pmatrix}Y_1\\\\\\vdots\\\\Y_N\\end{pmatrix}\n\\]\nと表せることがわかる．\n\n\n3.3 発展\nここで，最初の節 節 1.2 で紹介した2つの問題点に戻る．\n\n種々のタスクに対して，最適なカーネルが何かがわかっていない．\nデータ数が多すぎると実行不可能になる．\n\nこの2.について，カーネルRidge回帰の例だと，逆行列 $\\((K+n\\lambda_nI_n)^{-1}\\) の計算が実行不可能になるという形で現前する．しかし，Woodburyの公式から，低ランク近似が得られていれば，それを活用できる．一般にGram行列の固有値の減衰は速いことが知られており，この低ランク近似の戦略は筋が良いと言える．\n1.について，まずSVMなどの教師あり学習の設定では，CVを使うことでカーネル選択をすることができる．が，教師なし学習では一般的な方法はない．特にカーネル主成分分析（次節の例）．しかし，これを適応的に学習するというのは良いアイデアだろう．Multiple Kernel Learning (Gönen & Alpaydin, 2011) はカーネルの凸結合を学習し，Deep Kernel Learning (Wilson et al., 2016) はNNによってカーネルを学習する．\n\n\n3.4 例：主成分分析\n主成分分析を抽象的に理解すれば，分散が大きい方向に射影をすることで，「意味がある方向」を見つける手法なのであった．\n主成分方向とは\n\\[\n\\max_{a\\in\\mathbb{R}^p:\\|a\\|=1}\\sum_{i=1}^N(a^\\top(X_i-\\overline{X}))^2.\n\\]\nの解 \\(a\\in\\mathbb{R}^p\\) である．これは分散共分散行列の固有値問題を解くことに等価になる．\nこの手法を「カーネル化」するには，特徴空間で実行すれば良い．\n\\[\n\\max_{f:\\|f\\|_H=1}\\sum_{i=1}^N\\biggl(f\\,\\bigg|\\,\\Phi(X_i)-\\overline{\\Phi}(X)\\biggr)^2\n\\]\nこの解も，データの特徴ベクトルの張る有限部分空間内で調べれば十分なのである！というのも，正確には，平均を引いた次の形のみを考えれば良いことが判る：\n\\[\nf=\\sum_{i=1}^Nc_i\\biggl(\\Phi(X_i)-\\overline{\\Phi(X)}\\biggr)\n\\]\n実際には，中心化Gram行列の固有値問題に帰着する：\n\\[\n\\widetilde{K}_{ij}=k(X_i,X_j)-\\frac{1}{N}\\sum_{b=1}^Nk(X_i,X_b)\\qquad\\qquad\n\\]\n\\[\n\\qquad\\qquad-\\frac{1}{N}\\sum_{a=1}^Nk(X_a,X_j)+\\frac{1}{n^2}\\sum_{a,b=1}^Nk(X_a,X_b).\n\\]\n\n\n3.5 例：SVM\nデータ \\(\\{x_1,\\cdots,x_n\\}\\subset\\mathbb{R}^p\\) が線型分離可能であるとき，ハードマージン法と呼ばれる手法を用いて，これを分離する最大マージン超平面 \\[\nH_\\mathrm{max}:=\\mathop{\\mathrm{arg\\,max}}_{H\\subset\\mathbb{R}^p}\\min_{1\\le i\\le n}d(x_i,H)\n\\] を，凸二次計画問題を解くことによって見つけることができる．このとき，最大のマージンを達成する \\[\nd(x_{j},H)=\\min_{1\\le i\\le n}d(x_i,H)\n\\] ときの \\(x_j\\) （複数あり得る）をサポートベクトル といい，これが解 \\(H_{\\text{max}}\\) を特徴付ける．\nこの問題において，特徴写像 \\(\\Phi:\\mathbb{R}^p\\to H_k\\) を考えても，やはり解を \\(n\\) 次元部分空間 \\[\nv\\in\\left\\{\\sum_{j=1}^nc_j\\Phi(x_j)\\in H_k\\;\\middle|\\;c_j\\in\\mathbb{R}\\right\\}\n\\] 上で考えれば良いから， \\[\n\\underset{v,\\gamma}{\\text{minimize}}\\quad\\|v\\|^2_{H_k}=\\sum_{i,j=1}^nc_ic_jk(x_i,x_j)\n\\] \\[\n\\text{subject to}\\quad\\lambda_i\\left(\\sum_{j=1}^nc_jk(x_i,x_j)+\\gamma\\right)\\ge1\\quad(i\\in[n])\n\\] という，やはり凸二次計画問題を解けば良い．\n\n\n3.6 総括\n\n\n\n\n\n\nまとめ\n\n\n\n典型的には，線型手法の目的関数が \\((\\Phi(X_i)|\\Phi(X_j)),(f|\\Phi(X_i))\\) で表現され，さらに解がデータ数の次元を持った有限次元部分空間で見つかる．その結果，Gram行列の解析に帰着し，データ数 \\(n\\) に依存するが，個々のデータの形式に依らない！データはベクトルでなく，カーネルが定義できさえすれば，確率分布自体でも問題がない．\n\n\n\n\n\nbook cover"
  },
  {
    "objectID": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#footnotes",
    "href": "posts/2023/KernelMethods/KernelMethods4Mathematicians.html#footnotes",
    "title": "数学者のためのカーネル法概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Wolfgang Härdle & Sperlich, 2004, p. 162)↩︎\n(Ghojogh et al., 2021) など参考．↩︎\n一方でここにきて，現代におけるDNNの最先端とも言えるTransformerをSVMとみなせる，という報告も出てきた (Tarzanagh et al., 2023)↩︎\nこのようにして構成される行列をGram行列と呼ぶ．↩︎\n(Aronszajn, 1950) 参照．↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "",
    "text": "統計的検定の考え方と，その科学的な態度については，(大塚淳, 2020, pp. 97–106) が大変含蓄が深い．\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#仮設検定",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#仮設検定",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "1 仮設検定",
    "text": "1 仮設検定\n\n1.1 二項モデルでの検定\n\n\n\n\n\n\n問題\n\n\n\n冠動脈バイパス手術を受けた20歳の青年が３日後に死亡した．\n\n同病院では過去３年の30件のうち10人が術後１週間以内に死亡している．\n一般に術後１週間以内に死亡する確率は0.2である．\n\n不審だと言えるだろうか？言えるとしたらどのような意味で？\n\n\n「正常な範囲内の事象である」とする帰無仮説の下で，当該事象が起こる確率を計算する．これが5%以下だったら「不審だと思うに足る」と言えるだろう．\n本問題は死亡率 \\(p\\in[0,1]\\) という母数に関する検定問題と捉えることができ，すると帰無仮説は \\[\nH_0:p=0.2\n\\] というシンプルな表示を得る．\nこの下で，\\(n=30\\) として，確率変数 \\(X\\) を「術後１週間以内に死亡する人数」とすると，\\(X\\) は二項分布 \\(\\mathrm{Bin}(30,0.2)\\) に従う（二項分布の定義は 節 2.1 ）．\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n\n# Parameters for the binomial distribution\nn = 30  # number of trials\np = 0.2 # probability of success in each trial\n\n# Generating data for the binomial distribution\nx = np.arange(0, n+1)\ny = binom.pmf(x, n, p)\n\n# Plotting the binomial distribution\nplt.figure(figsize=(3.5, 3))\nplt.plot(x, y, 'bo', markersize=5)\nplt.vlines(x, 0, y, colors='b', lw=5)\nplt.title('Binomial Distribution - n=30, p=0.2')\nplt.xlabel('Number of Deaths')\nplt.ylabel('Mass')\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nこの図からも，10人以上になる確率は極めて小さいことが判るだろう．実際に計算してみると，\n\n\nCode\nprob_10_or_more = 1 - binom.cdf(9, n, p) - binom.pmf(10, n, p)/2\nprint(f\"p-value is {prob_10_or_more:.4f}\")\n\n\np-value is 0.0434\n\n\nとなる．この「帰無仮説 \\(H_0\\) （今回は「１週間死亡率は20%」）を仮定した下で，実際に観測した事象（今回は「30人のうち10人が一週間死亡」）が起こる条件付き確率」を \\(p\\)-値 と呼ぶ．1\n\n\n\n\n\n\n注（検定統計量の選び方）\n\n\n\n\n\n上で叙述したのは，「死亡者数」という離散確率変数を検定統計量に用いた場合である．しかし，本書 (草野耕一, 2016, p. 99) では，別の離散検定統計量に対して，正規近似を通じて計算している．これは \\(z\\)-検定と呼ばれるものである（ Wikipedia も参照）．\n計算機が得意ならば，直接計算で出した方が近似誤差がないため，好ましいだろう．実際，書籍で得られた値は \\(p=0.0344\\) であり，過小評価している．その論拠は「\\(pn,(1-p)n\\) のいずれも \\(5\\) 以上であれば正規分布と同一視して良いことが知られている」という点である．\n一方で，上の議論では \\(p=0.0611\\) と5%の水準を超えている．一方で，(Lancaster, 1961) の mid-\\(P\\) value と呼ばれる補正法を用いると \\(p=0.0434\\) となり，再び有意になる．\nこのことをどう評価するべきか……．技術的・専門的すぎてとても人口に膾炙するものではないと思うと同時に，非常に本来的ではない議論になっていると感ずる（ 節 1.5 ）．\n\n\n\n\n\n1.2 誤り\n帰無仮説を間違えて棄却してしまうことを，第一種の過誤 という．これは有意水準 \\(\\alpha\\) の値に一致する．\n統計的仮説検定の理論は，初めは Neyman と Pearson によって科学的発見の文脈で考えられたものであるため，帰無仮説の棄却は「科学的発見の萌芽」と同義と解すことが多い（(大塚淳, 2020, pp. 97–106) も参照）．その場合，第一種の誤りとは「本当はなんでもないのに大発見だと思い上がりってしまう確率」である．これを犯す確率を最も制限したい，という志向を持つ．\n次に，第一種の誤りの可能性 \\(\\alpha\\) を制限した上で，本当は帰無仮説が誤りなのに棄却できないリスク＝発見を検出できないリスクをなるべく下げることを二次的目標として考える．これを 第二種の過誤 という．その確率 \\(1-\\beta\\) に対して，\\(\\beta\\) を 検出力 (power) と呼ぶ．\nこれは 第二回で扱った検察官の誤謬 に通じる語用法である．\n\n\n1.3 独立性の検定\n\n\n\n\n\n\n問題\n\n\n\n次の条件を持つ学習教材が「必ず英語の成績が上がる」と言えるだろうか？\n\n全国共通模試で，英語の全国平均点は58点であった．\n当該教材を用いて勉強した者80名の平均は70点であり，標準偏差は15であった．\n\n\n\nこれは，当該教材を用いた群と用いていない群という「２つの標本」の間に差がないこと，今回では「平均が同じであること」を帰無仮説として，目下の証拠からこれを棄却出来るかを検定する問題として捉えることができる．\n\n\n1.4 区間推定\n\n\n1.5 ベイズ統計学\n\n伝統的統計学は客観確率を用いているので，母数の確からしさを１つの数値として示すことができない．伝統的統計学が示しうるものは，「母数がある範囲内にあればこの証拠が現れる確率はいくらであるか」でしかないのである．この矛盾をいかに克服するかは法律家と統計学者が共同して取り組むべき今後の課題であるが，１つの可能性として考えうることは伝統的統計学に代えてベイズ統計学の手法を用いることである．(草野耕一, 2016, p. 119)"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#定義集",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#定義集",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "2 定義集",
    "text": "2 定義集\n\\[\n[a,b]:=\\left\\{x\\in\\mathbb{R}\\mid a\\le x\\le b\\right\\}\n\\] は実数の区間を表す． \\[\n\\mathbb{N}^+:=\\left\\{1,2,3,\\cdots\\right\\}\n\\] は正の整数全体の集合を表す．詳しくは 本サイトの数学記法一覧 を参照．\n\n2.1 二項分布\n(草野耕一, 2016, p. 92) も参照．\n\n\n\n\n\n\n定義（二項分布，Bernoulli分布）\n\n\n\n\nパラメータ \\(n\\in\\mathbb{N}^+\\) と \\(p\\in[0,1]\\) に関する 二項分布 \\(\\mathrm{Bin}(n,p)\\) とは，集合 \\(\\{0,1,\\cdots,n\\}\\) 上の離散確率分布で， 確率質量関数 \\[\nb(x;n,p):=\\begin{pmatrix}n\\\\x\\end{pmatrix}p^x(1-p)^{n-x},\n\\] \\[\nx=0,1,\\cdots,n,\n\\] が定めるものをいう．\nパラメータ \\(p\\in[0,1]\\) に関する Bernoulli分布 \\(\\mathrm{Ber}(p)\\) とは，\\(n=1\\) の場合の二項分布 \\[\n\\mathrm{Ber}(p):=\\mathrm{Bin}(1,p)\n\\] をいう．\n\n\n\nただし，二項係数 \\(\\begin{pmatrix}n\\\\x\\end{pmatrix}\\) は高校数学では \\({}_nC_x\\) と表す場合が多い．前者の記法の美点は，関係式 \\[\n\\begin{align*}\n    \\begin{pmatrix}n\\\\x\\end{pmatrix}&=\\frac{n!}{x!(n-x)!}\\\\\n    &=\\frac{n}{x}\\frac{(n-1)!}{(x-1)!(n-x)!}\\\\\n    &=\\frac{n}{x}\\begin{pmatrix}n-1\\\\x-1\\end{pmatrix}\n\\end{align*}\n\\] が直感的に表せる点にある．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#footnotes",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(大塚淳, 2020, p. 105)，(草野耕一, 2016, p. 100)↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#今回の内容",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n\n1.1 第2章：意思決定\n第二章は 決定木 を用いた 決定分析 に関する章である．\n\n1.1.1 第2章1節「決定の木の作り方」\n決定木の美点は，あり得るシナリオを全て書き出すことが出来ること にある (Smith, 2010, p. 29) が，それも箇条書きをするのではなく，木構造に書くことで，\n\n計算機との親和性があり，自動発見に繋げられる．\n最適なポリシーを執るための，期待値の計算が容易になる．\n計算した結果から，自身の信念の構造を反省する契機になる．\n\nなどの利点が生まれる，という極めて古典的な技法 (Raiffa & Schlaifer, 1961) である．\nこれを用いて全てのシナリオと，それに対応する金銭的利得と確率を書き出し，各意思決定毎の期待値を計算することで，意思決定に役立てることができる．\n決定木の 節 (node) には2種類ある (Taroni et al., 2014, pp. 35–36)\n\n確率節 (Chance Node)：どっちに転ぶか判らない事象（確率変数）を表す節．\n決定節 (Decision Node)：意思決定者が選択する行動を表す節．\n\n教科書 (草野耕一, 2016) では 1.を \\(\\bigcirc\\) で，2.を \\(\\square\\) で表している．\n\n\n1.1.2 第2章第2節「リスク中立的な行為者」\n\n\n\n\n\n\n問題2-2\n\n\n\n上場会社Dテレビがその報道についてE社から訴訟を受けた．2億円支払えば和解に応じるという．\n\nEがDに勝訴する確率は8割で，その場合1億円の賠償請求が命じられる．\n訴訟費用は3000万円．\nEが米国で訴訟を起こした場合，管轄権が認められる可能性は1割であり，さらにその場合には9割で5億円の賠償で2億の訴訟費用，1割で unjust enrichment の法理が適用され50億賠償で5億の訴訟費用がかかるとする．\n\n\n\nこの問題は「キャッシュフローの期待値」を決定木を通じて算出することで教科書内で解かれるが，同時に重要な問題を提起している．\nそもそも，評価基準が「キャッシュフローの期待値」であるべきとは限らない．リスクに対する評価は人それぞれである．\n事実，もし50億の賠償命令が下った場合に倒産リスクが生じる場合，この事象は「50億円」という額面以上に避けるべき事象ということになるだろう．\nそこで，金銭的利得とは別に 効用 の概念を導入し，この効用の期待値によって意思決定をするための論理基盤として (von Neumann & Morgenstern, 1944) の 期待効用理論 を紹介している．\n\n\n1.1.3 コラム2-1：「期待効用理論」\n期待効用理論極めて古いが，現在でも不確実性の下での意思決定の定量的理論の騎手である (Dentcheva & Ruszczynski, 2013)．\n教科書 (草野耕一, 2016) では「効用」という概念を金銭的利得と関連付けて説明しているが，そのためにわかりにくい提示の仕方になっている．1\nそこでここでは，抽象的な定義を提示する．効用とはここでは，「行為者にとっての好ましさの度合いを，相対的に比較できるように定量化したもの」以上の意味はないものとする．\n\n\n\n\n\n\n定義 (preference relation) (Dentcheva & Ruszczynski, 2013)\n\n\n\n\\(X\\) を位相空間とする．完備な 前順序2 \\(\\precsim\\;\\subset X^2\\) を 選好関係 と呼ぶ．3\n\n（完備性）任意の \\(x,y\\in X\\) について，4 \\[x\\precsim y\\;\\lor\\;x\\succsim y.\\]\n（連続性）任意の \\(z\\in X\\) に対して， \\[\n\\left\\{v\\in X\\mid v\\precsim z\\right\\}\\overset{\\textrm{closed}}{\\subset}X,\n\\] \\[\n\\left\\{v\\in X\\mid v\\succsim z\\right\\}\\overset{\\textrm{closed}}{\\subset}X.\n\\]\n（独立性）任意の \\(x,y,z\\in X\\) に対して，\\(x\\succsim y\\) ならば，任意の \\(\\alpha\\in(0,1)\\) について \\[\n\\alpha x+(1-\\alpha)z\\succsim\\alpha y+(1-\\alpha)z.\n\\]\n（アルキメデス性）5 任意の \\(x,y,z\\in X\\) に対して，\\(x\\succsim y\\succsim z\\) ならば，ある \\(\\alpha,\\beta\\in(0,1)\\) が存在して， \\[\n\\alpha x+(1-\\alpha)z\\succsim y\\succsim\\beta x+(1-\\beta)z.\n\\]\n\n\n\n\n\n\n\n\n\n定理 (Dentcheva & Ruszczynski, 2013)\n\n\n\n\\(X\\) を Polish 空間，\\(\\precsim\\;\\subset P(X)^2\\) を連続で独立な選好関係とする．6 このとき，ある有界連続関数 \\(u\\in C_b(\\mathbb{R})\\) が存在して， \\[\nU(\\mu):=\\int_Xu(z)\\,\\mu(dz)\\quad(\\mu\\in P(X))\n\\] は \\[\n\\mu\\succsim\\nu\\quad\\Rightarrow\\quad U(\\mu)\\ge U(\\nu)\n\\] を満たす．\n\n\n教科書 (草野耕一, 2016) にいう「期待効用定理」とはこの定理を指すものと思われる．この \\(u\\in C_b(\\mathbb{R})\\) を 効用関数 (utility function) という．\n問題2-2で用いていた金銭的利得の空間とは \\(X=\\mathbb{R}\\) の場合であり，これは定理の条件を満たす．よって任意の連続で独立な選好関係 \\(\\succsim\\) を用意することで，金銭的利得の代わりに効用 \\(u\\) の期待値 \\(U\\) を用いれば，特定のリスク選好性に対応して，同様の議論を用いて意思決定分析が可能になる．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#決定木の応用",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#決定木の応用",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "2 決定木の応用",
    "text": "2 決定木の応用\n今回決定木を用いた意思決定の形式化の仕方を学んだが，この手法は多くの人間と機械との間で広く共有されて最も輝く．\n決定木は現代では一般に （確率的）グラフィカルモデル の例として捉えられており，そのグラフとしての構造は計算機との親和性が高く，「不確実性の下での意思決定が出来る人工知能・エキスパートシステムの作成」において重要な役割を果たすと考えられている (Sucar, 2021)．\n\n2.1 人間同士のコミュニケーションツールとしての応用\n\n2.1.1 世界銀行の太陽光発電＋蓄電池システムの導入へのイニシアティブ\n世界銀行は 11月28日 に，途上国向けに大規模な太陽光発電＋蓄電システムを導入するために，どのようにプロジェクトを進めれば良いかの実践的なフレームワークを提供する報告書 (Jain et al., 2023) を公開した（プレスリリース）．\n安定したエネルギー供給源の確保も多くの途上国にとって重要な課題であるが，太陽光発電システムを導入し，化石燃料への依存度を低減させることも（特に公的債務がかさんでいる国家では）同時に重要である．これを可能にするフレームワークを提供することが，本報告書の目的であるようである．\nそのフレームワークは4段階\n\n実行可能性の評価：長期的なコスト，既存電力網への統合の方法，需要予測などの予備調査．\nビジネスモデルの選択：二部料金契約，容量契約，混合契約の3つを提示している．\nリスク配分の方法\n競争入札による調達と実行\n\nからなるが，特に 2.のビジネスモデルの選択について，多くのケーススタディを通じて得た「どの変数に応じてどのモデルが選択されるべきか」の知見を決定木の形にまとめている．\n\n\n\nDecision Tree for Selecting a Business Model Figure 3.2 from (Jain et al., 2023, p. 41)\n\n\n\nThe report’s ready-to-use planning framework, the decision-making tree, sample business models, and the PPA template aim to streamline the adoption of solar-plus-storage projects that leverage private investments in countries where fuel-dependency is putting stress on limited public resources. (Jain et al., 2023)\n\n\n報告書は、12月初旬にアラブ首長国連邦のドバイで開催される気候変動枠組み条約第28回締約国会議（COP28）で発表される。–プレスリリース\n\n\n\n2.1.2 世界銀行の気候変動対策イニシアティブ\n世界銀行は過去にも，気候変動対策の分野でも，同様の報告書と簡単な決定木（フローチャート）を発表している．\n\nNo generally accepted methodology for assessing the significance of climate risks relative to all other risks to water resources projects currently exists. This book puts forth a decision support framework in the form of a decision tree to meet this need. (Ray & Brown, 2015)\n\n\n\n\n2.2 機械とのコミュニケーションツールとしての応用\n木構造というのは計算機にとっても扱いやすい構造であり，決定木をデータから学習することで大いに我々の意思決定に活用することができる．これが現代において，機械学習技術が我々に与えてくれる希望の形である．\n\n2.2.1 決定木学習\nどう考えても，決定木は人間が書くよりも，データから学習する方が良い．これが 決定木（学習） (Breiman et al., 1984) である．\nこれは初め，回帰木 という名前で，単関数の線型和を用いたノンパラメトリック回帰手法として導入された．さらにアンサンブル法と組み合わせて精度を向上させたものが ランダムフォレスト (Breiman, 2001) と呼ばれる手法であり，現在極めて主流な手法となっている．\n\n\n2.2.2 2017年度日本経済白書\n内閣府が発表している日本経済白書の 2017年度版 の第二章では「多様化する職業キャリアの現状と課題」を扱っている．\n第3節にて，リスキリングに関して調べられており，「自主的にキャリア設計をしたい人を決める変数は何か？」を決定木学習によって調べている．7\n\n分析結果をみると（第2－3－7図（2））、正社員では、自主的に職業生活設計を考えている割合は、将来に備える目的で自己啓発を実施している人では8割となるが、実施していない人では65％と大きく下がる。だが、実施していなくても大卒・院卒であれば、職業生活設計の自主性割合は73％と増えるが、大卒・院卒以外では59％と大きく差が開く。–日本経済2017-2018\n\n\n\n\n日本経済2017-2018 第2章第3節2 図2-3-7 より\n\n\n第1節では，転職市場の流動性を調べるにあたって，決定木を用いたブースティングにより，転職による賃金上昇の説明変数で，最も有力なものは何かを調べている．\n\nまず、転職前後の賃金変化と関係の深い変数を調べるため、機械学習の手法を用いて、転職後に賃金が変化する人の特徴について整理する。ここでは機械学習の分野でよく使われている「ランダム・フォレスト」という手法を用いた。同手法は、説明変数の数が多くても対応でき、それぞれの説明変数の「重要度」を算出できることから、転職者が持つ多数の特徴のうち、どこに注目するのが適切かを把握するのに有用であると考えられる。–日本経済2017-2018\n\nなお，結果は以下の通りである．\n\n\n\n日本経済2017-2018 第2章第1節2 図2-1-7 より\n\n\n\n\n2.2.3 ランダムフォレストとは何か？\nせっかくなので，日本経済 付記2-1 に付された簡潔な説明を引用する．\n\nランダム・フォレストは、学習データから「決定木」と呼ばれるツリー構造をしたグラフを大量に作成し、作成した決定木を元に多数決で最良の結果を導き出す方法である。–日本経済2017-2018\n\nランダム・フォレストはバギングと呼ばれるアンサンブル法の一種である．アンサンブル法 とは，性能の芳しくない小さな学習器をたくさん集め，これらを効率的に組み合わせることで全体として優れた性能を発揮するように組み合わせる，一段階高次元な機械学習手法であり，バギング とは「1つ1つは信頼出来ない出力でも，全ての平均を取れば性能が良くなる」という考え方に基づく手法である．8\n特に決定木に対してバギングを使用した場合，その手法は ランダム・フォレスト と呼ばれる．これがうまくいく理由は，アンサンブルの過程にある．学習の段階では何も最適化しておらず，アンサンブルの段階で最適化されているのである．\nアンサンブル法は極めてアドホックに感じられるかも知れないが，実践の場面では重要な技術であり，実際 Kaggle などのデータコンペティションでは最も広く見られる戦略の1つである．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#グラフィカルモデルによる意思決定支援",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#グラフィカルモデルによる意思決定支援",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "3 グラフィカルモデルによる意思決定支援",
    "text": "3 グラフィカルモデルによる意思決定支援\n本書では取り上げられなかったが，意思決定の場面こそベイズ統計学の本領 と言えるものである．9\n\n3.1 グラフィカルモデルの利用\n本書で解説されている決定木による意思決定分析の先に，近年では Bayesian Network などの新しい道具が，確率的グラフィカルモデリング の分野から追加されている (Smith, 2010)．\n\nThese graphical methods help draw different aspects of a decision problem together into a coherent whole and provide frameworks where data can be used to support a Bayesian decision analysis. (Smith, 2010, p. viii)\n\nこれは，決定木によるイベントの表現では，イベントの間の依存関係を表現することが出来ない（現実を単純化し過ぎている）という欠点を，木構造を一般のグラフ構造に拡張することで解決したものである．10\n構造は多少複雑になるが，視覚的な理解も引き続き用意である同時に，近年では多くの推論手法が提案されつつある (Sucar, 2021)．\n\n\n3.2 法廷での利用について\n(Smith, 2010, p. 20) では，法廷での DNA 証拠の使用をきっかけに，「確率」を用いた議論が法廷に導入されたことに触れて，ベイズの枠組みは次の意味で親和性が高いと論じている．そのことは，本勉強会で 第一章の内容 を学んできた皆さんには首肯いただけることだろう．\n\nベイズ統計学は演繹的論理の拡張であり，厳密で論理的なフレームワークを提供してくれる．\nどうしてその意思決定に至ったかを，透明で一貫性のある方法で説明することができる．\n\n\n\n3.3 法科学への応用\nBayesian Network は法科学への応用が進んでいる．11\n\nA problem that arises in a courtroom, affecting both lawyers, witnesses and jurors, is that several pieces of evidence have to be put together before a reasoned judgement can be reached: as when motive has to be considered along with material evidence. Probability is designed to effect such combinations but the accumulation of simple rules can produce complicated procedures. Methods of handling sets of evidence have been developed: for example Bayes nets (…). There is a fascinating interplay here between the lawyer and the scientist where they can learn from each other and develop tools that significantly assist in the production of a better judicial system.–Foreword by Dennis Lindley (Aitken & Taroni, 2004, p. 24)"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#footnotes",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n加えて，p.55 に「上記の公理からの演繹的推論によって次の結論が導き出される」とあるが，これは「効用」「単調増加」などの未定義用語を含むため，「演繹的推論によって」というのは誤謬というべきである．↩︎\nすなわち反射的 \\(x\\precsim\\) で推移的な二項関係↩︎\nさらに \\(X\\) を prospect space という．↩︎\nすなわち（反対称性を満たさないが）「全順序」であるということである．↩︎\n教科書 (草野耕一, 2016) では「連続性」とあったが，ここでは (von Neumann & Morgenstern, 1944) の原著に従う．↩︎\n記法については 数学記法一覧 参照．↩︎\n決定木学習について，付記2-1 にて「決定木による分類は、説明変数によるサンプルの分割を繰り返しながら徐々に分類目的（職業設計を自分で実施）の予測誤差を小さくしていく手法である。説明変数間の相互作用を考慮した分類が可能であり、複数の説明変数で分割していくことで職業設計を自分でしたい人の比率が高まる（低まる）樹形図（tree）が作成できる。」と述べられている．↩︎\nBootstrap aggregating の略である (Hansen, 2022, p. 927)．↩︎\n(Smith, 2010, p. 8) で，近年の決定分析の文脈でベイズの手法が興隆している理由の一つに，ベイズ計算手法の発展を挙げている．↩︎\n(Smith, 2010, p. 59) 2.8節も参照．↩︎\n(Taroni et al., 2014), (Smith, 2010, pp. 34–37) 2.3.1節↩︎"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html",
    "href": "posts/2024/Process/Discretization.html",
    "title": "確率過程の離散化",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n(Jacod & Protter, 2012) 第１章参考．\n参照過程は Brown 運動 \\((W_t)_{t\\in\\mathbb{R}_+}\\) のスケーリング \\[\nX=\\sigma W,\\quad(\\sigma&gt;0)\n\\] であるとする．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "1 正規化汎函数 \\(V^{'n}(f,X)\\)",
    "text": "1 正規化汎函数 \\(V^{'n}(f,X)\\)\n\n1.1 \\(t\\in\\mathbb{R}_+\\) 毎の収束\n\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は独立同分布であるが，正規化を施したことにより， \\[\n\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}=\\frac{X_{i\\Delta_n}-X_{(i-1)\\Delta_n}}{\\sqrt{\\Delta_n}}\\sim\\mathop{\\mathrm{N}}(0,c)\n\\] も離散化の段階 \\(n=0,1,\\cdots\\) に依らず独立同分布である．よって， \\[\nf\\left(\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}\\right)\\sim(\\rho_c(f),\\rho_c(f^2)-\\rho_c(f)^2)\n\\] を踏まえて，独立同分布列に対する０次と１次の漸近定理から \\[\nV^{'n}(f,X)_t\\overset{\\text{p}}{\\to}t\\rho_c(f)\n\\] \\[\n\\frac{V^{'n}(f,X)_t-t\\rho_c(f)}{\\sqrt{\\Delta_n}}\\overset{\\text{d}}{\\to}\\mathop{\\mathrm{N}}\\biggr(0,t(\\rho_c(f^2)-\\rho_c(f)^2)\\biggl)\n\\] が言えそうである．\n\n０次の漸近論で概収束は示せない．\n\n\n\n1.2 \\(\\mathbb{R}_+\\) 上の過程としての収束\n\\(\\mathbb{R}_+\\) で添字付けられた過程として，\\(D(\\mathbb{R}_+)\\) 上の Skorohod 位相について確率収束する．すなわち，任意の \\(t\\in[0,T]\\) に対して， \\[\n\\sup_{s\\le t}\\lvert Z^n_s-Z_s\\rvert\\overset{\\text{p}}{\\to}0.\n\\] 加えて，汎函数中心極限定理から， \\[\n\\left(\\frac{1}{\\sqrt{\\Delta_n}}(V^{'n}(f,X)_t-t\\rho_c(f))\\right)_{t\\ge0}\\overset{\\text{d}}{\\to}\\sqrt{\\rho_c(f^2)-\\rho_c(f)^2}B.\n\\] が Skorohod 位相に関して成り立つ．これはさらに安定収束もするのである．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "2 非正規化汎函数 \\(V^n(f,X)\\)",
    "text": "2 非正規化汎函数 \\(V^n(f,X)\\)\n正規化を施さないために，\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は \\(0\\) に漸近していき，関数 \\(f\\) の \\(0\\) での局所的な振る舞いが収束に影響を与えるようになる．"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html",
    "href": "posts/2024/Process/MartingaleProblem.html",
    "title": "マルチンゲール問題",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "href": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "title": "マルチンゲール問題",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Hoh, 1998, p. 28), (Criens et al., 2023)↩︎"
  },
  {
    "objectID": "posts/2024/Process/Coupling.html",
    "href": "posts/2024/Process/Coupling.html",
    "title": "確率測度のカップリング",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\nカップリングによる一様エルゴード性の証明は (Griffeath, 1975b)，エルゴード性の証明は (Kulik & Scheutzow, 2015)，指数エルゴード性は (Hairer & Mattingly, 2011) による．\nエルゴード定理自体は古くから示されているが，カップリングによる証明は極めて見通しがよく，もはや極めて自然で直感的に理解できる段階にまで達しているものと思われる．\nこのような感覚は，高度に洗練された数学的概念には，よく見られるものであるようである："
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html",
    "title": "Measurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性",
    "section": "",
    "text": "This entry has grown out of a question I answered on MathOverflow. I will try to explain the question and my answer in a more leisurely manner here."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#introduction",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#introduction",
    "title": "Measurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n\n\n\n\n\nQuestion\n\n\n\nFor Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), Minkowski sum is defined as \\[\nA+B:=\\left\\{a+b\\in\\mathbb{R}^n\\mid a\\in A,b\\in B\\right\\}.\n\\] We are interested in the following questions:\n\nUnder what conditions is \\(A+B\\) Borel?\nFor what \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\) is \\(A+B\\) not Borel?\n\n\n\n\\(A+B\\) being an image of a continuous mapping \\(+:\\mathbb{R}^n\\times\\mathbb{R}^n\\to\\mathbb{R}^n\\), \\(A+B\\) is an analytic (a.k.a. Souslin) set. Therefore, \\(A+B\\) is Lebesgue measurable, given all Souslin sets are universally measurable.1\nA statistician or probability theorist may come across this problem when considering isoperimetric inequalities. For example, the one by (Borell, 1975) and (Sudakov & Tsirel’son, 1974) goes as follows:\n\n\n\n\n\n\nTheorem2 (Gaussian isoperimetric inequality)\n\n\n\nLet \\(\\gamma_n:=\\mathop{\\mathrm{N}}_n(0,I_n)\\) be the standard Gaussian measure on \\(\\mathbb{R}^n\\), \\(u\\in\\partial B^n\\subset\\mathbb{R}^n\\) a unit vector, \\(A\\in\\mathcal{B}(\\mathbb{R}^n)\\) be a Borel measurable set, and \\[\nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\quad a\\in\\mathbb{R},\n\\] be a affine half-space satisfying \\(\\gamma_n(H_a)=\\gamma_n(A)\\). Then, the following inequality holds for all \\(\\epsilon&gt;0\\): \\[\n\\gamma_n(H_{a+\\epsilon})=\\gamma_n(A_\\epsilon),\\qquad\\epsilon&gt;0,\n\\] where \\(B^n\\overset{\\textrm{closed}}{\\subset}\\mathbb{R}^n\\) is a closed unit ball centered at the origin, and \\[\nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\:\\middle|\\:\\inf_{y\\in A}\\|x-y\\|_2\\le\\epsilon\\right\\}\n\\] is the closed \\(\\epsilon\\)-neighborhood of \\(A\\).\n\n\nHere, \\[\nA_\\epsilon=A+\\epsilon B^n\n\\] so the Borel measurability of \\(A+B^n(0,\\epsilon)\\) matters.\nOf course, \\(A_\\epsilon\\) is \\(\\gamma_n\\)-measurable, meaning that there exist Borel sets \\(B_1,B_2\\in\\mathcal{B}(\\mathbb{R}^n)\\) such that \\[\nB_1\\subset A_\\epsilon\\subset B_2,\n\\] \\[\n\\mu(B_2\\setminus B_1)=0.\n\\] Thus, the above theorem can be understood as implicitly assuming the Borel probability measure \\(\\gamma_n\\) to be completed in the Lebesgue sense.\nAs it turns out in Section 3.2, the Borel measurability of \\(A+B^n(0,\\epsilon)\\) is not guaranteed, despite the fact \\(B^n(0,\\epsilon)\\) is a closed and compact subset."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#conditions-assuring-to-be-borel",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#conditions-assuring-to-be-borel",
    "title": "Measurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性",
    "section": "2 Conditions assuring to be Borel",
    "text": "2 Conditions assuring to be Borel\n\n\n\n\n\n\nProposition\n\n\n\nLet \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\) be Borel sets.\n\nIf either \\(A\\) or \\(B\\) is open, then \\(A+B\\) is open.\nEven when \\(A\\) and \\(B\\) are closed, \\(A+B\\) may not be closed.\nAdditionally imposing either \\(A\\) or \\(B\\) to be compact, then \\(A+B\\) is closed.\n\nAll of the above statements remain valid when an arbitrary topological vector space is considered in place of \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nGiven \\[\nA+B=\\bigcup_{b\\in B}(A+b),\n\\] we see that \\(A+B\\) is open if either \\(A\\) or \\(B\\) is open. Note \\(\\bullet+b:\\mathbb{R}^n\\to\\mathbb{R}^n\\) is a homeomorphism, so \\(A+b\\) is open.\nLet \\(n=1\\) and \\[\nA:=\\mathbb{N}^+,\n\\] \\[\nB:=\\left\\{-n+\\frac{1}{n}\\in\\mathbb{R}\\:\\middle|\\:n=2,3,\\cdots\\right\\}.\n\\] Both sets \\(A,B\\) are discrete subsets of \\(\\mathbb{R}\\), hence closed. However, \\(A+B\\) is not closed. Indeed, \\(\\{1/n\\}_{n\\ge2}\\subset A+B\\) but its limit point \\(0\\notin A+B\\).\nLet us assume \\(A\\) to be compact and take an arbitrary sequence \\(\\{a_n+b_n\\}\\subset A+B\\) converging to a limit, denoted \\(x\\in\\overline{A+B}\\). Compactnes of \\(A\\) implies the existence of a convergent subsequence \\(\\{a_{n_k}\\}\\subset\\{a_n\\}\\) converging to some \\(a\\in A\\). Then, \\(\\{b_{n_k}\\}\\subset B\\) also converges and its limit is \\(x-a\\in B\\), which belongs to \\(B\\) because \\(B\\) is closed. Hence, \\(x=a+(x-a)\\in A+B\\), giving a sufficient condition for \\(A+B\\) to be closed."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#counterexamples",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#counterexamples",
    "title": "Measurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性",
    "section": "3 Counterexamples",
    "text": "3 Counterexamples\n\n3.1 Using subgroups of \\(\\mathbb{R}\\)\n(Erdös & Stone, 1969) gives a counterexample for \\(n\\ge2\\). Astonishingly, for the case \\(n=1\\), the counterexample consists of \\(A\\) being a Cantor, hence compact, set and \\(B\\) being a \\(G_\\delta\\) set.\n\n\n3.2 Using a non-Borel Souslin set of \\(\\mathbb{R}^2\\).\nFor every uncountable Polish space, there exists a non-Borel Souslin set,3 i.e., \\(\\mathcal{B}(X)\\subsetneq\\Sigma^1_1(X)\\), where \\(\\Sigma^1_1(X)\\) represents the class of all Souslin sets of \\(X\\).\nTaking \\(X=[-1,1]\\), we can construct a non-Borel Souslin set \\(A_1'\\in\\Sigma^1_1(X)\\setminus\\mathcal{B}(X)\\), and using this \\(A_1'\\) we are going to construct a counterexample for \\(n\\ge3\\).\nHere, we are in need of the following characterization of Souslin sets:\n\n\n\n\n\n\nTheorem4\n\n\n\nLet \\(X\\) be a Souslin space, a Souslin set which is also Hausdorff, and let \\(A\\subset X\\) its subset. The following are equivalent:\n\n\\(A\\) is a Souslin set;\n\\(A\\) can be represented as \\(A=\\mathrm{pr}_1(F)\\), where \\(F\\overset{\\textrm{closed}}{\\subset}X\\times\\mathbb{N}^\\infty\\);\n\\(A\\) can be represented as \\(A=\\mathrm{pr}_1(B)\\), where \\(B\\subset X\\times\\mathbb{R}\\) is Borel measurable.\n\n\n\nHere we take \\(X:=[-1,1]\\) and \\(A:=A_1'\\), we can find a Borel measurable subset \\(A'\\subset[-1,1]^2\\) such that \\(A_1'=\\mathrm{pr}_1(A')\\).5\nThe next step is crucial, where we map the Borel subset \\(A'\\) to a cylinder \\[\nC:=\\left\\{(x_1,x_2,x_3)\\in\\mathbb{R}^3\\mid x_2^2+x_3^2=1\\right\\},\n\\] using a homeomorphism \\(\\psi:\\mathbb{R}^2\\to\\mathbb{R}^2\\) which satisfies \\[\n\\begin{align*}\n    &\\psi([-1,1]\\times\\{0\\})\\\\\n    &\\qquad\\subset\\left\\{(x_1,x_2)\\in\\mathbb{R}^2\\mid x_1^2+x_2^2=1\\right\\}.\n\\end{align*}\n\\] Such a homeomorphism \\(\\psi\\) takes the segment \\([-1,1]\\) on the \\(x_1\\)-axis into the unit circumference \\(S^1\\) in the \\((x_1,x_2)\\)-plane.\nUsing \\(\\psi\\) as a building block, we constract a homeomorphism \\(\\Psi:\\mathbb{R}^3\\to\\mathbb{R}^3\\) by \\[\n\\Psi(x_1,x_2,x_3):=(x_1,\\psi(x_2,x_3)).\n\\] Such a homeomorphism \\(\\Psi\\) pastes the set \\(A'\\) onto the surface of the cylinder \\(C\\): \\(A:=\\Psi(A')\\subset C\\). Given that \\(\\Psi\\) is a homeomorphism, \\(A\\) is a Borel measurable set.6\nThus, \\(A\\subset C\\subset\\mathbb{R}^3\\) now satisfies the following properties: \\[\n\\biggr(A+B(0,1)\\biggl)\\cap\\biggr(\\mathbb{R}\\times\\{0\\}^2\\biggl)=A_1',\n\\] where \\(A_1'\\notin\\mathcal{B}(X)\\) is non-Borel and \\(B(0,1)\\subset\\mathbb{R}^3\\) is a closed unit ball centered at the origin.7 This scenario is impossible if \\(A+B(0,1)\\) is Borel measurable, since \\(\\mathbb{R}\\times\\{0\\}^2\\) is Borel measurable.\nThis idea is stimulated from (Luiro et al., 2014), which has an arXiv version, Example 2.4."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#footnotes",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#footnotes",
    "title": "Measurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Dudley, 2002, p. 497) Theorem 13.2.6, (Kechris, 1995, p. 155) Theorem 21.20.↩︎\n(Borell, 1975), (Sudakov & Tsirel’son, 1974), (Giné & Nickl, 2021, p. 31) Theorem 2.2.3.↩︎\n(Kechris, 1995, p. 85) Theorem 14.2.↩︎\n(Bogachev, 2007, p. 24) Theorem 6.7.2, (Kechris, 1995, p. 86) 14.3.↩︎\nFrom the theorem, we can find a Borel measurable subset \\(B\\subset[-1,1]\\times\\mathbb{R}\\) such that \\(A_1'=\\mathrm{pr}_1(B)\\). Then, we define \\(A':=B\\cap[-1,1]^2\\), which is Bore measurable and still has the property \\(A_1'=\\mathrm{pr}_1(A')\\).↩︎\nFor a complete separable metric space \\(X,Y\\), an image of a Borel set via a Borel measurable injection is again Borel measurable. (Bogachev, 2007, p. 30) Theorem 6.8.6.↩︎\nFor other notations, please consult this post.↩︎"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html",
    "href": "posts/2024/Julia/Julia0.html",
    "title": "俺のための Julia 入門０",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#概観",
    "href": "posts/2024/Julia/Julia0.html#概観",
    "title": "俺のための Julia 入門０",
    "section": "1 概観",
    "text": "1 概観\n\n\nCode\nusing Plots\n\nplot(sin, \n    x-&gt;sin(2x), \n    0, \n    2π, \n    leg=false, \n    fill=(0,:lavender))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1: Parametric Plots\n\n\n\n\n\n1.1 今から始める Julia\n実は Quarto は Julia をサポートしている．\nQuarto - Using Julia\n\n\n1.2 インストール\n\nまずは Julia をインストール\ncurl -fsSL https://install.julialang.org | sh\nすると，julia コマンドで対話的セッションを開始できる．これを Julia では REPL (Read-Eval-Print Loop) と呼ぶ．\n続いて，Quarto で julia ブロックを動かすには，IJulia パッケージをインストールする．次を REPL で実行する：\nusing Pkg\nPkg.add(\"IJulia\")\nusing IJulia\nnotebook()\nこれで，Jupyter Notebook で Julia を使うことができる．1\nRevise.jl も追加すると，Julia セッションを再起動するためが省ける\nusing Pkg\nPkg.add(\"Revise\")\nJupyter Cache も追加すると，ソースが変わらない限りその出力がキャッシュされ，再実行が控えられる．\nusing Conda\nConda.add(\"jupyter-cache\")\n\nGetting Started"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#julia-入門",
    "href": "posts/2024/Julia/Julia0.html#julia-入門",
    "title": "俺のための Julia 入門０",
    "section": "2 Julia 入門",
    "text": "2 Julia 入門\n\n2.1 歴史\nJulia は Why We Created Julia の文書と共に，2/14/2012 に公開された．\n論文 (Bezanson et al., 2017) も発表されて後，2018 年にはバージョン 1.0 がリリースされた．\n現在，12/26/2023 以来，v1.10.0 がリリースされている．"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#パッケージリスト",
    "href": "posts/2024/Julia/Julia0.html#パッケージリスト",
    "title": "俺のための Julia 入門０",
    "section": "3 パッケージリスト",
    "text": "3 パッケージリスト\n\n3.1 プロット\n\n3.1.1 Plots\n\n\nSoss\n\n\n\n\n3.2 Soss：確率的プログラミング\n\n\n3.3 数値実験2\n\n3.3.1 JLD2：数値実験の結果を保存"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#footnotes",
    "href": "posts/2024/Julia/Julia0.html#footnotes",
    "title": "俺のための Julia 入門０",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJupyter Notebook をインストールしていない場合，install Jupyter via Conda, y/n? [y]: n の確認がなされる．これに y で応えると，Conda.jl パッケージを通じて，Miniconda から最小限の Jupyter 環境がインストールされ，グローバル環境は変わらない．↩︎\nto reproduce weakly informative resampling codes↩︎"
  },
  {
    "objectID": "posts/2024/Review/Fearnhead+2017.html",
    "href": "posts/2024/Review/Fearnhead+2017.html",
    "title": "Fearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\nReferences\n\nFearnhead, P., Latuszynski, K., Roberts, G. O., & Sermaidis, G. (2017). Continious-time importance sampling: Monte carlo methods which avoid time-discretisation error. https://arxiv.org/abs/1712.06201"
  },
  {
    "objectID": "posts/2024/AI/RL.html",
    "href": "posts/2024/AI/RL.html",
    "title": "強化学習",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/AI/RL.html#導入",
    "href": "posts/2024/AI/RL.html#導入",
    "title": "強化学習",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 他の機械学習手法との違い\n強化学習は機械学習の３分類の１つとして「他の２類型（教師あり・教師なし学習）とは大きく文脈が違う」という注記と共によく紹介される．それもその通り．強化学習は，エージェントが世界の中でどのように行動するかを，環境との相互作用を通じて自律的に学ぶ，という人工知能分野の問題設定から生じた学問である (Sutton & Barto, 2018)．\n強化学習の設定にはいくつか特徴がある．\n\n試行錯誤の中で学ぶこと：教師あり学習のように，報酬を最大化するように学んでいくが，学習データというものはなく，試行錯誤の中で学ぶ必要がある．\n報酬は遅れてくるものもあること：行動の結果がすぐに報酬として返ってくるわけではなく，以前の行動が未来の報酬に影響を与えることがあり，それを踏まえて学習をすることが求められる．\n\nすると，強化学習は専ら動的計画法の議論が中心となる．\n\n\n1.2 強化学習の応用\n強化学習は，部分的に観測されている Markov 決定過程の最適制御として理解される．\n在庫管理 (Van Roy et al., 1997)，動的なチャンネルの割り当て (Singh & Bertsekas, 1996)，エレベータ制御 (Crites & Barto, 1998)，テーブルゲーム (Silver et al., 2018)，気候変動対策 (Rolnick et al., 2022) などにも用いられている．\nまた，深層学習と組み合わせることで，DeepMind の AlphaGo (Silver et al., 2016) と AlphaGoZero (Silver et al., 2017) は囲碁において人類の追随を許さない実力をつけた．\n今後も，人間のフィードバックによる強化学習 (RLHF: Reinforcement Learning through Human Feedback) (Christiano et al., 2017) や GNN のトレーニングなど，他の機械学習手法と組み合わせることでより大きな AI システムを作るにあたって，強化学習は必要不可欠な立場を占めていくだろう．\n\n\n1.3 歴史"
  },
  {
    "objectID": "posts/2024/AI/RL.html#markov-決定過程",
    "href": "posts/2024/AI/RL.html#markov-決定過程",
    "title": "強化学習",
    "section": "2 Markov 決定過程",
    "text": "2 Markov 決定過程\n\n\n\n\n\n\n定義 (Markov decision process, policy)1\n\n\n\n\n状態空間 \\(\\mathcal{X}\\)，行動空間 \\(\\mathcal{A}\\) という２つの可測空間と，確率核 \\[\nP:\\mathcal{X}\\times\\mathcal{A}\\to\\mathcal{X}\\times\\mathbb{R}\n\\] との3-組を Markov 決定過程 という．\n\\(\\mathcal{X},\\mathcal{A}\\) がいずれも有限集合であるとき，MDP \\((\\mathcal{X},\\mathcal{A},P)\\) を 有限 であるという．\n確率核 \\(\\pi:\\mathcal{X}\\to\\mathcal{A}\\) を 方策 という．\n\n\n\n多くの場合，\\(P(x,a)\\in\\mathcal{P}(\\mathcal{X}\\times\\mathbb{R})\\) は直積 \\[\nP(x,a)=P_{\\mathcal{X}}(x,a)\\otimes P_{\\mathcal{R}}(x,a)\n\\] で与えられるとする．"
  },
  {
    "objectID": "posts/2024/AI/RL.html#q-学習",
    "href": "posts/2024/AI/RL.html#q-学習",
    "title": "強化学習",
    "section": "3 \\(Q\\)-学習",
    "text": "3 \\(Q\\)-学習\n強化学習の最も標準的かつ古典的なアルゴリズムが，\\(Q\\)-学習 (Watkins, 1989), (Watkins & Dayan, 1992) と，その派生アルゴリズムである SARSA である (Powell, 2011, p. 122)．\n\n3.1 TD 学習\n推移確率が未知である場合，これをシミュレーションによって推定することができる．この設定では 時間差分学習 (Temporal Difference Learning) とも呼ばれる (Sutton, 1988)．"
  },
  {
    "objectID": "posts/2024/AI/RL.html#footnotes",
    "href": "posts/2024/AI/RL.html#footnotes",
    "title": "強化学習",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bellemare et al., 2023, p. 15), (Sutton & Barto, 2018, p. 48)．↩︎"
  },
  {
    "objectID": "posts/2024/AI/LLM.html",
    "href": "posts/2024/AI/LLM.html",
    "title": "大規模言語モデル",
    "section": "",
    "text": "Mistral AI は 2023 年に Google DeepMind の研究者１人と Meta Platform の元研究者２人によって設立されたフランス企業で，オープンソースでの大規模言語モデルの開発を行っている．\nLLama 2 70B モデルより性能が良いとされているが，ヨーロッパ系の言語５言語のみに特化しているモデルである．\nMistral Cookbook ではコミュニティによる Mistral AI の言語モデルの利用事例が公開されている．このいくつかを本記事では見て遊んでいく．\nAPI の利用には 登録が必要 であるが，サブスクリプションではなくて利用量に応じた課金方式である．"
  },
  {
    "objectID": "posts/2024/AI/LLM.html#rag",
    "href": "posts/2024/AI/LLM.html#rag",
    "title": "大規模言語モデル",
    "section": "0.1 RAG",
    "text": "0.1 RAG\nRAG (Retrieval-Augmented Generation) (Lewis et al., 2020) は，言語モデルと情報検索を次のように組み合わせることで，質問応答などのタスクでの性能を上げる手法である：\n\n情報検索を行い，関連があると思われる情報を知識ベースから抽出する．\n抽出した情報をプロンプトに含めて言語モデルに入力する．\n\nこれを Mistral を用いて実装してみる．\n\n0.1.1 Import needed packages\nThe first step is to install the needed packages mistralai and faiss-cpu and import the needed packages:\n! pip install faiss-cpu==1.7.4 mistralai==0.0.12\n\n\nCode\nfrom mistralai.client import MistralClient, ChatMessage\nimport requests\nimport numpy as np\nimport faiss\nimport os\nfrom getpass import getpass\n\napi_key= getpass(\"Type your API Key\")\nclient = MistralClient(api_key=api_key)\n\n\n\n\n0.1.2 外部データの下処理\nPaul Graham のエッセイを知識ベースとして用いることを考える．\n\n\nCode\nresponse = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt')\ntext = response.text\nlen(text)\n\n\nf = open('essay.txt', 'w')\nf.write(text)\nf.close()\n関連する情報の抽出を効率的に行うため，外部情報を小さなチャンクに分割することを考える．\nIn a RAG system, it is crucial to split the document into smaller chunks so that it’s more effective to identify and retrieve the most relevant information in the retrieval process later. In this example, we simply split our text by character, combine 2048 characters into each chunk, and we get 37 chunks.\n\n\nCode\nchunk_size = 2048\nchunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n\n\n\n\nCode\nlen(chunks)\n\n\n\n0.1.2.1 Considerations:\n\nChunk size: Depending on your specific use case, it may be necessary to customize or experiment with different chunk sizes and chunk overlap to achieve optimal performance in RAG. For example, smaller chunks can be more beneficial in retrieval processes, as larger text chunks often contain filler text that can obscure the semantic representation. As such, using smaller text chunks in the retrieval process can enable the RAG system to identify and extract relevant information more effectively and accurately. However, it’s worth considering the trade-offs that come with using smaller chunks, such as increasing processing time and computational resources.\nHow to split: While the simplest method is to split the text by character, there are other options depending on the use case and document structure. For example, to avoid exceeding token limits in API calls, it may be necessary to split the text by tokens. To maintain the cohesiveness of the chunks, it can be useful to split the text by sentences, paragraphs, or HTML headers. If working with code, it’s often recommended to split by meaningful code chunks for example using an Abstract Syntax Tree (AST) parser.\n\n\n\n\n0.1.3 Create embeddings for each text chunk\nFor each text chunk, we then need to create text embeddings, which are numeric representations of the text in the vector space. Words with similar meanings are expected to be in closer proximity or have a shorter distance in the vector space. To create an embedding, use Mistral’s embeddings API endpoint and the embedding model mistral-embed. We create a get_text_embedding to get the embedding from a single text chunk and then we use list comprehension to get text embeddings for all text chunks.\n\n\nCode\ndef get_text_embedding(input):\n    embeddings_batch_response = client.embeddings(\n          model=\"mistral-embed\",\n          input=input\n      )\n    return embeddings_batch_response.data[0].embedding\n\n\n\n\nCode\ntext_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n\n\n\n\nCode\ntext_embeddings.shape\n\n\n\n\nCode\ntext_embeddings\n\n\n\n\n0.1.4 Load into a vector database\nOnce we get the text embeddings, a common practice is to store them in a vector database for efficient processing and retrieval. There are several vector database to choose from. In our simple example, we are using an open-source vector database Faiss, which allows for efficient similarity search.\nWith Faiss, we instantiate an instance of the Index class, which defines the indexing structure of the vector database. We then add the text embeddings to this indexing structure.\n\n\nCode\nd = text_embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(text_embeddings)\n\n\n\n0.1.4.1 Considerations:\n\nVector database: When selecting a vector database, there are several factors to consider including speed, scalability, cloud management, advanced filtering, and open-source vs. closed-source.\n\n\n\n\n0.1.5 Create embeddings for a question\nWhenever users ask a question, we also need to create embeddings for this question using the same embedding models as before.\n\n\nCode\nquestion = \"What were the two main things the author worked on before college?\"\nquestion_embeddings = np.array([get_text_embedding(question)])\nquestion_embeddings.shape\n\n\n\n\nCode\nquestion_embeddings\n\n\n\n0.1.5.1 Considerations:\n\nHypothetical Document Embeddings (HyDE): In some cases, the user’s question might not be the most relevant query to use for identifying the relevant context. Instead, it maybe more effective to generate a hypothetical answer or a hypothetical document based on the user’s query and use the embeddings of the generated text to retrieve similar text chunks.\n\n\n\n\n0.1.6 Retrieve similar chunks from the vector database\nWe can perform a search on the vector database with index.search, which takes two arguments: the first is the vector of the question embeddings, and the second is the number of similar vectors to retrieve. This function returns the distances and the indices of the most similar vectors to the question vector in the vector database. Then based on the returned indices, we can retrieve the actual relevant text chunks that correspond to those indices.\n\n\nCode\nD, I = index.search(question_embeddings, k=2)\nprint(I)\n\n\n\n\nCode\nretrieved_chunk = [chunks[i] for i in I.tolist()[0]]\nprint(retrieved_chunk)\n\n\n\n0.1.6.1 Considerations:\n\nRetrieval methods: There are a lot different retrieval strategies. In our example, we are showing a simple similarity search with embeddings. Sometimes when there is metadata available for the data, it’s better to filter the data based on the metadata first before performing similarity search. There are also other statistical retrieval methods like TF-IDF and BM25 that use frequency and distribution of terms in the document to identify relevant text chunks.\nRetrieved document: Do we always retrieve individual text chunk as it is? Not always.\n\nSometimes, we would like to include more context around the actual retrieved text chunk. We call the actual retrieve text chunk “child chunk” and our goal is to retrieve a larger “parent chunk” that the “child chunk” belongs to.\nOn occasion, we might also want to provide weights to our retrieve documents. For example, a time-weighted approach would help us retrieve the most recent document.\nOne common issue in the retrieval process is the “lost in the middle” problem where the information in the middle of a long context gets lost. Our models have tried to mitigate this issue. For example, in the passkey task, our models have demonstrated the ability to find a “needle in a haystack” by retrieving a randomly inserted passkey within a long prompt, up to 32k context length. However, it is worth considering experimenting with reordering the document to determine if placing the most relevant chunks at the beginning and end leads to improved results.\n\n\n\n\n\n0.1.7 Combine context and question in a prompt and generate response\nFinally, we can offer the retrieved text chunks as the context information within the prompt. Here is a prompt template where we can include both the retrieved text and user question in the prompt.\n\n\nCode\nprompt = f\"\"\"\nContext information is below.\n---------------------\n{retrieved_chunk}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {question}\nAnswer:\n\"\"\"\n\n\n\n\nCode\ndef run_mistral(user_message, model=\"mistral-medium-latest\"):\n    messages = [\n        ChatMessage(role=\"user\", content=user_message)\n    ]\n    chat_response = client.chat(\n        model=model,\n        messages=messages\n    )\n    return (chat_response.choices[0].message.content)\n\n\n\n\nCode\nrun_mistral(prompt)\n\n\n\n0.1.7.1 Considerations:\n\nPrompting techniques: Most of the prompting techniques can be used in developing a RAG system as well. For example, we can use few-shot learning to guide the model’s answers by providing a few examples. Additionally, we can explicitly instruct the model to format answers in a certain way.\n\nIn the next sections, we are going to show you how to do a similar basic RAG with some of the popular RAG frameworks. We will start with LlamaIndex and add other frameworks in the future."
  },
  {
    "objectID": "posts/2024/AI/LLM.html#langchain",
    "href": "posts/2024/AI/LLM.html#langchain",
    "title": "大規模言語モデル",
    "section": "0.2 LangChain",
    "text": "0.2 LangChain\n\n\nCode\n!pip install langchain langchain-mistralai==0.0.4\n\n\n\n\nCode\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_mistralai.chat_models import ChatMistralAI\nfrom langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\n\n# Load data\nloader = TextLoader(\"essay.txt\")\ndocs = loader.load()\n# Split text into chunks\ntext_splitter = RecursiveCharacterTextSplitter()\ndocuments = text_splitter.split_documents(docs)\n# Define the embedding model\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n# Create the vector store\nvector = FAISS.from_documents(documents, embeddings)\n# Define a retriever interface\nretriever = vector.as_retriever()\n# Define LLM\nmodel = ChatMistralAI(mistral_api_key=api_key)\n# Define prompt template\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\nQuestion: {input}\"\"\")\n\n# Create a retrieval chain to answer questions\ndocument_chain = create_stuff_documents_chain(model, prompt)\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\nresponse = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\nprint(response[\"answer\"])"
  },
  {
    "objectID": "posts/2024/AI/LLM.html#llamaindex",
    "href": "posts/2024/AI/LLM.html#llamaindex",
    "title": "大規模言語モデル",
    "section": "0.3 LlamaIndex",
    "text": "0.3 LlamaIndex\n\n\nCode\n!pip install llama-index==0.10.13 llama-index-llms-mistralai==0.1.4 llama-index-embeddings-mistralai==0.1.3\n\n\n\n\nCode\nimport os\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.llms.mistralai import MistralAI\nfrom llama_index.embeddings.mistralai import MistralAIEmbedding\n\n# Load data\nreader = SimpleDirectoryReader(input_files=[\"essay.txt\"])\ndocuments = reader.load_data()\n# Define LLM and embedding model\nSettings.llm = MistralAI(model=\"mistral-medium\")\nSettings.embed_model = MistralAIEmbedding(model_name='mistral-embed')\n# Create vector store index\nindex = VectorStoreIndex.from_documents(documents)\n# Create query engine\nquery_engine = index.as_query_engine(similarity_top_k=2)\nresponse = query_engine.query(\n    \"What were the two main things the author worked on before college?\"\n)\nprint(str(response))"
  },
  {
    "objectID": "posts/2024/AI/BAI1_Dropout.html",
    "href": "posts/2024/AI/BAI1_Dropout.html",
    "title": "ベイズ機械学習１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n(Hinton et al., 2012), (Srivastava et al., 2014) による，ミニバッチごとに確率的に使わない結合を決定するという正則化の技法である．1"
  },
  {
    "objectID": "posts/2024/AI/BAI1_Dropout.html#dropout-による正則化",
    "href": "posts/2024/AI/BAI1_Dropout.html#dropout-による正則化",
    "title": "ベイズ機械学習１",
    "section": "1 Dropout による正則化",
    "text": "1 Dropout による正則化\n\n1.1 Bayes からの説明\nDropout による正則化は，Gauss 過程による近似とも見れ，Bayes 手法の持つ正則化効果と相通ずることが指摘されている (Gal & Ghahramani, 2016)．\n\n\n1.2 Monte Carlo Dropout\n(Gal & Ghahramani, 2016)"
  },
  {
    "objectID": "posts/2024/AI/BAI1_Dropout.html#footnotes",
    "href": "posts/2024/AI/BAI1_Dropout.html#footnotes",
    "title": "ベイズ機械学習１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGAN の数値実験 (Goodfellow et al., 2014, p. 6) にも，判別器を訓練するのに用いられている．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory.html",
    "href": "posts/2024/AI/Theory.html",
    "title": "統計的学習理論１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n機械学習と統計学に別を設けるならば，いずれもデータから構造を発見することを目標とするとしても，前者は明示的なプログラムを伴わない「自動化」を念頭におくものであると言える．この人間による介入をなるべく少なくしたいという志向が「学習」の名前に表れている．\nそれ故，機械学習の理論としては，通常の統計的決定理論の枠組みよりも，汎化性能に力点を置いたものとなっている．これを，径数模型の教師あり学習の場合に関して述べる．"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#機械学習の形式化",
    "href": "posts/2024/AI/Theory.html#機械学習の形式化",
    "title": "統計的学習理論１",
    "section": "1 機械学習の形式化",
    "text": "1 機械学習の形式化\n\n1.1 記法と用語1\n\nデータサイズを \\(n\\in\\mathbb{N}^+\\) で表す．\n訓練データ (sample) の全体を \\(S_n=\\{z_i\\}_{i=1}^n\\subset\\mathcal{X}\\times\\mathcal{Y}\\) と表す．2 \\(\\mathcal{X}\\) を入力空間，\\(\\mathcal{Y}\\) を出力空間と呼ぶ．3\n\\(\\mathcal{X},\\mathcal{Y}\\) はいずれも可測空間とし，可測関数 \\(h\\in\\mathcal{L}(\\mathcal{X},\\mathcal{Y})\\) を 推定量，部分集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) を 仮説集合 (hypothesis set) という．4\n\\(l(\\Delta_\\mathcal{Y})=\\{0\\}\\) を満たす関数 \\(l:\\mathcal{Y}^2\\to\\mathbb{R}_+\\) を 損失関数 という．5\n写像 \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) を（機械学習） アルゴリズム または学習者という．\n\n以降，データはある真の分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に従うものとし，\\((X,Y)\\sim\\mathbb{P}\\) と表す．サンプル \\(S_n=\\{z_i\\}_{i=1}^n\\) は \\((X,Y)\\sim\\mathbb{P}\\) の独立同分布な複製と仮定する．6\n\n\n\n\n\n\n損失関数の例\n\n\n\n\n\n\n\\(l:=1_{\\Delta_\\mathcal{Y}^\\complement}\\) は 0-1損失 と呼ばれ，主に分類問題で使われる．これについて，汎化誤差とは，7 \\[\nR(h)=\\mathbb{E}[l(h(X),Y)]=\\mathbb{P}[h(X)\\ne Y]\n\\]\n\\(\\mathcal{Y}=\\mathbb{R}^d\\) とし，\\(l(y_1,y_2)=\\|y_1-y_2\\|^2_2\\) とした場合を 二乗損失 といい，主に回帰問題の最小二乗法などで用いられる．\n\n\n\n\n\n\n1.2 汎化ギャップ\n\n\n\n\n\n\n(generalization) error, training error8\n\n\n\n\n定義 1 \\(l:\\mathcal{Y}^2\\to\\mathbb{R}_+\\) を損失関数とする．\n\n仮説 \\(h\\in\\mathcal{H}\\) の （汎化）誤差 または 危険 または 予測損失 とは， \\[\nR(h):=\\mathbb{E}[l(h(X),Y)]\n\\] をいう．\n仮説 \\(h\\in\\mathcal{H}\\) のサンプル \\(S_n=\\{(x_i,y_i)\\}_{i=1}^n\\) に関する 訓練誤差 または 経験損失 とは， \\[\n\\widehat{R}_n(h):=\\frac{1}{n}\\sum_{i=1}^n l(h(x_i),y_i)\n\\] をいう．\n差 \\(\\widehat{R}_n(h)-R(h)\\) を 汎化ギャップ という．\n\n\n\n\nアルゴリズム \\(A\\) にとって，汎化誤差は不可知であるが，訓練誤差は計算可能である．データが独立同分布に従うとする場合，経験損失は予測損失の不偏推定量であり，9 \\(n\\to\\infty\\) の漸近論もすでに準備が出来ている．\n従って，不可知である予測損失の最小化の代わりに，経験損失を最小化する予測器 \\[\n\\operatorname{ERM}_\\mathcal{H}(S_n)\\in\\mathop{\\mathrm{arg\\,min}}_{h\\in\\mathcal{H}}R_n(h)\n\\] を構成すれば良い，という指針があり得る．この枠組みを 経験リスク最小化 (Empirical Risk Minimization) といい，PAC 学習は，この ERM の枠組みがどれほどの意味で正しいかの定量的な検証になっている．\n\n\n1.3 経験リスク最小化の問題\nERM は一見，過学習の問題を孕んでいるように思える．\nそこで，あらかじめ学習者 \\(A\\) の値域 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) を制限することを考える．これを 帰納バイアス といい，正則化などの方法によって達成される．\nしかし，この漸近論が提供してくれない消息は複数ある．\n\n機械学習においては，仮説 \\(h\\) 自体がデータから決まる確率変数 \\(h_{S_n}:\\Omega\\to\\mathcal{H}\\) である場合が多い．これを考慮した収束が欲しい．\n\\(n\\) が有限の場合に非漸近論的消息が欲しい．\n\nそこで以降は，アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) を通じて，\\(h_{S_n}:=A(S_n)\\) と定まるとし，\\(h_{S_n}\\) を単に \\(h\\) ともかき，これをデータの関数とする．\nこの下で，\\(\\widehat{R}(h_{S_n})\\) と \\(R(h_{S_n})\\) の関係を考える．\n\n\n\n\n\n\n損失と誤差の区別\n\n\n\n\n\n(金森敬文, 2015, p. 13) では，（決定論的な）仮説 \\(h\\in\\mathcal{H}\\) に関して，\\(R(h)\\) を損失，データから決まる仮説 \\(h_{S_n}=A(S_n)\\) に関して，\\(\\operatorname{E}[R(h_{S_n})]\\) をリスクと呼び分けている．\n損失のうち，特に 0-1損失 \\[\nl=1_{\\Delta_\\mathcal{Y}^\\complement}\n\\]\nに関するものを誤差といい，この２語は殆ど交換可能な形で使う．その期待値をリスクと言う，という使い分けは一つ筋が通りそうである．\nただし，(Alquier, 2024), (Bousquet & Elisseeff, 2002), (Shalev-Shwartz & Ben-David, 2014) はいずれもリスクと誤差を交換可能な概念としている．\n\n\n\n\n\n1.4 PAC 学習\n機械学習を形式化する数理的枠組みのうち，PAC 学習 とは，\n\nProbably Approximately Correct Learning\n\nの略であり，(Valiant, 1984) によって提案されたものである．\n機械学習における哲学的な問題として，「そもそも不可知なリスク \\(R(h)\\) を最小化できるのか？」「できるとしたら，どのような場合においてか？」というものがあった．10\n\n\n\n\n\n\nagnostically PAC Learnable11\n\n\n\n\n定義 2 集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が（不可知論的な意味で） PAC 学習可能 であるとは，ある関数 \\[\nm_\\mathcal{H}:(0,1)^2\\to\\mathbb{N}\n\\] とアルゴリズム \\[\nA:(\\mathcal{X}\\times\\mathcal{Y})^{&lt;\\omega}\\to\\mathcal{H}\n\\] が存在して，任意の \\(\\epsilon,\\delta\\in(0,1)\\) と \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に対して，\\(m_\\mathcal{H}(\\epsilon,\\delta)\\) よりも多くの i.i.d. サンプルが存在すれば，\\(1-\\delta\\) 以上の確率で， \\[\nR(A(S_m))\\le\\min_{h\\in\\mathcal{H}}R(h)+\\epsilon\\quad(m\\ge m_\\mathcal{H}(\\epsilon,\\delta))\n\\] が成り立つことをいう．\n\n\n\nPAC 学習とは，分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に依らない真の誤差の評価を，確率論的に与えることを目的としており，Probably Approximately Correct の名前はその様子を端的に表現している．\n(Valiant, 1984) による PAC 学習可能性の定義には，計算量と計算時間の制約も入っていた．12 (Haussler & Warmuth, 1993, p. 292) によれば，PAC 学習の枠組みにより，計算効率性の研究者が，機械学習のアルゴリズムにも目を向け，協業を始めるきっかけになったとしている．\n\n\n\n\n\n\nagnostically PAC Learnable13\n\n\n\n\n定理 1 仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が有限ならば，（不可知論的な意味で）PAC 学習可能である．\n\n\n\n\n\n1.5 定理 1 の証明\n仮説集合 \\(\\mathcal{H}\\) が 一様収束性 を持つことを示せば良い，というように議論する．\nPAC 学習可能性（ 定義 2 ）は純粋に真の誤差の議論であるが，訓練誤差との関係に注目して示すのである．\n\n\n\n\n\n\n\\(\\epsilon\\)-representative, uniform convergence property14\n\n\n\n\n定義 3  \n\n訓練データ \\(S_n=\\{x_i\\}_{i=1}^n\\) が \\(\\epsilon\\)-代表的 であるとは，次を満たすことをいう： \\[\n\\lvert\\widehat{R}_n(h)-R(h)\\rvert\\le\\epsilon\\quad(h\\in\\mathcal{H}).\n\\]\n仮説集合 \\(\\mathcal{H}\\) が 一様収束性 を持つとは，任意の \\(\\epsilon,\\delta\\in(0,1)\\) と \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) について，十分大きな訓練データ \\(S_m\\) を取れば，\\(1-\\delta\\) 以上の確率で \\(S_m\\) は \\(\\epsilon\\)-代表的であることをいう．\nこのときのサンプル数の増加の速さを \\(m_\\mathcal{H}^{\\mathrm{UC}}(\\epsilon,\\delta)\\) と書く．\n\n\n\n補題 1 訓練データ \\(S_n\\) が \\(\\epsilon/2\\)-代表的ならば，任意の経験リスク最小化学習器 \\[\nh_{S_n}\\in\\mathop{\\mathrm{arg\\,min}}_{h\\in\\mathcal{H}}\\widehat{R}_n(h)\n\\] は \\[\nR(h_{S_n})\\le\\min_{h\\in\\mathcal{H}}R(h)+\\epsilon\n\\] を満たす．\n特に，\\(m_\\mathcal{H}^{\\mathrm{UC}}(\\epsilon,\\delta)\\) に関して一様収束性を持つならば，\\(m_\\mathcal{H}(\\epsilon/2,\\delta)\\le m_\\mathcal{H}^{\\mathrm{UC}}(\\epsilon,\\delta)\\) に関して（不可知論的な意味で）PAC 学習可能である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(h_{S_n}\\) の最小性に注意して，\n\\[\n\\begin{align*}\n    R(h_{S_n})&\\le\\widehat{R}_n(h_{S_n})+\\frac{\\epsilon}{2}\\\\\n    &\\le\\widehat{R}_n(h)+\\frac{\\epsilon}{2}\\\\\n    &\\le R(h)+\\epsilon.\n\\end{align*}\n\\]\n\n\n\nこうして，PAC 学習の枠組みは，（今回のケースでは）ERM の枠組みを肯定する結果を導いている．\nこれは，一様収束性が成り立つ仮説集合 \\(\\mathcal{H}\\) については，経験リスクは真のリスクに十分近いことを意味している．このような \\(\\mathcal{H}\\) を Glivenko-Cantelli クラス (Glivenko, 1933), (Cantelli, 1933) ともいう．15\nこうして，\n\n\n1.6 PAC 学習の基本定理\n実は，分類問題においては，一様収束性は，PAC 学習可能性を特徴付ける．\nこの証明は，VC 次元 (Vapnik & Chervonenkis, 1971) の概念による．\nしかし，一般の学習問題においても同じ状況というわけではない (Shalev-Shwartz et al., 2010)．多クラス分類でさえ同値性は崩れる (Daniely et al., 2011)．\n\n\n\n\n\n\nFundamental Theorem of Statistical Machine Learning16\n\n\n\n\n定理 2 分類問題 \\(\\mathcal{Y}=2\\) を 0-1 損失 \\(l=1_{\\Delta_\\mathcal{Y}^\\complement}\\) で考えるとする．仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) について，次は同値：\n\n\\(\\mathcal{H}\\) は一様収束性を持つ．\n\\(\\mathcal{H}\\) は（不可知論的な意味で）PAC 学習可能である．\n\\(\\mathcal{H}\\) は有限な VC 次元を持つ．\n\n\n\n\n\n\n1.7 Bayes ルール\n\n\n\n\n\n\nBayes error, Bayes rule17, excess risk / regret\n\n\n\n\n定義 4 損失関数 \\(l\\) に対して， \\[\n\\begin{align*}\n    R^*&:=\\inf_{h\\in\\mathcal{L}(\\mathcal{X};\\mathcal{Y})}R(h)\\\\\n    &=\\inf_{h\\in\\mathcal{L}(\\mathcal{X};\\mathcal{Y})}\\mathbb{E}[l(h(X),Y)]\n\\end{align*}\n\\] を Bayes 誤差 という．仮に右辺の下限が達成される \\(h^*\\in\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が存在するとき，これを Bayes 最適学習則 またはベイズルール という． \\[\n\\mathcal{E}(h):=R(h)-R^*\n\\] を 超過損失 という．\n\n\n\n\n\n\n\n\n\nBayes 規則の例18\n\n\n\n\n\n\\(\\mathcal{Y}=2\\) の場合，任意の \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に対して， \\[\nh^*(x):=\\begin{cases}\n1&\\mathbb{P}[Y=1\\,|\\,X=x]\\ge\\frac{1}{2},\\\\\n0&\\mathrm{otherwise}\n\\end{cases}\n\\] は Bayes 最適学習則である．\n\n\n\n\\[\n\\begin{align*}\n    \\mathcal{E}(\\widehat{h}_S)&=R(\\widehat{h}_S)-R(h^*)\\\\\n    &=\\biggr(R(\\widehat{h}_S)-\\inf_{h\\in\\mathcal{H}}R(h)\\biggl)+\\biggr(\\inf_{h\\in\\mathcal{H}}R(h)-R(h^*)\\biggl).\n\\end{align*}\n\\] 第一項を 推定誤差，第二項を 近似誤差 という．19\nここから，\\(\\overline{h}\\) を \\(\\inf_{h\\in\\mathcal{H}}R(h)\\) を達成する oracle machine とすると，推定誤差はさらに２項に分解して評価できる： \\[\n\\begin{align*}\n    &R(\\widehat{h}_n)-\\inf_{h\\in\\mathcal{H}}R(H)\\\\\n    &=R(\\widehat{h}_n)-R(\\overline{h})\\\\\n    &=\\underbrace{\\widehat{R}_n(\\widehat{h}_n)-\\widehat{R}_n(\\overline{h}_n)}_{\\le0}+R(\\widehat{h}_n)-\\widehat{R}_n(\\widehat{h}_n)+\\widehat{R}_n(\\overline{h})-R(\\overline{h})\\\\\n    &\\le\\biggl|\\widehat{R}_n(\\widehat{h}_n)-R(\\widehat{h}_n)\\biggr|+\\biggl|\\widehat{R}_n(\\overline{h})-R(\\overline{h})\\biggr|.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#統計的決定理論",
    "href": "posts/2024/AI/Theory.html#統計的決定理論",
    "title": "統計的学習理論１",
    "section": "2 統計的決定理論",
    "text": "2 統計的決定理論\nPAC 学習の枠組みを相対的に理解するため，統計的決定理論の目線から，同じ形式を見直してみる．\n\n2.1 枠組み\n最大の違いは，データ生成分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) にパラメトリックな仮定をおく点である．このとき，組 \\((\\mathcal{X}\\times\\mathcal{Y},(\\mathbb{P}_\\theta)_{\\theta\\in\\Theta})\\) を 統計的実験 ともいう．\n損失関数 \\(l:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}_+\\) は，より一般には，決定空間 \\(\\mathcal{Z}\\) に対して， \\[\nl:\\mathcal{Y}\\times\\mathcal{Z}\\to\\mathbb{R}_+\n\\] と定まるものである．\n\n\n2.2 一様最強力検定\n学習ではなく，検定の文脈では，PAC 同様全てのデータ生成分布 \\(\\mathbb{P}\\in\\mathcal{P}()\\) を考えるが，リスクが小さいことを要請する．"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#pac-bound",
    "href": "posts/2024/AI/Theory.html#pac-bound",
    "title": "統計的学習理論１",
    "section": "3 PAC bound",
    "text": "3 PAC bound\n\n3.1 定理\n\n\n\n\n\n\nPAC bound20\n\n\n\n\n定理 3 仮説集合 \\(\\mathcal{H}\\) が有限であるとする：\\(\\#\\mathcal{H}=:M&lt;\\infty\\)． このとき，任意の \\(\\epsilon\\in(0,1)\\) について， \\[\n\\mathbb{P}\\left[\\forall_{h\\in\\mathcal{H}}\\;R(h)-\\widehat{R}(h)\\le C\\sqrt{\\frac{\\log\\frac{M}{\\epsilon}}{2n}}\\right]\\ge1-\\epsilon.\n\\]\n\n\n\n仮説 \\(\\mathcal{H}\\) の数 \\(M\\) を増やすごとに，訓練データ数 \\(n\\) は \\(\\log M\\) のオーダーで増やす必要がある，ということになる．\n\n\n3.2 \\(\\biggl|\\widehat{R}_n(\\widehat{h}_n)-R(\\widehat{h}_n)\\biggr|\\) の評価\n\n\n3.3 \\(\\biggl|\\widehat{R}_n(\\overline{h})-R(\\overline{h})\\biggr|\\) の評価\n\n\n3.4 定理の一般化21\n\n一般の \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) への拡張は，VC次元の理論を用いて行われる（ 定理 2 など）．\nバウンドの変形に，Rademacher 複雑性も使われる．\n現実との乖離：現代の深層学習では \\(M\\) が極めて大きくなり，PAC 不等式はほとんど意味をなさない．これを包括できる理論が試みられている．"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#footnotes",
    "href": "posts/2024/AI/Theory.html#footnotes",
    "title": "統計的学習理論１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Mohri et al., 2018, pp. 9–10) と (Shalev-Shwartz & Ben-David, 2014, pp. 13–14), (Alquier, 2024) を参考にした．↩︎\nこれは訓練セット (training set) ともいう (Shalev-Shwartz & Ben-David, 2014, p. 14)．↩︎\n(Bousquet & Elisseeff, 2002) の用語に一致する．(Alquier, 2024, p. 2) では，\\(\\mathcal{X}\\) を object set，\\(\\mathcal{Y}\\) を label set と呼んでいる．(Shalev-Shwartz & Ben-David, 2014, pp. 13–14) は \\(\\mathcal{X}\\) を domain set，\\(\\mathcal{Y}\\) を label set と呼ぶ．↩︎\n(Valiant, 1984) では，\\(\\mathcal{Y}=2\\) の場合，元 \\(h\\in\\mathcal{H}\\) を 概念 (concept) ともいう．その他の場合を predicate とも呼んでいる．(Shalev-Shwartz & Ben-David, 2014, p. 14) では predictor, predictino rule, classifier とも呼ぶとしている．↩︎\n(Alquier, 2024, p. 177) を参考にした．\\(\\Delta_\\mathcal{Y}:=\\left\\{(y',y)\\in\\mathcal{Y}^2\\mid y=y'\\right\\}\\) を対角集合とした．↩︎\n(Alquier, 2024) 第4章ではこの i.i.d. 仮定を外している．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 24) などでは，\\(\\mathcal{Y}=2\\) として分類問題を考えていることもあり，専らこの損失を考えている．↩︎\n(Alquier, 2024, p. 4), (Mohri et al., 2018, p. 10)，(金森敬文, 2015, p. 7) を参考にした．(Shalev-Shwartz & Ben-David, 2014, p. 14) でも，generalization error, risk, error，さらには loss のいずれの名前でも呼ぶし，training error と empirical error /risk とも交換可能に使う，としている．↩︎\n(Mohri et al., 2018, pp. 10–11), (金森敬文, 2015, p. 8) など．↩︎\n(Haussler & Warmuth, 1993, p. 263) にある Valiant 本人による解説に，その哲学的なモチベーションがよく表れている．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 25) 定義3.3，(Mohri et al., 2018, p. 22) 定義2.14 など．元々の (Valiant, 1984) の定義では，\\(m\\) と計算時間の増加レートは \\(1/\\ep,1/\\delta\\) の多項式以下であるという制限もあった．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 28) も参照．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 34) 系4.6 など．↩︎\n(Shalev-Shwartz & Ben-David, 2014, pp. 31–32) 定義3.1 と 定義3.3．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 35) など．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 48) 定理6.7 など．↩︎\n(Mohri et al., 2018, p. 22) 定義2.15，(金森敬文, 2015, p. 9) を参考．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 25) など．↩︎\n(金森敬文, 2015, p. 17) を参考．↩︎\n(Alquier, 2024, p. 7) 定理1.2 など．↩︎\n(Devroye et al., 1996) 第11, 12章 参照．(Vapnik, 1998)．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory4.html",
    "href": "posts/2024/AI/Theory4.html",
    "title": "統計的学習理論４",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n機械学習モデルの社会実装が進むにつけて，経験リスク最小化 の枠組みでは足りず，さらに汎化性能を重要視した枠組みが必要になってくる．\n構造的リスク最小化 もその例であるが，基盤モデル の台頭を見た現代では，分布外リスク最小化 (IRM: Invariant Risk Minimization) という新たな枠組みが (Arjovsky et al., 2020) により提案されている．"
  },
  {
    "objectID": "posts/2024/AI/Theory4.html#ドメイン汎化-wang2023",
    "href": "posts/2024/AI/Theory4.html#ドメイン汎化-wang2023",
    "title": "統計的学習理論４",
    "section": "1 ドメイン汎化 (Wang et al., 2023)",
    "text": "1 ドメイン汎化 (Wang et al., 2023)\n現状の多くの理論と手法は，訓練データとテストデータは同じ標本を分割したものにすぎず，同じ分布に従うことを前提としている．しかし新たに生じた多くの応用の場面では，新たな分布に対しての汎化性能が特に肝要である．\nこれに対処するために，複数のドメインを用意し，未知のドメインに対する汎化性能を高めたいとする問題を ドメイン汎化 (domain generalization) または 分布外汎化 (out-of-distribution generalization) と呼ぶ．1\n\n1.1 枠組み\nドメイン汎化では，入力空間 \\(\\mathcal{X}\\) と出力空間 \\(\\mathcal{Y}\\) は固定されている．\nある分布 \\(\\mathbb{P}\\sim\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) からの独立同分布列 \\(\\mathcal{S}=\\{(x_i,y_i)\\}_{i=1}^N\\subset\\mathcal{X}\\times\\mathcal{Y}\\) を ドメイン という．\nドメイン汎化は，複数のドメイン \\(\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_K\\) が与えられた状態から，まだ見ぬドメイン \\(\\mathcal{S}\\) におけるリスクの最小化を目指す問題である．\n\n\n1.2 関連する話題\n\nマルチタスク学習 (Caruana, 1997)\n複数のタスクにおいて同時に良い性能を出すモデルを学習する枠組み．複数のドメイン \\(\\mathcal{S}_1,\\cdots,\\mathcal{S}_K\\) において平均的に良い性能を出すことを目指す，などの問題も含む．\n転移学習 (Zhuang et al., 2021)\n始域タスクと終域タスクが異なる場合の学習を指す．終域タスクが既知であるという前提があり，事前学習-事後調整 (pretraining-finetuning) という手法が最も一般的である．\nドメイン適応 (M. Wang & Deng, 2018)\n特にドメインが異なる場合の転移学習を指す．終域ドメインが既知であるという点がドメイン汎化と異なる．\nメタ学習 (Vanschoren, 2018), (Hospedales et al., 2022)\n新たなタスクに対して「学習法を学習する」というメタ的な学習を目指す．ドメイン汎化は同じタスクでドメインを変えたものに対する汎化を目指すため，メタ学習はドメイン汎化における有力な手法の一つということになる．\n継続学習 (continual / lifelong learning) (Biesialska et al., 2020)\n例示なし学習 (zero-shot learning)\n例示なしで新たなクラスに対する分類を行う問題．ドメイン汎化はクラスは同じで分布のみが異なる．\n\n\n\n1.3 ドメイン汎化の手法\n大きく分けて次の３通りの手法が存在する．\n\n表現学習 (representation learning)\n最も主要なアプローチは，ドメイン汎化に適した特徴空間をデザインすることである．主に次の２つの接近がある．\n\nドメイン不変な表現学習 (domain-invariant representation learning) を行うことを考える．主な手法には 分布外リスク最小化 による学習や，敵対的学習による方法 (Ganin et al., 2016) などがある．\n特徴分離 (feature disentanglement) により，ドメインに依存しない特徴とドメイン依存の特徴とを分離する．\n\nデータ操作 (data manipulation)\n同じくドメイン汎化に適した特徴空間をデザインするのが目的であるが，これを データ拡張 やデータの生成によって達成することを目指すこともできる．\n学習枠組み (learning paradigm)\n集合学習 (ensemble learning) や メタ学習 などのように，学習のアプローチから変えることも考えられる．\n\n\n1.3.1 分布外リスク最小化\nドメイン汎化が失敗する理由の一つに，因果関係がないが相関関係がある要素（擬似相関）を学習して予測に使ってしまうことがある．\nこの問題は分布外リスク最小化 (Arjovsky et al., 2020) によって対処できることが実験的に示されており，近年理論的な解明 (Toyota & Fukumizu, 2024) も進んでいる．"
  },
  {
    "objectID": "posts/2024/AI/Theory4.html#footnotes",
    "href": "posts/2024/AI/Theory4.html#footnotes",
    "title": "統計的学習理論４",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Wang et al., 2023) がドメイン汎化に対する最初のサーベイである．(Wang & Chen, 2023, p. 175) 11章 も参照．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory2.html",
    "href": "posts/2024/AI/Theory2.html",
    "title": "統計的学習理論２",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/AI/Theory2.html#pac-bayes",
    "href": "posts/2024/AI/Theory2.html#pac-bayes",
    "title": "統計的学習理論２",
    "section": "1 PAC-Bayes",
    "text": "1 PAC-Bayes\n通常の機械学習の枠組みでは，仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) を固定し，この中で最適な推定量 \\(\\overline{h}\\in\\mathcal{H}\\) を探すことに集中する．\n一方で，PAC-Bayes では，仮説集合 \\(\\mathcal{H}\\) 上の確率分布を学習し，最終的に投票 (vote) などの確率的な操作によって決めることを考え，これにも対応する理論を構築する．1\nこれは (Shawe-Taylor & Williamson, 1997) によって創始され， (McAllester, 1999) によって最初の定理が示された．(Seeger, 2002), (Catoni, 2007) も金字塔であり，後者は情報統計力学との関連を推し進めている．\n\n1.1 枠組み\nデータにより決まる確率測度 \\[\n\\widehat{\\rho}:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{P}(\\mathcal{H})\n\\] を考え，推定量をランダムに \\(\\widetilde{h}\\sim\\widehat{\\rho}\\) とサンプリングする．これを ランダム推定量 (randomized estimator) という．\n例えば \\(\\mathcal{Y}=2\\) においては，Gibbs 判別器と呼ばれる．2\nまた，最終的な推定量を積分により \\[\nh_{\\widehat{\\rho}}:=(\\widehat{\\rho}|h)\n\\] と決定しても良い．これを 集合推定量 (aggregated predictor) という．\nこれらの\n\n経験バウンド (empirical bound)：\\(R(\\widehat{h})-\\widehat{R}_n(h^*)\\)\n超過リスクバウンド (excess risk / oracle PAC bound)：\\(R(h_{\\widehat{\\rho}})-R(h^*)\\)\n\nを調べるのが PAC-Bayes である．\n\n\n1.2 KL-乖離度\nすると，\\(\\log M\\) の項に KL-乖離度が現れる．\n\n\n\n\n\n\nTip\n\n\n\n\n定義 1 (Kullback-Leibler divergence) \\(\\mu,\\nu\\in\\mathcal{P}(\\mathcal{H})\\) の Kullback-Leibler 乖離度 とは， \\[\n\\mathop{\\mathrm{KL}}(\\mu|\\nu):=\\begin{cases}\n\\int_\\mathcal{H}\\log\\left(\\frac{d \\mu}{d \\nu}(\\theta)\\right)\\mu(d\\theta)&\\mu\\ll\\nu,\\\\\n\\infty&\\mathrm{otherwise}.\n\\end{cases}\n\\] をいう．\n\n\n\n\n\n1.3 McAllester バウンド\n\n1.3.1 応用\nSGD で訓練されたニューラルネットワークに対しても適用されている (Clerico et al., 2023)．\n\n\n\nPAC-Bayes による汎化バウンド (Dziugaite & Roy, 2017)\n\n\n事後分布からサンプリングをすることで鋭い評価を得ている (Ujváry et al., 2023)．"
  },
  {
    "objectID": "posts/2024/AI/Theory2.html#footnotes",
    "href": "posts/2024/AI/Theory2.html#footnotes",
    "title": "統計的学習理論２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Alquier, 2024) Introduction より．↩︎\n(Schölkopf & Smola, 2002, p. 381) 定義12.23．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html",
    "href": "posts/2024/Computation/PGM2.html",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html#統計力学概観",
    "href": "posts/2024/Computation/PGM2.html#統計力学概観",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "1 統計力学概観",
    "text": "1 統計力学概観\n\n1.1 興りと運動論的方法\n統計力学は，マクロ観測量をミクロ情報のみから予測することを最大の目的とする．その成果の１つとして，マクロ観測量の揺らぎについても，より細かい分析を可能とする．1 特に物理的過程を確率過程としてもモデリングする非平衡統計力学は，粗視化と確率化が対応していることを示唆する．2\n\nThe aim of statistical mechanics is to predict the relations between the observable macroscopic properties of the system, given only a knowledge of the microscopic forces between the components. (Baxter, 1982, p. 1)\n\n(Boltzmann, 1872) による方程式 \\[\nS=k\\log W\n\\] は Max Planck が 1900 年前後にまとめたものである．3\n\n統計力学はこの１行から生まれたといってもよかろう． (戸田盛和 et al., 2011, p. vi)\n\nこれは希薄気体の熱力学を，ミクロの対象に関する密度関数の時間変化を記述することで基礎付けたもので，この接近を一般に 運動論的方法 (kinetic method) という．4\nこの手法がそのまま統計熱力学一般の基礎付けとはならず，その抽象的な枠組みは，後に (Gibbs, 1902) によって与えられた．5\nなお，運動論的方法は，統計物理学の端緒であると同時に，現在でも十分に理解されているとは言い難く，機械学習などとも結びつきながら現在でも活発に研究が進められている分野である (Villani, 2002)．\n\n\n1.2 磁性体の物理\n多くの物質は，外部磁場 \\(H&gt;0\\) を印加すると磁化する：\\(M(H)&gt;0\\)．これを 常磁性体 (paramagnetism) という．これは相互作用が無視できる要素から出来ているためと言える．そのため，理想体系 とも呼ばれる．6\n一方で鉄などの強磁性体では，外部磁場を印加したのち，これを取り除いても磁性が残り，これを 自発磁化 という：\\(M_0:=M(0)&gt;0\\)． 7\n一方で，磁場を反転させたとき，逆向きに磁荷されるから，\\(M(-H)=-M(H)\\) というべきである．すなわち，強磁性体は \\(H=0\\) において相転移を起こす．8\n強磁性体には (Weiss, 1907) が一定の理解を与え，これが理論が与えられた最初の秩序無秩序問題である．9 その後多くの模型が提案されているが，Ising 模型を初め，１次元の場合と２次元の特別な例のいくつかを除いて，相転移の厳密な理論は存在しない．10\nしかし強磁性体も，一定の温度以上では自発磁化を失い，常磁性体と同じように振る舞う．すなわち，\\(M\\) は \\(H=0\\) でも連続になる．これを Curie温度 といい，\\(T_C\\) で表す．11\n理論的には，\\(T=T_C\\) の場合，\\(H=0\\) では連続であるが傾きが発散して可微分ではないとする．\n物体の磁化 \\(H\\) を \\(T,H\\) の２変数関数と見做した場合，線分 \\([0,T_C]\\) を除いた領域で \\(H\\) は滑らかである，とも解釈できる．点 \\((T_C,0)\\) を 臨界点 (critical point) という．\n\\[\nM_0(T)=\\lim_{H\\to 0+}M(H,T).\n\\]\n\n\n1.3 磁化率\n磁化率を \\[\n\\chi(H,T):=\\frac{\\partial M(H,T)}{\\partial H}\n\\] と定義する．変数変換 \\(t=\\frac{T-T_C}{T_C}\\) も用いる．すると臨界点は \\((H,t)=(0,0)\\) にある．\n磁化率 \\(\\chi\\) は \\((H,t)=(0,0)\\) に，近づき方によって異なる非整数位数の極 \\[\n\\frac{\\chi(0,T)}{t^{-\\gamma}}=O(1)\\quad(t\\to0+),\n\\] \\[\n\\frac{\\chi(0,T)}{(-t)^{-\\gamma'}}=O(1)\\quad(t\\to0-)\n\\] を持つことが知られており，その指数 \\(\\gamma,\\gamma'\\) を 臨界指数 (critical exponents) という．12\nこの臨界指数は，一部のモデルでは斉次になる：\\(\\gamma=\\gamma'\\) ことを，(Griffiths, 1967) は２次と３次の Ising モデルで例を構成した．\nまた，臨界指数は具体的な Hamiltonian \\(H\\) の関数形には殆ど依らないと考えられている (Fisher, 1966)．これを 普遍性 という．13\n\n\n1.4 最近傍 Ising モデル\nIsing モデルは強磁性体の磁性体への相転移を記述するために (Lenz, 1920) によって導入された．命名は Lenz の指導の下で完成を見た Ising の博士論文 (Ising, 1925) から (Peieris, 1936) がつけたものであるが，Ising 本人は Lenz-Ising model としていた．14\nIsing モデルは格子点 \\(\\Lambda\\subset\\mathbb{Z}^d\\) 上の無向グラフ \\(G=(\\Lambda,\\mathcal{E})\\) 上に定義される．多くの場合 \\[\n\\Lambda:=B(n):=\\{-n,\\dots,n\\}^d,\n\\] \\[\n\\mathcal{E}:=\\biggl\\{\\{x,y\\}\\subset\\Lambda\\:\\bigg|\\:\\|x-y\\|_1=1\\biggr\\},\n\\] とする．\nIsing 模型最大の単純化として，各頂点を２値変数と同一視する．すなわち，ミクロ状態の全体を \\[\n\\Omega:=\\{\\pm1\\}^\\Lambda\\simeq_\\mathrm{Set}P(\\Lambda)\n\\] とし，配置集合 (configuration) ともいう．15\nこの上に Hamiltonian を \\[\n\\begin{align*}\n    H_{\\Lambda,h}(\\omega)&:=H_0(\\omega)+H_1(\\omega)\\\\\n    &:=-\\sum_{(i,j)\\in\\mathcal{E}}J_{ij}\\omega_i\\omega_j-h\\sum_{i\\in\\Lambda}\\omega_i\n\\end{align*}\n\\] と定義する．16 \\(h\\) は外部磁場である．Hamiltonian の値をエネルギーともいう．\nこの Hamiltonian を 最近傍 Ising 模型 (nearest-neighbor Ising model) という．相互作用を表す偶関数 \\(H_0\\) の関数系をもっと一般的に取ることで，より一般的な Ising 模型も構成される．17\n\\(J&gt;0\\) の場合，隣り合うスピンが揃っている方が Hamiltonian は小さいため，これは強磁性体の模型になっていると言える．18\nこの最近傍 Ising 模型についても，３次元以上の場合と，２次元で \\(h\\ne0\\) の場合はまだ解かれていない．19\nこの Hamiltonian \\(H_{\\Lambda,h}\\) が \\(\\Omega\\) 上に定める Gibbs 分布 とは，20 \\[\n\\mu_{\\Lambda;\\beta,h}(\\omega)\\propto e^{-\\beta H_{\\Lambda;h}(\\omega)}\n\\] をいう．規格化定数は \\[\nZ_{\\Lambda}(\\beta,h):=\\sum_{\\omega\\in\\Omega}e^{-\\beta H_{\\Lambda;h}(\\omega)}\n\\] \\[\n\\beta:=\\frac{1}{kT}\n\\] などと表され，分配関数 と呼ばれる．\nこの最も簡単とも思われる設定の下で，全磁化 \\[\nM_\\Lambda(\\omega):=\\sum_{i\\in\\Lambda}\\omega_i\n\\] または磁化密度 \\[\n\\frac{M_\\Lambda(\\omega)}{N},\\quad N:=\\lvert\\Lambda\\rvert\n\\] を確率変数と見て，Gibbs 分布の上で調べるのである．21\nIsing 模型は流体のモデルとしても用いられ，Ising 模型に従うと仮想される気体を 格子気体 (lattice gas) という．22 \\(J_{ij}\\) には Lennard-Jones ポテンシャルなどが用いられる．\n\n\n1.5 平衡統計力学の枠組み\nこのように，全ミクロ状態 \\(\\Omega\\) とその上の Hamiltonian \\(H:\\Omega\\to\\mathbb{R}\\) を考え，Gibbs 分布 \\(\\mu\\) と分配関数 \\[\nZ:=\\sum_{s\\in\\Omega}e^{-\\frac{H(s)}{kT}}\n\\] を考えるというのが，(Gibbs, 1902) が創始した枠組みである．23\nこうして，平衡統計力学の議論の中心は，Gibbs 分布に関する平均の計算である．24 得られる平均は，絶対温度 \\(T\\) と，\\(H\\) 内の変数（Ising 模型では外部磁場 \\(h\\)）に関する関数になる．\n同時に平衡統計力学の中心問題が明らかになる．それは分配関数 \\(Z\\) の計算が多くの模型では極めて難しいということである．\nこうして，Isign 模型のような簡単な模型から議論するか，平均の値を近似する手法を考えることになるが，特に臨界点付近では後者の方法は行き詰まる．25\n前者も，多くの正確に解かれた模型は，Ising 模型の例と見れる上に，殆どが２次元以下である．26\n\n\n1.6 Curie-Weiss 模型\n一方で，(Weiss, 1907) の理論は，Ising 模型の 平均場近似 に基づいていた．27\n３次元以上では解けていない Ising 模型を，平均場近似で近似的に解くことが出来る．ただし，１次元の場合でも相転移が起こるという都合の悪い面も出てくる．28\n\\(J_{ij}\\equiv J\\) とした最近傍 Ising 模型において， \\[\n-J\\sum_{(i,j)\\in\\mathcal{E}}\\omega_i\\omega_j=-(2dJ\\omega_i)\\cdot\\frac{1}{2d}\\sum_{(i,j)\\in\\mathcal{E}}\\omega_j\n\\] と見て，第２の因子 \\(\\frac{1}{2d}\\sum_{(i,j)\\in\\mathcal{E}}\\omega_j\\) を \\(\\omega_i\\) の周囲の局所的な平均磁化とみなす．\\(2d\\) はちょうど最近傍格子点の数であることに注意．\nこの値を，大域的な磁化密度 \\(\\frac{1}{N}\\sum_{j=1}^N\\omega_j\\)，\\(N:=\\lvert\\Lambda\\rvert\\) に置き換えることを Weiss近似 といい，平均場近似の歴史上最初の例であった．29\nこうして，次の Curie-Weiss Hamiltonian を得る： \\[\nH_{N;J,h}(\\omega):=-\\frac{dJ}{N}\\sum_{i,j=1}^N\\omega_i\\omega_j-h\\sum_{i=1}^N\\omega_i.\n\\]\n最近傍 Ising 模型と違って，積 \\(\\omega_i\\omega_j\\) を取る際の添字 \\(i,j\\) に制約はないから，大域的な相互作用も考えていることになる．ただし，この相互作用には一様性を課しており，粒子の個々の位置関係などは完全に捨象されている．実際，\\(N\\) 位の完全グラフ \\(K^n\\) 上の最近傍 Ising 模型とも見れる．\nそれ故，強相関電子系 など揺らぎの大きな系では，平均場近似では正確な結果は得られない．\n\n\n1.7 バンド理論\nバンド計算における 一電子近似 も平均場近似の例である．30\n固体内の電子は，まず化学結合に寄与する価電子 (valence electron) と原子核に束縛された核電子 (core electron) に分けられる．第一近似として，原子核とその核電子と，価電子とを，固体の独立な２つの構成要素と考えることが多い．31\nさらにはバンド理論では，電子と正孔の間の相互作用を捨象している．32\n\n\n1.8 半導体\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．33\nこのような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．34 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．\n(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，35 その後20世紀に入るとラジオに応用された．Braun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html#markov-確率場",
    "href": "posts/2024/Computation/PGM2.html#markov-確率場",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "2 Markov 確率場",
    "text": "2 Markov 確率場\n(Li, 2009) Chapter 2 も参照．"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html#footnotes",
    "href": "posts/2024/Computation/PGM2.html#footnotes",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Baxter, 1982, p. 1) の他に，(Friedli & Velenik, 2017, p. 18) にも同様の記述がある．↩︎\n(戸田盛和 et al., 2011, p. ix) 「物理的過程を確立的に捉えるとき，根元的なミクロのレベルから出発して，マクロのレベルに到達するには，ものの見方の粗さ (coarse graining) のさまざまな段階がある．それぞれの段階でインフォメーションが失われ，それに応じた確率化が行われる．」↩︎\n(戸田盛和 et al., 2011, p. vi), Eric Weisstein’s World of Physics．これは方程式というよりは，エントロピー（左辺）の数学的定義と読むべきである (Villani & Mouhot, 2015, p. 6)．↩︎\n(戸田盛和 et al., 2011, p. vi)．「ミクロの対象」と言ったが，Maxwell がこの理論を展開した当時 (Maxwell, 1867), (Maxwell, 1878) は原子論もまだ仮説の一つに過ぎなかった (Villani & Mouhot, 2015, p. 3)．↩︎\nstatisitcal ensemble の概念もこのときに導入された．Wikipedia↩︎\n同様の例として Boyle-Charles の法則に従う気体も挙げられている (戸田盛和 et al., 2011, p. 129)↩︎\n(Kittel, 2018, p. 323), (Baxter, 1982, p. 1)．↩︎\n(Baxter, 1982, p. 2)．「相転移を起こす系は一般に構成要素間に相互作用のある系であり，協力系 とも呼ばれ，相転移は 協力現象 と言われることもある」 (戸田盛和 et al., 2011, p. 129)．↩︎\n(戸田盛和 et al., 2011, p. viii)．↩︎\n(戸田盛和 et al., 2011, p. viii), (Baxter, 1982, p. v)．↩︎\n(Kittel, 2018, p. 323), (Huebener, 2019, pp. 176–177)．よく \\(T_C\\) で表される．一方で反磁性体では Néel温度 という．↩︎\n(戸田盛和 et al., 2011, p. 170) も参照．↩︎\n(Baxter, 1982, p. 7)．↩︎\n(Friedli & Velenik, 2017, p. 40)↩︎\n(Preston, 1974, p. 1) も参照．\\(B(n)\\) の \\(n\\to\\infty\\) の場合などが，熱力学的極限の例である． (Friedli & Velenik, 2017, p. 43)．↩︎\n(Friedli & Velenik, 2017, p. 42), (Baxter, 1982, p. 15) など参照．↩︎\n(Baxter, 1982) の1.7節が一般 Ising 模型，1.8節が最近傍 Ising 模型．↩︎\n(Friedli & Velenik, 2017, p. 42), (Baxter, 1982, p. 21) 参照．↩︎\n(Baxter, 1982, p. 21), (Friedli & Velenik, 2017, p. 60)．↩︎\n\\(\\Omega\\) 上の分布は 状態 ともいう (Preston, 1974, p. 2)．↩︎\n(Baxter, 1982, p. 17) も参照．↩︎\n(Baxter, 1982, p. 24) 1.9節, (戸田盛和 et al., 2011, p. 131)．↩︎\n(Baxter, 1982, p. 8)．↩︎\n(Baxter, 1982, p. 9)．↩︎\n(Baxter, 1982, p. 11)↩︎\n(Baxter, 1982, p. 14)．↩︎\n現在でこそ平均場近似と呼ばれるが，(Weiss, 1907) は分子場近似と呼んでいた．平均場近似の最初の例である．↩︎\n(戸田盛和 et al., 2011, p. 167) 参照．↩︎\n(戸田盛和 et al., 2011, p. 162), (Friedli & Velenik, 2017, pp. 60–61) も参照．二元合金においては Bragg-Williams 近似ともいう．↩︎\n(Madelung, 1978, p. 10)．↩︎\n(Madelung, 1978, p. 6)．↩︎\n(Madelung, 1978, p. 118)．↩︎\n(Böer & Pohl, 2018, p. 4), (Huebener, 2019, p. 73) Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う (Madelung, 1978, p. 17)．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である (Madelung, 1978, p. 211)．↩︎\n(Huebener, 2019, p. 75)．↩︎\n(Huebener, 2019, p. 73) 特に伝導体と半導体の境界部分で強く見られた．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/PGM.html",
    "href": "posts/2024/Computation/PGM.html",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n決定木，状態空間モデル，ニューラルネットワーク (Rumelhart et al., 1987)，構造方程式モデルはいずれもベイジアンネットワークの例と見れる．1\n（２次元以下の）Ising 模型 はマルコフネットワークの例である．\n以上は全て，確率的グラフィカルモデルの例である．\nベイジアンネットワークは，Markov 圏 上の図式のうち，特定のグラフ理論的な条件 2.1.2 を満たすものと見れる．\n(Wainwright & Jordan, 2008), (Chen, 2023) も参考になるであろう．"
  },
  {
    "objectID": "posts/2024/Computation/PGM.html#歴史と導入",
    "href": "posts/2024/Computation/PGM.html#歴史と導入",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "1 歴史と導入",
    "text": "1 歴史と導入\n\n1.1 導入\nグラフィカルモデル とは，多くの変数からなるモデルを，グラフを用いて表現することで，各変数の間の依存性・独立性を明確に表すと同時に，確率モデルを端的に定義する語彙である．2\nこの手法が特に肝要になるのが，自立して推論・意思決定を行うシステムの構築においてである．3 これはこの分野が不確実性を定量的に扱う必要があり，それ故確率的モデリングを必要とするためである．4\n知識表現や系のモデリングのために横断的に用いられる手法が 確率的グラフィカルモデリング である．「確率的」というのは確率論的・統計学的な手法の採用を指す．\n世界に対する知識には不確実性がつきものであり，これを反映した表現がより現実に即したモデルを生むことから，近年盛んに研究・応用されている（第 1.4 節）．5\n人間にとって視覚的にわかりやすいだけでなく，周辺化，極値の計算，条件付き確率の計算を高速化するという点で計算機にとっても極めてわかりやすい表示になる．6\n\nI have approximate answers and possible beliefs with different degrees of uncertainty about different things, but I am not absolutely sure of anything. – Richard Feynman\n\n\n\n1.2 確率的グラフィカルモデルの例\n確率的グラフィカルモデリングの例には，次のようなものがある：\n\n音声認識や天気予報の分野で，対象とは音声言語と地球の大気環境であるが，これらのモデルをグラフで表す際，状態空間モデル がよく用いられる．\n特に状態空間モデルが，Gauss 確率変数とその間の線型な依存関係のみからなるとき，これを（部分的に観測される）線型力学系 または Kalman filter ともいう．状態空間モデルの潜在変数が離散的であるとき，歴史的には隠れ Markov モデルという名前で用いられてきた．7\n医療診断では，複数の症状や検査結果，医学的指標との関連・相関・因果に関する知識を Bayesian Network （節 2.1） で表現する．\n因果推論の分野で，構造的因果モデル は Bayesian Network で表される (Pearl et al., 2016)．この文脈では DAG とも，汎函数因果モデル (Schölkopf, 2022) とも呼ぶ．8\n\n\n\n1.3 諸科学での知識表現の歴史\n多くの科学分野において，知識表現の知識とは，特に因果関係に関する知識のことを指すようである．これを捉えるために，グラフを用いることは自然な発想であり，計算機の登場以前にも，純粋に人間が理解を深めるための用途に，歴史上極めて早い時期から用いられていた．\n\n\n\n\n\n\n歴史9\n\n\n\n\n\n高次元分布において，成分間の独立性をグラフを用いて表現しようという発想は，その計算機との親和性が見つかる前に，種々の科学分野で試みられていた．\n\n(Gibbs, 1902) が統計力学の文脈で，相関粒子系の分布をグラフで表現した．\n(Wright, 1918) は骨格測定のデータを用いた因子分析で，（遺伝的な意味での）依存関係を，パス図と呼ばれる有向グラフを用いて表した．\n(H. Wold, 1954) とその教え子との (Jöreskog & Wold, 1982)，さらに (Blalock Jr., 1971) が社会学において，因果をグラフを用いて表す因子分析法を 構造方程式モデル (SEM: Structural Equation Model) の名前の下に普及させた．10\n経済学では特に 操作変数法 として独自の発展を遂げている．\nその後 (H. O. A. Wold & Strotz, 1960) は (Pearl, 2009) などの do-calculus に繋がっている．これはパス解析や構造方程式モデルのノンパラメトリックな拡張とも見れる．11\n統計学でも (Bartlett, 1935) が分割表分析において変数同士の相関の研究をしたが，界隈が本格的に受け入れたのはやっと 1960 年代以降である．\n\n\n\n\n\n\n1.4 人工知能分野での確率的モデリングの採用\n人工知能分野が確率的手法を採用したのは，エキスパートシステムの構築が志向された 1960 年代であった．12\n医療診断や油源探索における専門家に匹敵する判断力を持つアルゴリズムを構築する途上で，不確実性の度合いの定量化が必要となり，naive Bayes model と呼ばれる確率的モデルが採用された．特に (de Dombal et al., 1972) は限られた分野であるが人間を凌駕する診断正答率を示した．\nだがこの確率的アプローチは，主にその計算複雑性から 1970 年代では冬の時代を経験することとなり，エキスパートシステムも production rule framework や ファジー論理 (Zadeh, 1989) など，確率論に代わって他のアーキテクチャが試みられるようになっていった．\n\n\n1.5 Bayesian Network の登場\nこれを打開したのが\n\n(Pearl, 1988) による Bayesian network framework と，(Lauritzen & Spiegelhalter, 1988) による効率的な推論手法という理論的発展．\n(Heckerman et al., 1992), (Heckerman & Nathwani, 1992) が Bayesian network を病理学標本に応用して大きな成功を挙げたこと．\n\nの2つである．これにより，確率的グラフィカルモデル，また一般に確率的アプローチが広く受け入れられるようになった．\n\n\n1.6 確率的グラフィカルモデリングの美点\n確率的グラフィカルモデリングの美点は，人間（エキスパート）と計算機の協業を促進する共通言語としての働きが出来る点である．\n\n人間と計算機の双方にとって解釈しやすい 表現 である．\n確率的グラフィカルモデルで表現できる分布のクラスと，効率的に Bayes 推論 が可能な分布のクラスとが一致する．13\n人間と計算機の双方がモデリングに参加できる．後者によるモデリングは，学習 とも呼ばれることになる．14\n\nさらに，高次元分布 \\(P\\) の成分間の依存関係を効率よく捉える手法であるため，その背後にあるグラフが判れば，グラフの分離性（ 節 2.2, 節 2.3.5 ）を判定するだけで，\\(P\\) の独立性の情報を得ることが出来る．\n他にも，グラフの構造を用いて，\\(P\\) を効率的に表現し，本質的な次元を大幅に落として計算を効率化することもできる．"
  },
  {
    "objectID": "posts/2024/Computation/PGM.html#代表的なグラフィカルモデル",
    "href": "posts/2024/Computation/PGM.html#代表的なグラフィカルモデル",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "2 代表的なグラフィカルモデル",
    "text": "2 代表的なグラフィカルモデル\n知識のグラフ表現は，有向グラフを用いるか，無向グラフを用いるかによって大きく２つに大別できる．\n\n2.1 Bayesian Network\n\n2.1.1 例：naive Bayes model\nnaive Bayes model は Idiot Bayes model とも呼ばれる Bayesian Network の簡単な例である．\nこれは クラス と呼ばれる離散潜在変数 \\(C\\in\\{c^1,\\cdots,c^k\\}\\) を持つ次のようなモデルである．\n\n\n\nnaive Bayes model\n\n\nこの際，グラフィカルモデルに共通する用語を確認する．\n\nクラスの実現値 \\(c^i\\) を インスタンス と呼ぶ．\n潜在変数の実現値が確定することを，観測 の他に インスタンス化 ともいう．\nインスタンス化されたときに取る値は エビデンス とも呼ばれる．\n\n観測値 \\(X_1,\\cdots,X_n\\) は 特徴 (features) と呼ばれ，これはクラスを与えた下で互いに条件付き独立であるとする： \\[\n(X_i\\perp\\!\\!\\!\\perp\\boldsymbol{X}_{-i}\\mid C)\\;(i\\in[n]),\n\\] \\[\n\\boldsymbol{X}_{-i}:=(X_{1:i-1},X_{i+1:n}).\n\\]\nこうして得る階層モデルを naive Bayes model という．15 その結合密度は \\[\np(c,x_1,\\cdots,x_n)=p(c)\\prod_{i=1}^np(x_i|c)\n\\] と表せる．\n\n\n2.1.2 DAG\n\n\n\n\n\n\n定義16 （Bayesian Network structure）\n\n\n\n\n\n確率変数 \\(\\boldsymbol{X}:=(X_1,\\cdots,X_n)\\) に関する Bayesian Network 構造 とは，成分の全体 \\(\\mathcal{X}:=\\{X_1,\\cdots,X_n\\}\\) を節集合とした 有向非循環グラフ (directed acyclic graph, DAG) \\(\\mathcal{G}=(\\mathcal{X},\\mathcal{E})\\) をいう．\n\n\n\nBayesian network は belief network とも呼ばれる．17 決定分析で用いられる influence diagram / decision network はその一般化である．\n\n\n\n\n\n\n記法（親ノード，子孫ノード，非子孫ノード）\n\n\n\n\n\nグラフ \\(\\mathcal{G}\\) において，\n\n節 \\(X_i\\) からその親節の全体への対応を添字について表現したものを \\[\n\\pi:[n]\\to P([n])\n\\] で表す．\n節 \\(X_i\\) からその子節の全体への対応を添字について表現したものを \\[\n\\des:[n]\\to P([n])\n\\] で表す．\n次の対応を 非子孫ノード という： \\[\n\\nd(i):=[n]\\setminus(\\{i\\}\\cup\\des(i)).\n\\]\n\n\n\n\n\n\n\n\n\n\n定義18 （Directed Local Markov Independence）\n\n\n\n\n\nBayesian Network 構造 \\(\\mathcal{G}\\) が表現する条件付き独立性 \\[\nX_i\\perp\\!\\!\\!\\perp(X_j)_{j\\in\\nd(i)}\\mid (X_j)_{j\\in\\pi(i)}\n\\] を 局所依存性 といい，その（論理式の）全体を \\(\\mathcal{I}_l(\\mathcal{G})\\) で表す．\n\n\n\nBayesian Network が視覚的表現・記号論で，その表現する所の局所依存性が意味論であると言える．\n\n\n2.1.3 Bayesian Network の特徴付け\n\n\n\n\n\n\n定義19 （Independence Assertions）\n\n\n\n\n\n\\(P\\in\\mathcal{P}(\\mathcal{X})\\) をノードの集合 \\(\\mathcal{X}=\\{X_1,\\cdots,X_n\\}\\) 上の確率分布とする．\\((X_i)_{i=1}^n\\sim P\\) に関して成立する条件付き独立性の主張 \\[\n(X_i)_{i\\in I}\\perp\\!\\!\\!\\perp(X_j)_{j\\in J}\\mid (X_k)_{k\\in K}\n\\] \\[\nI\\sqcup J\\sqcup K\\subset[n]\n\\] の（論理式の）全体を \\(P\\) が含意する条件付き独立性 といい， \\(\\mathcal{I}(P)\\) で表す．\n\n\n\n\n\n\n\n\n\n定義20 （\\(I\\)-Map）\n\n\n\n\n\n\\(\\mathcal{I}\\) を確率変数 \\((X_1,\\cdots,X_n)\\) の成分間の条件付き独立性に関する論理式の全体，\\(\\mathcal{K}\\) を DAG とする．\\(\\mathcal{K}\\) が \\(\\mathcal{I}\\) の \\(I\\)-map であるとは， \\[\n\\mathcal{I}(\\mathcal{K})\\subset\\mathcal{I}\n\\] を満たすことをいう．\n\n\n\n\n\n\n\n\n\n定義21 （factorize, chain rule, local probabilistic model, Bayesian Network）\n\n\n\n\n\n\\(\\mathcal{G}\\) を確率変数 \\((X_1,\\cdots,X_n)\\) に関する Bayesian Network 構造とする．\n\n分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) が \\(\\mathcal{G}\\) に従って 分解する とは，\\((X_1,\\cdots,X_n)\\sim P\\) と仮定したとき，次が成り立つことをいう： \\[\n\\mathcal{L}[X_1,\\cdots,X_n]=\\prod^n_{i=1}\\mathcal{L}[X_i|(X_j)_{j\\in\\pi(i)}].\n\\]\nこの式を Bayesian Network \\(\\mathcal{G}\\) の 連鎖律 といい，右辺の因子 \\(\\mathcal{L}[X_i|(X_j)_{j\\in\\pi(i)}]\\) の全体を 条件付き確率分布族 または 局所モデル という．\nBayesian Network 構造 \\(\\mathcal{G}\\) とこれに沿って分解する分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) との組 \\((\\mathcal{G},P)\\) を，Bayesian Network という．\n\n\n\n\n\n\n\n\n\n\n命題22 （Bayesian Network の特徴付け）\n\n\n\n\n\n\\(\\mathcal{G}\\) を確率変数 \\((X_1,\\cdots,X_n)\\) に関する Bayesian Network 構造，\\(P\\in\\mathcal{P}(\\mathcal{X})\\) を確率分布とする．このとき，次は同値：\n\n\\(\\mathcal{G}\\) が \\(\\mathcal{I}(P)\\) の \\(I\\)-map である．\n\\(P\\) は \\(\\mathcal{G}\\) に従って分解する．\n\n\n\n\n\n\n\n2.2 Bayesian Network の分離性\n確率的グラフィカルモデルにおいて肝要なのは，グラフ内に存在する統計的独立性の全てをハイライトする ことである．するとこれを用いて，分布の効率的な表現と，クエリーへの回答を効率的に行うことが出来る．23\n\n2.2.1 ３節グラフの場合\n節が３つ \\(X,Y,Z\\) の場合の DAG は大別して３通り存在する．この場合で「分離性」の概念を説明する．\n３つの成分 \\((X,Y,Z)\\) が依存関係にある状態で，\\(Z\\) が観測された（インスタンス化された）とする．\nその場合に，\\(X,Y\\) 間の因果関係がどう変化するか？を考える．元々因果関係があったところから，24 これが解消されるとき，\\(X,Y\\) は \\(Z\\) を介して \\(d\\)-分離 であるという．25\n\n\n\n\n\n\n逐次結合の場合\n\n\n\n\n\n次のような逐次結合の場合，節 \\(X,Y\\) は，節 \\(Z\\) がインスタンス化されたとき \\(d\\)-分離 である，という．\n\n\n\n\n\n\n\n\nCausalTrail\n\n\n\nX\n\nX\n\n\n\nZ\n\nZ\n\n\n\nX-&gt;Z\n\n\n\n\n\nY\n\nY\n\n\n\nZ-&gt;Y\n\n\n\n\n\n\n\n\n図 1: 逐次結合 (Causal Trail)\n\n\n\n\n\n\\(X\\) を勉強量，\\(Z\\) を素点，\\(Y\\) を GPA とするとき，\\(Z\\) が観測されたならば，もはや勉強量は GPA に影響を与えない．ただし，相関は存在するだろうが．\n\n\n\n\n\n\n\n\n\n分岐結合の場合\n\n\n\n\n\n次のような分岐結合の場合，節 \\(X,Y\\) は，節 \\(Z\\) がインスタンス化されたとき \\(d\\)-分離 である，という．\n\n\n\n\n\n\n\n\nCausalTrail\n\n\n\nZ\n\nZ\n\n\n\nX\n\nX\n\n\n\nZ-&gt;X\n\n\n\n\n\nY\n\nY\n\n\n\nZ-&gt;Y\n\n\n\n\n\n\n\n\n図 2: 分岐結合 (Common Cause)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n合流結合の場合\n\n\n\n\n\n次のような合流結合の場合，節 \\(X,Y\\) は，節 \\(Z\\) またはその子孫節がインスタンス化されなければ，節 \\(Z\\) を介して \\(d\\)-分離 である，という．\n\n\n\n\n\n\n\n\nCausalTrail\n\n\n\nX\n\nX\n\n\n\nZ\n\nZ\n\n\n\nX-&gt;Z\n\n\n\n\n\nY\n\nY\n\n\n\nY-&gt;Z\n\n\n\n\n\n\n\n\n図 3: 合流結合 (Common Effect)\n\n\n\n\n\nこの構造は \\(v\\)-構造ともいう．26 この場合，\\(Z\\) が観測されたならば，\\(X,Y\\) は因果関係を持つようになる．\n\\(Z\\) が事象の有無で，\\(X,Y\\) のいずれかが起こった時に \\(Z\\) も起こるとしよう．いま \\(Z\\) が起こったこと \\(Z=1\\) が判明したとすると，\\(X,Y\\) のいずれか一方も起こっている必要がある．従って，\\(X=0\\) は \\(Y=1\\) を要請するという因果関係が生じる．\n\n\n\n\n\n2.2.2 一般の DAG の場合\n\n\n\n\n\n\n定義27 （active, \\(d\\)-Separated, Directed Global Markov Independencies）\n\n\n\n\n\n\\(\\mathcal{G}\\) を Bayesian Network 構造，\\(\\boldsymbol{Z}\\subset\\mathcal{X}\\) を観測された節とする．\n\n非有向道 \\(X_1\\rightleftharpoons\\cdots\\rightleftharpoons X_n\\) が \\(\\boldsymbol{Z}\\) の下でも active であるとは， 次の２条件を満たすことをいう：\n\n\\(\\{X_i\\}_{i=1}^n\\cap\\boldsymbol{Z}=\\emptyset\\)．\n任意の無向道内の合流結合 \\(X_{i-1}\\rightarrow X_i\\leftarrow X_{i+1}\\) について，\\(X_i\\) またはその子孫に \\(\\boldsymbol{Z}\\) の元が存在する．\n\n\\(\\boldsymbol{X}\\sqcup\\boldsymbol{Y}\\sqcup\\boldsymbol{Z}\\subset\\mathcal{X}\\) を節の集合とする．\\(\\boldsymbol{X},\\boldsymbol{Y}\\) が \\(\\boldsymbol{Z}\\) に関して \\(d\\)-分離 であるとは，任意の \\(X\\in\\boldsymbol{X}\\) と \\(Y\\in\\boldsymbol{Y}\\) と，\\(X,Y\\) を結ぶ無向道が，\\(\\boldsymbol{Z}\\) の下で active でないことをいう．このことを \\(\\dsep_\\mathcal{G}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\) と表す．28\n\\(\\mathcal{G}\\) 内の \\(d\\)-分離な組 \\((\\boldsymbol{X},\\boldsymbol{Y},\\boldsymbol{Z})\\) が表す条件付き独立性の条件式の全体を \\[\n\\mathcal{I}(\\mathcal{G}):=\\left\\{(\\boldsymbol{X}\\perp\\!\\!\\!\\perp\\boldsymbol{Y}|\\boldsymbol{Z})\\mid\\dsep_\\mathcal{G}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\right\\}.\n\\] この元を 大域的独立性 ともいう．\n\n\n\n\n局所依存性（ 節 2.1.2 ）は \\(d\\)-分離性の特別な場合であり，\\(\\mathcal{I}_l(\\mathcal{G})\\subset\\mathcal{I}(\\mathcal{G})\\) である．\n\n\n2.2.3 例\n\n\n\n\n\n\n\n\nExampleTrail\n\n\n\nX\n\nX\n\n\n\nA\n\nA\n\n\n\nX-&gt;A\n\n\n\n\n\nZ\n\nZ\n\n\n\nA-&gt;Z\n\n\n\n\n\nY\n\nY\n\n\n\nB\n\nB\n\n\n\nY-&gt;B\n\n\n\n\n\nB-&gt;Z\n\n\n\n\n\n\n\n\n図 4: 合流結合 (Common Effect)\n\n\n\n\n\nこの Bayesian Network 構造は，いつ \\(d\\)-分離になり，いつ \\(d\\)-分離ではないか？\n\n\n\n\n\n\n答え\n\n\n\n\n\n\nいずれも観測されない場合は \\(d\\)-分離である．\n\\(Z\\) が観測された場合，\\(A,B\\) のいずれかも観測されていれば，やはり \\(d\\)-分離である．\n\n\n\n\n\n\n2.2.4 \\(d\\)-分離性の特徴付け\n\n\n\n\n\n\n命題29 （\\(d\\)-分離性の特徴付け）\n\n\n\n\n\n\\(\\mathcal{G}\\) を Bayesian Network 構造，\\(P\\in\\mathcal{P}(\\mathcal{X})\\) を確率分布とする．\n\n\\(P\\) が \\(\\mathcal{G}\\) に沿って分解するならば，\\(\\mathcal{I}(\\mathcal{G})\\subset\\mathcal{I}(P)\\)．\n\\(\\mathcal{H}\\) に沿って分解する殆ど全ての \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して，上の逆も成り立ち，特に等号が成立する．\n\n\n\n\n\\(\\mathcal{G}\\) が定める分布族について，殆ど全ての分布が共通して持つ条件付き独立性の構造を，\\(\\mathcal{G}\\) から読み取れる \\(d\\)-分離性によって発見できるということになる．\nさらには，分布 \\(P\\) の独立性の情報を知りたい場合，この背後にあるグラフ \\(\\mathcal{G}\\) を探し出して，\\(d\\)-分離性を調べれば良い，ということでもであるのである．30\n\n\n2.2.5 \\(I\\)-同値性\n\n\n\n\n\n\n定義31 （\\(I\\)-Equivalence）\n\n\n\n\n\n２つの Bayesian Network 構造 \\(\\mathcal{G},\\mathcal{G}'\\) が \\(I\\)-同値 であるとは，\\(\\mathcal{I}(\\mathcal{G})=\\mathcal{I}(\\mathcal{G}')\\) が成り立つことをいう．\n\n\n\n\\(I\\) は写像であるから，この関係は確かに Bayesian Network 構造の全体（果てには有向グラフの全体）に同値関係を定める．\n\n\n\n\n\n\n命題32 （\\(I\\)-同値性の十分条件）\n\n\n\n\n\n２つの Bayesian Network 構造 \\(\\mathcal{G},\\mathcal{G}'\\) が\n\n同じスケルトンを持ち，33\n同じ \\(v\\)-構造を持つ\n\nならば，\\(I\\)-同値である．\n\n\n\n有向グラフ \\(\\mathcal{G}=(\\mathcal{X},\\mathcal{E})\\) の辺 \\((X,Y)\\in\\mathcal{E}\\) が 被覆されている とは， \\[\n\\pi(Y)=\\pi(X)\\cup\\{X\\}\n\\] を満たすことをいう．\n合流結合 \\(X\\rightarrow Z\\leftarrow Y\\) において，辺 \\(X\\to Z\\) は被覆されていない．\n\n\n\n\n\n\n命題34 （\\(I\\)-同値性の特徴付け）\n\n\n\n\n\n２つの Bayesian Network 構造 \\(\\mathcal{G},\\mathcal{G}'\\) について，次は同値：\n\n\\(\\mathcal{G},\\mathcal{G}'\\) は \\(I\\)-同値である．\n\\(\\mathcal{G}\\) に \\(I\\)-同値なグラフの列 \\(\\mathcal{G}=\\mathcal{G}_0,\\cdots,\\mathcal{G}_m=\\mathcal{G}'\\) であって，隣り合うグラフ \\(\\mathcal{G}_i,\\mathcal{G}_{i+1}\\;(i\\in m)\\) 同士は，被覆されている辺の向きの反転しか違わないものが存在する．\n\n\n\n\n\n\n\n2.3 Markov Network\n\n2.3.1 グラフ理論の準備\n\\(A\\) を集合とする． \\[\n[A]^k:=\\left\\{B\\in P(A)\\mid\\# B=k\\right\\}\n\\] とする．無向グラフとは集合 \\(V\\) と \\(E\\subset[V]^2\\) の組 \\(G:=(V,E)\\) のことをいう．35\nMarkov Network 構造 とは，任意の無向グラフをいう．\n２つの節 \\(x,y\\in V\\) が 隣接する (adjacent / neighbours) とは，\\(\\{x,y\\}\\in E\\) が成り立つことをいう．\n無向グラフ \\(G\\) が 完備 (complete) であるとは，任意の \\(x,y\\in V\\) について \\(\\{x,y\\}\\in E\\) が成り立つことをいう．このとき，頂点集合 \\(V\\) は クリーク (clique) であるという．位数 \\(n\\) の完備グラフは \\(K^n\\) で表される．36\n\\(K^r\\subset G\\) を満たす最大の数 \\[\n\\omega(G):=\\left\\{r\\in\\mathbb{N}\\mid K^r\\subset G\\right\\}\n\\] を クリーク数 といい，グラフの不変量となる．37\n弦グラフ (chordal / triangulated graph) とは，任意の長さ４以上のサイクルが弦を持つグラフを言う．38 弦グラフが，Bayesian Network と Markov Network の双方により表現可能であるグラフのクラスに一致する．\n\n\n2.3.2 Markov Network と Markov Random Field\nマルコフネットワークは，２次元のマルコフ確率場に等価である．39\n後者は Ising モデル の一般化である．40\n\n\n2.3.3 導入\nMarkov Network は相互作用に自然な双方向性がない場合でもモデリングを可能とする．\n例えば，集合 \\(\\{A,B,C,D\\}\\) 上の条件付き独立関係 \\[\n\\mathcal{I}:=\\left\\{\\substack{A\\perp\\!\\!\\!\\perp C|(B,D),\\\\B\\perp\\!\\!\\!\\perp D|(A,C)}\\right\\}\n\\] に関して，\\(\\mathcal{I}(\\mathcal{G})=\\mathcal{I}\\) を満たす Bayesian Network 構造 \\(\\mathcal{G}\\) は存在しない．\n一方で，分岐結合と合流結合とを区別できないため，因果性のような方向を持った依存関係は表現できない．\nMarkov Network では，節の間に自然な順序構造がないため，分布の表示が難しくなり，より純粋にグラフの分解に頼ることになる．それゆえ，データからの構造学習も遥かに難しくなる．41\nBayesian Network では条件付き確率密度のみで十分だったところを，これを一般化する概念である factor と呼ばれる概念によって達成する．\n条件付き確率密度 \\(p(x_1,\\cdots,x_m|y_1,\\cdots,y_k)\\) とは，形式的には，積空間 \\(\\prod_{i=1}^m\\mathrm{Im}\\,(X_i)\\times\\prod_{j=1}^k\\mathrm{Im}\\,(Y_j)\\) 上の（正規化された）関数である．一般に，確率変数の値域の積上の（正規化されているとは限らない）関数を ファクター と言う．\n\n\n2.3.4 ファクター\n確率変数の組 \\(\\boldsymbol{X}=(X_1,\\cdots,X_n)\\) 上の ファクター とは，ある部分集合 \\(\\{n_1,\\cdots,n_D\\}\\subset[n]\\) に対して，関数 \\((X_{n_1},\\cdots,X_{n_D})\\) の値域上に定義された関数 \\[\n\\phi:\\prod_{i=1}^D\\mathrm{Im}\\,(X_{n_i})\\to\\mathbb{R}\n\\] を言う．この定義域を スコープ と言う．42\n定義域 \\(a,b\\subset[n]\\) がかぶる２つのファクター \\(\\phi_1,\\phi_2,a\\cap b\\ne\\emptyset\\) が存在する場合，これらを接続して，\\(\\prod_{i\\in a\\cup b}\\mathrm{Im}\\,(X_i)\\) 上に定義された新たなファクターを作ることが出来る：43 \\[\n\\phi_1\\times\\phi_2(X_{a\\cup b}):=\\phi_1(X_a)\\phi_2(X_b).\n\\]\n\n\n\n\n\n\n定義44 （Gibbs distribution, factorization）\n\n\n\n\n\n\n離散確率変数の組 \\(\\boldsymbol{X}=(X_1,\\cdots,X_n)\\) とその上のファクター \\[\n\\Phi:=(\\phi_1(\\boldsymbol{D}_1),\\cdots,\\phi_m(\\boldsymbol{D}_m))\n\\] \\[\n\\boldsymbol{D}_j\\subset\\{X_i\\}_{i=1}^n\\quad(j\\in[m])\n\\] とが定める \\(\\prod_{i=1}^n\\mathrm{Im}\\,(X_i)\\) 上の Gibbs 分布 とは，密度 \\[\np_\\Phi(\\boldsymbol{x})=\\frac{1}{Z}\\prod_{j=1}^m\\phi_j(\\boldsymbol{D}_j)\n\\] が定める分布をいう．ここで \\(Z\\) は正規化定数であり，歴史的には 分配関数 と言う．45\nGibbs 分布 \\(p_\\Phi\\) が Markov network \\(\\mathcal{H}=(\\{X_i\\}_{i=1}^n,\\mathcal{E})\\) 上で 分解する とは，任意の \\(\\mathcal{D}_j\\subset\\{X_i\\}_{i=1}^n\\;(j\\in[m])\\) が \\(\\mathcal{H}\\) のクリークであることをいう．このとき，各ファクター \\(\\phi_1,\\cdots,\\phi_m\\) を clique potential という．\n\n\n\n\n\n\n2.3.5 Markov Network の分離性\n\n\n\n\n\n\n定義46 （Global Markov Independence）\n\n\n\n\n\n\\(\\mathcal{H}\\) を Markov network 構造とする．\n\n道 \\(X_1\\rightleftharpoons\\cdots\\rightleftharpoons X_n\\) が \\(\\boldsymbol{Z}\\subset\\{X_i\\}_{i=1}^n\\) が観測された下でも active であるとは，\\(\\{X_i\\}_{i=1}^n\\cap\\boldsymbol{Z}=\\emptyset\\) を満たすことをいう．\n節集合 \\(\\boldsymbol{X},\\boldsymbol{Y},\\boldsymbol{Z}\\) について，\\(\\boldsymbol{Z}\\) が \\(\\boldsymbol{X},\\boldsymbol{Y}\\) を 分離 するとは，任意の \\(X\\in\\boldsymbol{X}\\) と \\(Y\\in\\boldsymbol{Y}\\) と，\\(X,Y\\) を結ぶ道が，\\(\\boldsymbol{Z}\\) の下で active でないことをいう．このことを \\(\\sep_\\mathcal{H}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\) と表す．\n\n\n\\(\\mathcal{H}\\) 内の分離的な組 \\((\\boldsymbol{X},\\boldsymbol{Y},\\boldsymbol{Z})\\) が表す条件付き独立性の条件式の全体を \\[\n\\mathcal{I}(\\mathcal{H}):=\\left\\{(\\boldsymbol{X}\\perp\\!\\!\\!\\perp\\boldsymbol{Y}|\\boldsymbol{Z})\\mid\\sep_\\mathcal{H}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\right\\}\n\\] で表す．この元を 大域的独立性 ともいう．\n\n\n\n\n\n\n\n\n\n\n定理47 (Hammersley & Clifford, 1971)\n\n\n\n\n\n\\(P\\in\\mathcal{P}(\\mathcal{X})\\) をノードの集合 \\(\\mathcal{X}=\\{X_1,\\cdots,X_n\\}\\) 上の確率分布，\\(\\mathcal{H}\\) を \\(\\mathcal{X}\\) 上の Markov network 構造とする．このとき 1. \\(\\Rightarrow\\) 2. が成り立ち，\\(P\\) が \\(\\mathcal{X}\\) 全域を台に持つとき次は同値：\n\n\\(\\mathcal{H}\\) は \\(P\\) の \\(I\\)-map である：\\(\\mathcal{I}(\\mathcal{H})\\subset\\mathcal{I}(P)\\)．\n\\(P\\) は \\(\\mathcal{H}\\) に従って分解する Gibbs 分布である．\n\n\n\n\n(Besag, 1974) はこの定理に別証明を付し，植物生態学における空間統計モデルに応用している．48\nMarkov 確率場の結合分布を，条件付き分布の系から得ることは困難であるが，結局結合分布も Gibbs 分布になることが (Hammersley & Clifford, 1971) の定理からわかるので，Gibbs 分布を通じて計算することができる．\nこの「条件付き分布から結合分布が復元できる」という知見が Gibbs sampling の基礎となった．49 また統計的画像解析の基礎ともなった (Grenander, 1983)．\nまた (Geman & Geman, 1984) は，Markov 確率場でモデリングをし，その最大事後確率 MAP (Maximum a Posteriori) を目的関数として最適化を行う，という MAP-MRF アプローチを創始した (Li, 2009, p. 2)．\nさらに統計計算法の進展により，画像の低レイヤーな特徴を表現する（画像修復，物体発見など）だけでなく，高レイヤーな特徴（物体認識やマッチングなど）をも扱えることがわかっている (Gidas, 1989), (Li, 1991)．\n\n\n\n\n\n\n命題50 （分離性の特徴付け）\n\n\n\n\n\n\\(\\mathcal{H}\\) を Markov network 構造，\\(\\{X\\}\\sqcup\\{Y\\}\\sqcup\\boldsymbol{Z}\\subset\\mathcal{X}\\) を節の集合とする．このとき，次が成り立つ：\n\n\\(\\mathcal{H}\\) 内で \\(X,Y\\) は \\(\\boldsymbol{Z}\\) によって分離されないならば，ある \\(\\mathcal{H}\\) に沿って分解する分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) について，\\(X\\perp\\!\\!\\!\\perp Y|\\boldsymbol{Z}\\) が成り立つ．\n\\(\\mathcal{H}\\) に沿って分解する殆ど全ての \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して，\\(\\mathcal{I}(\\mathcal{H})=\\mathcal{I}(P)\\) が成り立つ．\n\n\n\n\nBeysian Network （ 節 2.2.4 ）の場合と違い，1. の主張が，\\(\\mathcal{H}\\) に沿って分解する全ての分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して成り立つとは限らない．\nしかし，殆ど全ての \\(\\mathcal{H}\\) に沿って分解する分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して成り立つ条件付き独立性は，グラフの構造から読み取れる．\n\n\n2.3.6 局所依存性\nBayesian Network の \\(d\\)-分離性に対応する分離性の概念を導入し，大域的独立性の概念を定義した．\nしかし，Bayesian Network の場合では有向グラフとしての構造からすぐに読み取れた局所依存性の概念は，Markov Network の場合では，グラフの構造からは読み取れない．\nそして２通りの定義が考え得る．局所依存性は，大域的依存性のサブセットであることに注意．そして，台を全体 \\(\\mathcal{X}\\) に持つ分布については，大域的依存性も含めて３つの定義は全て同値である．51\n\n\n\n2.4 Factor Graph\nMarkov network は Gibbs 分布の依存性を十分に表現できているわけではなかった（ 節 2.3.5 ）．これは特に，クリーク間の大小関係を把握できていないことに因る．\n\n\n\n\n\n\n定義52 （Factor Graph）\n\n\n\n\n\nMarkov newtork から，ファクターを表す節を（四角形で囲うなどして区別した形で）追加し，ファクターをそのスコープに入る変数と隣接するようにし，一方で変数を表す（元々の）節とファクターを表す節とが隣接しないように修正した ２部グラフ \\(\\mathcal{F}\\) を 因子グラフ という．\n分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) が \\(\\mathcal{F}\\) に関して 分解する とは，\\(\\mathcal{F}\\) が定める確率変数の組とその上のファクターが定める Gibbs 分布であることをいう．"
  },
  {
    "objectID": "posts/2024/Computation/PGM.html#footnotes",
    "href": "posts/2024/Computation/PGM.html#footnotes",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Taroni et al., 2014, p. 35)，(Sucar, 2021, p. x), (Clark, 2018) Graphical Models．(Jordan et al., 1999, p. 191) は 3.2節で Neural Networks as Graphical Models を扱っている．↩︎\n(Koller & Friedman, 2009, p. 3) 1.2.1，(Murphy, 2023, p. 143) 第4章．(Balgi et al., 2024) “As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships.”↩︎\n(Koller & Friedman, 2009, p. 1) 1.1 Motivation．↩︎\n一般に，特定のタスクに特化しながら，汎用性も持つエキスパートシステムを構築するためには，宣言型の知識表現 が良い接近として用いられる (Koller & Friedman, 2009, p. 1) 1.1 Motivation．declarative representation の他に model-based approach ともいう．これは対象となるシステムの構造に関する知識を，計算機が理解可能な形で表現するモデルベースな接近であり，「知識」と「推論」という異なるタスクを分離する点に妙がある．↩︎\n(Koller & Friedman, 2009, p. 2) 1.1 Motivation．Probabilistic models allow us to make this fact (= many systems cannot be specified deterministically.) explicit, and therefore often provide a model which is more faithful to reality.↩︎\n(Theodoridis, 2020, p. 772) なども参照．↩︎\n同様の用語の使い分けをしているものは，(Bishop & Bishop, 2024, p. 353)，(Theodoridis, 2020, p. 878) など．↩︎\n(Murphy, 2023, p. 211) 4.7節．↩︎\n(Koller & Friedman, 2009, pp. 12–14) 1.4節 など．↩︎\n一般に，SEM は (Jöreskog, 1970) が発祥と見られており，潜在変数モデルにもパス解析を拡張したもの，と説明される (Clark, 2018)．↩︎\n(黒木学 & 小林史明, 2012) など．↩︎\n(Koller & Friedman, 2009, pp. 12–14) 1.4節．↩︎\n(Koller & Friedman, 2009, pp. 5–6) 1.2.2節．高次元分布が成分間に依存を持つことと，その依存を用いてコンパクトに低次元で表現可能であることとは殆ど等価な事実である．↩︎\n(Koller & Friedman, 2009, p. 6) 1.2.2節．“Probabilistic graphical models support a data-driven approach to model construction that is very eﬀective in practice.”↩︎\n(Bishop, 2006, p. 46) などでも紹介されている．↩︎\n(Koller & Friedman, 2009, p. 57)↩︎\n(須山敦志, 2019, p. 4), (Li, 2009, p. 48)．Wikipedia も参照．↩︎\n(Koller & Friedman, 2009, p. 57)↩︎\n(Koller & Friedman, 2009, p. 60)↩︎\n(Koller & Friedman, 2009, p. 60)↩︎\n(Koller & Friedman, 2009, p. 62)↩︎\n(Koller & Friedman, 2009, p. 62) 定理3.1，定理3.2 p.63．(Howard & Matheson, 1984) による．↩︎\n(Koller & Friedman, 2009, p. 68)↩︎\nこれを trail が active である，ともいう．(Koller & Friedman, 2009, p. 71)．↩︎\nこの語は directed separation の略であり (Koller & Friedman, 2009, p. 71)，和語では 有向分離 ともいう．↩︎\n(Koller & Friedman, 2009, p. 71)↩︎\n(Koller & Friedman, 2009, pp. 71–72) 定義3.6, 3.7．↩︎\n\\(I(\\boldsymbol{X},\\boldsymbol{Y}|\\boldsymbol{Z})_\\mathcal{G}\\) と表すこともある．↩︎\n(Koller & Friedman, 2009, pp. 72–73) 定理3.3, 3.5．↩︎\n(Koller & Friedman, 2009, p. 78) 3.4節 の内容．↩︎\n(Koller & Friedman, 2009, p. 76) 定義3.9．↩︎\n(Koller & Friedman, 2009, p. 77) 定理3.7．↩︎\n有向グラフの スケルトン とは，同じ辺を持つ無向グラフのことである．↩︎\n(Koller & Friedman, 2009, p. 77) 定理3.8．↩︎\n(Diestel, 2017, pp. 1–2) 参照．↩︎\n(Diestel, 2017, p. 3) 参照．↩︎\n(Diestel, 2017, p. 135) 参照．↩︎\nすなわち，三角形以外の 誘導部分グラフ を部分グラフに持たないグラフをいう．(Diestel, 2017, p. 135) 参照．↩︎\n(Sucar, 2021, p. 94) も参照．(Li, 2009, p. 47) は，pairwise なマルコフ確率場もマルコフネットワークと見れることを指摘している．pairwise とは非零なポテンシャルを持つクリークが二点集合になるマルコフ確率場をいう．↩︎\n(Kindermann & Snell, 1980, p. 1)↩︎\n(Koller & Friedman, 2009, p. 106) 4.2節．↩︎\n(Koller & Friedman, 2009, p. 104) 定義4.1．↩︎\nただし，\\(\\phi_1(X_a)\\) とは \\(\\phi_1((X_i)_{i\\in a})\\) の略とした．↩︎\n(Koller & Friedman, 2009, p. 108) 定義4.3．↩︎\n(Koller & Friedman, 2009, p. 105) によると，当初統計物理学の分野の Markov 確率場の概念でこの用語が用いられたことが始まりとなっている．↩︎\n(Koller & Friedman, 2009, pp. 114–115) 定義4.8, 9．↩︎\n(Koller & Friedman, 2009, pp. 116–117) 定理4.1，定理4.2．↩︎\n(Li, 2009) の Rama Chellappa による foreword に “A big impetus to theoretical and practical considerations of 2D spatial interaction models, of which MRF’s form a subclass, was given by the seminal works of Julian Besag.” とある．“Labeling is also a natural representation for the study of MRF’s (Besag 1974).” は (Li, 2009, p. 3)．↩︎\n(Robert & Casella, 2011) (Li, 2009, p. 1) も参照．↩︎\n(Koller & Friedman, 2009, p. 117) 定理4.3．↩︎\nこれは台が縮退している場合は，自明な（決定論的な）独立性が生じてしまうためである．↩︎\n(Koller & Friedman, 2009, p. 123) 4.4.1.1．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI4.html",
    "href": "posts/2024/Computation/VI4.html",
    "title": "変分推論４",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Computation/VI4.html#導入",
    "href": "posts/2024/Computation/VI4.html#導入",
    "title": "変分推論４",
    "section": "1 導入",
    "text": "1 導入\n主成分分析 (PCA: Principal Component Analysis) も，\\(K\\)-平均法と同様に，次元削減，（非可逆）データ圧縮，特徴抽出，データ可視化の用途に用いられる．1\n主成分分析は，値の分散が最大となるような線型射影を求める問題 (Hotelling, 1933) とも，また（おろした垂線の足の二乗距離和の意味で）コストが最小になるような線型射影を求める問題 (Pearson, 1901) とも見ることができ，歴史的には広い分野で研究されてきた．\nまた，複数のデータに対して，同時に主成分分析を行う場合，これを 正準相関分析 (canonical correlation analysis) (Hotelling, 1936) ともいう．\n一方で潜在変数を導入し，「データ＝共通因子部＋独立因子部＋誤差」という分解を得ることを目標とする場合，これを 因子分析 (factor analysis) という (足立浩平, 2023)．2\nいずれも確率的モデリングとは無縁に見えるが，これらはいずれも線型 Gauss な潜在変数モデルの特殊化と見れる (Tipping & Bishop, 1999)．3 従って，EM アルゴリズムによる効率的な推定を初めとした，Bayes 学習，ニューラルネットワークなど，グラフィカルモデルとして様々な拡張が可能である．"
  },
  {
    "objectID": "posts/2024/Computation/VI4.html#footnotes",
    "href": "posts/2024/Computation/VI4.html#footnotes",
    "title": "変分推論４",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKosambi-Karhunen-Loéve 変換ともいう (Bishop & Bishop, 2024, p. 497)．↩︎\nすなわち，主成分分析が低階数近似で，因子分析が高階数近似になっている，という説明が１つありえる．↩︎\n(Bishop & Bishop, 2024) 第16章．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI3.html",
    "href": "posts/2024/Computation/VI3.html",
    "title": "変分推論３",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Computation/VI3.html#導入",
    "href": "posts/2024/Computation/VI3.html#導入",
    "title": "変分推論３",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 変分 Bayes 推論の立ち位置\nBayes 推論を実行するにあたって，サンプリング法は exact な方法であると言われる．これは十分な計算量を等価することで，任意の精度で事後分布を近似できるためである．\nこの性質は肝要であるが，真に厳密な近似を得ることよりも，ある程度の誤差を許容しながらも計算のコストを下げる方が重要である場面も多い．これを叶えてくれる，極めて自然な決定論的な近似手法が，変分推論である．\nBayes 事後分布に簡単な分布族を想定し，その中で KL 距離の意味で最も近い分布を，変分最適化によって探すのである．\nどれくらい実行し続けていれば欲しい精度が出るのか分かりにくい MCMC よりも，KL 距離という（実は訳のわかっていない）尺度が大切ということにして，これが目に見えて減少していく方がアルゴリズムとして達成感があるというのである．\n\n\n1.2 変分法の歴史\n変分法とは，関数空間上での微分法をいう．\n変分法自体は，多くの応用先に古くから用いられている．統計学 (Rustagi, 1976)，統計力学 (Parisi, 1988)，量子力学 (Sakurai, 1985)，有限要素解析 (Schwarz, 1988), (Bathe, 1996) などの教科書で触れられている．最大エントロピー法 (Kapur, 1989)，最小作用の原理 (Feynman et al., 1964) も変分法の例である．\nいずれの場面でも，変分法は困難な問題を，自由度を分解する (decoupling of the degrees of freedom) ことで，簡単な問題に分解する方法として用いられている (Jordan et al., 1999, p. 198)．典型的には，変分パラメータ (variational parameter) という追加の変数を導入する手続きを伴う．"
  },
  {
    "objectID": "posts/2024/Computation/VI3.html#sec-VB",
    "href": "posts/2024/Computation/VI3.html#sec-VB",
    "title": "変分推論３",
    "section": "2 変分 Bayes のアルゴリズム",
    "text": "2 変分 Bayes のアルゴリズム\n潜在変数を持つグラフィカルモデルの文脈では，EM アルゴリズムのような点推定によるパラメータ推定では汎化性能が伸びず，事後分布を導出したいが，その計算は困難である．これを打開すべく提案されたのが変分 Bayes 推定である (Attias, 1999)．\n\n2.1 アルゴリズムの前提\n変分 EM アルゴリズムは，\\(\\theta\\) の事前分布としてデルタ分布を置いていた場合の変分 Bayes アルゴリズムとみなせる．1\nモデルのパラメータや潜在変数を全て含めて \\(z\\) とし，尤度が \\[\np(x)=\\int_\\mathcal{Z}p(x,z)\\,dz\n\\] と与えられている解する．このとき，対数尤度の下界は \\[\n\\begin{align*}\n    \\log p(x)&=\\log\\int_\\mathcal{Z}p(x,z)\\,dz\\\\\n    &\\ge\\int_\\mathcal{Z}q(z|x)\\log\\frac{p(x,z)}{q(z|x)}\\,dz\\\\\n    &=\\int_\\mathcal{Z}q(z|x)\\log\\frac{p(z|x)p(x)}{q(z|x)}\\,dz\\\\\n    &=\\log p(x)-\\mathop{\\mathrm{KL}}(q,p)=:F(q).\n\\end{align*}\n\\] と表せるのであった．2\nこの \\(F\\) を変分下界または ELBO (Evidence Lower Bound) という．\\(F\\) を変分自由エネルギーともいう．\n無制約下では，\\(F\\) は \\(q=p\\) のときに最大となる．これが EM アルゴリズムの E-ステップなのであった．しかし，このステップは難しい場面も多い．\nそのような場合，まず \\(p\\) にニューラルネットワークなどのパラメトリックモデルをおき，そのパラメータを KL-距離を最適化することで求めることが考えられる．こうしてパラメトリックな変分 Bayes アルゴリズムを得る．\n\n\n2.2 平均場近似\n関数形ではなく，次のような仮定をおくことでも，変分 Bayes アルゴリズムが得られる．\n\\[\nq(z|x)=q(z_1|x)q(z_2|x)\n\\tag{1}\\]\nと仮定すると \\[\nF(q)=\\int_\\mathcal{Z}q(z_1|x)q(z_2|x)\\log\\frac{p(x,z)}{q(z_1|x)q(z_2|x)}\\,dz\n\\] の表示を得る．このような仮定は平均場近似とも呼ばれる．3\n実は，この表示ならば，\\(q(z_1|x)\\) と \\(q(z_2|x)\\) について逐次的に最大化していくための解析的な公式が求まる．\nさらに，\\(F\\) は各因子 \\(q(z_1|x),q(z_2|x)\\) に関して凸になるので，こうして得るアルゴリズムの収束も保証される．4\n\n2.2.1 VB-\\(E\\) ステップ\n\\(F\\) の \\(q(z_1|x)\\) に関する最大値は \\[\nq(z_1)\\propto e^{(q(z_2)dz_2\\,|\\log p(x,z_2|z_1))}\n\\] が与える．\n\n\n\n\n\n\n証明\n\n\n\n\n\nまず，\\(q(z_1|x)\\) について最大化することを考える．\\(F\\) の \\(q(z_1|x)\\) に関する最大化は， \\[\nL:=F(q)+\\lambda\\left(\\int_\\mathcal{Z}q(z_1)\\,dz_1-1\\right)\n\\] の \\(\\lambda\\) との同時最大化と同値である．これが Lagrange の未定乗数法 である．\n\\[\n\\begin{align*}\n    \\frac{\\delta L}{\\delta q(z_1|x)}&(q(z_2|x)dz_2|\\log p(x,z_1|z_2))-\\log q(z_1)+\\lambda+\\mathrm{const.}\n\\end{align*}\n\\] と計算できるから，これは \\[\nq(z_1)\\propto e^{(q(z_2)dz_2\\,|\\log p(x,z_2|z_1))}\n\\] にて最大化される．これが変分 Bayes アルゴリズムの \\(E\\)-ステップである．\n(Bishop, 2006, p. 465) はまた違った議論を提供している．\n\n\n\n\\(q(z_2)=\\delta(\\varphi)\\) であるとき， \\[\nq(z_1)\\propto p(x,z_1|\\varphi)\\propto p(z_1|x,\\varphi)\n\\tag{2}\\] であることに注意．\n\n\n2.2.2 VB-\\(M\\) ステップ\n全く同様にして， \\[\nq(z_2)\\propto p(z_2)e^{(q(z_1)dz_2\\,|\\log p(x,z_1|z_2))}\n\\] で最大化される．\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 自動正則化\nまたこの枠組みは，その他のベイズ的な手法と同様，過学習を防ぐ正則化が暗黙のうちに盛り込まれているともみなせる．5\n\\[\n\\begin{align*}\n    F(q)&=\\int_\\mathcal{Z}q(z_1|x)q(z_2|x)\\log\\frac{p(x,z)}{q(z_1|x)q(z_2|x)}\\,dz\\\\\n    &=\\int_\\mathcal{Z}q(z_1|x)q(z_2|x)\\log\\frac{p(x,z_1|z_2)p(z_2)}{q(z_1|x)q(z_2|x)}\\,dz\\\\\n    &=\\int_\\mathcal{Z}q(z_1|x)q(z_2|x)\\log\\frac{p(x,z_1|z_2)}{q(z_1|x)}dz\\\\\n    &\\qquad-\\mathop{\\mathrm{KL}}(q(-|x),p(-)).\n\\end{align*}\n\\]\n\n\n\n2.3 平均場近似の問題点\nいわば \\(q\\) の全ての周辺分布を「独立」だと解釈しているため，実際には変数間に強い相関があった際に，分散を過小評価する嫌いがある．"
  },
  {
    "objectID": "posts/2024/Computation/VI3.html#sec-EP",
    "href": "posts/2024/Computation/VI3.html#sec-EP",
    "title": "変分推論３",
    "section": "3 期待値伝播法",
    "text": "3 期待値伝播法\n\n3.1 導入\n節 2 では \\(\\mathop{\\mathrm{KL}}(q,p)\\) を最小化する \\(q\\) を探索したが，逆に \\(\\mathop{\\mathrm{KL}}(p,q)\\) を最小化すると考えるのが期待値伝播法 (EP: Expectation Propagation) (Minka, 2001a), (Minka, 2001b) である．\nなお，\\(\\mathop{\\mathrm{KL}}(p,q)\\) を，MCMC によって推定した勾配を用いて確率的勾配降下法によって最小化する手法も提案されている： Markovian Score Climbing (Naesseth et al., 2020), Joint Stochastic Approximation (Ou & Song, 2020) とこれらを包含する Markov Chain Score Ascent (Kim et al., 2022) など．\n\n\n3.2 \\(\\alpha\\)-乖離度\n期待値伝播法と変分 Bayes 推論との振る舞いの違いは，\\(\\alpha\\)-乖離度の振る舞いの変化によって理解できる．\n\n\n\n\n\n\n定義（\\(\\alpha\\)-divergence）(Amari, 1985, p. 85), (Cichocki et al., 2008, p. 1434)\n\n\n\n\\(\\alpha\\in\\mathbb{R}\\setminus\\{\\pm1\\}\\) に関して， \\[\nD_\\alpha(p,q):=\\frac{4}{1-\\alpha^2}\\left(1-\\int_\\mathcal{X}p(x)^{\\frac{1+\\alpha}{2}}q(x)^{\\frac{1-\\alpha}{2}}\\,dx\\right)\n\\] を \\(\\alpha\\)-乖離度 という．6\n\n\n\\(\\alpha\\to1\\) の極限では \\(\\mathop{\\mathrm{KL}}(p,q)\\) に収束し，\\(\\alpha\\to-1\\) の極限では \\(\\mathop{\\mathrm{KL}}(q,p)\\) に収束する．7 いわば，この２つの量を補間する量である．\n\\(\\alpha=0\\) の場合を，Hellinger 距離（の自乗）という．\n\\(\\alpha\\le-1\\) の場合は \\(D_\\alpha\\) は \\(\\frac{q}{p}\\) を含むため，\\(p\\) が零ならば \\(q\\) も零になるようになる：\\(q\\ll p\\)．実際，変分近似は分散を過小評価しがちである 節 2.3．\n一方で \\(\\alpha\\ge1\\) の場合は \\(D_\\alpha\\) は \\(\\frac{p}{q}\\) を含むため，\\(p\\) の台を \\(q\\) の台が含むようになる．\nこうして EP は，変分 Bayes よりも，複数の峰がある分布を平均したように，裾の広い近似を与えるという対照的な性質を持つ．\n\n\n3.3 Power-EP\n一般の \\(\\alpha\\)-乖離度を最小化する手法が Power-EP (Minka, 2004) である．\n多くのメッセージ伝播アルゴリズムもこの枠組みで導出できる (Minka, 2005)．8"
  },
  {
    "objectID": "posts/2024/Computation/VI3.html#footnotes",
    "href": "posts/2024/Computation/VI3.html#footnotes",
    "title": "変分推論３",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Wainwright & Jordan, 2008, p. 164) も参照．↩︎\nEM アルゴリズムに関する 前稿 も参照．↩︎\n(Bishop, 2006, p. 465)．↩︎\n(Bishop, 2006, p. 466)．↩︎\nその理由に関する洞察は，エントロピー項 \\(H(q)\\) が大きな役割を果たしているようである．(Khan & Rue, 2023) なども示唆的である．↩︎\n関連する乖離度に，Rényi の \\(\\alpha\\)-乖離度 がある．↩︎\n前者 \\(\\mathop{\\mathrm{KL}}(p,q)\\) を \\(q\\) に関して exclusive と言い，\\(\\mathop{\\mathrm{KL}}(q,p)\\) は \\(\\mathrm{supp}\\;(q)\\subset\\mathrm{supp}\\;(p)\\) を満たすため inclusive ともいう．(Kim et al., 2022) など．↩︎\n(Bishop, 2006, p. 517) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html",
    "href": "posts/2024/Particles/PFja.html",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html#導入粒子フィルターとは何か",
    "href": "posts/2024/Particles/PFja.html#導入粒子フィルターとは何か",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "1 導入：粒子フィルターとは何か？",
    "text": "1 導入：粒子フィルターとは何か？\n粒子フィルターとは，逐次モンテカルロ法とも呼ばれ，あらゆる状態空間モデル（非線型・非Gaussでも！）に適用可能なサンプリングベースの非線型フィルタリングアルゴリズムとして，(Gordon et al., 1993) と (Kitagawa, 1993) により独立に提案された．1 Kalman フィルターベースの手法の難点を克服したのである．\nフィルタリング分布を，粒子と呼ばれる有限個の重み付けられたサンプルで近似し，そのサンプルを更新していくことで，逐次的にフィルタリング分布を近似する手法であるため，粒子フィルターと呼ばれる．"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html#リサンプリング粒子フィルターの最重要メカニズム",
    "href": "posts/2024/Particles/PFja.html#リサンプリング粒子フィルターの最重要メカニズム",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "2 リサンプリング：粒子フィルターの最重要メカニズム",
    "text": "2 リサンプリング：粒子フィルターの最重要メカニズム\n逐次重点サンプリングとの違いは，表現する分布は変えずに，定期的に荷重を一様に均す リサンプリング という機構を導入しているために，荷重の縮退と呼ばれる，時間経過と共に有効サンプル数が極めて小さくなってしまう問題を回避している点にある．\n粒子フィルターの妙は，そのサンプリング段階にあると言える．"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html#粒子フィルターの適用範囲の拡大",
    "href": "posts/2024/Particles/PFja.html#粒子フィルターの適用範囲の拡大",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "3 粒子フィルターの適用範囲の拡大",
    "text": "3 粒子フィルターの適用範囲の拡大\nしかし，粒子フィルターは時系列構造を持ったグラフィカルモデルに限った推論アルゴリズムではなく，極めて汎用的な手法 であることがわかってきた．だから「逐次モンテカルロ法」という，「マルコフ連鎖モンテカルロ法」を連想させる名前も付いているのである．\n複雑な目標分布 \\(\\pi\\) からサンプリングすることは，機械学習において重要な問題の一つであるが，テンパリングの発想で，これを近似する列 \\[\n\\pi_1,\\pi_2,\\cdots,\\pi_n=\\pi\n\\] を構成し，これを逐次的に近似する粒子フィルターを構成することで，多峰性にも強いサンプリング手法として使うこともでき，これを SMC サンプラーと呼ぶ (Del Moral et al., 2006)．\nこの粒子フィルターの逐次的な構造と，テンパリングを組み合わせる手法は，多くの設定で MCMC よりも安定性のある推論手法を提供する．\n本稿も粒子フィルターの適用先を拡張する試みの一つであり，粒子フィルターを拡散過程に対して適用した場合の，リサンプリング効率と収束性について考察したものである．"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html#連続モデルへの適用のモチベーション",
    "href": "posts/2024/Particles/PFja.html#連続モデルへの適用のモチベーション",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "4 連続モデルへの適用のモチベーション",
    "text": "4 連続モデルへの適用のモチベーション\n区分確定的マルコフ過程 (PDMP: Piecewise Deterministic Markov Process) (Davis, 1984) とは，ジャンプのみをランダムな要素とし，それ以外の期間では決定論的な動きをする確率過程である．\n論文 (Peters & de With, 2012) において，Metropolis アルゴリズムの連続極限として PDMP が導入された．これを取り上げ，Bouncy Particle Sampler (BPS) として一般化して提示したのが (Bouchard-Côté et al., 2018) である．\nこれらのサンプラーは次のように動く：\n\n\nCode\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\n他の PDMP によるサンプリング法には，Zig-Zag sampler (Bierkens et al., 2019) がある：\n\n\nCode\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nこれらのサンプラーは，離散時間の MCMC アルゴリズムよりも効率が上がることが知られている．\n\nInterestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as they need only access a small sub-set of data points at each iteration, and yet are still guaranteed to target the true posterior distribution. (Fearnhead et al., 2018)\n\nRadford Neal がニューラルネットワークにおけるパラメータの事後分布を近似するために Hamiltonian Monte Carlo を導入したように (Neal, 1996)，連続時間の MCMC アルゴリズムは Bayes 深層学習に対する有力な手法となる可能性がある．2\nこうして，連続時間の MCMC アルゴリズムの持つ魅力的な性質が明らかになってきた．本稿は，その発見の過程を SMC において模倣し，粒子フィルターを連続モデルに適用してその連続極限に関して考察するものである．"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html#粒子フィルターのアルゴリズム的描写",
    "href": "posts/2024/Particles/PFja.html#粒子フィルターのアルゴリズム的描写",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "5 粒子フィルターのアルゴリズム的描写",
    "text": "5 粒子フィルターのアルゴリズム的描写"
  },
  {
    "objectID": "posts/2024/Particles/PFja.html#footnotes",
    "href": "posts/2024/Particles/PFja.html#footnotes",
    "title": "粒子フィルターの連続モデルへの適用（未完）",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Kitagawa, 1996) は粒子という用語を用いていたが，粒子フィルター の名前は (Carpenter et al., 1999) による．↩︎\n(Cobb & Jalaian, 2021) は HMC の拡張をしている．↩︎"
  },
  {
    "objectID": "posts/2024/Particles/ContinuousLimit.html",
    "href": "posts/2024/Particles/ContinuousLimit.html",
    "title": "粒子フィルターの連続極限",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\nリサンプリングと粒子フィルターは \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上に定義されているとし，この確率測度 \\(\\operatorname{P}\\) に関する期待値を \\(\\operatorname{E}\\) で表す．\n一方で，観測対象は \\((\\Omega',\\mathcal{F}',\\operatorname{P}')\\) 上の過程 \\(z:\\mathbb{R}_+\\times\\Omega'\\to\\mathbb{R}^d\\) とする．\n２つの分布の間の関係を導く．"
  },
  {
    "objectID": "posts/2024/Particles/ContinuousLimit.html#リサンプリング",
    "href": "posts/2024/Particles/ContinuousLimit.html#リサンプリング",
    "title": "粒子フィルターの連続極限",
    "section": "1 リサンプリング",
    "text": "1 リサンプリング\n\n1.1 形式化\n積空間 \\([N]^{[N]}\\) を 附番の空間 といい，元を \\(a\\in[N]^{[N]}\\) で表す．\n積空間 \\(\\mathbb{R}_+^N\\setminus\\{0\\}^N\\) を 荷重の空間 といい，元を \\(w\\in\\mathbb{R}_+^N\\setminus\\{0\\}^N\\) で表す．簡単のため，以降は \\(\\mathbb{R}_+^N\\) とも表してしまう．ノルム \\[\n\\|w\\|_1:=\\sum_{i=1}^Nw_i\n\\] を考える．\n\n\n\n\n\n\n定義（リサンプリング）\n\n\n\n\n粒子数 \\(N\\) に関する リサンプリング法 \\(r\\) とは，荷重の空間 \\(\\mathbb{R}_+^N\\) から附番の空間 \\([N]^{[N]}\\) への確率核 をいう．すなわち，荷重に応じて，附番の空間上の確率分布を対応させる関数 \\[\nr:\\mathbb{R}_+^N\\to\\mathcal{P}([N]^{[N]})\n\\] をいう．\nリサンプリング法 \\(r\\) が 不偏 であるとは，任意の番号 \\(j\\in[N]\\) に対して，その番号を得る粒子数の期待値が，荷重の通りになることをいう：\\(A\\sim r(w)\\) ならば， \\[\n\\begin{align*}\n\\operatorname{E}[\\#A^{-1}(j)]&=\\operatorname{E}\\left[\\sum_{i=1}^N1_{\\left\\{A(i)=j\\right\\}}\\right]\\\\\n&=N\\frac{w_j}{\\|w\\|_1}\\quad(j\\in[N]).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n定義に関する議論\n\n\n\n\n\n定義は (Chopin et al., 2022, p. 3199) を参考にした．リサンプリング法の定義は，確率核 \\(r\\) の終域をどうするかで議論があると思う．初めは \\(r:\\mathbb{R}_+^N\\to\\mathcal{P}([N])\\) と定めたくなる．しかしこれでは \\(N\\) 回の非独立的なサンプリング を表現できない．\\(r(w^{1:N})^{\\otimes N}\\) とするわけには行かないし，ベクトル \\(N\\cdot r(w^{1:N})^\\top\\) が個数の分布になるようにサンプリングせよ，ということだったら確率核になっていない．結局確率核は \\(r:\\mathbb{R}_+^N\\to\\mathcal{P}([N]^{[N]})\\) と取るしかない．すると，不偏性の表現が難しくなる．\n\n\n\n\n\n\n\n\n\nPAC-Bayes × リサンプリング？\n\n\n\n\n\n問題構成がすごく PAC-Bayes っぽい．というより，Gibbs classifier っぽい．\n\n\n\n\n\n\n\n\n\n空間 \\(\\mathcal{P}([N]^{[N]})\\) と \\(\\mathcal{M}^1([N]^{[N]})\\)\n\n\n\n\n\n離散空間上の測度 \\(\\mu\\in\\mathcal{M}^1([N]^{[N]})\\) は，その一点集合上での値 \\[\nf_\\mu(a):=\\mu(\\{a\\})\n\\] で一意に定まるため，\\(\\mathcal{M}^1([N]^{[N]})\\) は \\(\\mathrm{Map}([N]^{[N]},\\mathbb{R}_+)\\) と，\\(\\mathcal{P}([N]^{[N]})\\) は \\([N]^{[N]}\\) 上の確率質量関数全体の集合と同一視できる．\nさらに，関数空間 \\(\\mathrm{Map}([N]^{[N]},\\mathbb{R}_+)\\) に各点収束の位相を入れたものと，弱位相を備えた \\(\\mathcal{M}^1([N]^{[N]})\\) とは同相である．\nこれを，\\([N]^{[N]}\\) に限らず，任意の可算な離散空間 \\(S\\) において示す．すると \\(S\\) 上では，弱収束は全変動収束に等しい．1\n一般に (Scheffé, 1947) の定理より，密度が各点収束するならば，分布は全変動収束，特に弱収束する．逆に，\\(S\\) 上の分布が収束するとき，任意の一点集合 \\(\\{x\\}\\subset S\\) は開かつ閉であるから境界は空集合であり，(Alexandroff, 1940) の特徴付けから，密度（というより確率質量関数）も収束する．\nよって，\\(\\mathcal{M}^1([N]^{[N]})\\) 上の弱収束と，\\(\\mathrm{Map}([N]^{[N]},\\mathbb{R}_+)\\) 上の各点収束とは同値．いずれも第２可算だから，点列の収束が等しいことは位相も等しいことを含意する．\n\n\n\n以降，\\(\\mathcal{M}^1([N]^{[N]})\\) は全変動ノルムが備わったノルム空間と見る．これは，写像の空間 \\(\\mathrm{Map}([N]^{[N]},\\mathbb{R}_+)\\) に各点収束の位相が備わった空間と同一視できる．\n\n\n1.2 荷重が一様になる極限でも安定なリサンプリング法\n\n1.2.1 モチベーション\nリサンプリング法 \\(r:\\mathbb{R}_+^N\\to\\mathcal{P}([N]^{[N]})\\) に対して，荷重が一様に近づく（＝殆どリサンプリングをしなくなっていく）極限における性質を，連続関数 \\[\n\\begin{align*}\n    e:\\mathbb{R}_+^N\\times(0,1)&\\to \\mathbb{R}_+^N\\\\\n(v,\\Delta)\\quad&\\mapsto e^{-\\Delta v}:=(e^{-\\Delta v^1},\\cdots,e^{-\\Delta v^N})\n\\end{align*}\n\\] を通じて調べる： \\[\n\\widetilde{r}(v,\\Delta):=r\\circ e(v,\\Delta)=r(e^{-\\Delta v}).\n\\]\n\\(v\\in\\mathbb{R}_+^N\\) が大雑把に方向を意味し，\\(\\Delta&gt;0\\) が \\(0\\) に近づくほど荷重が一様に近づくようなつまみの役割を果たす．\n\\(\\Delta\\) をタイムステップとも考えると，粒子フィルターに \\(\\Delta\\searrow0\\) の極限が存在するためには，リサンプリング回数が爆発しないために， \\[\n\\widetilde{r}(v,\\Delta)(a)=O(\\Delta)\\quad(v\\in\\mathbb{R}_+^N,a\\ne\\mathrm{id}_{[N]})\n\\tag{1}\\] が成り立つ必要がある．\n\n\n1.2.2 リサンプリング安定性の定式化\n条件 (1) が成り立つことは， \\[\n\\iota(v,\\Delta):=\\frac{r(e^{-\\Delta v})}{\\Delta}\\quad\\in\\mathcal{M}^1([N]^{[N]}).\n\\] により定まる関数 \\[\n\\iota:\\mathbb{R}_+^N\\times(0,1)\\to\\mathcal{M}^1([N]^{[N]})\n\\] が，任意の \\(a\\in[N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\}\\) に関して \\(\\Delta\\searrow0\\) に関する極限 \\(\\lim_{\\Delta\\searrow0}\\iota(v,\\Delta)\\) を持つことに同値．\nここで，このリサンプリング強度関数 \\[\n\\iota:\\mathbb{R}^N_+\\times(0,1)\\to\\mathcal{M}^1([N]^{[N]})\n\\] は連続になるとする．これはリサンプリング法 \\(r:\\mathbb{R}_+^N\\to\\mathcal{P}([N]^{[N]})\\) が連続ならば成り立つ． 2\nすると結局，条件 (1) は，\\(\\iota\\) が連続な延長 \\[\n\\overline{\\iota}:\\mathbb{R}_+^N\\times[0,1)\\to\\mathcal{M}^1([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\})\n\\] を持てば十分．3\nこれは \\(r\\) が連続であることに加えて，上の収束が \\(v\\in\\mathbb{R}_+^N\\) 上一様に成り立つならば，成り立つ．\n\\(\\overline{\\iota}(v):=\\overline{\\iota}(v,0)\\) は 極限リサンプリング強度 であり，総和を取ったもの \\[\n\\begin{align*}\n    \\overline{\\iota}^*(w)&:=(\\overline{\\iota}(w,0)|1)\\\\\n    &=\\sum_{a\\in[N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\}}\\overline{\\iota}(w,0)(a)\\quad\\in\\mathbb{R}_+\n\\end{align*}\n\\] は 全リサンプリング率 と呼べる．4\n\n\n\n\n\n\n補題（ここまでの議論のまとめ）\n\n\n\n\nリサンプリング法 \\(r:\\mathbb{R}_+^N\\to\\mathcal{P}([N]^{[N]})\\) が連続で，\n\\([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\}\\) 上の関数の空間 \\(\\mathcal{M}^1([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\})\\) での収束 \\[\n\\overline{\\iota}(v)=\\lim_{\\Delta\\searrow0}\\frac{r(e^{-\\Delta v})}{\\Delta}\n\\] は \\(v\\in\\mathbb{R}^N_+\\) 上一様に成り立つ\n\nとする．このとき，極限リサンプリング強度 \\[\n\\overline{\\iota}:\\mathbb{R}_+^N\\to\\mathcal{M}^1([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\})\n\\] は \\(\\mathcal{M}^1([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\})\\) の弱位相について連続である．\n\n\n\n\n\n\n\n\n積空間上の連続関数の連続延長\n\n\n\n\n\n\\[\nf(x,y):=x^{\\frac{1}{y}}\n\\] によって \\(f:(0,1]\\times(0,1)\\to[0,1]\\) を定める．任意の \\(x\\in(0,1]\\) について，極限 \\[\n\\lim_{y\\searrow0}x^{\\frac{1}{y}}=\\begin{cases}\n0&x&lt;1,\\\\\n1&x=1.\n\\end{cases}\n\\] は存在するが，延長 \\[\n\\overline{f}:(0,1]\\times[0,1]\\to[0,1]\n\\] は連続ではない．\\(\\overline{f}(x,0)=\\delta_1(x)\\) である．\nしかし，距離空間 \\(X,Y\\) に対して，\\(f:X\\times Y\\to\\mathbb{R}\\) が，境界点 \\(\\overline{y}\\in\\partial Y\\) に対して，\\(f(x,y)\\to f(x,\\overline{y})\\) が \\(x\\in X\\) に関して一様に成り立つならば，\\(f:X\\times\\overline{Y}\\to\\mathbb{R}\\) は連続になる．\n一様収束するとは，任意の \\(\\epsilon&gt;0\\) に対して，ある \\(\\delta&gt;0\\) が存在して， \\[\n\\lvert y-\\overline{y}\\rvert&lt;\\delta\\Rightarrow\\|f(-,y)-f(-,\\overline{y})\\|_\\infty&lt;\\epsilon\n\\] が成り立つことをいう．このとき，\\(f(-,\\overline{y}):X\\to\\mathbb{R}\\) は連続になる．5\nすると，\\(f:X\\times\\overline{Y}\\to\\mathbb{R}\\) も連続であることが示せる．任意の収束点列 \\(\\{(x_n,y_n)\\}\\subset X\\times Y\\) を取る．収束先が \\((x,\\overline{y})\\in X\\times\\partial Y\\) の形である場合について，\\(f(x_n,y_n)\\to f(x,\\overline{y})\\) が成り立つことを示せばよい．\n実際このとき，ある \\(N\\in\\mathbb{N}\\) が存在して，任意の \\(n\\ge N\\) について \\[\n\\|f(-,y_n)-f(-,\\overline{y})\\|_\\infty&lt;\\frac{\\epsilon}{2}\n\\] かつ \\[\n\\lvert f(x_n,\\overline{y})-f(x,\\overline{y})\\rvert&lt;\\frac{\\epsilon}{2}\n\\] であるから， \\[\n\\begin{align*}\n    &\\lvert f(x_n,y_n)-f(x,\\overline{y})\\rvert\\\\\n    \\le&\\lvert f(x_n,y_n)-f(x_n,\\overline{y})\\rvert+\\lvert f(x_n,\\overline{y})-f(x,\\overline{y})\\rvert\\\\\n    &lt;&\\frac{\\epsilon}{2}+\\frac{\\epsilon}{2}=\\epsilon.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2024/Particles/ContinuousLimit.html#粒子フィルターの構成",
    "href": "posts/2024/Particles/ContinuousLimit.html#粒子フィルターの構成",
    "title": "粒子フィルターの連続極限",
    "section": "2 粒子フィルターの構成",
    "text": "2 粒子フィルターの構成\n\n\n\n\n\n\n目標となる Feynman-Kac 測度\n\n\n\n\\(\\mathbb{R}^d\\) 上の伊藤拡散 \\(\\{z_t\\}\\subset\\mathcal{L}(\\Omega';\\mathbb{R}^d)\\) は， \\[\nb_i,\\sigma_{ij}\\in\\mathrm{Lip}_b(\\mathbb{R}^d)\\quad(i,j\\in[d])\n\\] が定める確率微分方程式 \\[\nz_t=b(z_t)dt+\\sigma(z_t)dB_t\n\\] で定まるものとする．\nこれを参照過程とし，ポテンシャル \\(V\\in C_b(\\mathbb{R}^d)_+\\) が定める Feynman-Kac 測度を \\(\\Pi\\in C_c(D_{\\mathbb{R}^d}(\\mathbb{R}_+))^*\\) と表す： \\[\n\\Pi(f):=\\frac{1}{\\mathcal{Z}}\\operatorname{E}\\left[f(z_{[0,\\tau]})\\exp\\left(-\\int^\\tau_0V(z_u)\\,du\\right)\\right]\n\\] \\[\n\\mathcal{Z}:=\\operatorname{E}\\left[\\exp\\left(-\\int^\\tau_0V(z_u)\\,du\\right)\\right],\\qquad f\\in C_c(D_{\\mathbb{R}^d}(\\mathbb{R}_+);\\mathbb{R}^d).\n\\]\n\n\nこの \\(D_{\\mathbb{R}^d}(\\mathbb{R}_+)\\) 上の Feynman-Kac 測度を，離散時間粒子フィルターがどこまで近似できるかを考える．\n\n\n\n\n\n\n粒子フィルターの構成\n\n\n\n粒子フィルター \\(\\{X^\\Delta_k\\}\\subset\\mathcal{L}(\\Omega;M_{Nd}(\\mathbb{R}))\\) は最も自然に構成する．\n粒子数 \\(N\\) のリサンプリング法 \\(r\\) に関して，サンプリング間隔を \\(\\Delta&gt;0\\) として， \\[\n\\begin{align*}\n    (X^\\Delta_k)^i&:=(X_{k-1}^\\Delta)^{A_k(i)}+\\Delta\\cdot b\\left((X_{k-1}^\\Delta)^{A_k(i)}\\right)\\\\\n    &\\qquad+\\sigma\\left((X_{k-1}^\\Delta)^{A_k(i)}\\right)(B_{k\\Delta}^i-B_{(k-1)\\Delta}^i)\\\\\n    &\\qquad\\qquad(i\\in[N],k\\in\\mathbb{N})\\\\\n    A_k&\\overset{\\text{iid}}{\\sim}r(e^{-\\Delta V(X^\\Delta_k)})\n\\end{align*}\n\\] と再帰的に定める．\nこれを \\(D_{\\mathbb{R}^d}(\\mathbb{R}_+)\\)-過程と見たものを \\[\nZ_t^\\Delta:=X_{\\left\\lfloor\\frac{t}{\\Delta}\\right\\rfloor}^\\Delta\n\\] と表す．フィルトレーション \\(\\mathcal{F}_t\\) は \\((B_s)_{s\\in[0,t]}\\) と \\(\\left(A_{\\left\\lfloor\\frac{s}{\\Delta}\\right\\rfloor}\\right)_{s\\in[0,t]}\\) とが生成するものの，完備右連続化とする．\n\n\n\n任意の単調減少列 \\(\\{\\Delta_n\\}\\subset\\mathbb{R}^+\\) に対して，粒子フィルターの列 \\[\n\\{X^{\\Delta_n}\\}\\subset\\mathcal{L}(\\Omega;D_{M_{Nd}(\\mathbb{R})}(\\mathbb{R}_+))\n\\] は一様に緊密である．\n\n\n\n\n\n\n\n定理\n\n\n\nFeynman-Kac 測度を定める参照過程 \\(\\{z_t\\}\\) とポテンシャル \\(V:\\mathbb{R}^d\\to\\mathbb{R}_+\\) について，次を仮定する：\n\n\\[\nb_i,\\sigma_{ij}\\in\\mathrm{Lip}_b(\\mathbb{R}^d)\\quad(i,j\\in[d])\n\\]\n\\[\n\\inf_{x\\in\\mathbb{R}^d}\\inf_{\\substack{\\theta\\in\\mathbb{R}^d\\\\\\|\\theta\\|_2=1}}\\|\\sigma(x)\\theta\\|_2&gt;0\n\\]\n\\[V\\in C_b(\\mathbb{R}^d)_+\\]\n\nリサンプリング法 \\(r\\) について，極限リサンプリング測度 \\[\n\\overline{\\iota}:\\mathbb{R}_+^{N}\\to\\mathcal{M}^1([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\})\n\\] が存在し，連続であるとする．6\nこのとき，これが定める粒子フィルター \\(\\{X_k^\\Delta\\}_{k=0}^\\infty\\) の càdlàg 延長 \\(\\{Z_t^\\Delta\\}_{t\\in\\mathbb{R}_+}\\) は，任意の単調減少列 \\(\\{\\Delta_n\\}\\subset\\mathbb{R}^+\\) に対して，次を満たす：\n\nある Lévy 過程 \\(\\{Z_t\\}\\) に分布収束する： \\[\n(Z_t^{\\Delta_n})\\overset{\\text{d}}{\\to}(Z_t).\n\\]\n分布収束極限の生成作用素 \\(\\mathcal{L}\\) は，伊藤拡散 \\(\\{z_t\\}\\) の生成作用素 \\[\n\\begin{align*}\nLf(x)&=\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x)\\frac{\\partial ^2f}{\\partial x_i\\partial x_j}(x)\\\\\n&\\qquad+\\sum_{i=1}^db_i(x)\\frac{\\partial f}{\\partial x_i}(x)\\\\\n&\\qquad\\qquad(f\\in C_c^2(\\mathbb{R}^d),x\\in\\mathbb{R}^d)\n\\end{align*}\n\\] を用いて， \\[\n\\begin{align*}\n\\mathcal{L}f(x)&:=\\sum_{n=1}^N\\sum_{i=1}^db_i(x^n)\\frac{\\partial f}{\\partial x^n_i}(x)\\\\\n&\\qquad+\\sum_{n=1}^N\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x^n)\\frac{\\partial ^2f}{\\partial x^n_i\\partial x^n_j}(x)\\\\\n&\\qquad+\\sum_{a\\ne1:N}\\overline{\\iota}(V(x),a)\\biggr(f(x^{a(1:N)})-f(x^{1:N})\\biggl)\\\\\n&\\qquad\\qquad(f\\in C_c^2(\\mathbb{R}^{dN}),x\\in\\mathbb{R}^{dN},x^n\\in\\mathbb{R}^d)\n\\end{align*}\n\\] と表せる．\n\\(\\mathcal{V}\\in C_b(\\mathbb{R}_+\\times\\mathbb{R}^{dN})_+\\) を有界連続なポテンシャルとすると，任意の \\(f\\in C_b(\\mathbb{R}^{dN})\\) について， \\[\n\\begin{align*}\n\\lim_{n\\to\\infty}&\\operatorname{E}\\left[f(X^{\\Delta_n}_{\\lfloor\\tau/\\Delta_n\\rfloor})\\exp\\left(-\\sum_{k=0}^{\\lfloor\\tau/\\Delta_n\\rfloor-1}\\Delta_n\\mathcal{V}(k\\Delta_n,X^{\\Delta_n}_k)\\right)\\right]\\\\\n=&\\operatorname{E}\\left[f(Z_\\tau)\\exp\\left(-\\int^\\tau_0\\mathcal{V}(u,Z_u)\\,du\\right)\\right].\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2024/Particles/ContinuousLimit.html#footnotes",
    "href": "posts/2024/Particles/ContinuousLimit.html#footnotes",
    "title": "粒子フィルターの連続極限",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Dudley, 2002, p. 389) 演習11.1.2．↩︎\n\\(\\mathcal{P}([N]^{[N]})\\) 上の弱位相は，質量関数の各点収束に同値．よって，任意の附番 \\(a\\in[N]^{[N]}\\) に対して，\\(r(e^{-\\Delta w})(a)\\in[0,1]\\) が連続であることに同値．つまり，\\(a\\in[N]^{[N]}\\) の通りにリサンプリングされる確率が，荷重 \\(w\\in\\mathbb{R}_+^N\\) の変化に対して連続的に変化することを要請している．↩︎\nただしもちろん，\\(r\\) が連続，または \\(\\iota\\) が連続であるという仮定の下で．↩︎\nただし，\\((\\overline{\\iota}(w,0)|1)\\) の \\(1\\) は \\([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\}\\) 上の定値関数とした．↩︎\n(杉浦光夫, 1980, p. 305) 定理13.3 など．↩︎\nその結果，任意の \\(a\\in[N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\}\\) に関して，\\(\\mathbb{R}^{dN}\\ni x\\mapsto\\overline{\\iota}(V(x^1,\\cdots,x^N))(a)\\) が有界連続になる．↩︎"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html",
    "href": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html",
    "title": "ESRP HP 変更希望点まとめ",
    "section": "",
    "text": "Newsで「経済安全保障」タグを選択した状態 からページ下部の次ページに進むボタン（「２」か「→」）を押すと、何故かトップページに戻る\nNews\n\n\n\n英語のCollaborator一覧ページ から個別企業ページに飛ぶと日本語ページが表示される（英語ページがちゃんと存在してもこうなる）\nCollaborators\n\n\n\n例えば 経済安保国際連携のカテゴリページ にて，（日本語の内容の準備が後手後手になっている点をご容赦ください）表示形式が少し不自然な状態です．\n\n\n\n筆者のブラウザで開いた際のスクリーンショット\n\n\nまた，現在，新たにカテゴリを追加して合計10個になっておりますが，カテゴリページは合計７つしかございません．\n\nディスインフォ対策\n人権とビジネス\n沖縄と経済安保\n細胞農業と食糧安保\n経済安保国際連携\n経済安全保障\n若手経済安保研究会\n\nここまでは，最初にご依頼した際からある，デフォルトの７カテゴリでした．ここから新たに\n\n知的財産法\n先端技術と安全保障\n日英米豪印 (QUAD)\n\nを追加しましたので，対応するカテゴリページも用意したく存じます．\n\n\n\n現在，ランディングページ の Events の部分に，最新の Events を表示するように設定している状況です．\n\n\n\n\n\n\n写真 1: ランディングページのスクリーンショット\n\n\n\n\n\n\n写真 1 に対応する設定画面のスクリーンショット\n\n\nするとこの際，画面左半分に大きく表示された Events が，画面右半分に元々表示されていた Events と重複してしまいます．\n些細なことですが，こちらの修正をお願いできますならば，ぜひお願いしたく存じます．"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html#バグ修正",
    "href": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html#バグ修正",
    "title": "ESRP HP 変更希望点まとめ",
    "section": "",
    "text": "Newsで「経済安全保障」タグを選択した状態 からページ下部の次ページに進むボタン（「２」か「→」）を押すと、何故かトップページに戻る\nNews\n\n\n\n英語のCollaborator一覧ページ から個別企業ページに飛ぶと日本語ページが表示される（英語ページがちゃんと存在してもこうなる）\nCollaborators\n\n\n\n例えば 経済安保国際連携のカテゴリページ にて，（日本語の内容の準備が後手後手になっている点をご容赦ください）表示形式が少し不自然な状態です．\n\n\n\n筆者のブラウザで開いた際のスクリーンショット\n\n\nまた，現在，新たにカテゴリを追加して合計10個になっておりますが，カテゴリページは合計７つしかございません．\n\nディスインフォ対策\n人権とビジネス\n沖縄と経済安保\n細胞農業と食糧安保\n経済安保国際連携\n経済安全保障\n若手経済安保研究会\n\nここまでは，最初にご依頼した際からある，デフォルトの７カテゴリでした．ここから新たに\n\n知的財産法\n先端技術と安全保障\n日英米豪印 (QUAD)\n\nを追加しましたので，対応するカテゴリページも用意したく存じます．\n\n\n\n現在，ランディングページ の Events の部分に，最新の Events を表示するように設定している状況です．\n\n\n\n\n\n\n写真 1: ランディングページのスクリーンショット\n\n\n\n\n\n\n写真 1 に対応する設定画面のスクリーンショット\n\n\nするとこの際，画面左半分に大きく表示された Events が，画面右半分に元々表示されていた Events と重複してしまいます．\n些細なことですが，こちらの修正をお願いできますならば，ぜひお願いしたく存じます．"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html#デザイン変更",
    "href": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html#デザイン変更",
    "title": "ESRP HP 変更希望点まとめ",
    "section": "2 デザイン変更",
    "text": "2 デザイン変更\n\n2.1 研究員プロフィールの表示\n各ページのどこか（左側？）に、当該記事に関係する研究員のプロフィールが表示されるようにしたい．\n例えば こちらの News 記事 は，内部で NEWS REPORT AUTHOR として「関連する専門家」を選択しているので，該当する専門家のページ では下部にこの記事が表示される．\nしかし，News 記事本体のページでは，（記事本文で触れない限り）この情報は用いられない．\n顔写真と肩書きを簡単にまとめたプロフィールカードのようなものを作成できるようにし，関連づけられた News 記事と Reports 記事，そして 授業一覧 に，表示されるようにしたく存じます．\n\n\n2.2 ２つ以上のカテゴリの選択\nNews 記事と Reports 記事，そして Events には，現状カテゴリは１つしか表示できません．これはカテゴリの標準的な機能に則ったものだと思います．\nそこでですが，やはり２つ以上選択できるようにしたいので，「主要カテゴリ」と「副次カテゴリ」というように順序をつけた形でも良いので，何かしら自然な形で複数のカテゴリを選択できると，記事の分類がしやすくなると考えます．\nもし，カテゴリページ でリストされる際に不具合が生じそうならば，「副次カテゴリについては，カテゴリページには表示しない」という形でも良いと考えます．"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html#ビジュアルエディターの機能増強",
    "href": "posts/2024/Lifestyle/ESRP_HP/1月変更点.html#ビジュアルエディターの機能増強",
    "title": "ESRP HP 変更希望点まとめ",
    "section": "3 ビジュアルエディターの機能増強",
    "text": "3 ビジュアルエディターの機能増強\n\n3.1 記事ブロックの追加\n例えば この記事 では，リンクの埋め込みが美しく表示されています．これは標的リンクが WordPress でサポートされているためです．\n\n\n\n理想とするリンク埋め込みの表示例１\n\n\nこちらのページ では FRONTEO 社のウェブサイト上の記事 へのリンクを貼っていますが，FRONTEO 社の HP も WordPress で作成されているためか，うまく記事カードが作成されて埋め込まれています．\n\n\n\n\n\n\n写真 2: 理想とするリンク埋め込みの表示例２\n\n\n\n対応するビジュアルエディターのブロックは，「埋め込み」ブロックです．\n\n\n\n写真 2 は埋め込みブロックで実装されている（ビジュアルエディタのスクショ）\n\n\nですが，例えば このページ であるように，標的リンクが WordPress ではない場合，リンクが縮退してただの文章となってしまいます．また，本サイトのリンクの様式とも違うため，一見してリンクともわからない状態です．\n\n\n\n\n\n\n写真 3: 失敗しているリンク埋め込みの表示例\n\n\n\nこれのビジュアルエディターでのブロックは変わらず「埋め込み」ブロックです．\n\n\n\n写真 3 はやはり埋め込みブロックで実装されている（ビジュアルエディタのスクショ）\n\n\nこれをうまく表示させる方法として，１つは 外部プラグインである Embedly を導入することだと考えています．まさに私がここで使っているものです．\nWe Are CIPE - Center for International Private Enterprise\nですが下部に Embedly のウォーターマークが表示されるので，もし可能であれば自前でこの機能が用意できましたら，ご相談したく存じます．"
  },
  {
    "objectID": "posts/2024/Lifestyle/計算問題.html",
    "href": "posts/2024/Lifestyle/計算問題.html",
    "title": "「穴あき式」の考え方",
    "section": "",
    "text": "例題１\n\n\n\n\\[43-\\Box{}+28=56\\]\n全部で２ステップで考える．"
  },
  {
    "objectID": "posts/2024/Lifestyle/計算問題.html#sec-1",
    "href": "posts/2024/Lifestyle/計算問題.html#sec-1",
    "title": "「穴あき式」の考え方",
    "section": "1 数字の数を減らす",
    "text": "1 数字の数を減らす\nまずは式を簡単にすることを考える．\n\\[\n43-\\Box{}+28=56\n\\]\nは \\(43+28=71\\) を先に計算して，\n\\[\n71-\\Box{}=56\n\\tag{1}\\]\nと簡単にできる．\nこの時点でもう \\(\\Box{}\\) の値は解るかも知れないが，ここでは次の一手を講じてみる．"
  },
  {
    "objectID": "posts/2024/Lifestyle/計算問題.html#sec-2",
    "href": "posts/2024/Lifestyle/計算問題.html#sec-2",
    "title": "「穴あき式」の考え方",
    "section": "2 両辺に同じものを足し引きする",
    "text": "2 両辺に同じものを足し引きする\n式 (1) の両辺から \\(56\\) を引くと，\\(71-56=15\\) だから， \\[\n71-\\Box{}-56=56-56\n\\] \\[\n15-\\Box{}=0\n\\]\nと計算できる．もうすぐに \\(\\Box{}=15\\) が判る．"
  },
  {
    "objectID": "posts/2024/Lifestyle/計算問題.html#隠し手",
    "href": "posts/2024/Lifestyle/計算問題.html#隠し手",
    "title": "「穴あき式」の考え方",
    "section": "3 （隠し手）",
    "text": "3 （隠し手）\n式 1 の \\(71-\\Box{}=56\\) の状態から「両辺に \\(\\Box{}\\) を足す」と考えると，\n\\[\n71=56+\\Box{}\n\\]\nとなる．両辺から \\(56\\) を引くと，\n\\[\n15=\\Box{}\n\\]\nを得る．慣れたらこれの方が速いけど，「\\(\\Box{}\\) なんていう数字じゃない存在を，足し引きしても良いのか？」という疑問が出てきて，納得がいかないようならば，この方法は使わない方が良い．\n実際，文字式（方程式）の考え方は SAPIX の指導要領に反する．\nけど，開成の先生は，そういう（小学校の指導要領に含まれていない）解法を用いた場合でも，減点するなんてことはあり得ない，と漏らしている先生もいる（数学の清水先生）．"
  },
  {
    "objectID": "posts/2024/Lifestyle/計算問題.html#応用",
    "href": "posts/2024/Lifestyle/計算問題.html#応用",
    "title": "「穴あき式」の考え方",
    "section": "4 応用",
    "text": "4 応用\n\n\n\n\n\n\n例題２\n\n\n\n\\[3+\\Box{}-17=16\\]\n\n\n\n4.1 全く同じ方法でやってみると\nStep 1 「数字の数を減らす」を実行しよう．\\(3-17=-14\\) だから，\n\\[\n-14+\\Box{}=16\n\\]\n続いて，Step 2 両辺に \\(14\\) を足すと， \\[\n-14+\\Box{}+14=16+14\n\\] \\[\n\\Box{}=30.\n\\]\nこの最初の \\(-14\\) というのが受け入れられないかもしれない．実際，「負の数」という単元は中学１年生の範囲であるようだ．そこで，これを回避するために，Step 2 と Step 1 を逆にする．\n\n\n4.2 負の数を回避する\n「両辺に同じものを足し引きする」Step 2 を先取りして，先に両辺に \\(17\\) を足しておくと，\n\\[\n3+\\Box{}-17+17=16+17\n\\] \\[\n3+\\Box{}=33\n\\]\nになる．もう殆どわかるが，厳密にやると，両辺から \\(3\\) を引いて， \\[\n3+\\Box{}-3=33-3\n\\] \\[\n\\Box{}=30.\n\\]"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理_.html",
    "href": "posts/2024/数理法務/法律家のための統計数理_.html",
    "title": "法律家のための統計数理（？）多変量解析の基礎",
    "section": "",
    "text": "シリーズトップページはこちら．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n君塚さん に今回用いたスライドを特別に公開して良いとの許可をいただきました．どうもありがとうございます．\n以降は，講義を通じて筆者が理解した内容の，筆者自身のためのまとめであり，発表者の見解を代弁するものではない．シリーズトップページは こちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の目的と処罰対象",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の目的と処罰対象",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "1 刑法の目的と処罰対象",
    "text": "1 刑法の目的と処罰対象\n\n1.1 刑法の目的と機能\n\n\n\n\n\n\n刑法の目的：法益保護主義\n\n\n\n刑法は 法益 (Rechtsgut) の保護 を目的とするものであり，法益を侵害または危殆化する行為 を処罰対象とする．\n\n\n法益は，これが帰属する主体に応じて，\n\n個人法益\n社会法益\n国家法益\n\nに大別される．\n\n\n\n\n\n\n例：堕胎罪\n\n\n\n\n\n次の刑法の条文は，誰のどのような法益を保護していると言えるか？\n\n第二十九章　堕胎の罪 （堕胎） 第二百十二条　妊娠中の女子が薬物を用い、又はその他の方法により、堕胎したときは、一年以下の懲役に処する。\n\nこれには，すぐに考えつく立場が２つあるだろう．\n\n胎児の生命\n母体の健康\n\n１の立場では，立場上胎児をある種の「人間」と認めていることになり，２の立場では母体の一部と認めていることになる．\n\n\n\n\n\n\n\n\n\n例：胎児性過失致死傷の最高裁判例\n\n\n\n\n\n最高裁判所第三小法廷決定昭和63年2月29日 は，行為と結果の時点が分離していることが問題の１つである．というのも，胎児の段階で被った傷害による，出生後の過失致死傷が起こったという事件である．\n裁判要旨（の一部）は次のようなものであった，\n\n業務上の過失により、胎児に病変を発生させ、これに起因して出生後その人を死亡させた場合も、人である母体の一部に病変を発生させて人を死に致したものとして、業務上過失致死罪が成立する。\n\n\n\n\n\n\n1.2 刑法の処罰根拠\n\n\n\n\n\n\n刑法の処罰根拠\n\n\n\nこれには２つの説がある．\n\n法益侵害／結果無価値 (Erfolgsunwert) 説\n規範違反／行為無価値 (Handlungsunwert) 説\n\n\n\n\n1.2.1 法益侵害説\n\n\n1.2.2 規範違反説\n\n刑法とは，刑罰という制裁を予告することにより，人間の行動を心理的にコントロールして，犯罪から遠ざけようとするシステムにほかならない． (伊藤正己 & 加藤一郎, 2005, p. 111)\n\n\n\n\n1.3 刑法と道徳\n\n刑法は，法の中でも道徳と最も密接な関係を持つものであると言われている．刑法は，一般的に，「人ヲ殺シタル者ハ死刑又ハ無期若クハ三年以上ノ懲役ニ処ス」（同法一九九条），「他人ノ財物ヲ窃取シタル者.ハ窃盗ノ罪ト為シ十年以下ノ懲役ニ処ス」（同法二三五条）というふうに，裁判官が裁判をする場合の準則すなわち裁判規範のかたちで規定されているが，その背後には当然，「人を殺してはならない」，「他人の財物を窃取してはならない」という行為規範が前提とされており，これらはそれぞれ，「殺すなかれ」，「盗むなかれ」という道徳律を裏付けるものということができよう．しかし，刑法と道徳とは，必ずしも常にこのような相即不離の関係にあるわけではない．階級対立のある社会においては，階級間の価値意識の分裂に照応して，道徳もまた，支配階級の道徳と被支配階級の道徳とに分かれている．このように複数の道徳が存在する場合に，法によって裏うちされるのは，必ず支配階級の道徳である．従って，支配階級の道徳を反映した法の一環としての刑法と，被支配階級の道徳とは，しばしばするどく対立する．例えば，資本主義社会にあっては，ストライキは被支配階級である労働者がその生存を維持するための最も強力な手段であり，従って労働者にとって組合の決定によりストライキを決行する場合に団結を固くしてこれに積極的に参加することは，当然のモラルである．しかも，資本主義国家においては，このように労働者階級のモラルに忠実な行為がしばしば刑罰の対象とされてきたし，勤労者の団体行動権を保障した日本国憲法の下でさえも，たとえば，公務員のストライキを煽り，そそのかすような行為は犯罪とされているのである． (渡辺洋三, 1993, pp. 216–217)\n\n刑法は国民に対して道徳的な規範を示していると読み取れる．だが，そのことをどう評価すべきかは，筆者はまだわからない．時代と共に変わるべきものであるということは確かである．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の体系",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の体系",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "2 刑法の体系",
    "text": "2 刑法の体系\n\n2.1 犯罪の３成立要件\n\n\n\n\n\n\n犯罪の３成立要件\n\n\n\n\n構成要件該当性\n\n\n基本的には 条文に具体的に示されている要件に該当するか ということである．\nその主たる２大要素は 行為 と 結果 である．\nこれを 客観 と 主観 に分けて議論することも慣例である．\n\n\n違法性\n\n\n刑法の処罰根拠たる 法益への侵害と危殆化 が実現しているか（違法性阻却事由）を判断する．\n基本的には，正当行為・正当防衛・緊急避難（第35条から第37条）への該当の有無の判断である．\n法益保護の方向性を持った行為ならば，\n\n\n有責性\n\n\n法的避難可能性 の有無のこと．\n\n\n\n「構成要件」の段階に「行為」と「結果」をどこまで入れるかが，行為無価値と結果無価値．\n\n\n\n\n\n\n例：実質的違法論\n\n\n\n\n\n「貧しすぎて」は根拠条文がないが，違法性阻却事由になる．これは法益保護の方向だからである (厚生労働省, 2004)．\nこれには「当該行為の具体的状況その他諸般の事情を考慮に入れ、それが法秩序全体の見地から許容されるべきものであるか否か」を判別する作業を伴う．最高裁判所第二小法廷判決 昭和50年8月27日\n\n\n\n\n\n2.2 故意の概念\nこの概念が難しすぎる．\n\n「（構成要件的）故意」とは「犯罪構成要件の認識と認容」である．\n「責任故意」とは「違法性の認識と認容」である．\n\n\n\n\n\n\n\n例"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html#令和５年司法試験問題",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html#令和５年司法試験問題",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "3 令和５年司法試験問題",
    "text": "3 令和５年司法試験問題\n\n\n\n令和５年司法試験問題論文式試験問題集［刑事系科目第１問］\n\n\n\n\n\n\n\n\n事例１\n\n\n\n\n\n\n甲は、乙及び丙と共に、後記計画に基づき、常習的に高齢者から現金をだまし取っていた。その計画は、\n\n甲が資産家の名簿を見て、現金をだまし取る対象者を選定する。\n甲が警察官に成りすまして相手方に電話をかけ、「Ｘ警察署の○○です。この度、この地域を担当することになりました。今後、当署からの連絡はこの番号からかけますので、御登録をお願いします。」などとうそを言って、名前と電話番号を告げる（以下、この内容の電話を「１回目の電話」という。）。\nその翌日、甲が相手方に電話をかけ、「昨日電話した○○です。あなたの預金口座が、不正に利用されている疑いがあります。捜査のために必要なので、お持ちの預金口座に１００万円を超える残高があるようでしたら、速やかに全額を引き出して自宅に持ち帰った後、こちらに電話をください。」などとうそを言う（以下、この内容の電話を「２回目の電話」という。）。\n相手方に預金口座から現金を引き出させて、自宅にその現金を持ち帰らせる。\nその後、相手方からかかってきた電話で、甲が、相手方の現金引出しを確認した上、「これから警察官がそちらに向かいます。」とうそを言う。\nその約１時間後、乙及び丙が警察官を装って相手方の家を訪ねる。\n乙及び丙が、捜査のために必要なので現金を預けてほしい旨のうそを言い、その交付を受けて現金をだまし取る。\n\nというものであった。\n甲らは、上記計画に従い、以下の行為に及んだ。\n\n甲は、某月１日、名簿から現金をだまし取る対象者として高齢の男性Ａを選んだ。\n甲は、同日午前１０時、Ａに１回目の電話をかけた。\n甲は、同月２日午前１０時、Ａに２回目の電話をかけた。\n甲のうそを信用したＡは、預金口座から２００万円を引き出して自宅に持ち帰った。\n甲は、同日正午、Ａからかかってきた電話に出て、Ａが２００万円を引き出したことを確認した上、Ａに対し、「これから警察官がそちらに向かいます。」とうそを言った。\n乙及び丙は、甲の指示に基づき、同日午後１時、警察官を装ってＡ宅を訪ねた。しかし、乙らの姿を見て不審に思ったＡが玄関ドアを開けなかったため、乙及び丙は、捜査のために必要なので現金を預けてほしい旨のうそを言うことができないまま、Ａから現金をだまし取ることを断念した\n\n\n\n\n\n\n\n\n\n\n\n設問１\n\n\n\n\n\n【事例１】におけるＡに対する甲の罪責に関し、以下の⑴及び⑵について、答えなさい。なお、⑴及び⑵のいずれについても、自らの見解を問うものではない。\n\n甲に詐欺未遂罪の成立を認める立場から、その結論を導くために、どのような説明が考えられるか。詐欺罪が「人を欺いて財物を交付させ」るという手段・態様を限定した犯罪であるのに、その実行の着手に「現金の交付を求める文言を述べること」を要しないと考える理由に触れつつ論じなさい。\n1の説明に基づくと、上記 1~6 のうちどの時点で実行の着手を認めることになるのか。具体的事実に即して、それより前の時点との実質的相違を明らかにしつつ論じなさい。\n\n\n\n\n直観的には甲を正犯として罰するべきであると思われる．その方向性で議論するための，厳密な論拠を示すことが求められている．\n\n\n\n\n\n\n事例２\n\n\n\n\n\n【事例１】の１の事実に続けて、以下の事実があったものとする。\n\n甲は、上記計画に従い、某月５日午前１０時、名簿から現金をだまし取る対象者として高齢で一人暮らしの男性Ｂを選んだ上、Ｂに１回目の電話をかけ、さらに、同月６日午前１０時、２回目の電話をかけた。Ｂは、甲のうそを信用し、同日午前１０時３０分、預金口座から３００万円を引き出して自宅に持ち帰った。甲は、同日正午、Ｂからかかってきた電話で、Ｂが３００万円を引き出して自宅に持ち帰った旨を聞いたことから、「これから警察官がそちらに向かいます。」とうそを言い、Ｂは「分かりました。待っています。」と答えた。甲は、乙及び丙に対し、高齢で一人暮らしの男性Ｂがうそを信用し、３００万円を自宅に用意している旨を告げ、計画どおり、捜査のために必要なので現金を預けてほしい旨のうそを言って、３００万円をだまし取ってくるように指示し、乙及び丙はこれを了承した。\n乙は、甲の上記指示を受け、丙と共にＢ宅に向かうことにしたが、その道中で、Ｂを縛り上げてしまえば、より確実に現金を手に入れることができると考え、丙に対し、「ジジイをだますより、縛った方が確実に金を奪える。縛って、金を奪ってしまおうぜ。奪った３００万円を３人で分ければ問題ないだろう。」などと言い、丙はこれを了承した。そして、乙及び丙は、Ｂの手足を縛るためのロープと口を塞ぐための粘着テープを準備した上、同日午後１時、Ｂ宅へ赴き、インターホンを鳴らして警察官であることを告げ、Ｂに玄関ドアを開けさせた。乙及び丙は、直ちにＢ宅内に押し入り、Ｂの手足をそれぞれロープで縛り、口を粘着テープで塞ぎ、Ｂを床の上に倒した。そして、リビングルームに移動した乙及び丙は、Ｂが預金口座から引き出してテーブル上に置いていた上記３００万円を見付け、同日午後１時１０分、同３００万円を持ってＢ宅を出た。その後、乙及び丙は、甲に対し、いつもどおりのやり方でＢから３００万円をだまし取ってきたと虚偽の報告をし、それぞれ１００万円ずつ山分けした。\n同日午後３時、Ｂの娘ＣがＢ宅を訪れ、緊縛されたＢを発見した。Ｃから上記ロープ及び粘着テープを取り外してもらったＢは、立ち上がろうとしたものの、長時間の緊縛による足のしびれでふらついて倒れそうになった。そのため、Ｃは、Ｂを座らせ、そのままでいるように言った。Ｂは、それにもかかわらず、その１分後、Ｃがその場を離れた隙に、奪われた物の有無を確認するために立ち上がろうとした。その際、Ｂは、まだ上記足のしびれが残っていたために、転倒して床に頭を打ち付け、全治２週間を要する頭部打撲の傷害を負った。\n\n\n\n\n\n\n\n\n\n\n設問２\n\n\n\n\n\n【事例２】における甲、乙及び丙の罪責について、論じなさい（住居等侵入罪（刑法第１３０条）及び特別法違反の点は除く。）。\n\n\n\n\n\n\n\n\n\n事例３\n\n\n\n\n\n【事例２】の事実に続けて、以下の事実があったものとする。\n\nＹ警察署の警察官Ｄは、【事例２】に係る事件につき、乙に対する逮捕状を取得し、乙の逮捕に向かったところ、乙が細い路地を丁と共に歩いているのを発見した。Ｄは、逮捕のため、乙に接近しようとしたが、それに気付いた乙が走って逃げ出したため、急いで乙を追おうとした。丁は、乙が警察官に逮捕されそうになっていることを察し、乙を逃がそうと考え、怒号しながら両手を広げて立ちはだかり、道を塞いだ。そのため、Ｄは、直ちに乙を追い掛けることができず、乙を逮捕することができなかった。\nその後、Ｄは、Ｙ警察署の警察官５名に乙を追跡して逮捕するよう応援を要請した。丁は、警察官による乙の逮捕を妨害しようと考え、Ｙ警察署に電話をかけ、「Ｙ署近くの路上で、通り魔に刺されました。すぐに来てください。」などとうそを言った。そのため、上記警察官５名は、更なる通り魔事件発生への警戒等を行わざるを得なくなった結果、乙を追跡できず、乙を逮捕することができなかった。\n\n\n\n\n\n\n\n\n\n\n設問３\n\n\n\n\n\n【事例３】における前記６の事実につき、丁に業務妨害罪の成立を否定しつつ（丁による怒号などは、公務執行妨害罪における暴行・脅迫には当たらないが、業務妨害罪における威力には当たることを前提とする。）、前記７の事実につき、丁に上記警察官５名に対する業務妨害罪の成立を肯定する立場からは、その結論を導くために、どのような説明が考えられるか、論じなさい。なお、自らの見解を問うものではない。"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理___.html",
    "href": "posts/2024/数理法務/法律家のための統計数理___.html",
    "title": "法律家のための統計数理（？）AI の信頼性",
    "section": "",
    "text": "シリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理___.html#state-v.s.-loomis",
    "href": "posts/2024/数理法務/法律家のための統計数理___.html#state-v.s.-loomis",
    "title": "法律家のための統計数理（？）AI の信頼性",
    "section": "1 State v.s. Loomis",
    "text": "1 State v.s. Loomis\n(山本龍彦 & 尾崎愛美, 2018)"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html",
    "href": "posts/2024/Kernels/Deep2.html",
    "title": "数学者のための深層学習２",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#トランスフォーマー",
    "href": "posts/2024/Kernels/Deep2.html#トランスフォーマー",
    "title": "数学者のための深層学習２",
    "section": "1 トランスフォーマー",
    "text": "1 トランスフォーマー\n\n1.1 名前の由来と背景\nトランスフォーマー (Vaswani et al., 2017) は，注意 (attension) という機構を通じて，時系列データの依存関係を効率的に学習することの出来るモデルである．この「変換器」という名前は，後述の内部表現ベクトル \\(Y\\) を，入力 \\(X\\) から次元を変えずにより良いものに「変換する」というところから名前が付けられている．\n初めは自然言語処理（特に機械翻訳）の文脈で導入されたデコーダーとエンコーダーの組からなるモデルであるが，そのエンコーダー部分だけで言語，画像，動画などあらゆる系列データのモデリング全体で抜群の性能を発揮する上に，これら複数ドメインのデータを組み合わせてモデリングすることもできる（第 4 節）．\nさらに，トランスフォーマーはアーキテクチャとして（CNN や RNN などに比べると）シンプルであり，大規模なデータセットで大規模なモデルを訓練することが出来るスケーラビリティが魅力である．また，モデルの大きさに対して性能が単調に改善するというスケーリング則 (Hestness et al., 2017), (Kaplan et al., 2020) が成り立つことが示されており，大規模な資源を投下して大規模なモデルを作る経営判断も下しやすかった．\n\n\n\nScaling Laws (Kaplan et al., 2020)\n\n\nその後すぐに，一度大規模なモデルを訓練してしまえば，少しの修正を施すのみで種々の下流タスクに適用することが可能であることが発覚した．これを 基盤モデル という（第 3.2 節）．\n\n\n1.2 注意機構\nトランスフォーマーの核はその注意機構にある．とはいっても，注意機構自体はトランスフォーマー以前から存在した技術である．\n元々機械翻訳に用いられていたエンコーダー・デコーダー型の RNN の性能を向上させる機構として提案された (Bahdanau et al., 2015)．その後，(Vaswani et al., 2017) の Attention is All You Need とは，注意機構のみが重要で，RNN としての構造（や画像では畳み込みの構造）を排してシンプルにした方が更に性能が向上する，という報告である．\n時系列データの解析では，そして自然言語処理ではとりわけ，文脈というものが重要である．しかし文脈は長期の依存関係になることもしばしばあり，従来の RNN ではこのモデリングに苦労していた (bottleneck problem)．\n注意機構は，遠く離れた２つのトークンも直接に相互作用を持つアーキテクチャになっており，この点を抜本的に解決したものである．その結果，元の RNN のアーキテクチャも不要とするくらいのモデリング能力を，自然言語のみでなく，画像や動画に対しても示したのである．\n注意機構は自己注意と交差注意に分けられる．\n\n1.2.1 枠組み\nトランスフォーマーに入力する系列を \\(\\{x^n\\}_{n=1}^N\\subset\\mathbb{R}^D\\) で表す．生のデータをそのままモデルに入れるわけではないので，別の言葉で呼び変える．\n慣習として，特に言語データの場合は各 \\(x^n\\) を トークン (token) という．画像では パッチ (patch) ともいう．\n以降，\\(X:=(x^n)_{n=1}^N\\in M_{ND}(\\mathbb{R})\\) とも表す．\n\n\n1.2.2 自己注意機構のプロトタイプ\n自己注意機構とは，\\(Y=AX\\) によって定まる \\(M_{ND}(\\mathbb{R})\\) 上の線型変換 \\(X\\mapsto Y\\) のことである： \\[\ny^n=\\sum_{m=1}^N a^n_mx^m,\n\\tag{1}\\] \\[\na^n_m=\\frac{e^{(x^n)^\\top x^m}}{\\sum_{k=1}^Ne^{(x^n)^\\top x^k}}.\n\\tag{2}\\] ここで，\\(A=(a^n_m)_{n,m\\in[N]}\\in M_N(\\mathbb{R})\\) は 確率行列 をなし，その成分を 注意荷重 (attention weight) という．\nこの変換において，同じ \\(x^m\\) の値を，３回別々の意味で使われていることに注意する：\n\n式 1 における \\(x^m\\) は，新たな表現 \\(y^n\\) を作るためのプロトタイプにような働きをしている．これを 値 (value) という．\n式 2 において，内積が用いられており，\\(x^n\\) と \\(x^m\\) の類似度が測られている．\n\n\\(x^m\\) を，\\(x^m\\) が提供出来る情報を要約した量としての働きをし，鍵 (key) という．\n\\(x^n\\) は，\\(x^n\\) と関連すべき情報を要求する役割を果たし，クエリ (query) という．\n\n最終的に，鍵とクエリの類似度・マッチ度を，ソフトマックス関数 を通じて確率分布として表現し，値の空間 \\(\\{x^m\\}_{m=1}^N\\) 上の確率質量関数 \\(\\{a^n_m\\}_{m=1}^N\\) を得ている．これに関して 平均する ことで，鍵 \\(y^n\\) を得る．\n\n\n\n1.2.3 内積による自己注意機構\n３つの別々の役割を果たしている以上，それぞれ固有の表現を持っていても良いはずである．そこで，値，鍵，クエリに，それぞれにニューラルネットワーク \\(W_{(\\Lambda)}\\in M_{DD_{(\\Lambda)}}(\\mathbb{R})\\;(\\Lambda\\in\\{V,K,Q\\})\\) を与えて固有の表現 \\[\nx_{(\\Lambda)}^n:=XW_{(\\Lambda)}\n\\] を持たせ，この \\(W_{(\\Lambda)}\\) を誤差逆伝播法により同時に学習することとする．\nこうして得るのが，内積による自己注意機構 (dot-product self-attention mechanism) である．このとき，\\(D_{(K)}=D_{(Q)}\\) は必要だが，\\(y^n\\in\\mathbb{R}^{D_{(V)}}\\) は，元の次元 \\(D\\) と異なっても良いことに注意．\n最後に，ソフトマックス関数の適用において，勾配消失を回避するために，次元 \\(D_{(K)}\\) に応じたスケーリングを介して \\[\na^n_m=\\frac{e^{\\frac{\\left(x^n_{(Q)}\\right)^\\top x^m_{(K)}}{\\sqrt{D_K}}}}{\\sum_{k=1}^Ne^{\\frac{\\left(x^n_{(Q)}\\right)^\\top x^k_{(K)}}{\\sqrt{D_K}}}}\n\\] とする．これを最終的な 自己注意機構 (scaled dot-product self-attention mechanism) という．\n\n\n1.2.4 交差注意\nデコーダーとエンコーダーの接続部に用いられる 交差注意 (cross attention) については，ここでは触れない．\n\n\n1.2.5 マスキング\n実際に学習するとき，注意荷重 \\(A\\) は上三角部分が \\(-\\infty\\) になったものを用いる．\nこれは，次のトークンを予測するにあたって，そのトークンより後のトークンを見ないようにするためである．\n\n\n\n1.3 トランスフォーマーの全体\n注意機構に加えて，次の３要素を含め，典型的には 20 から 24 層を成した深層ニューラルネットワークがトランスフォーマーの全てである．1\n\n\n\nTransformer Architecture (Vaswani et al., 2017)\n\n\n\n1.3.1 多頭注意\n以上の自己注意機構を１単位として，これを複数独立に訓練し，最終的にはこれらの線型結合を採用する仕組みを 多頭注意 (multi-head attention) という．\nこれにより，種々の文脈をより頑健に読み取ることが出来るようである．\n\n\n1.3.2 残差結合と正規化\n更に勾配消失を回避するために，残差結合 を導入し，訓練の高速化のために正規化 (Ba et al., 2016) が導入される．\nそして，モデルを大規模化していくには，この「多頭注意＋残差結合と正規化」のブロックを積み重ねる．\n\n\n1.3.3 多層パーセプトロン\n注意機構は線型性が高いため，多頭注意の層の間に，通常の Feedforward ネットワークもスタックして，ネットワークの表現能力を保つ工夫もされる．\n\n\n1.3.4 正規化レイヤーについての補足\nレイヤー正則化 (layer normalization) (Ba et al., 2016) は，バッチ正規化 (batch normalization) (Ioffe & Szegedy, 2015) が RNN にも適するようにした修正として提案された．\nバッチ正規化は，ニューラルネットワークの内部層の学習が，手前の層のパラメータが時事刻々と変化するために安定した学習が出来ないという 内部共変量シフト (internal covariate shift) にあると突き止め，これをモデルアーキテクチャに正規化層を取り入れることで解決するものである．\n正規化層は，ニューラルネットワークへの入力を，平均が零で分散が \\(1\\) になるように変換する．元々，ニューラルネットワークの入力を正規化してから学習させることで学習が効率化されることは知られていた (LeCun et al., 2012) が，バッチ正規化は，これをバッチごとに，かつ，モデルの内部にも取り込んだものである．\nバッチ正規化は精度の上昇と訓練の加速をもたらす．これはバッチ正規化により大きな学習率で訓練しても活性化が発散せず，これにより訓練時間の短縮と，局所解に囚われにくく汎化性能の向上がもたらされているようである (Bjorck et al., 2018)．\n\n\n\n1.4 なぜトランスフォーマーはうまく行くのか？\n注意機構は全体として線型変換になっている．これをカーネル法などを用いて非線型にする試みは多くあるが，これは成功していない．2\nその代わり，トランスフォーマーのパラメータ数のほとんどは FF 層（ 節 1.3.3 ）によるものであり，この層が大きな表現能力を持っていることが，トランスフォーマーの性能を支えていると考えられている．3\n注意機構は，遠く離れた２つのトークンを直接相互作用可能にすることに妙がある．実際，注意機構は，荷重行列を入力から学習するような，荷重平均プーリング (weighted mean pooling) ともみなせる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#言語トランスフォーマー",
    "href": "posts/2024/Kernels/Deep2.html#言語トランスフォーマー",
    "title": "数学者のための深層学習２",
    "section": "2 言語トランスフォーマー",
    "text": "2 言語トランスフォーマー\nトランスフォーマーの訓練は，後述するように事前学習と事後調整からなる．事後調整は 節 3 で述べる．ここでは，事前学習を，言語を例に取って説明する．\nトランスフォーマーの事前学習とは レトロニム であり，トークン（≒単語）上の確率分布をモデリングをすることに他ならない．\n古典的には \\(n\\)-gram 節 2.2.1 などのモデルが用いられていたが，これをニューラルネットワークによって作ることはトランスフォーマー以前から試みられていた (Bengio et al., 2000)．\nその後，トランスフォーマーの登場まで，これには RNN 節 2.2.2 が主に用いられていた．しかし，RNN は長い系列に対しては勾配消失とボトルネック問題が起こりやすく，また，訓練の並列化が難しいという問題があった．\n\n2.1 言語の取り扱い\n\n2.1.1 単語の分散表現\n言語をそのまま扱うのではなく，トークン \\(x^n\\in\\mathbb{R}^D\\) の形に符号化する必要がある．\n言語には他にも改行や数式，コンピューターコードがあるが，まずは単語の表現を考える．\n単語を Euclid 空間内に埋め込んだものを 分散表現 (distributed representation) という．これを２層のニューラルネットワークで行う技術が word2vec である (Mikolov et al., 2013)．\nその訓練法には２つあり，窓の幅を \\(M=5\\) などとすると，\n\nCBOW (Continuous Bag of Words)：前後 \\(M\\) 語のみを見せて，中央の語を予測する．\nContinuous Skip-gram：中央の語を見せて，前後 \\(M\\) 語を予測する．\n\nという，いずれも教師なしの方法によって学習される．\n\n\n2.1.2 トークン化\nバイトペア符号化 (BPE: Byte Pair Encoding) (Sennrich et al., 2016) は，データ圧縮の手法であるが，単語に限らず種々のデータを含んだ文字列を符号化するのに用いられる．\n\n\n2.1.3 位置情報符号化\nトランスフォーマーはそのままではトークンの順番を考慮しないため，トークンの順番の情報も符号化時に含める必要がある．これを 位置情報符号化 (positional encoding) という (Dufter et al., 2021)．\nこのようにして，位置情報はトランスフォーマーのモデル構造を修正して組み込むのではなく，符号化の段階で組み込み，トランスフォーマーはそのまま使うのである．\nこれは，位置情報をトークンと同じ空間に埋め込んだ表現 \\(r^n\\) を学習し， \\[\n\\widetilde{x}^n:=x^n+r^n\n\\] を新たな符号とする．4\n\n\n\n2.2 従来の言語モデル\n文章をトークン列 \\(\\{x^n\\}_{n=1}^N\\subset\\mathbb{R}^D\\) に置き換えたあとに，この上の結合分布 \\(p(x^1,\\cdots,x^N)\\) をモデリングすることが，言語モデル の目標である．\n\n2.2.1 \\(n\\)-gram\n\\(n\\ge1\\) とし，\\(x_i=0\\;(i\\le0)\\) として， \\[\np(x_1,\\cdots,x_N)=\\prod_{i=1}^Np_{\\theta_i}(x_i|x_{i-n},\\cdots,x_{i-1})\n\\] という形で \\(p\\) をモデリングする．\nこれを \\(n\\)-gram モデルと呼ぶが，文章の長さ \\(N\\) が大きくなると，必要なパラメータ \\(\\theta_n\\) の数が増加する．\nこれに対処する方法としては，隠れ Markov モデル を用いることが考えられる．\nニューラルネットワーク (Bengio et al., 2000) を用いることも出来る．しかし，依存の長さ \\(n\\ge1\\) が固定されていることはやはり問題である．\n\n\n2.2.2 RNN\n長さ制限のない長期的な依存関係を Feedback network によって表現することが，(Mikolov et al., 2010) によって試みられた．\nこれは通常のニューラルネットワーク (FFN: Feedforward Network と呼ばれる) に，出力の一部を次の入力に使うという回帰的な流れを追加することで，隠れ Markov モデルのように次に持ち越される内部状態を持つことを可能にしたモデルである．\nしかしこれは学習が困難であることと，結局長期的な依存関係は効率的に学習されないという２つの問題があった．\n誤差の逆伝播を時間に対しても逆方向に繰り返す必要がある (Backpropagation through time) ので，長い系列に対しては逆伝播しなければいけない距離が長く，勾配消失・爆発が起こりやすい．これは長期的な依存関係を学習しにくいということももたらす．5 また，並列化も難しく，大規模なモデルの学習は難しい．\nこれに対処するために，モデルの構造を変えて過去の情報を流用しやすくする方法も種々提案された．LSTM (Long short-term memory) (Hochreiter & Schmidhuber, 1997) や GRU (Gated Recurrent Unit) (Cho et al., 2014) などがその例である．\n\n\n\n2.3 トランスフォーマーによる言語モデルとその訓練\nトランスフォーマーによる言語モデルの最大の美点は，自己教師あり学習による言語モデルの学習が可能である点である．これによりインターネットに蓄積していた大量のデータが利用可能になる．\nパラメータを自己教師あり学習により初期化することで，言語モデルの性能が大幅に改善できることは (Dai & Le, 2015) が LSTM 入りの RRN における実験を通じて最初に指摘したようである．\nラベルデータを必要とするならば，これは本当の意味でスケーラブルではなかったであろう．\n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) は，双方向エンコーダーである．\nGPT (Generative Pre-trained Transformer) (Radford et al., 2018) は，単方向デコーダーである．\nBART (Bidirectional and Auto-Regressive Transformer) (M. Lewis et al., 2020) は，双方向エンコーダーと単方向デコーダーの両方を持つ．\n\n\n2.3.1 デコーダーのみの言語モデル\nGPT などの生成モデルは，デコーダー部分のトランスフォーマーの機能を主に用いている．\nこれはまず，\n\nトークン列 \\((x^n)_{n=1}^{N-1}\\) を入力し，条件付き分布 \\(p(x^N|x_1,\\cdots,x^{N-1})\\) を得る．\n分布 \\(p(x^N|x_1,\\cdots,x^{N-1})\\) からサンプリングをする．\n\nの２段階で行われる．こうして \\((x^n)_{n=1}^N\\) を得たら，次は \\(x^{N+1}\\) を生成し，文章が終わるまでこれを続けることで，最終的な生成を完遂する．\n\n2.3.1.1 条件付き分布の表現\n大規模なデータセットの上で，文章を途中まで読み，次のトークンを推測する，という自己教師あり学習を行うことで，トークン上の条件付き分布を学習する．\nこの際に，先のトークンの情報は使わないように，注意機構を工夫 (masked / causal attention) して訓練する．\n\n\n2.3.1.2 条件付き分布からのサンプリング\n仮に最も確率の高いトークンを毎回選択する場合，出力は決定論的であり，同じ表現を繰り返すことが多くみられる．\n実は，より人間らしい表現は，確率の低いトークンもかなら頻繁に採用される (Holtzman et al., 2020)．\nかと言って，純粋なサンプリングをしたのでは，文章全体から見て意味をなさない場合も多い．\nこれを解決したのが top-\\(p\\) sampling / nucleus sampling (Holtzman et al., 2020) である．\nGPT-2 にも実装されている ようである．\n\n\n\n2.3.2 エンコーダーのみの言語モデル\nBERT (bidirectional encoder representations from transformers) (Devlin et al., 2019) などの言語理解モデルは，エンコーダ部分のトランスフォーマーの機能を主に用いている．その結果，生成は出来ない．\n訓練は，データセットから単語を確率的に脱落させ，これを補完するように訓練する．結果として，文章の前後両方 (bidirectional) の文脈を考慮するようになるのである．\n実際に使う際は，例えば感情の判別などでは，文章の冒頭に [class] などの特殊なトークンを置き，これをエンコーダーに通してトークンが何に置き換わるかを見ることで，判別を実行することができる．\n\n\n2.3.3 エンコーダー・デコーダーの言語モデル\nトランスフォーマーは原論文 (Vaswani et al., 2017) では，エンコーダーとデコーダーがセットになったモデルとして提案された．\nこれは機械翻訳を念頭に置いていたため，RNN の構造を引き継いだ形で提案されたためである．この場合，次のようにしてモデルは使われる\n\n入力 \\(X\\) をエンコーダーに通し，内部表現 \\(Z\\) を得る．\nこの内部表現 \\(Z\\) を元に，デコードした結果 \\(Y\\) を出力する．\n唯一，\\(Z\\) をデコーダーに渡す部分での注意機構層では，鍵と値としては \\(Z\\) を使うが，クエリとしては \\(Y\\) を使う．\n\n３の機構を エンコーダー・デコーダーの注意機構 (encoder-decoder / corss attention mechanism) といい，これによって \\(Z\\) と \\(Y\\) のトークンの間の類似度をモデルに取り入れる．\n\n\n\n2.4 近年の進展\n\n2.4.1 分布外データに対するロバスト性\nGPT-4 などの大規模言語モデルが，コンテクスト内学習 (in-context learning) が可能であることが，興味深い現象として解析されている．\nこの文脈内学習とは，パラメータをそのタスクに対して事後調整した訳でもないのに優れた性能を見せること を指す (Garg et al., 2022)．\n\n\n\nAn Example of In-context Learning (Bubeck et al., 2023)\n\n\nこれの理論的な解明が進んでいる．\nトランスフォーマーの注意機構をデータが通過する過程が，勾配降下を通じて局所モデルを学習する過程と等価になっている見方が指摘されている (von Oswald et al., 2023)．すなわち，トランスフォーマーを通過すること自体が，汎用的な学習そのものになっている (Akyürek et al., 2023), (Bai et al., 2023), (Guo et al., 2024), (Kim & Suzuki, 2024)．\n\n\n2.4.2 状態空間モデルによる依存構造モデリング\nトランスフォーマーの注意機構はデータ内の長期的な依存関係をモデリングできる点が革新的なのであった．\nしかし，そのシークエンスの長さに対して計算複雑性が非線型に増加してしまう点を改良すべく，種々の代替的なアーキテクチャが試みられており，中でも状態空間モデルを中間層に用いるモデルが注目されている．\nしかし，まだまだ長期的な依存関係の処理を苦手とするため，言語においては注意機構ほど性能が出ず，トランスフォーマーに比べて並列計算が難しいために実行速度が遅くなる，という問題がある．\nしかし，これらは解決可能で，トランスフォーマーの性能を凌駕する可能性があるとされている (Gu et al., 2022), (Fu et al., 2023)．まず，状態空間モデルのパラメータを入力から決めることで依存関係のモデリングを豊かにし（ 選択的状態空間モデル (Selective SSM)），並列可能なアルゴリズムも提案されている．\n実際に，選択的状態空間モデルを注意機構と多層パーセプトロン層の代わりに取り入れた Mamba (Gu & Dao, 2024) は同じサイズのトランスフォーマーの性能を凌駕する．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#sec-fine-tuning",
    "href": "posts/2024/Kernels/Deep2.html#sec-fine-tuning",
    "title": "数学者のための深層学習２",
    "section": "3 基盤モデル",
    "text": "3 基盤モデル\n大規模なトランスフォーマーを，インターネットに蓄積していた大量のデータを用いて訓練することにより得るモデルは，チャットボットや感情分析，要約など種々の下流タスクに少しの事後調整を施すだけで抜群の性能を発揮することが発見された．\nこれを 基盤モデル という．\n\n3.1 大規模言語モデル\n\n3.1.1 名前の由来と背景\n自然言語処理にトランスフォーマーを応用した例は大きな成功を見ている．GPT (Radford et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023b) のシリーズはその代表であり，特に GPT-4 はその文脈内学習能力（第 2.4.1 節）の高さから AGI の実現に向けた重要な一歩とも評されている (Bubeck et al., 2023)．\nその成功は，アーキテクチャとして優れているという点よりもむしろ，並列化が可能であり GPU などの計算資源を効率的に使える (Weng & Brockman, 2022) という点にあり，アーキテクチャの改良よりも計算資源の増強が最終的に大きな進歩をもたらすという側面が大きい，という認識が優勢になっている (R. Sutton, 2019)．これはスケーリング則として理論的にも理解が試みられている (Kaplan et al., 2020)．\nこの観点から，トランスフォーマーを用いた事前学習済みの言語モデルが，種々のタスクをほとんど例示なし (few-shot / zero-shot) で解ける能力を創発する程度に大きい場合，その規模が意味を持つことを強調して，大規模言語モデル (LLM: Large Language Model) とも呼ぶ (Zhao et al., 2023)．\n\n\n3.1.2 最適なモデルサイズ\n従来の LLM は，訓練データに対してモデルが大規模すぎる 可能性があることが (Hoffmann et al., 2022) で指摘された．\n\nSize of LLMs\n\n\n\n\n\n\n\nModel\nParameters\nTraining Tokens\n\n\n\n\nLaMDA (Thoppilan et al., 2022)\n137B\n168B\n\n\nGPT-3 (Brown et al., 2020)\n175B\n300B\n\n\nJurassic (Lieber et al., 2021)\n280B\n300B\n\n\nGopher (Rae et al., 2021)\n280B\n300B\n\n\nChinchilla\n70B\n1.4T\n\n\n\n最適なパラメータ-学習データサイズの比を考慮して設計された Chinchilla (Hoffmann et al., 2022) は，モデルのサイズは最も小さいにも拘らず，種々の下流タスクに対して，他のモデルを凌駕することが (Hoffmann et al., 2022) で報告されている．\n\n\n\nNumber of Training Tokens BabyLM Challenge\n\n\n\n\n\n\n\n\n種々の LLM とマルチモーダル化\n\n\n\n\n\nGoogle の GShard (Lepikhin et al., 2021)，PaML (Chowdhery et al., 2022)，M4 (Aharoni et al., 2019)，Google Brain の Switch Transformers (Fedus et al., 2022)，Google DeepMind の Gopher (Rae et al., 2021) などがある．\n最近のものでは，\n\nGoogle の LaMDA (Thoppilan et al., 2022) は会話に特化した LLM である．\nGoogle から 12/6/2023 に Gemini (Team et al., 2023) が発表され，2/16/2024 には Gemini 1.5 が発表された．\n\nこれに伴い，Bard と Duet AI はいずれも Gemini に名称変更された．\nGShard (Lepikhin et al., 2021) 同様，トランスフォーマーに加えて，新しいアーキテクチャである Sparsely-Gated MoE (Shazeer et al., 2017) が用いられている．これはモデルのパラメータを分割し（それぞれを専門家 expert という），１つの入力にはその一部分しか使わないようにすることでメモリを節約し並列化を可能にする手法である．\n文書から高精度にテキストを抽出する LMDX (Language Model-based Document Information Extraction and Localization) (Perot et al., 2023) も用いられている．\n\nOpenAI から 9/5/2023 に GPT-4V (OpenAI, 2023c) が発表され，ChatGPT にも実装された．\n\nMicrosoft の研究者も，GPT-4V の出来ること関する考察 (Yang et al., 2023) を発表している．\n\n\n\n\n\n\n\n3.1.3 訓練の並列化\nここまで大規模なモデルだと，訓練時の GPU の並行計算を適切に計画することが肝心になる．\n\n\n\nfour types of parallelism (Weng & Brockman, 2022)\n\n\nByteDance の MegaScale (Jiang et al., 2024) は，12,288 の GPU を用いながら，55.2 % の Model FLOPs Utilization を引き出した．\n更なる LLM の訓練の効率化には，GPU による並列計算におけるボトルネックである メモリ帯域幅 を克服するために，分散型訓練手法を採用することが提案されている．\n\n\n\n3.2 「基盤モデル」と事後調整\nGPT の P とは Pre-trained である．自己教師あり学習によって 事前学習 をしたあと，その後のタスクに応じて，教師あり学習によって 事後調整 (fine-tune) を行う．6\n事後調整では，目的関数に KL 乖離度を入れるなどして，元のモデルから遠く離れすぎないように工夫されている．\n事後調整を行う前の大規模言語モデルのことを，種々の応用や下流タスク (downstream task) の基礎となるモデルであることと，そのものでは未完成であることとを強調して，基盤モデル (foundation model) とも呼ばれる (Bommasani et al., 2021)．\n事後調整では，モデルの全体では規模が大きすぎるため，出力層の後に新しいニューラルネットを付加したり，最後の数層のみを追加で教師あり学習をしたりする方法が一般的である．または，LoRA (Low-Rank Adaptation) (E. J. Hu et al., 2021) では，トランスフォーマーの各層に新たな層を挿入し，これを学習する．\nこれは，事後調整に有効な内的次元は実際には小さく (Aghajanyan et al., 2021)，これに有効にアクセスし，効率的な事後調整を行うことが出来るという．さらに (Zhou et al., 2023) によると，事後調整に必要なラベル付きデータは，量よりも質が重要であり，LLaMA 5.1 に対しても多くて 1000 データで十分であるようである．\n事後調整には，他にも，ChatGPT のようなサービスを展開するために必要なユーザー体験の改善を目的としたものも含まれる．これは アラインメント とも呼ばれ，強化学習が用いられることが多い．実際，GPT-4 では 人間のフィードバックによる強化学習 (RLHF: Reinforcement Learning through Human Feedback) (Christiano et al., 2017) が用いられている (OpenAI, 2023b, p. 2)．\nこれについては第 3.5 節で改めて論じる．．\n\n\n3.3 プロンプトエンジニアリング\n基盤モデルには世界と人間に対する膨大な知識が含まれているが，使い方によって大きく性能が変わる．正しい条件付けを行うことで，内部に存在する知識をうまく引き出すことができる．これを大規模言語モデルでは prompt engineering (Liu et al., 2023) という．プロンプトの送り方によって性能がどう変わるかを調べる新たな分野である．\nその結果，プロンプト内で新たなタスクを定義するだけで，またはいくつか例を与えるだけで，これが解けてしまうこともわかっており，これを zero-shot または few-shot learning という．\n\n\n3.4 RAG\nLLM は世界に関する正確な知識を持っており，知識ベースとしての利用も期待されている (Petroni et al., 2019)．7 しかし，知識を正確に，そして信頼出来る形で引き出すことが難しいのであった．\n特に，出典を示すことや，最新の知識のアップデートなどが難題として待っている．\nそこで，LLM に（自由に外部情報を探索できるという意味で）ノンパラメトリックな知識ベースを接続することで解決するのが RAG (Retrieval-Augmented Generation) モデル (P. Lewis et al., 2020) である．\nDPR (Dense Passage Retriever) (Karpukhin et al., 2020) は文書を密に符号化する手法を開発し，これを用いて文書検索をすることで Q&A タスクを効率的に解く手法を提案した．このような文書の符号化器は 検索器 (retriever) と呼ばれる．\nRAG (P. Lewis et al., 2020) はこの検索器を BART (M. Lewis et al., 2020) に接続した．\nREALM (Retrieval-Augmented Language Model) (Guu et al., 2020) も同時期に提案されている．\nMeta での研究 (Yasunaga et al., 2023) はこの検索器を Text-to-Image トランスフォーマー である CM3 (Aghajanyan et al., 2022) と結合することで，初めて言語と画像の両方を扱える RAG モデル RA-CM3 (retrieval-augmented CM3) を構成した．\nWebGPT (Nakano et al., 2022) は，RAG や REAML が文書検索をしているところを，Web 検索を実行できるようにした GPT-3 (Brown et al., 2020) の事後調整である．\n\n\n3.5 アラインメント\nLLM などの機械学習モデルを訓練する際の目的関数は，そのままでは人間社会が要請するものとずれがあることが多い．これを修正するような試みを アラインメント (alignment) という．\n\nFor example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. (Ouyang et al., 2022)\n\n加えて，人間の選好は，0-1 損失関数で表現できるものではないことが多い．そこで，強化学習を用いることが考えられた．しかし，一々人間がフィードバックを与える方法はスケーラビリティに深刻な問題があるため，「人間の選好」をモデリングするニューラルネットワークを 代理モデル (surrogate model) として構築することも考える．\nその代表的な手法が 人間のフィードバックによる強化学習 (RLHF: Reinforcement Learning through Human Feedback) (Christiano et al., 2017) である．\nInstructGPT (Ouyang et al., 2022) は OpenAI API を通じて寄せられたフィードバックを用いて，PPO (Proximal Policy Optimization) アルゴリズム (Schulman et al., 2017) による強化学習により事後調整をしたものである．\n近接ポリシー最適化 (PPO: Proximal Policy Optimization) アルゴリズム (Schulman et al., 2017) は 信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) (Schulman et al., 2015) の洗練化として提案されたもので，現在の RLHF においても最も広く使われている手法である (Zheng et al., 2023)．\nInstructGPT が ChatGPT の前身となっている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#sec-multimodal-transformer",
    "href": "posts/2024/Kernels/Deep2.html#sec-multimodal-transformer",
    "title": "数学者のための深層学習２",
    "section": "4 多相トランスフォーマー",
    "text": "4 多相トランスフォーマー\nトランスフォーマーは自然言語処理の文脈で開発されたが，画像や動画，音声 (Radford et al., 2023a)，さらにはプログラミング言語 (Chen et al., 2021) にも適用されている．\n動画はまだしも画像には，直感的には時系列構造がないように思えるが，トランスフォーマーはもはや汎用のニューラルネットワークアーキテクチャとして使用できることが解りつつある．\nそれぞれの応用分野で モデルの構造は殆ど差異がなく，トークン化の手法などに差異があるのみのように見受けられる．8\n\n4.1 画像認識トランスフォーマー (ViT)\n画像の分類問題を解くためのエンコーダ・トランスフォーマーは ViT (Vision Transformer) (Dosovitskiy et al., 2021) と呼ばれており，ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) では未だ ResNet 系のモデルが優勢であった 2021 年に，これを超える性能を示した．\n実はモデルは殆どトランスフォーマーそのままであり，肝要であったのは画像をトークン化である．ピクセルをそのまま用いるのではなく，ある程度大きなピクセルの集合である パッチ (patch) を用いることで，計算量を下げる．(Dosovitskiy et al., 2021) では \\(16\\times16\\) サイズなどが採用された．\n一方で，画像を恣意的に系列化しているため，幾何学的な構造は１から学ぶ必要があり，最初からモデルに組み込まれている CNN よりは一般に多くの訓練データを必要とする．だが，これにより帰納バイアスが弱いということでもある．9\nViT はその後，動画も扱える ViViT (Arnab et al., 2021), あらゆるアスペクト比に対応する NaViT (Native Resolution ViT) (Dehghani et al., 2023) などの拡張が続いた．\nDeepMind の Perceiver (Jaegle et al., 2021) は画像，動画，音声のいずれのメディアの分類問題にも対応可能な，非対称な注意機構を持つトランスフォーマーである．\n\n\n4.2 画像生成トランスフォーマー\n一方でデコーダートランスフォーマーを用いて，画像の生成モデリングを行った最初の例は ImageGPT (Chen et al., 2020) である．\nなお，自己回帰的な生成モデルを通じて画像の生成を試みることは，CNN (van den Oord, Kalchbrenner, Vinyals, et al., 2016) や RNN (van den Oord, Kalchbrenner, & Kavukcuoglu, 2016) でも行われていた．\nこの際に判明したことには，画像の分類タスクでは連続表現が役に立っても，生成タスクでは高い解像度を持った画像の生成が難しく，離散表現が有効であることが知られている．しかしこれではデータ量が増えてしまうため，画像の ベクトル量子化 が行われることが多い．\nそこで，ImageGPT (Chen et al., 2020) でも \\(K\\)-平均法によるクラスタリングが行われており，さらに VQ-VAE を用いたデータ圧縮も行われている．\nImageGPT (Chen et al., 2020) では最終的に各ピクセルを one-hot 表現にまで落とし込み，これを GPT-2 モデル (Radford et al., 2019) につなげている．\nMUSE (Chang et al., 2023) も，トランスフォーマーを用いた画像生成モデルの例である．\n\n4.2.1 拡散モデルとの邂逅\n潜在拡散モデル で U-Net (Ronneberger et al., 2015) を用いていたところをトランスフォーマーに置換した 拡散トランスフォーマー (DiT: Diffusion Transformer) (Peebles & Xie, 2023) が発表された．\nその後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) (Ma et al., 2024) が発表された．\n\n\n\n4.3 Text-to-Image トランスフォーマー\nこの分野は (Reed et al., 2016) 以来，初めは GAN によるアプローチが試みられていた．\nGPT-3 (Brown et al., 2020) と ImageGPT (Chen et al., 2020) とは殆ど同じモデルを用いている．これらを組み合わせたデコーダー型のトランスフォーマーが DALL-E (Ramesh et al., 2021) である．10\nトークン化して仕舞えば，言語も画像も等価に扱えるというのである．Google の Parti (J. Yu et al., 2022) も同様のアプローチである．\nMeta の CM3 (Aghajanyan et al., 2022) と CM3leon (L. Yu et al., 2023) は画像と言語を両方含んだ HTML ドキュメントから学習している．\nGoogle DeepMind の Flamingo (Alayrac et al., 2022) は画像から言語を生成する．\n\n\n4.4 Image-to-Text トランスフォーマー\nOpenAI の CLIP (Contrastive Language-Image Pre-training) (Radford et al., 2021) は画像の表現学習をする視覚モデルである．これは DALL-E (Ramesh et al., 2021) と同時に開発された重要な構成要素である．\n一方で DALL-E2 (Ramesh et al., 2022) では，CLIP により画像を潜在空間にエンコードし，拡散モデルによってデコードする．\nDALL-E3 (OpenAI, 2023a) もその改良である．\n\n\n4.5 動画生成トランスフォーマー\n動画を画像の連続と見てトランスフォーマーを応用するアプローチは Latte (Latent Diffusion Transformer) (Rakhimov et al., 2020) に始まる．\nVideoGPT (Yan et al., 2021) では動画を 3D の CNN でデータ圧縮，VQ-VAE で量子化して離散的な潜在表現を得た後，GPT と殆ど似たトランスフォーマーに通して学習する．\nWayve の GAIA-1 (Generative AI for Autonomy) (A. Hu et al., 2023) も同様の手法で動画を生成しているが，その動画を用いて自動運転の強化学習に応用する点が画期的である．\nOpenAI は 2/15/2024 に Sora (Brooks et al., 2024) を発表した．これも 潜在拡散モデル (Rombach et al., 2022) 同様，自己符号化器による動画の潜在表現を得た上でパッチに分割し，この上で拡散トランスフォーマー (Peebles & Xie, 2023) の学習を行う．\n\n\n4.6 世界モデルとしてのトランスフォーマー\nトランスフォーマーを世界モデルとして用いて，シミュレーションを行い動画を生成し，これをモデルベースの強化学習 (R. S. Sutton & Barto, 2018) の材料とすることが広く提案されている．これは learning in imagination (Racanière et al., 2017) と呼ばれる．11\nIRIS (Imagination with auto-Regression over an Inner Speech) (Micheli et al., 2023) はこれに初めてトランスフォーマーを用いた世界モデルから動画生成をした．\nGAIA-1 (Generative AI for Autonomy) (A. Hu et al., 2023) は自動運転に特化した世界モデルを，トランスフォーマーを用いて構築している．\n他にも，動画生成を強化学習に応用する例としては，OpenAI による VPT (Video Pre-Training) (Baker et al., 2022) がある．\n\n\n4.7 音声生成トランスフォーマー\nOpenAI の Jukebox (Dhariwal et al., 2020) は，VQ-VAE を用いて音声データを圧縮・量子化し，トランスフォーマーに通したものである．\nこのトランスフォーマーは Sparse Transformer (Child et al., 2019) という，注意機構の計算効率を改良したモデルを用いている．\n\n\n4.8 Text-to-Speech トランスフォーマー\nMicrosoft Research の VALL-E (Wang et al., 2023) は，音声データをベクトル量子化によって言語データと全く同等に扱うことで，トランスフォーマーを用いて音声生成を行っている．\n\n\n4.9 Speach to Text トランスフォーマー\nOpenAI の Whisper (Radford et al., 2023b) は encoder-decoder 型のトランスフォーマーを用いている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#近年の動向",
    "href": "posts/2024/Kernels/Deep2.html#近年の動向",
    "title": "数学者のための深層学習２",
    "section": "5 近年の動向",
    "text": "5 近年の動向\n\n5.1 LLaMA の一般公開とその影響\nMeta AI が 7/18/2023 に LLM LLaMA (Touvron et al., 2023) を公開した．そして API を通じて利用する形ではなく，そのモデルのウェイトが公開されたため，Stanford 大学の Alpaca など，モデルの改良と研究が促進されている．\n特に事後調整のための公開データセットの整備が進んでおり，Alpaca では Self-Instruct (Wang et al., 2023) による効率的な alignment 技術が採用されている．\n産業界でも影響は大きい．ELYZA は 12/27/2023 に日本語に特化した LLM である ELYZA-japanese-Llama-2-13b を公開している．Stockmark も 10/27/2023 に Stockmark-13b を公開している．\nいずれも，開発費と開発時間が大幅に圧縮されたという．12\nIBM は 9/12/2023 に LLM Granite を発表している．加えて，プラットフォーム watsonx も提供しており，その上で RAG など独自の事後調整を可能にしている．\nIBM と Meta の２社が発起人となり，12/5/2023 に AI Alliance が発足し，オープンイノベーションを推進している．\nStable Diffusion (Rombach et al., 2022) もソースコードとウェイトが 一般公開 されている．\n\n\n5.2 LLM の経済的影響\n(Tamkin et al., 2021) は早い段階での OpenAI と Stanford 大学 HAI (Human-centered AI) との対談録である．\nOpenAI はコード生成能力の経済的な影響を重要なアジェンダとしている (Manning et al., 2022)．\nOpen AI の Codex (Chen et al., 2021) はプログラム言語を扱うトランスフォーマーであり，GitHub Copilot の元となっている．これが社会に与える影響も，新たな評価フレームワークと共に提案されている (Khlaaf et al., 2022)．\nLLM の労働市場へのインパクトも推定している (Eloundou et al., 2023)．これによると，アメリカの労働者の 80% が，LLM の導入により少なくとも仕事の 10% に影響が生じるとしている．さらに全体の 20% は仕事の半分以上が影響を受けるとしている．\n\n\n5.3 世界モデルとしての基盤モデル\n\n5.3.1 社会行動シミュレーターとしての LLM\n社会的なシミュレーションを LLM 内で行うことで，社会科学やビジネスの場面での意思決定を支援することが期待されている．\nLLM は人間の心の理論を理解し，その心情・意図を（ある程度）シミュレートすることが出来るようである (Andreas, 2022)．\nLLM でのシミュレーションを通じて，社会科学的な知識を引き出そうとする試みもある (Leng & Yuan, 2023)．\n\n\n\n5.4 LLM と経済安全保障\n\n5.4.1 幻覚の防止\nLLM が事実と異なる物語を生成することを 幻覚 (hallucination) と呼び，一部の応用では問題になることがある．\nこれを解決するにあたって，等角推測 (conformal prediction) と組み合わせ，出力の不確実性を評価することで幻覚を防止する手法が提案されている (Mohri & Hashimoto, 2024)．\n一般に意思決定の場面において AI を活用するには，不確実性の定量化が必要不可欠である．\nGPT-3 を Bayesian にし，自身の確証度合いを言表するように事後調整する研究が OpenAI で行われている (Lin et al., 2022)．\n\n\n5.4.2 ウォーターマーク\nウォーターマークを開発することで，LLM から出力された文章であることを高確率で検出できるようにする方法が，統計的仮説検定の技術を応用して提案されている (Kuditipudi et al., 2023)．\n\n\n5.4.3 偽情報対策\n生成 AI は，一国の政府が特定のプロパガンダを流布するための効果的な手段として選ばれることになる．その際の考え得る使用例と，それに対する対策が考えられてる (Goldstein et al., 2023)．\n\n\n5.4.4 開発規制\n(Anderljung et al., 2023) は先端的な AI を Frontier AI と呼び，これの開発過程におけるあるべき規制を模索している．監督当局に執行権を付与することやフロンティアAIモデルのライセンス制度などが議論されている．\n(Shoker et al., 2023) は LLM と国家安全保障との関係を議論している．信頼構築措置 (CBMs: Confidence-Building Measures) とは，国家間の敵意を減少させることで，衝突のリスクを減らす措置の全般をいう．元々は冷戦時代に提案された概念であるが，これを LLM 開発に適用することが具体的に提案されている．\n\n\n5.4.5 生物学的脅威\nLLM の登場により個人がエンパワーメントを受けており，生物学的脅威を作る障壁が低下していることは間違いない．\n(Patwardhan et al., 2024) では，生物学的リスクに焦点を当てて，AI による安全リスク評価の手法と事前警鐘システムを模索している．この研究では，LLM によりリスクが増加するという統計的に有意義な証拠は得られていないが，この方面の研究の草分けとなっている．\n\n\n\n5.5 アラインメント問題\nDALL-E2 では訓練前の緩和策も取られている (Nichol, 2022)．\n\n5.5.1 プログラムの支援\n遺伝的プログラムの改良の過程を模倣できる (Lehman et al., 2024) として，ソフトウェア開発やロボット開発分野での，プログラムの漸次的改良への応用が考えられている．\n困難なタスクに対して AI がアシストするという研究もある (Saunders et al., 2022)．これは最終的に，AI のアラインメントにおいても重要な技術になるとしている．\n\n\n5.5.2 超アラインメント問題\nこれは，OpenAI のアラインメント研究が次の３本の柱であることが背景にある (Leike et al., 2022)\n\n人間のフィードバックによる AI の強化学習\nAI の支援を通じて人間のフィードバックを正確にする\nAI を通じてアラインメントの研究を促進する\n\nの３つである．\n第３の柱として，GPT-4 (OpenAI, 2023b) によるシミュレーションを通じて，特定のニューロンがどのような出力に対応しているかを解明する手法を提案している (Leike et al., 2023)．これにより，人間が直接調べる行為が自動化され，アラインメントの研究が効率化され，スケーラブルな手法になるということである．\nさらに，将来的なアラインメントは RLHF では出来なくなっていく．人智を超えた超知能 (superintelligence) をアラインメントすることを，超アラインメントと呼び，OpenAI は 2023 年の暮れに 超アラインメントチーム を創設し，人間が「弱い監督者」となってしまった状況でもどのように超アラインメントを実行すれば良いかを研究するとしている．\n\n\n5.5.3 AI エージェントへの道\nOpenAI は能動的 AI システム (Agentic AI system) の構築に向けて，安全な運用と責任のある管理を目指す白書 (Shavit et al., 2023) を発表した．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#footnotes",
    "href": "posts/2024/Kernels/Deep2.html#footnotes",
    "title": "数学者のための深層学習２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n層の数などの表現は (Hashimoto, 2024) から．↩︎\n(Hashimoto, 2024) で聞きました．↩︎\n(Hashimoto, 2024) で聞きました．↩︎\nもちろん結合することも考えられているが，加算を行うことが現状の多数派であるようである (Hashimoto, 2024)．↩︎\n勾配の爆発に対しては gradient clipping などの対症療法が用いられる．↩︎\n後述のアラインメントも事後調整の一つであるが，これと区別して，教師ありの事後調整 (supervised fine-tuning) とも呼ばれる．↩︎\nパラメトリックな知識ベースとしての利用については (Raffel et al., 2020)，(Roberts et al., 2020) など．一方で (Marcus, 2020) などは，hallucination などの欠点を補う形で，古典的な知識ベースと連結したハイブリット型での使用を提案している．↩︎\nモデルの比較は (Raffel et al., 2020) などが行っている．↩︎\nトークン化に小規模な CNN を用いてデータ圧縮を行うこともある．↩︎\nすなわち，DALL-E は GPT-3 のマルチモーダルな実装である (Tamkin et al., 2021, p. 4)．↩︎\n(Ha & Schmidhuber, 2018) は RNN により世界モデルを構築している．(Kaiser et al., 2020) は動画から Atari を学習している．(Hafner et al., 2021) はさらに性能が良い．↩︎\n日経新聞 (2/19/2024)↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep7.html",
    "href": "posts/2024/Kernels/Deep7.html",
    "title": "数学者のための深層学習７",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n(Gao et al., 2020)\nエネルギーベースのモデル (EBM: Energy-based Model) とは， \\[\np(z)\\,\\propto\\,e^{-H(z)}\n\\] の形で与えられるノンパラメトリックモデルで，訓練データ \\(\\{(x_i,y_i)\\}_{i=1}^n\\) に対して最も低いエネルギーを割り当てるエネルギー関数 \\(H\\) を探す (energy minimization) ことで確率分布 \\(p(x)\\) を推定する手法である (LeCun et al., 2007)．\nこの形の分布族を 正準分布 または Gibbs 分布 (Koller & Friedman, 2009, p. 108), (Friedli & Velenik, 2017, p. 25)，または Boltzmann 分布 (Kim & Bengio, 2016), (Mézard & Montanari, 2009, p. 23), (Chewi, 2024) ともいう．1\nこの推定を，うまく損失関数を設定することで，最適化手法によって解くことが (LeCun et al., 2007) では考えられている．データの分布との KL 乖離度を，勾配降下法によって最小化することによって学習することも多いが，この場合 \\(p\\) からのサンプリングを必要とするため，GAN にヒントを得た敵対的な学習も考えられている (Kim & Bengio, 2016), (Gao et al., 2020)．\n回帰や分類などの古典的なタスクだけでなく，ほとんどの確率的モデルもこの手続きから理解することができ，この場合は EBM が非確率的な／最適化ベースの推論手法を提供するフレームワークとして働くことになる (LeCun et al., 2007, p. 192)．\nまた EBM は，入力 \\(x\\) と出力 \\(y\\) の整合性 \\(k(x,y)\\) を，エネルギーの言葉で与えているモデルであると見ることもできる．2"
  },
  {
    "objectID": "posts/2024/Kernels/Deep7.html#footnotes",
    "href": "posts/2024/Kernels/Deep7.html#footnotes",
    "title": "数学者のための深層学習７",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nということもあるようであるが，物理の用語では \\(e^{H(z)}\\) を Boltzmann 因子と呼ぶのみであるようである (田崎晴明, 2008, p. 107)．(Liu, 2004, p. 7) ではどちらも掲載している．正準集団は，NVT 一定集団ともいう．↩︎\n分野によっては，エネルギー関数 \\(H\\) を，contrast function, value functions, NLL (Negative Log-Likelihood) functions などとも呼ぶとしている (LeCun et al., 2007, p. 193)．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html",
    "href": "posts/2024/Kernels/Deep4.html",
    "title": "数学者のための深層学習４",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#確率的勾配降下法による変分ベイズ-sgvb",
    "href": "posts/2024/Kernels/Deep4.html#確率的勾配降下法による変分ベイズ-sgvb",
    "title": "数学者のための深層学習４",
    "section": "1 確率的勾配降下法による変分ベイズ (SGVB)",
    "text": "1 確率的勾配降下法による変分ベイズ (SGVB)\n変分自己符号化器 (Variational Auto-encoder) の説明に入る前に，変分ベイズ法において勾配を用いた最適化を実行するための汎用手法である SGVB (Stochastic Gradient Variational Bayes) について説明する．\nVAE は元々この SGVB という要素技術とセットで提案された (Kingma & Welling, 2014)．\n変分近似をする分布族 \\(\\{q_\\phi\\}\\) としてニューラルネットを用いた場合が，VAE であり，広く SGVB は，一般の連続な潜在変数を持った（有向）グラフィカルモデルに適用できる．\n\n1.1 SGVB のアイデア：勾配の Monte Carlo 推定\nGAN 同様，生成モデリングは，潜在空間 \\(Z\\) で条件付けた際の分布 \\(p(x|z)\\) をモデリングすることに等しい．GAN は \\(p(x|z)\\) を明示的に評価することを回避することで複雑な生成モデリングを達成していた．\n一方で，（周辺）尤度の評価を完全に回避せずとも，変分 Bayes 法 によるアプローチが可能である．\\(p(x|z)\\) に分布族 \\(q_\\phi(x|z)\\) を導入し，真の分布 \\(p\\) との KL-距離を最小にする \\(\\phi\\in\\Phi\\) を選ぶのである．\n変分 Bayes ではこれを解析的に実行する必要があった．そのため，分布族 \\(\\{q_\\phi\\}\\) を指数分布族や共役分布族に限るか，平均場近似を用いるか，などの強い仮定が必要で，これが複雑な生成モデリングを妨げていた．1\nそこで，一般の分布族 \\(\\{q_\\phi\\}\\) に対して勾配情報を用いた最適化が実施できるように，変分下界 \\(F(p_\\theta,q_\\phi)\\) 対する Monte Carlo 推定量を開発するのである．これが SGVB 推定量である．\n\n\n1.2 変分下界の復習\nデータ \\(X\\) の生成過程に，モデル \\(p_\\theta(z)p_\\theta(x|z)\\) を考える．これがニューラルネットワークによるモデルであるとすると，周辺尤度 \\[\np_\\theta(x)=\\int_\\mathcal{Z}p_\\theta(z)p_\\theta(x|z)\\,dz\n\\] や事後分布 \\(p_\\theta(z|x)\\) の評価は容易でない．\nそこで，\\(p_\\theta(z|x)\\) に対して，認識モデル \\(\\{q_\\phi(z|x)\\}_{\\phi\\in\\Phi}\\) を導入する．VAE 2 では，これもニューラルネットワークとし，\\((\\theta,\\phi)\\in\\Theta\\times\\Phi\\) を同時に SGD により学習することを考える．\n潜在変数 \\(Z\\) を情報源と見て，\\(q_\\theta(z|x)\\) を 符号化器 (encoder) と呼び，\\(p_\\theta(x|z)\\) を 復号器 (decoder) とも呼ぶ．\nこのとき，対数周辺尤度の変分下界は次のように表せるのであった：2 \\[\n\\begin{align*}\n    \\log p_\\theta(x)&=\\log\\int_\\mathcal{Z}p_\\theta(x,z)\\,dz\\\\\n    &=\\log\\int_\\mathcal{Z}q_\\phi(z)\\frac{p_\\theta(x,z)}{q_\\phi(z)}\\,dz\\\\\n    &\\ge\\int_\\mathcal{Z}q_\\phi(z)\\log\\frac{q_\\theta(x|z)p_\\theta(z)}{q_\\phi(z)}\\,dz\\\\\n    &=-\\mathop{\\mathrm{KL}}(q_\\phi,p_\\theta)+\\int_\\mathcal{Z}q_\\phi(z)\\log p_\\theta(x|z)\\,dz\\\\\n    &=:F(\\theta,\\phi;x)\n\\end{align*}\n\\]\nこの \\(F\\) を \\(\\theta,\\phi\\) に関して逐次的に最大化するのが変分 Bayes である．これを実行するために \\(q_\\phi\\) に平均場近似などをするのが旧来手法であるが，これ以上の近似をせずとも，\\(F\\) の勾配の推定量を用いて，\\(p_\\theta,q_\\phi\\) を同時に学習することが出来るというのである．\n\n\n1.3 SGVB 推定量\n例えば \\(F\\) を \\(\\phi\\) に関して勾配情報から最大化する際に，勾配 \\(D_\\phi F\\) の Monte Carlo 推定量が利用できる．しかし，単に \\(q_\\phi(z|x)\\) からのサンプルを用いた crude Monte Carlo では，この推定量の分散は非常に大きい (Paisley et al., 2012)．\nこれを 重点サンプリングの考え方により解決した のが \\(D_\\phi F,D_\\theta F\\) に対する SGVB 推定量である．3 (Kingma & Welling, 2014) では reparameterization trick と呼んでいる．\nある分布 \\(P\\in\\mathcal{P}(E)\\) と可微分同相 \\(g_\\phi:E\\times\\mathcal{X}\\to\\mathcal{Z}\\) であって \\[\ng_\\phi(\\epsilon,x)\\sim q_\\phi(z,x)\\quad(\\epsilon\\sim P)\n\\] を満たすものを見つけることができて，この \\(P\\) を提案分布とする重点サンプリング推定量 \\[\n\\begin{align*}\n    \\operatorname{E}_{q_\\phi}[f(Z)]&=\\operatorname{E}_{P}[f(g_\\phi(\\epsilon,x))]\\\\\n    &\\simeq\\frac{1}{M}\\sum_{i=1}^Mf(g_\\phi(\\epsilon^i,x))\n\\end{align*}\n\\] により，Monte Carlo 推定量の分散を減らすことができる．\\(f=F\\) と取ることで SGVB 推定量を得る．\nさらに，\\(\\mathcal{Z}\\) 上のモデル \\(q_\\phi(z),p_\\theta(z)\\) とが \\(d\\)-次元の正規分布であった場合， \\[\n-\\mathop{\\mathrm{KL}}(q_\\phi,p_\\theta)=\\frac{1}{2}\\sum_{j=1}^d\\biggr(1+\\log(\\sigma_j^2)-\\mu_j^2-\\sigma_j^2\\biggl)\n\\] と解析的に解けるので，結局 Monte Carlo 近似が必要なのは，再構成誤差を表す \\[\n\\int_\\mathcal{Z}q_\\phi(z)\\log p_\\theta(x|z)\\,dz\n\\] の部分だけである．\nこのような理由で，\\(q_\\phi(z),p_\\theta(z)\\) は典型的には正規分布としてモデリングされる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#sec-VAE",
    "href": "posts/2024/Kernels/Deep4.html#sec-VAE",
    "title": "数学者のための深層学習４",
    "section": "2 VAE (Kingma & Welling, 2014)",
    "text": "2 VAE (Kingma & Welling, 2014)\n\n\n\nSamples from a VQ-VAE Taken from Figure 6 (Razavi et al., 2019, p. 8)\n\n\n\n2.1 導入\nVariational Auto-encoder (Kingma & Welling, 2014), (Rezende et al., 2014) も GAN と同じく，深層生成モデル \\(p_\\theta\\) にもう１つの深層ニューラルネットワーク \\(q_\\phi\\) を対置するが，このニューラルネット \\(q_\\phi\\) は GAN のように判別をするのではなく，近似推論によってデータ生成源を再構成しようとする 認識モデル (recognition model) である．\\(q_\\phi\\) はエンコーダーとも呼ばれる．\nこの深層生成モデル \\(p_\\theta\\) と近似推論器 \\(q_\\phi\\) とを，同時に確率勾配降下法によって学習する (Kingma & Welling, 2019)．\nVAE のエンコーダー \\(q_\\phi\\) は動画データの圧縮表現の学習 (Brooks et al., 2024) など，その他の生成モデルの構成要素としても用いられる．\n\n\n2.2 VQ-VAE による画像の量子化\nVAE は画像データの生成にも応用されており，その際のデータ圧縮の技術（連続データである画像を離散化するので，ベクトル量子化 と呼ばれる）だけが取り出され，DALL-E (Ramesh et al., 2021) など，モデルの構成要素としても利用されている．\n\n2.2.1 VQ-VAE\nVQ-VAE (van den Oord et al., 2017), (Razavi et al., 2019) は，自己符号化器の中間表現に ベクトル量子化 を施し，JPEG (Wallace, 1992) のような画像データの圧縮を行うことで，不要な情報のモデリングを回避している．\n実際，元データの 30 分の 1 以下のサイズで学習を行い，最終的にデコーダーを用いて殆ど歪みなく再構成できるという．\nGAN は元データのうち，尤度が低い部分が無視され，サンプルの多様性が失われがちであったが，VQ-VAE はこの問題を解決している．また，GAN にはないようなモデル評価の指標が複数提案されている．\n\n\n2.2.2 連続緩和\n質的変数のサンプリングにおいて，Gumbel 分布を提案分布として重点サンプリングを行うことが有効である．この reparametrization trick を Gumbel Max Trick (Jang et al., 2017) という．\nConcrete (Continuous Relaxatino of Discrete) (Maddison et al., 2017) はこれを連続分布に拡張し，reparametrization trick に応用したものである．\nこれらの手法は VAE や DALL-E (Ramesh et al., 2021) の訓練にも応用されている．\n\n\n2.2.3 Codebook collapse\nVQ-VAE は符号帳 (codebook) に冗長性が生まれ，符号帳の一部が使われなくなるという問題がある．これを解決するためには，符号帳への対応を softmax 関数を用いて軟化することが dVAE (Ramesh et al., 2021) として考えられている．\nしかしこの dVAE も codebook collapse から完全に解放されるわけではない．これは softmax 関数の性質によると考えられ，実際，Dirichlet 事前分布を導入した Bayes モデルによって緩和される (Baykal et al., 2023)．\nこのような技術を エビデンス付き深層学習 (EDL: Evidential Deep Learning) (Sensoy et al., 2018), (Amini et al., 2020) という．4\n\n\n\n2.3 Wasserstein VAE (Tolstikhin et al., 2018)\nVAE は GAN よりも画像生成時の解像度が劣るという問題がある．\nこれを，目的関数を Wasserstein 距離に基づいて再定式化することで解決できるというのが Wasserstein Auto-encoder (Tolstikhin et al., 2018) である．\n\n\n2.4 beta-VAE (Higgins et al., 2017)\nVAE を画像の因子表現学習に用いる際に，解釈可能性を担保する教師なし学習手法である．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#footnotes",
    "href": "posts/2024/Kernels/Deep4.html#footnotes",
    "title": "数学者のための深層学習４",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nなお，平均場近似も極めて困難な解析的計算を必要とする (Kingma & Welling, 2014)．↩︎\n前稿 も参照．↩︎\n最適化の文脈において，目的関数の評価が困難であるとき，Monte Carlo 推定量でこれを代替する際，重点サンプリングを用いると良いことは従来提案されている (Geyer, 1996)．(Robert & Casella, 2004, p. 203) も参照．↩︎\nPresent Square 記事，GIGAZINE 記事 もある．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Optimization.html",
    "href": "posts/2024/Kernels/Optimization.html",
    "title": "最適化手法",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n非斉次 Markov 過程とも見れる？ (Robert & Casella, 2004, p. 162)．"
  },
  {
    "objectID": "posts/2024/Kernels/Optimization.html#確率的最適化の概観",
    "href": "posts/2024/Kernels/Optimization.html#確率的最適化の概観",
    "title": "最適化手法",
    "section": "1 確率的最適化の概観",
    "text": "1 確率的最適化の概観\n\n1.1 導入\nニューラルネットワークの訓練に関して，目的関数の勾配を上るようにパラメータを更新する手法が取られている．\n一度に全てのデータを使って勾配を計算する場合を バッチ学習 (batch method) と呼び，これが勾配降下法または最急降下法にあたる．\n一方で，データを分割して逐次的に勾配を計算し最適化を実施する手法を オンライン学習 (online learning) といい，これを 確率的勾配降下法 (stochastic gradient descent, SGD) または逐次的勾配降下法 (sequential gradient descent) と呼ぶ．1\n\n\n1.2 確率的最適化の歴史\n確率的最適化は，はじめは統計学の文脈で (Robbins & Monro, 1951) によってオンラインの最尤推定を題材に考察された．\nこれを一般化する形で (Kiefer & Wolfowitz, 1952) は 確率的勾配降下法 (SGD) を提案した．\nSGD を拡張し，適応的に学習率を調整する手法としては，AdaGrad (Duchi et al., 2011) や RMSprop (Tieleman & Hinton, 2012)，そしてこれら２つの長所を組み合わせた Adam (Kingma & Ba, 2017) が提案された．2\n\n\n1.3 (Robbins & Monro, 1951)\n目的関数が \\[\nh(x)=\\operatorname{E}[H(x,Z)]\n\\] の形で与えられるとする．3\n\\(h(x)=\\beta\\) の解 \\(x=\\theta\\) を求める問題を考える．のちに \\(\\max_{x\\in\\mathcal{X}}h(x)\\) を求める問題に拡張したのが (Kiefer & Wolfowitz, 1952) である．\n\n\n\n\n\n\n定理（Robbins-Monro アルゴリズム）4\n\n\n\n\\(Z_j\\overset{\\text{iid}}{\\sim}p(z|X_j)\\) とし， \\[\nX_{j+1}=X_j+\\gamma_j\\biggr(\\beta-H(Z_j,X_j)\\biggl)\n\\] によって Markov 連鎖 \\(\\{X_j\\}\\) を定める．\nこのとき，\\(\\{\\gamma_n\\}\\subset\\mathbb{R}^+\\) が次の３条件を満たすならば，\\(X_j\\xrightarrow{\\;\\text{a.s.}}\\theta\\) が成り立つ．\n\n\\[\n\\|\\gamma\\|_1=\\infty,\\quad\\|\\gamma\\|_2&lt;\\infty\n\\]\n\\(\\{X_j\\}\\) は有界 \\(\\sup_{j\\in\\mathbb{N}}\\lvert X_j\\rvert&lt;\\infty\\) で \\[\n\\operatorname{E}[H(X_j,Z_j)|Z_j]=h(X_j)\n\\]\nある \\(\\theta\\in\\mathcal{X}\\) が存在して，任意の \\(\\delta\\in(0,1)\\) について \\[\n\\inf_{\\delta\\le\\lvert\\theta-x\\rvert\\le1/\\delta}(x-\\theta)(h(x)-\\beta)&gt;0\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n(Robbins & Monro, 1951, pp. 401–402) の結果を拡張した形になっている．"
  },
  {
    "objectID": "posts/2024/Kernels/Optimization.html#sgd-の振る舞い",
    "href": "posts/2024/Kernels/Optimization.html#sgd-の振る舞い",
    "title": "最適化手法",
    "section": "2 SGD の振る舞い",
    "text": "2 SGD の振る舞い\n\n2.1 導入\nニューラルネットワークの訓練において，SGD は特に良い性質を示しているが，その理由は未だ十分に解明されていない．\n例えば，正則化に寄与している（暗黙的正則化 implicit regularization）ということが明らかになりつつある．5\n(Smith & Le, 2018) によると，鋭い谷 (sharp minima) に捕まりにくく，広い谷 (flat minima) に入りやすいという性質が汎化性能に寄与しているという．(Imaizumi & Schmidt-Hieber, 2023) は理論的な説明を与えた．"
  },
  {
    "objectID": "posts/2024/Kernels/Optimization.html#footnotes",
    "href": "posts/2024/Kernels/Optimization.html#footnotes",
    "title": "最適化手法",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bishop, 2006, p. 240) 5.2.4節．↩︎\n(麻生英樹 et al., 2015, p. 145) など．↩︎\nこの 稿 も参照．↩︎\n(Bouleau & Lépingle, 1993)，(Robert & Casella, 2004, p. 202) 定理5.24．↩︎\n(Murphy, 2022, p. 455), (Chizat & Bach, 2020), (Moroshko et al., 2020) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/SVM.html",
    "href": "posts/2024/Kernels/SVM.html",
    "title": "数学者のための Support Vector Machine 概観",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/SVM.html#分離超平面",
    "href": "posts/2024/Kernels/SVM.html#分離超平面",
    "title": "数学者のための Support Vector Machine 概観",
    "section": "2.1 分離超平面",
    "text": "2.1 分離超平面\n\n定義 1 (線型分離可能) データ \\(\\mathcal{D}:=\\{(x_i,y_i)\\}_{i=1}^n\\) が線型分離可能とは，ある \\(w,b\\in\\mathbb{R}^p\\) が存在して， \\[\n\\frac{y_i((w|x_i)+b)}{\\|w\\|}\\ge0\\quad i\\in[n]\n\\] が成り立つことをいう．\n\n\n定義 2 (Canonical Hyperplane) データ \\(\\mathcal{D}:=\\{(x_i,y_i)\\}_{i=1}^n\\) が定める標準超平面とは，次を満たす組 \\((w,b)\\in H\\times\\mathbb{R}\\) をいう： \\[\n\\min_{1\\le i\\le n}\\lvert(w|x_i)+b\\rvert=1.\n\\] すなわち， \\(\\{x_i\\}_{i=1}^n\\) との距離の最小値が \\(\\frac{1}{\\|w\\|}\\) となる超平面をいう．\n\nこのとき，線型分離可能ならば，標準超平面 \\((w,b)\\in H\\times\\mathbb{R}\\) は \\[\ny_i\\biggr((x_i|w)+b\\biggl)\\ge1\n\\] を満たす．"
  },
  {
    "objectID": "posts/2024/Kernels/SVM.html#マージン",
    "href": "posts/2024/Kernels/SVM.html#マージン",
    "title": "数学者のための Support Vector Machine 概観",
    "section": "2.2 マージン",
    "text": "2.2 マージン\n\n定義 3 (Geometrical Margin) 超平面 \\((w,b)\\in H\\times\\mathbb{R}\\) について，\n\n点 \\((x,y)\\in H\\times\\{\\pm1\\}\\) との幾何的マージンとは，\\[\\rho_{(w,b)}(x,y):=\\frac{y((w|x)+b)}{\\|w\\|}\\]という．\nデータ \\(\\mathcal{D}:=\\{(x_i,y_i)\\}_{i=1}^n\\) との幾何的マージンとは，\\[\\rho_{(w,b)}:=\\min_{1\\le i\\le n}\\rho_{(w,b)}(x_i,y_i)\\]をいう．\n\nすなわち，超平面 \\(\\mathcal{S}\\) に対して \\(d(\\mathcal{S},\\mathcal{D})\\) の値に他ならない．"
  },
  {
    "objectID": "posts/2024/Kernels/SVM.html#最小マージン超平面",
    "href": "posts/2024/Kernels/SVM.html#最小マージン超平面",
    "title": "数学者のための Support Vector Machine 概観",
    "section": "2.3 最小マージン超平面",
    "text": "2.3 最小マージン超平面\nここで，幾何的マージン \\[\n\\rho_{(w,b)}=\\min_{1\\le i\\le n}\\frac{y_i((w|x_i)+b)}{\\|w\\|}\n\\] の最大化は，線型分離可能である場合は分母が一定であるから \\(\\|w\\|\\) の最小化に等価である．したがって，標準超平面に対象を絞り，その中で \\(\\|w\\|\\) を最小にするような \\(w\\) を発見し，それに合わせて \\(b\\) を定めれば良い．\nこうして，判別のためのSVMの主最適化問題は次のようになる：\n\n定義 4 (Primal Optimization Problem) \\[\n\\begin{align*}\n\\mathop{\\mathrm{minimize}}_{w\\in H,b\\in\\mathbb{R}}&\\quad\\tau(w):=\\frac{\\|w\\|^2}{2}\\\\\n\\mathop{\\mathrm{subject to}}&\\quad c(w):=y_i\\biggr((x_i|w)+b\\biggl)\\ge1\\quad i\\in[n]\n\\end{align*}\n\\] 次の関数を未定乗数 \\(\\{\\alpha_i\\}_{i=1}^n\\subset\\mathbb{R}_+\\) を持ったLagrangianという \\[\nL(w,b,\\alpha):=\\frac{\\|w\\|^2}{2}-\\sum_{i=1}^n\\alpha_i\\biggr(y_i((x_i|w)+b)-1\\biggl)\n\\]\n\nこれを特には双対理論を用いる．\n\n定理 1 (Kuhn-Tucker Saddle Point Condition2) \\(H=\\mathbb{R}^p\\) とする．\\((\\overline{x},\\overline{\\alpha})\\in H\\times\\mathbb{R}_+^n\\) が次を満たすならば，\\(\\overline{x}\\in H\\) は上の最適化問題の解である： \\[\nL(\\overline{x},\\alpha)\\le L(\\overline{x},\\overline{\\alpha})\\le L(x,\\overline{\\alpha})\\quad(x\\in H,\\alpha\\in\\mathbb{R}_+^n)\n\\]\n\n\n定理 2 (KKT for Differentiable Convex Problems3) \\(\\tau,c\\) が \\(x:=(w,b)\\) について可微分であるとする．このとき，ある \\(\\overline{\\alpha}\\in\\mathbb{R}_+^n\\) が存在して次が成り立つならば，\\(\\overline{x}\\in\\mathbb{R}^p\\) は解である： \\[\n\\partial_wL(\\overline{x},\\overline{\\alpha})=\\partial_xf(\\overline{x})+\\sum_{i=1}^n\\overline{\\alpha}_i\\partial_xc_i(\\overline{x})=0\n\\] \\[\n\\partial_{\\alpha_i}L(\\overline{x},\\overline{\\alpha})=c_i(\\overline{x})\\le0\n\\] \\[\n\\sum_{i=1}^n\\overline{\\alpha}_ic_i(\\overline{x})=0\n\\]\n\n最初の条件から \\[\nw=\\sum_{i=1}^n\\alpha_iy_ix_i\n\\] が要請される．加えて，\\(\\overline{\\alpha}_i\\ne0\\) が成り立つ \\(i\\in[n]\\) においてのみ，制約条件の等号が成立する： \\[\n\\alpha_i\\biggr(y_i((x_i|w)+b)-1\\biggl)=0\\quad i\\in[n].\n\\] このときの \\(\\alpha_i&gt;0\\) を満たすデータ点 \\(x_i\\in H\\)，すなわち最小のマージンを達成するベクトルをサポートベクトルという．他のベクトルは \\(\\alpha_i=0\\) が成り立つために，本質的には関与してこないとみなされる．"
  },
  {
    "objectID": "posts/2024/Kernels/SVM.html#footnotes",
    "href": "posts/2024/Kernels/SVM.html#footnotes",
    "title": "数学者のための Support Vector Machine 概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Schölkopf & Smola, 2002) 第7章などを参考．↩︎\n(Schölkopf & Smola, 2002) 定理6.21 p.166．↩︎\n(Schölkopf & Smola, 2002, pp. 定理6.26 p.170．)↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel1.html",
    "href": "posts/2024/Kernels/Kernel1.html",
    "title": "カーネル法１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel1.html#導入",
    "href": "posts/2024/Kernels/Kernel1.html#導入",
    "title": "カーネル法１",
    "section": "1 導入",
    "text": "1 導入\n\nカーネル法とニューラルネットワークの比較\n\n\n\n\n\n\n\n\nKernel\nNN\n\n\n\n\n潜在空間\n無限\n有限\n\n\n基底関数\n固定\n適応的\n\n\n\nカーネル法の強みは，ノンパラメトリックモデリングを行った場合にある．\n(Song et al., 2013)\n\n\n\n\n\n\nCME (Conditional Mean Embedding)1, (Song et al., 2009)\n\n\n\n確率核 \\(P:\\mathcal{X}\\to\\mathcal{Y}\\) の再生核 Hilbert 空間 \\((H_\\mathcal{X},k_\\mathcal{X}),(H_\\mathcal{Y},k_\\mathcal{Y})\\) に関する 条件付き平均埋め込み とは，\\(\\mathcal{X}\\) 上の \\(H_\\mathcal{Y}\\)-値確率変数 \\[\nP_*(x):=\\int_{\\mathcal{Y}}k_{\\mathcal{Y}}(y,-)P(x,dy)\\in\\mathcal{L}(\\mathcal{X};H_\\mathcal{Y})\n\\] をいう．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel1.html#footnotes",
    "href": "posts/2024/Kernels/Kernel1.html#footnotes",
    "title": "カーネル法１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Li et al., 2022, p. 5) Def.2, (Park & Muandet, 2020, p. 4) Def.3.1↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes for Self Study",
    "section": "",
    "text": "サーベイ | レビュー | カテゴリ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSimulation\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC は従来法を超えるのか？どのような場面で使えるのか？この新たな手法を正しく受け止めるために，現状の MCMC への理解をまとめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n\n\n\n\nReview\n\n\n\n[@Duane+1987] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n\n\n\n\nReview\n\n\n\n[@Tartero-Krauth2023] は \n\n\n\n\n\n4/18/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n\n\n\n\nReview\n\n\n\n[@Metropolis+1953] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションにより計算を実行する手段である． \n\n\n\n\n\n4/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための統計力学２\n\n\n小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の数学的実体を速習することを目指す．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための統計力学１\n\n\nIsing 模型\n\n\n\nNature\n\n\nDeep\n\n\n\n数学者のために，統計力学の場面設定を数学的に理解することを試みる．統計力学の最も代表的なモデルである Ising 模型の数理も概観する．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションにより計算を実行する手段である． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n\n\n\n\nReview\n\n\n\n[@Peters-deWith2012] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\n\n\n\nReview\n\n\n\n[@Butkovsky-Veretennikov2013] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n\n\n\n\nReview\n\n\n\n[@Dai+2019] は有限混合で表される分布からのサンプリング法（Fusion 問題）に関する最初の理論解析である． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n[@Fearnhead+2017] は拡散過程を離散化誤差なしにシミュレーションする手法を提案している．逐次重点サンプリング（SIS）の連続時間極限を考えることで，提案過程と重点荷重との組がPDMPとなり，効率的なシミュレーションが可能になる． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習７\n\n\nエネルギーベースモデル\n\n\n\nDeep\n\n\n\n確率分布をエネルギーの言葉でモデリングする方法である．\n\n\n\n\n\n3/30/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\n\n\n\nProcess\n\n\n\nMarkov 過程のエルゴード性の証明は，カップリングの概念を用いれば極めて明瞭に見渡せる．\n\n\n\n\n\n3/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 連鎖\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖を良い例として，Markov 連鎖のエルゴード性に関連する概念を概観する．\n\n\n\n\n\n3/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論の中心に据えられるべき中心概念である．MCMC も確率核の分析に帰着する．\n\n\n\n\n\n3/24/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの微細化技術をレビューする．\n\n\n\n\n\n3/23/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\n\n\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n\n\n\n\n\n3/13/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\nState vs Loomis 判決を題材に，アルゴリズムと公平性を議論する．\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n転移学習とは\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．\n\n\n\n\n\n3/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\nPAC-Bayes は現実的に有用な鋭い PAC bound を得る新たな技術である．最適化の問題に帰着する点が研究を盛り上げている．Vapnik-Chervonenkis 理論の一般化であり，推定量上の確率分布を返すようなより一般的なアルゴリズムに対しても適用できる．\n\n\n\n\n\n3/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論４\n\n\n主成分分析\n\n\n\nComputation\n\n\nPython\n\n\n\n\\(K\\)-平均法は混合モデルの最尤推定アルゴリズムとみなせたように，主成分分析が一般の線型 Gauss 潜在変数モデルにおける最尤推定とみなせる．\n\n\n\n\n\n3/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP 配信マニュアル\n\n\nシンポジウムでの YouTube 配信のやり方\n\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n2/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門１\n\n\nJulia による確率的プログラミング\n\n\n\nJuliaLang\n\n\n\n数学者のために，深層生成モデルの１つ VAE を概観する．\n\n\n\n\n\n2/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門０\n\n\n数値計算への新たな接近\n\n\n\nJuliaLang\n\n\nLifestyle\n\n\n\n数学者のために，深層生成モデルの１つ VAE を概観する．\n\n\n\n\n\n2/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，「刑法入門」の内容を扱う．\n\n\n\n\n\n2/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n2023年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れ，近年の発展を概観する．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n深層生成モデルの１つ VAE は，統計モデルとしては変分 Bayes 推論アルゴリズムであり，変分下界をニューラルネットワークによって近似するというアプローチである．\n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル３\n\n\nグラフィカルモデルの推論\n\n\n\nBayesian\n\n\nComputation\n\n\n\n数学者のために，PGM (Probabilistic Graphical Model) の代表的な推論手法を紹介する．\n\n\n\n\n\n2/17/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nOptimization\n\n\n\n深層学習の学習における確率最適化アルゴリズムに関して概説する．\n\n\n\n\n\n2/16/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習５\n\n\n拡散モデル\n\n\n\nDeep\n\n\n\n数学者のために，深層生成モデルの１つである拡散モデルを概観する．\n\n\n\n\n\n2/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習６\n\n\n正規化流\n\n\n\nDeep\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n数学者のために，深層生成モデルを概観する．\n\n\n\n\n\n2/13/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析２\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価である．\n\n\n\n\n\n2/11/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する．\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である \\(K\\)-平均法の説明から始める．\\(K\\)-平均法は第一義的にはモデルフリーの（確率論と関係のない）クラスタリングアルゴリズムである．\n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\n\n\n\nProcess\n\n\nSimulation\n\n\n\n純粋跳躍過程の生成作用素を調べる．確率核 \\(\\mu\\) と到着強度 \\(\\lambda\\) という２つのパラメータは，それぞれジャンプ先を定める Markov 過程の遷移核と，ジャンプの起こりやすさを表す指数待ち時間のパラメータに対応する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\n\nNicolas Chopin の論文を読んで短くまとめたものです。\n\n\n\n\n\n1/30/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/29/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n粒子フィルターを拡散過程に対して適用することを考える．拡散過程の Euler-Maruyama 離散化に対して構成された粒子フィルターの，タイムステップを \\(0\\) にする極限 \\(\\Delta\\searrow0\\) での振る舞いを議論する．\n\n\n\n\n\n1/23/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n「穴あき式」の考え方\n\n\n\n\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n1/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\n\n\n\nProcess\n\n\n\nマルチンゲール問題とは何か？\n\n\n\n\n\n1/20/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装：リサンプリング編\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\n粒子フィルターにおいて，リサンプリングの段階が最も肝要で，効率に大きな影響を与える．本稿では，リサンプリングのアルゴリズムを複数紹介し，比較する．\n\n\n\n\n\n1/14/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\nESRP HP 変更希望点まとめ\n\n\n\n\n\n\nLifestyle\n\n\n\n2023年11月以降にチーム内で提案された変更希望点をまとめたものです．\n\n\n\n\n\n1/12/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書）第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\n\n\n\nProcess\n\n\n\n確率過程の離散化に関する漸近論的な結果を，Brown 運動を例に取り示す．\n\n\n\n\n\n1/09/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，\n\n\n\n\n\n1/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\n\n\n\nLifestyle\n\n\n\nVSCode で LaTeX を執筆するためのコツを収集していきます．A page collecting tips to author technical documents with VSCode. Also including a starting guide.\n\n\n\n\n\n12/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n数学者のために，PGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える，計算幾科学的技術であるとみなせる．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\n\n\n\nNature\n\n\nReview\n\n\n\n\n\n\n\n\n\n12/08/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\n\n\n\nLife\n\n\n\n筆者の数学を形作った書籍を編年体で紹介する．\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nGit覚書\n\n\n\n\n\n\nLifestyle\n\n\n\nGitとGitHubの仕組みを概観した覚書\n\n\n\n\n\n11/27/2023\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\n\n\n\nKernel\n\n\nOptimization\n\n\n\n数学者のために，SVMによるデータ解析が何をやっているのかを抽象的に説明する．\n\n\n\n\n\n11/18/2023\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSimulation\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\nParticles\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\nParticles\n\n\n\n1.1節”On the Origins of Feynman-Kac and Particle Models”を翻訳\n\n\n\n\n\n11/08/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\n\n\n\nLifestyle\n\n\n\nQuartoのチュートリアル＋紹介＋おすすめポイント\n\n\n\n\n\n11/04/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\n\n\n\nReview\n\n\n\n\n\n\n\n\n\n10/29/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recent.html",
    "href": "recent.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Notations | Categories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\nFeb 26, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\nFeb 25, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n2023年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れ，近年の発展を概観する．\n\n\n\n\n\nFeb 20, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n深層生成モデルの１つ VAE は，統計モデルとしては変分 Bayes 推論アルゴリズムであり，変分下界をニューラルネットワークによって近似するというアプローチである．\n\n\n\n\n\nFeb 18, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\nFeb 12, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\nFeb 11, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．\n\n\n\n\n\nFeb 11, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\nFeb 11, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\nFeb 10, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\nFeb 7, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である \\(K\\)-平均法の説明から始める．\\(K\\)-平均法は第一義的にはモデルフリーの（確率論と関係のない）クラスタリングアルゴリズムである．\n\n\n\n\n\nFeb 3, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\n\n\n\nProcess\n\n\nComputation\n\n\n\n純粋跳躍過程の生成作用素を調べる．確率核 \\(\\mu\\) と到着強度 \\(\\lambda\\) という２つのパラメータは，それぞれジャンプ先を定める Markov 過程の遷移核と，ジャンプの起こりやすさを表す指数待ち時間のパラメータに対応する．\n\n\n\n\n\nJan 31, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\nJan 24, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\nJan 19, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書）第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\nJan 11, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\nJan 10, 2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\nJan 5, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\nJan 5, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\nDec 23, 2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\n\n\n\nLifestyle\n\n\n\nVSCode で LaTeX を執筆するためのコツを収集していきます．A page collecting tips to author technical documents with VSCode. Also including a starting guide.\n\n\n\n\n\nDec 22, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n数学者のために，PGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える，計算幾科学的技術であるとみなせる．\n\n\n\n\n\nDec 20, 2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\nDec 20, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子フィルターは30年前に「万能」非線型フィルタリング手法として開発されたが，近年その真の力が明らかになりつつある．相関を持つ粒子系によって分布を逐次的に近似することで，複雑な分布からでも効率的にサンプリング出来るまたとない手法であるようだ．本稿では現在までのサンプラーとしてのSMC手法に対する理解をまとめる．\n\n\n\n\n\nDec 14, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\nDec 11, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\n\n\n\nNature\n\n\nReview\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\nDec 6, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\nDec 6, 2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\nDec 4, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\nDec 2, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\n\n\n\nLife\n\n\n\n筆者の数学を形作った書籍を編年体で紹介する．\n\n\n\n\n\nDec 1, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\nNov 25, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\nNov 23, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\nNov 22, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nComputation\n\n\nProbability\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\nNov 11, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\nParticles\n\n\n\n前文を翻訳\n\n\n\n\n\nNov 9, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\nParticles\n\n\n\n1.1節”On the Origins of Feynman-Kac and Particle Models”を翻訳\n\n\n\n\n\nNov 8, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\nNov 7, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\nNov 6, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\nNov 5, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\n\n\n\nLifestyle\n\n\n\nQuartoのチュートリアル＋紹介＋おすすめポイント\n\n\n\n\n\nNov 4, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\n\n\n\nReview\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html",
    "href": "posts/2024/Kernels/GP.html",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\nGauss 過程を用いた推論を実行するライブラリには，Matlab パッケージである GPML や，Python における GPy がある．"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html#gauss-過程回帰実践",
    "href": "posts/2024/Kernels/GP.html#gauss-過程回帰実践",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "1 Gauss 過程回帰（実践）",
    "text": "1 Gauss 過程回帰（実践）\n\n1.1 扱うデータ1\nデータ \\(x_1,\\cdots,x_{N},N=100\\) として，\\(\\mathop{\\mathrm{N}}(0,0.8^2)\\) に従う乱数を用意する．これに対して， \\[\ny_i=\\sin(3x_i)+\\epsilon,\n\\] \\[\n\\epsilon\\sim\\mathop{\\mathrm{N}}(0,0.09^2),\n\\] を通じて \\(y_1,\\cdots,y_N\\) を生成する．\n\nimport numpy as np\n\nN = 10\n\nnp.random.seed(1234)\n\nx = np.random.randn(N,1) * 0.8\n\ny = np.sin(3*x) + np.random.randn(N,1) * 0.09\n\nxs = np.linspace(-3,3,61).reshape(-1,1)\n\nこの非線型関数 \\(\\sin\\) を，Gauss 過程回帰がどこまで復元できるかが実験の主旨である．\n\n\n1.2 GPy を用いた場合\nGPy を用いて Gauss 過程回帰を行うには，GPy.models.gp_regression モジュールの GPRegression クラス\nclass GPRegression(X, Y, kernel=None, Y_metadata=None, normalizer=None, noise_var=1.0, mean_function=None)\nを用いる．ソースコードは こちら．\n引数のカーネル kernel は PGPy kernel オブジェクトを取り，デフォルトは rbf カーネルである．我々も RBF カーネル を用いることとする．これは GPy パッケージでは GPy.kern.src.rbf モジュールの RBF クラスで提供されている：\nclass RBF(input_dim, variance=1.0, lengthscale=None, ARD=False, active_dims=None, name='rbf', useGPU=False, inv_l=False)\nソースコードは こちら．\nモデルオブジェクトを初期化した後は次のように進む\n\noptimize メソッド でハイパーパラメータを最適化する．\noptimize(optimizer=None, start=None, messages=False, max_iters=1000, ipython_notebook=True, clear_after_finish=False, **kwargs)\nこれはインスタンスの self.log_likelihood と self.log_likelihood_gradient を用いて，負の対数尤度を最小化する形で行われる．\npredict メソッド でテスト点での予測を行う．\npredict(Xnew, full_cov=False, Y_metadata=None, kern=None, likelihood=None, include_likelihood=True)\n返り値は事後平均と事後分散を numpy.ndarray として返す．\nmatplotlib を用いて予測の結果をプロットする．\n\n\nimport GPy\nimport matplotlib.pyplot as plt\n\nkernel = GPy.kern.RBF(input_dim=1, variance=1.0)\nmodel = GPy.models.GPRegression(x, y, kernel)\n\nmodel.optimize()\nmu, var = model.predict(xs)\n\n# テスト点での平均と95%信頼区間のプロット\nupper = mu + 1.96*np.sqrt(var)\nlower = mu - 1.96*np.sqrt(var)\nplt.fill_between(xs[:,0], lower[:,0], upper[:,0], color='lightgray', label='95% confidence interval', alpha=0.5)\nplt.plot(xs, mu, label='Predicted mean')\nplt.scatter(x, y, c='r', label='Observations', s=10)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport GPy\nimport matplotlib.pyplot as plt\n\nkernel = GPy.kern.RBF(input_dim=1, variance=1.0)\nmodel = GPy.models.GPRegression(x, y, kernel)\nmodel.optimize()\n\nxs = np.linspace(x.min(), x.max(), 1000)[:, None]\nmu, var = model.predict(xs)\n\nupper = mu + 1.96 * np.sqrt(var)\nlower = mu - 1.96 * np.sqrt(var)\n\nfig, ax = plt.subplots(figsize=(6, 4))  # グラフサイズを小さく\n\n# 背景を白に\nax.set_facecolor('white')\n\n# グラフ領域を削除\nax.patch.set_visible(False)\n\n# 軸を細く\nax.spines['bottom'].set_linewidth(0.5)\nax.spines['left'].set_linewidth(0.5)\n\n# メモリを非表示\nax.tick_params(axis='both', which='both', length=0, labelleft=False, labelbottom=False, left=False, bottom=False)\n\n# 軸ラベルを削除\nax.set_xlabel('')\nax.set_ylabel('')\n\n# 凡例を非表示\nax.legend().set_visible(False)\n\n# データプロット\nax.fill_between(xs[:, 0], lower[:, 0], upper[:, 0], color='lightgray', alpha=0.5)\nax.plot(xs[:, 0], mu[:, 0], color='k', lw=1)\nax.scatter(x[:, 0], y[:, 0], c='b', s=30)\n\nplt.tight_layout(pad=0.2)\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n特に \\([-2,2]\\) の区間において，元の関数 \\(\\sin\\) をよく復元できていることが分かる．実際，\\(y=\\sin(3x)\\) と重ねてプロットすると次の通り：\n\n\n\n\n\n\n\n\n\n\n\n1.3 scikit-learn を用いた場合\n\n\n\n\n\n\n補足：scikit-learn における Gauss 過程回帰\n\n\n\n\n\nこのような単純な解析では，scikit-learn と用いるとより同じ分析が実行できる．\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n\ngp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1)\n\n# モデルの学習\ngp.fit(x, y.ravel())\n\nmu, s2 = gp.predict(xs, return_std=True)\n\n# テスト点での平均と95%信頼区間のプロット\nplt.fill_between(xs.ravel(), mu - 1.96 * s2, mu + 1.96 * s2, color='lightgray', label='95% confidence interval', alpha=0.5)\nplt.plot(xs, mu, label='Predicted mean')\nplt.scatter(x, y, c='r', label='Observations', s=10)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html#gauss-過程による分類",
    "href": "posts/2024/Kernels/GP.html#gauss-過程による分類",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "2 Gauss 過程による分類2",
    "text": "2 Gauss 過程による分類2\n本質的には Gauss 過程回帰と変わらないが，回帰の場合と変え得る．\n\n2.1 扱うデータ\nここでは， \\[\nm_1:=\\begin{pmatrix}3/4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}-3/4\\\\0\\end{pmatrix},\n\\] \\[\n\\Sigma_1:=\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix},\\quad\\Sigma_2:=\\begin{pmatrix}1&0.95\\\\0.95&1\\end{pmatrix},\n\\] とし，\\(\\mathop{\\mathrm{N}}_2(m_1,\\Sigma_1)\\) から \\(n_1:=320\\) データ，\\(\\mathop{\\mathrm{N}}_2(m_2,\\Sigma_2)\\) から \\(n_2:=160\\) データを生成する：\n\nn1, n2 = 320, 160\nS1 = np.eye(2)\nS2 = np.array([[1, 0.95], [0.95, 1]])\nm1 = np.array([0.75, 0])\nm2 = np.array([-0.75, 0])\n\nx1 = np.random.multivariate_normal(m1, S1, n1)\nx2 = np.random.multivariate_normal(m2, S2, n2)\n\nx = np.vstack((x1, x2))\n\ny1 = -np.ones(n1)\n1y2 = np.ones(n2)\ny = np.concatenate((y1, y2)).reshape(-1,1)\n\nplt.plot(x1[:, 0], x1[:, 1], 'o', label='Class 1')\nplt.plot(x2[:, 0], x2[:, 1], '*', label='Class 2')\nplt.legend()\nplt.show()\n\n\n1\n\nクラスラベルは \\(\\{\\pm1\\}\\) であることに注意．\n\n\n\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\n\\(n_1:n_2=2:1\\) であるから，このデータは Gauss 混合モデル \\[\n\\frac{2}{3}\\phi(x;m_1,\\Sigma_1)+\\frac{1}{3}\\phi(x;m_2,\\Sigma_2)\n\\tag{1}\\] からのデータと見れる．ただし，\\(\\phi(x;m,\\Sigma)\\) は \\(\\mathop{\\mathrm{N}}_2(\\mu,\\Sigma)\\) の密度関数とした．\nサンプリング点は \\([-4,4]^2\\) 内の幅 \\(0.1\\) の格子点とする：\n\nt1, t2 = np.meshgrid(np.arange(-4, 4.1, 0.1), np.arange(-4, 4.1, 0.1))\nt = np.column_stack([t1.flat, t2.flat])\n\n点 \\(x\\) でモデル 1 からのデータが観測されたとき，これがクラス \\(1,2\\) からのものである確率 \\(p_1,p_2\\) は \\[\n\\begin{align*}\n    p_1&=\\frac{n_1}{n_1+n_2}\\phi(x;m_1,\\Sigma_1)\\\\\n    &=\\frac{1}{2\\pi(n_1+n_2)}\\cdot n_1\\frac{e^{-\\frac{1}{2}(x-m_1)^\\top\\Sigma_1^{-1}(x-m_1)}}{\\sqrt{\\det\\Sigma_1}}\n\\end{align*}\n\\] \\[\np_2= \\frac{1}{2\\pi(n_1+n_2)}\\cdot n_2\\frac{e^{-\\frac{1}{2}(x-m_2)^\\top\\Sigma_2^{-1}(x-m_2)}}{\\sqrt{\\det\\Sigma_2}}\n\\] である．\nよって，\\(x\\in[-4,4]^2\\) がクラス \\(2\\) からのものである確率を，等高線 (contour) としてプロットすると，次の通りになる：\n\ninvS1 = np.linalg.inv(S1)\ninvS2 = np.linalg.inv(S2)\ndetS1 = np.linalg.det(S1)\ndetS2 = np.linalg.det(S2)\n\ntmm1 = t - m1\np1 = n1 * np.exp(-0.5 * np.sum(tmm1.dot(invS1) * tmm1, axis=1)) / np.sqrt(detS1)\n\ntmm2 = t - m2\np2 = n2 * np.exp(-0.5 * np.sum(tmm2.dot(invS2) * tmm2, axis=1)) / np.sqrt(detS2)\n\nposterior = p2 / (p1 + p2)\n\n# 等確率等高線のプロット\ncontour_levels = np.arange(0.1, 1, 0.1)\nplt.contour(t1, t2, posterior.reshape(t1.shape), levels=contour_levels)\n\n# データポイントのプロット\nplt.plot(x1[:, 0], x1[:, 1], 'o', label='Class 1', alpha=0.5)\nplt.plot(x2[:, 0], x2[:, 1], '*', label='Class 2', alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.2 モデル\n平均は \\(0\\) とし，共分散関数は 関連度自動決定 (ARD: Autonatic Relevance Determination) (MacKay, 1994), (Neal, 1996, p. 16) を用いる．\nこれは，２つの入力 \\(x_1,x_2\\) が異なる重要度を持つ場合，それぞれの入力に対するスケールパラメータを導入する手法である．\nこれは，GPy.kern.RBF 関数のキーワード引数 ARD=True を通じて実装できる：\n\nimport time\n\nstart_time = time.time()\n\nmeanfunc = GPy.mappings.Constant(2,1)\nkernel = GPy.kern.RBF(input_dim=2, ARD=True)\n\nmodel = GPy.models.GPClassification(x, y, kernel=kernel, mean_function=meanfunc)\nmodel.optimize()\n\n# テストデータセットに対する予済分布の計算\ny_pred, _ = model.predict(t)\n\nend_time = time.time()\n\n# 予測確率の等高線プロット\nplt.figure(figsize=(8, 6))\nplt.plot(x1[:,0], x1[:,1], 'o', label='Class 1', alpha=0.5)\nplt.plot(x2[:,0], x2[:,1], '*', label='Class 2', alpha=0.5)\ncontour = plt.contour(t1, t2, y_pred.reshape(t1.shape), levels=np.linspace(0, 1, 10))\nplt.clabel(contour, inline=1, fontsize=10)\nplt.legend()\nplt.show()\n\nelapsed_time = end_time - start_time\nprint(f\"実行時間: {elapsed_time:.1f} 秒\")\n\n\n\n\n\n\n\n\n実行時間: 15.4 秒\n\n\n図 1 の真の構造の特徴を捉えていることが判る．"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html#footnotes",
    "href": "posts/2024/Kernels/GP.html#footnotes",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDocumentation for GPML Matlab Code version 4.2 3c 節を参考にした．↩︎\nDocumentation for GPML Matlab Code version 4.2 4e 節を参考にした．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/GP2.html",
    "href": "posts/2024/Kernels/GP2.html",
    "title": "Gauss 過程を用いた統計解析２",
    "section": "",
    "text": "ガウス過程はベイズ統計の立場から見たカーネル法ということができます．(持橋大地 & 大羽成征, 2019)"
  },
  {
    "objectID": "posts/2024/Kernels/GP2.html#導入",
    "href": "posts/2024/Kernels/GP2.html#導入",
    "title": "Gauss 過程を用いた統計解析２",
    "section": "1 導入",
    "text": "1 導入\n独立同分布な事前分布の下で，１層の全結合ニューラルネットワークは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価である (Neal, 1996)．\nしたがって，(Williams, 1996) などの方法で対応する Gauss 過程が特定できれば，当該のニューラルネットワークと等価な Bayes 推論が可能になる．\nGauss 過程との同様の対応は，多層のニューラルネットワークの間にもつけられている (Lee et al., 2018)．この論文では，ニューラルネットワークの性能を凌駕することが観察されている．\nしかし現状でニューラルネットワークが Gauss 過程を越えるのは，推論手法である誤差逆伝播法 (Rumelhart et al., 1986) においてであろう．"
  },
  {
    "objectID": "posts/2024/Kernels/GP2.html#gauss-過程回帰の理論",
    "href": "posts/2024/Kernels/GP2.html#gauss-過程回帰の理論",
    "title": "Gauss 過程を用いた統計解析２",
    "section": "2 Gauss 過程回帰の理論",
    "text": "2 Gauss 過程回帰の理論\n\n2.1 カーネル\n\n2.1.1 動径基底関数カーネル\n動径基底関数カーネル (Radial Basis Function kernel) (持橋大地 & 大羽成征, 2019, p. 68) または Squared Exponential kernel (Rasmussen & Williams, 2006, p. 14) は \\[\nk(r):=\\sigma^2\\exp\\left(-\\frac{r^2}{2}\\right)\n\\] で定まるカーネルである．GPy での実装はこちら．\n\n\n2.1.2 定数カーネル\n\n\n\n2.2 推論手法\n\n2.2.1 Expectation Propagation 法\n\nEP might not converge in some cases since quadrature is used. GPML 4.2 Documentation\n\n\n\n2.2.2 FITC (Fully Independent Training Conditional) 推論\n\n\n2.2.3 MCMC\nMCMC は唯一ブラックボックスとして用いることが出来ない推論手法である．また，勾配ベースの周辺尤度最適化も MCMC では不可能である．\n\nInference by MCMC sampling is the only inference method that cannot be used as a black box. Also gradient-based marginal likelihood optimisation is not possible with MCMC. Please see usageSampling for a toy example illustrating the usage of the implemented samplers. GPML 4.2 Documentation\n\nこの関門が乗り越えられたならば，Gauss 過程による機械学習の応用は大きく進展するだろう．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html",
    "href": "posts/2024/Kernels/Deep.html",
    "title": "数学者のための深層学習１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#歴史",
    "href": "posts/2024/Kernels/Deep.html#歴史",
    "title": "数学者のための深層学習１",
    "section": "1 歴史",
    "text": "1 歴史\n深層学習とは，多層パーセプトロンを用いた機械学習の手法をいう．\n画像認識，自然言語処理，画像生成，タンパク質の構造予測など，多くの分野で成功を収めている．\n\n1.1 ニューラルネットワークとは？\n\n1.1.1 脳の解明\n(McCulloch & Pitts, 1943) は脳の神経回路を命題論理の枠組みで扱い，ニューロンの論理素子としての機能を考察した（McCulloch-Pitts の形式ニューロンや閾素子と呼ばれる）．なお，パーセプトロンはシグモイド関数を活性化関数に用いた場合，Turing 完全である (Siegelmann & Sontag, 1991)．\n脳を模したモデルとして，当然学習能力をどうモデルに組み込むかが問題になる．これはシナプスの結合荷重の変化によるものだという仮設は古くからあったが，最も明確な形で表現したのが (Hebb, 1949) であった．\n具体的には Hebb 則は，２つの正の相関を持つ（＝共起しやすい）ニューロンの間の荷重は増強される，というものである．1 これは連想記憶のモデルとして提案された（第 1.3.1 節も参照）．\n\n\n1.1.2 パーセプトロン\nパーセプトロンは，脳の記憶と認識のモデルとして，(Rosenblatt, 1958) が心理学の学会誌に発表した．2\n複雑過ぎるものに対する理論解析は進まなかった．\nそのような中で (Minsky & Papert, 1969) は線型分離不可能な問題に対しては線型単純パーセプトロンは解を見つけられないほど機能は劣り，かといって複雑なものは計算量が爆発するので，結局パーセプトロンは使い物にならないのではないかとの見方を示した (甘利俊一, 1989, p. 130)．\n現状の深層学習の成功を見ている者からすれば，これだけのことでパーセプトロンの研究が下火になってしまったことは驚くべきことに感じる．\n(甘利俊一, 1989) はこの点を，次のように断罪している．\n\nミンスキーらは，自分たちの立てた問題は正しく解いた．しかし問題の立て方を誤ったのである．(甘利俊一, 1989, p. 132)\n\n\n\n1.1.3 多層パーセプトロン\nパーセプトロンの発明の時点で，深層学習のモデルとしてはすでに完成していた．ただ，(Minsky & Papert, 1969) の指摘の通り，計算複雑性や学習アルゴリズムの問題が残り続けた．\n多層のパーセプトロンでは中間層を学習させることが課題であった．これに対しては，活性化関数に可微分なものを用いて確率的勾配降下法によって学習させることで解決できること (Amari, 1967) などは早い時期から議論されていた．この問題点は，局所解に囚われることであった．\n多層のパーセプトロンに対して，局所解に囚われるなどの問題はあれど，極めて多くの場合で誤差逆伝播法が有効な学習法になることが広く周知されたのは，(Rumelhart et al., 1986), (Rumelhart et al., 1987) であり，これが深層学習の第二次ブームの着火剤となった．3\nすぐさま英語の発音を学習させた研究 (Sejnowski & Rosenberg, 1987) の デモ も発表され，大きな波紋を呼び起こした．ここでも特徴的な内部表現が観察された．\n局所解に囚われることが解決された訳ではない．技術的な問題点（特に，結局深層モデルの訓練はうまくいかないこと）はあれど，ニューラルネットワークは実用的に極めて簡単に使えるモデルであると知らしめたのである．\n\nIn short, we believe that we have answered Minsky and Papert’s challenge and have found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced. (Rumelhart et al., 1987, p. 361)\n\n現在は，ドロップアウト や区分的線型な活性化関数 ReLU を用いるなどの工夫がなされている (Goodfellow et al., 2014)．\n\n\n\n1.2 深層化の歴史4\n\n1.2.1 深層化の障壁\n誤差逆伝播法をニューラルネットに使うことで，表現学習がなされることの発見 (Rumelhart et al., 1986) から，深層学習の分野は次の点で変化したという：\n\n神経科学・生物学的なモチベーションから遊離し，より確率論的・統計学的なアプローチが主流になった (Bishop & Bishop, 2024, p. 19)．\n多層のニューラルネットワークと高次元データに対する理論的な研究が加速した (MacKay, 2003, p. 535)．\n\nしかし，画像識別タスクに特化して結合構造を予め作り込むことで学習を容易にしてある畳み込みニューラルネットワーク (CNN) (LeCun et al., 1998) などを除いて，２層以上のニューラルネットワークの成功した応用例は殆どなかった．これは，勾配消失 により，どんなに多層なニューラルネットワークを構築しても，最後の２層程度しか意味のあるパラメータを学習できなかったためである．\nそのために，多層のニューラルネットワークでは表現学習は難しいという認識が広まり，複雑なタスクに対しては，問題固有の特徴抽出技法が編み出されるのみで，一般的な解決法はなかった．\n３層のニューラルネットワークも，隠れ層の幅が無限大の極限で，任意の連続関数を近似できるという普遍近似定理という抽象的な慰め (Hecht-Nielsen, 1989) があるのみであった．\nそこで多くの研究者は，SVM (Cortes & Vapnik, 1995) やカーネル法，Gauss 過程に現実的な打開策を求め始めたのである．これは冬の時代とも呼ばれる．\n\n\n1.2.2 ネオコグニトロンと CNN\n(Wiesel & Hubel, 1959) は猫の視覚野には，単純細胞と複雑細胞の２種類の細胞があることを発見した．また，生後すぐの猫を一日交代で異なる片目を覆って成長した猫では，多くの視覚野の神経細胞は単眼性になる (Hubel, 1967)．\nこれらの観察から，(von der Malsburg, 1973) や (Fukushima, 1975) は，特徴抽出細胞が自己形成するメカニズムを，自己組織化のキーワードの下で調べた．特に後者は コグニトロン というモデルを提案した．\n(Fukushima, 1980) の ネオコグニトロン は初の深層モデルの例と言える．5 これは，単純細胞層と複雑細胞層を交互に深く重ねたネットワークである．\nしかし 福島 は学習の問題を中心に扱った訳ではなかった．6 (LeCun et al., 1998) の畳み込みニューラルネットワーク LeNet は，ネオコグニトロンを誤差逆伝播法により教師あり学習させたものと言える．\nこのモデルは後に AlexNet としてダントツの性能で世界を驚かせることになる．\n画像データはピクセルを節としたグラフデータとみなすこともでき，CNN を一般化する形で GNN (Graph Neural Network) も提案されている (J. Zhou et al., 2020), (Wu et al., 2021), (Veličković, 2023)．\n\n\n1.2.3 自己符号化器\n(Cottrell & Munro, 1988) は中間層の幅を小さくし，入力信号自身を教師信号として誤差逆伝播法により学習することで，隠れ層に入力の低次元表現が学習され，主成分分析に用いることが出来ることを報告した．\nこれは入力層と出力層の素子数を一致させ，出力が入力に近づくように訓練される．このようなモデルを 自己符号化器 (autoencoder) または 自己連想ネットワーク (auto-associative neural network) と呼ぶ．\n内部表現を獲得するとされる中間層に対して，それより前半の層全体を符号化器，それ以降を複合化器と呼ぶ．7\n(Baldi & Hornik, 1989) は活性化関数がない３層の場合，自己符号化器を訓練させることは主成分分析を実行することに等価であることを示した．\nさらに層を増やすことで，非線型な次元圧縮が可能であることが (DeMers & Cottrell, 1992) で示された．この論文では，データの空間内の（非線型な）部分多様体の局所座標が学習されたり，人間の顔の写真のデータ圧縮が出来ることが実証されている．\n自己符号化器はスタッキングが可能である (Ballard, 1987)．このことが，次の節で述べる事前学習のアイデアに繋がる．\n\n\n1.2.4 自己符号化器を用いた事前学習\n自己符号化器自体を符号圧縮に用いることは，他の手法と比べて特別優れているという訳でもなかった．\nしかし，深層ニューラルネットワークを層ごとに事前に教師なし学習をすることで，後の教師あり学習において勾配消失などの問題が回避できるというアイデア (Hinton & Salakhutdinov, 2006), (Hinton et al., 2006) は画期的であり，深層学習 (Deep Learning) という言葉が広まったのもこの頃であるという (Schmidhuber, 2015, p. 96)．\n(Bengio et al., 2006) もこれを拡張し，この事前学習によって，素性の良い局所解の近くにパラメータの初期値が調整されるということを実験的に示している．\n\n\n1.2.5 AlexNet\nImageNet データベース (Deng et al., 2009) を用いた判別コンテスト ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) で，ダントツで優勝した AlexNet (Krizhevsky et al., 2012) が大きなターニングポイントとなった．\nこれも (LeCun et al., 1998) の CNN を基にした８層のモデルで，活性化関数には ReLU とドロップアウトによる正則化が用いられていた．学習も，NVIDIA 社の GPU を用いていた．\n\n\n1.2.6 ResNet\n20 層以上の多層ニューラルネットワークの学習の困難さを初めて解決したのが (He et al., 2016) の Residual Network (ResNet) であった．8\nこのモデルは 152 層もあったが効率的に訓練することが可能で，2015 年の ILSVRC でダントツで優勝し，しかも初めて人間の誤答率 5% (Dodge & Karam, 2017) を下回ったのである．\nそのアイデアは，各層の入力 \\(x\\) を出力に再度加算した形 \\[\ny=x+F(x)\n\\] で各層をデザインし，現状の入力からの差分 \\(F\\) のみを学習するとすることで勾配消失を回避する，というものであった．実際，この層の微分は \\[\n\\frac{d y}{d x}=1+F'(x)\n\\] と表せる．\nこのテクニックは トランスフォーマー (Vaswani et al., 2017) などのモデルでも用いられている．\n(Li et al., 2018) によると，残差レイヤーの追加は誤差関数を滑らかにし，近しい入力に対しても勾配が非常に大きくなってしまうことが効率的な学習を阻害してしまう問題 (shattered gradient problem) (Balduzzi et al., 2017) を解決している，という．\n\n\n\n1.3 その他のネットワーク\nニューラルネットワークは，内部に循環を持つかどうかで二分され，それぞれを Feedforward Network (FFN) と Feedback Network (FBN) と呼ぶ．9 Recurrent Neural Network (RNN) は FBN の例である．\nその他にも，ニューラルネットワークには様々なものがある．そもそも，ニューラルネットワークは学習機械として以外に，生物の神経機能のモデルとして，また複雑系としても研究されており，しばしば全く違ったアーキテクチャが考えられている．10\nここでは特に 教師なしニューラルネットワーク を取り上げる．11\n\n1.3.1 Hopfield ネットワークとスピングラス\n計算機の記憶と生物の記憶の相違点のうち，大きなものには連想性 (associativity) がある．アドレスで整理されているのではなく，内容で整理されているのである．1970 年代には，神経の連想記憶機能のモデルとしてのニューラルネットワークが多数提案された．\nそれには，ここまで議論してきた階層型のネットワークと異なり，ノード同士は 相互結合 しているものも含まれる．連想記憶のモデルとしては，特に相互結合で，どちらの方向に関する重みも同じであるもの（対称結合 ネットワーク）が多く，全結合の FBN である Hopfield network はその代表例である．12\n（連続変数の）Hopfield network の学習則は，Hebb 則 (Hebb, 1949) に基づいて，各ニューロン \\(x_j\\in[0,1]\\) についてその入力 \\[\na_i:=\\sum_{j}w_{ij}x_j\n\\] を計算し，\\(x_i\\gets\\tanh(a_i)\\) と更新する．\n実はこの学習則は必ず収束する．このことを，(Hopfield, 1982) はスピングラスのモデルと関連付けて示したことから，特に統計物理学の文脈で Hopfield network の名前がついた．13\n\n神経回路網の解析，とくに連想記憶モデルの解析が，スピングラスを解析する方法を用いて実行できるのではないかという考えが出てきて，大量の物理学者が神経回路網に注目しだした．こうしたアイデアの火付け役がホップフィールドと言われる．(甘利俊一, 1989, p. 105)\n\n対称結合のニューラルネットワークの学習則は，スピングラスと同様に，必ずポテンシャル関数が減少する方に動作するというのである．この連関を利用して，(Hopfield & Tank, 1985) は Hopfield ネットワークをアナログ回路に実装し，巡回セールスマン問題を解くという，最適化問題ソルバーとして利用してみせた．\n単体 Hopfield Network (Burns & Fukai, 2023) という拡張もあり，パラメータ数は変わらずとも記憶容量が増える．\n\n\n1.3.2 ボルツマンマシン (Ackley et al., 1985)\nボルツマンマシンは確率的 Hopfield ネットワークともいい，Hopfield ネットワークが統計物理のモデルとして近似するところの Gibbs 分布を，実際に持つ Markov 確率場 の一種である．14\nこれは，荷重を，確率 \\(\\frac{1}{1+e^{-2a_i}}\\) で \\(x_i=1\\)，そうでない場合は \\(x_i=-1\\) と定めることで得られる．\n\n\n1.3.3 深層ボルツマンマシン (Salakhutdinov & Hinton, 2009)\n\n\n1.3.4 深層信念ネットワーク (Hinton et al., 2006)\nこれは深層学習研究の皮切りになった確率的深層モデルである．\n\n\n\n1.4 Spiking Neural Network\n実際の神経細胞は 発火 という離散的なイベントを発生させ，その頻度やタイミングも大きな役割を持っている．これを取り入れたモデルを Spiking Neural Network (SNN) (Maass, 1997) と呼ぶ (岡島義憲, 2020)．\nSNN は現状の人工ニューラルネット (ANN: Artificial Neural Network) よりも，半導体上での計算を効率化することが出来る．そこで，SNN による深層学習が近年試みられている (Tavanaei et al., 2019)．\nMicrosoft の BitNet (Wang et al., 2023) も計算効率性を目指すにあたってそのアイデアを等しくする．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#ベイズ深層学習",
    "href": "posts/2024/Kernels/Deep.html#ベイズ深層学習",
    "title": "数学者のための深層学習１",
    "section": "2 ベイズ深層学習",
    "text": "2 ベイズ深層学習\n\n2.1 ネットワークの正則化と汎化性能\n良い汎化性能を得るためには，偏倚と分散のトレードオフ を乗り越える必要がある．\nデータセットに対してモデルの自由度が高すぎると，分散は小さくなれど，過学習を起こしてしまう．すなわち，大きなバイアスが導入され，汎化性能が悪くなる．一方で，モデルの自由度が低すぎると，平均的には正しい予測ができても，分散が大きくて役に立たない．\n大規模なデータセットを用いることは一つの解決である．自由度の高いモデルを用いても過学習が起こりにくくなるためである．15\n\n\n2.2 帰納バイアス\n正則化とは，モデルの自由度を制限することで過学習を抑制することである．これは，「正しいモデルは十分に滑らかであるはずである」という帰納バイアスを導入することで，モデルの汎化性能を改善させていることに等しい．16\n転移学習も一種の帰納バイアスの注入だと見れる．２つの異なるタスクの間に類似性が存在するという事前知識を注入することで，汎化性能を改善する手法だと思えるのである．\n汎化性能の高いモデルを作るということは，人類が解きたいタスクに普遍的に共通する特徴を捉え，これを帰納バイアスの形でモデルに注入することに等しい．No free lunch theorem (Wolpert, 1996) から，17 一般に特定のタスクに対する性能向上は，他のタスクに対する性能低下を伴うものであることが予想されるが，例えば推定関数が十分滑らかであるというのは，人間の認識特性上，有意義な結果にはほとんど普遍的に必要な条件である．\n\n\n2.3 ベイズ深層学習の美点\n(Gal & Ghahramani, 2016) などでは，\n\nデータの数が少なくとも，有効なパラメータ推定が可能である．\n過学習が起こりにくい．\n不確実性の定量化が自動でなされる．\n\nと説明される．\n\n\n2.4 ベイズ深層学習の例\nCNN は特に過学習しやすい上にサンプル効率性が悪い．その場合には，変分近似による Bayesian CNN は既存法と同等の性能を誇る (Gal & Ghahramani, 2016)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#代表的なアーキテクチャ",
    "href": "posts/2024/Kernels/Deep.html#代表的なアーキテクチャ",
    "title": "数学者のための深層学習１",
    "section": "3 代表的なアーキテクチャ",
    "text": "3 代表的なアーキテクチャ\nCNN はトランスフォーマーによりデータ間の関係を自動的に学習する枠組みが提案される前に，主に画像分野において，データの構造に関する事前知識をモデルに組み込んだ例として提案されたものである．\n世界初の深層学習モデルによる席巻は，CNN により，画像認識の分野において達成された．\n\n3.1 Computer Vision という分野\nComputer Vision という問題の複雑性が，ニューラルネットワークのアーキテクチャの開発を後押しした歴史がある．\n並行移動・拡大変換という2つの合同変換不変性に対して，画像の認識結果は不変であるべきである．\nこのような不変性，または 同変性 (equivariance) をモデルに取り入れる方法は大きく分けて４つある：\n\n誤差関数に正則化項を導入する18\n対称性を取り入れた潜在表現 を用いてその上で学習をする\n不変性を効率良く学習出来るように データセットを拡張する\n対称性を取り扱う構造をネットワークのアーキテクチャに組み込む\n\nCNN 節 1.2.2 は４番目のアプローチで歴史上最初に取られたものである．しかし，このどのアプローチも完全には不変性を取り入れることは出来ていないことも報告されている (Azulay & Weiss, 2019)．\n幾何学を種々の変換に対する不変性の研究と捉え直した Felix Klein の Erlangen program に倣い，種々の深層モデルの帰納バイアスとアーキテクチャを，幾何学的な変換から導出してシステマティックに理解する試み Geometric Deep Learning (Bronstein et al., 2021) がある（第 3.6 節）．\n\n\n3.2 物体認識 CNN\nCNN は画像の特徴を，階層的に学習出来るように誘導するような構造を持っている．\n多くの例では，畳み込み層とプーリング層が交互に繰り返され，最後に全結合層を持つような構造を持っている．\n\n3.2.1 局所的な特徴\n最初の素子は，画像の局所的な一部のみを入力として取る．その範囲を 受容野 (receptive field) と呼ぶ．この素子の荷重を フィルター または カーネル という．\n\n\n\n\n\n\n例：2次元の畳み込み層\n\n\n\n\n\n2次元での幅 \\(2k+1\\) の畳み込み層は，フィルター \\(\\mathrm{supp}\\;(\\psi)\\subset\\{0,\\pm1,\\cdots,\\pm k\\}\\) を用いて， \\[\nf^{\\text{out}}_{i,j}=\\phi\\left(\\sum_{a,b\\in\\mathbb{Z}}\\psi(i-a,j-b)f_{i,j}^{\\text{in}}+\\theta\\right)\n\\] と表せる．\n\n\n\n決まったカーネルに対して，この素子はカーネルの特徴にマッチした入力に対して，大きな出力を返す．\n次に，このフィルターを畳み込むことで，画像内の異なる位置に存在する特徴を検出する．畳み込み層は，荷重を共有した疎結合層ということになる．\n畳み込みを行うと，入力次元と出力次元が変わってしまうことがあるため，その場合は入力画像にパディングを施す．\n出力次元を小さくして，畳み込み特徴写像で大きな次元削減を行いたい場合，strided convolution を用いる．\n\n\n3.2.2 並行移動不変性\n畳み込みの結果が，特徴の位置の変化に対して不変になるようにする設計に，プーリング層 または ダウンサンプリング層 (down-sampling / sub-sampling) がある．\nプーリングも，受容野を持った素子と畳み込みからなるが，畳み込みに学習されるべきパラメータはなく，確定的な関数と畳み込まれる．\n代表的なプーリング関数には 最大プーリング (Y. Zhou & Chellappa, 1988) や平均プーリング，\\(l^2\\)-プーリングなどがある．\nプーリングは不変性の導入に加えて，畳み込み特徴のダウンサンプリングを行って，更なる次元削減を行う役割も果たす．\n\n\n\n3.3 画像分割 CNN\n画像分類では，１枚の画像に対して１つのクラスの対応づけたが，１つのピクセルに１つのクラスを対応づけることで，画像をクラスごとに分類することが考えられる．\n\n3.3.1 Up-sampling\n画像分類の問題では，最終的に全ピクセルから得た情報を１次元に圧縮することになる．一方で，十分な潜在表現を得たのちは up-sampling に転じる Encoder-Decoder 構造にすることで，最終的に元の画像サイズに戻しながら，画像分割問題を解くことが出来る (Long et al., 2015), (Noh et al., 2015), (Badrinarayanan et al., 2017)．\n(Badrinarayanan et al., 2017) は max-unpooling などの up-sampling 層の設計を考慮したが，これに学習可能なパラメータを増やした transpose convolution / deconvolution も提案された．\nPooling 層を一切用いず，down-sampling も up-sampling も畳み込み層のみによって行われる場合，これを 全畳み込みネットワーク (fully convolutional network) という (Long et al., 2015)．\n\n\n3.3.2 U-Net\nこの encoder-decoder 構造は，分類に必要のない情報を自動的に削減し，モデルのサイズを小さくするのには効果的であるが，タスクによっては元の画像の情報量を保ちたい場合がある．\nU-net (Ronneberger et al., 2015) は対応する down-sampling 層と up-sampling 層とを直接繋ぐ経路を追加することでこれを解決した．\n\n\n3.3.3 Capsule Networks\nプーリング層は並行移動不変性の概念を取り入れるが，回転や拡大などの変換に対する不変性を取り入れるにはデータ拡張に依るしかないのでは，CNN の学習はどうしても大規模なデータセットが必要になってしまう．\nそこで，アーキテクチャによる解決を試みたのが，畳み込み層に加えて カプセル層 を取り入れた Capsule Network (Sabour et al., 2017) である．\n\n\n\n3.4 Inpainting\n(Horita et al., 2023)\n\n\n3.5 スタイル転移\nCNN において，最初の方のレイヤーは局所的な特徴を捉えているが，後の方のレイヤーはスタイルなどの大域的な特徴を捉えている．\nこれを用いて，既存の画像の具体的な特徴を変えずに，他の画像からスタイルのみを転移する手法 Neural Style Transfer が提案された (Gatys et al., 2015), (Gatys et al., 2016)．\n\n\n3.6 幾何学的深層学習\n以上，種々のタスクに種々のアーキテクチャが存在することを見てきた．この状況は，19 世紀の幾何学と似ていると (Bronstein et al., 2021) はいう．\nこれらのアーキテクチャがどのような帰納バイアスを導入する役割を果たしているかを，不変性や同変性といった第一原理から理解する試みが 幾何学的深層学習 である．\n\nIn this text, we make a modest attempt to apply the Erlangen Programme mindset to the domain of deep learning, with the ultimate goal of obtaining a systematisation of this field and ‘connecting the dots’. We call this geometrisation attempt ‘Geometric Deep Learning’, and true to the spirit of Felix Klein, propose to derive different inductive biases and network architectures implementing them from first principles of symmetry and invariance. (Bronstein et al., 2021, p. 2)\n\n化学・生物・物理はいずれも対象の対称性をしっかり扱う理論を持っている．これらの分野に深層学習が広く取り入れられつつある今，深層学習の分野も対称性を第一原理として整理される脱皮が待たれているのである．\n\n3.6.1 幾何学と解析学は双対である\nMikhael Gromov によると，空間 \\(X\\) の解析学とは \\(X\\) 上の関数の研究で，\\(X\\) の幾何学とは \\(X\\) への関数の研究である (深谷賢治, 1997, p. 11)．\n\n\n\nGromov による幾何と解析の解釈\n\n\n同変性と共変性は，データ集合 \\(X\\) と群作用 \\(\\rho:G\\times X\\to X\\) との組 \\((X,\\rho)\\) を取り扱うから，確かに上述の定義に適っている．\n\n\n3.6.2 20 世紀の数学の方向\n(Gromov, 2001)\n\n\n3.6.3 共変性と同変性\n\n\n\n\n\n\n定義 (invariance, equivariance)19\n\n\n\n\\(X,Y\\) を集合，\\(G\\) を群で \\(X,Y\\) に左から作用するとする．20 関数 \\(\\varphi:X\\to Y\\) が\n\n\\(G\\) に関して 不変 であるとは，任意の \\(g\\in G\\) と \\(x\\in X\\) について \\[\n\\varphi(g\\cdot x)=\\varphi(x)\n\\] を満たすことをいう．\n\\(G\\) に関して 同変 であるとは，任意の \\(g\\in G\\) と \\(x\\in X\\) について \\[\n\\varphi(g\\cdot x)=g\\cdot\\varphi(x)\n\\] を満たすことをいう．\n\n\n\n物体認識 節 3.2 は不変的で，画像分割 節 3.3 は同変的な問題である．\nただし，不変性は，\\(G\\) の \\(Y\\) への作用が自明である場合の同変性と見れるため，同変性の方が一般的な概念であることに注意．\n\n\n3.6.4 G-CNN (T. Cohen & Welling, 2016)\n一般の群変換に対して，これを帰納バイアスとして取り入れる CNN である Group CNN (T. Cohen & Welling, 2016) が提案されており，医療画像解析での応用 (Lafarge et al., 2021) もなされている．\n\n\n3.6.5 Steerable CNN (T. S. Cohen & Welling, 2017)\nSteerable CNN ではチャンネルの間での対称性を取り入れることができる．\n(Weiler et al., 2018) により最先端の性能が発揮されている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#footnotes",
    "href": "posts/2024/Kernels/Deep.html#footnotes",
    "title": "数学者のための深層学習１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(MacKay, 2003, p. 506) など．↩︎\n入力層と中間層と出力層の３層のみからなるものモデルを単純パーセプトロンと呼ぶが，それだけでなく，多層のものや，フィードバック結合のあるものも含む，一般的な形で考えていた．↩︎\nオートエンコーダーを導入し，中間層で表現学習がなされること（最も幅の狭い中間層の活性化を通じてコンパクトに表現する，など）と，モーメンタムが学習を加速することを示している (Rumelhart et al., 1987, p. 330), (Schmidhuber, 2015)．「バックプロパゲーションがこの流行の起爆剤となったとさえ言える」 (甘利俊一, 1989, p. 144)．↩︎\n(Schmidhuber, 2015), (Bishop & Bishop, 2024), (麻生英樹 et al., 2015) を参考．↩︎\n(Schmidhuber, 2015, p. 90) など．↩︎\n教師なしの競合学習を試みたのみであった (麻生英樹 et al., 2015, p. 21)．↩︎\n三層の場合は，入力層を符号化器，出力層を複合化器とも言う．さらに中間層の幅が小さい場合，その形から hourgalss-type neural network とも呼ばれる (麻生英樹 et al., 2015, p. 91)．↩︎\n当時の CNN で深かったものには，19 層の CNN である VGGNet (Simonyan & Zisserman, 2015) がある．↩︎\n(MacKay, 2003, p. 505) など．↩︎\n(MacKay, 2003, p. 468) の導入も参照．↩︎\n(MacKay, 2003, p. 470) など．↩︎\n(MacKay, 2003, p. 503) 第42章，(麻生英樹 et al., 2015, p. 11) など．↩︎\n「このため，物理学者はこの種のモデルのことを Hopfield モデルと呼ぶが，この命名は適切とは思えない」(甘利俊一, 1989, p. 97) としている．↩︎\n“The popularity of the Boltzmann machine was primarily driven by its similarity to an activation model for neurons.” (Koller & Friedman, 2009, p. 126)．↩︎\n(Bishop & Bishop, 2024, p. 254) など．↩︎\n(Bishop & Bishop, 2024, p. 255) など．↩︎\n(Wolpert, 1996) は最適化の文脈での定理を示した．統計的機械学習の文脈での No free lunch theorem は，“Any classifier with finite sample error guarantees necessarily needs inductive bias: structural assumptions on either the function class or the sampling distribution.” と説明できる．(Shalev-Shwartz & Ben-David, 2014, p. 37) 定理5.1 も参照．↩︎\ntangent propagation (Simard et al., 1991) などがその例である．2の例とも見れる．↩︎\n(福水健次, 2024), (Bronstein et al., 2021, pp. 15–16) など．または nLab も参照．↩︎\nこのような集合 \\(X,Y\\) を \\(G\\)-集合 という．同変性 は \\(G\\)-集合の射と見れる．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html",
    "href": "posts/2024/Kernels/Deep5.html",
    "title": "数学者のための深層学習５",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#導入",
    "href": "posts/2024/Kernels/Deep5.html#導入",
    "title": "数学者のための深層学習５",
    "section": "1 導入",
    "text": "1 導入\n(Luo, 2022)\n\n1.1 歴史\n拡散モデルによる画像生成は，初め (Sohl-Dickstein et al., 2015) で提案された．1\nこれをスコアマッチングの手法 (Hyvärinen, 2005), (Vincent, 2011) と組み合わせたのが NCSN (Noise Conditioned Score Network) (Song & Ermon, 2020) や DDPM (Denoising Diffusion Probabilistic Model) (Ho et al., 2020) である．\nこの２つの手法は，ノイズスケジュールが異なるのみで本質的に同じ枠組みであるとみなせる (Huang et al., 2021)．\n\n\n1.2 拡散モデルの例\nADM (Ablated Diffusion Model) (Dhariwal & Nichol, 2021) は ImageNet データの判別において当時の最先端であった BigGAN (Brock et al., 2019) の性能を凌駕した．\nOpenAI の GLIDE (Guided Language to Image Diffusion for Generation and Editing) (Nichol et al., 2022) は，CLIP という画像符号化器と組み合わされた，テキスト誘導付き拡散モデルである．\nGoogle も Imagen (Saharia et al., 2022) という拡散モデルを開発している．\n(Yang et al., 2023) は動画生成に応用している．\n\n\n1.3 アイデア\nGAN，VAE，正規化流 などの生成モデルでは，潜在空間 \\(\\mathcal{Z}\\) からデータの空間 \\(\\mathcal{X}\\) への確率核を深層ニューラルネットワークで学習するのであった．\nニューラルネットワークの表現力は十分高いので，\\(\\mathcal{Z}\\) 上の事前分布は典型的には正規分布が用いられる．\n拡散モデルも，全くこの枠組みから逸脱するものではない．\nまず訓練データを，完全な Gauss 分布になるような変換を行う．これを複数段階に分けて提示し，この逆過程を学習する．画像においては，U-Net (Ronneberger et al., 2015) が典型的に用いられる．\n並列化が容易であり，スケーラブルな手法であるため，トランスフォーマーと組み合わせて画像と動画の生成に使われる．\nVAE や GAN と違い，１つのニューラルネットワークしか用いないため，学習が安定しやすい．\n一方で，生成時には逆変換を何度も繰り返す必要があるため，計算量が大きい．これを回避するために，生成を VAE 内の潜在空間で行うものを 潜在拡散モデル (latent diffusion model) (Rombach et al., 2022) という．これが Stable Diffusion の元となっている．\n\n\n1.4 トランスフォーマーとの邂逅\n潜在拡散モデル で U-Net (Ronneberger et al., 2015) を用いていたところをトランスフォーマーに置換した 拡散トランスフォーマー (DiT: Diffusion Transformer) (Peebles & Xie, 2023) が発表された．\nその後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) (Ma et al., 2024) が発表された．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#前向き方向符号化器",
    "href": "posts/2024/Kernels/Deep5.html#前向き方向符号化器",
    "title": "数学者のための深層学習５",
    "section": "2 前向き方向符号化器",
    "text": "2 前向き方向符号化器"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#後ろ向き復号化器",
    "href": "posts/2024/Kernels/Deep5.html#後ろ向き復号化器",
    "title": "数学者のための深層学習５",
    "section": "3 後ろ向き復号化器",
    "text": "3 後ろ向き復号化器"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#スコアマッチング",
    "href": "posts/2024/Kernels/Deep5.html#スコアマッチング",
    "title": "数学者のための深層学習５",
    "section": "4 スコアマッチング",
    "text": "4 スコアマッチング\n(Song, Durkan, et al., 2021)"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#誘導付き拡散モデル",
    "href": "posts/2024/Kernels/Deep5.html#誘導付き拡散モデル",
    "title": "数学者のための深層学習５",
    "section": "5 誘導付き拡散モデル",
    "text": "5 誘導付き拡散モデル"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#確率微分方程式との関係",
    "href": "posts/2024/Kernels/Deep5.html#確率微分方程式との関係",
    "title": "数学者のための深層学習５",
    "section": "6 確率微分方程式との関係",
    "text": "6 確率微分方程式との関係\n連続時間極限を取ることで，拡散過程が現れる (Tzen & Raginsky, 2019), (Song, Sohl-Dickstein, et al., 2021)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep5.html#footnotes",
    "href": "posts/2024/Kernels/Deep5.html#footnotes",
    "title": "数学者のための深層学習５",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVideoGPT の論文 (Yan et al., 2021) や，DALL-E2 の論文 (Ramesh et al., 2022)，GLIDE の論文 (Nichol et al., 2022) でも引用されている．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep6.html",
    "href": "posts/2024/Kernels/Deep6.html",
    "title": "数学者のための深層学習６",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n(Kobyzev et al., 2021), (Papamakarios et al., 2021)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep6.html#導入",
    "href": "posts/2024/Kernels/Deep6.html#導入",
    "title": "数学者のための深層学習６",
    "section": "1 導入",
    "text": "1 導入\nGAN，VAE，拡散モデル など，深層生成モデルは，潜在空間 \\(\\mathcal{Z}\\) 上の基底分布 \\(p_z\\) を，パラメータ \\(w\\in\\mathcal{W}\\) を持つ深層ニューラルネットによる変換 \\(f:\\mathcal{Z}\\times\\mathcal{W}\\to\\mathcal{X}\\) を通じて，押し出し \\(\\{f(w)_*p_z\\}_{w\\in\\mathcal{W}}\\) により \\(\\mathcal{X}\\) 上の分布をモデリングする．\nこれらのモデル \\(\\{f(w)_*p_z\\}_{w\\in\\mathcal{W}}\\) の尤度は解析的に表示できない．そこで，GAN (Goodfellow et al., 2014) は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものであり，VAE (Kingma & Welling, 2014) は変分下界を通じて尤度を近似するというものであった．\n正則化流 (normalizing flow / flow-based models) では，拡散モデル に似て，「逆変換」を利用することを考える．\nすなわち，\\(\\{f_w\\}\\subset\\mathcal{L}(\\mathcal{Z},\\mathcal{X})\\) が可逆であるように設計するのである．逆関数を \\(g_w:=f_w^{-1}\\) と表すと，\\(p_x(-|w)\\) は \\(p_z\\) の \\(g_w\\) による引き戻しの関係になっているから，変数変換 を通じて， \\[\np_x(x|w)=p_z(g_w(x))\\lvert\\det J_{g_w}(x)\\rvert\\;\\;\\text{a.s.}\n\\] が成立する．\nすると， \\[\n\\log p(x|w)=\\log p_z(g_w(x))+\\log\\lvert\\det J_{g_w}(x)\\rvert\n\\] を通じて，尤度の評価とパラメータの最尤推定が可能である．\n従って，可逆なニューラルネットワーク \\(\\{f_w\\}\\subset\\mathcal{L}(\\mathcal{Z},\\mathcal{X})\\) を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．\nこのとき，行列式 \\(\\det:\\mathrm{GL}_D(\\mathbb{R})\\to\\mathbb{R}^\\times\\) は群準同型であるから，\\(g_w\\) のヤコビアンは，各層のヤコビアンの積として得られる．\nこの条件はたしかにモデルに仮定を置いている（\\(p_z\\) は典型的に正規で，\\(f_w\\) は可逆である）．しかしそれでも，深層ニューラルネットワーク \\(\\{f_w\\}\\) の表現力は十分高いため，モデリングにも使うことは出来るだろうが，どちらかというと学習されたサンプラーのような立ち位置に理解しやすい (Gao et al., 2020)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep6.html#カップリング流",
    "href": "posts/2024/Kernels/Deep6.html#カップリング流",
    "title": "数学者のための深層学習６",
    "section": "2 カップリング流",
    "text": "2 カップリング流"
  },
  {
    "objectID": "posts/2024/Kernels/Deep6.html#自己回帰流",
    "href": "posts/2024/Kernels/Deep6.html#自己回帰流",
    "title": "数学者のための深層学習６",
    "section": "3 自己回帰流",
    "text": "3 自己回帰流"
  },
  {
    "objectID": "posts/2024/Kernels/Deep6.html#連続流",
    "href": "posts/2024/Kernels/Deep6.html#連続流",
    "title": "数学者のための深層学習６",
    "section": "4 連続流",
    "text": "4 連続流\n\n4.1 フローマッチング\nフローマッチング (Lipman et al., 2023), rectified flow (Liu et al., 2023)"
  },
  {
    "objectID": "posts/2024/Kernels/Deep6.html#確率的補間",
    "href": "posts/2024/Kernels/Deep6.html#確率的補間",
    "title": "数学者のための深層学習６",
    "section": "5 確率的補間",
    "text": "5 確率的補間\n(Albergo & Vanden-Eijnden, 2023) により提案されたもので，SiT (Scalable Interpolant Transformer) (Ma et al., 2024) でも用いられている技術である．\n(Albergo et al., 2023)"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html",
    "href": "posts/2024/Kernels/Deep3.html",
    "title": "数学者のための深層学習３",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#導入",
    "href": "posts/2024/Kernels/Deep3.html#導入",
    "title": "数学者のための深層学習３",
    "section": "1.1 導入",
    "text": "1.1 導入\nGAN 以前の深層生成モデルは，学習の難しさから，データ生成分布にパラメトリックな仮定をおき，その中で 最尤推定 を行うことが一般的であった．深層 Boltzmann マシン (Salakhutdinov & Hinton, 2009) もその例である．\n複雑なモデルで尤度を解析的に計算することは困難である．そのために，MCMC によるサンプリングによりこれを回避することを考え，その Markov 連鎖の遷移核を学習するという生成確率的ネットワーク (GSN: Generative Stochastic Network) などのアプローチ (Bengio et al., 2014) も提案されていた．\nGAN (Generative Adversarial Network) は，このような中で (Goodfellow et al., 2014) によって提案された深層生成モデルである．GAN も尤度の評価を必要としないが，MCMC などのサンプリング手法も用いず，ただ誤差逆伝播法のみによって学習が可能である．\n同時の深層学習は，ImageNet コンペティションにおいて大成功を収めた AlexNet (Krizhevsky et al., 2012) など，主に識別のタスクにおいて大きな成功を収めていたが，生成モデルにおいては芳しくなかった．\n主な障壁は\n\n分布の近似が難しいこと\n区分的線型な活性化関数を用いても勾配を通じた学習が難しいこと\n\nの２点であったが，GAN はこの２つの問題を回避すべく提案された．\n生成モデル \\(G\\) に対して，判別モデル \\(D\\) を対置し，加えて \\((G,D)\\) をセットで誤差逆伝播法とドロップアウト法 (Hinton et al., 2012)（当時深層識別モデルを最も成功させていた学習法）により学習可能にしたのである．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#枠組み",
    "href": "posts/2024/Kernels/Deep3.html#枠組み",
    "title": "数学者のための深層学習３",
    "section": "1.2 枠組み",
    "text": "1.2 枠組み\nデータの空間を \\(x\\in\\mathcal{X}\\) とし，潜在変数の値域 \\(\\mathcal{Z}\\) とその上の確率測度 \\(P_z\\in\\mathcal{P}(\\mathcal{Z})\\)，そして深層ニューラルネットワークのパラメータ空間 \\(\\Theta_g\\) を用意して，生成モデルを写像 \\(G:\\mathcal{Z}\\times\\Theta_g\\to\\mathcal{X}\\) とする．\n生成モデル \\(G\\) は押し出しによりモデル \\(\\{G(-,\\theta_g)_*P_z\\}_{\\theta_g\\in\\Theta_g}\\) を定める．\nこのモデルの密度（尤度）の評価を回避するために，これに判別モデル \\(D\\) を対置する．これは，パラメータ \\(\\theta_d\\in\\Theta_d\\) を通じて学習される写像 \\(D:\\mathcal{X}\\times\\Theta_d\\to[0,1]\\) とし，あるデータ \\(x\\in\\mathcal{X}\\) を観測した際に，これが \\(G\\) から生成されたものではなく，実際の訓練データである確率を \\(D(x)\\) によって近似することを目指す．\nこの組 \\((G,D)\\) に対して， \\[\nV(D,G):=\\operatorname{E}[\\log D(X)]+\\operatorname{E}[\\log(1-D(G(Z))]\n\\] \\[\nX\\sim P_{\\text{data}},\\quad Z\\sim P_z\n\\] を目的関数とし， \\[\n\\min_{G\\in\\mathrm{Hom}_\\mathrm{Mark}(\\mathcal{Z}\\times\\mathcal{G}_g,\\mathcal{X})}\\max_{D\\in\\mathcal{L}(\\mathcal{X};[0,1])}V(D,G)\n\\tag{1}\\] を解く，ミニマックスゲームを考える．1"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#理論",
    "href": "posts/2024/Kernels/Deep3.html#理論",
    "title": "数学者のための深層学習３",
    "section": "1.3 理論",
    "text": "1.3 理論\n\\(G\\) と \\(D\\) が表現するモデルが十分に大きいとき，すなわち \\(\\Theta_g,\\Theta_d\\) が十分に大きく，殆どノンパラメトリックモデルであるとみなせる場合には，学習基準 式 1 は真の生成分布 \\(P_{\\text{data}}\\) に収束するアルゴリズムを与える．\nこのことを示すには，\\(P_{\\text{data}}\\) が，式 1 の大域的最適解であることを示せば良い．\n\n\n\n\n\n\n定義 (Jensen-Shannon divergence)\n\n\n\n確率測度 \\(P,Q\\in\\mathcal{P}(\\mathcal{X})\\) に対して，\n\n\\[\n\\mathop{\\mathrm{KL}}(P,Q):=\\begin{cases}\n\\int_\\mathcal{X}\\log\\left(\\frac{d P}{d Q}\\right)\\,dP&P\\ll Q,\\\\\n\\infty&\\mathrm{otherwise}.\n\\end{cases}\n\\] を Kullback-Leibler 乖離度 という．\n\\[\n\\mathop{\\mathrm{JS}}(P,Q):=\\mathop{\\mathrm{KL}}\\left(P,\\frac{P+Q}{2}\\right)+\\mathop{\\mathrm{KL}}\\left(Q,\\frac{P+Q}{2}\\right)\n\\] を Jensen-Shannon 乖離度 という．\n\nこのとき，\\(\\sqrt{\\mathop{\\mathrm{JS}}}\\) は，任意の \\(\\sigma\\)-有限測度 \\(\\mu\\in\\mathcal{M}(\\mathcal{X})\\) に関して， \\[\n\\mathcal{P}_\\mu(\\mathcal{X}):=\\left\\{P\\in\\mathcal{P}(\\mathcal{X})\\mid P\\ll\\mu\\right\\}\n\\] 上に距離を定める．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nKL 乖離度は \\(P\\ne Q\\Rightarrow\\mathop{\\mathrm{KL}}(P,Q)&gt;0\\) を満たすが，対称性も三角不等式も満たさない．そもそも，\\(\\mathbb{R}_+\\)-値とは限らず，\\(\\infty\\) を取り得る．\nJS 乖離度は， \\[\nP\\ll\\frac{P+Q}{2}\n\\] であるから，\\(\\mathcal{P}(\\mathcal{X})^2\\) 上で常に \\(\\mathbb{R}_+\\)-値であることに注意．\n以降，\\(\\sqrt{\\mathop{\\mathrm{JS}}}\\) が距離であることを示す．\n\n\\(P=Q\\) のとき \\(\\mathop{\\mathrm{JS}}(P,Q)=0\\) であり，\\(P\\ne Q\\) のとき， \\[\nP\\ne\\frac{P+Q}{2}\n\\] であるから，\\(\\mathop{\\mathrm{JS}}(P,Q)&gt;0\\) である．\n対称性も直ちに従う．\nあとは三角不等式を示せば良いが，任意の \\(P,Q\\in\\mathcal{P}_\\mu(\\mathcal{X})\\) に関して，密度を \\[\np:=\\frac{d P}{d \\mu},\\quad q:=\\frac{d Q}{d \\mu}\n\\] と表すと， \\[\n\\sqrt{\\mathop{\\mathrm{JS}}(P,Q)}=\\left\\|\\sqrt{L(p,q)}\\right\\|_{L^2(\\mu)}\n\\] であることより，次の補題と \\(\\|-\\|_{L^2(\\mu)}\\) の三角不等式より従う．\n\n\n\n\n\n\n\n\n\n\n補題 (Endres & Schindelin, 2003, p. 1859)\n\n\n\n非負実数 \\(p,q\\in\\mathbb{R}_+\\) について， \\[\nL(p,q):=p\\log\\frac{2p}{p+q}+q\\log\\frac{2q}{p+q}\n\\] で定まる関数 \\(L:\\mathbb{R}_+^2\\to\\mathbb{R}_+\\) は，任意の \\(r\\in\\mathbb{R}_+\\) について， \\[\n\\sqrt{L(p,q)}\\le\\sqrt{L(p,r)}+\\sqrt{L(r,q)}\n\\] を満たす．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n右辺を \\[\nf(p,q,r):=\\sqrt{L(p,r)}+\\sqrt{L(r,q)}\n\\] とおいて，\\(r\\) に関する偏導関数の符号変化を調べる． \\[\n\\begin{align*}\n    \\frac{\\partial f}{\\partial r}&=\\frac{1}{2\\sqrt{L(p,r)}}\\frac{\\partial L}{\\partial r}(p,r)+\\frac{1}{2\\sqrt{L(r,q)}}\\frac{\\partial L}{\\partial r}(r,q)\\\\\n    &=\\frac{\\log\\frac{2r}{p+r}}{2\\sqrt{L(p,r)}}+\\frac{\\log\\frac{2r}{r+q}}{2\\sqrt{L(r,q)}}\\\\\n    &=\\frac{1}{\\sqrt{r}}\\left(\\frac{\\log\\frac{2}{x+1}}{2\\sqrt{L(x,1)}}+\\frac{\\log\\frac{2}{\\beta x+1}}{2\\sqrt{L(\\beta x,1)}}\\right).\n\\end{align*}\n\\tag{2}\\] ここで，\\(x:=\\frac{p}{r},\\beta x=\\frac{q}{r}\\) とおいた． \\[\np&lt;q\\quad\\Leftrightarrow\\quad\\beta&gt;1\n\\] と仮定しても一般性は失われない．\nそこで，\\(x\\in(-1,\\infty)\\setminus\\{1\\}\\) の関数 \\[\n\\begin{align*}\n    g(x)&:=\\frac{\\log\\frac{2}{x+1}}{\\sqrt{L(x,1)}}\\\\\n    &=\\frac{\\log\\frac{2}{x+1}}{\\sqrt{x\\log\\frac{2x}{x+1}+\\log\\frac{2}{x+1}}}\n\\end{align*}\n\\] の性質を調べる．\n実は \\(g'&gt;0\\;\\mathrm{on}\\;\\mathbb{R}_+\\setminus\\{1\\}\\) であり， \\[\n\\lim_{x\\to0+}g(x)=\\sqrt{\\log 2}&gt;0\n\\] \\[\n\\lim_{x\\to\\infty}g(x)=0\n\\] \\[\n\\lim_{x\\to1\\mp}g(x)=\\pm1\n\\] と併せると，\\(g((0,1))\\subset(0,1)\\)，\\(g((1,\\infty))\\subset(-1,0)\\) である．特に \\(\\lvert g\\rvert&lt;1\\)．\n\n\n\n\n\n\n\n\n\nこれより， 式 2 は \\(x=1,\\beta\\) と，その間で１回の計３回符号変化し，\\(x\\to\\infty\\) の極限では負である．\nよって，\\(f\\) は \\(r\\) の関数として，\\(r=p\\) で極小値，\\(r\\in(p,q)\\) のどこかで極大値を取り，\\(r=q\\) で再び極小値を取る． \\[\nf(p,q,p)=f(p,q,q)=\\sqrt{L(p,q)}\n\\] であるから，結論を得る．\n\n\n\n\n\n\n\n\n\n命題\n\n\n\n\\(P_0,P_1\\in\\mathcal{P}(\\mathcal{X})\\) を確率測度で，それぞれ密度 \\(p_0,p_1\\) を持つとする．\\(X_0\\sim P_0,X_1\\sim P_1\\) とする．このとき，\n\n最大化問題 \\[\nL:=\\sup_{D\\in\\mathcal{L}(\\mathcal{X};[0,1])}\\biggr(\\operatorname{E}[\\log D(X_0)]+\\operatorname{E}[\\log(1-D(X_1))]\\biggl)\n\\] はただ一つの解 \\[\nD^*(x)=\\frac{p_0(x)}{p_0(x)+p_1(x)}\n\\] を持つ．\n\\(\\mathop{\\mathrm{JS}}(P_0,P_1)\\) は \\(L\\) と定数の差を除いて一致する．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n目的関数は \\[\n\\begin{align*}\n&\\operatorname{E}[\\log D(X_0)]+\\operatorname{E}[\\log(1-D(X_1))]\\\\\n=&\\int_\\mathcal{X}\\log D\\cdot p_0\\,d\\mu+\\int_\\mathcal{X}\\log(1-D)\\cdot p_1\\,d\\mu\\\\\n=&\\int_\\mathcal{X}\\biggr(p_0\\log D+p_1\\log(1-D)\\biggl)\\,d\\mu\n\\end{align*}\n\\] と変形できる．いま，任意の \\(a,b\\in(0,1]\\) に関して， \\[\nf(t):=a\\log t+b\\log(1-t)\\quad(t\\in(0,1))\n\\] は \\(t=\\frac{a}{a+b}\\) 上で最大値を取る．\\(a,b\\) のどちらか一方のみが \\(0\\) である場合も含めてこの主張は成り立つ．よって， \\[\nD(x)=\\frac{p_0(x)}{p_0(x)+p_1(x)}\n\\] が目的関数を最大化することが判る．\n１より，\\(L\\) の上限 \\(\\sup\\) は達成されることがわかった： \\[\n\\begin{align*}\nL&=\\operatorname{E}[\\log D^*(X_0)]+\\operatorname{E}[\\log(1-D^*(X_1))]\\\\\n&=\\int_\\mathcal{X}\\left(p_0\\log\\frac{p_0}{p_0+p_1}+p_1\\log\\frac{p_1}{p_0+p_1}\\right)\\,d\\mu\\\\\n&=\\int_\\mathcal{X}\\biggr(p_0\\log\\frac{2p_0}{p_0+p_1}+p_1\\log\\frac{2p_1}{p_0+p_1}-p_0\\log 2-p_1\\log 2\\biggl)\\,d\\mu\\\\\n&=-2\\log2+\\mathop{\\mathrm{JS}}(P_0,P_1).\n\\end{align*}\n\\]\n\n\n\n\nこれより，訓練基準 式 1 はただ一つの大域的な最適解を持ち，これは \\(P_{\\text{data}}=G_*P_z\\) かつ \\(D^*=\\frac{1}{2}\\) のときに最小値 \\(-2\\log2\\) を取るということが判る．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#アルゴリズムとその収束",
    "href": "posts/2024/Kernels/Deep3.html#アルゴリズムとその収束",
    "title": "数学者のための深層学習３",
    "section": "1.4 アルゴリズムとその収束",
    "text": "1.4 アルゴリズムとその収束\n組 \\((G,D)\\) を勾配降下法により同時に学習するには，\n\n判別器 \\(D\\) の最大化ステップ\n\nミニバッチ \\(\\{z^i\\}_{i=1}^m\\) と \\(\\{x^i\\}_{i=1}^m\\) をそれぞれ \\(P_z\\) と \\(P_{\\text{data}}\\) からサンプリングする．\n確率的勾配 \\[\nD_{\\theta_d}\\frac{1}{m}\\sum_{i=1}^m\\left(\\log D(x^i)+\\log(1-D(G(z^i)))\\right)\n\\] の増加方向にパラメータ \\(\\theta_d\\) を更新する．\n\n生成モデル \\(G\\) の最小化ステップ\n\nミニパッチ \\(\\{z^i\\}_{i=1}^m\\) を \\(P_z\\) からサンプリングする．\n確率的勾配 \\[\nD_{\\theta_g}\\sum_{i=1}^m\\log\\biggr(1-D(G(z^i))\\biggl)\n\\] の減少方向にパラメータ \\(\\theta_g\\) を更新する．\n\n\nというアルゴリズムを実行すれば良い．(Goodfellow et al., 2014 p.) の数値実験ではモーメンタム法 (Rumelhart et al., 1987, p. 330) が用いられている．\n\n\n\n\n\n\n命題 (Goodfellow et al., 2014, p. 5)\n\n\n\nこのアルゴリズムは，次の３条件が成り立つならば，\\(G_*P_z\\) は \\(P_{\\text{data}}\\) に収束する：\n\nモデル \\(G,D\\) の表現力が十分大きい．\n判別器 \\(D\\) の最大化ステップにおいて，必ず \\(\\max_{D\\in\\mathcal{L}(\\mathcal{X};[0,1])}V(D,G)\\) が達成される．\n生成モデル \\(G\\) の最大化ステップにおいても，必ず \\(V(D,G)\\) が改善される．\n\n\n\n実際は，\\(G\\) はパラメトリックモデル \\(\\{G_*P_z(\\theta,-)\\}_{\\theta\\in\\Theta_g}\\) であるから，その分の誤差は残ることになる．\nまた，\\(D\\) が最適化されていない状況で \\(G\\) が学習されすぎると，多くの \\(z\\in\\mathcal{Z}\\) の値を \\(D\\) が不得意な判別点 \\(x\\in\\mathcal{X}\\) に対応させすぎてしまうことがあり得る．\n\\(P_{\\text{data}}\\) が強い多峰性を持つ場合でも効率よく学習することができる．これは同じ確率分布からのサンプリング手法として，MCMC にはない美点になり得る (Goodfellow et al., 2014, p. 6)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#補遺jensen-shannon-乖離度のその他の性質",
    "href": "posts/2024/Kernels/Deep3.html#補遺jensen-shannon-乖離度のその他の性質",
    "title": "数学者のための深層学習３",
    "section": "1.5 補遺：Jensen-Shannon 乖離度のその他の性質",
    "text": "1.5 補遺：Jensen-Shannon 乖離度のその他の性質\n\n1.5.1 情報理論からの導入\n乖離度としての Jensen-Shannon 乖離度は (Lin, 1991) で最初に導入されたようである．\nが，その以前から， \\[\n\\mathop{\\mathrm{JS}}(P,Q)=2H\\left(\\frac{P+Q}{2}\\right)-H(P)-H(Q)\n\\] という関係を通じて，(Rao, 1982, p. 25) などは右辺を Jensen 差分 (difference) と呼んでいたようである．(Rao, 1987, p. 222) は，\\(H\\) が Shannon のエントロピーではなくとも，有用な性質を持つことを情報幾何学の立場から議論している．\n\n\n1.5.2 JS 乖離度が定める距離\n\\[\n\\biggr(\\mathop{\\mathrm{JS}}(P,Q)\\biggl)^\\alpha\n\\] が \\(\\alpha=\\frac{1}{2}\\) において距離をなすことを示したが，実は一般の \\(\\alpha\\in(0,1/2]\\) に関して距離をなす (Osán et al., 2018)．\n\n\n1.5.3 変分問題としての特徴付け\n\n\n\n\n\n\n命題 (Nielsen, 2021, p. 6)\n\n\n\n任意の \\(P,Q\\in\\mathcal{P}_\\mu(\\mathcal{X})\\) について，\n\\[\n\\mathop{\\mathrm{JS}}(P,Q)=\\min_{R\\in\\mathcal{P}_\\mu(\\mathcal{X})}\\left\\{\\mathop{\\mathrm{KL}}(P,R)+\\mathop{\\mathrm{KL}}(Q,R)\\right\\}\n\\]\n\n\n\n\n1.5.4 有界な距離である\n\n\n\n\n\n\n命題 (Endres & Schindelin, 2003, p. 1859)\n\n\n\n\\(\\mathop{\\mathrm{JS}}:\\mathcal{P}_\\mu(\\mathcal{X})^2\\to\\mathbb{R}_+\\) は最大値 \\(\\sqrt{2\\log 2}\\) を持つ．\n\n\n\n\n1.5.5 \\(\\chi^2\\)-距離に漸近する (Endres & Schindelin, 2003, p. 1859)\n\n\n1.5.6 \\(f\\)-乖離度の例である\n\\(f\\)-乖離度の考え方は (Rényi, 1961, p. 561) で導入された．他，(Csiszár, 1963), (Morimoto, 1963), (Ali & Silvey, 1966) なども独立に導入している．\n\n\n\n\n\n\n定義 (\\(f\\)-divergence)\n\n\n\n\\(P\\ll Q\\) とする．凸関数 \\(f:\\mathbb{R}_+\\to\\mathbb{R}\\) に対して，\n\\[\nD_f(P,Q):=\\int_\\mathcal{X}f\\left(\\frac{d P}{d Q}\\right)\\,dQ\n\\] を \\(f\\)-乖離度 という．\n\n\nKL-乖離度は \\[\nf(x)=x\\log x\n\\] について，JS-乖離度は \\[\nf(x)=x\\log\\frac{2x}{x+1}+\\log\\frac{2}{x+1}\n\\] についての \\(f\\)-乖離度である．\n全変動ノルムも \\[\nf(x)=\\lvert x-1\\rvert\n\\] に関する \\(f\\)-乖離度である．\nさらには，\\(\\alpha\\)-乖離度 も \\(f\\)-乖離度の例である．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#f-gan",
    "href": "posts/2024/Kernels/Deep3.html#f-gan",
    "title": "数学者のための深層学習３",
    "section": "2.1 \\(f\\)-GAN",
    "text": "2.1 \\(f\\)-GAN\nJS-乖離度に限らず一般の \\(f\\)-乖離度 節 1.5.6 に関して，GAN が構成できる (Nowozin et al., 2016)．\nこの一般化により，GAN の枠組みの本質は凸解析に基づくものであることが明らかになる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#gan-の学習の問題点",
    "href": "posts/2024/Kernels/Deep3.html#gan-の学習の問題点",
    "title": "数学者のための深層学習３",
    "section": "2.2 GAN の学習の問題点",
    "text": "2.2 GAN の学習の問題点\n\nやはり多峰性に弱く，モードのうちいくつかが再現されないことがある (Mode collapse)．\n収束判定が困難である．これは学習基準が最小化ではなく均衡点を求めることにあることにも起因する．\n勾配消失が起こる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#wasserstein-gan",
    "href": "posts/2024/Kernels/Deep3.html#wasserstein-gan",
    "title": "数学者のための深層学習３",
    "section": "2.3 Wasserstein GAN",
    "text": "2.3 Wasserstein GAN\n最後の勾配消失の問題は，JS-乖離度の性質にあるとして，これを Wasserstein 距離に取り替える形で提案されたのが Wasserstein GAN である (Arjovsky et al., 2017)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#footnotes",
    "href": "posts/2024/Kernels/Deep3.html#footnotes",
    "title": "数学者のための深層学習３",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの基準にしたがって学習すると，\\(G\\) が外れすぎている際，\\(\\log(1-D(G(z)))\\) が殆ど \\(0\\) になり得る．そのような場合は，\\(\\log D(G(z))\\) の最大化を代わりに考えることで，学習が進むことがある (Goodfellow et al., 2014, p. 3)．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/GNN.html",
    "href": "posts/2024/Kernels/GNN.html",
    "title": "グラフニューラルネットワーク",
    "section": "",
    "text": "CNN の畳み込みも，Transformer の注意機構も，aggregation とみれる．\n次の基盤モデルのアーキテクチャの候補にもなる上，Bayesian にすることで小規模データの高精度な解析にも用いることができ，次世代のエキスパートシステムの最有力候補かも知れない．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理6.html",
    "href": "posts/2024/数理法務/法律家のための統計数理6.html",
    "title": "法律家のための統計数理（６）GPT 入門",
    "section": "",
    "text": "シリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理6.html#ニューラルネットワークの基礎",
    "href": "posts/2024/数理法務/法律家のための統計数理6.html#ニューラルネットワークの基礎",
    "title": "法律家のための統計数理（６）GPT 入門",
    "section": "1 ニューラルネットワークの基礎",
    "text": "1 ニューラルネットワークの基礎\n高等学校情報科「情報II」教員研修用教材 の 情報とデータサイエンス 第17章「ニューラルネットワークとその仕組み」も参照．\nまさか，この勉強会で扱った内容が，高校範囲に含まれることになるとは思わなかった．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理6.html#トランスフォーマー",
    "href": "posts/2024/数理法務/法律家のための統計数理6.html#トランスフォーマー",
    "title": "法律家のための統計数理（６）GPT 入門",
    "section": "2 トランスフォーマー",
    "text": "2 トランスフォーマー\nこちらの稿 を参照．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理__.html",
    "href": "posts/2024/数理法務/法律家のための統計数理__.html",
    "title": "法律家のための統計数理（？）数理ファイナンス入門",
    "section": "",
    "text": "シリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理__.html#大王製紙cb発行事件",
    "href": "posts/2024/数理法務/法律家のための統計数理__.html#大王製紙cb発行事件",
    "title": "法律家のための統計数理（？）数理ファイナンス入門",
    "section": "1 大王製紙CB発行事件",
    "text": "1 大王製紙CB発行事件\n\n\n\n\n\n\n大王製紙CB発行事件\n\n\n\n(東京地判, 平成30年９月20日) は\n\n東京証券取引所市場第１部上場会社 Z の取締役 Y1 〜 Y13 に対して，\nZ の株主 X が，\n株主総会決議を経ずに「特に有利な条件」で転換社債型新株予約権付社債を発行したこと（を含め計３点）が，\n会社法 429 条 1 項に基づく損害賠償責任に該当する\n\nとして訴えを起こした事件である．\n訴えは認められず，控訴も棄却された (潘阿憲, 2020), (川島いづみ, 2021)．\n\n\n\n新株予約権の公正な価値は，一般に，現在の株価，権利行使価額（転換価額），行使期間，無リスク利子率，株価変動性（ボラティリティ）等の要素をもとにオプション評価理論に基づき算出された新株予約権の発行時点における価額であると解されており，また，オプション評価理論において，新株予約権等のオプションの価値を評価する手法としては，ブラック＝ショールズ公式，二項モデル，モンテカルロ・シミュレーションといった複数の手法があるとされているが，評価に採用する要素及びその数値が同一であれば，算出される結果は基本的に等しくなるとされている。\n\n\nもっとも，証拠（甲３３，３４，丙４９の１・２）によれば，ボラティリティ（株価変動性）や無リスク利子率については，いかなる数値を採用するかにつき一定の裁量の余地があることが認められ，その意味で新株予約権の理論価値は常に機械的・一義的に定まるものではない。"
  },
  {
    "objectID": "posts/2024/Probability/Kernel.html",
    "href": "posts/2024/Probability/Kernel.html",
    "title": "確率核という概念",
    "section": "",
    "text": "確率核 \\(P\\) こそが Markov 連鎖の数学的本体である．\n可測空間 \\(E\\) 上の確率核 \\(P\\) が定まると，任意のスタート地点 \\(x\\in E\\) に対して，これを起点とする Markov 過程 \\(\\{X^x_n\\}_{n=0}^\\infty\\subset\\mathcal{L}(\\Omega)\\) が構成できる．\nここで謎の標本空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) が出てきたし，\\(X^x_n\\) は確率変数であり，取り扱いは困難である．\nMarkov 連鎖 \\(\\{X^x_n\\}_{n=0}^\\infty\\) が重要な基本語彙である理由は，\\(\\mathcal{P}(E)\\) 上の決定論的な力学系 \\(\\{(P^*)^n\\delta_x\\}_{n=0}^\\infty\\subset\\mathcal{P}(E)\\) を定めるためである．\nこれが \\(x\\in E\\) に依らずに同じ点に近づくことを 忘却性 (loss of memory)，近づく先がちゃんと確率測度になることを エルゴード性 (ergodicity) という．"
  },
  {
    "objectID": "posts/2024/Probability/Kernel.html#核の定義と性質",
    "href": "posts/2024/Probability/Kernel.html#核の定義と性質",
    "title": "確率核という概念",
    "section": "2 核の定義と性質",
    "text": "2 核の定義と性質\n\n2.1 定義\n\n\n\n\n\n\n定義 (kernel, probability kernel / Markov kernel)\n\n\n\n２つの可測空間 \\((E,\\mathcal{E}),(F,\\mathcal{F})\\) について，\n\n核 \\(T:E\\to F\\) とは，関数 \\(E\\times\\mathcal{F}\\to[0,\\infty]\\) であって次の２条件を満たすものをいう：1\n\n任意の \\(x\\in E\\) に対して，\\(T(x,-):\\mathcal{F}\\to[0,\\infty]\\) は測度を定める．2\n任意の \\(A\\in\\mathcal{F}\\) に対して，\\(T(-,A):E\\to[0,\\infty]\\) は可測関数である．\n\n更に (1) で \\(P(x,F)=1\\;(x\\in E)\\) も成り立つとき，これを Markov 核 または 確率核 という．3\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは，\\(\\sup_{x\\in E}\\|T(x,F)\\|&lt;\\infty\\) を満たすことをいう．4 すなわち，\\(E\\to M^1(F)\\) が有界な像を持つことをいう．\n\\(E\\) が確率空間でもあるとき，核は ランダム測度 に等価である．5\n\n\n\n\n\n\n\n\n\n命題 (核の作用の特徴付け)6\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間，\\(V:\\mathcal{L}(E)\\to\\mathcal{L}(E)\\) を正な線型写像とする．このとき，次は同値：\n\n\\(V\\) はある核の作用である．\n単調収束定理：任意の増加列 \\(\\{f_n\\}\\subset\\mathcal{L}(E)_+\\) に対して， \\[\nV\\left(\\lim_{n\\to\\infty}f_n\\right)=\\lim_{n\\to\\infty}Vf_n.\n\\]\n\n\n\n\n\n2.2 核の例\n核は極めて多くの重要な概念を一般化し，統一的な見方を提供してくれる．\n\n\n\n\n\n\n例（核概念の一般性）7\n\n\n\n\n\n\n超関数 \\(\\delta_x\\) に関して，\n\n\\(\\delta_xK(-)=K(x,-)\\) である．\n関数 \\(f:E\\to F\\) に対して，\\(K_f(x,-):=\\delta_{f(x)}\\) は決定論的な核を与える．\n\\(K_f:E\\to E\\) は \\(\\mathcal{L}(E)\\) に \\(f\\) の 前合成 として作用し，\\(\\mathcal{S}(E)\\) に \\(f\\) による 押し出し として作用する： \\[\n\\begin{align*}\nK_fg(x)&=\\int_F K_f(x,dy)g(y)=\\int_F\\delta_{f(x)}(dy)g(y)=g\\circ f(x),\\\\\n\\mu K_f(A)&=\\int_E\\mu(dx)K_f(x,A)=\\int_E\\mu(dx)\\delta_{f(x)}(A)\\\\\n&=\\mu(f^{-1}(A))=f_*\\mu(A).\n\\end{align*}\n\\]\n\\(E=F,f=\\mathrm{id}_E\\) と与えたとき，圏 Stoch の自己射 \\(\\mathrm{id}_E:E\\to E\\) を定める．\n\n関数同士の積も核である．関数同士の積も核である．\\(k:E\\to\\mathbb{R}\\) が \\(K(x,-):=k(x)\\delta_x\\) によって定める確率核 \\(K:E\\to E\\) は \\(Kf=kf\\) を満たす： \\[\nKf(x)=\\int_Ek(x)\\delta_x(dy)f(y)=k(x)f(x).\n\\]\n測度とは一点集合からの核 \\(\\nu:\\{*\\}\\to E\\) と考えられる．\n\n\n\n\n\n\n\n\n\n\n例（畳み込み核）8\n\n\n\n\n\n畳み込みも核である．測度 \\(\\pi\\in\\mathcal{M}(\\mathbb{R}^d)\\) に対して， \\[\nK(x,A):=(\\pi*\\delta_x)(A)=\\int_{\\mathbb{R}^d}\\delta_x(A-y)\\pi(dy)\n\\] とすると， \\[\n\\begin{align*}\nKf(x)&=\\int_{\\mathbb{R}^d}K(x,dz)f(z)=\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\delta_x(dz-y)\\pi(dy)f(z)=\\int_{\\mathbb{R}^d}f(x+y)\\pi(dy),\\\\\n\\mu K(A)&=\\int_{\\mathbb{R}^d}\\mu(dx)K(x,A)=\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\mu(dx)\\delta_x(A-y)\\pi(dy)\\\\\n&=\\int_{\\mathbb{R}^d}\\mu(A-y)\\pi(dy)=\\pi*\\mu(A).\n\\end{align*}\n\\] 実は畳み込み核は強 Feller である．すなわち， 任意の有界可測関数 \\(f\\in\\mathcal{L}_b(E)\\) に対して，\\(Kf\\in C(E)\\) は連続である．9\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n任意の \\(x_1,x_2\\in E\\) について，\\(f\\in\\mathcal{L}_b(E)\\) であるから，有界収束定理より \\[\n\\lvert Kf(x_1)-Kf(x_2)\\rvert\\le\\int_E\\lvert f(x_1+y)-f(x_2+y)\\rvert\\pi(dy)\\xrightarrow{x_2\\to x_1}0.\n\\]\n\n\n\n\n\n2.3 核の作用\n\n\n\n\n\n\n核の作用10\n\n\n\n\n\n核は写像 \\(E\\to\\mathcal{M}(F)\\) と見れるため，その全体は自然に実線型空間をなす．\n\n右作用 \\(\\mathcal{S}(E)\\times\\mathrm{Hom}_\\mathrm{Stoch}(E,F)\\to\\mathcal{S}(E)\\) を \\[(\\mu T)(A):=\\int_E\\mu(dx)T(x,A),\\qquad A\\in\\mathcal{F},\\] で定める．\n左作用 \\(\\mathrm{Hom}_\\mathrm{Stoch}(E,F)\\times\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[(Tf)(x):=\\int_FT(x,dy)f(y),\\qquad x\\in E,\\] で定める．11\n核 \\(S:F\\to G\\) に対して，合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[(T\\otimes S)(x,A\\times B):=\\int_AT(x,dy)S(y,B),\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\\] で定め，積を12 \\[(TS)(x,B):=(T\\otimes S)(x,F\\times B)=\\int_FT(x,dy)S(y,B),\\qquad(x\\in E,B\\in\\mathcal{G}),\\] で定める．\n\n\n\n\n\n\n\n\n\n\n命題（核の演算規則）13\n\n\n\n\n自然なペアリング \\((-|-):\\mathcal{S}(E)\\times\\mathcal{L}(F)\\to[-\\infty,\\infty]\\) に関して，（両辺が意味を持つ限り）随伴性が成り立つ： \\[(\\mu T|f)=(\\mu|Tf)\\]\n核は積に関して結合的である：\\((LM)N=L(MN)\\)．14\n\nまた，\\((MN)f=M(Nf)\\) も成り立つ．\\(E,F\\) が可算であるとき，\\(\\mathcal{P}(E),\\mathcal{P}(F)\\) は行ベクトルの空間となり，確率核は確率行列となる．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nFubini の定理より，次のように式変形できる： \\[\\begin{align*}\n    (\\mu|Pf)&=\\int_EPf(x)\\mu(dx)=\\int_E\\int_FP(x,dy)f(y)\\mu(dx)\\\\\n    &=\\int_F\\int_E\\mu(dx)P(x,dy)f(y)=\\int_F\\mu P(dy)f(y)=(\\mu P|f).\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n命題（作用の非拡大性）15\n\n\n\n\\(P\\) を（劣）確率核とする．\n\n左作用 \\(P:\\mathcal{L}_b(E)\\to\\mathcal{L}_b(E)\\) は非拡大写像を定める：\\(\\|Pf\\|_\\infty\\le\\|f\\|_\\infty\\)．\n右作用 \\(P^*:\\mathcal{P}(E)\\to \\mathcal{P}(F)\\) も非拡大写像を定める：\\(\\|\\mu P\\|_\\mathrm{TV}\\le\\|\\mu\\|_\\mathrm{TV}\\)．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    \\sup_{x\\in E}|Pf(x)|&=\\sup_{x\\in E}\\left|\\int_EP(x,dy)f(y)\\right|\\le\\|f\\|_\\infty\\sup_{x\\in E}\\int_EP(x,dy)=\\|f\\|_\\infty.\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2024/Probability/Kernel.html#真の見方",
    "href": "posts/2024/Probability/Kernel.html#真の見方",
    "title": "確率核という概念",
    "section": "3 真の見方",
    "text": "3 真の見方\n\\(F\\) が可分距離空間であるとき，確率核とは本質的に可測関数 \\(E\\to\\mathcal{P}(F)\\) である．\n\n3.1 核の可測性\n\n\n\n\n\n\n命題\n\n\n\n\\(F\\) を距離化可能とする．このとき，関数 \\(E\\times\\mathcal{B}(F)\\to[0,1]\\) について，(2)\\(\\Rightarrow\\)(1) が成り立ち，\\(F\\) が可分であるとき (1)\\(\\Rightarrow\\)(2) も成り立ち，２つは同値になる：\n\n\\(E\\times\\mathcal{B}(F)\\to[0,1]\\) は確率核である．\n\\(E\\to\\mathcal{P}(F)\\) は可測である．\n\n\nが成り立つとき， \\[x\\in E\\mapsto\\int_Ff(x,y)P(x,dy),\\qquad f\\in\\mathcal{L}_b(E\\times F)\\] も可測である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n16\n\n(2)\\(\\Rightarrow\\)(1)\n\n\\(\\sigma[\\{\\mathrm{ev}_B\\}_{B\\in\\mathcal{B}(F)}]\\subset\\mathcal{B}(F)\\) が成り立つから，任意の \\(B\\in\\mathcal{B}(F)\\) について合成 \\(P(-,B):E\\to\\mathcal{P}(F)\\xrightarrow{\\mathrm{ev}_B}[0,1]\\) が可測．\n\n(1)\\(\\Rightarrow\\)(2)\n\n\\(F\\) が可分に距離化可能であるとき，\\(\\mathcal{B}(F)=\\sigma[\\{\\mathrm{ev}_B\\}_{B\\in\\mathcal{B}(F)}]\\) が成り立つから，\\(E\\to\\mathcal{P}(F)\\) が可測であることは，任意の \\(B\\in\\mathcal{B}(F)\\) について合成 \\(P(-,B):E\\to\\mathcal{P}(F)\\xrightarrow{\\mathrm{ev}_B}[0,1]\\) が可測であることに同値．"
  },
  {
    "objectID": "posts/2024/Probability/Kernel.html#footnotes",
    "href": "posts/2024/Probability/Kernel.html#footnotes",
    "title": "確率核という概念",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Jacod & Shiryaev, 2003) p.65，(Kolokoltsov, 2011) 3.5節 p.110, (Klenke, 2020) 8.3節 p.204 では transition kernel，(Dellacherie & Meyer, 1988) p.1，(Revuz & Yor, 1999) 定義III.1.1.1 p.79，(Revuz, 1984) 定義1.1.1.1 p.8，(Kallenberg, 2017) p.16, (Bass, 2011) 定義19.2 p.154 では kernel と呼んでいる．↩︎\n(Revuz, 1984) 定義1.1.1.1 p.8 では符号付き測度であることを許しているが，我々はその場合は 符号付き核 と呼ぶこととしよう．↩︎\n(Crisan & Doucet, 2002) p.737 では Markov transition kernel，(Del Moral, 2004) p.9 では Markov kernel，(Kolokoltsov, 2011) 3.5節 p.110 では transition probability kernel or simply probability kernel と呼び，(Chopin & Papaspiliopoulos, 2020) 定義4.1 p.36, (Bremaud, 2020) 3.3.3節 p.135 では propability kernel，(Kulik, 2018) p.25 では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016) pp.151-152 は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie & Meyer, 1988) p.2 は Markovian kernel．(Kallenberg, 2017) p.29 と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．↩︎\n(Dellacherie & Meyer, 1988) p.2，(Kolokoltsov, 2011) 3.5節 p.110．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 も参照．↩︎\n(Revuz, 1984, p. 9) 命題1.3．↩︎\n(Dellacherie & Meyer, 1988) 7 p.4，(Revuz, 1984, p. 9)，(Revuz, 1984, p. 13) 演習1.13．↩︎\n(Revuz, 1984, p. 10)．p.36 も参照．↩︎\n(Revuz, 1984, p. 36)，(Revuz & Yor, 1999, p. 412)．↩︎\n作用は (Hairer, 2021), (Kallenberg, 2017, p. 16), (Dellacherie & Meyer, 1988, pp. 2–3) が同様に定めている．最後の文献によると，この記法は Hunt によるものだという．\\(T\\otimes S\\) の存在は (Gikhman & Skorokhod, 2004, p. 76) 定理II.4.1 で示されており，\\(\\otimes\\) を 直積，\\(\\cdot\\) を 畳み込み と呼んでいる．↩︎\nこれが \\(\\mathcal{E}\\)-可測であることは，\\(f\\) の単関数近似を考えることで示せる (Dellacherie & Meyer, 1988, p. 2)．↩︎\n(鎌谷研吾, 2021, p. 382) の呼称に一致．(Dellacherie & Meyer, 1988, p. 4) は composition，(Gikhman & Skorokhod, 2004, p. 76) 定理II.4.1 は畳み込みと呼ぶ．↩︎\n(1)は (Revuz, 1984, p. 9) と (Dellacherie & Meyer, 1988, pp. 5 p.3)，(2)は (Dellacherie & Meyer, 1988, pp. 7 p.4)．↩︎\nこれは一般の符号付き核については成り立たない (Revuz, 1984, p. 12)．↩︎\n(Revuz, 1984, p. 12)．↩︎\n(Kallenberg, 2017, p. 30) 補題1.14 では Borel 空間について示している．(Hairer, 2021) ではあらかじめ (2) を定義としている．(Del Moral, 2004, p. 7) は最初からこれによって \\(\\sigma\\)-代数を定義する．(Ambrosio et al., 2008, p. 121) も初めからこれを通じて，可分距離空間 \\(Y\\) に対する Borel 写像 \\(X\\to\\mathcal{P}(Y)\\) を定義する．(3) は (Ambrosio et al., 2008, p. 121)．↩︎"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html",
    "href": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html",
    "title": "ESRP 配信マニュアル",
    "section": "",
    "text": "ESRP 公式 YouTube チャンネル\n\n\nESRP は 2/25/2024 時点で 過去８回のシンポジウムを開催しており，うち７回で YouTube での同時配信をしました．\n\n\n事前準備から当日の配信まで，次の３ステップからなります：\n\nOBS Studio のインストールとセットアップ\nYouTube アカウントとの連携\n当日準備と配信の実行\n\n本記事では，これらを順番に説明していきます．"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#はじめに",
    "href": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#はじめに",
    "title": "ESRP 配信マニュアル",
    "section": "",
    "text": "ESRP 公式 YouTube チャンネル\n\n\nESRP は 2/25/2024 時点で 過去８回のシンポジウムを開催しており，うち７回で YouTube での同時配信をしました．\n\n\n事前準備から当日の配信まで，次の３ステップからなります：\n\nOBS Studio のインストールとセットアップ\nYouTube アカウントとの連携\n当日準備と配信の実行\n\n本記事では，これらを順番に説明していきます．"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#sec-1",
    "href": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#sec-1",
    "title": "ESRP 配信マニュアル",
    "section": "1 OBS Studio のインストールとセットアップ",
    "text": "1 OBS Studio のインストールとセットアップ\nOBS Studio は無料の配信ソフトウェアです．これを用いるのが最も簡単です．最も有名なソフトウェアでもあるので，Google 検索をすることで多くの情報が得られます．\n\n1.1 インストール方法\nこちらのサイト からダウンロードできます．\n\n\n1.2 セットアップ\nENEOS ホールからの配信では，\n\nENEOS ホール会場に備え付けられているカメラの映像を，HDMI を通じて取り込む．\nENEOS ホールのマイクの音声を，USB を通じて取り込む．\n\n必要があります．このように，配信に載せたい映像と音声をひとまとまりにしたものを OBS Studio では シーン と呼びます．\n\n1.2.1 シーンの作成\n\n\n\nシーンの作成方法\n\n\n\n\n1.2.2 カメラ映像の取り込み\n\n\n\nカメラ映像の取り込み方法\n\n\n\n\n1.2.3 マイク音声の取り込み\n\n\n\nマイク音声の取り込み方法"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#sec-2",
    "href": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#sec-2",
    "title": "ESRP 配信マニュアル",
    "section": "2 YouTube アカウントとの連携",
    "text": "2 YouTube アカウントとの連携\nGoogle アカウント側と，OBS Studio 側の両方での設定が必要です．\n\n2.1 Google アカウント側での作成\nESRP の YouTube チャンネルは ブランドアカウント という形態で運用されています．\nこれは，何か ESRP 専用の Google アカウントが存在して，そこにログインすればアクセスできるというわけではなく，既存の Google アカウントに紐付ける形でアクセス可能になる YouTube チャンネルだ，ということです．\nそのため，あなたの既存の Google アカウントに連携する必要がありますので，司馬 まで，Google アカウントのメールアドレスをご通知ください．\n\n\n2.2 OBS Studio 側での設定\nOBS Studio で「設定」タブを開き，「配信」を選択します．\n\n\n\nOBS Studio の設定画面で「配信」を開いた際の画面\n\n\nここで「アカウント接続」を押すと，Google アカウントのログイン画面へと移行します．\n第 2.1 節でのブランドアカウントとの連携が成功していれば，次のような画面に辿り着くはずです．\n\n\n\nOBS Studio で Google アカウントとの連携をする際の画面"
  },
  {
    "objectID": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#sec-3",
    "href": "posts/2024/Lifestyle/ESRP_Manual/ESRP配信マニュアル.html#sec-3",
    "title": "ESRP 配信マニュアル",
    "section": "3 当日準備と配信の実行",
    "text": "3 当日準備と配信の実行\n\n3.1 当日の持ち物\n\n3.1.1 キャプチャーボード\nESRP が所有しているキャプチャーボード があります．\n通常の方法で PC と HDMI をしても，PC からの映像の出力が出来るのみで，外部の映像情報を PC に取り込むことは出来ません．それにはキャプチャーボードという変換器が必要です．\nキャプチャーボードを通じて，USB Type-A で PC に接続することで，外部の映像情報を PC に取り込むことが出来ます．\n\n\n3.1.2 配信用 PC\n次のものを受け取ることが出来る必要があります：\n\nキャプチャーボードからの映像情報の入った UBS Type-A 端子\n会場のマイクからの音声情報の入った UBS Type-A 端子\n\nそのため，２つの USB Type-A 端子が必要で，これがない場合は UBS ハブ などが必要になるでしょう．\n\n\n\n3.2 配信の実行\nYouTube Studio 側での設定と，OBS Studio 側での読み込みとの，２つのステップが必要です．\n\n3.2.1 配信の枠を立てる\nYouTube Studio にログインし，「コンテンツ (Content)」から「ライブ配信 (Live)」を選択します．\n右上の CREATE から，Go live を選択します．\n\n\n\nYouTube Studio での配信の枠を立てる画面\n\n\nすると，配信設定画面に移りますので，ここで配信のタイトル・説明文・サムネイルを最新のものに変更し，配信開始時刻を設定します．\n\n\n\nYouTube Studio での配信の設定画面\n\n\n\n\n3.2.2 OBS Studio での配信の実行\n第 3.2.1 節で正しく設定できていれば，次のように OBS Studio からも確認できるはずです：\n\n\n\nOBS Studio での配信の選択画面\n\n\n最後に「配信を選択して配信開始」を押すと，配信が開始されます．\n\n\n3.2.3 配信中\n配信中に気をつけるべきことは主に次の２つです：\n\n\n\n\n\n\n１．「音声ミキサー」のパネルで，音量バーが平均的に黄色のレンジに入っていることを確認する．\n\n\n\n\n\n緑色だと（他の配信と比べて）音声が小さすぎる傾向にあります．別にそれで問題はないのですが，代わりとなる良い目安がないので，私はわかりやすいので黄色のレンジに入るように調整しています．\n一方で，赤色のレンジに入る期間が長すぎると，音が割れてしまうことがあります．\n\n\n\n\n\n\n\n\n\n２．スマホなどで実際に配信を確認して，画面の色や音声に問題がないか確認する．\n\n\n\n\n\n自分の目で確認することがとても大切です．\nこれを怠ったために，音声のノイズキャンセルが強すぎて，音が飛び飛びでしか聞こえず，とても配信が見れたものでないことがあります．\nせっかく配信をしているのに，あとから見返すことも出来ず，飛び飛びの音声だと意味も取り出せなくなってしまいます．\nとても勿体無いですから，是非最後の１ステップとして，配信を自分の目で一度は確認することをしてみてください．"
  },
  {
    "objectID": "posts/2024/Particles/resampling.html",
    "href": "posts/2024/Particles/resampling.html",
    "title": "粒子フィルターの実装：リサンプリング編",
    "section": "",
    "text": "Python を用いた粒子フィルター全体の実装は 粒子フィルターの実装 | Particles Package 参照．"
  },
  {
    "objectID": "posts/2024/Particles/resampling.html#リサンプリング法の形式化",
    "href": "posts/2024/Particles/resampling.html#リサンプリング法の形式化",
    "title": "粒子フィルターの実装：リサンプリング編",
    "section": "1 リサンプリング法の形式化",
    "text": "1 リサンプリング法の形式化\nリサンプリング法を調べるにあたって，これを数学的な枠組みに落とし込む必要があり，これが意外と一筋縄ではいかない．\n\n\n\n\n\n\n問題設定と記法\n\n\n\n\n状態空間を距離空間 \\(E\\) とし，この上の Markov 過程 \\(\\{X_n\\}_{n=0}^\\infty\\subset\\mathcal{L}(\\Omega;E)\\) を既知とし，参照過程 と呼ぶ．\nこの Markov 過程の，時刻 \\(t\\in\\mathbb{N}\\) までの見本道の分布を \\(\\mathbb{M}_t(dx_{0:t})\\in\\mathcal{P}(E^{t+1})\\) と表す．\n非負値な有界可測関数の列 \\(\\{G_n\\}_{n=0}^t\\subset \\mathcal{L}_b(E^2)_+\\) に関して， \\[\n\\mathbb{Q}_{t}(dx_{0:t}):=\\frac{1}{L_t}G_0(x_0)\\left(\\prod_{s=1}^tG_s(x_{s-1},x_s)\\right)\\mathbb{M}_t(dx_{0:t})\n\\] という形で表現される分布 \\(\\mathbb{Q}_t(dx_{0:t})\\in\\mathcal{P}(E^{t+1})\\) を，ポテンシャル関数 \\((G_n)_{n=0}^t\\) に関する Feynman-Kac 測度 という．1\n積測度の確率核による分解 \\[\n\\mathbb{M}_t(dx_{0:t})=\\mathbb{M}_0(dx_0)M_1(x_0,dx_1)\\cdots M_t(x_{t-1},dx_t)\n\\] を用いる．黒板ボールド \\(\\mathbb{M}_0,\\cdots,\\mathbb{M}_t\\) は積確率測度で，イタリック体 \\(M_1,\\cdots,M_t\\) は確率核を表す．2\n\n\n\n\n\n\n\n\n\n例（フィルタリング問題と Feynman-Kac 測度）\n\n\n\n\n\n\\(\\{X_n\\}\\) をシステムダイナミクス，\\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を観測モデル \\[\n\\operatorname{P}[Y_n\\in A|X_n=x]=\\int_Af_n(y|x)\\,dy\n\\] \\[\nA\\in\\mathcal{B}(\\mathbb{R}^d),\\quad x\\in E,\n\\] に従った観測の過程とした，状態空間モデル \\((X_n,Y_n)\\) におけるフィルタリング問題とは，\\(y_1,\\cdots,y_n,\\cdots\\in\\mathbb{R}^d\\) を観測として \\[\nG_n(x_{n-1},x_n):=f_n(y_n|x_n)\n\\] と定めた場合の Feynman-Kac 測度 \\(\\mathbb{Q}_t(dx_{0:t})\\) がフィルタリング分布に他ならないから，これを時刻 \\(t\\in\\mathbb{N}\\) 毎に観測 \\(y_1,\\cdots,y_t\\) から逐次推定する問題と解釈できる．3\n\n\n\n\n1.1 リサンプリングとは\nFeynman-Kac 測度を \\(\\mathbb{Q}_0,\\mathbb{Q}_1,\\cdots\\) と逐次的に推定していく問題を考える．\n\n時刻 \\(t=0\\) にて，提案分布を \\(\\mathbb{M}_0\\)，目標分布を \\(\\mathbb{Q}_0(dx_0)\\propto G_0(x_0)\\mathbb{M}_0(dx_0)\\) とした重点サンプリングを行って，\\(\\mathbb{Q}_0\\) の重点サンプリング推定量 \\[\n\\mathbb{Q}_0^N(dx_0):=\\sum_{i=1}^NW_0^{i}\\delta_{X_0^{i}}(dx_0),\n\\] \\[\nX_0^i\\overset{\\text{iid}}{\\sim}\\mathbb{M}_0,\\quad W^i_0:=\\frac{G_0(X_0^i)}{\\sum_{j=1}^NG_0(X_0^j)}\n\\] を得る．\n時刻 \\(t=1\\) では目標分布 \\(\\mathbb{Q}_1(dx_{0:1})\\propto G_1(x_0,x_1)\\mathbb{Q}_0(dx_0)M_1(x_0,dx_1)\\) を，提案分布 \\(\\mathbb{Q}_0(dx_0)M_1(x_0,dx_1)\\) から重点サンプリングすることを考える．\n\nそこで，提案分布 \\(\\mathbb{Q}_0(dx_0)M_1(x_0,dx_1)\\) に従う標本 \\(\\{(X_0^i,X_1^i)\\}_{i=1}^N\\) をどう得るかが問題になる．これを得たならば，残る重点サンプリングのステップとは，ポテンシャル \\(G_1(x_0,x_1)\\propto\\frac{d \\mathbb{Q}_1}{d \\mathbb{Q}_0\\otimes M_1}\\) に関して重み付けするのみである．\nまず，リサンプリングをしない粒子フィルターは，逐次重点サンプリングに等しい．その考え方は次の通りである：\n\n\n\n\n\n\n逐次重点サンプリングの考え方\n\n\n\n\\(\\mathbb{Q}_0\\) の近似 \\(\\mathbb{Q}_0^N\\) を得た Step 1. はこの提案分布を計算する途中だったと見做す．\nつまり，Step 1. の結果 \\(\\mathbb{Q}^N_0\\) をそのまま発展させて得る粒子の荷重和 \\[\n\\sum_{i=1}^NW^i_0(\\delta_{X_0^i}\\otimes\\delta_{X_1^i}),\n\\] \\[\nX_1^i\\sim M_1(X_0^i,-)\n\\] を，\\(\\mathbb{Q}_0\\otimes M_1\\) の良い近似として利用することが出来る．\nこれは極めて計算効率が良い戦略である．\n\n\nしかし荷重 \\(W_0^i\\le1\\) が引き継がれている点に注目して欲しい．ここに新たに \\(W_0^iW_1^i\\cdots\\) と連なっていくことになる．これでは，時間が経つ \\(t\\to\\infty\\) につれて荷重の分散が拡大し，殆ど１つの粒子しか Feynman-Kac 分布 \\(\\mathbb{Q}_t\\) の推定に寄与しないことになる．これでは，たくさん粒子を用意した意味がない．\nそこで，定期的にリサンプリングを行い，\\(\\mathbb{Q}_t^N\\) の表現を荷重 \\(\\{W_t^i\\}_{i=1}^N\\) に頼り切るのではなく，粒子の濃密で代替して，荷重の方は一様化に戻すという段階を挟む．\nこのリサンプリングの段階は，実用上はほとんどの場合，荷重 \\(\\{W_t^i\\}_{i=1}^N\\) の状態を監視して適応的に行うことが多い．\n\n\n\n\n\n\nリサンプリングの考え方\n\n\n\n計算が効率的である点を除けば，前段階 Step 1. の結果 \\(\\mathbb{Q}^N_0\\) をそのまま発展させる必要はなかったのである．\nむしろ，\\(\\mathbb{Q}^N_0\\otimes M_1\\) から直接 i.i.d. サンプリング（のようなもの）を行えば，追加の計算は必要になろうと，荷重が一様な \\(\\mathbb{Q}_0\\otimes M_1\\) の近似を得ることが出来る．\nこれはリサンプリングによって実現できる．\nというのは，まず \\(\\mathbb{Q}_0^N\\) からサンプリングを行う！．（完全に無駄なステップに見えるかも知れないが！）．\n正確に言えば，粒子番号 \\([N]=\\left\\{1,\\cdots,N\\right\\}\\) から，その重み \\(W_0^1,\\cdots,W_0^N\\) に従って，重複を許して \\(N\\) 個の番号 \\(A_1^1,\\cdots,A_1^N\\) をサンプリングする．すなわち，\\(A_1^1,\\cdots,A_1^N\\) を多項分布 \\(\\mathrm{Mult}_N(W_0^{1:N})\\) のサンプルとする．すると，元の近似と分布同等である： \\[\n\\mathbb{Q}^N_0=\\sum_{n=1}^NW_0^n\\delta_{X_0^n}\\overset{\\text{d}}{=}\\sum_{n=1}^N\\delta_{X_0^{A_1^n}}.\n\\] この操作を リサンプリング という．\n続いて，これを発展させる： \\[\nX_1^n\\sim M_1(X_0^{A_1^n},-),\\quad n\\in[N].\n\\]\nこうして得る粒子系 \\(\\{(X_0^{A_1^n},X_1^n)\\}_{n=1}^N\\) は \\(\\mathbb{Q}^N_0\\otimes M_1\\) を一様な荷重で近似している： \\[\n\\mathbb{Q}^N_0\\otimes M_1\\approx\\frac{1}{N}\\sum_{n=1}^N\\delta_{(X_0^{A_1^n},X_1^n)}.\n\\]\nただし，\\(\\mathbb{Q}^N_0\\otimes M_1\\) の i.i.d. サンプルではない 点に注意すべきである．新たな粒子 \\(X_1^n\\) の祖先番号 \\(A_1^{n}\\) に重複を許しているため，いずれか２つのサンプルは依存関係をもち得る．"
  },
  {
    "objectID": "posts/2024/Particles/resampling.html#footnotes",
    "href": "posts/2024/Particles/resampling.html#footnotes",
    "title": "粒子フィルターの実装：リサンプリング編",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(L_t\\) は積分を \\(1\\) にするための正規化定数である．詳しくは (Chopin & Papaspiliopoulos, 2020, pp. 51–52), (Del Moral & Penev, 2014, p. 239) も参照．↩︎\n確率核に関する記法は 本サイトの数学記法一覧 を参照．↩︎\n詳しくは (Chopin & Papaspiliopoulos, 2020, p. 53) 参照．↩︎"
  },
  {
    "objectID": "posts/2024/Particles/PF.html",
    "href": "posts/2024/Particles/PF.html",
    "title": "A Recent Development of Particle Methods",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\nThe following is a detailed version of the poster presented at MLSS2024, S3-41, March 8 (Fri) 18:00-19:30."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#what-is-particle-filter",
    "href": "posts/2024/Particles/PF.html#what-is-particle-filter",
    "title": "A Recent Development of Particle Methods",
    "section": "1 What Is Particle Filter?",
    "text": "1 What Is Particle Filter?\nParticle filters, also known as Sequential Monte Carlo methods (SMCs), were invented in (Kitagawa, 1993) and (Gordon et al., 1993) independently as an simulation-based algorithm which performs filtering in non-Gaussian and non-linear state space models, overcoming the weeknesses of then-standard Kalman-based filtering methods.1\nIn particle-based approaches, a filtering distribution is approximated by a cloud of weighted samples, hence giving rise to the term ‘particle filter’. The samples are propagated to approximate the next distribution, leading to efficient sequential estimation in dynamic settings.\nRecent developments have highlithgted the capability of particle filters as general-purpose samplers, extending their applicability beyond the traditional realm of temporal graphical models to a broader range of statistical inference problems. This versatility has earned them the alternative name ‘SMC’, a term reminiscent of ‘MCMC’. This poster trys to be another contribution in this direction."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#mcmc-vs.-smc",
    "href": "posts/2024/Particles/PF.html#mcmc-vs.-smc",
    "title": "A Recent Development of Particle Methods",
    "section": "2 MCMC vs. SMC",
    "text": "2 MCMC vs. SMC\nPDMPs (Piecewise Deterministic Markov Processes) (Davis, 1984), a type of continuous-time Markov processes with jumps as their only random components, play a complementary role to diffusion processes in stochastic modelling.2\nIn (Peters & de With, 2012), a PDMP was identified through the continuous limit of the MCMC, Metropolis-Hastings algorithm. The PDMP was further investigated and termed Bouncy Particle Sampler (BPS) in (Bouchard-Côté et al., 2018).\n\n\nCode\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nAlso, other types of continuous-time MCMC algorithms have been developed, such as the Zig-Zag sampler (Bierkens et al., 2019):\n\n\nCode\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nEnpirical evidence suggests that continuous-time MCMCs are more efficient than their discrete-time counterparts.\n\nInterestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as they need only access a small sub-set of data points at each iteration, and yet are still guaranteed to target the true posterior distribution. (Fearnhead et al., 2018)"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#inquiry-for-continuous-time-smc",
    "href": "posts/2024/Particles/PF.html#inquiry-for-continuous-time-smc",
    "title": "A Recent Development of Particle Methods",
    "section": "3 Inquiry for Continuous-time SMC",
    "text": "3 Inquiry for Continuous-time SMC\nDespite the success of continuous-time MCMC, the continuous-time limit of SMC has not been fully explored. The continuous-time limit of SMC is expected to be a jump process, which is similar to PDMP, but is more diffusion-like.\nMCMC has now taken a step ahead; it is time for SMC to explore its continuous-time limit!\n\n3.1 A Generic Particle Filter: An Algorithmic Description\n\n\n\nProcedure of a generic step of a particle filter at time \\(t\\)\n\n\n\nResampling Step\nParticles with high weights are duplicated, and those with the lowest weights are discarded.\nMovement Step\nSubsequently, a MCMC move is executed from the resampled particles.\n\nThe resampling step is the key difference from sequential importance sampling methods. Particle filters incorporate a resampling step to occasionally reset the weights of the samples, while maintaining the overall distribution they represent, in order to prevent the effective number of particles participating in the estimation from becoming too small–a situation also called weight degeneracy.\n\n\n3.2 A Necessary Condition: Resampling Stability\nIn order to have a time-step \\(\\Delta\\to0\\) limit, resampling events must occur with (at most linearly) decreasing frequency as \\(\\Delta\\to0\\).\nOnly the most efficient resampling schemes satisfy this property.\n\n\n\nRoot mean squared errors of marginal likelihood estimates (Chopin et al., 2022)"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#the-continuous-time-limit-process",
    "href": "posts/2024/Particles/PF.html#the-continuous-time-limit-process",
    "title": "A Recent Development of Particle Methods",
    "section": "4 The Continuous-time Limit Process",
    "text": "4 The Continuous-time Limit Process\nThe continuous-time limit process, if it exists, is characterized by a Feller-Dynkin process, whose infinitesimal generator is given by:\n\\[\n\\begin{align*}\n    \\mathcal{L}f(x)&=\\sum_{n=1}^N\\sum_{i=1}^db_i(x^n)\\frac{\\partial f}{\\partial x^n_i}(x)\\\\\n    &\\;\\;+\\sum_{n=1}^N\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x^n)\\frac{\\partial ^2f}{\\partial x^n_i\\partial x^n_j}(x)\\\\\n    &\\;\\;+\\sum_{a\\ne1:N}\\overline{\\iota}(V(x),a)\\biggr(f(x^{a(1:N)})-f(x^{1:N})\\biggl)\n\\end{align*}\n\\] \\[\n(f\\in C_c^2(\\mathbb{R}^{dN}),x\\in\\mathbb{R}^{dN},x^n\\in\\mathbb{R}^d)\n\\]\nwhen the latent process \\((X_t)\\) is an Itô process given by the generator:\n\\[\n\\begin{align*}\n    Lf(x)&=\\sum_{i=1}^db_i(x)\\frac{\\partial f}{\\partial x_i}(x)\\\\\n    &\\;\\;+\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x)\\frac{\\partial ^2f}{\\partial x_i\\partial x_j}(x)\n\\end{align*}\n\\] \\[\n(f\\in C_c^2(\\mathbb{R}^d),x\\in\\mathbb{R}^d)\n\\]\nFor details, please consult (Chopin et al., 2022, p. 3206), Theorem 19."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#conclusions",
    "href": "posts/2024/Particles/PF.html#conclusions",
    "title": "A Recent Development of Particle Methods",
    "section": "5 Conclusions",
    "text": "5 Conclusions\n\n\n\n\n\n\nSummaries\n\n\n\nSMC with efficient resampling schemes possess a continuous-time limit \\(\\Delta\\to0\\), which turns out to be a Feller-Dynkin process, a diffusion process with jumps, when \\((X_t)\\) is a diffusion."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#forthcoming-research",
    "href": "posts/2024/Particles/PF.html#forthcoming-research",
    "title": "A Recent Development of Particle Methods",
    "section": "6 Forthcoming Research",
    "text": "6 Forthcoming Research\n\n\n\n\n\n\nUltimate Purpose\n\n\n\nHow can we leverage the knowledge of the continuous-time limit process to design efficient Sequential Monte Carlo (SMC) samplers capable of sampling from posterior distributions of diffusions?\n\n\n\nWhat are the properties of this limit jump process, and how do they change with modifications to the underlying latent process?\nHow does the timing of resampling affect overall efficiency? Can insights be gained from the perspective of continuous-time limits?\nDoes the continuous-time limit process improve SMC efficiency when used for particle propagation?"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#footnotes",
    "href": "posts/2024/Particles/PF.html#footnotes",
    "title": "A Recent Development of Particle Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGood references are (Murphy, 2023) Chapter 13, and (Theodoridis, 2020, p. 881) Section 17.4.↩︎\n(Fearnhead et al., 2018) is a great introduction to this topic.↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI.html",
    "href": "posts/2024/Computation/VI.html",
    "title": "変分推論１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次の 稿 で，\\(K\\)-平均アルゴリズムの確率的な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す．\nより図が見やすい PDF 版は こちら．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#導入",
    "href": "posts/2024/Computation/VI.html#導入",
    "title": "変分推論１",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 歴史\nハード \\(K\\)-平均法はモデルフリーのクラスタリングアルゴリズムである．Voronoi 分割による競争学習の一形態とも見れる．1\n一方で原論文 (Lloyd, 1982) では，パルス符号変調 の文脈で，アナログ信号の量子化の方法として提案している．2\n実際，いまでも \\(K\\)-平均法は（非可逆）データ圧縮に用いられる．クラスター中心での画像の値と，それ以外では帰属先のクラスター番号のみを保存すれば良いというのである．このようなアプローチを ベクトル量子化 (vector quantization) という．3\nソフト \\(K\\)-平均法とは，このようなデータ点のクラスターへの一意な割り当てを，ソフトマックス関数を用いて軟化したアルゴリズムであり，多少アルゴリズムとしての振る舞いは改善するとされている．\n\n\n1.2 最適化アルゴリズムとしての見方\n\\(N\\) 個のデータ \\(\\{x_n\\}_{n=1}^N\\) の \\(K\\) クラスへの \\(K\\)-平均クラスタリングアルゴリズムは，ハードとソフトの二種類存在するが，いずれも \\[\nJ:=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\|x_n-\\mu_k\\|^2\n\\] という損失関数の逐次最小化アルゴリズムとみなせる．\nこの見方は，ML アルゴリズム への一般化の軸となる．\n\n\n1.3 用いるデータ\n実際のコードとデータを用いて \\(K\\)-平均法を解説する．\nまずは，解説にために作られた，次のような３つのクラスタからなる２次元のデータを考え，これの正しいクラスタリングを目指す．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#sec-hard-k-means",
    "href": "posts/2024/Computation/VI.html#sec-hard-k-means",
    "title": "変分推論１",
    "section": "2 ハード \\(K\\)-平均法",
    "text": "2 ハード \\(K\\)-平均法\n\n2.1 アルゴリズムの説明\nhead \\(K\\)-means algorithm はデータ \\(\\{x^{(n)}\\}_{n=1}^N\\subset\\mathbb{R}^I\\) とクラスタ数 \\(K\\in\\mathbb{N}^+\\)，そして初期クラスター中心 \\((m^{(k)})_{k=1}^K\\in(\\mathbb{R}^I)^K\\) の３組をパラメータに持つ．\nsoft \\(K\\)-means algorithm 3.1 はさらに硬度パラメータ \\(\\beta\\in\\mathbb{R}_+\\) を持つ．\nnumpy の提供する行列積を利用して，これを Python により実装した例を以下に示す．ソフト \\(K\\)-平均法の実装と対比できるように，負担率を通じた実装を意識した例である．\nアノテーションを付してあるので，該当箇所（右端の丸囲み数字）をクリックすることで適宜解説が読めるようになっている．\ndef hkmeans_2d(data, K, init, max_iter=100):\n    \"\"\"\n    ２次元データに対するハード K-平均法の実装例．\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n\n    Returns:\n    - clusters: (N,)-numpy.ndarray クラスター番号\n    \"\"\"\n\n1    N = data.shape[0]\n2    I = data.shape[1]\n3    m = init\n4    r = np.zeros((K, N), dtype=float)\n\n    for _ in range(max_iter):\n        # Assignment Step\n        for i in range(N):\n5            distances = np.array([d(data[i], m[:,k]) for k in range(K)])\n6            k_hat = np.argmin(distances)\n7            r[:,i] = 0\n            r[k_hat,i] = 1\n        \n        # Update Step\n8        new_m = np.zeros_like(m, dtype=float)\n9        numerator = np.dot(r, data)\n10        denominator = np.sum(r, axis=1)\n11        for k in range(K):\n            if denominator[k] &gt; 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = m[:,k]\n\n12        if np.allclose(m, new_m):\n            break\n        m = new_m\n    \n13    return np.argmax(r, axis=0)\n\n1\n\nデータ数を取得している．\n\n2\n\nデータの次元を取得している．今回はすべて２次元データを用いる．\n\n3\n\nクラスター中心に引数として受け取った初期値を代入. \\(2×K\\)-行列であることに注意．\n\n4\n\n負担率を \\(K×N\\)-行列として格納している．その理由は後ほど行列積を通じた計算を行うためである．dtype=float の理由は後述．\n\n5\n\nこの distances 変数は (K,)-numpy.ndarray になる．すなわち，第 \\(k\\) 成分が，第 \\(k\\) クラスター中心との距離となっているようなベクトルである．ただし，d は Euclid 距離を計算する関数として定義済みとした．\n\n6\n\n距離が最小となるクラスター番号 \\(\\hat{k}:=[\\mathop{\\mathrm{arg\\,min}}_{k\\in[K]}d(m_k,x_i)]\\) を，\\(i\\in[N]\\) 番目のデータについて求める．\n\n7\n\n\\(\\hat{k}\\) に基づいて負担率を更新するが，ループ内で前回の結果をリセットする必要があることに注意．\n\n8\n\nここで dtype=float と指定しないと，初め引数 init が整数のみで構成されていた場合に，Python の自動型付機能が int 型だと判定し，クラスター中心 m の値が整数に限られてしまう．すると，アルゴリズムがすぐに手頃な格子点に収束してしまう．\n\n9\n\nnumpy の行列積を計算する関数 np.dot を使用している．更新式 \\[\nm^{(k)}\\gets\\frac{\\sum_{n=1}^Nr^{(n)}_kx^{(n)}}{\\sum_{n=1}^Nr^{(n)}_k}\n\\] の分子を行列積と見たのである．\n\n10\n\n分母 (denominator) は \\((K,N)\\)-行列 r の行和として得られる．\n\n11\n\nゼロによる除算が起こらないように場合わけをしている．\n\n12\n\nクラスター中心がもはや変わらない場合はアルゴリズムを終了する．\n\n13\n\n負担率の最も大きいクラスター番号を返す．今回は hat_k の列をそのまま返せば良いが，soft \\(K\\)-means アルゴリズムにも通じる形で実装した．\n\n\n\n\n\n\n\n\n注：実際に用いる実装\n\n\n\n\n\nただし，本記事の背後では次の実装を用いる．\nクラスター中心の推移のヒストリーを保存して図示に利用したり，負担率 r の中身を見たりすることが出来るようにするため，assignment step と update step とに分けてクラスメソッドとして実装し，run メソッドでそれらを呼び出すようにしている．これに fetch_cluster と fetch_history メソッドを加えることで，クラスター番号とクラスター中心の推移を取得することが出来る．フィールド .r から（最終的な）負担率を見ることもできる．\n\n\nCode\nclass kmeans_2d:\n    \"\"\"\n    ２次元データに対するソフト K-平均法の実装．\n\n    Usage:\n        kmeans = kmeans_2d(data, K, init, beta)\n        kmeans.run()\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n    - beta: float 硬度パラメータ\n    \"\"\"\n\n    def __init__(self, data, K, init, beta, max_iter=100):\n        self.data = np.array(data, dtype=float)\n        self.K = K\n        self.init = np.array(init, dtype=float)\n        self.beta = float(beta)\n        self.max_iter = max_iter\n        self.N = data.shape[0]  # データ数\n        self.I = data.shape[1]  # 次元数 今回は２\n        self.m = init  # クラスター中心の初期化．2×K行列．\n        self.r = np.zeros((K, self.N), dtype=float)  # 負担率．K×N行列．\n        self.history = [init.copy()] # クラスター中心の履歴．2×K行列．\n    \n    def soft_assigment(self):\n        \"\"\"soft K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) ** 2 for j in range(self.K)]) # (N,)-numpy.ndarray\n            denominator_ = np.sum(np.exp(-self.beta * distances))  # 分母\n            self.r[:,i] = np.exp(- self.beta * distances) / denominator_\n\n    def hard_assigment(self):\n        \"\"\"hard K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray\n            k_hat = np.argmin(distances)  # 最小距離のクラスター番号\n            self.r[:,i] = 0  # 前のループの結果をリセット\n            self.r[k_hat,i] = 1\n    \n    def update(self):\n        \"\"\"クラスター中心の更新\"\"\"\n        new_m = np.zeros_like(self.m, dtype=float) # ここで float にしないと，クラスター中心が整数に限られてしまう．\n        numerator = np.dot(self.r, self.data)  # (K,2)-numpy.ndarray\n        denominator = np.sum(self.r, axis=1)  # 各クラスターの負担率の和\n        for k in range(self.K):\n            if denominator[k] &gt; 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = self.m[:,k]\n        self.m = new_m\n\n    def fetch_cluster(self):\n        \"\"\"最終的なクラスター番号を格納した (N,)-array を返す\"\"\"\n        return np.argmax(self.r, axis=0)\n    \n    def fetch_history(self):\n        \"\"\"クラスター中心の履歴を格納したリストを，３次元の np.array に変換して返す\"\"\"\n        return np.stack(self.history, axis=0)\n\n    def run_soft(self):\n        \"\"\"soft K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.soft_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n    \n    def run_hard(self):\n        \"\"\"hard K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.hard_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n\n\nなお，この実装は \\(\\beta\\ge500\\) などの場合にオーバーフローが起こることに注意．これへの対処は logsumexp の使用などが考えられる．\n\n\n\n\n\n2.2 初期値依存性\n次の２つの初期値を与えてみる． \\[\nm_1:=\\begin{pmatrix}4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3=\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] と，\\(m_2,m_3\\) は変えずに \\(m_1\\) の \\(y\\)-座標を \\(1\\) だけ下げたもの \\[\nm_1':=\\begin{pmatrix}4\\\\-1\\end{pmatrix}\n\\] とを初期値として与えてみる．\n\n\n\n\n\n\n\n\n図 1: ハード K-平均法によるクラスタリングの結果．初期値は \\((m_1,m_2,m_3)=\\left(\\begin{pmatrix}4\\\\0\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．\n\n\n\n\n\n正解数: 51     正解率: 56.7 %     反復数: 9 回\n\n\n別の初期値を与えてみる（右下の点 \\(m_1\\) を \\(1\\) だけ下に下げただけ）： \\[\n\\begin{pmatrix}4\\\\0\\end{pmatrix}=m_1\\mapsto m_1':=\\begin{pmatrix}4\\\\-1\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n図 2: ハード K-平均法によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 7 回\n\n\n結果が全く変わり，\\((m_1',m_2,m_3)\\) を与えた方が，大きく正解に近づいている．具体的には，右下の初期値 \\(m_1\\) は右上の島に行くが，\\(m_1'\\) は左下の島に行ってくれる．\nハード \\(K\\)-平均アルゴリズムは初期値に敏感である ことがよく分かる．\n\n\n2.3 局所解への収束\n直前の結果2ではクラスター２と３の境界線で４つのミスを犯しており，これを修正できないか試したい．\nそこで，答えに近いように， \\[\nm_1\\gets\\begin{pmatrix}2.5\\\\2\\end{pmatrix},\\;\\; m_2\\gets\\begin{pmatrix}-1\\\\-1\\end{pmatrix},\\;\\; m_3\\gets\\begin{pmatrix}1\\\\-2\\end{pmatrix},\n\\] を初期値として与えてみて，正答率の変化を観察する．\n\n\n\n\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 5 回\n\n\nもはや初期値から殆ど動いていないが，目標のクラスター３に分類された３つの点が，相変わらず３のままであり，加えてクラスター２の中心がこれらから逃げているようにも見えるので，クラスター２の初期値をよりクラスター３に近いように誘導し，クラスター３の中心をより右側から開始する：\n\\[\nm_2:\\begin{pmatrix}-1\\\\-1\\end{pmatrix}\\mapsto\\begin{pmatrix}0\\\\-2\\end{pmatrix}\\;\\; m_3:\\begin{pmatrix}1\\\\-2\\end{pmatrix}\\mapsto\\begin{pmatrix}2\\\\-2\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 6 回\n\n\nこんなに誘導をしても，正しく分類してくれない．\n実は，以上２つの初期値では，最終的に３つのクラスター中心は同じ値に収束している．よって，これ以上どのように初期値を変更しても，正答率は上がらないシナリオが考えられる．\n以上の観察から，ハード \\(K\\)-平均法はある種の 局所解に収束する ようなアルゴリズムであると考えられる．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#ソフト-k-平均法",
    "href": "posts/2024/Computation/VI.html#ソフト-k-平均法",
    "title": "変分推論１",
    "section": "3 ソフト \\(K\\)-平均法",
    "text": "3 ソフト \\(K\\)-平均法\n\n3.1 アルゴリズムの説明\nハード \\(K\\)-平均法2では，負担率 \\[\nr_{kn}\\gets\\delta_{k}(\\mathop{\\mathrm{arg\\,max}}_{i\\in[k]}d(m_i,x_n))\n\\] は \\(0,1\\) のいずれかの値しか取らなかった．この振る舞いを， \\[\n\\sigma(z;e)_i:=\\frac{e^{z_i}}{\\sum_{j=1}^Ke^{e_j}}\\quad(i\\in[K])\n\\] で定まる ソフトマックス関数 \\(\\sigma:\\mathbb{R}^K\\to(0,1)^K\\) を用いて，「軟化」する．\nここでは，\\(\\beta\\ge0\\) として， \\[\n\\sigma(z;e^{-\\beta})_i=\\frac{e^{-\\beta z_i}}{\\sum_{j=1}^Ke^{-\\beta e_j}}\n\\] の形で用い，\\(\\mathop{\\mathrm{arg\\,max}}\\) の代わりに \\[\n\\begin{align*}\n    r_{kn}&\\gets\\sigma(d(-,x_n)^2\\circ m;e^{-\\beta})_k\\\\\n    &=\\frac{e^{-\\beta d(m_k,x_n)^2}}{\\sum_{j=1}^K e^{-\\beta d(m_j,x_n)^2}}\n\\end{align*}\n\\] とする．ただし，\\(d\\) は \\(\\mathbb{R}^2\\) 上の Euclid 距離とした．\n\n3.1.1 硬度パラメータ\n\\(\\beta\\) は 硬度 (stiffness) または逆温度と呼ぶ．4 \\(\\sigma:=\\beta^{-1/2}\\) は距離の次元を持つ．\n\\(\\beta=0\\) のときは温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる \\(\\beta\\to\\infty\\) の極限が hard \\(K\\)-means アルゴリズムに相当する．\n逆温度 \\(\\beta\\) を連続的に変化させることで，クラスタ数に分岐が起こる，ある種の相転移現象を見ることができる．5\n\n\n3.1.2 実装\n実装は例えば hard \\(K\\)-means アルゴリズム2から，負担率計算の部分のみを変更すれば良い：\nfor i in range(N):\n1        distances = np.array([d(data[i], m[:,k]) for k in range(K)])\n2        denominator_ = np.sum(np.exp(-beta * distances))\n3        r[:,i] = np.exp(-beta * distances) / denominator_\n\n1\n\nデータ \\(x_i\\) とクラスター中心 \\((m_k)_{k=1}^K\\) との距離を計算し，ベクトル \\((d(x_n,m_k))_{k=1}^K\\) を distances に格納している．\n\n2\n\n負担率の計算 \\[\nr_{ik}=\\frac{\\exp(-\\beta d(m_k,x_i))}{\\sum_{j=1}^K\\exp(-\\beta d(m_j,x_i))}\n\\] を２段階に分けて行なっており，分母を先に計算して変数 denominator_ に格納している．\n\n3\n\nすでに計算してある分母 denominator_ を用いてデータ \\(x_i\\) の負担率 \\((r_{ki})_{k=1}^K\\) を計算し，\\((K,N)\\)-行列 r の各列に格納している．\n\n\n\n\n\n3.2 挙動の変化の観察\n逆温度をはじめに \\(\\beta=0.3\\) としてみる．図 1 と全く同様な初期値 \\[\nm_1:=\\begin{pmatrix}4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3=\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] を与えてみると，次の通りの結果を得る：\n\n\n\n\n\n\n\n\n図 3: 左がソフト K-平均法（\\(\\beta=1\\)），右がハード K-平均法によるクラスタリングの結果（図２の左と全く同じもの）．初期値は \\((m_1,m_2,m_3)=\\left(\\begin{pmatrix}4\\\\0\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．\n\n\n\n\n\n正解数: 44 vs. 51     正解率: 48.9 % vs. 56.7 %     反復数: 28 回 vs. 9 回\n\n\nクラスターの境界が変化しており，正解率は悪化している．さらに，反復数が９回であったところから，３倍に増えている（28回）．\nまた，右上の２つのクラスター中心の収束先は，微妙にずれているが ほとんど一致している 点も注目に値する．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\ncenters = history[-1, :, :]\ndf = pd.DataFrame(centers, columns=['Cluster1', 'Cluster2', 'Cluster3'])\nprint(df)\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.397456  2.397535 -0.036071\ny  2.047565  2.047580 -1.448288\n\n\n\n\n\n図 2 で与えた初期値 \\((m_1',m_2,m_3)\\) も与えてみる．\n\n\n\n\n\n\n\n\n図 4: ソフト K-平均法（\\(\\beta=1\\)）によるクラスタリングの結果，右がハード K-平均法によるクラスタリングの結果（図２の右と全く同じもの）．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 59 回 vs. 7 回\n\n\nクラスター境界と正答率は変わらないが，反復数がやはり７回から大きく増えている．\n結果はやはり 図 3 とは大きく異なっており，ハード \\(K\\)-平均法で観察された初期値鋭敏性が，変わらず残っている．\n加えてこの場合も 図 3 のクラスター１と２と同様に，クラスター２と３の中心がほぼ一致している．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.466833 -0.369537  0.447958\ny  2.124961 -1.076874 -1.543758\n\n\n\n\n\n\\(\\beta=0.3\\) の場合のソフト \\(K\\)-平均法は，この例では クラスター中心が融合する傾向にある ようである．\n一般に，\\(\\beta\\) が小さく，温度が大きいほど，エネルギーランドスケープに極小点が少なくなり，クラスターは同じ場所へ収束しやすくなると予想される．\n\n\n3.3 高温になるほどクラスター数は減少する\n初期値を直前で用いた \\[\nm_1\\gets\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\quad m_2\\gets\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3\\gets\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] で固定とし，さらに温度を上げて，逆温度を \\(\\beta=0.1\\) としてみる．\n\n\n\n\n\n\n\n\n図 5: ソフト K-平均法（左\\(\\beta=0.1\\)，右\\(\\beta=1\\)）によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 68 vs. 85     正解率: 75.6 % vs. 94.4 %     反復数: 101 回 vs. 59 回\n\n\n反復数はさらに増加し，全てがほとんど同じクラスターに属する結果となってしまった．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n\n\n   Cluster1  Cluster2  Cluster3\nx  1.715903  0.862511  1.329066\ny  1.012398 -0.099845  0.511186\n\n\n\n\n\n温度が大変に高い状態では，全てが乱雑で，３つのクラスターが一様・公平に負担率を持つようになった．そのため，第一歩からほとんど全体の中心へと移動し，反復数が減る．\n次に，温度を少し下げて，逆温度を \\(\\beta=2\\) としてみる．\n\n\n\n\n\n\n\n\n図 6: ソフト K-平均法（左\\(\\beta=10\\)，右\\(\\beta=1\\)）によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 17 回 vs. 59 回\n\n\n初めて soft \\(K\\)-means アルゴリズムを用いた場合で，３つのクラスター中心がはっきりと別れた．反復回数は，\\(\\beta=0.3\\) の場合と比べればやはり落ち着いている．\nしかし，正解率は head \\(K\\)-means の場合（ 図 2 など）と全く同じである．実は，最終的なクラスター中心も 図 2 の最終的なクラスター中心とほとんど同じになっている．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n今回のソフト \\(K\\)-平均法の最終的なクラスター中心\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.416113  0.881629 -1.338782\ny  2.086327 -1.934090 -0.816316\n\n\n図 2 のハード \\(K\\)-平均法の最終的なクラスター中心\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.426102  0.868333 -1.323353\ny  2.091429 -1.948458 -0.765176\n\n\n\n\n\n以上より，ソフト \\(K\\)-平均法は温度を上げるほどクラスター数が少なくなり，温度を下げるほどクラスター数は上がり，十分に温度を下げるとハード \\(K\\)-平均法に挙動が似通う．\n\n\n3.4 最適な硬度の選択\n\\(\\beta=0.2\\) ではクラスターが２つに縮退し，\\(\\beta=1\\) では hard \\(K\\)-means アルゴリズムの結果とほとんど変わらなくなる．その中間では次のように挙動が変わる：\n\n\n\n\n\n\n\n\n図 7: ソフト K-平均法によるクラスタリングの結果の比較（\\(\\beta=0.2\\) vs. \\(\\beta=0.25\\)）．\n\n\n\n\n\n正解数: 50 vs. 86\n正解率: 55.6 % vs. 95.6 %\n\n\n\n\n\n\n\n\n\n\n図 8: ソフト K-平均法によるクラスタリングの結果の比較（\\(\\beta=0.3\\) vs. \\(\\beta=0.5\\)）．\n\n\n\n\n\n正解数: 85 vs. 85\n正解率: 94.4 % vs. 94.4 %\n\n\nやはり，温度が高い場合はクラスター中心が合流・融合してしまいやすいが，冷却することでクラスター数は大きい状態で安定する，と言えるだろう．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#本番データセットでの実験",
    "href": "posts/2024/Computation/VI.html#本番データセットでの実験",
    "title": "変分推論１",
    "section": "4 本番データセットでの実験",
    "text": "4 本番データセットでの実験\n今まで使っていたデータ1.3はクラスターのオーバーラップはなかったため，いわば優しいデータであった．ここからはよりデータ生成過程が複雑なデータを用いて，ソフト \\(K\\)-平均法の挙動を観察する．\n\n4.1 データの概観\n今度は，次の４クラスのデータを用いる．\n\n\n\n\n\n\n\n\n\n実は，これは４つの Gauss 分布から生成されたデータである．\n\n\n4.2 最適な温度の選択\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正解数: 377 vs. 366     正解率: 83.8 % vs. 81.3 %     反復数: 49 回 vs. 62 回\n正解数: 386 vs. 375     正解率: 85.8 % vs. 83.3 %     反復数: 101 回 vs. 70 回\n正解数: 378 vs. 378     正解率: 84.0 % vs. 84.0 %     反復数: 39 回 vs. 14 回"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#実験結果まとめ",
    "href": "posts/2024/Computation/VI.html#実験結果まとめ",
    "title": "変分推論１",
    "section": "5 実験結果まとめ",
    "text": "5 実験結果まとめ\n\n\n\n\n\n\n結論\n\n\n\n\nデータ 1.3 に対して，（初期値 \\((m'_1,m_2,m_3)\\) で）ソフト \\(K\\)-平均法を適用すると，\n\n\\(\\beta\\ge2\\) の場合で結果はハード \\(K\\)-平均法と変わらなくなる．\n\\(\\beta=1\\) の場合で結果はクラスターがほとんど２つになり，\\(\\beta\\le0.5\\) では計算機上では実際に２つになってしまう．\n正答率は \\(1\\le\\beta\\le1.1\\) で最大であった．\n\\(\\beta\\) を大きくするほど，反復回数は減少していった．\n\nデータ 4.1 に対しても，以上の４点について同様の傾向が確認できた．\n\n\n\nこうしてソフト \\(K\\)-平均法とハード \\(K\\)-平均法の性質は分かった．主に\n\n初期値依存性\nクラスタ数 \\(K\\) の選択法\n\nの問題が未解決であり，恣意性が残る．\n誰がどう使ってもうまくいくようなアルゴリズムであると言うことは出来ない．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#footnotes",
    "href": "posts/2024/Computation/VI.html#footnotes",
    "title": "変分推論１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(MacKay, 2003, p. 285)．↩︎\nLloyd は 1957 年には発表していたが，論文の形になったのが 1982 である．\\(K\\)-means という名前の初出は (MacQueen, 1967) とされている．↩︎\n(MacKay, 2003, p. 284)，(Bishop, 2006, p. 429)．クラスター中心は 符号表ベクトル または 代表ベクトル (code-book vector) という．↩︎\nstiffness の用語は (MacKay, 2003, p. 289) から．実は各クラスターに Gauss モデルを置いた場合の分散 \\(\\sigma^2\\) に対して，\\(\\beta=\\frac{1}{2\\sigma^2}\\) の関係がある．次稿 参照．↩︎\n(MacKay, 2003, p. 291)．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html",
    "href": "posts/2024/Computation/VI2.html",
    "title": "変分推論２",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#最尤推定",
    "href": "posts/2024/Computation/VI2.html#最尤推定",
    "title": "変分推論２",
    "section": "1 最尤推定",
    "text": "1 最尤推定\nクラスタリングを一度さっぱり忘れて，最尤推定を思い出してみる．\n\n\n\n\n\n\n定義：最尤推定量1 (Fisher, 1912)\n\n\n\n\\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\subset\\mathcal{P}(\\mathcal{X})\\) を統計モデルで，ある共通の \\(\\sigma\\)-有限測度 \\(\\mu\\in\\mathcal{P}(\\mathcal{X})\\) に関して密度 \\(\\{p_\\theta\\}_{\\theta\\in\\Theta}\\) を持つとする．\n独立な観測 \\(X_1,\\cdots,X_n\\) の 最尤推定量 とは，モデルの対数尤度 \\[\n\\log p_\\theta\n\\] を通じて定まる次の目的関数 \\(\\ell_n:\\Theta\\to(-\\infty,0)\\) を最大化するような \\(M\\)-推定量 をいう： \\[\n\\ell_n(\\theta;X_1,\\cdots,X_n):=\\sum_{i=1}^n\\log p_\\theta(X_i),\\qquad\\theta\\in\\Theta.\n\\]\n\n\n\n1.1 最尤推定と最適化\nすなわち，最尤推定量とは最適化問題の解として定式化されるのである．\n最大値点であるということは，停留点である必要があるから，微分が零になるという条件を通じて解析的に求まることもある．この \\(Z\\)-推定量としての特徴付けは (Carmer, 1946, p. 498) による．\nまた，計算機的な方法では，Iterative Propertional Fittting や勾配に基づく最適化手法を用いることも考えられる (Robbins & Monro, 1951), (Fletcher, 1987)．\n最尤推定量が解析的に求まらない場面には，代表的には欠測モデルなどがある．欠測モデルは，観測される確率変数 \\(X\\) の他に，観測されない確率変数 \\(Z\\) も想定し，その同時分布を考えるモデルである．これにより，\\((X,Z)\\) 全体には単純な仮定しか置かずとも，\\(X\\) に対して複雑な分布を想定することが可能になるのである．\nこの場合には，モデルの構造を利用して最尤推定量を求めるための MM アルゴリズム (Sun et al., 2016) の例がある．これが EM アルゴリズム (Dempster et al., 1977) である．\n現在でも，その他の MM アルゴリズムが，種々の最適化問題に対する “problem-driven algorithm” であり続けている (T. T. Wu & Lange, 2010)．\n\n\n1.2 最尤推定と Bayes 推定\n最尤推定は，一様事前分布をおいた場合の MAP 推定 とみなせる．この意味で，Bayes 推定の特殊な場合である．\nBayes 推定は MCMC や SMC などのサンプリング法によって統一的に行えるが，殊に MAP 推定に対しては，効率的な最適化法として EM アルゴリズムが使える，ということである．\nより一般の Bayes 推定に対応できるような EM アルゴリズムの一般化が，近似アルゴリズムとして存在する．これが次稿で紹介する 変分推論 である．"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#sec-EM0",
    "href": "posts/2024/Computation/VI2.html#sec-EM0",
    "title": "変分推論２",
    "section": "2 EM アルゴリズム",
    "text": "2 EM アルゴリズム\nEM アルゴリズムは，混合モデルに対する最尤推定アルゴリズムである．一般に，目的関数が \\[\nh(x)=\\operatorname{E}[H(x,Z)]\n\\] と表せる場合に対する特殊な MM アルゴリズムである．2\n\n2.1 欠測データと混合モデル\n欠測データ (incomplete data) とは，２つの確率変数 \\((Z,X)\\) について次の図式が成り立つ際の，\\(Z\\) を潜在変数として，\\(X\\) からの観測とみなせるデータをいう (Dempster et al., 1977, p. 1)：\n\n\n\n\n\n\n図 1: Missing Data Model / Latent State Model / Completed Model for \\(X\\)\n\n\n\nこれは， \\[\np(x|\\theta)=\\int_{\\mathcal{Z}}p(x,z|\\theta)\\,dz\n\\tag{1}\\] という形の尤度を持つモデルである．3\nこれは潜在変数 \\(Z\\) を持つモデルの最も単純な例ともみなせる．特に \\(Z\\) が離散変数である場合，\\(X\\) に対する混合モデルともいう．隠れ Markov モデル はこの発展例である．4\nこのように，\\(X\\) の分布を，潜在変数 \\(Z\\) を追加して理解することを，モデルの 完備化 (completion) または 脱周辺化 (demarginalization)，またはデータの拡張 (data augmentation) ともいう．5\n\n\n2.2 EM アルゴリズム\n値域 \\(\\mathcal{Z}\\) を持つ潜在変数 \\(Z\\) とパラメータ \\(\\theta\\in\\Theta\\) に関して 式 1 で表せる尤度関数 \\(p(x|\\theta)\\) に関して，Jensen の不等式より，任意の \\(x,\\theta\\) で添字づけられた確率密度関数 \\(q:\\mathcal{Z}\\to\\mathbb{R}_+\\)6 とパラメータ \\(\\theta\\in\\Theta\\) について次の評価が成り立つ：\n\\[\n\\begin{align*}\n    \\log p(x|\\theta)&=\\log\\int_{\\mathcal{Z}}p(x,z|\\theta)\\,dz\\\\\n    &\\ge\\int_\\mathcal{Z}q(z|x,\\varphi)\\log\\frac{p(x,z|\\theta)}{q(z|x,\\varphi)}\\,dz\\\\\n    &=:F(q,\\theta).\n\\end{align*}\n\\tag{2}\\]\nこの事実に基づき，\\(F\\) を代理関数として，これを２つの変数 \\(q,\\theta\\) について交互に最大化するという手続きを，EM アルゴリズム という．7\n\n\\(E\\)-ステップ：\\(F\\) を \\(q\\) について最大化する． \\[\n\\begin{align*}\nF(q,\\theta)&=\\int_\\mathcal{Z}q(z|x,\\varphi)\\log\\frac{p(z|x,\\theta)p(x|\\theta)}{q(z|x,\\varphi)}\\,dz\\\\\n&=\\log p(x|\\theta)-\\mathop{\\mathrm{KL}}(q_\\varphi,p_\\theta).\n\\end{align*}\n\\] より，\\(q(z|x,\\varphi)=p(z|x,\\theta)\\) で最大化される．8\n\\(M\\)-ステップ：\\(F\\) を \\(\\theta\\) について最大化する． \\[\n\\begin{align*}\nF(q,\\theta)&=\\int_{\\mathcal{Z}}q(z|x,\\varphi)\\log p(x,z|\\theta)\\,dz\\\\\n&\\qquad-\\int_\\mathcal{Z}q(z|x,\\varphi)\\log q(z|x,\\varphi)\\,dz\\\\\n&=\\underbrace{(q_\\varphi dz\\,|\\log p_\\theta)}_{=:Q(\\theta|\\varphi,x)}+H(q_\\varphi)\n\\end{align*}\n\\] より，\\(Q\\) の停留点で最大化される．\n\n総じて，EM アルゴリズムは \\(p,q\\) の KL 乖離度を逐次的に最小化している．\n\n\n2.3 \\(E\\)-ステップの変形\n\\(M\\)-ステップにおける \\(F\\) の \\(\\theta\\) における最大化は \\(Q\\) の \\(\\theta\\) による最大化に等価であるから，\\(E\\)-ステップは結局，事後分布 \\(p(z|x,\\theta)\\) を計算し，これに関する積分である \\[\nQ(\\theta|\\varphi,x)=\\int_\\mathcal{Z}p(z|x,\\theta)\\log p(x,z|\\theta)\\,dz\n\\] を計算する，というステップになる．\nモデル \\(\\{p_\\theta\\}\\) を複雑にしすぎた場合，この \\(Q\\) の計算は困難で実行不可能になってしまう．解析的に \\(E\\)-ステップを実行したい場合，典型的には指数型分布族を仮定する．\nそこで，\\(Q\\) を Monte Carlo 推定量で代替して，それを最大化した場合の EM アルゴリズムを MCEM (Monte Carlo EM) という (Wei & Tanner, 1990a), (Wei & Tanner, 1990b)．典型的には Metropolis-Hastings アルゴリズムを用いることになり (Chau et al., 2021)，これがスケーラビリティ問題を産む．9\nまた，この \\(E\\)-ステップで必ずしも完全な最大化を達成する必要はない (Neal & Hinton, 1998), (Bishop, 2006, p. 454)．従って，\\(p(z|x,\\theta)\\) が複雑すぎる場合，十分近い \\(q\\) を選択してこれに関する積分として \\(Q\\) を近似することが考えられる．特に \\(p\\) を 変分近似 した場合，変分 EM アルゴリズムという (Wainwright & Jordan, 2008, p. 154)．\n\n\n2.4 \\(M\\)-ステップの変形\n\\(Q\\) の停留点を探すにあたって，典型的には微分が消える点を探す．\nしかしこれが難しい場合，厳密な最大化は行わず，代わりにせめて「現状よりは大きくする」ことを実行するアルゴリズムを用いた場合，これを 一般化 EM アルゴリズム (GEM: Generalized EM) ともいう (Bishop, 2006, p. 454), (Hastie et al., 2009, p. 277)．\n例えば，大域的最大化の代わりに条件付き最大化を行うこととする方法 ECM (Expectation Conditional Maximization) などがその例である (Meng & Rubin, 1991), (Meng & Rubin, 1993)．(Robert & Casella, 2004, p. 200) も参照．\n\n\n2.5 EM アルゴリズムの有効性\n\n\n\n\n\n\n命題：尤度は単調減少する (Dempster et al., 1977)10\n\n\n\n\\(\\{\\widehat{\\theta}_{(j)}\\}\\subset\\Theta\\) を EM アルゴリズムの \\(M\\)-ステップでの出力列とする．このとき，\n\\[\nL(\\widehat{\\theta}_{(j+1)}|x)\\ge L(\\widehat{\\theta}_{(j)}|x).\n\\] 等号成立は \\[\nQ(\\widehat{\\theta}_{(j+1)}|\\widehat{\\theta}_{(j)},x)=Q(\\widehat{\\theta}_{(j)}|\\widehat{\\theta}_{(j)},x)\n\\] の場合のみ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n命題：局所解への収束 (Boyles, 1983)-(C. F. J. Wu, 1983)11\n\n\n\n\\[\nQ(\\theta|\\theta_0,x):=\\int_\\mathcal{Z}p(z|\\theta,x)\\log p(\\theta|x,z)\\,dz\n\\] は \\(\\theta,\\theta_0\\in\\Theta\\) について連続であるとする．このとき，EM アルゴリズムの出力 \\(\\{\\widehat{\\theta}_(j)\\}\\) は尤度 \\(p(\\theta|x)\\) の停留点に単調に収束する．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\nよって，EM アルゴリズムは局所解には収束する．\nしかし，常に尤度が単調増加するという性質上，局所解に囚われてしまった場合，そこから逃れることはないことになる．\n大域解に収束することを保証したい場合は，異なる初期値で複数回 EM アルゴリズムを実行するか，擬似除冷 (simulated annealing)12 などの別の手法を用いることを考える必要がある (Finch et al., 1989)．"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#em-アルゴリズムの実装gauss-有限混合モデルの場合",
    "href": "posts/2024/Computation/VI2.html#em-アルゴリズムの実装gauss-有限混合モデルの場合",
    "title": "変分推論２",
    "section": "3 EM アルゴリズムの実装（Gauss 有限混合モデルの場合）",
    "text": "3 EM アルゴリズムの実装（Gauss 有限混合モデルの場合）\n負担率に確率モデルを置いた場合，ソフト \\(K\\)-平均アルゴリズム は EM アルゴリズムになる．\nEM アルゴリズムは一般に多峰性に弱いことをここで示す．13\n\n3.1 Guass 有限混合モデル\nここでは，以下の，有限な混合モデルで，さらに混合される分布は正規であるものを考える：\n\n\n\n\n\n\n定義 (Gaussian finite mixture model)\n\n\n\n集合 \\([K]\\) 上に値を取る隠れ変数 \\(Z\\) の確率質量関数を \\((p_k)_{k=1}^K\\) とする． \\[\np(x;(\\mu_k),(\\Sigma_k),(p_k))=\\sum_{k=1}^K p_k\\phi(x;\\mu_k,\\Sigma_k)\n\\tag{3}\\] として定まるモデル \\((p_{(\\mu_k),(\\Sigma_k),(p_k)})\\) を \\(\\mathbb{R}^d\\) 上の Gauss 有限混合モデル という．\nただし，\\(\\phi(x\\); \\(\\mu\\), \\(\\sigma)\\) は \\(\\mathop{\\mathrm{N}}_d(\\mu,\\sigma^2)\\) の密度とした．\n\n\n式 3 は \\((X,Z)\\) 上の結合分布の族を表しており，そのパラメータは \\(\\theta:=((\\mu_k),(\\Sigma_k),(p_k))\\) である．さらに，\\(\\theta_k:=(\\mu_k,\\Sigma_k)\\) と定める．\n\n\n3.2 Gauss 有限混合モデルでの EM アルゴリズム\n節 2.2 での議論を，今回の Gauss 有限混合モデルに当てはめてみる．\n対数周辺尤度は\n\\[\n\\begin{align*}\n    \\log p(x|\\theta)&=\\log\\left(\\sum_{k=1}^K q_k\\frac{p_k\\phi(x|\\theta_k)}{q_k}\\right)\\\\\n    &\\ge\\sum_{k=1}^Kq_k\\log\\left(\\frac{p_k\\phi(x|\\theta_k)}{q_k}\\right)\\\\\n    &=:F(q_k,\\theta).\n\\end{align*}\n\\]\nという下界を持つ．\nこれに基づき，観測 \\(\\{x^{(n)}\\}_{n=1}^N\\) と混合 Gauss モデル 3.1 に対する EM アルゴリズムは次の２段階を繰り返す：\n\n\\(E\\)-ステップ： \\[\n\\begin{align*}\n     r_k^{(n)}&\\gets\\operatorname{P}[Z=k|x^{(n)},\\theta]\\\\\n     &=\\frac{p_k\\phi(x^{(n)}|\\theta_k)}{\\sum_{j=1}^Kp_j\\phi(x^{(n)}|\\theta_j)}\n\\end{align*}\n\\] を計算して，\\(F\\) に代入する．\n\\(M\\)-ステップ：\\(F\\) を \\(\\theta\\) について最大化する．これは，次の値を計算することに等しい：\n\n\\[\n\\mu_k\\gets\\frac{\\sum_{n=1}^Nr_k^{(n)}x^{(n)}}{\\sum_{n=1}^Nr_k^{(n)}}\n\\]\n\\[\n\\Sigma_k\\gets\\frac{\\sum_{n=1}^Nr_{k}^{(n)}(x^{(n)}-\\mu_k)(x^{(n)}-\\mu_k)^\\top}{\\sum_{n=1}^Nr_{k}^{(n)}}\n\\]\n\\[\np_k\\gets\\frac{\\sum_{n=1}^Nr_{k}^{(n)}}{N}\n\\]\n\n\n\n\n\n\n\n\n\\(E\\)-ステップの導出\n\n\n\n\n\n最初のステップは Bayes の定理から，\n\\[\n\\begin{align*}\n    \\operatorname{P}[Z=k|x,\\theta]&=\\frac{p(x|z=k,\\theta)p(z=k|\\theta)}{p(x|\\theta)}\\\\\n    &=\\frac{p_k\\phi(x|\\theta_k)}{\\sum_{j=1}^Kp_j\\phi(x|\\theta_j)}.\n\\end{align*}\n\\] の計算に帰着する．\\(\\{p_k\\}\\) が一様で，\\(\\sigma_k=\\sigma\\) も一様であるとき，これはソフト \\(K\\)-平均法における 負担率 \\(r_{kn}\\) に他ならない．このとき， \\[\n\\beta=\\frac{1}{2\\sigma^2}.\n\\]\nこうして，負担率とは，「データ点がそのクラスターに属するという事後確率」としての意味も持つことが判った．\n\n\n\n\n\n\n\n\n\n\\(M\\)-ステップの導出14\n\n\n\n\n\n2,3,4 はそれぞれ条件 \\[\n\\frac{\\partial }{\\partial \\mu_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] \\[\n\\frac{\\partial }{\\partial \\Sigma_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] \\[\n\\frac{\\partial }{\\partial p_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] から出る．\n最大化に Newton-Raphson 法を用いたとも捉えられる．15\n\n\n\n\n\n3.3 \\(K\\)-平均アルゴリズムとの対応\n\\(E\\)-ステップが assignment ステップ，\\(M\\)-ステップが update ステップに対応する．\nハード \\(K\\)-平均法は，歪み尺度 (distortion measure) \\[\nJ(r,\\mu):=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\|x_n-\\mu_k\\|^2\n\\] を \\(r,\\mu\\) のそれぞれについて逐次的に最小化する手法とも見れる．16\n\n\n3.4 Gauss 混合モデルの場合17\n\\(K=2\\) での Gauss 混合分布 \\[\np\\mathop{\\mathrm{N}}(\\mu_1,\\sigma^2)+(1-p)\\mathop{\\mathrm{N}}(\\mu_2,\\sigma^2),\n\\tag{4}\\] \\[\np=0.7,\\quad\\sigma=1,\n\\] を考える．未知パラメータは \\(\\theta:=(\\mu_1,\\mu_2)\\) である．\n実は，混合モデルでは，ここまで単純な例でさえ，尤度は多峰性を持つ．\n試しに，\\((\\mu_1,\\mu_2)=(0,3.1)\\) として 500 個のデータを生成し，モデル 4 が定める尤度をプロットしてみると，次の通りになる：\n\n\n\n\n\n\n\n\n\n真値 \\((\\mu_1,\\mu_2)=(0,3.1)\\) で確かに最大になるが，\\((\\mu_1,\\mu_2)=(2,-0.5)\\) 付近で極大値を取っていることがわかる．\n\n\n3.5 EM アルゴリズムの初期値依存性\nEM アルゴリズムはその初期値依存性からランダムな初期値から複数回実行してみる必要がある．モデル 4 の場合，その結果は次のようになる：\n\n\nCode\nclass EM_1d:\n    \"\"\"\n    Gauss 有限混合モデルに対する EM アルゴリズム\n\n    Parameters:\n    - K (int): 混合成分の数．デフォルトは2．\n    - max_iter (int): アルゴリズムの最大反復回数．デフォルトは100．\n    - tol (float): 収束の閾値．連続する反復での対数尤度の差がこの値以下になった場合，アルゴリズムは収束したと見なされる．デフォルトは1e-4．\n    \"\"\"\n\n    def __init__(self, K=2, init=None, max_iter=100, tol=1e-4):\n        self.K = K\n        self.max_iter = max_iter\n        self.tol = tol\n\n        self.means = None\n        self.variances = None\n        self.mixing_coefficients = None\n        self.log_likelihood_history = []\n        self.mean_history = []\n        self.initial_value = init\n\n    def expectation(self, X):\n        \"\"\"\n        E ステップ\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        N = X.shape[0]\n        r = np.zeros((N, self.K))\n        for k in range(self.K):\n            pdf = norm.pdf(X, self.means[k], np.sqrt(self.variances[k]))\n            r[:, k] = self.mixing_coefficients[k] * pdf\n        r /= r.sum(axis=1, keepdims=True)\n        return r\n\n    def maximization(self, X, r):\n        \"\"\"\n        M ステップ\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        - r (ndarray): 負担率．\n        \"\"\"\n        N = X.shape[0]\n        Nk = r.sum(axis=0)\n        self.means = (X.T @ r / Nk).T\n        self.variances = np.zeros(self.K)\n        for k in range(self.K):\n            diff = X - self.means[k]\n            self.variances[k] = (r[:, k] @ (diff ** 2)) / Nk[k]\n        self.mixing_coefficients = Nk / N\n\n    def compute_log_likelihood(self, X):\n        \"\"\"\n        対数尤度の計算\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        log_likelihood = 0\n        for x in X:\n            log_likelihood += np.log(np.sum([self.mixing_coefficients[k] * norm.pdf(x, self.means[k], np.sqrt(self.variances[k])) for k in range(self.K)]))\n        return log_likelihood\n    \n    def fit(self, X):\n        \"\"\"\n        EM アルゴリズムの実行\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        N = X.shape[0]\n        np.random.seed(42)\n\n        if self.initial_value is None:\n            random_indeces = np.random.choice(N, self.K, replace=False)\n            self.initial_value = X[random_indeces]\n        self.means = self.initial_value\n        self.initial_value = self.means\n        self.variances = np.ones(self.K)\n        self.mixing_coefficients = np.ones(self.K) / self.K\n\n        # 反復\n        for _ in range(self.max_iter):\n            r = self.expectation(X)\n            self.maximization(X, r)\n            log_likelihood = self.compute_log_likelihood(X)\n            self.log_likelihood_history.append(log_likelihood)\n            self.mean_history.append(self.means)\n\n            if len(self.log_likelihood_history) &gt;= 2 and np.abs(self.log_likelihood_history[-1] - self.log_likelihood_history[-2]) &lt; self.tol:\n                break\n        \n        return self"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#monte-carlo-法による解決",
    "href": "posts/2024/Computation/VI2.html#monte-carlo-法による解決",
    "title": "変分推論２",
    "section": "4 Monte Carlo 法による解決",
    "text": "4 Monte Carlo 法による解決\n式 1 の逆向きの関係 \\[\n\\begin{align*}\n    p(\\theta|x)&=\\int_\\mathcal{Z}p(z,\\theta|x)\\,dz\\\\\n    &=\\int_\\mathcal{Z}p(\\theta|z,x)p(z|x)\\,dz\n\\end{align*}\n\\] も成り立つという 階層構造 (hierarchical structure) を持つモデルにおいて，Bayes 推論が Gibbs サンプラーによって実行できる (Robert, 1996)．18\nこのような欠測モデルの文脈で Gibbs サンプラーを用いる手法は，データ拡張 の名前でも知られる (Tanner & Wong, 1987)．\n加えて，初期値依存性や局所解へのトラップが懸念されるという EM アルゴリズムの問題点を，MCMC はいずれも持ち合わせていない．\nさらに，混合数 \\(K\\) に関する検定も構成できる (Mengersen & Robert, 1996) など，Gibbs サンプラーひとつで確率モデルに関する種々の情報を取り出せる．\n最尤推定の代わりに Bayes 推定を行なっているため，データ数が少なくとも，過学習の問題が起こりにくいという利点もある．\nBayes 階層モデルは複雑なモデルに対する表現力が高く，地球科学をはじめとして多くの応用分野で使われている (Hrafnkelsson, 2023)．\n\n4.1 Gibbs サンプリング\n高次元な確率変数 \\((U_1,\\cdots,U_K)\\) のシミュレーションを行いたい場合，直接行うのではなく，条件付き分布 \\(p(u_k|u_{-k})\\) からのサンプリングを繰り返すことでこれを行うことが出来る．19\n\n任意の初期値 \\(U_1^{(0)},\\cdots,U_K^{(0)}\\) を与える．\n各 \\(k\\in[K]\\) について， \\[\nU_k^{(t)}\\sim p(u_k|U_{-k}^{(t-1)})\n\\] をサンプリングする．\n十分時間が経過した際，アルゴリズムの出力 \\((U^{(t)}_1,\\cdots,U^{(t)}_K)\\) は \\((U_1,\\cdots,U_K)\\) と同分布になる．\n\n実際，\\(\\{(U_1^{(t)},\\cdots,U_K^{(t)})\\}_{t\\in\\mathbb{N}}\\) はエルゴード的な Markov 連鎖を定め，定常分布 \\(p(U_1,\\cdots,U_K)\\) を持つ．\n\n\n\n\n\n\n命題 (Diebolt & Robert, 1994)20\n\n\n\n\\(p(u_1|u_2)\\) が正，または \\(p(u_2|u_1)\\) が正ならば，Markov 連鎖 \\(\\{U_1^{(t)}\\},\\{U_2^{(t)}\\}\\) はいずれもエルゴード的で，不変分布 \\(p(u_1|u_2),p(u_2|u_1)\\) を持つ．\n\n\n\n\n4.2 確率的 EM アルゴリズム\nGibbs サンプリングアルゴリズムは，EM アルゴリズム2.2の変形とみなせる：\n\n\\(E\\)-ステップ：EM アルゴリズムでは \\[\nQ(\\theta|\\vartheta,x):=(p_\\vartheta dz\\,|\\log p_\\theta)\n\\] を評価するところであったが，Gibbs サンプリングでは，\\(p(z|x,\\vartheta)\\) のサンプリングを行う．\n\\(M\\)-ステップ：EM アルゴリズムでは \\[\n\\mathop{\\mathrm{arg\\,max}}_{\\vartheta\\in\\Theta}Q(\\vartheta|\\theta,x)=(p_\\theta dz\\,|\\log p_\\vartheta)\n\\] を求めるところであったが，Gibbs サンプリングでは，\\(p(\\theta|z,x)\\) のサンプリングを行う．\n\nこれは \\(E\\)-ステップでの \\(Q\\) 関数の評価が困難であるとき，\\(p(z|x,\\theta)\\) からのサンプリングでこれを回避できるという美点もある．\n\n4.2.1 EM アルゴリズムへの部分的な適用：\\(E\\)-ステップ\nまたこの美点のみを用いて，\\(p(z|x,\\theta)\\) からサンプリングをして \\(Q\\) の Monte Carlo 推定量 \\[\nQ(\\theta)=\\frac{1}{M}\\sum_{m=1}^M\\log p(x,z^{(m)}|\\theta)\n\\] を計算し，\\(M\\)-ステップとしてこれを最大化して \\(\\{\\widehat{\\theta}_j\\}\\) を得るという 確率的 EM アルゴリズム (Stochastic EM) も考えられる (Celeux & Diebolt, 1985)．21\nこの場合，\\(\\{\\widehat{\\theta}_j\\}\\) は多くの場合エルゴード的な Markov 連鎖を定めるが，これがどこに収束するかの特定が難しい (Diebolt & Ip, 1996)．\n\n\n4.2.2 EM アルゴリズムへの部分的な適用：\\(M\\)-ステップ\nGibbs サンプリングの考え方を \\(M\\)-ステップにのみ導入し，\\(M\\)-ステップを完全に最大化するのではなく「条件付き最大化」に置き換えても，EM アルゴリズム本来の収束性は保たれる．\\(\\theta=(\\theta_1,\\theta_2)\\) と分解できる際に，いずれか片方ずつのみを最大化する，などである．これを ECM (Expectation Conditional Maximization) アルゴリズムという (Meng & Rubin, 1991), (Meng & Rubin, 1993)．\n\\(M\\)-ステップのみを確率的にすることで，EM アルゴリズムの局所解へのトラップを改善することができる．そのような手法の例に，SAME (State Augmentation for Marginal Estimation) (Doucet et al., 2002) などがある．\n\n\n\n4.3 諸言\n欠測モデル2.1のように，一般に グラフィカルモデル として知られる，局所的な関係のみから指定されるモデルや潜在変数を持つモデルでは，Gibbs サンプリングにより効率的に結合分布からサンプリングができる．\nMCMC はグラフィカルモデルを用いた Bayes 推論の，強力な武器である．22"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#footnotes",
    "href": "posts/2024/Computation/VI2.html#footnotes",
    "title": "変分推論２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Carmer, 1946, p. 498) によると，(Fisher, 1912) が初出であるが，以前に Gauss がその特別な形を用いていた．また，(Carmer, 1946, p. 499) での定義はこことは違い，尤度関数の停留点（＝尤度方程式の解）と定義している．↩︎\n(Robert & Casella, 2004, p. 174) 式(5.8)．↩︎\n(Robert & Casella, 2004, p. 174) 式(5.7)．\\(p\\) を完備化された尤度 (completed likelihood) ともいう．\\(X\\) を incomplete data, \\((X,Z)\\) を complete data ともいう (Bishop, 2006, pp. 433, p.440)．↩︎\n特に，隠れ Markov モデルの文脈では，EM アルゴリズムは Baum-Welch アルゴリズム とも呼ばれる (Chopin & Papaspiliopoulos, 2020, p. 70)．↩︎\nそれぞれ，(Robert, 2007, p. 330)，(Robert & Casella, 2004, p. 176)，(Hastie et al., 2009, p. 276)，↩︎\n正確には確率核 \\(Q:\\mathcal{X}\\times\\Theta\\to\\mathcal{Z}\\)．↩︎\nこの \\(F\\) は多く \\(Q\\) とも表され，\\(Q\\)-関数ともいう．\\(p(x|\\theta)\\) やその対数は 証拠 (evidence) ともいうので，\\(F\\) は 証拠下界 (ELBO: Evidence Lower BOund) ともいう．↩︎\n式変形は (Bishop, 2006, p. 450) も参照．この \\(p(z|x,\\theta)\\) は観測 \\(x\\) の下での，潜在変数 \\(z\\) の条件付き分布である．しかし，このように双方を最大化ステップと見る変分法的な見方が出来るのである (Wainwright & Jordan, 2008, pp. 153–154), (Neal & Hinton, 1998), (Hastie et al., 2009, p. 277)．よって，この \\(E\\)-ステップも，GEM のように，必ずしも完全な最大化を達成する必要はないことがわかる (Neal & Hinton, 1998), (Bishop, 2006, p. 454)．例えば変分近似を行った場合，変分 EM アルゴリズムができあがる (Wainwright & Jordan, 2008, p. 154)．↩︎\n(Johnston et al., 2024) にも言及あり．↩︎\n(Robert & Casella, 2004, p. 177) 定理5.15，(Robert, 2007, p. 334) 演習6.52．↩︎\n(Robert & Casella, 2004, p. 178) 定理5.16．↩︎\nこの用語は (甘利俊一, 1989, p. 141) の 模擬除冷 の表現に触発された．↩︎\n(Robert & Casella, 2004) も参照．↩︎\n(Bishop, 2006, pp. 436–439) も参照．↩︎\n(MacKay, 2003, p. 303)．↩︎\n(Bishop, 2006, p. 424)．↩︎\n(Robert & Casella, 2004, pp. 181–182) 例5.19 も参照．↩︎\n(Robert, 2007, p. 307) も参照．↩︎\n\\(u_{-k}:=u_{1:(k-1),(k+1):K}\\) とした．↩︎\n(Robert, 2007, p. 309) 補題6.3.6，(鎌谷研吾, 2020, p. 139) 定理5.7．↩︎\n(Robert & Casella, 2004, p. 200) 5.5.1 節も参照．↩︎\n(Robert, 2007, p. 318) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/OT.html",
    "href": "posts/2024/Computation/OT.html",
    "title": "最適輸送",
    "section": "",
    "text": "The goal of this practical session is to introduce computational optimal transport (OT) in Python. you will familiarize yourself with OT by: 1. Computing “exact” unregularized optimal transport, using the Python library POT (Python Optimal Transport). 2. Computing entropic optimal transport, using first your own version of the Sinkhorn algorithm, then the Python library OTT-JAX (https://github.com/ott-jax/ott).\nIn order to lighten the reading of the notebook, we place the functions allowing to perform plots, that are used multiple times, in the section below.\nCode\ndef plot_weighted_points(\n    ax,\n    x, a,\n    y, b,\n    title=None, x_label=None, y_label=None\n):\n  ax.scatter(x[:,0], x[:,1], s=5000*a, c='r', edgecolors='k', label=x_label)\n  ax.scatter(y[:,0], y[:,1], s=5000*b, c='b', edgecolors='k', label=y_label)\n  for i in range(np.shape(x)[0]):\n      ax.annotate(str(i+1), (x[i,0], x[i,1]),fontsize=30,color='black')\n  for i in range(np.shape(y)[0]):\n      ax.annotate(str(i+1), (y[i,0], y[i,1]),fontsize=30,color='black')\n  if x_label is not None or y_label is not None:\n    ax.legend(fontsize=20)\n  ax.axis('off')\n  ax.set_title(title, fontsize=25)\n\ndef plot_assignement(\n    ax,\n    x, a,\n    y, b,\n    optimal_plan,\n    title=None, x_label=None, y_label=None\n):\n  plot_weighted_points(\n    ax=ax,\n    x=x, a=a,\n    y=y, b=b,\n    title=None,\n    x_label=x_label, y_label=y_label\n  )\n  for i in range(optimal_plan.shape[0]):\n      for j in range(optimal_plan.shape[1]):\n          ax.plot([x[i,0], y[j,0]], [x[i,1], y[j,1]], c='k', lw=30*optimal_plan[i,j], alpha=0.8)\n  ax.axis('off')\n  ax.set_title(title, fontsize=30)\n\ndef plot_assignement_1D(\n    ax,\n    x, y,\n    title=None\n):\n  plot_points_1D(\n    ax,\n    x, y,\n    title=None\n  )\n  x_sorted = np.sort(x)\n  y_sorted = np.sort(y)\n  assert len(x) == len(y), \"x and y must have the same shape.\"\n  for i in range(len(x)):\n    ax.hlines(\n        y=0,\n        xmin=min(x_sorted[i], y_sorted[i]),\n        xmax=max(x_sorted[i], y_sorted[i]),\n        color='k',\n        lw=10\n    )\n  ax.axis('off')\n  ax.set_title(title, fontsize=30)\n\ndef plot_points_1D(\n    ax,\n    x, y,\n    title=None\n):\n  n = len(x)\n  a = np.ones(n) / n\n  ax.scatter(x, np.zeros(n), s=1000*a, c='r')\n  ax.scatter(y, np.zeros(n), s=1000*b, c='b')\n  min_val = min(np.min(x), np.min(y))\n  max_val = max(np.max(x), np.max(y))\n  for i in range(n):\n      ax.annotate(str(i+1), xy=(x[i], 0.005), size=30, color='r', ha='center')\n  for j in range(n):\n      ax.annotate(str(j+1), xy=(y[j], 0.005), size=30, color='b', ha='center')\n  ax.axis('off')\n  ax.plot(np.linspace(min_val, max_val, 10), np.zeros(10))\n  ax.set_title(title, fontsize=30)\n\ndef plot_consistency(\n    ax,\n    reg_strengths,\n    plan_diff, distance_diff\n):\n  ax[0].loglog(reg_strengths, plan_diff, lw=4)\n  ax[0].set_ylabel('$||P^* - P_\\epsilon^*||_F$', fontsize=25)\n  ax[1].tick_params(which='both', size=20)\n  ax[0].grid(ls='--')\n  ax[1].loglog(reg_strengths, distance_diff, lw=4)\n  ax[1].set_xlabel('Regularization Strength $\\epsilon$', fontsize=25)\n  ax[1].set_ylabel(r'$ 100 \\cdot \\frac{\\langle C, P^*_\\epsilon \\rangle - \\langle C, P^* \\rangle}{\\langle C, P^* \\rangle} $', fontsize=25)\n  ax[1].tick_params(which='both', size=20)\n  ax[1].grid(ls='--')"
  },
  {
    "objectID": "posts/2024/Computation/OT.html#i.1-reminders-on-discrete-optimal-transport",
    "href": "posts/2024/Computation/OT.html#i.1-reminders-on-discrete-optimal-transport",
    "title": "最適輸送",
    "section": "1.1 I.1 Reminders on Discrete Optimal Transport",
    "text": "1.1 I.1 Reminders on Discrete Optimal Transport\nOptimal Transport is a theory that allows us to compare two (weighted) points clouds \\((x, a)\\) and \\((y, b)\\), where \\(x \\in \\mathbb{R}^{n \\times d}\\) and \\(y \\in \\mathbb{R}^{m \\times d}\\) are the locations of the \\(n\\) (resp. \\(m\\)) points in dimension \\(d\\), and \\(a \\in \\mathbb{R}^n\\), \\(b \\in \\mathbb{R}^m\\) are the weights. We ask that the total weights sum to one, i.e. \\(\\sum_{i=1}^n a_i = \\sum_{j=1}^m b_j = 1\\).\nThe basic idea of Optimal Transport is to “transport” the mass located at points \\(x\\) to the mass located at points \\(y\\).\nLet us denote by \\(U(a,b) := \\left\\{ P \\in \\mathbb{R}^{n \\times m} \\,|\\, P \\geq 0, \\sum_{j=1}^m P_{ij} = a_i, \\sum_{i=1}^n P_{ij} = b_j\\right\\}\\) the set of admissible transport plans.\nIf \\(P \\in U(a,b)\\), the quantity \\(P_{ij} \\geq 0\\) should be regarded as the mass transported from point \\(x_i\\) to point \\(y_j\\). For this reason, it is called a transport plan.\nWe will also consider a cost function \\(c : \\mathbb{R}^d \\times \\mathbb{R}^d → \\mathbb{R}\\) and the associated cost matrix \\(C = [c(x_i, y_j)]_{1\\leq i,j \\leq n,m}\\in \\mathbb{R}^{n \\times m}\\), containing the pairwise costs between the points of each point cloud \\(x\\) and \\(y\\). The quantity \\(C_{ij}\\) should be regarded as the cost paid for transporting one unit of mass from \\(x_i\\) to \\(y_j\\). This cost is usually computed using the positions \\(x_i\\) and \\(y_j\\), for example \\(C_{ij} = \\|x_i - y_j\\|_2\\) or \\(C_{ij} = \\|x_i - y_j\\|_2^2\\), but may be more exotic in some cases.\nThen transporting mass according to \\(P \\in U(a,b)\\) has a total cost of \\(\\sum_{i,j=1}^n P_{ij} C_{ij}\\).\nIn “Optimal Transport”, there is the word Optimal. Indeed, we want to find a transport plan \\(P \\in U(a,b)\\) that will minimize its total cost. In other words, we want to solve \\[\n    \\min_{P \\in U(a,b)} \\sum_{i,j=1}^n C_{ij }P_{ij} = \\min_{P \\in U(a,b)} ⟨C, P⟩.\n\\]\nThis problem is a Linear Program: the objective function is linear in the variable \\(P\\), and the constraints are linear in \\(P\\). We can thus solve this problem using classical Linear Programming algorithms, such as the simplex algorithm.\nIf \\(P^*\\) is a solution to the Optimal Transport problem, we will say that \\(P^*\\) is an optimal transport plan between \\((x, a)\\) and \\((y, b)\\), and that \\(\\sum_{ij} P^*_{ij} C_{ij}\\) is the optimal transport distance between \\((x, a)\\) and \\((y, b)\\): it is the minimal amount of “energy” that is necessary to transport the initial mass located at points \\(x\\) to the target mass lcoated at points \\(y\\).\nUsually, we represent the weighted point clouds by probability measures \\(\\mu = \\sum_{i=1}^n a_i \\delta_{x_i}\\) and \\(\\nu = \\sum_{j=1}^m b_j \\delta_{y_j}\\). Solving the above problem, we then say that we solve the optimal transport problem between the measures \\(\\mu\\) and \\(\\nu\\). Moreover, we note: \\[\nW_c(\\mu, \\nu) = \\min_{P \\in U(a,b)} ⟨C, P⟩.\n\\]"
  },
  {
    "objectID": "posts/2024/Computation/OT.html#i.2-computing-optimal-croissant-transport",
    "href": "posts/2024/Computation/OT.html#i.2-computing-optimal-croissant-transport",
    "title": "最適輸送",
    "section": "1.2 I.2 Computing Optimal “Croissant” Transport",
    "text": "1.2 I.2 Computing Optimal “Croissant” Transport\n\n1.2.1 Install\nFirst, you need to install a few packages:\n\n\nCode\n%pip install POT\n%pip install cloudpickle\n\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting POT\n  Downloading POT-0.9.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (30 kB)\nRequirement already satisfied: numpy&gt;=1.16 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from POT) (1.25.2)\nRequirement already satisfied: scipy&gt;=1.6 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from POT) (1.11.4)\nDownloading POT-0.9.3-cp39-cp39-macosx_11_0_arm64.whl (290 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/290.8 kB ? eta -:--:--   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/290.8 kB 730.0 kB/s eta 0:00:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/290.8 kB 482.6 kB/s eta 0:00:01   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.4/290.8 kB 580.5 kB/s eta 0:00:01   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.7/290.8 kB 519.7 kB/s eta 0:00:01   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/290.8 kB 516.0 kB/s eta 0:00:01   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/290.8 kB 516.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 112.6/290.8 kB 436.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 143.4/290.8 kB 426.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 153.6/290.8 kB 419.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 174.1/290.8 kB 417.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 194.6/290.8 kB 446.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 194.6/290.8 kB 446.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 204.8/290.8 kB 393.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 225.3/290.8 kB 387.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 235.5/290.8 kB 385.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 235.5/290.8 kB 385.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 256.0/290.8 kB 367.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 256.0/290.8 kB 367.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 276.5/290.8 kB 358.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 276.5/290.8 kB 358.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.8/290.8 kB 332.4 kB/s eta 0:00:00\nInstalling collected packages: POT\nSuccessfully installed POT-0.9.3\nNote: you may need to restart the kernel to use updated packages.\nDefaulting to user installation because normal site-packages is not writeable\nCollecting cloudpickle\n  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\nDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nInstalling collected packages: cloudpickle\nSuccessfully installed cloudpickle-3.0.0\nNote: you may need to restart the kernel to use updated packages.\n\n\nThen, load the required packages.\n\n\nCode\nimport ot\nimport numpy as np\nimport os\nfrom typing import Callable\nimport matplotlib.pyplot as plt\n\n\nFinally, connect the notebok to your drive to load some data that will be used for the experiments.\n\n\n1.2.2 Formalization of the problem\nWe will solve the Bakeries/Cafés problem of transporting croissants from a number of Bakeries to Cafés.\nWe use fictional positions, production and sale numbers. We impose that the total croissant production is equal to the number of croissants sold, so that Bakeries and Cafés can be represented as measures with the same total mass. Then, up to normalization, they can be processed as probability measures.\nMathematically, we have acess to the position of the \\(m\\) Bakeries as points in \\(\\mathbb{R}^2\\) via \\(x \\in \\mathbb{R}^{n \\times 2}\\) and their respective production via \\(a \\in \\mathbb{R}^m\\) which describe the source point cloud. The Cafés where the croissants are sold are also defined by their position \\(y \\in \\mathbb{R}^{m \\times 2}\\) and the quantity of croissants sold by \\(b \\in \\mathbb{R}^{m}\\).\nAfterwards, the Bakeries are represented by the probability measure \\(\\mu = \\sum_{i=1}^n a_i \\delta_{x_i}\\) and the Cafés by \\(\\nu = \\sum_{j=1}^n b_j \\delta_{y_j}\\). Calculating the optimal assignment of the croissants delivered by the Bakeries to the Cafés remains to calculating the optimal transport between the probability measures \\(\\mu\\) and \\(\\nu\\).\nLet’s download the data and check that the total croissant production is equal to the number of croissants sold.\n\n\nCode\n# Load the data\nimport pickle\nfrom urllib.request import urlopen\nimport cloudpickle as cp\n\ncroissants = cp.load(urlopen('https://marcocuturi.net/data/croissants.pickle'))\n\nbakery_pos = croissants['bakery_pos']\nbakery_prod = croissants['bakery_prod']\ncafe_pos = croissants['cafe_pos']\ncafe_prod = croissants['cafe_prod']\n\nprint('Bakery productions =', bakery_prod)\nprint('Total number of croissants =', bakery_prod.sum())\nprint(\"\")\nprint('Café sales =', cafe_prod)\nprint('Total number of croissants sold =', cafe_prod.sum())\n\n\nBakery productions = [31. 48. 82. 30. 40. 48. 89. 73.]\nTotal number of croissants = 441.0\n\nCafé sales = [82. 88. 92. 88. 91.]\nTotal number of croissants sold = 441.0\n\n\nWe now normalize the weight vectors \\(a\\) and \\(b\\), i.e. the production and the sales, to deal with probability measures.\n\n\nCode\nbakery_prod = bakery_prod / bakery_prod.sum()\ncafe_prod = cafe_prod / cafe_prod.sum()\n\n\nThen, we plot the probability measures (the weighted point clouds) in \\(\\mathbb{R}^2\\).\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 8))\nplot_weighted_points(\n    ax,\n    x=bakery_pos,\n    a=bakery_prod,\n    x_label=\"Bakeries\",\n    y=cafe_pos,\n    y_label=\"Cafés\",\n    b=cafe_prod,\n    title=\"Bakeries and Cafés\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Solving the problem\nTo compute the optimal transport, we will consider three different costs:\n\n\\(\\ell_1\\): \\(c(x, y) = \\|x - y\\|_1\\) , (Manhattan distance)\n\\(\\ell_2\\): \\(c(x, y) = \\|x - y\\|_2\\), (Euclidean distance)\n\\(\\ell_2^2\\): \\(c(x, y) = \\|x - y\\|_2^2\\) (Squared-Euclidean distance)\n\nNote that we expect different optimal transport plans for different costs.\n\nQuestion:\n\nComplete the following function that computes a cost matrix \\(C\\) from two set of points \\(x, y\\) and a cost function \\(c\\). Compute the three costs matrices \\(C_{\\ell_1}, C_{\\ell_2}, C_{\\ell_2^2}\\in \\mathbb{R}^{n \\times m}\\) using that function.\nWhat cost should be used to minimize the total distance traveled by the driver that delivers croissants from Bakeries to Cafés?\n\nAnswer:\n\n\nCode\nbakery_pos\n\n\narray([[184.86464733, 201.8163543 ],\n       [449.3486663 , 168.40784664],\n       [245.41756746, 288.12166576],\n       [273.95400109, 364.68282915],\n       [494.58935376, 336.8424061 ],\n       [738.19305545, 238.70491485],\n       [736.10502372, 375.12298779],\n       [537.74200949, 482.30861653]])\n\n\n\n\nCode\ncafe_pos\n\n\narray([[302.08410452, 442.78633642],\n       [345.1162221 , 368.52123027],\n       [449.226184  , 201.94529124],\n       [454.08464888, 387.95508982],\n       [627.60125204, 408.7770822 ]])\n\n\n\n\nCode\ndef get_cost_matrix(\n    x: np.ndarray,\n    y: np.ndarray,\n    cost_fn: Callable\n) -&gt; np.ndarray:\n  \"\"\"\n  Compute the pairwise cost matrix between the n points in ``x`` and the m points in ``y``.\n  It should output a matrix of size n x m.\n  \"\"\"\n  return np.array([cost_fn(x_,y_) for x_ in x for y_ in y]).reshape(x.shape[0],y.shape[0])\n\n\n# compute cost matrices for different costs\nC_l1 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn= lambda x,y : sum(np.abs(x-y))\n  )\n\nC_l2 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn= lambda x,y : sum((x-y)**2)\n)\nC_l2_sq = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn= lambda x,y : sum(np.sqrt((x-y)**2))\n)\n\n# print shapes of cost matrices\nprint(\n    f\"Shape of C_l1: {C_l1.shape}\\n\"\n    f\"Shape of C_l2: {C_l2.shape}\\n\"\n    f\"Shape of C_l2_sq: {C_l2_sq.shape}\"\n)\n\n\nShape of C_l1: (8, 5)\nShape of C_l2: (8, 5)\nShape of C_l2_sq: (8, 5)\n\n\n\nWe can now compute the Optimal Transport plan to transport the croissants from the bakeries to the cafés, for the three different costs.\n\nQuestion:\n\nComplete the following fuction that takes as input the cost matrix \\(C\\) and the weights vectors \\(a\\) and \\(b\\) and outputs the optimal transport plan and the optimal transport cost using the ot.emd function. It has an option to display the results.\nUse that function to compute and display the optiaml plan and the optimal cost for \\(\\ell_1, \\ell_2\\) and \\(\\ell_2^2\\) geometries.\n\nRemark: See https://pythonot.github.io/ for informations on the ot.emd function.\nAnswer:\n\n\nCode\ndef compute_transport(\n    C: np.ndarray,\n    a: np.ndarray,\n    b: np.ndarray,\n    verbose: bool = False,\n):\n  \"\"\"\n  Compute the optimal transport plan and the optimal transport cost\n  for cost matrix ``C`` and weight vectors $a$ and $b$.\n  If ``verbose`` is set to True, it displays the results.\n  \"\"\"\n  optimal_plan = ot.emd(a,b,C)\n  optimal_cost = np.sum(optimal_plan * C)\n  if verbose:\n    print(\n        f\"optimal transport plan: \\n{optimal_plan}\"\n    )\n    print(\n        f\"transport cost: {optimal_cost}\"\n    )\n  return optimal_plan, optimal_cost\n\n\n\n\nCode\n# l1 geometry\nprint(\"l1 geometry:\")\noptimal_plan_l1_croissant, optimal_cost_l1_croissant = compute_transport(\n    C=C_l1,\n    a=bakery_prod,\n    b=cafe_prod,\n    verbose=True\n)\n\n\nl1 geometry:\noptimal transport plan: \n[[0.07029478 0.         0.         0.         0.        ]\n [0.         0.         0.10884354 0.         0.        ]\n [0.05442177 0.13151927 0.         0.         0.        ]\n [0.         0.06802721 0.         0.         0.        ]\n [0.         0.         0.         0.09070295 0.        ]\n [0.         0.         0.09977324 0.00453515 0.00453515]\n [0.         0.         0.         0.         0.20181406]\n [0.06122449 0.         0.         0.10430839 0.        ]]\ntransport cost: 177.28420815406028\n\n\n\n\nCode\n# l2 geometry\nprint(\"l2 geometry:\")\noptimal_plan_l2_croissant, optimal_cost_l2_croissant = compute_transport(\n    C=C_l2,\n    a=bakery_prod,\n    b=cafe_prod,\n    verbose=True\n)\n\n\nl2 geometry:\noptimal transport plan: \n[[0.         0.07029478 0.         0.         0.        ]\n [0.         0.         0.10884354 0.         0.        ]\n [0.11791383 0.06802721 0.         0.         0.        ]\n [0.06802721 0.         0.         0.         0.        ]\n [0.         0.06122449 0.         0.02947846 0.        ]\n [0.         0.         0.09977324 0.00453515 0.00453515]\n [0.         0.         0.         0.         0.20181406]\n [0.         0.         0.         0.16553288 0.        ]]\ntransport cost: 24576.370543882178\n\n\n\n\nCode\n# squared l2 geometry\nprint(\"squared l2 geometry:\")\noptimal_plan_l2_sq_croissant, optimal_cost_l2_sq_croissant = compute_transport(\n    C=C_l2_sq,\n    a=bakery_prod,\n    b=cafe_prod,\n    verbose=True\n)\n\n\nsquared l2 geometry:\noptimal transport plan: \n[[0.07029478 0.         0.         0.         0.        ]\n [0.         0.         0.10884354 0.         0.        ]\n [0.05442177 0.13151927 0.         0.         0.        ]\n [0.         0.06802721 0.         0.         0.        ]\n [0.         0.         0.         0.09070295 0.        ]\n [0.         0.         0.09977324 0.00453515 0.00453515]\n [0.         0.         0.         0.         0.20181406]\n [0.06122449 0.         0.         0.10430839 0.        ]]\ntransport cost: 177.28420815406028\n\n\n\nNow, we can visualize the assignement induced by each geometry.\n\n\nCode\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_l1_croissant,\n         optimal_plan_l2_croissant,\n         optimal_plan_l2_sq_croissant]\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.4 In dimension \\(d = 1\\)\nLet assume in this subsection that the cost is of the form \\(c(x, y) = \\|x - y\\|_p^q\\) with \\(p, q \\geq 1\\), which covers the costs we considered in the previous examples, and that the points are in \\(\\mathbb{R}\\), i.e. \\(x_1, ..., x_n, y_1, ... , y_n \\in \\mathbb{R}\\). Then, computing OT boils down to sorting the points. Indeed, for all costs of the above form, the optimal permutation between \\(x\\) and \\(y\\) is \\(\\sigma^* = \\sigma_x^{-1} \\circ \\sigma_y\\) where \\(\\sigma_x\\) is the permutation sorting the \\(x_i\\) and \\(\\sigma_y\\) the one sorting the \\(y_i\\). In particular, one has:\n\\[\nW_c(\\mu, \\nu) = \\frac{1}{n} \\sum_{i=1}^n c(x_i, y_{\\sigma_x^{-1} \\circ \\sigma_y(i)}) = \\frac{1}{n} \\sum_{i=1}^n c(x_{\\sigma_x(i)}, y_{\\sigma_y(i)})\n\\]\nThus, to compute the optimal transport cost, it is sufficient to sort \\(x\\) and \\(y\\).\nLet’s check this fact on an example, by comparing the transport cost obtained by sorting the points to the one obtained with the function ot.emd. To simplify, we generate points \\(x,y \\subset \\mathbb{R}\\) s.t. \\(x\\) is sorted, i.e. \\(\\sigma_x = I_d\\) and then \\(\\sigma^*=\\sigma_y\\). Therefore, computing the optimal assignement amounts to sort \\(y\\).\n\n\nCode\n# generate points\nn = 5\nx = np.arange(0, 2*n, 2) + .25 * np.random.normal(size=(n,))\na = np.ones(n) / n\ny = np.arange(1, 2*n+1, 2) + .25 * np.random.normal(size=(n,))\nnp.random.shuffle(y)\nb = np.ones(n) / n\n\n# plot points\nfig, ax = plt.subplots(figsize=(12, 6))\nplot_points_1D(\n    ax,\n    x, y,\n    title=\"1D points\"\n)\n\n\n\n\n\n\n\n\n\n\nQuestion:\n\nFor \\(\\ell_1\\) and \\(\\ell_2^2\\) geometries (\\(\\ell_2\\) and \\(\\ell_1\\) coincides on \\(\\mathbb{R}\\)), compute the optimal assignement and optimal transport cost by sorting \\(y\\). Put the assignement into a vector \\(s \\in \\mathbb{R}^n\\), s.t. \\(x_i\\) is mapped to \\(y_{s_i}\\), i.e. \\(s_i = \\sigma^*(i)\\). Is it different according to the geometry?\nPut now the assignment you obtained by sorting the points in the form of a transport plan \\(P^* \\in \\mathbb{R}^{n \\times n}\\). Check that you obtain the results with ot.emd.\n\nAnswer:\n\n\nCode\n# sort the points\ny_sorted = np.sort(y)\n\n# get optimal assignment as a vector\nassignment = np.argsort(y)\n\n# transform it to a transport plan\noptimal_plan = np.zeros((n,n))\nfor i, idx in enumerate(assignment):\n    optimal_plan[i, idx] = 1 / n\nprint(\n    f\"optimal transport plan obtained by sorting the points:\\n {optimal_plan}\"\n)\n\n# The result doesn't match the lecturer's\n\n\noptimal transport plan obtained by sorting the points:\n [[0.2 0.  0.  0.  0. ]\n [0.  0.  0.  0.2 0. ]\n [0.  0.  0.  0.  0.2]\n [0.  0.  0.2 0.  0. ]\n [0.  0.2 0.  0.  0. ]]\n\n\n\n\nCode\n# l1 geometry\nprint(\"l1 geometry:\")\nC_l1 = get_cost_matrix(\n    x=x, y=y,\n    cost_fn=lambda x,y: np.sum(np.abs(x - y))\n)\noptimal_plan_l1, optimal_cost_l1 = compute_transport(\n    C=C_l1,\n    a=a,\n    b=b,\n    verbose=True\n)\nprint(\n    f\"is it equal to the one obtained by sorting the points? \"\n    f\"{np.array_equal(optimal_plan_l1, optimal_plan)}\"\n)\n\n\nl1 geometry:\noptimal transport plan: \n[[0.2 0.  0.  0.  0. ]\n [0.  0.  0.  0.2 0. ]\n [0.  0.  0.  0.  0.2]\n [0.  0.  0.2 0.  0. ]\n [0.  0.2 0.  0.  0. ]]\ntransport cost: 0.7019708286541314\nis it equal to the one obtained by sorting the points? True\n\n\n\n\nCode\n# squared l2 geometry\n\ndef is_permutation(matrix):\n    \"\"\"\n    Check if a given matrix is a permutation matrix.\n    \"\"\"\n    n, m = matrix.shape\n    if n != m:\n        return False\n    \n    row_sum = np.sum(matrix, axis=1)\n    col_sum = np.sum(matrix, axis=0)\n    \n    return np.all(row_sum == 1) and np.all(col_sum == 1) and np.all((matrix == 0) | (matrix == 1))\n\nC_l2_sq = get_cost_matrix(\n    x=x, y=y,\n    cost_fn=lambda x,y: np.sum((x - y) ** 2)\n)\noptimal_plan_l2_sq, optimal_cost_l2_sq = compute_transport(\n    C=C_l2_sq,\n    a=a,\n    b=b,\n    verbose=True\n)\nprint(\n    f\"is permutation matrix? {is_permutation(optimal_plan_l2_sq)}\"\n)\nprint(\n    f\"is it equal to the one obtained by sorting the points? \"\n    f\"{np.array_equal(optimal_plan_l2_sq, optimal_plan)}\"\n)\n\n\noptimal transport plan: \n[[0.2 0.  0.  0.  0. ]\n [0.  0.  0.  0.2 0. ]\n [0.  0.  0.  0.  0.2]\n [0.  0.  0.2 0.  0. ]\n [0.  0.2 0.  0.  0. ]]\ntransport cost: 0.5722818049480772\nis permutation matrix? False\nis it equal to the one obtained by sorting the points? True\n\n\n\nFinally, one can plot the assignement.\n\n\nCode\nfig, ax = plt.subplots(figsize=(12, 6))\nplot_assignement_1D(\n    ax,\n    x, y,\n    title=\"1D assignement\"\n)\nplt.show()"
  },
  {
    "objectID": "posts/2024/Computation/OT.html#ii.1-reminders-on-sinkhorn-algorithm",
    "href": "posts/2024/Computation/OT.html#ii.1-reminders-on-sinkhorn-algorithm",
    "title": "最適輸送",
    "section": "2.1 II.1 Reminders on Sinkhorn Algorithm",
    "text": "2.1 II.1 Reminders on Sinkhorn Algorithm\n\n2.1.1 Adding negative entropy as a regularizer\nIn real ML applications, we often deal with large numbers of points. In this case, cubic complexity linear programming algorithms are too costly. This motivates (among other reasons) the regularized approach \\[\n    \\min_{P \\in \\mathcal{U}(a,b)} \\langle C, P \\rangle + \\epsilon \\sum_{ij} P_{ij} [ \\log(P_{ij}) - 1].\n\\] For \\(\\epsilon\\) is sufficiently small, one expects to recover an approximation of the original optimal transport plan.\n\n\n2.1.2 The Sinkhorn iterates\nIn order to solve this problem, one can remark that the optimality conditions imply that a solution \\(P_\\epsilon^*\\) necessarily is of the form \\(P_\\epsilon^* = \\text{diag}(u) \\, K \\, \\text{diag}(v)\\), where \\(K = \\exp(-C/\\epsilon)\\) and \\(u,v\\) are two non-negative vectors.\n\\(P_\\epsilon^*\\) should verify the constraints, i.e. \\(P_\\epsilon^* \\in U(a,b)\\), so that \\[\n    P_\\epsilon^* 1_m = a \\text{  and  } (P_\\epsilon^*)^T 1_n = b\n\\] which can be rewritten as \\[\n    u \\odot (Kv) = a \\text{  and  } v \\odot (K^T u) = b\n\\]\nThen Sinkhorn’s algorithm alternates between the resolution of these two equations, and reads at iteration \\(t\\): \\[\n    u^{t+1} \\leftarrow \\frac{a}{Kv^t} \\text{  and  } v^{t+1} \\leftarrow \\frac{b}{K^T u^{t+1}}\n\\]\n\n\n2.1.3 Initialization and convergence\nUsually, it starts from \\(v^{0} = \\mathrm{1}_m\\) and alternate the above updates until \\(\\|u^{t+1} \\odot (Kv^{t+1}) - a\\|_1 + \\|v^{t+1} \\odot (K^T u^{t+1}) - b\\|_1 \\leq \\tau\\), where \\(\\tau &gt; 0\\) is a fixed convergence threshold. Actually, since at the end of each iteration, one exactly has \\(v^{t+1} \\odot (K^T u^{t+1}) = b\\), it just remains to test if \\(\\|u^{t+1} \\odot (Kv^{t+1}) - a\\|_1 \\leq \\tau\\).\nFrom an entropic optimal transport plan \\(P^*_\\epsilon\\), we can approximate the optimal transport cost by \\(\\sum_{i,j=1}^n P^*_{\\epsilon_{ij}} C_{ij} = ⟨C, P^*_\\epsilon⟩\\). For the rest of the section, we call this quantity the entropic optimal transport cost."
  },
  {
    "objectID": "posts/2024/Computation/OT.html#ii.2-using-your-own-sinkhorn",
    "href": "posts/2024/Computation/OT.html#ii.2-using-your-own-sinkhorn",
    "title": "最適輸送",
    "section": "2.2 II.2 Using your own Sinkhorn",
    "text": "2.2 II.2 Using your own Sinkhorn\n\n2.2.1 Sinkhorn Implementation\nIn this section, you will implement your own version of the Sinkhorn Algorithm.\n\nQuestion: Complete the following Sinkhorn algorithm, by:\n\nComputing the kernel matrix \\(K = \\exp(-C / \\epsilon)\\),\nStarting from \\(v^{0} = \\mathrm{1}_m\\),\nAlternating the updates \\(u^{t+1} \\odot (Kv^t) = a\\) and \\(v^{t+1} \\odot (K^T u^{t+1}) = b\\),\nDeclaring convergence when \\(\\|u^t \\odot (Kv^t) - a\\|_1 + \\|v^t \\odot (K^T u^t) - b\\|_1 \\leq \\tau\\).\n\nRemark: you should also use also a maximum number of iterations max_iter, to stop the algorithm after a fixed number of iterations if the convergence is not reached.\nAnswer:\n\n\nCode\ndef sinkhorn(\n    a: np.ndarray,\n    b: np.ndarray,\n    C: np.ndarray,\n    epsilon: float,\n    max_iters: int = 100,\n    tau: float = 1e-4\n) -&gt; np.ndarray:\n    \"\"\"\n    Sinnkhorn's algorithm. It should output the optimal transport plan.\n    \"\"\"\n\n    K = np.exp( -C / epsilon )\n    n, m = a.shape[0], b.shape[0]\n    v = np.ones((m,))\n    for _ in range(max_iters):\n        u = a / K.dot(v)\n        v = b / K.transpose().dot(u)\n    return u[:,None] * v[None,:] * K  # u_i, v_j, K_ij\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=1)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\n[0.2 0.2 0.2 0.2 0.2]\n[0.19683035 0.19891596 0.20045116 0.20138441 0.20241811]\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=1, max_iters=1000)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\n[0.2 0.2 0.2 0.2 0.2]\n[0.19999964 0.19999997 0.20000008 0.20000013 0.20000018]\n\n\n\n\nCode\ndef sinkhorn(\n    a: np.ndarray,\n    b: np.ndarray,\n    C: np.ndarray,\n    epsilon: float,\n    max_iters: int = 100,\n    tau: float = 1e-4\n) -&gt; np.ndarray:\n    \"\"\"\n    Sinnkhorn's algorithm. It should output the optimal transport plan.\n    \"\"\"\n\n    K = np.exp( -C / epsilon )\n    n, m = a.shape[0], b.shape[0]\n    v = np.ones((m,))\n    for i in range(max_iters):\n        u = a / K.dot(v)\n        v = b / K.transpose().dot(u)\n        if i % 10 == 0:\n            # compute row sum D(u) K D(v) = u * Kv\n            if np.sum(np.abs(u * K.dot(v) - a)) &lt; tau:\n                print('early termination: ' + str(i))\n                break\n    return u[:,None] * v[None,:] * K  # u_i, v_j, K_ij\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=1, max_iters=1000)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\nearly termination: 520\n[0.2 0.2 0.2 0.2 0.2]\n[0.19995703 0.19999617 0.20000967 0.20001566 0.20002147]\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=0.1, max_iters=10000)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\nearly termination: 3980\n[0.2 0.2 0.2 0.2 0.2]\n[0.19999999 0.19995007 0.19999999 0.20004994 0.20000002]\n\n\n\nNow, we can test the Sinkhorn algorithm on the “croissant” transport example.\n\nQuestion: * Complete the following fuction that takes as input the cost matrix \\(C\\) and the weights vectors \\(a\\) and \\(b\\) and outputs the entropic optimal transport plan and the entropic optimal transport cost using the sinkhorn function. As for the exact transport, it has an option to display the results. * Use that function on the croissant transport to compute and display the optimal plan and the optimal cost for the \\(\\ell_1, \\ell_2\\) and \\(\\ell_2^2\\) geometries. * Each time you run the Sinkhorn algorithm, you should use \\(\\epsilon = 0.1 \\cdot \\bar{C}\\), with \\(\\bar{C} = \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m C_{ij}\\) is the mean of the cost matrix. It remains to adapt the \\(\\epsilon\\) value according to the cost matrix, to control the magnitude of the entries of \\(C / \\epsilon\\). Why this strategy? What will happen if \\(\\epsilon\\) is too small compared to the entries of \\(C\\)?\nAnswer:\n\n\nCode\ndef compute_transport_sinkhorn(\n    C: np.ndarray,\n    a: np.ndarray,\n    b: np.ndarray,\n    epsilon: float,\n    max_iters: int = 10_000,\n    tau: float = 1e-4,\n    verbose: bool = False,\n):\n  \"\"\"\n  Compute the entropic optimal transport plan and the entropic optimal transport cost\n  for cost matrix ``C`` and weight vectors $a$ and $b$.\n  If ``verbose`` is set to True, it displays the results.\n  \"\"\"\n  optimal_plan_sinkhorn = sinkhorn(a, b, C, epsilon, max_iters, tau)\n  optimal_cost_sinkhorn = np.sum(optimal_plan_sinkhorn * C)\n  if verbose:\n    print(\n        f\"entropic optimal transport plan: \\n{optimal_plan_sinkhorn}\"\n    )\n    print(\n        f\"entropic transport cost: {optimal_cost_sinkhorn}\"\n    )\n  return optimal_plan_sinkhorn, optimal_cost_sinkhorn\n\n\n\n\nCode\n# l1 geometry\nprint(\"l1 geometry:\")\nC_l1 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn=lambda x,y: np.sum(np.abs(x - y))\n)\nepsilon = 1\noptimal_plan_sinkhorn_l1_croissant, optimal_cost_sinkhorn_l1_croissant = compute_transport_sinkhorn(\n    C=C_l1,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=True,\n)\n\n\nl1 geometry:\nearly termination: 5970\nentropic optimal transport plan: \n[[2.70428936e-002 4.32583290e-002 1.34214268e-047 1.83509051e-086\n  1.62880160e-235]\n [3.42773260e-084 1.30691077e-046 1.08827539e-001 1.90100995e-040\n  1.68731081e-189]\n [7.15328153e-002 1.14425257e-001 4.99347632e-122 4.85411038e-086\n  4.30844293e-235]\n [2.61705422e-002 4.18628990e-002 5.77469196e-189 1.77589404e-086\n  1.57625961e-235]\n [1.25908361e-049 4.80057848e-012 2.70172554e-084 9.07098043e-002\n  1.22406524e-115]\n [2.66904088e-052 1.01764013e-014 9.97892408e-002 1.92289196e-004\n  8.84651301e-003]\n [5.95876320e-051 4.18968529e-019 7.18872709e-119 4.29294956e-003\n  1.97502693e-001]\n [6.11947921e-002 7.27950530e-029 1.24902883e-128 1.04351442e-001\n  5.20381800e-060]]\nentropic transport cost: 177.27648952346257\n\n\n\n\nCode\nplt.imshow(optimal_plan_sinkhorn_l1_croissant)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# l2 geometry\nprint(\"l2 geometry:\")\nC_l2 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn=lambda x,y: np.linalg.norm(x - y, ord=2)\n)\nepsilon = np.mean(C_l2_sq) * 0.05 # compute the optimal value to avoid underflow\noptimal_plan_sinkhorn_l2_croissant, optimal_cost_sinkhorn_l2_croissant = compute_transport_sinkhorn(\n    C=C_l2,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=True\n)\n\n\nl2 geometry:\nearly termination: 7020\nentropic optimal transport plan: \n[[1.46504079e-002 5.56502260e-002 7.40541242e-072 3.32773896e-090\n  7.87177335e-233]\n [2.31223268e-072 9.35392699e-047 1.08836454e-001 7.14988318e-083\n  9.07714141e-184]\n [4.20605267e-002 1.43895972e-001 4.17834937e-102 1.86993999e-093\n  3.49162714e-240]\n [6.80326031e-002 4.60417773e-014 9.97752466e-153 1.38976468e-109\n  5.11767226e-259]\n [2.88900695e-023 2.87039283e-007 6.78893893e-055 9.07088796e-002\n  6.62649361e-106]\n [1.16058141e-027 2.33404495e-011 9.97803259e-002 4.49022688e-003\n  4.56609821e-003]\n [5.10518016e-050 1.41629037e-043 5.41176766e-068 2.80295221e-027\n  2.01783108e-001]\n [6.11975053e-002 2.46217455e-012 3.56278557e-101 1.04347379e-001\n  7.44656421e-057]]\nentropic transport cost: 139.5032457920226\n\n\n\n\nCode\n# squared l2 geometry\nprint(\"squared l2 geometry:\")\nC_l2_sq = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn=lambda x,y: np.sum((x - y) ** 2)\n)\nepsilon = np.mean(C_l2_sq) * 0.05 # compute the optimal value to avoid underflow\noptimal_plan_sinkhorn_l2_sq_croissant, optimal_cost_sinkhorn_l2_sq_croissant = compute_transport_sinkhorn(\n    C=C_l2_sq,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=True\n)\n\n\nsquared l2 geometry:\nearly termination: 390\nentropic optimal transport plan: \n[[9.15185856e-03 6.11459020e-02 2.53416007e-06 9.36830019e-11\n  5.88710931e-33]\n [1.98312263e-09 2.86298466e-05 1.08801507e-01 2.62245737e-07\n  1.22762517e-18]\n [1.02592189e-01 8.33635462e-02 3.95058637e-08 1.25018168e-08\n  7.16059834e-28]\n [6.36539082e-02 4.37874407e-03 9.20213996e-12 8.38044497e-09\n  1.80226054e-26]\n [1.15178824e-03 4.78786088e-02 4.40333811e-04 4.12385828e-02\n  1.04144097e-10]\n [1.27912352e-15 1.00116432e-09 9.93719135e-02 7.28353201e-04\n  8.73056135e-03]\n [5.51097456e-13 1.50024181e-09 4.51186568e-07 4.17827621e-03\n  1.97618514e-01]\n [9.39129734e-03 2.75105187e-03 4.51869851e-10 1.53400990e-01\n  1.31200779e-07]]\nentropic transport cost: 24883.330683517215\n\n\n\n\n\n2.2.2 The effect of \\(\\epsilon\\)\nNow we can display the transportation plans obtained with Sinkhorn’s algortihm, as we did for the exact OT.\n\n\nCode\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_sinkhorn_l1_croissant,\n         optimal_plan_sinkhorn_l2_croissant,\n         optimal_plan_sinkhorn_l2_sq_croissant]\n\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\n\n\n\n\n\n\n\nNote: There always is some transport at every edge in Sinkhorn algorithm’s output.\n\n\nCode\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_l1_croissant,\n         optimal_plan_l2_croissant,\n         optimal_plan_l2_sq_croissant]\n\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\n\n\n\n\n\n\n\nThe above transport plans are obtained for \\(\\epsilon = 0.1 \\cdot \\bar{C}\\). Let’s increase epsilon to \\(\\epsilon = 10 \\cdot \\bar{C}\\) and replot the optimal transport plans to visualize the effect of epsilon.\n\n\nCode\n# l1 geometry\nepsilon = 10 * np.mean(C_l1)\noptimal_plan_sinkhorn_l1_croissant, optimal_cost_sinkhorn_l1_croissant = compute_transport_sinkhorn(\n    C=C_l1,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=False,\n)\n\n# l2 geometry\nepsilon = 10 * np.mean(C_l2)\noptimal_plan_sinkhorn_l2_croissant, optimal_cost_sinkhorn_l2_croissant = compute_transport_sinkhorn(\n    C=C_l2,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=False\n)\n\n# squared l2 geometry\nepsilon = 10 * np.mean(C_l2_sq)\noptimal_plan_sinkhorn_l2_sq_croissant, optimal_cost_sinkhorn_l2_sq_croissant = compute_transport_sinkhorn(\n    C=C_l2_sq,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=False\n)\n\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_l1_croissant,\n         optimal_plan_l2_croissant,\n         optimal_plan_l2_sq_croissant]\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\nearly termination: 10\nearly termination: 10\nearly termination: 10\n\n\n\n\n\n\n\n\n\nNote: If the epsilon is large, the distribution is close to uniform.\n\nQuestion: What do you observe in relation to the transport plans obtained for the exact optimal transport?\nAnswer:\n\n\n\n2.2.3 Sinkhorn consistency\nWe now show that this Sinkhorn algorithm is consistent with classical optimal transport, using the “croissant” transport example and focusing on the \\(\\ell_2\\) cost.\n\nQuestion: Complete the following code to compute, for various \\(\\epsilon'\\), values on a regular grid: * Set \\(\\epsilon = \\epsilon' \\cdot \\bar{C}\\), * The deviation of the entropic optimal plan \\(P^*_\\epsilon\\) to the exact optimal plan \\(P^*\\), namely \\(\\|P^*_\\epsilon - P^*\\|_2\\). * The deviation of the entropic optimal cost \\(\\langle C, P^*_\\epsilon \\rangle\\) to the exact optimal plan \\(\\langle C, P^*_\\epsilon \\rangle\\), namely: \\(\\langle C, P^*_\\epsilon \\rangle - \\langle C, P^* \\rangle\\).\nWe remind that the excat optimal transport plan for the \\(\\ell_2\\) cost is stored as variable optimal_plan_l2_croissant.\nAnswer:\n\n\nCode\nplan_diff = []\ndistance_diff = []\ngrid = np.linspace(0.01, 5, 100)\nfor epsilon_prime in grid:\n  epsilon = epsilon_prime * np.mean(C_l2)\n  optimal_plan_sinkhorn_l2_croissant, optimal_cost_sinkhorn_l2_croissant = compute_transport_sinkhorn(\n      C=C_l2,\n      a=bakery_prod,\n      b=cafe_prod,\n      epsilon=epsilon,\n      verbose=False\n  )\n  assert optimal_cost_sinkhorn_l2_croissant != np.nan, (\n      \"Optimal cost is nan due to numerical instabilities.\"\n  )\n  plan_diff.append(\n      np.sum(np.abs(optimal_plan_sinkhorn_l2_croissant - optimal_plan_l2_croissant))\n  )\n  distance_diff.append(\n      optimal_cost_sinkhorn_l2_croissant - optimal_cost_l2_croissant\n  )\n\n\nearly termination: 2460\nearly termination: 220\nearly termination: 50\nearly termination: 30\nearly termination: 20\nearly termination: 20\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\n\n\n\nNow, let’s plot the results.\n\n\nCode\nfig, ax = plt.subplots(2, 1, figsize=(16, 5*2))\nreg_strengths = np.mean(C_l2) * grid\nplot_consistency(\n    ax,\n    reg_strengths,\n    plan_diff,\n    distance_diff\n)\n\nplt.show()\n\n\n/Users/hirofumi48/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning:\n\nData has no positive values, and therefore cannot be log-scaled.\n\n\n\n\n\n\n\n\n\n\nNote: The result is different from the lecturer’s."
  },
  {
    "objectID": "posts/2024/Computation/OT.html#ii.3-using-ott",
    "href": "posts/2024/Computation/OT.html#ii.3-using-ott",
    "title": "最適輸送",
    "section": "2.3 II.3 Using OTT",
    "text": "2.3 II.3 Using OTT\n\n2.3.1 Install OTT\nFirst, you need to install OTT.\n\n\nCode\n%pip install ott-jax\n\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting ott-jax\n  Downloading ott_jax-0.4.5-py3-none-any.whl.metadata (20 kB)\nCollecting jax&gt;=0.4.0 (from ott-jax)\n  Downloading jax-0.4.25-py3-none-any.whl.metadata (24 kB)\nCollecting jaxopt&gt;=0.8 (from ott-jax)\n  Downloading jaxopt-0.8.3-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: numpy&gt;=1.20.0 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from ott-jax) (1.25.2)\nCollecting lineax&gt;=0.0.1 (from ott-jax)\n  Downloading lineax-0.0.4-py3-none-any.whl.metadata (17 kB)\nCollecting ml-dtypes&gt;=0.2.0 (from jax&gt;=0.4.0-&gt;ott-jax)\n  Downloading ml_dtypes-0.3.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (20 kB)\nCollecting opt-einsum (from jax&gt;=0.4.0-&gt;ott-jax)\n  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: scipy&gt;=1.9 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from jax&gt;=0.4.0-&gt;ott-jax) (1.11.4)\nRequirement already satisfied: importlib-metadata&gt;=4.6 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from jax&gt;=0.4.0-&gt;ott-jax) (6.8.0)\nCollecting jaxlib&gt;=0.1.69 (from jaxopt&gt;=0.8-&gt;ott-jax)\n  Downloading jaxlib-0.4.25-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\nCollecting equinox&gt;=0.11.0 (from lineax&gt;=0.0.1-&gt;ott-jax)\n  Downloading equinox-0.11.3-py3-none-any.whl.metadata (18 kB)\nCollecting jaxtyping&gt;=0.2.20 (from lineax&gt;=0.0.1-&gt;ott-jax)\n  Downloading jaxtyping-0.2.28-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from lineax&gt;=0.0.1-&gt;ott-jax) (4.8.0)\nRequirement already satisfied: zipp&gt;=0.5 in /Users/hirofumi48/Library/Python/3.9/lib/python/site-packages (from importlib-metadata&gt;=4.6-&gt;jax&gt;=0.4.0-&gt;ott-jax) (3.17.0)\nCollecting typeguard==2.13.3 (from jaxtyping&gt;=0.2.20-&gt;lineax&gt;=0.0.1-&gt;ott-jax)\n  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nDownloading ott_jax-0.4.5-py3-none-any.whl (248 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/249.0 kB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/249.0 kB ? eta -:--:--   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/249.0 kB ? eta -:--:--   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/249.0 kB 412.3 kB/s eta 0:00:01   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.4/249.0 kB 587.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.9/249.0 kB 648.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/249.0 kB 498.2 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 112.6/249.0 kB 562.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 112.6/249.0 kB 562.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 143.4/249.0 kB 447.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 143.4/249.0 kB 447.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 163.8/249.0 kB 425.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 174.1/249.0 kB 401.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 174.1/249.0 kB 401.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 194.6/249.0 kB 395.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 204.8/249.0 kB 391.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 204.8/249.0 kB 391.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 245.8/249.0 kB 395.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 249.0/249.0 kB 393.9 kB/s eta 0:00:00\nDownloading jax-0.4.25-py3-none-any.whl (1.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.8 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.8 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.8 MB ? eta -:--:--   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.8 MB 514.5 kB/s eta 0:00:04   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/1.8 MB 295.8 kB/s eta 0:00:06   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 384.9 kB/s eta 0:00:05   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 384.9 kB/s eta 0:00:05   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 305.6 kB/s eta 0:00:06   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 394.6 kB/s eta 0:00:05   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 394.6 kB/s eta 0:00:05   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 348.1 kB/s eta 0:00:05   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/1.8 MB 379.8 kB/s eta 0:00:05   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 361.1 kB/s eta 0:00:05   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 384.4 kB/s eta 0:00:05   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 384.4 kB/s eta 0:00:05   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 367.0 kB/s eta 0:00:05   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 343.6 kB/s eta 0:00:05   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 362.1 kB/s eta 0:00:05   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 356.1 kB/s eta 0:00:05   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/1.8 MB 339.4 kB/s eta 0:00:05   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/1.8 MB 353.5 kB/s eta 0:00:05   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/1.8 MB 359.1 kB/s eta 0:00:05   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/1.8 MB 347.6 kB/s eta 0:00:05   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/1.8 MB 355.5 kB/s eta 0:00:05   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/1.8 MB 349.9 kB/s eta 0:00:05   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/1.8 MB 363.0 kB/s eta 0:00:05   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.8 MB 371.4 kB/s eta 0:00:04   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.8 MB 362.1 kB/s eta 0:00:04   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.8 MB 367.0 kB/s eta 0:00:04   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.8 MB 366.3 kB/s eta 0:00:04   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.8 MB 373.2 kB/s eta 0:00:04   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/1.8 MB 373.2 kB/s eta 0:00:04   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/1.8 MB 377.8 kB/s eta 0:00:04   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/1.8 MB 381.1 kB/s eta 0:00:04   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/1.8 MB 382.3 kB/s eta 0:00:04   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/1.8 MB 384.4 kB/s eta 0:00:04   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/1.8 MB 396.9 kB/s eta 0:00:04   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/1.8 MB 396.9 kB/s eta 0:00:04   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/1.8 MB 406.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/1.8 MB 412.2 kB/s eta 0:00:03   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/1.8 MB 412.2 kB/s eta 0:00:03   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/1.8 MB 401.6 kB/s eta 0:00:03   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/1.8 MB 407.4 kB/s eta 0:00:03   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/1.8 MB 410.4 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/1.8 MB 413.3 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/1.8 MB 409.7 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 0.7/1.8 MB 409.9 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 0.7/1.8 MB 410.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 0.7/1.8 MB 410.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 0.8/1.8 MB 406.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 0.8/1.8 MB 411.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 0.8/1.8 MB 411.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 0.8/1.8 MB 407.0 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 0.8/1.8 MB 403.3 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 0.8/1.8 MB 403.6 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 0.9/1.8 MB 409.3 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 0.9/1.8 MB 412.9 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 0.9/1.8 MB 413.4 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 0.9/1.8 MB 421.7 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 1.0/1.8 MB 420.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 1.0/1.8 MB 424.7 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 1.0/1.8 MB 416.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 1.0/1.8 MB 419.4 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 1.0/1.8 MB 421.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 1.0/1.8 MB 416.8 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 1.1/1.8 MB 420.4 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 1.1/1.8 MB 420.4 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 1.1/1.8 MB 409.6 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 1.1/1.8 MB 409.6 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 1.1/1.8 MB 409.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 1.1/1.8 MB 408.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 1.1/1.8 MB 405.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 1.1/1.8 MB 405.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 1.1/1.8 MB 405.5 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 1.1/1.8 MB 405.5 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 1.1/1.8 MB 398.6 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 1.2/1.8 MB 398.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 1.2/1.8 MB 398.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 1.2/1.8 MB 396.8 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 1.2/1.8 MB 396.8 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 1.2/1.8 MB 388.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 1.2/1.8 MB 387.4 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 1.2/1.8 MB 387.4 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 1.2/1.8 MB 387.8 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 1.3/1.8 MB 390.7 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 1.3/1.8 MB 386.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 1.3/1.8 MB 386.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 1.3/1.8 MB 386.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 1.3/1.8 MB 380.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 1.3/1.8 MB 380.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 1.3/1.8 MB 374.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 1.3/1.8 MB 374.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 1.3/1.8 MB 374.8 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 1.4/1.8 MB 374.6 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 1.4/1.8 MB 371.0 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 1.4/1.8 MB 372.0 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 1.4/1.8 MB 373.0 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 1.4/1.8 MB 373.0 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 1.4/1.8 MB 369.2 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 1.4/1.8 MB 370.5 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 1.4/1.8 MB 368.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 1.4/1.8 MB 368.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 1.5/1.8 MB 367.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 1.5/1.8 MB 368.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 1.5/1.8 MB 366.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 1.5/1.8 MB 366.2 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 1.5/1.8 MB 366.2 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 1.5/1.8 MB 364.2 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 1.5/1.8 MB 365.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 1.6/1.8 MB 364.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 1.6/1.8 MB 364.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 1.6/1.8 MB 367.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 1.6/1.8 MB 372.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 1.6/1.8 MB 372.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 1.7/1.8 MB 373.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 1.7/1.8 MB 376.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 1.7/1.8 MB 374.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 1.7/1.8 MB 380.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 1.8/1.8 MB 382.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 1.8/1.8 MB 382.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 1.8/1.8 MB 384.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 386.2 kB/s eta 0:00:00\nDownloading jaxopt-0.8.3-py3-none-any.whl (172 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/172.3 kB ? eta -:--:--   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/172.3 kB 958.6 kB/s eta 0:00:01   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/172.3 kB 548.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 71.7/172.3 kB 574.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 92.2/172.3 kB 658.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 122.9/172.3 kB 674.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 163.8/172.3 kB 679.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.3/172.3 kB 691.0 kB/s eta 0:00:00\nDownloading lineax-0.0.4-py3-none-any.whl (65 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/65.3 kB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/65.3 kB ? eta -:--:--   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/65.3 kB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.7/65.3 kB 639.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 61.4/65.3 kB 611.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.3/65.3 kB 598.2 kB/s eta 0:00:00\nDownloading equinox-0.11.3-py3-none-any.whl (167 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/167.9 kB ? eta -:--:--   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/167.9 kB 1.1 MB/s eta 0:00:01   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/167.9 kB 674.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 71.7/167.9 kB 655.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 112.6/167.9 kB 721.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 122.9/167.9 kB 678.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 122.9/167.9 kB 678.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 163.8/167.9 kB 600.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 kB 597.9 kB/s eta 0:00:00\nDownloading jaxlib-0.4.25-cp39-cp39-macosx_11_0_arm64.whl (66.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/66.5 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/66.5 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/66.5 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/66.5 MB 695.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/66.5 MB 729.8 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/66.5 MB 684.0 kB/s eta 0:01:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/66.5 MB 732.6 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/66.5 MB 678.8 kB/s eta 0:01:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/66.5 MB 682.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/66.5 MB 686.2 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.2/66.5 MB 697.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/66.5 MB 737.5 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/66.5 MB 695.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/66.5 MB 680.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/66.5 MB 672.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/66.5 MB 642.0 kB/s eta 0:01:44   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/66.5 MB 643.7 kB/s eta 0:01:43   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/66.5 MB 651.3 kB/s eta 0:01:42   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/66.5 MB 617.9 kB/s eta 0:01:47   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.4/66.5 MB 624.5 kB/s eta 0:01:46   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 627.3 kB/s eta 0:01:46   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 616.3 kB/s eta 0:01:48   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 614.7 kB/s eta 0:01:48   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 610.3 kB/s eta 0:01:49   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 610.3 kB/s eta 0:01:49   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 610.3 kB/s eta 0:01:49   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 610.3 kB/s eta 0:01:49   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 610.3 kB/s eta 0:01:49   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 610.3 kB/s eta 0:01:49   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.5/66.5 MB 503.5 kB/s eta 0:02:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/66.5 MB 505.2 kB/s eta 0:02:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/66.5 MB 527.7 kB/s eta 0:02:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/66.5 MB 527.1 kB/s eta 0:02:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.6/66.5 MB 530.5 kB/s eta 0:02:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 528.0 kB/s eta 0:02:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 523.6 kB/s eta 0:02:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/66.5 MB 513.2 kB/s eta 0:02:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 416.0 kB/s eta 0:02:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 416.0 kB/s eta 0:02:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 416.0 kB/s eta 0:02:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 416.0 kB/s eta 0:02:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 416.0 kB/s eta 0:02:38   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 386.8 kB/s eta 0:02:50   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 394.8 kB/s eta 0:02:47   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.8/66.5 MB 394.8 kB/s eta 0:02:47   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.9/66.5 MB 397.9 kB/s eta 0:02:45   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 434.0 kB/s eta 0:02:31   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 430.9 kB/s eta 0:02:33   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 429.4 kB/s eta 0:02:33   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 427.1 kB/s eta 0:02:34   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 427.1 kB/s eta 0:02:34   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 419.8 kB/s eta 0:02:36   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/66.5 MB 419.8 kB/s eta 0:02:36   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 419.7 kB/s eta 0:02:36   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 414.9 kB/s eta 0:02:38   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 414.9 kB/s eta 0:02:38   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 412.4 kB/s eta 0:02:39   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 411.8 kB/s eta 0:02:39   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 411.8 kB/s eta 0:02:39   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 407.1 kB/s eta 0:02:41   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 407.1 kB/s eta 0:02:41   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 400.8 kB/s eta 0:02:44   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 397.8 kB/s eta 0:02:45   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/66.5 MB 397.8 kB/s eta 0:02:45   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 392.4 kB/s eta 0:02:47   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 392.4 kB/s eta 0:02:47   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 392.4 kB/s eta 0:02:47   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 392.4 kB/s eta 0:02:47   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 384.0 kB/s eta 0:02:51   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 383.6 kB/s eta 0:02:51   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 383.6 kB/s eta 0:02:51   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 378.2 kB/s eta 0:02:53   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 378.2 kB/s eta 0:02:53   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 375.7 kB/s eta 0:02:54   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/66.5 MB 375.7 kB/s eta 0:02:54   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 373.0 kB/s eta 0:02:55   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 373.0 kB/s eta 0:02:55   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 367.1 kB/s eta 0:02:58   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 367.1 kB/s eta 0:02:58   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 367.1 kB/s eta 0:02:58   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 361.2 kB/s eta 0:03:01   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 361.2 kB/s eta 0:03:01   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 357.9 kB/s eta 0:03:03   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 358.0 kB/s eta 0:03:02   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/66.5 MB 358.0 kB/s eta 0:03:02   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 358.6 kB/s eta 0:03:02   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 358.6 kB/s eta 0:03:02   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 352.0 kB/s eta 0:03:06   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 354.1 kB/s eta 0:03:04   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 354.1 kB/s eta 0:03:04   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 351.5 kB/s eta 0:03:06   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 351.5 kB/s eta 0:03:06   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 347.6 kB/s eta 0:03:08   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 347.6 kB/s eta 0:03:08   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 348.3 kB/s eta 0:03:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/66.5 MB 348.3 kB/s eta 0:03:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/66.5 MB 348.1 kB/s eta 0:03:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/66.5 MB 347.4 kB/s eta 0:03:08   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/66.5 MB 347.4 kB/s eta 0:03:08   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/66.5 MB 346.2 kB/s eta 0:03:08   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/66.5 MB 348.5 kB/s eta 0:03:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/66.5 MB 347.6 kB/s eta 0:03:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/66.5 MB 349.3 kB/s eta 0:03:06   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/66.5 MB 355.2 kB/s eta 0:03:03   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/66.5 MB 355.4 kB/s eta 0:03:03   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/66.5 MB 358.3 kB/s eta 0:03:01   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/66.5 MB 363.6 kB/s eta 0:02:59   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/66.5 MB 369.1 kB/s eta 0:02:56   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 372.8 kB/s eta 0:02:54   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 374.8 kB/s eta 0:02:53   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 373.8 kB/s eta 0:02:54   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 373.8 kB/s eta 0:02:54   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 372.0 kB/s eta 0:02:54   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 370.6 kB/s eta 0:02:55   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/66.5 MB 372.7 kB/s eta 0:02:54   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/66.5 MB 375.7 kB/s eta 0:02:52   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/66.5 MB 378.3 kB/s eta 0:02:51   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/66.5 MB 376.6 kB/s eta 0:02:52   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/66.5 MB 379.8 kB/s eta 0:02:50   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/66.5 MB 379.7 kB/s eta 0:02:50   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/66.5 MB 381.8 kB/s eta 0:02:49   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/66.5 MB 384.0 kB/s eta 0:02:48   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/66.5 MB 385.7 kB/s eta 0:02:48   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/66.5 MB 387.4 kB/s eta 0:02:47   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/66.5 MB 387.3 kB/s eta 0:02:47   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/66.5 MB 391.6 kB/s eta 0:02:45   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/66.5 MB 392.0 kB/s eta 0:02:45   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/66.5 MB 393.4 kB/s eta 0:02:44   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/66.5 MB 391.4 kB/s eta 0:02:45   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/66.5 MB 392.4 kB/s eta 0:02:44   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/66.5 MB 393.9 kB/s eta 0:02:44   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/66.5 MB 396.8 kB/s eta 0:02:42   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/66.5 MB 396.8 kB/s eta 0:02:42   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/66.5 MB 397.1 kB/s eta 0:02:42   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/66.5 MB 398.1 kB/s eta 0:02:42   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/66.5 MB 400.2 kB/s eta 0:02:41   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/66.5 MB 399.8 kB/s eta 0:02:41   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/66.5 MB 403.7 kB/s eta 0:02:39   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/66.5 MB 403.7 kB/s eta 0:02:39   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 402.7 kB/s eta 0:02:40   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 402.7 kB/s eta 0:02:40   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 402.7 kB/s eta 0:02:40   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 398.2 kB/s eta 0:02:41   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 398.2 kB/s eta 0:02:41   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 395.9 kB/s eta 0:02:42   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 395.9 kB/s eta 0:02:42   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 393.2 kB/s eta 0:02:43   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/66.5 MB 393.2 kB/s eta 0:02:43   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/66.5 MB 394.6 kB/s eta 0:02:43   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/66.5 MB 398.3 kB/s eta 0:02:41   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/66.5 MB 397.6 kB/s eta 0:02:41   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/66.5 MB 398.8 kB/s eta 0:02:41   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/66.5 MB 400.2 kB/s eta 0:02:40   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/66.5 MB 402.0 kB/s eta 0:02:39   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/66.5 MB 403.0 kB/s eta 0:02:39   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/66.5 MB 403.2 kB/s eta 0:02:39   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/66.5 MB 406.0 kB/s eta 0:02:38   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/66.5 MB 406.7 kB/s eta 0:02:37   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/66.5 MB 407.9 kB/s eta 0:02:37   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/66.5 MB 409.7 kB/s eta 0:02:36   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/66.5 MB 410.2 kB/s eta 0:02:36   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/66.5 MB 413.1 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/66.5 MB 414.8 kB/s eta 0:02:34   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/66.5 MB 414.8 kB/s eta 0:02:34   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/66.5 MB 411.7 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 412.8 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 412.8 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 411.0 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 411.0 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 409.5 kB/s eta 0:02:36   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 409.4 kB/s eta 0:02:36   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/66.5 MB 409.6 kB/s eta 0:02:36   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/66.5 MB 410.7 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/66.5 MB 410.0 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/66.5 MB 411.1 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/66.5 MB 409.9 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/66.5 MB 409.4 kB/s eta 0:02:36   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/66.5 MB 410.3 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/66.5 MB 411.5 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/66.5 MB 410.7 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/66.5 MB 410.7 kB/s eta 0:02:35   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/66.5 MB 411.9 kB/s eta 0:02:34   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/66.5 MB 412.0 kB/s eta 0:02:34   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/66.5 MB 412.6 kB/s eta 0:02:34   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/66.5 MB 411.4 kB/s eta 0:02:34   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/66.5 MB 414.4 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/66.5 MB 413.6 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/66.5 MB 413.9 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/66.5 MB 413.8 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/66.5 MB 414.8 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/66.5 MB 414.3 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/66.5 MB 414.3 kB/s eta 0:02:33   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/66.5 MB 414.2 kB/s eta 0:02:33   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/66.5 MB 415.8 kB/s eta 0:02:32   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/66.5 MB 417.4 kB/s eta 0:02:32   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/66.5 MB 419.3 kB/s eta 0:02:31   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/66.5 MB 422.5 kB/s eta 0:02:30   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/66.5 MB 422.8 kB/s eta 0:02:30   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/66.5 MB 423.7 kB/s eta 0:02:29   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/66.5 MB 426.5 kB/s eta 0:02:28   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/66.5 MB 428.2 kB/s eta 0:02:27   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/66.5 MB 430.2 kB/s eta 0:02:27   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/66.5 MB 432.2 kB/s eta 0:02:26   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/66.5 MB 433.6 kB/s eta 0:02:25   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/66.5 MB 434.2 kB/s eta 0:02:25   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/66.5 MB 434.2 kB/s eta 0:02:25   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/66.5 MB 437.5 kB/s eta 0:02:24   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/66.5 MB 438.7 kB/s eta 0:02:23   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/66.5 MB 437.8 kB/s eta 0:02:24   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/66.5 MB 440.3 kB/s eta 0:02:23   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/66.5 MB 440.3 kB/s eta 0:02:23   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/66.5 MB 440.6 kB/s eta 0:02:23   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/66.5 MB 441.2 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/66.5 MB 441.9 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/66.5 MB 442.3 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/66.5 MB 442.3 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/66.5 MB 440.5 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/66.5 MB 441.8 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/66.5 MB 441.8 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/66.5 MB 442.1 kB/s eta 0:02:22   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/66.5 MB 443.3 kB/s eta 0:02:21   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/66.5 MB 444.1 kB/s eta 0:02:21   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/66.5 MB 445.0 kB/s eta 0:02:21   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/66.5 MB 444.1 kB/s eta 0:02:21   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.1/66.5 MB 446.8 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/66.5 MB 446.7 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/66.5 MB 446.6 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/66.5 MB 447.7 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/66.5 MB 448.8 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/66.5 MB 448.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 447.9 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 447.1 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 447.7 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 448.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/66.5 MB 449.6 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 437.1 kB/s eta 0:02:23   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 438.1 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/66.5 MB 429.4 kB/s eta 0:02:25   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/66.5 MB 438.4 kB/s eta 0:02:22   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/66.5 MB 439.7 kB/s eta 0:02:21   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/66.5 MB 442.9 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/66.5 MB 444.5 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/66.5 MB 444.5 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/66.5 MB 444.3 kB/s eta 0:02:20   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/66.5 MB 445.9 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/66.5 MB 447.2 kB/s eta 0:02:19   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/66.5 MB 448.6 kB/s eta 0:02:18   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/66.5 MB 449.9 kB/s eta 0:02:18   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/66.5 MB 451.0 kB/s eta 0:02:17   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/66.5 MB 452.4 kB/s eta 0:02:17   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/66.5 MB 451.4 kB/s eta 0:02:17   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/66.5 MB 453.6 kB/s eta 0:02:16   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/66.5 MB 454.3 kB/s eta 0:02:16   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/66.5 MB 454.2 kB/s eta 0:02:16   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 454.5 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 455.2 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 455.2 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 455.2 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 455.2 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 451.2 kB/s eta 0:02:17   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 450.7 kB/s eta 0:02:17   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/66.5 MB 451.1 kB/s eta 0:02:17   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/66.5 MB 451.0 kB/s eta 0:02:17   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/66.5 MB 451.9 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/66.5 MB 452.8 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/66.5 MB 453.6 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/66.5 MB 453.3 kB/s eta 0:02:16   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/66.5 MB 455.7 kB/s eta 0:02:15   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/66.5 MB 457.4 kB/s eta 0:02:14   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/66.5 MB 456.8 kB/s eta 0:02:15   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/66.5 MB 457.5 kB/s eta 0:02:14   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/66.5 MB 457.1 kB/s eta 0:02:14   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/66.5 MB 457.7 kB/s eta 0:02:14   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/66.5 MB 457.8 kB/s eta 0:02:14   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/66.5 MB 458.6 kB/s eta 0:02:14   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/66.5 MB 460.2 kB/s eta 0:02:13   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/66.5 MB 460.4 kB/s eta 0:02:13   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/66.5 MB 460.4 kB/s eta 0:02:13   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/66.5 MB 462.6 kB/s eta 0:02:12   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/66.5 MB 463.8 kB/s eta 0:02:12   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/66.5 MB 464.9 kB/s eta 0:02:12   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/66.5 MB 466.5 kB/s eta 0:02:11   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/66.5 MB 467.4 kB/s eta 0:02:11   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/66.5 MB 467.7 kB/s eta 0:02:11   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/66.5 MB 468.2 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/66.5 MB 468.4 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/66.5 MB 468.1 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/66.5 MB 467.9 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/66.5 MB 468.0 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/66.5 MB 468.6 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/66.5 MB 469.2 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/66.5 MB 469.0 kB/s eta 0:02:10   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/66.5 MB 468.9 kB/s eta 0:02:10   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/66.5 MB 469.4 kB/s eta 0:02:10   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/66.5 MB 470.2 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/66.5 MB 469.3 kB/s eta 0:02:10   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/66.5 MB 469.2 kB/s eta 0:02:10   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/66.5 MB 469.5 kB/s eta 0:02:10   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/66.5 MB 470.3 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/66.5 MB 471.3 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/66.5 MB 471.3 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/66.5 MB 471.2 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/66.5 MB 471.9 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/66.5 MB 471.0 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.2 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.6 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.6 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.7 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.7 kB/s eta 0:02:08   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.7 kB/s eta 0:02:08   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 471.7 kB/s eta 0:02:08   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/66.5 MB 469.1 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/66.5 MB 468.4 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/66.5 MB 468.0 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/66.5 MB 467.8 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/66.5 MB 468.2 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/66.5 MB 467.9 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/66.5 MB 467.9 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/66.5 MB 468.3 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/66.5 MB 467.7 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/66.5 MB 467.6 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/66.5 MB 467.4 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/66.5 MB 467.6 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/66.5 MB 466.8 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/66.5 MB 466.7 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/66.5 MB 467.5 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/66.5 MB 466.6 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.4/66.5 MB 466.7 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/66.5 MB 467.3 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/66.5 MB 467.3 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/66.5 MB 466.9 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/66.5 MB 467.5 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/66.5 MB 467.5 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/66.5 MB 467.0 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 467.0 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 466.9 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 467.1 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 466.4 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 466.0 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 465.7 kB/s eta 0:02:09   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/66.5 MB 465.6 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/66.5 MB 466.6 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/66.5 MB 467.1 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/66.5 MB 466.4 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/66.5 MB 466.2 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/66.5 MB 466.5 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/66.5 MB 466.5 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/66.5 MB 467.2 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/66.5 MB 466.9 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/66.5 MB 467.3 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/66.5 MB 468.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/66.5 MB 469.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/66.5 MB 469.4 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/66.5 MB 469.3 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 469.6 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 469.2 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 469.1 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 469.4 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 469.4 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 467.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/66.5 MB 467.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/66.5 MB 467.6 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/66.5 MB 468.0 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/66.5 MB 467.5 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/66.5 MB 468.1 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/66.5 MB 468.6 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/66.5 MB 467.0 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 467.6 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 467.6 kB/s eta 0:02:07   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 466.8 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 466.8 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 466.3 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 465.8 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/66.5 MB 465.8 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/66.5 MB 465.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/66.5 MB 465.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/66.5 MB 465.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/66.5 MB 464.8 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/66.5 MB 464.7 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/66.5 MB 464.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.9 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.9 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.1 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 464.4 kB/s eta 0:02:08   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 459.6 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/66.5 MB 459.2 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.9 kB/s eta 0:02:09   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.2 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.5 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.3 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.7 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.2 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.5/66.5 MB 459.2 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/66.5 MB 458.7 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/66.5 MB 458.7 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/66.5 MB 458.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/66.5 MB 458.8 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/66.5 MB 458.8 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/66.5 MB 458.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 458.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 457.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 457.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 457.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 457.0 kB/s eta 0:02:09   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 454.5 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 454.5 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 454.5 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 452.7 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 452.7 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 450.9 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 450.9 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 450.4 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/66.5 MB 450.4 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 448.7 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 448.7 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 447.5 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 447.4 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 447.1 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 447.6 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 447.6 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 445.8 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/66.5 MB 446.0 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/66.5 MB 446.2 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/66.5 MB 445.3 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/66.5 MB 445.5 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/66.5 MB 446.0 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/66.5 MB 446.2 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/66.5 MB 446.0 kB/s eta 0:02:12   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/66.5 MB 447.8 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/66.5 MB 447.8 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/66.5 MB 447.3 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/66.5 MB 447.4 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.0/66.5 MB 447.4 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.1/66.5 MB 447.6 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.1/66.5 MB 448.3 kB/s eta 0:02:11   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.1/66.5 MB 448.9 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/66.5 MB 449.6 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/66.5 MB 450.1 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/66.5 MB 451.3 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/66.5 MB 451.4 kB/s eta 0:02:10   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/66.5 MB 451.6 kB/s eta 0:02:09   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/66.5 MB 453.3 kB/s eta 0:02:09   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/66.5 MB 453.8 kB/s eta 0:02:09   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/66.5 MB 454.0 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/66.5 MB 453.8 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/66.5 MB 454.6 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/66.5 MB 454.7 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/66.5 MB 455.5 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/66.5 MB 455.1 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/66.5 MB 455.9 kB/s eta 0:02:08   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/66.5 MB 457.2 kB/s eta 0:02:07   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/66.5 MB 458.2 kB/s eta 0:02:07   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/66.5 MB 458.6 kB/s eta 0:02:07   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/66.5 MB 459.4 kB/s eta 0:02:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/66.5 MB 460.8 kB/s eta 0:02:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/66.5 MB 461.3 kB/s eta 0:02:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/66.5 MB 461.3 kB/s eta 0:02:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/66.5 MB 461.5 kB/s eta 0:02:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/66.5 MB 462.1 kB/s eta 0:02:05   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/66.5 MB 463.3 kB/s eta 0:02:05   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/66.5 MB 463.1 kB/s eta 0:02:05   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/66.5 MB 464.5 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.9/66.5 MB 465.1 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/66.5 MB 465.1 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/66.5 MB 465.6 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/66.5 MB 465.5 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/66.5 MB 465.5 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/66.5 MB 466.2 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/66.5 MB 466.5 kB/s eta 0:02:04   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/66.5 MB 467.1 kB/s eta 0:02:03   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/66.5 MB 468.1 kB/s eta 0:02:03   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/66.5 MB 468.7 kB/s eta 0:02:03   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/66.5 MB 469.5 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/66.5 MB 469.8 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/66.5 MB 469.5 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/66.5 MB 469.6 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/66.5 MB 469.9 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/66.5 MB 471.2 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/66.5 MB 471.3 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/66.5 MB 471.3 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/66.5 MB 471.9 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/66.5 MB 470.9 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/66.5 MB 470.6 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/66.5 MB 471.1 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/66.5 MB 471.1 kB/s eta 0:02:02   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/66.5 MB 471.7 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/66.5 MB 471.3 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/66.5 MB 471.4 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/66.5 MB 471.6 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/66.5 MB 472.0 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/66.5 MB 472.1 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/66.5 MB 472.1 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/66.5 MB 470.8 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/66.5 MB 471.3 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/66.5 MB 471.3 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/66.5 MB 471.6 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/66.5 MB 471.7 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/66.5 MB 472.1 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/66.5 MB 472.1 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/66.5 MB 472.1 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/66.5 MB 472.2 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/66.5 MB 471.7 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/66.5 MB 471.9 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 472.3 kB/s eta 0:02:00   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 472.4 kB/s eta 0:02:00   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 472.3 kB/s eta 0:02:00   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 472.3 kB/s eta 0:02:00   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 471.8 kB/s eta 0:02:00   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 470.5 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 470.5 kB/s eta 0:02:01   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/66.5 MB 470.5 kB/s eta 0:02:01   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/66.5 MB 471.3 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/66.5 MB 471.7 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/66.5 MB 471.8 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/66.5 MB 471.8 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/66.5 MB 471.3 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/66.5 MB 471.7 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/66.5 MB 472.3 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/66.5 MB 472.4 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/66.5 MB 472.7 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/66.5 MB 472.4 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/66.5 MB 473.5 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/66.5 MB 473.8 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/66.5 MB 473.7 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/66.5 MB 473.9 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/66.5 MB 473.7 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/66.5 MB 473.3 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 473.1 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 473.1 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 472.4 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 472.2 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 471.8 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 471.2 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 470.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/66.5 MB 470.2 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/66.5 MB 469.9 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/66.5 MB 469.3 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/66.5 MB 469.7 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/66.5 MB 469.4 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/66.5 MB 469.4 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.6/66.5 MB 469.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.6/66.5 MB 469.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.6/66.5 MB 469.1 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.6/66.5 MB 468.2 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.6/66.5 MB 468.6 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.6/66.5 MB 468.4 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 468.0 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 468.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 468.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 468.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 468.5 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 466.4 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/66.5 MB 466.8 kB/s eta 0:02:00   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/66.5 MB 470.3 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/66.5 MB 470.1 kB/s eta 0:01:59   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/66.5 MB 469.7 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/66.5 MB 469.5 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/66.5 MB 468.7 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/66.5 MB 468.3 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 468.4 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 467.9 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 468.6 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 469.2 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 469.2 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 469.2 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/66.5 MB 478.6 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 477.4 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 477.4 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 477.4 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.0/66.5 MB 474.8 kB/s eta 0:01:57   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/66.5 MB 459.1 kB/s eta 0:02:01   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 466.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/66.5 MB 463.0 kB/s eta 0:02:00   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.4/66.5 MB 465.1 kB/s eta 0:01:59   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.4/66.5 MB 469.0 kB/s eta 0:01:58   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.4/66.5 MB 468.3 kB/s eta 0:01:58   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/66.5 MB 489.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/66.5 MB 489.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/66.5 MB 489.2 kB/s eta 0:01:53   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/66.5 MB 489.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/66.5 MB 490.9 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 491.8 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 492.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 491.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 490.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 490.5 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 490.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/66.5 MB 490.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/66.5 MB 489.3 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/66.5 MB 489.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/66.5 MB 488.8 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/66.5 MB 488.4 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/66.5 MB 487.8 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/66.5 MB 487.5 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/66.5 MB 487.5 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/66.5 MB 487.4 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/66.5 MB 488.5 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/66.5 MB 489.0 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/66.5 MB 489.0 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/66.5 MB 488.3 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/66.5 MB 488.9 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/66.5 MB 489.5 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/66.5 MB 489.5 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/66.5 MB 488.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/66.5 MB 488.1 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/66.5 MB 487.8 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/66.5 MB 488.0 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/66.5 MB 488.0 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/66.5 MB 487.1 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/66.5 MB 486.9 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/66.5 MB 486.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/66.5 MB 486.6 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/66.5 MB 486.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/66.5 MB 486.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/66.5 MB 486.1 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/66.5 MB 486.2 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/66.5 MB 486.7 kB/s eta 0:01:52   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.4 kB/s eta 0:01:52   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/66.5 MB 486.7 kB/s eta 0:01:51   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/66.5 MB 482.7 kB/s eta 0:01:52   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/66.5 MB 491.9 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/66.5 MB 491.6 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/66.5 MB 491.1 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/66.5 MB 491.1 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/66.5 MB 490.3 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/66.5 MB 490.3 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/66.5 MB 490.4 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/66.5 MB 490.4 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/66.5 MB 489.2 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/66.5 MB 489.7 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/66.5 MB 488.3 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/66.5 MB 488.1 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/66.5 MB 489.4 kB/s eta 0:01:50   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/66.5 MB 491.2 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/66.5 MB 491.2 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/66.5 MB 491.1 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/66.5 MB 491.2 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/66.5 MB 490.3 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/66.5 MB 491.0 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/66.5 MB 490.8 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/66.5 MB 490.6 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/66.5 MB 490.7 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/66.5 MB 490.1 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/66.5 MB 491.3 kB/s eta 0:01:49   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/66.5 MB 491.5 kB/s eta 0:01:49   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/66.5 MB 490.9 kB/s eta 0:01:49   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/66.5 MB 492.0 kB/s eta 0:01:49   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.4/66.5 MB 491.8 kB/s eta 0:01:49   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.4/66.5 MB 492.2 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.4/66.5 MB 493.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.4/66.5 MB 492.6 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/66.5 MB 493.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/66.5 MB 494.1 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/66.5 MB 494.8 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/66.5 MB 495.5 kB/s eta 0:01:47   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/66.5 MB 495.4 kB/s eta 0:01:47   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/66.5 MB 494.8 kB/s eta 0:01:47   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/66.5 MB 494.8 kB/s eta 0:01:47   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/66.5 MB 494.1 kB/s eta 0:01:47   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/66.5 MB 493.6 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/66.5 MB 493.7 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.7/66.5 MB 492.3 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.7/66.5 MB 492.3 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.7/66.5 MB 492.1 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.7/66.5 MB 492.1 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/66.5 MB 491.7 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/66.5 MB 491.2 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/66.5 MB 490.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/66.5 MB 490.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/66.5 MB 490.1 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.9/66.5 MB 490.3 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.9/66.5 MB 489.9 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.9/66.5 MB 489.8 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.9/66.5 MB 489.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/66.5 MB 489.1 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/66.5 MB 489.1 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/66.5 MB 488.2 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/66.5 MB 488.3 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.0/66.5 MB 488.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/66.5 MB 489.0 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/66.5 MB 488.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/66.5 MB 488.5 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/66.5 MB 487.2 kB/s eta 0:01:48   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/66.5 MB 486.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/66.5 MB 487.2 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 486.8 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 487.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 487.6 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 487.6 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 486.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 486.6 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.2/66.5 MB 486.6 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/66.5 MB 486.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/66.5 MB 486.0 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/66.5 MB 485.9 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/66.5 MB 486.9 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/66.5 MB 486.9 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.4/66.5 MB 485.1 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.4/66.5 MB 485.1 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.4/66.5 MB 484.9 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.4/66.5 MB 485.6 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.5/66.5 MB 485.2 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.5/66.5 MB 485.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.5/66.5 MB 485.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.5/66.5 MB 485.6 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.5/66.5 MB 485.2 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/66.5 MB 486.0 kB/s eta 0:01:47   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/66.5 MB 484.7 kB/s eta 0:01:48   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/66.5 MB 497.3 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/66.5 MB 496.5 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/66.5 MB 496.2 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/66.5 MB 496.2 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/66.5 MB 494.4 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/66.5 MB 494.0 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/66.5 MB 494.0 kB/s eta 0:01:45   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/66.5 MB 491.8 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/66.5 MB 491.3 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/66.5 MB 491.2 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/66.5 MB 490.3 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/66.5 MB 489.4 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.9/66.5 MB 489.8 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.9/66.5 MB 490.3 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.9/66.5 MB 490.2 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.9/66.5 MB 490.2 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.9/66.5 MB 489.6 kB/s eta 0:01:46   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/66.5 MB 489.5 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/66.5 MB 488.5 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/66.5 MB 488.5 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/66.5 MB 488.4 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.1/66.5 MB 488.6 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.1/66.5 MB 488.7 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.1/66.5 MB 488.6 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.1/66.5 MB 488.6 kB/s eta 0:01:46   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/66.5 MB 489.2 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/66.5 MB 489.2 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/66.5 MB 492.0 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 493.0 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 493.0 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 493.0 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 490.5 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 491.3 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 490.8 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.3/66.5 MB 490.8 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.4/66.5 MB 491.5 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.4/66.5 MB 491.1 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.4/66.5 MB 490.8 kB/s eta 0:01:45   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.5/66.5 MB 490.8 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.5/66.5 MB 491.2 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.5/66.5 MB 491.5 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.6/66.5 MB 492.0 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.6/66.5 MB 491.5 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.6/66.5 MB 491.3 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.6/66.5 MB 491.0 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.6/66.5 MB 490.6 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/66.5 MB 491.4 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/66.5 MB 490.9 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/66.5 MB 490.7 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/66.5 MB 490.9 kB/s eta 0:01:44   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.8/66.5 MB 490.2 kB/s eta 0:01:44   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.8/66.5 MB 490.8 kB/s eta 0:01:44   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.8/66.5 MB 490.6 kB/s eta 0:01:44   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/66.5 MB 490.9 kB/s eta 0:01:44   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/66.5 MB 491.4 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/66.5 MB 491.4 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/66.5 MB 491.3 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/66.5 MB 491.0 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/66.5 MB 490.3 kB/s eta 0:01:44   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/66.5 MB 490.7 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.0/66.5 MB 491.2 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/66.5 MB 491.4 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/66.5 MB 491.6 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/66.5 MB 491.2 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/66.5 MB 492.8 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.1/66.5 MB 491.7 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.2/66.5 MB 491.9 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.2/66.5 MB 491.8 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.2/66.5 MB 492.2 kB/s eta 0:01:43   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/66.5 MB 492.9 kB/s eta 0:01:42   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/66.5 MB 493.1 kB/s eta 0:01:42   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/66.5 MB 493.2 kB/s eta 0:01:42   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/66.5 MB 494.0 kB/s eta 0:01:42   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/66.5 MB 494.4 kB/s eta 0:01:42   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/66.5 MB 497.1 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/66.5 MB 497.6 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.4/66.5 MB 497.7 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/66.5 MB 498.5 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/66.5 MB 498.5 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/66.5 MB 498.4 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/66.5 MB 498.4 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/66.5 MB 497.8 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/66.5 MB 498.9 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/66.5 MB 499.0 kB/s eta 0:01:41   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/66.5 MB 498.9 kB/s eta 0:01:40   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/66.5 MB 499.3 kB/s eta 0:01:40   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.6/66.5 MB 497.8 kB/s eta 0:01:41   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.7/66.5 MB 499.3 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.7/66.5 MB 499.3 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.7/66.5 MB 499.6 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.7/66.5 MB 500.4 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/66.5 MB 501.1 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/66.5 MB 501.1 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/66.5 MB 500.3 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/66.5 MB 501.2 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/66.5 MB 502.1 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/66.5 MB 502.4 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/66.5 MB 502.2 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/66.5 MB 502.4 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/66.5 MB 503.5 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/66.5 MB 503.5 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/66.5 MB 501.4 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/66.5 MB 503.0 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/66.5 MB 503.1 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/66.5 MB 502.1 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/66.5 MB 502.4 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/66.5 MB 502.1 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/66.5 MB 502.0 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/66.5 MB 502.1 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.2/66.5 MB 502.2 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.2/66.5 MB 503.4 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.2/66.5 MB 502.6 kB/s eta 0:01:39   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.2/66.5 MB 505.5 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 506.0 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 504.9 kB/s eta 0:01:38   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 491.3 kB/s eta 0:01:41   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 493.0 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 493.0 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/66.5 MB 491.8 kB/s eta 0:01:40   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.4/66.5 MB 491.0 kB/s eta 0:01:41   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.4/66.5 MB 495.9 kB/s eta 0:01:39   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.5/66.5 MB 498.3 kB/s eta 0:01:39   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.5/66.5 MB 498.6 kB/s eta 0:01:39   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/66.5 MB 503.3 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.8/66.5 MB 500.5 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.8/66.5 MB 500.5 kB/s eta 0:01:38   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.9/66.5 MB 508.1 kB/s eta 0:01:36   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.9/66.5 MB 508.1 kB/s eta 0:01:36   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.9/66.5 MB 508.1 kB/s eta 0:01:36   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.9/66.5 MB 508.1 kB/s eta 0:01:36   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.9/66.5 MB 508.1 kB/s eta 0:01:36   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.9/66.5 MB 506.7 kB/s eta 0:01:36   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 526.1 kB/s eta 0:01:32   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 525.6 kB/s eta 0:01:32   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 525.6 kB/s eta 0:01:32   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 524.7 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 524.7 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 524.7 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/66.5 MB 521.0 kB/s eta 0:01:33   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.4/66.5 MB 514.3 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.4/66.5 MB 514.2 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.4/66.5 MB 513.9 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 514.1 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 514.1 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 513.5 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 513.5 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 513.5 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 513.5 kB/s eta 0:01:34   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 509.7 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/66.5 MB 509.3 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 508.5 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 507.5 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 507.5 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 505.9 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 505.9 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 506.3 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 504.9 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.6/66.5 MB 505.4 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 505.4 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 504.0 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 504.0 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 503.7 kB/s eta 0:01:35   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 502.7 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 502.7 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 501.7 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 502.2 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/66.5 MB 501.2 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/66.5 MB 501.1 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/66.5 MB 500.2 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/66.5 MB 499.5 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/66.5 MB 499.3 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/66.5 MB 499.3 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.8/66.5 MB 497.0 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 496.9 kB/s eta 0:01:36   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 496.0 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 496.0 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 494.7 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 494.9 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 493.6 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 493.6 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 492.7 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.9/66.5 MB 492.7 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 491.5 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 491.9 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 491.9 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 491.7 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 491.1 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 491.1 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 489.1 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/66.5 MB 489.1 kB/s eta 0:01:37   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 488.4 kB/s eta 0:01:38   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 487.8 kB/s eta 0:01:38   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 487.1 kB/s eta 0:01:38   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 486.9 kB/s eta 0:01:38   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 486.9 kB/s eta 0:01:38   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 486.9 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 484.5 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/66.5 MB 483.7 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 483.3 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 483.4 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 483.4 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 483.0 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 483.0 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 482.2 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 482.4 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/66.5 MB 482.4 kB/s eta 0:01:38   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 480.3 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 479.7 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 479.5 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 479.3 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 478.8 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 478.8 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 477.6 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/66.5 MB 477.6 kB/s eta 0:01:39   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 475.9 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 475.9 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 475.9 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 474.6 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 474.6 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 473.1 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 473.1 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 472.0 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.4/66.5 MB 471.7 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 471.7 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 471.7 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 470.8 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 470.8 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 470.8 kB/s eta 0:01:40   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 469.2 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 469.4 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 468.6 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/66.5 MB 468.6 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 467.8 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 467.0 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 467.0 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 465.6 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 466.0 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 465.0 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 464.7 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.6/66.5 MB 464.7 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 465.1 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 464.7 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 464.5 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 464.5 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 463.3 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 463.3 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 462.6 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/66.5 MB 462.9 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 462.6 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 462.6 kB/s eta 0:01:41   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 461.0 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 461.0 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 460.4 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 460.4 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 459.7 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 460.5 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/66.5 MB 460.5 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 459.7 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 460.0 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 459.6 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 459.6 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 459.6 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 458.4 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 456.9 kB/s eta 0:01:42   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/66.5 MB 456.9 kB/s eta 0:01:42   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 456.6 kB/s eta 0:01:42   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 456.1 kB/s eta 0:01:42   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 456.6 kB/s eta 0:01:42   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 456.2 kB/s eta 0:01:42   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 456.2 kB/s eta 0:01:42   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 454.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 454.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 454.3 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/66.5 MB 454.3 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 453.2 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 453.2 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 452.6 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 452.1 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 451.1 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 451.1 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/66.5 MB 453.0 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 453.2 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 453.2 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 452.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 452.1 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 452.1 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 450.8 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 450.0 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 450.0 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.2/66.5 MB 449.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 448.8 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 448.8 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 448.8 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 448.6 kB/s eta 0:01:43   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 448.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 447.8 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 447.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 447.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/66.5 MB 447.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 446.6 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 446.7 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 446.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 445.6 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 445.6 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 444.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.4/66.5 MB 444.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/66.5 MB 443.4 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/66.5 MB 443.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/66.5 MB 443.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/66.5 MB 442.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/66.5 MB 442.8 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/66.5 MB 442.7 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 442.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 443.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 443.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 442.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 442.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 441.6 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.6/66.5 MB 440.9 kB/s eta 0:01:45   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/66.5 MB 442.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/66.5 MB 442.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/66.5 MB 441.8 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/66.5 MB 441.6 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/66.5 MB 441.4 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.7/66.5 MB 441.4 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 441.6 kB/s eta 0:01:44   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 441.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 441.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 441.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 440.0 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 440.5 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/66.5 MB 441.3 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.9/66.5 MB 441.2 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.9/66.5 MB 440.7 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.9/66.5 MB 440.9 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.9/66.5 MB 440.9 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 442.7 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 442.7 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 441.7 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 441.7 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 441.1 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 441.4 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 441.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/66.5 MB 441.0 kB/s eta 0:01:44   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 442.0 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 441.6 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 441.5 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 441.5 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 441.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 441.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/66.5 MB 441.4 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.2/66.5 MB 441.3 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.2/66.5 MB 441.3 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.2/66.5 MB 441.0 kB/s eta 0:01:43   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.2/66.5 MB 461.3 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/66.5 MB 460.5 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/66.5 MB 460.1 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/66.5 MB 459.0 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/66.5 MB 458.5 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/66.5 MB 458.0 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/66.5 MB 457.0 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/66.5 MB 456.3 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/66.5 MB 455.4 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/66.5 MB 454.1 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/66.5 MB 454.1 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/66.5 MB 452.6 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/66.5 MB 452.1 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/66.5 MB 451.3 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/66.5 MB 451.3 kB/s eta 0:01:40   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/66.5 MB 449.3 kB/s eta 0:01:41   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/66.5 MB 456.7 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/66.5 MB 456.1 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.5/66.5 MB 455.6 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/66.5 MB 454.5 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/66.5 MB 454.1 kB/s eta 0:01:39   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/66.5 MB 453.4 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/66.5 MB 453.7 kB/s eta 0:01:39   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/66.5 MB 452.8 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/66.5 MB 452.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 452.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 451.8 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 451.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 450.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 450.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 449.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/66.5 MB 448.2 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 447.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 446.6 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 446.6 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 445.2 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 445.2 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 443.5 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.8/66.5 MB 442.5 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.9/66.5 MB 441.7 kB/s eta 0:01:42   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.9/66.5 MB 442.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.9/66.5 MB 442.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.9/66.5 MB 442.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.9/66.5 MB 442.2 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/66.5 MB 441.5 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/66.5 MB 441.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/66.5 MB 441.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/66.5 MB 439.3 kB/s eta 0:01:42   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/66.5 MB 441.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.0/66.5 MB 440.5 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.1/66.5 MB 440.2 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.1/66.5 MB 440.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.1/66.5 MB 440.6 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.1/66.5 MB 441.0 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.1/66.5 MB 440.9 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.1/66.5 MB 440.9 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/66.5 MB 441.4 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/66.5 MB 441.5 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/66.5 MB 440.9 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/66.5 MB 440.7 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/66.5 MB 440.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/66.5 MB 440.1 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.3/66.5 MB 440.3 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.3/66.5 MB 440.2 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.3/66.5 MB 440.6 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.3/66.5 MB 439.8 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/66.5 MB 440.3 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/66.5 MB 441.2 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/66.5 MB 441.2 kB/s eta 0:01:40   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/66.5 MB 440.5 kB/s eta 0:01:41   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/66.5 MB 440.4 kB/s eta 0:01:41   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.4/66.5 MB 440.5 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.5/66.5 MB 439.8 kB/s eta 0:01:41   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.5/66.5 MB 440.9 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.5/66.5 MB 441.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.5/66.5 MB 440.8 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/66.5 MB 441.1 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/66.5 MB 441.3 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/66.5 MB 441.2 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/66.5 MB 441.4 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/66.5 MB 441.3 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 440.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 429.3 kB/s eta 0:01:43   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.7/66.5 MB 429.5 kB/s eta 0:01:42   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/66.5 MB 439.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/66.5 MB 439.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/66.5 MB 439.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/66.5 MB 439.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/66.5 MB 437.0 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.9/66.5 MB 437.0 kB/s eta 0:01:40   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/66.5 MB 441.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/66.5 MB 441.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/66.5 MB 440.8 kB/s eta 0:01:39   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.2/66.5 MB 441.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.3/66.5 MB 441.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.3/66.5 MB 441.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.3/66.5 MB 441.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.3/66.5 MB 440.3 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.3/66.5 MB 440.3 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/66.5 MB 440.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/66.5 MB 441.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/66.5 MB 441.1 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/66.5 MB 440.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/66.5 MB 441.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/66.5 MB 441.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 441.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 441.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 441.1 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 440.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 440.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 440.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.5/66.5 MB 439.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 439.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 439.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 438.8 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 438.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 438.1 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 437.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/66.5 MB 437.1 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/66.5 MB 436.8 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/66.5 MB 436.6 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/66.5 MB 436.3 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/66.5 MB 436.3 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/66.5 MB 435.9 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/66.5 MB 435.7 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.4 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 435.5 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.8/66.5 MB 427.2 kB/s eta 0:01:40   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.9/66.5 MB 428.9 kB/s eta 0:01:40   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.9/66.5 MB 428.9 kB/s eta 0:01:40   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 23.9/66.5 MB 427.7 kB/s eta 0:01:40   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/66.5 MB 430.6 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/66.5 MB 430.6 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/66.5 MB 430.6 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.1/66.5 MB 432.1 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.1/66.5 MB 432.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 432.1 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 432.1 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 432.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 431.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 431.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 431.1 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.2/66.5 MB 431.1 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/66.5 MB 430.1 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/66.5 MB 430.4 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/66.5 MB 430.4 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/66.5 MB 429.7 kB/s eta 0:01:39   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/66.5 MB 431.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/66.5 MB 430.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.4/66.5 MB 430.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.4/66.5 MB 430.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.4/66.5 MB 430.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.4/66.5 MB 430.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.4/66.5 MB 431.1 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 431.0 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 430.8 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 430.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 430.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 430.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 429.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 429.9 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/66.5 MB 428.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/66.5 MB 428.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/66.5 MB 429.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/66.5 MB 430.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/66.5 MB 430.4 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.7/66.5 MB 429.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.7/66.5 MB 429.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.7/66.5 MB 429.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.7/66.5 MB 429.7 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.7/66.5 MB 429.6 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.8/66.5 MB 429.6 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.8/66.5 MB 429.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.8/66.5 MB 429.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.8/66.5 MB 429.3 kB/s eta 0:01:38   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.9/66.5 MB 429.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.9/66.5 MB 429.3 kB/s eta 0:01:37   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.9/66.5 MB 429.0 kB/s eta 0:01:37   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 24.9/66.5 MB 429.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 24.9/66.5 MB 430.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/66.5 MB 430.7 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/66.5 MB 430.7 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/66.5 MB 430.3 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/66.5 MB 429.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/66.5 MB 429.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.0/66.5 MB 430.0 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.1/66.5 MB 429.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.1/66.5 MB 429.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.1/66.5 MB 429.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.1/66.5 MB 428.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.1/66.5 MB 428.7 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.2/66.5 MB 429.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.2/66.5 MB 429.0 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.2/66.5 MB 429.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.2/66.5 MB 429.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.2/66.5 MB 428.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.3/66.5 MB 428.3 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.3/66.5 MB 428.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.3/66.5 MB 428.7 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.3/66.5 MB 428.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.4/66.5 MB 428.4 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.4/66.5 MB 428.1 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.4/66.5 MB 428.1 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.4/66.5 MB 428.3 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 427.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 427.7 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 427.2 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 429.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 428.6 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 427.5 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.5/66.5 MB 427.5 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/66.5 MB 427.4 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/66.5 MB 426.6 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/66.5 MB 426.7 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/66.5 MB 426.6 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.7/66.5 MB 426.5 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.7/66.5 MB 426.4 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.7/66.5 MB 426.2 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.7/66.5 MB 425.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.7/66.5 MB 426.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 25.7/66.5 MB 425.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 424.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 424.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 423.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 423.9 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 423.9 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 424.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/66.5 MB 424.1 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 423.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 423.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 422.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 422.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 422.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 421.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/66.5 MB 421.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 421.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 409.7 kB/s eta 0:01:39   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.0/66.5 MB 410.0 kB/s eta 0:01:39   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.2/66.5 MB 415.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.3/66.5 MB 415.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.3/66.5 MB 415.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.3/66.5 MB 415.1 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.3/66.5 MB 415.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.3/66.5 MB 415.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.3/66.5 MB 415.0 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/66.5 MB 414.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/66.5 MB 414.3 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/66.5 MB 414.0 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/66.5 MB 413.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/66.5 MB 413.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/66.5 MB 413.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/66.5 MB 412.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/66.5 MB 412.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/66.5 MB 412.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/66.5 MB 412.2 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/66.5 MB 412.3 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/66.5 MB 411.5 kB/s eta 0:01:38   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.6/66.5 MB 411.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 26.6/66.5 MB 411.2 kB/s eta 0:01:38   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.6/66.5 MB 411.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.7/66.5 MB 411.8 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.7/66.5 MB 411.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.7/66.5 MB 411.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.7/66.5 MB 411.9 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.8/66.5 MB 412.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.8/66.5 MB 412.4 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.8/66.5 MB 412.3 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.8/66.5 MB 412.6 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.8/66.5 MB 412.5 kB/s eta 0:01:37   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.8/66.5 MB 413.0 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.9/66.5 MB 412.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.9/66.5 MB 412.6 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 26.9/66.5 MB 412.7 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.0/66.5 MB 412.7 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.0/66.5 MB 412.9 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.0/66.5 MB 413.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.0/66.5 MB 413.6 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.1/66.5 MB 413.9 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.1/66.5 MB 413.8 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.1/66.5 MB 413.7 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.1/66.5 MB 413.2 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.2/66.5 MB 413.1 kB/s eta 0:01:36   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.2/66.5 MB 414.2 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.2/66.5 MB 414.0 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.2/66.5 MB 413.7 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.3/66.5 MB 414.1 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.3/66.5 MB 414.6 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.3/66.5 MB 414.7 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.3/66.5 MB 414.4 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.4/66.5 MB 413.9 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.4/66.5 MB 413.7 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 27.4/66.5 MB 413.9 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.4/66.5 MB 414.2 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.5/66.5 MB 413.9 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.5/66.5 MB 413.5 kB/s eta 0:01:35   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.5/66.5 MB 424.0 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.5/66.5 MB 423.6 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.6/66.5 MB 424.6 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.6/66.5 MB 424.5 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.6/66.5 MB 423.7 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.6/66.5 MB 423.2 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.7/66.5 MB 422.9 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.7/66.5 MB 422.8 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.7/66.5 MB 422.3 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.7/66.5 MB 421.6 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.7/66.5 MB 421.2 kB/s eta 0:01:33   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.7/66.5 MB 421.3 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.8/66.5 MB 421.1 kB/s eta 0:01:32   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.8/66.5 MB 420.4 kB/s eta 0:01:33   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.8/66.5 MB 419.6 kB/s eta 0:01:33   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.9/66.5 MB 432.0 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.9/66.5 MB 431.2 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.9/66.5 MB 430.5 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.9/66.5 MB 429.6 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 27.9/66.5 MB 429.1 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.0/66.5 MB 428.2 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.0/66.5 MB 427.6 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.0/66.5 MB 427.3 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.0/66.5 MB 426.3 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.1/66.5 MB 427.3 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.1/66.5 MB 427.3 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.1/66.5 MB 425.7 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.1/66.5 MB 425.3 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.2/66.5 MB 427.4 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.2/66.5 MB 427.4 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.2/66.5 MB 426.9 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.2/66.5 MB 426.0 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 28.3/66.5 MB 425.6 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.3/66.5 MB 424.9 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.3/66.5 MB 424.0 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.3/66.5 MB 423.4 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.3/66.5 MB 423.0 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.4/66.5 MB 422.1 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.4/66.5 MB 421.7 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.4/66.5 MB 421.0 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.4/66.5 MB 421.4 kB/s eta 0:01:31   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.5/66.5 MB 423.7 kB/s eta 0:01:30   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.5/66.5 MB 432.8 kB/s eta 0:01:28   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.5/66.5 MB 431.9 kB/s eta 0:01:28   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.5/66.5 MB 431.2 kB/s eta 0:01:28   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.6/66.5 MB 430.6 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.6/66.5 MB 430.1 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.6/66.5 MB 429.0 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.6/66.5 MB 428.9 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.7/66.5 MB 428.4 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.7/66.5 MB 428.5 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.7/66.5 MB 428.4 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.7/66.5 MB 427.7 kB/s eta 0:01:29   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.7/66.5 MB 431.0 kB/s eta 0:01:28   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.8/66.5 MB 431.1 kB/s eta 0:01:28   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.8/66.5 MB 431.3 kB/s eta 0:01:28   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.8/66.5 MB 433.7 kB/s eta 0:01:27   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.9/66.5 MB 434.6 kB/s eta 0:01:27   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.9/66.5 MB 434.6 kB/s eta 0:01:27   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 28.9/66.5 MB 436.4 kB/s eta 0:01:27   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.0/66.5 MB 438.0 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.0/66.5 MB 437.4 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.0/66.5 MB 438.2 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.0/66.5 MB 439.3 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 29.1/66.5 MB 439.1 kB/s eta 0:01:26   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.3/66.5 MB 447.7 kB/s eta 0:01:24   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.3/66.5 MB 447.7 kB/s eta 0:01:24   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.4/66.5 MB 450.4 kB/s eta 0:01:23   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.4/66.5 MB 450.8 kB/s eta 0:01:23   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.4/66.5 MB 450.8 kB/s eta 0:01:23   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.4/66.5 MB 450.9 kB/s eta 0:01:23   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.5/66.5 MB 452.1 kB/s eta 0:01:22   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.5/66.5 MB 452.5 kB/s eta 0:01:22   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.5/66.5 MB 454.2 kB/s eta 0:01:22   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.5/66.5 MB 454.0 kB/s eta 0:01:22   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.5/66.5 MB 454.4 kB/s eta 0:01:22   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.6/66.5 MB 456.0 kB/s eta 0:01:21   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.6/66.5 MB 457.6 kB/s eta 0:01:21   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.6/66.5 MB 459.7 kB/s eta 0:01:21   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.6/66.5 MB 461.2 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.6/66.5 MB 461.2 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.7/66.5 MB 460.3 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.7/66.5 MB 460.4 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.7/66.5 MB 460.1 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.7/66.5 MB 460.8 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.7/66.5 MB 461.5 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.8/66.5 MB 462.3 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.8/66.5 MB 463.9 kB/s eta 0:01:20   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.8/66.5 MB 464.5 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.8/66.5 MB 464.4 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.9/66.5 MB 465.1 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.9/66.5 MB 466.2 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.9/66.5 MB 466.2 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.9/66.5 MB 466.2 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 29.9/66.5 MB 466.2 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 29.9/66.5 MB 466.5 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.0/66.5 MB 467.5 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.0/66.5 MB 467.1 kB/s eta 0:01:19   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.0/66.5 MB 469.1 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.0/66.5 MB 469.6 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.0/66.5 MB 469.7 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.0/66.5 MB 469.7 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.1/66.5 MB 470.1 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.1/66.5 MB 470.1 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.1/66.5 MB 469.9 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.1/66.5 MB 469.8 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.1/66.5 MB 469.4 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.1/66.5 MB 469.3 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 470.4 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 471.6 kB/s eta 0:01:18   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 472.2 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 472.2 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 472.6 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 472.6 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 472.6 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.2/66.5 MB 472.3 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 473.4 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 474.2 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 474.2 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 474.2 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 472.9 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 473.4 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 473.4 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.3/66.5 MB 472.4 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 472.8 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 472.0 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 472.0 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 471.3 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 471.3 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 471.2 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.4/66.5 MB 472.3 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.5/66.5 MB 472.8 kB/s eta 0:01:17   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.5/66.5 MB 474.9 kB/s eta 0:01:16   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.5/66.5 MB 476.1 kB/s eta 0:01:16   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.5/66.5 MB 475.2 kB/s eta 0:01:16   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.6/66.5 MB 477.1 kB/s eta 0:01:16   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.6/66.5 MB 478.6 kB/s eta 0:01:15   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.6/66.5 MB 480.2 kB/s eta 0:01:15   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.7/66.5 MB 481.7 kB/s eta 0:01:15   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.7/66.5 MB 483.2 kB/s eta 0:01:15   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 30.7/66.5 MB 483.9 kB/s eta 0:01:14   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.8/66.5 MB 484.5 kB/s eta 0:01:14   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.8/66.5 MB 484.9 kB/s eta 0:01:14   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.8/66.5 MB 485.1 kB/s eta 0:01:14   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.8/66.5 MB 484.1 kB/s eta 0:01:14   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.8/66.5 MB 487.5 kB/s eta 0:01:14   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.9/66.5 MB 488.2 kB/s eta 0:01:13   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.9/66.5 MB 489.3 kB/s eta 0:01:13   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 30.9/66.5 MB 489.4 kB/s eta 0:01:13   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.0/66.5 MB 490.1 kB/s eta 0:01:13   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.0/66.5 MB 491.9 kB/s eta 0:01:13   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.0/66.5 MB 493.6 kB/s eta 0:01:12   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.1/66.5 MB 493.5 kB/s eta 0:01:12   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.1/66.5 MB 495.5 kB/s eta 0:01:12   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.1/66.5 MB 495.5 kB/s eta 0:01:12   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.2/66.5 MB 495.5 kB/s eta 0:01:12   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.2/66.5 MB 496.2 kB/s eta 0:01:12   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.2/66.5 MB 497.7 kB/s eta 0:01:11   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.3/66.5 MB 498.7 kB/s eta 0:01:11   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.3/66.5 MB 498.7 kB/s eta 0:01:11   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.3/66.5 MB 499.3 kB/s eta 0:01:11   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.3/66.5 MB 500.1 kB/s eta 0:01:11   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.4/66.5 MB 500.2 kB/s eta 0:01:11   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.4/66.5 MB 502.3 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.5/66.5 MB 504.2 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.5/66.5 MB 504.2 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.5/66.5 MB 505.2 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.5/66.5 MB 505.4 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.5/66.5 MB 504.9 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 31.6/66.5 MB 505.9 kB/s eta 0:01:10   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.6/66.5 MB 506.0 kB/s eta 0:01:09   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.6/66.5 MB 506.2 kB/s eta 0:01:09   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.7/66.5 MB 507.8 kB/s eta 0:01:09   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.7/66.5 MB 509.8 kB/s eta 0:01:09   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.7/66.5 MB 509.4 kB/s eta 0:01:09   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.7/66.5 MB 510.3 kB/s eta 0:01:09   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.8/66.5 MB 511.1 kB/s eta 0:01:08   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.8/66.5 MB 511.2 kB/s eta 0:01:08   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.8/66.5 MB 512.3 kB/s eta 0:01:08   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.9/66.5 MB 513.6 kB/s eta 0:01:08   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.9/66.5 MB 514.4 kB/s eta 0:01:08   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 31.9/66.5 MB 515.6 kB/s eta 0:01:08   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.0/66.5 MB 515.9 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.0/66.5 MB 517.7 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.0/66.5 MB 517.0 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.0/66.5 MB 520.5 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.1/66.5 MB 520.9 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.1/66.5 MB 520.0 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.1/66.5 MB 520.1 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.1/66.5 MB 519.4 kB/s eta 0:01:07   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.1/66.5 MB 520.6 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.2/66.5 MB 520.4 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.2/66.5 MB 521.4 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.2/66.5 MB 523.1 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.2/66.5 MB 523.1 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.3/66.5 MB 522.5 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.3/66.5 MB 523.1 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.3/66.5 MB 524.5 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.3/66.5 MB 523.6 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.4/66.5 MB 523.6 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.4/66.5 MB 522.8 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 32.4/66.5 MB 524.0 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.4/66.5 MB 523.4 kB/s eta 0:01:06   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.4/66.5 MB 525.0 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.5/66.5 MB 526.2 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.5/66.5 MB 526.0 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.6/66.5 MB 527.0 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.6/66.5 MB 527.0 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.6/66.5 MB 526.3 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.6/66.5 MB 526.4 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.6/66.5 MB 528.6 kB/s eta 0:01:05   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.7/66.5 MB 529.7 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.7/66.5 MB 529.8 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.7/66.5 MB 530.3 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.8/66.5 MB 530.9 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.8/66.5 MB 530.7 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.8/66.5 MB 532.3 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.9/66.5 MB 532.6 kB/s eta 0:01:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.9/66.5 MB 550.9 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 32.9/66.5 MB 551.5 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.0/66.5 MB 551.3 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.0/66.5 MB 550.0 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.0/66.5 MB 549.0 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.1/66.5 MB 547.5 kB/s eta 0:01:02   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.1/66.5 MB 546.4 kB/s eta 0:01:02   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.2/66.5 MB 549.6 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.2/66.5 MB 550.4 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 33.2/66.5 MB 549.5 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.3/66.5 MB 547.9 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.3/66.5 MB 546.8 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.3/66.5 MB 545.7 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.4/66.5 MB 544.7 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.4/66.5 MB 543.3 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.4/66.5 MB 543.3 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.5/66.5 MB 546.1 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.5/66.5 MB 546.6 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.6/66.5 MB 548.1 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.6/66.5 MB 548.1 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.6/66.5 MB 547.5 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.6/66.5 MB 547.9 kB/s eta 0:01:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.7/66.5 MB 551.4 kB/s eta 0:01:00   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.7/66.5 MB 550.7 kB/s eta 0:01:00   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.7/66.5 MB 552.1 kB/s eta 0:01:00   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.8/66.5 MB 555.0 kB/s eta 0:00:59   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.8/66.5 MB 557.0 kB/s eta 0:00:59   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.9/66.5 MB 559.5 kB/s eta 0:00:59   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.9/66.5 MB 559.0 kB/s eta 0:00:59   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 33.9/66.5 MB 562.7 kB/s eta 0:00:58   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 34.0/66.5 MB 564.9 kB/s eta 0:00:58   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 34.0/66.5 MB 565.9 kB/s eta 0:00:58   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 34.0/66.5 MB 565.4 kB/s eta 0:00:58   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 34.1/66.5 MB 582.6 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.1/66.5 MB 580.8 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.1/66.5 MB 583.0 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.2/66.5 MB 582.9 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.2/66.5 MB 581.6 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.2/66.5 MB 584.3 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.3/66.5 MB 583.0 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.3/66.5 MB 581.4 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.4/66.5 MB 580.4 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.4/66.5 MB 580.6 kB/s eta 0:00:56   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.4/66.5 MB 583.3 kB/s eta 0:00:55   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.4/66.5 MB 585.0 kB/s eta 0:00:55   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.5/66.5 MB 588.2 kB/s eta 0:00:55   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.5/66.5 MB 589.6 kB/s eta 0:00:55   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.6/66.5 MB 591.7 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.6/66.5 MB 592.9 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.7/66.5 MB 594.5 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.7/66.5 MB 594.5 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.7/66.5 MB 594.9 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.7/66.5 MB 594.9 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.7/66.5 MB 595.8 kB/s eta 0:00:54   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.8/66.5 MB 599.2 kB/s eta 0:00:53   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.8/66.5 MB 601.0 kB/s eta 0:00:53   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.9/66.5 MB 600.5 kB/s eta 0:00:53   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 34.9/66.5 MB 603.2 kB/s eta 0:00:53   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 34.9/66.5 MB 604.7 kB/s eta 0:00:53   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.0/66.5 MB 605.9 kB/s eta 0:00:53   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.0/66.5 MB 606.7 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.0/66.5 MB 608.4 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.1/66.5 MB 608.7 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.1/66.5 MB 608.5 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.1/66.5 MB 610.3 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.2/66.5 MB 610.5 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.2/66.5 MB 612.4 kB/s eta 0:00:52   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.2/66.5 MB 613.6 kB/s eta 0:00:51   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.3/66.5 MB 613.0 kB/s eta 0:00:51   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.3/66.5 MB 615.9 kB/s eta 0:00:51   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.4/66.5 MB 617.6 kB/s eta 0:00:51   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.4/66.5 MB 619.5 kB/s eta 0:00:51   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.4/66.5 MB 620.5 kB/s eta 0:00:51   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.5/66.5 MB 621.6 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.5/66.5 MB 621.8 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.5/66.5 MB 621.8 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.5/66.5 MB 621.0 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.6/66.5 MB 621.1 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.6/66.5 MB 621.5 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.6/66.5 MB 622.1 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.6/66.5 MB 621.7 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.7/66.5 MB 621.3 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 35.7/66.5 MB 623.0 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 35.7/66.5 MB 624.3 kB/s eta 0:00:50   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 35.8/66.5 MB 628.7 kB/s eta 0:00:49   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 35.8/66.5 MB 630.0 kB/s eta 0:00:49   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 35.9/66.5 MB 630.1 kB/s eta 0:00:49   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 35.9/66.5 MB 632.4 kB/s eta 0:00:49   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 35.9/66.5 MB 632.6 kB/s eta 0:00:49   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.0/66.5 MB 633.6 kB/s eta 0:00:49   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.0/66.5 MB 638.6 kB/s eta 0:00:48   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.0/66.5 MB 640.8 kB/s eta 0:00:48   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.1/66.5 MB 641.7 kB/s eta 0:00:48   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.1/66.5 MB 645.2 kB/s eta 0:00:48   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.1/66.5 MB 647.1 kB/s eta 0:00:47   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.2/66.5 MB 679.1 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.2/66.5 MB 679.1 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.3/66.5 MB 677.8 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.3/66.5 MB 676.4 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.3/66.5 MB 675.0 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.4/66.5 MB 673.0 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.4/66.5 MB 671.2 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.4/66.5 MB 670.1 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.5/66.5 MB 670.3 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.5/66.5 MB 671.8 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 36.5/66.5 MB 671.9 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.6/66.5 MB 672.4 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.6/66.5 MB 677.3 kB/s eta 0:00:45   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.6/66.5 MB 679.3 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.7/66.5 MB 679.1 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.7/66.5 MB 684.0 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.8/66.5 MB 684.2 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.8/66.5 MB 685.1 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.8/66.5 MB 687.4 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.9/66.5 MB 687.1 kB/s eta 0:00:44   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 36.9/66.5 MB 688.0 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.0/66.5 MB 690.8 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.0/66.5 MB 691.4 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.0/66.5 MB 692.8 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.1/66.5 MB 693.1 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.1/66.5 MB 693.5 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.1/66.5 MB 695.0 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.2/66.5 MB 695.5 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.2/66.5 MB 696.9 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.2/66.5 MB 696.0 kB/s eta 0:00:43   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.3/66.5 MB 696.4 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.3/66.5 MB 697.2 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.3/66.5 MB 697.5 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 37.4/66.5 MB 700.9 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.4/66.5 MB 700.4 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.4/66.5 MB 702.2 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.5/66.5 MB 702.4 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.5/66.5 MB 701.7 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.5/66.5 MB 701.0 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.5/66.5 MB 700.6 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.6/66.5 MB 701.5 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.6/66.5 MB 704.0 kB/s eta 0:00:42   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.6/66.5 MB 704.2 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.7/66.5 MB 703.7 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.7/66.5 MB 706.4 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.8/66.5 MB 705.8 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.8/66.5 MB 706.4 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.8/66.5 MB 707.6 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.9/66.5 MB 707.9 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.9/66.5 MB 709.8 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 37.9/66.5 MB 712.9 kB/s eta 0:00:41   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.0/66.5 MB 715.0 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.0/66.5 MB 715.2 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.1/66.5 MB 716.2 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.1/66.5 MB 716.4 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.1/66.5 MB 716.8 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.1/66.5 MB 717.6 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.2/66.5 MB 719.8 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.2/66.5 MB 718.0 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 38.2/66.5 MB 718.1 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.3/66.5 MB 720.8 kB/s eta 0:00:40   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.3/66.5 MB 724.0 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.3/66.5 MB 724.1 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.4/66.5 MB 724.1 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.4/66.5 MB 724.8 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.5/66.5 MB 726.1 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.5/66.5 MB 726.5 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.5/66.5 MB 727.6 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.6/66.5 MB 729.7 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.6/66.5 MB 730.6 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.6/66.5 MB 732.8 kB/s eta 0:00:39   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.7/66.5 MB 733.0 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.7/66.5 MB 733.9 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.8/66.5 MB 735.4 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.8/66.5 MB 735.8 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.8/66.5 MB 737.2 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.9/66.5 MB 736.4 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.9/66.5 MB 739.2 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 38.9/66.5 MB 739.5 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 39.0/66.5 MB 743.0 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 39.0/66.5 MB 743.4 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 39.0/66.5 MB 744.8 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 745.0 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 743.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.1/66.5 MB 744.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.2/66.5 MB 720.0 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.2/66.5 MB 719.3 kB/s eta 0:00:38   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.3/66.5 MB 740.1 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.3/66.5 MB 740.1 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 39.6/66.5 MB 740.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 39.9/66.5 MB 754.2 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 39.9/66.5 MB 754.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 757.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 757.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.0/66.5 MB 755.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.3/66.5 MB 771.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.4/66.5 MB 760.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.6/66.5 MB 794.8 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.6/66.5 MB 796.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.6/66.5 MB 801.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.7/66.5 MB 803.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.7/66.5 MB 805.1 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 40.7/66.5 MB 805.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.7/66.5 MB 803.4 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.8/66.5 MB 802.9 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.8/66.5 MB 802.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.8/66.5 MB 800.8 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.8/66.5 MB 800.8 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.8/66.5 MB 798.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.9/66.5 MB 798.7 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.9/66.5 MB 798.7 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 40.9/66.5 MB 796.6 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.0/66.5 MB 798.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.0/66.5 MB 797.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.0/66.5 MB 797.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.1/66.5 MB 799.5 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.1/66.5 MB 799.6 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.1/66.5 MB 801.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.1/66.5 MB 799.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.1/66.5 MB 799.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.1/66.5 MB 799.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.2/66.5 MB 793.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.2/66.5 MB 792.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.2/66.5 MB 792.3 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.2/66.5 MB 791.5 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.3/66.5 MB 791.3 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.3/66.5 MB 788.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.3/66.5 MB 789.0 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.3/66.5 MB 787.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.4/66.5 MB 789.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.4/66.5 MB 790.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.4/66.5 MB 790.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.4/66.5 MB 790.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.4/66.5 MB 787.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.5/66.5 MB 785.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.5/66.5 MB 784.5 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.5/66.5 MB 785.6 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 41.6/66.5 MB 785.5 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.6/66.5 MB 785.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.6/66.5 MB 784.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.6/66.5 MB 783.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.6/66.5 MB 781.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 780.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 780.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 775.3 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 773.4 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 773.4 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 772.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.7/66.5 MB 771.4 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.8/66.5 MB 772.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.8/66.5 MB 772.0 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.8/66.5 MB 774.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.9/66.5 MB 774.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.9/66.5 MB 772.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.9/66.5 MB 770.8 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.9/66.5 MB 769.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.9/66.5 MB 768.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.9/66.5 MB 767.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.0/66.5 MB 766.8 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.0/66.5 MB 766.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.0/66.5 MB 765.5 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.0/66.5 MB 764.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.1/66.5 MB 766.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.1/66.5 MB 764.8 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.1/66.5 MB 764.3 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 765.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 763.0 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 763.0 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 763.0 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 760.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 759.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.2/66.5 MB 757.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.3/66.5 MB 758.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.3/66.5 MB 759.5 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.3/66.5 MB 756.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.3/66.5 MB 759.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.3/66.5 MB 757.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.4/66.5 MB 758.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 42.4/66.5 MB 755.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.4/66.5 MB 757.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.5/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.5/66.5 MB 759.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.5/66.5 MB 759.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.5/66.5 MB 759.4 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.5/66.5 MB 761.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.5/66.5 MB 761.2 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.6/66.5 MB 759.1 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 42.8/66.5 MB 701.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.0/66.5 MB 690.6 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.0/66.5 MB 689.6 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 689.5 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 689.5 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 685.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 685.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 684.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 684.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.1/66.5 MB 681.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.2/66.5 MB 681.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.2/66.5 MB 679.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.2/66.5 MB 679.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 43.2/66.5 MB 676.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.2/66.5 MB 676.6 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 676.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 674.6 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 673.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.3/66.5 MB 656.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.5/66.5 MB 663.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.5/66.5 MB 663.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.5/66.5 MB 662.5 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.5/66.5 MB 661.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.6/66.5 MB 660.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.6/66.5 MB 659.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.6/66.5 MB 658.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.6/66.5 MB 658.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.6/66.5 MB 655.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.6/66.5 MB 655.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 654.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 653.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 653.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 652.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 652.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 652.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 652.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 652.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 643.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 643.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.7/66.5 MB 640.2 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 640.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 640.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 637.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 639.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 637.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 637.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.8/66.5 MB 636.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.9/66.5 MB 635.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.9/66.5 MB 635.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.9/66.5 MB 635.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.9/66.5 MB 632.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.9/66.5 MB 632.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 43.9/66.5 MB 630.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 629.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 628.9 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 628.9 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 626.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 626.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 625.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 44.0/66.5 MB 625.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 624.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 624.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 624.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 620.1 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 620.3 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 619.4 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.1/66.5 MB 618.3 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.2/66.5 MB 616.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.2/66.5 MB 616.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.2/66.5 MB 615.9 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.2/66.5 MB 616.1 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.3/66.5 MB 615.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.3/66.5 MB 614.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.3/66.5 MB 614.0 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.3/66.5 MB 613.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.3/66.5 MB 612.4 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.3/66.5 MB 612.4 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 610.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 610.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 610.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 610.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 610.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 605.2 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.4/66.5 MB 604.0 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.5/66.5 MB 603.8 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.5/66.5 MB 603.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.5/66.5 MB 603.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.5/66.5 MB 603.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.5/66.5 MB 602.0 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.6/66.5 MB 601.2 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.6/66.5 MB 602.3 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.6/66.5 MB 601.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.6/66.5 MB 600.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.7/66.5 MB 600.4 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.7/66.5 MB 600.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.7/66.5 MB 600.3 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.8/66.5 MB 599.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.8/66.5 MB 599.2 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.8/66.5 MB 599.2 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.8/66.5 MB 597.6 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.8/66.5 MB 596.7 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.8/66.5 MB 595.5 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 44.9/66.5 MB 596.0 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 44.9/66.5 MB 596.3 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 44.9/66.5 MB 597.3 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.0/66.5 MB 597.1 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.0/66.5 MB 596.7 kB/s eta 0:00:37   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.0/66.5 MB 596.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.1/66.5 MB 596.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.1/66.5 MB 596.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.1/66.5 MB 596.8 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.2/66.5 MB 596.8 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.2/66.5 MB 596.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.2/66.5 MB 596.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.2/66.5 MB 596.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.3/66.5 MB 594.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.3/66.5 MB 594.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.3/66.5 MB 594.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.4/66.5 MB 594.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.4/66.5 MB 594.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.4/66.5 MB 594.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.4/66.5 MB 594.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.5/66.5 MB 594.8 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.5/66.5 MB 594.9 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.5/66.5 MB 594.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.5/66.5 MB 593.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.6/66.5 MB 593.2 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.6/66.5 MB 592.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.6/66.5 MB 591.8 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.7/66.5 MB 592.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.7/66.5 MB 592.4 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 45.7/66.5 MB 590.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.7/66.5 MB 591.0 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.7/66.5 MB 590.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.8/66.5 MB 590.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.8/66.5 MB 590.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.8/66.5 MB 590.2 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.8/66.5 MB 590.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.9/66.5 MB 590.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.9/66.5 MB 589.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.9/66.5 MB 590.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 45.9/66.5 MB 589.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.0/66.5 MB 588.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.0/66.5 MB 588.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.0/66.5 MB 588.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.0/66.5 MB 586.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.0/66.5 MB 586.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.0/66.5 MB 585.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.1/66.5 MB 585.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.1/66.5 MB 585.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.1/66.5 MB 584.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.1/66.5 MB 582.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.1/66.5 MB 582.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.1/66.5 MB 581.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.2/66.5 MB 581.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.2/66.5 MB 581.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.2/66.5 MB 580.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.2/66.5 MB 579.6 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.2/66.5 MB 579.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.3/66.5 MB 578.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.3/66.5 MB 578.5 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.3/66.5 MB 577.5 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.3/66.5 MB 576.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.3/66.5 MB 576.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.3/66.5 MB 575.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.4/66.5 MB 575.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.4/66.5 MB 574.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.4/66.5 MB 574.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.4/66.5 MB 574.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.4/66.5 MB 571.9 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.4/66.5 MB 571.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.5/66.5 MB 571.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.5/66.5 MB 571.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.5/66.5 MB 570.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.5/66.5 MB 570.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 46.5/66.5 MB 569.2 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.6/66.5 MB 568.1 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.6/66.5 MB 567.7 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.6/66.5 MB 568.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.6/66.5 MB 568.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.6/66.5 MB 565.9 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.7/66.5 MB 566.3 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.7/66.5 MB 565.6 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.7/66.5 MB 564.9 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.7/66.5 MB 564.5 kB/s eta 0:00:36   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.8/66.5 MB 565.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.8/66.5 MB 564.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.8/66.5 MB 564.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.8/66.5 MB 563.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.9/66.5 MB 563.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.9/66.5 MB 562.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.9/66.5 MB 562.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 46.9/66.5 MB 562.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.0/66.5 MB 561.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.0/66.5 MB 561.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.0/66.5 MB 561.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.1/66.5 MB 561.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.1/66.5 MB 561.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.1/66.5 MB 560.5 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.1/66.5 MB 560.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.2/66.5 MB 560.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.2/66.5 MB 559.1 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.2/66.5 MB 558.9 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.2/66.5 MB 559.0 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.3/66.5 MB 558.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.3/66.5 MB 558.4 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.3/66.5 MB 558.7 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 47.3/66.5 MB 558.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.4/66.5 MB 558.2 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.4/66.5 MB 557.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.4/66.5 MB 558.3 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.5/66.5 MB 557.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.5/66.5 MB 557.6 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.5/66.5 MB 557.8 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.5/66.5 MB 556.8 kB/s eta 0:00:35   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.6/66.5 MB 556.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.6/66.5 MB 556.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.6/66.5 MB 556.4 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.7/66.5 MB 556.6 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.7/66.5 MB 556.2 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.7/66.5 MB 555.9 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.7/66.5 MB 557.2 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.8/66.5 MB 557.2 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.8/66.5 MB 557.0 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.8/66.5 MB 556.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.9/66.5 MB 556.5 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.9/66.5 MB 556.2 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.9/66.5 MB 556.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 47.9/66.5 MB 555.8 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.0/66.5 MB 556.0 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.0/66.5 MB 555.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.0/66.5 MB 555.0 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.1/66.5 MB 555.0 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.1/66.5 MB 555.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.1/66.5 MB 554.9 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.1/66.5 MB 554.4 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.2/66.5 MB 554.3 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.2/66.5 MB 553.8 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 48.2/66.5 MB 553.1 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.2/66.5 MB 552.7 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.2/66.5 MB 552.8 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.3/66.5 MB 552.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.3/66.5 MB 550.8 kB/s eta 0:00:34   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.3/66.5 MB 551.4 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.3/66.5 MB 550.3 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.4/66.5 MB 550.3 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.4/66.5 MB 551.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.4/66.5 MB 552.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.4/66.5 MB 551.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.5/66.5 MB 551.3 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.5/66.5 MB 550.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.5/66.5 MB 549.9 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.5/66.5 MB 549.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.6/66.5 MB 549.1 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.6/66.5 MB 549.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.6/66.5 MB 549.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.6/66.5 MB 549.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.6/66.5 MB 547.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.7/66.5 MB 546.9 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.7/66.5 MB 546.6 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.7/66.5 MB 546.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.7/66.5 MB 545.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.8/66.5 MB 546.0 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.8/66.5 MB 545.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.8/66.5 MB 545.1 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.8/66.5 MB 545.6 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.9/66.5 MB 544.1 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.9/66.5 MB 544.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 48.9/66.5 MB 544.8 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 49.0/66.5 MB 544.2 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 49.0/66.5 MB 543.7 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 49.0/66.5 MB 543.5 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.1/66.5 MB 543.6 kB/s eta 0:00:33   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.1/66.5 MB 543.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.1/66.5 MB 543.7 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.2/66.5 MB 543.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.2/66.5 MB 543.9 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.2/66.5 MB 542.8 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.2/66.5 MB 542.8 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.2/66.5 MB 542.6 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.3/66.5 MB 542.0 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.3/66.5 MB 542.6 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.3/66.5 MB 542.6 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.4/66.5 MB 558.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.4/66.5 MB 558.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.4/66.5 MB 556.8 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.4/66.5 MB 556.3 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.5/66.5 MB 555.6 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.5/66.5 MB 555.3 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.5/66.5 MB 554.6 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.6/66.5 MB 555.3 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.6/66.5 MB 554.5 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.6/66.5 MB 553.4 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.6/66.5 MB 552.8 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.7/66.5 MB 551.2 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.7/66.5 MB 550.6 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.7/66.5 MB 548.9 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.7/66.5 MB 548.4 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.7/66.5 MB 547.6 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.8/66.5 MB 545.7 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.8/66.5 MB 544.4 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.8/66.5 MB 543.4 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 49.9/66.5 MB 551.5 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 49.9/66.5 MB 550.4 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 49.9/66.5 MB 549.0 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 49.9/66.5 MB 548.5 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 546.7 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 545.6 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 545.6 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.0/66.5 MB 543.1 kB/s eta 0:00:31   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.1/66.5 MB 527.8 kB/s eta 0:00:32   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.4/66.5 MB 543.4 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.4/66.5 MB 542.3 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.5/66.5 MB 541.3 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.5/66.5 MB 541.3 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.5/66.5 MB 539.2 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.5/66.5 MB 538.3 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.5/66.5 MB 536.7 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.6/66.5 MB 544.9 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.6/66.5 MB 543.7 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.6/66.5 MB 542.8 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.6/66.5 MB 541.4 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.7/66.5 MB 540.7 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 50.7/66.5 MB 540.7 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.7/66.5 MB 538.1 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.7/66.5 MB 538.1 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.7/66.5 MB 538.1 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.8/66.5 MB 534.8 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.8/66.5 MB 534.5 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.8/66.5 MB 534.6 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.9/66.5 MB 535.0 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.9/66.5 MB 535.0 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 50.9/66.5 MB 534.3 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.0/66.5 MB 535.5 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.0/66.5 MB 536.0 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.0/66.5 MB 536.0 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.0/66.5 MB 536.0 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.0/66.5 MB 533.2 kB/s eta 0:00:30   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.0/66.5 MB 533.5 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.1/66.5 MB 534.7 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.1/66.5 MB 534.3 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.1/66.5 MB 534.2 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.1/66.5 MB 534.2 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.2/66.5 MB 534.1 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.2/66.5 MB 533.4 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.2/66.5 MB 533.1 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.2/66.5 MB 533.2 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.2/66.5 MB 533.2 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.3/66.5 MB 532.3 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.3/66.5 MB 532.0 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.3/66.5 MB 530.4 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.3/66.5 MB 530.0 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.3/66.5 MB 529.6 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.4/66.5 MB 530.1 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.4/66.5 MB 532.2 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.4/66.5 MB 532.3 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.4/66.5 MB 533.2 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.5/66.5 MB 531.9 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.5/66.5 MB 531.5 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 51.5/66.5 MB 532.8 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.5/66.5 MB 532.8 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.5/66.5 MB 532.8 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.5/66.5 MB 532.8 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.6/66.5 MB 529.8 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.6/66.5 MB 529.7 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.6/66.5 MB 528.7 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.6/66.5 MB 529.0 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.6/66.5 MB 527.7 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.7/66.5 MB 529.5 kB/s eta 0:00:29   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.7/66.5 MB 529.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.7/66.5 MB 530.0 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.7/66.5 MB 529.0 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.7/66.5 MB 528.5 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.8/66.5 MB 528.1 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.8/66.5 MB 528.1 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.8/66.5 MB 527.2 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.8/66.5 MB 527.8 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.8/66.5 MB 527.6 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 51.9/66.5 MB 528.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.2/66.5 MB 528.0 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.2/66.5 MB 527.4 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.2/66.5 MB 527.4 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.3/66.5 MB 526.7 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.3/66.5 MB 526.3 kB/s eta 0:00:28   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.3/66.5 MB 526.0 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.3/66.5 MB 526.2 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 52.3/66.5 MB 526.0 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.4/66.5 MB 525.7 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.4/66.5 MB 524.8 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.4/66.5 MB 527.4 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.4/66.5 MB 527.9 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.4/66.5 MB 528.3 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.5/66.5 MB 528.1 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.5/66.5 MB 527.5 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.5/66.5 MB 527.3 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.5/66.5 MB 528.0 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.6/66.5 MB 527.5 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.6/66.5 MB 526.8 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.6/66.5 MB 527.3 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.6/66.5 MB 527.9 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.6/66.5 MB 527.3 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.6/66.5 MB 526.1 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 526.0 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 526.0 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 524.6 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 524.7 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 524.7 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 522.7 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.7/66.5 MB 523.4 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.8/66.5 MB 523.0 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.8/66.5 MB 522.8 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.8/66.5 MB 523.1 kB/s eta 0:00:27   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.8/66.5 MB 562.9 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.9/66.5 MB 562.1 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.9/66.5 MB 561.0 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.9/66.5 MB 559.7 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.9/66.5 MB 559.0 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 52.9/66.5 MB 557.6 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.0/66.5 MB 556.4 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.0/66.5 MB 555.4 kB/s eta 0:00:25   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.0/66.5 MB 569.6 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.1/66.5 MB 569.0 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.1/66.5 MB 566.9 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.1/66.5 MB 566.1 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.1/66.5 MB 564.6 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 53.2/66.5 MB 563.3 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.2/66.5 MB 562.0 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.2/66.5 MB 560.8 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.3/66.5 MB 560.6 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.3/66.5 MB 560.6 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.3/66.5 MB 563.5 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.3/66.5 MB 563.9 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.4/66.5 MB 566.1 kB/s eta 0:00:24   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.4/66.5 MB 568.6 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.4/66.5 MB 568.9 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.5/66.5 MB 568.4 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.5/66.5 MB 569.4 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.5/66.5 MB 569.6 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.5/66.5 MB 569.6 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.5/66.5 MB 581.0 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.6/66.5 MB 579.6 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.6/66.5 MB 578.6 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.6/66.5 MB 577.0 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.7/66.5 MB 576.1 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.7/66.5 MB 574.9 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.7/66.5 MB 574.3 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.7/66.5 MB 575.0 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.8/66.5 MB 576.0 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.8/66.5 MB 576.4 kB/s eta 0:00:23   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.9/66.5 MB 578.9 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.9/66.5 MB 578.8 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.9/66.5 MB 579.7 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 53.9/66.5 MB 579.7 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 54.0/66.5 MB 587.8 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 54.0/66.5 MB 587.8 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 54.0/66.5 MB 589.0 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 54.0/66.5 MB 588.2 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.0/66.5 MB 592.7 kB/s eta 0:00:22   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.1/66.5 MB 592.8 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.1/66.5 MB 593.0 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.1/66.5 MB 592.9 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.1/66.5 MB 597.8 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.2/66.5 MB 597.6 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.2/66.5 MB 597.8 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.2/66.5 MB 599.5 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.2/66.5 MB 599.5 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.3/66.5 MB 599.5 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.3/66.5 MB 599.9 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.3/66.5 MB 603.4 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.4/66.5 MB 603.6 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.4/66.5 MB 604.5 kB/s eta 0:00:21   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.4/66.5 MB 605.7 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.4/66.5 MB 605.5 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.5/66.5 MB 605.1 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.5/66.5 MB 606.6 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.5/66.5 MB 607.4 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.6/66.5 MB 609.1 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.6/66.5 MB 610.7 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.6/66.5 MB 610.7 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.6/66.5 MB 614.7 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.6/66.5 MB 615.1 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.7/66.5 MB 614.4 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.7/66.5 MB 614.0 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.7/66.5 MB 613.7 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.7/66.5 MB 615.0 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.8/66.5 MB 614.7 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.8/66.5 MB 613.5 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.8/66.5 MB 613.6 kB/s eta 0:00:20   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 54.8/66.5 MB 613.8 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 54.9/66.5 MB 614.5 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 54.9/66.5 MB 613.5 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 54.9/66.5 MB 613.7 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.0/66.5 MB 613.4 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.0/66.5 MB 613.9 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.0/66.5 MB 613.5 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.1/66.5 MB 616.4 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.1/66.5 MB 617.5 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.1/66.5 MB 616.3 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.1/66.5 MB 616.1 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.1/66.5 MB 616.1 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.2/66.5 MB 615.9 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.2/66.5 MB 615.6 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.2/66.5 MB 615.3 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.2/66.5 MB 615.3 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.3/66.5 MB 614.1 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.3/66.5 MB 613.5 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.3/66.5 MB 613.1 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.3/66.5 MB 612.4 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.4/66.5 MB 612.2 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.4/66.5 MB 611.9 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.4/66.5 MB 611.6 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.4/66.5 MB 610.8 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.5/66.5 MB 610.1 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.5/66.5 MB 611.8 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.5/66.5 MB 610.8 kB/s eta 0:00:19   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.5/66.5 MB 610.2 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.5/66.5 MB 610.5 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.6/66.5 MB 609.7 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.6/66.5 MB 609.5 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.6/66.5 MB 608.9 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 55.7/66.5 MB 609.1 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.7/66.5 MB 608.9 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.7/66.5 MB 608.0 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.7/66.5 MB 607.5 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.8/66.5 MB 609.0 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.8/66.5 MB 609.0 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.8/66.5 MB 609.2 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.8/66.5 MB 609.2 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.9/66.5 MB 607.7 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.9/66.5 MB 607.2 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 55.9/66.5 MB 609.2 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.0/66.5 MB 609.2 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.0/66.5 MB 609.0 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.0/66.5 MB 610.3 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.1/66.5 MB 610.1 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.1/66.5 MB 610.0 kB/s eta 0:00:18   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.1/66.5 MB 611.2 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.1/66.5 MB 610.8 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.2/66.5 MB 610.7 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.2/66.5 MB 610.8 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.2/66.5 MB 613.0 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.2/66.5 MB 613.7 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.3/66.5 MB 614.2 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.3/66.5 MB 613.7 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.3/66.5 MB 616.0 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.4/66.5 MB 616.0 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.4/66.5 MB 616.8 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.4/66.5 MB 618.3 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.5/66.5 MB 618.8 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 56.5/66.5 MB 619.1 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.5/66.5 MB 620.2 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.5/66.5 MB 620.8 kB/s eta 0:00:17   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.6/66.5 MB 622.5 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.6/66.5 MB 622.1 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.6/66.5 MB 622.1 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.7/66.5 MB 626.2 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.7/66.5 MB 625.6 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.7/66.5 MB 625.3 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.8/66.5 MB 628.5 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.8/66.5 MB 627.1 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.8/66.5 MB 626.9 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.8/66.5 MB 628.3 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.9/66.5 MB 630.0 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.9/66.5 MB 630.3 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 56.9/66.5 MB 630.7 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.0/66.5 MB 631.0 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.0/66.5 MB 631.7 kB/s eta 0:00:16   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.0/66.5 MB 631.6 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.1/66.5 MB 632.4 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.1/66.5 MB 632.9 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.1/66.5 MB 633.7 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.2/66.5 MB 634.6 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.2/66.5 MB 634.7 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.2/66.5 MB 634.4 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.3/66.5 MB 634.7 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.3/66.5 MB 634.3 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 57.3/66.5 MB 635.4 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.4/66.5 MB 635.5 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.4/66.5 MB 635.4 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.4/66.5 MB 635.9 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.4/66.5 MB 634.8 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.5/66.5 MB 633.8 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.5/66.5 MB 634.3 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.5/66.5 MB 634.2 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.5/66.5 MB 634.6 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.6/66.5 MB 634.3 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.6/66.5 MB 633.8 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.6/66.5 MB 633.8 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.6/66.5 MB 632.1 kB/s eta 0:00:15   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.7/66.5 MB 633.3 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.7/66.5 MB 633.2 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.7/66.5 MB 632.1 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.8/66.5 MB 633.3 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.8/66.5 MB 633.2 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.8/66.5 MB 632.6 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.8/66.5 MB 631.7 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.9/66.5 MB 632.5 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 57.9/66.5 MB 633.6 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.0/66.5 MB 633.1 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.0/66.5 MB 634.1 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.0/66.5 MB 634.1 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.1/66.5 MB 634.5 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.1/66.5 MB 635.2 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.1/66.5 MB 634.4 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.1/66.5 MB 633.6 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 58.2/66.5 MB 633.4 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.2/66.5 MB 632.8 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.2/66.5 MB 634.0 kB/s eta 0:00:14   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.3/66.5 MB 634.0 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.3/66.5 MB 634.8 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.4/66.5 MB 636.2 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.4/66.5 MB 637.2 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.4/66.5 MB 637.8 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.4/66.5 MB 637.8 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.5/66.5 MB 637.6 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.5/66.5 MB 638.8 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.5/66.5 MB 637.6 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.6/66.5 MB 639.1 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.6/66.5 MB 639.7 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.7/66.5 MB 639.7 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.7/66.5 MB 641.9 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.7/66.5 MB 642.4 kB/s eta 0:00:13   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.8/66.5 MB 644.2 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.9/66.5 MB 647.1 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.9/66.5 MB 648.3 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 58.9/66.5 MB 649.8 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 59.0/66.5 MB 649.4 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 59.0/66.5 MB 648.9 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 59.0/66.5 MB 648.5 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.1/66.5 MB 648.6 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.1/66.5 MB 647.0 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.1/66.5 MB 648.5 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.1/66.5 MB 648.7 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.2/66.5 MB 647.8 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.2/66.5 MB 649.7 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.2/66.5 MB 649.8 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.3/66.5 MB 648.7 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.3/66.5 MB 648.9 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.4/66.5 MB 648.4 kB/s eta 0:00:12   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.4/66.5 MB 648.2 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.4/66.5 MB 649.6 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.5/66.5 MB 650.5 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.5/66.5 MB 651.3 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.5/66.5 MB 650.8 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.6/66.5 MB 650.8 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.6/66.5 MB 651.7 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.6/66.5 MB 652.5 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.7/66.5 MB 654.1 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.7/66.5 MB 655.1 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.8/66.5 MB 656.6 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 59.8/66.5 MB 656.2 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 59.9/66.5 MB 656.4 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 59.9/66.5 MB 656.4 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 59.9/66.5 MB 655.1 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 59.9/66.5 MB 655.5 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 59.9/66.5 MB 654.1 kB/s eta 0:00:11   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.0/66.5 MB 654.7 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.0/66.5 MB 655.8 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.0/66.5 MB 655.8 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.1/66.5 MB 657.5 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.1/66.5 MB 658.3 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.1/66.5 MB 657.9 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.2/66.5 MB 658.6 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.2/66.5 MB 658.8 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.3/66.5 MB 661.4 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.3/66.5 MB 683.8 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.3/66.5 MB 682.7 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.4/66.5 MB 681.7 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.5/66.5 MB 657.7 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.5/66.5 MB 657.7 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.5/66.5 MB 657.7 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.5/66.5 MB 653.1 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 60.6/66.5 MB 651.8 kB/s eta 0:00:10   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 60.9/66.5 MB 671.6 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.0/66.5 MB 674.4 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.0/66.5 MB 674.4 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.0/66.5 MB 674.3 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.1/66.5 MB 674.3 kB/s eta 0:00:09   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.1/66.5 MB 674.2 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.1/66.5 MB 673.4 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.2/66.5 MB 674.6 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.2/66.5 MB 674.0 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.2/66.5 MB 673.4 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.2/66.5 MB 678.7 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.3/66.5 MB 678.4 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.3/66.5 MB 677.7 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.3/66.5 MB 677.9 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.4/66.5 MB 677.9 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.4/66.5 MB 681.4 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.4/66.5 MB 679.9 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.4/66.5 MB 679.9 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.4/66.5 MB 678.1 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 61.5/66.5 MB 681.0 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.5/66.5 MB 680.9 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.5/66.5 MB 682.6 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.6/66.5 MB 684.1 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.6/66.5 MB 683.5 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.6/66.5 MB 684.8 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.7/66.5 MB 685.8 kB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.7/66.5 MB 686.9 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.7/66.5 MB 687.4 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.8/66.5 MB 693.9 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.8/66.5 MB 694.2 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.8/66.5 MB 694.4 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.9/66.5 MB 696.6 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.9/66.5 MB 696.2 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 61.9/66.5 MB 698.7 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.0/66.5 MB 701.2 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.0/66.5 MB 703.0 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.0/66.5 MB 703.8 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.1/66.5 MB 703.8 kB/s eta 0:00:07   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.1/66.5 MB 742.2 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.1/66.5 MB 740.5 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.2/66.5 MB 737.6 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.2/66.5 MB 735.7 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.2/66.5 MB 735.7 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.2/66.5 MB 731.5 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.2/66.5 MB 729.4 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.3/66.5 MB 727.4 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.3/66.5 MB 725.1 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 62.3/66.5 MB 725.1 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 722.5 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 719.0 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 719.0 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 716.6 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 716.6 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 711.6 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.4/66.5 MB 709.7 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 711.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.5/66.5 MB 713.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.6/66.5 MB 693.8 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.7/66.5 MB 701.1 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.9/66.5 MB 717.9 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.9/66.5 MB 717.9 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.9/66.5 MB 717.9 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.9/66.5 MB 717.9 kB/s eta 0:00:06   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 62.9/66.5 MB 717.7 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 63.0/66.5 MB 728.2 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 63.1/66.5 MB 731.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 63.1/66.5 MB 731.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 63.1/66.5 MB 731.6 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 63.2/66.5 MB 733.4 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.2/66.5 MB 731.5 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.2/66.5 MB 733.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.2/66.5 MB 732.8 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.2/66.5 MB 730.6 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.3/66.5 MB 729.3 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.3/66.5 MB 728.3 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.3/66.5 MB 727.7 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.3/66.5 MB 728.9 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.4/66.5 MB 727.3 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.4/66.5 MB 727.6 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.4/66.5 MB 727.3 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.4/66.5 MB 725.8 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.4/66.5 MB 725.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.5/66.5 MB 724.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.5/66.5 MB 724.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.5/66.5 MB 722.1 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.5/66.5 MB 723.9 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.6/66.5 MB 724.0 kB/s eta 0:00:05   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.6/66.5 MB 724.0 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.7/66.5 MB 725.0 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.7/66.5 MB 726.0 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.7/66.5 MB 727.1 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.8/66.5 MB 730.1 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.8/66.5 MB 730.1 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.8/66.5 MB 729.7 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.9/66.5 MB 730.8 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.9/66.5 MB 731.0 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 63.9/66.5 MB 729.8 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 64.0/66.5 MB 730.2 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.0/66.5 MB 732.2 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.0/66.5 MB 733.8 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.1/66.5 MB 732.7 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.1/66.5 MB 734.6 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.2/66.5 MB 736.8 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.2/66.5 MB 736.6 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.2/66.5 MB 739.1 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.3/66.5 MB 737.3 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.3/66.5 MB 736.7 kB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.3/66.5 MB 739.3 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.4/66.5 MB 740.2 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.4/66.5 MB 736.4 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.4/66.5 MB 739.0 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.4/66.5 MB 739.0 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.5/66.5 MB 740.8 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.5/66.5 MB 741.1 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.6/66.5 MB 742.3 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.6/66.5 MB 743.4 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.7/66.5 MB 743.5 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.7/66.5 MB 743.7 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.7/66.5 MB 744.2 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.8/66.5 MB 744.2 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 64.8/66.5 MB 744.4 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 64.8/66.5 MB 747.7 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 64.9/66.5 MB 749.1 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 64.9/66.5 MB 751.2 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.0/66.5 MB 751.6 kB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.0/66.5 MB 755.0 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.0/66.5 MB 754.2 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.1/66.5 MB 754.5 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.1/66.5 MB 754.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.1/66.5 MB 754.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.2/66.5 MB 754.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.2/66.5 MB 755.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.2/66.5 MB 755.2 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.2/66.5 MB 755.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.3/66.5 MB 756.8 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.3/66.5 MB 757.5 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.4/66.5 MB 757.6 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.4/66.5 MB 757.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.4/66.5 MB 757.6 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.5/66.5 MB 760.2 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.5/66.5 MB 761.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.5/66.5 MB 761.7 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.5/66.5 MB 762.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.6/66.5 MB 763.3 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.6/66.5 MB 761.9 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 65.6/66.5 MB 763.1 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.7/66.5 MB 765.2 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.7/66.5 MB 768.4 kB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.7/66.5 MB 768.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.8/66.5 MB 767.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.8/66.5 MB 769.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.9/66.5 MB 770.2 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.9/66.5 MB 771.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.9/66.5 MB 769.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.9/66.5 MB 767.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 65.9/66.5 MB 767.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.0/66.5 MB 769.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.0/66.5 MB 767.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.0/66.5 MB 767.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.0/66.5 MB 767.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.1/66.5 MB 765.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.1/66.5 MB 768.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.2/66.5 MB 768.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.2/66.5 MB 771.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.2/66.5 MB 770.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.3/66.5 MB 770.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.3/66.5 MB 770.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.3/66.5 MB 769.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.3/66.5 MB 770.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.3/66.5 MB 770.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.4/66.5 MB 769.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.4/66.5 MB 769.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.4/66.5 MB 769.3 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 66.5/66.5 MB 770.1 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 MB 754.5 kB/s eta 0:00:00\nDownloading jaxtyping-0.2.28-py3-none-any.whl (40 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/40.7 kB ? eta -:--:--   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/40.7 kB ? eta -:--:--   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/40.7 kB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.7/40.7 kB 390.9 kB/s eta 0:00:00\nDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nDownloading ml_dtypes-0.3.2-cp39-cp39-macosx_10_9_universal2.whl (389 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/389.8 kB ? eta -:--:--   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/389.8 kB ? eta -:--:--   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/389.8 kB 664.9 kB/s eta 0:00:01   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.4/389.8 kB 615.5 kB/s eta 0:00:01   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.2/389.8 kB 675.8 kB/s eta 0:00:01   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.6/389.8 kB 757.3 kB/s eta 0:00:01   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.9/389.8 kB 638.5 kB/s eta 0:00:01   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.9/389.8 kB 638.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 153.6/389.8 kB 503.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 174.1/389.8 kB 504.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 204.8/389.8 kB 541.4 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 225.3/389.8 kB 538.0 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 235.5/389.8 kB 502.2 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 276.5/389.8 kB 552.9 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 286.7/389.8 kB 537.8 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 317.4/389.8 kB 537.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 337.9/389.8 kB 552.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 337.9/389.8 kB 552.6 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 358.4/389.8 kB 507.5 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 368.6/389.8 kB 499.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 389.8/389.8 kB 508.7 kB/s eta 0:00:00\nDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/65.5 kB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/65.5 kB ? eta -:--:--   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/65.5 kB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 41.0/65.5 kB 491.7 kB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 511.8 kB/s eta 0:00:00\nInstalling collected packages: typeguard, opt-einsum, ml-dtypes, jaxtyping, jaxlib, jax, jaxopt, equinox, lineax, ott-jax\nSuccessfully installed equinox-0.11.3 jax-0.4.25 jaxlib-0.4.25 jaxopt-0.8.3 jaxtyping-0.2.28 lineax-0.0.4 ml-dtypes-0.3.2 opt-einsum-3.3.0 ott-jax-0.4.5 typeguard-2.13.3\nNote: you may need to restart the kernel to use updated packages.\n\n\nThen we load the required pakages.\n\n\nCode\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nimport ott\nfrom ott.geometry import costs, pointcloud\nfrom ott.problems.linear import linear_problem\nfrom ott.solvers.linear import sinkhorn\n\n\n\n\n2.3.2 A world about OTT and JAX\nOTT is a python library that allows to compute and differentiate the entropic optimal transport. In this lab session, we will focus on entropic optimal transport computation, and not differentiation. differentiation will be takcled later.\nOTT is based on JAX, a package similar to PyTorch or TensorFlow, which allows to do automatic differentiation and GPU programming. It also provides useful primitives for efficient computation, such as the just-in-time (jit) compilation or the automatic vectorization map vmap. For more informations on JAX, see the tutorial https://jax.readthedocs.io/en/latest/notebooks/quickstart.html.\nUnlike PyTorch or TensorFlow, JAX is very close to numpy thanks to the jax.numpy package, which implements most of the numpy features, but for the JAX data structures. For this lab session, you only need to know how to manipulate jax.numpy Arrays and generate random numbers with jax.random.\nFirst, let’s have a look to jax.numpy and see that it works (almost) exactly as numpy. Usually, one imports jax.numpy as jnp as done in the above cells, and developp as with numpy, by just replacing np by jnp. Note that jax.numpy Arrays are called DeviceArray. For more informations on jax.numpy, see https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html.\n\n\nCode\nd = 5\nu = 5 * jnp.ones(5)\nId = jnp.eye(5)\nprint(type(u))\nprint(f\"u = {u}\")\nprint(f\"Id = {Id}\")\nprint(f\"Id @ u = {jnp.dot(Id, u)}\")\nprint(f\"sum(u) = {jnp.sum(u)}\")\nprint(f\"var(u) = {jnp.var(u)}\")\n\n\n&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\nu = [5. 5. 5. 5. 5.]\nId = [[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\nId @ u = [5. 5. 5. 5. 5.]\nsum(u) = 25.0\nvar(u) = 0.0\n\n\nWith numpy.random, you can generate random numbers on the fly without giving the seed. For example, np.random.rand() generates a random number \\(x \\sim U([0, 1])\\). Indeed, numpy.random uses an internal seed which is updated each time a random number generating function is called. On the other hand, with jax.random, we must give the seed each time we generate random numbers. To some extent, we want to always control the randomness. Moreover, we do not pass exactly a seed but a jax.random.PRNGKey key which is itself instantiated from a seed. Let’s see it on an example.\n\n\nCode\nrng = jax.random.PRNGKey(0)\nn, d = 13, 2\nx = jax.random.normal(rng, (n, d))\nprint(f\"x = {x}\")\n\n\nx = [[ 2.516351   -1.3947194 ]\n [-0.8633262   0.6413567 ]\n [-0.37789643 -0.6044598 ]\n [ 1.9069     -0.17918469]\n [-0.7583423  -0.5160155 ]\n [ 1.2666148  -0.12342127]\n [ 0.28430256 -0.17251171]\n [ 1.0661486   1.5814103 ]\n [-2.0284636  -0.13168257]\n [-0.14515765  0.21532312]\n [-0.69525063 -0.9314128 ]\n [-0.89809936 -0.25272107]\n [-0.34937173  1.8394127 ]]\n\n\nThen, to have new keys to generate new random numbers, we need to split the key via jax.random.split, which generate \\(n \\geq 2\\) new keys from a key.\n\n\nCode\nrng1, rng2, rng3 = jax.random.split(rng, 3)\na = jax.random.normal(rng1, (n, d))\nb = jax.random.normal(rng2, (n, d))\nc = jax.random.normal(rng2, (n, d))\nprint(f\"a = {a}\")\nprint(f\"b = {b}\")\nprint(f\"c = {c}\")\n\n\na = [[-0.38696066 -0.96707183]\n [ 1.0078175  -0.6096286 ]\n [-1.153353    1.0749092 ]\n [-1.2452031  -0.63885343]\n [ 0.01121208  0.2842425 ]\n [ 0.5296049   0.26609063]\n [ 0.8728492   1.0844501 ]\n [ 1.4472795  -0.82503337]\n [-0.41826957  0.21321987]\n [ 1.9602116   0.17687395]\n [-0.9978761  -2.0551765 ]\n [-0.4094941  -1.4577458 ]\n [-1.0969195  -0.66684234]]\nb = [[ 0.10911155 -0.45371595]\n [ 0.12062439 -0.06927001]\n [ 0.00600028  2.3732579 ]\n [-0.17656058  1.7653493 ]\n [-0.06429235  0.487175  ]\n [-1.1079016  -1.0277865 ]\n [-0.0553451  -0.28271845]\n [-0.9633478  -0.05370665]\n [ 0.20281292 -0.16658288]\n [ 0.8015828  -0.61697495]\n [-0.30176872 -1.1862007 ]\n [-3.106658   -0.03262986]\n [ 0.53711027  0.21359496]]\nc = [[ 0.10911155 -0.45371595]\n [ 0.12062439 -0.06927001]\n [ 0.00600028  2.3732579 ]\n [-0.17656058  1.7653493 ]\n [-0.06429235  0.487175  ]\n [-1.1079016  -1.0277865 ]\n [-0.0553451  -0.28271845]\n [-0.9633478  -0.05370665]\n [ 0.20281292 -0.16658288]\n [ 0.8015828  -0.61697495]\n [-0.30176872 -1.1862007 ]\n [-3.106658   -0.03262986]\n [ 0.53711027  0.21359496]]\n\n\nyou now know everything you need for the moment!\n\n\n2.3.3 Entropic optimal transport with OTT\nNow let’s use the implementation of the OTT Sinkhorn algorithm, on some random weighted point clouds. Then you will, by yourself, use it on the “croissant” transport example.\nLet’s first generate the data.\n\n\nCode\n# generate data\nrng = jax.random.PRNGKey(0)\nrng1, rng2 = jax.random.split(rng, 2)\nn, m, d = 13, 17, 2\nx = jax.random.normal(rng1, (n, d))\ny = jax.random.normal(rng2, (m, d)) + 1\na = jnp.ones(n) / n\nb = jnp.ones(m) / m\n\n\nThen, we have to define a PointCloud geometry which contains: * the point clouds x and y, * the cost function cost_fn, * the entropic regularization strength epsilon.\nNote that the geometry does not contain the weight vectors a and b, these are passed later.\nThe cost_fn should be an istance of ott.geometry.CostFn. Most of the usual costs are implemented. For example, the three costs \\(\\ell_1, \\ell_2\\) and \\(\\ell_2^2\\) are implemented. Here, we will focus on the \\(\\ell_2\\) cost, implemented by ott.geometry.costs.Euclidean. See https://ott-jax.readthedocs.io/en/latest/_autosummary/ott.geometry.costs.CostFn.html#ott.geometry.costs.CostFn for more information on the provided cost_fn.\nWe still choose epsilon to be \\(0.1 \\cdot \\bar{C}\\). To do this, we set relative_epsilon=True when instantiating the geometry. The term relative means that epsilon is chosen relatively to the mean of the cost matrix. Passing then epsilon=0.1, the value of epsilon used by Sinkhorn will be \\(0.1 \\cdot \\bar{C}\\).\n\n\nCode\n# define geometry\ngeom = pointcloud.PointCloud(\n    x=x, y=y,\n    cost_fn=costs.Euclidean(),\n    epsilon=1e-1,\n    relative_epsilon=True\n)\n\n\nWe then define an optimization problem from this geometry, which is the problem we will solve with the Sinkhorn algorithm. We instantiate this optimization problem as an object of the class linear_problem.LinearProblem. We pass the weight vectors a and b because they define the constraints of the linear problem. Then, we instantiate a Sinkhorn solver, object of the class sinkhorn.Sinkhorn, which we will use to solve this optimization problem.\nThe OTT library is designed in this way because it allows to solve other optimal transport problems, which do not necessarily have a linear problem structure, and which use other solvers than Sinkhorn.\n\n\nCode\n# create optimization problem\not_prob = linear_problem.LinearProblem(geom, a=a, b=b)\n\n# create sinkhorn solver\nsolver = sinkhorn.Sinkhorn(ot_prob)\n\n# solve the OT problem\not_sol = solver(ot_prob)\n\n\nThe ot output object contains several callables and properties, notably a boolean assessing the Sinkhorn convergence, the marginal errors throughtout iterations and the optimal transport plan.\n\n\nCode\nprint(\n    \" Sinkhorn has converged: \",\n    ot_sol.converged,\n    \"\\n\",\n    \"Error upon last iteration: \",\n    ot_sol.errors[(ot_sol.errors &gt; -1)][-1],\n    \"\\n\",\n    \"Sinkhorn required \",\n    jnp.sum(ot_sol.errors &gt; -1),\n    \" iterations to converge. \\n\",\n    \"entropic OT cost: \",\n    jnp.sum(ot_sol.matrix * ot_sol.geom.cost_matrix),\n)\n\n\n Sinkhorn has converged:  True \n Error upon last iteration:  0.00019092858 \n Sinkhorn required  5  iterations to converge. \n entropic OT cost:  29.436861\n\n\n\nQuestion: Compute the entropic optimal transport plan and cost for the “croissant” transport problem, with \\(\\ell_2\\) cost and \\(\\epsilon = 0.1 \\cdot \\bar{C}\\). Then, plot the optimal transport plan.\nAnswer:"
  },
  {
    "objectID": "posts/2024/Computation/PGM3.html",
    "href": "posts/2024/Computation/PGM3.html",
    "title": "数学者のための確率的グラフィカルモデル３",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Computation/PGM3.html#laplace-近似",
    "href": "posts/2024/Computation/PGM3.html#laplace-近似",
    "title": "数学者のための確率的グラフィカルモデル３",
    "section": "1 Laplace 近似",
    "text": "1 Laplace 近似\n(MacKay, 2003)"
  },
  {
    "objectID": "posts/2024/AI/RL2.html",
    "href": "posts/2024/AI/RL2.html",
    "title": "強化学習",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) (Schulman et al., 2015) から PPO Algorithm (Schulman et al., 2017)\nModel-based RL\n\n\n\n\nReferences\n\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2015). Trust region policy optimization. https://arxiv.org/abs/1502.05477\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. https://arxiv.org/abs/1707.06347"
  },
  {
    "objectID": "posts/2024/AI/Theory3.html",
    "href": "posts/2024/AI/Theory3.html",
    "title": "統計的学習理論３",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/AI/Theory3.html#正則化と汎化の関係",
    "href": "posts/2024/AI/Theory3.html#正則化と汎化の関係",
    "title": "統計的学習理論３",
    "section": "1 正則化と汎化の関係",
    "text": "1 正則化と汎化の関係\n\n1.1 非一様性1\nPAC 学習 の枠組みは，分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に依らない一様な評価を要請している．\nその結果，分類問題でも，PAC 学習可能であるためには仮設集合 \\(\\mathcal{H}\\) には VC 次元の有限性が必要十分となるのであった（統計的機械学習の基本定理）．\nこれは多くの場合強すぎる．\nそこで，必要な訓練データ数が，分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に依存することも許すことを考える．この緩めた学習可能性は，\\(\\mathcal{H}\\) が有限 VC 次元集合の有限合併であることに同値になる．\n\n\n\n\n\n\nnonuniformly learnable2\n\n\n\n\n定義 1 仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が 非一様に学習可能 であるとは，あるアルゴリズム \\(A\\) とある \\(h\\in\\mathcal{H}\\) に依存しても良い関数 \\[\nm^{\\mathrm{NUL}}:(0,1)^2\\times\\mathcal{H}\\to\\mathbb{N}\n\\] が存在して，任意の \\(\\epsilon,\\delta\\in(0,1)\\) と \\(h\\in\\mathcal{H}\\) と \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) について，\\(m\\ge m^{\\mathrm{NUL}}(\\epsilon,\\delta,h)\\) ならば確率 \\(1-\\delta\\) 以上で \\[\nR(A(S_m))\\le R(h)+\\epsilon\n\\] を満たすことをいう．\n\n\n\n論理的な構造は PAC 学習可能性 と非常に似ているが，確かに真に弱い条件になっている．\n\n\n\n\n\n\nCharacterization of Nonuniform Learnability3\n\n\n\n\n定理 1 分類問題 \\(\\mathcal{Y}=2\\) を 0-1 損失 \\(l=1_{\\Delta_\\mathcal{Y}^\\complement}\\) で考えるとする．仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) について，次は同値：\n\n\\(\\mathcal{H}\\) は非一様に学習可能である．\n\\(\\mathcal{H}\\) は有限 VC 次元集合の可算合併で表せる．\n\n\n\n\n\n\n1.2 構造的リスク最小化\nPAC バウンド の証明からも判る通り，推定誤差と近似誤差のトレードオフが存在する．\nすなわち，仮説の複雑性には代償がある．\nそこで，仮説集合 \\(\\mathcal{H}\\) を小さくする代わりに，アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) が探索する範囲を小さいものにし，実質的な仮説空間のサイズを抑えることも考えられる．これを 正則化 という．\n実際，深層学習も暗黙的正則化によりよい汎化性能を出していることが徐々に明らかになりつつある．4\nすなわち，真に汎化性能を上げたい場合は，経験誤差を最小化するだけでは十分ではなく，経験誤差を小さくしながら，関数が滑らかになるように帰納バイアスを入れる必要がある．\nこの枠組みを経験リスク最小化の代わりに，構造リスク最小化 といい，(Vapnik & Chervonenkis, 1974) により提案された．\nこの方向の研究の源流は，(Bousquet & Elisseeff, 2002) らの 安定性 の理論であった．これは「実質的な仮説空間」という考え方を導入することで，機械学習モデルの予測精度理論の精緻化も生んだ．\n\n\n1.3 枠組み：アルゴリズムに目を向ける\nリスクを \\[\nR(A,S):=\\operatorname{E}[l(A(S)(X),YT)]\n\\] 経験リスクと \\[\n\\widehat{R}(A,S_n):=\\frac{1}{n}\\sum_{i=1}^nl(A(S_n)(x_i),y_i)\n\\] として，アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) の関数とみる．5\nこの場合，仮説空間 \\(\\mathcal{H}\\) 上の一様な評価は，そもそも目指さない．\n\n\n1.4 安定性\n\n\n\n\n\n\nTip\n\n\n\n\n定義 2 (安定性6) アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) が，損失関数 \\(l\\) に関して \\(\\beta\\in(0,1)\\)-安定 であるとは，任意の \\(S\\subset(\\mathcal{X}\\times\\mathcal{Y})^n\\) に対して， \\[\n\\begin{align*}\n    &\\max_{i\\in[n]}\\operatorname{E}\\biggl[\\biggl|l(A(S)(x_i),y_i)\\\\\n    &\\qquad-l(A(S\\setminus\\{z_i\\})(x_i),y_i)\\biggr|\\biggr]\\le\\beta\n\\end{align*}\n\\] が成り立つことをいう．\n\n\n\nすなわち，学習データを１つ減らしたときの損失の変化が，ある一定以下であることをいう．\nこれは感度分析的な考え方であるが，実は正則化により，アルゴリズムは安定的な挙動をするようになり，安定性が汎化誤差の上界を与える！\n\n\n1.5 主結果\n\n\n\n\n\n\nTip\n\n\n\n\n定理 2 (安定なアルゴリズムに対する汎化バウンド7) \\(A\\) を \\(\\beta_1\\)-安定で，損失関数 \\(l\\) は上界 \\(M&gt;0\\) を持つとする．このとき，\\(1-\\delta\\) の確率で \\[\nR(A,S)\\le\\widehat{R}(A,S_n)+2\\beta+(4n\\beta+M)\\sqrt{\\frac{\\log1/\\delta}{2n}}.\n\\]\n\n\n\n\n\n1.6 アルゴリズムの安定性\n一方で，アルゴリズムの安定性を示すことは難しく，通常 admissibility と Bregman divergence を通じて議論されるようである．8"
  },
  {
    "objectID": "posts/2024/AI/Theory3.html#footnotes",
    "href": "posts/2024/AI/Theory3.html#footnotes",
    "title": "統計的学習理論３",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Shalev-Shwartz & Ben-David, 2014, p. 58) 第７章 も参照．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 59) 定義7.1．↩︎\n(Shalev-Shwartz & Ben-David, 2014, p. 59) 定理7.2 など．↩︎\n(Murphy, 2022, p. 455) も参照．↩︎\n(Bousquet & Elisseeff, 2002, p. 502)．↩︎\n(Bousquet & Elisseeff, 2002, p. 503)．↩︎\n(Bousquet & Elisseeff, 2002, p. 507) Theorem 12．↩︎\n(Bousquet & Elisseeff, 2002) 第５節．↩︎"
  },
  {
    "objectID": "posts/2024/AI/BAI.html",
    "href": "posts/2024/AI/BAI.html",
    "title": "これからはじめるベイズ機械学習",
    "section": "",
    "text": "現在，産業界における “AI” というと専ら，いくつかの限られた巨大 IT 企業が，巨大ニューラルネットワークを最尤推定で学習させ，これを基盤モデルとして公開し，我々一般庶民はそれを有効活用して下流タスクを安価に解くことだけ考えるという営みを指す．\nその産業や生活への破壊的な影響を憂慮しながらも，雨乞いをする日々である．\nAI はそんなものではない．AI はこれにかぎるものではない．\nAI が真に我々の友となり，我々の日常をほんとうに豊かにするは，AI の進歩だけが必要なのではなく，人間との協業が得意になる必要がある．\nそのための第一歩はすでに明らかである．不確実性の定量化 である．\nつまり，「その AI には何が出来て何が出来ないか」「AI の出力がいつ信頼にたるもので，いつ人間の介入が必要であるのか」がわかりやすい形で伝わるコミュニケーション様式をそなえている必要があるのである．1\n筆者の知る限り，ここにある全てのナラティブは現時点では全く広く語られているものではなく，筆者も最初の１年の研究生活を通じて朧げながら見えて来たばかりのものである．\n不確実性の定量化は，機械学習モデルを民主化し，我々の民芸に取り込むための重要な一歩である（のではないだろうか？）．\n本稿はこの発見を共有するために書いた．筆者の反芻不足から，冗長な部分も多いだろうが，少しでも，琴線に触れるものがあれば幸いである．2"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#ベイズ機械学習のすすめ",
    "href": "posts/2024/AI/BAI.html#ベイズ機械学習のすすめ",
    "title": "これからはじめるベイズ機械学習",
    "section": "1 ベイズ機械学習のすすめ",
    "text": "1 ベイズ機械学習のすすめ\n我々が AI をより信頼するためには，何が必要だろうか？\n筆者の考えでは，信頼への第一歩は 不確実性の定量化 が出来るようになることのはずである．\nそしてそのためには ベイズ機械学習 (Bayesian Machine Learning) の発展による本質的解決が必要不可欠である．本稿はこの点を説明するために執筆されたものである．\n筆者に言わせれば，ベイズ機械学習が，今後数年間で AI が経験すべき進展の方向である．この山を越えれば，今まででさえ思っても見なかった未来がひらけてくるだろう．\n\nAlthough considerable challenges remain, the coming decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework. (Ghahramani, 2015, p. 452)\n\n\n1.1 ベイズとは何か？\n機械学習において，確率論的なモデリングに基づいたアプローチを ベイズ機械学習 ともいう．典型的には，モデルの全変数上の結合分布をモデリングし，ベイズ規則によりパラメータのベイズ推定を行う，という手続きからなる．そのため，確率論的アプローチ や モデルベースアプローチ も同義語として用いられる．3\n一方で，頻度論的 という言葉は，よく非ベイズ的アプローチを示す接頭辞として用いられる．典型的には，損失関数を設定し，これを最小化するパラメータを探索することによって実行される．\nこの２つのアプローチは互いに対照的であり，統計学の始まりから基本的な二項対立の図式をなしてきた．\n\n\nContrast of the two main approachs to Machine Learning\n\n\n\n\n\n\n\n\nBayesian\nFrequentist\n\n\n\n\nInference is4\nMarginalization\nApproximation\n\n\nComputational Idea5\nIntegration\nOptimization\n\n\nObjective\nUncertainty Quantification\nRecovery of True Value\n\n\nEmphasis\nModelling\nInference\n\n\n\n\nしかし，機械学習の時代においては，互いの弱みを補間し合う形で発展していくと筆者は考える．特に，現状の推論偏重でモデリング軽視の風潮が，重要な実世界応用の多くを阻んでしまっている．機械学習の世界樹は実は２本あるのである．\n\n\n1.2 ベイズと頻度論との違い\nベイズと頻度論では，確率の解釈も異なるかも知れないが，数学的枠組みとしてはベイズの方が一般的な枠組みであり，また手続き上は，モデリングを重視するか，推論を重視するかの違いでしかない．\n実際，殆どの場合，頻度論的手法はある特定の事前分布を持ったベイズ手法とみなせ，逆も然りである．\nデータから推論を行うには，何らかの仮定が必ず必要であり，それを明示的にモデルに組み込むのがベイズで，推論アルゴリズムにより自動化する精神を持つのが頻度論的手法である．\nその結果，優秀な推論アルゴリズムが日夜驚異的なスピードで提案され，今や機械学習手法は教師あり学習・教師なし学習・強化学習の全てで目覚ましい発展を見た．\nしかし，ベイズと頻度論の２つの柱のバランスを欠いた発展はここまでである．今や，頻度論的な手法を採用した際に，自分たちがどのような仮定を置いたのか全く明瞭な知識を欠いてしまっている．一方で，現実のビッグで複雑なデータを扱うためには，もはや確率的なモデリングを避けては通れない．6\n極めて本質的で強大な敵に対面しつつあるのである．\nだが，現状の病理は明らかであり，頻度論とベイズの手法の間に対応をつけ，足並みを揃えることで次の前進が約束されてる．この意味で，２つの世界樹が必要なのである．\nさらに，ベイズ推論は帰納的推論の確率論的拡張と見れるため，エージェントの合理的な学習と意思決定の最良のモデル（の一つ）と信じられている．7\nしたがって，ベイズ流解釈により手法を理解し，最適化流解釈により手法を実装する．これがあるべき機械学習の未来であると筆者は考える．\n\n\n1.3 ２つの世界樹\n今こそ，この２つの手法は根底では繋がっていることをよく周知し，この２つの視座を往来しながら適材適所に使うことが大事だと筆者は考える．\nしかしそのためには，ベイズ機械学習の発展が遅れている現状を鑑みて，ベイズの手法のより一層の発展と理解の深化が必要である．8\n本章「ベイズ機械学習のすすめ」は，ベイズの手法の特に肝心と思われる３つの側面を指摘して終わる．以下３章を通じて，\n\n第 2 節 ベイズは不確実性を定量化する\nBayes の方が不確実性の定量化が得意であるため，そのような応用先では頻度論的な手法よりも，Bayes バージョンの手法を用いることが出来ると便利である．\n第 3 節 ベイズは分布という共通言語を与える\nBayes による統一的な扱いが理論的に有用である場面が増えている．その際に，Bayes による理論解析と最適化による実際の推論という適材適所の協業が未来の方向であるかも知れない．\n第 4 節 ベイズは理解を促進する\nベイズの手法が敬遠されていた理由も，換言すれば，「事前分布」という得体の知れないものを通じて，理論的深淵と直結するためである．ベイズ手法の研究が理論的な解明を要請する．だからこそ，数学者の魂を持った者がこの途を通ることは人類に大きく資すると筆者は考える．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-uncertainty-quantification",
    "href": "posts/2024/AI/BAI.html#sec-uncertainty-quantification",
    "title": "これからはじめるベイズ機械学習",
    "section": "2 ベイズは不確実性を定量化する",
    "text": "2 ベイズは不確実性を定量化する\n\n2.1 不確実性の定量化の必要性\n機械学習と統計学が単なる道具ではなく，人間のより大きなシステムの一環を単独で担う場面が増えてきた．例えば，\n\n金融・経営・政策決定などの分野で，意思決定に繋げるデータ解析をするとき\n科学において，発見や仮説を検証するためのデータ解析をするとき\nロボットや自動車などの自動化をし，社会に実装するとき9\n医療診断や裁判などの場面で，専門家を補助するシステムを作るとき\n\nこれらのいずれの例でも，システムの一部を担うにあたって，不確実性を定量化しておくことが欠かせない．その出力を用いるのが人間である場合も勿論，別の機械学習モデルである場合は尚更である．\nつまり，人間社会で優秀であるだけでなくホウレンソウと信頼獲得も重要であるように，機械学習モデルも性能の高さと正確さだけでなく，いつその結果を信頼して良いのかを「どの程度」という指標と共に知らせてくれることが信頼関係の基本となるだろう．\n実際，殆どの場面で，データから高い確証度で言えることと，そうではないことでは全く違う意味を持つ．それぞれの場面での例には，次のようなものがあるだろう：\n\nデータから高い確証度で言えることと，意思決定者による采配が必要な部分を分離できない限り，意思決定プロセスの一部として組み込むことが難しく，結局機械学習手法が全く採用されないということもあり得る．\n結果の再現可能性が科学の基本的な要請である以上，その結果の不確実性を実験結果に付記することは基本的な科学的態度である．後述（第 2.3.1 節）するが，\\(p\\)-値や信頼区間などの統計量はこれに応えるものではない．\nロボットや自動車の自動化 AI システムは，いくつかのモデルを組み合わせて作ることになるだろう．個々が十分な性能を持っていても，小さな誤差が累積してシステムとしての性能を著しく低下させることがある．これを防ぐために，統一した方法での不確実性の取り扱いが必要である．\n個々人の権利と法益が衝突する場面にも AI が利用されより良い生活が実現されるには，法的な解釈可能性が担保される必要があることが，実は大きな難関として我々を待っている．その第一歩は，不確実性の可視化になるだろう．10\n\n以上の内容は，結果の 解釈可能性 でも全く同じことが言えるだろう．\n\n\n2.2 信頼のおける AI システム\n上述の点をまとめると，機械学習手法と人間社会がよりよく共生していくには AI の 信頼性 (trustworthyness) が必要とされているのである．不確実性の定量化と解釈可能性は，AI が人間社会で信頼を獲得するにあたって根本的な要素になるだろう．\n現状の手法の延長でこの信頼性の問題は扱えず，新たな手法が必要とされている．Bayesian approach や probabilistic approach と呼ばれている試みは，まさにこれに応えるものであり，近年急速に発展している．\n\n\n2.3 不確実性を扱うには Bayes が必要である\n実装は頻度論的な手法の方が簡単で高速であることが多いが，不確実性の定量化には向かない．\nこのような場面では，頻度論的手法を頻度論的に改善する，という方向は筋が悪いと思われる．このようなときこそ，もう一つの世界樹であるベイズの方法を用いるべきである．\nこれを，科学における再現性の危機を例にとって確認したい．11\n\n2.3.1 再現性の危機\n多くの実験科学では不確実性の定量化が必要不可欠である (Krzywinski & Altman, 2013)．\n\nIt is necessary and true that all of the things we say in science, all of the conclusions, are uncertain … (Feynman, 1998)\n\n再現性の危機 (replication crisis) とは，多くの実験において報告されている統計的有意性が，再現実験において得られないことが多いという問題を指し，2010年代の初めから多くの科学分野において問題として取り上げられてきた．12\nその理由は明白である．信頼区間は集合値の推定量であるため，「分散」が十分大きいならば，データセットを変えて何回も計算することでいずれは非自明なものを得ることが出来るのである．そのため，信頼区間や \\(P\\)-値を報告するだけでは，結果の信頼性については何も保証されないのである．\nその結果多くの科学分野では Bayes 統計学による不確実性の定量化に移行しつつある (Herzog & Ostwald, 2013), (Trafimow & Marks, 2015), (Nuzzo, 2014)．\n信頼区間と信用区間の違いに注目して，その違いを解説する．\n\n\n2.3.2 信頼区間と信用区間\n「95 % の信頼区間」と言ったとき，「95 % の確率で真の値がその範囲に含まれるような区間」だと思いがちであるが，これはどちらかというと信用区間の説明であり，信頼区間は計算するごとに値が変わってしまう確率変数である ことを見落としがちである．13\nつまり，信頼区間は頻度論的な概念であり，「真の値」がまず存在し，区間自体が変動し，95 % の確率で被覆するというのである．今回見ている信頼区間が，別のデータセットで計算した場合にどう変わるかについては全く未知である．\nこのことは，信頼区間は「真のパラメータの値」で条件づけて得るものであるが，信用区間はデータによって条件づけて得るものであるという点で違う，とまとめられる．この２つの混同は「何で条件づけているか？」を意識することで回避することができる．14\n誤解を恐れず言うならば，再現性の危機とは，信頼区間というサイコロの出目によって科学が踊らされていたということに他ならない (Nuzzo, 2014)．15\n\n\n2.3.3 なぜベイズを用いれば良いのか？\nこれは，信頼区間や \\(P\\)-値などの頻度論的な手法は，しばしば尤度原理に違反するためである．16\n換言すれば，何らかのモデルと事前分布に関するベイズ手法と等価である，すなわち，Bayesianly justifiable (Rubin, 1984) とみなせない手法は，何らかの意味でデータを十分に反映できていない可能性が高くなる．\n従って，ベイズの手法が原理的に最も適切である場面が多い．一方でその計算の困難さや，全てのステップをモデリング段階に組み込む点を回避するために，種々の頻度論的な実装は考え得て，頻度論的な手法はそのような運用においては健全であるとの指標にもなる．17\nBayes により手法を理解し，頻度論的に手法を実装することが，あるべき姿勢であると思われる．\n\nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice. (Rubin, 1984)\n\n\n\n\n2.4 ベイズ深層学習という夢\n深層モデルはその性能の高さから，最も実世界応用が期待されるモデルであるが，パラメータが極めて多いため，特にベイズ化することが難しいと言われている．\n例えば，ハルシネーション (hallucination) として，LLM が「事実に基づかない」情報を生成してしまうことが問題とされているが，これも不確実性の定量化の問題に他ならない．18\nその他の場面でも，不確実性の定量化には conformal prediction などの事後的な手法が試みられている．19 これらはどのようなブラックボックスに対しても適用可能である一方で，対症療法というべきものであり，ベイズ流の解釈をすることで直接的に事後分布を求めるという根本的な解決にも，もっと注力されるべきである．\nベイズによる不確実性の定量化は，自然であるだけでなく，より有用な不確実性の定量化を与えるものだと予想している．20\n加えて，事前分布を変えることで，種々の帰納バイアスを加えるという「プロンプトエンジニアリング」ならぬ「プライヤーエンジニアリング」の理論が樹立できるかもしれない．すでに，公平性，同変性，スパース性，共変量シフトへの頑健性などを達成するための事前分布が考えられている．21\n\n\n2.5 分野全体の動向\n現状の機械学習モデルと実応用との乖離は，他の側面でも生じている．\nまず，訓練データが実際の運用環境を十分に反映できていないということは極めて頻繁に起こるだろう．この現象を 分布シフト といい，機械学種モデルの予測性能だけで無く，分布外汎化 (out-of-distribution generalization) 能力も重視するという潮流が生じている．\nさらに，一度訓練したモデルを，分布シフト自体が移り変わっていく環境で，微調整のみによって繰り返し使い続けるという使用を想定した 継続学習 (continual learning) という考え方もある．22\n章を変えて別の角度から議論を続けよう．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-distributional-representation",
    "href": "posts/2024/AI/BAI.html#sec-distributional-representation",
    "title": "これからはじめるベイズ機械学習",
    "section": "3 ベイズは分布という共通言語を与える",
    "text": "3 ベイズは分布という共通言語を与える\n\n3.1 継続学習という発想\n継続学習は，機械学習モデルをより動的で実際的な環境でも使えるようにするための新たな枠組みである．そこまで，教師あり学習モデルがすでに実用的な性能を獲得したということでもある．\nつまり，単に「教師あり」「教師なし」の１タスクを解く営みは爛熟しつつあり，機械学習の理論と応用の最先端は，より深い森に分け入りつつあるのである．\nここにおいて，ベイズ流の接近が統一的な取り扱いを与えるという美点が，さらに重要でもはや必要不可欠な役割を果たすものと思われる．\n\n3.1.1 ベイズ推論が与える統一的枠組み\nベイズ推論とは，事前分布 というものを設定して，これをデータによって更新するという営みである（その更新規則は Bayes の公式が与える）．\n事前分布をどう設定すれば良いか？の問題は，ベイズ推論の初期からの問題であった．極めて自由度が高いことが，逆にベイズ推論が実際のデータ解析の場面において敬遠される一因ともなっていた．\n\n\n3.1.2 ベイズと最適化との協業\nしかし，継続学習が当たり前になった社会において，全てのパラメータ値を事前分布と事後分布とみなし，全ての学習過程をベイズの公式という統一的な方法で更新すると捉えられることは，極めて大きな利点になり得る．\nというのも，継続学習においては，学習を繰り返すうちに過去に学んだ内容を忘れ去ってしまうという 壊滅的忘却 (catastrophically forgetting) が最大の困難である．\n理論的には，分布のベイズ更新の繰り返しとして見る方が極めて見通しが良い．一方で，事後分布の近似が十分でない場合，実際にベイズ更新を行うことは性能に悪影響を与える．\nそこで，理論解析や設計をベイズの観点から行い，実際の推論は最適化ベースで行うという適材適所により，壊滅的忘却を緩和できる可能性がある (Farquhar & Gal, 2019)．\n\n\n\n3.2 例：強化学習への分布によるアプローチ\n\nwe believe the value distribution has a central role to play in reinforcement learning. (Bellemare et al., 2017)"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-inductive-bias",
    "href": "posts/2024/AI/BAI.html#sec-inductive-bias",
    "title": "これからはじめるベイズ機械学習",
    "section": "4 ベイズは理解を促進する",
    "text": "4 ベイズは理解を促進する\n我々はもはや機械学習を通じて，自分たちが何をやっているのかわかっていない．この愚かさを AI に継がせてはならない．\n\n4.1 なぜベイズ法の発展が遅れたか？\nベイズ法の採用は，自分たちが何をやっているかへの理解と解釈可能性を刺激するという側面がある．\nその理由は簡単である．ベイズ推論は，モデルとその上の事前分布を定めれば，あとはベイズ更新規則をどう計算するかの問題となり，近似手法は様々あれど，もはや推論手法に選択の余地はない．\n換言すれば，その分解析者がモデルと事前分布の特定を全てこなす必要があるのであり，解析者に確率モデリングへの理解を強要するところがある．\nしかしこれは「面倒なことは全てアルゴリズムにやってほしい」という精神とは対立するため，ベイズの美点であると同時に，ベイズの発展を阻害してきた遠因の一つでもあった．\nこれを指して「事前分布の選択に恣意性が入る」という通り文句がよく使われるが，実際は，頻度論的手法における「どのような目的関数をどのように最適化すれば良いか？」という恣意性に変換されているのみであり，問題を先送りにして，「ベイズ法 対 頻度論的手法」という虚構の対立を作り上げているのみである．\n機械学習のポテンシャルが具現化したいまこそ，この困難に立ち向かう必要があるが，この問題は最適化や頻度論的な立場から見るより，ベイズの立場から見た方が，理論的な見通しが良いようである（第 4.4 節）．\n\n\n4.2 帰納バイアスの明確化の必要性\n機械学習の真の理解のためには，各モデルの帰納バイアスを明確化する必要がある．\n\n4.2.1 帰納バイアスとは何か？\n現状の AI システムは大量のラベル付きデータが必要であり，多くの現実的に有用なタスクでこのような教師データが用意できるわけではない．\n一方で，人間は遥かに少ないデータから効率的に学習することができる．\n\n\n\nNumber of Training Tokens BabyLM Challenge\n\n\nその違いは，進化が我々生物に授けた 帰納バイアス にあると考えられている．\n我々には遺伝的に継がれている生まれ持った学習特性があり，より効率的に学習出来るのかも知れない．\n事実，一度事前学習をした LLM は，極めて少ないデータにより新しいタスクを学習することができるがわかりつつある (Zhou et al., 2023)．LLM の事後調整に関する稿 も参照．\n\n\n4.2.2 事前分布に向き合わずにやり過ごしてきた\n現状，多くの機械学習手法は確率的な方法を取っていない．これは事前分布を明示せずに（ひょっとしたら明後日の方向に向かって）行われる Bayes 学習手法であるとみなせる．\n現状の機械学習の成功は，事前分布に関する知識なしに到達されたものであり，それ故の限界がある．例えば，現状のままではモデルにどのような帰納バイアスが組み込まれているか不明瞭である．23\n\n\n4.2.3 帰納バイアスに対するベイズ的視点\nデータの空間 \\(\\mathcal{X}\\) 上の任意のモデル \\(\\mathcal{M}\\) の周辺尤度 \\(p(x|\\mathcal{M})\\) は，24 ベイズ流には事後確率として捉えられ，全てのデータ \\(x\\in\\mathcal{X}\\) 上に有限な測度を定める．25\nよって，全てのモデルは，あるデータを得意とするならば他のデータについては不得意であることを免れない．これは no free lunch 定理と呼ばれる定理の一群により推測されており，分類問題などの簡単なタスクを除いて完全な形式的表現はまだ持たない作業仮設である．\n\n\n\nA Probabilistic Perspective of Genelization (Wilson & Izmailov, 2020)\n\n\n例えば，基盤モデル とは，インターネット上のデータから最大限人間の言語というものに関する帰納バイアスを取り込んだ，パラメータ上の初期設定であると見れる．\nこれは，あるパラメータ空間上の理想的な事前分布からのサンプリングであるかも知れない．それ故，種々の下流タスクに対して，小さなモデル変更のみにより適応することが出来る．\n大規模言語モデルの能力創発現象は，帰納バイアスを十分取り込むことにより自然に解かれるタスクであったのかもしれない．\n\n\n4.2.4 worst-case analysis からの脱皮\n帰納バイアスを明確にせず，やり過ごしてきたつけが，特に学習理論においても現れている．\n現状の統計的学習理論は全て，worst-case analysis であるが，実用上は全くそうではない．「動くモデル」には暗黙の帰納バイアスが入っており，これに明るくなる必要があるのである．\n2024 年に生きる我々は，worst-case analysis からの脱皮を迫られている．\n\n\n\n4.3 数学者の哲学\nBayes の見方は，機械学習モデルを底流する数理的枠組みになっている．仮に次の Mac Lane の言葉が数学者のあるべき態度の１つであるとするならば，この意味での数学者には Bayes の立場から機械学習を研究することを特におすすめする．\n\nHowever, I persisted in the position that as mathematicians we must know whereof we speak, be it a homotopy group or an adjoint functor. (Mac Lane, 1983, p. 55)\n\n数理統計学に始まり，数学者の統計や機械学習分野への参入は，推論手法の解析が想像されるかも知れない．\nしかし，真の数学的理解は，手法の数学的な機械仕掛けを紐解くだけでなく，それぞれの手法がモデルとしてどのような仮定の下で成り立っているかを，モデリングの観点から理解することにもあると筆者には思われる．\n現状，後者の視点が大変に不足しており，数理的な知識に支えられた大局観というものがない．個々の数学的な道具に捉われず，大局的な構造を捉える数理的枠組みが必要である．\nこれに応えるのがベイズの枠組みであると筆者は信じる．\n推論とモデリングという双対的な営みは深い数理的な構造を持っていることが明らかになりつつある．この大局的構造の解明と理論構築には，ベイズの観点から光を照らしてくれるような，Mac Lane の意味での数学者的な魂が必要とされているのである．\n\n4.3.1 Bayes の数学\nBayes 流の解釈では，どんなにモデルが複雑で巨大になろうとも，推論とは積分に他ならない．\n\n\n\nBayes’ Theorem (density form)\n\n\n全ての（尤度原理に則った）推論は，事後分布の関数としてなされる（べきである）．\n実際の実装は，その近似として実行される（べきである）．\nよって，実装とモデリングの段階を明確に分離する枠組みを提供している上に，極めて普遍的な枠組みである．\nというのも，Bayes 流のモデリングは，Markov 圏 上の図式と見ることができ（第 5.2 節），普遍的である上に，数学的にも最も直接的で直感的な表現であると思われる．\n圏として持つ代数的性質は，モデルの結合・分解が自由に出来るということに繋がり，モジュール性 が高いということになる．\n\nI basically know of two principles for treating complicated systems in simple ways: the ﬁrst is the principle of modularity and the second is the principle of abstraction. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. Michael I. Jordan excerpted from (Frey, 1998)\n\n分布を明示的に用いた 確率核 を通じてのモデリングは，なぜだか数学的に極めて自然なアプローチを提供してくれるようである．\n\n\n4.3.2 ベイズの代数・幾何・解析\n上述したように，ベイズのモデリング法と学習規則は本質的に代数的なところがある．\n加えて，分布を基本言語とするために，ベイズ推論においては空間 \\(\\mathcal{P}(\\mathcal{X})\\subset\\mathcal{M}^1(\\mathcal{X})\\) が極めて基本的な役割を果たす．\nサンプリングは \\(\\mathcal{P}(\\mathcal{X})\\) 上の幾何学に関係が深く，情報幾何学や最適輸送などの発展が見られている．\n一方で最適化は \\(\\mathcal{P}(\\mathcal{X})\\) 上の解析学に関係が深く，古くから機械学習分野では \\(\\mathcal{P}(\\mathcal{X})\\) 上の様々な汎函数が ダイバージェンス の名前で考察されており，その勾配流として種々の最適化手法が理解できる．\n\n\n4.3.3 Bayes に繋げる数学\n通常の頻度論的手法は，うまくいくことが先であり，理論が後付けされる．そしてその理論もどこか ad-hoc というべきであり，worst-case で漸近論的である．\nこれらに Bayes 的な解釈を与えることで，暗黙のうちにどのような仮定を課しているモデリング手法に相等するのか明確にされる．特に，非漸近論的な知見を与えてくれる数少ないの道の一つである．\n\n\n\n4.4 ベイズ推論とみる美点\nベイズ推論自体への理解だけでなく，種々の頻度論的手法を（特定の環境下での）ベイズ推論の近似として理解することは，新たなアルゴリズムの開発に有用であるという合意が形成されつつあるようである．26\n最適化に基づく手法の計算効率性は，正確なベイズ推論に勝る場面も多い．ここで注意すべきは，ベイズ推論の実行が肝要であり，その実装は最適化に依ろうと，積分近似に依ろうと大した違いではないのである．\n「ベイズ推論は多くの最尤法に基づく手法よりも，自然な正則化がなされるために過学習の問題がない．」と説明されるが本来は逆である．多くの最適化に基づく手法は，目的関数の選択に恣意性があり，その選択を誤り続けているために過学習という問題が生じている，という方が，後世の教科書に載る表現なのではないかと筆者は考えている．\nそこで，種々の既存手法のベイズ流の解釈を探究することは，より良い推論アルゴリズムの開発に資すると考えられている．\nこの方向の近年の発展をいくつか紹介したい．\n\n4.4.1 ベイズ学習規則\n現状の機械学習は，統計学，連続最適化，計算機科学の知識を総動員して開発された種々の推論手法によって支えられている．\nその性能は驚異的なスピードで向上しているが，それぞれの手法がどのような仮定をモデリングの段階で課しているかが不明瞭であり，どの手法を使うべきかの統一的な枠組みは得られていない．\nこの現状の抜本的な改善が，それぞれの手法のベイズ流の解釈を探究することで得られると考えられる．\nその枠組みの一つが ベイズ学習規則 (Khan & Rue, 2023) である．\n(Khan & Rue, 2023, p. 4) では，ベイズ流の解釈を持つ種々の手法が他より優れている理由として，目的関数に現れるエントロピー項が 自然勾配 の概念を通じて自然な正則化を与えることが，ベイズ学習規則という新たな理論的枠組みの中で示されている．\n\n\n4.4.2 例：強化学習\n強化学習でも，モデルベースのアプローチが取り入れられつつあり (Deisenroth & Rasmussen, 2011)，さらに学習と制御をベイズ推論と見ることが，アルゴリズムの設計において有用であることが提唱されつつある：\n\nCrucially, in the framework of PGMs, it is sufficient to write down the model and pose the question, and the objectives for learning and inference emerge automatically. (Levine, 2018)"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#bayes-機械学習の例",
    "href": "posts/2024/AI/BAI.html#bayes-機械学習の例",
    "title": "これからはじめるベイズ機械学習",
    "section": "5 Bayes 機械学習の例",
    "text": "5 Bayes 機械学習の例\n\n\n\n\n\n\n要約\n\n\n\n深層学習モデルにより教師あり学習は十分に発展し，多くの訓練データが得られる場面では驚異的な性能を発揮するようになった．\nこの発展は，モデリングの仮定に捉われずに純粋にアルゴリズムの開発に集中することが出来るという頻度論的な枠組みの利点を有効活用する形で達成された．\nしかし，殆どの実世界応用では，不確実性のモデリングが必要不可欠である．この点を後回しにして性能を追求することで得た栄華である．だからこそ，極めて高い性能を誇るモデルを，実世界応用の場面で有効活用する手段を我々はまだ知らないのである．\nその鍵はベイズにある．安全性，信頼性，柔軟性……．これらの21世紀の社会の要請に応えるためには，ベイズ機械学習手法の発展と，既存の手法のベイズ流の理解とが追いつくことが，第一歩である．\n\n\n既存の深層学習モデルは，「教師あり学習」という枠組みや，画像の分類タスクや自然言語処理のタスクなど，広く周知された問題設定とデータセットが存在する．\n一方で，ベイズ機械学習における対応物はまだ十分に周知されていないようである．\nベイズ機械学習では「損失を最小化する」という枠組みの中でなるべく性能の良い推論手法を探す，というわかりやすい枠組みがある訳ではないようである．\nそこで，本章ではベイズ機械学習の近年の発展を概観することを試みる．\n\n5.1 Bayes 深層学習\nニューラルネットワークモデルは，隠れ素子数が無限大になる極限において，Gauss 過程モデルに漸近することが知られている (Neal, 1996)．Gauss 過程とはノンパラメトリックなベイズ機械学習手法の代表である．この対応を通じて，深層学習のベイズ流の解釈が進められている．\nこの稿の執筆後，本稿をまとめるかのようなアブストラクトを持ったポジションペーパー (Papamarkou et al., 2024) が公開された\n\nIn the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. (Papamarkou et al., 2024)\n\n深層学習をベイズ化することで，上にあげた\n\n不確実性の自然な定量化\n継続学習への柔軟な接続\n科学的営みの促進\n\nなどが目指せる．特に，現状の大規模な基盤モデルをベイズ化する悲願を真っ向から論じている．\n\n\n5.2 確率的グラフィカルモデル\n歴史的に，（確率的）モデリングは，主に（確率的）グラフィカルモデルを通じて機械学習の分野に導入された．\nそのため，20世紀に入ったばかりの頃は，Bayes 機械学習の唯一の例は確率的グラフィカルモデルなのであった．27\nだが，確率的グラフィカルモデルは，極めて普遍的で，従来の因果推論・階層モデル・欠測モデル・潜在変数モデル・構造方程式モデルなどの発展を包含する統一的な枠組みであることをより広く認識すべきである．\n\n5.2.1 ベイジアンネットワーク\nBayesian Network は Markov 圏上の図式であり，方向関係のある変数間の関係をモデリングする最も直接的な方法である．\n\n\n5.2.2 構造的因果モデル\n\n\n5.2.3 階層モデル\n階層モデルとは，ベイズの枠組みでは，観測変数・潜在変数の区別なく，モデルを自由に結合出来る点を利用したモデリング手法である．\n\n\n5.2.4 モデルの属人化\n大きなデータも，属人化医療や推薦システムなど多くの文脈では小さなデータの寄せ集めであり，そうでなくともその構造を正しく捉え，全ての不確実性を取り入れた柔軟なモデリングをすることで，さらに密接な形で社会に取り入れることができる．28\n\n\n\n5.3 確率的プログラミング\n\n5.3.1 アルゴリズムのプログラミングから，モデルのプログラミングへ\nベイズ流の解釈では，解析者の恣意的な選択はモデリングの段階に集中しており，モデルが決定すれば推論手法は自動的に従う．\nこのパラダイムでは，推論手法は背後に隠し，解析者はモデリングに集中するための新たなプログラミング言語があっても良いはずである．\nこのような言語を 確率的プログラミング (Probabilistic Programming) 言語と呼ぶ．\n\n\n5.3.2 確率的プログラミングはグラフィカルモデルの拡張である\n確率的グラフィカルモデルをどのようにプログラムに落とし込むかというと，確率核をシミュレーターとして実装するのである．\n逆に，シミュレーションが可能な限りどのようなモデルも実装できるので，確率的グラフィカルモデルの真の拡張であると言える．29\n\n\n5.3.3 Simulation-based Inference\n上述の通り，シミュレーターがあればモデルが定義でき，モデルがあれば推論ができる．さらに，棄却法，重点サンプリング法，MCMC，SMC などの Monte Carlo 法のレパートリーにより，殆どあらゆるシミュレーションと推論が統一的に実行できる．これが Bayes 推論の強みである．\n\n\n\n5.4 Bayes 最適化\nベイズはシステムの一部として自然に組み込まれると論じたが（第 2.1 節），現状その最先端をいくのがベイズ最適化の分野である．\nベイズ最適化は最も簡単な形では，未知の関数 \\(f:X\\to\\mathbb{R}\\) の最大値点を求める問題を，逐次意思決定問題 として解く手法である．\nベイズ数値計算 (O’Hagan, 1991) の現代的な再解釈とも捉えられる．30\nこの際，未知の関数 \\(f\\) を Gauss 過程などでモデリングし，不確実性の高い点からサンプル \\(f(x_1),f(x_2),\\cdots\\) を取って最も効率の良い方法で最大化していくことを目指す．\nベイズ最適化は多腕バンディット問題と関係が深く，２つの問題は共に一方向のエージェント・環境相互作用しか仮定していないという形での強化学習への入り口である．\n\n\n5.5 確率的データ圧縮\n殆どの（可逆）データ圧縮アルゴリズムは，シンボルの列に対する確率的モデリングと等価である．31 そしてモデルの予測精度が良いほど，データの圧縮率は高い．\nしたがって，より良いベイズ（ノンパラメトリック）モデルの開発と，より幅広いデータに対するデータ圧縮技術の発展とは両輪である．32\n\n\n5.6 モデルの自動発見\n機械学習の精神の一つに，データからの知識獲得をなるべく自動化したいというものがある．\nベイズの方から，統計解析自体を自動化する Automatic Statistician (Lloyd et al., 2014) という試みがある．これはデータを説明するモデルを自動発見し，結果を自然言語でまとめてくれる上に，モデルに含まれる不確実性に関しても報告してくれる．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#footnotes",
    "href": "posts/2024/AI/BAI.html#footnotes",
    "title": "これからはじめるベイズ機械学習",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこれは Human-AI interaction におけるガイドライン (Amershi et al., 2019), (Bansal et al., 2019) でも明確にされている点である．この方向への試みの代表がベイズ機械学習，というわけではないが，筆者はベイズ機械学習の興隆は信頼のおける AI システムの構築にための極めて盤石な土台になるだろうと論じる．↩︎\n本稿執筆後に，ほとんど同じ論調を，深層学習や基盤モデルを中心に，遥かに明瞭に述べた論文 (Papamarkou et al., 2024) を見つけたので，賢明な読者はぜひこちらを参考にしていただきたい．↩︎\n(Broderick et al., 2023, p. 2) など．↩︎\nこの違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” (Ghahramani, 2015, p. 454)．↩︎\n“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” (Ghahramani, 2015, p. 454)．このように，その用いる手法も鮮やかに対照的に見えるが，積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．↩︎\n(Broderick et al., 2023) が極めて説得的にこの点を指摘している．↩︎\n合理的な信念の度合い (degree of belief) は確率の公理を満たす必要がある，という主張は Cox の名前でも呼ばれる．この点から，Bayes の定理は，帰納的推論の確率論的な拡張だとも捉えられる．“This justifies the use of subjective Bayesian probabilistic representations in artificial intelligence.” “Probabilistic modelling also has some conceptual advantages over alternatives because it is a normative theory for learning in artificially intelligent systems.” (Ghahramani, 2015, p. 453)．↩︎\n現状，日本にてベイズ機械学習を専業として研究を進めている人は Emtiyaz Khan に限ると思われる．(Ghahramani, 2015, p. 452) でも “Probabilistic approaches have only recently become a mainstream approach to artificial intellifence, robotics, and machine learning.” と述べられている．↩︎\n“The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars.” (Chen et al., 2023)．↩︎\nモデルの予測結果に不確実性の定量化が伴われていたならば，モデルを信用出来ない場面で意思決定者がこれを信用したため責任があるのか，使用者には非難可能性がないのか，モデル設計者に過失があったと言えるのかの議論に，足場を与えることが出来るだろう．↩︎\n(Gal & Ghahramani, 2016) も参照．↩︎\n心理学においては「再現性問題が大きく注目される大きな契機となった「超能力論文」が出版されたのが 2011 年である」 (平石界 & 中村大輝, 2022) ようである．計量経済学における 信頼性革命 (Angrist & Pischke, 2010) は，再現性の危機の，もう一つの革新的な解決法である．↩︎\n「それでは，信頼区間は不確実性の正しい定量化を与えないではないか！」ということになるが，その通りなのである．\\(P\\)-値を計算する過程とは，帰無仮説で条件付けているだけであり，データの関数でもある．\\(P\\)-値の確率変数としての分散が大きいほど，何回か同じ実験を繰り返せばすぐに小さな \\(P\\)-値が得られることになる．これは 基準確率の誤謬 と似ている．↩︎\n“Confidence intervals suffer from an inverse inference problem that is not very different from that suffered by the NHSTP. In the NHSTP, the problem is in traversing the distance from the probability of the finding, given the null hypothesis, to the probability of the null hypothesis, given the finding.” (Trafimow & Marks, 2015)↩︎\n(Nuzzo, 2014) には，Fisher が最初に用いてから，Neyman-Pearson 理論がこれを排除したものの，コミュニティが \\(P\\)-値を誤解して都合の良いように利用するようになるまでに至った歴史が説明されている．↩︎\n(Murphy, 2022, p. 201) の議論も参照．↩︎\n(Efron, 1986) も示唆深い．↩︎\n(Mohri & Hashimoto, 2024), (Papamarkou et al., 2024, p. 3) 2.1節 なども指摘している．↩︎\n(Novello et al., 2024) では out-of-distribution detection, (Mohri & Hashimoto, 2024) は LLM の hallucination への応用．↩︎\n「筆者は，conformal prediction などの post-hoc な手法は，便利かも知れないが，「信頼区間」や「\\(P\\)-値」のような側面（第 2.3 節）も併せ持つのではないかと危惧しながら見ている．」と当初は書いていたが，どうもそう簡単な話ではないようである．(Papamarkou et al., 2024) を読んで思った．↩︎\n(Papamarkou et al., 2024, p. 5) 3.4節．↩︎\n(Wang et al., 2024) が最新のサーベイであるようだ．↩︎\nPhilipp Hennig Probabilistic ML - Lecture 1 - Introduction “Statistical Learning Theory is about Bayesian Reasoning when you don’t say out aloud what the prior is.”↩︎\nこれを 証拠 (model evidence) ともいう．↩︎\n事前分布として非有限な測度を用いた場合など，例外もある．↩︎\n“most conventional optimization-based machine-learning approaches have probabilistic analogues that handle uncertainty in a more principled manner.” (Ghahramani, 2015, p. 458)．↩︎\n(Neal & Hinton, 1998) など．↩︎\n(Ghahramani, 2015, p. 458) はこれを モデルの属人化 (personalization of models) と呼んでいる．↩︎\n(Ghahramani, 2015, p. 453) “probabilistic programming offers an elegant way of generalizing graphical models, allowing a much richer representation of models.”．↩︎\n“More generally, Bayesian optimization is a special case of Bayesian numerical computation, which is re-emerging as a very active area of research, and includes topics such as solving ordinary differential equations and numerical integration.” (Ghahramani, 2015, p. 456)．↩︎\n“All commonly used lossless data compression algorithms (for example, gzip) can be viewed as probabilistic models of sequences of symbols.” (Ghahramani, 2015, p. 456)．↩︎\n(Steinruecken et al., 2015) は記号列に対するノンパラメトリックモデルを改良することで，データ圧縮アルゴリズム PPM を改良した良い例である．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html",
    "href": "posts/2024/AI/Semiconductor.html",
    "title": "半導体入門",
    "section": "",
    "text": "スライドを全画面で開くにはこちら"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#半導体入門",
    "href": "posts/2024/AI/Semiconductor.html#半導体入門",
    "title": "半導体入門",
    "section": "1 半導体入門",
    "text": "1 半導体入門\n\n1.1 半導体とは？\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．1\nこのような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．2 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．\n\n\n1.2 半導体発見の歴史\n(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，3 その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える (Sze & Lee, 2012, p. 1)．4\nBraun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．\n(Round, 1907) はダイオードが電界を印加することで発行することがある (electroluminescence) ことを発見した．\n\n\n1.3 基本用語のまとめ\n半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した ウエハ (wafer) 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．\n\n\n\nチップの断面構造\n\n\n一つのウエハから多数のチップが作成され，その各単位を ダイ (die) ともいう．\nしかし，普段我々が目にする IC チップ は パッケージ されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．\n32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．5\n\n\n1.4 集積回路が出来るまで\nまず回路を設計し，原版（マスター）を作る．これを フォトマスク (photomask) または レティクル (reticle) という．\nこれをウエハに転写するには，フォトリソグラフィ (Photolithography) を用いる．シリコンウエハの形成は，Czochralski 法 (Czochralski, 1918) による．6\nリソグラフィ自体は 1798 年からあり，Niépce が 歴青 が感光剤の役割を果たすことを発見し，カメラの発明と同時に発見された．\nエッチングに耐性のある感光剤を使えば，半導体デバイスの製造に応用できると気づいたのは (Andrus, 1957) である．この技術は半導体製造コストの 35 %を占めており，半導体市場の急成長はほとんどこの技術の進歩と両輪であると言う者も多い．7\nシリコン表面に酸化被膜を形成することで不純物原子の移動を阻止できることは (Frosch & Derick, 1957) が発見した．\n以上の技術を用いて，最初の IC は (Kilby, 1959) が作った．Jack Kilby はその後 2000 年にノーベル物理学賞を受賞する．\nしかし真に大量生産可能にし，半導体産業を大きくしたのは (Noyce, 1959) の発明であった．これは，現在主流の製法の基である Planar Process (Hoerni, 1960) で作られた．\n実際に，どのように半導体チップを製造するかについては次節 2 で詳しく解説する．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#sec-manufacturing",
    "href": "posts/2024/AI/Semiconductor.html#sec-manufacturing",
    "title": "半導体入門",
    "section": "2 半導体の製造",
    "text": "2 半導体の製造\n半導体の製造段階は，典型的には次の３つに大別される．\n\n設計\n\n回路・レイアウト作成 (design)\nフォトマスク (photomask) 作成\n\n前工程：次の３工程を繰り返ことで何層もの膜を積層する\n\n成膜 (deposition)\nパターン転写 (exposure)\n食刻 (etching)\n\n後工程\n\n角切り (dicing)\n封入 (packaging)\n品質検査 (inspection)\n\n\n日立の解説 も参照．\n\n2.1 設計工程\n18に分類されることもある．\n\n2.1.1 回路・レイアウト作成\nEDA (Eleotron Design Automation) を用いて設計を行う．\n\n\n\n2.2 前工程\n\n2.2.1 基本３工程\n成膜，パターン転写，食刻の３工程を繰り返すことでウエハ上に構造を作っていくのであるが，その目的は大きく５つに分類出来る．\n\n素子分離領域形成\n酸化被膜により，素子間の絶縁を形成する．\nwell 形成\nトランジスタの基盤となる領域に，食刻の代わりにイオンの添加する．\nトランジスタ形成\nウエハ基盤上にトランジスタ素子を形成する．\n電極形成\nシリコン基盤上のトランジスタに届くように，すでに形成された層に穴 (contact hole) をあけ，導体を埋め込む．\n配線層形成\n基本３工程を繰り返すことで，トランジスタ層上を分厚い配線層で覆う．8\n\n\n\n2.2.2 異物検査と洗浄\nほとんどの工程間に，異物検査と洗浄の工程が必要になる．\n日立の製品例，ウェーハ欠陥検査\n\n\n2.2.3 表面酸化\n熱酸化法では，酸素や高温のスチームを当てることで，表面に SiO2 の酸化被膜を形成し，絶縁体として用いる．\n\n\n2.2.4 成膜\n\n\n2.2.5 パターン転写\nフォトマスク上から紫外線を当てることで，フォトレジストを感光させる．次の工程で感光部分のみを食刻することで，パターン該当部分のみに酸化被膜を残すことが出来る．\n\n\n2.2.6 食刻\n現像後は，寸法計測を行う (ADI: After Development Inspection)．これは走査性電子顕微鏡 (SEM: Scanning Electron Microscope) である CD-SEM などを用いて行う（日立の製品例）\nこれにより，正しくパターンが転写されていることが確認されたのち，食刻を行い，再び寸法計測を行う（AEI: After Etch Inspection などと呼び分ける）．\n最後に，残ったフォトレジストはオゾンやプラズマにより灰化 (ashing) により除去する．\nコンダクターエンチング（日立の例）\n\n\n2.2.7 イオン添加\nイオンを注入するとアモルファスとなるため，一度再加熱をして (annealing) 再結晶化する．\n\n\n\n2.3 後工程"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#半導体デバイス",
    "href": "posts/2024/AI/Semiconductor.html#半導体デバイス",
    "title": "半導体入門",
    "section": "3 半導体デバイス",
    "text": "3 半導体デバイス\n\n3.1 構成要素\n整流作用を示す接合には，ショットキー接合と pn 接合がある．\n\n\n3.2 双極から MOS へ\nMOS (Metal-Oxide-Semiconductor) の３層構造を用いたトランジスタが採用される前は，双極トランジスタ (bipolar transistor) (Shockley, 1949) を用いた TTL (Transistor-Transistor Logic) 回路が主流であった．\nしかしこれは，トランジスタの間の絶縁が難しく，密度を上げることが難しかった．そのような中で MOSFET (Metal-Oxide-Silicon Field-Effect Transistor) が開発された (Kahng & Atalla, 1960)．MOSFET 技術は現代の半導体市場の 95% に関連する．9\nMOSFET は自己絶縁構造を持つため，これ以上の絶縁処理を必要とせず，双極トランジスタの 10 %の面積で済んだ．\n\n\n3.3 CMOS (Complementary Metal-Oxide-Semiconductor)\nMOS が初め採用した設計論理のは NMOS や PMOS であったが，常に直流電流を消費する必要があった．\nしかし現代では，P 型と N 型の MOSFET を相補的に用いる CMOS が集積回路における支配的な技術である．\nその秘訣は消費電力の少なさにあり，トランジスタの \\(0,1\\) の切り替えの際に生じる電力（動的エネルギー）のみが消費される．10\n\nCMOS技術は、当初アメリカの半導体業界では、当時より高性能だったNMOSを優先して見過ごされていた。しかし、CMOSは低消費電力であることから日本の半導体メーカーにいち早く採用され、さらに進化し、日本の半導体産業の隆盛につながった。CMOS\n\n\n\n3.4 現代に汎在する半導体\n計算機を構成する要素は多いが，Moore の法則により，多くが同一のチップの上に載ってしまい，不可視化が進んでいる．11\n\n3.4.1 プロセッサ\nコンピュータの CPU と言ったときに，１枚のチップを意味するようになったのは 1971 年の Intel 4004 が初めてである．12\n3mm × 4mm のチップ上に 2300 の MOSFET を備え，大きな机ほどの CPU を備えた IBM コンピュータに匹敵する処理能力を持っていた．13\n\n\n3.4.2 半導体メモリ\n現代でメモリといえば RAM (Random Access Memory) を指す．本来はアクセスする順番に制約があった SAM (Sequential Access Memory) に対して作られた言葉であったが，現代では ROM (Read Only Memory) との対義語として理解されることが多いようである．\nSRAM (Static RAM) は半導体メモリの一種であり，DRAM (Dynamic RAM) と比べて高速である．１ビットあたり６から８のトランジスタを使用したフリップフロップ回路により情報を記憶するため，定期的なリフレッシュが不要で高速な読み書きが可能であるが，集積率を上げることが出来ず，大容量メモリには向かない．14\nDRAM (Dynamic RAM) (Dennard, 1967) はチップ内にコンデンサを備えており，１つのコンデンサで１ビットを表現する．これを読み出すのに１つのトランジスタ MOSFET を使うのみであるから，SRAM に比べて安価であるが，電荷は時間と共に散逸するため，定期的にリフレッシュする必要があり，消費電力は大きい．15\n\n\n\nFrom (Patterson & Hennessy, 2014, p. 380)\n\n\nDRAM と SRAM はいずれも揮発性である．不揮発性の半導体メモリには フィラッシュメモリ がある．これには 蜉蝣ゲートMOSFET という素子で捉えた電子により情報を記憶することで不揮発性を実現している．\nフラッシュメモリには NAND 型と NOR 型があり，前者が主流である．\n\n\n\nMemory Revenue May, 2022. Source: Omedia\n\n\n\n\n3.4.3 Graphics Processing Unit\n描画を扱うチップは従来 VGA コントローラーと呼ばれていたが，1999 年には１つのチップで描画タスクの殆どをこなせるようになり，特に NVIDIA GeForce 256 は GPU という名称で売り出された．\nこうして GPU は元来の 3D グラフィクスに特化した存在から，徐々に CPU を補完する多様なタスクに柔軟に対応できるように，プログラム可能で，大量のコアを持って並列計算可能なものに進化していった．近年の CPU はマルチコアのものが多いが，現在の GPU は 1000 コアを超えるものも多い．\n\n\n3.4.4 Language Processing Unit\nLPU は，Google で TSU (Tensor Processing Unit) のプロジェクトに初期から従事していたエンジニア Jonathan Ross が 2016 年に創業したスタートアップ Groq の 登録商標 である．\nGroq が Samsung と協力して 実現した LPU (Abts et al., 2022) は eDRAM を持つ ASIC (Application Specific Integrated Circuit) であり，メモリのバンド幅と計算密度を増やし，逐次処理に特化することで特に言語処理に特化している．\n2/20/2024 に デモ を公開した．\nLPU は推論に，GPU は学習に特化しており，相補的な役割を演じながら AI の進化を支えていく可能性がある．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#物理",
    "href": "posts/2024/AI/Semiconductor.html#物理",
    "title": "半導体入門",
    "section": "4 物理",
    "text": "4 物理\n\n4.1 単体の半導体"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#文献レビュー",
    "href": "posts/2024/AI/Semiconductor.html#文献レビュー",
    "title": "半導体入門",
    "section": "5 文献レビュー",
    "text": "5 文献レビュー\n(Rudan et al., 2023) は辞典として使える．(Van Rossum, 2005) は凝縮系物理学のハンドブック内でのエントリ．\n\n5.1 産業\n(Miller, 2022) は Tufts 大学の Chris Miller による書籍．\n和書としては，(菊池正典, 2023) は NEC 社員による半導体業界の解説書．\n\n\n5.2 教科書\n(Sze & Lee, 2012) は浮遊ゲート MOSFET の発明者でもある Simon Min Sze（施敏）の著作で，半導体分野で最も多く引用される教科書とされている．第２版なら 和訳 もある．\n(May & Spanos, 2006) は California 大学 Davis 校の現学長 Gary May と California 大学 Berkeley 校の Costas Spanos による書籍．\n(May & Sze, 2003) もある．さらに発展的なものは (Pierret, 2003)．\n\n\n5.3 理論\n(Kittel, 2018) が固体物理学の標準的な入門書とされている．(Böer & Pohl, 2018) が特に半導体物理学の専門書になる．(Huebener, 2019) は前２つの橋渡しの役割をするが，重点は超伝導にある．\n\n\n5.4 その他\n(Richard, 2023) は SignalFire という VC に所属する著者による書．\n(Lau, 2021) は AMS Pacific Technology の 技術アドバイザー による書籍．\n(Evstigneev, 2022) はカナダ Memorial 大学 の 凝縮系物理学者 が書いた．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#footnotes",
    "href": "posts/2024/AI/Semiconductor.html#footnotes",
    "title": "半導体入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Böer & Pohl, 2018, p. 4), (Huebener, 2019, p. 73) Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う (Madelung, 1978, p. 17)．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である (Madelung, 1978, p. 211)．↩︎\n(Huebener, 2019, p. 75)．↩︎\n(Huebener, 2019, p. 73) 特に伝導体と半導体の境界部分で強く見られた．↩︎\n現代では，このような接合を金属-半導体接合 (metal-semiconductor contact) または Schottky 接合 といい，Ohmic 接合と対比する．↩︎\n(Patterson & Hennessy, 2014, p. 27) も参照．↩︎\n一方で GaAs の形成は Bridgman 法 (Bridgman, 1925) による．最も，この化合物が半導体であると発見されたのは (Welker, 1952) になってようやくのことである．(Sze & Lee, 2012, p. 6) も参照．↩︎\n(Sze & Lee, 2012, p. 6) など．↩︎\n今日の IC ではトランジスタ層は１層のみで，絶縁層で仕切ることで２〜８層の金属導体の配線層をその上に設ける． (Patterson & Hennessy, 2014, p. 26)．↩︎\n(Sze & Lee, 2012, p. 4) など．↩︎\n(Patterson & Hennessy, 2014, p. 41)．↩︎\n(Patterson & Hennessy, 2014, p. 379)．↩︎\n(Hoff et al., 1996) が開発者自ら歴史を振り返っている．当時は Intel も出来て３年しか経っていない新興企業であった．↩︎\n(Sze & Lee, 2012, p. 8) など．↩︎\n(Patterson & Hennessy, 2014, p. 379)．↩︎\n(Sze & Lee, 2012, p. 8)，(Patterson & Hennessy, 2014, p. 379) 参照．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体",
    "title": "半導体入門",
    "section": "",
    "text": "21世紀に入った時点で，大きく分けて 18 の半導体デバイスが存在する (Ng, 2002)．\n細かいものを含めると 140．\n全てトランジスタの組み合わせからなる．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体産業",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体産業",
    "title": "半導体入門",
    "section": "",
    "text": "世界の半導体出荷額は直近 10 年で２倍になっている"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体とは",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体とは",
    "title": "半導体入門",
    "section": "",
    "text": "物質としての半導体\n\n\n\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．1"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体の物理",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体の物理",
    "title": "半導体入門",
    "section": "",
    "text": "このような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．2 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体発見の歴史",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体発見の歴史",
    "title": "半導体入門",
    "section": "",
    "text": "(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，3 その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える (Sze & Lee, 2012, p. 1)．4\nBraun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#基本用語のまとめ",
    "href": "posts/2024/AI/Semiconductor_slides.html#基本用語のまとめ",
    "title": "半導体入門",
    "section": "",
    "text": "半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した ウエハ (wafer) 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．\n\n\n\nチップの断面構造\n\n\n一つのウエハから多数のチップが作成され，その各単位を ダイ (die) ともいう．\nしかし，普段我々が目にする IC チップ は パッケージ されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．\n32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．5"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#設計",
    "href": "posts/2024/AI/Semiconductor_slides.html#設計",
    "title": "半導体入門",
    "section": "2.1 設計",
    "text": "2.1 設計\nまず回路を設計し，原版（マスター）を作る．これを フォトマスク (photomask) または レティクル (reticle) という．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィ",
    "href": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィ",
    "title": "半導体入門",
    "section": "2.2 フォトリソグラフィ",
    "text": "2.2 フォトリソグラフィ\nこれをウエハに転写するには，フォトリソグラフィ (Photolithography) を用いる．シリコンウエハの形成は，Czochralski 法 (Czochralski, 1918) による．6"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィの歴史",
    "href": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィの歴史",
    "title": "半導体入門",
    "section": "2.3 フォトリソグラフィの歴史",
    "text": "2.3 フォトリソグラフィの歴史\nリソグラフィ自体は 1798 年からあり，Niépce が 歴青 が感光剤の役割を果たすことを発見し，カメラの発明と同時に発見された．\n\n\n\ncamera obscura"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#フォトリトグラフィの産業応用",
    "href": "posts/2024/AI/Semiconductor_slides.html#フォトリトグラフィの産業応用",
    "title": "半導体入門",
    "section": "2.4 フォトリトグラフィの産業応用",
    "text": "2.4 フォトリトグラフィの産業応用\nエッチングに耐性のある感光剤を使えば，半導体デバイスの製造に応用できると気づいたのは (Andrus, 1957) である．この技術は半導体製造コストの 35 %を占めており，半導体市場の急成長はほとんどこの技術の進歩と両輪であると言う者も多い．7\nシリコン表面に酸化被膜を形成することで不純物原子の移動を阻止できることは (Frosch & Derick, 1957) が発見した．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#最初の-ic",
    "href": "posts/2024/AI/Semiconductor_slides.html#最初の-ic",
    "title": "半導体入門",
    "section": "2.5 最初の IC",
    "text": "2.5 最初の IC\n以上の技術を用いて，最初の IC は Texas Instruments の Jack Kilby によって作られた．\n\n\n\nMiniaturized electronic circuits. U.S. Patent 3,138,743A (Kilby, 1959)\n\n\nJack Kilby はその後 2000 年にノーベル物理学賞を受賞する．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#最初の-ic-1",
    "href": "posts/2024/AI/Semiconductor_slides.html#最初の-ic-1",
    "title": "半導体入門",
    "section": "2.6 最初の IC",
    "text": "2.6 最初の IC\n\n\n\n設計図\n\n\n\n双極トランジスタ が１つ\n抵抗器３つ\nコンデンサ１つ\n全てゲルマニウムからなる．\n回路は導線"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#次の-ic",
    "href": "posts/2024/AI/Semiconductor_slides.html#次の-ic",
    "title": "半導体入門",
    "section": "2.7 次の IC",
    "text": "2.7 次の IC\nFairchild Semiconductor の Robert Noyce によって作られた．\n\n\n\nSemiconductor Device-and-Lead Structure. U.S. Patent 2,981,877A (Noyce, 1959)\n\n\nRobert Noyce はその後 Gordon Moore と Intel も創業し，the Mayor of Silicon Valley と呼ばれる．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#次の-ic-1",
    "href": "posts/2024/AI/Semiconductor_slides.html#次の-ic-1",
    "title": "半導体入門",
    "section": "2.8 次の IC",
    "text": "2.8 次の IC\n\n\n\n設計図\n\n\n\nリトグラフによりアルミニウムの配線を形成\nPlanar Process (Hoerni, 1960) （現在主流の製法）で製造\n１つのシリコン基盤上に作った\n→ monolithic で大量生産が可能\n\n1961 年から 1965 年は，NASA の Appolo 計画からの特需もあり，半導体産業は大きく成長した．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#設計-design",
    "href": "posts/2024/AI/Semiconductor_slides.html#設計-design",
    "title": "半導体入門",
    "section": "3.1 設計 (design)",
    "text": "3.1 設計 (design)\n\n\n\n世界初の１チップ CPU の設計図 from (Hoff et al., 1996, p. 11)"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor2.html",
    "href": "posts/2024/AI/Semiconductor2.html",
    "title": "半導体の微細化技術",
    "section": "",
    "text": "(内山貴之, 2015) より．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor2.html#euv-リトグラフィ",
    "href": "posts/2024/AI/Semiconductor2.html#euv-リトグラフィ",
    "title": "半導体の微細化技術",
    "section": "1 EUV リトグラフィ",
    "text": "1 EUV リトグラフィ\n極端紫外線リソグラフィ (Extreme Ultraviolet Lithography) は波長 13.5 nm で露光する技術である．\nオランダの ASML 社が唯一 EUV 露光装置を製造している．\n\nChina asks Netherlands, with world-leading chipmaking equipment, to not decouple\nここまで短い波長では光を透過するレンズはなく，反射鏡で光学系を構成する必要がある．13.5 nm という波長はミラーの反射率の極大点の１つをとって決定された． \\(0.68^{12}=0.98\\) というように重ねて用いる．\n\n\n\n(内山貴之, 2015)　より．"
  },
  {
    "objectID": "posts/2024/Review/Dai+2019.html",
    "href": "posts/2024/Review/Dai+2019.html",
    "title": "Dai+ (2019) Monte Carlo Fusion",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\nReferences\n\nDai, H., Pollock, M., & Roberts, G. (2019). MONTE CARLO FUSION. Journal of Applied Probability, 56(1), 174–191. http://www.jstor.org/stable/45277571"
  },
  {
    "objectID": "posts/2024/Julia/Julia1PPL.html",
    "href": "posts/2024/Julia/Julia1PPL.html",
    "title": "俺のための Julia 入門１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\nhttps://github.com/cscherrer/Soss.jl"
  },
  {
    "objectID": "posts/2024/Survey/NChopin.html",
    "href": "posts/2024/Survey/NChopin.html",
    "title": "Nicolas Chopin 論文のまとめ",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Survey/NChopin.html#年",
    "href": "posts/2024/Survey/NChopin.html#年",
    "title": "Nicolas Chopin 論文のまとめ",
    "section": "1 2022 年",
    "text": "1 2022 年\n\n1.1 On Resampling Schemes for Particle Filter with Weakly Informative Observations (Chopin et al., 2022)\n連続時間 Feynman-Kac 模型に対して，タイムステップ \\(\\Delta_n\\) で離散化した Feynman-Kac 模型に対して構成した粒子フィルターの，\\(\\Delta_n\\searrow0\\) での極限を調べた論文．\nあるリサンプリング強度関数 \\(\\iota:\\mathbb{R}^{dN}\\to\\mathcal{M}^1([N]^{[N]}\\setminus\\{\\mathrm{id}_{[N]}\\})\\) が存在して， \\[\n\\mathcal{L}^{\\text{jump}}f(x):=\\sum_{a\\in[N]^{[N]}\\setminus\\{1:N\\}}\\biggr(f(x^a)-f(x^{1:N})\\biggl)\\iota(x,a)\n\\] というジャンプに対応する部分が加わった生成子を持つ Feller-Dynkin 過程に分布収束することが示されている．\n詳しくは この稿参照．"
  },
  {
    "objectID": "posts/2024/Survey/NChopin.html#年-1",
    "href": "posts/2024/Survey/NChopin.html#年-1",
    "title": "Nicolas Chopin 論文のまとめ",
    "section": "2 2021 年",
    "text": "2 2021 年\n\n2.1 Sequential Monte Carlo Methods in Bayesian Joint Models for Longitudinal and Time-to-Event Data (Alvares et al., 2021)"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#有界測度の基本概念",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#有界測度の基本概念",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "1 有界測度の基本概念",
    "text": "1 有界測度の基本概念\n\n\n\n\n\n\n定義1 (\\(\\mu\\)-inner regular, Radon, tight, regular)\n\n\n\n\\(X\\) を位相空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界な符号付き Borel 測度とする．2\n\nBorel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-内部正則 であるとは， \\[\n\\lvert\\mu\\rvert(B)=\\sup_{K\\overset{\\textrm{cpt}}{\\subset}B}\\lvert\\mu\\rvert(K)\n\\] を満たすことをいう．すなわち，任意の \\(\\epsilon&gt;0\\) に対して，あるコンパクト部分集合 \\(K\\overset{\\textrm{cpt}}{\\subset}B\\) が存在して， \\[\n\\lvert\\mu\\rvert(B\\setminus K)&lt;\\epsilon\n\\] を満たすことをいう．3\n\\(\\mu\\) が Radon であるとは，任意の Borel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-内部正則であることをいう．4\n\\(\\mu\\) が 緊密 であるとは，全体集合 \\(X\\) が \\(\\mu\\)-内部正則であることをいう．5\n\\(\\mu\\) が 正則 であるとは，任意の \\(\\epsilon&gt;0\\) に対して，ある閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) が存在して，\\(F\\subset B\\) かつ \\[\nB\\setminus F\\in\\mathcal{B}(X),\\quad\\lvert\\mu\\rvert(B\\setminus F)&lt;\\epsilon\n\\] を満たすことをいう．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#riesz-正則性",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#riesz-正則性",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "2 Riesz 正則性",
    "text": "2 Riesz 正則性\n\n\n\n\n\n\n変種\n\n\n\n\\(X\\) を位相空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界な符号付き Borel 測度とする．\n\nBorel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-外部正則 であるとは， \\[\n\\lvert\\mu\\rvert(B)=\\inf_{B\\subset U\\overset{\\mathrm{open}}{\\subset}X}\\lvert\\mu\\rvert(U)\n\\] を満たすことをいう．6\n\\(\\mu\\) が Riesz 正則 であるとは，任意の Borel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-外部正則かつ \\(\\mu\\)-内部正則であることをいう．\n\nこの Riesz 正則という語用法は筆者限りのものである．(Halmos, 1950, p. 224), (Dunford & Schwartz, 1958, p. 137), (Folland, 1984, p. 205), (Lang, 1993, p. 265), (Conway, 2007, p. 380) などでは単にこれを regular と呼ぶ．\\(X\\) が局所コンパクト Hausdorff 空間であるとき，このような語用法の方が一般的である．\nさらに，上述のうち４文献で共通するように，非有界な符号付き測度 \\(\\mu\\in\\mathcal{S}(X)\\) を考える際は，最低限次の条件を課し，これも regular であるための条件に入れる：\n\n\\(\\mu\\) が 局所有界 であるとは，任意の \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) 上で有限値であることをいう．\n\n\n\n\n\n\n\n\n\n命題7 ：内部と外部の正則性\n\n\n\n\\(X\\) を局所コンパクト Hausdorff 空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界 Borel 測度とする．次は同値：\n\n\\(\\mu\\) は Riesz 正則である．\n任意のコンパクト集合は \\(\\mu\\)-外部正則である．\n任意の有界な開集合は \\(\\mu\\)-内部正則である．\n\nまた，\\(X\\) 上の任意の Baire 測度は Riesz 正則である．\n\n\n\n\n\n\n\n\n定理8 ：Riesz 正則測度の延長 (Alexandroff, 1940, p. 590)\n\n\n\n\\(X\\) をコンパクト空間，\\(\\mathcal{A}\\subset P(X)\\) を集合体，\\(\\mu:\\mathcal{A}\\to\\mathbb{C}\\) を Riesz 正則で有界な有限加法的関数とする．このとき，\\(\\mu\\) は \\(\\sigma\\)-加法的である．特に，\\(\\sigma(\\mathcal{A})\\) 上へのただ一つの \\(\\sigma\\)-加法的な延長を持ち，引き続き Riesz 正則である．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#footnotes",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#footnotes",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bogachev, 2007, pp. 68–69) 定義7.1.1, 7,1,4 と (Dudley, 2002, p. 224) に倣った．↩︎\n有界でない一般の符号付き測度に関しては，任意の \\(X\\) のコンパクト集合上で有限値であることを Borel 測度たる条件に加えることもある，例えば (Halmos, 1950, p. 223) 52節．↩︎\n(Dudley, 2002, p. 224) では単に regular と呼んでいるが，(Halmos, 1950, p. 224) に従って inner regular と呼ぶことにした．↩︎\n(Halmos, 1950, p. 224) ではこの条件を満たす \\(\\mu\\) を 正則 と呼んだ．↩︎\n(Dudley, 2002, p. 434) によると，最初の tight の定義は (Le Cam, 1957) による uniformly tight の定義であったようである．一点集合 \\(\\{P\\}\\) が一様に緊密であることと \\(P\\) が緊密であることとは同値になる．↩︎\n(Halmos, 1950, p. 224) に倣った．↩︎\n(Halmos, 1950, p. 228) 定理X.52.F, G．↩︎\n(Dunford & Schwartz, 1958, p. 138) 定理III.5.13, 定理III.5.14↩︎"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html",
    "href": "posts/2024/Process/PureJump.html",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#sec-problem",
    "href": "posts/2024/Process/PureJump.html#sec-problem",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "1 問題",
    "text": "1 問題\n\n\n\n\n\n\n次の生成作用素はどのような Markov 過程に対応するか？\n\n\n\n\\(E\\) を距離空間，\\(\\mu:E\\times\\mathcal{B}(E)\\to[0,1]\\) を確率核，\\(\\lambda\\in\\mathcal{L}_b(E)_+\\) を有界可測関数とする． \\[\nAf(x):=\\lambda(x)\\int_E\\biggr(f(y)-f(x)\\biggl)\\mu(x,dy),\\quad f\\in\\mathcal{L}_b(E),\n\\tag{1}\\] は有界作用素 \\(A\\in B(\\mathcal{L}_b(E))\\) を定め，一様連続半群 \\[\n\\{T_t:=e^{tA}\\}_{t\\in\\mathbb{R}_+}\\subset B(\\mathcal{L}_b(E))\n\\] を生成する．1 これに対応する Markov 過程はどのようなものだろうか？"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#構成",
    "href": "posts/2024/Process/PureJump.html#構成",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "2 構成",
    "text": "2 構成\n初期分布を \\(\\nu\\in\\mathcal{P}(E)\\) とする．\n\n2.1 構成１\n結論としては，畳み込み半群 \\(\\{\\mu^{\\otimes k}\\}_{k\\in\\mathbb{N}}\\) に対応する初期分布 \\(\\nu\\) の Markov 過程 \\(\\{Y_k\\}_{k=0}^\\infty\\) と，これと独立な指数確率変数列 \\(\\Delta_k\\overset{\\text{iid}}{\\sim}\\mathop{\\mathrm{Exp}}(1)\\) を用いて， \\[\nX_t:=\\begin{cases}\nY_0&0\\le t&lt;\\frac{\\Delta_0}{\\lambda(Y_0)}\\\\\nY_k&\\sum_{j=0}^{k-1}\\frac{\\Delta_j}{\\lambda(Y_j)}\\le t&lt;\\sum_{j=0}^{k}\\frac{\\Delta_j}{\\lambda(Y_j)}\n\\end{cases}\n\\] と構成した過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\) が，\\(\\{e^{tA}\\}\\) に対応する Markov 過程である．この事実は 節 2.2 で２つ目の構成と同時に証明される．\nなお，\\(\\lambda(x)=0\\) の場合は，ジャンプは起きないもの \\(\\frac{\\Delta}{\\lambda(x)}=\\infty\\) と解する．\nすなわち，関数 \\(\\lambda\\in\\mathcal{L}_b(E)\\) は位置 \\(x\\in E\\) からのジャンプの起こりやすさを表していると思える．具体的には，\\(\\frac{\\Delta_k}{\\lambda(y_k)}\\sim\\mathop{\\mathrm{Exp}}(\\lambda(y_k))\\) に注意すれば，ジャンプの起こりやすさは指数待ち時間のパラメータに対応する．\n\n\n2.2 構成２\n\\(\\lambda=0\\) の場合は零過程であるから， \\[\n\\lambda:=\\sup_{x\\in E}\\lambda(x)&gt;0\n\\] とし， \\[\n\\mu'(x,\\Gamma):=\\left(1-\\frac{\\lambda(x)}{\\lambda}\\right)\\delta_x(\\Gamma)+\\frac{\\lambda(x)}{\\lambda}\\mu(x,\\Gamma)\n\\] と定めると， \\[\nAf(x)=\\lambda\\int_E\\biggr(f(y)-f(x)\\biggl)\\mu'(x,dy)\n\\] とも表せる．\n畳み込み半群 \\(\\{\\mu'^{\\otimes k}\\}_{k\\in\\mathbb{N}}\\) に対応する初期分布 \\(\\nu\\) の Markov 過程 \\(\\{Y_k'\\}_{k=0}^\\infty\\) は \\(\\{Y_k\\}\\) とは分布同等でないが，これと独立な Poisson 過程 \\(\\{V_t\\}_{t\\in\\mathbb{R}_+}\\) に対して \\[\nX'_t:=Y'_{V_t}\\quad t\\in\\mathbb{R}_+\n\\] と構成される過程 \\(\\{X_t'\\}_{t\\in\\mathbb{R}_+}\\) はやはり \\(\\{e^{tA}\\}\\) に対応する Markov 過程である．\n\n\n\n\n\n\n命題2\n\n\n\n\n\\(\\{X'_t\\}\\) は \\(\\{X_t\\}\\) に分布同等である．\n\\(\\{Y'_k\\}\\) は Markov 性 \\[\n\\operatorname{E}[f(Y'_{k+V_t})|\\mathcal{F}_t]=P^kf(X'_t)\n\\] を満たす．ただし，\\(P\\) は \\(\\mu'\\) が定める作用，\\(\\mathcal{F}_t:=\\mathcal{F}_t^V\\lor\\mathcal{F}^{X'}_t\\) とした．\n\\(X'\\) は \\(\\{T_t\\}\\) に対応する Markov 過程である： \\[\n\\operatorname{E}[f(X'_{t+s})|\\mathcal{F}_t]=T_sf(X'_t).\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n2.3 まとめ\n式 1 で考えた生成作用素 \\(A\\) に対応する Markov 過程 \\(\\{X_t\\}\\) は，\\(\\mu\\) が定める Markov 過程 \\(\\{Y_k\\}\\) を，到着率 \\(\\lambda:=\\sup_{x\\in E}\\lambda(x)\\) の Poisson 過程で"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#区分的確定的-markov-過程",
    "href": "posts/2024/Process/PureJump.html#区分的確定的-markov-過程",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "3 区分的確定的 Markov 過程",
    "text": "3 区分的確定的 Markov 過程\nこのような純粋跳躍過程に，決定論的なフローを加えた Markov 過程のクラスを，(Davis, 1984) 以来 区分的確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) と呼ぶ．\n(Davis, 1984) は，PDMP は拡散過程に相補的なクラスであり，確率的モデリングと最適化において重要な役割を演じえることを見事に描き出した．\n\nthe class of “piecewise-deterministic” Markov processes, newly introduced here, provides a general family of models covering virtually all non-diffusion applications. (Davis, 1984)\n\n実際，PDMP を用いた MCMC である Piecewise Deterministic Monte Carlo は，高次元データと大規模データセットに対する効率的なサンプリング法開発の鍵と目されている．\n\n3.1 PDMP の例\nJoris Bierkens ら開発の R パッケージ RZigZag (GitHub / CRAN) を通じて実行してみる．\nRZigZag: Zig-Zag Sampler\ninstall.packages(\"Rcpp\")\ninstall.packages(\"RcppEigen\")\ninstall.packages(\"RZigZag\")\n\n3.1.1 Bouncy Particle Sampler (Bouchard-Côté et al., 2018)\n\n\nCode\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\np &lt;- ggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\nggsave(\"BPS.svg\", p, width=8, height=6)\n\n\n3.1.2 Zig-Zag Sampler (Bierkens et al., 2019)\n\n\nCode\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\np &lt;- ggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\nggsave(\"ZigZag.svg\", p, width=8, height=6)"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#footnotes",
    "href": "posts/2024/Process/PureJump.html#footnotes",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Pazy, 1983, p. 2) 参照．↩︎\n(Ethier & Kurtz, 1986, pp. 163–164)↩︎"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html",
    "href": "posts/2024/Process/ResidualWaitingTime.html",
    "title": "待ち時間の Markov 連鎖",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n空間 \\(E\\) 上の Markov連鎖は，\\(E\\) 上の確率測度の空間 \\(\\mathcal{P}(E)\\) 上に力学系 \\(((P^*)^n\\mu)_{n\\in\\mathbb{N}}\\) を定める．その不動点 \\(P^*\\mu=\\mu P=\\mu\\) が不変確率分布（平衡分布）である．\nこれは，Markov 連鎖の 確率核 \\(P\\) の 左作用 \\(P:\\mathcal{L}_b(E)\\to\\mathcal{L}_b(E)\\) の随伴作用素 \\(P^*:\\mathcal{P}(E)\\to\\mathcal{P}(E)\\) が \\(\\mathcal{P}(E)\\) に作用して得られる力学系である．\nこの力学系は \\(\\mathcal{P}(E)\\) 上に不動点を持つか？持つならば，どのようなノルムについてどのくらいの速さで収束するか？これが Markov 連鎖のエルゴード性の議論なのである．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#待ち時間の-markov-連鎖",
    "href": "posts/2024/Process/ResidualWaitingTime.html#待ち時間の-markov-連鎖",
    "title": "待ち時間の Markov 連鎖",
    "section": "1 待ち時間の Markov 連鎖",
    "text": "1 待ち時間の Markov 連鎖\n\n\n\n\n\n\n\\((p_i)\\sim\\mathcal{P}(\\mathbb{N})\\) の待ち時間を作り出す，次の確率核 \\((p_{ij})\\) を持つ \\(\\mathbb{N}=\\{0,1,\\cdots\\}\\) 上の非線型Markov連鎖 \\(X\\) を考える： \\[p_{(i+1)i)}=1,\\qquad p_{0i}=p_i,\\qquad i\\in\\mathbb{N}.\\] （無限次元の）確率行列 \\(P\\) は Frobenius の同伴行列 の転置の形をしている．\n\n任意の \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) とすると，\\(X\\) は既約で非周期的であり，再帰的である．\n\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) のとき，不変確率測度 \\[\\mu_i=\\frac{\\sum_{j=i}^\\infty p_j}{1+\\sum_{j=1}^\\infty jp_j}\\] をもち，そうでないときは零再帰的である．\n\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n分布 \\((p_i)\\) が偶数の上にしか台を持たないなどの状態では \\(X\\) は周期的になってしまうが， 任意の \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) ならば，任意の状態 \\(i\\in\\mathbb{N}\\) は本質的であり，互いに行き来できるため，既約であり，周期も持たない． 必ず有限時間内に原点に戻ってくるため，再帰的でもある．\n原点 \\(0\\) に初めて帰ってくる時刻を \\(T_0\\) とすると， \\[\\begin{align*}\n\\operatorname{E}_0[T_0]&=\\sum_{j=0}^\\infty(j+1)p_j\\\\\n&=1+\\sum_{j=0}^\\infty jp_j\\\\\n&=1+\\sum_{j=1}^\\infty jp_j.\n\\end{align*}\\] よって，正に再帰的であること \\(\\operatorname{E}_0[T_0]&lt;\\infty\\) は，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) に同値． このとき，離散エルゴード定理より，ただ一つの不変測度 \\((\\mu_n)\\in\\mathcal{P}(\\mathbb{N})\\) を持ち， \\[\\mu_i=\\frac{1}{\\operatorname{E}_i[T_i]},\\qquad i\\in\\mathbb{N},\\] と表せる．これにより \\(i=0\\) の場合はすぐに計算できるが，\\(i&gt;0\\) の場合は少し計算の見通しが良くない．そこで，必要条件 \\[\\mu_i=\\mu_{i+1}+\\mu_0p_i,\\qquad i\\in\\mathbb{N},\\] に注目すると，これを再帰的に適用することで， \\[\\begin{align*}\n\\mu_{i-1}&=\\mu_{i-1}-\\mu_0p_{i-1}\\\\\n&=\\mu_{i-2}-\\mu_0p_{i-2}-\\mu_0p_{i-1}\\\\\n&=\\cdots\\\\\n&=\\mu_0-\\mu_0\\sum_{j=0}^{i-1}p_j\\\\\n&=\\mu_0\\sum_{j=i}^\\infty p_j.\n\\end{align*}\\]\n\n\n\n\n\n1.1 モチベーション\n\n\n\nMarkov 連鎖 \\((X_n)\\) のアニメーション\n\n\n\n\n\n実世界での例 by Claude 3 Opus\n\n\n(Feller, 1967, p. 381) 例 XV.2.(k)，(Kulik, 2018, p. 22) 例 1.3.6 などでも扱われている．\n\n\n1.2 離散エルゴード定理\n\n\n\n\n\n\n離散エルゴード定理1\n\n\n\n\\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\subset L(\\Omega;\\mathcal{X})\\) をMarkov連鎖，\\(\\mathcal{X}\\) を可算集合とする． \\(X\\) が既約で非周期的ならば，次が成り立つ：\n\n任意の本質的な状態 \\(i\\in\\mathcal{X}\\) について， \\[p^n_{ij}\\xrightarrow{n\\to\\infty}\\mu_j=\\frac{1}{\\operatorname{E}_j[\\tau_j]},\\qquad j\\in\\mathcal{X}.\\]\n加えて \\(X\\) が正に再帰的であるならば，\\(\\mu:=\\{\\mu_i\\}_{i\\in\\mathcal{X}}\\) は \\(X\\) のただ一つの不変測度である．\n\\(X\\) が零再帰的である場合は，\\(\\mu_i\\equiv0\\) であり，\\(X\\) の不変測度は存在しない．\n\n\n\n\n\n1.3 離散 Markov 連鎖の概念"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "href": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "title": "待ち時間の Markov 連鎖",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Kulik, 2018, p. 16) 定理1.2.5．↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "",
    "text": "(草野耕一, 2016) の勉強会第1回の補足として，確率論の数学的枠組みを紹介する．\n参考書としては (大塚淳, 2020) もおすすめ．\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#今回の内容",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n\n1.1 本書の概観\n本書は「数理法務」＝「法の数理分析」に関する発展的内容を扱った書籍で，内容は大きく次の1から3の3つからなる：\n\n法の行動分析：法律家がとるべき行動を数理を用いて分析する．\n法の統計分析：事実の推定や因果関係の推定に統計手法を応用する．\n法の財務分析：企業や金融に関わる法事象をファイナンス理論を用いて分析する．\n法の経済分析：法を経済学的な観点から分析する（本書では扱われていない）．\n\n第1回勉強会では第1章「行動分析(1)事実認定」を扱った．事実認定を，Bayes推論の枠組みで捉え直し，法律家として誤謬やバイアスに陥ってしまうことを避けるツールとして，確率論を導入しており，「法の数理分析は役に立つ」ことを端的に実感できる，導入として極めて鮮やかな章になっている．\n\n\n1.2 主観確率とBayes計算\nまず，第1章は，事実認定の文脈で妥当な確率概念は「主観確率」であり，今後「確率」とはこの意味で用いることを注意喚起する内容から始まる．\n主観確率と客観確率の詳細な定義は本書を参照願いたいが，一言で言えば，後者は「人間に不可知な真の値」というものの存在を前提とするのに対し，前者はそれを仮定しない．\n従って，主観確率の考え方は，より多くのものに「確率」を導入することを可能にし，より柔軟な議論が可能であるが，その分数理的な困難も増し，真に発展が進んだと言えるのは，計算機が十分に爛熟した21世紀になってのことであると言える．この統計学分野を Bayes計算 (Bayesian Computation) といい，筆者の研究分野である．\n\nThe development of computing algorithms especially suited for Bayesian analysis in the 1990s together with the exponential growth of computing resources enabled Bayesian nonparametrics to go beyond the simplest problems and made it a universally applicable paradigm for inference. (Ghosal & van der Vaart, 2017)\n\n\n\n1.3 Bayes統計学とは？\n大雑把に言って，客観確率に基づく統計手法を 頻度論的手法 (frequentist methods)，主観確率に対する統計手法を Bayes手法 (Bayesian methods)という．一般に後者は前者を包含する（前者は後者の特別な場合1）と考えられる．しかしこれは「確率の解釈」が違うのみであり，数学としては確率の定義は1つである．「確率の解釈」については，双方の立場の中でもそれぞれ複数の立場が乱立しており，ここでは立ち入らない．と言っても，この注記も教科書的なもので，実用上不便を生じる場面はほとんどないだろう．\n\n不確実性を定量化するのに、ベイズ計算では必ず『確率』を使います。一般の人から見たら、統計で確率を使うのは当たり前と思うでしょうが、じつは他の統計手法ではそうでもなく、さまざまな解釈が生まれてしまう。定量化にはすべて統一的に確率を使うベイズ計算は、非常にシンプルなので、最終的にすべての統計はベイズに行き着くしかないと思っています。 鎌谷研吾\n\n\n\n1.4 Bayes確率の基礎付けの試み……！？\n法律家による事実認定の文脈においても，「真実はいつも1つだからそれを推定したい」と考えても，「不確実な中でも，判断を誤らないようにしたい」と考えても，どちらから議論しても良いことは納得いただけるだろう．ただ，一般の人の素朴な「確率」の理解は，Bayes流のものに近いと言われている．2\nそのこともあり，本書で「主観確率の考え方を採用する」というのは，「確率の解釈の議論はここではしない」「主観的な確信度合いの意味で，現実から多少の乖離を許す」という程度の意味であろう．\nしかし，本書の「主観確率」の議論は中途半端な取り扱いでは終わらず，興味深いことに，One More Step 1-1 (pp.9-10) にて，数理哲学者Donald A. Gilliesによる主観確率の測定による基礎付けの議論が紹介されていた．筆者は初耳の議論であり，己の議論の正統性・基礎付けに細心の注意を施す法律家の心が現れていると筆者は見た．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#sec-2",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#sec-2",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "2 【深掘り】確率の公理",
    "text": "2 【深掘り】確率の公理\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n2.1 「確率の公理」がなぜ重要なのか？\n本書1.1節では確率の性質が列挙されている．1.2節以降では，これらの性質が「確率の定義」として引用されるが，いまいちどれを指して「定義」と呼んでいるのか定かでない．\n数学的な議論に慣れたあとはそれでも良いかも知れないが，法学も初学の間は逐一根拠条文に戻ることが大切であるように，数学もはっきりと定義を列挙し，「それのみを根拠とすること」を徹底することが大事である．\nなお，数学では何を定義として採用するかに任意性がある場合が多いが，唯一やってはいけないことは「定義が曖昧な状態で進むこと」である．そこで，せっかくであるから，現代数学が定義する最も筋の良い定義を採用して，本書の内容を俯瞰することにする．\n\n人は，確率論のもった政治的，社会的意義を忘れてはならない．理知を一切の尺度として「代数学の炉火によって倫理学及び政治学を照さん」(Condorcet) という時代精神，神の啓示に代らんとする確率論，それはフランス革命の思想的基礎に連関することを見失ってはならないのである．(北川敏男, 1949)\n\n現代数学において，確率を特徴付けるものは「代数的性質」であり，それは次の3つのみに集約される．3\n\n\n2.2 確率の公理\n\n\n\n\n\n\n定義（確率） (Kolmogorov, 1931)\n\n\n\n集合 \\(\\Omega\\) 上の確率とは，次の3条件を満たす関数 \\(P:\\mathcal{P}(\\Omega)\\to\\mathbb{R}\\) である：4\n   [P1] \\(P(\\Omega)=1\\)．\n   [P2] \\(A\\cap B=\\emptyset\\) ならば， \\[P(A\\sqcup B)=P(A)+P(B).\\]\n   [P3] 任意の事象 \\(A\\subset\\Omega\\) について， \\[0\\le P(A).\\]5\nただし，\n\n\\(P\\) の定義域 \\(\\mathcal{P}(\\Omega)\\) は「\\(\\Omega\\) の部分集合全体の集合」のことである．これを \\(\\Omega\\) の冪集合という．\n\\(A\\sqcup B\\) とは， \\(A\\cap B=\\emptyset\\) が成り立つときの \\(A,B\\) の合併 \\(A\\cup B\\) を，\\(A\\cap B=\\emptyset\\) を強調して書き分ける記法とする．\n\n\n\nこの公理から，我々が日常的な感覚から「確率に成り立っていて欲しい性質」が全て導ける，ということが現代数学の重要な発見である．性質を見ていく前に，「定義」として，主要な概念に親しみやすい名前を付ける．そのすべての過程において，上の[P1], [P2], [P3]以外を用いていないことを確認することは，数学入門の際には非常に大事な営みである．6\n\n\n\n\n\n\n確率論に関連する用語\n\n\n\n全体集合 \\(\\Omega\\) は所与のものとする．7\n\n事象 とは，部分集合 \\(A\\subset\\Omega\\) のことをいう．\n事象 \\(A\\subset\\Omega\\) の補集合\\[A^\\complement=\\Omega\\setminus A=\\overline{A}:=\\left\\{\\omega\\in\\Omega\\mid \\omega\\notin A\\right\\}\\]を \\(A\\) の余事象という．左から順に，数学で一般によく使われる記号である．8\n2つの事象 \\(A,B\\subset\\Omega\\) が 排反 であるとは，集合として共通部分を持たないことをいう： \\(A\\cap B=\\emptyset\\)．\n\n\n\nこの3性質から，本書第1.1節にいう「確率の推論法則」が全て導出できる．\n\n\n2.3 式(1.1) p.4の証明\n\n\n\n\n\n\n式(1.1) p.4\n\n\n\n任意の事象 \\(A\\subset\\Omega\\) について，\\[0\\le P(A)\\le 1.\\]9\n\n\n\n\n\n\n\n\n式(1.2) p.5\n\n\n\n任意の事象 \\(A\\subset\\Omega\\) について， \\[\nP(A)+P(A^\\complement)=1.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(0\\le P(A)\\) は[P3]に他ならない．式(1.2)も[P2]から従う．\\(A\\subset\\Omega\\) の補集合を \\(A^\\complement:=\\Omega\\setminus A\\) で表すと， \\(P(A^\\complement)\\ge0\\) も成り立つから， \\[\n\\begin{align*}\nP(A)&\\le P(A)+P(A^\\complement)\\\\\n&=P(A\\sqcup A^\\complement)\\\\\n&=P(\\Omega)=1.\n\\end{align*}\n\\]\n\n\n\n\n2.4 式(1.3) p.5の証明\n\n\n\n\n\n\n式(1.3) p.5\n\n\n\n任意の \\(n\\ge1\\) について， \\(n\\) 個の事象 \\(A_1,\\cdots,A_n\\subset\\Omega\\) が互いに排反であるとき， \\[\n\\begin{align*}\n&P(A_1)+P(A_2)+\\cdots+P(A_n)\\\\\n&\\qquad\\qquad=P(A_1\\sqcup A_2\\sqcup\\cdots\\sqcup A_n).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(n\\) に関する数学的帰納法による．\n\n\n\n\n2.5 式(1.4) p.6の証明\n\n\n\n\n\n\n式(1.4) p.6\n\n\n\n任意の事象 \\(A,B\\subset\\Omega\\) について， \\[\nP(A)+P(B)=P(A\\cup B)+P(A\\cap B).\n\\]\n\n\n[P2] の条件は，\\(A_1,A_2\\) が排反である場合に限定しており，その制限が邪魔であった．ここで一般の加法公式を得ることになる．\n\n\n\n\n\n\n証明\n\n\n\n\\(C:=A\\cap B\\) とおくと，3つの集合 \\(A\\setminus B,C,B\\setminus A\\) が互いに排反であることから，\n\\[\n\\begin{align*}\n&\\quad P(A)+P(B)\\\\\n&=\\biggr(P(A\\setminus B)+P(C)\\biggl)+\\biggr(P(C)+P(B\\setminus A)\\biggl)\\\\\n&=\\biggr(P(A\\setminus B)+P(C)+P(B\\setminus A)\\biggl)+P(C)\\\\\n&=P(A\\cup B)+P(A\\cap B).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6 条件付き確率の定義\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\(A,B\\subset\\Omega\\) を事象とする． 事象 \\(A\\) が起こった場合の，事象 \\(B\\) の条件付き確率とは， \\[\nP(B|A):=\\begin{cases}\\frac{P(A\\cap B)}{P(A)}&P(A)\\ne0\\;\\text{のとき}\\\\0&P(A)=0\\;\\text{のとき}\\end{cases}\n\\] という値を指す．10\n\n\n\n\n2.7 式(1.7) p.7の証明\n\n\n\n\n\n\n式(1.7) p.7\n\n\n\n\\(A,B\\subset\\Omega\\) を事象，\\(P(A)&gt;0\\) とする． \\[\nP(A|B)+P(A^\\complement|B)=1.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\[\n\\begin{align*}\n&\\quad P(A|B)+P(A^\\complement|B)\\\\\n&=\\frac{P(A\\cap B)}{P(B)}+\\frac{P(A^\\complement\\cap B)}{P(B)}\\\\\n&\\overset{\\text{[P2]}}{=}\\frac{P((A\\cap B)\\sqcup (A^\\complement\\cap B))}{P(B)}\\\\\n&=\\frac{P(B)}{P(B)}=1.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#sec-independent",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#sec-independent",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "3 【重要概念】統計的独立性",
    "text": "3 【重要概念】統計的独立性\n\n3.1 定義\n\n\n\n\n\n\n定義（独立性）\n\n\n\n2つの事象 \\(A,B\\subset\\Omega\\) が独立であるとは，次を満たすことをいう： \\[\nP(A\\cap B)=P(A)P(B).\n\\] このとき， \\(A\\perp\\!\\!\\!\\perp B\\) と表す．\n\n\nこの式は本書p.7 (1.8)式に一致している．これを「積の公式」として導出しているが，これは実は独立性の定義とすべき性質である．その意味するところを次節で解説する．\n\n\n3.2 条件付き確率による特徴付け\n節 2 で「数学では何を定義として採用するかに任意性がある場合が多い」と言った．今回の「独立性」概念も，2つの同値な定義がある．しかし，「唯一やってはいけないことは定義が曖昧な状態で進むことである」とも言った．従って，どちらか片方を定義とし，「定義ともう一つの条件が同値である」という命題が生まれることになる．\nこの形の命題のことを（数学概念の）特徴付け という．このことを解説するWikipediaページもある．\n\n\n\n\n\n\n命題（独立性の特徴付け）\n\n\n\n2つの事象 \\(A,B\\subset\\Omega\\) について，次の2条件は同値：\n\n\\(A,B\\) は独立である：\\(A\\perp\\!\\!\\!\\perp B\\)．\n\\(P(B|A)=P(B)\\)．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(P[A]=0\\) の場合，任意の \\(B\\) について(1),(2)はいずれも常に成り立つ． あとは，\\(P[A]\\ne0\\)の場合を考える． すると，条件付き確率の定義 \\[P[A\\cap B]=P[A]P[B|A]\\] を考えれば，この右辺が\\(P[A]P[B]\\)に等しいことと，\\(P[B|A]=P[B]\\)であることとは同値．\n\n\n本書では2.の性質の方を定義としているが， \\(P(B|A)\\) という量は， \\(P(A)=0\\) の場合に定義に任意性が残る．従って，1.の方が定義として明瞭ということになる．\nさらに重要なことには，1.の方が一般個数の事象 \\(A_1,\\cdots,A_n\\) の場合に「独立性」の概念の拡張が可能であり，より本質的な定義だと思われる，ということが確率論の示唆である．実は，無限個の事象が独立であることも同様に定義する．\n\n\n\n\n\n\n定義（独立性）\n\n\n\n集合族 \\(\\{A_\\lambda\\}_{\\lambda\\in\\Lambda}\\subset\\mathcal{F}\\) が独立であるとは，任意の \\(n\\in\\mathbb{N}\\) 個の相異なる元 \\(A_{\\lambda_1},\\cdots,A_{\\lambda_n}\\) に対して， \\[P[A_{\\lambda_1}\\cap\\cdots\\cap A_{\\lambda_n}]=P[A_{\\lambda_1}]\\cdots P[A_{\\lambda_n}]\\] が成り立つことをいう．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#余談数学について",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#余談数学について",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "4 【余談】数学について",
    "text": "4 【余談】数学について\nここまでを読んだ読者の中で，「集合」「写像」の言葉に，定義が十分に提示されていないと感じたものがあるなら，あなたは極めて筋が良い．実は，これらの裏に全て厳密な定義があるのが数学であるが，今回は確率論に集中するために省いた．\n実際，確率論をKolmogorovによる確率の公理的な定義 (Kolmogorov, 1931) から始まる数学分野だとするならば，これはまだ100年の歴史もない，数学分野にしては極めて珍しい若い分野である．\n確率論の確率が遅れた理由は，「確率」の概念がつかみどころのない日常に根ざした概念であり，抽象化が本質的に難しいこともあるだろうが，第一に「集合」「写像」といった概念が十分に数学者の間で理解が深まるのを待つ必要があったということがある．\n現代の確率論では，「確率は測度の特別なものである」という態度をとっていることは本文中でも述べたが，この「測度」という概念の成立が，そもそもLebesgueによる積分論が確立される20世紀に入るのを待つ必要があった．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#footnotes",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(McElreath, 2020) 1.3節．頻度論はさらに「仮想的な反復」(imaginary resampling of data)を想定する，という性質を除けば，不確実性が観測からくるもののみである特別な場合が頻度論であると捉えられる．↩︎\n(McElreath, 2020) 1.3節，(Rubin, 1984)．↩︎\nWikipediaページ確率の公理も参照．↩︎\n関数とは，入力と出力の集合 \\(X,Y\\) の間に定まる対応であって，任意の入力 \\(x\\in X\\) に対してただ一つの出力 \\(y\\in Y\\) が対応するもののことをいう．この対応を \\(f(x)=y\\) と表す．↩︎\n後ろの2条件[P2], [P3] のみを満たす関数 \\(P\\) は「測度」という．そのため，確率は測度でもある．数学用語では「確率分布」は「確率測度」ともいう（例えばこのwikipediaページ）．↩︎\n[P1] などの P は Probability のつもりである．↩︎\n集合にも公理があり，現代数学はZFC公理系の下で展開される．が，ここでは深入りしない．↩︎\n\\(\\lnot A\\) という記法について，\\(\\lnot\\) は論理記号であるから，集合 \\(A\\) に用いることは好ましくない．↩︎\n確率は必ず\\(0\\)から\\(1\\)の値を取る，ということを主張している命題である．初学者はこれが「示すべき内容」として提示されていることに戸惑いを覚えるだろうが，現代数学では「これが示せるような必要最小限の定義が見つかった」ことに価値を見出す．↩︎\nここでは \\(P(A)=0\\) の場合は \\(0\\) としたが，実際はどんな値でも良い．↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "",
    "text": "Bayesian probability theory is sometimes called ‘common sense, ampliﬁed’. (MacKay, 2003, p. 293)\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html#今回の内容",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n第一審の裁判において，事実認定が中心的な問題である．この際に起き得る誤謬を，ベイズの方法を用いてどう回避できるか？という例が３つ挙げられている．\n\n1.1 主観確率の基本的な計算方法と捜査官の誤謬\n\n\n\n\n\n\n問題1-1\n\n\n\n殺人事件の加害者はAとBのどちらか？利用可能な情報は次のみ：被疑者のA, Bは同居しており，そのうちどちらかが加害者であることはわかっているものとする．\n\n台所に左利きの包丁があった．Aが左利きである確率はいくらか？ただし，人間が左利きである確率は1割とする．\nAは左利きであること，被害者の外傷の部位や凶器の形状から加害者も左利きであったことは確定的であるとする．Aに逮捕令状を出しても良いだろうか？ただし，利き手の情報を除けば，AかBかは完全に五分五分であるとする．なお，共犯はなく，どちらか一方の単独犯であることも確実であるとする．\n\n\n\n\n\n\n\n\n\n結論\n\n\n\n\nAもBも左利きである可能性があるため，5割より少し大きい．\nAもBも左利きである可能性があるため，9割強である．95%を一つの基準にするなら，逮捕令状は出すべきではない．\n\n\n\nこの問題のポイントは「条件付き確率」への理解である．\n\n\n\n\n\n\n論証\n\n\n\n\n事象を \\[\nA:=\\left\\{\\text{Aは左利きである}\\right\\}\n\\] \\[\nB:=\\left\\{\\text{Bは左利きである}\\right\\}\n\\] と定めると， \\[\n\\begin{align*}\n&\\quad\\;\\operatorname{P}[A\\cup B]\\\\\n&=\\operatorname{P}[A]+\\operatorname{P}[B]-\\operatorname{P}[A\\cap B]\\\\\n&=\\frac{19}{100}\n\\end{align*}\n\\] これより，条件付き確率の定義から \\[\n\\begin{align*}\n\\operatorname{P}[A|A\\cup B]&=\\frac{\\operatorname{P}[A]}{\\operatorname{P}[A\\cup B]}\\\\\n&=\\frac{10}{19}\\approx52.6\\%\n\\end{align*}\n\\]\n\n\n\n今回の肝は，事象 \\(A,B,C\\) を互いに独立に設定したために，各積事象 \\(A\\cap B,B\\cap C,C\\cap A\\) が悉く計算可能なものとして得られた，という点である．これを計算しておけば，欲しい値がこれらの言葉で得られているから，答えまで一本道で辿り着ける，という訳である．\n\n\n\n\n\n\nMonty Hall問題\n\n\n\nモンティ・ホール問題 も条件付けを正しく行えない（「何が分母か」を分別せず，違う次元の話を混同する）ことによって起こるパラドックスの有名な例である．\n\n\n\n\n\n\n\n\nまとめ\n\n\n\n「捜査官の誤謬」は「条件付け」を正しく行わないことにより起こる誤謬である．これを回避するには，独立な事象 \\(A,B\\) を抽出し，これらの確率を計算し，最終的に求めたい確率が何かを正しく特定することが重要である．\n\n\n\n\n1.2 ベイズの公式と検察官の誤謬\n\n\n\n\n\n\n問題1-2\n\n\n\nドーピング検査の結果から，ある日本選手Iが金メダルを剥奪された．弁護人としては，どのような弁護の筋があるか？\n\n本ドーピング検査において，禁止薬物をを用いていない人に対して陽性の結果が出る（偽陽性）確率は1%で，逆の偽陰性も1%である．\n日本選手で，禁止薬物を用いている割合は0.1%とする．当該日本選手Iもこの割合に従うものとする（とりわけ禁止薬物を使っていそうな理由・いそうでない理由はないものとする）．\n\n\n\n各事象を \\[\nA:=\\left\\{\\text{ I は薬物を使用していた}\\right\\}\n\\] \\[\nE:=\\left\\{\\text{ I に陽性反応が出た}\\right\\}\n\\] と設定する．今回は \\(A,E\\) は独立ではないことに注意．例えば，後からわかることだが， \\[\\operatorname{P}[A\\cap E]\\ne\\operatorname{P}[A]\\operatorname{P}[E]\\] である．ここで，条件付き確率の計算の問題に分け入ることになる．今回与えられている条件はそれぞれ，\n\\[\n\\begin{align*}\n\\text{1.}&\\qquad\\operatorname{P}[\\overline{E}|A]=\\frac{1}{100},\\\\\n&\\qquad\\operatorname{P}[E|\\overline{A}]=\\frac{1}{100}\\\\\n\\text{2.}&\\qquad\\operatorname{P}[A]=\\frac{1}{1000}\n\\end{align*}\n\\]\nと表現できており，知りたい値は，今現在Iが本当に薬を使っていたという確率 \\(\\operatorname{P}[A|E]\\) である．\n実は，これは全く大きな値ではない！これは，そもそも薬物を使っている人が少なく，健常な人の方が大多数であるために，検査で陽性が出たからといってそれが本当に薬物を使っている人から出た「真の陽性」である確率が極めて小さくなってしまうという普遍的な現象である．\n\n\n\n\n\n\n証明\n\n\n\n求めたい量 \\(\\operatorname{P}[A|E]\\) は \\[\n\\begin{align*}\n&\\quad\\;\\operatorname{P}[A|E]\\\\\n&=\\frac{\\operatorname{P}[A\\cap E]}{\\operatorname{P}[E]}\\\\\n&\\overset{\\text{(2)}}{=}\\frac{\\operatorname{P}[E|A]\\operatorname{P}[A]}{\\operatorname{P}[E]}\\\\\n&\\overset{\\text{(3)}}{=}\\frac{\\operatorname{P}[E|A]\\operatorname{P}[A]}{\\operatorname{P}[E|A]\\operatorname{P}[A]+\\operatorname{P}[E|\\overline{A}]\\operatorname{P}[\\overline{A}]}\n\\end{align*}\n\\tag{1}\\] と式変形できる．この右辺は，全て既知の値で表現できている．\nなお，途中の式変形については，条件付き確率の定義から\n\\[\n\\operatorname{P}[A\\cap E]=\\operatorname{P}[E|A]\\operatorname{P}[A]\n\\tag{2}\\]\nと，全確率の法則\n\\[\n\\begin{align*}\n&\\quad\\;\\operatorname{P}[E|A]\\operatorname{P}[A]+\\operatorname{P}[E|\\overline{A}]\\operatorname{P}[\\overline{A}]\\\\\n&=\\frac{\\operatorname{P}[E\\cap A]}{\\operatorname{P}[A]}\\operatorname{P}[A]\\\\\n&\\qquad\\qquad+\\frac{\\operatorname{P}[E\\cap\\overline{A}]}{\\operatorname{P}[\\overline{A}]}\\operatorname{P}[\\overline{A}]\\\\\n&=\\operatorname{P}[E\\cap A]+\\operatorname{P}[E\\cap\\overline{A}]\\\\\n&=\\operatorname{P}[E]\n\\end{align*}\n\\tag{3}\\] とを用いた．\n\n\n実際に計算してみると， \\[\n\\operatorname{P}[A|E]=\\frac{99}{1098}\\approx9.0\\%.\n\\] 選手Iは実際は薬を使っていない可能性の方がよっぽど高いのである．\n\n\n\n\n\n\nBase rate fallacy\n\n\n\nこの検察官の誤謬は，特に不良品検出の文脈では深刻なバイアスになり，英語では基準確率の誤謬ともいう．1\n\\(n\\) 人の母集団に，ある病気の検査を行うとしよう． \\[\\begin{cases}A_i:=\\left\\{i\\text{は有病}\\right\\},\\\\B_i:=\\left\\{i\\text{は陽性}\\right\\}.\\end{cases}\\quad i\\in[n].\\] としたとき，\n\n\\(\\alpha:=P[B_i|A_i^\\complement]\\) を偽陽率・危険度という．検定一般に言う，第一種の過誤率である．\n患者が有病であるときに陽性が出る確率 \\(1-\\alpha\\) の値を 感度(sensiticity)という．\n\\(\\beta:=P[B_i^\\complement|A_i]\\) を偽陰率という．検定一般に言う，第二種の過誤率である．\n患者が無病であるときに陰性が出る確率 \\(1-\\beta\\) の値を特異度(specificity)という．2 検定一般に言う検出力(power)である．\n\n統計的検定では第一種の過誤率を重く見て，これを制限した上での第二種の過誤率の低さを指標とする．このために「検出力」が重要．一方で失病検査の際は第一種の過誤率が大変重要であり，これに「感度」という名前がついている．\nこのような一般的な設定の下で，陽性の結果を見て，患者の有病率を \\(1-\\beta\\) だと結論づけてしまう誤謬を基準確率の誤謬という．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html#ベイズ統計学",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html#ベイズ統計学",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "2 ベイズ統計学",
    "text": "2 ベイズ統計学\n\n2.1 ベイズの公式\n節 1.2 で使った式変形 式 1 の最左辺と最右辺のみに注目して公式化すると，次のようになる： \nこれを（分割 \\(\\Omega=A\\sqcup\\overline{A}\\) に関する）Bayesの公式という．\nこれは独立性の特徴付け \\[\n\\operatorname{P}[A|E]=\\operatorname{P}[A],\\quad\\operatorname{P}[E|A]=\\operatorname{P}[E]\n\\] の一般化になっているともみれる．\n\n\n2.2 ベイズ統計学\n事前に確率 \\(\\operatorname{P}[A]\\) を想定しておく．そして，\\(A\\) に関連する観測の結果 \\(\\operatorname{P}[E|A]\\) を見てから，ベイズの公式を通じて \\(\\operatorname{P}[A|E]\\) を計算し，事象 \\(A\\) に関する理解を深める営みが，ベイズ統計学の雛形である．\nこの \\(\\operatorname{P}[A]\\) を事前確率，\\(\\operatorname{P}[A|E]\\) を事後確率という．\n\n事象 \\(A\\) として何を選んでも良い．\n事前情報 \\(\\operatorname{P}[A]\\) を推論に取り込む余地がある．\n\\(A\\) と \\(E\\) に対して多様な関係を想定できる．\n\\(\\operatorname{P}[A|E]\\) は一般にグラフの形で与えられるので，（他の統計手法と比べて）情報量が多い．\nベイズの公式が全てであり，何をやっているかがわかりやすい．\n\n点がよくベイズ統計学の美点として挙げられる．\n\n\n2.3 ベイズ計算\n前節で解説した通り，ベイズ統計学はベイズの公式が全てであり，原理的には極めて明快である．では，何が難しいかというと，一般的な形のベイズの公式 \\[\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{\\int_\\Theta p(x|\\theta)p(\\theta)\\,d\\theta}\n\\] は，最も単純な場合でも，計算が不可能であるという点である．積分は現代では高校で習う数学の範囲であるが，実際に計算できる積分など応用の現場では都合よく出てこないのである．\n従って，ベイズ統計学の研究において，計算手法の研究が極めて重要な位置を占める．この分野をベイズ計算というのである．詳しくは ベイズ計算とは何か の記事を参照してほしい．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html#footnotes",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Arias-Castro, 2022) と (Agresti, 2012, pp. 第2.1.3節 p.39), (Smith, 2010, pp. 22–23) も参照．↩︎\n(Yerushalmy, 1947) ↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#数学的骨格",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#数学的骨格",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "1 数学的骨格",
    "text": "1 数学的骨格\n詳しくは 本サイトの数学記法一覧 を参照．\n\n1.1 確率空間\n\n\n\n\n\n\n定義（確率空間）\n\n\n\n\n任意の集合 \\(\\Omega\\) に対して，確率の公理 [P1], [P2], [P3] を満たす関数 \\[\n\\operatorname{P}:\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\to[0,1]\n\\] が定義されているとき，組 \\((\\Omega,\\operatorname{P})\\) を 確率空間 (probability space) という．\n確率空間上の実数値の関数 \\[\nX:\\Omega\\to\\mathbb{R}\n\\] を 確率変数 (random variable) という．1\n確率変数 \\(X:\\Omega\\to\\mathbb{R}\\) に対して， \\[\n\\operatorname{P}^X[A]:=\\operatorname{P}[X\\in A]\n\\] で定まる実数 \\(\\mathbb{R}\\) 上の分布を，\\(X\\) の 確率分布 (probability distribution) という．\n\n\n\n集合 \\(\\Omega\\) というのを自由にとって良いというのが，確率論の懐の広さであり，統計学で出会う多種多様な問題に対応できる所以である．\nサイコロの出目を考える場合は \\(\\Omega=\\{1,2,3,4,5,6\\}\\) ととってその上の確率空間に関する理論を借りれば良い．殆どの場合は \\(\\Omega=\\mathbb{R}\\) と取ることになる．\n\n\n\n\n\n\n注（厳密な定義）\n\n\n\n\n\n本当は確率空間は３組 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) である．新たに加わった \\(\\mathcal{F}\\) とは何かというと，\\(\\operatorname{P}\\) の定義域であり，上の定義で \\[\n\\operatorname{P}:\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\to[0,1]\n\\] としていたところを \\[\n\\operatorname{P}:\\mathcal{F}\\to[0,1]\n\\] と，定義域を制限するのが厳密な定義である．\nしたがって，標本空間 \\(\\Omega\\) の部分集合はなんでも事象と呼んでいいかというと，数学的にはそうではなく，事象の全体 \\(\\mathcal{F}\\) は一定の（代数的な）規則を満たす必要がある（完全加法性 という）．\nこれは 測度論 (measure theory) と呼ばれる数学分野から得られる知見である．\nなぜ制限しなければいけないのか？は，そうしなければ数学的な矛盾が起こるからなのであるが，普通に統計学の目的で確率論を用いる範囲でこの矛盾に遭遇することは滅多にないので，ここでは触れない．\n\n\n\n\n\n1.2 確率変数の概念\n確率変数の概念は，確率論において最も重要なものである．David Mumford というフィールズ賞も受賞した世界的な数学者（専門が確率論というわけではない）も，次のように述べている：\n\nThe basic object of study in probability is the random variable and I will argue that it should be treated as a basic construct, like spaces, groups and functions, and it is artificial and unnatural to define it in terms of measure theory. (Mumford, 2000, p. 108)\n\n確率変数が重要な理由は，それは確率分布と違うということを徹底的に教えてくれることにある．換言すれば，日常的な感覚で確率を議論するとなかなかモヤモヤが解消せずに解った気になれない理由は，確率変数と確率分布という本来別々の存在を人間は混同してしまいがちだからからである，と教えてくれるのが現代の確率論なのである．\n中高の数学での「場合の数と確率」は特に混同の傾向が強い．三角関数がどのように社会の役に立つか不思議に思ったことがあるならば，あそこで習った初頭的な議論がどう統計学に応用されてどうして AI が生まれるに至ったのかたいへん不思議であろう．中高での離散的な議論を連続な場合にも通用するようにするためには，確率分布と確率変数を峻別することが肝要 である．\n確率変数は，「変数」の概念の確率化 である．変数は，高校数学などでも \\(x,y,z,\\cdots\\) と小文字で表したが，確率変数は \\(X,Y,Z,\\cdots\\) と大文字で表す． \\(\\Omega=\\{*\\}\\) と標本空間を一点集合と取った場合，確率変数は通常の決定論的な変数と同義になる．\n\n\n\n\n\n\n発展（確率過程）\n\n\n\n\n\n一方で，高校数学などでも扱う「関数」の概念の確率化は　確率過程 という．確率過程は名前は仰々しいかも知れないが，定義自体はなんてことはない，確率変数の集合のこと である．\n例えば，日付 \\(n\\) の株価 \\(X_n\\) の列 \\(X_1,X_2,X_3,\\cdots\\) は確率過程である．決定論的な関数 \\(n\\mapsto x_n\\) の確率化である．\n\n\n\n\n\n1.3 分布の押し出し\nでは実際に，確率分布と確率変数がどう違うかを説明する．\n確率分布は確率空間に宿るもので，確率変数は確率空間を繋ぐものである．\nサイコロを２回振った出目の全体を標本空間とするならば， \\[\n\\Omega:=[6]\\times[6]=\\left\\{(1,1),(1,2),\\cdots,(1.6),(2,1),\\cdots\\right\\}\n\\] という集合の上に，一様分布\n\n\n\n\n\n\n\n\n\nを定義して得る確率空間を考えるのが一つの良い方法であろう．\nこれが確率分布である．\n一方で，確率変数は，標本空間上の関数の全てである．例えば，出た目の和は確率変数である．\n最も重要なことは，確率変数は分布を押し出す ということである．\n実際，出た目の和は，確率分布を押し出して，次のような確率分布を定める："
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#今回の内容",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "2 今回の内容",
    "text": "2 今回の内容\n数学的骨格を理解した状態で，今回範囲の内容 (草野耕一, 2016, pp. 73–96) を整理する．\n\n2.1 母集団と標本 (pp.73-75)\n\n2.1.1 母集団\n標本調査が行われるとき，調査対象となる全体集団を 母集団 (population) という．全人口を精査することは困難であるため，ここから無作為に一部分を選ぶことになる．これを 標本調査 (survey sampling) といい，得られたデータを 標本 (sample) という．\nすなわち，データとは確率変数 \\(X_1,\\cdots,X_n\\) であり，これらの積が定める確率変数 \\[\nX=(X_1,\\cdots,X_n):\\Omega\\to\\mathbb{R}^n\n\\] を考え，母集団を \\(\\Omega\\) とし，確率変数 \\(X_1,\\cdots,X_n\\) を標本とするのである．\n\\(X_1,\\cdots,X_n\\) が，母集団となる確率空間 \\((\\Omega,\\operatorname{P})\\) に関する情報をなるべく効率よく伝えてくれるように設計するのが重要である．実際，現代の標本調査では，無作為抽出が基本であり，一昔前では電話番号台帳の下一桁を無作為に選び，電話を掛けるという方法が用いられた．当然この場合，電話を持っていない標本 \\(\\omega\\in\\Omega\\) についての情報は得られないので，その点に関する補正が必要になる，という具合である．\nこのように，「無作為」と言っても具体的にどのように選べば良いか？を考える分野を 標本調査法 (sampling theory) という．2 大統領選挙を通じての標本抽出法の発展の例は 節 3.2 に付した．\n\n\n2.1.2 統計量の例\n標本の関数を 統計量 (statistic) という．\n\n\n\n\n\n\n定義（３つの標本統計量）\n\n\n\n\\(x_1,\\cdots,x_n\\) を標本とする．\n\n次を 標本平均 という： \\[\n\\overline{x}:=\\frac{x_1+\\cdots+x_n}{n}.\n\\]\n次を 標本分散 という： \\[\ns^2:=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2.\n\\]\n標本分散の非負の平方根 \\(s:=\\sqrt{s^2}\\) を 標本標準偏差 という．\n次を 不偏分散 という： \\[\nu^2:=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\overline{x})^2.\n\\]\n\n\n\n\n\n\n\n\n\n命題（分散公式）\n\n\n\n標本分散 \\(s^2\\) と標本平均 \\(\\overline{x}^2\\) の間には次の関係が成り立つ： \\[\ns^2=\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2.\n\\] \\(\\frac{1}{n}\\sum_{i=1}^nx_i^2\\) という量を 標本の２次の絶対積率 という．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\n\\begin{align*}\n    s^2&=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^n(x_i^2-2x_i\\overline{x}+\\overline{x}^2)\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^nx_i^2-2\\overline{x}\\frac{1}{n}\\sum_{i=1}^nx_i+\\overline{x}^2\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2.\n\\end{align*}\n\\]\n\n\n\n\n\n2.1.3 母数とは何か？\n本書 (草野耕一, 2016, p. 75) に\n\n標本に統計量があるように母集団にもその特性を表す数値が備わっているはずであり，そのような数値のことを 母数 という．\n\nとあるが，この文脈では母数ではなく 特性値 という (竹村彰道, 2020)．特性値を母数と設定することが多いが，それはあくまで統計解析者の裁量である．\n母数とは次に示すように，推定対象として解析者が設定する母集団の特性値 である．標本統計量は実際に計算できるが，特性値と母数は未知である．\nよって，ほとんどの場合，推測統計の問題とは母数推定の問題に他ならない．\n\n\n\n\n\n\n定義（統計モデル，母数）\n\n\n\n\n母集団上の確率分布の族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を 統計モデル という．\n統計モデルを添字付ける「番号」 \\(\\theta\\in\\Theta\\) を 母数 という．\n\n\n\n真の分布を \\(\\operatorname{P}\\) としたとき，これを近似すると思われる分布族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を統計解析者が設定するのである．腕の見せ所である．\nあなたが保険数理士だとして，重大事故の発生確率を推定する際， \\[\n\\Omega=\\left\\{0,1,2,\\cdots\\right\\}\n\\] を一定期間内に起こる重大事故件数とすると，これに対する分布族は Poisson 分布族 \\(\\{\\mathrm{Pois}(\\lambda)\\}_{\\lambda&gt;0}\\) を取ると近似精度が良いことが知られている．Poisson 分布の母数 \\(\\lambda&gt;0\\) は 到着率 や 強度 と呼ばれる．\n\n\n\n2.2 統計推測の技法(1) (pp.75-85)\n本書 (草野耕一, 2016, pp. 75–85) の重大な特徴に，確率変数と確率分布を区別していないという問題がある．\n確率分布とは 第１回講義 で導入した，３つの公理を満たす集合関数 \\(\\operatorname{P}:P(\\Omega)\\to[0,1]\\) である．3 このとき，ペア \\((\\Omega,\\operatorname{P})\\) を，確率が定義された集合という意味で 確率空間 という．\n確率変数 とは，確率空間 \\((\\Omega,\\operatorname{P})\\) 上に定義された関数 \\(X:\\Omega\\to\\mathbb{R}\\) のことである．特に，値域が \\(\\mathbb{N}\\subset\\mathbb{R}\\) に限る場合を 離散変数 という．\n\n2.2.1 離散の場合\n確率変数 \\(X\\) の取り得る値が \\(\\mathbb{N}=\\{0,1,2,\\cdots\\}\\) に限る場合が離散の場合である．\n\n\n\n\n\n\n定義（期待値）\n\n\n\n\n非負値の関数 \\(f:\\mathbb{N}\\to[0,1]\\) であって次を満たすものを 確率（質量）関数 という： \\[\nf(n)=\\operatorname{P}[X=n]\n\\]\n確率関数 \\(f:\\mathbb{N}\\to[0,1]\\) に従う確率変数 \\(X:\\Omega\\to\\mathbb{N}\\) に対して，次の量 \\(\\operatorname{E}[X]\\) を 期待値 という： \\[\n\\operatorname{E}[X]:=\\sum_{n=1}^\\infty nf(n)=\\sum_{n=1}^\\infty n\\operatorname{P}[X=n].\n\\]\n次の量 \\(\\mathrm{V}[X]\\) を \\(X\\) の 分散 という： \\[\n\\mathrm{V}[X]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])^2\\biggr].\n\\]\n\n\n\n\n\n2.2.2 連続の場合\n\n\n\n\n\n\n定義（期待値）\n\n\n\n\\((\\Omega,\\operatorname{P})\\) を確率空間，\\(X:\\Omega\\to\\mathbb{R}\\) をその上の確率変数とする．\n\n非負値の関数 \\(F:\\mathbb{R}\\to[0,1]\\) であって次を満たすものを，\\(X\\) の （累積）分布関数 という： \\[\nF(x):=\\operatorname{P}[X\\le x].\n\\]\n非負値の関数 \\(p:\\mathbb{R}\\to\\mathbb{R}_+\\) であって次を満たすものが存在するならば，これを \\(X\\) の （確率）密度関数 という： \\[\n\\operatorname{P}[X\\in A]=\\int_Ap(x)\\,dx.\n\\]\n確率密度 \\(p\\) に従う確率変数 \\(X:\\Omega\\to\\mathbb{R}\\) に対して，次の量 \\(\\operatorname{E}[X]\\) が存在するならば，これを \\(X\\) の 期待値 という： \\[\n\\operatorname{E}[X]:=\\int_\\mathbb{R}xp(x)\\,dx.\n\\]\n次の量 \\(\\mathrm{V}[X]\\) が存在するならば，これを \\(X\\) の 分散 という： \\[\n\\mathrm{V}[X]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])^2\\biggr].\n\\]\n\n\n\n\n\n2.2.3 期待値の性質\n\n\n\n\n\n\n命題（期待値の性質）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n（期待値の線型性）任意の \\(a,b\\in\\mathbb{R}\\) について， \\[\n\\operatorname{E}[aX+bY]=a\\operatorname{E}[X]+b\\operatorname{E}[Y].\n\\]\n（分散公式）次が成り立つ： \\[\n\\mathrm{V}[X]=\\operatorname{E}[X^2]-(\\operatorname{E}[X])^2.\n\\]\n（分散の斉次性）任意の \\(a,b\\in\\mathbb{R}\\) について， \\[\n\\mathrm{V}[aX+b]=a^2\\mathrm{V}[X].\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n積分の線型性から従う．\n\nの期待値の線型性のみから従う．\n\n\n\n\n\n\n\n2.2.4 独立性と共分散\n\n\n\n\n\n\n定義（確率変数の独立性）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n\\(X,Y\\) が互いに 独立 であるとは，任意の事象 \\(A,B\\subset\\Omega\\) について， \\[\n\\operatorname{P}[X\\in A,Y\\in B]=\\operatorname{P}[X\\in A]\\operatorname{P}[Y\\in B]\n\\] を満たすことをいう．\n\\(X,Y\\) の 共分散 とは， \\[\n\\mathrm{Cov}[X,Y]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])(Y-\\operatorname{E}[Y])\\biggr]\n\\] をいう．\n\n\n\n\n\n\n\n\n\n命題（独立確率変数の性質）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n\\(X,Y\\) が独立ならば，次が成り立つ： \\[\n\\operatorname{E}[XY]=\\operatorname{E}[X]\\operatorname{E}[Y].\n\\]\n次が成り立つ： \\[\n\\mathrm{V}[X+Y]=\\mathrm{V}[X]+2\\mathrm{Cov}[X,Y]+\\mathrm{V}[Y].\n\\]\n（独立ならば無相関）\\(X,Y\\) が独立ならば， \\[\n\\mathrm{Cov}[X,Y]=0.\n\\] 特に，\\(X,Y\\) が独立ならば，\\(\\mathrm{V}\\) は加法を保存する．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\nLebesgue 積分論の議論が必要なので省略する．\n\nのみから従う．式変形は次の通り： \\[\n\\begin{align*}\n\\mathrm{V}[X+Y]&=\\operatorname{E}[(X+Y)^2]-\\biggr(\\operatorname{E}[X]+\\operatorname{E}[Y]\\biggl)^2\\\\\n&=\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n2.3 統計推測の技法(2) (pp.85-94)\n\n2.3.1 正規分布\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider\nimport seaborn as sns\n\n# 正規分布のグラフを描画する関数\ndef plot_normal_distribution(variance):\n    mean = 0  # 平均値\n    sigma = np.sqrt(variance)  # 標準偏差（分散の平方根）\n    \n    # 正規分布のデータを生成\n    x = np.linspace(-10, 10, 1000)\n    y = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (x - mean)**2 / (2 * sigma**2))\n    \n    # グラフを描画\n    plt.figure(figsize=(3, 2))\n    sns.lineplot(x=x, y=y)\n    plt.title(f'Normal Distribution with Variance {variance}')\n    plt.xlabel('Value')\n    plt.ylabel('Probability Density')\n    plt.show()\n\n# インタラクティブなウィジェットを作成\ninteract(plot_normal_distribution, variance=FloatSlider(value=1, min=0.1, max=5, step=0.1))\n\n\n\n\n\n&lt;function __main__.plot_normal_distribution(variance)&gt;\n\n\n\n\n2.3.2 Bernoulli分布と二項分布\n\n\n2.3.3 Poisson分布\n\n\n\n2.4 統計推測の技法(3) (pp.94-97)\nここで重要なトピックは不偏分散である．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#補足",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#補足",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "3 補足",
    "text": "3 補足\n\n3.1 第1節：母集団と標本\n\n3.1.1 「推測統計学」とは何か？\n本書 (草野耕一, 2016, p. 73) でも次のような注がなされている：\n\n推測統計学に対して，証拠から得られた情報をいかに効率的かつ明確に表現するかを研究する統計学の分野を記述統計学という (草野耕一, 2016, p. 73)．\n\n現代では「統計学」と言った際にほとんど推測統計学を指すと言っても過言ではない．4 つまり，現代では殆ど形骸化した区別である．この名称の本当の意味を理解するためには，歴史的な情緒を持った文脈が必要である．\n一言で言えば，推測統計学は Ronald A. Fisher の理論が出て来た際に，それ以前の Quetelet からの統計学との断絶を強調するために用いられた語であった．\n\n推計学 stochastic は推計と計画のための科学であり，その建設は主として英国の農学者 R. A. Fisher （現在 Cambridge 大学教授）の構想に懸る．–(増山元三郎, 1950, p. 3)\n\n\n\n\n\n\n\n戦後当時の理解\n\n\n\n\n\nその態度の違いは，次の例が鮮明に示している．\n\n例えば算術平均 mean という概念について：旧来の考え方に従えば平均 average は集団の ‘代表’ 値であると定義されている．5 従って，算術平均が該集団をよく ‘代表’ しうると考えられる場合にはこれを採用し，しからざる場合には度数分布の形を眺めた上で，その他の平均値，例えば並数 mode なり幾何平均 geometrical mean なりをもって，これに代えるのである．だから ‘代表値’ として如何なる平均値を選ぶかは，この際，全く個々人の常識に委ねられてしまう．これに反して推計学が算術平均を採用するのは，それが分布関数として表現せられた母集団の或る常数（即ち母数）の適切な平均値となりうる場合のみに限られる．従来統計学を定義して ‘平均の学である’ となす立場があるけれども（例えば A. L. Bowley）このような考え方こそ統計学の記述的性格を遺憾なく露呈するものであろう．統計学がこのような原理に立つ限り，それは爾後の行動に関し，形式以上に何ら指針を与える力をもちうるものではない．行動の正しい指針を与えないような学問は実は科学の名に値しないものというべきであろう．これにも拘らず推計学は吾国では最近に到るまで，科学・技術の分野でさえ仲々受け入れられず政治・経済の領域では殆ど問題にもされなかったのである．これに反し，米国などでは，推計学が社会・自然のあらゆる分野に進出し，第２次大戦の遂行にあたっても大きな貢献をなしたのであった．–(増山元三郎, 1950, p. 4)\n\n\n\n\n前者の記述統計学的な動機は，現代では「データサイエンス」のような分野に引き継がれている (Hoaglin et al., 2006)．\n\n\n\n3.2 Gallup事件\n\n3.2.1 1936年大統領選挙とクオータ抽出の重要性\n\n\n\n\n\n\nRoosevelt v.s. Landon (1936)\n\n\n\n\n背景には1929年10月24日の「暗黒の木曜日」に端を発した世界大恐慌があった．\n\n民主党 Franklin D. Roosevelt は再選を目指し，共和党の Alfred Landon が立ち向かった．\nRoosevelt の保守的な姿勢は大恐慌を食い止めるには力不足と思われ，再選の見込みは低いという意見も強く，The Literary Digest は237万人6 を対象に回収した調査結果から，57% の得票で Landon が勝つだろうと予測した．\n\n一方で Gallup 率いる the American Institute of Public Opinion は3000人の標本から Roosevelt が 55.7% の得票を得て当選するだろう，と予測した．7\n\n結果，Roosevelt が 60% の得票を得て，48州中46州を手にした．\nなぜ The Literary Digest は予測を誤ったのか？その原因は不適切な標本抽出法にあった．\n\nThe Literary Digest は自誌の購読者（大恐慌の最中でも購読した層）を対象に，そして自動車保有者と電話利用者の名簿を使って約1000万人に郵送し，回収された237万人の回答を用いた．\n過去5回の大統領選挙で的中させていたのは，経済的な状況があまり投票結果に影響しない時勢だったためと思われる．\n一方，Gallup は母集団を層別してサンプルサイズを割り当て，そのクオータに沿って標本を集める非確率的抽出法を用いていた．なお，Gallup は4ヶ月前から，The Literary Digest の予測は外れるだろうと新聞のコラム上で予言していた．\n\n\n\nこのエピソードは 不適切なデザイン下で収集された大量データよりも良いデザイン下で収集された少量のデータのほうがずっと役に立つ ということの好例として強調されることとなった．\nしかし，話はここでは終わらない．その Gallup も，後の大統領選挙で予測を大きく外している．\n\n\n3.2.2 1948年大統領選挙と無作為抽出の重要性\n\n\n\n\n\n\nTruman v.s. Dewwey (1948)\n\n\n\n\n1948年の選挙では，民主党は在任中に斃れた Roosevelt の後を継いでいた Harry Truman，共和党は4年前に Roosevelt に負けた Thomas Dewey が戦った．\nこの年の背景には公民権問題があり，共和党が20年ぶりに政権を奪還すると予想されており，Gallup もその例にもれなかった．\n\n結果，Truman が僅差で Dewey を破って当選した．\nなぜクオータ法を用いたサンプリングで実績を出した Gallup は，今回は予測を大きく誤ったのか？\n\n今回 Gallup も予想に失敗して世論調査そのものに懐疑の目が向けられたことを重く見て，検討委員会が設置された．\nそこで論点となったのが，当時 Gallup が用いていた割当法では，層内の個々の対象者の決定が調査員の個人的判断に委ねられていたことが多きなバイアスの原因となっていると予想された．\nその結果，今日では 無作為抽出 が大原則として一層強調されるエピソードとなっている (総務省統計局, 2023年4月21日確認)．\nだがこれ自体が原因だとは言えない．事実，調査担当員もそのことは自覚しており，予測を修正する調整技術を独自に開発して用いていたという (佐藤寧, 2020)．どんな標本調査法にも偏りがあり，これを修正するための予測モデルと併用するという営みは現在の無作為抽出法でも同様であり，これ自体が問題ではない．\n当時（現在も）広く用いられている電話調査という手法が，1948年代当時では裕福な有権者（電話を購入することができ，また不変の住所を維持していた）に偏ったサンプル抽出に導いたという議論もある．8\n\n\n\n\n\n\n(佐藤寧, 2020, p. 15) より\n\n\n1947年時点での GHQ による日本への統計指導でも，すでに無作為抽出法（当時は「任意見本法」）による調査が指導されている (佐藤寧, 2020)．よって，当時からクオータ法の問題は認識されており，これに必要な対策を打つ形で運用されていたと解すのが妥当であろう．\nなお，日本側のエピソードとして，統計数理研究所第７代所長も務めた 林知己夫 のオーラルヒストリーに次のような一節がある：\n\nそんな時に，CIEの担当官は，日本の新聞社を集めて，「アメリカではクォータサンプルでやっているけれど，そんなのはサンプリングじゃない」と，トルーマン，デューイの大統領選挙の予測を持ち出してきてですね，「これはクォータサンプリングでやったから間違えたんだ．こんなもの夢夢やるんじゃないぞ」と．そうしてみんな肝に銘じたんですよね．サンプリングは厳正にやらなきゃいけないって教わったわけです (高橋正樹, 2004)．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#footnotes",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし，\\(\\mathbb{R}\\) とは実数の全体からなる集合とした．↩︎\n(Wu & Thompson, 2020) など．↩︎\n数学では \\(P(\\Omega):=\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\) と表す．これを 冪集合 (power set) という．その頭文字の \\(P\\) である．↩︎\nなお，(増山元三郎, 1950) の序によると，推測統計学 という語は北川敏男によるものであり，増山は 推計学 (stochastics) と呼んでいる．↩︎\n\\(\\phi(y,y,\\cdots)=\\phi(x_1,x_2,\\cdots)\\) なる関係が成り立つとき，\\(y\\) を \\(x_1,x_2,\\cdots\\) の平均という．参照：(Jevons, 1879, p. 391)↩︎\n(中山健夫, 2003)↩︎\n(鈴木督久, 2021) によると，実際はこれは史実の誤解であるようだ．Gallup が「Digest は Landon が 56% だとして予測を誤るだろう」というコラムを新聞社に送付するのに用いた標本が3000なのであって，Gallup 自身の選挙予測調査の標本サイズは30万人であったという．なお，その際の抽出法については歴史的な文献が欠けており，知る由がないという．とは言え，それでも，「標本は量より質」という教訓になる，という意味では，象徴的なエピソードであることは間違いない．↩︎\nWikipedia の記述．↩︎"
  },
  {
    "objectID": "posts/2023/Probability/MarkovCategory.html",
    "href": "posts/2023/Probability/MarkovCategory.html",
    "title": "Markov Category (nLab) | 紹介",
    "section": "",
    "text": "0. はじめに\n綜合的確率論 (synthetic probability) とは，確率論の定義と定理を，その性質によって特徴づけようとする試みである．確率論が測度論に依拠しているのは「一つの実装例」に過ぎず，より普遍的で自然な定義が見つかるはずだ，というものである．思えば，条件付き期待値がa.s.にしか定まらないこと，多くの正則性条件が成り立つためには空間の可分性が必要であること．確率論には，あまりにも恣意的で非本質的な，「確率論に関係のない議論」が多いとは思わないか？これは，確率というものの数理的な構造を，我々が正しく把握できていないからなのではないか？\n\nThe basic object of study in probability is the random variable and I will argue that it should be treated as a basic construct . . . and it is artificial and unnatural to define it in terms of measure theory. (Mumford, 2000)\n\nこのような精神を持ち，具体的には圏論的な方法で，確率論のもう一つのモデルを構築しようとするのが綜合的確率論である．Anders Kock と Lewvere による綜合的微分幾何学 (synthetic differential geometry) からの接続を意識した命名であり，数学の各分野を「圏論化」することを，「綜合的」という形容詞で捉えようとしている．\nnLabとは，圏論的な視点から種々の数学・物理学・哲学の概念をまとめた，有志によって運営されているウィキである．今回は Markov圏のページ を翻訳．\n\n\n\nScreen Shot from nLab\n\n\n\n\n1．アイデア\nMarkov 圏の概念は，確率統計学の綜合的 (synthetic) な側面を表現する方法の1つである．すなわち，確率統計学を基礎付ける構造と公理からなり，これを用いて測度論を介することなく直接的に種々の定理が示せる．通常の測度論的な議論は，綜合的確率論のモデル（意味論）の1つであると見なされる．\n直感的に言えば，Markov 圏とは射が「確率変数」または「Markov 核」（ここから名前がついた）と見なせるような，確率論で用いられる圏である．標準的な例に，Kleisli 圏や確率モナドがあるが，Markov 圏は更に一般的な枠組みである．\n\n\n2．定義\nMarkov 圏とは，半デカルト対称モノイダル圏 \\((C,\\otimes,1)\\) であって，その対象 \\(X\\in C\\) が可換な内部余モノイドの構造を持つものである．余乗法と余単位写像は \\(\\mathrm{copy}:X\\to X\\otimes X\\) と \\(\\mathrm{delete}:X\\to1\\) とそれぞれ表される．\n複製写像とテンソル積の間に次の整合性条件を課す：任意の対象 \\(X,Y\\in C\\) に対して，\n\\[\n\\mathrm{copy}_{X\\otimes Y}=(\\mathrm{id}_X\\otimes b_{Y,X}\\otimes\\mathrm{id}_Y)(\\mathrm{copy}_X\\otimes\\mathrm{copy}_Y).\n\\]\nただし， \\(d\\) でブライダルを表す．\nまた，写像 \\(\\mathrm{delete}:X\\to 1\\) は， \\(1\\) が終対象であることから一意的であるため，更に \\(X\\) 内で自然であることに注意．一方で，複製写像は自然とは限らない．\n\n\n3．注\n\nA Markov category can equivalently be defined as a semicartesian symmetric monoidal category that supplies commutative comonoids.\n\n\n\n4．例\n\n有限集合と確率行列のなす圏 \\(\\mathtt{FinStoch}\\) ．\n可測空間とMarkov核のなす圏 \\(\\mathtt{Stoch}\\) ．\n任意のデカルトモノイダル圏 \\(C\\) が，モノイド単位を保存するモノイダルモナド \\(T\\) を持つならば，そのKleisli圏 \\(\\mathrm{Kl}(T)\\) はMarkov圏になる．\n\n\n\n5．決定論的な射\nMarkov圏の射 \\(f:X\\to Y\\) が決定論的であるとは，複製写像と可換であることをいう：\n\\[\n\\mathrm{copy}\\circ f=(f\\otimes f)\\circ\\mathrm{copy}.\n\\]\nこの定義のモチベーションは以下の通りである． \\(f\\) が例えば実数上の実確率変数で，入力に，サイコロの目を振ってでた値を加えるような関数であるとしよう．すると，入力 \\(x\\in\\mathbb{R}\\) に対して，サイコロを振り，出た目 \\(n\\in[6]\\) を加えて得た結果をコピーするから，左辺は \\((x+n,x+n)\\) である．一方で，まず入力 \\(x\\in\\mathbb{R}\\) を複製写像 \\(\\mathrm{copy}\\) に渡し， \\((x,x)\\) を得た後でサイコロを2回降り，出た目 \\(n_1,n_2\\in[6]\\) をそれぞれ加えると，右辺は \\((x+n_1,x+n_2)\\) となるが，別々の試行で出た目が一致する \\(n_1=n_2\\) とは限らない．この性質を，「ランダム性」の定義とする，というのである：つまりランダム性とは，2回行ったときに結果が異なり得る，という過程に宿るものとする．また，同値なことだが，その過程の前に情報を複製することと，その過程を見た後に情報を複製することとで，異なる状況を与えるような「過程」のことだとも理解できる．\n\n\n10．参考文献\nTobias Fritz（現在オーストリアInnsbruck大学）がこの分野の騎手であり，他にホモトピー型理論のレクチャーノートも執筆している．\n\nTobias Fritz (2019) A synthetic approach to Markov kernels, conditional independence and theorems on sufficient statistics. (arXiv:1908.07021)\nTobias Fritz and Eigil Fjeldgren Rischel (2019) The zero-one laws of Kolmogorov and Hewitt–Savage in categorical probability. (arXiv:1912.02769)\n\nこの研究の流れは，Bart Jacobsによるchannel perspectiveを汲んでいる．彼らは同様の概念をaffine CD-圏と呼んでいたようだ．\n\nBart Jacobs and Fabio Zanasi (2018) The Logical Essentials of Bayesian Reasoning. (arXiv:1804.01193)\n\n\n\n\n\n\nReferences\n\nMumford, D. (2000). The dawning of the age of stochasticity. In Mathematics : Frontiers and perspectives (pp. 197–218). American Mathematical Society. https://eudml.org/doc/289648"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html",
    "href": "posts/2023/Probability/条件付き期待値の問題.html",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "",
    "text": "条件付き期待値を，測度論から厳密に定義する際，ポイントは次の4点である．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\n\n\nポイント\n\n\n\n\n条件付き期待値は \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に対して \\(\\operatorname{E}[X|\\mathcal{G}]\\) の形で（\\(\\Omega\\) 上殆ど至る所）定義される確率変数である（ 節 1.1 ）．\n\\(\\operatorname{E}[X|Y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)]\\) の略記である（ 節 1.2 ）．\n\\(\\operatorname{E}[X|Y=y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)](Y^{-1}(y))\\) のことである（ 節 1.2 ）．\n\\(X\\in L^2(\\Omega)\\) でもあるとき，\\(\\operatorname{E}[X|\\mathcal{F}]\\) は \\(X\\) に \\(L^2(\\Omega)\\)-距離で最も近いような \\(\\mathcal{F}\\)-可測確率変数である（ 節 1.3 ）．\n条件付き確率は \\(\\operatorname{P}[Y\\in B|X]:=\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}|X]\\) と定義する（ 節 1.4 ）．\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き期待値）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間とし，\\(\\mathcal{G}\\) を \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数とする．可積分確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\) について， 次の2条件を満たす，\\(\\operatorname{P}\\)-零集合を除いて一意な確率変数を条件付き期待値といい，\\(\\operatorname{E}[X|\\mathcal{G}]\\) で表す．\n\n\\(\\mathcal{G}\\)-可測でもある \\(\\operatorname{P}\\)-可積分確率変数である．\n任意の \\(\\mathcal{G}\\)-可測集合 \\(B\\in\\mathcal{G}\\) 上では \\(X\\) と期待値が同じ確率変数になる：\\[\\operatorname{E}[X1_B]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}]1_B]\\]\n\n\n\n\\(\\operatorname{E}[X|\\mathcal{G}]\\) は \\(L^1(\\Omega,\\mathcal{G},\\operatorname{P})\\) の元であり，数学的対象としては「関数の同値類」である．関数としてはある零集合の上では定まらない．その任意の代表元も \\(\\operatorname{E}[X|\\mathcal{G}]\\) と表すことが多く，1 その場合は，多くの等式には a.s. (= almost surely) がついてまわることになる．\nもちろん，\\(L^1(\\operatorname{P})\\) 上の順序関係 \\(\\le\\) を \\[\nX\\le Y:\\Leftrightarrow X\\le Y\\;\\;\\text{a.s.}\n\\] と定義し，a.s. を省略して書いてもよい．\n\n\n\n\n\n\n証明\n\n\n\n\n\n定義の2条件のみから，\\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\operatorname{P}\\)-零集合を除いて一意に定まること（とその存在）を示す．\n\\[Q(B):=\\operatorname{E}[X,B]=\\operatorname{E}[1_BX]\\;(B\\in\\mathcal{G})\\] とおくことで，\\(Q\\) は可測空間 \\((\\Omega,\\mathcal{G})\\) 上の確率測度を定める． いま，\\(\\operatorname{P}|_\\mathcal{G}\\) に関して \\(Q\\) は絶対連続になっている：\\[\\forall_{B\\in\\mathcal{G}}\\;P(B)=0\\Rightarrow Q(B)=0.\\] これより，Radon-Nikodymの定理から， ある \\(\\mathcal{G}\\)-可測で \\(\\operatorname{P}\\)-可積分な可測関数 \\(Y:\\Omega\\to\\mathbb{R}\\) が，\\(\\operatorname{P}\\)-零集合上での違いを除いて一意的に存在して，\\[\\forall_{B\\in\\mathcal{G}}\\;Q(B)=\\int_BY(\\omega)P(d\\omega)\\] が成り立つ． よって，条件付き期待値 \\(Y\\) は確かに存在して（同値類 \\(L^1(\\operatorname{P})\\) の元としては）一意的で，(1),(2)が成り立つ．\n\n\n\n(Dudley, 2002, pp. 10.1節 p.336), (吉田朋広, 2006, p. 43) がおすすめな参照先．(舟木直久, 2004, p. 88) が入門しやすい．\\(X\\in L^2(\\Omega)\\) でいい場合は，より「射影」としてわかりやすい特徴付けがある（ 節 1.3 ）．これのおすすめは (Jacod & Protter, 2004, pp. 第23節 p.200), (Kallenberg, 2021, p. 164)．\n\n\n\n\n\n\n\n\n\n定義（確率変数を与えた下での条件付き期待値）\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間とする．確率変数 \\(X\\in\\mathcal{L}(\\Omega;E)\\) による \\(Y\\in\\mathcal{L}^1(\\Omega)\\) の条件付き期待値は，次を満たす可測関数 \\(\\operatorname{E}[Y|X=-]:E\\to\\mathbb{R}\\) のことをいう： \\[\n\\begin{align*}\n\\forall_{B\\in\\mathcal{E}}\\quad&\\int_{X^{-1}(B)}Y(\\omega)P(d\\omega)\\\\\n&\\quad=\\int_B\\operatorname{E}[Y|X=x]P^X(dx).\n\\end{align*}\n\\]\n\n\nすると，\\(X\\) が \\(\\Omega\\) 上に引き戻す \\(\\sigma\\)-代数 \\[\n\\sigma(X):=\\left\\{A\\subset\\Omega\\mid\\exists_{B\\in\\mathcal{E}}\\; X^{-1}(B)=A\\right\\}\n\\] を与えた下での条件付き期待値 \\(\\operatorname{E}[Y|\\sigma(X)]\\) と，次のように関係する．2 \\(\\operatorname{E}[Y|\\sigma(X)]\\) は定義 節 1.1 1から \\(\\sigma[X]\\)-可測であるが，可測性の特徴付け（後述）から，これはあるBorel可測関数 \\(f\\) について，\\[\\operatorname{E}[Y|X]=f(X)\\;\\;\\text{a.s.}\\] と表せる．この \\(f:\\mathcal{X}\\to\\mathbb{R}\\) が，\\(X\\) を与えた下での \\(Y\\) の条件付き期待値 \\(\\operatorname{E}[Y|X=-]\\) である．\nこの記法 \\(\\operatorname{E}[Y|X=x]\\) とは何かというと，\\(X\\) の値域 \\(\\mathcal{X}\\) 上の関数として，新たに \\[\\operatorname{E}[Y|X=x]:=f(x)\\;\\;\\text{a.s.}\\] と書くことにするのである．3 すると， \\[\n\\operatorname{E}[Y|X=x]|_{x=X(\\omega)}=\\operatorname{E}[Y|X](\\omega)\\;\\;\\text{a.s.}\n\\] も満たす．つまり，次の図式が可換である：\n\n\n\nCommutative Diagram for Conditional Expectations\n\n\n\n\n\n\n\n\n命題4 (Doob, 1953)\n\n\n\n\\(S\\) を位相空間，\\(X\\in \\mathcal{L}(\\Omega;S),Y\\in \\mathcal{L}(\\Omega)\\) を確率変数とする．次は同値：\n\n\\(Y\\)は \\(\\sigma(X)\\)-可測．\nあるBorel可測関数 \\(f:S\\to\\mathbb{R}\\) が存在して，\\(Y=f(X)\\) を満たす．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n2 \\(\\Rightarrow\\) 1 はすぐに従う．任意の \\(B\\in\\mathcal{B}(\\mathbb{R})\\) について，\\(f^{-1}(B)\\in\\mathcal{B}(S)\\) であるから， \\[Y^{-1}(B)=X^{-1}(f^{-1}(B))\\in\\sigma(X).\\] あとは 1 \\(\\Rightarrow\\) 2 を示せば良い．3段階で示す．\n\nまず \\(Y\\) が単関数 \\[Y=\\sum_{i=1}^nc_i1_{A_i},\\qquad c_i\\ne c_j\\;(i\\ne j).\\] の場合について示す．仮定より \\(A_i\\in\\sigma(X)\\) であるから，ある \\(B_i\\in\\mathcal{B}(S)\\) が存在して \\(A_i=X^{-1}(B_i)\\)．よって， \\[f(x):=\\sum_{i=1}^nc_i1_{B_i}(x).\\] と定めると \\(Y=f(X)\\)．\n次に \\(Y\\ge0\\;\\;\\text{a.s.}\\) の場合を考えると，正な単関数の単調増加列 \\(\\{Y_n\\}\\) で \\(Y\\) に収束するものが取れる．各 \\(Y_n\\) について，\\(f_n\\in\\mathcal{L}(S)\\) が存在して \\(Y_n=f_n(X)\\) が成り立つ．このとき，\\(f:=\\limsup_{n\\to\\infty}f_n\\) と定めれば， \\[\\begin{align*}\nY&=\\limsup_{n\\to\\infty}Y_n\\\\\n&=(\\limsup_{n\\to\\infty}f_n)(X)=f(X).\n\\end{align*}\\]\n一般の場合は \\(Y=Y^+-Y^-\\) の分解から従う．\n\n\n\n\n(Dudley, 2002, pp. 定理4.2.8 p.128) は \\(S=\\mathbb{R}\\) の場合，(Landkov, 1972) は \\(S=\\mathbb{R}^m\\) の場合, (Kallenberg, 2021, pp. 補題1.14 p.18) に一般の標準Borel空間の場合の証明がある．nLab も極めて参考になる．\n\n\n\n\\(L^2(\\Omega)\\subset L^1(\\Omega)\\) 上に議論を制限してみると，実は \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に関する条件付き期待値は，部分空間 \\[\nL^2_\\mathcal{G}(\\Omega):=\\left\\{X\\in L^2(\\Omega)\\:\\middle|\\:X\\,\\text{は}\\,\\mathcal{G}\\,\\text{-可測}\\right\\}\n\\] への射影になっている．\n\n\n\n\n\n\n定理（条件付き期待値の特徴付け）\n\n\n\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\subset\\mathcal{F}\\) と \\(X\\in\\mathcal{L}^2(\\Omega)\\) を考える． 任意の \\(\\widehat{X}_\\mathcal{G}\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)\\) について，次は同値：\n\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の \\(L^2_\\mathcal{G}(\\Omega)\\) への射影である： \\[\n\\begin{align*}\n&\\|X-\\widehat{X}_\\mathcal{G}\\|_{L^2(\\Omega)}\\\\\n&=\\inf_{X'\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)}\\|X-X'\\|_{L^2(\\Omega)}.\n\\end{align*}\n\\]\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の条件付き期待値である：\\[\\forall_{Z\\in L^2_\\mathcal{G}(\\Omega)}\\;\\operatorname{E}[ZX]=\\operatorname{E}[Z\\widehat{X}_\\mathcal{G}].\\]\n\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間，\\(\\mathcal{G}\\subset\\mathcal{F}\\) を部分 \\(\\sigma\\)-代数とする．\\(\\mathcal{G}\\) の定める条件付き確率を， \\[\n\\operatorname{P}[B|\\mathcal{G}](\\omega):=\\operatorname{E}[1_B|\\mathcal{G}](\\omega)\\;(B\\in\\mathcal{F})\n\\] で定める．\n\n\nしかしこの定義には問題がある．条件付き期待値 \\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\Omega\\) 上 \\(\\operatorname{P}\\text{-a.e.}\\) でしか定まらない（場合がある）から，\\(\\operatorname{P}\\) も一般には可算加法性をa.s.にしか満たさない： \\[\n\\operatorname{P}\\left[\\bigcap_{n\\in\\mathbb{N}}A_n\\,\\middle|\\,\\mathcal{G}\\right]=\\sum_{n\\in\\mathbb{N}}\\operatorname{P}[A_n]\\;\\;\\text{a.s.}\n\\] この式自体は後述の単調収束定理（ 節 2.3 ）から示せる．\nだが，\\(\\mathcal{G}\\) がある完備可分距離空間に値を取る確率変数 \\(Y\\) について \\(\\mathcal{G}=\\sigma(Y)\\) である場合など，殆どの場合で，うまく \\(\\operatorname{P}\\) を取ることが出来る．5 このように，a.s. 抜きで正式に確率測度として定まる場合，その確率核 \\(\\operatorname{P}:E\\times\\mathcal{G}\\to[0,1]\\) を，正則条件付き確率と呼び分ける．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#条件付き期待値の定義",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#条件付き期待値の定義",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "",
    "text": "条件付き期待値を，測度論から厳密に定義する際，ポイントは次の4点である．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\n\n\nポイント\n\n\n\n\n条件付き期待値は \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に対して \\(\\operatorname{E}[X|\\mathcal{G}]\\) の形で（\\(\\Omega\\) 上殆ど至る所）定義される確率変数である（ 節 1.1 ）．\n\\(\\operatorname{E}[X|Y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)]\\) の略記である（ 節 1.2 ）．\n\\(\\operatorname{E}[X|Y=y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)](Y^{-1}(y))\\) のことである（ 節 1.2 ）．\n\\(X\\in L^2(\\Omega)\\) でもあるとき，\\(\\operatorname{E}[X|\\mathcal{F}]\\) は \\(X\\) に \\(L^2(\\Omega)\\)-距離で最も近いような \\(\\mathcal{F}\\)-可測確率変数である（ 節 1.3 ）．\n条件付き確率は \\(\\operatorname{P}[Y\\in B|X]:=\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}|X]\\) と定義する（ 節 1.4 ）．\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き期待値）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間とし，\\(\\mathcal{G}\\) を \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数とする．可積分確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\) について， 次の2条件を満たす，\\(\\operatorname{P}\\)-零集合を除いて一意な確率変数を条件付き期待値といい，\\(\\operatorname{E}[X|\\mathcal{G}]\\) で表す．\n\n\\(\\mathcal{G}\\)-可測でもある \\(\\operatorname{P}\\)-可積分確率変数である．\n任意の \\(\\mathcal{G}\\)-可測集合 \\(B\\in\\mathcal{G}\\) 上では \\(X\\) と期待値が同じ確率変数になる：\\[\\operatorname{E}[X1_B]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}]1_B]\\]\n\n\n\n\\(\\operatorname{E}[X|\\mathcal{G}]\\) は \\(L^1(\\Omega,\\mathcal{G},\\operatorname{P})\\) の元であり，数学的対象としては「関数の同値類」である．関数としてはある零集合の上では定まらない．その任意の代表元も \\(\\operatorname{E}[X|\\mathcal{G}]\\) と表すことが多く，1 その場合は，多くの等式には a.s. (= almost surely) がついてまわることになる．\nもちろん，\\(L^1(\\operatorname{P})\\) 上の順序関係 \\(\\le\\) を \\[\nX\\le Y:\\Leftrightarrow X\\le Y\\;\\;\\text{a.s.}\n\\] と定義し，a.s. を省略して書いてもよい．\n\n\n\n\n\n\n証明\n\n\n\n\n\n定義の2条件のみから，\\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\operatorname{P}\\)-零集合を除いて一意に定まること（とその存在）を示す．\n\\[Q(B):=\\operatorname{E}[X,B]=\\operatorname{E}[1_BX]\\;(B\\in\\mathcal{G})\\] とおくことで，\\(Q\\) は可測空間 \\((\\Omega,\\mathcal{G})\\) 上の確率測度を定める． いま，\\(\\operatorname{P}|_\\mathcal{G}\\) に関して \\(Q\\) は絶対連続になっている：\\[\\forall_{B\\in\\mathcal{G}}\\;P(B)=0\\Rightarrow Q(B)=0.\\] これより，Radon-Nikodymの定理から， ある \\(\\mathcal{G}\\)-可測で \\(\\operatorname{P}\\)-可積分な可測関数 \\(Y:\\Omega\\to\\mathbb{R}\\) が，\\(\\operatorname{P}\\)-零集合上での違いを除いて一意的に存在して，\\[\\forall_{B\\in\\mathcal{G}}\\;Q(B)=\\int_BY(\\omega)P(d\\omega)\\] が成り立つ． よって，条件付き期待値 \\(Y\\) は確かに存在して（同値類 \\(L^1(\\operatorname{P})\\) の元としては）一意的で，(1),(2)が成り立つ．\n\n\n\n(Dudley, 2002, pp. 10.1節 p.336), (吉田朋広, 2006, p. 43) がおすすめな参照先．(舟木直久, 2004, p. 88) が入門しやすい．\\(X\\in L^2(\\Omega)\\) でいい場合は，より「射影」としてわかりやすい特徴付けがある（ 節 1.3 ）．これのおすすめは (Jacod & Protter, 2004, pp. 第23節 p.200), (Kallenberg, 2021, p. 164)．\n\n\n\n\n\n\n\n\n\n定義（確率変数を与えた下での条件付き期待値）\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間とする．確率変数 \\(X\\in\\mathcal{L}(\\Omega;E)\\) による \\(Y\\in\\mathcal{L}^1(\\Omega)\\) の条件付き期待値は，次を満たす可測関数 \\(\\operatorname{E}[Y|X=-]:E\\to\\mathbb{R}\\) のことをいう： \\[\n\\begin{align*}\n\\forall_{B\\in\\mathcal{E}}\\quad&\\int_{X^{-1}(B)}Y(\\omega)P(d\\omega)\\\\\n&\\quad=\\int_B\\operatorname{E}[Y|X=x]P^X(dx).\n\\end{align*}\n\\]\n\n\nすると，\\(X\\) が \\(\\Omega\\) 上に引き戻す \\(\\sigma\\)-代数 \\[\n\\sigma(X):=\\left\\{A\\subset\\Omega\\mid\\exists_{B\\in\\mathcal{E}}\\; X^{-1}(B)=A\\right\\}\n\\] を与えた下での条件付き期待値 \\(\\operatorname{E}[Y|\\sigma(X)]\\) と，次のように関係する．2 \\(\\operatorname{E}[Y|\\sigma(X)]\\) は定義 節 1.1 1から \\(\\sigma[X]\\)-可測であるが，可測性の特徴付け（後述）から，これはあるBorel可測関数 \\(f\\) について，\\[\\operatorname{E}[Y|X]=f(X)\\;\\;\\text{a.s.}\\] と表せる．この \\(f:\\mathcal{X}\\to\\mathbb{R}\\) が，\\(X\\) を与えた下での \\(Y\\) の条件付き期待値 \\(\\operatorname{E}[Y|X=-]\\) である．\nこの記法 \\(\\operatorname{E}[Y|X=x]\\) とは何かというと，\\(X\\) の値域 \\(\\mathcal{X}\\) 上の関数として，新たに \\[\\operatorname{E}[Y|X=x]:=f(x)\\;\\;\\text{a.s.}\\] と書くことにするのである．3 すると， \\[\n\\operatorname{E}[Y|X=x]|_{x=X(\\omega)}=\\operatorname{E}[Y|X](\\omega)\\;\\;\\text{a.s.}\n\\] も満たす．つまり，次の図式が可換である：\n\n\n\nCommutative Diagram for Conditional Expectations\n\n\n\n\n\n\n\n\n命題4 (Doob, 1953)\n\n\n\n\\(S\\) を位相空間，\\(X\\in \\mathcal{L}(\\Omega;S),Y\\in \\mathcal{L}(\\Omega)\\) を確率変数とする．次は同値：\n\n\\(Y\\)は \\(\\sigma(X)\\)-可測．\nあるBorel可測関数 \\(f:S\\to\\mathbb{R}\\) が存在して，\\(Y=f(X)\\) を満たす．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n2 \\(\\Rightarrow\\) 1 はすぐに従う．任意の \\(B\\in\\mathcal{B}(\\mathbb{R})\\) について，\\(f^{-1}(B)\\in\\mathcal{B}(S)\\) であるから， \\[Y^{-1}(B)=X^{-1}(f^{-1}(B))\\in\\sigma(X).\\] あとは 1 \\(\\Rightarrow\\) 2 を示せば良い．3段階で示す．\n\nまず \\(Y\\) が単関数 \\[Y=\\sum_{i=1}^nc_i1_{A_i},\\qquad c_i\\ne c_j\\;(i\\ne j).\\] の場合について示す．仮定より \\(A_i\\in\\sigma(X)\\) であるから，ある \\(B_i\\in\\mathcal{B}(S)\\) が存在して \\(A_i=X^{-1}(B_i)\\)．よって， \\[f(x):=\\sum_{i=1}^nc_i1_{B_i}(x).\\] と定めると \\(Y=f(X)\\)．\n次に \\(Y\\ge0\\;\\;\\text{a.s.}\\) の場合を考えると，正な単関数の単調増加列 \\(\\{Y_n\\}\\) で \\(Y\\) に収束するものが取れる．各 \\(Y_n\\) について，\\(f_n\\in\\mathcal{L}(S)\\) が存在して \\(Y_n=f_n(X)\\) が成り立つ．このとき，\\(f:=\\limsup_{n\\to\\infty}f_n\\) と定めれば， \\[\\begin{align*}\nY&=\\limsup_{n\\to\\infty}Y_n\\\\\n&=(\\limsup_{n\\to\\infty}f_n)(X)=f(X).\n\\end{align*}\\]\n一般の場合は \\(Y=Y^+-Y^-\\) の分解から従う．\n\n\n\n\n(Dudley, 2002, pp. 定理4.2.8 p.128) は \\(S=\\mathbb{R}\\) の場合，(Landkov, 1972) は \\(S=\\mathbb{R}^m\\) の場合, (Kallenberg, 2021, pp. 補題1.14 p.18) に一般の標準Borel空間の場合の証明がある．nLab も極めて参考になる．\n\n\n\n\\(L^2(\\Omega)\\subset L^1(\\Omega)\\) 上に議論を制限してみると，実は \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に関する条件付き期待値は，部分空間 \\[\nL^2_\\mathcal{G}(\\Omega):=\\left\\{X\\in L^2(\\Omega)\\:\\middle|\\:X\\,\\text{は}\\,\\mathcal{G}\\,\\text{-可測}\\right\\}\n\\] への射影になっている．\n\n\n\n\n\n\n定理（条件付き期待値の特徴付け）\n\n\n\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\subset\\mathcal{F}\\) と \\(X\\in\\mathcal{L}^2(\\Omega)\\) を考える． 任意の \\(\\widehat{X}_\\mathcal{G}\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)\\) について，次は同値：\n\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の \\(L^2_\\mathcal{G}(\\Omega)\\) への射影である： \\[\n\\begin{align*}\n&\\|X-\\widehat{X}_\\mathcal{G}\\|_{L^2(\\Omega)}\\\\\n&=\\inf_{X'\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)}\\|X-X'\\|_{L^2(\\Omega)}.\n\\end{align*}\n\\]\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の条件付き期待値である：\\[\\forall_{Z\\in L^2_\\mathcal{G}(\\Omega)}\\;\\operatorname{E}[ZX]=\\operatorname{E}[Z\\widehat{X}_\\mathcal{G}].\\]\n\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間，\\(\\mathcal{G}\\subset\\mathcal{F}\\) を部分 \\(\\sigma\\)-代数とする．\\(\\mathcal{G}\\) の定める条件付き確率を， \\[\n\\operatorname{P}[B|\\mathcal{G}](\\omega):=\\operatorname{E}[1_B|\\mathcal{G}](\\omega)\\;(B\\in\\mathcal{F})\n\\] で定める．\n\n\nしかしこの定義には問題がある．条件付き期待値 \\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\Omega\\) 上 \\(\\operatorname{P}\\text{-a.e.}\\) でしか定まらない（場合がある）から，\\(\\operatorname{P}\\) も一般には可算加法性をa.s.にしか満たさない： \\[\n\\operatorname{P}\\left[\\bigcap_{n\\in\\mathbb{N}}A_n\\,\\middle|\\,\\mathcal{G}\\right]=\\sum_{n\\in\\mathbb{N}}\\operatorname{P}[A_n]\\;\\;\\text{a.s.}\n\\] この式自体は後述の単調収束定理（ 節 2.3 ）から示せる．\nだが，\\(\\mathcal{G}\\) がある完備可分距離空間に値を取る確率変数 \\(Y\\) について \\(\\mathcal{G}=\\sigma(Y)\\) である場合など，殆どの場合で，うまく \\(\\operatorname{P}\\) を取ることが出来る．5 このように，a.s. 抜きで正式に確率測度として定まる場合，その確率核 \\(\\operatorname{P}:E\\times\\mathcal{G}\\to[0,1]\\) を，正則条件付き確率と呼び分ける．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#性質",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#性質",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "2 性質",
    "text": "2 性質\n\n2.1 作用素としての性質\n\\(\\mathcal{G}\\)-可測な可積分関数のなす部分空間を \\(L_{\\mathcal{G}}^1(\\Omega)\\subset L^1(\\Omega)\\) で表す．\n\n\n\n\n\n\n命題（条件付き期待値はノルム減少的な正作用素）\n\n\n\n条件付き期待値 \\(\\operatorname{E}_{\\mathcal{G}}:L^1(\\Omega)\\to L_{\\mathcal{G}}^1(\\Omega)\\) はノルム減少的で正な線型汎作用素である．すなわち，\n\n線型性：任意の実数 \\(a,b\\in\\mathbb{R}\\) について， \\[\\begin{align*}\n\\operatorname{E}[aX+bY|\\mathcal{G}]&=a\\operatorname{E}[X|\\mathcal{G}]\\\\\n&\\qquad+b\\operatorname{E}[Y|\\mathcal{G}]\\;\\;\\text{a.s.}\n\\end{align*}\\]\n正性：\\(X\\le Y\\;\\;\\text{a.s.}\\) ならば， \\[\\operatorname{E}[X|\\mathcal{G}]\\le\\operatorname{E}[Y|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\nJensenの不等式：\\(\\varphi:\\mathbb{R}\\to\\mathbb{R}\\) を凸関数とする．\\(\\varphi(X)\\in L^1(\\Omega)\\) ならば，\\[\\varphi(\\operatorname{E}[X|\\mathcal{G}])\\le\\operatorname{E}[\\varphi(X)|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n三角不等式：\\[\\lvert\\operatorname{E}[X|\\mathcal{G}]\\rvert\\le\\operatorname{E}[\\lvert X\\rvert|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n\nいずれも \\(L_{\\mathcal{G}}^1(\\Omega)\\) 上の等式・不等式であり，殆ど確実ににしか成り立たないことに注意．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n1は結局積分の線型性から従います．2は次のように議論できます．\n任意の \\(X\\in L^1(\\Omega)_+\\) について \\(\\operatorname{E}[X|\\mathcal{G}]\\in L^1(\\Omega)\\) を示せば良い．\\(A_n:=\\left\\{X'\\le 1/n\\right\\}\\in\\mathcal{G}\\) について，条件付き期待値の定義から，任意の \\(n\\in\\mathbb{N}^+\\) について， \\[\n\\begin{align*}\n0\\le\\operatorname{E}[X,A_n]&=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}],A_n]\\\\\n&\\le\\frac{1}{n}\\operatorname{P}[A_n].\n\\end{align*}\n\\] より，\\(\\lim_{n\\to\\infty}\\operatorname{P}[A_n]=0\\) が必要．これより， \\[\\operatorname{P}[\\operatorname{E}[X|\\mathcal{G}]&lt;0]\\le\\operatorname{P}[\\cup_{n=1}^\\infty A_n]=0.\\] が解る．\n3は単関数の場合から地道に示します．4はその特別の場合で \\(\\varphi(x)=\\lvert x\\rvert\\) と取った場合に当たります．\n\n\n\n\n\n2.2 Tower Property\n\n\n\n\n\n\n命題（繰り返し期待値の法則）\n\n\n\n2つの \\(\\sigma\\)-代数が \\(\\mathcal{G}_1\\subset\\mathcal{G}_2\\) を満たすならば，\\(\\operatorname{E}_{\\mathcal{G}_1}=\\operatorname{E}_{\\mathcal{G}_1}\\circ\\operatorname{E}_{\\mathcal{G}_2}\\)．すなわち， \\[\\operatorname{E}[X|\\mathcal{G}_1]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}_2]|\\mathcal{G}_1]\\;\\;\\text{a.s.}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n右辺を \\(Z\\) とおく．任意の \\(A\\in\\mathcal{G}_1\\) について，\\(A\\in\\mathcal{G}_2\\) でもあるから， \\[\\begin{align*}\n\\operatorname{E}[Z1_A]&=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}_1]1_A]\\\\\n&=\\operatorname{E}[X1_A].\n\\end{align*}\\]\n\n\n\n\n\n2.3 単調収束定理\n\n\n\n\n\n\n命題（条件付き期待値に対する単調収束定理）\n\n\n\n可積分な実確率変数の列 \\(\\{X_n\\}\\cup\\{X\\}\\subset L^1(\\Omega)\\) について， \\[X_n\\nearrow X\\;\\;\\text{a.s.}\\] \\[\\Rightarrow\\quad\\operatorname{E}[X_n|\\mathcal{G}]\\nearrow\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n条件付き期待値の正性 節 2.1 より， \\[\\operatorname{E}[X_n|\\mathcal{G}]\\le\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.},\\qquad n\\in\\mathbb{N}.\\] よって，有界な単調列は収束するから，ある \\(Y\\in L^1(\\Omega)\\) を \\(E[X_n|\\mathcal{G}]\\nearrow Y\\;\\;\\text{a.s.}\\) を満たすように定めることが出来る．同時に，通常の期待値に関する単調収束定理から， \\[\n\\begin{align*}\n\\operatorname{E}[X1_A]&=\\lim_{n\\to\\infty}\\operatorname{E}[X_n1_A]\\\\&=\\operatorname{E}[Y1_A]\\;(A\\in\\mathcal{G})\n\\end{align*}\n\\] が必要であるから，条件付き期待値の一意性より，\\(Y=\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n2.4 可測関数の取り出し\n\n\n\n\n\n\n命題（可測関数の取り出し）\n\n\n\n\\(X,XY\\in\\mathcal{L}^1(\\Omega)\\) を可積分，\\(Y\\in\\mathcal{L}_\\mathcal{G}(\\Omega)\\) を \\(\\mathcal{G}\\)-可測実確率変数とする．このとき，\n\n\\(XY\\in\\mathcal{L}^1(\\Omega)\\)ならば，\\[\\operatorname{E}[XY|\\mathcal{G}]=Y\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n特に，\\(\\operatorname{E}[Y|\\mathcal{G}]=Y\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n条件付き期待値の線型性から，\\(X,Y\\ge0\\) の場合について示せば良い．このとき，非負値単関数の収束列 \\(X_n\\nearrow X,Y_n\\nearrow Y\\) が取れる．\\(X_nY\\nearrow XY\\in L^1(\\Omega)\\) だから，単調収束定理 節 2.3 から \\[\\operatorname{E}[X_n|\\mathcal{G}]\\nearrow\\operatorname{E}[X|\\mathcal{G}]\\] \\[\n\\begin{align*}\n\\Rightarrow&\\quad Y\\operatorname{E}[X_n|\\mathcal{G}]\\nearrow Y\\operatorname{E}[X|\\mathcal{G}]\\\\\n\\quad\\land&\\quad\\operatorname{E}[X_nY|\\mathcal{G}]\\nearrow\\operatorname{E}[XY|\\mathcal{G}].\n\\end{align*}\\] よって，各 \\(n\\in\\mathbb{N}\\) について \\(Y\\operatorname{E}[X_n|\\mathcal{G}]=\\operatorname{E}[X_nY|\\mathcal{G}]\\) を示せば良い．単関数とは \\(X=1_C\\;(C\\in\\mathcal{G})\\) という形の関数の線型和だから，畢竟この形の関数について考えれば良いのである．任意の \\(B\\in\\mathcal{G}\\) について \\(C\\cap B\\in\\mathcal{G}\\) であるから， \\[\n\\begin{align*}\n\\int_B1_C\\operatorname{E}[Y|\\mathcal{G}]\\,d\\operatorname{P}&=\\int_{C\\cap B}\\operatorname{E}[Y|\\mathcal{G}]\\,d\\operatorname{P}\\\\\n&=\\int_{C\\cap B}Y\\,d\\operatorname{P}\\\\\n&=\\int_B1_CY\\,d\\operatorname{P}.\n\\end{align*}\n\\] 条件付き期待値の一意性より，\\(1_C\\operatorname{E}[Y|\\mathcal{G}]=\\operatorname{E}[1_CY|\\mathcal{G}]\\;\\;\\text{a.s.}\\) を得る．\n\n\n\n\n\n2.5 独立な場合\n\n\n\n\n\n\n命題（独立確率変数に対する性質）\n\n\n\n可積分実確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\)は \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) と独立とする．\n\n\\(\\operatorname{E}[X|\\mathcal{G}]=\\operatorname{E}[X]\\;\\;\\text{a.s.}\\)\n特に，\\(\\operatorname{E}[X|\\boldsymbol{2}]=\\operatorname{E}[X]\\;\\;\\text{a.s.}\\)．\n\nただし，\\(\\boldsymbol{2}=\\{\\emptyset,\\Omega\\}\\) とした．\n\n\n\n\n2.6 条件付き期待値のアトム上での値\n\n\n\n\n\n\n問題\n\n\n\n確率変数 \\(X,Y\\) とその値域の値 \\(y\\in\\mathcal{Y}\\) について， \\[\n\\operatorname{E}[X|Y=y]\\operatorname{P}[Y=y]=\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}]\n\\] はどう正当化されるか？\n\n\n\n\n\n\n\n\n説明\n\n\n\n\n\n\\(\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}]\\) の中身を \\(\\sigma(Y)\\) で条件付けてTower property（ 節 2.2 ）を使うと（定義 節 1.1 の条件2からと論じても良い），\\(1_{\\left\\{Y=y\\right\\}}\\) は \\(\\sigma(Y)\\)-可測だから，条件付き期待値の中身から出る（ 節 2.4 参照）．これによって正当化できる．式で表すと， \\[\n\\begin{align*}\n\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}]&=\\operatorname{E}[\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}|Y]]\\\\\n&=\\operatorname{E}[1_{\\left\\{Y=y\\right\\}}\\operatorname{E}[X|Y]]\\\\\n&=\\int_{\\mathcal{Y}}\\delta_y(y')\\operatorname{E}[X|Y=y']\\operatorname{P}(dy')\\\\\n&=\\operatorname{E}[X|Y=y]\\operatorname{P}[Y=y].\n\\end{align*}\n\\] ただし，\\(\\mathcal{Y}\\) 上の確率測度を \\(\\operatorname{P}\\) と置いた．\n\n\n\n条件付き確率の定義 節 1.4 から， \\[\n\\operatorname{P}[Y\\in B|X=x]:=\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}|X=x]\n\\] と議論できる．さらに \\(\\operatorname{P}[X=x]&gt;0\\) のとき， \\[\n\\begin{align*}\n    &=\\frac{\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}1_{\\left\\{X=x\\right\\}}]}{\\operatorname{P}[X=x]}\\\\\n    &=\\frac{\\operatorname{P}[Y\\in B,X=x]}{\\operatorname{P}[X=x]}\n\\end{align*}\n\\] という見慣れた表示を得る．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#更なる条件付け",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#更なる条件付け",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "3 更なる条件付け",
    "text": "3 更なる条件付け\n\n3.1 条件付き独立性\n\n\n\n\n\n\n定義（条件付き独立性）\n\n\n\n\\(\\mathcal{C}\\subset\\mathcal{F}\\) の下で，\\(\\mathcal{G}_1,\\cdots,\\mathcal{G}_n\\) が \\(\\mathcal{C}\\)-条件付き独立 であるとは，任意の \\(A_k\\in\\mathcal{G}_k\\;(k\\in[n])\\) について \\[\n\\operatorname{P}\\left[\\bigcap_{k\\in[n]}A_k\\:\\middle|\\:\\mathcal{C}\\right]\\overset{\\text{a.s.}}{=}\\prod_{k\\in[n]}\\operatorname{P}[A_k|\\mathcal{C}]\n\\] を満たすことをいう．\n\n\n\\(\\mathcal{C}=\\boldsymbol{2}\\) であるとき，通常の独立性に一致する（節 2.5 ）．また全ての確率変数は \\(\\mathcal{F}\\)-条件付き独立である（節 2.4 ）．\n\n\n\n\n\n\n命題（条件付き独立性の特徴付け Doob）\n\n\n\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{C},\\mathcal{G},\\mathcal{H}\\subset\\mathcal{F}\\) について，次は同値：6\n\n\\(\\mathcal{G}\\perp\\!\\!\\!\\perp\\mathcal{H}\\mid\\mathcal{C}\\)．\n任意の \\(H\\in\\mathcal{H}\\) について， \\[\n\\operatorname{P}[H|\\mathcal{G}\\lor\\mathcal{C}]\\overset{\\text{a.s.}}{=}\\operatorname{P}[H|\\mathcal{C}]\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n(1)\\(\\Rightarrow\\)(2)：任意の \\(C\\in\\mathcal{C},G\\in\\mathcal{G},H\\in\\mathcal{H}\\) を取る． \\[\n\\begin{align*}\n  &\\operatorname{E}[\\operatorname{P}[H|\\mathcal{C}]1_{G\\cap C}]=\\operatorname{E}[\\operatorname{P}[H|\\mathcal{C}]1_G1_C]\\\\\n  &=\\operatorname{E}[\\operatorname{E}[P[H|\\mathcal{C}]1_G|\\mathcal{C}]1_C]=\\operatorname{E}[\\operatorname{P}[H|\\mathcal{C}]P[G|\\mathcal{C}]1_C]\\\\\n  &=\\operatorname{E}[\\operatorname{P}[H\\cap G|\\mathcal{C}]1_C]=\\operatorname{E}[\\operatorname{E}[1_{H\\cap G}1_C|\\mathcal{C}]]\\\\\n  &=\\operatorname{E}[1_H1_{G\\cap C}].\n\\end{align*}\n\\] が成り立つ．\\(G\\cap C\\) という形の集合は，\\(\\mathcal{G}\\lor\\mathcal{C}\\) を生成する集合体であるから，単調族定理より，\\(\\mathcal{G}\\lor\\mathcal{C}\\) の元は，\\(C\\cap G\\) という形の集合の単調増大列の極限として得られる．\nよって単調収束定理から， \\(\\operatorname{P}[H|\\mathcal{C}]\\overset{\\text{a.s.}}{=}\\operatorname{P}[H|\\mathcal{C}\\lor\\mathcal{G}]\\)．\n(2)\\(\\Rightarrow\\)(1)：任意の \\(G\\in\\mathcal{G},H\\in\\mathcal{H}\\) を取る． \\[\n\\begin{align*}\n  &\\operatorname{P}[G\\cap H|\\mathcal{C}]=\\operatorname{E}[1_{G\\cap H}|\\mathcal{C}]\\\\\n  &=\\operatorname{E}[\\operatorname{E}[1_G1_H|\\mathcal{G}\\lor\\mathcal{C}]|\\mathcal{C}]=\\operatorname{E}[1_G\\operatorname{E}[1_H|\\mathcal{G}\\lor\\mathcal{C}]|\\mathcal{C}]\\\\\n  &=\\operatorname{E}[1_G\\operatorname{E}[1_H|\\mathcal{C}]|\\mathcal{C}]=\\operatorname{E}[1_H|\\mathcal{C}]\\operatorname{E}[1_G|\\mathcal{C}]=\\operatorname{P}[H|\\mathcal{C}]\\operatorname{P}[G|\\mathcal{C}].\n\\end{align*}\n\\]\n\n\n\n\n\n\n3.2 条件付き分散\n\n\n\n\n\n\n命題（Pythagorasの式）\n\n\n\n\\[\n\\|Y\\|^2_2=\\|Y-\\operatorname{E}[Y|\\mathcal{G}]\\|^2_2+\\|\\operatorname{E}[Y|\\mathcal{G}]\\|^2_2.\n\\]\n\n\nこれは条件付き期待値が \\(L^2(\\Omega)\\)-射影であるためである（ 節 1.3 ）．\n確率変数 \\(Y\\in\\mathcal{L}^2(\\Omega)\\) の \\(\\mathcal{G}\\) に関する条件付き分散を \\[\n\\begin{align*}\n    \\mathrm{V}[Y|\\mathcal{G}]&:=\\operatorname{E}\\left[(Y-\\operatorname{E}[Y|\\mathcal{G}])^2|\\mathcal{G}\\right]\\\\\n    &=\\operatorname{E}[Y^2|\\mathcal{G}]-\\operatorname{E}[Y|\\mathcal{G}]^2\n\\end{align*}\n\\] と定める．このとき，次の 全分散の公式 と呼ばれる関係が成り立つ： \\[\n\\mathrm{V}[Y]=\\operatorname{E}[\\mathrm{V}[Y|\\mathcal{G}]]+\\mathrm{V}[\\operatorname{E}[Y|\\mathcal{G}]].\n\\]\n\n\n\n\n\n\n説明\n\n\n\n\n\nPythagorasの関係から， \\[\n\\begin{align*}\n    \\operatorname{E}[Y^2]&=\\operatorname{E}[(Y-\\operatorname{E}[Y|\\mathcal{G}])^2]\\\\\n    &\\qquad+\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]^2].\n\\end{align*}\n\\] 両辺から \\[\n\\operatorname{E}[Y]^2=\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]]^2\n\\] を減じると，右辺第一項の \\(\\operatorname{E}[-]\\) の中身は中心化確率変数であることから， \\[\n\\begin{align*}\n    \\mathrm{V}[Y]&=\\mathrm{V}[Y-\\operatorname{E}[Y|\\mathcal{G}]]\\\\\n    &\\qquad+\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]^2]-\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]]^2\\\\\n    &=\\mathrm{V}[Y-\\operatorname{E}[Y|\\mathcal{G}]]+\\mathrm{V}[\\operatorname{E}[Y|\\mathcal{G}]].\n\\end{align*}\n\\] 最後に， \\[\n\\mathrm{V}[Y-\\operatorname{E}[Y|\\mathcal{G}]]=\\operatorname{E}[\\mathrm{V}[Y|\\mathcal{G}]]\n\\] より結論が従う．\n\n\n\n\n\n3.3 条件付き共分散\n\n\n\n\n\n\n定義（条件付き共分散）\n\n\n\n\\(X,Y\\in\\mathcal{L}^2(\\Omega)\\) の \\(\\mathcal{G}\\) に関する 条件付き共分散 を \\[\n\\begin{align*}\n    &\\mathrm{C}[X,Y|\\mathcal{G}]\\\\\n    &=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X|\\mathcal{G}])(Y-\\operatorname{E}[Y|\\mathcal{G}])\\bigg|\\mathcal{G}\\biggr]\\\\\n    &=\\operatorname{E}[XY|\\mathcal{G}]-\\operatorname{E}[X|\\mathcal{G}]\\operatorname{E}[Y|\\mathcal{G}].\n\\end{align*}\n\\] と定義する．\n\n\n\n\n\n\n\n\n命題（条件付き共分散公式）\n\n\n\n\\[\n\\begin{align*}\n    &\\mathrm{C}[X,Y]\\\\\n    &=\\operatorname{E}[\\mathrm{C}[X,Y|\\mathcal{G}]]+\\mathrm{C}[\\operatorname{E}[X|\\mathcal{G}],\\operatorname{E}[Y|\\mathcal{G}]].\n\\end{align*}\n\\]\n\n\n証明は (Kallenberg, 2021) 補題8.2 p.166 など．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#footnotes",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#footnotes",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Jacod & Shiryaev, 2003, p. 2) など．↩︎\n(Dudley, 2002, p. 340) など．↩︎\n(Kallenberg, 2021, p. 167)．↩︎\n(Kallenberg, 2021), (Dellacherie & Meyer, 1978)↩︎\n(Dudley, 2002, pp. 定理10.2.2 p.345)．一般には Borel空間に値を取る確率変数について成り立つ (Kallenberg, 2021, p. 165)．↩︎\n(Kallenberg, 2021, pp. 170–171) 定理8.9 も参照．↩︎"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html",
    "href": "posts/2023/Probability/Beta-Gamma.html",
    "title": "確率測度の変換則 | Gamma分布とBeta分布を例に",
    "section": "",
    "text": "Gamma確率変数と，その変換として得るBeta確率変数とに関する次の命題の証明を与える．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#gamma分布を見る",
    "href": "posts/2023/Probability/Beta-Gamma.html#gamma分布を見る",
    "title": "確率測度の変換則 | Gamma分布とBeta分布を例に",
    "section": "1 Gamma分布を見る",
    "text": "1 Gamma分布を見る\n\n1.1 定義\n\n\n\n\n\n\n定義（Gamma分布）\n\n\n\n可測空間 \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) 上の Gamma分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\;(\\alpha,\\nu&gt;0)\\) とは， 密度関数 \\[g(x;\\alpha,\\nu):=\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}1_{\\left\\{x&gt;0\\right\\}}\\] が定める分布をいう．実際，\\(t=\\alpha x\\) と変数変換すると， \\[\n\\begin{align*}\n&\\quad\\int_0^\\infty \\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}dx\\\\\n&=\\alpha^\\nu\\int^\\infty_0\\left(\\frac{t}{\\alpha}\\right)^{\\nu-1}e^{-t}\\frac{dt}{\\alpha}\\\\&=\\int^\\infty_0t^{\\nu-1}e^{-t}dt=\\Gamma(\\nu).\\end{align*}\\]\n\n\n\n\n1.2 形状\n\\(\\alpha\\) をレートパラメータ（スケールパラメータと呼ばれるものの逆数），\\(\\nu\\) を形状パラメータともいう．レートパラメータが大きいほど突起も大きく，手前に寄る．形状パラメータ \\(\\nu\\) は分布の形状を大きく司る．実際，先度と歪度は形状パラメータのみに依って \\[\\gamma_1=\\frac{2}{\\sqrt{\\nu}},\\qquad\\gamma_2=3+\\frac{6}{\\nu},\\] と記述される． その意味するところを感得するために，scipy.statsでの実装を用いてプロットしてみる．\n\n\nコードを表示\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gamma\n\nnu = 1.5  # 形状パラメーター\n\n# Gamma分布のPDFをグリッド上で計算\nx = np.linspace(0, 8, 100)\npdf = gamma.pdf(x, nu)\n\n# プロットの実行\nplt.figure(figsize=(3.2, 4.8)) # スマホサイズに合わせる\nplt.plot(x, pdf)\nplt.title('Gamma(1,3/2) Distribution')\nplt.ylabel('Density')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\n\n\nレートパラメータを固定し，形状パラメータを残した \\[\\chi^2(k):=\\mathrm{Gamma}(1/2,k/2)\\] を自由度 \\(k\\) のカイ自乗分布ということに注意．\n\n\n\n\n\n\n\n\n\n最後に，レートパラメータが大きいほど突起が大きくなる様子は次の通り：\n\n\n\n\n\n\n\n\n\nなお，形状パラメータが \\(\\nu=1\\) であるGamma分布のことを指数分布という： \\[\n\\mathop{\\mathrm{Exp}}(\\gamma):=\\mathrm{Gamma}(\\gamma,1)\\;(\\gamma&gt;0)\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#beta分布を見る",
    "href": "posts/2023/Probability/Beta-Gamma.html#beta分布を見る",
    "title": "確率測度の変換則 | Gamma分布とBeta分布を例に",
    "section": "2 Beta分布を見る",
    "text": "2 Beta分布を見る\n\n2.1 定義\n\n\n\n\n\n\n定義（Beta分布）\n\n\n\n可測空間 \\(((0,1),\\mathcal{B}((0,1)))\\)上の （第１種）ベータ分布 \\(\\mathrm{Beta}(\\alpha,\\beta)\\;(\\alpha,\\beta&gt;0)\\) とは， 密度関数 \\[\\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1_{(0,1)}(x)\\] が定める分布をいう．ただし，\\[B(\\alpha,\\beta)=\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}\\,dx.\\]\n\n\n\n\n2.2 形状\n次のような性質を持つ：1\n\n\\(\\alpha_1=\\alpha_2=1\\) のとき一様分布となり，\\(\\alpha_1=\\alpha_2&gt;1\\) の場合に左右対称な単峰性分布，\\(\\alpha_1=\\alpha_2&lt;1\\) の場合に左右対称なU字型の二峰性分布を得る．\nいずれも \\(1\\) より大きい場合，左のパラメータが大きい場合 \\(\\alpha_1&gt;\\alpha_2&gt;1\\) 左に，右のパラメータが大きい場合 \\(\\alpha_2&gt;\\alpha_1&gt;1\\) 右に歪んだ単峰性分布を得る．\nいずれも \\(1\\) より小さい場合はその逆．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#証明",
    "href": "posts/2023/Probability/Beta-Gamma.html#証明",
    "title": "確率測度の変換則 | Gamma分布とBeta分布を例に",
    "section": "3 証明",
    "text": "3 証明\n\n\n\n\n\n\n証明\n\n\n\n\\[\\begin{cases}\n    X_1=\\frac{Y_1}{Y_1+Y_2},\\\\\n    X_2=Y_1+Y_2.\n\\end{cases}\\] を逆に解くことで， \\[\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}=\\begin{pmatrix}x_1x_2\\\\x_2\\end{pmatrix}=:T(x_1,x_2)\\] を得る．\\(A:=(0,1)\\times(0,\\infty),B:=(0,\\infty)^2\\) と定めると，\\(T:A\\to B\\) は可微分同相で，Jacobianは \\[DT=\\begin{pmatrix}x_2&x_1\\\\0&1\\end{pmatrix},\\qquad J_T=x_2,\\] と計算でき，\\(A\\) 上で は消えない．\nよって \\((X_1,X_2)\\) の結合分布は \\[\n\\begin{align*}\n    &p(T(x_1,x_2))J_T(x_1,x_2)dx_1dx_2\\\\\n    &=\\frac{\\alpha^{\\nu_1}}{\\Gamma(\\nu_1)}y_1^{\\nu_1-1}e^{-\\alpha y_1}\\frac{\\alpha^{\\nu_2}}{\\Gamma(\\nu_2)}y_2^{\\nu_2-1}e^{-\\alpha y_2}\\\\\n    &\\qquad\\times\\frac{x_2}{(1-x_1)^2}\\,dx_1dx_2\\\\\n    &=\\frac{\\alpha^{\\nu_1}}{\\Gamma(\\nu_1)}x_1^{\\nu_1-1}x_2^{\\nu_1-1}e^{-\\alpha x_1x_2}\\frac{\\alpha^{\\nu_2}}{\\Gamma(\\nu_2)}x_2^{\\nu_2-1}\\\\\n    &\\qquad\\times(1-x_1)^{\\nu_2-1}e^{-\\alpha x_2(1-x_1)}x_2\\,dx_1dx_2\\\\\n    &=\\underbrace{\\frac{\\Gamma(\\nu_1+\\nu_2)}{\\Gamma(\\nu_1)\\Gamma(\\nu_2)}}_{=B(\\nu_1,\\nu_2)^{-1}}x_1^{\\nu_1-1}(1-x_1)^{\\nu_2-1}\\,dx_1\\\\\n    &\\qquad\\times\\frac{\\alpha^{\\nu_1+\\nu_2}}{\\Gamma(\\nu_1+\\nu_2)}x_2^{(\\nu_1+\\nu_2)-1}e^{-\\alpha x_2}\\,dx_2.\n\\end{align*}\n\\] これは \\(X_1\\) が \\(\\mathrm{Beta}(\\nu_1,\\nu_2)\\) に，\\(X_2\\) が \\(\\mathrm{Gamma}(\\alpha,\\nu_1+\\nu_2)\\) に独立に従った場合の密度になっている．\n\n\n\n3.1 余談\n総合研究大学院大学統計科学コース2018年8月実施の入試問題の第三問にて，本命題を背景とした問題が出題された．2\n\n\n\n\n\n\n第３問\n\n\n\n\n数直線 \\(\\mathbb{R}\\) 上の点Pの \\(x\\) 座標 \\(X\\) は \\(\\mathop{\\mathrm{N}}(0,1)\\) に従うとする． Pの原点からの距離の自乗の確率密度関数が \\[\\frac{1}{\\sqrt{2\\pi x}}e^{-\\frac{x}{2}},\\qquad(x&gt;0)\\] であることを示せ．\nEuclid空間 \\(\\mathbb{R}^n\\) 内の点Qの座標 \\((X_1,\\cdots,X_n)\\) は \\(\\mathop{\\mathrm{N}}_n(0,I_n)\\) に従うとする． Qの原点からの距離の自乗の確率密度関数が \\[\\frac{1}{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}x^{\\frac{n}{2}-1}e^{-\\frac{x}{2}},\\qquad(x&gt;0)\\] であることを示せ．\n(2)の確率密度関数を持つ分布を \\(\\chi^2(n)\\) という． 確率変数 \\(X,Y\\) は独立で \\(X\\sim\\chi^2(n),Y\\sim\\chi^2(m)\\) であるとする．このとき， \\[X+Y\\sim\\chi^2(n+m),\\] \\[\\frac{X}{X+Y}\\sim\\mathrm{Beta}(n/2,m/2),\\] であり，互いに独立であることを示せ．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-transform",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-transform",
    "title": "確率測度の変換則 | Gamma分布とBeta分布を例に",
    "section": "4 確率分布の変換則",
    "text": "4 確率分布の変換則\n\\(A,B\\subset\\mathbb{R}^d\\) を連結開集合，\\(C^1\\)-微分同相 \\(T:A\\overset{\\sim}{\\to}B\\) に対して，3 \\(B\\) 上の分布 \\(\\pi\\in\\mathcal{P}(B)\\) の \\(T\\) による引き戻し \\(T^*\\pi\\) の密度 \\(p^*\\) が，\\(\\pi\\) の密度 \\(p\\) と Jacobian \\(J_T(x)\\) の絶対値との積になる： \\[\np^*(x)=p(T(x))\\lvert J_T(x)\\rvert\\;\\;\\text{a.s.}\\quad(x\\in A).\n\\]\n\n\n\nCommutative diagram discribing current situations\n\n\n\n\n\n\n\n\n定理（変数変換）\n\n\n\n4 \\(A,B\\subset\\mathbb{R}^d\\) を連結開集合，\\(T:A\\overset{\\sim}{\\to}B\\) を \\(C^1\\)-微分同相，\\(f:B\\to\\mathbb{R}\\) を Lebesgue 可測関数とする．\n\n\\(f\\circ T:A\\to\\mathbb{R}\\) も Lebesgue 可測．\n\\(f\\) は非負関数とする．このとき， \\[\n\\begin{align*}\n\\int_Af(T(x))\\lvert J_T(x)\\rvert\\,dx=\\int_Bf(y)\\,dy.\n\\end{align*}\n\\]\n\n\n\nこの定理より，任意の可測集合 \\(A_0\\in\\mathcal{B}(A)\\) に対して， \\[\n\\begin{align*}\n    \\int_{A_0}p^*(x)\\,dx&=(T^*\\pi)[A_0]\\\\\n    &=\\int_{T(A_0)}p(y)\\,dy\\\\\n    &=\\int_{A_0}p(T(x))\\lvert J_T(x)\\rvert\\,dx.\n\\end{align*}\n\\] ただし，最後の等号は定理による．5 \\(A_0\\in\\mathcal{B}(A)\\) は任意だったから， \\[\np^*(x)=p(T(x))\\lvert J_T(x)\\rvert\\;\\;\\text{a.s.}\\quad(x\\in A).\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#footnotes",
    "href": "posts/2023/Probability/Beta-Gamma.html#footnotes",
    "title": "確率測度の変換則 | Gamma分布とBeta分布を例に",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Agresti, 2012) 第1.6.2節 Binomial Estimation: Beta and Logit-Normal Prior Distributions p.24 参照．↩︎\n過去9年分の入試問題の解答はこちらから↩︎\n記法 \\(\\overset{\\sim}{\\to}\\) は 記法一覧 参照．↩︎\n会田先生講義ノート 定理5.1 など参照．↩︎\n最後から2番目の等号は，測度の押し出し \\(T^*\\pi\\) の定義である．↩︎"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2004.html",
    "href": "posts/2023/Articles/DelMoral2004.html",
    "title": "書籍紹介 Del Moral (2004) Feynman-Kac Formulae",
    "section": "",
    "text": "book cover"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2004.html#書籍紹介-del-moral-2004-feynman-kac-formulae",
    "href": "posts/2023/Articles/DelMoral2004.html#書籍紹介-del-moral-2004-feynman-kac-formulae",
    "title": "書籍紹介 Del Moral (2004) Feynman-Kac Formulae",
    "section": "書籍紹介 Del Moral (2004) Feynman-Kac Formulae",
    "text": "書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n前文と一部の内容が著者のHPからご覧になれます．\nFeynman-Kac Formulae\nこの本はFeynman-Kac道測度とその粒子法による解釈とその各種科学分野への応用を統一的に扱った初のモノグラフ．"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2004.html#内容",
    "href": "posts/2023/Articles/DelMoral2004.html#内容",
    "title": "書籍紹介 Del Moral (2004) Feynman-Kac Formulae",
    "section": "内容",
    "text": "内容\n\nPreface\n本書はFeynman-Kac path distribution, 相関粒子系，系統木モデルを扱う．生物学，物理学，確率統計学，工学，信号処理に渡る共通の話題である．21世紀に入ってやっと理論の形が見えてきたので，ここに教科書を書く．粒子法とFeynman-Kacモデル自体は，統計物理学，とりわけ気体分子運動論に起源を持つが，この本はその知識がなくても読めるようになっている．確率過程の素養がある学部生，工学・統計学・生物学・物理学の大学院生が対象読者である．\nまた，Feynman-Kacモデルと粒子法の漸近理論の研究に必要な数学，経験過程論，大偏差解析，半群・マルチンゲール理論，カオスの伝播，測度値過程の集中，関数不等式，エルゴード係数，Markov作用素の引き戻し，非線形半群など，の参考書になるようにも用意した．\nまた本書には種々のFeynman-Kac distribution flow, 相関粒子モデルの例の宝庫になっている．制限Markov連鎖シミュレーション，吸収媒体内のランダム粒子，Schrödinger作用素とFeynman-Kac半群のスペクトル解析，稀事象解析，Dirichlet境界問題，非線型フィルタリング問題，相関Kalman-Bucyフィルタ，方向つきポリマーシミュレーション，相関Metropolisアルゴリズムなど，種々のモデルの粒子近似と収束解析の例が含まれている．\nこれほど物理学・工学・数学にまたがるトピックを書籍化できたことに感謝しかない．本書を書いた理由は，まず第一にFeynman-Kac path modelとその粒子近似を扱う書籍は皆無だったからである．しかし同時に，この理論は現代のBayes統計学，工学，物理学，生物学で用いられるモンテカルロ法の解析に共通の土台を提供する，極めて有用な理論である．さらに，オペレータ記法に対する恐怖心さえ払拭して貰えば，非線型問題や相関粒子近似の研究に，強力な道具となること間違いなしである．\n第1と12章では連続なモデルも扱っているが，離散時間のFeynman-Kacモデルに集中した．その理由は第一に，離散時間ならばMarkov過程の前提知識をほとんど用いないためである．これは本書の「なるべくself-containedな教科書を書く」という目的に合致している．第二に，連続な場合の漸近解析は，ほとんど離散の場合と同じ道を辿るからである．一方で多くのopen problemも残るが．\n\n\nSec1.1 On the Origins of Feynman-Kac and Particle Models\n\n\n\n\n\n\nポイント\n\n\n\nWienerの道積分は，量子の運動を確率測度の言葉で書いた．FeynmanはSchrödinger方程式と繋げて，ポテンシャルを与えたモデルにおいて，当該ポテンシャルが定める「確率測度の変換則」を与えた．これがFeynman-Kacの公式である． Feynman-Kac測度は，生きたり死んだりする粒子系の見本道の分布や，一般に淘汰圧や相互作用にさらされる粒子の分布のモデリングに広く有用であることが判りつつある． 尤度，淘汰圧，相互作用が「ポテンシャル」という一つの枠組みで捉えられるのである．ポテンシャルを尤度とすると，特定の粒子に淘汰圧をかけることで，効率的に探索することに使える．\n\n\nFeynman-Kac公式はFeynmanの1942年の博士論文に起源を持つ．これはSchrödinger方程式とWienerの道積分とをヒューリスティックに繋いだ．この研究は1950年代のMark Kacの研究に受け継がれることになる．ポテンシャルに従って運動する量子の半群を，汎函数の道積分の言葉で記述する，というアイデアである．直感的には，Feynman-Kac測度は粒子の経路の分布に，ポテンシャルの効果を組み込むということである．\nこれは「ポテンシャル関数が定める確率測度の変換」であるが，この発想が多くの数理物理，確率過程の分野の研究の方向性を決定づけた．そして今日，このモデルは多くの現象のモデリングに有用であることがわかっている．例えば物理学で，吸収的で非規則的な媒質内での単一粒子の見本道の分布を記述することが出来る．このモデルでは，ポテンシャル関数というのは，「死亡／生成率」を表している．\nより一般的に，物理化学における有向ポリマーなどの物理・生物学的存在のBoltzmann-Gibbs分布ともみなせる．この例では，ポテンシャル関数というのは，ハミルトニアンや，ともかく相互作用のエネルギー関数や淘汰圧に相当するものになる．さらに工学や統計学者の文脈では，ポテンシャル関数は，特定の観察過程に対する変数の条件付き確率（＝尤度）を表すことになる．この見方はフィルタリング問題やBayes解析をはじめとし，信号処理の分野で広く使われている．ともかく，ポテンシャルとは，観測過程や，参照道に対する，状態変数の尤度と同一視されるのである．\n確率的粒子算譜は，Monte Carlo法の一種である．その源は，確率を頻度として捉えたBernoulliの基礎づけから見られる．そこから現代の確率論の発展に至るまでの大きな一歩は，1920年代のMarkovによる「確率過程」という対象の創出である．Markov過程という概念は，種々の工学・自然科学的対象を，自然な形でモデリングするための最適な語彙を与えた．さらに，粒子法の，他の数値解放にない美点は，工学や自然科学が与える発展方程式に対して，「微視的な粒子解釈を与える」という点である．他にも，モデルの係数に正則性の仮定を必要としないこと，大規模モデルにも使えることなど，美点は尽きない．1さらに現在発見されつつあるもう一つの魅力として，分布の空間上で非線型方程式が数値的に解ける，という方面での応用である．これらの分布モデルの非線型な構造は，その粒子近似版のモデルに，自然な相互作用と分岐のメカニズムを課す．この近年の応用は，1960年代の流体力学と統計物理の発展に源を発する．この方面については，McKeanの開拓的仕事を参照すると良い．\nこの相関粒子法を工学や，とりわけ信号処理の分野に使うという応用は，さらに最近になってのことである．この方面での最初の厳密な研究は，1996年の非線型推定問題への粒子法の応用であるように思われる．この研究は，1990年台に初めて提案された新たな種の相関粒子モデルに対して，初めて厳密な収束の結果を与えた．同様の研究が4つ追随し，この種の相関粒子モデルが，大規模かつ非線型な測度値過程を数値的に解く手法として優れていることが判明した．同時期に，別の粒子分岐過程が，連続時間のフィルタリング問題を解く手法として独立に提案された．このときの手法は，現在でも非線型平滑化や道推定問題に使われている系統木粒子モデルの漸近解析に，ほとんどそのまま使えることが判明した（Del Moral and Miclo (2001) Genealogies and Increasing Propagation of Chaos for Feynman-Kac and Genetics Model）．\n本書の要点を掴むために，最初の例を与える．Singer modelと呼ばれており，レーダーのモデルである．3次元のMarkov過程 \\(X_n=(X_n^{(1)},X_n^{(2)},X_n^{(3)})\\) を考え，それぞれ加速度，速度，位置を表し，次のように発展するとする：\n\\[\n\\begin{cases}X_n^{(1)}=X_{n-1}^{(1)}+\\epsilon_nW_n\\\\X_n^{(2)}=(1-\\alpha\\Delta)X_{n-1}^{(2)}+\\beta\\Delta X_n^{(1)}\\\\X_n^{(3)}=X_{n-1}^{(3)}+\\Delta X_n^{(2)}\\end{cases}\n\\]\n\\(\\Delta\\in(0,1),\\alpha,\\beta\\in\\mathbb{R}\\) はサンプリング頻度とパラメータとする． \\(\\epsilon_n\\in2\\) はBernoulli確率変数， \\(W_n\\sim\\mathrm{U}([0,a])\\) などとモデリングしよう． \\(X_n\\) の観測は，次のように部分的になされる：\n\\[\nY_n=X_n^{(3)}+\\Delta V_n.\n\\]\nこの状態で， \\(X_0,\\cdots,X_n|Y_0,\\cdots,X_n\\) の分布を推定する問題を，非線型フィルタリング問題という．この「パスの分布」に対する粒子近似は，次のようにして与えられる．まず， \\(X_0\\) から \\(N\\) 個サンプリングして粒子とする： \\(X_0\\sim X_{0}^i\\)．次に，始めに定めたMarkov遷移確率に従って，発展させて，見本道 \\(X_{t_0,t_1}^i=(X_0^i,\\cdots,X_{t_i}^i)\\) を得る．それぞれの見本道の尤度は\n\\[\nW_{t_0,t_1}^i=\\exp\\left(-\\frac{1}{2}\\sum_{t_0\\le p&lt;t_1}(Y_p-X_p^{(3),i})^2\\right)\n\\]\nで与えられる．これは， \\([0,1]\\) の値で，与えられた見本道 \\(X_{t_0,t_1}^i\\) が「尤もらしいか」の度合いを定量評価していると見れる．この情報を取り入れて，配置 \\(\\{X_{t_1}^i\\}_{i=1}^N\\) を更新する必要があるが，そのやり方には様々ある．ここでは，現在の見本道 \\(\\{X_{t_0,t_1}^i\\}_{i=1}^N\\) の中から，分布\n\\[\nW_{t_0,t_1}^i\\delta_{X_{t_0,t_1}^i}+(1-W^i_{t_0,t_1})\\sum_{j=1}^N\\frac{W_{t_0,t_1}^j}{\\sum_{k=1}^NW_{t_0,t_1}^k}\\delta_{X_{t_0,t_1}^j}\n\\]\nでリサンプリングすることを考える．これは，尤度 \\(W_{t_0,t_1}^i\\) の確率でその見本道は残存させ，さもなくば，尤度の重み付けに従って他の見本道で置き換えてしまう，という確率的操作を施すということである．この更新ののち，新たに心機一転 \\(t_1(&gt;t_0=0)\\) から開始した見本道 \\(X_{t_1,t_2}^i=(X_{t_1}^i,\\cdots,X_{t_2}^i)\\) を得て，同じ操作を繰り返す．ただし， \\(X_{t_1}^i\\gets\\widehat{X}_{t_1}^i\\) として更新したものを用いる．\nこの操作をすると，各粒子は死亡・分岐を繰り返すように見える．同時に，遺伝的に効率的な探索を行えていることも分かる．尤度が高いところに自然に粒子が集中していくようにできているのである．\nこの例を見て，最も基本的な疑問は「この手法に理論的保証がつくだろうか？」という点になる． \\(N\\) の増加に対して，収束のスピードはどう速まっていくだろうか？この手法を他の最適化やシミュレーションに応用できないか？また，この遺伝的アルゴリズムを死亡と繁殖の過程と見たとき，その系統木について何が言えるだろうか？\nこのような漸近解析の中心的なアイデアは，目的の条件付き分布 \\(Y_1,\\cdots,Y_n|X_1,\\cdots,X_n\\) を，離散生成モデルとFeynman-Kac粒子近似モデルと関連づけることである．すると，系統木モデルの占有測度が，目的の条件付き分布 \\(Y_1,\\cdots,Y_n|X_1,\\cdots,X_n\\) に収束することを示せるのである！また，現在残存している個体の祖先の系列を， \\(Y_1,\\cdots,Y_n|X_1,\\cdots,X_n\\) の経路の近似的に独立なサンプルの集まりと近似的にみなせる．\nこの「うまくいっている個体を複製することで，状態空間を効率的に探索する」というのは，多くの確率的探索アルゴリズムの基本的な態度である．このアイデアは，どうやら1950年代の生物学での Rosenbluth の貢献と，物理学の Kahn and Harris の貢献とに端を発するようである．\n一般の距離空間上のFeynman-Kacモデルと粒子法の研究は，Del Moralに20世紀の終わりから21世紀の初めにかけて行われた．この手法の応用は大きく広がっており，Doucet (2001) では多くの応用が紹介されているが，これは物理学・数学的な側面から離陸しつつあることは残念なことである．\nこれらの発展は全て，古典的な遺伝的アルゴリズムのトピックと深く強い関連を持つ．遺伝的アルゴリズムは Holland (1975) によって始められ，それ以降大域的最適化の数値解法に広く用いられている．このアルゴリズムの収束解析は R. Cerf によって1994年から始められ， Del Moral and Miclo (1999) On the Convergence and Applications of the Generalized Simulated Annealing で洗練された．大偏差解析と対数ソボレフ不等式を取り入れた半群の方法によって，遺伝的アルゴリズムの集中性が示された．"
  },
  {
    "objectID": "posts/2023/Articles/DelMoral2004.html#footnotes",
    "href": "posts/2023/Articles/DelMoral2004.html#footnotes",
    "title": "書籍紹介 Del Moral (2004) Feynman-Kac Formulae",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFeynman Lecturesの第1章第2節で，「現代科学最大の発見を1つ挙げるとすると原子論だ」と言っている．粒子法によるモデリングは，人間が理解しやすいモデリングの究極形の1つであるかもしれない．↩︎"
  },
  {
    "objectID": "posts/2023/Articles/Wong+2023.html",
    "href": "posts/2023/Articles/Wong+2023.html",
    "title": "Serotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する",
    "section": "",
    "text": "Wong+ (2023) Serotonin reduction in post-acute sequelae of viral infection\n\n\n\n\nimg\n\n\nに衝撃の事実が書かれていました．アブストラクトのみから内容を概略すると，\n\nウイルスが腸管で持続的な感染を起こした場合は、トリプトファンの吸収が免疫活動により阻害されるようで、血液内のセロトニンが減少し、その状態が急性症状が落ち着いた後も戻らず、末梢の迷走神経に機能障害を起こし、認知機能への障害を自覚症状とするブレインフォッグを催す、という作用機序を示唆するデータを示している。ただし全てマウスでのデータ。\nブレインフォッグにSSRI（抗うつ剤だがパニック障害などにも適応あり）やトリプトファンサプリが有効であることを実証。\n以上の機序はあらゆるウイルスによる腸炎で起こるはずだが、SARS-Cov2は腸管感染と腸への持続感染を特に起こしやすく、社会問題化しやすかったという背景がある。\n\n現実とは，一体なんと複雑なのでしょうか．しかし，その一端を掴みつつあることが，とても喜ばしく感じます．\n筆者は過去に家庭教師先の教え子が，コロナウイルスワクチンの接種を機に深刻なブレインフォッグを催し，大学受験を１年遅らせる決断に至るまでを見届けたことがあります．当時はどう調べようにも，情報が限られていました．\nそこから約２年が経ち，その病理に始まり，「コロナに限らないということ」「ブレインフォッグも治るということ」が判ったのは大きな進歩だと感じられます．\nブレインフォッグに限らず，自律神経様の症状（めまいがする，眠れない，集中できない，頭がぼうっとする）は，多くの神経的疾患に付随する症状で，原因特定が難しく（病の原因と思われがちだが結果の場合が多い），周りからの理解が得にくいことが多いです．\nそれぞれの症例に対して，上述のような生理学的な要因が明らかになり，必ずしも心因性ではないこと（ましてや「気のせい」や「病は気から」などとんでもないこと）が周知されることは，このような現代特有の病気をも，現代が再包摂する第一歩になると感じます．\nメンタルヘルスについて"
  },
  {
    "objectID": "posts/2023/Lifestyle/AboutGit.html",
    "href": "posts/2023/Lifestyle/AboutGit.html",
    "title": "Git覚書",
    "section": "",
    "text": "Gitとは，.gitに「スナップショットのストリーム」を保存する形で動く分散型（全てのユーザーがデータベース＝リポジトリのミラーリングを持つ形）のバージョン管理システムである．ここに独自の形式でsnapshotが保存される．\n\n\n\nWorkflow\n\n\n\n\n\nGitで2つのブランチを統合する方法は3つある．なお，git pullはgit fetchとgit merge（またはgit rebase）の合成である．\n\n\n2つのブランチに違いがあると言っても，「リモートのmainブランチがローカルのmainブランチよりも進んでいる」などの単純な状況である場合は，ただブランチのポインターを「前に進める」だけで良い．\n基本，一人で開発しており，別の環境から作業を再開したい場合や，レポジトリが単一のブランチのみで開発が進んでいる場合などはこれで十分．\ngit fast-forwardというコマンドがある訳ではなく，ブランチに分岐がない場合にgit mergeを実行した際に自動的に起こる．\n\n\n\nコミットを「基底状態(base)からの差分」だとすれば，このコミットを，別のブランチに取り込んで足並みを揃える際にはrebaseが行われる．\n他のブランチに適用したいところのコミットが存在するブランチに移動し，適用したいブランチ（例えばtargetブランチ）に対して，\ngit rebase target\nと実行する．ローカルでrebaseの引数を省略した場合はrefs/heads/mainに適用される．\n\n\n\n2つのブランチの変更を組み合わせて，1つの新しいブランチにまとめる．\ngit merge\nにより自動で統合を試みるが，失敗した場合は手動で解決する必要あり．\n\n\n\n\n\nサル先生のGit入門"
  },
  {
    "objectID": "posts/2023/Lifestyle/AboutGit.html#導入",
    "href": "posts/2023/Lifestyle/AboutGit.html#導入",
    "title": "Git覚書",
    "section": "",
    "text": "Gitとは，.gitに「スナップショットのストリーム」を保存する形で動く分散型（全てのユーザーがデータベース＝リポジトリのミラーリングを持つ形）のバージョン管理システムである．ここに独自の形式でsnapshotが保存される．\n\n\n\nWorkflow\n\n\n\n\n\nGitで2つのブランチを統合する方法は3つある．なお，git pullはgit fetchとgit merge（またはgit rebase）の合成である．\n\n\n2つのブランチに違いがあると言っても，「リモートのmainブランチがローカルのmainブランチよりも進んでいる」などの単純な状況である場合は，ただブランチのポインターを「前に進める」だけで良い．\n基本，一人で開発しており，別の環境から作業を再開したい場合や，レポジトリが単一のブランチのみで開発が進んでいる場合などはこれで十分．\ngit fast-forwardというコマンドがある訳ではなく，ブランチに分岐がない場合にgit mergeを実行した際に自動的に起こる．\n\n\n\nコミットを「基底状態(base)からの差分」だとすれば，このコミットを，別のブランチに取り込んで足並みを揃える際にはrebaseが行われる．\n他のブランチに適用したいところのコミットが存在するブランチに移動し，適用したいブランチ（例えばtargetブランチ）に対して，\ngit rebase target\nと実行する．ローカルでrebaseの引数を省略した場合はrefs/heads/mainに適用される．\n\n\n\n2つのブランチの変更を組み合わせて，1つの新しいブランチにまとめる．\ngit merge\nにより自動で統合を試みるが，失敗した場合は手動で解決する必要あり．\n\n\n\n\n\nサル先生のGit入門"
  },
  {
    "objectID": "posts/2023/Lifestyle/AboutGit.html#仕組み",
    "href": "posts/2023/Lifestyle/AboutGit.html#仕組み",
    "title": "Git覚書",
    "section": "2 仕組み",
    "text": "2 仕組み\n\n2.1 .gitディレクトリの中身\n\nHEAD： 現在チェックアウトしているブランチを指します。これは通常、最新のコミットを指すリファレンスです。\nconfig： プロジェクト固有のGit設定を含みます。これには、リモートリポジトリのURL、ブランチの設定などが含まれます。\ndescription： GitWebプログラムで使用される説明ファイルですが、一般的なGit操作では使用されません。\nhooks/： クライアント側またはサーバー側のフックスクリプトを格納するディレクトリです。これらのスクリプトは、特定のイベント（例えばコミットやプッシュ）が発生した際に自動的に実行されます。\ninfo/： .gitignoreファイルに追跡しないパターンをグローバルに除外するためのファイルを含みます。\nobjects/： すべてのコンテンツ（コミット、ツリー、ブロブなど）が格納されるディレクトリです。これらはGitによって一意のハッシュ値で識別されます。\nrefs/： コミットオブジェクトを指し示すポインター（ブランチ、タグ、リモートなど）を含むディレクトリです。\nindex： ステージングエリアの情報を格納するファイルです。これは、git add などのコマンドでステージングされた変更を追跡します。"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html",
    "href": "posts/2023/Lifestyle/QuartoBasics.html",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "",
    "text": "筆者はQuartoを，「TeXにような使用感で数式・コードが併存する文章を書き，RStudioのような使用感でコードの実行やプレゼンができる，等号開発環境」と理解した． 前述のTeX, RStudioに慣れている人にとっては極めて低い限界コストで莫大な利益を得るだろう．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#デモページ",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#デモページ",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "1 デモページ",
    "text": "1 デモページ\n\n\n\n\n\n\nNote\n\n\n\nNote: The followings were pasted from the official documentation.1\n\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 4 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#使い方の概要",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#使い方の概要",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "2 使い方の概要",
    "text": "2 使い方の概要\nQuartoではこのようなNotebook-likeなドキュメントが，極めて簡単に＋凡ゆるフォーマットで作成できる． 特にVSCodeの拡張機能と組み合わせれば，RStudioのような隙のない統合開発環境が得られる．またVSCodeではビジュアルモードでの編集もサポートされており，Jupyter Notebookと全く同じ使用感で始められる．\n基本的な仕組みとして，自分で作成するのは .qmdファイルのみである．その後はquarto renderコマンドにより，コードブロックはJupyterによって処理され，全体はmarkdownに変換され，Pandocによってpdf, html, word など好きな形式に最終出力できる．\n拡張機能をオンにしたVSCodeではRun Cellボタンもあるので，ノートブック全体を毎度ビルドせずとも，コードブロックごとに実行して結果を見ることもできる．Ctrl+Enterで１行ごとに実行できる操作感はRStudioと同じである．\n各ファイルの冒頭にYAML blockを用意することで，ノートブックの詳細を調整できる（参照：HTML Options）．\n---\ntitle: \"Quarto Basics\"\nformat:\n  html:\n    code-fold: true\njupyter: python3\n---\n本文はmarkdown記法で書く．数式も使える： \\[\\mathrm{P}[|\\xi|&lt;t]\\le2e^{-\\frac{t^2}{2\\sigma^2}},\\qquad t&gt;0.\\]\nまた，コードブロックにもコメントアウトと接頭辞の組み合わせ#|を前につけることでYAMLで指示が出せる（参照：指示のリスト）．上のコードブロックには\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\nと追加されているために，出力された図にラベリングとキャプションが付いているのである．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#美点",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#美点",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "3 美点",
    "text": "3 美点\n\nレンダリングがとんでもなく速い．体感でTeXの10分の1である．\nそれでいて数式とコードブロックを併在させることが出来る．なお，明かにTeXを意識していることがわかる使用感になっているし，本の作成も可能としている．\nローカル環境で動く．Jupyter Notebookが続かない筆者にとって，この点は肝要である．\n私用の勉強ノートとしても使えると同時に，内容そのままブログとして公開できる．\nプレゼンテーションにも使える．\nすごい細かいが，例えばproject typeをwebsiteとしたリポジトリでquarto renderをしても，不要なファイルが自動で削除される．このような点がライトユーザーでもとにかく使いやすい．\nさらにインタラクティブな機能を実現してみたい．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#website-hostingのやり方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#website-hostingのやり方",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "4 Website Hostingのやり方",
    "text": "4 Website Hostingのやり方\n公式Guideを参考．\n\n4.1 Source Branchをmainと別ける\nまずgh-pagesという全く新しいブランチを作成する．既存のリポジトリのコミット履歴とは独立している新しいブランチを作るときは--orphanオプションが利用される．\n\n\nTerminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\ngit checkout main\n\n基本gh-pagesブランチには自分では立ち入らない．\n\n\n4.2 Publishコマンドによるサイトの公開\nmainブランチにいることを確認して，\n\n\nTerminal\n\nquarto publish gh-pages\n\nを実行．\nGitHubの方の設定Settings: Pagesで，Sourceをgh-pagesブランチの/(root)にしていることを確認すれば，これで無事サイトが公開されていることが確認できる．\n\n\n4.3 GitHub Actionの使用\nさらに，ローカル上でrenderするのではなく，コミットする度にGitHub上でレンダリングしてもらえるように自動化することもできる．こうするとスマホからも自分のサイトが更新できる．\nまず，GitHubの設定のActionsセクションのWorkflow permissionsから，読み書きの権限をGitHub Actionに付与する．\n続いて，次の内容のファイルを.github/workflows/publish.ymlに書き込む：\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nこれで，mainブランチにコミットする度に，GitHub上でrenderが実行されることとなる．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#pdfの作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#pdfの作り方",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "5 PDFの作り方",
    "text": "5 PDFの作り方\nLuaLaTeXを利用することで日本語を含んだPDFを作成できる．\n\n\nreport.qmd\n\ntitle: \"タイトル\"\nauthor: Hirofumi Shiba\ndate: 2023/12/11\nformat:\n  pdf:\n    toc: true\n    number-sections: true\n    colorlinks: true\n    include-in-header: \n      - file: ../_preamble.tex\npdf-engine: lualatex\ndocumentclass: ltjsarticle"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#スライドの作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#スライドの作り方",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "6 スライドの作り方",
    "text": "6 スライドの作り方"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#footnotes",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#footnotes",
    "title": "Quartoはじめて良かった | Quarto Basics in Japanese",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is footnote. What a great feature!↩︎"
  },
  {
    "objectID": "posts/2023/Life/BookRecommendation.html",
    "href": "posts/2023/Life/BookRecommendation.html",
    "title": "Influential Books Which Paved My Path into Mathematics",
    "section": "",
    "text": "I am currently a Ph.D. candidate specializing in Bayesian Computation. I pursued my studies in Mathematics at the University of Tokyo, where I laid a solid foundation to study Statistical Inference for Stochastic Processes, a field renowned for its rigorous and mathematically demanding nature.\nInitially, I had not planed to major in Mathemacis when I embarked on my freshman year with a curious mind.1 However, it was the faculty of the Mathematics Department at the university, along with the books listed below, that awakened my intrinsic interest in Mathematics. In the remainder of this article, I will explore how these books inspired me and molded my style."
  },
  {
    "objectID": "posts/2023/Life/BookRecommendation.html#footnotes",
    "href": "posts/2023/Life/BookRecommendation.html#footnotes",
    "title": "Influential Books Which Paved My Path into Mathematics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt the University of Tokyo, students don’t choose their major until the middle of their sophomore year. In June of the second year, they submit their preferences and the department they will advance to is determined based on their GPA ranking.↩︎\nSome readers might question how a truly new set can be defined, given that the two allowed methods appear quite restrictive. However, this issue is addressed by the axiom that guarantees the existence of the power set \\(P(X)\\) from any existing set \\(X\\). Additionally, it might be helpful to note that the only initial set is the empty set \\(\\emptyset\\). From this, we can generate an infinite hierarchy of power sets starting with \\(\\emptyset\\), accompanied by enormous auxiliary sets constructed in the two aforementioned manners.↩︎\nAs some readers might guess this, the majority of attendees were actually 3rd and 4th-year math students. We sophomores lacked even basic tools from Topology. Surprisingly, my first introduction to topological concepts was through Sheaf Theory.↩︎\nTake, for instance, the theory of Markov Categories, which offers a dual perspective on Probability Theory.↩︎"
  },
  {
    "objectID": "posts/2023/Life/MentalHealth.html",
    "href": "posts/2023/Life/MentalHealth.html",
    "title": "About Mental Health Issues",
    "section": "",
    "text": "メンタルヘルスを損なってしまったとき，「もう二度と以前の状態には戻れないのではないか」という絶望が最初に付きまとうと思う．結論から言うと，絶対戻れる．だが，今じゃない．\nそもそも，メンタルヘルスの問題は「病気」と「健常」の境界が曖昧になってしまったと感じるだろう．治ったと思ったら治っていなくて，「一体いつまで続くのだろう」「以前は『正常』だと思っていた状態に，二度と戻れないのではないか？」という反芻思考が心を襲うだろう．\nそれは当然である．あなたはまだ治っていない．そもそも普通の風邪だって，「病気」と「健常」の境界など無いに等しい．熱があると社会は許してくれるが，熱とはそもそもウイルス・病原菌の感染から身体を守るための「正常な」反応である．身体に標準に組み込まれた防衛機制の想定された挙動の範囲内である．風邪は感染の可能性もあるから社会の方から休養を許してくれるのである．\nメンタルヘルスの問題だって，（一定範囲では）身体の正常な反応である．だが，あなたは休養を取っただろうか？社会は多くの場合理解を示さない．その中で自分の身体を守る対策を毅然と断行しただろうか？1 もし，休み休み前に進むことが出来る環境だったならば，あなたはメンタルヘルスに悩んでいなかっただろうし，「正常」と「異常」の境界にも悩んでいなかっただろう．食事が喉を通らない，眠れない，些細な刺激が絶大なストレスになる，胃を痛める，これらの反応は適度な休養が取れていて心に弾力がある状態ならば，「大変な時期もあったが，それを乗り越えて，私は大きく成功できた」という美談で終われる「正常」な心の反応である．風邪も流行感冒もそうだろう．だが，休まないまま通ろうとすると，本当に「異常」になる．風邪を治さずに活動し続けて，拗らせた経験はあるか？インフルになっても普段の生活を続けたことはあるか？あなたはそれをやろうとしていたのである．身体が想定外の挙動を起こしがちになるのも仕方ないというものである．\nだから，あなたはそもそも治っていないのである．風邪の原因はウイルスであり，あなたは休養をして免疫機構に対処してもらう，そうして来ただろう．一方で心の病の原因は，人間関係と期待，社会的なプレッシャーがある立場，休養を許さないストレスフルな環境，あなたの場合はどれに該当するかわからないが，ほとんどの場合すぐには休養が取れない．だからこそあなたはメンタルヘルスを病んでいるのだろう．つまり，普段通りだと思っているあなたの心の風邪はまだ絶賛発熱中である．それどころか，何ヶ月も発熱したままである．早い段階で自分の免疫に治してもらわなければ，薬などの外部からの補助が必要になる（それでも治るが）．\n「一体いつまで続くのだろう？」「もう二度と正常の状態に戻れないのではないか？」という不安が的外れであることがわかっていただけただろうか？まずはストレスの原因がない状態に生活を持っていき，心の免疫が働く状態を整えよう．これには時間がかかるだろう．信頼できる人以外との関係を一度整理する必要があるし，ほとんどの場合金策の問題ですぐには十分に休めないかもしれない．さらに悪いことに「ストレスのない状態に生活を変える」こと自体が，あなたのアイデンティティの死を意味するかも知れない．だからあなたはボロボロになってこれ以上前に進めなくなるまで頑張ってしまったのだろう．だが，メンタルヘルスに変調をきたしてしまった場合，そのアイデンティティは少し修正せざるを得ない．ここはどうしても残酷な部分であるが，仕方のないことである．だが，考えてみてほしい．身体を強く病んでしまった人で，ここに深く絶望する人は少ない．\nストレスの原因のない状態に持っていってから，それでも治らないならばしっかり専門家に頼り，通院して服薬をすれば，絶対に治るし，元の「正常」な状態に戻る．そのときには，あなたは「正常」と「病気」との区別について，より深い理解を得ていることだろう．私がそうだった．"
  },
  {
    "objectID": "posts/2023/Life/MentalHealth.html#本当にあなたの運が良かっただけではと思う人へ",
    "href": "posts/2023/Life/MentalHealth.html#本当にあなたの運が良かっただけではと思う人へ",
    "title": "About Mental Health Issues",
    "section": "本当に？あなたの運が良かっただけでは？と思う人へ",
    "text": "本当に？あなたの運が良かっただけでは？と思う人へ\n「もう二度と治らないのだ」「脳の構造が変わってしまう」などと言う体験者の言葉は意外と多い．だが，これは「自分はメンタルヘルスの大きな病を抱えており，休養が必要である」ということを周囲に理解してもらうには多少荒技が必要であった人が，自身の休養を守るために使う表現としてもよく使われることに注意していただきたい．メンタルヘルスの病は，「一刻も早く治したい」と思っている人が全てではないことに注意する必要がある．彼らの言動に，あなたが傷ついたり，絶望する必要はない．治したいと思っているならば，専門家に相談すれば治る．度合いによってかかる時間は変わるかもしれないが，あなたが満足のいくレベルまで，治る．今すぐクリニックを検索して予約の電話を入れよう．良いクリニックなら1ヶ月前後待つことになるから，とりあえず予約だけして後からそれで良かったのか考えれば良い．運悪く，合うクリニックがすぐには見つからない可能性もあるが，とにかくトライし続けるのだ．正しい手を掴めば絶対に治るから．"
  },
  {
    "objectID": "posts/2023/Life/MentalHealth.html#footnotes",
    "href": "posts/2023/Life/MentalHealth.html#footnotes",
    "title": "About Mental Health Issues",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n当然，「メンタルヘルスは自分で守らなきゃいけない」という自衛に頼った構造になっている現代社会は，少しずつ変わっていくし，変わるべきだろう．だがいつの時代も自衛は大事だ．↩︎"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html",
    "href": "posts/2023/Surveys/ParticleFilter.html",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#フィルタリング問題の歴史",
    "href": "posts/2023/Surveys/ParticleFilter.html#フィルタリング問題の歴史",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "1 フィルタリング問題の歴史",
    "text": "1 フィルタリング問題の歴史\n\n1.1 フィルタリング（濾波）問題とは何か？\nフィルタ（濾波器）の第一義は，液体から不純物を取り除くための装置である．そのアナロジーで「フィルタリング問題」と言った場合は，信号処理の意味で電圧や電波の信号を「濾過」してノイズを除去し本当に注目したい部分を純粋化する営みのことを指す．凡ゆる通信機器において装置の熱運動によるノイズが入ることは避けられぬ自然の摂理であり，フィルタリング問題は普遍的な課題である．1\nGauss は天体観測の経験から「誤差論」と最小二乗法を発明し，これが現代の統計的推定理論の先駆けとなった．これと同じように，通信と制御の分野では「時々刻々と受信するデータから時々刻々と変化する信号をどのようにうまく濾波するか」という独自の課題から，独自の理論が発展していった．2\n特に，デジタル回路がない時代では，「どのような電気回路のシステムとして濾波機をデザインすれば良いか？」という電気工学的な回路設計の問題としての側面も大きかった．\n\n\n1.2 最初のフィルタリング理論\nこのアナログフィルタの時代で，フィルタリングの問題を統計的技術で解くための理論3 が，まず離散時間の場合が (Kolmogorov, 1941)，続いて連続時間の場合が (Wiener, 1949) によって模索された．4\nしかし，この Kolmogorov と Wiener 理論では「信号とノイズの過程が定常である」という仮定の下で展開されており，この定常性の制約が Kolmogorov-Wiener 理論の広い応用を阻んでいた．\nだがこれは，「当時の技術（抵抗器やコンデンサーなど）で実装出来る範囲」という制約がある上で考えられた理論としては，仕方ないことでもあった．更なる理論的発展も，デジタル技術の登場を待つ必要があった．\n\n\n1.3 デジタルフィルタリングの登場\nトランジスタというデジタル技術が使われるようになり，集積回路の製造技術が発達すると，「フィルタ（濾波器）」はアナログデジタル変換器，レジスタ，メモリ，マイクロプロセッサから構成されたデジタルフィルタが主に使われるようになった．アナログフィルタから物理的な姿は全く変わり，もはや肉眼では見えない装置になってしまったのである．5\nその中で (Kalman, 1960) が，定常性の仮定が満たされない場合でも使えるアルゴリズムである カルマンフィルター を提案すると，すぐに Apollo 計画に導入されてスペースシャトルの制御へ実用化され，更には海軍の潜水艦などにも応用されていった．6\nあらゆるシステムがデジタル化されていく中で，アナログフィルタは徹底的にデジタルフィルタに代替されるようになった．これに伴い，現代でフィルタリング問題と言った場合，現在 \\(t\\) までの観測 \\(Y_1,\\cdots,Y_t\\) からノイズを除去してメッセージ部分 \\(X_t\\) をなるべく正確に推定するアルゴリズム という完全に数学的で抽象的な存在として研究が進められていくことになる．\n\n\n1.4 状態空間モデルという語彙\nKalman の論文 (Kalman, 1960) の新規性はアルゴリズムだけではなく，状態空間モデル という枠組みを導入し，どのようなシステムに適用可能かに関する共通言語を提供したことが，即時的な応用に寄与した面もあるだろう．7 例えばこの語彙に従えば，「Kalman filter は状態空間モデルが線型正規であれば最適なフィルタリング手法」ということになる．\nこの状態空間モデルの枠組みと同様なものが (Baum & Petrie, 1966) によって隠れ Markov モデルとして展開され，こちらは (Baker, 1975) により音声認識に応用された．現代でも多くの音声認識システムは隠れ Markov モデルに基づく．8\n状態空間モデルはその後，(Akaike, 1974) で時系列モデリングに応用され，再び統計学の分野と深く関わるようになる．9\n\n\n1.5 カルマンフィルターの限界\nしかし Kalman filter にも大きな制約があった．それは モデルに線型かつ正規であるという大きな制約があった ということである．一方で現実のシステムは殆どが非線型性を持つ．\nそこで NASA の Ames 研究センター ではすぐに 拡張 Kalman フィルタ と共分散行列の二乗根を保持する実装が考案され，これが本当の意味で Kalman フィルターを実用に耐えるものにした (McGee & Schmidt, 1985)．10\nだが，「拡張」の名前の通り本質的な解決とは言えず，システムの非線型性が強い場合は性能が伸びない．11 こうして，計算機や情報通信技術の発展と共に複雑化していくシステムに併せて，様々なフィルターが考案されていく必要があるのである．統計計算の時代の黎明である．\n\n\n1.6 非線型性・非正規性という強敵\n実は，1960年に開発された Kalman filter を真に超克する手法は，1990年代に入るのを待つ必要がある．他のシミュレーションに基づく ベイズ計算手法 と同じく，計算機の十分な発達を待つ必要があったのである．\nというのも，Kalman filter は，線型正規状態空間モデルの下でフィルタリング分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\n\\tag{1}\\] は再び正規分布であり，平均と共分散という２つの量のみで完全に特徴付けられるため，これらのみを考慮し，微分方程式を解いて更新規則を事前に得ておくことで，計算量を大幅に削減することが出来る，というトリックに基づく．\nこのように線型性と正規性が同在する状況，または状態空間が有限であるなどの限られた状況でない限り，フィルタリング分布 式 1 は有限次元の十分統計量を持たない．12\n従って，一般の状況に対応出来るフィルタには，上述のような計算量削減のトリックは絶対に存在せず，正面から分布 式 1 を近似する方法を考える必要がある．これには新しいアイデアが必要であると同時に，一定の性能を持つ計算機の出現を待つ必要もあったのである．\n\n\n1.7 種々の近似戦略\nフィルタリング分布 式 1 \\[\\mathbb{P}_t(X_t\\in dx_t):=\\mathcal{L}[X_t|Y_{0:t}]\\] は Bayes の定理を通じて再帰的な関係 \\[\n\\begin{align*}\n    &\\mathbb{P}_{t-1}(X_t\\in dx_t|Y_{0:t-1})\\\\\n    &=\\int_{x_{t-1}\\in E}\\mathbb{P}_{t-1}(X_{t-1}\\in dx_{t-1}|\\\\\n    &\\qquad Y_{0:t-1}=y_{0:t-1})P_t(x_{t-1},dx_t),\\\\\n    &\\mathbb{P}_t(X_t\\in dx_t|Y_{0:t}=y_{0:t})\\\\\n    &=\\frac{1}{p_t(y_t|y_{0:t-1})}f_t(x_t|y_t)\\\\\n    &\\qquad\\mathbb{P}_{t-1}(X_t\\in dx_t|Y_{0:t-1}=y_{0:t-1}).\n\\end{align*}\n\\] を満たすが，この積分を計算する必要がある．13\n粒子フィルターでは相関を持った粒子系により，これらの値を逐次的に近似していくが，このアルゴリズムが出来る前に提案された近似手法を総覧する．\n\n1.7.1 解析的な方法\n節 1.5 で紹介した拡張 Kalman filter は，モデルが線型に近い場合には有効な近似手法であるが，一致推定量にはならない．この点を修正するために，線型近似の誤差を修正する IEKF (Iterated EKF) などが開発された．\n\n\n1.7.2 数値積分による方法\n状態空間 \\(E\\) の次元が3以下である場合，積分は数値積分法によっても効率的に近似できる (Kitagawa, 1987)．14\n特に状態空間が有限である場合は積分は加法に退化するため，正確に実行することができる．15 この場合が隠れMarkovモデルに当たる．\n隠れMarkovモデルに於て MAP 推定量を動的計画法に基づいて探索する Viterbi 算譜 (Forney, 1973), (Viterbi, 1982) とパラメータ推定のための EM アルゴリズムの変種 Baum-Welch 算譜 (Baum & Eagon, 1967), (Gopalakrishnan et al., 1989) とは音声認識の分野で広く使われており，また状態空間が近似的に離散である場合にも近似手法として採用される．16\n\n\n1.7.3 Gauss混合で近似する方法\nフィルタリング分布 式 1 を正規分布の有限混合で近似する方法は Gaussian sum filter と呼ばれる (Sorenson & Alspach, 1971)．これは多峰性を帯びる事後分布のモデリングに強く，物体追跡の分野で広く使われることとなったが，一般の設定に使える普遍的な手法ではなかった．17\n\n\n1.7.4 アンサンブルによる近似\nフィルタリング分布 式 1 を正規分布を表現する決定論的に設計された粒子系によって近似し，これをシステムモデルに従って伝播させる．これにより，フィルタリング分布の2次以下の積率までは性格に近似できていることになる．この手法を 無香 Kalman filter (Unscented Kalman filter) という (S. Julier et al., 2000), (S. J. Julier & Uhlmann, 2004)．\nこの手法は，宇宙から大気圏に再突入する弾道物体の追跡などの場面で，粒子フィルターよりやや正確性が劣るが計算が速く，実用的であることも知られている (Ristic et al., 2004, p. 101)．\nこの手法は本質的に (Evensen, 1994) の EnKF (Ensemble Kalman filter) によって拡張された（ 節 3.5 も参照）．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#粒子フィルターの発明",
    "href": "posts/2023/Surveys/ParticleFilter.html#粒子フィルターの発明",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "2 粒子フィルターの発明",
    "text": "2 粒子フィルターの発明\n線型性や正規性の仮定を 全く 必要とせず，あらゆる状態空間モデルに使えて，加えて高次元でも適用可能な夢の新フィルタリング手法は，確率的シミュレーションを利用する Monte Carlo 法の一種 であった．(Gordon et al., 1993) はこれを bootstrap filter という名前で発表し，角度観測のみを用いた物体追跡の問題 (bearings-only tracking) への応用も付した．\n実は，この bearings-only tracking は極めて非線型性が強い問題として古典的なものであり，従来の拡張 Kalman filter の方法では精度が全く伸びなかった．これに比べて bootstrap filter では圧倒的な性能改善が見られたのであった．18\nそれだけでなく，この手法はフィルタリング問題の範疇を超えて広範な応用先を見つけつつあり，MCMC と並ぶ ベイズ計算法 となっている（ 節 2.3 ）．\nなお，北川源四郎も同年（1993年）のカンファレンスにて，Monte Carlo filter の名前で同様のアルゴリズムを発表している．そのジャーナル版は (北川源四郎, 1996a)．19 日本語文献 (北川源四郎, 1996b) はウェブ上からも読める．\n\n2.1 逐次重点サンプリングの修正としての粒子フィルター\nこの手法は 重点サンプリング を繰り返すという逐次重点サンプリングの改良として開発された．逐次重点サンプリングのアイデアは古く，(Hammersley & Morton, 1954) で提案され，(Mayne, 1966), (Handschin & Mayne, 1969) で逐次推定に応用された．\nしかし，これには荷重の分散が指数増大するという致命的な欠点があった (Chopin, 2004)．特に何度か反復を経ると，１つの粒子を除いて他の粒子は全て荷重を殆ど持たなくなってしまうという現象が起こる (Del Moral & Doucet, 2003)．このように，荷重の分散が大きくなり，少数の粒子しか推定に関与しなくなる現象を 荷重の縮退 という．20\nそこで，(Rubin, 1987) のアイデアを基に，21 リサンプリング という新たな機構を取り入れることを考える．これは 遺伝的変異・選択機構 とも呼ばれ，22 尤度の高い粒子を複製する一方で尤度の低い粒子は削除するというものである\nこれにより定期的に荷重をリセットすることで分散の指数増大を抑えることができ，が保たれるのである．しかしながらこの仕組みにより粒子の間に相関が生じるために，理論的解析を困難にする．この点から粒子フィルターは （平均場）相関粒子法 ともいう．23\nリサンプリング機構の分計算負荷は上がるが，1990年代では計算機の性能はすでにこれを補って余りある段階に達していたのである．なお，(Gordon et al., 1993) はリサンプリング手法としては多項リサンプリングを採用しており，極めて実装が簡単という点も多くの応用を生んだ理由である．24\nリサンプリングの実際の実装については，粒子フィルターの実装の稿 も参照．\n\n\n2.2 粒子フィルターの応用\n(Gordon et al., 1993) による粒子フィルターの考案は，物体追跡（と防衛目的）への応用が念頭にあり，この分野では粒子フィルターが極めて有効である (Ristic et al., 2004)．これは非線型性をものともしない性質に加えて，事前情報を柔軟に取り入れやすいという粒子フィルターの性格も大きく貢献している．25\nコンピュータビジョンへの応用も早期から取り組まれており (Isard & Andrew, 1998)，これに続いてロボティクス，HCI (Human-Computer Interaction) 分野への応用もなされている (岡兼司, 2005), (Wills & Schön, 2023)．\n一方で，(北川源四郎, 1996a) は季節調整モデルなど非定常時系列への応用が念頭にあった．確率的ボラティリティモデルなど，ファイナンスで扱う時系列は非線型性・非正規性を示すと同時にデータ数も多い．逐次推定のステップ数が増えようとも誤差が蓄積しない粒子フィルターが見事に推定を実行する．26\n加えて，マクロ経済学の分野で 動学的確率的一般均衡モデル (DSGE) の推定にも応用されている．DSGE は非線型なミクロ経済学的モデルの上に構築された大規模なモデルで， 従来は MCMC を用いたベイズ推論が実行されていたが，粒子フィルターに焼戻し法や並列計算を組み合わせることでこの問題を回避でき，さらに事後分布が多峰性を持つ場合でも有用である (Herbst & Schorfheide, 2013)．27\n近年では他の社会科学分野でもベイジアンモデリングが用いられ（ベイズ計算の稿 参照），粒子フィルターも エージェント・ベースド・モデル への応用などが試みられている (Lux, 2018)．\n\n\n2.3 粒子フィルターの一般の推定問題への応用\nこうして，粒子フィルターは非正規・非線型フィルタリング問題の解決のために開発されたアルゴリズムであったが，非線型フィルタリングに限らず極めて広い問題へと応用出来ることが徐々に明らかになった．\n特にサンプラーとしても極めて有効であり（ 節 2.3.2 ），粒子フィルターは MCMC と併せて ベイズ計算 の主要トピックの１つに躍り出た (Martin et al., 2023, p. 11)．この意味で，粒子フィルターは広く 逐次 Monte Carlo 法（Sequential Monte Carlo methods 略して SMC ）とも呼ばれる．28\n\n2.3.1 ベイズ学習\n逐次的でない「静的」な設定の下での Bayes 推論に SMC を用いる方法は (Chopin, 2002) が草分け的な仕事をした．この枠組みでは，Bayes 事後分布 \\(\\pi(\\theta|y_1,\\cdots,y_N)\\) の近似において，途中の \\(\\pi(\\theta|y_1,\\cdots,y_n)\\) を経由して逐次的に近似することが，自然な計算コスト削減法として理解できる．\n\n\n2.3.2 サンプラーとしてのSMC\n複雑な分布からのサンプリングや，その正規化定数の計算という MCMC と同様の用途に SMC を使うこともできる (Del Moral et al., 2006)．\nSMC は 調音 (tempering) を通じてサンプリング問題に応用される．これは目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) に対して，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをするというアイデアである．\nこの媒介的な分布 \\(\\pi_n\\) を焼き戻し分布 (tempered distribution) または架橋分布 (bridging distribution) などとも呼ぶ．29\n詳しくは，SMC サンプラーの稿 を参照．\n\n\n2.3.3 最適化\nSMC を最適化へ応用することができる．\nまず，任意の確率的最適化アルゴリズムに対して，これを並列して実行し，うまくいっているものとうまくいっていないものの間に遺伝的変異・選択機構を導入することでより性能の良い発見的アルゴリズムを導出出来ることを (Aldous & Vazirani, 1994) が指摘しており，この機構を “go with the winners” と呼んでいる．\n古典的な大域的最適化法に 焼きなまし法 (simulated annealing) (Kirkpartick et al., 1983) があるが，(Schäfer, 2013) は特定の目的関数に対してこれを一般化して粒子法に基づく最適化法を提案し，特に多峰性を持つ場合に大きく性能を改善した．\n(Johansen et al., 2008) では潜在変数モデルのパラメータの最尤推定に，EM アルゴリズム (Dempster et al., 1977) の代わりに simulated annealing に基づいた手法を用いている．分布の台が最尤推定量に収束するような分布の列 \\[\n\\overline{\\pi}_{\\gamma_n}(\\theta)\\,\\propto\\,p(\\theta)p(y|\\theta)^{\\gamma_n}\n\\] を構成し，これから逐次的にサンプリングをするのである．最終的なアルゴリズムは，EM アルゴリズムや MCMC を用いる場合より，局所解に囚われることが少なく，初期値の設定に殆ど左右されないという利点がある．\nこの枠組みは一般の非凸最適化アルゴリズムになる可能性がある．30\n\n\n2.3.4 稀現象シミュレーション\nSMC によるサンプリングは，直接のシミュレーションが困難な分布 \\(\\pi_p\\) に対しても，\\(\\pi_1,\\cdots,\\pi_p\\) と逐次的に近似することでこれを可能にするというアイデアであった．\n特に，シミュレーションが困難である分布 \\(\\pi_p\\) の例として，極めて稀な事象 \\(A_p\\) に関する条件付き分布などがあり得る．これに対して事象列 \\(A_0\\supset\\cdots\\supset A_p\\) を取り， \\[\n\\pi_n(d\\theta)=\\frac{1}{L_n}1_{A_n}(\\theta)\\nu(d\\theta)\n\\] という仲介分布の列を取るのである．この問題を 稀現象シミュレーション という．\n\n\n2.3.5 確率的グラフィカルモデルの推論手法として\n変数間の統計的依存関係が無向グラフによって与えられるモデルを 確率的グラフィカルモデル という．ニューラルネットワーク (Rumelhart et al., 1987) も 状態空間モデル もその例である．\n確率的グラフィカルモデル自体多くの応用先をもち，疫学における疾病マッピング (Green & Richardson, 2002)，画像解析 (Carbonetto & Freitas, 2003) などにも応用されている．31\nこのようなモデルの尤度は極めて複雑になるが，これへ至る道を自然な方法で構成することができる (Hamze & de Freitas, 2005)．複雑なモデルでは MCMC は尤度の正規化定数を評価する方法を持たないが，SMC ではこれを自然に評価することができる．\n\n\n2.3.6 ポリマーシミュレーション\nポリマーシミュレーションにおいても重点サンプリング法と同じ発想が提案されたのは極めて早い段階であった (Rosenbluth & Rosenbluth, 1955)．加えてリサンプリングにあたる enrichment の考え方もあった (Wall & Erpenbeck, 1959)．\nなお，このような物理・化学的文脈では，状態空間を探索する主体としての意味を強調し，「粒子」の代わりに walker と呼ぶ．32\nこれら２つを組み合わせることで，長いポリマー鎖のシミュレーションを可能にする方法として，相関粒子法に基づくアルゴリズム PERM (pruned-enriched Rosenbluth algorithm) が提案されている (Grassberger, 1997)．\nこの手法はたんぱく質の折り畳み問題にも応用されている (Hansmann & Okamoto, 1999)．\n\n\n2.3.7 量子系シミュレーション\n量子多体系の基底状態のシミュレーションにおけるモンテカルロ法である QMC (Quantum Monte Carlo)33 でも， branching というリサンプリング機構を取り入れた相関粒子法が用いられている (Assaraf et al., 2000)．\nこれは大規模な疎行列の最小固有値・固有ベクトルを近似する手法として，量子系に限らない幅広い応用がある．34\n\n\n\n2.4 粒子フィルターの弱点\n\n計算量\n粒子フィルターは高い汎用性の代償として，多数の粒子による高精度な推論のためには多くの計算量を必要とすることは欠点に挙げられる．が，CPU や並列計算の発展により十分な量の粒子を用意できる場面も増えたため，その問題点も形骸化してきてると言える．35\n荷重の縮退\nまた粒子フィルターは，観測 \\(Y_t\\) の次元が大きいなど，観測から得られる情報量が多く，尤度（ポテンシャル）の尖度が高いとき，リサンプリング機構があってもやはり縮退を起こしてしまう．36 このような場合は，観測の情報を柔軟に取り入れた提案核を構築し，誘導粒子フィルターをうまく設計する必要がある．\n\n\n粒子フィルタを適用する際の課題の一つは，各粒子に割り当てられる重みが１粒子に集中する，いわゆる退化の問題を限られた数の粒子でいかに克服するかである．(上野玄太, 2019)\n\n\n高次元\n地球科学や天気予報の分野では \\(Y_t\\) は大きく（\\(10^7\\) を超えることもある），このような場合は粒子フィルターは実行可能でなくなる．加えて Kalman フィルターも逆行列の計算が不安定になり，アンサンブル Kalman フィルタという粒子法が用いられる（ 節 3.5 ）．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#今後の研究",
    "href": "posts/2023/Surveys/ParticleFilter.html#今後の研究",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "3 今後の研究",
    "text": "3 今後の研究\n粒子フィルターの更なる応用には次の点の研究が肝要である．\n\n3.1 Feynman-Kac 理論と粒子法の統一的理解\n粒子フィルターは状態空間モデルのフィルタリングに使えるだけでなく，Feynman-Kac 測度の確率的近似に使える汎用手法である というのがより新しい数学的理解である．37 この文脈では 相関粒子法 (interacting particle methods) と呼ばれる．38\n\nthe mathematical concepts and models are now at a point where they provide a very natural and unifying mathematical basis for a large class of Monte Carlo algorithms. (Del Moral & Doucet, 2014, p. 2)\n\nこの枠組みからならば，粒子フィルターに限らず，物理学・化学・工学で用いられている多くの 発見的手法 について，理論的な解析が可能になる可能性がある．\n\n\n3.2 提案分布の取り方\n節 2.4 で触れた通り，特に観測の情報量が大きい場合，提案分布の選び方が粒子フィルターの精度を大きく左右する．これは粒子を高確率領域に始めから誘導するように設計する 誘導粒子フィルター によってある程度対処できる．39\n参照 Markov 核 \\(M_t\\) がより良い攪拌性を持ち，ポテンシャル \\(G_t\\) が平坦であるほど性能は良い傾向があるが，40 このときの提案分布の取り方について普遍的な指針というものが得られていない．\n\nThe key factors for a successful application of particle filters in practice are therefore a good choice of the importance density and Rao-Blackwellization if possible. (Ristic et al., 2004) Epilogue\n\n(Guarniero et al., 2017) と (Heng et al., 2020) は提案分布にパラメトリックモデルを用意し，粒子推定量の分散を最小化するようにそのモデル内で逐次的に最適化していく機構を提案している．\n(Naesseth et al., 2015) が提唱する nested SMC は，は各時刻での提案分布を近似するために，もう一つのSMCを内部に走らせる．当然計算量は二倍になるが，それでも単純な bootstrap filter から大きく性能が改善する場合が多い．\n加えて機械学習の観点からも，真の事後分布との KL 距離を適応的に最小化する汎用手法に，提案分布のパラメトリックモデルにニューラルネットワークによる大型のパラメトリックモデルを併せる手法が提案されている (Gu et al., 2015)．\n\n\n3.3 高次元性への対応\n状態空間が高次元になることと，既存のモデルを状態空間モデルに定式化することに困難が伴うことが，朧げながら共通課題のように思われるが，その現れ方と解決法は個々の事例で異なる．\n\n3.3.1 部分的な線型構造の利用\n周辺化粒子フィルター，または Rao-Blackwellized particle filter とも呼ばれる方法である．これは多くの場面で，仮定されている状態空間モデルが部分的に線型である場合に，線型の部分をKalmanフィルターによって正確に解き，残った部分のみを粒子フィルターで解くことで，精度の向上とアルゴリズムの効率化を図る方法である．\n(Ristic et al., 2004, p. 287) では，探知前追跡 (track-before-detect) の問題41において，周辺化粒子フィルタが，必要な粒子数を大きく削減してくれることを紹介している．\nその他にも，問題毎の特有の構造を利用して計算量を削減・パフォーマンスを最適化することは重要な営みである．\n\n\n\n3.4 漸近論\n\n3.4.1 粒子数に関する漸近論\n目前の問題を，所与の精度で解くために必要な粒子数は幾ばくか？という問題は実用上も有益だと思われる．42\n\n\n3.4.2 時間離散化に関する漸近論\n(Chopin et al., 2022)\n\n\n\n3.5 EnKFの数学的解析\n状態空間が高次元である場合，（どうなる？）\nモデルが正規性を持つならば，EnKF (Ensemble Kalman Filter) (Evensen, 1994) が有効であり，データ同化の分野で広く使われている．43\n一方で，その数学的な振る舞いはほとんど解明されておらず，1次元の場合から調べている状況である (Del Moral & Horton, 2023)．\n\n\n3.6 粒子フィルターの応用\nリアルタイム性と ベイズ推定の意思決定への応用との相性の良さ が，今後多くの重要な応用を見る可能性がある．\n\n3.6.1 属人化医療への応用\n筆者は属人化医療への応用が大きなモチベーションになっている．44\n病気の進行（あるいは健康）のモニタリングのために，健康診断やウェアラブルデバイス，フォローアップから得られるデータは，治療の見直しや異常の早期発見のために即時処理されることが望ましい．これに逐次モンテカルロ法でBayes的に迫る研究がある (Alvares et al., 2021) ！\nさらに，個々人の日常生活のレベルではSMCを用いているものはどうやらまだなく，運動と睡眠時間の間の関係と処置効果をMonte Carlo法を用いて推定している研究はある (Daza & Schneider, 2022)．これを逐次化することで，よりリアルタイムで自分に合った生活習慣への示唆が得られるアプリを開発できるかもしれない．\nまた，属人化医療においてはシステム生物学的なモデルに基づいた薬効推定が欠かせない．小規模な患者群と測定時間（服薬時間）に関する不確定性を考慮した手法が (Krengel et al., 2013) で考察されている．\nまた，腫瘍サンプルに含まれる体細胞の突然変異に関するデータから，SMCを用いて腫瘍の発達と進行の状態を理解する手法も提案されている (Ogundijo et al., 2019)．\n\n\n3.6.2 ゲノム解析への応用\n次世代DNAシークエンサーでは，DNAの各塩基ごとに異なる蛍光物質を結合させ，蛍光の波長と強度により塩基を読み取る仕組みであり，蛍光強度の生データからDNA配列データへ変換するベースコールと呼ばれる段階で粒子フィルターを使うことも提案されている (Shen & Vikalo, 2012)．\n\n\n3.6.3 疫学への応用\nCovid-19のようなパンデミックにおいて，疫学モデルを通じて時間変動する再生産数をリアルタイムでモニタリングをして意思決定に繋げるためのSMC手法も考えられており，実際にノルウェーで使用され有効性が実証された (Strovik et al., 2023)．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#footnotes",
    "href": "posts/2023/Surveys/ParticleFilter.html#footnotes",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Anderson & Moore, 1979) 第1.1節．↩︎\n(有本卓, 1970)．↩︎\nこのフィルタリング問題の統計的な側面を，前述の電気工学的な側面から区別して，stochastic filteringと呼んだりもする．↩︎\n(Bain & Crisan, 2009) 1.3節．↩︎\n(青山友紀, 1986) 「当初は大型コンピュータを用いたシミュレーションの技法であったディジタルフィルタが，今では超LSI 1チップで実現されるまでになった．」他にも，発表時1986年では，音声信号を中心とする低周波帯域ではデジタルフィルタがアナログフィルタを駆逐しつつあること，通信システムのデジタル化に伴ってこの勢いは完全に代替するまで進むだろうとの筆者の考えが述べられている．↩︎\n(McGee & Schmidt, 1985) がNASAからの資料．また，(Del Moral & Penev, 2014) も参照．加えて，(Kalman, 1960) からちょうど10周年の文献 (有本卓, 1970) に当時の雰囲気も感じさせる良いサーベイがある．↩︎\n(Ristic et al., 2004, p. 3) “The state-space approach is convenient for handling multivariate data and nonlinear/non-Gaussian processes and it provides a significant advantage over traditional time-series techniques for these problems.”↩︎\n(Rabiner, 1989)．(Gales & Young, 2007) には “almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs” とある．↩︎\n(Kitagawa, 1998) で指摘されている．↩︎\nまた，同様にアポロ計画の中で，飛行体に載積出来るような小規模な計算資源と短い単語長でも安定してKalman filterが動くように，共分散行列の二乗根を保持するという “square-root” formulation of the filter が 1972年に考案されたことが，summary でも触れられている (McGee & Schmidt, 1985)．↩︎\n(S. J. Julier & Uhlmann, 2004, p. 402) など．拡張 Kalman filter は遷移関数を線型近似することに基づく．よって Jacobi 行列の計算が必要であり，このため 解析的方法 とも呼ばれる (Ristic et al., 2004, p. 21)．よって遷移関数が可微分でない場合も実行不可能である．とはいっても，拡張 Kalman filter はナビゲーションシステムや GPS のデファクトスタンダードである Wikipedia．(Ristic et al., 2004) 第7章では距離のみでの追跡 (range-only tracking) では拡張 Kalman filter の性能と大差なく，計算量の問題から EKF の選択を推奨している．第8章でも弾道物体追跡の問題で，限られた設定では粒子フィルターと同等の性能を見せている．↩︎\n(Ristic et al., 2004, p. 16)．↩︎\nこの結果は (Chopin & Papaspiliopoulos, 2020) 第5章 など．記法 \\(Y_{0:t}\\) は 本サイトの数学記法一覧 を参照↩︎\n(Crisan & Doucet, 2002) に3の数字が例示されている．↩︎\n(Chopin & Papaspiliopoulos, 2020) 第6章，(Ristic et al., 2004) 2.2節．↩︎\n(Ristic et al., 2004, p. 24)．↩︎\n(Ristic et al., 2004, p. 25)．↩︎\n(Gordon et al., 1993) の結果であると同時に，(Ristic et al., 2004) 第6章でも種々の手法と比較した数値実験がなされている．一方第7章にて，距離のみでの追跡 (range-only tracking) では拡張 Kalman filter の性能と大差なく，計算量の問題から EKF の選択を推奨している．↩︎\nDel MoralのWebサイトも歴史的背景に詳しい．↩︎\nweight degeneracy (Creal, 2011) p.253，(Cappé et al., 2005) 第7章，(Robert & Casella, 2004, p. 551) 14.3.3節 など．↩︎\nSIR (Sampling/Importance Resampling) Algorithm と呼ばれるものであった．これの逐次化が粒子フィルターだとみなせる (Robert & Casella, 2004, p. 552) 14.3.4節．↩︎\n(Del Moral & Doucet, 2014) などの用語である．↩︎\n(Del Moral & Horton, 2023) では mean-field type interacting particle methods と呼んでいる．呼び方については (Iba, 2001) と (Del Moral, 2013) と 紹介記事 も参照．↩︎\n(Creal, 2011) p.256．↩︎\n事前情報というのは，自動運転の文脈では自動車が動き得る領域というのは極めて限られている，というような事前に判明しているが，うまく取り入れにくい情報のことをいう．(Ristic et al., 2004) 第6章や第9章で繰り返し種々の設定で実証されている．(Yang et al., 2023) は水中での物体追跡が縮退により従来は粒子フィルタが使えなかった問題の解決を試みている．(Kummert et al., 2021) はロボット支援を用いた手術中に物体追跡を利用する際に，尤度が低すぎるなどの要素から追跡対象を失った状態を検出してアラートを出す機構を開発している．↩︎\nファイナンスにおける確率的ボラティリティモデルなどの例が挙げられている (Creal, 2011) p.256．↩︎\n粒子フィルターの経済学での応用が増えたきっかけが Fernández-Villaverde and Rubio-Ramírez (2005, 2007) による（小規模な）DSGEモデルの推定への応用だった (Creal, 2011, p. 246)．(矢野浩一, 2014) は実物景気循環モデル (Real Business Cycles Model) への応用を解説している．↩︎\n(Crisan & Doucet, 2002) ではすでに SMC とも呼ばれることが記されている．(Chopin & Papaspiliopoulos, 2020) 第3章にSMCのフィルタリング以外の多くの応用が紹介されている．↩︎\n(Behrens et al., 2012) はいずれも用いている．(Hamze & de Freitas, 2005) に tempered distribution の語用法がある，↩︎\n(Chopin & Papaspiliopoulos, 2020, pp. 3.4節 p.30)↩︎\n疾病マッピングは疾病の空間的な分布を把握すること，画像解析とは画像データから特徴を抽出してその意味論を理解することを指す．↩︎\n(Iba, 2001), (Kremer & Binder, 1988) などが良いレビューを提供している．(Assaraf et al., 2000) が量子系のシミュレーションの文脈で．↩︎\ndiffusion Monte Carlo, projector Monte Carlo などと呼ばれる手法に等しい．Green’s function Monte Carlo もポテンシャル \\(G\\) の取り方が違うのみである (Assaraf et al., 2000)．また (Iba, 2001) 3.1節 p.282 にも言及がある．↩︎\n(Iba, 2001) 3.1節 p.281．↩︎\n(矢野浩一, 2014) p.190，(Ristic et al., 2004) 前文 p.xi．↩︎\n(Chopin & Papaspiliopoulos, 2020, pp. 19.1節 p.371), (Creal, 2011, pp. 2.5.1節 p.258)．↩︎\n(Del Moral & Doucet, 2014) など．↩︎\n(Iba, 2001) などでは population Monte Carlo と呼ばれている．↩︎\nそのアイデアは (Doucet et al., 2001, pp. 79–95) 第4章 から．↩︎\n(Crisan & Doucet, 2002, p. 739) も参照．↩︎\n探知前追跡とは，信号が弱い，またはノイズが強い環境下において，信頼のおける初期信号を頼りにせずとも，物体追跡を実行するための手法．↩︎\n(Ristic et al., 2004, p. 288) に示唆されている．↩︎\n(Del Moral & Horton, 2023) は ensemble Kalman particle filtering methodology と呼んでいる．↩︎\n過去の記事でも触れた．↩︎"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html",
    "href": "posts/2023/Surveys/SSM.html",
    "title": "相関粒子系の社会実装",
    "section": "",
    "text": "要は僕の専門分野である訳だが，これが今回のビジネスモデルの「骨格」の部分になる．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\n\n\n定義 (State Space Model)\n\n\n\n状態空間モデル1とは，状態変数 \\(\\{X_t\\}_{t=0}^T\\) と，観測変数 \\(\\{Y_t\\}_{t=1}^T\\) の組からなる確率過程 \\[\n\\{(X_t,Y_t)\\}\\subset\\mathcal{L}(\\Omega;\\mathcal{X}\\times\\mathcal{Y})\n\\] であって，初期状態 \\(X_0\\) の分布と，\\(X_t,Y_t\\) の間の関係として次の2つの条件付き分布 \\(P_{t},F_t\\)\n\nシステムモデル \\[\nX_{t+1}|X_t\\sim P_{t+1}\\quad(t\\in T)\n\\]\n観測モデル \\[\nY_t|X_t\\sim F_t\\quad(t\\in[T])\n\\]\n\nを想定したものをいう．2\n\n\n\n状態空間モデルの図示（密度を持つ場合）\n\n\n\n\n\n\n\n\n\n\n状態空間モデルの例\n\n\n\n\n\nすなわち，初期状態 \\(X_0\\) の分布のモデル \\(\\{\\mathbb{P}_0^\\theta\\}\\)．Markov過程 \\(\\{X_t\\}\\) の遷移核のモデル \\(\\{P_t^\\theta\\}\\)，観測のモデル \\(\\{F_t^\\theta\\}\\) の3-組 \\((\\mathbb{P}_0^\\theta,P_t^\\theta,F_t^\\theta)\\) を 状態空間モデル という．また，過程 \\(\\{(X_t,Y_t)\\}\\) もMarkov過程になることが示せる．このため，状態空間モデルのことを 部分的に観測されるMarkov過程 (partially observed Markov process) とも表現する．\nその重要なサブクラスには，次のような名前が付いている：\n\n\\(X_0,X_1,\\cdots\\) が離散変数であるとき，特に 隠れ Markov モデル ともいう．\n\\(X_t,Y_t\\) がいずれも Gauss 確率変数で，\\(p_t^\\theta,f_t^\\theta\\) が線型であるとき，線型力学系 または Kalman フィルター ともいう．3\n\n\n\n\nただし，\\(X_t\\) は観測不能で，\\(Y_t\\) のみが観測されるものとする．従って，目標は \\(Y_1,\\cdots,Y_T\\) の値から \\(X_1,\\cdots,X_T\\) の値を推定することである．\n各時点 \\(t\\in[T]\\) において，現在までの観測 \\(Y_1,\\cdots,Y_t\\) から現在の状態 \\(X_t\\) を推定することを考える（フィルタリング問題4）．特に Bayes の枠組みでは，条件付き分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\\quad(t\\in [T])\n\\] を（逐次的に）推定することを目指す．5\nこの状態空間モデルのフィルタリング問題を解くためのアルゴリズムは多く知られているが，そのうち，モデル \\(X_{t+1}|X_t,Y_t|X_t\\) が高度に非線型でも通用する手法は粒子フィルターのみである．6\n粒子フィルターは，\\(X_t\\) の観測に関する事後分布を \\(N\\) 個（大量）の粒子によって近似する Bayes 推定手法で，各 \\(Y_t\\) の尤度の情報を重点リサンプリングによって取り入れながらも，計算コストを抑えながら \\(X_t\\) の事後分布を逐次近似していく．粒子フィルターとは何か？ も参照．\n\n\n\n要は，\\(Y_t\\) を安価に集めて，\\(X_t\\) を高値で売ることを考える．本当にこれがビジネスになるためには，２つの条件\n\n\\(X_t\\) は多くの人がリアルタイムに知りたいが，（少なくともリアルタイムには）知れない\n\\(Y_t\\) をたくさん集めれば \\(X_t\\) を推測できるが，簡単には推測するのに十分な次元の \\(Y_t\\) を用意できない\n\nを満たす必要がある．が，意外とこのようなものは多いかも知れない．\n一方で我々の売りは\n\n\n\n\n\n\n今回のビジネスモデルのコア\n\n\n\nどんなに推定しにくい \\(X_t\\) でも（モデルが複雑で尤度が解析的な表示を持たなくても），十分な情報を含む観測データ \\(Y_t\\) が得られれば，逐次推定できる．\n\n\nということになる．\n時系列データのオンライン推論は，需要が高い一方で実装が難しい．ここの乖離が好機になる可能性がある．"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#粒子ビジネスモデルモデルの基幹技術",
    "href": "posts/2023/Surveys/SSM.html#粒子ビジネスモデルモデルの基幹技術",
    "title": "相関粒子系の社会実装",
    "section": "",
    "text": "要は僕の専門分野である訳だが，これが今回のビジネスモデルの「骨格」の部分になる．\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\n\n\n定義 (State Space Model)\n\n\n\n状態空間モデル1とは，状態変数 \\(\\{X_t\\}_{t=0}^T\\) と，観測変数 \\(\\{Y_t\\}_{t=1}^T\\) の組からなる確率過程 \\[\n\\{(X_t,Y_t)\\}\\subset\\mathcal{L}(\\Omega;\\mathcal{X}\\times\\mathcal{Y})\n\\] であって，初期状態 \\(X_0\\) の分布と，\\(X_t,Y_t\\) の間の関係として次の2つの条件付き分布 \\(P_{t},F_t\\)\n\nシステムモデル \\[\nX_{t+1}|X_t\\sim P_{t+1}\\quad(t\\in T)\n\\]\n観測モデル \\[\nY_t|X_t\\sim F_t\\quad(t\\in[T])\n\\]\n\nを想定したものをいう．2\n\n\n\n状態空間モデルの図示（密度を持つ場合）\n\n\n\n\n\n\n\n\n\n\n状態空間モデルの例\n\n\n\n\n\nすなわち，初期状態 \\(X_0\\) の分布のモデル \\(\\{\\mathbb{P}_0^\\theta\\}\\)．Markov過程 \\(\\{X_t\\}\\) の遷移核のモデル \\(\\{P_t^\\theta\\}\\)，観測のモデル \\(\\{F_t^\\theta\\}\\) の3-組 \\((\\mathbb{P}_0^\\theta,P_t^\\theta,F_t^\\theta)\\) を 状態空間モデル という．また，過程 \\(\\{(X_t,Y_t)\\}\\) もMarkov過程になることが示せる．このため，状態空間モデルのことを 部分的に観測されるMarkov過程 (partially observed Markov process) とも表現する．\nその重要なサブクラスには，次のような名前が付いている：\n\n\\(X_0,X_1,\\cdots\\) が離散変数であるとき，特に 隠れ Markov モデル ともいう．\n\\(X_t,Y_t\\) がいずれも Gauss 確率変数で，\\(p_t^\\theta,f_t^\\theta\\) が線型であるとき，線型力学系 または Kalman フィルター ともいう．3\n\n\n\n\nただし，\\(X_t\\) は観測不能で，\\(Y_t\\) のみが観測されるものとする．従って，目標は \\(Y_1,\\cdots,Y_T\\) の値から \\(X_1,\\cdots,X_T\\) の値を推定することである．\n各時点 \\(t\\in[T]\\) において，現在までの観測 \\(Y_1,\\cdots,Y_t\\) から現在の状態 \\(X_t\\) を推定することを考える（フィルタリング問題4）．特に Bayes の枠組みでは，条件付き分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\\quad(t\\in [T])\n\\] を（逐次的に）推定することを目指す．5\nこの状態空間モデルのフィルタリング問題を解くためのアルゴリズムは多く知られているが，そのうち，モデル \\(X_{t+1}|X_t,Y_t|X_t\\) が高度に非線型でも通用する手法は粒子フィルターのみである．6\n粒子フィルターは，\\(X_t\\) の観測に関する事後分布を \\(N\\) 個（大量）の粒子によって近似する Bayes 推定手法で，各 \\(Y_t\\) の尤度の情報を重点リサンプリングによって取り入れながらも，計算コストを抑えながら \\(X_t\\) の事後分布を逐次近似していく．粒子フィルターとは何か？ も参照．\n\n\n\n要は，\\(Y_t\\) を安価に集めて，\\(X_t\\) を高値で売ることを考える．本当にこれがビジネスになるためには，２つの条件\n\n\\(X_t\\) は多くの人がリアルタイムに知りたいが，（少なくともリアルタイムには）知れない\n\\(Y_t\\) をたくさん集めれば \\(X_t\\) を推測できるが，簡単には推測するのに十分な次元の \\(Y_t\\) を用意できない\n\nを満たす必要がある．が，意外とこのようなものは多いかも知れない．\n一方で我々の売りは\n\n\n\n\n\n\n今回のビジネスモデルのコア\n\n\n\nどんなに推定しにくい \\(X_t\\) でも（モデルが複雑で尤度が解析的な表示を持たなくても），十分な情報を含む観測データ \\(Y_t\\) が得られれば，逐次推定できる．\n\n\nということになる．\n時系列データのオンライン推論は，需要が高い一方で実装が難しい．ここの乖離が好機になる可能性がある．"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#どんな-x_t-が売れるか",
    "href": "posts/2023/Surveys/SSM.html#どんな-x_t-が売れるか",
    "title": "相関粒子系の社会実装",
    "section": "2 どんな \\(X_t\\) が売れるか？",
    "text": "2 どんな \\(X_t\\) が売れるか？\n\n2.1 マクロ指標のナウキャスト\n最も示唆的と思われる例は，\\(X_t\\) としてGDP，商業販売額などのマクロ指標を取った場合だと思われる．\nマクロ指標は，各企業単体では推測できず，たとえ業界を絞っても各企業の売り上げデータやATM取引データなど，多くのデータを集めて高次元な \\(Y_t\\) を構成しなければ，信頼できる \\(X_t\\) の推定はできないだろう．高次元な \\(Y_t\\) から \\(X_t\\) をフィルタリング際の粒子法は安定せず，現在でも解決されていないオープンクエスチョンである．必然的にブルーオーシャンで誰も参入できない．\nさらに，マクロ指標はフィルタリングすること＝今現在の値を知ることに意味がある．理論的な障壁や技術的な障壁は高いが，経営判断に使ったり，投資判断に使ったり，需要は大きいと思われる．\n\\(Y_t\\) はデータとして広く流通しているわけではないならば（ATM利用データなど），技術力だけでなく，「信頼を得てデータを提供してもらっている」ことが競争力に加わっていき得る．\n\n\n2.2 天気予報\n\\(Y_t\\) が天気（降水量）というのがよくある．\\(X_t\\) が高次元になり，データ同化の問題になる．\nこの天気予報とデリバティブとの関係は？\n\n\n2.3 属人化医療\n個人的には，\\(X_t\\) は個人の体調スコア（あるいは特定の病気のリスク）で，\\(Y_t\\) がApple Watchなどのスマートデバイスからの心拍や体温や移動距離などの測定データ，という属人化医療の場面設定をよく考える．"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#サーベイ",
    "href": "posts/2023/Surveys/SSM.html#サーベイ",
    "title": "相関粒子系の社会実装",
    "section": "3 サーベイ",
    "text": "3 サーベイ\nまとめると，我々が提供できるものは高次元・大規模状態空間モデルの逐次推定手法の研究開発力．足りないものは研究成果と実装する時間と仲間と交渉力である．その代わり初期投資は極めて少なくて済む．\nまずは，前節での「\\(Y_t\\) を集めて \\(X_t\\) を売る」ビジネスモデルの実現に向けて，既存の成功事例を調べる．\n\n3.1 生態学\n生態学のモデリングにも状態空間モデルはよく使われる (Auger-Méthé et al., 2021)．特に非線型性が強く，粒子フィルターが有効になる (Chopin & Papaspiliopoulos, 2020, p. 19)．\n\n\n3.2 ナウキャスト\n元々は星野研究室の次の研究を知って，新里さんに紹介したときに得た着想であった．\nCard\n「ナウキャスト」「オルタナティブデータを活用した経済分析」と言った言葉でビジネス界で議論されているようだ．特に「ナウキャスト」という名前の会社はこの分野を開拓している．\nナウキャストとエム・データ、機関投資家向けオルタナティブデータ活用で協業\nオルタナティブデータを用いた経済活動分析"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#footnotes",
    "href": "posts/2023/Surveys/SSM.html#footnotes",
    "title": "相関粒子系の社会実装",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nここでの定義は (Chopin & Papaspiliopoulos, 2020, p. 42) に倣った．隠れMarkovモデルともいうが，こう言ったときは状態空間が有限集合であるという制約が暗につく．↩︎\n他の依存関係は仮定しないので，その図示は DAG となっている．状態空間モデルは Bayesian Network の例である．記法 \\(T,[T]\\) については 本サイトの数学記法一覧 を参照．↩︎\n(Bishop & Bishop, 2024, p. 353) での用語．↩︎\n一方で \\(Y_t\\) から，未来の値 \\(Y_{t+1}\\) を予測する問題を「予測問題」，\\(Y_{1},\\cdots,Y_t\\)から，過去の状態変数の値 \\(X_{s}\\;(s&lt;t)\\) を推定する問題を「平滑化問題」という．↩︎\n記法 \\(\\mathcal{L}\\) については 本サイトの数学記法一覧 を参照．ベイズ手法については ベイズ計算とは何か を参照．↩︎\n\\(Y_t|X_t,X_{t+1}|X_t\\) の線型性が高い場合は，Kalman filter とその変種，特に Extended Kalman filter の効率も良い場合が多い．↩︎"
  },
  {
    "objectID": "posts/Surveys/Notations.html",
    "href": "posts/Surveys/Notations.html",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "",
    "text": "1 集合\n  \n  1.1 集合\n  1.2 数\n  1.3 組\n  1.4 写像\n  1.5 圏\n  1.6 関数\n  1.7 演算\n  \n  2 空間\n  \n  2.1 位相\n  2.2 線型空間\n  2.3 Banach 空間\n  2.4 可測空間\n  2.5 確率空間\n  2.6 確率分布\n  \n  3 核\n  \n  3.1 測度\n  3.2 確率核\n  3.3 関数の空間\n  3.4 変形\n  3.5 作用素\n  \n  4 解析\n  \n  4.1 微分作用素\n  4.2 Fourier変換\n  4.3 超関数\n  4.4 確率解析\n  \n  5 過程\n  \n  5.1 確率変数の収束\n  5.2 確率過程\n  5.3 停止時"
  },
  {
    "objectID": "posts/Surveys/Notations.html#sec-set",
    "href": "posts/Surveys/Notations.html#sec-set",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "1 集合",
    "text": "1 集合\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\nここでは，あらゆる数学概念は，ZFC公理系 の下で集合として定義する．1 記号 \\(:=\\) は「右辺によって左辺を定義し，その結果等号が成り立つ」という主張の略記である．2\n\n1.1 集合\n\n空集合 を \\[\n\\emptyset:=\\{x\\mid x\\ne x\\}\n\\] で表す．3\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．4\n\\(A,B\\subset X\\) の 差 を \\[\nA\\setminus B:=\\left\\{a\\in A\\mid a\\notin B\\right\\}\n\\] で表す．\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cup B\\) と同じ数学的対象であるが，同時に \\(A\\cap B\\) という事実も主張するものとする．5\n対称差 を \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\] で表す．6\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．7 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．8\n例えば，\\(X\\in\\mathcal{L}(\\Omega)\\) を実確率変数，\\(A\\in\\mathcal{B}(\\mathbb{R})\\) を Borel 集合とすると， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] という略記を用いる．\n\n\n\n1.2 数\n\n自然数 を \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\},\n\\] によって帰納的に定義する．9\n自然数の集合を表すため，次の記法を用意する：10 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，11 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す： \\[\n\\mathbb{R}_+=[0,\\infty),\\quad\\mathbb{R}^+=(0,\\infty).\n\\]\n部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．12\n実数 \\(x\\in\\mathbb{R}\\) に対して，その整数部分を \\[\n\\lfloor x\\rfloor:=\\max\\{n\\in\\mathbb{Z}\\mid n\\le x\\}\n\\] と表す．13\n\n\n\n1.3 組\n\n\\(n\\)-組 を次のように帰納的に定める：14 \\[\n(x_1,x_2):=\\{\\{x_1\\},\\{x_1,x_2\\}\\},\n\\] \\[\n(x_1,\\cdots,x_n):=(x_1,(x_2,\\cdots,x_n)).\n\\]\n自然数の組を表すため，次の記法を用意する：15 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組を \\[\nX_{1:N}:=(X_1,\\cdots,X_N)\n\\] と表す．16\n\n\n\n1.4 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n写像 \\(f\\) の 値域 を \\[\\mathrm{Im}\\,f:=f(X)\\] で表す．\n\\(A\\subset X\\) への 制限 を \\(f|_A:A\\to Y\\) と表す．17\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．18\n\\(f_*\\) は部分集合 \\(A\\subset X\\) を像 \\(f(A)\\subset Y\\) に対応させる写像 \\[\nf_*:P(X)\\to P(Y)\\] と定義する．\n同様に写像 \\(f^*:P(Y)\\to P(X)\\) を定める： \\[\nf^*(B)=f^{-1}(B)\\quad(B\\subset Y).\n\\]\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．19\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．20\n写像 \\(f:X\\to Y\\) のうち，有限個の元を除いて \\(f(x)=0\\) を満たすものがなす全体を \\[\nY^{(X)}:=\\left\\{f\\in Y^X\\mid f=0\\;\\;\\text{f.e.}\\right\\}\n\\] と表す．21\n\\(P(X)\\) を \\(2^X\\) と同一視する．特に，\\(X\\) の有限部分集合の全体を \\[\n2^{(X)}=\\left\\{A\\in P(X)\\:\\middle|\\: A\\overset{\\text{finite}}{\\subset}X\\right\\}\n\\] と表す．22\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．23\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\[\n\\mathrm{pr}_i:\\prod_{i\\in I}X_i\\twoheadrightarrow X_i\n\\] で表す．24\n\\(x\\in X\\) での 評価写像 を \\[\n\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\n\\] で表す．25\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．\nしかしこの写像の値域も 族 と呼び，この場合は \\[\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\] と表す．26\n特に \\(I=\\mathbb{N}\\) のときは 列 ともいう．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．27\n\n\n\n1.5 圏\n\n集合の圏 を \\(\\mathrm{Set}\\) で表す．\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X)\n\\] を表す．28\n確率空間と確率核の圏を \\(\\mathrm{Stoch}\\) で表す．29\n圏 \\(C\\) の対象 \\(X,Y\\in C\\) の間の 射 の全体を \\(\\mathrm{Hom}_C(X,Y)\\) で表す．30\n特に \\[\nY^X=\\mathrm{Map}(X,Y)=\\mathrm{Hom}_\\mathrm{Set}(X,Y).\n\\]\n圏 \\(C\\) の対象 \\(X\\in C\\) の自己射の全体を \\[\n\\mathrm{End}_C(X):=\\mathrm{Hom}_C(X,X)\n\\] で表す．\nそのうち可逆なもののなす部分集合を \\(\\mathrm{Aut}_C(X)\\) で表す．集合 \\([n]\\) の 置換群 は \\(\\mathrm{Aut}_\\mathrm{Set}([n])\\) と表せる．\n\n\n\n1.6 関数\n\n\\(R\\) を環とする．\\(f_1,f_2\\in R^X\\) に対して， \\[\n(f_1+f_2)(x):=f_1(x)+f_2(x),\n\\] \\[\n(f_1f_2)(x):=f_1(x)f_2(x)\n\\] で定める演算により \\(R^X\\) も環とみなし，定値関数 \\(f\\equiv a\\in R\\) を \\(R\\) の元と同一視する．31\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を \\[\na\\lor b:=\\sup\\{a,b\\},\n\\] \\[\na\\land b:=\\inf\\{a,b\\},\n\\] で表す．32\n\\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right)\n\\] と表す．33\n\\(0\\) を持つ束においては次の略記を使う：34 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序集合 \\(Y\\) に値を取る関数 \\(f,g\\in\\mathrm{Map}(X,Y)\\) について，\\(f\\le g\\) とは \\[\n\\forall_{x\\in X}\\;f(x)\\le g(x)\n\\] の略記とする．\n同じ条件を，一階の量化記号 \\(\\forall\\) を省略して \\[\nf(x)\\le g(x)\\quad(x\\in X)\n\\] または \\(f\\le g\\) とも略記する．\n\\(Y\\) が束であるとき，この順序により関数の空間 \\(\\mathrm{Map}(X,Y)\\) は束となり，演算 \\(\\land,\\lor\\) が定まる．\n関数の列 \\(\\{f_n\\}\\subset Y^X\\) について，\\(f_n\\nearrow f\\) とは，収束 \\(f_n\\to f\\) だけでなく，\\(\\{f_n\\}\\) が単調増加であることも含意する．35\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\[\nO(g(x))\\;(x\\to x_0)\\] とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．36\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の意味でも使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0\n\\] を満たすこととする．\n\n\n\n1.7 演算\n\n次の演算規則を約束する：37 \\[\n\\prod_\\emptyset=1,\\quad\\sum_{\\emptyset}=0.\n\\]\n\\(0!=1\\) とする．"
  },
  {
    "objectID": "posts/Surveys/Notations.html#sec-space",
    "href": "posts/Surveys/Notations.html#sec-space",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "2 空間",
    "text": "2 空間\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．38\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．39\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．\n閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}\n\\] で表す．\n\n\n\n2.2 線型空間\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．40\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．41\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}\n\\] とも表す．42\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，43 共役転置を \\(A^*\\) で表す．44 \\(\\mathbb{F}=\\mathbb{C}\\) の場合は \\(A^\\top=A^*\\)．\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．45\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について， \\[\n\\begin{align*}\n  A&+B\\\\\n  &\\quad:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n  \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\},\n\\end{align*}\n\\] と表す．46\n集合 \\(A\\subset X\\) の 凸包 を \\(\\mathop{\\mathrm{Conv}}(A)\\) で表す．47\n集合 \\(A\\subset X\\) が生成する部分空間を \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x\n\\] で表す．48\n内積を \\((-|-)\\) で表す．49\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を50 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\mathop{\\mathrm{Tr}}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}\n\\] で表す．51\n\n\n\n2.3 Banach 空間\n\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．52 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n特に \\(J\\) が有限であるとき， \\[\n\\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n\\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．53\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．54\n距離空間 \\((T,d)\\) の 開球 を \\[\n\\begin{align*}\n  U_\\epsilon(t)&:=U(t;\\epsilon)\\\\\n  &:=\\left\\{s\\in T\\mid d(s,t)&lt;\\epsilon\\right\\}\n\\end{align*}\n\\] で表す．55\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．56\n単位閉球を \\(B:=B(0;1)\\) で表す．\n\\(\\mathbb{R}^n\\) のものである場合は特に \\(B^n\\) とも表す．57\n\\(\\mathbb{R}^n\\) の標準基底を \\[\ne_i=(0,\\cdots,0,1,0,\\cdots,0)\n\\] と表す．58\nBanach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．59\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}\n\\] で表す．60\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) として理解できる数学的対象の別記法と捉えられるように設計する．61\n\n\n2.4 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．62\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．63\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．64\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．65\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．66\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．67\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．68\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．69\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．70\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．71 \\(\\gamma_n:=\\mathop{\\mathrm{N}}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n2.5 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．72 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．73\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．74\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．75\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．76\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．77\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．78\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．79\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．80\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．81\n\n\n2.6 確率分布\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 確率測度 の全体を \\(\\mathcal{P}(E,\\mathcal{E})\\) と書く．\\(E\\) が位相空間であるとき，Borel 確率測度の全体を \\(\\mathcal{P}(E)\\) と略記する．82\n\\(E\\) を位相空間とする．\\((E,\\mathcal{B}(E))\\) 上の Radon 確率測度 の全体を \\[P(E)\\subset\\mathcal{P}(E)\\] で表す．83\n2つの確率分布 \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) の カップリング の全体を \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\] で表す．84\n\\(d\\)-次元 正規分布 を \\[\\mathop{\\mathrm{N}}_d(\\mu,\\Sigma)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．85\n集合 \\(A\\subset\\mathbb{R}^d\\) 上の 一様分布 を \\[\\mathrm{U}(A)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．\n点 \\(x\\in E\\) 上の Delta 測度 を \\(\\delta_x\\) で表す．86\n確率変数 \\(X\\sim\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の 分布関数 を \\[\n\\begin{align*}\n  F_X(a)&:=F_\\nu(a)\\\\\n  &:=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d]\\\\\n  &\\quad(a=a_{1:d}\\in\\mathbb{R}^d)\n\\end{align*}\n\\] で表す．\n\\(d=1\\) のとき，その一般化逆を \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\}\n\\] \\[\n(u\\in(0,1)^d)\n\\] で表す．87"
  },
  {
    "objectID": "posts/Surveys/Notations.html#sec-kernel",
    "href": "posts/Surveys/Notations.html#sec-kernel",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 核",
    "text": "3 核\n空間を導入した次は，その射を定義せねばなるまい．\n本節では，\\((E,\\mathcal{E})\\) を 可測空間 とする．88\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 89\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 90\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．91 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．92\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．93\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．94\n\n\n\n3.2 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：95\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．96 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．97\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 98\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．99\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．100\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．101\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．102\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．103\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.3 関数の空間\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．104\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．105\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．106\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．107 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．108\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．109\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．110\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．111\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．112\n\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．113\n\n\n3.4 変形\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．114\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．115\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．116\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．117\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．118\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．\n\n\n\n\n3.5 作用素\n\\(E,F\\) をノルム空間とする．\n\n作用素 \\(T:E\\to F\\) と言ったとき，線型写像 \\(T:E\\to F\\) を指すこととする．119\n\\(E\\) 内の作用素 \\(T:E\\supset\\mathcal{D}(T)\\to F\\) と言ったとき，ある \\(E\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to F\\) を指すこととする．120\n有界作用素の全体を \\(B(E,F)\\) で表す．121 \\(B(E):=B(E,E)\\) とする．\n連続作用素の全体を \\(L(E,F)\\) で表す．122"
  },
  {
    "objectID": "posts/Surveys/Notations.html#sec-analysis",
    "href": "posts/Surveys/Notations.html#sec-analysis",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 解析",
    "text": "4 解析\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．123\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．124\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．125\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．126 \\(m=1\\) のとき， \\[\n\\mathop{\\mathrm{\\mathrm{grad}}}u:=\\nabla u:=\\left(\\frac{\\partial u}{\\partial x_1},\\cdots,\\frac{\\partial u}{\\partial x_n}\\right)\n\\] とも表す．\n発散 を \\[\n\\mathop{\\mathrm{div}}u:=\\mathop{\\mathrm{Tr}}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．127\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n\\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n\\] と同一視する．128\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n\\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\mathop{\\mathrm{Tr}}(D^2u)\n\\] で定める．\n\n\n\n4.2 Fourier変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．129\n符号関数 を \\[\n\\mathop{\\mathrm{sgn}}(x):=2H(x)-1\n\\] で定める．130\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 超関数\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．131 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．132\n\n\n\n4.4 確率解析\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\\\\n    &\\qquad\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．133"
  },
  {
    "objectID": "posts/Surveys/Notations.html#sec-process",
    "href": "posts/Surveys/Notations.html#sec-process",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "5 過程",
    "text": "5 過程\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率変数の収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．134\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．135\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n\n\n\n5.2 確率過程\n（確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．136\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\(E^T\\) に定める写像 \\[\nX_-:\\Omega\\to E^T\n\\] を 転置 と呼ぶ．137\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいい，このような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．138\n\\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．139 ただし，\\(x(0-)=x(0)\\) とする．140\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の フィルトレーション \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系 \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．141\n加えて， \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\] と表す．142\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上のフィルトレーション \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) が完備性 \\[\n\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\n\\] を満たすとき，4-組 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) を 確率基底 という．143\n\n\n\n5.3 停止時144\n\n確率基底 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) 上の 停止時 とは，同じ確率空間 \\(\\Omega\\) 上の可測関数 \\(T:\\Omega\\to[0,\\infty]\\) であって， \\[\n\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t,\\qquad t\\in\\mathbb{R}_+,\n\\] も満たすものをいう．145\n停止時 \\(T\\) までの 情報 とは， \\[\n\\mathcal{F}_T:=\\left\\{A\\in\\mathcal{F}_\\infty\\mid\\forall_{t\\in\\mathbb{R}_+}\\;A\\cap\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t\\right\\}\n\\] で定まる \\(\\sigma\\)-代数をいう．"
  },
  {
    "objectID": "posts/Surveys/Notations.html#終わりに",
    "href": "posts/Surveys/Notations.html#終わりに",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "終わりに",
    "text": "終わりに\n\n本サイトの記法で筆者が最も注意することは，あらゆる記法を背後の数学的消息と調和するように定義するということであった．\nこれにあたり，あらゆる 数学的対象 を集合から構成する立場を取る一方で，理解するにあたっては 集合と写像（または関手）とを厳密に峻別する ということを徹底することを大事にした．\n例えば集合の合併と共通部分に \\(\\cap,\\cup\\) を用いること，直和と直積に \\(\\coprod,\\prod\\) を用いることは，圏論的な双対性を視覚的に認識しながら数学的議論を進めるためである．(斎藤毅, 2009, p. 37) にも詳しく解説されている．\n記法の開発は数学の重要な一部であると筆者は信じているのである．"
  },
  {
    "objectID": "posts/Surveys/Notations.html#footnotes",
    "href": "posts/Surveys/Notations.html#footnotes",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n集合のなす圏 \\(\\mathrm{Set}\\) は数学の基礎付けとして採用するのに極めて良い性質を持つ nLab．↩︎\n(Del Moral & Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) に一致する．\\(\\equiv,\\overset{\\text{def}}{=}\\) などもよく用いられる．(Crisan & Doucet, 2002), (Smith, 2010) では \\(\\overset{\\triangle}{=}\\) も用いられる．ここでは，これらの左右対称な記号は避けた．また，\\(=_{\\text{df}}\\) などを使うものもある (Quine & Szczotka, 1994)．↩︎\n(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 2) の定め方に一致する．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie & Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie & Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod & Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod & Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod & Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Jacod & Protter, 2012) では \\([x]\\) で表される．↩︎\n(Kuratowski, 1921) による定義である．(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 118), (斎藤毅, 2009, pp. 定義1.3.1 p.15) の定め方に一致する．また \\(n\\)-組を英語では tuple と呼ぶが，全く同じ対象をリスト (list) とも呼ぶ nLab Concept with an Attitude．↩︎\n(Chopin & Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(新井敏康, 2011, p. 119) などでは，\\(f\\restriction_A\\) とも表す．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné & Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(斎藤毅, 2007, pp. 例1.4.7 p.20) に従った．また f.e. とは with a finite number of exceptions の略で，「有限個の例外を除いて成り立つ」という意味である (伊藤清, 1991, p. 124)．↩︎\n(斎藤毅, 2009, p. 179) では \\(F(X)\\) と表記している．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(Billingsley, 1999), (Ethier & Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n(Fritz, 2020, p. 19), (Perrone, 2022) など．Markov圏の稿 も参照↩︎\nnLab の記法に一致する．(斎藤毅, 2020, p. 7) では \\(\\mathrm{Mor}_C(X,Y)\\) と表す．↩︎\n(Del Moral, 2004, p. 7) も参照．↩︎\n(Del Moral & Penev, 2014, p. xlvii), (Bogachev, 2007, p. 277) 4.1.(i) に一致する．(Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie & Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod & Protter, 2012), (Del Moral & Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\n(Del Moral & Penev, 2014, p. xlviii), (Del Moral, 2004, p. 10) の定義に一致する．これは \\(\\prod_{i\\in\\emptyset}X_i\\) が一点集合で，\\(\\coprod_{i\\in I}X_i\\) が空集合である消息の一般化と見れる．なお，集合 \\(X\\) の部分集合の空な族 \\((X_i)_{i\\in\\emptyset}\\) は存在し，それは \\(\\mathrm{Map}(\\emptyset,X_i)\\) のただ一つの元である．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(Bogachev & Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné & Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné & Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart & Wellner, 2023, p. 6) では 外確率 という．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Bogachev & Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(Bogachev, 2007, p. 23) に倣った．(Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie & Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (Bogachev, 2007, p. 189) 参照．↩︎\n(Giné & Nickl, 2021, p. 16), (Bogachev & Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart & Nualart, 2018, p. 8) に倣った．(Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n(Nualart & Nualart, 2018) に倣った．(Giné & Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart & Nualart, 2018, p. 1) に倣った．(Giné & Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral & Penev, 2014), (Dellacherie & Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné & Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart & Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain & Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral & Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné & Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral & Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral & Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n(Jacod & Shiryaev, 2003, p. 347), (Crisan & Doucet, 2002), (Ethier & Kurtz, 1986, p. 96), (Bogachev, 2007, p. 228) に一致する．(Kechris, 1995, p. 109), (Villani, 2009) はイタリックで \\(P(E)\\) と表す．↩︎\n(Pedersen, 1989, p. 72) に倣った．(Bogachev, 2007, p. 76) 第7.2節 では \\(\\mathcal{P}_r(X)\\) で表す．Radon 測度とは，内部正則性（＝緊密性） \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] を満たす Borel 測度をいう (Bogachev, 2007, pp. 68–69) 定義7.1.1, 7.1.4．↩︎\n(Kulik, 2018) が \\(\\mathcal{C}\\) で表すのに習った．(Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) では \\(\\Pi(\\mu,\\nu)\\) で，(Ethier & Kurtz, 1986, p. 96) では \\(\\mathcal{M}(\\mu,\\nu)\\) で，(Dudley, 2002, p. 420) 11.8節 は \\(M(\\mu,\\nu)\\) で表す．(Bogachev, 2007, p. 235) 8.10(viii)節と (Villani, 2009, p. 95) 注6.5 に倣い，カップリングの元は Radon なものに限っている点に注意．↩︎\n(竹村彰道, 2020) の記法に一致する．↩︎\nDirac 測度とも言う．(Jacod & Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) などは \\(\\epsilon_x\\) で表す．(Protter, 2005, p. 299) は Dirac 関数を \\(\\delta_x\\) で表す．↩︎\n(Gerber et al., 2019) の記法に一致．分位点関数 (quantile function) (竹村彰道, 2020, p. 16)，確率表現関数 (森口繁一, 1995) などともいう．(Dudley, 2002, p. 283) は \\(X_F\\) とも表している．↩︎\n可測空間を \\((E,\\mathcal{E})\\) で表すのは，(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) に倣った．↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay & Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford & Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford & Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné & Nickl, 2021, p. 2), (Villani, 2009) に一致する．(Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral & Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie & Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan & Doucet, 2002) に一致する．(Dellacherie & Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Revuz & Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho & Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod & Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan & Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal & van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin & Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie & Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas & Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal & van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf & Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman & Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman & Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie & Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (Bogachev, 2007, p. 262) 4.4節．(Dunford & Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral & Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg & Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart & Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier & Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné & Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart & Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie & Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\mathop{\\mathrm{sgn}}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\mathop{\\mathrm{sgn}}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart & Nualart, 2018, p. 31) に一致する．↩︎\n(Nualart & Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod & Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod & Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod & Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod & Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod & Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie & Meyer, 1978, p. 114) に倣った．↩︎\n右連続性と完備性を併せて，フィルトレーション付き確率空間 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) の 通常の条件 ともいう．(Protter, 2005, p. 3) など参照．↩︎\n(Dellacherie & Meyer, 1978) 49 115-IV では 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n(Jacod & Shiryaev, 2003, p. 4) 1.11，(Protter, 2005, p. 3) に従った．↩︎"
  },
  {
    "objectID": "static/Japanese.html",
    "href": "static/Japanese.html",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学５年一貫博士課程（統計科学コース）２年．\n統計数理研究所 鎌谷研吾 教授の下で，逐次モンテカルロ法 (SMC)，マルコフ連鎖モンテカルロ法 (MCMC) などのサンプリング手法を，マルコフ過程のエルゴード性の観点から研究しています．\n特に，従来の離散時間ベースの SMC と MCMC を連続時間ベースに捉え直すことで，高次元空間上の多峰性分布からも効率的にサンプリングする方法を検討しています．"
  },
  {
    "objectID": "static/Japanese.html#自己紹介",
    "href": "static/Japanese.html#自己紹介",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学５年一貫博士課程（統計科学コース）２年．\n統計数理研究所 鎌谷研吾 教授の下で，逐次モンテカルロ法 (SMC)，マルコフ連鎖モンテカルロ法 (MCMC) などのサンプリング手法を，マルコフ過程のエルゴード性の観点から研究しています．\n特に，従来の離散時間ベースの SMC と MCMC を連続時間ベースに捉え直すことで，高次元空間上の多峰性分布からも効率的にサンプリングする方法を検討しています．"
  },
  {
    "objectID": "static/Japanese.html#経歴",
    "href": "static/Japanese.html#経歴",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "2 経歴",
    "text": "2 経歴\n\n2023.7- 統計数理研究所 リサーチ・アシスタント\n2023.4- 東京大学先端科学技術研究センター 連携研究員"
  },
  {
    "objectID": "static/Japanese.html#学歴",
    "href": "static/Japanese.html#学歴",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "3 学歴",
    "text": "3 学歴\n\n2028.3 総合研究大学院大学 博士（統計科学）（見込み）\n2023.3 東京大学理学部 学士（数学）"
  },
  {
    "objectID": "static/Japanese.html#生い立ち",
    "href": "static/Japanese.html#生い立ち",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "4 生い立ち",
    "text": "4 生い立ち\n1999 年に横浜市磯子区に生まれる．家庭内での会話と小学３年生まで毎年夏休みに北京の実家に帰省していたこともあり，中国語も話せる．国籍はずっと日本．\nやっと物心がついてきた頃，当時「高校生クイズ」なる番組で開成高校が大活躍しており，その世界に入りたいと思うようになる．小学３年生で東京に引っ越し，無事その後開成中学に入学．憧れていたクイズに打ち込む．\n結局クイズ研究部で部長まで務めたが，どうも自分が憧れた存在に近づけているとは思えず，引退後は学問を志すようになる．浪人期に熱中した 原島力学 や ファインマン力学 が面白く，自分も物理学，あわよくば新時代の物理学に当たるものを作りたいと思うようになる．ひとまず大学入学後は理学部物理学科に進もうと思っていた．\n入学後，計算機が世間の潮流だと知り，ここに「新時代の数理科学」が眠っているのではないかと思うようになる．ということで計算機関連の数学がやりたいと思うようになるが，東京大学の 前期教養学部での数学の授業 が始まると「これだ」と直覚する．今後何を究めることになろうと，数学科での学部教育が自分に必須だと思い，計算機の追求は大学院ですることにして入学早々に数学科進学を決意．\n５月の数学科のガイダンスで，「応用に興味ある者にとって，数学科と他の学科との最大の違いは，ここでなら『数学に軸足を置いた応用』ができる」との言葉が背中を押したことも大きい．応用先が曖昧であった自分にとって，「軸足」の語がしっくり来た．入学１ヶ月の時点でもう堅く決意していたため，教養の統計学の授業も一切取らず，数学科目に集中した．数学原論 の出版に伴い開講された 圏と層 に巡り合い，より一層のめり込んでいく．\n\n無事数学科に進学したものの，家庭環境の急変から経済的にも学業的にも休めない状況が続き，大学３年の５月にパニック障害を患う．追試をくらった代数と幾何が半ばトラウマになり，自分は確率統計で食べていくのだと思うようになる．しかし治療に専念していた間でも，Pedersen の関数解析の本 だけは読めた．ずっと 基礎論 や 計算機科学 が好きであったが，解析が面白いと思ったのはこれが初めてだった．\n４年の講究では Nualart の Malliavin 解析の本 を読んだ．３年次は人生を立て直すので精一杯で授業が取れず，確率論は全て独学であったから確率過程の概念に大変難儀したが，確率解析と Malliavin 解析は関数解析の知識が非常に役に立ち，大変に面白かった．\n統計学となるとさらに独学で知識も穴だらけであったから，自分は確率過程とより計算機的な視点から研究の世界に入っていきたいと思っていたところ，指導教員から鎌谷先生の存在と統計計算の分野を知り，統数研の受験を決意する．たまたまこの年は統数研の改組があったため冬の受験のみであり，その頃には病は快方に向かっていたためよく対策できた．\n統数研では初年度から多くの学会に出させてもらったことが，後から思うとかけがえのない経験になった．MCMC の概念をなんとなく把握した程度だった夏の ICIAM の ４日目 のセッションで，「粒子系を輸送する」という最適輸送の見方を取り入れた最適化ベースのサンプリング法を知り衝撃を受けた．サンプリングも確率分布の空間 \\(\\mathcal{P}(X)\\) 上の 力学系と見れる と気づいて大変興味を持った．\nこれで機械学習というものを一気に身近に感じて本格的に興味を持ち，年度末に機械学習分野最大のサマースクール MLSS2024 に参加するため，なんとかポスター発表を用意した．今思うとこれがその後の方向性を決定付けた．誘ってもらった清水さんには感謝が尽きない．\nサマースクールでは全く知らない・興味のない分野の授業も聞くことになり，これが思いもしなかった出会いに導いてくれるのが美点である．特に Francesco Orabona 氏の最適化の授業と Marco Cuturi 氏の最適輸送の授業とが，最適化とサンプリングの双対性という統一的な観点に導いてくれた．また，ポスターを通じて Omar と知り合った出会いも大きい．本サイトのプロフィール写真も MLSS にて 黒木さん に撮ってもらったものである．\n２年目は MCMC をはじめとした Monte Carlo 法の総体を，物理学から徹底的に学ぶことにした．これが図らずも機械学習の数理に対する深い洞察を授けてくれた．"
  },
  {
    "objectID": "static/Japanese.html#言語",
    "href": "static/Japanese.html#言語",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "5 言語",
    "text": "5 言語\n\n日本語（母語）\n中国語（母語レベル）\n英語（TOEFL iBT 100）"
  },
  {
    "objectID": "static/Materials.html",
    "href": "static/Materials.html",
    "title": "Sessions",
    "section": "",
    "text": "ポスター発表 | Poster Sessions\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n統数研オープンハウス\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 24th, 2024\nISM and online (Hybrid)\n\n\n\n\n\n\n\nオープンハウス案内\n\n\n発表ポスター\n\n\n\nMLSS2024\n\n\n\n\n\n\n\n\nDates\nLocation\n\n\n\n\nMar. 4-15, 2024\nOIST, Okinawa, Japan\n\n\n\n\nThe Machine Learning Summer School (despite its name, it was held in spring due to typhoon considerations) offered me my first opportunity to present in an academic setting.\n\n\n\nGroup Photo at MLSS2024\n\n\n\n\n\nMy Poster: A Recent Development of Particle Filter\n\n\n\n\n\n\n勉強会 | Study Group\n\n\n法律家のための統計数理\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n数学と法学，双方からの交流と理解を図ります．\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nEmpirical Process Theory\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\nTextbook：Kengo Kato Empirical Process Theory (Lecture Note)\n\n\n\n担当分の発表資料\n\n\n\n\n\n\nその他 | Others\n\n\n学振 DC1\n\n\n\n\n\n\n\n\nPeriod\nApplication Category\n\n\n\n\nSpring, 2024\n12040 応用数学および統計数学関連\n\n\n\n\n\n\n\n応募資料"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html",
    "href": "posts/2024/AI/Semiconductor_slides.html",
    "title": "半導体入門",
    "section": "",
    "text": "半導体とは？なぜ重要なのか？\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n\n\n\n\n21世紀に入った時点で，大きく分けて 18 の半導体デバイスが存在する (Ng, 2002)．\n細かいものを含めると 140．\n全てトランジスタの組み合わせからなる．\n\n\n\n\n\n世界の半導体出荷額は直近 10 年で２倍になっている\n\n\n\n\n\n\n\n\n\n物質としての半導体\n\n\n\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．1\n\n\n\n\n\nこのような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．2 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．\n\n\n\n(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，3 その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える (Sze & Lee, 2012, p. 1)．4\nBraun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．\n\n\n\n半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した ウエハ (wafer) 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．\n\n\n\nチップの断面構造\n\n\n一つのウエハから多数のチップが作成され，その各単位を ダイ (die) ともいう．\nしかし，普段我々が目にする IC チップ は パッケージ されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．\n32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．5"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#footnotes",
    "href": "posts/2024/AI/Semiconductor_slides.html#footnotes",
    "title": "半導体入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Böer & Pohl, 2018, p. 4), (Huebener, 2019, p. 73) Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う (Madelung, 1978, p. 17)．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である (Madelung, 1978, p. 211)．↩︎\n(Huebener, 2019, p. 75)．↩︎\n(Huebener, 2019, p. 73) 特に伝導体と半導体の境界部分で強く見られた．↩︎\n現代では，このような接合を金属-半導体接合 (metal-semiconductor contact) または Schottky 接合 といい，Ohmic 接合と対比する．↩︎\n(Patterson & Hennessy, 2014, p. 27) も参照．↩︎\n一方で GaAs の形成は Bridgman 法 (Bridgman, 1925) による．最も，この化合物が半導体であると発見されたのは (Welker, 1952) になってようやくのことである．(Sze & Lee, 2012, p. 6) も参照．↩︎\n(Sze & Lee, 2012, p. 6) など．↩︎"
  },
  {
    "objectID": "static/Categories.html",
    "href": "static/Categories.html",
    "title": "Categories",
    "section": "",
    "text": "Mathematics\n\n\nProbability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nComputation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nProcess\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nComputation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nFunctional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nApplied Mathematics\n\n\n\\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOptimization\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nOptimization\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\nOptimization\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nNature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nBayesian Machine Learning\n\n\nBayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nParticles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nKernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nBayesian Computation\n\n\nComputation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nComputation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nComputation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nPython\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nJulia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\n\nArtificial Intelligence\n\nAI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nDeep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n数学者のための深層学習７\n\n\nエネルギーベースモデル\n\n\n\nDeep\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習５\n\n\n拡散モデル\n\n\n\nDeep\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習６\n\n\n正規化流\n\n\n\nDeep\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nFoundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nOthers\n\n\nReview\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\nReview\n\n\n\n\n2023-10-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSurveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nSurvey\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nLife\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nLifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nSeries\n\n\n法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html",
    "href": "static/PartialCategories.html",
    "title": "Categories",
    "section": "",
    "text": "Mathematics\n\n\nProbability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSimulation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nProcess\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nSimulation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nFunctional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nApplied Mathematics\n\n\n\\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\nOptimization\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\nNature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nBayesian Machine Learning\n\n\nBayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nParticles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nKernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nBayesian Computation\n\n\nComputation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSimulation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\nProcess\n\n\nSimulation\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSimulation\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nPython\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nJulia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items\n\n\n\n\n\nArtificial Intelligence\n\nAI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nDeep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nFoundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nOthers\n\n\nReview\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nNature\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\nParticles\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\nReview\n\n\n\n\n2023-10-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSurveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\nSurveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nLife\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nLifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nSeries\n\n\n法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#interest",
    "href": "index.html#interest",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "4 Interest",
    "text": "4 Interest\nI am deeply fascinated by\n\nProbabilistic Modelling, a domain dedicated to developing machine learning models that can simultaneously capture both structure and uncertainty in data in a principled manner, as well as\nBayesian Computation, which focuses on developing and improving computational methods for Bayesian inference and learning.\n\nMy current interest lies in understanding and acquiring unified perspectives on sampling and optimization as a flow on the space of probability measures, \\(\\mathcal{P}(X)\\).\nAt my core, I am an applied mathematician, driven by a desire to foster mutual understanding among humans, nature, and computers, utilizing mathematics as a common language. As we are still in the process of learning, it is remarkable that (probability) measures and (Markov) kernels have proven to be unreasonably effective in describing the world models of all three aforementioned entities.\n\nThe true spirit of delight, the exaltation, the sense of being more than Man, which is the touchstone of the highest excellence, is to be found in mathematics as surely as in poetry. – Bertrand Russell (1959) My Philosophical Development"
  },
  {
    "objectID": "static/Notations.html",
    "href": "static/Notations.html",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "",
    "text": "1 集合\n  \n  1.1 集合\n  1.2 数\n  1.3 組\n  1.4 写像\n  1.5 圏\n  1.6 関数\n  1.7 演算\n  \n  2 空間\n  \n  2.1 位相\n  2.2 線型空間\n  2.3 Banach 空間\n  2.4 可測空間\n  2.5 確率空間\n  2.6 確率分布\n  \n  3 核\n  \n  3.1 測度\n  3.2 確率核\n  3.3 関数の空間\n  3.4 変形\n  3.5 作用素\n  \n  4 解析\n  \n  4.1 微分作用素\n  4.2 Fourier変換\n  4.3 超関数\n  4.4 確率解析\n  \n  5 過程\n  \n  5.1 確率変数の収束\n  5.2 確率過程\n  5.3 停止時"
  },
  {
    "objectID": "static/Notations.html#sec-set",
    "href": "static/Notations.html#sec-set",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "1 集合",
    "text": "1 集合\n\n$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\nここでは，あらゆる数学概念は，ZFC公理系 の下で集合として定義する．1 記号 \\(:=\\) は「右辺によって左辺を定義し，その結果等号が成り立つ」という主張の略記である．2\n\n1.1 集合\n\n空集合 を \\[\n\\emptyset:=\\{x\\mid x\\ne x\\}\n\\] で表す．3\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．4\n\\(A,B\\subset X\\) の 差 を \\[\nA\\setminus B:=\\left\\{a\\in A\\mid a\\notin B\\right\\}\n\\] で表す．\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cup B\\) と同じ数学的対象であるが，同時に \\(A\\cap B\\) という事実も主張するものとする．5\n対称差 を \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\] で表す．6\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．7 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．8\n例えば，\\(X\\in\\mathcal{L}(\\Omega)\\) を実確率変数，\\(A\\in\\mathcal{B}(\\mathbb{R})\\) を Borel 集合とすると， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] という略記を用いる．\n\n\n\n1.2 数\n\n自然数 を \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\},\n\\] によって帰納的に定義する．9\n自然数の集合を表すため，次の記法を用意する：10 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，11 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す： \\[\n\\mathbb{R}_+=[0,\\infty),\\quad\\mathbb{R}^+=(0,\\infty).\n\\]\n部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．12\n実数 \\(x\\in\\mathbb{R}\\) に対して，その整数部分を \\[\n\\lfloor x\\rfloor:=\\max\\{n\\in\\mathbb{Z}\\mid n\\le x\\}\n\\] と表す．13\n\n\n\n1.3 組\n\n\\(n\\)-組 を次のように帰納的に定める：14 \\[\n(x_1,x_2):=\\{\\{x_1\\},\\{x_1,x_2\\}\\},\n\\] \\[\n(x_1,\\cdots,x_n):=(x_1,(x_2,\\cdots,x_n)).\n\\]\n自然数の組を表すため，次の記法を用意する：15 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組を \\[\nX_{1:N}:=(X_1,\\cdots,X_N)\n\\] と表す．16\n\n\n\n1.4 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n写像 \\(f\\) の 値域 を \\[\\mathrm{Im}\\,f:=f(X)\\] で表す．\n\\(A\\subset X\\) への 制限 を \\(f|_A:A\\to Y\\) と表す．17\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．18\n\\(f_*\\) は部分集合 \\(A\\subset X\\) を像 \\(f(A)\\subset Y\\) に対応させる写像 \\[\nf_*:P(X)\\to P(Y)\\] と定義する．\n同様に写像 \\(f^*:P(Y)\\to P(X)\\) を定める： \\[\nf^*(B)=f^{-1}(B)\\quad(B\\subset Y).\n\\]\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．19\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．20\n写像 \\(f:X\\to Y\\) のうち，有限個の元を除いて \\(f(x)=0\\) を満たすものがなす全体を \\[\nY^{(X)}:=\\left\\{f\\in Y^X\\mid f=0\\;\\;\\text{f.e.}\\right\\}\n\\] と表す．21\n\\(P(X)\\) を \\(2^X\\) と同一視する．特に，\\(X\\) の有限部分集合の全体を \\[\n2^{(X)}=\\left\\{A\\in P(X)\\:\\middle|\\: A\\overset{\\text{finite}}{\\subset}X\\right\\}\n\\] と表す．22\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．23\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\[\n\\mathrm{pr}_i:\\prod_{i\\in I}X_i\\twoheadrightarrow X_i\n\\] で表す．24\n\\(x\\in X\\) での 評価写像 を \\[\n\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\n\\] で表す．25\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．\nしかしこの写像の値域も 族 と呼び，この場合は \\[\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\] と表す．26\n特に \\(I=\\mathbb{N}\\) のときは 列 ともいう．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．27\n\n\n\n1.5 圏\n\n集合の圏 を \\(\\mathrm{Set}\\) で表す．\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X)\n\\] を表す．28\n確率空間と確率核の圏を \\(\\mathrm{Stoch}\\) で表す．29\n圏 \\(C\\) の対象 \\(X,Y\\in C\\) の間の 射 の全体を \\(\\mathrm{Hom}_C(X,Y)\\) で表す．30\n特に \\[\nY^X=\\mathrm{Map}(X,Y)=\\mathrm{Hom}_\\mathrm{Set}(X,Y).\n\\]\n圏 \\(C\\) の対象 \\(X\\in C\\) の自己射の全体を \\[\n\\mathrm{End}_C(X):=\\mathrm{Hom}_C(X,X)\n\\] で表す．\nそのうち可逆なもののなす部分集合を \\(\\mathrm{Aut}_C(X)\\) で表す．集合 \\([n]\\) の 置換群 は \\(\\mathrm{Aut}_\\mathrm{Set}([n])\\) と表せる．\n\n\n\n1.6 関数\n\n\\(R\\) を環とする．\\(f_1,f_2\\in R^X\\) に対して， \\[\n(f_1+f_2)(x):=f_1(x)+f_2(x),\n\\] \\[\n(f_1f_2)(x):=f_1(x)f_2(x)\n\\] で定める演算により \\(R^X\\) も環とみなし，定値関数 \\(f\\equiv a\\in R\\) を \\(R\\) の元と同一視する．31\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を \\[\na\\lor b:=\\sup\\{a,b\\},\n\\] \\[\na\\land b:=\\inf\\{a,b\\},\n\\] で表す．32\n\\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right)\n\\] と表す．33\n\\(0\\) を持つ束においては次の略記を使う：34 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序集合 \\(Y\\) に値を取る関数 \\(f,g\\in\\mathrm{Map}(X,Y)\\) について，\\(f\\le g\\) とは \\[\n\\forall_{x\\in X}\\;f(x)\\le g(x)\n\\] の略記とする．\n同じ条件を，一階の量化記号 \\(\\forall\\) を省略して \\[\nf(x)\\le g(x)\\quad(x\\in X)\n\\] または \\(f\\le g\\) とも略記する．\n\\(Y\\) が束であるとき，この順序により関数の空間 \\(\\mathrm{Map}(X,Y)\\) は束となり，演算 \\(\\land,\\lor\\) が定まる．\n関数の列 \\(\\{f_n\\}\\subset Y^X\\) について，\\(f_n\\nearrow f\\) とは，収束 \\(f_n\\to f\\) だけでなく，\\(\\{f_n\\}\\) が単調増加であることも含意する．35\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\[\nO(g(x))\\;(x\\to x_0)\\] とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．36\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の意味でも使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0\n\\] を満たすこととする．\n\n\n\n1.7 演算\n\n次の演算規則を約束する：37 \\[\n\\prod_\\emptyset=1,\\quad\\sum_{\\emptyset}=0.\n\\]\n\\(0!=1\\) とする．"
  },
  {
    "objectID": "static/Notations.html#sec-space",
    "href": "static/Notations.html#sec-space",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "2 空間",
    "text": "2 空間\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．38\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．39\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．\n閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}\n\\] で表す．\n\n\n\n2.2 線型空間\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．40\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．41\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}\n\\] とも表す．42\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，43 共役転置を \\(A^*\\) で表す．44 \\(\\mathbb{F}=\\mathbb{C}\\) の場合は \\(A^\\top=A^*\\)．\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．45\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について， \\[\n\\begin{align*}\n  A&+B\\\\\n  &\\quad:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n  \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\},\n\\end{align*}\n\\] と表す．46\n集合 \\(A\\subset X\\) の 凸包 を \\(\\mathop{\\mathrm{Conv}}(A)\\) で表す．47\n集合 \\(A\\subset X\\) が生成する部分空間を \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x\n\\] で表す．48\n内積を \\((-|-)\\) で表す．49\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を50 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\mathop{\\mathrm{Tr}}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}\n\\] で表す．51\n\n\n\n2.3 Banach 空間\n\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．52 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n特に \\(J\\) が有限であるとき， \\[\n\\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n\\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．53\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．54\n距離空間 \\((T,d)\\) の 開球 を \\[\n\\begin{align*}\n  U_\\epsilon(t)&:=U(t;\\epsilon)\\\\\n  &:=\\left\\{s\\in T\\mid d(s,t)&lt;\\epsilon\\right\\}\n\\end{align*}\n\\] で表す．55\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．56\n単位閉球を \\(B:=B(0;1)\\) で表す．\n\\(\\mathbb{R}^n\\) のものである場合は特に \\(B^n\\) とも表す．57\n\\(\\mathbb{R}^n\\) の標準基底を \\[\ne_i=(0,\\cdots,0,1,0,\\cdots,0)\n\\] と表す．58\nBanach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．59\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}\n\\] で表す．60\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) として理解できる数学的対象の別記法と捉えられるように設計する．61\n\n\n2.4 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．62\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．63\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．64\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．65\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．66\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．67\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．68\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．69\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．70\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．71 \\(\\gamma_n:=\\mathop{\\mathrm{N}}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n2.5 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．72 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．73\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．74\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．75\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．76\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．77\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．78\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．79\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．80\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．81\n\n\n2.6 確率分布\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 確率測度 の全体を \\(\\mathcal{P}(E,\\mathcal{E})\\) と書く．\\(E\\) が位相空間であるとき，Borel 確率測度の全体を \\(\\mathcal{P}(E)\\) と略記する．82\n\\(E\\) を位相空間とする．\\((E,\\mathcal{B}(E))\\) 上の Radon 確率測度 の全体を \\[P(E)\\subset\\mathcal{P}(E)\\] で表す．83\n2つの確率分布 \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) の カップリング の全体を \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\] で表す．84\n\\(d\\)-次元 正規分布 を \\[\\mathop{\\mathrm{N}}_d(\\mu,\\Sigma)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．85\n集合 \\(A\\subset\\mathbb{R}^d\\) 上の 一様分布 を \\[\\mathrm{U}(A)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．\n点 \\(x\\in E\\) 上の Delta 測度 を \\(\\delta_x\\) で表す．86\n確率変数 \\(X\\sim\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の 分布関数 を \\[\n\\begin{align*}\n  F_X(a)&:=F_\\nu(a)\\\\\n  &:=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d]\\\\\n  &\\quad(a=a_{1:d}\\in\\mathbb{R}^d)\n\\end{align*}\n\\] で表す．\n\\(d=1\\) のとき，その一般化逆を \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\}\n\\] \\[\n(u\\in(0,1)^d)\n\\] で表す．87"
  },
  {
    "objectID": "static/Notations.html#sec-kernel",
    "href": "static/Notations.html#sec-kernel",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 核",
    "text": "3 核\n空間を導入した次は，その射を定義せねばなるまい．\n本節では，\\((E,\\mathcal{E})\\) を 可測空間 とする．88\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 89\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 90\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．91 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．92\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．93\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．94\n\n\n\n3.2 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：95\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．96 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．97\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 98\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．99\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．100\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．101\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．102\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．103\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.3 関数の空間\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．104\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．105\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．106\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．107 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．108\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．109\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．110\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．111\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．112\n\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．113\n\n\n3.4 変形\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．114\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．115\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．116\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．117\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．118\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．\n\n\n\n\n3.5 作用素\n\\(E,F\\) をノルム空間とする．\n\n作用素 \\(T:E\\to F\\) と言ったとき，線型写像 \\(T:E\\to F\\) を指すこととする．119\n\\(E\\) 内の作用素 \\(T:E\\supset\\mathcal{D}(T)\\to F\\) と言ったとき，ある \\(E\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to F\\) を指すこととする．120\n有界作用素の全体を \\(B(E,F)\\) で表す．121 \\(B(E):=B(E,E)\\) とする．\n連続作用素の全体を \\(L(E,F)\\) で表す．122"
  },
  {
    "objectID": "static/Notations.html#sec-analysis",
    "href": "static/Notations.html#sec-analysis",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 解析",
    "text": "4 解析\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．123\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．124\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．125\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．126 \\(m=1\\) のとき， \\[\n\\mathop{\\mathrm{\\mathrm{grad}}}u:=\\nabla u:=\\left(\\frac{\\partial u}{\\partial x_1},\\cdots,\\frac{\\partial u}{\\partial x_n}\\right)\n\\] とも表す．\n発散 を \\[\n\\mathop{\\mathrm{div}}u:=\\mathop{\\mathrm{Tr}}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．127\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n\\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n\\] と同一視する．128\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n\\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\mathop{\\mathrm{Tr}}(D^2u)\n\\] で定める．\n\n\n\n4.2 Fourier変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．129\n符号関数 を \\[\n\\mathop{\\mathrm{sgn}}(x):=2H(x)-1\n\\] で定める．130\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 超関数\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．131 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．132\n\n\n\n4.4 確率解析\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\\\\n    &\\qquad\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．133"
  },
  {
    "objectID": "static/Notations.html#sec-process",
    "href": "static/Notations.html#sec-process",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "5 過程",
    "text": "5 過程\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率変数の収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．134\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．135\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n\n\n\n5.2 確率過程\n（確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．136\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\(E^T\\) に定める写像 \\[\nX_-:\\Omega\\to E^T\n\\] を 転置 と呼ぶ．137\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいい，このような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．138\n\\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．139 ただし，\\(x(0-)=x(0)\\) とする．140\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の フィルトレーション \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系 \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．141\n加えて， \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\] と表す．142\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上のフィルトレーション \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) が完備性 \\[\n\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\n\\] を満たすとき，4-組 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) を 確率基底 という．143\n\n\n\n5.3 停止時144\n\n確率基底 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) 上の 停止時 とは，同じ確率空間 \\(\\Omega\\) 上の可測関数 \\(T:\\Omega\\to[0,\\infty]\\) であって， \\[\n\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t,\\qquad t\\in\\mathbb{R}_+,\n\\] も満たすものをいう．145\n停止時 \\(T\\) までの 情報 とは， \\[\n\\mathcal{F}_T:=\\left\\{A\\in\\mathcal{F}_\\infty\\mid\\forall_{t\\in\\mathbb{R}_+}\\;A\\cap\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t\\right\\}\n\\] で定まる \\(\\sigma\\)-代数をいう．"
  },
  {
    "objectID": "static/Notations.html#終わりに",
    "href": "static/Notations.html#終わりに",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "終わりに",
    "text": "終わりに\n\n本サイトの記法で筆者が最も注意することは，あらゆる記法を背後の数学的消息と調和するように定義するということであった．\nこれにあたり，あらゆる 数学的対象 を集合から構成する立場を取る一方で，理解するにあたっては 集合と写像（または関手）とを厳密に峻別する ということを徹底することを大事にした．\n例えば集合の合併と共通部分に \\(\\cap,\\cup\\) を用いること，直和と直積に \\(\\coprod,\\prod\\) を用いることは，圏論的な双対性を視覚的に認識しながら数学的議論を進めるためである．(斎藤毅, 2009, p. 37) にも詳しく解説されている．\n記法の開発は数学の重要な一部であると筆者は信じているのである．"
  },
  {
    "objectID": "static/Notations.html#footnotes",
    "href": "static/Notations.html#footnotes",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n集合のなす圏 \\(\\mathrm{Set}\\) は数学の基礎付けとして採用するのに極めて良い性質を持つ nLab．↩︎\n(Del Moral & Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) に一致する．\\(\\equiv,\\overset{\\text{def}}{=}\\) などもよく用いられる．(Crisan & Doucet, 2002), (Smith, 2010) では \\(\\overset{\\triangle}{=}\\) も用いられる．ここでは，これらの左右対称な記号は避けた．また，\\(=_{\\text{df}}\\) などを使うものもある (Quine & Szczotka, 1994)．↩︎\n(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 2) の定め方に一致する．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie & Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie & Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod & Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod & Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod & Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Jacod & Protter, 2012) では \\([x]\\) で表される．↩︎\n(Kuratowski, 1921) による定義である．(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 118), (斎藤毅, 2009, pp. 定義1.3.1 p.15) の定め方に一致する．また \\(n\\)-組を英語では tuple と呼ぶが，全く同じ対象をリスト (list) とも呼ぶ nLab Concept with an Attitude．↩︎\n(Chopin & Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(新井敏康, 2011, p. 119) などでは，\\(f\\restriction_A\\) とも表す．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné & Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(斎藤毅, 2007, pp. 例1.4.7 p.20) に従った．また f.e. とは with a finite number of exceptions の略で，「有限個の例外を除いて成り立つ」という意味である (伊藤清, 1991, p. 124)．↩︎\n(斎藤毅, 2009, p. 179) では \\(F(X)\\) と表記している．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(Billingsley, 1999), (Ethier & Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n(Fritz, 2020, p. 19), (Perrone, 2022) など．Markov圏の稿 も参照↩︎\nnLab の記法に一致する．(斎藤毅, 2020, p. 7) では \\(\\mathrm{Mor}_C(X,Y)\\) と表す．↩︎\n(Del Moral, 2004, p. 7) も参照．↩︎\n(Del Moral & Penev, 2014, p. xlvii), (Bogachev, 2007, p. 277) 4.1.(i) に一致する．(Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie & Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod & Protter, 2012), (Del Moral & Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\n(Del Moral & Penev, 2014, p. xlviii), (Del Moral, 2004, p. 10) の定義に一致する．これは \\(\\prod_{i\\in\\emptyset}X_i\\) が一点集合で，\\(\\coprod_{i\\in I}X_i\\) が空集合である消息の一般化と見れる．なお，集合 \\(X\\) の部分集合の空な族 \\((X_i)_{i\\in\\emptyset}\\) は存在し，それは \\(\\mathrm{Map}(\\emptyset,X_i)\\) のただ一つの元である．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(Bogachev & Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné & Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné & Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart & Wellner, 2023, p. 6) では 外確率 という．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Bogachev & Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(Bogachev, 2007, p. 23) に倣った．(Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie & Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (Bogachev, 2007, p. 189) 参照．↩︎\n(Giné & Nickl, 2021, p. 16), (Bogachev & Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart & Nualart, 2018, p. 8) に倣った．(Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n(Nualart & Nualart, 2018) に倣った．(Giné & Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart & Nualart, 2018, p. 1) に倣った．(Giné & Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral & Penev, 2014), (Dellacherie & Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné & Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart & Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain & Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral & Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné & Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral & Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral & Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n(Jacod & Shiryaev, 2003, p. 347), (Crisan & Doucet, 2002), (Ethier & Kurtz, 1986, p. 96), (Bogachev, 2007, p. 228) に一致する．(Kechris, 1995, p. 109), (Villani, 2009) はイタリックで \\(P(E)\\) と表す．↩︎\n(Pedersen, 1989, p. 72) に倣った．(Bogachev, 2007, p. 76) 第7.2節 では \\(\\mathcal{P}_r(X)\\) で表す．Radon 測度とは，内部正則性（＝緊密性） \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] を満たす Borel 測度をいう (Bogachev, 2007, pp. 68–69) 定義7.1.1, 7.1.4．↩︎\n(Kulik, 2018) が \\(\\mathcal{C}\\) で表すのに習った．(Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) では \\(\\Pi(\\mu,\\nu)\\) で，(Ethier & Kurtz, 1986, p. 96) では \\(\\mathcal{M}(\\mu,\\nu)\\) で，(Dudley, 2002, p. 420) 11.8節 は \\(M(\\mu,\\nu)\\) で表す．(Bogachev, 2007, p. 235) 8.10(viii)節と (Villani, 2009, p. 95) 注6.5 に倣い，カップリングの元は Radon なものに限っている点に注意．↩︎\n(竹村彰道, 2020) の記法に一致する．↩︎\nDirac 測度とも言う．(Jacod & Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) などは \\(\\epsilon_x\\) で表す．(Protter, 2005, p. 299) は Dirac 関数を \\(\\delta_x\\) で表す．↩︎\n(Gerber et al., 2019) の記法に一致．分位点関数 (quantile function) (竹村彰道, 2020, p. 16)，確率表現関数 (森口繁一, 1995) などともいう．(Dudley, 2002, p. 283) は \\(X_F\\) とも表している．↩︎\n可測空間を \\((E,\\mathcal{E})\\) で表すのは，(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) に倣った．↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay & Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford & Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford & Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné & Nickl, 2021, p. 2), (Villani, 2009) に一致する．(Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral & Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie & Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan & Doucet, 2002) に一致する．(Dellacherie & Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Revuz & Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho & Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod & Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan & Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal & van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin & Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie & Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas & Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal & van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf & Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman & Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman & Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) や (Protter, 2005, p. 52) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie & Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (Bogachev, 2007, p. 262) 4.4節．(Dunford & Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral & Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg & Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart & Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier & Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné & Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart & Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie & Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\mathop{\\mathrm{sgn}}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\mathop{\\mathrm{sgn}}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart & Nualart, 2018, p. 31) に一致する．↩︎\n(Nualart & Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod & Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod & Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod & Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod & Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod & Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie & Meyer, 1978, p. 114) に倣った．↩︎\n右連続性と完備性を併せて，フィルトレーション付き確率空間 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) の 通常の条件 ともいう．(Protter, 2005, p. 3) など参照．↩︎\n(Dellacherie & Meyer, 1978) 49 115-IV では 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n(Jacod & Shiryaev, 2003, p. 4) 1.11，(Protter, 2005, p. 3) に従った．↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Notations | Categories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための統計力学２\n\n\n小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の数学的実体を速習することを目指す．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための統計力学１\n\n\nIsing 模型\n\n\n\nNature\n\n\nDeep\n\n\n\n数学者のために，統計力学の場面設定を数学的に理解することを試みる．統計力学の最も代表的なモデルである Ising 模型の数理も概観する．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とシミュレーション\n\n\n\nComputation\n\n\nSimulation\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションにより計算を実行する手段である． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習２\n\n\nトランスフォーマー\n\n\n\nDeep\n\n\n\n2023年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れ，近年の発展を概観する．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習４\n\n\n生成モデル VAE\n\n\n\nDeep\n\n\n\n深層生成モデルの１つ VAE は，統計モデルとしては変分 Bayes 推論アルゴリズムであり，変分下界をニューラルネットワークによって近似するというアプローチである．\n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習１\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習３\n\n\n生成モデル GAN\n\n\n\nDeep\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である \\(K\\)-平均法の説明から始める．\\(K\\)-平均法は第一義的にはモデルフリーの（確率論と関係のない）クラスタリングアルゴリズムである．\n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\n\n\n\n\nProcess\n\n\nSimulation\n\n\n\n純粋跳躍過程の生成作用素を調べる．確率核 \\(\\mu\\) と到着強度 \\(\\lambda\\) という２つのパラメータは，それぞれジャンプ先を定める Markov 過程の遷移核と，ジャンプの起こりやすさを表す指数待ち時間のパラメータに対応する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書）第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets | Minkowski 和の可測性\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode による LaTeX 執筆環境 | LaTeX authoring environment with VSCode\n\n\n\n\n\n\nLifestyle\n\n\n\nVSCode で LaTeX を執筆するためのコツを収集していきます．A page collecting tips to author technical documents with VSCode. Also including a starting guide.\n\n\n\n\n\n12/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワーク，マルコフネットワーク，ファクターグラフ\n\n\n\nBayesian\n\n\nComputation\n\n\n\n数学者のために，PGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える，計算幾科学的技術であるとみなせる．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\n\n\n\nNature\n\n\nReview\n\n\n\n\n\n\n\n\n\n12/08/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSimulation\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\n\n\n\n\nLife\n\n\n\n筆者の数学を形作った書籍を編年体で紹介する．\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則 | Gamma分布とBeta分布を例に\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSimulation\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\nParticles\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\nParticles\n\n\n\n1.1節”On the Origins of Feynman-Kac and Particle Models”を翻訳\n\n\n\n\n\n11/08/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたものTop5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nQuartoはじめて良かった | Quarto Basics in Japanese\n\n\n\n\n\n\nLifestyle\n\n\n\nQuartoのチュートリアル＋紹介＋おすすめポイント\n\n\n\n\n\n11/04/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\n\n\n\nReview\n\n\n\n\n\n\n\n\n\n10/29/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#概要",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#概要",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "1 概要",
    "text": "1 概要\n\nマルチンゲールによる導出\nLyapunov 型の条件が簡単に十分条件を与えるような，再帰時刻による取り扱い\n\n(Kulik, 2018, pp. 45–45) では，安定性の定理の証明に，マルチンゲールを用いた議論を用いている．エルゴード性を持つ Markov 連鎖は必ず\n\n局所的な集合上で良い攪拌性を持ち，\nその他の点に行ってしまった場合でも，「十分早く」その局所的な集合に戻ってくる\n\nという２つのモードを持つ．これを別々に解析する見通しの良い議論を与えてくれるのがマルチンゲールによる議論であるとしている (Kulik, 2018, p. 71) が，似たような議論をしているのが本論文 (Butkovsky & Veretennikov, 2013) である．1"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#記法",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#記法",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "2 記法",
    "text": "2 記法\n\n一様エルゴード性を strongly ergodic とも呼んでいる： \\[\n\\sup_{x\\in E}\\|P^n(x,-)-\\pi\\|_\\mathrm{TV}\\le Ce^{-\\lambda n}.\n\\]\n一方で，各点 \\(x\\in E\\) で \\[\n\\|P^n(x,-)-\\pi\\|_\\mathrm{TV}\\to0\n\\] が成り立つことを weakly ergodic と呼んでいる．\n\n本論文では，\\(\\lambda\\) を推定する (Diaconis & Stroock, 1991) 理論を，weakly ergodic の場合と非対称な場合に拡張する．\nすると，(Diaconis & Stroock, 1991) 理論では遷移確率核 \\(P\\) のスペクトルギャップであった \\(\\lambda\\) は，一般の設定の下でもある一般化した半群生成作用素のスペクトル半径に関係することがわかった．\n\n2.1 (Diaconis & Stroock, 1991) 理論\n一様エルゴード性の収束速度 \\(\\lambda\\) を定量化するアプローチの１つ．\n\n\n\n\n\n\n定理\n\n\n\n有限状態空間 \\(E\\) 上の \\(P\\)-一様 Markov 連鎖は，既約かつ対称ならば， \\[\n\\lambda&lt;\\log\\operatorname{Gap}(P)\n\\] \\[\n\\operatorname{Gap}(P):=\\max\\left\\{\\lvert\\lambda\\rvert\\in\\mathbb{R}_+\\mid 1&gt;\\lambda\\in\\mathrm{Sp}(P)\\right\\}\n\\]\n\n\n\n対称ならば \\(P=P^*\\)．\nスペクトルギャップは一般の正作用素に定義できる．\n\n\n\n2.2 (Vaserstein, 1969) による最適カップリングの構成\nこれを一般化したという．\n最適な Markov カップリングよりも，カップリング確率が高いカップリングがあるらしい（しかし Markov にならない）．この稿 に書いた．\n\n\n2.3 Lyapunov 型の条件\n(Douc et al., 2004), (Kalashinikov, 1973), (Lamperti, 1960), (Rosenthal, 2002), (Tweedie, 1981) による．"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#footnotes",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#footnotes",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nそれと (Durmus et al., 2016) を挙げている．↩︎\n(Butkovsky & Veretennikov, 2013, pp. 3521–3522) 補題2.2↩︎\n(Butkovsky & Veretennikov, 2013) 定理2.1．↩︎\n前節から判る通り，局所Dobrushin条件を満たす集合（small set ともいう）上では常に指数収束である．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Coupling.html#導入",
    "href": "posts/2024/Process/Coupling.html#導入",
    "title": "確率測度のカップリング",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 歴史\nカップリングの技術を最初に使ったのは Lévy の下で博士号を取得した年に発表された (Doeblin, 1938) だと言われている．1 Doeblin はその２年後に 25 歳で戦死する．2\nその後，(Doob, 1953), (Vaserstein, 1969), (Griffeath, 1975b), (Pitman, 1976), (Nummelin, 1984) などによって発展するが，Markov 連鎖のエルゴード性の解析に体系的に用いられるようになったのは極めて最近である．(Kulik, 2018) 参照．\n最終的にカップリングが成功する確率が最大になるようなカップリングが存在し，(Pitman, 1976), (Griffeath, 1975a) は maximal coupling と呼ぶが，これは必ず Markov 連鎖にならないことがわかっている．\n一方で，(Vaserstein, 1969) が考えたような「各段階でカップリング確率が最大になる」ようなカップリングは Markov 連鎖になり，これを 最適 Markov カップリング または (Kulik, 2018, p. 33) は greedy coupling などと呼ぶ．\n(Nummelin, 1984), (Lindvall, 1992) などが発展させた手法は generalized regeneration というもので，エルゴード性の分析とは少し方向が違う．\n\n\n1.2 定義\n\n\n1.3 概要\nカップリングにより，\\(\\mathcal{P}(E)\\) 上の２つの力学系 \\(\\{(P^*)^n\\mu\\}_{n\\in\\mathbb{N}},\\{(P^*)^n\\nu\\}_{n\\in\\mathbb{N}}\\) の距離を比べることが可能になる．\n\\(\\mu\\) を不変確率分布にとれば，\\(\\{(P^*)^n\\mu\\}_{n\\in\\mathbb{N}}\\) はその上でもはや動かない１点からなる力学系になるため，その場合はエルゴード速度に関する示唆を得られる．\n一般の場合は，\\(\\mathcal{P}(E)\\) はデルタ測度を極点に持つ凸集合であるから，\\(\\mu=\\delta_x,\\nu=\\delta_y\\) という場合を取れば良い．その際は，「スタート地点を忘れる速度」という意味で，Markov 連鎖の忘却速度を特徴付けることができる．"
  },
  {
    "objectID": "posts/2024/Process/Coupling.html#footnotes",
    "href": "posts/2024/Process/Coupling.html#footnotes",
    "title": "確率測度のカップリング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Butkovsky & Veretennikov, 2013) などに言及あり．↩︎\n自殺と言われているらしい．Wikipedia 参照．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#本論",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#本論",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "3 本論",
    "text": "3 本論\n\n3.1 カップリング補題\n本論に入る前に，次の結果を準備している：\n\n\n\n\n\n\n補題（カップリング不等式の評価）2\n\n\n\n\\(X_1,X_2\\) を確率核 \\(P\\) を持つ Markov 連鎖，\\(Z=(X_1,X_2)\\) をその最適 Markov カップリングを \\[\nX_n^1=:\\xi_n1_{\\left\\{\\zeta_n=0\\right\\}}+\\eta_n^11_{\\left\\{\\zeta_n=1\\right\\}}\n\\] \\[\nX_n^2=:\\xi_n1_{\\left\\{\\zeta_n=0\\right\\}}+\\eta_n^21_{\\left\\{\\zeta_n=1\\right\\}}\n\\] とする．このとき， \\[\n\\operatorname{P}[X_n^1\\ne X_n^2]\\le\\biggr(1-p_0\\biggl)\\operatorname{E}\\left[\\prod_{k=0}^{n-1}\\biggr(1-p(\\eta_k^1,\\eta_k^2)\\biggl)\\right]\n\\] \\[\np(x_1,x_2):=1-\\frac{1}{2}\\|P(x_1,-)-P(x_2,-)\\|_\\mathrm{TV}\n\\] \\[\np_0:=1-\\frac{1}{2}\\|\\operatorname{P}^{X_0^1}-\\operatorname{P}^{X_0^2}\\|_\\mathrm{TV}=\\operatorname{P}[X_0^1=X_0^2]\n\\] が成り立つ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n基本的な考え方は \\[\n\\begin{align*}\n    \\operatorname{P}[X^1_n\\ne X_n^2]&\\le\\operatorname{P}[\\zeta_0=1,\\zeta_1=1,\\cdots,\\zeta_{n}=1]\\\\\n    &=\\operatorname{E}[1_{\\left\\{\\zeta_0=1\\right\\}}1_{\\left\\{\\zeta_1=1\\right\\}}\\cdots1_{\\left\\{\\zeta_{n}=1\\right\\}}]\n\\end{align*}\n\\] である．これは，\\(X_n^1\\ne X_n^2\\) ならば \\(\\zeta_n=1\\) が必要であるため， \\[\n\\begin{align*}\n    \\left\\{X_n^1\\ne X_n^2\\right\\}&\\subset\\left\\{\\zeta_n=1\\right\\}\\\\\n    &=\\left\\{\\zeta_0=1,\\zeta_1=1,\\cdots,\\zeta_{n}=1\\right\\}\n\\end{align*}\n\\] であるが，逆は必ずしも成り立たないためである．\nこれに，\\(\\mathcal{F}_n:=\\sigma[X_n^1,X_n^2]\\) について， \\[\n\\begin{align*}\n    &\\operatorname{E}\\left[\\prod_{i=k}^n1_{\\left\\{\\zeta_i=1\\right\\}}\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}\\left[\\prod_{i=k-1}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta_i^2)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]1_{\\left\\{\\zeta_{k-1}=1\\right\\}}\n\\end{align*}\n\\] の \\(k=1\\) の場合を併せて結論を得る．この等式自体は降下法により示す．\n\\(k=n\\) の場合の式 \\[\n\\operatorname{E}[1_{\\left\\{\\zeta_n=1\\right\\}}\\,|\\,\\mathcal{F}_{n-1}]=\\biggr(1-p(\\eta_{n-1}^1,\\eta_{n-1}^2)\\biggl)1_{\\left\\{\\zeta_{n-1}=1\\right\\}}\n\\] は明らかである．\\(k&lt;n\\) の場合，帰納法の仮定と，\\(\\mathcal{F}_{k-1}\\) の下で \\(\\zeta_k\\) と \\((\\eta_k^1,\\eta_k^2)\\) は条件付き独立であるから，次のように式変形できる： \\[\n\\begin{align*}\n    &\\quad\\operatorname{E}\\left[\\prod_{i=k}^n1_{\\left\\{\\zeta_i=1\\right\\}}\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}\\left[1_{\\left\\{\\zeta_k=1\\right\\}}\\operatorname{E}\\left[\\prod_{i=k+1}^n1_{\\left\\{\\zeta_i=1\\right\\}}\\,\\middle|\\,\\mathcal{F}_k\\right]\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}\\left[1_{\\left\\{\\zeta_k=1\\right\\}}\\operatorname{E}\\left[\\prod_{i=k}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_k\\right]\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}[1_{\\left\\{\\zeta_k=1\\right\\}}\\,|\\,\\mathcal{F}_{k-1}]\\operatorname{E}\\left[\\prod_{i=k}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=1_{\\left\\{\\zeta_{k-1}=1\\right\\}}\\biggr(1-p(\\eta^1_{k-1},\\eta^2_{k-1})\\biggl)\\operatorname{E}\\left[\\prod_{i=k}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=1_{\\left\\{\\zeta_{k-1}=1\\right\\}}\\operatorname{E}\\left[\\prod_{i=k-1}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n\\end{align*}\n\\]\n\n\n\nこの補題により，確率核 \\(P\\) を共有する２つの Markov 過程 \\((X_n^1),(X_n^2)\\) が与えられたとき，これらのカップリング \\((\\widetilde{X}^1_n),(\\widetilde{X}^2_n)\\) を同一の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上に構成し，\\((\\widetilde{X}_n^1)\\overset{\\text{d}}{=}(X_n^1),(\\widetilde{X}_n^2)\\overset{\\text{d}}{=}(X_n^2)\\) であるが，全変動距離を \\(\\operatorname{P}\\) によって評価できるようになる．\n\n\n3.2 スペクトルギャップによる一様エルゴード速度\n次の積分作用素 \\(A:\\mathcal{L}_b(E^2)\\to\\mathcal{L}_b(E^2)\\) を考える： \\[\nAf(x):=\\biggr(1-p(x)\\biggl)\\operatorname{E}[f(\\eta_1)\\,|\\,\\eta_0=x]\n\\] \\[\n\\eta_i=\\begin{pmatrix}\\eta_i^1\\\\\\eta_i^2\\end{pmatrix},\\quad x=\\begin{pmatrix}x^1\\\\x^2\\end{pmatrix}.\n\\] このスペクトル半径 \\[\nr(A):=\\limsup_{n\\to\\infty}\\sqrt[n]{\\|A^n\\|}\n\\] が一様エルゴード性を引き起こすのである．\n\n\n\n\n\n\n証明の概略\n\n\n\n\n\nなお，この定義式は，後述の \\(r(A)\\le1\\) と併せると，任意の \\(\\epsilon&gt;0\\) に対して，ある \\(C&gt;0\\) が存在して，\\(r(A)\\le r(A)^{1-\\epsilon}\\) であるから， \\[\n\\begin{align*}\n    \\|A^n\\|&\\le C\\left(r(A)^{1-\\epsilon}\\right)^n\\\\\n    &=Ce^{n(1-\\epsilon)\\log r(A)}\\\\\n    &=Ce^{-n(1-\\epsilon)\\log\\lvert r(A)\\rvert}\n\\end{align*}\n\\] を含意することに注意．\n実は次の定理の証明は，次の不等式を導いているのみである： \\[\n\\frac{1}{2}\\|P^n(x,-)-P^n(y,-)\\|_\\mathrm{TV}\\le\\|A^{n}\\|.\n\\]\nこの２式より，直ちに証明が完成する．\n\n\n\nただし，作用素ノルム \\(\\|A^n\\|\\) は任意の \\(\\|1\\|=1\\) を満たす関数ノルムに関して構成して良い．\\(C_b(E^2)\\) を考えることも有用である．\nいずれにしろ， \\[\n\\|A\\|_\\infty=1-\\inf_{x\\in E^2}p(x)\\le1\n\\] という条件式は変わらない．作用素ノルムの劣乗法性から \\[\nr(A)=\\limsup_{n\\to\\infty}\\sqrt[n]{\\|A^n\\|_\\infty}\\le\\|A\\|_\\infty\\le1\n\\] であることに注意．\n\n\n\n\n\n\n定理3\n\n\n\n\n\\(A:\\mathcal{L}_b(E^2)\\to\\mathcal{L}_b(E^2)\\) は有界作用素であり，\\(r(A)\\le1\\) を満たす．\n\\(r(A)&lt;1\\) ならば，\\(X\\) はただ一つの不変確率測度を持ち，任意の \\(\\epsilon&gt;0\\) に対して，初期分布 \\(X_0^1,X_0^2\\) に依らないある \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le C(1-p_0)e^{-n\\lvert\\log r(A)\\rvert(1-\\epsilon)}.\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n一般に，任意の \\(f\\in\\mathcal{L}_b(E^2)\\) に対して，\\((\\eta_k)\\) のMarkov性より， \\[\n\\begin{align*}\n    \\operatorname{E}\\left[f(\\eta_n)\\prod_{i=0}^{n-1}\\biggr(1-p(\\eta_i)\\biggl)\\right]&=\\operatorname{E}\\left[\\operatorname{E}\\left[f(\\eta_n)\\prod_{i=0}^{n-1}\\biggr(1-p(\\eta_i)\\biggl)\\,\\middle|\\,\\eta_0,\\cdots,\\eta_{n-1}\\right]\\right]\\\\\n    &=\\operatorname{E}\\left[\\prod_{i=0}^{n-2}\\biggr(1-p(\\eta_i)\\biggl)\\cdot\\biggr(1-p(\\eta_{n-1})\\biggl)\\operatorname{E}[f(\\eta_n)\\,|\\,\\eta_{n-1}]\\right]\\\\\n    &=\\operatorname{E}\\left[\\prod_{i=0}^{n-2}\\biggr(1-p(\\eta_i)\\biggl)\\cdot Af(\\eta_{n-1})\\right]=\\operatorname{E}[A^nf(\\eta_0)].\n\\end{align*}\n\\] これより，\\(\\left\\{\\eta_n=0\\right\\}\\supset\\left\\{\\prod_{k=0}^{n-1}\\biggr(1-p(Z_k)\\biggl)=0\\right\\}\\) に注意すれば，次の評価を得る： \\[\n\\begin{align*}\n    \\|P^n(x,-)-P^n(y,-)\\|_\\mathrm{TV}&\\le2\\operatorname{P}_{(x,y)}^Q[X_n\\ne Y_n]\\le2\\operatorname{E}_{(x,y)}^{Q_\\perp}\\left[\\prod_{k=0}^{n-1}\\biggr(1-p(Z_k)\\biggl)\\right]\\\\\n    &=2\\operatorname{E}_{(x,y)}^{Q_\\perp}\\left[\\delta_1(\\eta_n)\\prod_{k=0}^{n-1}\\biggr(1-p(Z_k)\\biggl)\\right]=2\\operatorname{E}_{(x,y)}^{Q_\\perp}[A^{n-1}\\delta_1(\\eta_1)](1-p(\\eta_0))\\\\\n    &\\le2\\delta_{x,y}\\|A^{n-1}\\|_\\infty\n\\end{align*}\n\\] よって，\\(\\frac{1}{2}\\|P^n(x,-)-P^n(y,-)\\|\\) はちょうど作用素ノルム \\(\\|A^{n-1}\\|\\) を評価する問題に帰着する．\n\n\n\n\n\n3.3 再帰性と非一様エルゴード速度\n\\[\n\\|A\\|_\\infty=1-\\inf_{x\\in E^2}p(x)=1\n\\] の場合に当たるものであるが， \\[\nK(\\epsilon):=\\left\\{x\\in E^2\\mid p(x)\\ge\\epsilon\\right\\}\n\\] に無限回再帰するとき，その頻度に依存して，指数的か多項式的か決まる．4 この頻度は，次の帰着時間 \\(\\tau^B,T^B\\) の積率条件で記述される： \\[\n\\tau^B:=\\inf\\left\\{n\\ge 1\\mid\\eta_n\\in B\\right\\}\n\\] \\[\nT^B:=\\inf\\left\\{n\\ge 1\\mid(X_n^1,X_n^2)\\in B\\right\\}\n\\] ただし，\\(B\\in\\mathcal{E}^{\\otimes2}\\)．\n\n\n\n\n\n\n命題2.1\n\n\n\nある \\(\\epsilon,\\lambda,M&gt;0\\) と \\(\\mathcal{E}^{\\otimes2}\\ni B\\subset K(\\epsilon)\\) について，次が成り立つとする：\n\n\\(Q:=\\operatorname{E}[e^{\\lambda\\tau^B}]&lt;\\infty\\)．\n任意の \\(x\\in B\\) について，\\(\\operatorname{E}_x[e^{\\lambda\\tau^B}]\\le M\\)．\n\nこのとき，\\(X\\) はただ一つの不変確率測度 \\(\\pi\\) をもち，またある初期分布 \\((X_0^1,X_0^2)\\) に依らない \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le CQe^{-n\\theta}\n\\] \\[\n\\theta:=\\frac{\\lvert\\log(1-\\epsilon)\\rvert\\lambda}{\\log M+\\lvert\\log(1-\\epsilon)\\rvert}\n\\]\n\n\n\n\n\n\n\n\n定理2.2\n\n\n\nある \\(\\epsilon&gt;0,\\lambda&gt;0,M&gt;0,\\mathcal{E}^{\\otimes2}\\ni B\\subset K(\\epsilon)\\) について，次が成り立つとする：\n\n\\(Q_2:=\\operatorname{E}[e^{\\lambda T^B}]&lt;\\infty\\)．\n任意の \\(x\\in B\\setminus K(1)\\) について，\\(\\operatorname{E}_x[e^{\\lambda T^B}]&lt;M\\)．\n\nこのとき，過程 \\(X\\) はただ一つの不変確率分布 \\(\\pi\\) をもち，ある初期分布 \\((X_0^1,X_0^2)\\) に依存しない定数 \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le CQ_2e^{-n\\theta_1}\n\\] \\[\n\\theta_1:=\\frac{\\lvert\\log(1-\\epsilon)\\rvert\\lambda}{\\log M+3\\lvert\\log(1-\\epsilon)\\rvert}.\n\\]\n\n\n\n\n\n\n\n\n定理2.3\n\n\n\nある \\(\\epsilon&gt;0,\\lambda\\ge1,M&gt;0,\\mathcal{E}^{\\otimes2}\\ni B\\subset K(\\epsilon)\\) について，次が成り立つとする：\n\n\\(Q_3:=\\operatorname{E}[(T^B)^\\lambda]&lt;\\infty\\)．\n任意の \\(x\\in B\\setminus K(1)\\) に対して，\\(\\operatorname{E}_x[(T^B)^\\lambda]&lt;M\\)．\n\nこのとき，過程 \\(X\\) はただ一つの不変確率分布 \\(\\pi\\) をもち，任意の \\(\\lambda_1\\in(0,\\lambda)\\) に対して，ある初期分布 \\((X_0^1,X_0^2)\\) に依存しない定数 \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le CQ_3n^{-\\lambda_1}\n\\]"
  },
  {
    "objectID": "static/CV/HShiba.html",
    "href": "static/CV/HShiba.html",
    "title": "CV",
    "section": "",
    "text": "Experience\nThe Institute of Statistical Mathematics\n\nResearch Assistant 2023-2028\n\n\nEducation\nGraduate University for Advanced Studies, SOKENDAI, Ph.D. Statistical Science 2028\nThe University of Tokyo, B.A. Mathematics 2023\n\n\nPublications\n\n\n\nPresentations"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html",
    "href": "posts/2024/Computation/MCMC.html",
    "title": "新時代の MCMC を迎えるために",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n現状，HMC (Hamiltonian Monte Carlo) (Duane et al., 1987) というなんと約 40 年前の MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．1\nこの手法はもともと “別の HMC” たる混合モンテカルロ (HMC: hybrid Monte Carlo) (Duane et al., 1987) が量子力学系のシミュレーションに特化した MCMC であったところを，一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．\nということで，HMC は，オリジナルの MCMC がそうであったように，物理学の方から着想された効率的な MCMC 手法であったのです．\nHMC が，提案から 40 年目を迎える前に，更なる効率的な手法によって代替されようとしています．\nそのきっかけ (Peters & de With, 2012) も，やはり，物理学（今回は物質科学）からの着想でした．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#アーキテクチャ",
    "href": "posts/2024/Kernels/Deep.html#アーキテクチャ",
    "title": "数学者のための深層学習１",
    "section": "3 アーキテクチャ",
    "text": "3 アーキテクチャ\nCNN はトランスフォーマーによりデータ間の関係を自動的に学習する枠組みが提案される前に，主に画像分野において，データの構造に関する事前知識をモデルに組み込んだ例として提案されたものである．\n世界初の深層学習モデルによる席巻は，CNN により，画像認識の分野において達成された．\n\n3.1 Computer Vision という分野\nComputer Vision という問題の複雑性が，ニューラルネットワークのアーキテクチャの開発を後押しした歴史がある．\n並行移動・拡大変換という2つの合同変換不変性に対して，画像の認識結果は不変であるべきである．\nこのような不変性，または 同変性 (equivariance) をモデルに取り入れる方法は大きく分けて４つある：\n\n誤差関数に正則化項を導入する18\n対称性を取り入れた潜在表現 を用いてその上で学習をする\n不変性を効率良く学習出来るように データセットを拡張する\n対称性を取り扱う構造をネットワークのアーキテクチャに組み込む\n\nCNN 節 1.2.2 は４番目のアプローチで歴史上最初に取られたものである．しかし，このどのアプローチも完全には不変性を取り入れることは出来ていないことも報告されている (Azulay & Weiss, 2019)．\n幾何学を種々の変換に対する不変性の研究と捉え直した Felix Klein の Erlangen program に倣い，種々の深層モデルの帰納バイアスとアーキテクチャを，幾何学的な変換から導出してシステマティックに理解する試み Geometric Deep Learning (Bronstein et al., 2021) がある（第 3.6 節）．\n\n\n3.2 物体認識 CNN\nCNN は画像の特徴を，階層的に学習出来るように誘導するような構造を持っている．\n多くの例では，畳み込み層とプーリング層が交互に繰り返され，最後に全結合層を持つような構造を持っている．\n\n3.2.1 局所的な特徴\n最初の素子は，画像の局所的な一部のみを入力として取る．その範囲を 受容野 (receptive field) と呼ぶ．この素子の荷重を フィルター または カーネル という．\n\n\n\n\n\n\n例：2次元の畳み込み層\n\n\n\n\n\n2次元での幅 \\(2k+1\\) の畳み込み層は，フィルター \\(\\mathrm{supp}\\;(\\psi)\\subset\\{0,\\pm1,\\cdots,\\pm k\\}\\) を用いて， \\[\nf^{\\text{out}}_{i,j}=\\phi\\left(\\sum_{a,b\\in\\mathbb{Z}}\\psi(i-a,j-b)f_{i,j}^{\\text{in}}+\\theta\\right)\n\\] と表せる．\n\n\n\n決まったカーネルに対して，この素子はカーネルの特徴にマッチした入力に対して，大きな出力を返す．\n次に，このフィルターを畳み込むことで，画像内の異なる位置に存在する特徴を検出する．畳み込み層は，荷重を共有した疎結合層ということになる．\n畳み込みを行うと，入力次元と出力次元が変わってしまうことがあるため，その場合は入力画像にパディングを施す．\n出力次元を小さくして，畳み込み特徴写像で大きな次元削減を行いたい場合，strided convolution を用いる．\n\n\n3.2.2 並行移動不変性\n畳み込みの結果が，特徴の位置の変化に対して不変になるようにする設計に，プーリング層 または ダウンサンプリング層 (down-sampling / sub-sampling) がある．\nプーリングも，受容野を持った素子と畳み込みからなるが，畳み込みに学習されるべきパラメータはなく，確定的な関数と畳み込まれる．\n代表的なプーリング関数には 最大プーリング (Y. Zhou & Chellappa, 1988) や平均プーリング，\\(l^2\\)-プーリングなどがある．\nプーリングは不変性の導入に加えて，畳み込み特徴のダウンサンプリングを行って，更なる次元削減を行う役割も果たす．\n\n\n\n3.3 画像分割 CNN\n画像分類では，１枚の画像に対して１つのクラスの対応づけたが，１つのピクセルに１つのクラスを対応づけることで，画像をクラスごとに分類することが考えられる．\n\n3.3.1 Up-sampling\n画像分類の問題では，最終的に全ピクセルから得た情報を１次元に圧縮することになる．一方で，十分な潜在表現を得たのちは up-sampling に転じる Encoder-Decoder 構造にすることで，最終的に元の画像サイズに戻しながら，画像分割問題を解くことが出来る (Long et al., 2015), (Noh et al., 2015), (Badrinarayanan et al., 2017)．\n(Badrinarayanan et al., 2017) は max-unpooling などの up-sampling 層の設計を考慮したが，これに学習可能なパラメータを増やした transpose convolution / deconvolution も提案された．\nPooling 層を一切用いず，down-sampling も up-sampling も畳み込み層のみによって行われる場合，これを 全畳み込みネットワーク (fully convolutional network) という (Long et al., 2015)．\n\n\n3.3.2 U-Net\nこの encoder-decoder 構造は，分類に必要のない情報を自動的に削減し，モデルのサイズを小さくするのには効果的であるが，タスクによっては元の画像の情報量を保ちたい場合がある．\nU-net (Ronneberger et al., 2015) は対応する down-sampling 層と up-sampling 層とを直接繋ぐ経路を追加することでこれを解決した．\n\n\n3.3.3 Capsule Networks\nプーリング層は並行移動不変性の概念を取り入れるが，回転や拡大などの変換に対する不変性を取り入れるにはデータ拡張に依るしかないのでは，CNN の学習はどうしても大規模なデータセットが必要になってしまう．\nそこで，アーキテクチャによる解決を試みたのが，畳み込み層に加えて カプセル層 を取り入れた Capsule Network (Sabour et al., 2017) である．\n\n\n\n3.4 Inpainting\n(Horita et al., 2023)\n\n\n3.5 スタイル転移\nCNN において，最初の方のレイヤーは局所的な特徴を捉えているが，後の方のレイヤーはスタイルなどの大域的な特徴を捉えている．\nこれを用いて，既存の画像の具体的な特徴を変えずに，他の画像からスタイルのみを転移する手法 Neural Style Transfer が提案された (Gatys et al., 2015), (Gatys et al., 2016)．\n\n\n3.6 幾何学的深層学習\n以上，種々のタスクに種々のアーキテクチャが存在することを見てきた．この状況は，19 世紀の幾何学と似ていると (Bronstein et al., 2021) はいう．\nこれらのアーキテクチャがどのような帰納バイアスを導入する役割を果たしているかを，不変性や同変性といった第一原理から理解する試みが 幾何学的深層学習 である．\n\nIn this text, we make a modest attempt to apply the Erlangen Programme mindset to the domain of deep learning, with the ultimate goal of obtaining a systematisation of this field and ‘connecting the dots’. We call this geometrisation attempt ‘Geometric Deep Learning’, and true to the spirit of Felix Klein, propose to derive different inductive biases and network architectures implementing them from first principles of symmetry and invariance. (Bronstein et al., 2021, p. 2)\n\n化学・生物・物理はいずれも対象の対称性をしっかり扱う理論を持っている．これらの分野に深層学習が広く取り入れられつつある今，深層学習の分野も対称性を第一原理として整理される脱皮が待たれているのである．\n\n3.6.1 幾何学と解析学は双対である\nMikhael Gromov によると，空間 \\(X\\) の解析学とは \\(X\\) 上の関数の研究で，\\(X\\) の幾何学とは \\(X\\) への関数の研究である (深谷賢治, 1997, p. 11)．\n\n\n\nGromov による幾何と解析の解釈\n\n\n同変性と共変性は，データ集合 \\(X\\) と群作用 \\(\\rho:G\\times X\\to X\\) との組 \\((X,\\rho)\\) を取り扱うから，確かに上述の定義に適っている．\n\n\n3.6.2 20 世紀の数学の方向\n(Gromov, 2001)\n\n\n3.6.3 共変性と同変性\n\n\n\n\n\n\n定義 (invariance, equivariance)19\n\n\n\n\\(X,Y\\) を集合，\\(G\\) を群で \\(X,Y\\) に左から作用するとする．20 関数 \\(\\varphi:X\\to Y\\) が\n\n\\(G\\) に関して 不変 であるとは，任意の \\(g\\in G\\) と \\(x\\in X\\) について \\[\n\\varphi(g\\cdot x)=\\varphi(x)\n\\] を満たすことをいう．\n\\(G\\) に関して 同変 であるとは，任意の \\(g\\in G\\) と \\(x\\in X\\) について \\[\n\\varphi(g\\cdot x)=g\\cdot\\varphi(x)\n\\] を満たすことをいう．\n\n\n\n物体認識 節 3.2 は不変的で，画像分割 節 3.3 は同変的な問題である．\nただし，不変性は，\\(G\\) の \\(Y\\) への作用が自明である場合の同変性と見れるため，同変性の方が一般的な概念であることに注意．\n\n\n3.6.4 G-CNN (T. Cohen & Welling, 2016)\n一般の群変換に対して，これを帰納バイアスとして取り入れる CNN である Group CNN (T. Cohen & Welling, 2016) が提案されており，医療画像解析での応用 (Lafarge et al., 2021) もなされている．\n\n\n3.6.5 Steerable CNN (T. S. Cohen & Welling, 2017)\nSteerable CNN ではチャンネルの間での対称性を取り入れることができる．\n(Weiler et al., 2018) により最先端の性能が発揮されている．"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html",
    "href": "posts/2024/Review/Peters-deWith2012.html",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#概要",
    "href": "posts/2024/Review/Peters-deWith2012.html#概要",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "1 概要",
    "text": "1 概要\n統計用語でいう Bouncy Particle Sampler を，Metropolis-Hastings 法の連続時間極限として初めて提案した論文であるが，これが (Bouchard-Côté et al., 2018) に発見されるには６年の時間を要した．\nこれを event-driven rejection-free Monte Carlo 法と呼び，Lennard-Jones ポテンシャルを持った流体のシミュレーションで検証した．"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#背景",
    "href": "posts/2024/Review/Peters-deWith2012.html#背景",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "2 背景",
    "text": "2 背景\n\n2.1 ED-MD\n（古典）統計力学に従って粒子系をシミュレーションする際には，MD (molecular dynamics) と MC (Monte Carlo methods) の組み合わせ (Duane et al., 1987) を用いることになるが，現状いずれも Metropolis (Metropolis et al., 1953) の採択-棄却の枠組みで実行されるのが最も一般的である．\nMetropolis の方法だと，\n\n凝縮系では MD 法としてのタイムステップは小さく取る必要があると同時に棄却率が高くなる．\n希薄系ではタイムスケールは分子の衝突過程に依存するので，シミュレーション時間のほとんどは無駄に使う必要が出る．\n\nという難点があるが，これは最初の MD 法 (Alder & Wainwright, 1959) で取られたような event-driven な方法で改善できる．\n\n\n2.2 ED-MD の改良\nしかし 剛体球に対する ED-MD (Alder & Wainwright, 1959) にも難点がある．\nED-MD はポテンシャルが単関数である場合にまでしか一般化できないが，本手法は一般のポテンシャルに適用できる（ 節 2.6 も参照）．\n\n\n2.3 他の手法との比較\nEvent-Chain Monte Carlo 手法 (Bernard et al., 2009), (Bernard & Krauth, 2011) は，ランダムに粒子を１つ選択し，他の粒子を規定の距離移動するまで，衝突しても無視して動かし続ける手法である．これにより MCMC の収束が高速化されるというのである．本手法も，棄却の代わりに衝突に注目するという点では ECMC とアイデアが似ている．\nアルゴリズム的には kinetic / dynamic MD (Fichthorn & Weinberg, 1991) に似て Poisson 過程のシミュレーションに帰着する．だが本手法は，イベントをシミュレーションしたのちに，そのイベントの間の粒子系の動きは線形に補間される．\n\n\n2.4 分子動力学\n分子動力学法 (MD: Molecular Dynamics simulation) は，系のミクロなダイナミクスを実験的に調べる代わりに，シミュレーションによって必要条件を絞り込んでいく構成的な・計算機集約的なアプローチである．\n特に，多体系の運動方程式を計算機を用いて数値的に解くことを指す (Alder & Wainwright, 1959)．それ故，物理や化学はもちろん，生化学，物質科学，さらには工学分野にまで幅広く用いられるシミュレーション法になる．\n\nthe inescapable conclusion is that MD will — if it hasn’t already — become an indispensable part of the theorist’s toolbox. (Rapaport, 2004, p. ix)\n\n従来は実験と理論が相補的な関係にあったが，まるで基盤モデルが強化学習のための効率的な世界モデルになるように，計算機シミュレーションと Monte Carlo 法が新時代の科学の第三の要素になるのかも知れない．1\nアイデア自体は当然極めてナイーブであり，Laplace の悪魔や気体分子運動論の時代から大変に意識されたが，その最初の非自明な適用はデジタルコンピュータ完成のあとであった (Alder & Wainwright, 1958), (Alder & Wainwright, 1959)．\n彼らの関心は，ポロニウムなどの剛体球 (hard-sphere) の系とみれる物質の温度に依存した相転移現象にあった (Alder & Wainwright, 1957)．(Alder & Wainwright, 1959) では撃力相互作用 (impulsive interaction) が問題であったので，event-driven なシミュレーション (ED-MD scheme) が取られていたことは (Peters & de With, 2012) でも言及されている．当時の計算機では，500個の粒子を扱うことが限界であったことが興味深い．\nその他に，粒子に内部構造がない場合は time-driven な Newton 力学のシミュレーションで良いかもしれないが，剛体で体積を持つ場合は Euler 方程式，内部構造を持つ場合は Langrange 方程式のシミュレーションも伴うかもしれない (Rapaport, 2004, p. 4)．\n通常の平衡状態の（エネルギー・体積・粒子数一定の）分子系は小正準集団に対応するが，特定の温度条件下での物性を考えたい場合は，運動方程式を修正してシミュレーションする方法もある．この方法の欠点は，物性を再現できても，個々の分子の軌道を正しく模倣しているわけではないという点であるが，そもそもそのカオス的な振る舞いから，正しい軌道を得ることは諦めることが多い．このこともあり，MD 法で用いられる数値積分法は比較的低次元で軽量なものでも十分なのである (Rapaport, 2004, p. 4)．2\n\n2.4.1 他手法の可能性\nセルオートマトンや格子ボルツマン法などの格子ベースの手法の方が計算量は安価であるが，表現力に劣ることになる．\nLanvegin 方程式に基づいた Brownian dynamics などのさらに連続時間ベースの手法もある．\n\n\n\n2.5 統計力学で用いる分子動力学法\n統計力学で用いられる Monte Carlo 法と molecular dynamics には極めて深い繋がりがある．\n\n2.5.1 布置平均を取る方法\n一度 MD の文脈を取り去り，純粋に統計力学的な量を Monte Carlo 法によって計算したいとする．\nやりたいことは抽象的には，その統計的集団の Boltzmann 分布から系をサンプリングできれば良い (Liu, 2004, p. 184)．\n典型的には MCMC によって行うこととなるが，結局その提案は MD によって示唆されたものの方が効率が良くなる．\n\nThe advantage of the MD proposal is that the resulting MCMC moves follow the dynamics of the target distribution more closely. (Liu, 2004, p. 184)\n\nこれは結局，背後の物理学的な本質を捉えているためとも言える\n\nA major advantage of molecular dynamics simulation in physical systems is its reliance on basic physics principles (e.g. Newton’s equation), which has been shown by nature to work well. (Liu, 2004, p. 189)\n\n\n\n2.5.2 時間平均を取る方法\n統計力学において，マクロ物理量はアンサンブル平均として計算され，エルゴード仮説の下では１つの系の時間発展を追うことで計算できることになる．\nこれに MD を用いるとすると，自然と Monte Carlo シミュレーションのサブルーチンとして MD を用いることになるが，タイムステップが系の特徴的な事象をよく捉えるように注意して取る必要がある．特に，系のハミルトニアンが変化しすぎないように離散化誤差を抑える必要がある (Liu, 2004, p. 184)．\n特に MD シミュレーション自体が（温度と粒子数が一定な）小正準集団に対応することになる (Rapaport, 2004, p. 6)．\n\n混合モンテカルロ／分子動力学法：モンテカルロ法を用いた構造分布の生成において，試行に用いる構造をニュートンの運動方程式など決定論的手法に従って用意する手法．分子凝集系などの複雑な系に対して，効率的に構造空間探索を行えると期待される．(栗﨑 & 田中, 2022)\n\n\n\n2.5.3 Optimal scaling の問題\nここで，optimal scaling と並行な，タイムステップ選択の問題 がやはり中核となることが確認された．\n\nhow to choose a good step size has always been an art in the field. (Liu, 2004, p. 183)\n\n特に，凝縮系において系を放置し過ぎると，すぐにハミルトニアンが保存されなくなってしまうため，MD を用いた Monte Carlo 法では極めてタイムステップを細かくすることが必須であることが一番の問題である．\n\na main problem with MD simulation is the stringent requirement of a small time-step size. (Liu, 2004, p. 189)3\n\nかといって，MD から遊離した MCMC を実行して性能を上げるためには，何かしらの形で遷移核に分布の形状を通知したいところであるが，これは MD 法なしには難しいということになる．4\n明らかに分布が特殊な形状をしているのに，ランダムウォークがその方向を見つけるまで待つのでは効率が悪い．\n\n\n2.5.4 Hamiltonian Monte Carlo 法\nそこで，完全な MD シミュレーションは行わず，Hamiltonian からの知識を部分的に提案核に取り入れるというアイデアが HMC (hybrid Monte Carlo) (Duane et al., 1987) として示された．5\n後に，統計力学的な背景を持たないサンプリング問題についても，Hamilton 系から示唆された提案分布を用いる方が MCMC として性能が良いことが発見された．これが HMC (Hamiltonian Monte Carlo) 法 (Neal, 1996) である．\n\n\n\n2.6 ポテンシャルの役割\n例え量子系のシミュレーションであろうとも，Born-Oppenheimer などの近似を重ねて，原子に働く有効ポテンシャルにのみ注目することにより，古典系のシミュレーションと同様の方法で実行することができ，多くのマクロ的な性質を再現することが出来ることが，(Griebel et al., 2007, pp. 17–17) に詳細に説明されている．\n実際の系での粒子間相互作用を決定する際，純粋に Schrödinger 方程式などから理論的に導出するだけでなく，解析的に記述されたポテンシャルをフィッティングしてみてそれが理論や実験に合致するかどうかを見る統計的な方法もとられる (Griebel et al., 2007, p. 27)．このようなモデル選択の立場に立つのが良い (戸田盛和 et al., 2011, p. 34)．\n多くの場合ポテンシャルは，粒子間距離や角度，座標などの変数が入っており，これらのパラメータを推定することで探されるが，当然このステップは難しいものである：\n\nThe construction of good potentials is still a form of art and requires much skill, work, and intuition. (Griebel et al., 2007, p. 28)\n\nMorse ポテンシャルや Lennard-Jones ポテンシャルがその代表例である．いわば，これらのポテンシャルも模型なのである．半導体のモデリングなどでは，さらに複雑なポテンシャルが必要になる (Griebel et al., 2007, p. 30)．\nこのように，多体問題をポテンシャル関数によって解く手法は 1980 年代からである (Griebel et al., 2007, p. 30)．ポテンシャル関数の形によって、適した数値計算アルゴリズムが異なる。特に、長距離力の高速計算のためには、ポテンシャルの性質を巧みに利用したアルゴリズムが必要である。\n\n2.6.1 システム同定としての物理学\n\nWhat do we mean by “understanding” something? We can imagine that this complicated array of moving things which constitutes “the world” is something like a great chess game being played by the gods, and we are observers of the game. (Feynman et al., 1964)\n\n\n今まで見てきたように，状態空間モデルは運動学習過程を統一的に説明するモデルであり，最適推定や最適制御の定式化に欠かせない．一方，状態空間モデルに含まれる行列の値は，運動方程式などといった物理的要請から決定できることもあるが，運動適応の学習係数などは未知のパラメータであることが多い．従って，与えられた実験データから状態空間モデルのパラメタを決定する必要がある．このパラメタ推定は制御理論において システム同定 と呼ばれる．冒頭の引用のように，ファインマンは数学や物理を自然界のシステム同定とみなした．(田中宏和, 2019, p. 173)"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#footnotes",
    "href": "posts/2024/Review/Peters-deWith2012.html#footnotes",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“What distinguishes computer simulation in general from other forms of computation, if such a distinction can be made, is the manner in which the computer is used: instead of merely performing a calculation, the computer becomes the virtual laboratory in which a system is studied - a numerical experiment.” (Rapaport, 2004, p. 3)．↩︎\n内部構造がある分子をシミュレーションする場合はよりやわらかい相互作用を考える必要があり，その際は積分は高次元になる上に，内部の運動が高速であるためにタイムステップも細かくする必要が出てくる．さらに拘束条件が存在する場合は，積分法よりも高い精度で取り扱う必要があり，特別な注意を要する．↩︎\n“For example, the protein folding process takes about \\(10^{-3}\\) seconds in nature. A proper MD simulation of such a process needs a step size of order \\(10^{-12}\\) and will take about \\(10^6\\) days using a current computer.”↩︎\nFor example, if the system of interest consists of closely packed particles, a random proposal for moving a particle is most likely rejected because the proposed new position has been partially occupied by others. (Liu, 2004, p. 189)↩︎\n特に格子場の理論において，Fermion 自由度が存在する場合のシミュレーションを問題として扱っていた．↩︎"
  },
  {
    "objectID": "static/Slides.html",
    "href": "static/Slides.html",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html",
    "href": "static/Sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "ポスター発表 | Poster Sessions\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n統数研オープンハウス\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 24th, 2024\nISM and online (Hybrid)\n\n\n\n\n\n\n\nオープンハウス案内\n\n\n発表ポスター\n\n\n\nMLSS2024\n\n\n\n\n\n\n\n\nDates\nLocation\n\n\n\n\nMar. 4-15, 2024\nOIST, Okinawa, Japan\n\n\n\n\nThe Machine Learning Summer School (despite its name, it was held in spring due to typhoon considerations) offered me my first opportunity to present in an academic setting.\n\n\n\nGroup Photo at MLSS2024\n\n\n\n\n\nMy Poster: A Recent Development of Particle Filter\n\n\n\n\n\n\n勉強会 | Study Group\n\n\n法律家のための統計数理\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n数学と法学，双方からの交流と理解を図ります．\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\nEmpirical Process Theory\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\nTextbook：Kengo Kato Empirical Process Theory (Lecture Note)\n\n\n\n担当分の発表資料\n\n\n\n\n\n\nその他 | Others\n\n\n学振 DC1\n\n\n\n\n\n\n\n\nPeriod\nApplication Category\n\n\n\n\nSpring, 2024\n12040 応用数学および統計数学関連\n\n\n\n\n\n\n\n申請書（バージョン１）\n\n\n\n\n\n申請書（バージョン２）"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html",
    "href": "posts/2024/Computation/AboutSimulation.html",
    "title": "計算とは何か",
    "section": "",
    "text": "２つの方向で「シミュレーション」へのアプローチが急速に進みつつある．\n\n\n従来物理学においては，実験と理論が相補的な関係にあった．\n\nIf it disagrees with experiment, it’s wrong. In that simple statement, is the key to science. (Feynman, 1964)\n\n\nしかし今後は，計算機シミュレーションと Monte Carlo 法が新時代の科学の第三の要素になるのかも知れない．\n\nThese are some of the reasons why computer simulation has recently emerged as a third way in science besides the experimental and theoretical approach. (Griebel et al., 2007, p. 2)\n\n素粒子論など，実験が出来ないために理論の検証がまたれている場面では，計算機シミュレーションが有望な手法になる．また半導体や製薬業界など，実験が高くつく産業では急速に計算機シミュレーションによる代替が進んでいる．\n一方で，多体問題などの解析的な手法では刃が立たない場面では，数値計算が唯一の理論的進歩を与えてくれる手法になる．数値実験 という言葉に，「実験」の語が含まれている理由である．\n\nThe rapid development of parallel computing systems made it possible to recreate and predict physical processes on computers. (Griebel et al., 2007, Preface)\n\nしかし，これには新たな数理の発展が必要である．実験結果には統計的な解釈を加えないと無意味であったように，シミュレーションも新たな統計学・機械学習と共に科学に資される必要がある．\n\nExperimental practice rests on a long (occasionally blemished) tradition; computer simulation, because of its novelty, is still somewhat more haphazard, but methodologies are gradually evolving. The output of any simulation should be treated by the same statistical methods used in the analysis of experiments. (Rapaport, 2004, pp. 2–3)\n\n筆者は学部入学直後に物理学実験という全理系学生必修の講義を経験し，その初回講義は不確実性の扱いについてであった．実際に講堂に集まった全学生でサイコロをふり，記録をとり，大数の法則を体験するという極めて印象深い授業であった．\n未来の物理学実験の授業は，第２回授業ではコンピュータシミュレーションを扱うようになるのかも知れない．\n\n\n\n大規模言語モデル（LLM）は，物理学的なシミュレーションと相補的な世界モデルを提供しつつある．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#物理学的シミュレーション",
    "href": "posts/2024/Computation/AboutSimulation.html#物理学的シミュレーション",
    "title": "計算とは何か",
    "section": "1 物理学的シミュレーション",
    "text": "1 物理学的シミュレーション\n従来物理学においては，実験と理論が相補的な関係にあった．\n\nIf it disagrees with experiment, it’s wrong. In that simple statement, is the key to science. (Feynman, 1964)\n\nしかし今後は，計算機シミュレーションと Monte Carlo 法が新時代の科学の第三の要素になるのかも知れない．素粒子論など，実験が出来ないために理論の検証がまたれている場面では，計算機シミュレーションが有望な手法になる．一方で，多体問題などの解析的な手法では刃が立たない場面では，数値計算が唯一の理論的進歩を与えてくれる手法になる．\n\nThe rapid development of parallel computing systems made it possible to recreate and predict physical processes on computers. (Griebel et al., 2007, Preface)\n\nしかし，これには新たな数理の発展が必要である．実験結果には統計的な解釈を加えないと無意味であったように，シミュレーションも新たな統計学・機械学習と共に科学に資される必要がある．\n\nExperimental practice rests on a long (occasionally blemished) tradition; computer simulation, because of its novelty, is still somewhat more haphazard, but methodologies are gradually evolving. The output of any simulation should be treated by the same statistical methods used in the analysis of experiments. (Rapaport, 2004, p. 3)\n\n筆者は学部入学直後に物理学実験という全理系学生必修の講義を経験し，その初回講義は不確実性の扱いについてであった．実際に講堂に集まった全学生でサイコロをふり，記録をとり，大数の法則を体験するという極めて印象深い授業であった．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#意味論的シミュレーション",
    "href": "posts/2024/Computation/AboutSimulation.html#意味論的シミュレーション",
    "title": "計算とは何か",
    "section": "2 意味論的シミュレーション",
    "text": "2 意味論的シミュレーション\n大規模言語モデルは，物理学的なシミュレーションと相補的な世界モデルを提供しつつある．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#今後の世界においてシミュレーションが果たす役割はどこまで拡大するのだろうか",
    "href": "posts/2024/Computation/AboutSimulation.html#今後の世界においてシミュレーションが果たす役割はどこまで拡大するのだろうか",
    "title": "計算とは何か",
    "section": "",
    "text": "２つの方向で「シミュレーション」へのアプローチが急速に進みつつある．\n\n\n従来物理学においては，実験と理論が相補的な関係にあった．\n\nIf it disagrees with experiment, it’s wrong. In that simple statement, is the key to science. (Feynman, 1964)\n\n\nしかし今後は，計算機シミュレーションと Monte Carlo 法が新時代の科学の第三の要素になるのかも知れない．\n\nThese are some of the reasons why computer simulation has recently emerged as a third way in science besides the experimental and theoretical approach. (Griebel et al., 2007, p. 2)\n\n素粒子論など，実験が出来ないために理論の検証がまたれている場面では，計算機シミュレーションが有望な手法になる．また半導体や製薬業界など，実験が高くつく産業では急速に計算機シミュレーションによる代替が進んでいる．\n一方で，多体問題などの解析的な手法では刃が立たない場面では，数値計算が唯一の理論的進歩を与えてくれる手法になる．数値実験 という言葉に，「実験」の語が含まれている理由である．\n\nThe rapid development of parallel computing systems made it possible to recreate and predict physical processes on computers. (Griebel et al., 2007, Preface)\n\nしかし，これには新たな数理の発展が必要である．実験結果には統計的な解釈を加えないと無意味であったように，シミュレーションも新たな統計学・機械学習と共に科学に資される必要がある．\n\nExperimental practice rests on a long (occasionally blemished) tradition; computer simulation, because of its novelty, is still somewhat more haphazard, but methodologies are gradually evolving. The output of any simulation should be treated by the same statistical methods used in the analysis of experiments. (Rapaport, 2004, pp. 2–3)\n\n筆者は学部入学直後に物理学実験という全理系学生必修の講義を経験し，その初回講義は不確実性の扱いについてであった．実際に講堂に集まった全学生でサイコロをふり，記録をとり，大数の法則を体験するという極めて印象深い授業であった．\n未来の物理学実験の授業は，第２回授業ではコンピュータシミュレーションを扱うようになるのかも知れない．\n\n\n\n大規模言語モデル（LLM）は，物理学的なシミュレーションと相補的な世界モデルを提供しつつある．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#計算機とシミュレーションの違いとは何だろうか",
    "href": "posts/2024/Computation/AboutSimulation.html#計算機とシミュレーションの違いとは何だろうか",
    "title": "計算とは何か",
    "section": "2 計算機とシミュレーションの違いとは何だろうか？",
    "text": "2 計算機とシミュレーションの違いとは何だろうか？\n第 1.1 節で指摘した通り，シミュレーションとは本来は計算とは関係がなく，ある物理的な過程を別の物理的な過程によって模倣する行為である．\n\nWhat distinguishes computer simulation in general from other forms of computation, if such a distinction can be made, is the manner in which the computer is used: instead of merely performing a calculation, the computer becomes the virtual laboratory in which a system is studied - a numerical experiment. (Rapaport, 2004, p. 3)\n\nLLM では文字が，数値実験では数字が表象となっているに過ぎない．ここで文字や数字は，人間が人間の理解のために採用している形式である．VAE の中間層に特殊な形式を形式を矯正しているに過ぎない．\nすると，ここを最適な形式によって相互接続し，end-to-end にすることで更なる効率化を図ろうとする論理が考えられるが，これが機械学習の悲願なのかも知れない．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#bayesian-computation-に賭ける思い",
    "href": "posts/2024/Computation/AboutSimulation.html#bayesian-computation-に賭ける思い",
    "title": "計算とは何か",
    "section": "3 Bayesian Computation に賭ける思い",
    "text": "3 Bayesian Computation に賭ける思い\nもしかしたら，MCMC はシミュレーションで，SMC はコンピューテーションであるというべきなのかも知れない．前者の活躍の場は，誕生から革新まで，ずっと物理学と物質科学にイニシアティブがあった．後者は防衛目的をはじめとした computer vision の文脈から生まれ，現在も機械学習など計算機方面の応用と親和性が高い．\n２つは粒子 (particle) というキーワードでつながり合っており，２大ベイズ計算手法として双璧をなしている．\nこの２つのサンプリング手法はきっと相補的な役割を果たしながら，これからも大きく発展して固有の立ち位置を占めることになるだろう．僕をどのような旅に導いてくれるのだろうか．いずれにしろ，たくさんの人と関わることが出来ると思うと，非常にワクワクするし，良いテーマに出会ったなという感謝の気持ちでいっぱいになる．\nこの２つの世界樹の上から，自然科学と社会科学，計算機と人間，数学と言葉，どれもバランスよく考え続けることができたらと願っている．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#計算とシミュレーションの違いとは何だろうか",
    "href": "posts/2024/Computation/AboutSimulation.html#計算とシミュレーションの違いとは何だろうか",
    "title": "計算とは何か",
    "section": "2 計算とシミュレーションの違いとは何だろうか？",
    "text": "2 計算とシミュレーションの違いとは何だろうか？\n第 1.1 節で指摘した通り，シミュレーションとは本来は計算とは関係がなく，ある物理的な過程を別の物理的な過程によって模倣する行為である．\n\nWhat distinguishes computer simulation in general from other forms of computation, if such a distinction can be made, is the manner in which the computer is used: instead of merely performing a calculation, the computer becomes the virtual laboratory in which a system is studied - a numerical experiment. (Rapaport, 2004, p. 3)\n\nLLM では文字が，数値実験では数字が表象となっているに過ぎない．ここで文字や数字は，人間が人間の理解のために採用している形式である．VAE の中間層に特殊な形式を形式を矯正しているに過ぎない．\nすると，ここを最適な形式によって相互接続し，end-to-end にすることで更なる効率化を図ろうとする論理が考えられるが，これが機械学習の悲願なのかも知れない．\n\n2.1 Buffon の針と Monte Carlo 積分法\nLLM が計算によりシミュレーションを実行する手法であるとしたら，Monte Carlo 積分法は，シミュレーションによって計算を実行する手法である．\n(Liu, 2004 Preface) で Monte Carlo computation の最も初源的なアイデアは Buffon の針にあると指摘されている．\n\nThe idea of simulating random processes so as to help evaluate certain quantities of interest is now an essential part of scientific computing. (Liu, 2004)\n\nとなると，Bayesian computation とは，本質的にシミュレーションによるコンピューテーションへの反逆なのかもしれない．\n\n\nContrast of the two main approachs to Machine Learning\n\n\n\n\n\n\n\n\nBayesian\nFrequentist\n\n\n\n\nInference is1\nMarginalization\nApproximation\n\n\nMathematical Idea\nIntegration\nDifferentiation2\n\n\nComputational Idea3\nIntegration\nOptimization\n\n\nComputational Solution\nSimulation\nComputation\n\n\nObjective\nUncertainty Quantification\nRecovery of True Value\n\n\nEmphasis\nModelling\nInference\n\n\n\n\n重要な観察として，\n\n決定論的な数値計算法の方が収束が速い．グリッド法などは \\(O(n^{-1})\\)．\nしかし，シミュレーションベースの方法の方がスケールする．Monte Carlo 積分法にしか太刀打ちできない領域は大きい上に，スケールする MCMC の最も重要な性質は，不偏推定量を通じてサブサンプルのみによる実行が可能である点である．\n\nという２点があり，Bayesian / Frequentist 双対性の背後に隠れているものと似ている．\n\n\n2.2 最適化-シミュレーションの双対性\n積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．\nまた，最適化問題は simulated annealing (Kirkpartick et al., 1983) を通じてサンプリング問題としても解ける．"
  },
  {
    "objectID": "posts/2024/Computation/AboutSimulation.html#footnotes",
    "href": "posts/2024/Computation/AboutSimulation.html#footnotes",
    "title": "計算とは何か",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” (Ghahramani, 2015, p. 454)．↩︎\nor, variation.↩︎\n“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” (Ghahramani, 2015, p. 454)．↩︎"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#擬似テンパリング-marinari-parisi1992",
    "href": "posts/Surveys/SMCSamplers.html#擬似テンパリング-marinari-parisi1992",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "最適化手法である 焼きなまし法（または擬似アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 擬似テンパリング (simulated tempering) (Marinari & Parisi, 1992) である．\n擬似アニーリングでは温度は下がる一方であったのが，擬似テンパリングでは温度もある周辺分布に従って遷移する．擬似アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，擬似テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n擬似テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．8 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer & Thompson, 1995)．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#擬似アニーリング-kirkpartick1983",
    "href": "posts/Surveys/SMCSamplers.html#擬似アニーリング-kirkpartick1983",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "これは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．2 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，凝縮系物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．\n分子動力学 (molecular dynamics) などの文脈では Metropolis 法はちょうど分子運動のシミュレーションになっていることを踏まえれば，これを simulated annealing と呼ぶのは極めて鮮やかなアナロジーとなっている．焼きなまし法自体も，シミュレーション可能になったのである．\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\mathop{\\mathrm{arg\\,min}}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#mc3-並行テンパリング-geyer1991",
    "href": "posts/Surveys/SMCSamplers.html#mc3-並行テンパリング-geyer1991",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering4 または exchange Monte Carlo (Hukushima & Nemoto, 1996) による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，5 さらには population-based MCMC6 とも呼ばれる．\npopulation-based (Iba, 2001) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．7 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#導入",
    "href": "posts/2024/Computation/MCMC.html#導入",
    "title": "新時代の MCMC を迎えるために",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 筆者の目標：新時代のサンプラーの開発\n情報通信機器の発達によりデータが複雑で大規模化する現代では，モデルも同様に大規模で複雑化していく必要があります．OpenAI の ChatGPT や Sora，Anthropic の Claude-3 などの 基盤モデル はその象徴と言えるでしょう．\n筆者は，その中で 新時代の MCMC の開発を目標としています．\n高次元空間上の複雑な分布からも効率的にサンプリングできる MCMC 手法が開発された際には，多くの人が自分のノートパソコンで気軽にできるベイズ統計分析の幅が大きく広がることでしょう．\nそれこそ，ニューラルネットワークの表現力をフルに活用するだけでなく，ベイズ手法の強みも併せて，小規模データでも鮮やかな分析が簡単に出来るようになるかもしれません．\nそのような世界線こそ，AI 技術の民主化と呼ぶにふさわしい，来るべき未来だと筆者は信じています．\nまた，基盤モデルの Bayes 的な理解を進めることも，実は壮大ながらも，筆者の最終的な目標の一つであります．\n\n\n1.2 最先端 MCMC 手法の世代交代が近い！？\n現状，HMC (Hamiltonian Monte Carlo) (Neal, 1996) というなんと約 30 年前の MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．\nこの手法はもともと “別の HMC” たる混合モンテカルロ (HMC: hybrid Monte Carlo) (Duane et al., 1987) が量子力学系のシミュレーションに特化した MCMC であったところを，一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．\nということで，HMC は，オリジナルの MCMC がそうであったように，物理学の方から着想された効率的な MCMC 手法であったのです．\nHMC が，提案から 30 年目を迎える前に，更なる効率的な手法によって代替されようとしています．\nそのきっかけも，やはり，物理学（今回は物質科学）からの着想でした．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#テンパリング共通の問題p-の設定",
    "href": "posts/Surveys/SMCSamplers.html#テンパリング共通の問題p-の設定",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "しかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．8"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#evoluationary-monte-carlo",
    "href": "posts/Surveys/SMCSamplers.html#evoluationary-monte-carlo",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を， 1. ある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める． 2. \\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす． の繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang & Wong, 2000), (Liang & Wong, 2001) である．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#mcmc-の最先端はどうなっているか",
    "href": "posts/2024/Computation/MCMC.html#mcmc-の最先端はどうなっているか",
    "title": "新時代の MCMC を迎えるために",
    "section": "2 MCMC の最先端はどうなっているか？",
    "text": "2 MCMC の最先端はどうなっているか？\nMCMC は物理学者から物質科学者，そして統計学者から機械学習家まで，多くの人が幅広く用いる手法です．\nその結果，多くの同一の手法が違う名前で呼ばれていることも多く，現状の最先端ではどのようなことが起こっているのか見極めるのが困難です．\nここでは，上述のすべての分野に渡って共通して起こりつつある大きな地殻変動を紹介します．キーワードは 連続時間 MCMC です．2\n\n2.1 筆者の目標：新時代のサンプラーの開発\n情報通信機器の発達によりデータが複雑で大規模化する現代では，モデルも同様に大規模で複雑化していく必要があります．OpenAI の ChatGPT や Sora，Anthropic の Claude-3 などの 基盤モデル はその象徴と言えるでしょう．\n筆者は，その中で 新時代の MCMC の開発を目標としています．\n高次元空間上の複雑な分布からも効率的にサンプリングできる MCMC 手法が開発された際には，多くの人が自分のノートパソコンで気軽にできるベイズ統計分析の幅が大きく広がることでしょう．\nそれこそ，ニューラルネットワークの表現力をフルに活用するだけでなく，ベイズ手法の強みも併せて，小規模データでも鮮やかな分析が簡単に出来るようになるかもしれません．\nそのような世界線こそ，AI 技術の民主化と呼ぶにふさわしい，来るべき未来だと筆者は信じています．\nまた，基盤モデルの Bayes 的な理解を進めることも，実は壮大ながらも，筆者の最終的な目標の一つであります．\n\n\n2.2 最先端 MCMC 手法の世代交代が近い！？\n現状，HMC (Hamiltonian Monte Carlo) (Neal, 1996) というなんと約 30 年前の MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．\nこの手法はもともと “別の HMC” たる混合モンテカルロ (HMC: hybrid Monte Carlo) (Duane et al., 1987) が量子力学系のシミュレーションに特化した MCMC であったところを，一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．\nということで，HMC は，オリジナルの MCMC がそうであったように，物理学の方から着想された効率的な MCMC 手法であったのです．\nHMC が，提案から 30 年目を迎える前に，更なる効率的な手法によって代替されようとしています．\nそのきっかけ (Peters & de With, 2012) も，やはり，物理学（今回は物質科学）からの着想でした．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#footnotes",
    "href": "posts/2024/Computation/MCMC.html#footnotes",
    "title": "新時代の MCMC を迎えるために",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHamiltonian Monte Carlo の名称は (Neal, 2011) からで，元々は Hybrid Monte Carlo と呼ばれていました．分子動力学法 (Molecular Dynamics) と MCMC のハイブリッド，という意味でした．↩︎\n統計学界隈では (Hastings, 1970) を入れて，Metropolis-Hastings 法とも呼ばれる．↩︎\nただし，配置 \\(\\omega\\in\\Omega\\) は空間内にランダム（一様）に粒子 \\(N\\) 個を配置することで生成することとする．↩︎\nWhile (Metropolis et al., 1953) proposed the use of MCMC sampling to compute particular integrals in statistical mechanics, it was the Hastings paper that elevated the concept to a general one, and introduced it to the broader statistics community. (Martin et al., 2023, p. 7) 3.5節．↩︎\n(Martin et al., 2023, p. 8) 4節，(Robert & Casella, 2011, p. 102)．↩︎\n(Fearnhead et al., 2018) から取った用語です．コンピュータシミュレーションである以上，結局は離散化するのですが，粒子の動きは（従来の Metropolis-Hastings 法のような）Markov 連鎖であるというより，連続時間確率過程のような動きをする手法群であることには間違いありません．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#提案手法",
    "href": "posts/2024/Review/Peters-deWith2012.html#提案手法",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "3 提案手法",
    "text": "3 提案手法\nMetropolis scheme のように提案と棄却によって詳細釣り合い条件を満たすのではなく，ポテンシャルの壁にぶつかった際に（確率的に）跳ね返ることによって詳細釣り合い条件を達成することを考える．\n\\(U\\) が \\(U=1_{\\mathbb{R}_+}\\) である場合から初めて，単関数の場合，最後に連続関数の場合でどのようなアルゴリズムになるかを順に記述している．\n連続極限を取ることで，「どの時点まで衝突せずに直進できるか」を計算することに帰着する．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics0.html",
    "href": "posts/2024/Nature/StatisticalMechanics0.html",
    "title": "数学者のための統計力学１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n(西森秀稔, 2003)\n普通数学者がやらないことであるが，古典粒子系を例にとって，統計力学における基本的な用語を確認する．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics0.html#footnotes",
    "href": "posts/2024/Nature/StatisticalMechanics0.html#footnotes",
    "title": "数学者のための統計力学１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nすなわち，\\(\\Lambda\\) 上の線型束である．↩︎\n\\(V\\) は境界ポテンシャルといい，外場との相互作用がある場合に現れる．↩︎\nこれは粒子間相互作用が short-range であると仮定しているためである．重力やクーロン力を考えている訳ではない．↩︎"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#進化的モンテカルロ",
    "href": "posts/Surveys/SMCSamplers.html#進化的モンテカルロ",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang & Wong, 2000), (Liang & Wong, 2001) である．"
  },
  {
    "objectID": "posts/2024/Particles/ParticleMethods.html",
    "href": "posts/2024/Particles/ParticleMethods.html",
    "title": "粒子法の概観",
    "section": "",
    "text": "分子動力学法 (MD: molecular dynamics) の文脈で 粒子法 (particle methods) と言ったとき，\n\n古典的な質点には Newton 力学\n剛体で体積を持つ場合は Euler 方程式\n内部構造を持つ場合は Langrange 方程式\nさらに一般の場合は Hamilton 方程式\n\nに基づいて粒子の動きをシミュレーションすることを指し，多くの場合は Monte Carlo 法と組み合わせることで，粒子の動きを提案し，Metropolis 法によって補正することで，系の物理量の分布を求めたり，平均値を求めたりする営みを指す (Rapaport, 2004, p. 4), (Griebel et al., 2007, p. 17)．\nそもそも MCMC の手法 (Metropolis et al., 1953) 自体も，Schrödinger 方程式に基づいた基底状態のシミュレーションを実行するために生まれたものである．\nこれが，強力なのである．極めて強力であるが故に，計算科学の分野ではすでに，そして将来的には確実に，シミュレーションと数値実験が，実際の実験と似たような重要な役割を科学において担うことになると見られている．\nこの意味での粒子法と，統計学において粒子法と言った場合に想像されるであろう SMC や IPM (interacting particle methods) とは 実は数理的には同一物である ことを見たい．\n\nIf, in some cataclysm, all of scientific knowledge were to be destroyed, and only one sentence passed on to the next generations of creatures, what statement would contain the most information in the fewest words? I believe it is the atomic hypothesis (or the atomic fact, or whatever you wish to call it) that all things are made of atoms—little particles that move around in perpetual motion, attracting each other when they are a little distance apart, but repelling upon being squeezed into one another. In that one sentence, you will see, there is an enormous amount of information about the world, if just a little imagination and thinking are applied. (Feynman et al., 1964)\n\n一言で言えば，一般に粒子法はポテンシャル（ハミルトニアン）に従って粒子を伝播する方法であり，粒子フィルターは特にこれが Feynman-Kac 測度で与えられる場合に用いられる粒子法である．"
  },
  {
    "objectID": "posts/2024/Particles/ParticleMethods.html#導入",
    "href": "posts/2024/Particles/ParticleMethods.html#導入",
    "title": "粒子法の概観",
    "section": "",
    "text": "分子動力学法 (MD: molecular dynamics) の文脈で 粒子法 (particle methods) と言ったとき，\n\n古典的な質点には Newton 力学\n剛体で体積を持つ場合は Euler 方程式\n内部構造を持つ場合は Langrange 方程式\nさらに一般の場合は Hamilton 方程式\n\nに基づいて粒子の動きをシミュレーションすることを指し，多くの場合は Monte Carlo 法と組み合わせることで，粒子の動きを提案し，Metropolis 法によって補正することで，系の物理量の分布を求めたり，平均値を求めたりする営みを指す (Rapaport, 2004, p. 4), (Griebel et al., 2007, p. 17)．\nそもそも MCMC の手法 (Metropolis et al., 1953) 自体も，Schrödinger 方程式に基づいた基底状態のシミュレーションを実行するために生まれたものである．\nこれが，強力なのである．極めて強力であるが故に，計算科学の分野ではすでに，そして将来的には確実に，シミュレーションと数値実験が，実際の実験と似たような重要な役割を科学において担うことになると見られている．\nこの意味での粒子法と，統計学において粒子法と言った場合に想像されるであろう SMC や IPM (interacting particle methods) とは 実は数理的には同一物である ことを見たい．\n\nIf, in some cataclysm, all of scientific knowledge were to be destroyed, and only one sentence passed on to the next generations of creatures, what statement would contain the most information in the fewest words? I believe it is the atomic hypothesis (or the atomic fact, or whatever you wish to call it) that all things are made of atoms—little particles that move around in perpetual motion, attracting each other when they are a little distance apart, but repelling upon being squeezed into one another. In that one sentence, you will see, there is an enormous amount of information about the world, if just a little imagination and thinking are applied. (Feynman et al., 1964)\n\n一言で言えば，一般に粒子法はポテンシャル（ハミルトニアン）に従って粒子を伝播する方法であり，粒子フィルターは特にこれが Feynman-Kac 測度で与えられる場合に用いられる粒子法である．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics.html",
    "href": "posts/2024/Nature/StatisticalMechanics.html",
    "title": "数学者のための統計力学１",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n\n(西森秀稔, 2003)\n\n\n\n\nReferences\n\n西森秀稔. (2003). スピングラスと連想記憶. 岩波書店. https://www.iwanami.co.jp/book/b476276.html"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics0.html#古典粒子系",
    "href": "posts/2024/Nature/StatisticalMechanics0.html#古典粒子系",
    "title": "数学者のための統計力学１",
    "section": "1 古典粒子系",
    "text": "1 古典粒子系\n\n1.1 多様体 \\(\\Omega_{\\Lambda,N}\\)\n有界集合 \\(\\Lambda\\subset\\mathbb{R}^3\\) に囚われた古典的な \\(N\\) 粒子系を考えると，位置と速度によって各粒子は記述できるから， \\[\n\\Omega_{\\Lambda,N}:=(\\Lambda\\times\\mathbb{R}^3)^N\n\\] 内の点によって系が記述できる．これを 相空間 (phase space) という．1\n\n\n1.2 関数 \\(H_{\\Lambda,N}\\)\nこの上に エネルギー が定まるが，これは ハミルトニアン ともいう：2 \\[\nH_{\\Lambda,N}(Q,V)=\\sum_{i=1}^N\\frac{mv_i^2}{2}+\\sum_{i&lt;j\\in[N]}U(q_i-q_j)+\\sum_{i=1}^NV_b(q_i)\n\\] \\[\nQ=(q_1,\\cdots,q_N)\\in\\Lambda^{3N}\n\\]\n多くの場合，ポテンシャル関数 \\(U\\) はコンパクト台を持つ，すなわち，相互作用半径 (radius of interaction) \\(R\\) をもつとする：\\(\\mathrm{supp}\\;U\\subset U_R(0)\\)．3\n経験則的に，まずは Lennard-Jones ポテンシャル \\[\nU(r)=\\frac{A}{r^{12}}-\\frac{B}{r^6}\\quad A,B&gt;0\n\\] を用いてモデリングする場合が多い．\n系の時間発展は変換の族 \\(\\{S_t\\}\\subset\\mathrm{Aut}(\\Omega_{\\Lambda,N})\\) （動力学 dynamics という）によって記述され， 動力学は運動方程式から導出される． \\[\n\\frac{d q_i}{d t}=v_i\n\\] \\[\nm\\frac{d v_i}{d t}=-\\sum_{i\\ne j}\\nabla U(q_j-q_i)-\\nabla V_b(q_i)\n\\]\n\n\n1.3 部分多様体 \\(\\Omega_{\\Lambda,N,E}\\) と小正準分布\nこの動力学 \\(\\{S_t\\}\\) は体積とエネルギーを保存量にもつ．すなわち，各 \\(S_t\\) は保測的で，等エネルギー集合 \\[\n\\Omega_{\\Lambda,N,E}:=\\left\\{(Q,\\Lambda)\\in\\Omega_{\\Lambda,N}\\mid H_{\\Lambda,N}(Q,\\Lambda)=E\\right\\}\n\\] を不変部分集合にもつ．\n\\(\\Omega_{\\Lambda,N,E}\\) 上の測度 \\[\n\\nu_{\\Lambda,N,E}(B):=\\lim_{\\Delta E\\to0}\\frac{\\ell(\\Delta B)}{\\Delta E}\n\\] は \\(S_t\\) によって保存される．これを 小正準分布 (microcanonical measures) または Gelfand-Leray measures という．\nただし，\\(\\Delta B\\) は \\(x\\in B\\) から始まった \\(\\Omega_{\\Lambda,N,E}\\) の法線（面）で，\\(\\Omega_{\\Lambda,N,E+\\Delta E}\\) との交点で終わる線分の \\(x\\in B\\) に関する合併である．\n他にも，等位集合が \\(S_t\\) によって保存される関数 \\(I_i:\\Omega_{\\Lambda,N}\\to\\mathbb{R}\\) は存在し得て，その場合は全ての合併 \\(\\Omega_{\\Lambda,N,E,I_1,\\cdots,I_k}\\) 上に小正準分布が遺伝する．\n\n\n1.4 エルゴード仮説\nBoltzman のエルゴード仮説は，他の積分 \\(I_i\\) が存在しない場合，\\(\\Omega_{\\Lambda,N,E}\\) 上の \\(S_t\\)-不変で \\(\\nu_{\\Lambda,N,E}\\)-絶対連続な測度は，\\(\\nu_{\\Lambda,N,E}\\) の定数倍に限る，というものである．\n\n\n1.5 configuration gas\nもし粒子が全て動いていないならば，相空間は \\(\\mathbb{R}^{3N}=:\\Omega_{\\Lambda,N}^{\\text{conf}}\\) で良い．\nこれを configuration gas といい，古典粒子系の更なる理想化に当たる．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics0.html#格子模型",
    "href": "posts/2024/Nature/StatisticalMechanics0.html#格子模型",
    "title": "数学者のための統計力学１",
    "section": "2 格子模型",
    "text": "2 格子模型\n一部は実際の物理系の良いモデルとなっているが，より複雑なモデルの統計物理学的性質の良い第一近似としても用いられる．\n\n2.1 格子気体\nconfiguration gas から更なるモデルの簡略化を考える．\n粒子の位置は必ず格子点上にあるとすれば，配置空間はさらに \\(\\Lambda\\subset\\mathbb{Z}^3\\) に対して \\[\n\\Omega_{\\Lambda,N}=\\left\\{Q=(q_1,\\cdots,q_N)\\in\\Lambda^N\\mid q_i\\ne q_j\\;(i\\ne j)\\right\\}\n\\] と簡略化される．\n仮に外場もないとすると，ハミルトニアンは単にポテンシャルの和 \\[\nH(Q)=\\sum_{i&lt;j}U(q_i-q_j)\n\\] となる．\n\n\n2.2 スピン系\n有限集合 \\(\\Lambda\\subset\\mathbb{Z}^3\\) が粒子で満ちているとすると，配置空間は縮退する．\nその際のスピン系の相空間は，関数の集合 \\[\n\\Omega_\\Lambda:=S^\\Lambda\n\\] になる．\n\\(S=\\partial U_1(0)\\subset\\mathbb{R}^3\\) の場合を 平面回転子 (planar rotator) モデルという．\\(S=\\{\\pm1\\}\\) としても，物理系のモデルとして磁性体の第一近似として使える模型になる．\nハミルトニアンは，相互作用と外場の和として \\[\nH_\\Lambda(\\sigma)=\\sum_{x\\ne y\\in\\Lambda}U(x-y)\\sigma(x)\\sigma(y)+h\\sum_{x\\in\\Lambda}\\sigma(x)\n\\] と与えられる．\nコンパクト台を持つ関数 \\(U\\) が \\(U_1(0)\\cap\\mathbb{Z}^3\\) で消える場合を Ising 模型 という．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics0.html#参考文献",
    "href": "posts/2024/Nature/StatisticalMechanics0.html#参考文献",
    "title": "数学者のための統計力学１",
    "section": "3 参考文献",
    "text": "3 参考文献\n\n(Minlos, 2000)"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html",
    "href": "posts/2024/Nature/StatisticalMechanics2.html",
    "title": "数学者のための統計力学２",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$\n殆どの力学的現象は，相空間 \\(\\Omega\\subset\\mathbb{R}^{6N}\\) と呼ばれる シンプレクティック多様体 上の Hamiltonian flow \\((S_t)\\) として理解される．統計力学では，その上に確率測度 \\(P\\) を導入して運動を理解する．組 \\((\\Omega,P)\\) を統計集団という．\n例えば，\\(N\\)-粒子系 \\(\\Omega_{\\Lambda,N}\\) の統計的な振る舞いを最もよく記述する確率分布 \\(P\\in\\mathcal{P}(\\Omega_{\\Lambda,N})\\) を推定することを考える．\n系が平衡状態にない場合は分子の衝突を考える必要があり，現在でも理論は発展中であるが，すでに平衡状態に至ったとみなせる場合には，この確率分布 \\(P\\) というのは殆どわかる．\n仮に初期分布 \\(P_0\\in\\mathcal{P}(\\Omega_{\\Lambda,N})\\) を持っていた場合，分布は Hamilton 流の押し出しにより \\((S_t)_*P_0\\) というように \\(\\mathcal{P}(\\Omega_{\\Lambda,N})\\) 上の力学系として発展していく．1\n従って，平衡分布は必ず \\(S_t\\) の不動点，不変確率分布である必要があるが，これを満たす分布は複数存在する．そのうち特に意味のあるものが，小正準集団 \\((\\Omega_{\\Lambda,N,E},P^{\\text{microcanon}}_{\\Lambda,N,E})\\)，正準集団 \\((\\Omega_{\\Lambda,N},P^{\\text{canon}}_{\\Lambda,N,\\beta})\\)，大正準集団 \\((\\Omega_\\Lambda,P^{\\text{grandcanon}}_{\\Lambda,\\beta,\\mu})\\) などである．「小」「大」などの接頭辞は分布の台の大きさとも思える．\n確率的な系も \\(\\mathcal{P}(\\Omega_{\\Lambda,N})\\) 上で見れば決定論的な発展をするのが数理の妙である．\nこれら３つの分布はどれもそれだけで完全なものではなく，熱力学極限において真に物理的に意味のある量になると理解する．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#小正準分布",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#小正準分布",
    "title": "数学者のための統計力学２",
    "section": "1 小正準分布",
    "text": "1 小正準分布\n小正準測度 \\(\\nu_{\\Lambda,N,E}\\) を正規化したものも \\(S_t\\) の不変確率分布である： \\[\nP^{\\text{microcanon}}_{\\Lambda,N,E}(A):=\\frac{\\nu_{\\Lambda,N,E}(A)}{\\nu_{\\Lambda,N,E}(\\Omega_{\\Lambda,N,E})}\n\\]\n改めて，\\(P^{\\text{microcanon}}_{\\Lambda,N,E}\\) は等エネルギー面 \\(\\Omega_{\\Lambda,N,E}\\) 上にしか台を持たないことに注意．そのため，\\(P^{\\text{microcanon}}_{\\Lambda,N,E}\\) は \\(\\mathbb{R}^{6N}\\) 上の Lebesgue 測度 \\(\\ell_{6N}\\) に関しては絶対連続ではない．2\n\nこのようにエネルギーが一定であるような力学系の集団をミクロカノニカル集団，この集団の分布，すなわちそのエネルギーに対応する全ての微視状態へ，等しい確率を持って存在するような分布をミクロカノニカル分布という．(久保亮五, 2003, p. 27)\n\n孤立系など，エネルギー一定の系は Hamiltonian の等位集合上で運動をする．その上の小正準測度 / Gelfand-Leray 測度は，相空間上の自然な体積要素から誘導される．3 従って小正準分布は運動に関して不変ではあるが，だからと言ってどのようなマクロ系に対しても小正準分布が平衡分布になると約束する法則は何もない．孤立したマクロ系（エネルギー一定以外に制約がない場合）が小正準分布に従うことは，等重率の仮定の具体的な表現と見れる，一種のモデルの仮定である．4"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#gibbs-の正準分布",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#gibbs-の正準分布",
    "title": "数学者のための統計力学２",
    "section": "2 Gibbs の正準分布",
    "text": "2 Gibbs の正準分布\n系のエネルギーを指定するのではなく，逆温度を指定することで，相空間上で密度 \\[\np^{\\text{canon}}_{\\Lambda,N,\\beta}(Q,V)=\\frac{1}{Z_{\\Lambda,N,\\beta}}e^{-\\beta H_{\\Lambda,N}(Q,V)}\n\\] を持つ不変確率分布を得る．5 これを 正準分布 (canonical distribution) という．\n\\(\\beta&gt;0\\) を 逆温度，\\(Z_{\\Lambda,N,\\beta}\\) を 分配関数 (partition function / statistical sum) という．\n古典気体の \\(H\\) において位置に依存する項と速度に依存する項とが分かれているため，\\(P^{\\text{canon}}_{\\Lambda,N,\\beta}\\) において位置と速度は独立であり，速度 \\(v_1,\\cdots,v_N\\) も互いに独立でそれぞれは Gauss 分布に従う．これを Maxwell の法則 という．\nまた特に，Gibbs 測度の配置空間上での周辺分布は，configuration gas の Gibbs 測度に一致する．この意味で，古典気体の統計力学的性質は，configuration gas に帰着すると言える．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html",
    "href": "posts/2024/Nature/StatisticalMechanics1.html",
    "title": "数学者のための統計力学１",
    "section": "",
    "text": "普通数学者がやらないことであるが，古典粒子系を例にとって，統計力学における基本的な用語を確認する．\n統計力学といえども場面設定は力学であり，その形式はハミルトン形式が利用される．\nしかし，その運動方程式を解析的に分析するのではなく，相空間 \\(\\Omega\\) 上の（有界）測度に注目して，確率統計学，果てには計算統計学を利用して展開していく．\nちょうど，統計学ではここのサンプル \\(\\omega\\in\\Omega\\) よりも全体的な振る舞い \\(P\\in\\mathcal{P}(\\Omega)\\) や統計量 \\(\\Omega\\to\\mathbb{R}\\) の平均値や分散などの統計的な振る舞いに興味があるのと同じことである．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#古典粒子系",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#古典粒子系",
    "title": "数学者のための統計力学１",
    "section": "1 古典粒子系",
    "text": "1 古典粒子系\n\n1.1 多様体 \\(\\Omega_{\\Lambda,N}\\)\n有界集合 \\(\\Lambda\\subset\\mathbb{R}^3\\) に囚われた古典的な \\(N\\) 粒子系を考えると，位置と速度によって各粒子は記述できるから， \\[\n\\Omega_{\\Lambda,N}:=(\\Lambda\\times\\mathbb{R}^3)^N\n\\] 内の点によって系が記述できる．これを 相空間 (phase space) という．1\n\n\n1.2 関数 \\(H_{\\Lambda,N}\\)\nこの上に エネルギー が定まるが，これは ハミルトニアン ともいう：2 \\[\n\\begin{align*}\n    H_{\\Lambda,N}(Q,V)&=\\sum_{i=1}^N\\frac{mv_i^2}{2}+\\sum_{i&lt;j\\in[N]}U(q_i-q_j)\\\\\n    &\\qquad\\qquad+\\sum_{i=1}^NV_b(q_i)\n\\end{align*}\n\\] \\[\nQ=(q_1,\\cdots,q_N)\\in\\Lambda^{3N}\n\\]\n多くの場合，ポテンシャル関数 \\(U\\) はコンパクト台を持つ，すなわち，相互作用半径 (radius of interaction) \\(R\\) をもつとする：\\(\\mathrm{supp}\\;U\\subset U_R(0)\\)．3\n経験則的に，まずは Lennard-Jones ポテンシャル \\[\nU(r)=\\frac{A}{r^{12}}-\\frac{B}{r^6}\\quad A,B&gt;0\n\\] を用いてモデリングする場合が多い．\n系の時間発展は変換の族 \\(\\{S_t\\}\\subset\\mathrm{Aut}(\\Omega_{\\Lambda,N})\\) （動力学 dynamics という）によって記述され， 動力学は運動方程式から導出される． \\[\n\\frac{d q_i}{d t}=v_i\n\\] \\[\nm\\frac{d v_i}{d t}=-\\sum_{i\\ne j}\\nabla U(q_j-q_i)-\\nabla V_b(q_i)\n\\]\n\n\n1.3 部分多様体 \\(\\Omega_{\\Lambda,N,E}\\) と小正準分布\nこの動力学 \\(\\{S_t\\}\\) は体積とエネルギーを保存量にもつ．すなわち，各 \\(S_t\\) は保測的で，等エネルギー集合 \\[\n\\Omega_{\\Lambda,N,E}:=\\left\\{(Q,\\Lambda)\\in\\Omega_{\\Lambda,N}\\mid H_{\\Lambda,N}(Q,\\Lambda)=E\\right\\}\n\\] を不変部分集合にもつ．\n\\(\\Omega_{\\Lambda,N,E}\\) 上の測度 \\[\n\\nu_{\\Lambda,N,E}(B):=\\lim_{\\Delta E\\to0}\\frac{\\ell(\\Delta B)}{\\Delta E}\n\\] は \\(S_t\\) によって保存される．これを 小正準分布 (microcanonical measures) または Gelfand-Leray measures という．\nただし，\\(\\Delta B\\) は \\(x\\in B\\) から始まった \\(\\Omega_{\\Lambda,N,E}\\) の法線（面）で，\\(\\Omega_{\\Lambda,N,E+\\Delta E}\\) との交点で終わる線分の \\(x\\in B\\) に関する合併である．\n他にも，等位集合が \\(S_t\\) によって保存される関数 \\(I_i:\\Omega_{\\Lambda,N}\\to\\mathbb{R}\\) は存在し得て，その場合は全ての合併 \\(\\Omega_{\\Lambda,N,E,I_1,\\cdots,I_k}\\) 上に小正準分布が遺伝する．\n\n\n1.4 エルゴード仮説\nBoltzman のエルゴード仮説は，他の積分 \\(I_i\\) が存在しない場合，\\(\\Omega_{\\Lambda,N,E}\\) 上の \\(S_t\\)-不変で \\(\\nu_{\\Lambda,N,E}\\)-絶対連続な測度は，\\(\\nu_{\\Lambda,N,E}\\) の定数倍に限る，というものである．\nこれにより，特定の系が運動 \\(S_t\\) によって平衡状態に至った際，相空間上を旅する際に特定の状態が現れる頻度分布は，必ず小正準分布に一致することが帰結される．\n\n\n1.5 configuration gas\nもし粒子が全て動いていないならば，相空間は \\(\\mathbb{R}^{3N}=:\\Omega_{\\Lambda,N}^{\\text{conf}}\\) で良い．\nこれを configuration gas といい，古典粒子系の更なる理想化に当たる．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#格子模型",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#格子模型",
    "title": "数学者のための統計力学１",
    "section": "2 格子模型",
    "text": "2 格子模型\n一部は実際の物理系の良いモデルとなっているが，より複雑なモデルの統計物理学的性質の良い第一近似としても用いられる．\n加えて，このような離散的なグラフとしての表現を通じて，機械学習や情報科学との関わりを持つ．そのような分野は情報統計力学と呼ばれる (西森秀稔, 2003)．4\n\n2.1 格子気体\nconfiguration gas から更なるモデルの簡略化を考える．\n粒子の位置は必ず格子点上にあるとすれば，配置空間はさらに \\(\\Lambda\\subset\\mathbb{Z}^3\\) に対して \\[\n\\begin{align*}\n    &\\Omega_{\\Lambda,N}=\\\\\n    &\\left\\{Q=(q_1,\\cdots,q_N)\\in\\Lambda^N\\mid q_i\\ne q_j\\;(i\\ne j)\\right\\}\n\\end{align*}\n\\] と簡略化される．\n仮に外場もないとすると，ハミルトニアンは単にポテンシャルの和 \\[\nH(Q)=\\sum_{i&lt;j}U(q_i-q_j)\n\\] となる．\n\n\n2.2 スピン系\n有限集合 \\(\\Lambda\\subset\\mathbb{Z}^3\\) が粒子で満ちているとすると，配置空間は縮退する．\nその際のスピン系の相空間は，関数の集合 \\[\n\\Omega_\\Lambda:=S^\\Lambda\n\\] になる．\n\\(S=\\partial U_1(0)\\subset\\mathbb{R}^3\\) の場合を 平面回転子 (planar rotator) モデルという．5 \\(S=\\{\\pm1\\}\\) としても（Ising spin），6 物理系のモデルとして磁性体の第一近似として使える模型になる．\nハミルトニアンは，相互作用と外場の和として \\[\nH_\\Lambda(\\sigma)=\\sum_{x\\ne y\\in\\Lambda}U(x-y)\\sigma(x)\\sigma(y)+h\\sum_{x\\in\\Lambda}\\sigma(x)\n\\] と与えられる．\n\n2.2.1 Ising 模型\n関数 \\(U\\) が \\(U_1(0)\\cap\\mathbb{Z}^3\\) を台に持つ場合を（狭義の） Ising 模型 といい，\\(U\\) は \\(J\\) でも表す：7 \\[\nH_\\Lambda(\\sigma)=-J_{x,y}\\sum_{\\lvert x-y\\rvert=1}\\sigma(x)\\sigma(y)-h\\sum_{x\\in\\Lambda}\\sigma(x)\n\\]\n\\(J_{x,y}&gt;0\\) である場合，低温秩序相は強磁性体のモデルになっている．\\(J_{x,y}\\equiv J\\) という定値性の仮定もよく置かれる．\n\n\n2.2.2 Curie-Weiss 模型\nこの最近傍 Ising 模型に，Weiss 近似という平均場近似を施し，Curie-Weiss Hamiltonian \\[\nH_{\\Lambda}(\\sigma)=-\\frac{dJ}{\\lvert\\Gamma\\rvert}\\sum_{x\\ne y\\in\\Lambda}\\sigma(x)\\sigma(y)-h\\sum_{x\\in\\Lambda}\\sigma(x)\n\\] を用いる模型を Curie-Weiss 模型 という．\n相互作用がもはや最近傍同士ではなくなっている．これを 無限レンジ模型 ともいう (西森秀稔, 2003, p. 24)．\nこの式からは，各スピンが，具体的な他のスピンと相互作用するというより，全ての他スピンからなる平均場（有効磁場）と相互作用していると読める．8\nこのような平均場近似を施していても，低温の強磁性相と高温の常磁性相が別れることが観察される (Friedli & Velenik, 2017, p. 62)．\n\n\n\n2.3 スピングラス\n\\(J_{x,y}\\) の符号がバラバラである場合，これをスピングラスの模型という．9\nこのような模型では，低音秩序相が消えて，スピンがバラバラである状態（スピングラス相）も安定たり得ることがわかっている．\n特に，安定な状態が複数存在し，温度を少し変えるだけで全く性質の異なる別の状態へ系が移ることもよくある．自由エネルギーが強い多峰性を示すのである．\n\n\n\n最終講義 スピングラスと計算物性物理 p.28\n\n\nCuMn などはスピン間の相互作用が，RKKY (Ruderman-Kittel-Kazuya-Yoshida) 相互作用 \\[\nJ\\,\\propto\\,\\frac{\\cos(2k_Fr_{12})}{r_{12}^3}\n\\] により表され，これは符号が分子の \\(\\cos\\) により正にも負にもなり得る．\nAuFe もスピングラスである (Cannella & Mydosh, 1972)．10\n\n2.3.1 Edwards-Anderson 模型 (Edwards & Anderson, 1975)\n\\((J_{x,y})_{\\lvert x-y\\rvert=1}\\) を，グラフのエッジの集合上に定義された独立な Gauss 確率場とし，外場を考えないものを，Edwards-Anderson 模型 という： \\[\nH(\\sigma)=-\\sum_{\\lvert x-y\\rvert=1}J_{x,y}\\sigma(x)\\sigma(y).\n\\]\n確率変数 \\(J_{x,y}\\sim\\mathop{\\mathrm{N}}_1(0,N)\\) の平均（配位平均）と，アンサンブル平均という２つの平均を扱う必要がある点で極めて難しい模型となっている．\nこの模型において自由エネルギーを計算するために，分配関数の対数の平均を，分配関数の積率によって計算する \\[\n\\operatorname{E}[\\log Z]=\\lim_{n\\to\\infty}\\frac{\\operatorname{E}[Z^n]-1}{n}\n\\] という関係式を用いた．これを レプリカ法 という．ただし，期待値は \\(J_{x,y}\\) に関するもので，アンサンブル平均 \\(\\langle-\\rangle\\) とは関係ないことに注意．\n現状，特に Talagrand はレプリカ法の数学的妥当性について極めて懐疑的であるが，多くは数値実験により検証されており，何らかの本質を捉えていることは間違いない．11\n\n\n2.3.2 Sherrington-Kirkpatrick 模型 (Sherrington & Kirkpatrick, 1975)\nEA 模型を無限レンジにすることで，平均場近似が厳密解を与えるようにし，熱力学極限 \\[\n\\lim_{N\\to\\infty}\\frac{\\operatorname{E}[\\log Z_N]}{N}\n\\] を与えることでこれを解いたものである．その際にもレプリカ法が用いられた．\n著者のうちの Scott Kirkpatrick は 擬似アニーリング (Kirkpartick et al., 1983) の開発者でもある．\nしかしこの解（レプリカ対称な解）は初め低温域では破綻を起こすとされていた．(Parisi, 1980) がこの問題を解決し，任意の温度 \\(T&gt;0\\) での厳密解（レプリカ対称性破れ解）が得られた．これは Parisi ansatz と呼ばれる．12 この解は計算機シミュレーションと高い精度で一致し，常磁性相と強磁性相に加えて，スピングラス相を示す．\nParisi はこの業績で 2021 年にノーベル物理学賞を受賞した．その３番目に多く引用されている論文 (Marinari & Parisi, 1992) は 擬似テンパリング の提案論文である．\n\n\n2.3.3 Thouless-Anderson-Plamer 方程式 (Thouless et al., 1977) と近似メッセージ伝播 (Bolthausen, 2014)\n一方で，SK 模型に対して高温摂動展開により自由エネルギーを与えるアプローチもある．\n特に，熱力学極限に向かって漸近的に成り立つ次の方程式を TAP 方程式という： \\[\n\\langle\\sigma_i\\rangle\\approx\\tanh\\left(\\frac{\\beta}{\\sqrt{N}}\\sum_{j\\ne i}J_{ij}\\langle\\sigma_j\\rangle+h-\\beta^2(1-q)\\langle\\sigma_i\\rangle\\right)\n\\]\nこれの数学からの証明も近年試みられている (Talagrand, 2003), (Chatterjee, 2010)．しかし，厳密な証明は高温に限られ，低温域では解の一意性が失われるのが困難を窺わせる．２つの層の分離面としては Almeida-Thouless 線が提案されている．\nしかし (Bolthausen, 2014) は 近似メッセージパッシング に基づいて，この TAP 方程式の解を与えるアルゴリズムを提案した．このアルゴリズムは，高次元統計学において \\(M\\)-推定量を計算するのにも応用されている (Donoho & Montanari, 2016)．高次元漸近論は計算科学の進歩とともにあるのである．\n\n\n2.3.4 Hopfield 模型 (Hopfield, 1982)\nのちにスピングラスの理論は 連想記憶 にも応用され，広く情報処理の問題を統計力学の技法によって研究する情報統計力学という新たな分野が開拓された．\n連想記憶のニューラルネットワーク は無限レンジ，すなわち全結合のニューラルネットワークで，素子 \\(\\{S_i\\}_{i=1}^N\\subset\\mathrm{Map}(T;\\{\\pm1\\})\\) からなるとき， \\[\nS_i(t+\\Delta t)=\\mathop{\\mathrm{sgn}}\\left(\\sum_{j\\ne i}J_{ij}\\frac{S_j(t)+1}{2}-\\theta_i\\right)\n\\] という規則で運動する．\\(J_{ij},\\theta_i\\) がモデルパラメータである．\n\\(J_{ij}\\) をうまく「学習」できた際には，一部の初期値について，この運動の収束先として画像が「連想」出来る．記憶しておけるのである．13\n実は，\\(p\\) 個のパターン \\((\\xi^\\mu_i)_{i=1}^N\\in\\Omega\\;(\\mu=1,\\cdots,p)\\) を記憶させるには， \\[\nJ_{ij}=1_{\\left\\{i\\ne j\\right\\}}\\frac{1}{N}\\sum_{\\mu=1}^p\\xi_i^\\mu\\xi_j^\\mu\n\\] と設定すると良いことが知られており，これを Hebb 則 (Hebb, 1949) という．\n連想記憶がうまくいくためには，互いの直交性 \\[\n\\frac{1}{N}(\\xi^\\mu|\\xi^\\nu)=\\delta_{\\mu,\\nu}+O\\left(\\frac{1}{\\sqrt{N}}\\right)\n\\] が重要であることも知られている．\n実は，Hebb 則によるパラメータを備えた Hopfield 模型は，結合が対称である \\(J_{ij}=J_{ji}\\) とき，次の Hamiltonian を減少させる方向に運動する： \\[\nH=-\\frac{1}{2}\\sum_{i\\ne j}J_{ij}S_iS_j\n\\]\nここで係数 \\(J_{ij}\\) はデータ \\((\\xi^\\mu)_{\\mu=1}^p\\) から決まっているという意味では，確率変数であることに注意．\nこの模型を統計力学的に解析すると，Hopfield 模型は，想起相だけでなく，常磁性相ももち，その間にスピングラス相がある．これは \\(\\alpha=\\frac{p}{N}\\) を大きくすると到達することができる (西森秀稔, 2003, p. 57)．\n素子数一定の状況下で，覚えるパターン数を増やしすぎると，ある瞬間に相転移を起こして何も覚えなくなるのである．\n\n\n\n2.4 因子グラフ\n特に近距離相互作用のみを仮定している場面では，ハミルトニアン \\(H\\) やその他の物理量の 局所性 が目立った（コンパクト台を持つ関数になっている）．\nそのこともあり，物理系はグラフィカルモデル（Bayesian networks, Markov networks）としての表現と親和性があり，特に 因子グラフ も重要な形式として用いられる (Mézard & Montanari, 2009, p. 100)．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#参考文献",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#参考文献",
    "title": "数学者のための統計力学１",
    "section": "3 参考文献",
    "text": "3 参考文献\n\n(Minlos, 2000), (西森秀稔, 2003), (Altieri & Baity-Jesi, 2024), (Chatterjee, 2023), (Panchenko, 2012), (Talagrand, 2003), (Bolthausen, 2014)"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#footnotes",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#footnotes",
    "title": "数学者のための統計力学１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nすなわち，\\(\\Lambda\\) 上の線型束である．↩︎\n\\(V\\) は境界ポテンシャルといい，外場との相互作用がある場合に現れる．↩︎\nこれは粒子間相互作用が short-range であると仮定しているためである．重力やクーロン力を考えている訳ではない．↩︎\n統数研プロジェクト紹介 も参照．↩︎\n古典 Heisenberg 模型 はその例である．↩︎\n(西森秀稔, 2003, p. 4)．情報統計力学では，スピンをビットやニューロンの状態に見立てる．↩︎\n(Baxter, 1982, p. 21) では nearest-neighbour Ising model と呼んでいる．↩︎\n無限レンジの仮定をおくと，平均場近似は近似でなくなる，という論理的依存関係がある (西森秀稔, 2003, p. 26)．↩︎\n(Mézard & Montanari, 2009, p. 168) (西森秀稔, 2003, p. 16) など．スピングラスはもともと B. Coles が希薄合金の磁性を表現するために造語したが，現在は「全くランダムな状態でスピンが凍結した状態」という意味で使われるようになている (都福仁, 1977)．↩︎\nMydosh による講演が，髙山一氏のスピングラス研究の発端となったという（最終講義）．↩︎\n(田中利幸, 2007)↩︎\n(Panchenko, 2012) も参照．↩︎\nパターンを記憶させることを「埋め込む」ともいう (西森秀稔, 2003, p. 33)．↩︎"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#つの関係",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#つの関係",
    "title": "数学者のための統計力学２",
    "section": "3 ２つの関係",
    "text": "3 ２つの関係\n\\(\\Lambda_0\\subset\\Lambda\\) を部分領域として，この領域に \\(s\\) 個の粒子が存在する状態 \\[\n\\Omega_{\\Lambda,N}^{\\Lambda_0,s}:=\\left\\{(Q,V)\\in\\Omega_{V,N}\\mid\\lvert Q\\cap\\Lambda_0\\rvert=s\\right\\}\n\\] を考えると，この部分領域への小正準分布 \\(P^{\\text{microcanon}}_{\\Lambda,N,E}\\) の制限は正準分布になる (Minlos, 2000)．\nすなわち，ある領域 \\(\\Lambda_0\\) に \\(s\\) 個の粒子が存在する限り値が変わらないような量 \\(F:\\Omega_{\\Lambda,N}\\to\\mathbb{R}\\) は，小正準平均も正準平均も「ほとんど」変わらないことになる．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#大正準集団",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#大正準集団",
    "title": "数学者のための統計力学２",
    "section": "4 大正準集団",
    "text": "4 大正準集団\n大正準集団は，領域 \\(\\Lambda\\subset\\mathbb{R}^3\\) に粒子数 \\(N\\in\\mathbb{N}\\) を定めずに存在する互いに区別できない粒子の系を考えることになる（エネルギーの交換だけでなく粒子も交換する部分系など）： \\[\n\\Omega_\\Lambda=C_\\Lambda^{(0)}\\cup C_\\Lambda^{(1)}\\cup\\cdots\\cup C_\\lambda^{(N)}\\cup\\cdots\n\\]\nこの \\(\\Omega_\\Lambda\\) 上に，各 \\(C_\\Lambda^{(N)}\\) 上での Lebesgue 測度が誘導する測度 (Lebesgue-Poisson measure) \\[\n\\mu_\\Lambda^{(N)}(A):=\\frac{\\ell_{3N}(A)}{N!}\n\\] の貼り合わせとして定義される測度を正規化したものを \\(\\mu\\) とし，これを体積の代わりとする．\nこれを基底測度として密度 \\[\np^{\\text{grandcanon}}_{\\Lambda,\\beta,\\mu}(c):=\\frac{1}{\\Xi(\\Lambda,\\beta,\\mu)}e^{-\\beta(H_\\Lambda(c)+\\mu N(c))}\n\\] が定める分布を 大正準分布 という．\n\\(\\beta&gt;0\\) は逆温度であるが，\\(\\mu\\in\\mathbb{R}\\) は化学ポテンシャルである．\n小正準集団が正準集団になる際に，エネルギー一定の制約は解放されて，代わりに新たなパラメータ \\(\\beta&gt;0\\) を得た．ここからさらに粒子数一定の制約を解放し，代わりに新たなパラメータ \\(\\mu\\in\\mathbb{R}\\) を得たものが大正準集団である．この順に解析が容易になる．\n大正準分布を粒子数一定の条件で条件付けて得る \\(C_\\Lambda^{(N)}\\) 上の分布は正準分布に一致する．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#参考文献",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#参考文献",
    "title": "数学者のための統計力学２",
    "section": "6 参考文献",
    "text": "6 参考文献\n\n(Minlos, 2000), (西森秀稔, 2003), (Baxter, 1982)"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#footnotes",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#footnotes",
    "title": "数学者のための統計力学２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n運動は相空間の自然な体積形式 (Liouville 形式ともいう) を不変に保つという Liouville の定理 (Khinchin, 1949) は量子論でも成り立つ．(戸田盛和 et al., 2011, p. 23) これらの性質を抽象した形で，古典・量子力学は，統計力学に等重率の仮定を課す．↩︎\nただし，\\(\\Omega_{\\Lambda,N,E}\\) 上に制限された体積測度に関しては，密度 \\(\\frac{1}{\\lvert\\mathop{\\mathrm{\\mathrm{grad}}}H\\rvert}\\) を持つ (Khinchin, 1949)．↩︎\nこれを Liouville 形式だけでなく Liouville 測度ということもあるようである (砂田利一, 2004)．↩︎\n等重率の仮定は量子論の観点から「マクロな量子系では，ある平衡状態に対応する許容される量子状態の全てが区別できない」と説明されることも多い．いずれにしろ極めて非自明な主張であるが，これを認めてみると良い理論を得る．実際 (田崎晴明, 2008, p. 89) では等重率の仮定を「戦略」「等重率の原理に基づく確率モデル」と説明している．いわば，統計モデルの１つであり，モデル選択の観点に立っているのである．↩︎\n\\(S_t\\) は体積を保存し Hamiltonian も保存するため，これは明らかに \\(S_t\\) の不変確率分布である．↩︎"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#つの関係と熱力学極限",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#つの関係と熱力学極限",
    "title": "数学者のための統計力学２",
    "section": "3 ２つの関係と熱力学極限",
    "text": "3 ２つの関係と熱力学極限\n大雑把に捉えれば，エネルギー一定の力学系（例えば孤立系）の全ての状態の上の分布がミクロカノニカル分布であり，このエネルギーも動かした場合（例えば孤立系の部分系），各エネルギー上の分布はカノニカル分布になる．\n\\(\\Lambda_0\\subset\\Lambda\\) を部分領域として，この領域に \\(s\\) 個の粒子が存在する状態 \\[\n\\Omega_{\\Lambda,N}^{\\Lambda_0,s}:=\\left\\{(Q,V)\\in\\Omega_{V,N}\\mid\\lvert Q\\cap\\Lambda_0\\rvert=s\\right\\}\n\\] を考え，この部分領域への小正準分布 \\(P^{\\text{microcanon}}_{\\Lambda,N,E}\\) の制限を考えると，ある極限に関して正準分布になる (Minlos, 2000)．これがいわば 熱浴 や thermostat の概念である．\nすなわち，ある領域 \\(\\Lambda_0\\) に \\(s\\) 個の粒子が存在する限り値が変わらないような量 \\(F:\\Omega_{\\Lambda,N}\\to\\mathbb{R}\\) は，小正準平均も正準平均も「ほとんど」変わらないことになる．この極限の正確な意味は \\[\n\\Lambda\\nearrow\\mathbb{R}^v,N\\to\\infty,\\frac{N}{\\lvert\\Lambda\\rvert}\\to\\rho\\in\\mathbb{R},\\frac{E_i}{\\lvert\\Lambda_i\\rvert}\\to e\\in\\mathbb{R},\n\\] というものであり，これを 熱力学極限 という（第 5 節）．\n\n一般に極めて多数の自由度を持つ力学系 B と，問題の対象たる一つの力学系 A（上の例では \\(s\\) 個の粒子からなる系）とが結合しているときに，系 A の一つの微視状態の実現確率を与える．これはそのような条件の下に，前節に述べた等重率の仮定に代わるものである． 物理的な言葉に引き直せば，結局，熱容量の大きい物体と熱平衡を保つ任意の力学系の統計的分布を表す集団がカノニカル集団である．(久保亮五, 2003, p. 30)"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics2.html#sec-thermodynamic-limit",
    "href": "posts/2024/Nature/StatisticalMechanics2.html#sec-thermodynamic-limit",
    "title": "数学者のための統計力学２",
    "section": "5 熱力学極限",
    "text": "5 熱力学極限\n\\(F_B:\\Omega_\\Lambda\\to\\mathbb{R}\\) が 局所変数 であるとは， \\[\nF_B(c)=F_B(c\\cap B)\\quad(c\\in\\Omega_\\Lambda)\n\\] を満たすことをいう．\n熱力学極限 \\[\n\\lim_{\\Lambda\\nearrow\\mathbb{R}^3}\\langle F_B\\rangle_{\\Lambda,\\beta,\\mu}=\\langle F_B\\rangle_{\\infty,\\beta,\\mu}\n\\] は，ある空間上のある積分と捉えられる．これを 極限 Gibbs 測度 (limit Gibbs distribution) という．\nこの極限 Gibbs 測度が複数存在したり，良い性質が失われたりする現象を 相転移 といい，その際の \\((\\beta,\\mu)\\) を特異点という．\n熱力学では，系の変化が極めて緩慢であるために，一瞬一瞬において平衡状態が保たれているとみなせる物理的過程が扱われるため，大正準集団では \\(T=\\beta^{-1},\\mu\\)，正準集団では \\(T,\\rho\\)，小正準集団では \\(\\rho,e\\) などのマクロ変量によって記述できる理論になっている．\nそして熱力学極限において，これら３集団は等価であるから，これらはパラメータ変換の問題でしかなく，さらにこのパラメータ空間上に別の関数も導入される．特にエントロピー \\(s(e,\\rho)\\)，Helmholz の自由エネルギー（密度） \\(f(\\beta,\\rho)\\)，圧力 \\(p(\\beta,\\mu)\\) などであり，これらは互いに Legendre 変換により関係し合っている．\nこれらの熱力学的関数も，特定の統計力学的な量の熱力学極限として理解できる．\n\\[\ns(\\rho,e)=\\lim_{\\substack{\\Lambda\\nearrow\\mathbb{Z}^v\\\\\\frac{N}{\\lvert\\Lambda\\rvert}\\to\\rho\\\\\\frac{E}{\\Lambda}\\to e}}\\frac{\\log Q^{\\text{indistin}}(\\Lambda,E,N)}{\\lvert\\Lambda\\rvert}\n\\] \\[\nf(\\beta,\\rho)=-\\frac{1}{\\beta}\\lim_{\\substack{\\Lambda\\nearrow\\mathbb{Z}^3\\\\\\frac{N}{\\lvert\\Lambda\\rvert}\\to\\rho}}\\frac{\\log Z^{\\text{indistin}}(\\Lambda,N,\\beta)}{\\lvert\\Lambda\\rvert}\n\\] \\[\np(\\beta,\\mu)=\\frac{1}{\\beta}\\lim_{N\\nearrow\\mathbb{Z}^3}\\frac{\\log\\Xi(\\Lambda,\\mu,\\beta)}{\\lvert\\Lambda\\rvert}\n\\]\nただし，\\(Q^{\\text{indistin}},Z^{\\text{indistin}},\\Xi\\) はそれぞれ，（粒子が互いに区別がつかない場合の）小正準集団，正準集団，大正準集団の正規化定数とした．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-SA",
    "href": "posts/Surveys/SMCSamplers.html#sec-SA",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "これは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，凝縮系物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\mathop{\\mathrm{arg\\,min}}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-ST",
    "href": "posts/Surveys/SMCSamplers.html#sec-ST",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "最適化手法である 焼きなまし法（または擬似アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 擬似テンパリング (simulated tempering) (Marinari & Parisi, 1992) である．\n擬似アニーリングでは温度は下がる一方であったのが，擬似テンパリングでは温度もある周辺分布に従って遷移する．擬似アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，擬似テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n擬似テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．9 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer & Thompson, 1995)．"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#例",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#例",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "4 例",
    "text": "4 例\nあ"
  },
  {
    "objectID": "posts/2024/Probability/Kernel.html#導入",
    "href": "posts/2024/Probability/Kernel.html#導入",
    "title": "確率核という概念",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 確率核が数学的に重要な理由\n確率核 を確率空間の射とみる見方が急速に浸透しており，これは Markov 圏 の概念にも後押しされて急速に浸透しつつある．\n特にマルコフ過程の解析においては必要不可欠な役割を果たす．\nだが，まだ純粋に数学的な文献を除いて，あまりポピュラーな概念であるとは言えないのが現状であろう．\n\n\n1.2 圏論についての補足\nなお，射 という用語も，代数幾何から他の数学分野へ浸透したという実感があるが，近年は純粋数学のコミュニティからも出つつあると感じられる．射の概念について，重要なコメントをここに共有したい：\n\n余談だが、homomorphism の訳語として、準同形ということばが定着している。これは、同形もどきという意味だから、同形がだいじというブルバキの思想を反映したものといえよう。射のほうが基本的という、より現代的な視点にはそぐわないが、いまさら変えることもできないだろう。(斎藤毅, 2010)"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#背景",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#背景",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "2 背景",
    "text": "2 背景\n\n一様エルゴード性を strongly ergodic とも呼んでいる： \\[\n\\sup_{x\\in E}\\|P^n(x,-)-\\pi\\|_\\mathrm{TV}\\le Ce^{-\\lambda n}.\n\\]\n一方で，各点 \\(x\\in E\\) で \\[\n\\|P^n(x,-)-\\pi\\|_\\mathrm{TV}\\to0\n\\] が成り立つことを weakly ergodic と呼んでいる．\n\n本論文では，\\(\\lambda\\) を推定する (Diaconis & Stroock, 1991) 理論を，weakly ergodic の場合と非対称な場合に拡張する．\nすると，(Diaconis & Stroock, 1991) 理論では遷移確率核 \\(P\\) のスペクトルギャップであった \\(\\lambda\\) は，一般の設定の下でもある一般化した半群生成作用素のスペクトル半径に関係することがわかった．\n\n2.1 (Diaconis & Stroock, 1991) 理論\n一様エルゴード性の収束速度 \\(\\lambda\\) を定量化するアプローチの１つ．\n\n\n\n\n\n\n定理\n\n\n\n有限状態空間 \\(E\\) 上の \\(P\\)-一様 Markov 連鎖は，既約かつ対称ならば， \\[\n\\lambda&lt;\\log\\operatorname{Gap}(P)\n\\] \\[\n\\operatorname{Gap}(P):=\\max\\left\\{\\lvert\\lambda\\rvert\\in\\mathbb{R}_+\\mid 1&gt;\\lambda\\in\\mathrm{Sp}(P)\\right\\}\n\\]\n\n\n\n対称ならば \\(P=P^*\\)．\nスペクトルギャップは一般の正作用素に定義できる．\n\n\n\n2.2 (Vaserstein, 1969) による最適カップリングの構成\nこれを一般化したという．\n最適な Markov カップリングよりも，カップリング確率が高いカップリングがあるらしい（しかし Markov にならない）．この稿 に書いた．\n\n\n2.3 Lyapunov 型の条件\n(Douc et al., 2004), (Kalashinikov, 1973), (Lamperti, 1960), (Rosenthal, 2002), (Tweedie, 1981) による．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#mcmc-とは何か",
    "href": "posts/2024/Computation/MCMC.html#mcmc-とは何か",
    "title": "新時代の MCMC を迎えるために",
    "section": "1 MCMC とは何か？",
    "text": "1 MCMC とは何か？\nMCMC とは，確率変数をシミュレーションする際に用いられる汎用的アルゴリズムです．\n一様分布や正規分布などの名前がついた分布ではない場合，どのようにすればその分布に従う確率変数をシミュレーションできるのかは極めて難しい問題です．\n実際，MCMC では空間を時々刻々と移動するマルコフ連鎖をうまく構成し，その軌跡がちょうど確率変数のシミュレーションになっていると聞いても，なぜそのような回りくどい方法を使うのか？本当にうまくいくのか？疑問が絶えないでしょう．\nですが，MCMC が発明された経緯である物理学の問題から見てみると，実は極めて自然に思えてくるかもしれません．\n\n1.1 着想経緯\n\n\n\n\n\n\nよりみち：どうして MCMC が必要だったのか？\n\n\n\n\n\n(Metropolis et al., 1953) では，温度 \\(T\\) 一定の条件下で，\\(N\\) 粒子系をシミュレートし，任意の物理量 \\(F\\) に対してその相空間上の平均 \\[\n\\langle F\\rangle=\\frac{\\int Fe^{-\\frac{E}{kT}}dp}{\\int e^{-\\frac{E}{kT}}dp}\n\\] を効率的に計算するアルゴリズムが提案された．これが現在では Metropolis 法と呼ばれている．2\n(Metropolis et al., 1953) では \\(N\\) が数百になる場合を考えており，当然愚直な数値積分は（現代の計算機でも）実行可能ではない．そこで Monte Carlo 法を考えることになるが，当時 Monte Carlo 法といえば，一様乱数を用いた計算法の全般を指し，それを用いると \\(\\langle F\\rangle\\) を重点サンプリング推定量 \\[\n\\widehat{F}=\\frac{\\sum_{n=1}^NF(\\omega)e^{-\\frac{E(\\omega)}{kT}}}{\\sum_{n=1}^Ne^{-\\frac{E(\\omega)}{kT}}}\n\\] で推定することを指した．3\nしかしこれでは，配置 \\(\\omega\\in\\Omega\\) を完全に一様に生成するため，高エネルギーな配置も生成しやすく，そのようなサンプルは \\(\\langle F\\rangle\\) の推定にあまり寄与しない．\nこれを低減することが出来れば Monte Carlo 法の更なる効率改善に繋がる．こうして，Gibbs 分布 \\(\\frac{1}{Z}e^{-\\frac{E}{kT}}\\) からの直接的サンプリングを実行することが考えられた．\n\n\n\n(Metropolis et al., 1953) では，Boltzmann-Gibbs 分布 \\(\\frac{1}{Z}e^{-\\frac{E}{kT}}\\) から直接サンプリングする方法が探求されました．\nここでは簡単のため，１粒子が次のようなポテンシャルに従って運動する場合を考えましょう：\n\n\n\nポテンシャル \\(E\\) のプロット\n\n\nこのポテンシャルに関する Boltzmann-Gibbs 分布は次のような形になります：\n\n\n\nポテンシャル \\(E\\) が定める Botlzmann-Gibbs 分布のプロット\n\n\n低エネルギー状態が現れやすく，エネルギーが上がるにつれて急激に現れにくくなることがわかります．\n\n\nCode\nimport numpy as np\n\ndef U(x):\n    return x**2/2 + x**4/4\n\ndef pi(x):\n    return np.exp(-U(x))\n\ndef metropolis(num_samples, initial_state, verbose=False):\n    samples = [initial_state]\n    current_state = initial_state\n\n    accept = []\n\n    for _ in range(num_samples - 1):\n        proposed_state = current_state + np.random.uniform(-2,2)\n        acceptance_ratio = pi(proposed_state) / pi(current_state)\n        if np.random.rand() &lt; acceptance_ratio:\n            current_state = proposed_state\n            accept.append(True)\n        samples.append(current_state)\n\n    if verbose:\n      rate = len(accept) / num_samples\n      print(f'acceptance rate : {rate}')\n\n    return np.array(samples)\n\n# サンプル数と初期状態を固定\nnum_samples = 10000\ninitial_state = 0.0\n\n\n\n\nCode\nimport pints.plot\nimport matplotlib.pyplot as plt\n\nsamples_MH = metropolis(num_samples, initial_state)\nsamples_MH.shape = (num_samples,1)\npints.plot.autocorrelation(samples_MH, parameter_names=['Samples'], max_lags=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure()\nplt.plot(samples_MH[0:50], range(50), color='green', linestyle='dashed', label='First 50 samples')\nplt.legend(loc=4, prop={'size': 10})\nplt.title('Metropolis-Hastings')\nplt.xlabel('Sample value')\nplt.ylabel('Sample index')\nplt.ylim(-0.5, 49.5)  # 軸の範囲を設定\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2 統計学への応用\nこうして MCMC が発明されれば，すぐにイノベーションとして理解されたかというとそうではありませんでした．\nこの Metropolis の手法が極めて賢いシミュレーション手法であることは一目瞭然でも，一般の確率分布からのサンプリングに使える汎用アルゴリズムになっているという抽象的な観点が得られるまでには時間を要しました．これを成し遂げたのが (Hastings, 1970) でした．4\nさらにこの結果も見過ごされました．真にMCMC 法一般を統計学界隈に広め，ベイズ統計学の興隆につながったのは (Gelfand & Smith, 1990) だと言われます．5\n\n\n1.3 リフティング\n\n\nCode\ndef lifted_metropolis(num_samples, initial_state, verbose=False):\n    samples = [initial_state]\n    current_state = initial_state\n    lifting_variable = 1\n    accept = []\n\n    for _ in range(num_samples - 1):\n        delta = np.random.uniform(0,2)\n        proposed_state = current_state + lifting_variable * delta\n        acceptance_ratio = pi(proposed_state) / pi(current_state)\n\n        if np.random.rand() &lt; acceptance_ratio:\n            current_state = proposed_state\n            accept.append(True)\n        else:\n            lifting_variable = (-1) * lifting_variable\n\n        samples.append(current_state)\n    \n    if verbose:\n      rate = len(accept) / num_samples\n      print(f'acceptance rate : {rate}')\n\n    return np.array(samples)\n\n\n\n\nCode\nimport pints.plot\nimport matplotlib.pyplot as plt\n\nsamples_LMH = lifted_metropolis(num_samples, initial_state)\nsamples_LMH.shape = (num_samples,1)\npints.plot.autocorrelation(samples_LMH, parameter_names=['Samples'], max_lags=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure()\nplt.plot(samples_LMH[0:50], range(50), color='green', linestyle='dashed', label='First 50 samples')\nplt.legend(loc=4, prop={'size': 10})\nplt.title('Lifted Metropolis-Hastings')\nplt.xlabel('Sample value')\nplt.ylabel('Sample index')\nplt.ylim(-0.5, 49.5)  # 軸の範囲を設定\nplt.show()"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html",
    "href": "posts/2024/Review/Metropolis+1953.html",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#概要",
    "href": "posts/2024/Review/Metropolis+1953.html#概要",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "1 概要",
    "text": "1 概要\n相互作用する分子系からなる物質の状態方程式などの性質を調べるために使える汎用手法（ fast computing machine にぴったり！）を提案する．この手法は配置空間上の，提案分布を正確にした「修正版 Monte Carlo 積分法」だと捉えられる．MANIAC を用いて，２次元剛体球の場合のシミュレーション結果を付した．その結果を，自由体積状態方程式と，４項ビリアル係数展開と比較した．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#設定",
    "href": "posts/2024/Review/Metropolis+1953.html#設定",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "3 設定",
    "text": "3 設定\n古典統計を仮定し，２体間相互作用のみを考え，ポテンシャルは球対称であるとする（流体力学では通常の仮定である）．だが，温度や密度には全く仮定を置かない．1\n実際の計算のために，粒子数 \\(N\\) は several hundred に取る．そして正方形の中にいれ，境界条件を最小化するために同様の系が２次元に無限に連なっているとする．２つの粒子 \\(A\\) の他の粒子 \\(B\\) との最短距離を \\(d_{A,B}\\) とし，これのみが粒子 \\(A\\) にかかる主な力になるとする．\n仮に \\(N=1\\) だとしたら，これは cell method と呼ばれるモデルでもある．こうして粒子を増やすことで，単一相のシステムに対するより良いモデルになるだろうが，二相以上のシステムには限界がある．\n以上の仮定から，系のエネルギーが次のように与えられる： \\[\nE=\\frac{1}{2}\\sum_{i\\ne j\\in[N]}V(d_{ij}).\n\\]\nこの系の平衡状態の性質を計算するには，Gibbs の正準分布を利用し，計算したい物理量 \\(F\\) に対して \\[\n\\overline{F}=\\frac{\\int Fe^{-\\frac{E}{kT}}d^{2N}pd^{2N}q}{\\int e^{-\\frac{E}{kT}}d^{2N}pd^{2N}q}\n\\] を計算すれば良い．ただし，\\(d^{2n}pd^{2n}q\\) は \\(4N\\) 次元相空間上の体積要素である．\n加えて，ここではポテンシャル \\(V\\) は位置のみの引数としているから，\\(2N\\) 次元上でのみ計算すれば良い．\nこのような数百次元上での積分を数値的方法で実行するのは明らかに実行可能でないから，Monte Carlo 法に頼らざるを得ない．と言っても，決定論的な点で値を計算する代わりに，ランダムに点をうつ，というだけの違いではある．\n最も簡単な実装としては，ランダムに \\(N\\) 粒子を配置してエネルギーを計算し（重点荷重），これにウェイトをつけて足していくということが考えられる（重点サンプリングだ！）．しかし，低エネルギーの配置もたくさん生成してしまうから，これによる効率の低減が避けられない（提案分布が悪いのだ！）．\nそこで我々は modified Monte Carlo method を考える．そもそも確率 \\(e^{-\\frac{E}{kT}}\\) からサンプルを生成し，荷重を一様にすることを目指す．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#footnotes",
    "href": "posts/2024/Review/Metropolis+1953.html#footnotes",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(戸田盛和 et al., 2011, p. 14)↩︎\nさらに，Lennard-Jones ポテンシャルについての２次元のケースも考えており，次のレポートで報告される予定であるという．↩︎\nここで乱数生成法に関して注記されている．これは middle square process で生成する，としている．↩︎\nなお，periodic assumption をしているため（長方形の系の外には同様の系が無数に並んでいるとしたため），境界の外に出ようとした場合は衝突するのではなく，反対側の辺から入ってくるものとする．↩︎\n(Robert & Casella, 2011) も指摘している．だが，(戸田盛和 et al., 2011, p. 5) にも同じ意味で「エルゴード的」という単語を使っている記述がある．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#背景",
    "href": "posts/2024/Review/Metropolis+1953.html#背景",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "2 背景",
    "text": "2 背景\nむしろ cell method などの計算手法との類似で捉えており，(Hastings, 1970) に取り上げられるまで全く別個の，広く抽象的に応用されるポテンシャルを持つ技術であるという抽象的な観点とは離れた泥臭い探索を感じる．\nまだ Markov 連鎖という単語も使われていない．\n話の展開は，分子系のシミュレーションの問題を，Gibbs 分布に関する積分計算の問題に帰着し，Monte Carlo 法を用いるところまで還元するが，そこで不適当な提案分布による重点サンプリングを実行するよりかは，系の Gibbs 分布を直接シミュレーションすれば良い，というものである．\nよってここでいう修正 Monte Carlo 法とは，Monte Carlo 法（ここでは重点サンプリングと同義，（一様）乱数を用いた数値計算，くらいの意味）による乱数生成過程を，少しだけ MD 法の考え方を取り入れて，より Gibbs 分布に則した乱数生成をする，くらいの提案である．\n後世では次のようにも説明されている始末である：\n\nThe Monte Carlo method addresses the sampling problem more abstractly than molecular dynamics, as it samples (obtains samples \\(x\\) from) the distribution \\(\\pi_{24}(x)\\) without simulating a physical process. (Tartero & Krauth, 2023)\n\nもはや，愚直な Monte Carlo 法が MD 法で，運動方程式を解かない MD 法が Monte Carlo 法，という具合である．事実，気体の状態方程式は，気体分子の運動の力学には無関係に成り立つので，必ずしも正しい力学に従ったシミュレーションが必要というわけではないのである．1\nなお，Monte Carlo 法の提案者として Joseph Edward Mayer と Stanisław Ulam の名前を挙げている．\nここで Berni Alder の名前が上がっており，やはり MCMC の開発は MD と極めて深い関係にあることが伺える．\n\nThis method has been proposed independently by J. E. Mayer and by S. Ulam. Mayer suggested the method as a tool to deal with the problem of the liquid state, while Ulam proposed it as a procedure of general usefulness. B. Alder, J. Kirkwood, S. Frankel, and V. Lewinson discussed an application very similar to ours.\n\n\n2.1 Monte Carlo 法の起源について\nJohn von Neumann は Edward Teller と同郷であり，戦時中の ENIAC の開発にも関わっていたことから，偉い人たちを説得して最初の ENIAC のテストに熱核融合反応の計算問題を用いることにした．\n計算可能なモデルの構築をしたのが Metropolis である．これが完成し，実際にテストが行われたのは戦後の 1946 年であったが．\nそのお披露目会に居合わせた Stanislaw Ulam が，統計的サンプリング技術を電子計算機で復活させることを提案し，Johnny がすぐさまその重要性を理解した．これがモンテカルロ法の始まりとなった，という (Metropolis, 1987)．\nMetropolis は Ulam が着想を得た理由として，数学的な背景を持っていたために，統計的サンプリング技術が計算の難しさのために歴史に埋没したことを知っており，ENIAC のポテンシャルを見てこれと関連づけることに成功したのではないかと示唆している．\nそして Johnny の熱の入り用が周りも刺激した．1947 年には統計的サンプリングの中でも特に中性子の拡散問題を取り上げて当時の Los Alamos の理論部リーダーであった Robert Richtmyer に手紙を送った (Eckhardt, 1987)．こうして周りを巻き込んで大ごとになっていった．Monte Carlo 法という命名も 1947 年だったという\n\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo.”\n\n\nOn a less grand scale these events brought about a renascence of a mathematical technique known to the old guard as statistical sampling; in its new surroundings and owing to its nature, there was no denying its new name of the Monte Carlo method. (Metropolis, 1987)\n\n特に戦時中の関心もあり，核分裂時の 中性子の拡散 のシミュレーションが問題であった．Monte Carlo 法と呼んでいるが本質的に MD 法チックであり，ここの中性子の散乱・吸収・分裂の系譜をシミュレートすることで全体の統計的性質がわかる，というだけの話であった．\nその後 1952 年には後続機の MANIAC が開発され，nucleaer cascade と状態方程式も射程に入った．\nこの状態方程式を取り扱う際に (Metropolis et al., 1953) がさらに効率的な「モンテカルロ法」を発明したのである．これは Gibbs 分布を直接シミュレーションできるというブレイクスルーであり，「一般の確率分布からサンプリングできる」という今の理解とは大きく異なる文脈の中で発見されたと言うべきである．\n\n\n2.2 Monte Carlo 法とはなんだろうか\n思うに，「ランダムな方法を使って計算する」というのは外道に思えるかもしれない．\nだが，実はランダムな系の \\(\\mathcal{P}(E)\\) 上のダイナミクスの決定論的な計算になっているのかもしれない．\nそう思わせるだけの透徹性が測度論にはある．\nただ，(Metropolis, 1987) は Monte Carlo 法を 実験数学 (experimental mathematics) と呼んでおり，極めて物理学的な見方で評している：\n\nAt long last, mathematics achieved a certain parity–the twofold aspect of experiment and theory–that all other sciences enjoy.\n\n\n\n2.3 他のコメント\n\nNote that (Metropolis et al., 1953) move one particle at a time, rather than moving all of them together, which makes the initial algorithm appear a primitive kind of Gibbs sampler! (Robert & Casella, 2011)"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#アルゴリズムの記述",
    "href": "posts/2024/Review/Metropolis+1953.html#アルゴリズムの記述",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "4 アルゴリズムの記述",
    "text": "4 アルゴリズムの記述\nこれは次のようにする．まず適当に初期分布を決める（格子点上に \\(N\\) 粒子を配置するなど）．そしてこれをアップデートしていく： \\[\nX\\mapsto X+\\alpha\\xi_1\n\\] \\[\nY\\mapsto Y+\\alpha\\xi_2\n\\] \\(\\alpha&gt;0\\) は一度にどれくらい動かすかを調節するパラメータであり，\\(\\xi_1,\\xi_2\\in(0,1)\\) は一様乱数とする．2\nすなわち，\\((X,Y)\\) を中心とした一辺 \\(2\\alpha\\) の正方形の中で，新たな位置をランダムに決めるのである．3\nこの動きによるエネルギーの変化量 \\(\\Delta E\\) を計算し，\\(\\Delta E&lt;0\\) ならばこれを実行するが，\\(\\Delta E&gt;0\\) ならば確率 \\(e^{-\\frac{\\Delta E}{kT}}\\) によって採択する．\n仮に棄却されたとしても，そのポジションから新たな Monte Carlo 標本を取り，最終的に \\[\n\\overline{F}=\\frac{1}{M}\\sum_{j=1}^MF_j\n\\] を推定量とする．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#本論",
    "href": "posts/2024/Review/Metropolis+1953.html#本論",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "3 本論",
    "text": "3 本論\n\n3.1 設定\n古典統計を仮定し，２体間相互作用のみを考え，ポテンシャルは球対称であるとする（流体力学では通常の仮定である）．だが，温度や密度には全く仮定を置かない．2\n実際の計算のために，粒子数 \\(N\\) は several hundred に取る．そして正方形の中にいれ，境界条件を最小化するために同様の系が２次元に無限に連なっているとする．２つの粒子 \\(A\\) の他の粒子 \\(B\\) との最短距離を \\(d_{A,B}\\) とし，これのみが粒子 \\(A\\) にかかる主な力になるとする．\n仮に \\(N=1\\) だとしたら，これは cell method と呼ばれるモデルでもある．こうして粒子を増やすことで，単一相のシステムに対するより良いモデルになるだろうが，二相以上のシステムには限界がある．\n以上の仮定から，系のエネルギーが次のように与えられる： \\[\nE=\\frac{1}{2}\\sum_{i\\ne j\\in[N]}V(d_{ij}).\n\\]\nこの系の平衡状態の性質を計算するには，Gibbs の正準分布を利用し，計算したい物理量 \\(F\\) に対して \\[\n\\overline{F}=\\frac{\\int Fe^{-\\frac{E}{kT}}d^{2N}pd^{2N}q}{\\int e^{-\\frac{E}{kT}}d^{2N}pd^{2N}q}\n\\] を計算すれば良い．ただし，\\(d^{2n}pd^{2n}q\\) は \\(4N\\) 次元相空間上の体積要素である．\n加えて，ここではポテンシャル \\(V\\) は位置のみの引数としているから，\\(2N\\) 次元上でのみ計算すれば良い．\nこのような数百次元上での積分を数値的方法で実行するのは明らかに実行可能でないから，Monte Carlo 法に頼らざるを得ない．と言っても，決定論的な点で値を計算する代わりに，ランダムに点をうつ，というだけの違いではある．\n最も簡単な実装としては，ランダムに \\(N\\) 粒子を配置してエネルギーを計算し（重点荷重），これにウェイトをつけて足していくということが考えられる（重点サンプリングだ！）．しかし，高エネルギーの配置もたくさん生成してしまうから，これによる効率の低減が避けられない（提案分布が悪いのだ！）．\nそこで我々は modified Monte Carlo method を考える．そもそも確率 \\(e^{-\\frac{E}{kT}}\\) からサンプルを生成し，荷重を一様にすることを目指す．\n\n\n3.2 アルゴリズムの記述\nこれは次のようにする．まず適当に初期分布を決める（格子点上に \\(N\\) 粒子を配置するなど）．そしてこれをアップデートしていく： \\[\nX\\mapsto X+\\alpha\\xi_1\n\\] \\[\nY\\mapsto Y+\\alpha\\xi_2\n\\] \\(\\alpha&gt;0\\) は一度にどれくらい動かすかを調節するパラメータであり，\\(\\xi_1,\\xi_2\\in(0,1)\\) は一様乱数とする．3\nすなわち，\\((X,Y)\\) を中心とした一辺 \\(2\\alpha\\) の正方形の中で，新たな位置をランダムに決めるのである．4\nこの動きによるエネルギーの変化量 \\(\\Delta E\\) を計算し，\\(\\Delta E&lt;0\\) ならばこれを実行するが，\\(\\Delta E&gt;0\\) ならば確率 \\(e^{-\\frac{\\Delta E}{kT}}\\) によって採択する．\n仮に棄却されたとしても，そのポジションから新たな Monte Carlo 標本を取り，最終的に \\[\n\\overline{F}=\\frac{1}{M}\\sum_{j=1}^MF_j\n\\] を推定量とする．\n\n\n3.3 アルゴリズムの有効性の検証\nまずこの系はエルゴードであると主張しているが，その論証は「任意の粒子が任意の位置に行くポテンシャルがあるため，この手法はエルゴード的である」で終わっている．エルゴードという単語を「任意の状態からもう一つの任意の状態に遷移可能である」という意味で使っている．これは現代的には既約性という．5\n続いて，この系をたくさんコピーしてアンサンブルを考えたとき，状態 \\(r\\) にいるアンサンブルの数 \\(\\nu_r\\) は \\[\n\\nu_r\\,\\propto\\,e^{-\\frac{E_r}{kT}}\n\\] を満たすことを示したい．その論証は，上の比率から崩れていたら，平衡に至る方向へ移動が起こるということを具体的に議論している．\n以上の２点から，提案されたアルゴリズムは正準分布に収束することの根拠としている．\nこのアンサンブルによる考え方は極めて直感的に訴える．実際，この語彙を用いて，棄却された場合は元々の状態を Monte Carlo サンプルとしてダブルカウントすべきであることを説明している．これをしなければ，低エネルギー状態のアンサンブルの数を不当に低く評価してしまう，という説明である．\nただし，このアンサンブルによる考え方は自然に我々の思考を詳細釣り合い条件に絞っている．\n\n\n3.4 附言\n収束の速さについて注意喚起しているのみで，ステップサイズ \\(\\alpha&gt;0\\) は大きすぎても棄却率が高まり，小さすぎても攪拌が遅くなるということ以外具体的なことは触れていない．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#実験",
    "href": "posts/2024/Review/Metropolis+1953.html#実験",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "4 実験",
    "text": "4 実験\n\nIn the case of two-dimensional rigid spheres, runs made with 56 particles and with 224 particles agreed within statistical error. For a computing time of a few hours with presently available electronic computers, it seems possible to obtain the pressure for a given volume and temperature to an accuracy of a few percent. In the case of two-dimensional rigid spheres our results are in agreement with the free volume approximation for \\(A/A_0&lt; 1.8\\) and with a five-term virial expansion for \\(A/A_0&gt; 2.5\\). There is no indication of a phase transition.\n\n16 step を焼き入れとし，48-64 いてレーションを実行するのに，MANIAC で 4-5時間かかったという．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#終わりに",
    "href": "posts/2024/Computation/MCMC.html#終わりに",
    "title": "新時代の MCMC を迎えるために",
    "section": "3 終わりに",
    "text": "3 終わりに\n\n本ポスター執筆のきっかけは，情報統計力学の研究集会に出席したことでした．\n統計界隈では PDMP や連続時間 MCMC と呼ばれる手法は，物理学界隈では lifting や event-based simulation と呼ばれて活発に研究されていました．\nまさに同じ問題を解こうとしているのに，用語法が全く異なるのです！\n２つの分野の相互理解と知見の交換が進むことを目指し，これからも研究していきたいと考えます．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#新たな-mcmc",
    "href": "posts/2024/Computation/MCMC.html#新たな-mcmc",
    "title": "新時代の MCMC を迎えるために",
    "section": "2 新たな MCMC",
    "text": "2 新たな MCMC\nこうして MCMC は物理学者から物質科学者，そして統計学者から機械学習家まで，多くの人が幅広く用いる手法になりました．\nその結果，多くの同一の手法が違う名前で呼ばれていることも多く，現状の最先端ではどのようなことが起こっているのか見極めるのが困難です．\nここでは，上述のすべての分野に渡って共通して起こりつつある大きな地殻変動を紹介します．キーワードは 連続時間 MCMC です．6\n\n2.1 連続時間 MCMC\n\n\nCode\nimport math\n\ndef zigzag(num_samples, initial_state):\n    samples = [initial_state]\n    current_state = initial_state\n    lifting_variable = 1\n    t = 0\n\n    while t &lt; num_samples:\n        state_event = lifting_variable * np.sqrt(-1 + np.sqrt( 1 - 4 * np.log(np.random.rand()) ))\n        t_event = t + np.abs(state_event - current_state)\n        for _ in range(math.ceil(t),math.ceil(t_event)):\n            samples.append(current_state + lifting_variable * (t_event - t))\n        current_state = state_event\n        lifting_variable = (-1) * lifting_variable\n        t = t_event\n\n    return np.array(samples)\n\n\n\n\nCode\nimport pints.plot\nimport matplotlib.pyplot as plt\n\nsamples_zigzag = zigzag(num_samples, initial_state)\nsamples_zigzag.shape = (len(samples_zigzag),1)\npints.plot.autocorrelation(samples_zigzag, parameter_names=['Samples'], max_lags=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure()\nplt.plot(samples_zigzag[0:50], range(50), color='green', linestyle='dashed', label='First 50 samples')\nplt.legend(loc=4, prop={'size': 10})\nplt.title('Zig-Zag sampler')\nplt.xlabel('Sample value')\nplt.ylabel('Sample index')\nplt.ylim(-0.5, 49.5)  # 軸の範囲を設定\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.2 連続時間 MCMC の美点\n\n\n2.3 筆者の目標：新時代のサンプラーの開発\n情報通信機器の発達によりデータが複雑で大規模化する現代では，モデルも同様に大規模で複雑化していく必要があります．OpenAI の ChatGPT や Sora，Anthropic の Claude-3 などの 基盤モデル はその象徴と言えるでしょう．\n筆者は，その中で 新時代の MCMC の開発を目標としています．\n高次元空間上の複雑な分布からも効率的にサンプリングできる MCMC 手法が開発された際には，多くの人が自分のノートパソコンで気軽にできるベイズ統計分析の幅が大きく広がることでしょう．\nそれこそ，ニューラルネットワークの表現力をフルに活用するだけでなく，ベイズ手法の強みも併せて，小規模データでも鮮やかな分析が簡単に出来るようになるかもしれません．\nそのような世界線こそ，AI 技術の民主化と呼ぶにふさわしい，来るべき未来だと筆者は信じています．\nまた，基盤モデルの Bayes 的な理解を進めることも，実は壮大ながらも，筆者の最終的な目標の一つであります．"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html",
    "href": "posts/2024/Review/Tartero-Krauth2023.html",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html",
    "href": "posts/2024/Review/Duane+1987.html",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "",
    "text": "$$\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html#概要と背景",
    "href": "posts/2024/Review/Duane+1987.html#概要と背景",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "1 概要と背景",
    "text": "1 概要と背景\n\n1.1 HMC とは\nhybrid Monte Carlo とは，MD と MC（＝Metropolis 法） の融合を指す．\nすなわち，粒子を動かして，これを棄却手続きによって正準集団を作る MCMC 法を指すが，粒子の動かし方＝提案核を運動論から構成するというのである．1\nそもそも著者は (Duane, 1985) において，場の量子論をシミュレーションするための “hybrid algorithm” を提案していた．これは第三の量子化と呼ばれる (Parisi & Wu, 1981) の確率過程量子化に基づく Langevin algorithm と 小正準法（量子系に対する MD 法）のいいとこ取りをする確率的アルゴリズムである．\nここからさらに Monte Carlo を導入し，「MD を提案分布にする」という発想の転換がある．正確には，任意の Hamilton 力学系を提案分布にとっても，Metropolis 法が使えるということを示唆したのである．この Hamilton 力学系を正確に取ると，これは hybrid algorithm になる（古典系に対しては MD に一致するだろう）が，必ずしも正確に取る必要はないのである．\n\n\n1.2 概要\n(Duane et al., 1987) は格子上の場の理論における数値シミュレーション法として提案している．格子ゲージ理論は量子色力学で扱われる模型である．\n\n大きなステップサイズを用いても離散化誤差がない\nフェルミオン自由度を含む量子色力学系のシミュレーションに有効\n\nである点が abstract で触れられている．\n\n\n1.3 導入\nフェルミオン自由度がある系では，“Grassmann nature of the fermions” を除去するためにまず積分をして有効作用のみを取り出し，残りのボゾンのみを考えるが，このときに非常に遠距離な（非局所的な）相互作用になってしまう．\n従来法には次の２つがある：\n\nexact / entire Monte Carlo：ボゾンの局所的なアップデートは系の全体の状態をシミュレーションしないとわからないから，nested Monte Carlo ともいうべきサブルーチンを回す必要がある．pseudofermion を導入して，有効作用の変化を効率的に計算し，これを用いて元々のボゾン場をアップデートする．要は棄却のステップが大変に高価ということだろうか？\n運動方程式の計算：MD に対応する方法である．小さいステップサイズで系全体を運動方程式に沿ってアップデートしていくことで，非局所的な有効作用というものは考えなくて済む．しかし，運動方程式の決定論的計算に伴う truncation error が導入される．\n\n後者の計算効率性と，前者の正確性を両取りすることを考える．"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html#footnotes",
    "href": "posts/2024/Review/Duane+1987.html#footnotes",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“a form of the Metropolis algorithm in which candidate states are found by means of dynamical simulation.” (Neal, 1996, p. 55)↩︎\n(鎌谷研吾, 2020) より．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html#footnotes",
    "href": "posts/2024/Review/Tartero-Krauth2023.html#footnotes",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこのとき，thermostat は Maxwell 分布に従うわけではない点に注意．熱浴内の粒子と違って，thermostat は \\(x=0\\) に固定されているため（例えば無視できる幅で振動しているとする），Maxwell boundary condition と呼ばれる分布に従い，比較的に簡単にサンプリングできる．↩︎\n元の状態空間を拡張した上で詳細釣り合い条件の破れを導入する手法を総称して lifting と呼ぶ (酒井佑士, 2017)．特に２値空間との席をとってリフティングをする手法は (Turitsyn et al., 2011) による方法で，(酒井佑士, 2017, pp. 24–24) で詳細な解析が与えられている．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html#本論",
    "href": "posts/2024/Review/Duane+1987.html#本論",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "2 本論",
    "text": "2 本論\nHMC は結局完全に Metropolis 法 (Metropolis et al., 1953) の枠内であり，詳細釣り合い条件を満たしにいくことを考える．ただし，この枠組みの中で最も優秀な方法を考える，というのである．\n採択関数は \\[\n\\alpha(x,y)=1\\land\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}\n\\] で与えられるから，2 \\(q(x,y)\\) の計算が速いだけでなく，\\(q(y,x)\\) も得られやすい理論的に都合の良い提案核 \\(Q\\) を探すことを考える．\n\n2.1 先行研究\nこの考えは molecular dynamics と Langevin algorithm をハイブリッドするアルゴリズム (Duane, 1985), (Duane & Kogut, 1986) に基づく．\n場の理論において，確率過程量子化 (stochastic quantization) (Parisi & Wu, 1981) に基づく Langevin 方程式の方法と，小正準集団の方法（MD に近い，QCD の熱力学のシミュレーションにも使われる）という２つの方法が，特に力学的フェルミオンを含んだ系の数値シミュレーションにおいて魅力的な代替理論になっている．\nこの２つのシミュレーションのいいとこ取りをする hybrid algorithm が (Duane, 1985) で提案されており，(Duane & Kogut, 1986) で理論的な解析が進められた．これは，確率 \\(p\\Delta\\) によって，Langevin 法を用いるか，小正準法を決めるというアルゴリズムである．これは系をある熱浴に接続するという物理的な解釈を持つ．加えて，このアルゴリズムの軌跡は，ある古典的な運動方程式に従った奇跡ともみなせる．\n\n\n2.2 アイデア\nParisi-Wu の確率過程量子化では，仮想的な時間 \\(t\\) を導入して，場の量 \\(\\phi_i(x)\\) が Langevin 方程式に従って発展するとする．こうして定まる確率過程が \\(t\\to\\infty\\) の極限で場の量子論を与えるというのである．\nこれに倣い，仮想的な時間パラメータ \\(\\tau\\) と Hamilton 力学系を導入する．ここで補助変数として，共役運動量 \\(\\pi(t)\\) が導入される．\nもし Hamiltonian \\(H\\) を正確に対象の系と同様に取れば，採択率は \\(1\\) になり，これが hybrid algorithm (Duane, 1985) に他ならない．しかし，ずらしても良いのである．\n\n\n2.3 検証\n詳細釣り合い条件を満たすことを示している．詳細釣り合い条件が満たされる主な理由は Hamilton 力学系が可逆であることによる．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#テンパリング遷移-neal1996tt",
    "href": "posts/Surveys/SMCSamplers.html#テンパリング遷移-neal1996tt",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．13\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "href": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "MCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ (岡本祐幸, 2010)．\nmultilevel sampling とも呼ばれる．4\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Jasra et al., 2007)．\n\n\n(Torrie & Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．5\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering6 または exchange Monte Carlo (Hukushima & Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，7 さらには population-based MCMC8 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita & Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や擬似テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．9\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．10\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．11 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang & Wong, 2000), (Liang & Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または擬似アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 擬似テンパリング (simulated tempering) (Marinari & Parisi, 1992) である．12\n擬似アニーリングでは温度は下がる一方であったのが，擬似テンパリングでは温度もある周辺分布に従って遷移する．擬似アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，擬似テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n擬似テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．13 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer & Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg & Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．14"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#テンパリング法の分類",
    "href": "posts/Surveys/SMCSamplers.html#テンパリング法の分類",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "しかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．9\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang & Wong, 2000), (Liang & Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または擬似アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 擬似テンパリング (simulated tempering) (Marinari & Parisi, 1992) である．10\n擬似アニーリングでは温度は下がる一方であったのが，擬似テンパリングでは温度もある周辺分布に従って遷移する．擬似アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，擬似テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n擬似テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．11 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer & Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg & Neuhaus, 1991) はスピングラスの問題などでも大きな成果を挙げた．12"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "href": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．15\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．16 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nテンパリング遷移の後半のアルゴリズムを発展させた形である．\n\n\n\nこちらは擬似テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#その他の手法",
    "href": "posts/Surveys/SMCSamplers.html#その他の手法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe & Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html#背景",
    "href": "posts/2024/Review/Tartero-Krauth2023.html#背景",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "1 背景",
    "text": "1 背景\nKrauth は (Bernard et al., 2009) において event-chain Monte Carlo アルゴリズムを提案した．\nその後この手法は一般の連続系に適用できる形に拡張され，連続スピン系などにも適用されている (酒井佑士, 2017, p. 24)．"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html#本論",
    "href": "posts/2024/Review/Tartero-Krauth2023.html#本論",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "2 本論",
    "text": "2 本論\n１粒子が次の one-dimensional anharmonic potential \\[\nU_{24}(x)=\\frac{x^2}{2}+\\frac{x^4}{4}\n\\] に従って運動する場合を考える．\n\n\n\nポテンシャル \\(U\\) のプロット\n\n\nこのポテンシャルに関する Boltzmann-Gibbs 分布 \\(\\pi_{24}\\) は次の通り：\n\n\n\nポテンシャル \\(U\\) が定める Botlzmann-Gibbs 分布のプロット\n\n\n\\[\nZ(\\beta)=\\int^\\infty_{-\\infty}\\pi_{24}(x)\\,dx=\\frac{e^{\\frac{\\beta}{8}}}{\\sqrt{2}}K_{1/4}\\left(\\frac{\\beta}{8}\\right)\n\\]\n１粒子非調和振動子は，孤立系としては決定論的な力学系であるが，熱浴に接続すると区分確定的な系になる (Davis, 1984)．この系からは，Newton 力学と熱浴との相互作用の MD モデリングによって，\\(\\pi_{24}\\) からサンプリングすることができる．\nこの系を「熱浴に接続する」とは，速度を交換出来る仕組みを導入すれば良い．例えば振動中心 \\(x=0\\) に半透性の弾性的な物体（thermostat）を固定し，振動子が \\(x=0\\) を通る度に確率 \\(\\frac{1}{2}\\) で弾性衝突して速度を交換する系などとして考えられる．この半透性で弾性的な物体は無視できる幅で振動しながら熱浴と接続されており，温度が一定に保たれているとする．1\nこの系を十分に放置すると，粒子の位置 \\(x\\) は Boltzmann-Gibbs 分布 \\(\\pi_{24}\\,\\propto\\,e^{-\\beta U_{24}}\\) に従う．\n\nこれを，粒子の位置 \\(x\\) を力学に基づいて追跡することで \\(\\pi_{24}\\) からサンプリングすることも考えられる．これを MD 法という．\n一方で Monte Carlo 法によりサンプリングすることが出来る．ここでは Gauss 分布の方が裾が重いので，これを提案分布とした棄却法によりサンプリングできる．積分を実行する場合も重点サンプリング法の考え方で実行できる．\n(Metropolis et al., 1953) などではこの提案分布と対象分布の距離が離れすぎていることが問題なのであった．そこで Markov 連鎖を用いるのである．Metropolis 法とは，正方形の範囲への一様ランダムウォークから，採択-棄却のフィルターを通じて Markov 核を構成する普遍的手続きだと言える．\n目標分布が因子分解可能であるとき，別のフィルター factorized Metropolis filter を通じても詳細釣り合い条件から Markov 核が構成できる．これは独立な乱数を生成して，全員可決したときに採択する，というよりスピーディーな棄却手続きが可能で consensus と呼ばれている．\n(Chen et al., 1999), (Diaconis et al., 2000) によって詳細釣り合い条件を破る方法 lifting が提案された．これは補助変数法により，同じ方向に進み続けるように設計された Markov 連鎖である．2\nlifting を通じて，アルゴリズムを連続時間ベースにできる．これは，いちいち細かいステップサイズで「提案」するのではなく，次に棄却される位置と時刻をサンプリングすれば良い，というのである．event-driven にすることで連続時間ベースのシミュレーションが可能になるのである．\nこの event-driven なバージョンでも，factorized filter と consensus を応用できる．\n\\(U\\) の評価が高価である場合，これよりも採択率が下がるような bounding potential \\(\\widehat{U}\\) を用いることができる．これをフィルターを狭めるという感覚で thining (Lewis & Shedler, 1979) という．棄却された場合に，本格的に \\(U\\) を評価する．"
  }
]