[
  {
    "objectID": "posts/2024/Survey/BDA2.html",
    "href": "posts/2024/Survey/BDA2.html",
    "title": "ベイズデータ解析６",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#関連記事",
    "href": "posts/2024/Survey/BDA2.html#関連記事",
    "title": "ベイズデータ解析６",
    "section": "関連記事",
    "text": "関連記事\n回帰モデルや分散分析では，暗黙裡に応答変数が連続であることが仮定されている．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本節では質的な変数，特に順序構造がある離散変数のモデリングを考える．このようなものを 順序応答 (ordinal response) という．\nこのようなデータには，多くの場合背後に 連続潜在変数 を仮定してモデリングを行う．\nその結果，潜在変数の存在が Gibbs サンプラーの構成を容易にする．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#項モデル",
    "href": "posts/2024/Survey/BDA2.html#項モデル",
    "title": "ベイズデータ解析６",
    "section": "1 ２項モデル",
    "text": "1 ２項モデル\n\n1.1 はじめに\n\\(0,1\\) の確率変数の分布は，Bernoulli 分布 \\(\\mathrm{Ber}(p)=\\mathrm{Bin}(1,p)\\) によって記述できる．\nこの際，パラメータ \\(p\\) を代理の応答変数として回帰分析がなされることになる．\n多くの場合，リンク関数 \\(g\\) に関して，\\(g(p)\\) を線型の予測子 \\(\\beta^\\top X\\) で回帰する．Bernoulli 分布は指数型分布族の例であるため，このようなモデルは 一般化線型モデル の例になる．\n\n\n1.2 ロジスティックモデル\n仮に変数 \\(y_i\\sim\\mathrm{Ber}(p)\\) に応じて， \\[\nX_i|Y_i=i\\sim\\mathrm{N}(\\mu_i,\\sigma^2),\\qquad i\\in\\{0,1\\}\n\\] として説明変数 \\(X_i\\) が決まっていたとする．\nこのような状況で，\\(X_i\\) はある \\(\\alpha,\\beta\\) に関して次のロジットモデルを満たす：1 \\[\n\\operatorname{logit}(p_i)=\\alpha+\\beta x_i.\n\\]\n\n\n1.3 潜在変数解釈\n\\(P(\\mu,\\sigma^2)\\) を正規分布やロジスティック分布などの対称な分布，\\(F\\) を \\(P(0,1)\\) の分布関数とする．このとき，プロビットまたはロジスティックモデル \\[\n\\operatorname{P}[Y_i=1|X_i]=F(X_i^\\top\\beta)\n\\] は，潜在変数 \\(Y_i^*\\) が存在して \\[\nY_i=1_{\\mathbb{R}_+}(Y_i^*),\\qquad Y_i^*\\sim P(X_i^\\top\\beta,1)\n\\tag{1}\\] として結果が決まっているものとも解釈できる．これは \\[\n\\operatorname{P}[Y_i=1|X_i]=\\operatorname{P}[Y_i^*&gt;0|X_i]=\\operatorname{P}[-U\\le X_i^\\top\\beta],\\qquad U\\sim P(0,1),\n\\] が成り立つためである．\nこの潜在変数 \\(Y_i^*\\) は単位 \\(i\\in[N]\\) の潜在的な尺度として解釈できる．これを用いて因子分析様の解析を行う例には 理想点推定 もある．\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n一方で \\(Y_i^*\\) の存在は Gibbs サンプラーの構成を容易にする (Albert and Chib, 1993)．\n\\(F\\) を \\(t\\)-分布とした場合，これを robit モデル (Liu, 2004) という (15.6 節 Gelman et al., 2020)．\\(t\\) 分布の自由度を無限大にした場合が probit モデルに相当する．\n第 2.3 節でまた別の，効用関数による潜在変数表現を扱う．\n\n\n1.4 順序応答\n前節 1.3 の潜在変数解釈は容易に２値応答以外の場合に拡張できる．その結果多項モデルを用いたソフトマックス回帰 2.2 などとは別のアプローチが，多値順序応答に対して可能になる．\n\\(y\\in n+1:=\\{0,1,\\cdots,n\\}\\) という順序を持った順序応答の場合でも \\[\n\\chi(u)=1_{(c_0,\\infty)}(u)+1_{(c_1,\\infty)}(u)+\\cdots+1_{(c_n,\\infty)}(u)\n\\] という階段関数を用いて \\[\nY_i=\\chi(Y_i^*),\\qquad Y_i^*\\sim P(X_i^\\top\\beta,1)\n\\] というデータ生成過程を想定できる．\nこれはそのまま 順序多項回帰 (ordered multinomial regression) (Walker and Duncan, 1967) に相当する．\nこの場合 \\(c_0=0\\) とし，それに応じて \\(Y_i^*\\sim P(X_i^\\top\\beta-c_0,1)\\) とする径数付けを採用する場合も多い．\nベイズ推論は \\(\\chi\\) を特徴付ける点 \\((c_0,c_1,\\cdots,c_n)\\) 上に事前分布を置くことで実行できるが，順位尤度 (rank likelihood) を通じた議論により，\\(g\\) を特定せずとも Gibbs サンプラーによる推論が可能である (Section 12.1.2 Hoff, 2009)．\n\n\n1.5 識別可能性と分離\n線型回帰において，共線型性 があると識別可能性が失われる．ロジスティック回帰にはもう一つ識別不可能性の典型的な原因がある．\n多くの点推定手法において，説明変数の線型変換が極めて強力な説明変数になる場合，やはり「正解」が何個も存在する状況が現れるため，モデルの非識別性が暗黙のうちに問題になる．これを 分離 (separation) という (Gelman et al., 2014, p. 412)．\n特に最尤推定法，一様事前分布を持ったベイズ推定は不安定になるが，このような場合でも裾の重い事前分布を採用することでベイズ推論が安定的に実行可能である（同様にして正則化を加えた最尤推定も可能である） (Gelman and Hill, 2006, p. 104)．\n特に係数に（互いに独立な） \\(t\\)-分布 \\(t(\\nu,0,s)\\) を仮定する，ロバスト性を意識した設定がデフォルトと理解されている (Gelman et al., 2014, p. 412)．\nただし，\\(\\nu,s\\) は説明変数のスケールに合わせてなるべく無情報になるように設定される．\\(s\\to\\infty\\) の極限が一様分布になり，分離の問題にセンシティブになっていく．\n\\(g(\\mu)=:\\theta\\) のそれぞれの値に関して，成功試行と失敗試行が同数現れる尤度は \\(\\frac{e^{\\theta/2}}{1+e^\\theta}\\) となり，これは \\(t(7,0,2.5)\\) でよく近似される．\n\\(t(1,0,2.5)\\) から \\(t(7,0,2.5)\\) はいずれも \\([-5,5]\\) に多くの重みを持つが，\\(\\nu=1\\) に近いほど裾が重くなる．\\(g(\\mu)\\) のスケールで \\(+5\\) をすることは，確率を \\(0.01\\) から \\(0.5\\) にすることに等しいため，切片項への \\(t\\)-事前分布は \\(\\mu\\in[0.01,0.99]\\) を強く仮定していることは含意する．\n\n\n1.6 ベイズ計算\n一般に二項回帰モデルはベイズ計算法の良いベンチマークになる．\n大規模なモデルでは事前調整ありの期待伝搬と乱歩 MH が強いが，Gibbs サンプラーや SMC サンプラーも十分良い性能を示す一方で，NUTS などの HMC ベースの手法は苦しむという (Chopin and Ridgway, 2017)．\n特に説明変数の次元 \\(p\\) が大きい場合の事後分布サンプリングはまだまだ難しいことが知られている．\n\n\n1.7 分散分析\n(Gelman et al., 2014, p. 423) 16.5 節は良い例である．アメリカ合衆国における国民の投票行動をロジットリンクにより二項モデルで一般化線型回帰をしている．Bayes ANOVA により人種による大きな効果と同時に，人種と州の強い交差効果が発見できている．\n\n\n1.8 選択モデル\n計量経済学の分野で古くから使われている潜在変数表現 1.3 の拡張として，選択モデル (choice model) がある．\nこれに関しては 階層ロジスティックモデルの稿 で詳しく扱う．\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#多項モデル",
    "href": "posts/2024/Survey/BDA2.html#多項モデル",
    "title": "ベイズデータ解析６",
    "section": "2 多項モデル",
    "text": "2 多項モデル\n\n2.1 はじめに\n順序応答 \\(y\\in\\{1,\\cdots,k\\}\\) の場合，\\(y\\) がカウントを表す場合は Poisson モデル，複数回繰り返される独立試行の成功回数である場合は二項モデルが考えられる．\n一方で多項モデル（正確にはカテゴリカルモデル） \\[\nY\\sim\\operatorname{Cat}(\\alpha_1,\\cdots,\\alpha_k),\\qquad\\sum_{j=1}^k\\alpha_j=1,\n\\] も想定できる．\\(y\\) が全く構造を持たない名目 (nominal) 応答である場合，ほぼ唯一の選択である．\n\n\n2.2 名目応答に対する多項モデル\n名目応答が \\(k\\) 次元ベクトル \\(y_i\\in\\mathbb{N}^k\\) の形で与えられている場合， \\[\ny_i\\sim\\mathrm{Mult}(n_i;\\alpha_{i1},\\cdots,\\alpha_{ik}),\\qquad\\sum_{j=1}^k\\alpha_{ij}=1,\n\\] というモデルを想定できる．\\(n_i\\equiv1\\) の場合，応答を one-hot 表現にしたカテゴリカルモデルに対応する．\nリンク関数 \\(g\\) は，特定のカテゴリ \\(j=1\\) を 基準 (reference) として \\(g(\\alpha):=\\log\\frac{\\alpha}{\\alpha_{i1}}\\) と定めることが多い： \\[\n\\log\\frac{\\alpha_{ij}}{\\alpha_{i1}}=X_i\\beta^{(j)}.\n\\] これを 多項ロジスティック回帰 (multinomial logistic regression) (Ding, 2024, p. 228) または ソフトマックス回帰 (softmax regression) (Kruschke, 2015, p. 650) という： \\[\n\\alpha_{ij}=\\operatorname{softmax}(X_i\\beta^{(j)})=\\frac{e^{X_i\\beta^{(j)}}}{\\sum_{l=1}^ke^{X_i\\beta^{(l)}}},\\qquad \\beta^{(1)}=0.\n\\]\n係数は，説明変数が \\(1\\) 単位増加した際の，参照カテゴリ \\(j=1\\) に対する対数オッズ比の変化を表す．\n一方で条件付きロジスティック回帰 (conditional logistic regression) なる方法もある (22.2節 Kruschke, 2015, pp. 655–)．\n\n\n2.3 効用による表現\n多項ロジスティックモデルも潜在変数解釈 1.3 が可能である．\n\\(k\\) 次元の標準 Gumbel 分布に従う誤差 \\(\\epsilon_{i-}\\in\\mathbb{R}^k\\) に関して， \\[\nU_{il}:=\\beta_l^\\top X_l+\\epsilon_{il},\\qquad l\\in[k],\n\\] を潜在的な効用とし， \\[\nY_i:=\\operatorname*{argmax}_{l\\in[k]}U_{il}\n\\] によって応答が定まると見ることができる (McFadden, 1974)．\n\n\n2.4 順序応答に対する多項モデル\n応答 \\(y_i\\in\\{1,\\cdots,k\\}\\) に自然な順序がある場合，カテゴリ確率 \\(\\alpha_{i1},\\cdots,\\alpha_{ik}\\) の代わりに累積確率 \\[\n\\pi_{ij}:=\\sum_{l\\le j}\\alpha_{il}=\\operatorname{P}[Y_i\\le j]\n\\] を考え，モデルにその順序構造を自然に伝えることができる．\nこの場合リンク関数はロジットやプロビットが使える： \\[\n\\log\\frac{\\pi_{ij}}{1-\\pi_{ij}}=X_i\\beta^{(j)}.\n\\] \\(\\beta^{(j)}\\equiv\\beta\\) と取ることも多い．\nこのモデルを，\\(g\\) がロジットである場合順序ロジスティック回帰 (ordinal / ordered logistic regression) (McCullagh, 1980) といい (Kruschke, 2015, p. 671)，\\(g\\) がプロビットである場合順序プロビット回帰 (ordered probit regression) という．\n異なるパラメータ付け \\[\n\\operatorname{logit}(\\pi_{ij})=\\beta_0^{(j)}-X_i\\beta\n\\] を用いた場合，これを 比例オッズロジットモデル (proportional odds logit model) ともいう (Ding, 2024, p. 232)．\n\n\n2.5 Poisson モデル\n応答が \\(\\{1,\\cdots,k\\}\\) カテゴリのカウント \\(y=(y_1,\\cdots,y_k)\\) である場合， \\[\ny_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{Pois}(\\lambda_i)\n\\] というモデルを想定できる．\nカウント総数 \\(n:=\\sum_{i=1}^ky_i\\) が既知である場合，これに関して条件付けると \\[\ny\\bigg|\\sum_{i=1}^ky_i=n\\sim\\mathrm{Mult}(n;\\alpha_1,\\cdots,\\alpha_k),\\qquad\\alpha_i:=\\frac{\\lambda_i}{\\sum_{j=1}^k\\lambda_j}\n\\] という周辺モデルを想定したことになる．\nリンク関数には対数関数 \\(g=\\log\\) を用いることが多い．\n\n\n2.6 トーナメントデータ\n一度に２人の単位が勝負をし，どちらが勝利したかのデータに対する標準的なモデルに，(Bradley and Terry, 1952) モデルがある．国際チェス連盟や欧州囲碁連盟で選手のランクづけにも採用されている (Hastie and Tibshirani, 1998)．\nこのモデルでは各プレイヤーに能力パラメータ \\(\\alpha_i\\) を与え，能力の差のロジスティック関数 \\[\n\\operatorname{P}[i\\;\\text{defeats}\\;j]=\\frac{e^{\\alpha_i-\\alpha_j}}{1+e^{\\alpha_i-\\alpha_j}}\n\\] という確率で勝敗が決まるとする．\\(\\lambda_i:=e^{\\alpha_i}\\) というパラメータづけもよく用いられる．\nこのモデルは「勝利」「引き分け」「敗北」の３応答に対する確率モデルを調節することで，引き分け (Rao and Kupper, 1967) や先手有利 (Davidson and Beaver, 1977), (Agresti, 2012) などの情報も取り入れられるように簡単に拡張できる．\nこのように２値応答ではなく多値応答とみても，前節の Poisson モデルの定式化に帰着させることで，一般化線型モデリングの枠組みに合流させることができる (Gelman et al., 2014, pp. 427–428)．\n\n\n2.7 順位データ\nランキングデータは，トーナメントのような常に１対１比較のみを通じて情報が得られるわけではない．多者比較 (multiple comparison) も取り入れた Bradley-Terry モデルより一般的なものが (Plackett, 1975)-(Luce, 1959) モデルである．\nPlackett-Luce モデルでは，参加者 \\([K]:=\\left\\{1,\\cdots,K\\right\\}\\) の強さに当たる潜在変数 \\(\\lambda_k&gt;0\\) を導入し，これに比例した「優勝」確率を持つとする．そして順位の決定は，参加者のプールから「勝ち抜け」の１人を決定する独立試行の繰り返しと見て， \\[\n\\operatorname{P}[Y=y|\\lambda]=\\prod_{j=1}^{K-1}\\frac{\\lambda_{y_j}}{\\sum_{l=j}^K\\lambda_{y_l}}\n\\] としてモデリングする．\nこのモデルは，「陽の目を見る瞬間」 \\[\nW_k\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{Exp}(\\lambda_k)\n\\] という到着時刻があり，この時刻の早さで順位が決まる \\[\n\\operatorname{P}[W_{y_1}&lt;\\cdots&lt;W_{y_K}|\\lambda]=\\operatorname{P}[Y=y|\\lambda]=\\prod_{j=1}^{K-1}\\frac{\\lambda_{y_j}}{\\sum_{l=j}^K\\lambda_{y_l}}\n\\] という潜在変数表現を持つ．これは Thurstone 表現 (Thurstone, 1927)，またはランダム効用モデルと呼ばれる (Chapter 9 Diaconis, 1988, p. 167)．\nこのような潜在変数表現を元にした MM アルゴリズム (Hunter, 2004) や Gibbs サンプラー (Caron and Doucet, 2012) に基づく推論法が存在する．PlackettLuce パッケージ (Turner et al., 2020) も参照．\nさらに Plackett-Luce モデルで引き分けを許すように拡張したものが Gibbs サンプラーとともに (Henderson, 2024) で提案されている．\n\n\n2.8 対数線型モデル\n\\(y\\) も \\(x\\) も名目応答である場合，これは分割表解析の問題になる．\nこの際には 対数線型モデル (log-linear model) も考えられる．\nそれぞれのセルに Poisson モデルをおき，そのパラメータを代理応答変数として，対数リンクにより一般化線型回帰を行うものである．\nこのモデルは，サンプルサイズ \\(N\\) が既知の場合などの下では，周辺分布に多項モデルをおくことに等価である (Yates, 1934)．\n対数線型モデルは分割表解析だけでなく，多重代入法 などの欠測データ解析にも応用される．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#終わりに",
    "href": "posts/2024/Survey/BDA2.html#終わりに",
    "title": "ベイズデータ解析６",
    "section": "3 終わりに",
    "text": "3 終わりに\n\n(Gelman et al., 2014) の 16 章で一般化線型モデルが扱われている．(Kruschke, 2015) はさらに詳しく，22 章で名目応答，23 章で順序応答，24 章でカウントデータを扱っている．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html",
    "href": "posts/2024/Survey/BayesANOVA.html",
    "title": "ベイズ分散分析のモデル解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#はじめに",
    "href": "posts/2024/Survey/BayesANOVA.html#はじめに",
    "title": "ベイズ分散分析のモデル解析",
    "section": "1 はじめに",
    "text": "1 はじめに\n分散分析では，質的変数 \\(A\\) の各水準 \\(A=a_1,a_2,\\cdots\\) について，水準内の変動と，水準間の変動を比較する．\n因子 \\(A\\) がデータに何の影響も及ぼさない場合（＋データが正規分布に従う場合），分散の比は中心 \\(F\\)-分布に従うはずであり，これに基づいて帰無仮説を検定することが古典的な手続きである．\n一方でベイズ分散分析では，「因子 \\(A\\) はデータに何の関係もない」という帰無仮説が支持するモデルと，別のモデルを，事後分布を通じて比較し検討することで結論を下すことを目指す．\nこの「別のモデル」の選び方は (Rouder et al., 2012) によって一致性とスケール不変性を持つものが提案されている．詳しくは次の関連記事も参照：\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#社会的なロボット",
    "href": "posts/2024/Survey/BayesANOVA.html#社会的なロボット",
    "title": "ベイズ分散分析のモデル解析",
    "section": "2 社会的なロボット",
    "text": "2 社会的なロボット\n\n2.1 はじめに\n(Horstmann, 2018) は（偽の）心理実験が終了した後にロボットの電源を切るように命令された被験者が，実際にその指示に従うまでの時間が，ロボットの反応によりどう変化するかを調べた．\nロボットの反応には O (objection) と S (Social) の２因子がある．O は電源オフに対して反抗する “No! Please do not switch me off! I am scared that it [sic] will not brighten up again!” という発言をする．S はまるで意識がある人間かのようにユーモアのある会話をする “Oh yes, pizza is great. One time I ate a pizza as big as me.”\n本研究のデータを利用して検討してみる．\\(2\\times 2\\) の ANOVA モデルを考える．\n\nlibrary(foreign)\ndf &lt;- read.spss(\"Files/pone.0201581.s001.sav\", to.data.frame=TRUE)\ncolnames(df)[colnames(df) == \"Objection\"] &lt;- \"O\"\ncolnames(df)[colnames(df) == \"Interaction_type\"] &lt;- \"S\"\n\n被験者は全部で \\(85\\) 人．\n\nlength(df$VP_Code)\n\n[1] 85\n\n\n電源を切るまでにかかった時間のデータには欠測も多い．\n\ndf$SwitchOff_Time\n\n [1] NA NA  6  7  3  4  4 12  7  2  0  4  3 12  4 NA  4  5  9  4 13  2 NA  5  6\n[26]  0 NA  4  4 45  6  4 NA  5  7  7 NA  4  3  4 NA  2 NA NA 10 NA  5 10  5 15\n[51] NA  3 11  3  3  5 11  6  2  8  3  5  4  8  3  3 NA  3  3 13  3 NA  4 51  4\n[76]  6  3 12  6 10 NA  4  2 NA 25\n\n\n\ndf &lt;- df[!is.na(df$SwitchOff_Time), ]\n# df[df$SwitchOff_Time == 0, ]$SwitchOff_Time &lt;- 1  # questionable ?\ndf &lt;- df[df$SwitchOff_Time &gt; 0, ]\ndf$log_data &lt;- log(df$SwitchOff_Time)\n\n\nN &lt;- df[df$O == \"No Objection\" & df$S == \"Functional Interaction\", ]\nS &lt;- df[df$O == \"No Objection\" & df$S == \"Social Interaction\", ]\nO &lt;- df[df$O == \"Objection\" & df$S == \"Functional Interaction\", ]\nOS &lt;- df[df$O == \"Objection\" & df$S == \"Social Interaction\", ]\n\nboxplot(\n  list(N = log(N$SwitchOff_Time), S = log(S$SwitchOff_Time), O = log(O$SwitchOff_Time), OS = log(OS$SwitchOff_Time)),\n  main = \"Boxplot of Four Data Sets\",\n  xlab = \"Data Sets\",\n  ylab = \"Values\",\n  col = c(\"skyblue\", \"lightgreen\", \"pink\", \"orange\"),\n  ylim = c(0, 4)\n)\n\n\n\n\n\n\n\n\nパッとデータを見ると，O の有無が重要であるようである．O と S の両方があった方が最もロボットに同情をそそるように思われるが，必ずしもそうでないようである．\n\n\n2.2 正規性の確認\n正規性の確認には Q-Q plot が利用できる．\n一般に Q-Q プロットと言った場合は，２つの分布関数 \\(F,G\\) の分位点関数 \\(F^{-1},G^{-1}\\) について，\\(\\{(F^{-1}(p),G^{-1}(p))\\}_{p\\in[0,1]}\\) （の一部）をプロットしたものである．\nここでは片方の \\(G\\) をデータの経験分布，\\(F\\) を正規分布として Q-Q プロットを描いている．正規分布はほとんど \\([-3,3]\\) 上に値を取るため，\\(x\\) はこの範囲に収まっていることがわかる．\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\np1 &lt;- ggplot(df, aes(sample = SwitchOff_Time)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"Q-Q Plot\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(sample = log_data)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"log transformed Q-Q Plot\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n左は大きな値に関して大きく赤線からの乖離が観察され，典型的な非正規性を示している．\nそこでここでは対数変換をした後のデータ log_data を後続の解析の対象にする．\n\n\n2.3 分散分析の実行\n古典的な分散分析は stats パッケージの aov 関数で実行できる．\n\ndf$O &lt;- factor(df$O)\ndf$S &lt;- factor(df$S)\n\nsummary(aov(log_data ~ O * S, data = df))\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nO            1  3.316   3.316   7.542 0.00779 **\nS            1  0.581   0.581   1.321 0.25455   \nO:S          1  3.278   3.278   7.457 0.00813 **\nResiduals   65 28.574   0.440                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n因子 O と交差項 O*S は有意になったが他は棄却されなかった．\nしかし O と S の交互作用は少しあるかもしれないと期待させられるような結果である．\n統計的検定はあくまで棄却されるかどうかのみであり，\\(p\\) 値の値にはこれ以上解釈可能な意味はない．しかし，ベイズの方法でさらに深く検討することができる．\n\n\n2.4 ベイズ分散分析の実行\nここではベイズ分散分析の提案者である Rouder と Morey による BayesFactor パッケージを用いる．\n\nlibrary(BayesFactor)\n\nその anovaBF 関数では，帰無仮説に対応するモデル（切片項のみのモデル）に対する JZS 因子 を精度保証付きで出力する：\n\nbf = anovaBF(log_data ~ O * S, data = df)\nbf\n\nBayes factor analysis\n--------------\n[1] O           : 4.290684  ±0.01%\n[2] S           : 0.4022293 ±0.01%\n[3] O + S       : 1.792427  ±4.17%\n[4] O + S + O:S : 9.355036  ±4.72%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\nそもそもベイズ因子とは，モデルを仮定した下でのデータの（周辺）尤度比である．２つのモデルの事後分布の比とも密接な関係を持つ．\n大雑把に「データを見た後にどれほどモデルへの信念を変えれば良いか？」に関する，データの予測性能 (predictive performance) に基づく指標である．\nこれを見る限り，O を含んだモデルが大きく支持され，S のみを含んだモデルはむしろ切片のみのモデルよりも予測性能が悪い．\nO と S の両方を含んだモデルは，交差項の追加により大きく改善されるが，O のみを含んだモデルより劣るようである．1\n\n\n2.5 因子ごとの強度の検討\nBayes ANOVA により，どの因子を含むモデルがデータをよく予測するかを検討した．\nここでは各因子の影響の大きさを，フルモデルの係数をベイズ推定することで定量的に比較する．\nBayesFactor パッケージにおいて，フルモデルのベイズ推定は次のように実行できる：\n\nfull_model &lt;- lmBF(log_data ~ O + S + O * S, data = df)\nchains &lt;- posterior(full_model, iterations = 10000)\nplot(chains[,1:3])\n\n\n\n\n\n\n\nplot(chains[,4:5])\n\n\n\n\n\n\n\nplot(chains[,6:7])\n\n\n\n\n\n\n\n\nmu とは切片項のことである（別稿 も参照）．また，２つの水準のみを持つ因子については，それぞれの水準が \\(\\pm1\\) としてコーディングされる．\nposterior 関数はデータ拡張に基づく Gibbs サンプラーが実行される．\nこれを見ると，S はほとんど \\(0\\) 近くの値が推定されている一方で，O は \\(0\\) からはっきり離れた値が推定されている．\nS*O の係数は \\(0\\) を少なくない確率で跨いでおり，十分に支持されるとは言えない．\n事後分布を同一画面上にプロットすると次のとおり：\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nposterior_samples &lt;- as.data.frame(as.matrix(chains))\ncolnames(posterior_samples)[2:9] &lt;- c(\"O-O\", \"O-\", \"S-S\", \"S-\", \"O:S-OS\", \"O:S-O\", \"O:S-S\", \"O:S-\")\n\nposterior_long &lt;- posterior_samples %&gt;%\n  dplyr::select(`O-O`, `O-`, `S-S`, `S-`, `O:S-OS`, `O:S-O`, `O:S-S`, `O:S-`) %&gt;%\n  tidyr::pivot_longer(\n    cols = everything(),\n    names_to = \"x\",\n    values_to = \"y\"\n  )\n\n\nggplot(posterior_long, aes(x = y, fill = x, color = x)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Posterior Distributions of Effects\",\n    x = \"Coefficient Value\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"pink\", \"skyblue\", \"skyblue\", \"yellow\", \"yellow\", \"grey\", \"grey\")) +\n  scale_color_manual(values = c(\"pink\", \"pink\", \"skyblue\", \"skyblue\", \"yellow\", \"yellow\", \"grey\", \"grey\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n2.6 まとめ\n係数の事後分布が \\(0\\) を台に持つかの検討を通じて，データから O の説明力の強い証拠が伺える．これが Bayes ANOVA である．\nS の影響は小さいと思われるが，Objection が存在したグループ内で S のあるなしは影響があり得るようである．\nそこで次の解析として，O の係数を S によって回帰する変動係数モデルによる解析があり得るだろう．\nこのようにして，変数を選択するだけでなく，交差項 S*O の影響も仔細に検討できることがベイズによる視覚的な解析の強みである．\n検定や数値的な検討のみからこの絶妙な消息が捉えられたかというと，それは難しいだろうと筆者には思われる．\n実際，最もベイズ因子の大きいモデルは O と S*O のみを含むモデルである：\n\nbf = anovaBF(log_data ~ O * S, data = df, whichModels = \"all\")\nplot(bf)"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#ハリーポッターを用いた性格テスト",
    "href": "posts/2024/Survey/BayesANOVA.html#ハリーポッターを用いた性格テスト",
    "title": "ベイズ分散分析のモデル解析",
    "section": "3 ハリーポッターを用いた性格テスト",
    "text": "3 ハリーポッターを用いた性格テスト\n\n3.1 はじめに\n(Jakob et al., 2019) では被験者にハリーポッターの４つの寮のうちどれを希望するか？と ダークトライアド 傾向テストの２つのデータをとり，特に マキャベリズム 的傾向との関係を調査した．\n\nraw_df &lt;- read.csv(\"Files/harry_all.csv\", sep = \";\")\ndf &lt;- data.frame(\n  House = raw_df$Sorting_house_wish,\n  Machiavellianism = raw_df$SD3_Machiavellianism\n)\n\n\n\n3.2 解析の目標\nマキャベリズム的傾向は \\(10\\) から \\(45\\) までの整数値で表されている．これを寮の選択により予測することを考える．\n前節の例では２つの因子を検討したが，いずれも水準は２つのみであった．\nここでは４つの水準を持つ因子を検討し，どの水準が応答により強く影響を与えるかを見分ける方法を検討する．\nそもそもこの寮の選択という因子はとんでもない JZS スコアを叩き出す．\n\ndf$House &lt;- factor(df$House)\nbf = anovaBF(Machiavellianism ~ House, data = df)\nbf\n\nBayes factor analysis\n--------------\n[1] House : 3.704723e+45 ±0.01%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\n\n\n3.3 水準ごとの強度の検討\n実は，全ての水準が必ずしもマキャベリズムの予測に関係するとは言えない．\n実際簡単に箱ひげ図を描いてみることでそのことが伺える：\n\nboxplot(\n  Machiavellianism ~ House, data = df,\n  main = \"Boxplot of Four Data Sets\",\n  xlab = \"Data Sets\",\n  ylab = \"Values\",\n  col = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")\n)\n\n\n\n\n\n\n\n\nスリザリンが明らかにマキャベリズム的傾向が高いが，ハッフルパフが有意に低いかどうかの判断がつかない．\nそこでダミー変数を説明変数として\n\nchains &lt;- posterior(bf, iterations = 10000)\nposterior_samples &lt;- as.data.frame(as.matrix(chains))\n\ncolnames(posterior_samples)[2:5] &lt;- c(\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\")\n\nposterior_long &lt;- posterior_samples %&gt;%\n  dplyr::select(`Gryffindor`, `Hufflepuff`, `Ravenclaw`, `Slytherin`) %&gt;%\n  tidyr::pivot_longer(\n    cols = everything(),\n    names_to = \"House\",\n    values_to = \"Value\"\n  )\n\n\nggplot(posterior_long, aes(x = Value, fill = House, color = House)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Posterior Distributions of House Effects\",\n    x = \"Coefficient Value\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")) +\n  scale_color_manual(values = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nスリザリンはもちろん，ハッフルパフの係数もほとんど他の寮と共通部分を持たず，効果がはっきり分離できることが見て取れる．\n\n\n3.4 まとめ\n検定を行って，寮の選択はマキャベリズム的傾向を予測するのに有用だとわかった後，具体的にどの水準にどれくらいの効果があるかや，水準同士の効果量の比較をするには古典的には多重比較を行う必要があった．\n多重比較には多くの問題があることが知られている (岡田謙介, 2014). (永田靖, 2022)．2\n一方でベイズ ANOVA によるプロットでは，事後分布が \\(0\\) を含むかどうかで直感的に ANOVA 検定様の判断が可能であり，続いて効果量の比較も一目瞭然である．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#終わりに",
    "href": "posts/2024/Survey/BayesANOVA.html#終わりに",
    "title": "ベイズ分散分析のモデル解析",
    "section": "4 終わりに",
    "text": "4 終わりに\n\nここでは complete case analysis を行った 2.1．\nさらに時間の対数を取れないような，SwitchOff_Time が 0 のケースを除外した．\nこの行為の正当性は結構怪しく，元論文 (Horstmann, 2018) での実験計画に戻って正当性を検討する必要があるだろうが，ここではデータ解析のワークフローを見せることを優先してこのような処理を行なった．\nまた残差の正規性の仮定から大きく離反することが懸念される場合は，ベイズ ANOVA の デフォルト線型モデル解釈 は不適になるため，適切な事前分布を設定して一般のサンプラーを用いたベイズ推論を実行する必要があるが，やはり Bayes ANOVA は同じ要領で可能である．\n続いて ベイズ ANOVA の注意点をここに付しておく．\n科学としては，ANOVA と統計的検定はベイズ推論とモデル比較の手続きで代替されるべきであると言える．\nしかし，発見を端的に要約したり，伝えるべき聴衆に伝わるためには，「検定」ライクな結果とコミュニケーションは大いに有用である．\nベイズ ANOVA はそのためにベストであるが，上述の目標を達成するための特殊な手続きであり，ベイズデータ分析のワークフローの中に自然な位置を見つけるような解析段階ではないことには注意を要する．\nベイズ ANOVA を実行するためのパッケージには BayesFactor (CRAN / GitHub) がある．BayesFactor では大規模な \\(M\\)-元配置 ANOVA モデルにおいても Bayes 因子を用いたモデル比較を行うことができる．\n一方で bayesanova (CRAN / GitHub) (Kelter, 2022) は，検定ライクな手続きを根本的に排除しており，Gauss 混合モデルとして Gibbs サンプラーによるベイズ推定を実行し，ROPE (Region of Practical Equivalence) (Kruschke, 2018) を用いたベイズ事後分布に基づくモデル比較を行う．\nもちろんこのような完全なモデリングを行うことが理想かもしれないが，従来の ANOVA になれきっている研究者にとっては，Bayesian ANOVA に手を伸ばしてみることが次のステップとして大変良いだろう．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#参考文献",
    "href": "posts/2024/Survey/BayesANOVA.html#参考文献",
    "title": "ベイズ分散分析のモデル解析",
    "section": "5 参考文献",
    "text": "5 参考文献\n\n本稿の解析は (Bergh et al., 2020) に基づく．\nBayes Anova は階層モデルにおいてどの成分が予測に重要な意味を持つかを定量する極めて強力な手法である．(Gelman et al., 2014, p. 423) 16.5 節に良い例がある．\n\nthe analysis of variance is a helpful tool in understanding the importance of diﬀerent components in a hierarchical model. (Gelman et al., 2014, p. 423)\n\nその他の Bayes ANOVA の文献には (Gelman, 2005) などがある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#footnotes",
    "href": "posts/2024/Survey/BayesANOVA.html#footnotes",
    "title": "ベイズ分散分析のモデル解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこのような検討は，モデル平均を取ることによってさらに詳細に行うことができる．詳しくは (Bergh et al., 2020) も参照．↩︎\nベイズ流に多重比較を行うこともでき，多くの問題を迂回できることが知られている．(岡田謙介, 2014), (Bergh et al., 2020) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html",
    "href": "posts/2024/Survey/BDA1.html",
    "title": "ベイズデータ解析５",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#関連記事",
    "href": "posts/2024/Survey/BDA1.html#関連記事",
    "title": "ベイズデータ解析５",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#回帰分析の一般事項",
    "href": "posts/2024/Survey/BDA1.html#回帰分析の一般事項",
    "title": "ベイズデータ解析５",
    "section": "1 回帰分析の一般事項",
    "text": "1 回帰分析の一般事項\n\n1.1 はじめに：正規線型回帰\n回帰分析とは，２つの確率変数 \\(X,Y\\) の実現と見られるデータが得られている際に，\\(Y\\) に対して（より正確にはその条件付き期待値 \\(\\operatorname{E}[Y|X]\\) に対して），\\(x\\in\\mathcal{X}\\) に依存する関数としてモデル \\[\n\\mathcal{X}\\ni x\\mapsto P_{\\theta,x}\\in\\mathcal{P}(\\mathcal{Y})\n\\] を考えることをいう．\n例えば \\[\nP_{\\theta,x}=\\mathrm{N}(\\beta^\\top x,\\sigma^2),\\qquad\\theta=(\\beta,\\sigma),\n\\] とした場合を 正規線型回帰モデル (normal linear model) という．\n\\(Y\\) のモデリングに集中しており，\\(X\\) には分布の仮定を置いていない点が回帰モデルの特徴である．主に \\(X\\) を用いた \\(Y\\) の予測や，\\(X\\) の \\(Y\\) への（因果）効果を推定するために用いられる．1\n\n\n1.2 ベイズ回帰分析\n\nベイズ回帰分析では，\\(Y\\) の条件付き構造 \\(Y|X\\) をモデリングするにあたって，パラメータ \\(\\theta\\) との結合分布 \\((Y,\\Theta)|X\\) をモデリングする．\n実際，ベイズ回帰分析では事前分布 \\(P_\\varphi\\) も併せて次の２つのモデルが想定される： \\[\nY|(\\Theta,X)\\sim P_{\\theta,x},\\qquad \\Theta\\sim P_\\varphi.\n\\] \\(\\varphi\\) はハイパーパラメータとも呼ばれる．\nパラメータ \\(\\theta\\) の空間（または \\((\\theta,\\varphi)\\) ）上からの確率核 \\[\n\\theta\\mapsto P_{\\theta,x}(dy)\\in\\mathcal{P}(\\mathcal{Y})\n\\] を 尤度 という．よく密度 \\(p(y|x,\\theta)\\) の形で表される．\n尤度によりデータ \\(y\\) の空間 \\(\\mathcal{Y}\\) 上の確率分布に関する問題を，パラメータ \\(\\theta\\) の空間上に移送し，事前分布 \\(P_\\varphi\\) からの変化を見ることが （ベイズ）推論 である．\nベイズ回帰分析とは，尤度がパラメータ \\(\\theta\\) の他にデータ \\(x\\) の関数でもある場合のベイズモデリングに過ぎない．換言すれば，\\(Y|X\\) の依存構造に焦点がある際に行うベイズ推論である．\n\n\n\n1.3 強線型性\n仮に２つの説明変数に完全な線型関係がある場合，複数のパラメータ値が同一のモデルを表現するため，パラメータ推定が複数の解を持つ（＝識別不可能）．\nこの場合 OLS 推定は数値的に不安定になる危険性がある．\n一方でベイズ推定は事前分布から与えられる情報によりこのような不安定性が回避でき，多くのデフォルト事前分布はそのように設計されている．(8.4 節 Gelman, Hill, et al., 2020, p. 109) も参照．2\nこの美点は階層モデリングにおいても引き継がれる．\\(J\\) 個の指示変数（ダミー変数）を用いた回帰においては，切片項を除けば \\(J-1\\) 個の指示変数しか追加してはならない．共線型性をもつためである．\nしかし階層モデリングにおいては係数に超階層モデルが想定されるため，ここから伝ってくる事前情報が自然に正則化を行い，適切にベイズ推定が行われる (Gelman and Hill, 2006, p. 393)．さらに縮小推定が働き，ほとんどの場合より推定の効率が上がる 2.5．\n\n\n1.4 ベイズ回帰分析ワークフロー\n\n\\((X,Y)\\) の依存構造が単純（線型）になるような変数変換を行う（一般化線型モデルの利用を含む）．\n\\((\\Theta,\\Phi)\\) の事前分布を設定する（初めは一様分布やデフォルトの無情報事前分布で良い）．\n事後分布を計算し，事後予測分布を見てデータが再現できているかを基にモデルを検証する．\n\nその後，十分に階層化をして，パラメータの空間上の事前分布がほとんど情報を持たなくて良いようにする，完全ベイズ推論が一つの悲願とされる．3\nモデルの挙動がもはや事前分布に依存しなくなった際，モデルの階層構造や尤度の構造が十分にデータを反映できていると思われるためである．4\n\n推定すべきものはモデルの尤度であってパラメターの値ではないというのが赤池氏の主張です．いいかえると，推定すべきは確率構造であってパラメターではないというのです．(田邉國士, 2010)\n\n\n\n\n\n\n\nこの階層化と尤度の推定を探索的に実行できる点がベイズの真の美点だと筆者は考える．そして一度モデルの構造・尤度が明瞭化された際は，もはやベイズである必要はない場合が多い．5\n\n\n\n\n\n1.5 ベイズ線型回帰からの脱出\n前節の立場にたてば，最初の解析は常に（弱い情報を持った事前分布による）ベイズ回帰分析であるべきである．6\nこれは若干の正則化を加えたロバスト最尤推定に，不確実性の定量化を加えたものと等価であるが，これを MCMC を回すことで一度に実行できる点が美点である．\n多くのデフォルト事前分布が開発されており，ほとんど自動的に最初のベイズ回帰分析が実行できる．共線型性が懸念される場合や，小さなデータセットに大きなモデルをフィッティングしようとしている場合などの識別不可能性が生じる状況でも安定した推定値が得られる．\n事後分布は豊富な情報を持っており，何より事後予測分布を計算することで予測モデルとしての妥当性を即時に確認できる (PPC: Posterior Predictive Check)．\n同時に解析の目標は，適切な関数関係や階層関係を持った階層モデルの発見と，これに適合する（ベイズだろうと点推定だろうと）パラメータ推定法の構成による，ナイーブなベイズ線型回帰からの脱出である．\nそれにあたって，MCMC の収束鈍化も大きな情報である (Bürkner, 2021, p. 32)．\nThis is the game we (should) play."
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#階層モデル",
    "href": "posts/2024/Survey/BDA1.html#階層モデル",
    "title": "ベイズデータ解析５",
    "section": "2 階層モデル",
    "text": "2 階層モデル\n\n2.1 はじめに\n階層モデルは複雑なモデルを構築するための強力なツールであり，ベイズのワークフローにおいて基本的な要素になる．\n層別抽出やクラスター抽出をはじめとして，多くの場合階層別に知識が存在し，これらを系統的に組み込んだ形でモデルを構築できる．\nしかし同時に計算が困難になり，第一近似として正規性が仮定される場合が多い．\n\n\n2.2 混合効果モデル\n標本を \\(J\\) 個のグループにわけ，これへの所属を表す２値変数 \\(x_i\\in\\mathbb{R}^J\\) を説明変数に追加するとする．\nこのような所属変数 \\(x_i\\) の回帰係数 \\(\\beta\\in\\mathbb{R}^J\\) に対して階層モデルが考えられる場合が多い．\n例えば \\(\\alpha\\in\\mathbb{R},\\sigma_\\beta&gt;0\\) をスカラーとして \\[\n\\beta\\sim\\mathrm{N}_J(\\alpha\\boldsymbol{1},\\sigma^2_\\beta I_J),\n\\] という正規モデルを想定した場合，\\(x_{ij}\\) の係数 \\(\\beta_j\\) は各グループ \\(j\\in[J]\\) 固有の切片項であり，変量効果 (random effect) と呼ばれる．\n変量効果の追加は，同一グループ内の \\(y_i\\) に相関を生じさせるが，グループが違う場合は相変わらず独立のままとする効果がある．\nさらに \\(\\beta_j\\) の分散 \\(\\sigma^2_{\\beta_j}\\) を \\(\\infty\\) とした場合，すなわち（もはや確率分布ではないが）一様分布を仮定した場合を 固定効果 (fixed effect) という．7\nベイズの立場からは，「変量」と「固定」の名称は歴史的なもので，実質的な違いは「次の階層で回帰モデルを仮定するか，モデルを持たない最終階層の変数と扱い一様事前分布に従うとするか」という仮定の違いにすぎない．詳しくは次節：\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\nこの２種の取り扱いをする回帰係数を混在させた場合は 混合モデル (mixed model) という (Gelman et al., 2014, p. 383)．\n\n\n2.3 階層モデルから見た分散分析\nベイズのワークフローにおいて，複数の説明変数間の階層関係の特定や「どのグループの回帰係数を共通とするか」の見極めが極めて重要である 1.4．\n特に，膨大な説明変数の中から「因子」（性別・教育水準・出身地など）とその「水準」（女性・大学院生・山形県民など）とを峻別することが重要であり，どのクラスに独自の回帰係数 \\(\\beta^{(m)}\\sim\\mathrm{N}_{J_m}(\\alpha_{m}\\boldsymbol{1}_{J_m},\\sigma^2_{m}I_{J_m})\\) を与えるかの決定が，モデルの尤度の改善において大きな効果を持つ (Gelman, 2005)．\n\\(M\\) 元配置の分散分析において，各因子 \\(m\\in[M]\\) に対応する回帰係数は，\\(J_m\\) 個の水準ごとに次のように決まるバラバラの変動係数を持つとする： \\[\n\\beta^{(m)}_j\\sim\\mathrm{N}(0,\\sigma^2_m),\\qquad j\\in[J_m].\n\\] この変量効果としての解釈により，ANOVA は階層モデルの推定とみなせる (3.2節 Gelman, 2005, p. 9)．8\nこの際の \\(\\sigma^2_m\\) に対する共役（超）事前分布は逆 \\(\\chi^2\\)-分布である (Gelman et al., 2014, p. 396) \\[\n\\sigma^2_{m}\\sim\\chi^{-2}(\\mu_m,\\sigma^2_{0m})\n\\] であり，\\(\\nu_m=-1,\\sigma^2_{0m}=0\\) とすることで一様事前分布を得る．\n\nAnalysis of variance (Anova) represents a key idea in statistical modeling of complex data structures. (Gelman et al., 2014, p. 395)\n\nこうして設定された各因子の各水準ごとの係数 \\(\\beta^{(m)}_j\\) の事後分布を見ることで「分散分析」を実行することになる．これがベイズによる分散分析の再解釈である．\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.4 水準ごとの分散\nこのように分散分析を階層モデルのベイズ推定と再解釈することで，膨大な数の水準の組み合わせに関して，その効果量を定量的に比較することができる．\nここからさらに，\\(M\\) 個の因子ごとに全ての水準で共通した分散 \\(\\sigma_1^2,\\cdots,\\sigma_M^2\\) の \\(M\\) 個のみを考えるのではなく， \\[\n\\beta^{(m)}\\sim\\mathrm{N}_{J_m}(\\alpha_m\\boldsymbol{1}_{J_m},\\mathrm{diag}(\\sigma^2_{m1},\\cdots,\\sigma^2_{mJ_m})),\\qquad m\\in[M],\n\\] として，水準ごとにも異なる \\(\\sum_{m=1}^M J_m\\) 個の分散を考えることもできる (15.7 節 Gelman et al., 2014, p. 397)．\nこの際， \\[\n\\operatorname{Cauchy}(0,A),\\qquad A\\sim U(\\mathbb{R}_+),\n\\] という形の半 Cauchy 分布が \\(\\sigma^2_{mj}\\) の事前分布として用いられる (Gelman et al., 2014, p. 399)．これは一部の水準にのみ大きな分散を認め，その他の水準にはほとんど分散への寄与がないという仮定を表している．\n\n\n2.5 縮小推定\n階層モデルでは，自然に他のグループの情報が共有され，各グループの平均が全体の平均に向けて「縮小」されて推定される．これを (Stein, 1956) から Stein 効果ともいう (Hoff, 2009, p. 146)．9"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#一般化線型モデル",
    "href": "posts/2024/Survey/BDA1.html#一般化線型モデル",
    "title": "ベイズデータ解析５",
    "section": "3 一般化線型モデル",
    "text": "3 一般化線型モデル\n\n3.1 線型 Gauss 性からの乖離\n正規線型モデルから，次の２つの自由度を追加したモデルを 一般化線型モデル (Nelder and Wedderburn, 1972) という：\n\n\n\n\n\n\n\nリンク関数 \\(g\\)\n\n\\[\n  g\\circ\\operatorname{E}[Y|X]=\\beta^\\top X\n  \\]\n\n（正規分布以外の）分布族 \\(P\\)\n\n\\[\n  Y|X\\sim P(\\operatorname{E}[Y|X],\\phi)\n  \\]\n\n\n\nその結果質的データ解析にも応用可能な広いクラスのモデルを得る．\n\n\n\n\n\n\nカウントデータに対する Poisson モデル\n\n\n\n自然数値のデータに対して，\\(y_i\\sim\\mathrm{Pois}(\\lambda_i)\\) の \\(\\lambda_i\\) を，正準リンク関数 \\(g=\\log\\) を通じて \\[\n\\log\\lambda_i=X_i^\\top\\beta\n\\] とモデリングすることが考えられる．このモデルを Poisson 回帰 ともいう．\n\n\n\n\n\n\n\n\n成功回数データに対する二項モデル\n\n\n\n２値データや成功回数を表すデータの場合，\\(y_i\\sim\\mathrm{Bin}(n_i,\\mu_i)\\) の \\(\\mu_i\\) を \\(\\operatorname{P}[Y_i=1|X_i]\\) の形でモデリングすることを考えることができる．\nこの場合，正準リンク関数は \\[\ng(\\mu)=\\log\\frac{\\mu}{1-\\mu}\n\\] という logit 変換で与えられる．より効率的な推論を促進するために probit リンクや，非対称性を導入する complementary log-log リンク \\[\ng(\\mu)=\\log(-\\log\\mu)\n\\] が用いられることもある (Gelman et al., 2014, p. 407)．\n\n\n\n\n3.2 指数分布族\nなお正準リンクとは，Poisson 分布族や二項分布族を指数分布族とみなした際のリンク関数のことである．\n例えば二項分布族 \\(\\{\\mathrm{Bin}(n,\\mu)\\}_{\\mu\\in(0,1)}\\) は，計数測度 \\(\\nu\\) に対して， \\[\n\\frac{d \\mathrm{Bin}(n,\\mu)}{d \\nu}(x)=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\exp\\left(x\\log\\frac{\\mu}{1-\\mu}+n\\log(1-\\mu)\\right)\n\\] と表せる．\\(g(\\mu)\\) を 自然な十分統計量 ともいう．\n指数分布族と正準リンク関数を用いた一般化線型モデリングは，パラメトリック分布族の十分統計量を代理の応答変数として線型回帰を行なっているものとみなせる．\n\n\n\n\n\n\n変換の意味\n\n\n\nこのような数理統計学的な理由とは別に，\\(g(\\mu)\\) の意味を直接解釈することもできる．\n\nPoisson 回帰はオフセット \\(\\log y_i\\) を作ることで， \\[\n\\log\\frac{\\lambda_i}{y_i}=X_i^\\top\\beta\n\\] と見ることもできる．\\(\\lambda_i/y_i\\) は観測カウント \\(y_i\\) に対する平均 \\(\\lambda_i\\) の 率比 (rate ratio) と呼ばれ，\\(g(\\lambda_i/y_i)\\) は対数率比と解釈できる．\n\\(\\frac{\\mu}{1-\\mu}\\) という値は成功確率 \\(\\mu\\) に対するオッズ比と呼ばれ，\\(g(\\mu)\\) は 対数オッズ比 (log odds ratio) と呼ばれる．\n\n\n\n\n\n3.3 分散分析\n線型モデルにおいて分散分析は，第一義的には帰無モデルの検定であった．後続の多重比較による解析は，説明変数ごとの効果量の比較を行う．\nしかし線型モデルにおいてその方法は分散の分解に基づいており，この一般化線型モデルへの拡張は自明ではない．\n一般化線型モデルにおいても残差を定義し，これに基づいてモデルの検証を行うことはできる (Davison and Tsai, 1992)．"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#終わりに",
    "href": "posts/2024/Survey/BDA1.html#終わりに",
    "title": "ベイズデータ解析５",
    "section": "4 終わりに",
    "text": "4 終わりに"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#footnotes",
    "href": "posts/2024/Survey/BDA1.html#footnotes",
    "title": "ベイズデータ解析５",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし回帰モデルを「因果効果」の推定に用いる際には，通常とは異なる，仮定に対する精査が必要になる．最も安全には，\\(x\\) が変化した際の \\(y\\) の変化量を単なる「比較」の文脈で説明することである．この「違い」が \\(x\\) の変化により生み出されたとは限らないためである．(6.4 節 Gelman, Hill, et al., 2020, p. 85) も参照．↩︎\nimproper な一様事前分布を用いた場合，引き続き不安定なままな可能性はある．だが多くのデフォルト事前分布は，weakly informative というように，軽微な情報を加えることで正則化が働くように設定されている裾の（適度に）広い事前分布であることが多い．↩︎\n従来は事前分布の経験ベイズ推定と呼ばれていた考え方である (Gelman, Vehtari, et al., 2020, p. 6)．↩︎\n一方で多くの頻度論的な手法は，無情報事前分布を仮定したベイズ推論とみなせる．そこでベイズの，有効な頻度論的モデルを探索するための方法としての美点が見えてくるのである．↩︎\nこの点については (Gelman, 2014) も参照．大統領選における有権者の行動のモデリングを，ベイズ階層モデルに基づいて探索的に実行しており，“multilevel Bayesian modeling can be considered as an elaborate form of exploratory data analysis” と結論している．↩︎\nもちろん重要な事前情報や予備解析が存在する場合は，これを事前分布としてどう更新されるかをみるのが良い．(1.6 節 Gelman, Hill, et al., 2020, p. 16) に簡潔な概観的議論がある．↩︎\n(Bafumi and Gelman, 2007) では unmodeled varying intercept と呼んでいる．↩︎\nすると「自由度」とは変動係数の数に他ならない．↩︎\n同様の縮小効果を得るための点推定手続きが，経験ベイズ の名称で研究されている (Efron and Morris, 1973), (Efron and Morris, 1975)．これについては (久保川達也, 2006) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html",
    "href": "posts/2024/Survey/Survey1.html",
    "title": "ベイズデータ解析１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#関連記事",
    "href": "posts/2024/Survey/Survey1.html#関連記事",
    "title": "ベイズデータ解析１",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#分散分析",
    "href": "posts/2024/Survey/Survey1.html#分散分析",
    "title": "ベイズデータ解析１",
    "section": "1 分散分析",
    "text": "1 分散分析\n\n1.1 はじめに\n分散分析 (ANOVA: Analysis of Variance) は標本がある因子 \\(A,B,\\cdots\\) によって層別されている場合に，層間の平均効果 \\(\\mu_i\\) に差があるかどうかを検定する手法である： \\[\nH_0:\\mu_1=\\cdots=\\mu_p\\quad\\text{v.s.}\\quad H_1:\\exists_{i,j\\in[p]}\\;\\mu_i\\ne\\mu_j.\n\\]\n因子 \\(A,B,\\cdots\\) の総数に応じて，\\(A\\) のみの場合を一元配置分散分析，\\(A,B\\) の場合を二元配置分散分析などという．\n多くの場合，観測のモデルには 線型 Gauss の仮定が置かれる．例えば一元配置である場合， \\[\nY_{ij}=\\mu_i+\\epsilon_{ij},\\qquad i\\in[p],j\\in[n_i],\\epsilon_{ij}\\sim\\mathrm{N}(0,\\sigma^2),\n\\tag{1}\\] というモデルを仮定し，パラメータ \\(\\mu_i\\) に関して検定を行う．\n二元配置では \\[\nY_{ij}=\\mu+\\alpha_i+\\beta_j+\\epsilon_{ij}\n\\] となり，集団は２つの軸 \\(A,B\\) で層別されており（分割表の状態），それぞれの因子からの効果 \\(\\alpha_i,\\beta_j\\) が説明変数に加法的に加えられる．\n\n\n1.2 分散分析の抽象的な説明\n分散分析では観測 \\(Y_{ij}\\) の変動のうち，\\(H_0\\) の仮定の下で説明されなかった部分（残差） \\(R_1^2\\) と，そもそも線型 Gauss 模型 (1) では説明しきれない部分 \\(R_0^2\\) とに関して， \\[\nF:=\\frac{(R^2_1-R^2_0)/q}{R^2_0/(n-r)}\n\\] を考える．ただし，\\(r:=\\operatorname{rank}(X)\\) はデータの自由度とした．\nこの \\(F\\) は，各群への所属表すダミー変数を用いた回帰分析の残差 \\(R_0\\) と，各群への所属を考慮しない回帰分析による残差 \\(R_1\\) とを，自由度を考慮して比を取った形をしている．\nこの \\(F\\) は一般に非心 F-分布に従い，仮定 \\(H_0\\) が成り立つときのみ 中心 F-分布 \\(\\mathrm{F}(q,n-r)\\) に従う．これは各群への所属情報に何の情報量もない場合には，\\(F\\) が同じ平均を持つ正規確率変数の自乗和の比になるためである．\n換言すれば，非心パラメータ \\[\n\\delta:=\\frac{1}{\\sigma^2}\\sum_{i=1}^pn_i(\\mu_i-\\overline{\\mu})^2\n\\] に関して \\(H_0:\\delta=0\\) を検定することに等しい (Bertolino et al., 1990), (Solari et al., 2008)．\nこの \\(F\\) （または等価な \\(t,\\delta\\)）を検定統計量として仮設検定を実行するのが (repeated measures) ANOVA である．\n\n\n1.3 Gauss-Markov の仮定\n「標本が正規分布に従う母集団からの独立標本である」という帰無仮説を持つ検定に，(Shapiro and Wilk, 1965) の検定などがある．\n探索的な方法には Q-Q plot などがある．(Bergh et al., 2020) も参照．\n等分散の仮定をチェックする検定には (Mauchly, 1940) の検定や (Brown and Forsythe, 1974) の検定などがある．\n仮にこれらの仮定が破られた場合は，(Kruskal and Wallis, 1952) 検定などのランクベースの ANOVA 手法を用いることができる．1\nただし，検定はデータの一側面しか伝えていない．例えば，データが帰無仮説をどれほど支持しているかの尺度は検定では得られない．\nそれゆえ，ANOVA などのモデルの仮定を確認するためには，検定だけでなく他の探索的手法と組み合わせることが推奨される．(Tijmstra, 2018) も参照．\n\n\n1.4 「検定」に対する注意喚起\n一方で ANOVA を含めた検定は一面のみを強調する構造となっており，一連の統計解析の中で自然な位置付けを持つものでない．\n特に，\\(p\\)-値による仮設検定は「データが不十分であることにより判断ができない」ことと，「データと帰無仮説は激しく矛盾する」こととを区別できない．この意味でも限定的な情報量しか持たない．\n例えば \\(p\\)-値が小さく帰無仮説が棄却されたとしても，\\(p\\)-値はモデルの変化に対して頑健ではないかもしれず，実際はほとんど帰無仮説が成り立つことが真実かもしれない．\nこのような全貌を探索的に捉えるためには，検定を金科玉条とするのではなく，広くモデル比較・モデル選択の観点からアプローチすることが大切である．同様のことが (Rouder et al., 2016) でも論じられている．\nANOVA は，特に大規模なものが，現在でも実験心理学などの分野で広く用いられている．これは心理学では多くの因子 \\(A,B,C,\\cdots\\) が存在し，それぞれが複雑な関係にあるためである．\nしかし推定法も従来の \\(F\\)-検定を用いたのではその力を十分に発揮できない．解決は丁寧な階層モデリングとベイズによる探索的な解析にある．2\n\n”if you have a complicated data structure and are trying to set up a model, it can help to use multilevel modeling”—not just a simple unitswithin-groups structure but a more general approach with crossed factors where appropriate. This is the way that researchers in psychology use ANOVA, but they are often ill-served by the classical framework of mean squares and F-tests. We hope that our estimation-oriented approach will allow the powerful tools of Bayesian modeling to be used for the applied goals of inference about large numbers of structured parameters. (Gelman, 2005, p. 53)"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ分散分析",
    "href": "posts/2024/Survey/Survey1.html#ベイズ分散分析",
    "title": "ベイズデータ解析１",
    "section": "2 ベイズ分散分析",
    "text": "2 ベイズ分散分析\n\n2.1 はじめに\nベイズ分散分析 (Rouder et al., 2012), (Rouder et al., 2016) では，(1) などの線型モデルに対して，パラメータが零ベクトルであることに対する検定の代わりに，帰無仮説を表現するモデルに対する Bayes 因子を用いたモデル比較を行う．\nつまり，ベイズ分散分析と言った場合，「分散分析」の概念は完全に線型回帰モデルのベイズ推論に回収される．より正確には，階層モデルのベイズ推論は「分散分析」の正統な後継である (Gelman, 2005)．\n\nIn this sense, ANOVA is indeed a special case of linear regression, but only if hierarchical models are used. (Gelman, 2005, p. 2)\n\n(Rouder et al., 2012) はその際の標準的な事前分布の選択について議論している（第 2.4 節）．\n(Rouder et al., 2016) が指摘するように，分散分析がベイズ化される過程で，検定がモデル比較に置き換わっている．\n\n\n2.2 JZS 因子\nベイズ的な仮設検定は (Jeffreys, 1961) に源流を持つ．一般に，位置母数の事前分布に Cauchy 分布を用いることは (5.3節 Jeffreys, 1961) に源流を持つ．このことについては (Robert et al., 2009) も参照．\n同一の単位を繰り返し測定し， \\[\nY_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\mu,\\sigma^2),\\qquad i\\in[n],\n\\] に従って何らかの処置効果 \\(Y_i\\) 観測するとし，帰無仮説 \\(\\mu=0\\) の妥当性を議論したいとする．\nこの際，まずは効果サイズ (effect size) (Cohen, 1988) \\(\\delta:=\\mu/\\sigma\\) という無次元量にパラメータを変換し，これを Cauchy 分布と比較することを考える： \\[\nM_0:\\delta=0\\quad\\text{v.s.}\\quad M_1:\\delta\\sim\\mathrm{C}(0,1)\n\\]\n実際，この Cauchy 分布というのは変換 \\(J(\\nu,\\sigma):=\\frac{\\mu^2}{\\sigma^2}\\) を通じて \\(\\mathbb{R}\\) 上の Jeffreys 事前分布（この場合は Lebesgue 測度に一致）に（ほとんど）等価になる．\nこの２つのモデル \\(M_0,M_1\\) の残りの仮定は共通の Jeffreys の事前分布 \\(p(\\sigma^2)\\,\\propto\\,\\sigma^{-2}\\) で共通とし，Bayes 因子を算出する．\nこの値を検定統計量のように扱うとき，これを Jeffreys の名前に (Zellner and Siow, 1980) を加えて JZS 因子 (Bayarri and García-Donato, 2007) という．あるいは上述の事前分布の選び方を JZS 事前分布という．\nJZS 因子は，事前のモデル確率 \\(p(M_0),p(M_1)\\) に依らずに算出できる．\n\n\n2.3 \\(p\\)-値との違い\nJZS 因子は \\(p\\)-値と比較して，サンプルサイズが大きいほど保守的になる傾向がある．(Wetzels et al., 2011) は 2007 年に特定の実験心理学雑誌に投稿された 855 件の t-検定に対して，JZS 因子と \\(p\\)-値との値を報告してこれを観察している．\n(Bergh et al., 2023) は実例を用いて，特に複雑な心理学実験において，２つの解析手法はときに全く違う結論を導くことを例証している．\nまた JZS 因子は \\(N\\to\\infty\\) の極限で，\\(\\delta\\ne0\\) であった場合は \\(\\infty\\) に発散し，\\(\\delta=0\\) であった場合は \\(1\\) に収束するという性質を持つ．\n\n\n2.4 １元配置 ANOVA の線型モデル解釈\n１元配置 ANOVA のモデルを次のように表す： \\[\n\\boldsymbol{Y}=\\mu\\boldsymbol{1}_n+\\sigma\\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon},\\qquad\\boldsymbol{\\epsilon}|\\sigma^2\\sim\\mathrm{N}(\\boldsymbol{0},\\sigma^2I_n).\n\\tag{2}\\] 各水準 \\(i\\in[p]\\) に属するデータの数を \\(n_i\\) とすると \\[\n\\boldsymbol{X}=\\begin{pmatrix}\n\\boldsymbol{1}_{n_1} & \\boldsymbol{0} & \\cdots & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{1}_{n_2} & \\cdots & \\boldsymbol{0}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\boldsymbol{0} & \\boldsymbol{0} & \\cdots & \\boldsymbol{1}_{n_p}\n\\end{pmatrix},\\qquad\\boldsymbol{\\theta}=\\begin{pmatrix}\n\\mu_1\\\\\n\\mu_2\\\\\n\\vdots\\\\\n\\mu_p\n\\end{pmatrix}.\n\\] となる．\nこのとき，定数項 \\(\\mu\\) をすでに括り出しているので，\\(\\boldsymbol{\\theta}=0\\) の場合のモデルが帰無仮説に対応する．\n対立仮説としては，\\(\\boldsymbol{\\theta}\\) 上に次の \\(g\\)-prior を定める： \\[\n\\boldsymbol{\\theta}\\sim\\mathrm{N}(\\boldsymbol{0},G),\\qquad G=\\mathrm{diag}(g_1,\\cdots,g_p),\\qquad g_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^{-2}(1).\n\\] これは各 \\(\\mu_i\\) に対して独立な Cauchy 事前分布を仮定している．\n(Zellner and Siow, 1980) の事前分布 \\[\n\\boldsymbol{\\theta}|g\\sim\\mathrm{N}(\\boldsymbol{0},g(\\boldsymbol{X}^\\top\\boldsymbol{X}/n)^{-1}I_p)\n\\] や一般の \\(g\\)-prior との違いとして，スケール因子 \\((\\boldsymbol{X}^\\top\\boldsymbol{X}/n)^{-1}\\) がない形であるのは，ANOVA の説明変数 \\(\\boldsymbol{X}\\) が離散変数であるためである．\n\n\n2.5 ANOVA でのベイズ因子\n以上のモデルを，帰無仮説に対立させる「デフォルトモデル」として提案したのが (Rouder et al., 2012) である．\n特に \\(G=gI_p\\) の場合は，結果として得られるベイズ因子は１次元の積分のみを含むため，簡単な数値積分アルゴリズムによって計算可能である．\n\n\n2.6 多元配置ベイズ ANOVA\n多くの場合，被説明変数に関連すると予期される因子は複数存在する．ここでは２元配置の場合を考える．それぞれの因子が \\(a,b\\) 水準を持つとき，フルモデルは \\[\n\\boldsymbol{Y}=\\mu\\boldsymbol{1}+\\sigma\\biggr(\\boldsymbol{X}_\\alpha\\alpha+\\boldsymbol{X}_\\beta\\beta+\\boldsymbol{X}_{\\gamma}\\gamma\\biggl)+\\boldsymbol{\\epsilon},\n\\] と表せる．\n詳しくは (Section 8 Rouder et al., 2012) を参照．\n\n\n2.7 「ベイズ因子」に関する注意喚起\nベイズ因子を含め，周辺尤度 \\(p(\\theta|y)\\) （エビデンスともいう）を用いた指標は，モデルの仮定に対して感度が高い (Robert, 2016), (Kamary et al., 2018)．\nそのため「モデルのデータへの当てはまりの良さを１つの指標にまとめた値」としては少し心許ない．\n加えて，帰無仮説に対立させる仮説を構成して，二項対立の構造に持ち込むことは，自然なデータ解析のワークフローに必ずしも入ってこない．\nベイズ推論の仮定で得られる事後分布から，特定の仮説 \\(H:\\theta=\\theta_0\\) がまともかを評価する方法の方が，探索的データ解析の観点からは含意が多いことも多い．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ推論に基づく方法",
    "href": "posts/2024/Survey/Survey1.html#ベイズ推論に基づく方法",
    "title": "ベイズデータ解析１",
    "section": "3 ベイズ推論に基づく方法",
    "text": "3 ベイズ推論に基づく方法\n\n3.1 はじめに\nANOVA とベイズ ANOVA の究極的な目標は，平均が一致する \\[\nH_0:\\mu_1=\\cdots=\\mu_p\n\\] という仮説がデータからどれほど支持されるか／されないかを評価することにある．\n最も直接的な方法は，パラメータ空間上に描かれる事後分布を見ることである．信用区間を報告し，帰無仮説がそのどこに位置するかを見ても良いだろう．\n\n\n3.2 事後予測によるモデル検証\nこのように，ベイズ事後分布やそのサンプルを通じたモデル検証方法は posterior predictive check (Gelman and Shalizi, 2013) と呼ばれ，これを多角的に行うことが一つの理想形とされている (Gelman et al., 2020)．\n同様に (Kruschke, 2015) では，モデル (2) の形を一般化線型モデルの特別な場合と見て推定し，ANOVA をモデル比較の観点から適切に実行する方法を論じている．\nこのように，ANOVA を適切に扱うには，階層モデルとしての取り扱いが欠かせない．同様の議論は (Gelman, 2005) でも展開されている．\nここでは，以下の節で帰無仮説 \\(H_0\\) の妥当性を詳細に評価するための方法を見ていく．\n\n\n3.3 ベイズ事後 \\(p\\)-値\nBayes 因子の他に，検定統計量に対するベイズ事後予測分布を導出し，その裾確率を評価して \\(p\\)-値の代替とする方法もある．\nこれは 事後予測 \\(p\\)-値 (posterior predictive \\(p\\)-value) (Gelman et al., 2014, p. 146) と呼ばれる．\n\n\n3.4 ROPE (Region of Practical Equivalence)\nROPE は帰無仮説 \\(H_0\\) と区別がつかないとする区間である．\n帰無仮説が \\(H_0:\\theta=\\theta_0\\) という形をしていた場合，ほとんどの場合 \\(\\theta=\\theta_0+0.1\\) でも事実上変化はない．\nこのように，帰無仮説と同一視してしまう範囲を ROPE (Kruschke, 2015, p. 336) という．\n\n3.4.1 HDR の使用\nこの ROPE が 95% 最高密度信用領域 (HDR: Highest Density Region) と互いに素になるときに，「棄却」されたとする．\nただし，最高密度信用領域とは 95% 信用区間＝95% の事後確率を持つ領域のうち，面積が最も小さいもののことを言う．\nこの方法では，逆に HDR が ROPE を完全に含む場合，帰無仮説を「採択」するという積極的な意思決定も可能である．\nROPE と同様の考え方は，ベイズによる実験計画法でも range of equivalence (Freedman et al., 1984), (Spiegelhalter et al., 1994) の名前で用いられてきた歴史がある．\n\n\n3.4.2 ROPE の確率\nまたは，事後確率分布が ROPE 内にどれほどの確率を与えるかを見ることもできる (Kruschke, 2018)．\nROPE の応用と実装は (Kelter, 2022) も参照．\n\n\n\n3.5 混合モデリングによる方法\nベイズの方法の特徴は，検定・モデル比較と推論とに区別がないことである．\n加えて純粋に検定・モデル比較のための手続きを作るより（ベイズ因子など），推定の一種として扱った方がより多くの情報を引き出せるため，ワークフローとしては好ましい (Kruschke, 2011)．\n(Robert, 2016), (Kamary et al., 2018) では検定の対象となっているモデル \\[\nM_1:x\\sim f_1(x|\\theta_1)\\quad\\text{v.s.}\\quad M_2:x\\sim f_2(x|\\theta_2)\n\\] を，混合モデル \\[\nM_\\alpha:x\\sim\\alpha f_1(x|\\theta_1)+(1-\\alpha)f_2(x|\\theta_2)\n\\] の成分の１つとみなし，その荷重 \\(\\alpha\\) の事後分布を推定し，これを検定に用いるという方法が提案されている．\n\n\n3.6 ハイパーモデル上の推論による解決\n同様の発想により，ベイズ因子の計算と推論によるモデル比較とを，より大きなハイパーモデルを立てることで同時に実行することもできる．\n(Kruschke, 2011, p. 308) や (O’Neill and Kypraios, 2016) などで考えられている．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ統計解析に関する文献案内",
    "href": "posts/2024/Survey/Survey1.html#ベイズ統計解析に関する文献案内",
    "title": "ベイズデータ解析１",
    "section": "4 ベイズ統計解析に関する文献案内",
    "text": "4 ベイズ統計解析に関する文献案内\n応用分野の研究者に対する「なぜベイズを使うのか？」に対する端的な回答として，「統計的有意性」などの「わかりやすい」指標に飛びついた結果，真のデータの声を聞かずに自分の見たいものを見始めてしまうと言うことが少なく，「自己欺瞞に陥りにくい」という美点があることは，ベイズ機械学習の稿 でも触れた．\n\\(p\\)-値はそのような欺瞞を生む代用例であり，使用を禁止すべきとの声 (Blakeley B. McShane and Tackett, 2019) もある．その論拠は大まかに次のとおりである．\nそもそも \\(p\\)-値とは，「帰無仮説を採用したモデルはデータへの当てはまりの度合いが悪い」ということを言っているだけであり，\\(p\\)-値が十分に低ければそれ以上の情報は引き出せない．\n当然 \\(p\\)-値が \\(0.01\\) であることと \\(0.00001\\) であることは質的に全く変わらない (Gelman et al., 2014, p. 150)．\nそのことに加え \\(p\\)-値は必ずしも頑健な指標ではなく，帰無仮説を少し摂動させただけで \\(p\\) 値が大きくなってしまうかもしれない．そのような場合は結局ほとんど帰無的であり，「統計的有意性」はほとんど無意味になってしまう．同様の議論が (Gelman and Stern, 2006) で展開されている．\nこのような現状への対処として，応用分野の研究者にもベイズ統計学は根本的な解決法として広く推奨される (Dienes and Mclatchie, 2018)．(Wagenmakers et al., 2016) はその旨を２つの実例を通じて簡潔に実証しており，同時にベイズ統計学の考え方に対する洗練された導入を行なっている．\n(Bergh et al., 2020) は分散分析をベイズの方法によって実行する手引きを，特に JASP を用いて実演している．\nJASP のベイズ ANOVA のエンジンは R パッケージ BayesFactor (CRAN / GitHub) を用いている．BayesFactor では大規模な \\(M\\)-元配置 ANOVA モデルにおいても Bayes 因子を用いたモデル比較を行うことができる．\nベイズ ANOVA の R パッケージとしては bayesanova (CRAN / GitHub) (Kelter, 2022) もある．これは検定に似た行為を根本的に排除して Gauss 混合モデルとして Gibbs サンプラーによるベイズ推定を実行し，ROPE (Region of Practical Equivalence) (Kruschke, 2015, p. 336) (Kruschke, 2018) を用いたモデル比較を行う．\nもちろんこのような完全なモデリングを行うことが理想かもしれないが，従来の ANOVA になれきっている研究者にとっては，Bayesian ANOVA に手を伸ばしてみることが次のステップとして大変良いだろう．\nまた別の角度からの「ベイズを使うべき理由」としての説得的な議論としては，ベイズ階層モデリングは ANOVA の正統進化という理解 (Gelman, 2005) ができるという向きもある．\n以上の立場は (Gelman et al., 2014) や (Kruschke, 2015) などの標準的なベイズデータ解析の教科書でも一貫している．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#その他の文献案内",
    "href": "posts/2024/Survey/Survey1.html#その他の文献案内",
    "title": "ベイズデータ解析１",
    "section": "5 その他の文献案内",
    "text": "5 その他の文献案内\n\nF-検定については (吉田朋広, 2006) と (Solari et al., 2008) を参考にした．\nANOVA の歴史については (Tweney, 2014) を参照．(repeated measures) ANOVA は多重線型回帰のうち説明変数が離散変数である場合に相当するという理解は，一般化線型モデルの発展と普及に伴って理解が広がった．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#footnotes",
    "href": "posts/2024/Survey/Survey1.html#footnotes",
    "title": "ベイズデータ解析１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nもちろん，Stan などの確率的プログラミング言語を用いた完全なベイズモデリングはいつでも実行可能である．↩︎\nそして因子分析を通じて，記述統計学の正統進化であるということもできる！？ ANOVA の歴史については (Tweney, 2014) も参照．↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes for Self Study",
    "section": "",
    "text": "サーベイ | レビュー | カテゴリ\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n12/31/2024\n\n\nHirofumi Shiba\n\n\nDetails.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nノンパラメトリック回帰分析\n\n\n\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n\n\n\n\n\n12/30/2024\n\n\n司馬 博文\n\n\nNRegression.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n重回帰モデルにおける OLS 推定量は，部分回帰推定量としての解釈を持つ． この性質を用いた手法が媒介分析や操作変数法である． OLS 推定量は不均一分散の場合でも不偏性・一致性・漸近正規性を持ち得るが，漸近有効性は失われる． これを回復するには，誤差の分散を推定して重み付けを行う必要がある． このような方法は一般化最小二乗法と呼ばれる． さらに相関を持つデータを分析するために，より一般の共分散構造を持ったモデルに対してこの手法が拡張されている． 疫学では一般化推定方程式，さらに一般には計量経済学で一般化モーメント法と呼ばれる方法である． これらの方法は作業共分散の選択により，セミパラメトリック漸近最適な分散を達成したり，バイアスを小さくしたりできるが， いずれもトレードオフの範疇にある． \n\n\n\n\n\n12/29/2024\n\n\n司馬博文\n\n\nRegression.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\n\n\n\nProbability\n\n\nStatistics\n\n\n\nWe examine how to find & formally determine the likelihood function of hierarchical models. As a real-world example, we consider the ideal point model, also known as the 2-parameter logistic item response model. \n\n\n\n\n\n12/23/2024\n\n\nHirofumi Shiba\n\n\nlikelihood.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\nIdeal point models are 2-parameter item response model, tailored to the purpose of visualizing / measuring the ideological positions of the legislators / judges. [@Bafumi+2005] introduced a hierarchical structure to the model to deal with the problem of identifiability. In this article, we re-examine the model and show that the posterior distribution of the parameters (ideal points) is still bimodal, indicating its weak identifiability. \n\n\n\n\n\n12/22/2024\n\n\nHirofumi Shiba\n\n\nBafumi.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n12/21/2024\n\n\n司馬 博文\n\n\nBayesSticky.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n12/21/2024\n\n\n司馬 博文\n\n\nBayesTraverse.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n12/16/2024\n\n\n司馬 博文\n\n\nBayesNonparametrics.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n\n\n\n\n\n12/15/2024\n\n\n司馬 博文\n\n\nIdealPoint3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n項目反応モデルとは，被験者と項目のそれぞれが独自のパラメータを持った一般化線型混合効果モデルである． 被験者ごとの特性の違いや，項目ごとの性質の違いが視覚化できるが， 本稿では能力・難易度パラメータに更なる階層構造を考える． これにより能力パラメータを変化させている背後の要因や，項目特性と個人特性の交絡効果（特異項目機能）を解析することが可能になる． brms パッケージは極めて直感的な方法でモデルのフィッティングから事後分布の推論までを実行できる． \n\n\n\n\n\n12/14/2024\n\n\n司馬博文\n\n\nBayesGLMM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\nロジスティック回帰分析は離散的な応答データを扱うことのできる一般化線型モデルである． 他にも，高度に非線型な関係が予期される場合，ノンパラメトリック手法に移る前の簡単な非線型解析としても活躍する． 本稿では BMI と LDL の非線型関係に関する探索的手法として，順序ロジスティック回帰分析を実行する． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\nBayesGLM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nデータが自然な階層構造を持つ場合，これを取り入れた自然な事前分布を，一つ上の階層に回帰モデルを付け加えることで構成できる． このようなモデルをベイズ階層モデルという． 本稿ではベイズ階層モデルの縮小効果を概観する． 事前知識を構造に関する知識としてモデルに取り入れることでデータによりフィットする尤度構造を獲得することは，データ解析の一つの目標として，（線型）回帰モデルの自然な拡張と理解できる． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\nBDA3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬博文\n\n\nFixedRandom.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析８\n\n\n正規グラフィカルモデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬 博文\n\n\nBDA4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n点推定における変数選択法は，正則化項の追加によることが多い． これはベイズ推論では \\(0\\) 近傍に大きな確率を持った事前分布を仮定していることに等しい． ベイズの観点から適切な縮小事前分布を用意することで，大きな効果を持つ回帰係数は変えずに， 効果の小さい変数を排除することができる． 一般に LASSO よりも絞って選択してくれることが多い．\nまたベイズ変数選択では，\\(0\\) にアトムを持つ事前分布を用いることで，当該の変数がモデルに含まれる事後確率 (PIP: Posterior Inclusion Probability) を算出することができる． この方法ではモデルの空間を効率的に探索するサンプラーの開発が重要であるが， 近年では効率的なサンプラーが複数提案されている． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\nBayesSelection.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\nベイズ重回帰分析は解析者のデータへの理解を促進する強力な探索的データ解析手法である． このことを brms パッケージと BMI データを用いて例証する． １変数の場合から始め，変数を追加して挙動が変わるのを解釈・検証（残差プロット・事後予測プロット）しながら慎重に進んでいく． 交差検証による事後予測スコア elpd を用いて，データの非線型変換を利用することで，非線型な関係を見出す方法を扱う． ここまで行えば，データの階層化やノンパラメトリックな手法の採用などの次のステップが自然と見えてくるだろう． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\nBayesRegression.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する． 分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる． また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\nBDA1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n通常の回帰モデルは応答変数が連続であることが暗黙の仮定となっている． この節では，応答変数が質的変数である場合のモデリングを扱う． 質的変数は順序変数であるか名目変数であるか（順序の構造があるかないか）の峻別が重要である． いずれの場合でも多くのモデルが利用可能であり，その多くが一般化線型モデルの枠組みで統一的に扱うことができる． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\nBDA2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n12/01/2024\n\n\n司馬博文\n\n\nUCL.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n理想点解析とは，政治学において国会議員のイデオロギーを定量化・視覚化する方法論である．この手法は多くの側面を持ち，項目反応モデルであると同時に多次元展開法 (MDU: Multidimensional Unfolding)でもある． \n\n\n\n\n\n11/22/2024\n\n\n司馬 博文\n\n\nIdealPoint2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\nJulia に存在する粒子フィルター関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n10/26/2024\n\n\n司馬 博文\n\n\nAdvancedPS.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\nPDMP / 連続時間 MCMC とは 2018 年に以降活発に研究が進んでいる新たな MCMC アルゴリズムである． 実用化を遅らせていた要因として，種々のモデルに統一的な実装が難しく，モデルごとにコードを書き直す必要があったことが挙げられたが， この問題は自動微分の技術と，[@Corbella+2022], [@Sutton-Fearnhead2023] らの適応的で効率的な Poisson 点過程のシミュレーションの研究によって解決されつつある． ここでは [@Andral-Kamatani2024] の Python パッケージ pdmp_jax とこれに基づく Julia パッケージ PDMPFlux.jl を紹介する． \n\n\n\n\n\n10/17/2024\n\n\n司馬博文\n\n\nPDMPFlux.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程に駆動される SDE のエルゴード性\n\n\nカップリング法／最適輸送距離による証明\n\n\n\nProcess\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n10/14/2024\n\n\n司馬 博文\n\n\nPureJump1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\nSchrödinger 橋は\n\n\n\n\n\n10/06/2024\n\n\n司馬 博文\n\n\nSF1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である．今回は 公式の実装 を吟味する． \n\n\n\n\n\n10/06/2024\n\n\n司馬博文\n\n\nSB2-HandsOn.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\nLorenz’ 63, Lorenz’ 96 とはそれぞれ [@Lorenz1963], [@Lorenz1995] によって導入された大気モデルである． 前者はバタフライ効果の語源ともなった，最初に特定されたカオス力学系でもある． Navier-Stokes 方程式は流体の運動を記述する方程式である． これらはいずれもデータ同化・軌道推定技術のベンチマークとして用いられている． ここでそれぞれのモデルの数学的性質と Julia を通じたシミュレーションの方法をまとめる． \n\n\n\n\n\n10/05/2024\n\n\n司馬博文\n\n\nLorenz95.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n10/03/2024\n\n\n司馬 博文\n\n\nTrans1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n政治学における理想点解析とは，項目反応モデルを用いて裁判官や国会議員などの価値判断基準や「イデオロギー」を定量化・視覚化する方法である． ここでは既存のパッケージを用いて簡単に理想点解析を行う方法から始め， 自分で Stan コードを書いてモデルを推定する方法を紹介する． その際に最も重要な理想点モデルの性質として，識別可能性 の議論がある． これが保たれていないと，モデルの事後分布は多峰性を持ってしまい，推定をするたびに結果が異なったり，統計量の長期間平均が \\(0\\) になってしまったりしてしまう． \n\n\n\n\n\n10/02/2024\n\n\n司馬博文\n\n\nIdealPoint1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n10/01/2024\n\n\n司馬 博文\n\n\nTrans2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし，古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． 一方で，モデルの仮定を前面に出したベイズ的な解析手法は，データを探索的に吟味することができ，極めて微妙な消息も捉えることが可能になる． 本稿では特にベイズ ANOVA 手法 [@Gelman2005], [@Rouder+2012] を採用して，そのモデルケースを実証する． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\nBayesANOVA.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n応募法 (voluntary sampling) や多くのウェブアンケートは，確率標本抽出に該当しない．このような場合でも母集団に関する補助情報がある限り，バイアスを軽減し推定精度を高めることができる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\nSurvey4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n標本調査において欠測はつきものである．観測単位が欠測している場合 (unit nonresponse)，call-back や follow-up などの調査を行うか，それができない場合は 荷重校正 (calibration weighting) が可能である．一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\nSurvey3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． これらの問題点を解決策としてベイズの方法を導入し，ベイズ ANOVA，ベイズ推論とモデル比較が ANOVA の発展として得られることをみる． この拡張は，ANOVA の線型モデルとしての解釈を通じてなされ，ANOVA の「同じ係数を共有するクタスタ構造の特定手法」というより広い理解へ導かれる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\nSurvey1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n人間を対象にする介入の研究では，介入の前後で変化があったかが争点となる． この変化の量を表す平均処置効果 (ATE) を，なるべくモデルを仮定せずどこまで識別できるかが多くの場合論点になる． この際の枠組みが潜在結果モデルである． したがって，操作変数法などの交絡統制法がある一方で，ATE の推定にはモデルの誤特定に強いセミパラメトリックな手法が要請される． 一般化推定方程式，一般化モーメント法，経験尤度法などの方法がある． 本稿ではこれらの推定量を同一の枠組みの下でまとめる． 推定量の分散を求めるためには漸近論のほかにブートストラップ法も用いられる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\nSurvey2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n9/22/2024\n\n\n司馬博文\n\n\nBayesTrans.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能ですが，本稿では特に R からの利用方法をまとめます．\n\n\n\n\n\n9/19/2024\n\n\n司馬博文\n\n\nStan2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n医療技術評価における生存解析では，打ち切りデータを最もよく外挿できるハザードモデルが探索される． そこでモデルの選択が重要な課題になるが，ベイズの方法だとモデル平均というアイデアが使える． これを polyhazard model で実行するためのベイズ階層モデルとモデル平均法を紹介する． キーとなる記述は Zig-Zag サンプラーである． \n\n\n\n\n\n9/12/2024\n\n\n司馬 博文\n\n\nSurvivalAnalysis1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n本稿では生存時間解析の代表的なモデルを概観する． 特に医療技術評価への応用では，打ち切りデータを最もよく外挿できるハザードモデルが探索され，ベイズ推定が有効な方法としてよく選択される． 本稿では特に表現力の高い競合リスクモデルとして polyhazard model を紹介し，ベイズ推定の困難さを議論する．\n次稿ではこのモデルを Zig-Zag サンプラーでベイズ推定する方法を紹介する． \n\n\n\n\n\n9/12/2024\n\n\n司馬博文\n\n\nSurvivalAnalysis.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n最適輸送問題は変分法の黎明期に提案された変分問題の１つであるが，その発展は確率論の成熟を待つ必要があった．現代では多くの非正則な空間上に幾何学的な量を定義する普遍的な手法として理解されてから，多くのフィールズ賞受賞者を輩出する最も活発な分野の１つとなっている．ここまでの発展の歴史を本記事では概観したい．\n\n\n\n\n\n9/03/2024\n\n\n司馬博文\n\n\nOT.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．特に OU 過程の時間反転は雑音除去過程 (Denoising Diffusion) といい，サンプリングに利用されている．デノイジングスコアマッチングでは，時間反転に Hyvärinen スコアが出現することを利用してデータ分布のスコアを推定する．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．\n\n\n\n\n\n8/26/2024\n\n\n司馬博文\n\n\nDD1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\nSkilling-Hutchinson の跡推定量は，跡の計算 \\(O(d^2)\\) を \\(O(d)\\) に落とすことができる Monte Carlo 法である．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\nTrace.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\nNF4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\nHierarchicalModel.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n行列の特異値分解とは，正方行列の直交対角化を一般の行列に拡張したものである．特異値を大きいものから \\(r\\) 個選ぶことで，Hilbert-Schmidt ノルムの意味で最適な \\(r\\)-階数近似が構成できる．このことは主成分分析に応用を持つ．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\nSVD.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n生物情報学への応用を念頭に，tSNE と Diffusion Map について詳しく扱う．\n\n\n\n\n\n8/11/2024\n\n\n司馬 博文\n\n\nDiffusionMap.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\nカーネル法とは，半正定値カーネルを用いてデータを Hilbert 空間内に埋め込むことで，非線型な変換を行う統一的な手法である．再生核 Hilbert 空間の理論により，写した先における内積は，半正定値カーネルの評価を通じて効率的に計算できるため，無限次元空間上での表現に対する tractable な手段を提供する．適切な半正定値カーネルを用いることで，データの「類似度」を定義することができる．本稿では半正定値カーネルの理論と距離学習法を扱う．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\nKernel.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散模型は拡張性にも優れており，条件付けが容易である．現状は誘導付き拡散によってこれが実現されるが，連続的な条件付き生成のために，フローマッチングなる方法も提案された．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\nNF3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n画像と動画に関してだけでなく，化学分子の構造生成の分野でも拡散模型が state of the art となっている．これは，連続空間上だけでなく，グラフなどの離散空間上でも拡散模型が拡張されたことが大きい．本稿では，離散データを連続潜在空間に埋め込むことなく，直接離散空間上に拡散模型をデザインする方法をまとめる．\n\n\n\n\n\n8/09/2024\n\n\n司馬博文\n\n\nDiscreteDiffusion.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n\n\n\n\n\n8/06/2024\n\n\n司馬博文\n\n\nDDPM1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフロー学習手法は，画像と動画に関して 2024 年時点で最良の性能を誇る． これは統計的に言えば事後分布からの近似的サンプリングを実行していることに相当する． 近似的ではなく，正確に２つの分布を補間するような拡散過程を推定するためには Schrödinger 橋がある． Schrödinger 橋については 次稿 に譲るとし，本稿ではサンプラーとしての拡散モデルを復習する． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nSB0.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルは「データ過程をノイズに還元する Langevin ダイナミクスを時間反転する」という発想に基づいており，画像と動画の生成・条件付き生成タスクに関して 2024 年時点で最良の方法の１つである． この発想を正確なサンプリング法に昇華するためには，[@Deming-Stephan1940] の Iterative Proportional Fitting アルゴリズムを用いることができる． この方法は拡散モデルによる条件付き生成の加速法として [@Shi+2022] によって提案された． こうして得る拡散過程は Schrödinger Bridge とも呼ばれ，エントロピー最適輸送と深い関わりを持つ． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nSB1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である． 本記事では Schrödinger 橋を用いて DDS を正確にすることを考える． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\nSB3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\nSB2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフローによるサンプリング法は，画像と動画に関して 2024 年時点で最良の方法の１つである．本稿ではこれを統計に応用することを考える．\n生成モデリングを２つの密度の補間問題と捉え，Schrödinger 橋を用いた正確なサンプリング法を考える．この観点から展開されるブリッジマッチング（橋照合？）はフローマッチング，確率的補間，Rectified Flow などを綜合する枠組みとなる． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\nTransportMethods.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nEBM2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は normflows を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nNF2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて，GAN の実装の概要を見る．\n\n\n\n\n\n8/02/2024\n\n\n司馬 博文\n\n\nGAN.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\nEBM1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて， Ho et. al. [NeurIPS 33(2020)] による DDPM (Denoising Diffusion Probabilistic Model) の実装の概要を見る．DDPM は拡散模型の最初の例の１つであり，ノイズからデータ分布まで到達するフローを定める拡散過程（雑音除去過程）を，データをノイズにする拡散過程の時間反転として学習する方法である．画像や動画だけでなく，離散空間上でタンパク質などの構造生成でも state of the art の性能を示すモデルである．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\nDDPM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬博文\n\n\nManifold.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n表現学習，非線型独立成分分析など，「生成」以外の潜在変数模型の応用法を横断してレビューする．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．\n\n\n\n\n\n7/29/2024\n\n\n司馬博文\n\n\nNCL.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\nVAE.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\nBayes3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\nサンプリング，または Monte Carlo 法は，現代の統計学と機械学習において必要不可欠な道具となっている．それは一体どうしてだろうか？初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\nSampling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\nLogistic2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\nZigZagSubsampling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．初めに政治学における理想点解析の目的と役割を概観し，続いて多次元展開法と項目反応理論の２つの観点から理想点解析を眺める． \n\n\n\n\n\n7/16/2024\n\n\n司馬博文\n\n\nIdealPoint.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\nLogistic.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\nLangevin.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\nJulia に存在する MCMC 関連のパッケージをまとめ，多くの MCMC のパッケージを支える，Turing ecosystem の基盤となる抽象的なフレームワーク MCMCChains と AbstractMCMC を概観する．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\nMCMCwithJulia.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する Metropolis-Hastings 法と MALA 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\nMALAwithJulia.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する HMC 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\nHMCwithJulia.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\nZigZag.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\nLevy.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\nECMC (Event-chain Monte Carlo) 法は，平衡分布の直接的な評価を一度もすることなく，平衡分布からのサンプリングを達成する新たなモンテカルロ法である．非対称性をもち，従来手法より高い効率を持つ．実際，Metropolis 法の開発以来の興味の対象であった２次元剛体円板系の液相転移のシミュレーションに，約 60 年越しに成功している．\n\n\n\n\n\n6/29/2024\n\n\n司馬 博文\n\n\nStatisticalMechanics4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\nStatisticalMechanics3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\nPoisson.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\nBayes2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\nBayes1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\ncalculus.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of Applied Probability 53(2016) 410-20] は Metropolis-Hastings アルゴリズムの計算複雑性を論じたもの \n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\nRoberts-Rosenthal2016.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\nOrnstein-Uhlenbeck 過程は唯一の非自明な定常 Gauss-Markov 過程である．また，連続時間の自己回帰模型を与える重要な拡散過程である．加えて，その遷移半群は解析的な表示を持ち，Malliavin 解析でも基本的な意味を持つ．したがって，直感的な理解を涵養しておくことは非常に見返りが大きいことだろう．そこで，YUIMA パッケージを通じてシミュレーションを行いながら，Ornstein-Uhlenbeck のパラメータの意味と，遷移半群・生成作用素の直感的な理解の醸成を目指す．\n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\nOU1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫制博士課程（正確には，総合研究大学院大学統計科学コース）を紹介します．同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷のような研究環境が整っていると言えるでしょう．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\nSOKENDAI.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\nMCMC.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Statistical Science 16(2001) 351-67] は Metropolis-Hastings 法の最適スケーリングに関する結果をまとめ，実際の実装にその知見をどのように活かせば良いかを例示したレビュー論文である． \n\n\n\n\n\n5/21/2024\n\n\n司馬 博文\n\n\nRoberts-Rosenthal2001.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はこのような確率過程に対する統計推測を実行する方法を紹介します．yuima は従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．ほとんどの手法が，\\(N\\to\\infty,\\Delta_n\\to0\\) の極限で得られるデータ（高頻度データ）にも応用可能な手法となっています．\n\n\n\n\n\n5/18/2024\n\n\n司馬 博文\n\n\nYUIMA2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はそのような確率過程の汎函数の漸近展開に基づく計算方法を紹介します．確率変数の期待値を近似するのに Monte Carlo 法は普遍的な方法ですが，漸近展開が用いられる場合，その計算時間は比較にならないほど速くなります．\n\n\n\n\n\n5/18/2024\n\n\n司馬博文\n\n\nYUIMA1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．本稿では Stan 言語の基本をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\nStan1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．鋭意開発中のパッケージですが，すでに広範なクラスの確率微分方程式のシミュレーションが可能です．本稿では基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\nYUIMA.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\nR パッケージ YUIMA を用いた SDE のベイズ推定に，Stan を用いる方法を模索する．Stan は C++ を用いる独立した確率プログラミング言語で移植性は高いが，それ故 YUIMA からこれを用いる際に，専用のインターフェイスを考える必要が生じる．\n\n\n\n\n\n5/12/2024\n\n\n司馬 博文\n\n\nadastan.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行う R パッケージである． 基本的な線型回帰から固定・変量効果の追加まで極めて簡単に実行できる，大変実用的なパッケージである． 本稿では，brms の基本的な使い方とその実装を紹介する． その中で混合効果モデルについてレビューをする． ランダム効果の追加は縮小推定などの自動的な正則化を可能とする美点がある一方で，係数の不偏推定やロバスト推定に拘る場合はこれを避ける判断もあり得る． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\nbrms.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\nAppliedMath.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n5/07/2024\n\n\n司馬 博文\n\n\nMeasureTheory.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\nRoberts-Tweedie1996.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの． \n\n\n\n\n\n4/22/2024\n\n\n司馬 博文\n\n\nRoberts-Rosenthal1998.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\nDuane+1987.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\nTartero-Krauth2023.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\nMetropolis+1953.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子法とは空間や分布を多数の粒子の集合として離散化して表現・計算する技術の総称である．シミュレーションからデータ同化まで幅広い応用を持つ．この記事ではこれらの技術を「粒子」という軸でひとつの記事にまとめることを試みる． \n\n\n\n\n\n4/07/2024\n\n\n司馬 博文\n\n\nParticleMethods.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\nStatisticalMechanics2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\nStatisticalMechanics1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\nAboutSimulation.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\nPeters-deWith2012.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\nButkovsky-Veretennikov2013.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n論文メモ\n\n\n\nReview\n\n\n\n[@Dai+2019] は有限混合で表される分布からのサンプリング法（Fusion 問題）に関する最初の理論解析である． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\nDai+2019.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n[@Fearnhead+2017] は拡散過程を離散化誤差なしにシミュレーションする手法を提案している．逐次重点サンプリング（SIS）の連続時間極限を考えることで，提案過程と重点荷重との組がPDMPとなり，効率的なシミュレーションが可能になる． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\nFearnhead+2017.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬博文\n\n\nEBM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\n\n\n\nProcess\n\n\n\nMarkov 過程のエルゴード性の証明は，カップリングの概念を用いれば極めて明瞭に見渡せる．\n\n\n\n\n\n3/25/2024\n\n\n司馬 博文\n\n\nCoupling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\nResidualWaitingTime.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論，確率論，さらにはデータ解析の中心に据えられるべき中心概念であると言えるかもしれない．例えば，カーネル法とは確率核に沿った埋め込みである．MCMC の性質も，本質的に確率核の性質が決定する．また確率核は，確率空間の圏の射となる．このように，多くのデータ解析手法の中核に位置する数学的本体たる「確率核」への入門を目指すのが本記事である．\n\n\n\n\n\n3/24/2024\n\n\n司馬博文\n\n\nKernel.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの微細化技術をレビューする．\n\n\n\n\n\n3/23/2024\n\n\n司馬 博文\n\n\nSemiconductor2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\nBAI.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\nKernel1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\nLLM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\nPython で最適輸送写像を計算する方法を解説する． 直接最適輸送問題を POT (Python Optimal Transport) で解く．この方法は原子の数 \\(N\\) に対して \\(O(N^3\\log N)\\) の複雑性を持つ． 一方で，エントロピー正則化項 \\(\\epsilon\\operatorname{Ent}(\\pi)\\) を導入したエントロピー最適輸送問題は Sinkhorn アルゴリズムで高速に解くことができる． これには OTT-JAX パッケージを用いる． \\(\\epsilon\\to0\\) の極限で元の最適輸送問題の解を得る． \n\n\n\n\n\n3/13/2024\n\n\n司馬 博文\n\n\nOT1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\nState vs Loomis 判決を題材に，アルゴリズムと公平性を議論する．\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理___.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n転移学習とは\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\nTheory4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬博文\n\n\nGNN.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．\n\n\n\n\n\n3/03/2024\n\n\n司馬 博文\n\n\nTheory3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\nPAC-Bayes は現実的に有用な鋭い PAC bound を得る新たな技術である．最適化の問題に帰着する点が研究を盛り上げている．Vapnik-Chervonenkis 理論の一般化であり，推定量上の確率分布を返すようなより一般的なアルゴリズムに対しても適用できる．\n\n\n\n\n\n3/02/2024\n\n\n司馬 博文\n\n\nTheory2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\nSemiconductor.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\nPF.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，「刑法入門」の内容を扱う．\n\n\n\n\n\n2/21/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理7.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\nDeep2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\nDeep4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n深層学習の学習における確率最適化アルゴリズムに関して概説する．\n\n\n\n\n\n2/16/2024\n\n\n司馬 博文\n\n\nOptimization.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\nNF1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\nDiffusion.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\nNF.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n数学者のために，深層生成モデルを概観する．\n\n\n\n\n\n2/13/2024\n\n\n司馬 博文\n\n\nBAI1_Dropout.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\nVI3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nGP.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nGP2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nDeep.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nDeep3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\nVI2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n法律家のための統計数理6.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\nRL2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する．\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\nRL.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次稿 で，\\(K\\)-平均アルゴリズムの model-aware な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す． \n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\nVI.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\nPureJump.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/29/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理_.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n法律家のための統計数理5.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n粒子フィルターを拡散過程に対して適用することを考える．拡散過程の Euler-Maruyama 離散化に対して構成された粒子フィルターの，タイムステップを \\(0\\) にする極限 \\(\\Delta\\searrow0\\) での振る舞いを議論する．\n\n\n\n\n\n1/23/2024\n\n\n司馬 博文\n\n\nContinuousLimit.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\n\n\n\nProcess\n\n\n\nマルチンゲール問題とは何か？\n\n\n\n\n\n1/20/2024\n\n\nDraft Draft\n\n\nMartingaleProblem.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\nPGM2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n粒子フィルターは，リサンプリングを取り入れた逐次重点サンプリングと見れる．リサンプリングにより荷重の退化を防げるが本質的な問題は回避できないことが多い．本稿では，リサンプリングのアルゴリズムを複数紹介し比較する．\n\n\n\n\n\n1/14/2024\n\n\n司馬 博文\n\n\nresampling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n法律家のための統計数理4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\nTheory.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\n\n\n\nProcess\n\n\n\n確率過程の離散化に関する漸近論的な結果を，Brown 運動を例に取り示す．\n\n\n\n\n\n1/09/2024\n\n\nDraft Draft\n\n\nDiscretization.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\nMinkowskiSum.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\nRadonMeasures.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，\n\n\n\n\n\n1/02/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理__.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\nBranchingProcesses.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\nVSCode での LaTeX 環境構築に関するページ．\n\n\n\n\n\n12/22/2023\n\n\n司馬博文\n\n\nLaTeXwithVSCode.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n法律家のための統計数理3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\nPGM1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nMCMC\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\nSMCSamplers.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\nParticleFilter.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n法律家のための統計数理2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\nBayesianComp.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\nMentalHealth.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n条件付き期待値の問題.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\nBoundedMeasure.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\nBookRecommendation.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\nParticleFilter.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\nBeta-Gamma.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n書き起こし.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n法律家のための統計数理1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n独立性.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\n\n\n\nKernel\n\n\n\n数学者のために，SVMによるデータ解析が何をやっているのかを抽象的に説明する．\n\n\n\n\n\n11/18/2023\n\n\nDraft Draft\n\n\nSVM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n条件付き正規分布からのシミュレーション法.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\nMarkovCategory.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\nDelMoral2013.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\nFeynman-Kac モデルという物理モデルを定義し，逐次モンテカルロ法（粒子フィルター）をその Monte Carlo シミュレーション法として位置付けて解説した書籍である． 例として挙げられるトピックも物理学のものが多く，書籍のスタイルも物理学書のそれである． ここでは 1.1 節 “On the Origins of Feynman-Kac and Particle Models” の抄訳を通じて内容を概観したい． \n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\nDelMoral2004.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\nKernelMethods4Mathematicians.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\nSSM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\nChangedMyLife.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．公式の ギャラリー も参照．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\nQuartoBasics.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia のSymbol型とExpr型，そしてExpr型からExpr型への関数であるマクロを用いたメタプログラミングについて解説する．\n\n\n\n\n\n1/23/2022\n\n\n司馬 博文\n\n\nJulia6.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPython の import について\n\n\n\n\n\n\nPython\n\n\n\nPython の import について\n\n\n\n\n\n5/23/2021\n\n\n司馬 博文\n\n\nPython-import.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\nR0.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\nR1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\nR3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\nR2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\nR5.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\nR4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia でパッケージを作成する際の基礎知識をまとめる．\n\n\n\n\n\n9/10/2020\n\n\n司馬 博文\n\n\nJulia5.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は階層関係を明示的に宣言する必要のある名前的型付け言語であり，既存の型から自由な構成が可能なパラメトリック型付け言語である．\n\n\n\n\n\n9/09/2020\n\n\n司馬 博文\n\n\nJulia4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia において，関数はメソッドの貼り合わせである．スクリプト言語のように，気軽に関数定義を行うこともできれば（単一メソッドによる関数と解す），多重ディスパッチによる実装も可能である．\n\n\n\n\n\n9/08/2020\n\n\n司馬 博文\n\n\nJulia3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は 2012 年に公開された科学計算向きの動的型付け言語である．\n\n\n\n\n\n9/07/2020\n\n\n司馬 博文\n\n\nJulia2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は動的型付け言語で，型宣言 :: を省略すると全てのオブジェクトとマッチする Any 型と解釈される．一方で静的型付け言語のような豊かな型システムも持つ．これにより関数をメソッドのディスパッチにより実装するのが Julia の根幹思想である．メソッドのディスパッチについては次稿に譲り，ここでは基本的なデータ型について述べる．\n\n\n\n\n\n9/06/2020\n\n\n司馬 博文\n\n\nJulia1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\nJulia0.qmd\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html",
    "href": "static/Sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      5/24/2024.\n      司馬博文 .\n      \n        新時代の MCMC を迎えるために\n        : 連続時間アルゴリズムへの進化\n      .\n      \n        統数研オープンハウス.\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n    \n      2/25/2024.\n      Hirofumi Shiba.\n      \n        A Recent Development of Particle Methods\n        : Inquiry towards a Continuous Time Limit and Scalability\n      .\n      \n        MLSS2024 (OIST, Okinawa, Japan).\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#upcomings-newests",
    "href": "static/Sessions.html#upcomings-newests",
    "title": "Sessions",
    "section": "",
    "text": "連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#presentation-history",
    "href": "static/Sessions.html#presentation-history",
    "title": "Sessions",
    "section": "",
    "text": "5/24/2024.\n      司馬博文 .\n      \n        新時代の MCMC を迎えるために\n        : 連続時間アルゴリズムへの進化\n      .\n      \n        統数研オープンハウス.\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n    \n      2/25/2024.\n      Hirofumi Shiba.\n      \n        A Recent Development of Particle Methods\n        : Inquiry towards a Continuous Time Limit and Scalability\n      .\n      \n        MLSS2024 (OIST, Okinawa, Japan).\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#sec-法律家のための統計数理",
    "href": "static/Sessions.html#sec-法律家のための統計数理",
    "title": "Sessions",
    "section": "法律家のための統計数理",
    "text": "法律家のための統計数理\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n数学と法学，双方からの交流と理解を図ります．\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#empirical-process-theory",
    "href": "static/Sessions.html#empirical-process-theory",
    "title": "Sessions",
    "section": "Empirical Process Theory",
    "text": "Empirical Process Theory\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\nTextbook：Kengo Kato Empirical Process Theory (Lecture Note)\n\n\n\n担当分の発表資料"
  },
  {
    "objectID": "static/Sessions.html#学振-dc1",
    "href": "static/Sessions.html#学振-dc1",
    "title": "Sessions",
    "section": "学振 DC1",
    "text": "学振 DC1\n\n\n\n\n\n\n\n\n\n\nPeriod\nApplication Category\nSmall Category\n結果\n\n\n\n\nSpring, 2024\n解析学、応用数学およびその関連分野\n12040 応用数学および統計数学関連\n不採択 AT スコア 2.639（上位３割）\n\n\n\n\n本書類審査セットにおける 2024 年度の採択率は 11.6% でした．\n\n\n\n申請書（最終版，５月19日）\n\n\n\n\n\n参考：申請書（バージョン１，４月３日）\n\n\n\n\n評点結果\n\n\n\n\n\n\n\n着想およびオリジナリティ\n研究者としての資質\n総合評価\n\n\n\n\n3.50\n3.17\n3.17"
  },
  {
    "objectID": "static/PartialCategories.html#probability",
    "href": "static/PartialCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#process",
    "href": "static/PartialCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#functional-analysis",
    "href": "static/PartialCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\nFunctional Analysis\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#geometry",
    "href": "static/PartialCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mathcalpx",
    "href": "static/PartialCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#nature",
    "href": "static/PartialCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mcmc",
    "href": "static/PartialCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\n総研大５年一貫博士課程・中間評価\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#foundation",
    "href": "static/PartialCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#information",
    "href": "static/PartialCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#statistics",
    "href": "static/PartialCategories.html#statistics",
    "title": "Categories",
    "section": "2.5 Statistics",
    "text": "2.5 Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#computation-2",
    "href": "static/PartialCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#sampling",
    "href": "static/PartialCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#python",
    "href": "static/PartialCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#julia",
    "href": "static/PartialCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\n総研大５年一貫博士課程・中間評価\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#yuima",
    "href": "static/PartialCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#stan",
    "href": "static/PartialCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#r",
    "href": "static/PartialCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#bayesian",
    "href": "static/PartialCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#particles",
    "href": "static/PartialCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#kernels",
    "href": "static/PartialCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#ai",
    "href": "static/PartialCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#deep-learning",
    "href": "static/PartialCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#posters",
    "href": "static/PartialCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#reviews",
    "href": "static/PartialCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#surveys",
    "href": "static/PartialCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#slides",
    "href": "static/PartialCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\n総研大５年一貫博士課程・中間評価\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズとは何か\n\n\n数学による統一的アプローチ\n\n\n\nSlide\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nSlide\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#papers",
    "href": "static/PartialCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/PartialCategories.html#opinion",
    "href": "static/PartialCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#life",
    "href": "static/PartialCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n2024-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#lifestyle",
    "href": "static/PartialCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#法律家のための統計数理",
    "href": "static/PartialCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#俺のための-julia-入門",
    "href": "static/PartialCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Japanese.html",
    "href": "static/Japanese.html",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "私，司馬博文は総合研究大学院大学５年一貫博士課程（統計科学コース）２年で，ベイズ計算を専門に研究しています．\nベイズ推論は統計学・逆問題・システム制御・機械学習・認知科学などの分野において，確率論を応用するための指導原理を与える考え方です．\nそこで私はベイズ推論において目的に依存せずに使える汎用アルゴリズムを開発することで，確率論の応用を広げることを目指しています．\nPDMPFlux.jl や YUIMA などのパッケージ開発，ヘルスケア・ものづくり企業へのデータ解析コンサルティングもフリーランスで行っています．\n以上の研究・社会実装活動はすべて，疫学や政治科学，惑星地球科学などの領域で「世界をよりよく知るために，計算機をどう使えるか？」という共通の興味に支えられています．\n\n\n\n\n\n\n\n統計数理研究所 鎌谷研吾 先生と 矢野恵佑 先生の下で，モンテカルロ法を研究しています．特にマルコフ連鎖モンテカルロ法 (MCMC) や逐次モンテカルロ法 (SMC) など，ベイズ推論を実現するアルゴリズムを専門としています．\n確率過程の収束や，確率測度の空間 \\(\\mathcal{P}(E)\\) の幾何を議論することで，アルゴリズムの挙動を分析する数理的な枠組みの構築を目指しています．\nまた連続時間 MCMC を用いた事後分布サンプリングのための Julia パッケージ PDMPFlux.jl や，確率過程の統計推測のための R パッケージ YUIMA の開発にも取り組んでおり，モンテカルロ法とベイズ統計の応用に広く取り組んでいます．\n\n\n\n\n\n\nモンテカルロ計算 MCMC，SMC，PDMP などのシミュレーション技術．\n統計モデリング 政治学・疫学・惑星地球科学などへの応用．\nベイズ機械学習 カーネル法・ノンパラメトリクス．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024.9 – 現在．株式会社プリメディカ\n\n\n\n2023.4 – 現在．東京大学先端科学技術研究センター\n\n\n\n\n\n\n\n\n\n総合研究大学院大学．指導教員：鎌谷研吾，矢野恵佑．\n\n\n\n東京大学理学部数学科．指導教員：吉田朋広．"
  },
  {
    "objectID": "static/Japanese.html#研究",
    "href": "static/Japanese.html#研究",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "統計数理研究所 鎌谷研吾 先生と 矢野恵佑 先生の下で，モンテカルロ法を研究しています．特にマルコフ連鎖モンテカルロ法 (MCMC) や逐次モンテカルロ法 (SMC) など，ベイズ推論を実現するアルゴリズムを専門としています．\n確率過程の収束や，確率測度の空間 \\(\\mathcal{P}(E)\\) の幾何を議論することで，アルゴリズムの挙動を分析する数理的な枠組みの構築を目指しています．\nまた連続時間 MCMC を用いた事後分布サンプリングのための Julia パッケージ PDMPFlux.jl や，確率過程の統計推測のための R パッケージ YUIMA の開発にも取り組んでおり，モンテカルロ法とベイズ統計の応用に広く取り組んでいます．"
  },
  {
    "objectID": "static/Japanese.html#キーワード",
    "href": "static/Japanese.html#キーワード",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "モンテカルロ計算 MCMC，SMC，PDMP などのシミュレーション技術．\n統計モデリング 政治学・疫学・惑星地球科学などへの応用．\nベイズ機械学習 カーネル法・ノンパラメトリクス．"
  },
  {
    "objectID": "static/Japanese.html#経歴",
    "href": "static/Japanese.html#経歴",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "2024.9 – 現在．株式会社プリメディカ\n\n\n\n2023.4 – 現在．東京大学先端科学技術研究センター"
  },
  {
    "objectID": "static/Japanese.html#学位",
    "href": "static/Japanese.html#学位",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学．指導教員：鎌谷研吾，矢野恵佑．\n\n\n\n東京大学理学部数学科．指導教員：吉田朋広．"
  },
  {
    "objectID": "static/CV/cv.html#profile-and-skills",
    "href": "static/CV/cv.html#profile-and-skills",
    "title": "Hirofumi Shiba",
    "section": "Profile and Skills",
    "text": "Profile and Skills\nHirofumi is currently a Ph.D. candidate at the Institute of Statistical Mathematics, working under the supervision of Prof. Kengo Kamatani and Keisuke Yano.\nHe holds Japanese citizenship and is a native speaker of Japanese and Chinese. Additionally, He is fluent in English.\nHirofumi codes in Julia, Python, and R."
  },
  {
    "objectID": "static/CV/cv.html#research-interests",
    "href": "static/CV/cv.html#research-interests",
    "title": "Hirofumi Shiba",
    "section": "Research Interests",
    "text": "Research Interests\n\nMonte Carlo methods, including Sequential Monte Carlo and Markov Chain Monte Carlo.\nTransport methods, including Schrödinger Bridges and Normalizing Flows.\nBayesian modeling, including Political Science and Bioinformatics.\nBayesian machine learning, including Gaussian processes and hierarchical modeling."
  },
  {
    "objectID": "static/CV/cv.html#work-experience",
    "href": "static/CV/cv.html#work-experience",
    "title": "Hirofumi Shiba",
    "section": "Work Experience",
    "text": "Work Experience\n\nConsultant. PreMedica, Inc., Tokyo, Japan. 2024.9 – today\nProvided Bayesian data analysis solutions for clients in the healthcare industry.\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – today\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference.\nCooperative Researcher. RCAST, the University of Tokyo, Japan. 2023.4 – today\nResearch on trustworthy AI and machine learning from the viewpoint of economic security.\nData Scientist. IMIS Co., Ltd. 2022.8 – 2024.1\nProvided statistical analysis and machine learning solutions for clients in the manufacturing industry."
  },
  {
    "objectID": "static/CV/cv.html#education",
    "href": "static/CV/cv.html#education",
    "title": "Hirofumi Shiba",
    "section": "Education",
    "text": "Education\n\nPh.D. in Statistical Science. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2023.4 – 2028.3\nSuperivsor: Kengo Kamatani and Keisuke Yano\nB.A. in Mathematics. The University of Tokyo, Japan. 2019.4 – 2023.3\nSupervisor: Nakahiro Yoshida"
  },
  {
    "objectID": "static/CV/cv.html#research-stay",
    "href": "static/CV/cv.html#research-stay",
    "title": "Hirofumi Shiba",
    "section": "Research Stay",
    "text": "Research Stay\n\nUniversity College London，United Kingdom．2024.11.4 – 2024.12.2\nSupervisor: Alexandros Beskos"
  },
  {
    "objectID": "static/AllCategories.html#probability",
    "href": "static/AllCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#process",
    "href": "static/AllCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程に駆動される SDE のエルゴード性\n\n\nカップリング法／最適輸送距離による証明\n\n\n\nProcess\n\n\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\nProcess\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\nProcess\n\n\n\n\n2024-01-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#functional-analysis",
    "href": "static/AllCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\nFunctional Analysis\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#geometry",
    "href": "static/AllCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mathcalpx",
    "href": "static/AllCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#nature",
    "href": "static/AllCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mcmc",
    "href": "static/AllCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#foundation",
    "href": "static/AllCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n\n2024-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\n\n2024-03-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#information",
    "href": "static/AllCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#statistics",
    "href": "static/AllCategories.html#statistics",
    "title": "Categories",
    "section": "2.5 Statistics",
    "text": "2.5 Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#computation-2",
    "href": "static/AllCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#sampling",
    "href": "static/AllCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#python",
    "href": "static/AllCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nPython の import について\n\n\n\nPython\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#julia",
    "href": "static/AllCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-10-26\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#yuima",
    "href": "static/AllCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#stan",
    "href": "static/AllCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#r",
    "href": "static/AllCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#bayesian",
    "href": "static/AllCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析とモデル平均\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan と CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\nrstan による Stan の利用\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#particles",
    "href": "static/AllCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-10-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#kernels",
    "href": "static/AllCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#ai",
    "href": "static/AllCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#deep-learning",
    "href": "static/AllCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#posters",
    "href": "static/AllCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#reviews",
    "href": "static/AllCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-05-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#surveys",
    "href": "static/AllCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#slides",
    "href": "static/AllCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのモデル選択への応用\n\n\nReversible Jump Zig-Zag Sampler\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズとは何か\n\n\n数学による統一的アプローチ\n\n\n\nSlide\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nSlide\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#papers",
    "href": "static/AllCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/AllCategories.html#opinion",
    "href": "static/AllCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\nOpinion\n\n\n\n\n2024-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#life",
    "href": "static/AllCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n2024-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n生い立ち\n\n\n\nLife\n\n\n\n\n2024-01-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#lifestyle",
    "href": "static/AllCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#法律家のための統計数理",
    "href": "static/AllCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#俺のための-julia-入門",
    "href": "static/AllCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html",
    "href": "posts/2024/Process/PureJump1.html",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "",
    "text": "YUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#関連記事",
    "href": "posts/2024/Process/PureJump1.html#関連記事",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "",
    "text": "YUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#はじめに",
    "href": "posts/2024/Process/PureJump1.html#はじめに",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "1 はじめに",
    "text": "1 はじめに\n本稿では Lévy 過程 \\(\\{Z_t\\}\\) に駆動された SDE \\[\ndX_t=a(X_t)\\,dt+b(X_{t-})\\,dZ_t=a(X_t)\\,dt+\\sigma(X_t)\\,dW_t+\\int_{\\left\\{\\lvert u\\rvert\\le 1\\right\\}}c(X_{t-},u)\\,\\widetilde{N}(dtdu)+\\int_{\\left\\{\\lvert u\\rvert&gt;1\\right\\}}c(X_{t-},u)\\,N(dtdu)\n\\tag{1}\\] で駆動される確率過程 \\(\\{X_t\\}\\) のエルゴード性を議論する．\nLévy 過程 \\(\\{Z_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^m)\\) は拡散項を持たない \\(\\sigma\\equiv0\\) とし，跳躍係数 \\(c:\\mathbb{R}^m\\times\\mathbb{R}^m\\to M_m(\\mathbb{R})\\) も跳躍幅 \\(u\\) に関して線型 \\(c(x,u)=b(x)u\\) で，次の Ito-Lévy 分解を持つとする： \\[\nZ_t=\\int^t_0\\int_{\\lvert u\\rvert\\le1}u\\widetilde{N}(ds,du)+\\int^t_0\\int_{\\lvert u\\rvert&gt;1}uN(ds,du),\n\\] \\[\nN(ds,du):=ds\\,\\nu(du),\\qquad\\widetilde{N}(ds,du):=N(ds,du)-ds\\,\\nu(du),\\qquad\\int_{\\mathbb{R}^m}\\lvert u\\rvert^2\\land1\\nu(du)&lt;\\infty.\n\\] ただし \\(N\\) を強度測度 \\(ds\\,\\nu(du)\\) を持つ跳躍を表す Poisson 点過程とした．\nBorel 可測関数 \\(b:\\mathbb{R}^m\\to M_{m}(\\mathbb{R})\\) と \\(a:\\mathbb{R}^m\\to\\mathbb{R}^m\\) は局所 Lipschitz 連続で，線型増大条件 \\[\n\\lvert a(x)\\rvert^2+\\int_{\\lvert u\\rvert\\le1}\\lvert b(x)u\\rvert^2\\nu(du)\\le K(1+\\lvert x\\rvert^2),\\qquad x\\in\\mathbb{R}^m,\n\\] を満たすとする．このとき，SDE (1) には一意な強解 \\(\\{X_t\\}\\) が存在し，\\(X\\) は càdlàg な Markov 過程である．1\n加えて，\\(b\\) が有界であるという条件も引き続き課すこととする．\n伊藤の公式より，拡張生成作用素 \\[\n\\widehat{L}f(x):=\\left(Df(x)\\,|\\,a(x)\\right)+\\int_{\\mathbb{R}^m}\\biggr(f\\biggr(x+b(x)u\\biggl)-f(x)-1_{B^m}\\biggr((Df(x)|b(x)u)\\biggl)\\biggl)\\nu(du),\\qquad f\\in C^2(\\mathbb{R}^m),\n\\tag{2}\\] に関して \\(M_t^f:=f(X_t)-f(x)-\\int^t_0\\widehat{L}f(X_s)ds\\) で定まる càdlàg 過程 \\(\\{M^f_t\\}\\) は任意の \\(x\\in\\mathbb{R}^m\\) に関して局所 \\(\\operatorname{P}_x\\)-マルチンゲールである．\n拡散過程，例えば Langevin 動力学のエルゴード性証明\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nとの最大の違いは，ドリフト関数 \\(V\\in C^2(\\mathbb{R}^m)\\) に Lévy 測度 \\(\\nu\\) に関する可積分条件が加わることにある．そもそも \\(\\widehat{L}V\\) が well-defined であるためには，式 (2) の積分が発散してはならないのである．\nこのように \\(\\widehat{L}f(x)\\) の値が \\(f\\) の \\(x\\in\\mathbb{R}^m\\) 以外での値にも依存する性質を 非局所性 という．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#文献紹介",
    "href": "posts/2024/Process/PureJump1.html#文献紹介",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "2 文献紹介",
    "text": "2 文献紹介\n\nLévy 過程のエルゴード性の結果については，(3.4節 Kulik, 2018) によくまとまっている．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#footnotes",
    "href": "posts/2024/Process/PureJump1.html#footnotes",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ikeda and Watanabe, 1981, p. 245) 定理9.1．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html",
    "href": "posts/2024/Process/Discretization.html",
    "title": "確率過程の離散化",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n(Jacod and Protter, 2012) 第１章参考．\n参照過程は Brown 運動 \\((W_t)_{t\\in\\mathbb{R}_+}\\) のスケーリング \\[\nX=\\sigma W,\\quad(\\sigma&gt;0)\n\\] であるとする．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "1 正規化汎函数 \\(V^{'n}(f,X)\\)",
    "text": "1 正規化汎函数 \\(V^{'n}(f,X)\\)\n\n1.1 \\(t\\in\\mathbb{R}_+\\) 毎の収束\n\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は独立同分布であるが，正規化を施したことにより， \\[\n\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}=\\frac{X_{i\\Delta_n}-X_{(i-1)\\Delta_n}}{\\sqrt{\\Delta_n}}\\sim\\mathrm{N}(0,c)\n\\] も離散化の段階 \\(n=0,1,\\cdots\\) に依らず独立同分布である．よって， \\[\nf\\left(\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}\\right)\\sim(\\rho_c(f),\\rho_c(f^2)-\\rho_c(f)^2)\n\\] を踏まえて，独立同分布列に対する０次と１次の漸近定理から \\[\nV^{'n}(f,X)_t\\overset{\\text{p}}{\\to}t\\rho_c(f)\n\\] \\[\n\\frac{V^{'n}(f,X)_t-t\\rho_c(f)}{\\sqrt{\\Delta_n}}\\overset{\\text{d}}{\\to}\\mathrm{N}\\biggr(0,t(\\rho_c(f^2)-\\rho_c(f)^2)\\biggl)\n\\] が言えそうである．\n\n０次の漸近論で概収束は示せない．\n\n\n\n1.2 \\(\\mathbb{R}_+\\) 上の過程としての収束\n\\(\\mathbb{R}_+\\) で添字付けられた過程として，\\(D(\\mathbb{R}_+)\\) 上の Skorohod 位相について確率収束する．すなわち，任意の \\(t\\in[0,T]\\) に対して， \\[\n\\sup_{s\\le t}\\lvert Z^n_s-Z_s\\rvert\\overset{\\text{p}}{\\to}0.\n\\] 加えて，汎函数中心極限定理から， \\[\n\\left(\\frac{1}{\\sqrt{\\Delta_n}}(V^{'n}(f,X)_t-t\\rho_c(f))\\right)_{t\\ge0}\\overset{\\text{d}}{\\to}\\sqrt{\\rho_c(f^2)-\\rho_c(f)^2}B.\n\\] が Skorohod 位相に関して成り立つ．これはさらに安定収束もするのである．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "2 非正規化汎函数 \\(V^n(f,X)\\)",
    "text": "2 非正規化汎函数 \\(V^n(f,X)\\)\n正規化を施さないために，\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は \\(0\\) に漸近していき，関数 \\(f\\) の \\(0\\) での局所的な振る舞いが収束に影響を与えるようになる．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html",
    "href": "posts/2024/Process/Levy.html",
    "title": "Lévy 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nYUIMAについては次の記事も参照：\nPoisson 過程と複合 Poisson 過程については次の記事を参照："
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "href": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "title": "Lévy 過程を見てみよう",
    "section": "1 Lévy-Itô 分解",
    "text": "1 Lévy-Itô 分解\n\n1.1 加法過程の定義\n\n\n\n\n\n\n定義 (additive process, Lévy process)1\n\n\n\n確率過程 \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 加法過程 であるとは，最初の４条件を満たすことをいう．５条件全てを満たすとき，Lévy 過程 であるという．\n\n\\(X_0\\overset{\\text{a.s.}}{=}0\\)．\nある充満集合 \\(\\Omega_0\\subset\\Omega\\) が存在し，\\(t\\mapsto X_t(\\omega)\\) は càdlàg である．\n独立増分：任意の \\(0\\le t_0&lt;\\cdots&lt;t_n\\) について， \\[\n  X_{t_0},X_{t_1}-X_{t_0},X_{t_2}-X_{t_1},\\cdots,X_{t_n}-X_{t_{n-1}}\n  \\] は独立である．\n確率連続：任意の \\(\\epsilon&gt;0\\) と \\(t\\ge0\\) について， \\[\n  \\lim_{s\\to t}\\operatorname{P}[\\lvert X_s-X_t\\rvert&gt;\\epsilon]=0\n  \\] が成り立つ．\n定常増分：\\(X_{s+t}-X_s\\) の分布は \\(s\\ge0\\) に依らない．\n\n\n\n\n\n1.2 特性量\n加法過程 \\(\\{X_t\\}\\) について，\\(X_t\\) の分布は必ず \\(\\mathbb{R}^d\\) 上の無限可分分布になる．2\n加えて，加法過程の分布は１次元の有限次元分布族が特徴付ける．\n\n\n\n\n\n\n定理（加法過程の分布）3\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を加法過程とする．\n\n\\(\\mu_{s,t}\\) を \\(X_t-X_s\\) の分布とすると，これは無限可分であり，次を満たす： \\[\n\\mu_{s,t}*\\mu_{t,u}=\\mu_{s,u},\n\\] \\[\n\\mu_{s,s}=\\delta_0,\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\quad(s\\nearrow t),\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\qquad(t\\searrow s).\n\\]\n確率分布の族 \\(\\{\\mu_{s,t}\\}\\subset\\mathcal{P}(\\mathbb{R}^d)\\) が上の４式を満たすならば，これはある加法過程 \\(\\{X_t\\}\\) が定める分布である．\n２つの加法過程 \\(X,X'\\) が， \\[\nX_t\\overset{\\text{d}}{=}X'_t\\quad t\\in\\mathbb{R}_+\n\\] を満たすならば，\\(X\\overset{\\text{d}}{=}X'\\) が成り立つ．\n\n\n\nこのことにより，加法過程 \\(X\\) の分布は，各 \\(X_t\\) の無限可分分布を特徴付ける特性量 \\((A_t,\\nu_t,\\gamma(t))\\) によって特徴付けられる．\n\n\n\n\n\n\n無限可分分布の特徴付け (Khinchin and Lévy, 1936)4\n\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) について，次は同値：\n\n\\(f\\) は無限可分である．\n(Lévy) ある \\[\n  \\nu(\\{0\\})=0,\\qquad\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n  \\] を満たす測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}^d)\\) と対称な半正定値行列 \\(A\\in S_n(\\mathbb{R})_+\\) と \\(\\gamma\\in\\mathbb{R}^d\\) が一意的に存在して，次のように表せる： \\[\n  f(z)=\\exp\\left(-\\frac{1}{2}(z|Az)+i(\\gamma|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\\biggl)\\nu(dx)\\right).\n  \\]\n(Khinchin) ある有限測度 \\(\\Psi\\in\\mathcal{M}^1(\\mathbb{R}^d)\\) と \\(\\alpha\\in\\mathbb{R}^d\\) が存在して，次のように表せる： \\[\nf(u)=\\exp\\left(i(\\alpha|u)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|x)}-1-\\frac{i(u|x)}{1+\\lvert x\\rvert^2}\\biggl)\\frac{1+\\lvert x\\rvert^2}{\\lvert x\\rvert^2}\\Psi(dx)\\right).\n\\]\n\n\\(A\\) を Gauss 共分散，\\(\\nu\\) を Lévy 測度，\\(\\Psi\\) を Khintchine測度 という．5\n加えて，任意の半正定値行列 \\(A\\)，\\(\\gamma\\in\\mathbb{R}^d\\)，Lévy 測度 \\(\\nu\\) であって \\(\\nu(\\{0\\})=0\\) かつ \\[\n\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n\\] を満たすものに対して，\\((A,\\nu,\\gamma)\\) を特性量にもつ無限可分分布が存在する．\nKhintchin の表示はより直接的である上に，Khintchin 測度は有限になる．さらに，\\((\\alpha,\\Psi)\\) の収束が過程の収束にも対応する！6 だが，確率論的な意味付けに欠けるために，Lévy の表示の方をここでは用いる．\nLévy の表示の被積分関数 \\[\ne^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\n\\] は大変複雑であるが，こうしないと \\(\\nu\\)-可積分にならないのである．\n\\(\\nu\\) は \\(O(\\lvert x\\rvert^2)\\) 関数に関してならば \\(0\\) の近傍でも可積分であるから，\\(e^{i(z|x)}\\) から１次以下の項を \\(0\\) の近傍から取り去ることで可積分にしているのである．そのため，最後の項は \\(1_{\\left\\{B^d\\right\\}}\\) でなくとも， \\[\nc(x)=1+o(\\lvert x\\rvert)\\quad(\\lvert x\\rvert\\to0)\n\\] \\[\nc(x)=O(\\lvert x\\rvert^{-1})\\quad(\\lvert x\\rvert\\to\\infty)\n\\] の２条件を満たすものならばなんでも良い．だが，取り替える度に１次の項 \\(\\gamma\\in\\mathbb{R}^d\\) を変更する必要がある．\n一般に \\(\\gamma\\) はドリフトと呼んではいけない． \\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] を満たす場合のみ， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_0|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right)\n\\] と表示でき，この際の \\(\\gamma_0\\in\\mathbb{R}^d\\) を ドリフト と呼ぶ．\n逆に， \\[\n\\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] が成り立つとき， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_1|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)\\biggl)\\nu(dx)\\right)\n\\] と表示でき，\\(\\gamma_1\\) は \\(f\\) が定める確率分布の平均に一致する．7\n\n\n\nLévy 過程は，\\(A:=A_1,\\nu:=\\nu_1,\\gamma:=\\gamma(1)\\) について， \\[\nA_t=tA,\\quad\\nu_t=t\\nu,\\quad\\gamma_t=t\\gamma\n\\] と表せる場合に当たる．\n\n\n1.3 強度測度との関係\n\\(\\{(A_t,\\nu_t,\\gamma_t)\\}_{t\\in\\mathbb{R}_+}\\) を加法過程の特性量とする．\nこのとき， \\[\n\\widetilde{\\nu}([0,t]\\times B):=\\nu_t(B),\\qquad t\\ge0,B\\in\\mathcal{B}(\\mathbb{R}^d)\n\\] は \\(\\mathbb{R}_+\\times\\mathbb{R}^d\\) 上に測度を定める．\n\n\n\n\n\n\n命題（強度測度と特性測度の関係）8\n\n\n\n測度の族 \\(\\{\\nu_t\\}\\subset\\mathcal{M}(\\mathbb{R}^d)\\) と測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}_+\\times\\mathbb{R}^d)\\) について，次は同値：\n\n\\(\\widetilde{\\nu}\\) は次の２条件を満たす： \\[\n\\widetilde{\\nu}(\\{t\\}\\times\\mathbb{R}^d)=0,\n\\] \\[\n\\int_{[0,t]\\times\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\widetilde{\\nu}(dsdx)&lt;\\infty.\n\\]\n\\(\\{\\nu_t\\}\\) はある加法過程の特性測度である．\n\n\n\nよって，任意の加法過程について， \\[\n\\int_{\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\nu_t(dx)&lt;\\infty\n\\] が必要である．\nLévy 過程であるとき，定常増分であることが必要であるため，跳躍時刻は \\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程に従う必要がある．これより， \\[\n\\widetilde{\\nu}=\\ell_+\\otimes\\nu\n\\] と分解できる必要があり，この特性測度 \\(\\nu\\) が Lévy 測度である．このとき，\\(\\nu_t=t\\nu\\) かつ \\(\\widetilde{\\nu}(dsdx)=ds\\nu(dx)\\)．\n\n\n1.4 一般の分解\n\n\n\n\n\n\n定理 (Ito, 1941)9\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を特性量 \\(\\{(A_t,\\nu_t,\\gamma(t))\\}\\) を持つ加法過程とする． \\[\n\\eta(\\omega,B):=\\#\\left\\{t\\in\\mathbb{R}_+\\,\\middle|\\,\\begin{pmatrix}t\\\\X_t(\\omega)-X_{t-}(\\omega)\\end{pmatrix}\\in B\\right\\}\n\\] を \\(\\omega\\in\\Omega_0\\) 上で \\(B\\in\\mathcal{B}(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\})\\) に関して定める．\n\n\\(\\eta\\) は \\(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\}\\) 上の強度測度 \\(\\widetilde{\\nu}\\) を持った Poisson 点過程である．\nある充満集合 \\(\\Omega_1\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\\begin{align*}\n     X^1_t(\\omega)&:=\\lim_{\\epsilon\\searrow0}\\int_0^t\\int_{\\left\\{\\epsilon&lt;\\lvert x\\rvert\\le 1\\right\\}}x\\,\\widetilde{\\nu}(\\omega,dsdx)\\\\\n     &\\qquad+\\int_0^t\\int_{\\mathbb{R}^d\\setminus B^d}x\\,\\eta(\\omega,dsdx)\n   \\end{align*}\\] 収束は \\(t\\in\\mathbb{R}_+\\) に関して広義一様であり，\\(X^1\\) は特性量 \\(\\{(0,\\nu_t,0)\\}_{t\\in\\mathbb{R}_+}\\) が定める加法過程である．\n\\(X^2_t:=X_t-X_t^1\\) は殆ど確実に連続な見本道を持ち，特性量 \\(\\{(A_t,0,\\gamma(t))\\}\\) が定める加法過程である．\n\\(X^1\\perp\\!\\!\\!\\perp X^2\\) が成り立つ．\n\n\n\n\n\n1.5 B 型の場合\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] を満たす場合，Poisson 補過程によらない，より簡潔な表示を持つ．\n\n\n\n\n\n\n定理\n\n\n\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] が成り立つ場合，次が成り立つ：\n\nある充満集合 \\(\\Omega_3\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\n   X^3_t(\\omega):=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}x\\,\\eta(\\omega,dsdx).\n   \\] このとき，\\(X_t^3\\) の分布は複合 Poisson である： \\[\n   \\operatorname{E}[e^{i(z|X_t^3)}]=\\exp\\left(\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu_t(dx)\\right).\n   \\]\n\\(X^4_t:=X_t-X_t^3\\) は殆ど確実に連続な見本道を持ち，Gauss 過程を定める： \\[\n   \\operatorname{E}[e^{i(z|X_t^4)}]=\\exp\\left(-\\frac{1}{2}(z|A_tz)+i(\\gamma_0(t)|z)\\right).\n   \\]\n\\(X^3\\perp\\!\\!\\!\\perp X^4\\) が成り立つ．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-測度",
    "href": "posts/2024/Process/Levy.html#lévy-測度",
    "title": "Lévy 過程を見てみよう",
    "section": "2 Lévy 測度",
    "text": "2 Lévy 測度\n\n2.1 はじめに\n本節の目的は，Lévy 過程の次の３分類の見本道の違いを理解することである：10\n\n\n\n\n\n\n特性量 \\((A,\\nu,\\gamma)\\) を持つ Lévy 過程について，\n\nA 型：\\(A=0\\) かつ \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\nB 型：\\(A=0\\) かつ \\(\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\\) であるが，A 型ではない．\nC 型：それ以外．\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) 型は拡散項を持たず，確定的な動きと複合 Poisson 過程の和で表現される．ジャンプは離散的に起こる．\n\\(B\\) 型も拡散項を持たないが，\\(\\mathbb{R}_+\\) 上稠密な可算集合上でジャンプを繰り返す．Gamma 過程（第 3.6 節）がその例である．\n\\(A,B\\) は殆ど確実に任意の有界区間上で有界変動な見本道を持つが，\\(C\\) 型は有界変動ではない．11 Brown 運動と Cauchy 過程（第 4.4 節）がその例である．\n\n\n\n\n\n\n2.2 Lévy 測度が零ならば，Gauss 過程である\n\n\n\n\n\n\n命題（連続な Lévy 過程の特徴付け）12\n\n\n\nLévy 過程 \\(X\\) について，次の２条件は同値：\n\n\\(X\\) は殆ど確実に連続な見本道を持つ．\n\\(\\nu=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n時刻 \\(t&gt;0\\) までの跳躍回数を表す Poisson 過程 \\[\nN_t:=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}\\eta(dsdx)\\in[0,\\infty]\n\\] を考えると， \\[\n\\operatorname{E}[N_t]=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}ds\\nu(dx)=0.\n\\] すなわち，\\(N_t=0\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n2.3 区分定数ならば，A 型である．\n\n\n\n\n\n\n命題（A 型の見本道の特徴付け）13\n\n\n\nLévy 過程 \\(X\\) ついて，次の３条件は同値：\n\n\\(X\\) の見本道は，殆ど確実に区分的定数であり，有界区間上では有限回のジャンプしか起こらない．\n\\(X\\) は複合 Poisson 分布であるか，零であるかのいずれかである．\n\\(X\\) は A 型で，かつ \\(\\gamma_0=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n1 \\(\\Rightarrow\\) 2\n任意の有限区間内でのジャンプ回数は有限回であるため，ジャンプ回数の Poisson 過程 \\(N\\) について，\\(N_t\\sim\\mathrm{Pois}(t\\nu(\\mathbb{R}^d))\\) の母数は有限である必要がある．特に \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\\(X\\) に連続部分がないことを併せると，定理 1.5 より， \\[\n\\operatorname{E}[e^{i(z|X_t)}]=\\exp\\left(t\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right).\n\\] これは \\(\\mathrm{CP}(t,\\nu)\\) の特性関数である．\n2 \\(\\Rightarrow\\) 1\nこちらは省略する．\n\n\n\n\n純粋跳躍確率過程であっても，B 型ならば，見本道は区分的定数にはならない．Gamma 過程（第 3.6 節）がその例である．\n\n\n2.4 B 型の跳躍時刻\nLévy 過程の見本道は右連続であるから，\\(\\mathbb{R}_+\\) 上トータルの跳躍回数は殆ど確実に可算回である．\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) の場合は，有限区間上での跳躍回数も無限になる．\nさらに，次のことが言える：\n\n\n\n\n\n\n命題（B 型 Lévy 過程のジャンプ時刻）14\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) とする．このとき，跳躍時刻は殆ど確実に \\(\\mathbb{R}_+\\) 上稠密である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) のとき， \\[\nT_\\epsilon(\\omega):=\\inf\\left\\{t\\ge 0\\mid\\lvert X_t(\\omega)-X_{t-}(\\omega)\\rvert&gt;\\epsilon\\right\\}\n\\] とすると， \\[\n\\lim_{\\epsilon\\searrow0}\\operatorname{P}[T_\\epsilon\\le t]=1.\n\\] よって， \\[\n\\lim_{\\epsilon\\searrow0}T_\\epsilon=0\\;\\;\\text{a.s.}\n\\] これは，ある充満集合 \\(\\Omega_0\\subset\\Omega\\) の上で，\\(0\\) が \\(X\\) の跳躍時刻の触点になることを含意している．\nこれと同様の議論を任意の \\(s\\in\\mathbb{Q}\\cap\\mathbb{R}_+\\) について繰り返すことで，ある充満集合 \\[\n\\bigcap_{s\\in\\mathbb{Q}\\cap\\mathbb{R}_+}\\Omega_s\n\\] 上で，\\(X\\) の跳躍時刻の閉包が \\(\\mathbb{R}_+\\) 上で稠密になることがわかる．\n\n\n\n\n\n2.5 従属過程ならば B 型である\n\\(d=1\\) で，殆ど確実に単調増加な見本道を持つ Lévy 過程を 従属過程 (subordinator) という．15\n\n\n\n\n\n\n命題（単調増加性の特徴付け）16\n\n\n\n\\(d=1\\) とし，\\(X\\) を Lévy 過程とする．このとき，次は同値：\n\n\\(X\\) は従属過程である．\n\\(A=0,\\nu((-\\infty,0))=0\\) かつ \\[\n\\int_{[0,1)}x\\,\\nu(dx)&lt;\\infty\n\\] 加えて \\(\\gamma_0\\ge0\\) である．\n\n\n\n仮に \\(A=0,\\nu((-\\infty,0))=0\\) だが， \\[\n\\int_0^1x\\,\\nu(dx)=\\infty\n\\] であったとする．\nこのとき，正なジャンプとドリフトしか持たないはずであるから，場合によっては単調増加過程になっても良さそうなものである．\nしかし，このような過程が発散せずに well-defined であるということは，負の方向に無限に強いドリフトを持っており，これが正なジャンプを打ち消していることが必要である．\nそれ故，ジャンプの隙間では負方向のドリフトが競り勝ち，全体としては単調増加にならない．特に，任意の区間において単調増加にならない．17\n\n\n2.6 C 型ならば非有界変動である\n\n\n\n\n\n\n命題（見本道の変動）18\n\n\n\nLevy 過程 \\(X\\) について，\n\nA 型または B 型ならば，有界変動過程である．すなわち，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動である．\nC 型ならば，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "href": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "3 従属過程と Gamma 過程",
    "text": "3 従属過程と Gamma 過程\n\n3.1 はじめに\nGamma 過程は，拡散項もドリフト \\(\\gamma_0\\) も持たない，純粋跳躍な従属過程である．\nしかし，正のジャンプのみをもち，ジャンプだけで増加していく過程だからと言って，その見本道は区分的に定数ではない．\nその Lévy 測度は \\(\\nu((0,\\infty])=\\infty\\) を満たし，B 型に分類される．従って，\\(\\mathbb{R}_+\\) の稠密部分集合上でジャンプしており，見本道は殆ど確実に，任意の点 \\(t\\in\\mathbb{R}_+\\) で非連続である．\nGamma 過程は元々，(Moran, 1959) によりダムの貯水量のモデルとして導入された．\n\n\n\n\n\n\n証明\n\n\n\n\n\n見本道 \\(X_\\bullet(\\omega)\\) は，\\(\\mathbb{R}_+\\) のある稠密部分集合 \\(A\\subset\\mathbb{R}_+\\) 上でジャンプしているとする：\\(\\overline{A}=\\mathbb{R}_+\\)．\nこのとき，\\(\\mathbb{R}_+\\) の任意の点で \\(X_\\bullet(\\omega)\\) は非連続である．\n実際，任意の \\(t&gt;0\\) を取り，ここで連続であるとすると，任意の \\(t\\) への収束列 \\(\\{t_n\\}\\subset\\mathbb{R}_+\\) について，\\(X_{t_n}(\\omega)\\to X_t(\\omega)\\) が成り立つ必要があるが，\\(t\\) は \\(A\\) の触点でもあるので，これに収束する \\(A\\) の点列 \\(\\{t_n\\}\\subset A\\) が取れる．これを特に，下から単調に収束するように取る：\\(t_n\\searrow t\\)．\n\n\n\nしかし，\\(\\nu\\) は平均を持つために有界変動ではあり，実際シミュレーションによって得る見本道を見ても，殆どのジャンプは目に見えない．\n\n\n3.2 Gamma 分布\n\\(\\mathbb{R}\\) 上の Gamma 分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\) とは，密度関数 \\[\ng(x;\\alpha,\\nu):=\\frac{\\alpha^\\nu}{\\Gamma(\\nu)}x^{\\nu-1}e^{-\\alpha x}1_{\\mathbb{R}^+}(x)\n\\] が定める分布をいう．\\(\\alpha\\) をレート，\\(\\nu\\) を形状パラメータというのであった．\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.3 Gamma 点過程\n\\(\\sigma\\)-有限測度 \\(\\rho_0\\in\\mathcal{P}(E)\\) と Lévy 測度 \\(\\nu:=\\mathrm{Gamma}(\\alpha,0)\\)，すなわち \\[\n\\nu(dr):=\\frac{e^{-\\alpha r}}{r}1_{\\mathbb{R}^+}(r)\\,dr\n\\] について，\\(\\lambda:=\\rho_0\\otimes\\nu\\) で定まる強度測度を持つ \\(E\\times\\mathbb{R}_+\\) 上の Poisson 点過程 \\(\\xi\\) を Gamma 点過程 という．19\nこれは \\[\n\\xi(B)\\sim\\mathrm{Gamma}(\\alpha,\\rho_0(B))\n\\] を満たす複合 Poisson 点過程である．\\(\\rho_0\\) のことを形状測度ともいう．\n\n\n\n\n\n\nDirichlet 過程 (Ferguson, 1973) との関係20\n\n\n\n\n\n\\[\n\\Delta_n:=\\left\\{\\begin{pmatrix}p_1\\\\\\vdots\\\\p_n\\end{pmatrix}\\in[0,1]^n\\,\\middle|\\,\\sum_{i=1}^np_i=1\\right\\}\n\\] を \\(n-1\\)-単体とする．21 この上に台を持つ，パラメータ \\(\\alpha\\in(0,\\infty)^n\\) で定まる密度 \\[\nf(x)=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_n)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_n)}x_1^{\\alpha_1-1}\\cdots x_n^{\\alpha_n-1}1_{\\Delta_n}(x)\n\\] が定める分布 \\(\\mathrm{Dirichlet}(n,\\alpha)\\in\\mathcal{P}(\\Delta_n)\\) を Dirichlet 分布 という．\nここで，\\(E\\) 上の Gamma 点過程 \\(\\xi\\) は \\(\\rho_0(E)&lt;\\infty\\) を満たすとする．このとき，\\(E\\) の分割 \\[\nE=B_1\\sqcup\\cdots\\sqcup B_n\n\\] \\[\n\\rho_0(B_i)&gt;0\n\\] に対して， \\[\n(\\zeta(B_1),\\cdots,\\zeta(B_n))\\sim\\mathrm{Dirichlet}(n,\\alpha)\n\\] \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] が成り立ち，これは \\(\\xi(E)\\) と独立である．\nこのことをふまえて，\\(\\rho_0\\) が有限であるとき，ランダム確率測度 \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] を Dirichlet 過程 という．22\n\n\n\n\n\n3.4 Gamma 点過程の Lévy 測度は \\(0\\) の近傍で発散する\nしかし，\\(\\mathrm{Gamma}(\\alpha,0)\\) などという分布はなく， \\[\n\\nu(\\mathbb{R})=\\int^\\infty_0r^{-1}e^{-\\alpha r}dr=\\infty.\n\\]\nこのとき，任意の \\(\\rho_0\\) で測って正の測度を持つ集合 \\(\\rho_0(B)&gt;0\\;(B\\in\\mathcal{E})\\) に対して，\\(\\xi\\) は殆ど確実に無限個の点を \\(B\\) 内にもつ．23\nしかし，\\(\\rho_0(B)&lt;\\infty\\) ならば \\(\\xi(B)&lt;\\infty\\) ではある．すなわち，ジャンプ幅も含めて足し合わせると，収束する．これは，\\(\\nu\\) が平均を持つことによる： \\[\n\\int_0^\\infty r\\,\\nu(dr)=\\alpha^{-1}.\n\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n  \\xi(B)&=\\int^\\infty_0\\int_Br\\,\\eta(dsdr)\\\\\n  &=\\int_B\\rho_0(ds)\\int^\\infty_0r\\,\\nu(dr)\\\\\n  &=\\rho_0(B)\\alpha^{-1}\n\\end{align*}\\]\n\n\n\n\n\n3.5 従属過程\n一般に，\\(\\xi\\) を \\(\\mathbb{R}^+\\) 上の Lévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を持つ一様な複合 Poisson 点過程，すなわち \\(\\ell_+\\otimes\\nu\\) を強度測度とする \\(\\mathbb{R}_+\\times\\mathbb{R}^+\\) 上の Poisson 点過程とすると， \\[\nY_t(\\omega):=\\xi(\\omega,[0,t])\n\\] で定まる過程 \\(Y\\) は，一般に Lévy 測度 \\(\\nu\\) を持つ 従属過程 (subordinator) という．24\n\n\n3.6 Gamma 計数過程\nLévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を \\[\n\\nu(dr):=\\delta\\frac{e^{-\\gamma r}}{r}dr\n\\] \\[\n\\delta,\\gamma&gt;0,\n\\] で与えたとき，付随する従属過程 \\(\\{Y_t\\}\\) を Gamma 過程 といい，\\(\\mathrm{Gamma}(\\delta,\\gamma)\\) で表す．25\nこれは \\(Y_t\\sim\\mathrm{Gamma}(\\gamma,\\delta t)\\) を満たす Lévy 過程である．\n\n\n\n\n\n\n\n\n\n目視できないジャンプが無数に存在することが窺える．\n\n\n\n3.7 分散 Gamma 過程\n２つの独立な Gamma 過程 \\[\nX^+\\sim\\mathrm{Gamma}(\\delta,\\gamma^-),X^-\\sim\\mathrm{Gamma}(\\delta,\\gamma^+)\n\\] に対して， \\[\nX^0_t=X^+_t-X^-_t\n\\] と表せる Lévy 過程 \\(X^0\\) を 分散 Gamma 過程 という．26\n\n\n\n\n\n\n\n\n\n分散 Gamma 過程は，オプション価格の対数のモデルとして，Brown 運動より柔軟なモデルとしても用いられる (Madan et al., 1998)．\nこれは，Brown 運動の分散が Gamma 分布に従うとして得る過程であるとも見れる．実際，Brown 運動の時間を，Gamma 過程によって変換したものが分散 Gamma 過程である．\n実際，Brown 運動 \\(B\\) とこれと独立な Gamma 過程 \\(T\\) について， \\[\nX^0_t=B_{T_t}\n\\] と表せる．27"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "href": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "4 安定過程と Cauchy 過程",
    "text": "4 安定過程と Cauchy 過程\n\n4.1 安定分布\n\n4.1.1 定義\n\n\n\n\n\n\n定義 (stable)28\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して \\[\nf(t)^n=f(a_nt)e^{ib_nt}\n\\] が成り立つ無限可分分布の特性関数をいう．\n確率変数 \\(Y\\in\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して， \\[\nY_1+\\cdots+Y_n\\overset{\\text{d}}{=}a_nY+b_n\n\\] を満たすことをいう．\n\n\n\nすなわち，安定分布とは， \\[\nZ_n:=\\frac{\\sum_{i=1}^nY_i-b_n}{a_n}\n\\] という形の，独立同分布確率変数の正規化された和の列 \\(\\{Z_n\\}\\) の分布収束極限として現れ得る分布の全体を指すことになる．29\nまた，\\(a_n\\) は \\(a_n=n^{1/\\alpha}\\) という形に限り，この \\(\\alpha\\in(0,2]\\) を 安定指数 という．\n\n\n4.1.2 Lévy 測度の有限性\n安定指数 \\(\\alpha\\in(0,2)\\) を持つ安定分布の Lévy 測度は非有限であり，平均も持たない．\n\n\n\n\n\n\n命題（Lévy 測度の平均）30\n\n\n\n\\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) を \\(\\alpha\\)-安定分布とする．このとき，その Lévy 測度 \\(\\nu\\) について，次は同値：\n\n\\(\\alpha\\in(0,1)\\) である．\n\\(\\nu\\) は \\(B^d\\) 上で平均を持つ： \\[\n  \\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n次も同値：\n\n\\(\\alpha\\in(1,2)\\) である．\n\\(\\nu\\) は \\(\\mathbb{R}^d\\setminus B^d\\) 上で平均を持つ： \\[\n  \\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n\n\n\n\n\n4.2 回転対称な安定分布\n\n4.2.1 特性関数の表示\n安定分布は無限可分であるため，Lévy-Khintchin 分解を通じた特性関数の形が特徴付けられる．\n中でも，（回転）対称な安定分布は特に簡単な表示を持つ：\n\n\n\n\n\n\n命題 (Lévy-Khinchin 表示)31\n\n\n\n\\(P\\in\\mathcal{P}(\\mathbb{R}^d)\\) は回転対称であるとする．このとき，その特性関数 \\(\\varphi\\) について次は同値：\n\n\\(\\varphi\\) は安定である．\nある \\(c&gt;0\\) と \\(\\alpha\\in(0,2]\\) が存在して， \\[\n\\varphi(u)=e^{-c\\lvert u\\rvert^\\alpha}.\n\\]\n\nこの \\(\\alpha\\) を 安定指数 という．\n\n\n\n\n\n\n\n\n例（対称な安定分布）\n\n\n\n\n\\(\\alpha=2\\) の対称安定分布とは，中心化された正規分布である．\n\\(\\alpha=1\\) の対称安定分布とは，中心化された Cauchy 分布である．\n\n\n\n\n\n\n\n\n\n中心極限定理のスケーリングレートとしての安定指数\n\n\n\n\n\n\\(a_n\\) は従って，中心極限定理を実現するために必要なスケーリングレートを表す．\nこのことは，一般のエルゴード的な定常過程に対して一般化できる：32\n\\(\\{X_n\\}\\) を \\(\\alpha\\)-撹拌的な定常過程，\\(\\{a_n\\}\\subset\\mathbb{R}^+\\) を発散列とし， \\[\n\\frac{1}{a_n}\\sum_{j=1}^nX_j-b_n\n\\] は弱収束するとする．この極限分布は安定分布になり，安定指数を \\(\\alpha\\) とする．\nこのとき，(Karamata, 1933) の意味で緩変動な関数 \\(h\\) に対して，\\(a_n=n^{1/\\alpha}h(n)\\) と表せる： \\[\n\\lim_{n\\to\\infty}\\frac{h(tn)}{h(n)}=1\\quad(t&gt;0).\n\\]\n\n\n\n\n\n4.2.2 自己相似性\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) は自己相似性を持つ．\n一般に，Hurst 指数 \\(H&gt;0\\) に関して自己相似的 (self-similar) であるとは，任意の \\(a&gt;0\\) について \\[\n(Y_{at})\\overset{\\text{d}}{=}(a^HY_t)\n\\] を満たすことをいう．\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) については，\\(H=\\alpha^{-1}\\) と取れる．\nBrown 運動は \\(H=1/2\\) について自己相似である．\nまた，自己相似な Lévy 過程は，狭義の安定過程に限る．33\n\n\n\n4.3 安定従属過程\n\\(\\alpha\\in(0,1)\\) の安定指数を持つ安定過程は，従属過程になる．34\n\n\n\n\n\n\n例（Lévy 従属過程）35\n\n\n\n\n\nLévy 分布 \\(\\mathrm{Levy}(c):=\\mathrm{IG}(c^{1/2},0)\\) とは，密度 \\[\nf(x;c):=\\sqrt{\\frac{c}{2\\pi}}x^{-\\frac{3}{2}}e^{-\\frac{c}{2x}}1_{\\mathbb{R}^+}(x)\n\\] を持つ \\(\\mathbb{R}\\) 上の分布をいう．\nこれは次の特性関数を持ち，安定指数 \\(\\alpha=1/2\\) を持つ非対称な安定分布である： \\[\n\\varphi(u)=\\exp\\left(-\\sqrt{c\\lvert u\\rvert}\\biggr(1-i\\operatorname{sgn}(u)\\biggl)\\right).\n\\]\n安定指数 \\(1/2\\) の安定従属過程 \\(T\\) は Lévy 従属過程 とも呼ばれ， \\[\nT_t\\sim\\mathrm{Levy}(t^2/2)\n\\] を満たす．\nこれは，１次元 Brown 運動の到達時刻 \\[\nT_t:=\\inf\\left\\{s&gt;0\\mid B_s=\\frac{t}{\\sqrt{2}}\\right\\}\n\\] の過程として現れる．\nLévy 過程は逆正規過程の特殊な場合であり，これは一般の Gauss 過程の到達時刻の過程として現れる．36\n\n\n\n\n\n\n\n\n\n例（安定従属過程による従属操作）37\n\n\n\n\n\n\\(\\{\\tau_t\\}_{t\\in\\mathbb{R}_+}\\) を安定指数 \\(\\alpha\\in(0,1)\\) を持つ安定従属過程とする．\nこれと独立な Lévy 過程 \\(X\\) に対して，従属化 \\[\nt\\mapsto X_{\\tau_t}\n\\] は再び Lévy 過程である．\n特に，\\(X\\) を Brown 運動 \\(B\\) とすると，\\(B_{\\tau}\\) は安定指数 \\(2\\alpha\\) を持つ安定過程になる．\n例えば \\(\\tau_a\\) として \\[\nT_a:=\\inf\\left\\{t\\in\\mathbb{R}_+\\mid B_t=a\\right\\}\n\\] と取ると，これは安定指数 \\(1/2\\) を持つ安定従属過程（Lévy 従属過程）の修正である．38\nこれより，各 \\(a\\in\\mathbb{R}_+\\) への到達時刻で止めた Brown 運動の過程 \\(a\\mapsto B_{T_{a+}}\\) は対称な Cauchy 過程になる．\n\n\n\n\n\n4.4 Cauchy 過程\nCauchy 過程は安定指数 \\(\\alpha=1\\) を持つ狭義の対称安定過程である．39\n拡散項を持たないが，Lévy 測度は平均を持たず（命題 4.1.2），C 型の Lévy 過程である．\nすなわち，殆ど確実に，任意の区間上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#footnotes",
    "href": "posts/2024/Process/Levy.html#footnotes",
    "title": "Lévy 過程を見てみよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Nualart and Nualart, 2018, p. 158) 定義9.1.1，(Sato, 2013, p. 3) 定義1.6，(Rocha-Arteaga and Sato, 2019, pp. 12–13) 定義1.31に倣った．(Protter, 2005, p. 20) では(2)の\\(D\\)-過程という部分がないのみで，定理30 (Protter, 2005, p. 21) で常に\\(D\\)-修正が取れることを示している．(Le Gall, 2016, p. 175) 6.5.2節も同様の取り扱いである．(伊藤清, 1991, p. 306) は時間的一様性を所与のものとはせず，(1), (2), (3), (5)のみをLévy過程の定義としており，さらに(4)も満たすものを 一様Lévy過程 という．(Baudoin, 2014, pp. 89–90) 定義3.40では(5)がない．(Böttcher et al., 2013, p. 14) 例1.17では(1),(2)がない．(Osswald, 2012, pp. 258–259) は(1), (3), (4)を定義としている．(Applebaum, 2009, p. 43) は(1), (3), (4), (5)を定義としている．(佐藤健一, 1990) では全く同じものを加法過程と呼ぶが，(佐藤健一, 2011) は完全に一致する語用法をする（加法過程に確率連続性を課している点を除く）．↩︎\n(Sato, 2013, p. 47) 定理9.1 参照．↩︎\n(Sato, 2013, p. 51) 定理9.7参照．↩︎\n(Dudley, 2002, p. 327) 定理9.8.3，(Sato, 2013, p. 37) 定理8.1，(Rocha-Arteaga and Sato, 2019, p. 11) 定理1.28，(Baudoin, 2014, p. 91) 定理3.46，(Applebaum, 2009, p. 29) 定理1.2.14 など参照．↩︎\nGauss 共分散の用語は (Sato, 2013, p. 38) 定義8.2．Khintchine 測度は (Loéve, 1977, p. 343)，(Applebaum, 2009, p. 31)，(Böttcher et al., 2013, p. 33)，(Baudoin, 2014, p. 92) など．↩︎\n(Loéve, 1977, p. 343)↩︎\n(Sato, 2013, p. 39) 注8.4．↩︎\n特性測度の名前は (Revuz and Yor, 1999, p. 478) 演習 XII.1.18 など．命題は (Sato, 2013, p. 53) 注9.9も参照．↩︎\n(Sato, 2013, p. 120) 定理19.2より．(Protter, 2005, p. 31) 定理42 は Lévy 過程に限って示している．(1)は (伊藤清, 1991, p. 313) 補題5.3でも解説されている．(Protter, 2005, p. 26)定理35も参照．↩︎\nこの分類は (Sato, 2013, p. 65) 定義11.9に倣った．↩︎\n(Sato, 2013, p. 140) 定理21.9 参照．↩︎\n(Sato, 2013, p. 135) 定理21.1．↩︎\n(Lowther, 2011) 定理１，(Sato, 2013, p. 135) 定理21.2．↩︎\n(Sato, 2013, p. 136) 定理21.3．↩︎\n(Applebaum, 2009, p. 52)，(Baudoin, 2014, p. 95) 定義3.50，(Sato, 2013, p. 137) 定義21.4，(Iacus and Yoshida, 2018, p. 171) に倣った．(Kingman, 1992, p. 88) 8.4節，(Last and Penrose, 2017, p. 156) 例15.7 は命題の条件2の方を定義に用いている．↩︎\n(Sato, 2013, p. 137) 定理21.5．↩︎\n(Sato, 2013, p. 138) も参照．↩︎\n(Lowther, 2011) 定理２，(Sato, 2013, p. 140) 定理21.9．↩︎\n定義は (Last and Penrose, 2017, p. 155) 例15.6 に倣った．↩︎\n(Ghosal and van der Vaart, 2017, p. 562) 命題G.2.(i)，(Last and Penrose, 2017, p. 163) 演習15.1，(Kingman, 1992, pp. 92–) 9.2節．↩︎\n\\(n=2\\) を取ると１単体（線分），\\(n=3\\) と取ると２単体（三角形）を得る．↩︎\n(Kingman, 1992, p. 93)，(Ghosal and van der Vaart, 2017, p. 59) 定義4.1．↩︎\n\\(\\lambda(B)=\\rho_0(B)\\nu(\\mathbb{R})=\\infty\\) となるためである．(Last and Penrose, 2017, p. 163) 演習15.2も参照．↩︎\n(Kingman, 1992, p. 88) 8.4節，(Last and Penrose, 2017, p. 156) 例15.7 などの用語法．一般に subordinator とは，単調増加な Lévy 過程をいう (Sato, 2013, p. 137) 定義21.4，(Baudoin, 2014, p. 95) 定義3.50，(Iacus and Yoshida, 2018, p. 171)．これは，時間変数に関する変数変換を subordination と呼び，その際の変数変換に使えるためである．↩︎\n記法は (Iacus and Yoshida, 2018) による．(Applebaum, 2009, pp. 54–55) 例1.3.22，(Protter, 2005, p. 33) 例４も参照．↩︎\n(Iacus and Yoshida, 2018, p. 160) に倣った．↩︎\n(Lowther, 2011)，(Applebaum, 2009, p. 59) 例1.3.31 も参照．↩︎\n(Revuz and Yor, 1999, p. 116) 定義III.4.1，(Sato, 2013, p. 69) 定義13.1，(Shiryaev, 2016, p. 416) 定義3.6.2，(Loéve, 1977, p. 338)．↩︎\n(Shiryaev, 2016, p. 416) 定理3.6.3 も参照．↩︎\n(Sato, 2013, p. 80) 命題14.5．↩︎\n(Sato, 2013, p. 86) 定理14.14．(Shiryaev, 2016, p. 419) 定理3.6.4，(Loéve, 1977, p. 339)，(Dudley, 2002, p. 328) 定理9.8.4 は \\(d=1\\) の場合．↩︎\n(Ibragimov and Linnik, 1971, p. 316) 定理18.1.1 も参照．↩︎\n狭義の安定過程とは，\\(b_n\\equiv0\\) と取れることをいう (Sato, 2013, p. 69) 定義13.1．(Embrechts and Maejima, 2002)，(Applebaum, 2009, p. 51) 例1.3.14 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Sato, 2013, p. 138) 例21.7，(Applebaum, 2009, p. 53) 例1.3.18 も参照．↩︎\n(Applebaum, 2009 @/53) 例1.3.19 も参照．↩︎\n(Applebaum, 2009, p. 54) 例1.3.21 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Rogers and Williams, 2000, p. 133) も参照．↩︎\n(Revuz and Yor, 1999, p. 107) 命題III.3.9 も参照．↩︎\n(Sato, 2013, p. 87) 例14.17．↩︎"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html",
    "href": "posts/2023/Lifestyle/QuartoBasics.html",
    "title": "Quarto はじめて良かったこと",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#使い方の概要",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#使い方の概要",
    "title": "Quarto はじめて良かったこと",
    "section": "1 使い方の概要",
    "text": "1 使い方の概要\n\n1.1 はじめに\n本サイトは Quarto と，GitHub Actions によってホスティングされている．\n\n\n\n\n\n\n\nLévy 過程を見てみよう など，コードと数式を併せて書いている Jupyter Notebook のようなページ\nCV など，HTML と PDF の両方で見れるページ\nZig-Zag サンプラー など，HTML とスライド (reveal.js) の両方で見れるページ\n本ページなど，HTML とスライド (pptx)，typst PDF と LaTeX PDF と reveal.js のさまざまで見れるページ\n\n\n\n\n\n\n\n\n\n\n注\n\n\n\n\n\nスマホでは別フォーマットのページのリンクは表示されないようである．\n\n\n\n\n\nCode\nusing Plots\n\np = plot(sin, \n     x-&gt;sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1: Parametric Plots\n\n\n\n\nQuarto ではこのような数式・コードが共存するドキュメントが，極めて簡単に＋凡ゆるフォーマットで作成できる．\n計算統計の研究をしている筆者にとっては何より，数式とコードが自然に共存する PDF を簡単に書けること，そして自学のためのノートがそのまま HTML としてブログの形で公開できることが，大変嬉しかった．\n特に VSCode の拡張機能と組み合わせれば，RStudio のような隙のない統合開発環境が得られる．1\n基本的な仕組みとして，自分で作成するのは .qmdファイルのみである．\nその後はquarto renderコマンドにより，\n\n\n\n\n\n\n\nコードブロックは Jupyter によって処理され，\n全体は markdown に変換され，\nPandoc によってpdf, html, word など好きな形式に最終出力できる．\n\n\n\n\n拡張機能をオンにした VSCode ではRun Cellボタンもあるので，ノートブック全体を毎度ビルドせずとも，コードブロックごとに実行して結果を見ることもできる．\nCtrl+Enter で１行ごとに実行できる操作感は RStudio と同じである．\n\n\n1.2 美点\n\n\n\n\n\n\n\nレンダリングがとんでもなく速い．体感で TeX の10分の1である．\nそれでいて数式とコードブロックを併在させることが出来る．なお，明かに TeX を意識していることがわかる使用感になっているし，本の作成も可能としている．\n（ちょっと使いにくい）ブラウザ上ではなく，好きなエディタで動く．Jupyter Notebook が続かない筆者にとって，この点は肝要である．\n私用の勉強ノートとしても使えると同時に，内容そのままブログとして公開できる．\nプレゼンテーションの作成にも使える．\nすごい細かいが，例えば project type を website としたリポジトリでquarto renderをしても，不要なファイルが自動で削除される．このような点がライトユーザーでもとにかく使いやすい．\nさらにインタラクティブな機能を実現したブログを作ってみたい．\n\n\n\n\n\n\n1.3 YAML Header\n各ファイルの冒頭に YAML block を用意することで，ノートブックの詳細を調整できる（参照：HTML Options）．\n例えば本ページでは次のとおり：\n---\ntitle: \"Quarto はじめて良かった\"\nauthor: \"司馬博文\"\ndate: \"11/4/2023\"\ndate-modified: \"7/7/2024\"\ncategories: [Lifestyle]\nabstract: Quarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．\nabstract-title: 概要\nformat:\n  html:\n    mainfont: \"Gill Sans\"\n    theme: minty\n    css: assets/styles.css\n    toc: true\n    number-sections: true\n    highlight-style: ayu\n    code-block-border-left: \"#7CC4AC\"\n    code-overflow: scroll\n    toc-title: \"目次\"\n    abstract-title: \"概要\"\n---\n\n\n1.4 本文の書き方\n\n1.4.1 数式\n本文は markdown 記法で書く．数式も使える：\n\\[\n\\operatorname{P}[\\lvert\\xi\\rvert&lt;t]\\le2e^{-\\frac{t^2}{2\\sigma^2}},\\qquad t&gt;0.\n\\]\n\n\n1.4.2 コード\nまた，コードブロックにもコメントアウトと接頭辞の組み合わせ #| を前につけることでYAMLで指示が出せる（参照：指示のリスト）．上のコードブロックには\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\nと追加されているために，出力された図にラベリングとキャプションが付いているのである．\npip3 install jupyter-cache\nが必要であることに注意．\n\n\n\n1.5 カーネルの選択\n&gt; python3 -m venv GenAI\n\n&gt; source GenAI/bin/activate\nにより仮想環境を作成して入れるが，この環境を Jupyter notebook で使うにはもう一手間必要である．\n&gt; pip install ipykernel\n\n&gt; python -m ipykernel install --user --name=GenAI\nすると\njupyter kernelspec list\nにより見つかるようになっている．YAML header で jupyter: genai と指定すれば良い．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#website-の作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#website-の作り方",
    "title": "Quarto はじめて良かったこと",
    "section": "2 Website の作り方",
    "text": "2 Website の作り方\n公式 Guide を参考．\n\n2.1 Source Branchをmainと別ける\nまずgh-pagesという全く新しいブランチを作成する．既存のリポジトリのコミット履歴とは独立している新しいブランチを作るときは--orphanオプションが利用される．\n\n\nTerminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\ngit checkout main\n\n基本gh-pagesブランチには自分では立ち入らない．\n\n\n2.2 Publishコマンドによるサイトの公開\nmainブランチにいることを確認して，\n\n\nTerminal\n\nquarto publish gh-pages\n\nを実行．\nGitHubの方の設定Settings: Pagesで，Sourceをgh-pagesブランチの/(root)にしていることを確認すれば，これで無事サイトが公開されていることが確認できる．\n\n\n2.3 GitHub Action の使用\nさらに，ローカル上でrenderするのではなく，コミットする度にGitHub上でレンダリングしてもらえるように自動化することもできる．こうするとスマホからも自分のサイトが更新できる．\nまず，GitHubの設定のActionsセクションのWorkflow permissionsから，読み書きの権限をGitHub Actionに付与する．\n続いて，次の内容のファイルを.github/workflows/publish.ymlに書き込む：\n\n\n.github/workflows/publish.yml\n\non:\n  workflow_dispatch:\n  push:\n    branches: main\n\nname: Quarto Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          tinytex: true  # https://github.com/quarto-dev/quarto-actions/tree/main/setup\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Setting GH_TOKEN is recommended as installing TinyTeX will query the github API.\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n          # render: false  # https://quarto.org/docs/publishing/github-pages.html#additional-options\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n途中，tinytex: true とすることで，１つのページを HTML と pdf の両方で閲覧可能になる．本ブログでは，CV のページ でこの機能を使っている．\nこれで，mainブランチにコミットする度に，GitHub上でrenderが実行されることとなる．"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#pdf-の作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#pdf-の作り方",
    "title": "Quarto はじめて良かったこと",
    "section": "3 PDF の作り方",
    "text": "3 PDF の作り方\n\n3.1 LuaLaTeX を使う方法\nLuaLaTeX を利用することで日本語を含んだ PDF を作成できる．\n\n\nreport.qmd\n\ntitle: \"タイトル\"\nauthor: 司馬博文\ndate: 2023/12/11\nformat:\n  pdf:\n    toc: true\n    number-sections: true\n    urlcolor: minty\n    template-partials: \n      - ../../../assets/before-title.tex\n    keep-tex: true\n    block-headings: false\n    pdf-engine: lualatex\n    documentclass: ltjsarticle\n\n\n3.1.1 LuaLaTeX の注意\n\\int_{\\mathbb{R}}\nのような記法は，pdfLaTeX ではなぜかコンパイルが通るが，LuaLaTeX （や殆どの pdfLaTeX 以外のエンジン）ではエラーになる．\n\n\n3.1.2 LuaLaTeX の欠点\nltjsarticle クラスでは\nFont \\JY3/mc/m/n/10=file:HaranoAjiMincho-Regular.otf:-kern;jfm=ujis at 9.24713pt not loadable: metric data not found or bad.\n&lt;to be read again&gt; \nrelax \nl.79 \\kanjiencoding{JY3}\\selectfont\n                                 \\adjustbaseline\nというエラーが．一方で，bxjsarticle クラスでは\nLaTeX Error: File `haranoaji.sty' not found.\n\nType X to quit or &lt;RETURN&gt; to proceed,\nor enter new name. (Default extension: sty)\n\nEnter file name: \n! Emergency stop.\n&lt;read *&gt;\nというエラーが出る．\nローカルではインストールすれば良いだけであるが，これを GitHub Actions 上で実現する方法を考えあぐねていた．\n\n\n\n\n\n\n注（TeX Live のアップデート方法）\n\n\n\n\n\n年度を跨いだ TeX Live manager のアップデートは，次のようにする必要がある：\nwget http://mirror.ctan.org/systems/texlive/tlnet/update-tlmgr-latest.sh\nchmod +x update-tlmgr-latest.sh\nsudo ./update-tlmgr-latest.sh\n\n\n\n\n\n3.1.3 LuaLaTeX と日本語フォント\nなぜか\n\\usepackage[haranoaji,nfssonly]{luatexja-preset}\nで変わるのは英語文字だけである．\n\n\n3.1.4 GitHub Actions の修正\n次のようにして，Set up Quarto と Render and Publish の間に，TinyTeX と haranoaji.sty のインストールを使いすることで，GitHub 上でもレンダリングが可能になる．\n\n\npublish.yml\n\n- name: 'Install TinyTeX'  # https://github.com/quarto-dev/quarto-actions/tree/main/setup\n  env:\n    QUARTO_PRINT_STACK: true\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Setting GH_TOKEN is recommended as installing TinyTeX will query the github API.\n  run: |\n    quarto install tool tinytex --log-level warning\n    case $RUNNER_OS in \n      \"Linux\")\n          echo \"$HOME/bin\" &gt;&gt; $GITHUB_PATH\n          export PATH=\"$HOME/bin:$PATH\"\n          ;;\n       \"macOS\")\n          TLMGR_PATH=$(dirname $(find ~/Library/TinyTeX -name tlmgr))\n          echo $TLMGR_PATH &gt;&gt; $GITHUB_PATH\n          export PATH=\"$TLMGR_PATH:$PATH\"\n          ;;\n       \"Windows\")\n          TLMGR_PATH=$(dirname $(find $APPDATA/TinyTeX -name tlmgr.bat))\n          echo $TLMGR_PATH &gt;&gt; $GITHUB_PATH\n          export PATH=\"$TLMGR_PATH:$PATH\"\n          ;;\n        *)\n          echo \"$RUNNER_OS not supported\"\n          exit 1\n          ;;\n    esac\n    echo \"TinyTeX installed !\"\n    tlmgr install haranoaji   # Install haranoaji.sty\n  shell: bash\n\n\n\n3.1.5 ローカルの TinyTeX に haranoaji.sty をインストールする方法\ntlmgr install haranoaji\nだと，すでに TeX Live がローカルに存在する場合は，そちらにインストールされてしまう．\nquarto install tinytex\nでインストールされる TinyTeX は，ホームディレクトリ下の ~/Liberary/TinyTeX/ の bin 内にインストールされる．2\nそこの，tlmgr がインストールされている場所まで行って，\n./tlmgr install haranoaji\nを実行すると良い．\n❯ ./tlmgr install haranoaji        \ntlmgr: package repository https://mirror.las.iastate.edu/tex-archive/systems/texlive/tlnet/ (verified)\n[1/1, ??:??/??:??] install: haranoaji [25570k]\nrunning mktexlsr ...\ndone running mktexlsr.\ntlmgr: package log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr.log\ntlmgr: command log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr-commands.log\n\n\n\n3.2 Typst を用いる方法\nHP\n使うフォントは次のように，Google Fonts を通じて，GitHub Actions 上でインストールすることもできるだろう：\nwget https://github.com/google/fonts/raw/main/ofl/bizudpgothic/BIZUDPGothic-Regular.ttf\nwget https://github.com/google/fonts/raw/main/ofl/bizudpgothic/BIZUDPGothic-Bold.ttf\ntypst の pdf は数式の処理がまだ納得のいく設定が見つかっていないが，コードの扱いが非常に自然で，出来上がりも美しい．\nただし，事前に GitHub Actions の環境上に対応する日本語フォントを用意しておく必要がある．\n{yml filename=\"publish.yml\"} - name: Install Japanese Fonts   env:     GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # Setting GH_TOKEN is recommended as installing TinyTeX will query the github API.   run: |     git clone https://github.com/yuru7/udev-gothic.git     cd udev-gothic     sudo cp -r ./source /usr/share/fonts/truetype/udev-gothic     sudo fc-cache -f -v"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#スライドの作り方",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#スライドの作り方",
    "title": "Quarto はじめて良かったこと",
    "section": "4 スライドの作り方",
    "text": "4 スライドの作り方"
  },
  {
    "objectID": "posts/2023/Lifestyle/QuartoBasics.html#footnotes",
    "href": "posts/2023/Lifestyle/QuartoBasics.html#footnotes",
    "title": "Quarto はじめて良かったこと",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n特に，VSCode ではビジュアルモードでの編集もサポートされており，Jupyter Notebookと全く同じ使用感で始められる．↩︎\nMaxOS では．quarto --paths で確認可能．↩︎"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html",
    "href": "posts/2024/Process/MartingaleProblem.html",
    "title": "マルチンゲール問題",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "href": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "title": "マルチンゲール問題",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Hoh, 1998, p. 28), (Criens et al., 2023)↩︎"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html",
    "href": "posts/2024/Process/ResidualWaitingTime.html",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#導入",
    "href": "posts/2024/Process/ResidualWaitingTime.html#導入",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 Markov 過程のエルゴード性\n空間 \\(E\\) 上の Markov連鎖は，\\(E\\) 上の確率測度の空間 \\(\\mathcal{P}(E)\\) 上に力学系 \\(((P^*)^n\\mu)_{n\\in\\mathbb{N}}\\) を定める．その不動点 \\(P^*\\mu=\\mu P=\\mu\\) が不変確率分布（平衡分布）である．\nこれは，Markov 連鎖の 確率核 \\(P\\) の 左作用 \\(P:\\mathcal{L}_b(E)\\to\\mathcal{L}_b(E)\\) の随伴作用素 \\(P^*:\\mathcal{P}(E)\\to\\mathcal{P}(E)\\) が \\(\\mathcal{P}(E)\\) に作用して得られる力学系ともみれる．\n\n\n\n\n\n\nこの力学系 \\((\\mathcal{P}(E),P^*)\\) は不動点を持つか？持つならば，どのようなノルムについてどのくらいの速さで収束するか？\n\n\n\nこれが Markov 連鎖のエルゴード性の議論である．\n通常，Markov 過程のエルゴード性は全変動ノルムについて考慮されるが，近年は弱位相に関する議論も進んでいる．\n\n\n1.2 再起過程\n\n\n\n\n\n\n定義 (renewal process, counting process)1\n\n\n\n非負確率変数の独立同分布 \\(\\{T_n\\}\\subset\\mathcal{L}(\\Omega)_+\\) について，\n\n\\(\\{T_n\\}\\) が定めるランダムウォーク \\[S_0=0,\\qquad S_n:=T_1+\\cdots+T_n,\\qquad n\\ge1,\\] を 再起過程 または再生過程という．2 \\(\\{T_n\\}\\) を待ち時間 (interarrival times)，\\(S_n\\) を \\(n\\) 回目到着時刻という．\n再起過程 \\(\\{S_n\\}\\) の 再起回数過程 とは， \\[N_t:=\\sum_{n=0}^\\infty1_{[0,t]}(S_n)=\\sup_{n\\in\\mathbb{N}\\mid S_n\\le t}\\] をいう．\\(t\\mapsto\\operatorname{E}[N_t]\\) を再起関数という．\n\n\n\n再起過程 \\(\\{S_n\\}\\) は通常，繰り返し起こる事象の発生時間をモデル化するために用いられる．\nその代表的なものが，待ち時間 \\(\\{T_n\\}\\) を指数分布に取った場合である Poisson 過程である．\n再起過程は OR を中心として，多くの応用先を持つ：\n\n\n\n再起過程の応用例 by Claude 3 Opus\n\n\n\n\n1.3 付随する待ち時間の Markov 過程\n再起過程 \\(\\{S_n\\}\\) は，ある Markov 過程 \\(\\{X_t\\}\\) が原点に戻る時刻の列と捉えることで，エルゴード性を議論することができる．\nこのときの Markov 過程 \\(\\{X_t\\}\\) を，本稿では 待ち時間の Markov 過程 と呼ぶことにする．\n\n\n\n待ち時間の Markov 過程 \\((X_t)\\) のアニメーション．原点は左端としている．\n\n\n以下，第 2 節では離散時間で状態空間も離散 \\(\\mathbb{N}\\) の場合，第 3 節では連続時間で状態空間も連続 \\(\\mathbb{R}_+\\) の場合について，この待ち時間の Markov 過程 \\(X\\) のエルゴード性を調べる．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-chain",
    "href": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-chain",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "2 待ち時間の Markov 連鎖",
    "text": "2 待ち時間の Markov 連鎖\nまず，待ち時間 \\(T_n\\) は非負整数 \\(\\mathbb{N}=\\{0,1,\\cdots\\}\\) 値とし，その分布を \\((p_i)\\sim\\mathcal{P}(\\mathbb{N})\\) とする．\nこれが定める再生過程 \\(\\{S_n\\}\\) は，次の遷移確率 \\((p_{ij})\\) を持つ \\(\\mathbb{N}\\) 上の Markov 連鎖 \\(X\\) が原点 \\(0\\) に到着する時刻の列と同分布である： \\[\np_{(i+1)i}=1,\\qquad p_{0i}=p_i,\\qquad i\\in\\mathbb{N}.\n\\]\nこのとき，（無限次元の）確率行列 \\(P\\) は Frobenius の同伴行列 の転置の形をしている．\n\n2.1 エルゴード定理\n\n\n\n\n\n\n命題\n\n\n\n上で定義した Markov 連鎖 \\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\) について，\n\n任意の状態 \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) が成り立つとする．このとき，\\(X\\) は既約で非周期的であり，再帰的である．\nさらに，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) も満たすとき，\\(X\\) は不変確率測度 \\[\\mu_i=\\frac{\\sum_{j=i}^\\infty p_j}{1+\\sum_{j=1}^\\infty jp_j}\\] をもち，正に再帰的である．そうでないときは零再帰的である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n分布 \\((p_i)\\) が偶数の上にしか台を持たないなど，\\(\\mathrm{supp}\\;(p)\\) に周期がある場合は \\(X\\) は周期的になってしまうが，任意の \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) ならば，任意の状態 \\(i\\in\\mathbb{N}\\) は本質的であり，互いに行き来できるため既約であり，周期も持たない．必ず有限時間内に原点に戻ってくるため，再帰的でもある．\n原点 \\(0\\) に初めて帰ってくる時刻を \\(T_0\\) とすると， \\[\\begin{align*}\n\\operatorname{E}_0[T_0]&=\\sum_{j=0}^\\infty(j+1)p_j\\\\\n&=1+\\sum_{j=0}^\\infty jp_j\\\\\n&=1+\\sum_{j=1}^\\infty jp_j.\n\\end{align*}\\] よって，正に再帰的であること \\(\\operatorname{E}_0[T_0]&lt;\\infty\\) は，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) に同値． このとき，離散エルゴード定理より，ただ一つの不変測度 \\((\\mu_n)\\in\\mathcal{P}(\\mathbb{N})\\) を持ち， \\[\\mu_i=\\frac{1}{\\operatorname{E}_i[T_i]},\\qquad i\\in\\mathbb{N},\\] と表せる．これにより \\(i=0\\) の場合はすぐに計算できるが，\\(i&gt;0\\) の場合は少し計算の見通しが良くない．そこで，必要条件 \\[\\mu_i=\\mu_{i+1}+\\mu_0p_i,\\qquad i\\in\\mathbb{N},\\] に注目すると，これを再帰的に適用することで， \\[\\begin{align*}\n\\mu_{i-1}&=\\mu_{i-1}-\\mu_0p_{i-1}\\\\\n&=\\mu_{i-2}-\\mu_0p_{i-2}-\\mu_0p_{i-1}\\\\\n&=\\cdots\\\\\n&=\\mu_0-\\mu_0\\sum_{j=0}^{i-1}p_j\\\\\n&=\\mu_0\\sum_{j=i}^\\infty p_j.\n\\end{align*}\\]\n\n\n\n\nMarkov 連鎖の概念は次節で解説しているので，証明を読む前にぜひチェックしてください．\n\n\n2.2 離散 Markov 連鎖の概念\nまず，離散状態空間 \\(E\\) 上の Markov 連鎖は，各状態 \\(i\\in E\\) の分類から始まる．\n\n\n\n\n\n\n定義：状態の再帰性 (recurrent, transient, positive recurrent, null recurrent)\n\n\n\n\\(E\\) を可算集合，\\(\\{X_n\\}\\) を \\(E\\) 上の Markov 連鎖とする．状態 \\(i\\in E\\) について， \\[\n\\tau_i:=\\inf\\{n\\ge1\\mid X_n=i\\}\n\\] を到着時刻とする．\n\n\\(i\\) が 再帰的 な状態であるとは，Markov 連鎖 \\(\\{X_n\\}\\) が \\(i\\in E\\) からスタートした場合，必ずいずれ戻ってくることをいう： \\[\n\\operatorname{P}_i[\\tau_i&lt;\\infty]=1.\n\\] そうでない場合，\\(i\\in E\\) は 推移的 であるという．\n再帰的な状態 \\(i\\in E\\) がさらに 正に再帰的 であるとは，帰ってくる時刻の期待値が有限であることをいう： \\[\n\\operatorname{E}_i[\\tau_i]&lt;\\infty.\n\\] そうでない場合は 零再帰的 であるという．\n\n\n\n続いて，この状態 \\(i\\in E\\) 毎に定義した性質が，Markov 連鎖 \\(\\{X_n\\}\\) 全体の性質に直接に影響するためには，次の「既約性」の条件が必要である．\n状態 \\(i\\in E\\) から \\(j\\in E\\) へ 到達可能 であるとは，ある \\(n\\in\\mathbb{N}\\) が存在して \\(p_{ij}^n&gt;0\\) を満たすことをいう．これを \\(i\\to j\\) と表す．\n\n\n\n\n\n\n定義：既約性と非周期性\n\n\n\n\\(E\\) を可算集合，\\(\\{X_n\\}\\) を \\(E\\) 上の Markov 連鎖とし，その遷移確率を \\(p_{ij}^n=\\operatorname{P}_i[X_n=j]\\) と表す．\n\n状態 \\(i\\in E\\) が 本質的 であるとは，任意の到達可能な状態 \\(i\\to j\\) に対して，\\(j\\to i\\) でもあることをいう．\nMarkov 連鎖 \\(X\\) が 既約 であるとは，任意の本質的な状態 \\(i,j\\in E\\) が互いに到達可能であることをいう：\\(i\\leftrightarrow j\\)．\n\n\n\n\\(X\\) の遷移確率 \\(p\\) は，\\(E\\) の本質的な状態 \\(E_\\mathrm{ess}\\) 上に，互いに到達可能であるという関係 \\(\\leftrightarrow\\) を通じて同値類 \\(E_\\mathrm{ess}/\\leftrightarrow\\) を定めることが示せる．この同値類が１つに縮退することを，既約というのである．\nまた，周期 \\[\nd(i):=\\gcd\\{n\\ge1\\mid p_{ii}^n&gt;0\\}\n\\] は，先述の同値類 \\(E_\\mathrm{ess}/\\leftrightarrow\\) 上に関数を定める．この関数 \\(d:(E_\\mathrm{ess}/\\leftrightarrow)\\to\\mathbb{N}^+\\) が 定値関数 \\(1\\) となるとき，\\(X\\) を 非周期的 という．\n\n\n\n\n\n\n証明\n\n\n\n\n\n任意の \\(i,j,k\\in E\\) について，必ず \\[\np^{n+m}_{ik}\\ge p^n_{ij}p^m_{jk}\n\\] が成り立つ．\\(i\\to j\\) かつ \\(j\\to k\\) であるとき，ある \\(n,m\\ge1\\) が存在して \\(p^n_{ij}&gt;0\\) かつ \\(p^m_{jk}&gt;0\\) であるから，\\(p^{n+m}_{ik}&gt;0\\) である．よって，\\(i\\to j\\)．これより \\(\\leftrightarrow\\) は推移的である．反射性は \\(p_{ii}^0=1&gt;0\\) であるため，定義上成り立つ．対称性も成り立つ．\n続いて，\\(i\\leftrightarrow j\\) ならば，\\(d(i)=d(j)\\) を示す． \\[\nN_i:=\\gcd\\{n\\ge1\\mid p_{ii}^n&gt;0\\}\n\\] と表すと，\\(i\\leftrightarrow j\\) ならば \\(N_i\\ne\\emptyset\\) である．任意の \\(s\\in N_i\\) を取ると，仮定 \\(i\\leftrightarrow j\\) より，先ほどの議論と同様にして，ある \\(n,m\\ge1\\) が存在して， \\[\np^{n+m+ks}_{jj}\\ge p^m_{ji}p^s_{ii}p^n_{ij}&gt;0,\\qquad k=1,2,\\cdots.\n\\] よって，\\(d(j)|s\\) が必要であるから，\\(d(j)\\le d(i)\\) が結論づけられる．逆も全く同様に議論できるから，\\(d(j)=d(i)\\)．\n\n\n\n\n\n\n\n\n\n命題：既約な Markov 連鎖の再帰性\n\n\n\n状態 \\(i,j\\in E\\) は互いに到達可能であるとする：\\(i\\leftrightarrow j\\)．このとき，\\(i,j\\) の推移性・零再帰性・正再帰性は一致する．特に，Markov 連鎖 \\(X\\) が既約ならば，全ての状態が同じ再帰性を持つ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nひとまず (Hairer, 2021, p. 2) 参照．\n\n\n\nこうして，既約な Markov 連鎖 \\(P\\) の再帰性が議論できるようになる．推移的であるか，零再帰的であるか，正に再帰的であるかのいずれかである．\n\n\n2.3 離散エルゴード定理\nMarkov 連鎖 \\(X\\) が再帰的であるためには，既約性と非周期性が十分条件である．加えて，極限が零測度でなければ，正に再帰的である．\n\n\n\n\n\n\n離散エルゴード定理3\n\n\n\n\\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\subset L(\\Omega;E)\\) をMarkov連鎖，\\(E\\) を可算集合とする． \\(X\\) が既約で非周期的ならば，次が成り立つ：\n\n任意の本質的な状態 \\(i\\in E\\) について，次が成り立つ： \\[\np^n_{ij}\\xrightarrow{n\\to\\infty}\\mu_j=\\frac{1}{\\operatorname{E}_j[\\tau_j]},\\qquad j\\in E.\n\\] 特に，任意の開始地点 \\(i\\in E\\) について，\\((p_{i-}^n)\\) は \\(\\mu\\) に全変動収束する．\n加えて \\(X\\) が正に再帰的であるならば，\\(\\mu:=\\{\\mu_i\\}_{i\\in\\mathcal{X}}\\) は \\(X\\) のただ一つの不変確率測度である．\n\\(X\\) が零再帰的である場合は \\(\\mu_i\\equiv0\\) であり，\\(X\\) の不変確率測度は存在しない．\n\n\n\n状態空間 \\(E\\) が有限である場合，正に再帰的＝エルゴード的ならば，必ず収束は（全変動ノルムに関して）指数速度で起こる．\nしかし，\\(E\\) が可算無限である場合，速度は様々である．\n\\(\\mathbb{N}\\) 上の待ち時間の Markov 連鎖が，その良い例となっている．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-process",
    "href": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-process",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "3 待ち時間の Markov 過程",
    "text": "3 待ち時間の Markov 過程\n\n3.1 過程の定義\n待ち時間の分布 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) は非零な１次の積率を持つとする．\n\\(\\nu\\) が定める再生過程を作り出す Markov 過程 \\(\\{X_t\\}\\) とは，離散時間の場合（第 2 節）と同様，\n\n\\(\\mathbb{R}^+\\) 上で \\(\\dot{X}_t=-1\\)．\n\\(X_t=0\\) のとき，次の瞬間 \\(\\nu\\) に従って選択されたある正の値にジャンプする．\n\nこのとき，次が成り立つ：\n\n\n\n\n\n\n命題\n\n\n\n上で定義した Markov 過程 \\(\\{X_t\\}\\) について，\n\n生成作用素は次で定まる： \\[\nLf(x)=-\\frac{d f(x)}{d x},\\qquad f\\in\\mathcal{D}(L),\n\\] \\[\n\\mathcal{D}(L):=\\left\\{f\\in\\mathcal{L}^1(\\nu)\\,\\middle|\\,f(0)=\\int^\\infty_0f(x)\\nu(dx)\\right\\}.\n\\]\n次で定まる確率分布 \\(\\mu_*\\in\\mathcal{P}(\\mathbb{R}_+)\\) は \\(\\{X_t\\}\\) に関して不変である： \\[\n\\mu_*(dx)=c\\nu([x,\\infty])dx,\n\\] \\[\nc:=\\int^\\infty_0y\\nu(dy)\\in(0,\\infty).\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n収束 \\[\n\\frac{P_tf(x)-f(x)}{t}\\to-\\frac{d f(x)}{d x}\\qquad t\\to\\infty,\n\\] は，\\(x\\ne0\\) の場合直ちに成り立つ．\nこれが \\(x=0\\) の場合も含めて一様に成り立つことと，\\((\\nu|f)=f(0)\\) は同値である．\n任意の \\(f\\in C_c^1(\\mathbb{R}_+)\\cap\\mathcal{D}(L)\\) について，次のようにして \\((\\mu_*|Lf)=0\\) が示せるためである： \\[\\begin{align*}\n(\\mu_*|Lf)&=-\\int^\\infty_0f'(x)\\mu_*(dx)=-c\\int^\\infty_0f'(x)\\nu([x,\\infty))\\,dx\\\\\n&=-c\\biggl[f(x)\\nu([x,\\infty))\\biggr]^\\infty_0-c\\int^\\infty_0f(x)\\nu(dx)=cf(0)-cf(0)=0.\n\\end{align*}\\]\n\n\n\n\n\n\n3.2 多項式エルゴード性\n\n\n\n\n\n\n定理（待ち時間の Markov 連鎖の多項式エルゴード性）\n\n\n\n待ち時間の分布は \\(\\nu\\ll\\ell_1\\) で密度 \\(p\\) をもち，ある \\(\\zeta&gt;2\\) が存在して \\(p\\) は \\(x^{-\\zeta}\\) のレートを持つとする： \\[\n\\frac{c_-}{x^\\zeta}\\le p(x)\\le\\frac{c_+}{x^\\zeta}\n\\] このとき，次の多項式エルゴード性が成り立つ： \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le C\\frac{x^\\alpha}{t^{\\alpha-1}},\n\\] \\[\nt\\ge0,x\\in\\mathbb{R}_+,\\alpha\\in(0,\\zeta-1).\n\\] 加えて，次が成り立つ： \\[\n\\lim_{t\\to\\infty}\\frac{\\log\\|P_t(x,-)-\\mu_*\\|_\\mathrm{TV}}{\\log t}=2-\\xi.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(V(x):=x^{\\alpha}\\;\\mathrm{on}\\;[1,\\infty)\\;(\\alpha&gt;0)\\) という形の関数であって，\\(V\\in\\mathcal{D}(L)\\) を満たすものが存在する．\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\alpha\\in(0,\\zeta-1)\\) を満たすように取れば， \\[\n\\int^\\infty_0V(x)\\nu(dx)=\\int^1_0V(x)p(x)dx+c_+\\int_1^\\infty x^{\\alpha-\\xi}dx&lt;\\infty\n\\] より，この値を \\(V(0)\\) とし，\\((0,\\infty)\\) 上で \\(C^1\\)-級になるように繋げば良い．\n\n\n\nこのとき， \\[\nLV(x)=-\\alpha x^{\\alpha-1}=-\\alpha V(x)^{1-\\frac{1}{\\alpha}}\\qquad\\mathrm{on}\\;[1,\\infty)\n\\] が成り立つ．即ち，Lyapunov関数 \\(\\varphi(x):=\\alpha x^{1-\\frac{1}{\\alpha}}\\) に関する劣線型ドリフト条件を満たす．スケルトンの議論を通じて，多項式エルゴード定理より，\\(\\frac{(1-1/\\alpha)}{1-(1-1/\\alpha)}=\\alpha-1\\) のレートで収束する： \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le C\\frac{\\lvert x\\rvert^\\alpha}{t^{\\alpha-1}}.\n\\] ここで \\(\\alpha\\in(0,\\zeta-1)\\) は任意の値であったから， \\[\n\\log\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le\\log C+\\alpha\\log x-(\\alpha-1)\\log t\n\\] \\[\n\\therefore\\qquad\\limsup_{t\\to\\infty}\\frac{\\log\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}}{\\log t}\\le\\limsup_{\\alpha&lt;\\zeta-1}-(\\alpha-1)=2-\\zeta.\n\\]\n最後の主張を示す．まず，\\(LV\\) は上に有界であるから， \\[\n\\frac{d }{d t}P_tV(x)=P_tLV(x)\\le C\n\\] \\[\n\\therefore\\qquad P_tV(x)\\le Ct+x^\\alpha=:g(x,t).\n\\] 続いて，\\(\\frac{c_-}{x^\\zeta}\\le p(x)\\) より，\\(\\mu_*\\) の密度は下から評価できる： \\[\n\\frac{\\mu_*(dx)}{dx}=\\int^\\infty_xp(y)\\,dy\\ge\\int^\\infty_x\\frac{c_-}{y^\\zeta}\\,dy=cx^{1-\\zeta}.\n\\] これより， \\[\n\\mu_*[V&gt;R]=\\mu_*[x&gt;R^{1/\\alpha}]\\ge CR^{-\\frac{\\zeta-2}{\\alpha}}=:f(R)\n\\] を得る．以上の評価とから， \\[\n\\frac{1}{2}\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\ge f(R)-\\frac{g(x,t)}{R}=CR^{-\\frac{\\zeta-2}{\\alpha}}-\\frac{\\lvert x\\rvert^\\alpha+Ct}{R}\n\\] \\(R&gt;0\\) について最適化することで， \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\ge C\\biggr(\\lvert x\\rvert^\\alpha+Ct\\biggl)^{\\frac{\\zeta-2}{\\zeta-2-\\alpha}}.\n\\] 同様にして，\\(\\alpha\\nearrow\\zeta-1\\) を考えることで結論が従う．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#参考文献",
    "href": "posts/2024/Process/ResidualWaitingTime.html#参考文献",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "4 参考文献",
    "text": "4 参考文献\n\n離散時間の場合は (Kulik, 2018, p. 22) 例 1.3.6, (Feller, 1967, p. 381) 例 XV.2.(k)，連続時間の場合は (Hairer, 2021, pp. 35–36) を参考にした．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "href": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Resnick, 2002, p. 174), (Mitov and Omey, 2014, p. 1), (Nummelin, 1984, p. 49) 定義4.2 など参照．計数過程の用語については (Aalen, 1978, p. 701) の解説も参照．↩︎\n一般には和訳「再生過程」が定着しているだろう．だが regeneration process ではなく，renewal process なのである．生死というよりは，再起というべきだと考えるため，ここでは再起過程と呼ぶこととする．最大の欠点は「再帰」と音が同じことである．↩︎\n(Kulik, 2018, p. 16) 定理1.2.5，(Robert and Casella, 2004, p. 224) を参照．↩︎"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html",
    "href": "posts/2024/Process/ZigZag.html",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "1 Zig-Zag 過程",
    "text": "1 Zig-Zag 過程\n\n1.1 はじめに\n１次元の Zig-Zag 過程は元々，Curie-Weiss 模型 における Glauber 動力学を lifting により非可逆化して得る Markov 連鎖の，スケーリング極限として特定された Feller-Dynkin 過程である (Bierkens and Roberts, 2017)．\n区分確定的 Markov 過程（PDMP）といい，ランダムな時刻にランダムな動きをする以外は，決定論的な動きをする過程である．\n\n\n\n\\(\\mathbb{R}^2\\) 上の Gauss 分布に収束する Zig-Zag 過程の軌跡\n\n\nPDMP の一般論については次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\nただし Zig-Zag 過程は，(Goldstein, 1951) で電信方程式と関連して，同様の過程が扱われた歴史もある．\n\n\n1.2 設定\nZig-Zag 過程 \\(Z=(X,\\Theta)\\) の状態空間は，力学的な立場に立って \\(E=\\mathbb{R}^d\\times\\{\\pm1\\}^d\\) と見ることが多い．\n力学における相空間と同様，\\(X\\in\\mathbb{R}^d\\) が位置を，\\(\\theta\\in\\{\\pm1\\}^d\\) が速度を表すと解する．すなわち，Zig-Zag 過程は全座標系と \\(45\\) 度をなす方向に，常に一定の単位速さで \\(\\mathbb{R}^d\\) 上を運動する粒子とみなせる．\nすなわち，\\((x,\\theta)\\in E\\) から出発する Zig-Zag 過程は，次の微分方程式系で定まる決定論的なフロー \\(\\phi_{(x,\\theta)}:\\mathbb{R}\\to\\mathbb{R}^d\\) に従って運動する粒子とみなせる： \\[\n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\\qquad \\frac{d \\Theta_t}{d t}=0.\n\\]\n\n\n1.3 アルゴリズム\n\n1.3.1 全体像\nZig-Zag 過程 \\(Z\\) は次のようにしてシミュレーションできる：\n\n\n\n\n\n\nZig-Zag 過程のシミュレーション\n\n\n\n\nレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) から定まる強度関数 \\[\nm_i(t):=\\lambda_i(x+\\theta t,\\theta),\\qquad i\\in[d],\n\\] を持つ，\\(d\\) 個の独立な \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程の，最初の到着時刻 \\(T_1,\\cdots,T_d\\) をシミュレーションする．\n最初に到着した座標番号 \\(j:=\\operatorname*{argmin}_{i\\in[d]}T_i\\) について，時刻 \\(T_j\\) に速度成分 \\(\\theta_j\\) の符号を反転させる．すなわち，関数 \\[\nF_j(\\theta)_i:=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n\\] に従ってジャンプする．\n１に \\(t=T_j\\) として戻って，くり返す．\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の跳躍測度\n\n\n\n\n\nもう一つ，MCMC の文脈で自然な見方は，状態空間を \\[\nE=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\}\n\\] と取る見方である．これは (Davis, 1993) による 一般の PDMP の設定 と対応する．\nこの \\(E\\) 上で，レート関数 \\[\n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n\\] が定める強度 \\[\nM(t):=\\lambda(x+t\\theta,\\theta)\n\\] を持った \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程に従ってジャンプが訪れる．\nこの点過程に対して，確率核 \\[\nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\\] に 印付けられた点過程 が，\\(Z\\) の跳躍測度である．\n\n\n\n\n\n\n\n\n\n証明（２つの定義の等価性）\n\n\n\n\n\nZig-Zag 過程に対する２つの定義を与えた．これら２つが同分布の過程を定めることは (Corbella et al., 2022), (Hardcastle et al., 2024) などさまざまなところで触れられているが，証明が必要である．\nまず，\\(\\min_{i\\in[d]}T_i\\) が，強度関数 \\(M\\) が定める到着時刻に同分布であることを示す．\n各 \\(T_i\\) の密度は \\[\np_i(t)=m_i(t)e^{-M_i(t)}1_{(0,\\infty)}(t)\n\\] で与えられ，\\(T_i\\) は互いに独立だから，\\((T_1,\\cdots,T_d)\\) の結合密度もわかる．\n\\(T_1,\\cdots,T_d\\) を昇順に並べた順序統計量を \\[\nT_{(1)}\\le\\cdots\\le T_{(d)}\n\\] で表すとする．この \\(d\\) 次元確率ベクトルの密度 \\(p\\) は， \\[\np(t_1,\\cdots,t_d)=1_{\\left\\{t_1\\le\\cdots\\le t_d\\right\\}}(t_1,\\cdots,t_d)\\left(\\sum_{\\sigma\\in\\mathfrak{S}_d}\\prod_{i=1}^dm_i(t_{\\sigma(i)})e^{-M_i(t_{\\sigma(i)})}\\right)\n\\] と計算できる．\nこの \\(p\\) を \\(t_2,\\cdots,t_d\\) に関して積分することで，\\(T_1\\) の密度が得られる：1 \\[\\begin{align*}\n    p_{(1)}(t)&=\\int_{(0,\\infty)^{d-1}}p(t_1,\\cdots,t_d)\\,dt_2\\cdots dt_d\\\\\n    &=\\biggr(\\sum_{i=1}^dm_i(t_1)\\biggl)\\exp\\left(-\\sum_{i=1}^dM_i(t_1)\\right)=m(t_1)e^{-M(t_1)}.\n\\end{align*}\\]\nこれは確かに，強度関数 \\(m\\) が定める到着時刻の密度である．\n続いて，\\(j=\\operatorname*{argmin}_{i\\in[d]}T_i\\) の，\\(\\min_{i\\in[d]}T_i\\) に関する条件付き確率質量関数が \\[\nq(i|t)=\\frac{m_i(t)}{\\sum_{i=1}^dm_i(t)}\n\\] であることを示す．\nそのためには，任意の \\(i\\in[d]\\) と \\(A\\in\\mathcal{B}(\\mathbb{R}^+)\\) とに関して \\(\\left\\{T_{(1)}\\in A,T_{(1)}=T_i\\right\\}\\) という形の事象を計算し，密度が積の形で与えられることを見れば良い．\n\\[\\begin{align*}\n    &\\qquad\\operatorname{P}[T_{(1)}\\in A,T_{(1)}=T_i]\\\\\n    &=\\operatorname{P}[T_i\\in A,\\forall_{j\\ne i}\\;T_i\\le T_j]\\\\\n    &=\\int_Ap_i(t_i)\\,dt_i\\left(\\sum_{\\sigma\\in\\mathrm{Aut}([d]\\setminus\\{i\\})}\\int^\\infty_{t_i}p_{\\sigma(1)}(t_{\\sigma(1)})\\,dt_{\\sigma(1)}\\int^\\infty_{t_{\\sigma(1)}}p_{\\sigma(2)}(t_{\\sigma(2)})\\,dt_{\\sigma(2)}\\cdots\\int^\\infty_{t_{\\sigma(d-1)}}p_{\\sigma(d)}(t_{\\sigma(d)})\\,dt_{\\sigma(d)}\\right)\\\\\n    &=\\int_Am_i(t_i)\\exp\\left(-\\sum_{i=1}^dm_i(t_i)\\right)\\,dt_i\\\\\n    &=\\int_A\\frac{m_i(t_i)}{m(t_i)}m(t_i)e^{-M(t_i)}\\,dt_i.\n\\end{align*}\\]\nよって，\\(\\min_{i\\in[d]}T_i\\) と \\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) とに関する結合密度は，2 \\[\nq(i|t)p_{(1)}(t)\n\\] という積の形で与えられることがわかった．\n\n\n\n\n\n\nまとめ\n\n\n\n\n前述の定義は，\\(\\min_{i\\in[d]}T_i\\) の形で密度 \\(p_{(1)}\\) からシミュレーションし，\\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) の形で \\(q\\) からシミュレーションしている．\n後述の定義は，\\(p_{(1)}(t)\\) から直接シミュレーションし，再び \\(q(i|t)\\) から直接シミュレーションをする．\n\n１が２に等価であることがわかった．\n\n\n\n\n\n\n\n1.3.2 到着時刻 \\(T_i\\) のシミュレーション方法\nZig-Zag 過程のシミュレーションは，ほとんど強度\n\\[\nM_i(t):=\\int^t_0m_i(s)\\,ds\n\\] を持つ非一様 Poisson 点過程のシミュレーションに帰着される．\n実はこれは，指数分布確率変数 \\(E_i\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1)\\) について \\[\nT_i\\overset{\\text{d}}{=}M_i^{-1}(E_i)\n\\] と求まる．\n\n\n1.3.3 Poisson 剪定\n仮にこの逆関数 \\(M_i^{-1}\\) が得られない場合でも，剪定 (Lewis and Shedler, 1979) によって \\(T_i\\) は正確なシミュレーションが可能である．\nこの方法は，\\(M_i^{-1}\\) を数値的に計算するよりも遥かに速い．これは \\(M_i\\) の定義に積分が存在し，これが多くの場合高次元になるためである．\n\n\n\n1.4 レート関数の条件\nZig-Zag 過程 \\(Z\\) がどのような分布に従うかは，全てレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) に委ねられている．\n\n\n\n\n\n\nZig-Zag 過程のレート関数 \\(\\lambda_1,\\cdots,\\lambda_d:E\\to\\mathbb{R}_+\\) は，負の対数密度 \\(U\\in C^1(\\mathbb{R}^d)\\) に対して， \\[\n\\lambda_i(x,\\theta):=(\\theta_i\\partial_iU(x))_++\\gamma_i(x,\\theta_{-i})\\quad(i\\in[d])\n\\] と定める．\nただし，次を仮定する：\n\n\\(\\gamma_i:E\\to\\mathbb{R}_+\\) は，\\(\\theta_i\\) のみには依らない任意の非負連続関数3 \\[\n  \\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta)).\n  \\]\n\\(e^{-U}\\in\\mathcal{L}^1(\\mathbb{R}^d)\\) が成り立ち，\\(\\pi(dx)\\,\\propto\\,e^{-U(x)}dx\\) が確率測度を定める．\n\\(M_i\\) は \\(t\\to\\infty\\) の極限で発散する： \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\]\n\n\n\n\n\n\n\n\n\n\n注（細かい条件たちについて）\n\n\n\n\n\nまた， \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\] は \\(t\\to\\infty\\) の極限で発散する必要がある．\nさもなくば，\\(M_i:(0,L)\\to(0,\\infty)\\;(L\\in(0,\\infty])\\) の形で定まらず，\\(M_i\\) がこのような可微分同相を与えない場合は \\[\nT_i:=M_i^{-1}(E_i),\\qquad E_i\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1),\n\\] によるシミュレーションも不正確になる．\n\n\n\n\n\n\n\n\n\n(Bierkens et al., 2019, pp. 1294 定理2.2)\n\n\n\n上述のリフレッシュレート \\(\\lambda_1,\\cdots,\\lambda_d\\) に対して，定義 1.3 で定まる Zig-Zag 過程 \\(Z\\) は次の分布 \\(\\widetilde{\\pi}=\\pi\\otimes\\mathrm{U}(\\{\\pm1\\}^{d})\\in\\mathcal{P}(E)\\) を不変にする： \\[\n\\widetilde{\\pi}(dxd\\theta)=\\frac{1}{2^d}\\frac{e^{-U(x)}}{\\mathcal{Z}}\\,dxd\\theta\n\\]\n\n\n\n\n\n\n\n\n注（拡張の可能性について）\n\n\n\n\n\n\\(\\{\\pm1\\}^d\\) 上の周辺分布が一様分布になっていること，勾配ベクトル \\(DU\\) の情報のみを使っており，座標に沿った方向しか見ていないため \\(U\\) の異方性に大きく左右されること，これらが「必ずしもそうある必要はない」拡張可能な点である．\n\n\n\n\n\n1.5 エルゴード性の条件\n\\(\\pi\\) が不変分布になるための十分条件 1.4 は極めて緩かったが，MCMC として使えるためにはエルゴード性が成り立つ必要がある．\n\n\n\n\n\n\n(Bierkens and Roberts, 2017 定理５)\n\n\n\n\\(d=1\\) で，レート関数 \\(\\lambda:E\\to\\mathbb{R}_+\\) はある \\(x_0&gt;0\\) が存在して次を満たすとする： \\[\n\\inf_{x\\ge x_0}\\lambda(x,1)&gt;\\sup_{x\\ge x_0}\\lambda(x,-1),\n\\] \\[\n\\inf_{x\\le-x_0}\\lambda(x,-1)&gt;\\sup_{x\\le-x_0}\\lambda(x,1).\n\\]\nこのとき，ある関数 \\(f:E\\to[1,\\infty)\\) が存在して \\(f(x,\\theta)\\to\\infty\\;(\\lvert x\\rvert\\to\\infty)\\) が成り立ち，かつ \\[\n\\left\\|P^t\\left((x,\\theta),-\\right)-\\pi\\right\\|_\\mathrm{TV}\\le\\kappa f(x,\\theta)\\rho^t,\\qquad(x,\\theta)\\in E,t\\ge0,\\rho\\in(0,1).\n\\]\n\n\n\n\n1.6 Subsampling Technology\nZig-Zag 過程はレート関数 \\(\\lambda\\) の設計に大きな自由度があった（第 1.4 節）．\nこれにより，Zig-Zag 過程ではバイアスを導入しない subsampling が可能であり，これを通じて データサイズに依らない一定効率での事後分布サンプリングが可能になる という super-efficiency (Bierkens et al., 2019) と呼ばれる性質を持つ．\nこの性質が実用上は最も重要である．詳しくは，次の記事を参照：\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#シミュレーション",
    "href": "posts/2024/Process/ZigZag.html#シミュレーション",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "2 シミュレーション",
    "text": "2 シミュレーション\n\n2.1 １次元での例\nZigZag サンプラーは非対称なダイナミクスを持っており，その点が MALA (Metropolis-adjusted Langevin Algorithm) や HMC (Hamiltonian Monte Carlo) などの従来手法と異なる．\n１次元でその違いを確認するために，Cauchy 分布という裾の重い分布を用いる．Cauchy 分布 \\(\\mathrm{C}(\\mu,\\sigma)\\) は次のような密度を持つ： \\[\nf(x)=\\frac{1}{\\pi\\sigma}\\frac{1}{1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n\\]\nその裾の重さ故，平均も分散も存在しない（発散する）．\nこのとき，次のような観察が得られる：\n\n\n\n\n\n\n\nZigZag サンプラーは Cauchy 分布に対して，最頻値から十分遠くから開始しても，高速に最頻値に戻ってくる．\nMALA は diffusive behaviour が見られ，最頻値に戻るまでに時間がかかる．\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する ZigZag サンプラーの軌道\n\n\nMALA と比較すると，その再帰の速さが歴然としている：4\n\n\nCode\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing Distributions\nusing Random\n\nRandom.seed!(2)\n\n# Define the target distribution (1D Cauchy) using the `LogDensityProblem` interface\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Choose initial parameter value for 1D\ninitial_θ = [500.0]\n\n# Use automatic differentiation to compute gradients\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensityCauchy(0.0, 1.0))\n\n# Set up the sampler with a multivariate Gaussian proposal.\nσ² = 100\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\n# Sample from the posterior.\nchain = sample(model_with_ad, spl, 2000; initial_params=initial_θ, chain_type=StructArray, param_names=[\"θ\"])\n\n# plot\nθ_vector = chain.θ\nsample_values = zip(1:length(θ_vector), θ_vector)\np2 = plot_1dtraj(sample_values, title=\"1D MALA Sampler (Cauchy Distribution)\", markersize=0, ylim=(-30, 750), label=\"MALA_1D\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する MALA サンプラーの軌道\n\n\n２つを並べて比較すると，Langevin ダイナミクスの方は，少し diffusive な動き（random walk property と呼ばれる）が見られることがわかる．\n\n\n\n\n\n\n例（NUTS サンプラーの動き）\n\n\n\n\n\nNUTS サンプラーはステップサイズを極めて大きくするため，プロットによるダイナミクスの比較があまり意味を持たなくなってくる．\n実際見てみると恐ろしいものである：\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する NUTS サンプラーの軌道\n\n\n\n\n\n\n\n2.2 ２次元での例\n\n\n\n\n\n\n\n勾配 \\(-\\nabla\\log\\pi\\) を計算し，∇ϕ(x, i, Γ) の形で定義する．\n\n\n\n\n\\[\n\\Sigma^{-1}=\\begin{pmatrix}2&-1\\\\-1&2\\end{pmatrix}\n\\]\nで定まる分散共分散行列 \\(\\Sigma\\) を持った中心化された正規分布 \\(\\pi(x)dx=\\mathrm{N}_2(0,\\Sigma)(dx)\\) に対しては，対数尤度は\n\\[\\begin{align*}\n    \\log\\pi(x)&=-\\log\\left((2\\pi)^{d/2}(\\det\\Sigma)^{1/2}\\right)\\\\\n    &\\qquad\\quad-\\frac{1}{2}x^\\top\\Sigma^{-1}x\n\\end{align*}\\]\nであるから，\\(\\phi:=-\\log\\pi\\) の第 \\(i\\) 成分に関する微分は\n\\[\\begin{align*}\n    \\partial_i\\phi(x)&=\\frac{\\partial }{\\partial x_i}\\biggr(\\frac{1}{2}x^\\top\\Sigma^{-1}x\\biggl)\\\\\n    &=\\Sigma^{-1}x.\n\\end{align*}\\]\n\nusing ZigZagBoomerang\nusing SparseArrays\n\nd = 2\n\n# 対数尤度関数 ϕ の第 i 成分に関する微分を計算\n1Γ = sparse([1,1,2,2], [1,2,1,2], [2.0,-1.0,-1.0,2.0])\n2∇ϕ(x, i, Γ) = ZigZagBoomerang.idot(Γ, i, x)\n\n# 初期値\nt0 = 0.0\nx0 = randn(d)\nθ0 = rand([-1.0,1.0], d)\n\n# Rejection bounds\nc = 1.0 * ones(length(x0))\n\n# ZigZag 過程をインスタンス化\nZ = ZigZag(Γ, x0*0)\n\n# シミュレーション実行\nT = 20.0\nzigzag_trace, (tT, xT, θT), (acc, num) = spdmp(∇ϕ, t0, x0, θ0, T, c, Z, Γ; adapt=true)\n\n# 軌跡を離散化\ntraj = collect(zigzag_trace)\n\n\n1\n\n勾配関数∇ϕの計算のためには，共分散行列の逆（精度行列ともいう）をSparseMatrixCSC型で指定する必要があることに注意．idotの実装 も参照．\n\n2\n\nidotは，疎行列Γの第i列と，疎ベクトルxとの内積を高速に計算する関数．\n\n\n\n\n\n\n\n\n\n\nidotの定義\n\n\n\n\n\nidot(A,j,u)は，疎行列Aの第j列と，疎ベクトルuとの内積を高速に計算する関数である．\n1function idot(A::SparseMatrixCSC, j, x)\n2    rows = rowvals(A)\n3    vals = nonzeros(A)\n    s = zero(eltype(x))\n4    @inbounds for i in nzrange(A, j)\n5        s += vals[i]'*x[rows[i]][2]\n    end\n    s\nend\n\n1\n\nパッケージ内部で，位置 \\(x\\in\\mathbb{R}^d\\) は全て SparseSate 型に統一されている？\n\n2\n\n疎行列 A の行インデックスを取得．rowvals(A)はベクトルであり，第１列から順番に，非零要素のある行番号が格納されている．\n\n3\n\n非零要素の値が格納されている．\n\n4\n\n@inbounds は，範囲外アクセスを許容するマクロ．高速化のためだろう．nzrange は，A の第 j 列に非零要素がある範囲を，第 \\(1\\) 列から累積して何番目かで返す．すなわち，rows[i]で正確に第j列の非零要素の行番号を狙い撃ちしてイテレーションできる．\n\n5\n\nxの非零要素がある行番号 rows[i] における成分の値 u[rows[i]][2] はこのような表記になる．これと，A の非零要素 vals[i] との内積を計算．\n\n\nなお，通常の行列に対しては，次のように実装されている：\nidot(A, j, x) = dot((@view A[:, j]), x)"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "3 Zig-Zag サンプラーの実装",
    "text": "3 Zig-Zag サンプラーの実装\n\nZigZagBoomerang の実装を紹介する．\nJulia の MCMC パッケージ一般については次の稿を参照：\n\n    \n        \n            \n            \n                Julia による MCMC サンプリング\n                新時代の確率的プログラミング環境の構築に向けて"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#文献紹介",
    "href": "posts/2024/Process/ZigZag.html#文献紹介",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\nZig-Zag Sampler を導入したのは (Bierkens et al., 2019) であるが，ざっと仕組みを把握をしたいならば (Corbella et al., 2022) の第二章がよっぽどわかりやすいだろう．"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#footnotes",
    "href": "posts/2024/Process/ZigZag.html#footnotes",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n計算過程は省略したが，\\(d=2\\) の場合と，\\(d=3\\) の場合を少しやってみると良い．↩︎\n参照測度は，\\([d]\\) 上のものは計数測度 \\(\\#\\) をとっている．↩︎\n従って，レート関数 \\(\\lambda\\) は連続とする．この関数 \\(\\gamma_i\\) は，\\(U\\) の情報には依らない追加のリフレッシュ動作を仮定に加える．実際，\\(\\lambda_i(x,\\theta)-\\lambda_i(x,F_i(\\theta))=\\theta_i\\partial_iU(x)\\) である限り，\\(\\theta\\) と \\(F_i(\\theta)\\) の往来には影響を与えず釣り合っているため，どのような \\(\\gamma_i\\) をとっても，平衡分布には影響を与えない．しかし，高くするごとにアルゴリズムの対称性が上がるため，\\(\\gamma\\equiv0\\) とすることが Monte Carlo 推定量の漸近分散を最小にするという (Andrieu and Livingstone, 2021)．(Bierkens et al., 2021) でも同様の洞察がなされている．↩︎\n(Bierkens et al., 2019) にある提示の仕方である．Zig-Zag の 2000 単位時間を単純に MALA と比較はできないと筆者も考えるが，ダイナミクスに注目していただきたい．実際，自分で実装してみると，シード値をいじらないと，Zig-Zag は必ずしも 500 単位時間前後でモード \\(0\\) に戻るわけではない．↩︎\n実は６つ持つ．他の初期値は σ=(Vector(diag(Γ))).^(-0.5); λref=0.0, ρ=0.0↩︎"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html",
    "href": "posts/Surveys/SMCSamplers.html",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nSMC の文脈で，目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) が複雑であるとき，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをする，というアイデアを 調温 (tempering) という（粒子フィルターの稿 も参照）．\nこの tempering という考え方は本質的に逐次的な発想を持っているが，元々は SMC の文脈とは全く独立に，MCMC を多峰性を持つ複雑な分布に対しても使えるように拡張する研究で提案された．これらの手法が自然と SMC へと接続する様子を population Monte Carlo (Iba, 2001b), (Ajay Jasra et al., 2007) というキーワードで理解されている．\nまずはその歴史を概観する．いずれも，目標分布 \\(\\pi_p\\) が多峰性をもち，MCMC がうまく峰の間を遷移できずに正しいサンプリングができない（収束が遅くなる）問題を解決する文脈の中で捉えられる．\n\n\nこれは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3\n\n\n\nMCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15\n\n\n\n\n\n\ntempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nこの方法は混合モデルにおいて事後分布が多峰性を持つなどして Gibbs サンプラーがうまく収束しない場合でも，有効な MCMC サンプラーとなる (A. Jasra et al., 2005)．\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，18 拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．19 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw(X_{1:p}):=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．20\n従って，本当は \\(E^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．21\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．\n\n\n\n\n\n\n\n簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．\n\n\n\n\n\n\n\n\n目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-SA",
    "href": "posts/Surveys/SMCSamplers.html#sec-SA",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "これは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "href": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "MCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "href": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nこの方法は混合モデルにおいて事後分布が多峰性を持つなどして Gibbs サンプラーがうまく収束しない場合でも，有効な MCMC サンプラーとなる (A. Jasra et al., 2005)．\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，18 拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．19 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw(X_{1:p}):=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．20\n従って，本当は \\(E^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．21\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "href": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#その他の手法",
    "href": "posts/Surveys/SMCSamplers.html#その他の手法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#footnotes",
    "href": "posts/Surveys/SMCSamplers.html#footnotes",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの前にも，Umbrella sampling (Torrie and Valleau, 1977) が本質的には密度の調温のアイデアを用いていた．(Liu, 2004, pp. 206–) Section 10.1 も参照．↩︎\n分子動力学 (molecular dynamics) などの文脈では Metropolis 法はちょうど分子運動のシミュレーションになっていることを踏まえれば，これを simulated annealing と呼ぶのは極めて鮮やかなアナロジーとなっている．焼きなまし法自体も，シミュレーション可能になったのである．↩︎\n(Geman and Geman, 1984) によると，各 \\(\\pi_n\\) における MCMC move の回数を \\(N_n\\) とした場合，\\(O(\\log(N_1+\\cdots+N_n))\\) のオーダーで \\(T_n\\) を（十分遅く）変化させれば，この手法はほとんど確実に \\(\\operatorname*{argmin}h\\) 内に収束する．(Liu, 2004, pp. 209–) 10.2 節も参照．↩︎\n(岡本祐幸, 2010), (Iba, 2001a) など．↩︎\n(Liu, 2004, pp. 205–) Chapter 10. Multilevel Sampling and Optimization Methods も参照．↩︎\n(Liu, 2004, p. 207) も参照．↩︎\n(Chopin et al., 2023), (Liu, 2004, p. 4) でも (Geyer, 1991) を引用して PT と呼んでいる．一方で物理学の分野では (Hukushima and Nemoto, 1996) の exchange Monte Carlo や (Swendsen and Wang, 1986) などの文献もある．前者は (Liu, 2004, p. 4) が “is reminiscent of parallel tempering (Geyer, 1991)” と指摘しており，後者は (Bouchard-Côté et al., 2012) などが引用している．↩︎\n最終講義 スピングラスと計算物性物理 p.34 も参照．温度の違う熱浴につけたレプリカをシミュレートして，時々交換する，という見方ができるためにこう呼ぶ．↩︎\n(Ajay Jasra et al., 2007) は (Geyer, 1991) を指して population-based MCMC と呼んでおり，SMC も含めて population-based simulation と呼んでいる．population-based という言葉自体は (Iba, 2001b) からとったという．“we define a population-based simulation method as one which, instead of sampling a single (independent/dependent) sample, generates a collection of samples in parallel” と定義しており，大きく MCMC によるものと逐次重点サンプリングベースのものの２流儀あるとしている．(Liu, 2004, pp. 225–) 第11章なども参照．↩︎\n(岡本祐幸, 2010) など．↩︎\n(Behrens et al., 2012, p. 66) も参照．↩︎\n(Iba, 2001a) が良い解説を与えていると (Ajay Jasra et al., 2007) でも言及されている．ただし，(Iba, 2001a) はこの並行テンパリングだけでなく，模擬テンパリング，multicanonical Monte Carlo (Berg and Neuhaus, 1991) / Adaptive Umbrella Sampling (Torrie and Valleau, 1977) を総称して 拡張アンサンブル法 (Extended Ensemble Monte Carlo) と呼んでサーベイしていることに注意．↩︎\n(Lyubartsev et al., 1992) が引用されることもある．(酒井佑士, 2017), (岡本祐幸, 2010) など．method of expanded ensemble とも呼ばれる (岡本祐幸, 2010), (Iba, 2001a)．↩︎\n記法 \\([p]=\\{1,\\cdots,p\\}\\) は 本サイトの数学記法一覧 を参照↩︎\nその後すぐに分子シミュレーションの分野にも導入された．(岡本祐幸, 2010) も参照．↩︎\n(Behrens et al., 2012) も参照．↩︎\n(Chopin and Papaspiliopoulos, 2020, p. 33) で，SMC を調温に初めて応用した論文として紹介されている．p.352 では “An early version of SMC tempering (without resampling)” としている．↩︎\n\\(\\pi_p(x_p)/\\pi_0P_1P_2\\cdots P_p\\) は多くの場合計算不能である．↩︎\nただし，\\(P_i^{-1}\\) とは，\\[ P_i(x_{i-1},x_i)\\pi_{i-1}(x_i-1)=\\pi_i(x_i)P_i^{-1}(x_{i-1},x_i) \\] で定まる確率核とした．\\(\\otimes\\) の記法はこちらも参照．↩︎\nこのウェイトの表示は，\\(P_i^{-1}/P_i=\\pi_{i-1}/\\pi_i\\) が成り立つことから直ちに従う．↩︎\n(Doucet et al., 2022) も参照．↩︎"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#現在",
    "href": "static/CV/cv_Japanese.html#現在",
    "title": "司馬博文（しばひろふみ）",
    "section": "8/28/2024 現在",
    "text": "8/28/2024 現在\n総合研究大学院大学（統計科学コース）５年一貫博士課程２年目．\n\n日本語，中国語が母語で，英語も話せる（TOEFL iBT 100点）．Python, R でのコーディング経験３年以上，Julia １年以上．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#研究分野",
    "href": "static/CV/cv_Japanese.html#研究分野",
    "title": "司馬博文（しばひろふみ）",
    "section": "研究分野",
    "text": "研究分野\n\n輸送によるサンプリング法\n特に，シュレーディンガー橋や正規化フローなどの生成モデリング手法．\nモンテカルロ法\n特に，逐次モンテカルロ法（SMC）やマルコフ連鎖モンテカルロ法（MCMC）などのベイズ統計計算アルゴリズム．\n統計モデリング\n特に，政治学，疫学，惑星地球科学などの分野への応用．\nベイズ機械学習\n特に，ガウス過程や階層モデリング，ノンパラメトリクスなど．\nデータ駆動科学\n特に，データ埋め込み，可視化，軌道推定，データ同化など．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#学歴",
    "href": "static/CV/cv_Japanese.html#学歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "学歴",
    "text": "学歴\n\n博士（統計科学）. 総合研究大学院大学先端学術院統計科学コース. 2023.4 – 2028.3\n指導教員：鎌谷研吾教授，矢野恵佑准教授\n学士（理学）. 東京大学理学部数学科. 2019.4 – 2023.3\n指導教員：吉田朋広教授"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#職歴",
    "href": "static/CV/cv_Japanese.html#職歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "職歴",
    "text": "職歴\n\nリサーチ・アシスタント. 統計数理研究所. 2023.7 – 現在\n確率過程の統計推測のための R パッケージである YUIMA の開発などを通じ，モンテカルロ法とベイズ統計の応用に取り組む．\n連携研究員. 東京大学先端科学技術研究センター. 2023.4 – 現在\n信頼できる AI と機械学習の視点からの経済安全保障の研究．\nデータサイエンティスト. IMIS 研究所. 2022.8 – 2024.1\n製造業のクライアントに対して統計分析と機械学習のソリューションを提供．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#研究滞在",
    "href": "static/CV/cv_Japanese.html#研究滞在",
    "title": "司馬博文（しばひろふみ）",
    "section": "研究滞在",
    "text": "研究滞在\n\nユニバーシティ・カレッジ・ロンドン，イギリス．2024.11.4 – 2024.12.2\n受入教員：Alexandros Beskos 教授"
  },
  {
    "objectID": "static/English.html",
    "href": "static/English.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Notations | Categories | All Posts\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\n\n\nWe examine how to find & formally determine the likelihood function of hierarchical models. As a real-world example, we consider the ideal point model, also known as the 2-parameter logistic item response model. \n\n\n\n\n\nDec 23, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\nIdeal point models are 2-parameter item response model, tailored to the purpose of visualizing / measuring the ideological positions of the legislators / judges. [@Bafumi+2005] introduced a hierarchical structure to the model to deal with the problem of identifiability. In this article, we re-examine the model and show that the posterior distribution of the parameters (ideal points) is still bimodal, indicating its weak identifiability. \n\n\n\n\n\nDec 22, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\nFeb 25, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\nJan 5, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\nDec 1, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Notations.html#sec-set",
    "href": "static/Notations.html#sec-set",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "1 集合",
    "text": "1 集合\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nここでは，あらゆる数学概念は，ZFC公理系 の下で集合として定義する．1 記号 \\(:=\\) は「右辺によって左辺を定義し，その結果等号が成り立つ」という主張の略記である．2\n\n1.1 集合\n\n空集合 を \\[\n\\emptyset:=\\{x\\mid x\\ne x\\}\n\\] で表す．3\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．4\n\\(A,B\\subset X\\) の 差 を \\[\nA\\setminus B:=\\left\\{a\\in A\\mid a\\notin B\\right\\}\n\\] で表す．\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cup B\\) と同じ数学的対象であるが，同時に \\(A\\cap B\\) という事実も主張するものとする．5\n対称差 を \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\] で表す．6\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．7 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．8\n例えば，\\(X\\in\\mathcal{L}(\\Omega)\\) を実確率変数，\\(A\\in\\mathcal{B}(\\mathbb{R})\\) を Borel 集合とすると， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] という略記を用いる．\n\n\n\n1.2 構成\n\n自然数 を \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\},\n\\] によって帰納的に定義する．9\n自然数の集合を表すため，次の記法を用意する：10 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，11 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す： \\[\n\\mathbb{R}_+=[0,\\infty),\\quad\\mathbb{R}^+=(0,\\infty).\n\\]\n部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．12\n実数 \\(x\\in\\mathbb{R}\\) に対して，その整数部分を \\[\n\\lfloor x\\rfloor:=\\max\\{n\\in\\mathbb{Z}\\mid n\\le x\\}\n\\] と表す．13\n次の演算規則を約束する：14 \\[\n\\prod_\\emptyset=1,\\quad\\sum_{\\emptyset}=0.\n\\]\n\\(0!=1\\) とする．\n\\(n\\)-組 を次のように帰納的に定める：15 \\[\n(x_1,x_2):=\\{\\{x_1\\},\\{x_1,x_2\\}\\},\n\\] \\[\n(x_1,\\cdots,x_n):=(x_1,(x_2,\\cdots,x_n)).\n\\]\n自然数の組を表すため，次の記法を用意する：16 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組を \\[\nX_{1:N}:=(X_1,\\cdots,X_N)\n\\] と表す．17\n\n\n\n1.3 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n写像 \\(f\\) の 値域 を \\[\\mathrm{Im}\\,f:=f(X)\\] で表す．\n\\(A\\subset X\\) への 制限 を \\(f|_A:A\\to Y\\) と表す．18\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．19\n\\(f_*\\) は部分集合 \\(A\\subset X\\) を像 \\(f(A)\\subset Y\\) に対応させる写像 \\[\nf_*:P(X)\\to P(Y)\\] と定義する．\n同様に写像 \\(f^*:P(Y)\\to P(X)\\) を定める： \\[\nf^*(B)=f^{-1}(B)\\quad(B\\subset Y).\n\\]\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．20\n部分集合 \\(A\\subset X\\) の 指示関数 といった場合は \\[\n\\sigma_A:=0\\cdot1_A+\\infty\\cdot1_{A^\\complement}\n\\] を表す．21\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．22\n写像 \\(f:X\\to Y\\) のうち，有限個の元を除いて \\(f(x)=0\\) を満たすものがなす全体を \\[\nY^{(X)}:=\\left\\{f\\in Y^X\\mid f=0\\;\\;\\text{f.e.}\\right\\}\n\\] と表す．23\n\\(P(X)\\) を \\(2^X\\) と同一視する．特に，\\(X\\) の有限部分集合の全体を \\[\n2^{(X)}=\\left\\{A\\in P(X)\\:\\middle|\\: A\\overset{\\text{finite}}{\\subset}X\\right\\}\n\\] と表す．24\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．25\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\[\n\\mathrm{pr}_i:\\prod_{i\\in I}X_i\\twoheadrightarrow X_i\n\\] で表す．26\n\\(x\\in X\\) での 評価写像 を \\[\n\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\n\\] で表す．27\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．\nしかしこの写像の値域も 族 と呼び，この場合は \\[\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\] と表す．28\n特に \\(I=\\mathbb{N}\\) のときは 列 ともいう．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．29\n\n\n\n1.4 圏\n\n集合の圏 を \\(\\mathrm{Set}\\) で表す．\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X)\n\\] を表す．30\n確率空間と確率核の圏を \\(\\mathrm{Stoch}\\) で表す．31\n圏 \\(C\\) の対象 \\(X,Y\\in C\\) の間の 射 の全体を \\(\\mathrm{Hom}_C(X,Y)\\) で表す．32\n特に \\[\nY^X=\\mathrm{Map}(X,Y)=\\mathrm{Hom}_\\mathrm{Set}(X,Y).\n\\]\n圏 \\(C\\) の対象 \\(X\\in C\\) の自己射の全体を \\[\n\\mathrm{End}_C(X):=\\mathrm{Hom}_C(X,X)\n\\] で表す．\nそのうち可逆なもののなす部分集合を \\(\\mathrm{Aut}_C(X)\\) で表す．集合 \\([n]\\) の 置換群 は \\(\\mathrm{Aut}_\\mathrm{Set}([n])\\) と表せる．\n\n\n\n1.5 関数\n\n\\(R\\) を環とする．\\(f_1,f_2\\in R^X\\) に対して， \\[\n(f_1+f_2)(x):=f_1(x)+f_2(x),\n\\] \\[\n(f_1f_2)(x):=f_1(x)f_2(x)\n\\] で定める演算により \\(R^X\\) も環とみなし，定値関数 \\(f\\equiv a\\in R\\) を \\(R\\) の元と同一視する．33\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を \\[\na\\lor b:=\\sup\\{a,b\\},\n\\] \\[\na\\land b:=\\inf\\{a,b\\},\n\\] で表す．34\n\\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right)\n\\] と表す．35\n\\(0\\) を持つ束においては次の略記を使う：36 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序集合 \\(Y\\) に値を取る関数 \\(f,g\\in\\mathrm{Map}(X,Y)\\) について，\\(f\\le g\\) とは \\[\n\\forall_{x\\in X}\\;f(x)\\le g(x)\n\\] の略記とする．\n同じ条件を，一階の量化記号 \\(\\forall\\) を省略して \\[\nf(x)\\le g(x)\\quad(x\\in X)\n\\] または \\(f\\le g\\) とも略記する．\n\\(Y\\) が束であるとき，この順序により関数の空間 \\(\\mathrm{Map}(X,Y)\\) は束となり，演算 \\(\\land,\\lor\\) が定まる．\n関数の列 \\(\\{f_n\\}\\subset Y^X\\) について，\\(f_n\\nearrow f\\) とは，収束 \\(f_n\\to f\\) だけでなく，\\(\\{f_n\\}\\) が単調増加であることも含意する．37\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\[\nO(g(x))\\;(x\\to x_0)\\] とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．38\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の意味でも使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0\n\\] を満たすこととする．"
  },
  {
    "objectID": "static/Notations.html#sec-space",
    "href": "static/Notations.html#sec-space",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "2 空間",
    "text": "2 空間\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．39\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．40\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．\n閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}\n\\] で表す．\n\n\n\n2.2 線型空間\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．41\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．42 \\(S_n(\\mathbb{R})_+\\) で半正定値なものの全体を表す．43\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}\n\\] とも表す．44\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，45 共役転置を \\(A^*\\) で表す．46 \\(\\mathbb{F}=\\mathbb{C}\\) の場合は \\(A^\\top=A^*\\)．\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．47\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について， \\[\n\\begin{align*}\n  A&+B\\\\\n  &\\quad:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n  \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\},\n\\end{align*}\n\\] と表す．48\n集合 \\(A\\subset X\\) の 凸包 を \\(\\operatorname{Conv}(A)\\) で表す．49\n集合 \\(A\\subset X\\) が生成する部分空間を \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x\n\\] で表す．50\n内積を \\((-|-)\\) で表す．51\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を52 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\operatorname{Tr}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}\n\\] で表す．53\n\n\n\n2.3 Banach 空間\n\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．54 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n特に \\(J\\) が有限であるとき， \\[\n  \\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n  \\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．55\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．56\n距離空間 \\((T,d)\\) の 開球 を \\[\n\\begin{align*}\n  U_\\epsilon(t)&:=U(t;\\epsilon)\\\\\n  &:=\\left\\{s\\in T\\mid d(s,t)&lt;\\epsilon\\right\\}\n\\end{align*}\n\\] で表す．57\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．58\n単位閉球を \\(B:=B(0;1)\\) で表す．\n\\(\\mathbb{R}^n\\) のものである場合は特に \\(B^n\\) とも表す．59\n\\(\\mathbb{R}^n\\) の標準基底を \\[\ne_i=(0,\\cdots,0,1,0,\\cdots,0)\n\\] と表す．60\nBanach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．61\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}\n\\] で表す．62\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) として理解できる数学的対象の別記法と捉えられるように設計する．63\n\n\n2.4 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．64\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．65\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．66\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．67\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．68\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．69\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．70\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．71\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．72\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．73 \\(\\gamma_n:=\\mathrm{N}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n2.5 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．74 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．75\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．76\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．77\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．78\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．79\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．80\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．81\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．82\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．83"
  },
  {
    "objectID": "static/Notations.html#sec-kernel",
    "href": "static/Notations.html#sec-kernel",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 核",
    "text": "3 核\n空間を導入した次は，その射を定義せねばなるまい．\n本節では，\\((E,\\mathcal{E})\\) を 可測空間 とする．84\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 85\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 86\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．87 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．88\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．89\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．90\n\n\n\n3.2 確率分布\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 確率測度 の全体を \\(\\mathcal{P}(E,\\mathcal{E})\\) と書く．\\(E\\) が位相空間であるとき，Borel 確率測度の全体を \\(\\mathcal{P}(E)\\) と略記する．91\n\\(E\\) を位相空間とする．\\((E,\\mathcal{B}(E))\\) 上の Radon 確率測度 の全体を \\[P(E)\\subset\\mathcal{P}(E)\\] で表す．92\n２つの確率分布 \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) の カップリング の全体を \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\] で表す．93\n\\(d\\)-次元 正規分布 を \\[\\mathrm{N}_d(\\mu,\\Sigma)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．94 その密度は \\[\n  \\phi_d(x;\\mu,\\Sigma):=\\frac{d \\mathrm{N}_d(\\mu,\\Sigma)}{d \\ell_d}(x)\n  \\] で表す．\n集合 \\(A\\subset\\mathbb{R}^d\\) 上の 一様分布 を \\[\\mathrm{U}(A)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．\n点 \\(x\\in E\\) 上の Delta 測度 を \\(\\delta_x\\) で表す．95\n確率変数 \\(X\\sim\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の 分布関数 を \\[\n\\begin{align*}\n  F_X(a)&:=F_\\nu(a)\\\\\n  &:=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d]\\\\\n  &\\quad(a=a_{1:d}\\in\\mathbb{R}^d)\n\\end{align*}\n\\] で表す．\n\\(d=1\\) のとき，その一般化逆を \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\}\n\\] \\[\n(u\\in(0,1)^d)\n\\] で表す．96\n\n\n\n3.3 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：97\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．98 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．99\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 100\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．101\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．102\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．103\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．104\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．105\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.4 関数の空間\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．106\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．107\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．108\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．109 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．110\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．111\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．112\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．113\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．114\n\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．115\n\n\n3.5 作用素\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とし，\\(X,Y\\) をノルム空間とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．116\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．117\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．118\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．119\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．120\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．121\n\n作用素 \\(T:X\\to Y\\) と言ったとき，線型写像 \\(T:X\\to Y\\) を指すこととする．122\n\\(X\\) 内の作用素 \\(T:X\\supset\\mathcal{D}(T)\\to Y\\) と言ったとき，ある \\(X\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to Y\\) を指すこととする．123\n有界作用素の全体を \\(B(X,Y)\\) で表す．124 \\(B(X):=B(X,X)\\) とする．\n連続作用素の全体を \\(L(X,Y)\\) で表す．125"
  },
  {
    "objectID": "static/Notations.html#sec-analysis",
    "href": "static/Notations.html#sec-analysis",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 解析",
    "text": "4 解析\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．126\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．127\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．128\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．129 \\(m=1\\) のとき， \\[\n\\operatorname{grad}u:=\\nabla u:=(Du)^\\top=\\begin{pmatrix}\\frac{\\partial u}{\\partial x_1}\\\\\\vdots\\\\\\frac{\\partial u}{\\partial x_n}\\end{pmatrix}\n\\] とも表す．\n発散 を \\[\n\\operatorname{div}u:=\\nabla\\cdot u:=\\operatorname{Tr}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．130\n\\(u\\) が正方行列 \\(M_n(\\mathbb{R})\\)-値であった場合，行成分毎の適用 \\[\n  \\operatorname{div}u:=\\begin{pmatrix}\\operatorname{div}(u_{1-})\\\\\\vdots\\\\\\operatorname{div}(u_{n-})\\end{pmatrix}\n  \\] と解する．\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n  \\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n  \\] と同一視する．131\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n  \\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\operatorname{Tr}(D^2u)\n  \\] で定める．\n\n\n\n4.2 Fourier 変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．132\n符号関数 を \\[\n\\operatorname{sgn}(x):=2H(x)-1\n\\] で定める．133\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 超関数\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．134 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．135\n\n\n\n4.4 確率解析\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．136"
  },
  {
    "objectID": "static/Notations.html#sec-process",
    "href": "static/Notations.html#sec-process",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "5 過程",
    "text": "5 過程\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率変数の収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．137\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．138\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n\n\n\n5.2 確率基底\n（確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．139\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\(E^T\\) に定める写像 \\[\nX_-:\\Omega\\to E^T\n\\] を 転置 と呼ぶ．140\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいい，このような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．141\n\\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．142 ただし，\\(x(0-)=x(0)\\) とする．143\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の 情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系 \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．144\n加えて， \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\] と表す．145\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) とその上の情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) からなる 4-組 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) を 確率基底 という．\n確率基底が 完備 であるとは， \\[\n\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\n\\] を満たすことをいう．146\n\n\n\n5.3 可測性\n\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}}\\) がフィルトレーション \\((\\mathcal{F}_t)\\) に 適合的 であるとは， \\[\nX_t\\in\\mathcal{F}_t\\quad(t\\in\\mathbb{R})\n\\] を満たすことをいう．\n第一種不連続な見本道を持つ適合的な過程の全体を \\(\\mathbb{D}\\) で表す．一方で，càglàd な見本道を持つ適合的な過程の全体を \\(\\mathbb{L}\\) で表す．147\n\n\n\n5.4 停止時148\n\n確率基底 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) 上の 停止時 とは，同じ確率空間 \\(\\Omega\\) 上の可測関数 \\(T:\\Omega\\to[0,\\infty]\\) であって， \\[\n\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t,\\qquad t\\in\\mathbb{R}_+,\n\\] も満たすものをいう．149\n停止時 \\(T\\) までの 情報 とは， \\[\n\\mathcal{F}_T:=\\left\\{A\\in\\mathcal{F}_\\infty\\mid\\forall_{t\\in\\mathbb{R}_+}\\;A\\cap\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t\\right\\}\n\\] で定まる \\(\\sigma\\)-代数をいう．"
  },
  {
    "objectID": "static/Notations.html#終わりに",
    "href": "static/Notations.html#終わりに",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "終わりに",
    "text": "終わりに\n\n本サイトの記法で筆者が最も注意することは，あらゆる記法を背後の数学的消息と調和するように定義するということであった．\nこれにあたり，あらゆる 数学的対象 を集合から構成する立場を取る一方で，理解するにあたっては 集合と写像（または関手）とを厳密に峻別する ということを徹底することを大事にした．\n例えば集合の合併と共通部分に \\(\\cap,\\cup\\) を用いること，直和と直積に \\(\\coprod,\\prod\\) を用いることは，圏論的な双対性を視覚的に認識しながら数学的議論を進めるためである．(斎藤毅, 2009, p. 37) にも詳しく解説されている．\n記法の開発は数学の重要な一部であると筆者は信じているのである．"
  },
  {
    "objectID": "static/Notations.html#footnotes",
    "href": "static/Notations.html#footnotes",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n集合のなす圏 \\(\\mathrm{Set}\\) は数学の基礎付けとして採用するのに極めて良い性質を持つ nLab．↩︎\n(Del Moral and Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) に一致する．\\(\\equiv,\\overset{\\text{def}}{=}\\) などもよく用いられる．(Crisan and Doucet, 2002), (Smith, 2010) では \\(\\overset{\\triangle}{=}\\) も用いられる．ここでは，これらの左右対称な記号は避けた．また，\\(=_{\\text{df}}\\) などを使うものもある (Quine and Szczotka, 1994)．↩︎\n(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 2) の定め方に一致する．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie and Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie and Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod and Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod and Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod and Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Jacod and Protter, 2012) では \\([x]\\) で表される．↩︎\n(Del Moral and Penev, 2014, p. xlviii), (Del Moral, 2004, p. 10) の定義に一致する．これは \\(\\prod_{i\\in\\emptyset}X_i\\) が一点集合で，\\(\\coprod_{i\\in I}X_i\\) が空集合である消息の一般化と見れる．なお，集合 \\(X\\) の部分集合の空な族 \\((X_i)_{i\\in\\emptyset}\\) は存在し，それは \\(\\mathrm{Map}(\\emptyset,X_i)\\) のただ一つの元である．↩︎\n(Kuratowski, 1921) による定義である．(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 118), (斎藤毅, 2009, pp. 定義1.3.1 p.15) の定め方に一致する．また \\(n\\)-組を英語では tuple と呼ぶが，全く同じ対象をリスト (list) とも呼ぶ nLab Concept with an Attitude．↩︎\n(Chopin and Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(新井敏康, 2011, p. 119) などでは，\\(f\\restriction_A\\) とも表す．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\n(Brezis, 2011, p. 14) の使い分けに倣った．支持関数は英語で indicator function という (Beck, 2017, p. 14) 例2.1，(寒野善博，土谷隆, 2014, p. 110)，(Ekeland and Témam, 1999, p. 8)．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné and Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(斎藤毅, 2007, pp. 例1.4.7 p.20) に従った．また f.e. とは with a finite number of exceptions の略で，「有限個の例外を除いて成り立つ」という意味である (伊藤清, 1991, p. 124)．↩︎\n(斎藤毅, 2009, p. 179) では \\(F(X)\\) と表記している．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(Billingsley, 1999), (Ethier and Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n(Fritz, 2020, p. 19), (Perrone, 2024) など．Markov圏の稿 も参照↩︎\nnLab の記法に一致する．(斎藤毅, 2020, p. 7) では \\(\\mathrm{Mor}_C(X,Y)\\) と表す．↩︎\n(Del Moral, 2004, p. 7) も参照．↩︎\n(Del Moral and Penev, 2014, p. xlvii), (V. I. Bogachev, 2007, p. 277) 4.1.(i) に一致する．(V. I. Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie and Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod and Protter, 2012), (Del Moral and Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(V. I. Bogachev and Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Rogers and Williams, 2000, p. 110) V.1.3 では \\(S_n^+\\) の記法が用いられている．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．特に，古典力学や有限要素法の文献においては，二項積 の間の演算である二重点乗積を \\(:\\) で表したことから，この記法が用いられる．二項積については (Abraham et al., 1988, p. 341) も参照．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi), (Bakry et al., 2014, p. xv) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné and Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné and Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (V. I. Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(V. I. Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart and Wellner, 2023, p. 6) では 外確率 という．↩︎\n(V. I. Bogachev, 2007, p. 17) 定義1.5.1, (Vladimir I. Bogachev and Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (V. I. Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (V. I. Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(V. I. Bogachev, 2007, p. 23) に倣った．(V. I. Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie and Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(V. I. Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (V. I. Bogachev, 2007, p. 189) 参照．↩︎\n(Giné and Nickl, 2021, p. 16), (Vladimir I. Bogachev and Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 8) に倣った．(V. I. Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n(Nualart and Nualart, 2018) に倣った．(Giné and Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．(Giné and Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral and Penev, 2014), (Dellacherie and Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné and Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart and Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain and Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral and Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné and Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral and Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (V. I. Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(V. I. Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral and Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n可測空間を \\((E,\\mathcal{E})\\) で表すのは，(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) に倣った．↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay and Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(V. I. Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford and Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford and Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné and Nickl, 2021, p. 2), (Villani, 2009) に一致する．(V. I. Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral and Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(V. I. Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie and Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(V. I. Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan and Doucet, 2002) に一致する．(Dellacherie and Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Jacod and Shiryaev, 2003, p. 347), (Crisan and Doucet, 2002), (Ethier and Kurtz, 1986, p. 96), (V. I. Bogachev, 2007, p. 228) に一致する．(Kechris, 1995, p. 109), (Villani, 2009) はイタリックで \\(P(E)\\) と表す．↩︎\n(Pedersen, 1989, p. 72) に倣った．(V. I. Bogachev, 2007, p. 76) 第7.2節 では \\(\\mathcal{P}_r(X)\\) で表す．Radon 測度とは，内部正則性（＝緊密性） \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] を満たす Borel 測度をいう (V. I. Bogachev, 2007, pp. 68–69) 定義7.1.1, 7.1.4．↩︎\n(Kulik, 2018) が \\(\\mathcal{C}\\) で表すのに倣った．(Vladimir I. Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) では \\(\\Pi(\\mu,\\nu)\\) で，(Ethier and Kurtz, 1986, p. 96) では \\(\\mathcal{M}(\\mu,\\nu)\\) で，(Dudley, 2002, p. 420) 11.8節 は \\(M(\\mu,\\nu)\\)，(Figalli and Glaudo, 2023) では \\(\\Gamma(\\mu,\\nu)\\) で表す．(V. I. Bogachev, 2007, p. 235) 8.10(viii)節と (Villani, 2009, p. 95) 注6.5 に倣い，カップリングの元は Radon なものに限っている点に注意．↩︎\n(竹村彰道, 2020) の記法に一致する．↩︎\nDirac 測度とも言う．(Jacod and Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) などは \\(\\epsilon_x\\) で表す．(Protter, 2005, p. 299) は Dirac 関数を \\(\\delta_x\\) で表す．↩︎\n(Gerber et al., 2019) の記法に一致．分位点関数 (quantile function) (竹村彰道, 2020, p. 16)，確率表現関数 (森口繁一, 1995) などともいう．(Dudley, 2002, p. 283) は \\(X_F\\) とも表している．↩︎\n(Revuz and Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho and Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod and Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan and Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal and van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin and Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie and Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas and Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal and van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf and Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．(Heng et al., 2024) では \\(T=\\mu\\) という定値核の場合も同様の記法 \\(\\mu\\otimes S\\) を定義している．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) や (Protter, 2005, p. 52) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie and Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(V. I. Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (V. I. Bogachev, 2007, p. 262) 4.4節．(Dunford and Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral and Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg and Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(V. I. Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(V. I. Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier and Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné and Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie and Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\nこのような使い分けは (Nummelin, 1984, p. 1) に一致する．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart and Nualart, 2018, p. 31) に一致する．↩︎\n(Nualart and Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod and Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod and Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod and Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie and Meyer, 1978, p. 114), (Revuz and Yor, 1999, p. 42) に倣った．↩︎\n右連続性と完備性を併せて，フィルトレーション付き確率空間 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) の 通常の条件 ともいう．(Protter, 2005, p. 3) など参照．↩︎\n(Protter, 2005, p. 56)．↩︎\n(Dellacherie and Meyer, 1978) 49 115-IV では 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n(Jacod and Shiryaev, 2003, p. 4) 1.11，(Protter, 2005, p. 3) に従った．↩︎"
  },
  {
    "objectID": "static/ResearchJP.html",
    "href": "static/ResearchJP.html",
    "title": "研究紹介",
    "section": "",
    "text": "現状のモンテカルロ法の多く（Langevin Monte Carlo や Hamiltonian Monte Carlo など）は物理学的な由来を持ちますが，最も効率的な方法はそうではないかもしれません．\nコーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みをモンテカルロ法に取り入れることで，効率性をさらにあげることができるはずです．\nその第一歩が 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n\n\nZig-Zag サンプラー：非対称なダイナミクスを持つ MCMC アルゴリズムの例\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#mcmc-ダイナミクスの非対称化",
    "href": "static/ResearchJP.html#mcmc-ダイナミクスの非対称化",
    "title": "研究紹介",
    "section": "",
    "text": "現状のモンテカルロ法の多く（Langevin Monte Carlo や Hamiltonian Monte Carlo など）は物理学的な由来を持ちますが，最も効率的な方法はそうではないかもしれません．\nコーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みをモンテカルロ法に取り入れることで，効率性をさらにあげることができるはずです．\nその第一歩が 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n\n\nZig-Zag サンプラー：非対称なダイナミクスを持つ MCMC アルゴリズムの例\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#粒子輸送によるサンプリング",
    "href": "static/ResearchJP.html#粒子輸送によるサンプリング",
    "title": "研究紹介",
    "section": "粒子輸送によるサンプリング",
    "text": "粒子輸送によるサンプリング\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#smc-による軌道推定",
    "href": "static/ResearchJP.html#smc-による軌道推定",
    "title": "研究紹介",
    "section": "SMC による軌道推定",
    "text": "SMC による軌道推定\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\n2023-12-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html",
    "href": "static/Slides.html",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nReversible Jump Zig-Zag Sampler\n\n\nスライドはこちら．\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here.\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA MCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#upcomings-newests",
    "href": "static/Slides.html#upcomings-newests",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nReversible Jump Zig-Zag Sampler\n\n\nスライドはこちら．\n\n\n\n2025-01-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here.\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA MCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#history",
    "href": "static/Slides.html#history",
    "title": "Slides",
    "section": "History",
    "text": "History\n\n\n\n  \n    \n      1/10/2025.\n      司馬博文 .\n      \n        Zig-Zag サンプラーのモデル選択への応用\n        : Reversible Jump Zig-Zag Sampler\n      .\n      \n        中間評価・学生研究発表会.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n    \n  \n    \n      9/10/2024.\n      Hirofumi Shiba | 司馬博文.\n      \n        Zig-Zag Sampler\n        : A MCMC Game-Changer\n      .\n      \n        Seoul National University, Gwanak (관악) campus, South Korea.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n    \n  \n    \n      7/25/2024.\n      司馬博文 .\n      \n        Zig-Zag サンプラー\n        : 物理のくびきを超える MCMC\n      .\n      \n        ベイズ会（本郷キャンパス小島ホール第二セミナー室）.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes",
    "section": "",
    "text": "Notations\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n12/31/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n重回帰モデルにおける OLS 推定量は，部分回帰推定量としての解釈を持つ． この性質を用いた手法が媒介分析や操作変数法である． OLS 推定量は不均一分散の場合でも不偏性・一致性・漸近正規性を持ち得るが，漸近有効性は失われる． これを回復するには，誤差の分散を推定して重み付けを行う必要がある． このような方法は一般化最小二乗法と呼ばれる． さらに相関を持つデータを分析するために，より一般の共分散構造を持ったモデルに対してこの手法が拡張されている． 疫学では一般化推定方程式，さらに一般には計量経済学で一般化モーメント法と呼ばれる方法である． これらの方法は作業共分散の選択により，セミパラメトリック漸近最適な分散を達成したり，バイアスを小さくしたりできるが， いずれもトレードオフの範疇にある． \n\n\n\n\n\n12/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\n\n\n\nProbability\n\n\nStatistics\n\n\n\nWe examine how to find & formally determine the likelihood function of hierarchical models. As a real-world example, we consider the ideal point model, also known as the 2-parameter logistic item response model. \n\n\n\n\n\n12/23/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\nIdeal point models are 2-parameter item response model, tailored to the purpose of visualizing / measuring the ideological positions of the legislators / judges. [@Bafumi+2005] introduced a hierarchical structure to the model to deal with the problem of identifiability. In this article, we re-examine the model and show that the posterior distribution of the parameters (ideal points) is still bimodal, indicating its weak identifiability. \n\n\n\n\n\n12/22/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n項目反応モデルとは，被験者と項目のそれぞれが独自のパラメータを持った一般化線型混合効果モデルである． 被験者ごとの特性の違いや，項目ごとの性質の違いが視覚化できるが， 本稿では能力・難易度パラメータに更なる階層構造を考える． これにより能力パラメータを変化させている背後の要因や，項目特性と個人特性の交絡効果（特異項目機能）を解析することが可能になる． brms パッケージは極めて直感的な方法でモデルのフィッティングから事後分布の推論までを実行できる． \n\n\n\n\n\n12/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\nロジスティック回帰分析は離散的な応答データを扱うことのできる一般化線型モデルである． 他にも，高度に非線型な関係が予期される場合，ノンパラメトリック手法に移る前の簡単な非線型解析としても活躍する． 本稿では BMI と LDL の非線型関係に関する探索的手法として，順序ロジスティック回帰分析を実行する． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nデータが自然な階層構造を持つ場合，これを取り入れた自然な事前分布を，一つ上の階層に回帰モデルを付け加えることで構成できる． このようなモデルをベイズ階層モデルという． 本稿ではベイズ階層モデルの縮小効果を概観する． 事前知識を構造に関する知識としてモデルに取り入れることでデータによりフィットする尤度構造を獲得することは，データ解析の一つの目標として，（線型）回帰モデルの自然な拡張と理解できる． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n点推定における変数選択法は，正則化項の追加によることが多い． これはベイズ推論では \\(0\\) 近傍に大きな確率を持った事前分布を仮定していることに等しい． ベイズの観点から適切な縮小事前分布を用意することで，大きな効果を持つ回帰係数は変えずに， 効果の小さい変数を排除することができる． 一般に LASSO よりも絞って選択してくれることが多い．\nまたベイズ変数選択では，\\(0\\) にアトムを持つ事前分布を用いることで，当該の変数がモデルに含まれる事後確率 (PIP: Posterior Inclusion Probability) を算出することができる． この方法ではモデルの空間を効率的に探索するサンプラーの開発が重要であるが， 近年では効率的なサンプラーが複数提案されている． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\nベイズ重回帰分析は解析者のデータへの理解を促進する強力な探索的データ解析手法である． このことを brms パッケージと BMI データを用いて例証する． １変数の場合から始め，変数を追加して挙動が変わるのを解釈・検証（残差プロット・事後予測プロット）しながら慎重に進んでいく． 交差検証による事後予測スコア elpd を用いて，データの非線型変換を利用することで，非線型な関係を見出す方法を扱う． ここまで行えば，データの階層化やノンパラメトリックな手法の採用などの次のステップが自然と見えてくるだろう． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する． 分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる． また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n通常の回帰モデルは応答変数が連続であることが暗黙の仮定となっている． この節では，応答変数が質的変数である場合のモデリングを扱う． 質的変数は順序変数であるか名目変数であるか（順序の構造があるかないか）の峻別が重要である． いずれの場合でも多くのモデルが利用可能であり，その多くが一般化線型モデルの枠組みで統一的に扱うことができる． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n12/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\nPDMP / 連続時間 MCMC とは 2018 年に以降活発に研究が進んでいる新たな MCMC アルゴリズムである． 実用化を遅らせていた要因として，種々のモデルに統一的な実装が難しく，モデルごとにコードを書き直す必要があったことが挙げられたが， この問題は自動微分の技術と，[@Corbella+2022], [@Sutton-Fearnhead2023] らの適応的で効率的な Poisson 点過程のシミュレーションの研究によって解決されつつある． ここでは [@Andral-Kamatani2024] の Python パッケージ pdmp_jax とこれに基づく Julia パッケージ PDMPFlux.jl を紹介する． \n\n\n\n\n\n10/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n[@Vargas-Grathwohl-Doucet2023] の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である．今回は 公式の実装 を吟味する． \n\n\n\n\n\n10/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\nLorenz’ 63, Lorenz’ 96 とはそれぞれ [@Lorenz1963], [@Lorenz1995] によって導入された大気モデルである． 前者はバタフライ効果の語源ともなった，最初に特定されたカオス力学系でもある． Navier-Stokes 方程式は流体の運動を記述する方程式である． これらはいずれもデータ同化・軌道推定技術のベンチマークとして用いられている． ここでそれぞれのモデルの数学的性質と Julia を通じたシミュレーションの方法をまとめる． \n\n\n\n\n\n10/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n政治学における理想点解析とは，項目反応モデルを用いて裁判官や国会議員などの価値判断基準や「イデオロギー」を定量化・視覚化する方法である． ここでは既存のパッケージを用いて簡単に理想点解析を行う方法から始め， 自分で Stan コードを書いてモデルを推定する方法を紹介する． その際に最も重要な理想点モデルの性質として，識別可能性 の議論がある． これが保たれていないと，モデルの事後分布は多峰性を持ってしまい，推定をするたびに結果が異なったり，統計量の長期間平均が \\(0\\) になってしまったりしてしまう． \n\n\n\n\n\n10/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし，古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． 一方で，モデルの仮定を前面に出したベイズ的な解析手法は，データを探索的に吟味することができ，極めて微妙な消息も捉えることが可能になる． 本稿では特にベイズ ANOVA 手法 [@Gelman2005], [@Rouder+2012] を採用して，そのモデルケースを実証する． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n応募法 (voluntary sampling) や多くのウェブアンケートは，確率標本抽出に該当しない．このような場合でも母集団に関する補助情報がある限り，バイアスを軽減し推定精度を高めることができる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n標本調査において欠測はつきものである．観測単位が欠測している場合 (unit nonresponse)，call-back や follow-up などの調査を行うか，それができない場合は 荷重校正 (calibration weighting) が可能である．一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． これらの問題点を解決策としてベイズの方法を導入し，ベイズ ANOVA，ベイズ推論とモデル比較が ANOVA の発展として得られることをみる． この拡張は，ANOVA の線型モデルとしての解釈を通じてなされ，ANOVA の「同じ係数を共有するクタスタ構造の特定手法」というより広い理解へ導かれる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n人間を対象にする介入の研究では，介入の前後で変化があったかが争点となる． この変化の量を表す平均処置効果 (ATE) を，なるべくモデルを仮定せずどこまで識別できるかが多くの場合論点になる． この際の枠組みが潜在結果モデルである． したがって，操作変数法などの交絡統制法がある一方で，ATE の推定にはモデルの誤特定に強いセミパラメトリックな手法が要請される． 一般化推定方程式，一般化モーメント法，経験尤度法などの方法がある． 本稿ではこれらの推定量を同一の枠組みの下でまとめる． 推定量の分散を求めるためには漸近論のほかにブートストラップ法も用いられる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n9/22/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能ですが，本稿では特に R からの利用方法をまとめます．\n\n\n\n\n\n9/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n本稿では生存時間解析の代表的なモデルを概観する． 特に医療技術評価への応用では，打ち切りデータを最もよく外挿できるハザードモデルが探索され，ベイズ推定が有効な方法としてよく選択される． 本稿では特に表現力の高い競合リスクモデルとして polyhazard model を紹介し，ベイズ推定の困難さを議論する．\n次稿ではこのモデルを Zig-Zag サンプラーでベイズ推定する方法を紹介する． \n\n\n\n\n\n9/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n最適輸送問題は変分法の黎明期に提案された変分問題の１つであるが，その発展は確率論の成熟を待つ必要があった．現代では多くの非正則な空間上に幾何学的な量を定義する普遍的な手法として理解されてから，多くのフィールズ賞受賞者を輩出する最も活発な分野の１つとなっている．ここまでの発展の歴史を本記事では概観したい．\n\n\n\n\n\n9/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．特に OU 過程の時間反転は雑音除去過程 (Denoising Diffusion) といい，サンプリングに利用されている．デノイジングスコアマッチングでは，時間反転に Hyvärinen スコアが出現することを利用してデータ分布のスコアを推定する．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．\n\n\n\n\n\n8/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\nSkilling-Hutchinson の跡推定量は，跡の計算 \\(O(d^2)\\) を \\(O(d)\\) に落とすことができる Monte Carlo 法である．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n行列の特異値分解とは，正方行列の直交対角化を一般の行列に拡張したものである．特異値を大きいものから \\(r\\) 個選ぶことで，Hilbert-Schmidt ノルムの意味で最適な \\(r\\)-階数近似が構成できる．このことは主成分分析に応用を持つ．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\nカーネル法とは，半正定値カーネルを用いてデータを Hilbert 空間内に埋め込むことで，非線型な変換を行う統一的な手法である．再生核 Hilbert 空間の理論により，写した先における内積は，半正定値カーネルの評価を通じて効率的に計算できるため，無限次元空間上での表現に対する tractable な手段を提供する．適切な半正定値カーネルを用いることで，データの「類似度」を定義することができる．本稿では半正定値カーネルの理論と距離学習法を扱う．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散模型は拡張性にも優れており，条件付けが容易である．現状は誘導付き拡散によってこれが実現されるが，連続的な条件付き生成のために，フローマッチングなる方法も提案された．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n画像と動画に関してだけでなく，化学分子の構造生成の分野でも拡散模型が state of the art となっている．これは，連続空間上だけでなく，グラフなどの離散空間上でも拡散模型が拡張されたことが大きい．本稿では，離散データを連続潜在空間に埋め込むことなく，直接離散空間上に拡散模型をデザインする方法をまとめる．\n\n\n\n\n\n8/09/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n\n\n\n\n\n8/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフロー学習手法は，画像と動画に関して 2024 年時点で最良の性能を誇る． これは統計的に言えば事後分布からの近似的サンプリングを実行していることに相当する． 近似的ではなく，正確に２つの分布を補間するような拡散過程を推定するためには Schrödinger 橋がある． Schrödinger 橋については 次稿 に譲るとし，本稿ではサンプラーとしての拡散モデルを復習する． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルは「データ過程をノイズに還元する Langevin ダイナミクスを時間反転する」という発想に基づいており，画像と動画の生成・条件付き生成タスクに関して 2024 年時点で最良の方法の１つである． この発想を正確なサンプリング法に昇華するためには，[@Deming-Stephan1940] の Iterative Proportional Fitting アルゴリズムを用いることができる． この方法は拡散モデルによる条件付き生成の加速法として [@Shi+2022] によって提案された． こうして得る拡散過程は Schrödinger Bridge とも呼ばれ，エントロピー最適輸送と深い関わりを持つ． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は normflows を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて， Ho et. al. [NeurIPS 33(2020)] による DDPM (Denoising Diffusion Probabilistic Model) の実装の概要を見る．DDPM は拡散模型の最初の例の１つであり，ノイズからデータ分布まで到達するフローを定める拡散過程（雑音除去過程）を，データをノイズにする拡散過程の時間反転として学習する方法である．画像や動画だけでなく，離散空間上でタンパク質などの構造生成でも state of the art の性能を示すモデルである．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n表現学習，非線型独立成分分析など，「生成」以外の潜在変数模型の応用法を横断してレビューする．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．\n\n\n\n\n\n7/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\nサンプリング，または Monte Carlo 法は，現代の統計学と機械学習において必要不可欠な道具となっている．それは一体どうしてだろうか？初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．初めに政治学における理想点解析の目的と役割を概観し，続いて多次元展開法と項目反応理論の２つの観点から理想点解析を眺める． \n\n\n\n\n\n7/16/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫制博士課程（正確には，総合研究大学院大学統計科学コース）を紹介します．同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷のような研究環境が整っていると言えるでしょう．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はそのような確率過程の汎函数の漸近展開に基づく計算方法を紹介します．確率変数の期待値を近似するのに Monte Carlo 法は普遍的な方法ですが，漸近展開が用いられる場合，その計算時間は比較にならないほど速くなります．\n\n\n\n\n\n5/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．本稿では Stan 言語の基本をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．鋭意開発中のパッケージですが，すでに広範なクラスの確率微分方程式のシミュレーションが可能です．本稿では基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行う R パッケージである． 基本的な線型回帰から固定・変量効果の追加まで極めて簡単に実行できる，大変実用的なパッケージである． 本稿では，brms の基本的な使い方とその実装を紹介する． その中で混合効果モデルについてレビューをする． ランダム効果の追加は縮小推定などの自動的な正則化を可能とする美点がある一方で，係数の不偏推定やロバスト推定に拘る場合はこれを避ける判断もあり得る． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論，確率論，さらにはデータ解析の中心に据えられるべき中心概念であると言えるかもしれない．例えば，カーネル法とは確率核に沿った埋め込みである．MCMC の性質も，本質的に確率核の性質が決定する．また確率核は，確率空間の圏の射となる．このように，多くのデータ解析手法の中核に位置する数学的本体たる「確率核」への入門を目指すのが本記事である．\n\n\n\n\n\n3/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次稿 で，\\(K\\)-平均アルゴリズムの model-aware な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す． \n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\nVSCode での LaTeX 環境構築に関するページ．\n\n\n\n\n\n12/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nMCMC\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\nFeynman-Kac モデルという物理モデルを定義し，逐次モンテカルロ法（粒子フィルター）をその Monte Carlo シミュレーション法として位置付けて解説した書籍である． 例として挙げられるトピックも物理学のものが多く，書籍のスタイルも物理学書のそれである． ここでは 1.1 節 “On the Origins of Feynman-Kac and Particle Models” の抄訳を通じて内容を概観したい． \n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．公式の ギャラリー も参照．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html",
    "href": "posts/2024/Survey/BayesRegression.html",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#はじめに",
    "href": "posts/2024/Survey/BayesRegression.html#はじめに",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\nベイズ線型回帰分析は多くのデータ解析における「最初の一歩」である．ベイズ回帰分析から始まるベイズのワークフローや，理論的な背景は次稿を参照：\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nここではベイズ回帰モデルに変数を増やしていく際の解釈の変化や，変数の選択の問題などの実際的な問題を扱う．\n\nlibrary(readxl)\nraw_df &lt;- read_excel(path)"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html",
    "href": "posts/2024/Survey/BDA3.html",
    "title": "ベイズデータ解析７",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#関連記事",
    "href": "posts/2024/Survey/BDA3.html#関連記事",
    "title": "ベイズデータ解析７",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html",
    "href": "posts/2024/Lifestyle/FixedRandom.html",
    "title": "変量効果と固定効果",
    "section": "",
    "text": "階層モデル \\[\ny_{ij}=\\theta_j+\\epsilon_{ij}\n\\tag{1}\\] \\[\n\\theta_j=\\mu+\\gamma_j\n\\tag{2}\\] を考える．\\(j\\in[J]\\) がグループ，\\(i\\in[n_j]\\) はそのグループに所属する単位とする．層別・クラスター抽出された標本，パネルデータなど，好きなように解釈してほしい．\n(1), (2) を階層モデルと呼ぶ理由は，(1) で \\(y_{ij}\\) が最小単位として回帰されているのと同時に，(2) でグループごとの変数 \\(\\theta_j\\) にもモデルが設定されているためである．個人単位とグループ単位の２つの階層で回帰モデルが設定されているために 階層モデル という．\n(1) は他の個人レベル説明変数を含んでいても良い \\[\ny_{ij}=x_{ij}^\\top\\beta+\\theta_j+\\epsilon_{ij}\n\\tag{3}\\] が，ここでは問題にしない．\n本稿で問題にするのは，グループレベル変数 \\(\\theta_j\\) に対する２つの扱い方：「変量効果」と「固定効果」の違いである．\nこの名称の違いが表すものとして，単にモデルの違いなのか，それともモデルは同じで単に推定方法の違いなのか，という点からすでに混乱が見られる．\n\n\n\n\n\n\n筆者の結論\n\n\n\nThere are only two ways to make sense of the terminology “fixed effects” and “random effects”:\n\nEconometric manner:\n\nIf \\(\\theta_j\\) is not correlated with the vector of covariates \\(x_{ij}\\), then it is a random effect. Otherwise, it is a fixed effect. Random effects can be estimated via Generalized Least Squares (GLS).\n\nBiostatic manner:\n\nIf we assume a super-population model on \\(\\theta_j\\), then it is a random effect. If we don’t assume any additional hierarchical structure on \\(\\theta_j\\) (or just a flat prior), then it is a fixed effect.\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#概要",
    "href": "posts/2024/Lifestyle/FixedRandom.html#概要",
    "title": "変量効果と固定効果",
    "section": "",
    "text": "階層モデル \\[\ny_{ij}=\\theta_j+\\epsilon_{ij}\n\\tag{1}\\] \\[\n\\theta_j=\\mu+\\gamma_j\n\\tag{2}\\] を考える．\\(j\\in[J]\\) がグループ，\\(i\\in[n_j]\\) はそのグループに所属する単位とする．層別・クラスター抽出された標本，パネルデータなど，好きなように解釈してほしい．\n(1), (2) を階層モデルと呼ぶ理由は，(1) で \\(y_{ij}\\) が最小単位として回帰されているのと同時に，(2) でグループごとの変数 \\(\\theta_j\\) にもモデルが設定されているためである．個人単位とグループ単位の２つの階層で回帰モデルが設定されているために 階層モデル という．\n(1) は他の個人レベル説明変数を含んでいても良い \\[\ny_{ij}=x_{ij}^\\top\\beta+\\theta_j+\\epsilon_{ij}\n\\tag{3}\\] が，ここでは問題にしない．\n本稿で問題にするのは，グループレベル変数 \\(\\theta_j\\) に対する２つの扱い方：「変量効果」と「固定効果」の違いである．\nこの名称の違いが表すものとして，単にモデルの違いなのか，それともモデルは同じで単に推定方法の違いなのか，という点からすでに混乱が見られる．\n\n\n\n\n\n\n筆者の結論\n\n\n\nThere are only two ways to make sense of the terminology “fixed effects” and “random effects”:\n\nEconometric manner:\n\nIf \\(\\theta_j\\) is not correlated with the vector of covariates \\(x_{ij}\\), then it is a random effect. Otherwise, it is a fixed effect. Random effects can be estimated via Generalized Least Squares (GLS).\n\nBiostatic manner:\n\nIf we assume a super-population model on \\(\\theta_j\\), then it is a random effect. If we don’t assume any additional hierarchical structure on \\(\\theta_j\\) (or just a flat prior), then it is a fixed effect.\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#サーベイ",
    "href": "posts/2024/Lifestyle/FixedRandom.html#サーベイ",
    "title": "変量効果と固定効果",
    "section": "2 サーベイ",
    "text": "2 サーベイ\n\n\n\n\n\n\n\n計量経済学では，\\(x_{ij}\\) と \\(\\theta_j\\) の相関の有無に関する仮定の違いと，これに起因する自然な後続の（最小自乗）推定法の違いである．\n\n\n\n\n\n2.1 計量経済学\n(Hansen, 2022) を参照して，計量経済学で主流な固定効果と変量効果の違いの解釈を見る．\n(17.7 Hansen, 2022, pp. 603–) では，固定効果モデルは (3) のような他の説明変数 \\(x_{ij}\\) が存在する場合，\\(x_{ij}\\) と \\(u_{i}\\) の相関が危惧される際に使える「推定手法」（あるいは推定量の名前）として導入される．\n\nIn the econometrics literature if the stochastic structure of \\(\\theta_j\\) is treated as unknown and possibly correlated with \\(x_{ij}\\) then \\(\\theta_j\\) is called a fixed effect. (Hansen, 2022, p. 604)\n\nその推定手法とは，within transformation \\(\\dot{-}\\) を \\[\n\\dot{y}_{ij}=y_{ij}-\\overline{y}_j,\\qquad\\overline{y}_j:=\\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij}\n\\] と定めて，この変換をデータに適用してから回帰 \\[\n\\dot{y}_{ij}=\\dot{x}_{ij}^\\top\\beta+\\dot{\\epsilon}_{ij}\n\\tag{4}\\] を解く．この変換は \\(\\theta_j\\) を消去するように出来ているため，\\(\\theta_j\\) の性質や \\(x_{ij}\\) との相関に依らずに係数 \\(\\beta\\) を推定できるというのである．\n一方で within transformation を施さずに直接 GLS 推定して得る推定量を変量効果推定量，OLS 推定して得る推定量を pooled OLS と呼ぶ (17.6 Hansen, 2022, pp. 601–603)．\n多くの場合，変量効果モデル (3) では \\(\\theta_j\\) と \\(x_{ij}\\) との無相関性の他に，誤差の均一性 \\[\n\\theta_j\\sim(0,\\sigma^2_\\theta)\n\\] も仮定されるため，同グループ内の残差に \\[\n\\mathrm{V}[\\theta_j+\\epsilon_{ij}]=\\begin{pmatrix}\n\\sigma^2_\\theta+\\sigma^2_\\epsilon&\\sigma^2_\\theta&\\cdots&\\sigma^2_\\theta\\\\\n\\sigma^2_\\theta&\\sigma^2_\\theta+\\sigma^2_\\epsilon&\\cdots&\\sigma^2_\\theta\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\sigma^2_\\theta&\\sigma^2_\\theta&\\cdots&\\sigma^2_\\theta+\\sigma^2_\\epsilon\n\\end{pmatrix}\n\\] という共分散構造が仮定されていることになる．このような設定では GLS 推定量が BLUE を与える (Hayashi, 2000, p. 55)．\n\n\n\n\n\n\n筆者の結論\n\n\n\n計量経済学で変量効果モデル・固定効果モデルと呼んだときは，\\(\\theta_j\\) と \\(x_{ij}\\) との相関の有無に関する仮定の違いで呼び分けている．\nそれぞれのモデルで自然な OLS 様推定量があり，「変量効果」「固定効果」という名称はそのまま推定手法の違いも指すようである（もはや仮定の違いから始まるワークフローの名前になっている）．\n\n\n\n\n2.2 Gelman の見解\n以上の違いをモデルの違いとして理解した場合，変動効果モデルは \\(\\theta_j\\) に無相関性や等分散性を初めとするモデルをおいており，固定効果モデルはモデリングを放棄している，とも見れる．\nGelman もこのような観点に立っているものと思われる．\n\nThe term fixed effects is used in contrast to random effects—but not in a consistent way! Fixed eﬀects are usually deﬁned as varying coeﬃcients that are not themselves modeled. (Gelman and Hill, 2006, p. 245)\n\nこれをよく理解するために，\\(0,1\\) の指示変数（ダミー変数）からなる計画行列 \\(X\\) を導入した (1) の別の定式化を考える： \\[\n\\boldsymbol{y}=X\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}\n\\tag{5}\\] \\[\nX=\\begin{pmatrix}\n\\boldsymbol{1}_{n_1}&O&O&O\\\\\nO&\\boldsymbol{1}_{n_2}&O&O\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\nO&O&O&\\boldsymbol{1}_{n_J}\n\\end{pmatrix}\n\\] \\[\n\\boldsymbol{\\theta}=\\begin{pmatrix}\\theta_1\\\\\\vdots\\\\\\theta_J\\end{pmatrix},\\qquad\\left.\\boldsymbol{1}_{n_j}:=\\begin{pmatrix}1\\\\\\vdots\\\\1\\end{pmatrix}\\right\\}n_j\n\\]\nこれ以上何も考えず，(5) に対して OLS 推定を行うと，固定効果推定量と全く同じものが得られる (定理 17.1 Hansen, 2022, p. 610)．残差も同じである．\nすると Gelman による次の固定効果モデルの定義は，最終的に得られる MAP 推定量としては，計量経済学の文脈 2.1 のものと一致することになる：\n\n\n\n\n\n\n(15.2 節 Gelman et al., 2014, p. 383)\n\n\n\n独立な一様事前分布 \\[\n\\theta_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\mu,\\infty)\n\\] を仮定した場合の \\(\\theta_j\\) を 固定効果 と呼ぶ．\n階層的なモデル \\[\n\\theta_j=\\mu+\\gamma_j,\\qquad\\gamma_j\\sim\\mathrm{N}(0,\\sigma^2_\\theta)\n\\] を仮定した場合の \\(\\theta_j\\) を 変量効果 と呼ぶ．\n２つが同居するモデルを 混合効果モデル (mixed-effects model) という．\n\n\nこの語用法は，\\(\\theta_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\mu,\\infty)\\) の事前分布の下で MAP 推定した場合が，最尤推定量と一部の OLS 推定量と一致するため，計量経済学の文脈 2.1 のものと一致する．\nそれだけでなく変量効果モデルとは階層モデルであり，推定の際に縮小が働く．このような方法で推定されるモデルは疫学や生物統計学の分野でもよく使われてランダム効果モデルと呼ばれている（例えば (Robinson, 1991), (Solomon, 2005)）ため，この語用法とも一致することになる．\n以上の見解は (Gelman, 2005, p. 20) に端的に現れている：\n\nIn the Bayesian framework, this definition implies that fixed effects \\(\\theta_j\\) are estimated conditional on \\(\\sigma_\\theta=\\infty\\) and random effects \\(\\theta_j\\) are estimated conditional on \\(\\sigma_\\theta\\) from the posterior distribution. (Gelman, 2005, p. 20)\n\n\n\n2.3 ‘fix’, ‘random’ の名前の由来は？\nこの生物統計学的な語用法は，必ずしも fixed effects と対置するわけではない．グループ \\(j\\in[J]\\) に依存せず一定の値を取るいわば普通の「係数」と「変動係数」があるだけである．\n多くの場合，ランダム効果は局外母数であり，\\(\\beta\\) （多くの場合処置効果）の不偏推定が最大の目的である．計量経済学のように深刻な外生性が疑われる状況を扱うわけでもない．それゆえ階層モデルによる縮小推定が選好されるという背景もある．\n一方で計量経済学では効率が落ちても外生性への頑健性が選好されることが多いようである．\nすると次の語用法が出現し，‘fix’, ‘random’ の名前の由来も同時にうまく説明するように見える：\n\nEffects are ﬁxed if they are interesting in themselves or random if there is interest in the underlying population. (Searle et al., 1992, p. 7)\n\nだがこうするともはやモデルとしてどのような仮定の違いを表しているのかわからなくなり，専門用語というよりは日常用語である．\nそこで (Gelman, 2005, p. 21) ではこの意味では別の用語を採用するとしている：\n\nWe prefer to sidestep the overloaded terms “ﬁxed” and “random” with a cleaner distinction by simply renaming the terms. We deﬁne effects (or coefﬁcients) in a multilevel model as constant if they are identical for all groups in a population and varying if they are allowed to differ from group to group. (Gelman, 2005, p. 21)\n\n\n\n\n2.4 その他のベイジアンの見解\n(Hoff, 2009, p. 147) には次の記述がある：\n\nthe \\(\\theta_j\\)’s (or \\(\\gamma_j\\)’s) may be referred to as either “fixed effects” or “random effects” depending on how they are estimated.\n\nつまり推定手法の違いで呼び分けるとしている．\nこれを見ると，前節 2.2 の BDA が開陳しているような，（無理に）モデルの違いとして理解しようという立場はやや急進的であり，ここまでいくともはや「固定効果」「変量効果」という名称が意味を持たないとも思える．\n実際 (Gelman et al., 2014, p. 383) には次の注がある：\n\nThe terms ‘fixed’ and ‘random’ come from the non-Bayesian statistical tradition and are somewhat confusing in a Bayesian context where all unknown parameters are treated as ‘random’ or, equivalently, as having ﬁxed but unknown values.\n\nベイズの立場では潜在変数だろうがパラメータだろうが，未知である限り「固定」「変動」の区別はない，というのは正しいが，だとしたらこの名称は捨てるべきである．"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#footnotes",
    "href": "posts/2024/Lifestyle/FixedRandom.html#footnotes",
    "title": "変量効果と固定効果",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの生物統計学的な語用法は，必ずしも fixed effects と対置するわけではない．むしろ多くの文脈でランダム効果は局外母数であり，\\(\\beta\\) （多くの場合処置効果）の不偏推定が最大の目的である．計量経済学のように深刻な外生性が疑われる状況を扱うわけでもない．それゆえ階層モデルによる縮小推定が選好されるという背景もある．↩︎"
  },
  {
    "objectID": "posts/2024/Lifestyle/FixedRandom.html#筆者の結論",
    "href": "posts/2024/Lifestyle/FixedRandom.html#筆者の結論",
    "title": "変量効果と固定効果",
    "section": "3 筆者の結論",
    "text": "3 筆者の結論\n筆者は基本的には BDA の見解 2.2 を採用し，「固定効果」と「変動効果」の違いは，係数にモデルを想定しているかどうかの違いと理解することにした．\n「固定効果」と「変量効果」の語は（慣習を踏襲すべき場合を除いて）使わず，「変動係数」 (varying coefficient) と呼ぶつもりである．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#交差項",
    "href": "posts/2024/Survey/BayesRegression.html#交差項",
    "title": "ベイズ重回帰分析",
    "section": "2 交差項",
    "text": "2 交差項\n\nはじめに \\[\n\\texttt{BMI} = \\beta_0 + \\beta_{\\texttt{LAB}}\\cdot\\mathtt{LAB} + \\beta_{\\texttt{LDL}}\\cdot\\mathtt{LDL} + \\beta_{\\texttt{LAB:LDL}}\\cdot\\mathtt{LAB}\\cdot\\mathtt{LDL} + \\epsilon\n\\] \\[\n\\beta_0\\sim\\mathrm{t}(3;\\mu_0,3.4),\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2),\n\\] \\[\n\\beta_{\\texttt{LAB}},\\beta_{\\texttt{LDL}},\\beta_{\\texttt{LAB:LDL}}\\sim\\mathrm{N}(0,\\infty),\\qquad\\sigma\\sim\\mathrm{t}(3;0,3.4),\n\\] というモデルを考える．\n\nlibrary(brms)\nmodel1 &lt;- bf(\n  BMI ~ LAB\n)\nfit1 &lt;- brm(\n  formula = model1,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nlibrary(knitr)\nkable(get_prior(\n  formula = model1,\n  data = raw_df\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nLAB\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 22.7, 3.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 3.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nplot(fit1, variable = c(\"b_Intercept\", \"b_LAB\"))\n\n\n\n\n\n\n\n\n\nsummary(fit1)$fixed\n\n            Estimate Est.Error   l-95% CI  u-95% CI      Rhat Bulk_ESS Tail_ESS\nIntercept 20.6961127  0.428332 19.8724047 21.541559 1.0000894 9919.088 7202.522\nLAB        0.6119695  0.105872  0.4014823  0.817622 0.9998797 9249.283 7186.599\n\n\n\\(\\beta_{\\texttt{LAB}}\\) の最頻値＝最尤推定量は \\(0.6\\) である．これは，LAB が \\(1\\) 違う個人の間で BMI の値が約 \\(0.6\\) 違うと解釈できる．\n例えば LAB が \\(3.0\\) の個人の予測される BMI は \\[\n\\mathtt{BMI}\\approx20.7+0.6\\times3.0=22.5\n\\] となる．\nここに新たな変数 LDL を追加すると，LAB の係数 \\(\\beta_{\\texttt{LAB}}\\) は \\(0.6\\) から \\(0.5\\) に減少する．これはどういう意味だろうか？\n\nmodel2 &lt;- update(model1, BMI ~ LAB + LDL)\nfit2 &lt;- brm(\n  formula = model2,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\"))\n\n\n\n\n\n\n\n\n一般に係数の追加は層別に当たる．例えばこの結果は，LDL の値が同じ人の中では LAB が \\(1\\) 違う人の BMI の値が \\(0.5\\) 違うと解釈できる．\n\nsummary(fit2)$fixed\n\n              Estimate   Est.Error     l-95% CI    u-95% CI     Rhat  Bulk_ESS\nIntercept 20.166740267 0.510522239 1.916021e+01 21.16019758 1.000753 12221.847\nLAB        0.481395182 0.124645917 2.389870e-01  0.72592090 1.001291  9120.764\nLDL        0.008630676 0.004390414 1.012456e-04  0.01724088 1.000804  9642.207\n          Tail_ESS\nIntercept 7745.032\nLAB       6973.939\nLDL       7388.183\n\n\n\nmodel3 &lt;- update(model2, BMI ~ LAB * LDL)\nfit3 &lt;- brm(\n  formula = model3,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit3, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\", \"b_LAB:LDL\"))"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#線型重回帰",
    "href": "posts/2024/Survey/BayesRegression.html#線型重回帰",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "2 線型重回帰",
    "text": "2 線型重回帰\n\n2.1 ベイズ単回帰\n\nはじめに \\[\n\\texttt{BMI} = \\beta_0 + \\beta_{\\texttt{LAB}}\\cdot\\mathtt{LAB} + \\beta_{\\texttt{LDL}}\\cdot\\mathtt{LDL} + \\beta_{\\texttt{LAB:LDL}}\\cdot\\mathtt{LAB}\\cdot\\mathtt{LDL} + \\epsilon\n\\] \\[\n\\beta_0\\sim\\mathrm{t}(3;\\mu_0,3.4),\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2),\n\\] \\[\n\\beta_{\\texttt{LAB}},\\beta_{\\texttt{LDL}},\\beta_{\\texttt{LAB:LDL}}\\sim\\mathrm{N}(0,\\infty),\\qquad\\sigma\\sim\\mathrm{t}(3;0,3.4),\n\\] というモデルを考える．\n\nlibrary(brms)\nmodel1 &lt;- bf(\n  BMI ~ LAB\n)\nfit1 &lt;- brm(\n  formula = model1,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nlibrary(knitr)\nkable(get_prior(\n  formula = model1,\n  data = raw_df\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nLAB\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 22.7, 3.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 3.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nplot(fit1, variable = c(\"b_Intercept\", \"b_LAB\"))\n\n\n\n\n\n\n\n\n\nsummary(fit1)$fixed\n\n            Estimate Est.Error   l-95% CI  u-95% CI      Rhat Bulk_ESS Tail_ESS\nIntercept 20.6961127  0.428332 19.8724047 21.541559 1.0000894 9919.088 7202.522\nLAB        0.6119695  0.105872  0.4014823  0.817622 0.9998797 9249.283 7186.599\n\n\n\\(\\beta_{\\texttt{LAB}}\\) の最頻値＝最尤推定量は \\(0.6\\) である．これは，LAB が \\(1\\) 違う個人の間で BMI の値が約 \\(0.6\\) 違うと解釈できる．\n例えば LAB が \\(3.0\\) の個人の予測される BMI は \\[\n\\mathtt{BMI}\\approx20.7+0.6\\times3.0=22.5\n\\] となる．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\nしかしこの図を見ればわかる通り，LAB は BMI の変動の一部しか説明しておらず，上述ような点推定的な議論にどれほど意味があるかは疑問である．\nベイズ回帰では幅を持って結果を理解できるため，その美点を活かさない理由はない．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nsims &lt;- as.matrix(fit1)\nsims_to_display &lt;- sample(nrow(sims), 100)\nfor (i in sims_to_display) {\n  abline(sims[i, 1], sims[i, 2], col = \"gray\")\n}\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n\n\n\n2.2 モデルのチェック\nさらにベイズ模型は事後予測分布をプロットし，実際の観測データと比べることで，モデルがデータ生成過程をどれほど反映できているかが瞬時に把握できる：\n\nsynthetic_data &lt;- posterior_predict(fit1, newdata = data.frame(LAB = 3.0), ndraws = 10000)\nhist(synthetic_data, nclass = 100, xlab = \"BMI\", main = \"Predicted BMI for a person with LAB = 3.0\")\n\n\n\n\n\n\n\n\n\np1 &lt;- pp_check(fit1, ndraws = 100)\np1\n\n\n\n\n\n\n\n\n事後予測分布のプロットを見ると，モデルが取り逃がしている構造として，BMI の分布が左右で非対称であることがあることがわかる．回帰直線のプロット 図 1 を見ても，直線の上側の点の方が裾が広く分散している．\n「やせ」と「肥満」は対称ではないのである．\n残差をプロットすることでさらに明らかになる：\n\nres &lt;- residuals(fit1)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 2: 残差のプロット\n\n\n\n\n\n\n\n2.3 変数の追加\nここに新たな変数 LDL を追加すると，LAB の係数 \\(\\beta_{\\texttt{LAB}}\\) は \\(0.6\\) から \\(0.5\\) に減少する．これはどういう意味だろうか？\n\nmodel2 &lt;- update(model1, BMI ~ LAB + LDL)\nfit2 &lt;- brm(\n  formula = model2,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\"))\n\n\n\n\n\n\n\n\n一般に係数の追加は層別に当たる．例えばこの結果は，LDL の値が同じ人の中では LAB が \\(1\\) 違う人の BMI の値が \\(0.5\\) 違うと解釈できる．\n\nsummary(fit2)$fixed\n\n              Estimate   Est.Error     l-95% CI    u-95% CI     Rhat  Bulk_ESS\nIntercept 20.167095694 0.508697419 1.915738e+01 21.16817592 1.000156 12406.505\nLAB        0.480590230 0.123523343 2.345347e-01  0.72295784 1.000166  8773.965\nLDL        0.008655655 0.004414599 1.466805e-04  0.01750134 1.000146  8828.879\n          Tail_ESS\nIntercept 7872.585\nLAB       7388.178\nLDL       7208.248\n\n\nここで \\(\\beta_{\\texttt{LDL}}\\) の値が極めて小さいことに気づくかもしれない．これは LAB に比べて LDL の影響が小さいことを意味しない．なぜならばこの２つの変数はスケールが約 \\(10^2\\) 違うためである．LDL は 100 のスケール，LAB は 1 のスケールである．\n説明変数 LAB と LDL のどちらが重要か，どっちをモデルに含めるべきかは全く別の方法で議論する必要がある．\n\n\n2.4 データの正規化\nそこでデータを正規化してみる：\n\ndf &lt;- data.frame(\n  sBMI = scale(raw_df$BMI),\n  sLAB = scale(raw_df$LAB),\n  sLDL = scale(raw_df$LDL)\n)\n\nmodel2s &lt;- bf(sBMI ~ sLAB + sLDL)\nfit2s &lt;- brm(\n  formula = model2s,\n  data = df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2s, variable = c(\"b_Intercept\", \"b_sLAB\", \"b_sLDL\"))\n\n\n\n\n\n\n\n\n\nsummary(fit2s)$fixed\n\n              Estimate  Est.Error     l-95% CI   u-95% CI      Rhat Bulk_ESS\nIntercept 7.522651e-05 0.03425035 -0.068425590 0.06721458 1.0014686 9820.954\nsLAB      1.541776e-01 0.03921660  0.077864765 0.23129153 0.9998343 8961.051\nsLDL      7.719841e-02 0.04016989 -0.001446599 0.15438400 1.0000044 9394.795\n          Tail_ESS\nIntercept 7058.981\nsLAB      7946.039\nsLDL      8158.999\n\n\nデータを正規化してしまったため，直接的な係数の解釈はできないが，係数を相互に比較できる．\nまたその他のモデルの性質は変わらない．例えば事後予測分布も変わらない．\n\nlibrary(gridExtra)\np2s &lt;- pp_check(fit2s, ndraws = 100)\np2 &lt;- pp_check(fit2, ndraws = 100)\ngrid.arrange(p2, p2s, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n2.5 交差項の係数の解釈\n再び正規化する前のデータに戻る．\n\nmodel3 &lt;- update(model2, BMI ~ LAB * LDL)\nfit3 &lt;- brm(\n  formula = model3,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit3, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\", \"b_LAB:LDL\"))\n\n\n\n\n\n\n\n\n\nsummary(fit3)$fixed\n\n              Estimate   Est.Error     l-95% CI     u-95% CI     Rhat Bulk_ESS\nIntercept 18.343555254 1.604795320 15.143167287 21.369150246 1.000525 3244.231\nLAB        0.951188612 0.411125758  0.170115468  1.769360158 1.000082 3333.452\nLDL        0.023882438 0.013536050 -0.001720447  0.050917031 1.000414 3315.768\nLAB:LDL   -0.003783052 0.003173761 -0.010074600  0.002145523 1.000361 3166.146\n          Tail_ESS\nIntercept 4340.650\nLAB       4113.096\nLDL       4512.152\nLAB:LDL   3934.990\n\n\n交差項を含む線型回帰における係数の解釈はさらに限定的になる．\n\\(\\beta_{\\texttt{LAB}}\\) は LDL が \\(0\\) である人が仮にいたとした場合の，LAB が \\(1\\) 違う人の間の BMI の平均的な違いを表す，と解釈できる．（LDL の平均が \\(0\\) になるように変数変換をして回帰するともっと自然な解釈ができる）．\n\\(\\beta_{\\texttt{LAB:LDL}}\\) は片方の係数 \\(\\beta_{\\texttt{LAB}}\\) を固定した際，LDL が \\(1\\) だけ違うグループにおける係数 \\(\\beta_{\\texttt{LDL}}\\) との違いを表す．\nすなわち交差項の追加は，LDL に依って層別し，それぞれのグループに異なる \\(\\beta_{\\texttt{LAB}}\\) を推定することを可能にする．この点で階層モデリングに似ている．\n\n\n2.6 交差項の効果\n交差項 LAB*LDL の追加は，LDL の違うサブグループの間に異なる LAB をフィッティングすることを可能にする．\nこのことを最もよく見るには，LDL が上半分か下半分かで LAB の係数がどう変わるかを見るのが良い．\n\nraw_df$LDLcate2 &lt;- ifelse(raw_df$LDL &gt; median(raw_df$LDL), \"High\", \"Low\")\n\n\nmodel3_cate &lt;- bf(BMI ~ LAB * LDLcate2)\nfit3_cate &lt;- brm(\n  formula = model3_cate,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nb_hat &lt;- summary(fit3_cate)$fixed\nabline(b_hat[1,1], b_hat[2,1], col = \"red\")\nabline(b_hat[1,1] + b_hat[3,1], b_hat[2,1] + b_hat[4,1], col = \"blue\")\nlegend(\"topleft\", # または \"topright\", \"bottomleft\", \"bottomright\" など\n       legend = c(\"High\", \"Low\"),\n       col = c(\"red\", \"blue\"),\n       lty = 1)\n\n\n\n\n\n\n\n\nLDL が大きいと，LAB の BMI に与える影響は緩やかになることがわかる．LDL の方が LAB の代わりに BMI の増加を説明してしまっているとも考えられる．\n\n\n2.7 まとめ\n線型回帰において，説明変数の追加は，「他の説明変数を固定したグループ内での」係数の推定に変化する（階層モデリングにつながる見方）．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#文献紹介",
    "href": "posts/2024/Survey/BayesRegression.html#文献紹介",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "6 文献紹介",
    "text": "6 文献紹介\n\n(10 節 Gelman et al., 2020) に線型重回帰モデルにおいて，係数の解釈法が丁寧に解説されている．\n(11 節 Gelman et al., 2020) はモデルの検証法を扱っている．\n(Gelman et al., 2020) では基本的に rstanarm パッケージを用いているが，本稿では brms パッケージを用いた．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#はじめに",
    "href": "posts/2024/Survey/BDA3.html#はじめに",
    "title": "ベイズデータ解析７",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 階層モデル\nデータが自然な 階層構造 を持つとする．例えば標本がクラスター抽出された場合，パネルデータや経時的繰り返し観測による標本，共変量統制とマッチングなど，自然な階層構造を持つデータは多い．\nこのようなデータに対して，個々のサブグループに対して全く同じ回帰モデルを繰り返し適用し，結果のプロットを小窓に分割して並べることで，グループごとの効果の違いを比較することができる．\nこの方法はナイーブながらも有力で，(10.9 節 Gelman et al., 2020, p. 148) では “secret weapon” と呼んでいる．\nこの別々の回帰モデルはベイズ的に統合して推論することができる．このグループごとに変動する係数を許したモデルが（ベイズ）階層モデル である．\n\n\n1.2 いつ使うか？\n線型回帰の枠組みで交差項を入れることで，グループごとに緩やかに異なる回帰係数を許すことができる．\nさらには一般化線型モデルを用いることで，連続・離散データの簡単な非線型関係まで扱うことができる．\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nそれでも階層モデルに進む動機として次のような点が挙げられる：\n\n\n\n\n\n\n階層モデルの美点1\n\n\n\n\n説明変数が多すぎる場合は馬蹄事前分布などの構造を持った事前分布が使用可能である．しかしデータが階層構造を持つ場合は，その構造に関する事前知識を用いて，自然な事前分布を組織的に導入することができる．これが階層モデリングに他ならないとも見れる．その結果大量の説明変数がある場合でも，その関連度をモデルに知らせて適切に推定する方法にもなる2．\n単一のグループではデータ数が小さい場合に，グループレベルの回帰係数 \\(\\alpha_{j[i]}\\) を安定して推定する手法とも理解できる．他のグループの回帰モデルと組み合わせた大きな階層モデルをベイズすることで，確度が低い（が関連する）モデルも捨てずに推定に利用するため (partial pooling)，より好ましい統計的推定を実行できる．これは 縮小推定 のキーワードの下でも追及されており，階層モデルは自然な縮小推定が実行される典型的な設定である．3\nグループ毎の回帰係数の違いに興味がある場合，グループレベルの変動の分散 \\(\\sigma_j^2\\) を推定する最も自然なモデルである．実際，一階層の回帰分析は \\(\\sigma_j=0\\) の場合，グループ毎に別々の回帰分析を実行する場合は \\(\\sigma_j=\\infty\\) という（場合によっては非現実的な）仮定を置いた縮退した階層モデルと見れる．\n\n\n\n全体の係数のみに興味がある場合，グループレベルの変動やデータの階層構造は局外構造である．このような状況では，誤差が相関を持つ場合にも頑健な点推定手法として，一般化推定方程式法なども用いることができる．4"
  },
  {
    "objectID": "posts/2024/Survey/BDA4.html",
    "href": "posts/2024/Survey/BDA4.html",
    "title": "ベイズデータ解析８",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA4.html#関連記事",
    "href": "posts/2024/Survey/BDA4.html#関連記事",
    "title": "ベイズデータ解析８",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#コピュラモデル",
    "href": "posts/2024/Survey/BDA2.html#コピュラモデル",
    "title": "ベイズデータ解析６",
    "section": "3 コピュラモデル",
    "text": "3 コピュラモデル\n\n3.1 はじめに\n以上の質的変数のモデルは，いずれも潜在的な連続変数の離散化としてデータを理解するものであった．\n仮にこの潜在変数により興味がある場合，この潜在変数をより具体的に，特にその相関構造をモデリングしたいということになる．\n(Lopez-Paz et al., 2013)"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#文献紹介",
    "href": "posts/2024/Survey/BDA2.html#文献紹介",
    "title": "ベイズデータ解析６",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n(Gelman et al., 2014) の 16 章で一般化線型モデルが扱われている．(Kruschke, 2015) はさらに詳しく，22 章で名目応答，23 章で順序応答，24 章でカウントデータを扱っている．\n(12 章 Hoff, 2009) にて正規コピュラモデルが ordinal probit モデルの，潜在変数を多次元に拡張した場合として導入されている．\n(Chib and Winkelmann, 2001) はリンクを対数関数とし，Poisson 周辺分布を持つカウントデータのコピュラモデリングを実行している．\n(Quinn, 2017) は連続確率変数に対してコピュラモデリングを実行している．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#モデル検証",
    "href": "posts/2024/Survey/BayesRegression.html#モデル検証",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "3 モデル検証",
    "text": "3 モデル検証\n\n3.1 はじめに\n残差プロットや事後予測プロットによるモデルの検証は，解析と並行して見てきた．\nここではより詳細に，モデルの予測性能に基づいた検証・比較方法を見る．\n交差検証法によるスコア elpd_loo によるモデル比較が一つ推奨される．\n\n\n3.2 決定係数\n図 1 の回帰直線のプロットと 図 2 の残差プロットを見ると，残差がまだ構造を持っていることがわかる．\n\nres &lt;- residuals(fit3)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 3: fit3 の残差のプロット\n\n\n\n\n\nこの残差は標本分散 \\(\\widehat{\\sigma}^2\\)\n\nsigma &lt;- sqrt(var(res)[1,1])\nprint(sigma)\n\n[1] 3.394462\n\n\nを持っている．\nひとまず LAB と LDL について回帰をすることで，データの変動がどれほど説明できたかを考えてみよう．\n\\[\nR^2:=1-\\frac{\\widehat{\\sigma}^2}{s_y^2}=\\frac{s_y^2-\\widehat{\\sigma}^2}{s_y^2}\n\\]\nという値は 決定係数 と呼ばれ，データ \\(y\\) の分散 \\(s_y^2\\) のうち「説明された分散」の割合を表す．1\n\n1-sigma^2/var(raw_df$BMI)\n\n[1] 0.04403279\n\n\nデータの変動の \\(4\\%\\) しか説明できていないことがわかる．\n\n\n3.3 ベイズ決定係数\nベイズ決定係数 (Andrew Gelman and Vehtari, 2019) は brms パッケージで次のように計算できる：\n\nbayes_R2(fit3)\n\n     Estimate  Est.Error       Q2.5      Q97.5\nR2 0.04735052 0.01359373 0.02345481 0.07669648\n\n\n以上の \\(R^2\\) の議論では係数を点推定して「残差」を議論していた．\n他の可能性も考慮して推定を改善するのがベイズのやり方である．\nベイズ決定係数は，事後予測分布からのサンプルを用いて複数回予測値 \\(\\widehat{y}_i\\) を計算し， \\[\nR^2_{\\texttt{Bayes}}:=\\frac{\\mathrm{V}[\\widehat{y}]}{\\mathrm{V}[\\widehat{y}]+\\sigma^2}\n\\] という値で「データの変動のうち説明された割合」を表す．\n\n\n3.4 AIC\n(Akaike, 1974) は次のように定義される： \\[\n\\mathtt{AIC}=-2\\biggr(\\sup_\\theta\\log p(y|\\theta)\\biggl)+2p.\n\\]\n第１項は deviance とも呼ばれ，残差を表す．\nAIC は新たなデータ点が観測された際の，そのデータ点に対するデビアンス（ある種の損失）の推定量となっており，小さいほどよい．\nAIC と同様の推定を，計算機集約的に行う方法に次節の交差検証法がある：\n\n\n3.5 交差検証\n事後予測検証では事後分布と観測を比較したが，よりこの好ましくは新しい（推定に用いていない）データと突き合わせることである．\nLOO (Leave-One-Out) 交差検証 (Stone, 1974) では，データを１つだけ抜いてモデルを推定し，このモデルの予測値と実際の値を比較するモデル検証法である．\nbrms パッケージでは loo パッケージ を内部で利用して高速に計算することができる．\n\nloo(fit3)\n\n\nComputed from 10000 by 839 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -2220.7 26.8\np_loo         5.6  0.7\nlooic      4441.4 53.5\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n回帰分析において予測値と実際の（省いていた）データの乖離は，AIC を踏襲して事後予測分布のスコア関数で測る (Vehtari et al., 2017, p. 1414)．\nelpd (expected log predictive density) は，LOO 交差検証により得る，（省いていた）データの対数尤度の平均である： \\[\n\\mathrm{elpd}_{\\text{loo}}:=\\sum_{i=1}^n\\log p(y_i|y_{-i})=\\sum_{i=1}^n\\int p(y_i|\\theta)p(\\theta|y_{-i})\\,d\\theta.\n\\tag{1}\\]\nこの値が大きいほどモデルの予測が良い．一般に elpd は，一度見たことあるデータ点に対する事後予測スコアよりも低くなる．2 この際の差は p_loo が測っており，乖離が大きすぎるとモデルがデータに過適合していることを表す．\np_loo は有効パラメータ数の（一致）推定量である．今回のモデルには切片項と LAB, LDL, LAB:LDL そして sigma の５つのパラメータがあるが，それより \\(0.6\\) だけ大きい値が出ている．\n最後の列は情報量規準のスケールにしたものである： \\[\n\\mathtt{looic}=-2\\times\\mathrm{elpd}_{\\text{loo}}.\n\\]\n一般に LOO-CV は計算が大変であるが，loo パッケージは Pareto Smoothed Importance Sampling (PSIS) (Vehtari et al., 2024) を用いて高速に計算している．\n\\(k&gt;0.7\\) の場合はこれがうまくいっていないことを示唆する．この下で \\(\\mathtt{p_loo}&gt;p\\) はモデルの誤特定を示唆する．\n\n\n3.6 事後予測スコアによるモデル比較\nbrms パッケージでは loo_compare 関数で２つのモデルの elpd スコアを比較できる：\n\nloo_compare(loo(fit1), loo(fit2))\n\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit1 -0.9       2.1   \n\n\nelpd_diff の値は標準偏差と比べて大変に小さい．交差検証の観点からは，LDL の追加は BMI の予測の観点から全く違いがないことがわかる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#footnotes",
    "href": "posts/2024/Survey/BayesRegression.html#footnotes",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n決定係数は，回帰が OLS 推定量により推定された場合には \\(Y\\) と説明変数のベクトル \\(X\\) との間の 重相関係数 とも一致する (Mudholkar, 2014)．(Gelman et al., 2020, p. 168) も参照．一般に決定係数は母集団の多重相関係数の一致推定量だと見れる．こうみた場合のバイアスを低減・脱離した値に，自由度調整済み決定係数，(Olkin and Pratt, 1958) 推定量などが存在する．↩︎\n訓練に用いたデータ点に関する事後予測スコアはdeviance と関係があり，\\(-2\\) を乗じたものは deviance と同じスケールになる (Gelman et al., 2020, p. 174), (Vehtari et al., 2017, p. 1427)．AIC は deviance から \\(2p\\) を引いたものである (Gelman et al., 2020, p. 175)．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#非線型性への憧憬",
    "href": "posts/2024/Survey/BayesRegression.html#非線型性への憧憬",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "4 非線型性への憧憬",
    "text": "4 非線型性への憧憬\n\n4.1 はじめに\nLAB による BMI への回帰の残差には，左右の非対称性が見られる 2.2．\nそして更なる変数 LDL の追加は予測の観点では特に影響がないことがわかった 3.6．\nそこでここでは手軽に非線型性を取り入れる方法として，データを変換することを考える．\n第 2.4 節でデータを標準化すると回帰モデルの係数の解釈が容易になることみた．\nしかしデータの線型変換は線型回帰モデルを変えず，推定には何の影響も与えない．\nそこでここでは非線型な変換に注目する．\n\n\n4.2 被説明変数の対数変換\n線型回帰モデリングにおける最大の仮定は，説明変数の加法性と，\\(y\\) への効果の線型性である： \\[\ny=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots.\n\\]\nLAB や LDL の BMI への影響が線型であると見るのは相当横暴な仮定である．LAB と LDL が両方高いことが相乗的に BMI を高めるシナリオの方があり得そうである．\nBMI も LAB も LDL も正の値しか取らないこともあり，対数変換を考えることは良い第一歩だろう．\n\nmodel1_log &lt;- update(model1, log(BMI) ~ LAB)\nfit1_log &lt;- brm(\n  formula = model1_log,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nres &lt;- residuals(fit1_log)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals (on log scale)\")\nabline(0,0)\n\n\n\n\n\n\n\n\n残差プロットを見ると，元のスケールでの線型回帰 図 2 と比べて非対称性は軽減している．\n\n\nlibrary(bayesplot)\n\n\nyrep &lt;- posterior_predict(fit1_log, ndraws = 100)\np1_log &lt;- ppc_dens_overlay(raw_df$BMI, exp(yrep))\ngrid.arrange(p1, p1_log, nrow=1)\n\n\n\n\n\n\n\n\n事後予測分布も改善しているのがわかる．\n\n\n4.3 変換前後のモデルの比較：Jacobian の補正\nただし，説明変数を対数スケールに変換してしまったので，直接 LOO スコアを比較することはできないことに注意する：\n\nloo_fit1 &lt;- loo(fit1)\nloo_fit1_log &lt;- loo(fit1_log)\nloo_compare(loo_fit1, loo_fit1_log)\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n         elpd_diff se_diff\nfit1_log     0.0       0.0\nfit1     -2656.8       8.2\n\n\n実際，fit1 の elpd が \\(-2211\\) であるのに対し，fit1_log は \\(426\\) とスケールが全く違う：\n\nloo_fit1_log\n\n\nComputed from 10000 by 839 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo    435.6 21.8\np_loo         3.0  0.3\nlooic      -871.3 43.6\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nしかし elpd の定義 (1) を見ると，\\(y_i\\) の密度変換をするために，Jacobian の値を加えれば良いことがわかる．\nそれぞれの項は loo(fit1_log)$pointwise[,1] に格納されている：\n\nsum(loo_fit1_log$pointwise[,1])\n\n[1] 435.6273\n\n\n\nloo_fit1_Jacobian &lt;- loo_fit1_log\nloo_fit1_Jacobian$pointwise[,1] &lt;- loo_fit1_log$pointwise[,1] - log(raw_df$BMI)\nsum(loo_fit1_Jacobian$pointwise[,1])\n\n[1] -2189.131\n\n\n\nloo_compare(loo_fit1_Jacobian, loo_fit1)\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n         elpd_diff se_diff\nfit1_log   0.0       0.0  \nfit1     -32.0       6.7  \n\n\n\n\n\n\n\n\n背景\n\n\n\n\n\n\\(x\\) を BMI，\\(y:=\\log x\\) とする．fit1_log の elpd は \\[\n\\log p(y)\n\\] によって計算されているところを，変数変換 \\[\np(y)\\,dy=\\frac{p\\left(\\log x\\right)}{x}\\,dx\n\\] を施すことで \\[\n\\log p(y)\\mapsto\\log p(\\log x)-\\log x\n\\] という関係を得る．\n左辺が fit1_log の elpd のスケールであり，右辺の \\(\\log p(\\log x)\\) の部分が fit1 の elpd のスケールである．\\(-\\log x\\) の Jacobian に対応する部分を加える必要がある．\n\n\n\n被説明変数に対数変換を施すことで，モデルの予測性能が改善したことがわかる．\n\n\n4.4 説明変数の対数変換：log-log モデル\n同様に説明変数にも対数変換を施すことができる．\n\nmodel1_loglog &lt;- update(model1_log, log(BMI) ~ log(LAB))\nfit1_loglog &lt;- brm(\n  formula = model1_loglog,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nyrep &lt;- posterior_predict(fit1_loglog, ndraws = 100)\np1_loglog &lt;- ppc_dens_overlay(raw_df$BMI, exp(yrep))\ngrid.arrange(p1_log, p1_loglog, nrow=1)\n\n\n\n\n\n\n\n\n予測性能はわずかに悪化していることがわかる：\n\nloo_compare(loo(fit1_log), loo(fit1_loglog))\n\n            elpd_diff se_diff\nfit1_log     0.0       0.0   \nfit1_loglog -1.8       1.1   \n\n\n\nmedian(loo_R2(fit1_log))\n\n[1] 0.02481491\n\nmedian(loo_R2(fit1_loglog))\n\n[1] 0.02229489\n\n\nしかし log-log モデルの係数は弾力性 (elasticity) としての解釈ができることもあり，解釈性の観点から選好されることがある (Gelman et al., 2020, p. 195)．\n\n\n4.5 離散変数化\n離散変数の扱いは連続変数よりも難しくなる．基本的に一般化線型モデルの枠組みが必要になる．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nしかし連続変数間の関数関係が複雑だと思われる場合，ノンパラメトリック手法を求める前に，変数を離散化して解析することも得るものが大きい場合が多い．\nこのような設定は，離散変数を扱う積極的な理由になる．非線型性を扱うために設定を簡略化するのである．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html",
    "href": "posts/2024/Survey/BayesGLM.html",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#はじめに",
    "href": "posts/2024/Survey/BayesGLM.html#はじめに",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\n多くの社会的なデータは非数値的である．しかしその背後には潜在的な連続変数を想定することが多い．\n加えて，線型回帰分析の結果複雑な非線型関係が予期された際，本格的なノンパラメトリック推論に移る前に，離散変数の設定に換言して非線型性を扱いやすくするなど，離散変数を扱う積極的理由もある．\n本稿ではロジスティック回帰を主に扱う．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Computation/brms.html",
    "href": "posts/2024/Computation/brms.html",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#例で学ぶ-brms-の使い方",
    "href": "posts/2024/Computation/brms.html#例で学ぶ-brms-の使い方",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "1 例で学ぶ brms の使い方",
    "text": "1 例で学ぶ brms の使い方\n\n1.1 カウントデータのモデリング\nDocumentation で紹介されている，Epilepsy Seizures Data (Leppik et al., 1987)，(Thall and Vail, 1990) を用いた 例 を実行してみる：\n\nlibrary(brms)\nfit1 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            data = epilepsy, family = poisson())\n\nてんかん (epilepsy) 患者の発作回数 count を被説明変数とし，処置の有無を表す説明変数 Trt と患者毎のランダム誤差を表す切片項 (1|patient)，及び標準化された説明変数 zAge, zBase への依存構造を調べたい．\n被説明変数 count は離散変数であり，Poisson 分布に従うと仮定する．過分散への対応を次の段階で考慮する．\n\n\n\n\n\n\n説明変数\n\n\n\n\nzAge：標準化された年齢\nzBase：ベースの発作回数\nTrt：治療の有無を表す２値変数\n(1|patient)：患者ごとに異なるとした変動切片項\n\nzBase * Trtという記法は，この２つの交互作用もモデルに含めることを意味する．この項の追加により，モデルは zBase の違いに応じて Trt の効果が変わる度合い \\(\\beta_4\\) を取り入れることができる．\n\n\n\nkable(head(epilepsy))\n\n\n\n\nAge\nBase\nTrt\npatient\nvisit\ncount\nobs\nzAge\nzBase\n\n\n\n\n31\n11\n0\n1\n1\n5\n1\n0.4249950\n-0.7571728\n\n\n30\n11\n0\n2\n1\n3\n2\n0.2652835\n-0.7571728\n\n\n25\n6\n0\n3\n1\n2\n3\n-0.5332740\n-0.9444033\n\n\n36\n8\n0\n4\n1\n4\n4\n1.2235525\n-0.8695111\n\n\n22\n66\n0\n5\n1\n7\n5\n-1.0124085\n1.3023626\n\n\n29\n27\n0\n6\n1\n5\n6\n0.1055720\n-0.1580352\n\n\n\n\n\n\n\n\n\n\n\nデータの詳細\n\n\n\nepilepsyは59 人の患者に関して，４回の入院時の発作回数を記録した，全 236 データからなる．patientが患者を識別する ID であり，(1|patient)は患者ごとのランダム効果ということになる．\n\n\n従って本モデルはzAge, zBase, Trt, Trt*zBaseという固定効果（係数），(1|patient)というランダム効果を取り入れた（一般化線型）混合効果モデル である．回帰式は次の通り： \\[\ny_{it} = \\beta_1 \\cdot\\texttt{zAge}_i+ \\beta_2 \\cdot \\texttt{zBase}_i + \\beta_3 \\cdot \\texttt{Trt}_i\n\\] \\[\n+ \\beta_4 \\cdot (\\texttt{zBase}_i \\cdot \\texttt{Trt}_i) + \\alpha_i +\\epsilon_{it}.\n\\] ただし，\\(\\texttt{count}_{it}\\) の Poisson 母数を \\(\\lambda_{it}\\) として，\\(y_{it}:=\\log(\\lambda_{it})\\) とした．\n\n\n\n1.2 brm() 関数\n\n1.2.1 family 引数\nここでは被説明変数 \\(y\\) に仮定するパラメトリック分布族と，リンク関数 \\(g\\) を指定する：\nfamily = brmsfamily(family = \"&lt;family&gt;\", link = \"&lt;link&gt;\")\n多くの場合 link 引数は省略可能である．\nこの２つの情報を通じて，一般化線型モデルを取り扱うことができる．\n\n\n\n1.3 モデルの推定と結果の解釈\n\nsummary(fit1)\n\n Family: poisson \n  Links: mu = log \nFormula: count ~ zAge + zBase * Trt + (1 | patient) \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~patient (Number of levels: 59) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.58      0.07     0.46     0.75 1.00     1016     1779\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.78      0.12     1.54     2.02 1.01      959     1321\nzAge           0.09      0.09    -0.08     0.26 1.01      765     1399\nzBase          0.70      0.12     0.47     0.93 1.01      813     1369\nTrt1          -0.27      0.17    -0.61     0.06 1.00      817     1205\nzBase:Trt1     0.05      0.16    -0.26     0.36 1.01      819     1692\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n基本的な解析の前提がまず出力され，推定結果はグループ毎（今回は患者毎）の変数（今回は \\(\\alpha_i\\)）から表示される．\n後半に固定効果の係数，すなわち回帰係数の推定結果が表示される．\n治療効果Trtの係数は負で，平均的に処置効果はある可能性があるが，95% 信頼区間は \\(0\\) を跨いでいるという意味で，有意とは言えない．また，交差項zBase*Trtの係数は小さく，交互効果の存在を示す証拠はないと思われる．\n\\(\\widehat{R}\\) (Gelman and Rubin, 1992) は MCMC の収束に関する指標で，１より大きい場合，MCMC が収束していない可能性を意味する (Vehtari et al., 2021)．通説には \\(\\widehat{R}\\le1.1\\) などの基準がある．\n\n\n1.4 プロット\n変数を指定して，事後分布と MCMC の軌跡をプロットできる：\n\nplot(fit1, variable = c(\"b_Trt1\", \"b_zBase\"))\n\n\n\n\n\n\n\n\nより詳しく見るにはconditional_effects関数を用いることもできる．交差項の効果はほとんどないことがわかる：\n\nplot(conditional_effects(fit1, effects = \"zBase:Trt\"))\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\n\n\n1.5 モデルによる予測\nfit したモデル fit1 を用いて，平均年齢と平均ベースレートを持つ患者に対する治療効果を予測する：\n\nnewdata &lt;- data.frame(Trt = c(0, 1), zAge = 0, zBase = 0)\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5  Q97.5\n[1,]   5.9710  2.530960    2 11.025\n[2,]   4.5425  2.178275    1  9.000\n\n\n関数predict()は事後予測分布からのサンプリングを行う．一方で，関数fitted()は平均を返す．\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.953824  0.712907 4.682468 7.518386\n[2,] 4.525703  0.545437 3.518021 5.614800\n\n\n\n\n\n\n\n\n予測の出力\n\n\n\n\n\n従って，もう１度ずつ実行すると，predictでは値が変わるが，fittedでは同じ値が出力される．\n\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  5.90800  2.532418    2    11\n[2,]  4.57725  2.256725    1    10\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.953824  0.712907 4.682468 7.518386\n[2,] 4.525703  0.545437 3.518021 5.614800\n\n\n\n\n\n\n\n1.6 モデルの比較\nモデルfit1で行った Poisson 回帰分析は，fit1に含めた説明変数の違いを除けば，個々の観測が独立になる，という仮定の上に成り立っている（第 4.3 節）．\nこの仮定が破れているとき＝全ての説明変数をモデルに含めきれていないとき，Poisson 分布の性質 \\[\n\\operatorname{E}[X]=\\mathrm{V}[X]=\\lambda\\qquad (X\\sim\\mathrm{Pois}(\\lambda))\n\\] からの離反として現れ，この現象は 過分散（overdispersion）とも呼ばれる．\n\n1.6.1 観測レベルランダム効果\nということで，他の説明変数が存在した場合を想定して， Poisson 分布族ではなく，分散が平均よりも大きいような別の分布族を用いて，フィット度合いを比較してみることを考えたい．\nそこで，追加の変動をモデルに追加するべく，モデルfit1に観測ごとの切片項 \\(\\eta_{it}\\) を追加してみる（この手法は観測レベルランダム効果と呼ばれる．第 3.3 節参照）．\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.73 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.956 seconds (Warm-up)\nChain 1:                1.252 seconds (Sampling)\nChain 1:                3.208 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.8e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 2.046 seconds (Warm-up)\nChain 2:                1.325 seconds (Sampling)\nChain 2:                3.371 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.909 seconds (Warm-up)\nChain 3:                1.142 seconds (Sampling)\nChain 3:                3.051 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.9e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.783 seconds (Warm-up)\nChain 4:                1.106 seconds (Sampling)\nChain 4:                2.889 seconds (Total)\nChain 4: \n\n\n\n\n\nこうして得た２つのモデルfit1,fit2を比較する．\nLLO (Leave-One-Out) cross-validation が関数looによって実行できる：\nloo(fit1, fit2)\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit1, fit2)\n\nWarning: Found 9 observations with a pareto_k &gt; 0.7 in model 'fit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 61 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit1':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -670.5 35.9\np_loo        93.4 13.7\nlooic      1341.1 71.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.9]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     227   96.2%   135     \n   (0.7, 1]   (bad)        8    3.4%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   1    0.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -594.9 13.8\np_loo       107.4  7.0\nlooic      1189.8 27.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.5]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     175   74.2%   169     \n   (0.7, 1]   (bad)       57   24.2%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   4    1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2   0.0       0.0  \nfit1 -75.7      26.5  \n\n\n\n\n\nelpd_diff は expected log posterior density の差異を表す．fit2の方が大きく当てはまりが良いことが見て取れる．\nまた，WAIC (Watanabe-Akaike Information Criterion) も実装されている：\nprint(waic(fit1))\n\n\n\n\n\n\nWAIC の結果\n\n\n\n\n\n\nprint(waic(fit1))\n\nWarning: \n53 (22.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -668.6 36.8\np_waic        91.4 14.7\nwaic        1337.2 73.6\n\n53 (22.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nprint(waic(fit2))\n\nWarning: \n63 (26.7%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -572.5 12.0\np_waic        85.0  5.2\nwaic        1144.9 24.0\n\n63 (26.7%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\n\n\n\n他にも，reloo, kfold などの関数もある．\n\n\n\n\n\n\n他の関数一覧\n\n\n\n\n\n\nmethods(class=\"brmsfit\")\n\n [1] add_criterion           add_ic                  as_draws_array         \n [4] as_draws_df             as_draws_list           as_draws_matrix        \n [7] as_draws_rvars          as_draws                as.array               \n[10] as.data.frame           as.matrix               as.mcmc                \n[13] autocor                 bayes_factor            bayes_R2               \n[16] bridge_sampler          coef                    conditional_effects    \n[19] conditional_smooths     control_params          default_prior          \n[22] expose_functions        family                  fitted                 \n[25] fixef                   formula                 getCall                \n[28] hypothesis              kfold                   log_lik                \n[31] log_posterior           logLik                  loo_compare            \n[34] loo_linpred             loo_model_weights       loo_moment_match       \n[37] loo_predict             loo_predictive_interval loo_R2                 \n[40] loo_subsample           loo                     LOO                    \n[43] marginal_effects        marginal_smooths        mcmc_plot              \n[46] model_weights           model.frame             nchains                \n[49] ndraws                  neff_ratio              ngrps                  \n[52] niterations             nobs                    nsamples               \n[55] nuts_params             nvariables              pairs                  \n[58] parnames                plot                    post_prob              \n[61] posterior_average       posterior_epred         posterior_interval     \n[64] posterior_linpred       posterior_predict       posterior_samples      \n[67] posterior_smooths       posterior_summary       pp_average             \n[70] pp_check                pp_mixture              predict                \n[73] predictive_error        predictive_interval     prepare_predictions    \n[76] print                   prior_draws             prior_summary          \n[79] psis                    ranef                   reloo                  \n[82] residuals               restructure             rhat                   \n[85] stancode                standata                stanplot               \n[88] summary                 update                  VarCorr                \n[91] variables               vcov                    waic                   \n[94] WAIC                   \nsee '?methods' for accessing help and source code\n\n\n\n\n\n\n\n1.6.2 患者内の相関構造のモデリング\nまた，fit1において，同一患者の異なる訪問の間には全く相関がないと仮定されており，これは全く非現実的な仮定をおいてしまっていると言える．1\n患者内の相関構造は，brm()関数のautocor引数で指定できる（第 4.3.2 節）．\n例えば，全く構造を仮定しない場合は，unstrを指定する：\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\nWarning: Argument 'autocor' should be specified within the 'formula' argument.\nSee ?brmsformula for help.\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000137 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.37 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 4.067 seconds (Warm-up)\nChain 1:                2.528 seconds (Sampling)\nChain 1:                6.595 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.4e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 3.802 seconds (Warm-up)\nChain 2:                2.66 seconds (Sampling)\nChain 2:                6.462 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 4.17 seconds (Warm-up)\nChain 3:                2.317 seconds (Sampling)\nChain 3:                6.487 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 4.145 seconds (Warm-up)\nChain 4:                2.165 seconds (Sampling)\nChain 4:                6.31 seconds (Total)\nChain 4: \n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\n\n\nこのモデルもfit1より遥かに当てはまりが良く，fit2とほとんど同じ当てはまりの良さが見られる：\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit2,fit3)\n\nWarning: Found 61 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 62 observations with a pareto_k &gt; 0.7 in model 'fit3'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -594.9 13.8\np_loo       107.4  7.0\nlooic      1189.8 27.7\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.5]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     175   74.2%   169     \n   (0.7, 1]   (bad)       57   24.2%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   4    1.7%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit3':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -601.1 14.6\np_loo       112.2  7.8\nlooic      1202.2 29.3\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     174   73.7%   157     \n   (0.7, 1]   (bad)       54   22.9%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   8    3.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit3 -6.2       2.6   \n\n\n\n\n\n思ったよりもfit2の当てはまりが良いため，Poisson-対数正規混合モデリングを本格的に実施してみることが，次の選択肢になり得る（第 3.3 節参照）．\n\n\n\n1.7 その他の例\nSebastian Weber らにより，新薬の治験における実際の解析事例をまとめたウェブサイト が公開されている．2\n特に，13 章で，同様の経時的繰り返し観測データを扱っているが，ここではカウントデータではなく連続な応用変数が扱われている．\nbrms は他にもスプラインや Gauss 過程を用いた一般化加法モデルの推論も可能である (Bürkner, 2018)．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "href": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "2 ランダム効果モデルの正しい使い方",
    "text": "2 ランダム効果モデルの正しい使い方\n\n\n\n\n\n\n概要\n\n\n\n\nランダム効果モデル とは，グループ毎に異なる切片項 \\(\\alpha_{s[i]}\\) を追加し，これにも誤差を仮定してモデルに入れて得る階層モデルである（狭義の用法）．この意味では変動切片モデルともいう．\nしかしランダム効果 \\(\\alpha_{s[i]}\\) が（ユニットレベルの）説明変数 \\(x_i\\) と相関を持つ場合，推定量の一致性が失われる．これを回避するために，\\(x_i\\) の係数 \\(\\beta\\) にのみ関心がある場合は固定効果推定量や一般化推定方程式 (GEE) が用いられることも多い．\nだが，ランダム効果モデルにおいても未観測の説明変数が存在する場合でも，簡単なトリック（ランダム効果 \\(\\alpha_{s[i]}\\) のグループレベル説明変数に \\(\\overline{x}_s\\) を追加すること）で，推定量の一致性を回復することができる．\nこのトリックを取り入れたランダム効果モデルは，\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がない場合は固定効果モデルと等価な \\(\\beta\\) の推定量を与え，相関がある場合でも \\(\\beta\\) を一致推定し，各変動切片項 \\(\\alpha_{s[i]}\\) の構造にも洞察を与えてくれる．さらには内生性の度合いを推定するといった構造的な洞察も可能である (Chan and Tobias, 2020, p. 14)．\n以上のランダム効果モデルや固定効果モデルは，異なるモデルに対するベイズ推定と見れる．どのモデルを採用するのが良いか，実経計画の時点からは判らない場合，ベイズ階層モデルの方法で探索的に実行することも可能である．\n\n\n\nランダム効果モデルは「通常の固定効果のみを含んだ線型回帰モデルにランダム項を導入したもの」という意味でも用いられる．この場合は 線型混合モデル (Linear Mixed Model; LMM) の別名と理解できる．\n\n2.1 ランダム効果モデル\nランダム効果 は，変動する切片項 (Gelman, 2005) (Bafumi and Gelman, 2007) という別名も提案されているように，サブグループ毎に異なる切片項のことである．4\nユニット（個人などの最小単位）レベルの回帰式を書き下すと，グループ選択関数 \\(s:[n]\\to[S]\\;(S\\le n)\\) を通じて， \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\n\\tag{1}\\] というようになる．\nこれは，確率変数 \\(\\alpha_{s[i]}\\) の平均を \\(\\alpha_0\\) とすると，グループレベルの回帰式 \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad s\\in[S]\n\\tag{2}\\] が背後にある 階層モデル (multilevel / hierarchical model) だとみなすこともできる．\n\n\n2.2 説明変数との相関の問題\n\n2.2.1 問題の所在\nランダム効果では，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s}\\) が相関を持たないという仮定が Gauss-Markov の定理の仮定に等価になるため，これが違反されると \\(\\beta\\) の OLS 推定量の不偏性・一致性が約束されず，推定量の分散も大きくなる．5\n\n\n\n\n\n\nGauss-Markov の仮定\n\n\n\nユニットレベル回帰式 \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] において，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s[i]}\\) が相関を持たないこと．\n\n\n実際，ランダム効果モデルの階層構造を，(2) を (1) に代入することで一つの式にまとめると \\[\ny_i=\\alpha_0+\\beta x_i+\\underbrace{\\epsilon_i'}_{\\epsilon_i+\\eta_{s[i]}}\n\\tag{3}\\] を得る．\n\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がある場合，\\(x_i\\) と \\(\\eta_s\\) にも相関があるため，結果として (3) では説明変数と誤差 \\(\\epsilon_i'\\) に相関が生じてしまう．これは計量経済学では 内生性 (endogeneity) の問題と呼ばれているものに他ならない．\n\n\n2.2.2 代替モデル１：母数効果モデル\nそのため，ランダム効果モデルは避けられる傾向にあり，切片項 \\(\\alpha_{s[i]}\\equiv\\alpha_0\\) は変動しないとし，グループレベルの効果を無視してモデリングすることも多い： \\[\ny_i=\\alpha_0+\\beta x_i+\\epsilon_i.\n\\] このことを 完全プーリングモデル (complete pooling model) または母数効果モデルと呼び，ランダム効果モデルを 部分プーリングモデル (partial pooling model) と呼んで対比させることがある．6\n周辺モデル (marginal model) や 母平均モデル (population-average model) とも呼ばれる (Gardiner et al., 2009, p. 228)．\n実際，これ以上の仮定を置かず，ランダム効果は局外母数として（母数効果ともいう）一般化推定方程式の方法（第 2.6 節）によれば，\\(\\beta\\) の不偏推定が可能である．\nリンク関数 \\(g\\) を通じた非線型モデル \\[\ng(\\operatorname{E}[y_i|x_i])=\\beta x_i\n\\] であっても，指数型分布族を仮定すれば（すなわち一般化線型モデルについては），\\(\\beta\\) の一致推定が可能である．\nだが，切片項の変動を消してしまうことで，回帰係数 \\(\\beta\\) の推定に対する縮小効果（第 3.2 節）が得られないという欠点もあり，小地域推定などにおいては \\(\\alpha_{s[i]}\\) を確率変数とみなす積極的理由もある．この点については (久保川達也, 2006), (Sugasawa and Kubokawa, 2023) も参照．\n\n\n2.2.3 代替モデル２：固定効果モデル\n問題を起こさずに，しかしながらグループレベルの効果をモデリングしたい場合， \\[\ny_i=\\alpha_{s[i]}^{\\text{unmodeled}}+\\beta x_i+\\epsilon_i\n\\] として，グループ毎に変動する切片項 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を許すが，この変数自体にモデルは仮定しない，とすることもできる．\nしたがってグループ毎に別々の回帰分析を実行し，別々の切片 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を得て，\\(\\beta\\) の値はこれらのグループの間で適切に重みづけて最終的な推定値としているに等しい．\nすなわち，グループの数だけ，グループへの所属を表す２値変数 \\(1_{\\left\\{s[i]=s\\right\\}}\\) を導入し，\\(S\\) 個の項 \\(\\sum_{s=1}^S1_{\\left\\{s[i]=s\\right\\}}\\alpha_{s[i]}^{\\text{unmodeled}}\\) を説明変数に加えて回帰分析を行うことに等しい．\nまた群内平均を引いた値 \\(y_i-\\overline{y}_{s[i]}\\) を目的変数として，説明変数 \\(x_i-\\overline{x}_{s[i]}\\) により回帰分析を行うこととも等価である．この変換により \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) が消去されると考えられるのである．\n\n\n\n\n\n\n固定効果モデルの別名\n\n\n\n\n(Hansen, 2022) をはじめ，計量経済学では fixed effects model と呼ばれる．7\n(Bafumi and Gelman, 2007) は unmodeled varying intercept と呼んでいる．\nleast squares dummy variable regression とも呼べる．8\n\n\n\n\n\n2.2.4 固定効果 vs. 変量効果\n\n\n\n\n\n\n固定効果モデルの利点\n\n\n\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持ち得る場合も，固定効果モデルでは問題が生じない．9\n\n\n\n\n\n\n\n\n変量効果モデルの利点\n\n\n\n固定効果モデルでは異なるグループのデータが相互作用する機構がランダム効果モデルに比べて貧しい．\n一方でランダム効果モデルを用いた場合，外れ値グループが存在するなどノイズの大きなデータに対しても，\\(\\eta_s\\) を通じて緩やかに情報が伝達され，\\(\\beta\\) の値は平均へ縮小されて推定される（第 3.2 節）．これは Stein 効果とも呼ばれる (Hoff, 2009, p. 146)．固定効果モデルではそのような頑健性を持たない (Bafumi and Gelman, 2007, pp. 4–5)．\nさらには変量効果モデルにおいては \\(\\epsilon_i\\) と \\(\\eta_s\\) の相関を推定することができ，これは内生性の強さの尺度として使える (Chan and Tobias, 2020, p. 14)．\n\n\n固定効果モデルは \\(\\beta\\) （のみ）に関心がある場合，\\(\\alpha_{s[i]}\\) と \\(x_i\\) の相関の存在に対してロバストな推定法として有用であり，その理由で計量経済学（特に線型パネルデータ）では主流の推定手法となっている．10\n実際，\\(\\alpha_{s[i]}\\) と \\(x_i\\) が無相関であるとき，変量効果モデルと固定効果モデルは \\(\\beta\\) に関しては等価な推定量を与える．\n\nCurrent econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estmimator for linear panel data models. (Hansen, 2022, p. 624)\n\n逆に言えば，固定効果モデルは \\(x_i\\) と \\(\\alpha_{s[i]}\\) の構造のモデリングを放棄したモデリング法であり，各 \\(\\alpha_{s[i]}\\) の値にも興味がある場合，または \\(\\beta\\) のより精度の高い推定が実行したい場合には，やはり \\(\\alpha_{s[i]}\\) の誤差と相関構造もモデルに取り入れたランダム効果モデルを用いたいということになる．\n\n\n\n2.3 階層モデルによる総合\nランダム効果モデルは， \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad \\eta_s\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2),\n\\] というグループレベルの回帰モデルの想定された階層モデルと見れるのであった（第 2.1 節）．\nすると完全プーリングモデル（第 2.2.2 節）は \\(\\sigma^2\\to0\\) の場合，固定効果モデル（第 2.2.3 節）は \\(\\sigma^2\\to\\infty\\) の場合の，特定の点推定法と見れる．\n換言すれば，improper な一様事前分布 \\[\n\\alpha_s^{\\text{unmodeled}}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(\\alpha_0,\\infty)\n\\] を仮定した場合が固定効果モデルであると理解される (Gelman et al., 2014, p. 383)．\nこの点については次稿も参照：\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.4 ランダム効果モデルにおける相関のモデリング\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持つ場合に，効果 \\(\\beta\\) の OLS 推定量の一致性が保証されないことがランダム効果モデルの欠陥だと述べたが，実はこれは簡単な方法で解決できる．\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) との相関は，欠落変数（未観測の交絡因子）が存在するため，と考えることができる．\nランダム効果モデリングではこの欠落変数に対する操作変数を人工的に作り出すことができる．\nというのも，説明変数の平均 \\(\\overline{x}_s\\) を変動する切片項 \\(\\alpha_s\\) の説明変数として追加することで除去できる：11\n\\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i\n\\] \\[\n\\alpha_s=\\alpha_0+\\alpha_1\\overline{x}_s+\\eta_s\n\\tag{4}\\]\nこれにより，Gauss-Markov の仮定（外生性）が回復される．\n(Bafumi and Gelman, 2007, pp. 7–9) ではこの効果をシミュレーションによって検証している．\n\nPractitioners can get around this problem by taking advantage of the multilevel structure of their regression equation. (Bafumi and Gelman, 2007, p. 12)\n\n\n\n2.5 第三の名前：混合効果モデル\n以上，解説してきたランダム効果モデル／変量効果モデルであるが，混合効果モデル とも呼ばれる．12\n何を言っているのかわからないかもしれないが，式 (1) \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] において，\\(\\alpha_{s[i]}\\) がランダム効果であるが，回帰係数 \\(\\beta\\) を 固定効果 とも呼ぶことがあるのである．\nそしてこう見ると全体として固定効果と変量効果が同居した 混合（効果）モデル とも呼べそうである．\nこれは変動切片項だけを変量効果と呼ぶのではなく，一般の回帰係数 \\(\\beta x\\) もグループ \\(s\\in[S]\\) ごとに異なるものを許す場合は広義の変量効果と呼べることから生じる．\n\n\n\n\n\n\n\n\n線型混合モデルの別名\n\n\n\n式 (1) \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] で定義されるモデルは，(Chung et al., 2013) によると次のような複数の名前を持つ：\n\n線型混合モデル (linear mixed models) (Kincaid, 2005)\n階層モデル (hierarchical models)\nマルチレベル線型モデル (multilevel linear models)\n混合効果モデル (mixed-effects models) (Chung et al., 2015)\nランダム効果モデル (random effects model) (Hubbard et al., 2010) や (Bafumi and Gelman, 2007)．\n分散成分モデル (variance component model)13\n\n\n\n詳しくは (第6章 Gelman, 2005, pp. 20–) も参照．\n\n\n2.6 GEE との違い\n一般化推定方程式 (GEE: Generalized Estimating Equation) では，ランダム効果モデルにおける階層的な議論を全て「局外母数」として捨象し，母数 \\(\\beta\\) の推定に集中する見方をする．\nなお，GEE そのものはあらゆる一般化されたスコア方程式を指し得る．そのため分散成分の推定にも応用可能であろう．例えば (Sugasawa and Kubokawa, 2023, p. 13) など．\n\n\n\n\n\n\nGEE との違い\n\n\n\n\n回帰式が違う\nGEE の文脈ではよくモデル式 \\[\n   Y_{it}=\\alpha+\\beta_1x_{1,i,t}+\\cdots+\\beta_px_{p,i,t}\n   \\] にはランダムな切片項というものは見当たらない．その代わり，グループ間の依存関係は相関係数行列としてモデル化を行う．ランダム効果モデルでは，この相関構造をランダムな切片項を追加することで表現し，回帰式を複数立てることでモデルを表現するのと対照的である．\n推定目標が違う\nGEE は population average model でよく用いられる (Hubbard et al., 2010) ように，あくまで応答 \\(Y_{it}\\) の平均 \\(\\beta\\) の不偏推定が目標であり，共分散構造はいわば局外母数である．一方混合効果モデルは，その階層モデルとしての性質の通り，平均構造と分散構造のいずれも推定するという志向性がある．\n推定方法が違う\n頻度論的には，混合効果モデルは主に最尤法により推定される (Hubbard et al., 2010)．一方で GEE はモーメント法により推定され，最尤法ベースではない． \n\n\n\nGEE にとって相関構造は局外母数であり，正確な特定は目的に含まれない．この意味で GEE の相関係数⾏列におく仮定は「間違えていてもよい便宜的な仮定」であるため，作業相関係数行列 (working correlation coefficient matrix) とも呼ばれる．相関構造を誤特定していても，平均構造は一致推定が可能であり，ロバストである．両方の特定に成功した場合はセミパラメトリック有効性が達成される．\n一方で混合効果モデルは，階層モデルとして平均構造と分散構造のいずれにも明示的な仮定をおくため，片方（例えば共分散構造）の特定を間違えていた場合，もう片方の解釈性が失われる，というリスクがあると論じることができる．特に (Hubbard et al., 2010) に見られる論調である．\nしかし小地域推定や地域ごとのばらつきに注目した研究など，ユニットの平均効果ではなく個別効果に注目したい場合には混合効果モデルの方が適していることになる (Gardiner et al., 2009)．実際，モデルの特定に成功していれば，いずれのパラメータも最尤推定されるため一致性を持つ．\n\n\n\n\n2.7 ベイズ混合効果モデルという光\n第 2.3 節で見た通り，ベイズ統計学の立場からは，変量効果モデル・固定効果モデル・完全プーリングモデルはいずれもモデルの違いとして理解できる．\nそれぞれに自然な点推定法は違うかもしれないが，それだって特定の事前分布に関するベイズ推論の特殊な場合に過ぎない．\nそれぞれのモデルに関してベイズ推論をし，周辺化をして平均構造に関する marginal estimator を構成すれば GEE や固定効果推定量の代用になる上に，どのような構造的な仮定を置いてしまっているか反省する契機にもなる．\n計算機の性能と，計算統計手法の発展が目まぐるしい現代にて，過去の議論を踏襲しすぎることは，問題の本質を誤るということもあるのだろう．\n\nこの節はこれで終わり．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#混合効果モデリングのテクニック集",
    "href": "posts/2024/Computation/brms.html#混合効果モデリングのテクニック集",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "3 混合効果モデリングのテクニック集",
    "text": "3 混合効果モデリングのテクニック集\n\n\n\n\n\n\n概要\n\n\n\n\n混合効果モデルの最尤推定・ベイズ推定において，グループレベル変動 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になり得る．特に，グループ数 \\(S\\) が小さい場合に顕著である．\n分散成分を推定したあと，これを係数 \\(\\beta\\) の推定量に代入することで，縮小効果を持った効率的な推定量を得ることができる．\nカウントデータの Poisson モデルでは，「観測レベルのランダム効果」を追加することで，実質的に Poisson-対数正規混合モデリングを実行できる．\n\n\n\n\n3.1 グループレベル分散の推定\n\n3.1.1 問題\n変量効果モデル \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] の推定において，特にグループ数 \\(S\\) が小さい場合，グループレベルの変動切片項 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になったり，分散が負の値をとったりするという問題点が古くからある (Harville, 1977)．14\n変量効果 \\(\\eta_s\\) を \\(\\eta_s\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2_s)\\)，誤差を \\(\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2_e)\\) とすると，この \\(\\mathrm{V}[\\eta_s]\\) は次の形をもち，グループ間の相関構造のモデリングを一手に引き受けている： \\[\n\\mathrm{V}[\\eta_{s}]=\\sigma^2_sJ_{n_s}+\\sigma_e^2I_{n_s},\\qquad J_{n_s}:=\\boldsymbol{1}_{n_s}\\boldsymbol{1}_{n_s}^\\top.\n\\]\nEM アルゴリズムが提案されたばかりの頃 (Laird and Ware, 1982) では，共分散構造にパラメトリックな仮定をおいていたが，現代ではこれを取り去った最尤推定法・ベイズ推定法が主流である．\n\n\n3.1.2 退化しない共分散行列推定\nしかし，最尤推定法と，一定の事前分布を仮定したベイズ MAP 推定法では，推定された共分散行列が退化してしまったり，分散が負の値を取ってしまうことがある．\n打ち切り推定量 (Kubokawa and Srivastava, 1999), (Kubokawa, 2000) なども提案されているが，ベイズでは Wishart 事前分布を仮定することでこれが回避される (Chung et al., 2015)．15 これは最尤法の文脈では，penalized likelihood と等価になる (Chung et al., 2013)．\nモデルのサイズによっては，完全なベイズ推定を実行することが難しく，一部は等価な頻度論的な方法や近似を用いることもある．その際，最適化ソルバーの収束を速めるために，共分散構造に（データや計画とは無関係に）パラメトリックモデルを仮定してしまうこともある (Kincaid, 2005)．\n\n\n\n3.2 係数の縮小推定\n\n3.2.1 係数の２段階推定\n分散 \\(\\mathrm{V}[\\eta_s]\\) を推定して分散比 \\(\\rho:=\\sigma_v^2/\\sigma_e^2\\) の推定量 \\(\\widehat{\\rho}\\) を得て，これを最良線型不偏推定量 (BLUE) \\(\\widehat{\\beta}\\) に代入して得られる，グループごとの \\(y_s\\) の推定量に \\[\n\\widehat{y}_s:=\\frac{\\widehat{\\rho}n_s}{1+\\widehat{\\rho}n_s}\\overline{y}_s+\\frac{1}{1+\\widehat{\\rho}n_s}\\overline{x}_s^\\top\\widetilde{\\beta}(\\widehat{\\rho})\n\\] というものがあり，これを 経験 BLUE という (久保川達也, 2006, p. 143)．\nこれは，各グループ \\(s\\in[S]\\) における値 \\(y_s\\) を，単なる経験平均 \\(\\overline{y}_s\\) ではなく，全データプールから得られる推定量 \\(\\overline{x}_s^\\top\\widetilde{\\beta}(\\widehat{\\rho})\\) で補正した推定量になっている．\nこのことにより，各グループ \\(s\\in[S]\\) のデータ数が少なく，経験平均 \\(\\overline{y}_s\\) では分散が大きくなってしまう場合でも，安定した推定量を得ることができる．\n縮小推定は小地域推定 (Battese et al., 1988) に応用を持つ．例えば \\(s\\in[S]\\) をアメリカ合衆国の各州とし，投票行動のデータに応用した例が (Gelman, 2014) にある．\nこのように，変量効果 \\(\\alpha_{s[i]}\\) を追加したモデリングを実行することにより，グループごとの被説明変数を縮小推定することができる．\n\n\n3.2.2 経験ベイズ\n縮小推定の効用は初め，経験ベイズの枠組みで説明された．\n\n以上の考え方は，経験ベイズの枠組みで (Efron and Morris, 1975) の一連の論文の中で示されてきたものであり，ベイズ的アプローチの現実的な有用性は基本的には上述の考え方に基づいている．\n\nそもそも１元配置混合線型モデルは \\[\ny_{ij}=\\theta_{ij}+e_{ij},\\qquad \\theta_{ij}=x_{ij}^\\top\\beta+v_i\n\\] とも理解できる．これは階層モデル \\[\ny_{ij}\\sim\\mathrm{N}(\\theta_{ij},\\sigma^2_e),\\qquad\\theta_{ij}\\sim\\mathrm{N}(x_{ij}^\\top\\beta,\\sigma_v^2)\n\\] とも見れる．\n\\(\\beta,\\sigma^2_v,\\sigma^2_e\\) を未知母数として扱った場合を 経験ベイズモデル，変量として扱って更なる分布を仮定した場合を（狭義の） 階層ベイズ ともいう (久保川達也, 2006, p. 155)．\n\n\n\n3.3 カウントデータ過分散へのお手軽対処法\nこれはカウントデータのモデリング限定のテクニックである．\nカウントデータも，一般化線型（混合）モデルの範疇で扱うことができるため，リンク関数 \\(g\\) を通じてほとんど同等の扱いが可能である．\n\n3.3.1 負の二項分布によるモデリング\nカウントデータの基本は Poisson 分布であろうが，過分散を考慮するために負の二項分布でモデリングすることもできる．(17.2節 Gelman et al., 2014) なども参照．\n負の二項分布は例えばマーケティングにおいて，顧客の購買回数をモデル化する際に用いられる (森岡毅，今西聖貴, 2016)．\nこの行為は，Poisson 分布の Gamma 分布による混合分布族を用いた，混合モデリングを行っているとみなせる：\n\n\n\n\n\n\n命題\n\n\n\nPoisson 分布 \\(\\mathrm{Pois}(\\theta)\\) の \\(\\mathrm{Gamma}(\\alpha,\\nu)\\)-混合は負の二項分布 \\(\\mathrm{NB}\\left(\\nu,\\frac{\\alpha}{\\alpha+1}\\right)\\) になる．\nただし，負の二項分布 \\(\\mathrm{NB}(\\nu,p)\\) は，次の確率質量関数 \\(p(x;\\nu,p)\\) が定める \\(\\mathbb{N}\\) 上の確率分布である： \\[\np(x;\\nu,p)=\\begin{pmatrix}x+\\nu-1\\\\x\\end{pmatrix}p^\\nu(1-p)^x.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n確率分布の変換則より，次のように計算できる：\n\\[\\begin{align*}\n  p(x)&=\\int_{\\mathbb{R}_+}\\frac{\\theta^x}{x!}e^{-\\theta}\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu\\theta^{\\nu-1}e^{-\\alpha\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\int_{\\mathbb{R}_+}\\theta^{x+\\nu-1}e^{-(\\alpha+1)\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\frac{\\Gamma(x+\\nu)}{(\\alpha+1)^{x+\\nu}}\\\\\n  &=\\begin{pmatrix}\\nu+x-1\\\\x\\end{pmatrix}\\left(\\frac{1}{\\alpha+1}\\right)^x\\left(\\frac{\\alpha}{\\alpha+1}\\right)^\\nu.\n\\end{align*}\\]\nこの最右辺は，たしかに負の二項分布の質量関数である．\nこの証明方法と，Gamma 分布については次の記事を参照：\n\n  \n    \n      \n      \n        確率測度の変換則\n        Gamma 分布とBeta 分布を例に\n      \n    \n  \n\n\n\n\nこれは \\[\ny_{it}\\sim\\mathrm{Pois}(\\theta)\n\\] \\[\n\\theta\\sim\\mathrm{Gamma}(\\alpha,\\nu)\n\\] という Gamma 分布を仮定した経験ベイズモデル（第 3.2.2 節）に当たる．\nGamma 分布は Poisson 分布の共役事前分布であるため計算が容易であり，早くから質病地図の作成などにも用いられていた (Clayton and Kaldor, 1987), (丹後俊郎, 1988)．\n\n\n3.3.2 Poisson-対数正規混合によるモデリング\nPoisson 回帰\n\\[\n\\begin{align*} y_{it} & \\sim \\operatorname{Pois}(\\lambda_{s[i]}) \\\\ \\log(\\lambda_{s[i]}) & = \\alpha_i + \\eta_{it} \\\\ \\eta_{it} & \\sim \\operatorname{N}(0, \\sigma). \\end{align*}\n\\]\nを考えると，各 \\(y_{it}\\) を，（グループ毎に条件付ければ）Poisson 分布の対数正規分布による混合分布を用いてモデル化していることにあたる．\nこの，Poisson-対数正規分布族は，(Bulmer, 1974) により生物種の個体数分布のモデリングで，過分散を説明するために用いられている．\nすなわち，第 1.1 節のモデルの比較 1.5 で扱った，観測レベルランダム効果 (OLRE: Observation-level Random Effects) の方法は，観測毎に \\(\\eta_{it}\\) というランダム切片項を追加するだけで，本質的には Poisson-対数正規混合モデリングを実施する という，いわばハックのような使い方である．16\n今回はモデル比較の結果が良かったため，本格的に対数正規混合を実施してみるのも良いかもしれない．\n\n\n\n3.4 変量係数モデルによる非線型モデリング\n\n\n\n\n\n\n混合モデルの種々の拡張\n\n\n\n前節 3.3 では，カウントデータに適用するための一般化線型混合モデルをみた．\n(久保川達也, 2006) では，ここまで考慮した１元配置混合線型モデルの拡張をいくつか紹介している：\n\n各グループ \\(s\\in[S]\\) の中でもいくつかのクラスターに分けられる場合，２元配置混合モデル が考えられる： \\[\ny_{ijk}=x_{ijk}^\\top\\beta+v_i+u_{ij}+e_{ijk}.\n\\]\n誤差分散が一定であるという仮定が怪しい場合，変動分散モデル が考えられる．これは，グループ内の分散を \\(e_{ij}|\\sigma_i^2\\sim\\mathrm{N}(0,\\sigma_i^2)\\) とし，\\(\\sigma_i\\) をグループ内で同一の分布に従う i.i.d. と仮定した階層モデルをいう．\n係数 \\(\\beta\\) にもモデルを仮定した階層モデルは 変量係数モデル ともいう： \\[\n\\beta_i=W_i\\alpha+v_i.\n\\] 州ごとの，収入因子が投票行動に与える影響の差を突き止めた (Gelman, 2014) ではこの変量係数モデルを用いている．\n\n\n\n\n3.4.1 例：投票行動の州ごとの違い\n(Gelman, 2014) では州ごとの投票行動の違いを説明するために，まずは次のロジスティック混合モデルを考えている： \\[\n\\operatorname{Pr}(y_i=1)=\\operatorname{logit}^{-1}(\\alpha_{s[i]}+x_i\\beta)\n\\] \\[\n\\alpha_s=W_s^\\top\\gamma+\\epsilon_s,\\qquad\\epsilon_s\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\alpha).\n\\]\n\n\n\n\n\n\n各変数の説明\n\n\n\n\n\n\n\\(y_i\\in\\{0,1\\}\\) は共和党に投票したか，民主党に投票したかを表す２値変数．\n\\(x_i\\in\\{\\pm2,\\pm1,0\\}\\) は収入のレベルを５段階で表す離散変数．\n\\(W_j\\) は各州の共変量のベクトル．\n\n\n\n\nしかしこのままではモデルの当てはまりが良くなかった．これは州ごとに収入が投票に与える影響が異なるためであった．これを考慮するために，(Gelman, 2014) は変量係数モデルを用いた．\n\n\n3.4.2 混合モデルの変量係数モデル化\n\\(\\beta\\) を州ごとに変化させ，これに \\[\n\\beta_s=W_s^\\top\\gamma'+\\epsilon'_s,\\qquad \\epsilon'_s\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\beta),\n\\] というモデルをおく．\nこれにより，州ごとに変化する収入-投票関係をモデリングできる．\n\n\n3.4.3 非線型モデル化\nこれに加えて，\\(\\beta_s\\) を収入カテゴリのアイテム \\(x_i\\in\\{\\pm2,\\pm1,0\\}\\) ごとに変化させることも考えられる．\nこれは値も持つダミー変数 \\[\n\\boldsymbol{x}_i^j=(j-3)1_{\\left\\{x_i=j\\right\\}},\\qquad j\\in\\{1,2,3,4,5\\},\n\\] を成分にもつ \\(\\boldsymbol{x}_i\\in\\mathbb{Z}^5\\) を用いて， \\[\n\\operatorname{Pr}(y_i=1)=\\operatorname{logit}^{-1}(\\alpha_{s[i]}+\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}_{s[i]})\n\\tag{5}\\] というモデルを考えることにあたる．\nこの小さな変更により，非線型な関係もモデリングできるようになる．\n\n\n3.4.4 多重共線型性の霧消\nこのようなトリックが可能な理由は，ベイズ回帰においては多重線型性が問題にならないためである．\nモデル (5) では，３通りで収入が説明変数に入っている：\n\n各収入カテゴリのダミー変数 \\(1_{\\left\\{x_i=j\\right\\}}\\) として\n収入カテゴリの値 \\(\\boldsymbol{x}_i^j\\) として．\n州ごとの収入として \\(W_s\\) にも入っている．\n\nこのことに気づけただろうか？\n頻度論的に回帰分析を実行していたならば，このような多重共線性は問題になっていただろうが，階層ベイズモデリングにおいては有用なトリックとして積極的に活用することができる．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brmsの実装",
    "href": "posts/2024/Computation/brms.html#brmsの実装",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "4 brmsの実装",
    "text": "4 brmsの実装\nbrm 関数（コードは こちら）の実装を調べる．\n\n\n\n\n\n\n\nbrms\n\nStan コードを扱っている関数は .stancode() であった．最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\n\n.standata\n\n\n\n\n\n4.1 事前分布\nbrm 関数 では，デフォルトでは無情報事前分布が用いられる．\n\nDefault priors are chosen to be non or very weakly informative so that their influence on the results will be negligible and you usually don’t have to worry about them. However, after getting more familiar with Bayesian statistics, I recommend you to start thinking about reasonable informative priors for your model parameters: Nearly always, there is at least some prior information available that can be used to improve your inference.brm(): Fit Bayesian Generalized (Non-)Linear Multivariate Multilevel Models\n\n具体的には，ユニットレベルの回帰係数（クラス b）には一様分布が置かれる．\n\n\n4.2 回帰式\nbrm() 関数の第一引数 formula は，validate_formula 関数に渡される．\nこの関数は S3 のメソッドのディスパッチを用いて実装されており，brmsformula オブジェクトに対しては，validate_formula.brmsformula 関数が呼び出される．\nここでは autocor 引数が引かれている場合，出力の formula 属性に追加される：17\n\nfit3$formula\n\ncount ~ zAge + zBase * Trt + (1 | patient) \nautocor ~ unstr(time = visit, gr = patient)\n\n\nなお，brmsformula オブジェクトのコンストラクタは brmsformula() 関数 である．これは，R の formula オブジェクトを通じて，階層モデルを定義できるようになっている（実装はリスト）．\n\n\n4.3 共分散構造\n共分散構造は２つの観点から，brmsformula オブジェクトから自動的に指定される．\n１つ目がグルーピング構造（共分散行列のブロック構造）であり，これはgr関数 が使用される．\n２つ目がグループ内の相関構造であり，これは brm() 関数の autocor 引数を用いる．\n\n4.3.1 gr 関数\nこの関数は brm 関数の第一引数として与えられたモデル定義式から，暗黙のうちに内部で呼び出される．\n例えば，回帰式に (1|patient) が含まれていた場合， gr(patient) が呼び出される．\n共分散構造におく仮定について，重要なデフォルト設定が２つある：\n\n\n\n\n\n\n\nグループ間の相関構造は想定されている：cor=True．\n\nIf TRUE (the default), group-level terms will be modelled as correlated.gr(): Set up basic grouping terms in brms\n\n一方で，グループ内の相関構造は想定されておらず，独立とされている．具体的に指定したい場合は引数 cov を用いる．\n\nBy default, levels of the same grouping factor are modeled as independent of each other.gr(): Set up basic grouping terms in brms\n\n\nすなわち，\\(\\mathrm{V}[\\eta_s]\\) には一切仮定が置かれておらず（第 3.1 節），一方で \\(\\{\\epsilon_{it}\\}_{t=1}^T\\) は互いに独立とされている．\n\n\n\nまた，この二階層目の分布族（第 2.1 節での \\(\\alpha_i\\) と \\(\\eta_{it}\\)）は，分散共分散行列 \\(\\mathrm{V}[\\eta_s]\\) を持った正規分布がデフォルトで，現状他の分布族は指定できないでいる．\n\ndist: Name of the distribution of the group-level effects. Currently “gaussian” is the only option.gr(): Set up basic grouping terms in brms\n\n\n\n4.3.2 autocor 引数\nbrm() 関数には，autocor 引数 が用意されている．\ngr() のデフォルト値では独立とされていたグループ内の相関構造を，具体的に指定するのに用いられる．\n\n\n\n\n\n\n\nunstr：一才の仮定を置かない．\nAR：一次の自己相関構造．\n\n\n\n\n\n\n\n4.4 推論エンジン\nbrm 関数 は，Stan による MCMC サンプリングを通じて，事後分布を計算する．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#文献紹介",
    "href": "posts/2024/Computation/brms.html#文献紹介",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "5 文献紹介",
    "text": "5 文献紹介\n\nここでは計量経済学の呼称に従い，固定効果モデルと変量効果モデルと呼んだが，同じモデルを母数モデル (fixed effect model) と変量モデル (random effect model) と呼んだりもする (足立浩平, 2000)．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#acknowledgements",
    "href": "posts/2024/Computation/brms.html#acknowledgements",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\n\nI would like to extend my gratitude to Robert Long, who kindly shared me the knowledge about the covariance structure implicitly defined via brms formula on this Cross Validated post. His insights were instrumental in enhancing this work."
  },
  {
    "objectID": "posts/2024/Computation/brms.html#footnotes",
    "href": "posts/2024/Computation/brms.html#footnotes",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの区別については こちら も参照．↩︎\n通常は時間的に離れている観測は相関が薄いとしても，直近の観測と関連性が高いだろう．↩︎\nStatistical Modeling, Causal Inference, and Social Science における こちらのエントリ も参照．↩︎\nすなわち，ある super population を想定して，その確率分布の従う項と考えており，変量効果 とも呼ばれる．一方で未知母数とみなす場合は 母数効果 ともいう (久保川達也, 2006)．↩︎\n(Hansen, 2022, p. 333) 第12.3節，(Bafumi and Gelman, 2007, p. 3), (Hansen, 2022, p. 604)，(Gardiner et al., 2009, p. 228)．↩︎\n(Bafumi and Gelman, 2007, p. 5) や (久保川達也, 2006, p. 141), (Gelman et al., 2020) も参照．(Cunningham, 2021) は pooled OLS と呼んでいる．↩︎\n特にパネルデータの文脈では within estimator ともいう (Cunningham, 2021)．↩︎\n(Bafumi and Gelman, 2007, p. 5)，(Hansen, 2022, p. 609) 17.11節 など．狭義では，fixed effects model は within transformation を行い，グループ間の影響を引いたあとに回帰を実行する……という手続きを指すこともあるが，２つは等価な結果を生む．詳しくは (Cunningham, 2021) なども参照．↩︎\n(Hansen, 2022, p. 624) 17.25節．↩︎\n(Hansen, 2022, p. 624)，(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Hubbard et al., 2010) では両方の名前で呼んでいる．↩︎\n\\(\\mathrm{V}[\\eta_s]\\) はブロック行列の構造を持つためこう呼ばれる．(久保川達也, 2006, p. 141) でも LMM と併記されている．↩︎\n(Laird and Ware, 1982)，(Chung et al., 2013)，(Chung et al., 2015)，Statistical Modeling, Causal Inference, and Social Science ブログ 6/2/2023．↩︎\n逆 Wishart ではないらしい (Chung et al., 2015)．↩︎\nSolomon Kurtz (2021) による解説，RPubs も参照．↩︎\nLine 1363．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#終わりに",
    "href": "posts/2024/Survey/BayesRegression.html#終わりに",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "5 終わりに",
    "text": "5 終わりに\nBMI を LAB と LDL から予測する問題を，線型回帰モデルから始めた．\n交差項を追加することで，LDL が違う群に対して LAB がどう変わるかの層別の違いを見ることができる．\n事後予測分布によるモデルのチェックは残差プロットと同様に，極めて手軽かつ有力なモデル検証の方法である．\nこれにより関数関係の非線型性が疑われたため，被説明変数 BMI に対して対数変換を施して線型回帰をすると，予測性能の改善が見られた．\n事後予測分布のプロットだけでなく，その「よさ」の定量的な指標として交差検証による事後予測スコア elpd (Vehtari et al., 2017) があることを学んだ．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html",
    "title": "ベイズ変数選択",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#はじめに",
    "title": "ベイズ変数選択",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 （復習）ベイズデータ解析の第一歩\nデータの非線型変換も取り入れたベイズ線型重回帰分析は，多くの場合，データを理解するための最初の解析手法として選択される．\nその方法を brms パッケージを用いて実践したのが次の記事である：\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n前稿では BMI を LAB と LDL から予測する問題を，線型回帰モデルから始めた．\n交差項を追加することで，LDL が違う群に対して LAB がどう変わるかの層別の違いを見ることができる．\n事後予測分布によるモデルのチェックは残差プロットと同様に，極めて手軽かつ有力なモデル検証の方法である．\nこれにより関数関係の非線型性が疑われたため，被説明変数 BMI に対して対数変換を施して線型回帰をすると，予測性能の改善が見られた．\n事後予測分布のプロットだけでなく，その「よさ」の定量的な指標として交差検証による事後予測スコア elpd (Vehtari et al., 2017) があることを学んだ．\n\n\n1.2 ベイズから見た変数選択\nこうして予測力を基にモデル選択をする方法は得たわけであるが，純粋にベイズ的な観点から変数選択を行う方法が大きく分けて２つある．\n\n1.2.1 縮小事前分布による方法\n１つ目が「モデルに含まれる変数は少ないはずである」という信念を表現した事前分布を用いる方法である（第 2 節）．\nこれは馬蹄事前分布 (Carvalho et al., 2010)，Laplace 事前分布 / Bayesian Lasso (Park and Casella, 2008) などの global-local shrinkage prior を用いる方法である．\nこの方法は点推定や頻度論的な方法ではほとんど唯一の変数選択の方法であり，正則化またはスパース性 のキーワードの下で盛んに研究されている (Hastie et al., 2015)．\n\n\n1.2.2 ベイズ変数選択\n２つ目が spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) という \\(0\\) にマスを持つ事前分布を用いる方法である： \\[\np(dx)=\\prod_{i=1}^d\\biggr(\\omega_i\\phi_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{1}\\]\nこの方法では当該変数の 事後包含確率 (PIP: Posterior Inclusion Probability) を導出することができる．実際 (1) は混合分布の形をしており，spike \\(\\delta_0\\) と slab \\(\\phi_i\\) のどちらからサンプリングされるかを表す潜在変数 \\(\\gamma_i\\in\\{0,1\\}\\) を導入すれば，\\(\\operatorname{P}[\\gamma=0|\\mathcal{D}]\\) という事後確率こそが変数 \\(x_i\\) がモデルに入る事後確率である．\nPIP を用いることで「当該変数がモデルに含まれるか？」という問題に直接ベイズ的に答えることができる．これを ベイズ変数選択 という 3．\n一方で前述の global-local shrinkage prior でも，post-processing を通じて同様に PIP を近似的に算出することができる (Hahn and Carvalho, 2015)．\n\n\n\n1.3 ベイズモデル平均を見据えて\nこのようにベイズ変数選択 1.2.2 では，変数選択も統計的推論の問題として解く．\nこの方法は最適なレートで縮小する効果を持ち (Castillo et al., 2015)，また予測力にも優れる (Porwal and Raftery, 2022)．\n最終的には，適切に構造と事前分布が設定されたベイズモデルを用いて，ベイズ推論により変数の関連度を自動で判断して結果を出すことが理想である．その意味では全ての変数を（適切に）入れたモデルを用いることが好ましい．1\nベイズ変数選択はこの最終目標に向かうまでの探索的な中途解析と見ることもできる．\n実際，ベイズ変数選択により得た事後包含確率 PIP は，ベイズモデル平均 (BMA: Bayesian Model Averaging) (Hoeting et al., 1999) に用いることができる．\n変数選択・モデル選択を実行し，選ばれた単一のモデルで推論・予測を実行するよりも，尤度が必ずしも最も高いわけではないモデルも捨てずに推論に用いることで精度を上げることができる．\nこれがベイズモデル平均の考え方であり，ベイズの美点をフルに発揮する枠組みであると言える．実際，(Porwal and Raftery, 2022) では線型回帰モデルの変数選択において，３つの適応的 BMA 手法が全てのタスクでベストな予測性能を示したことを報告している．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#変数選択",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#変数選択",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "2 変数選択",
    "text": "2 変数選択\nここでは予測性能を elpd で見ることでモデルを選択するのではなく，ベイズ的な方法を取り入れて変数選択を行う方法をみる．\n\n2.1 多くの説明変数が存在する場合の事前分布\nstan_glm では回帰係数には適切な分散を持った独立な正規分布（\\(g\\)-prior）をデフォルトの事前分布としている．\nbrms では一様事前分布である．\n仮に説明変数が極めて多い場合，このデフォルト事前分布を採用し続けることは適切ではない．\n実際，独立な正規・一様分布に従う説明変数が大量にある場合，これは「ベイズ（事後平均）推定量の分散が大きい」という事前分布を採用していることに含意してしまう．\n仮に \\(\\sigma\\) にも同様の分散の大きい事前分布をおいているのならば辻褄は合うが，そうでないならばベイズ決定係数 \\(R^2\\) にほとんど \\(1\\) 近くの事前分布をおいていることに等価である．\nすなわち過学習されたモデルに強い事前分布をおいていることになる (Gelman et al., 2020, p. 208)．これは我々の信念と食い違うだろう．そもそも弱情報であるべきデフォルト事前分布としては相応しくない．\n\n\n2.2 正則事前分布\nまずは各変数の正規事前分布の分散を十分小さくして，誤差 \\(\\epsilon\\) の分散 \\(\\sigma^2\\) のスケールと同一にすることが考えられる．\nこの際 \\(R^2\\) にはほとんど無情報な事前分布が仮定されるのと同一である．\nさらに，仮に「多くの説明変数のうち，一部しか重要なものはなく，他の大部分はほとんど無関係である」と思っている，あるいは思いたいとする．変数選択を行いたい場合がこれにあたる．\nこの信念を正確に表現する事前分布の一つに馬蹄事前分布 (horseshoe prior) (Carvalho et al., 2010) とその正則化バージョン (Piironen and Vehtari, 2017) がある．\nこれらの分布は \\(R^2\\) 上の事前分布に，\\(0\\) 上にスパイクを生じさせる．シンプルなモデルを選好することになるのである．\nStan においては prior=hs によって指定できる (Gelman et al., 2020, p. 209)．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#ベイズ線型重回帰",
    "href": "posts/2024/Survey/BayesRegression.html#ベイズ線型重回帰",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "2 ベイズ線型重回帰",
    "text": "2 ベイズ線型重回帰\n\n2.1 ベイズ単回帰の実行と視覚化\n\n最終的には \\[\n\\texttt{BMI} = \\beta_0 + \\beta_{\\texttt{LAB}}\\cdot\\mathtt{LAB} + \\beta_{\\texttt{LDL}}\\cdot\\mathtt{LDL} + \\beta_{\\texttt{LAB:LDL}}\\cdot\\mathtt{LAB}\\cdot\\mathtt{LDL} + \\epsilon\n\\] \\[\n\\beta_0\\sim\\mathrm{t}(3;\\mu_0,3.4),\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2),\n\\] \\[\n\\beta_{\\texttt{LAB}},\\beta_{\\texttt{LDL}},\\beta_{\\texttt{LAB:LDL}}\\sim\\mathrm{N}(0,\\infty),\\qquad\\sigma\\sim\\mathrm{t}(3;0,3.4),\n\\] という５つのパラメータを持ったモデルを考えるが，ここではまず１つの説明変数 LAB にのみ注目する．\nなお，分散パラメータに出てくる \\(3.4\\) の数字は，被説明変数 BMI の標本分散である：\n\nsqrt(var(raw_df$BMI))\n\n[1] 3.471758\n\n\n\nlibrary(brms)\nmodel1 &lt;- bf(\n  BMI ~ LAB\n)\nfit1 &lt;- brm(\n  formula = model1,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nlibrary(knitr)\nkable(get_prior(\n  formula = model1,\n  data = raw_df\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nLAB\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 22.7, 3.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 3.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nplot(fit1, variable = c(\"b_Intercept\", \"b_LAB\"))\n\n\n\n\n\n\n\n\n\nsummary(fit1)$fixed\n\n            Estimate Est.Error   l-95% CI   u-95% CI     Rhat Bulk_ESS Tail_ESS\nIntercept 20.7016164 0.4252379 19.8602962 21.5361855 1.000239 10139.53 7329.774\nLAB        0.6106412 0.1044871  0.4069246  0.8194326 1.000457 10303.02 6531.444\n\n\n\\(\\beta_{\\texttt{LAB}}\\) の最頻値＝最尤推定量は \\(0.6\\) である．これは，LAB が \\(1\\) 違う個人の間で BMI の値が約 \\(0.6\\) 違うと解釈できる．\n例えば LAB が \\(3.0\\) の個人の予測される BMI は \\[\n\\mathtt{BMI}\\approx20.7+0.6\\times3.0=22.5\n\\] となる．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\nしかしこの図を見ればわかる通り，LAB は BMI の変動の一部しか説明しておらず，上述ような点推定的な議論にどれほど意味があるかは疑問である．\nベイズ回帰では幅を持って結果を理解できるため，その美点を活かさない理由はない．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nsims &lt;- as.matrix(fit1)\nsims_to_display &lt;- sample(nrow(sims), 100)\nfor (i in sims_to_display) {\n  abline(sims[i, 1], sims[i, 2], col = \"gray\")\n}\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n\n\n\n2.2 モデルのチェック：事後予測分布と残差プロット\nさらにベイズ模型は事後予測分布をプロットし，実際の観測データと比べることで，モデルがデータ生成過程をどれほど反映できているかが瞬時に把握できる：\n\nsynthetic_data &lt;- posterior_predict(fit1, newdata = data.frame(LAB = 3.0), ndraws = 10000)\nhist(synthetic_data, nclass = 100, xlab = \"BMI\", main = \"Predicted BMI for a person with LAB = 3.0\")\n\n\n\n\n\n\n\n\n\np1 &lt;- pp_check(fit1, ndraws = 100)\np1\n\n\n\n\n\n\n\n\n事後予測分布のプロットを見ると，モデルが取り逃がしている構造として，BMI の分布が左右で非対称であることがあることがわかる．回帰直線のプロット 図 1 を見ても，直線の上側の点の方が裾が広く分散している．\n「やせ」と「肥満」は対称ではないのである．\n残差をプロットすることでさらに明らかになる：\n\nres &lt;- residuals(fit1)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 2: 残差のプロット\n\n\n\n\n\nただし，残差と事後予測分布には，標本内のデータを見ているか，標本外のデータを想定しているかという大きな違いがある．\nこの２つの結果が乖離している場合，モデルが標本に過適合していることを疑う必要がある．具体的には次節 3 参照．\n\n\n2.3 変数の追加による係数の解釈の変化\nここに新たな変数 LDL を追加すると，LAB の係数 \\(\\beta_{\\texttt{LAB}}\\) は \\(0.6\\) から \\(0.5\\) に減少する．これはどういう意味だろうか？\n\nmodel2 &lt;- update(model1, BMI ~ LAB + LDL)\nfit2 &lt;- brm(\n  formula = model2,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\"))\n\n\n\n\n\n\n\n\n一般に係数の追加は層別に当たる．例えばこの結果は，LDL の値が同じ人の中では LAB が \\(1\\) 違う人の BMI の値が \\(0.5\\) 違うと解釈できる．\n\nsummary(fit2)$fixed\n\n              Estimate   Est.Error      l-95% CI    u-95% CI     Rhat  Bulk_ESS\nIntercept 20.163355017 0.517973058  1.914440e+01 21.17476148 1.000687 12617.051\nLAB        0.483922973 0.124983258  2.390836e-01  0.72708676 1.000109  9080.697\nLDL        0.008582008 0.004412872 -8.612193e-05  0.01716285 1.000207  9537.469\n          Tail_ESS\nIntercept 7619.117\nLAB       7383.503\nLDL       7324.199\n\n\nここで \\(\\beta_{\\texttt{LDL}}\\) の値が極めて小さいことに気づくかもしれない．これは LAB に比べて LDL の影響が小さいことを意味しない．なぜならばこの２つの変数はスケールが約 \\(10^2\\) 違うためである．LDL は 100 のスケール，LAB は 1 のスケールである．\n説明変数 LAB と LDL のどちらが重要か，どっちをモデルに含めるべきかは全く別の方法で議論する必要がある．\n\n\n2.4 データの正規化：係数同士の比較\n係数同士の比較をするためには，説明変数のスケールを揃える必要がある．\nそこでデータを正規化してみる：\n\ndf &lt;- data.frame(\n  sBMI = scale(raw_df$BMI),\n  sLAB = scale(raw_df$LAB),\n  sLDL = scale(raw_df$LDL)\n)\n\nmodel2s &lt;- bf(sBMI ~ sLAB + sLDL)\nfit2s &lt;- brm(\n  formula = model2s,\n  data = df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2s, variable = c(\"b_Intercept\", \"b_sLAB\", \"b_sLDL\"))\n\n\n\n\n\n\n\n\n\\(\\beta_{\\texttt{LDL}}\\) の方が \\(0\\) に近く推定されていることがわかる．\n\nsummary(fit2s)$fixed\n\n              Estimate  Est.Error      l-95% CI   u-95% CI     Rhat  Bulk_ESS\nIntercept 0.0002000558 0.03413500 -0.0678288418 0.06679971 1.000381 10219.138\nsLAB      0.1549410096 0.03976303  0.0768082214 0.23239875 1.000207  9432.452\nsLDL      0.0773993520 0.03996238 -0.0009089375 0.15622134 1.000184  9099.179\n          Tail_ESS\nIntercept 6757.463\nsLAB      7405.609\nsLDL      7524.666\n\n\nデータを正規化してしまったため，直接的な係数の解釈はできないが，係数を相互に比較できる．\n係数の大小を見ることで，LAB の方が有効な説明変数であるように思える．だが元々 LAB は \\(0\\) から離れた値だったが，LDL を入れた途端にいずれも \\(0\\) にかぶりかけている．これは２つの間に共線型性が存在するためである．\n\nplot(df$sLAB, df$sLDL, xlab = \"LAB\", ylab = \"LDL\")\nlm(sLDL ~ sLAB, data = df) %&gt;% abline()\n\n\n\n\n\n\n\n\nまたその他のモデルの性質は変わらない．例えば事後予測分布も変わらない．\n\nlibrary(gridExtra)\np2s &lt;- pp_check(fit2s, ndraws = 100)\np2 &lt;- pp_check(fit2, ndraws = 100)\ngrid.arrange(p2, p2s, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n2.5 交差項の係数の解釈\n再び正規化する前のデータに戻る．\n\nmodel3 &lt;- update(model2, BMI ~ LAB * LDL)\nfit3 &lt;- brm(\n  formula = model3,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit3, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\", \"b_LAB:LDL\"))\n\n\n\n\n\n\n\n\n\nsummary(fit3)$fixed\n\n              Estimate   Est.Error     l-95% CI     u-95% CI     Rhat Bulk_ESS\nIntercept 18.359567486 1.602638690 15.239030706 21.511927719 1.000538 3653.578\nLAB        0.946655585 0.410287745  0.146005654  1.745291095 1.000150 3690.989\nLDL        0.023793589 0.013531229 -0.002859942  0.050046848 1.000415 3761.381\nLAB:LDL   -0.003753507 0.003173122 -0.009944571  0.002469838 1.000165 3555.572\n          Tail_ESS\nIntercept 4433.506\nLAB       4735.780\nLDL       4718.947\nLAB:LDL   4255.115\n\n\n交差項を含む線型回帰における係数の解釈はさらに限定的になる．\n\\(\\beta_{\\texttt{LAB}}\\) は LDL が \\(0\\) である人が仮にいたとした場合の，LAB が \\(1\\) 違う人の間の BMI の平均的な違いを表す，と解釈できる．（LDL の平均が \\(0\\) になるように変数変換をして回帰するともっと自然な解釈ができる）．\n\\(\\beta_{\\texttt{LAB:LDL}}\\) は片方の係数 \\(\\beta_{\\texttt{LAB}}\\) を固定した際，LDL が \\(1\\) だけ違うグループにおける係数 \\(\\beta_{\\texttt{LDL}}\\) との違いを表す．\nすなわち交差項の追加は，LDL に依って層別し，それぞれのグループに異なる \\(\\beta_{\\texttt{LAB}}\\) を推定することを可能にする．この点で階層モデリングに似ている．\n\n\n2.6 交差項の層別効果の視覚化\n交差項 LAB*LDL の追加は，LDL の違うサブグループの間に異なる LAB をフィッティングすることを可能にする．\nこのことを最もよく見るには，LDL が上半分か下半分かで LAB の係数がどう変わるかを見るのが良い．\n\nraw_df$LDLcate2 &lt;- ifelse(raw_df$LDL &gt; median(raw_df$LDL), \"High\", \"Low\")\n\n\nmodel3_cate &lt;- bf(BMI ~ LAB * LDLcate2)\nfit3_cate &lt;- brm(\n  formula = model3_cate,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nb_hat &lt;- summary(fit3_cate)$fixed\nabline(b_hat[1,1], b_hat[2,1], col = \"red\")\nabline(b_hat[1,1] + b_hat[3,1], b_hat[2,1] + b_hat[4,1], col = \"blue\")\nlegend(\"topleft\", # または \"topright\", \"bottomleft\", \"bottomright\" など\n       legend = c(\"High\", \"Low\"),\n       col = c(\"red\", \"blue\"),\n       lty = 1)\n\n\n\n\n\n\n\n\nLDL が大きいと，LAB の BMI に与える影響は緩やかになることがわかる．LDL の方が LAB の代わりに BMI の増加を説明してしまっているとも考えられる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#sec-model-validation",
    "href": "posts/2024/Survey/BayesRegression.html#sec-model-validation",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "3 モデル検証",
    "text": "3 モデル検証\n\n3.1 はじめに\n残差プロットや事後予測プロットによるモデルの検証は，解析と並行して見てきた．\nここではより詳細に，モデルの予測性能に基づいた検証・比較方法を見る．\n交差検証法によるスコア elpd_loo によるモデル比較が一つ推奨される．\n\n\n3.2 決定係数\n図 1 の回帰直線のプロットと 図 2 の残差プロットを見ると，残差がまだ構造を持っていることがわかる．\n\nres &lt;- residuals(fit3)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 3: fit3 の残差のプロット\n\n\n\n\n\nこの残差は標本分散 \\(\\widehat{\\sigma}^2\\)\n\nsigma &lt;- sqrt(var(res)[1,1])\nprint(sigma)\n\n[1] 3.394462\n\n\nを持っている．\nひとまず LAB と LDL について回帰をすることで，データの変動がどれほど説明できたかを考えてみよう．\n\\[\nR^2:=1-\\frac{\\widehat{\\sigma}^2}{s_y^2}=\\frac{s_y^2-\\widehat{\\sigma}^2}{s_y^2}\n\\]\nという値は 決定係数 と呼ばれ，データ \\(y\\) の分散 \\(s_y^2\\) のうち「説明された分散」の割合を表す．1\n\n1-sigma^2/var(raw_df$BMI)\n\n[1] 0.04403279\n\n\nデータの変動の \\(4\\%\\) しか説明できていないことがわかる．\n\n\n3.3 ベイズ決定係数\nベイズ決定係数 (Andrew Gelman and Vehtari, 2019) は brms パッケージで次のように計算できる：\n\nbayes_R2(fit3)\n\n     Estimate  Est.Error       Q2.5      Q97.5\nR2 0.04734955 0.01373743 0.02254274 0.07643401\n\n\n以上の \\(R^2\\) の議論では係数を点推定して「残差」を議論していたが，モデルのパラメータ（の関数）である以上，ベイズ推定することもできる．\nベイズ決定係数（の事後予測値）は，事後予測分布からのサンプルを用いて複数回予測値 \\(\\widehat{y}_i\\) を計算し， \\[\nR^2_{\\texttt{Bayes}}:=\\frac{\\mathrm{V}[\\widehat{y}]}{\\mathrm{V}[\\widehat{y}]+\\sigma^2}\n\\] という値で「データの変動のうち説明された割合」を表す．\n\n\n3.4 AIC\n(Akaike, 1974) は次のように定義される： \\[\n\\mathtt{AIC}=-2\\biggr(\\sup_\\theta\\log p(y|\\theta)\\biggl)+2p.\n\\]\n第１項は deviance とも呼ばれ，残差を表す．\nAIC は新たなデータ点が観測された際の，そのデータ点に対するデビアンス（ある種の損失）の推定量となっており，小さいほどよい．\nAIC と同様の推定を，計算機集約的に行う方法に次節の交差検証法がある：\n\n\n3.5 交差検証と elpd\n事後予測検証では事後分布と観測を比較したが，よりこの好ましくは新しい（推定に用いていない）データと突き合わせることである．\nLOO (Leave-One-Out) 交差検証 (Stone, 1974) では，データを１つだけ抜いてモデルを推定し，このモデルの予測値と実際の値を比較するモデル検証法である．\nbrms パッケージでは loo パッケージ を内部で利用して高速に計算することができる．\n\nloo(fit3)\n\n\nComputed from 10000 by 839 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -2220.7 26.7\np_loo         5.6  0.7\nlooic      4441.4 53.5\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n回帰分析において予測値と実際の（省いていた）データの乖離は，AIC を踏襲して事後予測分布のスコア関数で測る (Vehtari et al., 2017, p. 1414)．\nelpd (expected log predictive density) は，LOO 交差検証により得る，（省いていた）データの対数尤度の平均である： \\[\n\\mathrm{elpd}_{\\text{loo}}:=\\sum_{i=1}^n\\log p(y_i|y_{-i})=\\sum_{i=1}^n\\int p(y_i|\\theta)p(\\theta|y_{-i})\\,d\\theta.\n\\tag{1}\\]\nこの値が大きいほどモデルの予測が良い．一般に elpd は，一度見たことあるデータ点に対する事後予測スコアよりも低くなる．2 この際の差は p_loo が測っており，乖離が大きすぎるとモデルがデータに過適合していることを表す．\np_loo は有効パラメータ数の（一致）推定量である．今回のモデルには切片項と LAB, LDL, LAB:LDL そして sigma の５つのパラメータがあるが，それより \\(0.6\\) だけ大きい値が出ている．\n最後の列は情報量規準のスケールにしたものである： \\[\n\\mathtt{looic}=-2\\times\\mathrm{elpd}_{\\text{loo}}.\n\\]\n一般に LOO-CV は計算が大変であるが，loo パッケージは Pareto Smoothed Importance Sampling (PSIS) (Vehtari et al., 2024) を用いて高速に計算している．\n\\(k&gt;0.7\\) の場合はこれがうまくいっていないことを示唆する．この下で \\(\\mathtt{p_loo}&gt;p\\) はモデルの誤特定を示唆する．\n\n\n3.6 事後予測スコアによるモデル比較\nbrms パッケージでは loo_compare 関数で２つのモデルの elpd スコアを比較できる：\n\nloo_compare(loo(fit1), loo(fit2))\n\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit1 -0.7       2.1   \n\n\nelpd_diff の値は標準偏差と比べて大変に小さい．交差検証の観点からは，LDL の追加は BMI の予測の観点から全く違いがないことがわかる．\n\n\n3.7 ベイズワークフロー\nこの elpd を基本としたモデルの比較は極めて強力である．\n線型モデルが適切な場合，適切な説明変数を新たに作ったり，不要な説明変数を除去して推定を安定化させることで，最適な予測力を持つ線型モデルが特定できる．\n(Gelman et al., 2020, p. 206) などの解析例も参照．\nその際に elpd は，事後予測分布のプロットを見るという視覚的な方法よりも定量的な指標として大活躍することになる．\nしかし時には線型性の仮定が不適切であるという仮説・結論に行き着く場合もある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#変数選択",
    "href": "posts/2024/Survey/BayesRegression.html#変数選択",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "5 変数選択",
    "text": "5 変数選択\n\n5.1 現状確認\nBMI を LAB と LDL から予測する問題を，線型回帰モデルから始めた．\n交差項を追加することで，LDL が違う群に対して LAB がどう変わるかの層別の違いを見ることができる．\n事後予測分布によるモデルのチェックは残差プロットと同様に，極めて手軽かつ有力なモデル検証の方法である．\nこれにより関数関係の非線型性が疑われたため，被説明変数 BMI に対して対数変換を施して線型回帰をすると，予測性能の改善が見られた．\n事後予測分布のプロットだけでなく，その「よさ」の定量的な指標として交差検証による事後予測スコア elpd (Vehtari et al., 2017) があることを学んだ．\n\n\n5.2 変数選択の技術\nここでは予測性能を elpd で見ることでモデルを選択するのではなく，ベイズ的な方法を取り入れて変数選択を行う方法をみる．\n\n\n5.3 多くの説明変数が存在する場合の事前分布\nstan_glm では回帰係数には適切な分散を持った独立な正規分布（\\(g\\)-prior）をデフォルトの事前分布としている．\nbrms では一様事前分布である．\n仮に説明変数が極めて多い場合，このデフォルト事前分布を採用し続けることは適切ではない．\n実際，独立な正規・一様分布に従う説明変数が大量にある場合，これは「ベイズ（事後平均）推定量の分散が大きい」という事前分布を採用していることに含意してしまう．\n仮に \\(\\sigma\\) にも同様の分散の大きい事前分布をおいているのならば辻褄は合うが，そうでないならばベイズ決定係数 \\(R^2\\) にほとんど \\(1\\) 近くの事前分布をおいていることに等価である．\nすなわち過学習されたモデルに強い事前分布をおいていることになる (Gelman et al., 2020, p. 208)．これは我々の信念と食い違うだろう．そもそも弱情報であるべきデフォルト事前分布としては相応しくない．\n\n\n5.4 正則事前分布\nまずは各変数の正規事前分布の分散を十分小さくして，誤差 \\(\\epsilon\\) の分散 \\(\\sigma^2\\) のスケールと同一にすることが考えられる．\nこの際 \\(R^2\\) にはほとんど無情報な事前分布が仮定されるのと同一である．\nさらに，仮に「多くの説明変数のうち，一部しか重要なものはなく，他の大部分はほとんど無関係である」と思っている，あるいは思いたいとする．変数選択を行いたい場合がこれにあたる．\nこの信念を正確に表現する事前分布の一つに馬蹄事前分布 (horseshoe prior) (Carvalho et al., 2010) とその正則化バージョン (Piironen and Vehtari, 2017) がある．\nこれらの分布は \\(R^2\\) 上の事前分布に，\\(0\\) 上にスパイクを生じさせる．シンプルなモデルを選好することになるのである．\nStan においては prior=hs によって指定できる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#おわりに",
    "href": "posts/2024/Survey/BayesRegression.html#おわりに",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "5 おわりに",
    "text": "5 おわりに\nBMI を LAB と LDL から予測する問題を，線型回帰モデルから始めた．\n交差項を追加することで，LDL が違う群に対して LAB がどう変わるかの層別の違いを見ることができる．\n事後予測分布によるモデルのチェックは残差プロットと同様に，極めて手軽かつ有力なモデル検証の方法である．\nこれにより関数関係の非線型性が疑われたため，被説明変数 BMI に対して対数変換を施して線型回帰をすると，予測性能の改善が見られた．\n事後予測分布のプロットだけでなく，その「よさ」の定量的な指標として交差検証による事後予測スコア elpd (Vehtari et al., 2017) があることを学んだ．\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#footnotes",
    "href": "posts/2024/Survey/BDA3.html#footnotes",
    "title": "ベイズデータ解析７",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(11.5 節 Gelman and Hill, 2006, p. 246) も参照．↩︎\n一般に階層モデルは，全ての説明変数を組み込んだモデルから，特定の係数の部分集合を括り出し，その部分集合に super-population model を仮定したものと見れる．(13.6 節 Gelman and Hill, 2006, p. 296) の議論も参照．↩︎\n例えば小地域推定においては極めて自然なモデルである (久保川達也, 2006), (Sugasawa and Kubokawa, 2023) も参照．↩︎\nこの点については (11.6 節 Gelman and Hill, 2006, p. 248)，(Carlin et al., 2001) などを参照．↩︎\nこの第二階層の構造を \\(\\alpha_j\\sim\\mathrm{N}(\\mu_\\alpha,\\sigma^2_\\alpha)\\) という事前分布とみて，この未知パラメータの推定から始める点推定法を経験ベイズという (Sugasawa and Kubokawa, 2023, pp. 11–12)．↩︎\n(Sugasawa and Kubokawa, 2023) では，\\(\\beta\\) が \\(j[i]\\in[J]\\) 毎に異なるとき，これを変量係数モデルと呼んでいる．ここでは (Gelman, 2005) などに倣って，全てまとめて変動係数 (varying coefficient) モデルと呼ぶことにする．↩︎\n(Sugasawa and Kubokawa, 2023, p. 10) に GLS 推定量が BLUE であることの証明がある．変量効果推定量については (17.6 Hansen, 2022, pp. 601–603) を参照した．↩︎\n正規性の仮定の下では一般に経験ベイズ推定量と一致することが (Sugasawa and Kubokawa, 2023, pp. 11–12) で示されている．↩︎"
  },
  {
    "objectID": "index.html#hello-im-hiro.",
    "href": "index.html#hello-im-hiro.",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Hello! I’m Hiro.",
    "text": "Hello! I’m Hiro.\nHirofumi Shiba is a Ph.D. student jointly supervised by Kengo Kamatani and Keisuke Yano at the Institute of Statistical Mathematics (ISM), Tokyo, Japan.\n\nMy research focuses around Monte Carlo algorithms and numerical computation. By making them more efficient, scalable, and automated, I aim to broaden the applications of statistics and machine learning.\nBefore pursuing my Ph.D., I studied Mathematics at the University of Tokyo."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Interests",
    "text": "Interests\n\nMonte Carlo Computation simulation techniques such as MCMC, SMC & PDMP\nBayesian Statistical Modelling especially applied to Political Science & Biostatistics\nBayesian Machine Learning such as Nonparametrics & Kernel Methods"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Research",
    "text": "Research\nMy research interests revolve around developing and analyzing stochastic simulation algorithms, by leveraging insights from their continuous-time limit dynamics. From this perspective, algorithms reveal their intrinsic properties, and my work aims to unify the understanding of various sampling algorithms under a common mathematical framework, i.e., gradient flows on the space of probability measures \\(\\mathcal{P}(X)\\).\nOn the other hand, I contribute to the R package YUIMA, and maintain the Julia package PDMPFlux.jl. Additionally, I engage in Bayesian data analysis in collaboration with healthcare and engineering companies."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Education",
    "text": "Education\n\n Ph.D. in Statistical Science, 2028 (expected)\nGraduate University for Advanced Studies, SOKENDAI, Japan\n\n\n B.Sc. in Mathematics, 2023\nUniversity of Tokyo, Japan"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Experience",
    "text": "Experience\n\n Cooperative Researcher, 2023.4 – present\nRCAST, the University of Tokyo \n\n\n Data Scientist, 2023.4 – present\nPreMedica Inc., Tokyo, Japan"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#ロジスティック回帰",
    "href": "posts/2024/Survey/BayesGLM.html#ロジスティック回帰",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "2 ロジスティック回帰",
    "text": "2 ロジスティック回帰"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#線型階層モデル",
    "href": "posts/2024/Survey/BDA3.html#線型階層モデル",
    "title": "ベイズデータ解析７",
    "section": "2 線型階層モデル",
    "text": "2 線型階層モデル\n\n2.1 導入\n階層モデル \\[\ny_i=\\alpha_{j[i]}+\\beta x_i+\\epsilon_i,\n\\] \\[\n\\alpha_j=\\mu_\\alpha+\\eta_j,\\qquad\\eta_j\\sim\\mathrm{N}(0,\\sigma^2_\\alpha),\n\\] は２つの別の階層にある回帰モデルが，１つに統合された形をしている．\n全く等価だが \\(D_i\\) を \\(i\\) 番目の成分のみが \\(1\\) のベクトル，\\(\\alpha=(\\alpha_1,\\cdots,\\alpha_J)^\\top\\) として \\[\ny_i=D_i\\alpha+\\beta x_i+\\epsilon_i\n\\] と表すことができる．こうみると \\(J\\) 個のクラスそれぞれに関する指示変数で定数項を置き換えた単一の回帰モデルと見ることもできる．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#線型変動係数モデル",
    "href": "posts/2024/Survey/BDA3.html#線型変動係数モデル",
    "title": "ベイズデータ解析７",
    "section": "2 線型変動係数モデル",
    "text": "2 線型変動係数モデル\n\n2.1 変量効果モデル\n階層モデル \\[\ny_i=\\alpha_{j[i]}+\\beta x_i+\\epsilon_i,\\qquad\\epsilon_i\\sim\\mathrm{N}(0,\\sigma^2_y),\n\\tag{1}\\] \\[\n\\alpha_j=\\mu_\\alpha+\\eta_j,\\qquad\\eta_j\\sim\\mathrm{N}(0,\\sigma^2_\\alpha),\n\\tag{2}\\] は２つの別の階層にある回帰モデルが，１つに統合された形をしている．5\n(1), (2) は 変量効果モデル，または NER (Nest Error Regression) model (Battese et al., 1988) と呼ばれる．6\n\n\n2.2 中庸としての階層モデル\n全く等価だが \\(D_i\\) を \\(i\\) 番目の成分のみが \\(1\\) のベクトル，\\(\\alpha=(\\alpha_1,\\cdots,\\alpha_J)^\\top\\) として \\[\ny_i=D_i\\alpha+\\beta x_i+\\epsilon_i\n\\] と表すことができる．\nこうみると \\(J\\) 個のクラスそれぞれに関する指示変数 \\(D_1,\\cdots,D_J\\) で定数項を置き換えた単一の回帰モデルと見ることもできる．仮に複数の変動係数が存在しても共線型性が問題にならないのは，\\(\\alpha\\) の事前分布に構造が入っているためである．\n階層モデル (1), (2) において \\(\\sigma^2_\\alpha\\to0\\) の極限を考えると，グループ毎に変動がないという単一の線型回帰モデルに帰着する．\n他方で実は \\(\\sigma^2_\\alpha\\to\\infty\\) の極限を考えると，古典的な点推定の文脈では，\\(J\\) グループのそれぞれに別々の線型回帰を実行するというモデルに帰着する．\nこれは「グループ毎に全く別々で互いに関係がない」というモデルであり，真実は２つの中庸にあると思われる．\nグループ毎の変動と，その間の緩い関係性の双方を許し，１つのモデルに取り込んだものが 階層モデル である．グループレベルの変動 \\(\\sigma^2_\\alpha\\) を推定するための自然なモデルでもある．\n\n\n2.3 BLUE\n線型最良不偏推定量 (BLUE) は (Henderson, 1950) によって提案された，\\(\\beta,\\alpha_{j[i]}\\) の推定量である．\n\\(\\beta\\) の推定量は計量経済学の文脈で変量効果推定量と呼ばれる GLS 推定量に他ならない．7\n\\(\\alpha_{j[i]}\\) の BLUE は，モデル (1), (2) においては \\[\n\\widehat{\\alpha}_j=\\frac{\\frac{n_j}{\\alpha_y^2}}{\\frac{n_j}{\\alpha_y^2}+\\frac{1}{\\sigma^2_\\alpha}}(\\overline{y}_j-\\beta\\overline{x}_j)+\\frac{\\frac{1}{\\sigma^2_\\alpha}}{\\frac{n_j}{\\alpha_y^2}+\\frac{1}{\\sigma^2_\\alpha}}\\mu_\\alpha\n\\] と表せる．8\n\n\n\n2.4 分散成分モデル\n\n2.4.1 一般的な定義\n(Henderson, 1950) に始まる分散成分モデルは，一般的には \\[\ny_{ij}=\\beta x_i+\\alpha_{j_1[i]}+\\cdots+\\alpha_{j_k[i]}+\\epsilon_{ij}\n\\] \\[\n\\alpha_{j_l}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\tau_l^2),\n\\] と表されるモデルと理解される (Sugasawa and Kubokawa, 2023, p. 73)．\\(k=1\\) の場合は (Fay and Herriot, 1979) モデルともいう．\n変量効果モデルにおいて誤差には完全な階層関係があったが，分散成分モデルはその non-nested な場合への拡張とみなせる．\n\n\n2.4.2 分散成分モデルの例\nこの場合データ \\(y_{ij}\\) の変動を成分ごとに分解しようという志向が，分散分析の発展と見れる．例えば \\[\ny_{ijk}=\\mu+\\mu_{1i}+\\mu_{2j}+\\epsilon_{ijk}\n\\] という形の場合は特に 二元分類モデル，\\(\\mu_{3ij}\\) も加えた場合は二元交差モデルと呼ばれる．\nさらにグループ階層のモデルに関して，分散成分モデルではグループレベル変数 \\(\\alpha_{j_l}\\) は互いに独立であるとしているが，空間モデルでは互いに相関を持つと仮定することもある．\nさらにこの階層は潜在変数の階層とみるとコピュラモデルや理想点モデルに近くなる．項目反応モデルは，個人毎の変動と項目毎の変動の，non-nested な２つの変動を持ったロジスティック階層モデル 3 になる．\n\n\n2.4.3 分散成分の点推定法\n変量効果モデルや分散成分モデルにも適用可能な，一般の階層モデルに適用可能な分散成分の推定法として，一般化推定方程式法がある．\nその特別な荷重を取った場合に REML (Restricted Maximum Likelihood) 推定量がある (2.3 節 Sugasawa and Kubokawa, 2023, p. 13)．\nこうして得た分散成分，特に分散比 \\(\\rho:=\\sigma^2_v/\\sigma^2_e\\) の推定量を得たあと，これを BLUE \\(\\widehat{\\beta}\\) に代入して得る２段階推定量を 経験 BLUE という (久保川達也, 2006, p. 143)．\nその縮小効果については次稿も参照：\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n2.5 ベイズ推定\nベイズ階層モデルの基礎は (Lindley and Smith, 1972) が敷いたと言われる．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#文献紹介",
    "href": "posts/2024/Survey/BDA3.html#文献紹介",
    "title": "ベイズデータ解析７",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n(12 章 Gelman and Hill, 2006) が筆者の知る限り最も丁寧な階層モデルへの導入である．\n変動係数モデルに関して (Sugasawa and Kubokawa, 2023) が極めて見通しが良い．小地域推定を主なテーマとしている．\n項目応答モデルと理想点モデルは (14.3 Gelman and Hill, 2006) において一般化線型階層モデルの文脈で扱われている．\n選択モデルは (27 章 Hansen, 2022) を参照．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#階層ロジスティックモデル",
    "href": "posts/2024/Survey/BDA3.html#階層ロジスティックモデル",
    "title": "ベイズデータ解析７",
    "section": "3 階層ロジスティックモデル",
    "text": "3 階層ロジスティックモデル"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#sec-multilevel-logistic",
    "href": "posts/2024/Survey/BDA3.html#sec-multilevel-logistic",
    "title": "ベイズデータ解析７",
    "section": "3 階層ロジスティックモデル",
    "text": "3 階層ロジスティックモデル\n\n3.1 項目応答モデル\n\n3.1.1 １母数ロジットモデル\n\\[\ng(x):=\\operatorname{logit}(x)=\\log\\frac{x}{1-x}\n\\] \\[\ng(\\mu_{i})=\\alpha_{j[i]}-\\beta_{k[i]},\\qquad\\mu_{i}=\\operatorname{P}[Y_{ik}=1],\n\\] というモデルを １母数応答モデル または (Rasch, 1960) モデルという．\nこのモデルは \\((\\alpha_j,\\beta_k)\\) 上の平行移動に関して識別不可能であるため，点推定の文脈では追加の制約条件が導入される．しかしベイズの文脈では不適な事前分布を避けることで自然に回避される．\n階層ベイズモデルでは，自然に関心が能力パラメータ \\(\\alpha_j\\) と難易度パラメータ \\(\\beta_k\\) の分散に向けられる： \\[\n\\alpha_j=\\mu_\\alpha+\\gamma_\\alpha X^\\alpha_j+\\epsilon_j,\\qquad\\epsilon_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\alpha),\n\\] \\[\n\\beta_k=\\mu_\\beta+X^\\beta_k+\\epsilon_k,\\qquad\\epsilon_k\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\beta).\n\\]\n\n\n3.1.2 ２母数ロジットモデル\nさらに項目毎の 識別力母数 (discrimination parameter) \\(\\gamma_k\\) を導入したモデル \\[\ng(\\mu_i)=\\gamma_{k[i]}\\biggr(\\alpha_{j[i]}-\\beta_{k[i]}\\biggl),\n\\] を ２母数ロジットモデル という．\n\n\n3.1.3 多次元の潜在空間\n読解力と数学力の別々の能力を要求するテストなどのデータに対して，多次元の潜在空間を持つモデルを考えたい．\n両方が要求される AND の論理の場合は \\[\n\\mu_i=g^{-1}\\biggr(\\gamma^{(1)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(1)}_{k[i]})\\biggl)g^{-1}\\biggr(\\gamma^{(2)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(2)}_{k[i]})\\biggl)\n\\] というモデルが考えられるかもしれない．OR の論理の各因数を \\(1-\\mu_i, 1-g^{-1}(-)\\) で置き換えれば良い．\n一方で潜在空間からの関数を設定しても良いだろう．最も直感的には能力値の和で \\[\n\\mu_i=g^{-1}\\biggr(\\gamma^{(1)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(1)}_{k[i]})+\\gamma^{(2)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(2)}_{k[i]})\\biggl)\n\\] と表すものである．\n\n\n3.1.4 応答曲線\nリンク関数 \\(g\\) の選択は，潜在変数 \\(\\alpha_j,\\beta_k,\\gamma_k\\) の変化が応答確率の変化にどう関係するかを規定する．\n\\(g\\) のプロットは ICC (Item Characteristic Curve) または trace line と呼ばれる (Fox, 2010, p. 6)．\nこれは \\(g(0.5)\\) を中心に対称になっているが，この対称性が不適切な場合も多い．\n(Bafumi et al., 2005) などは \\[\n\\mu_i=\\pi_1+(1-\\pi_0-\\pi_1)g^{-1}\\biggr(\\gamma_k(\\alpha_j-\\beta_k)\\biggl)\n\\] として，最低限の \\(y_i=0,1\\) への反応確率 \\(\\pi_0,\\pi_1\\) をあらかじめ設定し，\\([\\pi_0,\\pi_1]\\subset[0,1]\\) の範囲だけに応答曲線をフィッティングすることを考えた．\n\\(\\pi_0,\\pi_1\\) も推定すべきパラメータとする．\n\n\n\n3.2 選択モデル\n\n3.2.1 はじめに\nロジスティックモデルは１次元の潜在空間を持つ 選択モデルと見ることができる．\nこれをさらに精緻化し，価値関数や効用関数などを通じてアクターの意思決定と行動をより詳細にモデリングすることも考え得る．\nこのようなモデルはミクロ計量経済学で歴史的に考えられたほか，政治科学における 理想点モデルはこのような選択モデルを通じて生まれた．\n\n\n3.2.2 ロジットモデルになる場合\n個人 \\(i\\in[N]\\) ごとのパラメータ \\(a_i,b_i,c_i\\) が存在して， \\[\n\\operatorname{P}[Y_i=1]=\\operatorname{P}[a_i&gt;b_i+c_iX_i]=\\operatorname{P}\\left[\\frac{a_i-b_i}{c_i}&gt;X_i\\right]\n\\] によって意思決定が決まるとした場合，\\(d_i:=\\frac{a_i-b_i}{c_i}\\) と潜在変数 \\(x_i\\) との大小によって応答が変わると要約できる．\nすると \\(d_i\\) の分布関数 \\(F_i\\) がロジスティックであるか正規分布であるか \\(t\\)-分布であるかに依って，ロジスティック回帰・プロビット回帰・ロビット回帰というモデルが得られる．\n\n\n3.2.3 Tobit モデル\nさらに \\(y_i\\) の観測値が \\(x_i\\) の値に依存するというモデルが (Tobin, 1958) によって提案されている： \\[\nY=Y^*\\lor0.\n\\] \\[\nY^*_i=\\beta X_i+\\epsilon_i,\n\\]\nこのモデルは（１型）トービットモデルの他に，打ち切り回帰 (censored regression) とも呼ばれる．\nある閾値を超えた場合にプライバシーの問題で公開を制限する top-coding データなどに用いられる．\n\n\n3.2.4 Heckman モデル\n(Heckman, 1979) は標本に入るかどうかのメカニズムをモデリングする，選択バイアスのためのモデル \\[\nY=\\begin{cases}Y^*&S=1\\\\\\text{missing}&S=0\\end{cases}\n\\] \\[\nY^*=\\beta X+\\epsilon,\n\\] \\[\nS=1_{\\mathbb{R}_+}(S^*),\\qquad S^*=\\gamma Z+\\eta.\n\\] を提案した．これは２型トービットモデルとも呼ばれる．\nそしてこの共分散構造 \\[\n\\begin{pmatrix}\\epsilon\\\\\\eta\\end{pmatrix}\\sim\\mathrm{N}\\biggr(0,\\begin{pmatrix}\\sigma^2_\\epsilon&\\sigma_{\\epsilon\\eta}\\\\\\sigma_{\\epsilon\\eta}&1\\end{pmatrix}\\biggl)\n\\] を局外母数として \\(\\beta\\) を一致推定する．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#政治学における理想点解析",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#政治学における理想点解析",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "1 政治学における理想点解析",
    "text": "1 政治学における理想点解析\n\n1.1 はじめに\n理想点推定 は (Poole and Rosenthal, 1991) 以来，政治学において各アクターのイデオロギーを定量化・可視化する方法論として用いられている．\n理想点推定は，政治学における 空間モデリング の重要な一環であり，現代ではイデオロギーの「近さ」を定量化する際の多次元展開法の一種として，理想点推定を位置付けることができる．\n\n\n\n\n\n\n理想点推定の展開\n\n\n\n理想点推定は 態度測定 (飽戸弘, 1966) や 議員尺度化 (legislative scaling) (Jackman, 2001)，または 空間分析 (spatial analysis) (Davis et al., 1970), (岡田謙介 and 加藤淳子, 2016) とも呼ばれる．\n\n\n政治過程とは合意形成の過程である．これを各アクターが政策空間上に選好分布を持つとしてモデリングし，その上でのアクターの行動を分析することで政治・立法・司法過程の理解を試みることを 空間モデリング (Davis et al., 1970), (Enelow and Hinich, 1984) という．\n空間モデリングの政治学的な理論的根拠として 空間（競争）理論 (Downs, 1957) が源流にある．\n政治学における空間理論とは，イデオロギーの「近さ」が影響力を持つとする枠組みであり，はじめは１次元空間上での選挙と投票行動の公理的な分析に用いられた．\n\n\n1.2 空間競争理論 (Downs, 1957)\n空間理論はもともと，ゲーム理論における交渉理論 (bargaining theory) において (Hotelling, 1929) が雑貨店の立地の情報を考慮に入れたことから始まった．\n政治学，特に選挙競争において (Black, 1948) が空間競争理論，特に一次元の政策空間を導入し，公理的な議論を行なった：\n\n\n\n\n\n\n(中位投票者定理 Black, 1948)1\n\n\n\n一次元の政策空間上に投票者が単峰性の選好分布を持つ際，中位政策が Condorcet 勝者となる．\n\n\n(Downs, 1957) は (Black, 1948) が用意した政策空間とゲーム理論を合流させ，選挙競争と投票行動の分析に応用した．\n\n\n\n\n\n\n(Downs, 1957; Hotelling, 1929)2\n\n\n\n１次元の政策空間上の２政党競争において，いくつかの仮定の下で，ナッシュ均衡は両政党が中位政策を採用することである．\n\n\n\n\n1.3 空間競争モデルと理想点\n(Hotelling, 1929)-(Downs, 1957) のアプローチは政治的競争のモデルの出発点となり，政治的競争を人工的な空間上でモデリングする手法が広がった．\n例えば多くの選挙結果を分析する際，政策空間内での中位政策の位置の特定や，実際の政党の政策の中位政策からのズレが重要な意味を持つようになった．\n\n\n\n\n\n\n例：赤い州と青い州の問題\n\n\n\n\n\n米国での投票行動において，個人レベルでの選考と州レベルでの選考とが食い違うという問題が 21 世紀以来有名になっている (Gelman, 2014)．\n端的に言えば，政策的にはリベラルに位置する民主党は貧困層の見方であるが，その主な得票源は富裕層の多い州からのものである．\nこれは中位投票者が中産階級に位置するためであるという見方が一つ説明のつく仮説である (浅古泰史, 2016, p. 78)．\n一方で，ベイズ階層モデリングによる解析によって，各州の投票行動が大きく違うことが判明し，New York や California のように裕福な州では収入が投票行動に全く影響しないこともわかっている (Gelman, 2014)．\n\n\n\nこのように政策空間上にアクターをマッピングし，その上で競争をモデリングする手法は 空間モデル (spatial model) とも呼ばれる．\n特にアクターが政策空間上に持つ選好分布の最頻値を 理想点 (ideal point) という．\nさらには多次元に拡張された理論が多くの経済分析に応用されており，価格などの一次元的な尺度に限らずより一般的な選好を考慮した交渉の議論が可能になっている．3\n\none way to try to account for political choices is to imagine that each chooser occupies a fixed position in a space of one or more dimensions, and to suppose that every choice presented to him is a choice between two or more points in that space. (MacRae, 1958)\n\n\n\n1.4 理想点解析の発展\n現代では空間理論と空間モデルは，投票などの政治過程，そして議会などにおける立法過程の研究に応用される．広く交渉における空間理論については (林光, 2016) も参照．\nさらには純粋にイデオロギーという概念を定量化することにも用いられる．\n古くイデオロギーとは一見バラバラに見える政治的問題の相互の繋がりに関する信念体系である (Converse, 2006)．\n特にリベラル - 中道 - 保守，左 - 右などといった空間的な理解は長らく用いられているものであるが，これは本人が既存のイデオロギーに倣って行動しているというより，よく見られる一貫した行動パターンに名前をつけたものというべきである (Hinich and Pollard, 1981)．\n一貫した行動パターンの分類，その分類がどれほど行動の予測に有用であるか，これらの尺度は統計学の本領というべきである．\n\n\n1.5 点呼投票データ\n理想点解析で最もよく使われるデータとして，各政治家が審議期間にて表明した投票記録，特に 点呼投票 (Roll Call Voting) 記録が用いられる．\n点呼投票データを扱う展開法 (roll-call scaling method) として初めに提案されたものが NOMINATE (nominal, three-step estimation) (Poole and Rosenthal, 1985) であり，次の３段階からなる：\n\n議員の理想点の推定\n法案に対する応答が対応する点の推定\n議員の効用関数のパラメータ推定\n\n\n\n\n\n\n\nNOMINATE の発展\n\n\n\n\nD-NOMINATE (Poole and Rosenthal, 1991)\n点呼投票データの時系列構造も取り込めるようにした拡張．D は dynamic の略である．\nW-NOMINATE (Poole and Rosenthal, 1997)．\nW は weighted の略であり，パソコン上でも動くように設計されたアルゴリズム．現在は R パッケージ wnominate (Poole et al., 2011) で利用可能．\n(Heckman and Snyder, 1997) は同様の手法を因子分析の言葉で定式化している．\nDW-NOMINATE (McCarty et al., 1997)\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n1.6 項目反応モデルとしての理想点解析 (Clinton et al., 2004)\nNOMINATE の方法には政策次元が \\(K=1\\) などの隠匿された仮定があり，これらの仮定を緩めることが必ずしも簡単ではなく，モデル比較の議論となるとほとんど十分な理論的根拠を持たなかった．\n理想点推定を統計モデル，特に 項目反応モデル（第 3 節）とみなし，従来は局外母数とみなされた項目毎の母数も，ベイズの枠組みで同時に推論・モデル比較を行うことが (Jackman, 2000), (Jackman, 2001), (Clinton et al., 2004) によって提案された．4\nここでは (Imai et al., 2016) で「標準的な理想点モデル」とされている BIRT (Bayesian Item Response Theory) (Clinton et al., 2004) の定式化を紹介する．\n\n\n\n\n\n\n標準的な理想点モデル\n\n\n\n\n\\(i\\in[N]\\) 番目の議員が \\(j\\in[J]\\) 番目の法案に対して賛成ならば \\(y_{ij}=1\\)，反対ならば \\(y_{ij}=0\\) のデータが得られているとする．\nこのとき \\(i\\in[N]\\) 番目の議員の理想点 \\(x_i\\in\\mathbb{R}^K\\) は，\\(y_{ij}\\) を次のように予測する潜在変数とする： \\[\\begin{align*}\n  y_{ij}&=1_{\\mathbb{R}^+}(y^*_{ij})\\\\\n  y^*_{ij}&=\\alpha_j+x_i^\\top\\beta_j+\\epsilon_{ij},\\qquad\\epsilon_{ij}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1)\\\\\n  &=:\\widetilde{x}_i^\\top\\widetilde{\\beta}_j+\\epsilon_{ij}.\n\\end{align*}\\]\nすなわち \\(K\\)-母数のロジット項目反応モデル 3.4 において，議員ごとの母数である \\(x_i\\) を 理想点 と呼ぶ．項目識別母数 \\(\\beta_j\\) は法案ごとの性質の違いを表しているものと考える．\n換言すれば，次のプロビットモデルが想定されたことになる： \\[\n\\operatorname{P}[y_{ij}=1]=\\Phi(\\widetilde{x}_i^\\top\\widetilde{\\beta}_j).\n\\]\n\n\nこのモデルは潜在変数 \\(Y^*\\) とパラメータ \\((x_i)_{i=1}^N\\in\\mathbb{R}^{KN},(\\beta_j)_{j=1}^J\\in\\mathbb{R}^{KJ}\\) を持つ．\nプロビット項目反応モデル 3.4 は，項目反応モデルの文脈でデータ拡張に基づく Gibbs サンプリングによるベイズ推定が古くから議論されていた (Albert, 1992)．\n(Patz and Junker, 1999) はロジスティックモデルに対して Metropolis-Hastings within Gibbs アルゴリズムを提案している．5\n\n\n\n\n\n\n注：ランダム効用理論との離別\n\n\n\n\n\nここでは (Clinton et al., 2004) を踏襲した (Imai et al., 2016) の定式化に従った．\n(Clinton et al., 2004) の定式化は ランダム効用理論 (random utility framework) (McFadden, 1976) に従い，議員 \\(i\\in[N]\\) の効用関数を用いていたという点で NOMINATE (Poole and Rosenthal, 1985) を踏襲していた．\nだが，(Jackman, 2001) ではこれを統計モデル（項目反応モデル）として解釈する際に \\(U_i\\) を排している．\n(Clinton et al., 2004) の設定では，理想点 \\(x_i\\in\\mathbb{R}^K\\) は次の効用関数 \\(U_i:\\{\\zeta_j,\\psi_j\\}_{j=1}^J\\to\\mathbb{R}\\) を通じて意思決定に影響するとした： \\[\nU_i(\\zeta_j)=-\\lvert x_i-\\zeta_j\\rvert^2+\\eta_{ij},\n\\] \\[\nU_i(\\psi_j)=-\\lvert x_i-\\psi_j\\rvert^2+\\nu_{ij}.\n\\] ただし，\\(\\eta_{ij},\\nu_{ij}\\) は互いに独立な Gauss 誤差とし，\\(\\eta_{ij}-\\nu_{ij}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma_j^2)\\) とする．\n最終的に議員 \\(i\\in[N]\\) は \\[\ny_{ij}=1_{\\left\\{U_i(\\zeta_j)&gt;U_i(\\psi_j)\\right\\}}\n\\] として投票行動を表現する．\nこの統計モデルは \\[\n\\operatorname{P}[y_{ij}=1]=\\Phi(\\beta_j^\\top x_i-\\alpha_j)\n\\] \\[\n\\beta_j:=\\frac{2(\\zeta_j-\\psi_j)}{\\sigma_j},\\qquad\\alpha_j=\\frac{\\zeta_j^\\top\\zeta_j-\\psi_j^\\top\\psi_j}{\\sigma_j}\n\\] という probit モデルに等価になる．6\nなお誤差 \\(\\eta_{ij},\\nu_{ij}\\) を極値分布に従うとすると logit モデルを得る．logit モデルは NOMINATE (Poole and Rosenthal, 1985) 発表前に計算資源が少なかった時代に用いられていたが，その後は誤差の正規性の仮定が優先された (Poole and Rosenthal, 2001)．\n効用関数 \\(U\\) は NOMINATE では Gauss 密度，(Heckman and Snyder, 1997) と (Clinton et al., 2004) では二次関数を用いていた．\n(Clinton et al., 2004) では \\(x_i,\\widetilde{\\beta}_j\\) に独立な共役事前分布 \\[\np(x_1,\\cdots,x_N)=\\prod_{i=1}^N\\phi_K(x_i;\\mu_x,\\Sigma_x)\n\\] \\[\np(\\widetilde{\\beta}_1,\\cdots,\\widetilde{\\beta}_J)=\\prod_{j=1}^J\\phi_{K+1}(\\widetilde{\\beta}_j;\\mu_{\\widetilde{\\beta}},\\Sigma_{\\widetilde{\\beta}})\n\\] を仮定した．\\(\\phi_d\\) は \\(d\\) 次元の Gauss 密度である．\n(Clinton et al., 2004) ではこのモデルの２パラメータの項目反応モデルとの対応に基づいて，データ拡大に基づく Gibbs サンプラーによる推定が WinBUGS (Lunn et al., 2000) によりなされ，R パッケージに実装され，現在も pscl (Political Science Computational Laboratory) パッケージ (Zeileis et al., 2008) に実装されている．\n\n\n\n\n\n\n\n\n\n単一指標モデルとの関係\n\n\n\n\n\n被説明変数が \\(y_{ij}\\in\\{0,1\\}=2\\) であるこのモデルは，計量経済学では 二項選択モデル (binary choice model) として知られている (Chapter 25 Hansen, 2022, p. 801)．\n計量経済学ではプロビット，ロジットモデルの他に，リンク関数 \\(G\\) の関数系を局外母数としたセミパラメトリックモデルである 単一指標モデル (single-index model) \\[\n\\operatorname{P}[y_{ij}=1]=G(x^\\top_i\\beta_j)+\\epsilon_{ij}\n\\] が考えられる．\n特に \\[\ny_{ij}=1_{\\mathbb{R}^+}\\biggr(\\widetilde{x}_i^\\top\\widetilde{\\beta}_j+\\epsilon_{ij}\\biggl)\n\\] という（潜在変数）モデルでは，\\(\\epsilon_{ij}\\) の分布関数を \\(F\\) とすると， \\[\nY_{ij}\\sim\\mathrm{Ber}(F(X^\\top_i\\beta_j))\n\\] \\[\n\\operatorname{P}[y_{ij}=1]=F(x^\\top_i\\beta_j)\n\\] というモデルと等価になり，単一指標モデルに一致する (Section 25.4 Hansen, 2022, p. 804)．\nこの設定で \\(F\\) を未知のままでも \\(\\beta_j\\) に関してセミパラメトリック推定ができる (Klein and Spady, 1993)．\nただし，理想点推定の場合のように \\(X\\) に定数項があると識別可能性が失われるため，追加の制約が必要である．また \\(\\beta\\) も定数倍を除いて識別される．\n\n\n\n\n\n1.7 ベイズ計算の問題\n理想点推定にベイズモデルを立てて MCMC により推定する方法は動的なモデル (Clinton and Meirowitz, 2001), (Martin and Quinn, 2002)，戦略的投票 (Clinton and Meirowitz, 2017), 階層モデリング (Bafumi et al., 2005) へ拡張され，主流の方法となった．\nしかし (Martin and Quinn, 2002) では 47 年の米国最高裁データの分析に５日間かかっている．特に pscl (Zeileis et al., 2008) による Gibbs サンプリングがデータの不均衡性によって収束に苦しんでいる可能性がある．\nそこでベイズの方法で理想点解析をやりたいが，理想点推定はモデルが大規模になるために効率的な計算手法が必要となっている．\n\n\n1.8 変分 EM アルゴリズム\n(Imai et al., 2016) は高速なベイズ推論のために変分 EM アルゴリズムを提案し，emIRT パッケージに実装している．\n種々のタイプのモデル（多値反応モデル，動的モデル，階層モデル，テキストデータ）を考察しているので，種々の理想点解析モデルのレビューとしても有用である．\nその共通するアプローチは \\(Y^*\\) を欠測データと扱い，\\(\\widetilde{x}_i,\\widetilde{\\beta}_j\\) を同時に EM アルゴリズムにより推定し，特定の基準に基づいてアルゴリズムを停止することである．その途中で変分近似を用いる．\nベイズ的な不確実性の可視化を得るために NOMINATE のようにパラメトリックブートストラップ (Carroll et al., 2009), (Lewis and Poole, 2004) を行う．\n(Imai et al., 2016) の変分 EM アルゴリズムにより \\(d=1\\) 次元空間上の理想点を推定した結果が (三輪洋文, 2017) で公開されている：\n\n\n\nTwitter データとプロビットモデルによる理想点推定 (三輪洋文, 2017, p. 51)\n\n\n\n\n1.9 その他のデータ源の探索\n点呼投票データには，政党規律や 票取引 (logrolling) などの戦略的投票行動がある際には，必ずしも個人の政治的信条を反映しないという欠点がある．\nそこで点呼投票データの他に有用なデータ源の探索とそれを用いた理想点推定の方法が模索されており，データ統合が最終的な目標として目指されている．\n特に日本では政党規律が強く，点呼投票データが適さないため，政治家へのサーベイや質問，専門家調査 (加藤淳子, 2021) によってデータが収集されることが多いという (三輪洋文, 2017), (Miwa and Taniguchi, 2017)．\nこのテキストベースのアプローチは，政党が公開しているマニフェストなどの客観的なデータも取り入れることが可能であるという点に美点がある (岡田謙介 and 加藤淳子, 2016)．\nまた近年では，Twitter が政治家の政策と信条の空間的位置について多くの情報を含んでいる情報源として注目されている (Barberá, 2015), (三輪洋文, 2017)．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#多次元展開法としての理想点解析",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#多次元展開法としての理想点解析",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "2 多次元展開法としての理想点解析",
    "text": "2 多次元展開法としての理想点解析\n\n2.1 Twitter データと項目反応モデルを通じた多次元尺度展開\n(Barberá, 2015) は特に Twitter において誰が誰をフォローしているかのデータに注目した．\n\\((y_{ij})\\in M_n(2)\\) を，ユーザー \\(i\\) がユーザー \\(j\\) をフォローしているかを２値で表した \\(0,1\\) 成分行列とし，この関係が政策空間 \\(\\mathbb{R}^d\\) におけるユーザー \\(i,j\\) の距離の近さによって決定されているとする．\n\\(\\theta_i:[n]\\to\\mathbb{R}^d\\) をユーザーの政策空間への埋め込みとすると，\\(g\\) をリンク関数として \\[\ng\\biggr(\\operatorname{P}[Y_{ij}=1\\,|\\,\\alpha_j,\\beta_i,\\theta]\\biggl)=\\alpha_i+\\beta_j-d(\\theta_i,\\theta_j)\n\\] とするのである．\nただし，\\(\\alpha_j\\) は知名度，\\(\\beta_i\\) は政治的関心を表す説明変数とした．\nこれにより Gibbs サンプラーにより \\(\\alpha,\\beta,\\theta\\) の推定が可能になるが，この方法では推定が遅く，また大規模なデータや偏りのあるデータに弱い．\nこの問題点は Zig-Zag サンプラーによって解決され，さらに推定が高速になる．詳しくは次の稿も参照：\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.2 多次元展開法としての展開\n(Bakker and Poole, 2013) は理想点解析を多次元尺度法と見て，ベイズ化の方法を提案している．\n多次元空間への多次元尺度構成法は，非線型次元縮約法，多様体学習法，埋め込み法などといった種々の名前の下で考察されている．\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\nNo matching items\n\n\n逆に言えば，これらの他手法と比較したり，長所と短所を洗い出すことで，個々の手法に対する理解が深まるかもしれない．\n(Escolar et al., 2023) では特許のデータを用い，各企業を技術空間 \\(\\mathbb{R}^{430}\\) 内に埋め込んだ後，mapper (Singh et al., 2007) によりグラフ化したところ，企業の独自戦略が可視化されたという．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#sec-IRT",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#sec-IRT",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "3 項目反応理論",
    "text": "3 項目反応理論\n\n3.1 はじめに\n(Jackman, 2001), (Clinton et al., 2004) でも自覚されているように，理想点解析は多次元尺度構成法であると同時に，点呼投票という２値応答に特化した項目反応理論とも見れる．\n項目反応理論 (IRT: Item Response Theory) は 現代テスト理論 とも呼ばる．\n因子分析に基づいた古典テスト理論とは異なり，特定の項目に被験者がどのように応答するかを左右する種々の潜在変数を柔軟に取り入れることを可能にする モデルベース の枠組みである．7\nその柔軟性のため，コンピュータを通じた適応的なテスト などの現代的な設定における心理測定・行動計量の基礎を支えている．\n\n\n3.2 項目反応理論の歴史\n項目反応理論の初まりは (F. M. Lord et al., 1968) と ETS における実践・セミナーと目されている．\n書籍 (F. M. Lord et al., 1968) はテストに対して真に統計的でモデルベースな扱いを創始したと評されている (Embretson and Reise, 2000)．\nただし，同様の取り扱いはデンマークにて (Rasch, 1960) により早くから用いられており，この２つが IRT の源流とされている (Embretson and Reise, 2000)．\n(Rasch, 1960) のモデルは２値応答の確率を，個人と項目とのそれぞれ１母数の関数としてモデリングする最も単純なものであった．\n長らくこの研究はヨーロッパを出ず，(Fischer, 1973) がこれを拡張し翌年に教科書も書いたが，ドイツ語であったので世界的には広まらなかった．\n最終的に２つの流れが邂逅したのは Benjamin Wright を介してであった．\n1960 年に Rasch が Wright を訪問して以来，Rasch モデルの客観的測定 (objective measurement properties) の重要性を評価し，その推定方法を FORTRAN により実装した (Wright and Panchapakesan, 1969)．\nその後 Wright の下で学んだ多くの学生が (Rasch, 1960) のモデルに関して基礎的な研究を行なった．8\n\n\n3.3 項目反応モデルの応用\n項目反応モデルは個々人レベルの応答変数に基づいて，個人ごとに違う潜在変数 \\(\\theta_i\\) と項目ごとに違うパラメータ \\(\\xi_j\\) の推定を実行する際に広く用いられる．\n\\(\\theta_i\\) は典型的には個々人の「能力」といった概念構成を表すパラメータで 能力母数 (ability parameter) とも呼ばれる (Fox, 2010, p. 6)．一方 \\(\\xi_j\\) は難易度パラメータ (difficulty parameter) ともいう．\n項目反応モデルの用途は主に潜在変数の測定 (measurement) と多次元尺度構成 (scaling) との２つに分けられる．\n理想点解析は後者の用途に属する．これはパラメータ \\(\\theta_i\\) がテストの種類などの測定方法に依存せず，モデルが同一ならば一定した尺度を持つという項目反応モデルの美点に基づく．この普遍性を Rasch は 固有客観性 (specific objectivity) と呼んだ (井澤廣行, 2008, p. 51)．\nまた複数の項目反応モデルの結果の間で尺度を統一することを，特にテスト分析の分野では リンキング または 等化 (equating) という．9\n\n\n3.4 ２値反応の項目反応モデル\n項目反応モデルでは \\(\\theta_i\\) は応答確率を変化させるとする： \\[\n\\operatorname{P}[Y_{ij}=1]=g_j(\\theta_i),\\qquad i\\in[N],j\\in[J].\n\\] このリンク関数 \\(g_j\\) は 項目特性曲線 (ICC: Item Characteristic Curve / Trace Line) と呼ばれる．\n加えて \\(\\theta_i\\) の値で条件付けたとき，異なる項目への応用は互いに独立であると仮定する（局所独立性 という）：10 \\[\n\\operatorname{P}[Y_{i1}=1,\\cdots,Y_{iJ}=1]=\\prod_{j=1}^J\\operatorname{P}[Y_{ij}=1].\n\\]\n\n\n\n\n\n\n(Rasch, 1960) モデル\n\n\n\n１母数応答モデル (1PLM: one-parameter logistic model) または Rasch モデル とは，個人の母数 \\(\\theta_i\\) と項目の母数 \\(b_j\\) とが定めるロジスティックモデル \\[\n\\operatorname{P}[Y_{ij}=1]=\\biggr(1+e^{b_j-\\theta_i}\\biggl)^{-1}\n\\] である．\n\\(b_j-\\theta_i\\) は \\(b_j,\\theta_i\\) 双方の十分統計量であり，\\(\\theta_i\\) のみを条件付き最尤推定可能である．\n\n\n\\(b_j,\\theta_i\\) は同じ空間 \\(\\mathbb{R}\\) 上にプロットでき，同じ尺度を持つことに注意．\\(\\theta_i\\) が \\(b_j\\) からみて左右のどちらにあるかに依って，応答確率が \\(1/2\\) より大きいか小さいかが決まる．\n\n\n\n\n\n\n２母数ロジットモデル\n\n\n\n２母数ロジットモデル (2PLM: two-parameter logistic model) とは，項目 \\(j\\in[J]\\) が２つの母数 \\(a_j,b_j\\) でパラメータ付けられたロジスティックモデル \\[\n\\operatorname{P}[Y_{ij}=1]=\\biggr(1+e^{b_j-a_j\\theta_i}\\biggl)^{-1}\n\\] である．\\(a_j\\) は 項目識別力母数 (item discrimination parameter) ともいう．11\nもはや条件付き最尤推定は不可能であるが，(Bock and Lieberman, 1970) は能力母数を局外母数として項目母数を推定する方法を数値積分法によって与えた．\nEM アルゴリズムによる周辺最尤推定法 (Bock and Aitkin, 1981) は能力母数を局外母数と扱う教育の分野において現在でも標準的な方法の１つである．\n\n\nプロビットモデルも \\(n\\)-PNM (\\(n\\)-Parameter Normal ogive Model) (F. M. Lord et al., 1968, pp. 365–384) として古くから考えられていたが，Gibbs サンプリングの都合上ロジスティックモデルが好まれた．\nロジスティックモデルで推定された空間上で \\(d=1.7\\) のスケーリングの違いを除いて [-3,3] 上ではほとんど一致することが知られている (Hambleton, 1991, p. 15)．\n\n\n3.5 多値項目反応モデル\n正解・誤答の２値以外にも，部分点があるなどの多値項目 (polytomous item) に対する拡張が考えられている．\n\n\n\n\n\n\n(部分得点モデル Masters, 1982)\n\n\n\n部分得点モデル (PCM: Partical Credit Model) (Masters, 1982) とは，項目 \\(j\\in[J]\\) の応答がカテゴリ \\(c\\in[C_j]\\) に当たる確率を \\[\n\\operatorname{P}[Y_{ij}=c]=\\frac{e^{\\sum_{l=1}^c(\\theta_i-\\kappa_{kl})}}{\\sum_{r=1}^{C_j}e^{\\sum_{l=1}^r(\\theta_i-\\kappa_{kl})}}\n\\] で与える．\n\\(\\kappa_{kl}\\) は項目 \\(j\\in[J]\\) の step 難易度パラメータという．\n\n\n(Muraki, 1992) はこれを一般化し，EM アルゴリズムによる推定方法を与えている．\n\n\n\n\n\n\n(段階反応モデル Samejima, 1997)\n\n\n\n段階反応モデル (GRM: Graded Response Model) (Samejima, 1997) では，項目 \\(j\\in[J]\\) の応答確率の分布関数をモデリングし，カテゴリ \\(c\\in[C_j]\\) に当たる確率は \\[\n\\operatorname{P}[Y_{ij}=c]=\\biggr(e^{\\kappa_{j,c-1}-a_j\\theta_i}\\biggl)-\\biggr(e^{\\kappa_{j,c}-a_j\\theta_i}\\biggl)\n\\] で与えられる．\nただし，難易度パラメータには \\[\n-\\infty=\\kappa_{j,0}&lt;\\kappa_{j,1}&lt;\\cdots&lt;\\kappa_{j,C_j}=\\infty\n\\] という順序制約が必要になる．\n\n\n\n\n3.6 多次元の項目反応モデル\n\n\n\n\n\n\n多次元項目反応モデル\n\n\n\n\\(b_j-a_j^\\top\\theta_j\\) という指標を多次元化することで，(Rasch, 1960) のモデルを多次元化することができる： \\[\n\\operatorname{P}[Y_{ij}=1]=\\operatorname{expit}\\biggr(-b_j+a_j^\\top\\theta_i\\biggl).\n\\]\n\n\n空間理論（第 1.2 節）の端緒からして，単なる１次元の左-右といった軸ではなく，多次元の潜在空間上に各政治家の理想点を写像したい，という悲願がある (岡田謙介 and 加藤淳子, 2016)．\nこのように新たな次元も考慮に入れることで，リベラル - 保守といった概念への理解が進むことが期待される上に，予測などの下流タスクの精度の大きな向上も望めるだろう．\n一般に複雑な構成概念の精緻な検証が可能になる (坂本佑太朗 and 柴山直, 2017) ため，多次元項目反応モデルは近年注目されており，これを実現する統計計算法が必要とされている．\n特に識別可能性の問題が深刻になるが，それがベイズのアプローチでは，\\(\\ell_2\\)-ノルムベースであったところを \\(\\ell_1\\)-ノルムベースにすることで，推定の安定性と効率性が向上することなどが考えられている (Lim et al., 2024)．\n\n\n3.7 理想点解析の認知モデリングとしての展開\n従来の理想点解析における参照軸は，純粋に複雑な政治的現象を理解するための構成概念として利用された．\n一方で理想点解析と項目反応理論との類似性に気付いた以上，応答過程に認知科学的変数も取り入れることは自然な拡張の１つとして試みられてきた (Lee, 2001)．\n例えば個々人の認知過程の違い (Embretson (Whitely), 1984) (DIF: Differential Item Functioning) (Frederic M. Lord, 1980, p. 212) や発達段階の違い (Wilson, 1984) も変数に取り入れることが考えられている．\nそこで近年，理想点推定が出力する「次元」に対する人間の空間的認知との関係を明示的に取り入れたモデリングをしようという試みが，行動計量学との接点で考えられている (岡田謙介 and 加藤淳子, 2016)．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#文献紹介",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#文献紹介",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "4 文献紹介",
    "text": "4 文献紹介"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#footnotes",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(浅古泰史, 2016, p. 69) を参考．↩︎\n(浅古泰史, 2016, p. 75) を参考．↩︎\nこの交渉理論におけるコンテクストから，理想点 というのである．各主体が理想とする点，という意味である．↩︎\n“In short, the goal that Bayesian methods make plausible is a transformation of roll call analysis, from a technical scaling or measurement problem best left to psychometricians (witness the canonical status of NOMINATE scores) to something that scholars motivated primarily by substantive concerns can do for themselves.” (Jackman, 2001, p. 240)．↩︎\nさらに詳しくは (Fox, 2010, pp. 71–) も参照．↩︎\nprobit とは (Bliss, 1934) が probability unit から名付けた．↩︎\n例えば消費者の購買行動をモデリングする際は，選択疲れをした消費者は中間的な商品を選びやすいという 妥協効果 (compromise effect) (Simonson, 1989) などの文脈効果もモデルに入れる必要がある (加藤拓巳, 2021)．↩︎\nだが，Bock も Wright の教え子も主に教育学で活躍しており，最終的に心理学者に心理測定の基本として古典テスト理論を IRT が代替したのは 2000 年代になってからだったという (Embretson and Reise, 2000, p. 7)．↩︎\n２つは厳密には，等化は一番強い仮定のもとで行われるリンキングの一つである (宇佐美慧 et al., 2018)．例えば集団の基礎学力が違った場合，同一の困難度を測定するためでも別のバージョンのテストを作成する必要がある．等化は ICC が affine 合同である場合に affine 変換により可能である (宇佐美慧 et al., 2018)．↩︎\nこれが成り立つように，１つの設問で問われる能力は１つになるように設計することが原則である (宇佐美慧 et al., 2018)．↩︎\n３母数ロジットモデルにおいて加わる母数は当て推量母数／下方漸近パラメータとも呼ばれる (宇佐美慧 et al., 2018)．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html",
    "title": "理想点解析のハンズオン",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#前稿",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#前稿",
    "title": "理想点解析のハンズオン",
    "section": "前稿",
    "text": "前稿\n\n\n\n\n\n\n\n\n変動係数階層ロジスティックモデルとしての理想点解析\n\n\nPDMP サンプラーによる大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本稿では次の３つのパッケージを紹介する：\n\n\n\n\n\n\n\npscl (Zeileis et al., 2008)：GitHub, CRAN．(Arnold, 2018) も参照．\nMCMCpack (Martin et al., 2011)：GitHub, CRAN\nemIRT (Imai et al., 2016)：GitHub, CRAN"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#pscl-パッケージ",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#pscl-パッケージ",
    "title": "理想点解析のハンズオン",
    "section": "3 pscl パッケージ",
    "text": "3 pscl パッケージ\ninstall.packages(\"pscl\")\n\n3.1 voteview データ\nこのパッケージでは，Keith T. Poole と Howard Rosenthal が 1995 年から運営しているサイト voteview.com のデータを利用するための関数 readKH() が提供されている．\n例えば連邦議会 (U.S. Congress) 117 議会期 (Congress) 2021.1.3-2023.1.3 の上院 (Senate) の点呼投票データを読み込むには以下のようにする：1\n\nlibrary(pscl)\ns117 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S117_votes.ord\",\n                desc=\"117th U.S. Senate\")\n\ns117 は rollcall オブジェクト，８つのフィールドを持った配列である．\ns117$votes データは \\(n=104\\) 議員の計 \\(m=949\\) 回の投票からなる \\(10\\)-値の行列である．\n\nsummary(s117)\n\n\nSummary of rollcall object s117 \n\nDescription:     117th U.S. Senate \nSource:      https://voteview.com/static/data/out/votes/S117_votes.ord \n\nNumber of Legislators:       104\nNumber of Roll Call Votes:   949\n\n\nUsing the following codes to represent roll call votes:\nYea:         1 2 3 \nNay:         4 5 6 \nAbstentions:     7 8 9 \nNot In Legislature:  0 \n\nParty Composition:\n    D Indep     R \n   50     2    52 \n\nVote Summary:\n               Count Percent\n0 (notInLegis)  3544     3.6\n1 (yea)        55542    56.3\n6 (nay)        35995    36.5\n7 (missing)        5     0.0\n9 (missing)     3610     3.7\n\nUse summary(s117,verbose=TRUE) for more detailed information.\n\n\n\n\n3.2 点呼投票データ\n点呼投票データとは \\(n\\times m\\) の行列で，そのエントリーは２値変数である（今回は \\(1\\) か \\(6\\)）．\nしかし実際には種々の欠測により，\\(0,7,9\\) も使われる．\nこれをヒートマップで可視化してみる．\n\nlibrary(tidyverse)\n\nvotes_df &lt;- as.data.frame(s117$votes[1:15, 1:15]) %&gt;% rownames_to_column(\"Legislator\")  # 投票データをデータフレームに変換し、行名を列として追加\n\nvotes_long &lt;- votes_df %&gt;% pivot_longer(cols = -Legislator, names_to = \"Vote\", values_to = \"value\")  # データを長形式に変換\n\n\nggplot(votes_long, aes(x = Vote, y = Legislator, fill = value)) + geom_tile() + scale_fill_gradient(low = \"white\", high = \"red\") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Votes\", y = \"Legislators\", title = \"Voting Patterns\")  # ヒートマップを作成\n\n\n\n\n\n\n\n\n\n\n3.3 政党毎の賛成率\n政党でソートし，賛成率を最初の 15 法案についてプロットしたものは次の通り：\n\n\nCode\nlibrary(dplyr)\n\n# 政党ごとの賛成票の割合を計算\nparty_votes &lt;- s117$votes %&gt;%\n  as.data.frame() %&gt;%\n  mutate(party = s117$legis.data$party) %&gt;%\n  group_by(party) %&gt;%\n  summarise(across(everything(), ~mean(. == 1, na.rm = TRUE)))\n\n# データを長形式に変換\nparty_votes_long &lt;- party_votes %&gt;% pivot_longer(cols = -party, names_to = \"Vote\", values_to = \"value\")\n\n# DとRのデータのみを抽出\nparty_votes_d &lt;- party_votes_long %&gt;% filter(party == \"D\")\nparty_votes_r &lt;- party_votes_long %&gt;% filter(party == \"R\")\n\n# Democrats (D) のデータのみをプロット\nggplot(party_votes_d, aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value)) +\n  geom_line(color = \"blue\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\",\n       title = \"Proportion of Yea votes for Democrats\")\n\n\n\n# Democrats (D) と Republicans (R) のデータを同じプロットに追加\nggplot() +\n  geom_line(data = party_votes_d[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Democrat\"), linewidth = 0.5) +\n  geom_line(data = party_votes_r[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Republican\"), linewidth = 0.5) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\", color = \"Party\",\n       title = \"Proportion of Yea votes by Party\")\n\n\n\n\n\n\n\n\n民主党の 0-1 がはっきりした投票行動が見られる．\n\ns109 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S109_votes.ord\",\n                desc=\"109th U.S. Senate\")\n\nAttempting to read file in Keith Poole/Howard Rosenthal (KH) format.\nAttempting to create roll call object\n109th U.S. Senate \n102 legislators and 645 roll calls\nFrequency counts for vote types:\nrollCallMatrix\n    0     1     6     7     9 \n  645 40207 22650     1  2287 \n\n\n\n\n3.4 ベイズ推定\npscl パッケージでは，rollcall オブジェクトに対して ideal() 関数を用いてデータ拡張に基づく Gibbs サンプラーを通じた理想点解析を行うことができる．\nideal() 関数のマニュアル に記載された例では maxiter=260E3, burnin=10E3, thin=100 での実行が例示されているが，ここでは簡単に実行してみる．\n\nn &lt;- dim(s117$legis.data)[1]\nx0 &lt;- rep(0,n)\nx0[s117$legis.data$party==\"D\"] &lt;- -1\nx0[s117$legis.data$party==\"R\"] &lt;- 1\n\nlibrary(tictoc)\ntic(\"ideal() fitting\")\n\nid1 &lt;- ideal(s117,\n             d=1,\n             startvals=list(x=x0),\n             normalize=TRUE,\n             store.item=TRUE,\n             maxiter=10000,  # MCMCの反復回数\n             burnin=5000,\n             thin=50,  # 間引き間隔\n             verbose=TRUE)\ntoc()\n\nideal() fitting: 43.938 sec elapsed であった．\n\nplot(id1)\n\nLooking up legislator names and party affiliations\nin rollcall object s117 \n\n\n\n\n\n\n\n\n\nplot.ideal() 関数のマニュアル にある通り，shoALLNames = FALSE がデフォルトになっている．\n\nsummary(id1)  # 全議員の正確な推定値が見れる．\n\nもっとも保守的な議員として Trump，５番目にリベラルな議員として Biden の名前がみえる．Harris は中道である．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#mcmcpack-パッケージ",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#mcmcpack-パッケージ",
    "title": "理想点解析のハンズオン",
    "section": "4 MCMCpack パッケージ",
    "text": "4 MCMCpack パッケージ\n\n4.1 ロジットモデルの推定\n\nlibrary(MCMCpack)\n# データの生成\nx1 &lt;- rnorm(1000)  # 説明変数1\nx2 &lt;- rnorm(1000)  # 説明変数2\nXdata &lt;- cbind(1, x1, x2)  # デザイン行列\n\n# 真のパラメータ\ntrue_beta &lt;- c(0.5, -1, 1)\n\n# 応答変数の生成\np &lt;- exp(Xdata %*% true_beta) / (1 + exp(Xdata %*% true_beta))\ny &lt;- rbinom(1000, 1, p)\n\n# MCMClogitでサンプリング\nposterior &lt;- MCMClogit(y ~ x1 + x2,    # モデル式\n                      burnin = 1000,    # バーンイン期間\n                      mcmc = 10000,     # MCMCの反復回数\n                      thin = 1,         # 間引き数\n                      verbose = 1000)   # 進捗表示間隔\n\n\n# 結果の確認\nsummary(posterior)\n\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean      SD  Naive SE Time-series SE\n(Intercept)  0.4618 0.07436 0.0007436       0.002357\nx1          -0.8595 0.08552 0.0008552       0.002876\nx2           0.9190 0.08653 0.0008653       0.002891\n\n2. Quantiles for each variable:\n\n               2.5%     25%     50%     75%   97.5%\n(Intercept)  0.3176  0.4098  0.4623  0.5139  0.6070\nx1          -1.0326 -0.9167 -0.8586 -0.8006 -0.6986\nx2           0.7565  0.8575  0.9164  0.9773  1.0955\n\nplot(posterior)\n\n\n\n\n\n\n\n\n\n\n4.2 時系列理想点モデルの推定\nMCMCpack パッケージでは，時系列理想点モデルの推定に MCMCdynamicIRT1d() 関数が用意されている．\n\n# データの読み込み\ndata(Rehnquist)  # MCMCpackに含まれるSupreme Court（最高裁）の投票データ\n\n# 初期値の設定\ntheta.start &lt;- rep(0, 9)  # 9人の裁判官の初期値\ntheta.start[2] &lt;- -3      # Stevens裁判官の初期値\ntheta.start[7] &lt;- 2       # Thomas裁判官の初期値\n\n# MCMCの実行\nout &lt;- MCMCdynamicIRT1d(\n    t(Rehnquist[,1:9]),           # データ行列（転置して裁判官×案件の形に）\n    item.time.map=Rehnquist$time, # 各案件の時期情報\n    theta.start=theta.start,      # 初期値\n    mcmc=50000,                   # MCMCの反復回数\n    burnin=20000,                 # バーンイン期間\n    thin=5,                       # 間引き数\n    verbose=500,                  # 進捗表示間隔\n    tau2.start=rep(0.1, 9),      # τ²の初期値\n    e0=0, E0=1,                  # θの事前分布パラメータ\n    a0=0, A0=1,                  # αの事前分布パラメータ\n    b0=0, B0=1,                  # βの事前分布パラメータ\n    c0=-1, d0=-1,               # τ²の事前分布パラメータ\n    store.item=FALSE,            # アイテムパラメータを保存しない\n    theta.constraints=list(Stevens=\"-\", Thomas=\"+\")  # 識別制約\n)\n\ntheta_cols &lt;- grep(\"theta\", colnames(out), value=TRUE)\ntheta_mcmc &lt;- out[, theta_cols]\n\n# library(coda)\n# summary(theta_mcmc)  # codaのsummary関数で要約\nplot(theta_mcmc)\n\n\n\ntheta_means &lt;- colMeans(theta_mcmc)\ntime_points &lt;- unique(Rehnquist$time)\nn_subjects &lt;- 9  # 裁判官の数\n\n# 各裁判官の軌跡をプロット\nplot(time_points, theta_means[1:length(time_points)], \n     type=\"l\", ylim=range(theta_means),\n     xlab=\"Time\", ylab=\"Ideal Point\",\n     main=\"Estimated Ideal Points Over Time\")\n\n# 各裁判官を異なる色で追加\ncolors &lt;- rainbow(n_subjects)\ncolors[9] &lt;- \"blue\"\nfor(i in c(1,8,9)) {\n    lines(time_points, \n          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],\n          col=colors[i],\n          lwd=3)\n}\n\n# 凡例を追加\nlegend(\"topright\", \n       legend=unique(colnames(Rehnquist)[c(1,8,9)]),  # 裁判官の名前\n       col=colors[c(1,8,9)], \n       lty=1)\n\n\nたしかに William Rehnquist は共和党，Ruth Bader Ginsburg と Stephen Breyer は民主党である．\n\n\\(0\\) の上に位置している Antonin Scalia や Sandra Day O’Connor は保守党である．\n\\(0\\) よりも下に位置するもう一人は David Souter であるが，彼はもともと保守系と木されていたが，後年リベラルな傾向を示したとされる．2\n\n\n4.3 Martin-Quinn スコア\n\n\n\n\n\n\n\nmqscores.wustl.edu にて Martin-Quinn スコアが提供されている．\n\n\n\n\n\n\n4.4 変化点解析\n(Chib, 1998) に基づく変化点モデルのベイズ推定の関数 MCMCpoissonChange() も実装されている．詳しくは (Martin et al., 2011) 第4節参照．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#emirt-パッケージ",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#emirt-パッケージ",
    "title": "理想点解析のハンズオン",
    "section": "5 emIRT パッケージ",
    "text": "5 emIRT パッケージ\ninstall.packages(\"emIRT\")\nこのパッケージには備え付けの 80-110 議会期の上院における点呼投票データ dwnom がある．\nこのデータに対して，階層モデルを用いた理想点解析を行う関数 hierIRT() がある．\n\nlibrary(emIRT)\ndata(dwnom)\n\n## This takes about 10 minutes to run on 8 threads\n## You may need to reduce threads depending on what your machine can support\nlout &lt;- hierIRT(.data = dwnom$data.in,\n                    .starts = dwnom$cur,\n                    .priors = dwnom$priors,\n                    .control = {list(\n                    threads = 8,\n                    verbose = TRUE,\n                    thresh = 1e-4,\n                    maxit=200,\n                    checkfreq=1\n                        )})\n\n## Bind ideal point estimates back to legislator data\nfinal &lt;- cbind(dwnom$legis, idealpt.hier=lout$means$x_implied)\n\n## These are estimates from DW-NOMINATE as given on the Voteview example\n## From file \"SL80110C21.DAT\"\nnomres &lt;- dwnom$nomres\n\n## Merge the DW-NOMINATE estimates to model results by legislator ID\n## Check correlation between hierIRT() and DW-NOMINATE scores\nres &lt;- merge(final, nomres, by=c(\"senate\",\"id\"),all.x=TRUE,all.y=FALSE)\ncor(res$idealpt.hier, res$dwnom1d)"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#footnotes",
    "title": "理想点解析のハンズオン",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Souter was nominated to the Supreme Court without a significant”paper trail” but was expected to be a conservative justice. Within a few years of his appointment, Souter moved towards the ideological center. He eventually came to vote reliably with the Court’s liberal wing.” Wikipedia より引用．↩︎\n１つの議会期 (Congress) は２つの会期 (Session)，第１会期と第２会期から構成される．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html",
    "title": "ベイズ理想点解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#理想点解析と項目反応モデル",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#理想点解析と項目反応モデル",
    "title": "ベイズ理想点解析",
    "section": "1 理想点解析と項目反応モデル",
    "text": "1 理想点解析と項目反応モデル\n理想点解析とは，項目反応モデルを利用した各議員（立法府の構成員など）の行動様式・イデオロギーの定量化・視覚化の手法である．\n詳しくは次項を参照：\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本稿では理想点解析を目標として，項目反応モデル，特に二項選択モデルをベイズ推定する方法を考える．\nその中でも，理想点 \\(x_i\\in\\mathbb{R}^d\\) の次元 \\(d\\) に関してモデル選択を行うことを考える．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#変数選択と-sticky-pdmp-サンプラー",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#変数選択と-sticky-pdmp-サンプラー",
    "title": "ベイズ理想点解析",
    "section": "2 変数選択と Sticky PDMP サンプラー",
    "text": "2 変数選択と Sticky PDMP サンプラー\n\n2.1 はじめに\nまずは次の標準的な理想点モデルを考える (Imai et al., 2016, p. 633) 参照：\n\n\n\n\n\n\n標準的な理想点モデル\n\n\n\n\\(i\\in[N]\\) 番目の議員が \\(j\\in[J]\\) 番目の法案に対して賛成ならば \\(y^i_j=1\\)，反対ならば \\(y^i_j=0\\) のデータ \\(y\\in M_{N,J}(2)\\) が得られているとする．\nこのとき \\(i\\in[N]\\) 番目の議員の理想点 \\(x_i\\in\\mathbb{R}^d\\) は，\\(y^i\\) を次のように予測する潜在変数とする： \\[\\begin{align*}\n  y^i_j&=1_{\\mathbb{R}^+}(\\widetilde{y}^i_j)\\\\\n  \\widetilde{y}^i_j&=x_i^\\top\\beta_j+\\epsilon^i_j,\\qquad\\epsilon^i_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1).\n\\end{align*}\\]\nすなわち \\(d\\)-母数のロジット項目反応モデルにおいて，議員ごとの母数である \\(x_i\\in\\mathbb{R}^d\\) を 理想点 と呼ぶ．項目識別母数 \\(\\beta_j\\in\\mathbb{R}^d\\) は法案ごとの性質の違いを表しているものと考える．\n換言すれば，\\(\\Phi\\) を \\(\\mathrm{N}(0,1)\\) の分布関数として，次のプロビットモデルが想定されたことになる： \\[\n\\operatorname{P}[y^i_j=1]=\\Phi(x_i^\\top\\beta_j).\n\\]\n\n\nこのモデルの潜在変数 \\(\\widetilde{y}\\) とパラメータ \\((x_i)_{i=1}^N\\in M_{dN}(\\mathbb{R}),(\\beta_j)_{j=1}^J\\in M_{dJ}(\\mathbb{R})\\) の事後分布を推定すると同時に，理想点の次元 \\(d\\) についてモデル選択を行うことを考える．\n\n\n2.2 事後分布の表示\n\\((x_i),(\\beta_j)\\)\n\\[\\begin{align*}\n  p(\\widetilde{y},x,\\beta|y)&=\n\\end{align*}\\]\nこのモデルはテーブルデータ \\(y^i_j\\) の対称性の崩れに全ての情報が委ねられている．その性質上強い多峰性を持った事後分布になるはずであり，本質的に MCMC のサンプリングが困難である．\nそこでここでは PDMP サンプラーを用いた事後分布サンプリングを試みる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#モデルの拡張",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#モデルの拡張",
    "title": "ベイズ理想点解析",
    "section": "3 モデルの拡張",
    "text": "3 モデルの拡張\n\n3.1 時系列データへの適用\n(Martin and Quinn, 2002) は最高裁判事の理想点の経時変化を解析し，その MCMC を通じた推定法が MCMCpack パッケージで実装されている (Martin et al., 2011)．\nしかしこの方法は極めて時間がかかることで知られており，(Imai et al., 2016, p. 643) は代わりに変分ベイズ推論による推定法を提案し，５日の推定時間が４秒に短縮されたとしている．\n\n\n3.2 階層化\n(Bafumi et al., 2005) では従来のモデル \\[\ny^*_l=\\alpha_{j[l]}+x_{i[l]}^\\top\\beta_{j[l]}+\\epsilon_l,\\qquad\\epsilon_l\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1)\n\\] に次の階層構造を加えたモデルを考察している： \\[\nx_{i[l]}=\\gamma_{g[i[l]]}^\\top z_{i[l]}+\\eta_{i[l]},\\qquad\\eta_{i[l]}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_{g[i[l]]}).\n\\]\nこれにより議員 \\(i[l]\\in [N]\\) ごとに異なる共変量 \\(z_{i[l]}\\in\\mathbb{R}^M\\) をモデルに加味することが可能になっている．\n\\(g:[N]\\to\\mathbb{N}\\) は議員のグループを表している．\n\n\n\n\n\n\n線型トレンド時系列モデルとしての解釈\n\n\n\n\n\n\\(g:[N]\\to\\mathbb{N}\\) を議員番号とし，\\(i[l]\\) を当該議員の累計議員経験年数とする（○○議員３期目，など）．\n加えて \\(z_{i[l]}\\) を累計議員経験年数と取ると，議員ごとに係数 \\(\\gamma_{g[i[l]]}\\) を持った線型トレンドをモデリングすることに相当する．\nこのモデルは DW-NOMINATE (Poole and Rosenthal, 1997) が用いたのちに (Bailey, 2007) などでも継承されている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#クラスタリングと-zigzag-within-gibbs-サンプラー",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#クラスタリングと-zigzag-within-gibbs-サンプラー",
    "title": "ベイズ理想点解析",
    "section": "4 クラスタリングと ZigZag-within-Gibbs サンプラー",
    "text": "4 クラスタリングと ZigZag-within-Gibbs サンプラー"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#注",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#注",
    "title": "ベイズ理想点解析",
    "section": "5 注",
    "text": "5 注\n\n本稿は (Imai et al., 2016) と (Hardcastle et al., 2024) に触発されて書かれた．(Imai et al., 2016) では種々の理想点モデルを，最も純粋なものから真に興味深い複雑なものまで，変分ベイズ推論の観点で統一的に扱っている．(Hardcastle et al., 2024) では大規模な polyhazard モデルとそのモデル選択を，単一の PDMP サンプラーで行っている．\n(Imai et al., 2016) では変分 EM アルゴリズムとパラメトリックブートストラップが一貫して用いられているが，本稿はあくまで MCMC の正統進化としての PDMP サンプラーを主軸に据えることを提案する．政治科学におけるモデルは不確実性が大きく，これを直接に取り扱えるベイズの方法が望ましいと信じるためである（最終的な目標にモデル平均による推定がある）．\nベイズ計算手法としては，PDMP の採用により正確でバイアスのない推定を，従来よりもはるかに高速に推定できることを示す．加えて PDMP サンプラーでは次のような追加の利点がある\n\n(Imai et al., 2016) のようなパラメトリックブートストラップをせずとも自然な不確実性の定量化を与える．\nモデル選択と平均を同時に行うことができる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#acknowledgement",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#acknowledgement",
    "title": "ベイズ理想点解析",
    "section": "6 Acknowledgement",
    "text": "6 Acknowledgement\n\n\nThis work was supported in part by The Graduate University for Advanced Studies, SOKENDAI.\n\n\nThe author(s) would like to thank the Isaac Newton Institute for Mathematical Sciences, Cambridge, for support and hospitality during the programme Monte Carlo sampling: beyond the diffusive regime, where work on this paper was undertaken. This work was supported by EPSRC grant EP/Z000580/1."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#関連記事",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#関連記事",
    "title": "理想点解析のハンズオン",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-12-15\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本稿では実際に理想点モデルの推定を，Martin-Quinn により公開されている連邦最高裁判所の 1937 年から 2022 年までのデータを（MCMCpack パッケージを通じて）用いて行う．\n\nlibrary(MCMCpack)\ndata(Rehnquist)  # MCMCpackに含まれる U.S. Supreme Court（連邦最高裁）のデータ\nkable(head(Rehnquist))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRehnquist\nStevens\nO.Connor\nScalia\nKennedy\nSouter\nThomas\nGinsburg\nBreyer\nterm\ntime\n\n\n\n\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\nNA\n0\n1994\n1\n\n\n0\n1\n0\n1\n1\n1\n0\n1\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1994\n1\n\n\n\n\n\nこのデータは保守的な判断をする場合が \\(y_i=1\\)，リベラルな判断をする場合が \\(y_i=0\\) の２値データとなっている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#母数ロジットモデル",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#母数ロジットモデル",
    "title": "理想点解析のハンズオン",
    "section": "1 ２母数ロジットモデル",
    "text": "1 ２母数ロジットモデル\n\n1.1 モデルの概要\n(Bafumi et al., 2005) に倣い，次の２母数ロジットモデルをデータに適用することを考える： \\[\ng(x):=\\operatorname{logit}(x)=\\log\\frac{x}{1-x},\n\\] \\[\ng(\\mu_i)=\\gamma_{k[i]}\\biggr(\\alpha_{j[i]}-\\beta_{k[i]}\\biggl).\n\\] この際 \\(\\alpha_j\\) は \\(j\\) 番目の判事の 理想点 といい，\\(\\beta_k\\) は \\(k\\) 番目の事件の性質を表すパラメータである．\nものによっては判事の立場が関係ない事件もあるため，\\(\\gamma_k\\) が用意されている．\n基本的にこの識別パラメータが正になるように調整したいが，明示的にそうすることはしない．\n次節で説明する方法により，理想点 \\(\\alpha_j\\) が大きい場合は保守的な判断を下しやすいものと解釈できるように設計する（\\(\\alpha_j\\) を数直線上にプロットした際に，リベラルな場合に左に，保守的な場合に右に来るようにする）．\n\n\n1.2 識別可能性\n\\(\\alpha_j,\\beta_k\\) に同じ数を足した場合と \\(\\gamma_k\\) と \\((\\alpha_j,\\beta_k)\\) に同じ数を乗じた／除した場合，全く等価なモデルが得られる．すなわちスケールを定める必要がある．\n次のような事前分布と階層構造を置くことでこの問題を回避できる： \\[\n\\alpha_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1),\\qquad\\beta_k=\\mu_\\beta+\\epsilon_\\beta,\\qquad\\epsilon_\\beta\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\beta),\n\\] \\[\n\\gamma_k=\\mu_\\gamma+\\epsilon_\\gamma,\\qquad\\epsilon_\\gamma\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_\\gamma).\n\\]\n最後に \\(\\gamma_k\\) の符号の問題がある．これを \\(-1\\) 倍させることで \\(\\alpha_j,\\beta_k\\) の役割を \\(-1\\) 倍させることができる．このまま推定すると事後分布は \\(0\\) に関して対称な形を持つことになる．\n\\(\\gamma_k\\) の符号を制約したり，特定の判事の \\(\\alpha_j\\) を固定して参照点とするなどの方法があるかもしれないが，ここでは (2.2.3 節 Bafumi et al., 2005, p. 178) に倣って，階層モデルの方法により，構造的なやり方でモデルに情報を伝える．\nというのも，理想点 \\(\\alpha_j\\) に次の階層構造を入れるのである： \\[\n\\alpha_j=\\delta_0+\\delta_1 x_j+\\epsilon_j\\qquad\\epsilon_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1).\n\\]\n\\(x_j\\) は当該判事を示した大統領の所属政党を表す２値変数で，共和党ならば \\(x_j=1\\) とする．そして \\(\\delta_1\\) に \\(\\mathbb{R}_+\\) 上に台を持つ事前分布を置く．\n\n\n\n\n\n\n(Bafumi et al., 2005) による理想点モデルの階層化\n\n\n\nこのように共変量を適切な階層に追加することは，モデルに自然な形で正則化情報を伝えることに繋がり，モデルの識別やより現実的な推定値の獲得に繋がる．\n\n\n\n\n1.3 時系列理想点モデルの出力\n最初に MCMCpack パッケージを通じて理想点推定を簡単に実行する方法を見る．\n識別のためにはまず，Stevens 判事と Thomas 判事の位置を固定する方法を用いてみよう．\n\nlibrary(MCMCpack)\ndata(Rehnquist)  # MCMCpackに含まれるSupreme Court（最高裁）の投票データ\nkable(head(Rehnquist))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRehnquist\nStevens\nO.Connor\nScalia\nKennedy\nSouter\nThomas\nGinsburg\nBreyer\nterm\ntime\n\n\n\n\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\nNA\n0\n1994\n1\n\n\n0\n1\n0\n1\n1\n1\n0\n1\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1994\n1\n\n\n\n\n\nMCMCpack パッケージでは，時系列理想点モデルの推定に MCMCdynamicIRT1d() 関数が用意されている．\n\n# 初期値の設定\ntheta.start &lt;- rep(0, 9)  # 9人の裁判官の初期値\ntheta.start[2] &lt;- -3      # Stevens裁判官の初期値\ntheta.start[7] &lt;- 2       # Thomas裁判官の初期値\n\n# MCMCの実行\nout &lt;- MCMCdynamicIRT1d(\n    t(Rehnquist[,1:9]),           # データ行列（転置して裁判官×案件の形に）\n    item.time.map=Rehnquist$time, # 各案件の時期情報\n    theta.start=theta.start,      # 初期値\n    mcmc=2000,                   # MCMCの反復回数\n    burnin=2000,                 # バーンイン期間\n    thin=5,                       # 間引き数\n    verbose=500,                  # 進捗表示間隔\n    tau2.start=rep(0.1, 9),      # τ²の初期値\n    e0=0, E0=1,                  # θの事前分布パラメータ\n    a0=0, A0=1,                  # αの事前分布パラメータ\n    b0=0, B0=1,                  # βの事前分布パラメータ\n    c0=-1, d0=-1,               # τ²の事前分布パラメータ\n    store.item=FALSE,            # アイテムパラメータを保存しない\n    theta.constraints=list(Stevens=\"-\", Thomas=\"+\")  # 識別制約\n)\n\ntheta_cols &lt;- grep(\"theta\", colnames(out), value=TRUE)\ntheta_mcmc &lt;- out[, theta_cols]\n\n# library(coda)\n# summary(theta_mcmc)  # codaのsummary関数で要約\nplot(theta_mcmc)\n\n\n\n\n\n\n\ntheta_means &lt;- colMeans(theta_mcmc)\npattern &lt;- \"t11\"\ncol_names &lt;- colnames(theta_mcmc)\nselected_cols &lt;- grep(pattern, col_names)  # 正規表現にマッチする列のインデックスを取得\nselected_theta_mcmc &lt;- theta_mcmc[, selected_cols]\n\nquantiles_2_5 &lt;- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.025))\nquantiles_97_5 &lt;- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.975))\n\nggplot(data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = unname(theta_means[selected_cols]),\n  lower = unname(quantiles_2_5),\n  upper = unname(quantiles_97_5)\n), aes(x = mean, y = legislator)) + \n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Estimated Ideal Points (11th term)\", x = \"Ideal Point\", y = \"Legislator\")\n\n\n\n\n\n\n\n\ntime_points &lt;- unique(Rehnquist$time)\nn_subjects &lt;- 9  # 裁判官の数\n\n# 各裁判官の軌跡をプロット\nplot(time_points, theta_means[1:length(time_points)], \n     type=\"l\", ylim=range(theta_means),\n     xlab=\"Time\", ylab=\"Ideal Point\",\n     main=\"Estimated Ideal Points Over Time\")\n\n# 各裁判官を異なる色で追加\ncolors &lt;- rainbow(n_subjects)\ncolors[9] &lt;- \"blue\"\nfor(i in c(1,8,9)) {\n    lines(time_points, \n          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],\n          col=colors[i],\n          lwd=3)\n}\n\n# 凡例を追加\nlegend(\"topright\", \n       legend=unique(colnames(Rehnquist)[c(1,8,9)]),  # 裁判官の名前\n       col=colors[c(1,8,9)], \n       lty=1)\n\n\n\n\n\nたしかに William Rehnquist は共和党，Ruth Bader Ginsburg と Stephen Breyer は民主党である．\n\n\n\n\n\n\\(0\\) の上に位置している Anthony Kennedy や Sandra Day O’Connor はほとんど中道的だが，やや保守党寄りである． Antonin Scalia は特に保守的な立場であることが知られている．\n\\(0\\) よりも下に位置するもう一人は David Souter であるが，彼はもともと保守系と目されていたが，後年リベラルな傾向を示したとされる．1\n\n\n1.4 ２母数ロジットモデルの推定\n\n1.4.1 はじめに\nMCMCpack パッケージで理想点推定の出力がつかめたいま，本節では brms パッケージを用いて項目反応モデルとして具体的な手順を踏んで推定してみる．\n\nlibrary(tidyverse)\ndf &lt;- Rehnquist %&gt;%\n  # データを長形式に変換\n  pivot_longer(cols = -c(term, time), names_to = \"name\", values_to = \"y\") %&gt;%\n  # ケース ID を追加\n  mutate(case = (row_number() - 1) %/% 9 + 1)\n\n\n\n1.4.2 １母数モデル\n\nlibrary(brms)\nformula &lt;- bf(\n  y ~ 1 + (1 | case) + (1 | name)\n)\nfit &lt;- brm(\n  formula,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  chains = 4, cores = 4\n)\n\n\nsummary(fit)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ 1 + (1 | case) + (1 | name) \n   Data: df (Number of observations: 4343) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~case (Number of levels: 485) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.05      0.06     0.93     1.18 1.00     2039     2841\n\n~name (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.63      0.48     0.99     2.84 1.00     1245     1737\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.14      0.56    -1.23     0.97 1.00      640     1001\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n簡単なモデルであるが切片項の ESS が低く，すでに暗雲が立ち込めている．\n\nplot(fit)\n\n\n\n\n\n\n\n\nここには変動係数（我々の欲しい潜在変数）はパラメータとみなされておらず，推定値が表示されないので次のようにしてプロットする必要がある：\n\nranef_legislator &lt;- ranef(fit)$name\nposterior_means &lt;- ranef_legislator[,1,\"Intercept\"]\nlower_bounds &lt;- ranef_legislator[,3,\"Intercept\"]\nupper_bounds &lt;- ranef_legislator[,4,\"Intercept\"]\nplot_legislator &lt;- data.frame(\n  legislator = rownames(ranef_legislator),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_1PL &lt;- ggplot(plot_legislator, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"1PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Legislator\")\np_1PL\n\n\n\n\n\n\n\n\nThomas や Scalia，そして Stevens が極端であることはとらえているが，Stevens や Ginsburg らリベラルな判事は左側に来て欲しいのであった．\n誘導が成功しておらず，片方の峯からサンプリングしてしまっている．\n\nprior_summary(fit)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n student_t(3, 0, 2.5)        sd            case                  0   \n student_t(3, 0, 2.5)        sd Intercept  case                  0   \n student_t(3, 0, 2.5)        sd            name                  0   \n student_t(3, 0, 2.5)        sd Intercept  name                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\n\n\n\n1.4.3 ２母数モデル\nformula_2PL &lt;- bf(\n  y ~ 0 + (1 + name | case)\n)\nfit_2PL &lt;- update(fit, formula = formula_2PL, cores = 4, iter = 3000)\nsummary(fit_2PL)\nFamily: bernoulli \n  Links: mu = logit \nFormula: y ~ (1 | case) + (case | name) - 1 \n  Data: df (Number of observations: 4343) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n        total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~case (Number of levels: 485) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.05      0.06     0.93     1.18 1.00     1800     2643\n\n~name (Number of levels: 9) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)           1.59      0.44     0.96     2.70 1.00      767     1456\nsd(case)                0.00      0.00     0.00     0.00 1.00     1144     2343\ncor(Intercept,case)    -0.06      0.48    -0.87     0.87 1.00     4220     2337\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nplot(fit_2PL)\n\n\n\n\n\nranef_legislator &lt;- ranef(fit_2PL)$name\nposterior_means &lt;- ranef_legislator[,1,\"Intercept\"]\nlower_bounds &lt;- ranef_legislator[,3,\"Intercept\"]\nupper_bounds &lt;- ranef_legislator[,4,\"Intercept\"]\nplot_legislator &lt;- data.frame(\n  legislator = rownames(ranef_legislator),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_2PL &lt;- ggplot(plot_legislator, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Legislator\")\ngrid.arrange(p_1PL, p_2PL, nrow = 1)\n\nよりモデルの不確実性が減って，\\(0\\) の周りに縮小されたことがわかる．これは一般の項目反応モデルで見られる：\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items\n\n\n左派が右に表示されてしまっていることは変わらない．\n\n\n\n1.5 階層ベイズ推定\n\n1.5.1 指名大統領の政党属性\n続いて 1.2 で検討した，(Bafumi et al., 2005) による階層ベイズモデルにより緩やかに情報を伝えることで識別可能性を保つ方法を検討する（第 1.5.2 節）．\n\ndf &lt;- df %&gt;%\n  mutate(\n    nominator = case_when(\n      name %in% c(\"Rehnquist\", \"Stevens\") ~ \"Nixon\",\n      name %in% c(\"O.Connor\", \"Scalia\", \"Kennedy\") ~ \"Reagan\",\n      name %in% c(\"Souter\", \"Thomas\") ~ \"Bush\",\n      name %in% c(\"Breyer\", \"Ginsburg\") ~ \"Clinton\"\n    )\n  )\ndf$x &lt;- ifelse(\n  df$nominator %in% c(\"Nixon\", \"Reagan\", \"Bush\", \"Trump\"),\n  1, -1)\n\n\n\n1.5.2 階層２母数モデル\nx の情報を階層的に伝えるには，もはや brms パッケージでは実行できないようである．\nlibrary(rstan)\nstan_code &lt;- \"\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;\n  int&lt;lower=1&gt; J;\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;\n  vector[J] alpha;\n  vector[J] beta;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(delta);\n  lprior += std_normal_lpdf(gamma);\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += student_t_lpdf(theta | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  epsilon ~ normal(0, theta^2);\n  vector[n] eta;\n  for (k in 1:n) {\n    eta[k] = delta[i[k]] + Z[k] * gamma[i[k]] + epsilon[i[k]];\n  }\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * eta[k];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\"\ncase_number &lt;- as.integer(nrow(df) / 9)\nindicator_i &lt;- rep(1:9, times = case_number)\nindicator_j &lt;- rep(1:case_number, each = 9)\ndf$i &lt;- indicator_i\ndf$j &lt;- indicator_j\n\ndf_NA &lt;- df %&gt;% filter(!is.na(y))\n\ndata &lt;- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, Z = df_NA$x, i = df_NA$i, j = df_NA$j)\nfit &lt;- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n\n\n\n\n\n\nStan コードの出力\n\n\n\n\n\nbrms パッケージでは stancode() 関数を用いて Stan コードを出力できる．\nstancode(fit_2PL)\n結果は以下の通りになる：\n// generated with brms 2.21.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  vector[N] Z_2_2;\n  int&lt;lower=1&gt; NC_2;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  matrix[M_2, N_2] z_2;  // standardized group-level effects\n  cholesky_factor_corr[M_2] L_2;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  matrix[N_2, M_2] r_2;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_2] r_2_1;\n  vector[N_2] r_2_2;\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  // compute actual group-level effects\n  r_2 = scale_r_cor(z_2, sd_2, L_2);\n  r_2_1 = r_2[, 1];\n  r_2_2 = r_2[, 2];\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(to_vector(z_2));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n  vector&lt;lower=-1,upper=1&gt;[NC_2] cor_2;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_2) {\n    for (j in 1:(k - 1)) {\n      cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n    }\n  }\n}\nfunction ブロックでは scale_r_cor() 関数が定義されている．標準化された項目変数 \\((\\alpha_j,\\beta_j)\\) を z として，その各次元の標準偏差を含む２次元ベクトルを sd_2，Cholesky 因子を L_2 として，行列積 sd_2*L2 にベクトル z を掛けて返している．これは \\((\\alpha_j,\\beta_j)\\) を３つの要素（対角行列・Cholesky 因子・標準化されたベクトル）に因数分解して計算していることに起因する．\ndata ブロックでデータのコーディングを定めている．Y は \\(NJ\\) 行列である．case が ID2 で name が ID1 に対応する．判事 \\(i\\in[n]\\) は N_1 人，件数 \\(j\\in[J]\\) は N_2 件．このデータから M_1=1 次元の理想点を持った M_2=2 パラメータモデルを推定する．Z_1_1 が判事を表す標示変数で，項目を表す標示変数は Z_2_1 と Z_2_2 である．\nparameter ブロックでは標準化された理想点 z_1 と項目パラメータ z_2，それぞれの標準偏差 sd_1, sd_2 を宣言している．z_1 だけ N_1 ベクトル，z_2 は N_2 行 M_2 列の行列であることに注意．\ntransformed_parameters で真のスケールに戻す．r_1_1=(sd_1[1]*(z_1[1])) は理想点 \\(x_1\\) にあたり，r_2=scale_r_cor(z_2,sd_2,L_2) は項目パラメータ \\((\\alpha_j,\\beta_j)\\) にあたる．その後 r_2_1=r_2[,1] と r_2_2=r_2[,2] でそれぞれ \\(\\alpha_j\\) と \\(\\beta_j\\) に分解している．最後に sd_1, sd_2 に t-分布，L_2 に Cholesky 分布を事前分布として定義している．\nmodel で尤度を定義している．mu はこのブロックでしか使われない線型予測子の格納変数である．\nmu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\nにより \\(\\alpha_j+\\beta_j x_i\\) が計算されている．\\(j\\) は J_2[n], \\(i\\) は J_1[n] で表されている．最後に bernoulli_logit_lpmf(Y | mu) で尤度が定義される．\ngenerated quantities ブロックでは相関行列 cor_2 を計算している．\n\n\n\n#| output: false\nlibrary(rstan)\nstan_code &lt;- \"\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;\n  int&lt;lower=1&gt; J;\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;\n  vector[n] Z;\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;\n  vector[J] alpha;\n  vector[J] beta;\n\n  vector[N] delta;\n  vector[N] gamma;\n\n  vector[N] epsilon;\n  vector[N] theta;  // sd of epsilon\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(delta);\n  lprior += std_normal_lpdf(gamma);\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += student_t_lpdf(theta | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  epsilon ~ normal(0, theta^2);\n  vector[n] eta;\n  for (k in 1:n) {\n    eta[k] = delta[i[k]] + Z[k] * gamma[i[k]] + epsilon[i[k]];\n  }\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * eta[k];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\"\ncase_number &lt;- as.integer(nrow(df) / 9)\nindicator_i &lt;- rep(1:9, times = case_number)\nindicator_j &lt;- rep(1:case_number, each = 9)\ndf$i &lt;- indicator_i\ndf$j &lt;- indicator_j\n\ndf_NA &lt;- df %&gt;% filter(!is.na(y))\n\ndata &lt;- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, Z = df_NA$x, i = df_NA$i, j = df_NA$j)\nfit &lt;- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\nESS が低く，R-hat が大きく，さらに maximum-treedepth を越して発散したものがあるという．X[1] のサンプルを見てみると \\(10^7\\) というオーダーが出現していた．そこから X に事前分布を置き忘れていたことに気づいた．\nx_samples &lt;- extract(fit, pars = \"X\")$X\n\nplot_dataframe &lt;- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\nggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"Ideal Points of Rehnquist\")\n\nformula_2PL2 &lt;- bf(\n  y ~ gamma * eta,\n  gamma ~ (1|i|case),\n  eta ~ (1|i|case) + (1|name),\n  nl = TRUE\n)\nfit_2PL2 &lt;- brm(\n  formula_2PL2,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  chains = 4, cores = 4\n)\n\nWarning: Rows containing NAs were excluded from the model.\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: There were 114 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: The largest R-hat is 1.54, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点解析パッケージ",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点解析パッケージ",
    "title": "理想点解析のハンズオン",
    "section": "2 理想点解析パッケージ",
    "text": "2 理想点解析パッケージ\n本節では次の３つのパッケージを紹介する：\n\n\n\n\n\n\n\npscl (Zeileis et al., 2008)：GitHub, CRAN．(Arnold, 2018) も参照．\nMCMCpack (Martin et al., 2011)：GitHub, CRAN\nemIRT (Imai et al., 2016)：GitHub, CRAN\n\n\n\n\n\n2.1 pscl パッケージ\ninstall.packages(\"pscl\")\n\n2.1.1 voteview データ\nこのパッケージでは，Keith T. Poole と Howard Rosenthal が 1995 年から運営しているサイト voteview.com のデータを利用するための関数 readKH() が提供されている．\n例えば連邦議会 (U.S. Congress) 117 議会期 (Congress) 2021.1.3-2023.1.3 の上院 (Senate) の点呼投票データを読み込むには以下のようにする：2\n\nlibrary(pscl)\ns117 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S117_votes.ord\",\n                desc=\"117th U.S. Senate\")\n\ns117 は rollcall オブジェクト，８つのフィールドを持った配列である．\ns117$votes データは \\(n=104\\) 議員の計 \\(m=949\\) 回の投票からなる \\(10\\)-値の行列である．\n\nsummary(s117)\n\n\nSummary of rollcall object s117 \n\nDescription:     117th U.S. Senate \nSource:      https://voteview.com/static/data/out/votes/S117_votes.ord \n\nNumber of Legislators:       104\nNumber of Roll Call Votes:   949\n\n\nUsing the following codes to represent roll call votes:\nYea:         1 2 3 \nNay:         4 5 6 \nAbstentions:     7 8 9 \nNot In Legislature:  0 \n\nParty Composition:\n    D Indep     R \n   50     2    52 \n\nVote Summary:\n               Count Percent\n0 (notInLegis)  3544     3.6\n1 (yea)        55542    56.3\n6 (nay)        35995    36.5\n7 (missing)        5     0.0\n9 (missing)     3610     3.7\n\nUse summary(s117,verbose=TRUE) for more detailed information.\n\n\n\n\n2.1.2 点呼投票データ\n点呼投票データとは \\(n\\times m\\) の行列で，そのエントリーは２値変数である（今回は \\(1\\) か \\(6\\)）．\nしかし実際には種々の欠測により，\\(0,7,9\\) も使われる．\nこれをヒートマップで可視化してみる．\n\nlibrary(tidyverse)\n\nvotes_df &lt;- as.data.frame(s117$votes[1:15, 1:15]) %&gt;% rownames_to_column(\"Legislator\")  # 投票データをデータフレームに変換し、行名を列として追加\n\nvotes_long &lt;- votes_df %&gt;% pivot_longer(cols = -Legislator, names_to = \"Vote\", values_to = \"value\")  # データを長形式に変換\n\n\nggplot(votes_long, aes(x = Vote, y = Legislator, fill = value)) + geom_tile() + scale_fill_gradient(low = \"white\", high = \"red\") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Votes\", y = \"Legislators\", title = \"Voting Patterns\")  # ヒートマップを作成\n\n\n\n\n\n\n\n\n\n\n2.1.3 政党毎の賛成率\n政党でソートし，賛成率を最初の 15 法案についてプロットしたものは次の通り：\n\n\nCode\nlibrary(dplyr)\n\n# 政党ごとの賛成票の割合を計算\nparty_votes &lt;- s117$votes %&gt;%\n  as.data.frame() %&gt;%\n  mutate(party = s117$legis.data$party) %&gt;%\n  group_by(party) %&gt;%\n  summarise(across(everything(), ~mean(. == 1, na.rm = TRUE)))\n\n# データを長形式に変換\nparty_votes_long &lt;- party_votes %&gt;% pivot_longer(cols = -party, names_to = \"Vote\", values_to = \"value\")\n\n# DとRのデータのみを抽出\nparty_votes_d &lt;- party_votes_long %&gt;% filter(party == \"D\")\nparty_votes_r &lt;- party_votes_long %&gt;% filter(party == \"R\")\n\n# Democrats (D) のデータのみをプロット\nggplot(party_votes_d, aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value)) +\n  geom_line(color = \"blue\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\",\n       title = \"Proportion of Yea votes for Democrats\")\n\n\n\n# Democrats (D) と Republicans (R) のデータを同じプロットに追加\nggplot() +\n  geom_line(data = party_votes_d[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Democrat\"), linewidth = 0.5) +\n  geom_line(data = party_votes_r[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Republican\"), linewidth = 0.5) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\", color = \"Party\",\n       title = \"Proportion of Yea votes by Party\")\n\n\n\n\n\n\n\n\n民主党の 0-1 がはっきりした投票行動が見られる．\n\ns109 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S109_votes.ord\",\n                desc=\"109th U.S. Senate\")\n\nAttempting to read file in Keith Poole/Howard Rosenthal (KH) format.\nAttempting to create roll call object\n109th U.S. Senate \n102 legislators and 645 roll calls\nFrequency counts for vote types:\nrollCallMatrix\n    0     1     6     7     9 \n  645 40207 22650     1  2287 \n\n\n\n\n2.1.4 ベイズ推定\npscl パッケージでは，rollcall オブジェクトに対して ideal() 関数を用いてデータ拡張に基づく Gibbs サンプラーを通じた理想点解析を行うことができる．\nideal() 関数のマニュアル に記載された例では maxiter=260E3, burnin=10E3, thin=100 での実行が例示されているが，ここでは簡単に実行してみる．\n\nn &lt;- dim(s117$legis.data)[1]\nx0 &lt;- rep(0,n)\nx0[s117$legis.data$party==\"D\"] &lt;- -1\nx0[s117$legis.data$party==\"R\"] &lt;- 1\n\nlibrary(tictoc)\ntic(\"ideal() fitting\")\n\nid1 &lt;- ideal(s117,\n             d=1,\n             startvals=list(x=x0),\n             normalize=TRUE,\n             store.item=TRUE,\n             maxiter=10000,  # MCMCの反復回数\n             burnin=5000,\n             thin=50,  # 間引き間隔\n             verbose=TRUE)\ntoc()\n\nideal() fitting: 43.938 sec elapsed であった．\n\nplot(id1)\n\nLooking up legislator names and party affiliations\nin rollcall object s117 \n\n\n\n\n\n\n\n\n\nplot.ideal() 関数のマニュアル にある通り，shoALLNames = FALSE がデフォルトになっている．\n\nsummary(id1)  # 全議員の正確な推定値が見れる．\n\nもっとも保守的な議員として Trump，５番目にリベラルな議員として Biden の名前がみえる．Harris は中道である．\n\n\n\n2.2 MCMCpack パッケージ\n\n2.2.1 ロジットモデルの推定\n\nlibrary(MCMCpack)\n# データの生成\nx1 &lt;- rnorm(1000)  # 説明変数1\nx2 &lt;- rnorm(1000)  # 説明変数2\nXdata &lt;- cbind(1, x1, x2)  # デザイン行列\n\n# 真のパラメータ\ntrue_beta &lt;- c(0.5, -1, 1)\n\n# 応答変数の生成\np &lt;- exp(Xdata %*% true_beta) / (1 + exp(Xdata %*% true_beta))\ny &lt;- rbinom(1000, 1, p)\n\n# MCMClogitでサンプリング\nposterior &lt;- MCMClogit(y ~ x1 + x2,    # モデル式\n                      burnin = 1000,    # バーンイン期間\n                      mcmc = 10000,     # MCMCの反復回数\n                      thin = 1,         # 間引き数\n                      verbose = 1000)   # 進捗表示間隔\n\n\n# 結果の確認\nsummary(posterior)\n\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean      SD  Naive SE Time-series SE\n(Intercept)  0.4221 0.07468 0.0007468       0.002384\nx1          -0.8632 0.08332 0.0008332       0.002755\nx2           0.9785 0.08575 0.0008575       0.002845\n\n2. Quantiles for each variable:\n\n               2.5%     25%     50%     75%   97.5%\n(Intercept)  0.2747  0.3709  0.4232  0.4736  0.5646\nx1          -1.0329 -0.9174 -0.8613 -0.8074 -0.7010\nx2           0.8122  0.9193  0.9775  1.0360  1.1517\n\nplot(posterior)\n\n\n\n\n\n\n\n\n\n\n2.2.2 変化点解析\n(Chib, 1998) に基づく変化点モデルのベイズ推定の関数 MCMCpoissonChange() も実装されている．詳しくは (Martin et al., 2011) 第4節参照．\n\n\n\n2.3 emIRT パッケージ\ninstall.packages(\"emIRT\")\nこのパッケージには備え付けの 80-110 議会期の上院における点呼投票データ dwnom がある．\nこのデータに対して，階層モデルを用いた理想点解析を行う関数 hierIRT() がある．\n\nlibrary(emIRT)\ndata(dwnom)\n\n## This takes about 10 minutes to run on 8 threads\n## You may need to reduce threads depending on what your machine can support\nlout &lt;- hierIRT(.data = dwnom$data.in,\n                    .starts = dwnom$cur,\n                    .priors = dwnom$priors,\n                    .control = {list(\n                    threads = 8,\n                    verbose = TRUE,\n                    thresh = 1e-4,\n                    maxit=200,\n                    checkfreq=1\n                        )})\n\n## Bind ideal point estimates back to legislator data\nfinal &lt;- cbind(dwnom$legis, idealpt.hier=lout$means$x_implied)\n\n## These are estimates from DW-NOMINATE as given on the Voteview example\n## From file \"SL80110C21.DAT\"\nnomres &lt;- dwnom$nomres\n\n## Merge the DW-NOMINATE estimates to model results by legislator ID\n## Check correlation between hierIRT() and DW-NOMINATE scores\nres &lt;- merge(final, nomres, by=c(\"senate\",\"id\"),all.x=TRUE,all.y=FALSE)\ncor(res$idealpt.hier, res$dwnom1d)"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html",
    "href": "posts/2024/Kernels/HierarchicalModel.html",
    "title": "階層モデル再論",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#はじめに",
    "href": "posts/2024/Kernels/HierarchicalModel.html#はじめに",
    "title": "階層モデル再論",
    "section": "はじめに",
    "text": "はじめに\n潜在変数模型とはどうやらとんでもなく広い射程を持った対象であるようである．\n\n\n\n\n\n\n潜在変数モデルとは……\n\n\n\n\n心理学，経済学をはじめとして多くの分野で中心的に扱われてきたモデルである（構造方程式モデル，因子分析，項目応答モデル，同時方程式モデルなど）．\nベイズ統計学では 階層モデル (hierarchical model) として極めて重要な役割を果たす．\n生成モデリング（VAE, EBM, Diffusion, GAN, Probabilistic Graphical Model）も，観測変数上の周辺分布がデータ分布に近づくように潜在変数模型を学習する方法である．\n認知科学において，脳も潜在変数模型に基いてメンタルモデルを構成しているという仮説もある（InfoMax に関する稿も参照）．表現学習や独立成分分析の指導原理になっている側面がある．\n情報理論において，通信路の組み合わせは潜在変数模型としてモデリングできる．さらには，潜在変数模型は数学的には確率空間の圏上の図式であるとして研究されている (Perrone, 2024)．\n\n\n\nこのように種々の文脈で登場する潜在変数模型であるが，それぞれの文脈において「潜在変数」の果たす役割は全く違う．\nしかし，数学的には全く同じ枠組みで記述できる．従って，そのように扱うことは一定の価値を持つだろう．\n実際，近年になり，これから本稿で解説するように，潜在変数モデルの観点から心理学，経済学，環境科学，遺伝学，信号処理，逆問題，社会学，政治科学，マーケティング分野で独自に発展した手法が，特定の手法の特別な場合と見れるという理解が進み，手法の交流と知見の交換が進んでいる．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#本稿の目的",
    "href": "posts/2024/Kernels/HierarchicalModel.html#本稿の目的",
    "title": "階層モデル再論",
    "section": "本稿の目的",
    "text": "本稿の目的\n本稿では主成分分析，因子分析，構造方程式モデリング，混合モデル，独立成分分析を，潜在変数モデルとして解釈し，図式で理解する．\n確率変数を丸つきの大文字で表し，\\(X^i,Y^i\\) は観測変数，\\(Z^i\\) は潜在変数を表す．矢印は 確率核 を表す．\n\n\n\n混合モデル（第 4 節）\n\n\n種々の 多変量解析法 を（ベイズ）階層モデルとして統一的に理解すると同時に，それぞれの文脈での「使い方の違い」に注目することを目指す．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#sec-PCA",
    "href": "posts/2024/Kernels/HierarchicalModel.html#sec-PCA",
    "title": "階層モデル再論",
    "section": "1 主成分分析 (PCA)",
    "text": "1 主成分分析 (PCA)\n\n1.1 はじめに\n主成分分析では，\\(p\\) 次元のデータ \\(\\{x_i\\}_{i=1}^n\\subset\\mathbb{R}^p\\) の各成分を，より少数の潜在変数を持った１層の線型 Gauss 模型\n\nで説明しようとする．1\n歴史的に主成分分析は，おろした垂線の足の二乗距離和の意味でコストが最小になるような線型射影を求める問題 (Pearson, 1901) として最初に登場し，値の分散が最大となるような線型射影を求める問題 (Hotelling, 1933) として PCA の名前がつき，心理学分野，特に psychometrika で取り上げられて大きく発展した．\nこのような潜在変数モデルとしての見方は probabilistic PCA (Tipping and Bishop, 1999) / SPCA (Sensible PCA) (Roweis, 1997) として，因子分析から逆輸入する形で初めて自覚された見方である（第 2.3.1 節も参照）．\n確率的な見地から見れば，正規性を仮定した変数 \\(Z^1,\\cdots,Z^r\\) の事前分布が互いに独立なデルタ分布に縮退している場合が古典的な PCA である (Roweis, 1997)．\nいずれの場合も追加の過程なくしてモデルは識別可能性がなく，後続タスクに応じて種々の制約を追加することで所望の解を得る，という動的な使い方がなされる．\n以降，\\(X\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p),Z\\in\\mathcal{L}(\\Omega;\\mathbb{R}^r)\\) を確率変数， \\[\n\\boldsymbol{X}=(x_i^j)\\in M_{n,p}(\\mathbb{R}),\\boldsymbol{Z}=(z_i^j)\\in M_{n,r}(\\mathbb{R})\n\\] を行列することに注意．\n\n\n1.2 概要\nPCA ではデータ行列を \\[\n\\boldsymbol{X}:=\\begin{pmatrix}x_1^\\top\\\\\\vdots\\\\x_n^\\top\\end{pmatrix}\\in M_{n,p}(\\mathbb{R})\n\\] で定めたとき，データ次元 \\(p\\) より小さい数の成分 \\(r\\) で説明しようとする： \\[\n\\boldsymbol{X}\\approx\\boldsymbol{Z}C^\\top,\\qquad\\boldsymbol{Z}:=\\begin{pmatrix}z_1^\\top\\\\\\vdots\\\\z_n^\\top\\end{pmatrix}\\in M_{n,r}(\\mathbb{R}),C\\in M_{p,r}(\\mathbb{R}).\n\\]\n\n\n\n\n\n\n\n古典的には，\\(z_{ij}\\) を（主成分）得点 (score)，\\(Z^i\\) を 合成変量，\\(c_{ij}\\) を 負荷量 (loading) ともいう (足立浩平 and 山本倫生, 2024)．\n機械学習では \\(Z^1,\\cdots,Z^r\\) を 潜在因子，\\(W\\in M_{pr}(\\mathbb{R})\\) を 荷重 (weight) ともいう (Murphy, 2022)．\n\n\n\n\nこの問題は \\(\\boldsymbol{X}\\) の 特異値分解 (SVD) \\(\\boldsymbol{X}=U\\Sigma V^\\top\\) により解ける： \\[\n\\boldsymbol{Z}=U\\Sigma_{1:r}^\\alpha A=\\boldsymbol{X}(\\underbrace{V\\Sigma^{\\alpha-1}_{1:r}A}_{=:W}),\\qquad C:=V\\Sigma_{1:r}^{1-\\alpha}(A^{-1})^\\top.\n\\] ただし，\\(\\alpha\\in\\mathbb{R},A\\in\\mathrm{GL}_p(\\mathbb{R})\\) は任意である．この解は，特異値分解の性質により，残差を Hilbert-Schmidt ノルムの意味で最小にする： \\[\n\\min_C\\|\\boldsymbol{X}-\\boldsymbol{Z}C^\\top\\|_\\mathrm{HS}=\\min_C\\frac{1}{n}\\sum_{i=1}^n\\lvert x_i-Cz_i\\rvert^2=\\sigma_{r+1}\n\\tag{1}\\] この目的関数は復元誤差とも理解できる．ただし，\\(\\sigma_{r+1}\\) は行列 \\(\\boldsymbol{Z}C\\) の第 \\(r+1\\) 特異値である．\n\n\n1.3 主成分分散最大化\n荷重行列 \\(W\\) が \\(W^\\top W=I_r\\) を満たすという制約条件を追加すると，目的関数 (1) は潜在変数の分散を最大にすることと等価になる： \\[\n\\operatorname*{argmin}_{W}\\|\\boldsymbol{X}-\\boldsymbol{Z}W\\|_\\mathrm{HS}=\\operatorname*{argmin}_W\\operatorname{Tr}((\\boldsymbol{X}W)^\\top\\boldsymbol{X}W).\n\\tag{2}\\]\nすなわち，\\(\\boldsymbol{Z}=\\boldsymbol{X}W\\) の変動が差大になるようにすれば良い．\nそのためには，確率変数 \\(X\\) のデータ行列 \\(\\boldsymbol{X}\\) から計算した経験共分散行列 \\(S\\in M_{p}(\\mathbb{R})_+\\) の固有ベクトルのうち，対応する固有値が大きいものから \\(w_1,\\cdots,w_r\\) として荷重行列とすれば良い： \\[\nW:=(w_1\\;\\cdots\\;w_r).\n\\]\n実はこれは解の１つに過ぎず，\\(W\\) に右から直交行列を乗じて「回転」させたものは全て解になる．上の解は追加の条件 \\(Z^\\top Z=I_r\\) を課すことで特定される．\n\n\n1.4 計算上の注意\n各次元に関する長さのスケールを揃えるために，PCA を始める前にデータを正規化しておくか，または共分散行列 \\(S\\) の代わりに，相関行列を用いるべきである．\nまた，実際に最適化や相関行列の固有値分解をすることはなく，基本的に SVD の方が \\(O(np^2)+O(p^3)\\) と高速である (Unkel and Trendafilov, 2010)．\nさらに次元 \\(p\\) が高い場合は，確率的 SVD (Halko et al., 2011), (Drineas and Mahoney, 2016) を用いてさらに \\(O(nr^2)+(r^3)\\) まで削減できる．このような手法は確率的数値解析と呼ばれる (Murray et al., 2023)．\n\n\n1.5 線型射影による次元縮約\n\\(W^\\top W=I_r\\) の仮定の下で，PCA の目的関数 (1) は，潜在変数の分散最大化 (2) と見れるのだった．\nこれは同じ仮定の下で，データ変数 \\(X\\) の最小誤差の線型射影を求める問題とも見れる： \\[\n\\operatorname*{argmin}_W\\|\\boldsymbol{X}-\\boldsymbol{Z}W\\|_\\mathrm{HS}=\\operatorname*{argmin}_W\\|\\boldsymbol{X}-\\boldsymbol{X}WW^\\top\\|_\\mathrm{HS}.\n\\]\nなお，一般の行列 \\(A\\) について \\(P_A=A(A^{-1}A)^+A^\\top\\) は \\(\\mathrm{Im}\\,A\\) 上の直交射影になる．\\(A\\) が直交行列であるとき，\\(P_A=AA^\\top\\) が成り立つ．\n\n\n1.6 因子分析志向の主成分分析\n因子分析では，\\(Z^1,\\cdots,Z^r\\) を対等な因子と見て，それぞれのデータへの影響を調べたい．このような場合は， \\[\n\\frac{1}{n}\\boldsymbol{Z}^\\top\\boldsymbol{Z}=I_r\n\\] が自然な制約になる．この際の解は，直交行列 \\(T\\in O_r(\\mathbb{R})\\) の違いを除いて， \\[\n\\boldsymbol{Z}=\\sqrt{n}UT,\\qquad C=\\frac{1}{\\sqrt{n}}V\\Sigma_{1:r}T,\\qquad W=\\sqrt{n}V\\Sigma_{1:r}^{-1}T,\n\\] まで確定する．\nしばしば，追加の仮定 \\[\nC^\\top C=\\mathrm{diag}(\\rho_{1:r}),\\qquad \\rho_1\\ge\\cdots\\ge\\rho_r\\ge0\n\\] を課して得られる一意な解 \\[\n\\boldsymbol{Z}=\\sqrt{n}U,\\qquad C=\\frac{1}{\\sqrt{n}}V\\Sigma_{1:r},\\qquad W=\\sqrt{n}V\\Sigma_{1:r}^{-1},\n\\] を 初期解 と呼び，これを「回転」させることで他の解が探索され，所望の分解を探す．\n因子分析では (Thurstone, 1947) 以来，種々の回転法とアルゴリズムが蓄積している (足立浩平 and 山本倫生, 2024)．一般にこの文脈では，(Thurstone, 1947) にいう「単純構造」を達成した，解釈が容易な因子をドメイン知識に基づいて構成することを目指す．この「単純構造」とは，現代でいう一種の disentangled factor と理解できる．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#sec-FA",
    "href": "posts/2024/Kernels/HierarchicalModel.html#sec-FA",
    "title": "階層モデル再論",
    "section": "2 因子分析 (FA)",
    "text": "2 因子分析 (FA)\n\n2.1 はじめに\n主成分分析が「低階数近似」ならば，因子分析は「高階数近似」というべきである (足立浩平, 2023)．\n\nより正確には，因子分析は，観測の各次元 \\(X^1,\\cdots,X^p\\) ごとに「独自因子」\\(Z^1,\\cdots,Z^p\\) を想定しつつ，全観測に共通する「共通因子」\\(F^1,\\cdots,F^r\\) をどのように抽出できるかを考える，という志向性を持つ：\n\nこの意味では，FA は独自因子 \\(U^1,\\cdots,U^p\\) を追加した PCA とも理解できる．\n歴史的には (Spearman, 1904) が古典テスト理論の文脈で \\(r=1\\) の因子分析を，(Thurstone, 1947) が一般の \\(1\\le r&lt;p\\) の場合の因子分析を「回転」の手法と共に導入した．\nさらに興味深いことに，FA では PCA をはじめとした多くの多変量分析手法と違い，(Lawley, 1942), (Anderson and Rubin, 1956) らにより，初期から確率的な扱いが発展した手法である (足立浩平 and 山本倫生, 2024)．\nFA に倣う形で，PCA にも確率論的なアプローチが導入された (Tipping and Bishop, 1999), (Roweis, 1997)．\n\n\n2.2 概要\nFA では \\(\\boldsymbol{Z}=(\\boldsymbol{F}\\;\\boldsymbol{U})\\in M_{n,r+p}(\\mathbb{R})\\) の分解に基づき， \\[\n\\boldsymbol{X}\\approx\\boldsymbol{F}A^\\top+\\boldsymbol{U}\\Psi^{1/2},\\qquad A\\in M_{r,p}(\\mathbb{R}),\\Psi=\\mathrm{diag}(\\psi_1,\\cdots,\\psi_p)\\in M_p(\\mathbb{R}),\n\\] によってデータ行列 \\(\\boldsymbol{X}\\in M_{n,p}(\\mathbb{R})\\) を説明しようとする．2\nPCA よりさらに識別可能性は絶望的であるが，FA では潜在変数の解釈可能性担保のため，次の仮定を課す： \\[\n\\boldsymbol{1}_n^\\top\\boldsymbol{F}=\\boldsymbol{0}_r,\\qquad \\boldsymbol{1}_n^\\top\\boldsymbol{U}=\\boldsymbol{0}_p,\n\\] \\[\n\\boldsymbol{F}^\\top\\boldsymbol{F}=n\\boldsymbol{I}_r,\\qquad \\boldsymbol{U}^\\top\\boldsymbol{U}=n\\boldsymbol{I}_p,\\qquad\\boldsymbol{F}^\\top\\boldsymbol{U}=O.\n\\] すなわち，推定される確率変数 \\(F,U\\) が標準化されていて互いに無相関であるように誘導する．\nまた，\\(\\boldsymbol{U}\\) の経験分散が \\(\\Psi\\) になることに注意．\n\n\n\n\n\n\n\n古典的には，\\(f_{ij}\\) を共通因子，\\(\\psi_j\\) を独自因子の 得点 (score)，\\(a_{ij}\\) を 負荷量 (loading) ともいう (足立浩平 and 山本倫生, 2024)．\n機械学習では \\(Z^1,\\cdots,Z^r\\) を 潜在因子，\\(W\\in M_{pr}(\\mathbb{R})\\) を 荷重 (weight) ともいう (Murphy, 2022)．\n\n\n\n\nこの問題は，\\(C:=(A\\;\\Psi^{1/2})\\) と定めると，PCA と同じ問題 (1) に帰着される： \\[\n\\min_C\\|\\boldsymbol{X}-\\boldsymbol{Z}C^\\top\\|_\\mathrm{HS}.\n\\]\nこれはやはり特異値分解により解くことができる (De Leeuw, 2004)．\n解は直交行列による回転を除いても，やはり一意に定まらないようである．\n\n\n2.3 確率的アプローチ\nここで， \\[\nU:=\\begin{pmatrix}U^1\\\\\\vdots\\\\U^p\\end{pmatrix}\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p),\\qquad F:=\\begin{pmatrix}F^1\\\\\\vdots\\\\F^r\\end{pmatrix}\\in\\mathcal{L}(\\Omega;\\mathbb{R}^r),\n\\] を確率変数とすると， \\[\nX\\approx AF+\\Psi^{1/2}U\n\\tag{3}\\] によって \\(X\\) に確率モデルが誘導されることになる．\n\n2.3.1 正規性の仮定\n\\(U,F\\) に正規性の仮定をおけば，このモデルは EM アルゴリズムなどを用いて最尤推定できる (Rubin and Thayer, 1982), (Ghahramani and Hinton, 1996)．このような最尤推定のアプローチは (Lawley, 1942) から考えられていた．\nこの見方が PCA にも応用された．追加の仮定 \\[\nA^\\top A=I_{r},\\qquad \\Psi=\\sigma^2I_p,\n\\] の下での FA への確率論的アプローチを probabilistic PCA (Tipping and Bishop, 1999) / SPCA (Sensible PCA) (Roweis, 1997) という．\n\\(\\sigma\\to0\\) の極限で古典的 PCA が回復される．\n\n\n2.3.2 共分散構造分析\n一方で，分布の仮定は課さず，\\(X\\) の経験分散 \\(S\\) を，式 (3) の右辺の共分散 \\[\n\\Sigma:=AA^\\top+\\Psi\n\\] となるべく近づけるように学習する方法もある．\n例えば (Harman and Jones, 1966), (Harman and Fukuda, 1966) では，Hilbert-Schmidt ノルム \\(\\|S-\\Sigma\\|_\\mathrm{HS}\\) の最小化することで解を探索する方法が考慮された．\nこのように，データの共分散行列を低階数近似するアプローチは 共分散構造分析 (Bock and Bargmann, 1966) ともいう．\nさらに，確率論的なアプローチは一般の構造方程式モデル (SEM, 次節 3 参照) へと発展 (Karl Gustav Jöreskog, 1970), (Sörbom, 1974), (Karl G. Jöreskog, 1978) し，現状，共分散構造分析は SEM の特別な場合と解される．3\n\n\n\n2.4 スパース推定\nFA のモデルは識別可能とは程遠く，解釈可能性が重要である．(Thurstone, 1947) は因子付加行列が「単純構造」を持つことを一つの指標としたが，現代的にはスパース推定の言葉で与えられた 完全単純構造 (Bernaards and Jennrich, 2003) を仮定することが増えてきた．\nスパース PCA (Zou et al., 2006), (Ian T Jolliffe and Uddin, 2003) では，従来の SVD + 回転ではなく，LASSO 様の \\(L^1\\)-正則化項によって，解釈可能な因子付加行列を得ようとする．最終的に得られる目的関数は elastic net (Zou and Hastie, 2005) 様になる．\n等価だが，自動関連度決定 (ARD) を用いた Bayesian PCA (Bishop, 1998), (Archambeau and Bach, 2008) や spike-and-slab (Rattray et al., 2009) など，スパース性を促す事前分布を用いることもできる．\n\n\n2.5 その他の事前分布\n非正規な事前分布（特に Laplace 分布やロジスティック分布などの裾の重いもの）を用いることで，モデルが識別可能性を回復することがある．\nこのように，一般の設定で潜在変数モデルが識別可能になるための条件が，非線型独立分析の分野で提案されている (Khemakhem et al., 2020)．\n\n2.5.1 Gamma 分布\nまた，Gamma 事前分布は非負かつスパースな表現を促進し，カウントデータとよく用いられる (Canny, 2004)．\nこれは環境科学分野の Positive Matrix Factorization (Paatero and Tapper, 1994) や信号処理分野の Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999) の，確率論的な一般化と見れる (Buntine and Jakulin, 2006)．\n\n\n2.5.2 Dirichlet 分布\nまた，Dirichlet 事前分布を用いることで，潜在変数 \\(Z\\in\\mathcal{L}(\\Omega;\\mathbb{R}^r)\\) に \\[\n\\sum_{i=1}^rZ^i=1\n\\] が課されるため，「各次元への依存度」のような意味づけが可能になる．これは政治学における空間分析において，「どの立場への傾倒が強いか」を推定することにも用いられる (Buntine and Jakulin, 2006)．\nこのモデルは multinomial PCA (Buntine and Jakulin, 2006) の他に，遺伝学で admixture (Pritchard et al., 2000)，simplex factor analysis (Bhattacharya and Dunson, 2012), 科学出版で mixed-membership model (Erosheva et al., 2004)，マーケティングで user rating profile model (Marlin, 2003) など，種々の分野で独立に提案されている．\n\n\n\n2.6 非線型化\nFA の一般化の方向性として，正規性の緩和の他に，線型性の緩和があり得る．\nMCMC による推論 (Hoffman, 2017) をすることも，または指数型分布 (Collins et al., 2001) への拡張や，VAE による非線型化を通じて変分推論をすることも考えられる．\n自己符号化器 は，まさに非線型な潜在変数モデルに対する最尤推定を行っており，４層以上のニューラルネットワークを用いることで PCA を非線型化して一般化することができる．4\nまた，カーネル法と Gauss 過程により非線型化することもできる (Lawrence, 2005)．\n\n\n2.7 混合モデリング\n複数の線型 Gauss 因子分析モデルの重ね合わせとみなす mixture of factor analysers (Ghahramani and Hinton, 1996) も単純ながら表現が高く，EM アルゴリズムや SGD (Richardson and Weiss, 2018), (Zong et al., 2018) によって推定できる．\n(Richardson and Weiss, 2018) では生成モデルとしての性能も GAN と劣らないこと，VAE や GAN などの生成モデルよりも分布へのフィッティングが良いことを報告している．\nさらにこのアプローチはノンパラメトリックベイズ法につながる．この方法では，例えば (Paisley and Carin, 2009) では Beta 過程事前分布をおき，Gibbs サンプラーで推論することで，混合数 \\(K\\) も同時に自動で決定できる．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#sec-SEM",
    "href": "posts/2024/Kernels/HierarchicalModel.html#sec-SEM",
    "title": "階層モデル再論",
    "section": "3 構造方程式モデリング (SEM)",
    "text": "3 構造方程式モデリング (SEM)\n\n3.1 はじめに\n(K. G. Jöreskog, 1969) は因子分析モデルを潜在変数モデルとして，事前情報を取り入れるなど柔軟に用いた．\n特に，データを（現代でいう）訓練データと検証データに分けて，因子分析により推定された潜在変数間の関数関係を検定するための方法を提案し (K. G. Jöreskog and Lawley, 1968)，これを 検証的因子分析 (Confirmatory FA) と呼び，それ以前の手法に 探索的因子分析 というレトロニムを与えた．5\n最終的に，潜在変数同士により一般的な関数関係も考慮したものなど多くの潜在変数モデルが，共分散構造に基づいた非線型数値最適化を推論エンジンとして統一的に推定できることに辿り着いた．6\nこのことに加えて，潜在変数間の関数関係に適切な仮定をおくことで，因果推論・高次の因子分析・分散分析など従来考慮されなかった新たなタスクにも適用可能であることも了解された (Karl G. Jöreskog, 1978), (Bentler, 1980)．7\n現代では特徴抽出，生成，表現学習にも用いられていると思うと感慨である．\nこれを 共分散構造分析 または 構造方程式モデリング (SEM: Structural Equation Modeling) という．8 心理学の文脈では，潜在変数のことを 構成概念 (construct) と呼び，潜在変数間は無関係とした従来の因果分析モデルを 測定方程式 と呼ぶ．9\n\n\n3.2 部分最小自乗モデル (PLS)\nPLS (Partial Least Square) モデル (K. G. Jöreskog and Wold, 1982), (Gustafsson, 2001) では，次のような潜在変数モデルを用いて，２つの構成概念間の因果関係を評価しようとする (豊田秀樹, 1991)：\n\nなお，パス図において，潜在変数から観測変数に矢印が伸びている場合，これは影響的指標と呼ばれ，観測のモデルと解され，誤差が入ることが想定される (豊田秀樹, 1991)．10 逆の矢印は形成的指標という．\nすなわち，PLS では，\\(X^1,X^2,X^3,\\cdots\\) には，\\(Z^1,Z^2,\\cdots\\) とは独立な独自因子が作用していると仮定されている．\nこのような仮定は，\\(Y^1,Y^2,\\cdots\\) を被説明変数として，教師あり PCA (Yu et al., 2006) に有用である．\nというのも，被説明変数のうち必ずしも \\(Y^1,Y^2,\\cdots\\) に関係する要素が全てとは限らないために，\\(Z^1,Z^2\\) の間で間接的に回帰分析を行いたい場合に自然な設定である (Nounou et al., 2002)．\n\n\n3.3 構造方程式モデリングの発展\nPLS において，潜在変数から構成概念への矢印が全て影響的であった場合，これは潜在因子の間に関係が仮定されていることを除いて，（探索的）因子分析と等価になる．\n一般に，SEM は，潜在変数同士の関数関係も考慮した因子分析モデルだと理解できる．\nこのようなモデルは，社会学において 多重指標分析 と呼ばれていたモデルに相当し (白倉幸男, 1984) (清水和秋, 1989)，経済学において 同時方程式モデル と呼ばれていたモデルに相当する (Bentler, 1980)．11\n加えて，心理学・行動計量学においても，多くの既存の多変量解析法（因子分析，パス解析，二段階抽出モデル，潜在構造分析，項目反応モデルなど）はいずれも SEM の特殊な形だと解釈できることが自覚された (McArdle, 1984), (Muthén, 2002)．12\nこうして SEM の名と LISREL プログラムの下で，多くの社会科学分野で使われていたモデルが，形式的にはほとんど等価であるという了解が形成されていった．\nこのことから，SEM は第二世代の多変量解析 (Fornell, 1985) とも評される．13\n\n\n3.4 計算統計学という要素\n構造方程式モデリングが普及した理由の一つとして，計算機統計学の発展とうまく合流した点が見逃せない．\nそもそも Jöreskog は，因子分析を研究していた時期 (Karl G. Jöreskog, 1966) (K. G. Jöreskog, 1967) から，数値的な解法とコンピュータプログラムの開発にも重点を置いていた．特に，因子分析モデルを，DFP 法 に基づいて数値的に最尤推定する方法を提案した (K. G. Jöreskog, 1967)．\nSEM も，コンピュータプログラム LISREL (LInear Structural RELationships) (Jőreskog and Thiilo, 1972) の存在が，広い分野の人口に膾炙した要因として大きい (清水和秋, 1989), (Grimm and Yarnold, 2016)．\n構造方程式モデルがどのように因子分析，因果分析，共分散構造分析を統合し，LISREL プログラムと共に発展していたかは，(清水和秋, 1994) に大変わかりやすくまとまっている\n\n\n3.5 正準相関分析 (CCA)\n正準相関分析 (Hotelling, 1936) においては，２つの構成概念の間は相関関係で結び，すべての観測は形成的な影響を及ぼすとする（観測誤差は想定しない） (豊田秀樹, 1991)：\n\nこのモデルでは \\(X^1,X^2,X^3\\) とその潜在要因 \\(Z^1\\)，\\(X^4,X^5\\) とその潜在要因 \\(Z^2\\) とを完全に対等に扱い，その間の関係を理解しようとする．\n例えばマルチモーダル学習において，\\(X,Y\\) が類似したタスクに関するデータという場合に応用がある (岩瀬智亮 and 中山英樹, 2016)．また，PLS と共に特徴抽出にも用いられる (Sun et al., 2009)．\n複数の標本に対して同時に実行する主成分分析ともみなせるが，別々に PCA を実行した場合と違い「共通要因」を抽出することに志向がある (赤穂昭太郎, 2013)．\nなお，正準相関分析が，このような確率論的解釈ができることは (Bach and Jordan, 2005) で自覚されたことである．\nこの潜在変数モデルとしての観点から，\\(Z^3,Z^4,\\cdots\\) がある GCCA (Generalized CCA) (Horst, 1961)，指数分布族の場合 (Klami et al., 2010)，ニューラルネットワークにより非線型にした DCCA (Andrew et al., 2013)，さらに変分推論する場合 (Wang et al., 2017), (Suzuki et al., 2017) に拡張されている．\n質的データをダミーベクトルに変換して（一般化）正準相関分析を行う，質的データの解析法を 対応分析 (correspondence analysis) または 数量化第III類 ともいう．14"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#sec-MM",
    "href": "posts/2024/Kernels/HierarchicalModel.html#sec-MM",
    "title": "階層モデル再論",
    "section": "4 混合モデル (MM)",
    "text": "4 混合モデル (MM)\n\n4.1 はじめに\n混合モデルは，次のようなたいへん基本的な設定であるが，第 2.7 節で見たように，例えば因子分析モデルと組み合わせることで極めて豊かな表現力を持つ．\n\n\n\n\n\n混合モデルは SEM の別の選択肢としても使える．また，ランダム効果要因を明示的にモデルに組み込む意味で，一般線型モデルの確率論的な拡張と考えることもできる (狩野裕, 2002)．15\n\n\n4.2 正規混合モデル (GMM)\n\\(Z\\in\\mathcal{L}(\\Omega;[K])\\) は \\[\n[K]=\\{1,\\cdots,K\\}\n\\] に値を取る離散確率変数で，確率核 \\(Z\\to X\\) が \\[\np(x|z=k)\\,dx=\\mathrm{N}_p(\\mu_k,\\Sigma_k)\n\\] と表せる場合，\\(X\\) に課される仮定を 正規混合モデル (GMM: Gaussian Mixture Model) という．\n\\(Z\\sim\\mathrm{U}([K]),\\Sigma_k=I\\) の場合，これは \\(K\\)-平均クラスタリングに等価 なモデルとなる．\nこれは SGD により訓練をすることで，生成のタスクにおいても GAN に匹敵する性能も持つ (Richardson and Weiss, 2018)．\nまた，デノイジングや deblurring, inpainting, super-resolution などの画像逆問題は，巨大な GMM の潜在変数の推定として理解できる (Zoran and Weiss, 2011), (Papyan and Elad, 2016)．\n\n\n4.3 正規スケール混合モデル (GSM)\nGaussian scale mixture モデルとは， \\[\np(x|z)\\,dx=\\mathrm{N}_p(0,\\sigma_0^2z)\n\\] で定まる階層モデルである．\nこのモデルは，\\(Z\\) の分布により，種々の（特に裾の重い）分布を表せる：\n\n\n\n\n\n\n\n\\(Z\\sim\\mathrm{Ber}(\\pi)\\) のときを spike and slab 分布という： \\[\np(x)\\,dx=\\pi\\mathrm{N}(0,\\sigma_0^2)+(1-\\pi)\\delta_0.\n\\]\n\\(Z\\sim\\mathrm{C}(1)_+\\) のとき，馬蹄分布 (Carvalho et al., 2010) という．16\n\n\n\n\n\n\n4.4 潜在 Dirichlet 配分 (LDA)\n\n4.4.1 はじめに\n文書の埋め込み・数値表現を得るために，単語 \\(i\\in[M]\\) が文書 \\(j\\in[N]\\) に現れた回数をカウントした行列 \\(\\boldsymbol{C}\\in M_{MN}(\\mathbb{N})\\) を通じた主成分分析が用いることも考えられる．\nこれを 潜在意味索引 (LSI: Latent Semantic Indexing) (Deerwester et al., 1990) と呼ぶ．得られた低次元埋め込みを文書検索 (document retrieval) などに用いることもできる．\n\\(\\boldsymbol{C}\\) の列も単語とし，帯幅 \\(h&gt;0\\) を決めて，\\(h\\) 文字以内に単語 \\(i,j\\in[M]\\) が共起した回数を \\(C_{ij}\\) とすると，全く同様の手続きが，単語の埋め込みに応用できる．これを 潜在意味解析 (LSA: Latent Semantic Analysis) (Deerwester et al., 1990) と呼ぶ．\n\n\n4.4.2 確率的潜在意味索引 (PLSI)\n(Hofmann, 1999) による pLSI または aspect model は LSI を確率モデル，特に混合モデルとして解釈し直したものである．\n単語数よりも少ない数の トピック \\(Z\\) というものがあり，これが単語を決めている，というモデルを想定した．\n\n\n\n\n\nこのモデルを通じて，トピック \\(Z\\) の分布（あるいは，現代的には \\(\\Theta\\) の値）を「文書」の特徴量とする，というアイデアである．\n\n\n4.4.3 Dirichlet 事前分布の追加\n変数 \\(\\Theta\\) に Dirichlet 事前分布を追加し，完全なベイズの見方を提示したのが Latent Dirichlet Allocation (Blei et al., 2003) である．\n\\(\\Theta\\) を文書，\\(Z\\) トピック，\\(W\\) をトピックごとの語彙デッキとする．\n\n最終的に，トピック \\(Z\\) とその人の語彙 \\(W\\) が合わさって，単語 \\(X\\) が観測されるというモデルが考えられている．\n\n\n4.4.4 確率的トピックモデル\n自然言語処理において，単語分布のモデリングの潜在変数は トピック と呼ばれて，これを確率的にモデリングする手法は PTM (Probabilistic Topic Model) (Blei, 2012) と呼ばれている．\n「トピック」は短い文章の中でも激しく移り変わることが知られている (Church and Gale, 1991)．\nそのため，LDA では，\\(\\Theta\\) の事前分布と \\(W\\) の事前分布は， \\[\n\\mathrm{Dirichlet}(\\alpha\\boldsymbol{1}),\\qquad\\alpha&gt;0,\n\\] という形で，極めて小さい \\(\\alpha&gt;0\\) を設定し，特定のトピックがどの文書に現れるかは極めてスパースになるようにモデリングをする．\n\n\n4.4.5 推論\nLDA の推論手法には変分推論 (Blei et al., 2003) や Gibbs サンプリング (T. L. Griffiths and Steyvers, 2004)，そしてスペクトルに基づく方法 (Arora et al., 2013) がある．\nトピック数の決定には，尤度を 焼なまし重点サンプリング で計算する方法 (Wallach et al., 2009) の他，ノンパラメトリックベイズ法も用いられる (Yee Whye Teh and Blei, 2006)．\n\n\n4.4.6 時系列化\n単語の並びは明らかな方向性があり，対照的なモデリングはこの消息を取り逃がしていると考えられる．\nそこで，トピックの移り変わりを捉えるモデルとして dynamic topic model (Blei and Lafferty, 2006) がある．これは Kalman 平滑化と変分推論を組み合わせている様である．\nまた単語の時系列構造を捉えるために，LDA に隠れ Markov モデルを組み合わせた LDA-HMM (T. Griffiths et al., 2004) が提案された．TopicRNN (Dieng et al., 2017) ではより長距離の相関を捉えるために，RNN と組み合わせている．\n\n\n\n4.5 状態空間モデル (SSM)\n\n4.5.1 概要\n状態空間モデル (State Space Model) は，混合モデルの時系列化と捉えられる：\n\n潜在変数 \\(X_t\\) が離散的である場合は特に 隠れ Markov モデル (HMM: Hidden Markov Model) (Baum and Petrie, 1966) と呼ばれる．\nHMM に関しては早くから EM 様の推定手法 Baum-Welch アルゴリズム (Baum and Eagon, 1967), (Baum et al., 1970) が提案されているが，データサイズが大きい場合は SGD が用いられる．Blocked Gibbs サンプラー (Scott, 2002) や，潜在変数を消去して，周辺尤度に関してスペクトル法／テンソル分解 (Hsu et al., 2012), (Animashree Anandkumar et al., 2012), (Anima Anandkumar et al., 2015), (Obermeyer et al., 2019) を実行するなどの代替手法がある．\n\n\n4.5.2 構造的状態系列モデル (S4)\nS4 (Structured State Space Sequence) (Gu et al., 2022), (Gu et al., 2020), (Goel et al., 2022) とは，時系列を深層ニューラルネットワークの力でモデリングするために，線型 Gauss で単純な SMM を上下にスタックし深層にしたものである．各層は LSSL (Linear State Space Layer) と呼ばれる．\nさらに長距離の依存性に耐えるために，S5 (Smith et al., 2023) や Mamba (Gu and Dao, 2024) が提案されている．後者では，選択的に記憶を忘却できるような「選択」機構 (S6: Selective SSM) を導入している．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#sec-ICA",
    "href": "posts/2024/Kernels/HierarchicalModel.html#sec-ICA",
    "title": "階層モデル再論",
    "section": "5 独立成分分析 (ICA)",
    "text": "5 独立成分分析 (ICA)\n\n5.1 はじめに\n（線型）独立成分分析で用いるモデルは，PCA や FA のそれと全く変わらず，線型変換 \\(x_n=Az_n\\) でデータを説明しようとする：\n\nただし，潜在変数 \\(Z^1,\\cdots,Z^r\\) は互いに 独立 であるという「真の構造」が強く想定される場合に使われる．\n加えて，モデルの 識別可能性 を重視する．このために，（独立）因子分析（第 2.5 節）で考えたように，正規分布より裾の重い事前分布を導入することで，モデルの識別可能性を確約する．17\nこの意味で，確率モデルとしては PCA / FA に等価であるが，典型的な ICA の文脈では \\(Z^1,\\cdots,Z^r\\) は非正規確率変数であり，\\(A\\) を生成荷重や混合行列，\\(A^{-1}\\) を 認識荷重 (recognition weight) などという．\n\n\n5.2 推定手法\n最初に 音源分離 について適用された (Bell and Sejnowski, 1995) では，\\(X\\) と \\(Z\\) の相互情報量の最大化が目指された．\n最尤推定は EM アルゴリズムの他に近似 Newton 法で実行されることもあり，fast ICA (Hyvärinen and Oja, 2000) と呼ばれる．\nまた古典的には，探索的データ解析で考案された 射影追跡 (PP: Projection Pursuit) (Friedman and Tukey, 1974) みたく，学習される \\(Z\\) の分布が Gauss からなるべく遠いように学習することも考えられた．\ndisentangled な表現を学習したい場面では，\\(Z\\) の成分同士の相関が最小になるように学習される； \\[\n\\operatorname{KL}\\left(\\operatorname{P}^Z,\\bigotimes_{j=1}^r\\operatorname{P}^{Z_j}\\right).\n\\]\n最小情報コピュラに基づく方法も提案されている (Bedford et al., 2016), (Sei and Yano, 2024)．\n他にも表現学習や認知科学の文脈を踏襲して，InfoMax やスパース符号化などの原則がある．\n\n\n5.3 非線型化\n非線型独立成分分析は，表現学習の文脈でも研究されている．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#おわりに",
    "href": "posts/2024/Kernels/HierarchicalModel.html#おわりに",
    "title": "階層モデル再論",
    "section": "おわりに",
    "text": "おわりに\n現代の深層生成モデルは，いずれも非線型な潜在変数モデルであると理解できる．\nその意味で，次の記事は全て，本稿の続きであり，本稿は現代の機械学習の壮大な序章としても理解できる．\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n非線形性の他に本稿で扱わなかったものは深層モデルである．\nだがそもそも，現代のニューラルネットワークが深層化したのは，単純で可微分なモジュール性を保ちながら表現力を高めるためのトリックであり，確率論的には本稿で扱ったモデルと等価であるはずである．\nニューラルネットワークの他にも，計算のために深層化したモデルを考える場面は多い．例えばアニーリングを用いた SMC サンプラー は，グラフカルモデル \\(Z\\to X\\) の潜在変数 \\(Z\\) の推定を，人工的に時系列構造を見出して状態空間モデル 4.5 にあてはめてサンプリングしやすくする方法と言える．\nしかし，確率核は射をなすのだから，全てのモデルは本質的には一層であるとみなすこともできるのである．\nこの見方をとった方が計算効率が上がるという例もある．例えば (Chen et al., 2024) では，トランスフォーマーの注意機構をランダム Fourier 特徴写像で近似し，Monte Carlo 法によって元のモデルと等価な計算を安価に行っている．\nベイズ機械学習 や 位相的機械学習 をはじめとした，丁寧なモデルへの理解が，これからも手法への統一した視点からの理解と，応用分野を横断した相互理解を促進してくれるのではないかと，筆者は意気込んでいる．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#扱ったモデル一覧",
    "href": "posts/2024/Kernels/HierarchicalModel.html#扱ったモデル一覧",
    "title": "階層モデル再論",
    "section": "扱ったモデル一覧",
    "text": "扱ったモデル一覧\n\n\n\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\n\n\nFA\n\n\n\n\n\n\n\n\n\nPLS\n\n\n\n\n\n\n\nCCA\n\n\n\n\n\n\n\n\n\nPLSI\n\n\n\n\n\n\n\nLDA\n\n\n\n\n\n\n\n\n\nSSM\n\n\n\n\n\n\n\nICA\n\n\n\n\n\n\n\n\n(Murphy, 2023, p. 920) より，本稿で扱ったモデルのいくつかを含んだ，数式による一覧表．すでに図式による解説を受けた後だと，より見やすいだろう．\\(\\operatorname{Cat}(c|\\boldsymbol{\\pi})\\) は確率ベクトル \\(\\boldsymbol{\\pi}\\) が定める質量関数を表す．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#付録",
    "href": "posts/2024/Kernels/HierarchicalModel.html#付録",
    "title": "階層モデル再論",
    "section": "付録",
    "text": "付録\n\nここでは，歴史を感じる引用をいくつか紹介したい．\n\n心理測定学 (psychometrics) における因子分析，計量経済学 (econometrics) における同時方程式モデル (simultaneous equation models), そして生物測定学 (biometrics) におけるパス解析 (path analysis) を，共分散構造分析の下に統一化することが可能となった契機は，潜在変数 (latent variables) の概念である (Bentler, 1980)．(清水和秋, 1989)\n\nそして，異分野横断の知見交流が進んだ契機の一つは，LISREL プログラムの存在であった．(清水和秋, 1994) では，ETS での安定した研究環境が LISREL の継続的な保守を可能にして最終的には WINDOWS 上でも安定して提供され，これを用いることを通じて異分野を巻き込みながら構造方程式モデリングが発展していった様子が詳細に解説されている．LISREL はバージョン VI まである．\n\n紹介した文献からもわかるように，この分野は最近になってやっと日本では注目されてようになってきた。 このように日本へのこの方法論の導入が遅れた理由の一つはソフト流通の問題にあると筆者は考えている。青木 (1988) や土田 (1988) が述べているように， LISREL は大型計算機の場合， アメリカ産のコンビュータでしかサポートしてくれないとのことである。(清水和秋, 1989)\n\nそして現代はというと，計算機統計学と機械学習が先行し（過ぎ）ていると思える．\nもしその通りならば，種々の科学への応用とそれぞれ固有の課題への特殊化が，これからの未来を彩ってくれるのかもしれない．"
  },
  {
    "objectID": "posts/2024/Kernels/HierarchicalModel.html#footnotes",
    "href": "posts/2024/Kernels/HierarchicalModel.html#footnotes",
    "title": "階層モデル再論",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n図を見やすくするために，\\(X^1\\to X^{p-1}\\) や \\(X^2\\to X^p\\) などは省略している．↩︎\n(足立浩平 and 山本倫生, 2024), (足立浩平, 2023) によると，この行列分解による定式化は Henk A. K. Kiers によるもので，初出は同大学からの博士論文 (Socan, 2003) が最初ではないか，とのこと．この見方を MDFA (Matrix Decomposition Factor Analysis) と呼ぶ．(足立浩平 et al., 2019) も参照．↩︎\nただし，(星野崇宏 et al., 2005) は SEM をより一般的とし，共分散構造分析とは観測変数が連続な場合の下位モデルである，と解している．↩︎\n(江口真透, 1999) 第３節に，PCA をニューラルネットワークにより近似的に実行する方法が紹介されている．(Ghojogh et al., 2022) はサーベイを与えている．↩︎\n(豊田秀樹, 1992) では CFA を確認的因子分析と呼んでいる．(豊田秀樹, 1991) では，古典テスト理論を確認的因子分析の下位モデルとして紹介している．また，このような因果関係の確認的方法は，社会学における (Simon, 1957) の基準などが知られていた．↩︎\n(Karl Gustav Jöreskog, 1970) は具体的なモデルを例に取り，彼の検証的因果分析が，パス解析 (Wright, 1918), (Wright, 1921) のように因果分析に応用できることを示した結果だと言える (Asher, 1983)．この観点から，パス解析は「検証的因果推論」と表現することもできる (甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 73)．↩︎\n(狩野裕, 2002) は SEM の射程と得意・不得意を分析している．↩︎\n現代ではコンピュータの力により，新たに「生成」「表現学習」というタスクが加わったと思うと，感慨深い．↩︎\n(清水和秋, 1989), (豊田秀樹, 1992), (甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 82) も参照．↩︎\nまた，パス図では観測変数は四角で囲むべきであるが，ここでは省略した．↩︎\n同時方程式は潜在変数を持たない模型で，経済学におけるパス解析の継承と見れる (豊田秀樹, 2007)．特に Keynes 経済学におけるマクロな経済計画の発想で，Cowles 委員会 により 1940 年代から 1950 年代にかけて盛んに研究された．↩︎\n「従来から存在するがやや標準的でない分析方法がSEMの枠組みで実行できることも指摘しておきたい．たとえば，三相データの分析モデルである PARAFAC，行動遺伝学における ACE モデル，イプサティブデータの分析，潜在曲線モデル，潜在構造分析などの離散潜在変数のモデル，項目反応モデルなどである．加えて，SEM で実行できる新しいモデル，たとえば，多変量二段抽出モデル，平均に特色をもたせる三相データの分析モデルや因子分析と分散分析の統合モデルなどがある．」(狩野裕, 2002, p. 139)．↩︎\n多変量解析の高級言語とか形容することもあるという．構造方程式モデリングについては，(豊田秀樹, 1991), (狩野裕, 2002) も参照．↩︎\nオランダ学派を中心に等質性分析とも呼ぶ．↩︎\nただし，SEM は共分散構造，混合モデルは平均構造に分析の焦点がある，という志向の違いもある．(狩野裕, 2002) も参照．↩︎\n\\(\\mathrm{C}(\\sigma)_+\\) は Cauchy 分布 \\(\\mathrm{C}(0,\\sigma)\\) を \\(\\mathbb{R}_+\\) 上に制限したものである．truncated Cauchy または half-Cauchy という．↩︎\n(Hyvärinen and Oja, 2000) では，(Bell and Sejnowski, 1995) のように測定誤差を考えない場合を ICA といい，誤差も入る一般の場合を IFA (Independent Factor Analysis) と呼び分けている．(甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 110) も参照．「これを回転の不定性という．因子分析はさまざまな考察によって，この不定性を解消しようとする．独立成分分析は，非正規性を仮定すれば，この不定性が消えることを示したものとも言える」(甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 13)．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#項目応答モデル",
    "href": "posts/2024/Survey/BayesGLM.html#項目応答モデル",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "3 項目応答モデル",
    "text": "3 項目応答モデル\n\n3.1 データの概観\n(Vansteelandt, 2001), (Boeck and Wilson, 2004) による「怒るかどうか？」のデータ VerbAgg を用いる．混合モデルの点推定のためのパッケージ lme4 (Bates et al., 2015) で利用可能になっている．\n\nlibrary(lme4)\ndata(\"VerbAgg\", package = \"lme4\")\ndf &lt;- VerbAgg\n\n質問票は「自分が意思表示をしたのにバスが止まってくれなかったので悪態をついた」などのもので，同意できるかを３段階 “yes”, “perhaps”, “no” で評価する (Boeck and Wilson, 2004, pp. 7–8)．\n応答は３段階の順序応答 resp とこれを２段階にしたもの r2 である．\n\nkable(head(df))\n\n\n\n\nAnger\nGender\nitem\nresp\nid\nbtype\nsitu\nmode\nr2\n\n\n\n\n20\nM\nS1WantCurse\nno\n1\ncurse\nother\nwant\nN\n\n\n11\nM\nS1WantCurse\nno\n2\ncurse\nother\nwant\nN\n\n\n17\nF\nS1WantCurse\nperhaps\n3\ncurse\nother\nwant\nY\n\n\n21\nF\nS1WantCurse\nperhaps\n4\ncurse\nother\nwant\nY\n\n\n17\nF\nS1WantCurse\nperhaps\n5\ncurse\nother\nwant\nY\n\n\n21\nF\nS1WantCurse\nyes\n6\ncurse\nother\nwant\nY\n\n\n\n\n\n\n\n3.2 固定効果１母数モデル\n通常の１母数モデルに，過分散を説明するための固定効果の項 \\(\\alpha_0\\) を加えたモデルを考える：\n\\[\ng(\\operatorname{P}[Y_{ik}=1])=\\alpha_{j[i]}-\\beta_{k[i]}+\\alpha_0,\\qquad\\alpha_0\\sim\\mathrm{t}(3;0,2.5),\n\\] \\[\n\\alpha_j\\sim\\mathrm{N}(\\mu_\\alpha,\\sigma_\\alpha^2),\\quad\\mu_\\alpha\\sim\\mathrm{N}(0,3),\\quad\\sigma_\\alpha\\sim\\mathrm{N}(0,3),\n\\] \\[\n\\beta_k\\sim\\mathrm{N}(\\mu_\\beta,\\sigma_\\beta^2),\\quad\\mu_\\beta\\sim\\mathrm{N}(0,3),\\quad\\sigma_\\beta\\sim\\mathrm{N}(0,3).\n\\]\nsd というクラスはグループレベル変数の標準偏差を意味する．\n\\(\\alpha_j,\\beta_k\\) の定数の違いに関する識別不可能性は，いずれも \\(0\\) を中心とした\n\nformula_1PL &lt;- bf(r2 ~ 1 + (1|item) + (1|id))\nprior_1PL &lt;-  prior(\"normal(0,3)\", class=\"sd\", group = \"id\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\")\nfit_1PL &lt;- brm(\n  formula_1PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nprior_summary(fit_1PL)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n          normal(0,3)        sd              id                  0   \n          normal(0,3)        sd Intercept    id                  0   \n          normal(0,3)        sd            item                  0   \n          normal(0,3)        sd Intercept  item                  0   \n       source\n      default\n      default\n         user\n (vectorized)\n         user\n (vectorized)\n\n\nvectorized というのは，下記 Stan コード内で尤度は for 文で構成されるが，このループに入れなくて良いものがある場合をいう．\n\n\n\n\n\n\nStan コードの表示\n\n\n\n\n\nstancode(fit_1PL)\nによって推定に用いられた Stan コードが表示できる．\n次を見る限り，確かに意図したモデルになっている：\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += normal_lpdf(sd_1 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n  lprior += normal_lpdf(sd_2 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n-1*normal_lccdf(0|0,3) というのは定数であり，推定には全く影響を与えないが，後続の bridgesampling パッケージ (Gronau et al., 2020) によるモデル比較の API 構築のために付けられたものである (Bürkner, 2021, p. 21)．\n\n\n\n\n\nsummary(fit_1PL)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ 1 + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.39      0.07     1.26     1.53 1.00     1038     1802\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.22      0.19     0.91     1.65 1.01      679     1512\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.17      0.27    -0.70     0.34 1.01      431      814\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n低い ESS から変動効果の項 \\(\\epsilon_i\\) の推定に苦労していることがわかる．\n\nplot(fit_1PL)\n\n\n\n\n\n\n\n\nここにはグローバルなパラメータしか表示されておらず，ランダム効果の結果は次のように見る必要がある：\n\nlibrary(ggplot2)\nranef_item &lt;- ranef(fit_1PL)$item\nposterior_means &lt;- ranef_item[,1,1]\nlower_bounds &lt;- ranef_item[,3,1]\nupper_bounds &lt;- ranef_item[,4,1]\nplot_df_item &lt;- data.frame(\n  item = rownames(ranef_item),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n\n\np_PL1 &lt;- ggplot(plot_df_item, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Items\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\np_PL1\n\n\n\n\n\n\n\n\n多くの参加者にとって腹立たしい例とそうでない例が区別できているようである．\n\nplot_df_id &lt;- plot_df_id %&gt;% arrange(mean) %&gt;% mutate(rank = row_number())\np_PL1_id &lt;- ggplot(plot_df_id, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Individuals\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\np_PL1_id\n\n\n\n\n\n\n\n\nこうして怒りやすかった人を並べることができる．\nしかしガタガタしている区分定数的な模様が見れる．実はこれは item の分だけある．というのも，「何個の項目に Yes と答えたか」だけが \\(\\alpha_j\\) を決める要因になってしまっているためである．\nこれが項目識別のできない１母数モデルの限界である．\n\n\n3.3 固定効果２母数モデル\n項目識別力母数 \\(\\gamma_k\\) を導入する： \\[\ng(\\mu_i)=\\gamma_{k[i]}\\biggr(\\alpha_{j[i]}-\\beta_{k[i]}\\biggl),\n\\]\nすると追加の制約が必要になる．ここでは理想点モデルの場合と違い，研究のデザインから \\(\\gamma_{k[i]}\\) は正として良いだろう．\nこれを変数変換 \\(\\gamma_k=\\exp(\\log\\gamma_k)\\) によってモデルに知らせることとする．\n\nformula_2PL &lt;- bf(\n  r2 ~ exp(loggamma) * eta,\n  loggamma ~ 1 + (1|i|item),\n  eta ~ 1 + (1|i|item) + (1|id),\n  nl = TRUE\n)\n\n\\(g(\\mu_i)\\) の右辺はもはや \\(\\log\\gamma_k\\) の線型関数ではないので，これを nl=TRUE によって知らせる必要がある．\n|i| によって，\\(\\log\\gamma_k\\) と \\(\\eta_{jk}\\) 内の項 \\(\\beta_k\\) には相関があることを知らせている (Bürkner, 2018, p. 397)．項目難易度 \\(\\beta_k\\) が低いほど識別力 \\(\\log\\gamma_k\\) は低いとしているのである．\n\nprior_2PL &lt;-  prior(\"normal(0,5)\", class=\"b\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"b\", nlpar = \"loggamma\") +\n  prior(\"constant(1)\", class=\"sd\", group = \"id\", nlpar = \"eta\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"sd\", group = \"item\", nlpar = \"loggamma\")\n\nfit_2PL &lt;- brm(\n  formula = formula_2PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_2PL,\n  chains = 4, cores = 4\n)\n\nついに Stan が２分ほどかかるようになった上に，収束に苦労しており，ESS が低くなっている．\n\nsummary(fit_2PL)\n\nWarning: There were 1 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ exp(loggamma) * eta \n         loggamma ~ 1 + (1 | i | item)\n         eta ~ 1 + (1 | i | item) + (1 | id)\n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~item (Number of levels: 24) \n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(loggamma_Intercept)                    0.12      0.06     0.01     0.24 1.01\nsd(eta_Intercept)                         0.92      0.15     0.67     1.26 1.01\ncor(loggamma_Intercept,eta_Intercept)     0.36      0.34    -0.37     0.93 1.01\n                                      Bulk_ESS Tail_ESS\nsd(loggamma_Intercept)                     666      818\nsd(eta_Intercept)                         1234     2191\ncor(loggamma_Intercept,eta_Intercept)      317      445\n\n~id (Number of levels: 316) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(eta_Intercept)     1.00      0.00     1.00     1.00   NA       NA       NA\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nloggamma_Intercept     0.32      0.06     0.21     0.43 1.00     1251     2531\neta_Intercept         -0.13      0.20    -0.52     0.27 1.00     1410     2231\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nranef_item2 &lt;- ranef(fit_2PL)$item\nposterior_means &lt;- ranef_item2[,1,\"eta_Intercept\"]\nlower_bounds &lt;- ranef_item2[,3,\"eta_Intercept\"]\nupper_bounds &lt;- ranef_item2[,4,\"eta_Intercept\"]\nplot_df_item2 &lt;- data.frame(\n  item = rownames(ranef_item2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_PL2 &lt;- ggplot(plot_df_item2, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\ngrid.arrange(p_PL1, p_PL2, nrow = 1)\n\n\n\n\n\n\n\n\n識別力パラメータ \\(\\gamma_k\\) が \\(1\\) より大きい値をとっており，これが変動を吸収しているため，\\(\\alpha_j\\) は \\(0\\) に縮小されて推定されるようになっている．\n\nranef_id2 &lt;- ranef(fit_2PL)$id\nposterior_means &lt;- ranef_id2[,1,\"eta_Intercept\"]\nlower_bounds &lt;- ranef_id2[,3,\"eta_Intercept\"]\nupper_bounds &lt;- ranef_id2[,4,\"eta_Intercept\"]\nplot_df_id2 &lt;- data.frame(\n  id = rownames(ranef_id2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n\n\nplot_df_id2 &lt;- plot_df_id2 %&gt;% arrange(mean) %&gt;% mutate(rank = row_number())\np_PL2_id &lt;- ggplot(plot_df_id2, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\ngrid.arrange(p_PL1_id, p_PL2_id, nrow = 1)\n\n\n\n\n\n\n\n\n少し滑らかになっている．\n\ncor(ranef_id[,1,\"Intercept\"], ranef_id2[,1,\"eta_Intercept\"])\n\n[1] 0.999506\n\n\nしかし線型の相関になっており，軟化以上の変化は導入されなかったことがわかる．\nそれもそうである．モデルの表現力はあげたから解像度は高くなったが，モデルに新しい情報を入れたわけではないのである．\n\n\n3.4 共変量の追加\n理想点モデルなど多くの項目応答モデルは，\\(\\alpha_j,\\beta_k\\) の推定に終始してきたが，本当のリサーチクエスチョンはその先にある．\n個人レベルの共変量を追加した階層モデルを構築して，\\(\\alpha_j\\) の位置や応答の傾向への影響を調べることが真の目標であった．\n\n3.4.1 項目共変量の追加\n本データにおいて項目は \\(2\\times2\\times3\\) の split-plot デザインがなされている．\nmode とは「悪態をつきたい」と「咄嗟についてしまう」という２種の行動を区別するためのものである．この２つの行動容態は，本人の抑制的な意識が実際に働いたかどうかにおいて全く質的に異なる．モデルにこれを教えたらどうなるだろうか？\nsitu とはシチュエーションであり，自分に責任があるか（「店に入ろうとした瞬間閉店時間になった」など）他人に責任があるか（「バスが止まってくれなかった」など）の２項目がある．\nbtype は行動様式であり，「悪態をつく」「叱る」「怒鳴りつける」の３項目がある．後に行くほど他人への攻撃性が強い．\n最初に考えられるモデル\nr2 ~ btype + situ + mode + (1|item) + (1 + mode|id)\nは，元々の１母数モデルに変動切片項を３つ追加した上に，mode の係数を個人ごとに変えることを許したものである．これは mode の効果が個人ごとに異なるだろうという信念による．\nしかしこのモデルに至る前に，1 を 0 にすることで modedo と modewant 双方の標準偏差を推定することを考える（1 の場合は modewant の標準偏差の代わりに Intercept の標準偏差を推定する）．\n\nformula_1PL_cov &lt;- bf(\n  r2 ~ btype + situ + mode + (1|item) + (0 + mode|id)\n)\nfit_1PL_cov &lt;- brm(\n  formula = formula_1PL_cov,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL_cov)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ btype + situ + mode + (1 | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.47      0.09     1.30     1.65 1.00     1738\nsd(modedo)               1.67      0.10     1.49     1.87 1.00     1546\ncor(modewant,modedo)     0.77      0.04     0.69     0.84 1.00     1559\n                     Tail_ESS\nsd(modewant)             2865\nsd(modedo)               2608\ncor(modewant,modedo)     2767\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.46      0.09     0.31     0.67 1.00     1412     2136\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.88      0.25     1.41     2.39 1.00     1810     2154\nbtypescold    -1.13      0.24    -1.61    -0.65 1.00     1642     2241\nbtypeshout    -2.23      0.25    -2.70    -1.73 1.00     1865     2376\nsituself      -1.11      0.20    -1.52    -0.72 1.00     2064     1753\nmodedo        -0.78      0.21    -1.19    -0.37 1.00     2011     2208\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nmodedo の係数が負になっており，悪態をつきたくなっても，実際にする人の割合は下がることがわかる．\nだが係数の -0.77 が大きいかどうかがわからない．これには対数オッズ比のスケールから元のスケールに戻す便利な関数がある：\n\nconditional_effects(fit_1PL_cov, \"mode\")\n\n\n\n\n\n\n\n\n確率としての減少は軽微だがあることがわかる．次に気づくことは do の方がエラーバーが長いことである．２つの係数は相関しているので，頻度論的な検定は難しいかもしれないが，２つの標準偏差の差の事後分布を見ることでチェックすることができる：\n\nhyp &lt;- \"modedo - modewant &gt; 0\"\nhypothesis(fit_1PL_cov, hyp, class = \"sd\", group = \"id\")\n\nHypothesis Tests for class sd_id:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (modedo-modewant) &gt; 0      0.2      0.11     0.02     0.39      29.08\n  Post.Prob Star\n1      0.97    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n0.96 の確率で modedo の標準偏差の方が大きいことがわかるが，その差も 0.2 ほどで，対数オッズ比としては大したことがないと思われる．\n\n\n3.4.2 個人共変量の追加\nTrait Anger スコア (Spielberger, 2010) が個人ごとに算出されており（Anger 変数），そのスコアによってどのように項目への反応が違うかを調べる．こうするとどんどん心理学の研究っぽくなる．\n\nformula_1PL_cov_id &lt;- bf(\n  r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0+Gender|item) + (0+mode|id)\n)\nfit_1PL_cov_id &lt;- brm(\n  formula = formula_1PL_cov_id,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL_cov_id)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0 + Gender | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.49      0.09     1.33     1.67 1.00     1974\nsd(modedo)               1.58      0.10     1.40     1.77 1.00     1897\ncor(modewant,modedo)     0.78      0.04     0.70     0.85 1.00     1651\n                     Tail_ESS\nsd(modewant)             2484\nsd(modedo)               3087\ncor(modewant,modedo)     2026\n\n~item (Number of levels: 24) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(GenderF)              0.53      0.11     0.35     0.79 1.00     1371\nsd(GenderM)              0.34      0.11     0.16     0.59 1.00     1673\ncor(GenderF,GenderM)     0.78      0.18     0.34     0.99 1.00     2276\n                     Tail_ESS\nsd(GenderF)              1847\nsd(GenderM)              2495\ncor(GenderF,GenderM)     2384\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          0.71      0.44    -0.18     1.58 1.00     1464     2030\nAnger              0.06      0.02     0.02     0.10 1.00     1286     2052\nGenderM           -0.10      0.24    -0.58     0.38 1.00     1400     2128\nbtypescold        -1.02      0.22    -1.45    -0.60 1.00     1909     2251\nbtypeshout        -2.42      0.24    -2.91    -1.94 1.00     1862     2370\nsituself          -1.03      0.18    -1.39    -0.69 1.00     2167     2630\nmodedo            -0.98      0.24    -1.45    -0.51 1.00     1620     2560\nGenderM:modedo     0.89      0.24     0.43     1.38 1.00     2735     3081\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(fit_1PL_cov_id, c(\"Anger\", \"mode:Gender\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnger の値が大きいほど悪態をつく確率が綺麗に上がっていく様子がわかる．\n加えて，女性の方が悪態を吐こうと思っても，実際に行動に移すには大きな壁があることがわかる．こうして mode と Gender の間の交絡が陽の下に明らかになった．\nこのような，項目共変量と個人共変量の間の交絡は 特異項目機能 (DIF: Differential Item Functioning) と呼ばれる．項目の特性が，被験者のグループによって違った機能を示すことは，例えばテスト理論では個人の潜在特性を推定する際の重大なノイズ要因となっており，これを統制することが重要な課題になる．\n\n\n\n3.5 特異項目機能の解析\nこの特異項目機能を，項目の特性ごとにさらに詳しく見ていく．\n特に怒鳴りつける行動様式を除き，悪態をつく行為と叱る行為は，男性と女性において違う機能を持っているのではないか？という仮説を検証してみる．\n女性が実際に悪態をつく／叱る行為にだけマークをつけるダミー変数 dif を用意する：\n\ndf$dif &lt;- as.numeric(with(\n  df,\n  Gender == \"F\" & mode == \"do\" & btype %in% c(\"curse\", \"scold\")\n))\n\n\nformula_1PL_dif &lt;- bf(\n  r2 ~ Gender + dif + (1|item) + (1|id)\n)\nfit_1PL_dif &lt;- brm(\n  formula = formula_1PL_dif,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL_dif)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Gender + dif + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.40      0.07     1.26     1.55 1.00      972     1650\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.35      0.22     1.00     1.85 1.01      607     1267\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.07      0.28    -0.50     0.63 1.02      304      599\nGenderM      -0.01      0.20    -0.40     0.39 1.01      565     1240\ndif          -0.95      0.14    -1.22    -0.67 1.00     2893     2826\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\ndif の係数 -0.94 を見ることで，殊に「女性」と「実際に悪態を吐いたり叱ったりする」という組み合わせは特異な項目機能を持っていることがわかる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#文献案内",
    "href": "posts/2024/Survey/BayesGLM.html#文献案内",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "4 文献案内",
    "text": "4 文献案内\n\n(Bürkner, 2021) に項目応答モデルのベイズ的な扱いが取り上げられている．特にパッケージ brms を用いた例が３つある．\nDIF に関する日本語文献に (龍一, 2012) がある．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#文献紹介",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#文献紹介",
    "title": "ベイズ変数選択",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n変数選択のための事前分布とその \\(R^2\\) 上に定める事前分布については (12.7 節 Gelman et al., 2020) で丁寧に議論されている．\nこの第一のアプローチ・縮小事前分布については (Bhadra et al., 2019) のレビューがある．他には (Jim E. Griffin and Brown, 2021), (Jim E. Griffin and Brown, 2017), (Hahn and Carvalho, 2015) が詳しい．\n(George and McCulloch, 1993) による変数選択法が (Chapter 9 Hoff, 2009) で取り上げられている．\nベイズ変数選択手法の概観は (X. Liang et al., 2023) や (Jim E. Griffin, 2024) のイントロに圧倒されるほどまとまっている．(Jim E. Griffin, 2024) ではモデルの空間上に得られた事後分布の情報を効果的に表示するための「信用区間」の構成法を提案している．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brms-リンク集",
    "href": "posts/2024/Computation/brms.html#brms-リンク集",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "brms リンク集",
    "text": "brms リンク集\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nbrms: Bayesian Regression Models using ‘Stan’ リンク集\n\n\n\n\nr-project\nDocumentation\nGitHub\ndiscourse\n(Bürkner, 2017), (Bürkner, 2018), (Bürkner, 2021)\n\n\n\nダウンロードは：\ninstall.packages(\"brms\")"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans.html",
    "href": "posts/2024/TransDimensionalModels/Trans.html",
    "title": "超次元 MCMC",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans.html#関連ページ",
    "href": "posts/2024/TransDimensionalModels/Trans.html#関連ページ",
    "title": "超次元 MCMC",
    "section": "関連ページ",
    "text": "関連ページ\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/Trans.html#はじめに",
    "title": "超次元 MCMC",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 問題設定\n本稿では，状態空間が\n\\[\nE=\\bigcup_{k\\in[K]}E_k,\\qquad E_k:=\\{k\\}\\times\\mathbb{R}^{n_k},\n\\tag{1}\\]\nにより定義される場合を考える．\n\\(E_k=\\{k\\}\\times U_k\\;(U_k\\subset\\mathbb{R}^{n_k})\\) という一般の場合も簡単な修正により議論可能だろうから，まずは (1) に集中する．\nこのような設定はベイズ統計においてモデル選択や大きな階層モデルを考える場合に自然に現れ，通常のパラメータ推定よりも困難な設定を与える．\n\n\n1.2 例"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans.html#可逆な方法",
    "href": "posts/2024/TransDimensionalModels/Trans.html#可逆な方法",
    "title": "超次元 MCMC",
    "section": "2 可逆な方法",
    "text": "2 可逆な方法\n\n2.1 はじめに\n本稿ではまず，詳細釣り合い条件を満たすような MCMC のみに焦点を絞って，次元を超える MCMC をどのように構成できるかを議論する．\nこのような議論は，MCMC の黎明期である (Green, 1995) などで議論されたものである．\n\n\n2.2 一般論\n目標分布 \\(\\pi\\in\\mathcal{P}(E)\\) に対して提案核が \\[\nq(x,-)=\\sum_{m=1}^Mq_m(x,-)\n\\] という形で用意されている場合に，棄却率 \\(\\alpha_m(x,x')\\) をどう設定すれば所望の Metropolis-Hastings サンプラーが得られるかを考える．\n\n\n\n\n\n\n(Green, 1995, p. 715)\n\n\n\n\\(\\pi\\otimes q_m\\) がある \\(E^2\\) 上の対称な測度に関して密度 \\(f_m\\) を持つとする．このとき， \\[\n\\alpha_m(x,x')f_m(x,x')=\\alpha_m(x',x)f_m(x',x)\n\\] を満たすならば，Metropolis-Hastings サンプラーは \\(\\pi\\) を不変分布にもつ．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans.html#文献案内",
    "href": "posts/2024/TransDimensionalModels/Trans.html#文献案内",
    "title": "超次元 MCMC",
    "section": "3 文献案内",
    "text": "3 文献案内\n\nTrans-dimensional MCMC のトピックは，MCMC の黎明期の歴史に深く関わっている．\n(Besag and Green, 1993)"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans1.html",
    "href": "posts/2024/TransDimensionalModels/Trans1.html",
    "title": "超次元 Zig-Zag サンプラー",
    "section": "",
    "text": "理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html",
    "href": "posts/2024/Kernels/Manifold.html",
    "title": "非線型な次元縮約法の概観",
    "section": "",
    "text": "多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半正定値カーネルから距離学習まで\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#多様体仮説",
    "href": "posts/2024/Kernels/Manifold.html#多様体仮説",
    "title": "非線型な次元縮約法の概観",
    "section": "1.1 多様体仮説",
    "text": "1.1 多様体仮説\n「多様体」の名前の由来は，「高次元データは低次元の部分多様体としての構造を持つ」という 多様体仮説 (Fefferman et al., 2016) である．1\n特に多様体学習と呼ばれる際は，知識発見やデータ可視化を重視する志向がある．\n一方で自己符号化器などによる表現学習では，種々の下流タスクに有用な表現を得るための分布外汎化が重視される，と言えるだろう．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#距離学習との関係",
    "href": "posts/2024/Kernels/Manifold.html#距離学習との関係",
    "title": "非線型な次元縮約法の概観",
    "section": "1.2 距離学習との関係",
    "text": "1.2 距離学習との関係\n近年，対照学習による表現学習が注目されている．このアプローチには，目的関数にサンプル間の類似度に関する事前情報を含めやすいという美点がある．\nこのような表現学習法は 距離学習 (metric learning) とも呼ばれている．\n多くの多様体学習手法や，\\(\\mathbb{R}^d\\) への埋め込み手法は，なんらかの意味でサンプル間の類似度を保存する埋め込みを求めている (Agrawal et al., 2021)．\nこの意味で，「距離学習」というキーワードは，表現学習と多様体学習の交差点を意味していると言えるだろう．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#例細胞間の類似度",
    "href": "posts/2024/Kernels/Manifold.html#例細胞間の類似度",
    "title": "非線型な次元縮約法の概観",
    "section": "1.3 例：細胞間の類似度",
    "text": "1.3 例：細胞間の類似度\n現代のシークエンサー NGS (Next Generation Sequencer) では，単一の細胞が保持している mRNA の全体 scRNA-seq (single-cell RNA sequencing) を調べることができ，このような場合は極めて高次元のデータが大量に得られることになる．\n例えば COVID-19 重症患者の末梢免疫の状態を調べるために末梢血単核細胞 PBMC2 から scRNA-seq を行った例 (Wilk et al., 2020) では，全部で \\(n=44,721\\) の細胞のデータが，\\(p=26361\\) 次元のスパースなベクトルを扱っている．3"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#概要",
    "href": "posts/2024/Kernels/Manifold.html#概要",
    "title": "非線型な次元縮約法の概観",
    "section": "2.1 概要",
    "text": "2.1 概要\n多次元尺度法 (MDS: Multi-Dimensional Scaling) (Torgerson, 1952), (Kruskal, 1964) は，元のデータの「（非）類似度」を保存したままの低次元表現を探索する手法群である．\n後続の手法はいずれも高次元データ \\(\\{y_i\\}_{i=1}^n\\subset\\mathbb{R}^p\\) を所与とするが，MDS は本質的には類似度行列 \\(D\\in M_n(\\mathbb{R})\\) が与えられていればよく，解析対象は必ずしも \\(\\mathbb{R}^p\\)-値のデータである必要はない．\n類似度行列 \\(D\\) が与えられたのちは，埋め込み \\(\\{z_i\\}_{i=1}^n\\subset\\mathbb{R}^d\\) の要素間の類似度が \\(D\\) に近くなるように埋め込みを学習する．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#pca-としての-mds",
    "href": "posts/2024/Kernels/Manifold.html#pca-としての-mds",
    "title": "非線型な次元縮約法の概観",
    "section": "2.2 PCA としての MDS",
    "text": "2.2 PCA としての MDS\n特にデータ \\(\\{x_i\\}_{i=1}^n\\subset\\mathbb{R}^p\\) 間の Euclid 距離を \\(D\\) とし，埋め込み \\(\\{z_i\\}_{i=1}^n\\subset\\mathbb{R}^d\\) の間の経験共分散が \\(D\\) に Hilbert-Schmidt ノルムに関して最小化することを (Torgerson, 1952) は考えた．\nこれは (Eckart and Young, 1936) の定理 を通じて PCA に一致する，線型な次元縮約法となる．\n(Torgerson, 1952) のアプローチは 計量多次元尺度法と呼ばれ，一般のデータに適用可能な (Kruskal, 1964) のアプローチは非計量多次元尺度法と対照される．4"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#kruskal1964-stress",
    "href": "posts/2024/Kernels/Manifold.html#kruskal1964-stress",
    "title": "非線型な次元縮約法の概観",
    "section": "2.3 (Kruskal, 1964) Stress",
    "text": "2.3 (Kruskal, 1964) Stress\n前述の通り，\\(D\\) は Euclid 距離に基づくとは限らないし，そもそもデータはベクトルなど構造化されているものでなくても良い．\nこのような一般的な場面で \\(\\{z_i\\}_{i=1}^n\\subset\\mathbb{R}^d\\) 間の類似度の \\(D\\) との「近さ」を図る尺度として，Kruskal の Stress-1 (Kruskal, 1964) がよく用いられる： \\[\n\\mathcal{L}(E):=\\sqrt{\\frac{\\sum_{i_1&lt;i_2}\\left(z_{i_1i_2}-d_{i_1i_2}\\right)^2}{\\sum_{i_1&lt;i_2}d_{i_1i_2}^2}}.\n\\]\n勾配法を用いた最適化ベースの推論手法が使えるが，この目的関数に関しては SMACOF (Scaling by Majorizing a Complementary Function) (de Leeuw, 1977) という凸最適化アルゴリズムが提案されており，scikit-learn にも実装されている．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#sammon1969-stress",
    "href": "posts/2024/Kernels/Manifold.html#sammon1969-stress",
    "title": "非線型な次元縮約法の概観",
    "section": "2.4 (Sammon, 1969) Stress",
    "text": "2.4 (Sammon, 1969) Stress\n(Sammon, 1969) は探索的データ解析の文脈から，データの「構造」をよく保つために，\\(z_{i_1i_2}\\) の値が小さい場合は小さな変化も重視してくれるようにスケーリングを調整する新たなストレス関数を提案した：\n\\[\n\\frac{1}{\\sum_{i_1&lt;i_2}z_{i_1i_2}}\\sum_{i_1&lt;i_2}\\frac{\\left(z_{i_1i_2}-d_{i_1i_2}\\right)^2}{z_{i_1i_2}}.\n\\]\n係数 \\(\\frac{1}{\\sum_{i_1&lt;i_2}z_{i_1i_2}}\\) の存在は，勾配法による推論を効率的にする．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#sec-MDU",
    "href": "posts/2024/Kernels/Manifold.html#sec-MDU",
    "title": "非線型な次元縮約法の概観",
    "section": "2.5 多次元展開法 (MDU)",
    "text": "2.5 多次元展開法 (MDU)\n多次元展開法 (MDU: Multi-Dimensional Unfolding) は，個人の選考順位データに対処するために (Coombs, 1950) が提唱した，多次元尺度法 (MDS) の拡張である (足立浩平, 2000)．\nMDU ではさらに一般的な行列 \\(D\\) を取ることができる．というのも，MDS では暗黙のうちに行と列は同一物であり，\\(D\\) の対角成分は全て一定であるが，MDU では行と列で別の対象を取ることができる．\n例えば，個体 \\(i\\) が項目 \\(j\\) をどれくらい好むか？を \\(D\\) と取り，行と列を同一の平面上に バイプロット (Gabriel, 1971), (Gower, 2004) する．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#理想点推定",
    "href": "posts/2024/Kernels/Manifold.html#理想点推定",
    "title": "非線型な次元縮約法の概観",
    "section": "2.6 理想点推定",
    "text": "2.6 理想点推定\n特に政治科学の分野で用いられる多次元展開手法であり，\\(\\mathbb{R}^d\\;(d\\le3)\\) に埋め込むことが考えられる．\nその際は \\(\\mathbb{R}^d\\) 上への観測に至るまでの階層モデル（潜在変数モデル）を立てて，全体を MCMC により推定する方法が，(Martin and Quinn, 2002) 以来中心的である．\n(Imai et al., 2016)，(三輪洋文, 2017) は変分 EM アルゴリズムにより推定している．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#力指向グラフ展開-fdl-se",
    "href": "posts/2024/Kernels/Manifold.html#力指向グラフ展開-fdl-se",
    "title": "非線型な次元縮約法の概観",
    "section": "2.7 力指向グラフ展開 (FDL / SE)",
    "text": "2.7 力指向グラフ展開 (FDL / SE)\n力学モデルによるグラフ描画法 (Force-directed layout / Spring Embedder) (Tutte, 1963), (Eades, 1984), (Kamada and Kawai, 1989) は，グラフの頂点を質点，辺をバネに見立てて，グラフを \\(\\mathbb{R}^2\\) 上に展開する方法である．\n超大規模集積回路 (VLSI) の設計問題と両輪で発展してきた (Fisk et al., 1967), (Quinn and Breuer, 1979)．\nこのグラフ埋め込み法はポテンシャルエネルギーをストレスと見立てた MDS とみなせる．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#カーネル-pca-kpca-scholkopf1998",
    "href": "posts/2024/Kernels/Manifold.html#カーネル-pca-kpca-scholkopf1998",
    "title": "非線型な次元縮約法の概観",
    "section": "4.1 カーネル PCA (kPCA) (Schölkopf et al., 1998)",
    "text": "4.1 カーネル PCA (kPCA) (Schölkopf et al., 1998)\nカーネル法の見地からは，従来の PCA は線型な核を用いた場合のカーネル主成分分析だったと相対化される．\nしかし，少なくとも RBF カーネルを用いた場合は (Weinberger et al., 2004)，次元縮約の代わりにより高次元な空間に埋め込みがちである．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#半正定値埋め込み",
    "href": "posts/2024/Kernels/Manifold.html#半正定値埋め込み",
    "title": "非線型な次元縮約法の概観",
    "section": "4.2 半正定値埋め込み",
    "text": "4.2 半正定値埋め込み\nカーネル PCA を次元縮約のために用いたものが 半正定値埋め込み (semidefinite embedding) または 最大分散展開 (MVU: Maximum Vairance Unfolding) (Weinberger et al., 2004) である．\nこれは，カーネル PCA による埋め込みの中でも，元データ \\(y\\) と埋め込み \\(z\\) の間で \\[\n\\|z_i-z_j\\|_2=\\|y_i-y_j\\|_2,\\qquad(i,j)\\in G\n\\] を \\(K\\)-近傍 \\(G\\) に関して満たすような埋め込みの中で， \\[\n\\max_{z\\in\\mathbb{R}^d}\\sum_{i,j}\\|z_i-z_j\\|_2^2\n\\] を最大にするものを求めることを考える．\n幸い，これを満たすカーネル関数は半正定値計画によって解くことができ，このカーネル関数によるカーネル PCA 法が MVU である．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#はじめに-1",
    "href": "posts/2024/Kernels/Manifold.html#はじめに-1",
    "title": "非線型な次元縮約法の概観",
    "section": "5.1 はじめに",
    "text": "5.1 はじめに\nここまでの手法は畢竟，類似度行列 \\(Y\\) に関して，kPCA は特徴空間上でのスペクトル分解，Isomap はデータのなす \\(K\\)-近傍グラフ上でのスペクトル分解を考えている．\nこれに対して，スパースなスペクトル分解を用いることで，データの局所的構造がさらに尊重できることが，局所線型埋め込み (LLE: Local Linear Embedding) (Roweis and Saul, 2000) として提案された．\nこの方法は Isomap よりデータの摂動に関して頑健であることが知られている．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#アルゴリズム",
    "href": "posts/2024/Kernels/Manifold.html#アルゴリズム",
    "title": "非線型な次元縮約法の概観",
    "section": "5.2 アルゴリズム",
    "text": "5.2 アルゴリズム\nこの方法では，データ多様体の接空間に注目する．\nまず，各点をその \\(K\\)-近傍点の線型結合で表す方法を，次のように学習する： \\[\n\\widehat{W}=\\min_{W}\\sum_{i=1}^n\\left(x_i-\\sum_{j=1}^nw_{ij}x_j\\right)^2,\\qquad\\operatorname{subject to}\\begin{cases}w_{ij}=0&x_i,x_j\\,\\text{は}\\,K\\text{-近傍でない}\\\\\\sum_{j=1}^Nw_{ij}=1&\\text{任意の}\\,i\\in[N]\\,\\text{について}\\end{cases}\n\\] こうして得た \\(\\widehat{W}\\) はスパース行列になる．この \\(W\\) を通じて，局所構造を保った低次元埋め込みを構成する： \\[\n\\widehat{Z}=\\operatorname*{argmin}_Z\\sum_{i=1}^n\\left\\|z_i-\\sum_{j=1}^n\\widehat{w}_{ij}z_j\\right\\|_2^2.\n\\]\nこの最適化問題は，\\(I_n-W\\) の 特異値分解 に帰着する．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#hessian-埋め込み-he",
    "href": "posts/2024/Kernels/Manifold.html#hessian-埋め込み-he",
    "title": "非線型な次元縮約法の概観",
    "section": "5.3 Hessian 埋め込み (HE)",
    "text": "5.3 Hessian 埋め込み (HE)\nHessian Eigenmaps (Donoho and Grimes, 2003) は微分幾何学的見方を推し進め，Hessian からの情報を取り入れた局所線型埋め込みの変種である．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#接空間配置-tsa",
    "href": "posts/2024/Kernels/Manifold.html#接空間配置-tsa",
    "title": "非線型な次元縮約法の概観",
    "section": "5.4 接空間配置 (TSA)",
    "text": "5.4 接空間配置 (TSA)\nTangent Space Alignment (Zhang and Zha, 2004) はより明示的に接空間の構造に注目する．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#概要-1",
    "href": "posts/2024/Kernels/Manifold.html#概要-1",
    "title": "非線型な次元縮約法の概観",
    "section": "7.1 概要",
    "text": "7.1 概要\nSNE (Hinton and Roweis, 2002) では，\\(K\\)-近傍グラフを用いた Isomap を，ハードな帰属からソフトな帰属へ，確率分布を用いて軟化する．\nt-SNE (van der Maaten and Hinton, 2008) では SNE が Gauss 核を用いていたところを Laplace 核（\\(t\\)-分布）を用いることで，より分散した表現を得ることを目指す．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#sec-SNE",
    "href": "posts/2024/Kernels/Manifold.html#sec-SNE",
    "title": "非線型な次元縮約法の概観",
    "section": "7.2 確率的近傍埋め込み (SNE, Hinton and Roweis, 2002)",
    "text": "7.2 確率的近傍埋め込み (SNE, Hinton and Roweis, 2002)\nハイパーパラメータ \\(\\sigma_i\\) を残して， \\[\np_{j|i}:=\\frac{\\exp\\left(-\\frac{\\lvert x_i-x_j\\rvert^2}{2\\sigma_i^2}\\right)}{\\sum_{k\\neq i}\\exp\\left(-\\frac{\\lvert x_i-x_k\\rvert^2}{2\\sigma_i^2}\\right)}\n\\] と定める．\nこの \\((p_{j|i})\\) と，潜在空間における帰属確率 \\[\nq_{j|i}:=\\frac{e^{-\\lvert z_i-z_j\\rvert^2}}{\\sum_{k\\neq i}e^{-\\lvert z_i-z_k\\rvert^2}}\n\\] と一致するように埋め込みを学習する．すなわち，訓練目標は \\[\n\\mathcal{L}:=\\sum_{i=1}^n\\operatorname{KL}(p_{-|i},q_{-|i})=\\sum_{i,j=1}^np_{j|i}\\log\\frac{p_{j|i}}{q_{j|i}}\n\\] と定める．\nこの目的関数は凸ではなく，SGD で訓練可能であるが特殊なノイズスケジュールなどのテクニックがある．5\nSNE は，ある事前分布を課したスペクトル埋め込みとも見れる (Carreira-Perpiñan, 2010)．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#目的関数の対称化",
    "href": "posts/2024/Kernels/Manifold.html#目的関数の対称化",
    "title": "非線型な次元縮約法の概観",
    "section": "7.3 目的関数の対称化",
    "text": "7.3 目的関数の対称化\n\\(p_{i|j},q_{i|j}\\) ではなく，結合分布 \\(p_{ij},q_{ij}\\) を用いることで，目的関数を対称化することができる．\nこのとき，目的関数の勾配は次のようになる： \\[\n\\nabla_{z_i}\\mathcal{L}(Z)=2\\sum_{j=1}^n(z_j-z_i)(p_{ij}-q_{ij}).\n\\]"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#t-分布の導入",
    "href": "posts/2024/Kernels/Manifold.html#t-分布の導入",
    "title": "非線型な次元縮約法の概観",
    "section": "7.4 \\(t\\)-分布の導入",
    "text": "7.4 \\(t\\)-分布の導入\nt-SNE (van der Maaten and Hinton, 2008) は \\(p_{j|i}\\) の定義に Gauss 核を用いていたところを，Cauchy 分布の密度（Poisson 核）に置き換えたものである： \\[\nq_{ij}=\\frac{\\frac{1}{1+\\lvert z_i-z_j\\rvert^2}}{\\sum_{k&lt;l}\\frac{1}{1+\\lvert z_k-z_l\\rvert^2}}.\n\\]\nこのことにより，元々遠かったデータ点を引き寄せ過ぎてしまうこと (crowding problem) を回避できる．\n勾配は \\[\n\\nabla_{z_i}\\mathcal{L}(Z)=4\\sum_{j=1}^n(z_j-z_i)(p_{ij}-q_{ij})\\frac{1}{1+\\lvert z_i-z_j\\rvert^2}\n\\] で与えられ，対称化された SNE に，\\(z_i,z_j\\) の距離に従って調整する因子が追加された形になっている．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#実装",
    "href": "posts/2024/Kernels/Manifold.html#実装",
    "title": "非線型な次元縮約法の概観",
    "section": "7.5 実装",
    "text": "7.5 実装\nt-SNE は \\(O(n^2)\\) であるが，埋め込みの次元数が \\(d=2\\) などの低次元であるとき，これを \\(O(n\\log n)\\) にまで加速する実装が知られている (Maaten, 2014)．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#敏捷な埋め込み-ee",
    "href": "posts/2024/Kernels/Manifold.html#敏捷な埋め込み-ee",
    "title": "非線型な次元縮約法の概観",
    "section": "7.6 敏捷な埋め込み (EE)",
    "text": "7.6 敏捷な埋め込み (EE)\nt-SNE はハイパーパラメータ \\(\\sigma_i^2\\) の設定に敏感である上に訓練が局所解にトラップされるなど不安定で，またデータ内のノイズに弱いことが知られている (Wattenberg et al., 2016)．\n敏捷な埋め込み (EN: Elastic Net) (Carreira-Perpiñan, 2010) という，より安定的でノイズに頑健な手法が目的関数を修正することで得られている．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#largevis",
    "href": "posts/2024/Kernels/Manifold.html#largevis",
    "title": "非線型な次元縮約法の概観",
    "section": "7.7 LargeVis",
    "text": "7.7 LargeVis\nLargeVis (Tang et al., 2016) は t-SNE の計算量を軽減させるために，\\(K\\)-近傍グラフの計算に近似手法である ランダム射影木 (Dasgupta and Freund, 2008) を導入する．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#umap",
    "href": "posts/2024/Kernels/Manifold.html#umap",
    "title": "非線型な次元縮約法の概観",
    "section": "7.8 UMAP",
    "text": "7.8 UMAP\nUMAP (Uniform Manifold Approximation and Projection) (McInnes et al., 2018) は t-SNE より高速で，大域的構造をより尊重する手法として提案された．\n\n\n\nUMAP and t-SNE applied to the Fashion MNIST dataset; tap to visit https://pair-code.github.io/understanding-umap/"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#モデルベース手法",
    "href": "posts/2024/Kernels/Manifold.html#モデルベース手法",
    "title": "非線型な次元縮約法の概観",
    "section": "7.9 モデルベース手法",
    "text": "7.9 モデルベース手法\nt-SNE, VargeVis, UMAP はいずれも確率を導入しているが，完全にモデルベースの発想をしているわけではない．\n(Saul, 2020) はこれらの手法を 潜在変数モデル として定式化し，データサイズに合わせて EM アルゴリズムをはじめとした推定手法を議論している．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#はじめに-2",
    "href": "posts/2024/Kernels/Manifold.html#はじめに-2",
    "title": "非線型な次元縮約法の概観",
    "section": "8.1 はじめに",
    "text": "8.1 はじめに\n拡散埋め込み (Diffusion Map) (Coifman et al., 2005) では，データ上に乱歩を定めることでデータ多様体の局所構造を捉える．\nこの方法は Isomap 3 でグラフを用いて測地距離を近似したよりも，頑健なアルゴリズムを与える．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#phate",
    "href": "posts/2024/Kernels/Manifold.html#phate",
    "title": "非線型な次元縮約法の概観",
    "section": "8.2 PHATE",
    "text": "8.2 PHATE\nPHATE (Heat Diffusion for Affinity-based Transition Embedding) (Moon et al., 2019) は DEMaP (Denoised Embedding Manifold Preservation) という情報理論に基づく距離を定義し，データの局所構造を捉える．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#ivis",
    "href": "posts/2024/Kernels/Manifold.html#ivis",
    "title": "非線型な次元縮約法の概観",
    "section": "9.1 IVIS",
    "text": "9.1 IVIS\nIVIS (Szubert et al., 2019) は 三つ子損失を取り入れたシャムネットワーク による距離学習に基づく次元縮約法である．"
  },
  {
    "objectID": "posts/2024/Kernels/Manifold.html#footnotes",
    "href": "posts/2024/Kernels/Manifold.html#footnotes",
    "title": "非線型な次元縮約法の概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(本武陽一, 2017) も注目．↩︎\n(Peripheral Blood Mononuclear Cell)↩︎\nなお，(Wilk et al., 2020) では最初の 50 の主成分がプロットされている．↩︎\n(岡田謙介 and 加藤淳子, 2016) 参照．大変入門に良い日本語文献である．計量的多次元尺度法は 主座標分析 (PCoA: Principal Coordinate Analysis) (Young and Householder, 1938) とも呼ばれる．↩︎\nどうやら相転移境界が近いため，アニーリングが必要？(Wu and Fischer, 2020)．(Murphy, 2022, p. 700) 20.4.10.1節も参照．↩︎\nこのアルゴリズムは Morse 理論における概念である Reeb グラフの拡張と見れる．(Oudot, 2016) のスライドや (Schnider, 2024) の講義資料も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html",
    "href": "posts/2024/Kernels/Kernel.html",
    "title": "カーネル法の概観",
    "section": "",
    "text": "実践編（回帰と分類）\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理論編\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル PCA と SVM を例として\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル平均埋め込み\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最古にして最難のタスクと多様体学習\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#関連ページ",
    "href": "posts/2024/Kernels/Kernel.html#関連ページ",
    "title": "カーネル法の概観",
    "section": "",
    "text": "実践編（回帰と分類）\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理論編\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル PCA と SVM を例として\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル平均埋め込み\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最古にして最難のタスクと多様体学習\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#はじめに",
    "href": "posts/2024/Kernels/Kernel.html#はじめに",
    "title": "カーネル法の概観",
    "section": "1.1 はじめに",
    "text": "1.1 はじめに\nカーネル法は，カーネルの選択と構成が第一歩になる．\n例えば Gauss 過程 は，平均関数と共分散関数＝正定値カーネルを定めるごとに定まる．従って Gauss 過程回帰などを実行する前には，適切な事前 Gauss 過程を定める半正定値カーネルを選ぶ必要がある．\n\n\n\n\n\n\n定義（正定値核関数）1\n\n\n\n\n\n一般に 核 とは，可測関数 \\(E,F\\) の間の写像 \\(K:E\\to\\mathcal{S}(F)\\) をいう．ただし，\\(\\mathcal{S}(F)\\) は \\(F\\) 上の符号付き測度全体の集合とする．\n特に \\(F\\) 上の確率測度の全体 \\(\\mathcal{P}(F)\\) に値を取る核を 確率核 という．\n核（関数） とは，\\(F\\) 上に自然な \\(\\sigma\\)-有限測度 \\(\\nu\\in\\mathcal{S}(F)\\) がある際に，次を満たす積分核 \\(k:E\\times F\\to\\mathbb{R}\\) をいう： \\[\nK(x,A)=\\int_A k(x,y)\\,d\\nu(y).\n\\]\n正定値核 とは，この積分核 \\(k\\) であって，さらに半正定値関数でもあるものをいう．\n以降，本稿でカーネルと言った場合，積分核となる関数 \\(k:E\\times F\\to\\mathbb{R}\\) を指す．一般のカーネルについては，確率核の稿を参照．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#定常カーネル",
    "href": "posts/2024/Kernels/Kernel.html#定常カーネル",
    "title": "カーネル法の概観",
    "section": "1.2 定常カーネル",
    "text": "1.2 定常カーネル\n距離空間 \\((T,d)\\) 上の Gauss 過程 \\((X_t)\\) が定常的である場合，共分散関数 \\[\n\\mathrm{C}(s,t):=\\operatorname{E}\\biggl[(X_s-\\operatorname{E}[X_s])(X_t-\\operatorname{E}[X_t])\\biggr],\\qquad s,t\\in T\n\\] は距離 \\(d(s,t)\\) のみの関数になる．\nこのような半正定値関数 \\(\\mathrm{C}\\) の例を \\(T=\\mathbb{R}^d\\) として挙げる．\n\n1.2.1 Poisson 核\n\n\n\n\n\n\n定義 (Poisson kernel)\n\n\n\n（\\(\\mathbb{R}^d\\) 上の）Poisson 核とは，Cauchy 分布 \\(\\mathrm{C}(0,\\ell^{-1})\\) の特性関数 \\[\nK(x,y;\\ell)=\\exp\\left(-\\frac{\\|x-y\\|_1}{\\ell}\\right)\n\\] をいう．\n\n\n\n\n1.2.2 Gauss 核\n\n\n\n\n\n\n定義 (Gaussian Radial Basis Function kernel / Squared Exponential kernel)2\n\n\n\nGauss 核（動径基底関数カーネルともいう）とは，Gauss 分布 \\(\\mathrm{N}(0,\\ell^{-2})\\) の特性関数 \\[\nK(x,y;\\ell):=\\exp\\left(-\\frac{\\lvert x-y\\rvert^2}{2\\ell^2}\\right)\n\\] をいう．3\n\n\nRadial Basis Function とは動径 \\(r=\\lvert x\\rvert\\) の関数であることをいう．RBF カーネルと言ったとき特に Gauss 核を指すことが多いが，これは混乱を招く．(Murphy, 2023) では Squared Exponential kernel の語が使われているが，ここでは Gauss 核と呼ぶ．\n\n\n1.2.3 関連度自動決定核 (ARD)\n\n\n\n\n\n\n定義 (ARD: Autonatic Relevance Determination)4\n\n\n\nGauss カーネルの Euclid ノルムを Mahalanobis ノルムに変更したもの \\[\nK(r;\\Sigma,\\sigma^2)=\\sigma^2\\exp\\left(-\\frac{r^\\top\\Sigma^{-1}r}{2}\\right)\n\\] を関連度自動決定カーネルともいう．\n\n\nそもそも関連度自動決定 (MacKay, 1994), (Neal, 1996, p. 16) またはスパースベイズ学習 (Tipping, 2001) とは，ニューラルネットワークの最初のレイヤーの荷重をスパースにするために分散不定の正規分布を事前分布として導入する，という技法である (Loeliger et al., 2016)．\n一般に出力をスパースにするためのフレームワークとしても活用され，ARD 核はその最たる例である．\n\n\n1.2.4 Matérn 核\nARD 核は軟化性能を持つため，見本道は無限回微分可能になってしまう．\nこれが不適な状況下では，Matérn 核 \\[\nK(r;\\nu,\\ell)=\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\frac{\\sqrt{2\\nu}r}{\\ell}\\right)^\\nu K_\\nu\\left(\\frac{\\sqrt{2\\nu}r}{\\ell}\\right)\n\\] などが用いられることがある．ただし，\\(K_\\nu\\) は修正 Bessel 関数とする．\n\\(\\nu\\) は滑らか度を決定し，見本道は \\(\\lfloor\\nu\\rfloor\\) 階 \\(L^2\\)-微分可能になる．\\(\\nu\\to\\infty\\) の極限で Gauss 核に収束する．\n\\(\\nu=1/2\\) の場合 \\[\nK(r;1/2,\\ell)=\\exp\\left(-\\frac{r}{\\ell}\\right)\n\\] であり，対応する Gauss 過程は Ornstein-Uhlenbeck 過程 である．\n\n\n1.2.5 定常スペクトル核\n任意の（定常な）正定値関数は，ある関数 \\(p\\) に関して \\[\nK(r)=\\int_{\\mathbb{R}^d}p(\\omega)e^{i\\omega^\\top r}\\,d\\omega\n\\tag{1}\\] と表せる．この \\(p\\) は スペクトル密度 という．\n\\(K\\) が RBF 核であるとき，\\(p\\) もそうなる： \\[\np(\\omega)=\\sqrt{2\\pi\\ell^2}\\exp\\biggr(-2\\pi^2\\omega^2\\ell^2\\biggl).\n\\]\nこの対応を用いて，スペクトル密度 \\(p\\) をデザインすることで，様々な正定値カーネルを得ることが出来る．\n例えば spectral mixture kernel (Wilson and Adams, 2013) では，スケール母数と位置母数とについて RBF 核の混合を考えることで，新たな正定値カーネルを構成する．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#非定常カーネル",
    "href": "posts/2024/Kernels/Kernel.html#非定常カーネル",
    "title": "カーネル法の概観",
    "section": "1.3 非定常カーネル",
    "text": "1.3 非定常カーネル\n環境統計学などにおいて，空間相関の仕方が時間的に変化していくという設定がよくある．\nこのような場合は，一般の２変数の半正定値カーネル関数を考えることが有用である．\n\n1.3.1 多項式核\n\\[\nK(x,y)=(x^\\top y+c)^M\n\\] は非斉次項 \\(c\\) を持つ，\\(M\\) 次の多項式核と呼ばれる．\n\n\n1.3.2 Gibbs 核\nGibbs 核 (Gibbs, 1997) は，ハイパーパラメータ \\(\\sigma,\\ell\\) を入力に依存するようにした RBF 核である： \\[\nK(x,y)=\\sigma(x)\\sigma(y)\\sqrt{\\frac{2\\ell(x)\\ell(y)}{\\ell(x)^2+\\ell(y)^2}}\\exp\\left(-\\frac{\\lvert x-y\\rvert^2}{\\ell(x)^2+\\ell(y)^2}\\right).\n\\]\nこのようにすることで，\\(\\sigma,\\ell\\) を別の Gauss 過程でモデリングし，階層モデルを考えることもできる (Heinonen et al., 2016)．\n\n\n1.3.3 スペクトル核 (Remes et al., 2017)\n正定値核は Fourier 変換を通じて，スペクトル密度によって指定することもできる（Bochner の定理）．\nこの手法は，非定常核に対しても (Remes et al., 2017) が拡張している．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#位相空間上の核",
    "href": "posts/2024/Kernels/Kernel.html#位相空間上の核",
    "title": "カーネル法の概観",
    "section": "1.4 位相空間上の核",
    "text": "1.4 位相空間上の核\n文章上の string kernel (Lodhi et al., 2002) やグラフ上の graph kernel (Kriege et al., 2020) も考えられている．\n\n1.4.1 乱歩核\n(Borgwardt et al., 2006) は random walk kernel を提案しており，\\(\\mathbb{R}^d\\) へ埋め込まれるようなものの計算量は \\(O(n^3d)\\) である．\n\n\n1.4.2 Weisfeiler-Lehman 核\nさらに効率の良いカーネルとして Weisfeiler-Lehman グラフカーネル (Shervashidze et al., 2011) もある．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#weisfeiler-lehman-核",
    "href": "posts/2024/Kernels/Kernel.html#weisfeiler-lehman-核",
    "title": "カーネル法の概観",
    "section": "1.5 Weisfeiler-Lehman 核",
    "text": "1.5 Weisfeiler-Lehman 核\nさらに効率の良いカーネルとして Weisfeiler-Lehman カーネル (Shervashidze et al., 2011) もある．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#核の構成",
    "href": "posts/2024/Kernels/Kernel.html#核の構成",
    "title": "カーネル法の概観",
    "section": "1.5 核の構成",
    "text": "1.5 核の構成\n\n1.5.1 半正定値核のなす正錐\n半正定値核は \\(\\mathrm{Map}(T^2,\\mathbb{R})\\) 上で閉凸錐をなす．すなわち， \\[\nc_1K_1+c_2K_2,\\qquad c_1,c_2\\ge0,\n\\] とその各点収束極限は再び半正定値核である．\n\n\n1.5.2 半正定値構成\n\n\n\n\n\n\n命題\n\n\n\n\\(K:T^2\\to\\mathbb{C}\\) を半正定値，\\(f:\\mathcal{X}\\to\\mathbb{C}\\) を関数とする． \\[\n\\widetilde{K}(x,y):=f(x)K(x,y)\\overline{f(y)}\n\\] は再び半正定値である．\n\n\n\n\n1.5.3 核の押し出し\n\\(S^1\\simeq[0,2\\pi)\\) 上の確率分布は，方向データとして，海洋学における波の方向，気象学における風向のモデリングに応用を持つ．\n全射 \\(\\pi:\\mathbb{R}\\twoheadrightarrow S^1\\) に従って，\\(\\mathbb{R}\\)-値の Gauss 過程を，方向データ値の Gauss 過程に押し出すことが出来る (Jona-Lasinio et al., 2012)．\nこれに伴い，\\(\\mathbb{R}\\)-値の核 \\(K:\\mathbb{R}\\to\\mathcal{P}(\\mathbb{R})\\) を \\(S^1\\)-値に押し出すこともできる： \\[\n\\pi_*K:\\mathbb{R}\\to\\mathcal{P}(\\mathbb{R})\\xrightarrow{\\pi_*}\\mathcal{P}(S^1).\n\\]\n\\(\\pi\\) による Gauss 分布の押し出し \\(\\pi_*\\mathrm{N}_1(\\mu,\\sigma^2)\\) は wrapped normal distribution と呼ばれている．これに対応し，この Gauss 過程は wrapped Gaussian process と呼ばれている (Jona-Lasinio et al., 2012)．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#sec-RFF",
    "href": "posts/2024/Kernels/Kernel.html#sec-RFF",
    "title": "カーネル法の概観",
    "section": "1.6 核の Monte Carlo 近似",
    "text": "1.6 核の Monte Carlo 近似\n\n1.6.1 カーネルの近似\n以上，種々のカーネル関数を紹介してきたが，これらはデータに関して効率的に計算される必要がある．\n特に潜在空間上での Gram 行列の逆行列または Cholesky 分解を計算する \\(O(n^3)\\) の複雑性が難点である (Liu et al., 2020)．\nこのデータ数 \\(n\\) に関してスケールしない点が従来カーネル法の難点とされてきたが，これはランダムなカーネル関数を用いた Monte Carlo 近似によって高速化できる．\\(m\\) 個のランダムに選択された基底関数を用いれば，Monte Carlo 誤差を許して計算量は \\(O(nm+m^3)\\) にまで圧縮できる．\n\n\n1.6.2 Random Fourier Features\n正定値核のスペクトル表現 (1) を通じて，核の値 \\(K(x,y)\\) を Monte Carlo 近似をすることが出来る．\n例えば \\(K\\) が RBF 核であるとき，\\(p\\) は正規密度になるから，Gauss 確率変数からのサンプリングを通じてこれを実現できる： \\[\nK(x,y)\\approx\\phi(x)^\\top\\phi(y),\\qquad \\phi(x):=\\sqrt{\\frac{1}{D}}\\begin{pmatrix}\\sin(Z^\\top x)\\\\\\cos(Z^\\top x)\\end{pmatrix},Z=(z_{ij}),z_{ij}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^{-2}).\n\\]\nこれは核の値 \\(K(x,y)\\) を，逆に（ランダムに定まる）特徴ベクトル \\(\\phi(x),\\phi(y)\\) の値を通じて計算しているため，Random Fourier Features (Rahimi and Recht, 2007), (Sutherland and Schneider, 2015)，または Random Kitchen Sinks (Rahimi and Recht, 2008) と呼ばれる．\n\\(Z\\) の行を互いに直交するように取ることで，Monte Carlo 推定の精度が上がる．これを orthogonal random features (Yu et al., 2016) と呼ぶ．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#はじめに-1",
    "href": "posts/2024/Kernels/Kernel.html#はじめに-1",
    "title": "カーネル法の概観",
    "section": "2.1 はじめに",
    "text": "2.1 はじめに\n２つのデータ点 \\(x_1,x_2\\in\\mathcal{X}\\) に対して，その意味論的な距離 \\(d(x_1,x_2)\\) を学習することを考える．\nこれはある種の表現学習として，分類，クラスタリング，次元縮約 などの事前タスクとしても重要である．顔認識など，computer vision への応用が大きい．\n古典的には，\\(K\\)-近傍分類器と対置させ，これが最大の精度を発揮するような距離を学習することが考えられる\nまた，ニューラルネットワークにより埋め込み \\(f:\\mathcal{X}\\hookrightarrow\\mathbb{R}^d\\) を構成し，その後 \\(\\mathbb{R}^d\\) 上の Euclid 距離を \\(d\\) として用いるとき，これを 深層距離学習 (deep metric learning) という．\n深層距離学習では距離学習自体が下流タスクとなっており，その性能が深層埋め込み \\(f\\) に依存している．実際，深層距離学習の性能は芳しいと言えないことが知られている (Musgrave et al., 2020)．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#k-近傍分類",
    "href": "posts/2024/Kernels/Kernel.html#k-近傍分類",
    "title": "カーネル法の概観",
    "section": "2.2 \\(K\\)-近傍分類",
    "text": "2.2 \\(K\\)-近傍分類\nラベル付きデータ \\(\\mathcal{D}=\\{(x_i,y_i)\\}\\subset\\mathcal{X}\\times[C]\\) が与えられているとする．\n\\(K\\)-近傍分類法は，「\\(x\\) の近傍上位 \\(K\\) 個のデータに訊いてみる」という方法であり，こうして得る事後確率 \\[\np(y=c|x,\\mathcal{D})=\\frac{1}{K}\\sum_{i\\in\\mathcal{D}_K(x)}1_{\\left\\{y_i=c\\right\\}}\n\\] から \\(x\\) のラベルを予測する．\nこの事後分布をさらにクラスタリングに用いたものが \\(K\\)-平均法 (MacQueen, 1967), (Lloyd, 1982) である\n\\(K\\)-近傍法はそのシンプルな発想に拘らず一致性と，良い収束レートを持つ (Chaudhuri and Dasgupta, 2014)．\n一様カーネル \\[\nK(r;\\ell):=\\frac{1}{2\\ell}1_{[0,\\ell]}(r)\n\\] が定める密度推定量を，どの"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#mahalanobis-距離の学習",
    "href": "posts/2024/Kernels/Kernel.html#mahalanobis-距離の学習",
    "title": "カーネル法の概観",
    "section": "2.3 Mahalanobis 距離の学習",
    "text": "2.3 Mahalanobis 距離の学習\n\\[\nd(x_1,x_2;M):=\\sqrt{(x_1-x_2)^\\top M(x_1-x_2)}\n\\] というパラメトリックモデルを過程し，\\(M\\) を学習することを考える．\n\n2.3.1 大マージン最近傍 (LMNN, Kilian Q. Weinberger et al., 2005)\nLarge margin nearest neighbor (LMNN) (Kilian Q. Weinberger et al., 2005), (Kilian Q. Weinberger and Saul, 2009) は，\\(K\\)-近傍分類器による後続タスクが最も精度が良くなるように \\(M\\) を学習する方法をいう．\n各データ番号 \\(i\\in[n]\\) に対して，これと似ているデータ番号の集合 \\(N_i\\subset[n]\\) が与えられているとする（ラベルが同一であるデータ点など）．これに対して，\\(\\lambda\\in(0,1),m\\ge0\\) をハイパーパラメータとして， \\[\n\\mathcal{L}(M):=(1-\\lambda)\\mathcal{L}^-(M)+\\lambda\\mathcal{L}^+(M),\\qquad\\lambda\\in(0,1),\n\\] \\[\n\\mathcal{L}^-(M):=\\sum_{i=1}^n\\sum_{j\\in N_i}d(x_i,x_j;M)^2,\\quad\\mathcal{L}^+(M):=\\sum_{i=1}^n\\sum_{j\\in N_i}\\sum_{k=1}^N\\delta_{ik}\\biggr(m+d(x_i,x_j;M)^2-d(x_i,x_k;M)^2\\biggl)^2,\n\\] を最小化するように \\(M\\) を学習する．\n\\(\\mathcal{L}\\) は凸関数であるため，半正定値計画法が適用できる．また，\\(M:=W^\\top W\\) によりパラメータ変換をして，\\(W\\) に関して解くことで，問題の凸性を失う代わりに次元数を削減できる．\n\n\n2.3.2 近傍成分分析 (NCA, Goldberger et al., 2004)\n近傍成分分析 (NCA: Neighborhood Component Analysis) (Goldberger et al., 2004) では \\(W\\) を学習する．\n類似度行列 \\(W\\) に関して，確率的近傍埋め込み でも使うモデル \\[\np_{ij}^W:=\\frac{\\exp\\left(-\\lvert Wx_i-Wx_j\\rvert^2\\right)}{\\sum_{k\\neq i}\\exp\\left(-\\lvert Wx_i-Wx_k\\rvert^2\\right)}\n\\] を考える．各 \\(i\\in[n]\\) について，\\(x_i\\) 以外のデータから \\(x_j\\) のラベルを \\(1\\)-近傍分類器で正しく予測する確率が最大になるように， \\[\n\\mathcal{L}(W):=1-\\frac{1}{N}J(W),\\quad J(W):=\\sum_{i=1}^n\\sum_{(i,j)\\in E}p_{ij}^W\n\\] を最小化するように学習する．ただし，辺の集合 \\(E\\) は，ラベルの同じデータを結ぶとした．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#sec-deep-metric-learning",
    "href": "posts/2024/Kernels/Kernel.html#sec-deep-metric-learning",
    "title": "カーネル法の概観",
    "section": "2.4 深層距離学習",
    "text": "2.4 深層距離学習\n\n2.4.1 分類に基づく目的関数\n深層距離学習では目的関数の設定が重要である．\n最も初等的には，自己符号化器などで分類問題を解き，その内部表現（よく最後から２層目を用いる）での Euclid 距離を距離関数に用いる方法がある．\nしかし，距離の情報を学習するために，分類タスクは弱すぎるようである．\n\n\n2.4.2 ２者比較に基づく目的関数\n\\[\n\\mathcal{L}(\\theta;x_i,x_j):=\\delta_{y_i,y_j}d(z_i,z_j)^2+(1-\\delta_{y_i,y_j})\\biggr(m-d(z_i,z_j)^2\\biggl)_+,\\qquad z_i=f_\\theta(x_i)\n\\] という損失関数は 対照的損失 (contrastive loss) (Chopra et al., 2005) と呼ばれる．\nこの損失はラベル \\(y_i,y_j\\) が同一のデータ \\(x_i,x_j\\) の潜在表現の距離を近づけ，ラベルが異なるデータは \\(m\\) 以上は話すように埋め込み \\(f_\\theta\\) を学習する．\nこの際に用いるニューラルネットワークは，同時に２つの入力 \\(x_i,x_j\\) をとって学習することから，双子ネットワーク (Siamese network) とも呼ばれる．\n\n\n2.4.3 ３者比較に基づく目的関数\nこの方法は直ちに三子損失 (triplet loss) (Schroff et al., 2015)，\\(n\\)-ペア損失 (\\(n\\)-pair loss) (Sohn, 2016), (Oord et al., 2018) に拡張された．\nこのことにより，\\(x_i,x_j\\) の「近さ」のスケールと「遠さ」のスケールが一致し，安定した結果が得られる．\n三子損失は，各データ \\(x_i\\) に対して，「似ている」ペア \\(x_i^+\\) と「似ていない」ペア \\(x_i^-\\) を事前に選び， \\[\n\\mathcal{L}(\\theta;x_i,x_i^+,x_i^-):=\\biggr(d_\\theta(x_i,x_i^+)^2-d_\\theta(x_i,x_i^-)^2+m\\biggl)_+,\\qquad m\\in\\mathbb{R}\n\\] と定められる．このとき，\\(x_i\\) は参照点 (anchor) と呼ばれる．\nこの方法は \\(x_i^+,x_i^-\\) を選ばなければいけないが，その分拡張性に優れる．ノイズ対照学習 の稿も参照．\n\\(n\\)-ペア損失では，負のデータ \\(x_i^-\\) をさらに増やす．これは (Oord et al., 2019 Contrastive Predictive Coding) にて，InfoMax の観点から表現学習に用いられたものと一致する．\n\n\n2.4.4 ３者比較の加速\n負の例 \\(x_i^-\\) を特に情報量が高いもの (hard negatives, Faghri et al., 2018) を選ぶことで，学習を加速させることができる．\nこれは，３者損失を提案した Google の FaceNet (Schroff et al., 2015) で考えられた戦略である．\nクラスラベルが得られる場合，各クラスから代表的なデータを選んでおくことで \\(O(n)\\) にまで加速できる (Movshovitz-Attias et al., 2017)．この代表点は固定して１つに定める必要はなく，ソフトな形で選べる (Qian et al., 2019)．"
  },
  {
    "objectID": "posts/2024/Kernels/Kernel.html#footnotes",
    "href": "posts/2024/Kernels/Kernel.html#footnotes",
    "title": "カーネル法の概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Murphy, 2022, p. 565) 17.1節は，半正定値核のことを Mercer 核とも呼んでいる．↩︎\nRBF は (持橋大地 and 大羽成征, 2019, p. 68)，SE は (Rasmussen and Williams, 2006, p. 14) の用語．(Murphy, 2023) では両方が併記されている．Gaussian kernel とも呼ばれる．↩︎\n他のパラメータの入れ方もある．例えば GPy での実装 は \\(\\sigma^2\\exp\\left(-\\frac{r^2}{2}\\right)\\) を採用している．Fourier 変換や偏微分方程式論の文脈では \\(\\frac{1}{(4\\pi t)^{d/2}}\\exp\\left(-\\frac{r^2}{4}\\right)\\) も良く用いられる．これは熱方程式の基本解になるためである．↩︎\n(MacKay, 1994), (Neal, 1996, p. 16) なども参照．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html",
    "href": "posts/2024/Kernels/NCL.html",
    "title": "表現学習と非線型独立成分分析",
    "section": "",
    "text": "半正定値カーネルから距離学習まで\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最古にして最難のタスクと多様体学習\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル５\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#関連ページ",
    "href": "posts/2024/Kernels/NCL.html#関連ページ",
    "title": "表現学習と非線型独立成分分析",
    "section": "",
    "text": "半正定値カーネルから距離学習まで\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最古にして最難のタスクと多様体学習\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル５\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#表現学習とは何か",
    "href": "posts/2024/Kernels/NCL.html#表現学習とは何か",
    "title": "表現学習と非線型独立成分分析",
    "section": "1 表現学習とは何か？",
    "text": "1 表現学習とは何か？\n\n\n\n\n\n\n表現学習５つのアプローチ\n\n\n\n\n教師あり学習による表現学習\n生成による表現学習\n自己教師あり表現学習\nノイズ対照による表現学習\n独立成分分析による表現学習\n\n\n\n極めて高精度な分類器が完成してすぐのころ，分類タスクが極めて上手なニューラルネットワークは他の下流タスクでも良い成績が観察され，最初に考えられた方法が１であった（距離学習でも同様）．\n一方でこのスキームではすぐにドメインシフトと転移学習が問題になった．\nこれを克服するのが２の方法である．高精度なデータを生成できる深層潜在模型が学習された場合，その潜在変数は現実の何らかの表象になっているだろう，というアイデアは analysis-by-synthesis (Roberts, 1963), (Lee and Mumford, 2003) とも呼ばれている．\nこの方法は，文字のストローク（トメ，ハネ）が集まった構造に注目するなど，データの生成過程がある程度明らかなものでは特に性能が良い (Lake et al., 2015)．\n\\(\\beta\\)-VAE (Higgins et al., 2017) や BiGAN (Donahue et al., 2017) はその例であるが，ImageNet などの大規模データに対する分類や分割のタスクで十分な性能はまだ見られていないという．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#生成から雑音除去へ",
    "href": "posts/2024/Kernels/NCL.html#生成から雑音除去へ",
    "title": "表現学習と非線型独立成分分析",
    "section": "2 生成から雑音除去へ",
    "text": "2 生成から雑音除去へ\n２よりも表現学習として良い性質を持つのが３である．\n生成のためには大変多くの特徴量が必要であるが，下流タスクに重要なのはその一部のみに限る．このような場合，Denoising Autoencoder (Vincent et al., 2008) のように「データにノイズを印加してこれを戻すのに必要な知識は何か？」を問うことが極めて普遍的な力を持つ．\n雑音除去と同様に，表現学習に極めて有効なタスクがマスク除去 (Devlin et al., 2019) である．これは画像領域にも応用されている：BEiT (Bao et al., 2022)．masked autoencoder (He et al., 2022) が現在の state of the art であるようである．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#sec-NCL4RL",
    "href": "posts/2024/Kernels/NCL.html#sec-NCL4RL",
    "title": "表現学習と非線型独立成分分析",
    "section": "3 対照学習による表現学習",
    "text": "3 対照学習による表現学習\nノイズ対照学習に基づいた方法が第４勢力として登場（再興）してきている．multiview representation learning とも呼ばれる．\n\n3.1 ノイズ対照学習 (NCL)\nこの方法では，雑音やマスク除去とは違った方法で，「真のデータをノイズと見分ける」という予測問題として表現学習を解く．1\nこの方法は最初自然言語処理で大きな成功を収めた (Mnih and Kavukcuoglu, 2013)．例えば word2vec (Mikolov, Chen, et al., 2013), (Mikolov, Sutskever, et al., 2013) も NCL に基づく．\n一方で前述の通り，BART や GPT などの現代の言語モデルは，ノイズ対照の先へ行き，デノイジングやデマスキングによる表現学習を行っている．\nしかし NCL には，雑音・マスク除去と違い，ある程度どのようなデータを「似ている」とするかの制御が効くという美点がある．これを 距離学習 ともいう．2\n発展した対照学習法，例えば CPC (Contrastive Predictive Coding) (Oord et al., 2019) は，言語，音声，画像，３次元空間での強化学習など，多くの領域で有力な代替を提供するようである．\nまた CLIP (Radford et al., 2019) では，データのモーダリティを超えて，言語と画像の関係について大規模に事前学習をさせることが可能になっている．\n\n\n3.2 対照的予測符号化 (CPC) (Oord et al., 2019)\n\n\n\nContrastive Predictive Coding (Oord et al., 2019)\n\n\nまずエンコーダー \\(z_t=g_{\\text{enc}}(x_t)\\) を作る．続いて，自己回帰モデル \\(g_{\\text{ar}}\\) を用いて \\(z_{1:t}\\) を要約して予測しようとする．\nこの段階で潜在表現 \\(c_t=g_{\\text{ar}}(z_{1:t})\\) が作られることを期待するのであるが，直接 \\(p(x|c)\\) を予測しようとしてしまうと，必ずしも有用な潜在表現 \\(c\\) が得られるとは限らない．\nそこで，距離 \\(k\\) だけ離れたデータ \\(x_{t+k}\\) の尤度比 \\[\nf_k(x_{t+k},c_t)\\,\\propto\\,\\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}\n\\] を， \\[\nf_k(x_{t+k},c_t)=\\exp\\left(z_{t+k}^\\top W_kc_t\\right)\n\\] の形で予測しようとし，この荷重 \\(W_k\\) の推定を考える．\nこれは，表現学習においては予測 \\(p(x|c)\\) が至上命題であるわけではなく，\\(x\\) と \\(c\\) の相互情報量が近ければ十分であるために用意された，表現学習のための代理目標（InfoMax (Linsker, 1988) ともいう）であり，InfoNCE 損失 または \\(n\\)-ペア損失 (Sohn, 2016) とも呼ばれる．3\nこのモデルに対しては，GAN 様の敵対的生成であるノイズ対照学習の損失を用いることができる．(Oord et al., 2019) では，エンコーダとして残差接続を持つ strided convolutional layer が，自己回帰モデルとして GRU (Gated Recurrent Unit) (Cho et al., 2014) という RNN の変種が使われている．\nこうして推定された \\((z_t,c_t)\\) は，\\(x_{1:t}\\) までのヒストリを見た要約が欲しい場合は \\(c_t\\) を，そうでない場合は \\(z_t\\) を，データ \\(x_t\\) の潜在表現として使える．\n\n\n3.3 対照的言語-画像事前学習 (CLIP) (Radford et al., 2019)\nノイズ対照学習に基づくアプローチの美点は，別のモーダリティを持つデータを統合しやすい点にある．\nこれを用いて，言語と画像の関係について大規模に事前学習をさせたのが OpenAI の CLIP (Radford et al., 2019) である．\n\n\n\n画像に対する種々のノイズ対照学習法がどのようなノイズと対照させるか (Murphy, 2023, p. 1055)\n\n\n対照学習による深層距離学習において重要なのは，正のノイズと負のノイズを各サンプル \\(x\\) に対してどう作るか？である (Tian, Sun, et al., 2020)．\nSimCLR (Chen et al., 2020) は，\\(x\\) に対する変換（ランダムなトリミング，リサイズ，並行移動など）を学習し，データ拡張によって正のノイズと負のノイズを作る．\nCMC (Contrastive Multiple Coding) (Tian, Krishnan, et al., 2020) は，\\(x\\) の輝度 (luma) と彩度 (chroma) を取り出して正のノイズと負のノイズとする．\nSupCon (Supervised Contrastive Learning) (Khosla et al., 2020) は画像に対するラベルングが得られるとき，これを教師的に用いて正のノイズと負のノイズを作る．これは 近傍成分分析 (NCA) (Goldberger et al., 2004) と対照学習を組み合わせた発想であり，実際後続の分類タスクがうまく，ロバストになるという．\n\n\n3.4 非対照学習\nVision Transformer (ViT) で用いられる DINO (Self-Distilation with no Labels) (Caron et al., 2021) などは，負のノイズを使わず，正のノイズのみを使った表現学習法である．\nBYOL (Bootstrap Your Own Latent) (Grill et al., 2020) も負のノイズを使わない手法であるが，目的関数には似ているノイズを寄せるための項しかなく，深層表現が退化しない理由はどうやら学習ダイナミクスの方にあるという．\nBarlow Twins (Zbontar et al., 2021) では，正のノイズとの間の，各特徴量に関する相関係数行列 \\(C\\) から定まる \\[\n\\mathcal{L}:=\\sum_{i=1}^p(1-C_{ii})^2+\\lambda\\sum_{i\\ne j}C_{ij}^2\n\\] を目的関数とする．\n第二項の存在により，負のノイズがなくとも表現が縮退することが回避される．この方法は，HSIC (Gretton et al., 2007) などのカーネル独立性検定法を，表現学習に応用している形とみれる．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#独立成分分析による表現学習",
    "href": "posts/2024/Kernels/NCL.html#独立成分分析による表現学習",
    "title": "表現学習と非線型独立成分分析",
    "section": "4 独立成分分析による表現学習",
    "text": "4 独立成分分析による表現学習\n表現学習の１つの目標である disentangle とは，要因ごとにデータ内の変動を説明して分離することをいう．\nこれを達成するには，データやモデルに追加の仮定が必要な場合が多い (Locatello et al., 2020)．どのような状況で安定した disentanglement が可能であるかについて，独立成分分析の知見，特に指数型分布族と識別可能性の概念を通じて理解する試みがある (Khemakhem et al., 2020), (Roeder et al., 2021), (Hälvä et al., 2021)．\n特に，独立成分分析が目指すように，現実に何らかの意味で則した方法でデータの潜在表現を得ることが，表現学習で最も好ましい，あるべき disentanglement であるとするならば，「深層模型がいつ識別可能になるか？」は基本的な問題だというべきだろう (Khemakhem et al., 2020)．4\nこのような立場を 因果的表現学習 (causal representation learning) ともいう．5\nVAE などの深層生成モデル，ノイズ対照学習，独立成分分析などはいずれも，多層の階層モデルを学習するという点では共通しており，１つの分野の発見が他に資することが多い．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#sec-identifiability",
    "href": "posts/2024/Kernels/NCL.html#sec-identifiability",
    "title": "表現学習と非線型独立成分分析",
    "section": "5 深層潜在モデルの識別可能性",
    "text": "5 深層潜在モデルの識別可能性\n仮に追加に観測されている変数 \\(u\\) が存在して，事前分布 \\(p_\\theta(z|u)\\) は \\(z\\) 上で積の形に分解し，指数型分布族に属するとする．すなわち，潜在変数は \\(U\\) で条件づければ互いに独立であるとする．この仮定が識別可能性の鍵となる (Hyvarinen et al., 2019)．\n\\(u\\) はタイムスタンプや前時点での観測，信頼できないラベルなどがありえる (Hyvärinen and Morioka, 2016)．\n観測 \\(X\\) と潜在変数 \\(Z\\) に対して，\\(\\theta=(f,T,\\lambda)\\) をパラメータとして \\[\np_\\theta(x,z|u)=p_f(x|z)p_{T,\\lambda}(z|u),\n\\] \\[\nX=f(Z)+\\epsilon,\\qquad \\epsilon\\sim p_\\epsilon(\\epsilon).\n\\] という形のモデルは，\\(p_{T,\\lambda}\\) が十分統計量 \\(T\\) とパラメータ \\(\\lambda\\) を持つ指数型分布族である限り，いくつかの正則性条件を満たせば識別可能になる：\n\n\n\n\n\n\n(Khemakhem et al., 2020 定理１)\n\n\n\n次の４条件が成り立つ場合，パラメータ \\(\\theta\\) は，ある線型変換 \\(A\\) に対して \\[\nT\\circ f^{-1}=A\\circ \\widetilde{T}\\circ\\widetilde{f}^{-1}+c\n\\] の違いを除いて識別可能である：\n\n\\(p_\\epsilon\\) の特性関数は殆ど至る所零にならない．\n\\(f\\) は単射である．\n十分統計量 \\(\\{T_{i,j}\\}_{i\\in[n],j\\in[k]}\\) は殆ど至る所可微分で，任意の測度正集合上に線型独立な関数を定める．\nある点 \\(u^0,\\cdots,u^{nk}\\) が存在して，行列 \\((\\lambda(u^1)\\;\\cdots\\;\\lambda(u^{nk}))-(\\lambda(u^0)\\;\\cdots\\;\\lambda(u^0))\\) は可逆：\n\n\n\n加えて，モデルが真の分布を含む場合，変分下界の最大化は上述の線型変換 \\(A\\) の違いを除いて \\(\\theta\\) の一致推定に成功する．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#非線型独立成分分析",
    "href": "posts/2024/Kernels/NCL.html#非線型独立成分分析",
    "title": "表現学習と非線型独立成分分析",
    "section": "6 非線型独立成分分析",
    "text": "6 非線型独立成分分析\n非線型独立成分分析は，ある独立な成分からなる潜在変数 \\[\np(z)=\\prod_{i=1}^dp_i(z_i)\n\\] に対して，観測がこの非線型変換 \\(x=f(z)\\) であると仮定し，データ生成過程を特定しようとする営みである．\nこれは上のモデルの \\(\\epsilon=0\\) とした場合に他ならない．\nつまるところ，従来からの深層生成モデリングのうち，統計的に特別な意味を持つものが非線型独立成分分析と捉えることもできるはずである．すなわち，生成モデルと非線型独立成分分析は，モデルの骨子自体は共通で，その適用目的が違うに過ぎない（この稿 も参照）．\nただし，統計モデルと見る以上は識別可能性が肝要である．しかし近年の ICA は，識別可能性を緩めた形 5 で得ることに成功しており，これにより深層生成モデルとの同一視が進むことになる (Hyvarinen et al., 2019), (Khemakhem et al., 2020)．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#vae-の識別可能性",
    "href": "posts/2024/Kernels/NCL.html#vae-の識別可能性",
    "title": "表現学習と非線型独立成分分析",
    "section": "7 VAE の識別可能性",
    "text": "7 VAE の識別可能性\nこれにより，VAE などの深層生成モデルをより統計的に意味のあるものとすることができる．上述の定理により識別可能性を確保した VAE を iVAE (identifiable VAE) (Khemakhem et al., 2020) と呼ぶ．\nまた逆の方向には，非線型 ICA モデルを変分ベイズや確率的勾配降下法により推定することができる．\nまた (Kivva et al., 2021), (Kivva et al., 2022), (Lopez et al., 2024) によると，VAE の事前分布が特定の混合分布の形を持つならば，補助変数 \\(u\\) が存在しない場合でも，VAE は識別可能な因果グラフを与えるという．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#sec-InfoMax",
    "href": "posts/2024/Kernels/NCL.html#sec-InfoMax",
    "title": "表現学習と非線型独立成分分析",
    "section": "8 InfoMax",
    "text": "8 InfoMax\nCPC 3.2 が目指したように，元データの情報量を最大限保った潜在表現を獲得することが，後続タスクにおいて有利になるだろう．\n実は，生物の脳の認識様式もこれに沿っていると考えられ，視覚神経に関する研究を起源として 効率的符号化仮説 (Efficient Coding Hypothesis) (H. B. Barlow, 1961), (H. B. Barlow, 1972) または情報処理分野において 情報量最大化仮説 (InfoMax) (Linsker, 1988), (Bell and Sejnowski, 1995) と呼ばれている．\nこの仮説は視覚の研究における vision as inverse graphics (Romaszko et al., 2017) / analysis by synthesis (Kersten et al., 2004), (Yuille and Kersten, 2006) から，一般に脳が外界モデルを獲得するプロセスに拡張され，ベイズ脳仮説 (Doya et al., 2006) とも呼ばれる (島崎秀昭, 2019)．\n情報理論においては，information bottleneck principle (Tishby et al., 2000), (Tishby and Zaslavsky, 2015) としても継承されている．"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#参考文献",
    "href": "posts/2024/Kernels/NCL.html#参考文献",
    "title": "表現学習と非線型独立成分分析",
    "section": "9 参考文献",
    "text": "9 参考文献\n\n(Hyvärinen and Morioka, 2016), (Hyvarinen and Morioka, 2017), (Hyvarinen et al., 2019) は深層潜在モデルが識別可能になるための条件を示した非線型 ICA の論文である．\nEfficient coding 仮説と InfoMax については，(島崎秀昭, 2019) が大変良い日本語文献である．\n\n例えばハエの視覚細胞を用いた実験で神経細胞の非線形な応答関数が外界の視覚刺激の分布に適応し，出力が一様に分布することで神経細胞のダイナミックレンジが効率よく使用されていることが示されている (Laughlin, 1981), (Brenner et al., 2000)．このように非線形器を外界の分布に適応させる過程を学習と呼ぶ． (島崎秀昭, 2019)"
  },
  {
    "objectID": "posts/2024/Kernels/NCL.html#footnotes",
    "href": "posts/2024/Kernels/NCL.html#footnotes",
    "title": "表現学習と非線型独立成分分析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n予測符号化 (predictive coding) (Elias, 1955) は従来からデータ圧縮の原理であると同時に，認知科学において，脳のメンタルモデルとしても有名である (Rao and Ballard, 1999)．↩︎\n(Murphy, 2023, p. 1056) 第32.3.4.2節も参照．↩︎\n相互情報量は\\[I(x;c)=\\sum p(x,c)\\log\\frac{p(x|c)}{p(x)}\\] と表される．密度比の推定が成功していれば，相互情報量は殆ど変わらない．↩︎\nThe advantage of the new framework over typical deep latent-variable models used with VAEs is that we actually recover the original latents, thus providing principled disentanglement. (Khemakhem et al., 2020) Section 6．↩︎\n(Murphy, 2023, p. 1060) 33.4.1節も参照．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans2.html",
    "href": "posts/2024/TransDimensionalModels/Trans2.html",
    "title": "超次元 Zig-Zag サンプラー",
    "section": "",
    "text": "点呼投票データに対しては (Clinton et al., 2004) がベイズ的なアプローチを創始した．\nその際は政策空間の適切な次元 \\(K\\) に対しての（ドメインエキスパートによる）事前知識を自由に取り入れられる点が利点とされた．\nここでは政策空間の適切な事件 \\(K\\) も推論の対象としたベイズモデル選択法を，１度の MCMC サンプリングで実行することを考える．\n\n\n二項選択モデルで，説明変数の次元が大きく，説明変数間の交互作用が強く，また事前分布の裾が重い場合，特に困難な事後分布を定める．\nこのような設定はベイズ計算手法のベンチマークに適している (Chopin and Ridgway, 2017)．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans2.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/Trans2.html#はじめに",
    "title": "超次元 Zig-Zag サンプラー",
    "section": "",
    "text": "点呼投票データに対しては (Clinton et al., 2004) がベイズ的なアプローチを創始した．\nその際は政策空間の適切な次元 \\(K\\) に対しての（ドメインエキスパートによる）事前知識を自由に取り入れられる点が利点とされた．\nここでは政策空間の適切な事件 \\(K\\) も推論の対象としたベイズモデル選択法を，１度の MCMC サンプリングで実行することを考える．\n\n\n二項選択モデルで，説明変数の次元が大きく，説明変数間の交互作用が強く，また事前分布の裾が重い場合，特に困難な事後分布を定める．\nこのような設定はベイズ計算手法のベンチマークに適している (Chopin and Ridgway, 2017)．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#多項ロジスティック回帰",
    "href": "posts/2024/Survey/BayesGLM.html#多項ロジスティック回帰",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "2 多項ロジスティック回帰",
    "text": "2 多項ロジスティック回帰\n\n2.1 はじめに\nここでは BMI と LDL コレステロールの関係を見る．\n\n2.1.1 BMI と LDL の関係\n一般に HDL コレステロールは BMI と正の相関がある（特に２型糖尿病患者では (Hussain et al., 2019)）．\n\ncor(raw_df$BMI, raw_df$HDL)\n\n[1] -0.3689261\n\n\n一方で LDL コレステロールと BMI の相関は弱い：\n\ncor(raw_df$BMI, raw_df$LDL)\n\n[1] 0.158966\n\n\nしかし全く無関係ではないように見える：\n\nboxplot(\n  raw_df$LDL ~ raw_df$obesity,\n  col = c(\"pink\", \"lightgreen\", \"skyblue\"),\n  main = \"LDL classified by BMI\",\n  xlab = \"BMI\",\n  ylab = \"LDL\"\n)\n\n\n\n\n\n\n\n\nこれは LDL と BMI の関係は非線型性が高く，その非線型関係が男女，さらに年齢で違うためかもしれない (Li et al., 2021)．\nこの関係を詳しく見ていくことで何か発見があるかもしれないだろう．\n\n\n2.1.2 LAB\nLAB (Inoue et al., 2010) は酸化変性した LDL のことで，別名超悪玉コレステロールと知られる．LDL コレステロールよりも動脈硬化リスクを（特に残余リスクとして）反映する新しいバイオマーカーになり得ると期待されている (Okamura et al., 2013)．\nLAB と LDL の相関は高くない：\n\ncor(raw_df$BMI, raw_df$LAB)\n\n[1] 0.1952133\n\n\n\nboxplot(\n  raw_df$LAB ~ raw_df$obesity,\n  col = c(\"gray\", \"pink\", \"skyblue\"),\n  main = \"LAB classified by BMI\",\n  xlab = \"BMI\",\n  ylab = \"LAB\"\n)\n\n\n\n\n\n\n\n\nここでは LAB と LDL の BMI への影響を比較したい．\n\n\n2.1.3 BMI の離散化\nBMI と LDL の関数関係は非線型性が予期される (Li et al., 2021)．\nそこで BMI を直接被説明変数とするのではなく，離散化した順序変数 obesity を導入する：\nraw_df &lt;- raw_df %&gt;%\n  mutate(obesity = case_when(\n  BMI &lt; 18.5 ~ 1,  # underweight\n  BMI &lt; 25 ~ 2,    # normal\n  BMI &gt;= 25 ~ 3    # obese\n  ))\n\n\n\n2.2 LDL の予測力\nLDL を直接用いて推定すると係数が極めて小さくなるため，対数変換によりスケールを変換して説明変数に入れる：\n\\[\n\\operatorname{P}[\\texttt{obesity}&gt;1]=g^{-1}(\\beta_{\\texttt{LDL}}\\cdot\\log(\\texttt{LDL})-c_1)\n\\] \\[\n\\operatorname{P}[\\texttt{obesity}&gt;2]=g^{-1}(\\beta_{\\texttt{LDL}}\\cdot\\log(\\texttt{LDL})-c_2)\n\\]\n\nformula_LDL &lt;- bf(\n  obesity ~ log(LDL),\n  family = cumulative(link = \"logit\")\n)\n# prior_LDL &lt;- prior(normal(0,0.1), class = b, coef = \"logLDL\")\nfit_LDL &lt;- brm(\n  formula_LDL,\n  data = raw_df,\n  chains = 4, cores = 4\n#  prior = prior_LDL\n)\n\n\nsummary(fit_LDL)\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: obesity ~ log(LDL) \n   Data: raw_df (Number of observations: 839) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     2.05      1.26    -0.42     4.52 1.00     2671     2228\nIntercept[2]     5.62      1.27     3.12     8.12 1.00     2606     2280\nlogLDL           0.97      0.27     0.46     1.50 1.00     2622     2226\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLDL が上がるごとに，normal や underweight から obese に移る確率が上がるのが見える．\n\nconditional_effects(fit_LDL, \"LDL\", categorical = TRUE)\n\n\n\n\n\n\n\n\n\nprior_summary(fit_LDL)\n\n                prior     class   coef group resp dpar nlpar lb ub       source\n               (flat)         b                                         default\n               (flat)         b logLDL                             (vectorized)\n student_t(3, 0, 2.5) Intercept                                         default\n student_t(3, 0, 2.5) Intercept      1                             (vectorized)\n student_t(3, 0, 2.5) Intercept      2                             (vectorized)\n\n\n\\(c_1,c_2\\) には \\(0\\) を中心とした \\(t\\)-事前分布が置かれているため，識別性は保たれると考えて良い．\n\n\n2.3 LAB との比較\n実は LDL よりも LAB の方が少し予測力が高い．しかも LAB は \\([1,10]\\) に値を取るので，対数変換をするよりも平方根変換をする方が自然である：\n\nformula_LAB &lt;- bf(\n  obesity ~ sqrt(LAB),\n  family = cumulative(link = \"logit\")\n)\nfit_LAB &lt;- brm(\n  formula_LAB,\n  data = raw_df,\n  chains = 4, cores = 4\n)\n\n\nloo_compare(loo(fit_LDL), loo(fit_LAB))\n\n        elpd_diff se_diff\nfit_LAB  0.0       0.0   \nfit_LDL -2.2       3.9   \n\n\n\nconditional_effects(fit_LAB, \"LAB\", categorical = TRUE)\n\n\n\n\n\n\n\n\nLAB が上昇すると underweight, normal から obese に移る確率がグンと上がるのが見える．\n\n\n2.4 双方の採用\n前節で LAB と LDL を比較すると，前者のみを用いたモデルの方が予測力が高いことを見た．\nでは両方をモデルに入れてベイズ推論をすることで，２つの情報を統合したより良いモデルができるだろうか？\n\ndf_double &lt;- data.frame(\n  obesity = raw_df$obesity,\n  z_sqrt_LAB = scale(sqrt(raw_df$LAB)),\n  z_log_LDL = scale(log(raw_df$LDL))\n)\nformula_double &lt;- bf(\n  obesity ~ z_sqrt_LAB + z_log_LDL,\n  family = cumulative(link = \"logit\")\n)\nfit_double &lt;- brm(\n  formula_double,\n  data = df_double,\n  chains = 4, cores = 4\n)\n\n実際予測性能の面では両方入れたモデルの方が良いようである：\n\nloo_compare(loo(fit_LAB), loo(fit_double))\n\n           elpd_diff se_diff\nfit_double  0.0       0.0   \nfit_LAB    -0.3       1.7   \n\n\n\nplot(fit_double, variable = c(\"b_z_sqrt_LAB\", \"b_z_log_LDL\"))\n\n\n\n\n\n\n\n\nLDL の方が僅かに縮小されて推定されていることがわかる．\nさらには LAB と LDL がモデルに入っている確率を出す方法は別稿で追求する：\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.5 交絡の存在\n\nformula_double_confound &lt;- bf(\n  obesity ~ z_sqrt_LAB * z_log_LDL,\n  family = cumulative(link = \"logit\")\n)\nfit_double_confound &lt;- brm(\n  formula_double_confound,\n  data = df_double,\n  chains = 4, cores = 4\n)\n\n\nplot(fit_double_confound, variable = c(\"b_z_sqrt_LAB\", \"b_z_log_LDL\", \"b_z_sqrt_LAB:z_log_LDL\"))\n\n\n\n\n\n\n\n\n若干の交絡の存在が疑われる．LDL が大きいほど，LAB と BMI との関係は減少していき，逆もまた然りである．\nこれは共線型性が強いので当然とも思われる：\n\ncor(df_double$z_sqrt_LAB, df_double$z_log_LDL)\n\n[1] 0.5204453\n\n\nしかし必ずしも条件数が大きいわけではない．\n\nX &lt;- model.matrix(~ z_sqrt_LAB + z_log_LDL, data = df_double)\nkappa(X)\n\n[1] 1.879479\n\n\n\n\n2.6 名目モデルを使ってしまったら？\nunderweight, normal, obese 間の順序構造を無視して，カテゴリカル分布を通じてモデリングをしても，実は当てはまりは必ずしも悪くない．\n\nformula_nominal &lt;- bf(\n  obesity ~ LAB,\n  family = categorical(link = \"logit\")\n)\nfit_nominal &lt;- brm(\n  formula_nominal,\n  data = raw_df,\n  chains = 4, cores = 4\n)\n\n\nwaic(fit_LAB)\n\n\nComputed from 4000 by 839 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -686.0 18.5\np_waic         2.8  0.1\nwaic        1372.0 37.0\n\nwaic(fit_nominal)\n\n\nComputed from 4000 by 839 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -685.8 18.6\np_waic         3.7  0.2\nwaic        1371.6 37.2\n\n\nほとんど質的には一致した結果を得る：\n\nconditional_effects(fit_nominal, \"LAB\", categorical = TRUE)\n\n\n\n\n\n\n\n\nひょっとしたら，BMI を離散化したという点で順序変数に思えるかもしれないが，これは単なる思い込みで，「痩せている」ことと「太っている」ことに順序関係を仮定することはむしろノイズになっているのかもしれない．\n\n\n\n2.7 非線型関係の追求\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-16\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.8 共変量の追加と効果の観察\n\nraw_df$sqrt_LAB &lt;- sqrt(raw_df$LAB)\nraw_df$log_Age &lt;- log(raw_df$Age)\nformula_LAB_cov &lt;- bf(\n  obesity ~ log_Age + SEX + log_Age:SEX + (0 + SEX | sqrt_LAB),\n  family = categorical(link = \"logit\")\n)\nfit_LAB_cov &lt;- brm(\n  formula_LAB_cov,\n  data = raw_df,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_LAB_cov)\n\n Family: categorical \n  Links: mu2 = logit; mu3 = logit \nFormula: obesity ~ log_Age + SEX + log_Age:SEX + (0 + SEX | sqrt_LAB) \n   Data: raw_df (Number of observations: 839) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~sqrt_LAB (Number of levels: 56) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(mu2_SEXfemale)                  0.20      0.14     0.01     0.54 1.00\nsd(mu2_SEXmale)                    0.20      0.14     0.01     0.53 1.00\nsd(mu3_SEXfemale)                  0.48      0.23     0.05     0.94 1.01\nsd(mu3_SEXmale)                    0.28      0.19     0.01     0.70 1.00\ncor(mu2_SEXfemale,mu2_SEXmale)    -0.12      0.57    -0.97     0.92 1.00\ncor(mu3_SEXfemale,mu3_SEXmale)    -0.10      0.54    -0.96     0.90 1.00\n                               Bulk_ESS Tail_ESS\nsd(mu2_SEXfemale)                  1288     1735\nsd(mu2_SEXmale)                    1118     1308\nsd(mu3_SEXfemale)                   792      963\nsd(mu3_SEXmale)                    1027     1614\ncor(mu2_SEXfemale,mu2_SEXmale)     2097     2182\ncor(mu3_SEXfemale,mu3_SEXmale)     1638     1708\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmu2_Intercept          -4.06      1.92    -7.89    -0.26 1.00     2108     2674\nmu3_Intercept          -6.39      2.35   -11.05    -1.71 1.00     2049     2334\nmu2_log_Age             1.55      0.50     0.55     2.56 1.00     2069     2656\nmu2_SEXmale            12.79      5.40     2.66    24.16 1.00      994     1566\nmu2_log_Age:SEXmale    -2.98      1.34    -5.79    -0.43 1.00     1004     1534\nmu3_log_Age             1.81      0.61     0.63     3.02 1.00     2028     2315\nmu3_SEXmale            14.91      5.62     4.64    26.55 1.00      933     1490\nmu3_log_Age:SEXmale    -3.31      1.40    -6.16    -0.73 1.00      945     1525\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(fit_LAB_cov, \"SEX\", categorical = TRUE)\n\n\n\n\n\n\n\n\n一般に男性の方が太っている確率が高くなる．これはよく知られているようである．\n\nconditional_effects(fit_LAB_cov, \"log_Age\", conditions = list(SEX = \"male\"), categorical = TRUE)\n\n\n\n\n\n\n\nconditional_effects(fit_LAB_cov, \"log_Age\", conditions = list(SEX = \"female\"), categorical = TRUE)\n\n\n\n\n\n\n\n\n女性は年齢とともに太る傾向が見えるが，男性はそうでもない（むしろ逆である）ようである．\nそして共変量を追加したことでモデルの予測力が大きく良くなっている：\n\nloo_compare(loo(fit_LAB), loo(fit_LAB_cov))\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n            elpd_diff se_diff\nfit_LAB_cov   0.0       0.0  \nfit_LAB     -11.2       8.1  \n\n\n\npp_check(fit_LAB_cov)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\n\n\n2.9 その他の共変量の探索\nBMI と LDL は，骨ミネラル密度 (BMD: Bone Mineral Density) に因果的な影響を与えることが知られている (Wu, 2024)．\nP1NP は骨形成マーカー，ALP は骨の形成や骨疾患の評価に役立つ酵素である．\ndeoxypyridinolin (DPD) は骨を構成するⅠ型コラーゲンを束ねる蛋白質で、骨吸収の指標になる．\n他に Ca, Mg などの値もある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html",
    "href": "posts/2024/Survey/BayesGLMM.html",
    "title": "brms を用いたベイズ混合ロジスティック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html#はじめに",
    "href": "posts/2024/Survey/BayesGLMM.html#はじめに",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\n多くの社会的なデータは非数値的である．しかしその背後には潜在的な連続変数を想定することが多い．\n加えて，線型回帰分析の結果複雑な非線型関係が予期された際，本格的なノンパラメトリック推論に移る前に，離散変数の設定に換言して非線型性を扱いやすくするなど，離散変数を扱う積極的理由もある．\n本稿ではロジスティック回帰を主に扱う．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\npscl, MCMCpack, emIRT パッケージ\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html#多項ロジスティック回帰",
    "href": "posts/2024/Survey/BayesGLMM.html#多項ロジスティック回帰",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "2 多項ロジスティック回帰",
    "text": "2 多項ロジスティック回帰\n\n2.1 データの概観\n\nboxplot(\n  raw_df$LAB ~ raw_df$obesity,\n  col = c(\"gray\", \"pink\", \"skyblue\"),\n  main = \"LAB classified by BMI\",\n  xlab = \"BMI\",\n  ylab = \"LAB\"\n)\n\n\n\n\n\n\n\n\nLAB の値が大きいほど\n\n\n2.2 共変量\nP1NP は骨形成マーカー，ALP は骨の形成や骨疾患の評価に役立つ酵素である．\ndeoxypyridinolin (DPD) は骨を構成するⅠ型コラーゲンを束ねる蛋白質で、骨吸収の指標になる．\n他に Ca, Mg などの値もある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html#項目応答モデル",
    "href": "posts/2024/Survey/BayesGLMM.html#項目応答モデル",
    "title": "brms を用いたベイズ混合ロジスティック回帰分析",
    "section": "1 項目応答モデル",
    "text": "1 項目応答モデル\n\n1.1 データの概観\n(Vansteelandt, 2001), (Boeck and Wilson, 2004) による「怒るかどうか？」のデータ VerbAgg を用いる．混合モデルの点推定のためのパッケージ lme4 (Bates et al., 2015) で利用可能になっている．\n\nlibrary(lme4)\ndata(\"VerbAgg\", package = \"lme4\")\ndf &lt;- VerbAgg\n\n質問票は「自分が意思表示をしたのにバスが止まってくれなかったので悪態をついた」などのもので，同意できるかを３段階 “yes”, “perhaps”, “no” で評価する (Boeck and Wilson, 2004, pp. 7–8)．\n応答は３段階の順序応答 resp とこれを２段階にしたもの r2 である．\n\nkable(head(df))\n\n\n\n\nAnger\nGender\nitem\nresp\nid\nbtype\nsitu\nmode\nr2\n\n\n\n\n20\nM\nS1WantCurse\nno\n1\ncurse\nother\nwant\nN\n\n\n11\nM\nS1WantCurse\nno\n2\ncurse\nother\nwant\nN\n\n\n17\nF\nS1WantCurse\nperhaps\n3\ncurse\nother\nwant\nY\n\n\n21\nF\nS1WantCurse\nperhaps\n4\ncurse\nother\nwant\nY\n\n\n17\nF\nS1WantCurse\nperhaps\n5\ncurse\nother\nwant\nY\n\n\n21\nF\nS1WantCurse\nyes\n6\ncurse\nother\nwant\nY\n\n\n\n\n\n\n\n1.2 固定効果１母数モデル\n通常の１母数モデルに，過分散を説明するための固定効果の項 \\(\\alpha_0\\) を加えたモデルを考える：\n\\[\ng(\\operatorname{P}[Y_{ik}=1])=\\alpha_{j[i]}-\\beta_{k[i]}+\\alpha_0,\\qquad\\alpha_0\\sim\\mathrm{t}(3;0,2.5),\n\\] \\[\n\\alpha_j\\sim\\mathrm{N}(\\mu_\\alpha,\\sigma_\\alpha^2),\\quad\\mu_\\alpha\\sim\\mathrm{N}(0,3),\\quad\\sigma_\\alpha\\sim\\mathrm{N}(0,3),\n\\] \\[\n\\beta_k\\sim\\mathrm{N}(\\mu_\\beta,\\sigma_\\beta^2),\\quad\\mu_\\beta\\sim\\mathrm{N}(0,3),\\quad\\sigma_\\beta\\sim\\mathrm{N}(0,3).\n\\]\nsd というクラスはグループレベル変数の標準偏差を意味する．\n\\(\\alpha_j,\\beta_k\\) の定数の違いに関する識別不可能性は，いずれも \\(0\\) を中心とした\n\nformula_1PL &lt;- bf(r2 ~ 1 + (1|item) + (1|id))\nprior_1PL &lt;-  prior(\"normal(0,3)\", class=\"sd\", group = \"id\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\")\nfit_1PL &lt;- brm(\n  formula_1PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nprior_summary(fit_1PL)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n          normal(0,3)        sd              id                  0   \n          normal(0,3)        sd Intercept    id                  0   \n          normal(0,3)        sd            item                  0   \n          normal(0,3)        sd Intercept  item                  0   \n       source\n      default\n      default\n         user\n (vectorized)\n         user\n (vectorized)\n\n\nvectorized というのは，下記 Stan コード内で尤度は for 文で構成されるが，このループに入れなくて良いものがある場合をいう．\n\n\n\n\n\n\nStan コードの表示\n\n\n\n\n\nstancode(fit_1PL)\nによって推定に用いられた Stan コードが表示できる．\n次を見る限り，確かに意図したモデルになっている：\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += normal_lpdf(sd_1 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n  lprior += normal_lpdf(sd_2 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n-1*normal_lccdf(0|0,3) というのは定数であり，推定には全く影響を与えないが，後続の bridgesampling パッケージ (Gronau et al., 2020) によるモデル比較の API 構築のために付けられたものである (Bürkner, 2021, p. 21)．\n\n\n\n\n\nsummary(fit_1PL)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ 1 + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.39      0.07     1.25     1.54 1.00     1004     1840\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.22      0.19     0.91     1.64 1.00      471     1009\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.16      0.24    -0.63     0.31 1.00      355      812\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n低い ESS から変動効果の項 \\(\\epsilon_i\\) の推定に苦労していることがわかる．\n\nplot(fit_1PL)\n\n\n\n\n\n\n\n\nここにはグローバルなパラメータしか表示されておらず，ランダム効果の結果は次のように見る必要がある：\n\nlibrary(ggplot2)\nranef_item &lt;- ranef(fit_1PL)$item\nposterior_means &lt;- ranef_item[,1,1]\nlower_bounds &lt;- ranef_item[,3,1]\nupper_bounds &lt;- ranef_item[,4,1]\nplot_df_item &lt;- data.frame(\n  item = rownames(ranef_item),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n\n\np_PL1 &lt;- ggplot(plot_df_item, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Items\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\np_PL1\n\n\n\n\n\n\n\n\n多くの参加者にとって腹立たしい例とそうでない例が区別できているようである．\n\nplot_df_id &lt;- plot_df_id %&gt;% arrange(mean) %&gt;% mutate(rank = row_number())\np_PL1_id &lt;- ggplot(plot_df_id, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Individuals\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\np_PL1_id\n\n\n\n\n\n\n\n\nこうして怒りやすかった人を並べることができる．\nしかしガタガタしている区分定数的な模様が見れる．実はこれは item の分だけある．というのも，「何個の項目に Yes と答えたか」だけが \\(\\alpha_j\\) を決める要因になってしまっているためである．\nこれが項目識別のできない１母数モデルの限界である．\n\n\n1.3 固定効果２母数モデル\n項目識別力母数 \\(\\gamma_k\\) を導入する： \\[\ng(\\mu_i)=\\gamma_{k[i]}\\biggr(\\alpha_{j[i]}-\\beta_{k[i]}\\biggl),\n\\]\nすると追加の制約が必要になる．ここでは理想点モデルの場合と違い，研究のデザインから \\(\\gamma_{k[i]}\\) は正として良いだろう．\nこれを変数変換 \\(\\gamma_k=\\exp(\\log\\gamma_k)\\) によってモデルに知らせることとする．\n\nformula_2PL &lt;- bf(\n  r2 ~ exp(loggamma) * eta,\n  loggamma ~ 1 + (1|i|item),\n  eta ~ 1 + (1|i|item) + (1|id),\n  nl = TRUE\n)\n\n\\(g(\\mu_i)\\) の右辺はもはや \\(\\log\\gamma_k\\) の線型関数ではないので，これを nl=TRUE によって知らせる必要がある．\n|i| によって，\\(\\log\\gamma_k\\) と \\(\\eta_{jk}\\) 内の項 \\(\\beta_k\\) には相関があることを知らせている (Bürkner, 2018, p. 397)．項目難易度 \\(\\beta_k\\) が低いほど識別力 \\(\\log\\gamma_k\\) は低いとしているのである．\n\nprior_2PL &lt;-  prior(\"normal(0,5)\", class=\"b\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"b\", nlpar = \"loggamma\") +\n  prior(\"constant(1)\", class=\"sd\", group = \"id\", nlpar = \"eta\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"sd\", group = \"item\", nlpar = \"loggamma\")\n\nfit_2PL &lt;- brm(\n  formula = formula_2PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_2PL,\n  chains = 4, cores = 4\n)\n\nついに Stan が２分ほどかかるようになった上に，収束に苦労しており，ESS が低くなっている．\n\nsummary(fit_2PL)\n\nWarning: There were 1 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ exp(loggamma) * eta \n         loggamma ~ 1 + (1 | i | item)\n         eta ~ 1 + (1 | i | item) + (1 | id)\n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~item (Number of levels: 24) \n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(loggamma_Intercept)                    0.12      0.06     0.01     0.24 1.01\nsd(eta_Intercept)                         0.93      0.16     0.68     1.28 1.00\ncor(loggamma_Intercept,eta_Intercept)     0.31      0.36    -0.46     0.90 1.01\n                                      Bulk_ESS Tail_ESS\nsd(loggamma_Intercept)                     712      874\nsd(eta_Intercept)                         1076     1826\ncor(loggamma_Intercept,eta_Intercept)      264      645\n\n~id (Number of levels: 316) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(eta_Intercept)     1.00      0.00     1.00     1.00   NA       NA       NA\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nloggamma_Intercept     0.32      0.06     0.20     0.44 1.00     1193     1895\neta_Intercept         -0.14      0.20    -0.52     0.25 1.00     1186     1676\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nranef_item2 &lt;- ranef(fit_2PL)$item\nposterior_means &lt;- ranef_item2[,1,\"eta_Intercept\"]\nlower_bounds &lt;- ranef_item2[,3,\"eta_Intercept\"]\nupper_bounds &lt;- ranef_item2[,4,\"eta_Intercept\"]\nplot_df_item2 &lt;- data.frame(\n  item = rownames(ranef_item2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_PL2 &lt;- ggplot(plot_df_item2, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\ngrid.arrange(p_PL1, p_PL2, nrow = 1)\n\n\n\n\n\n\n\n\n識別力パラメータ \\(\\gamma_k\\) が \\(1\\) より大きい値をとっており，これが変動を吸収しているため，\\(\\alpha_j\\) は \\(0\\) に縮小されて推定されるようになっている．\n\nranef_id2 &lt;- ranef(fit_2PL)$id\nposterior_means &lt;- ranef_id2[,1,\"eta_Intercept\"]\nlower_bounds &lt;- ranef_id2[,3,\"eta_Intercept\"]\nupper_bounds &lt;- ranef_id2[,4,\"eta_Intercept\"]\nplot_df_id2 &lt;- data.frame(\n  id = rownames(ranef_id2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n\n\nplot_df_id2 &lt;- plot_df_id2 %&gt;% arrange(mean) %&gt;% mutate(rank = row_number())\np_PL2_id &lt;- ggplot(plot_df_id2, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\ngrid.arrange(p_PL1_id, p_PL2_id, nrow = 1)\n\n\n\n\n\n\n\n\n少し滑らかになっている．\n\ncor(ranef_id[,1,\"Intercept\"], ranef_id2[,1,\"eta_Intercept\"])\n\n[1] 0.9994873\n\n\nしかし線型の相関になっており，軟化以上の変化は導入されなかったことがわかる．\nそれもそうである．モデルの表現力はあげたから解像度は高くなったが，モデルに新しい情報を入れたわけではないのである．\n\n\n1.4 共変量の追加\n理想点モデルなど多くの項目応答モデルは，\\(\\alpha_j,\\beta_k\\) の推定に終始してきたが，本当のリサーチクエスチョンはその先にある．\n個人レベルの共変量を追加した階層モデルを構築して，\\(\\alpha_j\\) の位置や応答の傾向への影響を調べることが真の目標であった．\n\n1.4.1 項目共変量の追加\n本データにおいて項目は \\(2\\times2\\times3\\) の split-plot デザインがなされている．\nmode とは「悪態をつきたい」と「咄嗟についてしまう」という２種の行動を区別するためのものである．この２つの行動容態は，本人の抑制的な意識が実際に働いたかどうかにおいて全く質的に異なる．モデルにこれを教えたらどうなるだろうか？\nsitu とはシチュエーションであり，自分に責任があるか（「店に入ろうとした瞬間閉店時間になった」など）他人に責任があるか（「バスが止まってくれなかった」など）の２項目がある．\nbtype は行動様式であり，「悪態をつく」「叱る」「怒鳴りつける」の３項目がある．後に行くほど他人への攻撃性が強い．\n最初に考えられるモデル\nr2 ~ btype + situ + mode + (1|item) + (1 + mode|id)\nは，元々の１母数モデルに変動切片項を３つ追加した上に，mode の係数を個人ごとに変えることを許したものである．これは mode の効果が個人ごとに異なるだろうという信念による．\nしかしこのモデルに至る前に，1 を 0 にすることで modedo と modewant 双方の標準偏差を推定することを考える（1 の場合は modewant の標準偏差の代わりに Intercept の標準偏差を推定する）．\n\nformula_1PL_cov &lt;- bf(\n  r2 ~ btype + situ + mode + (1|item) + (0 + mode|id)\n)\nfit_1PL_cov &lt;- brm(\n  formula = formula_1PL_cov,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL_cov)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ btype + situ + mode + (1 | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.47      0.09     1.30     1.65 1.00     1898\nsd(modedo)               1.67      0.10     1.48     1.88 1.00     1932\ncor(modewant,modedo)     0.77      0.04     0.69     0.84 1.00     1674\n                     Tail_ESS\nsd(modewant)             3005\nsd(modedo)               2826\ncor(modewant,modedo)     2675\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.46      0.09     0.32     0.68 1.00     1643     2370\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.88      0.23     1.42     2.32 1.00     1923     2640\nbtypescold    -1.12      0.24    -1.60    -0.63 1.00     2004     2515\nbtypeshout    -2.23      0.25    -2.73    -1.74 1.00     1962     2482\nsituself      -1.12      0.21    -1.53    -0.70 1.00     1941     2500\nmodedo        -0.77      0.21    -1.18    -0.33 1.00     2114     2523\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nmodedo の係数が負になっており，悪態をつきたくなっても，実際にする人の割合は下がることがわかる．\nだが係数の -0.77 が大きいかどうかがわからない．これには対数オッズ比のスケールから元のスケールに戻す便利な関数がある：\n\nconditional_effects(fit_1PL_cov, \"mode\")\n\n\n\n\n\n\n\n\n確率としての減少は軽微だがあることがわかる．次に気づくことは do の方がエラーバーが長いことである．２つの係数は相関しているので，頻度論的な検定は難しいかもしれないが，２つの標準偏差の差の事後分布を見ることでチェックすることができる：\n\nhyp &lt;- \"modedo - modewant &gt; 0\"\nhypothesis(fit_1PL_cov, hyp, class = \"sd\", group = \"id\")\n\nHypothesis Tests for class sd_id:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (modedo-modewant) &gt; 0      0.2      0.12     0.01     0.39       23.1\n  Post.Prob Star\n1      0.96    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n0.96 の確率で modedo の標準偏差の方が大きいことがわかるが，その差も 0.2 ほどで，対数オッズ比としては大したことがないと思われる．\n\n\n1.4.2 個人共変量の追加\nTrait Anger スコア (Spielberger, 2010) が個人ごとに算出されており（Anger 変数），そのスコアによってどのように項目への反応が違うかを調べる．こうするとどんどん心理学の研究っぽくなる．\n\nformula_1PL_cov_id &lt;- bf(\n  r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0+Gender|item) + (0+mode|id)\n)\nfit_1PL_cov_id &lt;- brm(\n  formula = formula_1PL_cov_id,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4,\n  iter = 3000  # これ以上大きくすると GitHub にあげられない\n)\n\n\nsummary(fit_1PL_cov_id)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0 + Gender | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.49      0.09     1.32     1.67 1.00     1998\nsd(modedo)               1.58      0.10     1.40     1.78 1.00     1715\ncor(modewant,modedo)     0.78      0.04     0.70     0.85 1.00     1529\n                     Tail_ESS\nsd(modewant)             2740\nsd(modedo)               2531\ncor(modewant,modedo)     2380\n\n~item (Number of levels: 24) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(GenderF)              0.52      0.11     0.35     0.77 1.01     1342\nsd(GenderM)              0.34      0.11     0.16     0.57 1.00     1659\ncor(GenderF,GenderM)     0.78      0.18     0.32     0.99 1.00     2178\n                     Tail_ESS\nsd(GenderF)              2491\nsd(GenderM)              2553\ncor(GenderF,GenderM)     1911\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          0.74      0.44    -0.14     1.59 1.00     1293     2072\nAnger              0.06      0.02     0.02     0.09 1.00     1274     2101\nGenderM           -0.10      0.24    -0.56     0.38 1.00      931     1839\nbtypescold        -1.03      0.22    -1.46    -0.60 1.00     1931     2144\nbtypeshout        -2.43      0.25    -2.90    -1.94 1.00     1472     1396\nsituself          -1.04      0.18    -1.38    -0.68 1.00     1919     2346\nmodedo            -0.98      0.23    -1.43    -0.51 1.00     1738     2315\nGenderM:modedo     0.89      0.24     0.40     1.35 1.00     2525     2932\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(fit_1PL_cov_id, \"Anger\")\n\n\n\n\n個人共変量の効果と，特異項目機能\n\n\n\n\n\nconditional_effects(fit_1PL_cov_id, \"mode:Gender\")\n\n\n\n\n\n\n\n\n\nAnger の値が大きいほど悪態をつく確率が綺麗に上がっていく様子がわかる．\n加えて，女性の方が悪態を吐こうと思っても，実際に行動に移すには大きな壁があることがわかる．こうして mode と Gender の間の交絡が陽の下に明らかになった．\nこのような，項目共変量と個人共変量の間の交絡は 特異項目機能 (DIF: Differential Item Functioning) と呼ばれる．項目の特性が，被験者のグループによって違った機能を示すことは，例えばテスト理論では個人の潜在特性を推定する際の重大なノイズ要因となっており，これを統制することが重要な課題になる．\n\n\n\n1.5 特異項目機能の解析\nこの特異項目機能を，項目の特性ごとにさらに詳しく見ていく．\n特に怒鳴りつける行動様式を除き，悪態をつく行為と叱る行為は，男性と女性において違う機能を持っているのではないか？という仮説を検証してみる．\n女性が実際に悪態をつく／叱る行為にだけマークをつけるダミー変数 dif を用意する：\n\ndf$dif &lt;- as.numeric(with(\n  df,\n  Gender == \"F\" & mode == \"do\" & btype %in% c(\"curse\", \"scold\")\n))\n\n\nformula_1PL_dif &lt;- bf(\n  r2 ~ Gender + dif + (1|item) + (1|id)\n)\n\n\nfit_1PL_dif &lt;- brm(\n  formula = formula_1PL_dif,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 3, cores = 3,\n  # iter = 3000  # これ以上大きくすると GitHub にあげられない\n)\n\n\nsummary(fit_1PL_dif)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Gender + dif + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.40      0.07     1.26     1.54 1.00      793     1557\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.34      0.21     1.00     1.84 1.01      684     1088\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.12      0.31    -0.51     0.72 1.00      304      502\nGenderM      -0.01      0.21    -0.45     0.41 1.00      573      864\ndif          -0.95      0.14    -1.23    -0.67 1.00     2875     1778\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\ndif の係数 -0.94 を見ることで，殊に「女性」と「実際に悪態を吐いたり叱ったりする」という組み合わせは特異な項目機能を持っていることがわかる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html#文献案内",
    "href": "posts/2024/Survey/BayesGLMM.html#文献案内",
    "title": "brms を用いたベイズ混合ロジスティック回帰分析",
    "section": "2 文献案内",
    "text": "2 文献案内\n\n(Bürkner, 2021) に項目応答モデルのベイズ的な扱いが取り上げられている．特にパッケージ brms を用いた例が３つある．\nDIF に関する日本語文献に (龍一, 2012) がある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#階層ロジットモデル",
    "href": "posts/2024/Survey/BayesGLM.html#階層ロジットモデル",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "3 階層ロジットモデル",
    "text": "3 階層ロジットモデル\n\n3.1 はじめに\nデータの不均衡性に着想を得て，「太っているかいないか？」の後に「痩せ気味かどうか？」という２つのロジット回帰の連続とみることを考える．\n\nraw_df &lt;- raw_df %&gt;%\n  mutate(obesity_former = case_when(\n  BMI &lt; 25 ~ 0,    # normal\n  BMI &gt;= 25 ~ 1    # obese\n  ))\nraw_df &lt;- raw_df %&gt;%\n  mutate(obesity_latter = case_when(\n  BMI &lt; 18.5 ~ 0,    # underweight\n  BMI &gt;= 18.5 ~ 1    # normal\n  ))\n\n\nformula_former &lt;- bf(\n  obesity_former ~ LAB,\n  family = bernoulli(link = \"logit\")\n)\nfit_former &lt;- brm(\n  formula_former,\n  data = raw_df,\n  chains = 4, cores = 4\n)\nformula_latter &lt;- bf(\n  obesity_latter ~ LAB,\n  family = bernoulli(link = \"logit\")\n)\nfit_latter &lt;- brm(\n  formula_latter,\n  data = raw_df,\n  chains = 4, cores = 4\n)"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html",
    "title": "階層ベイズ理想点解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html#前稿",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html#前稿",
    "title": "階層ベイズ理想点解析",
    "section": "1 前稿",
    "text": "1 前稿\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html#モデル",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html#モデル",
    "title": "階層ベイズ理想点解析",
    "section": "2 モデル",
    "text": "2 モデル\n\n2.1 構造\n議会での点呼投票データ \\(\\{Y_{i,j}\\}_{i\\in[N],j\\in[J]}\\) を考える．２母数ロジットモデル \\[\n\\operatorname{P}[Y_{i,j}=1]=\\Phi(\\alpha_j+\\beta_jX_i)\n\\tag{1}\\] によって各議員 \\(i\\in[N]\\) の理想点 \\(X_i\\in\\mathbb{R}\\) を推定することを考える．さらにここに階層モデル \\[\nX_i=Z_i^\\top\\gamma_{g(i)}+\\epsilon_i,\\qquad\\epsilon_i\\sim\\mathrm{N}(0,\\sigma^2),\n\\tag{2}\\] を考える．ただし \\(Z_i\\in\\mathbb{R}^p\\) は議員ごとの共変量， \\[\ng:[N]\\to[G]\n\\] は議員の項目応答特性のクラスタリングとする．\n\n\n2.2 事前分布\n第一階層 (1) の２母数ロジットモデルには \\[\n\\alpha_j\\sim\\mathrm{N}(0,\\sigma_\\alpha^2),\\quad\\beta_j\\sim\\mathrm{N}(0,\\sigma_\\beta^2)\n\\] という正規事前分布を仮定する．\n第二階層 (2) の共変量にはスパース性を促進する spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定する： \\[\np(d\\gamma_{g(i)})=\\omega_{g(i)}\\delta_0(d\\gamma_{g(i)})+(1-\\omega_{g(i)})p_0(d\\gamma_{g(i)}).\n\\] \\[\n\\omega_{g(i)}\\sim\\operatorname{Beta}(a,b).\n\\] ただし \\(p_0\\) は多様な理想点を促進するために t-分布とする．\n\\(\\sigma\\) には half-Cauchy 事前分布を仮定する： \\[\n\\sigma\\sim\\text{Half-Cauchy}(0,1).\n\\]\n最後にグループ所属 \\(g\\) には，最大クラスタ数 \\(G_\\max=10\\) を仮定し， \\[\n\\operatorname{P}[g(i)=g]=\\frac{1}{G},\\qquad G\\sim U([G_\\max])\n\\] とする．"
  },
  {
    "objectID": "posts/2024/Survey/BayesNonparametrics.html",
    "href": "posts/2024/Survey/BayesNonparametrics.html",
    "title": "brms を用いたノンパラメトリック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nformula_nonlin &lt;- bf(\n  obesity ~ t2(z_sqrt_LAB, z_log_LDL),\n  family = cumulative(link = \"logit\")\n)\nfit_nonlin &lt;- brm(\n  formula_nonlin,\n  data = df_double,\n  chains = 4, cores = 4\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-regularization",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-regularization",
    "title": "ベイズ変数選択",
    "section": "2 縮小事前分布による方法",
    "text": "2 縮小事前分布による方法\n\n2.1 多くの説明変数が存在する場合の事前分布\nstan_glm では回帰係数には適切な分散を持った独立な正規分布（\\(g\\)-prior）をデフォルトの事前分布としている．\nbrms では一様事前分布である．\n仮に説明変数が極めて多い場合，このデフォルト事前分布を採用し続けることは適切ではない．\n実際，独立な正規・一様分布に従う説明変数が大量にある場合，これは「ベイズ（事後平均）推定量の分散が大きい」という事前分布を採用していることに含意してしまう．\n仮に \\(\\sigma\\) にも同様の分散の大きい事前分布をおいているのならば辻褄は合うが，そうでないならばベイズ決定係数 \\(R^2\\) にほとんど \\(1\\) 近くの事前分布をおいていることに等価である．\nすなわち過学習されたモデルに強い事前分布をおいていることになる (Gelman et al., 2020, p. 208)．これは我々の信念と食い違うだろう．そもそも弱情報であるべきデフォルト事前分布としては相応しくない．\n\n\n2.2 正則事前分布\nまずは各変数の正規事前分布の分散を十分小さくして，誤差 \\(\\epsilon\\) の分散 \\(\\sigma^2\\) のスケールと同一にすることが考えられる．\nこの際 \\(R^2\\) にはほとんど無情報な事前分布が仮定されるのと同一である．\nさらに，仮に「多くの説明変数のうち，一部しか重要なものはなく，他の大部分はほとんど無関係である」と思っている，あるいは思いたいとする．変数選択を行いたい場合がこれにあたる．\nこの信念を正確に表現する事前分布の一つに馬蹄事前分布 (horseshoe prior) (Carvalho et al., 2010) とその正則化バージョン (Piironen and Vehtari, 2017) がある．\nこれらの分布は \\(R^2\\) 上の事前分布に，\\(0\\) 上にスパイクを生じさせる．シンプルなモデルを選好することになるのである．\nStan においては prior=hs によって指定できる (Gelman et al., 2020, p. 209)．\n\n\n2.3 Local Scale Mixture\n多くの正則化事前分布は次のような正規分布の local scale mixture (West, 1987) の形をしている：2 \\[\n\\pi(\\beta_j|\\lambda)=\\int_\\mathbb{R}\\phi(\\beta_j|0,\\lambda^2\\lambda^2_j)\\pi(\\lambda_j^2)\\,d\\lambda_j.\n\\]\n\\(\\pi\\) が２点のみに台を持つ場合が spike-and-slab (1) であった (Polson and Scott, 2011)．SSVS （第 3.2 節）も含む．\\(\\pi\\) が絶対連続である場合も次のような例を持つ (Polson and Scott, 2012)：\n\n\n\n\n\n\n\n二重指数分布 (double exponential) / Bayesian Lasso (Park and Casella, 2008), (Hans, 2009)\n馬蹄事前分布 (horseshoe prior) (Carvalho et al., 2010)\nBayesian elastic net (Hans, 2011)\n\n\n\n\n(Ishwaran and Rao, 2005) はこれらの研究より早い段階で，\\(\\pi\\) に \\(0\\) の近くと \\(0\\) から大きく離れた二峰を持つ分布を用意している．\n\n\n2.4 ベイズ縮小の効果\nスケールパラメータ \\(\\lambda\\) は，LASSO (Tibshirani, 1996) では CV などの基準により選択することになるが，\\(\\lambda\\) を推定してモデル平均を行うことでより高い推定精度を得ることができる (Hans, 2009)．\n同様にして事後平均推定量により推定精度は改善されるが，ほとんど確実にこれはスパースではない．従って推定量のスパース性と推定精度はトレードオフの関係にあり，完全にベイジアンに Bayesian LASSO を実行すると本末転倒に陥るという一面もある．\nこのためベイズ縮小事前分布を用いた場合，自動的にスパース性に基づいたモデル選択ができるというわけではなく，事後モデル確率 (posterior model probability) を最大にするものを見つけるという post-processing が必要になる (Section 1.5 Hahn and Carvalho, 2015, p. 438), (Piironen et al., 2020), (Jim E. Griffin, 2024)．\nしかし以上のベイズモデル選択の手続きを踏むことによって，事後モデル確率を最大にするものという統計的・決定理論的に根拠を持ったモデル選択を実行することができる．\nLASSO はベイズモデル選択の結果よりも予測性能が必ずしも高いわけではないにも拘らず，より多くの変数をモデルに残しがちであることも報告されている (Porwal and Raftery, 2022, p. 3)．これは馬蹄事前分布などの最新の縮小事前分布は，回帰係数の効果量やモデルの大きさなどに応じて適応的に正則化の強さを加減しているためだとも言える (Li et al., 2023)．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-variable-selection",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-variable-selection",
    "title": "ベイズ変数選択",
    "section": "3 ベイズ変数選択",
    "text": "3 ベイズ変数選択\n\n3.1 はじめに：階層モデリング\nベイズ変数選択 では，説明変数 \\(\\{x_i\\}_{i=1}^p\\) のそれぞれがモデルに含まれるかを意味する潜在変数 \\(\\{\\gamma_i\\}_{i=1}^p\\in\\{0,1\\}^p=:\\Gamma\\) の事後分布を算出して，特定の変数がモデルに含まれる確率を算出する．\n最終的にこの確率分布は，ベイズモデル平均 (BMA: Bayesian Model Averaging) と言って，それぞれのモデルの事後予測を平均するためのプライヤーとして用いることもできる．\nこの方法では \\[\np(\\gamma)=\\prod_{i=1}^p\\omega_i^{\\gamma_i}(1-\\omega_i)^{1-\\gamma_i},\\qquad p(\\sigma^2)\\,\\propto\\,\\sigma^{-2}\n\\] \\[\np(\\beta|\\sigma,\\gamma)\\,d\\beta=\\mathrm{N}_p(0,\\Sigma(\\sigma,\\gamma))\n\\] という階層構造を通じて，回帰モデルに潜在変数 \\(\\gamma\\) を導入する．\\(\\Sigma\\) は (X. Liang et al., 2022) では独立，(George and McCulloch, 1997) では \\(g\\)-prior とする： \\[\n\\Sigma(\\sigma,\\gamma):=g\\sigma^2(X^\\top_\\gamma X_\\gamma)^{-1}.\n\\] \\(g\\) は global scale parameter と呼ばれ，これにさらに hyperprior を設定することもある (F. Liang et al., 2008), (Ley and Steel, 2009)．\nこのアプローチは (George and McCulloch, 1997) らによって創始された．（仮に \\(p\\) が比例的に増えるとしても） \\(n\\to\\infty\\) の極限で PIP は正しいモデル上の Delta 測度に収束する (Shang and Clayton, 2011)．\n\n\n3.2 確率的探索法\n特に (George and McCulloch, 1993) では \\[\n\\beta_i|\\gamma_i\\sim(1-\\gamma_i)\\mathrm{N}(0,\\sigma_i^2)+\\gamma_i\\mathrm{N}(0,c_i^2\\sigma_i^2)\n\\tag{2}\\] という構造を設定し，データ拡張に基づく Gibbs サンプラーによって推定することを提案した．\nこの方法は 確率的探索法 (SSVS: Stochastic Search Variable Selection) と呼ばれる．3\nしかし (2) は spike-and-slab (1) の近似になっているため，\\(\\gamma_i=1\\) の事後確率は正確に PIP になっているわけではない．\nこの近似は Gibbs サンプラーを高速にするという利点はあったかもしれないが，現代では spike-and-slab (1) に直接適用できる高速なサンプラーが多数開発されている．\n\n\n3.3 Add-Delete-Swap による探索\n計量化学 (chemometrics) では \\(p\\) が特に高次元になり得る．(Brown et al., 1998) は近赤外線分光法で得られたデータから，予測に有用な波長を選択する問題に対処するためにベイズ変数選択の方法を用いることを考えた．\nそのためにまず第 3.1 節の階層モデルを多次元化し，推定には乱歩 MH 法を用いた．\n(Brown et al., 1998) の乱歩 MH 法の提案核は，Add-Delete-Swap の動きをするものであった：\n\n\n\n\n\n\n次の２つの動きを，それぞれ確率 \\(\\phi,1-\\phi\\) で行う；\n\nAdding or Deleting\n新たな変数 \\(x_i\\) をランダムに選び，まだモデルに入っていない場合は入れ，すでにモデルに含まれている場合は取り除く．\nSwapping\nモデルに含まれていない変数 \\(x_i\\) と含まれている変数 \\(x_j\\) をそれぞれランダムに選び，入れ替える．\n\n\n\n\n(Yang et al., 2016) は MH 法の計算複雑性を解析し，\\(p\\) が大きい場合にも計算量が線型にしか増加しない乱歩 MH 法を提案した．\n\n\n3.4 超次元 MCMC による PIP 算出\n事後包含確率を出すにあたって，Reversible-Jump MCMC (Green, 1995) などの超次元手法を用いることも考えられる．\n超次元 MCMC とは，一般に複数のモデルから同時にサンプリングするための用いられ，ベイズ変数選択法は分解可能なグラフィカルモデルに対する超次元 MCMC 法の特別な場合と見れる (Godsill, 2001, p. 232)．\n詳しくは別稿で取り上げるが，特に Sticky PDMP (Bierkens et al., 2023) は有力な PIP 算出法になる．\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.5 計算の問題\nベイズ変数選択とはモデル空間 \\(\\Gamma=\\{0,1\\}^p\\) 上の事後分布を計算することであるが，これを効率的に行う MCMC を構成することが中心的な問題になる．\nここまで叙述してきた Gibbs サンプラー（例えば確率的探索法 3.2 など）は，\\(p=2\\) の極めて簡単な設定で簡単に崩壊する．というのも，２つの同等な説明力を持つ確率変数が強い相関を持つ場合，\\((\\beta,\\gamma)\\) の同時分布は強い二峰性を持つ．\nその結果ただナイーブに Gibbs サンプラーを適用しただけでは片方の峰しか見つけることができず，PIP について偏った結果を出してしまう (Section 5.1 Zanella and Roberts, 2019)．\n推定したいモデルが階層モデルである限り，多峰性の問題は常に付きものである．\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\\(p=2\\) の時でさえ深刻になり得る多峰性の問題に加えて，共変量の数 \\(p\\) が大きい現代的な問題に対処する必要もあることを思えば，ベイズ変数選択の問題は，多峰性に強い効率的なベイズ計算法を開発するという普遍的な課題に回収されるのである．\n\n\n3.6 Tempered Gibbs サンプラー\n目標分布の条件付き分布 \\(f(x_i|x_{-i})\\) が多峰性を持つ場合，何らかの軟化 \\(g(x_i|x_{-i})\\) を考えることがあり得る．これに対して \\[\np_i(x):=\\frac{g(x_i|x_{-i})}{f(x_i|x_{-i})}\n\\] と定め，まず \\((p_1(x),\\cdots,p_p(x))\\) に従って \\(i\\in[p]\\) を選び，続いて \\(x_i\\sim g(x_i|x_{-i})\\) をサンプリングする random scan Gibbs サンプラーを考えると，これはやはり \\(f\\) を不変分布にもつ．\nこの方法では \\(p_i(x)\\) で各説明変数 \\(x_i\\) に傾斜をつけているために，特に PIP の高い \\(i\\in[p]\\) から優先的にサンプリングすることができる．この方法は後述 3.7 節の informed MCMC の先駆けとなった．\n\n\n\n3.7 Locally Informed MH Samplers\nAdd-Delete-Swap による乱歩 MH 法 3.3 の設計は，見通しの良い素朴な構成であるが，効率的な動きである保証は全くない．さらには \\(\\phi\\) やそれぞれの候補を持ってくる確率など，ユーザーが調整する必要があるハイパーパラメータも多い．\n対称かつ局所的なサンプラーの中では，採択率が高いほど効率が良い (Peskun, 1973), (Tierney, 1998)．そこで離散空間上の乱歩 MH 法の採択率を上げるために，現在位置の周囲の点の情報を収集して次の動きを決めるサンプラーが Informed MCMC の名前の下で開発されている (X. Liang et al., 2022, p. 84)．\n多くの locally informed MCMC (Zanella, 2020) では，通常の乱歩 MH 核 \\(Q\\) を基底核 (base kernel) として，これを近傍での事後分布の様子を要約した 釣り合い関数 (balancing function) \\(g\\) を用いて修正することで効率的な動きを達成する： \\[\nq_g(\\gamma,\\gamma')\\,\\propto\\,g\\left(\\frac{\\pi(\\gamma')}{\\pi(\\gamma)}\\right)q(\\gamma,\\gamma').\n\\]\nこの中でも LIT (Locally Informed and Thresholded proposal) (Zhou et al., 2022) は釣り合い関数として閾値関数 (threshold function) \\[\ng(t)=p^L\\land(p^l\\lor t),\\qquad-\\infty&lt;l&lt;L&lt;\\infty.\n\\] を用い，さらに提案核 \\(Q\\) を単なる一様分布ではなく第 3.3 節で考えられた Add-Delete-Swap 核 (Brown et al., 1998) にとることで，一定の条件の下で 次元 \\(p\\) に依存しない収束速度 を達成することを示した．\n\n\n3.8 適応的なサンプラー\nLIT は固定した基底核 \\(Q\\) を取り，そこから \\(g\\) で修正することを基本戦略としていた．一方でそもそも基底核 \\(Q\\) を適応的に調整していくメカニズムを導入することができる．\nASI (Adaptively Scaled Individual adaptation) (J. E. Griffin et al., 2021) では，(Brown et al., 1998) の Add-Delete-Swap 核 3.3 \\[\nq_\\eta(\\gamma,\\gamma')=\\prod_{j=1}^pq_{\\eta,j}(\\gamma_j,\\gamma_j'),\\qquad \\eta=(A_1,\\cdots,A_p,D_1,\\cdots,D_p)\\in(0,1)^{2p},\n\\] \\[\nq_{\\eta,j}(0,1)=\\eta_j=A_j,\\qquad q_{\\eta,j}(1,0)=\\eta_{p+j}=D_j,\n\\] を元にして，\\(\\eta=(A,D)\\) を適応的に更新していくことを考える．その際の目安は，\\(x_j\\) の PIP \\(\\pi_j\\) から定まる \\[\nA_j^\\mathrm{opt}:=1\\land\\frac{\\pi_j}{1-\\pi_j},\\qquad D_j^\\mathrm{opt}:=1\\land\\frac{1-\\pi_j}{\\pi_j},\n\\] である．これの推定量 \\(\\eta^{(i)}\\) を各段階で構成した上で，総じた採択率を調整する学習率のようなパラメータ \\(\\zeta^{(i)}\\) も導入し，(Robbins and Monro, 1951) の方法で更新していく．\n実は ASI は (X. Liang et al., 2023) がいう 確率近傍サンプラー (random neighbourhood sampler) の例になっている．これは Add-Delete-Swap (Brown et al., 1998) のように提案される近傍が，補助的な離散確率変数 \\(k\\in[K]\\) によって定まるような乱歩 MH 法をいう．\n(X. Liang et al., 2023) では ASI によって構成される確率的近傍の中から，さらに locally informed に次の動きを選ぶことを提案し， 適応的確率近傍 (ARNI: Adaptive Random Neighbourhood Informed) サンプラーと呼んでいる．これにより ASI で上がりきらなかった採択率を押し上げつつ，計算量を抑えつつも良い近傍を提案するメカニズムを取り入れることができる．\nこれにより \\(p\\) に依らない収束だけでなく，計算複雑性も \\(p\\) の次元に対してスケールするものが得られると期待される．\n\n\n3.9 非可逆なサンプラー\nInformed MCMC とは，連続空間上で言えば，MALA などの勾配情報を利用したサンプラーである．このような乱歩 MH 法の修正として得られる効率的なサンプラーを (X. Liang et al., 2022) は 近傍サンプラー (neighbourhood sampler) と呼んでいる．\nMALA などの Langevin 拡散を元にした乱歩 MH 法は革新的であったが，現在最も効率的なサンプラーは，局所的な動きを廃した HMC 法と，非可逆な動きを達成する PDMP (Piecewise Deterministic Markov Process) / ECMC (Event-Chain Monte Carlo) である．\n離散空間上でもこれらのサンプラーに対応するものは高い効率を示すだろうと思われる．\n一般の離散空間上でも局所性の打開には (Nishimura et al., 2020)，可逆性の打開には (Koskela, 2022) などの試みがあるが，殊に変数選択に関しては Sticky PDMP (Bierkens et al., 2023) という画期的な手法が開発されている．\nこれに関しては次稿で詳しく取り上げる：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#footnotes",
    "title": "ベイズ変数選択",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n例えば (Barr et al., 2013) では検証的仮説検定の設定で，どこまでランダム効果をモデルに入れるかを議論しており，「全部入れるべき」という結論を一定の前提の下で導いている．↩︎\n(Hahn and Carvalho, 2015, p. 436) は local scale mixture と呼んでいる．他にこの観点は (Jim E. Griffin and Brown, 2017), (Polson and Scott, 2012) でも議論されている．↩︎\n変数減少法やステップワイズ法などのヒューリスティックな方法に対しての「確率的探索法」という名称だったのだと思われる．特に (George and McCulloch, 1993) ではデータ拡張に基づく Gibbs サンプラーを用いており，その様子が「確率的探索」に見えるのだと思われる．↩︎"
  },
  {
    "objectID": "posts/2024/Slides/Master.html",
    "href": "posts/2024/Slides/Master.html",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n体積測度 \\mu が等しい可測集合のうち，球が最小の測度を持つ．\n\n\n\n\n\n\n\nBorel 可測集合 A\\subset\\mathbb{R}^n に関して，\\epsilon-閉近傍 を次のように定める： \nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\mid d(x,A)\\le t\\right\\},\\qquad t&gt;0.\n\n実は A は Borel 可測とは限らないが，Lebesgue 可測ではある．\n\n\n\n\n\n\n\n\n\n\n古典的集中不等式 (Schmidt, 1948)-(Lévy, 1951)\n\n\n\nn-次元球面 S^n\\subset\\mathbb{R}^{n+1} に関して，A\\subset S^n を Borel 可測，C を同体積の（測地）球とすると， \n\\mu(C_\\epsilon)\\le\\mu(A_\\epsilon),\\qquad\\epsilon&gt;0.\n\n\n\n\n\n\n\n\n\n\n\n\n(Giné and Nickl, 2021, p. 31) 定理 2.2.3\n\n\n\n\\gamma_n を \\mathbb{R}^n 上の標準正規分布とする．A\\subset\\mathbb{R}^n を Borel 可測， \nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\qquad a\\in\\mathbb{R},u\\in\\mathbb{R}^n\\setminus\\{0\\},\n を同体積の affine 半空間 とすると， \n\\gamma_n(H_a+\\epsilon B^n)\\le\\overline{\\gamma_n}(A_\\epsilon+\\epsilon B^n),\\qquad\\epsilon&gt;0.\n\n\n\n\\mathbb{R}^n だけでなく \\mathbb{R}^\\infty 上でも成り立つ．\n\n\n\n\n\n\n\n\n\n(Borell, 1975)-(Sudakov and Tsirel’son, 1974)\n\n\n\n\\{X_t\\}_{t\\in T} を可分な中心 Gauss 過程で，ほとんど確実に上限 \\|X\\|_\\infty が有限であるとする．このとき，\\|X\\|_\\infty の中央値 M に関して，1 \n\\operatorname{P}\\biggl[\\biggl|\\|X\\|_\\infty-M\\biggr|&gt;u\\biggr]\\le\\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right),\\qquad u&gt;0,\\sigma^2:=\\sup_{t\\in T}\\mathrm{V}[X_t].\n\n\n\n同様の命題を平均値の周りに関しても示せる．係数 2 が前につくものは (Gross, 1975) による正規分布に関する対数 Sobolev 不等式から導ける．"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#gauss-集中不等式",
    "href": "posts/2024/Slides/Master.html#gauss-集中不等式",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "1 Gauss 集中不等式",
    "text": "1 Gauss 集中不等式\n\n1.1 等周不等式\n体積測度 \\mu が等しい可測集合のうち，球が最小の測度を持つ．\n\n\n\n\n\n\n\nBorel 可測集合 A\\subset\\mathbb{R}^n に関して，\\epsilon-閉近傍 を次のように定める： \nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\mid d(x,A)\\le t\\right\\},\\qquad t&gt;0.\n\n実は A は Borel 可測とは限らないが，Lebesgue 可測ではある．\n\n\n\n\n\n\n\n\n\n\n古典的集中不等式 (Schmidt, 1948)-(Lévy, 1951)\n\n\n\nn-次元球面 S^n\\subset\\mathbb{R}^{n+1} に関して，A\\subset S^n を Borel 可測，C を同体積の（測地）球とすると， \n\\mu(C_\\epsilon)\\le\\mu(A_\\epsilon),\\qquad\\epsilon&gt;0."
  },
  {
    "objectID": "posts/2024/Slides/Master.html#markov-過程の収束",
    "href": "posts/2024/Slides/Master.html#markov-過程の収束",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "2 Markov 過程の収束",
    "text": "2 Markov 過程の収束\n\n2.1 劣指数エルゴード定理\n\n\n\n\n\n\nステートメント\n\n\n\nE を Polish距離空間，X を Feller-Dynkin 過程とする．連続関数 V:E\\to[1,\\infty) が存在して後述の２条件を満たすならば，任意の T&gt;0 に対して C&gt;0 が存在して次が成り立つ： \n\\|P^t(x,-)-P^t(y,-)\\|_\\mathrm{TV}\\le C\\frac{V(x)+V(y)}{\\lambda(t)},\\qquad x,y\\in E,t\\ge T.\n \n\\lambda(t):=\\Phi^{-1}(t),\\Phi(u):=\\int^u_1\\frac{ds}{\\phi(s)}.\n\n\n\nこの V は ドリフト関数 ともいう．証明法は (Kulik, 2018) が扱う skelton 連鎖 X_n:=X_{hn}\\;(h&gt;0,n=1,2,\\cdots) に帰着する方法と，再起過程 (regeneration process) を用いた (Hairer, 2021) による直接的方法がある．\n\n\n2.2 成立条件\n\n\n\n\n\n\n成立条件\n\n\n\n\n条件１：弱いドリフト条件\nある K\\in\\mathbb{R} と全射かつ単調増加な狭義凹関数 \\phi:\\mathbb{R}_+\\to\\mathbb{R}_+ が存在して \nV(X_t)-Kt+\\int^t_0\\phi(V(X_s))\\,ds\n は任意の x\\in E に関して \\operatorname{P}_x-優マルチンゲールである．\n条件２：強い局所 Dobrushin 条件\n任意の c\\ge1 に関して下部集合 V^{-1}([1,c]) はコンパクトで，ある h&gt;0 が存在して P^h は V^{-1}([1,c]) 上局所 Dobrushin である： \n\\sup_{(x,y)\\in B_c}\\|P^h(x,-)-P^h(y,-)\\|_\\mathrm{TV}&lt;2,\\qquad B_c:=\\left\\{(x,y)\\in E^2\\mid V(x)+V(y)\\le c\\right\\}.\n\n\n\n\n\n\n2.3 Langevin 拡散のエルゴード性"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#最適輸送",
    "href": "posts/2024/Slides/Master.html#最適輸送",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "3 最適輸送",
    "text": "3 最適輸送"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#zig-zag-サンプラー",
    "href": "posts/2024/Slides/Master.html#zig-zag-サンプラー",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "4 Zig-Zag サンプラー",
    "text": "4 Zig-Zag サンプラー\n\n\n\nOutput from anim_traj() in PDMPFlux.jl package"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html",
    "title": "超次元 MCMC",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#関連ページ",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#関連ページ",
    "title": "超次元 MCMC",
    "section": "関連ページ",
    "text": "関連ページ\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#はじめに",
    "title": "超次元 MCMC",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 問題設定\n本稿では，状態空間が \\[\nE=\\bigcup_{k\\in[K]}E_k,\\qquad E_k:=\\{k\\}\\times\\mathbb{R}^{n_k},\n\\tag{1}\\] により定義される場合を考える．\n空間の次元 \\(n_k\\) は一定とは限らないため，このような状態空間 \\(E\\) 上に適用される MCMC を 超次元 (trans-dimensional) MCMC と呼ぶ．\n\nこのような設定はまずモデル選択において自然に生じる．超次元 MCMC により，モデル（の添え字）の空間 \\[\n[K]:=\\{1,\\cdots,K\\}\n\\] 上に定まる周辺事後分布を通じて，モデル選択をすることができる．\n\n\n1.2 例\n最初に超次元 MCMC を考えた (Green, 1995) では炭鉱事故データに Poisson モデルを適用し，そのレート関数の変化点解析に応用している．\n変化点の存在も数 \\(k\\in\\mathbb{N}\\) も不明であるとするとき，\\(k\\) 個の変化点の位置と，これによって生じる \\(k+1\\) 個の区間上のレートの値で，モデルの次元 \\[\nn_k=2k+1\n\\] が変化する設定が自然に生じる．\n続いて２次元の変化点解析として，画像の区分け (segmentation) や物体認識の問題が Voronoi 分割を通じて扱えることが説明されている (Green, 1995, p. 724)．\n似た発想として，節点の個数と位置を不定として曲線のフィッティングをする問題にも (Denison et al., 1998) で応用されている．\n(Richardson and Green, 1997) では多峰性のある酵素活性データに対して正規混合モデルを適用しており，そのサンプリングに混合比に応じてパラメータ空間の間をジャンプする超次元 MCMC を応用している．\n現代では混合数 \\(K\\) も不定としてデータから学ぶベイズ推定は当然のものとなっているが，初の分析 (Richardson and Green, 1997) は超次元 MCMC の開発 (Green, 1995) を待って初めて可能になったものである．\n\n\n1.3 滞在時間比によるモデル事後確率の推定\nモデル \\(k\\in[K]\\) に関する 周辺尤度 (marginal likelihood) とは，尤度をパラメータ \\(\\theta\\) の事前分布に関して平均をとったもの \\[\np(D|k)=\\int_{E_k}p(D|k,\\theta_k)p(\\theta_k|k)\\,d\\theta_k\n\\] をいう．\\(D\\) はデータとした．よく \\(m_k(D)\\) とも表記される．\n周辺尤度の比 \\[\nB_{k',k}:=\\frac{p(D|k')}{p(D|k)}\n\\] は Bayes 因子 (Kass and Raftery, 1995) と呼ばれる．\n超次元 MCMC によるモデル選択の美点の１つとして，事後モデル確率 \\[\np(k|D)\\,\\propto\\,p(D|k)p(k)\n\\] が，MCMC のモデル \\(k\\in[K]\\) への滞在時間の割合によって簡単に計算できることが挙げられる．この方法によるモデル選択は (Carlin and Polson, 1991) などの極めて初期の論文でも提案されている．\nただし，全ての周辺尤度 \\(p(D|k)\\;k\\in[K]\\) が得られればベイズ因子も事後モデル確率も計算できるため，これらの値を得るためだけならば必ずしも MCMC による必要はないことに注意する．または MCMC の出力を使ってより効率的に推定することができる (Chib, 1995)．\n(Han and Carlin, 2001) は簡便性と得られる情報量とについて，超次元 MCMC と直接周辺尤度を推定する方法とはトレードオフの関係にあるとしている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#可逆な方法",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#可逆な方法",
    "title": "超次元 MCMC",
    "section": "2 可逆な方法",
    "text": "2 可逆な方法\n\n2.1 はじめに\n本稿ではまず，詳細釣り合い条件を満たすような MCMC のみに焦点を絞って，次元を超える MCMC をどのように構成できるかを議論する．\n一般論（第 2.2 節）が (Green, 1995) で議論されている．結局こうして定義されるクラスは，より一般の空間への MH アルゴリズムの拡張となっている：\n\nIn brief, reversible jump MCMC is a random sweep Metropolis± Hastings method adapted for general state spaces. (Richardson and Green, 1997, p. 736)\n\n\n\n2.2 一般論\n目標分布 \\(\\pi\\in\\mathcal{P}(E)\\) に対して提案核が \\[\nq(x,-)=\\sum_{m=1}^Mq_m(x,-)\n\\] という形で用意されている場合に，棄却率 \\(\\alpha_m(x,x')\\) をどう設定すれば所望の \\(\\pi\\) に収束する Markov 連鎖が得られるかを考える．\n\n\n\n\n\n\n(Green, 1995, p. 715)\n\n\n\n\\(\\pi\\otimes q_m\\) がある \\(E^2\\) 上の対称な測度に関して密度 \\(f_m\\) を持つとする．このとき， \\[\n\\alpha_m(x,x')f_m(x,x')=\\alpha_m(x',x)f_m(x',x)\n\\] を満たすならば，Metropolis-Hastings サンプラーは \\(\\pi\\) を不変分布にもつ．\n\n\n(Peskun, 1973) の含意として受容確率はなるべく高い方が良いから，結局は \\[\n\\alpha_m(x,x')=1\\land\\frac{\\pi(dx')q_m(x',dx)}{\\pi(dx)q_m(x,dx')}.\n\\] と取ることにあたる．\nこの命題の前提条件である，「\\(\\pi\\otimes q_m\\) がある \\(E^2\\) 上の対称な測度に関して密度 \\(f_m\\) を持つ」という条件は dimension-matching requirement と呼ばれる．\n仮に \\(q_m\\) はモデル間のジャンプのみを提案する核であるとする．簡単のために次元は下がるとしよう．その際，ジャンプ先の次元と同じ次元を持った部分多様体上でしか \\(q_m\\) によるジャンプは起こってはいけないことが要求される．\n\n\n2.3 ジャンプによる方法\n\n2.3.1 Jump-Diffusion を用いた方法\n次元の違う空間をジャンプによって往来するという発想自体は (Grenander and Miller, 1994) が画像認識の分野で提案している．この論文では birth-death の代わりに creation-annihilation と呼ばれている．\nジャンプは Poisson レートに従って起こり，それ以外の間は Langevin 拡散によって平衡分布からのサンプリングを行う．\n後にこれを一般のモデル選択に応用したものが (Phillips and Smith, 1995) で議論されており，(Green, 1995) の可逆超次元 MCMC 2.2 の特別な場合に当たる (Green, 1995, p. 716)．\nなお，MALA (Metropolis-adjusted Langevin algorithm) はこの論文 (Grenander and Miller, 1994) へのコメント (J. E. Besag, 1994) で提案されたものである．\n\n\n2.3.2 PDMP への Jump の導入\nJump-Diffusion と全く同様の発想で，PDMP に可逆なジャンプを導入することを提案したのが (Chevallier et al., 2023) である．\nただし (Grenander and Miller, 1994) の提案のように，特定のレートに従ってジャンプをするのではなく，特定の active boundary に到達したら一定の確率でジャンプをするというようにトリガーを決める．\nこうすることで超次元ジャンプを起こすために追加の Poisson 過程をシミュレートする必要がなくなり，計算コストが落ちる．(Grenander and Miller, 1994) の方法は，その意味で大変計算コストが高いものだったのである．\nそして (Chevallier et al., 2023) の Reversible Jump PDMP では，ジャンプする度に「いつ元のモデルに戻ってくるか」を決める指数到着時間をシミュレートし，それまでの間別のモデルに滞在する，という動きをする．\n\n\n\n2.4 混合モデルへの応用\n\n2.4.1 Split-Merge の導入\n前述の通り (Richardson and Green, 1997) は混合モデルからのサンプリングに超次元 MCMC を応用している．\nここでは超次元跳躍は，新たな混合成分の導入／削除に用いられる．(Richardson and Green, 1997) は birth-death と呼んでいる．\nそして混合成分が追加／削除される前後に，既存の成分の分割／合併 (split-merge) を行うことで，混合モデルからのサンプリングを行う．\nこの際 (Richardson and Green, 1997) は２次までの積率を変えないように split-merge を行うことを提案しており，これを 積率マッチング (moment matching) ともいう (Fan et al., 2024, p. 10)．\n\n\n2.4.2 Birth-Death 点過程によるサンプリング\n(Stephens, 2000) は混合モデルのパラメータを，複数の \\(\\Theta\\) 上の点を重み付きで足し合わせたものと見ることで，\\(\\Theta\\) 上の事後分布を \\(\\Theta\\) 上の \\([0,1]\\)-印付き点過程をシミュレーションすることを通じて得られることを示した．\n実はこの見方は，第 2.2 節のサンプラーの極限として得られるものである (Cappé et al., 2003)．\n同様のメカニズムを，総リスク数不明の競合リスクモデルに PDMP サンプラーと組み合わせて，ベイズモデル平均に適用したのが (Hardcastle et al., 2024) である．\n\n\n\n2.5 積空間上の方法\n\n2.5.1 ベイズモデルとしての吸収\n超次元 MCMC と同じタイミング (Carlin and Chib, 1995) で，比較したい２つのモデルがあった場合，これらを結合し，足りない specification には “psudo-prior” を設定することで一つの巨大なモデルとみなし，Gibbs サンプラーによって推論するという発想があった．\nこの考え方は現代もモデル平均につながるが，比較したいモデルが多かったり，\\(K\\) を無限大にしたい場合は pseudo-prior の設定が煩雑である．\nということでこの方法は必ずしもスケールしないが，超次元 MCMC を包含する広いクラスの手法を特別な場合として含む (Godsill, 2001)．\n\n\n2.5.2 population-based method\npopulation-based methods の考え方を導入することで，超次元 MCMC の収束を加速することができる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#文献案内",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#文献案内",
    "title": "超次元 MCMC",
    "section": "4 文献案内",
    "text": "4 文献案内\n\nTrans-dimensional MCMC のトピックは，MCMC の黎明期の歴史に深く関わっている．\n(J. Besag and Green, 1993)\nSticky PDMP の間は極めて画期的なアイデアになり，今後数年で PDMP サンプラーをベイズ推論の workhorse algorithm に押し上げるポテンシャルがあるものと筆者は考えている．\n実際，連続時間 MCMC アルゴリズムは従来法と大きく違い，収束が多少早い程度ではコミュニティへの浸透が遅いと思われていたが，モデル選択や高次元・多峰性分布への推論に特に優れた応用を見せ始めた今，その重要性が高まっていると言えるだろう．"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html",
    "href": "posts/2024/Slides/IRT-ZigZag.html",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "",
    "text": "Output from anim_traj() in PDMPFlux.jl package\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#zig-zag-サンプラー",
    "href": "posts/2024/Slides/IRT-ZigZag.html#zig-zag-サンプラー",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "",
    "text": "Output from anim_traj() in PDMPFlux.jl package\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Slides/Master_Slides.html#等周不等式",
    "href": "posts/2024/Slides/Master_Slides.html#等周不等式",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "1.1 等周不等式",
    "text": "1.1 等周不等式\n体積測度 \\mu が等しい可測集合のうち，球が最小の測度を持つ．\n\n\n\n\nBorel 可測集合 A\\subset\\mathbb{R}^n に関して，\\epsilon-閉近傍 を次のように定める： \nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\mid d(x,A)\\le t\\right\\},\\qquad t&gt;0.\n\n実は A は Borel 可測とは限らないが，Lebesgue 可測ではある．\n\n\n\n\n\n\n\n\n古典的集中不等式 (Schmidt, 1948)-(Lévy, 1951)\n\n\nn-次元球面 S^n\\subset\\mathbb{R}^{n+1} に関して，A\\subset S^n を Borel 可測，C を同体積の（測地）球とすると， \n\\mu(C_\\epsilon)\\le\\mu(A_\\epsilon),\\qquad\\epsilon&gt;0."
  },
  {
    "objectID": "posts/2024/Slides/Master_Slides.html#正規分布に関する等周不等式",
    "href": "posts/2024/Slides/Master_Slides.html#正規分布に関する等周不等式",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "1.2 正規分布に関する等周不等式",
    "text": "1.2 正規分布に関する等周不等式\n\n\n\n\n(Giné and Nickl, 2021, p. 31) 定理 2.2.3\n\n\n\\gamma_n を \\mathbb{R}^n 上の標準正規分布とする．A\\subset\\mathbb{R}^n を Borel 可測， \nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\qquad a\\in\\mathbb{R},u\\in\\mathbb{R}^n\\setminus\\{0\\},\n を同体積の affine 半空間 とすると， \n\\gamma_n(H_a+\\epsilon B^n)\\le\\overline{\\gamma_n}(A_\\epsilon+\\epsilon B^n),\\qquad\\epsilon&gt;0.\n\n\n\n\n\n\\mathbb{R}^n だけでなく \\mathbb{R}^\\infty 上でも成り立つ．"
  },
  {
    "objectID": "posts/2024/Slides/Master_Slides.html#中央値周りへの集中不等式",
    "href": "posts/2024/Slides/Master_Slides.html#中央値周りへの集中不等式",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "1.3 中央値周りへの集中不等式",
    "text": "1.3 中央値周りへの集中不等式\n\n\n\n\n(Borell, 1975)-(Sudakov and Tsirel’son, 1974)\n\n\n\\{X_t\\}_{t\\in T} を可分な中心 Gauss 過程で，ほとんど確実に上限 \\|X\\|_\\infty が有限であるとする．このとき，\\|X\\|_\\infty の中央値 M に関して，1 \n\\operatorname{P}\\biggl[\\biggl|\\|X\\|_\\infty-M\\biggr|&gt;u\\biggr]\\le\\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right),\\qquad u&gt;0,\\sigma^2:=\\sup_{t\\in T}\\mathrm{V}[X_t].\n\n\n\n\n\n同様の命題を平均値の周りに関しても示せる．係数 2 が前につくものは (Gross, 1975) による正規分布に関する対数 Sobolev 不等式から導ける．\nこの設定では \\|X\\|_\\infty は連続分布をもち，M は一意に定まる．"
  },
  {
    "objectID": "posts/2024/Slides/Master_Slides.html#劣指数エルゴード定理",
    "href": "posts/2024/Slides/Master_Slides.html#劣指数エルゴード定理",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "2.1 劣指数エルゴード定理",
    "text": "2.1 劣指数エルゴード定理\n\n\n\n\nステートメント\n\n\nE を Polish距離空間，X を Feller-Dynkin 過程とする．連続関数 V:E\\to[1,\\infty) が存在して後述の２条件を満たすならば，任意の T&gt;0 に対して C&gt;0 が存在して次が成り立つ： \n\\|P^t(x,-)-P^t(y,-)\\|_\\mathrm{TV}\\le C\\frac{V(x)+V(y)}{\\lambda(t)},\\qquad x,y\\in E,t\\ge T.\n \n\\lambda(t):=\\Phi^{-1}(t),\\Phi(u):=\\int^u_1\\frac{ds}{\\phi(s)}.\n\n\n\n\n\nこの V は ドリフト関数 ともいう．証明法は (Kulik, 2018) が扱う skelton 連鎖 X_n:=X_{hn}\\;(h&gt;0,n=1,2,\\cdots) に帰着する方法と，再起過程 (regeneration process) を用いた (Hairer, 2021) による直接的方法がある．"
  },
  {
    "objectID": "posts/2024/Slides/Master_Slides.html#成立条件",
    "href": "posts/2024/Slides/Master_Slides.html#成立条件",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "2.2 成立条件",
    "text": "2.2 成立条件\n\n\n\n\n成立条件\n\n\n\n条件１：弱いドリフト条件\nある K\\in\\mathbb{R} と全射かつ単調増加な狭義凹関数 \\phi:\\mathbb{R}_+\\to\\mathbb{R}_+ が存在して \nV(X_t)-Kt+\\int^t_0\\phi(V(X_s))\\,ds\n は任意の x\\in E に関して \\operatorname{P}_x-優マルチンゲールである．\n条件２：強い局所 Dobrushin 条件\n任意の c\\ge1 に関して下部集合 V^{-1}([1,c]) はコンパクトで，ある h&gt;0 が存在して P^h は V^{-1}([1,c]) 上局所 Dobrushin である： \n\\sup_{(x,y)\\in B_c}\\|P^h(x,-)-P^h(y,-)\\|_\\mathrm{TV}&lt;2,\\qquad B_c:=\\left\\{(x,y)\\in E^2\\mid V(x)+V(y)\\le c\\right\\}."
  },
  {
    "objectID": "posts/2024/Slides/Master_Slides.html#langevin-拡散のエルゴード性",
    "href": "posts/2024/Slides/Master_Slides.html#langevin-拡散のエルゴード性",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "2.3 Langevin 拡散のエルゴード性",
    "text": "2.3 Langevin 拡散のエルゴード性"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brms-リンク集-.unnumbered-unlisted",
    "href": "posts/2024/Computation/brms.html#brms-リンク集-.unnumbered-unlisted",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "1 brms リンク集 {.unnumbered unlisted}",
    "text": "1 brms リンク集 {.unnumbered unlisted}\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nbrms: Bayesian Regression Models using ‘Stan’ リンク集\n\n\n\n\nr-project\nDocumentation\nGitHub\ndiscourse\n(Bürkner, 2017), (Bürkner, 2018), (Bürkner, 2021)\n\n\n\nダウンロードは：\ninstall.packages(\"brms\")"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#カウントデータで学ぶ-brms-の使い方",
    "href": "posts/2024/Computation/brms.html#カウントデータで学ぶ-brms-の使い方",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "1 カウントデータで学ぶ brms の使い方",
    "text": "1 カウントデータで学ぶ brms の使い方\n\n1.1 モデルの概要\nDocumentation で紹介されている Epilepsy Seizures Data (Leppik et al., 1987)，(Thall and Vail, 1990) を用いた 例 を実行してみる：\n\nlibrary(brms)\nformula1 &lt;- bf(count ~ zAge + zBase * Trt + (1|patient))\nfit1 &lt;- brm(formula = formula1, data = epilepsy, family = poisson())\n\n\n1.1.1 formula について\nてんかん (epilepsy) 患者の発作回数 count を被説明変数とし，処置の有無を表す説明変数 Trt と患者毎のランダム誤差を表す切片項 (1|patient)，及び標準化された説明変数 zAge, zBase への依存構造を調べたい．\n\n\n\n\n\n\n説明変数\n\n\n\n\nzAge：標準化された年齢\nzBase：ベースの発作回数\nTrt：治療の有無を表す２値変数\n(1|patient)：患者ごとに異なるとした変動切片項\n\nzBase * Trtという記法は，この２つの交互作用もモデルに含めることを意味する．この項の追加により，モデルは zBase の違いに応じて Trt の効果が変わる度合い \\(\\beta_4\\) を取り入れることができる．\n\n\nこのような処置効果 \\(\\beta_3\\) を調べるモデルでは，回帰係数を（因果）効果 (effect) とも呼ぶことに注意．\n\nkable(head(epilepsy))\n\n\n\n\nAge\nBase\nTrt\npatient\nvisit\ncount\nobs\nzAge\nzBase\n\n\n\n\n31\n11\n0\n1\n1\n5\n1\n0.4249950\n-0.7571728\n\n\n30\n11\n0\n2\n1\n3\n2\n0.2652835\n-0.7571728\n\n\n25\n6\n0\n3\n1\n2\n3\n-0.5332740\n-0.9444033\n\n\n36\n8\n0\n4\n1\n4\n4\n1.2235525\n-0.8695111\n\n\n22\n66\n0\n5\n1\n7\n5\n-1.0124085\n1.3023626\n\n\n29\n27\n0\n6\n1\n5\n6\n0.1055720\n-0.1580352\n\n\n\n\n\n\n\n\n\n\n\nデータの詳細\n\n\n\nepilepsyは59 人の患者に関して，４回の入院時の発作回数を記録した，全 236 データからなる．patientが患者を識別する ID であり，(1|patient)は患者ごとのランダム効果ということになる．\n\n\n\n\n1.1.2 family=poisson() について\n被説明変数 count は離散変数であり，Poisson 分布に従うと仮定する．過分散への対応を次の段階で考慮する．\n従って本モデルはzAge, zBase, Trt, Trt*zBaseという固定効果（係数），(1|patient)というランダム効果を取り入れた（一般化線型）混合効果モデル である．回帰式は次の通り： \\[\ny_{it} = \\beta_1 \\cdot\\texttt{zAge}_i+ \\beta_2 \\cdot \\texttt{zBase}_i + \\beta_3 \\cdot \\texttt{Trt}_i\n\\] \\[\n+ \\beta_4 \\cdot (\\texttt{zBase}_i \\cdot \\texttt{Trt}_i) + \\alpha_i +\\epsilon_{it}.\n\\] ただし，\\(\\texttt{count}_{it}\\) の Poisson 母数を \\(\\lambda_{it}\\) として，\\(y_{it}:=\\log(\\lambda_{it})\\) とした．\nfamily=poisson() は次の略記である：\nfamily = brmsfamily(family = \"&lt;family&gt;\", link = \"&lt;link&gt;\")\n多くの場合 link 引数は省略可能である．この２つの情報を通じて，一般化線型モデルを取り扱うことができる．\n\n\n\n1.2 モデルの推定と結果の解釈\nbrm() 関数による推定結果は，返り値として渡される brmsfit オブジェクトに対して summary() メソッドを適用して見ることができる：\n\nsummary(fit1)\n\n Family: poisson \n  Links: mu = log \nFormula: count ~ zAge + zBase * Trt + (1 | patient) \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~patient (Number of levels: 59) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.58      0.07     0.46     0.75 1.00     1016     1779\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.78      0.12     1.54     2.02 1.01      959     1321\nzAge           0.09      0.09    -0.08     0.26 1.01      765     1399\nzBase          0.70      0.12     0.47     0.93 1.01      813     1369\nTrt1          -0.27      0.17    -0.61     0.06 1.00      817     1205\nzBase:Trt1     0.05      0.16    -0.26     0.36 1.01      819     1692\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n基本的な解析の前提がまず出力され，推定結果はグループレベル変数（今回は患者ごとの変量効果 \\(\\alpha_i\\)，コードだと (1|patient)）から表示される．\n後半に固定効果の係数，特にユニットレベルの回帰係数と切片項の推定結果が表示される．\n結果の解釈をしてみる．治療効果Trtの係数は負で，平均的に処置効果はある可能性があるが，95% 信頼区間は \\(0\\) を跨いでいるという意味では，有意とは言えない．\nまた交差項zBase*Trtの係数は小さく，交互効果の存在を示す証拠はないと思われる．\n\\(\\widehat{R}\\) (Gelman and Rubin, 1992) とは MCMC の収束に関する指標で，１より大きい場合，MCMC が収束していない可能性を意味する (Vehtari et al., 2021)．通説には \\(\\widehat{R}\\le1.1\\) などの基準がある．\n\n\n1.3 プロット\n変数を指定して，事後分布と MCMC の軌跡をプロットできる：\n\nplot(fit1, variable = c(\"b_Trt1\", \"b_zBase\", \"b_zBase:Trt1\"))\n\n\n\n\n\n\n\n\nより詳しく見るにはconditional_effects関数を用いることもできる．\n\nconditional_effects(fit1, effects = \"zBase:Trt\")\n\n\n\n\n\n\n\n図 1: 交差項の効果（ベースレートの違いによる処置効果の違い）\n\n\n\n\n\n処置群 Trt=1 の方がカウントは減っているのは見えるが，モデルの不確実性に比べてその減少量は十分に大きいとは言えない．\nベースレート zBase が大きいほどカウントは大きい．しかしベースレートが大きいほど処置効果も大きい（交互効果がある）ようには見えない．\n\n\n1.4 モデルによる予測\nfit したモデル fit1 を用いて，平均年齢と平均ベースレートを持つ患者に対する治療効果を予測する：\n\nnewdata &lt;- data.frame(Trt = c(0, 1), zAge = 0, zBase = 0)\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5  Q97.5\n[1,]   5.9710  2.530960    2 11.025\n[2,]   4.5425  2.178275    1  9.000\n\n\n関数predict() は事後予測分布からのサンプリングを１回行う．一方で，関数fitted() は事後予測分布の平均を返す．1\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.953824  0.712907 4.682468 7.518386\n[2,] 4.525703  0.545437 3.518021 5.614800\n\n\n\n\n1.5 モデルの比較\nモデルfit1で行った Poisson 回帰分析は，fit1に含めた説明変数の違いを除けば，個々の観測が独立になる，という仮定の上に成り立っている（第 4.3 節）．\nこの仮定が破れているとき＝全ての説明変数をモデルに含めきれていないとき，Poisson 分布の性質 \\[\n\\operatorname{E}[X]=\\mathrm{V}[X]=\\lambda\\qquad (X\\sim\\mathrm{Pois}(\\lambda))\n\\] からの離反として現れ，この現象は 過分散（overdispersion）とも呼ばれる．\n\n1.5.1 観測レベルランダム効果\nということで，他の未観測の説明変数が存在した場合を想定して， Poisson 分布族ではなく，分散が平均よりも大きいような別の分布族を用いて，フィット度合いを比較してみることを考えたい．\nそこで追加の変動をモデルに追加するべく，モデルfit1に観測ごとの切片項 \\(\\eta_{it}\\) を追加してみる（この手法は観測レベルランダム効果と呼ばれる．第 3.3 節参照）．\n\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\nこうして得た２つのモデル fit1,fit2 を比較する．\nLLO (Leave-One-Out) 交差検証 (cross-validation) が関数 loo によって実行できる：\nloo(fit1, fit2)\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit1, fit2)\n\nWarning: Found 8 observations with a pareto_k &gt; 0.7 in model 'fit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 68 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit1':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -672.1 37.0\np_loo        94.9 14.9\nlooic      1344.2 74.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     228   96.6%   140     \n   (0.7, 1]   (bad)        7    3.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   1    0.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -596.2 13.9\np_loo       108.4  7.1\nlooic      1192.4 27.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.7]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     168   71.2%   201     \n   (0.7, 1]   (bad)       62   26.3%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   6    2.5%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2   0.0       0.0  \nfit1 -75.9      27.6  \n\n\n\n\n\nelpd_diff は expected log posterior density の差異を表す．fit2 の方が大きく当てはまりが良いことが見て取れる．\nまた WAIC (Watanabe-Akaike Information Criterion) も実装されている：\nprint(waic(fit1))\n\n\n\n\n\n\nWAIC の結果\n\n\n\n\n\n\nprint(waic(fit1))\n\nWarning: \n51 (21.6%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -669.0 36.9\np_waic        91.8 14.8\nwaic        1338.0 73.8\n\n51 (21.6%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nprint(waic(fit2))\n\nWarning: \n68 (28.8%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -573.0 12.0\np_waic        85.3  5.1\nwaic        1146.0 24.0\n\n68 (28.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\n\n\n\n他にも，reloo, kfold などの関数もある．\n\n\n\n\n\n\n他の関数一覧\n\n\n\n\n\n\nmethods(class=\"brmsfit\")\n\n [1] add_criterion           add_ic                  as_draws_array         \n [4] as_draws_df             as_draws_list           as_draws_matrix        \n [7] as_draws_rvars          as_draws                as.array               \n[10] as.data.frame           as.matrix               as.mcmc                \n[13] autocor                 bayes_factor            bayes_R2               \n[16] bridge_sampler          coef                    conditional_effects    \n[19] conditional_smooths     control_params          default_prior          \n[22] expose_functions        family                  fitted                 \n[25] fixef                   formula                 getCall                \n[28] hypothesis              kfold                   log_lik                \n[31] log_posterior           logLik                  loo_compare            \n[34] loo_linpred             loo_model_weights       loo_moment_match       \n[37] loo_predict             loo_predictive_interval loo_R2                 \n[40] loo_subsample           loo                     LOO                    \n[43] marginal_effects        marginal_smooths        mcmc_plot              \n[46] model_weights           model.frame             nchains                \n[49] ndraws                  neff_ratio              ngrps                  \n[52] niterations             nobs                    nsamples               \n[55] nuts_params             nvariables              pairs                  \n[58] parnames                plot                    post_prob              \n[61] posterior_average       posterior_epred         posterior_interval     \n[64] posterior_linpred       posterior_predict       posterior_samples      \n[67] posterior_smooths       posterior_summary       pp_average             \n[70] pp_check                pp_mixture              predict                \n[73] predictive_error        predictive_interval     prepare_predictions    \n[76] print                   prior_draws             prior_summary          \n[79] psis                    ranef                   reloo                  \n[82] residuals               restructure             rhat                   \n[85] stancode                standata                stanplot               \n[88] summary                 update                  VarCorr                \n[91] variables               vcov                    waic                   \n[94] WAIC                   \nsee '?methods' for accessing help and source code\n\n\n\n\n\n\n\n1.5.2 患者内の相関構造のモデリング\nまた fit1 において，同一患者の異なる訪問の間には全く相関がないと仮定されており，これは全く非現実的な仮定をおいてしまっていると言える．2\n患者内の相関構造は，brm() 関数の autocor 引数で指定できる（第 4.3.2 節）．\n例えば，全く構造を仮定しない場合は unstrを指定する：\n\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\nこのモデルも fit1 より遥かに当てはまりが良く，fit2 とほとんど同じ当てはまりの良さが見られる：\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit2,fit3)\n\nWarning: Found 68 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 57 observations with a pareto_k &gt; 0.7 in model 'fit3'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -596.2 13.9\np_loo       108.4  7.1\nlooic      1192.4 27.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.7]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     168   71.2%   201     \n   (0.7, 1]   (bad)       62   26.3%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   6    2.5%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit3':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -600.8 15.0\np_loo       112.1  8.2\nlooic      1201.6 29.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.5]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     179   75.8%   196     \n   (0.7, 1]   (bad)       47   19.9%   &lt;NA&gt;    \n   (1, Inf)   (very bad)  10    4.2%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit3 -4.6       2.9   \n\n\n\n\n\n思ったよりも fit2 の当てはまりが良いため，Poisson-対数正規混合モデリングを本格的に実施してみることが次の選択肢になり得る（第 3.3 節参照）．\n\n\n\n1.6 その他の例\nSebastian Weber らにより，新薬の治験における実際の解析事例をまとめたウェブサイト が公開されている．3\n特にその 13 章 では同様の経時的繰り返し観測データを扱っているが，ここではカウントデータではなく連続な応用変数が扱われている．\nbrms は他にもスプラインや Gauss 過程を用いた一般化加法モデルの推論も可能である (Bürkner, 2018)．"
  },
  {
    "objectID": "posts/2024/Life/Bio.html",
    "href": "posts/2024/Life/Bio.html",
    "title": "生い立ち",
    "section": "",
    "text": "1999 年に横浜市磯子区に生まれる． \n物心がついてきた頃，当時は「高校生クイズ」なる番組で開成高校が大活躍しており，その世界に入りたいと思うようになる．高校生たちが紙とペンだけから，たった5分で「宇宙の年齢を推定せよ」という問題にも「このヒエログリフ碑文を読解せよ」という問題にも糸口を見つけて正解に辿りつく姿には大変憧れた． \n小学３年生で東京に引っ越し，無事その後開成中学に入学．憧れていたクイズに打ち込み，自身も高校生クイズに出場することが叶うが，アメリカ進出手前で敗退．番組も学問的な内容から方針を変えてしまっており，「憧れの舞台」というものがすでに失くなってしまっていたことも少し残念であった．\nしかし高校の頃はよく本を読むことは心がけた．期末試験前で授業がなくなるタイミングなどは絶賛の好機であった．統計を通じて「履歴書が厚手で紙質が良いほど合格が出やすい」などという人間行動の不思議が暴けることに驚き，統計科学の研究者に憧れたものだった．\nそこで部活引退後は学問を志すようになる．数学で自然の音が聴こえるようになりたいと思い，浪人期は物理に熱中した．原島力学 や ファインマン力学 が面白く，自分も物理学がやりたいと思うようになるが，肝心の「自然現象にはあまり興味がない」という難点があった．物理学以外の応用数学分野も知らなかったので，ひとまず大学入学後は理学部物理学科に進もうと思っていた．\n\n入学後，計算機が世間の潮流だと知り，ここに「新時代の応用数理」が眠っているのではないかと思うようになる．ということで計算機関連の数学がやりたいと思うようになるが，東京大学の 前期教養学部での数学の授業 が始まると「これだ」と直覚する．自分はずっと数学の応用が好きだが，最初の「数学の応用」は数学の内部で起こることに気づき，学部は絶対数学科だと進学を決意．\n５月の数学科のガイダンスで，「応用に興味ある者にとって，数学科と他の学科との最大の違いは，ここでなら『数学に軸足を置いた応用』ができる」との言葉が背中を押したことも大きい．応用先が曖昧であった自分にとって，「軸足」の語がしっくり来た．入学１ヶ月の時点でもう堅く決意していたため，教養の統計学の授業も一切取らず，数学科目に集中した．数学原論 の出版に伴い開講された 圏と層 に巡り合い，より一層のめり込んでいく．\n\n\n  \n    数学科に進むかどうかは点数の高低ではなく来年の夏に死がどれぐらい怖いかによって決定される— Hirofumi Shiba (@ano2math5)\n    May 11, 2019\n  \n\n\n無事数学科に進学したものの，家庭環境の急変から経済的にも学業的にも休めない状況が続き，大学３年の５月にパニック障害を患う．追試をくらった代数と幾何が半ばトラウマになり，自分は確率統計で食べていくのだと思うようになる．しかし治療に専念していた間でも，Pedersen の関数解析の本 だけは読めた．ずっと 基礎論 や 計算機科学 が好きであったが，解析が面白いと思ったのはこれが初めてだった．\n４年の講究では Nualart の Malliavin 解析の本 を読んだ．３年次は人生を立て直すので精一杯で授業が取れず，確率論は全て独学であったから確率過程の概念に大変難儀したが，確率解析と Malliavin 解析は関数解析の知識が非常に役に立ち，大変に面白かった．\nしかし自分はやはり「応用先としての数学」には何も興味を持てず，かといって前例の少ない進学をするための準備時間も取れなかった．そんな中で，指導教員から鎌谷先生の存在と計算統計学の分野を知り，統数研の受験を決意する．たまたまこの年は統数研の改組があったため冬の受験のみであり，その頃には病は快方に向かっていたためよく対策できたのである．\n統数研では初年度から多くの学会に出させてもらったことがかけがえのない経験になった．MCMC の概念をなんとなく把握できた程度だった夏の ICIAM の ４日目 のセッションで，「粒子系を輸送する」という最適輸送の見方を取り入れた最適化ベースのサンプリング法を知り衝撃を受けた．サンプリングは確率分布の空間 \\(\\mathcal{P}(X)\\) 上の 力学系と見れる と気づいて大変興味を持った．\nサンプリング法も輸送もとびきりに面白かった．これで機械学習というものを一気に身近に感じて本格的に興味を持ち，年度末に機械学習分野最大のサマースクール MLSS2024 に参加するため，なんとかポスター発表を用意した．今思うとこれがその後の研究の方向性を決定付けた．誘ってもらった清水さんには感謝が尽きない．\nサマースクールでは全く知らない・興味のない分野の授業も聞くことになり，これが思いもしなかった出会いに導いてくれるのが美点であった．Francesco Orabona 氏の最適化の授業と Marco Cuturi 氏の最適輸送の授業とは当時から面白かったが，グラフニューラルネットワークと群論による位相的機械学習の授業も後から効いてきた．「数学は一つ」という思いを強めるばかりであった．"
  },
  {
    "objectID": "static/Bio.html",
    "href": "static/Bio.html",
    "title": "生い立ち",
    "section": "",
    "text": "1999 年に横浜市磯子区に生まれる． \n物心がついてきた頃，当時は「高校生クイズ」なる番組で開成高校が大活躍しており，その世界に入りたいと思うようになる．高校生たちが紙とペンだけから，たった5分で「宇宙の年齢を推定せよ」という問題にも「このヒエログリフ碑文を読解せよ」という問題にも糸口を見つけて正解に辿りつく姿には大変憧れた． \n小学３年生で東京に引っ越し，無事その後開成中学に入学．憧れていたクイズに打ち込み，自身も高校生クイズに出場することが叶うが，アメリカ進出手前で敗退．番組も学問的な内容から方針を変えてしまっており，「憧れの舞台」というものがすでに失くなってしまっていたことも少し残念であった．\nしかし高校の頃はよく本を読むことは心がけた．期末試験前で授業がなくなるタイミングなどは絶賛の好機であった．統計を通じて「履歴書が厚手で紙質が良いほど合格が出やすい」などという人間行動の不思議が暴けることに驚き，統計科学の研究者に憧れたものだった．\nそこで部活引退後は学問を志すようになる．数学で自然の音が聴こえるようになりたいと思い，浪人期は物理に熱中した．原島力学 や ファインマン力学 が面白く，自分も物理学がやりたいと思うようになるが，肝心の「自然現象にはあまり興味がない」という難点があった．物理学以外の応用数学分野も知らなかったので，ひとまず大学入学後は理学部物理学科に進もうと思っていた．\n\n入学後，計算機が世間の潮流だと知り，ここに「新時代の応用数理」が眠っているのではないかと思うようになる．ということで計算機関連の数学がやりたいと思うようになるが，東京大学の 前期教養学部での数学の授業 が始まると「これだ」と直覚する．自分はずっと数学の応用が好きだが，最初の「数学の応用」は数学の内部で起こることに気づき，学部は絶対数学科だと進学を決意．\n５月の数学科のガイダンスで，「応用に興味ある者にとって，数学科と他の学科との最大の違いは，ここでなら『数学に軸足を置いた応用』ができる」との言葉が背中を押したことも大きい．応用先が曖昧であった自分にとって，「軸足」の語がしっくり来た．入学１ヶ月の時点でもう堅く決意していたため，教養の統計学の授業も一切取らず，数学科目に集中した．数学原論 の出版に伴い開講された 圏と層 に巡り合い，より一層のめり込んでいく．\n\n\n  \n    数学科に進むかどうかは点数の高低ではなく来年の夏に死がどれぐらい怖いかによって決定される— Hirofumi Shiba (@ano2math5)\n    May 11, 2019\n  \n\n\n無事数学科に進学したものの，家庭環境の急変から経済的にも学業的にも休めない状況が続き，大学３年の５月にパニック障害を患う．追試をくらった代数と幾何が半ばトラウマになり，自分は確率統計で食べていくのだと思うようになる．しかし治療に専念していた間でも，Pedersen の関数解析の本 だけは読めた．ずっと 基礎論 や 計算機科学 が好きであったが，解析が面白いと思ったのはこれが初めてだった．\n４年の講究では Nualart の Malliavin 解析の本 を読んだ．３年次は人生を立て直すので精一杯で授業が取れず，確率論は全て独学であったから確率過程の概念に大変難儀したが，確率解析と Malliavin 解析は関数解析の知識が非常に役に立ち，大変に面白かった．\nしかし自分はやはり「応用先としての数学」には何も興味を持てず，かといって前例の少ない進学をするための準備時間も取れなかった．そんな中で，指導教員から鎌谷先生の存在と計算統計学の分野を知り，統数研の受験を決意する．たまたまこの年は統数研の改組があったため冬の受験のみであり，その頃には病は快方に向かっていたためよく対策できたのである．\n統数研では初年度から多くの学会に出させてもらったことがかけがえのない経験になった．MCMC の概念をなんとなく把握できた程度だった夏の ICIAM の ４日目 のセッションで，「粒子系を輸送する」という最適輸送の見方を取り入れた最適化ベースのサンプリング法を知り衝撃を受けた．サンプリングは確率分布の空間 \\(\\mathcal{P}(X)\\) 上の 力学系と見れる と気づいて大変興味を持った．\nサンプリング法も輸送もとびきりに面白かった．これで機械学習というものを一気に身近に感じて本格的に興味を持ち，年度末に機械学習分野最大のサマースクール MLSS2024 に参加するため，なんとかポスター発表を用意した．今思うとこれがその後の研究の方向性を決定付けた．誘ってもらった清水さんには感謝が尽きない．\nサマースクールでは全く知らない・興味のない分野の授業も聞くことになり，これが思いもしなかった出会いに導いてくれるのが美点であった．Francesco Orabona 氏の最適化の授業と Marco Cuturi 氏の最適輸送の授業とは当時から面白かったが，グラフニューラルネットワークと群論による位相的機械学習の授業も後から効いてきた．「数学は一つ」という思いを強めるばかりであった．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html#計算",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html#計算",
    "title": "階層ベイズ理想点解析",
    "section": "3 計算",
    "text": "3 計算\nモデル (1), (2) の尤度 \\[\np(y_{ij}|z_i,\\alpha_j,\\beta_j,\\gamma_g,g,\\sigma)\n\\] は \\(g\\) に関してのみ微分可能でない．\nそこで基本的には \\(\\gamma\\in\\mathbb{R}^p\\) と \\(\\alpha_j,\\beta_j\\in\\mathbb{R},\\sigma\\in\\mathbb{R}_+\\) のサンプリングには Sticky PDMP サンプラー (Chevallier et al., 2023), (Bierkens et al., 2023) を用い，\\(g\\in [G]^{[N]}\\) のサンプリングには次の２つの時計を追加して行う： \\[\n\\Lambda^S(t):=\\Lambda_0^S\\left(1\\land\\frac{p(g')}{p(g)}\\right),\n\\] \\[\n\\Lambda^K(t)=\\Lambda_0^K.\n\\]\n\\(\\Lambda^S\\) により候補 \\(g'\\in [G]^{[N]}\\) への遷移を行い，\\(\\Lambda^K\\) により候補 \\(g'\\in [G]^{[N]}\\) の更新をある確率核 \\(q_g(g,-)\\) に従って行う．\n広大な離散空間 \\([G]^{[N]}\\) 上を Poisson 跳躍により歩き回る．これは Zig-Zag within Gibbs (Sachs et al., 2023), (Hardcastle et al., 2024) の考え方である．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#はじめに",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "1 はじめに",
    "text": "1 はじめに\nZig-Zag サンプラーなどの PDMP サンプラーは \\(\\mathbb{R}^d\\) またはその領域上の確率分布からサンプリングするための，連続時間アルゴリズムである．\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n一方でこれらのサンプラーは離散空間上では使えない．\nその際は連続時間で動く PDMP と，Metropolis-Hastings 法などの従来の MCMC 手法を統合して動かす必要がある．\nこのように離散空間と連続空間の合併上で動くサンプラー（の一部）を (Tierney, 1994) は hybrid サンプラーと呼んでいる．\nしかしこの名前は hybrid Monte Carlo (Duane et al., 1987) と紛らわしいから，(Green, 1995, p. 714) から “traverse” sampler とここでは呼ぶことにする．\n(Sachs et al., 2023) は Zig-Zag サンプラーともう１つの離散時間 MCMC を，Gibbs 様の考え方で組み合わせた GZZ (Gibbs Zig-Zag) サンプラーを提案した（第 2 節）．\n一方で (Hardcastle et al., 2024) では，旧来は点過程からのサンプリングに用いられていた技術であった Birth-Death 過程を用いて統合する方法が提案されている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#はじめに",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#sec-GZZ",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#sec-GZZ",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "2 Gibbs Zig-Zag サンプラー",
    "text": "2 Gibbs Zig-Zag サンプラー\n\n2.1 はじめに\n\\(\\zeta\\in\\mathbb{R}^d\\) からのサンプリングを， \\[\n\\zeta=(\\xi,\\alpha)\\in\\mathbb{R}^p\\times\\mathbb{R}^r,\\qquad p+r=d,\n\\] というように分解して考え，\\(\\xi\\in\\mathbb{R}^p\\) には Zig-Zag サンプラーを適用するが，\\(\\alpha\\in\\mathbb{R}^r\\) にはしないとする．\nこのような例は階層モデル \\[\nX_i\\text{i.i.d.}p(x|\\xi),\\qquad\\xi|\\alpha\\sim p(\\xi|\\alpha),\\qquad\\alpha\\sim p(\\alpha),\n\\] の文脈で自然に現れる．実際，ポテンシャル（負の対数尤度関数）は \\[\nU(\\zeta)=U^0(\\xi,\\alpha)+\\sum_{i=1}^NU^i(\\xi),\n\\] \\[\nU^0(\\xi,\\alpha)=-\\log p(\\xi|\\alpha)-\\log p(\\alpha),\\qquad U^i(\\xi)=-\\log p(x_i|\\xi),\n\\] と表せる．\n\n\n2.2 サンプラーの設計\n\\(\\xi\\) の Zig-Zag サンプラーの生成作用素を \\(L_\\xi\\) とする．\\(\\alpha\\) からサンプリングをする MCMC の確率核を \\(Q\\) とし，ある定数 \\(\\eta&gt;0\\) をパラメータにもつ Poisson 点過程が到着するたびに \\(Q\\) により \\(\\alpha\\) の値を動かすとする．\nすると全体としてのサンプラーの生成作用素は次のように表せる： \\[\nL=L_\\xi+\\eta L_\\alpha,\n\\] \\[\nL_\\alpha f((\\xi,\\theta),\\alpha)=\\int_{\\mathbb{R}^r}\\biggr(f((\\xi,\\theta),\\alpha')-f((\\xi,\\theta),\\alpha)\\biggl)Q(\\alpha,d\\alpha').\n\\]"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#birth-death-サンプラー",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#birth-death-サンプラー",
    "title": "連続・離散時間を往来する MCMC サンプラー",
    "section": "3 Birth-Death サンプラー",
    "text": "3 Birth-Death サンプラー"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#関連記事",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#関連記事",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点解析パッケージ一覧",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点解析パッケージ一覧",
    "title": "理想点解析のハンズオン",
    "section": "3 理想点解析パッケージ一覧",
    "text": "3 理想点解析パッケージ一覧\n本節では次の３つのパッケージを紹介する：\n\n\n\n\n\n\n\npscl (Zeileis et al., 2008)：GitHub, CRAN．(Arnold, 2018) も参照．\nMCMCpack (Martin et al., 2011)：GitHub, CRAN\nemIRT (Imai et al., 2016)：GitHub, CRAN\n\n\n\n\n\n3.1 pscl パッケージ\ninstall.packages(\"pscl\")\n\n3.1.1 voteview データ\nこのパッケージでは，Keith T. Poole と Howard Rosenthal が 1995 年から運営しているサイト voteview.com のデータを利用するための関数 readKH() が提供されている．\n例えば連邦議会 (U.S. Congress) 117 議会期 (Congress) 2021.1.3-2023.1.3 の上院 (Senate) の点呼投票データを読み込むには以下のようにする：2\n\nlibrary(pscl)\ns117 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S117_votes.ord\",\n                desc=\"117th U.S. Senate\")\n\ns117 は rollcall オブジェクト，８つのフィールドを持った配列である．\ns117$votes データは \\(n=104\\) 議員の計 \\(m=949\\) 回の投票からなる \\(10\\)-値の行列である．\n\nsummary(s117)\n\n\nSummary of rollcall object s117 \n\nDescription:     117th U.S. Senate \nSource:      https://voteview.com/static/data/out/votes/S117_votes.ord \n\nNumber of Legislators:       104\nNumber of Roll Call Votes:   949\n\n\nUsing the following codes to represent roll call votes:\nYea:         1 2 3 \nNay:         4 5 6 \nAbstentions:     7 8 9 \nNot In Legislature:  0 \n\nParty Composition:\n    D Indep     R \n   50     2    52 \n\nVote Summary:\n               Count Percent\n0 (notInLegis)  3544     3.6\n1 (yea)        55542    56.3\n6 (nay)        35995    36.5\n7 (missing)        5     0.0\n9 (missing)     3610     3.7\n\nUse summary(s117,verbose=TRUE) for more detailed information.\n\n\n\n\n3.1.2 点呼投票データ\n点呼投票データとは \\(n\\times m\\) の行列で，そのエントリーは２値変数である（今回は \\(1\\) か \\(6\\)）．\nしかし実際には種々の欠測により，\\(0,7,9\\) も使われる．\nこれをヒートマップで可視化してみる．\n\nlibrary(tidyverse)\n\nvotes_df &lt;- as.data.frame(s117$votes[1:15, 1:15]) %&gt;% rownames_to_column(\"Legislator\")  # 投票データをデータフレームに変換し、行名を列として追加\n\nvotes_long &lt;- votes_df %&gt;% pivot_longer(cols = -Legislator, names_to = \"Vote\", values_to = \"value\")  # データを長形式に変換\n\n\nggplot(votes_long, aes(x = Vote, y = Legislator, fill = value)) + geom_tile() + scale_fill_gradient(low = \"white\", high = \"red\") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Votes\", y = \"Legislators\", title = \"Voting Patterns\")  # ヒートマップを作成\n\n\n\n\n\n\n\n\n\n\n3.1.3 政党毎の賛成率\n政党でソートし，賛成率を最初の 15 法案についてプロットしたものは次の通り：\n\n\nCode\nlibrary(dplyr)\n\n# 政党ごとの賛成票の割合を計算\nparty_votes &lt;- s117$votes %&gt;%\n  as.data.frame() %&gt;%\n  mutate(party = s117$legis.data$party) %&gt;%\n  group_by(party) %&gt;%\n  summarise(across(everything(), ~mean(. == 1, na.rm = TRUE)))\n\n# データを長形式に変換\nparty_votes_long &lt;- party_votes %&gt;% pivot_longer(cols = -party, names_to = \"Vote\", values_to = \"value\")\n\n# DとRのデータのみを抽出\nparty_votes_d &lt;- party_votes_long %&gt;% filter(party == \"D\")\nparty_votes_r &lt;- party_votes_long %&gt;% filter(party == \"R\")\n\n# Democrats (D) のデータのみをプロット\nggplot(party_votes_d, aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value)) +\n  geom_line(color = \"blue\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\",\n       title = \"Proportion of Yea votes for Democrats\")\n\n\n\n# Democrats (D) と Republicans (R) のデータを同じプロットに追加\nggplot() +\n  geom_line(data = party_votes_d[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Democrat\"), linewidth = 0.5) +\n  geom_line(data = party_votes_r[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Republican\"), linewidth = 0.5) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\", color = \"Party\",\n       title = \"Proportion of Yea votes by Party\")\n\n\n\n\n\n\n\n\n民主党の 0-1 がはっきりした投票行動が見られる．\n\ns109 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S109_votes.ord\",\n                desc=\"109th U.S. Senate\")\n\nAttempting to read file in Keith Poole/Howard Rosenthal (KH) format.\nAttempting to create roll call object\n109th U.S. Senate \n102 legislators and 645 roll calls\nFrequency counts for vote types:\nrollCallMatrix\n    0     1     6     7     9 \n  645 40207 22650     1  2287 \n\n\n\n\n3.1.4 ベイズ推定\npscl パッケージでは，rollcall オブジェクトに対して ideal() 関数を用いてデータ拡張に基づく Gibbs サンプラーを通じた理想点解析を行うことができる．\nideal() 関数のマニュアル に記載された例では maxiter=260E3, burnin=10E3, thin=100 での実行が例示されているが，ここでは簡単に実行してみる．\n\nn &lt;- dim(s117$legis.data)[1]\nx0 &lt;- rep(0,n)\nx0[s117$legis.data$party==\"D\"] &lt;- -1\nx0[s117$legis.data$party==\"R\"] &lt;- 1\n\nlibrary(tictoc)\ntic(\"ideal() fitting\")\n\nid1 &lt;- ideal(s117,\n             d=1,\n             startvals=list(x=x0),\n             normalize=TRUE,\n             store.item=TRUE,\n             maxiter=10000,  # MCMCの反復回数\n             burnin=5000,\n             thin=50,  # 間引き間隔\n             verbose=TRUE)\ntoc()\n\nideal() fitting: 43.938 sec elapsed であった．\n\nplot(id1)\n\nLooking up legislator names and party affiliations\nin rollcall object s117 \n\n\n\n\n\n\n\n\n\nplot.ideal() 関数のマニュアル にある通り，shoALLNames = FALSE がデフォルトになっている．\n\nsummary(id1)  # 全議員の正確な推定値が見れる．\n\nもっとも保守的な議員として Trump，５番目にリベラルな議員として Biden の名前がみえる．Harris は中道である．\n\n\n\n3.2 MCMCpack パッケージ\n\n3.2.1 ロジットモデルの推定\n\nlibrary(MCMCpack)\n# データの生成\nx1 &lt;- rnorm(1000)  # 説明変数1\nx2 &lt;- rnorm(1000)  # 説明変数2\nXdata &lt;- cbind(1, x1, x2)  # デザイン行列\n\n# 真のパラメータ\ntrue_beta &lt;- c(0.5, -1, 1)\n\n# 応答変数の生成\np &lt;- exp(Xdata %*% true_beta) / (1 + exp(Xdata %*% true_beta))\ny &lt;- rbinom(1000, 1, p)\n\n# MCMClogitでサンプリング\nposterior &lt;- MCMClogit(y ~ x1 + x2,    # モデル式\n                      burnin = 1000,    # バーンイン期間\n                      mcmc = 10000,     # MCMCの反復回数\n                      thin = 1,         # 間引き数\n                      verbose = 1000)   # 進捗表示間隔\n\n\n# 結果の確認\nsummary(posterior)\n\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean      SD  Naive SE Time-series SE\n(Intercept)  0.4464 0.07613 0.0007613       0.002471\nx1          -0.8935 0.08239 0.0008239       0.002677\nx2           0.9617 0.08706 0.0008706       0.002880\n\n2. Quantiles for each variable:\n\n               2.5%     25%     50%     75%   97.5%\n(Intercept)  0.2962  0.3953  0.4464  0.4972  0.6027\nx1          -1.0560 -0.9485 -0.8931 -0.8391 -0.7310\nx2           0.7910  0.9017  0.9608  1.0206  1.1369\n\nplot(posterior)\n\n\n\n\n\n\n\n\n\n\n3.2.2 変化点解析\n(Chib, 1998) に基づく変化点モデルのベイズ推定の関数 MCMCpoissonChange() も実装されている．詳しくは (Martin et al., 2011) 第4節参照．\n\n\n\n3.3 emIRT パッケージ\ninstall.packages(\"emIRT\")\nこのパッケージには備え付けの 80-110 議会期の上院における点呼投票データ dwnom がある．\nこのデータに対して，階層モデルを用いた理想点解析を行う関数 hierIRT() がある．\n\nlibrary(emIRT)\ndata(dwnom)\n\n## This takes about 10 minutes to run on 8 threads\n## You may need to reduce threads depending on what your machine can support\nlout &lt;- hierIRT(.data = dwnom$data.in,\n                    .starts = dwnom$cur,\n                    .priors = dwnom$priors,\n                    .control = {list(\n                    threads = 8,\n                    verbose = TRUE,\n                    thresh = 1e-4,\n                    maxit=200,\n                    checkfreq=1\n                        )})\n\n## Bind ideal point estimates back to legislator data\nfinal &lt;- cbind(dwnom$legis, idealpt.hier=lout$means$x_implied)\n\n## These are estimates from DW-NOMINATE as given on the Voteview example\n## From file \"SL80110C21.DAT\"\nnomres &lt;- dwnom$nomres\n\n## Merge the DW-NOMINATE estimates to model results by legislator ID\n## Check correlation between hierIRT() and DW-NOMINATE scores\nres &lt;- merge(final, nomres, by=c(\"senate\",\"id\"),all.x=TRUE,all.y=FALSE)\ncor(res$idealpt.hier, res$dwnom1d)"
  },
  {
    "objectID": "posts/2024/R/Stan2.html",
    "href": "posts/2024/R/Stan2.html",
    "title": "R 上の Stan インターフェイス",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#概観",
    "href": "posts/2024/R/Stan2.html#概観",
    "title": "R 上の Stan インターフェイス",
    "section": "概観",
    "text": "概観\nRStan は Rcpp や inline といったパッケージにより C++ を R から呼び出すことで，Stan とのインターフェイスを実現している．\n一方で CmdStanR は CmdStan という Stan のコマンドラインインターフェイスを R から呼び出すことで，Stan とのインターフェイスを実現している．"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#rstan-パッケージ",
    "href": "posts/2024/R/Stan2.html#rstan-パッケージ",
    "title": "R 上の Stan インターフェイス",
    "section": "1 RStan パッケージ",
    "text": "1 RStan パッケージ\n\n\n\n\n\n\nインストール方法\n\n\n\n\n\n詳しくは RStan Getting Started を参照ください．\nすでに存在する場合は，次を実行してからインストールします．\nremove.packages(\"rstan\")\nif (file.exists(\".RData\")) file.remove(\".RData\")\nインストールは，ほとんどの場合，次の１行で済みます：\ninstall.packages(\"rstan\", repos = \"https://cloud.r-project.org/\", dependencies = TRUE)\nRStan の利用のためには，c++ コンパイラが必要です．\nXCode コマンドラインツールをインストールすることにより，/Library/Developer/CommandLineTools/usr/bin に clang++ がインストールされます．\nclang++ -v -E -x c++ /dev/null\n現在では，macrtools を通じて C++ コンパイラを R 内でインストールすることもできます．\n次のコードが実行されれば，インストールは成功しています．\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\n\n\n\n\n\n\n\n\nコンパイラ最適化 (MacOS)\n\n\n\n\n\nRStan Getting Started の Configuring C Toolchain for Mac では，次のようなコンパイラの最適化が推奨されています：\ndotR &lt;- file.path(Sys.getenv(\"HOME\"), \".R\")\nif (!file.exists(dotR)) dir.create(dotR)\nM &lt;- file.path(dotR, \"Makevars\")\nif (!file.exists(M)) file.create(M)\narch &lt;- ifelse(R.version$arch == \"aarch64\", \"arm64\", \"x86_64\")\ncat(paste(\"\\nCXX17FLAGS += -O3 -mtune=native -arch\", arch, \"-ftemplate-depth-256\"),\n    file = M, sep = \"\\n\", append = FALSE)\nこれにより ~/.R/Makevars に次のような行が追加されます：\nCXX17FLAGS += -O3 -mtune=native -arch arm64 -ftemplate-depth-256\n\n\n\n\n1.1 stan 関数\nRStan パッケージの本体は stan 関数である：\nstan(file, model_name = \"anon_model\", model_code = \"\", fit = NA, data = list(), pars = NA, chains = 4, iter = 2000, warmup = floor(iter/2), thin = 1, init = \"random\", seed = sample.int(.Machine$integer.max, 1), algorithm = c(\"NUTS\", \"HMC\", \"Fixed_param\"), control = NULL, sample_file = NULL, diagnostic_file = NULL, save_dso = TRUE, verbose = FALSE, include = TRUE, cores = getOption(\"mc.cores\", 1L), open_progress = interactive() && !isatty(stdout()) && !identical(Sys.getenv(\"RSTUDIO\"), \"1\"), ..., boost_lib = NULL, eigen_lib = NULL)\n\n1.1.1 モデルの受け渡し\nmodel_code=\"\" が Stan モデルを定義するコードを，文字列として直接受け渡すための引数である．\n返り値はフィット済みの stanfit オブジェクトである．\n他の方法は次のとおり：\n\nfile としてファイルへのパスを渡す\nフィット済みの stanfit オブジェクトを fit 引数として渡す\n\n\n\n1.1.2 重要な引数\n\ndata：データを与える．list 型．\niter：繰り返し回数．デフォルトは 2000．\nchains：チェイン数．デフォルトは 4．\n\n\n\n1.1.3 stanfit オブジェクト\nstan 関数は Stan モデルを C++ に変換して実行し，結果を stanfit オブジェクトとして返す．\nこれに対して print, summary, plot などのメソッドが利用可能である．\nさらに，次の様にして MCMC サンプルを取り出すことができる：\n\nas.array メソッドを用いて MCMC サンプルを array 型で取り出す\nextract メソッドを用いて MCMC サンプルを list 型で取り出す\nposterior ライブラリの as_draws_df メソッドを用いて MCMC サンプルを df 型で取り出す．種々のデータ型 &lt;format&gt; に対して as_draws_&lt;format&gt; が存在する．\n\n取り出した MCMC サンプルは bayesplot パッケージの mcmc_trace, mcmc_dens などの関数を用いて可視化することができる．\n\n\n1.1.4 例１：軌道と事後分布の可視化\n\nscode &lt;- \"\nparameters {\n  array[2] real y;\n}\nmodel {\n  y[1] ~ normal(0, 1);\n  y[2] ~ double_exponential(0, 2);\n}\n\"\nfit &lt;- stan(model_code = scode, iter = 10000, chains = 4, verbose = FALSE)\n\n\nlibrary(bayesplot)\n\nmcmc_trace(as.array(fit), pars = c(\"y[1]\", \"y[2]\"))\n\n\n\n\n軌道のプロット\n\n\n\n\n\nmcmc_dens(as.array(fit), pars = c(\"y[1]\", \"y[2]\"))\n\n\n\n\n密度のプロット\n\n\n\n\n\n\n1.1.5 例２：確率過程の統計推測\nOU 過程\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nに対して，stan 関数でベイズ推定を実行してみます．\n\nlibrary(yuima)\nmodel &lt;- setModel(drift = \"theta*(mu-X)\", diffusion = \"sigma\", state.variable = \"X\")\n\nパラメータは \\[\n\\begin{pmatrix}\\theta\\\\\\mu\\\\\\sigma\\end{pmatrix}\n=\n\\begin{pmatrix}1\\\\0\\\\0.5\\end{pmatrix}\n\\tag{1}\\] として YUIMA を用いてシミュレーションをし，そのデータを与えてパラメータが復元できるかをみます．\n\nlibrary(rstan)\nexcode &lt;- \"data {\n            int N;\n            real x[N+1];\n            real h;\n          }\n\n          parameters {\n            real theta;\n            real mu;\n            real&lt;lower=0&gt; sigma;\n          }\n\n          model {\n            x[1] ~ normal(0,1);\n            for(n in 2:(N+1)){\n              x[n] ~ normal(x[n-1] + theta * (mu - x[n-1]) * h,  sqrt(h) * sigma);\n            }\n          }\"\n\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(theta = 1, mu = 0, sigma = 0.5), xinit = rnorm(1))\nsde_dat &lt;- list(N =  yuima@sampling@n,\n                  x = as.numeric(simulation@data@original.data),\n                  h=yuima@sampling@Terminal/yuima@sampling@n)\n\n\n# シミュレーション結果\nplot(simulation)\n\n\n\n\n\n\n\n\n\n# ベイズ推定\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nfit &lt;- stan(model_code=excode, data = sde_dat, iter = 1000, chains = 4)\n\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\ntheta    0.79    0.07 0.93   -0.40    0.03    0.50    1.44    2.96   175 1.02\nmu      -0.06    0.46 3.15   -6.14   -0.68   -0.40    0.10    8.62    46 1.11\nsigma    0.48    0.00 0.01    0.46    0.48    0.48    0.49    0.51   585 1.00\nlp__  3127.12    0.06 1.17 3124.36 3126.47 3127.21 3127.95 3128.86   330 1.01\n\nSamples were drawn using NUTS(diag_e) at Sun Dec 22 10:18:54 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nパラメータ (1) がよく推定できていることがわかる．特に \\(\\sigma\\) が安定して推定できている：\n\nplot(fit)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\n\nlibrary(\"bayesplot\")\nlibrary(\"rstanarm\")\nlibrary(\"ggplot2\")\n\nposterior &lt;- as.matrix(fit)\nplot_title &lt;- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(posterior,\n           pars = c(\"theta\", \"mu\", \"sigma\"),\n           prob = 0.8) + plot_title\n\n\n\n\n\n\n\n\n\n\n\n1.2 トラブルシューティング\n\n1.2.1 cmath が見つからない\nQuitting from lines 329-343 (adastan.qmd) \n\n compileCode(f, code, language = language, verbose = verbose) でエラー: \n  using C++ compiler: ‘Apple clang version 16.0.0 (clang-1600.0.26.3)’using C++17using SDK: ‘MacOSX15.0.sdk’In file included from &lt;built-in&gt;:1:In file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:In file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:In file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found  679 | #include &lt;cmath&gt;      |          ^~~~~~~1 error generated.make: *** [file2546221168fc.o] Error 1\n 呼び出し:  .main ... cxxfunctionplus -&gt; &lt;Anonymous&gt; -&gt; cxxfunction -&gt; compileCode\n 追加情報:  警告メッセージ: \n1:  パッケージ 'rstan' はバージョン 4.3.1 の R の下で造られました  \n2:  パッケージ 'bayesplot' はバージョン 4.3.1 の R の下で造られました  \n3:  パッケージ 'rstanarm' はバージョン 4.3.1 の R の下で造られました  \n\n\nQuitting from lines 329-343 (adastan.qmd) \n sink(type = \"output\") でエラー:  コネクションが不正です \n 呼び出し:  .main ... eval -&gt; stan -&gt; stan_model -&gt; cxxfunctionplus -&gt; sink\n 実行が停止されました \n大変長く書いてあるが，要は fatal error: 'cmath' file not found である．\n筆者の場合は純粋な clang++ の問題であった：\n❯ echo '#include &lt;cmath&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    double result = std::sqrt(16.0);\n    std::cout &lt;&lt; \"The square root of 16 is \" &lt;&lt; result &lt;&lt; std::endl;\n    return 0;\n}' &gt; test.cpp\n\n\n~\n❯ clang++ -std=c++17 test.cpp -o test\n\ntest.cpp:1:10: fatal error: 'cmath' file not found\n    1 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nこのような場合は，まず Xcode の再インストールをすると良い．\nsoftwareupdate --list\nの出力を用いて，次のようにする：\nsoftwareupdate -i \"Command Line Tools (macOS High Sierra version 10.13) for Xcode-10.1\"\nまたは次のようにする：\nsudo rm -rf /Library/Developer/CommandLineTools\nxcode-select --install"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#cmdstanr-パッケージ",
    "href": "posts/2024/R/Stan2.html#cmdstanr-パッケージ",
    "title": "R 上の Stan インターフェイス",
    "section": "3 CmdStanR パッケージ",
    "text": "3 CmdStanR パッケージ\nCmdStanPy, CmdStanR はいずれも Stan のインターフェースである．\nCmdStanR は R6 オブジェクトを用いており，大変現代的な実装を持っている．\n\n\n\n\n\n\nインストール方法\n\n\n\n\n\nGetting Started with CmdStanR に従って実行します．\nrepos 引数を省略するとインストールできないことがあります．\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\nCmdStanR の利用のためには，CmdStan が必要です．\nCmdStanR を直接インストールすることもできますが，CmdStanR 内部からインストールすることもできます．\nlibrary(cmdstanr)\ninstall_cmdstan(cores = 4)\n\ncmdstan_version()\n\n[1] \"2.36.0\"\n\n\n多くの場合，自動で CMDSTAN 環境変数にパスが設定されます．次のいずれかの方法で確認できます：\nSys.getenv(\"CMDSTAN\")\ncmdstan_path()\nCmdStanR の美点の一つは，install_cmdstan() により CmdStan をアップデートすることで最新の Stan を R から簡単に利用できることである．\n一方で RStan はパッケージ自体のアップデートを待つ必要がある．\n\n\n\n\n3.1 モデル定義\ncmdstan_model() 関数は，Stan 言語による記述されたモデル定義を，C++ コードにコンパイルし，その結果を R6 オブジェクトとして返す．1\ncmdstan_model(stan_file = NULL, exe_file = NULL, compile = TRUE, ...)\n返り値は CmdStanModel オブジェクトである．ただし R6 オブジェクトでもあり，R6 流のメソッドの呼び方 $ が使える．\n\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file)\n\nStan 言語による定義は次のようにして確認できる：\n\nmod$print()\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0, upper=1&gt; y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  theta ~ beta(1, 1); // uniform prior on interval 0,1\n  y ~ bernoulli(theta);\n}\n\nnames(mod$variables())\n\n[1] \"parameters\"             \"included_files\"         \"data\"                  \n[4] \"transformed_parameters\" \"generated_quantities\"  \n\nnames(mod$variables()$transformed_parameters)\n\ncharacter(0)\n\n\n元となったファイルのパスも stan_file(), exe_file() で確認できる．\n\n\n3.2 Stan コードの操作\nwrite_stan_file() 関数は Stan コードをファイルに書き出すことができる：\nwrite_stan_file(\n  code,\n  dir = getOption(\"cmdstanr_write_stan_file_dir\", tempdir()),\n  basename = NULL,\n  force_overwrite = FALSE,\n  hash_salt = \"\"\n)\nグローバル環境変数が設定されていない限り，tempdir() で一時ファイルが作成される．これは R セッションの終了とともに削除される．\nstan_file_variables &lt;- write_stan_file(\"\ndata {\n  int&lt;lower=1&gt; J;\n  vector&lt;lower=0&gt;[J] sigma;\n  vector[J] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] theta_raw;\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * theta_raw;\n}\nmodel {\n  target += normal_lpdf(tau | 0, 10);\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(theta_raw | 0, 1);\n  target += normal_lpdf(y | theta, sigma);\n}\n\")\nmod_v &lt;- cmdstan_model(stan_file_variables)\nvariables &lt;- mod_v$variables()\n\n\n3.3 サンプリング\n\ndata_list &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000 # print update every 1000 iters\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.3 seconds.\n\n\n返り値 fit は CmdStanMCMC オブジェクトであり，summary() などのメソッドが使用可能である．\nsummary() メソッドは，posterior パッケージのメソッド summarise_draws() を自動で使うようになっている．\n\nfit$summary()\n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad      q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -7.30  -7.00  0.797 0.345 -8.86   -6.75   1.00    1923.    2017.\n2 theta     0.257  0.241 0.124 0.127  0.0800  0.485  1.00    1232.    1477.\n\nfit$summary(variables = c(\"theta\", \"lp__\"), \"mean\", \"sd\")\n\n# A tibble: 2 × 3\n  variable   mean    sd\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 theta     0.257 0.124\n2 lp__     -7.30  0.797\n\n\n同様にして draws() メソッドで bayesplot パッケージが呼び出される．\n\nmcmc_hist(fit$draws(\"theta\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/2024/R/Stan2.html#文献紹介",
    "href": "posts/2024/R/Stan2.html#文献紹介",
    "title": "R 上の Stan インターフェイス",
    "section": "4 文献紹介",
    "text": "4 文献紹介"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#footnotes",
    "href": "posts/2024/R/Stan2.html#footnotes",
    "title": "R 上の Stan インターフェイス",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n実際には，最初に CmdStanModel オブジェクトを生成し，compile() メソッドを呼び出している．これが compile = TRUE フラッグの存在意義である．↩︎"
  },
  {
    "objectID": "posts/2024/R/Stan1.html",
    "href": "posts/2024/R/Stan1.html",
    "title": "Stan 入門",
    "section": "",
    "text": "GitHub, Documentation, Reference Manual．\n\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#リンク集",
    "href": "posts/2024/R/Stan1.html#リンク集",
    "title": "Stan 入門",
    "section": "",
    "text": "GitHub, Documentation, Reference Manual．\n\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#stan-言語",
    "href": "posts/2024/R/Stan1.html#stan-言語",
    "title": "Stan 入門",
    "section": "1 Stan 言語",
    "text": "1 Stan 言語\n\n1.1 はじめに\nStan 言語は確率モデルを３つのブロックに分けて記述する．その３つとはデータ，パラメータとモデルである．\n\n以上の３要素により，事後分布が定まる．それぞれの要素は，対応した名前を持ったスコープ {} 内で，この順番で定義される慣習がある．\n以前のスコープ内で定義された識別子は，その後のスコープでも利用可能になる．\n\n\nstan_code.stan\n\ndata {\n    int&lt;lower=0&gt; N;  // N &gt;= 0\n    array[N] int&lt;lower=0, upper=1&gt; y;  // y[n] in {0, 1}\n}\nparameters {\n    real&lt;lower=0, upper=1&gt; theta;  // theta in [0, 1]\n}\nmodel {\n    theta ~ beta(1, 1);  // uniform prior\n    y ~ bernoulli(theta);  // observation model\n}\n\nその後 Stan プログラムは事後分布の対数密度を表す C++ 関数にコンパイルされる．\n最終的にこの対数尤度を用いて，その勾配を自動微分により計算し，Hamiltonian Monte Carlo による事後分布サンプリングを実行する．\nStan エコシステムの詳説は次節 2 に回し，ここでは Stan 言語の基本に集中する．\n\n\n1.2 確率的プログラミング言語\nStan 言語のように確率モデルを記述する（より正確には事後分布の尤度を記述する高級）言語を 確率的プログラミング言語 (PPL: Probabilistic Programming Language) という．\n最初期の確率的プログラミング言語の１つに WinBUGS (Lunn et al., 2000) の実装に代表される BUGS (Bayesian analysis Using Gibbs Sampling) がある．\nBUGS では事後分布を詳細に 有向グラフィカルモデル (DAG) で記述し，Gibbs Sampling により事後分布をサンプリングするという仕組みであった．\nStan 言語はさらに柔軟に，（正規化されているとは限らない）対数尤度を記述できるようになっている．\n理想的にはあらゆる確率モデルを扱いたいものであるが，現状の Stan 言語はパラメータとしては連続変数のみを持つモデルのみが定義可能である．\n\n\n1.3 data ブロック\nStan は静的な型システムを持ち，変数宣言の際には必ず型を指定する必要がある．\n\n\n\n\n\n\nStan の代表的なデータ型\n\n\n\n\n実数 \\(x\\in\\mathbb{R}\\)\nreal x;\n実数 \\(x\\in[a,b]\\)\nreal&lt;lower=a upper=b&gt; x;\n単体 \\(x\\in[0,1]^N,\\sum_{n=1}^Nx_n=1\\)\nsimplex[N] x;\n自然数 \\(N\\in\\mathbb{N}\\)\nint&lt;lower=0&gt; N;\n配列 \\(x\\in\\mathbb{R}^N\\)：純粋なコンテナ型であり，線型代数ライブラリは適用不可．\narray[N] real x;\nreal x[N] という記法は Stan 2.26 以降使われないことに注意（docs 参照）．\n\n\n\nなお，各ブロック内において，あらゆる変数宣言は全ての非宣言的文の前に来る必要がある．\n{\n    real variable1 = 5;\n    variable1 /= 2;  // ここでエラー\n    real variable2 = exp(variable1);\n}\n\n\n1.4 transformed data ブロック\ndata ブロックでは許されないが，一般に Stan 言語では変数宣言と同時に代入もできる．\n代入を省略した場合は NaN によって初期化される．\n\n\n\n\n\n\nStan の代表的な線型代数関連のデータ型\n\n\n\n\nベクトル \\(x\\in\\mathbb{R}^5\\)：c++ の線型代数ライブラリが使える\nvector[5] x = [0, 1, 2, 3, 4];\n横ベクトル \\(y\\in(\\mathbb{R}^5)^*\\)：\\(y*x\\) が計算可能．\nrow_vector[5] y = [1, 2, 3, 4, 5]';\nx*y; // 計算可能\n行列 \\(A\\in M_{N,M}(\\mathbb{R})\\)\nmatrix[N, M] A;\n\n\n\ntransformed data ブロックでは，観測される訳でもなければパラメータでもないような，内部で使われる変数が定義される．\n\n\n1.5 parameters ブロック\nparameters ブロックも同様にして変数を宣言する．\nmodel ブロックで使われる変数は，data, parameters ブロックのいずれかで宣言されている必要がある．\n\\(x\\in[a,b]^N\\) のような制約領域を持つ変数が parameters ブロックで宣言された場合，Stan は内部でパラメータ変換を行い，\\([a,b]\\) を \\(\\mathbb{R}\\) 上に写してサンプリングを実行する．\nGamma 分布のように台が \\(\\mathbb{R}\\) の部分集合になるような事前分布を扱う場合，対応する制約領域をパラメータに定義することがサンプリングの効率を上げる．\n一方で，\\(\\mathbb{R}\\) 全体を台に持つ事前分布を持つパラメータに対して制約領域を宣言した場合，truncate をした事前分布を定義したことに等価になる．\n\n\n1.6 model ブロック\nモデルブロックでは対数密度関数の値が変数 target として保持されており，target() 関数でアクセス可能である．\n次の３つの文は等価になる：\nbeta ~ normal(0, 1);\nbeta ~ normal(beta | 0, 1);  // syntax sugar\ntarget += normal_lpdf(beta | 0, 1);  // 実際の処理に近い\n\n\n1.7 ループと制御\nfor (n in N1:N2) {\n  // Statements executed for each N1 &lt;= n &lt;= N2\n}\n\nif (condition) {\n  // Statements evaluated if condition is true\n} else {\n  // Statements evaluated if condition is false\n}\n\n\n1.8 transformed parameters / Generated Quantities ブロック\ntransformed parameters ブロックと Generated Quantities ブロックでは，parameters ブロックで宣言されたパラメータの関数として推定対象を定義する．\nこの推定対象は推論後に事後平均が取られて報告される．\n２つの違いは，transformed parameters ブロックでは model ブロックの前に配置され，model ブロックでも使えるのに対し，Generated Quantities ブロックでは model ブロックの後に配置され，純粋に事後分布の関数として処理される点である．\nなお，関数定義は functions ブロックで行う．\nfunctions {\n    real baseline(real a1, real a2, real theta) {\n        return a1 * theta + a2;\n    }\n}"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#sec-stan-ecosystem",
    "href": "posts/2024/R/Stan1.html#sec-stan-ecosystem",
    "title": "Stan 入門",
    "section": "2 Stan エコシステムの概観",
    "text": "2 Stan エコシステムの概観\n\n2.1 Stan の推論エンジン\nStan の推論エンジンとして，HMC の他に２つの C++ アルゴリズムが用意されており，それぞれ BFGS 法による点推定と変分推定を実装している．\n\n\n2.2 Stan 数学ライブラリ\nStan 言語により定義された確率モデル（対数密度関数）が実際に評価可能にするための C++ 関数のライブラリである．\n使える関数のリストは Stan Functions Reference を参照．\n基本的な数学的関数や統計的関数に加えて，自動微分が実装されており，対数密度関数の勾配や Hessian も計算可能である．\n\n\n2.3 Stan インターフェイス\n\n2.3.1 CmdStan の仕組み\nCmdStan は makefiles の集合からなる最も軽量な，コマンドラインベースのインターフェイスである．\nこれを直接 R で wrap した CmdStanR パッケージ や CmdStanPy パッケージが存在し，同時に Julia Stan.jl, Mathematica MathematicaStan, Matlab MatlabStan, Stata StataStan からも利用可能である．\nStan の Math ライブラリ，Algorithm ライブラリなどの出力をテキストファイルで出力してやり取りする．\nCmdStan は最も軽量なインターフェイスであり，Stan の性能を純粋に引き出す場合に使われる．\n\n\n2.3.2 CmdStan のインストール\nCmdStan Installation によると，conda による方法とソースからのインストールの2つの方法がある．\n一方で次稿で扱う CmdStanR を通じてインストールすることもできる：\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.3.3 RStan と PyStan\nR と Python という２大言語を Stan と直接繋げるインターフェイスを提供している．\nCmdStan のように一度テキストファイルに書き出すということがなく，メモリ上でやり取りされるが，それ故に CmdStan よりも追加の処理が多くなりがちである．"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#文献紹介",
    "href": "posts/2024/R/Stan1.html#文献紹介",
    "title": "Stan 入門",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n手軽に概要を掴むには Michael Betancourt によるブログ記事 An introduction to Stan が良い．\nより本格的な解説論文には (Gelman et al., 2015), (Carpenter et al., 2017) がある．\n公式の文献紹介 が stan.org から出ているが，情報が古い．\nまた，Stan には 日本語のマニュアル もある：stan-ja (GitHub)．"
  },
  {
    "objectID": "posts/2024/R/adastan.html",
    "href": "posts/2024/R/adastan.html",
    "title": "SDE のベイズ推定入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nR の YUIMA パッケージに関する詳細は，次の記事も参照："
  },
  {
    "objectID": "posts/2024/R/adastan.html#問題提起",
    "href": "posts/2024/R/adastan.html#問題提起",
    "title": "SDE のベイズ推定入門",
    "section": "1 問題提起",
    "text": "1 問題提起\n\n1.1 はじめに\n確率過程の統計推測を行うための R パッケージ yuima では多次元の SDE \\[\ndX_t=b_t^\\theta(X_t)\\,dt+\\sigma_t^\\phi(X_t)\\,dW_t,\\qquad X_0\\in\\mathbb{R}^d,\n\\] に対してパラメータ \\(\\theta\\in\\mathbb{R}^{d_\\theta},\\phi\\in\\mathbb{R}^{d_\\phi}\\) の推定を実行することができる（YUIMA の記事 も参照）．\nYUIMA におけるパラメータ \\(\\theta,\\varphi\\) の推定には，qmle() による擬似最尤推定量を用いることも，adaBayes() による一般化ベイズによる事後平均推定を実行することも可能である．\n現状の adaBayes() 関数では，自前のランダムウォーク MH アルゴリズムや p-CN アルゴリズムを用いている．\n\n\n1.2 確率的プログラミング言語 Stan との連携\nStan は Hamiltonian Monte Carlo 法を用いた事後分布サンプリングを，確率モデルを定義するだけで実行することができる言語である．\n加えて，バックグラウンドで C++ を用いているため，非常に高速な MCMC 計算が可能である．\n確率的プログラミング言語 Stan については次の記事も参照：\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\nNo matching items\n\n\nRStan は確率的プログラミング言語 Stan とのインターフェイスを提供するパッケージであり，これを用いることで Stan を通じた HMC を用いた事後分布からのサンプリングが実行できる．\nここでは adaBayes() 関数をさらに効率的にするために，Stan を用いた事後分布サンプリングを可能にする関数 adaStan() の実装を試みる．\n\n\n1.3 アイデアのスケッチ\n具体的には，R において次のような関数を定義することになるだろう．\n必ずしもベストな方法ではないかもしれないが，まずは RStan を用いてスケッチをしてみる．改良版は第 4 節参照．\n\nlibrary(yuima)\nlibrary(rstan)\n\n1yuima_to_stan &lt;- function(yuima){\n  excode &lt;- 'data {\\n  int N;\\n  vector[N+1] x;\\n  real T;\\n  real h;\\n}\nparameters {\\n'\n\n2  for(i in 1:length(yuima@model@parameter@all)){\n    excode &lt;- paste(excode, \" real\", yuima@model@parameter@all[i], \";\\n\")\n  }\n\n3  excode &lt;- paste(excode,\"\\n}\")\n\n4  excode &lt;- paste(excode,'\\nmodel {\\n  x[1] ~ normal(0,1);\\n  for(n in 2:(N+1)){')\n\n  excode &lt;- paste(excode,\n5    \"\\n  x[n] ~ normal(x[n-1] + h *\", gsub(\"x\", \"x[n-1]\", yuima@model@drift), \",sqrt(h) *\", gsub(\"x\", \"x[n-1]\", yuima@model@diffusion[[1]]),\");\\n  }\")\n\n  excode &lt;- paste(excode,'\\n}\\n')\n}\n\n6adaStan &lt;- function(yuima){\n  excode &lt;- yuima_to_stan(yuima)\n\n7  sde_dat &lt;- list(N =  yuima@sampling@n,\n    x = as.numeric(yuima@data@original.data), \n    T=yuima@sampling@Terminal,\n    h=yuima@sampling@Terminal/yuima@sampling@n)\n\n  fit &lt;- stan(model_code=excode,\n    data = sde_dat, \n    iter = 1000,\n8    chains = 4)\n\n  return(fit)\n}\n\n\n1\n\nStan モデルのコード（パラメータ部分は未定）を文字列として excode 変数に格納する．\n\n2\n\nここからが adaStan 関数の本体である．Yuima モデルの全てのパラメータについてループを開始して，excode にパラメータの宣言を追加していく．\n\n3\n\nここでついに Stan モデルのパラメータの定義部分が完成する．\n\n4\n\n最後はモデルの定義部分を追加して，Stan モデルのコードが完成する．最初の観測値 x[1] は \\(\\mathrm{N}(0,1)\\) に従う．\n\n5\n\nそれ以降の観測値 x[n] は，前の観測値 x[n-1] に drift 項と diffusion 項を加えたものに従う．これを実装するために，Yuima モデルの drift 項と diffusion 項の定義文を呼び出し，x を x[n-1] に置換することで Stan モデルのコードに埋め込む．\n\n6\n\nadaStan という関数を定義する．この関数は，Yuima パッケージのオブジェクトを引数として受け取り，Stan での推定を行い，その結果を fit オブジェクトとして返す．\n\n7\n\nStan での推定を実行するために，Yuima モデルのデータを Stan モデルに渡すためのリスト sde_dat を作成する．\n\n8\n\n最後に Stan モデルをコンパイルして実行し，結果を fit オブジェクトとして返す．\n\n\n\n\nyuima オブジェクトのスロットの存在のチェックや変数名 x の表記揺れなど，細かな問題も多いだろうが，殊に Stan との接続においてより良い方法を模索したい．\n\n\n\n\n\n\n問題点\n\n\n\n関数内部で Stan コードを文字列として生成していることがダサい．\nより良いコードオブジェクトの取り扱い方や，Stan とのより安全で効率的なインターフェイスを模索したい．\n\n\nStan を使う以上，どこかで Stan モデルの情報を受け渡すことは必要になるが，できることならばもっと良い方法を考えたい．\n\n\n1.4 問題点\nadaStan() 関数の挙動を詳しく見るために，次の具体例を考える．\nYUIMA を通じて１次元 OU 過程\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nをシミュレーションをするためには，次のようにモデル定義をする：\n\nmodel &lt;- setModel(drift = \"theta*(mu-x)\", diffusion = \"sigma\", state.variable = \"x\", solve.variable = \"x\")\n\nこれだけで，YUIMA は勝手にパラメータを識別してくれる：\n\nmodel@drift\n\nexpression((theta * (mu - x)))\n\nmodel@diffusion[[1]]\n\nexpression((sigma))\n\n\nこれを通じて生成される Stan モデル文は\ndata {\n  int N;\n  vector[N+1] x;\n  real T;\n  real h;\n}\n\nparameters {\n  real theta;\n  real mu;\n  real sigma;\n}\n\nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){\n    x[n] ~ normal(x[n-1] + h * theta * (mu - x[n-1]),\n                  sqrt(h) * sigma);\n  }\n}\nとなるべきであるが，実際その通りになる：\n\nx &lt;- setYuima(model = model)\nstancode &lt;- yuima_to_stan(x)\ncat(stancode)\n\ndata {\n  int N;\n  vector[N+1] x;\n  real T;\n  real h;\n}\nparameters {\n  real theta ;\n  real mu ;\n  real sigma ;\n \n} \nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){ \n  x[n] ~ normal(x[n-1] + h * (theta * (mu - x[n-1])) ,sqrt(h) * (sigma) );\n  } \n}\n\n\n\n\n1.5 CmdStanR による方法\n\nlibrary(cmdstanr)\nstan_file_variables &lt;- write_stan_file(stancode)\nmod &lt;- cmdstan_model(stan_file_variables)\n\n\nmod$print()\n\ndata {\n  int N;\n  vector[N+1] x;\n  real T;\n  real h;\n}\nparameters {\n  real theta ;\n  real mu ;\n  real sigma ;\n \n} \nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){ \n  x[n] ~ normal(x[n-1] + h * (theta * (mu - x[n-1])) ,sqrt(h) * (sigma) );\n  } \n}\n\n\nなどとすることで，一時ファイル上で stan ファイルとバイナリファイルを作成・操作することができる．\n\nmod$stan_file()\n\n[1] \"/var/folders/7c/j9mzb7pn0wn1k_f9j58c8y480000gn/T/RtmpVCwRPC/model_28b66dedb9ecfce181f60d7f27d0c76d.stan\"\n\n\n\n\n1.6 ベクトル化\nmodel {\n  x[1] ~ normal(0, 1);  // 初期（値の事前）分布\n  x[2:(N + 1)] ~ normal(x[1:N] + h * theta * (rep_vector(mu, N) - x[1:N]), sqrt(h) * sigma);  // xの2番目からN+1番目までをベクトル化して定義\n}\nとした方が Stan コードの処理（特に自動微分の計算）が速くなる．\n\n\n1.7 一般化ベイズへの拡張\n第 1.3 節のサンプルコードでは，adaBayes() はまだ拡散過程のみに対応しており，シミュレーションベースの推定をしている．\n一方で Stan では target 変数を用いて，擬似尤度などを用いた一般化ベイズ推定も実行できる．1\ntarget += normal_lpdf(y | mu, sigma);"
  },
  {
    "objectID": "posts/2024/R/adastan.html#stan-インターフェイス",
    "href": "posts/2024/R/adastan.html#stan-インターフェイス",
    "title": "SDE のベイズ推定入門",
    "section": "2 Stan インターフェイス",
    "text": "2 Stan インターフェイス\n\n2.1 RStan パッケージ\n詳しくは次稿参照：\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n\n\n2024-09-19\n\n\n\n\n\n\nNo matching items\n\n\n前述の OU 過程 1.4\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nで stan 関数でベイズ推定を実行してみます．\nパラメータは \\[\n\\begin{pmatrix}\\theta\\\\\\mu\\\\\\sigma\\end{pmatrix}\n=\n\\begin{pmatrix}1\\\\0\\\\0.5\\end{pmatrix}\n\\] として YUIMA を用いてシミュレーションをし，そのデータを与えてパラメータが復元できるかをみます．\n\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(theta = 1, mu = 0, sigma = 0.5), xinit = rnorm(1))\n\n\n# シミュレーション結果\nplot(simulation)\n\n\n\n\n\n\n\n\nさて，このシミュレーション結果から，adaStan() 関数でパラメータが復元できるかを確認しましょう．\n\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\nfit &lt;- adaStan(simulation)\n\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\ntheta    2.93    0.32 2.00   -0.16    1.48    2.74    4.36    7.11    39 1.09\nmu       0.07    0.09 1.62   -0.85   -0.12   -0.06    0.04    3.34   300 1.01\nsigma    0.47    0.00 0.01    0.45    0.47    0.47    0.48    0.49   137 1.03\nlp__  3152.13    0.11 1.40 3148.92 3151.24 3152.39 3153.24 3154.02   151 1.04\n\nSamples were drawn using NUTS(diag_e) at Fri Sep 20 17:17:46 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nplot(fit)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\n\nlibrary(\"bayesplot\")\nlibrary(\"rstanarm\")\nlibrary(\"ggplot2\")\n\nposterior &lt;- as.matrix(fit)\nplot_title &lt;- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\n\n\nmcmc_areas(posterior,\n           pars = c(\"theta\", \"mu\", \"sigma\"),\n           prob = 0.8) + plot_title\n\n\n\n\n\n\n\n\n\\(\\sigma\\) の推定はよくできているが \\(\\mu\\) の精度はあまりよくなく，\\(\\theta\\) はバイアスがある様で，自信を持って間違えることも多い．\n\n\n2.2 brms パッケージ\nbrms や rethinking も，背後で Stan を利用している．これらが文字式をどのように取り扱っているかを調査する．\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n\n\n2024-05-12\n\n\n\n\n\n\nNo matching items\n\n\nStan コードを扱っている関数は .stancode() であった．\n最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\n最終的にはこれらの関数も，第 1.3 節のサンプルコードと同様の要領で paste0 を使っていた．\n\n\n\n\n\n\npaste0 と paste の違い\n\n\n\n\n\n\n# paste0() の使用例\nresult1 &lt;- paste0(\"Hello\", \"world\")\nprint(result1)  # \"Helloworld\"\n\n[1] \"Helloworld\"\n\n# paste() の使用例\nresult2 &lt;- paste(\"Hello\", \"world\")\nprint(result2)  # \"Hello world\"\n\n[1] \"Hello world\"\n\nresult3 &lt;- paste(\"Hello\", \"world\", sep = \"-\")\nprint(result3)  # \"Hello-world\"\n\n[1] \"Hello-world\""
  },
  {
    "objectID": "posts/2024/R/adastan.html#テンプレート操作",
    "href": "posts/2024/R/adastan.html#テンプレート操作",
    "title": "SDE のベイズ推定入門",
    "section": "3 テンプレート操作",
    "text": "3 テンプレート操作\n\n\n\n\n\n\nR の Expression について\n\n\n\n\n\nオブジェクト志向言語ではコード自体もオブジェクトであり，これを R では Expression と呼ぶ．\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\n\n\n2021-05-07\n\n\n\n\n\n\nNo matching items\n\n\n１つのクラスからなるわけではなく，call, symbol, constant, pairlist の４つの型からなる．2\n次のような操作ができる3\nrlang::expr がコンストラクタである：\n\nlibrary(rlang)\nz &lt;- expr(y &lt;- x*10)\nz\n\ny &lt;- x * 10\n\n\nexpression オブジェクトは base::eval() で評価できる：\n\nx &lt;- 4\neval(z)\ny\n\n[1] 40\n\n\nexpression には list のようにアクセス可能である：4\n\nf &lt;- expr(f(x = 1, y = 2))\n\nf$z &lt;- 3\nf\n\nf(x = 1, y = 2, z = 3)\n\n\n\nf[[2]] &lt;- NULL\nf\n\nf(y = 2, z = 3)\n\n\n\n\n\n\n3.1 glue パッケージ\ninstall.packages(\"glue\")\nglue （CRAN, Docs）パッケージは文字列リテラルを扱うパッケージである．\n\nlibrary(glue)\n\nWarning: パッケージ 'glue' はバージョン 4.3.1 の R の下で造られました\n\nname &lt;- \"  Hirofumi\\n  Shiba\\n\"\nmajor &lt;- \"Mathematics\"\nglue::glue('My name is {name}. I study {major}. Nice to meet you!')  # 名前空間の衝突を避けるために :: を使う\n\nMy name is   Hirofumi\n  Shiba\n. I study Mathematics. Nice to meet you!\n\n\n\nglue::glue(\" real {param};\", param = yuima@model@parameter@all, .collapse = \"\\n\")\n\n real theta;\n real mu;\n real sigma;\n\n\nこれを用いると，adaStan() は次のように可読性が高い形で書き直すことができる：\n\nyuima_to_stan_glued &lt;- function(yuima) {\n  # パラメータの定義部分を作成\n  parameters &lt;- glue::glue(\"real {param};\", param = yuima@model@parameter@all)\n  parameters &lt;- paste(parameters, collapse = \"\\n  \")\n\n  # drift と diffusion の式内の 'x' を 'x[n-1]' に置換\n  drift &lt;- gsub(\"x\", \"x[n-1]\", yuima@model@drift)\n  diffusion &lt;- gsub(\"x\", \"x[n-1]\", yuima@model@diffusion[[1]])\n  \n  # Stanコード全体を作成\n  template &lt;- \n'data {{\n  int N;\n  array[N+1] real x;\n  real T;\n  real h;\n}}\nparameters {{\n  {parameters}\n}}\nmodel {{\n  x[1] ~ normal(0, 1);\n  for(n in 2:(N+1)) {{\n    x[n] ~ normal(x[n-1] + h * {drift}, sqrt(h) * {diffusion});\n  }}\n}}'\n  excode &lt;- glue::glue(template, .trim = FALSE)  # parameters が複数行に渡る場合でも分離して出力しない\n  \n  return(excode)\n}\n\n\nyuima_to_stan_glued(yuima)\n\ndata {\n  int N;\n  array[N+1] real x;\n  real T;\n  real h;\n}\nparameters {\n  real theta;\n  real mu;\n  real sigma;\n}\nmodel {\n  x[1] ~ normal(0, 1);\n  for(n in 2:(N+1)) {\n    x[n] ~ normal(x[n-1] + h * (theta * (mu - x[n-1])), sqrt(h) * (sigma));\n  }\n}\n\n\n\n\n3.2 whisker パッケージ\ninstall.packages(\"whisker\")\nwhisker パッケージ（CRAN, GitHub）は Web を中心に採用されているテンプレートシステム Mustache に基づく機能 whisker.render() を提供している．\nwhisker.render(template, data = parent.frame(), partials = list(),\ndebug = FALSE, strict = TRUE)\n\nlibrary(whisker)\ntemplate &lt;-\n'Hello {{name}}\nYou have just won ${{value}}!\n{{#in_ca}}\nWell, ${{taxed_value}}, after taxes.\n{{/in_ca}}'\ndata &lt;- list(name = \"Hirofumi\"\n, value = 10000\n, taxed_value = 10000 - (10000 * 0.4)\n, in_ca = TRUE\n)\nwhisker.render(template, data)\n\n[1] \"Hello Hirofumi\\nYou have just won $10000!\\nWell, $6000, after taxes.\\n\"\n\n\nMustache の記法は Manual を参照．"
  },
  {
    "objectID": "posts/2024/R/adastan.html#sec-final-form",
    "href": "posts/2024/R/adastan.html#sec-final-form",
    "title": "SDE のベイズ推定入門",
    "section": "4 最終的なコード",
    "text": "4 最終的なコード\nadaStan.R を参照．\n\nsource(\"adaStan.R\")\n\nmodel &lt;- setModel(drift = \"theta*(mu-x)\", diffusion = \"sigma\", state.variable = \"x\", solve.variable = \"x\")\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(theta = 1, mu = 0, sigma = 0.5), xinit = rnorm(1))\n\nfit &lt;- adaStan(simulation, iter=2000, rstan=FALSE)\n\n\nfit$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad        q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     3101.    3102.    1.52   1.30   3098.     3103.     1.02     245.\n2 theta       2.23     2.31  1.35   1.43     -0.0763    4.39   1.03     160.\n3 mu          0.141    0.162 0.508  0.147    -0.918     0.849  1.04     118.\n4 sigma       0.499    0.499 0.0111 0.0110    0.481     0.518  1.00    2235.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nmcmc_hist(fit$draws(\"theta\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nなるほど！2000 回ほど繰り返すと，事後分布は２峰性をもち，MAP 推定量はかろうじて正解に近づいている．"
  },
  {
    "objectID": "posts/2024/R/adastan.html#終わりに",
    "href": "posts/2024/R/adastan.html#終わりに",
    "title": "SDE のベイズ推定入門",
    "section": "5 終わりに",
    "text": "5 終わりに\n\n\nGPT o1 によるアイデア"
  },
  {
    "objectID": "posts/2024/R/adastan.html#footnotes",
    "href": "posts/2024/R/adastan.html#footnotes",
    "title": "SDE のベイズ推定入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n例えば Cox 回帰など．Cox 回帰の Stan での実装は こちらの記事 も参照．↩︎\n(Wickham, 2019) 第17章２節．↩︎\n(Wickham, 2019) 第18章↩︎\n(Wickham, 2019) 第17章２節．↩︎"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#rstanarm-パッケージ",
    "href": "posts/2024/R/Stan2.html#rstanarm-パッケージ",
    "title": "R 上の Stan インターフェイス",
    "section": "2 rstanarm パッケージ",
    "text": "2 rstanarm パッケージ\n\n2.1 はじめに\nrstanarm は他の R パッケージと同様のインターフェースで推論を実行するためのパッケージであり，MCMC の実行には rstan をバックエンドで用いる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点モデルとは何か",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点モデルとは何か",
    "title": "理想点解析のハンズオン",
    "section": "1 理想点モデルとは何か？",
    "text": "1 理想点モデルとは何か？\n\n1.1 MCMCpack パッケージ\nはじめに，理想点モデルではどのようなことができるかをみるために，MCMCpack パッケージを通じて理想点推定を簡単に実行する方法を見る．\n理想点モデルでは識別性が一つの論点になる（第 2.5 節）が，ここでは簡単に，Stevens 判事と Thomas 判事の位置を固定する方法を用いてみよう．\nMCMCpack パッケージでは，時系列理想点モデルの推定に MCMCdynamicIRT1d() 関数が用意されている．\n\n# 初期値の設定\ntheta.start &lt;- rep(0, 9)  # 9人の裁判官の初期値\ntheta.start[2] &lt;- -3      # Stevens裁判官の初期値\ntheta.start[7] &lt;- 2       # Thomas裁判官の初期値\n\n# MCMCの実行\nout &lt;- MCMCdynamicIRT1d(\n    t(Rehnquist[,1:9]),           # データ行列（転置して裁判官×案件の形に）\n    item.time.map=Rehnquist$time, # 各案件の時期情報\n    theta.start=theta.start,      # 初期値\n    mcmc=2000,                   # MCMCの反復回数\n    burnin=2000,                 # バーンイン期間\n    thin=5,                       # 間引き数\n    verbose=500,                  # 進捗表示間隔\n    tau2.start=rep(0.1, 9),      # τ²の初期値\n    e0=0, E0=1,                  # θの事前分布パラメータ\n    a0=0, A0=1,                  # αの事前分布パラメータ\n    b0=0, B0=1,                  # βの事前分布パラメータ\n    c0=-1, d0=-1,               # τ²の事前分布パラメータ\n    store.item=FALSE,            # アイテムパラメータを保存しない\n    theta.constraints=list(Stevens=\"-\", Thomas=\"+\")  # 識別制約\n)\n\ntheta_cols &lt;- grep(\"theta\", colnames(out), value=TRUE)\ntheta_mcmc &lt;- out[, theta_cols]\n\n# library(coda)\n# summary(theta_mcmc)  # codaのsummary関数で要約\nplot(theta_mcmc)\n\n\n\n\n\n\n\n\n1.2 理想点の推定\n出力は各最高裁判事の理想点の事後分布である．\n\ntheta_means &lt;- colMeans(theta_mcmc)\npattern &lt;- \"t11\"\ncol_names &lt;- colnames(theta_mcmc)\nselected_cols &lt;- grep(pattern, col_names)  # 正規表現にマッチする列のインデックスを取得\nselected_theta_mcmc &lt;- theta_mcmc[, selected_cols]\n\nquantiles_2_5 &lt;- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.025))\nquantiles_97_5 &lt;- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.975))\n\nggplot(data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = unname(theta_means[selected_cols]),\n  lower = unname(quantiles_2_5),\n  upper = unname(quantiles_97_5)\n), aes(x = mean, y = legislator)) + \n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Estimated Ideal Points (11th term)\", x = \"Ideal Point\", y = \"Legislator\")\n\n\n\n\n\n\n\n\n\n\n1.3 時系列化\n(Martin and Quinn, 2002) ではこの理想点の時系列的な変化を調べた．\ntime_points &lt;- unique(Rehnquist$time)\nn_subjects &lt;- 9  # 裁判官の数\n\n# 各裁判官の軌跡をプロット\nplot(time_points, theta_means[1:length(time_points)], \n     type=\"l\", ylim=range(theta_means),\n     xlab=\"Time\", ylab=\"Ideal Point\",\n     main=\"Estimated Ideal Points Over Time\")\n\n# 各裁判官を異なる色で追加\ncolors &lt;- rainbow(n_subjects)\ncolors[9] &lt;- \"blue\"\nfor(i in c(1,8,9)) {\n    lines(time_points, \n          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],\n          col=colors[i],\n          lwd=3)\n}\n\n# 凡例を追加\nlegend(\"topright\", \n       legend=unique(colnames(Rehnquist)[c(1,8,9)]),  # 裁判官の名前\n       col=colors[c(1,8,9)], \n       lty=1)\n\n\n\n\n\nたしかに William Rehnquist は共和党，Ruth Bader Ginsburg と Stephen Breyer は民主党である．\n\n\n\n\n\n\\(0\\) の上に位置している Anthony Kennedy や Sandra Day O’Connor はほとんど中道的だが，やや保守党寄りである． Antonin Scalia は特に保守的な立場であることが知られている．\n\\(0\\) よりも下に位置するもう一人は David Souter であるが，彼はもともと保守系と目されていたが，後年リベラルな傾向を示したとされる．1"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#母数ロジットモデルの推定",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#母数ロジットモデルの推定",
    "title": "理想点解析のハンズオン",
    "section": "2 ２母数ロジットモデルの推定",
    "text": "2 ２母数ロジットモデルの推定\n\n2.1 はじめに\nMCMCpack パッケージで理想点推定の出力がつかめたいま，より詳しくモデルを見ていく．\n本節では rstan パッケージを用いて，項目反応モデルとして具体的な手順を踏んで推定してみる．\n\nlibrary(tidyverse)\ndf &lt;- Rehnquist %&gt;%\n  # データを長形式に変換\n  pivot_longer(cols = -c(term, time), names_to = \"name\", values_to = \"y\") %&gt;%\n  # ケース ID を追加\n  mutate(case = (row_number() - 1) %/% 9 + 1)\n\n\n\n2.2 １母数モデル（brms パッケージ）\nまずは最も簡単な項目反応モデルとして，\\(g\\) を logit リンクとして， \\[\ng(\\operatorname{P}[Y_{ij}=1])=\\alpha_0+\\alpha_j-x_i\n\\] というモデルを推定することを考えよう．\nbrms パッケージを用いれば，他の R パッケージと同様のインターフェイスで推定を行うことができる．\n\nlibrary(brms)\nformula &lt;- bf(\n  y ~ 1 + (1 | case) + (1 | name)\n)\nfit_1PL &lt;- brm(\n  formula,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ 1 + (1 | case) + (1 | name) \n   Data: df (Number of observations: 4343) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~case (Number of levels: 485) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.05      0.06     0.93     1.18 1.00     2016     2773\n\n~name (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.62      0.47     0.99     2.75 1.01      875     1534\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.11      0.54    -1.15     1.00 1.00      652     1263\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n簡単なモデルであるが切片項の ESS が低く，すでに暗雲が立ち込めている．\n\nplot(fit_1PL)\n\n\n\n\n\n\n\n\nここには変動係数（我々の欲しい潜在変数）はパラメータとみなされておらず，推定値が表示されないので次のようにしてプロットする必要がある：\n\nranef_legislator &lt;- ranef(fit_1PL)$name\nposterior_means &lt;- ranef_legislator[,1,\"Intercept\"]\nlower_bounds &lt;- ranef_legislator[,3,\"Intercept\"]\nupper_bounds &lt;- ranef_legislator[,4,\"Intercept\"]\nplot_legislator &lt;- data.frame(\n  legislator = rownames(ranef_legislator),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_1PL &lt;- ggplot(plot_legislator, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"1PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Legislator\")\np_1PL\n\n\n\n\n\n\n\n\nThomas や Scalia，そして Stevens が極端であることはとらえているが，Stevens や Ginsburg らリベラルな判事は左側に来て欲しいのであった．\n誘導が成功しておらず，片方の峯からサンプリングしてしまっている．\n\nprior_summary(fit_1PL)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n student_t(3, 0, 2.5)        sd            case                  0   \n student_t(3, 0, 2.5)        sd Intercept  case                  0   \n student_t(3, 0, 2.5)        sd            name                  0   \n student_t(3, 0, 2.5)        sd Intercept  name                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\n\n\n\n2.3 １母数モデル（rstan パッケージ）\nbrms パッケージ内で生成される Stan コードを参考にして，自分で Stan コードを書いて推定することもできる．\n\n\n\n\n\n\nStan コードの出力\n\n\n\n\n\nstancode(fit_1PL)\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n\n\n\nlibrary(rstan)\nstan_code &lt;- \"\ndata {\n  int&lt;lower=1&gt; n;  // data size: n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  real alpha_zero;  // intercepts\n  vector[J] alpha;  // item effects\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(alpha_zero | 3, 0, 2.5);\n  lprior += student_t_lpdf(alpha | 3, 0, 2.5);\n  lprior += student_t_lpdf(X | 3, 0, 2.5);\n}\nmodel {\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] - X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu + alpha_zero);\n  target += lprior;\n}\n\"\ncase_number &lt;- as.integer(nrow(df) / 9)\nindicator_i &lt;- rep(1:9, times = case_number)\nindicator_j &lt;- rep(1:case_number, each = 9)\ndf$i &lt;- indicator_i\ndf$j &lt;- indicator_j\n\ndf_NA &lt;- df %&gt;% filter(!is.na(y))\n\ndata &lt;- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit &lt;- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\nall_samples &lt;- extract(fit, pars = \"X\")$X\nx_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe &lt;- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\np &lt;- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"1PL Model (RStan)\")\n\nbrms による推定結果と，左右が逆になっている．これはモデル式を alpha[j[k]] - X[i[k]] と Stan コードに記入しており，brms の設定と（たまたま）逆になったためである．\n余談であるが，\\(\\alpha_0\\) を固定された切片項でなくて，変動させる（変量効果にする）ことで，事後分散は大きく縮まる．\n係数 \\(\\alpha_j,x_i\\) の事前分布を正規分布に変更したりしても結果はほとんど変わらない．\n\n\n2.4 ２母数モデル\n(Bafumi et al., 2005) など，多くの理想点モデルでは２母数ロジットモデルが用いられる： \\[\ng(x):=\\operatorname{logit}(x)=\\log\\frac{x}{1-x},\n\\] \\[\ng(\\mu_{i,j})=\\alpha_j+\\beta_j x_i=\\beta_j\\biggr(\\widetilde{\\alpha}_j+x_i\\biggl).\n\\] この際 \\(x_i\\) は \\(i\\) 番目の判事の 理想点 といい，\\(\\alpha_j,\\beta_j\\) は \\(j\\) 番目の事件の性質を表すパラメータである．\nものによっては判事の立場が関係ない事件もあるため，\\(\\beta_j\\) が用意されている．\n基本的にこの識別パラメータが正になるように調整したいが，明示的にそうすることはしない．\n次節で説明する方法により，理想点 \\(x_i\\) が大きい場合は保守的な判断を下しやすいものと解釈できるように設計することができる（\\(x_i\\) を数直線上にプロットした際に，リベラルな場合に左に，保守的な場合に右に来るようにする）が，ここではストレートに実装してみよう．\nlibrary(rstan)\nstan_code &lt;- \"\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  vector[J] alpha;  // item effects\n  vector[J] beta;  // item discremination\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\"\ncase_number &lt;- as.integer(nrow(df) / 9)\nindicator_i &lt;- rep(1:9, times = case_number)\nindicator_j &lt;- rep(1:case_number, each = 9)\ndf$i &lt;- indicator_i\ndf$j &lt;- indicator_j\n\ndf_NA &lt;- df %&gt;% filter(!is.na(y))\n\ndata &lt;- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit &lt;- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\nall_samples &lt;- extract(fit, pars = \"X\")$X\nx_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe &lt;- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\nggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"Ideal Points of Rehnquist\")\n\n２つを並べてみると\n\n\n\n\n\n\n\n\n\n1PL Model\n\n\n\n\n\n\n\n2PL Model\n\n\n\n\n\nモデルの自由度が上がったことにより，事後分散が大幅に小さくなっていることがわかる．\nまた，1PL の場合と再び左右が反転し，最後のリベラルな判事の２人 Ginsburg と Breyer が右に来てしまっている．\nこれはモデル式を mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]]; と足し算に戻したからだろうか？\n実はそうではない．実際，何度か MCMC を回し直すと，ちゃんとリベラルな判事が左に来てくれることもある．\n即ち，事後分布が多峰性を持つ のである！\n\n\n\n2.5 識別可能性\n実は２母数ロジットモデルでは３つ識別不可能性を引き起こす対称性がある：\n\n\n\n\n\n\n(Section 2 Bafumi et al., 2005)\n\n\n\n\n加法的別名 (additive aliasing)\n\\(\\widetilde{\\alpha}_j,x_i\\) に同じ数を足した場合と \\(\\beta_j\\) と \\((\\widetilde{\\alpha}_j,x_i)\\) に同じ数を乗じた／除した場合，全く等価なモデルが得られる．すなわちスケールを定める必要がある．\n乗法的別名 (additive aliasing)\n\\(\\beta_j\\) を \\(a&gt;0\\) 倍し，\\(\\widetilde{\\alpha}_j,x_i\\) を \\(1/a\\) 倍すると等価な尤度を定める．\n反転対称性 (reflection invariance)\n最後に \\(\\beta_j\\) の符号の問題があるためである．\\(\\beta_j\\) を \\(-1\\) 倍させることで \\(\\widetilde{\\alpha}_j,x_i\\) の役割を \\(-1\\) 倍させることができる．このまま推定すると事後分布は \\(0\\) に関して対称な形を持つことになる．\n\n\n\n\n\\(\\beta_j\\) の符号を制約したり，特定の判事の \\(x_i\\) を固定して参照点とするなどの方法があるかもしれないが，ここでは (2.2.3 節 Bafumi et al., 2005, p. 178) に倣って，階層モデルの方法により，構造的なやり方でモデルに情報を伝える．\nというのも，理想点 \\(x_i\\) に次の階層構造を入れるのである： \\[\nx_i=\\delta+\\gamma z_i+\\epsilon_i\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1).\n\\]\n\\(z_i\\) は当該判事を示した大統領の所属政党を表す２値変数で，共和党ならば \\(z_i=1\\) とする．そして \\(\\gamma\\) に \\(\\mathbb{R}_+\\) 上に台を持つ事前分布を置く．\n\n\n\n\n\n\n(Bafumi et al., 2005) による理想点モデルの階層化\n\n\n\nこのように共変量を適切な階層に追加することは，モデルに自然な形で正則化情報を伝えることに繋がり，モデルの識別やより現実的な推定値の獲得に繋がる．\n\n\n\n\n2.6 階層ベイズ推定\n\n2.6.1 指名大統領の政党属性\n続いて 2.5 で検討した，(Bafumi et al., 2005) による階層ベイズモデルにより緩やかに情報を伝えることで識別可能性を保つ方法を検討する（第 2.6.2 節）．\n\ndf &lt;- df %&gt;%\n  mutate(\n    nominator = case_when(\n      name %in% c(\"Rehnquist\", \"Stevens\") ~ \"Nixon\",\n      name %in% c(\"O.Connor\", \"Scalia\", \"Kennedy\") ~ \"Reagan\",\n      name %in% c(\"Souter\", \"Thomas\") ~ \"Bush\",\n      name %in% c(\"Breyer\", \"Ginsburg\") ~ \"Clinton\"\n    )\n  )\ndf$x &lt;- ifelse(\n  df$nominator %in% c(\"Nixon\", \"Reagan\", \"Bush\", \"Trump\"),\n  1, -1)\n\n\n\n2.6.2 階層２母数モデル\nx の情報を階層的に伝えるには，もはや brms パッケージでは実行できないようである．\n\n\n\n\n\n\nfit_2PL::brmsfit で用いた Stan コードの出力\n\n\n\n\n\nbrms パッケージでは stancode() 関数を用いて Stan コードを出力できる．\nstancode(fit_2PL)\n結果は以下の通りになる：\n// generated with brms 2.21.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  vector[N] Z_2_2;\n  int&lt;lower=1&gt; NC_2;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  matrix[M_2, N_2] z_2;  // standardized group-level effects\n  cholesky_factor_corr[M_2] L_2;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  matrix[N_2, M_2] r_2;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_2] r_2_1;\n  vector[N_2] r_2_2;\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  // compute actual group-level effects\n  r_2 = scale_r_cor(z_2, sd_2, L_2);\n  r_2_1 = r_2[, 1];\n  r_2_2 = r_2[, 2];\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(to_vector(z_2));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n  vector&lt;lower=-1,upper=1&gt;[NC_2] cor_2;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_2) {\n    for (j in 1:(k - 1)) {\n      cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n    }\n  }\n}\nfunction ブロックでは scale_r_cor() 関数が定義されている．標準化された項目変数 \\((\\alpha_j,\\beta_j)\\) を z として，その各次元の標準偏差を含む２次元ベクトルを sd_2，Cholesky 因子を L_2 として，行列積 sd_2*L2 にベクトル z を掛けて返している．これは \\((\\alpha_j,\\beta_j)\\) を３つの要素（対角行列・Cholesky 因子・標準化されたベクトル）に因数分解して計算していることに起因する．\ndata ブロックでデータのコーディングを定めている．Y は \\(NJ\\) 行列である．case が ID2 で name が ID1 に対応する．判事 \\(i\\in[n]\\) は N_1 人，件数 \\(j\\in[J]\\) は N_2 件．このデータから M_1=1 次元の理想点を持った M_2=2 パラメータモデルを推定する．Z_1_1 が判事を表す標示変数で，項目を表す標示変数は Z_2_1 と Z_2_2 である．\nparameter ブロックでは標準化された理想点 z_1 と項目パラメータ z_2，それぞれの標準偏差 sd_1, sd_2 を宣言している．z_1 だけ N_1 ベクトル，z_2 は N_2 行 M_2 列の行列であることに注意．\ntransformed_parameters で真のスケールに戻す．r_1_1=(sd_1[1]*(z_1[1])) は理想点 \\(x_1\\) にあたり，r_2=scale_r_cor(z_2,sd_2,L_2) は項目パラメータ \\((\\alpha_j,\\beta_j)\\) にあたる．その後 r_2_1=r_2[,1] と r_2_2=r_2[,2] でそれぞれ \\(\\alpha_j\\) と \\(\\beta_j\\) に分解している．最後に sd_1, sd_2 に t-分布，L_2 に Cholesky 分布を事前分布として定義している．\nmodel で尤度を定義している．mu はこのブロックでしか使われない線型予測子の格納変数である．\nmu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\nにより \\(\\alpha_j+\\beta_j x_i\\) が計算されている．\\(j\\) は J_2[n], \\(i\\) は J_1[n] で表されている．最後に bernoulli_logit_lpmf(Y | mu) で尤度が定義される．\ngenerated quantities ブロックでは相関行列 cor_2 を計算している．\n\n\n\n\n\nFiles/bafumi.stan\n\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  vector[N] Z;  // covariates for judges\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points for judges\n  vector[J] alpha;\n  vector[J] beta;\n\n  real delta;\n  real gamma;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(delta | 3, 0, 2.5);\n  lprior += student_t_lpdf(gamma | 3, 0, 2.5);\n  lprior += student_t_lpdf(alpha | 3, 0, 2.5);\n  lprior += student_t_lpdf(beta | 3, 0, 2.5);\n  lprior += std_normal_lpdf(X);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  X ~ normal(delta + Z * exp(gamma), sigma);\n\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\n\n\n\n\n階層モデルの推定結果\n\n\n\n\n\n\n\n\n\n\n\n今回の推定結果\n\n\n\n\n\n\n\n２母数モデル 2.4 の推定結果\n\n\n\n\n\n\n\n\n2.7 識別性の影響\n実は階層的に緩やかにしか情報を伝えていないために，今回の \\(\\gamma\\) への事前分布の設定では，まだ \\(0\\) の近くに密集することでもう一つの峰を作り出してしまうようである．\n例えば，Stevens, Souter, Ginsburg, Breyer らリベラルな判事の初期位置を右側の \\(1\\) に，Thomas, Scalia ら保守派の判事の初期位置を \\(-1\\) に，そのほか中道的な判事の初期位置を \\(0\\) にしてサンプリングをすると，ほぼ確実に左右が逆になった結果が出てくる．\nその際は \\(\\gamma\\) の事後分布が大きく違う．\ninit_values &lt;- list(X = c(-1.0,1.0,1.0,-1.0,-1.0,0.0,0.0,1.0,1.0), alpha = rep(0.0, case_number), beta = rep(0.0, case_number), delta = 0.0, gamma = 0.0, sigma = 1.0)\nfit &lt;- stan(\"Files/bafumi.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000, init = rep(list(init_values), 4))\n\n\n\n\n\n\n\n\n\n\n理想点の事後平均の位置が正しい場合の回帰係数 \\(\\gamma\\) の事後分布\n\n\n\n\n\n\n\n理想点の事後平均の位置が間違っている場合の回帰係数 \\(\\gamma\\) の事後分布\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点の事後平均の位置が正しい場合の回帰係数 \\(\\delta\\) の事後分布\n\n\n\n\n\n\n\n理想点の事後平均の位置が間違っている場合の回帰係数 \\(\\delta\\) の事後分布\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点の事後平均の位置が正しい場合の理想点の事後分布\n\n\n\n\n\n\n\n理想点の事後平均の位置が間違っている場合の理想点の事後分布\n\n\n\n\n\n\n\n2.8 事後分布の２峰性\n\nしかし \\(\\alpha_j,\\beta_j,\\delta,\\gamma\\) の事前分布はあまり大きな影響はないようである．\\(\\gamma\\) の中心も少しズラしたくらいでは関係ない．\\(\\gamma\\) を分散 \\(1\\) で平均 \\(10\\) の正規分布などにすると，\\(\\gamma\\) の事後分布は右に引っ張れる．\n\n\n\n\n\n\n\n\n\n元々の事前分布\n\n\n\n\n\n\n\n\\(\\gamma\\) を右に引っ張った場合の事後分布\n\n\n\n\n\nしかし理想点の結果自体は（左右の別を除いて）ほとんど変わらない：\n\n\n\n\n\n\n\n\n\n元々のモデルの結果\n\n\n\n\n\n\n\n\\(\\gamma\\) の事後分布を右に引っ張った場合\n\n\n\n\n\n次は初期値をランダムとして９回実行して得る結果である：\n\n\nexperiment.r\n\nfor (i in 1:9) {\n  execution_time &lt;- system.time({\n    fit &lt;- stan(\"Files/bafumi_normal.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n  })['elapsed']\n  all_samples &lt;- extract(fit, pars = \"X\")$X\n  last_1000_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n  plot_dataframe &lt;- data.frame(\n      legislator = colnames(Rehnquist)[1:9],\n      mean = apply(last_1000_samples, 2, mean),\n      lower = apply(last_1000_samples, 2, quantile, probs = 0.025),\n      upper = apply(last_1000_samples, 2, quantile, probs = 0.975)\n  )\n  title &lt;- paste0(\"Elapsed time: \", execution_time, \" seconds\")\n  p &lt;- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n      geom_point() +\n      geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n      labs(x = \"Mean\", y = \"Legislator\", title = title)\n  ggsave(paste0(\"Files/experiment_\", i, \".png\"), p)\n}\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.99 seconds\n\n\n\n\n\n\n\nElapsed time: 17.01 seconds\n\n\n\n\n\n\n\nElapsed time: 16.30 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 15.00 seconds\n\n\n\n\n\n\n\nElapsed time: 14.98 seconds\n\n\n\n\n\n\n\nElapsed time: 15.06 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.42 seconds\n\n\n\n\n\n\n\nElapsed time: 16.57 seconds\n\n\n\n\n\n\n\nElapsed time: 17.18 seconds\n\n\n\n\n\nなんと，緩やかな情報伝達に拘らず，判事の理想点はほぼ完全に識別されているが，方向が違う！\nサンプリングを繰り返すことで結果がよく移り変わる．即ち，この事後分布は２峰あるようである．\nそして burn-in 後の 1000 回のサンプリング期間では，異なる峰の間を移り変わることはほとんどないようである．\nこの点については次の稿も参照：\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#introduction",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#introduction",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n1.1 The 2PL Model\nSuppose we have a binary response variable \\(Y_{i,j}\\) for the \\(i\\in[N]\\)-th judge and the \\(j\\in[J]\\)-th case.\n2-parameter logistic model (2PL) can be written as follows: \\[\nY_{i,j}\\sim\\operatorname{Bernoulli}(\\mu_{i,j}),\n\\tag{1}\\] \\[\n\\operatorname{logit}(\\mu_{i,j})=\\alpha_j+\\beta_j X_i=:\\beta_j\\biggr(\\widetilde{\\alpha}_j+X_i\\biggl).\n\\tag{2}\\]\nAlthough the model is called the 2PL model, we have three parameters \\(\\alpha_j,\\beta_j,X_i\\) in total, two for the cases \\(j\\in[J]\\) and one for the judges \\(i\\in[N]\\).\nIn IRT (Item Response Theory) vocabulary, we call \\(\\alpha_j\\) the difficulty, and \\(\\beta_j\\) the discrimination parameter of the \\(j\\)-th ‘item’.\n\\(X_i\\) may be called the latent trait or ability parameter of the \\(i\\)-th ‘unit’ in that context, but here we call it the ideal point of the \\(i\\)-th judge.\n\n\n1.2 The Problem of Identifiability\nAs (Section 2 Bafumi et al., 2005) nicely categorized, the above model has three sources of non-identifiability:\n\n\n\n\n\n\nSources of non-identifiability\n\n\n\n\nAdditive aliasing / base point indeterminancy\nFor any \\(c\\in\\mathbb{R}\\), the transformation \\[\n(\\beta_j,\\widetilde{\\alpha}_j,X_i)\\mapsto(\\beta_j-c,\\widetilde{\\alpha}_j+c,X_i+c),\\qquad c\\in\\mathbb{R},\n\\] does not change the likelihood.\nMultiplicative aliasing / scaling indeterminancy\nSame applies to the following transformation: \\[\n(\\beta_j,\\widetilde{\\alpha}_j,X_i)\\mapsto(c^{-1}\\beta_j,c\\widetilde{\\alpha}_j,cX_i),\\qquad c&gt;0.\n\\]\nReflection invariance / sign indeterminacy\n\\[\n(\\beta_j,\\widetilde{\\alpha}_j,X_i)\\mapsto(-\\beta_j,-\\widetilde{\\alpha}_j,-X_i)\n\\]\n\n\n\nAlthough all of the three problems may be settled by setting informative prior distributions to one of the \\(\\alpha_j,\\beta_j,X_i\\)’s, e.g., \\(X_i\\sim N(0,1)\\), the authors (Bafumi et al., 2005) propose a different approach to the thrid problem, reflection invariance.\n\n\n1.3 Resolution by Hierarchical Structure\n(Bafumi et al., 2005) proceed to introduce a person(/judge)-level predictor \\(Z_i\\) to the model, i.e., \\[\nX_i\\sim\\mathrm{N}(\\delta+\\gamma Z_i,\\sigma^2)\n\\] to indirectly inform the model of the correct sign of the ideal points.\nSpecifically, \\(Z_i\\in\\{\\pm1\\}\\) corresponds to the party of the nominating president of \\(i\\)-th judge; \\(Z_i=+1\\) corresponds to the Republican party, and \\(Z_i=-1\\) corresponds to the Democratic party.\nIn this way, (Bafumi et al., 2005) tried to guide the model & likelihood to have only one mode, where liberal judges would be on the left and conservative judges would be on the right on the \\(X_i\\in\\mathbb{R}\\) axis.\n\n\n1.4 The Problem Remains …\nLet us consider the data from the 1994-2004 terms of the U.S. Supreme Court here, although (Bafumi et al., 2005) used the data from the 1954-2000 terms. The data is available as Rehnquist via the MCMCpack package (Martin et al., 2011) in R.\n\nlibrary(MCMCpack)\ndata(Rehnquist)\nkable(head(Rehnquist))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRehnquist\nStevens\nO.Connor\nScalia\nKennedy\nSouter\nThomas\nGinsburg\nBreyer\nterm\ntime\n\n\n\n\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\nNA\n0\n1994\n1\n\n\n0\n1\n0\n1\n1\n1\n0\n1\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1994\n1\n\n\n\n\n\nLet us first see the ‘correct’ output from the model. The precise meaning of ‘correct’ will be clarified later in the next Section 2, along with the Stan codes.\n\n\n\nA ‘correct’ output from bafumi_normal.stan\n\n\nWe see four judges classified as liberal, namely Stevens, Souter, Ginsburg & Breyer. The last two judges were nominated by Bill Clinton, a Democrat, while the first two were nominated by Republican presidents.\nThis result aligns with the common understanding of the Supreme Court justices. For instance, we quote a sentence from the wikipedia page of John Paul Stevens:\n\nDespite being a registered Republican who throughout his life identified as a conservative, Stevens was considered to have been on the liberal side of the Court at the time of his retirement.\n\nThe similar situation applies to David Souter.\nHere we notice that two liberal judges have \\(Z_i=+1\\), while the other two have \\(Z_i=-1\\). The predictive ability of the covariate \\(Z_i\\) is (presumably) weak during the 1994-2004 term.\nTherefore, the information from \\(Z_i\\) about the sign of the ideal points may not be strong enough to resolve the identifiability problem. In that case, the posterior distribution would be bimodal. Indeed, this is the case, as we will see next."
  },
  {
    "objectID": "posts/2024/R/Stan1.html#stan-言語の基本文法",
    "href": "posts/2024/R/Stan1.html#stan-言語の基本文法",
    "title": "Stan 入門",
    "section": "1 Stan 言語の基本文法",
    "text": "1 Stan 言語の基本文法\n\n1.1 はじめに\nStan 言語は確率モデルを３つのブロックに分けて記述する．その３つとはデータ，パラメータとモデルである．\n\n以上の３要素により，事後分布が定まる．それぞれの要素は，対応した名前を持ったスコープ {} 内で，この順番で定義される慣習がある．\n以前のスコープ内で定義された識別子は，その後のスコープでも利用可能になる．\n\n\nstan_code.stan\n\ndata {\n    int&lt;lower=0&gt; N;  // N &gt;= 0\n    array[N] int&lt;lower=0, upper=1&gt; y;  // y[n] in {0, 1}\n}\nparameters {\n    real&lt;lower=0, upper=1&gt; theta;  // theta in [0, 1]\n}\nmodel {\n    theta ~ beta(1, 1);  // uniform prior\n    y ~ bernoulli(theta);  // observation model\n}\n\nその後 Stan プログラムは事後分布の対数密度を表す C++ 関数にコンパイルされる．\n最終的にこの対数尤度を用いて，その勾配を自動微分により計算し，Hamiltonian Monte Carlo による事後分布サンプリングを実行する．\nStan エコシステムの詳説は次節 2 に回し，ここでは Stan 言語の基本に集中する．\n\n\n1.2 確率的プログラミング言語\nStan 言語のように確率モデルを記述する（より正確には事後分布の尤度を記述する高級）言語を 確率的プログラミング言語 (PPL: Probabilistic Programming Language) という．\n最初期の確率的プログラミング言語の１つに WinBUGS (Lunn et al., 2000) の実装に代表される BUGS (Bayesian analysis Using Gibbs Sampling) がある．\nBUGS では事後分布を詳細に 有向グラフィカルモデル (DAG) で記述し，Gibbs Sampling により事後分布をサンプリングするという仕組みであった．\nStan 言語はさらに柔軟に，（正規化されているとは限らない）対数尤度を記述できるようになっている．\n理想的にはあらゆる確率モデルを扱いたいものであるが，現状の Stan 言語はパラメータとしては連続変数のみを持つモデルのみが定義可能である．\n\n\n1.3 data ブロック\nStan は静的な型システムを持ち，変数宣言の際には必ず型を指定する必要がある．\n\n\n\n\n\n\nStan の代表的なデータ型\n\n\n\n\n実数 \\(x\\in\\mathbb{R}\\)\nreal x;\n実数 \\(x\\in[a,b]\\)\nreal&lt;lower=a upper=b&gt; x;\n単体 \\(x\\in[0,1]^N,\\sum_{n=1}^Nx_n=1\\)\nsimplex[N] x;\n自然数 \\(N\\in\\mathbb{N}\\)\nint&lt;lower=0&gt; N;\n配列 \\(x\\in\\mathbb{R}^N\\)：純粋なコンテナ型であり，線型代数ライブラリは適用不可．\narray[N] real x;\nreal x[N] という記法は Stan 2.26 以降使われないことに注意（docs 参照）．\n\n\n\nなお，各ブロック内において，あらゆる変数宣言は全ての非宣言的文の前に来る必要がある．\n{\n    real variable1 = 5;\n    variable1 /= 2;  // ここでエラー\n    real variable2 = exp(variable1);\n}\n\n\n1.4 transformed data ブロック\ndata ブロックでは許されないが，一般に Stan 言語では変数宣言と同時に代入もできる．\n代入を省略した場合は NaN によって初期化される．\n\n\n\n\n\n\nStan の代表的な線型代数関連のデータ型\n\n\n\n\nベクトル \\(x\\in\\mathbb{R}^5\\)：c++ の線型代数ライブラリが使える\nvector[5] x = [0, 1, 2, 3, 4];\n横ベクトル \\(y\\in(\\mathbb{R}^5)^*\\)：\\(y*x\\) が計算可能．\nrow_vector[5] y = [1, 2, 3, 4, 5]';\nx*y; // 計算可能\n行列 \\(A\\in M_{N,M}(\\mathbb{R})\\)\nmatrix[N, M] A;\n\n\n\ntransformed data ブロックでは，観測される訳でもなければパラメータでもないような，内部で使われる変数が定義される．\n\n\n1.5 parameters ブロック\nparameters ブロックも同様にして変数を宣言する．\nmodel ブロックで使われる変数は，data, parameters ブロックのいずれかで宣言されている必要がある．\n\\(x\\in[a,b]^N\\) のような制約領域を持つ変数が parameters ブロックで宣言された場合，Stan は内部でパラメータ変換を行い，\\([a,b]\\) を \\(\\mathbb{R}\\) 上に写してサンプリングを実行する．\nGamma 分布のように台が \\(\\mathbb{R}\\) の部分集合になるような事前分布を扱う場合，対応する制約領域をパラメータに定義することがサンプリングの効率を上げる．\n一方で，\\(\\mathbb{R}\\) 全体を台に持つ事前分布を持つパラメータに対して制約領域を宣言した場合，truncate をした事前分布を定義したことに等価になる．\n\n\n1.6 model ブロック\nモデルブロックでは対数密度関数の値が変数 target として保持されており，target() 関数でアクセス可能である．\n次の３つの文は等価になる：\nbeta ~ normal(0, 1);\nbeta ~ normal(beta | 0, 1);  // syntax sugar\ntarget += normal_lpdf(beta | 0, 1);  // 実際の処理に近い\n\n\n1.7 ループと制御\nfor (n in N1:N2) {\n  // Statements executed for each N1 &lt;= n &lt;= N2\n}\n\nif (condition) {\n  // Statements evaluated if condition is true\n} else {\n  // Statements evaluated if condition is false\n}\n\n\n1.8 transformed parameters / Generated Quantities ブロック\ntransformed parameters ブロックと Generated Quantities ブロックでは，parameters ブロックで宣言されたパラメータの関数として推定対象を定義する．\nこの推定対象は推論後に事後平均が取られて報告される．\n２つの違いは，transformed parameters ブロックでは model ブロックの前に配置され，model ブロックでも使えるのに対し，Generated Quantities ブロックでは model ブロックの後に配置され，純粋に事後分布の関数として処理される点である．\nなお，関数定義は functions ブロックで行う．\nfunctions {\n    real baseline(real a1, real a2, real theta) {\n        return a1 * theta + a2;\n    }\n}"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#estimation-by-stan",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#estimation-by-stan",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "2 Estimation by Stan",
    "text": "2 Estimation by Stan\n\n\nbafumi_normal.stan\n\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  vector[N] Z;  // covariates for judges\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points for judges\n  vector[J] alpha;\n  vector[J] beta;\n\n  real delta;\n  real gamma;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(delta);\n  lprior += std_normal_lpdf(gamma);\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  X ~ normal(delta + Z * exp(gamma), 1);\n\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\n\n\nFiles/experiment.r\n\nfor (i in 1:100) {\n  fit &lt;- stan(\"bafumi_normal.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n\n  all_samples &lt;- extract(fit, pars = \"X\")$X\n  last_1000_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n  mean &lt;- apply(last_1000_samples, 2, mean)\n  if (mean[9] &gt; 0.5) {\n    count &lt;- count + 1\n  }\n}\nprint(count)\n\n\n\n[1] 52\n\n\nThe following plots are some (first 9) of the results from the experiment.\n\n\n\n\n\n\n\n\n\nElapsed time: 16.99 seconds\n\n\n\n\n\n\n\nElapsed time: 17.01 seconds\n\n\n\n\n\n\n\nElapsed time: 16.30 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 15.00 seconds\n\n\n\n\n\n\n\nElapsed time: 14.98 seconds\n\n\n\n\n\n\n\nElapsed time: 15.06 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.42 seconds\n\n\n\n\n\n\n\nElapsed time: 16.57 seconds\n\n\n\n\n\n\n\nElapsed time: 17.18 seconds"
  },
  {
    "objectID": "static/ResearchJP.html#階層モデルへの応用",
    "href": "static/ResearchJP.html#階層モデルへの応用",
    "title": "研究紹介",
    "section": "階層モデルへの応用",
    "text": "階層モデルへの応用\nベイズ統計が最も力を発揮する設定は，階層モデリングやノンパラメトリックモデリングなどの複雑なモデルです．\n\n項目反応モデル\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#sec-bafumi-stan",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#sec-bafumi-stan",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "2 Estimation by Stan",
    "text": "2 Estimation by Stan\nThe 2PL model (1), (2) can be written in Stan as follows:\n\n\nbafumi_normal.stan\n\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  vector[N] Z;  // covariates for judges\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points for judges\n  vector[J] alpha;\n  vector[J] beta;\n\n  real delta;\n  real gamma;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(delta);\n  lprior += std_normal_lpdf(gamma);\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  X ~ normal(delta + Z * exp(gamma), 1);\n\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\nUsing this Stan code, we run the following experiment, where we run 4 chains in parallel, each with 4000 iterations, 3000 of which are used for warmup. The chains are initialized randomly.\n\n\nFiles/experiment.r\n\nfor (i in 1:100) {\n  fit &lt;- stan(\"bafumi_normal.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n\n  all_samples &lt;- extract(fit, pars = \"X\")$X\n  last_1000_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n  mean &lt;- apply(last_1000_samples, 2, mean)\n  if (mean[9] &gt; 0.5) {\n    count &lt;- count + 1\n  }\n}\nprint(count)\n\n\n\n[1] 52\n\n\nThe following plots are some (first 9) of the results from the experiment.\n\n\n\n\n\n\n\n\n\nElapsed time: 16.99 seconds\n\n\n\n\n\n\n\nElapsed time: 17.01 seconds\n\n\n\n\n\n\n\nElapsed time: 16.30 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 15.00 seconds\n\n\n\n\n\n\n\nElapsed time: 14.98 seconds\n\n\n\n\n\n\n\nElapsed time: 15.06 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.42 seconds\n\n\n\n\n\n\n\nElapsed time: 16.57 seconds\n\n\n\n\n\n\n\nElapsed time: 17.18 seconds\n\n\n\n\n\nWe see that the result has two patterns and they are in symmetry with each other.\nThis phenomenon proves that the posterior distribution of the ideal points is bimodal, indicating the weak identifiability of the model."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#conclusion",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#conclusion",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "3 Conclusion",
    "text": "3 Conclusion\n(Bafumi et al., 2005)’s hierarchical resolution of the identifiability problem by the covariate \\(Z_i\\) will fail if the covariate \\(Z_i\\) is not informative enough.\nIn that case, the posterior distribution of the ideal points will be bimodal."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "2 Sticky PDMP",
    "text": "2 Sticky PDMP\n\n2.1 設定\nパラメータ \\(x\\in\\mathbb{R}^d\\) 上に spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) \\[\np(dx)=\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{1}\\] を導入して，ベイズ変数選択を行うとしよう．\nこのとき，モデルの対数尤度を \\(\\ell(x):=\\log p(y|x)\\) とすると，事後分布は \\[\np(x|y)\\,dx\\,\\propto\\,p(y|x)p(dx)=e^{\\ell(x)}\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)+(1-\\omega_i)\\delta_0(x_i)\\biggl)\n\\tag{2}\\] と表せる．\nそこでこの設定を少し抽象化して，ポテンシャル \\(\\Psi:\\mathbb{R}^d\\to\\mathbb{R}\\) を通じて \\[\n\\mu(dx)=Ce^{-\\Psi(x)}\\prod_{i=1}^d\\left(dx_1+\\frac{1}{\\kappa_i}\\delta_0(dx_i)\\right)\n\\tag{3}\\] と表せる分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) からサンプリングする問題を考える．\n事後分布 (2) は \\[\n\\kappa_i=\\frac{\\omega_i}{1-\\omega_i}p_i(0)(&gt;0)\n\\] と取った場合にあたる．\nこの \\(\\mu\\) (3) は Lebesgue 測度に関して絶対連続でないため密度を持たず，通常の勾配を用いた MCMC 法を直接は適用できない．\n\n\n2.2 他手法との比較\n\n\n\n\n\n\n\n(3) のような非絶対連続分布からのサンプリングは難しい．そのため spike-and-slab 事前分布 (1) の \\(\\delta_0\\) を分散の小さな正規分布などに軟化した絶対連続な分布に置き換えて，これに対して HMC などの勾配ベースの MCMC 法を適用することもあり得る (Goldman et al., 2022)．\nあるいは初めから Laplace 分布や馬蹄事前分布などの絶対連続なスパース誘導事前分布を用い，事後分布は Gibbs サンプラーなどの勾配情報を用いないサンプラーで行う (Griffin and Brown, 2021)，というアプローチもあり得る．\n\n\n\n\nしかし (3) のようなアトムを持った分布に直接適用できる Sticky PDMP によるアプローチは，２の方法と違って非可逆なモデル間ジャンプを達成する効率的なサンプラーである．\nさらにその上，Reversible-Jump MCMC の拡張と見れる通り，特定の部分空間にトラップされていた総時間を計算することで，ベイズ因子を計算せずに事後包含確率 (PIP: Posterior Inclusion Probability) を，１の方法と違って誤差なく計算できるという美点がある．\n\n\n2.3 サンプラー\n\n\n\n\n\n\nThe Sticky Zig-Zag Sampler (Bierkens et al., 2023)\n\n\n\n\n基本的には \\[\n\\mathbb{R}^d\\times\\{\\pm1\\}^d\n\\]\n上を動く Zig-Zag サンプラーである．\n任意の座標成分が \\(0\\) になったとき，すなわち\n\\[\n\\left\\{(x,v)\\in\\mathbb{R}^d\\times\\{\\pm 1\\}^d\\mid\\exists i\\in[d]\\;x_i=0\\right\\}\n\\] を通過したとき，座標成分 \\(x_i\\) は \\(0\\) に固定され，Poisson ジャンプが生じるまでそのままである．\n\n\n\nこれは一見 Markov 過程にならないが，状態空間を拡張して \\[\nE:=\\mathbb{R}_{00}^d\\times\\{\\pm1\\}^d,\\qquad\\mathbb{R}_{00}:=(-\\infty,0^-]\\sqcup[0^+,\\infty)\n\\] と考えると，この上の Markov 過程になる．\n第 \\(i\\in[d]\\) 成分が \\((x_i,v_i)=(0^-,-1)\\) または \\((0^+,1)\\) になったとき，この座標は \\((x_i,v_i)\\) のまま動かなくなり，この期間だけ \\(v_i\\) は必ずしも「速さ」を表さなくなる，と解する．\nトラップ状態から脱出するレートは \\(\\kappa_i\\) とする．\n\n\n2.4 エルゴード性\nこうして構成された Zig-Zag サンプラーは，分布 \\(\\mu\\) の裾が重すぎない場合，具体的にはある \\(c&gt;d,c'\\in\\mathbb{R}\\) が存在して \\[\n\\Psi(x)&gt;c\\log\\lvert x\\rvert-c',\\qquad x\\in\\mathbb{R}^d,\n\\] を満たすとき（全変動ノルムに関して）エルゴード的である (Prop. A.9 Bierkens et al., 2023, p. 20)．\n\n\n\n\n\n\n(\\(0\\) への再起時刻 Bierkens et al., 2023, p. 6)\n\n\n\n\\(\\mathbb{R}^d\\) の原点への再起時刻 \\(T_0\\) の期待値は \\[\n\\operatorname{E}[T_0]=\\frac{1-\\mu(\\{0\\})}{d\\kappa\\mu(\\{0\\})}.\n\\]\n\n\n\n\n2.5 極限\nSticky Zig-Zag サンプラーは，spike-and-slab 事前分布 (1) を，十分小さい標準偏差 \\(c_i&gt;0\\) を持つ正規分布で近似した \\[\n\\widetilde{p}(dx)=\\sum_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_1+(1-\\omega_i)\\mathrm{N}(0,c_i^2)\\biggl)\n\\] に対する Zig-Zag サンプラーの，\\(c_i\\to0\\) の極限の場合と見れる (Chevallier et al., 2023, p. 2917)．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#非可逆な方法",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#非可逆な方法",
    "title": "超次元 MCMC",
    "section": "3 非可逆な方法",
    "text": "3 非可逆な方法\n\n3.1 リフティングを用いる方法\nリフティングは状態空間を拡張し，MH 法を非可逆にする方法である (Diaconis et al., 2000), (Chen et al., 1999)．\nこの方法をモデル間のジャンプに応用したものが (Gagnon and Doucet, 2020) で議論されている．\n\n\n3.2 非可逆なサンプラーを用いる方法\n連続時間ベースの MCMC 法は，その非可逆なダイナミクスから従来の MCMC よりも良い収束性を持つ (Andrieu and Livingstone, 2021)．\n連続時間 MCMC は従来とは全く異なるアルゴリズムをもち，Poisson 過程の到着によりランダムなジャンプをし，それまでは決定論的な動きを続ける（Zig-Zag サンプラーや BPS サンプラーでは直進）．\nこのようなサンプラーにモデル間の移動を導入するには，新たなタイマー（Poisson 過程）を導入して，その到着のたびにジャンプをすれば良い．\nこの考え方を推し進めることで，非可逆なモデル間ジャンプをデザインすることができる．\n\n\n3.3 Sticky PDMP\nモデル選択の文脈では，\\(E_k\\) の間に自然な包含関係がある場合が多い．特に飽和モデルを \\(x\\in\\mathbb{R}^p\\) として，この係数に spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定して変数選択をする状況を考える： \\[\np(dx)=\\prod_{i=1}^p\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl).\n\\]\n\\(\\mathbb{R}^p\\) 上でサンプリングを開始し，特定の部分空間に到達する（＝どれかの座標成分が \\(0\\) になる）たびに，その部分モデルにどれくらいの時間とどまるかを決める「タイマー」を開始し，その間部分空間内のみを探索する，と設計する．\nタイマーが鳴った際は止めていた（速度成分を \\(0\\) にしていた）座標成分を，タイマーが開始された状況と同じ速度で動かし始める．\nこうして得られるサンプラーは Sticky PDMP (Bierkens et al., 2023) と呼ばれ，非可逆なサンプラーダイナミクスに依存してタイマーが発動するために，モデル間の非可逆なジャンプを達成することになる．\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.4 境界の導入による方法\nモデル選択の文脈では \\(E_k\\) の間に自然な包含関係があった (nested models) が，合祖木の空間やグラフの空間など，従来から Monte Carlo シミュレーションが困難な離散構造は多く知られている．\nそこで一般的な設定を考えたいが，その際に使える Sticky PDMP の一般化のような方法が (Koskela, 2022) で提案されている．\nこの方法では \\(\\mathbb{F}\\) を一般的な可算集合とし， \\[\n\\mathbb{F}\\leftarrow\\bigsqcup_{k\\in\\mathbb{F}}\\Omega_m=:\\Omega,\\qquad\\Omega_m\\overset{\\mathrm{open}}{\\subset}\\mathbb{R}^d,\n\\] をその上のファイバー束とする．各被覆の境界 \\[\n\\partial\\Omega:=\\bigsqcup_{k\\in\\mathbb{F}}\\partial\\Omega_k\n\\] からサンプラーが出ようとするときに，ある核に従って \\(\\mathbb{F}\\) をジャンプするとするのである．\nこれにより \\(\\mathbb{F}\\) 上でも非可逆な動きをするサンプラーが，連続変数の空間と離散変数の空間 \\(\\mathbb{F}\\) の合併上で構成できる．\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html",
    "href": "posts/2024/Probability/likelihood.html",
    "title": "Likelihood of Hierarchical Models",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#problem",
    "href": "posts/2024/Probability/likelihood.html#problem",
    "title": "Likelihood of Hierarchical Models",
    "section": "1 Problem",
    "text": "1 Problem\n\n1.1 Starting Point\nAs a starting point, consider a simple univariate linear regression model with 3 parameters, i.e., \\(\\alpha,\\beta,\\sigma^2\\): \\[\n\\operatorname{E}[Y|X]=\\alpha+\\beta X+\\epsilon,\\qquad\\epsilon\\sim N(0,\\sigma^2).\n\\tag{1}\\] Since (1) is just another expression for \\(Y|X\\sim N(\\alpha+\\beta X,\\sigma^2)\\), the likelihood function is given by \\[\np(y|\\alpha,\\beta,\\sigma)=\\phi(y|\\alpha+\\beta x,\\sigma^2),\n\\] given data \\(x\\in\\mathbb{R}\\), where \\(\\phi(-|\\mu,\\sigma^2)\\) represents the Gaussian density for \\(N(\\mu,\\sigma^2)\\).\n\n\n1.2 Hierarchical Model\nTo make our model (1) more realistic, let’s say, we would like to model variation in the intercept \\(\\alpha\\) by imposing another regression structure (you might call it super-population structure): \\[\n\\alpha=\\mu_\\alpha+\\epsilon_\\alpha,\\qquad\\epsilon_\\alpha\\sim N(0,\\sigma_\\alpha^2).\n\\tag{2}\\]\nNow, what does the likelihood function \\(p(y|\\mu_\\alpha,\\sigma_\\alpha,\\beta,\\sigma)\\) of this model look like?\nThe answer is normal likelihood.\n\n\n1.3 Likelihood Calculation\nFirst, rewrite (1) and (2) as \\[\nY|X,\\alpha\\sim N(\\alpha+\\beta X,\\sigma^2),\n\\] \\[\n\\alpha|\\mu_\\alpha,\\sigma_\\alpha\\sim N(\\mu_\\alpha,\\sigma_\\alpha^2).\n\\]\nUsing the formula (note this is not always true, see Section 2.4) \\[\np(y|\\mu_\\alpha,\\sigma_\\alpha,\\beta,\\sigma)=\\int_\\mathbb{R}p(y|\\alpha,\\beta,\\sigma)p(\\alpha|\\mu_\\alpha,\\sigma_\\alpha)\\,d\\alpha,\n\\] we get a normal density \\[\np(y|\\mu_\\alpha,\\sigma_\\alpha,\\beta,\\sigma)=\\phi(y|\\widetilde{\\mu},\\widetilde{\\sigma}^2),\n\\] where \\[\n\\widetilde{\\mu}(\\beta,\\mu_\\alpha):=\\beta x+\\mu_\\alpha,\n\\] \\[\n\\widetilde{\\sigma}^2(\\sigma,\\sigma_\\alpha):=\\sigma^2+\\sigma_\\alpha^2.\n\\]\nThis also follows from the reproducing property of normal distribution: \\[\nN(\\beta X,\\sigma^2)*N(\\mu_\\alpha,\\sigma_\\alpha^2)=N(\\beta X+\\mu_\\alpha,\\sigma^2+\\sigma_\\alpha^2).\n\\]"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#real-world-example-ideal-point-model",
    "href": "posts/2024/Probability/likelihood.html#real-world-example-ideal-point-model",
    "title": "Likelihood of Hierarchical Models",
    "section": "3 Real-world Example: Ideal Point Model",
    "text": "3 Real-world Example: Ideal Point Model\n\n3.1 Specification\nLet’s consider the following hierarchical model with 6 parameters, i.e., \\(\\alpha,\\beta,\\gamma,\\delta,\\sigma^2\\) and \\(X\\): \\[\nY\\sim\\operatorname{Bernoulli}(\\mu),\n\\] \\[\n\\operatorname{logit}(\\mu)=\\alpha+\\beta X,\n\\] \\[\nX=\\gamma+\\delta Z+\\epsilon,\\qquad\\epsilon\\sim N(0,\\sigma^2).\n\\]\nWe see the hierarchical structure \\[\nY|X\\sim\\operatorname{Bernoulli}(\\operatorname{logit}^{-1}(\\alpha+\\beta X)),\n\\] \\[\nX|Z\\sim N(\\gamma+\\delta Z,\\sigma^2),\n\\] just as in Section 1.3.\nThis time, however, we cannot detour the calculation by the tower property, while in Section 1.3 we could sanity check the result by the reproducing property of normal distribution.\n\n\n3.2 Likelihood Calculation\nThrough the core Equation 3,\n\\[\\begin{align*}\n  p(y|\\alpha,\\beta,\\gamma,\\delta,\\sigma)&=\\int_\\mathbb{R}p(y|\\alpha,\\beta,x)p(x|\\gamma,\\delta,\\sigma)\\,dx\\\\\n  &=1_{\\left\\{1\\right\\}}(y)\\int_\\mathbb{R}\\operatorname{logit}^{-1}(\\alpha+\\beta x)\\phi(x|\\gamma+\\delta z,\\sigma)\\,dx\\\\\n  &\\qquad+1_{\\left\\{0\\right\\}}(y)\\int_\\mathbb{R}\\biggr(1-\\operatorname{logit}^{-1}(\\alpha+\\beta x)\\biggl)\\phi(x|\\gamma+\\delta z,\\sigma)\\,dx\\\\\n  &=\\qquad\\cdots\\cdots\n\\end{align*}\\]\nWe won’t proceed any more, observing the integral is not always tractable.\nIf the likelihood involves an intractable integral, how can we proceed maximum likelihood / Bayesian estimation?\n\n\n3.3 Data Augmentation\nWe can still find the mode of the likelihood function by the EM algorithm (Dempster et al., 1977).\nThe case is similar in the Bayesian approach, where we enlarge the parameter space by treating \\(x\\) as a parameter (or a latent variable) and sample from \\(x\\) as well.\nThis is called data augmentation approach, initiated in the EM algorithm, later applied to Bayesian computations in (Tanner and Wong, 1987).\nAlthough \\(p(y|\\alpha,\\beta,\\gamma,\\delta,\\sigma)\\) might involve possibly intractable integrals over \\(x\\), \\(p(y|\\textcolor{red}{x},\\alpha,\\beta,\\gamma,\\delta,\\sigma)\\) does not, since it holds that \\[\np(y|x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)=p(y|x,\\alpha,\\beta)\n\\] given that \\(y\\) and \\(\\gamma,\\delta,\\sigma\\) are conditionally independent given \\(x\\).\nTherefore, the posterior distribution of \\((x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)\\) is given by\n\\[\np(x,\\alpha,\\beta,\\gamma,\\delta,\\sigma|y)\\,\\propto\\,p(y|x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)p(x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)\n\\] \\[\n=p(y|x,\\alpha,\\beta)p(x|\\gamma,\\delta,\\sigma)p(\\alpha,\\beta,\\gamma,\\delta,\\sigma).\n\\tag{5}\\]\nBasically, higher level parameters \\(\\gamma,\\delta,\\sigma\\) are treated as a part of the machinery that systematically defines a prior for the lower level parameter \\(x\\). In fact, this was the motivation when the hierarchical structure is first introduced in (Lindley and Smith, 1972).\n\n\n3.4 Bayesian Computation\nUsing the decomposition (5), we can readily run MCMC algorithms to sample from the posterior distribution.\nIntroducing another latent variable, we may also run the Gibbs sampler based on Polya-Gamma decomposition as in (Polson et al., 2013).\nAlternatively, we can run the HMC (Hamiltonian Monte Carlo) algorithm based on the gradient of the log posterior density.\nIn the following articles, we prepared you with the codes for gradient-based Bayesian estimation, including HMCs via Stan and Zig-Zag via PDMPFlux. Visit the following articles (although they are in Japanese) for more details:\n\n\n\nListing 1"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#background-in-probability",
    "href": "posts/2024/Probability/likelihood.html#background-in-probability",
    "title": "Likelihood of Hierarchical Models",
    "section": "2 Background in Probability",
    "text": "2 Background in Probability\n\n2.1 Core Proposition\nAbstracting away by setting \\[\n\\theta:=(\\beta,\\sigma),\\qquad\\varphi:=(\\mu_\\alpha,\\sigma_\\alpha),\n\\] the core identity was the following: \\[\np(y|\\theta,\\varphi)=\\int p(y|\\theta,\\alpha)p(\\alpha|\\varphi)\\,d\\alpha.\n\\tag{3}\\]\nHaving such a good notation, this formula might look obvious for stats people, but how actually do we prove it?\n\n\n2.2 Conditional Probability\nEssentially, Equation 3 comes from the tower property of conditional expectation:\n\n\n\n\n\n\nTower Property\n\n\n\nFor sigma algebras \\(\\mathcal{G}_1,\\mathcal{G}_2\\), \\[\n\\mathcal{G}_1\\subset\\mathcal{G}_2\\quad\\Rightarrow\\quad\\operatorname{E}[X|\\mathcal{G}_1]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}_2]|\\mathcal{G}_1]\\quad\\operatorname{P}\\text{-a.s.}\n\\] for any random variable \\(X\\) on \\((\\Omega,\\mathcal{F},\\operatorname{P})\\), \\(\\mathcal{G}_1,\\mathcal{G}_2\\subset\\mathcal{F}\\).\n\n\nLet’s see how we apply this tower property to our problem:\n\n\n\n\n\n\n(Th’m 8.15 Kallenberg, 2021, p. 174)\n\n\n\nFor any \\(A\\in\\mathcal{F}\\), \\[\n\\operatorname{P}[A|X]=\\operatorname{E}\\biggl[\\operatorname{P}[A|X,Y]\\,\\bigg|\\,X\\biggr]\\quad\\operatorname{P}\\text{-a.s.}\n\\tag{4}\\]\n\n\nConcerning the definition of conditional probability, we first have the definition of conditional expectation, through which we define conditional probability as1 \\[\n\\operatorname{P}[A|X]:=\\operatorname{E}[1_A|X].\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\operatorname{E}[1_A|X]=\\operatorname{E}\\biggl[\\operatorname{E}[1_A|X,Y]\\,\\bigg|\\,X\\biggr]\\quad\\operatorname{P}\\text{-a.s.}\n\\] holds from tower property, noting that \\[\n\\sigma(X)\\subset\\sigma(X,Y).\n\\]\n\n\n\n\n\n2.3 Conditional Density\nThe last piece we need is the definition of conditional density.\n\n\n\n\n\n\nDef (Conditional Density)\n\n\n\nFor absolutely continuous random variables \\(X,Y\\) on \\(\\mathbb{R}^d\\), we define the conditional density of \\(Y\\) given \\(X\\) as \\[\np(y|x):=\\frac{p(x,y)}{p(x)}1_{\\left\\{p(x)&gt;0\\right\\}},\n\\] where \\(p(x,y)\\) is the joint density of \\((X,Y)\\) and \\(p(x)\\) is the marginal density of \\(X\\).\n\n\n\n\n\n\n\n\nCharacterization of Conditional Density\n\n\n\n\\[\n\\operatorname{E}[Y|X=x]=\\int_{\\mathbb{R}^d}y\\,p(y|x)\\,dy\\quad\\operatorname{P}^X\\text{-a.s.}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince conditional expectation is unique up to \\(\\operatorname{P}\\)-null sets, we’ll check the RHS (right-hand side) satisfies the conditions of conditional expectation.\n\nBy Fubini’s theorem, the RHS is a measurable function of \\(x\\) and is \\(\\operatorname{P}^X\\)-integrable.\nFor all \\(\\sigma(X)\\)-measurable set \\(A\\in\\sigma(X)\\),\n\\[\n\\int_A\\int_{\\mathbb{R}^d}y\\,p(y|x)\\,dy\\,p(x)\\,dx=\\int_{A\\times\\mathbb{R}^d}y\\,p(x,y)\\,dxdy=\\operatorname{E}[Y1_A].\n\\]\n\n\n\n\nPlugging in this characterization into (4), we get \\[\n\\operatorname{P}[A|X=x]=\\int_{\\mathbb{R}^d}\\operatorname{P}[A|X=x,Y=y]\\,p(y|x)\\,dy.\n\\] Denoting the density of \\(\\operatorname{P}[A|X=x]\\) as \\(p(a|x)\\), we have \\[\np(a|x)=\\int_{\\mathbb{R}^d}p(a|x,y)\\,p(y|x)\\,dy.\n\\]\n\n\n2.4 Last Piece\nReterning to the notation in (3), we have \\[\np(y|\\theta,\\varphi)=\\int p(y|\\theta,\\varphi,\\alpha)p(\\alpha|\\varphi)\\,d\\alpha.\n\\]\nThis is slightly different from (3) in that there is \\(p(y|\\theta,\\varphi,\\alpha)\\) instead of \\(p(y|\\theta,\\alpha)\\).\nSo the last piece we need is the modeling assumption in the regression setting of (1), which is conditional independence between \\(Y\\) and the hyperparameters \\((\\mu_\\alpha,\\sigma_\\alpha)\\) given \\(\\alpha\\): \\[\nY\\perp\\!\\!\\!\\perp\\varphi\\mid\\alpha.\n\\]\nUnder this assumption, we have2 \\[\np(y|\\theta,\\alpha,\\varphi)=p(y|\\theta,\\alpha).\n\\]\nIn the regression setting of (1), \\[\np(y|\\beta,\\sigma,\\alpha,\\mu_\\alpha,\\sigma_\\alpha)=p(y|\\beta,\\sigma,\\alpha).\n\\]\n\n\n\n\n\n\nWe implicitly assumed \\[\np(y|\\beta,\\sigma,\\alpha,\\mu_\\alpha,\\sigma_\\alpha)=p(y|\\beta,\\sigma,\\alpha),\n\\] since it was reasonable to assume the conditional independence between \\(Y\\) and the hyperparameters \\((\\mu_\\alpha,\\sigma_\\alpha)\\) given \\(\\alpha\\): \\[\nY\\perp\\!\\!\\!\\perp(\\mu_\\alpha,\\sigma_\\alpha)\\mid\\alpha.\n\\]"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#footnotes",
    "href": "posts/2024/Probability/likelihood.html#footnotes",
    "title": "Likelihood of Hierarchical Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor this definition of conditional probability, see (Kallenberg, 2021, p. 167), (Dudley, 2002, p. 347). If we are to define conditional probability first, we need the concept of regular conditional probability, which is not discussed here.↩︎\nsee, for example, (Th’m 8.9 Kallenberg, 2021, p. 170)↩︎"
  },
  {
    "objectID": "English.html",
    "href": "English.html",
    "title": "Entries in English",
    "section": "",
    "text": "Notations | Categories | All Posts\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here. \n\n\n\n\n\nOct 29, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\nFeb 25, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\nThe Machine Learning Summer School 2024 (despite its name, it was held in spring due to typhoon considerations) offered me my first opportunity to present in an academic setting.\n\n\n\n\n\nFeb 25, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\nJan 5, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\nDec 1, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html",
    "href": "posts/2023/Probability/条件付き期待値の問題.html",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "",
    "text": "条件付き期待値を，測度論から厳密に定義する際，ポイントは次の4点である．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\nポイント\n\n\n\n\n条件付き期待値は \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に対して \\(\\operatorname{E}[X|\\mathcal{G}]\\) の形で（\\(\\Omega\\) 上殆ど至る所）定義される確率変数である（ 節 1.1 ）．\n\\(\\operatorname{E}[X|Y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)]\\) の略記である（ 節 1.2 ）．\n\\(\\operatorname{E}[X|Y=y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)](Y^{-1}(y))\\) のことである（ 節 1.2 ）．\n\\(X\\in L^2(\\Omega)\\) でもあるとき，\\(\\operatorname{E}[X|\\mathcal{F}]\\) は \\(X\\) に \\(L^2(\\Omega)\\)-距離で最も近いような \\(\\mathcal{F}\\)-可測確率変数である（ 節 1.3 ）．\n条件付き確率は \\(\\operatorname{P}[Y\\in B|X]:=\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}|X]\\) と定義する（ 節 1.4 ）．\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き期待値）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間とし，\\(\\mathcal{G}\\) を \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数とする．可積分確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\) について， 次の2条件を満たす，\\(\\operatorname{P}\\)-零集合を除いて一意な確率変数を条件付き期待値といい，\\(\\operatorname{E}[X|\\mathcal{G}]\\) で表す．\n\n\\(\\mathcal{G}\\)-可測でもある \\(\\operatorname{P}\\)-可積分確率変数である．\n任意の \\(\\mathcal{G}\\)-可測集合 \\(B\\in\\mathcal{G}\\) 上では \\(X\\) と期待値が同じ確率変数になる：\\[\\operatorname{E}[X1_B]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}]1_B]\\]\n\n\n\n\\(\\operatorname{E}[X|\\mathcal{G}]\\) は \\(L^1(\\Omega,\\mathcal{G},\\operatorname{P})\\) の元であり，数学的対象としては「関数の同値類」である．関数としてはある零集合の上では定まらない．その任意の代表元も \\(\\operatorname{E}[X|\\mathcal{G}]\\) と表すことが多く，1 その場合は，多くの等式には a.s. (= almost surely) がついてまわることになる．\nもちろん，\\(L^1(\\operatorname{P})\\) 上の順序関係 \\(\\le\\) を \\[\nX\\le Y:\\Leftrightarrow X\\le Y\\;\\;\\text{a.s.}\n\\] と定義し，a.s. を省略して書いてもよい．\n\n\n\n\n\n\n証明\n\n\n\n\n\n定義の2条件のみから，\\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\operatorname{P}\\)-零集合を除いて一意に定まること（とその存在）を示す．\n\\[Q(B):=\\operatorname{E}[X,B]=\\operatorname{E}[1_BX]\\;(B\\in\\mathcal{G})\\] とおくことで，\\(Q\\) は可測空間 \\((\\Omega,\\mathcal{G})\\) 上の確率測度を定める． いま，\\(\\operatorname{P}|_\\mathcal{G}\\) に関して \\(Q\\) は絶対連続になっている：\\[\\forall_{B\\in\\mathcal{G}}\\;P(B)=0\\Rightarrow Q(B)=0.\\] これより，Radon-Nikodymの定理から， ある \\(\\mathcal{G}\\)-可測で \\(\\operatorname{P}\\)-可積分な可測関数 \\(Y:\\Omega\\to\\mathbb{R}\\) が，\\(\\operatorname{P}\\)-零集合上での違いを除いて一意的に存在して，\\[\\forall_{B\\in\\mathcal{G}}\\;Q(B)=\\int_BY(\\omega)P(d\\omega)\\] が成り立つ． よって，条件付き期待値 \\(Y\\) は確かに存在して（同値類 \\(L^1(\\operatorname{P})\\) の元としては）一意的で，(1),(2)が成り立つ．\n\n\n\n(Dudley, 2002, pp. 10.1節 p.336), (吉田朋広, 2006, p. 43) がおすすめな参照先．(舟木直久, 2004, p. 88) が入門しやすい．\\(X\\in L^2(\\Omega)\\) でいい場合は，より「射影」としてわかりやすい特徴付けがある（ 節 1.3 ）．これのおすすめは (Jacod and Protter, 2004, pp. 第23節 p.200), (Kallenberg, 2021, p. 164)．\n\n\n\n\n\n\n\n\n\n定義（確率変数を与えた下での条件付き期待値）\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間とする．確率変数 \\(X\\in\\mathcal{L}(\\Omega;E)\\) による \\(Y\\in\\mathcal{L}^1(\\Omega)\\) の条件付き期待値は，次を満たす可測関数 \\(\\operatorname{E}[Y|X=-]:E\\to\\mathbb{R}\\) のことをいう： \\[\n\\begin{align*}\n\\forall_{B\\in\\mathcal{E}}\\quad&\\int_{X^{-1}(B)}Y(\\omega)P(d\\omega)\\\\\n&\\quad=\\int_B\\operatorname{E}[Y|X=x]P^X(dx).\n\\end{align*}\n\\]\n\n\nすると，\\(X\\) が \\(\\Omega\\) 上に引き戻す \\(\\sigma\\)-代数 \\[\n\\sigma(X):=\\left\\{A\\subset\\Omega\\mid\\exists_{B\\in\\mathcal{E}}\\; X^{-1}(B)=A\\right\\}\n\\] を与えた下での条件付き期待値 \\(\\operatorname{E}[Y|\\sigma(X)]\\) と，次のように関係する．2 \\(\\operatorname{E}[Y|\\sigma(X)]\\) は定義 節 1.1 1から \\(\\sigma[X]\\)-可測であるが，可測性の特徴付け（後述）から，これはあるBorel可測関数 \\(f\\) について，\\[\\operatorname{E}[Y|X]=f(X)\\;\\;\\text{a.s.}\\] と表せる．この \\(f:\\mathcal{X}\\to\\mathbb{R}\\) が，\\(X\\) を与えた下での \\(Y\\) の条件付き期待値 \\(\\operatorname{E}[Y|X=-]\\) である．\nこの記法 \\(\\operatorname{E}[Y|X=x]\\) とは何かというと，\\(X\\) の値域 \\(\\mathcal{X}\\) 上の関数として，新たに \\[\\operatorname{E}[Y|X=x]:=f(x)\\;\\;\\text{a.s.}\\] と書くことにするのである．3 すると， \\[\n\\operatorname{E}[Y|X=x]|_{x=X(\\omega)}=\\operatorname{E}[Y|X](\\omega)\\;\\;\\text{a.s.}\n\\] も満たす．つまり，次の図式が可換である：\n\n\n\nCommutative Diagram for Conditional Expectations\n\n\n\n\n\n\n\n\n命題4 (Doob, 1953)\n\n\n\n\\(S\\) を位相空間，\\(X\\in \\mathcal{L}(\\Omega;S),Y\\in \\mathcal{L}(\\Omega)\\) を確率変数とする．次は同値：\n\n\\(Y\\)は \\(\\sigma(X)\\)-可測．\nあるBorel可測関数 \\(f:S\\to\\mathbb{R}\\) が存在して，\\(Y=f(X)\\) を満たす．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n2 \\(\\Rightarrow\\) 1 はすぐに従う．任意の \\(B\\in\\mathcal{B}(\\mathbb{R})\\) について，\\(f^{-1}(B)\\in\\mathcal{B}(S)\\) であるから， \\[Y^{-1}(B)=X^{-1}(f^{-1}(B))\\in\\sigma(X).\\] あとは 1 \\(\\Rightarrow\\) 2 を示せば良い．3段階で示す．\n\nまず \\(Y\\) が単関数 \\[Y=\\sum_{i=1}^nc_i1_{A_i},\\qquad c_i\\ne c_j\\;(i\\ne j).\\] の場合について示す．仮定より \\(A_i\\in\\sigma(X)\\) であるから，ある \\(B_i\\in\\mathcal{B}(S)\\) が存在して \\(A_i=X^{-1}(B_i)\\)．よって， \\[f(x):=\\sum_{i=1}^nc_i1_{B_i}(x).\\] と定めると \\(Y=f(X)\\)．\n次に \\(Y\\ge0\\;\\;\\text{a.s.}\\) の場合を考えると，正な単関数の単調増加列 \\(\\{Y_n\\}\\) で \\(Y\\) に収束するものが取れる．各 \\(Y_n\\) について，\\(f_n\\in\\mathcal{L}(S)\\) が存在して \\(Y_n=f_n(X)\\) が成り立つ．このとき，\\(f:=\\limsup_{n\\to\\infty}f_n\\) と定めれば， \\[\\begin{align*}\nY&=\\limsup_{n\\to\\infty}Y_n\\\\\n&=(\\limsup_{n\\to\\infty}f_n)(X)=f(X).\n\\end{align*}\\]\n一般の場合は \\(Y=Y^+-Y^-\\) の分解から従う．\n\n\n\n\n(Dudley, 2002, pp. 定理4.2.8 p.128) は \\(S=\\mathbb{R}\\) の場合，(Landkov, 1972) は \\(S=\\mathbb{R}^m\\) の場合, (Kallenberg, 2021, pp. 補題1.14 p.18) に一般の標準Borel空間の場合の証明がある．nLab も極めて参考になる．\n\n\n\n\\(L^2(\\Omega)\\subset L^1(\\Omega)\\) 上に議論を制限してみると，実は \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に関する条件付き期待値は，部分空間 \\[\nL^2_\\mathcal{G}(\\Omega):=\\left\\{X\\in L^2(\\Omega)\\:\\middle|\\:X\\,\\text{は}\\,\\mathcal{G}\\,\\text{-可測}\\right\\}\n\\] への射影になっている．\n\n\n\n\n\n\n定理（条件付き期待値の特徴付け）\n\n\n\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\subset\\mathcal{F}\\) と \\(X\\in\\mathcal{L}^2(\\Omega)\\) を考える． 任意の \\(\\widehat{X}_\\mathcal{G}\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)\\) について，次は同値：\n\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の \\(L^2_\\mathcal{G}(\\Omega)\\) への射影である： \\[\n\\begin{align*}\n&\\|X-\\widehat{X}_\\mathcal{G}\\|_{L^2(\\Omega)}\\\\\n&=\\inf_{X'\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)}\\|X-X'\\|_{L^2(\\Omega)}.\n\\end{align*}\n\\]\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の条件付き期待値である：\\[\\forall_{Z\\in L^2_\\mathcal{G}(\\Omega)}\\;\\operatorname{E}[ZX]=\\operatorname{E}[Z\\widehat{X}_\\mathcal{G}].\\]\n\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間，\\(\\mathcal{G}\\subset\\mathcal{F}\\) を部分 \\(\\sigma\\)-代数とする．\\(\\mathcal{G}\\) の定める条件付き確率を， \\[\n\\operatorname{P}[B|\\mathcal{G}](\\omega):=\\operatorname{E}[1_B|\\mathcal{G}](\\omega)\\;(B\\in\\mathcal{F})\n\\] で定める．\n\n\nしかしこの定義には問題がある．条件付き期待値 \\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\Omega\\) 上 \\(\\operatorname{P}\\text{-a.e.}\\) でしか定まらない（場合がある）から，\\(\\operatorname{P}\\) も一般には可算加法性をa.s.にしか満たさない： \\[\n\\operatorname{P}\\left[\\bigcap_{n\\in\\mathbb{N}}A_n\\,\\middle|\\,\\mathcal{G}\\right]=\\sum_{n\\in\\mathbb{N}}\\operatorname{P}[A_n]\\;\\;\\text{a.s.}\n\\] この式自体は後述の単調収束定理（ 節 2.3 ）から示せる．\nだが，\\(\\mathcal{G}\\) がある完備可分距離空間に値を取る確率変数 \\(Y\\) について \\(\\mathcal{G}=\\sigma(Y)\\) である場合など，殆どの場合で，うまく \\(\\operatorname{P}\\) を取ることが出来る．5 このように，a.s. 抜きで正式に確率測度として定まる場合，その確率核 \\(\\operatorname{P}:E\\times\\mathcal{G}\\to[0,1]\\) を，正則条件付き確率と呼び分ける．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#条件付き期待値の定義",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#条件付き期待値の定義",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "",
    "text": "条件付き期待値を，測度論から厳密に定義する際，ポイントは次の4点である．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\nポイント\n\n\n\n\n条件付き期待値は \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に対して \\(\\operatorname{E}[X|\\mathcal{G}]\\) の形で（\\(\\Omega\\) 上殆ど至る所）定義される確率変数である（ 節 1.1 ）．\n\\(\\operatorname{E}[X|Y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)]\\) の略記である（ 節 1.2 ）．\n\\(\\operatorname{E}[X|Y=y]\\) というのは，\\(\\operatorname{E}[X|\\sigma(Y)](Y^{-1}(y))\\) のことである（ 節 1.2 ）．\n\\(X\\in L^2(\\Omega)\\) でもあるとき，\\(\\operatorname{E}[X|\\mathcal{F}]\\) は \\(X\\) に \\(L^2(\\Omega)\\)-距離で最も近いような \\(\\mathcal{F}\\)-可測確率変数である（ 節 1.3 ）．\n条件付き確率は \\(\\operatorname{P}[Y\\in B|X]:=\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}|X]\\) と定義する（ 節 1.4 ）．\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き期待値）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間とし，\\(\\mathcal{G}\\) を \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数とする．可積分確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\) について， 次の2条件を満たす，\\(\\operatorname{P}\\)-零集合を除いて一意な確率変数を条件付き期待値といい，\\(\\operatorname{E}[X|\\mathcal{G}]\\) で表す．\n\n\\(\\mathcal{G}\\)-可測でもある \\(\\operatorname{P}\\)-可積分確率変数である．\n任意の \\(\\mathcal{G}\\)-可測集合 \\(B\\in\\mathcal{G}\\) 上では \\(X\\) と期待値が同じ確率変数になる：\\[\\operatorname{E}[X1_B]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}]1_B]\\]\n\n\n\n\\(\\operatorname{E}[X|\\mathcal{G}]\\) は \\(L^1(\\Omega,\\mathcal{G},\\operatorname{P})\\) の元であり，数学的対象としては「関数の同値類」である．関数としてはある零集合の上では定まらない．その任意の代表元も \\(\\operatorname{E}[X|\\mathcal{G}]\\) と表すことが多く，1 その場合は，多くの等式には a.s. (= almost surely) がついてまわることになる．\nもちろん，\\(L^1(\\operatorname{P})\\) 上の順序関係 \\(\\le\\) を \\[\nX\\le Y:\\Leftrightarrow X\\le Y\\;\\;\\text{a.s.}\n\\] と定義し，a.s. を省略して書いてもよい．\n\n\n\n\n\n\n証明\n\n\n\n\n\n定義の2条件のみから，\\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\operatorname{P}\\)-零集合を除いて一意に定まること（とその存在）を示す．\n\\[Q(B):=\\operatorname{E}[X,B]=\\operatorname{E}[1_BX]\\;(B\\in\\mathcal{G})\\] とおくことで，\\(Q\\) は可測空間 \\((\\Omega,\\mathcal{G})\\) 上の確率測度を定める． いま，\\(\\operatorname{P}|_\\mathcal{G}\\) に関して \\(Q\\) は絶対連続になっている：\\[\\forall_{B\\in\\mathcal{G}}\\;P(B)=0\\Rightarrow Q(B)=0.\\] これより，Radon-Nikodymの定理から， ある \\(\\mathcal{G}\\)-可測で \\(\\operatorname{P}\\)-可積分な可測関数 \\(Y:\\Omega\\to\\mathbb{R}\\) が，\\(\\operatorname{P}\\)-零集合上での違いを除いて一意的に存在して，\\[\\forall_{B\\in\\mathcal{G}}\\;Q(B)=\\int_BY(\\omega)P(d\\omega)\\] が成り立つ． よって，条件付き期待値 \\(Y\\) は確かに存在して（同値類 \\(L^1(\\operatorname{P})\\) の元としては）一意的で，(1),(2)が成り立つ．\n\n\n\n(Dudley, 2002, pp. 10.1節 p.336), (吉田朋広, 2006, p. 43) がおすすめな参照先．(舟木直久, 2004, p. 88) が入門しやすい．\\(X\\in L^2(\\Omega)\\) でいい場合は，より「射影」としてわかりやすい特徴付けがある（ 節 1.3 ）．これのおすすめは (Jacod and Protter, 2004, pp. 第23節 p.200), (Kallenberg, 2021, p. 164)．\n\n\n\n\n\n\n\n\n\n定義（確率変数を与えた下での条件付き期待値）\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間とする．確率変数 \\(X\\in\\mathcal{L}(\\Omega;E)\\) による \\(Y\\in\\mathcal{L}^1(\\Omega)\\) の条件付き期待値は，次を満たす可測関数 \\(\\operatorname{E}[Y|X=-]:E\\to\\mathbb{R}\\) のことをいう： \\[\n\\begin{align*}\n\\forall_{B\\in\\mathcal{E}}\\quad&\\int_{X^{-1}(B)}Y(\\omega)P(d\\omega)\\\\\n&\\quad=\\int_B\\operatorname{E}[Y|X=x]P^X(dx).\n\\end{align*}\n\\]\n\n\nすると，\\(X\\) が \\(\\Omega\\) 上に引き戻す \\(\\sigma\\)-代数 \\[\n\\sigma(X):=\\left\\{A\\subset\\Omega\\mid\\exists_{B\\in\\mathcal{E}}\\; X^{-1}(B)=A\\right\\}\n\\] を与えた下での条件付き期待値 \\(\\operatorname{E}[Y|\\sigma(X)]\\) と，次のように関係する．2 \\(\\operatorname{E}[Y|\\sigma(X)]\\) は定義 節 1.1 1から \\(\\sigma[X]\\)-可測であるが，可測性の特徴付け（後述）から，これはあるBorel可測関数 \\(f\\) について，\\[\\operatorname{E}[Y|X]=f(X)\\;\\;\\text{a.s.}\\] と表せる．この \\(f:\\mathcal{X}\\to\\mathbb{R}\\) が，\\(X\\) を与えた下での \\(Y\\) の条件付き期待値 \\(\\operatorname{E}[Y|X=-]\\) である．\nこの記法 \\(\\operatorname{E}[Y|X=x]\\) とは何かというと，\\(X\\) の値域 \\(\\mathcal{X}\\) 上の関数として，新たに \\[\\operatorname{E}[Y|X=x]:=f(x)\\;\\;\\text{a.s.}\\] と書くことにするのである．3 すると， \\[\n\\operatorname{E}[Y|X=x]|_{x=X(\\omega)}=\\operatorname{E}[Y|X](\\omega)\\;\\;\\text{a.s.}\n\\] も満たす．つまり，次の図式が可換である：\n\n\n\nCommutative Diagram for Conditional Expectations\n\n\n\n\n\n\n\n\n命題4 (Doob, 1953)\n\n\n\n\\(S\\) を位相空間，\\(X\\in \\mathcal{L}(\\Omega;S),Y\\in \\mathcal{L}(\\Omega)\\) を確率変数とする．次は同値：\n\n\\(Y\\)は \\(\\sigma(X)\\)-可測．\nあるBorel可測関数 \\(f:S\\to\\mathbb{R}\\) が存在して，\\(Y=f(X)\\) を満たす．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n2 \\(\\Rightarrow\\) 1 はすぐに従う．任意の \\(B\\in\\mathcal{B}(\\mathbb{R})\\) について，\\(f^{-1}(B)\\in\\mathcal{B}(S)\\) であるから， \\[Y^{-1}(B)=X^{-1}(f^{-1}(B))\\in\\sigma(X).\\] あとは 1 \\(\\Rightarrow\\) 2 を示せば良い．3段階で示す．\n\nまず \\(Y\\) が単関数 \\[Y=\\sum_{i=1}^nc_i1_{A_i},\\qquad c_i\\ne c_j\\;(i\\ne j).\\] の場合について示す．仮定より \\(A_i\\in\\sigma(X)\\) であるから，ある \\(B_i\\in\\mathcal{B}(S)\\) が存在して \\(A_i=X^{-1}(B_i)\\)．よって， \\[f(x):=\\sum_{i=1}^nc_i1_{B_i}(x).\\] と定めると \\(Y=f(X)\\)．\n次に \\(Y\\ge0\\;\\;\\text{a.s.}\\) の場合を考えると，正な単関数の単調増加列 \\(\\{Y_n\\}\\) で \\(Y\\) に収束するものが取れる．各 \\(Y_n\\) について，\\(f_n\\in\\mathcal{L}(S)\\) が存在して \\(Y_n=f_n(X)\\) が成り立つ．このとき，\\(f:=\\limsup_{n\\to\\infty}f_n\\) と定めれば， \\[\\begin{align*}\nY&=\\limsup_{n\\to\\infty}Y_n\\\\\n&=(\\limsup_{n\\to\\infty}f_n)(X)=f(X).\n\\end{align*}\\]\n一般の場合は \\(Y=Y^+-Y^-\\) の分解から従う．\n\n\n\n\n(Dudley, 2002, pp. 定理4.2.8 p.128) は \\(S=\\mathbb{R}\\) の場合，(Landkov, 1972) は \\(S=\\mathbb{R}^m\\) の場合, (Kallenberg, 2021, pp. 補題1.14 p.18) に一般の標準Borel空間の場合の証明がある．nLab も極めて参考になる．\n\n\n\n\\(L^2(\\Omega)\\subset L^1(\\Omega)\\) 上に議論を制限してみると，実は \\(\\mathcal{F}\\) の部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) に関する条件付き期待値は，部分空間 \\[\nL^2_\\mathcal{G}(\\Omega):=\\left\\{X\\in L^2(\\Omega)\\:\\middle|\\:X\\,\\text{は}\\,\\mathcal{G}\\,\\text{-可測}\\right\\}\n\\] への射影になっている．\n\n\n\n\n\n\n定理（条件付き期待値の特徴付け）\n\n\n\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\subset\\mathcal{F}\\) と \\(X\\in\\mathcal{L}^2(\\Omega)\\) を考える． 任意の \\(\\widehat{X}_\\mathcal{G}\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)\\) について，次は同値：\n\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の \\(L^2_\\mathcal{G}(\\Omega)\\) への射影である： \\[\n\\begin{align*}\n&\\|X-\\widehat{X}_\\mathcal{G}\\|_{L^2(\\Omega)}\\\\\n&=\\inf_{X'\\in\\mathcal{L}^2_\\mathcal{G}(\\Omega)}\\|X-X'\\|_{L^2(\\Omega)}.\n\\end{align*}\n\\]\n\\(\\widetilde{X}_\\mathcal{G}\\) は \\(X\\) の条件付き期待値である：\\[\\forall_{Z\\in L^2_\\mathcal{G}(\\Omega)}\\;\\operatorname{E}[ZX]=\\operatorname{E}[Z\\widehat{X}_\\mathcal{G}].\\]\n\n\n\n\n\n\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を確率空間，\\(\\mathcal{G}\\subset\\mathcal{F}\\) を部分 \\(\\sigma\\)-代数とする．\\(\\mathcal{G}\\) の定める条件付き確率を， \\[\n\\operatorname{P}[B|\\mathcal{G}](\\omega):=\\operatorname{E}[1_B|\\mathcal{G}](\\omega)\\;(B\\in\\mathcal{F})\n\\] で定める．\n\n\nしかしこの定義には問題がある．条件付き期待値 \\(\\operatorname{E}[X|\\mathcal{G}]\\) が \\(\\Omega\\) 上 \\(\\operatorname{P}\\text{-a.e.}\\) でしか定まらない（場合がある）から，\\(\\operatorname{P}\\) も一般には可算加法性をa.s.にしか満たさない： \\[\n\\operatorname{P}\\left[\\bigcap_{n\\in\\mathbb{N}}A_n\\,\\middle|\\,\\mathcal{G}\\right]=\\sum_{n\\in\\mathbb{N}}\\operatorname{P}[A_n]\\;\\;\\text{a.s.}\n\\] この式自体は後述の単調収束定理（ 節 2.3 ）から示せる．\nだが，\\(\\mathcal{G}\\) がある完備可分距離空間に値を取る確率変数 \\(Y\\) について \\(\\mathcal{G}=\\sigma(Y)\\) である場合など，殆どの場合で，うまく \\(\\operatorname{P}\\) を取ることが出来る．5 このように，a.s. 抜きで正式に確率測度として定まる場合，その確率核 \\(\\operatorname{P}:E\\times\\mathcal{G}\\to[0,1]\\) を，正則条件付き確率と呼び分ける．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#性質",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#性質",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "2 性質",
    "text": "2 性質\n\n2.1 作用素としての性質\n\\(\\mathcal{G}\\)-可測な可積分関数のなす部分空間を \\(L_{\\mathcal{G}}^1(\\Omega)\\subset L^1(\\Omega)\\) で表す．\n\n\n\n\n\n\n命題（条件付き期待値はノルム減少的な正作用素）\n\n\n\n条件付き期待値 \\(\\operatorname{E}_{\\mathcal{G}}:L^1(\\Omega)\\to L_{\\mathcal{G}}^1(\\Omega)\\) はノルム減少的で正な線型汎作用素である．すなわち，\n\n線型性：任意の実数 \\(a,b\\in\\mathbb{R}\\) について， \\[\\begin{align*}\n\\operatorname{E}[aX+bY|\\mathcal{G}]&=a\\operatorname{E}[X|\\mathcal{G}]\\\\\n&\\qquad+b\\operatorname{E}[Y|\\mathcal{G}]\\;\\;\\text{a.s.}\n\\end{align*}\\]\n正性：\\(X\\le Y\\;\\;\\text{a.s.}\\) ならば， \\[\\operatorname{E}[X|\\mathcal{G}]\\le\\operatorname{E}[Y|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\nJensenの不等式：\\(\\varphi:\\mathbb{R}\\to\\mathbb{R}\\) を凸関数とする．\\(\\varphi(X)\\in L^1(\\Omega)\\) ならば，\\[\\varphi(\\operatorname{E}[X|\\mathcal{G}])\\le\\operatorname{E}[\\varphi(X)|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n三角不等式：\\[\\lvert\\operatorname{E}[X|\\mathcal{G}]\\rvert\\le\\operatorname{E}[\\lvert X\\rvert|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n\nいずれも \\(L_{\\mathcal{G}}^1(\\Omega)\\) 上の等式・不等式であり，殆ど確実ににしか成り立たないことに注意．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n1は結局積分の線型性から従います．2は次のように議論できます．\n任意の \\(X\\in L^1(\\Omega)_+\\) について \\(\\operatorname{E}[X|\\mathcal{G}]\\in L^1(\\Omega)\\) を示せば良い．\\(A_n:=\\left\\{X'\\le 1/n\\right\\}\\in\\mathcal{G}\\) について，条件付き期待値の定義から，任意の \\(n\\in\\mathbb{N}^+\\) について， \\[\n\\begin{align*}\n0\\le\\operatorname{E}[X,A_n]&=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}],A_n]\\\\\n&\\le\\frac{1}{n}\\operatorname{P}[A_n].\n\\end{align*}\n\\] より，\\(\\lim_{n\\to\\infty}\\operatorname{P}[A_n]=0\\) が必要．これより， \\[\\operatorname{P}[\\operatorname{E}[X|\\mathcal{G}]&lt;0]\\le\\operatorname{P}[\\cup_{n=1}^\\infty A_n]=0.\\] が解る．\n3は単関数の場合から地道に示します．4はその特別の場合で \\(\\varphi(x)=\\lvert x\\rvert\\) と取った場合に当たります．\n\n\n\n\n\n2.2 Tower Property\n\n\n\n\n\n\n命題（繰り返し期待値の法則）\n\n\n\n2つの \\(\\sigma\\)-代数が \\(\\mathcal{G}_1\\subset\\mathcal{G}_2\\) を満たすならば，\\(\\operatorname{E}_{\\mathcal{G}_1}=\\operatorname{E}_{\\mathcal{G}_1}\\circ\\operatorname{E}_{\\mathcal{G}_2}\\)．すなわち， \\[\\operatorname{E}[X|\\mathcal{G}_1]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}_2]|\\mathcal{G}_1]\\;\\;\\text{a.s.}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n右辺を \\(Z\\) とおく．任意の \\(A\\in\\mathcal{G}_1\\) について，\\(A\\in\\mathcal{G}_2\\) でもあるから， \\[\\begin{align*}\n\\operatorname{E}[Z1_A]&=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}_1]1_A]\\\\\n&=\\operatorname{E}[X1_A].\n\\end{align*}\\]\n\n\n\n\n\n2.3 単調収束定理\n\n\n\n\n\n\n命題（条件付き期待値に対する単調収束定理）\n\n\n\n可積分な実確率変数の列 \\(\\{X_n\\}\\cup\\{X\\}\\subset L^1(\\Omega)\\) について， \\[X_n\\nearrow X\\;\\;\\text{a.s.}\\] \\[\\Rightarrow\\quad\\operatorname{E}[X_n|\\mathcal{G}]\\nearrow\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n条件付き期待値の正性 節 2.1 より， \\[\\operatorname{E}[X_n|\\mathcal{G}]\\le\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.},\\qquad n\\in\\mathbb{N}.\\] よって，有界な単調列は収束するから，ある \\(Y\\in L^1(\\Omega)\\) を \\(E[X_n|\\mathcal{G}]\\nearrow Y\\;\\;\\text{a.s.}\\) を満たすように定めることが出来る．同時に，通常の期待値に関する単調収束定理から， \\[\n\\begin{align*}\n\\operatorname{E}[X1_A]&=\\lim_{n\\to\\infty}\\operatorname{E}[X_n1_A]\\\\&=\\operatorname{E}[Y1_A]\\;(A\\in\\mathcal{G})\n\\end{align*}\n\\] が必要であるから，条件付き期待値の一意性より，\\(Y=\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n2.4 可測関数の取り出し\n\n\n\n\n\n\n命題（可測関数の取り出し）\n\n\n\n\\(X,XY\\in\\mathcal{L}^1(\\Omega)\\) を可積分，\\(Y\\in\\mathcal{L}_\\mathcal{G}(\\Omega)\\) を \\(\\mathcal{G}\\)-可測実確率変数とする．このとき，\n\n\\(XY\\in\\mathcal{L}^1(\\Omega)\\)ならば，\\[\\operatorname{E}[XY|\\mathcal{G}]=Y\\operatorname{E}[X|\\mathcal{G}]\\;\\;\\text{a.s.}\\]\n特に，\\(\\operatorname{E}[Y|\\mathcal{G}]=Y\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n条件付き期待値の線型性から，\\(X,Y\\ge0\\) の場合について示せば良い．このとき，非負値単関数の収束列 \\(X_n\\nearrow X,Y_n\\nearrow Y\\) が取れる．\\(X_nY\\nearrow XY\\in L^1(\\Omega)\\) だから，単調収束定理 節 2.3 から \\[\\operatorname{E}[X_n|\\mathcal{G}]\\nearrow\\operatorname{E}[X|\\mathcal{G}]\\] \\[\n\\begin{align*}\n\\Rightarrow&\\quad Y\\operatorname{E}[X_n|\\mathcal{G}]\\nearrow Y\\operatorname{E}[X|\\mathcal{G}]\\\\\n\\quad\\land&\\quad\\operatorname{E}[X_nY|\\mathcal{G}]\\nearrow\\operatorname{E}[XY|\\mathcal{G}].\n\\end{align*}\\] よって，各 \\(n\\in\\mathbb{N}\\) について \\(Y\\operatorname{E}[X_n|\\mathcal{G}]=\\operatorname{E}[X_nY|\\mathcal{G}]\\) を示せば良い．単関数とは \\(X=1_C\\;(C\\in\\mathcal{G})\\) という形の関数の線型和だから，畢竟この形の関数について考えれば良いのである．任意の \\(B\\in\\mathcal{G}\\) について \\(C\\cap B\\in\\mathcal{G}\\) であるから， \\[\n\\begin{align*}\n\\int_B1_C\\operatorname{E}[Y|\\mathcal{G}]\\,d\\operatorname{P}&=\\int_{C\\cap B}\\operatorname{E}[Y|\\mathcal{G}]\\,d\\operatorname{P}\\\\\n&=\\int_{C\\cap B}Y\\,d\\operatorname{P}\\\\\n&=\\int_B1_CY\\,d\\operatorname{P}.\n\\end{align*}\n\\] 条件付き期待値の一意性より，\\(1_C\\operatorname{E}[Y|\\mathcal{G}]=\\operatorname{E}[1_CY|\\mathcal{G}]\\;\\;\\text{a.s.}\\) を得る．\n\n\n\n\n\n2.5 独立な場合\n\n\n\n\n\n\n命題（独立確率変数に対する性質）\n\n\n\n可積分実確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\)は \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) と独立とする．\n\n\\(\\operatorname{E}[X|\\mathcal{G}]=\\operatorname{E}[X]\\;\\;\\text{a.s.}\\)\n特に，\\(\\operatorname{E}[X|\\boldsymbol{2}]=\\operatorname{E}[X]\\;\\;\\text{a.s.}\\)．\n\nただし，\\(\\boldsymbol{2}=\\{\\emptyset,\\Omega\\}\\) とした．\n\n\n\n\n2.6 条件付き期待値のアトム上での値\n\n\n\n\n\n\n問題\n\n\n\n確率変数 \\(X,Y\\) とその値域の値 \\(y\\in\\mathcal{Y}\\) について， \\[\n\\operatorname{E}[X|Y=y]\\operatorname{P}[Y=y]=\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}]\n\\] はどう正当化されるか？\n\n\n\n\n\n\n\n\n説明\n\n\n\n\n\n\\(\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}]\\) の中身を \\(\\sigma(Y)\\) で条件付けてTower property（ 節 2.2 ）を使うと（定義 節 1.1 の条件2からと論じても良い），\\(1_{\\left\\{Y=y\\right\\}}\\) は \\(\\sigma(Y)\\)-可測だから，条件付き期待値の中身から出る（ 節 2.4 参照）．これによって正当化できる．式で表すと， \\[\n\\begin{align*}\n\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}]&=\\operatorname{E}[\\operatorname{E}[X1_{\\left\\{Y=y\\right\\}}|Y]]\\\\\n&=\\operatorname{E}[1_{\\left\\{Y=y\\right\\}}\\operatorname{E}[X|Y]]\\\\\n&=\\int_{\\mathcal{Y}}\\delta_y(y')\\operatorname{E}[X|Y=y']\\operatorname{P}(dy')\\\\\n&=\\operatorname{E}[X|Y=y]\\operatorname{P}[Y=y].\n\\end{align*}\n\\] ただし，\\(\\mathcal{Y}\\) 上の確率測度を \\(\\operatorname{P}\\) と置いた．\n\n\n\n条件付き確率の定義 節 1.4 から， \\[\n\\operatorname{P}[Y\\in B|X=x]:=\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}|X=x]\n\\] と議論できる．さらに \\(\\operatorname{P}[X=x]&gt;0\\) のとき， \\[\n\\begin{align*}\n    &=\\frac{\\operatorname{E}[1_{\\left\\{Y\\in B\\right\\}}1_{\\left\\{X=x\\right\\}}]}{\\operatorname{P}[X=x]}\\\\\n    &=\\frac{\\operatorname{P}[Y\\in B,X=x]}{\\operatorname{P}[X=x]}\n\\end{align*}\n\\] という見慣れた表示を得る．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#更なる条件付け",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#更なる条件付け",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "3 更なる条件付け",
    "text": "3 更なる条件付け\n\n3.1 条件付き独立性\n\n\n\n\n\n\n定義（条件付き独立性）\n\n\n\n\\(\\mathcal{C}\\subset\\mathcal{F}\\) の下で，\\(\\mathcal{G}_1,\\cdots,\\mathcal{G}_n\\) が \\(\\mathcal{C}\\)-条件付き独立 であるとは，任意の \\(A_k\\in\\mathcal{G}_k\\;(k\\in[n])\\) について \\[\n\\operatorname{P}\\left[\\bigcap_{k\\in[n]}A_k\\:\\middle|\\:\\mathcal{C}\\right]\\overset{\\text{a.s.}}{=}\\prod_{k\\in[n]}\\operatorname{P}[A_k|\\mathcal{C}]\n\\] を満たすことをいう．\n\n\n\\(\\mathcal{C}=\\boldsymbol{2}\\) であるとき，通常の独立性に一致する（節 2.5 ）．また全ての確率変数は \\(\\mathcal{F}\\)-条件付き独立である（節 2.4 ）．\n\n\n\n\n\n\n命題（条件付き独立性の特徴付け Doob）\n\n\n\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{C},\\mathcal{G},\\mathcal{H}\\subset\\mathcal{F}\\) について，次は同値：6\n\n\\(\\mathcal{G}\\perp\\!\\!\\!\\perp\\mathcal{H}\\mid\\mathcal{C}\\)．\n任意の \\(H\\in\\mathcal{H}\\) について， \\[\n\\operatorname{P}[H|\\mathcal{G}\\lor\\mathcal{C}]\\overset{\\text{a.s.}}{=}\\operatorname{P}[H|\\mathcal{C}]\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n(1)\\(\\Rightarrow\\)(2)：任意の \\(C\\in\\mathcal{C},G\\in\\mathcal{G},H\\in\\mathcal{H}\\) を取る． \\[\n\\begin{align*}\n  &\\operatorname{E}[\\operatorname{P}[H|\\mathcal{C}]1_{G\\cap C}]=\\operatorname{E}[\\operatorname{P}[H|\\mathcal{C}]1_G1_C]\\\\\n  &=\\operatorname{E}[\\operatorname{E}[P[H|\\mathcal{C}]1_G|\\mathcal{C}]1_C]=\\operatorname{E}[\\operatorname{P}[H|\\mathcal{C}]P[G|\\mathcal{C}]1_C]\\\\\n  &=\\operatorname{E}[\\operatorname{P}[H\\cap G|\\mathcal{C}]1_C]=\\operatorname{E}[\\operatorname{E}[1_{H\\cap G}1_C|\\mathcal{C}]]\\\\\n  &=\\operatorname{E}[1_H1_{G\\cap C}].\n\\end{align*}\n\\] が成り立つ．\\(G\\cap C\\) という形の集合は，\\(\\mathcal{G}\\lor\\mathcal{C}\\) を生成する集合体であるから，単調族定理より，\\(\\mathcal{G}\\lor\\mathcal{C}\\) の元は，\\(C\\cap G\\) という形の集合の単調増大列の極限として得られる．\nよって単調収束定理から， \\(\\operatorname{P}[H|\\mathcal{C}]\\overset{\\text{a.s.}}{=}\\operatorname{P}[H|\\mathcal{C}\\lor\\mathcal{G}]\\)．\n(2)\\(\\Rightarrow\\)(1)：任意の \\(G\\in\\mathcal{G},H\\in\\mathcal{H}\\) を取る． \\[\n\\begin{align*}\n  &\\operatorname{P}[G\\cap H|\\mathcal{C}]=\\operatorname{E}[1_{G\\cap H}|\\mathcal{C}]\\\\\n  &=\\operatorname{E}[\\operatorname{E}[1_G1_H|\\mathcal{G}\\lor\\mathcal{C}]|\\mathcal{C}]=\\operatorname{E}[1_G\\operatorname{E}[1_H|\\mathcal{G}\\lor\\mathcal{C}]|\\mathcal{C}]\\\\\n  &=\\operatorname{E}[1_G\\operatorname{E}[1_H|\\mathcal{C}]|\\mathcal{C}]=\\operatorname{E}[1_H|\\mathcal{C}]\\operatorname{E}[1_G|\\mathcal{C}]=\\operatorname{P}[H|\\mathcal{C}]\\operatorname{P}[G|\\mathcal{C}].\n\\end{align*}\n\\]\n\n\n\n\n\n\n3.2 条件付き分散\n\n\n\n\n\n\n命題（Pythagorasの式）\n\n\n\n\\[\n\\|Y\\|^2_2=\\|Y-\\operatorname{E}[Y|\\mathcal{G}]\\|^2_2+\\|\\operatorname{E}[Y|\\mathcal{G}]\\|^2_2.\n\\]\n\n\nこれは条件付き期待値が \\(L^2(\\Omega)\\)-射影であるためである（ 節 1.3 ）．\n確率変数 \\(Y\\in\\mathcal{L}^2(\\Omega)\\) の \\(\\mathcal{G}\\) に関する条件付き分散を \\[\n\\begin{align*}\n    \\mathrm{V}[Y|\\mathcal{G}]&:=\\operatorname{E}\\left[(Y-\\operatorname{E}[Y|\\mathcal{G}])^2|\\mathcal{G}\\right]\\\\\n    &=\\operatorname{E}[Y^2|\\mathcal{G}]-\\operatorname{E}[Y|\\mathcal{G}]^2\n\\end{align*}\n\\] と定める．このとき，次の 全分散の公式 と呼ばれる関係が成り立つ： \\[\n\\mathrm{V}[Y]=\\operatorname{E}[\\mathrm{V}[Y|\\mathcal{G}]]+\\mathrm{V}[\\operatorname{E}[Y|\\mathcal{G}]].\n\\]\n\n\n\n\n\n\n説明\n\n\n\n\n\nPythagorasの関係から， \\[\n\\begin{align*}\n    \\operatorname{E}[Y^2]&=\\operatorname{E}[(Y-\\operatorname{E}[Y|\\mathcal{G}])^2]\\\\\n    &\\qquad+\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]^2].\n\\end{align*}\n\\] 両辺から \\[\n\\operatorname{E}[Y]^2=\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]]^2\n\\] を減じると，右辺第一項の \\(\\operatorname{E}[-]\\) の中身は中心化確率変数であることから， \\[\n\\begin{align*}\n    \\mathrm{V}[Y]&=\\mathrm{V}[Y-\\operatorname{E}[Y|\\mathcal{G}]]\\\\\n    &\\qquad+\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]^2]-\\operatorname{E}[\\operatorname{E}[Y|\\mathcal{G}]]^2\\\\\n    &=\\mathrm{V}[Y-\\operatorname{E}[Y|\\mathcal{G}]]+\\mathrm{V}[\\operatorname{E}[Y|\\mathcal{G}]].\n\\end{align*}\n\\] 最後に， \\[\n\\mathrm{V}[Y-\\operatorname{E}[Y|\\mathcal{G}]]=\\operatorname{E}[\\mathrm{V}[Y|\\mathcal{G}]]\n\\] より結論が従う．\n\n\n\n\n\n3.3 条件付き共分散\n\n\n\n\n\n\n定義（条件付き共分散）\n\n\n\n\\(X,Y\\in\\mathcal{L}^2(\\Omega)\\) の \\(\\mathcal{G}\\) に関する 条件付き共分散 を \\[\n\\begin{align*}\n    &\\mathrm{C}[X,Y|\\mathcal{G}]\\\\\n    &=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X|\\mathcal{G}])(Y-\\operatorname{E}[Y|\\mathcal{G}])\\bigg|\\mathcal{G}\\biggr]\\\\\n    &=\\operatorname{E}[XY|\\mathcal{G}]-\\operatorname{E}[X|\\mathcal{G}]\\operatorname{E}[Y|\\mathcal{G}].\n\\end{align*}\n\\] と定義する．\n\n\n\n\n\n\n\n\n命題（条件付き共分散公式）\n\n\n\n\\[\n\\begin{align*}\n    &\\mathrm{C}[X,Y]\\\\\n    &=\\operatorname{E}[\\mathrm{C}[X,Y|\\mathcal{G}]]+\\mathrm{C}[\\operatorname{E}[X|\\mathcal{G}],\\operatorname{E}[Y|\\mathcal{G}]].\n\\end{align*}\n\\]\n\n\n証明は (Kallenberg, 2021) 補題8.2 p.166 など．"
  },
  {
    "objectID": "posts/2023/Probability/条件付き期待値の問題.html#footnotes",
    "href": "posts/2023/Probability/条件付き期待値の問題.html#footnotes",
    "title": "条件付き期待値の測度論的基礎付け",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Jacod and Shiryaev, 2003, p. 2) など．↩︎\n(Dudley, 2002, p. 340) など．↩︎\n(Kallenberg, 2021, p. 167)．↩︎\n(Kallenberg, 2021), (Dellacherie and Meyer, 1978)↩︎\n(Dudley, 2002, pp. 定理10.2.2 p.345)．一般には Borel空間に値を取る確率変数について成り立つ (Kallenberg, 2021, p. 165)．↩︎\n(Kallenberg, 2021, pp. 170–171) 定理8.9 も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html",
    "href": "posts/2024/Julia/PDMPFluxSlides.html",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "",
    "text": "Output from anim_traj() in PDMPFlux.jl package\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\nPDMPFlux.jl: package for MCMC sampling.\n\n\n\n\nInstead of Markov Chains, PDMPFlux.jl uses:\n\nCurrently, most PDMPs move piecewise linearly.\n\n\n\n\n\n\n\n\nPDMP\nDiffusion\n\n\n\n\nDiffuse?\nNo\nYes\n\n\nJump?\nYes\nNo\n\n\nDriving noise\nPoisson\nGauss\n\n\nPlot\n\n\n\n\n\n\n\n\n\nusing PDMPFlux\n\nfunction U_Cauchy(x::Vector)\n    return log(1 + x.^2)\nend\n\ndim = 10\nsampler = ZigZagAD(dim, U_Cauchy)  # Instantiate a sampler\nInputs: Dimension d, and any U that satisfies\n\np(x) \\,\\propto\\,\\exp\\left\\{ -U(x) \\right\\},\\qquad x\\in\\mathbb{R}^d.\n\n\n\nU may be called potential, or negative log-density.\n\n\n\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)  # Hyperparameters\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\nOutput: N samples from p\\,\\propto\\,e^{-U}.\n\n\n\nN_sk: number of orange points, N: number of samples, xinit, vinit: initial position and velocity.\n\n\n\nFunction sample is a wrapper of:\ntraj = sample_skeleton(sampler, N_sk, xinit, vinit)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, traj)  # get samples from the skeleton points\ntraj contains a list \\{x_i\\}_{i=1}^{N_{sk}} of orange points\n\n\n\n\ndiagnostic(traj)\n\n\n\n\n\nWe see acceptance rate is a bit low due to the long tails of p."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html#introduction",
    "href": "posts/2024/Julia/PDMPFluxSlides.html#introduction",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "",
    "text": "Output from anim_traj() in PDMPFlux.jl package\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\nPDMPFlux.jl: package for MCMC sampling.\n\n\n\n\nInstead of Markov Chains, PDMPFlux.jl uses:\n\nCurrently, most PDMPs move piecewise linearly.\n\n\n\n\n\n\n\n\nPDMP\nDiffusion\n\n\n\n\nDiffuse?\nNo\nYes\n\n\nJump?\nYes\nNo\n\n\nDriving noise\nPoisson\nGauss\n\n\nPlot\n\n\n\n\n\n\n\n\n\nusing PDMPFlux\n\nfunction U_Cauchy(x::Vector)\n    return log(1 + x.^2)\nend\n\ndim = 10\nsampler = ZigZagAD(dim, U_Cauchy)  # Instantiate a sampler\nInputs: Dimension d, and any U that satisfies\n\np(x) \\,\\propto\\,\\exp\\left\\{ -U(x) \\right\\},\\qquad x\\in\\mathbb{R}^d.\n\n\n\nU may be called potential, or negative log-density.\n\n\n\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)  # Hyperparameters\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\nOutput: N samples from p\\,\\propto\\,e^{-U}.\n\n\n\nN_sk: number of orange points, N: number of samples, xinit, vinit: initial position and velocity.\n\n\n\nFunction sample is a wrapper of:\ntraj = sample_skeleton(sampler, N_sk, xinit, vinit)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, traj)  # get samples from the skeleton points\ntraj contains a list \\{x_i\\}_{i=1}^{N_{sk}} of orange points\n\n\n\n\ndiagnostic(traj)\n\n\n\n\n\nWe see acceptance rate is a bit low due to the long tails of p."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html#sec-PDMP",
    "href": "posts/2024/Julia/PDMPFluxSlides.html#sec-PDMP",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2 What is PDMP?",
    "text": "2 What is PDMP?\n\n\n\nAnimated by (Grazzi, 2020)\n\n\n\n2.1 Two Key Changes in MCMC History\n\nLifting: MH (Metropolis-Hastings) → Lifted MH\nContinuous-time Limit: Lifted MH → Zig-Zag Sampler\n\n\n\n\n\n\n\n\n\n\n(MH Metropolis et al., 1953)\n\n\n\n\n\n\n\n(Lifted MH Turitsyn et al., 2011)\n\n\n\n\n\n\n\n(Zig-Zag Bierkens et al., 2019)\n\n\n\n\n\n\n\n2.2 What’s Wrong with MH?: Reversibility\nReversibility (a.k.a detailed balance): \np(x)q(x|y)=p(y)q(y|x).\n In words: \n\\text{Probability}[\\text{Going}\\;x\\to y]=\\text{Probability}[\\text{Going}\\;y\\to x].\n  Harder to explore the entire space\n Slow mixing of MH\n\n\n2.3 Lifting into a Larger State Space\n\n\n\n\n\n\n\n\n\n\n\nq^{(+1)}: Only propose \\rightarrow moves\nq^{(-1)}: Only propose \\leftarrow moves\n\n Once going uphill, it continues to go uphill.\n This is irreversible, since\n\\begin{align*}\n  &\\text{Probability}[x\\to y]\\\\\n  &\\qquad\\ne\\text{Probability}[y\\to x].\n\\end{align*}\n\n\n\n\n\n2.4 Continuous-time Limit: A Strategy for Efficient Computing\n‘Limiting case of lifted MH’ means that we only simulate where we should flip the momentum \\sigma\\in\\{\\pm1\\} in Lifted MH.\n\n\n\n\n\n\n(1d Zig Zag sampler Bierkens et al., 2019)\n\n\n\nInput: Gradient \\nabla\\log p of log target density p\nFor n\\in\\{1,2,\\cdots,N\\}:\n\nSimulate an first arrival time T_n of a Poisson point process (described in the next slide)\nLinearly interpolate until time T_n: \nX_t = X_{T_{n-1}} + \\sigma(t-T_{n-1}),\\qquad t\\in[T_{n-1},T_n].\n\nGo back to Step 1 with the momentum \\sigma\\in\\{\\pm1\\} flipped\n\n\n\n\n\n2.5 Summary\n\nPDMPs have irreversible dynamics\nPDMPs are easy to simulate\n Promising for high-dimensional problems\n\nUsing PDMPFlux.jl, I want to fit a large Bayesian model, showcasing the usefulness of PDMP samplers."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html#references",
    "href": "posts/2024/Julia/PDMPFluxSlides.html#references",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "3 References",
    "text": "3 References\n\n\n\n\n\n\n\n\n\nFor further details, please see my old introductory slides via the above QR code\n\n\n\n\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nGrazzi, S. (2020). Piecewise deterministic monte carlo.\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953). Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6), 1087–1092.\n\n\nTuritsyn, K. S., Chertkov, M., and Vucelja, M. (2011). Irreversible Monte Carlo algorithms for Efficient Sampling. Physica D-Nonlinear Phenomena, 240(5-Apr), 410–414."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#overview",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#overview",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nPDMPFlux.jl: package for MCMC sampling."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#whats-different-detailed-in-sec-pdmp",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#whats-different-detailed-in-sec-pdmp",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.2 What’s Different? (Detailed in Section 2)",
    "text": "1.2 What’s Different? (Detailed in Section 2)\nInstead of Markov Chains, PDMPFlux.jl uses:\n\nCurrently, most PDMPs move piecewise linearly."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#whats-pdmp-detailed-in-sec-pdmp",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#whats-pdmp-detailed-in-sec-pdmp",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.3 What’s PDMP? (Detailed in Section 2)",
    "text": "1.3 What’s PDMP? (Detailed in Section 2)\n\n\n\n\n\nPDMP\nDiffusion\n\n\n\n\nDiffuse?\nNo\nYes\n\n\nJump?\nYes\nNo\n\n\nDriving noise\nPoisson\nGauss\n\n\nPlot"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#how-to-use-13",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#how-to-use-13",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.4 How to Use? (1/3)",
    "text": "1.4 How to Use? (1/3)\nusing PDMPFlux\n\nfunction U_Cauchy(x::Vector)\n    return log(1 + x.^2)\nend\n\ndim = 10\nsampler = ZigZagAD(dim, U_Cauchy)  # Instantiate a sampler\nInputs: Dimension d, and any U that satisfies\n\np(x) \\,\\propto\\,\\exp\\left\\{ -U(x) \\right\\},\\qquad x\\in\\mathbb{R}^d.\n\n\n\nU may be called potential, or negative log-density."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#how-to-use-23",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#how-to-use-23",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.5 How to Use? (2/3)",
    "text": "1.5 How to Use? (2/3)\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)  # Hyperparameters\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\nOutput: N samples from p\\,\\propto\\,e^{-U}.\n\n\n\nN_sk: number of orange points, N: number of samples, xinit, vinit: initial position and velocity."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#how-to-use-33",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#how-to-use-33",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.6 How to Use? (3/3)",
    "text": "1.6 How to Use? (3/3)\nFunction sample is a wrapper of:\ntraj = sample_skeleton(sampler, N_sk, xinit, vinit)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, traj)  # get samples from the skeleton points\ntraj contains a list \\{x_i\\}_{i=1}^{N_{sk}} of orange points"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#diagnostic",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#diagnostic",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "1.7 Diagnostic",
    "text": "1.7 Diagnostic\ndiagnostic(traj)\n\n\n\n\n\nWe see acceptance rate is a bit low due to the long tails of p."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#two-key-changes-in-mcmc-history",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#two-key-changes-in-mcmc-history",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2.1 Two Key Changes in MCMC History",
    "text": "2.1 Two Key Changes in MCMC History\n\nLifting: MH (Metropolis-Hastings) → Lifted MH\nContinuous-time Limit: Lifted MH → Zig-Zag Sampler\n\n\n\n\n\n\n\n\n\n\n(MH Metropolis et al., 1953)\n\n\n\n\n\n\n\n(Lifted MH Turitsyn et al., 2011)\n\n\n\n\n\n\n\n(Zig-Zag Bierkens et al., 2019)"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#whats-wrong-with-mh-reversibility",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#whats-wrong-with-mh-reversibility",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2.2 What’s Wrong with MH?: Reversibility",
    "text": "2.2 What’s Wrong with MH?: Reversibility\nReversibility (a.k.a detailed balance): \np(x)q(x|y)=p(y)q(y|x).\n In words: \n\\text{Probability}[\\text{Going}\\;x\\to y]=\\text{Probability}[\\text{Going}\\;y\\to x].\n  Harder to explore the entire space\n Slow mixing of MH"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#lifting-into-a-larger-state-space",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#lifting-into-a-larger-state-space",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2.3 Lifting into a Larger State Space",
    "text": "2.3 Lifting into a Larger State Space\n\n\n\n\n\n\n\n\n\n\n\nq^{(+1)}: Only propose \\rightarrow moves\nq^{(-1)}: Only propose \\leftarrow moves\n\n Once going uphill, it continues to go uphill.\n This is irreversible, since\n\\begin{align*}\n  &\\text{Probability}[x\\to y]\\\\\n  &\\qquad\\ne\\text{Probability}[y\\to x].\n\\end{align*}"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#continuous-time-limit-a-strategy-for-efficient-computing",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#continuous-time-limit-a-strategy-for-efficient-computing",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2.4 Continuous-time Limit: A Strategy for Efficient Computing",
    "text": "2.4 Continuous-time Limit: A Strategy for Efficient Computing\n‘Limiting case of lifted MH’ means that we only simulate where we should flip the momentum \\sigma\\in\\{\\pm1\\} in Lifted MH.\n\n\n\n\n(1d Zig Zag sampler Bierkens et al., 2019)\n\n\nInput: Gradient \\nabla\\log p of log target density p\nFor n\\in\\{1,2,\\cdots,N\\}:\n\nSimulate an first arrival time T_n of a Poisson point process (described in the next slide)\nLinearly interpolate until time T_n: \nX_t = X_{T_{n-1}} + \\sigma(t-T_{n-1}),\\qquad t\\in[T_{n-1},T_n].\n\nGo back to Step 1 with the momentum \\sigma\\in\\{\\pm1\\} flipped"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux_Slides.html#summary",
    "href": "posts/2024/Julia/PDMPFlux_Slides.html#summary",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2.5 Summary",
    "text": "2.5 Summary\n\nPDMPs have irreversible dynamics\nPDMPs are easy to simulate\n Promising for high-dimensional problems\n\nUsing PDMPFlux.jl, I want to fit a large Bayesian model, showcasing the usefulness of PDMP samplers."
  },
  {
    "objectID": "posts/2024/Process/Levy.html#終わりに",
    "href": "posts/2024/Process/Levy.html#終わりに",
    "title": "Lévy 過程を見てみよう",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nLévy 測度 \\[\n\\int_{\\mathbb{R}^d}(\\lvert u\\rvert^2\\land1)\\,\\nu(du)&lt;\\infty\n\\] に関して最も興味深いのは，跳躍測度の焦点になるのは，裾の重さではなくて極小の跳躍の量であるということである．\n裾とは別に，極小の跳躍の和が発散するかどうかが B 型と C 型を分ける．"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#gauss-過程の上限の集中不等式",
    "href": "posts/2024/Slides/Master.html#gauss-過程の上限の集中不等式",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n体積測度 \\mu が等しい可測集合のうち，球が最小の測度を持つ．\n\n\n\n\n\n\n\nBorel 可測集合 A\\subset\\mathbb{R}^n に関して，\\epsilon-閉近傍 を次のように定める： \nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\mid d(x,A)\\le t\\right\\},\\qquad t&gt;0.\n\n実は A は Borel 可測とは限らないが，Lebesgue 可測ではある．\n\n\n\n\n\n\n\n\n\n\n古典的集中不等式 (Schmidt, 1948)-(Lévy, 1951)\n\n\n\nn-次元球面 S^n\\subset\\mathbb{R}^{n+1} に関して，A\\subset S^n を Borel 可測，C を同体積の（測地）球とすると， \n\\mu(C_\\epsilon)\\le\\mu(A_\\epsilon),\\qquad\\epsilon&gt;0.\n\n\n\n\n\n\n\n\n\n\n\n\n(Giné and Nickl, 2021, p. 31) 定理 2.2.3\n\n\n\n\\gamma_n を \\mathbb{R}^n 上の標準正規分布とする．A\\subset\\mathbb{R}^n を Borel 可測， \nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\qquad a\\in\\mathbb{R},u\\in\\mathbb{R}^n\\setminus\\{0\\},\n を同体積の affine 半空間 とすると， \n\\gamma_n(H_a+\\epsilon B^n)\\le\\overline{\\gamma_n}(A_\\epsilon+\\epsilon B^n),\\qquad\\epsilon&gt;0.\n\n\n\n\\mathbb{R}^n だけでなく \\mathbb{R}^\\infty 上でも成り立つ．\n\n\n\n\n\n\n\n\n\n(Borell, 1975)-(Sudakov and Tsirel’son, 1974)\n\n\n\n\\{X_t\\}_{t\\in T} を可分な中心 Gauss 過程で，ほとんど確実に上限 \\|X\\|_\\infty が有限であるとする．このとき，\\|X\\|_\\infty の中央値 M に関して，1 \n\\operatorname{P}\\biggl[\\biggl|\\|X\\|_\\infty-M\\biggr|&gt;u\\biggr]\\le\\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right),\\qquad u&gt;0,\\sigma^2:=\\sup_{t\\in T}\\mathrm{V}[X_t].\n\n\n\n同様の命題を平均値の周りに関しても示せる．係数 2 が前につくものは (Gross, 1975) による正規分布に関する対数 Sobolev 不等式から導ける．"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#footnotes",
    "href": "posts/2024/Slides/Master.html#footnotes",
    "title": "Zig-Zag サンプラーのモデル選択への応用",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの設定では \\|X\\|_\\infty は連続分布をもち，M は一意に定まる．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の理論",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の理論",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "2 Sticky PDMP の理論",
    "text": "2 Sticky PDMP の理論\n\n2.1 設定\nパラメータ \\(x\\in\\mathbb{R}^d\\) 上に spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) \\[\np(dx)=\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{1}\\] を導入して，ベイズ変数選択を行うとしよう．\nこのとき，モデルの対数尤度を \\(\\ell(x):=\\log p(y|x)\\) とすると，事後分布は \\[\np(x|y)\\,dx\\,\\propto\\,p(y|x)p(dx)=e^{\\ell(x)}\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)+(1-\\omega_i)\\delta_0(x_i)\\biggl)\n\\tag{2}\\] と表せる．\nそこでこの設定を少し抽象化して，ポテンシャル \\(\\Psi:\\mathbb{R}^d\\to\\mathbb{R}\\) を通じて \\[\n\\mu(dx)=Ce^{-\\Psi(x)}\\prod_{i=1}^d\\left(dx_1+\\frac{1}{\\kappa_i}\\delta_0(dx_i)\\right)\n\\tag{3}\\] と表せる分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) からサンプリングする問題を考える．\n事後分布 (2) は \\[\n\\kappa_i=\\frac{\\omega_i}{1-\\omega_i}p_i(0)(&gt;0)\n\\] と取った場合にあたる．\nこの \\(\\mu\\) (3) は Lebesgue 測度に関して絶対連続でないため密度を持たず，通常の勾配を用いた MCMC 法を直接は適用できない．\n\n\n2.2 他手法との比較\n\n\n\n\n\n\n\n(3) のような非絶対連続分布からのサンプリングは難しい．そのため spike-and-slab 事前分布 (1) の \\(\\delta_0\\) を分散の小さな正規分布などに軟化した絶対連続な分布に置き換えて，これに対して HMC などの勾配ベースの MCMC 法を適用することもあり得る (Goldman et al., 2022)．\nあるいは初めから Laplace 分布や馬蹄事前分布などの絶対連続なスパース誘導事前分布を用い，事後分布は Gibbs サンプラーなどの勾配情報を用いないサンプラーで行う (Griffin and Brown, 2021)，というアプローチもあり得る．\n\n\n\n\nしかし (3) のようなアトムを持った分布に直接適用できる Sticky PDMP によるアプローチは，２の方法と違って非可逆なモデル間ジャンプを達成する効率的なサンプラーである．\nさらにその上，Reversible-Jump MCMC の拡張と見れる通り，特定の部分空間にトラップされていた総時間を計算することで，ベイズ因子を計算せずに事後包含確率 (PIP: Posterior Inclusion Probability) を，１の方法と違って誤差なく計算できるという美点がある．\n\n\n2.3 サンプラー\n\n\n\n\n\n\nThe Sticky Zig-Zag Sampler (Bierkens et al., 2023)\n\n\n\n\n基本的には \\[\n\\mathbb{R}^d\\times\\{\\pm1\\}^d\n\\]\n上を動く Zig-Zag サンプラーである．\n任意の座標成分が \\(0\\) になったとき，すなわち\n\\[\n\\left\\{(x,v)\\in\\mathbb{R}^d\\times\\{\\pm 1\\}^d\\mid\\exists i\\in[d]\\;x_i=0\\right\\}\n\\] を通過したとき，座標成分 \\(x_i\\) は \\(0\\) に固定され，Poisson ジャンプが生じるまでそのままである．\n\n\n\nこれは一見 Markov 過程にならないが，状態空間を拡張して \\[\nE:=\\mathbb{R}_{00}^d\\times\\{\\pm1\\}^d,\\qquad\\mathbb{R}_{00}:=(-\\infty,0^-]\\sqcup[0^+,\\infty)\n\\] と考えると，この上の Markov 過程になる．\n第 \\(i\\in[d]\\) 成分が \\((x_i,v_i)=(0^-,-1)\\) または \\((0^+,1)\\) になったとき，この座標は \\((x_i,v_i)\\) のまま動かなくなり，この期間だけ \\(v_i\\) は必ずしも「速さ」を表さなくなる，と解する．\nトラップ状態から脱出するレートは \\(\\kappa_i\\) とする．\n\n\n2.4 エルゴード性\nこうして構成された Zig-Zag サンプラーは，分布 \\(\\mu\\) の裾が重すぎない場合，具体的にはある \\(c&gt;d,c'\\in\\mathbb{R}\\) が存在して \\[\n\\Psi(x)&gt;c\\log\\lvert x\\rvert-c',\\qquad x\\in\\mathbb{R}^d,\n\\] を満たすとき（全変動ノルムに関して）エルゴード的である (Prop. A.9 Bierkens et al., 2023, p. 20)．\n\n\n\n\n\n\n(\\(0\\) への再起時刻 Bierkens et al., 2023, p. 6)\n\n\n\n\\(\\mathbb{R}^d\\) の原点への再起時刻 \\(T_0\\) の期待値は \\[\n\\operatorname{E}[T_0]=\\frac{1-\\mu(\\{0\\})}{d\\kappa\\mu(\\{0\\})}.\n\\]\n\n\n\n\n2.5 極限\nSticky Zig-Zag サンプラーは，spike-and-slab 事前分布 (1) を，十分小さい標準偏差 \\(c_i&gt;0\\) を持つ正規分布で近似した \\[\n\\widetilde{p}(dx)=\\sum_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_1+(1-\\omega_i)\\mathrm{N}(0,c_i^2)\\biggl)\n\\] に対する Zig-Zag サンプラーの，\\(c_i\\to0\\) の極限の場合と見れる (Chevallier et al., 2023, p. 2917)．\n\n\n\n(Chevallier et al., 2023, p. 2918)\n\n\n\n\n2.6 一般化\n一般に２つの空間 \\(E_i,E_j\\) の間のジャンプをデザインするとする．\n任意に部分集合 \\(\\Gamma_{i,j}\\subset E_i\\) を定めて，ここを通過する度に確率 \\(p_{i,j}&gt;0\\) でジャンプするとすると，ここから戻る Poisson レート \\(\\beta_{i,j}:E_j\\to\\mathbb{R}_+\\) は \\[\n\\beta_{i,j}(z):=p_{i,j}\\frac{}{}\n\\]"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#traverse-サンプラー",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#traverse-サンプラー",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "3 Traverse サンプラー",
    "text": "3 Traverse サンプラー\n\n3.1 はじめに\n(Koskela, 2022) により，任意の可算空間 \\(\\mathbb{F}\\) に対して，\\(\\mathbb{F}\\) と連続空間との合併からサンプリングをするサンプラーが提案されている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#積空間上の方法",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#積空間上の方法",
    "title": "超次元 MCMC",
    "section": "3 積空間上の方法",
    "text": "3 積空間上の方法\n\n3.1 ベイズモデルとしての吸収\n超次元 MCMC と同じタイミング (Carlin and Chib, 1995) で，比較したい２つのモデルがあった場合，これらを結合し，足りない specification には “psudo-prior” を設定することで一つの巨大なモデルとみなし，Gibbs サンプラーによって推論するという発想があった．\nこの考え方は現代もモデル平均につながるが，比較したいモデルが多かったり，\\(K\\) を無限大にしたい場合は pseudo-prior の設定が煩雑である．\nということでこの方法は必ずしもスケールしないが，超次元 MCMC を包含する広いクラスの手法を特別な場合として含む (Godsill, 2001)．\n\n\n3.2 population-based method\npopulation-based methods の考え方を導入することで，超次元 MCMC の収束を加速することができる．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-population",
    "href": "posts/Surveys/SMCSamplers.html#sec-population",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "MCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html",
    "href": "posts/2024/Stat/Regression.html",
    "title": "セミパラメトリック重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n回帰モデルの点推定で最も広く用いられるアルゴリズムは OLS である： \\[\n\\widehat{\\beta}:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\lvert Y-Xb\\rvert^2_2.\n\\]\nその他の推定法は別稿で扱う：\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\nOLS は応答 \\(Y\\) を最もよく復元する推定量 \\(X\\widehat{\\beta}\\) を構成する．損失を \\(Y\\) のなす Euclid 空間 \\(\\mathbb{R}^n\\) 内の距離とした \\(M\\)-推定量である．\n尤度を使わない推定法であるが，Gauss-Markov モデル（均一誤差モデル） \\[\n\\operatorname{E}[\\epsilon|X]=0,\\qquad\\mathrm{C}[\\epsilon|X]=\\sigma^2I_n\n\\] に関しては，誤差 \\(\\epsilon\\) の分布に依らず，（セミパラメトリック）漸近有効性を持つ．しかも \\(\\epsilon_i\\) は i.i.d. とは限らない．\nその構成から予期される通り，OLS 推定量は極めて良い線型代数的な性質を持つ．実際， \\[\n\\widehat{\\beta}=(X^\\top X)^{-1}X^\\top Y\n\\] という表示をもち，\\(X\\widehat{\\beta}\\) は \\(Y\\) の \\(X\\) の列ベクトルの貼る空間への線型射影である．\nここでは重回帰モデルにおける OLS 推定量の性質を調べる．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#概要",
    "href": "posts/2024/Stat/Regression.html#概要",
    "title": "重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n回帰モデルの点推定で最も広く用いられるアルゴリズムは OLS である： \\[\n\\widehat{\\beta}:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\lvert Y-Xb\\rvert^2_2.\n\\]\nその他の推定法は別稿で扱う：\n\n\n\n\n\n\n\n\n\n一般化モーメント法と一般化推定方程式\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\nOLS は応答 \\(Y\\) を最もよく復元する推定量 \\(X\\widehat{\\beta}\\) を構成する．ただし損失は \\(Y\\) のなす Euclid 空間 \\(\\mathbb{R}^n\\) 内の距離に関して測るものとする．\nこの構成から予期される通り，OLS は極めて良い線型代数的な性質を持つ．実際， \\[\n\\widehat{\\beta}=(X^\\top X)^{-1}X^\\top Y\n\\] という表示をもち，\\(X\\widehat{\\beta}\\) は \\(Y\\) の \\(X\\) の列ベクトルの貼る空間への線型射影である．\nここでは重回帰モデルにおける OLS 推定量の性質を調べる．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#部分回帰の問題",
    "href": "posts/2024/Stat/Regression.html#部分回帰の問題",
    "title": "回帰分析",
    "section": "2 部分回帰の問題",
    "text": "2 部分回帰の問題\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n2.1 設定\n計画行列 \\(X=(X_1\\;X_2)\\) に関して，回帰モデル \\[\nY=X\\widehat{\\beta}+\\widehat{\\epsilon}\n\\tag{1}\\] に対して，\\(X_1\\) を入れなかった場合 \\[\nY=X_2\\widetilde{\\beta}_2+\\widetilde{\\epsilon}\n\\tag{2}\\] を考える．\n部分モデル (2) の回帰係数 \\(\\widetilde{\\beta}_2\\) は \\[\n\\widetilde{\\beta}_2=(X_2^\\top X_2)^{-1}X_2^\\top Y\n\\] で得られる．\n\n\n2.2 Frisch-Waugh-Lovell の定理\n\n\n\n\n\n\nFrisch-Waugh-Lovell の定理\n\n\n\n\\(X_1\\) の列ベクトルが貼る空間の \\(\\mathbb{R}^n\\) 上の補空間への射影を \\[\nH_2:=I_n-H_1,\\qquad H_1:=X_1(X_1^\\top X_1)^{-1}X_1^\\top\n\\] で表すと， \\[\n\\widehat{\\beta}_2=(\\widetilde{X}_2^\\top\\widetilde{X}_2)^{-1}\\widetilde{X}_2^\\top\\widetilde{Y},\\qquad\\widetilde{X}_2:=H_2X_2,\\widetilde{Y}:=H_2Y.\n\\]\n\n\n\n\n2.3 leverage score\n射影行列 \\[\nH:=X(X^\\top X)^{-1}X^\\top\n\\] は鍵となる値で，この対角成分は leverage score と呼ばれ，次を満たす1 \\[\n\\operatorname{tr}(H)=\\operatorname{rank}(H)=p.\n\\]\n\n\n2.4 VIF\n\n\n\n\n\n\n命題2\n\n\n\n\\(\\widehat{\\beta}_j\\) を \\(Y\\) を \\((1_n,X_1,\\cdots,X_q)\\) に関して回帰した際の係数とする．真のモデルがある関数 \\(f\\) に関して \\(y_i=f(x_i)+\\epsilon_i\\) で \\(\\epsilon_i\\sim(0,\\sigma^2)\\) が互いに相関を持たない場合，次が成り立つ： \\[\n\\mathrm{V}[\\widehat{\\beta}_j]=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_{ij}-\\overline{x}_j)^2}\\frac{1}{1-R^2_j}.\n\\] ただし \\(R_j^2\\) とは \\(X_j\\) を \\((1_n,X_1,\\cdots,X_{j-1},X_{j+1},\\cdots,X_q)\\) に関して回帰した際の決定係数とした．\n\n\nこの際，最初の因子は \\(Y\\) を \\((1_n,X_j)\\) に関して回帰した際の係数 \\(\\widetilde{\\beta}_j\\) の分散に一致する．従って次の因子 \\[\n\\operatorname{VIF}_j:=1/(1-R_j^2)\n\\] は，他の説明変数 \\(X_1,\\cdots,X_{j-1},X_{j+1},\\cdots,X_q\\) を加えたことによる，\\(X_j\\) の推定係数の増大具合を表す．\nこれを 分散拡大係数 (VIF: Variance Inflation Factor) と呼ぶ．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#footnotes",
    "href": "posts/2024/Stat/Regression.html#footnotes",
    "title": "セミパラメトリック重回帰分析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ding, 2024, p. 81) 定理9.1．The proof of Theorem 9.1 is very simple. However, it is one of the most insightful formulas in statistics.↩︎\n生態学的誤謬 (ecological fallacy) ともいう．↩︎\nこの意味での「内生性」は，「外生的じゃない」こととも意味がズレてしまう．\\(\\operatorname{E}[\\epsilon|X]=0\\) を満たすならば \\(\\mathrm{C}[\\epsilon,X]=0\\) が必要であるから，「内生的ならば外生的でない」は成り立つ．ここでは内生的じゃないことを 広義外生性 と呼ぼう．また多くの場合他の経済学の文脈では，「モデル内で決定される変数」程度の意味で内生変数と呼ぶことも多い．↩︎\n処置変数と相関を持たないということは，非交絡性 \\(Y_i\\perp\\!\\!\\!\\perp Z_i\\mid U_i\\) よりは弱い条件である．なお，この「非交絡性」は疫学の言い方であり，計量経済学では 無視可能性 または \\(U_i\\) が観測可能である場合は selection on observables などとも呼ぶ．逆に言えば，交絡とは selection on unobservables のことである．↩︎\n(Ding, 2024, p. 95) も参照．↩︎\n(Ding, 2024, p. 130) 定理13.1．↩︎\n(Ding, 2024, p. 44) 定理6.1．↩︎\n(Ding, 2024, p. 268) 定理24.2．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#footnotes",
    "href": "posts/2024/Survey/BDA2.html#footnotes",
    "title": "ベイズデータ解析６",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ding, 2024, p. 222) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#重回帰",
    "href": "posts/2024/Stat/Regression.html#重回帰",
    "title": "セミパラメトリック重回帰分析",
    "section": "2 重回帰",
    "text": "2 重回帰\n\n2.1 設定\n計画行列 \\(X=(X_1\\;X_2)\\) に関して，回帰モデル \\[\nY=X\\widehat{\\beta}+\\widehat{\\epsilon}\n\\tag{1}\\] に対して，\\(X_1\\) を入れなかった場合 \\[\nY=X_2\\widetilde{\\beta}_2+\\widetilde{\\epsilon}\n\\tag{2}\\] を考える．\n部分モデル (2) の回帰係数 \\(\\widetilde{\\beta}_2\\) は \\[\n\\widetilde{\\beta}_2=(X_2^\\top X_2)^{-1}X_2^\\top Y\n\\] で得られる．\n\n\n2.2 Frisch-Waugh-Lovell の定理\n次の結果は少なくとも (Yule, 1907) から知られていたが，計量経済学では (Frisch and Waugh, 1933) と (Lovell, 1963) の名前で知られる．\n\n\n\n\n\n\nFrisch-Waugh-Lovell の定理\n\n\n\n\\(X_1\\) の列ベクトルが貼る空間の \\(\\mathbb{R}^n\\) 上の補空間への射影を \\[\nH_2:=I_n-H_1,\\qquad H_1:=X_1(X_1^\\top X_1)^{-1}X_1^\\top\n\\] で表すと， \\[\n\\widehat{\\beta}_2=(\\widetilde{X}_2^\\top\\widetilde{X}_2)^{-1}\\widetilde{X}_2^\\top\\widetilde{Y},\\qquad\\widetilde{X}_2:=H_2X_2,\\widetilde{Y}:=H_2Y.\n\\]\n\n\nすなわち，\\(X_1,X_2\\) で回帰した (1) の係数 \\(\\widehat{\\beta}_2\\) は，まず \\(X_2\\) を \\(X_1\\) を説明変数で回帰した後に，(2) の代わりに \\(Y\\) をその残差 \\(\\widetilde{X}_2\\) で回帰して得る係数に等しい．\nこれを重回帰係数の 部分回帰係数 としての解釈とも呼ぶ (Ding, 2024, p. 60)．\n\n\n2.3 Cochran の公式\n\n\n\n\n\n\n(Cochran, 1938)1\n\n\n\n\\(X_1\\) を \\(X_2\\) で回帰した際の係数を \\(\\widehat{\\delta}\\) とする： \\[\nX_1=X_2\\widehat{\\delta}+\\widehat{U}.\n\\] このとき， \\[\n\\widetilde{\\beta}_2=\\widehat{\\beta}_2+\\widehat{\\delta}\\widehat{\\beta}_1.\n\\]\n\n\nこれは \\(X_2\\) の \\(Y\\) への影響のうち，\\(X_1\\) を通じたもの \\(\\widehat{\\delta}\\widehat{\\beta}_1\\) とそうでないものとを分解していると見れる．\\(\\widehat{\\beta}_2\\) は \\(\\widehat{\\beta}_1\\) の方向に縮小するとも見れる．\n\\(\\widehat{\\delta}\\widehat{\\beta}_1\\) の符号によっては，\\(\\widetilde{\\beta}_2,\\widehat{\\beta}_2\\) の符号が異なることがある．このような現象は (Simpson, 1951) のパラドックスともいう．2\n計量経済学では \\(\\widehat{\\delta}\\widehat{\\beta}_1\\) の項を 欠落変数バイアス (omitted variable bias) とも呼ぶ． \\[\n\\widehat{\\delta}=\\frac{\\mathrm{C}[X_1,X_2]}{\\sqrt{\\mathrm{V}[X_1]\\mathrm{V}[X_2]}}\n\\] であるから，\\(X_1,X_2\\) が無相関であった場合はこの項は零になる．\nすなわち，誤差 \\(\\epsilon\\) が外生性の仮定 \\(\\operatorname{E}[\\epsilon|X]=0\\) を満たすまでに十分多くの説明変数を回帰モデルに入れないと，OLS 推定量はバイアスを持ってしまう．軽量経済学において，\\(X\\) が \\(\\epsilon\\) と相関を持つことを 内生性 (endogeneity) という (B. E. Hansen, 2022, p. 335), (Hayashi, 2000, p. 64)．3\n(Baron and Kenny, 1986) の媒介分析はこのように OLS 推定を複数の回帰モデルに対して実行し，直接効果と間接効果の量を推定する．この手続きは (Wright, 1918) のパス分析と深い関係がある．\n\n\n2.4 交絡と共変量統制\n具体的に，処置変数を \\(Z_i\\in\\{0,1\\}\\) とした回帰分析 \\[\nY_i=\\widetilde{\\beta}_0+\\widetilde{\\beta}_1Z_i+\\widetilde{\\beta}_2^\\top X_i+\\widetilde{\\epsilon}_i\n\\] を考える．この際，欠落した説明変数 \\(U_i\\) であって，処置変数 \\(Z_i\\) と相関を持つものを 交絡因子 という．4\nフルモデル \\[\nY_i=\\widehat{\\beta}_0+\\widehat{\\beta}_1Z_i+\\widehat{\\beta}_2^\\top X_i+\\widehat{\\beta}_3^\\top U_i+\\widehat{\\epsilon}_i\n\\] に関して，Cochran の公式によれば，\\(Z_i\\) を \\(U_i\\) に関して回帰した際の \\(U_i\\) の係数を \\(\\widehat{\\delta}\\) とすると， \\[\n\\widetilde{\\beta}_1-\\widehat{\\beta}_1=\\widehat{\\beta}_3\\widehat{\\delta}\n\\] が成り立つ．加えて，\\(U_i\\) を \\(X_i\\) に関して回帰して得る残差を \\(e_i\\) とすると，\\(\\widehat{\\delta}\\) の値はこの \\(e_i\\) の値のグループ間差に等しい： \\[\n\\widehat{\\delta}=\\overline{e}^{(1)}-\\overline{e}^{(0)}.\n\\]\nすなわち，\\(X_i\\) で説明される分を除いて，\\(U_i\\) の値が処置群と管理群とで平均的に大きな差があるほど，交絡によるバイアスは大きいものとなる．\n\n\n2.5 leverage score\n射影行列 \\[\nH:=X(X^\\top X)^{-1}X^\\top\n\\] は鍵となる値で，この対角成分は leverage score と呼ばれ，次を満たす5 \\[\n\\operatorname{tr}(H)=\\operatorname{rank}(H)=p.\n\\]\n\n\n2.6 VIF\n\n\n\n\n\n\n命題6\n\n\n\n\\(\\widehat{\\beta}_j\\) を \\(Y\\) を \\((1_n,X_1,\\cdots,X_q)\\) に関して回帰した際の係数とする．真のモデルがある関数 \\(f\\) に関して \\(y_i=f(x_i)+\\epsilon_i\\) で \\(\\epsilon_i\\sim(0,\\sigma^2)\\) が互いに相関を持たない場合，次が成り立つ： \\[\n\\mathrm{V}[\\widehat{\\beta}_j]=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_{ij}-\\overline{x}_j)^2}\\frac{1}{1-R^2_j}.\n\\] ただし \\(R_j^2\\) とは \\(X_j\\) を \\((1_n,X_1,\\cdots,X_{j-1},X_{j+1},\\cdots,X_q)\\) に関して回帰した際の決定係数とした．\n\n\nこの際，最初の因子は \\(Y\\) を \\((1_n,X_j)\\) に関して回帰した際の係数 \\(\\widetilde{\\beta}_j\\) の分散に一致する．従って次の因子 \\[\n\\operatorname{VIF}_j:=1/(1-R_j^2)\n\\] は，他の説明変数 \\(X_1,\\cdots,X_{j-1},X_{j+1},\\cdots,X_q\\) を加えたことによる，\\(X_j\\) の推定係数の増大具合を表す．\nこれを 分散拡大係数 (VIF: Variance Inflation Factor) と呼ぶ．\n\n\n2.7 Bias-Variance Tradeoff\n一般に全ての関連する説明変数を入れた方が現実に近く，推定・予測精度は高くなると考えられる．\nしかし VIF の命題から，説明変数を増やすたびに OLS 推定量の分散は増大することがわかる．\nこのようなトレードオフを バイアス-分散トレードオフ (Bias-Variance Tradeoff) という．\n\n\n2.8 操作変数\n仮に \\(X_2\\) が内生性を持つとする： \\[\n\\operatorname{E}[\\epsilon|X_1]=0,\\qquad\\operatorname{E}[\\epsilon|X_2]\\ne0.\n\\]\n\n\n\n\n\n\nこのとき \\(U\\) であって次を満たすものを 操作変数 という：\n\n（広義）外生性 \\[\n\\mathrm{C}[U,\\epsilon]=0\n\\]\n関連性 \\[\n\\mathrm{C}[U,X_2]\\ne0\n\\]\n\n\n\n\n操作変数 \\(U\\) を用いれば，回帰モデル (1) の両辺の \\(U\\) との相関を考えると，外生性から \\[\n\\mathrm{C}[U,V]=\\mathrm{C}[U,X]\\widehat{\\beta}\n\\] が成り立ち，これを通じて \\(\\widehat{\\beta}\\) を推定できる．これを IV 推定量 という．\n\\(\\epsilon,\\epsilon_2\\) の相関を測ることで，内生性の強さを定量化することもできる (Chan and Tobias, 2020, p. 14)．\n\n\n2.9 ２段階 OLS\n以上の手続きは，ここまで議論してきた方法の特別な場合である．\n実際，\\(X_2\\) を \\(U\\) に関して回帰することを考える： \\[\nX_2=U\\delta+\\epsilon_2.\n\\]\nこの回帰により得る推定値 \\(\\widehat{X}_2=U\\widehat{\\delta}\\) は \\(\\epsilon\\) と相関を持たない．相関が取り除かれた成分を射影によって取り出していると見れる．\n続いて \\(X_2\\) を \\(\\widehat{X}_2\\) に取り替えて，\\(Y\\) に向かって回帰することで得る推定量を TSLS (Two-Stage Least Squares) 推定量 という．\n\\(U\\) が２値変数であるときは (Wald, 1940) 推定量ともいう．\\(X_2\\) の次元と \\(U\\) の次元が一致するとき，TSLS 推定量は IV 推定量と一致する．\n一般に TSLS も一致性と漸近正規性を持つ (B. E. Hansen, 2022, pp. 351–352)．"
  },
  {
    "objectID": "posts/2024/Stat/S-Regression.html",
    "href": "posts/2024/Stat/S-Regression.html",
    "title": "誤差分布に頑健な回帰分析",
    "section": "",
    "text": "自乗残差最小化の視点から\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Stat/S-Regression.html#概要",
    "href": "posts/2024/Stat/S-Regression.html#概要",
    "title": "誤差分布に頑健な回帰分析",
    "section": "",
    "text": "自乗残差最小化の視点から\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#序",
    "href": "posts/2024/Stat/Regression.html#序",
    "title": "セミパラメトリック重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n回帰モデルの点推定で最も広く用いられるアルゴリズムは OLS である： \\[\n\\widehat{\\beta}:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\lvert Y-Xb\\rvert^2_2.\n\\]\nその他の推定法は別稿で扱う：\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\nOLS は応答 \\(Y\\) を最もよく復元する推定量 \\(X\\widehat{\\beta}\\) を構成する．損失を \\(Y\\) のなす Euclid 空間 \\(\\mathbb{R}^n\\) 内の距離とした \\(M\\)-推定量である．\n尤度を使わない推定法であるが，Gauss-Markov モデル（均一誤差モデル） \\[\n\\operatorname{E}[\\epsilon|X]=0,\\qquad\\mathrm{C}[\\epsilon|X]=\\sigma^2I_n\n\\] に関しては，誤差 \\(\\epsilon\\) の分布に依らず，（セミパラメトリック）漸近有効性を持つ．しかも \\(\\epsilon_i\\) は i.i.d. とは限らない．\nその構成から予期される通り，OLS 推定量は極めて良い線型代数的な性質を持つ．実際， \\[\n\\widehat{\\beta}=(X^\\top X)^{-1}X^\\top Y\n\\] という表示をもち，\\(X\\widehat{\\beta}\\) は \\(Y\\) の \\(X\\) の列ベクトルの貼る空間への線型射影である．\nここでは重回帰モデルにおける OLS 推定量の性質を調べる．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#不均一分散",
    "href": "posts/2024/Stat/Regression.html#不均一分散",
    "title": "セミパラメトリック重回帰分析",
    "section": "3 不均一分散",
    "text": "3 不均一分散\n\n3.1 OLS の漸近正規性\nGuass-Markov モデルの線型モデルとしての最大の仮定は，均一の分散 \\(\\sigma^2\\) を仮定していたことである．\nしかしこの仮定を外しても，OLS 推定量は不偏性・一致性・漸近正規性を持つ（この順に追加の条件が厳しくなる）．\n\n\n\n\n\n\n命題7\n\n\n\n\\[\nY_i=X_i\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma_i^2),\n\\] に関して，計画行列 \\(X\\) は最大階数であるとする．このとき，次が成り立つ：\n\nOLS 推定量 \\(\\widehat{\\beta}\\) は不偏性を持つ．\n次の \\(B_n\\) が可逆な極限 \\(B_n\\to B\\) を持つとき，一致性も持つ： \\[\nB_n:=\\frac{1}{n}\\sum_{i=1}^nX_i^\\top X_i\\in\\mathrm{GL}_{p\\land n}(\\mathbb{R}).\n\\]\n２の条件に加えて，\\(x_i,\\epsilon_i\\) が３次のモーメントを持つとき，漸近正規性も成り立つ：\n\n\n\n\n\n\n\n\n\n反例\n\n\n\n\n\n一致性（と漸近正規性）の成立のために追加の条件が入っていることがわかる．不偏性さえあれば，「極限での不偏性」とも思える一致性が成り立って然るべきな気がする．\nこの追加の条件は，有限個の \\(Y_i\\) の説明にしか参加しない予測子を排除するためにある．\n例えばある分布 \\(P(\\mu,\\sigma^2)\\) に関して \\(Y_i\\overset{\\text{i.i.d.}}{\\sim}P(\\mu_i,\\sigma_i^2)\\) とする． \\[\nY_i=\\beta_1X_i^{(1)}+\\beta_2X_i^{(2)}+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}P(0,\\sigma_i^2)\n\\] \\[\nX_i^{(1)}=1_{\\left\\{1\\right\\}}(i),\\qquad X_i^{(2)}=1_{\\left\\{2,\\cdots,n\\right\\}}(i)\n\\] とモデルすると，計画行列はフルランクであるが，OLS 推定量は \\(\\widehat{\\beta}_1=\\epsilon_1\\sim P(0,\\sigma_1^2)\\) となる．\nこれは標本サイズ \\(n\\) に依らない値であり，\\(n\\to\\infty\\) を考えても \\(\\widehat{\\beta}_1\\) は一致性はもたず，漸近正規分布もしない．一方で \\(\\widehat{\\beta}_2\\) はする．\n\n\n\n\n\n3.2 EHW 頑健標準誤差\nこの漸近正規性に基づく分散推定量 \\[\n\\widehat{V}:=n^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^nX_i^\\top X_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\epsilon_i^2X_i^\\top X_i\\right)\\left(\\frac{1}{n}\\sum_{i=1}^nX_i^\\top X_i\\right)^{-1}\n\\] は，誤差分布が不均一な場合でも頑健な分散推定量となる．\nこれを計量経済学では (White, 1980) の推定量と呼ぶが，初めに提案したのは (Eicker, 1967) と (Huber, 1967) であるようである．\n\n\n3.3 有効性\nでは OLS は何を失うのか？\n\\(\\sigma^2\\) が不均一になった場合，観測によってノイズの大きさが違うわけである．\nしたがって特に情報量が大きい観測と，ノイズが大きくてあまり意味をなさない観測というものが相対的に出てくる．\nこれを峻別して適切に観測に重み付けることが必要である．\nこれができない OLS は有効性を失う．代わりに重み付けを行った OLS は有効性を持つ．\n\n\n\n\n\n\nBLUE\n\n\n\n既知の正定値行列 \\(\\Sigma\\) に関して， \\[\n\\operatorname{E}[\\epsilon]=0,\\qquad\\mathrm{C}[\\epsilon]=\\sigma^2\\Sigma,\n\\] を満たすとする．このとき，BLUE は次のように表せる： \\[\n\\widehat{\\beta}_\\Sigma=(X^\\top\\Sigma^{-1}X)^{-1}X^\\top\\Sigma^{-1}Y.\n\\]\n\n\n\n\n3.4 WLS\n第 3.1 節で考えた不均一分散の設定は \\[\n\\Sigma=\\mathrm{diag}(\\sigma_1^2,\\cdots,\\sigma_n^2)=:\\mathrm{diag}(w_1^{-1},\\cdots,w_n^{-1})\n\\] の場合に当たる．このときの BLUE は次の最適化条件でも特徴付けられる： \\[\n\\widehat{\\beta}_w:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}(Y-Xb)^\\top\\Sigma^{-1}(Y-Xb)=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\sum_{i=1}^nw_i\\lvert Y_i-X_ib\\rvert^2_2.\n\\tag{3}\\] これを WLS (Weighted Least Squares) 推定量 という．\n一般には解析を始める前に \\(\\Sigma\\) の形は未知であるから，これの推定から始める．その手続きを計量経済学では FGLS (Feasible Generalized Least Squares) と呼ぶ．\nこの重み付けの考え方は標本抽出の際にも重要であり，(Horvitz and Thompson, 1952) の逆確率重み付け法とも呼ばれる：\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.5 局所線型回帰\n局所線型回帰 (local linear regression) はカーネル法を用いてデータ点を適切に重み付けることで，非線型な回帰を達成する方法である．\n具体的には，基準点 \\(x_0\\) の近傍でのベストな線型近似 \\[\ny(x)=\\alpha+\\beta(x-x_0)\n\\] を得るために，あるカーネル \\(K\\) と帯域幅 \\(h&gt;0\\) を通じて \\[\n(\\widehat{\\alpha},\\widehat{\\beta}):=\\operatorname*{argmin}_{a,b}\\sum_{i=1}^nw_i\\biggl|y_i-a-b(x_i-x_0)\\biggr|^2_2,\\qquad w_i:=K\\left(\\frac{x_i-x_0}{h}\\right),\n\\] によって定める．\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.6 一般化線型モデルの解放\n一般化線型モデル \\[\n\\operatorname{E}[Y_i|X_i]=\\mu(X_i\\beta)\n\\] は基本的に分布が特定されたパラメトリックモデルである．これについても，EHW 頑健標準偏差推定量 3.2 に当たる，分布の誤特定に頑健な標準偏差推定量が存在する．\nその肝となる事実は，あらゆる関数 \\(\\widetilde{\\sigma}^2(x,\\beta)\\) に関して， \\[\n\\operatorname{E}\\left[\\sum_{i=1}^n\\frac{Y_i-\\mu(X_i\\beta)}{\\widetilde{\\sigma}^2(X_i,\\beta)}\\frac{\\partial \\mu(X_i\\beta)}{\\partial \\beta}\\right]=0\n\\tag{4}\\] が真値 \\(\\beta\\) に関して成り立ち続けることである．\nしたがって \\(\\widetilde{\\sigma}^2\\) を何らかの方法で決定し，これに関して式 (4) を通じた \\(M\\)-推定量 \\(\\widehat{\\beta}\\) が構成できる．\\(\\widetilde{\\sigma}^2(x_i,\\beta)=\\mathrm{V}[Y_i|X_i=x_i]\\) と正しく特定できた場合，これは最尤推定量になる．\n多くの場合 \\(\\widetilde{\\sigma}^2\\) は一般化線型モデルの仮定に基づいて算出するが，語特定されているものとしている場合が多く，作業分散 ともいう．\n\\((X_i,Y_i)\\) が独立同分布に従うとするとき，第 3.1 節のような漸近正規性の結果は，一般の \\(M\\)-推定量に関する次の結果から導かれる：\n\n\n\n\n\n\nRestricted Mean Model に対する \\(M\\)-推定8\n\n\n\n\\((X_i,Y_i)\\) が独立同分布に従うとする．このとき， \\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta)\\Rightarrow\\mathrm{N}(0,B^{-1}MB^{-1}),\n\\] \\[\nB:=\\operatorname{E}\\left[\\frac{1}{\\widetilde{\\sigma}^2(x,\\beta)}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta^\\top}\\right],\\qquad\nM:=\\operatorname{E}\\left[\\frac{\\sigma^2(x)}{\\widetilde{\\sigma}^2(x,\\beta)^2}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta^\\top}\\right].\n\\]\n\n\nこの結果を用いれば，指数型分布族などのパラメトリックモデルに依らずとも，漸近論に基礎付けられた点推定が達成できる．\nなお指数分布族の仮定の下で \\(\\widetilde{\\sigma}^2\\) がが正しく特定されていた場合，\\(B=M\\) は Fisher 情報行列となる．\n\n\n3.7 相関の考慮\nここまでの議論をまとめよう．OLS の漸近正規性 3.1 は，誤差分布が不均一であるばかりでなく，\\(Y_i\\) が相関を持つ場合（\\(\\Sigma\\) の非対角成分が非零の場合）でも成り立つ．\nGLS (Generalized Least Squares) はこの相関を持つ場合でもセミパラメトリック漸近最適性を達成する．\nこの結果を任意の逆リンク \\(\\mu\\) に関して \\[\n\\operatorname{E}[Y_i|X_i]=\\mu(X_i\\beta)\n\\] という非線型な回帰モデルにも拡張することを考えたいが，前節ではまだ \\(Y_i\\) が i.i.d. であるという仮定を置いていた．\n最後にこの仮定を取り払い，一般の誤差分布 \\(\\mathrm{C}[\\epsilon|X]=\\Sigma\\) を考えたい．\nこのために開発されたのが 一般化推定方程式 (GEE: Generalized Estimating Equations) (Liang and Zeger, 1986) である．\n\n\n3.8 一般化推定方程式\n\\((X_i,Y_i)\\) を i.i.d. とした場合の推定方程式 (4) を拡張した推定方程式 \\[\n\\sum_{i=1}^n\\frac{\\partial \\mu(X_i\\beta)}{\\partial \\beta}\\widetilde{\\Sigma}^{-1}(X_i,\\beta)\\biggr(Y_i-\\mu(X_i\\beta)\\biggl)=0\n\\tag{5}\\] を一般化推定方程式といい，\\(\\widetilde{\\Sigma}(X_i,\\beta)\\) を 作業共分散行列 という．\nこの式は今までで最も一般的な形をしており，最適化条件 (3) で推定を実行する GLS に対して，１次の最適性条件に基づいて導出する方法ということができる．それ故，逆リンク \\(\\mu\\) の一般性も許容できている．\n実際，一般化推定方程式 (5) は，最適化条件 (3) を \\(b\\) に関して微分して得る一次の最適性条件に見える．\n一般化推定方程式 (5) による推定も，i.i.d. とは限らない場合の \\(M\\)-推定の理論から，漸近正規性が導ける．この漸近論から得られる EHW 推定量の一般化は，\\(\\mu\\) の特定さえ正しければ，\\(\\widetilde{\\Sigma}\\) の誤特定に頑健な分散推定量となる (Liang and Zeger, 1986), (Altonji and Segal, 1996)．\n\n\n3.9 GEE の仮定\nしかし GEE には重要な仮定 \\[\n\\operatorname{E}[Y_{it}|X_i]=\\operatorname{E}[Y_{it}|X_{it}],\\qquad t\\in[n_i],\n\\] が存在する．これは \\((X_{it},Y_{it})\\) が状態空間モデルに従うことを意味する．\nしかし \\(\\widetilde{\\Sigma}\\) の非対角成分が零になる，独立作業共分散行列を用いた場合は，この仮定が成り立たない場合でも，\\(\\mu\\) の特定が正しければやはり一致性が成り立つが，推定量の分散は少し膨らむ．\nまた関数関係 \\(\\mu\\) が \\(t\\in[n_i]\\) に依存しないという仮定も含まれている．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html",
    "href": "posts/2024/Survey/Survey3.html",
    "title": "ベイズデータ解析３",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#関連記事",
    "href": "posts/2024/Survey/Survey3.html#関連記事",
    "title": "ベイズデータ解析３",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#有限標本論の概要",
    "href": "posts/2024/Survey/Survey3.html#有限標本論の概要",
    "title": "ベイズデータ解析３",
    "section": "1 有限標本論の概要",
    "text": "1 有限標本論の概要\n\n1.1 設定\n\\([N]\\) を母集団とする．\n\\([N]\\) の部分集合の全体 \\(P([N])\\) 上の確率分布を 抽出計画 (sampling design) といい，ある既知の抽出分布に従って得られる標本 \\(S\\subset[N]\\) を 確率標本 (probability sample) という．日本語では 無作為抽出標本 などとも呼ばれる．\n\n\n\n\n\n\n抽出計画の例\n\n\n\n\n\n抽出計画 \\(\\mathcal{L}[S]\\) には，\n\n単純無作為抽出 (SRS: Simple Random Sampling)\n系統無作為抽出 (Systematic Random Sampling)\n層別抽出 (Stratified Sampling)：母集団を層別し，各層の間では独立な抽出を行う．層ごとに抽出計画は異なっても良い．条件付きランダム化 (conditional randomization) ともいう (Section 2.2 Hernán and Robins, 2020, p. 17)．\nクラスター抽出 (Cluster Random Sampling)：クラスターがまずランダム抽出され，そのクラスター内の全構成員が標本に加わる．クラスターのことを PSU (Primary Sampling Unit) ともいう．クラスター内でもランダム抽出が行われた場合，２段階クラスターサンプリング という．\n\nなどの方法が存在する．\n例えば，日本の国勢調査は２段階の層別抽出である．\n\n\n\n確率標本 \\(S\\) では（１次の）包含確率 \\[\n\\pi_i:=\\operatorname{P}[i\\in S]=\\operatorname{P}[I=1],\\qquad I:=1_{i\\in S}.\n\\] が定まる．\n\n\n\n\n\n\n狭義の「確率標本」\n\n\n\n\n\n先ほど，\\(\\mathcal{P}([N])\\) 上の確率変数を確率標本と呼ぶとしたが，正確に \\(S\\) が 確率標本 と呼ばれるためには，\\(\\pi_i&gt;0\\) が母集団 \\(i\\in[N]\\) の全域で成り立つことが必要である (Kim, 2024, p. 12)．\n\n\n\n\n\n1.2 Horvitz-Thompson 推定量\n確率標本 \\(S\\in\\mathcal{L}(\\Omega;\\mathcal{P}([N]))\\) に対しては，ある量 \\(y\\) についての母集団の総和 \\[\nY:=\\sum_{i=1}^Ny_i\n\\] が \\[\n\\widehat{Y}_\\mathrm{HT}:=\\sum_{i\\in S}\\frac{y_i}{\\pi_i}\n\\] により不偏推定できる．\n\\(\\widehat{Y}_\\mathrm{HT}\\) は (Horvitz and Thompson, 1952) 推定量と呼ばれる．1\n\n\n\n\n\n\n(Sen, 1953)-(Yates and Grundy, 1953)\n\n\n\nHorvitz-Thompson 推定量の分散は次で与えられる：\n\\[\n\\mathrm{V}[\\widehat{Y}_\\mathrm{HT}]=\\sum_{i,j=1}^N\\biggr(\\pi_{ij}-\\pi_i\\pi_j\\biggl)\\frac{y_iy_j}{\\pi_i\\pi_j}.\n\\] ただし， \\[\n\\pi_{ij}:=\\operatorname{P}[i\\in S,j\\in S]=\\operatorname{P}[I=1=J].\n\\] を２次の包含確率という．\n\n\nHorvitz-Thompson 推定量の要点には，「計画した欠損ならば，重みづけによって不偏推定量を得ることができる」という点にある．\nそこで抽出計画が不明な場合もこれを推定し，バイアスを補正しようとするアプローチを傾向スコアの方法，または 擬似ランダム化 (pseudo-randomization) の方法という．\n\n\n1.3 「校正」：効率の改善に向けて\nHT 推定量は確率標本 \\(S\\) の分布，すなわち抽出計画に依らずに不偏性を持つ．\nこれを計画不偏性 (design-unbiasedness) というが，この性質を持つ線型な推定量は HT に限られる．\nしかし，HT 推定量はいつでも分散が最小というわけではない．\n計画不偏性は bias-variance trade-off の観点からは欠点でもあり，それゆえ抽出計画に関する情報を用いて分散を低減することも考えられる．\n特に，HT 推定量の荷重 \\((\\pi_i^{-1})\\) を，補助変数 \\(x_i\\) に関する 外部一致性 \\[\n\\sum_{i\\in S}w_ix_i=\\overline{x}\n\\] を保ちながら新しいもの \\((w_i)\\) に変更するものが多く考えられた．\n\n\n\n\n\n\n「外部一致性」の別名\n\n\n\n有限標本論は普遍的な統計推測の基礎であると言える．\n実際，この外部一致性の条件は多くの分野で考慮されており，種々の名前が与えられている．\n\n有限標本論：校正条件 (calibration condition / benchmarking property) （第 2.3 節）\n欠測データ解析：共変量バランシング (covariate balancing) (Imai and Ratkovic, 2014)\n機械学習（継続学習）：共変量シフト (covariate shift) (Shimodaira, 2000)\n\n\n\nこのアプローチを 荷重校正 (calibration weighting) という．\n次章にてこれ以降，種々の荷重校正推定量を紹介する．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#回帰推定量",
    "href": "posts/2024/Survey/Survey3.html#回帰推定量",
    "title": "ベイズデータ解析３",
    "section": "2 回帰推定量",
    "text": "2 回帰推定量\n\n2.1 はじめに\n前述の通り，補助変数 \\(x\\) が母集団上で知られている場合に，ここから抽出計画に対する追加情報を抽出して推定量に組み込むことで，計画的に欠測させられたデータ（＝確率標本）に対する不偏推定量 (Horvitz and Thompson, 1952) （以降 HT 推定量という）の効率を改善することを考える．\n以降，補助変数 \\(x\\in\\mathbb{R}^p\\) は母集団上で既知であるとし，その総和を \\[\nX:=\\sum_{i\\in[N]}x_i\n\\] で表す．\n\n\n2.2 比による校正\n補助変数の次元が \\(p=1\\) のとき，最も安直には \\(X\\) の HT 推定量から，真の値 \\(X\\) との「ズレ方」を用いて，\\(Y\\) の推定量を「校正」することができる．\n\\[\n\\widehat{Y}_{\\mathrm{R}}:=\\widehat{Y}_\\mathrm{HT}\\frac{X}{\\widehat{X}_\\mathrm{HT}}\n\\] とできるだろう．\nこの推定量は ratio estimator などと呼ばれ，性能の代わりにバイアスが生じてしまう．\n一般に，\\(X,Y\\) が正の相関を持つとき大きな分散低減が得られる (Deng and Wu, 1987), (Kim, 2024, p. 92)．\n\\(x_i=1\\) と取った場合を Hajék 推定量ともいう．Hajék 推定量が HT 推定量よりも推奨される状況が (Särndal et al., 1992, p. 182) にリストされている．\nこの推定量は昔は計算の簡単さから使われていたが，一般の次の回帰推定量の方が MSE が小さいことが知られている (Deng and Wu, 1987)．\n\n\n2.3 回帰推定量\n超母集団模型 \\[\nY=X^\\top\\beta+\\epsilon,\\qquad\\epsilon\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2)\n\\] を想定し，得られている標本のみから \\(\\widehat{\\beta}\\) を推定する．こうして得られる \\[\n\\widehat{y}_i:=x_i^\\top\\widehat{\\beta},\\qquad\\widehat{\\beta}:=\\left(\\sum_{i\\in S}\\pi_i^{-1}x_ix_i^\\top\\right)^{-1}\\sum_{i\\in S}\\pi_i^{-1}x_iy_i\n\\] の総和が，\\(Y\\) に対する 回帰推定量 (regression estimator) と呼ばれる．2\nこれは \\((y_i)\\in\\mathbb{R}^n\\) に関する線型推定量になっている．加えて，外部一致性 \\[\n\\sum_{i\\in S}w_ix_i=\\overline{x}\n\\tag{1}\\] を満たす荷重 \\[\nw_i:=\\overline{X}^\\top\\left(\\sum_{i\\in S}\\pi_i^{-1}x_ix_i^\\top\\right)^{-1}\\pi_i^{-1}x_i\n\\] に関して， \\[\n\\widehat{Y}_{\\mathrm{reg}}=\\sum_{i\\in S}w_iy_i\n\\] という形の線型推定量になっている．\n式 (1) を 外部一致性 (external consistency)，または 校正条件 (calibration / benchmarking property) (Deville and Särndal, 1992) という．\n回帰推定量は \\(X,Y\\) の関係に依らず一致性を持ち，\\(X,Y\\) の間の相関の絶対値が大きいほど分散低減効果が高くなる (Kim, 2024, p. 95)．3\n\n\n2.4 事後層別化\n事後層別化 (post-stratification / stratification after selection) は標本抽出の結果を見て標本を層別化する手法であるが，回帰推定量の特別な場合と見れる．\n母集団が \\(G\\) 個の層に分けられるとする：\\(N=N_1+\\cdots+N_G\\)．\nこのとき，\\(i\\in[N]\\) 番目の単位が層 \\(g\\in[G]\\) に属するかどうかの指示変数 \\(x_{ig}\\in2\\) のベクトル \\(x_i:=(x_{i1},\\cdots,x_{iG})^\\top\\in2^G\\) に関する回帰推定量 \\[\\begin{align*}\n  \\widehat{Y}_{\\mathrm{post}}&:=\\sum_{i=1}^Nx_i^\\top\\left(\\sum_{i\\in S}\\pi_i^{-1}x_ix_i^\\top\\right)^{-1}\\sum_{i\\in S}\\pi_i^{-1}x_iy_i\\\\\n  &=\\sum_{g=1}^G\\sum_{i\\in S_g}\\pi_i^{-1}\\frac{N_g}{\\widehat{N}_g}y_i,\\qquad\\widehat{N}_g:=\\sum_{i\\in S}\\pi_i^{-1}x_{ig}.\n\\end{align*}\\] を事後層別化推定量という．\nMRP (Multilevel Regression and Post-stratification) (Gelman and Little, 1997), (Gelman, 2014) は事後層別化の階層モデル・縮小推定版である．\n\n\n2.5 ランキング法／繰り返し比例的フィッティング法\n(Deming and Stephan, 1940) では 1940 年の国勢調査の結果の分析を考えていた．\n特に，基本的な情報は全数調査されるが，詳細な情報は標本調査でしか得られない状況下で，母集団の \\(I\\times J\\) 分割表の各セル \\(U_{ij}\\) の値 \\(N_{ij}\\) の推定を考えていた．\nただし，周辺和 \\(N_{i-},N_{-j}\\) は全数調査で得られているとする．\nこのとき，\\(N_{ij}\\) の推定量の候補として \\[\n\\frac{n_{ij}}{n_{i-}}N_i,\\quad\\frac{n_{ij}}{n_{-j}}N_{-j},\\quad\\frac{n_{ij}}{n}N\n\\] の３つが考えられる．３番目が良いと考えるかもしれないが，その結果得られる分割表は周辺和を保存しない．\nこの問題は次のような形でも現れる：指示変数 \\[\nx_k=(x_{1-k},\\cdots,x_{I-k},x_{-1k},\\cdots,x_{-Jk}),\\qquad x_{ijk}:=1_{U_{ij}}(k),\n\\] に基づく事後層別化推定量 \\[\n\\widehat{Y}_{\\mathrm{post}}=\\sum_{i\\in S}\\pi_i^{-1}g_i(S)y_i,\\qquad g_i(S):=\\left(\\sum_{k=1}^Nx_k\\right)^\\top\\left(\\sum_{k\\in S}\\pi_k^{-1}x_kx_k^\\top\\right)^{-1}x_i\n\\] を考えたいが，これが \\(\\operatorname{rank}\\left(\\sum_{k\\in S}\\pi_k^{-1}x_kx_k^\\top\\right)=I+J-1\\) であるため，一意な表示を持たない．\n\\(g_i(S)\\) の候補のうち，次を満たす \\(g_i\\) を選ぶことが目標である： \\[\n\\sum_{k\\in S}\\frac{g_k}{\\pi_k}x_{i-k}=\\sum_{k=1}^Nx_{i-k}=N_{i-},\n\\tag{2}\\] \\[\n\\sum_{k\\in S}\\frac{g_k}{\\pi_k}x_{-jk}=\\sum_{k=1}^Nx_{-jk}=N_{-j}.\n\\tag{3}\\]\n\n\n\n\n\n\n(Iterative Proportional Fitting / Ranking algorithm Deming and Stephan, 1940)\n\n\n\n\n\\(g^{(0)}_k\\gets1\\) と初期化する．\n\\(x_{i-k}=1\\) すなわち \\(k\\in U_{i-}\\) であるとき， \\[\n  g^{(t+1)}_k\\gets g_k^{(t)}\\frac{\\sum_{k=1}^Nx_{i-k}}{\\sum_{k\\in S}\\frac{g^{(t)}_k}{\\pi_k}x_{i-k}}.\n  \\] これにより条件 (2) が満たされる．\n\\(z_{-jk}=1\\) すなわち \\(k\\in U_{-j}\\) であるとき， \\[\n  g^{(t+2)}_k\\gets g_k^{(t+1)}\\frac{\\sum_{k=1}^Nx_{-jk}}{\\sum_{k\\in S}\\frac{g^{(t+1)}_k}{\\pi_k}x_{-jk}}.\n  \\] これにより条件 (3) が満たされる．\n収束するまで繰り返す．\n\n\n\nこれは特定の目的関数を最小化することに等しい．(Deming and Stephan, 1940, p. 428), (Zieschang, 1990), (Jean-Claude Deville and Sautory, 1993) も参照．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#荷重校正推定量",
    "href": "posts/2024/Survey/Survey3.html#荷重校正推定量",
    "title": "ベイズデータ解析３",
    "section": "3 荷重校正推定量",
    "text": "3 荷重校正推定量\n\n3.1 はじめに\n回帰推定量は \\(X\\) から \\(Y\\) に関する情報を抽出することで，HT 推定量の効率を改善することができる方法である．\nしかし，HT のもう一つの魅力的な性質であった 計画一致性 (design consistency) が失われている．\n回帰推定量の性質である 外部一致性 (external consistency) を保ちながら，別の解を見つけることで，回帰推定量を一般化する形で計画一致性を持つ効率的な推定量を構成することを考える．\n実はこの方法は，モデリングの観点からは \\(X,Y\\) の間のモデルを，標本レベルから母集団レベルに一般化することに相当する．こうして考えられる超母集団モデルを 一般化回帰モデル (GREG: Generalized Regression) という．\nこのような方法で HT 推定量を改善した計画一致性を持つ推定量を model-assisted estimator，特に特定の制約下最適化問題の解として与えられるものを 校正推定量 (calibrated estimator) という．\n校正推定量は計画一致性を持つために，傾向スコアの推定に成功していれば不偏性が保証される．この性質は二重頑健推定量の構成の基礎となる．\n\n\n3.2 差分推定量\n補助的な量 \\(y_i^{(0)}\\) が母集団全体で観測されている場合， \\[\n\\widehat{Y}_{\\mathrm{diff}}:=\\sum_{i=1}^Ny_i^{(0)}+\\sum_{i\\in S}\\pi_i^{-1}\\left(y_i-y_i^{(0)}\\right)\n\\] は 差分推定量 (difference estimator) と呼ばれる．\nHT 推定量同様不偏であるが，分散の値は変化し，特に \\(y_i^{(0)}\\) が \\(y_i\\) の良い近似であるほど分散が小さくなる (Kim, 2024, p. 99)．\nこの \\(y_i\\) の proxy とも言える量 \\(y_i^{(0)}\\) を，他の共変量 \\(x_i\\) から回帰により構成することで，回帰推定量（第 2.3 節）よりも複雑な \\(x_i,y_i\\) 関係もうまく取り込んだ分散低減が可能になる．\nこのように（暗黙裡にでも）モデルを用いており，加えて モデルの特定が成功しているかに依らず HT 推定量を改善できる 方法を model-assisted estimation といい，校正推定量の基本的な考え方である．\n\n\n3.3 一般化回帰モデルと射影推定量\nまず母集団 \\([N]\\) に応用 \\(Y\\) のモデルを当てはめる： \\[\ny_i=x_i^\\top\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,c_i(x_i)\\sigma^2).\n\\tag{4}\\] このように母集団に置かれるモデルを 超母集団モデル (superpopulation model) (Isaki and Fuller, 1982) という．\n特に式 (4) の Gauss-Markov 型の超母集団モデルを 一般化回帰モデル (GREG: Generalized Regression) ともいう．\nこれを解いて得る推定量 \\(\\widehat{y}_i=x_i^\\top\\widehat{\\beta}_c\\) の総和として得られる推定量 \\[\n\\widehat{Y}_{\\mathrm{P}}:=\\sum_{i=1}^N\\widehat{y}_i\n\\] を（モデルベースの） 射影推定量 (projection estimator) という．\n射影推定量は計画一致性を持つとは限らない．\n仮に GREG モデルで \\[\n\\frac{c_i}{\\pi_i}\\parallel x_i\n\\] が成り立つならば，内部バイアス校正 (IBC: Internally Biased Calibration) (Firth and Bennett, 1998) 条件 \\[\n\\sum_{i\\in S}\\frac{1}{\\pi_i}(y_i-\\widehat{y}_i)=0\n\\] が成り立つ．\nこの IBC が，射影推定量が抽出計画に依らずに一致性を持つための十分条件である (補題9.1 Kim, 2024, p. 100)．\n\n\n3.4 一般化最小二乗法 (GLS)\n当然 GREG モデルが IBC 条件を満たすとは限らない．\nそのような場合でも計画一致性を持つような推定量を考えたい．実は， \\[\n\\widehat{Y}_{\\mathrm{GREG}}:=\\widehat{Y}_\\mathrm{HT}+\\biggr(X-\\widehat{X}_\\mathrm{HT}\\biggl)^\\top\\widehat{\\beta}_c\n\\] は計画一致性を持つ．\nこれは 一般化回帰推定量 (GREG: Generalized Regression Estimator) または計量経済学において GLS (Generalized Least Squares) (Aitken, 1936) と呼ばれる．4\n一般化回帰推定量は次の最適化による特徴付けがある： \\[\n\\widehat{Y}_{\\mathrm{GREG}}=\\sum_{i\\in S}\\widehat{\\omega}_iy_i,\\qquad\\widehat{\\omega}_i:=\\pi_i^{-1}+\\left(X-\\widehat{X}_\\mathrm{HT}\\right)^\\top\\left(\\sum_{i\\in S}\\frac{1}{c_i}x_ix_i^\\top\\right)^{-1}\\frac{x_i}{c_i}.\n\\] この荷重 \\(\\widehat{\\omega}_i\\) は，校正条件 (calibration constraint) （式 (1) との違いに注意）を満たすものの中で \\[\nQ(\\omega):=\\sum_{i\\in S}(\\omega_i-d_i)^2c_i,\\qquad d_i:=\\pi_i^{-1},\\quad\\operatorname{subject to}\\sum_{i\\in S}\\omega_ix_i=\\sum_{i=1}^Nx_i.\n\\] を最小にするものとも特徴付けられる (Kim, 2024, p. 102)．\n特に，\\(\\widehat{w}_i\\xrightarrow[n\\to\\infty]{\\mathrm{p}}d_i\\)．\n\n\n3.5 校正推定量\n一般に，校正条件制約を満たす \\((\\omega_i)\\) のうち，凸関数 \\(G\\) が定める目的関数 \\[\nQ(\\omega):=\\sum_{i\\in S}d_iG\\left(\\frac{\\omega_i}{d_i}\\right)c_i\n\\] を最小にするものを 校正荷重 (calibration weight)，校正荷重に関する線型推定量を 校正推定量 (calibration estimator) という (Deville and Särndal, 1992), (Kim, 2024, p. 103)．\nほとんどの校正推定量は漸近的に GREG 推定量に一致する．\n一般に，有限母集団に対する確率標本からの一様最小分散不偏推定量 (UMVUE) は存在しない (Godambe and Joshi, 1965) が，GREG 推定量は「期待漸近分散」の下界を達成する (Isaki and Fuller, 1982)．\n\n\n3.6 最適校正推定量\n特に， \\[\nQ(\\omega)=\\sum_{i\\in S}\\omega_i^2c_i\n\\] を最小化するものは 最適校正推定量 (optimal calibrated estimator) と呼ばれる (Kim, 2024, p. 110)．\nこれはモデルの視点からは \\(x\\) を拡張して人工的に IBC 条件を満たすようにした射影推定量（第 3.3 節）とも見れる．\n最適校正推定量は超母集団モデル (4) が誤特定されている場合に GREG 推定量より良い性能を示す (Kim, 2024, p. 112)．\nGREG モデルより一般的な超母集団モデルに対しての同様の手続きは モデル校正 (model calibration) (Wu and Sitter, 2001) と呼ばれている．この方法では \\(X,Y\\) の関係を推定し，\\(Y\\) の線型推定量を \\(m(X)\\) の形で構成してから，最適構成推定量の議論に還元する．\n\n\n3.7 一般化エントロピー法\n最適構成推定量の構成に倣い， \\[\nQ(\\omega):=\\sum_{i\\in S}G(\\omega_i)c_i\\qquad\\operatorname{subject to}\\sum_{i\\in S}\\omega_ig(d_i)c_i=\\sum_{i=1}^Ng(d_i)c_i\n\\] の最小化により校正荷重を構成する方法を 一般化エントロピー法 (generalized entropy method) (Kwon et al., 2024) という．\nこれは目的関数には計画荷重 \\(d_i=\\pi_i^{-1}\\) が入っていないが，制約条件に入っていることで計画一致性を達成している．\n超母集団モデルである GREG モデルが正しく特定されているならば (Godambe and Joshi, 1965) の下界を達成するが，そうでなくとも一致性は保たれる上に，一般の校正推定量（第 3.5 節）よりも分散は小さいである (Kwon et al., 2024)．5"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#欠測データの扱い",
    "href": "posts/2024/Survey/Survey3.html#欠測データの扱い",
    "title": "ベイズデータ解析３",
    "section": "4 欠測データの扱い",
    "text": "4 欠測データの扱い\n\n4.1 はじめに\n観測単位が欠測している場合 (unit nonresponse)，call-back / follow-up 調査を行うか，それができない場合は次の２つの対処が可能である：\n\n\n\n\n\n\n単位欠測の扱い\n\n\n\n\n欠測メカニズムを抑える共変量は見えている場合（MAR 条件），傾向スコア推定量が利用可能（第 4.2 節）．これは欠測メカニズムのモデリングに基づく．\n一般の校正推定量に対しても，\n\n\n\n単位欠測の場合は，２段階の標本抽出と状況が似ているのである．さらには，非確率標本（調査観察データ，ビッグデータなど）の扱いとも似通う．これについては次稿も参照．\n一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる．6\n現状は多重代入法（第 5.2 節）が主流であると言える (Buuren, 2018)．\n\n\n4.2 傾向スコア推定量\n標本の観測 \\(Y_i\\) は，\\(\\delta_i=0\\) のとき欠損しているとする．\n\n4.2.1 MAR 条件：欠測のメカニズムを抑える共変量が観測できている\n加えて，標本全体についてある変数 \\(X\\) が観測できており，これについて次の条件が成り立つとする：\n\n\n\n\n\n\n(MAR condition Rubin, 1976)7\n\n\n\n欠測の指示変数 \\(\\delta\\) について， \\[\n\\operatorname{P}[\\delta=1|X,Y]=\\operatorname{P}[\\delta=1|X]=:p(X)\n\\] が成り立つ．\n\n\nこれは条件付き独立性 \\(\\delta\\perp\\!\\!\\!\\perp Y\\mid X\\) よりも弱い条件で，MAR (Missing At Random) の条件と呼ばれる．8\n\n\n4.2.2 欠測メカニズムの推定\n欠測確率 \\(p(x):=\\operatorname{P}[\\delta=1|X=x]\\) にノンパラメトリックなモデル \\(p_\\phi(x)\\) を課したとする．\nこのとき，パラメータ \\(\\phi\\) は擬似最尤推定量 \\(\\widehat{\\phi}\\) により一致推定をすることができる．\n\n\n4.2.3 傾向スコア推定量\n仮に母平均 \\[\nY:=\\sum_{i=1}^Ny_i\n\\] が推定対象であったとしよう．\nこのとき，推定された \\(\\widehat{\\phi}\\) を元に，次の推定量が構成できる：\n\\[\n\\widehat{Y}_\\mathrm{PS}:=\\sum_{i\\in\\delta^{-1}(1)}\\frac{1}{\\pi_i}\\frac{y_i}{p_{\\widehat{\\phi}}(x_i)}.\n\\]\n\n\n\n\n\n\n命題（傾向スコア推定量の一致性）9\n\n\n\n欠測確率 \\(p\\) のモデル \\(p_\\phi(x)\\) の特定に成功しているとき，ある正則性に関する条件が満たされる限り，傾向スコア推定量 \\(\\widehat{Y}_\\mathrm{PS}\\) は一致推定量に \\(n^{-1}\\) のオーダーで漸近する．\n\n\n\n\n\n4.3 校正推定量\nある校正荷重 \\((d_i)\\) に関して，計画一致性を持つ推定量 \\[\n\\widehat{Y}=\\sum_{i\\in S}d_iy_i\n\\] を考えているが，単位欠測により特定の \\(y_i\\) が得られず，計算できないものとする．\nこの場合でも，応答があった部分標本 \\[\nS_R:=\\delta^{-1}(1)\n\\] 上の校正推定量 \\[\n\\widehat{Y}_\\omega:=\\sum_{i\\in S_R}\\omega_iy_i\n\\] であって，欠測メカニズム \\(p(x)\\) の特定か，または超母集団モデル \\[\ny_i=x_i^\\top\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,c_i\\sigma^2)\n\\] の特定に成功すれば一致性を持つ，二重頑健なものを構成できる (Kim and Haziza, 2014)．\n\n\n4.4 代入法とその不偏性条件\n項目非反応がある場合，代入値を \\(y_i^*\\) として \\[\n\\widehat{Y}_{\\mathrm{I}}:=\\sum_{i\\in S}\\frac{1}{\\pi_i}\\biggr(\\delta_iy_i+(1-\\delta_i)y_i^*\\biggl)\n\\] による推定が試みられる．\n代入 \\(y_i^*\\) を行うことでリストワイズの削除をするよりも推定の効率を上げることができる．\n\n\n\n\n\n\n(代入推定量の不偏性 Kim, 2024, p. 162)\n\n\n\n\\[\n\\operatorname{E}[Y^*|\\delta=0]=\\operatorname{E}[Y|\\delta=1]\n\\] が成り立つならば，\\(\\widehat{Y}_\\mathrm{I}\\) は不偏推定量である．\n\n\nこの条件は，標本内で MAR 条件（第 4.2.1 節）が成り立つとき： \\[\nY|(X,\\delta=1)=Y|(X,\\delta=0),\n\\tag{5}\\] \\(Y^*\\) を \\(Y|(X,\\delta)\\) からのサンプリングで代入すれば達成される．\nさらに強い条件 \\[\n\\delta\\perp\\!\\!\\!\\perp Y\\mid X\n\\] が成り立つとき，標本内の MAR 条件が成り立つ．\n換言すれば代入法において，欠測の原因 \\(X\\) を突き止め，欠測したグループにおける \\(Y\\) の値 \\(Y|(X,\\delta=1)\\) にモデル (outcome model) を立て，そこからサンプリングをすることを目指す．\n\n\n4.5 回帰による代入\n仮に共変量 \\(X\\) が \\(Y\\) と強い相関を持つとする．このように線型回帰模型を背後に想定することが適切な場合は，よく次のような手続きで代入がされる．\nまず共変量により母集団を \\([N]=N_1+\\cdots+N_G\\) 個に層別化し，それぞれの層で \\[\nY_i=X_i^\\top\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2)\n\\tag{6}\\] というセミパラメトリック回帰モデルを考える．\n次に推定されたモデルを用いて，\\(\\epsilon_i^*\\sim(0,\\sigma^2)\\) を残差 \\[\n\\widehat{\\epsilon}_i:=y_i-x_i^\\top\\widehat{\\beta}\n\\] の分布から（リ）サンプリングし， \\[\ny_i^*\\gets x_i^\\top\\widehat{\\beta}+\\epsilon_i^*\n\\] を代入値とする．\n以上の手続きは 確率的回帰代入法 (stochastic regression imputation) と呼ばれる．平均を代入する場合は単に回帰代入法または条件付き平均代入法 (conditional mean imputation) という．\n\\(Y\\) と強い相関を持つ補助変数 \\(X\\) がいつでも見つかるとは限らない．\nその場合は Gauss-Markov モデル (6) を一般の統計モデルに一般化すれば良い．\n\n\n4.6 マッチングによる代入\n層の中の他のセルをランダムに選んでその値を代入する hot deck imputation や，セルの加重平均を代入する fractional hot deck も同様の考え方に基づく (Fuller and Kim, 2005)．\nこのような手法は マッチング と呼ばれ，カーネル法と関連が深い (Cheng, 1994)．加重平均は対象のセルとの関連度を「距離」によって測り，距離を計算するのに使われる変数は キー ともいう (高井啓二 et al., 2016, p. 110)．傾向スコアマッチングでは傾向スコアがキーである．\n最も単純には同一データセット内の最も似ている単位を持ち出してその値を代入するのがマッチングであるが，最も洗練された方法としては類似度に依存して関連度を自動的に重みづけて，データセット全体で加重平均をとっても良いわけである．\n他の標本の値を参考にする場合は cold deck imputation という．\nなどの Least squares method も同様の考え方に基づく (Little, 1992)．\n\n\n4.7 母集団モデルによる代入法\n一方で，母集団上での \\(Y,X\\) の関係についてモデルを立てて \\(Y|X\\) からサンプリングをすることも考えられる．\n\n\n\n\n\n\n注（無情報サンプリング条件）\n\n\n\n\n\n母集団の分布と標本の分布が一致するとき，無情報サンプリング (noninformative sampling) が実施されたという．そうでない場合は informative sampling という．\nサンプリングが無情報であるための十分条件には \\[\n\\operatorname{P}[I=1|X,Y]=\\operatorname{P}[I=1|X]\n\\] が挙げられる．(Sugden and Smith, 1984) はこれを無情報サンプリング条件という．\nこの下では母集団のモデルと標本のモデルとは一致するが，一般にはこの２つは厳密に峻別しなければ混乱の源である．\n\n\n\n標本内の MAR 条件 (5) だけでなく，母集団上で MAR 条件が成り立つ場合は，\\(Y|X\\) の尤度を \\(f_\\theta(y|x)\\) としてモデリングをし，これを \\[\n\\ell(\\theta):=\\sum_{i\\in S}w_i\\delta_i\\log f_\\theta(y_i|x_i)\n\\] の最大化によって \\(M\\)-推定することが考えられる．10\nただし，\\(w_i\\) は \\(Y\\) の計画一致性を持つ校正推定量を定める校正荷重であるとする．\\(w_i\\) の存在は標本と母集団のズレに起因する．\n最終的に学習されたモデル \\(f_\\theta(y|x_i)\\) からのサンプリングによって代入値 \\(y_i^*\\) を生成する．\nこのモデル \\(f_\\theta(y|x_i)\\) を当てはまりの度合いを見ながらベイズ推論によって得る方法もよく取られるようになっている (C. K. Enders et al., 2020)．\n母集団上の MAR 条件が成り立たない場合は \\(Y|(X,\\delta=0)\\) のモデリングを考える必要がある．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#多重代入法",
    "href": "posts/2024/Survey/Survey3.html#多重代入法",
    "title": "ベイズデータ解析３",
    "section": "5 多重代入法",
    "text": "5 多重代入法\n\n5.1 はじめに\nベイズの観点からは，欠測データとパラメータとは違いがない (Chapter 18 Gelman et al., 2014, p. 449)．\nベイズ事後分布は欠測データとパラメータの上に同時に定まり，欠測データに関して積分をすることで最終的な推論が実行される．\nこれを模倣する形で提案されたのが 多重代入法 (MI: Multiple Imputation) (Rubin, 1978), (Rubin, 1987) である．\n多重代入法ではベイズ事後分布から補完値を複数生成し，複数の擬似完全データに関して同じ解析を実行し，最後に結果を平均する．\n擬似完全データに対する解析が一貫したベイズ推論であった場合，この一連の手続きによって（近似的な）ベイズ推論が実行されることになる．\nしかしデータの補完とその後の擬似完全データ解析は 融和性 (congeniality) を保つ限り別の方法を用いても良いように拡張された (Meng, 1994)．\nこのことにより多重代入法は広く使われるようになっている．\n\n\n5.2 多重代入法\n多重代入法では，モデルベースの代入法（第 4.7 節）をさらに推し進める．\n本来の推定量 \\[\n\\widehat{Y}=\\sum_{i\\in S}w_iy_i\n\\] を代入推定量 \\[\n\\widehat{Y}_\\mathrm{I}=\\sum_{i\\in S}w_i\\biggr(\\delta_iy_i+(1-\\delta_i)y_i^*\\biggl),\\qquad y_i^*\\sim f_\\theta(y_i|x_i)\n\\] で模倣する際，ベイズ事後予測分布で \\[\ny_i^*\\sim f(y_i|y_{\\text{obs}})\n\\] によって補間することが理想的である．\n\n\n\n\n\n\n(Multiple Imputation Rubin, 1978)\n\n\n\n\n事後予測分布から補間値を \\(M\\) 個生成する： \\[\n  y_i^{(j)}\\sim f(y_i|y_{\\text{obs}}),\\qquad j\\in[M].\n  \\]\nそれぞれの補間値について推定量 \\(\\widehat{Y}^{(j)}\\) を計算し，その平均を最終的な推定値とする： \\[\\newcommand{\\MI}{\\mathrm{MI}}\n  \\widehat{Y}_\\MI:=\\frac{1}{M}\\sum_{j=1}^M\\widehat{Y}^{(j)}.\n  \\]\n\n\n\n(Royston and White, 2011) は \\(M\\approx10^3\\) を推奨している．\n\n\n5.3 連鎖方程式による多重代入\n多重代入法において事後予測分布から補間値を生成することは，\\(Y\\) に関してモデルを立てる必要があるためネックになりがちである．\n相互条件付き識別性 (FCS: Fully Conditional Specification) (Stef Van Buuren and Rubin, 2006) が成り立つモデルについては，モデルの具体的な形に依らない Gibbs サンプラーによるサンプリングが可能になる．\nこれを 連鎖方程式による多重代入 (MICE: Multiple Imputation by Chained Equations) (Buuren and Groothuis-Oudshoorn, 2011) といい，R 言語 mice パッケージで実装されている．\n\nその実用性も相まってか，近年の Lancet 誌，New England Journal of Medicine 誌のレビューでは，欠測データの取り扱いに最も多く用いられている手法は MICE であるという報告もある(Hayati Rezvan et al., 2015)． (久史, 2017, p. 75)\n\n\n\n5.4 その他の代入法\nランダムな欠損ではなく，計画された大規模な欠損がある場合は，two-phase sampling の考え方を応用することができる (Kim, 2024, p. 173)．\nなお，全ての代入法はモデル \\(Y|(X,\\delta)\\) の特定を間違えると，\\(\\widehat{Y}\\) の不偏性が失われることに注意 (Hayati Rezvan et al., 2015)．\n\n\n5.5 代入をしない\n代入をせず，欠測しているなら欠測したままで最尤推定を実行することも考えられる．\nこのアプローチは 完全情報最尤推定 (FIML: Full Information Maximum Likelihood)，より最近では　pairwise likelihood estimation とも呼ばれる．11\n欠測が \\(Y\\) に依存しない場合，この「最尤推定量」は MAR の下で一致性と漸近正規性を持つ．12\nただし，推定されたモデルから，欠測値を代入してから結果を出してももちろん良い．ベイズの観点からは，モデルの平均を取ってから予測することに当たる．13\n(1.6節 Buuren, 2018) も参照．\n\n\n5.6 欠測値をどう扱うべきか？\nいつでも多重代入法を使えば良いというものではない．\n例えば \\((X,Y)\\) の関数関係が知りたい回帰分析の状況下で被説明変数 \\(Y\\) の欠損は，これを無視してリストワイズ消去をした complete-case analysis が代入法と等価になる．\n他にも complete-case analysis や代入をしない方がむしろ適切な場合は多い (2.7節 Buuren, 2018)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#終わりに",
    "href": "posts/2024/Survey/Survey3.html#終わりに",
    "title": "ベイズデータ解析３",
    "section": "6 終わりに",
    "text": "6 終わりに"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#footnotes",
    "href": "posts/2024/Survey/Survey3.html#footnotes",
    "title": "ベイズデータ解析３",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInverse probability weighting estimator ともいう (Hernán and Robins, 2020, p. 22)．↩︎\n結果的に，Weighted Least Squares と同じ形になっている．WLS は誤差分散が既知の形 \\(W^{-1}:=\\mathrm{diag}(\\sigma_1^2,\\cdots,\\sigma_n^2)\\) をしている場合の最良線型不偏推定量 (BLUE) である．一般に最小二乗法は広い設定で BLUE を与え続け，一般の既知の分散 \\(V(X)\\) を持つ場合は GLS (Generalized Least Squares) と呼ばれる．\\(V(X)\\) が既知である場合などなく，一般にはこれの推定から始める必要があり，これは Feasible GLS と呼ばれる (Hayashi, 2000, p. 59)．↩︎\nこの抽出計画に依らない性質を以て，(Särndal et al., 1992) は model-assisted 推定量と呼んでいる．model-dependent 推定量とは対照的である．↩︎\nこの２つの類似性は (Zieschang, 1990) が指摘している．一般の回帰分析の設定下では “GLS is more efficient than OLS under heteroscedasticity (also spelled heteroskedasticity) or autocorrelation” などと説明される．↩︎\nただし，余分な項があるために，正しく特定されている下では校正推定量よりもやや分散が大きい．↩︎\n総務省統計局では，Imputation の訳語として「補定」を用いる．↩︎\n最も古典的な形のものであり，母集団上の条件であることから，population MAR とも呼ばれる．母集団上の MAR と抽出計画の無視可能性 (Sugden and Smith, 1984) との２条件が成り立つとき，標本の MAR が成り立つ (Berg et al., 2016)．↩︎\n\\(Y\\to X\\to\\delta\\) が Markov 連鎖をなす，とも換言できる．↩︎\n(Kim, 2024, p. 154) 定理12.1も参照．↩︎\n一方で，重み付き推定方程式の解として定まる \\(Z\\)-推定量として構成することもできる．(5.2節 高井啓二 et al., 2016, p. 163)．↩︎\n完全情報最尤推定の言葉は初期の構造方程式モデリングプログラム AMOS に組み込まれて有名になっていた (Craig K. Enders and Bandalos, 2001)．直接尤度 (direct likelihood) または観測尤度 (observed likelihood) の方法ともいう (狩野裕, 2019)．完全尤度 (full likelihood) の用語は (高井啓二 et al., 2016) など．↩︎\n(狩野裕, 2019) に素晴らしい解説がある．日本語の文献としては (高井啓二 et al., 2016) もあり，第５章で推定方程式の観点から解説されている．↩︎\nそういえば Bayes 的な integral out に関して doubly robust という考え方はないのか？doubly robust の Bayesian counterpart はなんだろう？↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html",
    "href": "posts/2024/Survey/Survey4.html",
    "title": "ベイズデータ解析４",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#関連記事",
    "href": "posts/2024/Survey/Survey4.html#関連記事",
    "title": "ベイズデータ解析４",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#非確率標本とは何か",
    "href": "posts/2024/Survey/Survey4.html#非確率標本とは何か",
    "title": "ベイズデータ解析４",
    "section": "1 非確率標本とは何か？",
    "text": "1 非確率標本とは何か？\n\nGenerally speaking, these designs have not been explored in detail by survey researchers even though they are frequently used in other applied research ﬁelds. (Baker et al., 2013, p. 91)\n\n母集団 \\([N]\\) から部分集合 \\(S\\subset[N]\\) が標本として抽出されたとする．\nこの抽出計画 (sampling design / mechanism) が未知である場合，これを 非確率標本 (nonprobability sample) という．\n\n1.1 非確率標本\n確率抽出 (probability sampling) とは \\([N]\\) の部分集合の全体 \\(P([N])\\) 上の既知の確率分布に従っているとみなせる標本で，さらに何人も標本に選ばれる確率が零でないもの \\[\n\\pi_i:=\\operatorname{P}[i\\in S]&gt;0\n\\] をいう．詳しくは，前稿 も参照．\nすなわち非確率標本とは，\\(S\\in\\mathcal{L}(\\Omega;P([N]))\\) の従う分布が未知であったり，抽出計画上絶対に標本に入り得ない単位が存在する場合をいう．\n\n\n1.2 例\n母集団 \\([N]\\) を国民全体だとした場合，確率抽出は国勢調査規模の営為によってのみしか達成し得ない．\n多くの科学分野で実施されるような，特定の学校の学生や特定の地域の構成員を対象としたサンプル　便宜的標本 (convenience sample) は全て非確率標本に分類されることになる．\nまた多くのウェブアンケート代行業者は，事前にアンケートに協力することを約束したユーザーのプールからランダムに抽出して実行する．このような，自主的な応募によって得られたパネルを opt-in panel / panel of volunteers といい，ここからのサンプルもまた便宜的（二段階抽出）標本である．\n以上の理由から，多くの「ビッグデータ」と呼ばれるデータは非確率標本である (Meng, 2018), (Jae-Kwang Kim and Tam, 2021)．\nそのほかの非確率的標本の例については，(Section 3 AAOR, 2013) を参照．\n\n\n1.3 自己選択バイアスの問題\nこのような非確率標本では，特定のクラスの単位を包摂できていない問題 (frame undercoverage) や，自ら進んで応募して標本に入ることで生じる交絡とバイアス (self-selection bias) が問題になる．1\n端的に言えば，ランダムな欠測 (MAR: Missing At Random) (Rubin, 1976) の仮定が成り立たず，多くの欠測データ手法はそのままでは適用できないことが問題になる．\n\n\n1.4 データ統合\n非確率標本単体では出来ることが限られているかもしれないが，補助情報と組み合わせてモデルを立てることで統計的推論を試みることができる．\n\n\n典型的なデータの例\n\n\n\n\n\n\n\n\n\nData\nDesign\nRepresentative?\nX\nY\n\n\n\n\nA\nProbability\nYes\nX\nmissing\n\n\nB\nNonprobability\nNo\nX\nY\n\n\n\n\n確率標本 A をビッグデータ B と紐づけられるという状況はかなら理想的であるが，仮にこのような dual frame estimation (Hartley, 1962), (Skinner and Rao, 1996) の一部として非確率標本を扱えるときは，B を A の補助情報とみることで従来の校正荷重による推定の理論が利用可能になる．校正推定量については前稿も参照．\n例えば A を実験データ，B を観察データとしたデータ統合の試みは計量経済学においても進んでいる (Athey et al., 2019), (Athey et al., 2020), (Park and Sasaki, 2024)．B をオルタナティブデータと呼ぶ向きもある．\n実はこれから見るように，非確率標本の過小包摂性 (under coverage) は，単純ランダム抽出ではない抽出計画による確率標本のバイアス補正の議論に帰着し，自己選択バイアス (self-selection bias) の補正は欠測データの議論に帰着する (Jae-Kwang Kim and Tam, 2021)．\n\n\n1.5 データ統合の方法\n大きく分けて次の３通りが考えられる (Salvatore et al., 2024)：\n\n\n\n\n\n\n\n荷重校正による方法 (Elliot, 2009), (Robbins et al., 2020)\n\n非確率標本はあくまで確率標本の補助情報とし，荷重校正を実施する．\n\n擬似ランダム化による方法 (Elliott and Valliant, 2017)\n\n自然によるランダム化が行われたとし，これを推定するステップを追加することで確率標本の議論に帰着させる．\n\n大量代入 (mass imputation) による方法 (Jae Kwang Kim et al., 2021)\n\n\n\n\n\n\n1.6 バイアス低減\n各単位 \\(i\\in[N]\\) が標本に包含される確率 \\[\n\\pi_i:=\\operatorname{P}[i\\in S],\\qquad i\\in [N],\n\\] が未知である場合でも，母集団 \\([N]\\) 上で \\[\n\\pi_i^{-1}\\,\\propto\\,x_i^\\top\\lambda,\\qquad i\\in[N],\n\\] を満たす補助変数 \\(x_i\\;(i\\in[N])\\) が利用可能ならば，推定のバイアスを低減することが可能である．\n\n\n1.7 傾向スコア\nしたがって \\(\\pi_i\\) を推定することが問題になる．\n\\(\\delta_i:=1_S(i)\\) が \\(\\delta_i=1\\) を満たすときのみ \\(y_i\\) が観測されるとすると，\n\\[\n\\pi(x):=\\operatorname{P}[\\delta=1|X=x]\n\\] を 包含確率 または 傾向スコア (propensity score) (Rosenbaum and Rubin, 1983) という．2\n「未知のランダム化メカニズム \\(\\pi\\)」を想定し，これを推定することで確率標本の議論に帰着させるというアプローチは quasi-randomization approach とも呼ばれる (Elliott and Valliant, 2017), (Beresovsky et al., 2024)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#校正推定量",
    "href": "posts/2024/Survey/Survey4.html#校正推定量",
    "title": "ベイズデータ解析４",
    "section": "2 校正推定量",
    "text": "2 校正推定量\n\n2.1 確率標本に対する校正推定量\nGREG モデルと呼ばれる超母集団模型 \\[\ny_i=x_i^\\top\\beta+e_i,\\qquad e_i\\overset{\\text{i.i.d.}}{\\sim}(0,c_i(x_i)\\sigma^2),\n\\tag{1}\\] を仮定する．校正条件 \\[\n\\sum_{i\\in S}\\omega_ix_i=\\sum_{i=1}^Nx_i\n\\tag{2}\\] を満たす荷重 \\((\\omega_i)\\) を用いた線型推定量 \\[\n\\widehat{Y}_{\\mathrm{cal}}:=\\sum_{i\\in S}\\omega_iy_i\n\\] を 校正推定量 (calibration estimator) といい，抽出計画が 無視可能 (ignorable) である限り \\(Y\\) の不偏推定量になる．\nここまでは 前稿 で見た通りである．\n\n\n2.2 非確率標本に対する校正推定量\nこうなると \\(\\sum_{i=1}^Nx_i\\) が判明・推定すれば良いので，校正推定量に関しては 欠測データに対する対処 と同様に，傾向スコアの推定を通じて非確率標本に対応することができる．\nこれには超母集団模型 (1) に加えて，傾向スコア \\[\n\\operatorname{P}[\\delta=1|X=x]=:\\pi(x)\n\\] に対してもモデル \\((\\pi_\\phi)\\) をおく必要がある．\nこのとき，\\(G\\in C^2(\\mathbb{R})\\) を強凸関数，\\(g:=G'\\) として \\[\nQ(\\omega):=\\sum_{i\\in S}G(\\omega_i)c_i(x_i)\n\\] を，校正条件 (2) と完全情報の下で最尤推定された \\(\\widehat{\\phi}\\) を用いて推定した傾向スコア \\(\\widehat{\\pi}_i:=\\pi(\\widehat{\\phi}(x_i))\\) に関して \\[\n\\sum_{i\\in S}\\omega_ig(\\widehat{\\pi}_i^{-1})c_i=\\sum_{i=1}^Ng(\\widehat{\\pi}_i^{-1})c_i(x_i)\n\\tag{3}\\] を満たす中で最小化する荷重 \\((\\omega_i)\\) を用いた校正推定量は，二重頑健性を持つ．\n制約 (3) は選択バイアスを抑える役割を持ち，脱偏倚制約 (de-biasing constraint) とも呼ばれる (Jae Kwang Kim, 2024, p. 198)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#文献案内",
    "href": "posts/2024/Survey/Survey4.html#文献案内",
    "title": "ベイズデータ解析４",
    "section": "3 文献案内",
    "text": "3 文献案内\n\n(Jae Kwang Kim, 2024) を最も参考にした．他によく読んだものは (AAOR, 2013), (Elliott and Valliant, 2017)．\nセミパラメトリック推定に関する日本語文献は (逸見昌之, 2014)．\n非確率標本の確率標本と組み合わせた利用については，計量経済学の文献を除いても (Lohr and Raghunathan, 2017), (Meng, 2018), (Hand, 2018), (Robbins et al., 2020), (Rao, 2021), (Beaumont and Rao, 2021), (Angelopoulos et al., 2023), (Golini and Righi, 2024), (Salvatore et al., 2024) などがあり，大変盛り上がってきている印象がある．"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#footnotes",
    "href": "posts/2024/Survey/Survey4.html#footnotes",
    "title": "ベイズデータ解析４",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nすごく大雑把には，収入が高い人ほど収入に関するアンケートに参加しやすい，ウェブに関心のある人ほどウェブアンケートを受けやすい，など．↩︎\n包含確率の用語は標本調査論による．傾向スコアは欠測データ解析による．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html",
    "href": "posts/2024/Survey/Survey2.html",
    "title": "ベイズデータ解析２",
    "section": "",
    "text": "分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#関連記事",
    "href": "posts/2024/Survey/Survey2.html#関連記事",
    "title": "ベイズデータ解析２",
    "section": "",
    "text": "分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#はじめに",
    "href": "posts/2024/Survey/Survey2.html#はじめに",
    "title": "ベイズデータ解析２",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 概観\n現代の因果推論は平均処置効果 (ATE) \\[\n\\tau:=\\operatorname{E}[Y_i^1]-\\operatorname{E}[Y_i^0]\n\\] と関連する推定対象 (estimand / target parameter) に集中している．\n\n1.1.1 潜在結果モデル\nこのように 因果効果 と呼ばれる推定対象を設定し，良い実験計画を構築してこれを推定するという枠組みは (Neyman et al., 1990) から始まるもので，(Rubin, 1974) の因果モデルや 潜在結果モデル (potential outcome model)，または 反実仮想モデル (counterfactual model) とも呼ばれる．\n\n\n\n\n\n\n潜在結果モデルを用いた因果推論\n\n\n\n潜在結果モデルは特定の介入の因果効果を推定する枠組みであり，十分に考慮された実験計画が必要であるが，それにより不必要なモデリングの議論を避けることができる．\nこのために，極めて多くの人間を対象とする科学分野で潜在結果モデルが用いられている．\n\n計量経済学\n医学・疫学・生物統計学\n社会学 (Morgan and Winship, 2014)\n\n\n\nこの方法では，ランダム化された実験，あるいは欠測メカニズムが推定しやすいように工夫された「擬似実験」を行なうことで，ほとんどモデリングの議論を表に出さずとも ATE やその他の実験科学者が設定する量を不偏推定可能にする，というアプローチをとる．1\n最終的に ATE の推定においては，各個体 \\(i\\in[n]\\) に対して処置を行った場合の結果 \\(Y_i^1\\) と行わなかった場合の結果 \\(Y_i^0\\) とのいずれかは必ず欠測するということである．2\n端的に言えば，統計学のサンプリング論と科学の実験計画論との邂逅である (Ding, 2024a)．\n\n\n1.1.2 構造的因果モデル\nしかし他の多くの科学では実験的介入が難しかったり，実験計画だけでは背後の交絡要因が統制しきれない状況がある．\nまたはそもそも，科学的な興味の対象が「因果効果」だけでなくデータの背後にある「モデル」にもある場合も多い．\nこのような場合には，変数同士の関係を丁寧にモデリングし，加えて識別可能性を確保するなどの理論的な配慮が欠かせない．\nこれを可能にするのが 構造的因果モデル (SCM: Structural Causal Model) またはノンパラメトリック構造方程式モデルの枠組みである (Bongers et al., 2021)．\n多くはグラフィカルモデルと計算機的な方法を組み合わせることで，推定可能な高次元モデルを構築する．このモデルに対する「変換」として，介入操作と因果効果を定義する．\n\n\n\n\n\n\n構造的因果モデルを用いた因果推論\n\n\n\n自然を対象にする科学分野を中心として，介入できない状況下での因果推論が構造的因果モデルの枠組みで行われる．\n\n地球科学 (Runge et al., 2023)\n機械学習：エキスパートシステム (Pearl, 1988)，継続学習 (Cui and Athey, 2022)，反実仮想機械学習 (M. Huber, 2023)\n医学・疫学・生物統計学：標的学習 (targeted learning) (Laan and Rose, 2011)\n\nモデリングが必要不可欠であるため，計算的にも困難な問題となる．それゆえ計算機科学や機械学習の分野で盛んに研究されている．\n\n\n\n\n1.1.3 まとめ\n\n\n\n\n\n\n\n潜在結果アプローチ：実験を行うことでモデリングを回避する．\n構造的因果モデル：モデリングを行い，モデルの変換として因果効果を定義する．特定の条件下 (Rubin, 1980) で離散変数に実験処置介入を行った場合として，潜在結果アプローチを含むとも見れる．3\n\n\n\n\n歴史的には，どちらかというと構造的因果モデル → 潜在結果モデルという順に注目された．\nこの現象は特に経済学で顕著に起こった．Cowles 委員会のイニシアティブの下で，初めは構造的なアプローチを取っていた経済学が，実験事実との乖離が激しいことの自覚から，実験と統計的推論を取り入れるように生まれ変わった現象は 信頼性革命 と呼ばれている．\n本章では以降，各分野における因果推論の歴史を議論する．\n\n\n\n1.2 経済学における因果推論の歴史\n経済学において，(Haavelmo, 1943) は「構造推定」の枠組みで政策介入の因果効果を推定しようとした．4\n構造推定では「同時方程式」により統計モデルを定義するが，その際には識別可能性が問題になる．\n当時の計算資源では十分な推定を実行することができず，加えて，マクロなモデルに対する「介入」の定義を正しく与えていなかった (Lucas, 1976)．5\n時代が下ると，このアプローチでの経済学は大きな批判に晒され，より実験的なアプローチを採用するように変化を余儀なくされた．これが信頼性革命である．\n\n\n1.3 計量経済学における信頼性革命\n(Leamer, 1983) は計量経済学の手法と古典的な実験科学とを比較し，計量経済学の信頼性は感度解析とロバストを取り入れることによって（のみ）回復されるだろうと論じた．\n(LaLonde, 1986) は職業訓練の効果に関する観察研究と実験研究の結果が大きく異なることを示し，当時の計量経済学が抱えていた体質に抜本的改革を迫った．\n(Leamer, 1983) が「よく計画された実験を超える統計的手法など出てこないかもしれない」と論じていた通り，その後の信頼性革命は主に実験計画を改善することと擬似ランダム化の適切な取り扱いによって達成された (Angrist and Pischke, 2010)．\nそのキーワードは「自然実験」や「擬似実験」と呼ばれており，サーベイ手法での「擬似ランダム化」アプローチに相当する．擬似ランダム化については次稿も参照．\n\n\n1.4 媒介分析\n媒介分析 (mediation analysis) (Robins and Greenland, 1992) においては，因果の流れが複数あり得る場合に，媒介因子 \\(Z\\) を経由した 間接効果 の量を総合効果の中から識別することを目標とする．\n\n\n\n媒介変数 \\(Z\\) を表す図式\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n一方でモデリングに基づいた方法も可能である (Pearl, 2012), (Nguyen et al., 2021)．6\nはじめ社会学や社会心理学においてはモデルによる媒介分析が試みられていた (Alwin and Hauser, 1975), (Baron and Kenny, 1986)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#交絡調整法",
    "href": "posts/2024/Survey/Survey2.html#交絡調整法",
    "title": "ベイズデータ解析２",
    "section": "2 交絡調整法",
    "text": "2 交絡調整法\n\n2.1 はじめに\n因果推論において，実験計画による工夫に限界がある際は，条件 \\[\n(Y^0_i,Y^1_i)\\perp\\!\\!\\!\\perp A_i|X_i\n\\tag{1}\\] を満たす共変量 \\(X_i\\) の特定を目指す．\nこの \\(X_i\\) を 交絡因子 (cofounders) といい，条件 (1) を 非交絡性 (unconfoundedness) または 無視可能性 (ignorability) という．\n\n\n2.2 操作変数法\n操作変数 (instrumental variable) とは，処置変数（または説明変数）をよく予測するような補助変数であり，補助変数と処置変数の間の関係を推定することで擬似的に層別サンプリングが行われたとみなせるようなものである．7\n未観測の交絡因子が予期される場合でも，操作変数が利用可能である場合はこれを調整することができる．\n\n\n\n\n\n\n計量経済学的な説明8\n\n\n\n\n\n回帰モデル \\[\nY=X_1^\\top\\beta_1+X_2^\\top\\beta_2+\\epsilon\n\\] において，\\(X_1\\) は外生性を持つが，\\(X_2\\) は内生性を持ってしまうとする： \\[\n\\operatorname{E}[X_1\\epsilon]=0,\\quad\\operatorname{E}[X_2\\epsilon]\\ne0.\n\\] このとき，次の３条件を満たす \\(X\\) と同次元の \\(Z\\) を操作変数という：\n\n外生性：\\(\\operatorname{E}[Z\\epsilon]=0\\)．\n多重線型性の非存在：\\(\\operatorname{rank}\\operatorname{E}[ZZ^\\top]&gt;0\\)．\n関連性：\\(\\operatorname{rank}\\operatorname{E}[ZX^\\top]=\\operatorname{rank}\\operatorname{E}[X]\\)．\n\n操作変数 \\(Z_2\\) を \\(X_2\\) の代わりに説明変数に用いることで内生性の問題が除去される．このため \\(Z_2\\) は 排除されていた外生変数 (excluded exogenous variable) ともいう．\n\n\n\n操作変数が存在するとき，遵守者の平均処置効果，すなわち 局所平均処置効果 (LATE: Local Average Treatment Effect) が識別可能になる (Imbens and Angrist, 1994)．\n\n\n2.3 回帰非連続デザイン\n回帰不連続デザイン (RDD: Regression Discontinuity Design) では，割り当ての閾値の近傍では擬似ランダム化が行われていると仮定できる状況において，閾値の近傍に位置した部分標本を用いて，その部分標本での処置効果を推定する．\n\n\n2.4 差の差法\n差分の差法 (DID: Difference-in-Differences) は，被曝群と比較群それぞれの処置前後の差分に現れる差分を，処置効果の近似とみなす方法である．\n被曝群と比較群をマッチングすることで共変量を統制することが期待されるが，処置の有無と関係を持つ未統制の共変量の調整が問題となる (Bertrand et al., 2004)．\n\n\n2.5 周辺構造モデル\n周辺構造モデル (marginal structural model) は平均処置効果をパラメータに持つモデルであり (Robins, 2000)，潜在結果変数の（周辺）平均構造をモデリングする： \\[\ng(\\operatorname{E}[Y^a_i|L_i])=\\psi_0+\\psi_1a+\\psi_2L_i+\\psi_3L_ia.\n\\]\nさらに \\(Y^a|L,A\\) の分布に指数型分布族を仮定したものは 一般化線型モデル (GLM) と呼ばれるが，周辺（構造）モデルは GLM から分布の仮定を除去した一般化とも見れる．9\n統計ソフトの充実によりよく使われるようになったが，後述の構造的ネストモデルと \\(G\\)-推定の方が一般的であり，より効率的である (Vansteelandt and Joffe, 2014) との見解もある．\n\n\n2.6 構造的平均モデルと \\(G\\)-推定\n\\(G\\)-推定 (Robins et al., 2000) は不服従など処置変数 \\(D\\) に依存した交絡を調整するために，構造的平均モデル，パラメトリック \\(G\\)-公式 (Robins, 1986)，構造的ネストモデル (structural nested model) (Robins et al., 1992) と同時に提案された推定手法である．\n構造的平均モデル (SMM: Structural Mean Model) では，リンク関数 \\(g\\) の自由度を残して \\[\ng(\\operatorname{E}[Y^a|L=l,A=a])-g(\\operatorname{E}[Y^0|L=l,A=a])=\\gamma^*(l,a;\\psi^*)\n\\] により処置 \\(A=a\\) の平均因果効果にパラメトリックな仮定をおく．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#モデルフリー推定手法",
    "href": "posts/2024/Survey/Survey2.html#モデルフリー推定手法",
    "title": "ベイズデータ解析２",
    "section": "3 モデルフリー推定手法",
    "text": "3 モデルフリー推定手法\n\n3.1 はじめに\n疫学では一般化推定方程式，計量経済学では一般化モーメント法など，モデルを全面に押し出さずに推定目標を定義し，これを推定する手法が用いられる．\nこのように関心のある母数以外の 局外母数 (nuisance parameter) にはモデルを明示的に想定しない手法を セミパラメトリック法 (semi-parametric method) という．\nこのような手法では，興味のあるパラメータがはっきりしているため，それ以外のモデルの仮定にはひとまず興味がなく，誤特定の下でも効率的な推論ができるロバスト性が重視される．9\n\n\n3.2 共通の枠組み\nある関数 \\(g\\) に関して， \\[\n\\operatorname{E}[g(\\beta,X,Y)]=0\n\\tag{2}\\] によって推定対象 \\(\\beta\\) を特徴付ける場面は多い．10\n条件 (2) によって推定対象 \\(\\beta\\) が定義されているとき，標本上の対応する方程式 \\[\n\\frac{1}{n}\\sum_{i=1}^ng(\\beta,X_i,Y_i)=0\n\\tag{3}\\] の解として推定量を構成することが自然な発想になる．\n\n\n\n\n\n\n名前一覧\n\n\n\n\n数理統計学・頑健統計では極値点として定義される \\(M\\)-推定量（第 3.3 節）と区別して \\(Z\\)-推定量 とも呼ばれる (van der Vaart, 1998)．\n疫学では \\(g\\) を 推定関数 (Godambe, 1997)，式 (2) を推定関数の 不偏性，式 (3) を 推定方程式 という．\n計量経済学では 一般化モーメント法 (GMM: Generalized Method of Moments) (L. P. Hansen, 1982) として知られる．式 (3) を モーメント方程式 という．主に構造方程式モデルの推定に用いられた．\n\n\n\n特に一般化モーメント法は，モデルの議論を伴わないリサーチクエスチョンに応えるために格好の枠組みである．\n例えば線型回帰係数に対する OLS 推定量は \\(g(\\beta,X)=X(Y-X^\\top\\beta)\\) によって定まる一般化モーメント推定量である．\n操作変数推定量は \\(g(\\beta,X,Y,Z)=Z(Y-X^\\top\\beta)\\) によって定まる一般化モーメント推定量である．\nこのような推定量はモデルに依存しない方法を与える上に，漸近論の観点で好ましい性質を持つ (L. P. Hansen, 1982)．\n\n\n3.3 \\(M\\)-推定量\n最尤推定量のように，特定の目的関数 \\[\nM_n(\\theta):=\\frac{1}{n}\\sum_{i=1}^nm_\\theta(X_i)\n\\] を最大化する点として定義される推定量 \\(\\widehat{\\theta}\\) は \\(M\\)-推定量 と呼ばれる．\n同時に最尤推定量は，スコア関数の零点としても特徴付けられる (Carmer, 1946)．\n大変大雑把に言えば，モーメント法 → 最尤推定量 → \\(Z\\)-推定量という歴史的な流れがある (Le Cam, 1952)．\n\n\n\n\n\n\n名前一覧\n\n\n\n\n頑健統計において \\(M\\)-推定量 (P. J. Huber, 1964) と呼ばれる．11\n数理統計において最小コントラスト推定量 (Pfanzagl, 1969) と呼ばれる．\n\n\n\n頑健統計に起源を持つように，\\(M\\)-推定量，一般に \\(Z\\)-推定量は極めて安定して一致性と漸近正規性をもつ．\n推定方程式 (3) を近似的に解いても大丈夫だし，推定関数の不偏性 (2) が漸近的にしか成り立たなくてもほとんど問題がない．\n\n\n3.4 一般化推定方程式\n一般化推定方程式は，一般化線型モデル (Nelder and Wedderburn, 1972) に基づいて推定した \\(V_i\\) を代入した擬似スコア \\[\nU(Y_j,\\beta|X_j)=(\\partial_\\beta\\mu(X_j^\\top\\beta))^\\top V_i^{-1}(Y_i-\\mu(X_i^\\top\\beta))\n\\] を推定関数に用いた一般化モーメント法をいう．\n\\(U\\) を擬似スコアと呼んだのは，誤差の分散 \\(V_i\\) の計算に用いたモデルは仮定として認めているわけではなく，主に推定効率や所望の推定量の性質を得るために「作業仮設として」設定されたものであるためである．12\n\\(V_i\\) は作業共分散⾏列ともいう．\\(V_i\\) を代入した尤度を 擬似尤度 (quasi-likelihood) (Wedderburn, 1974) ともいう．\nこの方法では，真の誤差分布が相関を持つようなものであった場合でも，平均構造に関する逆リンク関数 \\(\\mu\\) の特定にさえ成功すれば，\\(\\beta\\) に関して一致推定を可能にする．\n誤差分布を特定せずとも実行できる最尤推定の一般化とも見れる．\n\\(V_i\\) の特定に成功した場合は，セミパラメトリック最適な推定量を与える．\n二次のモーメントに関しても関心がある場合は，混合効果モデルなどを通じたモデル化が必要になる．\n\n\n3.5 経験尤度法\nデータ \\(\\{x_i\\}_{i=1}^n\\subset\\mathbb{R}^d\\) に関する 経験尤度 (empirical likelihood) (Owen, 1988) とは，分布関数の汎函数 \\[\nL(F):=\\prod_{i=1}^nF(X_i)-F(X_i-)=\\prod_{i=1}^n\\operatorname{P}[X=x_i]\n\\] をいう．\nこの観点から，経験分布関数 \\(F_n\\) は経験尤度を最大にするノンパラメトリック推定量である．\n最尤法はモデルの全ての母数を特定化しない限り実行できないが，経験尤度の最大化ならば可能である．13\n\\[\n\\sum_{i=1}^np_ig(x_i,\\theta)=0\n\\] を満たす中で経験尤度を最大化する \\(\\theta\\) を 最大経験尤度推定量 (MELE: maximum empirical likelihood estimator) (Qin and Lawless, 1994) という．\nこの推定量は漸近正規性を持ち，一般化推定方程式や一般化モーメント法の代替手法として期待されている．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#文献案内",
    "href": "posts/2024/Survey/Survey2.html#文献案内",
    "title": "ベイズデータ解析２",
    "section": "4 文献案内",
    "text": "4 文献案内"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#footnotes",
    "href": "posts/2024/Survey/Survey2.html#footnotes",
    "title": "ベイズデータ解析２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n科学的な興味の対象は多くの場合モデル全体ではなく，特定の１つのパラメータであり，そのような場合は実験計画を工夫することでモデルに関係なく推定可能になるという発想は (Heckman, 2010) により “Marschak’s Maxim” と呼ばれる．詳しくは (Laan and Rose, 2011) のPearl による foreword も参照．↩︎\n因果推論の根本問題 (Holland, 1986) とも呼ばれる．↩︎\n(学, 2016) も参照．↩︎\n(川口康平 and 澤田真行, 2024), (Pearl, 2015) も参照．↩︎\n詳しくは ルーカス批判 (Wikipedia) も参照．↩︎\n(Laan and Rose, 2011) の Pearl による Foreword も参照．(Ryosuke Fujii and Suzuki, 2022) の オンラインページも参照．↩︎\nただし当然結果に依存してはいけない．(Section 2 Imbens and Angrist, 1994, p. 468) や (Section 3.1 Hernán and Robins, 2020, p. 28) も参照．↩︎\n(Section 12.5 B. E. Hansen, 2022, p. 335) も参照．↩︎\n(Ding, 2024b, p. 278) も参照．↩︎\nそれゆえ最尤法やベイズ法のように，一旦はモデルの想定が必要な手法が忌避されるところがある．↩︎\n\\(X,Y\\) を任意の定数とした際に，\\(\\beta\\) に関して一意な解を持つとき，モーメント条件は 識別可能 であるという．↩︎\n他には \\(L\\)-, \\(R\\)-推定量があった (Section 3.2 P. J. Huber, 1981, p. 43)．↩︎\n\\(V_i\\) の特定に成功している場合，そのパラメトリックモデルに関するスコア関数に一致する．↩︎\n一般化推定方程式のように，作業的な値を代入して得る尤度は 擬似尤度 (quasi-likelihood) という．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#終わりに",
    "href": "posts/2024/Stat/Regression.html#終わりに",
    "title": "セミパラメトリック重回帰分析",
    "section": "4 終わりに",
    "text": "4 終わりに\n\n以上の枠組みは全て 一般化モーメント法 (GMM: Generalized Method of Moments) (L. P. Hansen, 1982) の枠組みの中に位置する．\nGMM という名前は，OLS 推定の１次の最適性条件として得る直交条件 \\[\n\\operatorname{E}[X(Y-\\mu(X\\beta))]=0\n\\] が，モーメント法の課す条件と似ており，どれも \\[\n\\operatorname{E}[g(\\beta)]=0\n\\] という形をしているという点から来る．\nさらには経験尤度法 (Owen, 1988), (Qin and Lawless, 1994) も漸近正規性を持つノンパラメトリック手法であり，GMM の後釜として期待されている．特に直交条件の数が多い GMM よりバイアスが少ない．\nしかし GMM の方が分散が大きいことがあり，bias-variance のトレードオフがある (Newey and Smith, 2004)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#誤差分布に仮定を置かない推定手法",
    "href": "posts/2024/Survey/Survey2.html#誤差分布に仮定を置かない推定手法",
    "title": "ベイズデータ解析２",
    "section": "3 誤差分布に仮定を置かない推定手法",
    "text": "3 誤差分布に仮定を置かない推定手法\n\n3.1 はじめに\n疫学や医療統計では一般化推定方程式，さらに一般的な枠組みとして計量経済学では一般化モーメント法など，モデルを全面に押し出さずに推定目標を定義し，これを最尤推定にヒントをもらった方法で推定する手法が用いられる．\nこのように関心のある母数以外の 局外母数 (nuisance parameter) にはモデルを明示的に想定しない手法を セミパラメトリック法 (semi-parametric method) という．\nこのような手法では，興味のあるパラメータがはっきりしているため，それ以外のモデルの仮定にはひとまず興味がなく，誤特定の下でも効率的な推論ができるロバスト性が重視される．10\n\n\n3.2 共通の枠組み\nある関数 \\(g\\) に関して， \\[\n\\operatorname{E}[g(\\beta,X,Y)]=0\n\\tag{2}\\] によって推定対象 \\(\\beta\\) を特徴付ける場面は多い．11\n条件 (2) によって推定対象 \\(\\beta\\) が定義されているとき，標本上の対応する方程式 \\[\n\\frac{1}{n}\\sum_{i=1}^ng(\\beta,X_i,Y_i)=0\n\\tag{3}\\] の解として推定量を構成することが自然な発想になる．\nこの枠組みは種々の名前で呼ばれる．\n\n\n\n\n\n\n名前一覧\n\n\n\n\n数理統計学・頑健統計では極値点として定義される \\(M\\)-推定量（第 3.3 節）と区別して \\(Z\\)-推定量 とも呼ばれる (van der Vaart, 1998)．\n疫学では \\(g\\) を 推定関数 (Godambe, 1997)，式 (2) を推定関数の 不偏性，式 (3) を（一般化） 推定方程式 という．\n計量経済学では 一般化モーメント法 (GMM: Generalized Method of Moments) (L. P. Hansen, 1982) として知られる．式 (3) を モーメント方程式 という．\n\n\n\n一般化モーメント法は，\\(Y\\) の分布に関する議論を伴わないリサーチクエスチョンに応えるために格好の枠組みである．\n例えば線型回帰係数に対する OLS 推定量は \\(g(\\beta,X)=X(Y-X^\\top\\beta)\\) によって定まる一般化モーメント推定量である．\n操作変数推定量は \\(g(\\beta,X,Y,Z)=Z(Y-X^\\top\\beta)\\) によって定まる一般化モーメント推定量である．\nこのクラスの推定量はモデルに依存しない方法を与える上に，漸近正規性などの好ましい性質を持つ (L. P. Hansen, 1982) ため，これを通じて推定量の分散を推定することもできる．\n\n\n3.3 \\(M\\)-推定量\nなお，前述の推定量は \\(M\\)-推定量と呼ばれてしまうこともあるが，数理統計学では，最尤推定量のように，特定の目的関数 \\[\nM_n(\\theta):=\\frac{1}{n}\\sum_{i=1}^nm_\\theta(X_i)\n\\] を最大化する点として定義される推定量 \\(\\widehat{\\theta}\\) は \\(M\\)-推定量 と呼ばれる．\n同時に最尤推定量は，スコア関数の零点としても特徴付けられる (Carmer, 1946)．こう見た場合を \\(Z\\)-推定量と呼び分けることにする．\n大変大雑把に言えば，モーメント法 → 最尤推定量 → \\(Z\\)-推定量という歴史的な流れがある (Le Cam, 1952)．\n\n\n\n\n\n\n\\(M\\)-推定量の別名一覧\n\n\n\n\n頑健統計において \\(M\\)-推定量 (P. J. Huber, 1964) と呼ばれる．12\n数理統計において最小コントラスト推定量 (Pfanzagl, 1969) と呼ばれる．\n\n\n\n頑健統計に起源を持つ \\(M\\)-推定量，一般に \\(Z\\)-推定量は，分布の誤特定の下でも極めて安定して一致性と漸近正規性をもつ推定量のクラスとして導入された．\n推定方程式 (3) を近似的に解いても大丈夫だし，推定関数の不偏性 (2) が漸近的にしか成り立たなくてもほとんど問題がない．\n以下では，特別な \\(Z\\)-推定量を見ていく．\n\n\n3.4 一般化推定方程式\n一般化推定方程式は，一般化線型モデル (Nelder and Wedderburn, 1972) に基づいて推定した \\(V_i\\) を代入した擬似スコア \\[\nU(Y_j,\\beta|X_j)=(\\partial_\\beta\\mu(X_j^\\top\\beta))^\\top V_i^{-1}(Y_i-\\mu(X_i^\\top\\beta))\n\\] を推定関数に用いた一般化モーメント法をいう．\n\\(U\\) を擬似スコアと呼んだのは，誤差の分散 \\(V_i\\) の計算に用いたモデルは仮定として認めているわけではなく，主に推定効率や所望の推定量の性質を得るために「作業仮設として」設定されたものであるためである．13\n\\(V_i\\) は作業共分散⾏列ともいう．\\(V_i\\) を代入した尤度を 擬似尤度 (quasi-likelihood) (Wedderburn, 1974) ともいう．\nこの方法では，真の誤差分布が相関を持つようなものであった場合でも，平均構造に関する逆リンク関数 \\(\\mu\\) の特定にさえ成功すれば，\\(\\beta\\) に関して一致推定を可能にする．\n誤差分布を特定せずとも実行できる最尤推定の一般化とも見れる．\n\\(V_i\\) の特定に成功した場合は，セミパラメトリック最適な推定量を与える．\n二次のモーメントに関しても関心がある場合は，混合効果モデルなどを通じたモデル化が必要になる．\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.5 経験尤度法\nデータ \\(\\{x_i\\}_{i=1}^n\\subset\\mathbb{R}^d\\) に関する 経験尤度 (empirical likelihood) (Owen, 1988) とは，分布関数の汎函数 \\[\nL(F):=\\prod_{i=1}^nF(X_i)-F(X_i-)=\\prod_{i=1}^n\\operatorname{P}[X=x_i]\n\\] をいう．\nこの観点から，経験分布関数 \\(F_n\\) は経験尤度を最大にするノンパラメトリック推定量である．\n最尤法はモデルの全ての母数を特定化しない限り実行できないが，経験尤度の最大化ならば可能である．14\n\\[\n\\sum_{i=1}^np_ig(x_i,\\theta)=0\n\\] を満たす中で経験尤度を最大化する \\(\\theta\\) を 最大経験尤度推定量 (MELE: maximum empirical likelihood estimator) (Qin and Lawless, 1994) という．\nこの推定量は漸近正規性を持ち，一般化推定方程式や一般化モーメント法の代替手法として期待されている．"
  },
  {
    "objectID": "posts/2024/Stat/NRegression.html",
    "href": "posts/2024/Stat/NRegression.html",
    "title": "ノンパラメトリック回帰分析",
    "section": "",
    "text": "自乗残差最小化の視点から\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Stat/NRegression.html#ノンパラメトリック回帰",
    "href": "posts/2024/Stat/NRegression.html#ノンパラメトリック回帰",
    "title": "ノンパラメトリック回帰分析",
    "section": "1 ノンパラメトリック回帰",
    "text": "1 ノンパラメトリック回帰\n\n1.1 カーネル密度推定量 (KDE)\nデータ \\(\\{x_n\\}\\subset\\mathcal{X}\\) と半正定値核 \\(K\\) に対して， \\[\np(x|\\{x_n\\})=\\frac{1}{N}\\sum_{n=1}^NK_\\ell(x,x_n)\n\\] は再び半正定値核である．これを Parzen 窓推定量 または カーネル密度推定量 という．\nこれはデータの経験分布と確率核 \\(K\\) との畳み込みになっている．\\(K\\) として Gauss 核を用いると，これはデータ分布の軟化として使え，デノイジングスコアマッチングなどに応用を持つ．\nただし，\\(\\ell\\) は 幅 (bandwidth) とよばれるハイパーパラメータである．例えば \\(K\\) が動径 \\(r\\) の関数であるとき， \\[\nK_\\ell(r):=\\frac{1}{\\ell}K\\left(\\frac{r}{\\ell}\\right)\n\\] などと導入できる．\n\n\n1.2 カーネル回帰\nデータが \\(\\mathcal{D}=\\{(x_i,y_i)\\}_{i=1}^n\\) という形で与えられ，平均 \\(\\operatorname{E}[Y|X,\\mathcal{D}]\\) を推定することを考える．\nこの際，まず結合密度を次の形で推定する： \\[\np(y,x|\\mathcal{D})=\\frac{1}{n}\\sum_{i=1}^nK_\\ell(x,x_i)K_\\ell(y,y_i)\n\\] これを用いると，次のように平均が推定できる： \\[\n\\operatorname{E}[Y|X,\\mathcal{D}]=\\int_{\\mathcal{Y}} yp(y|X,\\mathcal{D})\\,dy=\\sum_{i=1}^ny_iw_i(x),\\qquad w_i(x):=\\frac{K_\\ell(x,x_i)}{\\sum_{j=1}^nK_\\ell(x,x_j)}.\n\\]\nこの手続きを，カーネル回帰 / カーネル平滑化，または回帰関数に関する (Nadaraya, 1964)-(Watson, 1964) 推定量という．\n\n\n1.3 局所線型回帰 (LLR)\nカーネル回帰では \\(\\operatorname{E}[Y|X,\\mathcal{D}]\\) を，\\(\\{y_i\\}\\) の適切な線型和として予測していた．実は \\[\n\\sum_{i=1}^ny_iw_i(x)=\\min_\\beta\\sum_{i=1}^n(y_i-\\beta)^2K_\\ell(x,x_i)\n\\] の解として特徴付けられる．\n代わりに， \\[\n\\mu(x):=\\min_{\\beta}\\sum_{i=1}^n\\biggr(y_i-\\beta^\\top\\phi(x_i)\\biggl)^2K_\\ell(x,x_i)\n\\] によって \\(\\operatorname{E}[Y|X,\\mathcal{D}]\\) を予測することを，局所線型回帰 (LLR: locally linear regression) または LOWESS (Locally Weighted Scatterplot Smoothing) (Cleveland, 1979), (Cleveland and Devlin, 1988)，または Savitsky-Golay フィルター (Savitzky and Golay, 1964) という．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html",
    "title": "ベイズ生存時間解析",
    "section": "",
    "text": "Zig-Zag サンプラーによるモデル平均法\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#次稿",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#次稿",
    "title": "ベイズ生存時間解析",
    "section": "",
    "text": "Zig-Zag サンプラーによるモデル平均法\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#生存時間と競合リスクの解析",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#生存時間と競合リスクの解析",
    "title": "ベイズ生存時間解析",
    "section": "1 生存時間と競合リスクの解析",
    "text": "1 生存時間と競合リスクの解析\n\n1.1 はじめに\n医療経済学 などにおける 医療技術評価 (HTA: Health Technology Assessment) とは，新たな医療技術を臨床試験で評価し，リスクやコストを勘案して新技術の既存法との効果や安全性を評価・比較する 決定理論的な枠組み である．\nこの枠組みでは非負確率変数 \\(Y\\) に対して，例えば次のような平均の計算が必要になる： \\[\n\\operatorname{E}[Y]=\\int^\\infty_0S_Y(y)\\,dy,\\qquad S_Y(y):=\\operatorname{P}[Y\\ge y].\n\\tag{1}\\]\n医療技術評価の文脈において \\(Y\\) は被験者のイベントまでの時間 (time-to-event) を表す確率変数で，\\(S_Y\\) は \\(Y\\) の 生存（割合）関数 といい，\\(Y\\) の分布関数 \\(F_Y\\) と次の関係を持つ： \\[\nS_Y(y)=1-F_Y(y).\n\\]\n平均 (1) を計算するためには，観測の打ち切りを乗り越えて \\(S_Y\\) を \\([0,\\infty)\\) 全域で推定する必要があり，そのためにはベイズ手法が有望視されている（第 1.5 節）．\n\n\n1.2 生存時間解析とは\nイベントまでの時間 \\(Y\\) の生存関数 \\(S_Y\\) を推定する問題は 生存時間解析 (survival analysis) または信頼性解析として知られ，ほとんど統計学の起源と同時に始まる長い歴史を持つ．1\n共変量 \\(X\\) の生存時間 \\(Y\\) への影響を調べる際には，生存関数 \\(S_Y\\) の代わりに ハザード関数 （または瞬間故障率，死亡率） \\[\nh(y):=\\frac{f(y)}{S(t)}=-\\frac{d \\log S(y)}{d t},\\qquad f(y)=F'(y)=-S'(y),\n\\] もモデリングの対象になる．ただし \\(f\\) は \\(Y\\) の確率密度関数とした．\nハザード関数 \\(h\\) は，被験者の生存割合が \\(S(y)\\) である段階での，次の単位時間でのイベント発生率を表す．ハザード関数が判れば，生存関数は \\[\nS(y)=\\exp\\left(-\\int^y_0h(t)\\,dt\\right)\n\\] によって復元される．\nさらに究極の目標として，生存時間 \\(Y\\) に影響を与える共変量 \\(X\\) の成分を特定することがある．この際に用いるモデルを 競合リスクモデル という（第 1.7 節）．\n競合する \\(K\\) 個のリスク要因がそれぞれハザード \\(h_1,\\cdots,h_K\\) を持つとき，イベント発生時刻 \\(Y\\) のハザードは和 \\[\nh_Y(y)=\\sum_{j=1}^Kh_j(y)\n\\] で与えられる．これを 多ハザード模型 (Polyhazard model)（第 1.6 節），\\(h_i\\) を 原因別ハザード という．\n\n\n1.3 生存関数推定\n\n1.3.1 欠測の問題\n生存時間解析における最大の問題は観測の 打ち切り (censoring) である．\n換言すれば，ほとんどの生存時間データでは種々の理由で被験者が脱落し，追跡終了時点以降はイベントの発生を確認できないのである．\nそこで，５割生存時間 (MST: Median Survival Time) を代わりに推定対象としたり，ある打ち切り時刻 \\(T&gt;0\\) までの区間のみに限って \\(S:[0,T]\\to[0,1]\\) を推定することが考えられた．\n\n\n1.3.2 パラメトリックモデル\n\\(S_Y\\) をパラメトリックな分布族の中から推定することが考えられる．\nこの場合，形状母数 \\(\\nu&gt;0\\) によってハザード関数の増加・減少を柔軟にモデリングできる Weibull 分布 \\(\\mathrm{W}(\\nu,\\alpha)\\) または (Rosin and Rammler, 1933) の分布 \\[\nf(x;\\nu,\\alpha)=\\frac{\\nu}{\\alpha}\\left(\\frac{x}{\\alpha}\\right)^{\\nu-1}\\exp\\left(-\\left(\\frac{x}{\\alpha}\\right)^\\nu\\right)1_{\\left\\{x&gt;0\\right\\}},\\qquad\\alpha&gt;0,\\nu&gt;0,\n\\] \\[\nh(t)=\\frac{\\nu}{\\alpha}\\left(\\frac{t}{\\alpha}\\right)^{\\nu-1}1_{\\left\\{t&gt;0\\right\\}}\n\\] をはじめとして，Gamma 分布，対数正規分布，対数ロジスティック分布分布，パレート分布など，極値分布の指数変換様の分布族が用いられる．\n\n\n1.3.3 ノンパラメトリックモデル\nこれには (Kaplan and Meier, 1958) や (Cutler and Ederer, 1958) の方法が古来より有名である．\n\n\n\n1.4 回帰\n\n1.4.1 ハザードの回帰\n生存時間解析の主な目的は，生存曲線の正確な描画というより，生存時間を決定する要因の特定にある．\n(Cox, 1972) はベースとなる生存関数 \\(S_0\\) (baseline survival curve) を局外母数として，\\(S\\) と \\(S_0\\) の関係をパラメトリックにモデリングする．\nより正確には，ハザード関数 \\(h\\) のベース \\(h_0\\) からの比の対数を，線型な予測子 \\[\n\\log \\frac{h(y)}{h_0(y)}=X^\\top\\beta+\\epsilon\n\\] によってモデリングする．\n一般にハザードの比を \\[\nh(y|x)=h_0(y)c(X^\\top\\beta)\n\\] とモデリングするものを 乗法的ハザードモデル といい，特に \\(c=\\exp\\) と取った場合を Cox の 比例ハザードモデル (Cox’s proportional hazard model) とも呼ばれる．\nこの方法は，打ち切りデータへの対処が簡便になることが美点である．\n\n\n1.4.2 生存時間の回帰\n生存時間 \\(Y\\) の対数を直接 \\[\n\\log Y=\\mu+\\beta^\\top X+\\epsilon\n\\] によってモデリングする方法は 加速故障時間モデル (AFT: Accelerated Failure Time Model) (Wei, 1992) と呼ばれる．\nこのモデルはハザード関数に，\\(h_0\\) を \\(x=0\\) の場合のハザード関数として \\[\nh(y|x)=e^{-\\beta^\\top x}h_0(ye^{-\\beta^\\top x}),\n\\] という乗法的な仮定をおいていることに相当する．\n\n\n\n1.5 生存関数推定再論\n一方で医療行為の社会的な影響も考える HTA の目標を達成するためには，式 (1) のような量を計算する必要がある．\nそのためには，生存曲線の推定と同時に打ち切り時点以降の外挿もできるようなモデルを考える必要があるが，Kaplan-Meier 法などのノンパラメトリック法は（現状）この用途には用いることができない．\n表現力が高いパラメトリックモデルをベイズ推定することが，非常に魅力的な解決策として考えられ，実際 NICE のガイドラインでも推奨されている (Latimer, 2011)．\nその際の魅力的なモデルに polyhazard model (Berger and Sun, 1993), (Louzada-Neto, 1999) がある．\n(Latimer, 2013) では現状の HTA 分析では，生存時間モデルに対してモデル検証・モデル選択が不十分であることに警鐘が鳴らされている．\npolyhazard model のような階層モデルを効率的にベイズ推定・モデル平均化ができるような MCMC 法が開発されることは，このモデル検証の手続きを自動化したり，より手軽にするために非常に重要である．\n\n\n1.6 多ハザードモデルの表現力\nPolyhazard model もハザード関数をモデリングするが， \\[\nh_Y(y)=\\sum_{j=1}^Kh_j(y)\n\\] という形でモデリングし，個々の \\(h_j\\) にパラメトリックな仮定をおく．\n仮に \\(h_j\\) として，形状母数 \\(\\nu&gt;0\\) と位置母数 \\(\\mu:=\\alpha^{-\\nu}&gt;0\\) を持つ Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) のハザード関数2 \\[\nh_{\\mathrm{W}}(y):=\\mu\\nu y^{\\nu-1}\n\\] と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) のハザード関数 \\[\nh_{\\mathrm{LL}}(y):=\\frac{\\left(\\frac{\\nu}{\\mu}\\right)\\left(\\frac{y}{\\mu}\\right)^{\\nu-1}}{1+\\left(\\frac{y}{\\mu}\\right)^\\nu}\n\\] の２つのみを考えたとしても，複数のパラメトリックモデルを足し合わせることで驚異的な表現力を達成することができる．\n\n\n\n(Hardcastle et al., 2024, p. 5) より．\n\n\n\n\n1.7 競合リスク解析\n\n1.7.1 モデルの解釈\npolyhazard モデルでは各 \\(h_k\\) の前に係数がついていない点には注意が必要である．\n各 \\(h_k\\) は実在の原因のハザードを表しており，各被験者は同時に \\(K\\) 個のリスクに晒されているというモデルである（第 1.2 節）．\nこのようなモデルを 競合リスクモデル (competing risk model) ともいう．\n例えば，心臓の移植後のハザード曲線はバスタブ曲線の形を持ち，少なくとも２つの別々の競合するリスク要因が存在することが窺える (Demiris et al., 2015)．\n\n\n1.7.2 競合リスクモデリング\nしかし，リスク因子はほとんどの場合観測できず，潜在変数となる．\\(K\\) の数も不確実である．\nこのような識別不可能なモデルは，階層モデルとしてベイズ推論を実行することが向いていると言えるかも知れない．\nさらに発展的なモデルにはマルチステートモデルもある (齋藤哲雄 and 室谷健太, 2023), (Saito and Murotani, 2024)．\n\n\n\n1.8 ベイズ階層多ハザードモデル\n(Hardcastle et al., 2024) では HTA への応用を念頭に，完全なベイズ階層多ハザードモデルの推定を試みている．ここではそのモデルの詳細を紹介する．\n\n1.8.1 第１階層\n各個別要因 \\(k\\in[K]\\) の形状母数 \\(\\nu_k\\) と位置母数 \\(\\mu_k\\) に階層構造 \\[\n\\log(\\nu_k)=\\alpha_k\\sim\\mathrm{N}(0,\\sigma_\\alpha^2)\n\\tag{2}\\] \\[\n\\log(\\mu_k)=\\beta_{k,0}+\\sum_{j\\in\\left\\{j\\in[p]\\mid\\gamma_{kj}=1\\right\\}}x_j\\beta_{k,j},\\qquad\\beta_{k,0}\\sim\\mathrm{N}(0,\\sigma_{\\beta_0}^2)\n\\tag{3}\\] を考える．ただし，\\(\\gamma_{k,j}\\in2\\) は共変量 \\(x_j\\) が \\(k\\in[K]\\) 番目の部分モデルに参加するかどうかを決める指示変数とする．\n残っているパラメータ \\(\\beta_{k,j}\\;(j\\in[p])\\) には \\[\n\\beta_{k,j}\\sim(1-\\omega)\\delta_0+\\omega\\operatorname{N}(0,\\sigma_\\beta^2)\n\\] と spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定し，変数選択を促進する．\n以降，\\(\\theta_k=(\\nu_k,\\mu_k)\\) とし，\\((K,\\gamma,\\theta)\\) を本モデルのパラメータとする．\n\n\n1.8.2 第二階層\n\\(\\sigma_\\alpha^2=2,\\sigma_{\\beta_0}^2=5\\) は固定してしまうと，\\(\\phi:=(\\omega,\\sigma_\\beta)\\) がハイパーパラメータとして残っている．これには \\[\n\\omega\\sim\\operatorname{Beta}(a,b)\n\\] \\[\n\\sigma_\\beta\\sim\\operatorname{HalfCauchy}(0,1)\n\\] という事前分布をおく．\n前者はモデルのサイズについて Beta-二項分布を仮定することに等価である (3.1 節 Ley and Steel, 2009)．後者は (Gelman, 2006), (Polson and Scott, 2012) の推奨の通りである．\n\n\n1.8.3 \\(\\mathcal{P}(E)\\) 上の事前分布\n実はまだ第一階層のパラメータが残っている．ハザードの数 \\(K\\) と \\(h_k\\) の関数形をどうするかである．\nここでは Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) の２つ \\[\nD=\\{\\mathrm{W}(\\nu,\\mu),\\mathrm{LL}(\\nu,\\mu)\\}\n\\] から等確率で \\[\nK\\sim\\mathrm{Pois}_{&gt;0}(\\xi)\n\\] 個選ぶこととする．\nハイパーパラメータ \\(\\xi\\) については，(Hardcastle et al., 2024) では \\(\\xi=2\\) としている．この根拠は \\[\n\\operatorname{P}[K&gt;4]\\approx0.061\n\\] であることとしている．\n\n\n1.8.4 部分的な階層モデル\n元々部分的に階層化された多ハザードモデルは広く考えられていた．\n(Demiris et al., 2015) は \\(K\\in\\{1,2,3,4\\}\\) の poly-Weibull モデルを推定し，その適合具合を比較した．\n(Benaglia et al., 2015) では \\(K=2\\) に限ったが，bi-Weibull モデルと bi-Gompertz モデルを推定し，視覚化手法によりモデルの適合具合を比較した．\nこのように個別のモデルを推定してベストのものを選び出す方法は，モデルの仮定を緩和して多くの変数を動かすようにすればするほど難しくなっていく．\n例えば分布族も種々のものを考え，\\(K\\in\\{1,2,3,4\\}\\) のいずれも考えるとなると，推定すべき個別のモデルは乗法的に増加していく．\nそこで (Negrín et al., 2017) は，モデルを「比較」して「選択」するよりもむしろ，ベイズモデル平均 (BMA: Bayesian Model Averaging) を用いることでモデルの不確実性を考慮しつつ最終的なモデルを得ることを提案した．\nしかし，そのモデルは \\(K=2\\) の Weibull 分布族にのみ限っていた．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#pdmp-によるベイズ競合リスク分析",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#pdmp-によるベイズ競合リスク分析",
    "title": "ベイズ生存時間解析",
    "section": "2 PDMP によるベイズ競合リスク分析",
    "text": "2 PDMP によるベイズ競合リスク分析\n(Hardcastle et al., 2024) では完全なベイズ階層 polyhazard モデルに対するサンプラーを Zig-Zag サンプラーに基づいて構成している．ここではこれを紹介する．\n\n2.1 課題\n完全なベイズ階層多ハザードモデルが考えられていなかったことは，次の３つの問題によると考えられる：\n\n\n\n\n\n\nZig-Zag サンプラーが挑む課題\n\n\n\n\n次元の変化 (trans-dimensionality)\n\nラベル \\(k\\in[K]\\) が変わる毎に共変量の数が違い，従って \\(\\beta_k\\) の次元が変わる．従ってサンプラーは，次元の異なる空間の間を往復する必要がある．\n\n尤度の変化\n\n加えて関数空間上にも事前分布をおいているため，\\(h_k\\) の関数形も途中で変わる（第 1.8.3 節）．これに伴い，サンプリングするべき尤度の形も変わってしまう．\n\n識別不可能性\n\nこのモデルは潜在的な \\(K\\) 個のリスクを仮定しており，全てに同等な同等な仮定をおいているため当然識別不可能である．その上事後分布が \\(K!\\) 個の峰を持つ対称性を持ってしまう．このことは MCMC の収束を遅くする上に，最終的に得られる推定量を変に平均化してしまう恐れがある．\n\n\nそこで局外母数 \\((K,D,\\gamma,\\phi)\\) と推定対象 \\(\\theta\\in\\mathbb{R}^{2K+\\lvert\\gamma\\rvert_1}\\) のサンプリングに，多峰性分布につよい Zig-Zag サンプラーを用いる．\n\n\n2.2 \\(\\theta\\) の Zig-Zag サンプリング\nZig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\nモデルが複雑であることに加えて尤度が変動することもあり，Poisson 点過程に対する解析的に上界が見つかるはずもないため，Automatic Zig-Zag (Corbella et al., 2022) を Concave-Convex PDMP (Sutton and Fearnhead, 2023) で修正して用いる．\n続きは次稿参照：\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#文献紹介",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#文献紹介",
    "title": "ベイズ生存時間解析",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n生存時間解析について (武冨奈菜美 and 山本和嬉, 2023) は網羅的で入門的な日本語文献である．\n(西川正子, 2008) は競合リスクモデルに焦点を当てた日本語文献である．(齋藤哲雄 and 室谷健太, 2023), (Saito and Murotani, 2024) はマルチステートモデルも扱っている．(森満, 2016) は医学者による説明が与えられている．\n本項は (Hardcastle et al., 2024) を大きく参考にした．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#footnotes",
    "title": "ベイズ生存時間解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(武冨奈菜美 and 山本和嬉, 2023) は Halley の生命表や Daniel Bernoulli の競合リスク解析の例を挙げている．生命表については ベイズ計算の稿 も参照．↩︎\n位置母数 \\(\\mu:=\\alpha^{-\\nu}&gt;0\\) の変換により第 1.3.2 節のハザード関数と見た目が異なることに注意．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html",
    "title": "ベイズ階層多ハザードモデル",
    "section": "",
    "text": "生存曲線のベイズ階層モデルによる外挿\n\n\n\n\n\n\n\nNo matching items\n\n\nZig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#前稿",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#前稿",
    "title": "ベイズ階層多ハザードモデル",
    "section": "",
    "text": "生存曲線のベイズ階層モデルによる外挿\n\n\n\n\n\n\n\nNo matching items\n\n\nZig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#ベイズ階層多ハザードモデル",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#ベイズ階層多ハザードモデル",
    "title": "ベイズ階層多ハザードモデル",
    "section": "1 ベイズ階層多ハザードモデル",
    "text": "1 ベイズ階層多ハザードモデル\n\n1.1 多ハザードモデルの表現力\nPolyhazard model もハザード関数をモデリングするが， \\[\nh_Y(y)=\\sum_{j=1}^Kh_j(y)\n\\] という形でモデリングし，個々の \\(h_j\\) にパラメトリックな仮定をおく．\n仮に \\(h_j\\) として，形状母数 \\(\\nu&gt;0\\) と位置母数 \\(\\mu:=\\alpha^{-\\nu}&gt;0\\) を持つ Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) のハザード関数 \\[\nh_{\\mathrm{W}}(y):=\\mu\\nu y^{\\nu-1}\n\\] と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) のハザード関数 \\[\nh_{\\mathrm{LL}}(y):=\\frac{\\left(\\frac{\\nu}{\\mu}\\right)\\left(\\frac{y}{\\mu}\\right)^{\\nu-1}}{1+\\left(\\frac{y}{\\mu}\\right)^\\nu}\n\\] の２つのみを考えたとしても，複数のパラメトリックモデルを足し合わせることで驚異的な表現力を達成することができる．\n\n\n\n(Hardcastle et al., 2024, p. 5) より．\n\n\n\n\n1.2 ベイズ階層多ハザードモデル\n(Hardcastle et al., 2024) では HTA への応用を念頭に，完全なベイズ階層多ハザードモデルの推定を試みている．\n\n1.2.1 第１階層\n各個別要因 \\(k\\in[K]\\) の形状母数 \\(\\nu_k\\) と位置母数 \\(\\mu_k\\) に階層構造 \\[\n\\log(\\nu_k)=\\alpha_k\\sim\\mathrm{N}(0,\\sigma_\\alpha^2)\n\\tag{1}\\] \\[\n\\log(\\mu_k)=\\beta_{k,0}+\\sum_{j\\in\\left\\{j\\in[p]\\mid\\gamma_{kj}=1\\right\\}}x_j\\beta_{k,j},\\qquad\\beta_{k,0}\\sim\\mathrm{N}(0,\\sigma_{\\beta_0}^2)\n\\tag{2}\\] を考える．ただし，\\(\\gamma_{k,j}\\in2\\) は共変量 \\(x_j\\) が \\(k\\in[K]\\) 番目の部分モデルに参加するかどうかを決める指示変数とする．\n式 (2) で残っているパラメータ \\(\\beta_{k,j}\\;(j\\in[p])\\) には \\[\n\\beta_{k,j}\\sim(1-\\omega)\\delta_0+\\omega\\operatorname{N}(0,\\sigma_\\beta^2)\n\\] と spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定し，変数選択を促進する．選択された変数については \\(\\gamma_{k,j}=1\\) とする．\n以降，\\(\\theta_k=(\\nu_k,\\mu_k)\\) とし，\\((K,\\gamma,\\theta)\\) を本モデルのパラメータと理解する（\\(K\\) の事前分布は後述 1.2.3）．\n\n\n1.2.2 第２階層\n\\(\\sigma_\\alpha^2=2,\\sigma_{\\beta_0}^2=5\\) は固定してしまうと，\\(\\phi:=(\\omega,\\sigma_\\beta)\\) がハイパーパラメータとして残っている．これには \\[\n\\omega\\sim\\operatorname{Beta}(a,b)\n\\] \\[\n\\sigma_\\beta\\sim\\operatorname{HalfCauchy}(0,1)\n\\] という事前分布をおき，\\(a=b=4\\) と固定する．\n前者はモデルのサイズについて Beta-二項分布を仮定することに等価である (3.1 節 Ley and Steel, 2009)．後者は (Gelman, 2006), (Polson and Scott, 2012) の推奨の通りである．\n\n\n1.2.3 \\(\\mathcal{P}(E)\\) 上の事前分布\n実はまだ第一階層のパラメータが残っている．ハザードの数 \\(K\\) と \\(h_k\\) の関数形をどうするかである．\nここでは Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) の２つ \\[\nD=\\{\\mathrm{W}(\\nu,\\mu),\\mathrm{LL}(\\nu,\\mu)\\}\n\\] から等確率で \\[\nK\\sim\\mathrm{Pois}_{&gt;0}(\\xi)\n\\] 個選ぶこととする．\nハイパーパラメータ \\(\\xi\\) については，(Hardcastle et al., 2024) では \\(\\xi=2\\) としている．その根拠の１つに \\[\n\\operatorname{P}[K\\ge5]\\approx0.061\n\\] を満たす性質が，実際の応用で５つ以上の競合リスクが存在する場面は稀であることに整合することを挙げている："
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#pdmp-によるベイズ競合リスク分析",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#pdmp-によるベイズ競合リスク分析",
    "title": "ベイズ階層多ハザードモデル",
    "section": "2 PDMP によるベイズ競合リスク分析",
    "text": "2 PDMP によるベイズ競合リスク分析\n\n2.1 はじめに\n前節で定義されたモデルのパラメータ空間 \\[\n(K,D,\\gamma,\\phi,\\theta)\n\\] 上での事後分布サンプリングを行うことを考える．\\(\\phi=(\\omega,\\sigma_\\beta)\\) はハイパーパラメータである．\n基本的には \\(\\theta=(\\theta_k)=(\\nu_k,\\mu_k)\\) の事後分布サンプリングを考えるのであるが，\\(K,\\gamma\\) の如何によって \\(\\theta\\) の次元が変化する．\n加えて \\(D\\) の如何によって尤度が変化するから，Poisson 点過程の強度関数に解析的な上界が見つかるはずもないため，Automatic Zig-Zag (Corbella et al., 2022) と Concave-Convex PDMP (Sutton and Fearnhead, 2023) を組み合わせて用いる．\n\n\n2.2 \\(\\theta\\) の Zig-Zag サンプリング\npolyhazard model の標準的なサンプラーには Gibbs サンプラーや NUTS サンプラーがあり得るが，いずれも複数の次元の間を飛び回れるように拡張するには，空間の間の跳躍をうまく設計する必要がある．\nこの点で Zig-Zag サンプラーは従来のサンプリング法と対等であるが，今回の設定には多峰性の懸念も存在する．\nというのも，\\(D\\) によって指定される分布は互いに交換可能であるため，混合モデリングにおける label switching problem (Jasra et al., 2005) 同様に事後分布は必然的に多峰性（対称性）を帯びるはずである．\n多峰性への対処という点では，Zig-Zag サンプラーに軍配が上がるはずである (Andrieu and Livingstone, 2021)．\n\n\n2.3 ハイパーパラメータ \\(\\phi\\) のサンプリング\n\\((\\theta,\\phi)\\) 上から結合分布を Zig-Zag サンプリングすることも可能であるが，\\(\\theta\\) は \\(\\phi\\) に依存するため，事後分布は強い相関構造を持つと予想される．\n一方でこの条件付き構造は Gibbs サンプラーの発想で有効に利用したいものである．これには Zig-Zag within Gibbs (Sachs et al., 2023) を用いることができる．\nこの方法によれば，ハイパーパラメータ \\(\\phi=(\\omega,\\sigma_\\beta)\\) を固定した下で他の変数をサンプリングすることになる．この設定では次に論じるように変数選択の指示変数 \\(\\gamma\\) に対する効率的なシミュレーション法も導く．\n\n\n2.4 変数選択 \\(\\gamma\\) のサンプリング\n一般にベイズ変数選択は \\(\\gamma\\) のような指示変数のサンプリングに問題を帰着させる方法 (Zanella, 2020) が state-of-the-art とされる．\nしかし各 \\(\\gamma\\in 2^{Kp}\\) に対応するモデルの（周辺）尤度の計算が困難な場面は多く，特に今回の生存モデルはその一例である (Liang et al., 2023)．\nしかし Zig-Zag サンプラーでは \\(\\{\\beta_{k,j}=0\\}\\) のなす部分空間を通過する際に一定の確率でランダムな時間ここに囚われることで，自然な形で trans-dimensional なサンプリングが可能になることが (Chevallier et al., 2023), (Bierkens et al., 2023) により同時に提案されている．\nこの際 \\(\\{\\beta_{k,j}=0\\}\\) から脱出する確率は時刻や位置に依らず（ハイパーパラメータ \\(\\omega\\) の下で）一定であり，一様な Poisson 点過程のシミュレーションにより実現される．この意味で Zig-Zag サンプラーによる変数選択は，周辺尤度の計算を回避した効率的な計算法を提供する．\n\n\n2.5 \\(K,D\\) のサンプリング\nここでは birth-death-swap 過程を用いる．というのも，３つの Poisson 点過程 \\(\\Lambda^b,\\Lambda^d,\\Lambda^s\\) を用いて，これが到着するごとに確率核 \\(q_b,q_d,q_s\\) に従って新たなハザード関数の追加・消去・取り替えが起こる．\n取り替えは必ずしも必要がないが，これの追加により多峰性の問題 2.2 が解決され，サンプラーの収束が改善されるという．\nbirth-death 過程は次の詳細釣り合い条件を満たせば良い： \\[\n\\Lambda^b(t)\\pi(\\theta,D,K)q_b(u)=\\Lambda^d(t)\\pi(\\theta',D',K')q_d(u').\n\\]\nこの方法は Zig-Zag within Gibbs (Sachs et al., 2023) の Gibbs 核を，(Green, 1995) の超次元跳躍核に取り替えていることに等しい．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html",
    "href": "posts/2024/Julia/PDMPFlux.html",
    "title": "PDMPFlux.jl パッケージ",
    "section": "",
    "text": "PDMP (Piecewise Deterministic Markov Process) または 連続時間 MCMC とは，名前の通り MCMC のサンプリングを連続時間で行うアルゴリズムである：\n\n\n\nZig-Zag サンプラーが２次元の正規分布からサンプリングを実行している様子\n\n\n\n\n\n\n\n\nPDMP サンプラーの特徴\n\n\n\n\n方向転換をするオレンジ色の点を一定の法則（Poisson 点過程）に従って定めることで，軌跡全体が目標の分布に従う．\n目標の関数を緑色の軌跡上で線積分をすれば期待値が得られる．一定のステップサイズで切り出してサンプルとし，従来の MCMC output と同様に使っても良い．\n非可逆 なダイナミクスをもち，HMC や MALA などの可逆なサンプラーよりも高次元分布や多様性に強いと期待されている．\n\n\n\n詳しくは次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nusing PDMPFlux\n\n\n\n\n\n\n自動微分を使う場合（推奨）\n\n\n\n\nまずポテンシャル＝負の対数尤度を（定数倍の違いを除いて）自分で定義する：\nfunction U_Gauss(x::Vector)\n    return sum(x.^2) / 2\nend\n標準正規分布 \\(\\mathrm{N}_d(0,I_d)\\) だと，対数尤度は \\[\n\\log\\phi(x) = - \\frac{1}{2}\\sum_{i=1}^d x_i^2 - \\frac{d}{2}\\log(2\\pi)\n\\] 第二項は定数であるから無視して良い．\\(U(x) = -\\log\\phi(x)\\) とすべき点に注意（先頭のマイナス符号は落とす）．\n次のようにしてサンプラーをインスタンス化する：\ndim = 10\nsampler = ZigZagAD(dim, U_Gauss)\nハイパーパラメータを与えてサンプリングを実行する．\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\n結果の可視化を行う．\njointplot(samples)\n\n\n\n\n\n\n\n\n\n\n自動微分を使わず，ポテンシャルの勾配を手動で与える場合\n\n\n\n\n\nsampler = ZigZag(dim, grad_U, grid_size=grid_size)  # initialize your Zig-Zag sampler\noutput = sample_skeleton(sampler, N_sk, xinit, vinit, verbose = true)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, output)  # get samples from the skeleton points\nとできる．\noutput::PDMPHistory にはサンプラーの挙動を確認するための多くのメソッドが定義されている：\nplot_traj(output, 10000)\ndiagnostic(output)"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#連続時間-mcmc-について",
    "href": "posts/2024/Julia/PDMPFlux.html#連続時間-mcmc-について",
    "title": "PDMPFlux.jl パッケージ",
    "section": "",
    "text": "連続時間 MCMC とは名前の通り MCMC のサンプリングを連続時間で行うアルゴリズムである：\n\n\n\nZig-Zag サンプラーが２次元の正規分布からサンプリングを実行している様子\n\n\n方向転換をするオレンジ色の点を一定の法則に従って定めることで，軌跡全体が目標の分布に従う．\nすなわち，目標の関数を緑色の軌跡上で線積分をすれば期待値が得られる．\n詳しくは次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nusing PDMPFlux\n\n\n\n\n\n\n自動微分を使う場合（推奨）\n\n\n\n\nまずポテンシャル＝負の対数尤度を（定数倍の違いを除いて）自分で定義する：\nfunction U_Gauss(x::Vector)\n    return sum(x.^2) / 2\nend\n標準正規分布 \\(\\mathrm{N}_d(0,I_d)\\) だと，対数尤度は \\[\n\\log\\phi(x) = - \\frac{1}{2}\\sum_{i=1}^d x_i^2 - \\frac{d}{2}\\log(2\\pi)\n\\] 第二項は定数であるから無視して良い．\\(U(x) = -\\log\\phi(x)\\) とすべき点に注意（先頭のマイナス符号は落とす）．\n次のようにしてサンプラーをインスタンス化する：\ndim = 10\nsampler = ZigZagAD(dim, U_Gauss)\nハイパーパラメータを与えてサンプリングを実行する．\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\n結果の可視化を行う．\njointplot(samples)\n\n\n\n\n\n\n\n\n\n\n自動微分を使わず，ポテンシャルの勾配を手動で与える場合\n\n\n\n\n\nsampler = ZigZag(dim, grad_U, grid_size=grid_size)  # initialize your Zig-Zag sampler\noutput = sample_skeleton(sampler, N_sk, xinit, vinit, verbose = true)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, output)  # get samples from the skeleton points\nとできる．\noutput::PDMPHistory にはサンプラーの挙動を確認するための多くのメソッドが定義されている：\nplot_traj(output, 10000)\ndiagnostic(output)"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#pdmp_jax-パッケージ",
    "href": "posts/2024/Julia/PDMPFlux.html#pdmp_jax-パッケージ",
    "title": "PDMPFlux.jl パッケージ",
    "section": "2 pdmp_jax パッケージ",
    "text": "2 pdmp_jax パッケージ\n本節では PDMP のシミュレーションに自動微分を応用した (Andral and Kamatani, 2024) のアルゴリズムを紹介する．\n\n2.1 デモ\n(Neal, 2003) が slice sampling のデモ用に定義した 漏斗分布 を考える： \\[\np(y,x)=\\phi(y;0,3)\\prod_{i=1}^9\\phi(x_i;0,e^{y/2}),\\qquad y\\in\\mathbb{R},x\\in\\mathbb{R}^9.\n\\]\nimport jax\nfrom jax.scipy.stats import multivariate_normal\nfrom jax.scipy.stats import norm\n\nimport jax.numpy as jnp\n\ndef funnel(d=10, sig=3, clip_y=11):\n  \"\"\"Funnel distribution for testing. Returns energy and sample functions.\"\"\"\n\n  def unbatched(x):\n    y = x[0]\n    log_density_y = - y**2 / 6\n\n    variance_other = jnp.exp(y/2)\n\n    log_density_other = - jnp.sum(x[1:]**2) / (2 * variance_other)\n\n    return - log_density_y - log_density_other\n\n  def sample_data(n_samples):\n    # sample from Nd funnel distribution\n    y = (sig * jnp.array(np.random.randn(n_samples, 1))).clip(-clip_y, clip_y)\n    x = jnp.array(np.random.randn(n_samples, d - 1)) * jnp.exp(-y / 2)\n    return jnp.concatenate((y, x), axis=1)\n\n  return unbatched, sample_data\nimport pdmp_jax as pdmp\ndim = 10\nU, _ = funnel(d=dim)\ngrad_U = jax.grad(U)\nseed = 8\nxinit = jnp.ones((dim,)) # initial position\nvinit = jnp.ones((dim,))  # initial velocity\ngrid_size = 0\nN_sk = 100000 # number of skeleton points\nN = 100000 # number of samples\nsampler = pdmp.ZigZag(dim, grad_U, grid_size)\n# sample the skeleton of the process\nout = sampler.sample_skeleton(N_sk, xinit, vinit, seed, verbose = True)  # takes only 3 seconds on my M1 Mac\n# sample from the skeleton\nsample = sampler.sample_from_skeleton(N,out)\nimport seaborn as sns\nsns.jointplot(x = sample[:,0],y = sample[:,1])\nplt.show()\n\n\nnumber of error bound : 46817\n\n\n2.2 サンプリングループの構造\n\n\n\n\n\n\ngraph TD;\n  A[\"one_step()\"] --&gt;|state.indicator=false| B[\"one_step_while()\"]\n  B --&gt; C{tp &lt;= state.horizon}\n    C --&gt;|No| D[move_to_horizon]\n    C --&gt;|Yes| E[move_before_horizon]\n    E --&gt;|state.accept=false| G[inner_while]\n    G --&gt; I{ar &lt;= 1.0}\n    I --&gt;|No| J[error_acceptance]\n    I --&gt;|Yes| K[ok_acceptance]\n    K --&gt;|reject| L[\"if_reject()\"]\n    L --&gt;|場合によっては move_to_horizon2| N[\"inner_while() か one_step_while() まで戻る\"]\n    K --&gt;|accept| O[\"if_accept()\n    state.indicator=true\n    \"]\n    D --&gt; B\n    J --&gt;|horizon を縮める| N\n\n\n\n\n\n\n\nar=lambda_t/state.lambda_bar によって Poisson thinning を行う．\nただし，lambda_bar とは近似的な上界であり，「最も近い直前の grid 上の点での値」でしかない．当然 lambda_t を超過し得る．そのような場合に error_acceptance() に入る．\nerror_acceptance() に入った場合，horizon を縮めてより慎重に同じ区間を Poisson thinning しなおす．adaptive=true の場合はこのタイミングで horizon を恒久的に縮める．\n最後 if_reject() に入った場合，horizon に到達したら one_step_while() まで戻るが，そうでない場合は inner_while() まで戻る実装がなされている．\n\n\n\n\n2.3 適応的なステップサイズ"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#pdmpflux.jl-パッケージ",
    "href": "posts/2024/Julia/PDMPFlux.html#pdmpflux.jl-パッケージ",
    "title": "PDMPFlux.jl パッケージ",
    "section": "3 PDMPFlux.jl パッケージ",
    "text": "3 PDMPFlux.jl パッケージ\n\n3.1 デモ\n\nusing PDMPFlux\n\nusing Random, Distributions, Plots, LaTeXStrings, Zygote, LinearAlgebra\n\n\"\"\"\n    Funnel distribution for testing. Returns energy and sample functions.\n    For reference, see Neal, R. M. (2003). Slice sampling. The Annals of Statistics, 31(3), 705–767.\n\"\"\"\nfunction funnel(d::Int=10, σ::Float64=3.0, clip_y::Int=11)\n\n    function neg_energy(x::Vector{Float64})\n        v = x[1]\n        log_density_v = logpdf(Normal(0.0, 3.0), v)\n        variance_other = exp(v)\n        other_dim = d - 1\n        cov_other = I * variance_other\n        mean_other = zeros(other_dim)\n        log_density_other = logpdf(MvNormal(mean_other, cov_other), x[2:end])\n        return - log_density_v - log_density_other\n    end\n\n    function sample_data(n_samples::Int)\n        # sample from Nd funnel distribution\n        y = clamp.(σ * randn(n_samples, 1), -clip_y, clip_y)\n        x = randn(n_samples, d - 1) .* exp.(-y / 2)\n        return hcat(y, x)\n    end\n\n    return neg_energy, sample_data\nend\n\nfunction plot_funnel(d::Int=10, n_samples::Int=10000)\n    _, sample_data = funnel(d)\n    data = sample_data(n_samples)\n\n    # 最初の2次元を抽出（yとx1）\n    y = data[:, 1]\n    x1 = data[:, 2]\n\n    # 散布図をプロット\n    scatter(y, x1, alpha=0.5, markersize=1, xlabel=L\"y\", ylabel=L\"x_1\", \n            title=\"Funnel Distribution (First Two Dimensions' Ground Truth)\", grid=true, legend=false, color=\"#78C2AD\")\n\n    # xlim と ylim を追加\n    xlims!(-8, 8)  # x軸の範囲を -8 から 8 に設定\n    ylims!(-7, 7)  # y軸の範囲を -7 から 7 に設定\nend\nplot_funnel()\n\nfunction run_ZigZag_on_funnel(N_sk::Int=100_000, N::Int=100_000, d::Int=10, verbose::Bool=false)\n    U, _ = funnel(d)\n    grad_U(x::Vector{Float64}) = gradient(U, x)[1]\n    xinit = ones(d)\n    vinit = ones(d)\n    seed = 2024\n    grid_size = 0  # constant bounds\n    sampler = ZigZag(d, grad_U, grid_size=grid_size)\n    out = sample_skeleton(sampler, N_sk, xinit, vinit, seed=seed, verbose = verbose)\n    samples = sample_from_skeleton(sampler, N, out)\n    return out, samples\nend\noutput, samples = run_ZigZag_on_funnel()  # ４分かかる\n\njointplot(samples)\n\n\n\nこのデモコードは Zygote.jl による自動微分を用いると 5:29 かかっていたところが，ForwardDiff.jl による自動微分を用いると 0:21 に短縮された．\n\n\n3.2 Zygote.jl と ForwardDiff.jl による自動微分\n\n\n\n\n\n\n\nJuliaDiff：Differentiation tools in Julia\nZygote (Docs / GitHub)\nForwardDiff (Docs / GitHub)\n\n\n\n\nZygote.jl は FluxML が開発する Julia の自動微分パッケージである．\n\nusing Zygote\n@time Zygote.gradient(x -&gt; 3x^2 + 2x + 1, 5)\n\n  0.830714 seconds (3.45 M allocations: 168.328 MiB, 3.95% gc time, 99.98% compilation time)\n\n\n(32.0,)\n\n\n\nf(x::Vector{Float64}) = 3x[1]^2 + 2x[2] + 1\ng(x) = Zygote.gradient(f,x)\ng([1.0,2.0])\n\n([6.0, 2.0],)\n\n\n大変柔軟な実装を持っており，広い Julia 関数を微分できる．\nForwardDiff.jl (Revels et al., 2016) は Zygote.jl よりも高速な自動微分を特徴としている．\n\nusing ForwardDiff\n@time ForwardDiff.derivative(x -&gt; 3x^2 + 2x + 1, 5)\n\n  0.068804 seconds (260.59 k allocations: 12.725 MiB, 16.89% gc time, 99.92% compilation time)\n\n\n32\n\n\nしかし定義域の次元が \\(100\\) 以上の場合は ReverseDiff.jl の方が高速になる．\n\n\n3.3 Brent の最適化\nOptim.jl は Julia の最適化パッケージであり，デフォルトで Brent の最適化アルゴリズムを提供する．\n\nusing Optim\nf(x) = (x-1)^2\nresult = optimize(f, 0.0, 1.0)\nresult.minimizer\n\n0.999999984947842\n\n\n\n\n3.4 StatsPlots.jl による可視化\nStatsPlots は現在 Plots.jl に統合されている．\nまた PDMPFlux.jl は marginalhist を wrap した jointplot() 関数を提供する．\n\n\n3.5 ProgressBars.jl による進捗表示\nProgressBars.jl は tqdm の Julia wrapper を提供する．PDMPFlux.jl ではこちらを採用して，サンプリングの実行進捗を表示する．\nなお ProgressMeter.jl も同様の機能を提供しており，有名な別の PDMP パッケージである ZigZagBoomerang.jl ではこちらを採用している．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#終わりに",
    "href": "posts/2024/Julia/PDMPFlux.html#終わりに",
    "title": "PDMPFlux.jl パッケージ",
    "section": "4 終わりに",
    "text": "4 終わりに\n\n今後の確率的プログラミングの１つの焦点は自動微分かもしれない．\n今回のパッケージ開発で，少なくとも v0.2.0 の時点では，プログラムに与える U_grad は多くの場合（10 次元の多変量 Gauss，50 次元の Banana など） Zygote.jl が少し速い（Funnel 分布では ForwardDiff.jl が速い）．\nしかし上界を構成する際の func の微分は ForwardDiff.jl の方が圧倒的に速い．大変に不可思議である．\nだから現在の実装は Zygote.jl と ForwardDiff.jl の両方を用いている．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#todo",
    "href": "posts/2024/Julia/PDMPFlux.html#todo",
    "title": "PDMPFlux.jl パッケージ",
    "section": "5 ToDo",
    "text": "5 ToDo\n\n\nZig-Zag 以外のサンプラーの実装\nZigZag(dim) は自動で知ってほしい\nTry clause 内の else を用いているので Julia 1.8 以上が必要．\nMCMCChains のような plot.jl を完成させる．\nPDMPFlux.jl のドキュメントを整備する．\nZigZagBoomerang.jl を見習って統合したり API をつけたり？\nTuring エコシステムと統合できたりしないか？\nRng を指定できるようにする？\npdmp-jax では 37 秒前後かかる Banana density の例が，PDMPFlux.jl では 2 分前後かかる．\n\nしかし，Julia の方が数値誤差が少ないのか，banana potential の対称性がうまく結果に出る．尾が消えたりしない．\n→ ForwardDiff.jl を採用したところ，02:05 から 10 分以上に変化した．ReverseDiff.jl を採用したところ 4:44 になった．50 次元というのが微妙なところなのかもしれない．\n\nFunnel 分布で試したところ，PDMPFlux.jl の棄却率が極めて高い．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#付録他に模索した可能性",
    "href": "posts/2024/Julia/PDMPFlux.html#付録他に模索した可能性",
    "title": "PDMPFlux.jl パッケージ",
    "section": "6 付録：他に模索した可能性",
    "text": "6 付録：他に模索した可能性"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#速習-pdmp",
    "href": "posts/2024/Julia/PDMPFlux.html#速習-pdmp",
    "title": "PDMPFlux.jl パッケージ",
    "section": "",
    "text": "PDMP (Piecewise Deterministic Markov Process) または 連続時間 MCMC とは，名前の通り MCMC のサンプリングを連続時間で行うアルゴリズムである：\n\n\n\nZig-Zag サンプラーが２次元の正規分布からサンプリングを実行している様子\n\n\n\n\n\n\n\n\nPDMP サンプラーの特徴\n\n\n\n\n方向転換をするオレンジ色の点を一定の法則（Poisson 点過程）に従って定めることで，軌跡全体が目標の分布に従う．\n目標の関数を緑色の軌跡上で線積分をすれば期待値が得られる．一定のステップサイズで切り出してサンプルとし，従来の MCMC output と同様に使っても良い．\n非可逆 なダイナミクスをもち，HMC や MALA などの可逆なサンプラーよりも高次元分布や多様性に強いと期待されている．\n\n\n\n詳しくは次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nusing PDMPFlux\n\n\n\n\n\n\n自動微分を使う場合（推奨）\n\n\n\n\nまずポテンシャル＝負の対数尤度を（定数倍の違いを除いて）自分で定義する：\nfunction U_Gauss(x::Vector)\n    return sum(x.^2) / 2\nend\n標準正規分布 \\(\\mathrm{N}_d(0,I_d)\\) だと，対数尤度は \\[\n\\log\\phi(x) = - \\frac{1}{2}\\sum_{i=1}^d x_i^2 - \\frac{d}{2}\\log(2\\pi)\n\\] 第二項は定数であるから無視して良い．\\(U(x) = -\\log\\phi(x)\\) とすべき点に注意（先頭のマイナス符号は落とす）．\n次のようにしてサンプラーをインスタンス化する：\ndim = 10\nsampler = ZigZagAD(dim, U_Gauss)\nハイパーパラメータを与えてサンプリングを実行する．\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\n結果の可視化を行う．\njointplot(samples)\n\n\n\n\n\n\n\n\n\n\n自動微分を使わず，ポテンシャルの勾配を手動で与える場合\n\n\n\n\n\nsampler = ZigZag(dim, grad_U, grid_size=grid_size)  # initialize your Zig-Zag sampler\noutput = sample_skeleton(sampler, N_sk, xinit, vinit, verbose = true)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, output)  # get samples from the skeleton points\nとできる．\noutput::PDMPHistory にはサンプラーの挙動を確認するための多くのメソッドが定義されている：\nplot_traj(output, 10000)\ndiagnostic(output)"
  },
  {
    "objectID": "posts/2024/Julia/Details.html",
    "href": "posts/2024/Julia/Details.html",
    "title": "Implementation Details of PDMPFlux.jl",
    "section": "",
    "text": "composites / constructs such as BoundBox, PDMPState, PDMPHistory are defined.\n\nBoundBox is basically a grid, together with the values on it, to perform thinning."
  },
  {
    "objectID": "posts/2024/Julia/Details.html#directory-structure-of-pdmpflux.jlsrc",
    "href": "posts/2024/Julia/Details.html#directory-structure-of-pdmpflux.jlsrc",
    "title": "Implementation Details of PDMPFlux.jl",
    "section": "",
    "text": "composites / constructs such as BoundBox, PDMPState, PDMPHistory are defined.\n\nBoundBox is basically a grid, together with the values on it, to perform thinning."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の実装",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の実装",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "3 Sticky PDMP の実装",
    "text": "3 Sticky PDMP の実装"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#実装",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#実装",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "4 実装",
    "text": "4 実装\n\n\nStickyZigZag.jl として実装したが，任意の PDMP サンプラーを受け取って Sticky 化する，というようなコードは書けないか？"
  }
]