[
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html",
    "href": "posts/2023/Probability/Beta-Gamma.html",
    "title": "確率測度の変換則",
    "section": "",
    "text": "Gamma 確率変数と，その変換として得る Beta 確率変数とに関する次の命題の証明を与える（第 3 節）．\n証明に先んじて，Gamma 分布と Beta 分布の性質を，それぞれ第 1 節と第 2 節で見ていく．\n最後に，確率分布の変換の計算方法をまとめる（第 4 節）．混合分布や複合分布など，特定の可微分変換で密度関数がどう変化するかの計算の基礎となる．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-Gamma",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-Gamma",
    "title": "確率測度の変換則",
    "section": "1 Gamma 分布を見る",
    "text": "1 Gamma 分布を見る\n\n1.1 定義\n\n\n\n\n\n\n定義（Gamma 分布）\n\n\n\n可測空間 \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) 上の Gamma分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\;(\\alpha,\\nu&gt;0)\\) とは， 密度関数 \\[g(x;\\alpha,\\nu):=\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}1_{\\left\\{x&gt;0\\right\\}}\\] が定める分布をいう．実際，\\(t=\\alpha x\\) と変数変換すると， \\[\n\\begin{align*}\n&\\quad\\int_0^\\infty \\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}dx\\\\\n&=\\alpha^\\nu\\int^\\infty_0\\left(\\frac{t}{\\alpha}\\right)^{\\nu-1}e^{-t}\\frac{dt}{\\alpha}\\\\&=\\int^\\infty_0t^{\\nu-1}e^{-t}dt=\\Gamma(\\nu).\\end{align*}\\]\n\n\n\n\n1.2 形状\n\\(\\alpha\\) をレートパラメータ（スケールパラメータと呼ばれるものの逆数），\\(\\nu\\) を形状パラメータともいう．レートパラメータが大きいほど突起も大きく，手前に寄る．形状パラメータ \\(\\nu\\) は分布の形状を大きく司る．\n実際，先度と歪度は形状パラメータのみに依って \\[\\gamma_1=\\frac{2}{\\sqrt{\\nu}},\\qquad\\gamma_2=3+\\frac{6}{\\nu},\\] と記述される．\nその意味するところを感得するために，scipy.statsでの実装を用いてプロットしてみる．\n\n\nコードを表示\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gamma\n\nnu = 1.5  # 形状パラメーター\n\n# Gamma分布のPDFをグリッド上で計算\nx = np.linspace(0, 8, 100)\npdf = gamma.pdf(x, nu)\n\n# プロットの実行\nplt.figure(figsize=(3.2, 4.8)) # スマホサイズに合わせる\nplt.plot(x, pdf)\nplt.title('Gamma(1,3/2) Distribution')\nplt.ylabel('Density')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\n\n\nレートパラメータを固定し，形状パラメータを残した \\[\\chi^2(k):=\\mathrm{Gamma}(1/2,k/2)\\] を自由度 \\(k\\) のカイ自乗分布ということに注意．\n\n\n\n\n\n\n\n\n\n最後に，レートパラメータが大きいほど突起が大きくなる様子は次の通り：\n\n\n\n\n\n\n\n\n\nなお，形状パラメータが \\(\\nu=1\\) である Gamma 分布のことを指数分布という： \\[\n\\operatorname{Exp}(\\gamma):=\\mathrm{Gamma}(\\gamma,1)\\;(\\gamma&gt;0)\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-Beta",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-Beta",
    "title": "確率測度の変換則",
    "section": "2 Beta分布を見る",
    "text": "2 Beta分布を見る\n\n2.1 定義\n\n\n\n\n\n\n定義（Beta 分布）\n\n\n\n可測空間 \\(((0,1),\\mathcal{B}((0,1)))\\)上の （第１種）ベータ分布 \\(\\mathrm{Beta}(\\alpha,\\beta)\\;(\\alpha,\\beta&gt;0)\\) とは， 密度関数 \\[\\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1_{(0,1)}(x)\\] が定める分布をいう．ただし，\\[B(\\alpha,\\beta)=\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}\\,dx.\\]\n\n\n\n\n2.2 形状\n次のような性質を持つ：1\n\n\n\n\n\n\n\n\\(\\alpha_1=\\alpha_2=1\\) のとき一様分布となり，\\(\\alpha_1=\\alpha_2&gt;1\\) の場合に左右対称な単峰性分布，\\(\\alpha_1=\\alpha_2&lt;1\\) の場合に左右対称な U 字型の二峰性分布を得る．\nいずれも \\(1\\) より大きい場合，左のパラメータが大きい場合 \\(\\alpha_1&gt;\\alpha_2&gt;1\\) 左に，右のパラメータが大きい場合 \\(\\alpha_2&gt;\\alpha_1&gt;1\\) 右に歪んだ単峰性分布を得る．\nいずれも \\(1\\) より小さい場合はその逆．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-proof",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-proof",
    "title": "確率測度の変換則",
    "section": "3 証明",
    "text": "3 証明\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{cases}\n    X_1=\\frac{Y_1}{Y_1+Y_2},\\\\\n    X_2=Y_1+Y_2.\n\\end{cases}\\] を逆に解くことで， \\[\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}=\\begin{pmatrix}x_1x_2\\\\x_2\\end{pmatrix}=:T(x_1,x_2)\\] を得る．\\(A:=(0,1)\\times(0,\\infty),B:=(0,\\infty)^2\\) と定めると，\\(T:A\\to B\\) は可微分同相で，Jacobianは \\[DT=\\begin{pmatrix}x_2&x_1\\\\0&1\\end{pmatrix},\\qquad J_T=x_2,\\] と計算でき，\\(A\\) 上で は消えない．\nよって \\((X_1,X_2)\\) の結合分布は \\[\n\\begin{align*}\n    &p(T(x_1,x_2))J_T(x_1,x_2)dx_1dx_2\\\\\n    &=\\frac{\\alpha^{\\nu_1}}{\\Gamma(\\nu_1)}y_1^{\\nu_1-1}e^{-\\alpha y_1}\\frac{\\alpha^{\\nu_2}}{\\Gamma(\\nu_2)}y_2^{\\nu_2-1}e^{-\\alpha y_2}\\\\\n    &\\qquad\\times\\frac{x_2}{(1-x_1)^2}\\,dx_1dx_2\\\\\n    &=\\frac{\\alpha^{\\nu_1}}{\\Gamma(\\nu_1)}x_1^{\\nu_1-1}x_2^{\\nu_1-1}e^{-\\alpha x_1x_2}\\frac{\\alpha^{\\nu_2}}{\\Gamma(\\nu_2)}x_2^{\\nu_2-1}\\\\\n    &\\qquad\\times(1-x_1)^{\\nu_2-1}e^{-\\alpha x_2(1-x_1)}x_2\\,dx_1dx_2\\\\\n    &=\\underbrace{\\frac{\\Gamma(\\nu_1+\\nu_2)}{\\Gamma(\\nu_1)\\Gamma(\\nu_2)}}_{=B(\\nu_1,\\nu_2)^{-1}}x_1^{\\nu_1-1}(1-x_1)^{\\nu_2-1}\\,dx_1\\\\\n    &\\qquad\\times\\frac{\\alpha^{\\nu_1+\\nu_2}}{\\Gamma(\\nu_1+\\nu_2)}x_2^{(\\nu_1+\\nu_2)-1}e^{-\\alpha x_2}\\,dx_2.\n\\end{align*}\n\\] これは \\(X_1\\) が \\(\\mathrm{Beta}(\\nu_1,\\nu_2)\\) に，\\(X_2\\) が \\(\\mathrm{Gamma}(\\alpha,\\nu_1+\\nu_2)\\) に独立に従った場合の密度になっている．\n\n\n\n\n3.1 余談\n総合研究大学院大学統計科学コース2018年8月実施の入試問題の第三問にて，本命題を背景とした問題が出題された．2\n\n\n\n\n\n\n第３問\n\n\n\n\n数直線 \\(\\mathbb{R}\\) 上の点Pの \\(x\\) 座標 \\(X\\) は \\(\\mathrm{N}(0,1)\\) に従うとする． Pの原点からの距離の自乗の確率密度関数が \\[\\frac{1}{\\sqrt{2\\pi x}}e^{-\\frac{x}{2}},\\qquad(x&gt;0)\\] であることを示せ．\nEuclid空間 \\(\\mathbb{R}^n\\) 内の点Qの座標 \\((X_1,\\cdots,X_n)\\) は \\(\\mathrm{N}_n(0,I_n)\\) に従うとする． Qの原点からの距離の自乗の確率密度関数が \\[\\frac{1}{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}x^{\\frac{n}{2}-1}e^{-\\frac{x}{2}},\\qquad(x&gt;0)\\] であることを示せ．\n(2)の確率密度関数を持つ分布を \\(\\chi^2(n)\\) という． 確率変数 \\(X,Y\\) は独立で \\(X\\sim\\chi^2(n),Y\\sim\\chi^2(m)\\) であるとする．このとき， \\[X+Y\\sim\\chi^2(n+m),\\] \\[\\frac{X}{X+Y}\\sim\\mathrm{Beta}(n/2,m/2),\\] であり，互いに独立であることを示せ．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-transform",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-transform",
    "title": "確率測度の変換則",
    "section": "4 確率分布の変換則",
    "text": "4 確率分布の変換則\n\\(A,B\\subset\\mathbb{R}^d\\) を連結開集合，\\(C^1\\)-微分同相 \\(T:A\\overset{\\sim}{\\to}B\\) に対して，3 \\(B\\) 上の分布 \\(\\pi\\in\\mathcal{P}(B)\\) の \\(T\\) による引き戻し \\(T^*\\pi\\) の密度 \\(p^*\\) が，\\(\\pi\\) の密度 \\(p\\) と Jacobian \\(J_T(x)\\) の絶対値との積になる： \\[\np^*(x)=p(T(x))\\lvert J_T(x)\\rvert\\;\\;\\text{a.s.}\\quad(x\\in A).\n\\]\n\n\n\nCommutative diagram discribing current situations\n\n\n\n\n\n\n\n\n定理（変数変換）4\n\n\n\n\\(A,B\\subset\\mathbb{R}^d\\) を連結開集合，\\(T:A\\overset{\\sim}{\\to}B\\) を \\(C^1\\)-微分同相，\\(f:B\\to\\mathbb{R}\\) を Lebesgue 可測関数とする．\n\n\\(f\\circ T:A\\to\\mathbb{R}\\) も Lebesgue 可測．\n\\(f\\) は非負関数とする．このとき， \\[\n\\begin{align*}\n\\int_Af(T(x))\\lvert J_T(x)\\rvert\\,dx=\\int_Bf(y)\\,dy.\n\\end{align*}\n\\]\n\n\n\nこの定理より，任意の可測集合 \\(A_0\\in\\mathcal{B}(A)\\) に対して， \\[\n\\begin{align*}\n    \\int_{A_0}p^*(x)\\,dx&=(T^*\\pi)[A_0]\\\\\n    &=\\int_{T(A_0)}p(y)\\,dy\\\\\n    &=\\int_{A_0}p(T(x))\\lvert J_T(x)\\rvert\\,dx.\n\\end{align*}\n\\] ただし，最後の等号は定理による．5 \\(A_0\\in\\mathcal{B}(A)\\) は任意だったから， \\[\np^*(x)=p(T(x))\\lvert J_T(x)\\rvert\\;\\;\\text{a.s.}\\quad(x\\in A).\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#footnotes",
    "href": "posts/2023/Probability/Beta-Gamma.html#footnotes",
    "title": "確率測度の変換則",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Agresti, 2012) 第1.6.2節 Binomial Estimation: Beta and Logit-Normal Prior Distributions p.24 参照．↩︎\n過去9年分の入試問題の解答はこちらから↩︎\n記法 \\(\\overset{\\sim}{\\to}\\) は 記法一覧 参照．↩︎\n会田先生講義ノート 定理5.1 など参照．↩︎\n最後から2番目の等号は，測度の押し出し \\(T^*\\pi\\) の定義である．↩︎"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html",
    "href": "posts/2023/Particles/ParticleFilter.html",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "",
    "text": "Nicolas Chopin による逐次モンテカルロ法のための Python パッケージ particles の実装を参考に，NumPy, SciPy のみを用いて1から粒子フィルターを実装することで，その仕組みを理解することを目指す．"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#リサンプリングの実装",
    "href": "posts/2023/Particles/ParticleFilter.html#リサンプリングの実装",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "1 リサンプリングの実装",
    "text": "1 リサンプリングの実装\nまずリサンプリング法を実装する．今回は系統的リサンプリング法 (Carpenter et al., 1999) を用いることとする．1\n\n1.1 システマティックリサンプリング\n系統的リサンプリングとは，粒子数 \\(N\\) から \\(M\\) 個のサンプルを復元抽出する方法であって，次の2段階からなる．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n各区間 \\(\\left[\\frac{n-1}{M},\\frac{n}{M}\\right]\\subset[0,1]\\;(n\\in[M])\\) での変動の由来をただ一つのサンプル \\(U\\sim\\mathrm{U}([0,1])\\) から取ってしまい， \\[\nU^{(n)}:=\\frac{n-1+U}{N}\\quad(n\\in[M])\n\\] と乱数列を定める．\n正規化荷重 \\(\\{w^{(n)}\\}_{n=1}^N\\) が定める累積和 \\[\nF(n):=\\sum_{i=1}^nw^{(i)}\n\\] に対して，この一般化逆関数 \\(F^-:[0,1]\\to[N]\\) を通じて，\\(A^{n}:=F^{-1}(U^{(n)})\\;(n\\in[M])\\) をサンプルとする．\n\nこれにより，粒子の添字の対応 \\((1,\\cdots,N)\\mapsto A^{1:M}\\) が得られる．\n\n\nCode\nimport numpy as np\nfrom numba import jit\n\n@jit(nopython=True)\ndef inverse_cdf(su, W):\n    \"\"\"Inverse CDF algorithm for a finite distribution.\n    su: (M,) ndarray of sorted uniform variables\n    W: (N,) ndarray of normalized weights\"\"\"\n    j = 0\n    s = W[0]\n    M = su.shape[0]\n    A = np.empty(M, dtype=np.int64)\n    for n in range(M):\n        while su[n] &gt; s:\n            j += 1\n            s += W[j]\n        A[n] = j\n    return A\n\ndef systematic(W, M):\n    \"\"\"Systematic resampling\n    W: (N,) ndarray of normalized weights\n    M : number of resampled points\"\"\"\n    su = (random.rand(1) + np.arange(M)) / M\n    return inverse_cdf(su, W)\n\n\n\n\n1.2 荷重を保持するWeightsクラス\n次に，SMC に繋げるために，粒子の荷重を保持するためのクラスを定義する．粒子の荷重は極めて小さくなり得るため，対数によって保持する．このクラス内の属性として，正規化荷重もESSも得られるようにする：\n\nlw：正規化されていない荷重を対数で保持\nw：正規化された荷重\nESS：有効サンプル数2\n\nこれらの属性を __init__(lw) 内で計算する．加えて add(delta) メソッドで，incremental weightsを乗じるルーチンを用意する．\n\n\nCode\nclass Weights:\n    \"\"\"A class to hold the N weights of the particles\"\"\"\n    def __init__(self, lw=None):\n        self.lw = lw  # t=0で呼ばれた際はNoneである\n        if lw is not None:\n            self.lw[np.isnan(self.lw)] = -np.inf  # 欠損値処理\n            m = self.lw.max()\n            w = np.exp(self.lw - m)  # 大きすぎる値にならないように\n            s = w.sum()\n            self.W = w / s  # 正規化荷重\n            self.ESS = 1.0 / np.sum(self.W ** 2)\n            self.log_mean = m + np.log(s / self.N)\n    \n    @property\n    def N(self):\n        \"\"\"Number of particles\"\"\"\n        return 0 if self.lw is None else self.lw.shape[0]\n\n    def add(self, delta):\n        \"\"\"Add increment weights delta to the log weights\"\"\"\n        if self.lw is None:\n            return self.__class__(lw=delta)\n        else:\n            return self.__class__(lw=self.lw + delta)\n\n\n初期化は \\[\nW^i=\\frac{e^{\\log w^i-m}}{\\sum_{j=1}^Ne^{\\log w^j-m}}\n\\] \\[\nm:=\\log\\left(\\max_{i\\in[N]}w^i\\right)\n\\] に基づいて計算されている．log_mean は \\[\n\\begin{align*}\n    &\\log\\left(\\max_{i\\in[N]}w^i\\right)+\\log\\left(\\frac{\\sum_{j=1}^Ne^{\\log w^j-m}}{N}\\right)\\\\\n    &=\\log\\left(\\frac{1}{N}\\sum_{j=1}^Nw^j\\right)\n\\end{align*}\n\\] という値である．"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#粒子の情報保持particlehistoryクラス",
    "href": "posts/2023/Particles/ParticleFilter.html#粒子の情報保持particlehistoryクラス",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "2 粒子の情報保持：ParticleHistoryクラス",
    "text": "2 粒子の情報保持：ParticleHistoryクラス\n\n2.1 情報を収集するCollectorクラス\nSMCの結果をプロットするために，各時間で粒子の標本統計量を SMC クラス（ Section 3 ）から適宜抜き出して保存しておくためのクラス Summaries を作成する．抜き出すためのメソッドを Collector クラスの継承クラスとして定義する．\n\n\nCode\nclass Collector:\n    \"\"\"Base class for collectors\"\"\"\n    def __init__(self, **kwargs):\n        self.summary = []\n\n    def collect(self, smc):\n        self.summary.append(self.fetch(smc))\n\nclass ESSs(Collector):\n    summary_name = \"ESSs\"\n    def fetch(self, smc):\n        return smc.wgts.ESS\n\nclass LogLts(Collector):\n    summary_name = \"LogLts\"\n    def fetch(self, smc):\n        return smc.logLt\n\nclass Rs_flags(Collector):\n    summary_name = \"Rs_flags\"\n    def fetch(self, smc):\n        return smc.rs_flag\n\nclass Moments(Collector):\n    \"\"\"Collects empirical moments of the particles\"\"\"\n    summary_name = \"Moments\"\n    def fetch(self, smc):\n        m = np.average(smc.X, weights=smc.wgts.W, axis=0)\n        m2 = np.average(smc.X ** 2, weights=smc.wgts.W, axis=0)\n        v = m2 - m ** 2\n        return {\"mean\": m, \"var\": v}\n\ndefault_collector_cls = [ESSs, LogLts, Rs_flags]\n\n\n\n\n2.2 標本統計量を保持するSummariesクラス\nこのクラスはデフォルトで用意されている default_collector_cls に加えて，cols引数で指定されたメソッドを追加し，collect() メソッドが呼ばれるとこれらを集めて属性として保持する．\n\n\nCode\nclass Summaries:\n    \"\"\"A class to hold the summaries of the SMC algorithm\"\"\"\n    def __init__(self, cols):\n        self._collectors = [cls() for cls in default_collector_cls]\n        if cols is not None:\n            self._collectors.extend(col() for col in cols)\n        for col in self._collectors:\n            setattr(self, col.summary_name, col.summary)\n\n    def collect(self, smc):\n        for col in self._collectors:\n            col.collect(smc)\n\n\n\n\n2.3 ヒストリを保持するParticleHistoryクラス\ndequeオブジェクト としてヒストリを格納するためのクラスParticleHistory実装する．これにより直前 \\(k\\) ステップの情報だけを保持出来るように作れるが，今回はプロットのために全履歴を保持する．\n\n\nCode\nclass ParticleHistory:\n    \"\"\"History of the particles\n    Full history that keeps all the particle systems based on lists.\n    \"\"\"\n    def __init__(self, fk):\n        self.X, self.A, self.wgts = [], [], []\n        self.fk = fk\n\n    def save(self, smc):\n        self.X.append(smc.X)\n        self.A.append(smc.A)\n        self.wgts.append(smc.wgts)\n\n\n\n\nCode\ndef generate_hist_obj(option, smc):\n    if option is True:\n        return ParticleHistory(smc.fk)\n    else:\n        return None"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#sec-SMC",
    "href": "posts/2023/Particles/ParticleFilter.html#sec-SMC",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "3 実行部分：SMCクラス",
    "text": "3 実行部分：SMCクラス\nこのクラスがやるべきことは多い．Feynman-Kacモデル fk（ Section 4.1 で後述），粒子数 N，リサンプリング法 resampling を引数に取り，粒子フィルターを実行する．\n最も大事なこととして，本クラスはイテレータとして定義し，__next__ メソッドを実装する．そして run() メソッドで __next__ を終了するまで繰り返し呼び出すことでイテレータプロトコルを実行する．\n__next__メソッドでは，次のような処理を行う：\n\n終了フラッグ fk.done(self) が立っているかどうかを確認する．\n\\(t=0\\) の場合，最初の粒子を初期分布 \\(M_0\\) から \\(N\\) 個サンプリングする．\n\\(t&gt;0\\) の場合は，リサンプリングと粒子移動を行う．これは resample_move() メソッドで行う．\n\nリサンプリングフラッグ fk.time_to_resample(self) が立っている場合にリサンプリングを systematic メソッド（ Section 1.1 ）により行う．これにより，移動（変異）する粒子 \\(A^{1:N}_t\\) を確定させる．\n確率核 \\(M_t(X_{t-1}^{A_t^{1:N}},-)\\) に従って，粒子 \\(X_t^{1:N}\\) をサンプリングする．\n\n粒子の荷重を更新する．これは reweight_particles() メソッドで行う．\ncompute_summariesメソッドを呼び出して，粒子の標本統計量を Summaries クラスに，ヒストリを Particle History クラスに追記する．\n時刻 \\(t\\) を進めて 3.に戻る．\n\n\n\nCode\nclass SMC:\n    \"\"\"Metaclass for SMC algorithms\"\"\"\n\n    def __init__(\n        self,\n        fk=None,\n        N=100,\n        resampling=\"systematic\",\n        ESSrmin=0.5,\n        store_history=False,\n        collect=None,\n    ):\n\n        self.fk = fk\n        self.N = N\n        self.resampling = resampling\n        self.ESSrmin = ESSrmin\n\n        # initialisation\n        self.t = 0\n        self.rs_flag = False  # no resampling at time 0, by construction\n        self.logLt = 0.0\n        self.wgts = Weights()\n        self.X, self.Xp, self.A = None, None, None\n\n        self.summaries = Summaries(collect)\n        self.hist = generate_hist_obj(store_history, self)\n\n    def generate_particles(self):\n        \"\"\"Generate particles at time t=0\"\"\"\n        self.X = self.fk.M0(self.N)\n    \n    def reset_weights(self):\n        \"\"\"Reset weights to uniform after a resamping step\"\"\"\n        self.wgts = Weights()\n    \n    def resample_move(self):\n        \"\"\"Adaptively resample and move particles at time t\"\"\"\n        self.rs_flag = self.fk.time_to_resample(self)\n        if self.rs_flag:\n            self.A  = systematic(self.wgts.W, M=self.N)\n            self.Xp = self.X[self.A]\n            self.reset_weights()\n        else:\n            self.A = np.arange(self.N)\n            self.Xp = self.X\n        self.X = self.fk.M(self.t, self.Xp)\n\n    def reweight_particles(self):\n        \"\"\"Reweight particles at time t\"\"\"\n        self.wgts = self.wgts.add(self.fk.logG(self.t, self.Xp, self.X))\n\n    def compute_summaries(self):\n        \"\"\"Compute summaries at time t\"\"\"\n        if self.t &gt; 0:  # なぜかこれを前におかないとUnboundLocalErrorが出る\n            prec_log_mean_w = self.log_mean_w\n        self.log_mean_w = self.wgts.log_mean\n        if self.t == 0 or self.rs_flag:\n            self.loglt = self.log_mean_w\n        else:\n            self.loglt = self.log_mean_w - prec_log_mean_w\n        self.logLt += self.loglt\n\n        self.hist.save(self)\n        self.summaries.collect(self)\n\n    def __next__(self):\n        \"\"\"One step of the SMC algorithm\"\"\"\n        if self.fk.done(self):\n            raise StopIteration\n        if self.t == 0:\n            self.generate_particles()\n        else:\n            self.resample_move()\n        self.reweight_particles()\n        self.compute_summaries()\n        self.t += 1\n\n    def __iter__(self):\n        return self\n\n    def run(self):\n        \"\"\"Run the SMC algorithm until completion\"\"\"\n        for _ in self:\n            pass"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#粒子フィルタの実行東京の年別気温データ",
    "href": "posts/2023/Particles/ParticleFilter.html#粒子フィルタの実行東京の年別気温データ",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "4 粒子フィルタの実行：東京の年別気温データ",
    "text": "4 粒子フィルタの実行：東京の年別気温データ\n\n4.1 Feynman-Kacモデルの枠組み\nparticle パッケージの抽象クラス FeynmanKac は次のメソッドを持つ．3\n\nM0(N): 初期分布 \\(M_0\\) から \\(N\\) 個のサンプルを生成する．\nM(t, xp): カーネル \\(M_t(x_{t-1}|-)\\) から \\(X_t\\) をサイズ xp.shape[0] で生成する．\nlogG(t, xp, x): ポテンシャル \\(G_t(x_{t-1},x_t)\\) の対数を返す．\n\n加えて，粒子フィルターの実行時に必要なフラグも用意する．\n\ntime_to_resample(smc): smc オブジェクトを引数に取り，その属性 smc.aux.ESS, smc.ESSrmin からリサンプリングが必要かどうかを判定する．\ndone(smc): smc オブジェクトを引数に取り，その属性 smc.t, smc.T からアルゴリズムを終了すべきかどうかを判定する．\n\nparticle パッケージを使うときは FeynmanKac クラスを継承して用いることになるが，ここでは自分で定義していく．\n\n\n4.2 使用するデータ\n気象庁が HP にて公開している1876年から2022年までの計147年分の東京の年別気温データを用いる．\n\n\nCode\nimport pandas as pd\n\ndata = pd.read_csv(\"TemperatureDataAtTokyo.csv\")\nprint(data.describe())\n\n\n                年度         日平均         日最高        日最低          最高          最低\ncount   147.000000  147.000000  147.000000  147.00000  147.000000  147.000000\nmean   1949.000000   14.963946   19.337415   11.12517   35.098639   -4.317687\nstd      42.579338    1.132396    0.875794    1.46614    1.674956    2.439366\nmin    1876.000000   12.900000   17.500000    8.30000   31.600000   -9.200000\n25%    1912.500000   14.000000   18.700000    9.90000   34.000000   -6.150000\n50%    1949.000000   14.800000   19.300000   10.80000   34.900000   -4.700000\n75%    1985.500000   15.800000   19.900000   12.30000   36.200000   -2.250000\nmax    2022.000000   17.300000   21.300000   13.90000   39.500000    0.900000\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(3.5, 3))\n\nplt.title(\"temperature in Tokyo\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature (Celsius)\")\n\nplt.scatter(data['年度'], data['日平均'], s=2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4.3 気温の1次のトレンドモデル\n気温の観測値 \\(\\{y_k\\}\\) に対して，1次元の線型Gauss状態空間モデル \\[\n\\begin{cases}\nx_k=x_{k-1}+v_k,\\\\\ny_k=x_k+w_k.\n\\end{cases}\n\\tag{1}\\] \\[\nv_k\\overset{\\text{iid}}{\\sim}\\mathrm{N}(0,Q^2),\\quad w_k\\overset{\\text{iid}}{\\sim}\\mathrm{N}(0,R^2),\n\\] を想定する．このモデルを1次のトレンドモデルという (北川, 2005, p. 第11章)．\nこれをSMCメソッド（ Section 3 ）に渡せるように実装するには次のようにする：\n\n\nCode\nfrom numpy import random\nfrom scipy import stats\n\nclass Bootstrap:\n    \"\"\"Abstract base class for Feynman-Kac models derived from State Space Model (1).\n    \"\"\"\n\n    def __init__(self, data, T, R, Q):\n        self.data = data\n        self.T = T\n        self.R = R\n        self.Q = Q\n    \n    def M0(self, N):\n        \"\"\"Sample N times from initial distribution M0 of the FK model\"\"\"\n        return random.normal(loc=13.6, scale=self.Q, size=N)\n    \n    def M(self, t, xp):  # xp: resampled previous state\n        \"\"\"Sample Xt from kernel Mt conditioned on Xt-1=xp\"\"\"\n        return random.normal(loc=xp, scale=self.Q, size=xp.shape[0])\n    \n    def logG(self, t, xp, x):  # x: current state\n        \"\"\"Evaluate the log potential Gt(xt-1,xt)\"\"\"\n        return stats.norm.logpdf(self.data[t], loc=x, scale=self.R)\n    \n    def time_to_resample(self, smc):\n        \"\"\"Return True if resampling is needed\"\"\"\n        return smc.wgts.ESS &lt; smc.N * smc.ESSrmin\n    \n    def done(self, smc):\n        \"\"\"Return True if the algorithm is done\"\"\"\n        return smc.t &gt;= self.T\n\n\n\n\n4.4 \\((R,Q)=(0.2,0.1)\\) の場合\n仮に \\((R,Q)=(0.2,0.1)\\) としてみる．すなわち，システムノイズ \\(Q^2=0.1\\) が小さく，観測ノイズ \\(R^2=0.4\\) はそれよりは大きいとしている．\n\n\nCode\nmodel1 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.2, Q=0.1)\nPF1 = SMC(fk=model1, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF1.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF1.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n\n\n\n図 1: (R,Q)=(0.2,0.1) の場合の粒子フィルターの実行結果\n\n\n\n\n\n少し揺らぎながらも，トレンドとして気温が上昇していく様子が見られる．\n\n\n4.5 \\((R,Q)=(0.7,0.1)\\) の場合\n濾波して得たトレンドの揺らぎが少し大きいと思われたため，観測誤差はもう少し大きいものとして \\((R,Q)=(0.7,0.1)\\) としてみる．\n\n\nCode\nmodel4 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.7, Q=0.1)\nPF4 = SMC(fk=model4, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF4.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF4.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.7,0.1) の場合の粒子フィルターの実行結果\n\n\n\n\nこうしてトレンドとして少しばかり直線的なものが得られた．やはり上昇トレンドが見られる．\n\n\n4.6 \\((R,Q)=(0.2,0.01)\\) の場合\n\\(Q^2=10^{-4}\\) としてシステムノイズは極めて小さいと想定してみる．「トレンドは殆ど変化しない」という仮定を置いたことになる．\n\n\nCode\nmodel2 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.2, Q=0.01)\nPF2 = SMC(fk=model2, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF2.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF2.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,0.01) の場合の粒子フィルターの実行結果\n\n\n\n\nあまり良い当てはまりを見せないため，この気温の時系列を全てが観測誤差によるものだと理解するのは妥当ではないと考えられる．\n\n\n4.7 \\((R,Q)=(0.2,1)\\) の場合\n逆にシステムノイズを極めて大きい値 \\(Q^2=1\\) と設定する．トレンドは年別の揺らぎが大きいと想定したことになる．\n\n\nCode\nmodel3 = Bootstrap(data=data['日平均'], T=data.shape[0], R=0.2, Q=1.0)\nPF3 = SMC(fk=model3, N=1000, resampling=\"systematic\", ESSrmin=0.5, collect=[Moments], store_history=True)\nPF3.run()\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot([m['mean'] for m in PF3.summaries.Moments], label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,1) の場合の粒子フィルターの実行結果\n\n\n\n\nとんでもない過適応を見せて，全てをトレンドとして説明してしまっており，これもまた妥当ではないと考えられる．\n\n\n4.8 カルマンフィルタとの比較\n線型Gaussモデルを想定しているため，粒子フィルターは \\(N\\to\\infty\\) の極限で最適フィルターであるカルマンフィルターに一致するはずである．そこで，pykalman パッケージを用いてこれを実装する．\\((R,Q)=(0.2,0.1)\\) とする．\n\n\nCode\nfrom pykalman import KalmanFilter\nKF1 = KalmanFilter(initial_state_mean=13.6, initial_state_covariance=0.1,\n                   transition_matrices=1, observation_matrices=1,\n                   transition_covariance=0.1, observation_covariance=0.2, n_dim_state=1, n_dim_obs=1)\nKF1 = KF1.em(data['日平均'], n_iter=5)  # EMアルゴリズムの過適応回避のため\n(filtered_state_means, filtered_state_covariances) = KF1.filter(data['日平均'])\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot(filtered_state_means, label='filtered temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,0.1) の場合のKalmanフィルターの実行結果\n\n\n\n\nたしかに 図 1 と極めて似通った結果になっている．\n\n\n4.9 カルマン平滑化の結果\n\n\nCode\n(smoothed_state_means, smoothed_state_covariances) = KF1.smooth(data['日平均'])\n\n\n\n\nCode\nplt.figure(figsize=(3.5, 3))\nplt.plot(data['日平均'], label='data', linestyle='', marker='.')\nplt.plot(smoothed_state_means, label='smoothed temperature trend')\nplt.show()\n\n\n\n\n\n(R,Q)=(0.2,0.1) の場合のKalman平滑化の実行結果\n\n\n\n\nより滑らかなトレンドが得られている．"
  },
  {
    "objectID": "posts/2023/Particles/ParticleFilter.html#footnotes",
    "href": "posts/2023/Particles/ParticleFilter.html#footnotes",
    "title": "粒子フィルターの実装 | Particles Package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n実はこのリサンプリング法は (Kitagawa, 1996) の付録で原型が（全く決定論的なアルゴリズムとして）提案されている↩︎\n有効サンプル数の定義については (Chopin and Papaspiliopoulos, 2020) 参照．↩︎\nFeynman-Kacモデルなどの用語については (Chopin and Papaspiliopoulos, 2020) 参照．↩︎"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html",
    "href": "posts/2023/Surveys/BayesianComp.html",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "",
    "text": "History of Bayesian Computation (Martin et al., 2023, p. 4)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズまでの統計学の黎明",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズまでの統計学の黎明",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.1 ベイズまでの統計学の黎明",
    "text": "1.1 ベイズまでの統計学の黎明\n統計学の黎明を要請したものは，社会への不安であった．筆者に言わせれば，この社会への不安を直視したのがドイツ，数で解決しようとしたのがイギリスで，解決への筋道を確率論で基礎づけたのがフランスである．\n\nプロシヤにおける国勢学，イギリスにおける政治算術，フランスにおける古典確率論–統計学はこれら３つの異った厳選を持つと言われる (増山元三郎, 1950, p. 6)．\n\n17世紀初頭から何度も流行を繰り返し，遂に1665年にはロンドンの人口の1/4を死に至らしめた ペストの大流行 は恐怖の対象であった．パンデミックは現代でも恐怖の対象であるが，当時はその全貌の把握が難しく，これが第一に切望された．月の運行による健康被害，国王の統治が疫病を引き起こす，などの俗見が流布していた時代である．しかし，「数」という解決手段は極めて功を奏した．\n数による解決が他でもないイギリスから生まれたことは，Francis Bacon 1561-1626 と Thomas Hobbes 1588-1679 に象徴される自然科学の風土，「Aristotelesの三段論法を通じて，経験的に因果関係を発見することで，我々は自然を理解できる」という希望が当時のイギリスには存在したことが挙げられる．\n\n海へ行け，きっと獲物があるぞという先輩が Bacon であった．漁獲法一般の講義をする先生が，例えば後世の J. S. Mill の帰納論理学に相当するのである．統計学を作った漁師たちは，Mill 先生の帰納法の論理学の講義などは，上の空で聞いた．そして各自の漁獲法を自らの浜で覚えたのである． (北川敏男, 1949, p. 12)\n\n\n新大陸の発見，東洋への海路の開拓により，世界商業の中心は砂漠の商隊と地中海の商人とを介して栄えていたイタリアから，スペイン・ポルトガル・オランダを経て16世紀の終り頃には，すでにイギリスに移っていたのである．ここに興隆の一路を辿る市民社会・殷盛を極める海上貿易・繁栄する英都の商業・封建制の崩壊を示す Cromwell 革命 (1649) 後のイギリス社会に，市民科学としての政治算術が起ったことは敢えて異とするに足りないであろう (増山元三郎, 1950, p. 10)．\n\n\n1.1.1 John Grauntの死亡表\nペスト流行の激しさの判定に寄与する人口状況を，最初に数によって理解しようとしたのが John Graunt 1620-1674 であった．\n当時の英国王立理学協会1 は，封建的な諸関係の崩壊解消と同時に，商品生産・貨幣による売買の全面支配によって貨幣的表現が富の大部分に侵入したことにより新たに誕生した市民階級が勢力を占めており，Graunt もこのような商人階級の出身であった．\nそのような身分の Graunt が英国王の推薦を受けて王立協会員の名誉を勝ち取った論文 (Graunt, 1662) は，ギルド発行の死亡統計 Bills of Mortality と教会に蓄積していた統計資料2 から統計的な処理を通じて世界初の「死亡表」を作成し，次の内容を初めて結論づけた．\n\n36%の幼児は６歳未満で死亡する．\n洗礼数をみると，男女比は16:15くらいである．\n都市の死亡率は地方より高い．\nLondonの城外では死亡率は３倍である．\n\n加えてLondonの世帯数を3通りの方法で推算し，世帯数は5万であろうと結論づけた．なお，当時の俗見ではLondon人口は100万と言われていた．\nその後このような「生命表」は精緻化の一途を辿り，イギリスのギルド的な共助制度の土壌の上で，生命保険の成立という実を結んだ．\n\n\n1.1.2 統計学への期待と希望\nこのイギリスの数を使った解決は，政治算術学派 と呼ばれ，海外への輸出が進んだ．\nドイツの牧師 Johann Peter Süβmilch 1707-1767 は Graunt に倣って，教会に蓄積していた統計資料を用い，出生率の性別比が長期的には女性1,000対男性1,050に収束することを発見した．\n中でも特に，「たくさんのデータを集めると何かが見えてくる」ことに大きな希望を持ち，Graunt が教会の資料に注目したことを Columbus の新大陸発見になぞらえている．そう，歴史上最初の統計分析は，教会の資料によるものであったのである．\n\n若し我々が家を一軒一軒数えていくならば，ある家では娘だけに，またある家では息子だけに，あるいはそうでなくとも，非常に不釣り合いな両者の配合にでくわすであろう．小さな社会や村落でも秩序的なものを認めることは，容易ではない．（中略）．かかる場合に，誰が，能く規則と秩序とに想達し得るだろう．所で，教会の記録はこの秩序の確認のための大きな手段である．それは教会用及び世俗用のためにすでに数世紀前から取られ，とくに宗教改革後はかなり正確にとられてきた．誰がそれを利用したか？その発見はアメリカ発見と同時に可能であったのだ．（中略）それをGrauntがなし得たのである． –Süβmilch (1741) 『神の秩序』 (Göttliche Ordnung) 訳文は (北川敏男, 1949) より．\n\nこのように Süβmilch は男児の出生率の方が高いことを神の存在証明と見なしたのであった．この宗教的な外被を取り去るには，確率論の登場をまたねばならなかったが，これにはさらにフランスの学派が合流するのを待つ必要があり，それには100年を要したのであった（ Section 1.7 も参照）．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズが取り組んだ問題",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズが取り組んだ問題",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.2 ベイズが取り組んだ問題",
    "text": "1.2 ベイズが取り組んだ問題\nというわけで，\\(i=1,\\cdots,n\\) 番目の世帯の新生児が，男児である \\(y_i=1\\) か女児である \\(y_i=0\\) かのデータなどから，人口・疫病・国家動態に役立つ知識を引き出すことが当時の重要な問題意識であることをわかっていただけただろう．\nイギリスの牧師 Thomas Bayes 1701-1761 は，より抽象的な設定で統計的推定の問題を研究していた．Bayes は就中，次のような区間推定の問題を考えていた．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\nベイズが取り組んだ問題（現代語訳）\n\n\n\n2値のデータ \\(Y_i\\in\\{0,1\\}\\) は，ある未知の「成功率」 \\(\\theta\\in(0,1)\\) に従って， \\[\nY_i=\\begin{cases}\n1&\\text{確率 }\\theta\\text{ で}\\\\\n0&\\text{残りの確率} 1-\\theta\\text{ で}\n\\end{cases}\n\\] という値を取るとする．3 このようなデータの独立観測標本 \\(\\boldsymbol{y}:=(y_1,\\cdots,y_n)^\\top\\) から．神のみぞ知る，このデータ \\(\\boldsymbol{y}\\) を生み出した真の成功率 \\(\\theta\\) が，区間 \\((a,b)\\subset(0,1)\\) に入っているという確率 \\(\\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]\\) をどう見積もれば良いか？\n\n\nここでは引き続き \\(Y_i\\) は性別で，\\(\\theta\\) は男児が生まれる確率 \\(\\theta=\\operatorname{P}[Y_i=1]\\) だと解釈する．Bayes 自身は「ある未知の位置に白線が引かれたテーブル上にボールを \\(n\\) 個転がし，それぞれの領域に幾つのボールが入ったかの情報のみから，白線の位置を推定する」という表現によって問題を定式化した (Bayes, 1763)．これは後世ではビリヤード台の問題とも呼ばれた．こちらのサイトも参照．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズのアイデア",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズのアイデア",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.3 ベイズのアイデア",
    "text": "1.3 ベイズのアイデア\n彼の発想は極めてシンプルであり，次の3段階によって推定を試みた：\n\n事前分布 \\(p(\\theta)\\) と呼ばれる，最初の \\(\\theta\\in(0,1)\\) に対する予想 \\(p(\\theta)\\) を自由に表現する．4\n事前分布のデータ \\(\\boldsymbol{y}\\) を観測した下での条件付き分布 \\[p(\\theta|\\boldsymbol{y}):=\\frac{p(\\theta,\\boldsymbol{y})}{p(\\boldsymbol{y})}\\] を計算する．これを事後分布という．\nこの事後分布 \\(p(\\theta|\\boldsymbol{y})\\) の形から区間推定を実行する．\n\nこの 3.の部分は，Bayes が特に区間推定に拘ったためのものであり，点推定でも良ければ次期予測でも良い．推定対象 3.を目的に応じて自由に入れ替えても，1.と 2.の部分が同じように動作するということ，これがベイズ統計学の枠組みである．\nそれだけに事後分布というものが表現力に富んでいるのである．また，以下の例で納得していただけるかもしれないが，ベイズ統計学の手続きは「眼前のデータは，事前の信念を変えるのにどれほど説得的であるか？」という観点からも見れ，定量的であると同時に定性的な判断も可能にする． Section 3.6 で紹介するように，この特徴は意思決定への応用おいても重要である．\n\n\n\n\n\n\n問題に対する (Bayes, 1763) の解決\n\n\n\n\nまず，事前分布 \\(p(\\theta)\\) を設定する．Bayes は前述のビリヤードの問題を考えていたこともあり， 「\\(\\theta\\in(0,1)\\) は全く予想がつかない」「どんな \\(\\theta\\) も同様にあり得る」という立場を取った．横軸を \\(\\theta\\in(0,1)\\) の値，縦軸を「主観的にあり得ると思う度合い」として図で表すと次の通りである：\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the range for x-axis\nx = np.linspace(0, 1, 1000)\n\n# Uniform distribution density function is constant\ny = np.ones_like(x)\n\n# Plot the graph\nplt.figure(figsize=(3, 2)) # Size suitable for a smartphone screen\nplt.plot(x, y, label='Uniform Distribution (0,1)', color=(0.35, 0.71, 0.73, 1))\nplt.fill_between(x, y, color=(0.35, 0.71, 0.73, 0.3))\nplt.xlabel('x')\nplt.ylim(0, 1.5)\nplt.ylabel('Density')\nplt.title('Posterior Distribution')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nこの図が表す \\((0,1)\\) 上の確率分布を一様分布という．このように，一様分布とは「どのような \\(\\theta\\) の値も同様に確からしい」という予想の表現である．\n\n次に，データ \\(\\boldsymbol{y}=(y_1,\\cdots,y_n)^\\top\\) が観測された後の条件付き分布 \\(p(\\theta|\\boldsymbol{y})\\) を計算することで，本データ \\(\\boldsymbol{y}\\) が事前の信念 \\(p(\\theta)\\) をどのように変えてしまうかを観る．簡単な確率論の結果として，条件付き分布は次の公式によって計算できる（講義ノートも参照）：5 \\[\np(\\theta|\\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|\\theta)p(\\theta)}{\\int_\\Theta p(\\boldsymbol{y}|\\theta)p(\\theta)\\,d\\theta}\n\\tag{1}\\]\n例えば 日本の2021年の出生児性別のデータ を用いると次のようになる．\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# パラメータの設定\nn = 811622\nmale = 415903\nfemale = n - male\n\n# ベータ分布のPDFを計算\nx = np.linspace(0, 1, 1000)\ny = beta.pdf(x, 1+male, 1+female)\n\n# プロット\nplt.figure(figsize=(3, 2))\nplt.plot(x, y, label=f'Beta({1+male}, {1+female})', color=(0.35, 0.71, 0.73, 1))\nplt.fill_between(x, y, color=(0.35, 0.71, 0.73, 0.3))\nplt.xlabel('p')\nplt.xlim(0.4, 0.6)\nplt.ylabel('Probability Density')\nplt.title('Bayesian Posterior Distribution')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nこうして極めて鋭い事後分布が出来た．事前に設定した分布 \\(p(\\theta)\\) は極めて平坦な一様分布であったのに，それをデータで条件付けた \\(p(\\theta|\\boldsymbol{y})\\) には極めて鋭いスパイクが現れたのである．式 1 を認めるならば，この図は「男児の方が女児よりも生まれる確率が高い」ことの証拠として，極めて説得的ではないだろうか？\n\nでは区間推定の例として，\\((a,b)=(0.5,1.0)\\) として，「男児の方が女児よりも多い確率」を推定しよう．これは次を計算することになる： \\[\n\\begin{align*}\n&\\operatorname{P}\\left[\\frac{1}{2}&lt;\\theta&lt;1\\right]\\\\\n&=\\int^1_{\\frac{1}{2}}p(\\boldsymbol{y}|\\theta)\\,d\\theta.\n\\end{align*}\n\\]\n\n\n\nCode\nprint(sum(y[500:600])/1000)\n\n\n1.0030977682960893\n\n\nもはや丸め誤差により \\(1\\) を越してしまっている．ほとんど確実に「男児の方が生まれる確率が高い」と結論づけて良いだろう．\n\n\nこの (Bayes, 1763) が実行したように，事後分布 \\(p(\\theta|\\boldsymbol{y})\\) をみて \\(\\theta\\) に関する推論をする，という立場からの統計的営み全体をベイズ統計学．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-fundamental-problem-of-Bayes",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-fundamental-problem-of-Bayes",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.4 ベイズ統計学の基本問題",
    "text": "1.4 ベイズ統計学の基本問題\n事後分布 \\(p(\\theta|\\boldsymbol{y})\\) を導く際に用いた条件付き確率の公式である 式 1 \\[\np(\\theta|\\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|\\theta)p(\\theta)}{\\int_\\Theta p(\\boldsymbol{y}|\\theta)p(\\theta)\\,d\\theta}\\quad\\text{(1)}\n\\] は Bayesの公式 と呼ばれるようになった．今回の場合では，Pythonコードをご覧になった方はわかったかもしれないが，事後分布は \\[\np(\\theta|\\boldsymbol{y})=\\frac{\\theta^m(1-\\theta)^{n-m}}{B(m+1,n-m+1)}\n\\] となり，これはパラメータの空間 \\((0,1)\\) 上の Beta分布 と呼ばれるものである．\n現代のベイズ統計学の多くの統計量は，ある可積分関数 \\(g:\\Theta\\to\\mathcal{X}\\) を用いて \\[\n\\operatorname{E}[g(\\theta)|\\boldsymbol{y}]=\\int_{\\Theta}g(\\theta)p(\\theta|\\boldsymbol{y})\\,d\\theta\n\\tag{2}\\] と表される．先ほどの Bayes の区間推定の例では \\(g=1_{(a,b)}\\) と取った場合に当たる．6 実は，この積分は，この最も簡単と思われる \\(p(\\theta),p(\\boldsymbol{y}|\\theta)\\) の設定でも，殆ど計算できないのである．\n鮮やかな解決法を提示したかと思えば，結局実行出来ないのでは全く本末転倒である！そのこともあってか，論文 (Bayes, 1763) は実は Bayes の死後に Richard Price によって投稿されたものであり，生前に自ら投稿・発表した訳ではなかった．7 当然，発表当時は全く注目を受けなかった (Stigler, 1990)．\n\nHence, despite the analytical availability of \\(p(\\theta|\\boldsymbol{y})\\) via (2)–“Bayes’ rule” as it is now known-—the quantity that was of interest to Bayes needed to be estimated, or computed. The quest for a computational solution to a Bayesian problem was thus born. (Martin et al., 2023, p. 2)\n\n\n\n\n\n\n\nまとめ：ベイズ統計学の基本問題\n\n\n\nベイズの提示した統一的な統計推測の枠組み\n\n推定したい値 \\(\\theta\\) の空間上に事前分布 \\(p(\\theta)\\) を設定する．\n事前分布 \\(p(\\theta)\\) のデータ \\(\\boldsymbol{y}\\) に関する条件付き分布として事後分布 \\(p(\\theta|\\boldsymbol{y})\\) を得る．\n\nは非常に自然で，特に確率分布 \\(p(\\theta),p(\\theta|\\boldsymbol{y})\\) を簡単に視覚化できる現代では「データ \\(\\boldsymbol{y}\\) は，パラメータ \\(\\theta\\) に対する事前の信念をどれほど変えるに値するか？」を定量的にも定性的にも実感出来るという美点がある．\nしかしながら，モデル \\(p(\\theta),p(\\boldsymbol{y}|\\theta)\\) の設定をいくら簡単にしても根本的に計算が困難で実行不可能なのである．これを解決する分野をベイズ計算という．ベイズの論文 (Bayes, 1763) でも，計算法の開発が約半分を占めた．8 このように，Bayes統計学は当初からBayes計算の問題を懐胎していたのである．\n\nIn short, the implementation of all forms of Bayesian analysis relies heavily on numerical computation. (Martin et al., 2023, p. 2)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-Laplace",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-Laplace",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.5 Laplaceの近似",
    "text": "1.5 Laplaceの近似\nフランスの数学者 Laplace は25歳時の初めての統計に関する著作 (Laplace, 1774) を発表した．この中で，Bayes が解こうとしたものと全く同じ\n\\[\\begin{align*}\n    &\\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]\\\\\n    &\\quad=\\frac{\\int^b_a\\theta^m(1-\\theta)^{n-m}\\,d\\theta}{B(m+1,n-m+1)}\n\\end{align*} \\tag{3}\\]\nという積分計算の問題を，被積分関数を \\[\nf(\\theta):=\\frac{\\log p(\\theta|\\boldsymbol{y})}{n}\n\\] を用いて指数関数の形に表すことで解いた：9 \\[\n\\begin{align*}\n    \\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]&=\\int^b_ap(\\theta|\\boldsymbol{y})\\,d\\theta\\\\\n    &=\\int^b_ae^{nf(\\theta)}\\,d\\theta\n\\end{align*}\n\\] この形に変形することがどのように役立つかは，次の定理が説明してくれる：\n\n\n\n\n\n\n定理（Laplace近似）\n\n\n\n10 関数 \\(f:[a,b]\\to\\mathbb{R}\\) はただ一つの最大値を \\(x_0\\in(a,b)\\) で取り，\\(f''(x_0)&gt;0\\) を満たすとする．このとき \\(n\\to\\infty\\) の極限について， \\[\n\\int^b_ae^{nf(x)}\\,dx\\sim\\sqrt{-\\frac{2\\pi}{nf''(x_0)}}e^{nf(x_0)}\n\\]\n\n\nこれを \\(f\\) の二次近似について適用することで，あらゆる確率分布 \\(p(\\theta|\\boldsymbol{y})\\,d\\theta\\) に関する積分 式 3 を，その正規近似に関する積分で近似できるのである．\nこの手法は現在のBayes計算手法のアイデアの源泉であり続けている (Rue et al., 2009), (MacKay, 2003)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズ統計学の長く苦しい時代",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズ統計学の長く苦しい時代",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.6 ベイズ統計学の長く苦しい時代",
    "text": "1.6 ベイズ統計学の長く苦しい時代\n「ベイズの枠組みは理念的に好ましかろうと，実際には実行不可能である」というベイズ統計学の基本問題は，Laplace が普遍的な近似計算法を開発したこと（ Section 1.5 ）を除いて，次の進展を見るには計算機の発明と普及を待つ必要があった．その間実に2世紀超えである．\nまた，Laplace の近似手法は普遍的であり，Bayes の最初の設定のような簡単な設定の \\(p(\\theta|\\boldsymbol{y}),g\\) に限らずとも使えるという，ベイズ統計学に大きく資する特徴も備えていたが，パラメータ \\(\\theta\\in(0,1)\\) の次元が1ではなくなると途端に使えなくなるという欠点がある．\n\n（前略）ベイズ統計学の有用性は以前から理解されていたが，この問題の抜本的な解決は1980年代まで待たざるを得なかった．それ以前は，ベイズの定理自体は18世紀に早々に発見されたにもかかわらず，長い間，確率の解釈，事前分布の設定，事後分布の計算の困難さのために哲学的議論に終始し，実用化にはほど遠かったのである．実用化の扉の鍵となったのは，一つは計算機の急速な発達，もう一つは計算集約的な画期的アルゴリズムの提案である． (樋口知之, 2014, p. 17)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-France",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-France",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.7 フランスでの確率論の歴史",
    "text": "1.7 フランスでの確率論の歴史\nこのように，Laplace が，ベイズ統計学暗黒の時代の中で唯一の小さな前進を生んだ．それだけでなく，Laplace は後の1812年に当時の確率論最大の集大成と言える大著『確率論の解析理論』を産んでおり，これが他でもないフランスから生まれたことにも相応の理由があった．\nまず第一に，賭博の流行により，確率というものの理解と征服が嘱望された．\n\nその（確率論の）発展の動きを与えたものは，交易を賭ける商業資本家が占星術よりも確実な指導をこの学術に求めるという様な社会が基盤となって存在したことである．例えば，17世紀中葉のPascalとFermatの間の往復文書に取り扱われたカード遊びの数学的問題が，広く人々の関心を呼び起こした事情の裏には，至富の途を確実に求める商人たちの渇望が学問が外の世界にあったことを忘れてはならない． (北川敏男, 1949)\n\n第二に，統計的現象を神学的な畏怖の対象と見るのではなく，自然科学による自然の理解と征服の文脈の最先端として理解する土壌がフランスにあったことが指摘できる．\n\n17, 18世紀の啓蒙的合理主義は，偶然的な事象に対しても数学的な取り扱いを行うことに特別の興味を持った．思想的にはこの時代精神こと確率論を発展させた最大の動力であった．その駆使する数学解析の多彩と合理主義の徹底とに於て，Laplace の大著はよくこの時代を代表するものと言うべきであろう．\n\n当時の財務総監 Jacques Turgot を通じてパリ造幣局の監査官も務めた Nicolas de Condorcet 1743-94 は Laplace の確率論を積極的に社会分析に応用した．「社会数学」と呼んだこの運動は社会学の源流ともみなされる． 当然後進も Laplace に続いた．ベルギーの数学者 Adolphe Quetelet 1796-74 は Laplace の確率論を社会に応用することを目指し「社会物理学」なる分野を創始し，BMIの別名「ケトレー指数」にも名を残している．11\n\n古典確率論の一応の完成は典雅に見えるであろう．だが人は，確率論のもった政治的，社会的意義を忘れてはならない．理知を一切の尺度として「代数学の炉火によって倫理学及び政治学を照さん」(Condorcet) という時代精神，神の啓示に代らんとする確率論，それはフランス革命の思想的基礎に連関することを見失ってはならないのである． (北川敏男, 1949)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#quetelet-による綜合",
    "href": "posts/2023/Surveys/BayesianComp.html#quetelet-による綜合",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.8 Quetelet による綜合",
    "text": "1.8 Quetelet による綜合\nQuetelet は応用統計学の祖とも呼ばれる．\n\n［政治算術学派から人口理論 (Malthus, 1798) をはじめとする数多くの統計的法則の発見など，多くの成果があったにも拘らず，］18世紀の後半は理論的成果の観点からは全く空白の時代であった．吾々はその原因を方法論が進歩しなかった点に見出しうる．先験的対数法則を中枢とする古典確率論の方法がこれと合流するに至るまで，統計学の理論的分野は足踏みを続けざるを得なかったのである．後述 Quetelet の手による，この合流の着手は，応に近代的意味における統計学の発足を示すものであるとともに，記述統計学の定礎を意味するものでなければならなかった．(増山元三郎, 1950, p. 12)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-MCMC",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-MCMC",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.1 マルコフ連鎖によるモンテカルロ法の発明",
    "text": "2.1 マルコフ連鎖によるモンテカルロ法の発明\n乱数のシミュレーションを用いた確率的なアルゴリズムをモンテカルロ法と総称する．これは Metropolis が同僚 Ulam のポーカー好きから，モナコの首都 Monte Carlo にちなんで名付けたものである (Nicholas Metropolis and Ulam, 1949)．このようなアルゴリズムが最初に生まれたのが，第二次世界大戦中の Los Alamos研究所 で進行中だった原爆開発計画である Manhattan計画 においてである．\n当時の問題は，原子爆弾着火時における Schödinger 作用素の基底状態のエネルギーを計算することにあった．抽象的には，\\(p\\) を \\(N\\) 個の粒子が従う Boltzmann 分布として，積分 式 2 を計算することにあった：\n\\[\n\\operatorname{E}[g(\\boldsymbol{\\theta})]=\\int_\\Theta g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\\quad\\text{(2)}\n\\]\nただし，\n\n積分領域 \\(\\Theta\\) が \\(2N\\) 次元というとてつもない高次元空間上であること\n分布 \\(p\\) は定数倍を除いてしか計算できない\n\nという，2つの大きな制約があった．1.のために通常の数値積分法が使えず，また 2.により \\(p\\) からの直接の乱数シミュレーションが出来ないので，\\(p\\) からの乱数 \\(X_1,\\cdots,X_M\\) を十分多く生成することで積分 式 2 を \\[\n\\frac{1}{M}\\sum_{i=1}^Mg(X_i)\n\\] によって近似するという通常の Monte Carlo 積分法を実行することも出来ない．そこで，Metropolis ら当時の Los Alamos に集まった物理学者たちは新しい方法を考える必要があった．\n最終的な解決 (N. Metropolis et al., 1953) は，Monte Carlo 法の中でもとりわけ画期的な発想によるものであった．それは，Markov 連鎖を用いるということである．Markov連鎖とは（ある一定の条件を満たす）確率過程のクラスであり，\\(p\\) から直接のシミュレーションが出来ない状況でも，\\(p\\) に収束するようなMarkov 連鎖を構成することは可能だったのである．\n制約 1.と 2.は広く物理学とベイズ統計学の至る所で見られる障壁であり，これをものともしない汎用アルゴリズムの発明は極めて大きなブレイクスルーであった．(Dongarra and Sullivan, 2000) は Metropolis アルゴリズムを理学・工学分野に20世紀最大の影響を与えたアルゴリズムの1つとしている．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-importance-sampling",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-importance-sampling",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.2 重点サンプリング法の発明",
    "text": "2.2 重点サンプリング法の発明\n実は Manhattan 計画に最中に，もう一つのサンプリング技法が生まれていた．厚い壁で中性子線とガンマ線がどのように吸収されるかに取り組んでいたグループにて，Herman Kahn らが中心となり，式 2 の分布 \\(p\\) に関する積分が \\[\n\\begin{align*}\n    \\operatorname{E}[g(\\boldsymbol{\\theta})]&=\\int_\\Theta g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\\\\\n    &=\\int_\\Theta\\frac{g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p^*(\\boldsymbol{\\theta})}p^*(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\n\\end{align*}\n\\] という式変形により，別の分布 \\(p^*\\) からのサンプリングを通じて計算できる，という技法が利用された．彼らはこれに重点サンプリング法という名前をつけた．これは Gerald Goertzel による命名である可能性が高い (Andral, 2022)．\nなお，当時は \\(p\\) からのサンプリングを回避できるという点よりも，\\(p^*\\) をうまく選ぶことにより元々の \\(p\\) を用いた Monte Carlo 積分法を適用するよりも近似の精度をあげることが出来るという点の方が注目された (Hammersley and Handscomb, 1964)．\n前節の Metropolis 法がMCMCの先駆けであるとしたら，この2つの美点を持った重点サンプリング法は，SMC（粒子フィルター） の先駆けであった．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#mcmcの普及とギブスサンプラー",
    "href": "posts/2023/Surveys/BayesianComp.html#mcmcの普及とギブスサンプラー",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.3 MCMCの普及とギブスサンプラー",
    "text": "2.3 MCMCの普及とギブスサンプラー\nMetropolis 法の発明から，すぐにMCMCの画期性が広く認識された訳ではなかった．特に，元々物理学の文脈で発明されたこともあり，統計学の文脈への応用が始まるには (Hastings, 1970) の仕事を待つ必要があった．\nしかし1970年代とはマイクロプロセッサが開発されたばかりの時代であり，12 MCMCが実際の統計解析の現場で採用可能な計算手法になるとは（そもそも現代のように小型なコンピュータを個人が所有するようになるとは）夢にも思われなかった時代であったが，ここからたったの20年で現代人の生活とベイズ統計学は大きく変わることになる．\n\n各人が安価に高性能なコンピュータを所有するようになった．\n高次元分布からのサンプリングを可能にするアルゴリズムが発見された．\n\nの2点が最後に加わることで，MCMCがベイズ計算法不動の金科玉条となった．\nこの 2.は計算機の性能の問題だけでなく，統計的画像処理 の分野から Gibbsサンプラー という新たなアルゴリズムが生まれた (Geman and Geman, 1984) ことによって実現された．13 これは，パラメータが \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)^\\top\\) と表されるとき，適切に定めた初期値 \\(\\theta_2^{(0)}\\) から初めて，条件付き分布からのサンプリング \\[\n\\theta_1^{(i)}\\sim p_1(\\theta_1^{(i)}|\\theta_2^{(i-1)},\\boldsymbol{y}),\n\\] \\[\n\\theta_2^{(i)}\\sim p_2(\\theta_2^{(i)}|\\theta_1^{(i)},\\boldsymbol{y}),\n\\] を繰り返すことで，最終的に \\(\\boldsymbol{\\theta}^{(i)}:=(\\theta_1^{(i)},\\theta_2^{(i)})^\\top\\) は全体として \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) に従うように収束する，という技法である．\nGibbs 法により，パラメータ \\(\\boldsymbol{\\theta}\\) の次元が大きく，直接のサンプリングが難しい場合や，条件付き分布の系はわかっているが結合分布がわからない場合14 でも，\\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots)\\) というように低次元変数の結合と理解することで，あるいは補助変数を追加してわざと問題を高次元化してでもそのような状況をうまく作り出すことで (Tanner and Wong, 1987) ，部分的な低次元サンプリングから組み上げることが出来るようになった．これを データ拡張 ともいう．さらにその後も，このアイデアが (Roberts and Rosenthal, 1999) のスライスサンプラーにつながっている．\nこの点をはっきり強調して示し，ベイズ統計学がすでに実行可能なものになっており，ベイズ統計学の基本問題（ Section 1.4 ）もすでに過去の遺物となっているということを，統計学界隈に広く知らしめたのが (Gelfand and Smith, 1990) であった．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#擬似周辺尤度法",
    "href": "posts/2023/Surveys/BayesianComp.html#擬似周辺尤度法",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.1 擬似周辺尤度法",
    "text": "3.1 擬似周辺尤度法\n実は尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) が解析的に得られない場合や計算が極めて困難になる場合でも，この不偏推定量があればMCMCを実行して事後分布を得るのに十分である (Andrieu and Roberts, 2009)．この尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) の不偏推定量を得るのに粒子フィルターを用いた場合を，特に粒子MCMCという (Andrieu et al., 2010)．\nこのときの不偏推定量の性能が最終的な Monte Carlo 推定量に影響する．不偏推定量の分散を改善するには，サブルーチンである粒子フィルターの反復数を増やす必要がある．すると本体であるMCMCの反復数とのトレードオフが生じる．こうしてアルゴリズムの最適な調整が課題になる．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#高次元問題に対処するmcmc",
    "href": "posts/2023/Surveys/BayesianComp.html#高次元問題に対処するmcmc",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.2 高次元問題に対処するMCMC",
    "text": "3.2 高次元問題に対処するMCMC\nほとんどのMCMC手法は，データサイズやモデルのパラメータサイズの増加に対して，計算負荷が飛躍的に上昇する次元の呪いに苦しむ．これを克服する手法はscalabilityの名の下に盛んに研究されている (鎌谷研吾, 2021, p. 394)．\n\n対象分布の探索を効率よく行う手法として，HMC (Hamiltonian Monte Carlo) 法が提案された (Neal, 2011)．他にも NUTS (No U-Turn Sampling) (Hoffman and Gelman, 2014), Metropolis-Adjusted Langevin Algorithm (Roberts and Tweedie, 1996), Stochastic Gradient MCMC (Nemeth and Fearnhead, 2021), PDMP (区分的確定なMCMC) (Bierkens et al., 2018), (Fearnhead et al., 2018) とジグザグサンプラーなどがある，\nより良い提案分布の選択法について，MH法の最適スケーリング法，適応的サンプリング，焼き戻しなどの手法がある．\n並列計算による効率化の方向性には，並列MCMC，完全サンプリングなどの手法がある．\n他の分散低減法に，Rao-Blackwell化 (Casella and Robert, 1996)，操作変数法などがある．\n\nジグザグサンプラーについては，以下の記事も参照：\n\n  \n    \n      \n      \n        新時代の MCMC を迎えるために\n        モンテカルロ法の発展とは，背後の物理現象からの離陸の歴史でした．非対称な Metropolis-Hastings アルゴリズムがどのように生まれたかと，その最先端のアイデアと言える連続時間 MCMC 法の歴史を，サンプルコード付きで紹介します．（2024 年度統計数理研究所オープンハウス）"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#近似ベイズ手法",
    "href": "posts/2023/Surveys/BayesianComp.html#近似ベイズ手法",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.3 近似ベイズ手法",
    "text": "3.3 近似ベイズ手法\n上述までの手法はいずれもシミュレーションを十分多く行えば（理論的には）任意の精度で正しい値を得ることができるが，15 その適用範囲やスケーラビリティが課題なのであった．そこで同時に，最初からある許容精度を定めた下での近似を実行することとし，代わりにより広い適用可能性と計算速度を得るための手法も探求されている．これを近似ベイズ法という．\n一つのアプローチはシミュレーションによる方法である．これにはABC (Approximation Bayesian Computation) (Tavaré et al., 1997) と BSL (Bayesian synthetic likelihood) (Price et al., 2018) の2つの手法があるが，いずれもデータ生成過程（モデル）の複雑性と高次元性という２つの障壁が併存したときでも使える手法である．ABCではまず事後分布 \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) をある低次元な要約統計量 \\(S:\\mathcal{Y}\\to\\mathbb{R}^d\\) を用いて \\(p(\\boldsymbol{\\theta}|S(\\boldsymbol{y}))\\) で近似し，さらに尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) を直接評価することは回避し，シミュレーションのみを用いて \\(p(\\boldsymbol{\\theta}|S(\\boldsymbol{y}))\\) を推定する．BSLはさらに尤度 \\(p(S(\\boldsymbol{y})|\\boldsymbol{\\theta})\\) にパラメトリックな仮定をおく．\n第二に最適化による方法がある．変分ベイズ手法とは，これは大きなパラメトリックモデル \\(\\{q^*(\\boldsymbol{\\theta})\\}\\) の中から \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) に最も近いものを選ぶ手法である．一方で INLA (integrated nested Laplace approximation) とは，Laplaceの近似（ Section 1.5 ）に最適化を組み合わせて高次元の問題にも対応する．\nABCでは逐次モンテカルロ法も大きな役割を果たしており，ABC-SMC (Sisson et al., 2007)，ABCフィルタリング (Jasra et al., 2012)，更には変分Bayes法への応用 (Tran et al., 2017) なども進んでいる．\n変分Bayesの枠組みでは，モデルの誤想定に頑健な手法の開発も試みられている (Wang and Blei, 2019)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-BayesianModeling",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-BayesianModeling",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.4 ベイズ統計モデリングが理論モデルの実証に役立つ",
    "text": "3.4 ベイズ統計モデリングが理論モデルの実証に役立つ\nベイズモデリングの有用性は，（上述のベイズ計算の問題を除けば）どんなに複雑で大規模なモデルでも，統一的な思想と方法で対応できる点にある．\n\nメカニズムを明示的に表現した数理社会学の数理モデルを，論理的に飛躍することなくダイレクトに統計モデルへと接続できるベイズ統計モデリングは，理論モデルベースの実証研究と相性のよい，たいへん便利な方法と言えるだろう． (浜田宏, 2022, p. 137)\n\nMCMCの開発とパッケージへの実装，そして安価で高性能な計算機が普及してからというもの，ベイズ統計学の興隆は目覚ましく，現在ではベイズ統計学は統計学に関する論文の1割強を占め，諸科学分野全体に浸透しつつある．経済学・心理学への応用は早かったのに比べて，政治科学・社会科学への応用は遅れ気味であり，社会学での使用はまだ稀であると言える (Lynch and Bartlett, 2019)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズによる逆問題",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズによる逆問題",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.5 ベイズによる逆問題",
    "text": "3.5 ベイズによる逆問題\nベイズ統計学の枠組みは，逆問題の文脈においても有用である．逆問題とは，観測データ \\(\\boldsymbol{y}\\) が与えられたときに，そのデータを説明するようなモデルのパラメータ \\(\\boldsymbol{\\theta}\\) を推定する問題である．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-BayesianCausalInference",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-BayesianCausalInference",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.6 ベイズによる因果推論",
    "text": "3.6 ベイズによる因果推論\n前節に挙げたベイズモデリングの美点は因果推論の文脈でも全く同様である．特に因果推論の問題では推定対象が複雑であることが多いが，このような場合でも全く同じ枠組みを提供してくれるのがベイズである．頻度論的接近では設定に応じた個別具体的な議論がベイズ計算の問題に帰着する点が利点として働くことは多いようである (Li et al., 2023)．\n実際，ベイズノンパラメトリック手法は2016年の大西洋因果推論カンファレンスのコンペティションで大きな成功を見ている (Dorie et al., 2019)．加えて強い理論的な保証も得られつつあり (Ray and van der Vaart, 2020)，これにより因果推論分野で大きな注目を集めている (Linero and Antonelli, 2023), (Daniels et al., 2023)．\n加えて，「あらゆる種の不確実性に対する統一的な定量化を与える」というベイズの性質は，因果推論から意思決定までの接続を地続きにし，例えば属人化医療などの現場でのダイナミックな意思決定に活用できることが期待される．\n\n不確実性を定量化するのに、ベイズ計算では必ず『確率』を使います。一般の人から見たら、統計で確率を使うのは当たり前と思うでしょうが、じつは他の統計手法ではそうでもなく、さまざまな解釈が生まれてしまう。定量化にはすべて統一的に確率を使うベイズ計算は、非常にシンプルなので、最終的にすべての統計はベイズに行き着くしかないと思っています．鎌谷研吾\n\nしかし，ベイズの方法が因果推論の分野で普及するための障壁は，近づきやすさにあると議論できる (Li et al., 2023)．従来の頻度論的な因果推論手法の成功には，潜在反応モデルの特定を殆どしなくて良いこと（モデルフリー），実装が簡単であることが少なからず寄与しているとすれば，ベイズ的接近もこれに当たるものを提供できるようになる必要があるだろう．Stan言語 (Carpenter et al., 2017) はこの方向への大きな試みである．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズ学習",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズ学習",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.7 ベイズ学習",
    "text": "3.7 ベイズ学習\n機械学習の手法を用いてベイズ推論を実行する営みをベイズ学習，または単に「機械学習への確率論的アプローチ」と言ってベイズの枠組みを暗に指す場合も多い (Murphy, 2022), (Ghahramani, 2013)．\n古典的な統計手法と同様，多くの既存の（頻度論的）手法にはベイズ手法の対応物が存在する．ベイズの方法だと推定の確信度合いもセットで定量化され，頻度論的対応物よりも得られる情報が多い一方で，計算は既存手法よりも難しいことが多いという構造は，機械学習においても変わらない．\n実際，現存のニューラルネットワークの訓練法を超えるベイズ計算法が今後提案されるとは考えにくいが，その最適化する所の目的関数が例えば正則化項付きの平均自乗誤差である場合は，ある正規事前分布と正規尤度に対するMAP推定量に対応する (Seitz 2022)．畢竟，多くの既存手法も「ベイズ学習を非ベイズ的な方法で実行している」と捉えられるのである（逆も然り）．\n中でもベイズ学習を採用するのが良い場面としては，モデルの大きさに対して学習に使えるデータの数が少ない場合や，モデルに事前情報を組み込みたい場合16 ，さらには医療・政策への応用など意思決定に繋げるために不確実性の定量化が肝要な場面などがあり得る．\n実際，ベイジアン・ニューラルネットワークでは計算の困難ささえ乗り越えれば，複数の適切なモデルに対し，事後分布によって平均を取って最終的なモデルとすることで，過学習を防止し (Mackay, 1995)，大きな性能改善を得ることができる (Wilson and Izmailov, 2020)．17"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#世紀の統計学とベイズの役割",
    "href": "posts/2023/Surveys/BayesianComp.html#世紀の統計学とベイズの役割",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.8 21世紀の統計学とベイズの役割",
    "text": "3.8 21世紀の統計学とベイズの役割\nこのように，21世紀に入ってからベイズの成功は目まぐるしく，この傾向はさらに進むと思われる．これは統計計算の手法の進化によって達成された．今後とも統計計算の手法は，シミュレーション・変分法・最適化の垣根を超えて多様化の一途を辿るだろう (Green et al., 2015, p. 857)．\nその中でも筆者は，ベイズ手法が提供する事後分布として得られる不確実性の表現・視覚化が，計算機・自然・人間の間のよきインターフェイスとなっていくことを願っている．18\n\nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice-—appropriate frequency calculations help to define such a tie. (Rubin, 1984)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#footnotes",
    "href": "posts/2023/Surveys/BayesianComp.html#footnotes",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n正式名称をThe Royal Society for the Improvement of Natural Knowledge by Experimentという↩︎\n当局の人間に死亡を報告する義務は全くなかった。その代わり、それぞれの教区では2人かそれ以上の死体を調査し、死因を決定する義務を負う調査員を任命していた。「調査員」は死亡を報告する毎に遺族より少額の手数料を徴収する資格が与えられていたので、教区では任命しなければ貧困のため救貧税による支援が必要となりそうな人間を割り当てていた。（Wikipediaページより）↩︎\nこれは統計的モデルとしてBernoulli分布 \\(Y_i|\\theta\\overset{\\text{iid}}{\\sim}\\mathrm{Ber}(\\theta)\\) を仮定するということである．↩︎\nパラメータ \\(\\theta\\) は「男児が生まれる確率」であるが，これ自体にも事前分布という「確率」\\(p(\\theta)\\) を導入することに戸惑う読者も居るだろう．しかし，これがベイズ統計学の特徴である．「男児が生まれる確率 \\(\\theta\\)」だろうとなんだろうと，「わからない」「不確実性がある」と主観的に感じるあらゆる対象に，確率分布を導入して事後分布を得ることで推論を実行する，これがベイズ統計学の枠組みの普遍性であり，無差別性であり，有用性を支えている．↩︎\n各 \\(\\theta\\) の下で目の前のデータ \\(y_1,\\cdots,y_n\\) が生成される確率 \\(p(\\boldsymbol{y}|\\theta)\\) が低いということは，「その \\(\\theta\\) から生成されたデータである確率は低い」という逆の発想ができる．そこで \\(p(\\boldsymbol{y}|\\theta)\\) という条件付き確率を尤度ともいう．今回は \\(p(\\boldsymbol{y}|\\theta)=\\theta^{\\sum_{i=1}^ny_i}(1-\\theta)^{\\sum_{i=1}^n(1-y_i)}\\) である．↩︎\nさらに，\\(g(\\theta)=\\theta^p\\) と取った場合，事後積率という統計量になる．等に \\(p=1\\) の場合が事後平均である．↩︎\nなお，1763に出版されたものはPriceによる補遺も付いた短縮版であり，全文は1974年に出版された．(Stigler, 1990)↩︎\npp.376-403 がBayesの論文の本論の内容であり pp.399-403 で計算法を３つのルールにまとめているが，その導出部は一部「長すぎるから掲載を省略する」とされている．↩︎\n一方で，Bayesの逆確率の問題への言及自体は，Laplaceの後年の1781年の著作Mémoire sur les probabilitésへのCondorcetによる序文で初めて登場する (Martin et al., 2023, p. 5)．↩︎\nnCatLab 参照．↩︎\n(安藤洋美, 1995) も参照．↩︎\n1970年にインテルが世界初の DRAMである Intel 1103 を発売した．Wikipediaページ参照．↩︎\n物理学では Heat Bath 法と呼ばれ古くから同様のアルゴリズムが存在したが，統計学界隈では現在でも Gibbs サンプラーと呼ばれる．↩︎\n統計的画像処理など，Markov 確率場 によってモデリングされる対象においてはよくある状況である．↩︎\nこの性質を指して，approximateの対義語としてexactという形容詞で表現される．↩︎\nfunctional Bayes (Sun et al., 2019) という手法では，希望する入力と出力の組を事前に用意するのみで，適切な事前分布を提案してくれる枠組みである．↩︎\n(Wilson and Izmailov, 2020) によると，二重降下現象も見られない．↩︎\n推定結果に自信がないときはそう表明してくれる機械は親しみやすい．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html",
    "href": "posts/2024/Kernels/GP.html",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nGauss 過程を用いた推論を実行するライブラリには，Matlab パッケージである GPML や，Python における GPy がある．"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html#gauss-過程回帰実践",
    "href": "posts/2024/Kernels/GP.html#gauss-過程回帰実践",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "1 Gauss 過程回帰（実践）",
    "text": "1 Gauss 過程回帰（実践）\n\n1.1 扱うデータ1\nデータ \\(x_1,\\cdots,x_{N},N=100\\) として，\\(\\mathrm{N}(0,0.8^2)\\) に従う乱数を用意する．これに対して， \\[\ny_i=\\sin(3x_i)+\\epsilon,\n\\] \\[\n\\epsilon\\sim\\mathrm{N}(0,0.09^2),\n\\] を通じて \\(y_1,\\cdots,y_N\\) を生成する．\n\n\nCode\nimport numpy as np\n\nN = 10\n\nnp.random.seed(1234)\n\nx = np.random.randn(N,1) * 0.8\n\ny = np.sin(3*x) + np.random.randn(N,1) * 0.09\n\nxs = np.linspace(-3,3,61).reshape(-1,1)\n\n\nこの非線型関数 \\(\\sin\\) を，Gauss 過程回帰がどこまで復元できるかが実験の主旨である．\n\n\n1.2 GPy を用いた場合\nGPy を用いて Gauss 過程回帰を行うには，GPy.models.gp_regression モジュールの GPRegression クラス\nclass GPRegression(X, Y, kernel=None, Y_metadata=None, normalizer=None, noise_var=1.0, mean_function=None)\nを用いる．ソースコードは こちら．\n引数のカーネル kernel は PGPy kernel オブジェクトを取り，デフォルトは rbf カーネルである．我々も RBF カーネル を用いることとする．これは GPy パッケージでは GPy.kern.src.rbf モジュールの RBF クラスで提供されている：\nclass RBF(input_dim, variance=1.0, lengthscale=None, ARD=False, active_dims=None, name='rbf', useGPU=False, inv_l=False)\nソースコードは こちら．\nモデルオブジェクトを初期化した後は次のように進む\n\noptimize メソッド でハイパーパラメータを最適化する．\noptimize(optimizer=None, start=None, messages=False, max_iters=1000, ipython_notebook=True, clear_after_finish=False, **kwargs)\nこれはインスタンスの self.log_likelihood と self.log_likelihood_gradient を用いて，負の対数尤度を最小化する形で行われる．\npredict メソッド でテスト点での予測を行う．\npredict(Xnew, full_cov=False, Y_metadata=None, kern=None, likelihood=None, include_likelihood=True)\n返り値は事後平均と事後分散を numpy.ndarray として返す．\nmatplotlib を用いて予測の結果をプロットする．\n\n\n\nCode\nimport GPy\nimport matplotlib.pyplot as plt\n\nkernel = GPy.kern.RBF(input_dim=1, variance=1.0)\nmodel = GPy.models.GPRegression(x, y, kernel)\n\nmodel.optimize()\nmu, var = model.predict(xs)\n\n# テスト点での平均と95%信頼区間のプロット\nupper = mu + 1.96*np.sqrt(var)\nlower = mu - 1.96*np.sqrt(var)\nplt.fill_between(xs[:,0], lower[:,0], upper[:,0], color='lightgray', label='95% confidence interval', alpha=0.5)\nplt.plot(xs, mu, label='Predicted mean')\nplt.scatter(x, y, c='r', label='Observations', s=10)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport GPy\nimport matplotlib.pyplot as plt\n\nkernel = GPy.kern.RBF(input_dim=1, variance=1.0)\nmodel = GPy.models.GPRegression(x, y, kernel)\nmodel.optimize()\n\nxs = np.linspace(x.min(), x.max(), 1000)[:, None]\nmu, var = model.predict(xs)\n\nupper = mu + 1.96 * np.sqrt(var)\nlower = mu - 1.96 * np.sqrt(var)\n\nfig, ax = plt.subplots(figsize=(6, 4))  # グラフサイズを小さく\n\n# 背景を白に\nax.set_facecolor('white')\n\n# グラフ領域を削除\nax.patch.set_visible(False)\n\n# 軸を細く\nax.spines['bottom'].set_linewidth(0.5)\nax.spines['left'].set_linewidth(0.5)\n\n# メモリを非表示\nax.tick_params(axis='both', which='both', length=0, labelleft=False, labelbottom=False, left=False, bottom=False)\n\n# 軸ラベルを削除\nax.set_xlabel('')\nax.set_ylabel('')\n\n# 凡例を非表示\nax.legend().set_visible(False)\n\n# データプロット\nax.fill_between(xs[:, 0], lower[:, 0], upper[:, 0], color='lightgray', alpha=0.5)\nax.plot(xs[:, 0], mu[:, 0], color='k', lw=1)\nax.scatter(x[:, 0], y[:, 0], c='b', s=30)\n\nplt.tight_layout(pad=0.2)\nplt.show()\n\n\n /var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_9031/3363038710.py:35: UserWarning:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\n特に \\([-2,2]\\) の区間において，元の関数 \\(\\sin\\) をよく復元できていることが分かる．実際，\\(y=\\sin(3x)\\) と重ねてプロットすると次の通り：\n\n\n\n\n\n\n\n\n\n\n\n1.3 scikit-learn を用いた場合\n\n\n\n\n\n\n補足：scikit-learn における Gauss 過程回帰\n\n\n\n\n\nこのような単純な解析では，scikit-learn と用いるとより同じ分析が実行できる．\n\n\nCode\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n\ngp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1)\n\n# モデルの学習\ngp.fit(x, y.ravel())\n\nmu, s2 = gp.predict(xs, return_std=True)\n\n# テスト点での平均と95%信頼区間のプロット\nplt.fill_between(xs.ravel(), mu - 1.96 * s2, mu + 1.96 * s2, color='lightgray', label='95% confidence interval', alpha=0.5)\nplt.plot(xs, mu, label='Predicted mean')\nplt.scatter(x, y, c='r', label='Observations', s=10)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html#gauss-過程による分類",
    "href": "posts/2024/Kernels/GP.html#gauss-過程による分類",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "2 Gauss 過程による分類2",
    "text": "2 Gauss 過程による分類2\n本質的には Gauss 過程回帰と変わらないが，回帰の場合と変え得る．\n\n2.1 扱うデータ\nここでは， \\[\nm_1:=\\begin{pmatrix}3/4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}-3/4\\\\0\\end{pmatrix},\n\\] \\[\n\\Sigma_1:=\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix},\\quad\\Sigma_2:=\\begin{pmatrix}1&0.95\\\\0.95&1\\end{pmatrix},\n\\] とし，\\(\\mathrm{N}_2(m_1,\\Sigma_1)\\) から \\(n_1:=320\\) データ，\\(\\mathrm{N}_2(m_2,\\Sigma_2)\\) から \\(n_2:=160\\) データを生成する：\n\n\nCode\nn1, n2 = 320, 160\nS1 = np.eye(2)\nS2 = np.array([[1, 0.95], [0.95, 1]])\nm1 = np.array([0.75, 0])\nm2 = np.array([-0.75, 0])\n\nx1 = np.random.multivariate_normal(m1, S1, n1)\nx2 = np.random.multivariate_normal(m2, S2, n2)\n\nx = np.vstack((x1, x2))\n\ny1 = -np.ones(n1)\n1y2 = np.ones(n2)\ny = np.concatenate((y1, y2)).reshape(-1,1)\n\nplt.plot(x1[:, 0], x1[:, 1], 'o', label='Class 1')\nplt.plot(x2[:, 0], x2[:, 1], '*', label='Class 2')\nplt.legend()\nplt.show()\n\n\n\n1\n\nクラスラベルは \\(\\{\\pm1\\}\\) であることに注意．\n\n\n\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\n\\(n_1:n_2=2:1\\) であるから，このデータは Gauss 混合モデル \\[\n\\frac{2}{3}\\phi(x;m_1,\\Sigma_1)+\\frac{1}{3}\\phi(x;m_2,\\Sigma_2)\n\\tag{1}\\] からのデータと見れる．ただし，\\(\\phi(x;m,\\Sigma)\\) は \\(\\mathrm{N}_2(\\mu,\\Sigma)\\) の密度関数とした．\nサンプリング点は \\([-4,4]^2\\) 内の幅 \\(0.1\\) の格子点とする：\n\n\nCode\nt1, t2 = np.meshgrid(np.arange(-4, 4.1, 0.1), np.arange(-4, 4.1, 0.1))\nt = np.column_stack([t1.flat, t2.flat])\n\n\n点 \\(x\\) でモデル 1 からのデータが観測されたとき，これがクラス \\(1,2\\) からのものである確率 \\(p_1,p_2\\) は \\[\n\\begin{align*}\n    p_1&=\\frac{n_1}{n_1+n_2}\\phi(x;m_1,\\Sigma_1)\\\\\n    &=\\frac{1}{2\\pi(n_1+n_2)}\\cdot n_1\\frac{e^{-\\frac{1}{2}(x-m_1)^\\top\\Sigma_1^{-1}(x-m_1)}}{\\sqrt{\\det\\Sigma_1}}\n\\end{align*}\n\\] \\[\np_2= \\frac{1}{2\\pi(n_1+n_2)}\\cdot n_2\\frac{e^{-\\frac{1}{2}(x-m_2)^\\top\\Sigma_2^{-1}(x-m_2)}}{\\sqrt{\\det\\Sigma_2}}\n\\] である．\nよって，\\(x\\in[-4,4]^2\\) がクラス \\(2\\) からのものである確率を，等高線 (contour) としてプロットすると，次の通りになる：\n\n\nCode\ninvS1 = np.linalg.inv(S1)\ninvS2 = np.linalg.inv(S2)\ndetS1 = np.linalg.det(S1)\ndetS2 = np.linalg.det(S2)\n\ntmm1 = t - m1\np1 = n1 * np.exp(-0.5 * np.sum(tmm1.dot(invS1) * tmm1, axis=1)) / np.sqrt(detS1)\n\ntmm2 = t - m2\np2 = n2 * np.exp(-0.5 * np.sum(tmm2.dot(invS2) * tmm2, axis=1)) / np.sqrt(detS2)\n\nposterior = p2 / (p1 + p2)\n\n# 等確率等高線のプロット\ncontour_levels = np.arange(0.1, 1, 0.1)\nplt.contour(t1, t2, posterior.reshape(t1.shape), levels=contour_levels)\n\n# データポイントのプロット\nplt.plot(x1[:, 0], x1[:, 1], 'o', label='Class 1', alpha=0.5)\nplt.plot(x2[:, 0], x2[:, 1], '*', label='Class 2', alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.2 モデル\n平均は \\(0\\) とし，共分散関数は 関連度自動決定 (ARD: Autonatic Relevance Determination) (MacKay, 1994), (Neal, 1996, p. 16) を用いる．\nこれは，２つの入力 \\(x_1,x_2\\) が異なる重要度を持つ場合，それぞれの入力に対するスケールパラメータを導入する手法である．\nこれは，GPy.kern.RBF 関数のキーワード引数 ARD=True を通じて実装できる：\n\n\nCode\nimport time\n\nstart_time = time.time()\n\nmeanfunc = GPy.mappings.Constant(2,1)\nkernel = GPy.kern.RBF(input_dim=2, ARD=True)\n\nmodel = GPy.models.GPClassification(x, y, kernel=kernel, mean_function=meanfunc)\nmodel.optimize()\n\n# テストデータセットに対する予済分布の計算\ny_pred, _ = model.predict(t)\n\nend_time = time.time()\n\n# 予測確率の等高線プロット\nplt.figure(figsize=(8, 6))\nplt.plot(x1[:,0], x1[:,1], 'o', label='Class 1', alpha=0.5)\nplt.plot(x2[:,0], x2[:,1], '*', label='Class 2', alpha=0.5)\ncontour = plt.contour(t1, t2, y_pred.reshape(t1.shape), levels=np.linspace(0, 1, 10))\nplt.clabel(contour, inline=1, fontsize=10)\nplt.legend()\nplt.show()\n\nelapsed_time = end_time - start_time\nprint(f\"実行時間: {elapsed_time:.1f} 秒\")\n\n\n\n\n\n\n\n\n\n実行時間: 14.3 秒\n\n\n図 1 の真の構造の特徴を捉えていることが判る．"
  },
  {
    "objectID": "posts/2024/Kernels/GP.html#footnotes",
    "href": "posts/2024/Kernels/GP.html#footnotes",
    "title": "Gauss 過程を用いた統計解析１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDocumentation for GPML Matlab Code version 4.2 3c 節を参考にした．↩︎\nDocumentation for GPML Matlab Code version 4.2 4e 節を参考にした．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/VAE.html",
    "href": "posts/2024/Kernels/VAE.html",
    "title": "VAE：変分自己符号化器",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/VAE.html#vae-kingma-welling2014",
    "href": "posts/2024/Kernels/VAE.html#vae-kingma-welling2014",
    "title": "VAE：変分自己符号化器",
    "section": "1 VAE (D. Kingma and Welling, 2014)",
    "text": "1 VAE (D. Kingma and Welling, 2014)\n\n1.1 導入\nPyTorch を用いることで詳細を省略し，VAE の構造を概観することとする．\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image, make_grid\n\n今回は，MNIST データセットを用い，隠れ次元 400 を通じて潜在次元 200 まで圧縮する．\n\ndataset_path = '~/hirofumi/datasets'\n\nDEVICE = torch.device(\"mps\")\n\nbatch_size = 100\n\nx_dim = 784\nhidden_dim = 400\nlatent_dim = 200\n\nlr = 1e-3\n\nepochs = 30\n\n\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nmnist_transform = transforms.Compose([\n        transforms.ToTensor(),\n])\n\nkwargs = {'num_workers': 0, 'pin_memory': True} \n\ntrain_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\ntest_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)\n\nPyTorch の Dataset と DataLoader は，訓練やテスト用のデータセットの簡単なアクセスと，それに対する iterable オブジェクトを提供する．\n\n\n\n\n\n\nM2 Mac 上での実行\n\n\n\n\n\nまず，次のようにして仮想環境を用意する：\npython3 -m venv VAE\nsource VAE/bin/activate\npip install torch\nM2 Mac では Metal Performance Shaders (MPS) という Apple の GPU アクセラレーション技術が利用可能で，PyTorch 1.12 からはこれをサポートしている．\n\nimport torch\nprint(torch.__version__)\nprint(torch.backends.mps.is_available())\n\n2.4.0\nTrue\n\n\n\n\n\n\n\n\n\n\n\nDataLoader worker (pid(s) 9044) exited unexpectedly\n\n\n\n\n\n上記のエラーは，DataLoader が並列処理によりデータを読み込むことに失敗したことを意味する．\nメモリ不足も考えられるが，num_workers=0 として単一プロセスで実行することでもエラーが抑えられる．\n今回は軽量な計算であるから，これで良いということである．\n\n\n\n\n\n1.2 モデルの定義\n\n1.2.1 エンコーダー\nエンコーダーはデータを受け取り，２層の全結合隠れ層を通じて，「平均」と「対数分散」の名前がついた計 400 次元の潜在表現を得る．\n\nclass Encoder(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(Encoder, self).__init__()\n\n1        self.FC_input = nn.Linear(input_dim, hidden_dim)\n        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n        self.FC_var   = nn.Linear(hidden_dim, latent_dim)\n        \n        self.LeakyReLU = nn.LeakyReLU(0.2)\n        \n        self.training = True\n        \n    def forward(self, x):\n        h_       = self.LeakyReLU(self.FC_input(x))\n2        h_       = self.LeakyReLU(self.FC_input2(h_))\n        mean     = self.FC_mean(h_)\n3        log_var  = self.FC_var(h_)\n        \n        return mean, log_var\n\n\n1\n\nnn.Linear は PyTorch による全結合層 \\(y=xA^\\top+b\\) の実装である．\n\n2\n\nここまで２層の全結合層にデータを通して，最終的な出力h_を得ており，次の段階で最終的な潜在表現を得る．\n\n3\n\n最後の隠れ層の出力h_に関して平均と対数分散という名前のついた最終的な出力を，やはり全結合層を通じて得る（最終層なので活性化なし）．\n\n\n\n\n\n\n1.2.2 デコーダー\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, hidden_dim, output_dim):\n        super(Decoder, self).__init__()\n        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_output = nn.Linear(hidden_dim, output_dim)\n        \n        self.LeakyReLU = nn.LeakyReLU(0.2)\n        \n    def forward(self, x):\n        h     = self.LeakyReLU(self.FC_hidden(x))\n        h     = self.LeakyReLU(self.FC_hidden2(h))\n        \n1        x_hat = torch.sigmoid(self.FC_output(h))\n        return x_hat\n\n\n1\n\n最後の出力は，エンコーダーとは違い，シグモイド関数を通して確率分布x_hatとする．\n\n\n\n\n\n\n1.2.3 モデル\nVAE はエンコーダーとデコーダーを連結し，１つのニューラルネットワークとして学習する．\n\nclass Model(nn.Module):\n    def __init__(self, Encoder, Decoder):\n        super(Model, self).__init__()\n        self.Encoder = Encoder\n        self.Decoder = Decoder\n        \n    def reparameterization(self, mean, var):\n1        epsilon = torch.randn_like(var).to(DEVICE)\n2        z = mean + var*epsilon\n        return z\n        \n                \n    def forward(self, x):\n3        mean, log_var = self.Encoder(x)\n4        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n5        x_hat            = self.Decoder(z)\n        \n6        return x_hat, mean, log_var\n\n\n1\n\nこれは サンプリングイプシロン と呼ばれる値である．\n\n2\n\nここで reparametrization trick を行っている．\n\n3\n\n入力 x があったならば，まずエンコーダーに通して mean, log_var を得る．\n\n4\n\n元々 log_var の名前の通り対数分散として扱うこととしていたので，２で割り指数関数に通すことで標準偏差を得る．この平均と標準偏差について reparametrization trick を実行し，デコーダーに繋ぐ．\n\n5\n\nデコーダーではデータの潜在表現 z を受け取り，デコードしたものを x_hat とする．\n\n6\n\n返り値は，デコーダーの出力 x_hat だけでなく，潜在表現 mean, log_var も含むことに注意．\n\n\n\n\n\nencoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\ndecoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n\n1model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)\n\n\n1\n\n.to(DEVICE) により，モデルを M2 Mac の MPS デバイス上に移送している．\n\n\n\n\n\n\n\n1.3 モデルの訓練\n最適化には Adam (D. P. Kingma and Ba, 2017) を用い，バイナリ交差エントロピー（BCE）を用いる．これは nn.BCELoss に実装がある．\n\nfrom torch.optim import Adam\n\nBCE_loss = nn.BCELoss()\n\ndef loss_function(x, x_hat, mean, log_var):\n    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n    KLD      = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n\n    return reproduction_loss + KLD\n\n\noptimizer = Adam(model.parameters(), lr=lr)\n\nここでの損失関数は，真のデータ x をデコーダーが復元できているかを交差エントロピーで測った reproduction_loss と，潜在表現がどれだけ \\(\\mathrm{N}_d(0,I_d),d=200\\) に近いかを KL 乖離度で測った KLD の和で定義されている．1\nVAE の標準的な目的関数 とは違う形をしていることに注意．\n\n\n\n\n\n\n訓練の実行\n\n\n\n\n\n\nimport time\n\nprint(\"Start training VAE...\")\n1model.train()\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    overall_loss = 0\n    for batch_idx, (x, _) in enumerate(train_loader):\n2        x = x.view(batch_size, x_dim)\n3        x = x.to(DEVICE)\n\n4        optimizer.zero_grad()\n\n        x_hat, mean, log_var = model(x)\n        loss = loss_function(x, x_hat, mean, log_var)\n        \n        overall_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n\ntotal_time = time.time() - start_time\nprint(\"Finish!! Total time: \", total_time)\n\n\n1\n\nPyTorch のモデルオブジェクトを訓練モードにするメソッド．Dropout や Batch Normalization 層がある場合は，これにより訓練時の挙動を示すようになる．\n\n2\n\n事前に定めた batch_size に従ってバッチを展開．\n\n3\n\nデータを GPU に移動．\n\n4\n\n勾配をゼロに初期化するとのこと．\n\n\n\n\nStart training VAE...\n    Epoch 1 complete!   Average Loss:  174.45440591089314\n    Epoch 2 complete!   Average Loss:  129.366964569856\n    Epoch 3 complete!   Average Loss:  116.69913125065213\n    Epoch 4 complete!   Average Loss:  112.35357475675605\n    Epoch 5 complete!   Average Loss:  109.92540430339629\n    Epoch 6 complete!   Average Loss:  108.33972220954195\n    Epoch 7 complete!   Average Loss:  107.14160705668301\n    Epoch 8 complete!   Average Loss:  106.20647785371452\n    Epoch 9 complete!   Average Loss:  105.42789199446995\n    Epoch 10 complete!  Average Loss:  104.9622272798414\n    Epoch 11 complete!  Average Loss:  104.39062490218072\n    Epoch 12 complete!  Average Loss:  103.96445004369261\n    Epoch 13 complete!  Average Loss:  103.49826466963168\n    Epoch 14 complete!  Average Loss:  103.10589126408598\n    Epoch 15 complete!  Average Loss:  102.75672558104654\n    Epoch 16 complete!  Average Loss:  102.47924416671015\n    Epoch 17 complete!  Average Loss:  102.25440002543301\n    Epoch 18 complete!  Average Loss:  102.09212112961707\n    Epoch 19 complete!  Average Loss:  101.80551883347245\n    Epoch 20 complete!  Average Loss:  101.61719139646807\n    Epoch 21 complete!  Average Loss:  101.45744191164962\n    Epoch 22 complete!  Average Loss:  101.27843764672892\n    Epoch 23 complete!  Average Loss:  101.1599205544397\n    Epoch 24 complete!  Average Loss:  101.04871442638773\n    Epoch 25 complete!  Average Loss:  100.8713441999687\n    Epoch 26 complete!  Average Loss:  100.73917756808223\n    Epoch 27 complete!  Average Loss:  100.68475770163815\n    Epoch 28 complete!  Average Loss:  100.58486650928631\n    Epoch 29 complete!  Average Loss:  100.44012970836812\n    Epoch 30 complete!  Average Loss:  100.39852615687604\nFinish!! Total time:  130.87328815460205\n\n\n\n\n\n\n\n1.4 モデルの評価\nテスト用データの最初のバッチについて処理し，入力データと出力データを見比べてみる．\n\nmodel.eval()\n\n1with torch.no_grad():\n    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n        x = x.view(batch_size, x_dim)\n        x = x.to(DEVICE)\n        \n        x_hat, _, _ = model(x)\n\n\n        break\n\n\n1\n\n勾配評価を無効化するコンテクストマネージャーで，メモリの使用を節約できるという．\n\n\n\n\n  0%|          | 0/100 [00:00&lt;?, ?it/s]  0%|          | 0/100 [00:00&lt;?, ?it/s]\n\n\nimport matplotlib.pyplot as plt\n\ndef show_image(x, idx):\n    x = x.view(batch_size, 28, 28)\n\n    fig = plt.figure()\n    plt.imshow(x[idx].cpu().numpy())\n\nshow_image(x, idx=0)\nshow_image(x_hat, idx=0)\n\n\n\n\n\n\n\n\n\n\n\n(a) 左がテストデータ，右がその VAE による復元\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n図 1\n\n\n\n左が入力で右が出力である．\n\n\n1.5 データの生成\nここで，エンコーダを取り外してデコーダーからデータを生成する．\n損失関数（第 1.3 節）には，潜在空間におけるデータを標準正規分布に近付けるための項が入っていたため，データの潜在表現は極めて標準正規分布に近いとみなすことにする．\nすると，潜在表現と同じ次元の正規乱数から，データセットに極めて似通ったデータが生成できるだろう．\nwith torch.no_grad():\n    noise = torch.randn(batch_size, latent_dim).to(DEVICE)\n    generated_images = decoder(noise)\n\nsave_image(generated_images.view(batch_size, 1, 28, 28), 'generated_sample.png')\nfor i in range(4):\n    show_image(generated_images, idx=i)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n図 2"
  },
  {
    "objectID": "posts/2024/Kernels/VAE.html#vq-vae-vandenoord2017",
    "href": "posts/2024/Kernels/VAE.html#vq-vae-vandenoord2017",
    "title": "VAE：変分自己符号化器",
    "section": "2 VQ-VAE (van den Oord et al., 2017)",
    "text": "2 VQ-VAE (van den Oord et al., 2017)\n\n2.1 導入\n\nDEVICE = torch.device(\"mps\")\n\nbatch_size = 128\nimg_size = (32, 32)\n\ninput_dim = 3\nhidden_dim = 512\nlatent_dim = 16\nn_embeddings= 512\noutput_dim = 3\ncommitment_beta = 0.25\n\nlr = 2e-4\n\nepochs = 50\n\nprint_step = 50\n\n\nfrom torchvision.datasets import CIFAR10\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nmnist_transform = transforms.Compose([\n        transforms.ToTensor(),\n])\n\nkwargs = {'num_workers': 1, 'pin_memory': True} \n\ntrain_dataset = CIFAR10(dataset_path, transform=mnist_transform, train=True, download=True)\ntest_dataset  = CIFAR10(dataset_path, transform=mnist_transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n\n2.2 モデルの定義\n\n2.2.1 エンコーダー\nVQ-VAE は画像への応用を念頭に置いているため，エンコーダーには CNN アーキテクチャ を採用する．\n\nclass Encoder(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=(4, 4, 3, 1), stride=2):\n        super(Encoder, self).__init__()\n        \n        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_size\n        \n        self.strided_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, stride, padding=1)\n        self.strided_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, stride, padding=1)\n        \n        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_3, padding=1)\n        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_4, padding=0)\n        \n        self.proj = nn.Conv2d(hidden_dim, output_dim, kernel_size=1)\n        \n    def forward(self, x):\n        \n        x = self.strided_conv_1(x)\n        x = self.strided_conv_2(x)\n        \n        x = F.relu(x)\n        y = self.residual_conv_1(x)\n        y = y+x\n        \n        x = F.relu(y)\n        y = self.residual_conv_2(x)\n        y = y+x\n        \n        y = self.proj(y)\n        return y"
  },
  {
    "objectID": "posts/2024/Kernels/VAE.html#参考文献",
    "href": "posts/2024/Kernels/VAE.html#参考文献",
    "title": "VAE：変分自己符号化器",
    "section": "3 参考文献",
    "text": "3 参考文献\n\n本稿は，Minsu Jackson Kang 氏 による チュートリアル を参考にした．\nVAE には数々の変種があるが，その PyTorch による簡単な実装は Anand K Subramanian の このレポジトリ にリストアップされている．\nVAE の潜在表現は t-SNE などを用いて可視化でき，(Murphy, 2023, p. 635) の例などでも，潜在空間において手書き数字がクラスごとによく分離されていることが確認できる．"
  },
  {
    "objectID": "posts/2024/Kernels/VAE.html#footnotes",
    "href": "posts/2024/Kernels/VAE.html#footnotes",
    "title": "VAE：変分自己符号化器",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nなお，mean.pow(2) は Julia の mean.^2 に同じ．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html",
    "href": "posts/2024/Kernels/Deep.html",
    "title": "数学者のための深層学習概観",
    "section": "",
    "text": "深層生成モデル２\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル３\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル５\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル１\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル６\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n深層生成モデル４\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#歴史",
    "href": "posts/2024/Kernels/Deep.html#歴史",
    "title": "数学者のための深層学習概観",
    "section": "1 歴史",
    "text": "1 歴史\n深層学習とは，多層パーセプトロンを用いた機械学習の手法をいう．\n\n\n1.1 ニューラルネットワークの黎明\n\n1.1.1 脳の解明\n(McCulloch and Pitts, 1943) は脳の神経回路を命題論理の枠組みで扱い，ニューロンの論理素子としての機能を考察した（McCulloch-Pitts の形式ニューロンや閾素子と呼ばれる）．なお，パーセプトロンはシグモイド関数を活性化関数に用いた場合，Turing 完全である (Siegelmann and Sontag, 1991)．\n脳を模したモデルとして，当然学習能力をどうモデルに組み込むかが問題になる．これはシナプスの結合荷重の変化によるものだという仮設は古くからあったが，最も明確な形で表現したのが (Hebb, 1949) であった．\n具体的には Hebb 則は，２つの正の相関を持つ（＝共起しやすい）ニューロンの間の荷重は増強される，というものである．1 これは連想記憶のモデルとして提案された（第 2.7.1 節も参照）．\n\n\n1.1.2 パーセプトロン\nパーセプトロンは，脳の記憶と認識のモデルとして，(Rosenblatt, 1958) が心理学の学会誌に発表した．2\n複雑過ぎるものに対する理論解析は進まなかった．\nそのような中で (Minsky and Papert, 1969) は線型分離不可能な問題に対しては線型単純パーセプトロンは解を見つけられないほど機能は劣り，かといって複雑なものは計算量が爆発するので，結局パーセプトロンは使い物にならないのではないかとの見方を示した (甘利俊一, 1989, p. 130)．\n現状の深層学習の成功を見ている者からすれば，これだけのことでパーセプトロンの研究が下火になってしまったことは驚くべきことに感じる．\n(甘利俊一, 1989) はこの点を，次のように断罪している．\n\nミンスキーらは，自分たちの立てた問題は正しく解いた．しかし問題の立て方を誤ったのである．(甘利俊一, 1989, p. 132)\n\n\n\n1.1.3 多層パーセプトロン\nパーセプトロンの発明の時点で，深層学習のモデルとしてはすでに完成していた．ただ，(Minsky and Papert, 1969) の指摘の通り，計算複雑性や学習アルゴリズムの問題が残り続けた．\n多層のパーセプトロンでは中間層を学習させることが課題であった．これに対しては，活性化関数に可微分なものを用いて確率的勾配降下法によって学習させることで解決できること (Amari, 1967) などは早い時期から議論されていた．この問題点は，局所解に囚われることであった．\n多層のパーセプトロンに対して，局所解に囚われるなどの問題はあれど，極めて多くの場合で誤差逆伝播法が有効な学習法になることが広く周知されたのは，(Rumelhart et al., 1986), (Rumelhart et al., 1987) であり，これが深層学習の第二次ブームの着火剤となった．3\nすぐさま英語の発音を学習させた研究 (Sejnowski and Rosenberg, 1987) の デモ も発表され，大きな波紋を呼び起こした．ここでも特徴的な内部表現が観察された．\n局所解に囚われることが解決された訳ではない．技術的な問題点（特に，結局深層モデルの訓練はうまくいかないこと）はあれど，ニューラルネットワークは実用的に極めて簡単に使えるモデルであると知らしめたのである．\n\nIn short, we believe that we have answered Minsky and Papert’s challenge and have found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced. (Rumelhart et al., 1987, p. 361)\n\n現在は，ドロップアウト や区分的線型な活性化関数 ReLU を用いるなどの工夫がなされている (Goodfellow et al., 2014)．\n\n\n\n1.2 深層化の歴史4\n\n1.2.1 深層化の障壁\n誤差逆伝播法をニューラルネットに使うことで，表現学習がなされることの発見 (Rumelhart et al., 1986) から，深層学習の分野は次の点で変化したという：\n\n神経科学・生物学的なモチベーションから遊離し，より確率論的・統計学的なアプローチが主流になった (Bishop and Bishop, 2024, p. 19)．\n多層のニューラルネットワークと高次元データに対する理論的な研究が加速した (MacKay, 2003, p. 535)．\n\nしかし，画像識別タスクに特化して結合構造を予め作り込むことで学習を容易にしてある畳み込みニューラルネットワーク (CNN) (LeCun et al., 1998) などを除いて，２層以上のニューラルネットワークの成功した応用例は殆どなかった．これは，勾配消失 により，どんなに多層なニューラルネットワークを構築しても，最後の２層程度しか意味のあるパラメータを学習できなかったためである．\nそのために，多層のニューラルネットワークでは表現学習は難しいという認識が広まり，複雑なタスクに対しては，問題固有の特徴抽出技法が編み出されるのみで，一般的な解決法はなかった．\n３層のニューラルネットワークも，隠れ層の幅が無限大の極限で，任意の連続関数を近似できるという普遍近似定理という抽象的な慰め (Hecht-Nielsen, 1989) があるのみであった．\nそこで多くの研究者は，SVM (Cortes and Vapnik, 1995) やカーネル法，Gauss 過程に現実的な打開策を求め始めたのである．これは冬の時代とも呼ばれる．\n\n\n1.2.2 ネオコグニトロンと CNN\n(Wiesel and Hubel, 1959) は猫の視覚野には，単純細胞と複雑細胞の２種類の細胞があることを発見した．また，生後すぐの猫を一日交代で異なる片目を覆って成長した猫では，多くの視覚野の神経細胞は単眼性になる (Hubel, 1967)．\nこれらの観察から，(von der Malsburg, 1973) や (Fukushima, 1975) は，特徴抽出細胞が自己形成するメカニズムを，自己組織化のキーワードの下で調べた．特に後者は コグニトロン というモデルを提案した．\n(Fukushima, 1980) の ネオコグニトロン は初の深層モデルの例と言える．5 これは，単純細胞層と複雑細胞層を交互に深く重ねたネットワークである．\nしかし 福島 は学習の問題を中心に扱った訳ではなかった．6 (LeCun et al., 1998) の畳み込みニューラルネットワーク LeNet は，ネオコグニトロンを誤差逆伝播法により教師あり学習させたものと言える．\nこのモデルは後に AlexNet としてダントツの性能で世界を驚かせることになる．\n画像データはピクセルを節としたグラフデータとみなすこともでき，CNN を一般化する形で GNN (Graph Neural Network) も提案されている (J. Zhou et al., 2020), (Wu et al., 2021), (Veličković, 2023)．\n\n\n1.2.3 自己符号化器\n(Cottrell and Munro, 1988) は中間層の幅を小さくし，入力信号自身を教師信号として誤差逆伝播法により学習することで，隠れ層に入力の低次元表現が学習され，主成分分析に用いることが出来ることを報告した．\nこれは入力層と出力層の素子数を一致させ，出力が入力に近づくように訓練される．このようなモデルを 自己符号化器 (autoencoder) または 自己連想ネットワーク (auto-associative neural network) と呼ぶ．\n内部表現を獲得するとされる中間層に対して，それより前半の層全体を符号化器，それ以降を復号化器と呼ぶ．7\n(Baldi and Hornik, 1989) は活性化関数がない３層の場合，自己符号化器を訓練させることは主成分分析を実行することに等価であることを示した．\nさらに層を増やすことで，非線型な次元圧縮が可能であることが (DeMers and Cottrell, 1992) で示された．この論文では，データの空間内の（非線型な）部分多様体の局所座標が学習されたり，人間の顔の写真のデータ圧縮が出来ることが実証されている．\n自己符号化器はスタッキングが可能である (Ballard, 1987)．このことが，次の節で述べる事前学習のアイデアに繋がる．\n\n\n1.2.4 自己符号化器を用いた事前学習 (Bengio et al., 2006)\n自己符号化器自体を符号圧縮と表現学習に用いることは，他の手法と比べて特別優れているという訳でもなかった．\nしかし，深層ニューラルネットワークを 層ごとに事前に教師なし学習をする ことで，後の教師あり学習において勾配消失などの問題が回避できるというアイデア (Hinton and Salakhutdinov, 2006), (Hinton et al., 2006) は画期的であり，深層学習 (Deep Learning) という言葉が広まったのもこの頃であるという (Schmidhuber, 2015, p. 96)．8\n入力に近い方から，１層ごとに，前層の出力を入力として自己符号化器と見て訓練するのである (Bengio et al., 2006)．(Larochelle et al., 2007) はこの事前学習によって，素性の良い局所解の近くにパラメータの初期値が調整されるということを実験的に示している．\nこの発見は深層模型の訓練を可能にするという大きなブレイクスルーを，教師なしの表現学習と教師あり学習による調整とに問題を分離して解くことによって成し遂げたと言える．\nまた，単純な自己符号化器の代わりに，ノイズが加えられたデータを入力しこれを復元するようにに学習する denoising autoencoder を用いた方が，深層モデルにより良い初期値を与える潜在表現を獲得できることが報告されている (Vincent et al., 2008)．9\n\n\n1.2.5 AlexNet (Krizhevsky et al., 2012)\nImageNet データベース (Deng et al., 2009) を用いた判別コンテスト ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) で，ダントツで優勝した AlexNet (Krizhevsky et al., 2012) が大きなターニングポイントとなった．\nこれも (LeCun et al., 1998) の CNN を基にした８層のモデルで，活性化関数には ReLU とドロップアウトによる正則化が用いられていた．学習も，NVIDIA 社の GPU を用いていた．\n\n\n1.2.6 ResNet (He et al., 2016)\n20 層以上の多層ニューラルネットワークの学習の困難さを初めて解決したのが (He et al., 2016) の Residual Network (ResNet) であった．10\nこのモデルは 152 層もあったが効率的に訓練することが可能で，2015 年の ILSVRC でダントツで優勝し，しかも初めて人間の誤答率 5% (Dodge and Karam, 2017) を下回ったのである．\nそのアイデアは，各層の入力 \\(x\\) を出力に再度加算した形 \\[\ny=x+F(x)\n\\] で各層をデザインし，現状の入力からの差分 \\(F\\) のみを学習するとすることで勾配消失を回避する，というものであった．実際，この層の微分は \\[\n\\frac{d y}{d x}=1+F'(x)\n\\] と表せる．\nこのテクニックは トランスフォーマー (Vaswani et al., 2017) などのモデルでも用いられている．\n(Li et al., 2018) によると，残差レイヤーの追加は誤差関数を滑らかにし，近しい入力に対しても勾配が非常に大きくなってしまうことが効率的な学習を阻害してしまう問題 (shattered gradient problem) (Balduzzi et al., 2017) を解決している，という．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#アーキテクチャ",
    "href": "posts/2024/Kernels/Deep.html#アーキテクチャ",
    "title": "数学者のための深層学習概観",
    "section": "2 アーキテクチャ",
    "text": "2 アーキテクチャ\n\n2.1 導入\nニューラルネットワークは，内部に循環を持つかどうかで二分され，それぞれを Feedforward Network (FFN) と Feedback Network (FBN) と呼ぶ．11\nFFN は 多層パーセプトロン (MLP) ともいう．12\nRecurrent Neural Network (RNN) 2.4 や Hopfield ネットワーク 2.7.1 は FBN の例である．\n多層パーセプトロンは分類や回帰などの統計学的な用途に主に用いられるが，FBN は学習機械として以外に，生物の神経機能のモデルとしてや，組合せ最適化ソルバーとしても用いられる．13\n\n\n\n2.2 CNN：畳み込みニューラルネットワーク\nCNN はその畳み込み層に特徴付けられる画像に特化した FNN アーキテクチャである．\n第 4 節で詳しく扱う．\n\n\n2.3 AE：自己符号化器\n\n\n\nExample of Autoencoder from (Murphy, 2023, p. 635)\n\n\n自己符号化器または自己連想ニューラルネットワーク (auto-associative neural network) 1.2.3 は図のような砂時計型の，入力 \\(x\\) と出力 \\(y\\) の次元数が一致したアーキテクチャを持ち， \\[\n\\mathcal{L}(\\theta)=\\|y-x\\|^2_2\n\\] などの入力の復元誤差を目的関数として訓練される．\n\n\n2.4 RNN：再帰的ニューラルネットワーク\n再帰的な層を持ち，自己回帰モデル 3 を実装する FBN の例である．\n\n\n\nExample of Recurrent Neural Network from (Murphy, 2023, p. 636)\n\n\n第 3.2 節でも詳しく扱う．\n\n\n2.5 Transformer\nRNN の最大の難点は，隠れ次元 \\(z_t\\) が \\(x_{1:t}\\) までの入力の要約になっており，\\(x_1\\) などの最初の方の情報がどんどん薄れていく点にある．\nそこで，時点 \\(t\\) でも \\(x_1\\) など以前の情報にワンステップでアクセスできるような機構である 注意機構 が考案された．これがエンコーダーのみのトランスフォーマーである．\nデコーダーのみのトランスフォーマーは masked attenstion という技術を用いて \\(x_{1:t-1}\\) のみで条件づけて \\(x_t\\) を生成することができる．\n一方で，入力全体で条件づけて，文章を生成することもでき，これが最初に (Vaswani et al., 2017) によって提案された，最も一般的な形の encoder-decoder トランスフォーマーである．\n\n    \n        \n            \n            \n                トランスフォーマー\n                深層生成モデル１\n            \n        \n    \n\n\n\n2.6 GNN：グラフニューラルネットワーク\n従来の NN は辺の存在しない退化したグラフに対するものだとして，NN を一般のグラフデータに対して拡張することが近年考えられている．\n\n    \n        \n            \n            \n                グラフニューラルネットワーク\n                \n            \n        \n    \n\n\n\n2.7 非有向ネットワーク\n無向グラフが定めるニューラルネットワークは，エネルギーベースモデル (EBM: Energy-Based Model) で主に用いられる．\n\n2.7.1 Hopfield ネットワークとスピングラス\n計算機の記憶と生物の記憶の相違点のうち，大きなものには連想性 (associativity) がある．アドレスで整理されているのではなく，内容で整理されているのである．1970 年代には，神経の連想記憶機能のモデルとしてのニューラルネットワークが多数提案された．\nそれには，ここまで議論してきた階層型のネットワークと異なり，ノード同士は 相互結合 しているものも含まれる．連想記憶のモデルとしては，特に相互結合で，どちらの方向に関する重みも同じであるもの（対称結合 ネットワーク）が多く，全結合の FBN である Hopfield network はその代表例である．14\n（連続変数の）Hopfield network の学習則は，Hebb 則 (Hebb, 1949) に基づいて，各ニューロン \\(x_j\\in[0,1]\\) についてその入力 \\[\na_i:=\\sum_{j}w_{ij}x_j\n\\] を計算し，\\(x_i\\gets\\tanh(a_i)\\) と更新する．\n実はこの学習則は必ず収束する．このことを，(Hopfield, 1982) はスピングラスのモデルと関連付けて示したことから，特に統計物理学の文脈で Hopfield network の名前がついた．15\n\n神経回路網の解析，とくに連想記憶モデルの解析が，スピングラスを解析する方法を用いて実行できるのではないかという考えが出てきて，大量の物理学者が神経回路網に注目しだした．こうしたアイデアの火付け役がホップフィールドと言われる．(甘利俊一, 1989, p. 105)\n\n対称結合のニューラルネットワークの学習則は，スピングラスと同様に，必ずポテンシャル関数が減少する方に動作するというのである．この連関を利用して，(Hopfield and Tank, 1985) は Hopfield ネットワークをアナログ回路に実装し，巡回セールスマン問題を解くという，最適化問題ソルバーとして利用してみせた．\n単体 Hopfield Network (Burns and Fukai, 2023) という拡張もあり，パラメータ数は変わらずとも記憶容量が増える．\n\n\n2.7.2 ボルツマンマシン (Ackley et al., 1985)\nボルツマンマシンは確率的 Hopfield ネットワークともいい，Hopfield ネットワークが統計物理のモデルとして近似するところの Gibbs 分布を，実際に持つ Markov 確率場 の一種である．16\nこれは，荷重を，確率 \\(\\frac{1}{1+e^{-2a_i}}\\) で \\(x_i=1\\)，そうでない場合は \\(x_i=-1\\) と定めることで得られる．\n\n\n2.7.3 深層ボルツマンマシン (Salakhutdinov and Hinton, 2009)\n\n\n2.7.4 深層信念ネットワーク (Hinton et al., 2006)\nこれは深層学習研究の皮切りになった確率的深層モデルである．\n\n\n\n2.8 Spiking Neural Network\n実際の神経細胞は 発火 という離散的なイベントを発生させ，その頻度やタイミングも大きな役割を持っている．これを取り入れたモデルを Spiking Neural Network (SNN) (Maass, 1997) と呼ぶ (岡島義憲, 2020)．\nSNN は現状の人工ニューラルネット (ANN: Artificial Neural Network) よりも，半導体上での計算を効率化することが出来る．そこで，SNN による深層学習が近年試みられている (Tavanaei et al., 2019)．\nMicrosoft の BitNet (Wang et al., 2023) も計算効率性を目指すにあたってそのアイデアを等しくする．\n\n\n2.9 ベイズからの見方\n\n2.9.1 ネットワークの正則化と汎化性能\n良い汎化性能を得るためには，偏倚と分散のトレードオフ を乗り越える必要がある．\nデータセットに対してモデルの自由度が高すぎると，分散は小さくなれど，過学習を起こしてしまう．すなわち，大きなバイアスが導入され，汎化性能が悪くなる．一方で，モデルの自由度が低すぎると，平均的には正しい予測ができても，分散が大きくて役に立たない．\n大規模なデータセットを用いることは一つの解決である．自由度の高いモデルを用いても過学習が起こりにくくなるためである．17\n\n\n2.9.2 帰納バイアス\n正則化とは，モデルの自由度を制限することで過学習を抑制することである．これは，「正しいモデルは十分に滑らかであるはずである」という帰納バイアスを導入することで，モデルの汎化性能を改善させていることに等しい．18\n転移学習も一種の帰納バイアスの注入だと見れる．２つの異なるタスクの間に類似性が存在するという事前知識を注入することで，汎化性能を改善する手法だと思えるのである．\n汎化性能の高いモデルを作るということは，人類が解きたいタスクに普遍的に共通する特徴を捉え，これを帰納バイアスの形でモデルに注入することに等しい．No free lunch theorem (Wolpert, 1996) から，19 一般に特定のタスクに対する性能向上は，他のタスクに対する性能低下を伴うものであることが予想されるが，例えば推定関数が十分滑らかであるというのは，人間の認識特性上，有意義な結果にはほとんど普遍的に必要な条件である．\n\n\n2.9.3 ベイズ深層学習の美点\n(Gal and Ghahramani, 2016) などでは，\n\nデータの数が少なくとも，有効なパラメータ推定が可能である．\n過学習が起こりにくい．\n不確実性の定量化が自動でなされる．\n\nと説明される．\n\n\n2.9.4 ベイズ深層学習の例\nCNN は特に過学習しやすい上にサンプル効率性が悪い．その場合には，変分近似による Bayesian CNN は既存法と同等の性能を誇る (Gal and Ghahramani, 2016)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#sec-AR",
    "href": "posts/2024/Kernels/Deep.html#sec-AR",
    "title": "数学者のための深層学習概観",
    "section": "3 自己回帰モデル",
    "text": "3 自己回帰モデル\n\n3.1 導入\n長さ \\(T\\) のデータベクトルをモデリングするとき， \\[\np(x_{1:T})=\\prod_{t=1}^Tp(x_t|x_{1:t-1})\n\\] という分解を用いることができる．このようなアプローチを機械学習では 自己回帰モデル という．20\n\n\n3.2 RNN\nしかし，このようなモデリング法は \\(T\\) が大きくなるにつれて，条件づける変数 \\(x_{1:t-1}\\) が高次元になるため，モデリングが難しくなる．\nかと言って，\\(\\{x_t\\}_{t=1}^T\\) を Markov 連鎖とみなしてモデリングするわけにもいかない．\n一つの解決法が 状態空間モデル / 隠れ Markov モデル によるものである．\\(x_{1:t}\\) はある状態変数 \\(z_t\\) にある既知の確率法則で圧縮できると仮定し，これのみを持ち越す方法である．\n特に，\\(X_{1:t-1}\\to Z_t\\) の対応が決定論的であるとき，これを多層パーセプトロンによってモデリングしたものを 再帰ニューラルネットワーク という．\n\\(x_t=f(z_{t-1},x_{t-1})\\) の関係を学習した後は，\\(x_1\\) から再帰的に \\(f\\) に通すことでデータ \\(x_{1:T}\\) を生成する．\n\n\n3.3 機械学習の中で占める役割\nAR モデルは尤度を効率的に扱えるため，計算や最適化が簡単である．実際，RNN や CNN などの自己回帰モデルが，歴史上最初に発達したアーキテクチャである．\n一方で，データの生成が逐次的であるために生成が遅いことや，内部で潜在表現を学習するということがないために表現学習に使うことができない点が欠点と言える．21"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#sec-CNN",
    "href": "posts/2024/Kernels/Deep.html#sec-CNN",
    "title": "数学者のための深層学習概観",
    "section": "4 CNN",
    "text": "4 CNN\nCNN はトランスフォーマーによりデータ間の関係を自動的に学習する枠組みが提案される前に，主に画像分野において，データの構造に関する事前知識をモデルに組み込んだ例として提案されたものである．\n世界初の深層学習モデルによる席巻は，CNN により，画像認識の分野において達成された．\n\n4.1 Computer Vision という分野\nComputer Vision という問題の複雑性が，ニューラルネットワークのアーキテクチャの開発を後押しした歴史がある．\n並行移動・拡大変換という2つの合同変換不変性に対して，画像の認識結果は不変であるべきである．\nこのような不変性，または 同変性 (equivariance) をモデルに取り入れる方法は大きく分けて４つある：\n\n誤差関数に正則化項を導入する22\n対称性を取り入れた潜在表現 を用いてその上で学習をする\n不変性を効率良く学習出来るように データセットを拡張する\n対称性を取り扱う構造をネットワークのアーキテクチャに組み込む\n\nCNN は４番目のアプローチで歴史上最初に取られたものである．しかし，このどのアプローチも完全には不変性を取り入れることは出来ていないことも報告されている (Azulay and Weiss, 2019)．\n幾何学を種々の変換に対する不変性の研究と捉え直した Felix Klein の Erlangen program に倣い，種々の深層モデルの帰納バイアスとアーキテクチャを，幾何学的な変換から導出してシステマティックに理解する試み Geometric Deep Learning (Bronstein et al., 2021) がある（第 4.6 節）．\n\n\n4.2 物体認識 CNN\nCNN は画像の特徴を，階層的に学習出来るように誘導するような構造を持っている．\n多くの例では，畳み込み層とプーリング層が交互に繰り返され，最後に全結合層を持つような構造を持っている．\n\n4.2.1 局所的な特徴\n最初の素子は，画像の局所的な一部のみを入力として取る．その範囲を 受容野 (receptive field) と呼ぶ．この素子の荷重を フィルター または カーネル という．\n\n\n\n\n\n\n例：2次元の畳み込み層\n\n\n\n\n\n2次元での幅 \\(2k+1\\) の畳み込み層は，フィルター \\(\\mathrm{supp}\\;(\\psi)\\subset\\{0,\\pm1,\\cdots,\\pm k\\}\\) を用いて， \\[\nf^{\\text{out}}_{i,j}=\\phi\\left(\\sum_{a,b\\in\\mathbb{Z}}\\psi(i-a,j-b)f_{i,j}^{\\text{in}}+\\theta\\right)\n\\] と表せる．\n\n\n\n決まったカーネルに対して，この素子はカーネルの特徴にマッチした入力に対して，大きな出力を返す．\n次に，このフィルターを畳み込むことで，画像内の異なる位置に存在する特徴を検出する．畳み込み層は，荷重を共有した疎結合層ということになる．\n畳み込みを行うと，入力次元と出力次元が変わってしまうことがあるため，その場合は入力画像にパディングを施す．\n出力次元を小さくして，畳み込み特徴写像で大きな次元削減を行いたい場合，strided convolution を用いる．\n\n\n4.2.2 並行移動不変性\n畳み込みの結果が，特徴の位置の変化に対して不変になるようにする設計に，プーリング層 または ダウンサンプリング層 (down-sampling / sub-sampling) がある．\nプーリングも，受容野を持った素子と畳み込みからなるが，畳み込みに学習されるべきパラメータはなく，確定的な関数と畳み込まれる．\n代表的なプーリング関数には 最大プーリング (Y. Zhou and Chellappa, 1988) や平均プーリング，\\(l^2\\)-プーリングなどがある．\nプーリングは不変性の導入に加えて，畳み込み特徴のダウンサンプリングを行って，更なる次元削減を行う役割も果たす．\n\n\n\n4.3 画像分割 CNN\n画像分類では，１枚の画像に対して１つのクラスの対応づけたが，１つのピクセルに１つのクラスを対応づけることで，画像をクラスごとに分類することが考えられる．\n\n4.3.1 Up-sampling\n画像分類の問題では，最終的に全ピクセルから得た情報を１次元に圧縮することになる．一方で，十分な潜在表現を得たのちは up-sampling に転じる Encoder-Decoder 構造にすることで，最終的に元の画像サイズに戻しながら，画像分割問題を解くことが出来る (Long et al., 2015), (Noh et al., 2015), (Badrinarayanan et al., 2017)．\n(Badrinarayanan et al., 2017) は max-unpooling などの up-sampling 層の設計を考慮したが，これに学習可能なパラメータを増やした transpose convolution / deconvolution も提案された．\nPooling 層を一切用いず，down-sampling も up-sampling も畳み込み層のみによって行われる場合，これを 全畳み込みネットワーク (fully convolutional network) という (Long et al., 2015)．\n\n\n4.3.2 U-Net\nこの encoder-decoder 構造は，分類に必要のない情報を自動的に削減し，モデルのサイズを小さくするのには効果的であるが，タスクによっては元の画像の情報量を保ちたい場合がある．\nU-net (Ronneberger et al., 2015) は対応する down-sampling 層と up-sampling 層とを直接繋ぐ経路を追加することでこれを解決した．\n\n\n4.3.3 Capsule Networks\nプーリング層は並行移動不変性の概念を取り入れるが，回転や拡大などの変換に対する不変性を取り入れるにはデータ拡張に依るしかないのでは，CNN の学習はどうしても大規模なデータセットが必要になってしまう．\nそこで，アーキテクチャによる解決を試みたのが，畳み込み層に加えて カプセル層 を取り入れた Capsule Network (Sabour et al., 2017) である．\n\n\n\n4.4 Inpainting\n(Horita et al., 2023)\n\n\n4.5 スタイル転移\nCNN において，最初の方のレイヤーは局所的な特徴を捉えているが，後の方のレイヤーはスタイルなどの大域的な特徴を捉えている．\nこれを用いて，既存の画像の具体的な特徴を変えずに，他の画像からスタイルのみを転移する手法 Neural Style Transfer が提案された (Gatys et al., 2015), (Gatys et al., 2016)．\n\n\n4.6 幾何学的深層学習\n以上，種々のタスクに種々のアーキテクチャが存在することを見てきた．この状況は，19 世紀の幾何学と似ていると (Bronstein et al., 2021) はいう．\nこれらのアーキテクチャがどのような帰納バイアスを導入する役割を果たしているかを，不変性や同変性といった第一原理から理解する試みが 幾何学的深層学習 である．\n\nIn this text, we make a modest attempt to apply the Erlangen Programme mindset to the domain of deep learning, with the ultimate goal of obtaining a systematisation of this field and ‘connecting the dots’. We call this geometrisation attempt ‘Geometric Deep Learning’, and true to the spirit of Felix Klein, propose to derive different inductive biases and network architectures implementing them from first principles of symmetry and invariance. (Bronstein et al., 2021, p. 2)\n\n化学・生物・物理はいずれも対象の対称性をしっかり扱う理論を持っている．これらの分野に深層学習が広く取り入れられつつある今，深層学習の分野も対称性を第一原理として整理される脱皮が待たれているのである．\n\n4.6.1 幾何学と解析学は双対である\nMikhael Gromov によると，空間 \\(X\\) の解析学とは \\(X\\) 上の関数の研究で，\\(X\\) の幾何学とは \\(X\\) への関数の研究である (深谷賢治, 1997, p. 11)．\n\n\n\nGromov による幾何と解析の解釈\n\n\n同変性と共変性は，データ集合 \\(X\\) と群作用 \\(\\rho:G\\times X\\to X\\) との組 \\((X,\\rho)\\) を取り扱うから，確かに上述の定義に適っている．\n\n\n4.6.2 20 世紀の数学の方向\n(Gromov, 2001)\n\n\n4.6.3 共変性と同変性\n\n\n\n\n\n\n定義 (invariance, equivariance)23\n\n\n\n\\(X,Y\\) を集合，\\(G\\) を群で \\(X,Y\\) に左から作用するとする．24 関数 \\(\\varphi:X\\to Y\\) が\n\n\\(G\\) に関して 不変 であるとは，任意の \\(g\\in G\\) と \\(x\\in X\\) について \\[\n\\varphi(g\\cdot x)=\\varphi(x)\n\\] を満たすことをいう．\n\\(G\\) に関して 同変 であるとは，任意の \\(g\\in G\\) と \\(x\\in X\\) について \\[\n\\varphi(g\\cdot x)=g\\cdot\\varphi(x)\n\\] を満たすことをいう．\n\n\n\n物体認識 Section 4.2 は不変的で，画像分割 Section 4.3 は同変的な問題である．\nただし，不変性は，\\(G\\) の \\(Y\\) への作用が自明である場合の同変性と見れるため，同変性の方が一般的な概念であることに注意．\n\n\n4.6.4 G-CNN (T. Cohen and Welling, 2016)\n一般の群変換に対して，これを帰納バイアスとして取り入れる CNN である Group CNN (T. Cohen and Welling, 2016) が提案されており，医療画像解析での応用 (Lafarge et al., 2021) もなされている．\n\n\n4.6.5 Steerable CNN (T. S. Cohen and Welling, 2017)\nSteerable CNN ではチャンネルの間での対称性を取り入れることができる．\n(Weiler et al., 2018) により最先端の性能が発揮されている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#終わりに",
    "href": "posts/2024/Kernels/Deep.html#終わりに",
    "title": "数学者のための深層学習概観",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nもはやニューラルネットワークは，layered differentiable model (Oord et al., 2019) と呼んだ方がその数学的な存在をよく表すだろうと思う．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep.html#footnotes",
    "href": "posts/2024/Kernels/Deep.html#footnotes",
    "title": "数学者のための深層学習概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(MacKay, 2003, p. 506) など．↩︎\n入力層と中間層と出力層の３層のみからなるものモデルを単純パーセプトロンと呼ぶが，それだけでなく，多層のものや，フィードバック結合のあるものも含む，一般的な形で考えていた．↩︎\nオートエンコーダーを導入し，中間層で表現学習がなされること（最も幅の狭い中間層の活性化を通じてコンパクトに表現する，など）と，モーメンタムが学習を加速することを示している (Rumelhart et al., 1987, p. 330), (Schmidhuber, 2015)．「バックプロパゲーションがこの流行の起爆剤となったとさえ言える」 (甘利俊一, 1989, p. 144)．↩︎\n(Schmidhuber, 2015), (Bishop and Bishop, 2024), (麻生英樹 et al., 2015) を参考．↩︎\n(Schmidhuber, 2015, p. 90) など．↩︎\n教師なしの競合学習を試みたのみであった (麻生英樹 et al., 2015, p. 21)．↩︎\n三層の場合は，入力層を符号化器，出力層を復号化器とも言う．さらに中間層の幅が小さい場合，その形から hourgalss-type neural network とも呼ばれる (麻生英樹 et al., 2015, p. 91)．↩︎\nHinton のこの２つの論文は，深層信念ネットワーク 2.7.4 において実証されていた．↩︎\nさらに，Hinton らが示した成功は，深層信念ネットワークは制約付き Boltzmann マシン (RBM) として各層が事前訓練されたが，これが denoising autoencoder と似たノイズへのロバスト性を示すために起こったものではないかと予想している (Vincent et al., 2008) 第６節．↩︎\n当時の CNN で深かったものには，19 層の CNN である VGGNet (Simonyan and Zisserman, 2015) がある．↩︎\n(MacKay, 2003, p. 505)，(Murphy, 2023, p. 633) 16.3.1節など．↩︎\n(Murphy, 2023, p. 632) 16.3.1節など．↩︎\n(MacKay, 2003, p. 468) の導入も参照．↩︎\n(MacKay, 2003, p. 503) 第42章，(麻生英樹 et al., 2015, p. 11) など．↩︎\n「このため，物理学者はこの種のモデルのことを Hopfield モデルと呼ぶが，この命名は適切とは思えない」(甘利俊一, 1989, p. 97) としている．↩︎\n“The popularity of the Boltzmann machine was primarily driven by its similarity to an activation model for neurons.” (Koller and Friedman, 2009, p. 126)．↩︎\n(Bishop and Bishop, 2024, p. 254) など．↩︎\n(Bishop and Bishop, 2024, p. 255) など．↩︎\n(Wolpert, 1996) は最適化の文脈での定理を示した．統計的機械学習の文脈での No free lunch theorem は，“Any classifier with finite sample error guarantees necessarily needs inductive bias: structural assumptions on either the function class or the sampling distribution.” と説明できる．(Shalev-Shwartz and Ben-David, 2014, p. 37) 定理5.1 も参照．↩︎\n(Murphy, 2023, p. 811) 22章など．↩︎\n(Murphy, 2023, p. 812) など．↩︎\ntangent propagation (Simard et al., 1991) などがその例である．2の例とも見れる．↩︎\n(福水健次, 2024), (Bronstein et al., 2021, pp. 15–16) など．または nLab も参照．↩︎\nこのような集合 \\(X,Y\\) を \\(G\\)-集合 という．同変性 は \\(G\\)-集合の射と見れる．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html",
    "href": "posts/2024/Kernels/Deep3.html",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#導入",
    "href": "posts/2024/Kernels/Deep3.html#導入",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "1.1 導入",
    "text": "1.1 導入\nGAN 以前の深層生成モデルは，学習の難しさから，データ生成分布にパラメトリックな仮定をおき，その中で 最尤推定 を行うことが一般的であった．深層 Boltzmann マシン (Salakhutdinov and Hinton, 2009) もその例である．\n複雑なモデルで尤度を解析的に計算することは困難である．そのために，MCMC によるサンプリングによりこれを回避することを考え，その Markov 連鎖の遷移核を学習するという生成確率的ネットワーク (GSN: Generative Stochastic Network) などのアプローチ (Bengio et al., 2014) も提案されていた．\nGAN (Generative Adversarial Network) は，このような中で (Goodfellow et al., 2014) によって提案された深層生成モデルである．GAN も尤度の評価を必要としないが，MCMC などのサンプリング手法も用いず，ただ誤差逆伝播法のみによって学習が可能である．\n同時の深層学習は，ImageNet コンペティションにおいて大成功を収めた AlexNet (Krizhevsky et al., 2012) など，主に識別のタスクにおいて大きな成功を収めていたが，生成モデルにおいては芳しくなかった．\n主な障壁は\n\n分布の近似が難しいこと\n区分的線型な活性化関数を用いても勾配を通じた学習が難しいこと\n\nの２点であったが，GAN はこの２つの問題を回避すべく提案された．\n生成モデル \\(G\\) に対して，判別モデル \\(D\\) を対置し，加えて \\((G,D)\\) をセットで誤差逆伝播法とドロップアウト法 (Hinton et al., 2012)（当時深層識別モデルを最も成功させていた学習法）により学習可能にしたのである．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#枠組み",
    "href": "posts/2024/Kernels/Deep3.html#枠組み",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "1.2 枠組み",
    "text": "1.2 枠組み\nデータの空間を \\(x\\in\\mathcal{X}\\) とし，潜在変数の値域 \\(\\mathcal{Z}\\) とその上の確率測度 \\(P_z\\in\\mathcal{P}(\\mathcal{Z})\\)，そして深層ニューラルネットワークのパラメータ空間 \\(\\Theta_g\\) を用意して，生成モデルを写像 \\(G:\\mathcal{Z}\\times\\Theta_g\\to\\mathcal{X}\\) とする．\n生成モデル \\(G\\) は押し出しによりモデル \\(\\{G(-,\\theta_g)_*P_z\\}_{\\theta_g\\in\\Theta_g}\\) を定める．\nこのモデルの密度（尤度）の評価を回避するために，これに判別モデル \\(D\\) を対置する．これは，パラメータ \\(\\theta_d\\in\\Theta_d\\) を通じて学習される写像 \\(D:\\mathcal{X}\\times\\Theta_d\\to[0,1]\\) とし，あるデータ \\(x\\in\\mathcal{X}\\) を観測した際に，これが \\(G\\) から生成されたものではなく，実際の訓練データである確率を \\(D(x)\\) によって近似することを目指す．\nこの組 \\((G,D)\\) に対して， \\[\nV(D,G):=\\operatorname{E}[\\log D(X)]+\\operatorname{E}[\\log(1-D(G(Z))]\n\\] \\[\nX\\sim P_{\\text{data}},\\quad Z\\sim P_z\n\\] を目的関数とし， \\[\n\\min_{G\\in\\mathrm{Hom}_\\mathrm{Mark}(\\mathcal{Z}\\times\\mathcal{G}_g,\\mathcal{X})}\\max_{D\\in\\mathcal{L}(\\mathcal{X};[0,1])}V(D,G)\n\\tag{1}\\] を解く，ミニマックスゲームを考える．1"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#理論",
    "href": "posts/2024/Kernels/Deep3.html#理論",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "1.3 理論",
    "text": "1.3 理論\n\\(G\\) と \\(D\\) が表現するモデルが十分に大きいとき，すなわち \\(\\Theta_g,\\Theta_d\\) が十分に大きく，殆どノンパラメトリックモデルであるとみなせる場合には，学習基準 式 1 は真の生成分布 \\(P_{\\text{data}}\\) に収束するアルゴリズムを与える．\nこのことを示すには，\\(P_{\\text{data}}\\) が，式 1 の大域的最適解であることを示せば良い．\n\n\n\n\n\n\n定義 (Jensen-Shannon divergence)\n\n\n\n確率測度 \\(P,Q\\in\\mathcal{P}(\\mathcal{X})\\) に対して，\n\n\\[\n\\operatorname{KL}(P,Q):=\\begin{cases}\n\\int_\\mathcal{X}\\log\\left(\\frac{d P}{d Q}\\right)\\,dP&P\\ll Q,\\\\\n\\infty&\\mathrm{otherwise}.\n\\end{cases}\n\\] を Kullback-Leibler 乖離度 という．\n\\[\n\\operatorname{JS}(P,Q):=\\operatorname{KL}\\left(P,\\frac{P+Q}{2}\\right)+\\operatorname{KL}\\left(Q,\\frac{P+Q}{2}\\right)\n\\] を Jensen-Shannon 乖離度 という．\n\nこのとき，\\(\\sqrt{\\operatorname{JS}}\\) は，任意の \\(\\sigma\\)-有限測度 \\(\\mu\\in\\mathcal{M}(\\mathcal{X})\\) に関して， \\[\n\\mathcal{P}_\\mu(\\mathcal{X}):=\\left\\{P\\in\\mathcal{P}(\\mathcal{X})\\mid P\\ll\\mu\\right\\}\n\\] 上に距離を定める．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nKL 乖離度は \\(P\\ne Q\\Rightarrow\\operatorname{KL}(P,Q)&gt;0\\) を満たすが，対称性も三角不等式も満たさない．そもそも，\\(\\mathbb{R}_+\\)-値とは限らず，\\(\\infty\\) を取り得る．\nJS 乖離度は， \\[\nP\\ll\\frac{P+Q}{2}\n\\] であるから，\\(\\mathcal{P}(\\mathcal{X})^2\\) 上で常に \\(\\mathbb{R}_+\\)-値であることに注意．\n以降，\\(\\sqrt{\\operatorname{JS}}\\) が距離であることを示す．\n\n\\(P=Q\\) のとき \\(\\operatorname{JS}(P,Q)=0\\) であり，\\(P\\ne Q\\) のとき， \\[\nP\\ne\\frac{P+Q}{2}\n\\] であるから，\\(\\operatorname{JS}(P,Q)&gt;0\\) である．\n対称性も直ちに従う．\nあとは三角不等式を示せば良いが，任意の \\(P,Q\\in\\mathcal{P}_\\mu(\\mathcal{X})\\) に関して，密度を \\[\np:=\\frac{d P}{d \\mu},\\quad q:=\\frac{d Q}{d \\mu}\n\\] と表すと， \\[\n\\sqrt{\\operatorname{JS}(P,Q)}=\\left\\|\\sqrt{L(p,q)}\\right\\|_{L^2(\\mu)}\n\\] であることより，次の補題と \\(\\|-\\|_{L^2(\\mu)}\\) の三角不等式より従う．\n\n\n\n\n\n\n\n\n\n\n補題 (Endres and Schindelin, 2003, p. 1859)\n\n\n\n非負実数 \\(p,q\\in\\mathbb{R}_+\\) について， \\[\nL(p,q):=p\\log\\frac{2p}{p+q}+q\\log\\frac{2q}{p+q}\n\\] で定まる関数 \\(L:\\mathbb{R}_+^2\\to\\mathbb{R}_+\\) は，任意の \\(r\\in\\mathbb{R}_+\\) について， \\[\n\\sqrt{L(p,q)}\\le\\sqrt{L(p,r)}+\\sqrt{L(r,q)}\n\\] を満たす．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n右辺を \\[\nf(p,q,r):=\\sqrt{L(p,r)}+\\sqrt{L(r,q)}\n\\] とおいて，\\(r\\) に関する偏導関数の符号変化を調べる． \\[\n\\begin{align*}\n    \\frac{\\partial f}{\\partial r}&=\\frac{1}{2\\sqrt{L(p,r)}}\\frac{\\partial L}{\\partial r}(p,r)+\\frac{1}{2\\sqrt{L(r,q)}}\\frac{\\partial L}{\\partial r}(r,q)\\\\\n    &=\\frac{\\log\\frac{2r}{p+r}}{2\\sqrt{L(p,r)}}+\\frac{\\log\\frac{2r}{r+q}}{2\\sqrt{L(r,q)}}\\\\\n    &=\\frac{1}{\\sqrt{r}}\\left(\\frac{\\log\\frac{2}{x+1}}{2\\sqrt{L(x,1)}}+\\frac{\\log\\frac{2}{\\beta x+1}}{2\\sqrt{L(\\beta x,1)}}\\right).\n\\end{align*}\n\\tag{2}\\] ここで，\\(x:=\\frac{p}{r},\\beta x=\\frac{q}{r}\\) とおいた． \\[\np&lt;q\\quad\\Leftrightarrow\\quad\\beta&gt;1\n\\] と仮定しても一般性は失われない．\nそこで，\\(x\\in(-1,\\infty)\\setminus\\{1\\}\\) の関数 \\[\n\\begin{align*}\n    g(x)&:=\\frac{\\log\\frac{2}{x+1}}{\\sqrt{L(x,1)}}\\\\\n    &=\\frac{\\log\\frac{2}{x+1}}{\\sqrt{x\\log\\frac{2x}{x+1}+\\log\\frac{2}{x+1}}}\n\\end{align*}\n\\] の性質を調べる．\n実は \\(g'&gt;0\\;\\mathrm{on}\\;\\mathbb{R}_+\\setminus\\{1\\}\\) であり， \\[\n\\lim_{x\\to0+}g(x)=\\sqrt{\\log 2}&gt;0\n\\] \\[\n\\lim_{x\\to\\infty}g(x)=0\n\\] \\[\n\\lim_{x\\to1\\mp}g(x)=\\pm1\n\\] と併せると，\\(g((0,1))\\subset(0,1)\\)，\\(g((1,\\infty))\\subset(-1,0)\\) である．特に \\(\\lvert g\\rvert&lt;1\\)．\n\n\n\n\n\n\n\n\n\nこれより， 式 2 は \\(x=1,\\beta\\) と，その間で１回の計３回符号変化し，\\(x\\to\\infty\\) の極限では負である．\nよって，\\(f\\) は \\(r\\) の関数として，\\(r=p\\) で極小値，\\(r\\in(p,q)\\) のどこかで極大値を取り，\\(r=q\\) で再び極小値を取る． \\[\nf(p,q,p)=f(p,q,q)=\\sqrt{L(p,q)}\n\\] であるから，結論を得る．\n\n\n\n\n\n\n\n\n\n命題\n\n\n\n\\(P_0,P_1\\in\\mathcal{P}(\\mathcal{X})\\) を確率測度で，それぞれ密度 \\(p_0,p_1\\) を持つとする．\\(X_0\\sim P_0,X_1\\sim P_1\\) とする．このとき，\n\n最大化問題 \\[\nL:=\\sup_{D\\in\\mathcal{L}(\\mathcal{X};[0,1])}\\biggr(\\operatorname{E}[\\log D(X_0)]+\\operatorname{E}[\\log(1-D(X_1))]\\biggl)\n\\] はただ一つの解 \\[\nD^*(x)=\\frac{p_0(x)}{p_0(x)+p_1(x)}\n\\] を持つ．\n\\(\\operatorname{JS}(P_0,P_1)\\) は \\(L\\) と定数の差を除いて一致する．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n目的関数は \\[\n\\begin{align*}\n&\\operatorname{E}[\\log D(X_0)]+\\operatorname{E}[\\log(1-D(X_1))]\\\\\n=&\\int_\\mathcal{X}\\log D\\cdot p_0\\,d\\mu+\\int_\\mathcal{X}\\log(1-D)\\cdot p_1\\,d\\mu\\\\\n=&\\int_\\mathcal{X}\\biggr(p_0\\log D+p_1\\log(1-D)\\biggl)\\,d\\mu\n\\end{align*}\n\\] と変形できる．いま，任意の \\(a,b\\in(0,1]\\) に関して， \\[\nf(t):=a\\log t+b\\log(1-t)\\quad(t\\in(0,1))\n\\] は \\(t=\\frac{a}{a+b}\\) 上で最大値を取る．\\(a,b\\) のどちらか一方のみが \\(0\\) である場合も含めてこの主張は成り立つ．よって， \\[\nD(x)=\\frac{p_0(x)}{p_0(x)+p_1(x)}\n\\] が目的関数を最大化することが判る．\n１より，\\(L\\) の上限 \\(\\sup\\) は達成されることがわかった： \\[\n\\begin{align*}\nL&=\\operatorname{E}[\\log D^*(X_0)]+\\operatorname{E}[\\log(1-D^*(X_1))]\\\\\n&=\\int_\\mathcal{X}\\left(p_0\\log\\frac{p_0}{p_0+p_1}+p_1\\log\\frac{p_1}{p_0+p_1}\\right)\\,d\\mu\\\\\n&=\\int_\\mathcal{X}\\biggr(p_0\\log\\frac{2p_0}{p_0+p_1}+p_1\\log\\frac{2p_1}{p_0+p_1}-p_0\\log 2-p_1\\log 2\\biggl)\\,d\\mu\\\\\n&=-2\\log2+\\operatorname{JS}(P_0,P_1).\n\\end{align*}\n\\]\n\n\n\n\nこれより，訓練基準 式 1 はただ一つの大域的な最適解を持ち，これは \\(P_{\\text{data}}=G_*P_z\\) かつ \\(D^*=\\frac{1}{2}\\) のときに最小値 \\(-2\\log2\\) を取るということが判る．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#アルゴリズムとその収束",
    "href": "posts/2024/Kernels/Deep3.html#アルゴリズムとその収束",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "1.4 アルゴリズムとその収束",
    "text": "1.4 アルゴリズムとその収束\n組 \\((G,D)\\) を勾配降下法により同時に学習するには，\n\n判別器 \\(D\\) の最大化ステップ\n\nミニバッチ \\(\\{z^i\\}_{i=1}^m\\) と \\(\\{x^i\\}_{i=1}^m\\) をそれぞれ \\(P_z\\) と \\(P_{\\text{data}}\\) からサンプリングする．\n確率的勾配 \\[\nD_{\\theta_d}\\frac{1}{m}\\sum_{i=1}^m\\left(\\log D(x^i)+\\log(1-D(G(z^i)))\\right)\n\\] の増加方向にパラメータ \\(\\theta_d\\) を更新する．\n\n生成モデル \\(G\\) の最小化ステップ\n\nミニパッチ \\(\\{z^i\\}_{i=1}^m\\) を \\(P_z\\) からサンプリングする．\n確率的勾配 \\[\nD_{\\theta_g}\\sum_{i=1}^m\\log\\biggr(1-D(G(z^i))\\biggl)\n\\] の減少方向にパラメータ \\(\\theta_g\\) を更新する．\n\n\nというアルゴリズムを実行すれば良い．(Goodfellow et al., 2014 p.) の数値実験ではモーメンタム法 (Rumelhart et al., 1987, p. 330) が用いられている．\n\n\n\n\n\n\n命題 (Goodfellow et al., 2014, p. 5)\n\n\n\nこのアルゴリズムは，次の３条件が成り立つならば，\\(G_*P_z\\) は \\(P_{\\text{data}}\\) に収束する：\n\nモデル \\(G,D\\) の表現力が十分大きい．\n判別器 \\(D\\) の最大化ステップにおいて，必ず \\(\\max_{D\\in\\mathcal{L}(\\mathcal{X};[0,1])}V(D,G)\\) が達成される．\n生成モデル \\(G\\) の最大化ステップにおいても，必ず \\(V(D,G)\\) が改善される．\n\n\n\n実際は，\\(G\\) はパラメトリックモデル \\(\\{G_*P_z(\\theta,-)\\}_{\\theta\\in\\Theta_g}\\) であるから，その分の誤差は残ることになる．\nまた，\\(D\\) が最適化されていない状況で \\(G\\) が学習されすぎると，多くの \\(z\\in\\mathcal{Z}\\) の値を \\(D\\) が不得意な判別点 \\(x\\in\\mathcal{X}\\) に対応させすぎてしまうことがあり得る．\n\\(P_{\\text{data}}\\) が強い多峰性を持つ場合でも効率よく学習することができる．これは同じ確率分布からのサンプリング手法として，MCMC にはない美点になり得る (Goodfellow et al., 2014, p. 6)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#補遺jensen-shannon-乖離度のその他の性質",
    "href": "posts/2024/Kernels/Deep3.html#補遺jensen-shannon-乖離度のその他の性質",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "1.5 補遺：Jensen-Shannon 乖離度のその他の性質",
    "text": "1.5 補遺：Jensen-Shannon 乖離度のその他の性質\n\n1.5.1 情報理論からの導入\n乖離度としての Jensen-Shannon 乖離度は (Lin, 1991) で最初に導入されたようである．\nが，その以前から， \\[\n\\operatorname{JS}(P,Q)=2H\\left(\\frac{P+Q}{2}\\right)-H(P)-H(Q)\n\\] という関係を通じて，(Rao, 1982, p. 25) などは右辺を Jensen 差分 (difference) と呼んでいたようである．(Rao, 1987, p. 222) は，\\(H\\) が Shannon のエントロピーではなくとも，有用な性質を持つことを情報幾何学の立場から議論している．\n\n\n1.5.2 JS 乖離度が定める距離\n\\[\n\\biggr(\\operatorname{JS}(P,Q)\\biggl)^\\alpha\n\\] が \\(\\alpha=\\frac{1}{2}\\) において距離をなすことを示したが，実は一般の \\(\\alpha\\in(0,1/2]\\) に関して距離をなす (Osán et al., 2018)．\n\n\n1.5.3 変分問題としての特徴付け\n\n\n\n\n\n\n命題 (Nielsen, 2021, p. 6)\n\n\n\n任意の \\(P,Q\\in\\mathcal{P}_\\mu(\\mathcal{X})\\) について，\n\\[\n\\operatorname{JS}(P,Q)=\\min_{R\\in\\mathcal{P}_\\mu(\\mathcal{X})}\\left\\{\\operatorname{KL}(P,R)+\\operatorname{KL}(Q,R)\\right\\}\n\\]\n\n\n\n\n1.5.4 有界な距離である\n\n\n\n\n\n\n命題 (Endres and Schindelin, 2003, p. 1859)\n\n\n\n\\(\\operatorname{JS}:\\mathcal{P}_\\mu(\\mathcal{X})^2\\to\\mathbb{R}_+\\) は最大値 \\(\\sqrt{2\\log 2}\\) を持つ．\n\n\n\n\n1.5.5 \\(\\chi^2\\)-距離に漸近する (Endres and Schindelin, 2003, p. 1859)\n\n\n1.5.6 \\(f\\)-乖離度の例である\n\\(f\\)-乖離度の考え方は (Rényi, 1961, p. 561) で導入された．他，(Csiszár, 1963), (Morimoto, 1963), (Ali and Silvey, 1966) なども独立に導入している．\n\n\n\n\n\n\n定義 (\\(f\\)-divergence)\n\n\n\n\\(P\\ll Q\\) とする．凸関数 \\(f:\\mathbb{R}_+\\to\\mathbb{R}\\) に対して，\n\\[\nD_f(P,Q):=\\int_\\mathcal{X}f\\left(\\frac{d P}{d Q}\\right)\\,dQ\n\\] を \\(f\\)-乖離度 という．\n\n\nKL-乖離度は \\[\nf(x)=x\\log x\n\\] について，JS-乖離度は \\[\nf(x)=x\\log\\frac{2x}{x+1}+\\log\\frac{2}{x+1}\n\\] についての \\(f\\)-乖離度である．\n全変動ノルムも \\[\nf(x)=\\lvert x-1\\rvert\n\\] に関する \\(f\\)-乖離度である．\nさらには，\\(\\alpha\\)-乖離度 も \\(f\\)-乖離度の例である．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#f-gan",
    "href": "posts/2024/Kernels/Deep3.html#f-gan",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "2.1 \\(f\\)-GAN",
    "text": "2.1 \\(f\\)-GAN\nJS-乖離度に限らず一般の \\(f\\)-乖離度 Section 1.5.6 に関して，GAN が構成できる (Nowozin et al., 2016)．\nこの一般化により，GAN の枠組みの本質は凸解析に基づくものであることが明らかになる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#gan-の学習の問題点",
    "href": "posts/2024/Kernels/Deep3.html#gan-の学習の問題点",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "2.2 GAN の学習の問題点",
    "text": "2.2 GAN の学習の問題点\n\nやはり多峰性に弱く，モードのうちいくつかが再現されないことがある (Mode collapse)．\n収束判定が困難である．これは学習基準が最小化ではなく均衡点を求めることにあることにも起因する．\n勾配消失が起こる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#wasserstein-gan",
    "href": "posts/2024/Kernels/Deep3.html#wasserstein-gan",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "2.3 Wasserstein GAN",
    "text": "2.3 Wasserstein GAN\n最後の勾配消失の問題は，JS-乖離度の性質にあるとして，これを Wasserstein 距離に取り替える形で提案されたのが Wasserstein GAN である (Arjovsky et al., 2017)．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep3.html#footnotes",
    "href": "posts/2024/Kernels/Deep3.html#footnotes",
    "title": "GAN：敵対的生成ネットワーク",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの基準にしたがって学習すると，\\(G\\) が外れすぎている際，\\(\\log(1-D(G(z)))\\) が殆ど \\(0\\) になり得る．そのような場合は，\\(\\log D(G(z))\\) の最大化を代わりに考えることで，学習が進むことがある (Goodfellow et al., 2014, p. 3)．↩︎"
  },
  {
    "objectID": "posts/2024/Particles/PF.html",
    "href": "posts/2024/Particles/PF.html",
    "title": "A Recent Development of Particle Methods",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nThe following is a detailed version of the poster presented at MLSS2024, S3-41, March 8 (Fri) 18:00-19:30."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#what-is-particle-filter",
    "href": "posts/2024/Particles/PF.html#what-is-particle-filter",
    "title": "A Recent Development of Particle Methods",
    "section": "1 What Is Particle Filter?",
    "text": "1 What Is Particle Filter?\nParticle filters, also known as Sequential Monte Carlo methods (SMCs), were invented in (Kitagawa, 1993) and (Gordon et al., 1993) independently as an simulation-based algorithm which performs filtering in non-Gaussian and non-linear state space models, overcoming the weeknesses of then-standard Kalman-based filtering methods.1\nIn particle-based approaches, a filtering distribution is approximated by a cloud of weighted samples, hence giving rise to the term ‘particle filter’. The samples are propagated to approximate the next distribution, leading to efficient sequential estimation in dynamic settings.\nRecent developments have highlithgted the capability of particle filters as general-purpose samplers, extending their applicability beyond the traditional realm of temporal graphical models to a broader range of statistical inference problems. This versatility has earned them the alternative name ‘SMC’, a term reminiscent of ‘MCMC’. This poster trys to be another contribution in this direction."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#mcmc-vs.-smc",
    "href": "posts/2024/Particles/PF.html#mcmc-vs.-smc",
    "title": "A Recent Development of Particle Methods",
    "section": "2 MCMC vs. SMC",
    "text": "2 MCMC vs. SMC\nPDMPs (Piecewise Deterministic Markov Processes) (Davis, 1984), a type of continuous-time Markov processes with jumps as their only random components, play a complementary role to diffusion processes in stochastic modelling.2\nIn (Peters and de With, 2012), a PDMP was identified through the continuous limit of the MCMC, Metropolis-Hastings algorithm. The PDMP was further investigated and termed Bouncy Particle Sampler (BPS) in (Bouchard-Côté et al., 2018).\n\n\nCode\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nAlso, other types of continuous-time MCMC algorithms have been developed, such as the Zig-Zag sampler (Bierkens et al., 2019):\n\n\nCode\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nEnpirical evidence suggests that continuous-time MCMCs are more efficient than their discrete-time counterparts.\n\nInterestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as they need only access a small sub-set of data points at each iteration, and yet are still guaranteed to target the true posterior distribution. (Fearnhead et al., 2018)"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#inquiry-for-continuous-time-smc",
    "href": "posts/2024/Particles/PF.html#inquiry-for-continuous-time-smc",
    "title": "A Recent Development of Particle Methods",
    "section": "3 Inquiry for Continuous-time SMC",
    "text": "3 Inquiry for Continuous-time SMC\nDespite the success of continuous-time MCMC, the continuous-time limit of SMC has not been fully explored. The continuous-time limit of SMC is expected to be a jump process, which is similar to PDMP, but is more diffusion-like.\nMCMC has now taken a step ahead; it is time for SMC to explore its continuous-time limit!\n\n3.1 A Generic Particle Filter: An Algorithmic Description\n\n\n\nProcedure of a generic step of a particle filter at time \\(t\\)\n\n\n\nResampling Step\nParticles with high weights are duplicated, and those with the lowest weights are discarded.\nMovement Step\nSubsequently, a MCMC move is executed from the resampled particles.\n\nThe resampling step is the key difference from sequential importance sampling methods. Particle filters incorporate a resampling step to occasionally reset the weights of the samples, while maintaining the overall distribution they represent, in order to prevent the effective number of particles participating in the estimation from becoming too small–a situation also called weight degeneracy.\n\n\n3.2 A Necessary Condition: Resampling Stability\nIn order to have a time-step \\(\\Delta\\to0\\) limit, resampling events must occur with (at most linearly) decreasing frequency as \\(\\Delta\\to0\\).\nOnly the most efficient resampling schemes satisfy this property.\n\n\n\nRoot mean squared errors of marginal likelihood estimates (Chopin et al., 2022)"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#the-continuous-time-limit-process",
    "href": "posts/2024/Particles/PF.html#the-continuous-time-limit-process",
    "title": "A Recent Development of Particle Methods",
    "section": "4 The Continuous-time Limit Process",
    "text": "4 The Continuous-time Limit Process\nThe continuous-time limit process, if it exists, is characterized by a Feller-Dynkin process, whose infinitesimal generator is given by:\n\\[\n\\begin{align*}\n    \\mathcal{L}f(x)&=\\sum_{n=1}^N\\sum_{i=1}^db_i(x^n)\\frac{\\partial f}{\\partial x^n_i}(x)\\\\\n    &\\;\\;+\\sum_{n=1}^N\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x^n)\\frac{\\partial ^2f}{\\partial x^n_i\\partial x^n_j}(x)\\\\\n    &\\;\\;+\\sum_{a\\ne1:N}\\overline{\\iota}(V(x),a)\\biggr(f(x^{a(1:N)})-f(x^{1:N})\\biggl)\n\\end{align*}\n\\] \\[\n(f\\in C_c^2(\\mathbb{R}^{dN}),x\\in\\mathbb{R}^{dN},x^n\\in\\mathbb{R}^d)\n\\]\nwhen the latent process \\((X_t)\\) is an Itô process given by the generator:\n\\[\n\\begin{align*}\n    Lf(x)&=\\sum_{i=1}^db_i(x)\\frac{\\partial f}{\\partial x_i}(x)\\\\\n    &\\;\\;+\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x)\\frac{\\partial ^2f}{\\partial x_i\\partial x_j}(x)\n\\end{align*}\n\\] \\[\n(f\\in C_c^2(\\mathbb{R}^d),x\\in\\mathbb{R}^d)\n\\]\nFor details, please consult (Chopin et al., 2022, p. 3206), Theorem 19."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#conclusions",
    "href": "posts/2024/Particles/PF.html#conclusions",
    "title": "A Recent Development of Particle Methods",
    "section": "5 Conclusions",
    "text": "5 Conclusions\n\n\n\n\n\n\nSummaries\n\n\n\nSMC with efficient resampling schemes possess a continuous-time limit \\(\\Delta\\to0\\), which turns out to be a Feller-Dynkin process, a diffusion process with jumps, when \\((X_t)\\) is a diffusion."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#forthcoming-research",
    "href": "posts/2024/Particles/PF.html#forthcoming-research",
    "title": "A Recent Development of Particle Methods",
    "section": "6 Forthcoming Research",
    "text": "6 Forthcoming Research\n\n\n\n\n\n\nUltimate Purpose\n\n\n\nHow can we leverage the knowledge of the continuous-time limit process to design efficient Sequential Monte Carlo (SMC) samplers capable of sampling from posterior distributions of diffusions?\n\n\n\nWhat are the properties of this limit jump process, and how do they change with modifications to the underlying latent process?\nHow does the timing of resampling affect overall efficiency? Can insights be gained from the perspective of continuous-time limits?\nDoes the continuous-time limit process improve SMC efficiency when used for particle propagation?"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#footnotes",
    "href": "posts/2024/Particles/PF.html#footnotes",
    "title": "A Recent Development of Particle Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGood references are (Murphy, 2023) Chapter 13, and (Theodoridis, 2020, p. 881) Section 17.4.↩︎\n(Fearnhead et al., 2018) is a great introduction to this topic.↩︎"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html",
    "href": "posts/2024/Nature/StatisticalMechanics1.html",
    "title": "統計力学における基本的な模型の総覧",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#導入",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#導入",
    "title": "統計力学における基本的な模型の総覧",
    "section": "1 導入",
    "text": "1 導入\n統計力学では多数の粒子系のマクロな性質の記述・予測を目指す．\nそのモデルは大きく２つに大別される：1\n\n\n\n\n\n\n\n古典粒子系（第 2 節）：連続な位置と速度変数を持つ粒子系．主に ソフトマター の解析で用いられる模型である．\n格子模型（第 3 節）：位置は \\(\\mathbb{Z}^d\\) の部分集合に固定され，速度も持たないスピンの系．主に ハードな凝縮系の解析 で用いられる．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#sec-soft-matter",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#sec-soft-matter",
    "title": "統計力学における基本的な模型の総覧",
    "section": "2 古典粒子系",
    "text": "2 古典粒子系\n\n2.1 導入\n古典粒子を支配する物理法則は古典力学の枠組みで記述される．\nしかし，その運動方程式を解析的に分析するのではなく，相空間 \\(\\Omega\\) 上の（有界）測度に注目して，確率統計学，果てには計算統計学を利用して展開していく．\nちょうど，統計学では個々のサンプル \\(\\omega\\in\\Omega\\) よりも全体的な振る舞い \\(P\\in\\mathcal{P}(\\Omega)\\) や統計量 \\(\\Omega\\to\\mathbb{R}\\) の平均値や分散などの統計的な振る舞いに興味があるのと同じことである．\n\n\n2.2 相空間 \\(\\Omega_{\\Lambda,N}\\)\n有界集合 \\(\\Lambda\\subset\\mathbb{R}^3\\) に囚われた古典的な \\(N\\) 粒子系を考えると，位置と速度によって各粒子は記述できるから， \\[\n\\Omega_{\\Lambda,N}:=(\\Lambda\\times\\mathbb{R}^3)^N\n\\] 内の点によって系が記述できる．これを 相空間 (phase space) という．2\nまた，系が周期的な境界条件を満たす場合など，相空間として \\(d\\)-次元トーラス \\(\\mathbb{T}^d\\) 上の余接束を考えた方がより良い模型となる場合も多い．3\n\n\n2.3 ハミルトニアン \\(H_{\\Lambda,N}\\)\n古典粒子系では，相空間上の ハミルトニアン の形で力学が与えられることが典型的である：4 \\[\n\\begin{align*}\n    H_{\\Lambda,N}(Q,V)&=\\sum_{i=1}^N\\frac{mv_i^2}{2}+\\sum_{i&lt;j\\in[N]}U(q_i-q_j)\\\\\n    &\\qquad\\qquad+\\sum_{i=1}^NV_b(q_i)\n\\end{align*}\n\\] \\[\nQ=(q_1,\\cdots,q_N)\\in\\Lambda^{3N}\n\\]\n多くの場合，ポテンシャル関数 \\(U\\) はコンパクト台を持つ，すなわち，相互作用半径 (radius of interaction) \\(R\\) をもつとする：\\(\\mathrm{supp}\\;U\\subset U_R(0)\\)．5\n\n\n\n\n\n\nポテンシャルの例（Lennard-Jones ポテンシャル）\n\n\n\n\n\n経験則的に，分子系は Lennard-Jones ポテンシャル（第 2.8.1 節） \\[\nU(r)=\\frac{A}{r^{12}}-\\frac{B}{r^6}\\quad A,B&gt;0\n\\] を用いてモデリングする場合が多い．\n次の論文 (Peters and de With, 2012) では，この Lennard-Jones ポテンシャルを用いた分子動力学法を考察している：\n\n    \n        \n            \n            \n                Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n                Peters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である (Nemeth & Fearnhead, 2021)．\n            \n        \n    \n\n\n\n\n\n\n2.4 ダイナミクス \\(\\{S_t\\}\\)\n系の時間発展は変換の1-径数群 \\(\\{S_t\\}\\subset\\mathrm{Aut}(\\Omega_{\\Lambda,N})\\) （動力学 dynamics という）によって記述される．\nこの動力学は Hamiltonian を通じた運動方程式から導出される． \\[\n\\frac{d q_i}{d t}=v_i\n\\] \\[\nm\\frac{d v_i}{d t}=-\\sum_{i\\ne j}\\nabla U(q_j-q_i)-\\nabla V_b(q_i)\n\\]\n\n\n2.5 部分多様体 \\(\\Omega_{\\Lambda,N,E}\\) と小正準分布\nこの動力学 \\(\\{S_t\\}\\) は体積とエネルギーを保存量にもつ．すなわち，各 \\(S_t\\) は保測的で，等エネルギー集合 \\[\n\\Omega_{\\Lambda,N,E}:=\\left\\{(Q,\\Lambda)\\in\\Omega_{\\Lambda,N}\\mid H_{\\Lambda,N}(Q,\\Lambda)=E\\right\\}\n\\] を不変部分集合にもつ．\n\\(\\Omega_{\\Lambda,N,E}\\) 上の測度 \\[\n\\nu_{\\Lambda,N,E}(B):=\\lim_{\\Delta E\\to0}\\frac{\\ell(\\Delta B)}{\\Delta E}\n\\] は \\(S_t\\) によって保存される．6 これを 小正準分布 (microcanonical measures) または Gelfand-Leray measures という．7\nただし，\\(\\Delta B\\) は \\(x\\in B\\) から始まった \\(\\Omega_{\\Lambda,N,E}\\) の法線（面）で，\\(\\Omega_{\\Lambda,N,E+\\Delta E}\\) との交点で終わる線分の \\(x\\in B\\) に関する合併である．\n\n\n2.6 エルゴード仮説\nエネルギー以外にも，等位集合が \\(S_t\\) によって保存される相空間上の関数 \\(I_i:\\Omega_{\\Lambda,N}\\to\\mathbb{R}\\) は存在し得て，その場合は全ての合併 \\(\\Omega_{\\Lambda,N,E,I_1,\\cdots,I_k}\\) 上にも小正準分布が遺伝することになる．\nしかし，他の積分 \\(I_i\\) が存在しない場合，\\(\\Omega_{\\Lambda,N,E}\\) 上の \\(S_t\\)-不変で \\(\\nu_{\\Lambda,N,E}\\)-絶対連続な測度は，\\(\\nu_{\\Lambda,N,E}\\) の定数倍に限る．これを Boltzman のエルゴード仮説という．\nこの仮説により，特定の系が運動 \\(S_t\\) によって平衡状態に至った際，相空間上を旅する際に特定の状態が現れる頻度分布は，必ず小正準分布に一致することが帰結される．\n\n\n2.7 円板模型\n３次元ではなく，２次元で考えた古典粒子形を，円板モデル (disk model) という．\n\n2.7.1 剛体円板模型\n剛体円板模型 (hard-disk model) では，粒子の半径 \\(\\sigma&gt;0\\) が定める密度 \\[\n\\eta:=\\frac{N\\pi\\sigma^2}{L^2}\n\\] ハイパーパラメータとし，粒子同士が重なっていない任意の配置は全て一様に現れるとする模型である．8\nこの模型ですでに相転移 (melting transition) が起こることが知られている．9\nこの模型は (Metropolis et al., 1953) による Monte Carlo 法の開発から，現代の (Bernard et al., 2009) による Event Chain Monte Carlo 法によるシミュレーションまで，計算科学における中心的対象となっている．(Li et al., 2022) はそのことを，生物学におけるショウジョウバエに当たると表現している．\n\n\n2.7.2 液相転移\nしかし，液相転移（特に fluid-hexatic 相転移）の完全な熱力学的性質の解明は，(Metropolis et al., 1953) の時点から数値実験によるアプローチが試みられていたが，(Bernard and Krauth, 2011) の event chain Monte Carlo 法によるシミュレーションを通じた研究まで待つ必要があった．\n(Bernard and Krauth, 2011) は液相転移における種々の物理量（位置相関関数，方向相関関数など）の平均を記述する理論も提示した．10\nこれは (Engel et al., 2013) の分子動力学と大規模並行 Metropolis 計算により補強され，最終的にコロイドを用いた実験で確証された (Thorneywork et al., 2017)．\nこの剛体円板模型における相転移を基本として，種々のより複雑なソフトマター（フィルム，懸濁液など）の液相転移の研究に応用が進んでいる．\n\n\n2.7.3 柔らかい円板模型\n次のポテンシャル \\(U\\) を持った粒子系を 柔らかい円板模型 (soft-disk model) という： \\[\nU(x;k,\\eta,\\epsilon,N):=\\sum_{i&lt;j}U'(x_i,x_j;k,\\eta,\\epsilon),\n\\] \\[\nU'(x_i,x_j;k,\\eta,\\epsilon):=\\epsilon\\left(\\frac{2\\sigma}{d(x_i,x_j)}\\right)^k.\n\\]\n剛体円板模型は \\(k\\to\\infty\\) の極限を取ったものと理解できる．\n\n\n2.7.4 混合模型\n上述の模型はそれぞれ単独で興味の対称であるが，実際の粒子系をモデリングする際は，適切な重みを持って足し合わせることで，より当てはまりのよい模型を作る際の１成分として用いることも多い．11\n\n\n\n2.8 剛球模型\n\n2.8.1 Lennard-Jones 模型\n次で定まるポテンシャルを持つ，\\(\\mathbb{T}^3\\) 上の模型を Lennard-Jones 模型 という： \\[\nU(x;\\eta,\\sigma,\\epsilon,N):=\\sum_{i&lt;j}U'(x_i,x_j;\\sigma,\\epsilon),\n\\]\n\\[\\begin{align*}\n    U'(x_i,x_j;\\sigma,\\epsilon)&:=4\\epsilon\\biggr(\\left(\\frac{\\sigma}{d(x_i,x_j)}\\right)^{12}\\\\\n    &\\qquad\\quad-\\left(\\frac{\\sigma}{d(x_i,x_j)}\\right)^6\\biggl).\n\\end{align*}\\]\n\n\nCode\nusing Plots\n\nfunction lennard_jones(r, σ, ε)\n    return 4 * ε * ((σ / r)^12 - (σ / r)^6)\nend\n\nε = 1.0  # スケーリング\nσ = 1.0  # 粒子の半径\n\nr_range = 0.8:0.01:3.0\n\nV = [lennard_jones(r, σ, ε) for r in r_range]\n\n# プロットを作成\nplt = plot(r_range, V, \n    xlabel=\"r/σ\", ylabel=\"V(r)/ε\", \n    title=\"Lennard-Jones Potential\",\n    label=\"LJ Potential\",\n    lw=2, \n    legend=:topright,\n    ylims=(-1.2, 1),\n    color=\"#78C2AD\")\n\n# x軸とy軸に0の線を追加\nhline!([0], color=:black, linestyle=:dash, label=\"\")\n# vline!([0], color=:black, linestyle=:dash, label=\"\")\n\ndisplay(plt)\n# savefig(plt, \"Lennard-Jones.svg\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n柔らかい円板模型（第 2.7.3 節）の，３次元における精緻化として理解できる．電気的に中性な原子からなる粒子系のモデルである．\n\\(r^{-12}\\) の項を Pauli 斥力項，\\(-r^{-6}\\) の項を van der Waals 引力項という．12\n\n\n2.8.2 Coulomb ポテンシャル\n水などの分極を持った分子を考える場合，Lennard-Jones ポテンシャルに，次の Coulomb ポテンシャルを加えてモデリングする： \\[\nU_c(x_i,x_j;c_i,c_j)=\\frac{1}{4\\pi\\epsilon_0}\\frac{c_ic_j}{d(x_i,x_j)}.\n\\]\n\n\n2.8.3 結合ポテンシャル\n水などの分子では，分子同士の相互作用のほかに，分子内の原子同士の化学結合を通じた相互作用を考える必要がある．13\n\\(\\mathbb{T}^3\\) 上の３原子分子に対する調和伸縮ポテンシャル (harmonic bond-stretching potential) は14 \\[\nU_s(x_i,x_j;r_0,k_b):=\\frac{k_b}{2}\\biggr(d(x_i,x_j)-r_0\\biggl)^2,\n\\] 調和角度ポテンシャル (harmonic bond-angle potential) は \\[\nU_a(x_i,x_j,x_k;\\phi_0,k_a):=\\frac{k_a}{2}\\biggr(\\phi(x_i,x_j,x_k)-\\phi_0\\biggl)^2.\n\\] ただし， \\[\n\\phi(x_i,x_j,x_k):=\\arccos\\left(\\frac{x_{ij}^\\top x_{jk}}{d(x_i,x_j)d(x_j,x_k)}\\right)\n\\] は結合角とした．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#sec-hard-matter",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#sec-hard-matter",
    "title": "統計力学における基本的な模型の総覧",
    "section": "3 格子模型",
    "text": "3 格子模型\n一部は実際の物理系の良いモデルとなっているが，より複雑なモデルの統計物理学的性質の良い第一近似としても用いられる．\n加えて，特にスピングラス（第 4 節）は，このような離散的なグラフとしての表現を通じて，機械学習や情報科学との関わりを持つ．そのような分野は情報統計力学と呼ばれる (西森秀稔, 2003)．15\n\n3.1 格子気体\n古典粒子模型（第 2 節）のうち，位置は格子点上に固定され，速度も持たないとしたものである．\n\n3.1.1 格子気体模型の骨格\n粒子の位置は必ず格子点上にあるとすれば，配置空間はさらに \\(\\Lambda\\subset\\mathbb{Z}^3\\) に対して \\[\n\\Omega_{\\Lambda,N}=\\left\\{Q=\\begin{pmatrix}q_1\\\\\\vdots\\\\q_N\\end{pmatrix}\\in\\Lambda^N\\,\\middle|\\, q_i\\ne q_j\\;(i\\ne j)\\right\\}\n\\] と簡略化される．\n仮に外場もないとすると，ハミルトニアンは単にポテンシャルの和 \\[\nH(Q)=\\sum_{i&lt;j}U(q_i-q_j)\n\\] となる．\n\n\n3.1.2 平均場模型\nvan der Waals の格子気体の模型に，平均場近似を導入したものを，独立に考察した者の名前から，(Husimi, 1954)-(Temperley, 1954) 模型という．16\nIsing 模型における Curie-Weiss 模型（第 3.4 節）に対応する．\n\n\n\n3.2 スピン系\n格子気体のように，あるグラフ \\(\\mathcal{G}=(\\Gamma,\\mathcal{E})\\) 上に粒子が配置されていると考える．17\nただし，この粒子は スピン と呼ばれ，特定の追加の自由度を持つとする．\nスピン系の相空間は，関数の集合 \\[\n\\Omega_\\Lambda:=S^\\Lambda\n\\] になる．\n\n3.2.1 \\(n\\)-ベクトル模型\n\\(S=\\partial B^n\\subset\\mathbb{R}^n\\) と，\\(n-1\\) 次元球面上に取る場合を，\\(n\\)-ベクトル模型 という．18\n\n\n\n\n\n\n\n\\(n=2\\) の場合を \\(XY\\) 模型 という．\n\\(n=3\\) の場合を Heisenberg 模型 という．\n\n\n\n\n\n\n\n\n\n\n(Kosterlitz and Thouless, 1973) 転移19\n\n\n\n\n\nXY 模型では Kosterlitz-Thouless 転移 が起こることが予想され，1978年にヘリウム４で観測された．\n一般に低次元ではゆらぎが大きくなり秩序相が不安定になって相転移が起こらなくなる．\nIsing 模型では 1-2 次元の境界で，Heisenberg 模型では 2-3 次元の境界でこの現象が見られる．\n一方で，XY 模型は２次元で特殊な相転移（トポロジカル相転移）を起こす．このような転移は他にも Josephson 接合で見られる．\n\n\n\n\n\n3.2.2 Potts 模型\n\\(q\\)-状態 Potts 模型 (Potts, 1952) とは，\\(S:=[q]=\\{1,\\cdots,q\\}\\) として，\\(q\\) 種類のスピンを持つ模型のことをいう：20\n\\(q=2\\) の場合が Ising 模型に当たる．\nPotts 模型は特に，画像のモデルとしても用いられる (Storath et al., 2015)．\n\n\n3.2.3 Ising スピン\n\\(S=\\{\\pm1\\}\\) としても，物理系のモデルとして磁性体の第一近似として使える模型になる．これを Ising スピン という．21\nハミルトニアンは，相互作用と外場の和として \\[\nH_\\Lambda(\\sigma)=\\sum_{x\\ne y\\in\\Lambda}U(x-y)\\sigma(x)\\sigma(y)+h\\sum_{x\\in\\Lambda}\\sigma(x)\n\\] の形で与えられる．\n\n\n\n3.3 Ising 模型\n\n3.3.1 一般的な定義\n一般に，Ising 模型では２粒子間の相互作用のみが考えられる．\nこの場合，一般のグラフ \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\) に対して， \\[\nH_\\mathcal{G}(\\sigma):=-\\frac{J}{2}\\sum_{i=1}^N\\sum_{(i,j)\\in\\mathcal{E}}x_ix_j-h\\sum_{i=1}^Nx_i\n\\] という形で Hamiltonian を定義することが多い．\n\\(J\\) を 交換相互作用定数 (exchange / coupling constant) という．\\(h\\) は外場である．\n\n\n3.3.2 最近傍 Ising 模型\nポテンシャル \\(U\\) が \\(U_1(0)\\cap\\mathbb{Z}^3\\) を台に持つ場合を 最近傍 Ising 模型 ともいい，\\(U\\) は \\(J\\) でも表す：22 \\[\nH_\\Lambda(\\sigma)=-J\\sum_{\\lvert x-y\\rvert=1}\\sigma(x)\\sigma(y)-h\\sum_{x\\in\\Lambda}\\sigma(x).\n\\]\nこれは，上述のグラフによる定義で，グラフとして \\(\\mathbb{Z}^3\\) 上の格子を取った場合に当たる．Metropolis 法によるシミュレーションが こちら から見れる．\n１次元の最近傍 Ising 模型には相転移が存在しない (Ising, 1925) が，２次元以上では存在することが知られている．その場合，低温秩序相は強磁性，高温相は常磁性を示す．\n２次元かつ \\(h=0\\) の場合は (Onsager, 1944) により自由エネルギーの解析形が特定された．３次元の場合は conformal bootstrap により厳密な数値解が求められるようになっている (El-Showk et al., 2012)．23\n\n\n\n3.4 Curie-Weiss 模型\n最近傍 Ising 模型に，Weiss 近似 (Weiss, 1907) という平均場近似を施して得る Curie-Weiss Hamiltonian \\[\nH_{\\Lambda}(\\sigma)=-\\frac{dJ}{\\lvert\\Gamma\\rvert}\\sum_{x\\ne y\\in\\Lambda}\\sigma(x)\\sigma(y)-h\\sum_{x\\in\\Lambda}\\sigma(x)\n\\] を用いる模型を Curie-Weiss 模型 という．24\nこの式からは，各スピンが，具体的な他のスピンと相互作用するというより，全ての他スピンからなる平均場（有効磁場）と相互作用していると読める．25\nこのような平均場近似を施していても，Curie-Weiss 模型は（連続な）相転移を示す．26 しかし，自由エネルギーは非凸関数になっており，他にも \\(h=0\\) の場合に非物理的な解が現れるなど，平均場近似の痕跡が随所に見られる．\nCurie-Weiss 模型における磁化の Lifted Metropolis-Hastings サンプラーのスケール極限には，Zig-Zag 過程が現れる (Bierkens and Roberts, 2017)．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#sec-spin-glass",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#sec-spin-glass",
    "title": "統計力学における基本的な模型の総覧",
    "section": "4 スピングラス",
    "text": "4 スピングラス\nスピングラスも格子模型の一種であるが，新たなランダム性と相を持つ模型である．\n\n4.1 スピングラスの定義\nHamiltonian\n\\[\nH(\\sigma):=-\\sum_{p=1}^{P}\\sum_{i_1&lt;\\cdots&lt;i_p}J_{i_1,\\cdots,i_p}\\sigma_{i_p}\\cdots\\sigma_{i_p}\n\\]\nにおいて，\\(p\\ge2\\) かつ \\(J_{i_1,\\cdots,i_p}\\) の符号がバラバラである場合，これをスピングラスの模型という．27\nこのような模型では，低温秩序相が消えて，スピンがバラバラである状態（スピングラス相）が出現し得る．特に，安定な状態が複数存在し，温度を少し変えるだけで全く性質の異なる別の状態へ系が移ることもよくある．換言すれば，自由エネルギーが強い多峰性を示すことは，スピングラスの特徴付けと理解される．\n\n\n\n最終講義 スピングラスと計算物性物理 p.28\n\n\n\n\n\n\n\n\nスピングラスの例\n\n\n\n\n\nCuMn などはスピン間の相互作用が，RKKY (Ruderman-Kittel-Kazuya-Yoshida) 相互作用 \\[\nJ\\,\\propto\\,\\frac{\\cos(2k_Fr_{12})}{r_{12}^3}\n\\] により表され，これは符号が分子の \\(\\cos\\) により正にも負にもなり得る．\nAuFe もスピングラスである (Cannella and Mydosh, 1972)．28\n\n\n\n\n\n4.2 Edwards-Anderson 模型 (Edwards and Anderson, 1975)\n\\((J_{x,y})_{\\lvert x-y\\rvert=1}\\) を，グラフのエッジの集合上に定義された独立な Gauss 確率場とし，外場を考えないものを，Edwards-Anderson 模型 という： \\[\nH(\\sigma)=-\\sum_{\\lvert x-y\\rvert=1}J_{x,y}\\sigma(x)\\sigma(y).\n\\]\n確率変数 \\(J_{x,y}\\sim\\mathrm{N}_1(0,N)\\) の平均（配位平均）と，アンサンブル平均という２つの平均を扱う必要がある点で極めて難しい模型となっている．\nこの模型において自由エネルギーを計算するために，分配関数の対数の平均を，分配関数の積率によって計算する \\[\n\\operatorname{E}[\\log Z]=\\lim_{n\\to\\infty}\\frac{\\operatorname{E}[Z^n]-1}{n}\n\\] という関係式を用いた．これを レプリカ法 という．ただし，期待値は \\(J_{x,y}\\) に関するもので，アンサンブル平均 \\(\\langle-\\rangle\\) とは関係ないことに注意．\n現状，特に Talagrand はレプリカ法の数学的妥当性について極めて懐疑的であるが，多くは数値実験により検証されており，何らかの本質を捉えていることは間違いない．29\n\n\n4.3 Sherrington-Kirkpatrick 模型 (Sherrington and Kirkpatrick, 1975)\nEA 模型を無限レンジにすることで，平均場近似が厳密解を与えるようにし，熱力学極限 \\[\n\\lim_{N\\to\\infty}\\frac{\\operatorname{E}[\\log Z_N]}{N}\n\\] を与えることでこれを解いたものである．その際にもレプリカ法が用いられた．\n著者のうちの Scott Kirkpatrick は 擬似アニーリング (Kirkpartick et al., 1983) の開発者でもある．\nしかしこの解（レプリカ対称な解）は初め低温域では破綻を起こすとされていた．(Parisi, 1980) がこの問題を解決し，任意の温度 \\(T&gt;0\\) での厳密解（レプリカ対称性破れ解）が得られた．これは Parisi ansatz と呼ばれる．30 この解は計算機シミュレーションと高い精度で一致し，常磁性相と強磁性相に加えて，スピングラス相を示す．\nParisi はこの業績で 2021 年にノーベル物理学賞を受賞した．その３番目に多く引用されている論文 (Marinari and Parisi, 1992) は 擬似テンパリング の提案論文である．\n\n\n4.4 Thouless-Anderson-Plamer 方程式 (Thouless et al., 1977) と近似メッセージ伝播 (Bolthausen, 2014)\n一方で，SK 模型に対して高温摂動展開により自由エネルギーを与えるアプローチもある．\n特に，熱力学極限に向かって漸近的に成り立つ次の方程式を TAP 方程式という： \\[\\begin{align*}\n    \\langle\\sigma_i\\rangle&\\approx\\tanh\\biggr(\\frac{\\beta}{\\sqrt{N}}\\sum_{j\\ne i}J_{ij}\\langle\\sigma_j\\rangle\\\\\n    &\\qquad\\quad\\qquad\\quad+h-\\beta^2(1-q)\\langle\\sigma_i\\rangle\\biggl)\n\\end{align*}\\]\nこれの数学からの証明も近年試みられている (Talagrand, 2003), (Chatterjee, 2010)．しかし，厳密な証明は高温に限られ，低温域では解の一意性が失われるのが困難を窺わせる．２つの層の分離面としては Almeida-Thouless 線が提案されている．\nしかし (Bolthausen, 2014) は 近似メッセージパッシング に基づいて，この TAP 方程式の解を与えるアルゴリズムを提案した．このアルゴリズムは，高次元統計学において \\(M\\)-推定量を計算するのにも応用されている (Donoho and Montanari, 2016)．高次元漸近論は計算科学の進歩とともにあるのである．\n\n\n4.5 Hopfield 模型 (Hopfield, 1982)\nのちにスピングラスの理論は 連想記憶 にも応用され，広く情報処理の問題を統計力学の技法によって研究する情報統計力学という新たな分野が開拓された．\n連想記憶のニューラルネットワーク は無限レンジ，すなわち全結合のニューラルネットワークで，素子 \\(\\{S_i\\}_{i=1}^N\\subset\\mathrm{Map}(T;\\{\\pm1\\})\\) からなるとき， \\[\nS_i(t+\\Delta t)=\\operatorname{sgn}\\left(\\sum_{j\\ne i}J_{ij}\\frac{S_j(t)+1}{2}-\\theta_i\\right)\n\\] という規則で運動する．\\(J_{ij},\\theta_i\\) がモデルパラメータである．\n\\(J_{ij}\\) をうまく「学習」できた際には，一部の初期値について，この運動の収束先として画像が「連想」出来る．記憶しておけるのである．31\n実は，\\(p\\) 個のパターン \\((\\xi^\\mu_i)_{i=1}^N\\in\\Omega\\;(\\mu=1,\\cdots,p)\\) を記憶させるには， \\[\nJ_{ij}=1_{\\left\\{i\\ne j\\right\\}}\\frac{1}{N}\\sum_{\\mu=1}^p\\xi_i^\\mu\\xi_j^\\mu\n\\] と設定すると良いことが知られており，これを Hebb 則 (Hebb, 1949) という．\n連想記憶がうまくいくためには，互いの直交性 \\[\n\\frac{1}{N}(\\xi^\\mu|\\xi^\\nu)=\\delta_{\\mu,\\nu}+O\\left(\\frac{1}{\\sqrt{N}}\\right)\n\\] が重要であることも知られている．\n実は，Hebb 則によるパラメータを備えた Hopfield 模型は，結合が対称である \\(J_{ij}=J_{ji}\\) とき，次の Hamiltonian を減少させる方向に運動する： \\[\nH=-\\frac{1}{2}\\sum_{i\\ne j}J_{ij}S_iS_j\n\\]\nここで係数 \\(J_{ij}\\) はデータ \\((\\xi^\\mu)_{\\mu=1}^p\\) から決まっているという意味では，確率変数であることに注意．\nこの模型を統計力学的に解析すると，Hopfield 模型は，想起相だけでなく，常磁性相ももち，その間にスピングラス相がある．これは \\(\\alpha=\\frac{p}{N}\\) を大きくすると到達することができる (西森秀稔, 2003, p. 57)．\n素子数一定の状況下で，覚えるパターン数を増やしすぎると，ある瞬間に相転移を起こして何も覚えなくなるのである．\n\n\n4.6 因子グラフ\n特に近距離相互作用のみを仮定している場面では，ハミルトニアン \\(H\\) やその他の物理量の 局所性 が目立った（コンパクト台を持つ関数になっている）．\nそのこともあり，物理系はグラフィカルモデル（Bayesian networks, Markov networks）としての表現と親和性があり，特に 因子グラフ が重要な形式として用いられる (Mézard and Montanari, 2009, p. 100)．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#参考文献",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#参考文献",
    "title": "統計力学における基本的な模型の総覧",
    "section": "5 参考文献",
    "text": "5 参考文献\n\n(Minlos, 2000), (西森秀稔, 2003), (Altieri and Baity-Jesi, 2024), (Chatterjee, 2023), (Panchenko, 2012), (Talagrand, 2003), (Bolthausen, 2014), (Faulkner and Livingstone, 2024)．"
  },
  {
    "objectID": "posts/2024/Nature/StatisticalMechanics1.html#footnotes",
    "href": "posts/2024/Nature/StatisticalMechanics1.html#footnotes",
    "title": "統計力学における基本的な模型の総覧",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Faulkner and Livingstone, 2024, p. 3) 2.1節も参照．↩︎\nすなわち，\\(\\Lambda\\) 上の線型束である．↩︎\n(Faulkner and Livingstone, 2024, pp. 3–4 2.2節) も参照．↩︎\n\\(V\\) は境界ポテンシャルといい，外場との相互作用がある場合に現れる．↩︎\nこれは粒子間相互作用が short-range であると仮定しているためである．重力やクーロン力を考えている訳ではない．↩︎\n(Petersen, 1983, p. 6) 命題2.2．↩︎\nこれらの確率測度については 次稿 も参照．↩︎\n(Faulkner and Livingstone, 2024, p. 8) 3.2 節に倣った．↩︎\n(Faulkner and Livingstone, 2024, p. 8) 3.2 節も参照．↩︎\n液相転移においては，まず位置の秩序が破壊され，次に方向性の秩序が破壊される．従って，液相と固体相の間に別の相が定義でき，これを hexatic phase という．これを最初に記述したのが KTHNY 理論である (Kosterlitz and Thouless, 1973)．詳しくは (Faulkner and Livingstone, 2024, pp. 10–11) 3.2.2 節も参照．↩︎\n(Faulkner and Livingstone, 2024, p. 8) 3.2 節も参照．↩︎\n瞬間的な電気的双極子が互いに引きつけ合うことにより生じる引力のことである．(Faulkner and Livingstone, 2024, p. 11) 3.3 節も参照．↩︎\n(Faulkner and Livingstone, 2024, p. 13) 3.6 節も参照．↩︎\nこれが二次関数になっていることは，Hooke の法則 による．しかし，例えばグラフェンなどに対しては，その結合の強さを加味するために４次関数を用いる (Wei et al., 2011)．(Faulkner and Livingstone, 2024, p. 12) 3.5 節も参照．↩︎\n統数研プロジェクト紹介 も参照．↩︎\n(Kac and Thompson, 1966) も参照．↩︎\n例えば格子点の集合 \\(\\Gamma\\subset\\mathbb{Z}^n\\) など．↩︎\n(西森秀稔, 2005, p. 100) に倣った．↩︎\n(西森秀稔, 2005, p. 100) も参照．↩︎\n(西森秀稔, 2005, p. 15)，(Mézard and Montanari, 2009, p. 26) 例2.2 に倣った．↩︎\n(西森秀稔, 2003, p. 4)．情報統計力学では，スピンをビットやニューロンの状態に見立て，そのモデルとしても頻繁に用いられる．↩︎\n(Baxter, 1982, p. 21) では nearest-neighbour Ising model と呼んでいる．↩︎\nその代わり臨界指数が有理数であるかもわからない状況となっている．臨界点においてはスケール変換に関して不変な理論になることから，共形場理論で解析が可能である．２次元では臨界指数の予測をしてくれる一般論があるが，高次元の共形場理論の挑戦が期待される．↩︎\nWeiss は 分子場 と呼んだ．相互作用がもはや最近傍同士ではなくなっている．この点から 無限レンジ模型 ともいう．(西森秀稔, 2003, p. 24) も参照．↩︎\n無限レンジの仮定をおくと，平均場近似は近似でなくなる，という論理的依存関係がある (西森秀稔, 2003, p. 26)．↩︎\n例えば (Friedli and Velenik, 2017, p. 62) を参照．↩︎\n(Mézard and Montanari, 2009, p. 242) (西森秀稔, 2003, p. 16) など．スピングラスはもともと B. Coles が希薄合金の磁性を表現するために造語したが，現在は「全くランダムな状態でスピンが凍結した状態」という意味で使われるようになている (都福仁, 1977)．↩︎\nMydosh による講演が，髙山一氏のスピングラス研究の発端となったという（最終講義）．↩︎\n(田中利幸, 2007)↩︎\n(Panchenko, 2012) も参照．↩︎\nパターンを記憶させることを「埋め込む」ともいう (西森秀稔, 2003, p. 33)．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/R0.html",
    "href": "posts/2024/Computation/R0.html",
    "title": "R の概観",
    "section": "",
    "text": "リンク集\n\n\n\n\nR Manuals (by R Development Core Team)\nAll R Language Documentation\nR Language Definition"
  },
  {
    "objectID": "posts/2024/Computation/R0.html#r-の概要",
    "href": "posts/2024/Computation/R0.html#r-の概要",
    "title": "R の概観",
    "section": "1 R の概要",
    "text": "1 R の概要\nR 言語とは\n\n統計計算のための言語と環境の総称\n最新の技術や方法が簡単に導入できることも多い\n\n新しい技法のデモとして論文でも実装される．\nPython や Julia と同様．\n\nAustria 中心の開発．データの分類・集計・整理の機能は織り込み済み．\n\nニュージーランドのオークランド大学の Ross Ihaka と Robert Clifford Gentleman により作られた\nBBC のグラフは R で書いている．BBC が R 用のパッケージをリリースしている．"
  },
  {
    "objectID": "posts/2024/Computation/R0.html#基本型",
    "href": "posts/2024/Computation/R0.html#基本型",
    "title": "R の概観",
    "section": "2 基本型",
    "text": "2 基本型\n\n2.1 データ構造：built-in は５つ\n\nvector\n\nscaler オブジェクトは長さ１の vector として実装されている？\n\nscaler は数値，文字列，論理値など．\n\nc()が constructor\nx[]で indexing できる\n\n[]の中身はベクトルで指定できる！これが slice の代わり．\n\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- c(10, 20, 30, 40, 50)\nx[1]\n\n[1] 10\n\n\n\nx[2:4]\n\n[1] 20 30 40\n\n\n\n\n\n\nmatrix\n\nmatrix() がコンストラクタ\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\nmat[2, 3]\n\n[1] 8\n\n\n\n\n\n\nlist：ベクトルとの違いはデータ型が不均一であるのを認めること．\n\nlist() がコンストラクタ\n多分データ分析じゃない計算機命令的な棲み分け．一番 csv みたい，\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nlst &lt;- list(name = \"Alice\", age = 25, scores = c(90, 85, 88))\nlst\n\n$name\n[1] \"Alice\"\n\n$age\n[1] 25\n\n$scores\n[1] 90 85 88\n\n\n\nlst[[1]]\n\n[1] \"Alice\"\n\n\n\nlst$name\n\n[1] \"Alice\"\n\n\n\n\n\n\ndata frame：要は実データに即した構造で，行列の拡張．csv みたいな．長さの等しい vector の list．\n\ndata.frame がコンストラクタ．\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\ndf &lt;- data.frame(name = c(\"Alice\", \"Bob\"), age = c(25, 30))\ndf\n\n   name age\n1 Alice  25\n2   Bob  30\n\n\n\ndf[1, 2]\n\n[1] 25\n\n\n\ndf$name\n\n[1] \"Alice\" \"Bob\"  \n\n\n\n\n\n\narray：ベクトル，行列のその先へ\n\narrayがコンストラクタ\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\narr &lt;- array(1:8, dim = c(2, 2, 2))\narr\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\n\n\narr[1, 2, 2]\n\n[1] 7\n\n\n\n\n\n\n\n2.2 データ型：built-in は 251\ntypeof()で調べる\n\nNote that in the C code underlying R, all objects are pointers to a structure with typedef SEXPREC; the different R data types are represented in C by SEXPTYPE, which determines how the information in the various parts of the structure is used.\n\n\nint：整数\ndouble：クラスとしてはnumericとして実装されている．浮動小数点\n\n3.4e-2は \\(3.4×10^-2\\) で表す．\n\ncomplex：1iを虚数単位とし，a+biと表す．\ncharacter\n\npaste(x,y,[sep=“ “])\n\nlogical：Boole 値\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\n\ntypeof(1i)\n\n[1] \"complex\"\n\n\n\ntypeof(FALSE)\n\n[1] \"logical\"\n\n\n\n\n\n\nprint(pi, digit=12)\n\n任意精度表示．多分 print.numeric_version への dispatch\n\n\n\nprint(pi, digit=12)\n\n[1] 3.14159265359\n\n\n\n\n2.3 予約語とそのデータ型\n\npi, e：double型\nInf, NaN：doule型\nT,F：TRUE と FALSE の予約．logical 型\n\nas.numeric()で1,0に埋め込まれる．\n\nNA：Not Available，統計データの欠損を表す．\n\n各モードに１つずつ存在する generic な存在である．\nNA_integer_, NA_real_, NA_complex_, NA_character_, NA_logical_, ……\n\nLETTERS, letters：アルファベットのベクトル，character 型\nNULL：NULL 型でモードを持たない特殊なオブジェクト．\\(\\emptyset\\) のこと．\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\n\n\n\n\n\n2.4 データのクラス\nclass()で調べることが出来る．\n多くはデータ型（typeof()）と一致する．\n\ntypeof(pi)\n\n[1] \"double\"\n\n\n\nclass(pi)\n\n[1] \"numeric\"\n\n\n\n\n2.5 Logical：２値関数\nベクトルを各成分ごとに評価して，logical vector を返す関数．\n\n!=, &gt;=, &lt;=\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(3, 2, 1)\nx != y\n\n[1]  TRUE FALSE  TRUE\n\n\n\nx &gt;= y\n\n[1] FALSE  TRUE  TRUE\n\n\n\nx &lt;= y\n\n[1]  TRUE  TRUE FALSE\n\n\n\n\n\n\nis.null(), is.na(), is.nan(), is.finite(), is.infinite()\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nis.na(c(1, NA, 3))\n\n[1] FALSE  TRUE FALSE\n\n\n\nis.finite(c(1, Inf, NA, NaN))\n\n[1]  TRUE FALSE FALSE FALSE\n\n\n\n\n\n\nidentical(1,1.0)：Tである，いずれも倍精度浮動小数点で表現されるので．\nall.equal()：「ほぼ同じ」かどうか．数値の近似比較に使える．\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nidentical(1, 1.0)\n\n[1] TRUE\n\n\n\nall.equal(1, 1.0000001)\n\n[1] \"Mean relative difference: 1e-07\"\n\n\n\n\n\n\n&,|：論理積・和\n&&, ||：条件式の論理積・和\n\n&,|と違い，ベクトルには使えない．\n\nxor()：排他的論理和\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- c(TRUE, FALSE, TRUE)\ny &lt;- c(TRUE, TRUE, FALSE)\nx & y\n\n[1]  TRUE FALSE FALSE\n\n\n\nx && y\n\nError in x && y: 'length = 3' in coercion to 'logical(1)'\n\n\n\nxor(x, y)\n\n[1] FALSE  TRUE  TRUE\n\n\n\n\n\n\n%in%：match の二項関係版 interface．\n\n\"%in%\" &lt;- function(x, table) match(x, table, nomatch = 0) &gt; 0が現状の定義\n\nwith(data, expr, …)\n\ndata：data frame\nexprはdataの列名に関する expression をベタがき\n列ごとにexprを実行した結果を返す．\n\nmatch(x, table)\n\nx %in% tableと使える．\nx：vector\ntable：vector\n返り値：logical vector\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- c(1, 2, 3)\ntable &lt;- c(2, 3, 4)\nx %in% table\n\n[1] FALSE  TRUE  TRUE\n\n\n\nmatch(x, table)\n\n[1] NA  1  2\n\n\n\ndata &lt;- data.frame(a = 1:3, b = 4:6)\nwith(data, a + b)\n\n[1] 5 7 9\n\n\n\n\n\n\n\n2.6 mode について\n\nstr(obj)：R object の構造を教えてくれる．structure\nmode(obj)：オブジェクトのモード＝データ型を返す．\nclass(obj)：R は全てのものはオブジェクトだから，class を返す．この値に従って dispatch されている．\n\n\nFunction mode gives information about the mode of an object in the sense of Becker, Chambers & Wilks (1988), and is more compatible with other implementations of the S language. – R Language Definition\n\n\nstorage.mode(obj)：オブジェクトの storage mode を返す．\n\n\nFinally, the function storage.mode returns the storage mode of its argument in the sense of Becker et al. (1988). – R Language Definition\n\n\nhelp()：?keywordと等価．\n\nTrig {base}：パッケージ base の Trig についての説明．\ngraphics::hist：はパッケージ graphics の関数 hist について．\n特殊関数を調べるには””で escape する必要があることがある．\n\nexample()：R のこの機能やばすぎる\n\nhelp 内の例を実行\ndemo()がさらにある．\n\nhelp.search()：??”keyword”と等価．\n\nキーワード検索\nGoogle 検索と同じ要領で使ってください．"
  },
  {
    "objectID": "posts/2024/Computation/R0.html#属性",
    "href": "posts/2024/Computation/R0.html#属性",
    "title": "R の概観",
    "section": "3 属性",
    "text": "3 属性\nNULL以外の全てのオブジェクトはattributeを持ち得る．attributeとは，全ての成分に名前がついたpairlistである．\n\nattributes(y ~ x1 + x2)\n\n$class\n[1] \"formula\"\n\n$.Environment\n&lt;environment: R_GlobalEnv&gt;\n\n\n属性は，第一義的には R にクラス構造を実装するのに使われる．class属性を評価し，そのオブジェクトにどの function dispatch を適用するかを決定する．\n\n  \n    \n      \n      \n        R の概観\n        R は統計計算のための言語です．  \n      \n    \n  \n\n\n3.1 names\nnamesは，ベクトルの各要素に名前をつけるための属性であり，indexing にも使われる．\n\n\n3.2 class\nclassは，オブジェクトのクラスを指定する文字列である．"
  },
  {
    "objectID": "posts/2024/Computation/R0.html#r-のオブジェクト志向構造",
    "href": "posts/2024/Computation/R0.html#r-のオブジェクト志向構造",
    "title": "R の概観",
    "section": "4 R のオブジェクト志向構造",
    "text": "4 R のオブジェクト志向構造\n\n4.1 Polymorphism（多態的）なオブジェクト志向\n\npolymorphic な関数を generic function という．2\nR では、クラスはオブジェクトに付随する属性として扱われるものの一つであり、リストとして保持される。その「クラス」という付加情報によって，同じ関数名でも挙動が違う，というのが R の関数である．\n\nR の OO システムは３つあり，S3, S4, R5 という．\n\nS3 implements a style of object oriented programming called generic-function OO. This is different to most programming languages, like Java, C++ and C#, which implement message-passing OO. In message-passing style, messages (methods) are sent to objects and the object determines which function to call. Typically this object has a special appearance in the method call, usually appearing before the name of the method/message: e.g. canvas.drawRect(“blue”). S3 is different. While computations are still carried out via methods, a special type of function called a generic function decides which method to call. Methods are defined in the same way as a normal function, but are called in a different way, as we’ll see shortly.\n\n\n\n\n\n\n\nS3 について\n\n\n\n\n\nS3クラス（S言語ver.3という意味） 初期のRクラス構造で現在もRの有力なクラスパラダイム． 組み込みクラスのほとんどがS3パラダイム． 1. 全てがリストであり，新たにクラス名属性を持つとクラスになる． 2. method dispatch機能の実装のためにある 1. 「ジェネリック関数が呼び出された時に，実引数のmodeを見て適切なクラスメソッドに引き渡す機能」があるのでgeneric functionが定義できる． 2. plot()で適切に動くのも，全てのオブジェクトが密かにクラスが違うからである．ジェネリック関数printの呼び出しprint(lm)はlmクラスのメソッドprint.lm()にディスパッチされる．\n実装に使われている関数 * attr()： * class()：クラス属性のベクトル．ここへ付与する形でS3は実装されている．\nそれを確認できる関数 * attributes(obj)：\\(nameや\\)classなどの属性を格納したlist． * 実はリストのtag（\\(で参照されるやつ）はnamesというattributeである．\n    * それに次いで２番目が\\)classというattributeになる． * 通常のlist objectを表示した時，最後の要素がattr(,”class”)となるのは，通常のリストの構造は１列目の要素としたら，２列目の要素ということである． * ＃これがリストの多次元入れ子構造． * unclass(obj)：クラス属性を外せる．lmなど，ほとんどがクラスという属性を持ったlist． * methods(fun)：generic methodの全てを表示する． * がついているものは，デフォルトのbase名前空間にはない関数．  getAnywhere()：あらゆる名前空間からその名前を持つobjectを持ってくる． * 標準名前空間にない存在はnamespace:::nameでアクセスできる． * methods(,”classname”)と使うと，classnameという名前を持ったクラスにdispatchされているmethodを表示する．\n\n\n\n\n\n4.2 クラスシステム\n\nlist などはクラスとして実装されていて，class(object) で確認できる．\nmode(object) &lt;- value は storage mode のこと．\n\ntypeof()を wrap している．\n&lt;- valueとできるのは：“logical”, “integer”, “double”, “complex”, “raw”, “character”, “list”, “expression”, “name”, “symbol” and “function”\n\ntypeof(object)\n\n\n\n4.3 S3 クラス（S 言語 ver.3 という意味）\n初期の R クラス構造で現在も R の有力なクラスパラダイム． 組み込みクラスのほとんどが S3 パラダイム． 1. 全てがリストであり，新たにクラス名属性を持つとクラスになる． 2. method dispatch機能の実装のためにある 1. 「ジェネリック関数が呼び出された時に，実引数のmodeを見て適切なクラスメソッドに引き渡す機能」があるのでgeneric functionが定義できる． 2. plot()で適切に動くのも，全てのオブジェクトが密かにクラスが違うからである．ジェネリック関数printの呼び出しprint(lm)はlmクラスのメソッドprint.lm()にディスパッチされる．\n実装に使われている関数 * attr()： * class()：クラス属性のベクトル．ここへ付与する形でS3は実装されている．\nそれを確認できる関数 * attributes(obj)：\\(nameや\\)classなどの属性を格納したlist． * 実はリストのtag（\\(で参照されるやつ）はnamesというattributeである．\n    * それに次いで２番目が\\)classというattributeになる． * 通常のlist objectを表示した時，最後の要素がattr(,”class”)となるのは，通常のリストの構造は１列目の要素としたら，２列目の要素ということである． * ＃これがリストの多次元入れ子構造． * unclass(obj)：クラス属性を外せる．lmなど，ほとんどがクラスという属性を持ったlist． * methods(fun)：generic methodの全てを表示する． * がついているものは，デフォルトのbase名前空間にはない関数．  getAnywhere()：あらゆる名前空間からその名前を持つobjectを持ってくる． * 標準名前空間にない存在はnamespace:::nameでアクセスできる． * methods(,”classname”)と使うと，classnameという名前を持ったクラスにdispatchされているmethodを表示する．\nクラス定義のための機構 * print() * prints its argument and returns it invisibly (via ‘invisible(x)’) * 新しいクラスを作ったら，そのためのprintを定義するのが流れ．\n\n\n4.4 S4 クラス\nS3 クラスとの棲み分けは，安全性で，まだ存在しないクラスコンポーネントにアクセスできなくなった．"
  },
  {
    "objectID": "posts/2024/Computation/R0.html#footnotes",
    "href": "posts/2024/Computation/R0.html#footnotes",
    "title": "R の概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Wickham, 2019) 第12章．↩︎\nc++ならば仮想関数という．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/calculus.html",
    "href": "posts/2024/Computation/calculus.html",
    "title": "R による記号微分入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/calculus.html#概要",
    "href": "posts/2024/Computation/calculus.html#概要",
    "title": "R による記号微分入門",
    "section": "概要",
    "text": "概要\nR パッケージcalculusは，記号微分と数値微分・積分をシームレスに繋いだ機能を提供するパッケージである．\n数値微積分はRcppパッケージとcubatureパッケージを通じてC++バックエンドを用いて提供する．記号微分は外部の記号計算ソフトウェアに依らない実装を提供している．\ninstall.packages(\"calculus\")\n\n\n\n\n\n\n例\n\n\n\n\n\n\nEinstein の縮約記法\nLevi-Civita 記号の計算\n一般化 Kronecker デルタ\nTaylor 展開\n高階導関数\n多変数 Hermite 多項式\n常微分方程式\n微分作用素\n\n\n\n\n\n\n\n\n\n\n関連パッケージとの違い\n\n\n\n\n\n\nnumDerivは数値微分による Jacobian, Hessian 計算を提供するが，それ以上の高階の微分ができず，テンソル値関数の微分もできない．\ntensorAは Einstein の縮約記法を提供するが，２階のテンソルまでに限る．\nmpolyは１変数の Hermite 多項式を提供するが，多変数には対応していない．\npracmaは１変数の Taylor 展開を提供するが，多変数には対応していない．\ncubatureは多変数関数の数値積分に対応しているが，その他の（直交）座標には対応していない．\n\n特に R は記号計算が苦手である．外部の記号計算ソフトウェアに繋ぐことは試みがあるかもしれないが，R 独自のパッケージ内で扱うシステムは従来なかった．\n\nRyacasはYacasという外部の記号計算ソフトウェアへのインターフェースを提供する．\ncaracasは R-Python インターフェースreticulateを通じてSymPyという Python の記号計算ライブラリへのインターフェースを提供する．\n\nこれにより，R の，新たな統計推測手法を実装し論文として公開するために用いる言語という性質を活かすことが，パッケージcalculusの目的にある (Guidotti, 2022, p. 4)．\n\n\n\n確率過程に対する漸近展開公式では，1000 を超える連立常微分方程式系を解き，その解を通じて多変数 Hermite 多項式の和を計算する必要があり，その際に YUIMA パッケージにおいて記号微分を実行する際に用いられている：\n\n  \n    \n      \n      \n        YUIMA 入門\n        確率微分方程式のシミュレーションと推測のためのパッケージ`yuima`の構造と使い方をまとめます．"
  },
  {
    "objectID": "posts/2024/Computation/calculus.html#基本",
    "href": "posts/2024/Computation/calculus.html#基本",
    "title": "R による記号微分入門",
    "section": "1 基本",
    "text": "1 基本\n\n1.1 概観\nベクトル，行列，テンソルはいずれも配列 array として実装されている．特に，ベクトルと行列はテンソルの特殊な場合と理解できるように，実装上も，統一的に Einstein の縮約記法が適用可能である．\nすべての関数（＝数学的演算）は，数値バージョンと記号演算バージョンのディスパッチとして実装される．\n\n\n1.2 演算\nnumeric, complex, character, expression のいずれかの型を持つ，同次元の配列に対して定義されている．\n例えば１次元のcharacterに対しては：\n\nlibrary(calculus)\n(\"a + b\" %prod% 1i) %sum% (0 %prod% \"c\") %diff% (expression(d + e) %div% 3)\n\n[1] \"((a + b) * (0+1i)) - ((d + e) / 3)\"\n\n\nここで，1i, 0, expression(d+e),3はいずれもcharacterに変換されてから実行されている：\n\ntypeof(.Last.value)\n\n[1] \"list\"\n\n\nこの段階では+0などを省くのみで，評価や簡約化はされない．\n\n\n\n\n\n\n() の消去\n\n\n\n\n\n\\[\n(a+b)(c+d)\n\\] を \\[\na+bc+d\n\\] と解釈してしまうことなどを防ぐため，すべての変数は()で囲まれる．\nこの挙動はoptions(calculus.auto.wrap=FALSE)で無効にできる．\n\n\n\n\n\n1.3 評価\n関数evaluateが提供されている．\n\nx &lt;- array(letters[1:6], dim=c(2,3))\nevaluate(x, var=c(a=1, b=2, c=3, d=4, e=5, f=6))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n関数evaluateはベクトル化されている：\n\nvar &lt;- data.frame(a=c(1,3), b=2:3, c=3:4, d=4:5, e=5:6, f=6:7)\nevaluate(x, var=var)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    2    3    4    5    6\n[2,]    3    3    4    5    6    7\n\n\n\n\n\n\n\n\nR Base の eval 関数との違い\n\n\n\n\n\neval関数では，１つの変数の代入しか行えない．\n複数与えられた場合でも，最後の１つのみ評価した結果が返される：\n\nvar_list &lt;- list(a=1, b=2, c=3, d=4, e=5, f=6)\neval(parse(text=x), envir=var_list)\n\n[1] 6\n\n\nevaluate関数と同様の結果を得るには，成分ごとに繰り返し適用する必要がある．例えば次のように：\n\napply(x, c(1, 2), function(expr) eval(parse(text=expr), envir=var_list))\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6"
  },
  {
    "objectID": "posts/2024/Computation/calculus.html#微積分",
    "href": "posts/2024/Computation/calculus.html#微積分",
    "title": "R による記号微分入門",
    "section": "2 微積分",
    "text": "2 微積分\n\n2.1 記号微分\n関数derivativeは関数を表す文字列fと，微分する変数名を表すvarの２つの引数を取る．\n\nderivative(f=\"sin(x)\", var=\"x\")\n\n[1] \"cos(x)\"\n\n\n多変数も同様．階数は引数orderで指定する： \\[\n\\frac{\\partial }{\\partial x}\\frac{\\partial ^2}{\\partial y^2}y^2\\sin(x)=2\\cos(x)\n\\]\n\nderivative(f = \"y^2 * sin(x)\", var = c(\"x\", \"y\"), order = c(1, 2))\n\n[1] \"2 * cos(x)\"\n\n\n\n\n2.2 比較\n\n2.2.1 基本的な構文の違い\n\\[\n\\frac{\\partial }{\\partial x}\\sin(x)\\bigg|_{x_0}\n\\]\nは次のように計算できる：\n\nsym &lt;- derivative(f=\"sin(x)\", var=c(x=0))\nnum &lt;- derivative(f=function(x) sin(x), var=c(x=0))\n\n\n\n出力\nresult &lt;- data.frame(\n  Method = c(\"Symbolic\", \"Numeric\"),\n  Value = c(sym, num)\n)\nprint(result)\n\n\n    Method Value\n1 Symbolic     1\n2  Numeric     1\n\n\n\n\n2.2.2 ４階微分での比較\n\\[\n\\frac{\\partial ^4}{\\partial x^4}\\sin(x)\\bigg|_{x=0}\n\\] は次のように計算できる：\n\nsym &lt;- derivative(f=\"sin(x)\", var=c(x=0), order=4)\nnum &lt;- derivative(f=function(x) sin(x), var=c(x=0), order=4)\n\n\n\n出力\nresult &lt;- data.frame(\n  Method = c(\"Symbolic\", \"Numeric\"),\n  Value = c(sym, num)\n)\nprint(result)\n\n\n    Method         Value\n1 Symbolic  0.000000e+00\n2  Numeric -9.767766e-12\n\n\n\n\n2.2.3 多変数での比較\n\\[\n\\frac{\\partial }{\\partial x}\\frac{\\partial ^2}{\\partial y^2}y^2\\sin(x)\\bigg|_{(x,y)=(0,0)}\n\\]\n\nf &lt;- function(x, y) y^2 * sin(x)\nderivative(f, var=c(x=0, y=0), order=c(1, 2), accuracy=6)\n\n[1] 2"
  },
  {
    "objectID": "posts/2024/Computation/R1.html",
    "href": "posts/2024/Computation/R1.html",
    "title": "R（１）基本文法",
    "section": "",
    "text": "全ては関数である．\n全てはベクトルであり，複雑な構造は dispatch の賜物である．"
  },
  {
    "objectID": "posts/2024/Computation/R1.html#基本",
    "href": "posts/2024/Computation/R1.html#基本",
    "title": "R（１）基本文法",
    "section": "1 基本",
    "text": "1 基本\n\n1.1 対話モード\nバッチ処理はR CMD BATCH file.zで行う．\n\nWhen a user types a command at the prompt (or when an expression is read from a file) the first thing that happens to it is that the command is transformed by the parser into an internal representation. The evaluator executes parsed R expressions and returns the value of the expression. All expressions have a value. This is the core of the language. – R Language Definition\n\n\nR：どこからでも対話モードで実行可能なのが R．\n[1]はすぐ隣の要素の index．ALGOL 系と違い，1から index する．\n\nベクトルは基本横で[1]という行の名前の後に表示される，行列の行は[1,]と indexing される．indexing の表示の違いでクラスの違いがわかる．\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- c(10, 20, 30, 40)\nx\n\n[1] 10 20 30 40\n\n\n\nmat &lt;- matrix(1:6, nrow = 2)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\n\n\n関数名直打ちは定義が帰ってくる．\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nsum\n\nfunction (..., na.rm = FALSE)  .Primitive(\"sum\")\n\n\n\n\n\n\n()も，直打ちも，generic function の神である print 関数を呼んでいる．\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- 42\nx\n\n[1] 42\n\n\n\n(x)\n\n[1] 42\n\n\n\n\n\n\nworking directory の概念がある．\n\ngetwd(), setwd()\n\n\n\n\n1.2 関数\n\nkeyword をつけて呼び出せるが，位置が正しければ省略可能\n\nf(arg1=value1, arg2=value2)\n\nR は 関数型言語 で，全ての実装は関数になっている．が，認知容易性のために予約語として実装された特殊文字を用いて，中置記法が使える．\n\n+などの二項演算子は”+”(1,2)として使える．\ninなどの Condition Flow の宣言っぽい演算子も実装は関数．\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\n`+`(1, 2)\n\n[1] 3\n\n\n\n1 %in% c(1, 2, 3)\n\n[1] TRUE\n\n\n\n\n\n\n関数適用の大原則：リサイクル\nlogical の==も，matrix(3,3,3)も recycle される．\n\n(1,2,3) == (2,3,4)はベクトルの同一性を表さず，(F,F,F)を返し，matrix(3,3,3)は３でrecycleされた3×3行列．\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nc(1, 2, 3) + c(4, 5)\n\nWarning in c(1, 2, 3) + c(4, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1] 5 7 7\n\n\n\nc(1, 2, 3) == c(2, 3, 4)\n\n[1] FALSE FALSE FALSE\n\n\n\nmatrix(3, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    3    3    3\n[2,]    3    3    3\n[3,]    3    3    3\n\n\n\n\n\n\n\n1.3 オブジェクト操作\n?Syntaxで演算の速さなどがわかる．\n\n代入\n\nfoo &lt;- objectまたはobject -&gt; foo\n&lt;&lt;-は再帰的代入\n\n関数の中でしか使われず，親 environment やグローバル環境の中からも右辺を探す．右辺が見つかったら再びそれを代入する．\n\n=は代入もできる\n\n\nthe operator = is only allowed at the top level (e.g., in the complete expression typed at the command prompt) or as one of the subexpressions in a braced list of expressions.\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nfoo &lt;- 1\n\nchange_foo &lt;- function() {\n  foo &lt;&lt;- foo + 1\n}\n\nchange_foo()\nprint(foo)\n\n[1] 2\n\n\n\n\n\n\nevaluetion\n\n(object)\n\nprint(object) と等価．console だと裸で良い．\n\n\n;\n\n命令の併記\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nx &lt;- 1; y &lt;- 2; z &lt;- x + y\nprint(z)\n\n[1] 3\n\n\n\n\n\n\n変数名に数字が使えるのいいな．\n\n語頭は数字にはできない．\n\n\n\n\n\n\n\n\n演算子まとめ\n\n\n\n\n\n\n\nR Language Definition\n\n\nOperator\nDescription\n\n\n\n\n-\nMinus, can be unary or binary\n\n\n+\nPlus, can be unary or binary\n\n\n!\nUnary not\n\n\n~\nTilde, used for model formulae, can be either unary or binary\n\n\n?\nHelp\n\n\n:\nSequence, binary (in model formulae: interaction)\n\n\n*\nMultiplication, binary\n\n\n/\nDivision, binary\n\n\n^\nExponentiation, binary\n\n\n%x%\nSpecial binary operators, x can be replaced by any valid name\n\n\n%%\nModulus, binary\n\n\n%/%\nInteger divide, binary\n\n\n%*%\nMatrix product, binary\n\n\n%o%\nOuter product, binary\n\n\n%x%\nKronecker product, binary\n\n\n%in%\nMatching operator, binary (in model formulae: nesting)\n\n\n%||%\nNull coalescing operator, binary\n\n\n&lt;\nLess than, binary\n\n\n&gt;\nGreater than, binary\n\n\n==\nEqual to, binary\n\n\n&gt;=\nGreater than or equal to, binary\n\n\n&lt;=\nLess than or equal to, binary\n\n\n&\nAnd, binary, vectorized\n\n\n&&\nAnd, binary, not vectorized\n\n\n|\nOr, binary, vectorized\n\n\n||\nOr, binary, not vectorized\n\n\n&lt;-\nLeft assignment, binary\n\n\n-&gt;\nRight assignment, binary\n\n\n$\nList subset, binary"
  },
  {
    "objectID": "posts/2024/Computation/R1.html#制御",
    "href": "posts/2024/Computation/R1.html#制御",
    "title": "R（１）基本文法",
    "section": "2 制御",
    "text": "2 制御\n\n{suite}：ブロック\nブロックは閉じるまで評価されない．\n\n{ x &lt;- 0\nx + 5\n}\n\n[1] 5\n\n\nif, else, else if：条件分岐\nif ( statement1 )\n    statement2\nelse\n    statement3\nfor, while, repeat：反復\nfor ( name in vector ) {\n    statement\n}\nwhile ( statement1 ) {\n    statement2\n}\nrepeat {\n    statement\n    if ( condition ) break\n}\nswitch (statement, list)：statementを評価した結果得る数値を用いて，listを indexing して（評価して）返す．\n\nswitchは関数として実装されているが，「評価」のステップがあるために制御構文として使える．\n\n\nx &lt;- 3\nswitch(x, 2+2, mean(1:10), rnorm(5))\n\n[1] -1.21239418 -0.42072586  0.93179085  0.70058355  0.03332041\n\n\n\nswitch(2, 2+2, mean(1:10), rnorm(5))\n\n[1] 5.5\n\n\n\n関数型言語なので，forもifもfunctionも関数だと思った方がいい．()と{}を取り，{}はブロックの意味しか持たず，改行を挟まないなら省略して良い．（C, C++, Python, PerlなどのALGOL系と全く同じ）．文は改行文字かセミコロンで区切る． なるべく制御（再帰構造）を避けるのが関数型言語としての目標になる．\nControl 多分if(cond)というのは引数だ．全ての関数は引数を離して書いても良い．$も同様． 改行を経ないexprの併記は;を用いる．{}は改行を超えてコンパイラにまとまりを通知する．インデントは視認性以外の意味はなさそう．\n\nif節\n\n構文：if(Cond) {expr} else {expr}\n\nCondとは長さ1のlogical vectorに評価可能なobject\nいずれの{}もなくても動くが，行を分けたいなら必要．document読む限りつけるのが推奨されている．\n\nifelse(test, yes, no)：testには条件式（logical型に変換可能なobject），yes/noには返すすべき文字列．\n\nifelseにはない．\n\n\nfor文\n\n構文：for(var in seq) expr\n\nseqは最初に評価される．\nlength(seq)=0ならすぐにloopが終わる\ninは特別なControl Flowのための予約語．\n\n\nwhile文\n\n構文：while(Cond) expr\n\nその他\n\nrepeat expr\nbreak：loopの外の最初の文に制御を移す\nnext\n\n\nvalueについて：全部静かに返り値を返している * ifはcondの評価の結果をlogicalを返す * for, while, repeatはNULLを返す * break, nextはloopの中で制御を移すので，返り値は持たない．"
  },
  {
    "objectID": "posts/2024/Computation/R1.html#関数定義",
    "href": "posts/2024/Computation/R1.html#関数定義",
    "title": "R（１）基本文法",
    "section": "3 関数定義",
    "text": "3 関数定義\nfunction object を返す constructor\nfunction(arglist) expr [return(value)] - arglistはname=exprの形のexpressionも許容． - exprの中に前の引数を入れてさえ良い． - 参照時，名前つき引数指定 option = value では、引数名 option は一意的に決定される限り、先頭の文字列だけを与えるだけで良い！！！何それwww - returnが関数とは．関数型言語 - valueもexpressionで良い． - returnを省略すると，最後になされた評価の値が返される．通常は実行時間が速くなるので省略されるが可読性は下がるかも． - valueを省略するとNULLが返される．\nリストを引数として受け入れたい * 特殊引数… * この後に定義された引数は，keyword付きで参照しないとエラーになる．"
  },
  {
    "objectID": "posts/2024/Computation/R3.html",
    "href": "posts/2024/Computation/R3.html",
    "title": "R（３）リスト",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/R3.html#はじめに",
    "href": "posts/2024/Computation/R3.html#はじめに",
    "title": "R（３）リスト",
    "section": "はじめに",
    "text": "はじめに\nR における行列 (matrix) は dim 属性を持ったベクトルであった．これと同様に，R におけるリストには names 属性があり，リストの各内容に名前をつける役割を持つ．そしてこの names 属性は$を使ってアクセスできる．\nR におけるすべてのオブジェクトは class 属性を持っている．S3 におけるすべてのオブジェクトは class 属性を持つリストとして実装されているためである．オブジェクトが関数に呼ばれた際は，このclass属性の値を確認して，適切なメソッドが呼ばれる．\n\n\n\n\n\n\n例\n\n\n\n\n\n\nexample_list &lt;- list(a=1,b=2)\nclass(example_list) &lt;- \"my_class\"\nexample_list\n\n$a\n[1] 1\n\n$b\n[1] 2\n\nattr(,\"class\")\n[1] \"my_class\"\n\n\n\nattributes(example_list)\n\n$names\n[1] \"a\" \"b\"\n\n$class\n[1] \"my_class\""
  },
  {
    "objectID": "posts/2024/Computation/R3.html#リスト",
    "href": "posts/2024/Computation/R3.html#リスト",
    "title": "R（３）リスト",
    "section": "1 リスト",
    "text": "1 リスト\nR.versionやhist()の返り値などは，breaks, countsオプションを持ったリストである．これはarrtibutes()あるいはstr()あるいは直うち，またはclass毎に誂えられた generic 関数であるprint()やsummary()で確認できる．\n\n\n\n\n\n\n例\n\n\n\n\n\n\nattributes(R.version)\n\n$names\n [1] \"platform\"       \"arch\"           \"os\"             \"system\"        \n [5] \"status\"         \"major\"          \"minor\"          \"year\"          \n [9] \"month\"          \"day\"            \"svn rev\"        \"language\"      \n[13] \"version.string\" \"nickname\"      \n\n$class\n[1] \"simple.list\"\n\n\n\nstr(R.version)\n\nList of 14\n $ platform      : chr \"aarch64-apple-darwin20\"\n $ arch          : chr \"aarch64\"\n $ os            : chr \"darwin20\"\n $ system        : chr \"aarch64, darwin20\"\n $ status        : chr \"\"\n $ major         : chr \"4\"\n $ minor         : chr \"4.0\"\n $ year          : chr \"2024\"\n $ month         : chr \"04\"\n $ day           : chr \"24\"\n $ svn rev       : chr \"86474\"\n $ language      : chr \"R\"\n $ version.string: chr \"R version 4.4.0 (2024-04-24)\"\n $ nickname      : chr \"Puppy Cup\"\n - attr(*, \"class\")= chr \"simple.list\"\n\n\n\nsummary(R.version)\n\n               Length Class  Mode     \nplatform       1      -none- character\narch           1      -none- character\nos             1      -none- character\nsystem         1      -none- character\nstatus         1      -none- character\nmajor          1      -none- character\nminor          1      -none- character\nyear           1      -none- character\nmonth          1      -none- character\nday            1      -none- character\nsvn rev        1      -none- character\nlanguage       1      -none- character\nversion.string 1      -none- character\nnickname       1      -none- character\n\n\n\n\n\n\n1.1 コンストラクタ\n\n構成\n\nlist(arg1,arg2,…)\nname=arg1とすると[[1]]ではなく$nameという名前付きで list にできる．\n\n名前\n\nnames(list) &lt;- c(“name1”,”name2”)：後からタグ付けで参照できる．\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nexample_list\n\n$a\n[1] 1\n\n$b\n[1] 2\n\nattr(,\"class\")\n[1] \"my_class\"\n\n\n\nexample_list[[1]]\n\n[1] 1\n\n\n\nexample_list[[a]]\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\n\nexample_list$a\n\n[1] 1\n\n\n\n\n\n\n\n1.2 参照方法\n\nL[[1]]またはL$name：要素の参照，次元が違うのに注意．\n\n[1]が行ベクトルを表す．それが[[1]], [[2]], ……と進んでく．\n\n$で参照した場合は，最初の数文字で予測できれば省略可能．\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nnames(example_list) &lt;- c(\"aaaaa\",\"bbbbb\")\n\n\nexample_list\n\n$aaaaa\n[1] 1\n\n$bbbbb\n[1] 2\n\nattr(,\"class\")\n[1] \"my_class\"\n\n\n\nexample_list$a\n\n[1] 1"
  },
  {
    "objectID": "posts/2024/Computation/R3.html#データフレーム",
    "href": "posts/2024/Computation/R3.html#データフレーム",
    "title": "R（３）リスト",
    "section": "2 データフレーム",
    "text": "2 データフレーム\n各要素がベクトルであるリストのこと．従って明らかに縦にベクトル構造を持つ．対称ではないのは数学と同じ．\n\n2.1 コンストラクタ\n\n構成\n\ndata.frame(…, colname=vector, …)\n\n列の名前をつけながら．行の名前のデフォルトは \\(\\mathbb{N}\\)\n行列を引かせても良い．\n\n\n名前\n\nrow.names(df) &lt;- vector：行に名前をつける．\nnames(df)：すでについている名前\n\ncolumn に時間や変数などの構造化されたデータが来がち．\n\n\n\n\n\n2.2 参照方法\n\n\n\n\n\n\n基本\n\n\n\n, 付きのオブジェクトで言及するとベクトルを得る．ベクトルオブジェクトで言及すると部分フレームを得る．（多分呼ばれてる関数が違う）\n\n\n\ndf[n]：\\(n\\) 列目を data-frame 列として得る\n\n行列同様，列に対して特別な扱いをしている．\n\ndf[n,]：\\(n\\) 行目を data-frame 行として得る．\ndf[,n]：\\(n\\) 列目を data-frame 行として得る．\nsubset(x)：切り出して使う．\ndf[-n]：除外\n\n論理値で指定\n\ndf$Name：列の名前で indexing して，必ずベクトルを得る．\n[,drop=TRUE]：が隠れているので，これをFALSEにして強制的にdfを返すことができる．\n列の２番目の index はc(1,5)の代わりにc(Month,Day)で参照してもよく，$の後と同様””で String 型にする必要はない．"
  },
  {
    "objectID": "posts/2024/Computation/R2.html",
    "href": "posts/2024/Computation/R2.html",
    "title": "R（２）ベクトル",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/R2.html#概観",
    "href": "posts/2024/Computation/R2.html#概観",
    "title": "R（２）ベクトル",
    "section": "1 概観",
    "text": "1 概観\n\n1.1 すべてはベクトルである\nデータ型 logical, integer, double, complex, character, raw は全てベクトルと理解される．1 逆に，全てのベクトルはこの６つのいずれかの atomic data type を持つ．2\n4.2, four point two はいずれもデータ型こそ違えど長さ１のベクトルである．\n\n\n1.2 他のデータ型との関係\n全てはベクトルで，行列も配列もリストも，それに特殊な属性を付与したもの．\n\n1.2.1 行列と配列\ndim属性を持つベクトルのことである．各次元に名前がつくとdimnames属性も持つ．\n\n\n1.2.2 リスト\nリストは各要素のデータ型が異なっていても良く，“generic vector” というべきものである．\n\n  \n    \n      \n      \n        R（３）リスト\n        R におけるリストは，独自の index `$` を持った構造体であり，Python の dictionary， Perl の hash table に似ている．`$` は S3 の機能で，S4 は `@` である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである． \n      \n    \n  \n\nベクトルは基本横で[1]という行の名前の後に表示される，行列はそのdim属性のために基本縦で[,1]と indexing される．indexing の表示の違いでクラスの違いがわかる．\nまずベクトルの時点で定義される演算を紹介し，インデックスを紹介する．全てのオブジェクトはベクトルなので，ベクトルをベクトルでインデックスするという奇怪な状況が発生する．また R では演算のほとんどがベクトル化されていて，これを使いこなすことが高速化の最初の手法になる． 次に行列を定義し，行列特有の演算を見る．"
  },
  {
    "objectID": "posts/2024/Computation/R2.html#ベクトル",
    "href": "posts/2024/Computation/R2.html#ベクトル",
    "title": "R（２）ベクトル",
    "section": "2 ベクトル",
    "text": "2 ベクトル\nスカラーなどの概念はない，全てはベクトル． 文字列も，character modeの長さ１ベクトル．\n\n2.1 構成\n\n基本的にはベクトルのためのコンストラクタc(arg1,arg2,...)を用いる．実は浮動小数点モードを生成する．\n\n引数もベクトルなので，本質は concatenation\nデータ型が強制される，これが list との違い．\n\na:b：a&lt;bならば増加，a&gt;bならば減少の，step=1のベクトルの生成．実は整数モードを生成する．\nseq(from=1,to=1,by=((to - from)/(length.out - 1)))\n\n上の方法だと1:0は[1] 1 0を返すので安全でない．\nseq(NULL)はNULLを返すので安全．\n\nrep(y,times=n,each,length.out)\n\ntimes：=3でn回繰り返す．\neach：=3とすると各要素を３回繰り返す．\nlength.out：=3とすると出力の長さを制限\n\n\n\n要素ごとに代入したい場合は宣言が必要（ベクトルを指すと判明していないポインタに向かって indexing をするy[2]という書き方は禁忌）．y &lt;- vector(length=n)としてポインタを作ってから，y[1]&lt;-5などと代入していく．\n\nベクトルの再割り当ては時間がかかるので，関数定義で返り値のためにベクトルの積み上げをしたい場合は，ret &lt;- vector(length=n)とした方がいい．返す前にret &lt;- ret[1:n]として未使用部分を無くすために再割り当てをして2回で済ませるのがいい．\n\n\n\n\n2.2 単項演算 \\(V\\to V\\)\n\n操作\n\nrev(x)：反転\nt(x)：転置\n\n表示\n\nhead(x[, n=6L])\ntail(x[, n=6L])\n\n\n\n\n2.3 index 付与：自分で index を変えられる．\n\nnames属性を付与できるが，リストにはならない．\n\nnames(x) &lt;- ：colnames か rownames か，どちらか適切な方．\nnames(x) &lt;- NULL：削除．\n\n\n\n\n2.4 単項演算 \\(V\\to K\\)：統計的特徴の抽出\n\n長さ：length(x)\n\n行列などに対しては，属性を外してベクトルと見て演算して，属性を戻すとかそういう感じ．\n\nmean(a, na.rm=F)：算術平均．na.rm=TでNAがあっても適用可能になる．\nsum(a)：総和\nprod(a)：総積\nmax(a)：\nsd(x)：standard deviation\n\n\n\n2.5 単項演算 \\(V\\to2\\)：条件\n\nall(expr)：expr の結果である logical vector が，全てTかどうかを判定して長さ１ベクトルを返す\nany(expr)：expr の結果である logical vector が，Tが存在するかを判定して長さ１ベクトルを返す\n\n\n\n2.6 indexing：ベクトルの構成法３つ:,c,seqに対応した indexing 法がある．\n\ny[c(1,3,4)]：scaler は退化したベクトルなのでこれがあるべき姿\ny[-c(1,3,4)]：除外は負数を用いる．y[-length(y)]は最後の要素の除外．y[-1:-2]は1,2番目の除外．\ntail(x,[n=]1)：右後ろから indexing\n\n\n\n2.7 数値演算 \\(V^2\\to V\\)\n\n同次元演算\n\na+b, a-b\na*b：Hadamard積，a/b\na %*% b：内積\n%o%：外積\n%/%：整数除算\na^b：冪で，**でも実行できるが非推奨．\n%%：mod．情報落ちを検出したら警告が出るようになっている．\n\nrecycle：長さがどちらかの定数倍である時，演算を繰り返す．\n\n\n\n2.8 文字列演算\n\npaste(“str1”, “str2”)：concatenation\nstrsplit(x, split, fixed = FALSE, perl = FALSE, useBytes = FALSE)\nベクトルは C の配列のように，連続的に格納されているので，要素の挿入や削除は新しく作るのと等価．実装は C の pointer と同じで，再代入はポインタの指す先を変えることで再割り当てを実現する．"
  },
  {
    "objectID": "posts/2024/Computation/R2.html#行列",
    "href": "posts/2024/Computation/R2.html#行列",
    "title": "R（２）ベクトル",
    "section": "3 行列",
    "text": "3 行列\ndim属性（ncolとnrow）のついたベクトルで，縦に並んだベクトルをdim属性を参照しながら横にずらしながら構成される．\n\n3.1 構成\n\nベクトルへの属性付与による構成\n\nベクトルにdim属性を付与するコンストラクタmatrix([data=]a,[nrow=]m,[ncol=]n)\n\naが scaler の場合m×nを a-fill する，aがベクトルの場合m×nに fill していく．\nfill の順番は列ごとで，byrow=TRUEとすると行ごとになる．\nこの逆変換はas.vector(A)\n\n\nベクトルの結合による構成\n\nrbind(a,b,c)：行ベクトルとして結合，あるいは行列の横結合．\ncbind(a,b,c)：列ベクトルとして結合，あるいは行列の縦結合．\ndim(x) &lt;- c(n,m)：ベクトルに次元のデータを加えて，行列に変換する．\n\n\n\nメモリ確保はmatrix(nrow=a,ncol=b)でなされる．\n\n\n\n3.2 操作\n\nrownames(), colnames() &lt;- name：名前をつけられる．\nt(A)：転置\n\n\n\n3.3 indexing 周り\n\ndim(A)：返り値は長さ2のベクトル\nnrow(A)=dim(A)[1]\nncol(A)=dim(A)[2]\nA[行,列]：空欄を渡すと走る，長さ2以上のベクトルを渡すと複数選択\n\n[,2]などでも行ベクトルで帰ってくる．これは全てのベクトルは横で表示されるから．\n[n]だと，列を繋げた順序についての indexing で，要素が返ってくる．\n\n抽出した部分行列に値が部分代入できる．\n\n\n\n3.4 一項演算\n\nノルム\n\nnorm(A [,type=c(“o”, ”I”, ”F”, ”M”, ”2”)])\n\none, infinity, flobenius = Eucleadean as a vector, maximum modulus, 2はspectral norm．\n\\(l^2\\)-ノルムはsqrt(a %*% a)::matrixでいける\n\n\n正方行列\n\ndet(A)\nsum(diag(A))：\\(\\operatorname{Tr}\\)\n\n\n\n\n3.5 二項演算\n\n同次元演算\n\nA+B, A-B\nA*B, A/B：Hadamard積\nA %*% B：行列積\n\n逆行列solve(A)\n一般化逆行列 (Moore-Penrose pseudoinverse)\nlibrary(MASS)\nginv(A)"
  },
  {
    "objectID": "posts/2024/Computation/R2.html#generic-operator",
    "href": "posts/2024/Computation/R2.html#generic-operator",
    "title": "R（２）ベクトル",
    "section": "4 generic operator",
    "text": "4 generic operator\n行列はベクトルなので，ほとんどの関数はベクトルだと思って作用する．属性を外してベクトルと見て演算して，属性を戻すとかそういう感じ．\n\n4.1 ベクトルと行列で挙動が違う generic 単項関数 \\(K^n\\to V\\)\n\ndiag(x)：引数がベクトルなら対角行列の作成．diag(rep(1,n))=En．行列ならベクトルを返す．\n\n\n\n4.2 ベクトルも行列も同等に扱う二項演算\n\n+, -：長さが整数倍ならば，repしてから足す．\nHadamard積 *：scaler と行列ならば普段の積．%*%ではエラーになる．\n%*%：これは数学でも generic よね．\n\nxA=行ベクトル，Ax=列ベクトルと返り値が定まり，xの縦横に依らない．\n\nsolve(A,B)：Bがベクトル \\(b\\) の時，\\(Ax=b\\) を解く．行列のときは \\(AX=B\\) を解く．\n\nBを省略すると対角行列とみなされ，したがって解は \\(X=A^{-1}\\)\n\n\n\n\n4.3 数学関数はベクトル化されている．\n\nsin(), exp(), abs(), sqrt(), log(), log10(), acos(), sinpi()\n\n1:10*2は(1:10)*2だが，1:10^2は1:100\nlog([x=]b,[base=exp(1)])\nacos：逆三角関数\nsinpi(3/2)：sin(pi*3/2)に同じ\n\nround(), floor(), ceiling()\nfactorial()\nx&gt;3などの評価：条件評価も実際は関数．\n\n3が recycle されてxと同じ長さになる．\npoint-wise に評価が行われて，\nlogical のベクトルを返す．\n\n複数のベクトルを値に取る関数\n\npmax()\ncumsum(), cumprod()\n\n\n\n\n4.4 関数のベクトル化\n\nsapply(x,f)\n\nfをxの各要素に適用し，返り値を行列とする．\nfがベクトルの長さを伸ばすような関数だった場合，行列化処理と合成するのが自然で，これを勝手にやってくれる．"
  },
  {
    "objectID": "posts/2024/Computation/R2.html#配列",
    "href": "posts/2024/Computation/R2.html#配列",
    "title": "R（２）ベクトル",
    "section": "5 配列",
    "text": "5 配列\n\\(n\\) 次元リストで，一番一般的なデータ構造．\n\n構成\n\narray(data=NA, dim=length(data), dimnames=NULL)\n\ndim=c(3,4,2)などができる．\ndimnamesはもともと数字で，,,1という調子で切り出して表示される（list みたいな）"
  },
  {
    "objectID": "posts/2024/Computation/R2.html#footnotes",
    "href": "posts/2024/Computation/R2.html#footnotes",
    "title": "R（２）ベクトル",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR Language Definition. 2. Objects↩︎\nR Language Definition. 2.1.1 Vectors↩︎"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html",
    "href": "posts/2024/Computation/MCMC.html",
    "title": "新時代の MCMC を迎えるために",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#導入",
    "href": "posts/2024/Computation/MCMC.html#導入",
    "title": "新時代の MCMC を迎えるために",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 MCMC 小史\n現状，HMC (Hamiltonian Monte Carlo) という約 40 年前に提案された MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．1\nこの手法はもともと (Duane et al., 1987) が場の量子論に特化した Monte Carlo 法として提案したものであったところを，(Neal, 1994) が一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．\nということで，HMC は，オリジナルの MCMC が物理学者 (Metropolis et al., 1953) に由るように，物理学において着想された MCMC 手法であったのです．\nそのHMC が，提案から 40 年目を迎える前に，更なる効率的な手法によって代替されようとしています．\nそのきっかけ (Peters and de With, 2012) も，やはり，物理学（正確には物質科学）からの着想でした．\n\n\n1.2 MCMC とは何か？\nMCMC とは，確率変数をシミュレーションする際に用いられる汎用的アルゴリズムです．\n一様分布や正規分布などの名前がついた分布ではない場合，どのようにすればその分布に従う確率変数をシミュレーションできるのか？は，古くからの問題でした．\n実際，「MCMC では空間を探索するマルコフ連鎖を構成し，その足跡を辿るとちょうど確率変数のシミュレーションになっている」と種明かしを聞いても，「なぜそのような回りくどい方法を使うのか？」「もっと良い方法はないのか？」と思っても当然でしょう．\nですが，MCMC を，発明された経緯を辿り，物理学の問題意識から見てみると，実は極めて自然な発想に思えてくるかもしれません．\n以降，MCMC の起源である物理系のシミュレーション（第 2 節）を例に取り，分子動力学法（第 2.1 節），Metropolis 法（第 2.2 節）を復習します．\n\n\n\n\n\n\n\n\n\n分子動力学法の出力（第 2.1 節）2\n\n\n\n\n\n\n\nMetropolis 法の出力（第 2.2 節）\n\n\n\n\n\nこれを基礎として，近年提案された非対称な MCMC 手法（第 3 章），そして最新の連続時間 MCMC 手法（第 4 章）を紹介します．\n\n\n\n\n\n\n\n\n\n非対称 MCMC 法（Lifted Metropolis 法）の出力（第 3 章）\n\n\n\n\n\n\n\n連続時間 MCMC 法（Zig-Zag サンプラー）の出力（第 4 章）\n\n\n\n\n\n\n\n1.3 自己相関・軌跡の一覧\n\n\n\n\n\n\n\n\n\nMetropolis 法の自己相関関数（第 2.2 節）\n\n\n\n\n\n\n\n非対称 MCMC 法（Lifted Metropolis 法）の自己相関関数（第 3 章）\n\n\n\n\n\n\n\n連続時間 MCMC 法（Zig-Zag サンプラー）の自己相関関数（第 4 章）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis 法の軌跡（第 2.2 節）\n\n\n\n\n\n\n\n非対称 MCMC 法（Lifted Metropolis 法）の軌跡（第 3 章）\n\n\n\n\n\n\n\n連続時間 MCMC 法（Zig-Zag サンプラー）の軌跡（第 4 章）"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#sec-origin",
    "href": "posts/2024/Computation/MCMC.html#sec-origin",
    "title": "新時代の MCMC を迎えるために",
    "section": "2 MCMC の起源",
    "text": "2 MCMC の起源\n\n\n\n\n\n\nよりみち：どうして MCMC が必要だったのか？\n\n\n\n\n\n(Metropolis et al., 1953) では，温度 \\(T\\) 一定の条件下で \\(N\\) 粒子系をシミュレートし，任意の物理量 \\(F\\) に対してその相空間上の平均 \\[\n\\langle F\\rangle=\\frac{\\int Fe^{-\\frac{E}{kT}}dp}{\\int e^{-\\frac{E}{kT}}dp}\n\\] を効率的に計算する汎用アルゴリズムが提案された．これが現在では Metropolis 法と呼ばれている．3\n(Metropolis et al., 1953) では \\(N\\) が数百になる場合を考えており（時代を感じるスケール感），当然愚直な数値積分は現代の計算機でも実行可能ではない．そこで Monte Carlo 法を考えることになるが，当時 Monte Carlo 法といえば，一様乱数を用いた計算法の全般を指し，具体的には \\(\\langle F\\rangle\\) を重点サンプリング推定量 \\[\n\\widehat{F}=\\frac{\\sum_{n=1}^NF(\\omega)e^{-\\frac{E(\\omega)}{kT}}}{\\sum_{n=1}^Ne^{-\\frac{E(\\omega)}{kT}}}\n\\] で推定することを指した．4\nしかしこれでは，高エネルギーな状態・低エネルギーな状態を全く区別せず，状態 \\(\\omega\\in\\Omega\\) を完全に一様に生成するため，その分だけ非効率である．\nこれを低減することが出来れば Monte Carlo 法の更なる効率改善に繋がる．こうして，Gibbs 分布 \\(\\frac{1}{Z}e^{-\\frac{E}{kT}}\\) から直接的サンプリングする方法が模索されたのである．\n\n\n\n(Metropolis et al., 1953) では，エネルギー \\(E\\) を持つ系の Boltzmann-Gibbs 分布 \\(\\frac{1}{Z}e^{-\\frac{E}{kT}}\\) から直接サンプリングする方法が探求されました．\nここでは簡単のため，１粒子が次のようなポテンシャル \\[\nU(x)=\\frac{x^2}{2}+\\frac{x^4}{4}\n\\] に従って運動する場合を考えましょう：\n\n\n\n\n\n\n図 1: ポテンシャル \\(U\\) のプロット\n\n\n\nこのポテンシャルに関する Boltzmann-Gibbs 分布 \\(\\pi\\,\\propto\\,e^{-\\beta U}\\) は次のような形になります：5\n\n\n\n\n\n\n図 2: ポテンシャル \\(U\\) が定める Botlzmann-Gibbs 分布のプロット\n\n\n\n\n２つのプロットを見比べると，低エネルギー状態ほど出現確率が高く，エネルギーが上がるにつれて急激に出現確率が下がることがわかります．以降，\\(\\beta=1\\) としましょう．6\n\n2.1 分子動力学法\n統計力学によれば，\\(\\beta=1\\) で定まる温度とポテンシャル \\(U\\) を持つ Boltzmann-Gibbs 分布 \\(e^{-U}\\) は，温度 \\(T=\\frac{1}{k_B\\beta}\\) を持つ熱浴に接している力学系を，長時間シミュレーションして時間平均を取ることでサンプリングできるはずです．\nこのように，力学に基づいて物理過程を数値シミュレーションをすることを通じてサンプリングを達成する方法を 分子動力学法 といいます．\nこれを実際にやってみます．図 1 で定めたポテンシャルを持つ粒子を考えます．7\n続いてこれを温度 \\(T=\\frac{1}{k_B\\beta}\\) を持つ熱浴と相互作用させます．例えば，ポテンシャル 1 の \\(x=0\\) の位置に半透性の壁を置き，確率 \\(1/2\\) でこの温度 \\(T\\) の壁の粒子と弾性衝突するとします．（残りの確率 \\(1/2\\) では衝突せずに通過する）．\n壁の粒子の速度は Maxwell の境界条件から与えられるものとすれば，次のようにして粒子の位置 \\(x\\) がシミュレートできます：8\n\n\nCode\nimport numpy as np\n\nnp.random.seed(2024)\n\ndef U(x):\n    return x**2/2 + x**4/4\n\ndef pi(x):\n    return np.exp(-U(x))\n\ndef md(num_samples, initial_state, initial_velocity, timestep=0.1):\n    samples = [initial_state]\n    current_state = initial_state\n    current_velocity = initial_velocity\n    current_time = 0\n\n    for _ in range(num_samples - 1):\n        proposed_state = current_state + current_velocity * timestep\n        current_time += timestep\n        if current_state * proposed_state &lt; 0 and np.random.rand() &lt; 1/2:\n            current_velocity = (-1) * np.sign(current_velocity) * np.sqrt(-2 * np.log(np.random.rand() + 1e-7))\n        else:\n            current_velocity = current_velocity - ( current_state + current_state ** 3 ) * timestep\n            current_state = proposed_state\n        samples.append(current_state)\n\n    return np.array(samples)\n\n# サンプル数と初期条件を固定\nnum_samples = 10000\ninitial_state = 0.0\ninitial_velocity = 1.0\n\nsamples_MD = md(num_samples * 10, initial_state, initial_velocity, timestep=0.01)\n\n\n\n\n\n\n\n\n\n\n図 3: 分子動力学法からのサンプル\n\n\n\n\n\nこの方法は極めて収束が遅く，イテレーション数を \\(10^6\\) 以上に取らないと目標分布 \\(e^{-U}\\) の良い近似とならないことを思い知りました（上図も \\(10^6\\) サンプルで生成しています）．なお，以降の MCMC 法ではいずれもイテレーション数は一桁少ない \\(10^5\\) としています．\n\n\n\n\n\n\n\n\n\nたしかに，目標分布 \\(e^{-U}\\) に収束しそうですね．\n\n\n2.2 Metropolis 法\nもちろん，分布 \\(e^{-U}\\) をサンプリングするために，必ずしも背景にある物理過程まで戻ってシミュレーションをする必要はありません．\nそこで，シミュレーションは簡単なランダムウォークで行い，その結果を適切に修正することで目標分布に収束させる方法が (Metropolis et al., 1953) で考えられました．\n(Metropolis et al., 1953) の手法は，現在では random walk Metropolis-Hastings 法と呼ばれます．\nこの背後の物理現象から離陸する一歩が，分子動力学法と MCMC 法とを分けるものでした．\n\n\nCode\ndef metropolis(num_samples, initial_state, verbose=False):\n    samples = [initial_state]\n    current_state = initial_state\n\n    accept = []\n\n    for _ in range(num_samples - 1):\n        proposed_state = current_state + np.random.uniform(-2,2)\n        acceptance_ratio = pi(proposed_state) / pi(current_state)\n        if np.random.rand() &lt; acceptance_ratio:\n            current_state = proposed_state\n            accept.append(True)\n        samples.append(current_state)\n\n    if verbose:\n        rate = len(accept) / num_samples\n        print(f'acceptance rate : {rate}')\n\n    return np.array(samples)\n\n\n\n\n\n\n\n\n\n\n図 4: Metropolis 法からのサンプル\n\n\n\n\n\nサンプル数は分子動力学法の \\(1/10\\) であるにも拘らず，目標分布 \\(e^{-U}\\) の良い近似を得ています．\n一般に，MCMC からのサンプルの質の良さは，自己相関関数 を見ることで評価できます．9\nMetropolis 法の自己相関関数を計算してみると，横軸の Lag が大きくなればなるほど Autocorrelation の値は小さくなっています．\n\n\n\n\n\n\n\n\n図 5: Metropolis 法の自己相関関数\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 6: Metropolis 法の軌跡\n\n\n\n\n\n上図は Metropolis 法で構成される Markov 連鎖の軌跡を表しています．行ったり来たりしているのがわかります．棄却率は５割弱です．10\n\n\n2.3 統計学への応用\nこうして MCMC が発明されれば，すぐにイノベーションとして理解されたかというとそうではありませんでした．\nこの Metropolis の手法が極めて賢いシミュレーション手法であることは一目瞭然でも，一般の確率分布からのサンプリングに使える汎用アルゴリズムになっているという抽象的な観点が得られるまでには時間を要しました．これを成し遂げたのが (Hastings, 1970) でした．11\nさらに，Hastings のこの結果も見過ごされたと言って良いでしょう．真にMCMC を統計学界隈に広め，現代におけるベイズ統計学の興隆の契機となったのは階層モデリングにおける Gibbs サンプリングの有用性を強調した (Gelfand and Smith, 1990) だと言われます．12\n当時，代替手法としては複雑な数値アルゴリズムしかなかったベイズ統計学において，MCMC は汎用的で実装も容易であることが周知され，ベイズ統計学が普及するきっかけとなりました．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#sec-LMH",
    "href": "posts/2024/Computation/MCMC.html#sec-LMH",
    "title": "新時代の MCMC を迎えるために",
    "section": "3 非対称化への試み",
    "text": "3 非対称化への試み\n\n3.1 対称性という制約\nここでもう一度 Metropolis 法の軌跡 図 6 を見てみましょう．\n\n\n\n図 6 Metropolis 法の軌跡\n\n\n最初の 50 サンプルしか表示していませんから，運が悪いとうまく見つからないかもしれませんが，「一度歩んだルートを，その後すぐに逆に戻ってしまう」という事象が発生しやすいことが観察できますでしょうか？\nこれを 対称性 (reversibility) または 可逆性 と言います．\nMetropolis 法は構成上，この対称性を持つことが必要ですが，対称であるが故に一箇所に長時間とどまってしまうことが多くなります．\nその結果，対象分布が複雑で多峰性を持つ場合は，もっといろんなモード（峰）からもサンプリングをしてほしいのに，長時間１つの峰から離れられずにいることがあります．\nコーヒーに砂糖を溶かすことを考えてみましょう．砂糖の粒が拡散するのに任せておくと，最終的には均一に溶けるでしょうが，莫大な時間がかかります．スプーンで混ぜるなどして，砂糖が元の場所にとどまらずに移動し続けるようにすれば，はるかに速く平衡状態に到達できるでしょう．\nこれが 非対称化 のアイデアです．数ある Metropolis 法の改良の方向の中でも，この対称性を破るという試みは特に注目されてきました．\n\n\n3.2 リフティング\nMetropolis 法を非対称化するアプローチに，リフティング (Chen et al., 1999) と呼ばれる方法があります．\nこれは，元々の状態空間を２つの「モード」\\(+1\\) と \\(-1\\) に分裂させ，\\(+1\\) のモードではひたすら右側に，\\(-1\\) のモードではひたすら左側に移動するようする方法です．\n２つのモード \\(+1,-1\\) の間を遷移する確率を調整することで，最終的な不変分布は変わらないようにすることができます．\nこうすることで，対称性を破り，一度「この方向に行く！」と決めたら行き続けるようにしながら，収束先は変わらないように変更することが出来るのです．\n実際に Metropolis 法に適用した Lifted Metropolis-Hastings 法 (Turitsyn et al., 2011) を実装してみましょう：\n\n\nCode\ndef lifted_metropolis(num_samples, initial_state, verbose=False):\n    samples = [initial_state]\n    current_state = initial_state\n    lifting_variable = 1\n    accept = []\n\n    for _ in range(num_samples - 1):\n        delta = np.random.uniform(0,2)\n        proposed_state = current_state + lifting_variable * delta\n        acceptance_ratio = pi(proposed_state) / pi(current_state)\n\n        if np.random.rand() &lt; acceptance_ratio:\n            current_state = proposed_state\n            accept.append(True)\n        else:\n            lifting_variable = (-1) * lifting_variable\n\n        samples.append(current_state)\n    \n    if verbose:\n        rate = len(accept) / num_samples\n        print(f'acceptance rate : {rate}')\n\n    return np.array(samples)\n\n\n新しく追加されたリフティング変数 \\(\\sigma\\in\\{\\pm1\\}\\) に依存して，\\(\\sigma=+1\\) の場合には右方向に，\\(\\sigma=-1\\) の場合は左方向にしか提案を出さない MH 法と見れます．\n\n\n\n\n\n\n\n\n図 7: 非対称 Metropolis 法からのサンプル\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 8: 非対称 Metropolis 法の自己相関関数\n\n\n\n\n\n自己相関関数を見ると，Metropolis 法よりも急速に減衰していることがわかります．むしろ，過減衰のように自己相関関数が負になっていることもあります．\nこれは，一度「この方向に行く！」と決めたら行き続けるように設計したために，正の値が出たしばらくあとは負の値が，負の値が出たしばらくあとは正の値が出やすいようになってしまっているためです．\nしたがってこれは１次元の分布を考えていることに起因するため，殊更問題とすべきではないでしょう．\n\n\n\n\n\n\n\n\n図 9: 非対称 Metropolis 法の軌跡\n\n\n\n\n\n\n\n3.3 リフティングの有用性\n今回のような単純なポテンシャル \\(U\\) （図 1） だけでなく，統計力学における磁性体のモデルである Curie-Weiss モデルのハミルトニアン\n\\[\nH_n(x)=-\\frac{d\\beta}{2n}\\sum_{i,j=1}^nx_ix_j-h\\sum_{i=1}^nx_i,\n\\] \\[\nh\\in\\mathbb{R},x\\in\\{\\pm1\\}^n,\\quad n,d=1,2,\\cdots\n\\]\nが定める Boltzmann-Gibbs 分布 \\(e^{-H}\\) に対する Lifted Metropolis-Hastings も，単純な Metropolis-Hastings 法よりも効率的であることが知られています．13\n具体的には，モデルのパラメータ数 \\(n\\) に対して，緩和時間を \\(\\sqrt{n}\\) のオーダーだけ改善することが，(Turitsyn et al., 2011) では数値実験で，(Bierkens and Roberts, 2017) では理論的に検証されているのです．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#sec-PDMP",
    "href": "posts/2024/Computation/MCMC.html#sec-PDMP",
    "title": "新時代の MCMC を迎えるために",
    "section": "4 新たな MCMC",
    "text": "4 新たな MCMC\n\n\n4.1 背後の物理現象からの更なる離陸\n第 2 章で，分子動力学法（第 2.1 節）から，提案分布を背後の物理現象とは全く関係ないランダムウォークとすることで，Metropolis 法（第 2.2 節）は一気に効率的なサンプリング法となったことを見ました．\nしかし，Metropolis 法はまだ思考が物理に引っ張られているのかも知れません．平衡統計力学において，ミクロの状態は等価で，ミクロなダイナミクスは可逆と考えられます．その前提が，知らず知らずのうちにまだ埋め込まれたままだと言えるでしょう．\nそこで，スプーンでかき混ぜるように，遷移を非対称にすることで，より効率的なサンプリング法となることを前章 3 で見ました．\nここでは，さらに暗黙の思い込みから解き放たれようとします．それは，シミュレーションするにあたって，必ずしも離散時間ステップに囚われる必要はない ということです．\nもう一度，Lifted Metropolis-Hastings 法の軌跡を見てみましょう：\n\n\n\n図 9 非対称 Metropolis 法の軌跡\n\n\nこの軌跡から得られる情報のほとんどは，「どこで折り返したか？」です．\nですから，この軌跡をシミュレーションするにあたって，一歩一歩採択-棄却のステップを繰り返す必要はなく，「どこで折り返すか？」を先に計算できてしまえば，あとは好きなステップサイズで切り出してサンプルとすれば良いのです．\n実は，「折り返す地点だけを効率的に計算する」ことが可能であり，それが 連続時間 MCMC のアイデアです．\n\n\n4.2 連続時間 MCMC\nLifted Metropolis-Hastings の適切な連続時間極限 \\(\\Delta t\\to0\\) を考えることで，「折り返す」という事象（が起こった回数）は Poisson 過程に従うことが導けます．\nすると，「折り返す」事象が起こるまでの待ち時間 (interarrival time) は指数分布に従うことがわかります．これに基づいて，「折り返す」事象が起こる時刻を計算し，そこまでの軌跡を直線で補間すれば，Lifted Metropolis-Hastings 法（の連続時間極限）の軌跡が模倣できることになります．\n最終的に得られる過程は，ランダムな時刻に「折り返す」事象が起こり，その間は確定的な動き（等速直線運動）をするというもので，このような過程を 区分確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) と呼びます．\nこのような PDMP は，Lifted Metropolis-Hastings 以外にも種々の MCMC 法の極限から見つかっており，その中でも特に有名なのが次の Zig-Zag sampler です：\n\n\nCode\nimport math\n\ndef zigzag(num_samples, initial_state, step=1):\n    samples = [initial_state]\n    trajectory = [initial_state]\n    current_state = initial_state\n    lifting_variable = 1\n    t = 0\n\n    while t &lt; num_samples * step:\n        state_event = lifting_variable * np.sqrt(-1 + np.sqrt( 1 - 4 * np.log(np.random.rand()) ))\n        t_event = t + np.abs(state_event - current_state)\n        for _ in np.arange(np.ceil(t/step)*step, np.ceil(t_event/step)*step, step):\n              samples.append(current_state + lifting_variable * (_ - t))\n        current_state = state_event\n        trajectory.append(current_state)\n        lifting_variable = (-1) * lifting_variable\n        t = t_event\n\n    return np.array(samples), np.array(trajectory)\n\n\n\n\n\nCode\nsamples_zigzag, trajectory_zigzag = zigzag(num_samples, initial_state, step=2)\nplt.figure(figsize=(3.5, 3))\nplt.hist(samples_zigzag, bins=50, density=True, alpha=0.7, color=minty)\nplt.show()\n\n\n\n\n\n\n\n\n図 10: Zig-Zag サンプラーからのサンプル\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 11: Zig-Zag サンプラーの自己相関関数\n\n\n\n\n\n自己相関関数は，Lifted Metropolis-Hastings 法と同様に急激に下がって負の値に突き抜けたあとは，少し振動が残っているのがわかります．\n全３サンプラーの比較は第 1.3 節をご覧ください．\n次の軌跡を見て分かる通り，モードである \\(x=0\\) を中心に激しく往復するので，直後のサンプルとは負の相関が出やすいようです．\n\n\n\n\n\n\n\n\n図 12: Zig-Zag サンプラーの軌跡\n\n\n\n\n\n連続時間極限 \\(\\Delta t\\to0\\) をとっているということは，「極めて小さいステップサイズでの random walk Metropolis 法（第 2.2 節）」に相当します．従って，一度折り返したら，原点 \\(x=0\\) を超えるまでは絶対に棄却されません．\nそのため，このように往復するような軌跡が得られます．\n\n\n4.3 連続時間 MCMC の美点\n前節では，必ずしも PDMP 法である Zig-Zag sampler が，Lifted Metropolis-Hastings 法より，自己相関関数の観点で良いとは言い切れないことを見ました．\nしかし，今回の設定は１次元という特殊な条件下であることを考慮に入れる必要があります．\n１次元なので Zig-Zag サンプラーは行き来することしか出来ていませんが，14 ２次元以上，特に高次元の場合は，Zig-Zag サンプラーは極めて効率的に状態空間を探索できることが期待されます．\n例えば，標準正規分布に対する２次元での軌跡は次の通りです：\n\n\n\n\n\n\n図 13: Zig-Zag Sampler in \\(\\mathbb{R}^2\\)\n\n\n\nそして何より，軌道が効率的な空間の探索に有利であるだけでなく，正確なサブサンプリングを取り入れることが可能です．すなわち，ほとんどの他手法と違って，バイアスを導入することなく，データの一部のみを用いてアルゴリズムを走らせることができます．\nしたがって，従来の MCMC 法が採択-棄却のステップにおいて尤度を評価する必要があり，データサイズ \\(n\\) に対して \\(O(n)\\) の計算量を要するのに対して，\\(O(n)\\) に比例する焼き入れのステップを除けば，\\(O(1)\\) の複雑でほとんど i.i.d. なサンプルを得ることができます (Bierkens et al., 2019)．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#終わりに",
    "href": "posts/2024/Computation/MCMC.html#終わりに",
    "title": "新時代の MCMC を迎えるために",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nこうみると，MCMC は物理学の問題意識から生まれた手法でありながら，背後の物理現象を模倣することから離れていくことで，計算手法としての効率を高めていく一途を辿っていることがわかります．\nそう見ると，新時代の大規模データと大規模モデルが課す MCMC の次なる脱皮は，連続時間 MCMC で間違いないような気がしてくるのですが……．まだ筆者にははっきりとは見えてきません．\nまた本稿では１つの流れしか取り上げておらず，例えば HMC とその非対称化がどのような位置づけにあるかもまだ考慮中です．\n統計力学，統計学，機械学習が交差するなんとも面白いテーマです．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#参考文献",
    "href": "posts/2024/Computation/MCMC.html#参考文献",
    "title": "新時代の MCMC を迎えるために",
    "section": "6 参考文献",
    "text": "6 参考文献\n\n本稿では，用いたコードの導出を一切触れませんでした．これについては，文献 (Tartero and Krauth, 2023) をご参照ください．非調和振動子を系にとり，正準集団とみなすことで，分子動力学法，メトロポリス法からそのリフティングまで，種々のサンプラーを同じ題材で比較するアイデアをもらいました．こんなにわかりやすい解析ができるのかと心底驚きました．\n続いて，Metropolis-Hastings 法 → 非対称 MCMC → 連続時間 MCMC という発展の過程を，背後の物理過程の模倣からの離陸という視点で統一的に捉えることが出来るということは，(Turitsyn et al., 2011) の魅力的なイントロダクションで気付かされました．本文献はリフティングによる MCMC の非可逆化を抽象的に定式化して数値実験で検証したものであり，「ねじれ詳細釣り合い条件」を導入した点で，アイデアの宝庫といえます．\nリフティングによる Metropolis 法の非対称化について，(酒井佑士, 2017) は貴重な日本語文献です．当該文献の第３章（の第２節）にここで紹介した内容が詳しくまとめられています．\n第 4.3 節で紹介しましたように，Zig-Zag サンプラーを用いたサンプリングではそのスケーラビリティが魅力です．この点については，Zig-Zag サンプラーが提案された論文 (Bierkens et al., 2019) でも，前面に押し出して解説されています．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#footnotes",
    "href": "posts/2024/Computation/MCMC.html#footnotes",
    "title": "新時代の MCMC を迎えるために",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHamiltonian Monte Carlo の名称は (Neal, 2011) からで，元々は Hybrid Monte Carlo と呼ばれていました．分子動力学法 (Molecular Dynamics) と MCMC のハイブリッド，という意味でした．Stan で実装されている MCMC アルゴリズムについては こちら を参照．↩︎\n特に収束が遅く，他の手法と比べてイテレーション数を 10　倍にしています．↩︎\n統計学界隈では (Hastings, 1970) を入れて，Metropolis-Hastings 法とも呼ばれる．↩︎\nただし，配置 \\(\\omega\\in\\Omega\\) は空間内にランダム（一様）に粒子 \\(N\\) 個を配置することで生成することとする．↩︎\n１粒子系なので相互作用はなく，\\(E=U\\)．↩︎\n\\(\\beta=1\\) と約束することは，系の温度を \\(T=k_B^{-1}\\) に固定することにあたります．↩︎\n図 1 で定めたポテンシャルを持つ力学系には，代表的なものは（非調和）振動子や，あるいは \\(U\\) の形をした谷を行ったり来たりするボールを考えても構いません．↩︎\n詳しい議論は (Tartero and Krauth, 2023) をご参照ください．大変教育的な入門です．↩︎\n自己相関関数が大きいほど，その Markov 連鎖を用いて構成した Monte Carlo 推定量の漸近分散が大きくなります．加えて，自己相関関数の裾が重すぎると，例えエルゴード性を持っており大数の法則が成り立とうとも，中心極限定理が成り立たなくなります．換言すれば，\\(n^{-1/2}\\) よりも遅い収束レートになってしまいます．↩︎\n一般には，ランダムウォーク MH 法において，採択率を \\(0.2\\le\\alpha\\le0.4\\) 前後に抑えるのが良いとされています (Roberts et al., 1997)．これは状態空間の次元が無限に漸近する設定下での，漸近論的な結果ですが，低次元の場合でも極めて良い指標になることが (Gelman et al., 1996) で実証されています．また今回も，１次元であるにも拘らず，たしかに棄却率が半分を越さないほうが，自己相関が小さくなる傾向が確認されました．しかし今回は対象分布の裾が極めて軽いので，あまり大きなムーブは要らず，ステップサイズの最大値を \\(2\\)，採択率は \\(0.5\\) 近くにしました．他の手法，LMH と Zig-Zag もステップサイズの最大値が \\(2\\) になるように統一しました．↩︎\n“While (Metropolis et al., 1953) proposed the use of MCMC sampling to compute particular integrals in statistical mechanics, it was the Hastings paper that elevated the concept to a general one, and introduced it to the broader statistics community.” (Martin et al., 2023, p. 7) 3.5節．↩︎\n(Martin et al., 2023, p. 8) 4節，(Robert and Casella, 2011, p. 102) など．↩︎\nただし，\\(e^{-H}\\) が多峰性を示す低温領域では，LMH の方が効率的であるというはっきりとした理論保証はまだありません．↩︎\n今回は対象分布の減衰が極めて激しかったために差が現れにくかったのだと考えられます．Zig-Zag サンプラーは１次元でも，（広い設定の下で）（そして特に目標分布の裾が重いときに）ランダムウォーク・メトロポリス法や Metripolis-adjusted Langevin algorithm よりも速い収束レートを持ちます (Vasdekis and Roberts, 2022)，↩︎\n提案分布の．実際の軌跡の１ステップでの移動距離とは違う．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/R5.html",
    "href": "posts/2024/Computation/R5.html",
    "title": "R（５）統計処理",
    "section": "",
    "text": "runif(n [,min=0, max=1])：区間上の一様分布\n\ndunif：density\npunif：distribution function\nqunif：quantile function\n\n`rnorm(n [, mean=0, sd=1])：正規分布\n\ndnorm\npnorm\nqnorm\n\nsample(x, size [,replace=FALSE, prob=NULL])：ベクトルxの中からsize個をランダム抽出\n\n元々が非復元抽出なので，size=length(x)とすると置換．replace=TRUEとすると復元抽出．\nprobに vector を引かせると，xの要素にでやすさの重みがつく．\n\n\n\n\n\n\n{ts.plot()}\n\n\n\n\n\nhist(x, …)\n\nbreaks=“Starges”：bin の数を Starges の公式で定めているところを，scaler nで指定できる．\ncol=“NULL”：bars を fill する色．lightgreenなどにできる．\nmain, xlab, ylab：title()で get する attribution\nright=“TRUE”：デフォルトでは bin は right-closed である．\n\nplot(x[, y, …])\n\nxが適切な構造を持つなら{y}はいらない．\narray を渡すと，タイル図になる．\ndf を渡すと，散布図になる．\n\nimage(x)\n\n\n\n\n\nhcl.colors(n)：n色の pallete を作成する．\n\n\n\n\ndata()で一覧を見れる\n\nco2\n\nMauna Loa の CO2 concentration\n\nvolcano\n\nAukland’s Maunga Whau の topographic data\n10m×10m 範囲\n\nTitanic\n\n4-dimentional array\n\nairquality\n\nNew York Airquality Measurement\n６変数についての data frame\n\njpdata\n\n統計局からの県別データで utf8 なのでread.csv(file=“”, fileEncoding=“utf8”)が安全．\njpdata1.csv：対象データ\njpdata2.csv：対象データの内容\njpdata3.csv：圏別と地域の対応関係\n\ntokyo_weather\n\n気象庁からのデータ．\n\ntokyo_covid19\n\n東京の stopcovid19.metro.tokyo.lg.jp のデータ項目．\n\n\n\n\n\na symbolic description of the model to be fitted\n\n\ny ~ x1 + x2 + ...\n\n\n\n\nclass\nformula\n\n\ntypeof\nlanguage\n\n\nmode\ncall\n\n\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nformula &lt;- y ~ x1 + x2 + x3\n\n\ntypeof(formula)\n\n[1] \"language\"\n\n\n\nclass(formula)\n\n[1] \"formula\"\n\n\n\nclass(formula)\n\n[1] \"formula\"\n\n\n\nstr(formula)\n\nClass 'formula'  language y ~ x1 + x2 + x3\n  ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n\n\n\n\n\n\n\n\n\nsummary(object, ...)\n\n\n\n\nlm(formula, data, subset, weights, na.action, method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...)\n\nformula：vector ~ vectorの形の\n最小二乗法を使って，formula y~xを関係式 \\(y = a + bx\\) の \\(a,b\\) の値を定める．\n\n推定値は $coefficients に格納される．"
  },
  {
    "objectID": "posts/2024/Computation/R5.html#統計処理",
    "href": "posts/2024/Computation/R5.html#統計処理",
    "title": "R（５）統計処理",
    "section": "",
    "text": "runif(n [,min=0, max=1])：区間上の一様分布\n\ndunif：density\npunif：distribution function\nqunif：quantile function\n\n`rnorm(n [, mean=0, sd=1])：正規分布\n\ndnorm\npnorm\nqnorm\n\nsample(x, size [,replace=FALSE, prob=NULL])：ベクトルxの中からsize個をランダム抽出\n\n元々が非復元抽出なので，size=length(x)とすると置換．replace=TRUEとすると復元抽出．\nprobに vector を引かせると，xの要素にでやすさの重みがつく．\n\n\n\n\n\n\n{ts.plot()}\n\n\n\n\n\nhist(x, …)\n\nbreaks=“Starges”：bin の数を Starges の公式で定めているところを，scaler nで指定できる．\ncol=“NULL”：bars を fill する色．lightgreenなどにできる．\nmain, xlab, ylab：title()で get する attribution\nright=“TRUE”：デフォルトでは bin は right-closed である．\n\nplot(x[, y, …])\n\nxが適切な構造を持つなら{y}はいらない．\narray を渡すと，タイル図になる．\ndf を渡すと，散布図になる．\n\nimage(x)\n\n\n\n\n\nhcl.colors(n)：n色の pallete を作成する．\n\n\n\n\ndata()で一覧を見れる\n\nco2\n\nMauna Loa の CO2 concentration\n\nvolcano\n\nAukland’s Maunga Whau の topographic data\n10m×10m 範囲\n\nTitanic\n\n4-dimentional array\n\nairquality\n\nNew York Airquality Measurement\n６変数についての data frame\n\njpdata\n\n統計局からの県別データで utf8 なのでread.csv(file=“”, fileEncoding=“utf8”)が安全．\njpdata1.csv：対象データ\njpdata2.csv：対象データの内容\njpdata3.csv：圏別と地域の対応関係\n\ntokyo_weather\n\n気象庁からのデータ．\n\ntokyo_covid19\n\n東京の stopcovid19.metro.tokyo.lg.jp のデータ項目．\n\n\n\n\n\na symbolic description of the model to be fitted\n\n\ny ~ x1 + x2 + ...\n\n\n\n\nclass\nformula\n\n\ntypeof\nlanguage\n\n\nmode\ncall\n\n\n\n\n\n\n\n\n\n\n例\n\n\n\n\n\n\nformula &lt;- y ~ x1 + x2 + x3\n\n\ntypeof(formula)\n\n[1] \"language\"\n\n\n\nclass(formula)\n\n[1] \"formula\"\n\n\n\nclass(formula)\n\n[1] \"formula\"\n\n\n\nstr(formula)\n\nClass 'formula'  language y ~ x1 + x2 + x3\n  ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n\n\n\n\n\n\n\n\n\nsummary(object, ...)\n\n\n\n\nlm(formula, data, subset, weights, na.action, method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts = NULL, offset, ...)\n\nformula：vector ~ vectorの形の\n最小二乗法を使って，formula y~xを関係式 \\(y = a + bx\\) の \\(a,b\\) の値を定める．\n\n推定値は $coefficients に格納される．"
  },
  {
    "objectID": "posts/2024/Computation/R5.html#データの扱い",
    "href": "posts/2024/Computation/R5.html#データの扱い",
    "title": "R（５）統計処理",
    "section": "2 データの扱い",
    "text": "2 データの扱い\nR：５．データI/Oと整形 indexing / filteringによる整形と書き出し NAが入っているデータフレームを扱うときは，状況依存の振る舞いをするので文法に注意する．\nLogical Expressionの復習 ベクトルをrecycleにより各成分ごとに評価して，logical vectorを返す2値関数である．\n\n==, !=, &gt;=, &lt;=：T==1はTが返る．\nis.null(), is.na(), is.nan(), is.finite(), is.infinite(),\nidentical(1,1.0)：Tである，1もdouble型（倍精度浮動小数点）で表現されるので．\n\nベクトルとしての同一性はall(x==y)などで表現できる．\n\nall.equal()\n&,|：論理積・和\n&&, ||：条件式の論理積・和\nxor()：排他的論理和\n%in%：matchの二項関係版interface．\n\n“%in%” &lt;- function(x, table) match(x, table, nomatch = 0) &gt; 0が現状の定義\n\n\n関数型インターフェース * match(x, table) * x %in% tableと使える． * x：vector * table：vector * 返り値：logical vector\nfiltering：Logical Vectorを介してデータを選び出す技法\n基本：v[ vを含むexpr ]\nインデックス番号の取得 * which(expr with x)：logical vector xからTUREのindex numberの列を抽出する * 帰ってきたベクトルで元のdfをindexingできる * which.min(x)などもある\n\nx[index]：logical vectorでindexingするとTRUEのみを選び出す．\n\nx[cond]：x&gt;3は評価するとlogical vectorである．\nx[which(cond)]：numeric vectorでindexしている点だけ違う．\nx[which(cond),]：行全体を見たい場合．これからwhichを抜かすとNAなので参照不可能．\n\ndf\\(Ozone&gt;10：でも参照できる．\n  * データフレームの各列はベクトルなリストなので，\\)はベクトルが返る\n-：省く\n\n-WindでWindの列を除く．Windの列についてだけ全てFalseのベクトルを返しているのか？\n\n\n条件式評価の関数型インターフェース * with(data, expr, …) * exprはlogical expression to evaluateをベタがき * ifelse(b,u,v) * b：logical expression． * u：bの第i要素がTの場合，uiが代入される． * v：bの第i要素がFの場合，viが代入される． * 出来上がったuとvの折衷ベクトルが返る．\nデータの取り出し\n次のレベルのインターフェース * subset(x, subset, select, drop=FALSE)：切り出して使う． * x：vectors, matrices, data frames * subset：行に関する条件式で，Ozone&gt;=50など． * select：列に関する条件式で，data frameのcolumnを選び出す手法を指示するvector．c(Wind,Day)など． * drop：結果が１次元情報になった時，データ型をvectorにするかどうか．データフレームの構造を落とさないためにはFALSEを指定しなければならない． * 特にlogical vectorとの違いは，NAを残さないこと． * split(x, f, drop=FALSE, …) * merge(x, y, …)\n\ncbind()でdata frameをくっつける．\n\n部分代入\n\nx[expr with x] &lt;- 0：条件式を満たす部分のみ0にする．\n\nデータ整形\nデータを適切にいくつかに区分し，それぞれに統計量をあてがう． * aggregate(formula, data, FUN, …) * formula：y ~ xなど，xに従ってyを分類する．xは月など．yは.とするとdataを走る． * FUN：meanなどの統計関数． * data：dfまたはlist．\n種々の適切な変換関数からなるgeneric function * transform(_data, …) * _data：The object to be transformed\n入出力 utils package\n\nwrite.csv(x, file=“”)\n\nread関数は基本的にread.tableが全ての機能を持っており，そのほかは特殊化． 必ずdata.frameを返す． * read.table(file=“”,header=FALSE, …) * 変数名をつけるheaderがfileの一行目に存在しないデータを読み込むための関数 * read.csv(file=“”, fileEncoding=“utf8”) * Comma Separated Valueを読み込むためにdefaults値をあらかじめ設定してあるread.table() * row.names=1：numeric nを引くと，n行目を列の名前とみなす．"
  },
  {
    "objectID": "posts/2024/Computation/R4.html",
    "href": "posts/2024/Computation/R4.html",
    "title": "R（４）メタプログラミング",
    "section": "",
    "text": "オブジェクト志向言語ではコード自体もオブジェクトであり，R では call, expression, name の３つのモードから構成される．1\n１つのクラスからなるわけではなく，call, symbol, constant, pairlist の４つの型からなる．2\n\n\nR の内部で用いられるデータ型であり，Lisp の dotted-pair listに似ている．\npairlistオブジェクトは３つの slot をもち，CAR, CDR, TAGと呼ばれる． * CARとCDRは IBM の 60 年代のコンピュータのaddress,decrementレジスタに由来し，リストの head と tail を指す． * TAGは文字列オブジェクトである．\npairlistの R 内での機能はlistと全く同じで，indexing syntax [[]]も同一である．\n\n\n\n\n次のような操作ができる3\nrlang::expr がコンストラクタである：\n\nz &lt;- rlang::expr(y &lt;- x*10)\nz\n\ny &lt;- x * 10\n\n\n\nstr(z)\n\n language y &lt;- x * 10\n\n\n\n\n\n\n\n\n\n\n\ntypeof\nclass\nmode\n\n\n\n\nlanguage\n&lt;-\ncall\n\n\n\n\n\n\n\nexpression オブジェクトは base::eval() で評価できる：\n\nx &lt;- 4\neval(z)\ny\n\n[1] 40\n\n\n\n\n\nexpression には list のようにアクセス可能である：4\n\nlibrary(rlang)\nf &lt;- expr(f(x = 1, y = 2))\n\nf$z &lt;- 3\nf\n\nf(x = 1, y = 2, z = 3)\n\n\n\nf[[2]] &lt;- NULL\nf\n\nf(y = 2, z = 3)"
  },
  {
    "objectID": "posts/2024/Computation/R4.html#expression-とは",
    "href": "posts/2024/Computation/R4.html#expression-とは",
    "title": "R（４）メタプログラミング",
    "section": "",
    "text": "オブジェクト志向言語ではコード自体もオブジェクトであり，R では call, expression, name の３つのモードから構成される．1\n１つのクラスからなるわけではなく，call, symbol, constant, pairlist の４つの型からなる．2\n\n\nR の内部で用いられるデータ型であり，Lisp の dotted-pair listに似ている．\npairlistオブジェクトは３つの slot をもち，CAR, CDR, TAGと呼ばれる． * CARとCDRは IBM の 60 年代のコンピュータのaddress,decrementレジスタに由来し，リストの head と tail を指す． * TAGは文字列オブジェクトである．\npairlistの R 内での機能はlistと全く同じで，indexing syntax [[]]も同一である．\n\n\n\n\n次のような操作ができる3\nrlang::expr がコンストラクタである：\n\nz &lt;- rlang::expr(y &lt;- x*10)\nz\n\ny &lt;- x * 10\n\n\n\nstr(z)\n\n language y &lt;- x * 10\n\n\n\n\n\n\n\n\n\n\n\ntypeof\nclass\nmode\n\n\n\n\nlanguage\n&lt;-\ncall\n\n\n\n\n\n\n\nexpression オブジェクトは base::eval() で評価できる：\n\nx &lt;- 4\neval(z)\ny\n\n[1] 40\n\n\n\n\n\nexpression には list のようにアクセス可能である：4\n\nlibrary(rlang)\nf &lt;- expr(f(x = 1, y = 2))\n\nf$z &lt;- 3\nf\n\nf(x = 1, y = 2, z = 3)\n\n\n\nf[[2]] &lt;- NULL\nf\n\nf(y = 2, z = 3)"
  },
  {
    "objectID": "posts/2024/Computation/R4.html#メタプログラミング",
    "href": "posts/2024/Computation/R4.html#メタプログラミング",
    "title": "R（４）メタプログラミング",
    "section": "2 メタプログラミング",
    "text": "2 メタプログラミング\n\n2.1 enexpr 関数\n\ncapture_it &lt;- function(x) {\n  expr(x)\n}\ncapture_it(a + b + c)\n\nx\n\n\n\ncapture_it &lt;- function(x) {\n  enexpr(x)\n}\ncapture_it(a + b + c)\n\na + b + c"
  },
  {
    "objectID": "posts/2024/Computation/R4.html#footnotes",
    "href": "posts/2024/Computation/R4.html#footnotes",
    "title": "R（４）メタプログラミング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR Language Definition↩︎\n(Wickham, 2019) 第17章２節．↩︎\n(Wickham, 2019) 第18章↩︎\n(Wickham, 2019) 第17章２節．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/brms.html",
    "href": "posts/2024/Computation/brms.html",
    "title": "R によるベイズ混合モデリング入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nダウンロードは：1"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#sec-example",
    "href": "posts/2024/Computation/brms.html#sec-example",
    "title": "R によるベイズ混合モデリング入門",
    "section": "1 例：カウントデータのモデリング",
    "text": "1 例：カウントデータのモデリング\nDocumentation で紹介されている，Epilepsy Seizures Data (Leppik et al., 1987)，(Thall and Vail, 1990) を用いた 例 を実行してみる：\nlibrary(brms)\nfit1 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            data = epilepsy, family = poisson())\nてんかん (epilepsy) 患者の発作回数countを被説明変数とし，処置の効果を表す説明変数Trtと患者毎のランダムな切片項(1|patient)とzAge,zBaseへの依存構造を調べたい．被説明変数countのモデルとしては，Poisson 分布族を用いる．\n\n\n\n\n\n\n説明変数\n\n\n\n\nzAge：標準化された年齢\nzBase：ベースの発作回数\nTrt：治療の有無を表す２値変数\n(1|patient)：患者ごとに異なるとした切片項\n\nzBase * Trtという記法は，この２つの交互作用もモデルに含めることを意味する．\nepilepsyは59 人の患者に関して，４回の入院時の発作回数を記録した，全 236 データからなる．patientが患者を識別する ID であり，(1|patient)は患者ごとのランダム効果ということになる．\n従って本モデルはzAge, zBase, Trt, Trt*zBaseという固定効果，(1|patient)というランダム効果を取り入れた混合効果モデルということになり，回帰式は次の通りである： \\[\ny_{it} = \\beta_1 \\cdot\\texttt{zAge}_i+ \\beta_2 \\cdot \\texttt{zBase}_i + \\beta_3 \\cdot \\texttt{Trt}_i\n\\] \\[\n+ \\beta_4 \\cdot (\\texttt{zBase}_i \\cdot \\texttt{Trt}_i) + \\alpha_i +\\epsilon_{it}.\n\\] ただし，\\(\\texttt{count}_{it}\\) の Poisson 母数を \\(\\lambda_{it}\\) として，\\(y_{it}:=\\log(\\lambda_{it})\\) とした．\nベースの発作回数が高いほど，治療効果が高い／低いのではないか？という仮説を検証する，zBase*Trtを曝露因子としたモデルである．\n\n\n\n\n\n\n\n\n全データ\n\n\n\n\n\n\nepilepsy\n\n    Age Base Trt patient visit count obs        zAge        zBase\n1    31   11   0       1     1     5   1  0.42499501 -0.757172825\n2    30   11   0       2     1     3   2  0.26528351 -0.757172825\n3    25    6   0       3     1     2   3 -0.53327400 -0.944403322\n4    36    8   0       4     1     4   4  1.22355252 -0.869511123\n5    22   66   0       5     1     7   5 -1.01240850  1.302362646\n6    29   27   0       6     1     5   6  0.10557201 -0.158035233\n7    31   12   0       7     1     6   7  0.42499501 -0.719726725\n8    42   52   0       8     1    40   8  2.18182153  0.778117253\n9    37   23   0       9     1     5   9  1.38326402 -0.307819631\n10   28   10   0      10     1    14  10 -0.05413949 -0.794618924\n11   36   52   0      11     1    26  11  1.22355252  0.778117253\n12   24   33   0      12     1    12  12 -0.69298550  0.066641363\n13   23   18   0      13     1     4  13 -0.85269700 -0.495050128\n14   36   42   0      14     1     7  14  1.22355252  0.403656259\n15   26   87   0      15     1    16  15 -0.37356249  2.088730734\n16   26   50   0      16     1    11  16 -0.37356249  0.703225054\n17   28   18   0      17     1     0  17 -0.05413949 -0.495050128\n18   31  111   0      18     1    37  18  0.42499501  2.987437121\n19   32   18   0      19     1     3  19  0.58470651 -0.495050128\n20   21   20   0      20     1     3  20 -1.17212000 -0.420157930\n21   29   12   0      21     1     3  21  0.10557201 -0.719726725\n22   21    9   0      22     1     3  22 -1.17212000 -0.832065024\n23   32   17   0      23     1     2  23  0.58470651 -0.532496228\n24   25   28   0      24     1     8  24 -0.53327400 -0.120589134\n25   30   55   0      25     1    18  25  0.26528351  0.890455552\n26   40    9   0      26     1     2  26  1.86239852 -0.832065024\n27   19   10   0      27     1     3  27 -1.49154300 -0.794618924\n28   22   47   0      28     1    13  28 -1.01240850  0.590886756\n29   18   76   1      29     1    11  29 -1.65125450  1.676823640\n30   32   38   1      30     1     8  30  0.58470651  0.253871861\n31   20   19   1      31     1     0  31 -1.33183150 -0.457604029\n32   30   10   1      32     1     3  32  0.26528351 -0.794618924\n33   18   19   1      33     1     2  33 -1.65125450 -0.457604029\n34   24   24   1      34     1     4  34 -0.69298550 -0.270373532\n35   30   31   1      35     1    22  35  0.26528351 -0.008250835\n36   35   14   1      36     1     5  36  1.06384102 -0.644834526\n37   27   11   1      37     1     2  37 -0.21385099 -0.757172825\n38   20   67   1      38     1     3  38 -1.33183150  1.339808745\n39   22   41   1      39     1     4  39 -1.01240850  0.366210159\n40   28    7   1      40     1     2  40 -0.05413949 -0.906957223\n41   23   22   1      41     1     0  41 -0.85269700 -0.345265731\n42   40   13   1      42     1     5  42  1.86239852 -0.682280626\n43   33   46   1      43     1    11  43  0.74441801  0.553440656\n44   21   36   1      44     1    10  44 -1.17212000  0.178979662\n45   35   38   1      45     1    19  45  1.06384102  0.253871861\n46   25    7   1      46     1     1  46 -0.53327400 -0.906957223\n47   26   36   1      47     1     6  47 -0.37356249  0.178979662\n48   25   11   1      48     1     2  48 -0.53327400 -0.757172825\n49   22  151   1      49     1   102  49 -1.01240850  4.485281100\n50   32   22   1      50     1     4  50  0.58470651 -0.345265731\n51   25   41   1      51     1     8  51 -0.53327400  0.366210159\n52   35   32   1      52     1     1  52  1.06384102  0.029195264\n53   21   56   1      53     1    18  53 -1.17212000  0.927901651\n54   41   24   1      54     1     6  54  2.02211002 -0.270373532\n55   32   16   1      55     1     3  55  0.58470651 -0.569942327\n56   26   22   1      56     1     1  56 -0.37356249 -0.345265731\n57   21   25   1      57     1     2  57 -1.17212000 -0.232927432\n58   36   13   1      58     1     0  58  1.22355252 -0.682280626\n59   37   12   1      59     1     1  59  1.38326402 -0.719726725\n60   31   11   0       1     2     3  60  0.42499501 -0.757172825\n61   30   11   0       2     2     5  61  0.26528351 -0.757172825\n62   25    6   0       3     2     4  62 -0.53327400 -0.944403322\n63   36    8   0       4     2     4  63  1.22355252 -0.869511123\n64   22   66   0       5     2    18  64 -1.01240850  1.302362646\n65   29   27   0       6     2     2  65  0.10557201 -0.158035233\n66   31   12   0       7     2     4  66  0.42499501 -0.719726725\n67   42   52   0       8     2    20  67  2.18182153  0.778117253\n68   37   23   0       9     2     6  68  1.38326402 -0.307819631\n69   28   10   0      10     2    13  69 -0.05413949 -0.794618924\n70   36   52   0      11     2    12  70  1.22355252  0.778117253\n71   24   33   0      12     2     6  71 -0.69298550  0.066641363\n72   23   18   0      13     2     4  72 -0.85269700 -0.495050128\n73   36   42   0      14     2     9  73  1.22355252  0.403656259\n74   26   87   0      15     2    24  74 -0.37356249  2.088730734\n75   26   50   0      16     2     0  75 -0.37356249  0.703225054\n76   28   18   0      17     2     0  76 -0.05413949 -0.495050128\n77   31  111   0      18     2    29  77  0.42499501  2.987437121\n78   32   18   0      19     2     5  78  0.58470651 -0.495050128\n79   21   20   0      20     2     0  79 -1.17212000 -0.420157930\n80   29   12   0      21     2     4  80  0.10557201 -0.719726725\n81   21    9   0      22     2     4  81 -1.17212000 -0.832065024\n82   32   17   0      23     2     3  82  0.58470651 -0.532496228\n83   25   28   0      24     2    12  83 -0.53327400 -0.120589134\n84   30   55   0      25     2    24  84  0.26528351  0.890455552\n85   40    9   0      26     2     1  85  1.86239852 -0.832065024\n86   19   10   0      27     2     1  86 -1.49154300 -0.794618924\n87   22   47   0      28     2    15  87 -1.01240850  0.590886756\n88   18   76   1      29     2    14  88 -1.65125450  1.676823640\n89   32   38   1      30     2     7  89  0.58470651  0.253871861\n90   20   19   1      31     2     4  90 -1.33183150 -0.457604029\n91   30   10   1      32     2     6  91  0.26528351 -0.794618924\n92   18   19   1      33     2     6  92 -1.65125450 -0.457604029\n93   24   24   1      34     2     3  93 -0.69298550 -0.270373532\n94   30   31   1      35     2    17  94  0.26528351 -0.008250835\n95   35   14   1      36     2     4  95  1.06384102 -0.644834526\n96   27   11   1      37     2     4  96 -0.21385099 -0.757172825\n97   20   67   1      38     2     7  97 -1.33183150  1.339808745\n98   22   41   1      39     2    18  98 -1.01240850  0.366210159\n99   28    7   1      40     2     1  99 -0.05413949 -0.906957223\n100  23   22   1      41     2     2 100 -0.85269700 -0.345265731\n101  40   13   1      42     2     4 101  1.86239852 -0.682280626\n102  33   46   1      43     2    14 102  0.74441801  0.553440656\n103  21   36   1      44     2     5 103 -1.17212000  0.178979662\n104  35   38   1      45     2     7 104  1.06384102  0.253871861\n105  25    7   1      46     2     1 105 -0.53327400 -0.906957223\n106  26   36   1      47     2    10 106 -0.37356249  0.178979662\n107  25   11   1      48     2     1 107 -0.53327400 -0.757172825\n108  22  151   1      49     2    65 108 -1.01240850  4.485281100\n109  32   22   1      50     2     3 109  0.58470651 -0.345265731\n110  25   41   1      51     2     6 110 -0.53327400  0.366210159\n111  35   32   1      52     2     3 111  1.06384102  0.029195264\n112  21   56   1      53     2    11 112 -1.17212000  0.927901651\n113  41   24   1      54     2     3 113  2.02211002 -0.270373532\n114  32   16   1      55     2     5 114  0.58470651 -0.569942327\n115  26   22   1      56     2    23 115 -0.37356249 -0.345265731\n116  21   25   1      57     2     3 116 -1.17212000 -0.232927432\n117  36   13   1      58     2     0 117  1.22355252 -0.682280626\n118  37   12   1      59     2     4 118  1.38326402 -0.719726725\n119  31   11   0       1     3     3 119  0.42499501 -0.757172825\n120  30   11   0       2     3     3 120  0.26528351 -0.757172825\n121  25    6   0       3     3     0 121 -0.53327400 -0.944403322\n122  36    8   0       4     3     1 122  1.22355252 -0.869511123\n123  22   66   0       5     3     9 123 -1.01240850  1.302362646\n124  29   27   0       6     3     8 124  0.10557201 -0.158035233\n125  31   12   0       7     3     0 125  0.42499501 -0.719726725\n126  42   52   0       8     3    21 126  2.18182153  0.778117253\n127  37   23   0       9     3     6 127  1.38326402 -0.307819631\n128  28   10   0      10     3     6 128 -0.05413949 -0.794618924\n129  36   52   0      11     3     6 129  1.22355252  0.778117253\n130  24   33   0      12     3     8 130 -0.69298550  0.066641363\n131  23   18   0      13     3     6 131 -0.85269700 -0.495050128\n132  36   42   0      14     3    12 132  1.22355252  0.403656259\n133  26   87   0      15     3    10 133 -0.37356249  2.088730734\n134  26   50   0      16     3     0 134 -0.37356249  0.703225054\n135  28   18   0      17     3     3 135 -0.05413949 -0.495050128\n136  31  111   0      18     3    28 136  0.42499501  2.987437121\n137  32   18   0      19     3     2 137  0.58470651 -0.495050128\n138  21   20   0      20     3     6 138 -1.17212000 -0.420157930\n139  29   12   0      21     3     3 139  0.10557201 -0.719726725\n140  21    9   0      22     3     3 140 -1.17212000 -0.832065024\n141  32   17   0      23     3     3 141  0.58470651 -0.532496228\n142  25   28   0      24     3     2 142 -0.53327400 -0.120589134\n143  30   55   0      25     3    76 143  0.26528351  0.890455552\n144  40    9   0      26     3     2 144  1.86239852 -0.832065024\n145  19   10   0      27     3     4 145 -1.49154300 -0.794618924\n146  22   47   0      28     3    13 146 -1.01240850  0.590886756\n147  18   76   1      29     3     9 147 -1.65125450  1.676823640\n148  32   38   1      30     3     9 148  0.58470651  0.253871861\n149  20   19   1      31     3     3 149 -1.33183150 -0.457604029\n150  30   10   1      32     3     1 150  0.26528351 -0.794618924\n151  18   19   1      33     3     7 151 -1.65125450 -0.457604029\n152  24   24   1      34     3     1 152 -0.69298550 -0.270373532\n153  30   31   1      35     3    19 153  0.26528351 -0.008250835\n154  35   14   1      36     3     7 154  1.06384102 -0.644834526\n155  27   11   1      37     3     0 155 -0.21385099 -0.757172825\n156  20   67   1      38     3     7 156 -1.33183150  1.339808745\n157  22   41   1      39     3     2 157 -1.01240850  0.366210159\n158  28    7   1      40     3     1 158 -0.05413949 -0.906957223\n159  23   22   1      41     3     4 159 -0.85269700 -0.345265731\n160  40   13   1      42     3     0 160  1.86239852 -0.682280626\n161  33   46   1      43     3    25 161  0.74441801  0.553440656\n162  21   36   1      44     3     3 162 -1.17212000  0.178979662\n163  35   38   1      45     3     6 163  1.06384102  0.253871861\n164  25    7   1      46     3     2 164 -0.53327400 -0.906957223\n165  26   36   1      47     3     8 165 -0.37356249  0.178979662\n166  25   11   1      48     3     0 166 -0.53327400 -0.757172825\n167  22  151   1      49     3    72 167 -1.01240850  4.485281100\n168  32   22   1      50     3     2 168  0.58470651 -0.345265731\n169  25   41   1      51     3     5 169 -0.53327400  0.366210159\n170  35   32   1      52     3     1 170  1.06384102  0.029195264\n171  21   56   1      53     3    28 171 -1.17212000  0.927901651\n172  41   24   1      54     3     4 172  2.02211002 -0.270373532\n173  32   16   1      55     3     4 173  0.58470651 -0.569942327\n174  26   22   1      56     3    19 174 -0.37356249 -0.345265731\n175  21   25   1      57     3     0 175 -1.17212000 -0.232927432\n176  36   13   1      58     3     0 176  1.22355252 -0.682280626\n177  37   12   1      59     3     3 177  1.38326402 -0.719726725\n178  31   11   0       1     4     3 178  0.42499501 -0.757172825\n179  30   11   0       2     4     3 179  0.26528351 -0.757172825\n180  25    6   0       3     4     5 180 -0.53327400 -0.944403322\n181  36    8   0       4     4     4 181  1.22355252 -0.869511123\n182  22   66   0       5     4    21 182 -1.01240850  1.302362646\n183  29   27   0       6     4     7 183  0.10557201 -0.158035233\n184  31   12   0       7     4     2 184  0.42499501 -0.719726725\n185  42   52   0       8     4    12 185  2.18182153  0.778117253\n186  37   23   0       9     4     5 186  1.38326402 -0.307819631\n187  28   10   0      10     4     0 187 -0.05413949 -0.794618924\n188  36   52   0      11     4    22 188  1.22355252  0.778117253\n189  24   33   0      12     4     4 189 -0.69298550  0.066641363\n190  23   18   0      13     4     2 190 -0.85269700 -0.495050128\n191  36   42   0      14     4    14 191  1.22355252  0.403656259\n192  26   87   0      15     4     9 192 -0.37356249  2.088730734\n193  26   50   0      16     4     5 193 -0.37356249  0.703225054\n194  28   18   0      17     4     3 194 -0.05413949 -0.495050128\n195  31  111   0      18     4    29 195  0.42499501  2.987437121\n196  32   18   0      19     4     5 196  0.58470651 -0.495050128\n197  21   20   0      20     4     7 197 -1.17212000 -0.420157930\n198  29   12   0      21     4     4 198  0.10557201 -0.719726725\n199  21    9   0      22     4     4 199 -1.17212000 -0.832065024\n200  32   17   0      23     4     5 200  0.58470651 -0.532496228\n201  25   28   0      24     4     8 201 -0.53327400 -0.120589134\n202  30   55   0      25     4    25 202  0.26528351  0.890455552\n203  40    9   0      26     4     1 203  1.86239852 -0.832065024\n204  19   10   0      27     4     2 204 -1.49154300 -0.794618924\n205  22   47   0      28     4    12 205 -1.01240850  0.590886756\n206  18   76   1      29     4     8 206 -1.65125450  1.676823640\n207  32   38   1      30     4     4 207  0.58470651  0.253871861\n208  20   19   1      31     4     0 208 -1.33183150 -0.457604029\n209  30   10   1      32     4     3 209  0.26528351 -0.794618924\n210  18   19   1      33     4     4 210 -1.65125450 -0.457604029\n211  24   24   1      34     4     3 211 -0.69298550 -0.270373532\n212  30   31   1      35     4    16 212  0.26528351 -0.008250835\n213  35   14   1      36     4     4 213  1.06384102 -0.644834526\n214  27   11   1      37     4     4 214 -0.21385099 -0.757172825\n215  20   67   1      38     4     7 215 -1.33183150  1.339808745\n216  22   41   1      39     4     5 216 -1.01240850  0.366210159\n217  28    7   1      40     4     0 217 -0.05413949 -0.906957223\n218  23   22   1      41     4     0 218 -0.85269700 -0.345265731\n219  40   13   1      42     4     3 219  1.86239852 -0.682280626\n220  33   46   1      43     4    15 220  0.74441801  0.553440656\n221  21   36   1      44     4     8 221 -1.17212000  0.178979662\n222  35   38   1      45     4     7 222  1.06384102  0.253871861\n223  25    7   1      46     4     3 223 -0.53327400 -0.906957223\n224  26   36   1      47     4     8 224 -0.37356249  0.178979662\n225  25   11   1      48     4     0 225 -0.53327400 -0.757172825\n226  22  151   1      49     4    63 226 -1.01240850  4.485281100\n227  32   22   1      50     4     4 227  0.58470651 -0.345265731\n228  25   41   1      51     4     7 228 -0.53327400  0.366210159\n229  35   32   1      52     4     5 229  1.06384102  0.029195264\n230  21   56   1      53     4    13 230 -1.17212000  0.927901651\n231  41   24   1      54     4     0 231  2.02211002 -0.270373532\n232  32   16   1      55     4     3 232  0.58470651 -0.569942327\n233  26   22   1      56     4     8 233 -0.37356249 -0.345265731\n234  21   25   1      57     4     1 234 -1.17212000 -0.232927432\n235  36   13   1      58     4     0 235  1.22355252 -0.682280626\n236  37   12   1      59     4     2 236  1.38326402 -0.719726725\n\n\n\n\n\n\n1.1 モデルの推定とプロット\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nlibrary(brms)\nfit1 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            data = epilepsy, family = poisson())\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.907 seconds (Warm-up)\nChain 1:                0.743 seconds (Sampling)\nChain 1:                1.65 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.918 seconds (Warm-up)\nChain 2:                0.721 seconds (Sampling)\nChain 2:                1.639 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.864 seconds (Warm-up)\nChain 3:                0.726 seconds (Sampling)\nChain 3:                1.59 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.074 seconds (Warm-up)\nChain 4:                0.806 seconds (Sampling)\nChain 4:                1.88 seconds (Total)\nChain 4: \n\n\n\n\n\n\nsummary(fit1)\n\n Family: poisson \n  Links: mu = log \nFormula: count ~ zAge + zBase * Trt + (1 | patient) \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~patient (Number of levels: 59) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.58      0.07     0.46     0.74 1.00      804     1363\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.77      0.12     1.53     2.00 1.01      636      888\nzAge           0.10      0.09    -0.07     0.27 1.00      715     1483\nzBase          0.70      0.12     0.46     0.95 1.00      589     1231\nTrt1          -0.26      0.17    -0.59     0.07 1.01      656     1034\nzBase:Trt1     0.05      0.16    -0.27     0.38 1.00      680     1426\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n基本的な解析の前提がまず出力され，推定結果はグループ毎（今回は患者毎）の変数（今回は \\(\\alpha_i\\)）から表示される．\n後半に固定効果の係数，すなわち回帰係数の推定結果が表示される．\n治療効果Trtの係数は負で，平均的に処置効果はある可能性があるが，95% 信頼区間は \\(0\\) を跨いでいるという意味で，有意とは言えない．また，交差項zBase*Trtの係数は小さく，交互効果の存在を示す証拠はないと思われる．\n\\(\\widehat{R}\\) が１より大きい場合，MCMC が収束していない可能性を意味する (Vehtari et al., 2021)．通説には \\(\\widehat{R}\\le1.1\\) などの基準がある．\n変数を指定して，事後分布と MCMC の軌跡をプロットできる：\n\nplot(fit1, variable = c(\"b_Trt1\", \"b_zBase\"))\n\n\n\n\n\n\n\n\nより詳しく見るにはconditional_effects関数を用いることもできる．交差項の効果はほとんどないことがわかる：\n\nplot(conditional_effects(fit1, effects = \"zBase:Trt\"))\n\n\n\n\n\n\n\n\n\n\n1.2 モデルによる予測\nfit したモデル fit1 を用いて，平均年齢と平均ベースレートを持つ患者に対する治療効果を予測する：\n\nnewdata &lt;- data.frame(Trt = c(0, 1), zAge = 0, zBase = 0)\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]   5.8870  2.501858    2    11\n[2,]   4.5655  2.156588    1     9\n\n\n関数predict()は事後予測分布からのサンプリングを行う．一方で，関数fitted()は平均を返す．\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.905118 0.7105245 4.614680 7.413713\n[2,] 4.537683 0.5298045 3.559239 5.647088\n\n\n\n\n\n\n\n\n予測の出力\n\n\n\n\n\n従って，もう１度ずつ実行すると，predictでは値が変わるが，fittedでは同じ値が出力される．\n\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5 Q97.5\n[1,]  5.94525  2.551251    2    12\n[2,]  4.57525  2.225101    1     9\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.905118 0.7105245 4.614680 7.413713\n[2,] 4.537683 0.5298045 3.559239 5.647088\n\n\n\n\n\n\n\n1.3 モデルの比較\nモデルfit1で行った Poisson 回帰分析は，fit1に含めた説明変数の違いを除けば，個々の観測が独立になる，という仮定の上に成り立っている（第 4.3 節）．\nこの仮定が破れているとき＝全ての説明変数をモデルに含めきれていないとき，Poisson 分布の性質 \\[\n\\operatorname{E}[X]=\\mathrm{V}[X]=\\lambda\\qquad (X\\sim\\mathrm{Pois}(\\lambda))\n\\] からの離反として現れ，この現象は 過分散（overdispersion）とも呼ばれる．\n\n1.3.1 観測レベルランダム効果\nということで，他の説明変数が存在した場合を想定して， Poisson 分布族ではなく，分散が平均よりも大きいような別の分布族を用いて，フィット度合いを比較してみることを考えたい．\nそこで，追加の変動をモデルに追加するべく，モデルfit1に観測ごとの切片項 \\(\\eta_{it}\\) を追加してみる（この手法は観測レベルランダム効果と呼ばれる．第 3.2 節参照）．\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.74 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.09 seconds (Warm-up)\nChain 1:                1.138 seconds (Sampling)\nChain 1:                3.228 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.9e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.918 seconds (Warm-up)\nChain 2:                1.139 seconds (Sampling)\nChain 2:                3.057 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.983 seconds (Warm-up)\nChain 3:                1.133 seconds (Sampling)\nChain 3:                3.116 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.971 seconds (Warm-up)\nChain 4:                1.151 seconds (Sampling)\nChain 4:                3.122 seconds (Total)\nChain 4: \n\n\n\n\n\nこうして得た２つのモデルfit1,fit2を比較する．\nLLO (Leave-One-Out) cross-validation が関数looによって実行できる：\nloo(fit1, fit2)\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit1, fit2)\n\nWarning: Found 8 observations with a pareto_k &gt; 0.7 in model 'fit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 59 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit1':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -670.9 36.0\np_loo        93.2 13.7\nlooic      1341.7 72.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     228   96.6%   77      \n   (0.7, 1]   (bad)        5    2.1%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   3    1.3%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -595.2 14.0\np_loo       108.0  7.2\nlooic      1190.4 28.0\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     177   75.0%   85      \n   (0.7, 1]   (bad)       52   22.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   7    3.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2   0.0       0.0  \nfit1 -75.7      26.5  \n\n\n\n\n\nelpd_diff は expected log posterior density の差異を表す．fit2の方が大きく当てはまりが良いことが見て取れる．\nまた，WAIC (Watanabe-Akaike Information Criterion) も実装されている：\nprint(waic(fit1))\n\n\n\n\n\n\nWAIC の結果\n\n\n\n\n\n\nprint(waic(fit1))\n\nWarning: \n53 (22.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -667.4 36.1\np_waic        89.8 13.8\nwaic        1334.9 72.3\n\n53 (22.5%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nprint(waic(fit2))\n\nWarning: \n63 (26.7%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -571.4 11.9\np_waic        84.3  5.1\nwaic        1142.8 23.9\n\n63 (26.7%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\n\n\n\n他にも，reloo, kfold などの関数もある．\n\n\n\n\n\n\n他の関数一覧\n\n\n\n\n\n\nmethods(class=\"brmsfit\")\n\n [1] add_criterion           add_ic                  as_draws_array         \n [4] as_draws_df             as_draws_list           as_draws_matrix        \n [7] as_draws_rvars          as_draws                as.array               \n[10] as.data.frame           as.matrix               as.mcmc                \n[13] autocor                 bayes_factor            bayes_R2               \n[16] bridge_sampler          coef                    conditional_effects    \n[19] conditional_smooths     control_params          default_prior          \n[22] expose_functions        family                  fitted                 \n[25] fixef                   formula                 getCall                \n[28] hypothesis              kfold                   log_lik                \n[31] log_posterior           logLik                  loo_compare            \n[34] loo_linpred             loo_model_weights       loo_moment_match       \n[37] loo_predict             loo_predictive_interval loo_R2                 \n[40] loo_subsample           loo                     LOO                    \n[43] marginal_effects        marginal_smooths        mcmc_plot              \n[46] model_weights           model.frame             nchains                \n[49] ndraws                  neff_ratio              ngrps                  \n[52] niterations             nobs                    nsamples               \n[55] nuts_params             nvariables              pairs                  \n[58] parnames                plot                    post_prob              \n[61] posterior_average       posterior_epred         posterior_interval     \n[64] posterior_linpred       posterior_predict       posterior_samples      \n[67] posterior_smooths       posterior_summary       pp_average             \n[70] pp_check                pp_mixture              predict                \n[73] predictive_error        predictive_interval     prepare_predictions    \n[76] print                   prior_draws             prior_summary          \n[79] psis                    ranef                   reloo                  \n[82] residuals               restructure             rhat                   \n[85] stancode                standata                stanplot               \n[88] summary                 update                  VarCorr                \n[91] variables               vcov                    waic                   \n[94] WAIC                   \nsee '?methods' for accessing help and source code\n\n\n\n\n\n\n\n1.3.2 患者内の相関構造のモデリング\nまた，fit1において，同一患者の異なる訪問の間には全く相関がないと仮定されており，これは全く非現実的な仮定をおいてしまっていると言える．2\n患者内の相関構造は，brm()関数のautocor引数で指定できる（第 4.3.2 節）．\n例えば，全く構造を仮定しない場合は，unstrを指定する：\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\n\n\n\n\n\nフィッティングの出力\n\n\n\n\n\n\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\nWarning: Argument 'autocor' should be specified within the 'formula' argument.\nSee ?brmsformula for help.\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000148 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 4.102 seconds (Warm-up)\nChain 1:                2.262 seconds (Sampling)\nChain 1:                6.364 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.9e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 4.034 seconds (Warm-up)\nChain 2:                3.134 seconds (Sampling)\nChain 2:                7.168 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 4.157 seconds (Warm-up)\nChain 3:                2.383 seconds (Sampling)\nChain 3:                6.54 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3.8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 4.12 seconds (Warm-up)\nChain 4:                2.316 seconds (Sampling)\nChain 4:                6.436 seconds (Total)\nChain 4: \n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\n\n\n\nこのモデルもfit1より遥かに当てはまりが良く，fit2とほとんど同じ当てはまりの良さが見られる：\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit2,fit3)\n\nWarning: Found 59 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 51 observations with a pareto_k &gt; 0.7 in model 'fit3'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -595.2 14.0\np_loo       108.0  7.2\nlooic      1190.4 28.0\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.6]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     177   75.0%   85      \n   (0.7, 1]   (bad)       52   22.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   7    3.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit3':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -599.7 14.6\np_loo       111.2  7.7\nlooic      1199.5 29.2\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.5]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     185   78.4%   55      \n   (0.7, 1]   (bad)       45   19.1%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   6    2.5%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit3 -4.5       2.9   \n\n\n\n\n\n思ったよりもfit2の当てはまりが良いため，Poisson-対数正規混合モデリングを本格的に実施してみることが，次の選択肢になり得る（第 3.2 節参照）．\n\n\n\n1.4 その他の例\nSebastian Weber らにより，新薬の治験における実際の解析事例をまとめたウェブサイト が公開されている．3\n特に，13 章で，同様の経時的繰り返し観測データを扱っているが，ここではカウントデータではなく連続な応用変数が扱われている．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "href": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "title": "R によるベイズ混合モデリング入門",
    "section": "2 ランダム効果モデルの正しい使い方",
    "text": "2 ランダム効果モデルの正しい使い方\n\n\n\n\n\n\n概要\n\n\n\n\nランダム効果モデルとは，グループ毎に異なる切片項 \\(\\alpha_{s[i]}\\) を追加し，これにも誤差を仮定してモデルに入れて得る階層モデルである．\nしかし，\\(\\alpha_{s[i]}\\) が（ユニットレベルの）説明変数 \\(x_i\\) と相関を持つ場合，推定量の一致性が失われる．これを回避するために，\\(x_i\\) の係数 \\(\\beta\\) にのみ関心がある場合は，固定効果モデルが用いられることも多い．\nだが，簡単なトリック（\\(\\alpha_{s[i]}\\) の説明変数に \\(\\overline{x}_s\\) を追加すること）で，推定量の一致性を回復することができる．\nこのトリックを取り入れたランダム効果モデルは，\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がない場合は固定効果モデルと等価な \\(\\beta\\) の推定量を与え，相関がある場合でも，\\(\\beta\\) を一致推定し，各変動切片項 \\(\\alpha_{s[i]}\\) の構造にも洞察を与えてくれる．\n\n\n\n\n2.1 ランダム効果のモデル\nランダム効果は，変動する切片項と呼んだ方がわかりやすい (Bafumi and Gelman, 2007) と言われるように，サブグループ毎に異なる切片を加えたモデルである．\n従って，ユニットレベルの回帰式を書き下すと，グループ選択関数 \\(s:[n]\\to[S]\\;(S\\le n)\\) を通じて， \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\n\\tag{1}\\] というようになる．\nこれは，確率変数 \\(\\alpha_{s[i]}\\) の平均を \\(\\alpha_0\\) とすると，グループレベルの回帰式 \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad s\\in[S]\n\\tag{2}\\] が背後にある階層モデルだとみなすこともできる．\n\n\n2.2 説明変数との相関の問題\n\n2.2.1 問題の所在\nランダム効果では，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s[i]}\\) が相関を持たないという仮定が Gauss-Markov の定理の仮定に等価になるため，これが違反されると推定量の不偏性・一致性が約束されず，推定量の分散も大きくなる．4\n実際，ランダム効果モデルの階層構造を，式 2 を 式 1 に代入することで一つの式にまとめると \\[\ny_i=\\alpha_0+\\beta x_i+\\underbrace{\\epsilon_i'}_{\\epsilon_i+\\eta_{s[i]}}\n\\tag{3}\\] を得る．5 \\(x_i\\) と \\(\\alpha_{s[i]}\\) の相関をモデルに含めていない場合，\\(x_i\\) と \\(\\eta_s\\) が相関を持ってしまい，結果として 式 3 では説明変数と誤差 \\(\\epsilon_i'\\) に相関が生じてしまう．6\n\n\n2.2.2 業界の現状\nそのため，ランダム効果モデルは避けられる傾向にあり，切片項 \\(\\alpha_{s[i]}\\equiv\\alpha_0\\) は変動しないとし，グループレベルの効果を無視してモデリングすることも多い： \\[\ny_i=\\alpha_0+\\beta x_i+\\epsilon_i.\n\\] このことを complete pooling model と呼び，ランダム効果モデルを partial pooling model と対比させることがある．7\n実際，これ以上の仮定を置かず，ランダム効果は局外母数として一般化推定方程式の方法（第 2.4.1 節）によれば，\\(\\beta\\) の不偏推定が可能である．\nリンク関数 \\(g\\) を通じた非線型モデル \\[\ng(\\operatorname{E}[y_i|x_i])=\\beta x_i\n\\] であっても，指数型分布族を仮定すれば，\\(\\beta\\) の一致推定が可能である．\nこのような場合は，marginal model や population-average model とも呼ばれる (Gardiner et al., 2009, p. 228)．\n\n\n2.2.3 固定効果モデルという解決\n問題を起こさずに，しかしながらグループレベルの効果をモデリングしたい場合， \\[\ny_i=\\alpha_{s[i]}^{\\text{unmodeled}}+\\beta x_i+\\epsilon_i\n\\] \\[\n\\alpha_s^{\\text{unmodeled}}\\sim\\mathrm{N}(\\alpha_0,\\infty)\n\\] として，グループ毎に変動する切片項 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を許すが，この変数自体にモデルは仮定しない．\nこのようなモデルは，グループ毎に別々の回帰分析を実行し，\\(\\beta\\) の値はこれらのグループの間で適切に重みづけて最終的な推定値としているに等しい．\nすなわち，グループの数だけ，グループへの所属を表す２値変数 \\(1_{s[i]=s}\\) を導入し，\\(S\\) 個の項 \\(\\sum_{s=1}^S1_{s[i]=s}\\alpha_{s[i]}^{\\text{unmodeled}}\\) を説明変数に加えて回帰分析を行うことに等しい．\n\n\n\n\n\n\n名前\n\n\n\n\n(Bafumi and Gelman, 2007) は unmodeled varying intercept と呼んでいる．\n(Hansen, 2022) をはじめ，計量経済学では fixed effects model と呼ばれる．\nleast squares dummy variable regression とも呼べる．8\n\n\n\n\n\n\n\n\n\n利点\n\n\n\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持ち得る場合も，固定効果モデルでは関係がない．9\n\n\n\n\n\n\n\n\n問題点\n\n\n\n異なるグループのデータが相互作用する機構がランダム効果モデルに比べて貧しい．\n（正しく特定された）ランダム効果モデルの方は，外れ値グループが存在しても，\\(\\eta_s\\) を通じて緩やかに情報が伝達され，\\(\\beta\\) の値は平均へ縮小されて推定されるが，固定効果モデルではそのような頑健性を持たない．10\n\n\n固定効果モデルは \\(\\beta\\) （のみ）に関心がある場合，\\(\\alpha_{s[i]}\\) と \\(x_i\\) の相関の存在に対してロバストな推定法として有用であり，その理由で計量経済学（特に線型パネルデータ）では主流の推定手法となっている．11 実際，\\(\\alpha_{s[i]}\\) と \\(x_i\\) が無相関であるとき，\\(\\beta\\) に関しては等価な推定量を与える．\n\nCurrent econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estmimator for linear panel data models. (Hansen, 2022, p. 624)\n\n逆に言えば，固定効果モデルは \\(x_i\\) と \\(\\alpha_{s[i]}\\) の構造のモデリングを放棄したモデリング法であり，各 \\(\\alpha_{s[i]}\\) の値にも興味がある場合，および \\(\\beta\\) のグループ毎の値も考えたい場合にはやはり \\(\\alpha_{s[i]}\\) の誤差と相関構造もモデルに取り入れたランダム効果モデルを用いたい，ということになる．\n\n\n\n2.3 ランダム効果モデルにおける相関のモデリング\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) との相関は，欠落変数が存在するため，と考えることができる．\nそして，この相関は，説明変数の平均 \\(\\overline{x}_s\\) を変動する切片項 \\(\\alpha_s\\) の説明変数として追加することで除去できる：12\n\\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i\n\\] \\[\n\\alpha_s=\\alpha_0+\\alpha_1\\overline{x}_s+\\eta_s\n\\]\nこれにより，Gauss-Markov の仮定（外生性）が回復される．\n(Bafumi and Gelman, 2007, pp. 7–9) にシミュレーションによる検証が掲載されている．\n\nPractitioners can get around this problem by taking advantage of the multilevel structure of their regression equation. (Bafumi and Gelman, 2007, p. 12)\n\n\n\n2.4 名前の混乱をどうするか？\n以上，解説してきた「ランダム効果モデル」であるが，混合効果モデルとも呼ばれる．13\n何を言っているのかわからないかもしれないが，式 式 1 \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\n\\] において，\\(\\alpha_{s[i]}\\) がランダム効果であるが，回帰係数 \\(\\beta\\) を固定効果とも呼ぶのである．\n現代的には，必要ならば \\(\\beta\\) を確率変数とみなしても良いだろうが，慣習的にそう呼ぶため，これに従わざるを得ない，というのが (Hansen, 2022, p. 625) などを見る限り共通了解であるようである．\nこれが計量経済学における固定効果モデル（第 2.2.3 節）の名前の由来である．14 固定効果モデルは，たしかに（ユニットレベルでの回帰係数という意味での）「固定効果」を表す変数しか含んでいない（少なくとも見た目上は）．\nそこで，式 式 1 自体は，固定効果と変量効果の両方を含んだ 混合（効果）モデル というのである．\n\n\n\n\n\n\n名前\n\n\n\n(Chung et al., 2013) によると\n\n線型混合モデル (linear mixed models) (Kincaid, 2005)\n階層モデル (hierarchical models)\nマルチレベル線型モデル (multilevel linear models)\n混合効果モデル (mixed-effects models) (Chung et al., 2015)\nランダム効果モデル (random effects model) (Hubbard et al., 2010) （え？）\n分散成分モデル (variance component model)15\n\nなどと呼ばれる．\nただし，ランダム効果モデルと呼んでしまうことも多い．(Bafumi and Gelman, 2007) のアブストラクトなど．\n\n\n\n2.4.1 GEE との違い\n\n\n\n\n\n\n一般化推定方程式 (GEE: Generalized Estimating Equation) との違い\n\n\n\n\n回帰式が違う\n線型の場合の GEE は \\[\n   Y_{it}=\\alpha+\\beta_1x_{1,i,t}+\\cdots+\\beta_px_{p,i,t}\n   \\] とも表され，ランダムな切片項というものは見当たらない．その代わり，グループ間の影響は相関係数行列としてモデル化を行う．ランダム効果モデルでは，この相関構造を，ランダムな切片項を追加し，その回帰式も立てることでモデルに取り込む．\n推定目標が違う\nGEE は population average model でよく用いられる (Hubbard et al., 2010) ように，あくまで応答 \\(Y_{it}\\) の平均の不偏推定が目標であり，共分散構造はいわば局外母数である．一方，混合効果モデルは，その階層モデルとしての性質の通り，平均構造と分散構造のいずれも推定対象として扱う志向性がある．\n推定方法が違う\n混合効果モデルは主に最尤法により推定される (Hubbard et al., 2010)．GEE はモーメント法により推定され，最尤法ベースではないため，完全にランダムとは言えない欠測がある場合は弱く，IPW などの方法が用いられる．\n\n\n\nGEE にとって相関構造は局外母数であり，正確な特定は目的に含まれない．この意味で GEE の相関係数⾏列におく仮定は「間違えていてもよい便宜的な仮定」であるため，作業相関係数行列 (working correlation coefficient matrix) とも呼ばれる．相関構造を誤特定していても，平均構造は一致推定が可能であり，ロバストである．両方の特定に成功した場合はセミパラメトリック有効性が達成される．\n一方で，混合効果モデルは，階層モデルとして，平均構造と分散構造のいずれにも明示的な仮定をおくため，片方（例えば共分散構造）の特定を間違えていた場合，もう片方の解釈性が失われる，というリスクがあると論じることができる．特に (Hubbard et al., 2010) に見られる論調である．\nしかし，子供の身長の成長曲線の描画が主な研究目標である場合など，ユニットの平均効果ではなく各個人に注目したい場合には，（特に変動係数を取り入れた）混合効果モデルの方が適していることになる (Gardiner et al., 2009)．実際，モデルの特定に成功していれば，いずれのパラメータも最尤推定されるため，一致性を持つ．\n従って，モデル選択において用いられる基準も違う．GEE における作業相関係数行列と説明変数の選択には QIC (Quasi-likelihood Information Criterion) が，混合効果モデルには AIC や BIC （または cAIC や mAIC (Vaida and Blanchard, 2005)）が用いられる (Gardiner et al., 2009, p. 228)．\n\n\n\n2.4.2 ベイズ混合効果モデルという光……？\nしかし，結局ベイズ統計学の立場からは，２つの違いはほとんど重要ではなく，混合効果モデルを推定した後に，周辺化をして平均構造に関する marginal estimator を構成すれば，GEE の代用になっているのではないか？\n計算機の性能と，計算統計手法の発展が目まぐるしい現代にて，過去の議論を踏襲しすぎることは，問題の本質を誤るということもあるのだろう．\nということで，以上議論したグループレベル構造を持ったデータに対する２階の階層モデルを，本稿では「混合効果モデル」と呼ぶことにする．\nこの節はこれで終わり．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#混合効果モデリングにおける注意点",
    "href": "posts/2024/Computation/brms.html#混合効果モデリングにおける注意点",
    "title": "R によるベイズ混合モデリング入門",
    "section": "3 混合効果モデリングにおける注意点",
    "text": "3 混合効果モデリングにおける注意点\n\n\n\n\n\n\n概要\n\n\n\n\n混合効果モデルの推定において，グループレベル変動 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になり得る．特に，グループ数 \\(S\\) が小さい場合に顕著である．\nカウントデータの Poisson モデルでは，観測レベルのランダム効果を追加することで，実質的に Poisson-対数正規混合モデリングを実行できる．\n\n\n\n\n3.1 グループレベル分散の推定\n混合効果モデル（階層モデル）の推定において，特にグループ数 \\(S\\) が小さい場合，グループレベルの変動切片項 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になるという問題点が古くからの問題である (Harville, 1977)．16\nこの \\(\\mathrm{V}[\\eta_s]\\) は何の仮定も置かれておらず，グループ間の相関構造のモデリングを一手に引き受けている．EM アルゴリズムが提案されたばかりの頃 (Laird and Ware, 1982) では，共分散構造にパラメトリックな仮定をおいていたが，現代ではこれを取り去った最尤推定法・ベイズ推定法が主流である．\nしかし，最尤推定法と，一定の事前分布を仮定したベイズ MAP 推定法では，推定された共分散行列が退化してしまうことがある．これは Wishart 事前分布を仮定することでこれが回避される (Chung et al., 2015)．17 これは最尤法の文脈では，penalized likelihood と等価になる (Chung et al., 2013)．\nモデルのサイズによっては，完全なベイズ推定を実行することが難しく，一部は等価な頻度論的な方法や近似を用いることもある．その際，最適化ソルバーの収束を速めるために，共分散構造に（データや計画とは無関係に）パラメトリックモデルを仮定してしまうこともある (Kincaid, 2005)．\n\n\n3.2 カウントデータのモデリング\nカウントデータの基本は Poisson 分布であろうが，過分散を考慮するために，負の二項分布でモデリングすることもできる．これは例えば，マーケティングにおいて，顧客の購買回数をモデル化する際に用いられる (森岡毅，今西聖貴, 2016)．\nこの行為は，Poisson 分布の Gamma 分布による混合分布族を用いた，混合モデリングを行っているとみなせる：\n\n\n\n\n\n\n命題\n\n\n\nPoisson 分布 \\(\\mathrm{Pois}(\\theta)\\) の \\(\\mathrm{Gamma}(\\alpha,\\nu)\\)-混合は負の二項分布 \\(\\mathrm{NB}\\left(\\nu,\\frac{\\alpha}{\\alpha+1}\\right)\\) になる．\nただし，負の二項分布 \\(\\mathrm{NB}(\\nu,p)\\) は，次の確率質量関数 \\(p(x;\\nu,p)\\) が定める \\(\\mathbb{N}\\) 上の確率分布である： \\[\np(x;\\nu,p)=\\begin{pmatrix}x+\\nu-1\\\\x\\end{pmatrix}p^\\nu(1-p)^x.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n確率分布の変換則より，次のように計算できる：\n\\[\\begin{align*}\n  p(x)&=\\int_{\\mathbb{R}_+}\\frac{\\theta^x}{x!}e^{-\\theta}\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu\\theta^{\\nu-1}e^{-\\alpha\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\int_{\\mathbb{R}_+}\\theta^{x+\\nu-1}e^{-(\\alpha+1)\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\frac{\\Gamma(x+\\nu)}{(\\alpha+1)^{x+\\nu}}\\\\\n  &=\\begin{pmatrix}\\nu+x-1\\\\x\\end{pmatrix}\\left(\\frac{1}{\\alpha+1}\\right)^x\\left(\\frac{\\alpha}{\\alpha+1}\\right)^\\nu.\n\\end{align*}\\]\nこの最右辺は，たしかに負の二項分布の質量関数である．\nこの証明方法と，Gamma 分布については次の記事を参照：\n\n  \n    \n      \n      \n        確率測度の変換則\n        Gamma 分布とBeta 分布を例に\n      \n    \n  \n\n\n\n\nPoisson 回帰\n\\[\n\\begin{align*} y_{it} & \\sim \\operatorname{Pois}(\\lambda_{s[i]}) \\\\ \\log(\\lambda_{s[i]}) & = \\alpha_i + \\eta_{it} \\\\ \\eta_{it} & \\sim \\operatorname{N}(0, \\sigma). \\end{align*}\n\\]\nを考えると，各 \\(y_{it}\\) を，（グループ毎に条件付ければ）Poisson 分布の対数正規分布による混合分布を用いてモデル化していることにあたる．\nこの，Poisson-対数正規分布族は，(Bulmer, 1974) により生物種の個体数分布のモデリングで，過分散を説明するために用いられている．\nすなわち，第 1 節のモデルの比較 1.3 で扱った，観測レベルランダム効果 (OLRE: Observation-level Random Effects) の方法は，観測毎に \\(\\eta_{it}\\) というランダム切片項を追加するだけで，本質的には Poisson-対数正規混合モデリングを実施する という，いわばハックのような使い方である．18\n今回はモデル比較の結果が良かったため，本格的に対数正規混合を実施してみるのも良いかもしれない．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brmsの実装",
    "href": "posts/2024/Computation/brms.html#brmsの実装",
    "title": "R によるベイズ混合モデリング入門",
    "section": "4 brmsの実装",
    "text": "4 brmsの実装\nbrm 関数（コードは こちら）の実装を調べる．\n\n\n\n\n\n\n\nbrms\n\nStan コードを扱っている関数は .stancode() であった．最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\n\n.standata\n\n\n\n\n\n4.1 事前分布\nbrm関数 では，デフォルトでは無情報事前分布が用いられる．\n\nDefault priors are chosen to be non or very weakly informative so that their influence on the results will be negligible and you usually don’t have to worry about them. However, after getting more familiar with Bayesian statistics, I recommend you to start thinking about reasonable informative priors for your model parameters: Nearly always, there is at least some prior information available that can be used to improve your inference.brm(): Fit Bayesian Generalized (Non-)Linear Multivariate Multilevel Models\n\n\n\n4.2 回帰式\nbrm()関数の第一引数は，validate_formula関数に渡される．\nこの関数は S3 のメソッドのディスパッチを用いて実装されており，brmsformulaオブジェクトに対しては，validate_formula.brmsformula関数が呼び出される．\nここではautocor引数が引かれている場合，出力のformula属性に追加される：19\n\nfit3$formula\n\ncount ~ zAge + zBase * Trt + (1 | patient) \nautocor ~ unstr(time = visit, gr = patient)\n\n\nなお，brmsformulaオブジェクトのコンストラクタは brmsformula()関数 である．これは，R のformulaオブジェクトを通じて，階層モデルを定義できるようになっている（実装はリスト）．\n\n\n4.3 共分散構造\n共分散構造は２つの観点から，brmsformulaオブジェクトから自動的に指定される．\n１つ目がグルーピング構造（共分散行列のブロック構造）であり，これはgr関数 が使用される．\n２つ目がグループ内の相関構造であり，これはbrm()関数のautocor引数を用いる．\n\n4.3.1 gr関数\nこの関数はbrm関数の第一引数として与えられたモデル定義式から，暗黙のうちに内部で呼び出される．\n例えば，回帰式に(1|patient)が含まれていた場合，gr(patient)が呼び出される．\n共分散構造におく仮定について，重要なデフォルト設定が２つある：\n\n\n\n\n\n\n\nグループ間の相関構造は想定されている：cor=True．\n\nIf TRUE (the default), group-level terms will be modelled as correlated.gr(): Set up basic grouping terms in brms\n\n一方で，グループ内の相関構造は想定されておらず，独立とされている．具体的に指定したい場合は引数covを用いる．\n\nBy default, levels of the same grouping factor are modeled as independent of each other.gr(): Set up basic grouping terms in brms\n\n\nすなわち，\\(\\mathrm{V}[\\eta_s]\\) には一切仮定が置かれておらず（第 3.1 節），一方で \\(\\{\\epsilon_{it}\\}_{t=1}^T\\) は互いに独立とされている．\n\n\n\nまた，この二階層目の分布族（第 2.1 節での \\(\\alpha_i\\) と \\(\\eta_{it}\\)）は，分散共分散行列 \\(\\mathrm{V}[\\eta_s]\\) を持った正規分布がデフォルトで，現状他の分布族は指定できないでいる．\n\ndist: Name of the distribution of the group-level effects. Currently “gaussian” is the only option.gr(): Set up basic grouping terms in brms\n\n\n\n4.3.2 autocor引数\nbrm()関数には，autocor引数 が用意されている．\ngr()のデフォルト値では独立とされていたグループ内の相関構造を，具体的に指定するのに用いられる．\n\n\n\n\n\n\n\nunstr：一才の仮定を置かない．\nAR：一次の自己相関構造．\n\n\n\n\n\n\n\n4.4 推論エンジン\nbrm関数 は，Stan による MCMC サンプリングを通じて，事後分布を計算する．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#acknowledgements",
    "href": "posts/2024/Computation/brms.html#acknowledgements",
    "title": "R によるベイズ混合モデリング入門",
    "section": "5 Acknowledgements",
    "text": "5 Acknowledgements\n\nI would like to extend my gratitude to Robert Long, who kindly shared me the knowledge about the covariance structure implicitly defined via brms formula on this Cross Validated post. His insights were instrumental in enhancing this work."
  },
  {
    "objectID": "posts/2024/Computation/brms.html#footnotes",
    "href": "posts/2024/Computation/brms.html#footnotes",
    "title": "R によるベイズ混合モデリング入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nR を最新バージョン 4.3.1 → 4.4.0 にアップデートしなければインストールに失敗したことに注意．↩︎\n通常は時間的に離れている観測は相関が薄いとしても，直近の観測と関連性が高いだろう．↩︎\nStatistical Modeling, Causal Inference, and Social Science における こちらのエントリ も参照．↩︎\n(Hansen, 2022, p. 333) 第12.3節，(Bafumi and Gelman, 2007, p. 3), (Hansen, 2022, p. 604)，(Gardiner et al., 2009, p. 228)．↩︎\nこのような誤差項の構造 \\(e_{it}=\\alpha_i+\\epsilon_{it}\\) を一元誤差成分モデル (one-way error component model) ともいう (Hansen, 2022, p. 600)．↩︎\nこの，説明変数と誤差の間に相関があることを，計量経済学では 内生性 (endogeneity) という．↩︎\n(Bafumi and Gelman, 2007, p. 5)．↩︎\n(Bafumi and Gelman, 2007, p. 5)，(Hansen, 2022, p. 609) 17.11節 など．狭義では，fixed effects model は within transformation を行い，グループ間の影響を引いたあとに回帰を実行する……という手続きを指すこともあるが，２つは等価な結果を生む．詳しくは (Cunningham, 2021) なども参照．↩︎\n(Hansen, 2022, p. 624) 17.25節．↩︎\n(Bafumi and Gelman, 2007, pp. 4–5)．↩︎\n(Hansen, 2022, p. 624)，(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Hubbard et al., 2010) では両方の名前で呼んでいる．↩︎\n(Hansen, 2022, p. 625) 17.25節．疫学・生物統計学では，実験計画法でしか「固定効果」「変量効果モデル」とは言わない，という認識であることも筆者は聞いたことがある．↩︎\n\\(\\mathrm{V}[\\eta_s]\\) はブロック行列の構造を持つためこう呼ばれるs．↩︎\n(Laird and Ware, 1982)，(Chung et al., 2013)，(Chung et al., 2015)，Statistical Modeling, Causal Inference, and Social Science ブログ 6/2/2023．↩︎\n逆 Wishart ではないらしい (Chung et al., 2015)．↩︎\nSolomon Kurtz (2021) による解説，RPubs も参照．↩︎\nLine 1363．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI.html",
    "href": "posts/2024/Computation/VI.html",
    "title": "変分推論１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次の 稿 で，\\(K\\)-平均アルゴリズムの確率的な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す．\nより図が見やすい PDF 版は こちら．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#導入",
    "href": "posts/2024/Computation/VI.html#導入",
    "title": "変分推論１",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 歴史\nハード \\(K\\)-平均法はモデルフリーのクラスタリングアルゴリズムである．Voronoi 分割による競争学習の一形態とも見れる．1\n一方で原論文 (Lloyd, 1982) では，パルス符号変調 の文脈で，アナログ信号の量子化の方法として提案している．2\n実際，いまでも \\(K\\)-平均法は（非可逆）データ圧縮に用いられる．クラスター中心での画像の値と，それ以外では帰属先のクラスター番号のみを保存すれば良いというのである．このようなアプローチを ベクトル量子化 (vector quantization) という．3\nソフト \\(K\\)-平均法とは，このようなデータ点のクラスターへの一意な割り当てを，ソフトマックス関数を用いて軟化したアルゴリズムであり，多少アルゴリズムとしての振る舞いは改善するとされている．\n\n\n1.2 最適化アルゴリズムとしての見方\n\\(N\\) 個のデータ \\(\\{x_n\\}_{n=1}^N\\) の \\(K\\) クラスへの \\(K\\)-平均クラスタリングアルゴリズムは，ハードとソフトの二種類存在するが，いずれも \\[\nJ:=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\|x_n-\\mu_k\\|^2\n\\] という損失関数の逐次最小化アルゴリズムとみなせる．\nこの見方は，ML アルゴリズム への一般化の軸となる．\n\n\n1.3 用いるデータ\n実際のコードとデータを用いて \\(K\\)-平均法を解説する．\nまずは，解説にために作られた，次のような３つのクラスタからなる２次元のデータを考え，これの正しいクラスタリングを目指す．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#sec-hard-k-means",
    "href": "posts/2024/Computation/VI.html#sec-hard-k-means",
    "title": "変分推論１",
    "section": "2 ハード \\(K\\)-平均法",
    "text": "2 ハード \\(K\\)-平均法\n\n2.1 アルゴリズムの説明\nhead \\(K\\)-means algorithm はデータ \\(\\{x^{(n)}\\}_{n=1}^N\\subset\\mathbb{R}^I\\) とクラスタ数 \\(K\\in\\mathbb{N}^+\\)，そして初期クラスター中心 \\((m^{(k)})_{k=1}^K\\in(\\mathbb{R}^I)^K\\) の３組をパラメータに持つ．\nsoft \\(K\\)-means algorithm 3.1 はさらに硬度パラメータ \\(\\beta\\in\\mathbb{R}_+\\) を持つ．\nnumpy の提供する行列積を利用して，これを Python により実装した例を以下に示す．ソフト \\(K\\)-平均法の実装と対比できるように，負担率を通じた実装を意識した例である．\nアノテーションを付してあるので，該当箇所（右端の丸囲み数字）をクリックすることで適宜解説が読めるようになっている．\ndef hkmeans_2d(data, K, init, max_iter=100):\n    \"\"\"\n    ２次元データに対するハード K-平均法の実装例．\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n\n    Returns:\n    - clusters: (N,)-numpy.ndarray クラスター番号\n    \"\"\"\n\n1    N = data.shape[0]\n2    I = data.shape[1]\n3    m = init\n4    r = np.zeros((K, N), dtype=float)\n\n    for _ in range(max_iter):\n        # Assignment Step\n        for i in range(N):\n5            distances = np.array([d(data[i], m[:,k]) for k in range(K)])\n6            k_hat = np.argmin(distances)\n7            r[:,i] = 0\n            r[k_hat,i] = 1\n        \n        # Update Step\n8        new_m = np.zeros_like(m, dtype=float)\n9        numerator = np.dot(r, data)\n10        denominator = np.sum(r, axis=1)\n11        for k in range(K):\n            if denominator[k] &gt; 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = m[:,k]\n\n12        if np.allclose(m, new_m):\n            break\n        m = new_m\n    \n13    return np.argmax(r, axis=0)\n\n1\n\nデータ数を取得している．\n\n2\n\nデータの次元を取得している．今回はすべて２次元データを用いる．\n\n3\n\nクラスター中心に引数として受け取った初期値を代入. \\(2×K\\)-行列であることに注意．\n\n4\n\n負担率を \\(K×N\\)-行列として格納している．その理由は後ほど行列積を通じた計算を行うためである．dtype=float の理由は後述．\n\n5\n\nこの distances 変数は (K,)-numpy.ndarray になる．すなわち，第 \\(k\\) 成分が，第 \\(k\\) クラスター中心との距離となっているようなベクトルである．ただし，d は Euclid 距離を計算する関数として定義済みとした．\n\n6\n\n距離が最小となるクラスター番号 \\(\\hat{k}:=[\\operatorname*{argmin}_{k\\in[K]}d(m_k,x_i)]\\) を，\\(i\\in[N]\\) 番目のデータについて求める．\n\n7\n\n\\(\\hat{k}\\) に基づいて負担率を更新するが，ループ内で前回の結果をリセットする必要があることに注意．\n\n8\n\nここで dtype=float と指定しないと，初め引数 init が整数のみで構成されていた場合に，Python の自動型付機能が int 型だと判定し，クラスター中心 m の値が整数に限られてしまう．すると，アルゴリズムがすぐに手頃な格子点に収束してしまう．\n\n9\n\nnumpy の行列積を計算する関数 np.dot を使用している．更新式 \\[\nm^{(k)}\\gets\\frac{\\sum_{n=1}^Nr^{(n)}_kx^{(n)}}{\\sum_{n=1}^Nr^{(n)}_k}\n\\] の分子を行列積と見たのである．\n\n10\n\n分母 (denominator) は \\((K,N)\\)-行列 r の行和として得られる．\n\n11\n\nゼロによる除算が起こらないように場合わけをしている．\n\n12\n\nクラスター中心がもはや変わらない場合はアルゴリズムを終了する．\n\n13\n\n負担率の最も大きいクラスター番号を返す．今回は hat_k の列をそのまま返せば良いが，soft \\(K\\)-means アルゴリズムにも通じる形で実装した．\n\n\n\n\n\n\n\n\n注：実際に用いる実装\n\n\n\n\n\nただし，本記事の背後では次の実装を用いる．\nクラスター中心の推移のヒストリーを保存して図示に利用したり，負担率 r の中身を見たりすることが出来るようにするため，assignment step と update step とに分けてクラスメソッドとして実装し，run メソッドでそれらを呼び出すようにしている．これに fetch_cluster と fetch_history メソッドを加えることで，クラスター番号とクラスター中心の推移を取得することが出来る．フィールド .r から（最終的な）負担率を見ることもできる．\n\n\nCode\nclass kmeans_2d:\n    \"\"\"\n    ２次元データに対するソフト K-平均法の実装．\n\n    Usage:\n        kmeans = kmeans_2d(data, K, init, beta)\n        kmeans.run()\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n    - beta: float 硬度パラメータ\n    \"\"\"\n\n    def __init__(self, data, K, init, beta, max_iter=100):\n        self.data = np.array(data, dtype=float)\n        self.K = K\n        self.init = np.array(init, dtype=float)\n        self.beta = float(beta)\n        self.max_iter = max_iter\n        self.N = data.shape[0]  # データ数\n        self.I = data.shape[1]  # 次元数 今回は２\n        self.m = init  # クラスター中心の初期化．2×K行列．\n        self.r = np.zeros((K, self.N), dtype=float)  # 負担率．K×N行列．\n        self.history = [init.copy()] # クラスター中心の履歴．2×K行列．\n    \n    def soft_assigment(self):\n        \"\"\"soft K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) ** 2 for j in range(self.K)]) # (N,)-numpy.ndarray\n            denominator_ = np.sum(np.exp(-self.beta * distances))  # 分母\n            self.r[:,i] = np.exp(- self.beta * distances) / denominator_\n\n    def hard_assigment(self):\n        \"\"\"hard K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray\n            k_hat = np.argmin(distances)  # 最小距離のクラスター番号\n            self.r[:,i] = 0  # 前のループの結果をリセット\n            self.r[k_hat,i] = 1\n    \n    def update(self):\n        \"\"\"クラスター中心の更新\"\"\"\n        new_m = np.zeros_like(self.m, dtype=float) # ここで float にしないと，クラスター中心が整数に限られてしまう．\n        numerator = np.dot(self.r, self.data)  # (K,2)-numpy.ndarray\n        denominator = np.sum(self.r, axis=1)  # 各クラスターの負担率の和\n        for k in range(self.K):\n            if denominator[k] &gt; 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = self.m[:,k]\n        self.m = new_m\n\n    def fetch_cluster(self):\n        \"\"\"最終的なクラスター番号を格納した (N,)-array を返す\"\"\"\n        return np.argmax(self.r, axis=0)\n    \n    def fetch_history(self):\n        \"\"\"クラスター中心の履歴を格納したリストを，３次元の np.array に変換して返す\"\"\"\n        return np.stack(self.history, axis=0)\n\n    def run_soft(self):\n        \"\"\"soft K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.soft_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n    \n    def run_hard(self):\n        \"\"\"hard K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.hard_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n\n\nなお，この実装は \\(\\beta\\ge500\\) などの場合にオーバーフローが起こることに注意．これへの対処は logsumexp の使用などが考えられる．\n\n\n\n\n\n2.2 初期値依存性\n次の２つの初期値を与えてみる． \\[\nm_1:=\\begin{pmatrix}4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3=\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] と，\\(m_2,m_3\\) は変えずに \\(m_1\\) の \\(y\\)-座標を \\(1\\) だけ下げたもの \\[\nm_1':=\\begin{pmatrix}4\\\\-1\\end{pmatrix}\n\\] とを初期値として与えてみる．\n\n\n\n\n\n\n\n\n図 1: ハード K-平均法によるクラスタリングの結果．初期値は \\((m_1,m_2,m_3)=\\left(\\begin{pmatrix}4\\\\0\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．\n\n\n\n\n\n正解数: 51     正解率: 56.7 %     反復数: 9 回\n\n\n別の初期値を与えてみる（右下の点 \\(m_1\\) を \\(1\\) だけ下に下げただけ）： \\[\n\\begin{pmatrix}4\\\\0\\end{pmatrix}=m_1\\mapsto m_1':=\\begin{pmatrix}4\\\\-1\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n図 2: ハード K-平均法によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 7 回\n\n\n結果が全く変わり，\\((m_1',m_2,m_3)\\) を与えた方が，大きく正解に近づいている．具体的には，右下の初期値 \\(m_1\\) は右上の島に行くが，\\(m_1'\\) は左下の島に行ってくれる．\nハード \\(K\\)-平均アルゴリズムは初期値に敏感である ことがよく分かる．\n\n\n2.3 局所解への収束\n直前の結果2ではクラスター２と３の境界線で４つのミスを犯しており，これを修正できないか試したい．\nそこで，答えに近いように， \\[\nm_1\\gets\\begin{pmatrix}2.5\\\\2\\end{pmatrix},\\;\\; m_2\\gets\\begin{pmatrix}-1\\\\-1\\end{pmatrix},\\;\\; m_3\\gets\\begin{pmatrix}1\\\\-2\\end{pmatrix},\n\\] を初期値として与えてみて，正答率の変化を観察する．\n\n\n\n\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 5 回\n\n\nもはや初期値から殆ど動いていないが，目標のクラスター３に分類された３つの点が，相変わらず３のままであり，加えてクラスター２の中心がこれらから逃げているようにも見えるので，クラスター２の初期値をよりクラスター３に近いように誘導し，クラスター３の中心をより右側から開始する：\n\\[\nm_2:\\begin{pmatrix}-1\\\\-1\\end{pmatrix}\\mapsto\\begin{pmatrix}0\\\\-2\\end{pmatrix}\\;\\; m_3:\\begin{pmatrix}1\\\\-2\\end{pmatrix}\\mapsto\\begin{pmatrix}2\\\\-2\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 6 回\n\n\nこんなに誘導をしても，正しく分類してくれない．\n実は，以上２つの初期値では，最終的に３つのクラスター中心は同じ値に収束している．よって，これ以上どのように初期値を変更しても，正答率は上がらないシナリオが考えられる．\n以上の観察から，ハード \\(K\\)-平均法はある種の 局所解に収束する ようなアルゴリズムであると考えられる．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#ソフト-k-平均法",
    "href": "posts/2024/Computation/VI.html#ソフト-k-平均法",
    "title": "変分推論１",
    "section": "3 ソフト \\(K\\)-平均法",
    "text": "3 ソフト \\(K\\)-平均法\n\n3.1 アルゴリズムの説明\nハード \\(K\\)-平均法2では，負担率 \\[\nr_{kn}\\gets\\delta_{k}(\\operatorname*{argmax}_{i\\in[k]}d(m_i,x_n))\n\\] は \\(0,1\\) のいずれかの値しか取らなかった．この振る舞いを， \\[\n\\sigma(z;e)_i:=\\frac{e^{z_i}}{\\sum_{j=1}^Ke^{e_j}}\\quad(i\\in[K])\n\\] で定まる ソフトマックス関数 \\(\\sigma:\\mathbb{R}^K\\to(0,1)^K\\) を用いて，「軟化」する．\nここでは，\\(\\beta\\ge0\\) として， \\[\n\\sigma(z;e^{-\\beta})_i=\\frac{e^{-\\beta z_i}}{\\sum_{j=1}^Ke^{-\\beta e_j}}\n\\] の形で用い，\\(\\operatorname*{argmax}\\) の代わりに \\[\n\\begin{align*}\n    r_{kn}&\\gets\\sigma(d(-,x_n)^2\\circ m;e^{-\\beta})_k\\\\\n    &=\\frac{e^{-\\beta d(m_k,x_n)^2}}{\\sum_{j=1}^K e^{-\\beta d(m_j,x_n)^2}}\n\\end{align*}\n\\] とする．ただし，\\(d\\) は \\(\\mathbb{R}^2\\) 上の Euclid 距離とした．\n\n3.1.1 硬度パラメータ\n\\(\\beta\\) は 硬度 (stiffness) または逆温度と呼ぶ．4 \\(\\sigma:=\\beta^{-1/2}\\) は距離の次元を持つ．\n\\(\\beta=0\\) のときは温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる \\(\\beta\\to\\infty\\) の極限が hard \\(K\\)-means アルゴリズムに相当する．\n逆温度 \\(\\beta\\) を連続的に変化させることで，クラスタ数に分岐が起こる，ある種の相転移現象を見ることができる．5\n\n\n3.1.2 実装\n実装は例えば hard \\(K\\)-means アルゴリズム2から，負担率計算の部分のみを変更すれば良い：\nfor i in range(N):\n1        distances = np.array([d(data[i], m[:,k]) for k in range(K)])\n2        denominator_ = np.sum(np.exp(-beta * distances))\n3        r[:,i] = np.exp(-beta * distances) / denominator_\n\n1\n\nデータ \\(x_i\\) とクラスター中心 \\((m_k)_{k=1}^K\\) との距離を計算し，ベクトル \\((d(x_n,m_k))_{k=1}^K\\) を distances に格納している．\n\n2\n\n負担率の計算 \\[\nr_{ik}=\\frac{\\exp(-\\beta d(m_k,x_i))}{\\sum_{j=1}^K\\exp(-\\beta d(m_j,x_i))}\n\\] を２段階に分けて行なっており，分母を先に計算して変数 denominator_ に格納している．\n\n3\n\nすでに計算してある分母 denominator_ を用いてデータ \\(x_i\\) の負担率 \\((r_{ki})_{k=1}^K\\) を計算し，\\((K,N)\\)-行列 r の各列に格納している．\n\n\n\n\n\n3.2 挙動の変化の観察\n逆温度をはじめに \\(\\beta=0.3\\) としてみる．図 1 と全く同様な初期値 \\[\nm_1:=\\begin{pmatrix}4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3=\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] を与えてみると，次の通りの結果を得る：\n\n\n\n\n\n\n\n\n図 3: 左がソフト K-平均法（\\(\\beta=1\\)），右がハード K-平均法によるクラスタリングの結果（図２の左と全く同じもの）．初期値は \\((m_1,m_2,m_3)=\\left(\\begin{pmatrix}4\\\\0\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．\n\n\n\n\n\n正解数: 44 vs. 51     正解率: 48.9 % vs. 56.7 %     反復数: 28 回 vs. 9 回\n\n\nクラスターの境界が変化しており，正解率は悪化している．さらに，反復数が９回であったところから，３倍に増えている（28回）．\nまた，右上の２つのクラスター中心の収束先は，微妙にずれているが ほとんど一致している 点も注目に値する．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\ncenters = history[-1, :, :]\ndf = pd.DataFrame(centers, columns=['Cluster1', 'Cluster2', 'Cluster3'])\nprint(df)\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.397456  2.397535 -0.036071\ny  2.047565  2.047580 -1.448288\n\n\n\n\n\n図 2 で与えた初期値 \\((m_1',m_2,m_3)\\) も与えてみる．\n\n\n\n\n\n\n\n\n図 4: ソフト K-平均法（\\(\\beta=1\\)）によるクラスタリングの結果，右がハード K-平均法によるクラスタリングの結果（図２の右と全く同じもの）．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 59 回 vs. 7 回\n\n\nクラスター境界と正答率は変わらないが，反復数がやはり７回から大きく増えている．\n結果はやはり 図 3 とは大きく異なっており，ハード \\(K\\)-平均法で観察された初期値鋭敏性が，変わらず残っている．\n加えてこの場合も 図 3 のクラスター１と２と同様に，クラスター２と３の中心がほぼ一致している．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.466833 -0.369537  0.447958\ny  2.124961 -1.076874 -1.543758\n\n\n\n\n\n\\(\\beta=0.3\\) の場合のソフト \\(K\\)-平均法は，この例では クラスター中心が融合する傾向にある ようである．\n一般に，\\(\\beta\\) が小さく，温度が大きいほど，エネルギーランドスケープに極小点が少なくなり，クラスターは同じ場所へ収束しやすくなると予想される．\n\n\n3.3 高温になるほどクラスター数は減少する\n初期値を直前で用いた \\[\nm_1\\gets\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\quad m_2\\gets\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3\\gets\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] で固定とし，さらに温度を上げて，逆温度を \\(\\beta=0.1\\) としてみる．\n\n\n\n\n\n\n\n\n図 5: ソフト K-平均法（左\\(\\beta=0.1\\)，右\\(\\beta=1\\)）によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 68 vs. 85     正解率: 75.6 % vs. 94.4 %     反復数: 101 回 vs. 59 回\n\n\n反復数はさらに増加し，全てがほとんど同じクラスターに属する結果となってしまった．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n\n\n   Cluster1  Cluster2  Cluster3\nx  1.715903  0.862511  1.329066\ny  1.012398 -0.099845  0.511186\n\n\n\n\n\n温度が大変に高い状態では，全てが乱雑で，３つのクラスターが一様・公平に負担率を持つようになった．そのため，第一歩からほとんど全体の中心へと移動し，反復数が減る．\n次に，温度を少し下げて，逆温度を \\(\\beta=2\\) としてみる．\n\n\n\n\n\n\n\n\n図 6: ソフト K-平均法（左\\(\\beta=10\\)，右\\(\\beta=1\\)）によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 17 回 vs. 59 回\n\n\n初めて soft \\(K\\)-means アルゴリズムを用いた場合で，３つのクラスター中心がはっきりと別れた．反復回数は，\\(\\beta=0.3\\) の場合と比べればやはり落ち着いている．\nしかし，正解率は head \\(K\\)-means の場合（ 図 2 など）と全く同じである．実は，最終的なクラスター中心も 図 2 の最終的なクラスター中心とほとんど同じになっている．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n今回のソフト \\(K\\)-平均法の最終的なクラスター中心\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.416113  0.881629 -1.338782\ny  2.086327 -1.934090 -0.816316\n\n\n図 2 のハード \\(K\\)-平均法の最終的なクラスター中心\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.426102  0.868333 -1.323353\ny  2.091429 -1.948458 -0.765176\n\n\n\n\n\n以上より，ソフト \\(K\\)-平均法は温度を上げるほどクラスター数が少なくなり，温度を下げるほどクラスター数は上がり，十分に温度を下げるとハード \\(K\\)-平均法に挙動が似通う．\n\n\n3.4 最適な硬度の選択\n\\(\\beta=0.2\\) ではクラスターが２つに縮退し，\\(\\beta=1\\) では hard \\(K\\)-means アルゴリズムの結果とほとんど変わらなくなる．その中間では次のように挙動が変わる：\n\n\n\n\n\n\n\n\n図 7: ソフト K-平均法によるクラスタリングの結果の比較（\\(\\beta=0.2\\) vs. \\(\\beta=0.25\\)）．\n\n\n\n\n\n正解数: 50 vs. 86\n正解率: 55.6 % vs. 95.6 %\n\n\n\n\n\n\n\n\n\n\n図 8: ソフト K-平均法によるクラスタリングの結果の比較（\\(\\beta=0.3\\) vs. \\(\\beta=0.5\\)）．\n\n\n\n\n\n正解数: 85 vs. 85\n正解率: 94.4 % vs. 94.4 %\n\n\nやはり，温度が高い場合はクラスター中心が合流・融合してしまいやすいが，冷却することでクラスター数は大きい状態で安定する，と言えるだろう．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#本番データセットでの実験",
    "href": "posts/2024/Computation/VI.html#本番データセットでの実験",
    "title": "変分推論１",
    "section": "4 本番データセットでの実験",
    "text": "4 本番データセットでの実験\n今まで使っていたデータ1.3はクラスターのオーバーラップはなかったため，いわば優しいデータであった．ここからはよりデータ生成過程が複雑なデータを用いて，ソフト \\(K\\)-平均法の挙動を観察する．\n\n4.1 データの概観\n今度は，次の４クラスのデータを用いる．\n\n\n\n\n\n\n\n\n\n実は，これは４つの Gauss 分布から生成されたデータである．\n\n\n4.2 最適な温度の選択\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正解数: 377 vs. 366     正解率: 83.8 % vs. 81.3 %     反復数: 49 回 vs. 62 回\n正解数: 386 vs. 375     正解率: 85.8 % vs. 83.3 %     反復数: 101 回 vs. 70 回\n正解数: 378 vs. 378     正解率: 84.0 % vs. 84.0 %     反復数: 39 回 vs. 14 回"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#実験結果まとめ",
    "href": "posts/2024/Computation/VI.html#実験結果まとめ",
    "title": "変分推論１",
    "section": "5 実験結果まとめ",
    "text": "5 実験結果まとめ\n\n\n\n\n\n\n結論\n\n\n\n\nデータ 1.3 に対して，（初期値 \\((m'_1,m_2,m_3)\\) で）ソフト \\(K\\)-平均法を適用すると，\n\n\\(\\beta\\ge2\\) の場合で結果はハード \\(K\\)-平均法と変わらなくなる．\n\\(\\beta=1\\) の場合で結果はクラスターがほとんど２つになり，\\(\\beta\\le0.5\\) では計算機上では実際に２つになってしまう．\n正答率は \\(1\\le\\beta\\le1.1\\) で最大であった．\n\\(\\beta\\) を大きくするほど，反復回数は減少していった．\n\nデータ 4.1 に対しても，以上の４点について同様の傾向が確認できた．\n\n\n\nこうしてソフト \\(K\\)-平均法とハード \\(K\\)-平均法の性質は分かった．主に\n\n初期値依存性\nクラスタ数 \\(K\\) の選択法\n\nの問題が未解決であり，恣意性が残る．\n誰がどう使ってもうまくいくようなアルゴリズムであると言うことは出来ない．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#footnotes",
    "href": "posts/2024/Computation/VI.html#footnotes",
    "title": "変分推論１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(MacKay, 2003, p. 285)．↩︎\nLloyd は 1957 年には発表していたが，論文の形になったのが 1982 である．\\(K\\)-means という名前の初出は (MacQueen, 1967) とされている．↩︎\n(MacKay, 2003, p. 284)，(Bishop, 2006, p. 429)．クラスター中心は 符号表ベクトル または 代表ベクトル (code-book vector) という．↩︎\nstiffness の用語は (MacKay, 2003, p. 289) から．実は各クラスターに Gauss モデルを置いた場合の分散 \\(\\sigma^2\\) に対して，\\(\\beta=\\frac{1}{2\\sigma^2}\\) の関係がある．次稿 参照．↩︎\n(MacKay, 2003, p. 291)．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html",
    "href": "posts/2024/Computation/VI2.html",
    "title": "変分推論２",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#最尤推定",
    "href": "posts/2024/Computation/VI2.html#最尤推定",
    "title": "変分推論２",
    "section": "1 最尤推定",
    "text": "1 最尤推定\nクラスタリングを一度さっぱり忘れて，最尤推定を思い出してみる．\n\n\n\n\n\n\n定義：最尤推定量1 (Fisher, 1912)\n\n\n\n\\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\subset\\mathcal{P}(\\mathcal{X})\\) を統計モデルで，ある共通の \\(\\sigma\\)-有限測度 \\(\\mu\\in\\mathcal{P}(\\mathcal{X})\\) に関して密度 \\(\\{p_\\theta\\}_{\\theta\\in\\Theta}\\) を持つとする．\n独立な観測 \\(X_1,\\cdots,X_n\\) の 最尤推定量 とは，モデルの対数尤度 \\[\n\\log p_\\theta\n\\] を通じて定まる次の目的関数 \\(\\ell_n:\\Theta\\to(-\\infty,0)\\) を最大化するような \\(M\\)-推定量 をいう： \\[\n\\ell_n(\\theta;X_1,\\cdots,X_n):=\\sum_{i=1}^n\\log p_\\theta(X_i),\\qquad\\theta\\in\\Theta.\n\\]\n\n\n\n1.1 最尤推定と最適化\nすなわち，最尤推定量とは最適化問題の解として定式化されるのである．\n最大値点であるということは，停留点である必要があるから，微分が零になるという条件を通じて解析的に求まることもある．この \\(Z\\)-推定量としての特徴付けは (Carmer, 1946, p. 498) による．\nまた，計算機的な方法では，Iterative Propertional Fittting や勾配に基づく最適化手法を用いることも考えられる (Robbins and Monro, 1951), (Fletcher, 1987)．\n最尤推定量が解析的に求まらない場面には，代表的には欠測モデルなどがある．欠測モデルは，観測される確率変数 \\(X\\) の他に，観測されない確率変数 \\(Z\\) も想定し，その同時分布を考えるモデルである．これにより，\\((X,Z)\\) 全体には単純な仮定しか置かずとも，\\(X\\) に対して複雑な分布を想定することが可能になるのである．\nこの場合には，モデルの構造を利用して最尤推定量を求めるための MM アルゴリズム (Sun et al., 2016) の例がある．これが EM アルゴリズム (Dempster et al., 1977) である．\n現在でも，その他の MM アルゴリズムが，種々の最適化問題に対する “problem-driven algorithm” であり続けている (T. T. Wu and Lange, 2010)．\n\n\n1.2 最尤推定と Bayes 推定\n最尤推定は，一様事前分布をおいた場合の MAP 推定 とみなせる．この意味で，Bayes 推定の特殊な場合である．\nBayes 推定は MCMC や SMC などのサンプリング法によって統一的に行えるが，殊に MAP 推定に対しては，効率的な最適化法として EM アルゴリズムが使える，ということである．\nより一般の Bayes 推定に対応できるような EM アルゴリズムの一般化が，近似アルゴリズムとして存在する．これが次稿で紹介する 変分推論 である．"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#sec-EM0",
    "href": "posts/2024/Computation/VI2.html#sec-EM0",
    "title": "変分推論２",
    "section": "2 EM アルゴリズム",
    "text": "2 EM アルゴリズム\nEM アルゴリズムは，混合モデルに対する最尤推定アルゴリズムである．一般に，目的関数が \\[\nh(x)=\\operatorname{E}[H(x,Z)]\n\\] と表せる場合に対する特殊な MM アルゴリズムである．2\n\n2.1 欠測データと混合モデル\n欠測データ (incomplete data) とは，２つの確率変数 \\((Z,X)\\) について次の図式が成り立つ際の，\\(Z\\) を潜在変数として，\\(X\\) からの観測とみなせるデータをいう (Dempster et al., 1977, p. 1)：\n\n\n\n\n\n\n図 1: Missing Data Model / Latent State Model / Completed Model for \\(X\\)\n\n\n\nこれは， \\[\np(x|\\theta)=\\int_{\\mathcal{Z}}p(x,z|\\theta)\\,dz\n\\tag{1}\\] という形の尤度を持つモデルである．3\nこれは潜在変数 \\(Z\\) を持つモデルの最も単純な例ともみなせる．特に \\(Z\\) が離散変数である場合，\\(X\\) に対する混合モデルともいう．隠れ Markov モデル はこの発展例である．4\nこのように，\\(X\\) の分布を，潜在変数 \\(Z\\) を追加して理解することを，モデルの 完備化 (completion) または 脱周辺化 (demarginalization)，またはデータの拡張 (data augmentation) ともいう．5\n\n\n2.2 EM アルゴリズム\n値域 \\(\\mathcal{Z}\\) を持つ潜在変数 \\(Z\\) とパラメータ \\(\\theta\\in\\Theta\\) に関して 式 1 で表せる尤度関数 \\(p(x|\\theta)\\) に関して，Jensen の不等式より，任意の \\(x,\\theta\\) で添字づけられた確率密度関数 \\(q:\\mathcal{Z}\\to\\mathbb{R}_+\\)6 とパラメータ \\(\\theta\\in\\Theta\\) について次の評価が成り立つ：\n\\[\n\\begin{align*}\n    \\log p(x|\\theta)&=\\log\\int_{\\mathcal{Z}}p(x,z|\\theta)\\,dz\\\\\n    &\\ge\\int_\\mathcal{Z}q(z|x,\\varphi)\\log\\frac{p(x,z|\\theta)}{q(z|x,\\varphi)}\\,dz\\\\\n    &=:F(q,\\theta).\n\\end{align*}\n\\tag{2}\\]\nこの事実に基づき，\\(F\\) を代理関数として，これを２つの変数 \\(q,\\theta\\) について交互に最大化するという手続きを，EM アルゴリズム という．7\n\n\\(E\\)-ステップ：\\(F\\) を \\(q\\) について最大化する． \\[\n\\begin{align*}\nF(q,\\theta)&=\\int_\\mathcal{Z}q(z|x,\\varphi)\\log\\frac{p(z|x,\\theta)p(x|\\theta)}{q(z|x,\\varphi)}\\,dz\\\\\n&=\\log p(x|\\theta)-\\operatorname{KL}(q_\\varphi,p_\\theta).\n\\end{align*}\n\\] より，\\(q(z|x,\\varphi)=p(z|x,\\theta)\\) で最大化される．8\n\\(M\\)-ステップ：\\(F\\) を \\(\\theta\\) について最大化する． \\[\n\\begin{align*}\nF(q,\\theta)&=\\int_{\\mathcal{Z}}q(z|x,\\varphi)\\log p(x,z|\\theta)\\,dz\\\\\n&\\qquad-\\int_\\mathcal{Z}q(z|x,\\varphi)\\log q(z|x,\\varphi)\\,dz\\\\\n&=\\underbrace{(q_\\varphi dz\\,|\\log p_\\theta)}_{=:Q(\\theta|\\varphi,x)}+H(q_\\varphi)\n\\end{align*}\n\\] より，\\(Q\\) の停留点で最大化される．\n\n総じて，EM アルゴリズムは \\(p,q\\) の KL 乖離度を逐次的に最小化している．\n\n\n2.3 \\(E\\)-ステップの変形\n\\(M\\)-ステップにおける \\(F\\) の \\(\\theta\\) における最大化は \\(Q\\) の \\(\\theta\\) による最大化に等価であるから，\\(E\\)-ステップは結局，事後分布 \\(p(z|x,\\theta)\\) を計算し，これに関する積分である \\[\nQ(\\theta|\\varphi,x)=\\int_\\mathcal{Z}p(z|x,\\theta)\\log p(x,z|\\theta)\\,dz\n\\] を計算する，というステップになる．\nモデル \\(\\{p_\\theta\\}\\) を複雑にしすぎた場合，この \\(Q\\) の計算は困難で実行不可能になってしまう．解析的に \\(E\\)-ステップを実行したい場合，典型的には指数型分布族を仮定する．\nそこで，\\(Q\\) を Monte Carlo 推定量で代替して，それを最大化した場合の EM アルゴリズムを MCEM (Monte Carlo EM) という (Wei and Tanner, 1990a), (Wei and Tanner, 1990b)．典型的には Metropolis-Hastings アルゴリズムを用いることになり (Chau et al., 2021)，これがスケーラビリティ問題を産む．9\nまた，この \\(E\\)-ステップで必ずしも完全な最大化を達成する必要はない (Neal and Hinton, 1998), (Bishop, 2006, p. 454)．従って，\\(p(z|x,\\theta)\\) が複雑すぎる場合，十分近い \\(q\\) を選択してこれに関する積分として \\(Q\\) を近似することが考えられる．特に \\(p\\) を 変分近似 した場合，変分 EM アルゴリズムという (Wainwright and Jordan, 2008, p. 154)．\n\n\n2.4 \\(M\\)-ステップの変形\n\\(Q\\) の停留点を探すにあたって，典型的には微分が消える点を探す．\nしかしこれが難しい場合，厳密な最大化は行わず，代わりにせめて「現状よりは大きくする」ことを実行するアルゴリズムを用いた場合，これを 一般化 EM アルゴリズム (GEM: Generalized EM) ともいう (Bishop, 2006, p. 454), (Hastie et al., 2009, p. 277)．\n例えば，大域的最大化の代わりに条件付き最大化を行うこととする方法 ECM (Expectation Conditional Maximization) などがその例である (Meng and Rubin, 1991), (Meng and Rubin, 1993)．(Christian P. Robert and Casella, 2004, p. 200) も参照．\n\n\n2.5 EM アルゴリズムの有効性\n\n\n\n\n\n\n命題：尤度は単調減少する (Dempster et al., 1977)10\n\n\n\n\\(\\{\\widehat{\\theta}_{(j)}\\}\\subset\\Theta\\) を EM アルゴリズムの \\(M\\)-ステップでの出力列とする．このとき，\n\\[\nL(\\widehat{\\theta}_{(j+1)}|x)\\ge L(\\widehat{\\theta}_{(j)}|x).\n\\] 等号成立は \\[\nQ(\\widehat{\\theta}_{(j+1)}|\\widehat{\\theta}_{(j)},x)=Q(\\widehat{\\theta}_{(j)}|\\widehat{\\theta}_{(j)},x)\n\\] の場合のみ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n命題：局所解への収束 (Boyles, 1983)-(C. F. J. Wu, 1983)11\n\n\n\n\\[\nQ(\\theta|\\theta_0,x):=\\int_\\mathcal{Z}p(z|\\theta,x)\\log p(\\theta|x,z)\\,dz\n\\] は \\(\\theta,\\theta_0\\in\\Theta\\) について連続であるとする．このとき，EM アルゴリズムの出力 \\(\\{\\widehat{\\theta}_(j)\\}\\) は尤度 \\(p(\\theta|x)\\) の停留点に単調に収束する．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\nよって，EM アルゴリズムは局所解には収束する．\nしかし，常に尤度が単調増加するという性質上，局所解に囚われてしまった場合，そこから逃れることはないことになる．\n大域解に収束することを保証したい場合は，異なる初期値で複数回 EM アルゴリズムを実行するか，擬似除冷 (simulated annealing)12 などの別の手法を用いることを考える必要がある (Finch et al., 1989)．"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#em-アルゴリズムの実装gauss-有限混合モデルの場合",
    "href": "posts/2024/Computation/VI2.html#em-アルゴリズムの実装gauss-有限混合モデルの場合",
    "title": "変分推論２",
    "section": "3 EM アルゴリズムの実装（Gauss 有限混合モデルの場合）",
    "text": "3 EM アルゴリズムの実装（Gauss 有限混合モデルの場合）\n負担率に確率モデルを置いた場合，ソフト \\(K\\)-平均アルゴリズム は EM アルゴリズムになる．\nEM アルゴリズムは一般に多峰性に弱いことをここで示す．13\n\n3.1 Guass 有限混合モデル\nここでは，以下の，有限な混合モデルで，さらに混合される分布は正規であるものを考える：\n\n\n\n\n\n\n定義 (Gaussian finite mixture model)\n\n\n\n集合 \\([K]\\) 上に値を取る隠れ変数 \\(Z\\) の確率質量関数を \\((p_k)_{k=1}^K\\) とする． \\[\np(x;(\\mu_k),(\\Sigma_k),(p_k))=\\sum_{k=1}^K p_k\\phi(x;\\mu_k,\\Sigma_k)\n\\tag{3}\\] として定まるモデル \\((p_{(\\mu_k),(\\Sigma_k),(p_k)})\\) を \\(\\mathbb{R}^d\\) 上の Gauss 有限混合モデル という．\nただし，\\(\\phi(x\\); \\(\\mu\\), \\(\\sigma)\\) は \\(\\mathrm{N}_d(\\mu,\\sigma^2)\\) の密度とした．\n\n\n式 3 は \\((X,Z)\\) 上の結合分布の族を表しており，そのパラメータは \\(\\theta:=((\\mu_k),(\\Sigma_k),(p_k))\\) である．さらに，\\(\\theta_k:=(\\mu_k,\\Sigma_k)\\) と定める．\n\n\n3.2 Gauss 有限混合モデルでの EM アルゴリズム\n節 2.2 での議論を，今回の Gauss 有限混合モデルに当てはめてみる．\n対数周辺尤度は\n\\[\n\\begin{align*}\n    \\log p(x|\\theta)&=\\log\\left(\\sum_{k=1}^K q_k\\frac{p_k\\phi(x|\\theta_k)}{q_k}\\right)\\\\\n    &\\ge\\sum_{k=1}^Kq_k\\log\\left(\\frac{p_k\\phi(x|\\theta_k)}{q_k}\\right)\\\\\n    &=:F(q_k,\\theta).\n\\end{align*}\n\\]\nという下界を持つ．\nこれに基づき，観測 \\(\\{x^{(n)}\\}_{n=1}^N\\) と混合 Gauss モデル 3.1 に対する EM アルゴリズムは次の２段階を繰り返す：\n\n\\(E\\)-ステップ： \\[\n\\begin{align*}\n     r_k^{(n)}&\\gets\\operatorname{P}[Z=k|x^{(n)},\\theta]\\\\\n     &=\\frac{p_k\\phi(x^{(n)}|\\theta_k)}{\\sum_{j=1}^Kp_j\\phi(x^{(n)}|\\theta_j)}\n\\end{align*}\n\\] を計算して，\\(F\\) に代入する．\n\\(M\\)-ステップ：\\(F\\) を \\(\\theta\\) について最大化する．これは，次の値を計算することに等しい：\n\n\\[\n\\mu_k\\gets\\frac{\\sum_{n=1}^Nr_k^{(n)}x^{(n)}}{\\sum_{n=1}^Nr_k^{(n)}}\n\\]\n\\[\n\\Sigma_k\\gets\\frac{\\sum_{n=1}^Nr_{k}^{(n)}(x^{(n)}-\\mu_k)(x^{(n)}-\\mu_k)^\\top}{\\sum_{n=1}^Nr_{k}^{(n)}}\n\\]\n\\[\np_k\\gets\\frac{\\sum_{n=1}^Nr_{k}^{(n)}}{N}\n\\]\n\n\n\n\n\n\n\n\n\\(E\\)-ステップの導出\n\n\n\n\n\n最初のステップは Bayes の定理から，\n\\[\n\\begin{align*}\n    \\operatorname{P}[Z=k|x,\\theta]&=\\frac{p(x|z=k,\\theta)p(z=k|\\theta)}{p(x|\\theta)}\\\\\n    &=\\frac{p_k\\phi(x|\\theta_k)}{\\sum_{j=1}^Kp_j\\phi(x|\\theta_j)}.\n\\end{align*}\n\\] の計算に帰着する．\\(\\{p_k\\}\\) が一様で，\\(\\sigma_k=\\sigma\\) も一様であるとき，これはソフト \\(K\\)-平均法における 負担率 \\(r_{kn}\\) に他ならない．このとき， \\[\n\\beta=\\frac{1}{2\\sigma^2}.\n\\]\nこうして，負担率とは，「データ点がそのクラスターに属するという事後確率」としての意味も持つことが判った．\n\n\n\n\n\n\n\n\n\n\\(M\\)-ステップの導出14\n\n\n\n\n\n2,3,4 はそれぞれ条件 \\[\n\\frac{\\partial }{\\partial \\mu_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] \\[\n\\frac{\\partial }{\\partial \\Sigma_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] \\[\n\\frac{\\partial }{\\partial p_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] から出る．\n最大化に Newton-Raphson 法を用いたとも捉えられる．15\n\n\n\n\n\n3.3 \\(K\\)-平均アルゴリズムとの対応\n\\(E\\)-ステップが assignment ステップ，\\(M\\)-ステップが update ステップに対応する．\nハード \\(K\\)-平均法は，歪み尺度 (distortion measure) \\[\nJ(r,\\mu):=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\|x_n-\\mu_k\\|^2\n\\] を \\(r,\\mu\\) のそれぞれについて逐次的に最小化する手法とも見れる．16\n\n\n3.4 Gauss 混合モデルの場合17\n\\(K=2\\) での Gauss 混合分布 \\[\np\\mathrm{N}(\\mu_1,\\sigma^2)+(1-p)\\mathrm{N}(\\mu_2,\\sigma^2),\n\\tag{4}\\] \\[\np=0.7,\\quad\\sigma=1,\n\\] を考える．未知パラメータは \\(\\theta:=(\\mu_1,\\mu_2)\\) である．\n実は，混合モデルでは，ここまで単純な例でさえ，尤度は多峰性を持つ．\n試しに，\\((\\mu_1,\\mu_2)=(0,3.1)\\) として 500 個のデータを生成し，モデル 4 が定める尤度をプロットしてみると，次の通りになる：\n\n\n&lt;&gt;:42: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:43: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:44: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:42: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:43: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:44: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12294/1951455018.py:42: SyntaxWarning: invalid escape sequence '\\m'\n  plt.title('Log Likelihood for 500 Observations with $(\\mu_1,\\mu_2)=(0,3.1)$')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12294/1951455018.py:43: SyntaxWarning: invalid escape sequence '\\m'\n  plt.xlabel('$\\mu_1$')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12294/1951455018.py:44: SyntaxWarning: invalid escape sequence '\\m'\n  plt.ylabel('$\\mu_2$')\n\n\n\n\n\n\n\n\n\n真値 \\((\\mu_1,\\mu_2)=(0,3.1)\\) で確かに最大になるが，\\((\\mu_1,\\mu_2)=(2,-0.5)\\) 付近で極大値を取っていることがわかる．\n\n\n3.5 EM アルゴリズムの初期値依存性\nEM アルゴリズムはその初期値依存性からランダムな初期値から複数回実行してみる必要がある．モデル 4 の場合，その結果は次のようになる：\n\n\nCode\nclass EM_1d:\n    \"\"\"\n    Gauss 有限混合モデルに対する EM アルゴリズム\n\n    Parameters:\n    - K (int): 混合成分の数．デフォルトは2．\n    - max_iter (int): アルゴリズムの最大反復回数．デフォルトは100．\n    - tol (float): 収束の閾値．連続する反復での対数尤度の差がこの値以下になった場合，アルゴリズムは収束したと見なされる．デフォルトは1e-4．\n    \"\"\"\n\n    def __init__(self, K=2, init=None, max_iter=100, tol=1e-4):\n        self.K = K\n        self.max_iter = max_iter\n        self.tol = tol\n\n        self.means = None\n        self.variances = None\n        self.mixing_coefficients = None\n        self.log_likelihood_history = []\n        self.mean_history = []\n        self.initial_value = init\n\n    def expectation(self, X):\n        \"\"\"\n        E ステップ\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        N = X.shape[0]\n        r = np.zeros((N, self.K))\n        for k in range(self.K):\n            pdf = norm.pdf(X, self.means[k], np.sqrt(self.variances[k]))\n            r[:, k] = self.mixing_coefficients[k] * pdf\n        r /= r.sum(axis=1, keepdims=True)\n        return r\n\n    def maximization(self, X, r):\n        \"\"\"\n        M ステップ\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        - r (ndarray): 負担率．\n        \"\"\"\n        N = X.shape[0]\n        Nk = r.sum(axis=0)\n        self.means = (X.T @ r / Nk).T\n        self.variances = np.zeros(self.K)\n        for k in range(self.K):\n            diff = X - self.means[k]\n            self.variances[k] = (r[:, k] @ (diff ** 2)) / Nk[k]\n        self.mixing_coefficients = Nk / N\n\n    def compute_log_likelihood(self, X):\n        \"\"\"\n        対数尤度の計算\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        log_likelihood = 0\n        for x in X:\n            log_likelihood += np.log(np.sum([self.mixing_coefficients[k] * norm.pdf(x, self.means[k], np.sqrt(self.variances[k])) for k in range(self.K)]))\n        return log_likelihood\n    \n    def fit(self, X):\n        \"\"\"\n        EM アルゴリズムの実行\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        N = X.shape[0]\n        np.random.seed(42)\n\n        if self.initial_value is None:\n            random_indeces = np.random.choice(N, self.K, replace=False)\n            self.initial_value = X[random_indeces]\n        self.means = self.initial_value\n        self.initial_value = self.means\n        self.variances = np.ones(self.K)\n        self.mixing_coefficients = np.ones(self.K) / self.K\n\n        # 反復\n        for _ in range(self.max_iter):\n            r = self.expectation(X)\n            self.maximization(X, r)\n            log_likelihood = self.compute_log_likelihood(X)\n            self.log_likelihood_history.append(log_likelihood)\n            self.mean_history.append(self.means)\n\n            if len(self.log_likelihood_history) &gt;= 2 and np.abs(self.log_likelihood_history[-1] - self.log_likelihood_history[-2]) &lt; self.tol:\n                break\n        \n        return self\n\n\n\n\n&lt;&gt;:32: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:33: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:34: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:32: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:33: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:34: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12294/163584773.py:32: SyntaxWarning: invalid escape sequence '\\m'\n  axs[1].set_title('Mean Values Progress ($\\mu_1,\\mu_2$)')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12294/163584773.py:33: SyntaxWarning: invalid escape sequence '\\m'\n  axs[1].set_xlabel('$\\mu_1$')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12294/163584773.py:34: SyntaxWarning: invalid escape sequence '\\m'\n  axs[1].set_ylabel('$\\mu_2$')"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#sec-data-augmentation",
    "href": "posts/2024/Computation/VI2.html#sec-data-augmentation",
    "title": "変分推論２",
    "section": "4 Monte Carlo 法による解決",
    "text": "4 Monte Carlo 法による解決\n式 1 の逆向きの関係 \\[\n\\begin{align*}\n    p(\\theta|x)&=\\int_\\mathcal{Z}p(z,\\theta|x)\\,dz\\\\\n    &=\\int_\\mathcal{Z}p(\\theta|z,x)p(z|x)\\,dz\n\\end{align*}\n\\] も成り立つという 階層構造 (hierarchical structure) を持つモデルにおいて，Bayes 推論が Gibbs サンプラーによって実行できる (C. P. Robert, 1996)．18\nこのような欠測モデルの文脈で Gibbs サンプラーを用いる手法は，データ拡張 の名前でも知られる (Tanner and Wong, 1987)．\n加えて，初期値依存性や局所解へのトラップが懸念されるという EM アルゴリズムの問題点を，MCMC はいずれも持ち合わせていない．\nさらに，混合数 \\(K\\) に関する検定も構成できる (Mengersen and Robert, 1996) など，Gibbs サンプラーひとつで確率モデルに関する種々の情報を取り出せる．\n最尤推定の代わりに Bayes 推定を行なっているため，データ数が少なくとも，過学習の問題が起こりにくいという利点もある．\nBayes 階層モデルは複雑なモデルに対する表現力が高く，地球科学をはじめとして多くの応用分野で使われている (Hrafnkelsson, 2023)．\n\n4.1 Gibbs サンプリング\n高次元な確率変数 \\((U_1,\\cdots,U_K)\\) のシミュレーションを行いたい場合，直接行うのではなく，条件付き分布 \\(p(u_k|u_{-k})\\) からのサンプリングを繰り返すことでこれを行うことが出来る．19\n\n任意の初期値 \\(U_1^{(0)},\\cdots,U_K^{(0)}\\) を与える．\n各 \\(k\\in[K]\\) について， \\[\nU_k^{(t)}\\sim p(u_k|U_{-k}^{(t-1)})\n\\] をサンプリングする．\n十分時間が経過した際，アルゴリズムの出力 \\((U^{(t)}_1,\\cdots,U^{(t)}_K)\\) は \\((U_1,\\cdots,U_K)\\) と同分布になる．\n\n実際，\\(\\{(U_1^{(t)},\\cdots,U_K^{(t)})\\}_{t\\in\\mathbb{N}}\\) はエルゴード的な Markov 連鎖を定め，定常分布 \\(p(U_1,\\cdots,U_K)\\) を持つ．\n\n\n\n\n\n\n命題 (Jean Diebolt and Robert, 1994)20\n\n\n\n\\(p(u_1|u_2)\\) が正，または \\(p(u_2|u_1)\\) が正ならば，Markov 連鎖 \\(\\{U_1^{(t)}\\},\\{U_2^{(t)}\\}\\) はいずれもエルゴード的で，不変分布 \\(p(u_1|u_2),p(u_2|u_1)\\) を持つ．\n\n\n\n\n4.2 確率的 EM アルゴリズム\nGibbs サンプリングアルゴリズムは，EM アルゴリズム2.2の変形とみなせる：\n\n\\(E\\)-ステップ：EM アルゴリズムでは \\[\nQ(\\theta|\\vartheta,x):=(p_\\vartheta dz\\,|\\log p_\\theta)\n\\] を評価するところであったが，Gibbs サンプリングでは，\\(p(z|x,\\vartheta)\\) のサンプリングを行う．\n\\(M\\)-ステップ：EM アルゴリズムでは \\[\n\\operatorname*{argmax}_{\\vartheta\\in\\Theta}Q(\\vartheta|\\theta,x)=(p_\\theta dz\\,|\\log p_\\vartheta)\n\\] を求めるところであったが，Gibbs サンプリングでは，\\(p(\\theta|z,x)\\) のサンプリングを行う．\n\nこれは \\(E\\)-ステップでの \\(Q\\) 関数の評価が困難であるとき，\\(p(z|x,\\theta)\\) からのサンプリングでこれを回避できるという美点もある．\n\n4.2.1 EM アルゴリズムへの部分的な適用：\\(E\\)-ステップ\nまたこの美点のみを用いて，\\(p(z|x,\\theta)\\) からサンプリングをして \\(Q\\) の Monte Carlo 推定量 \\[\nQ(\\theta)=\\frac{1}{M}\\sum_{m=1}^M\\log p(x,z^{(m)}|\\theta)\n\\] を計算し，\\(M\\)-ステップとしてこれを最大化して \\(\\{\\widehat{\\theta}_j\\}\\) を得るという 確率的 EM アルゴリズム (Stochastic EM) も考えられる (Celeux and Diebolt, 1985)．21\nこの場合，\\(\\{\\widehat{\\theta}_j\\}\\) は多くの場合エルゴード的な Markov 連鎖を定めるが，これがどこに収束するかの特定が難しい (J. Diebolt and Ip, 1996)．\n\n\n4.2.2 EM アルゴリズムへの部分的な適用：\\(M\\)-ステップ\nGibbs サンプリングの考え方を \\(M\\)-ステップにのみ導入し，\\(M\\)-ステップを完全に最大化するのではなく「条件付き最大化」に置き換えても，EM アルゴリズム本来の収束性は保たれる．\\(\\theta=(\\theta_1,\\theta_2)\\) と分解できる際に，いずれか片方ずつのみを最大化する，などである．これを ECM (Expectation Conditional Maximization) アルゴリズムという (Meng and Rubin, 1991), (Meng and Rubin, 1993)．\n\\(M\\)-ステップのみを確率的にすることで，EM アルゴリズムの局所解へのトラップを改善することができる．そのような手法の例に，SAME (State Augmentation for Marginal Estimation) (Doucet et al., 2002) などがある．\n\n\n\n4.3 諸言\n欠測モデル2.1のように，一般に グラフィカルモデル として知られる，局所的な関係のみから指定されるモデルや潜在変数を持つモデルでは，Gibbs サンプリングにより効率的に結合分布からサンプリングができる．\nMCMC はグラフィカルモデルを用いた Bayes 推論の，強力な武器である．22"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#footnotes",
    "href": "posts/2024/Computation/VI2.html#footnotes",
    "title": "変分推論２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Carmer, 1946, p. 498) によると，(Fisher, 1912) が初出であるが，以前に Gauss がその特別な形を用いていた．また，(Carmer, 1946, p. 499) での定義はこことは違い，尤度関数の停留点（＝尤度方程式の解）と定義している．↩︎\n(Christian P. Robert and Casella, 2004, p. 174) 式(5.8)．↩︎\n(Christian P. Robert and Casella, 2004, p. 174) 式(5.7)．\\(p\\) を完備化された尤度 (completed likelihood) ともいう．\\(X\\) を incomplete data, \\((X,Z)\\) を complete data ともいう (Bishop, 2006, pp. 433, p.440)．↩︎\n特に，隠れ Markov モデルの文脈では，EM アルゴリズムは Baum-Welch アルゴリズム とも呼ばれる (Chopin and Papaspiliopoulos, 2020, p. 70)．↩︎\nそれぞれ，(Christian P. Robert, 2007, p. 330)，(Christian P. Robert and Casella, 2004, p. 176)，(Hastie et al., 2009, p. 276)，↩︎\n正確には確率核 \\(Q:\\mathcal{X}\\times\\Theta\\to\\mathcal{Z}\\)．↩︎\nこの \\(F\\) は多く \\(Q\\) とも表され，\\(Q\\)-関数ともいう．\\(p(x|\\theta)\\) やその対数は 証拠 (evidence) ともいうので，\\(F\\) は 証拠下界 (ELBO: Evidence Lower BOund) ともいう．↩︎\n式変形は (Bishop, 2006, p. 450) も参照．この \\(p(z|x,\\theta)\\) は観測 \\(x\\) の下での，潜在変数 \\(z\\) の条件付き分布である．しかし，このように双方を最大化ステップと見る変分法的な見方が出来るのである (Wainwright and Jordan, 2008, pp. 153–154), (Neal and Hinton, 1998), (Hastie et al., 2009, p. 277)．よって，この \\(E\\)-ステップも，GEM のように，必ずしも完全な最大化を達成する必要はないことがわかる (Neal and Hinton, 1998), (Bishop, 2006, p. 454)．例えば変分近似を行った場合，変分 EM アルゴリズムができあがる (Wainwright and Jordan, 2008, p. 154)．↩︎\n(Johnston et al., 2024) にも言及あり．↩︎\n(Christian P. Robert and Casella, 2004, p. 177) 定理5.15，(Christian P. Robert, 2007, p. 334) 演習6.52．↩︎\n(Christian P. Robert and Casella, 2004, p. 178) 定理5.16．↩︎\nこの用語は (甘利俊一, 1989, p. 141) の 模擬除冷 の表現に触発された．↩︎\n(Christian P. Robert and Casella, 2004) も参照．↩︎\n(Bishop, 2006, pp. 436–439) も参照．↩︎\n(MacKay, 2003, p. 303)．↩︎\n(Bishop, 2006, p. 424)．↩︎\n(Christian P. Robert and Casella, 2004, pp. 181–182) 例5.19 も参照．↩︎\n(Christian P. Robert, 2007, p. 307) も参照．↩︎\n\\(u_{-k}:=u_{1:(k-1),(k+1):K}\\) とした．↩︎\n(Christian P. Robert, 2007, p. 309) 補題6.3.6，(鎌谷研吾, 2020, p. 139) 定理5.7．↩︎\n(Christian P. Robert and Casella, 2004, p. 200) 5.5.1 節も参照．↩︎\n(Christian P. Robert, 2007, p. 318) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/OT.html",
    "href": "posts/2024/Computation/OT.html",
    "title": "最適輸送",
    "section": "",
    "text": "The goal of this practical session is to introduce computational optimal transport (OT) in Python. you will familiarize yourself with OT by: 1. Computing “exact” unregularized optimal transport, using the Python library POT (Python Optimal Transport). 2. Computing entropic optimal transport, using first your own version of the Sinkhorn algorithm, then the Python library OTT-JAX (https://github.com/ott-jax/ott).\nIn order to lighten the reading of the notebook, we place the functions allowing to perform plots, that are used multiple times, in the section below.\nCode\ndef plot_weighted_points(\n    ax,\n    x, a,\n    y, b,\n    title=None, x_label=None, y_label=None\n):\n  ax.scatter(x[:,0], x[:,1], s=5000*a, c='r', edgecolors='k', label=x_label)\n  ax.scatter(y[:,0], y[:,1], s=5000*b, c='b', edgecolors='k', label=y_label)\n  for i in range(np.shape(x)[0]):\n      ax.annotate(str(i+1), (x[i,0], x[i,1]),fontsize=30,color='black')\n  for i in range(np.shape(y)[0]):\n      ax.annotate(str(i+1), (y[i,0], y[i,1]),fontsize=30,color='black')\n  if x_label is not None or y_label is not None:\n    ax.legend(fontsize=20)\n  ax.axis('off')\n  ax.set_title(title, fontsize=25)\n\ndef plot_assignement(\n    ax,\n    x, a,\n    y, b,\n    optimal_plan,\n    title=None, x_label=None, y_label=None\n):\n  plot_weighted_points(\n    ax=ax,\n    x=x, a=a,\n    y=y, b=b,\n    title=None,\n    x_label=x_label, y_label=y_label\n  )\n  for i in range(optimal_plan.shape[0]):\n      for j in range(optimal_plan.shape[1]):\n          ax.plot([x[i,0], y[j,0]], [x[i,1], y[j,1]], c='k', lw=30*optimal_plan[i,j], alpha=0.8)\n  ax.axis('off')\n  ax.set_title(title, fontsize=30)\n\ndef plot_assignement_1D(\n    ax,\n    x, y,\n    title=None\n):\n  plot_points_1D(\n    ax,\n    x, y,\n    title=None\n  )\n  x_sorted = np.sort(x)\n  y_sorted = np.sort(y)\n  assert len(x) == len(y), \"x and y must have the same shape.\"\n  for i in range(len(x)):\n    ax.hlines(\n        y=0,\n        xmin=min(x_sorted[i], y_sorted[i]),\n        xmax=max(x_sorted[i], y_sorted[i]),\n        color='k',\n        lw=10\n    )\n  ax.axis('off')\n  ax.set_title(title, fontsize=30)\n\ndef plot_points_1D(\n    ax,\n    x, y,\n    title=None\n):\n  n = len(x)\n  a = np.ones(n) / n\n  ax.scatter(x, np.zeros(n), s=1000*a, c='r')\n  ax.scatter(y, np.zeros(n), s=1000*b, c='b')\n  min_val = min(np.min(x), np.min(y))\n  max_val = max(np.max(x), np.max(y))\n  for i in range(n):\n      ax.annotate(str(i+1), xy=(x[i], 0.005), size=30, color='r', ha='center')\n  for j in range(n):\n      ax.annotate(str(j+1), xy=(y[j], 0.005), size=30, color='b', ha='center')\n  ax.axis('off')\n  ax.plot(np.linspace(min_val, max_val, 10), np.zeros(10))\n  ax.set_title(title, fontsize=30)\n\ndef plot_consistency(\n    ax,\n    reg_strengths,\n    plan_diff, distance_diff\n):\n  ax[0].loglog(reg_strengths, plan_diff, lw=4)\n  ax[0].set_ylabel('$||P^* - P_\\epsilon^*||_F$', fontsize=25)\n  ax[1].tick_params(which='both', size=20)\n  ax[0].grid(ls='--')\n  ax[1].loglog(reg_strengths, distance_diff, lw=4)\n  ax[1].set_xlabel('Regularization Strength $\\epsilon$', fontsize=25)\n  ax[1].set_ylabel(r'$ 100 \\cdot \\frac{\\langle C, P^*_\\epsilon \\rangle - \\langle C, P^* \\rangle}{\\langle C, P^* \\rangle} $', fontsize=25)\n  ax[1].tick_params(which='both', size=20)\n  ax[1].grid(ls='--')\n\n\n&lt;&gt;:87: SyntaxWarning: invalid escape sequence '\\e'\n&lt;&gt;:91: SyntaxWarning: invalid escape sequence '\\e'\n&lt;&gt;:87: SyntaxWarning: invalid escape sequence '\\e'\n&lt;&gt;:91: SyntaxWarning: invalid escape sequence '\\e'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12472/867410469.py:87: SyntaxWarning: invalid escape sequence '\\e'\n  ax[0].set_ylabel('$||P^* - P_\\epsilon^*||_F$', fontsize=25)\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_12472/867410469.py:91: SyntaxWarning: invalid escape sequence '\\e'\n  ax[1].set_xlabel('Regularization Strength $\\epsilon$', fontsize=25)"
  },
  {
    "objectID": "posts/2024/Computation/OT.html#i.1-reminders-on-discrete-optimal-transport",
    "href": "posts/2024/Computation/OT.html#i.1-reminders-on-discrete-optimal-transport",
    "title": "最適輸送",
    "section": "1.1 I.1 Reminders on Discrete Optimal Transport",
    "text": "1.1 I.1 Reminders on Discrete Optimal Transport\nOptimal Transport is a theory that allows us to compare two (weighted) points clouds \\((x, a)\\) and \\((y, b)\\), where \\(x \\in \\mathbb{R}^{n \\times d}\\) and \\(y \\in \\mathbb{R}^{m \\times d}\\) are the locations of the \\(n\\) (resp. \\(m\\)) points in dimension \\(d\\), and \\(a \\in \\mathbb{R}^n\\), \\(b \\in \\mathbb{R}^m\\) are the weights. We ask that the total weights sum to one, i.e. \\(\\sum_{i=1}^n a_i = \\sum_{j=1}^m b_j = 1\\).\nThe basic idea of Optimal Transport is to “transport” the mass located at points \\(x\\) to the mass located at points \\(y\\).\nLet us denote by \\(U(a,b) := \\left\\{ P \\in \\mathbb{R}^{n \\times m} \\,|\\, P \\geq 0, \\sum_{j=1}^m P_{ij} = a_i, \\sum_{i=1}^n P_{ij} = b_j\\right\\}\\) the set of admissible transport plans.\nIf \\(P \\in U(a,b)\\), the quantity \\(P_{ij} \\geq 0\\) should be regarded as the mass transported from point \\(x_i\\) to point \\(y_j\\). For this reason, it is called a transport plan.\nWe will also consider a cost function \\(c : \\mathbb{R}^d \\times \\mathbb{R}^d → \\mathbb{R}\\) and the associated cost matrix \\(C = [c(x_i, y_j)]_{1\\leq i,j \\leq n,m}\\in \\mathbb{R}^{n \\times m}\\), containing the pairwise costs between the points of each point cloud \\(x\\) and \\(y\\). The quantity \\(C_{ij}\\) should be regarded as the cost paid for transporting one unit of mass from \\(x_i\\) to \\(y_j\\). This cost is usually computed using the positions \\(x_i\\) and \\(y_j\\), for example \\(C_{ij} = \\|x_i - y_j\\|_2\\) or \\(C_{ij} = \\|x_i - y_j\\|_2^2\\), but may be more exotic in some cases.\nThen transporting mass according to \\(P \\in U(a,b)\\) has a total cost of \\(\\sum_{i,j=1}^n P_{ij} C_{ij}\\).\nIn “Optimal Transport”, there is the word Optimal. Indeed, we want to find a transport plan \\(P \\in U(a,b)\\) that will minimize its total cost. In other words, we want to solve \\[\n    \\min_{P \\in U(a,b)} \\sum_{i,j=1}^n C_{ij }P_{ij} = \\min_{P \\in U(a,b)} ⟨C, P⟩.\n\\]\nThis problem is a Linear Program: the objective function is linear in the variable \\(P\\), and the constraints are linear in \\(P\\). We can thus solve this problem using classical Linear Programming algorithms, such as the simplex algorithm.\nIf \\(P^*\\) is a solution to the Optimal Transport problem, we will say that \\(P^*\\) is an optimal transport plan between \\((x, a)\\) and \\((y, b)\\), and that \\(\\sum_{ij} P^*_{ij} C_{ij}\\) is the optimal transport distance between \\((x, a)\\) and \\((y, b)\\): it is the minimal amount of “energy” that is necessary to transport the initial mass located at points \\(x\\) to the target mass lcoated at points \\(y\\).\nUsually, we represent the weighted point clouds by probability measures \\(\\mu = \\sum_{i=1}^n a_i \\delta_{x_i}\\) and \\(\\nu = \\sum_{j=1}^m b_j \\delta_{y_j}\\). Solving the above problem, we then say that we solve the optimal transport problem between the measures \\(\\mu\\) and \\(\\nu\\). Moreover, we note: \\[\nW_c(\\mu, \\nu) = \\min_{P \\in U(a,b)} ⟨C, P⟩.\n\\]"
  },
  {
    "objectID": "posts/2024/Computation/OT.html#i.2-computing-optimal-croissant-transport",
    "href": "posts/2024/Computation/OT.html#i.2-computing-optimal-croissant-transport",
    "title": "最適輸送",
    "section": "1.2 I.2 Computing Optimal “Croissant” Transport",
    "text": "1.2 I.2 Computing Optimal “Croissant” Transport\n\n1.2.1 Install\nFirst, you need to install a few packages:\n\n\nCode\n%pip install POT\n%pip install cloudpickle\n\n\nCollecting POT\n  Using cached POT-0.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (32 kB)\nRequirement already satisfied: numpy&gt;=1.16 in /Users/hirofumi48/hirofumi/GenAI/lib/python3.12/site-packages (from POT) (1.26.4)\nRequirement already satisfied: scipy&gt;=1.6 in /Users/hirofumi48/hirofumi/GenAI/lib/python3.12/site-packages (from POT) (1.12.0)\nUsing cached POT-0.9.4-cp312-cp312-macosx_11_0_arm64.whl (304 kB)\nInstalling collected packages: POT\nSuccessfully installed POT-0.9.4\nNote: you may need to restart the kernel to use updated packages.\nCollecting cloudpickle\n  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\nUsing cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nInstalling collected packages: cloudpickle\nSuccessfully installed cloudpickle-3.0.0\nNote: you may need to restart the kernel to use updated packages.\n\n\nThen, load the required packages.\n\n\nCode\nimport ot\nimport numpy as np\nimport os\nfrom typing import Callable\nimport matplotlib.pyplot as plt\n\n\nFinally, connect the notebok to your drive to load some data that will be used for the experiments.\n\n\n1.2.2 Formalization of the problem\nWe will solve the Bakeries/Cafés problem of transporting croissants from a number of Bakeries to Cafés.\nWe use fictional positions, production and sale numbers. We impose that the total croissant production is equal to the number of croissants sold, so that Bakeries and Cafés can be represented as measures with the same total mass. Then, up to normalization, they can be processed as probability measures.\nMathematically, we have acess to the position of the \\(m\\) Bakeries as points in \\(\\mathbb{R}^2\\) via \\(x \\in \\mathbb{R}^{n \\times 2}\\) and their respective production via \\(a \\in \\mathbb{R}^m\\) which describe the source point cloud. The Cafés where the croissants are sold are also defined by their position \\(y \\in \\mathbb{R}^{m \\times 2}\\) and the quantity of croissants sold by \\(b \\in \\mathbb{R}^{m}\\).\nAfterwards, the Bakeries are represented by the probability measure \\(\\mu = \\sum_{i=1}^n a_i \\delta_{x_i}\\) and the Cafés by \\(\\nu = \\sum_{j=1}^n b_j \\delta_{y_j}\\). Calculating the optimal assignment of the croissants delivered by the Bakeries to the Cafés remains to calculating the optimal transport between the probability measures \\(\\mu\\) and \\(\\nu\\).\nLet’s download the data and check that the total croissant production is equal to the number of croissants sold.\n\n\nCode\n# Load the data\nimport pickle\nfrom urllib.request import urlopen\nimport cloudpickle as cp\n\ncroissants = cp.load(urlopen('https://marcocuturi.net/data/croissants.pickle'))\n\nbakery_pos = croissants['bakery_pos']\nbakery_prod = croissants['bakery_prod']\ncafe_pos = croissants['cafe_pos']\ncafe_prod = croissants['cafe_prod']\n\nprint('Bakery productions =', bakery_prod)\nprint('Total number of croissants =', bakery_prod.sum())\nprint(\"\")\nprint('Café sales =', cafe_prod)\nprint('Total number of croissants sold =', cafe_prod.sum())\n\n\nBakery productions = [31. 48. 82. 30. 40. 48. 89. 73.]\nTotal number of croissants = 441.0\n\nCafé sales = [82. 88. 92. 88. 91.]\nTotal number of croissants sold = 441.0\n\n\nWe now normalize the weight vectors \\(a\\) and \\(b\\), i.e. the production and the sales, to deal with probability measures.\n\n\nCode\nbakery_prod = bakery_prod / bakery_prod.sum()\ncafe_prod = cafe_prod / cafe_prod.sum()\n\n\nThen, we plot the probability measures (the weighted point clouds) in \\(\\mathbb{R}^2\\).\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 8))\nplot_weighted_points(\n    ax,\n    x=bakery_pos,\n    a=bakery_prod,\n    x_label=\"Bakeries\",\n    y=cafe_pos,\n    y_label=\"Cafés\",\n    b=cafe_prod,\n    title=\"Bakeries and Cafés\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Solving the problem\nTo compute the optimal transport, we will consider three different costs:\n\n\\(\\ell_1\\): \\(c(x, y) = \\|x - y\\|_1\\) , (Manhattan distance)\n\\(\\ell_2\\): \\(c(x, y) = \\|x - y\\|_2\\), (Euclidean distance)\n\\(\\ell_2^2\\): \\(c(x, y) = \\|x - y\\|_2^2\\) (Squared-Euclidean distance)\n\nNote that we expect different optimal transport plans for different costs.\n\nQuestion:\n\nComplete the following function that computes a cost matrix \\(C\\) from two set of points \\(x, y\\) and a cost function \\(c\\). Compute the three costs matrices \\(C_{\\ell_1}, C_{\\ell_2}, C_{\\ell_2^2}\\in \\mathbb{R}^{n \\times m}\\) using that function.\nWhat cost should be used to minimize the total distance traveled by the driver that delivers croissants from Bakeries to Cafés?\n\nAnswer:\n\n\nCode\nbakery_pos\n\n\narray([[184.86464733, 201.8163543 ],\n       [449.3486663 , 168.40784664],\n       [245.41756746, 288.12166576],\n       [273.95400109, 364.68282915],\n       [494.58935376, 336.8424061 ],\n       [738.19305545, 238.70491485],\n       [736.10502372, 375.12298779],\n       [537.74200949, 482.30861653]])\n\n\n\n\nCode\ncafe_pos\n\n\narray([[302.08410452, 442.78633642],\n       [345.1162221 , 368.52123027],\n       [449.226184  , 201.94529124],\n       [454.08464888, 387.95508982],\n       [627.60125204, 408.7770822 ]])\n\n\n\n\nCode\ndef get_cost_matrix(\n    x: np.ndarray,\n    y: np.ndarray,\n    cost_fn: Callable\n) -&gt; np.ndarray:\n  \"\"\"\n  Compute the pairwise cost matrix between the n points in ``x`` and the m points in ``y``.\n  It should output a matrix of size n x m.\n  \"\"\"\n  return np.array([cost_fn(x_,y_) for x_ in x for y_ in y]).reshape(x.shape[0],y.shape[0])\n\n\n# compute cost matrices for different costs\nC_l1 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn= lambda x,y : sum(np.abs(x-y))\n  )\n\nC_l2 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn= lambda x,y : sum((x-y)**2)\n)\nC_l2_sq = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn= lambda x,y : sum(np.sqrt((x-y)**2))\n)\n\n# print shapes of cost matrices\nprint(\n    f\"Shape of C_l1: {C_l1.shape}\\n\"\n    f\"Shape of C_l2: {C_l2.shape}\\n\"\n    f\"Shape of C_l2_sq: {C_l2_sq.shape}\"\n)\n\n\nShape of C_l1: (8, 5)\nShape of C_l2: (8, 5)\nShape of C_l2_sq: (8, 5)\n\n\n\nWe can now compute the Optimal Transport plan to transport the croissants from the bakeries to the cafés, for the three different costs.\n\nQuestion:\n\nComplete the following fuction that takes as input the cost matrix \\(C\\) and the weights vectors \\(a\\) and \\(b\\) and outputs the optimal transport plan and the optimal transport cost using the ot.emd function. It has an option to display the results.\nUse that function to compute and display the optiaml plan and the optimal cost for \\(\\ell_1, \\ell_2\\) and \\(\\ell_2^2\\) geometries.\n\nRemark: See https://pythonot.github.io/ for informations on the ot.emd function.\nAnswer:\n\n\nCode\ndef compute_transport(\n    C: np.ndarray,\n    a: np.ndarray,\n    b: np.ndarray,\n    verbose: bool = False,\n):\n  \"\"\"\n  Compute the optimal transport plan and the optimal transport cost\n  for cost matrix ``C`` and weight vectors $a$ and $b$.\n  If ``verbose`` is set to True, it displays the results.\n  \"\"\"\n  optimal_plan = ot.emd(a,b,C)\n  optimal_cost = np.sum(optimal_plan * C)\n  if verbose:\n    print(\n        f\"optimal transport plan: \\n{optimal_plan}\"\n    )\n    print(\n        f\"transport cost: {optimal_cost}\"\n    )\n  return optimal_plan, optimal_cost\n\n\n\n\nCode\n# l1 geometry\nprint(\"l1 geometry:\")\noptimal_plan_l1_croissant, optimal_cost_l1_croissant = compute_transport(\n    C=C_l1,\n    a=bakery_prod,\n    b=cafe_prod,\n    verbose=True\n)\n\n\nl1 geometry:\noptimal transport plan: \n[[0.07029478 0.         0.         0.         0.        ]\n [0.         0.         0.10884354 0.         0.        ]\n [0.05442177 0.13151927 0.         0.         0.        ]\n [0.         0.06802721 0.         0.         0.        ]\n [0.         0.         0.         0.09070295 0.        ]\n [0.         0.         0.09977324 0.00453515 0.00453515]\n [0.         0.         0.         0.         0.20181406]\n [0.06122449 0.         0.         0.10430839 0.        ]]\ntransport cost: 177.28420815406028\n\n\n\n\nCode\n# l2 geometry\nprint(\"l2 geometry:\")\noptimal_plan_l2_croissant, optimal_cost_l2_croissant = compute_transport(\n    C=C_l2,\n    a=bakery_prod,\n    b=cafe_prod,\n    verbose=True\n)\n\n\nl2 geometry:\noptimal transport plan: \n[[0.         0.07029478 0.         0.         0.        ]\n [0.         0.         0.10884354 0.         0.        ]\n [0.11791383 0.06802721 0.         0.         0.        ]\n [0.06802721 0.         0.         0.         0.        ]\n [0.         0.06122449 0.         0.02947846 0.        ]\n [0.         0.         0.09977324 0.00453515 0.00453515]\n [0.         0.         0.         0.         0.20181406]\n [0.         0.         0.         0.16553288 0.        ]]\ntransport cost: 24576.370543882178\n\n\n\n\nCode\n# squared l2 geometry\nprint(\"squared l2 geometry:\")\noptimal_plan_l2_sq_croissant, optimal_cost_l2_sq_croissant = compute_transport(\n    C=C_l2_sq,\n    a=bakery_prod,\n    b=cafe_prod,\n    verbose=True\n)\n\n\nsquared l2 geometry:\noptimal transport plan: \n[[0.07029478 0.         0.         0.         0.        ]\n [0.         0.         0.10884354 0.         0.        ]\n [0.05442177 0.13151927 0.         0.         0.        ]\n [0.         0.06802721 0.         0.         0.        ]\n [0.         0.         0.         0.09070295 0.        ]\n [0.         0.         0.09977324 0.00453515 0.00453515]\n [0.         0.         0.         0.         0.20181406]\n [0.06122449 0.         0.         0.10430839 0.        ]]\ntransport cost: 177.28420815406028\n\n\n\nNow, we can visualize the assignement induced by each geometry.\n\n\nCode\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_l1_croissant,\n         optimal_plan_l2_croissant,\n         optimal_plan_l2_sq_croissant]\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2.4 In dimension \\(d = 1\\)\nLet assume in this subsection that the cost is of the form \\(c(x, y) = \\|x - y\\|_p^q\\) with \\(p, q \\geq 1\\), which covers the costs we considered in the previous examples, and that the points are in \\(\\mathbb{R}\\), i.e. \\(x_1, ..., x_n, y_1, ... , y_n \\in \\mathbb{R}\\). Then, computing OT boils down to sorting the points. Indeed, for all costs of the above form, the optimal permutation between \\(x\\) and \\(y\\) is \\(\\sigma^* = \\sigma_x^{-1} \\circ \\sigma_y\\) where \\(\\sigma_x\\) is the permutation sorting the \\(x_i\\) and \\(\\sigma_y\\) the one sorting the \\(y_i\\). In particular, one has:\n\\[\nW_c(\\mu, \\nu) = \\frac{1}{n} \\sum_{i=1}^n c(x_i, y_{\\sigma_x^{-1} \\circ \\sigma_y(i)}) = \\frac{1}{n} \\sum_{i=1}^n c(x_{\\sigma_x(i)}, y_{\\sigma_y(i)})\n\\]\nThus, to compute the optimal transport cost, it is sufficient to sort \\(x\\) and \\(y\\).\nLet’s check this fact on an example, by comparing the transport cost obtained by sorting the points to the one obtained with the function ot.emd. To simplify, we generate points \\(x,y \\subset \\mathbb{R}\\) s.t. \\(x\\) is sorted, i.e. \\(\\sigma_x = I_d\\) and then \\(\\sigma^*=\\sigma_y\\). Therefore, computing the optimal assignement amounts to sort \\(y\\).\n\n\nCode\n# generate points\nn = 5\nx = np.arange(0, 2*n, 2) + .25 * np.random.normal(size=(n,))\na = np.ones(n) / n\ny = np.arange(1, 2*n+1, 2) + .25 * np.random.normal(size=(n,))\nnp.random.shuffle(y)\nb = np.ones(n) / n\n\n# plot points\nfig, ax = plt.subplots(figsize=(12, 6))\nplot_points_1D(\n    ax,\n    x, y,\n    title=\"1D points\"\n)\n\n\n\n\n\n\n\n\n\n\nQuestion:\n\nFor \\(\\ell_1\\) and \\(\\ell_2^2\\) geometries (\\(\\ell_2\\) and \\(\\ell_1\\) coincides on \\(\\mathbb{R}\\)), compute the optimal assignement and optimal transport cost by sorting \\(y\\). Put the assignement into a vector \\(s \\in \\mathbb{R}^n\\), s.t. \\(x_i\\) is mapped to \\(y_{s_i}\\), i.e. \\(s_i = \\sigma^*(i)\\). Is it different according to the geometry?\nPut now the assignment you obtained by sorting the points in the form of a transport plan \\(P^* \\in \\mathbb{R}^{n \\times n}\\). Check that you obtain the results with ot.emd.\n\nAnswer:\n\n\nCode\n# sort the points\ny_sorted = np.sort(y)\n\n# get optimal assignment as a vector\nassignment = np.argsort(y)\n\n# transform it to a transport plan\noptimal_plan = np.zeros((n,n))\nfor i, idx in enumerate(assignment):\n    optimal_plan[i, idx] = 1 / n\nprint(\n    f\"optimal transport plan obtained by sorting the points:\\n {optimal_plan}\"\n)\n\n# The result doesn't match the lecturer's\n\n\noptimal transport plan obtained by sorting the points:\n [[0.2 0.  0.  0.  0. ]\n [0.  0.  0.  0.2 0. ]\n [0.  0.  0.2 0.  0. ]\n [0.  0.  0.  0.  0.2]\n [0.  0.2 0.  0.  0. ]]\n\n\n\n\nCode\n# l1 geometry\nprint(\"l1 geometry:\")\nC_l1 = get_cost_matrix(\n    x=x, y=y,\n    cost_fn=lambda x,y: np.sum(np.abs(x - y))\n)\noptimal_plan_l1, optimal_cost_l1 = compute_transport(\n    C=C_l1,\n    a=a,\n    b=b,\n    verbose=True\n)\nprint(\n    f\"is it equal to the one obtained by sorting the points? \"\n    f\"{np.array_equal(optimal_plan_l1, optimal_plan)}\"\n)\n\n\nl1 geometry:\noptimal transport plan: \n[[0.2 0.  0.  0.  0. ]\n [0.  0.  0.  0.2 0. ]\n [0.  0.  0.2 0.  0. ]\n [0.  0.  0.  0.  0.2]\n [0.  0.2 0.  0.  0. ]]\ntransport cost: 1.0741774732734841\nis it equal to the one obtained by sorting the points? True\n\n\n\n\nCode\n# squared l2 geometry\n\ndef is_permutation(matrix):\n    \"\"\"\n    Check if a given matrix is a permutation matrix.\n    \"\"\"\n    n, m = matrix.shape\n    if n != m:\n        return False\n    \n    row_sum = np.sum(matrix, axis=1)\n    col_sum = np.sum(matrix, axis=0)\n    \n    return np.all(row_sum == 1) and np.all(col_sum == 1) and np.all((matrix == 0) | (matrix == 1))\n\nC_l2_sq = get_cost_matrix(\n    x=x, y=y,\n    cost_fn=lambda x,y: np.sum((x - y) ** 2)\n)\noptimal_plan_l2_sq, optimal_cost_l2_sq = compute_transport(\n    C=C_l2_sq,\n    a=a,\n    b=b,\n    verbose=True\n)\nprint(\n    f\"is permutation matrix? {is_permutation(optimal_plan_l2_sq)}\"\n)\nprint(\n    f\"is it equal to the one obtained by sorting the points? \"\n    f\"{np.array_equal(optimal_plan_l2_sq, optimal_plan)}\"\n)\n\n\noptimal transport plan: \n[[0.2 0.  0.  0.  0. ]\n [0.  0.  0.  0.2 0. ]\n [0.  0.  0.2 0.  0. ]\n [0.  0.  0.  0.  0.2]\n [0.  0.2 0.  0.  0. ]]\ntransport cost: 1.1915868174068251\nis permutation matrix? False\nis it equal to the one obtained by sorting the points? True\n\n\n\nFinally, one can plot the assignement.\n\n\nCode\nfig, ax = plt.subplots(figsize=(12, 6))\nplot_assignement_1D(\n    ax,\n    x, y,\n    title=\"1D assignement\"\n)\nplt.show()"
  },
  {
    "objectID": "posts/2024/Computation/OT.html#ii.1-reminders-on-sinkhorn-algorithm",
    "href": "posts/2024/Computation/OT.html#ii.1-reminders-on-sinkhorn-algorithm",
    "title": "最適輸送",
    "section": "2.1 II.1 Reminders on Sinkhorn Algorithm",
    "text": "2.1 II.1 Reminders on Sinkhorn Algorithm\n\n2.1.1 Adding negative entropy as a regularizer\nIn real ML applications, we often deal with large numbers of points. In this case, cubic complexity linear programming algorithms are too costly. This motivates (among other reasons) the regularized approach \\[\n    \\min_{P \\in \\mathcal{U}(a,b)} \\langle C, P \\rangle + \\epsilon \\sum_{ij} P_{ij} [ \\log(P_{ij}) - 1].\n\\] For \\(\\epsilon\\) is sufficiently small, one expects to recover an approximation of the original optimal transport plan.\n\n\n2.1.2 The Sinkhorn iterates\nIn order to solve this problem, one can remark that the optimality conditions imply that a solution \\(P_\\epsilon^*\\) necessarily is of the form \\(P_\\epsilon^* = \\text{diag}(u) \\, K \\, \\text{diag}(v)\\), where \\(K = \\exp(-C/\\epsilon)\\) and \\(u,v\\) are two non-negative vectors.\n\\(P_\\epsilon^*\\) should verify the constraints, i.e. \\(P_\\epsilon^* \\in U(a,b)\\), so that \\[\n    P_\\epsilon^* 1_m = a \\text{  and  } (P_\\epsilon^*)^T 1_n = b\n\\] which can be rewritten as \\[\n    u \\odot (Kv) = a \\text{  and  } v \\odot (K^T u) = b\n\\]\nThen Sinkhorn’s algorithm alternates between the resolution of these two equations, and reads at iteration \\(t\\): \\[\n    u^{t+1} \\leftarrow \\frac{a}{Kv^t} \\text{  and  } v^{t+1} \\leftarrow \\frac{b}{K^T u^{t+1}}\n\\]\n\n\n2.1.3 Initialization and convergence\nUsually, it starts from \\(v^{0} = \\mathrm{1}_m\\) and alternate the above updates until \\(\\|u^{t+1} \\odot (Kv^{t+1}) - a\\|_1 + \\|v^{t+1} \\odot (K^T u^{t+1}) - b\\|_1 \\leq \\tau\\), where \\(\\tau &gt; 0\\) is a fixed convergence threshold. Actually, since at the end of each iteration, one exactly has \\(v^{t+1} \\odot (K^T u^{t+1}) = b\\), it just remains to test if \\(\\|u^{t+1} \\odot (Kv^{t+1}) - a\\|_1 \\leq \\tau\\).\nFrom an entropic optimal transport plan \\(P^*_\\epsilon\\), we can approximate the optimal transport cost by \\(\\sum_{i,j=1}^n P^*_{\\epsilon_{ij}} C_{ij} = ⟨C, P^*_\\epsilon⟩\\). For the rest of the section, we call this quantity the entropic optimal transport cost."
  },
  {
    "objectID": "posts/2024/Computation/OT.html#ii.2-using-your-own-sinkhorn",
    "href": "posts/2024/Computation/OT.html#ii.2-using-your-own-sinkhorn",
    "title": "最適輸送",
    "section": "2.2 II.2 Using your own Sinkhorn",
    "text": "2.2 II.2 Using your own Sinkhorn\n\n2.2.1 Sinkhorn Implementation\nIn this section, you will implement your own version of the Sinkhorn Algorithm.\n\nQuestion: Complete the following Sinkhorn algorithm, by:\n\nComputing the kernel matrix \\(K = \\exp(-C / \\epsilon)\\),\nStarting from \\(v^{0} = \\mathrm{1}_m\\),\nAlternating the updates \\(u^{t+1} \\odot (Kv^t) = a\\) and \\(v^{t+1} \\odot (K^T u^{t+1}) = b\\),\nDeclaring convergence when \\(\\|u^t \\odot (Kv^t) - a\\|_1 + \\|v^t \\odot (K^T u^t) - b\\|_1 \\leq \\tau\\).\n\nRemark: you should also use also a maximum number of iterations max_iter, to stop the algorithm after a fixed number of iterations if the convergence is not reached.\nAnswer:\n\n\nCode\ndef sinkhorn(\n    a: np.ndarray,\n    b: np.ndarray,\n    C: np.ndarray,\n    epsilon: float,\n    max_iters: int = 100,\n    tau: float = 1e-4\n) -&gt; np.ndarray:\n    \"\"\"\n    Sinnkhorn's algorithm. It should output the optimal transport plan.\n    \"\"\"\n\n    K = np.exp( -C / epsilon )\n    n, m = a.shape[0], b.shape[0]\n    v = np.ones((m,))\n    for _ in range(max_iters):\n        u = a / K.dot(v)\n        v = b / K.transpose().dot(u)\n    return u[:,None] * v[None,:] * K  # u_i, v_j, K_ij\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=1)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\n[0.2 0.2 0.2 0.2 0.2]\n[0.19808311 0.19848551 0.19985837 0.20054246 0.20303055]\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=1, max_iters=1000)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\n[0.2 0.2 0.2 0.2 0.2]\n[0.19999998 0.19999998 0.19999999 0.2        0.20000005]\n\n\n\n\nCode\ndef sinkhorn(\n    a: np.ndarray,\n    b: np.ndarray,\n    C: np.ndarray,\n    epsilon: float,\n    max_iters: int = 100,\n    tau: float = 1e-4\n) -&gt; np.ndarray:\n    \"\"\"\n    Sinnkhorn's algorithm. It should output the optimal transport plan.\n    \"\"\"\n\n    K = np.exp( -C / epsilon )\n    n, m = a.shape[0], b.shape[0]\n    v = np.ones((m,))\n    for i in range(max_iters):\n        u = a / K.dot(v)\n        v = b / K.transpose().dot(u)\n        if i % 10 == 0:\n            # compute row sum D(u) K D(v) = u * Kv\n            if np.sum(np.abs(u * K.dot(v) - a)) &lt; tau:\n                print('early termination: ' + str(i))\n                break\n    return u[:,None] * v[None,:] * K  # u_i, v_j, K_ij\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=1, max_iters=1000)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\nearly termination: 430\n[0.2 0.2 0.2 0.2 0.2]\n[0.19997956 0.19998222 0.1999924  0.19999835 0.20004747]\n\n\n\n\nCode\nP = sinkhorn(a, b, C_l2_sq, epsilon=0.1, max_iters=10000)\nprint(P.sum(axis=0))\nprint(P.sum(axis=1))\n\n\n[0.2 0.2 0.2 0.2 0.2]\n[0.19995986 0.19997993 0.2        0.20002007 0.20004014]\n\n\n\nNow, we can test the Sinkhorn algorithm on the “croissant” transport example.\n\nQuestion: * Complete the following fuction that takes as input the cost matrix \\(C\\) and the weights vectors \\(a\\) and \\(b\\) and outputs the entropic optimal transport plan and the entropic optimal transport cost using the sinkhorn function. As for the exact transport, it has an option to display the results. * Use that function on the croissant transport to compute and display the optimal plan and the optimal cost for the \\(\\ell_1, \\ell_2\\) and \\(\\ell_2^2\\) geometries. * Each time you run the Sinkhorn algorithm, you should use \\(\\epsilon = 0.1 \\cdot \\bar{C}\\), with \\(\\bar{C} = \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m C_{ij}\\) is the mean of the cost matrix. It remains to adapt the \\(\\epsilon\\) value according to the cost matrix, to control the magnitude of the entries of \\(C / \\epsilon\\). Why this strategy? What will happen if \\(\\epsilon\\) is too small compared to the entries of \\(C\\)?\nAnswer:\n\n\nCode\ndef compute_transport_sinkhorn(\n    C: np.ndarray,\n    a: np.ndarray,\n    b: np.ndarray,\n    epsilon: float,\n    max_iters: int = 10_000,\n    tau: float = 1e-4,\n    verbose: bool = False,\n):\n  \"\"\"\n  Compute the entropic optimal transport plan and the entropic optimal transport cost\n  for cost matrix ``C`` and weight vectors $a$ and $b$.\n  If ``verbose`` is set to True, it displays the results.\n  \"\"\"\n  optimal_plan_sinkhorn = sinkhorn(a, b, C, epsilon, max_iters, tau)\n  optimal_cost_sinkhorn = np.sum(optimal_plan_sinkhorn * C)\n  if verbose:\n    print(\n        f\"entropic optimal transport plan: \\n{optimal_plan_sinkhorn}\"\n    )\n    print(\n        f\"entropic transport cost: {optimal_cost_sinkhorn}\"\n    )\n  return optimal_plan_sinkhorn, optimal_cost_sinkhorn\n\n\n\n\nCode\n# l1 geometry\nprint(\"l1 geometry:\")\nC_l1 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn=lambda x,y: np.sum(np.abs(x - y))\n)\nepsilon = 1\noptimal_plan_sinkhorn_l1_croissant, optimal_cost_sinkhorn_l1_croissant = compute_transport_sinkhorn(\n    C=C_l1,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=True,\n)\n\n\nl1 geometry:\nearly termination: 5970\nentropic optimal transport plan: \n[[2.70428936e-002 4.32583290e-002 1.34214268e-047 1.83509051e-086\n  1.62880160e-235]\n [3.42773260e-084 1.30691077e-046 1.08827539e-001 1.90100995e-040\n  1.68731081e-189]\n [7.15328153e-002 1.14425257e-001 4.99347632e-122 4.85411038e-086\n  4.30844293e-235]\n [2.61705422e-002 4.18628990e-002 5.77469196e-189 1.77589404e-086\n  1.57625961e-235]\n [1.25908361e-049 4.80057848e-012 2.70172554e-084 9.07098043e-002\n  1.22406524e-115]\n [2.66904088e-052 1.01764013e-014 9.97892408e-002 1.92289196e-004\n  8.84651301e-003]\n [5.95876320e-051 4.18968529e-019 7.18872709e-119 4.29294956e-003\n  1.97502693e-001]\n [6.11947921e-002 7.27950530e-029 1.24902883e-128 1.04351442e-001\n  5.20381800e-060]]\nentropic transport cost: 177.27648952346257\n\n\n\n\nCode\nplt.imshow(optimal_plan_sinkhorn_l1_croissant)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# l2 geometry\nprint(\"l2 geometry:\")\nC_l2 = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn=lambda x,y: np.linalg.norm(x - y, ord=2)\n)\nepsilon = np.mean(C_l2_sq) * 0.05 # compute the optimal value to avoid underflow\noptimal_plan_sinkhorn_l2_croissant, optimal_cost_sinkhorn_l2_croissant = compute_transport_sinkhorn(\n    C=C_l2,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=True\n)\n\n\nl2 geometry:\nearly termination: 7630\nentropic optimal transport plan: \n[[1.45723094e-002 5.57286119e-002 4.52226467e-078 6.57135558e-098\n  4.90090011e-253]\n [2.03126664e-078 1.29205714e-050 1.08836101e-001 7.18737300e-090\n  1.30762518e-199]\n [4.21394722e-002 1.43817785e-001 5.23057723e-111 1.75969152e-101\n  4.52621359e-261]\n [6.80328677e-002 3.53699442e-015 4.37006861e-166 4.82784371e-119\n  1.41868621e-281]\n [3.95246793e-025 8.87825346e-008 1.14170135e-059 9.07093831e-002\n  4.93816071e-115]\n [8.50518905e-030 4.11363436e-012 9.97806792e-002 4.48802273e-003\n  4.56760571e-003]\n [2.94525670e-054 2.68613937e-047 6.03760182e-074 1.51890993e-029\n  2.01781601e-001]\n [6.11963938e-002 2.70578880e-013 5.19780707e-110 1.04349079e-001\n  1.09181366e-061]]\nentropic transport cost: 139.5029250047335\n\n\n\n\nCode\n# squared l2 geometry\nprint(\"squared l2 geometry:\")\nC_l2_sq = get_cost_matrix(\n    x=bakery_pos, y=cafe_pos,\n    cost_fn=lambda x,y: np.sum((x - y) ** 2)\n)\nepsilon = np.mean(C_l2_sq) * 0.05 # compute the optimal value to avoid underflow\noptimal_plan_sinkhorn_l2_sq_croissant, optimal_cost_sinkhorn_l2_sq_croissant = compute_transport_sinkhorn(\n    C=C_l2_sq,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=True\n)\n\n\nsquared l2 geometry:\nearly termination: 390\nentropic optimal transport plan: \n[[9.15185856e-03 6.11459020e-02 2.53416007e-06 9.36830019e-11\n  5.88710931e-33]\n [1.98312263e-09 2.86298466e-05 1.08801507e-01 2.62245737e-07\n  1.22762517e-18]\n [1.02592189e-01 8.33635462e-02 3.95058637e-08 1.25018168e-08\n  7.16059834e-28]\n [6.36539082e-02 4.37874407e-03 9.20213996e-12 8.38044497e-09\n  1.80226054e-26]\n [1.15178824e-03 4.78786088e-02 4.40333811e-04 4.12385828e-02\n  1.04144097e-10]\n [1.27912352e-15 1.00116432e-09 9.93719135e-02 7.28353201e-04\n  8.73056135e-03]\n [5.51097456e-13 1.50024181e-09 4.51186568e-07 4.17827621e-03\n  1.97618514e-01]\n [9.39129734e-03 2.75105187e-03 4.51869851e-10 1.53400990e-01\n  1.31200779e-07]]\nentropic transport cost: 24883.330683517215\n\n\n\n\n\n2.2.2 The effect of \\(\\epsilon\\)\nNow we can display the transportation plans obtained with Sinkhorn’s algortihm, as we did for the exact OT.\n\n\nCode\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_sinkhorn_l1_croissant,\n         optimal_plan_sinkhorn_l2_croissant,\n         optimal_plan_sinkhorn_l2_sq_croissant]\n\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\n\n\n\n\n\n\n\nNote: There always is some transport at every edge in Sinkhorn algorithm’s output.\n\n\nCode\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_l1_croissant,\n         optimal_plan_l2_croissant,\n         optimal_plan_l2_sq_croissant]\n\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\n\n\n\n\n\n\n\nThe above transport plans are obtained for \\(\\epsilon = 0.1 \\cdot \\bar{C}\\). Let’s increase epsilon to \\(\\epsilon = 10 \\cdot \\bar{C}\\) and replot the optimal transport plans to visualize the effect of epsilon.\n\n\nCode\n# l1 geometry\nepsilon = 10 * np.mean(C_l1)\noptimal_plan_sinkhorn_l1_croissant, optimal_cost_sinkhorn_l1_croissant = compute_transport_sinkhorn(\n    C=C_l1,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=False,\n)\n\n# l2 geometry\nepsilon = 10 * np.mean(C_l2)\noptimal_plan_sinkhorn_l2_croissant, optimal_cost_sinkhorn_l2_croissant = compute_transport_sinkhorn(\n    C=C_l2,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=False\n)\n\n# squared l2 geometry\nepsilon = 10 * np.mean(C_l2_sq)\noptimal_plan_sinkhorn_l2_sq_croissant, optimal_cost_sinkhorn_l2_sq_croissant = compute_transport_sinkhorn(\n    C=C_l2_sq,\n    a=bakery_prod,\n    b=cafe_prod,\n    epsilon=epsilon,\n    verbose=False\n)\n\nfig, ax = plt.subplots(\n    1, 3, figsize=(9*3, 7)\n)\nplans = [optimal_plan_l1_croissant,\n         optimal_plan_l2_croissant,\n         optimal_plan_l2_sq_croissant]\ntitles = [r\"$\\ell_1$ geometry\", r\"$\\ell_2$ geometry\", r\"$\\ell_2^2$ geometry\"]\n\nfor axes, plan, title in zip(ax, plans, titles):\n  plot_assignement(\n      ax=axes,\n      x=bakery_pos, a=bakery_prod, x_label=\"Bakeries\",\n      y=cafe_pos, b=cafe_prod, y_label=\"Cafés\",\n      optimal_plan=plan,\n      title=title\n  )\nplt.show()\n\n\nearly termination: 10\nearly termination: 10\nearly termination: 10\n\n\n\n\n\n\n\n\n\nNote: If the epsilon is large, the distribution is close to uniform.\n\nQuestion: What do you observe in relation to the transport plans obtained for the exact optimal transport?\nAnswer:\n\n\n\n2.2.3 Sinkhorn consistency\nWe now show that this Sinkhorn algorithm is consistent with classical optimal transport, using the “croissant” transport example and focusing on the \\(\\ell_2\\) cost.\n\nQuestion: Complete the following code to compute, for various \\(\\epsilon'\\), values on a regular grid: * Set \\(\\epsilon = \\epsilon' \\cdot \\bar{C}\\), * The deviation of the entropic optimal plan \\(P^*_\\epsilon\\) to the exact optimal plan \\(P^*\\), namely \\(\\|P^*_\\epsilon - P^*\\|_2\\). * The deviation of the entropic optimal cost \\(\\langle C, P^*_\\epsilon \\rangle\\) to the exact optimal plan \\(\\langle C, P^*_\\epsilon \\rangle\\), namely: \\(\\langle C, P^*_\\epsilon \\rangle - \\langle C, P^* \\rangle\\).\nWe remind that the excat optimal transport plan for the \\(\\ell_2\\) cost is stored as variable optimal_plan_l2_croissant.\nAnswer:\n\n\nCode\nplan_diff = []\ndistance_diff = []\ngrid = np.linspace(0.01, 5, 100)\nfor epsilon_prime in grid:\n  epsilon = epsilon_prime * np.mean(C_l2)\n  optimal_plan_sinkhorn_l2_croissant, optimal_cost_sinkhorn_l2_croissant = compute_transport_sinkhorn(\n      C=C_l2,\n      a=bakery_prod,\n      b=cafe_prod,\n      epsilon=epsilon,\n      verbose=False\n  )\n  assert optimal_cost_sinkhorn_l2_croissant != np.nan, (\n      \"Optimal cost is nan due to numerical instabilities.\"\n  )\n  plan_diff.append(\n      np.sum(np.abs(optimal_plan_sinkhorn_l2_croissant - optimal_plan_l2_croissant))\n  )\n  distance_diff.append(\n      optimal_cost_sinkhorn_l2_croissant - optimal_cost_l2_croissant\n  )\n\n\nearly termination: 2460\nearly termination: 220\nearly termination: 50\nearly termination: 30\nearly termination: 20\nearly termination: 20\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\nearly termination: 10\n\n\n\nNow, let’s plot the results.\n\n\nCode\nfig, ax = plt.subplots(2, 1, figsize=(16, 5*2))\nreg_strengths = np.mean(C_l2) * grid\nplot_consistency(\n    ax,\n    reg_strengths,\n    plan_diff,\n    distance_diff\n)\n\nplt.show()\n\n\n/Users/hirofumi48/hirofumi/GenAI/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nNote: The result is different from the lecturer’s."
  },
  {
    "objectID": "posts/2024/Computation/OT.html#ii.3-using-ott",
    "href": "posts/2024/Computation/OT.html#ii.3-using-ott",
    "title": "最適輸送",
    "section": "2.3 II.3 Using OTT",
    "text": "2.3 II.3 Using OTT\n\n2.3.1 Install OTT\nFirst, you need to install OTT.\n\n\nCode\n%pip install ott-jax\n\n\nCollecting ott-jax\n  Using cached ott_jax-0.4.7-py3-none-any.whl.metadata (20 kB)\nCollecting jax&gt;=0.4.0 (from ott-jax)\n  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\nCollecting jaxopt&gt;=0.8 (from ott-jax)\n  Using cached jaxopt-0.8.3-py3-none-any.whl.metadata (2.6 kB)\nCollecting lineax&gt;=0.0.5 (from ott-jax)\n  Using cached lineax-0.0.5-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy&gt;=1.20.0 in /Users/hirofumi48/hirofumi/GenAI/lib/python3.12/site-packages (from ott-jax) (1.26.4)\nCollecting jaxlib&lt;=0.4.31,&gt;=0.4.30 (from jax&gt;=0.4.0-&gt;ott-jax)\n  Downloading jaxlib-0.4.31-cp312-cp312-macosx_11_0_arm64.whl.metadata (983 bytes)\nCollecting ml-dtypes&gt;=0.2.0 (from jax&gt;=0.4.0-&gt;ott-jax)\n  Using cached ml_dtypes-0.4.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\nCollecting opt-einsum (from jax&gt;=0.4.0-&gt;ott-jax)\n  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: scipy&gt;=1.10 in /Users/hirofumi48/hirofumi/GenAI/lib/python3.12/site-packages (from jax&gt;=0.4.0-&gt;ott-jax) (1.12.0)\nCollecting equinox&gt;=0.11.3 (from lineax&gt;=0.0.5-&gt;ott-jax)\n  Using cached equinox-0.11.4-py3-none-any.whl.metadata (18 kB)\nCollecting jaxtyping&gt;=0.2.20 (from lineax&gt;=0.0.5-&gt;ott-jax)\n  Using cached jaxtyping-0.2.33-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /Users/hirofumi48/hirofumi/GenAI/lib/python3.12/site-packages (from lineax&gt;=0.0.5-&gt;ott-jax) (4.12.2)\nCollecting typeguard==2.13.3 (from jaxtyping&gt;=0.2.20-&gt;lineax&gt;=0.0.5-&gt;ott-jax)\n  Using cached typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nUsing cached ott_jax-0.4.7-py3-none-any.whl (279 kB)\nDownloading jax-0.4.31-py3-none-any.whl (2.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/2.0 MB ? eta -:--:--   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/2.0 MB 8.3 MB/s eta 0:00:01   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/2.0 MB 9.5 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 1.0/2.0 MB 9.6 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 1.4/2.0 MB 9.7 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 1.8/2.0 MB 10.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 9.7 MB/s eta 0:00:00\nUsing cached jaxopt-0.8.3-py3-none-any.whl (172 kB)\nUsing cached lineax-0.0.5-py3-none-any.whl (66 kB)\nUsing cached equinox-0.11.4-py3-none-any.whl (175 kB)\nDownloading jaxlib-0.4.31-cp312-cp312-macosx_11_0_arm64.whl (70.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/70.1 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.3/70.1 MB 9.9 MB/s eta 0:00:08   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.7/70.1 MB 10.9 MB/s eta 0:00:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/70.1 MB 11.2 MB/s eta 0:00:07   ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/70.1 MB 11.5 MB/s eta 0:00:06   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/70.1 MB 11.1 MB/s eta 0:00:07   ━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/70.1 MB 11.1 MB/s eta 0:00:07   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/70.1 MB 11.3 MB/s eta 0:00:06   ━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/70.1 MB 11.4 MB/s eta 0:00:06   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/70.1 MB 11.3 MB/s eta 0:00:06   ━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/70.1 MB 11.3 MB/s eta 0:00:06   ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/70.1 MB 11.3 MB/s eta 0:00:06   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/70.1 MB 11.4 MB/s eta 0:00:06   ━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/70.1 MB 11.4 MB/s eta 0:00:06   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/70.1 MB 11.5 MB/s eta 0:00:06   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/70.1 MB 11.3 MB/s eta 0:00:06   ━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.9/70.1 MB 11.3 MB/s eta 0:00:06   ━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/70.1 MB 11.2 MB/s eta 0:00:06   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/70.1 MB 11.3 MB/s eta 0:00:06   ━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/70.1 MB 11.2 MB/s eta 0:00:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/70.1 MB 11.3 MB/s eta 0:00:06   ━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/70.1 MB 11.4 MB/s eta 0:00:06   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/70.1 MB 11.4 MB/s eta 0:00:06   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/70.1 MB 11.4 MB/s eta 0:00:06   ━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/70.1 MB 11.4 MB/s eta 0:00:06   ━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/70.1 MB 11.4 MB/s eta 0:00:06   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/70.1 MB 11.5 MB/s eta 0:00:06   ━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/70.1 MB 11.6 MB/s eta 0:00:06   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/70.1 MB 11.5 MB/s eta 0:00:06   ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.9/70.1 MB 11.4 MB/s eta 0:00:06   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/70.1 MB 11.4 MB/s eta 0:00:05   ━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/70.1 MB 11.6 MB/s eta 0:00:05   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/70.1 MB 11.3 MB/s eta 0:00:05   ━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/70.1 MB 11.3 MB/s eta 0:00:05   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.0/70.1 MB 11.3 MB/s eta 0:00:05   ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.5/70.1 MB 11.3 MB/s eta 0:00:05   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.9/70.1 MB 11.2 MB/s eta 0:00:05   ━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/70.1 MB 11.0 MB/s eta 0:00:05   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/70.1 MB 11.3 MB/s eta 0:00:05   ━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/70.1 MB 11.3 MB/s eta 0:00:05   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/70.1 MB 11.1 MB/s eta 0:00:05   ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.1/70.1 MB 11.1 MB/s eta 0:00:05   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.4/70.1 MB 11.1 MB/s eta 0:00:05   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/70.1 MB 10.8 MB/s eta 0:00:05   ━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/70.1 MB 10.8 MB/s eta 0:00:05   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/70.1 MB 10.6 MB/s eta 0:00:05   ━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.9/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.4/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.7/70.1 MB 10.6 MB/s eta 0:00:05   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.2/70.1 MB 10.6 MB/s eta 0:00:05   ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 22.6/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.0/70.1 MB 10.6 MB/s eta 0:00:05   ━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.4/70.1 MB 10.6 MB/s eta 0:00:05   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.9/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.4/70.1 MB 10.8 MB/s eta 0:00:05   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 24.9/70.1 MB 10.8 MB/s eta 0:00:05   ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 25.3/70.1 MB 10.8 MB/s eta 0:00:05   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 26.2/70.1 MB 10.7 MB/s eta 0:00:05   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 26.7/70.1 MB 10.8 MB/s eta 0:00:05   ━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━ 27.1/70.1 MB 10.8 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 27.6/70.1 MB 10.8 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 28.0/70.1 MB 11.0 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 28.3/70.1 MB 11.0 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━ 28.6/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 29.0/70.1 MB 10.8 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 29.3/70.1 MB 10.9 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 29.6/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 30.0/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━ 30.3/70.1 MB 10.5 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 30.8/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━ 31.3/70.1 MB 10.5 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 31.7/70.1 MB 10.5 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 32.1/70.1 MB 10.5 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 32.6/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━ 33.0/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 33.5/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━ 34.0/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 34.4/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━ 34.9/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 35.3/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━ 35.8/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 36.2/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 36.7/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 37.1/70.1 MB 10.6 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━ 37.7/70.1 MB 10.7 MB/s eta 0:00:04   ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━ 38.1/70.1 MB 10.7 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 38.6/70.1 MB 11.0 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 39.0/70.1 MB 11.2 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 39.2/70.1 MB 11.0 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 39.5/70.1 MB 10.7 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 39.9/70.1 MB 11.0 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━ 40.3/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 40.7/70.1 MB 11.0 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━ 41.1/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 41.4/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━ 41.9/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 42.3/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━ 42.8/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 43.2/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━ 43.7/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 44.1/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━ 44.6/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 45.1/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━ 45.5/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 45.9/70.1 MB 10.9 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━ 46.0/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 46.6/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━ 47.0/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 47.5/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━ 47.9/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 48.4/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━ 48.9/70.1 MB 10.5 MB/s eta 0:00:03   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 49.3/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 49.4/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━ 49.8/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 50.1/70.1 MB 10.4 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━ 50.6/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 51.0/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━ 51.5/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 51.9/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 52.3/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━ 52.5/70.1 MB 10.1 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━ 53.3/70.1 MB 10.4 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 53.7/70.1 MB 10.4 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━ 54.2/70.1 MB 10.5 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 54.6/70.1 MB 10.4 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━ 55.0/70.1 MB 10.4 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 55.4/70.1 MB 10.3 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 55.9/70.1 MB 10.4 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 56.4/70.1 MB 10.8 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━ 56.8/70.1 MB 10.8 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 57.3/70.1 MB 10.7 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━ 57.7/70.1 MB 10.7 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 58.1/70.1 MB 10.7 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━ 58.6/70.1 MB 10.8 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 59.1/70.1 MB 10.7 MB/s eta 0:00:02   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━ 59.5/70.1 MB 10.8 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 59.9/70.1 MB 11.2 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━ 60.3/70.1 MB 11.4 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 60.6/70.1 MB 11.2 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 61.0/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━ 61.2/70.1 MB 10.9 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━ 61.7/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 62.2/70.1 MB 10.9 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━ 62.7/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 63.1/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 63.5/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━ 63.9/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 64.4/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━ 64.8/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━ 65.3/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 65.7/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━ 66.2/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 66.7/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━ 67.1/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 67.6/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━ 68.1/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 68.5/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 69.0/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 69.4/70.1 MB 11.0 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 69.9/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 70.1/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 70.1/70.1 MB 11.1 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.1/70.1 MB 10.2 MB/s eta 0:00:00\nUsing cached jaxtyping-0.2.33-py3-none-any.whl (42 kB)\nUsing cached typeguard-2.13.3-py3-none-any.whl (17 kB)\nUsing cached ml_dtypes-0.4.0-cp312-cp312-macosx_10_9_universal2.whl (394 kB)\nUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nInstalling collected packages: typeguard, opt-einsum, ml-dtypes, jaxtyping, jaxlib, jax, jaxopt, equinox, lineax, ott-jax\nSuccessfully installed equinox-0.11.4 jax-0.4.31 jaxlib-0.4.31 jaxopt-0.8.3 jaxtyping-0.2.33 lineax-0.0.5 ml-dtypes-0.4.0 opt-einsum-3.3.0 ott-jax-0.4.7 typeguard-2.13.3\nNote: you may need to restart the kernel to use updated packages.\n\n\nThen we load the required pakages.\n\n\nCode\nimport jax\nimport jax.numpy as jnp\nimport jax.random as random\nimport ott\nfrom ott.geometry import costs, pointcloud\nfrom ott.problems.linear import linear_problem\nfrom ott.solvers.linear import sinkhorn\n\n\n\n\n2.3.2 A world about OTT and JAX\nOTT is a python library that allows to compute and differentiate the entropic optimal transport. In this lab session, we will focus on entropic optimal transport computation, and not differentiation. differentiation will be takcled later.\nOTT is based on JAX, a package similar to PyTorch or TensorFlow, which allows to do automatic differentiation and GPU programming. It also provides useful primitives for efficient computation, such as the just-in-time (jit) compilation or the automatic vectorization map vmap. For more informations on JAX, see the tutorial https://jax.readthedocs.io/en/latest/notebooks/quickstart.html.\nUnlike PyTorch or TensorFlow, JAX is very close to numpy thanks to the jax.numpy package, which implements most of the numpy features, but for the JAX data structures. For this lab session, you only need to know how to manipulate jax.numpy Arrays and generate random numbers with jax.random.\nFirst, let’s have a look to jax.numpy and see that it works (almost) exactly as numpy. Usually, one imports jax.numpy as jnp as done in the above cells, and developp as with numpy, by just replacing np by jnp. Note that jax.numpy Arrays are called DeviceArray. For more informations on jax.numpy, see https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html.\n\n\nCode\nd = 5\nu = 5 * jnp.ones(5)\nId = jnp.eye(5)\nprint(type(u))\nprint(f\"u = {u}\")\nprint(f\"Id = {Id}\")\nprint(f\"Id @ u = {jnp.dot(Id, u)}\")\nprint(f\"sum(u) = {jnp.sum(u)}\")\nprint(f\"var(u) = {jnp.var(u)}\")\n\n\n&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\nu = [5. 5. 5. 5. 5.]\nId = [[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\nId @ u = [5. 5. 5. 5. 5.]\nsum(u) = 25.0\nvar(u) = 0.0\n\n\nWith numpy.random, you can generate random numbers on the fly without giving the seed. For example, np.random.rand() generates a random number \\(x \\sim U([0, 1])\\). Indeed, numpy.random uses an internal seed which is updated each time a random number generating function is called. On the other hand, with jax.random, we must give the seed each time we generate random numbers. To some extent, we want to always control the randomness. Moreover, we do not pass exactly a seed but a jax.random.PRNGKey key which is itself instantiated from a seed. Let’s see it on an example.\n\n\nCode\nrng = jax.random.PRNGKey(0)\nn, d = 13, 2\nx = jax.random.normal(rng, (n, d))\nprint(f\"x = {x}\")\n\n\nx = [[ 2.516351   -1.3947194 ]\n [-0.8633262   0.6413567 ]\n [-0.37789643 -0.6044598 ]\n [ 1.9069     -0.17918469]\n [-0.7583423  -0.5160155 ]\n [ 1.2666148  -0.12342127]\n [ 0.28430256 -0.17251171]\n [ 1.0661486   1.5814103 ]\n [-2.0284636  -0.13168257]\n [-0.14515765  0.21532312]\n [-0.69525063 -0.9314128 ]\n [-0.89809936 -0.25272107]\n [-0.34937173  1.8394127 ]]\n\n\nThen, to have new keys to generate new random numbers, we need to split the key via jax.random.split, which generate \\(n \\geq 2\\) new keys from a key.\n\n\nCode\nrng1, rng2, rng3 = jax.random.split(rng, 3)\na = jax.random.normal(rng1, (n, d))\nb = jax.random.normal(rng2, (n, d))\nc = jax.random.normal(rng2, (n, d))\nprint(f\"a = {a}\")\nprint(f\"b = {b}\")\nprint(f\"c = {c}\")\n\n\na = [[-0.38696066 -0.96707183]\n [ 1.0078175  -0.6096286 ]\n [-1.153353    1.0749092 ]\n [-1.2452031  -0.63885343]\n [ 0.01121208  0.2842425 ]\n [ 0.5296049   0.26609063]\n [ 0.8728492   1.0844501 ]\n [ 1.4472795  -0.82503337]\n [-0.41826957  0.21321987]\n [ 1.9602116   0.17687395]\n [-0.9978761  -2.0551765 ]\n [-0.4094941  -1.4577458 ]\n [-1.0969195  -0.66684234]]\nb = [[ 0.10911155 -0.45371595]\n [ 0.12062439 -0.06927001]\n [ 0.00600028  2.3732579 ]\n [-0.17656058  1.7653493 ]\n [-0.06429235  0.487175  ]\n [-1.1079016  -1.0277865 ]\n [-0.0553451  -0.28271845]\n [-0.9633478  -0.05370665]\n [ 0.20281292 -0.16658288]\n [ 0.8015828  -0.61697495]\n [-0.30176872 -1.1862007 ]\n [-3.106658   -0.03262986]\n [ 0.53711027  0.21359496]]\nc = [[ 0.10911155 -0.45371595]\n [ 0.12062439 -0.06927001]\n [ 0.00600028  2.3732579 ]\n [-0.17656058  1.7653493 ]\n [-0.06429235  0.487175  ]\n [-1.1079016  -1.0277865 ]\n [-0.0553451  -0.28271845]\n [-0.9633478  -0.05370665]\n [ 0.20281292 -0.16658288]\n [ 0.8015828  -0.61697495]\n [-0.30176872 -1.1862007 ]\n [-3.106658   -0.03262986]\n [ 0.53711027  0.21359496]]\n\n\nyou now know everything you need for the moment!\n\n\n2.3.3 Entropic optimal transport with OTT\nNow let’s use the implementation of the OTT Sinkhorn algorithm, on some random weighted point clouds. Then you will, by yourself, use it on the “croissant” transport example.\nLet’s first generate the data.\n\n\nCode\n# generate data\nrng = jax.random.PRNGKey(0)\nrng1, rng2 = jax.random.split(rng, 2)\nn, m, d = 13, 17, 2\nx = jax.random.normal(rng1, (n, d))\ny = jax.random.normal(rng2, (m, d)) + 1\na = jnp.ones(n) / n\nb = jnp.ones(m) / m\n\n\nThen, we have to define a PointCloud geometry which contains: * the point clouds x and y, * the cost function cost_fn, * the entropic regularization strength epsilon.\nNote that the geometry does not contain the weight vectors a and b, these are passed later.\nThe cost_fn should be an istance of ott.geometry.CostFn. Most of the usual costs are implemented. For example, the three costs \\(\\ell_1, \\ell_2\\) and \\(\\ell_2^2\\) are implemented. Here, we will focus on the \\(\\ell_2\\) cost, implemented by ott.geometry.costs.Euclidean. See https://ott-jax.readthedocs.io/en/latest/_autosummary/ott.geometry.costs.CostFn.html#ott.geometry.costs.CostFn for more information on the provided cost_fn.\nWe still choose epsilon to be \\(0.1 \\cdot \\bar{C}\\). To do this, we set relative_epsilon=True when instantiating the geometry. The term relative means that epsilon is chosen relatively to the mean of the cost matrix. Passing then epsilon=0.1, the value of epsilon used by Sinkhorn will be \\(0.1 \\cdot \\bar{C}\\).\n\n\nCode\n# define geometry\ngeom = pointcloud.PointCloud(\n    x=x, y=y,\n    cost_fn=costs.Euclidean(),\n    epsilon=1e-1,\n    relative_epsilon=True\n)\n\n\nWe then define an optimization problem from this geometry, which is the problem we will solve with the Sinkhorn algorithm. We instantiate this optimization problem as an object of the class linear_problem.LinearProblem. We pass the weight vectors a and b because they define the constraints of the linear problem. Then, we instantiate a Sinkhorn solver, object of the class sinkhorn.Sinkhorn, which we will use to solve this optimization problem.\nThe OTT library is designed in this way because it allows to solve other optimal transport problems, which do not necessarily have a linear problem structure, and which use other solvers than Sinkhorn.\n\n\nCode\n# create optimization problem\not_prob = linear_problem.LinearProblem(geom, a=a, b=b)\n\n# create sinkhorn solver\nsolver = sinkhorn.Sinkhorn(ot_prob)\n\n# solve the OT problem\not_sol = solver(ot_prob)\n\n\nThe ot output object contains several callables and properties, notably a boolean assessing the Sinkhorn convergence, the marginal errors throughtout iterations and the optimal transport plan.\n\n\nCode\nprint(\n    \" Sinkhorn has converged: \",\n    ot_sol.converged,\n    \"\\n\",\n    \"Error upon last iteration: \",\n    ot_sol.errors[(ot_sol.errors &gt; -1)][-1],\n    \"\\n\",\n    \"Sinkhorn required \",\n    jnp.sum(ot_sol.errors &gt; -1),\n    \" iterations to converge. \\n\",\n    \"entropic OT cost: \",\n    jnp.sum(ot_sol.matrix * ot_sol.geom.cost_matrix),\n)\n\n\n Sinkhorn has converged:  True \n Error upon last iteration:  0.00019063428 \n Sinkhorn required  5  iterations to converge. \n entropic OT cost:  29.436863\n\n\n\nQuestion: Compute the entropic optimal transport plan and cost for the “croissant” transport problem, with \\(\\ell_2\\) cost and \\(\\epsilon = 0.1 \\cdot \\bar{C}\\). Then, plot the optimal transport plan.\nAnswer:"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html",
    "href": "posts/2024/Computation/YUIMA.html",
    "title": "YUIMA 入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#yuimaとは",
    "href": "posts/2024/Computation/YUIMA.html#yuimaとは",
    "title": "YUIMA 入門",
    "section": "1 YUIMAとは？",
    "text": "1 YUIMAとは？\nR パッケージyuimaは，Lévy 過程または分数 Brown 運動が駆動する確率微分方程式が定める過程という，極めて一般的なクラスの確率過程を扱う基盤パッケージである．\n加えて，シミュレーションと推論のためのメソッドが提供されている．\nR で通常の統計推測に用いられるformulaオブジェクトのように，直感的な記法でモデル（yuima.modelオブジェクト）を定義できる．\nyuima.modelオブジェクトに対してtoLatex()関数を適用すると，モデルを LaTeX 記法で記述した文字列に変換することもできる．1\nこのように YUIMA は，確率微分方程式の更なる研究や，より複雑なモデルや推測手続きのための基盤インフラとなることを目指している．2\n\n\n\n\n\n\nリンク集\n\n\n\n\n\n\nR-Forge3\nStable version on CRAN.\nGitHub\nyuimaGUI package\nrdrr.io ページ4\n論文 (Brouste et al., 2014)\n\nパッケージ開発者らによるオープンアクセス論文．\n\n\n\n\n\n\n\n\n\n\n\nインストール\n\n\n\n\n\ninstall.packages(\"yuima\", repos=\"http://R-Forge.R-project.org\")\n\nlibrary(yuima)  # ライブラリの読み込み\n\n??yuima  # Man pageを開く"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#yuimaの構造とモデル定義",
    "href": "posts/2024/Computation/YUIMA.html#yuimaの構造とモデル定義",
    "title": "YUIMA 入門",
    "section": "2 YUIMAの構造とモデル定義",
    "text": "2 YUIMAの構造とモデル定義\n\n2.1 クラス構造\n各クラスの含まれる属性・スロット（上段）と，そのクラスのオブジェクトに使えるメソッド（下段），そして継承関係は以下の通りである．yuimaクラスは，複数の構成要素（モデル，データ，手法）の集約になっている．5\n\n\n\n\n\nclassDiagram\n  yuima o-- yuima_model\n  yuima o-- yuima_sampling\n  yuima o-- yuima_data\n  yuima_model o-- model_parameter\n  class yuima{\n    yuima.model::model\n    yuima.sampling::sampling\n    yuima.data::data\n    -yuima.characteristic::characteristic \n    -yuima.functional::functional\n    initialize()\n    show()\n    plot()\n    simulate()\n  }\n  class yuima_model{\n    expr::drift\n    expr::diffusion\n    num::hurst\n    chr::time.variable\n    chr::state.variable\n    chr::solve.variable\n    -model.parameter::parameter\n    simulate()\n  }\n  class yuima_sampling{\n    num::Initial\n    num::Terminal\n    num::n\n    -num::delta\n    -list[num]::grid\n  }\n  class yuima_data{\n    original.data\n    zoo.data\n  }\n  class model_parameter{\n    list[chr]::all\n    chr::common\n    chr::diffusion\n    chr::drift\n    chr::jump\n    chr::measure\n  }\n\n\n\n\n\n\n\n\n\n\n\nclassDiagram\n  yuima o-- yuima_characteristic\n  yuima o-- yuima_functional\n  class yuima{\n    yuima.model::model\n    yuima.sampling::sampling\n    yuima.data::data\n    yuima.characteristic::characteristic \n    yuima.functional::functional\n    initialize()\n    show()\n    simulate()\n  }\n  class yuima_characteristic{\n    int::equation.number\n    num::time.scale\n  }\n  class yuima_functional{\n    F\n    list::f\n    num::xinit\n    num::e\n  }\n\n\n\n\n\n\n\n\n2.2 setModel()コンストラクタ\nyuima.model.Rにおいて次の定義を持つ，yuima.modelオブジェクトのコンストラクタである．\nsetModel &lt;- function(drift=NULL,\n                     diffusion=NULL,\n                     hurst=0.5,\n                     jump.coeff=NULL,\n                     measure=list(),\n                     measure.type=character(),\n                     state.variable=\"x\",\n                     jump.variable=\"z\",\n                     time.variable=\"t\",\n                     solve.variable,\n                     xinit=NULL){...}\n\n\n2.3 1次元の場合\n拡散モデル（Ornstein-Uhlenbeck過程） \\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t,\\quad (\\mu,\\theta,\\sigma,X_0)=(0,1,0.5,0)\n\\] からシミュレーションをするためのサンプルコードは次の通り．他に，駆動過程を一般のHurst指数 \\(H\\) についての分数Brown運動 \\(W^H\\) としたり，ジャンプを導入したモデルを指定することもできる．6\n\n\nCode\n# 確率微分方程式モデルの設定\nmodel &lt;- setModel(drift = \"theta*(mu-X)\", diffusion = \"sigma\", state.variable = \"X\")\n\n# サンプリングスキームの設定\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\n\n# yuimaオブジェクトの作成\nyuima &lt;- setYuima(model = model, sampling = sampling)\n\n# シミュレーションの実行\nsimulation &lt;- simulate(yuima, true.parameter = c(mu = 0,theta = 1, sigma = 0.5), \n                       xinit = 0.02)\n\n\n\n\nCode\n# 結果のプロット\nplot(simulation)\n\n\n\n\n\n\n\n\n\n\n\n2.4 多次元の場合\nまず，所望のモデルを行列の言葉で書く．例えば，2次元の場合で次のようなモデルを考える： \\[\n\\begin{pmatrix}dX_t^{(1)}\\\\dX_t^{(2)}\\end{pmatrix}=\\begin{pmatrix}-3X_t^{(1)}\\\\-X_t^{(1)}2X_t^{(2)}\\end{pmatrix}dt+\\begin{pmatrix}1&0&X_t^{(2)}\\\\X_t^{(1)}&3&0\\end{pmatrix}\\begin{pmatrix}dW_t^{(1)}\\\\dW_t^{(2)}\\\\dW_t^{(3)}\\end{pmatrix}\n\\]\nシミュレーションは次のように，各項の係数をベクトル・行列の形式でsetModelへと引き渡すことで行える． この場合，左辺を明示するためにsolve.variableのベクトルとしての指定が欠かせなくなる．7\n\n\nCode\nsolve_variable &lt;- c(\"X1\", \"X2\")\ndrift &lt;- c(\"-3*X1\", \"-X1-2*X2\")\ndiffusion &lt;- matrix(c(\"1\", \"X1\", \"0\", \"3\", \"X2\", \"0\"), 2, 3)\nmodel &lt;- setModel(drift=drift, diffusion=diffusion, solve.variable=solve_variable)\nsimulation &lt;- simulate(model)\n\n\n\n\nCode\nplot(simulation, plot.type=\"single\", lty=1:2)\n\n\n\n\n\n\n\n\n\n\n\n2.5 分数Gaussなノイズ\n標準分数Brown運動 \\(W^H\\) とは，Hurst指数 \\(H\\in(0,1)\\) に対して， \\[\n\\operatorname{E}[W^H_s,W^H_t]=\\frac{1}{2}\\biggr(\\lvert s\\rvert^{2H}+\\lvert t\\rvert^{2H}-\\lvert t-s\\rvert^{2H}\\biggl)\n\\] という共分散構造を持った中心Gauss過程である．\\(H\\ne1/2\\) のとき，もはやMarkov過程でもセミマルチンゲールでもない．特に \\(H&gt;1/2\\) のときに長期的な依存を持った振る舞いをし，これが多くの応用を呼んでいる．これに対して，SDE \\[\ndX_t=a(X_t)dt+b(X_t)dW_t^H\n\\] で定まるモデルを定義できる．(Brouste et al., 2014, pp. 3.5節 p.15)\n例えば，次の分数OU過程 \\[\ndY_t=3Y_tdt+dW_t^H\n\\] は次のように定義する：\n\n\nCode\nmod4A &lt;- setModel(drift=\"3*y\", diffusion=1, hurst=0.3, solve.var=\"y\")\nmod4B &lt;- setModel(drift=\"3*y\", diffusion=1, hurst=0.7, solve.var=\"y\")\nsim1 &lt;- simulate(mod4A, sampling = setSampling(n=1000))\nsim2 &lt;- simulate(mod4B, sampling = setSampling(n=1000))\npar(mfrow=c(2,1), mar=c(2,3,1,1))\n\n\n\n\nCode\nplot(sim1, main=\"H=0.3\")\n\n\n\n\n\n\n\n\n\nCode\nplot(sim2, main=\"H=0.7\")\n\n\n\n\n\n\n\n\n\nこのシミュレーション法はCholesky法と(Wood and Chan, 1994) 法から選択が可能である．simulateメソッドのキーワード引数methodfGn=\"WoodChan\", methodfGn=\"Cholesky\"によって可能である．\n\n\n2.6 Lévy過程\n複合Poisson過程 \\(Z_t\\) とは，Poisson時間に特定の分布に従うサイズの跳躍が起こるという過程である．これを用いて， \\[\ndX_t=a(t,X_t,\\theta)dt+b(t,X_t,\\theta)dW_t+dZ_t\n\\] というSDEを通じてジャンプを持つ過程が定義できる．さらに \\(Z_t\\) の項に係数 \\(c\\) を持たせるには，\\(X\\) のジャンプを定めるランダム測度 \\[\n\\mu(dt,dz)=\\sum_{s&gt;0}1_{\\left\\{\\Delta Z_s\\ne 0\\right\\}}\\delta_{(s,\\Delta Z_s)}(dt,dz)\n\\] により，複合Poisson過程が \\[\nZ_t=\\int^t_0\\int_{\\lvert z\\rvert\\le1}z(\\mu(ds,dz)-\\nu(dz)ds)+\\int^t_0\\int_{\\lvert z\\rvert&gt;1}z\\mu(ds,dz)\n\\] と表せ，ジャンプ過程 \\(X\\) が，一般の関数 \\(c\\) を用いて \\[\ndX_t=a(t,X_t,\\theta)dt+b(t,X_t,\\theta)dW_t+\\int_{\\lvert z\\rvert&gt;1}c(X_{t-},z)\\mu(dt,dz)\n\\] \\[\n\\qquad+\\int_{0&lt;\\lvert z\\rvert\\le1}c(X_{t-},z)(\\mu(dt,dz)-\\nu(dz)dt)\n\\] と表せる．\n\\(\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) がLévy測度であるとは， \\(\\nu(\\{0\\})=0\\) かつ \\[\n\\int_{\\mathbb{R}^d}(1\\land\\lvert z\\rvert^2)\\nu(dz)&lt;\\infty\n\\] を満たすことをいう．\n例えば，強度 \\(\\lambda=10\\) でGauss分布で大きさが決まる跳躍を持つLévy過程は，measure.type=\"CP\"によって指定する．平均 \\(0\\) のOU過程 \\[\ndX_t=-\\theta X_tdt+\\sigma dW_t+dZ_t\n\\] は次のように定義できる：\n\n\nCode\nmod5 &lt;- setModel(drift=c(\"-theta*x\"), diffusion=\"sigma\", jump.coeff=\"1\",\n      measure=list(intensity=\"10\", df=list(\"dnorm(z,0,1)\")), measure.type=\"CP\",\n      solve.variable=\"x\")\nsim5 &lt;- simulate(mod5, true.p = list(theta=1, sigma=3), sampling=setSampling(n=1000))\n\n\n\n\nCode\nplot(sim5)\n\n\n\n\n\n\n\n\n\n一方で，逆正規分布の大きさのジャンプを持つLévy測度 \\(\\nu\\) をもち，Poisson成分を持たないOU過程 \\[\ndX_t=-xdt+dZ_t\n\\] は次のように定義できる：\n\n\nCode\nmod6 &lt;- setModel(drift=\"-x\", xinit=1, jump.coeff=\"1\",\n      measure.type=\"code\", measure=list(df=\"rIG(z,1,0.1)\"))\nsim6 &lt;- simulate(mod6, sampling=setSampling(Terminal=10, n=10000))\n\n\n\n\nCode\nplot(sim6)"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#シミュレーションとサンプリング",
    "href": "posts/2024/Computation/YUIMA.html#シミュレーションとサンプリング",
    "title": "YUIMA 入門",
    "section": "3 シミュレーションとサンプリング",
    "text": "3 シミュレーションとサンプリング\n\n3.1 simulate()関数の使い方\nyuimaまたはyuima.modelで定められたモデルから，Euler-Maruyama法によるシミュレーションを実行する関数．8 simulate.Rにてポリモーフィックな実装がなされている．\nsimulate(object, nsim=1, seed=NULL, xinit, true.parameter, space.discretized = FALSE, \n increment.W = NULL, increment.L = NULL, method = \"euler\", hurst, methodfGn = \"WoodChan\",\n    sampling=sampling, subsampling=subsampling, ...)\n返り値は，引数objectに，シミュレーション結果をyuima.dataフィールドに格納したyuimaオブジェクトであり，それに対してplot()が使える．\n\nobjectがyuimaオブジェクトの場合\n\n\n\n3.2 時間離散化Euler-Maruyama法\n時間にグリッド \\(0=\\tau_0&lt;\\tau_1&lt;\\cdots&lt;\\tau_j&lt;\\cdots\\) を導入し，連続過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\) の離散化 \\(\\{\\widetilde{X}_{\\tau_j}\\}_{j\\in\\mathbb{N}}\\) を \\[\n\\widetilde{X}_{\\tau_{j+1}}:=\\widetilde{X}_{\\tau_j}+b(\\tau_j,\\widetilde{X}_{\\tau_j})(\\tau_{j+1}-\\tau_j)+c(\\tau_j,\\widetilde{X}_{\\tau_j})(W_{\\tau_{j+1}}-W_{\\tau_j})\n\\]\nと定義する． \\(W_{\\tau_{j+1}}-W_{\\tau_j}\\overset{\\text{iid}}{\\sim}\\mathrm{N}(0,\\tau_{j+1}-\\tau_j)\\) を利用してサンプリングできる．\n\n\n3.3 空間離散Euler-Maruyama法\nsimulateメソッドはこの方法も（ジャンプを持たない過程について）実装している．現状，駆動過程が1次元のSDEに対してのみである．\nこれは，時間離散化 \\(\\{\\tau_j\\}_{j\\in\\mathbb{N}}\\subset\\mathbb{R}\\) は次のように取る方法である： \\[\n\\tau_0:=0,\\quad\\tau_{j+1}:=\\inf\\left\\{t&gt;\\tau_j\\mid\\lvert W_t-W_{\\tau_j}\\rvert=\\epsilon\\right\\}\n\\] ただし，実装の上では \\[\n\\epsilon^2:=\\frac{T}{n}=\\Delta_n\n\\] としている．この方法は通常の時間離散化法よりも3倍高速になることが経験的に知られている．これはspace.discretized=TRUEで指定できる．デフォルトは=FALSEとなっており，時間離散化の方である．\n他にも (Iacus, 2008) のシミュレーション法が実装予定である．\n\n\n3.4 setSamplingコンストラクタ\nsimulate関数はyuima.samplingオブジェクトも引数としてsampling = sampというように受け入れる．そして出力にそのまま引き継がれる．\n\n\nsetSampling.Rd\n\nsetSampling(Initial = 0, Terminal = 1, n = 100, delta, \n   grid, random = FALSE, sdelta=as.numeric(NULL), \n   sgrid=as.numeric(NULL), interpolation=\"pt\" )\n\n例えば次の2次元モデルを考える\n\\[\ndX_t^{(1)}=-\\theta X^{(1)}_tdt+dW_t^{(1)}+X_t^{(2)}dW_t^{(3)}\n\\] \\[\ndX_t^{(2)}=-(X_t^{(1)}+\\gamma X_t^{(2)})dt+X_t^{(1)}dW_t^{(1)}+\\delta dW_t^{(2)}\n\\]\n時区間 \\([0,3]\\) 上のグリッドから \\(n=3000\\) の粒度で観測するサンプリングオブジェクトは次のように定義する：\n\n\nCode\nsol &lt;- c(\"x1\", \"x2\")\nb &lt;- c(\"-theta*x1\", \"-x1-gamma*x2\")\ns &lt;- matrix(c(\"1\", \"x1\", \"0\", \"delta\", \"x2\", \"0\"), 2, 3)\nmyModel &lt;- setModel(drift=b, diffusion=s, solve.variable=sol)\nsamp &lt;- setSampling(Terminal=3, n=3000)\n\n\n\n\nCode\nsim2 &lt;- simulate(myModel, sampling=samp)\nstr(sim2@sampling)\n\n\nFormal class 'yuima.sampling' [package \"yuima\"] with 11 slots\n  ..@ Initial      : num 0\n  ..@ Terminal     : num [1:2] 3 3\n  ..@ n            : int [1:2] 3000 3000\n  ..@ delta        : num 0.001\n  ..@ grid         :List of 1\n  .. ..$ : num [1:3001] 0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 ...\n  ..@ random       : logi FALSE\n  ..@ regular      : logi TRUE\n  ..@ sdelta       : num(0) \n  ..@ sgrid        : num(0) \n  ..@ oindex       : num(0) \n  ..@ interpolation: chr \"pt\"\n\n\nこれはsampそのものである．\n\n\n3.5 subsampling関数の使い方\n２つの独立な指数分布を指定することで，Poisson到着時間を用いてサンプリングする，というyuima.samplingオブジェクトは次のように定義する：\n\n\nCode\nnewsamp &lt;- setSampling(random=list(rdist=c(function(x)\n      + rexp(x, rate=10), function(x) rexp(x, rate=20))))\n\n\nこれを用いてサブサンプリングを実行できる：\n\n\nCode\nnewdata &lt;- subsampling(sim2, sampling=newsamp)\nplot(sim2, plot.type=\"single\", lty=c(1,3), ylab=\"sim2\")\npoints(get.zoo.data(newdata)[[1]], col=\"red\")\npoints(get.zoo.data(newdata)[[2]], col=\"green\", pch=18)\n\n\n\n\n\n\n\n\n\n赤が \\(X_t^{(1)}\\)，緑が \\(X_t^{(2)}\\) の見本道でどこがサブサンプリングされたかを示している．\nこのサンプリング法はランダムに行った．これを示すフラグがregularである：\n\n\nCode\nstr(newsamp@regular)\n\n\n logi FALSE\n\n\n一方で，決定論的に，決まった周波数でサブサンプリングもできる：\n\n\nCode\nnewsamp2 &lt;- setSampling(delta=c(0.1, 0.2))\n\n\nWarning in yuima.warn(\"'Terminal' (re)defined.\"): \nYUIMA: 'Terminal' (re)defined.\n\n\nCode\nnewdata2 &lt;- subsampling(sim2, sampling=newsamp2)\nplot(sim2, plot.type=\"single\", lty=c(1,3), ylab=\"sim2\")\npoints(get.zoo.data(newdata2)[[1]], col=\"red\")\npoints(get.zoo.data(newdata2)[[2]], col=\"green\", pch=18)\n\n\n\n\n\n\n\n\n\n赤色のサブサンプリングは，緑色の２倍の頻度（半分の周波数）で行われている．\n\n\n3.6 サンプリングとサブサンプリングの組み合わせ\nシミュレーション研究で，よく高頻度のサンプリングを行った後，推定のためにより低い頻度でのデータを抽出する，ということが行われる．これは１行で表現できる：\n\n\nCode\nY.sub &lt;- simulate(myModel, sampling=setSampling(delta=0.001, n=1000), subsampling=setSampling(delta=0.01, n=100))\nY &lt;- simulate(myModel, sampling=setSampling(delta=0.001, n=1000))\n\n\n\n\nCode\nplot(Y.sub, plot.type=\"single\")\npoints(get.zoo.data(Y.sub)[[1]], col=\"red\")\npoints(get.zoo.data(Y.sub)[[2]], col=\"green\", pch=18)\n\n\n\n\n\n\n\n\n\n\n\n3.7 zooとの依存関係\nzooというパッケージ (Zeileis and Grothendieck, 2005) が，時系列データを保存するために内部で用いられる．\nより柔軟な時系列データの保存法ができたら，このdependencyは脱したいと考えられているようである．"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#漸近展開",
    "href": "posts/2024/Computation/YUIMA.html#漸近展開",
    "title": "YUIMA 入門",
    "section": "4 漸近展開",
    "text": "4 漸近展開\n確率変数の期待値を近似するのに，Monte Carlo法は普遍的な方法であるが，漸近展開もこれの代替手法である．精度さえ良ければ，計算時間は比較にならないほど速い．\n\n4.1 場面設定\n例えば，\\(d\\)-次元拡散過程 \\(X=(X^{(\\epsilon)}_t)_{t\\in[0,T]},\\epsilon\\in(0,1]\\) を次のように定める： \\[\nX^{(\\epsilon)}_t=x_0+\\int^t_0a(X_s^{(\\epsilon)},\\epsilon)ds+\\int^t_0b(X_s^{(\\epsilon)},\\epsilon)dW_s.\n\\]\nただし，\\(W_t\\) は \\(r\\)-次元Wiener過程とした．そして，その汎函数\n\\[\nF^{(\\epsilon)}=\\sum_{\\alpha=0}^r\\int^T_0f_\\alpha(X_t^{(\\epsilon)},\\epsilon)dW_t^\\alpha+F(X_T^{(\\epsilon)},\\epsilon),\\quad W^0_t=t\n\\]\nを決めたいとする．例えば，Black-Scholesモデル \\[\ndX_t^{(\\epsilon)}=\\mu X_t^{(\\epsilon)}dt+\\epsilon X_t^{(\\epsilon)}dW_t\n\\] において，利子率が零である場合のアジアオプションの価格は \\[\n\\operatorname{E}\\left[\\max\\left(\\frac{1}{T}\\int^T_0X_t^{(\\epsilon)}dt-K,0\\right)\\right]\n\\] と表せる（これは線型である）．これは \\[\nF^{(\\epsilon)}=\\frac{1}{T}\\int^T_0X_t^{(\\epsilon)}dt,\\quad r=1\n\\] と定めた場合に相当する．また，\\(F^{(\\epsilon)}=X^{(\\epsilon)}_T\\) とした場合に当たる \\[\n\\operatorname{E}[(X^{(\\epsilon)}_T-K)\\lor0]\n\\] がヨーロピアンコールオプションの価格になる．この値は閉じた形を持っているが，Asian optionについてはこの線型な設定においてさえ数値計算法が要請される．\n\n\n4.2 渡部理論\nここでは，\\(\\epsilon\\searrow0\\) の極限で系が決定論的であるとする．すなわち， \\[\nb(-,0)=0\n\\] \\[\nf_\\alpha(-,0)=0\\quad(\\alpha\\in[r])\n\\] とする．すると \\(X_t^{(0)}\\) は次の常微分方程式 \\[\n\\frac{d X_t^{(0)}}{d t}=a(X_t^{(0)},0),\\quad X_0^{(0)}=x_0\n\\] の解であるから，\\(F^{(0)}\\) も定数 \\[\nF^{(0)}=\\int^T_0f_0(X_t^{(0)},0)dt+F(X_T^{(0)},0)\n\\] で与えられる．\nさらに，\\(a,b,f_\\alpha,F\\) がしかるべき正則性条件を満たすとき，汎函数 \\(F^{(\\epsilon)}\\) にはある版が存在して \\(\\epsilon\\in[0,1)\\) に関して殆ど確実に滑らかである．特に， \\[\n\\widetilde{F}^{(\\epsilon)}:=\\frac{F^{(\\epsilon)}-F^{(0)}}{\\epsilon}\n\\] は次の確率展開を持つ： \\[\n\\widetilde{F}^{(\\epsilon)}\\sim\\widetilde{F}^{[0]}+\\epsilon\\widetilde{F}^{[1]}+\\epsilon^2\\widetilde{F}^{[2]}+\\cdots\\quad(\\epsilon\\searrow0)\n\\]\nこの展開は，Malliavin解析のSobolev空間において厳密に成り立つ．これを導くのが (Watanabe, 1987) の理論である．\nこれに基づき，汎函数 \\(F^{(\\epsilon)}\\) の近似を構成する機能がyuimaに実装されている．この漸近展開をオプションの価格付けに応用したのが (Yoshida, 1992) である．\n\n\n4.3 setFunctionalコンストラクタ\nBlack-Scholesモデル \\[\ndX_t^{(\\epsilon)}=\\mu X_t^{(\\epsilon)}dt+\\epsilon X_t^{(\\epsilon)}dW_t\n\\] が定める幾何Brown運動 \\((X_t)\\) のパラメータが \\(\\mu=1,x_0=1\\) を満たす場合において，Asian call optionの価格は，汎函数 \\[\ng(x):=\\max\\left(F^{(0)}-K+\\epsilon x,0\\right)\n\\] を計算すれば良い．\n\\(F^{(\\epsilon)}\\) の極限 \\(F^{(0)}\\) の値は，関数F0をyuima.functionalスロットが埋まったyuimaオブジェクトに適用することで得られる．\n\n\nCode\nmodel &lt;- setModel(drift=\"x\", diffusion=matrix(\"x*e\", 1, 1))\nK &lt;- 100\nyuima &lt;- setYuima(model=model, sampling=setSampling(Terminal=1, n=1000))\nyuima &lt;- setFunctional(yuima, f=list(expression(x/T), expression(0)), F=0, xinit=150, e=0.5)\nF0 &lt;- F0(yuima)\n\n\n\n\nCode\nprint(F0)\n\n\n[1] 257.6134\n\n\n\n\nCode\ng &lt;- function(x) {\n  tmp &lt;- (F0 - 100) + (0.5*x)\n  tmp[(0.5*x) &lt; (100-F0)] &lt;- 0\n  tmp\n}\nasymp &lt;- asymptotic_term(yuima, block=10, expression(0), g)\n\n\n\n\nCode\nstr(asymp)\n\n\nList of 3\n $ d0: num 159\n $ d1: num -3.93\n $ d2: num [1, 1] 4\n\n\nこれを適切な和をとれば良い．\n\n\nCode\ne = 0.5\nasy1 &lt;- asymp$d0 + e*asymp$d1\nasy2 &lt;- asymp$d0 + e*asymp$d1 + e^2 * asymp$d2\n\n\nはAsian call priceの，それぞれ1次と2次の漸近展開を与える．\n\n\n4.4 CIR過程の推測\n\\(X_t\\) が幾何Brown運動の場合にしか (Levy, 1992) の近似は用いることはできない．例えばCIR模型 \\[\ndX_t=0.9X_tdt+\\epsilon\\sqrt{X_t}dW_t,\\quad X_0=1,\n\\] の場合でも，漸近展開は有効である．この状態でのEuropean call optionの価格は，2次までの漸近展開が与える値が，100万データ数によるMonte Carlo推定量の精度に匹敵し，当然計算量は圧倒的に少ない．"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#推論",
    "href": "posts/2024/Computation/YUIMA.html#推論",
    "title": "YUIMA 入門",
    "section": "5 推論",
    "text": "5 推論\nほとんどの手法が，\\(N\\to\\infty,\\Delta_n\\to0\\) の極限で得られるデータ（高頻度データ）にも応用可能な手法になっている．\n\n5.1 setYuimaコンストラクタ\nmodel, sampling のみから作れば，サンプリングをし，その結果をyuima.dataオブジェクトとして格納する．\n一方で，dataをyuimaオブジェクトに持たせる方法でもある：\nmy.yuima &lt;- setYuima(data=setData(myData), model=myModel)\ndata &lt;- read.csv(\"http://chart.yahoo.com/table.csv?s=IBM&g=d&x=.csv\")\nx &lt;- setYuima(data = setData(data$Close))\nstr(x@data)\n\n\n5.2 擬似最尤推定9\n\\(r\\)-次元Wiener過程 \\(\\{W_t\\}\\) が定める拡散過程\n\\[\ndX_t=a(X_t,\\theta_2)dt+b(X_t,\\theta_1)dW_t\n\\]\nのパラメータ \\(\\theta_1\\in\\Theta_1\\subset\\mathbb{R}^p,\\theta_2\\in\\Theta_2\\subset\\mathbb{R}^q\\) の推定を考えたい．データ \\[\n\\boldsymbol{X}_n:=(X_{t_i})_{i=0}^n,t_i:=i\\Delta_n\n\\] に関する対数尤度は次の擬似対数尤度を用いて近似できる：\n\\[\n\\ell_n(\\boldsymbol{X}_n,\\theta)=-\\frac{1}{2}\\sum_{i=1}^n\\left(\\log\\det(\\Sigma_{i-1}(\\theta_1))+\\frac{1}{\\Delta_n}\\Sigma_{i-1}^{-1}(\\theta_1)\\biggr((\\Delta X_i-\\Delta_na_{i-1}(\\theta_2))^{\\otimes 2}\\biggl)\\right)\n\\] ただし \\[\n\\Delta X_i:=X_{t_i}-X_{t_{i-1}},\\Sigma_i(\\theta_1):=\\Sigma(\\theta_1,X_{t_i})\n\\] \\[\na_i(\\theta_2):=a(X_{t_i},\\theta_2),\\Sigma:=b^{\\otimes2},A^{\\otimes2}:=AA^\\top\n\\] とした．これに対して \\[\n\\widehat{\\theta}:=\\operatorname*{argmax}_{\\theta}\\ell_n(\\boldsymbol{X}_n,\\theta)\n\\] と定めるのである．\n実は，高頻度極限 \\(\\Delta_n\\to0,n\\to\\infty\\) において，\\(\\widehat{\\theta}_1\\) は漸近的に混合正規である (Genon-Catalot and Jacod, 1993)．\\(\\widehat{\\theta}_2\\) も一致性を持つには，条件 \\(T=n\\Delta_n\\to\\infty\\) が必要である．これは \\(T\\) が有限であるとき \\(\\theta_2\\) のFisher情報量も有限であり，それゆえこの設定したでは 一致推定量が存在しないためである．\\(T\\to\\infty\\) の下でエルゴード条件を仮定すれば，一致性だけでなく漸近正規性も達成される．局所Gauss近似を成立させるためには \\(n\\Delta_n^2\\to0\\) が必要であるが，これでは条件が強すぎる．任意の \\(p\\ge2\\) に対して条件 \\(n\\Delta_n^p\\to0\\) しか満たさない場合での適応的推定法が (Uchida and Yoshida, 2012) によって，大偏差原理による議論に基づいて提案された．\n\\(T\\) が大きくない場合， \\(\\theta_2\\) には小標本的な影響が現れる．これは適応的Bayes推定でも議論する．適応的Bayes事後平均推定量は，上述の \\(\\widehat{\\theta}_1,\\widehat{\\theta}_2\\) と全く同じ漸近的性質を持つ．\nこれを実装するqmle関数が，stats4標準のmle関数に似せて作られている．\nqmle(yuima, start, method = \"L-BFGS-B\", fixed = list(),\nprint = FALSE, envir = globalenv(), lower, upper, joint = FALSE, Est.Incr =\"NoIncr\",\naggregation = TRUE, threshold = NULL, rcpp =FALSE, ...)\nstartは最適化を始める初期値を，名前に対応づける辞書の形で与える．yuimaオブジェクトはmodelとdataのスロットが埋められていなければならない．最適化はBFGSによって行われる．\nジャンプ過程への応用も今後予定されている．\n\n\n5.3 QMLEの例\n例えば，次のモデル \\[\ndX_t=(2-\\theta_2X_t)dt+(1+X_t^2)^{\\theta_1}dW_t,\\quad X_0=1,\n\\] を考え，真値を \\(\\theta_1=0.2,\\theta_2=0.3\\) としてデータを生成する．\n\n\nCode\nymodel &lt;- setModel(drift=\"(2-theta2*x)\", diffusion=\"(1+x^2)^theta1\")\nn &lt;- 750\nysamp &lt;- setSampling(Terminal=n^(1/3), n=n)\nyuima &lt;- setYuima(model=ymodel, sampling=ysamp)\nyuima &lt;- simulate(yuima, xinit=1, true.parameter=list(theta1=0.2, theta2=0.3))\n\n\nこれに対して，QMLEを実行してみる：\n\n\nCode\nparam.init &lt;- list(theta2=0.5, theta1=0.5)\nlow.par &lt;- list(theta1=0, theta2=0)\nupp.par &lt;- list(theta1=1, theta2=1)\nmle1 &lt;- qmle(yuima, start=param.init, lower=low.par, upper=upp.par)\nsummary(mle1)\n\n\nQuasi-Maximum likelihood estimation\n\nCall:\nqmle(yuima = yuima, start = param.init, lower = low.par, upper = upp.par)\n\nCoefficients:\n        Estimate  Std. Error\ntheta1 0.2025147 0.006989654\ntheta2 0.2106767 0.108228544\n\n-2 log L: -131.9554 \n\n\n\\(\\theta_2\\) の推定の方が圧倒的に難しいらしいことがよくわかる．\n\n\n5.4 適応的Bayes推論\n\\(r\\)-次元モデル\n\\[\ndX_t=a(X_t,\\theta_2)dt+b(X_t,\\theta_1)dW_t\n\\]\nの擬似対数尤度\n\\[\n\\ell_n(\\boldsymbol{X}_n,\\theta)=-\\frac{1}{2}\\sum_{i=1}^n\\left(\\log\\det(\\Sigma_{i-1}(\\theta_1))+\\frac{1}{\\Delta_n}\\Sigma_{i-1}^{-1}(\\theta_1)\\biggr((\\Delta X_i-\\Delta_na_{i-1}(\\theta_2))^{\\otimes 2}\\biggl)\\right)\n\\]\nを考える．\nまず，任意に初期値 \\(\\theta^\\star_2\\in\\Theta_2\\) を取り，\\(\\theta_1\\) に事前分布 \\(\\pi_1\\) を導入して，これに基づいてBayes事後平均推定量を \\[\n\\widetilde{\\theta}_1:=\\frac{\\int_{\\Theta_1}\\theta_1\\exp(\\ell_n(\\boldsymbol{X}_n,(\\theta_1,\\theta_2^\\star)))\\pi_1(\\theta_1)d\\theta_1}{\\int_{\\Theta_1}\\exp(\\ell_n(\\boldsymbol{X}_n,(\\theta_1,\\theta_2^\\star)))\\pi_1(\\theta_1)d\\theta_1}\n\\] と定める．\\(\\pi_1\\) は \\(\\Theta_1\\) 全域を台に持つならば，高頻度極限で良い漸近的性質を持つ．続いて，\\(\\theta_2\\) に事前分布 \\(\\pi_2\\) を導入して，\\(\\widetilde{\\theta}_1\\) からBayes事後平均推定量を \\[\n\\widetilde{\\theta}_2:=\\frac{\\int_{\\Theta_2}\\theta_1\\exp(\\ell_n(\\boldsymbol{X}_n,(\\widetilde{\\theta}_1,\\theta_2)))\\pi_2(\\theta_2)d\\theta_2}{\\int_{\\Theta_2}\\exp(\\ell_n(\\boldsymbol{X}_n,(\\widetilde{\\theta}_1,\\theta_2)))\\pi_2(\\theta_2)d\\theta_2}\n\\]\n\n\n5.5 adaBayesの例\n同様のモデル \\[\ndX_t=(2-\\theta_2X_t)dt+(1+X_t^2)^{\\theta_1}dW_t,\\quad X_0=1,\n\\] を考え，真値を \\(\\theta_1=0.2,\\theta_2=0.3\\) としてデータを生成する．\n\n\nCode\nymodel &lt;- setModel(drift=\"(2-theta2*x)\", diffusion=\"(1+x^2)^theta1\")\nn &lt;- 750\nysamp &lt;- setSampling(Terminal=n^(1/3), n=n)\nyuima &lt;- setYuima(model=ymodel, sampling=ysamp)\nyuima &lt;- simulate(yuima, xinit=1, true.parameter=list(theta1=0.2, theta2=0.3))\n\n\n加えて，一様事前分布を用意する．\n\n\nCode\nprior &lt;- list(theta2=list(measure.type=\"code\", df=\"dunif(theta2,0,1)\"),\n      theta1=list(measure.type=\"code\", df=\"dunif(theta1,0,1)\"))\nbayes1 &lt;- adaBayes(yuima, start=param.init, prior=prior, mcmc=1000)\nsummary(bayes1)\n\n\n  Length    Class     Mode \n       1 adabayes       S4 \n\n\nCode\nsummary(mle1)\n\n\nQuasi-Maximum likelihood estimation\n\nCall:\nqmle(yuima = yuima, start = param.init, lower = low.par, upper = upp.par)\n\nCoefficients:\n        Estimate  Std. Error\ntheta1 0.2025147 0.006989654\ntheta2 0.2106767 0.108228544\n\n-2 log L: -131.9554 \n\n\n\n\nCode\nstr(bayes1)\n\n\nFormal class 'adabayes' [package \"yuima\"] with 6 slots\n  ..@ mcmc       :List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ accept_rate:List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ coef       : Named num [1:2] 0.212 0.327\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n  ..@ call       : language adaBayes(yuima = yuima, start = param.init, prior = prior, mcmc = 1000)\n  ..@ vcov       : num [1:2, 1:2] 0 0 0 0\n  ..@ fullcoef   : Named num [1:2] 0.212 0.327\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n\n\n\n5.5.1 バグについて\n\ncoef(summary(bayes1))はエラーを生じる． &gt; Error in object$coefficients : $ operator is invalid for atomic vectors &gt; Calls: .main … eval_with_user_handlers -&gt; eval -&gt; eval -&gt; coef -&gt; coef -&gt; coef.default\nadaBayesの後ろの方に必須の引数mcmcがある．Manpageを読む限りiterationと同じ役割では？\n\n\n\n5.5.2 小標本性がドリフト推定に与えるバイアス\nドリフト係数 \\(a(X_t,\\theta_2)\\) に関する推定は，\\([0,T]\\) の長さに強い影響を受けることが理論的にも知られている．数値実験では \\(n=750\\) かつ \\[\nT=n^{\\frac{1}{3}}\\approx9.09\n\\] と取った．これをさらに \\(n=500,T\\approx7.94\\) とすると，\n\n\nCode\nn &lt;- 500\nysamp &lt;- setSampling(Terminal=n^(1/3), n=n)\nyuima &lt;- setYuima(model=ymodel, sampling=ysamp)\nyuima &lt;- simulate(yuima, xinit=1, true.parameter=list(theta1=0.2, theta2=0.3))\nmle2 &lt;- qmle(yuima, start=param.init, lower=list(theta1=0, theta2=0), upper=list(theta1=1, theta2=1))\nbayes2 &lt;- adaBayes(yuima, start=param.init, prior=prior, mcmc=1000)\n\n\n\n\nCode\nsummary(mle2)\n\n\nQuasi-Maximum likelihood estimation\n\nCall:\nqmle(yuima = yuima, start = param.init, lower = list(theta1 = 0, \n    theta2 = 0), upper = list(theta1 = 1, theta2 = 1))\n\nCoefficients:\n        Estimate Std. Error\ntheta1 0.2054158 0.02372179\ntheta2 0.9999339 0.29625725\n\n-2 log L: -437.895 \n\n\nCode\nstr(bayes2)\n\n\nFormal class 'adabayes' [package \"yuima\"] with 6 slots\n  ..@ mcmc       :List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ accept_rate:List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ coef       : Named num [1:2] 0.21 0.719\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n  ..@ call       : language adaBayes(yuima = yuima, start = param.init, prior = prior, mcmc = 1000)\n  ..@ vcov       : num [1:2, 1:2] 0 0 0 0\n  ..@ fullcoef   : Named num [1:2] 0.21 0.719\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n\n\n小標本でも適応的Bayes推定量はよく振る舞う．一方で，QMLEでは劣化が激しい．\\(\\widehat{\\theta}_1\\) は小標本の影響を \\(\\widehat{\\theta}_2\\) ほどは大きく受けない．\n\n\n\n5.6 非同期共分散推定10\n伊藤過程が非同期に離散観測する設定を考える．\n\n\n5.7 変化点解析11\n\n\n5.8 LASSOモデル選択12"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#bayes推論",
    "href": "posts/2024/Computation/YUIMA.html#bayes推論",
    "title": "YUIMA 入門",
    "section": "6 Bayes推論",
    "text": "6 Bayes推論\n\n6.1 adaBayesモジュールについて\nSDEモデル内のパラメータに対して適応的Bayes推論を実行するためのモジュールである．\n\nR-Forge上のDocumentation\nR-Forge上のSource\nGitHub上のSource"
  },
  {
    "objectID": "posts/2024/Computation/YUIMA.html#footnotes",
    "href": "posts/2024/Computation/YUIMA.html#footnotes",
    "title": "YUIMA 入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Brouste et al., 2014) 第７節も参照．↩︎\n(Brouste et al., 2014) 概要より．↩︎\nR-Forge とは，SourceForge に似た，R 言語のパッケージ開発者向けの協力的な開発環境を提供するプラットフォーム．↩︎\nrdrr は CRAN や GitHub だけでなく，Bioconductor や R-Forge も含めて検索可能にしているサイト．↩︎\nただし，marmaidの記法としてクラス名に.を許さないので，yuima.modelの代わりに図中ではyuima_modelと記述している．↩︎\nモデルの左辺 \\(dX_t\\) はyuima.modelのスロットsolve.variableがstate.variableに暗黙のうちに一致させているところから暗黙に読み込んでいる．↩︎\n(Brouste et al., 2014) 第3.4節 pp.12-13．↩︎\nただし，分数Gaussノイズが仮定された場合はCholesky法または (Wood and Chan, 1994) の手法による．↩︎\n(Brouste et al., 2014, p. 第6.2節)↩︎\n(Brouste et al., 2014, p. 第6.4節)↩︎\n(Brouste et al., 2014, p. 第6.5節)↩︎\n(Brouste et al., 2014, p. 第6.6節)↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html",
    "href": "posts/2024/Stat/Logistic.html",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#ロジットモデルのベイズ推定",
    "href": "posts/2024/Stat/Logistic.html#ロジットモデルのベイズ推定",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "1 ロジットモデルのベイズ推定",
    "text": "1 ロジットモデルのベイズ推定\n応答変数 \\(Y\\in\\mathcal{L}(\\Omega;2)\\) は，説明変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p)\\) の関数であるとして，係数 \\(\\xi\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p)\\) をロジスティック回帰モデル \\[\n\\operatorname{P}[Y=1\\,|\\,X,\\xi]=g^{-1}(X^\\top\\xi)=\\frac{\\exp(X^\\top\\xi)}{1+\\exp(X^\\top\\xi)}\n\\tag{1}\\] を通じて定める．ただし， \\[\ng(x):=\\log\\frac{x}{1-x}\n\\] は ロジット関数，\\(g^{-1}\\) は ロジスティック関数 という．1\nこのパラメータ \\(\\xi\\) をベイス推定することを考える．即ち，データ \\(\\{(y^i,x^i)\\}_{i=1}^n\\subset2\\times\\mathbb{R}^p\\) と事前分布 \\(p_0(\\xi)d\\xi\\in\\mathcal{P}(\\mathbb{R}^p)\\) を通じて，事後分布 \\[\n\\pi(\\xi)\\,\\propto\\,p_0(\\xi)\\prod_{i=1}^n\\frac{\\exp(y^i(x^i)^\\top\\xi)}{1+\\exp((x^i)^\\top\\xi)}=e^{-U(\\xi)}\n\\] を計算することを考える．ただし， \\[\\begin{align*}\n    U(\\xi)&:=-\\log p_0(\\xi)-\\sum_{i=1}^n\\log\\left(\\frac{\\exp(y^i(x^i)^\\top\\xi)}{1+\\exp((x^i)^\\top\\xi)}\\right)\\\\\n    &=:U_0(\\xi)+U_1(\\xi)\n\\end{align*}\\] と定めた．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#ロジットモデルの事後分布サンプラー",
    "href": "posts/2024/Stat/Logistic.html#ロジットモデルの事後分布サンプラー",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "2 ロジットモデルの事後分布サンプラー",
    "text": "2 ロジットモデルの事後分布サンプラー\nロジットリンクによる変換が複雑であるため，ロジスティック回帰は（完全な）ベイズ推定を実行することが難しいモデルとして知られてきた．\n一方で，リンク関数 \\(g\\) を標準正規分布 \\(\\mathrm{N}(0,1)\\) の分布関数の逆関数に取り替えた プロビットモデル は，Gaussian data augmentation (Albert and Chib, 1993) と呼ばれるデータ拡張に基づく Gibbs サンプラーが早くから提案されており，これにより効率的なベイズ推論が可能となっていた．\nプロビットモデルはロジットモデルに似ており，実用上はただ裾の重さが違うのみであると言って良い (Gelman et al., 2014, p. 407)．そのこともあり，プロビットモデルのベイズ推論は計量経済学や政治科学で広く使われている手法となったが，ロジットモデルのベイズ推論の応用は遅れた (Polson et al., 2013)．\nしかし実は，ロジットモデルの事後分布 \\(\\pi\\) も正規分布の Pólya-Gamma 混合として表すことができ，データ拡張によって効率的な Gibbs サンプラーを構成することができる (Polson et al., 2013)．現在ではこのデータ拡張 Gibbs サンプラーが，標準的な事後分布サンプラーとなっている．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#gibbs-サンプラーの課題不均衡データ",
    "href": "posts/2024/Stat/Logistic.html#gibbs-サンプラーの課題不均衡データ",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "3 Gibbs サンプラーの課題：不均衡データ",
    "text": "3 Gibbs サンプラーの課題：不均衡データ\nデータもモデルも大規模になっていく現代では，このようなデータ拡張に基づく Gibbs サンプラーは，特定の条件が揃うと極めて収束が遅くなる場面が少なくないことが明らかになってきている．\nそのうちの１つのパターンが大規模な 不均衡データ (Johndrow et al., 2019)，すなわち，特定のラベルが極めて稀少なカテゴリカルデータである．このようなデータに対しては，プロビットモデルやロジットモデルに限らず，ほとんど全てのデータ拡張に基づく Gibbs サンプラーが低速化することが報告されている：\n\nWe have found that this behavior occurs routinely, essentially regardless of the type and complexity of the statistical model, if the data are large and imbalanced. (Johndrow et al., 2019, p. 1395)"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#実験不均衡データでの収束鈍化",
    "href": "posts/2024/Stat/Logistic.html#実験不均衡データでの収束鈍化",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "4 実験：不均衡データでの収束鈍化",
    "text": "4 実験：不均衡データでの収束鈍化\nここでは問題を簡単にし，カテゴリーが２つの場合，即ち２値のスパースデータ \\(y^i\\in2=\\{0,1\\}\\) の場合を考える．\nこの下で， \\[\n\\sum_{i=1}^n y^i\\,\\bigg|\\,n\\sim\\mathrm{Bin}(n,g^{-1}(\\theta)),\\qquad\\theta\\sim\\mathrm{N}(0,B),\n\\] すなわち，モデル (1) において \\[\np=1,\\qquad X=1,\\qquad p_0(\\xi)d\\xi=\\mathrm{N}(a,B),\n\\] とした，説明変数なしの切片項のみでの回帰分析の場合を考える．この場合，ポテンシャルは次のように表される： \\[\n-U(\\xi)=\\xi\\sum_{i=1}^ny^i-n\\log(1+e^{\\xi})-\\frac{(\\xi-a)^2}{2B}-\\frac{1}{2}\\log2\\pi B.\n\\]\nここまで単純化した設定でも，前述の Gibbs サンプラーの収束鈍化が見られることを検証する．ここでは\n\n(a,B) = (0,100.0)\n\nそして \\[\n\\sum_{i=1}^ny^i=1\n\\] を保ちながら \\(n\\to\\infty\\) として実験するが，(Johndrow et al., 2019) では， \\[\n\\sum_{i=1}^ny^i\\ll n\n\\] である大規模不均衡データである限り，\\((a,B)\\) の値に依らず同様の結果が得られることが報告されている．\n\n\n\n\n\n\n実装の詳細\n\n\n\n\n\nMetropolis-Hastings 法は，Turing Institute による Julia の AdvancedMH.jl パッケージなどを通じて実装することができる：\n\n    \n        \n            \n            \n                Metropolis-Hastings サンプラー\n                Julia と Turing エコシステムを用いて\n            \n        \n    \n\n\nusing AdvancedMH\nusing Distributions\nusing MCMCChains\nusing ForwardDiff\nusing StructArrays\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\n\n# Define the components of a basic model.\nstruct LogTargetDensity_Logistic\n    a::Float64\n    B::Float64\n    n::Int64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensity_Logistic, ξ) = -log(2π * p.B) - (ξ[1] - p.a)^2/(2 * p.B) + ξ[1] - p.n * log(1 + exp(ξ[1]))\nLogDensityProblems.dimension(p::LogTargetDensity_Logistic) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensity_Logistic}) = LogDensityProblems.LogDensityOrder{0}()\n\nfunction MHSampler(n::Int64; discard_initial=30000)\n\n    model_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensity_Logistic(a, B, n))\n\n    spl = RWMH(MvNormal(zeros(1), I))\n\n    chain = sample(model_with_ad, spl, 50000; chain_type=Chains, param_names=[\"ξ\"])\n\n    return chain\nend\n\n# ξ_vector = MHSampler(10000)\n# plot(ξ_vector, title=\"Plot of \\$\\\\xi\\$ values\", xlabel=\"Index\", ylabel=\"ξ\", legend=false, color=\"#78C2AD\")\n\nMHSampler (generic function with 1 method)\n\n\n\nusing DataFrames\nusing Plots\n\nn_list = [10, 100, 1000, 10000]\n\nelapsed_time_Metropolis = @elapsed begin\n    chains = [MHSampler(n) for n in n_list]\nend\n\nautos = [DataFrame(autocor(chain, lags=1:100)) for chain in chains]\n\nMHChain = chains\n\ncombined_df = vcat(autos..., source=:chain)\n\nlag_columns = names(combined_df)[2:101]\nlags = 1:100\n\np_Metropolis = plot(\n    title = \"Metropolis\",\n    xlabel = \"Lag\",\n    ylabel = \"Autocorrelation\",\n    legend = :topright,\n    background_color = \"#F0F1EB\"\n)\n\nfor (i, n) in zip(1:4, n_list)\n    plot!(\n        p_Metropolis,\n        lags,\n        Array(combined_df[i, lag_columns]),\n        label = \"n = $n\",\n        linewidth = 2\n    )\nend\n\nSampling:   1%|▍                                        |  ETA: 0:00:10Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\nパッケージ PolyaGammaSamplers は現在，過去のバージョンの依存関係を必要とするので，グローバルの環境とは分離しておくのが良い．\nここでは，Pólya-Gamma 分布のサンプラーの実装 PolyaGammaSamplers を参考にして，直接次のように定義する．\n\nusing Random\nusing StatsFuns\n\nstruct PolyaGammaPSWSampler{T &lt;: Real} &lt;: Sampleable{Univariate, Continuous}\n    b::Int\n    z::T\nend\n\nstruct JStarPSWSampler{T &lt;: Real} &lt;: Sampleable{Univariate, Continuous}\n    z::T\nend\n\nfunction Base.rand(rng::AbstractRNG, s::PolyaGammaPSWSampler)\n    out = 0.0\n    s_aux = JStarPSWSampler(s.z / 2)\n    for _ in 1:s.b\n        out += rand(rng, s_aux) / 4\n    end\n    return out\nend\n\nfunction Base.rand(rng::AbstractRNG, s::JStarPSWSampler)\n    z = abs(s.z)  # modified to avoid negative z\n    t = 0.64\n    μ = 1 / z\n    k = π^2 / 8 + z^2 / 2\n    p = (π / 2 / k) * exp(- k * t) \n    q = 2 * exp( - z) * cdf(InverseGaussian(μ, 1.0), t)\n    while true\n        # Simulate a candidate x\n        u = rand(rng)\n        v = rand(rng)\n        if (u &lt; p / (p + q))\n            # (Truncated Exponential)\n            e = randexp(rng)\n            x = t + e / k\n        else\n            # (Truncated Inverse Gaussian)\n            x = randtigauss(rng, z, t)\n        end\n        # Evaluate if the candidate should be accepted\n        s = a_xnt(x, 0, t)\n        y = v * s\n        n = 0\n        while true\n            n += 1\n            if (n % 2 == 1)\n                s += a_xnt(x, n, t)\n                y &gt; s && break\n            else\n                s -= a_xnt(x, n, t)\n                y &lt; s && return x\n            end\n        end\n    end\nend\n\n# Return ``a_n(x)`` for a given t, see [1], eqs. (12)-(13)\n# Equations (12)-(13) in [1]\n# Note: \n# This is a literal transcription from the article's formula\n# except for the letter case\nfunction a_xnt(x::Real, n::Int, t::Real)\n    x ≤ t ? a_xnt_left(x, n, t) : a_xnt_right(x, n, t)\nend\n\n# Return ``a_n(x)^L`` for a given t\n# Equation (12) in [1]\n# Note: \n# This is a literal transcription from the article's formula\n# except for the letter case\nfunction a_xnt_left(x::Real, n::Int, t::Real)\n    π * (n + 0.5) * (2 / π / x)^(3 / 2) * exp(- 2 * (n + 0.5)^2 / x)\nend\n\n# Return ``a_n(x)^R`` for a given t, see [1], eq. (13)\n# Equation (13) in [1]\n# Note: \n# This is a literal transcription from the article's formula\n# except for the letter case\nfunction a_xnt_right(x::Real, n::Int, t::Real)\n    π * (n + 0.5) * exp(- (n + 0.5)^2 * π^2 * x / 2)\nend\n\n# Simulate from an IG(μ, 1) distribution\n# Algorithms 2-3 in [1]'s supplementary material\n# Note: \n# This is a literal transcription from the article's pseudo code\n# except for the letter case\nfunction randtigauss(rng::AbstractRNG, z::Real, t::Real)\n    1 / z &gt; t ? randtigauss_v1(rng, z, t) : randtigauss_v2(rng, z, t)\nend\n\n# Simulate from an IG(μ, 1) distribution, for μ := 1 / z &gt; t;\n# Algorithms 2 in [1]'s supplementary material\n# Note:\n# This is a literal transcription from the article's pseudo code\n# except for the letter case and one little a detail: the \n# original condition  `x &gt; R` must be replaced by `x &gt; t`\nfunction randtigauss_v1(rng::AbstractRNG, z::Real, t::Real)\n    x = t + one(t)\n    α = zero(t)\n    while rand(rng) &gt; α\n        e = randexp(rng) # In [1]: E \n        é = randexp(rng) # In [1]: E'\n        while e^2 &gt; (2 * é / t)\n            e = randexp(rng)\n            é = randexp(rng)\n        end\n        x = t / (1 + t * e)^2 \n        α = exp(- z^2 * x / 2)\n    end\n    return x\nend\n\n# Simulate from an IG(μ, 1) distribution, for μ := 1 / z ≤ t\n# Algorithms 3 in [1]'s supplementary material\n# Note: This is a literal transcription from the article's pseudo code\nfunction randtigauss_v2(rng::AbstractRNG, z::Real, t::Real)\n    x = t + one(t)\n    μ = 1 / z\n    while x &gt; t \n        y = randn(rng)^2\n        x = μ + μ^2 * y / 2 - μ * √(4 * μ * y + (μ * y)^2) / 2\n        if rand(rng) &gt; μ / (μ + x)\n            x = μ^2 / x\n        end\n    end\n    return x\nend\n\nrandtigauss_v2 (generic function with 1 method)\n\n\n\n# using PolyaGammaSamplers\n\nfunction PGSampler(n::Int64; discard_initial=30000, iter_number=50000, initial_ξ=0.0, B=100)\n\n    λ = 1 - n/2\n\n    ξ_list = [initial_ξ]\n    ω_list = []\n\n    while length(ξ_list) &lt; iter_number\n        ξ = ξ_list[end]\n        ω_sampler = PolyaGammaPSWSampler(n, ξ)\n        ω_new = rand(ω_sampler)\n        push!(ω_list, ω_new)\n        ξ_sampler = Normal((ω_new + B^(-1))^(-1) * λ, (ω_new + B^(-1))^(-1))\n        ξ_new = rand(ξ_sampler)\n        push!(ξ_list, ξ_new)\n    end\n\n    return Chains(ξ_list[discard_initial+1:end])\nend\n\nfunction Distributions.mean(s::PolyaGammaPSWSampler)\n    s.b * inv(2.0 * s.z) * tanh(s.z / 2.0)\nend\n\nfunction Distributions.var(s::PolyaGammaPSWSampler)\n    s.b * inv(4 * s.z^3) * (sinh(s.z) - s.z) * (sech(s.z / 2)^2)\nend\n\n\nelapsed_time_PolyaGamma = @elapsed begin\n    chains = [PGSampler(n) for n in n_list]\nend\nautos = [DataFrame(autocor(chain, lags=1:100)) for chain in chains]\n\nPGChain = chains\n\ncombined_df = vcat(autos..., source=:chain)\n\nlag_columns = names(combined_df)[2:101]\nlags = 1:100\n\np_PolyaGamma = plot(\n    title = \"Pólya-Gamma\",\n    xlabel = \"Lag\",\n    ylabel = \"Autocorrelation\",\n    legend = (0.65, 0.35),\n    background_color = \"#F0F1EB\"\n)\n\nfor (i, n) in zip(1:4, n_list)\n    plot!(\n        p_PolyaGamma,\n        lags,\n        Array(combined_df[i, lag_columns]),\n        label = \"n = $n\",\n        linewidth = 2,\n    )\nend\n\n\nprintln(\"Elapsed time: $elapsed_time_Metropolis seconds v.s. $elapsed_time_PolyaGamma seconds\")\n\nElapsed time: 2.233712584 seconds v.s. 77.15691 seconds\n\n\nPG サンプラーは MH 法に比べ恐ろしいほどに時間がかかる．これは，Turing のパッケージの最適化が優秀であるのか，Pólya-Gamma サンプラーの宿命であるのか，引き続き調べる必要がある．\n\n\n\n\n\nCode\nplot(p_Metropolis, p_PolyaGamma, layout=(1,2), background_color = \"#F0F1EB\")"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#理論収束鈍化の理由",
    "href": "posts/2024/Stat/Logistic.html#理論収束鈍化の理由",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "5 理論：収束鈍化の理由",
    "text": "5 理論：収束鈍化の理由\n前節の設定の下で，計算複雑性のオーダーが，Metropolis 法では最悪で \\((\\log n)^3\\) であるのに対し，Gibbs サンプラーでは最高でも \\(n^{3/2}(\\log n)^{2.5}\\) のオーダーになることが (Johndrow et al., 2019) で示されている．\nその理由は，提案分布と対象分布のズレに由来することも (Johndrow et al., 2019) は明らかにしている．\n\\(\\sum_{i=1}^ny^i\\) の値を固定して \\(n\\to\\infty\\) の極限を取った場合，事後分布は次のように，負方向にスライドしながら，幅が狭まっていく．その幅の縮小レートは \\(n^{-1/2}\\) ではなく，約 \\((\\log n)^{-1}\\) になる．\n\nusing StatsPlots\nusing LaTeXStrings\n\nplot(\n    plot(MHChain[1], title=L\"n=10\", color=\"#78C2AD\"),\n    plot(MHChain[2], title=L\"n=100\", color=\"#78C2AD\"),\n    plot(MHChain[3], title=L\"n=1000\", color=\"#78C2AD\"),\n    plot(MHChain[4], title=L\"n=10000\", color=\"#78C2AD\"),\n    layout=(4,1),\n    size=(1000, 800),\n    #background_color = \"#F0F1EB\"\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n一方で，提案分布は \\(\\xi_t\\) をモードとした場合，\\(\\xi_{t+1}\\) もモードの周りに幅 \\(\\frac{(\\log n)^{3/2}}{n^{1/2}}\\) で集中してしまう．すなわち，提案のステップサイズが事後分布のスケールに比べて極めて小さくなってしまう．\n\nplot(\n    plot(PGChain[1], title=L\"n=10\", color=\"#78C2AD\"),\n    plot(PGChain[2], title=L\"n=100\", color=\"#78C2AD\"),\n    plot(PGChain[3], title=L\"n=1000\", color=\"#78C2AD\"),\n    plot(PGChain[4], title=L\"n=10000\", color=\"#78C2AD\"),\n    layout=(4,1),\n    size=(1000, 800),\n    #background_color = \"#F0F1EB\"\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMH 法でサンプルした事後分布に比べて，より鋭くなっていることがわかるだろう（\\(y\\) 軸のスケールに注目）．\nこれにより，分布の十分な探索が阻害され，サンプル間の自己相関が高くなってしまうという問題が起こるようである．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#比例的高次元では-map-にバイアスが残る",
    "href": "posts/2024/Stat/Logistic.html#比例的高次元では-map-にバイアスが残る",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "6 比例的高次元では MAP にバイアスが残る",
    "text": "6 比例的高次元では MAP にバイアスが残る\nロジスティック回帰において，\\(p\\ll n\\) が満たされない設定下では，最尤推定量（MAP 推定量）がバイアスを持つ (Sur and Candès, 2019)．\n後編に続く：\n\n    \n        \n            \n            \n                大規模な不均衡データに対するロジスティック回帰（後編）\n                離散時間 MCMC から連続時間 MCMC へ"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#footnotes",
    "href": "posts/2024/Stat/Logistic.html#footnotes",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(x\\) が確率を表すとき \\(\\frac{x}{1-x}\\) という量はオッズともいい，それゆえ \\(p\\) のロジットは対数オッズ比ともいう．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html",
    "href": "posts/2024/Stat/Logistic2.html",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n前稿はこちら："
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#導入",
    "href": "posts/2024/Stat/Logistic2.html#導入",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.1 導入",
    "text": "1.1 導入\n説明変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p)\\) と係数 \\(\\xi\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p)\\) に関するロジスティックモデル\n\\[\n\\operatorname{P}[Y=1\\,|\\,X,\\xi]=g^{-1}(X^\\top\\xi)=\\frac{\\exp(X^\\top\\xi)}{1+\\exp(X^\\top\\xi)}\\tag{1}\n\\]\nにおいて，\\(\\{y^i\\}\\) または \\(\\{x^i_j\\}\\) が不均衡であった場合は，前節で見たように Gibbs サンプラーによる方法ではスケールしない．\nよく調整された Metropolis-Hastings 法を用いることが解決の１つとしてあり得るが，やはり本当に大きな \\(n,p\\) に関してスケールしないことが問題である．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#事後分布の特徴",
    "href": "posts/2024/Stat/Logistic2.html#事後分布の特徴",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.2 事後分布の特徴",
    "text": "1.2 事後分布の特徴\n実は，ロジスティック回帰ではポテンシャル \\(U\\) の勾配が有界になり，簡単なサブサンプリングが可能である．\n事後分布の負の対数密度は \\[\\begin{align*}\n    U(\\xi)&:=-\\log p_0(\\xi)-\\sum_{i=1}^n\\log\\left(\\frac{\\exp(y^i(x^i)^\\top\\xi)}{1+\\exp((x^i)^\\top\\xi)}\\right)\\\\\n    &=:U_0(\\xi)+U_1(\\xi)\n\\end{align*}\\] と表せる．\n\\[\nU_1(\\xi)=\\frac{1}{n}\\sum_{j=1}^nU^j_1(\\xi)\n\\] \\[\nU_1^j(\\xi)=-n\\log\\left(\\frac{\\exp\\left(y^j(x^j)^\\top\\xi\\right)}{1+\\exp\\left((x^j)^\\top\\xi\\right)}\\right)\n\\] であるから， \\[\n\\partial_iU^j_1(\\xi)=n\\frac{x^j_i\\exp\\left((x^j)^\\top\\xi\\right)}{1+\\exp\\left((x^j)^\\top\\xi\\right)}-ny^jx^j_i&lt;nx^j_i(1-y^j)\n\\] を得る．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#poisson-剪定",
    "href": "posts/2024/Stat/Logistic2.html#poisson-剪定",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.3 Poisson 剪定",
    "text": "1.3 Poisson 剪定\n特に各 \\(\\partial_iU_1(\\xi)\\) のバウンドとして \\[\n\\lvert\\partial_iU^j_1(\\xi)\\rvert\\le n\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\qquad i\\in[d]\n\\tag{1}\\] を得るから， \\[\n\\lambda_i(\\xi,\\theta)=\\biggr(\\theta_i\\partial_iU(\\xi)\\biggl)_+\\le\\biggr(n\\theta_i\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\biggl)_+\n\\] を元に剪定を実行できる．\n\n\nサブサンプリングなしの Zig-Zag 過程のシミュレーションをする関数 ZZ1d() を定義\nusing ZigZagBoomerang\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing Statistics  # just for sure\nusing StatsFuns\n\n\"\"\"\n    ∇U(i,j,ξ,x,y)\n        i ∈ [d]: 次元を表すインデックス\n        j ∈ [n]: サンプル番号を表すインデックス\n        ξ: パラメータ空間 R^d 上の位置\n        他，観測 (x,y) を引数にとる．\n    この関数を実装する際，log の中身をそのまま計算しようとすると大変大きくなり，数値的に不安定になる（除算の後は 1 近くになるはずだが，Inf になってしまう）\n\"\"\"\n∇U(i::Int64, j::Int64, ξ, x::Matrix{Float64}, y::Vector{Float64}) = length(y) * x[i,j] * (logistic(dot(x[:,j],ξ)) - y[j])\n\n\"\"\"\n    ∇U(i,ξ,x,y)：∇U(i,j,ξ,x,y) を全データ j ∈ [n] について足し合わせたもの\n        i ∈ [d]: 次元を表すインデックス\n        ξ: パラメータ空間 R^d 上の位置\n        他，観測 (x,y) を引数にとる．\n\"\"\"\nfunction ∇U(i::Int64, ξ, x::Matrix{Float64}, y::Vector{Float64})\n    n = length(y)\n    U_list = []\n    for j in 1:n\n        push!(U_list, ∇U(i, j, ξ, x, y))\n    end\n    return mean(U_list)\nend\n\nfunction  ∇U(ξ, x::Matrix{Float64}, y::Vector{Float64})  # 1次元の場合のショートカット\n    return ∇U(1, ξ, x, y)\nend\n\npos(x) = max(zero(x), x)\n\n\"\"\"\n    λ(i, ξ, θ, ∇U, x, y)：第 i ∈ [d] 次元のレート関数\n        i ∈ [d]: 次元を表すインデックス\n        (ξ,θ): E 上の座標\n        ∇U\n        (x,y): 観測\n\"\"\"\nλ(i::Int64, ξ, θ, ∇U, x, y) = pos(θ[i] * ∇U(i, ξ, x, y))\nλ(ξ, θ, ∇U, x, y) = pos(θ * ∇U(ξ, x, y))  # 1次元の場合のショートカット\n\n\"\"\"\n    λ(τ, a, b)：代理レート関数の時刻 τ における値\n        τ: 時間\n        a,b: 1次関数の係数\n\"\"\"\nλ_bar(τ, a, b) = pos(a + b*τ)\n\n\"\"\"\n`x`: current location, `θ`: current velocity, `t`: current time,\n\"\"\"\nfunction move_forward(τ, t, ξ, θ, ::ZigZag1d)\n    τ + t, ξ + θ*τ , θ\nend\n\n\"\"\"\n    ZZ1d(∇U, ξ, θ, T, x, y, Flow; rng=Random.GLOBAL_RNG, ab=ab_Global)：ZigZag sampler without subsampling\n        `∇U`: gradient of the negative log-density\n        `(ξ,θ)`: initial state\n        `T`: Time Horizon\n        `(x,y)`: observation\n        `Flow`: continuous dynamics\n\n        `a+bt`: computational bound for intensity m(t)\n\n        `num`: ポアソン時刻に到着した回数\n        `acc`: 受容回数．`acc/num` は acceptance rate\n\"\"\"\nfunction ZZ1d(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_Global)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        l, lb = λ(ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n                println(l)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\na_Global(ξ, θ, x, y) = length(y) * maximum(abs.(vec(x)))\nb_Global(ξ, θ, x, y) = 0\n\nab_Global(ξ, θ, x, y, ::ZigZag1d) = (a_Global(ξ, θ, x, y), b_Global(ξ, θ, x, y))\n\nそこで，さらにタイトに，次の affine 関数による評価を考えることになる： \\[\n\\overline{m}_i(t):=a_i+b_it,\\qquad i\\in[d],\n\\] \\[\na_i:=(\\theta_i\\partial_iU(\\xi_*))_++C_i\\lvert\\xi-\\xi_*\\rvert\n\\] \\[\nb_i:=C_i\\sqrt{d},\\qquad C_i:=\\frac{n}{4}\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\lvert x^j\\rvert.\n\\]\n\n\n観測を生成\nusing StatsFuns\nusing Distributions\n\nξ0 = [1.0] # True value\nn_list = [10, 100, 1000]  # 実験で用いるサンプルサイズの列\n\nΣ = [2]\nx = rand(MvNormal(ξ0, Σ), n_list[end])\ny = rand.(Bernoulli.(logistic.(ξ0*x)))  # BitVector になってしまう\ny = Float64.(vec(y))  # Vector{Float64} に変換\n\n\n\nusing Statistics\nusing LinearAlgebra\n\n\"\"\"\n    U(ξ, x, y)：ポテンシャル関数\n        ξ: パラメータ空間上の点\n        (x,y): 観測\n\"\"\"\nfunction U(ξ, x, y)\n    n = length(y)\n    U_list = []\n    for j in 1:n\n        push!(U_list, U(j, ξ, x, y))\n    end\n    return mean(U_list)\nend\nfunction U(j, ξ, x, y)\n    n = length(y)\n    product = dot(x[:,j],ξ)\n    return -n * log(exp(y[j] * product) / (1 + exp(product)))\nend\n\nusing Optim\n\nresult = optimize(ξ -&gt; U(ξ, x, y), [0.0], LBFGS())\nξ_star = Optim.minimizer(result)\n\nfunction C(ξ, θ, x, y)\n    n = length(y)\n    max_value = maximum(x.^2)\n    return n * max_value / 4\nend\n\na_Affine(ξ, θ, x, y) = pos(θ * ∇U(ξ_star,x,y)) + C(ξ, θ, x, y) * abs(ξ - ξ_star[1])\nb_Affine(ξ, θ, x, y) = C(ξ, θ, x, y)\n\n# computational bounds for intensity m(t)\nab_Affine(ξ, θ, x, y, ::ZigZag1d) = (a_Affine(ξ, θ, x, y), b_Affine(ξ, θ, x, y))"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#サブサンプリング",
    "href": "posts/2024/Stat/Logistic2.html#サブサンプリング",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.4 サブサンプリング",
    "text": "1.4 サブサンプリング\nPoisson 過程の強度関数を確率化し，\\(K\\sim\\mathrm{U}([n])\\) に対して \\[\nm_i^K(t):=\\biggr(\\theta_i E^K_i(x+\\theta t)\\biggl)_+\n\\] \\[\nE^K_i(x):=\\partial_iU(\\xi_*)+\\partial_iU^K(\\xi)-\\partial_iU^K(\\xi_*)\n\\] としても，引き続き同様の上界を持つ．\nここで，\\(m_i^K\\) の評価は \\(m_i\\) より \\(n\\) 倍軽量になっていることに注意．\n\n\nサブサンプリングありの Zig-Zag 過程のシミュレーションをする関数 ZZ1d_SS() と ZZ1d_CV() を定義\nusing ZigZagBoomerang\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing Statistics  # just for sure\nusing StatsFuns\n\nfunction λj_Global(j::Int64, ξ, θ, ∇U, x, y)\n    Eʲ = ∇U(1, j, ξ, x, y)\n    return pos(θ * Eʲ)\nend\n\nfunction ZZ1d_SS(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_Global)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        j = rand(1:length(y))\n        l, lb = λj_Global(j, ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\nfunction λj(j::Int64, ξ, θ, ∇U, x, y)\n    Eʲ = ∇U(ξ_star, x, y) + ∇U(1, j, ξ, x, y) - ∇U(1, j, ξ_star, x, y)\n    return pos(θ * Eʲ)\nend\n\nfunction ZZ1d_CV(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_Affine)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        j = rand(1:length(y))\n        l, lb = λj(j, ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#数値実験による性能比較",
    "href": "posts/2024/Stat/Logistic2.html#数値実験による性能比較",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.5 数値実験による性能比較",
    "text": "1.5 数値実験による性能比較\n\n\nサブサンプリングなしの実験を実行する関数 experiment_ZZ() を定義\nusing Statistics\n\nfunction ESS(samples::Vector{Float64}, T, dt)\n    B = T / dt\n    V = (dt / T) * sum(samples.^2) - ((dt / T) * sum(samples))^2\n    Y = samples .* sqrt(T / B)\n    ESS = T * V / var(Y)\n    return ESS\nend\n\nfunction getESSperEpoch(ab, T ,dt, x, y; ξ0=0.0, θ0=1.0)\n    trace, epochs, acc = ZZ1d(∇U, ξ0, θ0, T, x, y, ZigZag1d(); ab=ab)\n    traj = discretize(trace, ZigZag1d(), dt)\n    return ESS(traj.x, T, dt) / epochs[end]\nend\n\nN = 10\nT = 500.0\ndt = 0.1\n\nfunction experiment_ZZ(N, T, dt; ξ0=0.0, θ0=1.0, n_list=[10, 100, 1000])  # サブサンプリングなしの ZZ() に関して N 回実験\n    ESSs_sum_Affine = zero(n_list)\n    ESSs_sum_Global = zero(n_list)\n\n    for _ in 1:N\n        ESSs_Affine = []\n        ESSs_Global = []\n        for n in n_list\n            push!(ESSs_Affine, getESSperEpoch(ab_Affine, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_Global, getESSperEpoch(ab_Global, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n        end\n        ESSs_sum_Affine = [ESSs_sum_Affine ESSs_Affine]\n        ESSs_sum_Global = [ESSs_sum_Global ESSs_Global]\n    end\n    return mean(ESSs_sum_Affine, dims=2), var(ESSs_sum_Affine, dims=2), mean(ESSs_sum_Global, dims=2), var(ESSs_sum_Global, dims=2)\nend\n\n# ESS_Affine, var_ESS_Affine, ESS_Global, var_ESS_Global = experiment_ZZ(2, T, dt; ξ0=0.0, θ0=1.0, n_list=n_list)\n\n\n\n\n実験には 10分 かかるので，保持した実行結果を用いる\nusing JLD2\n\n@load \"Logistic2_Experiment1.jld2\" ESS_Affine var_ESS_Affine ESS_Global var_ESS_Global\n\n\n\n\nサブサンプリング付きの実験を実行する関数 experiment_ZZ() を定義\nusing Statistics\n\n# function ESS(samples::Vector{Float64}, T, dt)\n#     B = T / dt\n#     V = (dt / T) * sum(samples.^2) - ((dt / T) * sum(samples))^2\n#     Y = samples .* sqrt(T / B)\n#     ESS = T * V / var(Y)\n#     return ESS\n# end\n\nfunction getESSperEpoch_SS(ab, ZZ, T ,dt, x, y; ξ0=0.0, θ0=1.0)\n    trace, epochs, acc = ZZ(∇U, ξ0, θ0, T, x, y, ZigZag1d(); ab=ab)\n    traj = discretize(trace, ZigZag1d(), dt)\n    return ESS(traj.x, T, dt) * length(y) / epochs[end]  # サブサンプリングをしているので length(y) で補正する必要あり\nend\n\nN = 10\nT = 500.0\ndt = 0.1\n\nfunction experiment_ZZ_SS(N, T, dt; ξ0=0.0, θ0=1.0, n_list=[10, 100, 1000])  # サブサンプリングなしの ZZ() に関して N 回実験\n    ESSs_sum_CV = zero(n_list)\n    ESSs_sum_SS = zero(n_list)\n\n    for _ in 1:N\n        ESSs_CV = []\n        ESSs_SS = []\n        for n in n_list\n            push!(ESSs_CV, getESSperEpoch_SS(ab_Affine, ZZ1d_CV, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_SS, getESSperEpoch_SS(ab_Global, ZZ1d_SS, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n        end\n        ESSs_sum_CV = [ESSs_sum_CV ESSs_CV]\n        ESSs_sum_SS = [ESSs_sum_SS ESSs_SS]\n    end\n    return mean(ESSs_sum_CV, dims=2), var(ESSs_sum_CV, dims=2), mean(ESSs_sum_SS, dims=2), var(ESSs_sum_SS, dims=2)\nend\n\n# ESS_CV, var_ESS_CV, ESS_SS, var_ESS_SS = experiment_ZZ_SS(2, T, dt; ξ0=0.0, θ0=1.0, n_list=n_list)\n\n\n\n\n結果をプロット\nq = addPlot(q, n_list, ESS_CV, sqrt.(var_ESS_CV); label=\"ZZ-CV\", color=\"darkorange\")\nq = addPlot(q, n_list, ESS_SS, sqrt.(var_ESS_SS); label=\"ZZ-SS\", color=\"blue\")\nq = plot!(q, legend=:bottomleft)\ndisplay(q)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n実際に用いたコードはこちら．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#比較対象mala",
    "href": "posts/2024/Stat/Logistic2.html#比較対象mala",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.6 比較対象：MALA",
    "text": "1.6 比較対象：MALA\n\n\nMALA によるサンプリングを実行\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing LinearAlgebra\n\nstruct LogTargetDensity\n    x::Matrix{Float64}\n    y::Vector{Float64}\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensity, ξ) = -U(ξ, p.x, p.y)\nLogDensityProblems.dimension(p::LogTargetDensity) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensity}) = LogDensityProblems.LogDensityOrder{0}()\n\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensity(x, y))\n\nσ² = 0.0001\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\nchain = sample(model_with_ad, spl, 2000; initial_params=ξ0, chain_type=StructArray, param_names=[\"ξ\"], stats=true)\n\ntraj_MALA = chain.ξ"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#有効サンプル数について",
    "href": "posts/2024/Stat/Logistic2.html#有効サンプル数について",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.7 有効サンプル数について",
    "text": "1.7 有効サンプル数について\n時区間 \\([0,T]\\) における ZigZag 過程の，関数 \\(h\\in\\mathcal{L}^2(\\mathbb{R}^d)\\) に関する 有効サンプル数 (ESS) とは \\[\n\\widehat{\\operatorname{ESS}}:=T\\frac{\\widehat{\\mathrm{V}_\\pi[h]}}{\\widehat{\\sigma^2_h}}\n\\] \\[\n\\widehat{\\mathrm{V}_\\pi[h]}:=\\frac{1}{T}\\int^T_0h(X_s)^2\\,ds-\\left(\\frac{1}{T}\\int^T_0h(X_s)\\,ds\\right)^2,\n\\] \\[\n\\widehat{\\sigma^2_h}:=\\frac{1}{B-1}\\sum_{i=1}^B(Y_i-\\overline{Y})^2,\\quad Y_i:=\\sqrt{\\frac{B}{T}}\\int^{\\frac{iT}{B}}_{\\frac{(i-1)T}{B}}h(X_s)\\,ds\n\\] で定まる値である．\n例えば次のようにして計算できる：\nfunction ESS(samples::Vector{Float64}, T, dt)\n    V = (dt / T) * sum(samples.^2) - ((dt / T) * sum(samples))^2\n    Y = samples .* sqrt(T / B)\n    ESS = T * V / var(Y)\n    return ESS\nend"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#重点サブサンプリング-sen2020",
    "href": "posts/2024/Stat/Logistic2.html#重点サブサンプリング-sen2020",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.8 重点サブサンプリング (Sen et al., 2020)",
    "text": "1.8 重点サブサンプリング (Sen et al., 2020)\n一様でないサブサンプリングを導入することで，Zig-Zag サンプラーを不均衡データにも強くすると同時に，サブサンプリングの効率を上げることもできる．\n強度関数 \\[\nm_i^K(t)=\\biggr(\\theta_iE^K_i(x+\\theta t)\\biggl)_+\n\\] は， \\[\n\\operatorname{E}\\biggl[E^K_i(\\xi)\\biggr]=\\partial_iU(\\xi)\n\\] を満たす限り，\\(K\\sim\\mathrm{U}([n])\\) に限る必要はなかったのである．\nすなわち，\\((p_x)\\) をある \\([n]\\) 上の分布 \\(\\nu\\in\\mathcal{P}([n])\\) の質量関数として \\[\n\\partial_iV_1^J(\\xi):=\\frac{1}{p_J}\\partial_iU^J(\\xi)\\qquad J\\sim\\nu\n\\] と定めると， \\[\n\\lvert\\partial_iV_i^j(\\xi)\\rvert\\le\\max_{j\\in[n]}\\frac{\\lvert x_i^j\\rvert}{p_j}\n\\] が成り立つ．式 (1) は \\(p_j\\equiv1/n\\) の場合であったのである．\n換言すれば， \\[\np_j\\,\\propto\\,\\lvert x^j_i\\rvert\n\\] と定めることで，Poisson 強度関数 \\(m^j_i\\) の上界をタイトにすることができ，その結果剪定の効率が上がる．\n\na_IS(ξ, θ, x, y) = sum(abs.(x))\nb_IS(ξ, θ, x, y) = 0\n\nab_IS(ξ, θ, x, y, ::ZigZag1d) = (a_IS(ξ, θ, x, y), b_IS(ξ, θ, x, y))\n\n\n\n重点サブサンプリングによる ZigZag サンプラー ZZ1d_IS() を定義\nusing StatsBase\n\nfunction λj_IS(j::Int64, ξ, θ, ∇U, x, y)\n    pj = abs(x[1,j]) / sum(abs.(x))  # x がスパースだと 0 になりやすいことに注意\n    Eʲ = ∇U(1, j, ξ, x, y) / (length(y) * pj)\n    return pos(θ * Eʲ)\nend\n\nfunction ZZ1d_IS(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_IS)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n    n = length(y)\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        j = sample(1:n, Weights(abs.(vec(x))))\n        l, lb = λj_IS(j, ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n実験を実行する関数 experiment_ZZ_IS() を定義\nfunction experiment_ZZ_IS(N, T, dt; ξ0=0.0, θ0=1.0, n_list=[10, 100, 1000])  # 重点サブサンプリング ZZ1d_IS() に関して N 回実験\n    ESSs_sum_IS = zero(n_list)\n\n    for _ in 1:N\n        ESSs_IS = []\n        for n in n_list\n            push!(ESSs_IS, getESSperEpoch_SS(ab_IS, ZZ1d_IS, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n        end\n        ESSs_sum_IS = [ESSs_sum_IS ESSs_IS]\n    end\n    return mean(ESSs_sum_IS, dims=2), var(ESSs_sum_IS, dims=2)\nend\n\nESS_IS, var_ESS_IS = experiment_ZZ_IS(2, 500.0, 1.0; ξ0=0.0, θ0=1.0, n_list=n_list)\n\n\n\n\n結果をプロット\nr = addPlot(q, n_list, ESS_IS, sqrt.(var_ESS_IS); label=\"ZZ-IS\", color=\"green\")\ndisplay(r)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n制御変数による方法は \\(n\\to\\infty\\) の漸近論に基づいているので，観測数が増えるほど効率は上がっていく．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#大規模不均衡データ",
    "href": "posts/2024/Stat/Logistic2.html#大規模不均衡データ",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1.9 大規模不均衡データ",
    "text": "1.9 大規模不均衡データ\n大規模不均衡データでは，事後分布が十分な集中性を持たないために制御変数による方法 ZZ-CV が十分な効率改善を示さないが，重点サブサンプリングによれば Poisson 強度関数のタイトな上界を引き続き構成できる．\nここでは，\\(\\xi_0=1\\) を真値とし，次のような１次元データを考える： \\[\nX^j\\overset{\\text{iid}}{\\sim}(1-\\alpha)\\delta_0+\\alpha\\mathrm{N}(1,2)\n\\] \\[\n\\operatorname{P}[Y^j=1]=\\frac{1}{1+e^{-X^j}}\n\\]\n\nξ0 = [1.0] # True value\nΣ = [2]\nn = 1000\n\nfunction sample_SparseData(n::Int64, α::Float64; ρ=MvNormal(ξ0, Σ))\n    x = []\n    while length(x) &lt; n\n        rand() &lt; α ? push!(x, rand(ρ)[1]) : push!(x, 0.0)\n    end\n    x = Float64.(reshape(x,1,:))\n    y = rand.(Bernoulli.(logistic.(ξ0*x)))\n    y = Float64.(vec(y))\n    return x, y\nend\n\nα_list = [1, 0.1, 0.01]\n\nx_Sparse, y_Sparse = [], []\n\nfor α in α_list\n    x, y = sample_SparseData(n, α)\n    push!(x_Sparse, x)\n    push!(y_Sparse, y)\nend\n\n\nusing Optim\n\na_Sparse(ξ_star, ξ, θ, x, y) = pos(θ * ∇U(ξ_star,x,y)) + C(ξ, θ, x, y) * abs(ξ - ξ_star[1])\nξ_star_list = []\nα_list = [1, 0.1, 0.01]\n\nfor α in 1:length(α_list)\n    result = optimize(ξ -&gt; U(ξ, x_Sparse[α], y_Sparse[α]), [0.0], LBFGS())\n    ξ = Optim.minimizer(result)\n    push!(ξ_star_list, ξ)\nend\n\nfunction experiment_Sparse(N, T, dt; ξ0=0.0, θ0=1.0, α_list=[1, 0.1, 0.01, 0.001])\n    # ESSs_sum_CV = zero(α_list)\n    ESSs_sum_SS = zero(α_list)\n    ESSs_sum_IS = zero(α_list)\n\n    for _ in 1:N\n        # ESSs_CV = []\n        ESSs_SS = []\n        ESSs_IS = []\n        for i in 1:length(α_list)\n            # ab_Sparse(ξ, θ, x, y, ::ZigZag1d) = (a_Sparse(ξ_star_list[i], ξ, θ, x, y), b_Affine(ξ, θ, x, y))\n            # push!(ESSs_CV, getESSperEpoch_SS(ab_Sparse, ZZ1d_CV, T, dt, x_Sparse[i], y_Sparse[i]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_SS, getESSperEpoch_SS(ab_Global, ZZ1d_SS, T, dt, x_Sparse[i], y_Sparse[i]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_IS, getESSperEpoch_SS(ab_IS, ZZ1d_IS, T, dt, x_Sparse[i], y_Sparse[i]; ξ0=ξ0, θ0=θ0))\n        end\n        # ESSs_sum_CV = [ESSs_sum_CV ESSs_CV]\n        ESSs_sum_SS = [ESSs_sum_SS ESSs_SS]\n        ESSs_sum_IS = [ESSs_sum_IS ESSs_IS]\n    end\n    # return mean(ESSs_sum_CV, dims=2), var(ESSs_sum_CV, dims=2), mean(ESSs_sum_SS, dims=2), var(ESSs_sum_SS, dims=2), mean(ESSs_sum_IS, dims=2), var(ESSs_sum_IS, dims=2)\n    return mean(ESSs_sum_SS, dims=2), var(ESSs_sum_SS, dims=2), mean(ESSs_sum_IS, dims=2), var(ESSs_sum_IS, dims=2)\nend\n\nN = 2\nT = 500.0\ndt = 0.1\n\nESS_SS, var_ESS_SS, ESS_IS, var_ESS_IS = experiment_Sparse(N, T, dt; ξ0=0.0, θ0=1.0, α_list=α_list)\n\n\n\nプロット\nusing LaTeXStrings\np = startPlot(α_list, ESS_SS, sqrt.(var_ESS_SS); label=\"ZZ-SS\", xlabel=L\"Sparsity $\\alpha$\", color=\"blue\"#, background_color=true\n)\np = addPlot(p, α_list, ESS_IS, sqrt.(var_ESS_IS); label=\"ZZ-IS\", color=\"green\")\np = addPlot(p, α_list, ESS_CV, sqrt.(var_ESS_CV); label=\"ZZ-CV\", color=\"darkorange\")\ndisplay(p)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nここでは問題にしないが，圧倒的に実行時間が重点サブサンプリングの方が短い．\n\n\n\n\n\n\n注\n\n\n\n\n\n\\(\\alpha&lt;10^{-3}\\) の領域では動作が不安定になる．論文 (Sen et al., 2020) でもこの領域は触れられていない．しかし， \\[\n\\#\\left\\{i\\in[n]\\mid y^i=1\\right\\}\\approx500\n\\] であるため，特に理由は見つからない．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html",
    "href": "posts/2024/Stat/ZigZagSubsampling.html",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nMCMC の計算複雑性のボトルネックは，尤度の評価にある．各ステップで全てのデータを用いて尤度を計算する必要がある点が，MCMC を深層学習などの大規模データの設定への応用を難しくしている (Murphy, 2023, p. 647)．\nサブサンプリングが可能であることと，複数の効率的なサブサンプリング法の提案により，Zig-Zag 過程は次世代のサンプラーとして圧倒的なスケーラビリティ（Super Efficient Bayesian Inference (Bierkens et al., 2019)）を示すのではないかと期待されている．1"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#対数尤度の勾配を不変推定する",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#対数尤度の勾配を不変推定する",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "1 対数尤度の勾配を不変推定する",
    "text": "1 対数尤度の勾配を不変推定する\n\\(p(x)\\) を事前分布，\\(p(y|x)\\) を観測のモデル（または尤度）とし，データ \\(y_1,\\cdots,y_n\\) は互いに独立であるとする．\nこのとき，事後分布 \\(\\pi(x):=p(x|y)\\) と Hamiltonian \\(U\\) は次のように表せる： \\[\n\\pi(x)\\,\\propto\\,\\left(\\prod_{k=1}^n p(y_k|x)\\right)p(x)\n\\] \\[\\begin{align*}\n   U(x)&=-\\sum_{k=1}^n\\log p(y_k|x)-\\log p(x)\\\\\n   &=\\frac{1}{n}\\sum_{k=1}^n\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl)=:\\frac{1}{n}\\sum_{k=1}^nU^k(x).\n\\end{align*}\\]\nこのとき，\\(U\\) の導関数 \\(\\partial_i U(x)\\) は，独立な観測 \\(y_1,\\cdots,y_n\\) について項別微分をして平均をとったものに等しい： \\[\n\\partial_iU(x)=\\frac{1}{n}\\sum_{k=1}^nE^k_i(x),\n\\tag{1}\\] \\[\nE^k_i(x):=\\partial_iU^k(x)=\\frac{\\partial }{\\partial x_i}\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl).\n\\]\nよって，精度は劣るかもしれないが，一様に選んだ \\(K\\sim\\mathrm{U}([n])\\) から定まる \\(E^K_i\\) の値は \\(\\partial_i U(x)\\) の不偏推定量となっている．この発想により，ZZ-SS という新たなアルゴリズムを構成できる．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#サブサンプリングを取り入れた-zig-zag-サンプラー",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#サブサンプリングを取り入れた-zig-zag-サンプラー",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "2 サブサンプリングを取り入れた Zig-Zag サンプラー",
    "text": "2 サブサンプリングを取り入れた Zig-Zag サンプラー\nこの各 \\(E^K_i\\) が定める強度関数 \\[\nm^K_i(t):=\\biggr(\\theta E^K_i(x+\\theta t)\\biggl)_+=\\biggr(\\theta\\partial_iU^K(x+\\theta t)\\biggl)_+\n\\] を用いた Zig-Zag サンプラーを (Bierkens et al., 2019) では ZZ-SS (Zig-Zag with Sub-Sampling) と呼んでいる．\n\\[\n\\max_{k\\in[n]}m^k_i\\le M_i\n\\] を満たす連続関数 \\(M_i\\) を用いて次のようにシミュレーションすることができる：\n\n\n\n\n\n\n(Bierkens et al., 2019, p. 1303 アルゴリズム３)\n\n\n\n\n代理強度関数 \\(M_1,\\cdots,M_d\\) を持つ互いに独立な \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程の到着時刻 \\(T_1,\\cdots,T_d\\) をシミュレーションする．\n最初に到着した座標番号 \\(j:=\\operatorname*{argmin}_{i\\in[d]}T_i\\) について，確率 \\[\n\\frac{m^K_j(T_j)}{M_j(T_j)},\\qquad K\\sim\\mathrm{U}([n]),\n\\] で時刻 \\(T_j\\) に速度成分 \\(\\theta_j\\) の符号を反転させる．\n１に \\(t=T_j\\) として戻って，繰り返す．\n\n\n\n\n\n\n\n\n\n部分サンプリングにより不変分布が変わらないことの証明2\n\n\n\n\n\nZZ-SS によってシミュレートされる過程は，レート関数 \\[\n\\lambda_i(x,\\theta)=\\operatorname{E}[(\\theta E^K_i(x))_+]=\\frac{1}{n}\\sum_{k=1}^n(\\theta E^k_i(x))_+\n\\] を持った Zig-Zag 過程に等しい\nこれは，元々のレート関数に対して， \\[\n\\gamma_i(x,\\theta):=\\frac{1}{n}\\sum_{k=1}^n(\\theta_iE^k_i(x))_+-\\left(\\theta_i\\frac{1}{n}\\sum_{k=1}^nE^k_i(x)\\right)_+\n\\] という項を加えて得る Zig-Zag サンプラーともみなすことができる．非負性は関数 \\((x)_+:=x\\lor0\\) の凸性から従う．最後に \\(\\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta))\\) を確認すれば良い．\nこれは \\[\\begin{align*}\n  &\\qquad\\frac{1}{n}\\sum_{k=1}^n\\biggr(\\theta_iE_i^k(x)\\biggl)_+-\\frac{1}{n}\\sum_{k=1}^n\\biggr(-\\theta_iE_i^k(x)\\biggl)_+\\\\\n  &=\\frac{1}{n}\\sum_{k=1}^n\\left((\\theta_iE_i^k(x))_+-(-\\theta_iE_i^k(x))_+\\right)=\\frac{1}{n}\\sum_{k=1}^n\\theta_iE_i^k(x)\n\\end{align*}\\] であることから従う．\nこうして，サブサンプリングの実行による精度の劣化が，(Andrieu and Livingstone, 2021) の枠組みで捉えられる，ということでもある（レート関数が増加したので，スイッチングイベントが増え，diffusive な動きが増加する）．\n\n\n\n例えば \\(p(y_k|x)\\) が Cauchy 密度であるなど \\(\\partial_iU\\) が有界であるとき，\\(M_i:=\\max_{x\\in\\mathbb{R}^d}\\partial_iU(x)\\) などと選ぶことができる．\\(M_i\\) をより \\(\\partial_iU\\) に近く選ぶほど剪定の効率は上がるが，\\(M_i\\) を複雑にしすぎると今度は \\(M_i\\) を強度とする Poisson 点過程のシミュレーションが困難になる．\nそのため，ZZ-SS では代理レート関数 \\(M_i\\) は大きく取る必要があり，尤度関数の評価の回数が増える．そのため，アルゴリズムの計算複雑性は上がっていることに注意 (Bierkens et al., 2019, p. 1302 第４節)．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#sec-ZZ-CV",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#sec-ZZ-CV",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "3 制御変数による分散低減",
    "text": "3 制御変数による分散低減\n\\(\\partial_iU(x)\\) が Lipschitz 連続であるとき，\\(E_i^k\\) をある参照点 \\(\\partial_iU(x_*)\\) とそこからの乖離と取ることで \\(n\\to\\infty\\) の極限で分散が抑えられる．\nこうすることで，\\(M_i\\) を１次関数としたまま，より小さく \\(E_i^k\\) にフィットするように取ることができる．\n\n\n\n\n\n\n命題\n\n\n\n任意の \\(i\\in[d]\\) について，ある \\(C_i&gt;0\\) が存在して， \\[\n\\lvert\\partial_iU(x)-\\partial_iU(y)\\rvert\\le C_i\\lvert x-y\\rvert,\\quad(x,y)\\in\\mathbb{R}^{2d},\n\\] が成り立つとする．このとき， \\[\nM_i(t):=a_i+b_it\n\\] \\[\na_i:=(\\theta_i\\partial_iU(x_*))_++C_i\\|x-x_*\\|_p,\\quad b_i:=C_id^{1/p}\n\\] と定めれば， \\[\nm_i^k\\le M_i\n\\] が成り立つ．ただし， \\[\n\\partial_iU(x)=\\frac{1}{n}\\sum_{k=1}^nE^k_i(x),\\tag{1}\n\\] \\[\nE^k_i(x):=\\partial_iU(x_*)+\\partial_iU^k(x)-\\partial_iU^k(x_*),\n\\] \\[\nm^k_i(t):=\\biggr(\\theta E_i^k(x+\\theta t)\\biggl)_+,\n\\] とした．\n\n\nこの仮定は例えば \\(\\partial_iU\\) が有界な導関数を持つならば成り立つ．\\(p(y_k|x)\\) が Gauss 密度であるやさらに裾が重いときは成り立つ．\n次のようにして参照点 \\(x_*\\) を選ぶ事前処理を行うことで，データのサイズに依存しない計算複雑性で事後分布からの正確なサンプリングが可能になる．\n\n\n\n\n\n\npreprocessing for ZZ-CV\n\n\n\n\n\\(x_*:=\\operatorname*{argmin}_{x\\in\\mathbb{R}^d}U(x)\\) を探索する．\n\\(\\partial_iU(x_*),\\partial_iU^k(x_*)\\) を計算する．\n\nこの２つはいずれも \\(O(n)\\) の複雑性で実行できる．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#zz-cv-のスケーリング",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#zz-cv-のスケーリング",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "4 ZZ-CV のスケーリング",
    "text": "4 ZZ-CV のスケーリング\nこのとき，\\(x_*\\) を定める事前処理が，\\(\\widehat{x}\\) を最尤推定量として， \\[\n\\|x_*-\\widehat{x}\\|_p=O(n^{-1/2})\\quad(n\\to\\infty)\n\\] 程度の正確性があれば，事後分布の最尤推定量周りの漸近展開 (Johnson, 1970) を通じて， \\[\n\\|x-x_*\\|_p=O_p(n^{-1/2})\\quad(n\\to\\infty)\n\\] \\[\n\\partial_iU(x_*)=O_p(n^{1/2})\\quad(n\\to\\infty)\n\\] が成り立つ．\n\n\n\n\n\n\nZig-Zag 過程のスケーリング3\n\n\n\n\n\n事後分布に対する Zig-Zag 過程は，\\(\\sqrt{n}\\) だけ時間を加速したものが \\(\\mathrm{N}_d(0,i(x_0))\\) を標的にする Zig-Zag 過程に収束するから，\\(O(n^{-1/2})\\) のタイムステップで区切ってサンプルとすることができる．\nしかし \\[\n\\max_{k\\in[n]}\\biggr(\\theta_i\\partial_iU^k(x+\\theta t)\\biggl)_+\\le M_i\n\\] を満たす \\(M_i\\) は \\(O(n^{\\alpha})\\;(\\alpha\\ge1/2)\\) のスケールで増大していく．\n各スイッチングイベントにおいて，全データにアクセスする \\(O(n)\\) の計算複雑性が必要であるから，総じて \\(O(n^{\\alpha+1/2})\\) の計算複雑性となる．\n\n\n\nZZ-CV が平衡に至っている場合は \\(x\\) はほとんど \\(x_*\\) に集積するため， \\[\n\\lvert E^k_i(x)\\rvert=\\biggl|\\partial_iU(x_*)+\\partial_iU^k(x)-\\partial_iU^k(x_*)\\biggr|=O(n^{1/2})\n\\] が成り立つ．よってこれを抑える \\(M_i\\) も \\(O(n^{1/2})\\) で済み，必要以上に大きい代理レート関数を用意して剪定する必要がない．\n全データにアクセスする \\(O(n)\\) のステップもないために，事前処理 3 と十分平衡に至っているとみなせるまでの burn-in を除いて，\\(O(1)\\) の計算複雑性でサンプリングが可能である．このことを (Bierkens et al., 2019) は super-efficiency と呼ぶ．\n\n\n\n\n\n\n更なるスケーラブル手法の可能性（事後分布が集中する場合のみ？）\n\n\n\n\n\n他に，事後分布の集中領域でうまくスイッチング回数が抑えられる \\(\\lambda_i\\) が構成できたならば，低い計算複雑性を達成できるだろう．\nZZ-CV では，これに事後分布の Gauss 近似を用いたことになる．\nまた，\\(U\\) の２階微分が有界でない場合，この枠組みが使えない．実際，(Bierkens et al., 2019, pp. 1315 第6.5節) ではこの場合での数値実験の結果が示されており，事後分布が集積しないために super-efficiency は得られていない．\n参照点 \\(x_*\\) を複数取る拡張なども (Bierkens et al., 2019, p. 第7節) で考えられている．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#数値実験mse-の比較",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#数値実験mse-の比較",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "5 数値実験：MSE の比較",
    "text": "5 数値実験：MSE の比較\nある Gauss 分布に従うデータを生成する： \\[\nY^j\\overset{\\text{iid}}{\\sim}\\mathrm{N}(x_0,\\sigma^2),\\qquad j\\in[n],\n\\] 分散 \\(\\sigma^2\\) は既知として，位置母数 \\(x\\in\\mathbb{R}\\) を推定する問題を考える．\n事前分布を \\(\\mathrm{N}(0,\\rho^2)\\) とすると，定数の違いを除いて \\[\\begin{align*}\n    U(x)&=\\frac{x^2}{2\\rho^2}+\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x-y^j)^2\\\\\n    &=\\frac{1}{n}\\sum_{j=1}^n\\left(\\frac{x^2}{2\\rho^2}+\\frac{n}{2\\sigma^2}(x-y^j)^2\\right)=:\\frac{1}{n}\\sum_{j=1}^nU^j(x)\n\\end{align*}\\] であるから， \\[\\begin{align*}\n    U'(x)&=\\frac{x}{\\rho^2}+\\frac{1}{\\sigma^2}\\sum_{j=1}^n(x-y^j)\\\\\n    &=\\frac{x}{\\rho^2}+\\frac{n}{\\sigma^2}(x-\\overline{y}),\n\\end{align*}\\] \\[\nU''(x)=\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}.\n\\]\n従って，Zig-Zag 過程のイベントの強度関数は \\[\\begin{align*}\n    m(t)&=\\biggr(\\theta U'(x+\\theta t)\\biggl)_+\\\\\n    &=\\left(\\frac{\\theta(x+\\theta t)}{\\rho^2}+\\frac{\\theta}{\\sigma^2}\\sum_{j=1}^n(x+\\theta t-y^j)\\right)_+\\\\\n    &=\\left(\\frac{\\theta x}{\\rho^2}+\\frac{\\theta}{\\sigma^2}\\sum_{j=1}^n(x-y^j)+t\\left(\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}\\right)\\right)_+\n\\end{align*}\\] と表せ，これは１次関数 \\((a+bt)_+\\) の形であるから直接のシミュレーションが可能である．4\n\n\nサブサンプリングなしの Zig-Zag 過程のシミュレーションをする関数 ZZ() を定義\nusing ZigZagBoomerang\nusing Distributions\nusing Random\n\nλ(∇U, x, θ, F::ZigZag1d) = pos(θ*∇U(x)) # rate function on E\nλ_bar(τ, a, b) = pos(a + b*τ)  # affine proxy\n\n\"\"\"\n`x`: current location, `θ`: current velocity, `t`: current time,\n\"\"\"\nfunction move_forward(τ, t, x, θ, ::ZigZag1d)\n    τ + t, x + θ*τ , θ\nend\n\n\"\"\"\n    `∇U`: gradient of the negative log-density\n    `(x,θ)`: initial state\n    `T`: Time Horizon    \n    `a+bt`: computational bound for intensity m(t)\n\n    `num`: ポアソン時刻に到着した回数\n    `acc`: 受容回数．`acc/num` は acceptance rate\n\"\"\"\nfunction ZZ(∇U, x::Float64, θ::Float64, T::Float64, y, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_ZZ)\n    t = zero(T)\n    Ξ = [(t, x, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(x, θ, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, x, θ = move_forward(τ, t, x, θ, Flow)\n        l, lb = λ(∇U, x, θ, Flow), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, x, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(x, θ, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n今回の設定に応じたレート関数 (a+bt)+ を用意\npos(x) = max(zero(x), x)  # positive part\na(x, θ, ρ, σ, y) = θ * x / ρ^2 + (θ/σ^2) * sum(x .- y)\nb(x, θ, ρ, σ, y) = ρ^(-2) + length(y)/σ^2\n\nρ, σ, x0, θ0 = 1.0, 1.0, 1.0, 1.0\nn1, n2 = 100, 10^4\nTrueDistribution = Normal(x0, σ)\ny1 = rand(TrueDistribution, n1)\ny2 = rand(TrueDistribution, n2)\n\n# computational bounds for intensity m(t)\nab_ZZ_n1(x, θ, ::ZigZag1d) = (a(x, θ, ρ, σ, y1), b(x, θ, ρ, σ, y1))\nab_ZZ_n2(x, θ, ::ZigZag1d) = (a(x, θ, ρ, σ, y2), b(x, θ, ρ, σ, y2))\n\n∇U1(x) = x/ρ^2 + (length(y1)/σ^2) * (x - mean(y1)) \n∇U2(x) = x/ρ^2 + (length(y2)/σ^2) * (x - mean(y2)) \n\n# T = 2500.0\n# trace_ZZ1, epochs_ZZ1, acc_ZZ1 = ZZ(∇U1, x0, θ0, T, ZigZag1d(); ab=ab_ZZ_n1)\n# trace_ZZ2, num_ZZ2, acc_ZZ2 = ZZ(∇U2, x0, θ0, T, ZigZag1d(); ab=ab_ZZ_n2)\n# dt = 0.01\n# traj_ZZ1 = discretize(trace_ZZ1, ZigZag1d(), dt)\n# traj_ZZ2 = discretize(trace_ZZ2, ZigZag1d(), dt)\n\n\n\n\nN 回 ZZ() を実行して，その事後平均の MSE を計算する関数 experiment() を定義\nfunction SquaredError(sample::Vector{Float64}, y)\n    True_Posterior_Mean = sum(y) / (length(y) + 1)\n    return (mean(sample) - True_Posterior_Mean)^2\nend\n\n\"\"\"\n    epoch_list: 注目するエポック数のリスト\n    N: 実験回数\n\"\"\"\nfunction experiment(epoch_list, T, dt, N, ∇U, x0, θ0, y, Sampler; ab=ab_ZZ_n1)\n    SE_sum = zero(epoch_list)\n    acc_list = []\n    for _ in 1:N\n        trace_ZZ1, epochs_ZZ1, acc_ZZ1 = Sampler(∇U, x0, θ0, T, y, ZigZag1d(); ab=ab)\n        push!(acc_list, acc_ZZ1)\n        traj_ZZ1 = discretize(trace_ZZ1, ZigZag1d(), dt)\n        SE_list = []\n        for T in epoch_list\n            epoch = findfirst(x -&gt; x &gt; T, epochs_ZZ1) - 1\n            t = findfirst(x -&gt; x &gt; trace_ZZ1[epoch][1], traj_ZZ1.t) - 1\n            SE = SquaredError(traj_ZZ1.x[1:t], y)\n            push!(SE_list, SE)\n        end\n        SE_sum += SE_list\n    end\n    return SE_sum ./ N, mean(acc_list)\nend\n\n\n\n\n実験の実行\nusing Plots\n\nT = 3000.0\nepoch_list = [10.0, 100.0, 1000.0, 10000.0]\ndt = 0.01\nN = 10\n\nMSE_ZZ1, acc = experiment(epoch_list, T, dt, N, ∇U1, x0, θ0, y1, ZZ; ab=ab_ZZ_n1)\np = plot(#epoch_list, MSE_ZZ1,\n    xscale=:log10,\n    yscale=:log10,\n    xlabel=\"epochs\",\n    ylabel=\"MSE\"\n    # ,background_color = \"#F0F1EB\"\n    )\nscatter!(p, epoch_list, MSE_ZZ1,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"#78C2AD\",\n    label=nothing\n    )\n\nusing GLM, DataFrames\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_ZZ1))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(p, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:solid,\n    linewidth=2,\n    color=\"#78C2AD\",\n    label=\"ZZ\"\n    )\n\n# display(p)\n\nprintln(\"Average acceptance rate: $acc\")\n\n\n[ Info: Precompiling GLM [38e38edf-8417-5370-95a0-9cbb8c7f171a]\n\n\nAverage acceptance rate: 1.0\n\n\nより，たしかに剪定なしの正確なシミュレーションができている．\n一方で， \\[\nU^j(x):=\\frac{x^2}{2\\rho^2}+\\frac{n}{2\\sigma^2}(x-y^j)^2,\n\\] \\[\n\\lambda^j(x,\\theta):=\\biggr(\\theta(U^j)'(x)\\biggl)_+\n\\] としてサブサンプリングを取り入れることを考えるが，これを同じ \\((a+bt)_+\\) ではバウンド出来ない：\n\n\nZZ-SS (ZigZag with Subsampling) の定義\nλj(j,x,θ,y) = pos(θ * (x/ρ^2 + length(y)/σ^2 * (x - y[j])))\n\nfunction ZZ_SS(∇U, x::Float64, θ::Float64, T::Float64, y, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_ZZ)\n    t = zero(T)\n    Ξ = [(t, x, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(x, θ, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, x, θ = move_forward(τ, t, x, θ, Flow)\n        j = rand(1:length(y))\n        l, lb = λj(j, x, θ, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            if l &gt; lb + 0.01\n                # println(l-lb)\n                acc += 1  #  overflow を数えるように変更済み！注意！\n            end\n            θ = -θ\n            push!(Ξ, (t, x, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(x, θ, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n実験の実行\nusing LaTeXStrings\n\nMSE_ZZ_SS, acc = experiment(epoch_list, T, dt, N, ∇U1, x0, θ0, y1, ZZ_SS; ab=ab_ZZ_n1)\nprintln(L\"上界 $(a+bt)_+$ を超えてしまう平均的割合: \", \"$acc\")\n\n\n上界 $(a+bt)_+$ を超えてしまう平均的割合: 0.49946960772656174\n\n\nしかし，ZZ-CV アルゴリズムではこのようなことは起こらない．実際，次の等式が成り立つ： \\[\nU'(x_*)+(U^j)'(x)-(U^j)'(x_*)=U'(x),\\qquad x,x_*\\in\\mathbb{R}.\n\\]\nこのモデルにおける MAP 推定量は \\[\n\\widehat{x}:=\\frac{\\overline{y}}{1+\\frac{\\sigma^2}{n\\rho^2}}\n\\] である．\n\n\nZZ-CV (ZigZag with Control Variates) の定義\nx_star = mean(y1) / (1 + σ^2/(length(y1) * ρ^2))\n\nC(ρ, σ, y) = ρ^(-2) + length(y)/σ^2\na(x, θ, ρ, σ, y) = pos(θ*∇U1(x_star)) + C(ρ, σ, y) * abs(x - x_star)\nb(x, θ, ρ, σ, y) = C(ρ, σ, y)\n\n# New Computational Bounds for ZZ-CV\nab_ZZ_CV(x, θ, ::ZigZag1d) = (a(x, θ, ρ, σ, y1), b(x, θ, ρ, σ, y1))\n\nfunction ZZ_CV(∇U, x::Float64, θ::Float64, T::Float64, y, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_ZZ_CV)\n    t = zero(T)\n    Ξ = [(t, x, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(x, θ, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, x, θ = move_forward(τ, t, x, θ, Flow)\n        # j = rand(1:length(y))  # 今回はたまたま要らない\n        l, lb =λ(∇U, x, θ, Flow), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, x, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(x, θ, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n実験の実行\nMSE_ZZ_CV, acc = experiment(epoch_list, T, dt, N, ∇U1, x0, θ0, y1, ZZ_CV; ab=ab_ZZ_CV)\n\nq = scatter(p, epoch_list, MSE_ZZ_CV,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"#E95420\",\n    label=nothing\n    )\n\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_ZZ_CV))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(q, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:dash,\n    linewidth=2,\n    color=\"#E95420\",\n    label=\"ZZ-CV (without amendment)\"\n    )\n\ndisplay(q)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n一見すると ZZ-CV が負けているように見える．しかし，点線で描いているのは，横軸が epoch であることを正しく考慮していない間違ったプロットであるためである．\n(Bierkens et al., 2019, p. 1310) において epoch とは，計算量の１単位分としており，ZZ における１回の到着時刻のシミュレーションは，ZZ-CV の \\(n\\) 回分に当たる．これを考慮に入れてプロットし直すと次の通りになる：\n\n\n実験の実行\nT_SuperEfficient = 300000.0\nepoch_list_SuperEfficient = [1000.0, 10000.0, 100000.0, 1000000.0]\n\n@time MSE_ZZ_CV, acc = experiment(epoch_list_SuperEfficient, T_SuperEfficient, dt, N, ∇U1, x0, θ0, y1, ZZ_CV; ab=ab_ZZ_CV)\n\nscatter!(p, epoch_list, MSE_ZZ_CV,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"#E95420\",\n    label=nothing\n    )\n\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_ZZ_CV))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(p, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:solid,\n    linewidth=2,\n    color=\"#E95420\",\n    label=\"ZZ-CV\"\n    )\n\ndisplay(p)\n\n\n 41.246237 seconds (2.26 G allocations: 42.069 GiB, 6.85% gc time, 0.00% compilation time)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこれは換言すれば横軸が「ズルい」ということでもあるが，同時に \\(n\\to\\infty\\) の極限では，圧倒的に ZZ-CV が効率的になるということでもある．5"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#非一様な部分サンプリング",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#非一様な部分サンプリング",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "6 非一様な部分サンプリング",
    "text": "6 非一様な部分サンプリング\n当然，必ずしも一様な分解 \\[\nU(x)=\\frac{1}{n}\\sum_{j=1}^nU^j(x)\n\\] に基づいた一様なサブサンプリング \\(K\\sim\\mathrm{U}([n])\\) を行う必要はない．\n剪定の手続きを棄却法とみると，重点サンプリングのアイデアを導入することで制御変数に依らない分散低減が狙える (Sen et al., 2020 importance subsampling strategy)．\n特に，比例的高次元極限や，不均衡データに対するロジスティック回帰では，事後分布が十分な集中性を持たないために制御変数の方法 3 が十分な効率改善を示さないが，この重点サブサンプリングによれば効率の改善が見込める．\n詳しくは，次稿参照：\n\n    \n        \n            \n            \n                大規模な不均衡データに対するロジスティック回帰（後編）\n                離散時間 MCMC から連続時間 MCMC へ"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#footnotes",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#footnotes",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの２点が両方肝心である．効率的なサブサンプリング推定量の開発が (Fearnhead et al., 2018) 以来議論の焦点になっている．↩︎\n(Vasdekis, 2021, p. 25) や (Bierkens et al., 2019, pp. 1302 定理4.1) も参照．↩︎\n(Bierkens et al., 2019, pp. 1306–) 第5.1節参照．↩︎\n実装は ZigZagBoomerang パッケージの zigzagboom1d.jl を参考にした．↩︎\nただし，例えば今回も計算時間で言えば長くなっていることに注意．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html",
    "href": "posts/2024/Stat/Bayes1.html",
    "title": "ベイズ統計学と統計物理学",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n本稿では全ての結果に数学的に厳密な証明をつけることを優先し，簡単なモデルを取り上げた．\n一般のモデルと，スピングラス理論との大局観は，次稿を参照："
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#bayes-推定",
    "href": "posts/2024/Stat/Bayes1.html#bayes-推定",
    "title": "ベイズ統計学と統計物理学",
    "section": "1 Bayes 推定",
    "text": "1 Bayes 推定\n符号の誤り訂正，または denoising の文脈で Bayes 推定を考える．\n雑音が加わる通信路から受け取った観測から真の信号を推定するというデノイジング問題は，Bayes 推定が自然に選好される格好の設定である．\n\n1.1 デノイジング問題としての設定\n情報源は無記憶で，確率分布 \\(p(x)dx\\) に従うとし，通信路は，確率核 \\(p(y|x)dy\\) に従うとする．\n送信符号は１つの実数 \\(x^*\\sim p(x)dx\\) であったとし，この単一の入力 \\(x^*\\) を \\(n\\) 回独立に観測する： \\[\ny_1,\\cdots,y_n\\overset{\\text{iid}}{\\sim}p(y|x^*)dy.\n\\]\nこの観測を経たあと，送信符号 \\(x^*\\) はいったい何だったのかを推定することを考えると，極めて自然に Bayes 推定が選択される．\n\n\n\n\n\n\n純粋な解釈を持つ Bayes 推定\n\n\n\n\n\nまず，何の観測もない場合，\\(x^*\\) の確率分布は \\(p(x)dx\\) である（設定上）．\nしかし，すでに観測 \\(y_1,\\cdots,y_n\\) を経ている．よってこの事象の上での \\(x^*\\) の条件付き分布を計算すれば良いことになる： \\[\np(x|y_1,\\cdots,y_n) = \\frac{p(\\boldsymbol{y}|x)p(x)}{p(\\boldsymbol{y})}\n\\] \\[\n= \\frac{\\displaystyle p(y_1|x)\\cdots p(y_n|x)p(x)}{\\displaystyle\\int_\\mathbb{R}p(y_1|x)\\cdots p(y_n|x)p(x)\\,dx}\n\\tag{1}\\]\nこうして，誤り訂正符号の文脈では，Bayes 事前確率・事後確率が純粋に確率（または頻度）としての解釈を持つ．通常の統計的枠組みよりも仮定が多いが，その仮定の形式が Bayes 推定の形式にピッタリなのである．\n信号処理と Bayesian formalism の類似性は (Iba, 1999, p. 3876)，(Zdeborová and Krzakala, 2016, p. 456) でも指摘されている．\nまた，統計力学をベイズ推定の特殊な場合とみなせるという見方は，(Jaynes, 1957) から始まる．1\n\n\n\n\n\n1.2 例：Delta-Gauss 混合\n入力は \\(1/2\\) の確率で \\(x=0\\) だが，もう \\(1/2\\) の確率で標準正規分布 \\(\\mathrm{N}(0,1)\\) に従うとする： \\[\np(x)dx=\\frac{1}{2}\\delta_0(dx)+\\frac{1}{2}\\mathrm{N}(0,1)(dx).\n\\] 通信路は加法的 Gauss 型であるとする： \\[\np(x,y)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-y)^2}{2}\\right).\n\\] では真値を \\(x^*=1\\) として，\\(n=10\\) 回の観測データを生成してみる：\n\nusing Random\n\nRandom.seed!(123)\ndata = 1 .+ randn(10)\nprint(data)\n\n[1.8082879284649667, -0.12207250811417336, -0.10463610232929588, 0.5830073648350667, 1.2875879806238557, 1.2298186980518677, 0.5782313356003073, -0.355590621101197, -0.08821851393628699, 1.7037583257923017]\n\n\n愚直な計算から，\\(p(\\boldsymbol{y})\\) は次の積分から得られる：\n\n\\[\\begin{align*}\n    p(\\boldsymbol{y})&=\\int_\\mathbb{R}p(\\boldsymbol{y}|x)p(x)dx\\\\\n    &=\\int_\\mathbb{R}\\prod_{i=1}^n p(y_i|x)p(x)dx\\\\\n    &=\\frac{1}{2}\\frac{1}{(2\\pi)^{n+1}}\\int_\\mathbb{R}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-x)^2+x^2\\right)dx\\\\\n    &\\qquad+\\frac{1}{2}\\frac{1}{(2\\pi)^n}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-x)^2\\right)dx.\n\\end{align*}\\]\n\nこれを実際に計算した結果は (Krazakala and Zdeborová, 2024, p. 117) で与えられている．\nここでは，邪教のような計算を啓示するのではなく，Julia と Turing (Gu et al., 2022) を通じて計算する方法を紹介する (Storopoli, 2021)．\n\nusing Turing, Distributions\n\nprior = MixtureModel([Normal(0.0, 1.0), Normal(0.0, 0.005)], [0.5, 0.5])\n\n@model function mixed_normal_model(data)\n    μ ~ prior\n    for i in 1:length(data)\n        data[i] ~ Normal(μ, 1)\n    end\nend\n\nmodel = mixed_normal_model(data)\n\n\nchain = sample(model, NUTS(), 100000)\n\n\nusing MCMCChains, Plots, StatsPlots\n\nplot(chain[:μ], seriestype = :density, xlabel = \"x*\", ylabel = \"Density\", title = \"Posterior Distribution\", color = \"#78C2AD\")\n\nvline!([1], color = :pink, linewidth = 2, label = \"x^* = 1\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n注（\\(x^*=0\\) でのアトムについて）\n\n\n\n\n\n\\(x^*=0\\) に原子が存在する．\nここでは密度関数は滑らかに見えるが，実際には \\(x^*=0\\) で不連続である．\nこれは，計算の都合上，\\(\\delta_0\\) を \\(\\mathrm{N}(0,0.005)\\) で代用したためである．\n\n\n\n\n\n1.3 MAP 推定量\n\n\n\n\n\n\n定義 (maximum a posterior estimator)\n\n\n\n\\[\n\\widehat{x}:=\\operatorname*{argmax}_{x\\in\\mathbb{R}}\\;p(x|y_1,\\cdots,y_n)\n\\] で定まる推定量 \\(\\mathbb{R}^n\\to\\mathbb{R}\\) を MAP 推定量 という．\n\n\n例 1.2 などをはじめ，ほとんどの場面では良い推定量を与え，多くの場合の最初のチョイスとして適しているかもしれない．\nしかし，例 1.2 をさらに変形することで，次のような事後分布が得られる状況は容易に想像がつく：\n\n\nCode\nfunction f(x)\n    if -1.8 &lt; x &lt; -1.6\n        return 2.5\n    elseif 0 &lt; x &lt; 1.5\n        return 1\n    else \n        return 0\n    end\nend\n\nplot(f, -2, 2, size=(600, 400), legend=false, color=\"#78C2AD\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\n\n1.4 ベイズ最適推定量\n図 1 の状況でも，たしかに１点のみを選ぶならば MAP 推定量で良いかもしれないが，\\(x\\le0\\) である確率は \\(x\\ge0\\) である確率よりも低いため，この意味では，\\(x\\in[0,3/2]\\) の範囲に推定量が収まっていた方が好ましいかもしれない．\n推定量 \\(\\widehat{x}_n:\\mathbb{R}^n\\to\\mathbb{R}\\) を評価するには，何らかのアプリオリな 損失 の概念が必要である．これを損失関数 \\(L:\\mathbb{R}^2\\to\\mathbb{R}\\) という形で与える．\nすると，損失の期待値が計算可能になり，これを 危険 という： \\[\nR(\\widehat{x}_n):=\\operatorname{E}[L(\\widehat{x}_n(\\boldsymbol{Y}),X)].\n\\]\nこのリスクを最小化する推定量を ベイズ最適推定量，その際のリスクを ベイズリスク という．2\n\n1.4.1 \\(l^2\\)-ベイズ最適推定量\n\n\n\n\n\n\n命題\n\n\n\n\\(L(x,y):=(x-y)^2\\) と定める．このとき，事後平均 がベイズリスクを達成する： \\[\n\\widehat{x}(\\boldsymbol{y}):=\\int_\\mathbb{R}xp(x|\\boldsymbol{y})\\,\ndx.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n図 1 の場合では，事後平均は約 \\(0.1\\) で，かろうじて正になる．\n\n\n1.4.2 \\(l^1\\)-ベイズ最適推定量\n\n\n\n\n\n\n命題\n\n\n\n\\(L(x,y):=\\lvert x-y\\rvert\\) と定める．このとき，事後中央値 がベイズリスクを達成する．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n図 1 の場合では，事後中央値は \\(0.5\\) となる．大変頑健な推定だと言えるだろう．\n\n\n1.4.3 最適決定規則\n今回の誤り訂正符号の文脈の目標は，\\(x^*\\) の復元であることを思い出せば，今回の損失は \\(L(x,y)=\\delta_0(x-y)\\) ととり，復号が成功する確率を最大とする推定量が「最良」と言うべきであろう： \\[\nR(\\widehat{x}_n)=\\operatorname{P}[\\widehat{x}_n(\\boldsymbol{Y})=X]\n\\]\nこれは結局 MAP 推定量 \\[\n\\widehat{x}_n:=\\operatorname*{argmax}_{x\\in\\mathbb{R}}p(x|\\boldsymbol{y})\n\\] で与えられることになる．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#統計物理からの視点",
    "href": "posts/2024/Stat/Bayes1.html#統計物理からの視点",
    "title": "ベイズ統計学と統計物理学",
    "section": "2 統計物理からの視点",
    "text": "2 統計物理からの視点\n真のシグナル \\(x^*\\in\\mathbb{R}\\) を，Bayes 事後平均によって点推定する問題を，統計力学の観点から考察する．\n次節 3 で，スパースな一般次元のベクトル \\(x^*\\in\\mathbb{R}^d\\) を復元する問題に拡張する．\n\n2.1 事後分布をある物理系の平衡分布と見る\nベイズの公式 (1) が与える事後分布 \\(p(x|\\boldsymbol{y})\\) は，次の Boltzmann-Gibbs 分布として理解できる： \\[\np(x|\\boldsymbol{y})=\\frac{e^{\\log p(\\boldsymbol{y}|x)p(x)}}{p(\\boldsymbol{y})}\n\\] \\[\n=\\frac{e^{-H(x,\\boldsymbol{y})}}{Z(\\boldsymbol{y})},\n\\] \\[\nH(x,\\boldsymbol{y}):=-\\log p(\\boldsymbol{y}|x)-\\log p(x),\n\\] \\[\nZ(\\boldsymbol{y}):=p(y).\n\\]\nすなわち，Bayes 事後分布 \\(p(x|\\boldsymbol{y})\\) は，\\(\\mathbb{R}\\times\\mathbb{R}^n\\) を配位空間に持ち，Hamiltonian \\(H(x,\\boldsymbol{y})\\) を持つ正準集団の平衡分布と捉えることができる．\n従って，Bayes 事後平均とは，この系に関する熱平均になる．加えて，MAP 推定量とは，この系に関する基底状態となる．\nしかし，この系の，物理的な意義どころか，統計的な意義も定かではない．\n\n\n2.2 もう一つの見方\n今回，通信路は加法的に Gauss ノイズを加えるものとしたのであった： \\[\np(\\boldsymbol{y}|x)d\\boldsymbol{y}=\\mathrm{N}(x,\\sigma^2)^{\\otimes n}(d\\boldsymbol{y})\n\\] この場合，前節 2.1 でみた方法の他に，事後分布 \\(p(x|\\boldsymbol{y})\\) を次のように理解することもできる： \\[\np(x|\\boldsymbol{y})=\\frac{e^{-H(x,\\boldsymbol{y})}}{Z(\\boldsymbol{y})},\n\\] \\[\nH(x,\\boldsymbol{y}):=\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)-\\log p(x),\n\\] \\[\nZ(\\boldsymbol{y}):=\\left(\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\frac{e^{-\\frac{\\lvert\\boldsymbol{y}\\rvert^2}{2\\sigma^2}}}{p(\\boldsymbol{y})}\\right)^{-1}.\n\\]\nこの Hamiltonian \\(H\\) により定まる系の分配関数 \\(Z(\\boldsymbol{y})\\) は，情報源 \\(p(x)\\) と Gauss 型通信路 \\(p(\\boldsymbol{y}|x)\\) で与えられた観測 \\(\\boldsymbol{y}\\) に関する周辺モデル \\(p(\\boldsymbol{y})\\) と，完全にランダムなホワイトノイズ \\(\\mathrm{N}(0,\\sigma^2)^{\\otimes n}\\) との尤度比，または ベイズ因子 になっている．\n\n\n\n\n\n\n式変形\n\n\n\n\n\n\\[\\begin{align*}\n    p(x|\\boldsymbol{y})&=p(\\boldsymbol{y}|x)\\frac{p(x)}{p(\\boldsymbol{y})}\\\\\n    &=\\frac{p(x)}{p(\\boldsymbol{y})}\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x-y_i)^2}\\\\\n    &=\\frac{p(x)}{p(\\boldsymbol{y})}\\frac{e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^ny_i^2}}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)}\\\\\n    &=\\frac{p(x)e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)}}{Z(\\boldsymbol{y})}\\\\\n    &=\\frac{1}{Z(\\boldsymbol{y})}\\exp\\left(\\log p(x)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)\\right)\n\\end{align*}\\]\n\n\n\nさらに，この系 \\(H\\) における自由エントロピーは，\\(p(\\boldsymbol{y})\\) と \\(\\mathrm{N}(0,\\sigma^2)^{\\otimes n}\\) との間の KL 乖離度となっている： \\[\nF_n:=\\int_{\\mathbb{R}^n}\\log\\frac{p(\\boldsymbol{y})}{q(\\boldsymbol{y})}p(\\boldsymbol{y})\\,d\\boldsymbol{y}=\\operatorname{KL}(p,q).\n\\] ただし，\\(q\\) は \\(\\mathrm{N}(0,\\sigma^2)^{\\otimes n}\\) の密度とした．\nこの系の他の物理量も，自由エネルギーと定数倍違うのみとなっている：\n\n\n\n\n\n\n命題（エントロピーと相互情報量）3\n\n\n\nこの Hamiltonian \\(H\\) を持つ系について，\n\nエントロピー \\(H\\) は次で与えられる：\n\\[\\begin{align*}\n     H(Y)&:=-\\int_{\\mathbb{R}^n}(\\log p(\\boldsymbol{y}))p(\\boldsymbol{y})\\,d\\boldsymbol{y}\\\\\n     &=\\frac{n}{2}\\log(2\\pi\\sigma^2)\\\\\n     &\\qquad+\\frac{1}{2\\sigma^2}\\int_{\\mathbb{R}^n}\\lvert\\boldsymbol{y}\\rvert^2p(\\boldsymbol{y})\\,d\\boldsymbol{y}-F_n.\n\\end{align*}\\]\n相互情報量 \\(I\\) は次で与えられる：\n\\[\\begin{align*}\n     I(X,Y)&:=\\operatorname{KL}(p(x,y),p(x)p(y))\\\\\\\\\n     &=\\frac{n}{2\\sigma^2}\\int_{\\mathbb{R}}x^2p(x)\\,dx-F_n.\n\\end{align*}\\]\n\n特に，いずれも自由エネルギーの定数倍である\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\nの式変形は次のとおり： \\[\\begin{align*}\n    H(Y)&:=-\\int_{\\mathbb{R}^n}(\\log p(\\boldsymbol{y}))p(\\boldsymbol{y})\\,d\\boldsymbol{y}\\\\\n    &=-\\int_{\\mathbb{R}^n}p(\\boldsymbol{y})d\\boldsymbol{y}\\log q(\\boldsymbol{y})-F_n\\\\\n    &=-\\int_{\\mathbb{R}^n}\\biggr(-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{\\lvert\\boldsymbol{y}\\rvert^2}{2\\sigma^2}\\biggl)p(\\boldsymbol{y})d\\boldsymbol{y}-F_n\\\\\n    &=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{n}{2\\sigma^2}\\int_{\\mathbb{R}}y_i^2p(y_i)\\,dy_i-F_n.\n\\end{align*}\\]\n次の関係式を用いる： \\[\nI(X,Y)=H(Y)-H(Y|X)\n\\] \\(H(Y)\\) は 1. から判明しており，\\(H(Y|X)\\) は次のように計算できる：\n\\[\\begin{align*}\nH(Y|X)&=-\\int_{\\mathbb{R}^{n+1}}\\log p(\\boldsymbol{y}|x)p(\\boldsymbol{y}|x)p(x)\\,dxd\\boldsymbol{y}\\\\\n&=-\\int_{\\mathbb{R}^{n+1}}\\log\\left(\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-x)^2\\right)\\right)p(\\boldsymbol{y}|x)p(x)\\,dxd\\boldsymbol{y}\\\\\n&=\\int_{\\mathbb{R}^{n+1}}\\left(\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{\\sum_{i=1}^n(x-y_i)^2}{2\\sigma^2}\\right)p(\\boldsymbol{y}|x)p(x)\\,dxd\\boldsymbol{y}\\\\\n&=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\int_\\mathbb{R}\\left(\\int_{\\mathbb{R}^n}\\sum_{i=1}^n(x-y_i)^2p(\\boldsymbol{y}|x)\\,d\\boldsymbol{y}\\right)\\,p(x)dx\\\\\n&=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{n\\sigma^2}{2\\sigma^2}\\int_\\mathbb{R}p(x)\\,dx\\\\\n&=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{n}{2}.\n\\end{align*}\\]\n\nこれと，\\(y_i\\) には \\(x_i\\) とこれと独立な分散 \\(\\sigma^2\\) のノイズが加わって得られる値であることから，次のように計算できる： \\[\\begin{align*}\n    I(X,Y)&=H(Y)-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{n}{2}\\\\\n    &=-F_n-\\frac{n}{2}+\\frac{n}{2\\sigma^2}\\int_\\mathbb{R}y_i^2p(y_i)\\,dy_i\\\\\n    &=-F_n-\\frac{n}{2}+\\frac{n}{2\\sigma^2}\\left(\\sigma^2+\\int_\\mathbb{R}x^2p(x)\\,dx\\right)\\\\\n    &=-F_n+\\frac{n}{2\\sigma^2}\\int_\\mathbb{R}x^2p(x)\\,dx.\n\\end{align*}\\]\n\n\n\n\n\n2.3 西森対称性\n\n\n\n\n\n\n命題 (Nishimori, 1980)4\n\n\n\n\\(P:\\mathbb{R}\\to\\mathbb{R}^n\\) を確率核，\\(X^*\\in\\mathcal{L}(\\Omega)\\) は分布 \\(\\nu\\in\\mathcal{P}(\\mathbb{R})\\) を持ち，\\(Y\\in\\mathcal{L}(\\Omega;\\mathbb{R}^n)\\) の分布は \\[\n\\mu(dy)=\\int_\\mathbb{R}\\nu(dx)P(x,dy)\n\\] で定まるとする．ここで，\\(Y\\) で条件づけた \\(X^*\\) の確率分布を \\(P^{X|Y}\\) とする： \\[\n\\nu(dx)=\\int_{\\mathbb{R}^n}\\mu(dy)P^{X|Y}(y,dx).\n\\] \\[\nX^{(1)},\\cdots,X^{(k)}\\overset{\\text{iid}}{\\sim}P^{X|Y}\n\\] を独立な確率変数列とすると，次が成り立つ：任意の \\(f\\in\\mathcal{L}_b(\\mathbb{R}^{n+k})\\) について，\n\\[\\begin{align*}\n    &\\quad\\operatorname{E}\\biggl[f(Y,X^{(1)},\\cdots,X^{(k)})\\biggr]\\\\\n    &=\\operatorname{E}\\left[f(Y,X^{(1)},\\cdots,X^{(k-1)},X^*)\\right].\n\\end{align*}\\]\n\n\n\\(X^{(1)},\\cdots,X^{(k)}\\) で表した確率変数の \\(P^{X|Y}\\) に関する積分を Boltzmann 積分または熱平均と呼び，観測を作り出す過程 \\((X^*,Y)\\) に関する積分を無秩序積分 (disorder expectation) または quenched average という．5\n\n\n\n\n\n\n証明\n\n\n\n\n\nこの設定の下で，\\((X^*,Y)\\) の結合分布が次の２通りで表せていることに注意： \\[\n\\nu(dx)P(x,dy)=\\mu(dy)P^{X|Y}(y,dx).\n\\] 従って， \\[\\begin{align*}\n    &\\quad\\nu(dx)P(x,dy)P^{X|Y}(y,dx^{(1)},\\cdots,dx^{(k)})\\\\\n    &=\\mu(dy)P^{X|Y}(y,dx)P^{X|Y}(y,dx^{(1)},\\cdots,dx^{(k)})\n\\end{align*}\\] に関して \\(f(Y,X^{(1)},\\cdots,X^{(k)})\\) の期待値を取ると，次のように計算できる： \\[\\begin{align*}\n    &\\quad\\operatorname{E}\\biggl[f(Y,X^{(1)},\\cdots,X^{(k)})\\biggr]\\\\\n    &=\\int_{\\mathbb{R}^{n+k}}f(y,x^{(1)},\\cdots ,x^{(k-1)},x^{(k)})P^{X|Y}(y,dx^{(1)})\\cdots P^{X|Y}(y,dx^{(k)})\\mu(dy)\\\\\n    &=\\int_{\\mathbb{R}^{n+k}}f(y,x^{(1)},\\cdots,x^{(k-1)},x)P^{X|Y}(y,dx^{(1)})\\cdots P^{X|Y}(y,dx^{(k-1)})\\nu(dx)P(x,dy)\\\\\n    &=\\operatorname{E}\\left[f(Y,X^{(1)},\\cdots,X^{(k-1)},X^*)\\right].\n\\end{align*}\\]\n確率核 \\(P\\) にまつわる記法は次の記事も参照：\n\n    \n        \n            \n            \n                数学記法一覧\n                本サイトで用いる記法と規約\n            \n        \n    \n\n\n\n\n\n\n\n\n\n\n物理的な解釈\n\n\n\n\n\n\\(P^{X|Y}\\) に関する積分を \\(\\langle-\\rangle\\) で表すことで，何をどのように物理的に解釈しているかが明確になる： \\[\n\\langle X\\rangle=\\operatorname{E}[X|Y].\n\\]\nこの見方を採用すると，期待値を \\[\n\\operatorname{E}\\biggl[f(Y,X^{(1)},\\cdots,X^{(k)})\\biggr]=\\operatorname{E}^Y\\left[\\left\\langle f(Y,X^{(1)},\\cdots,X^{(k)})\\right\\rangle\\right]\n\\] と二段階で捉えていることになる．右辺の外側の期待値は単に \\(Y\\) のみに関してとっていることになる．\n第 2.2 節で考えたモデル \\(H\\) における Boltzmann 分布が \\(P^{X|Y}\\) となり，平均 \\(\\langle-\\rangle\\) はこれに関する平均となる．\n一方で，Hamiltonian \\(H\\) にもランダム性が残っているのであり，これに関する平均が \\((X^*,Y)\\) に関する平均に当たる．\nこうして，ベイズ統計モデルはスピングラス系（特にランダムエネルギーモデル (Derrida, 1980)）と同一視できるようになる．\nだが同時に，スピングラスのサンプリングを困難にする多谷構造も，ベイズ統計に輸入されるのである……．\nスピングラスについては，次の記事も参照：\n\n    \n        \n            \n            \n                数学者のための統計力学１\n                Ising 模型とスピングラス\n            \n        \n    \n\n\n\n\n\n\n2.4 最小自乗誤差推定量\n第 1.4.1 節で扱った最小自乗誤差推定量の自乗誤差は次のように計算できる：\n\n\n\n\n\n\n命題（最小自乗誤差の表示）\n\n\n\n\\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) と表す．事後平均推定量の自乗誤差は次のように表せる： \\[\\begin{align*}\n    \\DeclareMathOperator{\\MMSE}{MMSE}\n    \\MMSE&:=\\operatorname{E}\\biggl[\\biggr(X-\\operatorname{E}[X|Y]\\biggl)^2\\biggr]\\\\\n    &=\\operatorname{E}[X^2]-\\operatorname{E}\\biggl[\\langle X\\rangle^2\\biggr].\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    \\operatorname{E}[\\mathrm{V}[X|Y]]&=\\operatorname{E}\\biggl[\\biggr(\\operatorname{E}[X|Y]-X\\biggl)^2\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2-2X\\operatorname{E}[X|Y]+X^2\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2\\biggr]-2\\operatorname{E}\\biggl[X\\operatorname{E}[X|Y]\\biggr]+\\operatorname{E}[X^2]\\\\\n    &=\\operatorname{E}[X^2]-\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2\\biggr].\n\\end{align*}\\]\nこの変形では，繰り返し期待値の法則 \\[\\begin{align*}\n    \\operatorname{E}\\biggl[X\\operatorname{E}[X|Y]\\biggr]&=\\operatorname{E}\\biggl[\\biggl[X\\operatorname{E}[X|Y]\\,\\bigg|\\,Y\\biggr]\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2\\biggr]\n\\end{align*}\\] を西森の対称性の代わりに用いたことになる．\n換言すれば，西森の対称性を証明したのと同様の方法を本命題にも適用したため，直接命題の適用は回避したことになる．\n\n\n\n\n\n2.5 KL 乖離度の微分としての分散\n次の定理は，相互情報量 \\(I\\) と MMSE を結びつける定理であるため，(Guo et al., 2005) 以来，I-MMSE 定理 と呼ばれている．\n\n\n\n\n\n\n命題（自由エネルギーによる特徴付け） (Guo et al., 2005)6\n\n\n\n簡単のため，\\(n=1\\) とする．このとき，\\(\\beta:=\\sigma^{-2}\\) に関して，次の式が成り立つ：\n\n自由エネルギー \\(F_1\\) について， \\[\n\\frac{\\partial F_1}{\\partial \\beta}=\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}.\n\\]\n相互情報量 \\(I(X,Y)\\) について，\n\\[\\begin{align*}\n    \\frac{\\partial I(X,Y)}{\\partial \\beta}&=\\frac{\\operatorname{E}[X^2]-\\operatorname{E}[\\langle X\\rangle^2]}{2}\\\\\n    &=\\frac{\\MMSE}{2}\n\\end{align*}\\]\n\nただし，\\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) と定めた．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    \\log\\frac{p(y)}{q(y)}&=\\frac{\\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb{R}e^{-\\frac{(x-y)^2}{2\\sigma^2}}p(x)\\,dx}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{y^2}{2\\sigma^2}}}\\\\\n    &=\\int_\\mathbb{R}e^{-\\frac{x^2-2xy}{2\\sigma^2}}p(x)\\,dx.\n\\end{align*}\\]\nこの \\(p(y)dy\\) に関する積分を \\(\\beta\\) で微分すれば良いのであるが，\\(p(y)\\) 自体が \\(\\sigma\\) に依存し，そのまま計算したのでは大変煩雑になる．\nそこで，\\(y=\\sigma z+x^*\\) と変数変換をすることで，標準 Gauss 確率変数 \\(Z\\sim\\gamma_1\\) と真値 \\(X^*\\) は独立で，\\(\\sigma\\) も含まないから，次のように計算ができる： \\[\\begin{align*}\n    F_1(\\beta)&=\\int_\\mathbb{R}\\log\\frac{p(y)}{q(y)}p(y)\\,dy\\\\\n    &=\\operatorname{E}\\biggl[\\log\\left(\\int_\\mathbb{R}e^{-\\frac{x^2-2xY}{2\\sigma^2}}p(x)\\,dx\\right)\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\log\\left(\\int_\\mathbb{R}\\exp\\left(-\\frac{x^2}{2\\sigma^2}+\\frac{xZ}{\\sigma}+\\frac{xX^*}{\\sigma^2}\\right)p(x)\\,dx\\right)\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\log\\left(\\int_\\mathbb{R}\\exp\\left(-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta\\right)p(x)\\,dx\\right)\\biggr].\n\\end{align*}\\]\nこうして，最右辺の期待値に関する密度はもう \\(\\beta\\) に依存しないから，次のように微分が計算できる．ただし， \\[\n\\exp\\left(-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta\\right)p(x)\\,dx\n\\] は \\(p(x|y)\\) の定数倍，すなわち Gibbs 測度 \\(\\frac{e^{-H(x,y)}}{Z(y)}dx\\) の定数倍であることに注意して，これらに関する平均を引き続き \\(\\langle-\\rangle:=\\operatorname{E}[-|Y]\\) で表すこととすると，\n\\[\\begin{align*}\n    \\frac{\\partial F_1(\\beta)}{\\partial \\beta}&=\n    \\operatorname{E}\\left[\\frac{\\displaystyle\\int_\\mathbb{R}\\biggr(-\\frac{x^2}{2}+\\frac{xZ}{2\\sqrt{\\beta}}+xX^*\\biggl)e^{-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta}p(x)dx}{\\displaystyle\\int_\\mathbb{R}\\exp\\left(-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta\\right)p(x)\\,dx}\\right]\\\\\n    &=-\\frac{\\operatorname{E}[\\langle X^2\\rangle]}{2}+\\frac{1}{2\\sqrt{\\beta}}\\operatorname{E}[\\langle X\\rangle Z]+\\operatorname{E}[\\langle X\\rangle X^*].\n\\end{align*}\\]\n\\(\\langle X\\rangle\\) が中に入っているのは，\\(\\sigma[X^*,Z]\\) に関する条件付き期待値を \\(\\operatorname{E}\\) 内で取ったためと捉えられる．\nまず，第二項は，Stein の補題 2.6 の系から， \\[\\begin{align*}\n    \\frac{1}{2\\sqrt{\\beta}}\\operatorname{E}[\\langle X\\rangle Z]&=\\frac{1}{2\\sqrt{\\beta}}\\sqrt{\\beta}\\biggr(\\operatorname{E}[\\langle X^2\\rangle]-\\operatorname{E}[\\langle X\\rangle^2]\\biggl)\\\\\n    &=\\frac{\\operatorname{E}[\\langle X^2\\rangle]}{2}-\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}\n\\end{align*}\\]\n第三項は，西森の対称性から， \\[\\begin{align*}\n    \\operatorname{E}[\\langle X\\rangle X^*]=\\operatorname{E}[\\langle X\\rangle^2].\n\\end{align*}\\]\n以上を併せると， \\[\n\\frac{\\partial F_1(\\beta)}{\\partial \\beta}=\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}.\n\\]\n\nの主張も，\\(n=1\\) のときは \\[\nI(X,Y)=\\frac{\\beta}{2}\\int_\\mathbb{R}x^2p(x)\\,dx-F_1\n\\] であったために従う： \\[\\begin{align*}\n\\frac{\\partial I(X,Y)}{\\partial \\beta}&=\\frac{\\operatorname{E}[X^2]}{2}-\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}\\\\\n&=\\frac{\\MMSE}{2}.\n\\end{align*}\\]\n\n\n\n\n\\(\\operatorname{E}[\\langle X\\rangle^2]\\) は \\(q\\) とも表され，スピングラスの 秩序パラメータ ともいう．7\n\\(I\\) の \\(\\beta\\) に関する二回微分を計算することより，\\(\\beta\\) に関する凸関数であることもわかる．\n\n\n2.6 Stein の補題\n\n\n\n\n\n\n命題 (Stein, 1972)\n\n\n\n可積分な確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\) について，次は同値：\n\n\\(X\\) の分布は標準Gaussである：\\(X\\sim\\gamma_1\\)．\n任意の \\(f\\in C^1_b(\\mathbb{R})\\) について， \\[\n\\operatorname{E}[f'(X)]=\\operatorname{E}[Xf(X)]&lt;\\infty.\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n(1)\\(\\Rightarrow\\)(2)\n\\(f,f'\\) はいずれも有界としたから， \\[\n  \\operatorname{E}[f'(X)],\\operatorname{E}[f(X)]&lt;\\infty\n  \\] が成り立つ．\n\\(\\gamma_1\\) の密度 \\(\\phi\\) は \\(\\phi'(x)=-x\\phi(x)\\) を満たすことに注意すれば，部分積分により， \\[\\begin{align*}\n      \\operatorname{E}[f'(X)]&=-\\int_\\mathbb{R}f(x)\\phi'(x)dx\\\\\n      &=\\int_\\mathbb{R}f(x)x\\phi(x)dx=\\operatorname{E}[f(X)X].\n  \\end{align*}\\]\n(2)\\(\\Rightarrow\\)(1)\n\\(X\\) は可積分としたから，特性関数 \\(\\varphi(u):=\\operatorname{E}[e^{iuX}]\\) は少なくとも \\(C^1(\\mathbb{R})\\)-級で，その微分は \\(\\phi(x):=e^{iux}\\) に関する仮定 \\(\\operatorname{E}[\\phi'(X)]=\\operatorname{E}[X\\phi(X)]\\) を通じて \\[\\begin{align*}\n      \\varphi'(u)&=i\\operatorname{E}[Xe^{iuX}]=-u\\operatorname{E}[e^{iuX}]\\\\\n      &=-u\\varphi(u),\\qquad u\\in\\mathbb{R},\n  \\end{align*}\\] と計算できる．\nこの微分方程式は規格化条件 \\(\\varphi(0)=1\\) の下で一意な解 \\(\\varphi(u)=e^{-\\frac{u^2}{2}}\\) を持つ．\n\n\n\n\n\n\n\n\n\n\n系\n\n\n\n\\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) とこれと独立な \\(Z\\sim\\gamma_1\\) に関して，次が成り立つ：\n\\[\\begin{align*}\n    \\operatorname{E}[\\langle X\\rangle Z]&=\\operatorname{E}\\left[\\frac{\\partial \\langle X\\rangle}{\\partial Z}\\right]\\\\\n    &=\\sqrt{\\beta}\\biggr(\\operatorname{E}[\\langle X^2\\rangle]-\\operatorname{E}[\\langle X\\rangle^2]\\biggl)\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n最初の等号 \\[\n\\operatorname{E}[\\langle X\\rangle Z]=\\operatorname{E}\\left[\\frac{\\partial \\langle X\\rangle}{\\partial Z}\\right]\n\\] は Stein の補題による．\n続いて， \\[\n\\langle X\\rangle=\\int_\\mathbb{R}xp(x|Y)\\,dx\n\\] は \\(Y=\\sigma Z+X^*\\) を通じてのみ \\(Z\\) に依存するから， \\[\n\\frac{\\partial \\langle X\\rangle}{\\partial Z}=\\frac{\\partial \\langle X\\rangle}{\\partial Y}\\frac{d Y}{d Z}=\\sigma\\frac{\\partial \\langle X\\rangle}{\\partial Y}\n\\tag{2}\\] が成り立つ．あとは \\(\\langle X\\rangle=\\operatorname{E}[X|Y]\\) の \\(Y\\) に関する微分を計算すれば良い．\n\\[\n\\frac{\\partial p(y|x)}{\\partial y}=-\\frac{y-x}{\\sigma^2}p(y|x).\n\\] また， \\[\np(y)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb{R}e^{-\\frac{(x-y)^2}{2\\sigma^2}}p(x)\\,dx\n\\] であったから， \\[\\begin{align*}\n    p'(y)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb{R}-\\frac{y-x}{\\sigma^2}e^{-\\frac{(x-y)^2}{2\\sigma^2}}p(x)\\,dx\\\\\n    &=-\\int_\\mathbb{R}\\frac{y-x}{\\sigma^2}p(y|x)p(x)\\,dx\\\\\n    &=-\\int_\\mathbb{R}\\frac{y-x}{\\sigma^2}p(x|y)p(y)\\,dx\\\\\n    &=-\\frac{y-\\langle X\\rangle}{\\sigma^2}p(y).\n\\end{align*}\\] \\[\n\\therefore\\quad\\frac{p'(Y)}{p(Y)}=\\frac{\\langle X\\rangle-Y}{\\sigma^2}\n\\]\nに注意すれば，次のように計算できる：\n\\[\\begin{align*}\n    \\frac{\\partial \\langle X\\rangle}{\\partial Y}&=\\frac{\\partial }{\\partial Y}\\int_\\mathbb{R}xp(x|Y)\\,dx\\\\\n    &=\\frac{\\partial }{\\partial Y}\\int_\\mathbb{R}x\\frac{p(Y|x)p(x)}{p(Y)}\\,dx\\\\\n    &=\\int_\\mathbb{R}x\\frac{\\partial }{\\partial Y}\\frac{p(Y|x)p(x)}{p(Y)}\\,dx\\\\\n    &=\\int_\\mathbb{R}x\\frac{\\partial p(Y|x)}{\\partial Y}\\frac{p(x)}{p(Y)}\\,dx\\\\\n    &\\qquad+\\int_\\mathbb{R}x\\left(\\frac{\\partial }{\\partial Y}\\frac{1}{p(Y)}\\right)p(Y|x)p(x)\\,dx\\\\\n    &=-\\int_\\mathbb{R}x\\frac{Y-x}{\\sigma^2}p(x|Y)\\,dx\\\\\n    &\\qquad-\\frac{p'(Y)}{p(Y)}\\int_\\mathbb{R}xp(x|Y)\\,dx\\\\\n    &=-\\frac{Y}{\\sigma^2}\\langle X\\rangle+\\frac{\\langle X^2\\rangle}{\\sigma^2}\\\\\n    &\\qquad-\\langle X\\rangle\\frac{\\langle X\\rangle-Y}{\\sigma^2}\\\\\n    &=\\frac{1}{\\sigma^2}\\biggr(\\langle X^2\\rangle-\\langle X\\rangle^2\\biggl).\n\\end{align*}\\]\n最後，式 (\\(\\ref{eq-chain-rule}\\)) より， \\[\\begin{align*}\n    \\frac{\\partial \\langle X\\rangle}{\\partial Z}&=\\sigma\\frac{\\partial \\langle X\\rangle}{\\partial Y}\\\\\n    &=\\sqrt{\\beta}\\biggr(\\langle X^2\\rangle-\\langle X\\rangle^2\\biggl).\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#sec-sparse-vector-denoising",
    "href": "posts/2024/Stat/Bayes1.html#sec-sparse-vector-denoising",
    "title": "ベイズ統計学と統計物理学",
    "section": "3 スパースベクトルの復号",
    "text": "3 スパースベクトルの復号\nスパースベクトルの信号推定問題を考える．ここまで，\\(x,x^*\\) は \\(\\mathbb{R}\\) の点としてきたが，本節では，\\(\\mathbb{R}^d,d=2^N\\) の one-hot ベクトルであるとする．\n\n3.1 設定\n真の信号 \\(x^*\\in\\mathbb{R}^d\\) は，\\(d\\) 次元の one-hot ベクトルであるとする．すなわち，次の集合 \\(\\Delta_d\\) の元であるとする： \\[\n\\Delta_d:=\\left\\{x\\in\\mathbb{Z}^d\\mid\\|x\\|_1=1\\right\\}.\n\\]\n加えて，\\(\\Delta_d\\) 上の一様分布に従うとする：\\(x^*\\sim\\mathrm{U}(\\Delta_d)\\)．\nこれを分散 \\(\\frac{\\sigma^2}{N}\\) を持った Gauss ノイズを通じて観測する： \\[\nY\\sim\\mathrm{N}_d\\left(x^*,\\frac{\\sigma^2}{N}I_d\\right).\n\\]\n事前分布を \\(p(x)\\)，\\(Y\\) の分布を \\(p(y)\\) とすると，Bayes の定理より， \\[\\begin{align*}\n    p(x|y)&=\\frac{p(x)}{p(y)}\\prod_{i=1}^d\\frac{e^{-\\frac{(y_i-x_i)^2}{2\\sigma^2/N}}}{\\sqrt{2\\pi\\sigma^2/N}}\\\\\n    &=\\frac{\\prod_{i=1}^d\\phi(y_i;0,\\sigma^2/N)}{p(y)}\\\\\n    &\\qquad\\times \\frac{1}{2^N}\\prod_{i=1}^d\\exp\\left(-\\frac{x_i^2-2x_iy_i}{2\\sigma^2/N}\\right)\n\\end{align*}\\]\nただし，\\(\\phi(-;\\mu,\\sigma):=\\frac{d \\mathrm{N}_1(\\mu,\\sigma)}{d \\ell_1}\\) を正規密度とした．\n\n\n3.2 スピングラス系との同一視\nこの事後分布 \\(p(x|y)\\) は，次の Hamiltonian \\(H\\) に関する，逆温度 \\(\\beta=\\sigma^{-2}\\) での Boltzmann 分布とみなせる：\n\\[\\begin{align*}\n    p(x|y)&=\\frac{1}{\\mathcal{Z}}e^{-\\beta H(x,y)}\\\\\n    H(x,y)&:=-N\\sigma^2\\log2\\\\\n    &\\qquad-\\frac{N}{2}\\sum_{i,j=1}^dx_i^*x_j^*\\sqrt{(1-2y_i)(1-2y_j)}\n\\end{align*}\\]\n\n\n\n\n\n\n注\n\n\n\n\n\nこの Hamiltonian は非物理的なものであり，planted ensemble とも呼ばれる．(Murphy, 2023, p. 843) での clamped という表現と同じニュアンスである．詳しくは次項参照：\n\n    \n        \n            \n            \n                ベイズ統計学とスピングラス\n                誤り訂正符号を題材にして\n            \n        \n    \n\nまた，\\(H\\) の表示については，\\(x^*\\in\\Delta_d\\) であるから，\\(i,j\\in[d]\\) と２つの和をとっているように見えるが，二つの添え字が一致している場合しか非零な値は取らず，結局 \\[\nH(x,y)=-\\frac{N}{2}\\sum_{j=1}^d(2y_j-1)x_j+N\\sigma^2\\log2\n\\] であることに注意．\n\n\n\nなお，分配関数は \\[\n\\mathcal{Z}:=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(\\frac{N}{2\\sigma^2}(2y_i-1)\\right),\n\\] と表される．\n\n\n3.3 自由エネルギー密度の計算\n系が用意されたら，統計力学はまず，代表的な物理量の熱力学極限を計算する．特に自由エネルギー密度は，熱力学極限 \\(N\\to\\infty\\) において自己平均性（確率論では集中性という性質）を示すことが期待される．\n自由エネルギー密度 \\(\\Phi\\) は，熱力学極限を通じて \\[\n\\Phi(\\beta):=\\lim_{N\\to\\infty}\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\n\\] と定まる．\nI-MMSE 定理（第 2.5 節）はこの多次元の \\(X\\) に関しても有効であり， \\[\n\\Phi_N(\\beta):=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\n\\] に関して， \\[\n\\frac{\\partial \\Phi_N(\\beta)}{\\partial \\beta}=\\frac{\\operatorname{E}\\biggl[\\lvert\\langle X\\rangle\\rvert^2\\biggr]}{2}\n\\] が成り立つ．\n\n\n\n\n\n\n証明\n\n\n\n\n\nまず有限の \\(N\\in\\mathbb{N}^+\\) で計算する．\n\\[\n\\frac{\\partial \\Phi_N(\\beta)}{\\partial \\beta}=\\frac{1}{N}\\operatorname{E}\\left[\\frac{1}{\\mathcal{Z}}\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}\\right]\n\\] の右辺を求めれば良い．\n\\[\nY_i=X_i^*+\\frac{\\sigma}{\\sqrt{N}}Z_i\n\\] であり，one-hot ベクトル \\(x\\in\\Delta_d\\) については \\[\nx_j=x_j^2,\\quad\\lvert x\\rvert^2=1\n\\] でもあることに注意すれば，\n\\[\\begin{align*}\n    \\mathcal{Z}&=\\sum_{i=1}^de^{-H(x^{(i)},Y)}\\\\\n    &=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(-\\frac{N\\beta}{2}\\lvert x^{(i)}\\rvert^2+N\\beta(Y|x^{(i)})\\right)\\\\\n    &=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(-\\frac{N\\beta}{2}\\lvert x^{(i)}\\rvert^2+N(X^*|x^{(i)})\\beta+(Z|x^{(i)})\\sqrt{N\\beta}\\right)\n\\end{align*}\\] \\[\n\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}=\\sum_{i}^d\\biggr(-\\frac{N}{2}\\lvert x^{(i)}\\rvert^2+N(X^*|x^{(i)})+\\frac{\\sqrt{N}}{2\\sqrt{\\beta}}(Z|x^{(i)})\\biggl)e^{-H(x^{(i)},Y)}\n\\] \\[\n\\frac{1}{\\mathcal{Z}}\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}=-\\frac{N}{2}\\left\\langle\\lvert X\\rvert^2\\right\\rangle+N\\biggl\\langle(X^*|X)\\biggr\\rangle+\\frac{\\sqrt{N}}{2\\sqrt{\\beta}}\\biggl\\langle(Z|X)\\biggr\\rangle\n\\] \\[\n\\frac{1}{N}\\operatorname{E}\\left[\\frac{1}{\\mathcal{Z}}\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}\\right]=\\operatorname{E}\\left[\\left\\langle-\\frac{\\langle\\lvert X\\rvert^2\\rangle}{2}+\\biggl\\langle(X^*|X)\\biggr\\rangle+\\frac{\\biggl\\langle(Z|X)\\biggr\\rangle}{2\\sqrt{N\\beta}}\\right\\rangle\\right]\n\\] これは１次元の場合の計算（第 2.5 節）と同様に， \\[\n\\frac{\\partial \\Phi_N(\\beta)}{\\partial \\beta}=\\frac{\\operatorname{E}[\\lvert\\langle X\\rangle\\rvert^2]}{2}\n\\] と計算できる．\n\n\n\nよって，\\(N\\to\\infty\\) の極限でも右辺が定数に収束し（右辺は \\(2^N\\) 項和を含む），この種の関係式が成り立ち続けるならば，\\(\\Phi\\) は \\(\\beta\\) の一次関数の形であるはずである．実際，次が示せる：\n\n\n\n\n\n\n命題8\n\n\n\n\\(\\Delta:=\\sigma^2\\) の関数として，自由エネルギー密度 \\(\\Phi\\) は次で定まる関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}_+\\) に一致する： \\[\nf(\\Delta):=\n\\begin{cases}\n\\frac{1}{2\\Delta}-\\log 2&\\Delta\\le\\Delta_c,\\\\\n0&\\Delta\\ge\\Delta_c.\n\\end{cases}\n\\] \\[\n\\Delta_c:=\\frac{1}{2\\log 2}\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n次節 3.4 では \\(\\Phi_N\\) を \\(f\\) によって上下から評価することで，数学的に厳密な証明を与える．ここでは，本ベイズモデルをランダムエネルギーモデルとみなし，レプリカ法によって計算した場合の証明の概略を付す．\n\\(n\\) 個のレプリカを複製した際の，それぞれの配置 \\((i_1,\\cdots,i_n)\\in[d]^n\\) について足し合わせることで，\\(Z^n\\) の disorder average を取る．\n最終的に次の表示を得る： \\[\ne^{nN\\left(\\log 2+\\frac{\\beta}{2}\\right)}\\approx\\int dQdM\\,e^{Ns(Q,M)+N\\beta\\left(\\sum_{a=1}^nM_a+\\frac{1}{2}\\sum_{a,b=1}^nQ_{a,b}\\right)}\n\\] \\[\n=:\\int dQdM\\,e^{Ng(\\Delta,Q,M)}.\n\\tag{3}\\] ただし，\\(M_a:=\\delta_{i_a,1}\\in2\\) は磁化のベクトル，\\(Q_{ab}:=\\delta_{i_a,i_b}\\) は overlap matrix と呼ばれる．\n積分はこの \\(M_a,Q_{ab}\\) の全体について実行され，同じ \\(M_a,Q_{ab}\\) の値を取る配置の数を \\[\n\\#(Q,M)=:e^{Ns(Q,M)}\n\\] と表した．\n\\(N\\to\\infty\\) の極限では，式 (3) に対して Laplace 近似を実行すれば良い．\n従って，\\(Q,M\\) の構造のうち，特に支配的なものの特定に成功すれば，解が求まることになる．\n最初の仮定として，レプリカ \\(i_1,\\cdots,i_d\\) は交換可能で見分けがつかないはずだろう，という replica symmetric ansatz が考えられる．\nこのレプリカ対称性を仮定すると，次の３通りまでシナリオが絞られる：\n\n任意のレプリカは同一の状態にある：\\(i_a\\equiv i\\in[d]\\)．だが，正しいレプリカではない \\(i\\ne1\\)．\nこのとき，\\(Q_{ab}\\equiv 1,M_a\\equiv0\\) となり，\\(s(Q,M)=\\log2\\)，かつ \\(g(\\beta,Q)=\\log2+n^2/\\Delta\\)．\nこれは \\(N\\to\\infty\\) の極限で \\(n\\) に関して線型ではなく，物理的な解が得られるとは思えず，レプリカ法も解析接続に失敗する．\n任意のレプリカは全て正しい状態にある：\\(i_a\\equiv1\\)．\n\\(Q_{ab}=M_a\\equiv1\\) となり，\\(s(Q,M)=0,g(\\beta,Q)=n/\\Delta+n^2/2\\Delta\\)．\nレプリカ内に少なくとも２つの違う状態がある：\nこのとき，\\(g(\\beta,Q)=n/2\\Delta+n\\log2\\)．\n\n２と３の場合から，次の２つが解の候補として回収できた： \\[\n\\operatorname{E}[Z^n]=e^{-nN\\left(\\log 2-\\frac{\\beta}{2}\\right)},\n\\] \\[\n\\operatorname{E}[Z^n]=0.\n\\]\nこの２つは，次節 3.4 から導かれる厳密な結果に一致する．\n最後，自由エネルギーの \\(\\Delta\\) に関する凸性と解析性から，結論を得る．\n\n\n\n\n\n3.4 自由エネルギーの上下評価\nレプリカ法を回避し，数学的に厳密な証明を与えるには，次のように上下から評価することになる．\n\n\n\n\n\n\n補題（下界）9\n\n\n\n\\[\n\\Phi_N(\\Delta)\\ge f(\\Delta)\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n一般に下からの評価は，\\(Z_i\\) に関する項をまとめて無視して良いために，簡単である．\n\\[\nY_i=X_i^*+\\frac{\\sigma}{\\sqrt{N}}Z_i,\n\\] \\[\nZ_i\\overset{\\text{iid}}{\\sim}\\mathrm{N}_1(0,1),\n\\]\nであることから，\\(i^*\\) を \\(X^*_{i^*}=1\\) を満たす添字 \\(i^*\\in[d]\\) とすると，\n\\[\\begin{align*}\n    \\mathcal{Z}&=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(\\frac{NY_i}{\\sigma^2}-\\frac{N}{2\\sigma^2}\\right)\\\\\n    &=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(\\frac{NX^*_i}{\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_i-\\frac{N}{2\\sigma^2}\\right)\\\\\n    &\\ge\\frac{1}{2^N}\\exp\\left(\\frac{N}{\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_{i^*}-\\frac{N}{2\\sigma^2}\\right)\\\\\n    &=\\frac{1}{2^N}\\exp\\left(\\frac{N}{2\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_{i^*}\\right)\n\\end{align*}\\]\nと評価できる．\n\\[\n\\log\\mathcal{Z}\\ge\\frac{N}{2\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_{i^*}-N\\log 2.\n\\] \\[\\begin{align*}\n    \\Phi_N(\\Delta)&=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\\\\\n    &\\ge\\frac{1}{N}\\left(\\frac{N}{2\\sigma^2}-N\\log 2\\right)\\\\\n    &=\\frac{1}{2\\Delta}-\\log2.\n\\end{align*}\\]\nこれより，低温領域に於ては， \\[\n\\Phi_N\\ge f\\;\\mathrm{on}\\;(0,\\Delta_c)\n\\] が確認できた．\n続いて， \\[\\begin{align*}\n    \\frac{\\partial \\Phi_N(\\Delta)}{\\partial \\Delta}&=\\frac{\\partial \\Phi_N(\\Delta)}{\\partial \\beta}\\frac{d \\beta}{d \\Delta}\\\\\n    &=\\frac{\\operatorname{E}[\\lvert\\langle X\\rangle\\rvert^2]}{2}\\left(-\\frac{1}{\\Delta^2}\\right)\\le0\n\\end{align*}\\] であることと， \\[\n\\lim_{\\Delta\\to\\infty}\\Phi_N(\\Delta)=0\n\\] であることから，\\([\\Delta_c,\\infty)\\) 上においても非負であることがわかる．\n以上より， \\[\n\\Phi_N\\ge f\\;\\mathrm{on}\\;\\mathbb{R}_+.\n\\]\n\n\n\n\n\n\n\n\n\n補題（上界）10\n\n\n\n\\[\n\\Phi_N(\\Delta)\\le f(\\Delta)+o(1)\\quad(N\\to\\infty)\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\Phi_N\\) の上界は，Boltzmann 分布 \\(p(x|y)\\) に関する期待値に対して，Jensen の不等式を用いることで導かれる．このような不等式は annealed bound とも呼ばれる．\n真値 \\(x^*\\) とノイズ \\(z\\) が確定している（従って，\\(x^*_{i^*}=1\\) を満たす \\(i^*\\in[d]\\) も確定している）とすると， \\[\\begin{align*}\n    \\mathcal{Z}&=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(-\\frac{N\\beta}{2}+Nx_i^*\\beta+z_i\\sqrt{N\\beta}\\right)\\\\\n    &=\\frac{1}{2^N}\\left(\\exp\\left(\\frac{N\\beta}{2}+z_{i^*}\\sqrt{N\\beta}\\right)+\\sum_{i\\ne i^*}\\exp\\left(-\\frac{N\\beta}{2}+z_i\\sqrt{N\\beta}\\right)\\right)\n\\end{align*}\\] \\[\n\\log\\mathcal{Z}=\\log\\left(\\exp\\left(\\frac{N\\beta}{2}+z_{i^*}\\sqrt{N\\beta}-N\\log 2\\right)+\\sum_{i\\ne i^*}\\frac{1}{2^N}\\exp\\left(-\\frac{N\\beta}{2}*Z_i\\sqrt{N\\beta}\\right)\\right)\n\\] と計算できる．凹関数 \\(\\log\\) に対する Jensen の不等式より， \\[\\begin{align*}\n    \\operatorname{E}[\\log\\mathcal{Z}]&\\le\\operatorname{E}\\biggl[\\log\\biggr(\\exp\\biggr(\\frac{N\\beta}{2}+Z_{i^*}\\sqrt{N\\beta}-N\\log 2\\biggl)\\\\\n    &\\qquad\\quad\\qquad+\\sum_{i\\ne i^*}\\operatorname{E}_{Z_{-i^*}}\\biggl[\\frac{1}{2^N}\\exp\\biggr(-\\frac{N\\beta}{2}+Z_i\\sqrt{N\\beta}\\biggl)\\biggr]\\biggl)\\biggr]\\\\\n    &=\\operatorname{E}\\left[\\log\\left(\\exp\\left(\\frac{N\\beta}{2}+Z_{i^*}\\sqrt{N\\beta}-N\\log 2\\right)+\\frac{2^{N-1}}{2^N}\\right)\\right]\\\\\n    &\\le\\operatorname{E}\\left[\\log\\left(\\exp\\left(\\frac{N\\beta}{2}+Z_{i^*}\\sqrt{N\\beta}-N\\log 2\\right)+1\\right)\\right]\\\\\n    &=\\operatorname{E}\\left[\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}e^{Z_{i^*}\\sqrt{N\\beta}}+1\\biggl)\\right].\n\\end{align*}\\]\n最初の等号にて，\\(\\xi\\sim\\mathrm{N}(\\mu,\\sigma^2)\\) の積率母関数が \\[\n\\operatorname{E}[e^{t\\xi}]=\\exp\\left(\\mu t+\\frac{\\sigma^2t^2}{2}\\right)\n\\] で表せることを用いた．\nこの \\(Z_{i^*}\\) という確率変数は複雑に定まっており，これを直接議論することは後回しにする．\n本補題の主張は，純粋に関数 \\[\ng(z):=\\exp\\left(N\\left(\\frac{\\beta}{2}-\\log 2\\right)+z\\sqrt{N\\beta}\\right)+1\n\\] の性質を考察するだけで従う．この関数 \\(g\\) を用いると， \\[\\begin{align*}\n    \\Phi_N(\\beta)&=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\\\\\n    &\\le\\frac{1}{N}\\operatorname{E}\\biggl[\\log(g(Z_{i^*}))\\biggr]\n\\end{align*}\\] と表せる．\\(g\\) も \\(\\log g\\) も凸関数であるから， \\[\\begin{align*}\n    \\log g(\\lvert z\\rvert)&\\le\\log g(0)+\\lvert z\\rvert\\frac{d }{d z}\\log g(\\lvert z\\rvert)\\\\\n    &=\\log g(0)+\\frac{g'(\\lvert z\\rvert)}{g(\\lvert z\\rvert)}\\\\\n    &\\le\\log g(0)+\\lvert z\\rvert\\sqrt{N\\beta}\\\\\n    &=\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}+1\\biggl)+\\lvert z\\rvert\\sqrt{N\\beta}.\n\\end{align*}\\]\nこれより， \\[\\begin{align*}\n    \\Phi_N(\\beta)&\\le\\frac{1}{N}\\operatorname{E}\\biggl[\\log(g(\\lvert Z_{i^*}\\rvert))\\biggr]\\\\\n    &\\le\\frac{1}{N}\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}+1\\biggl)+\\sqrt{\\frac{\\beta}{N}}\\operatorname{E}[\\lvert Z_{i^*}\\rvert].\n\\end{align*}\\]\nよって，\\(Z_{i^*}\\) が可積分であることさえ認めれば，\\(N\\to\\infty\\) のとき， \\[\n\\Delta\\ge\\Delta_c\\quad\\Leftrightarrow\\quad\\frac{\\beta}{2}\\le\\log2\n\\] のとき，\\(\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}+1\\biggl)\\ge0\\) であり，そうでない場合は \\[\n\\log(1+e^x)\\le x\\quad x\\in\\mathbb{R}_+\n\\] より， \\[\n\\Phi_N(\\Delta)\\le\\frac{\\beta}{2}-\\log2+o(1)\\quad(N\\to\\infty)\n\\] を得る．\n\n\n\n\n\n3.5 解釈\n\\(N\\to\\infty\\) の極限において，高温領域 \\(\\Delta&gt;\\Delta_c\\) において，最小自乗誤差（MMSE）は \\(1\\) であり，何をどうしても復号することはできない．Gauss ノイズ \\(\\Delta=\\sigma^2\\) が大きすぎるのである．\n一方で，低温領域 \\(\\Delta&lt;\\Delta_c\\) において，\\(\\partial_\\beta f=\\frac{1}{2}\\) であり，従って MMSE は \\(0\\) になる．よって完全な誤りのない復号が可能であるはずである．\n\n\n\n\n\n\n設定（第 3.1 節）が与えられたならば，立ち所に，\\(\\Delta\\) が大きいほど復号が難しいことは予想がつく．しかし，ある閾値 \\(\\Delta_c\\) に依存して，少なくとも \\(N\\to\\infty\\) の極限では，\n\n２つの領域で全く異なる振る舞いをすること\n２つの振る舞いのみに分類される（ある一定以上の確率で復号が可能である，などの中間状態がない）こと\n\nは驚きである．\n\n\n\n実は，高温領域 \\(\\Delta&gt;\\Delta_c\\) では，自由エネルギー \\(F_N\\) が \\(0\\) に指数収束する．\\(F_N\\) とは，完全な Gauss ノイズ \\[\nq(y)dy:=\\mathrm{N}_d\\biggr(0,\\frac{\\Delta}{N}I_d\\biggl)\n\\] と \\(p(y)\\) との KL 距離距離であったから，メッセージ \\(x^*\\) を完全な雑音と見分けることが加速度的に難しくなっていくのである．\n\n\n3.6 自由エネルギーのさらに鋭い評価\n\n\n\n\n\n\n命題（KL 乖離度は指数収束する）11\n\n\n\n高温領域 \\(\\Delta&gt;\\Delta_c\\) において，ある \\(C&gt;0\\) が存在して，任意の \\(N\\in\\mathbb{N}^+\\) について次が成り立つ：\n\\[\\begin{align*}\n    F_N(\\Delta)=\\operatorname{KL}\\biggr(p(y)\\,\\bigg|\\,q(y)\\biggl)\\le e^{-CN}\n\\end{align*}\\]\nただし，\\(p\\) は観測信号 \\(Y\\sim\\mathrm{N}_d\\left(X^*,\\frac{\\Delta}{N}I_d\\right)\\) の密度，\\(q\\) は \\(\\mathrm{N}_d\\left(0,\\frac{\\Delta}{N}I_d\\right)\\) の密度とした．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n第 3.4 節で示した \\(\\Phi_N\\) の上界評価では， \\[\\begin{align*}\n    \\Phi_N(\\beta)&=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\\\\\n    &\\le\\frac{1}{N}\\operatorname{E}\\left[\\log\\left(\\exp\\left(N\\left(\\frac{\\beta}{2}-\\log 2\\right)+Z_i^*\\sqrt{N\\beta}\\right)+1\\right)\\right]\n\\end{align*}\\] を得て，\\(Z_i^*\\) の評価を回避したのであった．\nこれに正面から取り組むことで，高温領域 \\(\\Delta&gt;\\Delta_c\\) での \\(\\Phi_N\\) の指数収束を示すことができる．\n高温領域 \\(\\Delta&gt;\\Delta_c\\) では， \\[\nf:=-\\frac{\\beta}{2}+\\log2&gt;0\n\\] が成り立つ．\n\\(Z_{i^*}\\) は，まず \\(X^*\\) によって条件付ければ \\(Z_{i^*}|X^*\\sim\\mathrm{N}(0,1)\\) であるから， \\[\\begin{align*}\n    N\\Phi_N(\\beta)&\\le\\operatorname{E}\\left[\\log\\biggr(e^{-fN+Z_{i^*}\\sqrt{N\\beta}}+1\\biggl)\\right]\\\\\n    &=\\int_\\mathbb{R}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-fN+z\\sqrt{N\\beta}}+1\\biggl)\\,dz\\\\\n    &=\\left(\\int^{R}_{-\\infty}+\\int_{R}^\\infty\\right)\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-fN+z\\sqrt{N\\beta}}+1\\biggl)\\,dz\n\\end{align*}\\] と分解すると，まず \\((-\\infty,R)\\) 上の積分は \\(N\\to\\infty\\) に関して指数収束する．\n実際，\\(R&gt;0\\) に対して \\(N\\in\\mathbb{N}^+\\) を十分大きく取ることで，ある \\(\\epsilon&gt;0\\) が存在して \\[\n-fN+z\\sqrt{N\\beta}\\le-\\epsilon fN\n\\] を満たすようにできるから，\n\\[\\begin{align*}\n    &\\quad\\int^{R}_{-\\infty}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-fN+z\\sqrt{N\\beta}}+1\\biggl)\\,dz\\\\\n    &&lt;\\int_\\mathbb{R}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-\\epsilon fN}+1\\biggl)\\,dz\\\\\n    &=\\log\\left(1+e^{-\\epsilon fN}\\right)\\le e^{-\\epsilon fN}.\n\\end{align*}\\] 最後の不等式は \\[\n\\log(1+x)\\le x\\quad x\\in\\mathbb{R}\n\\] による．\n従って，あとは \\((R,\\infty)\\) 上の積分が指数収束することを示せば良いが，再び \\(\\log(1+x)\\le x\\) に注意して \\[\\begin{align*}\n    &\\quad\\int^\\infty_\\mathbb{R}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\left(e^{-fN+z\\sqrt{N\\beta}}+1\\right)\\,dz\\\\\n    &\\le\\int^\\infty_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}}e^{-fN+z\\sqrt{N\\beta}-\\frac{z^2}{2}}\\,dz\\\\\n    &=e^{\\frac{N\\beta}{2}-fN}\\int^\\infty_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(z-\\sqrt{N\\beta})^2}{2}}\\,dz\\\\\n    &=e^{\\frac{N\\beta}{2}-fN}\\operatorname{P}\\biggl[Z&gt;R-\\sqrt{N\\beta}\\biggr]\n\\end{align*}\\] と評価できるから，あとは \\(Z\\sim\\gamma_1\\) の尾部確率が指数収束するかどうか（正確には劣 Gauss 性を持つかどうか）の問題に帰着する．\n実はこれは yes である．中心化された確率変数の積率母関数が，ある \\(\\sigma&gt;0\\) に関して \\[\n\\operatorname{E}[e^{\\lambda\\xi}]\\le e^{\\frac{\\lambda^2\\sigma^2}{2}}\\quad\\lambda\\in\\mathbb{R}\n\\] を満たすことは，ある \\(\\kappa&gt;0\\) が存在して \\[\n\\operatorname{P}[\\lvert\\xi\\rvert\\ge t]\\le 2e^{-\\frac{t^2}{2\\kappa^2}}\n\\] を満たすことに同値になる．特に，\\(\\Rightarrow\\) 方向には \\(\\kappa=\\sigma\\) ととれる．12 \\(\\Phi_N\\) の上界評価（第 3.4 節）で触れたように，（中心化された）Gauss 確率変数はこれを満たす．\nよって， \\[\\begin{align*}\n    &\\quad e^{\\frac{N\\beta}{2}-fN}\\operatorname{P}\\biggl[Z&gt;R-\\sqrt{N\\beta}\\biggr]\\\\\n    &\\le e^{\\frac{N\\beta}{2}-fN}e^{-\\frac{(R-\\sqrt{N\\beta})^2}{2}}\\\\\n    &=e^{\\frac{N\\beta}{2}-fN}e^{-\\frac{R^2}{2}+\\sqrt{N\\beta}R-\\frac{N\\beta}{2}}\\\\\n    &=\\exp\\left(-\\frac{R^2}{2}+\\sqrt{N\\beta}R-Nf\\right).\n\\end{align*}\\]\nこの最右辺，ある定数 \\(C&gt;0\\) が存在して，\\(e^{-CN}\\) で抑えられる．\n\nあるいは，任意の \\(N\\in\\mathbb{N}^+\\) に対して \\[\nR:=\\sqrt{\\frac{N}{\\beta}}f-\\delta\\quad\\delta&gt;0\n\\] と取ることで，\\((-\\infty,R),(R,\\infty)\\) 上の積分のそれぞれについて，同様の評価を得る．\n\n以上より， \\[\\begin{align*}\n    F_N(\\Delta)&=N\\Phi_N\\\\\n    &\\le e^{-KN}.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n命題（劣 Gauss 確率変数の特徴付け）13\n\n\n\n\\(X\\in L(\\Omega)\\) を確率変数とする．このとき，次の４条件は同値で，さらに \\(\\{K_1,\\cdots,K_5\\}\\subset\\mathbb{R}^+\\) はある \\(C\\in\\mathbb{R}\\) が存在して，\\(\\forall_{i,j\\in[5]}\\;K_j\\le CK_i\\) を満たすように取れる．\n\n尾部確率の評価：ある \\(K_1&gt;0\\) が存在して，14 \\[\n\\mathrm{P}[\\lvert X\\rvert\\ge t]\\le 2e^{-\\frac{t^2}{K_1^2}},\\qquad t\\ge0.\n\\]\n\\(L^p\\)-ノルムの評価：ある \\(K_2&gt;0\\) が存在して， \\[\n\\|X\\|_{L^p}\\le K_2\\sqrt{p},\\qquad p\\ge1.\n\\]\n\\(X^2\\) の積率母関数の \\(0\\) の近傍での評価：ある \\(K_3&gt;0\\) が存在して， \\[\n\\lvert\\lambda\\rvert\\le\\frac{1}{K_3}\\quad\\Rightarrow\\quad \\mathrm{E}[e^{\\lambda^2 X^2}]\\le e^{K_3^2\\lambda^2}.\n\\]\n\\(X^2\\) の積率母関数のある1点での値：ある \\(K_4&gt;0\\) が存在して， \\[\n\\operatorname{E}\\left[e^{\\frac{X^2}{K_4^2}}\\right]\\le2.\n\\]\nさらに，\\(X\\) が中心化されている場合，次とも同値：ある \\(K_5&gt;0\\) が存在して， \\[\n\\operatorname{E}[e^{\\lambda X}]\\le e^{K^2_5\\lambda^2},\\qquad\\lambda\\in\\mathbb{R}.\n\\]\n\n以上の同値な条件を満たす確率変数 \\(X\\) を 劣 Gauss (sub-Gaussian) という．\n\n\n\n\n3.7 まとめ\n大変単純化された設定 toy model であったが，比例的高次元極限 \\(N\\to\\infty\\) において，厳密に示せる相転移を示す模型である．\nすなわち，ただ一つの非零成分 \\(1\\) に対して，ノイズの分散 \\(\\sigma^2\\) が \\((2\\log2)^{-1}\\) より大きいかどうかで，これが復号可能かどうかが決まる．\nこの \\(2\\log2\\) という値は (Donoho and Johnstone, 1994) が universal threshold と呼ぶ値の例であり，ランダムエネルギーモデルのスピングラス相転移境界と対応する．15\n一般のモデルでは，この臨界温度の値は不明である上に，\\(\\Delta&gt;\\Delta_c\\) の高温領域での効率的な推定法が見つかっていない場合も多い．\nこのような，高次元統計推測の問題においては，統計物理学，特にスピングラス理論の知見が活発に応用されて，漸近極限における相図の解明，統計計算手法の開発が目指されている．\n\n\n\nSchematic representation of a typical high-dimensional inference problem from (Zdeborová and Krzakala, 2016, p. 466)\n\n\n\n    \n        \n            \n            \n                ベイズ統計学とスピングラス\n                誤り訂正符号を題材にして"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#終わりに",
    "href": "posts/2024/Stat/Bayes1.html#終わりに",
    "title": "ベイズ統計学と統計物理学",
    "section": "4 終わりに",
    "text": "4 終わりに\n\n第 3 節において，統計力学の知見は，レプリカ法などの計算手法を通じて，\\(N\\to\\infty\\) における漸近極限として，「大規模な確率分布から平均値を計算するための”ツールボックス”」(樺島祥介, 2003) として働いている．\n\n大自由度系では，次元に関して計算量が指数約に爆発してしまう．平衡統計力学の歴史はこの問題との戦いの歴史である，と言っても過言ではない．(樺島祥介 and 杉浦正康, 2008, p. 22)"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#footnotes",
    "href": "posts/2024/Stat/Bayes1.html#footnotes",
    "title": "ベイズ統計学と統計物理学",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Zdeborová and Krzakala, 2016, p. 467) も参照．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 25) や (Krazakala and Zdeborová, 2024, p. 119) に倣った．↩︎\n(Krazakala and Zdeborová, 2024, p. 122)．↩︎\n(Zdeborová and Krzakala, 2016, p. 464)，(Krazakala and Zdeborová, 2024, p. 123) 定理13，(Iba, 1999, pp. 3876–3877) などで扱われている．西森ライン上のみで見られる性質であるため，西森対称性と呼ぶ．西森ラインについては 次項 も参照．↩︎\n(Mézard and Montanari, 2009, p. 249) や (Iba, 1999, p. 3876) などでは thermal average と quenched average の用語が採用されている．↩︎\n(Krazakala and Zdeborová, 2024, p. 124) 定理14．↩︎\n(西森秀稔, 2005, p. 123)なども参照．↩︎\n(Krazakala and Zdeborová, 2024, p. 125) 定理15，(樺島祥介 and 杉浦正康, 2008, p. 14) なども参照．証明は (Krazakala and Zdeborová, 2024, p. 133) 7.B 節 を参考にした．↩︎\n(Krazakala and Zdeborová, 2024, p. 125)補題８．↩︎\n(Krazakala and Zdeborová, 2024, p. 126)補題９．↩︎\n(Krazakala and Zdeborová, 2024, p. 132)補題10．↩︎\n(Vershynin, 2018, p. 25)命題2.5.2など．↩︎\n(Vershynin, 2018, p. 25)命題2.5.2．↩︎\nこの定数2は，1よりも真に大きい定数ならばなんでも良い．(Vershynin, 2018, p. 27)注2.5.3 も参照．↩︎\n(Krzakala et al., 2015, p. 8) また p.16 も参照すべし．↩︎"
  },
  {
    "objectID": "posts/2024/AI/LLM.html",
    "href": "posts/2024/AI/LLM.html",
    "title": "大規模言語モデル",
    "section": "",
    "text": "Mistral AI は 2023 年に Google DeepMind の研究者１人と Meta Platform の元研究者２人によって設立されたフランス企業で，オープンソースでの大規模言語モデルの開発を行っている．\nLLama 2 70B モデルより性能が良いとされているが，ヨーロッパ系の言語５言語のみに特化しているモデルである．\nMistral Cookbook ではコミュニティによる Mistral AI の言語モデルの利用事例が公開されている．このいくつかを本記事では見て遊んでいく．\nAPI の利用には 登録が必要 であるが，サブスクリプションではなくて利用量に応じた課金方式である．"
  },
  {
    "objectID": "posts/2024/AI/LLM.html#rag",
    "href": "posts/2024/AI/LLM.html#rag",
    "title": "大規模言語モデル",
    "section": "0.1 RAG",
    "text": "0.1 RAG\nRAG (Retrieval-Augmented Generation) (Lewis et al., 2020) は，言語モデルと情報検索を次のように組み合わせることで，質問応答などのタスクでの性能を上げる手法である：\n\n情報検索を行い，関連があると思われる情報を知識ベースから抽出する．\n抽出した情報をプロンプトに含めて言語モデルに入力する．\n\nこれを Mistral を用いて実装してみる．\n\n0.1.1 Import needed packages\nThe first step is to install the needed packages mistralai and faiss-cpu and import the needed packages:\n! pip install faiss-cpu==1.7.4 mistralai==0.0.12\n\n\nCode\nfrom mistralai.client import MistralClient, ChatMessage\nimport requests\nimport numpy as np\nimport faiss\nimport os\nfrom getpass import getpass\n\napi_key= getpass(\"Type your API Key\")\nclient = MistralClient(api_key=api_key)\n\n\n\n\n0.1.2 外部データの下処理\nPaul Graham のエッセイを知識ベースとして用いることを考える．\n\n\nCode\nresponse = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt')\ntext = response.text\nlen(text)\n\n\nf = open('essay.txt', 'w')\nf.write(text)\nf.close()\n関連する情報の抽出を効率的に行うため，外部情報を小さなチャンクに分割することを考える．\nIn a RAG system, it is crucial to split the document into smaller chunks so that it’s more effective to identify and retrieve the most relevant information in the retrieval process later. In this example, we simply split our text by character, combine 2048 characters into each chunk, and we get 37 chunks.\n\n\nCode\nchunk_size = 2048\nchunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n\n\n\n\nCode\nlen(chunks)\n\n\n\n0.1.2.1 Considerations:\n\nChunk size: Depending on your specific use case, it may be necessary to customize or experiment with different chunk sizes and chunk overlap to achieve optimal performance in RAG. For example, smaller chunks can be more beneficial in retrieval processes, as larger text chunks often contain filler text that can obscure the semantic representation. As such, using smaller text chunks in the retrieval process can enable the RAG system to identify and extract relevant information more effectively and accurately. However, it’s worth considering the trade-offs that come with using smaller chunks, such as increasing processing time and computational resources.\nHow to split: While the simplest method is to split the text by character, there are other options depending on the use case and document structure. For example, to avoid exceeding token limits in API calls, it may be necessary to split the text by tokens. To maintain the cohesiveness of the chunks, it can be useful to split the text by sentences, paragraphs, or HTML headers. If working with code, it’s often recommended to split by meaningful code chunks for example using an Abstract Syntax Tree (AST) parser.\n\n\n\n\n0.1.3 Create embeddings for each text chunk\nFor each text chunk, we then need to create text embeddings, which are numeric representations of the text in the vector space. Words with similar meanings are expected to be in closer proximity or have a shorter distance in the vector space. To create an embedding, use Mistral’s embeddings API endpoint and the embedding model mistral-embed. We create a get_text_embedding to get the embedding from a single text chunk and then we use list comprehension to get text embeddings for all text chunks.\n\n\nCode\ndef get_text_embedding(input):\n    embeddings_batch_response = client.embeddings(\n          model=\"mistral-embed\",\n          input=input\n      )\n    return embeddings_batch_response.data[0].embedding\n\n\n\n\nCode\ntext_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n\n\n\n\nCode\ntext_embeddings.shape\n\n\n\n\nCode\ntext_embeddings\n\n\n\n\n0.1.4 Load into a vector database\nOnce we get the text embeddings, a common practice is to store them in a vector database for efficient processing and retrieval. There are several vector database to choose from. In our simple example, we are using an open-source vector database Faiss, which allows for efficient similarity search.\nWith Faiss, we instantiate an instance of the Index class, which defines the indexing structure of the vector database. We then add the text embeddings to this indexing structure.\n\n\nCode\nd = text_embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(text_embeddings)\n\n\n\n0.1.4.1 Considerations:\n\nVector database: When selecting a vector database, there are several factors to consider including speed, scalability, cloud management, advanced filtering, and open-source vs. closed-source.\n\n\n\n\n0.1.5 Create embeddings for a question\nWhenever users ask a question, we also need to create embeddings for this question using the same embedding models as before.\n\n\nCode\nquestion = \"What were the two main things the author worked on before college?\"\nquestion_embeddings = np.array([get_text_embedding(question)])\nquestion_embeddings.shape\n\n\n\n\nCode\nquestion_embeddings\n\n\n\n0.1.5.1 Considerations:\n\nHypothetical Document Embeddings (HyDE): In some cases, the user’s question might not be the most relevant query to use for identifying the relevant context. Instead, it maybe more effective to generate a hypothetical answer or a hypothetical document based on the user’s query and use the embeddings of the generated text to retrieve similar text chunks.\n\n\n\n\n0.1.6 Retrieve similar chunks from the vector database\nWe can perform a search on the vector database with index.search, which takes two arguments: the first is the vector of the question embeddings, and the second is the number of similar vectors to retrieve. This function returns the distances and the indices of the most similar vectors to the question vector in the vector database. Then based on the returned indices, we can retrieve the actual relevant text chunks that correspond to those indices.\n\n\nCode\nD, I = index.search(question_embeddings, k=2)\nprint(I)\n\n\n\n\nCode\nretrieved_chunk = [chunks[i] for i in I.tolist()[0]]\nprint(retrieved_chunk)\n\n\n\n0.1.6.1 Considerations:\n\nRetrieval methods: There are a lot different retrieval strategies. In our example, we are showing a simple similarity search with embeddings. Sometimes when there is metadata available for the data, it’s better to filter the data based on the metadata first before performing similarity search. There are also other statistical retrieval methods like TF-IDF and BM25 that use frequency and distribution of terms in the document to identify relevant text chunks.\nRetrieved document: Do we always retrieve individual text chunk as it is? Not always.\n\nSometimes, we would like to include more context around the actual retrieved text chunk. We call the actual retrieve text chunk “child chunk” and our goal is to retrieve a larger “parent chunk” that the “child chunk” belongs to.\nOn occasion, we might also want to provide weights to our retrieve documents. For example, a time-weighted approach would help us retrieve the most recent document.\nOne common issue in the retrieval process is the “lost in the middle” problem where the information in the middle of a long context gets lost. Our models have tried to mitigate this issue. For example, in the passkey task, our models have demonstrated the ability to find a “needle in a haystack” by retrieving a randomly inserted passkey within a long prompt, up to 32k context length. However, it is worth considering experimenting with reordering the document to determine if placing the most relevant chunks at the beginning and end leads to improved results.\n\n\n\n\n\n0.1.7 Combine context and question in a prompt and generate response\nFinally, we can offer the retrieved text chunks as the context information within the prompt. Here is a prompt template where we can include both the retrieved text and user question in the prompt.\n\n\nCode\nprompt = f\"\"\"\nContext information is below.\n---------------------\n{retrieved_chunk}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {question}\nAnswer:\n\"\"\"\n\n\n\n\nCode\ndef run_mistral(user_message, model=\"mistral-medium-latest\"):\n    messages = [\n        ChatMessage(role=\"user\", content=user_message)\n    ]\n    chat_response = client.chat(\n        model=model,\n        messages=messages\n    )\n    return (chat_response.choices[0].message.content)\n\n\n\n\nCode\nrun_mistral(prompt)\n\n\n\n0.1.7.1 Considerations:\n\nPrompting techniques: Most of the prompting techniques can be used in developing a RAG system as well. For example, we can use few-shot learning to guide the model’s answers by providing a few examples. Additionally, we can explicitly instruct the model to format answers in a certain way.\n\nIn the next sections, we are going to show you how to do a similar basic RAG with some of the popular RAG frameworks. We will start with LlamaIndex and add other frameworks in the future."
  },
  {
    "objectID": "posts/2024/AI/LLM.html#langchain",
    "href": "posts/2024/AI/LLM.html#langchain",
    "title": "大規模言語モデル",
    "section": "0.2 LangChain",
    "text": "0.2 LangChain\n\n\nCode\n!pip install langchain langchain-mistralai==0.0.4\n\n\n\n\nCode\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_mistralai.chat_models import ChatMistralAI\nfrom langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\n\n# Load data\nloader = TextLoader(\"essay.txt\")\ndocs = loader.load()\n# Split text into chunks\ntext_splitter = RecursiveCharacterTextSplitter()\ndocuments = text_splitter.split_documents(docs)\n# Define the embedding model\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n# Create the vector store\nvector = FAISS.from_documents(documents, embeddings)\n# Define a retriever interface\nretriever = vector.as_retriever()\n# Define LLM\nmodel = ChatMistralAI(mistral_api_key=api_key)\n# Define prompt template\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\nQuestion: {input}\"\"\")\n\n# Create a retrieval chain to answer questions\ndocument_chain = create_stuff_documents_chain(model, prompt)\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\nresponse = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\nprint(response[\"answer\"])"
  },
  {
    "objectID": "posts/2024/AI/LLM.html#llamaindex",
    "href": "posts/2024/AI/LLM.html#llamaindex",
    "title": "大規模言語モデル",
    "section": "0.3 LlamaIndex",
    "text": "0.3 LlamaIndex\n\n\nCode\n!pip install llama-index==0.10.13 llama-index-llms-mistralai==0.1.4 llama-index-embeddings-mistralai==0.1.3\n\n\n\n\nCode\nimport os\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.llms.mistralai import MistralAI\nfrom llama_index.embeddings.mistralai import MistralAIEmbedding\n\n# Load data\nreader = SimpleDirectoryReader(input_files=[\"essay.txt\"])\ndocuments = reader.load_data()\n# Define LLM and embedding model\nSettings.llm = MistralAI(model=\"mistral-medium\")\nSettings.embed_model = MistralAIEmbedding(model_name='mistral-embed')\n# Create vector store index\nindex = VectorStoreIndex.from_documents(documents)\n# Create query engine\nquery_engine = index.as_query_engine(similarity_top_k=2)\nresponse = query_engine.query(\n    \"What were the two main things the author worked on before college?\"\n)\nprint(str(response))"
  },
  {
    "objectID": "posts/2024/Julia/HMCwithJulia.html",
    "href": "posts/2024/Julia/HMCwithJulia.html",
    "title": "Hamiltonian Monte Carlo 法",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nJulia による MCMC パッケージの概観は次の稿も参照："
  },
  {
    "objectID": "posts/2024/Julia/HMCwithJulia.html#advancedhmc.jl",
    "href": "posts/2024/Julia/HMCwithJulia.html#advancedhmc.jl",
    "title": "Hamiltonian Monte Carlo 法",
    "section": "1 AdvancedHMC.jl",
    "text": "1 AdvancedHMC.jl\nCauchy 分布に HMC を適用するとぶっ壊れる！？\n\nusing AdvancedHMC, ForwardDiff\nusing LogDensityProblems\nusing LinearAlgebra\nusing Plots\n\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\n# Define the target distribution (1D Cauchy) using the `LogDensityProblem` interface\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\nfunction HMC_sample(initial_θ)\n\n    # Choose initial parameter value for 1D\n    initial_θ = [initial_θ]\n\n    # Define the Cauchy distribution with location and scale\n    loc, scale = 0.0, 1.0\n    ℓπ = LogTargetDensityCauchy(loc, scale)\n\n    # Set the number of samples to draw and warmup iterations\n    n_samples, n_adapts = 2_000, 1\n\n    # Define a Hamiltonian system\n    metric = DiagEuclideanMetric(1)\n    hamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n\n    # Define a leapfrog solver, with the initial step size chosen heuristically\n    initial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\n    integrator = Leapfrog(initial_ϵ)\n\n    # Define an HMC sampler with the following components\n    #   - multinomial sampling scheme,\n    #   - generalised No-U-Turn criteria, and\n    #   - windowed adaption for step-size and diagonal mass matrix\n    kernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))\n    adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n\n    # Run the sampler to draw samples from the specified Cauchy distribution, where\n    #   - `samples` will store the samples\n    #   - `stats` will store diagnostic statistics for each sample\n    samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts; progress=true)\n\n    # Print the results\n    sample_values = [s[1] for s in samples]\n\n    p = plot(1:length(samples), sample_values,\n                label=\"HMC trajectory\",\n                title=\"1D HMC Sampler (Cauchy distribution)\",\n                xlabel=\"t\",\n                ylabel=\"X\",\n                linewidth=2,\n                marker=:circle,\n                markersize=2,\n                markeralpha=0.6,\n                color=\"#78C2AD\")\nend\n\nHMC_sample(50.0)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/gA66f/src/ProgressMeter.jl:590\nSampling 100%|███████████████████████████████| Time: 0:00:00\n  iterations:                                   2000\n  ratio_divergent_transitions:                  0.02\n  ratio_divergent_transitions_during_adaption:  0.0\n  n_steps:                                      3\n  is_accept:                                    true\n  acceptance_rate:                              4.073791219423035e-7\n  log_density:                                  -10.758856435114664\n  hamiltonian_energy:                           13.458401323991495\n  hamiltonian_energy_error:                     0.0\n  max_hamiltonian_energy_error:                 811.5344241268535\n  tree_depth:                                   2\n  numerical_error:                              false\n  step_size:                                    324.39138009045865\n  nom_step_size:                                324.39138009045865\n  is_adapt:                                     false\n  mass_matrix:                                  DiagEuclideanMetric([1.0])\n┌ Info: Finished 2000 sampling steps for 1 chains in 1.282884958 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=324.0), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 0.30445073109268245\n└   average_acceptance_rate = 0.22613549485908152\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 50 にした場合\n\n\n\nHMC_sample(0.0)\n\n┌ Info: Finished 2000 sampling steps for 1 chains in 0.044786417 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=14.7), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 2.018632634147994\n└   average_acceptance_rate = 0.00031706816850295914\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 0 にした場合\n\n\n\nHMC_sample(500.0)\n\n┌ Info: Finished 2000 sampling steps for 1 chains in 0.056288833 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=1670.0), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 0.2517834551566722\n└   average_acceptance_rate = 0.20632568153387063\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 500 にした場合\n\n\n\nHMC_sample(100.0)\n\n┌ Info: Finished 2000 sampling steps for 1 chains in 0.043402167 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=956.0), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 1.998017004885298\n└   average_acceptance_rate = 6.0828891145860786e-9\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 100 にした場合"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html",
    "href": "posts/2024/Julia/MCMCwithJulia.html",
    "title": "Julia による MCMC サンプリング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html#導入",
    "href": "posts/2024/Julia/MCMCwithJulia.html#導入",
    "title": "Julia による MCMC サンプリング",
    "section": "1 導入",
    "text": "1 導入\n具体的なサンプラーは次の稿も参照：\n\n    \n        \n            \n            \n                Julia による Hamiltonian Monte Carlo\n                \n            \n        \n    \n\n\n    \n        \n            \n            \n                Julia による Metropolis-Hastings サンプラー\n                \n            \n        \n    \n\n\n1.1 依存パッケージ一覧\n\nAbstractMCMC.jl (GitHub / Juliapackages / HP) は MCMC サンプリングのための抽象的なインターフェースを提供するパッケージ．後述の Turing.jl エコシステムの基盤となる．\nLogDensityProblems.jl：対数密度を扱うフレームワーク．\n\n\n\n1.2 MCMC パッケージ一覧\n\nAdaptiveMCMC.jl (Juliapackages / Docs) は適応的な乱歩 MH アルゴリズムを提供するパッケージ．\n\n関連に AdaptiveParticleMCMC.jl (GitHub) がある．これは SequentialMonteCarlo.jl (GitHub) に基づいている．\n\nMamba.jl (Docs / GitHub) Markov chain Monte Carlo (MCMC) for Bayesian analysis in julia\nKissMCMC (Juliapackages / GitHub) は ‘Keep it simple, stupid, MCMC’ ということで，計量な MCMC を提供するパッケージ．\nDynamicHMC.jl (GitHub) は NUTS サンプラーを提供するパッケージ．Tamás K. Papp による．\nSGMCMC.jl (GitHub) は Oxford CSML チームによる，Bayesian deep learning に向けたサンプラーを提供するパッケージ．\n\n\n\n1.3 連続時間 MCMC パッケージ一覧\n\n\n\n\n\n\n連続時間 MCMC\n\n\n\n\nZigZagBoomerang.jl (GitHub / Juliapackages)\nPDMP.jl (GitHub / Docs) は 2018 年まで Alan Turing Institute によって開発されていたパッケージ．\nPiecewiseDeterministicMarkovProcesses.jl (GitHub / Docs / (Veltz, 2015) / HP of Dr. Veltz / Discource) は細胞生物学におけるモデリング手法としての PDMP を提供するパッケージである．\n\n\n\n\n\n1.4 確率的プログラミング\n代表的なものは次の２つである：\n\n\n\n\n\n\nベイズ推論のためのパッケージ\n\n\n\n\nTuring.jl (HP / GitHub / Juliapackages / (Ge et al., 2018))\nSoss.jl (GitHub / Docs)\n\n\n\nただし，現状の Soss.jl v0.21.2 (6/22/2022) は FillArrays のバージョンを 0.13.11 以下に制限してしまうため（最新は 1.11.0），これが Turing.jl の最新バージョン v0.33.1 と衝突してしまう．\nインストールは１つの環境にどちらか１つのみにすることが推奨される．\n\n\n1.5 Turing ecosystem\n\n\n\n\n\n\nTuring ecosystem 一覧\n\n\n\n\nAdvancedPS (GitHub / Docs) は Turing.jl による粒子フィルターベースのサンプラーを提供するパッケージ．\nAdvancedHMC (GitHub)\nAdvancedMH (GitHub)\nAdvancedVI (GitHub) 変分推論を提供するパッケージ．\nBijectors.jl (GitHub) 正則化流などによる分布の変換を提供するパッケージ．\n\n\n\n(Storopoli, 2021) も参照．"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html#abstractmcmc.jl-の枠組み",
    "href": "posts/2024/Julia/MCMCwithJulia.html#abstractmcmc.jl-の枠組み",
    "title": "Julia による MCMC サンプリング",
    "section": "2 AbstractMCMC.jl の枠組み",
    "text": "2 AbstractMCMC.jl の枠組み\nAbstractMCMC.jl は，DensityProblem をはじめとした AbstractModel と AbstractSampler のデータ構造を提供する，Turing エコシステムの根幹部分を支えるパッケージである．\n\n2.1 LogDensityProblem\nAbstractMCMC.jl は，Tamás K. Papp による LogDensityProblem.jl の wrapper を提供している．\nsample 関数なども，AbstractModel の他に，longdensity オブジェクトに対するメソッドも定義されている．"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html#mcmcchains.jl-と-abstractmcmc.jl",
    "href": "posts/2024/Julia/MCMCwithJulia.html#mcmcchains.jl-と-abstractmcmc.jl",
    "title": "Julia による MCMC サンプリング",
    "section": "3 MCMCChains.jl と AbstractMCMC.jl",
    "text": "3 MCMCChains.jl と AbstractMCMC.jl\nTuring によるエコシステムは，この MCMC のデザインパターンを利用している．\n\nusing MCMCChains\nusing StatsPlots\n\n# Define the experiment\nn_iter = 100\nn_name = 3\nn_chain = 2\n\n# experiment results\nval = randn(n_iter, n_name, n_chain) .+ [1, 2, 3]'\nval = hcat(val, rand(1:2, n_iter, 1, n_chain))\n\n# construct a Chains object\nchn = Chains(val, [:A, :B, :C, :D])\n\n# visualize the MCMC simulation results\nplot(chn; size=(840, 600))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1 sample 関数\nsample 関数は，MCMCCahins での実装 と AbstractMCMC での実装 との２つがある．\n\n3.1.1 使い方\nAbstractModel に対するデフォルトは次のように始まる：\nfunction mcmcsample(\n    rng::Random.AbstractRNG,\n    model::AbstractModel,\n    sampler::AbstractSampler,\n    N::Integer;\n    progress=PROGRESS[],\n    progressname=\"Sampling\",\n    callback=nothing,\n    discard_initial=0,\n    thinning=1,\n    chain_type::Type=Any,\n    initial_state=nothing,\n    kwargs...,\n)\ninisital_params を指定した場合：\nfunction mcmcsample(\n    rng::Random.AbstractRNG,\n    model::AbstractModel,\n    sampler::AbstractSampler,\n    ::MCMCThreads,\n    N::Integer,\n    nchains::Integer;\n    progress=PROGRESS[],\n    progressname=\"Sampling ($(min(nchains, Threads.nthreads())) threads)\",\n    initial_params=nothing,\n    initial_state=nothing,\n    kwargs...,\n)\n\n\n\n3.2 autocor 関数\nChains オブジェクトに対する autocor 関数が，次のように定義されている：\nfunction autocor(\n    chains::Chains;\n    append_chains = true,\n    demean::Bool = true,\n    lags::AbstractVector{&lt;:Integer} = _default_lags(chains, append_chains),\n    kwargs...\n)\n    funs = Function[]\n    func_names = @. Symbol(\"lag \", lags)\n    for i in lags\n        push!(funs, x -&gt; autocor(x, [i], demean=demean)[1])\n    end\n\n    return summarize(\n        chains, funs...;\n        func_names = func_names,\n        append_chains = append_chains,\n        name = \"Autocorrelation\",\n        kwargs...\n    )\nend"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html",
    "href": "posts/2024/Julia/Julia0.html",
    "title": "俺のための Julia 入門（０）",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#julia-のすすめ",
    "href": "posts/2024/Julia/Julia0.html#julia-のすすめ",
    "title": "俺のための Julia 入門（０）",
    "section": "1 Julia のすすめ",
    "text": "1 Julia のすすめ\n\n1.1 導入\nJulia は Why We Created Julia の文書と共に，2/14/2012 に公開された科学計算向けの言語である．論文 (Bezanson et al., 2017) が発表されて後，2018 年にはバージョン 1.0 がリリースされた．\n現在の最新は，12/26/2023 リリースの v1.10.0 である．\n\n\n\n\n\n\nリンク集\n\n\n\n\nJulia Documentation\nJulia Discourse\nJulia Forem\n\n\n\n\n\n1.2 Quarto で始める Julia\nQuarto は Jupyter を通じて，Pytho, R だけでなく Julia もサポートしている．1\n\n\nCode\nusing Plots\n\nplot(sin, \n    x-&gt;sin(2x), \n    0, \n    2π, \n    leg=false, \n    fill=(0,:lavender))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1: Parametric Plots\n\n\n\n\n\n\n\n\n\n\nJulia のインストール方法\n\n\n\n\n\nQuarto で Julia を始めるために，最も簡便なインストール方法は こちら．\n\nまずは Julia をインストール\ncurl -fsSL https://install.julialang.org | sh\nすると，julia コマンドで対話的セッションを開始できる．これを Julia では REPL (Read-Eval-Print Loop) と呼ぶ．\n続いて，Quarto で julia ブロックを動かすには，IJulia パッケージをインストールする．次を REPL で実行する：\nusing Pkg\nPkg.add(\"IJulia\")\nusing IJulia\nnotebook()\nこれで，Jupyter Notebook で Julia を使うことができる．2\nRevise.jl も追加すると，Julia セッションを再起動するためが省ける\nusing Pkg\nPkg.add(\"Revise\")\nJupyter Cache も追加すると，ソースが変わらない限りその出力がキャッシュされ，再実行が控えられる．\nusing Conda\nConda.add(\"jupyter-cache\")\n\nGetting Started\n\n\n\n\n\n\n\n\n\nJulia と IJulia のアップデート方法\n\n\n\n\n\njuliaup update\nその後，\njupyter kernelspec list\nで表示されるカーネルもアップデートする必要がある．アップデートは再インストールによる：\nusing IJulia\ninstallkernel(\"Julia\")\n\n\n\n\n\n1.3 パッケージ管理\nJulia を始めるにあたって最も心強いのは，Julia 独自のパッケージマネジャー Pkg の存在である．\njuliaキーワードで立ち上がる対話的実行環境 REPL（第 [@-sec-REPL] 節）において，] キーでパッケージモードに入り，add でパッケージを追加する．\n(@v1.10) pkg&gt; status  # または st\nでパッケージの情報が表示される．\n(@v1.10) pkg&gt; update  # または up\nでアップデートが可能である．\n\n\n\n\n\n\nトラブルシューティング\n\n\n\n\n\n(@v1.10) pkg&gt; status --outdated -m\nにより，アップデートがあるパッケージの一覧を得ることができる．\nものによっては，アップデートを阻害している依存関係が見れる．\n(@v1.10) pkg&gt; rm Example\n\n\n\n\n\n1.4 プロジェクト\nパッケージ管理システム Pkg が提供する環境分離システムの単位を プロジェクト という．3\ngenerateは新たなディレクトリと，Project.toml と src/MyPackage.jl を作成する．\n(@v1.10) pkg&gt; generate MyProject\nactivate . はパスを引数に取り，そのディレクトリにある Project.toml を有効化する．\n(@v1.10) pkg&gt; activate MyProject\n引数なしで activate を実行することで，デフォルトの環境に戻る．\n(@v1.10) pkg&gt; instantiate\nは，working directory にプロジェクトファイル（Project.toml と Manifest.toml）を作成する．\n\n\n1.5 数学表記\n何よりコードの見た目の特徴に，LaTeX コマンドと数学記号が使える ことがある．\nlaw tex を打って tab を押すと処理されるのである！\n\nα = 2  # \\alpha と打って tab を押すと処理される．\nβ = 3\nγ = α + β\n\nprintln(\"α + β = $γ\")\n\nα + β = 5\n\n\n加えて，プロットのタイトルにも使える：\n\n\nCode\nusing Plots\n\nplot(1:10, (1:10).^2, title=\"Plot of \\$y = x^2\\$\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこれと合わせ，変数名にはUnicodeが許される．÷, ^ などの使用が直感的に行える．\n\n÷\n\ndiv (generic function with 57 methods)\n\n\n\nx = 2^3  # 2の3乗\nprintln(\"2^3 = $x\")\n\n2^3 = 8\n\n\nまた，変数の前に数値をつけると暗黙に乗算と解釈する．\nそのほかにも関数定義など，数学的にも直感的に読める，文芸的プログラミングの精神が込められている言語である．\n\n\n1.6 Shell コマンド\npwd()\ncd()\nなどのコマンドが使える．"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#概要",
    "href": "posts/2024/Julia/Julia0.html#概要",
    "title": "俺のための Julia 入門（０）",
    "section": "2 概要",
    "text": "2 概要\n\n\n2.1 クラス構造\n\n\n\n\n\n\n\n組み込み型，組み込み関数はいずれもBaseクラスに属する．\n\n\n\n\n\n\n2.2 Read-Eval-Print Loop\njulia コマンドで起動される\n\n—project\n\n—project=@. の略記．\n現在のディレクトリ . にある Project.toml によるプロジェクトを有効化する，\nPERL における activate コマンドに対応する．\n環境変数 JULIA_PROJECT を @. に変更することと等価．\n\nCtrl+Dかexit()で終了\nans：直近の evaluated term が格納されて居る．\n\n５つのモードがある．\n\n\n\n\n\n\n\nJuliaモード\nhelp：? で発動．backspace で戻る．使いやすい！\npackage：] で発動．標準では GitHub のリポジトリ上の任意のモジュールがインストール可能．\nshell：; で発動\nsearch"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#pkg",
    "href": "posts/2024/Julia/Julia0.html#pkg",
    "title": "俺のための Julia 入門（０）",
    "section": "3 Pkg",
    "text": "3 Pkg\n]で package mode に入る．\npkg&gt; add &lt;package name&gt;  # 追加\nusing &lt;filename without extension&gt;  # 使用\nhelp  # 使用可能なコマンドの一覧\n\nst (= statusの alias)\n\n現在のプロジェクトがどの config file によるかの確認\n\nデフォルトは Status ~/.julia/environments/v1.5/Project.toml\nこういうデフォルトプロジェクトはユーザー固有になる（ホームディレクトリの下にあるから）\n\n現在インストールされている外部パッケージの状態\n\nrm (= removeの alias)\n\ninstall した package を消去する\n\nup (= updateの alias)\n\n引数無しで用いると，install している全ての package の更新\n\ngc (garbage collection)\n\npackage の追加・削除・消去を繰り返すと不要なデータが蓄積するらしい．これを自動的に削除してくれる．\n\n\n\n3.1 プロット\n\n3.1.1 Plots\n\n\nSoss\n\n\n\n\n3.2 Soss：確率的プログラミング\n\n\n3.3 数値実験4\n\n3.3.1 JLD2：数値実験の結果を保存\n\n\n3.3.2 LaTeXStrings：LaTeX 文字列を扱う\n\n\n\n3.4 DownloadしたPackageのリスト\nstandard library = built-in packagesであるが，Base以外は，addして依存パッケージとして逐一追加する必要がある．これがprojectという観念である．\n\nPkg.jl\n\nJuliaのPackage managerはこのdocumantationを見る．\n\nIJulia.jl\n\nJupyter NotebookをJuliaで使う\nJupyter Notebookは，ウェブブラウザで動作するプログラムの対話型実行環境で，Julia, Python, R, Rubyなどの言語に対応して居る．Jupyter Notebookでは，ノートブックと呼ばれるドキュメントを作成し，その中でプログラムの記述と実行，メモの作成，保存と共有などを行うことができる． using IJulia notebook()\ncondaのJupyter Notebookを使うのか？\n\nPluto.jl\n\n] add Pluto\njulia&gt; import Pluto\njulia&gt; Pluto.run(1234)\nで，ブラウザ上でコードする．\n\nImages.jl\n\nusing Imagesobject name = load(filename::AbstractString)\n\nFileIO.jl\n\nhigh-level support for formatted filesの関数loadとsaveの２つを提供する．\n\nJDF2.jl\n\nJulia objectの保存．\n\nHDF5\n\n大規模改装データの保存用フォーマット（科学技術界の標準）．JDF2はこのサブセット．\n\nJSON.jl\nDataStructure.jl\n\nヒープや優先度付きキューなどの，直積の上に構造をのせたデータ型\n\nAtom\n\nなんかJuliaとAtom相性いいんだよなぁ．\n\nJuno\nQuartzImageIO\n\n画像ファイルのIOするときに必要だった記憶\n\nDistributions\n\nPackage management Config fileとしてはTOML形式を採用している．可読性も高い． * ディクショナリ構造に明確にマッピングされるように設計されている * Tom’s Obvious, Minimal Language * TOMLの構文は、大部分がキーと値の組 1. key = “value” 3. [テーブル名] 4. # コメント * の3種類からなる。\n\nProject.toml\n\n現在のProjectが依存しているpackageを管理する．\n\npackageとしてダウンロードしたdirectoryにあるProject.tomlを読んで，依存パッケージもダウンロードするようになっている．\n\nプロジェクト自体のメタデータや依存パッケージの一覧が収められているテーブル．\n開発者も書き足せる．\n\nproject名など．\n\n[deps] テーブルには，(package名) = “UUID”が納められている．\n\nこういうのは人間が書くものではない．UUID複雑．これをするのがAdd commandの実装に他ならない．\n\n\nManifest.toml\n\n実際にJuliaの実行時に使うべきパッケージの正確なversionやインストール場所を管理するテーブル．\nこのファイルは，package manegerがProject.tomlを参照しながら依存解決して生成する．\n\n\npkg管理モードのコマンド * activate . * &gt; Activating environment at ~/tmp/myproject/Project.toml * Project.tomlを読み込んで，新たなプロジェクトを実行する． * Prompt名が新たなプロジェクト名に更新される． * installされているpackageも刷新される． * これはうまくできているなぁ！ * REPLじゃない時は—project=@.オプションでjuliaから実行する．\nPackage作成\nJulia packageに必要な要素は次のとおり． * README.md * LICENSE * Project.toml * src * source code * Package名と同じ名前の.jlファイル． * test * test code * docs * documentation * deps * パッケージの依存ライブラリやビルドスクリプトを収める * 継続的インテグレーションのための設定ファイル ＊Manifest.tomlは開発環境に特異的で，バージョン管理ツール(GitHub)で共有しないのが一般的．\npkg管理モードのコマンド * generate MyPackage * MyPackageディレクトリを作り，その中にProject.tomlとsrc/MyPackage.jlを作る． * Project.tomlには，UUIDを毎回ランダムに生成し，versionやnameやauthorが登録される． * こうしてつくったMyPackageはすぐにusing MyPackageとMyPackage.greet()で呼べる．"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#footnotes",
    "href": "posts/2024/Julia/Julia0.html#footnotes",
    "title": "俺のための Julia 入門（０）",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuarto – Using Julia↩︎\nJupyter Notebook をインストールしていない場合，install Jupyter via Conda, y/n? [y]: n の確認がなされる．これに y で応えると，Conda.jl パッケージを通じて，Miniconda から最小限の Jupyter 環境がインストールされ，グローバル環境は変わらない．↩︎\n正確には，Project.toml（とManifest.toml）により定義される写像がプロジェクトである．↩︎\nto reproduce weakly informative resampling codes↩︎"
  },
  {
    "objectID": "posts/2024/Julia/MALAwithJulia.html",
    "href": "posts/2024/Julia/MALAwithJulia.html",
    "title": "Metropolis-Hastings サンプラー",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nJulia による MCMC パッケージの概観は次の稿も参照："
  },
  {
    "objectID": "posts/2024/Julia/MALAwithJulia.html#advancedmh.jl",
    "href": "posts/2024/Julia/MALAwithJulia.html#advancedmh.jl",
    "title": "Metropolis-Hastings サンプラー",
    "section": "1 AdvancedMH.jl",
    "text": "1 AdvancedMH.jl\n\n\n\n\n\n\n注（アップデートの問題）\n\n\n\n\n\nTuring をはじめとし，AdvancedMH.jl, AdvancedHMC, MCMCChains, AbstractMCMC などのバージョンが最新版に出来ないことがある．\nSoss, PolyaGammaSamplers などとの相性が特に悪く，これらのいずれかがあると，Turing エコシステムのバージョンが著しく制限されるようである．\nこれらを削除して\n(@v1.10) pkg&gt; add Turing@0.33.1\nとすれば良い．\n\n\n\n\n1.1 例\n\nusing AdvancedMH\nusing Distributions\nusing MCMCChains\nusing ForwardDiff\nusing StructArrays\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\n\n# Define the components of a basic model.\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Use automatic differentiation to compute gradients\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensityCauchy(0.0, 1.0))\n\n# Set up the sampler with a multivariate Gaussian proposal.\nσ² = 0.01\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n#spl = RWMH(MvNormal(zeros(2), I))\n\n# Sample from the posterior.\nchain = sample(model_with_ad, spl, 2000; initial_params=ones(1), chain_type=StructArray, param_names=[\"θ\"])\n\n# plot\nθ_vector = chain.θ\nplot(θ_vector, title=\"Plot of \\$\\\\theta\\$ values\", xlabel=\"Index\", ylabel=\"θ\", legend=false, color=\"#78C2AD\")"
  },
  {
    "objectID": "posts/2024/Julia/MALAwithJulia.html#その他のパッケージ",
    "href": "posts/2024/Julia/MALAwithJulia.html#その他のパッケージ",
    "title": "Metropolis-Hastings サンプラー",
    "section": "2 その他のパッケージ",
    "text": "2 その他のパッケージ\n\n2.1 AdaptiveMCMC.jl\nusing Pkg\nPkg.add(\"AdaptiveMCMC\")\n\n# Taken from https://mvihola.github.io/docs/AdaptiveMCMC.jl/\n\n# Load the package\nusing AdaptiveMCMC\n\n# Define a function which returns log-density values:\nlog_p(x) = -.5*sum(x.^2)\n\n# Run 10k iterations of the Adaptive Metropolis:\nout = adaptive_rwm(zeros(2), log_p, 1_000; algorithm=:am)\n\nusing MCMCChains, StatsPlots # Assuming MCMCChains & StatsPlots are installed...\nc = Chains(out.X[1,:], start=out.params.b, thin=out.params.thin)\np = plot(c)\np\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsavefig(p, \"AdaptiveMCMC.svg\")\n\n\"/Users/hirofumi48/162348.github.io/posts/2024/Julia/AdaptiveMCMC.svg\"\n\n\n\n\n2.2 KissMCMC.jl\nGitHub\nusing Pkg\nPkg.add(\"KissMCMC\")\n\nusing KissMCMC\n# the distribution to sample from,\n1logpdf(x::T) where {T} = x&lt;0 ? -convert(T,Inf) : -x\n# initial point of walker\ntheta0 = 0.5\n\n# Metropolis MCMC sampler:\nsample_prop_normal(theta) = 1.5*randn() + theta # samples the proposal (or jump) distribution\nthetas, accept_ratio = metropolis(logpdf, sample_prop_normal, theta0, niter=10^5)\nprintln(\"Accept ratio Metropolis: $accept_ratio\")\n\n# emcee MCMC sampler:\nthetase, accept_ratioe = emcee(logpdf, make_theta0s(theta0, 0.1, logpdf, 100), niter=10^5)\n# check convergence using integrated autocorrelation\nthetase, accept_ratioe = squash_walkers(thetase, accept_ratioe) # puts all walkers into one\nprintln(\"Accept ratio emcee: $accept_ratio\")\n\nusing Plots\nhistogram(thetas, normalize=true, fillalpha=0.4)\nhistogram!(thetase, normalize=true, fillalpha=0.1)\nplot!(0:0.01:5, map(x-&gt;exp(logpdf(x)[1]), 0:0.01:5), lw=3)\n\n\n1\n\nwhere {T&lt;:Any}の略記が入っている．任意の型を変数と取る（危ない）関数logpdfを定義している．convert(T,Inf)は，値Intを型Tに変換している．これは \\(\\operatorname{Exp}(1)\\) の対数尤度を表す．\n\n\n\n\nAccept ratio Metropolis: 0.40872\nAccept ratio emcee: 0.40872"
  },
  {
    "objectID": "posts/2024/Process/adastan.html",
    "href": "posts/2024/Process/adastan.html",
    "title": "SDE のベイズ推定入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nR の YUIMA パッケージに関する詳細は，次の記事も参照："
  },
  {
    "objectID": "posts/2024/Process/adastan.html#ベイズ推定の実行",
    "href": "posts/2024/Process/adastan.html#ベイズ推定の実行",
    "title": "SDE のベイズ推定入門",
    "section": "1 ベイズ推定の実行",
    "text": "1 ベイズ推定の実行\nベイズ推定には R パッケージ RStan を通じて，確率的プログラミング言語 Stan を用いることを考える．Stan については次の記事も参照：\n\n  \n    \n      \n      \n        Stan 入門\n        Stan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．その方法をまとめます．\n      \n    \n  \n\n具体的には，R において，次のような関数を定義する：\n1adastan &lt;- function(yuima){\n2  excode &lt;- 'data {\n              int N; \n              real x[N+1]; \n              real T; \n              real h; \n            }\n            parameters {\n            '\n\n3  for(i in 1:length(yuima@model@parameter@all)){\n    excode &lt;- paste(excode,\n      \"real&lt;lower=0&gt;\",\n      yuima@model@parameter@all[i], \";\"\n      )\n    }\n\n4  excode &lt;- paste(excode,\"}\")\n\n5  excode &lt;- paste(excode,\n    'model {\n    x[1] ~ normal(0,1);\n    for(n in 2:(N+1)){'\n    )\n\n6  excode &lt;- paste(excode,\n    \"x[n] ~ normal(x[n-1] + h *\", \n    gsub(\"x\", \"x[n-1]\", yuima@model@drift), \n    \",sqrt(h) *\", \n    gsub(\"x\", \"x[n-1]\", yuima@model@diffusion[[1]]),\");\"\n    )  \n\n  excode &lt;- paste(excode,'}}')\n\n\n\n7  sde_dat &lt;- list(N =  yuima@sampling@n,\n    x = as.numeric(sim@data@original.data), \n    T=yuima@sampling@Terminal,\n    h=yuima@sampling@Terminal/yuima@sampling@n)\n\n  fit &lt;- stan(model_code=excode,\n    data = sde_dat, \n    iter = 1000,\n8    chains = 4)\n\n  return(fit)\n}\n\n1\n\nadastan という関数を定義する．この関数は，Yuima パッケージのオブジェクトを引数として受け取り，Stan での推定を行い，その結果を fit オブジェクトとして返す．\n\n2\n\nStan モデルのコード（パラメータ部分は未定）を文字列として excode 変数に格納する．\n\n3\n\nここからが adastan 関数の本体である．Yuima モデルの全てのパラメータについてループを開始して，excode にパラメータの宣言を追加していく．\n\n4\n\nここでついに Stan モデルのパラメータの定義部分が完成する．\n\n5\n\n最後はモデルの定義部分を追加して，Stan モデルのコードが完成する．最初の観測値 x[1] は \\(\\mathrm{N}(0,1)\\) に従う．\n\n6\n\nそれ以降の観測値 x[n] は，前の観測値 x[n-1] に drift 項と diffusion 項を加えたものに従う．これを実装するために，Yuima モデルの drift 項と diffusion 項の定義文を呼び出し，x を x[n-1] に置換することで Stan モデルのコードに埋め込む．\n\n7\n\nStan での推定を実行するために，Yuima モデルのデータを Stan モデルに渡すためのリスト sde_dat を作成する．\n\n8\n\n最後に Stan モデルをコンパイルして実行し，結果を fit オブジェクトとして返す．\n\n\nこれが最初に思いつく，最も直接的な方法かも知れないが，このままではいくつかの問題がある：\n\n\n\n\n\n\n問題点\n\n\n\n\n関数内部で Stan model のコードを文字列として生成していることが苦しい．\n\n\n\nそこで，もっと良い方法を考えたい．\n\n1.1 問題点\n具体例を見てみる．YUIMA を通じて１次元 OU 過程\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nをシミュレーションをするためには，次のようにモデル定義をする：\n\nlibrary(yuima)\nmodel &lt;- setModel(drift = \"theta*(mu-X)\", diffusion = \"sigma\", state.variable = \"X\")\n\nこれだけで，YUIMA は勝手にパラメータを識別してくれる：\n\nstr(model@parameter)\n\nFormal class 'model.parameter' [package \"yuima\"] with 7 slots\n  ..@ all      : chr [1:3] \"theta\" \"mu\" \"sigma\"\n  ..@ common   : chr(0) \n  ..@ diffusion: chr \"sigma\"\n  ..@ drift    : chr [1:2] \"theta\" \"mu\"\n  ..@ jump     : chr(0) \n  ..@ measure  : chr(0) \n  ..@ xinit    : chr(0) \n\n\n\nstr(model@drift)\n\n  expression((theta * (mu - X)))\n\n\n\nstr(model@diffusion)\n\nList of 1\n $ :  expression((sigma))\n\n\nこれを通じて生成される Stan モデル文は\ndata {\n  int N;\n  real x[N+1];\n  real T;\n  real h;\n}\n\nparameters {\n  real&lt;lower=0&gt; theta;\n  real&lt;lower=0&gt; mu;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){\n    x[n] ~ normal(x[n-1] + h * theta * (mu - x[n-1]),\n                  sqrt(h) * sigma);\n  }\n}\nとなるべきである．\nYUIMA の時点で文字列を直接取り扱っており，オブジェクト model がそれを格納しているのだから，もはやこのまま用いることは極めて自然である．\n\n\n1.2 Expression とは\nオブジェクト志向言語ではコード自体もオブジェクトであり，これを R では Expression と呼ぶ．\n\n  \n    \n      \n      \n        R（４）メタプログラミング\n        Expression について\n      \n    \n  \n\n１つのクラスからなるわけではなく，call, symbol, constant, pairlist の４つの型からなる．1\n次のような操作ができる2\nrlang::expr がコンストラクタである：\n\nz &lt;- rlang::expr(y &lt;- x*10)\nz\n\ny &lt;- x * 10\n\n\nexpression オブジェクトは base::eval() で評価できる：\n\nx &lt;- 4\neval(z)\ny\n\n[1] 40\n\n\nexpression には list のようにアクセス可能である：3\n\nf &lt;- expr(f(x = 1, y = 2))\n\nf$z &lt;- 3\nf\n\nf(x = 1, y = 2, z = 3)\n\n\n\nf[[2]] &lt;- NULL\nf\n\nf(y = 2, z = 3)"
  },
  {
    "objectID": "posts/2024/Process/adastan.html#rstan-について",
    "href": "posts/2024/Process/adastan.html#rstan-について",
    "title": "SDE のベイズ推定入門",
    "section": "2 RStan について",
    "text": "2 RStan について\n\n2.1 stan 関数\nRStan パッケージの本体は stan 関数である：\nstan(file, model_name = \"anon_model\", model_code = \"\", fit = NA,\n  data = list(), pars = NA,\n  chains = 4, iter = 2000, warmup = floor(iter/2), thin = 1,\n  init = \"random\", seed = sample.int(.Machine$integer.max, 1),\n  algorithm = c(\"NUTS\", \"HMC\", \"Fixed_param\"),\n  control = NULL, sample_file = NULL, diagnostic_file = NULL,\n  save_dso = TRUE, verbose = FALSE, include = TRUE,\n  cores = getOption(\"mc.cores\", 1L),\n  open_progress = interactive() && !isatty(stdout()) &&\n                  !identical(Sys.getenv(\"RSTUDIO\"), \"1\"),\n  ...,\n  boost_lib = NULL, eigen_lib = NULL\n  )\n重要な引数のみピックアップすると，\n\nmodel_code=\"\"：Stan モデルを定義するコードを，文字列として直接受け渡す．他の方法は file としてファイルへのパスを渡すか，フィット済みのものを fit オブジェクトとして渡すか，の２つのみである．\ndata：\n\n\n\n2.2 例\n前述の OU 過程 [-@#sec-exp-OU]\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nで stan 関数でベイズ推定を実行してみます．\nパラメータは \\[\n\\begin{pmatrix}\\theta\\\\\\mu\\\\\\sigma\\end{pmatrix}\n=\n\\begin{pmatrix}1\\\\0\\\\0.5\\end{pmatrix}\n\\] として YUIMA を用いてシミュレーションをし，そのデータを与えてパラメータが復元できるかをみます．\n\nlibrary(rstan)\nexcode &lt;- \"data {\n            int N;\n            real x[N+1];\n            real T;\n            real h;\n          }\n\n          parameters {\n            real&lt;lower=0&gt; theta;\n            real&lt;lower=0&gt; mu;\n            real&lt;lower=0&gt; sigma;\n          }\n\n          model {\n            x[1] ~ normal(0,1);\n            for(n in 2:(N+1)){\n              x[n] ~ normal(x[n-1] + h * theta * (mu - x[n-1]),\n                            sqrt(h) * sigma);\n            }\n          }\"\n\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(mu = 0,theta = 1, sigma = 0.5), \n                       xinit = 0.02)\nsde_dat &lt;- list(N =  yuima@sampling@n,\n                  x = as.numeric(simulation@data@original.data), \n                  T=yuima@sampling@Terminal,\n                  h=yuima@sampling@Terminal/yuima@sampling@n)\n\n\n# シミュレーション結果\nplot(simulation)\n\n\n\n\n\n\n\n\n\n# ベイズ推定\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nfit &lt;- stan(model_code=excode, data = sde_dat, iter = 1000, chains = 4)\n\nlibrary(\"bayesplot\")\nlibrary(\"rstanarm\")\nlibrary(\"ggplot2\")\n\nposterior &lt;- as.matrix(fit)\nplot_title &lt;- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\n\n\nmcmc_areas(posterior,\n           pars = c(\"theta\", \"mu\", \"sigma\"),\n           prob = 0.8) + plot_title\n\n\n\n\n\n\n\n\n\n\n2.3 調査：他の Stan インターフェース\nbrms や rethinking も，背後で Stan を利用している．これらが文字式をどのように取り扱っているかを調査する．\n\n  \n    \n      \n      \n        R によるベイズ推定入門\n        brms を用いたベイズ回帰分析入門\n      \n    \n  \n\nStan コードを扱っている関数は .stancode() であった．最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\nまず，paste0 を使っていた．\n\n# paste0() の使用例\nresult1 &lt;- paste0(\"Hello\", \"world\")\nprint(result1)  # \"Helloworld\"\n\n[1] \"Helloworld\"\n\n# paste() の使用例\nresult2 &lt;- paste(\"Hello\", \"world\")\nprint(result2)  # \"Hello world\"\n\n[1] \"Hello world\"\n\nresult3 &lt;- paste(\"Hello\", \"world\", sep = \"-\")\nprint(result3)  # \"Hello-world\"\n\n[1] \"Hello-world\""
  },
  {
    "objectID": "posts/2024/Process/adastan.html#footnotes",
    "href": "posts/2024/Process/adastan.html#footnotes",
    "title": "SDE のベイズ推定入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Wickham, 2019) 第17章２節．↩︎\n(Wickham, 2019) 第18章↩︎\n(Wickham, 2019) 第17章２節．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html",
    "href": "posts/2024/Process/Poisson.html",
    "title": "Poisson 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nYUIMAについては次の記事も参照："
  },
  {
    "objectID": "posts/2024/Process/Poisson.html#poisson-過程",
    "href": "posts/2024/Process/Poisson.html#poisson-過程",
    "title": "Poisson 過程を見てみよう",
    "section": "1 Poisson 過程",
    "text": "1 Poisson 過程\n\n1.1 導入\nPoisson 過程と言った場合，Poisson 点過程 \\(\\eta\\) と Poisson 計数過程 \\(N\\) の２つを峻別する必要がある．1\n\n\n\n\n\n\n\\(E=[0,T]\\)，\\(\\lambda&gt;0\\) をレートとした場合，\n\n\\(E\\) 上の（一様な） Poisson 点過程 \\(\\eta\\) とは，\\(X_k\\overset{\\text{iid}}{\\sim}\\mathrm{U}([0,T])\\) を一様確率変数，\\(\\kappa\\sim\\mathrm{Pois}(\\lambda T)\\) を Poisson 確率変数として， \\[\n  \\eta=\\sum_{k=1}^\\kappa\\delta_{X_k}\n  \\] で表せる 確率核 \\(\\Omega\\to E\\) をいう．\n\\(E\\) 上の（一様な） Poisson 計数過程 \\(N\\) とは，点過程 \\(\\eta\\) の \\([0,t]\\) 内の点の数 \\[\n  N_t:=\\#\\left\\{n\\in\\mathbb{N}\\mid X_n\\le t\\right\\}=\\sum_{n=1}^\\infty1_{\\left\\{X_n\\le t\\right\\}}\n  \\] で定まる 確率過程 \\(\\{N_t\\}\\subset\\mathcal{L}(\\Omega)\\) をいう．\n\n\n\n\nこのとき， \\[\n\\eta([0,t])\\sim\\mathrm{Pois}(\\lambda t),\n\\] \\[\nN_t\\sim\\mathrm{Pois}(\\lambda t),\n\\] が成り立ち，\\(N\\) は Lévy 過程になる．\n\\(N\\) は Lévy 過程の中でも，大きさ１の跳躍のみで増加するものとして特徴付けられる．2\n\\(N\\) は，ランダム測度 \\(\\eta\\) が定める \\[\nN_t(\\omega)=\\eta(\\omega,[0,t])=\\int_0^t\\eta(\\omega,ds)\n\\] とも理解できる．\n\n\n1.2 点過程の定義\n\n\n\n\n\n\n定義 (point process)3\n\n\n\n\\((E,\\mathcal{E})\\) を測度空間とする．\\(E\\) 上の 点過程 とは，次の条件を満たす確率核 \\(\\eta:\\Omega\\to E\\) をいう： \\[\n\\eta(\\omega,B)\\in\\mathbb{N}\\cup\\{\\infty\\},\\qquad\\omega\\in\\Omega,B\\in\\mathcal{E}.\n\\]\n\n\n従って，任意の点過程 \\(\\eta\\) に対して，積分 \\[\n(\\eta|f)(\\omega):=\\int_E\\eta(\\omega,dx)f(x)\\in[-\\infty,\\infty]\n\\] が定まる．\n点過程 \\(\\eta\\) が 真の点過程 であるとは，ある \\(E\\)-値確率変数の列 \\(X_1,X_2,\\cdots\\) と，\\(\\mathbb{N}\\cup\\{\\infty\\}\\)-値確率変数 \\(\\kappa\\) が存在して， \\[\n\\eta=\\sum_{n=1}^\\kappa \\delta_{X_n}\\;\\;\\text{a.s.}\n\\] と表せることとする．\n\n\n\n\n\n\n真の点過程になるための条件\n\n\n\n\n\n(Revuz and Yor, 1999, p. 471) 定義XII.1.1 は違う定義を与えている：\n\n\\(E\\) 上の点過程とは，\\(E_\\delta:=E\\cup\\{\\delta\\}\\) 上の確率過程 \\(\\{e_t\\}_{t\\in\\mathbb{R}_+}\\subset\\mathcal{L}(\\Omega;E_\\delta)\\) であって，次の２条件を満たすものをいう：\n\n\\((t,\\omega)\\mapsto e_t(\\omega)\\) は \\(\\mathcal{B}(\\mathbb{R}^+)\\otimes\\mathcal{F}\\)-可測である．\n\\(D_\\omega:=\\left\\{t\\in\\mathbb{R}_+\\mid e_t(\\omega)\\ne\\delta\\right\\}\\) は殆ど確実に可算である．\n\n\n(Kingman, 2006) の定義は「\\(E\\) 内のランダムに決まる可算集合」というもので，これと等価な定義である．この定義は，上で与えた確率核による定義よりも強い（範囲が狭い）ものである．\n確率核の意味での点過程が，追加で次の条件を満たすとき，真の点過程になる．4 特に，(Revuz and Yor, 1999) と (Kingman, 2006) の意味でも点過程である．\n\nある可算な分割 \\(\\sqcup_{n\\in\\mathbb{N}}B_n=E\\) が存在して， \\[\n\\operatorname{P}[\\eta(B_n)&lt;\\infty]=1\n\\] である．\n\n\n\\(E\\) が Polish 空間で，\\(\\eta(\\omega,-)\\) が（殆ど確実に）任意の有界集合上で有限ならば，これを満たす．\n\\(\\sigma\\)-有限な強度測度を持つ Poisson 過程もこれを満たす（第 1.4 節）．\n\n\n\n\n\n\n\n\n\n\n例 (Cox, 1955 過程)5\n\n\n\n\n\n\\(\\xi\\) を確率核 \\(\\Omega\\to E\\) とする．すなわち，可測写像 \\(\\Omega\\to\\mathcal{P}(E)\\) であるとする．このような \\(\\xi\\) をランダム測度ともいう．\nランダム測度 \\(\\xi\\) によって定まる強度測度を持った Poisson 過程 \\(\\eta\\) を，Cox 過程 または二重確率 Poisson 過程という．\n式で表すと，強度測度 \\(\\lambda\\) を持つ Poisson 点過程の分布を \\(\\Pi_\\lambda\\in\\mathcal{P}(E)\\) とすると， \\[\n\\eta|\\xi\\overset{\\text{d}}{=}\\Pi_\\xi\n\\] を満たす点過程 \\(\\eta:\\Omega\\to E\\) をいう．\n\n\n\n\n\n1.3 強度測度\n点過程には「各集合 \\(B\\in\\mathcal{E}\\) に平均何個の点が入るか」を表す 強度測度 \\(\\lambda\\) が定まる．平均測度 とも呼ばれる．\nこの強度測度 \\(\\lambda\\) は \\(\\operatorname{E}[d\\eta]\\) のようなものであり，Fubini の定理のような性質 \\[\n\\operatorname{E}\\left[\\int_E u\\,d\\eta\\right]=\\int_Eu\\operatorname{E}[d\\eta]\n\\] が成り立つ．これを Campbell の公式 という．\n\n\n\n\n\n\n定義 (intensity / mean measure)6\n\n\n\n\\(\\eta\\) を可測空間 \\((E,\\mathcal{E})\\) 上の点過程とする．\\(\\eta\\) の 強度測度 とは，次で定まる測度 \\(\\lambda:\\mathcal{E}\\to[0,\\infty]\\) をいう： \\[\n\\lambda(B):=\\operatorname{E}[\\eta(B)],\\qquad B\\in\\mathcal{E}.\n\\]\n\n\n\n\n\n\n\n\n命題 (Campbell, 1909 の公式)7\n\n\n\n\\(\\eta\\)　を点過程，\\(\\lambda\\) をその強度測度とする．任意の \\(u\\in\\mathcal{L}(E)\\) について，\\((\\eta|u)\\) は確率変数であり，加えて \\(u\\in\\mathcal{L}^1(\\lambda)\\) または \\(u\\ge0\\) である場合，次が成り立つ： \\[\n\\operatorname{E}\\left[\\int_Eu(x)\\eta(dx)\\right]=\\int_Eu(x)\\lambda(dx).\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(u\\) がある \\(B\\in\\mathcal{E}\\) の特性関数 \\(u=1_B\\) である場合， \\[\n\\int_Eu(x)\\eta(dx)=\\eta(B)\n\\] であり，点過程 \\(\\eta\\) の定義からこれは確率変数である．\n期待値の線型性より一般の単関数，単調収束定理より一般の可測関数についても成り立つ．\n\\(u\\in\\mathcal{L}^1(\\lambda)\\) であるとき，\\(u\\) に収束する単関数列 \\[\nu_n:=\\sum_{i=1}^{m_n}a_i^{(n)}1_{B_i^{(n)}}\\nearrow u\n\\] を取ると，Lebesgue の優収束定理より次のように結論づけられる： \\[\\begin{align*}\n    \\operatorname{E}\\left[\\int_Eu(x)\\eta(dx)\\right]&=\\lim_{n\\to\\infty}\\operatorname{E}\\left[\\int_Eu_n(x)\\eta(dx)\\right]\\\\\n    &=\\lim_{n\\to\\infty}\\operatorname{E}\\left[\\sum_{i=1}^{m_n}a_i^{(n)}\\eta(B_i^{(n)})\\right]\\\\\n    &=\\lim_{n\\to\\infty}\\sum_{i=1}^{m_n}a_i^{(n)}\\lambda(B_i^{(n)})\\\\\n    &=\\int_Eu(x)\\lambda(dx)=(\\lambda|u)&lt;\\infty.\n\\end{align*}\\]\n\\(u\\ge0\\) の場合も同様で，Lebesgue の優収束定理が単調収束定理に変わるのみである．\n\n\n\n\n\n1.4 Poisson 点過程の定義\n\n\n\n\n\n\n定義 (Poisson (point) process / random measure)8\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間，\\(\\lambda:\\mathcal{E}\\to[0,\\infty]\\) を測度とする．強度（測度） \\(\\lambda\\) を持つ Poisson 過程 とは，次の２条件を満たす点過程 \\(\\eta\\) をいう：\n\n任意の \\(B\\in\\mathcal{E}\\) に対して，\\(\\eta(B)\\sim\\mathrm{Pois}(\\lambda(B))\\)．\n任意の \\(m\\in\\mathbb{N}\\) と互いに素な可測集合 \\(B_1,\\cdots,B_m\\in\\mathcal{E}\\) について，\\(\\eta(B_1),\\cdots,\\eta(B_m)\\) は独立である．\n\n\n\n２つの強度測度 \\(\\lambda,\\lambda'\\) は \\(\\sigma\\)-有限であるとする．\nこのとき，\\(\\lambda=\\lambda'\\) ならば，これを強度とする Poisson 過程は分布同等である．9\n加えて，\\(\\sigma\\)-有限な強度測度を持つ Poisson 過程は，真の点過程（と同分布）である：\n\n\n\n\n\n\n命題（Poisson 過程が真の点過程となるとき）10\n\n\n\n\\(\\lambda\\) を \\(\\sigma\\)-有限とする．このとき，ある確率変数の列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) と \\(\\mathbb{N}\\cup\\{\\infty\\}\\)-値確率変数 \\(\\kappa\\) が存在して， \\[\n\\eta:=\\sum_{n=1}^\\kappa\\delta_{X_n}\n\\] は強度 \\(\\lambda\\) の Poisson 過程となる．\n\n\n実は Poisson 点過程は，Poisson 分布と同様に，可算な範囲で再生性がある：\\(\\eta^{(1)},\\eta^{(2)},\\cdots\\) を独立な Poisson 点過程とすると， \\[\n\\eta:=\\sum_{k=1}^\\infty\\eta^{(k)}\n\\] も Poisson 点過程であり，強度測度は \\(\\nu:=\\sum_{k=1}^\\infty\\lambda^{(k)}\\) となる．11\n従って，実際は \\(\\sigma\\)-有限な強度測度を持つ Poisson 過程よりもさらに一般的な設定で上の定理が成り立つ．\n\n\n1.5 例\n\n\n\n\n\n\n\n強度測度が有限であるとき，複合二項過程として極めて明瞭な理解ができる．\n区間上の Poisson 過程は，一様分布に関する複合二項過程になる．点の間の間隔は指数分布に従う．12\n\n\n\n\n\n1.5.1 複合二項過程\n\\(\\mu\\in\\mathcal{P}(E),\\pi\\in\\mathcal{P}(\\mathbb{N})\\) とする．\\(X_k\\overset{\\text{iid}}{\\sim}\\mu\\) と \\(\\kappa\\sim\\pi\\) について， \\[\n\\eta:=\\sum_{k=1}^\\kappa\\delta_{X_k}\n\\] は点過程を定める．これを サンプリング分布 \\(\\mu\\) を持った \\(\\pi\\) による 複合二項過程 という．13\nある \\(\\gamma&gt;0\\) に関して \\(\\pi=\\mathrm{Pois}(\\gamma)\\) と取った場合，\\(\\eta\\) は強度 \\(\\gamma\\mu\\) を持った Poisson 過程となる．\n複合二項過程について，次が成り立つ：\n\n\n\n\n\n\n命題（有限な強度を持つ Poisson 過程の特徴付け）14\n\n\n\n\\(\\lambda:\\mathcal{E}\\to(0,\\infty)\\) を有限測度とする．このとき，点過程 \\(\\eta\\) について，次の２条件は同値である：\n\n\\(\\eta\\) は \\(\\lambda\\) を強度測度とする Poisson 過程である．\nサンプリング分布 \\(\\mu:=\\lambda(E)^{-1}\\lambda\\) を持つ \\(\\mathrm{Pois}(\\lambda(E))\\)-複合二項過程である．\n\n特に，\\(\\eta(E)=m\\) で条件づけた分布は，\\(\\delta_{X_1}+\\cdots+\\delta_{X_m}\\) に等しい．\n\n\n\n\n1.5.2 \\(\\mathbb{R}_+\\) 上の Poisson 過程\n\\(\\mathbb{R}_+\\) の Poisson 過程で，強度が \\(\\mathbb{R}_+\\) 上の Lebesgue 測度の定数倍であるものを 一様 Poisson 過程 といい，\\(\\lambda&gt;0\\) を レート ともいう．\n\n\n\n\n\n\n命題（\\(\\mathbb{R}_+\\) 上の Poisson 点過程の特徴付け）15\n\n\n\n\\(\\eta\\) を \\(\\mathbb{R}_+\\) 上の点過程とする．このとき，\\(\\lambda&gt;0\\) に関して次は同値：\n\n\\(\\eta\\) はレート \\(\\lambda\\) を持った一様 Poisson 過程である．\n到着時刻 \\[\nT_n(\\omega):=\\inf\\left\\{t\\ge 0\\mid \\eta(\\omega,[0,t])\\ge n\\right\\}\n\\] の間隔は \\(\\operatorname{Exp}(\\lambda)\\) の独立同分布である．\n\n\n\n\n\n\n1.6 印付き過程\n\n\n\n\n\n\n定義 (\\(K\\)-marked process)16\n\n\n\n\\(\\eta\\) を \\(E\\) 上の真の点過程，\\(K:E\\to F\\) を確率核，\\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega;F)\\) を確率変数列とする．加えて，任意の \\(m\\in\\mathbb{N}\\cup\\{\\infty\\}\\) について，\\(\\kappa=m\\) と \\(X_1,\\cdots,X_m\\) の値で条件づけた \\(Y_i\\) の分布は，もはや \\(X_i\\) の値にしか依らずに独立に定まるとする： \\[\n(Y_n)_{n\\le m}\\,|\\,\\kappa=m,(X_n)_{n\\le m}\\overset{\\text{d}}{=}\\otimes_{n=1}^mK(X_n,-)\n\\] このとき，\\(E\\times F\\) 上の真の点過程 \\[\n\\xi:=\\sum_{n=1}^\\kappa\\delta_{(X_n,Y_n)}\n\\] を \\(X\\) の \\(K\\) で印付けられた過程 または \\(K\\)-付印過程という．\n\n\n\\(\\eta\\) を \\(\\sigma\\)-有限な強度 \\(\\lambda\\) を持つ Poisson 過程とするとき，\\(\\eta\\) の任意の可測写像 \\(T:E\\to F\\) による像は \\(T_*\\lambda\\) を強度とする Poisson 過程となり，\\(\\eta\\) の任意の \\(K\\)-印付き過程もやはり Poisson 過程になる．17\n\\(K:E\\to\\mathcal{P}(F)\\) が定値関数になる場合（すなわち \\(\\{Y_n\\}\\) が \\(X_n\\) の値に依らずに独立同分布に定まる場合），これを 独立 \\(K\\)-付印 という．\n\n\n\n\n\n\n例（降水量の時系列）\n\n\n\n\n\n雨がふるという事象が Poisson 点過程に従い，その際の降水量がランダムに決まるため，降水量の時系列は古くから独立付印 Poisson 過程としてモデル化されている (Todorovic and Yevjevich, 1969)．18\n\n\n\n\n\n\n\n\n\n命題（一様 Poisson 過程の独立付印過程の特徴付け）\n\n\n\n\\[\n\\xi=\\sum_{n=1}^\\infty1_{\\left\\{T_n&lt;\\infty\\right\\}}\\delta_{(T_n,Y_n)}\n\\] を \\(\\mathbb{R}_+\\times E\\) 上の点過程， \\[\n\\eta=\\sum_{n=1}^\\infty1_{\\left\\{T_n&lt;\\infty\\right\\}}\\delta_{T_n}\n\\] を \\(\\mathbb{R}_+\\) 上の点過程とする．\n\\(\\gamma&gt;0\\) と \\(\\mu\\in\\mathcal{P}(E)\\) について，次は同値：\n\n\\(\\xi\\) はレート \\(\\gamma\\) を持つ一様 Poisson 過程 \\(\\eta\\) の \\(\\mu\\)-独立付印である．\n\\(T_1,Y_1,T_2-T_1,Y_2,\\cdots\\) は独立で，\\(T_n-T_{n-1}\\sim\\operatorname{Exp}(\\gamma)\\) かつ \\(Y_n\\overset{\\text{iid}}{\\sim}\\mu\\) である．\n\n\n\n\n\n1.7 剪定\n\n\n\n\n\n\n定義 (\\(p\\)-thinning)19\n\n\n\n\\(p:E\\to[0,1]\\) を可測関数とし，確率核 \\(K:E\\to2\\) を次で定める： \\[\nK_p(x,-):=\\biggr(1-p(x)\\biggl)\\delta_0+p(x)\\delta_1.\n\\]\n\\(E\\) 上の真の点過程 \\(X\\) の，この確率核に関する \\(E\\times2\\) 上の \\(K\\)-印付き過程 \\(M\\) に対して， \\[\n(\\omega,B)\\mapsto M(\\omega,B\\times\\{1\\})\n\\] で定まる \\(E\\) 上の真の点過程を \\(X\\) の \\(p\\)-剪定 という．\n\n\nすなわち，点 \\(x\\in E\\) に定まる所定の確率 \\(p(x)\\) に関して，確率 \\(p(X_n)\\) で点 \\(X_n\\) を脱落させて得る点過程を，\\(p\\)-剪定という．\n\\(p\\)-剪定は強度 \\(p(x)\\lambda(dx)\\) を持つ Poisson 過程となる．加えて，\\(1-p\\)-剪定と違いに独立になる．20\n\n\n\n\n\n\n例（非一様な Poisson 過程のシミュレーション）\n\n\n\n\n\nこの剪定によって，連続なレート関数 \\(\\lambda\\) を持つ非一様な Poisson 過程をシミュレーションする方法が (Lewis and Shedler, 1979) で提案され，(Ogata, 1981) が一般の点過程に拡張した．\n実際，yuimaでもこの方法が採用されている（第 1.9.2 節）．\nアルゴリズムは次のとおりである：\n\n\\(\\lambda\\le\\lambda^*\\) を満たす２つのレート関数を持つ Poisson 過程 \\(\\eta,\\eta^*\\) を考える．\\(\\eta^*([0,x_0])\\) 内の点 \\[\nX_1^*,X_2^*,\\cdots,X_{\\eta^*([0,x_0])}^*\n\\] を生成し，それぞれの点を確率 \\(1-\\frac{\\lambda(X_i^*)}{\\lambda^*(X_i^*)}\\) で取り除く．すると，残った点は \\[\n\\frac{\\lambda(X_i^*)}{\\lambda^*(X_i^*)}\\lambda^*(dx)=\\lambda(dx)\n\\] をレート関数にもつ Poisson 過程の分布，すなわち \\(\\eta\\) の分布に従う．\n\n最も簡単な場合としては，\\(\\lambda^*:=\\max_{t\\in[0,x_0]}\\lambda(t)\\) などと取れば良いが，\\(\\lambda\\) が変動が激しい関数である場合，より効率が良い方法があるかもしれない．\nすると，定値なレート関数を持つ Poisson 過程 \\(\\eta^*\\) は，指数分布に従う確率変数をシミュレーションすることなどによって得られる（命題 1.5.2）．\n\n\n\n\n\n1.8 Poisson 過程に関する積分\n\n1.8.1 直接の積分\n\n\n\n\n\n\n命題（Poisson 点過程との積分が定まるための条件）21\n\n\n\n\\((E,\\mathcal{E},\\lambda)\\) を \\(\\sigma\\)-有限な測度空間，\\(\\eta\\) を \\(E\\) 上の強度測度 \\(\\lambda\\) を持つ Poisson 点過程とする．\nこのとき，\\(f\\in\\mathcal{L}(E)_+\\) について，次は同値：\n\n\\(\\operatorname{P}[(\\eta|f)&lt;\\infty]=1\\)．\n次が成り立つ： \\[\n\\int_E(f\\land1)d\\lambda&lt;\\infty.\n\\]\n\nまた，これらの条件が成り立たないとき，\\(\\operatorname{P}[(\\eta|f)=\\infty]=1\\) が成り立つ．\n\n\n従って，\\(f\\in\\mathcal{L}^1(\\lambda)\\) に関して，\\((\\eta|f)\\) は殆ど確実に有限になる．\n\n\n1.8.2 補過程に関する積分\n加えて，中心化された積分 \\[\nI(f):=(\\eta|f)-(\\lambda|f)\\in L^2(\\operatorname{P})\n\\] は \\(L^1(\\lambda)\\cap L^2(\\lambda)\\) 上の等長作用素で，\\(L^2(\\lambda)\\) 上に有界延長する．22\n\n\n1.8.3 Poisson 積分の分布\n\n\n\n\n\n\n命題（Poisson 積分の分布）23\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間，\\(\\eta\\) を有限な強度測度 \\(\\lambda\\) を持つ Poisson 点過程とする．このとき，\\(\\eta\\) に関する \\(h\\in\\mathcal{L}^1(\\lambda;\\mathbb{R}^d)\\) の積分 \\[\n(\\eta|h)(\\omega)=\\int_Eh(z)\\eta(\\omega,dz)\n\\] の第二キュムラント母関数は，次のように表される： \\[\\begin{align*}\n    \\psi(u)&=\\int_E\\biggr(e^{i(u|h(z))}-1\\biggl)\\lambda(dz)\\\\\n    &=\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|x)}-1\\biggl)(h_*\\lambda)(dx).\n\\end{align*}\\] すなわち，\\((\\eta|h)\\) は複合 Poisson 分布 \\(\\mathrm{CP}(\\|h_*\\lambda\\|_\\mathrm{TV},\\|h_*\\lambda\\|_\\mathrm{TV}^{-1}h_*\\lambda)\\) に従う，平均 \\((\\lambda|h)\\)，分散 \\((\\lambda|h^2)\\) の確率変数である．\n\n\n\n\n\n1.9 Poisson 計数過程のシミュレーション\nyuimaパッケージでは，Poisson 計数過程は複合 Poisson 計数過程の特別な場合として扱うため，シミュレーション法は第 2.5 節で扱い，ここでは結果のみを示す．\n\n1.9.1 一様な Poisson 計数過程\n強度 \\(\\lambda&gt;0\\) を持つ（一様な） Poisson 計数過程とは，\\(\\mathbb{R}_+\\) 上のレート \\(\\lambda&gt;0\\) を持つ一様な Poisson 点過程（第 1.5.2 節）\\(\\eta\\) に対して， \\[\nN_t(\\omega):=\\eta(\\omega,[0,t])\n\\] で定まる Lévy 過程である．\nレート \\(\\lambda&gt;0\\) はジャンプの頻度を表している：\n\n\n\n\n\n\n\n\n\n\n\n1.9.2 非一様な Poisson 計数過程\n強度関数 \\(\\lambda:\\mathbb{R}_+\\to\\mathbb{R}^+\\) を持つ 非一様な Poisson 計数過程 とは，全く同様な定義 \\[\nN_t(\\omega):=\\eta(\\omega,[0,t])\n\\] をし，ただ \\(\\eta\\) の強度測度を \\(\\lambda(t)\\ell_+(dt)\\) に置き換えたものである．\n例えば，強度関数 \\[\n\\lambda(t)=10e^{-\\frac{t}{5}}\n\\] を持つ非一様な Poisson 過程は次のような見本道を持つ：\n\n\n\n\n\n\n\n\n\n時間が経つごとに強度関数 \\(\\lambda\\) の値（赤線）が小さくなり，それに伴ってジャンプの頻度が減少していくことがわかる．"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html#複合-poisson-過程",
    "href": "posts/2024/Process/Poisson.html#複合-poisson-過程",
    "title": "Poisson 過程を見てみよう",
    "section": "2 複合 Poisson 過程",
    "text": "2 複合 Poisson 過程\n\n2.1 導入\n点過程としての複合 Poisson 過程は，印付けられた Poisson 点過程（第 1.6 節）から構成される．\n\n2.1.1 計数過程として\nPoisson 過程 \\(N\\) は，大きさ１の跳躍のみで増加する Lévy 過程として特徴付けられるのであった．\nこの跳躍の大きさを任意の確率分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R})\\) に従ったものに変更したもの \\[\\begin{align*}\n    M_t&:=\\sum_{n=1}^{N_t}Y_n\\\\\n    &=\\sum_{k=1}^\\kappa Y_k1_{\\left\\{X_k\\le t\\right\\}}\n\\end{align*}\\] が 複合 Poisson （計数）過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) である．24\n\\(\\mu=\\delta_1\\) の場合が Poisson 過程に当たる．25\n\n\n\n\n\n\n注（従属操作による見方）26\n\n\n\n\n\n\\(Y_n\\overset{\\text{iid}}{\\sim}\\mu\\) について，これが定めるランダムウォーク \\[\nS_n:=\\sum_{k=1}^nY_k\n\\] を 再起過程 (renewal process) という．27\n\\(\\mu=\\operatorname{Exp}(\\lambda)\\) の場合の再起過程が定める計数過程 \\[\nN_t:=\\sum_{n=0}^\\infty1_{[0,t]}(S_n)\n\\] が Poisson 過程なのであった．\nこれに対して，\\(N_t\\) を従属過程とした従属 \\(M_t:=S_{N_t}\\) が複合 Poisson 過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) と理解できる．\n\n\n\n\n\n2.1.2 点過程として\nこの複合 Poisson 過程は，印付けられた Poisson 過程 \\[\n\\eta:=\\sum_{n=1}^\\kappa\\delta_{(X_n,Y_n)}\n\\] が \\(E\\times\\mathbb{R}\\) 上の強度測度 \\(\\mu\\otimes\\lambda\\ell\\) を持つ Poisson 点過程であり，各 \\(X_n\\) に \\(Y_n\\) の重みをつけて足し合わせた点過程 \\[\n\\xi(\\omega,B):=\\int_{B\\times\\mathbb{R}}r\\,\\eta(\\omega,dydr)\n\\] が基になっている．\nこれが複合 Poisson 点過程であり，\\(\\xi\\) に関する積分として \\(M\\) が理解できる（第 1.8.1 節参照）： \\[\nM_t(\\omega)=\\xi(\\omega,[0,t])=\\int^t_0\\xi(\\omega,ds).\n\\]\n\n\n\n\n\n\n注（Lévy 過程としての複合 Poisson 過程）28\n\n\n\n\n\n一様 Poisson 過程は定数の Lévy 測度 \\(\\lambda&gt;0\\) を持つ，特性量 \\((0,0,\\lambda\\delta_1)\\) が定める Lévy 過程である．\n一方で，一様な複合 Poisson 過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) は特性量 \\((0,0,\\lambda\\mu)\\) を持つ Lévy 過程である．\n従って，Lévy 測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}\\) はジャンプの頻度，\\(\\|\\nu\\|_\\mathrm{TV}^{-1}\\nu\\) がジャンプの分布を表すと言える．\nしかし，一般に Lévy 測度は，\\(\\left\\{z\\in\\mathbb{R}^d\\mid\\lvert z\\rvert&gt;\\epsilon\\right\\}\\) 上では有限であっても，\\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上で有限とは限らない．\\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上でも有限である場合，跳躍部分は定義 2.4.1 の意味でも複合 Poisson 過程である．\nすなわち，有限時区間内では（殆ど確実に）有限回しかジャンプしない純粋跳躍 Lévy 過程は，全て一様な複合 Poisson 計数過程である．29\n一般の Lévy 測度を持つ Lévy 過程は，一様な複合 Poisson 部分を跳躍部分にもつ Lévy 過程の，広義 \\(\\mathbb{H}^2\\)-収束極限として表される．30\n\n\n\n\n\n2.1.3 複合 Poisson 点過程の普遍性\n違いに素な \\(A,B\\in\\mathcal{E}\\) に対して \\(\\xi(A)\\perp\\!\\!\\!\\perp\\xi(B)\\) を満たすようなランダム測度 \\(\\xi\\) は，固定した原子を持たないならば，ある決定論的な測度と複合 Poisson 点過程の和として表せる．31\n\n\n2.1.4 点過程と計数過程の峻別\nここでも \\(\\xi\\) と \\(M\\) は全く異なる数学的対称であり，区別を要する．\n特に，\\(\\xi\\) は確率核，\\(M\\) は Lévy 過程である．\nしかし，Lévy 過程に関する確率積分を定義する際，２つの概念は密接に関連する．\n\n\n\n\n\n\nLévy 過程が駆動する確率積分\n\n\n\n\n\nというのも，\\(\\xi\\) の背後には，拡張された空間 \\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程 \\(\\eta\\) があり，\\(M\\) と \\(\\eta\\) が密接に関連するのである．\n\\(M\\) に関する確率積分は \\[\ndM_t=\\eta(dzdt)\n\\] と理解でき，\\(\\eta\\) のランダム測度としての理解が活躍する．\n\\(\\eta\\) の平均測度は \\(\\nu\\otimes\\ell_+\\) と表され，この \\(\\nu\\) を Lévy 測度という．\nすなわち，Lévy 過程に関する確率積分とは，Lévy 過程に付随する Poisson 点過程に関する確率積分に他ならない．\n\n\n\n\n\n\n2.2 複合 Poisson 点過程の定義\n\n2.2.1 一般的な定義\n\n\n\n\n\n\n定義 (compound Poisson process)32\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間とする．\\(E\\) 上の 複合 Poisson 分布 とは，\\(E\\times(\\mathbb{R}\\setminus\\{0\\})\\) 上の Poisson 過程 \\(\\eta\\) を用いて \\[\n\\xi(\\omega,B)=\\int_{B\\times\\mathbb{R}\\setminus\\{0\\}}r\\,\\eta(\\omega,dydr),\n\\] \\[\nB\\in\\mathcal{E},\n\\] と表せる確率核 \\(\\xi:\\Omega\\to E\\) をいう．\n\n\n\\(E\\times\\mathbb{R}\\setminus\\{0\\}\\) 上の Poisson 点過程 \\(\\eta\\) の第二成分が，\\(E\\) 上のランダム点の「重み」のようなもので，\\(\\xi\\) は重みをつけて点を積分したものと理解できる．これは一般化された Poisson 複合の手続きに思える．\n\n\n\n\n\n\n\\(\\mathbb{R}^d\\) 上の複合 Poisson 分布\n\n\n\n\n\n上述の定義を，\\(\\mathbb{R}^d\\) 上の分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の Poisson 複合 \\(\\mathrm{CP}(\\lambda,\\mu)\\in\\mathcal{P}(\\mathbb{R}^d)\\) と比較してみよう．\nこれは \\[\nX:=\\sum_{n=1}^NZ_n,\\qquad Z_n\\overset{\\text{iid}}{\\sim}\\mu,N\\sim\\mathrm{Pois}(\\lambda)\n\\] で定まる \\(X\\) の分布であり，特性関数は \\[\n\\varphi_X(u)=\\exp\\left(\\lambda\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|y)}-1\\biggl)\\mu(dy)\\right)\n\\] で与えられる．\n複合 Poisson 過程はこれを過程に一般化したものであり，Poisson 複合の部分は \\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程の積分として一般化されている．\n\n\n\n\n\n2.2.2 ジャンプ時刻とジャンプ幅が独立に決まる場合\n第 1.6 節で扱ったような，（一様とは限らない）Poisson 点過程の独立付印の場合が特に重要なクラスである．\nこれは，Lévy 過程のジャンプ測度が，このクラスの複合 Poisson 点過程になるためである．33\n\n\n\n\n\n\n命題 (\\(\\rho_0\\)-symmetric compound Poisson process with Lévy measure \\(\\nu\\))34\n\n\n\n\\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程 \\(\\eta\\) の強度測度が \\(\\lambda=\\rho_0\\otimes\\nu\\) で表せ，さらに次を満たすとする：\n\n\\(\\rho_0\\in\\mathcal{P}(E)\\) は \\(\\sigma\\)-有限．\n\\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) は \\[\n\\int_0^\\infty(r\\land1)\\nu(dr)&lt;\\infty\n\\] を満たす．\n\nこのとき，次が成り立つ：\n\n\\(\\xi\\) は \\(\\rho_0\\)-対称である：\\(\\rho_0(B)=\\rho_0(B')\\) ならば，\\(\\xi(B)\\overset{\\text{d}}{=}\\xi(B')\\)．\n任意の \\(\\epsilon&gt;0\\) について \\(\\nu([\\epsilon,\\infty))&lt;\\infty\\)．\n\n\n\n\n\n\n\n\n\n例（Lévy 過程の跳躍測度）35\n\n\n\n\n\nLévy 測度 \\(\\nu\\) を持つ Lévy 過程 \\((L_t)\\) を考える．\nこのとき \\(\\nu\\) は，\\(\\nu(\\{0\\})=0\\) で， \\[\n\\int_\\mathbb{R}(1\\land\\lvert z\\rvert^2)\\nu(dz)&lt;\\infty\n\\] を満たすとされる．\n\\[\n\\mathcal{A}_\\nu:=\\left\\{A\\in\\mathcal{B}(\\mathbb{R}\\setminus\\{0\\})\\mid \\nu(A)&lt;\\infty\\right\\}\n\\] に対して， \\[\nN([s,t]\\times A):=\\sum_{s\\le r\\le t}1_A(\\Delta L_r)\n\\] とすると，時刻 \\([s,t]\\) 間の，幅が \\(A\\in\\mathcal{A}_\\nu\\) に入るジャンプの数を表す確率変数となる．\nこれは \\(\\mathbb{R}_+\\times\\mathbb{R}\\setminus\\{0\\}\\) 上の Poisson 点過程に延長し，強度 \\(\\ell_+\\otimes\\nu\\) を持つ．\nすなわち，\\(N\\) は Lévy 測度 \\(\\nu\\) を持つ \\(\\ell_+\\)-対称な複合 Poisson 点過程である．この \\(N\\) を 跳躍測度 (jump measure) ともいう．36\nLévy 測度 \\(\\nu\\) は，単位時間あたり（\\(\\ell_+\\) で測って測度 \\(1\\) の集合あたり）の平均ジャンプ数を表す： \\[\n\\nu(A)=\\operatorname{E}[N([0,1]\\times A)].\n\\]\nなお，一般の加法過程（非一様 Lévy 過程）に対しても，同様に跳躍測度は \\((\\mathbb{R}^d\\setminus\\{0\\})\\times\\mathbb{R}^+\\) 上の Poisson 点過程になる．37\n\n\n\n\n\n\n2.3 複合 Poisson 点過程に関する積分\n\\(E\\) 上の複合 Poisson 点過程 \\(\\xi\\) とは，\\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程 \\(\\eta\\) に他ならないため，Poisson 点過程に関する積分を通じて，\\(f\\in\\mathcal{L}(E)\\) に関して \\[\n\\int_Ef(z)\\,\\xi(dz)=\\int_Erf(y)\\,\\eta(dydr)\n\\] と定義できる．38\nCampbell の定理 1.3 より，\\((r,z)\\mapsto rf(z)\\) が \\(\\eta\\) の強度測度 \\(\\lambda\\) に関して可積分ならば，右辺は可積分であるから，\\((\\xi|f)\\) も可積分な確率変数を定める．\n\n\n\n\n\n\n例（Lévy 過程の Lévy-Itô 分解）39\n\n\n\n\n\nLévy 過程（一般に加法過程）は，跳躍部分と連続部分との独立和に分解でき，これが Lévy-Itô 分解である．\n跳躍部分は，複合 Poisson 点過程 \\(\\xi\\) （跳躍測度）に関する積分として表せる．\nしかし，Lévy 測度は \\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上で有限であるのみで，\\(0\\) の近傍で発散する場合がある．これにより，小さなジャンプを繰り返し，純粋なジャンプの和は発散することがある．そのために，\\(0\\) の近傍での跳躍部分については，連続部分から項を借りて，中心化された Poisson 積分（第 1.8 節）に関して表示する必要がある．40\nこれを実際に見てみよう．\\(L\\) を特性量 \\((\\beta,\\alpha^2,\\nu)\\) を持つ Lévy 過程，\\(\\eta\\) を跳躍測度とする．\n\n強度測度 \\(\\lambda\\) について， \\[\n   \\int_{\\mathbb{R}^d}z\\,\\lambda(dz)\n   \\] は \\(0\\) の近傍で発散し得るため，単に \\[\n   \\int^t_0\\int_{\\mathbb{R}^d}z\\,\\eta(dsdz)\n   \\] として跳躍部分を表そうとしても，well-defined とは限らない．\nしかし Lévy 測度は，\\(B^d\\subset\\mathbb{R}^d\\) を単位閉球として， \\[\n     \\int_{B^d}z^2\\,\\nu(dz)&lt;\\infty\n     \\] は満たすから，中心化された Poisson 積分（第 1.8.2 節） \\[\n     \\int^t_0\\int_{B^d}z\\,\\widehat{\\eta}(dsdz)&lt;\\infty\n     \\] は定義され，\\(\\mathcal{L}^2(\\operatorname{P})\\) の元である：補過程に関する Poisson 積分の等長性より，41 \\[\\begin{align*}\n         \\operatorname{E}\\left[\\left(\\int^t_0\\int_{B^d}z\\,\\widehat{\\eta}(dsdz)\\right)^2\\right]&=\\int^t_0\\int_{B^d}z^2\\,dt\\lambda(dz)\\\\\n         &=t\\int_{B^d}z^2\\,\\lambda(dz)&lt;\\infty.\n     \\end{align*}\\]\n一方で \\(\\mathbb{R}^d\\setminus B^d\\) 上では， \\[\n   \\int_{\\mathbb{R}^d\\setminus B^d}(z\\land1)\\,\\nu(dz)=\\nu(\\mathbb{R}^d\\setminus B^d)&lt;\\infty\n   \\] であるから，単に \\(\\eta\\) に関して積分しただけでも，殆ど確実に有限な値を持つ（第 1.8.1 節）．\n以上より，任意の Lévy 過程 \\(L\\) は次の分解として常に表示できる： \\[\n     L_t=\\beta t+\\alpha B_t+\\int^t_0\\int_{B^d}z\\widehat{\\eta}(dsdz)+\\int^t_0\\int_{\\left\\{\\lvert z\\rvert&gt;1\\right\\}}z\\eta(dsdz).\n     \\] このとき，\\(B\\perp\\!\\!\\!\\perp N\\) であり，最後の項は有限和としても表せる： \\[\n     \\int^t_0\\int_{\\mathbb{R}^d\\setminus B^d}zN(dsdz)=\\sum_{0\\le r\\le t}\\Delta L_r1_{\\mathbb{R}^d\\setminus B^d}(\\Delta L_r).\n     \\] この項が有限であることは，Lévy 測度 \\(\\nu\\) が \\(\\mathbb{R}^d\\setminus B^d\\) 上で有限であることによる．\n追加で Lévy 測度 \\(\\nu\\) が \\[\n   \\int_{B^d}\\lvert z\\rvert\\,\\nu(dz)&lt;\\infty\n   \\] も満たすような Lévy 過程（一般には加法過程）\\(L\\) については，跳躍部分は \\[\n   L^{\\text{jump}}_t:=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}z\\,\\eta(dsdz)\n   \\] と表せ，\\(L-L^{\\text{jump}}\\) は連続過程で，\\(L^{\\text{jump}}\\) と独立である．42 このような分解ができるとき，\\(L^{\\text{jump}}\\) を 跳躍部分，残りを 連続部分 という．\n\nすなわち，Lévy 過程の中には，無数の小さな跳躍部分が連続部分が相殺しているために収束しているものがあり，そのような場合は明確に跳躍部分と連続部分に分離できないものがある．\n\n\n\n\n\n\n\n\n\n例（ショット雑音）43\n\n\n\n\n\nある \\(\\widetilde{k}\\in\\mathcal{L}^1(\\ell_d)\\) が定める核関数 \\[\nk(x,y):=\\widetilde{k}(x-y)\n\\] と，ある平均を持つ測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R})\\) が定める強度測度 \\(\\lambda:=\\ell_d\\otimes\\nu\\) を持つ \\(\\mathbb{R}^d\\times\\mathbb{R}\\) 上の Poisson 点過程 \\(\\eta\\) に関して， \\[\nY_x:=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}}r\\widetilde{k}(x-y)\\eta(dydr)\n\\] で定まる確率場 \\(\\{Y_x\\}_{x\\in\\mathbb{R}^d}\\) を，Poisson 過程 \\(\\eta\\) が駆動する ショット雑音 という．\n\n\n\n\n\n2.4 複合 Poisson 計数過程の定義\n単に複合 Poisson 過程といった場合，通常，ここでいう一様な複合 Poisson 計数過程を指すことが多い．\n\n2.4.1 一様な場合\n\n\n\n\n\n\n定義 (compound Poisson processes)44\n\n\n\n\\(\\lambda&gt;0\\) を正数，\\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) を確率分布とする．一様な 複合 Poisson 過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) とは，強度 \\(\\lambda\\) を持つ（一様な） Poisson 過程 \\(N\\) が定める Lévy 過程 \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) をいう： \\[\nX_t:=\\sum_{n=1}^{N_t}Y_n,\n\\] \\[\nY_n\\overset{\\text{iid}}{\\sim}\\mu.\n\\]\n\n\nこの \\(X_t\\) は Lévy 過程になっており，\\(X_t\\) の特性関数は \\[\n\\varphi(u)=\\exp\\left(\\lambda t\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|y)}-1\\biggl)\\mu(dy)\\right)\n\\] で表される．45 すなわち，\\(X_t\\) は複合 Poisson 分布 \\(\\mathrm{CP}(\\lambda t,\\mu)\\) に従う．\n\n\n2.4.2 非一様・非有限な場合\nすなわち，複合 Poisson 分布が一様であるとは，背後にある \\(\\mathbb{R}^d\\times\\mathbb{R}_+\\) 上の Poisson 点過程が，\\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程（第 1.5.2 節）の独立付印過程になっていることをいう（第 1.6 節）．\nさらに，Lévy 測度が \\(\\lambda\\mu\\) という有限測度になるという重要な仮定も含まれている．\n一方で，一般の Lévy 分布の跳躍測度は，\\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程に基づくとは限らない（第 2.2.2 節）上に，\\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上有限とも限らない．\nこのような場合でも，一般の複合 Poisson 点過程 \\(\\xi\\) を通じて，複合 Poisson 計数過程が \\[\nM_t(\\omega):=\\xi(\\omega,[0,t])\n\\] と定義できる．\n\\(\\xi\\) が \\(\\mathbb{R}_+\\times\\mathbb{R}^d\\) 上の Poisson 点過程 \\(\\eta\\) が定めるものであるとすると， \\[\nM_t(\\omega)=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}r\\,\\eta(\\omega,dsdr)\n\\] と表せる．\n\n\n\n2.5 シミュレーション\n\n2.5.1 複合 Poisson 過程のシミュレーション\nyuimaパッケージには，複合 Poisson 分布専用のコンストラクタsetPoissonが用意されている．このコンストラクタは２つの引数を持つ：\n\n\n\n\n\n\nsetPoissonの引数46\n\n\n\n\nintensity：強度（関数） \\(\\lambda&gt;0\\) として用いる変数名を文字列で指定する．\ndf：跳躍測度 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) として用いる分布名を，文字列のリストで指定する．\nscale=1：\\(\\|\\nu\\|_\\mathrm{TV}\\) の値のこと．\ndimension=1：\\(d\\) の値．\n\n\n\n\n\n\n\n\n\n例（一様 Poisson 過程の例）47\n\n\n\n\n\nPoisson 過程はジャンプの大きさが定数１の複合 Poisson 過程であるから，dfとしては \\(\\delta_1\\) を意味するdconst(z,1)を指定する．\n\nlibrary(yuima)\n\nmod1 &lt;- setPoisson(intensity=\"lambda\", df=list(\"dconst(z,1)\"))\n\n\nTerminal &lt;- 30\nsamp &lt;- setSampling(Terminal=Terminal, n=3000)\nset.seed(123)\npoisson1 &lt;- simulate(mod1, true.par=list(lambda=1), sampling=samp)\n\n\nplot(poisson1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n例（正規分布に従う跳躍を持つ複合 Poisson 過程）48\n\n\n\n\n\ndfとして正規分布族dnorm(z,mu,sigma)を指定すれば良い．\n\nmod2 &lt;- setPoisson(intensity=\"lambda\", df=list(\"dnorm(z,mu,sigma)\"))\npoisson2 &lt;- simulate(mod2, sampling=samp, true.par=list(lambda=1, mu=0, sigma=2))\n\n跳躍分布が \\(\\mu=\\mathrm{N}_1(0,2)\\) である場合の複合 Poisson 過程をシミュレーションは次のとおり：\n\nplot(poisson2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 複合 Poisson 過程が駆動する Lévy 過程のシミュレーション\nこの場合はsetModel内でモデルを定義する．\n\n\n\n\n\n\nsetModelの引数49\n\n\n\n\njump.coeff：\\(dM_t\\) 項の係数を，文字列のベクトルで指定する．\nmeasure.type：CPと指定することで，measure引数が複合 Poisson 過程 \\(M\\) に付随する Poisson 点過程 \\(\\eta\\) の強度測度と解釈される．\nmeasure：\\(\\eta\\) の強度測度 \\(\\lambda\\otimes\\nu\\) を，intensityとdfのリストとして指定する．\n\n\n\n\n\n\n\n\n\n例（複合 Poisson 跳躍を持つ Lévy 過程）50\n\n\n\n\n\n複合 Poisson 過程 \\(M\\sim\\mathrm{CP}(\\lambda,\\mathrm{N}(2,0.1))\\) が定める SDE \\[\ndX_t=-\\theta X_tdt+\\sigma dB_t+\\left(\\gamma+\\frac{X_{t-}}{\\sqrt{1+X^2_{t-}}}\\right)dM_t\n\\] \\[\nX_0=0\n\\] は次のように定義する：\n\nmodJump &lt;- setModel(drift = c(\"-theta*x\"), diffusion = \"sigma\", jump.coeff=c(\"gamma+x/sqrt(1+x^2)\"), measure = list(intensity=\"lambda\", df=list(\"dnorm(z, -3, 1)\")), measure.type=\"CP\", solve.variable=\"x\")\n\nsamp &lt;- setSampling(n=10000,Terminal=10)\n\nX &lt;- simulate(modJump, xinit=2, sampling=samp, true.par= list(theta=2, sigma=0.5,gamma=0.3,lambda=0.5))\nplot(X)"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html#footnotes",
    "href": "posts/2024/Process/Poisson.html#footnotes",
    "title": "Poisson 過程を見てみよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Nualart and Nualart, 2018, p. 159) 例9.1.3，(Revuz and Yor, 1999, p. 471) 定義XII.1.3 などは，Poisson 計数過程の方を Poisson 過程と呼んでおり，(Last and Penrose, 2017) などは Poisson 点過程の方を Poisson 過程と呼んでいる．(Eberle, 2012, p. 18) では，ここでは点過程とランダム測度を混用しているが，この２つを使い分けている．(Vasdekis, 2021) も，ここでいう計数過程を点過程と呼んでいる．↩︎\n(Revuz and Yor, 1999, p. 472) 命題XII.1.4など．↩︎\n(Kallenberg, 2017, p. 49)，(Last and Penrose, 2017, p. 11) 定義2.1 に倣った．全く同じ概念を (伊藤清, 1991, p. 298) は 偶然配置 と呼ぶ．↩︎\n(Last and Penrose, 2017, p. 48) 系6.5 を参照．↩︎\n(Last and Penrose, 2017, p. 129) 定義13.5 に倣った．↩︎\n(伊藤清, 1991, p. 298) に倣った．↩︎\n(Last and Penrose, 2017, p. 13) 命題2.7 に倣った．(Campbell, 1909) は元々真空管内の shot noise の研究をしていた．結果の一部は G. H. Hardy にもよるという．↩︎\n(Nualart and Nualart, 2018, p. 160) 定義9.2.1，(Sato, 2013, p. 119) 第4章19節，(Resnick, 2002, p. 303)，(Sato, 2013, p. 119) 定義19.1，(Last and Penrose, 2017, p. 19) 定義3.1 に倣った．↩︎\n(Revuz and Yor, 1999, p. 476) 命題 XII.1.12，(Last and Penrose, 2017, p. 20) 命題3.2 はより一般的な状況で示している．↩︎\n(Nualart and Nualart, 2018, p. 160) 定理9.2.2 では，\\(\\lambda\\) はアトムを持たないとし，\\(E\\) を Polish 空間として示している．(Last and Penrose, 2017, p. 22) 系3.7 はここよりも一般的な設定で示している．↩︎\n(Eberle, 2012, p. 19) 定理1.6も参照．↩︎\n逆に，指数分布のシミュレーションにより，一様分布の順序統計量が効率的にシミュレーションできる．このことは粒子フィルターにおけるリサンプリングに応用できる．(Chopin and Papaspiliopoulos, 2020, p. 113) 命題9.1も参照．↩︎\n\\(\\pi=\\delta_m\\) であるとき，\\(\\eta\\) を単に二項（点）過程という．これは \\(\\eta(B)\\sim\\mathrm{Bin}(m,\\mu(B))\\) が成り立つためである．(Last and Penrose, 2017, p. 20) 定義3.4も参照．↩︎\n(Last and Penrose, 2017, p. 23) 命題3.8も参照．↩︎\n(Last and Penrose, 2017, p. 59) 定理7.2参照．↩︎\n(Last and Penrose, 2017, p. 40) 定義5.3 に倣った．(Kingman, 1992, p. 55) 5.2節も参照．↩︎\n例えば (Kingman, 1992, p. 55)，(Last and Penrose, 2017, p. 42) 命題5.5 を参照．↩︎\n日本語文献としては，(西村克己 and 江藤剛治, 1981) も参照．↩︎\n(Last and Penrose, 2017, p. 43) 定義5.7 に従った．↩︎\n(Last and Penrose, 2017, p. 44) 系5.9 参照．↩︎\n(Last and Penrose, 2017, p. 111) 命題12.1 参考．↩︎\n(Nualart and Nualart, 2018, p. 163) 9.3節，(Last and Penrose, 2017, p. 113) 命題12.4 など．↩︎\n(Sato, 2013, p. 123) 定理19.5 や (Eberle, 2012, p. 20) 定理1.7 は強度測度 \\(\\lambda\\) が有限な場合について述べている，(Nualart and Nualart, 2018, p. 164) は補過程に関する積分に関して述べており，そのために追加で\\(-iu\\int_Eh(z)\\lambda(dz)\\) の項を持つ．．↩︎\n(Nualart and Nualart, 2018, p. 159) 例9.1.4，(Last and Penrose, 2017, p. 155) 例15.5 など．↩︎\n(Applebaum, 2009, p. 50) も参照．↩︎\n(Iacus and Yoshida, 2018, p. 171) 4.8.1 節も参照．↩︎\n(Resnick, 2002, p. 174)，(Mitov and Omey, 2014, p. 1)，(Nummelin, 1984, p. 49) 定義4.2 などに倣った．↩︎\n(Nualart and Nualart, 2018, pp. 159–160) も参照．↩︎\n\\(\\left\\{z\\in\\mathbb{R}^d\\mid\\lvert z\\rvert&gt;\\epsilon\\right\\}\\) 上で有限でないと，これを Lévy 測度にもつ Lévy 過程は存在しない．(Eberle, 2012, p. 20) 定理1.7も参照．↩︎\n(Eberle, 2012, p. 26) 定理1.10 も参照．↩︎\n(Kingman, 1992, p. 79) 第８章，(Last and Penrose, 2017) 第 15.3 節参照．↩︎\n(Last and Penrose, 2017, p. 153) に倣った．↩︎\n一様じゃない Lévy 過程，すなわち加法過程の場合は，\\(\\lambda((0,t]\\times B)=\\nu_t(B)\\) という関係にある．これはもはや独立付印ではないが，やはり印付きの Poisson 過程が肝心であることは変わらない．↩︎\n(Last and Penrose, 2017, p. 155) も参照，↩︎\n(Nualart and Nualart, 2018, p. 162) 例9.2.4 を参考．↩︎\n(Nualart and Nualart, 2018, p. 162) 例9.2.4 による用語法．\\(N\\) をランダム測度として跳躍測度と呼んでいる．↩︎\n(Sato, 2013, p. 120) 定理19.2(i) を参照．↩︎\n(Nualart and Nualart, 2018, p. 164) 例9.3.1，(Last and Penrose, 2017, p. 161) 15.4節なども参照．↩︎\n(Nualart and Nualart, 2018, p. 164) 例9.3.1，(Sato, 2013, p. 120) 定理19.2，(Protter, 2005, p. 31) 定理42 など参照．↩︎\n(Sato, 2013, p. 119) が指摘するように，まるで Cauchy の主値積分である．↩︎\nただし，\\(\\widehat{\\eta}([0,t]\\times A):=\\eta([0,t]\\times A)-t\\nu(A)\\) を中心化した Poisson 過程とした．これを Poisson 補過程 (compensated Poisson process) ともいう．↩︎\n(Sato, 2013, p. 121) 定理19.3 も参照．↩︎\n(Last and Penrose, 2017, p. 162) 例15.14 を参考にした．↩︎\n(Iacus and Yoshida, 2018, p. 137), p.158，(Sato, 2013, p. 18) 定理4.3，(Protter, 2005, p. 33) 例2，(Baudoin, 2014, p. 90) 演習3.44，(Applebaum, 2009, p. 49) 命題1.3.11 に倣った．↩︎\n(Sato, 2013, p. 20) 命題4.5など参照．↩︎\n(Iacus and Yoshida, 2018, p. 137) 参照．↩︎\n(Iacus and Yoshida, 2018, pp. 137–138) も参照．↩︎\n(Iacus and Yoshida, 2018, p. 139) も参照．↩︎\nLévy 過程が駆動する SDE モデルの定義方法は (Iacus and Yoshida, 2018, p. 191) 4.11.3 節を参照．↩︎\n(Iacus and Yoshida, 2018, p. 191) 参照．↩︎"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html",
    "href": "posts/2024/Process/PureJump.html",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#sec-pure-jump",
    "href": "posts/2024/Process/PureJump.html#sec-pure-jump",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "1 純粋跳躍過程",
    "text": "1 純粋跳躍過程\n\n1.1 導入\n\n\n\n\n\n\n次の生成作用素はどのような Markov 過程に対応するか？\n\n\n\n\\(E\\) を距離空間，\\(\\mu:E\\times\\mathcal{B}(E)\\to[0,1]\\) を確率核，\\(\\lambda\\in\\mathcal{L}_b(E)_+\\) を有界可測関数とする． \\[\nAf(x):=\\lambda(x)\\int_E\\biggr(f(y)-f(x)\\biggl)\\mu(x,dy),\\quad f\\in\\mathcal{L}_b(E),\n\\tag{1}\\] は有界作用素 \\(A\\in B(\\mathcal{L}_b(E))\\) を定め，一様連続半群 \\[\n\\{T_t:=e^{tA}\\}_{t\\in\\mathbb{R}_+}\\subset B(\\mathcal{L}_b(E))\n\\] を生成する．1 これに対応する Markov 過程はどのようなものだろうか？\n\n\n\n\n\n\n\n\n端的な回答\n\n\n\n\n\n第 1.4 節をご覧ください．\n\n\n\n初期分布を \\(\\nu\\in\\mathcal{P}(E)\\) とする．\n\n\n1.2 構成１\n累積するジャンプを足し合わせた値として，畳み込み半群 \\(\\{\\mu^{\\otimes k}\\}_{k\\in\\mathbb{N}}\\) に対応する初期分布 \\(\\nu\\) の Markov 連鎖 \\(\\{Y_k\\}_{k=0}^\\infty\\) を用意する．\nこれと独立な指数確率変数列 \\(\\Delta_k\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1)\\) を用いて， \\[\nX_t:=\\begin{cases}\nY_0&0\\le t&lt;\\frac{\\Delta_0}{\\lambda(Y_0)}\\\\\nY_k&\\sum_{j=0}^{k-1}\\frac{\\Delta_j}{\\lambda(Y_j)}\\le t&lt;\\sum_{j=0}^{k}\\frac{\\Delta_j}{\\lambda(Y_j)}\n\\end{cases}\n\\] と構成した過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\) が，\\(\\{e^{tA}\\}\\) に対応する Markov 過程になる．2\nなお，\\(\\lambda(x)=0\\) の場合は，ジャンプは起きないもの \\(\\frac{\\Delta}{\\lambda(x)}=\\infty\\) と解する．この場合は零過程である．一般に関数 \\(\\lambda\\in\\mathcal{L}_b(E)\\) は位置 \\(x\\in E\\) からのジャンプの起こりやすさを表していると思える．\n\n\n1.3 構成２\n\\(\\lambda=0\\) の場合は零過程であるから， \\[\n\\lambda:=\\sup_{x\\in E}\\lambda(x)&gt;0\n\\] とし，新たな確率核 \\(\\mu':E\\to E\\)を \\[\n\\mu'(x,\\Gamma):=\\left(1-\\frac{\\lambda(x)}{\\lambda}\\right)\\delta_x(\\Gamma)+\\frac{\\lambda(x)}{\\lambda}\\mu(x,\\Gamma)\n\\] と定めると，生成作用素 \\(A\\) は \\[\nAf(x)=\\lambda\\int_E\\biggr(f(y)-f(x)\\biggl)\\mu'(x,dy)\n\\] とも表せる．\nこのとき，畳み込み半群 \\(\\{\\mu'^{\\otimes k}\\}_{k\\in\\mathbb{N}}\\) に対応する初期分布 \\(\\nu\\) の Markov 連鎖 \\(\\{Y_k'\\}_{k=0}^\\infty\\) は \\(\\{Y_k\\}\\) とは分布同等でない．\nだが，この \\(\\{Y_k'\\}\\) に対しては，独立な Poisson 過程 \\(\\{V_t\\}_{t\\in\\mathbb{R}_+}\\) に対して \\[\nX'_t:=Y'_{V_t}\\quad t\\in\\mathbb{R}_+\n\\] と構成される過程 \\(\\{X_t'\\}_{t\\in\\mathbb{R}_+}\\) はやはり \\(\\{e^{tA}\\}\\) に対応する Markov 過程である．\n\n\n\n\n\n\n命題3\n\n\n\n\n\\(\\{X'_t\\}\\) は \\(\\{X_t\\}\\) に分布同等である．\n\\(\\{Y'_k\\}\\) は Markov 性 \\[\n\\operatorname{E}[f(Y'_{k+V_t})|\\mathcal{F}_t]=P^kf(X'_t)\n\\] を満たす．ただし，\\(P\\) は \\(\\mu'\\) が定める作用，\\(\\mathcal{F}_t:=\\mathcal{F}_t^V\\lor\\mathcal{F}^{X'}_t\\) とした．\n\\(X'\\) は \\(\\{T_t\\}\\) に対応する Markov 過程である： \\[\n\\operatorname{E}[f(X'_{t+s})|\\mathcal{F}_t]=T_sf(X'_t).\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n1.4 まとめ\n\nすなわち \\(X\\) は非一様な複合 Poisson 過程になる．\n各点 \\(x\\in E\\) に於て，レート \\(\\lambda(x)\\) で，分布 \\(\\mu(x,dy)\\) に従う点へとジャンプをする．4\n具体的には，次のように整理できる：\n強度測度 \\(\\lambda(x)\\mu(x,dy)dx\\) を持つ \\(E^2\\) 上の Poisson 点過程 \\(\\eta\\) に対して， \\[\n\\xi(A)=\\int_{A\\times E}y\\,\\eta(dxdy)\n\\] で定まる非一様な複合 Poisson 点過程 \\(\\xi\\) に対応する．\n例えば \\(E=\\mathbb{R}\\) のとき， \\[\nM_t:=\\xi([0,t])=\\int_0^t\\xi(ds)\n\\] が，所定の生成作用素 \\(\\{T_t\\}\\) を持つ加法過程になる．\n\\(\\lambda\\) を有界としているから，有界区間上でのジャンプ数は有限である．Gamma 過程 のように，\\(\\mathbb{R}\\) の稠密部分集合上でジャンプを繰り返す，というようなことは起こり得ない．\nPoisson 過程については次の稿も参照：\n\n   \n      \n         \n         \n            Poisson 過程を見てみよう\n            YUIMA パッケージを用いたシミュレーションを通じて"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#区分的確定的-markov-過程",
    "href": "posts/2024/Process/PureJump.html#区分的確定的-markov-過程",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "2 区分的確定的 Markov 過程",
    "text": "2 区分的確定的 Markov 過程\n\n2.1 導入\n前節で調べた純粋跳躍過程に，決定論的な動きを加えた Markov 過程のクラスを，(Davis, 1984) 以来 区分的確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) と呼ぶ．\nPDMP は拡散項を持たない Lévy 過程とも理解できる．5\n実際，(Davis, 1984) は拡散過程に相補的なクラスとして導入し，(Davis, 1993) は PDMP が待ち行列，ポートフォリオ最適化，確率的スケジューリング，標的追跡，保険数理，資源最適化など，広く確率的モデリングと最適化において重要な役割を演じることを見事に描き出した．6\n\nthe class of “piecewise-deterministic” Markov processes, newly introduced here, provides a general family of models covering virtually all non-diffusion applications. (Davis, 1984)\n\n実際，PDMP を用いた MCMC である Piecewise Deterministic Monte Carlo または連続時間 MCMC は，高次元データと大規模データセットに対する効率的なサンプリング法開発の鍵と目されている．\n次の記事も参照：\n\n   \n      \n         \n         \n            新時代の MCMC を迎えるために\n            連続時間アルゴリズムへの進化\n         \n      \n   \n\n\n\n2.2 設定\n\n2.2.1 空間\nPDMP は動きのモードが移り変わっても良い．これを表すパラメータ \\(v\\in K\\) を導入する．7\nこのパラメータ空間上に，PDMP が動く範囲の次元を表す関数 \\(d:K\\to\\mathbb{N}^+\\) を導入し，パラメータが \\(v\\in K\\) である際は，\\(M_v\\subset\\mathbb{R}^{d(v)}\\) として，積空間 \\(M_v\\times\\{v\\}\\) 上を運動するものとする．\n総合して，PDMP の状態空間を \\(M_v\\) の直和 \\[\nE:=\\bigcup_{v\\in K}M_v\\times\\{v\\}\n\\] と定める．8\n\n\n2.2.2 フロー\n\\(\\mathfrak{X}_v\\) を \\(M_v\\) 上のベクトル場とし，積分曲線 \\[\n\\frac{d \\phi_v(x,t)}{d t}=\\mathfrak{X}_v(\\phi_v(x,t)),\n\\] \\[\n\\phi_v(x,0)=x\n\\] が存在するとする．9\n\\(\\lambda:E\\to\\mathbb{R}_+\\) は \\(s\\mapsto\\lambda(\\phi_v(x,s),v)\\) が局所可積分であるような関数とし， \\[\n\\Lambda(t):=\\int^t_0\\lambda(\\phi(s,x))ds\n\\] で表す．\n\n\n2.2.3 境界\n\\(M_v\\subset\\mathbb{R}^{d(v)}\\) の境界のうち，積分曲線が到達し得る部分を \\[\n\\partial^*M_v:=\\left\\{y\\in\\partial M_v\\,\\middle|\\,\\exists_{t&gt;0}\\;\\exists_{x\\in M_v}\\;\\phi_v(x,t)=y\\right\\}\n\\] で表し，その直和を \\(\\Gamma^*:=\\bigcup_{v\\in K}\\partial^*M_v\\times\\{v\\}\\) とする．\nさらにそのうち，正な確率を持って到達可能な部分を \\[\n\\Gamma:=\\left\\{(y,v)\\in\\Gamma^*\\mid\\lim_{x\\to y}\\operatorname{P}_{(x,v)}[T_1=t^*(x,v)]=1\\right\\}\n\\]\n点 \\((x,v)\\in E\\) からフローに従って運動した場合の境界 \\(\\Gamma^*\\) への到達時刻を \\[\nt^*(x,v):=\\inf\\left\\{t\\ge 0\\mid\\phi_v(x,t)\\in\\partial^*M_v\\right\\}\n\\] とする．\n\n\n\n2.3 定義\n\n\n\n\n\n\n定義 (PDMP: Piecewise Deterministic Markov Process)10\n\n\n\n\\((\\phi(t,-))\\) をフロー，\\(\\lambda:E\\to\\mathbb{R}_+\\) をレート関数，\\(Q:(E\\cup\\Gamma^*)\\times\\mathcal{E}\\to[0,1]\\) を確率核とする．\n３組 \\((\\phi,\\lambda,Q)\\) が定める PDMP \\(Z\\) とは，次のように帰納的に定まる確率過程をいう：\n\nあるスタート地点 \\(z\\in E\\) から，一様乱数 \\(U_1\\sim\\mathrm{U}([0,1])\\) とフロー \\(\\phi\\) が定めるジャンプ時刻 \\[\nT_1:=\\inf\\left\\{t\\ge 0\\,\\middle|\\,e^{-\\Lambda(t)}\\le U_1\\right\\}\\lor t^*(x,v)\n\\] まで，フローの通りに移動するとする： \\[\nZ_t=\\phi(t,x),\\quad t&lt; T_1,\n\\]\n時刻 \\(T_1\\) での位置は確率核 \\(Q\\) に従って決まるとする： \\[\nZ_{T_1}\\sim Q\\biggr((\\phi(T_1,x),v),-\\biggl).\n\\]\nこの手続きを，\\(Z_{T_1}\\) を次のスタート地点として繰り返すのが特性量 \\((\\phi,\\lambda,Q)\\) が定める PDMP \\(Z\\) である．\n\n\n\n\\(\\lambda\\) は局所有限であるから，有限時区間上でのジャンプ数は有限になる．境界への到達によるジャンプ数も同様の性質を満たすと仮定する．11\nこうして得た PDMP は，Feller-Dynkin 過程であるとは限らないにも拘らず，時間的に一様な強 Markov 過程である．12\n\n\n2.4 PDMP の拡張生成作用素\nPDMP の拡張生成作用素は，ちょうど（有限な Lévy 測度を持つ）純粋跳躍過程の生成作用素 1 にドリフト項を加えたものになる．\n\n\n\n\n\n\n命題（PDMP の拡張生成作用素の定義域の特徴付け）13\n\n\n\n\n\n\\(X\\) を PDMP とする．このとき，\\(X\\) の拡張生成作用素 \\(\\widehat{A}\\) について，可測関数 \\(f\\in\\mathcal{L}(E\\cup\\Gamma)\\) が \\(f\\in\\mathcal{D}(\\widehat{A})\\) であるとは，次と同値：\n\n絶対連続性：任意の \\(x\\in E\\) について， \\[\nt\\mapsto f(\\phi(t,x))\n\\] は \\[\ns_*(x):=\\inf\\left\\{t\\ge 0\\mid\\operatorname{P}_x[T_1&gt;t]=0\\right\\}\n\\] に関して，\\(t\\in[0,s_*(x))\\) 上で絶対連続である．\n境界条件： \\[\n\\Gamma:=\\left\\{x\\in E\\mid\\exists_{t&gt;0}\\;\\exists_{z\\in E}\\;x=\\phi(t,z)\\right\\}\n\\] 上では \\[\nf(x)=\\int_Ef(y)Q(x,dy),\\quad x\\in\\Gamma\n\\] を満たす．\n跳躍の局所可積分性：\\(E\\times\\mathbb{R}_+\\) 上のランダム関数 \\[\n(x,s)\\mapsto Bf(x,s):=f(x_s)-f(X_{s-})\n\\] は，\\(X\\) の跳躍が定める \\(E\\times\\mathbb{R}_+\\) 上の Poisson 点過程 \\(\\eta\\) について，ある発散する停止時の増加列 \\(\\tau_n\\) が存在して， \\[\n\\operatorname{E}\\left[\\int_0^{\\tau_n}\\int_E\\lvert Bf(x,s)\\rvert\\eta(dxds)\\right]&lt;\\infty\n\\] \\[\nn=1,2,\\cdots\n\\] が成り立つ．\n\n\n\n\n\n\n\n\n\n\n命題（PDMP の拡張生成作用素の特徴付け）14\n\n\n\n\\(X\\) を PDMP とする．このとき，\\(X\\) の拡張生成作用素 \\(\\widehat{A}\\) について，\\(\\mathcal{D}(\\widehat{A})\\) の全域で次が成り立つ： \\[\n\\widehat{A}f(x)=\\mathfrak{X}f(x)+\\lambda(x)\\int_E\\biggr(f(y)-f(x)\\biggl)Q(x,dy).\n\\]\n\n\n\n\n\n\n\n\n注（第一項の well-definedness）15\n\n\n\n\n\n\\(\\mathfrak{X}\\) の標準座標 \\(\\frac{\\partial }{\\partial x_1},\\cdots,\\frac{\\partial }{\\partial x_d}\\) に関する成分表示を \\(g^i\\) とすると，\\(\\mathfrak{X}f(x)\\) は \\((g(x)|\\nabla f(x))\\) とも表せる．\nこの \\(\\nabla f\\) は，\\(f\\in C^1(M_v)\\) である場合は問題がないが，一般に単に絶対連続である場合は問題になる可能性がある．\nしかしそれでも，次を満たす関数 \\(\\nabla f\\) として用意することができる： \\[\nf(\\phi_v(x,t),v)-f(x,v)=\\int^t_0\\nabla f(\\phi_v(x,s),v)ds.\n\\]\n\n\n\n\n\n2.5 PDMP が Feller-Dynkin 過程であるとき\n\n\n\n\n\n\n命題（PDMP の Feller 性の特徴付け）16\n\n\n\n\\(X\\) を PDMP とする．次の全てが成り立つならば，\\(X\\) は Feller-Dynkin 過程である：\n\n任意の \\(x\\in E\\) について， \\[\nt_*(x):=\\inf\\left\\{t&gt;0\\mid\\phi(t,x)\\in\\partial E\\right\\}=\\infty.\n\\]\n\\(\\lambda\\in C_b(E)\\)．\n\\(Q\\) も Feller 性を持つ：\\(Q(C_b(E))\\subset C(E)\\)．\n\n\n\n\n\n\n\n\n\n注（right process としての PDMP）\n\n\n\n\n\nしかし，PDMP は常に (Meyer, 1966) にいう right process ではある．すなわち，次の４条件を満たす：\n\n\\(E\\) は Lusin 空間である．\n\\(P_t(\\mathcal{L}_b(E))\\subset\\mathcal{L}_b(E)\\)．\n\\(X\\) は殆ど確実に右連続な見本道を持つ．\n\\(f\\) が \\(\\alpha\\)-excessive function ならば，\\(t\\mapsto f\\circ X_t\\) も殆ど確実に右連続な見本道を持つ．\n\n\n\n\n\n\n2.6 PDMP のシミュレーション\n\n2.6.1 到着時刻のシミュレーション\nレート関数が \\(\\lambda&gt;0\\) であるとき，Poisson イベントのシミュレーションは次のように，指数分布に従う確率変数のシミュレーションによって行える．\n累積関数を \\[\n\\Lambda(t):=\\int^t_0\\lambda(s)ds\n\\] とすると，\\(N_t\\sim\\mathrm{Pois}(\\Lambda(t))\\) であるから，最初のイベントの到着時刻 \\(T_1\\) は，次の生存関数によって特徴付けられる： \\[\n\\operatorname{P}[T_1\\ge t]=\\operatorname{P}[N_t=0]=\\exp\\left(-\\Lambda(t)\\right).\n\\]\nここで，\\(E\\sim\\operatorname{Exp}(1)\\) とすると， \\[\n\\operatorname{P}[\\Lambda^{-1}(E)\\ge t]=\\operatorname{P}\\left[E\\ge\\Lambda(t)\\right]=e^{-\\Lambda(t)}.\n\\] 従って，\\(T_1\\overset{\\text{d}}{=}f^{-1}(E)\\) である．\nすなわち，\\(E\\) を通じて \\[\nT_1':=\\inf\\left\\{t\\ge 0\\,\\middle|\\,\\int^t_0\\lambda(s)ds=E\\right\\}\n\\] を計算すれば，最初の到着時刻が計算できる．17\n\n\n2.6.2 PDMP のためのパッケージ\nJoris Bierkens ら開発の R パッケージ RZigZag (GitHub / CRAN) を通じて実行してみる．\ninstall.packages(\"Rcpp\")\ninstall.packages(\"RcppEigen\")\ninstall.packages(\"RZigZag\")"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#zig-zag-sampler-bierkens2019",
    "href": "posts/2024/Process/PureJump.html#zig-zag-sampler-bierkens2019",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "3 Zig-Zag Sampler (Bierkens et al., 2019)",
    "text": "3 Zig-Zag Sampler (Bierkens et al., 2019)\n\n3.1 導入\n１次元の Zig-Zag 過程は元々，Curie-Weiss 模型 における Glauber 動力学を lifting により非可逆化して得る Markov 連鎖の，スケーリング極限として特定された Feller-Dynkin 過程である (Bierkens and Roberts, 2017)．\nただし，(Goldstein, 1951) で電信方程式と関連して，同様の過程が扱われた歴史もある．\n\n\n3.2 設定\nZig-Zag 過程 \\(Z=(X,\\Theta)\\) の状態空間は \\(E=\\mathbb{R}^d\\times\\{\\pm1\\}^d\\) と見ることが多い．\n\\(\\theta\\in\\{\\pm1\\}^d\\) は速度を表す．すなわち，全座標系と \\(45\\) 度をなす方向に，常に一定の単位速度で \\(\\mathbb{R}^d\\) 上を運動するとする．\n換言すれば，決定論的なフローは次のように定める： \\[\n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\n\\] \\[\n\\frac{d \\Theta}{d t}=0,\\qquad\\Theta_0=0.\n\\]\n\n\n3.3 アルゴリズム\n後述の関数 \\(\\lambda\\) に対して，各座標 \\(i\\in[d]\\) におけるレートを \\[\nm_i(t):=\\lambda(x+\\theta t,\\theta)\n\\] で定めた \\(\\mathbb{R}^d\\) 上の Poisson 過程を考える．\n多次元の Poisson 過程の各成分の跳躍は独立だから，18 それぞれの成分ごとに Poisson 到着時刻 \\(T_i\\;(i\\in[d])\\) をシミュレーションし，最初に到着したものを \\(T_j\\) とすると，関数 \\[\nF_j(\\theta)_i=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n\\] に従ってジャンプすると考えて良い．\n\n\n\n\n\n\n注（第 2.2 節の設定との対応）\n\n\n\n\n\n状態空間は \\[\nE=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\}\n\\] とみなすところから始める必要がある．\n加えて，レート関数は \\[\n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n\\] であり，跳躍測度は点過程 \\[\nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\\] とみなした場合に当たる．\n\n\n\n\n\n3.4 レート関数\nPDMP の妙は全てレート関数に宿っている．\nレート \\(\\lambda:E\\to\\mathbb{R}_+\\) は，負の対数密度 \\(U\\in C^1(\\mathbb{R}^d)\\) が定める目標分布 \\(\\pi(dx)\\,\\propto\\,e^{-U(x)}dx\\) に対して， \\[\n\\lambda_i(x,\\theta):=(\\theta_i\\partial_iU(x))_++\\gamma_i(x,\\theta_{-i})\\quad(i\\in[d])\n\\] と定める．ただし，\\(\\gamma_i\\) は，\\(\\theta_i\\) のみには依らない任意の連続関数 \\(\\gamma_i:E\\to\\mathbb{R}_+\\) とした．19\n\n\n\n\n\n\n(Bierkens et al., 2019, pp. 1294 定理2.2)\n\n\n\n\\(U\\in C^1(\\mathbb{R}^d)\\) は \\(e^{-U}\\in\\mathcal{L}^1(\\mathbb{R}^d)\\) を満たすとする．このとき，\\(Z\\) は次の分布 \\(\\mu=\\pi\\otimes\\mathrm{U}(\\{\\pm1\\}^{d})\\in\\mathcal{P}(E)\\) を不変にする： \\[\n\\mu(dxd\\theta)=\\frac{1}{2^d\\mathcal{Z}}e^{-U(x)}\\,dxd\\theta\n\\]\n\n\n\n\n3.5 部分サンプリング\nMCMC の計算複雑性のボトルネックは，尤度の評価にある．各ステップで全てのデータを用いて尤度を計算する必要がある点が，MCMC を深層学習などの大規模データの設定への応用を難しくしている (Murphy, 2023, p. 647)．\n\\(p(x)\\) を事前分布，\\(p(y|x)\\) を観測のモデル（または尤度）とし，データ \\(y_1,\\cdots,y_n\\) は互いに独立であるとする．このとき，事後分布 \\(\\pi(x):=p(x|y)\\) は \\[\n\\pi(x)\\,\\propto\\,\\left(\\prod_{k=1}^n p(y_k|x)\\right)p(x)\n\\] より，Hamiltonian \\(U\\) は \\[\\begin{align*}\n   U(x)&=-\\sum_{k=1}^n\\log p(y_k|x)-\\log p(x)\\\\\n   &=\\frac{1}{n}\\sum_{k=1}^n\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl).\n\\end{align*}\\] と表せる．\nすなわち，各微分係数 \\(\\partial_i U(x)\\) は，独立な観測 \\(y_1,\\cdots,y_n\\) が定める統計量 \\[\nE^i_k(x):=\\frac{\\partial }{\\partial x_i}\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl)\n\\] の平均により推定される値となっている．\nよって，精度は劣るかもしれないが，一様に選んだ \\(K\\sim\\mathrm{U}([n])\\) から定まる \\(E^i_K\\) の値は \\(\\partial_i U(x)\\) の不偏推定量となっている．20\n\n\n\n\n\n\n部分サンプリングにより不変分布が変わらないことの証明\n\n\n\n\n\nこれは，元々のレート関数 \\[\n\\lambda_i(x,\\theta)=\\frac{1}{n}\\sum_{k=1}^n(\\theta E^i_k(x))_+\n\\] に対して， \\[\n\\gamma_i(x,\\theta):=\\frac{1}{n}\\sum_{k=1}^n(\\theta_iE^i_k(x))_+-\\left(\\theta_i\\frac{1}{n}\\sum_{k=1}^nE^i_k(x)\\right)_+\n\\] という項を加えて得る Zig-Zag サンプラーとみなせるためである．\nこうして定義された \\(\\gamma_i\\) が非負関数である限り，平衡分布は \\(\\mu\\) のままであるが，これは関数 \\((-)_+\\) の凸性から従う．\nこうして，サブサンプリングの実行による精度の劣化が，(Andrieu and Livingstone, 2021) の枠組みで捉えられる，ということでもある．\n加えて，レート関数が大きくなっており，したがってそもそも尤度関数の評価の回数が増えていることにも注意．\n\n\n\n\n\n3.6 制御変数による分散低減\n上述の \\(\\partial_iU(x)\\) の不偏推定量の分散は，制御変数の方法を用いて低減できる．これにより，事前処理の部分を除けば，データのサイズに依存しない計算複雑性で事後分布からの正確なサンプリングが可能になる．\n\n\n3.7 シミュレーション\n\n\nCode\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#78C2AD\"), plot.title=element_text(color=\"#78C2AD\"))\n\n\n\n\n\n\\(\\mathrm{N}_2\\left(\\begin{pmatrix}2\\\\2\\end{pmatrix},\\begin{pmatrix}3&1\\\\1&3\\end{pmatrix}\\right)\\) に対する Zig-Zag 過程\n\n\n\n\n\n\nCode\nset.seed(123)\ndim &lt;- 2\ndof &lt;- 1\nresult &lt;- ZigZagStudentT(dof, dim, n_iter=1000, sphericallySymmetric = TRUE)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#78C2AD\"), plot.title=element_text(color=\"#78C2AD\"))\n\n\n\n\n\nCauchy 分布 \\(\\mathrm{C}(0,1)=\\mathrm{t}(1)\\) に対する Zig-Zag 過程\n\n\n\n\n続きは次の稿も参照：\n\n   \n      \n         \n         \n            Zig-Zag サンプラー\n            ジャンプと確定的な動きによる新たな MCMC 手法"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#bouncy-particle-sampler-bouchard-cote2018-bps",
    "href": "posts/2024/Process/PureJump.html#bouncy-particle-sampler-bouchard-cote2018-bps",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "4 Bouncy Particle Sampler (Bouchard-Côté et al., 2018)",
    "text": "4 Bouncy Particle Sampler (Bouchard-Côté et al., 2018)\n\n4.0.1 部分サンプリング\nZig-Zag サンプラーに対応するサブサンプリングの技術を (Pakman et al., 2017) が提案している．\n\n\n4.0.2 シミュレーション\n\n\nCode\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#78C2AD\"), plot.title=element_text(color=\"#78C2AD\"))\n\n\n\n\n\n\\(\\mathrm{N}_2\\left(\\begin{pmatrix}2\\\\2\\end{pmatrix},\\begin{pmatrix}3&1\\\\1&3\\end{pmatrix}\\right)\\) に対する Zig-Zag 過程"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#footnotes",
    "href": "posts/2024/Process/PureJump.html#footnotes",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Pazy, 1983, p. 2) 参照．↩︎\nこの事実は第 1.3 節で２つ目の構成と同時に証明される．↩︎\n(Ethier and Kurtz, 1986, pp. 163–164) 参照．↩︎\n\\(f(y+x)-f(x)\\) ではなくて \\(f(y)-f(x)\\) であるので，\\(\\mu(x,dy)\\) は必ずしも現在地点 \\(x\\) からみた変位の分布ではないことに注意．↩︎\nただし，ジャンプが頻繁すぎないことも必要である．(Davis, 1993, p. 60) 仮定24.4 では，有界区間上のジャンプは有限回であると過程している：\\(\\operatorname{E}[N_t]&lt;\\infty\\;(t\\in\\mathbb{R}_+)\\)．すなわち，A 型までの加法過程を PDMP というのであって，B 型と C 型は，シミュレーションが容易な連続時間過程であるという美点を逃してしまう．加法過程の分類については，この 稿 も参照．↩︎\n当時は連続時間確率過程といえば拡散過程であり，SDE によるモデリングが興隆した時代であった．(Davis, 1993) では，必ずしも SDE を使うことが自然なモデリング方法でないにも拘らず，無理やり SDE の枠組みに落とし込もうとする当時の慣行を批判し，PDMP はこのギャップを埋めるために開発した，としている．なお，(Davis, 1993) では PDMP ではなく PDP と呼んでいる．↩︎\n(Davis, 1993, p. 57) では \\(K\\) は可算という仮定をおいている．↩︎\n積空間としての \\(\\sigma\\)-代数を導入する．↩︎\n(Davis, 1993) では，\\(\\mathfrak{X}_v\\) は局所 Lipcthiz 連続である上に，（暗黙のうちに）完備で，\\(\\phi_v\\) は任意の \\(t\\in\\mathbb{R}\\) について定義されるとしていることに注意．(G. Vasdekis and Roberts, 2023) では違う．↩︎\n(Davis, 1993, p. 58) と (Georgios Vasdekis, 2021, pp. 15–16) に倣った．↩︎\nこのことを (Davis, 1993, p. 60) 仮定24.4としている．↩︎\n(Davis, 1984)，(Davis, 1993, p. 64) 定理25.5も参照．↩︎\n(Davis, 1993, p. 69) 定理26.14．↩︎\n(Davis, 1993, p. 69) 定理26.14 の後半．↩︎\n(Georgios Vasdekis, 2021, p. 18) 系2.3.4 も参照．↩︎\n(Davis, 1993, p. 77) 定理27.6．↩︎\n簡単な対象分布では，\\(\\Lambda(t)=E\\) の解が解析的に求まる事が多い．これが intractable である場合は，剪定 を用い，\\(\\lambda^*\\) としては affine 関数を用いる事が多いが，一般に３次の多項式までならば解の公式があるために，\\(\\Lambda(t)=E\\) を解析的に解く方法から効率的にシミュレーションできる．どれくらい \\(\\lambda^*\\) として \\(\\lambda\\) に近いものを選べば良いかは不明．(Georgios Vasdekis, 2021, p. 13) 命題2.2.2も参照．↩︎\n(Revuz and Yor, 1999, p. 473) 命題XII.1.7．↩︎\n従って，レート関数 \\(\\lambda\\) は連続とする．この関数 \\(\\gamma_i\\) は，\\(U\\) の情報には依らない追加のリフレッシュ動作を仮定に加える．実際，\\(\\lambda_i(x,\\theta)-\\lambda_i(x,F_i(\\theta))=\\theta_i\\partial_iU(x)\\) である限り，\\(\\theta\\) と \\(F_i(\\theta)\\) の往来には影響を与えず釣り合っているため，どのような \\(\\gamma_i\\) をとっても，平衡分布には影響を与えない．しかし，高くするごとにアルゴリズムの対称性が上がるため，\\(\\gamma\\equiv0\\) とすることが Monte Carlo 推定量の漸近分散を最小にするという (Andrieu and Livingstone, 2021)．↩︎\nこのとき，必ずしも \\(K\\sim\\mathrm{U}([n])\\) とする必要はなく，特定の観測に重みをおいても良い (Sen et al., 2020)．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Levy.html",
    "href": "posts/2024/Process/Levy.html",
    "title": "Lévy 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nYUIMAについては次の記事も参照：\nPoisson 過程と複合 Poisson 過程については次の記事を参照："
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "href": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "title": "Lévy 過程を見てみよう",
    "section": "1 Lévy-Itô 分解",
    "text": "1 Lévy-Itô 分解\n\n1.1 加法過程の定義\n\n\n\n\n\n\n定義 (additive process, Lévy process)1\n\n\n\n確率過程 \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 加法過程 であるとは，最初の４条件を満たすことをいう．５条件全てを満たすとき，Lévy 過程 であるという．\n\n\\(X_0\\overset{\\text{a.s.}}{=}0\\)．\nある充満集合 \\(\\Omega_0\\subset\\Omega\\) が存在し，\\(t\\mapsto X_t(\\omega)\\) は càdlàg である．\n独立増分：任意の \\(0\\le t_0&lt;\\cdots&lt;t_n\\) について， \\[\n  X_{t_0},X_{t_1}-X_{t_0},X_{t_2}-X_{t_1},\\cdots,X_{t_n}-X_{t_{n-1}}\n  \\] は独立である．\n確率連続：任意の \\(\\epsilon&gt;0\\) と \\(t\\ge0\\) について， \\[\n  \\lim_{s\\to t}\\operatorname{P}[\\lvert X_s-X_t\\rvert&gt;\\epsilon]=0\n  \\] が成り立つ．\n定常増分：\\(X_{s+t}-X_s\\) の分布は \\(s\\ge0\\) に依らない．\n\n\n\n\n\n1.2 特性量\n加法過程 \\(\\{X_t\\}\\) について，\\(X_t\\) の分布は必ず \\(\\mathbb{R}^d\\) 上の無限可分分布になる．2\n加えて，加法過程の分布は１次元の有限次元分布族が特徴付ける．\n\n\n\n\n\n\n定理（加法過程の分布）3\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を加法過程とする．\n\n\\(\\mu_{s,t}\\) を \\(X_t-X_s\\) の分布とすると，これは無限可分であり，次を満たす： \\[\n\\mu_{s,t}*\\mu_{t,u}=\\mu_{s,u},\n\\] \\[\n\\mu_{s,s}=\\delta_0,\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\quad(s\\nearrow t),\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\qquad(t\\searrow s).\n\\]\n確率分布の族 \\(\\{\\mu_{s,t}\\}\\subset\\mathcal{P}(\\mathbb{R}^d)\\) が上の４式を満たすならば，これはある加法過程 \\(\\{X_t\\}\\) が定める分布である．\n２つの加法過程 \\(X,X'\\) が， \\[\nX_t\\overset{\\text{d}}{=}X'_t\\quad t\\in\\mathbb{R}_+\n\\] を満たすならば，\\(X\\overset{\\text{d}}{=}X'\\) が成り立つ．\n\n\n\nこのことにより，加法過程 \\(X\\) の分布は，各 \\(X_t\\) の無限可分分布を特徴付ける特性量 \\((A_t,\\nu_t,\\gamma(t))\\) によって特徴付けられる．\n\n\n\n\n\n\n無限可分分布の特徴付け (Khinchin and Lévy, 1936)4\n\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) について，次は同値：\n\n\\(f\\) は無限可分である．\n(Lévy) ある \\[\n  \\nu(\\{0\\})=0,\\qquad\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n  \\] を満たす測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}^d)\\) と対称な半正定値行列 \\(A\\in S_n(\\mathbb{R})_+\\) と \\(\\gamma\\in\\mathbb{R}^d\\) が一意的に存在して，次のように表せる： \\[\n  f(z)=\\exp\\left(-\\frac{1}{2}(z|Az)+i(\\gamma|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\\biggl)\\nu(dx)\\right).\n  \\]\n(Khinchin) ある有限測度 \\(\\Psi\\in\\mathcal{M}^1(\\mathbb{R}^d)\\) と \\(\\alpha\\in\\mathbb{R}^d\\) が存在して，次のように表せる： \\[\nf(u)=\\exp\\left(i(\\alpha|u)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|x)}-1-\\frac{i(u|x)}{1+\\lvert x\\rvert^2}\\biggl)\\frac{1+\\lvert x\\rvert^2}{\\lvert x\\rvert^2}\\Psi(dx)\\right).\n\\]\n\n\\(A\\) を Gauss 共分散，\\(\\nu\\) を Lévy 測度，\\(\\Psi\\) を Khintchine測度 という．5\n加えて，任意の半正定値行列 \\(A\\)，\\(\\gamma\\in\\mathbb{R}^d\\)，Lévy 測度 \\(\\nu\\) であって \\(\\nu(\\{0\\})=0\\) かつ \\[\n\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n\\] を満たすものに対して，\\((A,\\nu,\\gamma)\\) を特性量にもつ無限可分分布が存在する．\nKhintchin の表示はより直接的である上に，Khintchin 測度は有限になる．さらに，\\((\\alpha,\\Psi)\\) の収束が過程の収束にも対応する！6 だが，確率論的な意味付けに欠けるために，Lévy の表示の方をここでは用いる．\nLévy の表示の被積分関数 \\[\ne^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\n\\] は大変複雑であるが，こうしないと \\(\\nu\\)-可積分にならないのである．\n\\(\\nu\\) は \\(O(\\lvert x\\rvert^2)\\) 関数に関してならば \\(0\\) の近傍でも可積分であるから，\\(e^{i(z|x)}\\) から１次以下の項を \\(0\\) の近傍から取り去ることで可積分にしているのである．そのため，最後の項は \\(1_{\\left\\{B^d\\right\\}}\\) でなくとも， \\[\nc(x)=1+o(\\lvert x\\rvert)\\quad(\\lvert x\\rvert\\to0)\n\\] \\[\nc(x)=O(\\lvert x\\rvert^{-1})\\quad(\\lvert x\\rvert\\to\\infty)\n\\] の２条件を満たすものならばなんでも良い．だが，取り替える度に１次の項 \\(\\gamma\\in\\mathbb{R}^d\\) を変更する必要がある．\n一般に \\(\\gamma\\) はドリフトと呼んではいけない． \\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] を満たす場合のみ， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_0|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right)\n\\] と表示でき，この際の \\(\\gamma_0\\in\\mathbb{R}^d\\) を ドリフト と呼ぶ．\n逆に， \\[\n\\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] が成り立つとき， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_1|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)\\biggl)\\nu(dx)\\right)\n\\] と表示でき，\\(\\gamma_1\\) は \\(f\\) が定める確率分布の平均に一致する．7\n\n\n\nLévy 過程は，\\(A:=A_1,\\nu:=\\nu_1,\\gamma:=\\gamma(1)\\) について， \\[\nA_t=tA,\\quad\\nu_t=t\\nu,\\quad\\gamma_t=t\\gamma\n\\] と表せる場合に当たる．\n\n\n1.3 強度測度との関係\n\\(\\{(A_t,\\nu_t,\\gamma_t)\\}_{t\\in\\mathbb{R}_+}\\) を加法過程の特性量とする．\nこのとき， \\[\n\\widetilde{\\nu}([0,t]\\times B):=\\nu_t(B),\\qquad t\\ge0,B\\in\\mathcal{B}(\\mathbb{R}^d)\n\\] は \\(\\mathbb{R}_+\\times\\mathbb{R}^d\\) 上に測度を定める．\n\n\n\n\n\n\n命題（強度測度と特性測度の関係）8\n\n\n\n測度の族 \\(\\{\\nu_t\\}\\subset\\mathbb{M}(\\mathbb{R}^d)\\) と測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}_+\\times\\mathbb{R}^d)\\) について，次は同値：\n\n\\(\\widetilde{\\nu}\\) は次の２条件を満たす： \\[\n\\widetilde{\\nu}(\\{t\\}\\times\\mathbb{R}^d)=0,\n\\] \\[\n\\int_{[0,t]\\times\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\widetilde{\\nu}(dsdx)&lt;\\infty.\n\\]\n\\(\\{\\nu_t\\}\\) はある加法過程の特性測度である．\n\n\n\nよって，任意の加法過程について， \\[\n\\int_{\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\nu_t(dx)&lt;\\infty\n\\] が必要である．\nLévy 過程であるとき，定常増分であることが必要であるため，跳躍時刻は \\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程に従う必要がある．これより， \\[\n\\widetilde{\\nu}=\\ell_+\\otimes\\nu\n\\] と分解できる必要があり，この特性測度 \\(\\nu\\) が Lévy 測度である．このとき，\\(\\nu_t=t\\nu\\) かつ \\(\\widetilde{\\nu}(dsdx)=ds\\nu(dx)\\)．\n\n\n1.4 一般の分解\n\n\n\n\n\n\n定理 (Ito, 1941)9\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を特性量 \\(\\{(A_t,\\nu_t,\\gamma(t))\\}\\) を持つ加法過程とする． \\[\n\\eta(\\omega,B):=\\#\\left\\{t\\in\\mathbb{R}_+\\,\\middle|\\,\\begin{pmatrix}t\\\\X_t(\\omega)-X_{t-}(\\omega)\\end{pmatrix}\\in B\\right\\}\n\\] を \\(\\omega\\in\\Omega_0\\) 上で \\(B\\in\\mathcal{B}(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\})\\) に関して定める．\n\n\\(\\eta\\) は \\(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\}\\) 上の強度測度 \\(\\widetilde{\\nu}\\) を持った Poisson 点過程である．\nある充満集合 \\(\\Omega_1\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\\begin{align*}\n     X^1_t(\\omega)&:=\\lim_{\\epsilon\\searrow0}\\int_0^t\\int_{\\left\\{\\epsilon&lt;\\lvert x\\rvert\\le 1\\right\\}}x\\,\\widetilde{\\nu}(\\omega,dsdx)\\\\\n     &\\qquad+\\int_0^t\\int_{\\mathbb{R}^d\\setminus B^d}x\\,\\eta(\\omega,dsdx)\n   \\end{align*}\\] 収束は \\(t\\in\\mathbb{R}_+\\) に関して広義一様であり，\\(X^1\\) は特性量 \\(\\{(0,\\nu_t,0)\\}_{t\\in\\mathbb{R}_+}\\) が定める加法過程である．\n\\(X^2_t:=X_t-X_t^1\\) は殆ど確実に連続な見本道を持ち，特性量 \\(\\{(A_t,0,\\gamma(t))\\}\\) が定める加法過程である．\n\\(X^1\\perp\\!\\!\\!\\perp X^2\\) が成り立つ．\n\n\n\n\n\n1.5 B 型の場合\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] を満たす場合，Poisson 補過程によらない，より簡潔な表示を持つ．\n\n\n\n\n\n\n定理\n\n\n\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] が成り立つ場合，次が成り立つ：\n\nある充満集合 \\(\\Omega_3\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\n   X^3_t(\\omega):=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}x\\,\\eta(\\omega,dsdx).\n   \\] このとき，\\(X_t^3\\) の分布は複合 Poisson である： \\[\n   \\operatorname{E}[e^{i(z|X_t^3)}]=\\exp\\left(\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu_t(dx)\\right).\n   \\]\n\\(X^4_t:=X_t-X_t^3\\) は殆ど確実に連続な見本道を持ち，Gauss 過程を定める： \\[\n   \\operatorname{E}[e^{i(z|X_t^4)}]=\\exp\\left(-\\frac{1}{2}(z|A_tz)+i(\\gamma_0(t)|z)\\right).\n   \\]\n\\(X^3\\perp\\!\\!\\!\\perp X^4\\) が成り立つ．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-測度",
    "href": "posts/2024/Process/Levy.html#lévy-測度",
    "title": "Lévy 過程を見てみよう",
    "section": "2 Lévy 測度",
    "text": "2 Lévy 測度\n\n2.1 導入\n本節の目的は，Lévy 過程の次の３分類の見本道の違いを理解することである：10\n\n\n\n\n\n\n特性量 \\((A,\\nu,\\gamma)\\) を持つ Lévy 過程について，\n\nA 型：\\(A=0\\) かつ \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\nB 型：\\(A=0\\) かつ \\(\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\\) であるが，A 型ではない．\nC 型：それ以外．\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) 型は拡散項を持たず，確定的な動きと複合 Poisson 過程の和で表現される．ジャンプは離散的に起こる．\n\\(B\\) 型も拡散項を持たないが，\\(\\mathbb{R}_+\\) 上稠密な可算集合上でジャンプを繰り返す．Gamma 過程（第 3.6 節）がその例である．\n\\(A,B\\) は殆ど確実に任意の有界区間上で有界変動な見本道を持つが，\\(C\\) 型は有界変動ではない．11 Brown 運動と Cauchy 過程（第 4.4 節）がその例である．\n\n\n\n\n\n\n2.2 Lévy 測度が零ならば，Gauss 過程である\n\n\n\n\n\n\n命題（連続な Lévy 過程の特徴付け）12\n\n\n\nLévy 過程 \\(X\\) について，次の２条件は同値：\n\n\\(X\\) は殆ど確実に連続な見本道を持つ．\n\\(\\nu=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n時刻 \\(t&gt;0\\) までの跳躍回数を表す Poisson 過程 \\[\nN_t:=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}\\eta(dsdx)\\in[0,\\infty]\n\\] を考えると， \\[\n\\operatorname{E}[N_t]=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}ds\\nu(dx)=0.\n\\] すなわち，\\(N_t=0\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n2.3 区分定数ならば，A 型である．\n\n\n\n\n\n\n命題（A 型の見本道の特徴付け）13\n\n\n\nLévy 過程 \\(X\\) ついて，次の３条件は同値：\n\n\\(X\\) の見本道は，殆ど確実に区分的定数であり，有界区間上では有限回のジャンプしか起こらない．\n\\(X\\) は複合 Poisson 分布であるか，零であるかのいずれかである．\n\\(X\\) は A 型で，かつ \\(\\gamma_0=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n1 \\(\\Rightarrow\\) 2\n任意の有限区間内でのジャンプ回数は有限回であるため，ジャンプ回数の Poisson 過程 \\(N\\) について，\\(N_t\\sim\\mathrm{Pois}(t\\nu(\\mathbb{R}^d))\\) の母数は有限である必要がある．特に \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\\(X\\) に連続部分がないことを併せると，定理 1.5 より， \\[\n\\operatorname{E}[e^{i(z|X_t)}]=\\exp\\left(t\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right).\n\\] これは \\(\\mathrm{CP}(t,\\nu)\\) の特性関数である．\n2 \\(\\Rightarrow\\) 1\nこちらは省略する．\n\n\n\n\n純粋跳躍確率過程であっても，B 型ならば，見本道は区分的定数にはならない．Gamma 過程（第 3.6 節）がその例である．\n\n\n2.4 B 型の跳躍時刻\nLévy 過程の見本道は右連続であるから，\\(\\mathbb{R}_+\\) 上トータルの跳躍回数は殆ど確実に可算回である．\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) の場合は，有限区間上での跳躍回数も無限になる．\nさらに，次のことが言える：\n\n\n\n\n\n\n命題（B 型 Lévy 過程のジャンプ時刻）14\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) とする．このとき，跳躍時刻は殆ど確実に \\(\\mathbb{R}_+\\) 上稠密である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) のとき， \\[\nT_\\epsilon(\\omega):=\\inf\\left\\{t\\ge 0\\mid\\lvert X_t(\\omega)-X_{t-}(\\omega)\\rvert&gt;\\epsilon\\right\\}\n\\] とすると， \\[\n\\lim_{\\epsilon\\searrow0}\\operatorname{P}[T_\\epsilon\\le t]=1.\n\\] よって， \\[\n\\lim_{\\epsilon\\searrow0}T_\\epsilon=0\\;\\;\\text{a.s.}\n\\] これは，ある充満集合 \\(\\Omega_0\\subset\\Omega\\) の上で，\\(0\\) が \\(X\\) の跳躍時刻の触点になることを含意している．\nこれと同様の議論を任意の \\(s\\in\\mathbb{Q}\\cap\\mathbb{R}_+\\) について繰り返すことで，ある充満集合 \\[\n\\bigcap_{s\\in\\mathbb{Q}\\cap\\mathbb{R}_+}\\Omega_s\n\\] 上で，\\(X\\) の跳躍時刻の閉包が \\(\\mathbb{R}_+\\) 上で稠密になることがわかる．\n\n\n\n\n\n2.5 従属過程ならば B 型である\n\\(d=1\\) で，殆ど確実に単調増加な見本道を持つ Lévy 過程を 従属過程 (subordinator) という．15\n\n\n\n\n\n\n命題（単調増加性の特徴付け）16\n\n\n\n\\(d=1\\) とし，\\(X\\) を Lévy 過程とする．このとき，次は同値：\n\n\\(X\\) は従属過程である．\n\\(A=0,\\nu((-\\infty,0))=0\\) かつ \\[\n\\int_{[0,1)}x\\,\\nu(dx)&lt;\\infty\n\\] 加えて \\(\\gamma_0\\ge0\\) である．\n\n\n\n仮に \\(A=0,\\nu((-\\infty,0))=0\\) だが， \\[\n\\int_0^1x\\,\\nu(dx)=\\infty\n\\] であったとする．\nこのとき，正なジャンプとドリフトしか持たないはずであるから，場合によっては単調増加過程になっても良さそうなものである．\nしかし，このような過程が発散せずに well-defined であるということは，負の方向に無限に強いドリフトを持っており，これが正なジャンプを打ち消していることが必要である．\nそれ故，ジャンプの隙間では負方向のドリフトが競り勝ち，全体としては単調増加にならない．特に，任意の区間において単調増加にならない．17\n\n\n2.6 C 型ならば非有界変動である\n\n\n\n\n\n\n命題（見本道の変動）18\n\n\n\nLevy 過程 \\(X\\) について，\n\nA 型または B 型ならば，有界変動過程である．すなわち，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動である．\nC 型ならば，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "href": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "3 従属過程と Gamma 過程",
    "text": "3 従属過程と Gamma 過程\n\n3.1 導入\nGamma 過程は，拡散項もドリフト \\(\\gamma_0\\) も持たない，純粋跳躍な従属過程である．\nしかし，正のジャンプのみをもち，ジャンプだけで増加していく過程だからと言って，その見本道は区分的に定数ではない．\nその Lévy 測度は \\(\\nu((0,\\infty])=\\infty\\) を満たし，B 型に分類される．従って，\\(\\mathbb{R}_+\\) の稠密部分集合上でジャンプしており，見本道は殆ど確実に，任意の点 \\(t\\in\\mathbb{R}_+\\) で非連続である．\nGamma 過程は元々，(Moran, 1959) によりダムの貯水量のモデルとして導入された．\n\n\n\n\n\n\n証明\n\n\n\n\n\n見本道 \\(X_\\bullet(\\omega)\\) は，\\(\\mathbb{R}_+\\) のある稠密部分集合 \\(A\\subset\\mathbb{R}_+\\) 上でジャンプしているとする：\\(\\overline{A}=\\mathbb{R}_+\\)．\nこのとき，\\(\\mathbb{R}_+\\) の任意の点で \\(X_\\bullet(\\omega)\\) は非連続である．\n実際，任意の \\(t&gt;0\\) を取り，ここで連続であるとすると，任意の \\(t\\) への収束列 \\(\\{t_n\\}\\subset\\mathbb{R}_+\\) について，\\(X_{t_n}(\\omega)\\to X_t(\\omega)\\) が成り立つ必要があるが，\\(t\\) は \\(A\\) の触点でもあるので，これに収束する \\(A\\) の点列 \\(\\{t_n\\}\\subset A\\) が取れる．これを特に，下から単調に収束するように取る：\\(t_n\\searrow t\\)．\n\n\n\nしかし，\\(\\nu\\) は平均を持つために有界変動ではあり，実際シミュレーションによって得る見本道を見ても，殆どのジャンプは目に見えない．\n\n\n3.2 Gamma 分布\n\\(\\mathbb{R}\\) 上の Gamma 分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\) とは，密度関数 \\[\ng(x;\\alpha,\\nu):=\\frac{\\alpha^\\nu}{\\Gamma(\\nu)}x^{\\nu-1}e^{-\\alpha x}1_{\\mathbb{R}^+}(x)\n\\] が定める分布をいう．\\(\\alpha\\) をレート，\\(\\nu\\) を形状パラメータというのであった．\n\n  \n    \n      \n      \n        確率測度の変換則\n        Gamma 分布と Beta 分布を例に\n      \n    \n  \n\n\n\n3.3 Gamma 点過程\n\\(\\sigma\\)-有限測度 \\(\\rho_0\\in\\mathcal{P}(E)\\) と Lévy 測度 \\(\\nu:=\\mathrm{Gamma}(\\alpha,0)\\)，すなわち \\[\n\\nu(dr):=\\frac{e^{-\\alpha r}}{r}1_{\\mathbb{R}^+}(r)\\,dr\n\\] について，\\(\\lambda:=\\rho_0\\otimes\\nu\\) で定まる強度測度を持つ \\(E\\times\\mathbb{R}_+\\) 上の Poisson 点過程 \\(\\xi\\) を Gamma 点過程 という．19\nこれは \\[\n\\xi(B)\\sim\\mathrm{Gamma}(\\alpha,\\rho_0(B))\n\\] を満たす複合 Poisson 点過程である．\\(\\rho_0\\) のことを形状測度ともいう．\n\n\n\n\n\n\nDirichlet 過程 (Ferguson, 1973) との関係20\n\n\n\n\n\n\\[\n\\Delta_n:=\\left\\{\\begin{pmatrix}p_1\\\\\\vdots\\\\p_n\\end{pmatrix}\\in[0,1]^n\\,\\middle|\\,\\sum_{i=1}^np_i=1\\right\\}\n\\] を \\(n-1\\)-単体とする．21 この上に台を持つ，パラメータ \\(\\alpha\\in(0,\\infty)^n\\) で定まる密度 \\[\nf(x)=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_n)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_n)}x_1^{\\alpha_1-1}\\cdots x_n^{\\alpha_n-1}1_{\\Delta_n}(x)\n\\] が定める分布 \\(\\mathrm{Dirichlet}(n,\\alpha)\\in\\mathcal{P}(\\Delta_n)\\) を Dirichlet 分布 という．\nここで，\\(E\\) 上の Gamma 点過程 \\(\\xi\\) は \\(\\rho_0(E)&lt;\\infty\\) を満たすとする．このとき，\\(E\\) の分割 \\[\nE=B_1\\sqcup\\cdots\\sqcup B_n\n\\] \\[\n\\rho_0(B_i)&gt;0\n\\] に対して， \\[\n(\\zeta(B_1),\\cdots,\\zeta(B_n))\\sim\\mathrm{Dirichlet}(n,\\alpha)\n\\] \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] が成り立ち，これは \\(\\xi(E)\\) と独立である．\nこのことをふまえて，\\(\\rho_0\\) が有限であるとき，ランダム確率測度 \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] を Dirichlet 過程 という．22\n\n\n\n\n\n3.4 Gamma 点過程の Lévy 測度は \\(0\\) の近傍で発散する\nしかし，\\(\\mathrm{Gamma}(\\alpha,0)\\) などという分布はなく， \\[\n\\nu(\\mathbb{R})=\\int^\\infty_0r^{-1}e^{-\\alpha r}dr=\\infty.\n\\]\nこのとき，任意の \\(\\rho_0\\) で測って正の測度を持つ集合 \\(\\rho_0(B)&gt;0\\;(B\\in\\mathcal{E})\\) に対して，\\(\\xi\\) は殆ど確実に無限個の点を \\(B\\) 内にもつ．23\nしかし，\\(\\rho_0(B)&lt;\\infty\\) ならば \\(\\xi(B)&lt;\\infty\\) ではある．すなわち，ジャンプ幅も含めて足し合わせると，収束する．これは，\\(\\nu\\) が平均を持つことによる： \\[\n\\int_0^\\infty r\\,\\nu(dr)=\\alpha^{-1}.\n\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n  \\xi(B)&=\\int^\\infty_0\\int_Br\\,\\eta(dsdr)\\\\\n  &=\\int_B\\rho_0(ds)\\int^\\infty_0r\\,\\nu(dr)\\\\\n  &=\\rho_0(B)\\alpha^{-1}\n\\end{align*}\\]\n\n\n\n\n\n3.5 従属過程\n一般に，\\(\\xi\\) を \\(\\mathbb{R}^+\\) 上の Lévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を持つ一様な複合 Poisson 点過程，すなわち \\(\\ell_+\\otimes\\nu\\) を強度測度とする \\(\\mathbb{R}_+\\times\\mathbb{R}^+\\) 上の Poisson 点過程とすると， \\[\nY_t(\\omega):=\\xi(\\omega,[0,t])\n\\] で定まる過程 \\(Y\\) は，一般に Lévy 測度 \\(\\nu\\) を持つ 従属過程 (subordinator) という．24\n\n\n3.6 Gamma 計数過程\nLévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を \\[\n\\nu(dr):=\\delta\\frac{e^{-\\gamma r}}{r}dr\n\\] \\[\n\\delta,\\gamma&gt;0,\n\\] で与えたとき，付随する従属過程 \\(\\{Y_t\\}\\) を Gamma 過程 といい，\\(\\mathrm{Gamma}(\\delta,\\gamma)\\) で表す．25\nこれは \\(Y_t\\sim\\mathrm{Gamma}(\\gamma,\\delta t)\\) を満たす Lévy 過程である．\n\n\n\n\n\n\n\n\n\n目視できないジャンプが無数に存在することが窺える．\n\n\n\n3.7 分散 Gamma 過程\n２つの独立な Gamma 過程 \\[\nX^+\\sim\\mathrm{Gamma}(\\delta,\\gamma^-),X^-\\sim\\mathrm{Gamma}(\\delta,\\gamma^+)\n\\] に対して， \\[\nX^0_t=X^+_t-X^-_t\n\\] と表せる Lévy 過程 \\(X^0\\) を 分散 Gamma 過程 という．26\n\n\n\n\n\n\n\n\n\n分散 Gamma 過程は，オプション価格の対数のモデルとして，Brown 運動より柔軟なモデルとしても用いられる (Madan et al., 1998)．\nこれは，Brown 運動の分散が Gamma 分布に従うとして得る過程であるとも見れる．実際，Brown 運動の時間を，Gamma 過程によって変換したものが分散 Gamma 過程である．\n実際，Brown 運動 \\(B\\) とこれと独立な Gamma 過程 \\(T\\) について， \\[\nX^0_t=B_{T_t}\n\\] と表せる．27"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "href": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "4 安定過程と Cauchy 過程",
    "text": "4 安定過程と Cauchy 過程\n\n4.1 安定分布\n\n4.1.1 定義\n\n\n\n\n\n\n定義 (stable)28\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して \\[\nf(t)^n=f(a_nt)e^{ib_nt}\n\\] が成り立つ無限可分分布の特性関数をいう．\n確率変数 \\(Y\\in\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して， \\[\nY_1+\\cdots+Y_n\\overset{\\text{d}}{=}a_nY+b_n\n\\] を満たすことをいう．\n\n\n\nすなわち，安定分布とは， \\[\nZ_n:=\\frac{\\sum_{i=1}^nY_i-b_n}{a_n}\n\\] という形の，独立同分布確率変数の正規化された和の列 \\(\\{Z_n\\}\\) の分布収束極限として現れ得る分布の全体を指すことになる．29\nまた，\\(a_n\\) は \\(a_n=n^{1/\\alpha}\\) という形に限り，この \\(\\alpha\\in(0,2]\\) を 安定指数 という．\n\n\n4.1.2 Lévy 測度の有限性\n安定指数 \\(\\alpha\\in(0,2)\\) を持つ安定分布の Lévy 測度は非有限であり，平均も持たない．\n\n\n\n\n\n\n命題（Lévy 測度の平均）30\n\n\n\n\\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) を \\(\\alpha\\)-安定分布とする．このとき，その Lévy 測度 \\(\\nu\\) について，次は同値：\n\n\\(\\alpha\\in(0,1)\\) である．\n\\(\\nu\\) は \\(B^d\\) 上で平均を持つ： \\[\n  \\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n次も同値：\n\n\\(\\alpha\\in(1,2)\\) である．\n\\(\\nu\\) は \\(\\mathbb{R}^d\\setminus B^d\\) 上で平均を持つ： \\[\n  \\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n\n\n\n\n\n4.2 回転対称な安定分布\n\n4.2.1 特性関数の表示\n安定分布は無限可分であるため，Lévy-Khintchin 分解を通じた特性関数の形が特徴付けられる．\n中でも，（回転）対称な安定分布は特に簡単な表示を持つ：\n\n\n\n\n\n\n命題 (Lévy-Khinchin 表示)31\n\n\n\n\\(P\\in\\mathcal{P}(\\mathbb{R}^d)\\) は回転対称であるとする．このとき，その特性関数 \\(\\varphi\\) について次は同値：\n\n\\(\\varphi\\) は安定である．\nある \\(c&gt;0\\) と \\(\\alpha\\in(0,2]\\) が存在して， \\[\n\\varphi(u)=e^{-c\\lvert u\\rvert^\\alpha}.\n\\]\n\nこの \\(\\alpha\\) を 安定指数 という．\n\n\n\n\n\n\n\n\n例（対称な安定分布）\n\n\n\n\n\\(\\alpha=2\\) の対称安定分布とは，中心化された正規分布である．\n\\(\\alpha=1\\) の対称安定分布とは，中心化された Cauchy 分布である．\n\n\n\n\n\n\n\n\n\n中心極限定理のスケーリングレートとしての安定指数\n\n\n\n\n\n\\(a_n\\) は従って，中心極限定理を実現するために必要なスケーリングレートを表す．\nこのことは，一般のエルゴード的な定常過程に対して一般化できる：32\n\\(\\{X_n\\}\\) を \\(\\alpha\\)-撹拌的な定常過程，\\(\\{a_n\\}\\subset\\mathbb{R}^+\\) を発散列とし， \\[\n\\frac{1}{a_n}\\sum_{j=1}^nX_j-b_n\n\\] は弱収束するとする．この極限分布は安定分布になり，安定指数を \\(\\alpha\\) とする．\nこのとき，(Karamata, 1933) の意味で緩変動な関数 \\(h\\) に対して，\\(a_n=n^{1/\\alpha}h(n)\\) と表せる： \\[\n\\lim_{n\\to\\infty}\\frac{h(tn)}{h(n)}=1\\quad(t&gt;0).\n\\]\n\n\n\n\n\n4.2.2 自己相似性\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) は自己相似性を持つ．\n一般に，Hurst 指数 \\(H&gt;0\\) に関して自己相似的 (self-similar) であるとは，任意の \\(a&gt;0\\) について \\[\n(Y_{at})\\overset{\\text{d}}{=}(a^HY_t)\n\\] を満たすことをいう．\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) については，\\(H=\\alpha^{-1}\\) と取れる．\nBrown 運動は \\(H=1/2\\) について自己相似である．\nまた，自己相似な Lévy 過程は，狭義の安定過程に限る．33\n\n\n\n4.3 安定従属過程\n\\(\\alpha\\in(0,1)\\) の安定指数を持つ安定過程は，従属過程になる．34\n\n\n\n\n\n\n例（Lévy 従属過程）35\n\n\n\n\n\nLévy 分布 \\(\\mathrm{Levy}(c):=\\mathrm{IG}(c^{1/2},0)\\) とは，密度 \\[\nf(x;c):=\\sqrt{\\frac{c}{2\\pi}}x^{-\\frac{3}{2}}e^{-\\frac{c}{2x}}1_{\\mathbb{R}^+}(x)\n\\] を持つ \\(\\mathbb{R}\\) 上の分布をいう．\nこれは次の特性関数を持ち，安定指数 \\(\\alpha=1/2\\) を持つ非対称な安定分布である： \\[\n\\varphi(u)=\\exp\\left(-\\sqrt{c\\lvert u\\rvert}\\biggr(1-i\\operatorname{sgn}(u)\\biggl)\\right).\n\\]\n安定指数 \\(1/2\\) の安定従属過程 \\(T\\) は Lévy 従属過程 とも呼ばれ， \\[\nT_t\\sim\\mathrm{Levy}(t^2/2)\n\\] を満たす．\nこれは，１次元 Brown 運動の到達時刻 \\[\nT_t:=\\inf\\left\\{s&gt;0\\mid B_s=\\frac{t}{\\sqrt{2}}\\right\\}\n\\] の過程として現れる．\nLévy 過程は逆正規過程の特殊な場合であり，これは一般の Gauss 過程の到達時刻の過程として現れる．36\n\n\n\n\n\n\n\n\n\n例（安定従属過程による従属操作）37\n\n\n\n\n\n\\(\\{\\tau_t\\}_{t\\in\\mathbb{R}_+}\\) を安定指数 \\(\\alpha\\in(0,1)\\) を持つ安定従属過程とする．\nこれと独立な Lévy 過程 \\(X\\) に対して，従属化 \\[\nt\\mapsto X_{\\tau_t}\n\\] は再び Lévy 過程である．\n特に，\\(X\\) を Brown 運動 \\(B\\) とすると，\\(B_{\\tau}\\) は安定指数 \\(2\\alpha\\) を持つ安定過程になる．\n例えば \\(\\tau_a\\) として \\[\nT_a:=\\inf\\left\\{t\\in\\mathbb{R}_+\\mid B_t=a\\right\\}\n\\] と取ると，これは安定指数 \\(1/2\\) を持つ安定従属過程（Lévy 従属過程）の修正である．38\nこれより，各 \\(a\\in\\mathbb{R}_+\\) への到達時刻で止めた Brown 運動の過程 \\(a\\mapsto B_{T_{a+}}\\) は対称な Cauchy 過程になる．\n\n\n\n\n\n4.4 Cauchy 過程\nCauchy 過程は安定指数 \\(\\alpha=1\\) を持つ狭義の対称安定過程である．39\n拡散項を持たないが，Lévy 測度は平均を持たず（命題 4.1.2），C 型の Lévy 過程である．\nすなわち，殆ど確実に，任意の区間上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#footnotes",
    "href": "posts/2024/Process/Levy.html#footnotes",
    "title": "Lévy 過程を見てみよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Nualart and Nualart, 2018, p. 158) 定義9.1.1，(Sato, 2013, p. 3) 定義1.6，(Rocha-Arteaga and Sato, 2019, pp. 12–13) 定義1.31に倣った．(Protter, 2005, p. 20) では(2)の\\(D\\)-過程という部分がないのみで，定理30 (Protter, 2005, p. 21) で常に\\(D\\)-修正が取れることを示している．(Le Gall, 2016, p. 175) 6.5.2節も同様の取り扱いである．(伊藤清, 1991, p. 306) は時間的一様性を所与のものとはせず，(1), (2), (3), (5)のみをLévy過程の定義としており，さらに(4)も満たすものを 一様Lévy過程 という．(Baudoin, 2014, pp. 89–90) 定義3.40では(5)がない．(Böttcher et al., 2013, p. 14) 例1.17では(1),(2)がない．(Osswald, 2012, pp. 258–259) は(1), (3), (4)を定義としている．(Applebaum, 2009, p. 43) は(1), (3), (4), (5)を定義としている．(佐藤健一, 1990) では全く同じものを加法過程と呼ぶが，(佐藤健一, 2011) は完全に一致する語用法をする（加法過程に確率連続性を課している点を除く）．↩︎\n(Sato, 2013, p. 47) 定理9.1 参照．↩︎\n(Sato, 2013, p. 51) 定理9.7参照．↩︎\n(Dudley, 2002, p. 327) 定理9.8.3，(Sato, 2013, p. 37) 定理8.1，(Rocha-Arteaga and Sato, 2019, p. 11) 定理1.28，(Baudoin, 2014, p. 91) 定理3.46，(Applebaum, 2009, p. 29) 定理1.2.14 など参照．↩︎\nGauss 共分散の用語は (Sato, 2013, p. 38) 定義8.2．Khintchine 測度は (Loéve, 1977, p. 343)，(Applebaum, 2009, p. 31)，(Böttcher et al., 2013, p. 33)，(Baudoin, 2014, p. 92) など．↩︎\n(Loéve, 1977, p. 343)↩︎\n(Sato, 2013, p. 39) 注8.4．↩︎\n特性測度の名前は (Revuz and Yor, 1999, p. 478) 演習 XII.1.18 など．命題は (Sato, 2013, p. 53) 注9.9も参照．↩︎\n(Sato, 2013, p. 120) 定理19.2より．(Protter, 2005, p. 31) 定理42 は Lévy 過程に限って示している．(1)は (伊藤清, 1991, p. 313) 補題5.3でも解説されている．(Protter, 2005, p. 26)定理35も参照．↩︎\nこの分類は (Sato, 2013, p. 65) 定義11.9に倣った．↩︎\n(Sato, 2013, p. 140) 定理21.9 参照．↩︎\n(Sato, 2013, p. 135) 定理21.1．↩︎\n(Lowther, 2011) 定理１，(Sato, 2013, p. 135) 定理21.2．↩︎\n(Sato, 2013, p. 136) 定理21.3．↩︎\n(Applebaum, 2009, p. 52)，(Baudoin, 2014, p. 95) 定義3.50，(Sato, 2013, p. 137) 定義21.4，(Iacus and Yoshida, 2018, p. 171) に倣った．(Kingman, 1992, p. 88) 8.4節，(Last and Penrose, 2017, p. 156) 例15.7 は命題の条件2の方を定義に用いている．↩︎\n(Sato, 2013, p. 137) 定理21.5．↩︎\n(Sato, 2013, p. 138) も参照．↩︎\n(Lowther, 2011) 定理２，(Sato, 2013, p. 140) 定理21.9．↩︎\n定義は (Last and Penrose, 2017, p. 155) 例15.6 に倣った．↩︎\n(Ghosal and van der Vaart, 2017, p. 562) 命題G.2.(i)，(Last and Penrose, 2017, p. 163) 演習15.1，(Kingman, 1992, pp. 92–) 9.2節．↩︎\n\\(n=2\\) を取ると１単体（線分），\\(n=3\\) と取ると２単体（三角形）を得る．↩︎\n(Kingman, 1992, p. 93)，(Ghosal and van der Vaart, 2017, p. 59) 定義4.1．↩︎\n\\(\\lambda(B)=\\rho_0(B)\\nu(\\mathbb{R})=\\infty\\) となるためである．(Last and Penrose, 2017, p. 163) 演習15.2も参照．↩︎\n(Kingman, 1992, p. 88) 8.4節，(Last and Penrose, 2017, p. 156) 例15.7 などの用語法．一般に subordinator とは，単調増加な Lévy 過程をいう (Sato, 2013, p. 137) 定義21.4，(Baudoin, 2014, p. 95) 定義3.50，(Iacus and Yoshida, 2018, p. 171)．これは，時間変数に関する変数変換を subordination と呼び，その際の変数変換に使えるためである．↩︎\n記法は (Iacus and Yoshida, 2018) による．(Applebaum, 2009, pp. 54–55) 例1.3.22，(Protter, 2005, p. 33) 例４も参照．↩︎\n(Iacus and Yoshida, 2018, p. 160) に倣った．↩︎\n(Lowther, 2011)，(Applebaum, 2009, p. 59) 例1.3.31 も参照．↩︎\n(Revuz and Yor, 1999, p. 116) 定義III.4.1，(Sato, 2013, p. 69) 定義13.1，(Shiryaev, 2016, p. 416) 定義3.6.2，(Loéve, 1977, p. 338)．↩︎\n(Shiryaev, 2016, p. 416) 定理3.6.3 も参照．↩︎\n(Sato, 2013, p. 80) 命題14.5．↩︎\n(Sato, 2013, p. 86) 定理14.14．(Shiryaev, 2016, p. 419) 定理3.6.4，(Loéve, 1977, p. 339)，(Dudley, 2002, p. 328) 定理9.8.4 は \\(d=1\\) の場合．↩︎\n(Ibragimov and Linnik, 1971, p. 316) 定理18.1.1 も参照．↩︎\n狭義の安定過程とは，\\(b_n\\equiv0\\) と取れることをいう (Sato, 2013, p. 69) 定義13.1．(Embrechts and Maejima, 2002)，(Applebaum, 2009, p. 51) 例1.3.14 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Sato, 2013, p. 138) 例21.7，(Applebaum, 2009, p. 53) 例1.3.18 も参照．↩︎\n(Applebaum, 2009 @/53) 例1.3.19 も参照．↩︎\n(Applebaum, 2009, p. 54) 例1.3.21 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Rogers and Williams, 2000, p. 133) も参照．↩︎\n(Revuz and Yor, 1999, p. 107) 命題III.3.9 も参照．↩︎\n(Sato, 2013, p. 87) 例14.17．↩︎"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html",
    "href": "posts/2024/Process/ZigZag.html",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "1 Zig-Zag 過程",
    "text": "1 Zig-Zag 過程\n\n1.1 導入\n１次元の Zig-Zag 過程は元々，Curie-Weiss 模型 における Glauber 動力学を lifting により非可逆化して得る Markov 連鎖の，スケーリング極限として特定された Feller-Dynkin 過程である (Bierkens and Roberts, 2017)．\n区分確定的 Markov 過程（PDMP）といい，ランダムな時刻にランダムな動きをする以外は，決定論的な動きをする過程である．\n\n\n\n\\(\\mathbb{R}^2\\) 上の Gauss 分布に収束する Zig-Zag 過程の軌跡\n\n\nPDMP の一般論については次の記事も参照：\n\n    \n        \n            \n            \n                純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n                ジャンプと確定的な動きによる新たな MCMC 手法\n            \n        \n    \n\nただし Zig-Zag 過程は，(Goldstein, 1951) で電信方程式と関連して，同様の過程が扱われた歴史もある．\n\n\n1.2 設定\nZig-Zag 過程 \\(Z=(X,\\Theta)\\) の状態空間は，力学的な立場に立って \\(E=\\mathbb{R}^d\\times\\{\\pm1\\}^d\\) と見ることが多い．\n力学における相空間と同様，\\(X\\in\\mathbb{R}^d\\) が位置を，\\(\\theta\\in\\{\\pm1\\}^d\\) が速度を表すと解する．すなわち，Zig-Zag 過程は全座標系と \\(45\\) 度をなす方向に，常に一定の単位速さで \\(\\mathbb{R}^d\\) 上を運動する粒子とみなせる．\nすなわち，\\((x,\\theta)\\in E\\) から出発する Zig-Zag 過程は，次の微分方程式系で定まる決定論的なフロー \\(\\phi_{(x,\\theta)}:\\mathbb{R}\\to\\mathbb{R}^d\\) に従って運動する粒子とみなせる： \\[\n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\\qquad \\frac{d \\Theta_t}{d t}=0.\n\\]\n\n\n1.3 アルゴリズム\nZig-Zag 過程 \\(Z\\) は次のようにしてシミュレーションできる：\n\n\n\n\n\n\nZig-Zag 過程のシミュレーション\n\n\n\n\nレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) から定まる強度関数 \\[\nm_i(t):=\\lambda_i(x+\\theta t,\\theta),\\qquad i\\in[d],\n\\] を持つ，\\(d\\) 個の独立な \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程の，最初の到着時刻 \\(T_1,\\cdots,T_d\\) をシミュレーションする．\n最初に到着した座標番号 \\(j:=\\operatorname*{argmin}_{i\\in[d]}T_i\\) について，時刻 \\(T_j\\) に速度成分 \\(\\theta_j\\) の符号を反転させる．すなわち，関数 \\[\nF_j(\\theta)_i:=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n\\] に従ってジャンプする．\n１に \\(t=T_j\\) として戻って，くり返す．\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の跳躍測度\n\n\n\n\n\nもう一つ，MCMC の文脈で自然な見方は，状態空間を \\[\nE=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\}\n\\] と取る見方である．これは (Davis, 1993) による 一般の PDMP の設定 と対応する．\nこの \\(E\\) 上で，レート関数 \\[\n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n\\] が定める強度 \\[\nM(t):=\\lambda(x+t\\theta,\\theta)\n\\] を持った \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程に従ってジャンプが訪れる．\nこの点過程に対して，確率核 \\[\nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\\] に 印付けられた点過程 が，\\(Z\\) の跳躍測度である．\n\n\n\n\n\n\n\n\n\n証明（２つの定義の等価性）\n\n\n\n\n\nZig-Zag 過程に対する２つの定義を与えたが，これら２つが同分布の過程を定めることは証明が必要である．\nまず，\\(\\min_{i\\in[d]}T_i\\) が，強度関数 \\(M\\) が定める到着時刻に同分布であることを示す．\n各 \\(T_i\\) の密度は \\[\np_i(t)=m_i(t)e^{-M_i(t)}1_{(0,\\infty)}(t)\n\\] で与えられ，\\(T_i\\) は互いに独立だから，\\((T_1,\\cdots,T_d)\\) の結合密度もわかる．\n\\(T_1,\\cdots,T_d\\) を昇順に並べた順序統計量を \\[\nT_{(1)}\\le\\cdots\\le T_{(d)}\n\\] で表すとする．この \\(d\\) 次元確率ベクトルの密度 \\(p\\) は， \\[\np(t_1,\\cdots,t_d)=1_{\\left\\{t_1\\le\\cdots\\le t_d\\right\\}}(t_1,\\cdots,t_d)\\left(\\sum_{\\sigma\\in\\mathfrak{S}_d}\\prod_{i=1}^dm_i(t_{\\sigma(i)})e^{-M_i(t_{\\sigma(i)})}\\right)\n\\] と計算できる．\nこの \\(p\\) を \\(t_2,\\cdots,t_d\\) に関して積分することで，\\(T_1\\) の密度が得られる：1 \\[\\begin{align*}\n    p_{(1)}(t)&=\\int_{(0,\\infty)^{d-1}}p(t_1,\\cdots,t_d)\\,dt_2\\cdots dt_d\\\\\n    &=\\biggr(\\sum_{i=1}^dm_i(t_1)\\biggl)\\exp\\left(-\\sum_{i=1}^dM_i(t_1)\\right)=m(t_1)e^{-M(t_1)}.\n\\end{align*}\\]\nこれは確かに，強度関数 \\(m\\) が定める到着時刻の密度である．\n続いて，\\(j=\\operatorname*{argmin}_{i\\in[d]}T_i\\) の，\\(\\min_{i\\in[d]}T_i\\) に関する条件付き確率質量関数が \\[\nq(i|t)=\\frac{m_i(t)}{\\sum_{i=1}^dm_i(t)}\n\\] であることを示す．\nそのためには，任意の \\(i\\in[d]\\) と \\(A\\in\\mathcal{B}(\\mathbb{R}^+)\\) とに関して \\(\\left\\{T_{(1)}\\in A,T_{(1)}=T_i\\right\\}\\) という形の事象を計算し，密度が積の形で与えられることを見れば良い．\n\\[\\begin{align*}\n    &\\qquad\\operatorname{P}[T_{(1)}\\in A,T_{(1)}=T_i]\\\\\n    &=\\operatorname{P}[T_i\\in A,\\forall_{j\\ne i}\\;T_i\\le T_j]\\\\\n    &=\\int_Ap_i(t_i)\\,dt_i\\left(\\sum_{\\sigma\\in\\mathrm{Aut}([d]\\setminus\\{i\\})}\\int^\\infty_{t_i}p_{\\sigma(1)}(t_{\\sigma(1)})\\,dt_{\\sigma(1)}\\int^\\infty_{t_{\\sigma(1)}}p_{\\sigma(2)}(t_{\\sigma(2)})\\,dt_{\\sigma(2)}\\cdots\\int^\\infty_{t_{\\sigma(d-1)}}p_{\\sigma(d)}(t_{\\sigma(d)})\\,dt_{\\sigma(d)}\\right)\\\\\n    &=\\int_Am_i(t_i)\\exp\\left(-\\sum_{i=1}^dm_i(t_i)\\right)\\,dt_i\\\\\n    &=\\int_A\\frac{m_i(t_i)}{m(t_i)}m(t_i)e^{-M(t_i)}\\,dt_i.\n\\end{align*}\\]\nよって，\\(\\min_{i\\in[d]}T_i\\) と \\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) とに関する結合密度は，2 \\[\nq(i|t)p_{(1)}(t)\n\\] という積の形で与えられることがわかった．\n\n\n\n\n\n\nまとめ\n\n\n\n\n前述の定義は，\\(\\min_{i\\in[d]}T_i\\) の形で密度 \\(p_{(1)}\\) からシミュレーションし，\\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) の形で \\(q\\) からシミュレーションしている．\n後述の定義は，\\(p_{(1)}(t)\\) から直接シミュレーションし，再び \\(q(i|t)\\) から直接シミュレーションをする．\n\n１が２に等価であることがわかった．\n\n\n\n\n\n\n1.3.1 到着時刻 \\(T_i\\) のシミュレーション方法\n\\[\nM_i(t):=\\int^t_0m_i(s)\\,ds\n\\] と定めると，指数分布確率変数 \\(E_i\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1)\\) について \\[\nT_i\\overset{\\text{d}}{=}M_i^{-1}(E_i)\n\\] が成り立つ．\n仮にこの逆関数 \\(M_i^{-1}\\) が得られない場合でも，剪定 (Lewis and Shedler, 1979) によって \\(T_i\\) は正確なシミュレーションが可能である．\n\n\n\n1.4 レート関数の条件\nZig-Zag 過程 \\(Z\\) がどのような分布に従うかは，全てレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) に委ねられている．\n\n\n\n\n\n\nZig-Zag 過程のレート関数 \\(\\lambda_1,\\cdots,\\lambda_d:E\\to\\mathbb{R}_+\\) は，負の対数密度 \\(U\\in C^1(\\mathbb{R}^d)\\) に対して， \\[\n\\lambda_i(x,\\theta):=(\\theta_i\\partial_iU(x))_++\\gamma_i(x,\\theta_{-i})\\quad(i\\in[d])\n\\] と定める．\nただし，次を仮定する：\n\n\\(\\gamma_i:E\\to\\mathbb{R}_+\\) は，\\(\\theta_i\\) のみには依らない任意の非負連続関数3 \\[\n  \\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta)).\n  \\]\n\\(e^{-U}\\in\\mathcal{L}^1(\\mathbb{R}^d)\\) が成り立ち，\\(\\pi(dx)\\,\\propto\\,e^{-U(x)}dx\\) が確率測度を定める．\n\\(M_i\\) は \\(t\\to\\infty\\) の極限で発散する： \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\]\n\n\n\n\n\n\n\n\n\n\n注（細かい条件たちについて）\n\n\n\n\n\nまた， \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\] は \\(t\\to\\infty\\) の極限で発散する必要がある．\nさもなくば，\\(M_i:(0,L)\\to(0,\\infty)\\;(L\\in(0,\\infty])\\) の形で定まらず，\\(M_i\\) がこのような可微分同相を与えない場合は \\[\nT_i:=M_i^{-1}(E_i),\\qquad E_i\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1),\n\\] によるシミュレーションも不正確になる．\n\n\n\n\n\n\n\n\n\n(Bierkens et al., 2019, pp. 1294 定理2.2)\n\n\n\n上述のリフレッシュレート \\(\\lambda_1,\\cdots,\\lambda_d\\) に対して，定義 1.3 で定まる Zig-Zag 過程 \\(Z\\) は次の分布 \\(\\widetilde{\\pi}=\\pi\\otimes\\mathrm{U}(\\{\\pm1\\}^{d})\\in\\mathcal{P}(E)\\) を不変にする： \\[\n\\widetilde{\\pi}(dxd\\theta)=\\frac{1}{2^d}\\frac{e^{-U(x)}}{\\mathcal{Z}}\\,dxd\\theta\n\\]\n\n\n\n\n\n\n\n\n注（拡張の可能性について）\n\n\n\n\n\n\\(\\{\\pm1\\}^d\\) 上の周辺分布が一様分布になっていること，勾配ベクトル \\(DU\\) の情報のみを使っており，座標に沿った方向しか見ていないため \\(U\\) の異方性に大きく左右されること，これらが「必ずしもそうある必要はない」拡張可能な点である．\n\n\n\n\n\n1.5 エルゴード性の条件\n\\(\\pi\\) が不変分布になるための十分条件 1.4 は極めて緩かったが，MCMC として使えるためにはエルゴード性が成り立つ必要がある．\n\n\n\n\n\n\n(Bierkens and Roberts, 2017 定理５)\n\n\n\n\\(d=1\\) で，レート関数 \\(\\lambda:E\\to\\mathbb{R}_+\\) はある \\(x_0&gt;0\\) が存在して次を満たすとする： \\[\n\\inf_{x\\ge x_0}\\lambda(x,1)&gt;\\sup_{x\\ge x_0}\\lambda(x,-1),\n\\] \\[\n\\inf_{x\\le-x_0}\\lambda(x,-1)&gt;\\sup_{x\\le-x_0}\\lambda(x,1).\n\\]\nこのとき，ある関数 \\(f:E\\to[1,\\infty)\\) が存在して \\(f(x,\\theta)\\to\\infty\\;(\\lvert x\\rvert\\to\\infty)\\) が成り立ち，かつ \\[\n\\left\\|P^t\\left((x,\\theta),-\\right)-\\pi\\right\\|_\\mathrm{TV}\\le\\kappa f(x,\\theta)\\rho^t,\\qquad(x,\\theta)\\in E,t\\ge0,\\rho\\in(0,1).\n\\]"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#subsampling-technology",
    "href": "posts/2024/Process/ZigZag.html#subsampling-technology",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "2 Subsampling Technology",
    "text": "2 Subsampling Technology\n\n    \n        \n            \n            \n                Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n                大規模モデル・大規模データに対する MCMC を目指して"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#シミュレーション",
    "href": "posts/2024/Process/ZigZag.html#シミュレーション",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "3 シミュレーション",
    "text": "3 シミュレーション\n\n3.1 １次元での例\nZigZag サンプラーは非対称なダイナミクスを持っており，その点が MALA (Metropolis-adjusted Langevin Algorithm) や HMC (Hamiltonian Monte Carlo) などの従来手法と異なる．\n１次元でその違いを確認するために，Cauchy 分布という裾の重い分布を用いる．Cauchy 分布 \\(\\mathrm{C}(\\mu,\\sigma)\\) は次のような密度を持つ： \\[\nf(x)=\\frac{1}{\\pi\\sigma}\\frac{1}{1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n\\]\nその裾の重さ故，平均も分散も存在しない（発散する）．\nこのとき，次のような観察が得られる：\n\n\n\n\n\n\n\nZigZag サンプラーは Cauchy 分布に対して，最頻値から十分遠くから開始しても，高速に最頻値に戻ってくる．\nMALA は diffusive behaviour が見られ，最頻値に戻るまでに時間がかかる．\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する ZigZag サンプラーの軌道\n\n\nMALA と比較すると，その再帰の速さが歴然としている：4\n\n\nCode\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing Distributions\nusing Random\n\nRandom.seed!(2)\n\n# Define the target distribution (1D Cauchy) using the `LogDensityProblem` interface\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Choose initial parameter value for 1D\ninitial_θ = [500.0]\n\n# Use automatic differentiation to compute gradients\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensityCauchy(0.0, 1.0))\n\n# Set up the sampler with a multivariate Gaussian proposal.\nσ² = 100\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\n# Sample from the posterior.\nchain = sample(model_with_ad, spl, 2000; initial_params=initial_θ, chain_type=StructArray, param_names=[\"θ\"])\n\n# plot\nθ_vector = chain.θ\nsample_values = zip(1:length(θ_vector), θ_vector)\np2 = plot_1dtraj(sample_values, title=\"1D MALA Sampler (Cauchy Distribution)\", markersize=0, ylim=(-30, 750), label=\"MALA_1D\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する MALA サンプラーの軌道\n\n\n２つを並べて比較すると，Langevin ダイナミクスの方は，少し diffusive な動き（random walk property と呼ばれる）が見られることがわかる．\n\n\n\n\n\n\n例（NUTS サンプラーの動き）\n\n\n\n\n\nNUTS サンプラーはステップサイズを極めて大きくするため，プロットによるダイナミクスの比較があまり意味を持たなくなってくる．\n実際見てみると恐ろしいものである：\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する NUTS サンプラーの軌道\n\n\n\n\n\n\n\n3.2 ２次元での例\n\n\n\n\n\n\n\n勾配 \\(-\\nabla\\log\\pi\\) を計算し，∇ϕ(x, i, Γ) の形で定義する．\n\n\n\n\n\\[\n\\Sigma^{-1}=\\begin{pmatrix}2&-1\\\\-1&2\\end{pmatrix}\n\\]\nで定まる分散共分散行列 \\(\\Sigma\\) を持った中心化された正規分布 \\(\\pi(x)dx=\\mathrm{N}_2(0,\\Sigma)(dx)\\) に対しては，対数尤度は\n\\[\\begin{align*}\n    \\log\\pi(x)&=-\\log\\left((2\\pi)^{d/2}(\\det\\Sigma)^{1/2}\\right)\\\\\n    &\\qquad\\quad-\\frac{1}{2}x^\\top\\Sigma^{-1}x\n\\end{align*}\\]\nであるから，\\(\\phi:=-\\log\\pi\\) の第 \\(i\\) 成分に関する微分は\n\\[\\begin{align*}\n    \\partial_i\\phi(x)&=\\frac{\\partial }{\\partial x_i}\\biggr(\\frac{1}{2}x^\\top\\Sigma^{-1}x\\biggl)\\\\\n    &=\\Sigma^{-1}x.\n\\end{align*}\\]\n\nusing ZigZagBoomerang\nusing SparseArrays\n\nd = 2\n\n# 対数尤度関数 ϕ の第 i 成分に関する微分を計算\n1Γ = sparse([1,1,2,2], [1,2,1,2], [2.0,-1.0,-1.0,2.0])\n2∇ϕ(x, i, Γ) = ZigZagBoomerang.idot(Γ, i, x)\n\n# 初期値\nt0 = 0.0\nx0 = randn(d)\nθ0 = rand([-1.0,1.0], d)\n\n# Rejection bounds\nc = 1.0 * ones(length(x0))\n\n# ZigZag 過程をインスタンス化\nZ = ZigZag(Γ, x0*0)\n\n# シミュレーション実行\nT = 20.0\nzigzag_trace, (tT, xT, θT), (acc, num) = spdmp(∇ϕ, t0, x0, θ0, T, c, Z, Γ; adapt=true)\n\n# 軌跡を離散化\ntraj = collect(zigzag_trace)\n\n\n1\n\n勾配関数∇ϕの計算のためには，共分散行列の逆（精度行列ともいう）をSparseMatrixCSC型で指定する必要があることに注意．idotの実装 も参照．\n\n2\n\nidotは，疎行列Γの第i列と，疎ベクトルxとの内積を高速に計算する関数．\n\n\n\n\n\n\n\n\n\n\nidotの定義\n\n\n\n\n\nidot(A,j,u)は，疎行列Aの第j列と，疎ベクトルuとの内積を高速に計算する関数である．\n1function idot(A::SparseMatrixCSC, j, x)\n2    rows = rowvals(A)\n3    vals = nonzeros(A)\n    s = zero(eltype(x))\n4    @inbounds for i in nzrange(A, j)\n5        s += vals[i]'*x[rows[i]][2]\n    end\n    s\nend\n\n1\n\nパッケージ内部で，位置 \\(x\\in\\mathbb{R}^d\\) は全て SparseSate 型に統一されている？\n\n2\n\n疎行列 A の行インデックスを取得．rowvals(A)はベクトルであり，第１列から順番に，非零要素のある行番号が格納されている．\n\n3\n\n非零要素の値が格納されている．\n\n4\n\n@inbounds は，範囲外アクセスを許容するマクロ．高速化のためだろう．nzrange は，A の第 j 列に非零要素がある範囲を，第 \\(1\\) 列から累積して何番目かで返す．すなわち，rows[i]で正確に第j列の非零要素の行番号を狙い撃ちしてイテレーションできる．\n\n5\n\nxの非零要素がある行番号 rows[i] における成分の値 u[rows[i]][2] はこのような表記になる．これと，A の非零要素 vals[i] との内積を計算．\n\n\nなお，通常の行列に対しては，次のように実装されている：\nidot(A, j, x) = dot((@view A[:, j]), x)"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "4 Zig-Zag サンプラーの実装",
    "text": "4 Zig-Zag サンプラーの実装\nZigZagBoomerang の実装を紹介する．\nJulia の MCMC パッケージ一般については次の稿を参照：\n\n    \n        \n            \n            \n                Julia による MCMC サンプリング\n                新時代の確率的プログラミング環境の構築に向けて\n            \n        \n    \n\n\n4.1 ZigZag サンプラーを提供しているパッケージ一覧\n\n4.1.1 Rパッケージ\nJoris Bierkens ら開発の R パッケージ RZigZag (GitHub / CRAN) が最も手軽に実行できる．\n\n\n4.1.2 Juliaパッケージ\n一方で，(Bierkens et al., 2022) では Julia によるパッケージ ZigZagBoomerang（GitHub / ANN / docs） も提供している．名前によらず，2022.1 月リリースの v.0.11 以降は BPS もサポートしている．\nusing Pkg\nPkg.add(\"ZigZagBoomerang\")\n\n\n\n4.2 ZigZagオブジェクト\n\n\n\n\n\n\nZigZag &lt;: ContinuousDynamics  &lt;: Any は次のフィールドを持つ：5\n\nΓ::SparseMatrixCSC：ポテンシャル関数\nμ::Vector：平均\n\n\n\n\n\n\n\n\n\n\n注（ZigZag のメソッド）\n\n\n\n\n\nstruct ZigZag{T,S,S2,R} &lt;: ContinuousDynamics\n    Γ::T\n    μ::S\n    σ::S2\n    λref::R\n    ρ::R\n    ρ̄::R\nend\nZigZag(Γ, μ, σ=(Vector(diag(Γ))).^(-0.5); λref=0.0, ρ=0.0) = ZigZag(Γ, μ, σ, λref, ρ, sqrt(1-ρ^2))\ndynamics.jlにて，現在時刻にランダム時刻τを加算し，位置xを更新するが，θにはまだ触れない．\nfunction move_forward!(τ, t, x, θ, Z::Union{BouncyParticle, ZigZag})\n    t += τ\n    x .+= θ .* τ\n    t, x, θ\nend\n内積∇ϕxを用いて，位置xに対する第i成分の反射を計算する．\nfunction reflect!(i, ∇ϕx::Number, x, θ, F::Union{ZigZag, FactBoomerang})\n    θ[i] = -θ[i]\n    θ\nend\nfunction reflect!(i, ∇ϕx, x, θ, F::Union{ZigZag, FactBoomerang})\n    θ[i] = θ[i] - (2*dot(∇ϕx, θ[i])/normsq(∇ϕx))*∇ϕx\n    θ\nend"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#footnotes",
    "href": "posts/2024/Process/ZigZag.html#footnotes",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n計算過程は省略したが，\\(d=2\\) の場合と，\\(d=3\\) の場合を少しやってみると良い．↩︎\n参照測度は，\\([d]\\) 上のものは計数測度 \\(\\#\\) をとっている．↩︎\n従って，レート関数 \\(\\lambda\\) は連続とする．この関数 \\(\\gamma_i\\) は，\\(U\\) の情報には依らない追加のリフレッシュ動作を仮定に加える．実際，\\(\\lambda_i(x,\\theta)-\\lambda_i(x,F_i(\\theta))=\\theta_i\\partial_iU(x)\\) である限り，\\(\\theta\\) と \\(F_i(\\theta)\\) の往来には影響を与えず釣り合っているため，どのような \\(\\gamma_i\\) をとっても，平衡分布には影響を与えない．しかし，高くするごとにアルゴリズムの対称性が上がるため，\\(\\gamma\\equiv0\\) とすることが Monte Carlo 推定量の漸近分散を最小にするという (Andrieu and Livingstone, 2021)．(Bierkens et al., 2021) でも同様の洞察がなされている．↩︎\n(Bierkens et al., 2019) にある提示の仕方である．Zig-Zag の 2000 単位時間を単純に MALA と比較はできないと筆者も考えるが，ダイナミクスに注目していただきたい．実際，自分で実装してみると，シード値をいじらないと，Zig-Zag は必ずしも 500 単位時間前後でモード \\(0\\) に戻るわけではない．↩︎\n実は６つ持つ．他の初期値は σ=(Vector(diag(Γ))).^(-0.5); λref=0.0, ρ=0.0↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes for Self Study",
    "section": "",
    "text": "サーベイ | レビュー | カテゴリ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルの実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散過程\n\n\n\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\n拡散模型にヒントを得た輸送によるサンプリング法\n\n\n\nSampling\n\n\nProcess\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．このような拡散模型の考え方はサンプリングにも用いることができる．\n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて，GAN の実装の概要を見る．\n\n\n\n\n\n8/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．今回は PyTorch を用いて，DDPM の実装の概要を見る．\n\n\n\n\n\n8/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n多様体学習\n\n\n\n\n\n\nDeep\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元削減と多様体学習がある．一般に表現学習はパラメトリックであるとするならば，次元削減と多様体学習はノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nノイズ対照学習\n\n\n\n\n\n\nDeep\n\n\n\n深層潜在モデルを敵対的に学習させる手法である NCL (Noise-Contrastive Learning) について述べる．\n\n\n\n\n\n7/29/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nffmpeg による音声・動画の変換\n\n\n\n\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n7/18/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析\n\n\n\n\n\n\nBayesian\n\n\nMCMC\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論として用いられる，多次元尺度構成法 (MDS: Multidimensional Scaling) の一手法である． \n\n\n\n\n\n7/16/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\nJulia に存在する MCMC 関連のパッケージをまとめ，多くの MCMC のパッケージを支える，Turing ecosystem の基盤となる抽象的なフレームワーク MCMCChains と AbstractMCMC を概観する．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する Metropolis-Hastings 法と MALA 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する HMC 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\nECMC (Event-chain Monte Carlo) 法は，平衡分布の直接的な評価を一度もすることなく，平衡分布からのサンプリングを達成する新たなモンテカルロ法である．非対称性をもち，従来手法より高い効率を持つ．実際，Metropolis 法の開発以来の興味の対象であった２次元剛体円板系の液相転移のシミュレーションに，約 60 年越しに成功している．\n\n\n\n\n\n6/29/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP HP マニュアル\n\n\n記事の作成から公開まで\n\n\n\nEconomic Security\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n6/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of Applied Probability 53(2016) 410-20] は Metropolis-Hastings アルゴリズムの計算複雑性を論じたもの \n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\nOrnstein-Uhlenbeck 過程は唯一の非自明な定常 Gauss-Markov 過程である．また，連続時間の自己回帰模型を与える重要な拡散過程である．加えて，その遷移半群は解析的な表示を持ち，Malliavin 解析でも基本的な意味を持つ．したがって，直感的な理解を涵養しておくことは非常に見返りが大きいことだろう．そこで，YUIMA パッケージを通じてシミュレーションを行いながら，Ornstein-Uhlenbeck のパラメータの意味と，遷移半群・生成作用素の直感的な理解の醸成を目指す．\n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫博士課程を紹介します，同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷です．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Statistical Science 16(2001) 351-67] は Metropolis-Hastings 法の最適スケーリングに関する結果をまとめ，実際の実装にその知見をどのように活かせば良いかを例示したレビュー論文である． \n\n\n\n\n\n5/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．その方法をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージyuimaは，従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しをすべく，広範な確率微分方程式のシミュレーションと統計推測を可能とするパッケージです．その機能の全貌と，基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行うための R パッケージである．本稿では，brms の基本的な使い方と，その実装を紹介する．また，ランダム効果モデルや一般化推定方程式などの文脈で扱われる種々のモデルを，階層モデルの観点から統一的にレビューし，ベイズ統計学の観点の透徹性を強調する．現代では，まだ複雑で大規模なモデルに対して，完全なベイズ推定を実行するには困難が多く，ベイズのモデリング上の透徹性と，最適化による点推定の計算上の優位性とのバランスを考えることが重要である． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nSDE のベイズ推定を，R パッケージ YUIMA を通じて実行する方法を紹介する．\n\n\n\n\n\n5/12/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n5/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの． \n\n\n\n\n\n4/22/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションにより計算を実行する手段である． \n\n\n\n\n\n4/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n\n\n\n\nReview\n\n\n\n[@Dai+2019] は有限混合で表される分布からのサンプリング法（Fusion 問題）に関する最初の理論解析である． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n[@Fearnhead+2017] は拡散過程を離散化誤差なしにシミュレーションする手法を提案している．逐次重点サンプリング（SIS）の連続時間極限を考えることで，提案過程と重点荷重との組がPDMPとなり，効率的なシミュレーションが可能になる． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\n\n\n\nProcess\n\n\n\nMarkov 過程のエルゴード性の証明は，カップリングの概念を用いれば極めて明瞭に見渡せる．\n\n\n\n\n\n3/25/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論の中心に据えられるべき中心概念である．MCMC も確率核の分析に帰着する．\n\n\n\n\n\n3/24/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの微細化技術をレビューする．\n\n\n\n\n\n3/23/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n\n\n\n\n\n3/13/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\nState vs Loomis 判決を題材に，アルゴリズムと公平性を議論する．\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n転移学習とは\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．\n\n\n\n\n\n3/03/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\nPAC-Bayes は現実的に有用な鋭い PAC bound を得る新たな技術である．最適化の問題に帰着する点が研究を盛り上げている．Vapnik-Chervonenkis 理論の一般化であり，推定量上の確率分布を返すようなより一般的なアルゴリズムに対しても適用できる．\n\n\n\n\n\n3/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP 配信マニュアル\n\n\nシンポジウムでの YouTube 配信のやり方\n\n\n\nEconomic Security\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n2/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP 配信マニュアル\n\n\nシンポジウムでの YouTube 配信のやり方\n\n\n\nEconomic Security\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n2/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，「刑法入門」の内容を扱う．\n\n\n\n\n\n2/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れ，近年の発展と予想される安全保障上の懸念も概観する．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル３\n\n\nグラフィカルモデルの推論\n\n\n\nBayesian\n\n\nComputation\n\n\n\n数学者のために，PGM (Probabilistic Graphical Model) の代表的な推論手法を紹介する．\n\n\n\n\n\n2/17/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n深層学習の学習における確率最適化アルゴリズムに関して概説する．\n\n\n\n\n\n2/16/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n数学者のために，深層生成モデルを概観する．\n\n\n\n\n\n2/13/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析２\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価である．\n\n\n\n\n\n2/11/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する．\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である \\(K\\)-平均法の説明から始める．\\(K\\)-平均法は第一義的にはモデルフリーの（確率論と関係のない）クラスタリングアルゴリズムである．\n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\n\nNicolas Chopin の論文を読んで短くまとめたものです。\n\n\n\n\n\n1/30/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/29/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n粒子フィルターを拡散過程に対して適用することを考える．拡散過程の Euler-Maruyama 離散化に対して構成された粒子フィルターの，タイムステップを \\(0\\) にする極限 \\(\\Delta\\searrow0\\) での振る舞いを議論する．\n\n\n\n\n\n1/23/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n「穴あき式」の考え方\n\n\n\n\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n1/21/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\n\n\n\nProcess\n\n\n\nマルチンゲール問題とは何か？\n\n\n\n\n\n1/20/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装：リサンプリング編\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\n粒子フィルターにおいて，リサンプリングの段階が最も肝要で，効率に大きな影響を与える．本稿では，リサンプリングのアルゴリズムを複数紹介し，比較する．\n\n\n\n\n\n1/14/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\n\n\n\nProcess\n\n\n\n確率過程の離散化に関する漸近論的な結果を，Brown 運動を例に取り示す．\n\n\n\n\n\n1/09/2024\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，\n\n\n\n\n\n1/02/2024\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLaTeX authoring environment with VSCode\n\n\n\n\n\n\nLifestyle\n\n\n\nA page collecting tips to author technical documents with VSCode. Also including a starting guide.\n\n\n\n\n\n12/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\n\n\n\nInformation\n\n\nReview\n\n\n\n\n\n\n\n\n\n12/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nGit覚書\n\n\n\n\n\n\nLifestyle\n\n\n\nGitとGitHubの仕組みを概観した覚書\n\n\n\n\n\n11/27/2023\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\n\n\n\nKernel\n\n\n\n数学者のために，SVMによるデータ解析が何をやっているのかを抽象的に説明する．\n\n\n\n\n\n11/18/2023\n\n\nDraft Draft\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\n1.1節”On the Origins of Feynman-Kac and Particle Models”を翻訳\n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\n\n\n\nReview\n\n\n\n\n\n\n\n\n\n10/29/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia のSymbol型とExpr型，そしてExpr型からExpr型への関数であるマクロを用いたメタプログラミングについて解説する．\n\n\n\n\n\n1/23/2022\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は 2012 年に公開された科学計算向きの動的型付け言語である．\n\n\n\n\n\n9/10/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型宣言\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia では::演算子で型宣言をする．これを省略するとx::Anyの略記とみなされる．\n\n\n\n\n\n9/09/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia において，関数はメソッドの貼り合わせである．スクリプト言語のように，気軽に関数定義を行うこともできれば（単一メソッドによる関数と解す），多重ディスパッチによる実装も可能である．\n\n\n\n\n\n9/08/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は 2012 年に公開された科学計算向きの動的型付け言語である．\n\n\n\n\n\n9/07/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は動的型付け言語で，型宣言は optional であるが，豊かな型システムを持つ．\n\n\n\n\n\n9/06/2020\n\n\n司馬 博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html",
    "href": "static/Sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      5/24/2024.\n      司馬博文 .\n      \n        新時代の MCMC を迎えるために\n        : 連続時間アルゴリズムへの進化\n      .\n      \n        統数研オープンハウス.\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n    \n      2/25/2024.\n      Hirofumi Shiba.\n      \n        A Recent Development of Particle Methods\n        : Inquiry towards a Continuous Time Limit and Scalability\n      .\n      \n        MLSS2024 (OIST, Okinawa, Japan).\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#upcomings-newests",
    "href": "static/Sessions.html#upcomings-newests",
    "title": "Sessions",
    "section": "",
    "text": "連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#presentation-history",
    "href": "static/Sessions.html#presentation-history",
    "title": "Sessions",
    "section": "",
    "text": "5/24/2024.\n      司馬博文 .\n      \n        新時代の MCMC を迎えるために\n        : 連続時間アルゴリズムへの進化\n      .\n      \n        統数研オープンハウス.\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n    \n      2/25/2024.\n      Hirofumi Shiba.\n      \n        A Recent Development of Particle Methods\n        : Inquiry towards a Continuous Time Limit and Scalability\n      .\n      \n        MLSS2024 (OIST, Okinawa, Japan).\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#sec-法律家のための統計数理",
    "href": "static/Sessions.html#sec-法律家のための統計数理",
    "title": "Sessions",
    "section": "法律家のための統計数理",
    "text": "法律家のための統計数理\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n数学と法学，双方からの交流と理解を図ります．\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Sessions.html#empirical-process-theory",
    "href": "static/Sessions.html#empirical-process-theory",
    "title": "Sessions",
    "section": "Empirical Process Theory",
    "text": "Empirical Process Theory\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\nTextbook：Kengo Kato Empirical Process Theory (Lecture Note)\n\n\n\n担当分の発表資料"
  },
  {
    "objectID": "static/Sessions.html#学振-dc1",
    "href": "static/Sessions.html#学振-dc1",
    "title": "Sessions",
    "section": "学振 DC1",
    "text": "学振 DC1\n\n\n\n\n\n\n\n\n\nPeriod\nApplication Category\nSmall Category\n\n\n\n\nSpring, 2024\n解析学、応用数学およびその関連分野\n12040 応用数学および統計数学関連\n\n\n\n\n\n\n\n申請書（最終版，５月19日）"
  },
  {
    "objectID": "static/Notations.html#sec-set",
    "href": "static/Notations.html#sec-set",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "1 集合",
    "text": "1 集合\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nここでは，あらゆる数学概念は，ZFC公理系 の下で集合として定義する．1 記号 \\(:=\\) は「右辺によって左辺を定義し，その結果等号が成り立つ」という主張の略記である．2\n\n1.1 集合\n\n空集合 を \\[\n\\emptyset:=\\{x\\mid x\\ne x\\}\n\\] で表す．3\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．4\n\\(A,B\\subset X\\) の 差 を \\[\nA\\setminus B:=\\left\\{a\\in A\\mid a\\notin B\\right\\}\n\\] で表す．\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cup B\\) と同じ数学的対象であるが，同時に \\(A\\cap B\\) という事実も主張するものとする．5\n対称差 を \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\] で表す．6\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．7 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．8\n例えば，\\(X\\in\\mathcal{L}(\\Omega)\\) を実確率変数，\\(A\\in\\mathcal{B}(\\mathbb{R})\\) を Borel 集合とすると， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] という略記を用いる．\n\n\n\n1.2 構成\n\n自然数 を \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\},\n\\] によって帰納的に定義する．9\n自然数の集合を表すため，次の記法を用意する：10 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，11 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す： \\[\n\\mathbb{R}_+=[0,\\infty),\\quad\\mathbb{R}^+=(0,\\infty).\n\\]\n部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．12\n実数 \\(x\\in\\mathbb{R}\\) に対して，その整数部分を \\[\n\\lfloor x\\rfloor:=\\max\\{n\\in\\mathbb{Z}\\mid n\\le x\\}\n\\] と表す．13\n次の演算規則を約束する：14 \\[\n\\prod_\\emptyset=1,\\quad\\sum_{\\emptyset}=0.\n\\]\n\\(0!=1\\) とする．\n\\(n\\)-組 を次のように帰納的に定める：15 \\[\n(x_1,x_2):=\\{\\{x_1\\},\\{x_1,x_2\\}\\},\n\\] \\[\n(x_1,\\cdots,x_n):=(x_1,(x_2,\\cdots,x_n)).\n\\]\n自然数の組を表すため，次の記法を用意する：16 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組を \\[\nX_{1:N}:=(X_1,\\cdots,X_N)\n\\] と表す．17\n\n\n\n1.3 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n写像 \\(f\\) の 値域 を \\[\\mathrm{Im}\\,f:=f(X)\\] で表す．\n\\(A\\subset X\\) への 制限 を \\(f|_A:A\\to Y\\) と表す．18\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．19\n\\(f_*\\) は部分集合 \\(A\\subset X\\) を像 \\(f(A)\\subset Y\\) に対応させる写像 \\[\nf_*:P(X)\\to P(Y)\\] と定義する．\n同様に写像 \\(f^*:P(Y)\\to P(X)\\) を定める： \\[\nf^*(B)=f^{-1}(B)\\quad(B\\subset Y).\n\\]\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．20\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．21\n写像 \\(f:X\\to Y\\) のうち，有限個の元を除いて \\(f(x)=0\\) を満たすものがなす全体を \\[\nY^{(X)}:=\\left\\{f\\in Y^X\\mid f=0\\;\\;\\text{f.e.}\\right\\}\n\\] と表す．22\n\\(P(X)\\) を \\(2^X\\) と同一視する．特に，\\(X\\) の有限部分集合の全体を \\[\n2^{(X)}=\\left\\{A\\in P(X)\\:\\middle|\\: A\\overset{\\text{finite}}{\\subset}X\\right\\}\n\\] と表す．23\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．24\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\[\n\\mathrm{pr}_i:\\prod_{i\\in I}X_i\\twoheadrightarrow X_i\n\\] で表す．25\n\\(x\\in X\\) での 評価写像 を \\[\n\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\n\\] で表す．26\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．\nしかしこの写像の値域も 族 と呼び，この場合は \\[\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\] と表す．27\n特に \\(I=\\mathbb{N}\\) のときは 列 ともいう．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．28\n\n\n\n1.4 圏\n\n集合の圏 を \\(\\mathrm{Set}\\) で表す．\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X)\n\\] を表す．29\n確率空間と確率核の圏を \\(\\mathrm{Stoch}\\) で表す．30\n圏 \\(C\\) の対象 \\(X,Y\\in C\\) の間の 射 の全体を \\(\\mathrm{Hom}_C(X,Y)\\) で表す．31\n特に \\[\nY^X=\\mathrm{Map}(X,Y)=\\mathrm{Hom}_\\mathrm{Set}(X,Y).\n\\]\n圏 \\(C\\) の対象 \\(X\\in C\\) の自己射の全体を \\[\n\\mathrm{End}_C(X):=\\mathrm{Hom}_C(X,X)\n\\] で表す．\nそのうち可逆なもののなす部分集合を \\(\\mathrm{Aut}_C(X)\\) で表す．集合 \\([n]\\) の 置換群 は \\(\\mathrm{Aut}_\\mathrm{Set}([n])\\) と表せる．\n\n\n\n1.5 関数\n\n\\(R\\) を環とする．\\(f_1,f_2\\in R^X\\) に対して， \\[\n(f_1+f_2)(x):=f_1(x)+f_2(x),\n\\] \\[\n(f_1f_2)(x):=f_1(x)f_2(x)\n\\] で定める演算により \\(R^X\\) も環とみなし，定値関数 \\(f\\equiv a\\in R\\) を \\(R\\) の元と同一視する．32\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を \\[\na\\lor b:=\\sup\\{a,b\\},\n\\] \\[\na\\land b:=\\inf\\{a,b\\},\n\\] で表す．33\n\\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right)\n\\] と表す．34\n\\(0\\) を持つ束においては次の略記を使う：35 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序集合 \\(Y\\) に値を取る関数 \\(f,g\\in\\mathrm{Map}(X,Y)\\) について，\\(f\\le g\\) とは \\[\n\\forall_{x\\in X}\\;f(x)\\le g(x)\n\\] の略記とする．\n同じ条件を，一階の量化記号 \\(\\forall\\) を省略して \\[\nf(x)\\le g(x)\\quad(x\\in X)\n\\] または \\(f\\le g\\) とも略記する．\n\\(Y\\) が束であるとき，この順序により関数の空間 \\(\\mathrm{Map}(X,Y)\\) は束となり，演算 \\(\\land,\\lor\\) が定まる．\n関数の列 \\(\\{f_n\\}\\subset Y^X\\) について，\\(f_n\\nearrow f\\) とは，収束 \\(f_n\\to f\\) だけでなく，\\(\\{f_n\\}\\) が単調増加であることも含意する．36\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\[\nO(g(x))\\;(x\\to x_0)\\] とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．37\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の意味でも使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0\n\\] を満たすこととする．"
  },
  {
    "objectID": "static/Notations.html#sec-space",
    "href": "static/Notations.html#sec-space",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "2 空間",
    "text": "2 空間\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．38\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．39\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．\n閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}\n\\] で表す．\n\n\n\n2.2 線型空間\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．40\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．41 \\(S_n(\\mathbb{R})_+\\) で半正定値なものの全体を表す．42\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}\n\\] とも表す．43\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，44 共役転置を \\(A^*\\) で表す．45 \\(\\mathbb{F}=\\mathbb{C}\\) の場合は \\(A^\\top=A^*\\)．\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．46\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について， \\[\n\\begin{align*}\n  A&+B\\\\\n  &\\quad:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n  \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\},\n\\end{align*}\n\\] と表す．47\n集合 \\(A\\subset X\\) の 凸包 を \\(\\operatorname{Conv}(A)\\) で表す．48\n集合 \\(A\\subset X\\) が生成する部分空間を \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x\n\\] で表す．49\n内積を \\((-|-)\\) で表す．50\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を51 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\operatorname{Tr}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}\n\\] で表す．52\n\n\n\n2.3 Banach 空間\n\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．53 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n特に \\(J\\) が有限であるとき， \\[\n  \\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n  \\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．54\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．55\n距離空間 \\((T,d)\\) の 開球 を \\[\n\\begin{align*}\n  U_\\epsilon(t)&:=U(t;\\epsilon)\\\\\n  &:=\\left\\{s\\in T\\mid d(s,t)&lt;\\epsilon\\right\\}\n\\end{align*}\n\\] で表す．56\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．57\n単位閉球を \\(B:=B(0;1)\\) で表す．\n\\(\\mathbb{R}^n\\) のものである場合は特に \\(B^n\\) とも表す．58\n\\(\\mathbb{R}^n\\) の標準基底を \\[\ne_i=(0,\\cdots,0,1,0,\\cdots,0)\n\\] と表す．59\nBanach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．60\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}\n\\] で表す．61\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) として理解できる数学的対象の別記法と捉えられるように設計する．62\n\n\n2.4 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．63\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．64\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．65\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．66\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．67\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．68\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．69\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．70\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．71\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．72 \\(\\gamma_n:=\\mathrm{N}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n2.5 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．73 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．74\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．75\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．76\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．77\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．78\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．79\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．80\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．81\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．82"
  },
  {
    "objectID": "static/Notations.html#sec-kernel",
    "href": "static/Notations.html#sec-kernel",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 核",
    "text": "3 核\n空間を導入した次は，その射を定義せねばなるまい．\n本節では，\\((E,\\mathcal{E})\\) を 可測空間 とする．83\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 84\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 85\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．86 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．87\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．88\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．89\n\n\n\n3.2 確率分布\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 確率測度 の全体を \\(\\mathcal{P}(E,\\mathcal{E})\\) と書く．\\(E\\) が位相空間であるとき，Borel 確率測度の全体を \\(\\mathcal{P}(E)\\) と略記する．90\n\\(E\\) を位相空間とする．\\((E,\\mathcal{B}(E))\\) 上の Radon 確率測度 の全体を \\[P(E)\\subset\\mathcal{P}(E)\\] で表す．91\n2つの確率分布 \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) の カップリング の全体を \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\] で表す．92\n\\(d\\)-次元 正規分布 を \\[\\mathrm{N}_d(\\mu,\\Sigma)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．93 その密度は \\[\n  \\phi_d(x;\\mu,\\Sigma):=\\frac{d \\mathrm{N}_d(\\mu,\\Sigma)}{d \\ell_d}(x)\n  \\] で表す．\n集合 \\(A\\subset\\mathbb{R}^d\\) 上の 一様分布 を \\[\\mathrm{U}(A)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．\n点 \\(x\\in E\\) 上の Delta 測度 を \\(\\delta_x\\) で表す．94\n確率変数 \\(X\\sim\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の 分布関数 を \\[\n\\begin{align*}\n  F_X(a)&:=F_\\nu(a)\\\\\n  &:=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d]\\\\\n  &\\quad(a=a_{1:d}\\in\\mathbb{R}^d)\n\\end{align*}\n\\] で表す．\n\\(d=1\\) のとき，その一般化逆を \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\}\n\\] \\[\n(u\\in(0,1)^d)\n\\] で表す．95\n\n\n\n3.3 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：96\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．97 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．98\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 99\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．100\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．101\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．102\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．103\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．104\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.4 関数の空間\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．105\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．106\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．107\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．108 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．109\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．110\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．111\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．112\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．113\n\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．114\n\n\n3.5 作用素\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とし，\\(X,Y\\) をノルム空間とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．115\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．116\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．117\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．118\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．119\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．120\n\n作用素 \\(T:X\\to Y\\) と言ったとき，線型写像 \\(T:X\\to Y\\) を指すこととする．121\n\\(X\\) 内の作用素 \\(T:X\\supset\\mathcal{D}(T)\\to Y\\) と言ったとき，ある \\(X\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to Y\\) を指すこととする．122\n有界作用素の全体を \\(B(X,Y)\\) で表す．123 \\(B(X):=B(X,X)\\) とする．\n連続作用素の全体を \\(L(X,Y)\\) で表す．124"
  },
  {
    "objectID": "static/Notations.html#sec-analysis",
    "href": "static/Notations.html#sec-analysis",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 解析",
    "text": "4 解析\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．125\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．126\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．127\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．128 \\(m=1\\) のとき， \\[\n\\operatorname{grad}u:=\\nabla u:=(Du)^\\top=\\begin{pmatrix}\\frac{\\partial u}{\\partial x_1}\\\\\\vdots\\\\\\frac{\\partial u}{\\partial x_n}\\end{pmatrix}\n\\] とも表す．\n発散 を \\[\n\\operatorname{div}u:=\\nabla\\cdot u:=\\operatorname{Tr}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．129\n\\(u\\) が正方行列 \\(M_n(\\mathbb{R})\\)-値であった場合，行成分毎の適用 \\[\n  \\operatorname{div}u:=\\begin{pmatrix}\\operatorname{div}(u_{1-})\\\\\\vdots\\\\\\operatorname{div}(u_{n-})\\end{pmatrix}\n  \\] と解する．\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n  \\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n  \\] と同一視する．130\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n  \\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\operatorname{Tr}(D^2u)\n  \\] で定める．\n\n\n\n4.2 Fourier 変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．131\n符号関数 を \\[\n\\operatorname{sgn}(x):=2H(x)-1\n\\] で定める．132\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 超関数\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．133 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．134\n\n\n\n4.4 確率解析\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．135"
  },
  {
    "objectID": "static/Notations.html#sec-process",
    "href": "static/Notations.html#sec-process",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "5 過程",
    "text": "5 過程\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率変数の収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．136\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．137\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n\n\n\n5.2 確率基底\n（確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．138\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\(E^T\\) に定める写像 \\[\nX_-:\\Omega\\to E^T\n\\] を 転置 と呼ぶ．139\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいい，このような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．140\n\\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．141 ただし，\\(x(0-)=x(0)\\) とする．142\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の 情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系 \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．143\n加えて， \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\] と表す．144\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) とその上の情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) からなる 4-組 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) を 確率基底 という．\n確率基底が 完備 であるとは， \\[\n\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\n\\] を満たすことをいう．145\n\n\n\n5.3 可測性\n\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}}\\) がフィルトレーション \\((\\mathcal{F}_t)\\) に 適合的 であるとは， \\[\nX_t\\in\\mathcal{F}_t\\quad(t\\in\\mathbb{R})\n\\] を満たすことをいう．\n第一種不連続な見本道を持つ適合的な過程の全体を \\(\\mathbb{D}\\) で表す．一方で，càglàd な見本道を持つ適合的な過程の全体を \\(\\mathbb{L}\\) で表す．146\n\n\n\n5.4 停止時147\n\n確率基底 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) 上の 停止時 とは，同じ確率空間 \\(\\Omega\\) 上の可測関数 \\(T:\\Omega\\to[0,\\infty]\\) であって， \\[\n\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t,\\qquad t\\in\\mathbb{R}_+,\n\\] も満たすものをいう．148\n停止時 \\(T\\) までの 情報 とは， \\[\n\\mathcal{F}_T:=\\left\\{A\\in\\mathcal{F}_\\infty\\mid\\forall_{t\\in\\mathbb{R}_+}\\;A\\cap\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t\\right\\}\n\\] で定まる \\(\\sigma\\)-代数をいう．"
  },
  {
    "objectID": "static/Notations.html#終わりに",
    "href": "static/Notations.html#終わりに",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "終わりに",
    "text": "終わりに\n\n本サイトの記法で筆者が最も注意することは，あらゆる記法を背後の数学的消息と調和するように定義するということであった．\nこれにあたり，あらゆる 数学的対象 を集合から構成する立場を取る一方で，理解するにあたっては 集合と写像（または関手）とを厳密に峻別する ということを徹底することを大事にした．\n例えば集合の合併と共通部分に \\(\\cap,\\cup\\) を用いること，直和と直積に \\(\\coprod,\\prod\\) を用いることは，圏論的な双対性を視覚的に認識しながら数学的議論を進めるためである．(斎藤毅, 2009, p. 37) にも詳しく解説されている．\n記法の開発は数学の重要な一部であると筆者は信じているのである．"
  },
  {
    "objectID": "static/Notations.html#footnotes",
    "href": "static/Notations.html#footnotes",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n集合のなす圏 \\(\\mathrm{Set}\\) は数学の基礎付けとして採用するのに極めて良い性質を持つ nLab．↩︎\n(Del Moral and Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) に一致する．\\(\\equiv,\\overset{\\text{def}}{=}\\) などもよく用いられる．(Crisan and Doucet, 2002), (Smith, 2010) では \\(\\overset{\\triangle}{=}\\) も用いられる．ここでは，これらの左右対称な記号は避けた．また，\\(=_{\\text{df}}\\) などを使うものもある (Quine and Szczotka, 1994)．↩︎\n(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 2) の定め方に一致する．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie and Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie and Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod and Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod and Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod and Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Jacod and Protter, 2012) では \\([x]\\) で表される．↩︎\n(Del Moral and Penev, 2014, p. xlviii), (Del Moral, 2004, p. 10) の定義に一致する．これは \\(\\prod_{i\\in\\emptyset}X_i\\) が一点集合で，\\(\\coprod_{i\\in I}X_i\\) が空集合である消息の一般化と見れる．なお，集合 \\(X\\) の部分集合の空な族 \\((X_i)_{i\\in\\emptyset}\\) は存在し，それは \\(\\mathrm{Map}(\\emptyset,X_i)\\) のただ一つの元である．↩︎\n(Kuratowski, 1921) による定義である．(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 118), (斎藤毅, 2009, pp. 定義1.3.1 p.15) の定め方に一致する．また \\(n\\)-組を英語では tuple と呼ぶが，全く同じ対象をリスト (list) とも呼ぶ nLab Concept with an Attitude．↩︎\n(Chopin and Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(新井敏康, 2011, p. 119) などでは，\\(f\\restriction_A\\) とも表す．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné and Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(斎藤毅, 2007, pp. 例1.4.7 p.20) に従った．また f.e. とは with a finite number of exceptions の略で，「有限個の例外を除いて成り立つ」という意味である (伊藤清, 1991, p. 124)．↩︎\n(斎藤毅, 2009, p. 179) では \\(F(X)\\) と表記している．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(Billingsley, 1999), (Ethier and Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n(Fritz, 2020, p. 19), (Perrone, 2022) など．Markov圏の稿 も参照↩︎\nnLab の記法に一致する．(斎藤毅, 2020, p. 7) では \\(\\mathrm{Mor}_C(X,Y)\\) と表す．↩︎\n(Del Moral, 2004, p. 7) も参照．↩︎\n(Del Moral and Penev, 2014, p. xlvii), (V. I. Bogachev, 2007, p. 277) 4.1.(i) に一致する．(V. I. Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie and Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod and Protter, 2012), (Del Moral and Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(V. I. Bogachev and Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Rogers and Williams, 2000, p. 110) V.1.3 では \\(S_n^+\\) の記法が用いられている．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．特に，古典力学や有限要素法の文献においては，二項積 の間の演算である二重点乗積を \\(:\\) で表したことから，この記法が用いられる．二項積については (Abraham et al., 1988, p. 341) も参照．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi), (Bakry et al., 2014, p. xv) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné and Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné and Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (V. I. Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(V. I. Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart and Wellner, 2023, p. 6) では 外確率 という．↩︎\n(V. I. Bogachev, 2007, p. 17) 定義1.5.1, (Vladimir I. Bogachev and Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (V. I. Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (V. I. Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(V. I. Bogachev, 2007, p. 23) に倣った．(V. I. Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie and Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(V. I. Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (V. I. Bogachev, 2007, p. 189) 参照．↩︎\n(Giné and Nickl, 2021, p. 16), (Vladimir I. Bogachev and Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 8) に倣った．(V. I. Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n(Nualart and Nualart, 2018) に倣った．(Giné and Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．(Giné and Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral and Penev, 2014), (Dellacherie and Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné and Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart and Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain and Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral and Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné and Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral and Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (V. I. Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(V. I. Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral and Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n可測空間を \\((E,\\mathcal{E})\\) で表すのは，(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) に倣った．↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay and Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(V. I. Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford and Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford and Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné and Nickl, 2021, p. 2), (Villani, 2009) に一致する．(V. I. Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral and Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(V. I. Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie and Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(V. I. Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan and Doucet, 2002) に一致する．(Dellacherie and Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Jacod and Shiryaev, 2003, p. 347), (Crisan and Doucet, 2002), (Ethier and Kurtz, 1986, p. 96), (V. I. Bogachev, 2007, p. 228) に一致する．(Kechris, 1995, p. 109), (Villani, 2009) はイタリックで \\(P(E)\\) と表す．↩︎\n(Pedersen, 1989, p. 72) に倣った．(V. I. Bogachev, 2007, p. 76) 第7.2節 では \\(\\mathcal{P}_r(X)\\) で表す．Radon 測度とは，内部正則性（＝緊密性） \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] を満たす Borel 測度をいう (V. I. Bogachev, 2007, pp. 68–69) 定義7.1.1, 7.1.4．↩︎\n(Kulik, 2018) が \\(\\mathcal{C}\\) で表すのに倣った．(Vladimir I. Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) では \\(\\Pi(\\mu,\\nu)\\) で，(Ethier and Kurtz, 1986, p. 96) では \\(\\mathcal{M}(\\mu,\\nu)\\) で，(Dudley, 2002, p. 420) 11.8節 は \\(M(\\mu,\\nu)\\) で表す．(V. I. Bogachev, 2007, p. 235) 8.10(viii)節と (Villani, 2009, p. 95) 注6.5 に倣い，カップリングの元は Radon なものに限っている点に注意．↩︎\n(竹村彰道, 2020) の記法に一致する．↩︎\nDirac 測度とも言う．(Jacod and Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) などは \\(\\epsilon_x\\) で表す．(Protter, 2005, p. 299) は Dirac 関数を \\(\\delta_x\\) で表す．↩︎\n(Gerber et al., 2019) の記法に一致．分位点関数 (quantile function) (竹村彰道, 2020, p. 16)，確率表現関数 (森口繁一, 1995) などともいう．(Dudley, 2002, p. 283) は \\(X_F\\) とも表している．↩︎\n(Revuz and Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho and Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod and Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan and Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal and van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin and Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie and Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas and Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal and van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf and Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．(Heng et al., 2024) では \\(T=\\mu\\) という定値核の場合も同様の記法 \\(\\mu\\otimes S\\) を定義している．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) や (Protter, 2005, p. 52) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie and Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(V. I. Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (V. I. Bogachev, 2007, p. 262) 4.4節．(Dunford and Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral and Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg and Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(V. I. Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(V. I. Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier and Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné and Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie and Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\nこのような使い分けは (Nummelin, 1984, p. 1) に一致する．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart and Nualart, 2018, p. 31) に一致する．↩︎\n(Nualart and Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod and Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod and Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod and Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie and Meyer, 1978, p. 114), (Revuz and Yor, 1999, p. 42) に倣った．↩︎\n右連続性と完備性を併せて，フィルトレーション付き確率空間 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) の 通常の条件 ともいう．(Protter, 2005, p. 3) など参照．↩︎\n(Protter, 2005, p. 56)．↩︎\n(Dellacherie and Meyer, 1978) 49 115-IV では 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n(Jacod and Shiryaev, 2003, p. 4) 1.11，(Protter, 2005, p. 3) に従った．↩︎"
  },
  {
    "objectID": "static/English.html",
    "href": "static/English.html",
    "title": "Entries in English",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2024-02-25\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n2024-01-05\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nLaTeX authoring environment with VSCode\n\n\n\n\n\nA page collecting tips to author technical documents with VSCode. Also including a starting guide.\n\n\n\n\n\n2023-12-22\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n2023-12-01\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#probability",
    "href": "static/AllCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#process",
    "href": "static/AllCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nSchrödinger 橋\n\n\n拡散模型にヒントを得た輸送によるサンプリング法\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析２\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\nProcess\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\nProcess\n\n\n\n\n2024-01-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#functional-analysis",
    "href": "static/AllCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#geometry",
    "href": "static/AllCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mathcalpx",
    "href": "static/AllCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#nature",
    "href": "static/AllCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mcmc",
    "href": "static/AllCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nMCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析\n\n\n\nBayesian\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#foundation",
    "href": "static/AllCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n\n2024-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\n\n2024-03-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#information",
    "href": "static/AllCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nInformation\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#economic-security",
    "href": "static/AllCategories.html#economic-security",
    "title": "Categories",
    "section": "2.5 Economic Security",
    "text": "2.5 Economic Security\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#computation-2",
    "href": "static/AllCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送\n\n\nSinkhorn アルゴリズム\n\n\n\nComputation\n\n\nP(X)\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル３\n\n\nグラフィカルモデルの推論\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2024-02-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#simulation",
    "href": "static/AllCategories.html#simulation",
    "title": "Categories",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#python",
    "href": "static/AllCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装：リサンプリング編\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#julia",
    "href": "static/AllCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nMCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型宣言\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#yuima",
    "href": "static/AllCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#stan",
    "href": "static/AllCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#r",
    "href": "static/AllCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#bayesian",
    "href": "static/AllCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析\n\n\n\nBayesian\n\n\nMCMC\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル３\n\n\nグラフィカルモデルの推論\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2024-02-17\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析２\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#particles",
    "href": "static/AllCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装：リサンプリング編\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#kernels",
    "href": "static/AllCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析２\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#ai",
    "href": "static/AllCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#deep-learning",
    "href": "static/AllCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n多様体学習\n\n\n\nDeep\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\nノイズ対照学習\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#posters",
    "href": "static/AllCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#reviews",
    "href": "static/AllCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n\nReview\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n\nReview\n\n\n\n\n2024-05-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n\nReview\n\n\n\n\n2024-04-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nInformation\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\nReview\n\n\n\n\n2023-10-29\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#surveys",
    "href": "static/AllCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nMCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nNicolas Chopin 論文のまとめ\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#slides",
    "href": "static/AllCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#papers",
    "href": "static/AllCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/AllCategories.html#opinion",
    "href": "static/AllCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\nOpinion\n\n\n\n\n2024-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#life",
    "href": "static/AllCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#lifestyle",
    "href": "static/AllCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nffmpeg による音声・動画の変換\n\n\n\nLifestyle\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n「穴あき式」の考え方\n\n\n\nLifestyle\n\n\n\n\n2024-01-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLaTeX authoring environment with VSCode\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\nGit覚書\n\n\n\nLifestyle\n\n\n\n\n2023-11-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#法律家のための統計数理",
    "href": "static/AllCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#俺のための-julia-入門",
    "href": "static/AllCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）モジュール\n\n\nモジュールとパッケージ作成\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型宣言\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html",
    "href": "posts/Surveys/SMCSamplers.html",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nSMC の文脈で，目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) が複雑であるとき，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをする，というアイデアを 調温 (tempering) という（粒子フィルターの稿 も参照）．\nこの tempering という考え方は本質的に逐次的な発想を持っているが，元々は SMC の文脈とは全く独立に，MCMC を多峰性を持つ複雑な分布に対しても使えるように拡張する研究で提案された．これらの手法が自然と SMC へと接続する様子を population Monte Carlo (Yukito Iba, 2001), (Jasra et al., 2007) というキーワードで理解されている．\nまずはその歴史を概観する．いずれも，目標分布 \\(\\pi_p\\) が多峰性をもち，MCMC がうまく峰の間を遷移できずに正しいサンプリングができない（収束が遅くなる）問題を解決する文脈の中で捉えられる．\n\n\nこれは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3\n\n\n\nMCMC を複数同時に実行する手法を 拡張アンサンブル法 という (YUKITO Iba, 2001)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Yukito Iba, 2001), (Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Yukito Iba, 2001) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (YUKITO Iba, 2001)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15\n\n\n\n\n\n\ntempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．18 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw:=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．19\n従って，本当は \\(\\mathcal{X}^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．20\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．\n\n\n\n\n\n\n\n簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．\n\n\n\n\n\n目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-SA",
    "href": "posts/Surveys/SMCSamplers.html#sec-SA",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "これは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "href": "posts/Surveys/SMCSamplers.html#拡張アンサンブル法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "MCMC を複数同時に実行する手法を 拡張アンサンブル法 という (YUKITO Iba, 2001)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Yukito Iba, 2001), (Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Yukito Iba, 2001) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (YUKITO Iba, 2001)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "href": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．18 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw:=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．19\n従って，本当は \\(\\mathcal{X}^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．20\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#その他の手法",
    "href": "posts/Surveys/SMCSamplers.html#その他の手法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#footnotes",
    "href": "posts/Surveys/SMCSamplers.html#footnotes",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの前にも，Umbrella sampling (Torrie and Valleau, 1977) が本質的には密度の調温のアイデアを用いていた．(Liu, 2004, pp. 206–) Section 10.1 も参照．↩︎\n分子動力学 (molecular dynamics) などの文脈では Metropolis 法はちょうど分子運動のシミュレーションになっていることを踏まえれば，これを simulated annealing と呼ぶのは極めて鮮やかなアナロジーとなっている．焼きなまし法自体も，シミュレーション可能になったのである．↩︎\n(Geman and Geman, 1984) によると，各 \\(\\pi_n\\) における MCMC move の回数を \\(N_n\\) とした場合，\\(O(\\log(N_1+\\cdots+N_n))\\) のオーダーで \\(T_n\\) を（十分遅く）変化させれば，この手法はほとんど確実に \\(\\operatorname*{argmin}h\\) 内に収束する．(Liu, 2004, pp. 209–) 10.2 節も参照．↩︎\n(岡本祐幸, 2010), (YUKITO Iba, 2001) など．↩︎\n(Liu, 2004, pp. 205–) Chapter 10. Multilevel Sampling and Optimization Methods も参照．↩︎\n(Liu, 2004, p. 207) も参照．↩︎\n(Chopin et al., 2023), (Liu, 2004, p. 4) でも (Geyer, 1991) を引用して PT と呼んでいる．一方で物理学の分野では (Hukushima and Nemoto, 1996) の exchange Monte Carlo や (Swendsen and Wang, 1986) などの文献もある．前者は (Liu, 2004, p. 4) が “is reminiscent of parallel tempering (Geyer, 1991)” と指摘しており，後者は (Bouchard-Côté et al., 2012) などが引用している．↩︎\n最終講義 スピングラスと計算物性物理 p.34 も参照．温度の違う熱浴につけたレプリカをシミュレートして，時々交換する，という見方ができるためにこう呼ぶ．↩︎\n(Jasra et al., 2007) は (Geyer, 1991) を指して population-based MCMC と呼んでおり，SMC も含めて population-based simulation と呼んでいる．population-based という言葉自体は (Yukito Iba, 2001) からとったという．“we define a population-based simulation method as one which, instead of sampling a single (independent/dependent) sample, generates a collection of samples in parallel” と定義しており，大きく MCMC によるものと逐次重点サンプリングベースのものの２流儀あるとしている．(Liu, 2004, pp. 225–) 第11章なども参照．↩︎\n(岡本祐幸, 2010) など．↩︎\n(Behrens et al., 2012, p. 66) も参照．↩︎\n(YUKITO Iba, 2001) が良い解説を与えていると (Jasra et al., 2007) でも言及されている．ただし，(YUKITO Iba, 2001) はこの並行テンパリングだけでなく，模擬テンパリング，multicanonical Monte Carlo (Berg and Neuhaus, 1991) / Adaptive Umbrella Sampling (Torrie and Valleau, 1977) を総称して 拡張アンサンブル法 (Extended Ensemble Monte Carlo) と呼んでサーベイしていることに注意．↩︎\n(Lyubartsev et al., 1992) が引用されることもある．(酒井佑士, 2017), (岡本祐幸, 2010) など．method of expanded ensemble とも呼ばれる (岡本祐幸, 2010), (YUKITO Iba, 2001)．↩︎\n記法 \\([p]=\\{1,\\cdots,p\\}\\) は 本サイトの数学記法一覧 を参照↩︎\nその後すぐに分子シミュレーションの分野にも導入された．(岡本祐幸, 2010) も参照．↩︎\n(Behrens et al., 2012) も参照．↩︎\n(Chopin and Papaspiliopoulos, 2020, p. 33) で，SMC を調温に初めて応用した論文として紹介されている．p.352 では “An early version of SMC tempering (without resampling)” としている．↩︎\nただし，\\(P_i^{-1}\\) とは，\\[ P_i(x_{i-1},x_i)\\pi_{i-1}(x_i-1)=\\pi_i(x_i)P_i^{-1}(x_{i-1},x_i) \\] で定まる確率核とした．\\(\\otimes\\) の記法はこちらも参照．↩︎\nこのウェイトの表示は，\\(P_i^{-1}/P_i=\\pi_{i-1}/\\pi_i\\) が成り立つことから直ちに従う．↩︎\n(Doucet et al., 2022) も参照．↩︎"
  },
  {
    "objectID": "static/CV/cv.html#profile-and-skills",
    "href": "static/CV/cv.html#profile-and-skills",
    "title": "Hirofumi Shiba",
    "section": "Profile and Skills",
    "text": "Profile and Skills\nHirofumi is currently a Ph.D. candidate at the Institute of Statistical Mathematics, working under the supervision of Prof. Kengo Kamatani.\nHe holds Japanese citizenship and is a native speaker of Japanese and Chinese. Additionally, He is fluent in English.\nHirofumi codes in Julia, Python, and R."
  },
  {
    "objectID": "static/CV/cv.html#research-interests",
    "href": "static/CV/cv.html#research-interests",
    "title": "Hirofumi Shiba",
    "section": "Research Interests",
    "text": "Research Interests\n\nMonte Carlo methods, including Markov Chain Monte Carlo and Sequential Monte Carlo.\nStochastic processes, including Markov processes and Stochastic Differential Equations.\nMachine Learning and Bayesian statistics, including Bayesian deep learning and multilevel modeling."
  },
  {
    "objectID": "static/CV/cv.html#work-experience",
    "href": "static/CV/cv.html#work-experience",
    "title": "Hirofumi Shiba",
    "section": "Work Experience",
    "text": "Work Experience\n\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – today\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference.\nCooperative Researcher. RCAST, the University of Tokyo, Japan. 2023.4 – today\nResearch on trustworthy AI and machine learning from the viewpoint of economic security.\nData Scientist. IMIS Co., Ltd. 2022.8 – 2024.1\nProvided statistical analysis and machine learning solutions for clients in the manufacturing industry."
  },
  {
    "objectID": "static/CV/cv.html#education",
    "href": "static/CV/cv.html#education",
    "title": "Hirofumi Shiba",
    "section": "Education",
    "text": "Education\n\nPh.D. in Statistical Science. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2023.4 – 2028.3\nSuperivsor: Kengo Kamatani\nB.A. in Mathematics. The University of Tokyo, Japan. 2019.4 – 2023.3"
  },
  {
    "objectID": "static/Japanese.html",
    "href": "static/Japanese.html",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学５年一貫博士課程（統計科学コース）２年．専門はベイズ計算．\n\n\n統計数理研究所 鎌谷研吾 教授の下で，マルコフ連鎖モンテカルロ法 (MCMC) や逐次モンテカルロ法 (SMC) などの計算統計の手法を，確率過程の観点から研究しています．\nまた，確率過程の統計推測のための R パッケージである YUIMA の開発などを通じ，モンテカルロ法とベイズ統計の応用にも広く取り組んでいます．\n\n\n\n現状のモンテカルロ法の多く（Langevin Monte Carlo や Hamiltonian Monte Carlo など）は物理学的な由来を持ちますが，最も効率的な方法はそうではないかもしれません．\nコーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みをモンテカルロ法に取り入れることで，効率性をさらにあげることができるはずです．\nその第一歩が 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n  \n    \n      \n      \n        \n        モンテカルロ法の発展とは，背後の物理現象からの離陸の歴史でした．非対称な Metropolis-Hastings アルゴリズムがどのように生まれたかと，その最先端のアイデアと言える連続時間 MCMC 法の歴史を，サンプルコード付きで紹介します．（2024 年度統計数理研究所オープンハウス）\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n2023.7 – 現在．統計数理研究所\n\n\n\n2023.4 – 現在．東京大学先端科学技術研究センター\n\n\n\n\n\n\n\n\n総合研究大学院大学．指導：鎌谷研吾．\n\n\n\n東京大学理学部数学科．指導：吉田朋広．"
  },
  {
    "objectID": "static/Japanese.html#自己紹介",
    "href": "static/Japanese.html#自己紹介",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "統計数理研究所 鎌谷研吾 教授の下で，マルコフ連鎖モンテカルロ法 (MCMC) や逐次モンテカルロ法 (SMC) などの計算統計の手法を，確率過程の観点から研究しています．\nまた，確率過程の統計推測のための R パッケージである YUIMA の開発などを通じ，モンテカルロ法とベイズ統計の応用にも広く取り組んでいます．"
  },
  {
    "objectID": "static/Japanese.html#研究",
    "href": "static/Japanese.html#研究",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "現状のモンテカルロ法の多く（Langevin Monte Carlo や Hamiltonian Monte Carlo など）は物理学的な由来を持ちますが，最も効率的な方法はそうではないかもしれません．\nコーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みをモンテカルロ法に取り入れることで，効率性をさらにあげることができるはずです．\nその第一歩が 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n  \n    \n      \n      \n        \n        モンテカルロ法の発展とは，背後の物理現象からの離陸の歴史でした．非対称な Metropolis-Hastings アルゴリズムがどのように生まれたかと，その最先端のアイデアと言える連続時間 MCMC 法の歴史を，サンプルコード付きで紹介します．（2024 年度統計数理研究所オープンハウス）"
  },
  {
    "objectID": "static/Japanese.html#経歴",
    "href": "static/Japanese.html#経歴",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "2023.7 – 現在．統計数理研究所\n\n\n\n2023.4 – 現在．東京大学先端科学技術研究センター"
  },
  {
    "objectID": "static/Japanese.html#学位",
    "href": "static/Japanese.html#学位",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学．指導：鎌谷研吾．\n\n\n\n東京大学理学部数学科．指導：吉田朋広．"
  },
  {
    "objectID": "static/PartialCategories.html#probability",
    "href": "static/PartialCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#process",
    "href": "static/PartialCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#functional-analysis",
    "href": "static/PartialCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#geometry",
    "href": "static/PartialCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mathcalpx",
    "href": "static/PartialCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#nature",
    "href": "static/PartialCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mcmc",
    "href": "static/PartialCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nMCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#foundation",
    "href": "static/PartialCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#information",
    "href": "static/PartialCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nInformation\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#economic-security",
    "href": "static/PartialCategories.html#economic-security",
    "title": "Categories",
    "section": "2.5 Economic Security",
    "text": "2.5 Economic Security\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#computation-2",
    "href": "static/PartialCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#simulation",
    "href": "static/PartialCategories.html#simulation",
    "title": "Categories",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#python",
    "href": "static/PartialCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#julia",
    "href": "static/PartialCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nMCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#yuima",
    "href": "static/PartialCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#stan",
    "href": "static/PartialCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#r",
    "href": "static/PartialCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#bayesian",
    "href": "static/PartialCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#particles",
    "href": "static/PartialCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#kernels",
    "href": "static/PartialCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#ai",
    "href": "static/PartialCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#deep-learning",
    "href": "static/PartialCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#posters",
    "href": "static/PartialCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#reviews",
    "href": "static/PartialCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\nInformation\n\n\nReview\n\n\n\n\n2023-12-08\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\nReview\n\n\n\n\n2023-10-29\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#surveys",
    "href": "static/PartialCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nMCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#slides",
    "href": "static/PartialCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#papers",
    "href": "static/PartialCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/PartialCategories.html#opinion",
    "href": "static/PartialCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#life",
    "href": "static/PartialCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#lifestyle",
    "href": "static/PartialCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLaTeX authoring environment with VSCode\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#法律家のための統計数理",
    "href": "static/PartialCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#俺のための-julia-入門",
    "href": "static/PartialCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html",
    "href": "static/Slides.html",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nMCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#upcomings-newests",
    "href": "static/Slides.html#upcomings-newests",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nMCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#presentation-history",
    "href": "static/Slides.html#presentation-history",
    "title": "Slides",
    "section": "Presentation History",
    "text": "Presentation History"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Notations | Categories | English Entries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP HP マニュアル\n\n\n記事の作成から公開まで\n\n\n\nEconomic Security\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n6/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫博士課程を紹介します，同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷です．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR によるベイズ混合モデリング入門\n\n\nbrms を用いた混合効果モデリング入門\n\n\n\nBayesian\n\n\nMCMC\n\n\nStan\n\n\nR\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行うための R パッケージである．本稿では，brms の基本的な使い方と，その実装を紹介する．また，ランダム効果モデルや一般化推定方程式などの文脈で扱われる種々のモデルを，階層モデルの観点から統一的にレビューし，ベイズ統計学の観点の透徹性を強調する．現代では，まだ複雑で大規模なモデルに対して，完全なベイズ推定を実行するには困難が多く，ベイズのモデリング上の透徹性と，最適化による点推定の計算上の優位性とのバランスを考えることが重要である． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における [@Parisi-Wu1981] の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の [@Hastings1970] による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である [@Nemeth-Fearnhead2021]． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP 配信マニュアル\n\n\nシンポジウムでの YouTube 配信のやり方\n\n\n\nEconomic Security\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n2/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nESRP 配信マニュアル\n\n\nシンポジウムでの YouTube 配信のやり方\n\n\n\nEconomic Security\n\n\nLifestyle\n\n\n\n\n\n\n\n\n\n2/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n2/25/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\nEconomic Security\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れ，近年の発展と予想される安全保障上の懸念も概観する．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析１\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\nEconomic Security\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である \\(K\\)-平均法の説明から始める．\\(K\\)-平均法は第一義的にはモデルフリーの（確率論と関係のない）クラスタリングアルゴリズムである．\n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLaTeX authoring environment with VSCode\n\n\n\n\n\n\nLifestyle\n\n\n\nA page collecting tips to author technical documents with VSCode. Also including a starting guide.\n\n\n\n\n\n12/22/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\n\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nインターネットとは AS 間が BGP で相互接続された裏路地である\n\n\n\n\n\n\nInformation\n\n\nReview\n\n\n\n\n\n\n\n\n\n12/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か | About Particle Filter\n\n\n\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\n1.1節”On the Origins of Feynman-Kac and Particle Models”を翻訳\n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nKernel Methods for Mathematicians\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSerotonin Reduction in Post-acute Sequelae of Viral Infection | ウイルスの腸管持続感染によって血中セロトニン濃度が低下する\n\n\n\n\n\n\nReview\n\n\n\n\n\n\n\n\n\n10/29/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）\n\n\n数値計算への新たな接近\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html",
    "href": "posts/2024/Samplers/Diffusion.html",
    "title": "拡散模型",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#導入",
    "href": "posts/2024/Samplers/Diffusion.html#導入",
    "title": "拡散模型",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 アイデア\n拡散モデルによる画像生成は，初め (Sohl-Dickstein et al., 2015) で提案され，(Ho et al., 2020) で DDPM (Denoising Diffusion Probabilistic Model) として拡張された．1\nVAE, EBM, 正則化流 はいずれもノイズからデータの分布までの変換を学ぼうとするが，拡散模型のアイデアは，データをノイズにする方向の方が圧倒的に簡単であることを利用する．\nまず訓練データを，完全な Gauss 分布になるような変換を拡散過程によって行う．これを複数段階に分けて実行し，エンコーダー \\[\nq(x_0,x_T)dx_T=\\int\\cdots\\int q(x_0,x_1)dx_1\\cdots q(x_{T-1},x_T)dx_{T-1}\n\\] を得る．\n続いて，この逆過程 \\(p_\\theta(x_t,x_{t-1})\\) を VAE 様の方法で学習しようというのである．\nこの際，スコアマッチング を使うこともでき，NCSN (Noise Conditioned Score Network) (Y. Song, Durkan, et al., 2021) などの方法が提案されている．\nこの２つの手法は，ノイズスケジュールが異なるのみで本質的に同じ枠組みであるとみなせる (Huang et al., 2021)．\n\n\n1.2 拡散モデルの例\n\n1.2.1 ADM (Dhariwal and Nichol, 2021)\nADM (Ablated Diffusion Model) (Dhariwal and Nichol, 2021) は ImageNet データの判別において当時の最先端であった BigGAN (Brock et al., 2019) の性能を凌駕した．\nそのアーキテクチャには U-Net (Ronneberger et al., 2015) が用いられた．\n\n\n1.2.2 GLIDE (Nichol et al., 2022)\nOpenAI の GLIDE (Guided Language to Image Diffusion for Generation and Editing) (Nichol et al., 2022) は，CLIP (Contrastive Language-Image Pre-training) というトランスフォーマーベースの画像符号化器と組み合わされた，テキスト誘導付き拡散モデルである．\n\n\n1.2.3 Imagen (Saharia, Chan, Saxena, et al., 2022)\nGoogle も Imagen (Saharia, Chan, Saxena, et al., 2022) というCascaded Generation 5.3 に基づいた誘導付き拡散モデルを開発している．\nT5-XXL (Raffel et al., 2020) に基づく言語モデルを通じて言語と画像を同等の潜在空間にのせ，U-Net アーキテクチャを持った VDM 2.4 でモデリングすることで，高精度な text-to-image を実現している．\nPalette (Saharia, Chan, Chang, et al., 2022) は同様の仕組みで image-to-image を実現している．\n\n\n\n1.2.4 潜在拡散模型\nVAE や GAN と違い，１つのニューラルネットワークしか用いないため，学習が安定しやすい．\n一方で，生成時には逆変換を何度も繰り返す必要があるため，計算量が大きい．これを回避するために，生成を VAE 内の潜在空間で行うものを 潜在拡散モデル (latent diffusion model) (Rombach et al., 2022) という．これが Stable Diffusion の元となっている．\n\n\n1.2.5 トランスフォーマーとの邂逅\n並列化が容易であり，スケーラブルな手法であるため，トランスフォーマーと組み合わせて画像と動画の生成に使われる．\n潜在拡散モデルで U-Net (Ronneberger et al., 2015) を用いていたところをトランスフォーマーに置換した 拡散トランスフォーマー (DiT: Diffusion Transformer) (Peebles and Xie, 2023) が発表された．\nその後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) (Ma et al., 2024) が発表された．\n\n\n1.2.6 Discrete Denoising Diffusion Probabilistic Models (D3PM) (Austin et al., 2021)\nImagen にように言語を連続な潜在空間に埋め込む他に，直接離散空間上にも拡散模型を用いる事ができる．\n実はこのように設計された拡散模型は，BERT (Lewis et al., 2020) などのマスク付き言語モデルと等価になる．2\nMaskGIT (Masked Generative Image Transformer) (Chang et al., 2022) はこの枠組みに，画像をベクトル量子化して載せる．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#sec-DDPM",
    "href": "posts/2024/Samplers/Diffusion.html#sec-DDPM",
    "title": "拡散模型",
    "section": "2 デノイジング拡散模型 (DDPM)",
    "text": "2 デノイジング拡散模型 (DDPM)\n\n2.1 導入\nDDPM はエンコーダー \\(q\\) とデコーダー \\(p_\\theta\\) がそれぞれ複数層からなるような，階層的 VAE ともみなせる．\n加えて，隠れ層は全て入力 \\(x_0\\) と同じ次元 \\(d\\) で作る点は，正則化流にも似ている（可逆とは限らないが）．\nまた，エンコーダー \\(q\\) は \\[\nq(x_0,x_T)\\,dx_T=\\mathrm{N}_d(0,I_d)\n\\] を満たすように，ノイズスケジュール \\(\\{\\beta_t\\}\\) の自由度のみを残して \\[\nq(x_{t-1},x_t)\\,dx_t:=\\mathrm{N}_d\\left(\\sqrt{1-\\beta_t}x_{t-1},\\beta_tI_d\\right),\\qquad\\beta_t\\in(0,1),\n\\] で固定し，学習すべきパラメータは入れない．\n\n\n2.2 デコーダーの設計\n次の分布は解析的に求まる： \\[\nq((x_t,x_0),x_{t-1})\\,dx_{t-1}:=\\mathrm{N}_d\\left(\\widetilde{\\mu}_t(x_t,x_0),\\widetilde{\\beta}_tI_d\\right).\n\\]\nこのことに基づいて， \\[\np_\\theta(x_t,x_{t-1})=\\mathrm{N}_d\\biggr(\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t)\\biggl),\\qquad\\Sigma_\\theta(x_t,t):=\\sigma_t^2I_d,\n\\tag{1}\\] とモデリングする．さらに \\(\\sigma^2_t\\in\\{\\beta_t,\\widetilde{\\beta}_t\\}\\) としてしまうことも多い．\n総じて，データ分布を \\[\np_\\theta(x_0):=\\int_{\\mathbb{R}^{d(T-1)}}p(x_T)p_\\theta(x_T,x_{T-1})\\cdots p_\\theta(x_1,x_0)dx_{T-1}\\cdots dx_1\n\\] としてモデリングする．ただし，\\(p(x_T)\\,dx_T=\\mathrm{N}_d(0,I_d)\\)．\n\n\n2.3 変分推論\nこのモデルの対数尤度は，次のように下から評価できる： \\[\\begin{align*}\n    \\log p_\\theta(x_0)&=\\log\\int_{\\mathbb{R}^{d(T-1)}}p_\\theta(x_{0:T})\\,dx_{1:T}\\\\\n    &=\\log\\left(\\int_{\\mathbb{R}^{d(T-1)}}\\frac{p_\\theta(x_{0:T})}{q(x_{1:T}|x_0)}q(x_{1:T}|x_0)\\,dx_{1:T}\\right)\\\\\n    &\\ge\\int_{\\mathbb{R}^{d(T-1)}}\\log\\left(p(x_T)\\prod_{t=1}^T\\frac{p_\\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\\right)q(x_{1:T}|x_0)\\,dx_{1:T}=:-\\mathcal{L}(x_0)\n\\end{align*}\\]\nこのとき， \\[\n\\mathcal{L}(x_0)=\\operatorname{KL}\\biggr(q(x_T|x_0),p(x_T)\\biggl)+\\sum_{t=2}^T\\int_{\\mathbb{R}^d}\\operatorname{KL}\\biggr(q(x_{t-1}|x_t,x_0),p_\\theta(x_{t-1}|x_t)\\biggl)q(x_t|x_0)\\,dx_t-\\int_{\\mathbb{R}^d}\\log p_\\theta(x_0|x_1)q(x_1|x_0)\\,dx_1\n\\] と展開できるが，登場する密度は全て正規密度であるため，全て計算できる．\nだが (Ho et al., 2020) では，実際の訓練は正確な変分推論を実行するのではなく，この \\(\\mathcal{L}(x_0)\\) を簡略化したもの \\[\nL'=\\|\\epsilon-\\epsilon_0(X_t,t)\\|^2,\\qquad t\\sim\\mathrm{U}([T]),x_t\\sim q_0(X_t),\n\\] を用いた．(Choi et al., 2022) この議論をさらに進めている．\n\n\n2.4 Variational Diffusion Model (VDM) (Kingma et al., 2021)\nVDM では正確に \\(\\mathcal{L}(x_0)\\) を最小化し，真の変分推論を実行することを試みる．\nエンコーダーの平均と分散 \\[\nq(x_t|x_0)=\\mathrm{N}_d\\biggr(\\widehat{\\alpha}_tx_0,\\widehat{\\sigma}_t^2I_d\\biggl)\n\\] に関して \\(\\mathcal{L}(x_0)\\) を最適化する．これは SNR (Signal-to-Noise Ratio) \\[\nR(t)=\\frac{\\widehat{\\alpha}_t^2}{\\widehat{\\sigma}_t^2}=:e^{-\\gamma_\\phi(t)}\n\\] をモデリングすることに等しく，\\(\\gamma_\\phi\\) のモデリングにニューラルネットワークを用いることを考える．\n(Kingma et al., 2021) はこのようにして正確な目的関数を定義し直し，さらにこれを推定する際に QMC によるより分散の小さい Monte Carlo 推定量を用いることを提案した．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#スコアマッチング",
    "href": "posts/2024/Samplers/Diffusion.html#スコアマッチング",
    "title": "拡散模型",
    "section": "3 スコアマッチング",
    "text": "3 スコアマッチング\n\n3.1 導入\nスコアマッチングとはエネルギーベースモデルの文脈で生まれた手法であるが，そもそもスコア \\[\n\\nabla_x\\log p(x)\n\\] 自体を学習することを生成モデリングの中心に据えることが SGM (Score-based Generative Model) (Y. Song and Ermon, 2019) で考えられた．エネルギー関数とスコア関数，どっちを学習の中心に据えるかについては (Salimans and Ho, 2021) も参照．\nしかしスコアを学習するにあたって最も致命的な点は，\\(\\nabla_x\\log p(x)\\) の値は \\(p\\) を一意的に定めないということである．特に， \\[\np=\\pi p_0+(1-\\pi)p_1,\\qquad\\pi\\in(0,1)\n\\] という関係があり，\\(p_0,p_1\\) の台が互いに素であったとき，\\(\\nabla_x\\log p\\) からは \\(\\pi\\in(0,1)\\) を定めるための情報が完全に消えてしまう．\n\n\n\nデータが青，学習されたスコアが緑．\n\n\n(Y. Song, Durkan, et al., 2021)"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#誘導付き拡散モデル",
    "href": "posts/2024/Samplers/Diffusion.html#誘導付き拡散モデル",
    "title": "拡散模型",
    "section": "5 誘導付き拡散モデル",
    "text": "5 誘導付き拡散モデル"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#確率微分方程式との関係",
    "href": "posts/2024/Samplers/Diffusion.html#確率微分方程式との関係",
    "title": "拡散模型",
    "section": "5 確率微分方程式との関係",
    "text": "5 確率微分方程式との関係\n連続時間極限を取ることで，拡散過程が現れる (Tzen and Raginsky, 2019), (Y. Song, Sohl-Dickstein, et al., 2021)．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#参考文献",
    "href": "posts/2024/Samplers/Diffusion.html#参考文献",
    "title": "拡散模型",
    "section": "6 参考文献",
    "text": "6 参考文献\n\nAwesome-Diffusion-Models，What’s the score?．\n良いサーベイには次がある：(Luo, 2022)，(McAllester, 2023)．\n古くからあり，すでに出版されているものには，(Yang et al., 2023), (Cao et al., 2024)．CVPR のチュートリアルが (Kreis et al., 2022), (J. Song et al., 2023)．\n拡散モデルのサンプリングを加速する手法に関するサーベイは (Nichol and Dhariwal, 2021), (Croitoru et al., 2023) など．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#footnotes",
    "href": "posts/2024/Samplers/Diffusion.html#footnotes",
    "title": "拡散模型",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVideoGPT の論文 (Yan et al., 2021) や，DALL-E2 の論文 (Ramesh et al., 2022)，GLIDE の論文 (Nichol et al., 2022) でも引用されている．↩︎\n(Murphy, 2023, p. 880) 25.7.5 節も参照．↩︎\n(Murphy, 2023, p. 867) 25.3.3 節も参照．↩︎\nStein のスコア関数ともいう．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html",
    "href": "posts/2024/Samplers/DDPM.html",
    "title": "拡散模型の実装",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#数学的骨格",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#数学的骨格",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "1 数学的骨格",
    "text": "1 数学的骨格\n詳しくは 本サイトの数学記法一覧 を参照．\n\n1.1 確率空間\n\n\n\n\n\n\n定義（確率空間）\n\n\n\n\n任意の集合 \\(\\Omega\\) に対して，確率の公理 [P1], [P2], [P3] を満たす関数 \\[\n\\operatorname{P}:\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\to[0,1]\n\\] が定義されているとき，組 \\((\\Omega,\\operatorname{P})\\) を 確率空間 (probability space) という．\n確率空間上の実数値の関数 \\[\nX:\\Omega\\to\\mathbb{R}\n\\] を 確率変数 (random variable) という．1\n確率変数 \\(X:\\Omega\\to\\mathbb{R}\\) に対して， \\[\n\\operatorname{P}^X[A]:=\\operatorname{P}[X\\in A]\n\\] で定まる実数 \\(\\mathbb{R}\\) 上の分布を，\\(X\\) の 確率分布 (probability distribution) という．\n\n\n\n集合 \\(\\Omega\\) というのを自由にとって良いというのが，確率論の懐の広さであり，統計学で出会う多種多様な問題に対応できる所以である．\nサイコロの出目を考える場合は \\(\\Omega=\\{1,2,3,4,5,6\\}\\) ととってその上の確率空間に関する理論を借りれば良い．殆どの場合は \\(\\Omega=\\mathbb{R}\\) と取ることになる．\n\n\n\n\n\n\n注（厳密な定義）\n\n\n\n\n\n本当は確率空間は３組 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) である．新たに加わった \\(\\mathcal{F}\\) とは何かというと，\\(\\operatorname{P}\\) の定義域であり，上の定義で \\[\n\\operatorname{P}:\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\to[0,1]\n\\] としていたところを \\[\n\\operatorname{P}:\\mathcal{F}\\to[0,1]\n\\] と，定義域を制限するのが厳密な定義である．\nしたがって，標本空間 \\(\\Omega\\) の部分集合はなんでも事象と呼んでいいかというと，数学的にはそうではなく，事象の全体 \\(\\mathcal{F}\\) は一定の（代数的な）規則を満たす必要がある（完全加法性 という）．\nこれは 測度論 (measure theory) と呼ばれる数学分野から得られる知見である．\nなぜ制限しなければいけないのか？は，そうしなければ数学的な矛盾が起こるからなのであるが，普通に統計学の目的で確率論を用いる範囲でこの矛盾に遭遇することは滅多にないので，ここでは触れない．\n\n\n\n\n\n1.2 確率変数の概念\n確率変数の概念は，確率論において最も重要なものである．David Mumford というフィールズ賞も受賞した世界的な数学者（専門が確率論というわけではない）も，次のように述べている：\n\nThe basic object of study in probability is the random variable and I will argue that it should be treated as a basic construct, like spaces, groups and functions, and it is artificial and unnatural to define it in terms of measure theory. (Mumford, 2000, p. 108)\n\n確率変数が重要な理由は，それは確率分布と違うということを徹底的に教えてくれることにある．換言すれば，日常的な感覚で確率を議論するとなかなかモヤモヤが解消せずに解った気になれない理由は，確率変数と確率分布という本来別々の存在を人間は混同してしまいがちだからからである，と教えてくれるのが現代の確率論なのである．\n中高の数学での「場合の数と確率」は特に混同の傾向が強い．三角関数がどのように社会の役に立つか不思議に思ったことがあるならば，あそこで習った初頭的な議論がどう統計学に応用されてどうして AI が生まれるに至ったのかたいへん不思議であろう．中高での離散的な議論を連続な場合にも通用するようにするためには，確率分布と確率変数を峻別することが肝要 である．\n確率変数は，「変数」の概念の確率化 である．変数は，高校数学などでも \\(x,y,z,\\cdots\\) と小文字で表したが，確率変数は \\(X,Y,Z,\\cdots\\) と大文字で表す． \\(\\Omega=\\{*\\}\\) と標本空間を一点集合と取った場合，確率変数は通常の決定論的な変数と同義になる．\n\n\n\n\n\n\n発展（確率過程）\n\n\n\n\n\n一方で，高校数学などでも扱う「関数」の概念の確率化は　確率過程 という．確率過程は名前は仰々しいかも知れないが，定義自体はなんてことはない，確率変数の集合のこと である．\n例えば，日付 \\(n\\) の株価 \\(X_n\\) の列 \\(X_1,X_2,X_3,\\cdots\\) は確率過程である．決定論的な関数 \\(n\\mapsto x_n\\) の確率化である．\n\n\n\n\n\n1.3 分布の押し出し\nでは実際に，確率分布と確率変数がどう違うかを説明する．\n確率分布は確率空間に宿るもので，確率変数は確率空間を繋ぐものである．\nサイコロを２回振った出目の全体を標本空間とするならば， \\[\n\\Omega:=[6]\\times[6]=\\left\\{(1,1),(1,2),\\cdots,(1.6),(2,1),\\cdots\\right\\}\n\\] という集合の上に，一様分布\n\n\n\n\n\n\n\n\n\nを定義して得る確率空間を考えるのが一つの良い方法であろう．\nこれが確率分布である．\n一方で，確率変数は，標本空間上の関数の全てである．例えば，出た目の和は確率変数である．\n最も重要なことは，確率変数は分布を押し出す ということである．\n実際，出た目の和は，確率分布を押し出して，次のような確率分布を定める："
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#今回の内容",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "2 今回の内容",
    "text": "2 今回の内容\n数学的骨格を理解した状態で，今回範囲の内容 (草野耕一, 2016, pp. 73–96) を整理する．\n\n2.1 母集団と標本 (pp.73-75)\n\n2.1.1 母集団\n標本調査が行われるとき，調査対象となる全体集団を 母集団 (population) という．全人口を精査することは困難であるため，ここから無作為に一部分を選ぶことになる．これを 標本調査 (survey sampling) といい，得られたデータを 標本 (sample) という．\nすなわち，データとは確率変数 \\(X_1,\\cdots,X_n\\) であり，これらの積が定める確率変数 \\[\nX=(X_1,\\cdots,X_n):\\Omega\\to\\mathbb{R}^n\n\\] を考え，母集団を \\(\\Omega\\) とし，確率変数 \\(X_1,\\cdots,X_n\\) を標本とするのである．\n\\(X_1,\\cdots,X_n\\) が，母集団となる確率空間 \\((\\Omega,\\operatorname{P})\\) に関する情報をなるべく効率よく伝えてくれるように設計するのが重要である．実際，現代の標本調査では，無作為抽出が基本であり，一昔前では電話番号台帳の下一桁を無作為に選び，電話を掛けるという方法が用いられた．当然この場合，電話を持っていない標本 \\(\\omega\\in\\Omega\\) についての情報は得られないので，その点に関する補正が必要になる，という具合である．\nこのように，「無作為」と言っても具体的にどのように選べば良いか？を考える分野を 標本調査法 (sampling theory) という．2 大統領選挙を通じての標本抽出法の発展の例は Section 3.2 に付した．\n\n\n2.1.2 統計量の例\n標本の関数を 統計量 (statistic) という．\n\n\n\n\n\n\n定義（３つの標本統計量）\n\n\n\n\\(x_1,\\cdots,x_n\\) を標本とする．\n\n次を 標本平均 という： \\[\n\\overline{x}:=\\frac{x_1+\\cdots+x_n}{n}.\n\\]\n次を 標本分散 という： \\[\ns^2:=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2.\n\\]\n標本分散の非負の平方根 \\(s:=\\sqrt{s^2}\\) を 標本標準偏差 という．\n次を 不偏分散 という： \\[\nu^2:=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\overline{x})^2.\n\\]\n\n\n\n\n\n\n\n\n\n命題（分散公式）\n\n\n\n標本分散 \\(s^2\\) と標本平均 \\(\\overline{x}^2\\) の間には次の関係が成り立つ： \\[\ns^2=\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2.\n\\] \\(\\frac{1}{n}\\sum_{i=1}^nx_i^2\\) という量を 標本の２次の絶対積率 という．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\n\\begin{align*}\n    s^2&=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^n(x_i^2-2x_i\\overline{x}+\\overline{x}^2)\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^nx_i^2-2\\overline{x}\\frac{1}{n}\\sum_{i=1}^nx_i+\\overline{x}^2\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2.\n\\end{align*}\n\\]\n\n\n\n\n\n2.1.3 母数とは何か？\n本書 (草野耕一, 2016, p. 75) に\n\n標本に統計量があるように母集団にもその特性を表す数値が備わっているはずであり，そのような数値のことを 母数 という．\n\nとあるが，この文脈では母数ではなく 特性値 という (竹村彰道, 2020)．特性値を母数と設定することが多いが，それはあくまで統計解析者の裁量である．\n母数とは次に示すように，推定対象として解析者が設定する母集団の特性値 である．標本統計量は実際に計算できるが，特性値と母数は未知である．\nよって，ほとんどの場合，推測統計の問題とは母数推定の問題に他ならない．\n\n\n\n\n\n\n定義（統計モデル，母数）\n\n\n\n\n母集団上の確率分布の族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を 統計モデル という．\n統計モデルを添字付ける「番号」 \\(\\theta\\in\\Theta\\) を 母数 という．\n\n\n\n真の分布を \\(\\operatorname{P}\\) としたとき，これを近似すると思われる分布族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を統計解析者が設定するのである．腕の見せ所である．\nあなたが保険数理士だとして，重大事故の発生確率を推定する際， \\[\n\\Omega=\\left\\{0,1,2,\\cdots\\right\\}\n\\] を一定期間内に起こる重大事故件数とすると，これに対する分布族は Poisson 分布族 \\(\\{\\mathrm{Pois}(\\lambda)\\}_{\\lambda&gt;0}\\) を取ると近似精度が良いことが知られている．Poisson 分布の母数 \\(\\lambda&gt;0\\) は 到着率 や 強度 と呼ばれる．\n\n\n\n2.2 統計推測の技法(1) (pp.75-85)\n本書 (草野耕一, 2016, pp. 75–85) の重大な特徴に，確率変数と確率分布を区別していないという問題がある．\n確率分布とは 第１回講義 で導入した，３つの公理を満たす集合関数 \\(\\operatorname{P}:P(\\Omega)\\to[0,1]\\) である．3 このとき，ペア \\((\\Omega,\\operatorname{P})\\) を，確率が定義された集合という意味で 確率空間 という．\n確率変数 とは，確率空間 \\((\\Omega,\\operatorname{P})\\) 上に定義された関数 \\(X:\\Omega\\to\\mathbb{R}\\) のことである．特に，値域が \\(\\mathbb{N}\\subset\\mathbb{R}\\) に限る場合を 離散変数 という．\n\n2.2.1 離散の場合\n確率変数 \\(X\\) の取り得る値が \\(\\mathbb{N}=\\{0,1,2,\\cdots\\}\\) に限る場合が離散の場合である．\n\n\n\n\n\n\n定義（期待値）\n\n\n\n\n非負値の関数 \\(f:\\mathbb{N}\\to[0,1]\\) であって次を満たすものを 確率（質量）関数 という： \\[\nf(n)=\\operatorname{P}[X=n]\n\\]\n確率関数 \\(f:\\mathbb{N}\\to[0,1]\\) に従う確率変数 \\(X:\\Omega\\to\\mathbb{N}\\) に対して，次の量 \\(\\operatorname{E}[X]\\) を 期待値 という： \\[\n\\operatorname{E}[X]:=\\sum_{n=1}^\\infty nf(n)=\\sum_{n=1}^\\infty n\\operatorname{P}[X=n].\n\\]\n次の量 \\(\\mathrm{V}[X]\\) を \\(X\\) の 分散 という： \\[\n\\mathrm{V}[X]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])^2\\biggr].\n\\]\n\n\n\n\n\n2.2.2 連続の場合\n\n\n\n\n\n\n定義（期待値）\n\n\n\n\\((\\Omega,\\operatorname{P})\\) を確率空間，\\(X:\\Omega\\to\\mathbb{R}\\) をその上の確率変数とする．\n\n非負値の関数 \\(F:\\mathbb{R}\\to[0,1]\\) であって次を満たすものを，\\(X\\) の （累積）分布関数 という： \\[\nF(x):=\\operatorname{P}[X\\le x].\n\\]\n非負値の関数 \\(p:\\mathbb{R}\\to\\mathbb{R}_+\\) であって次を満たすものが存在するならば，これを \\(X\\) の （確率）密度関数 という： \\[\n\\operatorname{P}[X\\in A]=\\int_Ap(x)\\,dx.\n\\]\n確率密度 \\(p\\) に従う確率変数 \\(X:\\Omega\\to\\mathbb{R}\\) に対して，次の量 \\(\\operatorname{E}[X]\\) が存在するならば，これを \\(X\\) の 期待値 という： \\[\n\\operatorname{E}[X]:=\\int_\\mathbb{R}xp(x)\\,dx.\n\\]\n次の量 \\(\\mathrm{V}[X]\\) が存在するならば，これを \\(X\\) の 分散 という： \\[\n\\mathrm{V}[X]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])^2\\biggr].\n\\]\n\n\n\n\n\n2.2.3 期待値の性質\n\n\n\n\n\n\n命題（期待値の性質）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n（期待値の線型性）任意の \\(a,b\\in\\mathbb{R}\\) について， \\[\n\\operatorname{E}[aX+bY]=a\\operatorname{E}[X]+b\\operatorname{E}[Y].\n\\]\n（分散公式）次が成り立つ： \\[\n\\mathrm{V}[X]=\\operatorname{E}[X^2]-(\\operatorname{E}[X])^2.\n\\]\n（分散の斉次性）任意の \\(a,b\\in\\mathbb{R}\\) について， \\[\n\\mathrm{V}[aX+b]=a^2\\mathrm{V}[X].\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n積分の線型性から従う．\n\nの期待値の線型性のみから従う．\n\n\n\n\n\n\n\n2.2.4 独立性と共分散\n\n\n\n\n\n\n定義（確率変数の独立性）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n\\(X,Y\\) が互いに 独立 であるとは，任意の事象 \\(A,B\\subset\\Omega\\) について， \\[\n\\operatorname{P}[X\\in A,Y\\in B]=\\operatorname{P}[X\\in A]\\operatorname{P}[Y\\in B]\n\\] を満たすことをいう．\n\\(X,Y\\) の 共分散 とは， \\[\n\\mathrm{Cov}[X,Y]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])(Y-\\operatorname{E}[Y])\\biggr]\n\\] をいう．\n\n\n\n\n\n\n\n\n\n命題（独立確率変数の性質）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n\\(X,Y\\) が独立ならば，次が成り立つ： \\[\n\\operatorname{E}[XY]=\\operatorname{E}[X]\\operatorname{E}[Y].\n\\]\n次が成り立つ： \\[\n\\mathrm{V}[X+Y]=\\mathrm{V}[X]+2\\mathrm{Cov}[X,Y]+\\mathrm{V}[Y].\n\\]\n（独立ならば無相関）\\(X,Y\\) が独立ならば， \\[\n\\mathrm{Cov}[X,Y]=0.\n\\] 特に，\\(X,Y\\) が独立ならば，\\(\\mathrm{V}\\) は加法を保存する．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\nLebesgue 積分論の議論が必要なので省略する．\n\nのみから従う．式変形は次の通り： \\[\n\\begin{align*}\n\\mathrm{V}[X+Y]&=\\operatorname{E}[(X+Y)^2]-\\biggr(\\operatorname{E}[X]+\\operatorname{E}[Y]\\biggl)^2\\\\\n&=\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n2.3 統計推測の技法(2) (pp.85-94)\n\n2.3.1 正規分布\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider\nimport seaborn as sns\n\n# 正規分布のグラフを描画する関数\ndef plot_normal_distribution(variance):\n    mean = 0  # 平均値\n    sigma = np.sqrt(variance)  # 標準偏差（分散の平方根）\n    \n    # 正規分布のデータを生成\n    x = np.linspace(-10, 10, 1000)\n    y = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (x - mean)**2 / (2 * sigma**2))\n    \n    # グラフを描画\n    plt.figure(figsize=(3, 2))\n    sns.lineplot(x=x, y=y)\n    plt.title(f'Normal Distribution with Variance {variance}')\n    plt.xlabel('Value')\n    plt.ylabel('Probability Density')\n    plt.show()\n\n# インタラクティブなウィジェットを作成\ninteract(plot_normal_distribution, variance=FloatSlider(value=1, min=0.1, max=5, step=0.1))\n\n\n\n\n\n&lt;function __main__.plot_normal_distribution(variance)&gt;\n\n\n\n\n2.3.2 Bernoulli分布と二項分布\n\n\n2.3.3 Poisson分布\n\n\n\n2.4 統計推測の技法(3) (pp.94-97)\nここで重要なトピックは不偏分散である．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#補足",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#補足",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "3 補足",
    "text": "3 補足\n\n3.1 第1節：母集団と標本\n\n3.1.1 「推測統計学」とは何か？\n本書 (草野耕一, 2016, p. 73) でも次のような注がなされている：\n\n推測統計学に対して，証拠から得られた情報をいかに効率的かつ明確に表現するかを研究する統計学の分野を記述統計学という (草野耕一, 2016, p. 73)．\n\n現代では「統計学」と言った際にほとんど推測統計学を指すと言っても過言ではない．4 つまり，現代では殆ど形骸化した区別である．この名称の本当の意味を理解するためには，歴史的な情緒を持った文脈が必要である．\n一言で言えば，推測統計学は Ronald A. Fisher の理論が出て来た際に，それ以前の Quetelet からの統計学との断絶を強調するために用いられた語であった．\n\n推計学 stochastic は推計と計画のための科学であり，その建設は主として英国の農学者 R. A. Fisher （現在 Cambridge 大学教授）の構想に懸る．–(増山元三郎, 1950, p. 3)\n\n\n\n\n\n\n\n戦後当時の理解\n\n\n\n\n\nその態度の違いは，次の例が鮮明に示している．\n\n例えば算術平均 mean という概念について：旧来の考え方に従えば平均 average は集団の ‘代表’ 値であると定義されている．5 従って，算術平均が該集団をよく ‘代表’ しうると考えられる場合にはこれを採用し，しからざる場合には度数分布の形を眺めた上で，その他の平均値，例えば並数 mode なり幾何平均 geometrical mean なりをもって，これに代えるのである．だから ‘代表値’ として如何なる平均値を選ぶかは，この際，全く個々人の常識に委ねられてしまう．これに反して推計学が算術平均を採用するのは，それが分布関数として表現せられた母集団の或る常数（即ち母数）の適切な平均値となりうる場合のみに限られる．従来統計学を定義して ‘平均の学である’ となす立場があるけれども（例えば A. L. Bowley）このような考え方こそ統計学の記述的性格を遺憾なく露呈するものであろう．統計学がこのような原理に立つ限り，それは爾後の行動に関し，形式以上に何ら指針を与える力をもちうるものではない．行動の正しい指針を与えないような学問は実は科学の名に値しないものというべきであろう．これにも拘らず推計学は吾国では最近に到るまで，科学・技術の分野でさえ仲々受け入れられず政治・経済の領域では殆ど問題にもされなかったのである．これに反し，米国などでは，推計学が社会・自然のあらゆる分野に進出し，第２次大戦の遂行にあたっても大きな貢献をなしたのであった．–(増山元三郎, 1950, p. 4)\n\n\n\n\n前者の記述統計学的な動機は，現代では「データサイエンス」のような分野に引き継がれている (Hoaglin et al., 2006)．\n\n\n\n3.2 Gallup事件\n\n3.2.1 1936年大統領選挙とクオータ抽出の重要性\n\n\n\n\n\n\nRoosevelt v.s. Landon (1936)\n\n\n\n\n背景には1929年10月24日の「暗黒の木曜日」に端を発した世界大恐慌があった．\n\n民主党 Franklin D. Roosevelt は再選を目指し，共和党の Alfred Landon が立ち向かった．\nRoosevelt の保守的な姿勢は大恐慌を食い止めるには力不足と思われ，再選の見込みは低いという意見も強く，The Literary Digest は237万人6 を対象に回収した調査結果から，57% の得票で Landon が勝つだろうと予測した．\n\n一方で Gallup 率いる the American Institute of Public Opinion は3000人の標本から Roosevelt が 55.7% の得票を得て当選するだろう，と予測した．7\n\n結果，Roosevelt が 60% の得票を得て，48州中46州を手にした．\nなぜ The Literary Digest は予測を誤ったのか？その原因は不適切な標本抽出法にあった．\n\nThe Literary Digest は自誌の購読者（大恐慌の最中でも購読した層）を対象に，そして自動車保有者と電話利用者の名簿を使って約1000万人に郵送し，回収された237万人の回答を用いた．\n過去5回の大統領選挙で的中させていたのは，経済的な状況があまり投票結果に影響しない時勢だったためと思われる．\n一方，Gallup は母集団を層別してサンプルサイズを割り当て，そのクオータに沿って標本を集める非確率的抽出法を用いていた．なお，Gallup は4ヶ月前から，The Literary Digest の予測は外れるだろうと新聞のコラム上で予言していた．\n\n\n\nこのエピソードは 不適切なデザイン下で収集された大量データよりも良いデザイン下で収集された少量のデータのほうがずっと役に立つ ということの好例として強調されることとなった．\nしかし，話はここでは終わらない．その Gallup も，後の大統領選挙で予測を大きく外している．\n\n\n3.2.2 1948年大統領選挙と無作為抽出の重要性\n\n\n\n\n\n\nTruman v.s. Dewwey (1948)\n\n\n\n\n1948年の選挙では，民主党は在任中に斃れた Roosevelt の後を継いでいた Harry Truman，共和党は4年前に Roosevelt に負けた Thomas Dewey が戦った．\nこの年の背景には公民権問題があり，共和党が20年ぶりに政権を奪還すると予想されており，Gallup もその例にもれなかった．\n\n結果，Truman が僅差で Dewey を破って当選した．\nなぜクオータ法を用いたサンプリングで実績を出した Gallup は，今回は予測を大きく誤ったのか？\n\n今回 Gallup も予想に失敗して世論調査そのものに懐疑の目が向けられたことを重く見て，検討委員会が設置された．\nそこで論点となったのが，当時 Gallup が用いていた割当法では，層内の個々の対象者の決定が調査員の個人的判断に委ねられていたことが多きなバイアスの原因となっていると予想された．\nその結果，今日では 無作為抽出 が大原則として一層強調されるエピソードとなっている (総務省統計局, 2023年4月21日確認)．\nだがこれ自体が原因だとは言えない．事実，調査担当員もそのことは自覚しており，予測を修正する調整技術を独自に開発して用いていたという (佐藤寧, 2020)．どんな標本調査法にも偏りがあり，これを修正するための予測モデルと併用するという営みは現在の無作為抽出法でも同様であり，これ自体が問題ではない．\n当時（現在も）広く用いられている電話調査という手法が，1948年代当時では裕福な有権者（電話を購入することができ，また不変の住所を維持していた）に偏ったサンプル抽出に導いたという議論もある．8\n\n\n\n\n\n\n(佐藤寧, 2020, p. 15) より\n\n\n1947年時点での GHQ による日本への統計指導でも，すでに無作為抽出法（当時は「任意見本法」）による調査が指導されている (佐藤寧, 2020)．よって，当時からクオータ法の問題は認識されており，これに必要な対策を打つ形で運用されていたと解すのが妥当であろう．\nなお，日本側のエピソードとして，統計数理研究所第７代所長も務めた 林知己夫 のオーラルヒストリーに次のような一節がある：\n\nそんな時に，CIEの担当官は，日本の新聞社を集めて，「アメリカではクォータサンプルでやっているけれど，そんなのはサンプリングじゃない」と，トルーマン，デューイの大統領選挙の予測を持ち出してきてですね，「これはクォータサンプリングでやったから間違えたんだ．こんなもの夢夢やるんじゃないぞ」と．そうしてみんな肝に銘じたんですよね．サンプリングは厳正にやらなきゃいけないって教わったわけです (高橋正樹, 2004)．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#footnotes",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし，\\(\\mathbb{R}\\) とは実数の全体からなる集合とした．↩︎\n(Wu and Thompson, 2020) など．↩︎\n数学では \\(P(\\Omega):=\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\) と表す．これを 冪集合 (power set) という．その頭文字の \\(P\\) である．↩︎\nなお，(増山元三郎, 1950) の序によると，推測統計学 という語は北川敏男によるものであり，増山は 推計学 (stochastics) と呼んでいる．↩︎\n\\(\\phi(y,y,\\cdots)=\\phi(x_1,x_2,\\cdots)\\) なる関係が成り立つとき，\\(y\\) を \\(x_1,x_2,\\cdots\\) の平均という．参照：(Jevons, 1879, p. 391)↩︎\n(中山健夫, 2003)↩︎\n(鈴木督久, 2021) によると，実際はこれは史実の誤解であるようだ．Gallup が「Digest は Landon が 56% だとして予測を誤るだろう」というコラムを新聞社に送付するのに用いた標本が3000なのであって，Gallup 自身の選挙予測調査の標本サイズは30万人であったという．なお，その際の抽出法については歴史的な文献が欠けており，知る由がないという．とは言え，それでも，「標本は量より質」という教訓になる，という意味では，象徴的なエピソードであることは間違いない．↩︎\nWikipedia の記述．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#ハイパーパラメーターの設定",
    "href": "posts/2024/Samplers/DDPM.html#ハイパーパラメーターの設定",
    "title": "拡散模型の実装",
    "section": "1 ハイパーパラメーターの設定",
    "text": "1 ハイパーパラメーターの設定\n\\(\\beta_0=10^{-4}\\) から \\(\\beta_T=0.02\\) までを，n_timesteps = 1000 等分し，その間のダイナミクスを hidden_dim = 256 次元の CNN ８層で学習する．\n\n\n必要なパッケージの読み込み\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image, make_grid\nfrom tqdm import tqdm\nfrom torch.optim import Adam\n\nimport math\n\ndataset_path = '~/hirofumi/datasets'\n\n\n\nDEVICE = torch.device(\"mps\")\n\ndataset = 'MNIST'\nimg_size = (32, 32, 3)   if dataset == \"CIFAR10\" else (28, 28, 1) # (width, height, channels)\n\ntimestep_embedding_dim = 256\nn_layers = 8\nhidden_dim = 256\nn_timesteps = 1000\nbeta_minmax=[1e-4, 2e-2]\n\ntrain_batch_size = 128\ninference_batch_size = 64\nlr = 5e-5\nepochs = 100\n\nseed = 1234\n\nhidden_dims = [hidden_dim for _ in range(n_layers)]\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n\n\nデータセットの読み込み\nfrom torchvision.datasets import MNIST, CIFAR10\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n])\n\nkwargs = {'num_workers': 0, 'pin_memory': True}  # 今回は軽量だし worker number は 0 にする\n\nif dataset == 'CIFAR10':\n    train_dataset = CIFAR10(dataset_path, transform=transform, train=True, download=True)\n    test_dataset  = CIFAR10(dataset_path, transform=transform, train=False, download=True)\nelse:\n    train_dataset = MNIST(dataset_path, transform=transform, train=True, download=True)\n    test_dataset  = MNIST(dataset_path, transform=transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False,  **kwargs)"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#参考",
    "href": "posts/2024/Samplers/DDPM.html#参考",
    "title": "拡散模型の実装",
    "section": "6 参考",
    "text": "6 参考\n\n本稿は，Minsu Jackson Kang 氏 による チュートリアル を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#モデル定義",
    "href": "posts/2024/Samplers/DDPM.html#モデル定義",
    "title": "拡散模型の実装",
    "section": "2 モデル定義",
    "text": "2 モデル定義\n\n2.1 タイムステップ \\(t\\) の位置埋め込み\n(Ho et al., 2020) ではトランスフォーマー (Vaswani et al., 2017) 同様の sinusoidal positional encoding を用いてタイムステップ \\(t\\) の情報をデータに統合したものが，パラメータの共有された CNN に与えられる．\nこうすることで n_timesteps = 1000 の別々の NN を訓練するより遥かに効率的に設計できている．\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\n\n\n2.2 ニューラルネットワークの構成\nここでは同じ次元の CNN を重ねて作ることとする．(Ho et al., 2020) では U-Net (Ronneberger et al., 2015) アーキテクチャを用いている．\n\nclass ConvBlock(nn.Conv2d):\n    \"\"\"\n        Conv2D Block\n            Args:\n                x: (N, C_in, H, W)\n            Returns:\n                y: (N, C_out, H, W)\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, activation_fn=None, drop_rate=0.,\n                    stride=1, padding='same', dilation=1, groups=1, bias=True, gn=False, gn_groups=8):\n        \n        if padding == 'same':\n            padding = kernel_size // 2 * dilation\n\n        super(ConvBlock, self).__init__(in_channels, out_channels, kernel_size,\n                                            stride=stride, padding=padding, dilation=dilation,\n                                            groups=groups, bias=bias)\n\n        self.activation_fn = nn.SiLU() if activation_fn else None\n        self.group_norm = nn.GroupNorm(gn_groups, out_channels) if gn else None\n        \n    def forward(self, x, time_embedding=None, residual=False):\n        \n        if residual:\n            # in the paper, diffusion timestep embedding was only applied to residual blocks of U-Net\n            x = x + time_embedding\n            y = x\n            x = super(ConvBlock, self).forward(x)\n            y = y + x\n        else:\n            y = super(ConvBlock, self).forward(x)\n        y = self.group_norm(y) if self.group_norm is not None else y\n        y = self.activation_fn(y) if self.activation_fn is not None else y\n        \n        return y\n\n\n\n2.3 デコーダーの定義\n\nclass Denoiser(nn.Module):\n    \n    def __init__(self, image_resolution, hidden_dims=[256, 256], diffusion_time_embedding_dim = 256, n_times=1000):\n        super(Denoiser, self).__init__()\n        \n        _, _, img_C = image_resolution\n        \n        self.time_embedding = SinusoidalPosEmb(diffusion_time_embedding_dim)\n        \n        self.in_project = ConvBlock(img_C, hidden_dims[0], kernel_size=7)\n        \n        self.time_project = nn.Sequential(ConvBlock(diffusion_time_embedding_dim, hidden_dims[0], kernel_size=1, activation_fn=True),ConvBlock(hidden_dims[0], hidden_dims[0], kernel_size=1))\n        \n        self.convs = nn.ModuleList([ConvBlock(in_channels=hidden_dims[0], out_channels=hidden_dims[0], kernel_size=3)])\n        \n        for idx in range(1, len(hidden_dims)):\n            self.convs.append(ConvBlock(hidden_dims[idx-1], hidden_dims[idx], kernel_size=3, dilation=3**((idx-1)//2),activation_fn=True, gn=True, gn_groups=8))                                \n\n        self.out_project = ConvBlock(hidden_dims[-1], out_channels=img_C, kernel_size=3)\n        \n        \n    def forward(self, perturbed_x, diffusion_timestep):\n        y = perturbed_x\n        \n        diffusion_embedding = self.time_embedding(diffusion_timestep)\n        diffusion_embedding = self.time_project(diffusion_embedding.unsqueeze(-1).unsqueeze(-2))\n        \n        y = self.in_project(y)\n        \n        for i in range(len(self.convs)):\n            y = self.convs[i](y, diffusion_embedding, residual = True)\n            \n        y = self.out_project(y)\n            \n        return y\n    \nmodel = Denoiser(image_resolution=img_size,\n                 hidden_dims=hidden_dims, \n                 diffusion_time_embedding_dim=timestep_embedding_dim, \n                 n_times=n_timesteps).to(DEVICE)\n\n\n\n2.4 エンコーダーの定義\n\nclass Diffusion(nn.Module):\n    def __init__(self, model, image_resolution=[32, 32, 3], n_times=1000, beta_minmax=[1e-4, 2e-2], device='cuda'):\n    \n        super(Diffusion, self).__init__()\n    \n        self.n_times = n_times\n        self.img_H, self.img_W, self.img_C = image_resolution\n\n        self.model = model\n        \n        # define linear variance schedule(betas)\n        beta_1, beta_T = beta_minmax\n        betas = torch.linspace(start=beta_1, end=beta_T, steps=n_times).to(device) # follows DDPM paper\n        self.sqrt_betas = torch.sqrt(betas)\n                                     \n        # define alpha for forward diffusion kernel\n        self.alphas = 1 - betas\n        self.sqrt_alphas = torch.sqrt(self.alphas)\n        alpha_bars = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n        \n        self.device = device\n    \n    def extract(self, a, t, x_shape):\n        \"\"\"\n            from lucidrains' implementation\n                https://github.com/lucidrains/denoising-diffusion-pytorch/blob/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L376\n        \"\"\"\n        b, *_ = t.shape\n        out = a.gather(-1, t)\n        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n    \n    def scale_to_minus_one_to_one(self, x):\n        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n        return x * 2 - 1\n    \n    def reverse_scale_to_zero_to_one(self, x):\n        return (x + 1) * 0.5\n    \n    def make_noisy(self, x_zeros, t): \n        # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n        epsilon = torch.randn_like(x_zeros).to(self.device)\n        \n        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars, t, x_zeros.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, t, x_zeros.shape)\n        \n        # Let's make noisy sample!: i.e., Forward process with fixed variance schedule\n        #      i.e., sqrt(alpha_bar_t) * x_zero + sqrt(1-alpha_bar_t) * epsilon\n        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n    \n        return noisy_sample.detach(), epsilon\n    \n    \n    def forward(self, x_zeros):\n        x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n        \n        B, _, _, _ = x_zeros.shape\n        \n        # (1) randomly choose diffusion time-step\n        t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(self.device)\n        \n        # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n        perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n        \n        # (3) predict epsilon(noise) given perturbed data at diffusion-timestep t.\n        pred_epsilon = self.model(perturbed_images, t)\n        \n        return perturbed_images, epsilon, pred_epsilon\n    \n    \n    def denoise_at_t(self, x_t, timestep, t):\n        B, _, _, _ = x_t.shape\n        if t &gt; 1:\n            z = torch.randn_like(x_t).to(self.device)\n        else:\n            z = torch.zeros_like(x_t).to(self.device)\n        \n        # at inference, we use predicted noise(epsilon) to restore perturbed data sample.\n        epsilon_pred = self.model(x_t, timestep)\n        \n        alpha = self.extract(self.alphas, timestep, x_t.shape)\n        sqrt_alpha = self.extract(self.sqrt_alphas, timestep, x_t.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, timestep, x_t.shape)\n        sqrt_beta = self.extract(self.sqrt_betas, timestep, x_t.shape)\n        \n        # denoise at time t, utilizing predicted noise\n        x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n        \n        return x_t_minus_1.clamp(-1., 1)\n                \n    def sample(self, N):\n        # start from random noise vector, x_0 (for simplicity, x_T declared as x_t instead of x_T)\n        x_t = torch.randn((N, self.img_C, self.img_H, self.img_W)).to(self.device)\n        \n        # autoregressively denoise from x_T to x_0\n        #     i.e., generate image from noise, x_T\n        for t in range(self.n_times-1, -1, -1):\n            timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(self.device)\n            x_t = self.denoise_at_t(x_t, timestep, t)\n        \n        # denormalize x_0 into 0 ~ 1 ranged values.\n        x_0 = self.reverse_scale_to_zero_to_one(x_t)\n        \n        return x_0\n    \n    \ndiffusion = Diffusion(model, image_resolution=img_size, n_times=n_timesteps, \n                      beta_minmax=beta_minmax, device=DEVICE).to(DEVICE)\n\noptimizer = Adam(diffusion.parameters(), lr=lr)\ndenoising_loss = nn.MSELoss()\n\n\n\n2.5 エンコーディングの様子\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"Number of model parameters: \", count_parameters(diffusion))\n\nNumber of model parameters:  4870913\n\n\n\n\nデータセットの読み込みと show_image() の定義\nmodel.eval()\nfor batch_idx, (x, _) in enumerate(test_loader):\n    x = x.to(DEVICE)\n    perturbed_images, epsilon, pred_epsilon = diffusion(x)\n    perturbed_images = diffusion.reverse_scale_to_zero_to_one(perturbed_images)\n    break\n\ndef show_image(x, idx):\n    fig = plt.figure()\n    plt.imshow(x[idx].transpose(0, 1).transpose(1, 2).detach().cpu().numpy())\n\n\nshow_image(perturbed_images, idx=0)\nshow_image(perturbed_images, idx=1)\nshow_image(perturbed_images, idx=2)\n\n\n\n\n\n\n\n\n\n\n\n(a) 左がテストデータ，右に行くほど完全なノイズに近づく．\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n図 1"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "",
    "text": "(草野耕一, 2016) の勉強会第1回の補足として，確率論の数学的枠組みを紹介する．\n参考書としては (大塚淳, 2020) もおすすめ．\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#今回の内容",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n\n1.1 本書の概観\n本書は「数理法務」＝「法の数理分析」に関する発展的内容を扱った書籍で，内容は大きく次の1から3の3つからなる：\n\n法の行動分析：法律家がとるべき行動を数理を用いて分析する．\n法の統計分析：事実の推定や因果関係の推定に統計手法を応用する．\n法の財務分析：企業や金融に関わる法事象をファイナンス理論を用いて分析する．\n法の経済分析：法を経済学的な観点から分析する（本書では扱われていない）．\n\n第1回勉強会では第1章「行動分析(1)事実認定」を扱った．事実認定を，Bayes推論の枠組みで捉え直し，法律家として誤謬やバイアスに陥ってしまうことを避けるツールとして，確率論を導入しており，「法の数理分析は役に立つ」ことを端的に実感できる，導入として極めて鮮やかな章になっている．\n\n\n1.2 主観確率とBayes計算\nまず，第1章は，事実認定の文脈で妥当な確率概念は「主観確率」であり，今後「確率」とはこの意味で用いることを注意喚起する内容から始まる．\n主観確率と客観確率の詳細な定義は本書を参照願いたいが，一言で言えば，後者は「人間に不可知な真の値」というものの存在を前提とするのに対し，前者はそれを仮定しない．\n従って，主観確率の考え方は，より多くのものに「確率」を導入することを可能にし，より柔軟な議論が可能であるが，その分数理的な困難も増し，真に発展が進んだと言えるのは，計算機が十分に爛熟した21世紀になってのことであると言える．この統計学分野を Bayes計算 (Bayesian Computation) といい，筆者の研究分野である．\n\nThe development of computing algorithms especially suited for Bayesian analysis in the 1990s together with the exponential growth of computing resources enabled Bayesian nonparametrics to go beyond the simplest problems and made it a universally applicable paradigm for inference. (Ghosal and van der Vaart, 2017)\n\n\n\n1.3 Bayes統計学とは？\n大雑把に言って，客観確率に基づく統計手法を 頻度論的手法 (frequentist methods)，主観確率に対する統計手法を Bayes手法 (Bayesian methods)という．一般に後者は前者を包含する（前者は後者の特別な場合1）と考えられる．しかしこれは「確率の解釈」が違うのみであり，数学としては確率の定義は1つである．「確率の解釈」については，双方の立場の中でもそれぞれ複数の立場が乱立しており，ここでは立ち入らない．と言っても，この注記も教科書的なもので，実用上不便を生じる場面はほとんどないだろう．\n\n不確実性を定量化するのに、ベイズ計算では必ず『確率』を使います。一般の人から見たら、統計で確率を使うのは当たり前と思うでしょうが、じつは他の統計手法ではそうでもなく、さまざまな解釈が生まれてしまう。定量化にはすべて統一的に確率を使うベイズ計算は、非常にシンプルなので、最終的にすべての統計はベイズに行き着くしかないと思っています。 鎌谷研吾\n\n\n\n1.4 Bayes確率の基礎付けの試み……！？\n法律家による事実認定の文脈においても，「真実はいつも1つだからそれを推定したい」と考えても，「不確実な中でも，判断を誤らないようにしたい」と考えても，どちらから議論しても良いことは納得いただけるだろう．ただ，一般の人の素朴な「確率」の理解は，Bayes流のものに近いと言われている．2\nそのこともあり，本書で「主観確率の考え方を採用する」というのは，「確率の解釈の議論はここではしない」「主観的な確信度合いの意味で，現実から多少の乖離を許す」という程度の意味であろう．\nしかし，本書の「主観確率」の議論は中途半端な取り扱いでは終わらず，興味深いことに，One More Step 1-1 (pp.9-10) にて，数理哲学者Donald A. Gilliesによる主観確率の測定による基礎付けの議論が紹介されていた．筆者は初耳の議論であり，己の議論の正統性・基礎付けに細心の注意を施す法律家の心が現れていると筆者は見た．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#sec-2",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#sec-2",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "2 【深掘り】確率の公理",
    "text": "2 【深掘り】確率の公理\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n2.1 「確率の公理」がなぜ重要なのか？\n本書1.1節では確率の性質が列挙されている．1.2節以降では，これらの性質が「確率の定義」として引用されるが，いまいちどれを指して「定義」と呼んでいるのか定かでない．\n数学的な議論に慣れたあとはそれでも良いかも知れないが，法学も初学の間は逐一根拠条文に戻ることが大切であるように，数学もはっきりと定義を列挙し，「それのみを根拠とすること」を徹底することが大事である．\nなお，数学では何を定義として採用するかに任意性がある場合が多いが，唯一やってはいけないことは「定義が曖昧な状態で進むこと」である．そこで，せっかくであるから，現代数学が定義する最も筋の良い定義を採用して，本書の内容を俯瞰することにする．\n\n人は，確率論のもった政治的，社会的意義を忘れてはならない．理知を一切の尺度として「代数学の炉火によって倫理学及び政治学を照さん」(Condorcet) という時代精神，神の啓示に代らんとする確率論，それはフランス革命の思想的基礎に連関することを見失ってはならないのである．(北川敏男, 1949)\n\n現代数学において，確率を特徴付けるものは「代数的性質」であり，それは次の3つのみに集約される．3\n\n\n2.2 確率の公理\n\n\n\n\n\n\n定義（確率） (Kolmogorov, 1931)\n\n\n\n集合 \\(\\Omega\\) 上の確率とは，次の3条件を満たす関数 \\(P:\\mathcal{P}(\\Omega)\\to\\mathbb{R}\\) である：4\n   [P1] \\(P(\\Omega)=1\\)．\n   [P2] \\(A\\cap B=\\emptyset\\) ならば， \\[P(A\\sqcup B)=P(A)+P(B).\\]\n   [P3] 任意の事象 \\(A\\subset\\Omega\\) について， \\[0\\le P(A).\\]5\nただし，\n\n\\(P\\) の定義域 \\(\\mathcal{P}(\\Omega)\\) は「\\(\\Omega\\) の部分集合全体の集合」のことである．これを \\(\\Omega\\) の冪集合という．\n\\(A\\sqcup B\\) とは， \\(A\\cap B=\\emptyset\\) が成り立つときの \\(A,B\\) の合併 \\(A\\cup B\\) を，\\(A\\cap B=\\emptyset\\) を強調して書き分ける記法とする．\n\n\n\nこの公理から，我々が日常的な感覚から「確率に成り立っていて欲しい性質」が全て導ける，ということが現代数学の重要な発見である．性質を見ていく前に，「定義」として，主要な概念に親しみやすい名前を付ける．そのすべての過程において，上の[P1], [P2], [P3]以外を用いていないことを確認することは，数学入門の際には非常に大事な営みである．6\n\n\n\n\n\n\n確率論に関連する用語\n\n\n\n全体集合 \\(\\Omega\\) は所与のものとする．7\n\n事象 とは，部分集合 \\(A\\subset\\Omega\\) のことをいう．\n事象 \\(A\\subset\\Omega\\) の補集合\\[A^\\complement=\\Omega\\setminus A=\\overline{A}:=\\left\\{\\omega\\in\\Omega\\mid \\omega\\notin A\\right\\}\\]を \\(A\\) の余事象という．左から順に，数学で一般によく使われる記号である．8\n2つの事象 \\(A,B\\subset\\Omega\\) が 排反 であるとは，集合として共通部分を持たないことをいう： \\(A\\cap B=\\emptyset\\)．\n\n\n\nこの3性質から，本書第1.1節にいう「確率の推論法則」が全て導出できる．\n\n\n2.3 式(1.1) p.4の証明\n\n\n\n\n\n\n式(1.1) p.4\n\n\n\n任意の事象 \\(A\\subset\\Omega\\) について，\\[0\\le P(A)\\le 1.\\]9\n\n\n\n\n\n\n\n\n式(1.2) p.5\n\n\n\n任意の事象 \\(A\\subset\\Omega\\) について， \\[\nP(A)+P(A^\\complement)=1.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(0\\le P(A)\\) は[P3]に他ならない．式(1.2)も[P2]から従う．\\(A\\subset\\Omega\\) の補集合を \\(A^\\complement:=\\Omega\\setminus A\\) で表すと， \\(P(A^\\complement)\\ge0\\) も成り立つから， \\[\n\\begin{align*}\nP(A)&\\le P(A)+P(A^\\complement)\\\\\n&=P(A\\sqcup A^\\complement)\\\\\n&=P(\\Omega)=1.\n\\end{align*}\n\\]\n\n\n\n\n2.4 式(1.3) p.5の証明\n\n\n\n\n\n\n式(1.3) p.5\n\n\n\n任意の \\(n\\ge1\\) について， \\(n\\) 個の事象 \\(A_1,\\cdots,A_n\\subset\\Omega\\) が互いに排反であるとき， \\[\n\\begin{align*}\n&P(A_1)+P(A_2)+\\cdots+P(A_n)\\\\\n&\\qquad\\qquad=P(A_1\\sqcup A_2\\sqcup\\cdots\\sqcup A_n).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(n\\) に関する数学的帰納法による．\n\n\n\n\n2.5 式(1.4) p.6の証明\n\n\n\n\n\n\n式(1.4) p.6\n\n\n\n任意の事象 \\(A,B\\subset\\Omega\\) について， \\[\nP(A)+P(B)=P(A\\cup B)+P(A\\cap B).\n\\]\n\n\n[P2] の条件は，\\(A_1,A_2\\) が排反である場合に限定しており，その制限が邪魔であった．ここで一般の加法公式を得ることになる．\n\n\n\n\n\n\n証明\n\n\n\n\\(C:=A\\cap B\\) とおくと，3つの集合 \\(A\\setminus B,C,B\\setminus A\\) が互いに排反であることから，\n\\[\n\\begin{align*}\n&\\quad P(A)+P(B)\\\\\n&=\\biggr(P(A\\setminus B)+P(C)\\biggl)+\\biggr(P(C)+P(B\\setminus A)\\biggl)\\\\\n&=\\biggr(P(A\\setminus B)+P(C)+P(B\\setminus A)\\biggl)+P(C)\\\\\n&=P(A\\cup B)+P(A\\cap B).\n\\end{align*}\n\\]\n\n\n\n\n&lt;&gt;:12: SyntaxWarning: invalid escape sequence '\\B'\n&lt;&gt;:13: SyntaxWarning: invalid escape sequence '\\A'\n&lt;&gt;:12: SyntaxWarning: invalid escape sequence '\\B'\n&lt;&gt;:13: SyntaxWarning: invalid escape sequence '\\A'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_22320/1376407110.py:12: SyntaxWarning: invalid escape sequence '\\B'\n  venn.get_label_by_id('10').set_text('A\\B')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_22320/1376407110.py:13: SyntaxWarning: invalid escape sequence '\\A'\n  venn.get_label_by_id('01').set_text('B\\A')\n\n\n\n\n\n\n\n\n\n\n\n2.6 条件付き確率の定義\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\(A,B\\subset\\Omega\\) を事象とする． 事象 \\(A\\) が起こった場合の，事象 \\(B\\) の条件付き確率とは， \\[\nP(B|A):=\\begin{cases}\\frac{P(A\\cap B)}{P(A)}&P(A)\\ne0\\;\\text{のとき}\\\\0&P(A)=0\\;\\text{のとき}\\end{cases}\n\\] という値を指す．10\n\n\n\n\n2.7 式(1.7) p.7の証明\n\n\n\n\n\n\n式(1.7) p.7\n\n\n\n\\(A,B\\subset\\Omega\\) を事象，\\(P(A)&gt;0\\) とする． \\[\nP(A|B)+P(A^\\complement|B)=1.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\[\n\\begin{align*}\n&\\quad P(A|B)+P(A^\\complement|B)\\\\\n&=\\frac{P(A\\cap B)}{P(B)}+\\frac{P(A^\\complement\\cap B)}{P(B)}\\\\\n&\\overset{\\text{[P2]}}{=}\\frac{P((A\\cap B)\\sqcup (A^\\complement\\cap B))}{P(B)}\\\\\n&=\\frac{P(B)}{P(B)}=1.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#sec-independent",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#sec-independent",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "3 【重要概念】統計的独立性",
    "text": "3 【重要概念】統計的独立性\n\n3.1 定義\n\n\n\n\n\n\n定義（独立性）\n\n\n\n2つの事象 \\(A,B\\subset\\Omega\\) が独立であるとは，次を満たすことをいう： \\[\nP(A\\cap B)=P(A)P(B).\n\\] このとき， \\(A\\perp\\!\\!\\!\\perp B\\) と表す．\n\n\nこの式は本書p.7 (1.8)式に一致している．これを「積の公式」として導出しているが，これは実は独立性の定義とすべき性質である．その意味するところを次節で解説する．\n\n\n3.2 条件付き確率による特徴付け\nSection 2 で「数学では何を定義として採用するかに任意性がある場合が多い」と言った．今回の「独立性」概念も，2つの同値な定義がある．しかし，「唯一やってはいけないことは定義が曖昧な状態で進むことである」とも言った．従って，どちらか片方を定義とし，「定義ともう一つの条件が同値である」という命題が生まれることになる．\nこの形の命題のことを（数学概念の）特徴付け という．このことを解説するWikipediaページもある．\n\n\n\n\n\n\n命題（独立性の特徴付け）\n\n\n\n2つの事象 \\(A,B\\subset\\Omega\\) について，次の2条件は同値：\n\n\\(A,B\\) は独立である：\\(A\\perp\\!\\!\\!\\perp B\\)．\n\\(P(B|A)=P(B)\\)．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(P[A]=0\\) の場合，任意の \\(B\\) について(1),(2)はいずれも常に成り立つ． あとは，\\(P[A]\\ne0\\)の場合を考える． すると，条件付き確率の定義 \\[P[A\\cap B]=P[A]P[B|A]\\] を考えれば，この右辺が\\(P[A]P[B]\\)に等しいことと，\\(P[B|A]=P[B]\\)であることとは同値．\n\n\n本書では2.の性質の方を定義としているが， \\(P(B|A)\\) という量は， \\(P(A)=0\\) の場合に定義に任意性が残る．従って，1.の方が定義として明瞭ということになる．\nさらに重要なことには，1.の方が一般個数の事象 \\(A_1,\\cdots,A_n\\) の場合に「独立性」の概念の拡張が可能であり，より本質的な定義だと思われる，ということが確率論の示唆である．実は，無限個の事象が独立であることも同様に定義する．\n\n\n\n\n\n\n定義（独立性）\n\n\n\n集合族 \\(\\{A_\\lambda\\}_{\\lambda\\in\\Lambda}\\subset\\mathcal{F}\\) が独立であるとは，任意の \\(n\\in\\mathbb{N}\\) 個の相異なる元 \\(A_{\\lambda_1},\\cdots,A_{\\lambda_n}\\) に対して， \\[P[A_{\\lambda_1}\\cap\\cdots\\cap A_{\\lambda_n}]=P[A_{\\lambda_1}]\\cdots P[A_{\\lambda_n}]\\] が成り立つことをいう．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#余談数学について",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#余談数学について",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "4 【余談】数学について",
    "text": "4 【余談】数学について\nここまでを読んだ読者の中で，「集合」「写像」の言葉に，定義が十分に提示されていないと感じたものがあるなら，あなたは極めて筋が良い．実は，これらの裏に全て厳密な定義があるのが数学であるが，今回は確率論に集中するために省いた．\n実際，確率論をKolmogorovによる確率の公理的な定義 (Kolmogorov, 1931) から始まる数学分野だとするならば，これはまだ100年の歴史もない，数学分野にしては極めて珍しい若い分野である．\n確率論の確率が遅れた理由は，「確率」の概念がつかみどころのない日常に根ざした概念であり，抽象化が本質的に難しいこともあるだろうが，第一に「集合」「写像」といった概念が十分に数学者の間で理解が深まるのを待つ必要があったということがある．\n現代の確率論では，「確率は測度の特別なものである」という態度をとっていることは本文中でも述べたが，この「測度」という概念の成立が，そもそもLebesgueによる積分論が確立される20世紀に入るのを待つ必要があった．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#footnotes",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(McElreath, 2020) 1.3節．頻度論はさらに「仮想的な反復」(imaginary resampling of data)を想定する，という性質を除けば，不確実性が観測からくるもののみである特別な場合が頻度論であると捉えられる．↩︎\n(McElreath, 2020) 1.3節，(Rubin, 1984)．↩︎\nWikipediaページ確率の公理も参照．↩︎\n関数とは，入力と出力の集合 \\(X,Y\\) の間に定まる対応であって，任意の入力 \\(x\\in X\\) に対してただ一つの出力 \\(y\\in Y\\) が対応するもののことをいう．この対応を \\(f(x)=y\\) と表す．↩︎\n後ろの2条件[P2], [P3] のみを満たす関数 \\(P\\) は「測度」という．そのため，確率は測度でもある．数学用語では「確率分布」は「確率測度」ともいう（例えばこのwikipediaページ）．↩︎\n[P1] などの P は Probability のつもりである．↩︎\n集合にも公理があり，現代数学はZFC公理系の下で展開される．が，ここでは深入りしない．↩︎\n\\(\\lnot A\\) という記法について，\\(\\lnot\\) は論理記号であるから，集合 \\(A\\) に用いることは好ましくない．↩︎\n確率は必ず\\(0\\)から\\(1\\)の値を取る，ということを主張している命題である．初学者はこれが「示すべき内容」として提示されていることに戸惑いを覚えるだろうが，現代数学では「これが示せるような必要最小限の定義が見つかった」ことに価値を見出す．↩︎\nここでは \\(P(A)=0\\) の場合は \\(0\\) としたが，実際はどんな値でも良い．↩︎"
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Hello!",
    "text": "Hello!\nMy name is Hirofumi Shiba. Feel free to call me Hiro.\nI am an applied mathematician by training, now working on Computational Statistics.\nI am currently a Ph.D. candidate at the Institute of Statistical Mathematics (ISM) in Tokyo, Japan, advised by Prof. Kengo Kamatani.\nMy research focuses on developing more efficient, scalable and automated sampling algorithms, such as Markov Chain Monte Carlo and Sequential Monte Carlo, to broaden the applications of statistics and machine learning."
  },
  {
    "objectID": "index.html#keywords",
    "href": "index.html#keywords",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Keywords",
    "text": "Keywords\n\nMonte Carlo Computation (MCMC, SMC, etc.)\nTransport Methods (Schrödinger Bridges, Normalizing Flows, etc.)\nBayesian Modeling (Political Science, Biology, etc.)\nBayesian Machine Learning (Gaussian Processes, BDL, etc.)"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Research Interests",
    "text": "Research Interests\nMy research interests center around developing and analysing sampling algorithms, by leveraging insights from their continuous-time limit dynamics. From this perspective, algorithms reveal their intrinsic properties, and my work aims to unify the understanding of various algorithms under a common framework."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Education",
    "text": "Education\n\n Ph.D. in Statistical Science, 2028 (expected)\nGraduate University for Advanced Studies, SOKENDAI, Japan\n\n\n B.A. in Mathematics, 2023\nUniversity of Tokyo, Japan"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Experience",
    "text": "Experience\n\n Cooperative Researcher\nRCAST, the University of Tokyo Economic Security Resarch Program\n\n\n Research Assistant\nResearch Organization of Information and Systems, Tokyo, Japan"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Biography",
    "text": "Biography\n\nI was born in Yokohama, Japan in May 1999. While holding Japanese citizenship, I am a native speaker of Japanese and Mandarin Chinese.\nFrom the very beginning of my academic journey, I have been fascinated by Mathematics. To me, it is an organized wisdom to exploit connections between seemingly unrelated concepts.\nAs Poincaré noted “Mathematics is the art of giving the same name to different things,” transforming mere analogy into profound insights readily applicable.\nCurrently, I work on Computational Statistics, seeing it from the perspective of stochastic processes as a system of evolving probability measures.\nBefore this role, I was a data scientist at IMIS Co., Ltd., providing data analysis and consulting for Japanese manufacturing companies.\nAs a Bayesian computation practitioner, I also contribute to the R package YUIMA, an open-source project designed for simulating and inferring multidimensional stochastic differential equations."
  },
  {
    "objectID": "posts/2024/Kernels/GAN.html",
    "href": "posts/2024/Kernels/GAN.html",
    "title": "GAN の実装",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/GAN.html#文献",
    "href": "posts/2024/Kernels/GAN.html#文献",
    "title": "GAN の実装",
    "section": "1 文献",
    "text": "1 文献\n\n本稿は，Minsu Jackson Kang 氏 による チュートリアル を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html",
    "href": "posts/2024/Samplers/EBM2.html",
    "title": "エネルギーベースモデル",
    "section": "",
    "text": "李飞氏 による 実装 を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html#文献",
    "href": "posts/2024/Samplers/EBM2.html#文献",
    "title": "エネルギーベースモデル",
    "section": "",
    "text": "李飞氏 による 実装 を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html",
    "href": "posts/2024/Samplers/NF2.html",
    "title": "正規化流",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html#文献",
    "href": "posts/2024/Samplers/NF2.html#文献",
    "title": "正規化流",
    "section": "1 文献",
    "text": "1 文献\n\n本稿は，Eric Jang 氏 による チュートリアル を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/SM.html",
    "href": "posts/2024/Samplers/SM.html",
    "title": "スコアマッチング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/SM.html#sgm-score-based-generative-model",
    "href": "posts/2024/Samplers/SM.html#sgm-score-based-generative-model",
    "title": "スコアマッチング",
    "section": "1 SGM (Score-based Generative Model)",
    "text": "1 SGM (Score-based Generative Model)\n\n1.1 目標のデータ\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_swiss_roll\n\nimport jax\nimport jax.numpy as jnp\n\ntry:\n    from flax import linen as nn  # The Linen API\nexcept ModuleNotFoundError:\n    %pip install -qq flax\n    from flax import linen as nn  # The Linen API\nfrom flax.training import train_state  # Useful dataclass to keep train state\n\ntry:\n    import optax  # Optimizers\nexcept ModuleNotFoundError:\n    %pip install -qq optax\n    import optax  # Optimizers\n\nfrom functools import partial\n\nfrom IPython.display import clear_output\n\n\n\n\nCode\ndef sample_batch(size, noise=1.0):\n    x, _ = make_swiss_roll(size, noise=noise)\n    x = x[:, [0, 2]] / 10.0\n    return np.array(x)\n\n\n# plt.figure(figsize=[16, 16])\nplt.scatter(*sample_batch(10**4).T, alpha=0.1)\nplt.axis(\"off\")\nplt.tight_layout()\n# plt.savefig(\"swiss_roll.png\")\n\n\n\n\n\n\n\n\n\n\n\n1.2 モデルの定義\n\n\nCode\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n\n        x = nn.Dense(128)(x)\n        x = nn.softplus(x)\n        x = nn.Dense(128)(x)\n        x = nn.softplus(x)\n        x = nn.Dense(2)(x)\n\n        return x\n\n\n\n\n1.3 モデルの訓練\n\n\nCode\n@jax.jit\ndef compute_loss(params, inputs):\n    #  a function that computes jacobian by forward mode differentiation\n    jacobian = jax.jacfwd(Model().apply, argnums=-1)\n\n    # we use jax.vmap to vectorize jacobian function along batch dimension\n    batch_jacobian = jax.vmap(partial(jacobian, {\"params\": params}))(inputs)  # [batch, dim, dim]\n\n    trace_jacobian = jnp.trace(batch_jacobian, axis1=1, axis2=2)\n    output_norm_sq = jnp.square(Model().apply({\"params\": params}, inputs)).sum(axis=1)\n\n    return jnp.mean(trace_jacobian + 1 / 2 * output_norm_sq)\n\n\n\n\nCode\n@jax.jit\ndef train_step(state, batch, key):\n    \"\"\"Train for a single step.\"\"\"\n    loss = compute_loss(state.params, batch)\n    grads = jax.grad(compute_loss, argnums=0)(state.params, batch)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\ndef create_train_state(rng, learning_rate):\n    \"\"\"Creates initial `TrainState`.\"\"\"\n    net = Model()\n    params = net.init(rng, jnp.ones([128, 2]))[\"params\"]\n    tx = optax.adam(learning_rate)\n    return train_state.TrainState.create(apply_fn=net.apply, params=params, tx=tx)\n\ndef train_loop(key, train_step, nsteps):\n    key, subkey = jax.random.split(key)\n    state = create_train_state(subkey, 1e-3)\n    del subkey  # Must not be used anymore.\n    loss_history = []\n    for i in range(nsteps):\n        x = sample_batch(size=128)\n        key, subkey = jax.random.split(key)\n        state, loss = train_step(state, x, subkey)\n        loss_history.append(loss.item())\n\n        if i % 200 == 0:\n            clear_output(True)\n            plt.figure(figsize=[16, 8])\n            plt.subplot(1, 2, 1)\n            plt.title(\"mean loss = %.3f\" % jnp.mean(jnp.array(loss_history[-32:])))\n            plt.scatter(jnp.arange(len(loss_history)), loss_history)\n            plt.grid()\n\n            plt.subplot(1, 2, 2)\n            xx = jnp.stack(jnp.meshgrid(jnp.linspace(-1.5, 2.0, 50), jnp.linspace(-1.5, 2.0, 50)), axis=-1).reshape(\n                -1, 2\n            )\n            scores = Model().apply({\"params\": state.params}, xx)\n            scores_norm = jnp.linalg.norm(scores, axis=-1, ord=2, keepdims=True)\n            scores_log1p = scores / (scores_norm + 1e-9) * jnp.log1p(scores_norm)\n\n            plt.quiver(*xx.T, *scores_log1p.T, width=0.002, color=\"green\")\n            plt.xlim(-1.5, 2.0)\n            plt.ylim(-1.5, 2.0)\n            plt.show()\n\n    return state\n\nstate = train_loop(jax.random.PRNGKey(seed=42), train_step, 10000)\n\n\n\n\n\n\n\n\n\n\n\n1.4 学習されたスコア\n\n\nCode\n# plt.figure(figsize=[16, 16])\n\nxx = jnp.stack(jnp.meshgrid(jnp.linspace(-1.5, 1.5, 50), jnp.linspace(-1.5, 1.5, 50)), axis=-1).reshape(-1, 2)\nscores = Model().apply({\"params\": state.params}, xx)\nscores_norm = jnp.linalg.norm(scores, axis=-1, ord=2, keepdims=True)\nscores_log1p = scores / (scores_norm + 1e-9) * jnp.log1p(scores_norm)\n\nplt.quiver(*xx.T, *scores_log1p.T, width=0.002, color=\"green\")\nplt.scatter(*sample_batch(10_000).T, alpha=0.1)\nplt.axis(\"off\")\nplt.tight_layout()\n# plt.savefig(\"score_matching_swiss_roll.png\")\n\n\n\n\n\n\n\n\n図 1"
  },
  {
    "objectID": "posts/2024/Samplers/SM.html#ssm-sliced-score-matching",
    "href": "posts/2024/Samplers/SM.html#ssm-sliced-score-matching",
    "title": "スコアマッチング",
    "section": "2 SSM (Sliced Score Matching)",
    "text": "2 SSM (Sliced Score Matching)"
  },
  {
    "objectID": "posts/2024/Samplers/SM.html#jax-framework",
    "href": "posts/2024/Samplers/SM.html#jax-framework",
    "title": "スコアマッチング",
    "section": "3 JAX framework",
    "text": "3 JAX framework\n\n3.1 導入\nGoogle の JAX (GitHub) とは，科学計算と機械学習のためのフレームワークである．\nAutograd (GitHub) を用いて，Python のビルトイン関数や NumPy 関数を自動微分することができる．\n今回は JAX エコシステムの一つである，深層学習のためのフレームワークである Flax，特に Linen モジュール (docs / GitHub) を用いた．"
  },
  {
    "objectID": "posts/2024/Samplers/SM.html#文献",
    "href": "posts/2024/Samplers/SM.html#文献",
    "title": "スコアマッチング",
    "section": "4 文献",
    "text": "4 文献\n\nコードは (Murphy, 2023) の こちら を参考にした．\n(Song et al., 2019) のコードは このレポジトリ で公開されている．\nUvA DL Tutorial も参照．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#sec-NCSM",
    "href": "posts/2024/Samplers/Diffusion.html#sec-NCSM",
    "title": "拡散模型",
    "section": "3 スコアマッチング",
    "text": "3 スコアマッチング\n\n3.1 導入\nスコアマッチングとはエネルギーベースモデルの文脈で生まれた手法であるが，そもそもスコア \\[\n\\nabla_x\\log p(x)\n\\] 自体を学習することを生成モデリングの中心に据えることが SGM (Score-based Generative Model) (Y. Song and Ermon, 2019) で考えられた．エネルギー関数とスコア関数，どっちを学習の中心に据えるかについては (Salimans and Ho, 2021) も参照．\nしかしスコアを学習するにあたって最も致命的な点は，\\(\\nabla_x\\log p(x)\\) の値は \\(p\\) を一意的に定めないということである．特に， \\[\np=\\pi p_0+(1-\\pi)p_1,\\qquad\\pi\\in(0,1)\n\\] という関係があり，\\(p_0,p_1\\) の台が互いに素であったとき，\\(\\nabla_x\\log p\\) からは \\(\\pi\\in(0,1)\\) を定めるための情報が完全に消えてしまう．\n\n\n\nデータが青，学習されたスコアが緑．\n\n\n\n\n3.2 デノイジングによる解決\n(Y. Song and Ermon, 2019), (Y. Song and Ermon, 2020), (Y. Song, Sohl-Dickstein, et al., 2021) はデータにノイズを印加し，これを除去する方向に学習することで，スコアマッチングの問題点を克服することを考えた．\nノイズを印加するとは，Gauss 核との畳み込みにより分布を軟化していくことに相当し，アニーリングと同じ効果を持つ．その結果，多峰性が消失してスコアマッチングが正確になる．加えて MCMC によるサンプリングも容易になる．\nあとは，ノイズを失くしていく極限を取ると，限りなく正しくデータ分布を学ぶことができるようになっていく．\n\n\n3.3 Noise Conditional Score Network (NCSN) (Y. Song, Durkan, et al., 2021)\nこの「データにノイズを印加して学びやすくし，徐々にノイズを除去していく」というアイデアは拡散模型のそれに他ならず，スコアマッチングと拡散模型が邂逅するのは時間の問題だったと言えるだろう．\nスコアのスケール \\(\\sigma\\) の Gauss ノイズによる軟化列を \\(s_\\theta(x,\\sigma)\\) と表すと，デノイジングスコアマッチング (DNS) の目的関数の軟化列を得る： \\[\n\\mathcal{L}(\\theta;\\sigma)=\\frac{1}{2}\\operatorname{E}\\left[\\left\\|s_\\theta(\\widetilde{X},\\sigma)+\\frac{\\widetilde{X}-X}{\\sigma^2}\\right\\|^2_2\\right].\n\\] これを組み合わせた目的関数 \\[\n\\mathcal{L}(\\theta;\\sigma_{1:T})=\\sum_{t=1}^T\\lambda_t\\mathcal{L}(\\theta;\\sigma_t),\\qquad\\lambda_t&gt;0,\n\\] を最終的な目的関数とする．\n\n\n3.4 DDPM との対応"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#sec-SGM",
    "href": "posts/2024/Samplers/Diffusion.html#sec-SGM",
    "title": "拡散模型",
    "section": "3 スコアベースの生成模型 (SGM)",
    "text": "3 スコアベースの生成模型 (SGM)\n\n3.1 導入\nスコアマッチングとはエネルギーベースモデルの文脈で生まれた手法であるが，そもそもスコア \\[\n\\nabla_x\\log p(x)\n\\] 自体を学習することを生成モデリングの中心に据えることが SGM (Score-based Generative Model) (Y. Song and Ermon, 2019) で考えられた．エネルギー関数とスコア関数，どっちを学習の中心に据えるかについては (Salimans and Ho, 2021) も参照．\nしかしスコアを学習するにあたって最も致命的な点は，\\(\\nabla_x\\log p(x)\\) の値は \\(p\\) を一意的に定めないということである．特に， \\[\np=\\pi p_0+(1-\\pi)p_1,\\qquad\\pi\\in(0,1)\n\\] という関係があり，\\(p_0,p_1\\) の台が互いに素であったとき，\\(\\nabla_x\\log p\\) からは \\(\\pi\\in(0,1)\\) を定めるための情報が完全に消えてしまう．\n\n\n\nデータが青，学習されたスコアが緑．\n\n\n\n\n3.2 デノイジングによる解決\n(Y. Song and Ermon, 2019), (Y. Song and Ermon, 2020), (Y. Song, Sohl-Dickstein, et al., 2021) はデータにノイズを印加し，これを除去する方向に学習することで，スコアマッチングの問題点を克服することを考えた．\nノイズを印加するとは，Gauss 核との畳み込みにより分布を軟化していくことに相当し，アニーリングと同じ効果を持つ．その結果，多峰性が消失してスコアマッチングが正確になる．加えて MCMC によるサンプリングも容易になる．\nあとは，ノイズを失くしていく極限を取ると，限りなく正しくデータ分布を学ぶことができるようになっていく．\n\n\n3.3 Noise Conditional Score Network (NCSN) (Y. Song, Durkan, et al., 2021)\nこの「データにノイズを印加して学びやすくし，徐々にノイズを除去していく」というアイデアは拡散模型のそれに他ならず，スコアマッチングと拡散模型が邂逅するのは時間の問題だったと言えるだろう．\nスコアのスケール \\(\\sigma\\) の Gauss ノイズによる軟化列を \\(s_\\theta(x,\\sigma)\\) と表すと，デノイジングスコアマッチング (DNS) の目的関数の軟化列を得る： \\[\n\\mathcal{L}(\\theta;\\sigma)=\\frac{1}{2}\\operatorname{E}\\left[\\left\\|s_\\theta(\\widetilde{X},\\sigma)+\\frac{\\widetilde{X}-X}{\\sigma^2}\\right\\|^2_2\\right].\n\\] これを組み合わせた目的関数 \\[\n\\mathcal{L}(\\theta;\\sigma_{1:T})=\\sum_{t=1}^T\\lambda_t\\mathcal{L}(\\theta;\\sigma_t),\\qquad\\lambda_t&gt;0,\n\\] を最終的な目的関数とする．\n\n\n3.4 DDPM との対応\n以上，完全にスコアマッチングの観点から述べたが，NCSN は，ノイズスケジュールが異なるのみで本質的に DDPM 2 と同じ枠組みであるとみなせる (Huang et al., 2021)．\n実際， \\[\n\\mathcal{L}(\\theta;\\sigma_t^2)=\\mathcal{L}_{\\text{simple}}\n\\] が成り立つ．3"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#連続時間極限",
    "href": "posts/2024/Samplers/Diffusion.html#連続時間極限",
    "title": "拡散模型",
    "section": "4 連続時間極限",
    "text": "4 連続時間極限\n\n4.1 導入\nここまで，タイムステップ \\(t\\) の取り方の議論をしていなかった．これはサンプリングに用いている拡散過程を離散化しているわけだから，その数学を借りることで更なる知見が得られる (Tzen and Raginsky, 2019), (Y. Song, Sohl-Dickstein, et al., 2021)．\n加えて，データ分布をノイズ分布に還元するフローは，何も拡散過程だけが与えるわけではない．確定的なものも含め，ほとんどあらゆるダイナミクスを代わりに用いることができる．\n特に，MCMC がベクトル場に関する決定論的なフローで効率化させられるように，SDE の代わりに ODE を使うことで拡散模型のサンプリングを効率化する事ができる．\n\n\n4.2 前向き拡散過程\nデータ分布を正規分布に還元する際に DDPM 2 で用いた拡散過程は，パラメータ \\(\\beta(t)\\) を持った \\(0\\) に回帰的な拡散過程である： \\[\ndX_t=-\\frac{\\beta(t)}{2}X_tdt+\\sqrt{\\beta(t)}dB_t,\\qquad\\beta\\left(\\frac{t}{T}\\right)=T\\beta_t.\n\\]\nこのことは (Y. Song, Sohl-Dickstein, et al., 2021) ですでに自覚されている．\n一方で，SGM 3 では \\[\ndX_t=\\sqrt{\\frac{d }{d t}\\sigma(t)^2}dB_t\n\\] で定まる拡散過程を用いる．\n\n\n4.3 後ろ向き拡散\n(Anderson, 1982), (Haussmann and Pardoux, 1986) によると，一般に \\[\ndX_t=f_t(X_t)\\,dt+\\sigma_t\\,dB_t\n\\] という SDE の時間反転は， \\[\ndY_t=\\biggr(f_t(Y_t)-\\sigma_t^2\\nabla_x\\log q_t(Y_t)\\biggl)\\,dt+\\sigma_t\\,dB_{-t}\n\\] が定める．Hyvärinen スコア関数が出てくるのである．4\n特に，DDPM では \\[\ndY_t=\\biggr(-\\frac{\\beta_t}{2}Y_t-\\beta_t\\nabla_{x}\\log q_t(Y_t)\\biggl)\\,dt+\\sqrt{\\beta_t}\\,dB_{-t}\n\\] と表せる．\nこのスコア関数 \\(\\nabla_x\\log q_t(X_t)\\) を DSM によって推定した \\[\ndY_t=-\\frac{\\beta_t}{2}\\biggr(Y_t+2s_\\theta(Y_t,t)\\biggl)\\,dt+\\sqrt{\\beta_t}\\,dB_{-t}\n\\] でデータ分布からサンプリングすることができるのである．なお，拡散過程のサンプリングは難しい問題であり，最も直接的には Euler-Maruyama 離散化を通じれば良い．\n\n\n4.4 等価な分布フローを定める確定的ダイナミクス\n代わりに ODE（(Y. Song, Sohl-Dickstein, et al., 2021 Sec D.3) が probability flow ODE と呼ぶ式） \\[\n\\frac{d x_t}{d t}=f(x,t)-\\frac{g(t)^2}{2}\\nabla_x\\log p_t(x)\n\\] で定まる確定的ダイナミクス \\(\\{x_t\\}\\) を用いても，同様にデータ分布は \\(\\mathrm{N}_d(0,I_d)\\) に還元される．\n\n\n4.5 後ろ向き ODE\nこの ODE を推定した \\[\n\\frac{d y_t}{d t}=f(y,t)-\\frac{g(t)^2}{2}s_\\theta(y_t,t)\n\\] を Euler 法，またはより高次な Heun 法などによって逆から解くことができる (Karras et al., 2022)．\nこれは連続時間正規化流，特に Neural ODE と等価なモデリングをすることになる．\n実は SDE は \\[\ndY_t=\\biggr(f(y,t)-\\frac{g(t)^2}{2}s_\\theta(y_t,t)\\biggl)-\\frac{\\beta_t}{2}s_\\theta(y_t,t)dt+\\sqrt{\\beta_t}\\,dB_t\n\\] と，上記の確率的フロー ODE に Langevin 拡散の項を加えた形になっており，ODE によるアプローチはこの追加の Langevin 拡散項を消去することに等しい．\n\n\n4.6 サンプリング加速法\nODE を用いることで拡散模型のサンプリング速度が向上する．(Nichol and Dhariwal, 2021) も参照．\n\n4.6.1 Denoising Diffusion Implicit Model (DDIM) (J. Song et al., 2021)\nモデリングに用いる拡散過程の Markov 性を崩すことで，DDPM 2 より 10 倍から 50 倍速いサンプリングを実現したもの．\n\n\n4.6.2 逐次サンプリング問題としての解決\nデコーダーに用いるモデル (1) の表現力を上げることで，ステップサイズを大きくしてもサンプリングの性能を悪化させないようにすることを試みる．\n例えば (Gao et al., 2021) は，ノイズ分布からデータ分布までのアニーリング列を，EBM の列として捉えてフィッティングをしている．同様のスキームで，(Xiao et al., 2022) は GAN を用いている．\n\n\n4.6.3 蒸留\n(Salimans and Ho, 2022) は Progressive Distillation と呼ばれる拡散過程の蒸留手法を提案している．\n学習済みの拡散モデル（(Salimans and Ho, 2022) では DDIM）から，徐々にステップ数を減らした蒸留モデルを作成していく．\n\n\n4.6.4 潜在拡散模型 (LDM) (Rombach et al., 2022)\nまず画像データを VAE などで学習した潜在空間に変換し，その上で拡散模型でモデリングをする．\nLatent Score-based Generative Model (LSGM) (Vahdat et al., 2021) では，VAE と拡散模型を同時に訓練することを考えている．\nこの手法は，潜在表現さえ適切に見つければ，複数のドメインのデータを同時に扱えるという美点がある．\nまた，(Pandey et al., 2022) では，まず VAE による生成モデルと作成し，その精度が足りない最終的な出力を高画質にするステップにのみ拡散模型を用いるスキームを提案している．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#誘導付き拡散模型",
    "href": "posts/2024/Samplers/Diffusion.html#誘導付き拡散模型",
    "title": "拡散模型",
    "section": "5 誘導付き拡散模型",
    "text": "5 誘導付き拡散模型\n拡散模型の更なる美点には，条件付けが可能で拡張性に優れているという点もある．\n\n5.1 Classifier Guidance (CG) (Dhariwal and Nichol, 2021)\n\n\n5.2 Classifier-Free Diffusion Guidance (Ho and Salimans, 2021)\n\n\n5.3 Cascaded Generation (Ho et al., 2022)"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html",
    "href": "posts/2024/Samplers/Schrodinger.html",
    "title": "Schrödinger 橋",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#導入",
    "href": "posts/2024/Samplers/Schrodinger.html#導入",
    "title": "Schrödinger 橋",
    "section": "1 導入",
    "text": "1 導入\n潜在空間 \\(\\mathcal{X}\\) 上の事前分布 \\(\\mu\\) と，尤度が確率核 \\(\\mathcal{X}\\to\\mathcal{Y}\\) \\[\nx\\mapsto g(y|x)\\,dy\n\\] の形で与えられているとする．\n\n\n\n\n\n\n\n\n\n名称\n正規化定数の不明な分布に使えるか？\nIPF が必要か？\n\n\n\n\nDDPS 2\n\n\n\n\nDSB-PS 3\n\n\n\n\nDDGS 4\n\n\n\n\nDSB-GS 5"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#雑音除去拡散による事後分布サンプリング-ddps",
    "href": "posts/2024/Samplers/Schrodinger.html#雑音除去拡散による事後分布サンプリング-ddps",
    "title": "Schrödinger 橋",
    "section": "2 雑音除去拡散による事後分布サンプリング (DDPS)",
    "text": "2 雑音除去拡散による事後分布サンプリング (DDPS)\n\n2.1 雑音除去拡散 (DD)\n拡散模型 は，次で定まる OU 過程によってデータ分布を \\(\\mathrm{N}_d(0,I_d)\\) にまで破壊しているとみなせる (Y. Song et al., 2021)： \\[\ndX_t=-\\frac{1}{2}X_t\\,dt+dB_t,\\qquad X_0\\sim p(x|y).\n\\]\nただし，この過程は指数エルゴード性を持つと言っても，完全に \\(\\mathrm{N}_d(0,I_d)\\) に従うようになるのは \\(t\\to\\infty\\) の極限においてである．この極限においては，\\(p(x|y)\\) はもやは \\(y\\) に依らなくなる．\nこの \\((X_t)\\) の有限時区間 \\([0,T]\\) における時間反転は，\\((X_t)\\) の密度を \\(p_t(x_t|y)\\) で表すと， \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log p_{T-t}(Z_t|y)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T|y),\n\\tag{1}\\] の弱解になる (Anderson, 1982), (Haussmann and Pardoux, 1986)．この \\((Z_t)_{t\\in[0,T]}\\) を 雑音除去拡散 (Denoising Diffusion) という．\n\n\n2.2 \\((Z_t)\\) からのサンプリング\nすると残りの問題は，拡散過程 \\((Z_t)_{t\\in[0,T]}\\) からのサンプリングになるが，これは \\(\\log p_{T-t}(Z_t|y)\\) という項の評価と \\(p_T(x_T|y)\\) からのサンプリングが必要である．\n\\((Z_t)\\) を \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+s_{T-t}^\\theta(Z_t,y)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] で近似することが (Y. Song et al., 2021) の方法である．思い切って \\(\\mathrm{N}_d(0,I_d)\\approx p_T(x_T|y)\\) としてしまい，\\(s_t^\\theta(x_t,y)\\) のモデリングに特化するのである．\nこの過程 \\((Z_t)\\) が定める測度を \\(\\mathbb{Q}_y^\\theta\\in\\mathcal{P}(C([0,T];\\mathcal{X}))\\) と表すと，訓練目標は KL 乖離度の期待値 \\[\\begin{align*}\n    \\mathcal{L}(\\theta)&:=2\\operatorname{E}\\biggl[\\operatorname{KL}\\biggr(\\mathbb{P}_Y,\\mathbb{Q}_Y^\\theta\\biggl)\\biggr]\\\\\n    &=\\int^T_0\\operatorname{E}\\biggl[\\left\\|s^\\theta_t(X_t,Y)-\\nabla_x\\log p_{t|0}(X_t|X_0)\\right\\|^2\\biggr]\\,dt+\\mathrm{const.}\n\\end{align*}\\] が考えられる．ただし，\\(\\mathbb{P}_Y\\) は \\((X_t)\\) の分布，\\(p_{t|0}\\) は \\((X_t)\\) の遷移密度を表す．この損失は DSM (Vincent, 2011) で与えられたものに等しい．\n\\[\n(X_0,Y)\\sim p(x,y)=g(y|x)\\mu(x)\n\\] からのシミュレーションが可能であるならば，この目的関数は確率的最適化アルゴリズムによって最適化できる．\nこうして，雑音除去拡散サンプラー (DDPS: Denoising Diffusion Posterior Sampler) を得る．\n\n\n2.3 近似ベイズ計算への応用\n事前分布と尤度 \\(g(y|x)\\) からのサンプリングが可能な状況は，生成モデリングの他に Simulation-based Inference などの近似推論でもあり得る．\n実際，この DDPS は従来の ABC (Approximate Bayesian Computation) 法の代替になり得る．\nさらに，拡散模型の加速法 （Progressive Distillation (Salimans and Ho, 2022) など）が DDPS にも応用可能である．\n\n\n2.4 逆問題への応用\nサンプルが画像だとしても，画像修復 (inpainting) や高解像度化 (super-resolution) などの逆問題応用が豊富に存在する．\nこのような，単一の \\(Y=y\\) を固定した状況で潜在変数 \\(X_T\\) からサンプリングをしたい場合では，\\(\\log p_t(x_t|y)\\) を一緒くたに \\(s^\\theta_{t}(x_t,y)\\) に取り替えてしまうのではなく，次の事前分布と尤度への分解に基づいて扱うこともできる：\n\\[\n\\nabla_x\\log p_t(x_t|y)=\\nabla_x\\log\\mu_t(x_t)+\\nabla_x\\log g_t(y|x_t),\n\\] \\[\n\\mu_t(x_t):=\\int_\\mathcal{X}\\mu(x_0)p_{t|0}(x_t|x_0)\\,dx_0,\\qquad g_t(y|x_t):=\\int_\\mathcal{X}g(y|x_0)p_{0|t}(x_0,x_t)\\,dx_0.\n\\]\nこの第一項は \\(s_t^\\theta(x_t)\\) により統一的にモデリングでき，同様に \\(X_0\\sim\\mu(x)\\) から始まる雑音化過程 \\((X_t)\\) の分布を \\(\\mathbb{P}\\) として \\(\\operatorname{KL}(\\mathbb{P},\\mathbb{Q}^\\theta)\\) 最小化問題として処理できる．\n\\(g_t(y|x_t)\\) の項も近似可能である．(Chung et al., 2023) では条件付き誘導が，(J. Song et al., 2023) では Monte Carlo 法が用いられている．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html",
    "href": "posts/2024/Samplers/NF.html",
    "title": "正規化流",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#正則化流-nf-tabak-vanden-eijnden2010",
    "href": "posts/2024/Samplers/NF.html#正則化流-nf-tabak-vanden-eijnden2010",
    "title": "正規化流",
    "section": "1 正則化流 (NF) (Esteban G. Tabak and Vanden-Eijnden, 2010)",
    "text": "1 正則化流 (NF) (Esteban G. Tabak and Vanden-Eijnden, 2010)\n\n1.1 原理\nGAN，VAE，拡散モデル など，深層生成モデルは，潜在空間 \\(\\mathcal{Z}\\) 上の基底分布 \\(p(z)dz\\) を，パラメータ \\(w\\in\\mathcal{W}\\) を持つ深層ニューラルネットによる変換 \\(f_w:\\mathcal{Z}\\times\\mathcal{W}\\to\\mathcal{X}\\) を通じて，押し出し \\[\np_w(x):=(f_w)_*p(x)\n\\] により \\(\\mathcal{X}\\) 上の分布をモデリングする．\nこのモデル \\(\\{p_w\\}_{w\\in\\mathcal{W}}\\) の尤度は解析的に表示できない．そこで，GAN (Goodfellow et al., 2014) は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものであり，VAE (Diederik Kingma and Welling, 2014) は変分下界を通じて尤度を近似するというものであった．\n正則化流 (normalizing flow / flow-based models) では，拡散モデル に似て，「逆変換」を利用することを考える．\nすなわち，\\(\\{f_w\\}\\subset\\mathcal{L}(\\mathcal{Z},\\mathcal{X})\\) が可逆であるように設計するのである．逆関数を \\(g_w:=f_w^{-1}\\) と表すと，\\(p_w(x)dx\\) は \\(p(z)dz\\) の \\(g_w\\) による引き戻しの関係になっているから，変数変換 を通じて， \\[\np_w(x)=p(g_w(x))\\lvert\\det J_{g_w}(x)\\rvert\\;\\mathrm{a.e.}\\;\n\\] が成立する．\nすると， \\[\n\\log p_w(x)=\\log p(g_w(x))+\\log\\lvert\\det J_{g_w}(x)\\rvert\n\\] を通じて，尤度の評価とパラメータの最尤推定が可能である．\n\n\n1.2 実装\n上述のアイデアは，\\(f\\) が表現力が高く数値的に可微分なモデルで表現できれば最も強力な形で実装できるが，これがまさにニューラルネットワークである．\n従って，可逆なニューラルネットワーク \\(\\{f_w\\}\\subset\\mathcal{L}(\\mathcal{Z},\\mathcal{X})\\) を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．1\nこのとき，行列式 \\(\\det:\\mathrm{GL}_n(\\mathbb{R})\\to\\mathbb{R}^\\times\\) は群準同型であるから，\\(g_w\\) のヤコビアンは，各層のヤコビアンの積として得られ，その対数は \\[\n\\log\\left|\\det J_{g_w}(x)\\right|=\\sum_{i=1}^n\\log\\left|\\det J_{g_i}(z_i)\\right|,\\qquad z_i:=f_i\\circ\\cdots\\circ f_1(z),\n\\] と得られる．\nこの条件はたしかにモデルに仮定を置いている（\\(p(z)dz\\) は典型的に正規で，\\(f_w\\) は可逆である）．しかしそれでも，深層ニューラルネットワーク \\(\\{f_w\\}\\) の表現力は十分高いため，多様な密度のモデリングにも使うことが出来る (Papamakarios et al., 2021)．\nまた一方で，変分推論における \\(E\\)-ステップ（変分分布について期待値を取るステップ）などにおいて，複雑な分布からのサンプラーとしても用いられる (Gao et al., 2020)．\n\n\n1.3 応用\n\n1.3.1 ベイズ推定\n変分推論において，変分事後分布のモデリングにフローが使えることを提唱し，フローベースのアプローチを有名にしたのが (Rezende and Mohamed, 2015) であった．\nこのような変分ベイズ推論，特にベイズ深層学習に向けたフローベースモデルが盛んに提案されている：Householder flow (Tomczak and Welling, 2017) は VAE の改良, IAF (D. Kingma et al., 2016), 乗法的正則化流 (Louizos and Welling, 2017), Sylvester flow (Berg et al., 2019) など．\n\n\n1.3.2 ベイズ計算\nNeural Importance Sampling (Müller et al., 2019) とは，困難な分布からの重点サンプリングの際に，提案分布を正則化流で近似する方法である．\nBoltzmann Generator (Noé et al., 2019) は名前の通り，多体系の平衡分布から正則化流でサンプリングをするという手法である．\n(Hoffman et al., 2019) は IAF を用いて目標分布を学習し，学習された密度 \\(q\\) で変換後の分布から MCMC サンプリングをすることで効率がはるかに改善することを報告した．実際，フローによる変換を受けた後は対象分布は正規分布に近くなることから，MCMC サンプリングを減速させる要因の多くが消滅していることが期待される．\n\n\n1.3.3 密度推定\n目標の分布 \\(p_w\\) を Guass 分布 \\(p\\) からの写像 \\((f_w)_*p\\) として捉える発想は，まずなんといっても密度推定に用いられた (S. Chen and Gopinath, 2000)．\nこの \\(f_w\\) のモデリングにニューラルネットワークを用いるという発想は (Rippel and Adams, 2013) の Deep Density Model 以来であるようだ．\nその後同様の発想は非線型独立成分分析 (Dinh et al., 2015)，引き続き密度推定 (Dinh et al., 2017) に用いられた．\n現在は MAF 2.2.1 の性能が圧倒的である．\n\n\n1.3.4 表現学習\n通常の VAE などは，あくまで \\(p(x|z)\\) を学習する形をとるが，正則化流を用いて結合分布 \\(p(x,z)\\) を学習することで，双方を対等にモデリングすることができる．\nこれを flow-based hybrid model (Nalisnick et al., 2019) という．これは予測と生成のタスクでも良い性能を見せるが，分布外検知などの応用も示唆している．\n異常検知 (Zhang et al., 2020)，不確実性定量化 (Charpentier et al., 2020) のような種々の下流タスクに用いられた場合は，VAE など NN を２つ用いる手法よりもモデルが軽量で，順方向での１度の使用で足りるなどの美点があるという．\n\n\n1.3.5 生成\n画像生成への応用が多い：GLOW (D. P. Kingma and Dhariwal, 2018), 残差フロー (R. T. Q. Chen et al., 2019) など．\n動画に対する応用も提案されている：VideoFlow (Kumar et al., 2020)．\n言語に対する応用もある (Tran et al., 2019)．言語は離散データであることが難点であるが，潜在空間上でフローを使うことも提案されている (Ziegler and Rush, 2019)．\n\n\n1.3.6 蒸留\n純粋な生成モデリングの他に，IAF 2.2.2 は (Oord et al., 2018) において音声生成モデルの蒸留に用いられた．\nWaveFLOW (Prenger et al., 2018), FloWaveNet (Kim et al., 2019) などもカップリング層を取り入れて WaveNet の高速化に成功している．\n\n\n1.3.7 SBI\nモデルの尤度は隠されており，入力 \\(\\theta\\) に対して，\\(p(x|\\theta)\\) からのサンプルのみが利用可能であるとする．このような状況でベイズ推論を行う問題を simulation-based inference (SBI) という．これはデータの分布のサンプルから学習して，似たようなデータを増やすという生成モデルのタスクに似ている．\nこの際，任意の分布 \\(p(\\theta)\\) に対して，結合分布 \\[\np(x,\\theta)=p(x|\\theta)p(\\theta)\n\\] を，シミュレータから得られるサンプルのみから，フローベースモデルにより学習してしまうことで，ベイズ推論が可能になる (Papamakarios et al., 2019)．\nこの方法はさらに，事後分布推定に特化することで，SMC-ABC などの従来の近似ベイズ計算技法の性能を超えていくようである (Greenberg et al., 2019)．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#離散流-dnf",
    "href": "posts/2024/Samplers/NF.html#離散流-dnf",
    "title": "正規化流",
    "section": "2 離散流 (DNF)",
    "text": "2 離散流 (DNF)\n\n2.1 カップリング流\nある可微分同相 \\(f_\\theta:\\mathbb{R}^m\\to\\mathbb{R}^m\\) と任意の関数 \\(\\Theta:\\mathbb{R}^{n-m}\\to\\mathbb{R}^{n-m}\\) について， \\[\n(z^{(1)},z^{(2)})\\mapsto (f_{\\Theta(z^{(2)})}(z^{(1)}),z^{(2)})\n\\] で定まる変換を カップリング層 という．\nこの逆変換は \\[\n(x^{(1)},x^{(2)})\\mapsto (f^{-1}_{\\Theta(x^{(2)})}(x^{(1)}),x^{(2)})\n\\] で与えられる．\n加えて，後半の成分 \\((-)^{(2)}\\) には変換を施していないので，カップリング層の Jacobian は \\(f\\) の Jacobian に一致する．\n\\(f_\\theta\\) は各成分が可逆になるように設計することで \\(f_\\theta^{-1}\\) が計算しやすくされることが多い．\n\n\n2.2 自己回帰流\n入力 \\(z\\in\\mathbb{R}^n\\) を形式上時系列と見做し，ある可微分関数 \\(f_\\theta:\\mathbb{R}\\to\\mathbb{R}\\) と任意の関数列 \\(\\Theta_i\\) について， \\[\nx_i=f_{\\Theta_i(x_{1:i-1})}(z_i)\n\\] と再帰的に定義していく変換 \\(z\\mapsto x\\) を 自己回帰流 という．\nこの逆は \\[\nz_i=f_{\\Theta_i(x_{1:i-1})}^{-1}(x_i)\n\\] で与えられる．\n自己回帰流の Jacobi 行列は上三角行列になるので，Jacobian は効率的に計算できる．\n\n2.2.1 マスク付き自己回帰流 (MAF)\n\\(\\Theta_i\\) は典型的に単一の自己回帰型のニューラルネットワークを用いてモデリングする．その際，\\(\\Theta_i\\) が \\(x_{1:i-1}\\) のみに依存するようにマスクをする MAF (Masked Autoregressive Flow) (Papamakarios et al., 2017)．\n(Papamakarios et al., 2017) では \\(f\\) は affine 関数としている．\n\n\n2.2.2 逆自己回帰流 (IAF)\n元から \\(f^{-1}\\) の方をモデリングする方法を IAF (Inverse Autoregressive Flow) (D. Kingma et al., 2016) という．順方向のものが密度評価が速いのに比べ，IAF はサンプリングに使われる．\nParallel WaveNet (Oord et al., 2018) では，WaveNet モデル \\(p_t\\) からのサンプリングを加速させる方法として IAF \\(p_s\\) を用いた．\n\\(p_t\\) の密度評価は高速であったため，\\(\\operatorname{KL}(p_s,p_t)\\) の値は，\\(p_t\\) の評価と IAF \\(p_s\\) からのサンプリングによって効率的に計算できる．\nこの KL 乖離度を最適化することで \\(p_t\\) のモデルを \\(p_s\\) に移すことで，サンプルの質を保ちながらサンプリングを加速することに成功した．\n\n\n\n2.3 残差フロー\n残差接続 \\[\nu\\mapsto u+F(u)\n\\] において，残差ブロック \\(F\\) が縮小写像になるようにすれば，全体として可逆な層となる．\niResNet (Invertible Residual Networks) (Behrmann et al., 2019) では，これを CNN において実装した．\n\n2.3.1 Jacobian を推定する方法\nこれは極めて一般的な層を定めてしまい，逆変換が不明であるが，Jacobian は \\[\n\\log\\lvert\\det(I+J_F)\\rvert=\\sum_{k=1}^\\infty\\frac{(-1)^{k+1}}{k}\\operatorname{Tr}(J_F^k)\n\\] を通じて，\\(\\operatorname{Tr}(J_F^k)\\) が Hutchinson の跡推定量により，その無限和が Russian-roulette 推定量 (Hutchinson, 1990), (Skilling, 1989) により不偏推定できる (R. T. Q. Chen et al., 2019)．\n\n\n2.3.2 Jacobian が計算可能な場合\n\\(J_F\\) のランクが低い場合は，\\(I+J_F\\) の形の行列の Jacobian は効率的に計算できる．\nPlanar flow (Rezende and Mohamed, 2015) では隠れ素子数１の残差接続層を用いたものである．\nSylvester flow (Berg et al., 2019) も．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#sec-CNF",
    "href": "posts/2024/Samplers/NF.html#sec-CNF",
    "title": "正規化流",
    "section": "3 連続流 (CNF)",
    "text": "3 連続流 (CNF)\n\n3.1 フローマッチング (Lipman et al., 2023)\nフローマッチング (Lipman et al., 2023), rectified flow (Liu et al., 2023)"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#確率的補間",
    "href": "posts/2024/Samplers/NF.html#確率的補間",
    "title": "正規化流",
    "section": "4 確率的補間",
    "text": "4 確率的補間\n(Michael Samuel Albergo and Vanden-Eijnden, 2023) により提案されたもので，SiT (Scalable Interpolant Transformer) (Ma et al., 2024) でも用いられている技術である．\n(Michael S. Albergo et al., 2023)"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#文献",
    "href": "posts/2024/Samplers/NF.html#文献",
    "title": "正規化流",
    "section": "5 文献",
    "text": "5 文献\n\nJanosh Riebesell のリポジトリ awesome-normalizing-flows に実装がまとめられている．\n(Murphy, 2023) 第23章は入門に良い．詳細はサーベイ (Kobyzev et al., 2021), (Papamakarios et al., 2021) に譲られている．\n(S. Chen and Gopinath, 2000) の時点では同様のアイデアは Gaussianization と呼ばれていた．\n(Esteban G. Tabak and Vanden-Eijnden, 2010), (E. G. Tabak and Turner, 2013) が正則化流の提案と命名を行った．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#footnotes",
    "href": "posts/2024/Samplers/NF.html#footnotes",
    "title": "正規化流",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの考え方は，VAE ではデコーダーをエンコーダーを用いていたものを，１つの可逆な NN で済ましているようにみなせる．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html",
    "href": "posts/2024/Samplers/EBM.html",
    "title": "エネルギーベースモデル",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#エネルギーベースモデル",
    "href": "posts/2024/Samplers/EBM.html#エネルギーベースモデル",
    "title": "エネルギーベースモデル",
    "section": "1 エネルギーベースモデル",
    "text": "1 エネルギーベースモデル\n\n1.1 導入\nエネルギーベースのモデル (EBM: Energy-based Model) とは，密度が \\[\np(x,z)\\,\\propto\\,e^{-H(x,z)}\n\\] の形で与えられるモデルをいう．1\nこれは統計物理学的な系との対応が意識されているように，入力 \\(z\\) と出力 \\(x\\) の整合性 \\(k(x,z)\\) をエネルギー \\(H(x,z)\\) の言葉で与えているモデルであると見ることができ，このエネルギー \\(H\\) をニューラルネットワークによってモデル化するのである．\nすると EBM の最尤推定とは，訓練データ \\(\\{x_i\\}_{i=1}^n\\) に対して最も低いエネルギーを割り当てるエネルギー関数 \\(H_\\theta\\) を探すという基底状態探索の問題に対応する (LeCun et al., 2007)．\nこれで EBM の概要は終わりであるが，このような極めて一般的な設定で有用な一般論が得られることは驚くべきことである．\n\n\n\n1.2 例\n\n1.2.1 有向グラフを EBM とみなす\nVAE や 正則化流 などの生成モデルや，トランスフォーマー などの自己回帰モデルも EBM とみなせる．\n特にこれらの生成モデルを事後調整しようと思うと，自然に EBM の構造が出てくる．例えば，LLM における RLHF などの事後調整は，生成モデル \\(p(x|z)\\) とある目的関数 \\(\\phi\\) に関して \\[\np'(x|z):=\\frac{1}{Z(z)}p(x|z)\\phi(z)\n\\] によって分布を調整する営みであると見ると，やはり正規化定数を除いた分布族が定まる (Zhao et al., 2024)．2\n\n\n1.2.2 無向グラフ\nMarkov 確率場などの無向グラフィカルモデルで定義される分布族や，正規化定数を除いて定義された確率分布族は，自然に EBM とみなせる．3\n(Hinton and Teh, 2001) で論じられている通り，音声や画像などのデータに対する独立成分分析では，DAG などの有向グラフによるモデリングでは正確な推論ができないため，無向グラフによるより良いモデリング法が 積スパートモデル (PoE: Product of Experts) などを通じて探求されていた．\n\n\n1.2.3 Boltzmann 機械\n例えば Ising 模型の一種でもある Hopfield ネットワーク (Hopfield, 1982) と Boltzmann 機械 (Ackley et al., 1985) は最も有名な EBM の１つである．4\nしかしこの場合，分配関数 \\(Z_\\theta\\) の計算が難しく（ほとんどの統計力学模型の中心問題である），EBM を訓練することが難しかった．\nこのようなモデルに対する最初の実用的な訓練手法（というより SGD の目的関数）は，Boltzmann 機械に対して与えられた Contrastive Divergence (Hinton, 2002) であった．\nこれ以降正規化定数が不明である模型も効率的に訓練可能であることが周知され，ICA に応用した論文 (Teh et al., 2003) で “Energy-based Model” の名前が造語された．\n\n\n1.2.4 深層化\n制約付き Boltzmann 機械に対する greedy, layer-by-layer の事前学習を取り入れることで，深層化しても訓練が効率的に行われるようになった (Hinton et al., 2006), (Hinton and Salakhutdinov, 2006)．\nしかしこの方法により訓練されたモデルは深層信念機械というべきものであり，一般的な Boltmzmann 機械とは違った．深層 Boltzmann 機械の訓練法は (Salakhutdinov and Hinton, 2009) で確立された．\n画像生成の分野においても，生成 CNN (generative ConvNet) (Xie et al., 2016) 以降，\\(H_\\theta\\) のモデリングには深層の CNN が用いられるようになっていった．\n\n\n\n1.3 分配関数\nエネルギーベースモデルでは，分布は正規化定数を除いて定まっており， \\[\nZ_\\theta:=\\int e^{-H(x,\\theta)}dx\n\\] も既知ではないとする．この点が EBM の表現力を支える自由度の高さであるが，5 EBM の訓練にあたっては \\(Z_\\theta\\) の推定というタスクが増える．\nこれが 自己回帰モデル や フローベースモデル との最大の違いである．\nまた，GAN や VAE も含めた生成モデルでは，生成のタスクが念頭にあるためにモデルを有向グラフによって表現するが，EBM はより一般の状況も考える広いクラスだと言える．\n\\(Z_\\theta\\) の推定という追加のタスクには，典型的には MCMC が用いられ，最尤推定 2 が完遂させられるが，近年スコアマッチング（第 3 節）や ノイズ対照学習 などの新たな手法が提案されている．\n\n\n1.4 エネルギー\n仮に，２つの学習済みモデル \\(p_1(x),p_2(x)\\) が EBM の形で得られており，それぞれのエネルギーが \\(H_1,H_2\\) で与えられるとする．この際，\\(H:=H_1+H_2\\) をエネルギーにもつ EBM は \\(p\\,\\propto\\,p_1p_2\\) であり，このモデルは \\(p_1\\) でも \\(p_2\\) でも高確率であるような \\(x\\) を高く評価することになる．\nこのようにして得るモデル \\(p\\) を 積スパートモデル (PoE: Product of Experts) (Hinton, 2002) という．6 (Hinton, 2002) は引き続き，対照分離度 (contrast divergence) 2.3 による訓練法を提案している．\n例えば (Murphy, 2023, p. 840) では，タンパク質構造の生成モデルを作りたいとして，\\(p_1\\) を「常温で安定であるタンパク質」の生成モデル，\\(p_2\\) を「COVID-19 のスパイクタンパク質に結合するタンパク質」の生成モデルとして説明している．\n従ってこの \\(H\\) は，データの好ましさを表すパラメータと考えられ，他モデルへの移転にも使えることが期待される．\\(H\\) は，contrast function, value function, 負の対数尤度などとも呼ぶ (LeCun et al., 2007, p. 193)．\n\n\n1.5 応用\n２次元の密度推定に対して，FCE (Flow Contrastive Estimation) により学習させた EBM は，正則化流 よりも遥かに小さいネットワークサイズで高い性能を示した (Gao et al., 2020)．\n\n\n1.6 推定手法\nContrastive Divergence 2.3 は漸近的に消えないバイアスを持つ (Carreira-Perpiñán and Hinton, 2005) が，SM 3 と NCE 4 は一致推定量である．\n\n1.6.1 スコアマッチング\nスコアマッチング（第 3 節）のアイデアは，データ分布の密度 \\(p(x)\\) とモデル \\(p_\\theta(x)\\) とのスコア関数をマッチングさせることを狙う： \\[\n\\nabla_x\\log p(x)=\\nabla_x\\log p_\\theta(x).\n\\] このとき，両辺を積分すると，正規化条件から \\(p(x)=p_\\theta(x)\\) が従う．\nこの考え方は EBM だけでなく，尤度の評価は困難でも対数尤度の評価は可能である文脈で普遍的に有効である．\n例えば 自己符号化器 においても用いられるアイデアである (Vincent, 2011), (Swersky et al., 2011)．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#sec-ML",
    "href": "posts/2024/Samplers/EBM.html#sec-ML",
    "title": "エネルギーベースモデル",
    "section": "2 最尤推定",
    "text": "2 最尤推定\n\n2.1 導入\nEBM は，データの分布との KL 乖離度，または等価なことだが対数尤度の期待値 \\[\n\\operatorname{E}[\\log p_\\theta]\n\\] を最小化することによって学習することが考えられる (LeCun et al., 2007)．\nしかし尤度の評価は正規化定数 \\(Z_\\theta:=\\int e^{-H_\\theta(x)}\\,dx\\) の評価が必要であるため，一般の設定では実行できないが，勾配 \\[\n\\nabla_\\theta\\log p_\\theta(x)=-\\nabla_\\theta H_\\theta(x)-\\nabla_\\theta\\log Z_\\theta\n\\tag{1}\\] は近似できる．\\(\\nabla_\\theta H_\\theta(x)\\) はニューラルネットワークの自動微分で計算することができ，２項目は \\[\n\\nabla_\\theta\\log Z_\\theta=-(p_\\theta|\\nabla_\\theta H_\\theta)\n\\] の関係を用いて，\\(p_\\theta\\) からのサンプルを用いた Monte Carlo 推定量で評価できる (Younes, 1999)．\n正規化定数の不明な密度 \\(p_\\theta\\) からのサンプリングといえば，MCMC と SMC である．この Monte Carlo 近似を通じて，確率的勾配降下法によって最尤推定が実行できる．\n\n\n2.2 確率勾配 Langevin 動力学 (SGLD) (Welling and Teh, 2011)\nこの際の Monte Carlo 推定には，多少バイアスがあっても高速に収束してくれる MCMC が欲しい．そこで提案されたのが 確率勾配 Langevin 動力学 である．\nこれは (Hyvärinen, 2005) のスコア関数7 \\[\n\\nabla_x\\log p_\\theta(x)=-\\nabla_x H_\\theta(x)\n\\tag{2}\\] の値から情報を得て，\\(x\\) の空間上で効率的な Markov 連鎖のダイナミクスを構成する方法である．\n\\(H_\\theta\\) の勾配が急であればあるほど高速に収束するが，Zig-Zag サンプラー などの PDMP 手法の方が高速に収束する．\n\n\n2.3 対照分離度 (CD)\nSGD の各ステップに Monte Carlo 推定が必要であるが，毎度 MCMC を十分な数だけ回して，十分に分散が小さい勾配の推定量を得る必要はない．\nこのように，系統的に MCMC を打ち切って，手早く計算された勾配の推定量を通じて SGD により最尤推定を行う方法 (short-run MCMC) の代表的なものに，対照分離度 (CD: Contrastive Divergence) (Hinton, 2002) がある．\nCD による訓練では，バッチごとに提供された訓練データ \\(x_n\\) を開始地点として，一定の回数 \\(T\\) だけ MCMC を回す．多くの場合 \\(T=1\\) でさえある (CD-1 algorithm)．\n\n2.3.1 RBM での CD の例\n(Hinton, 2002) は Gibbs サンプリングが可能な潜在変数を持つ EBM モデルである制限付き Boltzmann マシン (RBM) について CD を提案した．\n特に簡単な binary RBM は，次のエネルギーが定める Markov 確率場である： \\[\nH_w(x,z)=\\sum_{d\\in[D],k\\in[K]}x_dz_kW_{dk}+\\sum_{d=1}^Dx_db_d+\\sum_{k=1}^KZ_kx_k.\n\\] 観測 \\(x_d\\) を入力して学習させたとき，\\(z_k\\) も似たような値だった場合，それが強化されるように \\(W_{dk}\\) が更新されるというように，Hebb 則に則った学習が行われる．\nこのとき， \\[\n\\frac{\\partial }{\\partial w_{dk}} H_w(x,z)=z_dz_k\n\\] より，対数尤度の勾配 (1) の期待値は \\[\n\\operatorname{E}[\\nabla_w\\log p_w(x)]=-\\operatorname{E}[xz^\\top]-(p_\\theta(x)|xz^\\top).\n\\]\n第一項はデータ \\(x_n\\) に対して \\(x_n\\operatorname{E}[z|x_n,W]^\\top\\) によって，第二項は \\(x_n\\) を初期値として \\(T\\) 回 MCMC を回して得られたサンプル \\(x_n'\\) を用いて \\(x_n'\\operatorname{E}[z|x_n',W]^\\top\\) によって推定される．\nこの手続きは，\\(p_0\\) をデータ \\(\\{x_i\\}_{i=1}^n\\) の分布として， \\[\\newcommand{\\CD}{\\operatorname{CD}}\n\\CD_T:=\\operatorname{KL}(p_0,p_\\infty)-\\operatorname{KL}(p_T,p_\\infty)\n\\] を最小化することに相当している．\n\n\n\n2.3.2 PCD：効率的な不偏推定を目指して\nデータ点を取り替えるごとに \\(p_0\\) を取り替えるのではなく，\\(p_0\\) を以前の MCMC の終わり値から定めた場合の CD の変形を PCD (Persistent Contrastive Divergence) (Tieleman, 2008), (Tieleman and Hinton, 2009) という．\n確かにバッチごとに \\(\\theta\\) がアップデートされるため，MCMC の目標分布 \\(p_\\theta\\) は取り替える必要があるから，CD-\\(T\\) のように毎回新たな MCMC を回す必要があるように思えるかもしれない．しかし，\\(\\theta\\) の更新は総じて大変小さなものであるとすると，真のモデル分布 \\(p_\\theta\\) からはずれていくかもしれないが，１つの収束した MCMC からサンプリングし続けた方が良い可能性がある．\n更に，完全に同じ MCMC を走らせ続けるというところから，リサンプリングを取り入れて \\(\\theta\\) のアップデートに収束性を保ちながら対応することで，GAN に匹敵する性能と，分布の峰を正確に再現できるという GAN にはない美点を獲得できるという (Du and Mordatch, 2019)．\n\n\n2.3.3 MCMC による生成\n一方で，正しい \\(p_\\theta\\) によく収束する short-run MCMC が CD 法により訓練できたならば，これは効率的な生成モデルとなるかもしれない．\n(Nijkamp et al., 2019) は，MCMC が EBM モデルに対置されている analysis by synthesis スキーム (Grenander and Miller, 1994) と見て，この short-run MCMC をよく学ぶことに特化したアプローチ Short-Run MCMC as Generator or Flow Model を提案した．\nこのアプローチでは，MCMC は毎回同じ初期分布（ノイズの分布）からスタートさせ，\\(T\\) の値も固定する．このようなスキームで学習された EBM は全く良い性能を持たないが，EBM から返ってくる Hyvärinen スコアを持った勾配 MCMC 法は，生成モデルとして良い性能を持つという (Nijkamp et al., 2019)．\n\n\n2.3.4 安定した CD 訓練に向けて\n(Du et al., 2021) は，この MCMC に情報を与える Hyvärinen スコアの変化が CD 訓練の重要な要素であり，これを正確に扱うことがを安定化させることを報告した．\n特にこれは，従来 CD フレームワークの目的関数が捨象していた「第三項」 \\[\n\\frac{\\partial q_\\theta}{\\partial \\theta}\\frac{\\partial \\operatorname{KL}(q_\\theta,p_\\theta)}{\\partial q_\\theta}\n\\] を目的関数に含めることとして捉えられる (Du et al., 2021)．ただし，\\(q_\\theta\\) は真のデータ分布を初期分布として \\(T\\) ステップ MCMC を実行して得られる分布とした．\n\n\n2.3.5 adversarial CD\n尤度 \\(p_\\theta\\) の評価を迂回するため，GAN にヒントを得た (Finn et al., 2016)，２つのネットワークを対置させて行う敵対的な学習も考えられている (Kim and Bengio, 2016)．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#sec-SM",
    "href": "posts/2024/Samplers/EBM.html#sec-SM",
    "title": "エネルギーベースモデル",
    "section": "3 スコアマッチング (Hyvärinen, 2005)",
    "text": "3 スコアマッチング (Hyvärinen, 2005)\n\n3.1 導入\nEBM のスコア関数 (2) を \\[\ns_\\theta(x):=\\nabla_x\\log p_\\theta(x)=-\\nabla_xH_\\theta(x)\\tag{2}\n\\] と定め，データ分布 \\(p\\) との Fisher 乖離度を最小化するスキームが (Hyvärinen, 2005) の スコアマッチング 目的関数である： \\[\nD_F(p,p_\\theta)=\\frac{\\|s-s_\\theta\\|_{L^2(p)}^2}{2}=\\operatorname{E}\\left[\\frac{1}{2}\\biggl|\\nabla_x\\log p(X)-\\nabla_x\\log p_\\theta(X)\\biggr|^2\\right].\n\\tag{3}\\]\nこの際，データ分布のスコア \\(s(x)=\\nabla_x\\log p(x)\\) の計算を回避することが焦点になる．\n\n\n3.2 部分積分 (Hyvärinen, 2005)\n次が成り立つことがあり得る： \\[\\begin{align*}\n    D_F(p,p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\lvert s_\\theta(X)\\rvert^2+\\operatorname{div}(s_\\theta(X))\\right]+(\\theta\\;\\text{に依らない定数})\\\\\n    &=\\operatorname{E}\\left[\\frac{1}{2}\\lvert\\nabla H_\\theta(X)\\rvert^2-\\mathop{}\\!\\mathbin\\bigtriangleup H_\\theta(X)\\right]+(\\theta\\;\\text{に依らない定数})\n\\end{align*}\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    D_F(p,p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\biggl|\\nabla_x\\log p(x)-\\nabla_x\\log p_\\theta(x)\\biggr|^2\\right]\\\\\n    &=\\frac{1}{2}\\int_{\\mathbb{R}^d}\\sum_{i=1}^d\\left(\\frac{1}{p(x)}\\frac{\\partial p}{\\partial x_i}(x)-s_\\theta(x)_i\\right)^2p(x)\\,dx\\\\\n    &=\\frac{1}{2}\\int_{\\mathbb{R}^d}\\frac{\\lvert Dp\\rvert^2}{p}\\,dx-\\sum_{i=1}^d\\int_{\\mathbb{R}^d}s_\\theta(x)_i\\frac{\\partial p}{\\partial x_i}(x)+\\frac{1}{2}\\int_{\\mathbb{R}^d}\\lvert s_\\theta(x)\\rvert^2p(x)\\,dx\\\\\n    &=\\mathrm{const.}+\\int_{\\mathbb{R}^d}\\operatorname{Tr}(Ds_\\theta)p\\,dx+\\operatorname{E}\\left[\\frac{1}{2}\\lvert s_\\theta(x)\\rvert^2\\right]\\\\\n    &=\\operatorname{E}\\left[\\frac{1}{2}\\lvert s_\\theta(x)\\rvert^2+\\operatorname{div}(s_\\theta(x))\\right]+\\mathrm{const.}\n\\end{align*}\\]\n\n\n\nこの右辺はデータのスコアを含まないので，\\(p_\\theta\\) の２階微分が計算可能ならば計算できるが，データの次元 \\(d\\) に関するスケールは悪い．\n加えて，\\(D_F(p,p_\\theta)\\) をここから \\(\\theta\\) に関して微分することが困難である．独立成分分析モデルを除いて，(Köster and Hyvärinen, 2007), (Köster et al., 2009) などのニューラルネットワークモデルにも応用されたが，画像などの実データに直接適用することは困難であった．特に，\\(H_\\theta\\) の勾配と Laplacian を解析的に計算してから実装していた．\n\n\n3.3 正則化スコアマッチング (RSM) (Kingma and LeCun, 2010)\n画像データなどの量子化されたデータの密度 \\(p(x)\\) は可微分ではない上に，有限な台を持ってしまうため \\(s\\) は well-defined ではない．\nこのような量子化されたデータ \\(x\\) に対して，Gauss ノイズを加えたもの \\[\n\\widetilde{x}=x+\\epsilon,\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2I)\n\\] は連続なデータに変貌する（Gauss 核が軟化子として働く）．\nこの \\(\\epsilon\\) だけ摂動されたデータの分布 \\(\\widetilde{p}\\) に対して，スコアマッチング目的関数は \\[\\begin{align*}\n    D_F(\\widetilde{p},p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\lvert\\nabla H_\\theta\\rvert^2-\\mathop{}\\!\\mathbin\\bigtriangleup H_\\theta+\\frac{\\sigma^2}{2}\\|D^2H_\\theta\\|^2_2\\right]+O(\\epsilon^2)\\\\\n    &=D_F(p,p_\\theta)+\\frac{\\sigma^2}{2}\\operatorname{E}\\biggl[\\|D^2H_\\theta\\|_2^2\\biggr]+O(\\epsilon^2)\n\\end{align*}\\] と表せる (Kingma and LeCun, 2010)．\nこの目的関数は，Hessian \\(D^2H\\) の非対角項を対角項で近似し，結局は元の目的関数に対して正則化項 \\[\n\\lambda\\mathop{}\\!\\mathbin\\bigtriangleup H_\\theta,\\qquad\\lambda\\approx\\frac{\\sigma^2}{2}\n\\] を加えたものとして訓練をする．\nさらに (Kingma and LeCun, 2010) は，従来のように，解析的に微分が計算可能な \\(H_\\theta\\) を採用するのではなく，誤差逆伝播を２回行う Double Backpropagation (Drucker and Le Cun, 1992) によってこの目的関数の勾配 \\(\\frac{d D_F(p,p_\\theta)}{d \\theta}\\) を自動微分で計算する方法を提案した．\n\n\n3.4 Denoising スコアマッチング (DSM) (Vincent, 2011)\nRSM の場合と違い，次のような展開もできる： \\[\nD_F(\\widetilde{p},p_\\theta)=\\frac{1}{2}\\operatorname{E}\\left[\\left\\|s_\\theta(\\widetilde{X})-\\frac{X-\\widetilde{X}}{\\sigma^2}\\right\\|^2_2\\right].\n\\]\nすなわち，ノイズ消去方向のベクトル \\(\\frac{x-\\widetilde{x}}{\\sigma^2}\\) に一致するようにモデルのスコア \\(s_\\theta\\) を学習することが考えられる．\nこうすることで，\\(D^2H_\\theta\\) の計算を回避することができる．\nただし，RSM と DSM に共通することであるが，あくまで \\(D_F(\\widetilde{p},p_\\theta)\\) はノイズの入ったデータ分布を学習してしまうのであり，\\(\\sigma\\to0\\) が志向されるが，\\(\\sigma\\) が小さいほど \\(D_F(\\widetilde{p},p_\\theta)\\) に関する推定は不安定になる．8\n\n\n3.5 スライススコアマッチング (SSM) (Song et al., 2019)\nSSM (Sliced Score Matching) (Song et al., 2019) ではスコアマッチング目的関数 (3) 自体を，sliced Fisher 乖離度 \\[\\begin{align*}\n    D_{SF}(p,p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\biggr(V^\\top s(X)-V^\\top s_\\theta(X)\\biggl)^2\\right]\\\\\n    &=\\operatorname{E}\\left[\\frac{1}{2}\\sum_{i=1}^d\\left(\\frac{\\partial H_\\theta(X)}{\\partial x_i}V_i\\right)^2+\\sum_{i,j=1}^d\\frac{\\partial ^2H_\\theta(X)}{\\partial x_i\\partial x_j}V_iV_j\\right]+\\mathrm{const.}\n\\end{align*}\\] を最小化する．ただし，\\(V\\) はランダムな \\(\\mathbb{R}^d\\) 上のベクトルである．(Song et al., 2019) では \\(V\\sim\\mathrm{N}_d(0,I_d)\\) とすることで Monte Carlo 近似による追加の誤差を回避している．\nやはり \\(D^2H_\\theta\\) が登場しているように思えるが， \\[\n\\sum_{i,j=1}^d\\frac{\\partial ^2H_\\theta}{\\partial x_i\\partial x_j}V_iV_j=\\sum_{i=1}^d\\frac{\\partial }{\\partial x_i}V_i\\sum_{j=1}^d\\frac{\\partial H_\\theta}{\\partial x_j}V_j=\\sum_{i=1}^d\\frac{\\partial }{\\partial x_i}V_i(DH_\\theta|V)\n\\] の表示により，一度内積 \\((DH_\\theta|V)\\) を計算してしまえば，以降，この項は \\(O(d)\\) のオーダーで計算できる．\nなお，\\(V\\sim\\mathrm{N}_d(0,I_d)\\) と取った際の目的関数 \\(D_{SF}\\) は，元の Fisher 乖離度による目的関数に対して，Hutchinson の跡推定量（正規化流でも出てきた）により Jacobian \\(Ds_\\theta\\) を推定したものとも同一視できる．\n\n\n3.6 SM 目的関数の理論\nスコアマッチングの目的関数 \\(D_F(p,p_\\theta)\\) は，Contrastive Divergence のある極限に一致する (Hyvarinen, 2007)．\nステップサイズ \\(\\epsilon&gt;0\\) の Langevin Monte Carlo 法により \\(p_\\theta\\) からサンプリングした場合の対数尤度の期待値は \\[\n\\operatorname{E}[\\nabla_\\theta\\log p_\\theta]=\\frac{\\epsilon^2}{2}\\nabla_\\theta D_F(p,p_\\theta)+o(\\epsilon^2)\n\\] と Monte Carlo 近似されるという (Hyvarinen, 2007)．\nここで de Bruijin の関係 \\[\n\\frac{d }{d t}\\operatorname{KL}(\\widetilde{p}_t,\\widetilde{p}_{\\theta,t})=-\\frac{1}{2}D_F(\\widetilde{p}_t,\\widetilde{p}_{\\theta,t})\n\\] に似た消息が生じている (Lyu, 2009)．なお，\\(\\widetilde{p}\\) とは，\\(p,p_\\theta\\) の分散 \\(t^2\\) を持った Gaussian i.i.d. ノイズによる摂動とする．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#sec-NCE",
    "href": "posts/2024/Samplers/EBM.html#sec-NCE",
    "title": "エネルギーベースモデル",
    "section": "4 ノイズ対照学習 (NCE) (M. Gutmann and Hyvärinen, 2010)",
    "text": "4 ノイズ対照学習 (NCE) (M. Gutmann and Hyvärinen, 2010)\n\n4.1 導入\n既知の密度 \\(p_n\\) と対置させ，これをノイズとしてデータ分布 \\(p\\) との識別を繰り返すことで学習を行う．\n\\(Y\\sim\\mathrm{Ber}\\left(\\frac{\\nu}{1+\\nu}\\right)\\) に関する混合を \\[\n\\widetilde{X}:=YX+(1-Y)N,\\qquad N\\sim p_n,X\\sim p,\n\\] と定め，これに対して混合分布族 \\[\np_{n,\\theta}:=\\frac{1}{1+\\nu}p_n+\\frac{\\nu}{1+\\nu}p_\\theta\n\\] を KL-乖離度の意味でマッチさせることを目指す．\n\n\n4.2 ノイズ \\(p_n\\) の選び方\n重点サンプリング法の提案分布のように，\\(p_n\\) は真のデータ分布 \\(p\\) に近ければ近いほどよい (M. U. Gutmann and Hirayama, 2011)．\n従って多くの方法では，\\(p_n\\) を適応的に選ぶことが考えられる．\n\n\n4.3 スコアマッチングによる解釈\n\\(p_n\\) を \\(p\\) の摂動 \\[\np_n(x):=p(x-v)\n\\] とした場合，\\(\\|v\\|_2\\to0\\) の極限において，\\(V\\) に関するスライススコアマッチングの目的関数 3.5 に一致する (M. U. Gutmann and Hirayama, 2011), (Song et al., 2019)．\n\n\n4.4 FCE (Flow Contrastive Estimation) (Gao et al., 2020)"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#その他の訓練方法",
    "href": "posts/2024/Samplers/EBM.html#その他の訓練方法",
    "title": "エネルギーベースモデル",
    "section": "5 その他の訓練方法",
    "text": "5 その他の訓練方法\n\n5.1 Stein 乖離度\nStein 乖離度は，Fisher 乖離度の１つの \\(s_\\theta\\) を動かして得るダイバージェンスである： \\[\nD_S(p,p_\\theta):=\\sup_{f\\in\\mathcal{F}}\\operatorname{E}\\left[s_\\theta(X)^\\top f(X)+\\operatorname{div}(f(X))\\right].\n\\]\nFisher 乖離度の難点は発散項の計算が \\(O(d^2)\\) の計算量を持ってしまうことであった．Stein 乖離度はこれをカーネル法の方法で迂回することができる．\n\\(\\mathcal{F}\\) がある RKHS の単位閉球であった場合，発散項は定数になる (Chwialkowski et al., 2016), (Q. Liu et al., 2016)．\n\n\n5.2 敵対的な訓練\n(Murphy, 2023) 24.5.3 も参照．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#文献",
    "href": "posts/2024/Samplers/EBM.html#文献",
    "title": "エネルギーベースモデル",
    "section": "6 文献",
    "text": "6 文献\n\nEnergy-based model は独立成分分析の研究 (Teh et al., 2003) において命名され，スコアマッチングの (Hyvärinen, 2005) も非線型独立成分分析の大家である．\nAwesome-EBM レポジトリは，種々の EBM のリストを与えている．\n系統的なイントロダクションには (Bishop and Bishop, 2024) 14.3節，(Murphy, 2023) 24章が良い．\n訓練方法について (Song and Kingma, 2021) が詳細なサーベイを与えている．上の２つの教科書の記述も多くはこのサーベイに基づいている．\n(Gao et al., 2020)"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#footnotes",
    "href": "posts/2024/Samplers/EBM.html#footnotes",
    "title": "エネルギーベースモデル",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnon-normalized probabilistic model ともいう (Song and Kingma, 2021)．この形の分布族を 正準分布 または Gibbs 分布 (Koller and Friedman, 2009, p. 108), (Friedli and Velenik, 2017, p. 25)，または Boltzmann 分布 (Kim and Bengio, 2016), (Mézard and Montanari, 2009, p. 23), (Chewi, 2024) ともいう．物理の用語では \\(e^{H(z)}\\) を Boltzmann 因子と呼ぶのみであるようである (田崎晴明, 2008, p. 107)．(J. S. Liu, 2004, p. 7) ではどちらも掲載している．正準集団は，NVT 一定集団ともいう．↩︎\n今回の表示は特に SMC と相性が良く，(Zhao et al., 2024) では \\(p'(x|z)\\) を段階的に近似する列を通じた twisted SMC という手法を提案している．↩︎\nだが，正規化定数 \\(Z\\) が評価できないという前提で EBM の理論は進むため，有向グラフで定義される VAE などのモデルを EBM とみる積極的な理由はない．↩︎\nなお，2つの Hamiltonian \\(H\\) の形は同じで温度が違うのみである．加えて，用途も違う：Hopfield ネットワークは連想記憶のモデル，Boltzmann 機械はデータ分布の（生成）モデリングに用いられる (Carbone, 2024, p. 4)．↩︎\n例えば \\(H\\) のモデリングには GNN，CNN などが自由に使える．↩︎\nMoE (Mixture of Expert) と並んで使う用語である．↩︎\n通常，スコア関数といったとき，微分はパラメータ \\(\\theta\\) について取ることに注意．↩︎\n\\(D_F(\\widetilde{p},p_\\theta)\\) を近似する Monte Carlo 推定量の分散が大きくなる．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#schrödinger-橋による事後分布サンプリング-dsb-ps",
    "href": "posts/2024/Samplers/Schrodinger.html#schrödinger-橋による事後分布サンプリング-dsb-ps",
    "title": "Schrödinger 橋",
    "section": "3 Schrödinger 橋による事後分布サンプリング (DSB-PS)",
    "text": "3 Schrödinger 橋による事後分布サンプリング (DSB-PS)\n\n3.1 導入\n\\(p_T(x_T|y)\\approx\\mathrm{N}_d(0,I_d)\\) の近似を成り立たせるために \\(T\\) を十分大きく取る必要がある問題は，OU 過程の代わりに Schrödinger 橋を用いることで解決できることが (Shi et al., 2022) で提案された．\nSchrödinger 橋自体は，(De Bortoli et al., 2021) などから拡散模型への応用は議論されていた．\n\n\n3.2 定義\nSchrödinger 橋 (SB) とは， \\[\n\\Pi^*:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}_0}\\operatorname{KL}(\\Pi,\\mathbb{P}),\n\\] \\[\n\\mathcal{P}_0:=\\biggl\\{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))\\,\\bigg|\\,\\Pi_0(x_0,y_0)=p(x_0,y_0),\\Pi_T(x_T,y_T)=\\mathrm{N}_d(0,I_d)p(y_T)\\biggr\\},\n\\] によって定まる確率分布に従う確率過程をいう．ただし，\\(\\mathbb{P}:=\\mathbb{P}_{y_0}\\otimes\\delta_{p(y)}\\) とした．\\(\\delta_{p(y)}\\) は次で定まる確率分布である： \\[\ndY_t=0,\\qquad Y_0\\sim p(y).\n\\]\nこれは表示 \\[\n\\Pi^*=\\mathbb{P}^*_{y_0}\\otimes\\delta_{p(y)}\n\\] を持つから，\\(Z_0\\sim\\mathrm{N}_d(0,I_d)\\) に従う過程 \\((Z_t)\\) をシミュレーションすることで， \\[\nZ_T\\sim\\Pi^*_0(x|y)=p(x_0|y)\\qquad p(y)\\text{-a.s.}\n\\] が成り立つ．\n\n\n3.3 SB のシミュレーション\nSB 問題の解 \\(\\Pi\\) は 逐次的比例フィッティング (IPF: Iterative Proportional Fitting) により得られる．\n\n3.3.1 IPF とは\nIPF アルゴリズムは離散的な形で (Fortet, 1940) が Schrödinger 方程式の研究で，(Deming and Stephan, 1940) が分割表データ解析の研究で提案している．その手続きw (Ireland and Kullback, 1968) が距離の最小化として特徴付け，(Kullback, 1968) が確率密度に対しても一般化した．\nIPF は元々，指定した２つの確率ベクトル \\(r\\in(0,\\infty)^{d_r},c\\in(0,\\infty)^{d_c}\\) を周辺分布に持つ結合分布（カップリング）のうち，指定の行列 \\(W\\in M_{d_rd_c}(\\mathbb{R}_+)\\) に最も近い KL 乖離度を持つカップリングを見つけるための逐次アルゴリズムである (Kurras, 2015)．\n種々の分野で再発見され，複数の名前を持っているようである．例：Sheleikhovskii 法，Kruithof アルゴリズム，Furness 法，Sinkhorn-Knopp アルゴリズム，RAS 法など (Kurras, 2015)．\n\\(W\\) の成分が正である場合は，(Sinkhorn, 1967) がアルゴリズムの収束と解の一意性を示している．1\nしかし，\\(W\\) の成分が零を含む場合，零成分の位置に依存してアルゴリズムは収束しないことがあり得ることを，(Sinkhorn and Knopp, 1967) が \\(d_r=d_c=1\\) の場合について示している．\n\n\n3.3.2 アルゴリズム\nIPF アルゴリズムは，観念的には，２つの周辺分布のうち片方を制約に課しながら，KL 距離を最小にする射影を返していく：\n\\[\n\\Pi^{2n+1}:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))}\\biggl\\{\\operatorname{KL}(\\Pi,\\Pi^{2n})\\,\\bigg|\\,\\Pi_T=\\mathrm{N}_d(0,I_d)\\otimes p(y_T)dy_T\\biggr\\},\n\\] \\[\n\\Pi^{2n+2}:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))}\\biggl\\{\\operatorname{KL}(\\Pi,\\Pi^{2n+1})\\,\\bigg|\\,\\Pi_0(x_0,y_0)=p(x_0,y_0)\\biggr\\}.\n\\]\n今回の場合， \\[\n\\Pi^{2n+1}=\\mathbb{P}_{y_T}^{2n+1}\\otimes\\delta_{p(y)},\\qquad\\Pi^{2n+2}=\\mathbb{P}^{2n+2}_{y_0}\\otimes\\delta_{p(y)},\n\\] と分解される．ただし，\\(\\mathbb{P}_{y_T}^{2n+1}\\) は次で定まる \\((Z_t)\\) の時間反転 \\[\ndZ_t=f_{T-t}^{2n+1}(Z_t,y_T)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),f_t^{2n+1}(x_t,y):=-f_t^{2n}(x_t,y)+\\nabla_{x}\\log\\Pi^{2n}_t(x_t|y),\n\\] \\(\\mathbb{P}_{y_0}^{2n+2}\\) は次で定まる \\((X_t)\\) の経路測度となる： \\[\ndX_t=f^{2n+2}_t(X_t,y_0)\\,dt+dB_t,\\qquad X_0\\sim p(x|y_0)\\,dx,f_t^{2n+2}(x_t,y):=-f_t^{2n+1}(x_t,y)+\\nabla_x\\log\\Pi_t^{2n+1}(x_t|y).\n\\] ただし，\\(f^0_t(x_t)=-x_t/2\\)．\n\n\n3.3.3 DDPS との関係\n最初のイテレーション \\(n=0\\) における \\(\\mathbb{P}^1_y\\) が雑音除去拡散 (1) に対応する．\nしかし，IPF アルゴリズムのイテレーションを繰り返していくごとに，\\(T&gt;0\\) が十分に大きくない場合でも正確に \\(\\mathrm{N}_d(0,I_d)\\) にデータ分布を還元する SB が得られるようになっていく．\n\n\n3.3.4 DSB-PS\nこの際，スコア \\(\\nabla_z\\log p_{T-t}(Z_t|y)\\) から始まり，\\(f^{n}_t\\;(n\\ge2)\\) の推定も逐次的に行なわなければならない点については，mean-matching (De Bortoli et al., 2021), (Shi et al., 2022) という方法が考えられている．\nこの方法を用いて，IPF アルゴリズムが収束するまで実行して最終的に得るサンプラーを Schrödinger 橋サンプラー (DSB-PS: Diffusion Schrödinger Bridge Posterior Sampling) という．"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#footnotes",
    "href": "posts/2024/Samplers/Schrodinger.html#footnotes",
    "title": "Schrödinger 橋",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし，(Deming and Stephan, 1940) にも (Fortet, 1940) にも言及しておらず，Markov 連鎖の遷移確率の推定という文脈で研究している．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#雑音除去拡散によるサンプリング-ddgs",
    "href": "posts/2024/Samplers/Schrodinger.html#雑音除去拡散によるサンプリング-ddgs",
    "title": "Schrödinger 橋",
    "section": "4 雑音除去拡散によるサンプリング (DDGS)",
    "text": "4 雑音除去拡散によるサンプリング (DDGS)\n\n4.1 導入\n(Vargas et al., 2023) は次の２点を克服するサンプラーを提案した．\n\n4.1.1 条件付き生成\nDDPS と DSB-PS は，\\(y\\in\\mathcal{Y}\\) について一様に均してしまった 償却推論 を行っている．\n特殊な \\(y\\in\\mathcal{Y}\\) に対しても，これにフィットしたモデルを作りたい状況がある．\n\n\n4.1.2 正規化定数のわからない分布\n同時に DDPS と DSB-PS は，正規化定数の不明な分布などからのサンプリングには使えない．\nここでは， \\[\np(x)=\\frac{\\gamma(x)}{Z},\\qquad Z:=\\int_\\mathcal{X}\\gamma(x)\\,dx\n\\] という形で，\\(\\gamma\\) のみを与えられた場合を考える．\nDDPS において第 2.2 節で考えたように，\\((X_0,Y)\\) からのサンプルが得られないため，\\(\\nabla_x\\log p_t(x_t)\\) の項の近似に関しては別のアプローチを考える必要がある．\n\n\n\n4.2 \\(h\\)-変換としての表示\n雑音除去拡散 (1) の \\(\\nabla_x\\log p_t(x_t)\\) の表示が消えるような変数変換を考える．\nまず，OU 過程 \\((X_t)\\) を定常分布 \\(X_0\\sim\\mathrm{N}_d(0,I_d)\\) から始めた場合の分布を \\(\\mathbb{M}\\) とすると，この逆は \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] である．この過程の \\(\\mathbb{M}\\) の下での \\(h\\)-変換は， \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log h_{T-t}(Z_t)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T),\n\\] \\[\nh_t(x_t):=\\int_\\mathcal{X}\\Phi(x_0)m_{T|T-t}(x_0|x_t)\\,dx_0,\\qquad \\Phi(x_0):=\\frac{p(x_0)}{\\phi_d(x_0;0,I_d)}\n\\] と表せる．ただし \\(m\\) は OU 過程 \\((X_t)\\) の遷移密度とした．\nこの表示に対するパラメトリックな近似 \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+u^\\theta_{T-t}(Z_t)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] の分布を \\(\\mathbb{Q}^\\theta\\) で表し，\\(\\operatorname{KL}(\\mathbb{P},\\mathbb{Q}^\\theta)\\) を最小化することが最初に思いつくが，これでは \\(\\mathbb{P}\\) からのサンプル，従って \\(p\\) からのサンプルを必要としてしまう．\n\n\n4.3 逆 KL-乖離度の最適制御\n\\(h\\)-変換をした理由は，\\(\\operatorname{KL}(\\mathbb{Q}^\\theta,\\mathbb{P})\\) ならば計算できる点にある．\n\\[\n\\mathcal{L}(\\theta):=\\operatorname{KL}(\\mathbb{Q}^\\theta,\\mathbb{P})=\\operatorname{E}_{\\mathbb{Q}^\\theta}\\left[\\frac{1}{2}\\int^T_0\\|u^\\theta_{T-t}(Z_t)\\|^2\\,dt-\\log\\Phi(Z_T)\\right]\n\\] については，\\(\\log\\Phi(Z_T)\\) には \\(\\theta\\) が出現しないため，第一項のみに集中すれば良い．\nそうすると，これは KL 最適制御問題として解くことができる．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html",
    "href": "posts/2023/Surveys/ParticleFilter.html",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#フィルタリング問題の歴史",
    "href": "posts/2023/Surveys/ParticleFilter.html#フィルタリング問題の歴史",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "1 フィルタリング問題の歴史",
    "text": "1 フィルタリング問題の歴史\n\n1.1 フィルタリング（濾波）問題とは何か？\nフィルタ（濾波器）の第一義は，液体から不純物を取り除くための装置である．そのアナロジーで「フィルタリング問題」と言った場合は，信号処理の意味で電圧や電波の信号を「濾過」してノイズを除去し本当に注目したい部分を純粋化する営みのことを指す．凡ゆる通信機器において装置の熱運動によるノイズが入ることは避けられぬ自然の摂理であり，フィルタリング問題は普遍的な課題である．1\nGauss は天体観測の経験から「誤差論」と最小二乗法を発明し，これが現代の統計的推定理論の先駆けとなった．これと同じように，通信と制御の分野では「時々刻々と受信するデータから時々刻々と変化する信号をどのようにうまく濾波するか」という独自の課題から，独自の理論が発展していった．2\n特に，デジタル回路がない時代では，「どのような電気回路のシステムとして濾波機をデザインすれば良いか？」という電気工学的な回路設計の問題としての側面も大きかった．\n\n\n1.2 最初のフィルタリング理論\nこのアナログフィルタの時代で，フィルタリングの問題を統計的技術で解くための理論3 が，まず離散時間の場合が (Kolmogorov, 1941)，続いて連続時間の場合が (Wiener, 1949) によって模索された．4\nしかし，この Kolmogorov と Wiener 理論では「信号とノイズの過程が定常である」という仮定の下で展開されており，この定常性の制約が Kolmogorov-Wiener 理論の広い応用を阻んでいた．\nだがこれは，「当時の技術（抵抗器やコンデンサーなど）で実装出来る範囲」という制約がある上で考えられた理論としては，仕方ないことでもあった．更なる理論的発展も，デジタル技術の登場を待つ必要があった．\n\n\n1.3 デジタルフィルタリングの登場\nトランジスタというデジタル技術が使われるようになり，集積回路の製造技術が発達すると，「フィルタ（濾波器）」はアナログデジタル変換器，レジスタ，メモリ，マイクロプロセッサから構成されたデジタルフィルタが主に使われるようになった．アナログフィルタから物理的な姿は全く変わり，もはや肉眼では見えない装置になってしまったのである．5\nその中で (Kalman, 1960) が，定常性の仮定が満たされない場合でも使えるアルゴリズムである カルマンフィルター を提案すると，すぐに Apollo 計画に導入されてスペースシャトルの制御へ実用化され，更には海軍の潜水艦などにも応用されていった．6\nあらゆるシステムがデジタル化されていく中で，アナログフィルタは徹底的にデジタルフィルタに代替されるようになった．これに伴い，現代でフィルタリング問題と言った場合，現在 \\(t\\) までの観測 \\(Y_1,\\cdots,Y_t\\) からノイズを除去してメッセージ部分 \\(X_t\\) をなるべく正確に推定するアルゴリズム という完全に数学的で抽象的な存在として研究が進められていくことになる．\n\n\n1.4 状態空間モデルという語彙\nKalman の論文 (Kalman, 1960) の新規性はアルゴリズムだけではなく，状態空間モデル という枠組みを導入し，どのようなシステムに適用可能かに関する共通言語を提供したことが，即時的な応用に寄与した面もあるだろう．7 例えばこの語彙に従えば，「Kalman filter は状態空間モデルが線型正規であれば最適なフィルタリング手法」ということになる．\nこの状態空間モデルの枠組みと同様なものが (Baum and Petrie, 1966) によって隠れ Markov モデルとして展開され，こちらは (Baker, 1975) により音声認識に応用された．現代でも多くの音声認識システムは隠れ Markov モデルに基づく．8\n状態空間モデルはその後，(Akaike, 1974) で時系列モデリングに応用され，再び統計学の分野と深く関わるようになる．9\n\n\n1.5 カルマンフィルターの限界\nしかし Kalman filter にも大きな制約があった．それは モデルに線型かつ正規であるという大きな制約があった ということである．一方で現実のシステムは殆どが非線型性を持つ．\nそこで NASA の Ames 研究センター ではすぐに 拡張 Kalman フィルタ と共分散行列の二乗根を保持する実装が考案され，これが本当の意味で Kalman フィルターを実用に耐えるものにした (McGee and Schmidt, 1985)．10\nだが，「拡張」の名前の通り本質的な解決とは言えず，システムの非線型性が強い場合は性能が伸びない．11 こうして，計算機や情報通信技術の発展と共に複雑化していくシステムに併せて，様々なフィルターが考案されていく必要があるのである．統計計算の時代の黎明である．\n\n\n1.6 非線型性・非正規性という強敵\n実は，1960年に開発された Kalman filter を真に超克する手法は，1990年代に入るのを待つ必要がある．他のシミュレーションに基づく ベイズ計算手法 と同じく，計算機の十分な発達を待つ必要があったのである．\nというのも，Kalman filter は，線型正規状態空間モデルの下でフィルタリング分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\n\\tag{1}\\] は再び正規分布であり，平均と共分散という２つの量のみで完全に特徴付けられるため，これらのみを考慮し，微分方程式を解いて更新規則を事前に得ておくことで，計算量を大幅に削減することが出来る，というトリックに基づく．\nこのように線型性と正規性が同在する状況，または状態空間が有限であるなどの限られた状況でない限り，フィルタリング分布 式 1 は有限次元の十分統計量を持たない．12\n従って，一般の状況に対応出来るフィルタには，上述のような計算量削減のトリックは絶対に存在せず，正面から分布 式 1 を近似する方法を考える必要がある．これには新しいアイデアが必要であると同時に，一定の性能を持つ計算機の出現を待つ必要もあったのである．\n\n\n1.7 種々の近似戦略\nフィルタリング分布 式 1 \\[\\mathbb{P}_t(X_t\\in dx_t):=\\mathcal{L}[X_t|Y_{0:t}]\\] は Bayes の定理を通じて再帰的な関係 \\[\n\\begin{align*}\n    &\\mathbb{P}_{t-1}(X_t\\in dx_t|Y_{0:t-1})\\\\\n    &=\\int_{x_{t-1}\\in E}\\mathbb{P}_{t-1}(X_{t-1}\\in dx_{t-1}|\\\\\n    &\\qquad Y_{0:t-1}=y_{0:t-1})P_t(x_{t-1},dx_t),\\\\\n    &\\mathbb{P}_t(X_t\\in dx_t|Y_{0:t}=y_{0:t})\\\\\n    &=\\frac{1}{p_t(y_t|y_{0:t-1})}f_t(x_t|y_t)\\\\\n    &\\qquad\\mathbb{P}_{t-1}(X_t\\in dx_t|Y_{0:t-1}=y_{0:t-1}).\n\\end{align*}\n\\] を満たすが，この積分を計算する必要がある．13\n粒子フィルターでは相関を持った粒子系により，これらの値を逐次的に近似していくが，このアルゴリズムが出来る前に提案された近似手法を総覧する．\n\n1.7.1 解析的な方法\n節 1.5 で紹介した拡張 Kalman filter は，モデルが線型に近い場合には有効な近似手法であるが，一致推定量にはならない．この点を修正するために，線型近似の誤差を修正する IEKF (Iterated EKF) などが開発された．\n\n\n1.7.2 数値積分による方法\n状態空間 \\(E\\) の次元が3以下である場合，積分は数値積分法によっても効率的に近似できる (Kitagawa, 1987)．14\n特に状態空間が有限である場合は積分は加法に退化するため，正確に実行することができる．15 この場合が隠れMarkovモデルに当たる．\n隠れMarkovモデルに於て MAP 推定量を動的計画法に基づいて探索する Viterbi 算譜 (Forney, 1973), (Viterbi, 1982) とパラメータ推定のための EM アルゴリズムの変種 Baum-Welch 算譜 (Baum and Eagon, 1967), (Gopalakrishnan et al., 1989) とは音声認識の分野で広く使われており，また状態空間が近似的に離散である場合にも近似手法として採用される．16\n\n\n1.7.3 Gauss混合で近似する方法\nフィルタリング分布 式 1 を正規分布の有限混合で近似する方法は Gaussian sum filter と呼ばれる (Sorenson and Alspach, 1971)．これは多峰性を帯びる事後分布のモデリングに強く，物体追跡の分野で広く使われることとなったが，一般の設定に使える普遍的な手法ではなかった．17\n\n\n1.7.4 アンサンブルによる近似\nフィルタリング分布 式 1 を正規分布を表現する決定論的に設計された粒子系によって近似し，これをシステムモデルに従って伝播させる．これにより，フィルタリング分布の2次以下の積率までは性格に近似できていることになる．この手法を 無香 Kalman filter (Unscented Kalman filter) という (S. Julier et al., 2000), (S. J. Julier and Uhlmann, 2004)．\nこの手法は，宇宙から大気圏に再突入する弾道物体の追跡などの場面で，粒子フィルターよりやや正確性が劣るが計算が速く，実用的であることも知られている (Ristic et al., 2004, p. 101)．\nこの手法は本質的に (Evensen, 1994) の EnKF (Ensemble Kalman filter) によって拡張された（ 節 3.5 も参照）．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#粒子フィルターの発明",
    "href": "posts/2023/Surveys/ParticleFilter.html#粒子フィルターの発明",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "2 粒子フィルターの発明",
    "text": "2 粒子フィルターの発明\n線型性や正規性の仮定を 全く 必要とせず，あらゆる状態空間モデルに使えて，加えて高次元でも適用可能な夢の新フィルタリング手法は，確率的シミュレーションを利用する Monte Carlo 法の一種 であった．(Gordon et al., 1993) はこれを bootstrap filter という名前で発表し，角度観測のみを用いた物体追跡の問題 (bearings-only tracking) への応用も付した．\n実は，この bearings-only tracking は極めて非線型性が強い問題として古典的なものであり，従来の拡張 Kalman filter の方法では精度が全く伸びなかった．これに比べて bootstrap filter では圧倒的な性能改善が見られたのであった．18\nそれだけでなく，この手法はフィルタリング問題の範疇を超えて広範な応用先を見つけつつあり，MCMC と並ぶ ベイズ計算法 となっている（ 節 2.3 ）．\nなお，北川源四郎も同年（1993年）のカンファレンスにて，Monte Carlo filter の名前で同様のアルゴリズムを発表している．そのジャーナル版は (北川源四郎, 1996a)．19 日本語文献 (北川源四郎, 1996b) はウェブ上からも読める．\n\n2.1 逐次重点サンプリングの修正としての粒子フィルター\nこの手法は 重点サンプリング を繰り返すという逐次重点サンプリングの改良として開発された．逐次重点サンプリングのアイデアは古く，(Hammersley and Morton, 1954) で提案され，(Mayne, 1966), (Handschin and Mayne, 1969) で逐次推定に応用された．\nしかし，これには荷重の分散が指数増大するという致命的な欠点があった (Chopin, 2004)．特に何度か反復を経ると，１つの粒子を除いて他の粒子は全て荷重を殆ど持たなくなってしまうという現象が起こる (Del Moral and Doucet, 2003)．このように，荷重の分散が大きくなり，少数の粒子しか推定に関与しなくなる現象を 荷重の縮退 という．20\nそこで，(Rubin, 1987) のアイデアを基に，21 リサンプリング という新たな機構を取り入れることを考える．これは 遺伝的変異・選択機構 とも呼ばれ，22 尤度の高い粒子を複製する一方で尤度の低い粒子は削除するというものである\nこれにより定期的に荷重をリセットすることで分散の指数増大を抑えることができ，が保たれるのである．しかしながらこの仕組みにより粒子の間に相関が生じるために，理論的解析を困難にする．この点から粒子フィルターは （平均場）相関粒子法 ともいう．23\nリサンプリング機構の分計算負荷は上がるが，1990年代では計算機の性能はすでにこれを補って余りある段階に達していたのである．なお，(Gordon et al., 1993) はリサンプリング手法としては多項リサンプリングを採用しており，極めて実装が簡単という点も多くの応用を生んだ理由である．24\nリサンプリングの実際の実装については，粒子フィルターの実装の稿 も参照．\n\n\n2.2 粒子フィルターの応用\n(Gordon et al., 1993) による粒子フィルターの考案は，物体追跡（と防衛目的）への応用が念頭にあり，この分野では粒子フィルターが極めて有効である (Ristic et al., 2004)．これは非線型性をものともしない性質に加えて，事前情報を柔軟に取り入れやすいという粒子フィルターの性格も大きく貢献している．25\nコンピュータビジョンへの応用も早期から取り組まれており (Isard and Andrew, 1998)，これに続いてロボティクス，HCI (Human-Computer Interaction) 分野への応用もなされている (岡兼司, 2005), (Wills and Schön, 2023)．\n一方で，(北川源四郎, 1996a) は季節調整モデルなど非定常時系列への応用が念頭にあった．確率的ボラティリティモデルなど，ファイナンスで扱う時系列は非線型性・非正規性を示すと同時にデータ数も多い．逐次推定のステップ数が増えようとも誤差が蓄積しない粒子フィルターが見事に推定を実行する．26\n加えて，マクロ経済学の分野で 動学的確率的一般均衡モデル (DSGE) の推定にも応用されている．DSGE は非線型なミクロ経済学的モデルの上に構築された大規模なモデルで， 従来は MCMC を用いたベイズ推論が実行されていたが，粒子フィルターに焼戻し法や並列計算を組み合わせることでこの問題を回避でき，さらに事後分布が多峰性を持つ場合でも有用である (Herbst and Schorfheide, 2013)．27\n近年では他の社会科学分野でもベイジアンモデリングが用いられ（ベイズ計算の稿 参照），粒子フィルターも エージェント・ベースド・モデル への応用などが試みられている (Lux, 2018)．\n\n\n2.3 粒子フィルターの一般の推定問題への応用\nこうして，粒子フィルターは非正規・非線型フィルタリング問題の解決のために開発されたアルゴリズムであったが，非線型フィルタリングに限らず極めて広い問題へと応用出来ることが徐々に明らかになった．\n特にサンプラーとしても極めて有効であり（ 節 2.3.2 ），粒子フィルターは MCMC と併せて ベイズ計算 の主要トピックの１つに躍り出た (Martin et al., 2023, p. 11)．この意味で，粒子フィルターは広く 逐次 Monte Carlo 法（Sequential Monte Carlo methods 略して SMC ）とも呼ばれる．28\n\n2.3.1 ベイズ学習\n逐次的でない「静的」な設定の下での Bayes 推論に SMC を用いる方法は (Chopin, 2002) が草分け的な仕事をした．この枠組みでは，Bayes 事後分布 \\(\\pi(\\theta|y_1,\\cdots,y_N)\\) の近似において，途中の \\(\\pi(\\theta|y_1,\\cdots,y_n)\\) を経由して逐次的に近似することが，自然な計算コスト削減法として理解できる．\n\n\n2.3.2 サンプラーとしてのSMC\n複雑な分布からのサンプリングや，その正規化定数の計算という MCMC と同様の用途に SMC を使うこともできる (Del Moral et al., 2006)．\nSMC は 調音 (tempering) を通じてサンプリング問題に応用される．これは目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) に対して，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをするというアイデアである．\nこの媒介的な分布 \\(\\pi_n\\) を焼き戻し分布 (tempered distribution) または架橋分布 (bridging distribution) などとも呼ぶ．29\n詳しくは，SMC サンプラーの稿 を参照．\n\n  \n    \n    \n      粒子フィルターを用いたサンプリング | About SMC Samplers\n      粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n    \n  \n\n\n\n2.3.3 最適化\nSMC を最適化へ応用することができる．\nまず，任意の確率的最適化アルゴリズムに対して，これを並列して実行し，うまくいっているものとうまくいっていないものの間に遺伝的変異・選択機構を導入することでより性能の良い発見的アルゴリズムを導出出来ることを (Aldous and Vazirani, 1994) が指摘しており，この機構を “go with the winners” と呼んでいる．\n古典的な大域的最適化法に 焼きなまし法 (simulated annealing) (Kirkpartick et al., 1983) があるが，(Schäfer, 2013) は特定の目的関数に対してこれを一般化して粒子法に基づく最適化法を提案し，特に多峰性を持つ場合に大きく性能を改善した．\n(Johansen et al., 2008) では潜在変数モデルのパラメータの最尤推定に，EM アルゴリズム (Dempster et al., 1977) の代わりに simulated annealing に基づいた手法を用いている．分布の台が最尤推定量に収束するような分布の列 \\[\n\\overline{\\pi}_{\\gamma_n}(\\theta)\\,\\propto\\,p(\\theta)p(y|\\theta)^{\\gamma_n}\n\\] を構成し，これから逐次的にサンプリングをするのである．最終的なアルゴリズムは，EM アルゴリズムや MCMC を用いる場合より，局所解に囚われることが少なく，初期値の設定に殆ど左右されないという利点がある．\nこの枠組みは一般の非凸最適化アルゴリズムになる可能性がある．30\n\n\n2.3.4 稀現象シミュレーション\nSMC によるサンプリングは，直接のシミュレーションが困難な分布 \\(\\pi_p\\) に対しても，\\(\\pi_1,\\cdots,\\pi_p\\) と逐次的に近似することでこれを可能にするというアイデアであった．\n特に，シミュレーションが困難である分布 \\(\\pi_p\\) の例として，極めて稀な事象 \\(A_p\\) に関する条件付き分布などがあり得る．これに対して事象列 \\(A_0\\supset\\cdots\\supset A_p\\) を取り， \\[\n\\pi_n(d\\theta)=\\frac{1}{L_n}1_{A_n}(\\theta)\\nu(d\\theta)\n\\] という仲介分布の列を取るのである．この問題を 稀現象シミュレーション という．\n\n\n2.3.5 確率的グラフィカルモデルの推論手法として\n変数間の統計的依存関係が無向グラフによって与えられるモデルを 確率的グラフィカルモデル という．ニューラルネットワーク (Rumelhart et al., 1987) も 状態空間モデル もその例である．\n確率的グラフィカルモデル自体多くの応用先をもち，疫学における疾病マッピング (Green and Richardson, 2002)，画像解析 (Carbonetto and Freitas, 2003) などにも応用されている．31\nこのようなモデルの尤度は極めて複雑になるが，これへ至る道を自然な方法で構成することができる (Hamze and de Freitas, 2005)．複雑なモデルでは MCMC は尤度の正規化定数を評価する方法を持たないが，SMC ではこれを自然に評価することができる．\n\n\n2.3.6 ポリマーシミュレーション\nポリマーシミュレーションにおいても重点サンプリング法と同じ発想が提案されたのは極めて早い段階であった (Rosenbluth and Rosenbluth, 1955)．加えてリサンプリングにあたる enrichment の考え方もあった (Wall and Erpenbeck, 1959)．\nなお，このような物理・化学的文脈では，状態空間を探索する主体としての意味を強調し，「粒子」の代わりに walker と呼ぶ．32\nこれら２つを組み合わせることで，長いポリマー鎖のシミュレーションを可能にする方法として，相関粒子法に基づくアルゴリズム PERM (pruned-enriched Rosenbluth algorithm) が提案されている (Grassberger, 1997)．\nこの手法はたんぱく質の折り畳み問題にも応用されている (Hansmann and Okamoto, 1999)．\n\n\n2.3.7 量子系シミュレーション\n量子多体系の基底状態のシミュレーションにおけるモンテカルロ法である QMC (Quantum Monte Carlo)33 でも， branching というリサンプリング機構を取り入れた相関粒子法が用いられている (Assaraf et al., 2000)．\nこれは大規模な疎行列の最小固有値・固有ベクトルを近似する手法として，量子系に限らない幅広い応用がある．34\n\n\n\n2.4 粒子フィルターの弱点\n\n計算量\n粒子フィルターは高い汎用性の代償として，多数の粒子による高精度な推論のためには多くの計算量を必要とすることは欠点に挙げられる．が，CPU や並列計算の発展により十分な量の粒子を用意できる場面も増えたため，その問題点も形骸化してきてると言える．35\n荷重の縮退\nまた粒子フィルターは，観測 \\(Y_t\\) の次元が大きいなど，観測から得られる情報量が多く，尤度（ポテンシャル）の尖度が高いとき，リサンプリング機構があってもやはり縮退を起こしてしまう．36 このような場合は，観測の情報を柔軟に取り入れた提案核を構築し，誘導粒子フィルターをうまく設計する必要がある．\n\n\n粒子フィルタを適用する際の課題の一つは，各粒子に割り当てられる重みが１粒子に集中する，いわゆる退化の問題を限られた数の粒子でいかに克服するかである．(上野玄太, 2019)\n\n\n高次元\n地球科学や天気予報の分野では \\(Y_t\\) は大きく（\\(10^7\\) を超えることもある），このような場合は粒子フィルターは実行可能でなくなる．加えて Kalman フィルターも逆行列の計算が不安定になり，アンサンブル Kalman フィルタという粒子法が用いられる（ 節 3.5 ）．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#今後の研究",
    "href": "posts/2023/Surveys/ParticleFilter.html#今後の研究",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "3 今後の研究",
    "text": "3 今後の研究\n粒子フィルターの更なる応用には次の点の研究が肝要である．\n\n3.1 Feynman-Kac 理論と粒子法の統一的理解\n粒子フィルターは状態空間モデルのフィルタリングに使えるだけでなく，Feynman-Kac 測度の確率的近似に使える汎用手法である というのがより新しい数学的理解である．37 この文脈では 相関粒子法 (interacting particle methods) と呼ばれる．38\n\nthe mathematical concepts and models are now at a point where they provide a very natural and unifying mathematical basis for a large class of Monte Carlo algorithms. (Del Moral and Doucet, 2014, p. 2)\n\nこの枠組みからならば，粒子フィルターに限らず，物理学・化学・工学で用いられている多くの 発見的手法 について，理論的な解析が可能になる可能性がある．\n\n\n3.2 提案分布の取り方\n節 2.4 で触れた通り，特に観測の情報量が大きい場合，提案分布の選び方が粒子フィルターの精度を大きく左右する．これは粒子を高確率領域に始めから誘導するように設計する 誘導粒子フィルター によってある程度対処できる．39\n参照 Markov 核 \\(M_t\\) がより良い攪拌性を持ち，ポテンシャル \\(G_t\\) が平坦であるほど性能は良い傾向があるが，40 このときの提案分布の取り方について普遍的な指針というものが得られていない．\n\nThe key factors for a successful application of particle filters in practice are therefore a good choice of the importance density and Rao-Blackwellization if possible. (Ristic et al., 2004) Epilogue\n\n(Guarniero et al., 2017) と (Heng et al., 2020) は提案分布にパラメトリックモデルを用意し，粒子推定量の分散を最小化するようにそのモデル内で逐次的に最適化していく機構を提案している．\n(Naesseth et al., 2015) が提唱する nested SMC は，は各時刻での提案分布を近似するために，もう一つのSMCを内部に走らせる．当然計算量は二倍になるが，それでも単純な bootstrap filter から大きく性能が改善する場合が多い．\n加えて機械学習の観点からも，真の事後分布との KL 距離を適応的に最小化する汎用手法に，提案分布のパラメトリックモデルにニューラルネットワークによる大型のパラメトリックモデルを併せる手法が提案されている (Gu et al., 2015)．\n\n\n3.3 高次元性への対応\n状態空間が高次元になることと，既存のモデルを状態空間モデルに定式化することに困難が伴うことが，朧げながら共通課題のように思われるが，その現れ方と解決法は個々の事例で異なる．\n\n3.3.1 部分的な線型構造の利用\n周辺化粒子フィルター，または Rao-Blackwellized particle filter とも呼ばれる方法である．これは多くの場面で，仮定されている状態空間モデルが部分的に線型である場合に，線型の部分をKalmanフィルターによって正確に解き，残った部分のみを粒子フィルターで解くことで，精度の向上とアルゴリズムの効率化を図る方法である．\n(Ristic et al., 2004, p. 287) では，探知前追跡 (track-before-detect) の問題41において，周辺化粒子フィルタが，必要な粒子数を大きく削減してくれることを紹介している．\nその他にも，問題毎の特有の構造を利用して計算量を削減・パフォーマンスを最適化することは重要な営みである．\n\n\n\n3.4 漸近論\n\n3.4.1 粒子数に関する漸近論\n目前の問題を，所与の精度で解くために必要な粒子数は幾ばくか？という問題は実用上も有益だと思われる．42\n\n\n3.4.2 時間離散化に関する漸近論\n(Chopin et al., 2022)\n\n\n\n3.5 EnKFの数学的解析\n状態空間が高次元である場合，（どうなる？）\nモデルが正規性を持つならば，EnKF (Ensemble Kalman Filter) (Evensen, 1994) が有効であり，データ同化の分野で広く使われている．43\n一方で，その数学的な振る舞いはほとんど解明されておらず，1次元の場合から調べている状況である (Del Moral and Horton, 2023)．\n\n\n3.6 粒子フィルターの応用\nリアルタイム性と ベイズ推定の意思決定への応用との相性の良さ が，今後多くの重要な応用を見る可能性がある．\n\n3.6.1 属人化医療への応用\n筆者は属人化医療への応用が大きなモチベーションになっている．44\n病気の進行（あるいは健康）のモニタリングのために，健康診断やウェアラブルデバイス，フォローアップから得られるデータは，治療の見直しや異常の早期発見のために即時処理されることが望ましい．これに逐次モンテカルロ法でBayes的に迫る研究がある (Alvares et al., 2021) ！\nさらに，個々人の日常生活のレベルではSMCを用いているものはどうやらまだなく，運動と睡眠時間の間の関係と処置効果をMonte Carlo法を用いて推定している研究はある (Daza and Schneider, 2022)．これを逐次化することで，よりリアルタイムで自分に合った生活習慣への示唆が得られるアプリを開発できるかもしれない．\nまた，属人化医療においてはシステム生物学的なモデルに基づいた薬効推定が欠かせない．小規模な患者群と測定時間（服薬時間）に関する不確定性を考慮した手法が (Krengel et al., 2013) で考察されている．\nまた，腫瘍サンプルに含まれる体細胞の突然変異に関するデータから，SMCを用いて腫瘍の発達と進行の状態を理解する手法も提案されている (Ogundijo et al., 2019)．\n\n\n3.6.2 ゲノム解析への応用\n次世代DNAシークエンサーでは，DNAの各塩基ごとに異なる蛍光物質を結合させ，蛍光の波長と強度により塩基を読み取る仕組みであり，蛍光強度の生データからDNA配列データへ変換するベースコールと呼ばれる段階で粒子フィルターを使うことも提案されている (Shen and Vikalo, 2012)．\n\n\n3.6.3 疫学への応用\nCovid-19のようなパンデミックにおいて，疫学モデルを通じて時間変動する再生産数をリアルタイムでモニタリングをして意思決定に繋げるためのSMC手法も考えられており，実際にノルウェーで使用され有効性が実証された (Strovik et al., 2023)．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#footnotes",
    "href": "posts/2023/Surveys/ParticleFilter.html#footnotes",
    "title": "粒子フィルターとは何か | About Particle Filter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Anderson and Moore, 1979) 第1.1節．↩︎\n(有本卓, 1970)．↩︎\nこのフィルタリング問題の統計的な側面を，前述の電気工学的な側面から区別して，stochastic filteringと呼んだりもする．↩︎\n(Bain and Crisan, 2009) 1.3節．↩︎\n(青山友紀, 1986) 「当初は大型コンピュータを用いたシミュレーションの技法であったディジタルフィルタが，今では超LSI 1チップで実現されるまでになった．」他にも，発表時1986年では，音声信号を中心とする低周波帯域ではデジタルフィルタがアナログフィルタを駆逐しつつあること，通信システムのデジタル化に伴ってこの勢いは完全に代替するまで進むだろうとの筆者の考えが述べられている．↩︎\n(McGee and Schmidt, 1985) がNASAからの資料．また，(Del Moral and Penev, 2014) も参照．加えて，(Kalman, 1960) からちょうど10周年の文献 (有本卓, 1970) に当時の雰囲気も感じさせる良いサーベイがある．↩︎\n(Ristic et al., 2004, p. 3) “The state-space approach is convenient for handling multivariate data and nonlinear/non-Gaussian processes and it provides a significant advantage over traditional time-series techniques for these problems.”↩︎\n(Rabiner, 1989)．(Gales and Young, 2007) には “almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs” とある．↩︎\n(Kitagawa, 1998) で指摘されている．↩︎\nまた，同様にアポロ計画の中で，飛行体に載積出来るような小規模な計算資源と短い単語長でも安定してKalman filterが動くように，共分散行列の二乗根を保持するという “square-root” formulation of the filter が 1972年に考案されたことが，summary でも触れられている (McGee and Schmidt, 1985)．↩︎\n(S. J. Julier and Uhlmann, 2004, p. 402) など．拡張 Kalman filter は遷移関数を線型近似することに基づく．よって Jacobi 行列の計算が必要であり，このため 解析的方法 とも呼ばれる (Ristic et al., 2004, p. 21)．よって遷移関数が可微分でない場合も実行不可能である．とはいっても，拡張 Kalman filter はナビゲーションシステムや GPS のデファクトスタンダードである Wikipedia．(Ristic et al., 2004) 第7章では距離のみでの追跡 (range-only tracking) では拡張 Kalman filter の性能と大差なく，計算量の問題から EKF の選択を推奨している．第8章でも弾道物体追跡の問題で，限られた設定では粒子フィルターと同等の性能を見せている．↩︎\n(Ristic et al., 2004, p. 16)．↩︎\nこの結果は (Chopin and Papaspiliopoulos, 2020) 第5章 など．記法 \\(Y_{0:t}\\) は 本サイトの数学記法一覧 を参照↩︎\n(Crisan and Doucet, 2002) に3の数字が例示されている．↩︎\n(Chopin and Papaspiliopoulos, 2020) 第6章，(Ristic et al., 2004) 2.2節．↩︎\n(Ristic et al., 2004, p. 24)．↩︎\n(Ristic et al., 2004, p. 25)．↩︎\n(Gordon et al., 1993) の結果であると同時に，(Ristic et al., 2004) 第6章でも種々の手法と比較した数値実験がなされている．一方第7章にて，距離のみでの追跡 (range-only tracking) では拡張 Kalman filter の性能と大差なく，計算量の問題から EKF の選択を推奨している．↩︎\nDel MoralのWebサイトも歴史的背景に詳しい．↩︎\nweight degeneracy (Creal, 2011) p.253，(Cappé et al., 2005) 第7章，(Robert and Casella, 2004, p. 551) 14.3.3節 など．↩︎\nSIR (Sampling/Importance Resampling) Algorithm と呼ばれるものであった．これの逐次化が粒子フィルターだとみなせる (Robert and Casella, 2004, p. 552) 14.3.4節．↩︎\n(Del Moral and Doucet, 2014) などの用語である．↩︎\n(Del Moral and Horton, 2023) では mean-field type interacting particle methods と呼んでいる．呼び方については (Iba, 2001) と (Del Moral, 2013) と 紹介記事 も参照．↩︎\n(Creal, 2011) p.256．↩︎\n事前情報というのは，自動運転の文脈では自動車が動き得る領域というのは極めて限られている，というような事前に判明しているが，うまく取り入れにくい情報のことをいう．(Ristic et al., 2004) 第6章や第9章で繰り返し種々の設定で実証されている．(Yang et al., 2023) は水中での物体追跡が縮退により従来は粒子フィルタが使えなかった問題の解決を試みている．(Kummert et al., 2021) はロボット支援を用いた手術中に物体追跡を利用する際に，尤度が低すぎるなどの要素から追跡対象を失った状態を検出してアラートを出す機構を開発している．↩︎\nファイナンスにおける確率的ボラティリティモデルなどの例が挙げられている (Creal, 2011) p.256．↩︎\n粒子フィルターの経済学での応用が増えたきっかけが Fernández-Villaverde and Rubio-Ramírez (2005, 2007) による（小規模な）DSGEモデルの推定への応用だった (Creal, 2011, p. 246)．(矢野浩一, 2014) は実物景気循環モデル (Real Business Cycles Model) への応用を解説している．↩︎\n(Crisan and Doucet, 2002) ではすでに SMC とも呼ばれることが記されている．(Chopin and Papaspiliopoulos, 2020) 第3章にSMCのフィルタリング以外の多くの応用が紹介されている．↩︎\n(Behrens et al., 2012) はいずれも用いている．(Hamze and de Freitas, 2005) に tempered distribution の語用法がある，↩︎\n(Chopin and Papaspiliopoulos, 2020, pp. 3.4節 p.30)↩︎\n疾病マッピングは疾病の空間的な分布を把握すること，画像解析とは画像データから特徴を抽出してその意味論を理解することを指す．↩︎\n(Iba, 2001), (Kremer and Binder, 1988) などが良いレビューを提供している．(Assaraf et al., 2000) が量子系のシミュレーションの文脈で．↩︎\ndiffusion Monte Carlo, projector Monte Carlo などと呼ばれる手法に等しい．Green’s function Monte Carlo もポテンシャル \\(G\\) の取り方が違うのみである (Assaraf et al., 2000)．また (Iba, 2001) 3.1節 p.282 にも言及がある．↩︎\n(Iba, 2001) 3.1節 p.281．↩︎\n(矢野浩一, 2014) p.190，(Ristic et al., 2004) 前文 p.xi．↩︎\n(Chopin and Papaspiliopoulos, 2020, pp. 19.1節 p.371), (Creal, 2011, pp. 2.5.1節 p.258)．↩︎\n(Del Moral and Doucet, 2014) など．↩︎\n(Iba, 2001) などでは population Monte Carlo と呼ばれている．↩︎\nそのアイデアは (Doucet et al., 2001, pp. 79–95) 第4章 から．↩︎\n(Crisan and Doucet, 2002, p. 739) も参照．↩︎\n探知前追跡とは，信号が弱い，またはノイズが強い環境下において，信頼のおける初期信号を頼りにせずとも，物体追跡を実行するための手法．↩︎\n(Ristic et al., 2004, p. 288) に示唆されている．↩︎\n(Del Moral and Horton, 2023) は ensemble Kalman particle filtering methodology と呼んでいる．↩︎\n過去の記事でも触れた．↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "",
    "text": "統計的検定の考え方と，その科学的な態度については，(大塚淳, 2020, pp. 97–106) が大変含蓄が深い．\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#仮設検定",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#仮設検定",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "1 仮設検定",
    "text": "1 仮設検定\n\n1.1 二項モデルでの検定\n\n\n\n\n\n\n問題\n\n\n\n冠動脈バイパス手術を受けた20歳の青年が３日後に死亡した．\n\n同病院では過去３年の30件のうち10人が術後１週間以内に死亡している．\n一般に術後１週間以内に死亡する確率は0.2である．\n\n不審だと言えるだろうか？言えるとしたらどのような意味で？\n\n\n「正常な範囲内の事象である」とする帰無仮説の下で，当該事象が起こる確率を計算する．これが5%以下だったら「不審だと思うに足る」と言えるだろう．\n本問題は死亡率 \\(p\\in[0,1]\\) という母数に関する検定問題と捉えることができ，すると帰無仮説は \\[\nH_0:p=0.2\n\\] というシンプルな表示を得る．\nこの下で，\\(n=30\\) として，確率変数 \\(X\\) を「術後１週間以内に死亡する人数」とすると，\\(X\\) は二項分布 \\(\\mathrm{Bin}(30,0.2)\\) に従う（二項分布の定義は Section 2.1 ）．\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n\n# Parameters for the binomial distribution\nn = 30  # number of trials\np = 0.2 # probability of success in each trial\n\n# Generating data for the binomial distribution\nx = np.arange(0, n+1)\ny = binom.pmf(x, n, p)\n\n# Plotting the binomial distribution\nplt.figure(figsize=(3.5, 3))\nplt.plot(x, y, 'bo', markersize=5)\nplt.vlines(x, 0, y, colors='b', lw=5)\nplt.title('Binomial Distribution - n=30, p=0.2')\nplt.xlabel('Number of Deaths')\nplt.ylabel('Mass')\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nこの図からも，10人以上になる確率は極めて小さいことが判るだろう．実際に計算してみると，\n\n\nCode\nprob_10_or_more = 1 - binom.cdf(9, n, p) - binom.pmf(10, n, p)/2\nprint(f\"p-value is {prob_10_or_more:.4f}\")\n\n\np-value is 0.0434\n\n\nとなる．この「帰無仮説 \\(H_0\\) （今回は「１週間死亡率は20%」）を仮定した下で，実際に観測した事象（今回は「30人のうち10人が一週間死亡」）が起こる条件付き確率」を \\(p\\)-値 と呼ぶ．1\n\n\n\n\n\n\n注（検定統計量の選び方）\n\n\n\n\n\n上で叙述したのは，「死亡者数」という離散確率変数を検定統計量に用いた場合である．しかし，本書 (草野耕一, 2016, p. 99) では，別の離散検定統計量に対して，正規近似を通じて計算している．これは \\(z\\)-検定と呼ばれるものである（ Wikipedia も参照）．\n計算機が得意ならば，直接計算で出した方が近似誤差がないため，好ましいだろう．実際，書籍で得られた値は \\(p=0.0344\\) であり，過小評価している．その論拠は「\\(pn,(1-p)n\\) のいずれも \\(5\\) 以上であれば正規分布と同一視して良いことが知られている」という点である．\n一方で，上の議論では \\(p=0.0611\\) と5%の水準を超えている．一方で，(Lancaster, 1961) の mid-\\(P\\) value と呼ばれる補正法を用いると \\(p=0.0434\\) となり，再び有意になる．\nこのことをどう評価するべきか……．技術的・専門的すぎてとても人口に膾炙するものではないと思うと同時に，非常に本来的ではない議論になっていると感ずる（ Section 1.5 ）．\n\n\n\n\n\n1.2 誤り\n帰無仮説を間違えて棄却してしまうことを，第一種の過誤 という．これは有意水準 \\(\\alpha\\) の値に一致する．\n統計的仮説検定の理論は，初めは Neyman と Pearson によって科学的発見の文脈で考えられたものであるため，帰無仮説の棄却は「科学的発見の萌芽」と同義と解すことが多い（(大塚淳, 2020, pp. 97–106) も参照）．その場合，第一種の誤りとは「本当はなんでもないのに大発見だと思い上がりってしまう確率」である．これを犯す確率を最も制限したい，という志向を持つ．\n次に，第一種の誤りの可能性 \\(\\alpha\\) を制限した上で，本当は帰無仮説が誤りなのに棄却できないリスク＝発見を検出できないリスクをなるべく下げることを二次的目標として考える．これを 第二種の過誤 という．その確率 \\(1-\\beta\\) に対して，\\(\\beta\\) を 検出力 (power) と呼ぶ．\nこれは 第二回で扱った検察官の誤謬 に通じる語用法である．\n\n\n1.3 独立性の検定\n\n\n\n\n\n\n問題\n\n\n\n次の条件を持つ学習教材が「必ず英語の成績が上がる」と言えるだろうか？\n\n全国共通模試で，英語の全国平均点は58点であった．\n当該教材を用いて勉強した者80名の平均は70点であり，標準偏差は15であった．\n\n\n\nこれは，当該教材を用いた群と用いていない群という「２つの標本」の間に差がないこと，今回では「平均が同じであること」を帰無仮説として，目下の証拠からこれを棄却出来るかを検定する問題として捉えることができる．\n\n\n1.4 区間推定\n\n\n1.5 ベイズ統計学\n\n伝統的統計学は客観確率を用いているので，母数の確からしさを１つの数値として示すことができない．伝統的統計学が示しうるものは，「母数がある範囲内にあればこの証拠が現れる確率はいくらであるか」でしかないのである．この矛盾をいかに克服するかは法律家と統計学者が共同して取り組むべき今後の課題であるが，１つの可能性として考えうることは伝統的統計学に代えてベイズ統計学の手法を用いることである．(草野耕一, 2016, p. 119)"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#定義集",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#定義集",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "2 定義集",
    "text": "2 定義集\n\\[\n[a,b]:=\\left\\{x\\in\\mathbb{R}\\mid a\\le x\\le b\\right\\}\n\\] は実数の区間を表す． \\[\n\\mathbb{N}^+:=\\left\\{1,2,3,\\cdots\\right\\}\n\\] は正の整数全体の集合を表す．詳しくは 本サイトの数学記法一覧 を参照．\n\n2.1 二項分布\n(草野耕一, 2016, p. 92) も参照．\n\n\n\n\n\n\n定義（二項分布，Bernoulli分布）\n\n\n\n\nパラメータ \\(n\\in\\mathbb{N}^+\\) と \\(p\\in[0,1]\\) に関する 二項分布 \\(\\mathrm{Bin}(n,p)\\) とは，集合 \\(\\{0,1,\\cdots,n\\}\\) 上の離散確率分布で， 確率質量関数 \\[\nb(x;n,p):=\\begin{pmatrix}n\\\\x\\end{pmatrix}p^x(1-p)^{n-x},\n\\] \\[\nx=0,1,\\cdots,n,\n\\] が定めるものをいう．\nパラメータ \\(p\\in[0,1]\\) に関する Bernoulli分布 \\(\\mathrm{Ber}(p)\\) とは，\\(n=1\\) の場合の二項分布 \\[\n\\mathrm{Ber}(p):=\\mathrm{Bin}(1,p)\n\\] をいう．\n\n\n\nただし，二項係数 \\(\\begin{pmatrix}n\\\\x\\end{pmatrix}\\) は高校数学では \\({}_nC_x\\) と表す場合が多い．前者の記法の美点は，関係式 \\[\n\\begin{align*}\n    \\begin{pmatrix}n\\\\x\\end{pmatrix}&=\\frac{n!}{x!(n-x)!}\\\\\n    &=\\frac{n}{x}\\frac{(n-1)!}{(x-1)!(n-x)!}\\\\\n    &=\\frac{n}{x}\\begin{pmatrix}n-1\\\\x-1\\end{pmatrix}\n\\end{align*}\n\\] が直感的に表せる点にある．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#footnotes",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(大塚淳, 2020, p. 105)，(草野耕一, 2016, p. 100)↩︎"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html",
    "href": "posts/2023/Surveys/SSM.html",
    "title": "相関粒子系の社会実装",
    "section": "",
    "text": "要は僕の専門分野である訳だが，これが今回のビジネスモデルの「骨格」の部分になる．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n定義 (State Space Model)\n\n\n\n状態空間モデル1とは，状態変数 \\(\\{X_t\\}_{t=0}^T\\) と，観測変数 \\(\\{Y_t\\}_{t=1}^T\\) の組からなる確率過程 \\[\n\\{(X_t,Y_t)\\}\\subset\\mathcal{L}(\\Omega;\\mathcal{X}\\times\\mathcal{Y})\n\\] であって，初期状態 \\(X_0\\) の分布と，\\(X_t,Y_t\\) の間の関係として次の2つの条件付き分布 \\(P_{t},F_t\\)\n\nシステムモデル \\[\nX_{t+1}|X_t\\sim P_{t+1}\\quad(t\\in T)\n\\]\n観測モデル \\[\nY_t|X_t\\sim F_t\\quad(t\\in[T])\n\\]\n\nを想定したものをいう．2\n\n\n\n状態空間モデルの図示（密度を持つ場合）\n\n\n\n\n\n\n\n\n\n\n状態空間モデルの例\n\n\n\n\n\nすなわち，初期状態 \\(X_0\\) の分布のモデル \\(\\{\\mathbb{P}_0^\\theta\\}\\)．Markov過程 \\(\\{X_t\\}\\) の遷移核のモデル \\(\\{P_t^\\theta\\}\\)，観測のモデル \\(\\{F_t^\\theta\\}\\) の3-組 \\((\\mathbb{P}_0^\\theta,P_t^\\theta,F_t^\\theta)\\) を 状態空間モデル という．また，過程 \\(\\{(X_t,Y_t)\\}\\) もMarkov過程になることが示せる．このため，状態空間モデルのことを 部分的に観測されるMarkov過程 (partially observed Markov process) とも表現する．\nその重要なサブクラスには，次のような名前が付いている：\n\n\\(X_0,X_1,\\cdots\\) が離散変数であるとき，特に 隠れ Markov モデル ともいう．\n\\(X_t,Y_t\\) がいずれも Gauss 確率変数で，\\(p_t^\\theta,f_t^\\theta\\) が線型であるとき，線型力学系 または Kalman フィルター ともいう．3\n\n\n\n\nただし，\\(X_t\\) は観測不能で，\\(Y_t\\) のみが観測されるものとする．従って，目標は \\(Y_1,\\cdots,Y_T\\) の値から \\(X_1,\\cdots,X_T\\) の値を推定することである．\n各時点 \\(t\\in[T]\\) において，現在までの観測 \\(Y_1,\\cdots,Y_t\\) から現在の状態 \\(X_t\\) を推定することを考える（フィルタリング問題4）．特に Bayes の枠組みでは，条件付き分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\\quad(t\\in [T])\n\\] を（逐次的に）推定することを目指す．5\nこの状態空間モデルのフィルタリング問題を解くためのアルゴリズムは多く知られているが，そのうち，モデル \\(X_{t+1}|X_t,Y_t|X_t\\) が高度に非線型でも通用する手法は粒子フィルターのみである．6\n粒子フィルターは，\\(X_t\\) の観測に関する事後分布を \\(N\\) 個（大量）の粒子によって近似する Bayes 推定手法で，各 \\(Y_t\\) の尤度の情報を重点リサンプリングによって取り入れながらも，計算コストを抑えながら \\(X_t\\) の事後分布を逐次近似していく．粒子フィルターとは何か？ も参照．\n\n\n\n要は，\\(Y_t\\) を安価に集めて，\\(X_t\\) を高値で売ることを考える．本当にこれがビジネスになるためには，２つの条件\n\n\\(X_t\\) は多くの人がリアルタイムに知りたいが，（少なくともリアルタイムには）知れない\n\\(Y_t\\) をたくさん集めれば \\(X_t\\) を推測できるが，簡単には推測するのに十分な次元の \\(Y_t\\) を用意できない\n\nを満たす必要がある．が，意外とこのようなものは多いかも知れない．\n一方で我々の売りは\n\n\n\n\n\n\n今回のビジネスモデルのコア\n\n\n\nどんなに推定しにくい \\(X_t\\) でも（モデルが複雑で尤度が解析的な表示を持たなくても），十分な情報を含む観測データ \\(Y_t\\) が得られれば，逐次推定できる．\n\n\nということになる．\n時系列データのオンライン推論は，需要が高い一方で実装が難しい．ここの乖離が好機になる可能性がある．"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#粒子ビジネスモデルモデルの基幹技術",
    "href": "posts/2023/Surveys/SSM.html#粒子ビジネスモデルモデルの基幹技術",
    "title": "相関粒子系の社会実装",
    "section": "",
    "text": "要は僕の専門分野である訳だが，これが今回のビジネスモデルの「骨格」の部分になる．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n定義 (State Space Model)\n\n\n\n状態空間モデル1とは，状態変数 \\(\\{X_t\\}_{t=0}^T\\) と，観測変数 \\(\\{Y_t\\}_{t=1}^T\\) の組からなる確率過程 \\[\n\\{(X_t,Y_t)\\}\\subset\\mathcal{L}(\\Omega;\\mathcal{X}\\times\\mathcal{Y})\n\\] であって，初期状態 \\(X_0\\) の分布と，\\(X_t,Y_t\\) の間の関係として次の2つの条件付き分布 \\(P_{t},F_t\\)\n\nシステムモデル \\[\nX_{t+1}|X_t\\sim P_{t+1}\\quad(t\\in T)\n\\]\n観測モデル \\[\nY_t|X_t\\sim F_t\\quad(t\\in[T])\n\\]\n\nを想定したものをいう．2\n\n\n\n状態空間モデルの図示（密度を持つ場合）\n\n\n\n\n\n\n\n\n\n\n状態空間モデルの例\n\n\n\n\n\nすなわち，初期状態 \\(X_0\\) の分布のモデル \\(\\{\\mathbb{P}_0^\\theta\\}\\)．Markov過程 \\(\\{X_t\\}\\) の遷移核のモデル \\(\\{P_t^\\theta\\}\\)，観測のモデル \\(\\{F_t^\\theta\\}\\) の3-組 \\((\\mathbb{P}_0^\\theta,P_t^\\theta,F_t^\\theta)\\) を 状態空間モデル という．また，過程 \\(\\{(X_t,Y_t)\\}\\) もMarkov過程になることが示せる．このため，状態空間モデルのことを 部分的に観測されるMarkov過程 (partially observed Markov process) とも表現する．\nその重要なサブクラスには，次のような名前が付いている：\n\n\\(X_0,X_1,\\cdots\\) が離散変数であるとき，特に 隠れ Markov モデル ともいう．\n\\(X_t,Y_t\\) がいずれも Gauss 確率変数で，\\(p_t^\\theta,f_t^\\theta\\) が線型であるとき，線型力学系 または Kalman フィルター ともいう．3\n\n\n\n\nただし，\\(X_t\\) は観測不能で，\\(Y_t\\) のみが観測されるものとする．従って，目標は \\(Y_1,\\cdots,Y_T\\) の値から \\(X_1,\\cdots,X_T\\) の値を推定することである．\n各時点 \\(t\\in[T]\\) において，現在までの観測 \\(Y_1,\\cdots,Y_t\\) から現在の状態 \\(X_t\\) を推定することを考える（フィルタリング問題4）．特に Bayes の枠組みでは，条件付き分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\\quad(t\\in [T])\n\\] を（逐次的に）推定することを目指す．5\nこの状態空間モデルのフィルタリング問題を解くためのアルゴリズムは多く知られているが，そのうち，モデル \\(X_{t+1}|X_t,Y_t|X_t\\) が高度に非線型でも通用する手法は粒子フィルターのみである．6\n粒子フィルターは，\\(X_t\\) の観測に関する事後分布を \\(N\\) 個（大量）の粒子によって近似する Bayes 推定手法で，各 \\(Y_t\\) の尤度の情報を重点リサンプリングによって取り入れながらも，計算コストを抑えながら \\(X_t\\) の事後分布を逐次近似していく．粒子フィルターとは何か？ も参照．\n\n\n\n要は，\\(Y_t\\) を安価に集めて，\\(X_t\\) を高値で売ることを考える．本当にこれがビジネスになるためには，２つの条件\n\n\\(X_t\\) は多くの人がリアルタイムに知りたいが，（少なくともリアルタイムには）知れない\n\\(Y_t\\) をたくさん集めれば \\(X_t\\) を推測できるが，簡単には推測するのに十分な次元の \\(Y_t\\) を用意できない\n\nを満たす必要がある．が，意外とこのようなものは多いかも知れない．\n一方で我々の売りは\n\n\n\n\n\n\n今回のビジネスモデルのコア\n\n\n\nどんなに推定しにくい \\(X_t\\) でも（モデルが複雑で尤度が解析的な表示を持たなくても），十分な情報を含む観測データ \\(Y_t\\) が得られれば，逐次推定できる．\n\n\nということになる．\n時系列データのオンライン推論は，需要が高い一方で実装が難しい．ここの乖離が好機になる可能性がある．"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#どんな-x_t-が売れるか",
    "href": "posts/2023/Surveys/SSM.html#どんな-x_t-が売れるか",
    "title": "相関粒子系の社会実装",
    "section": "2 どんな \\(X_t\\) が売れるか？",
    "text": "2 どんな \\(X_t\\) が売れるか？\n\n2.1 マクロ指標のナウキャスト\n最も示唆的と思われる例は，\\(X_t\\) としてGDP，商業販売額などのマクロ指標を取った場合だと思われる．\nマクロ指標は，各企業単体では推測できず，たとえ業界を絞っても各企業の売り上げデータやATM取引データなど，多くのデータを集めて高次元な \\(Y_t\\) を構成しなければ，信頼できる \\(X_t\\) の推定はできないだろう．高次元な \\(Y_t\\) から \\(X_t\\) をフィルタリング際の粒子法は安定せず，現在でも解決されていないオープンクエスチョンである．必然的にブルーオーシャンで誰も参入できない．\nさらに，マクロ指標はフィルタリングすること＝今現在の値を知ることに意味がある．理論的な障壁や技術的な障壁は高いが，経営判断に使ったり，投資判断に使ったり，需要は大きいと思われる．\n\\(Y_t\\) はデータとして広く流通しているわけではないならば（ATM利用データなど），技術力だけでなく，「信頼を得てデータを提供してもらっている」ことが競争力に加わっていき得る．\n\n\n2.2 天気予報\n\\(Y_t\\) が天気（降水量）というのがよくある．\\(X_t\\) が高次元になり，データ同化の問題になる．\nこの天気予報とデリバティブとの関係は？\n\n\n2.3 属人化医療\n個人的には，\\(X_t\\) は個人の体調スコア（あるいは特定の病気のリスク）で，\\(Y_t\\) がApple Watchなどのスマートデバイスからの心拍や体温や移動距離などの測定データ，という属人化医療の場面設定をよく考える．"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#サーベイ",
    "href": "posts/2023/Surveys/SSM.html#サーベイ",
    "title": "相関粒子系の社会実装",
    "section": "3 サーベイ",
    "text": "3 サーベイ\nまとめると，我々が提供できるものは高次元・大規模状態空間モデルの逐次推定手法の研究開発力．足りないものは研究成果と実装する時間と仲間と交渉力である．その代わり初期投資は極めて少なくて済む．\nまずは，前節での「\\(Y_t\\) を集めて \\(X_t\\) を売る」ビジネスモデルの実現に向けて，既存の成功事例を調べる．\n\n3.1 生態学\n生態学のモデリングにも状態空間モデルはよく使われる (Auger-Méthé et al., 2021)．特に非線型性が強く，粒子フィルターが有効になる (Chopin and Papaspiliopoulos, 2020, p. 19)．\n\n\n3.2 ナウキャスト\n元々は星野研究室の次の研究を知って，新里さんに紹介したときに得た着想であった．\nCard\n「ナウキャスト」「オルタナティブデータを活用した経済分析」と言った言葉でビジネス界で議論されているようだ．特に「ナウキャスト」という名前の会社はこの分野を開拓している．\nナウキャストとエム・データ、機関投資家向けオルタナティブデータ活用で協業\nオルタナティブデータを用いた経済活動分析"
  },
  {
    "objectID": "posts/2023/Surveys/SSM.html#footnotes",
    "href": "posts/2023/Surveys/SSM.html#footnotes",
    "title": "相関粒子系の社会実装",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nここでの定義は (Chopin and Papaspiliopoulos, 2020, p. 42) に倣った．隠れMarkovモデルともいうが，こう言ったときは状態空間が有限集合であるという制約が暗につく．↩︎\n他の依存関係は仮定しないので，その図示は DAG となっている．状態空間モデルは Bayesian Network の例である．記法 \\(T,[T]\\) については 本サイトの数学記法一覧 を参照．↩︎\n(Bishop and Bishop, 2024, p. 353) での用語．↩︎\n一方で \\(Y_t\\) から，未来の値 \\(Y_{t+1}\\) を予測する問題を「予測問題」，\\(Y_{1},\\cdots,Y_t\\)から，過去の状態変数の値 \\(X_{s}\\;(s&lt;t)\\) を推定する問題を「平滑化問題」という．↩︎\n記法 \\(\\mathcal{L}\\) については 本サイトの数学記法一覧 を参照．ベイズ手法については ベイズ計算とは何か を参照．↩︎\n\\(Y_t|X_t,X_{t+1}|X_t\\) の線型性が高い場合は，Kalman filter とその変種，特に Extended Kalman filter の効率も良い場合が多い．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#schrödinger-橋によるサンプリング-dsb-gs",
    "href": "posts/2024/Samplers/Schrodinger.html#schrödinger-橋によるサンプリング-dsb-gs",
    "title": "Schrödinger 橋",
    "section": "5 Schrödinger 橋によるサンプリング (DSB-GS)",
    "text": "5 Schrödinger 橋によるサンプリング (DSB-GS)\n\n5.1 導入\n全く同様にして，Schrödinger 橋としての見方を導入することにより，DDGS の効率はさらに上げられる．\n加えて，無雑音極限において，Schrödinger 橋問題は，エントロピー正則化を持つ最適輸送問題と Monge-Kantorovich 問題と関連がある (De Bortoli et al., 2021, p. 3.1節)．\nこの場合も，\\(T\\to\\infty\\) の極限において，DDGS は Schrödinger 橋の近似を与える．\n\n\n5.2 Schrödiger-Föllmer サンプラー\n\\(\\mathbb{M}\\) を OU 過程と取る代わりに，\\(\\Pi_T(x_T)\\) を Dirac 測度として Brown 橋を取ることもできる．これが (Föllmer, 1985) 以来のアプローチである．\nこのアプローチでは，IPF は２回のイテレーションで収束するという美点がある．このための数値的方法も広い分野で提案されている：(Barr et al., 2020), (Zhang et al., 2021)．\n終端の測度を Dirac 測度としていることの綻びが数値的な不安定性に現れやすいことが (Vargas et al., 2023) で述べられている．"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#sec-DDPS",
    "href": "posts/2024/Samplers/Schrodinger.html#sec-DDPS",
    "title": "Schrödinger 橋",
    "section": "2 雑音除去拡散による事後分布サンプリング (DDPS)",
    "text": "2 雑音除去拡散による事後分布サンプリング (DDPS)\n\n2.1 雑音除去拡散 (DD)\n拡散模型 は，次で定まる OU 過程によってデータ分布を \\(\\mathrm{N}_d(0,I_d)\\) にまで破壊しているとみなせる (Y. Song et al., 2021)： \\[\ndX_t=-\\frac{1}{2}X_t\\,dt+dB_t,\\qquad X_0\\sim p(x|y).\n\\]\nただし，この過程は指数エルゴード性を持つと言っても，完全に \\(\\mathrm{N}_d(0,I_d)\\) に従うようになるのは \\(t\\to\\infty\\) の極限においてである．この極限においては，\\(p(x|y)\\) はもやは \\(y\\) に依らなくなる．\nこの \\((X_t)\\) の有限時区間 \\([0,T]\\) における時間反転は，\\((X_t)\\) の密度を \\(p_t(x_t|y)\\) で表すと， \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log p_{T-t}(Z_t|y)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T|y),\n\\tag{1}\\] の弱解になる (Anderson, 1982), (Haussmann and Pardoux, 1986)．この \\((Z_t)_{t\\in[0,T]}\\) を 雑音除去拡散 (Denoising Diffusion) という．\n\n\n2.2 \\((Z_t)\\) からのサンプリング\nすると残りの問題は，拡散過程 \\((Z_t)_{t\\in[0,T]}\\) からのサンプリングになるが，これは \\(\\log p_{T-t}(Z_t|y)\\) という項の評価と \\(p_T(x_T|y)\\) からのサンプリングが必要である．\n\\((Z_t)\\) を \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+s_{T-t}^\\theta(Z_t,y)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] で近似することが (Y. Song et al., 2021) の方法である．思い切って \\(\\mathrm{N}_d(0,I_d)\\approx p_T(x_T|y)\\) としてしまい，\\(s_t^\\theta(x_t,y)\\) のモデリングに特化するのである．\nこの過程 \\((Z_t)\\) が定める測度を \\(\\mathbb{Q}_y^\\theta\\in\\mathcal{P}(C([0,T];\\mathcal{X}))\\) と表すと，訓練目標は KL 乖離度の期待値 \\[\\begin{align*}\n    \\mathcal{L}(\\theta)&:=2\\operatorname{E}\\biggl[\\operatorname{KL}\\biggr(\\mathbb{P}_Y,\\mathbb{Q}_Y^\\theta\\biggl)\\biggr]\\\\\n    &=\\int^T_0\\operatorname{E}\\biggl[\\left\\|s^\\theta_t(X_t,Y)-\\nabla_x\\log p_{t|0}(X_t|X_0)\\right\\|^2\\biggr]\\,dt+\\mathrm{const.}\n\\end{align*}\\] が考えられる．ただし，\\(\\mathbb{P}_Y\\) は \\((X_t)\\) の分布，\\(p_{t|0}\\) は \\((X_t)\\) の遷移密度を表す．この損失は DSM (Vincent, 2011) で与えられたものに等しい．\n\\[\n(X_0,Y)\\sim p(x,y)=g(y|x)\\mu(x)\n\\] からのシミュレーションが可能であるならば，この目的関数は確率的最適化アルゴリズムによって最適化できる．\nこうして，雑音除去拡散サンプラー (DDPS: Denoising Diffusion Posterior Sampler) を得る．\n\n\n2.3 近似ベイズ計算への応用\n事前分布と尤度 \\(g(y|x)\\) からのサンプリングが可能な状況は，生成モデリングの他に Simulation-based Inference などの近似推論でもあり得る．\n実際，この DDPS は従来の ABC (Approximate Bayesian Computation) 法の代替になり得る．\nさらに，拡散模型の加速法 （Progressive Distillation (Salimans and Ho, 2022) など）が DDPS にも応用可能である．\n\n\n2.4 逆問題への応用\nサンプルが画像だとしても，画像修復 (inpainting) や高解像度化 (super-resolution) などの逆問題応用が豊富に存在する．\nこのような，単一の \\(Y=y\\) を固定した状況で潜在変数 \\(X_T\\) からサンプリングをしたい場合では，\\(\\log p_t(x_t|y)\\) を一緒くたに \\(s^\\theta_{t}(x_t,y)\\) に取り替えてしまうのではなく，次の事前分布と尤度への分解に基づいて扱うこともできる：\n\\[\n\\nabla_x\\log p_t(x_t|y)=\\nabla_x\\log\\mu_t(x_t)+\\nabla_x\\log g_t(y|x_t),\n\\] \\[\n\\mu_t(x_t):=\\int_\\mathcal{X}\\mu(x_0)p_{t|0}(x_t|x_0)\\,dx_0,\\qquad g_t(y|x_t):=\\int_\\mathcal{X}g(y|x_0)p_{0|t}(x_0,x_t)\\,dx_0.\n\\]\nこの第一項は \\(s_t^\\theta(x_t)\\) により統一的にモデリングでき，同様に \\(X_0\\sim\\mu(x)\\) から始まる雑音化過程 \\((X_t)\\) の分布を \\(\\mathbb{P}\\) として \\(\\operatorname{KL}(\\mathbb{P},\\mathbb{Q}^\\theta)\\) 最小化問題として処理できる．\n\\(g_t(y|x_t)\\) の項も近似可能である．(Chung et al., 2023) では条件付き誘導が，(J. Song et al., 2023) では Monte Carlo 法が用いられている．"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#sec-DSB-PS",
    "href": "posts/2024/Samplers/Schrodinger.html#sec-DSB-PS",
    "title": "Schrödinger 橋",
    "section": "3 Schrödinger 橋による事後分布サンプリング (DSB-PS)",
    "text": "3 Schrödinger 橋による事後分布サンプリング (DSB-PS)\n\n3.1 導入\n\\(p_T(x_T|y)\\approx\\mathrm{N}_d(0,I_d)\\) の近似を成り立たせるために \\(T\\) を十分大きく取る必要がある問題は，OU 過程の代わりに Schrödinger 橋を用いることで解決できることが (Shi et al., 2022) で提案された．\nSchrödinger 橋自体は，(De Bortoli et al., 2021) などから拡散模型への応用は議論されていた．\n\n\n3.2 定義\nSchrödinger 橋 (SB) とは， \\[\n\\Pi^*:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}_0}\\operatorname{KL}(\\Pi,\\mathbb{P}),\n\\] \\[\n\\mathcal{P}_0:=\\biggl\\{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))\\,\\bigg|\\,\\Pi_0(x_0,y_0)=p(x_0,y_0),\\Pi_T(x_T,y_T)=\\mathrm{N}_d(0,I_d)p(y_T)\\biggr\\},\n\\] によって定まる確率分布に従う確率過程をいう．ただし，\\(\\mathbb{P}:=\\mathbb{P}_{y_0}\\otimes\\delta_{p(y)}\\) とした．\\(\\delta_{p(y)}\\) は次で定まる確率分布である： \\[\ndY_t=0,\\qquad Y_0\\sim p(y).\n\\]\nこれは表示 \\[\n\\Pi^*=\\mathbb{P}^*_{y_0}\\otimes\\delta_{p(y)}\n\\] を持つから，\\(Z_0\\sim\\mathrm{N}_d(0,I_d)\\) に従う過程 \\((Z_t)\\) をシミュレーションすることで， \\[\nZ_T\\sim\\Pi^*_0(x|y)=p(x_0|y)\\qquad p(y)\\text{-a.s.}\n\\] が成り立つ．\n\n\n3.3 SB のシミュレーション\nSB 問題の解 \\(\\Pi\\) は 逐次的比例フィッティング (IPF: Iterative Proportional Fitting) により得られる．\n\n3.3.1 IPF とは\nIPF アルゴリズムは離散的な形で (Fortet, 1940) が Schrödinger 方程式の研究で，(Deming and Stephan, 1940) が分割表データ解析の研究で提案している．その手続きw (Ireland and Kullback, 1968) が距離の最小化として特徴付け，(Kullback, 1968) が確率密度に対しても一般化した．\nIPF は元々，指定した２つの確率ベクトル \\(r\\in(0,\\infty)^{d_r},c\\in(0,\\infty)^{d_c}\\) を周辺分布に持つ結合分布（カップリング）のうち，指定の行列 \\(W\\in M_{d_rd_c}(\\mathbb{R}_+)\\) に最も近い KL 乖離度を持つカップリングを見つけるための逐次アルゴリズムである (Kurras, 2015)．\n種々の分野で再発見され，複数の名前を持っているようである．例：Sheleikhovskii 法，Kruithof アルゴリズム，Furness 法，Sinkhorn-Knopp アルゴリズム，RAS 法など (Kurras, 2015)．\n\\(W\\) の成分が正である場合は，(Sinkhorn, 1967) がアルゴリズムの収束と解の一意性を示している．1\nしかし，\\(W\\) の成分が零を含む場合，零成分の位置に依存してアルゴリズムは収束しないことがあり得ることを，(Sinkhorn and Knopp, 1967) が \\(d_r=d_c=1\\) の場合について示している．\n\n\n3.3.2 アルゴリズム\nIPF アルゴリズムは，観念的には，２つの周辺分布のうち片方を制約に課しながら，KL 距離を最小にする射影を返していく：\n\\[\n\\Pi^{2n+1}:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))}\\biggl\\{\\operatorname{KL}(\\Pi,\\Pi^{2n})\\,\\bigg|\\,\\Pi_T=\\mathrm{N}_d(0,I_d)\\otimes p(y_T)dy_T\\biggr\\},\n\\] \\[\n\\Pi^{2n+2}:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))}\\biggl\\{\\operatorname{KL}(\\Pi,\\Pi^{2n+1})\\,\\bigg|\\,\\Pi_0(x_0,y_0)=p(x_0,y_0)\\biggr\\}.\n\\]\n今回の場合， \\[\n\\Pi^{2n+1}=\\mathbb{P}_{y_T}^{2n+1}\\otimes\\delta_{p(y)},\\qquad\\Pi^{2n+2}=\\mathbb{P}^{2n+2}_{y_0}\\otimes\\delta_{p(y)},\n\\] と分解される．ただし，\\(\\mathbb{P}_{y_T}^{2n+1}\\) は次で定まる \\((Z_t)\\) の時間反転 \\[\ndZ_t=f_{T-t}^{2n+1}(Z_t,y_T)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),f_t^{2n+1}(x_t,y):=-f_t^{2n}(x_t,y)+\\nabla_{x}\\log\\Pi^{2n}_t(x_t|y),\n\\] \\(\\mathbb{P}_{y_0}^{2n+2}\\) は次で定まる \\((X_t)\\) の経路測度となる： \\[\ndX_t=f^{2n+2}_t(X_t,y_0)\\,dt+dB_t,\\qquad X_0\\sim p(x|y_0)\\,dx,f_t^{2n+2}(x_t,y):=-f_t^{2n+1}(x_t,y)+\\nabla_x\\log\\Pi_t^{2n+1}(x_t|y).\n\\] ただし，\\(f^0_t(x_t)=-x_t/2\\)．\n\n\n3.3.3 DDPS との関係\n最初のイテレーション \\(n=0\\) における \\(\\mathbb{P}^1_y\\) が雑音除去拡散 (1) に対応する．\nしかし，IPF アルゴリズムのイテレーションを繰り返していくごとに，\\(T&gt;0\\) が十分に大きくない場合でも正確に \\(\\mathrm{N}_d(0,I_d)\\) にデータ分布を還元する SB が得られるようになっていく．\n\n\n3.3.4 DSB-PS\nこの際，スコア \\(\\nabla_z\\log p_{T-t}(Z_t|y)\\) から始まり，\\(f^{n}_t\\;(n\\ge2)\\) の推定も逐次的に行なわなければならない点については，mean-matching (De Bortoli et al., 2021), (Shi et al., 2022) という方法が考えられている．\nこの方法を用いて，IPF アルゴリズムが収束するまで実行して最終的に得るサンプラーを Schrödinger 橋サンプラー (DSB-PS: Diffusion Schrödinger Bridge Posterior Sampling) という．"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#sec-DDGS",
    "href": "posts/2024/Samplers/Schrodinger.html#sec-DDGS",
    "title": "Schrödinger 橋",
    "section": "4 雑音除去拡散によるサンプリング (DDGS)",
    "text": "4 雑音除去拡散によるサンプリング (DDGS)\n\n4.1 導入\n(Vargas et al., 2023) は次の２点を克服するサンプラーを提案した．\n\n4.1.1 条件付き生成\nDDPS と DSB-PS は，\\(y\\in\\mathcal{Y}\\) について一様に均してしまった 償却推論 を行っている．\n特殊な \\(y\\in\\mathcal{Y}\\) に対しても，これにフィットしたモデルを作りたい状況がある．\n\n\n4.1.2 正規化定数のわからない分布\n同時に DDPS と DSB-PS は，正規化定数の不明な分布などからのサンプリングには使えない．\nここでは， \\[\np(x)=\\frac{\\gamma(x)}{Z},\\qquad Z:=\\int_\\mathcal{X}\\gamma(x)\\,dx\n\\] という形で，\\(\\gamma\\) のみを与えられた場合を考える．\nDDPS において第 2.2 節で考えたように，\\((X_0,Y)\\) からのサンプルが得られないため，\\(\\nabla_x\\log p_t(x_t)\\) の項の近似に関しては別のアプローチを考える必要がある．\n\n\n\n4.2 \\(h\\)-変換としての表示\n雑音除去拡散 (1) の \\(\\nabla_x\\log p_t(x_t)\\) の表示が消えるような変数変換を考える．\nまず，OU 過程 \\((X_t)\\) を定常分布 \\(X_0\\sim\\mathrm{N}_d(0,I_d)\\) から始めた場合の分布を \\(\\mathbb{M}\\) とすると，この逆は \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] である．この過程の \\(\\mathbb{M}\\) の下での \\(h\\)-変換は， \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log h_{T-t}(Z_t)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T),\n\\] \\[\nh_t(x_t):=\\int_\\mathcal{X}\\Phi(x_0)m_{T|T-t}(x_0|x_t)\\,dx_0,\\qquad \\Phi(x_0):=\\frac{p(x_0)}{\\phi_d(x_0;0,I_d)}\n\\] と表せる．ただし \\(m\\) は OU 過程 \\((X_t)\\) の遷移密度とした．\nこの表示に対するパラメトリックな近似 \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+u^\\theta_{T-t}(Z_t)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] の分布を \\(\\mathbb{Q}^\\theta\\) で表し，\\(\\operatorname{KL}(\\mathbb{P},\\mathbb{Q}^\\theta)\\) を最小化することが最初に思いつくが，これでは \\(\\mathbb{P}\\) からのサンプル，従って \\(p\\) からのサンプルを必要としてしまう．\n\n\n4.3 逆 KL-乖離度の最適制御\n\\(h\\)-変換をした理由は，\\(\\operatorname{KL}(\\mathbb{Q}^\\theta,\\mathbb{P})\\) ならば計算できる点にある．\n\\[\n\\mathcal{L}(\\theta):=\\operatorname{KL}(\\mathbb{Q}^\\theta,\\mathbb{P})=\\operatorname{E}_{\\mathbb{Q}^\\theta}\\left[\\frac{1}{2}\\int^T_0\\|u^\\theta_{T-t}(Z_t)\\|^2\\,dt-\\log\\Phi(Z_T)\\right]\n\\] については，\\(\\log\\Phi(Z_T)\\) には \\(\\theta\\) が出現しないため，第一項のみに集中すれば良い．\nそうすると，これは KL 最適制御問題として解くことができる．"
  },
  {
    "objectID": "posts/2024/Samplers/Schrodinger.html#sec-DSB-GS",
    "href": "posts/2024/Samplers/Schrodinger.html#sec-DSB-GS",
    "title": "Schrödinger 橋",
    "section": "5 Schrödinger 橋によるサンプリング (DSB-GS)",
    "text": "5 Schrödinger 橋によるサンプリング (DSB-GS)\n\n5.1 導入\n全く同様にして，Schrödinger 橋としての見方を導入することにより，DDGS の効率はさらに上げられる．\n加えて，無雑音極限において，Schrödinger 橋問題は，エントロピー正則化を持つ最適輸送問題と Monge-Kantorovich 問題と関連がある (De Bortoli et al., 2021, p. 3.1節)．\nこの場合も，\\(T\\to\\infty\\) の極限において，DDGS は Schrödinger 橋の近似を与える．\n\n\n5.2 Schrödiger-Föllmer サンプラー\n\\(\\mathbb{M}\\) を OU 過程と取る代わりに，\\(\\Pi_T(x_T)\\) を Dirac 測度として Brown 橋を取ることもできる．これが (Föllmer, 1985) 以来のアプローチである．\nこのアプローチでは，IPF は２回のイテレーションで収束するという美点がある．このための数値的方法も広い分野で提案されている：(Barr et al., 2020), (Zhang et al., 2021)．\n終端の測度を Dirac 測度としていることの綻びが数値的な不安定性に現れやすいことが (Vargas et al., 2023) で述べられている．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#有界測度の基本概念",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#有界測度の基本概念",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "1 有界測度の基本概念",
    "text": "1 有界測度の基本概念\n\n\n\n\n\n\n定義1 (\\(\\mu\\)-inner regular, Radon, tight, regular)\n\n\n\n\\(X\\) を位相空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界な符号付き Borel 測度とする．2\n\nBorel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-内部正則 であるとは， \\[\n\\lvert\\mu\\rvert(B)=\\sup_{K\\overset{\\textrm{cpt}}{\\subset}B}\\lvert\\mu\\rvert(K)\n\\] を満たすことをいう．すなわち，任意の \\(\\epsilon&gt;0\\) に対して，あるコンパクト部分集合 \\(K\\overset{\\textrm{cpt}}{\\subset}B\\) が存在して， \\[\n\\lvert\\mu\\rvert(B\\setminus K)&lt;\\epsilon\n\\] を満たすことをいう．3\n\\(\\mu\\) が Radon であるとは，任意の Borel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-内部正則であることをいう．4\n\\(\\mu\\) が 緊密 であるとは，全体集合 \\(X\\) が \\(\\mu\\)-内部正則であることをいう．5\n\\(\\mu\\) が 正則 であるとは，任意の \\(\\epsilon&gt;0\\) に対して，ある閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) が存在して，\\(F\\subset B\\) かつ \\[\nB\\setminus F\\in\\mathcal{B}(X),\\quad\\lvert\\mu\\rvert(B\\setminus F)&lt;\\epsilon\n\\] を満たすことをいう．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#riesz-正則性",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#riesz-正則性",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "2 Riesz 正則性",
    "text": "2 Riesz 正則性\n\n\n\n\n\n\n変種\n\n\n\n\\(X\\) を位相空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界な符号付き Borel 測度とする．\n\nBorel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-外部正則 であるとは， \\[\n\\lvert\\mu\\rvert(B)=\\inf_{B\\subset U\\overset{\\mathrm{open}}{\\subset}X}\\lvert\\mu\\rvert(U)\n\\] を満たすことをいう．6\n\\(\\mu\\) が Riesz 正則 であるとは，任意の Borel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-外部正則かつ \\(\\mu\\)-内部正則であることをいう．\n\nこの Riesz 正則という語用法は筆者限りのものである．(Halmos, 1950, p. 224), (Dunford and Schwartz, 1958, p. 137), (Folland, 1984, p. 205), (Lang, 1993, p. 265), (Conway, 2007, p. 380) などでは単にこれを regular と呼ぶ．\\(X\\) が局所コンパクト Hausdorff 空間であるとき，このような語用法の方が一般的である．\nさらに，上述のうち４文献で共通するように，非有界な符号付き測度 \\(\\mu\\in\\mathcal{S}(X)\\) を考える際は，最低限次の条件を課し，これも regular であるための条件に入れる：\n\n\\(\\mu\\) が 局所有界 であるとは，任意の \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) 上で有限値であることをいう．\n\n\n\n\n\n\n\n\n\n命題7 ：内部と外部の正則性\n\n\n\n\\(X\\) を局所コンパクト Hausdorff 空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界 Borel 測度とする．次は同値：\n\n\\(\\mu\\) は Riesz 正則である．\n任意のコンパクト集合は \\(\\mu\\)-外部正則である．\n任意の有界な開集合は \\(\\mu\\)-内部正則である．\n\nまた，\\(X\\) 上の任意の Baire 測度は Riesz 正則である．\n\n\n\n\n\n\n\n\n定理8 ：Riesz 正則測度の延長 (Alexandroff, 1940, p. 590)\n\n\n\n\\(X\\) をコンパクト空間，\\(\\mathcal{A}\\subset P(X)\\) を集合体，\\(\\mu:\\mathcal{A}\\to\\mathbb{C}\\) を Riesz 正則で有界な有限加法的関数とする．このとき，\\(\\mu\\) は \\(\\sigma\\)-加法的である．特に，\\(\\sigma(\\mathcal{A})\\) 上へのただ一つの \\(\\sigma\\)-加法的な延長を持ち，引き続き Riesz 正則である．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#footnotes",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#footnotes",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bogachev, 2007, pp. 68–69) 定義7.1.1, 7,1,4 と (Dudley, 2002, p. 224) に倣った．↩︎\n有界でない一般の符号付き測度に関しては，任意の \\(X\\) のコンパクト集合上で有限値であることを Borel 測度たる条件に加えることもある，例えば (Halmos, 1950, p. 223) 52節．↩︎\n(Dudley, 2002, p. 224) では単に regular と呼んでいるが，(Halmos, 1950, p. 224) に従って inner regular と呼ぶことにした．↩︎\n(Halmos, 1950, p. 224) ではこの条件を満たす \\(\\mu\\) を 正則 と呼んだ．↩︎\n(Dudley, 2002, p. 434) によると，最初の tight の定義は (Le Cam, 1957) による uniformly tight の定義であったようである．一点集合 \\(\\{P\\}\\) が一様に緊密であることと \\(P\\) が緊密であることとは同値になる．↩︎\n(Halmos, 1950, p. 224) に倣った．↩︎\n(Halmos, 1950, p. 228) 定理X.52.F, G．↩︎\n(Dunford and Schwartz, 1958, p. 138) 定理III.5.13, 定理III.5.14↩︎"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html",
    "href": "posts/2023/Processes/BranchingProcesses.html",
    "title": "分岐過程",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html#離散分岐過程",
    "href": "posts/2023/Processes/BranchingProcesses.html#離散分岐過程",
    "title": "分岐過程",
    "section": "1 離散分岐過程",
    "text": "1 離散分岐過程\n\n1.1 定義\n\n\n\n\n\n\n定義1 ：Bienaymé-Galton-Watson 過程 (Bienaymé, 1845), (Watson and Galton, 1875)\n\n\n\n\\(Y_1,Y_2,\\cdots\\) を \\(\\mathbb{Z}\\)-値確率変数の列とし，\\(\\mu\\in\\mathcal{P}(\\mathbb{Z})\\) を初期分布とする．これに対して \\[\nX_0\\sim\\mu,\\quad X_{n+1}:=1_{\\left\\{X_n\\ge 1\\right\\}}\\sum_{j=T_{n-1}+1}^{T_{n-1}+X_n}Y_j,\n\\] \\[\nT_{-1}:=0,\\quad T_n:=\\sum_{j=0}^nX_j\\quad(n\\in\\mathbb{N}),\n\\] と定める．この過程 \\(\\{X_n\\}_{n=0}^\\infty\\) を 分岐過程 といい，分布を \\(\\operatorname{P}_\\mu\\) と表す．\n特に \\(\\{Y_n\\}_{n=1}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{N})\\) が非負で独立同分布である場合，過程 \\(\\{X_n\\}_{n=0}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{N})\\) を Bienaymé-Galton-Watson 分岐過程 という．\n\n\n\\(X_n\\) を第 \\(n\\) 世代の個体数と解釈してみよう．\\(T_n\\) は第 \\(n\\) 世代までを含め，存在し得た全個体数を意味することとなる．\n第 \\(n\\) 世代を構成する \\(X_n\\) 個体がそれぞれ \\(Y_{T_{n-1}+1},\\cdots,Y_{T_{n-1}+X_n}=Y_{T_n}\\) 人の家族を遺して消滅し，これらの子孫が第 \\(n+1\\) 世代 \\(X_{n+1}\\) の構成員となる．\n\\(X_n\\le0\\) となった場合は以降も常に \\(X_m=0\\;(m\\ge n)\\) が成り立ち，\\(0\\) が吸収点となる．\n\\(X_n,Y_n\\) に負の整数値も許す場合は，2つの種族の個体数の差を考える場合などと解釈できる．\n\n\n1.2 乱歩への埋め込み\n初期分布を \\(\\mu=\\delta_1\\) とすると，\\(T_0=X_0=1\\;\\;\\text{a.s.}\\)．すると，任意の \\(m\\in\\mathbb{N}^+\\) について，事象 \\(\\left\\{X_m\\ge 1\\right\\}\\) の上では， \\[\nT_{n}=1+\\sum_{j=1}^{T_{n-1}}Y_j\\quad(0\\le n\\le m+1)\n\\tag{1}\\] \\[\n\\begin{align*}\n    X_{n}&=T_{n}-T_{n-1}\\\\\n    &=1+\\sum_{j=1}^{T_{n-1}}Y_j-T_{n-1}\\\\\n    &=1+\\sum_{j=1}^{T_{n-1}}\\widetilde{Y}_j\\quad(0\\le n\\le m+1)\n\\end{align*}\n\\tag{2}\\] \\[\n\\widetilde{Y}_j:=Y_j-1\\quad(j\\in\\mathbb{N}^+)\n\\] が成り立つ．\nよって，絶滅しないという事象 \\[\n\\bigcap_{m\\in\\mathbb{N}^+}\\left\\{X_m\\ge 1\\right\\}=\\left\\{\\lim_{n\\to\\infty}T_n=\\infty\\right\\}\n\\] の上では，\\(X_0=1\\;\\;\\text{a.s.}\\) から始まる乱歩 (random walk) \\[\nX_n=1+\\widetilde{S}_{T_{n-1}}\\quad(n\\in\\mathbb{N})\n\\] \\[\n\\widetilde{S}_n:=\\sum_{j=1}^n\\widetilde{Y}_j\\quad(n\\in\\mathbb{N})\n\\] と理解できる．\n\n\n1.3 Markov性\n\n\n\n\n\n\n命題2 （Markov性）\n\n\n\n\\(\\{X_n\\}_{n=0}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{Z})\\) を，初期分布 \\(\\mu=\\delta_1\\) と確率変数列 \\(\\{Y_n\\}_{n=1}^\\infty\\subset\\mathcal{L}(\\Omega;\\mathbb{Z})\\) が定める分岐過程，\\(\\mathcal{F}_n:=\\sigma(X_k|k\\in n+1)\\) を \\((X_n)\\) が定める filtration とする．\n\n任意の \\(n\\in\\mathbb{N}^+\\) について \\(\\operatorname{E}_{\\delta_1}[Y_{n+1}|Y_1,\\cdots,Y_n]=a\\) ならば， \\[\n\\operatorname{E}_{\\delta_1}[X_{n+1}|\\mathcal{F}_n]=aX_n^+,\\quad n\\in\\mathbb{N}.\n\\]\n加えて \\(\\mathrm{V}_{\\delta_1}[Y_{n+1}|Y_1,\\cdots,Y_n]=\\sigma^2\\;(n\\in\\mathbb{N}^+)\\) ならば， \\[\n\\mathrm{V}_{\\delta_1}[X_{n+1}|\\mathcal{F}_n]=\\sigma^2X_n^+,\\quad n\\in\\mathbb{N}.\n\\]\nさらに \\(\\{Y_n\\}_{n=1}^\\infty\\) が独立であるならば，\\(\\{(X_n,T_{n-1})\\}_{n=1}^\\infty\\) は Markov 連鎖であるが，\\(\\{X_n\\}_{n=0}^\\infty\\) は Markov 連鎖であるとは限らない．\nさらに \\(\\{Y_n\\}_{n=1}^\\infty\\) が同分布であるならば，\\(\\{X_n\\}_{n=0}^\\infty\\) と \\(\\{(X_n,T_{n-1})\\}_{n=1}^\\infty\\) はいずれも時間的に一様な Markov 連鎖である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n任意の \\(\\{r_k\\}_{k=1}^n\\subset\\mathbb{Z}\\) について，等式 \\[\n\\operatorname{E}_{\\delta_1}[X_{n+1}|X_1=r_1,\\cdots,X_n=r_n]=ar_n^+\n\\] を示せば良い．\\(r_n\\le0\\) の場合は直ちに従うから，\\(r_n\\ge1\\) として考える．この下では変形 式 1 を用いることが出来るから， \\[\n\\begin{align*}\n&\\left\\{\\forall_{j\\in[n]}\\;X_j=r_j\\right\\}\\\\\n&=\\left\\{\\forall_{j\\in[n-1]}\\;T_j=i_j,X_n=r_n\\right\\}\\\\\n&=\\biggl\\{\\forall_{j\\in[n-1]}\\;\\sum_{k=1}^{i_{j-1}}Y_k=i_j-1,\\\\\n&\\qquad\\qquad\\qquad\\sum_{j=i_{n-2}+1}^{i_{n-1}}Y_j=r_n\\biggr\\}\\\\\n&=:B_n\n\\end{align*}\n\\] \\[\ni_j:=1+r_1+\\cdots+r_j,\\quad j\\in[n-1],\n\\] が成り立つから，仮定より \\[\n\\operatorname{E}_{\\delta_1}[Y_{j}1_{B_n}]=a\\operatorname{P}_{\\delta_1}[B_n]\\quad(j\\ge i_{n-1}+1)\n\\] であることに注意して， \\[\n\\begin{align*}\n&\\operatorname{E}_{\\delta_1}[X_{n+1}|X_1=r_1,\\cdots,X_n=r_n]\\\\\n&\\qquad\\cdot\\operatorname{P}_{\\delta_1}[X_1=r_1,\\cdots,X_n=r_n]\\\\\n&=\\operatorname{E}_{\\delta_1}[X_{n+1}1_{\\left\\{X_1=r_1,\\cdots,X_n=r_n\\right\\}}]\\\\\n&=\\operatorname{E}_{\\delta_1}\\left[1_{B_n}\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}Y_j\\right]\\\\\n&=ar_n\\operatorname{P}_{\\delta_1}[B_n]\n\\end{align*}\n\\] を得る．両辺を \\(\\operatorname{P}_{\\delta_1}[B_n]\\) で割ると，初めの式を得る．3\n同様にして， \\[\n\\begin{align*}\n&\\mathrm{V}_{\\delta_1}[X_{n+1}|X_1=r_1,\\cdots,X_n=r_n]\\\\\n&=\\operatorname{E}_{\\delta_1}[X_{n+1}^2|B_n]-\\operatorname{E}_{\\delta_1}[X_{n+1}|B_n]^2\\\\\n&=\\operatorname{E}_{\\delta_1}\\left[\\left(\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}Y_j\\right)^2\\:\\middle|\\:B_n\\right]\\\\\n&\\qquad\\qquad-\\operatorname{E}_{\\delta_1}\\left[\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}Y_j\\:\\middle|\\:B_n\\right]^2\\\\\n&=\\sum_{j=i_{n-1}+1}^{i_{n-1}+r_n}\\biggr(\\operatorname{E}_{\\delta_1}[Y_j^2|B_n]-\\operatorname{E}_{\\delta_1}[Y_j|B_n]^2\\biggl)\\\\\n&=\\sigma^2r_n^+.\n\\end{align*}\n\\] ただし，途中の式変形で，任意の \\(i_{n-1}&lt;j&lt;k\\le i_{n}\\) について \\[\n\\begin{align*}\n\\operatorname{E}_{\\delta_1}[Y_jY_k|B_n]&=\\operatorname{E}_{\\delta_1}\\biggl[Y_j\\operatorname{E}[Y_k|B_n,Y_j]\\:\\bigg|\\:B_n\\biggr]\\\\\n&=\\operatorname{E}_{\\delta_1}[Y_ja|B_n]\\\\\n&=a^2=\\operatorname{E}_{\\delta_1}[Y_j|B_n]\\operatorname{E}_{\\delta_1}[Y_k|B_n]\n\\end{align*}\n\\] が成り立つことを用いた．\n\\((Y_k)_{k=1}^\\infty\\) が独立であるとき，\\((X_k)_{k=0}^\\infty\\) も独立である．これより，任意の時点 \\(m_1&lt;\\cdots&lt;m_k&lt;n\\in\\mathbb{N}\\) について，\\(T_{m_k-1}\\) が与えられた下で，\\(X_k\\;(k\\ge m_k)\\) と \\(\\mathcal{F}_{m_k-1}\\) とは独立である．よって特に，\\((X_{m_k},T_{m_k-1})\\) が与えられた下で，\\((X_n,T_{n-1})\\) と \\((X_{m_i},T_{m_i-1})\\;(i&lt;k)\\) とは独立である．よって，条件付き独立性の性質 より \\[\n\\begin{align*}\n&\\operatorname{P}_{\\delta_1}\\biggl[(X_{n},T_{n-1})=(r_{n},s_{n-1})\\:\\bigg|\\:\\\\\n&\\qquad\\qquad\\forall_{i\\in[k]}\\;(X_{m_i},T_{m_i-1})=(r_{m_i},s_{m_i-1})\\biggr]\\\\\n&=\\operatorname{P}_{\\delta_1}\\biggl[(X_n,T_{n-1})=(r_n,s_{n-1})\\:\\bigg|\\:\\\\\n&\\qquad\\qquad(X_{m_k},T_{m_k-1})=(r_{m_k},s_{m_k-1})\\biggr]\n\\end{align*}\n\\] が成り立つ．\n\n\n\n\n\n\n\n1.4 生存確率\n\n\n\n\n\n\n生存確率\n\n\n\n\\(\\{Z_n\\}_{n=0}^\\infty\\) を BGW 過程で，\\(\\operatorname{P}[Y_1\\in 2]&lt;1\\) とする．このとき，\\(\\mu:=\\operatorname{E}[Y_1]\\in[0,\\infty]\\) について， 4\n\n\\(\\mu\\le1\\) ならば， \\[\n\\operatorname{P}_1[\\forall_{n\\in\\mathbb{N}}\\;Z_n\\ge1]=0.\n\\]\n\\(\\mu&gt;1\\) ならば，ある \\(q\\in[0,1)\\) が存在して \\[\n\\operatorname{P}_1[\\forall_{n\\in\\mathbb{N}}\\;Z_n\\ge1]=1-q&gt;0\n\\] を満たし，\\(q\\in[0,1)\\) は \\(Y_1\\) の確率母関数 \\[\ng(z):=\\sum_{k=0}^\\infty\\operatorname{P}[Y_1=k]z^k\n\\] に関する方程式 \\(g(z)=z\\) の \\([0,1)\\) 上でのただ一つの解である．\n\n\n\nすなわち，「未来永劫絶滅しない」確率が正であるかどうかは \\(Y_1\\) の期待値 \\(\\mu\\) のみに依存するが，その確率 \\(1-q\\) は \\(Y_1\\) の分布によって定まる． 5\n\n\n\n\n\n\n証明"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html#連続分岐過程",
    "href": "posts/2023/Processes/BranchingProcesses.html#連続分岐過程",
    "title": "分岐過程",
    "section": "2 連続分岐過程",
    "text": "2 連続分岐過程\n\n2.1 定義\n\n\n\n\n\n\n定義6 ：連続分岐過程\n\n\n\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}_+)\\) が（連続時間）分岐過程 であるとは， \\[\nQ_t(x,-)*Q_t(y,-)=Q_t(x+y,-)\n\\] を満たす確率核の半群 \\(\\{Q_t\\}_{t\\in\\mathbb{R}_+}\\subset B(\\mathcal{L}_b(\\mathbb{R}_+))\\) が定めるMarkov過程であることをいう．\n\n\nたしかに \\(Q_t(0,-)=\\delta_0\\;(t\\in\\mathbb{R}_+)\\) を満たす．\n\n\n2.2 分岐性\n\n\n\n\n\n\n命題7 （分岐性）\n\n\n\n\\(\\{X_t\\},\\{X'_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}_+)\\) を遷移半群 \\(\\{Q_t\\}\\subset B(\\mathcal{L}_b(\\mathbb{R}_+))\\) を共通とする，互いに独立な分岐過程とする． このとき，\\(\\{X_t+X'_t\\}\\) もやはり \\(\\{Q_t\\}\\) を遷移半群に持つ分岐過程である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(X_0,X_0'\\) の分布をそれぞれ \\(\\nu,\\nu'\\in P(\\mathbb{R})\\) とすると， \\(X_0\\perp\\!\\!\\!\\perp X_0'\\) より， \\(X_0+X_0'\\sim\\nu*\\nu'\\) である． すると，任意の \\(B\\in\\mathcal{B}(\\mathbb{R}_+)\\) に対して， \\[\\begin{align*}\n&\\operatorname{P}[X_t+X_t'\\in B]\\\\\n&=\\operatorname{E}[1_{\\left\\{X_t+X_t'\\in B\\right\\}}]\\\\\n&=\\operatorname{E}\\biggl[\\operatorname{E}[1_{\\left\\{X_t\\in B-x'\\right\\}}|X_t'=x']\\biggr]\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(x,B-x')\\nu(dx)\\operatorname{P}^{X_t'}(dx')\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(x,B-x')\\nu(dx)Q_t(y,dx')\\nu'(dy)\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(x+y,B)\\nu(dx)\\nu'(dy)\\\\\n&=\\int_{\\mathbb{R}_+}\\int_{\\mathbb{R}_+}Q_t(z,B)(\\nu*\\nu')(dz).\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n補題8 （畳み込み測度に関する積分）\n\n\n\n\\(f\\in L(\\mathbb{R}^d,\\nu_1*\\nu_2)\\) について， \\[\n\\int_{\\mathbb{R}^d}f(z)(\\nu_1*\\nu_2)(dz)=\\int_{\\mathbb{R}^d}f(x+y)\\nu_1(dx)\\nu_2(dy).\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nまず \\(f=1_B\\;(B\\in\\mathcal{B}(\\mathbb{R}^d))\\) という定義関数の場合は，定義からすぐに従う．一般の \\(f\\) については単関数近似による．\n\n\n\n\n\n2.3 Feller性\n\n\n\n\n\n\n命題9\n\n\n\n分岐過程 \\(\\{X_t\\}\\) の遷移半群 \\(\\{Q_t\\}\\) は\n\n任意 の\\(x&gt;0,t&gt;0\\) について，\\(Q_t(x,\\{0\\})&lt;1\\)．\n\\(Q_t(x,-)\\overset{\\text{d}}{\\to}\\delta_x(-)\\;(t\\to\\infty)\\)．\n\nを満たすとする．このとき，\\((Q_t)\\) はFeller半群である．\n\n\n\n\n\n\n\n\n証明"
  },
  {
    "objectID": "posts/2023/Processes/BranchingProcesses.html#footnotes",
    "href": "posts/2023/Processes/BranchingProcesses.html#footnotes",
    "title": "分岐過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの一般的な定義は (Quine and Szczotka, 1994) を参考にした．歴史は (Kendall, 1975), (Heyde and Seneta, 1977) が詳しい．(Bienaymé, 1845) が初めに考察をしたが，注目されなかった．(Watson and Galton, 1875) は注目されたが，数学的な議論には誤りが含まれていた．↩︎\n(Quine and Szczotka, 1994, p. 1208) 命題1．↩︎\n条件付き期待値のアトムの上での性質 も参照．↩︎\n(Schinazi, 2014, p. 19) 第2章定理1.1，(Bhattacharya and Waymire, 2021, p. 114) 定理9.1 など．↩︎\nこの \\(\\mu\\) を 基本再生産数 と呼ぶこともある Wikipedia．↩︎\n(Le Gall, 2016, p. 177) を参考にした．↩︎\n(Le Gall, 2016, p. 177) は branching property と呼んでいる．↩︎\n(Applebaum, 2009, p. 22) 命題1.2.2．↩︎\n(Le Gall, 2016, p. 177) 命題6.22．↩︎"
  },
  {
    "objectID": "static/AllCategories.html#sampling",
    "href": "static/AllCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋\n\n\n拡散模型にヒントを得た輸送によるサンプリング法\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nSampling\n\n\nMCMC\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#モデル訓練",
    "href": "posts/2024/Samplers/DDPM.html#モデル訓練",
    "title": "拡散模型の実装",
    "section": "3 モデル訓練",
    "text": "3 モデル訓練\n\nprint(\"Start training DDPMs...\")\nmodel.train()\n\nimport time\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    noise_prediction_loss = 0\n    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        optimizer.zero_grad()\n\n        x = x.to(DEVICE)\n        \n        noisy_input, epsilon, pred_epsilon = diffusion(x)\n        loss = denoising_loss(pred_epsilon, epsilon)\n        \n        noise_prediction_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n    \ntotal_time = time.time() - start_time\nprint(\"Finish!! Total time: \", total_time)\n\n\n\n\n\n\n\n注\n\n\n\n\n\nこの訓練コードも，前述の train_loader の定義が if __name__ == '__main__': と同じ if ブロックに入れる必要がある．\nさもなくば，並列処理が実行できず，訓練には大変な時間がかかる．1\nそこでここでは別の Python ファイルで実行して，結果を読み込むこととする．\n\nmodel.eval()\ngenerated_images = torch.load(\"Files/generated_images.pt\")\n\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_14934/878273190.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  generated_images = torch.load(\"Files/generated_images.pt\")\n\n\nGPU に乗った状態で読み込まれることに注意．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#データ生成",
    "href": "posts/2024/Samplers/DDPM.html#データ生成",
    "title": "拡散模型の実装",
    "section": "4 データ生成",
    "text": "4 データ生成\n\n\n生成データの用意\nmodel.eval()\n\nwith torch.no_grad():\n    generated_images = diffusion.sample(N=inference_batch_size)\n\n\nfor i in range(4):\n    show_image(generated_images, idx=i)\n\n\n\n\n\n\n\n\n\n\n\n(a) 生成された画像\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n図 2\n\n\n\nfor i in range(4, 7):\n    show_image(generated_images, idx=i)\n\n\n\n\n\n\n\n\n\n\n\n(a) 生成された画像\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n図 3\n\n\n\n\nshow_image(generated_images, idx=7)\n\n\n\n\n\n\n\n図 4: 生成された画像"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#モデル評価",
    "href": "posts/2024/Samplers/DDPM.html#モデル評価",
    "title": "拡散模型の実装",
    "section": "5 モデル評価",
    "text": "5 モデル評価\n\ndef draw_sample_image(x, postfix):\n  \n    plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    plt.title(\"Visualization of {}\".format(postfix))\n    plt.imshow(np.transpose(make_grid(x.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))\n\n\ndraw_sample_image(perturbed_images, \"Perturbed Images\")\n\n\n\n\n\n\n\n\n\ndraw_sample_image(generated_images, \"Generated Images\")\n\n\n\n\n\n\n\n\n\ndraw_sample_image(x[:inference_batch_size], \"Ground-truth Images\")"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#footnotes",
    "href": "posts/2024/Samplers/DDPM.html#footnotes",
    "title": "拡散模型の実装",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nkwargs = {'num_workers': 5, 'pin_memory': True, 'prefetch_factor': 2} でも１エポック 12 分以上なので，40 時間以上はかかる．さらにこの場合エポック 18 で RuntimeError: Shared memory manager connection has timed out を得たため，num_workers=0 とせざるを得なかった．↩︎"
  },
  {
    "objectID": "posts/2024/AI/BAI.html",
    "href": "posts/2024/AI/BAI.html",
    "title": "これからはじめるベイズ機械学習",
    "section": "",
    "text": "現在，産業界における “AI” というと専ら，いくつかの限られた巨大 IT 企業が，巨大ニューラルネットワークを最尤推定で学習させ，これを基盤モデルとして公開し，我々一般庶民はそれを有効活用して下流タスクを安価に解くことだけ考えるという営みを指す．\nその産業や生活への破壊的な影響を憂慮しながらも，雨乞いをする日々である．\nAI はそんなものではない．AI はこれにかぎるものではない．\nAI が真に我々の友となり，我々の日常をほんとうに豊かにするは，AI の進歩だけが必要なのではなく，人間との協業が得意になる必要がある．\nそのための第一歩はすでに明らかである．不確実性の定量化 である．\nつまり，「その AI には何が出来て何が出来ないか」「AI の出力がいつ信頼にたるもので，いつ人間の介入が必要であるのか」がわかりやすい形で伝わるコミュニケーション様式をそなえている必要があるのである．1\n筆者の知る限り，ここにある全てのナラティブは現時点では全く広く語られているものではなく，筆者も最初の１年の研究生活を通じて朧げながら見えて来たばかりのものである．\n不確実性の定量化は，機械学習モデルを民主化し，我々の民芸に取り込むための重要な一歩である（のではないだろうか？）．\n本稿はこの発見を共有するために書いた．筆者の反芻不足から，冗長な部分も多いだろうが，少しでも，琴線に触れるものがあれば幸いである．2"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#ベイズ機械学習のすすめ",
    "href": "posts/2024/AI/BAI.html#ベイズ機械学習のすすめ",
    "title": "これからはじめるベイズ機械学習",
    "section": "1 ベイズ機械学習のすすめ",
    "text": "1 ベイズ機械学習のすすめ\n我々が AI をより信頼するためには，何が必要だろうか？\n筆者の考えでは，信頼への第一歩は 不確実性の定量化 が出来るようになることのはずである．\nそしてそのためには ベイズ機械学習 (Bayesian Machine Learning) の発展による本質的解決が必要不可欠である．本稿はこの点を説明するために執筆されたものである．\n筆者に言わせれば，ベイズ機械学習が，今後数年間で AI が経験すべき進展の方向である．この山を越えれば，今まででさえ思っても見なかった未来がひらけてくるだろう．\n\nAlthough considerable challenges remain, the coming decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework. (Ghahramani, 2015, p. 452)\n\n\n1.1 ベイズとは何か？\n機械学習において，確率論的なモデリングに基づいたアプローチを ベイズ機械学習 ともいう．典型的には，モデルの全変数上の結合分布をモデリングし，ベイズ規則によりパラメータのベイズ推定を行う，という手続きからなる．そのため，確率論的アプローチ や モデルベースアプローチ も同義語として用いられる．3\n一方で，頻度論的 という言葉は，よく非ベイズ的アプローチを示す接頭辞として用いられる．典型的には，損失関数を設定し，これを最小化するパラメータを探索することによって実行される．\nこの２つのアプローチは互いに対照的であり，統計学の始まりから基本的な二項対立の図式をなしてきた．\n\n\nContrast of the two main approachs to Machine Learning\n\n\n\n\n\n\n\n\nBayesian\nFrequentist\n\n\n\n\nInference is4\nMarginalization\nApproximation\n\n\nComputational Idea5\nIntegration\nOptimization\n\n\nObjective\nUncertainty Quantification\nRecovery of True Value\n\n\nEmphasis\nModelling\nInference\n\n\n\n\nしかし，機械学習の時代においては，互いの弱みを補間し合う形で発展していくと筆者は考える．特に，現状の推論偏重でモデリング軽視の風潮が，重要な実世界応用の多くを阻んでしまっている．機械学習の世界樹は実は２本あるのである．\n\n\n1.2 ベイズと頻度論との違い\nベイズと頻度論では，確率の解釈も異なるかも知れないが，数学的枠組みとしてはベイズの方が一般的な枠組みであり，また手続き上は，モデリングを重視するか，推論を重視するかの違いでしかない．\n実際，殆どの場合，頻度論的手法はある特定の事前分布を持ったベイズ手法とみなせ，逆も然りである．\nデータから推論を行うには，何らかの仮定が必ず必要であり，それを明示的にモデルに組み込むのがベイズで，推論アルゴリズムにより自動化する精神を持つのが頻度論的手法である．\nその結果，優秀な推論アルゴリズムが日夜驚異的なスピードで提案され，今や機械学習手法は教師あり学習・教師なし学習・強化学習の全てで目覚ましい発展を見た．\nしかし，ベイズと頻度論の２つの柱のバランスを欠いた発展はここまでである．今や，頻度論的な手法を採用した際に，自分たちがどのような仮定を置いたのか全く明瞭な知識を欠いてしまっている．一方で，現実のビッグで複雑なデータを扱うためには，もはや確率的なモデリングを避けては通れない．6\n極めて本質的で強大な敵に対面しつつあるのである．\nだが，現状の病理は明らかであり，頻度論とベイズの手法の間に対応をつけ，足並みを揃えることで次の前進が約束されてる．この意味で，２つの世界樹が必要なのである．\nさらに，ベイズ推論は帰納的推論の確率論的拡張と見れるため，エージェントの合理的な学習と意思決定の最良のモデル（の一つ）と信じられている．7\nしたがって，ベイズ流解釈により手法を理解し，最適化流解釈により手法を実装する．これがあるべき機械学習の未来であると筆者は考える．\n\n\n1.3 ２つの世界樹\n今こそ，この２つの手法は根底では繋がっていることをよく周知し，この２つの視座を往来しながら適材適所に使うことが大事だと筆者は考える．\nしかしそのためには，ベイズ機械学習の発展が遅れている現状を鑑みて，ベイズの手法のより一層の発展と理解の深化が必要である．8\n本章「ベイズ機械学習のすすめ」は，ベイズの手法の特に肝心と思われる３つの側面を指摘して終わる．以下３章を通じて，\n\n第 2 節 ベイズは不確実性を定量化する\nBayes の方が不確実性の定量化が得意であるため，そのような応用先では頻度論的な手法よりも，Bayes バージョンの手法を用いることが出来ると便利である．\n第 3 節 ベイズは分布という共通言語を与える\nBayes による統一的な扱いが理論的に有用である場面が増えている．その際に，Bayes による理論解析と最適化による実際の推論という適材適所の協業が未来の方向であるかも知れない．\n第 4 節 ベイズは理解を促進する\nベイズの手法が敬遠されていた理由も，換言すれば，「事前分布」という得体の知れないものを通じて，理論的深淵と直結するためである．ベイズ手法の研究が理論的な解明を要請する．だからこそ，数学者の魂を持った者がこの途を通ることは人類に大きく資すると筆者は考える．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-uncertainty-quantification",
    "href": "posts/2024/AI/BAI.html#sec-uncertainty-quantification",
    "title": "これからはじめるベイズ機械学習",
    "section": "2 ベイズは不確実性を定量化する",
    "text": "2 ベイズは不確実性を定量化する\n\n2.1 不確実性の定量化の必要性\n機械学習と統計学が単なる道具ではなく，人間のより大きなシステムの一環を単独で担う場面が増えてきた．例えば，\n\n金融・経営・政策決定などの分野で，意思決定に繋げるデータ解析をするとき\n科学において，発見や仮説を検証するためのデータ解析をするとき\nロボットや自動車などの自動化をし，社会に実装するとき9\n医療診断や裁判などの場面で，専門家を補助するシステムを作るとき\n\nこれらのいずれの例でも，システムの一部を担うにあたって，不確実性を定量化しておくことが欠かせない．その出力を用いるのが人間である場合も勿論，別の機械学習モデルである場合は尚更である．\nつまり，人間社会で優秀であるだけでなくホウレンソウと信頼獲得も重要であるように，機械学習モデルも性能の高さと正確さだけでなく，いつその結果を信頼して良いのかを「どの程度」という指標と共に知らせてくれることが信頼関係の基本となるだろう．\n実際，殆どの場面で，データから高い確証度で言えることと，そうではないことでは全く違う意味を持つ．それぞれの場面での例には，次のようなものがあるだろう：\n\nデータから高い確証度で言えることと，意思決定者による采配が必要な部分を分離できない限り，意思決定プロセスの一部として組み込むことが難しく，結局機械学習手法が全く採用されないということもあり得る．\n結果の再現可能性が科学の基本的な要請である以上，その結果の不確実性を実験結果に付記することは基本的な科学的態度である．後述（第 2.3.1 節）するが，\\(p\\)-値や信頼区間などの統計量はこれに応えるものではない．\nロボットや自動車の自動化 AI システムは，いくつかのモデルを組み合わせて作ることになるだろう．個々が十分な性能を持っていても，小さな誤差が累積してシステムとしての性能を著しく低下させることがある．これを防ぐために，統一した方法での不確実性の取り扱いが必要である．\n個々人の権利と法益が衝突する場面にも AI が利用されより良い生活が実現されるには，法的な解釈可能性が担保される必要があることが，実は大きな難関として我々を待っている．その第一歩は，不確実性の可視化になるだろう．10\n\n以上の内容は，結果の 解釈可能性 でも全く同じことが言えるだろう．\n\n\n2.2 信頼のおける AI システム\n上述の点をまとめると，機械学習手法と人間社会がよりよく共生していくには AI の 信頼性 (trustworthyness) が必要とされているのである．不確実性の定量化と解釈可能性は，AI が人間社会で信頼を獲得するにあたって根本的な要素になるだろう．\n現状の手法の延長でこの信頼性の問題は扱えず，新たな手法が必要とされている．Bayesian approach や probabilistic approach と呼ばれている試みは，まさにこれに応えるものであり，近年急速に発展している．\n\n\n2.3 不確実性を扱うには Bayes が必要である\n実装は頻度論的な手法の方が簡単で高速であることが多いが，不確実性の定量化には向かない．\nこのような場面では，頻度論的手法を頻度論的に改善する，という方向は筋が悪いと思われる．このようなときこそ，もう一つの世界樹であるベイズの方法を用いるべきである．\nこれを，科学における再現性の危機を例にとって確認したい．11\n\n2.3.1 再現性の危機\n多くの実験科学では不確実性の定量化が必要不可欠である (Krzywinski and Altman, 2013)．\n\nIt is necessary and true that all of the things we say in science, all of the conclusions, are uncertain … (Feynman, 1998)\n\n再現性の危機 (replication crisis) とは，多くの実験において報告されている統計的有意性が，再現実験において得られないことが多いという問題を指し，2010年代の初めから多くの科学分野において問題として取り上げられてきた．12\nその理由は明白である．信頼区間は集合値の推定量であるため，「分散」が十分大きいならば，データセットを変えて何回も計算することでいずれは非自明なものを得ることが出来るのである．そのため，信頼区間や \\(P\\)-値を報告するだけでは，結果の信頼性については何も保証されないのである．\nその結果多くの科学分野では Bayes 統計学による不確実性の定量化に移行しつつある (Herzog and Ostwald, 2013), (Trafimow and Marks, 2015), (Nuzzo, 2014)．\n信頼区間と信用区間の違いに注目して，その違いを解説する．\n\n\n2.3.2 信頼区間と信用区間\n「95 % の信頼区間」と言ったとき，「95 % の確率で真の値がその範囲に含まれるような区間」だと思いがちであるが，これはどちらかというと信用区間の説明であり，信頼区間は計算するごとに値が変わってしまう確率変数である ことを見落としがちである．13\nつまり，信頼区間は頻度論的な概念であり，「真の値」がまず存在し，区間自体が変動し，95 % の確率で被覆するというのである．今回見ている信頼区間が，別のデータセットで計算した場合にどう変わるかについては全く未知である．\nこのことは，信頼区間は「真のパラメータの値」で条件づけて得るものであるが，信用区間はデータによって条件づけて得るものであるという点で違う，とまとめられる．この２つの混同は「何で条件づけているか？」を意識することで回避することができる．14\n誤解を恐れず言うならば，再現性の危機とは，信頼区間というサイコロの出目によって科学が踊らされていたということに他ならない (Nuzzo, 2014)．15\n\n\n2.3.3 なぜベイズを用いれば良いのか？\nこれは，信頼区間や \\(P\\)-値などの頻度論的な手法は，しばしば尤度原理に違反するためである．16\n換言すれば，何らかのモデルと事前分布に関するベイズ手法と等価である，すなわち，Bayesianly justifiable (Rubin, 1984) とみなせない手法は，何らかの意味でデータを十分に反映できていない可能性が高くなる．\n従って，ベイズの手法が原理的に最も適切である場面が多い．一方でその計算の困難さや，全てのステップをモデリング段階に組み込む点を回避するために，種々の頻度論的な実装は考え得て，頻度論的な手法はそのような運用においては健全であるとの指標にもなる．17\nBayes により手法を理解し，頻度論的に手法を実装することが，あるべき姿勢であると思われる．\n\nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice. (Rubin, 1984)\n\n\n\n\n2.4 ベイズ深層学習という夢\n深層モデルはその性能の高さから，最も実世界応用が期待されるモデルであるが，パラメータが極めて多いため，特にベイズ化することが難しいと言われている．\n例えば，ハルシネーション (hallucination) として，LLM が「事実に基づかない」情報を生成してしまうことが問題とされているが，これも不確実性の定量化の問題に他ならない．18\nその他の場面でも，不確実性の定量化には conformal prediction などの事後的な手法が試みられている．19 これらはどのようなブラックボックスに対しても適用可能である一方で，対症療法というべきものであり，ベイズ流の解釈をすることで直接的に事後分布を求めるという根本的な解決にも，もっと注力されるべきである．\nベイズによる不確実性の定量化は，自然であるだけでなく，より有用な不確実性の定量化を与えるものだと予想している．20\n加えて，事前分布を変えることで，種々の帰納バイアスを加えるという「プロンプトエンジニアリング」ならぬ「プライヤーエンジニアリング」の理論が樹立できるかもしれない．すでに，公平性，同変性，スパース性，共変量シフトへの頑健性などを達成するための事前分布が考えられている．21\n\n\n2.5 分野全体の動向\n現状の機械学習モデルと実応用との乖離は，他の側面でも生じている．\nまず，訓練データが実際の運用環境を十分に反映できていないということは極めて頻繁に起こるだろう．この現象を 分布シフト といい，機械学種モデルの予測性能だけで無く，分布外汎化 (out-of-distribution generalization) 能力も重視するという潮流が生じている．\nさらに，一度訓練したモデルを，分布シフト自体が移り変わっていく環境で，微調整のみによって繰り返し使い続けるという使用を想定した 継続学習 (continual learning) という考え方もある．22\n章を変えて別の角度から議論を続けよう．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-distributional-representation",
    "href": "posts/2024/AI/BAI.html#sec-distributional-representation",
    "title": "これからはじめるベイズ機械学習",
    "section": "3 ベイズは分布という共通言語を与える",
    "text": "3 ベイズは分布という共通言語を与える\n\n3.1 継続学習という発想\n継続学習は，機械学習モデルをより動的で実際的な環境でも使えるようにするための新たな枠組みである．そこまで，教師あり学習モデルがすでに実用的な性能を獲得したということでもある．\nつまり，単に「教師あり」「教師なし」の１タスクを解く営みは爛熟しつつあり，機械学習の理論と応用の最先端は，より深い森に分け入りつつあるのである．\nここにおいて，ベイズ流の接近が統一的な取り扱いを与えるという美点が，さらに重要でもはや必要不可欠な役割を果たすものと思われる．\n\n3.1.1 ベイズ推論が与える統一的枠組み\nベイズ推論とは，事前分布 というものを設定して，これをデータによって更新するという営みである（その更新規則は Bayes の公式が与える）．\n事前分布をどう設定すれば良いか？の問題は，ベイズ推論の初期からの問題であった．極めて自由度が高いことが，逆にベイズ推論が実際のデータ解析の場面において敬遠される一因ともなっていた．\n\n\n3.1.2 ベイズと最適化との協業\nしかし，継続学習が当たり前になった社会において，全てのパラメータ値を事前分布と事後分布とみなし，全ての学習過程をベイズの公式という統一的な方法で更新すると捉えられることは，極めて大きな利点になり得る．\nというのも，継続学習においては，学習を繰り返すうちに過去に学んだ内容を忘れ去ってしまうという 壊滅的忘却 (catastrophically forgetting) が最大の困難である．\n理論的には，分布のベイズ更新の繰り返しとして見る方が極めて見通しが良い．一方で，事後分布の近似が十分でない場合，実際にベイズ更新を行うことは性能に悪影響を与える．\nそこで，理論解析や設計をベイズの観点から行い，実際の推論は最適化ベースで行うという適材適所により，壊滅的忘却を緩和できる可能性がある (Farquhar and Gal, 2019)．\n\n\n\n3.2 例：強化学習への分布によるアプローチ\n\nwe believe the value distribution has a central role to play in reinforcement learning. (Bellemare et al., 2017)"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-inductive-bias",
    "href": "posts/2024/AI/BAI.html#sec-inductive-bias",
    "title": "これからはじめるベイズ機械学習",
    "section": "4 ベイズは理解を促進する",
    "text": "4 ベイズは理解を促進する\n我々はもはや機械学習を通じて，自分たちが何をやっているのかわかっていない．この愚かさを AI に継がせてはならない．\n\n4.1 なぜベイズ法の発展が遅れたか？\nベイズ法の採用は，自分たちが何をやっているかへの理解と解釈可能性を刺激するという側面がある．\nその理由は簡単である．ベイズ推論は，モデルとその上の事前分布を定めれば，あとはベイズ更新規則をどう計算するかの問題となり，近似手法は様々あれど，もはや推論手法に選択の余地はない．\n換言すれば，その分解析者がモデルと事前分布の特定を全てこなす必要があるのであり，解析者に確率モデリングへの理解を強要するところがある．\nしかしこれは「面倒なことは全てアルゴリズムにやってほしい」という精神とは対立するため，ベイズの美点であると同時に，ベイズの発展を阻害してきた遠因の一つでもあった．\nこれを指して「事前分布の選択に恣意性が入る」という通り文句がよく使われるが，実際は，頻度論的手法における「どのような目的関数をどのように最適化すれば良いか？」という恣意性に変換されているのみであり，問題を先送りにして，「ベイズ法 対 頻度論的手法」という虚構の対立を作り上げているのみである．\n機械学習のポテンシャルが具現化したいまこそ，この困難に立ち向かう必要があるが，この問題は最適化や頻度論的な立場から見るより，ベイズの立場から見た方が，理論的な見通しが良いようである（第 4.4 節）．\n\n\n4.2 帰納バイアスの明確化の必要性\n機械学習の真の理解のためには，各モデルの帰納バイアスを明確化する必要がある．\n\n4.2.1 帰納バイアスとは何か？\n現状の AI システムは大量のラベル付きデータが必要であり，多くの現実的に有用なタスクでこのような教師データが用意できるわけではない．\n一方で，人間は遥かに少ないデータから効率的に学習することができる．\n\n\n\nNumber of Training Tokens BabyLM Challenge\n\n\nその違いは，進化が我々生物に授けた 帰納バイアス にあると考えられている．\n我々には遺伝的に継がれている生まれ持った学習特性があり，より効率的に学習出来るのかも知れない．\n事実，一度事前学習をした LLM は，極めて少ないデータにより新しいタスクを学習することができるがわかりつつある (Zhou et al., 2023)．LLM の事後調整に関する稿 も参照．\n\n\n4.2.2 事前分布に向き合わずにやり過ごしてきた\n現状，多くの機械学習手法は確率的な方法を取っていない．これは事前分布を明示せずに（ひょっとしたら明後日の方向に向かって）行われる Bayes 学習手法であるとみなせる．\n現状の機械学習の成功は，事前分布に関する知識なしに到達されたものであり，それ故の限界がある．例えば，現状のままではモデルにどのような帰納バイアスが組み込まれているか不明瞭である．23\n\n\n4.2.3 帰納バイアスに対するベイズ的視点\nデータの空間 \\(\\mathcal{X}\\) 上の任意のモデル \\(\\mathcal{M}\\) の周辺尤度 \\(p(x|\\mathcal{M})\\) は，24 ベイズ流には事後確率として捉えられ，全てのデータ \\(x\\in\\mathcal{X}\\) 上に有限な測度を定める．25\nよって，全てのモデルは，あるデータを得意とするならば他のデータについては不得意であることを免れない．これは no free lunch 定理と呼ばれる定理の一群により推測されており，分類問題などの簡単なタスクを除いて完全な形式的表現はまだ持たない作業仮設である．\n\n\n\nA Probabilistic Perspective of Genelization (Wilson and Izmailov, 2020)\n\n\n例えば，基盤モデル とは，インターネット上のデータから最大限人間の言語というものに関する帰納バイアスを取り込んだ，パラメータ上の初期設定であると見れる．\nこれは，あるパラメータ空間上の理想的な事前分布からのサンプリングであるかも知れない．それ故，種々の下流タスクに対して，小さなモデル変更のみにより適応することが出来る．\n大規模言語モデルの能力創発現象は，帰納バイアスを十分取り込むことにより自然に解かれるタスクであったのかもしれない．\n\n\n4.2.4 worst-case analysis からの脱皮\n帰納バイアスを明確にせず，やり過ごしてきたつけが，特に学習理論においても現れている．\n現状の統計的学習理論は全て，worst-case analysis であるが，実用上は全くそうではない．「動くモデル」には暗黙の帰納バイアスが入っており，これに明るくなる必要があるのである．\n2024 年に生きる我々は，worst-case analysis からの脱皮を迫られている．\n\n\n\n4.3 数学者の哲学\nBayes の見方は，機械学習モデルを底流する数理的枠組みになっている．仮に次の Mac Lane の言葉が数学者のあるべき態度の１つであるとするならば，この意味での数学者には Bayes の立場から機械学習を研究することを特におすすめする．\n\nHowever, I persisted in the position that as mathematicians we must know whereof we speak, be it a homotopy group or an adjoint functor. (Mac Lane, 1983, p. 55)\n\n数理統計学に始まり，数学者の統計や機械学習分野への参入は，推論手法の解析が想像されるかも知れない．\nしかし，真の数学的理解は，手法の数学的な機械仕掛けを紐解くだけでなく，それぞれの手法がモデルとしてどのような仮定の下で成り立っているかを，モデリングの観点から理解することにもあると筆者には思われる．\n現状，後者の視点が大変に不足しており，数理的な知識に支えられた大局観というものがない．個々の数学的な道具に捉われず，大局的な構造を捉える数理的枠組みが必要である．\nこれに応えるのがベイズの枠組みであると筆者は信じる．\n推論とモデリングという双対的な営みは深い数理的な構造を持っていることが明らかになりつつある．この大局的構造の解明と理論構築には，ベイズの観点から光を照らしてくれるような，Mac Lane の意味での数学者的な魂が必要とされているのである．\n\n4.3.1 Bayes の数学\nBayes 流の解釈では，どんなにモデルが複雑で巨大になろうとも，推論とは積分に他ならない．\n\n\n\nBayes’ Theorem (density form)\n\n\n全ての（尤度原理に則った）推論は，事後分布の関数としてなされる（べきである）．\n実際の実装は，その近似として実行される（べきである）．\nよって，実装とモデリングの段階を明確に分離する枠組みを提供している上に，極めて普遍的な枠組みである．\nというのも，Bayes 流のモデリングは，Markov 圏 上の図式と見ることができ（第 5.2 節），普遍的である上に，数学的にも最も直接的で直感的な表現であると思われる．\n圏として持つ代数的性質は，モデルの結合・分解が自由に出来るということに繋がり，モジュール性 が高いということになる．\n\nI basically know of two principles for treating complicated systems in simple ways: the ﬁrst is the principle of modularity and the second is the principle of abstraction. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. Michael I. Jordan excerpted from (Frey, 1998)\n\n分布を明示的に用いた 確率核 を通じてのモデリングは，なぜだか数学的に極めて自然なアプローチを提供してくれるようである．\n\n\n4.3.2 ベイズの代数・幾何・解析\n上述したように，ベイズのモデリング法と学習規則は本質的に代数的なところがある．\n加えて，分布を基本言語とするために，ベイズ推論においては空間 \\(\\mathcal{P}(\\mathcal{X})\\subset\\mathcal{M}^1(\\mathcal{X})\\) が極めて基本的な役割を果たす．\nサンプリングは \\(\\mathcal{P}(\\mathcal{X})\\) 上の幾何学に関係が深く，情報幾何学や最適輸送などの発展が見られている．\n一方で最適化は \\(\\mathcal{P}(\\mathcal{X})\\) 上の解析学に関係が深く，古くから機械学習分野では \\(\\mathcal{P}(\\mathcal{X})\\) 上の様々な汎函数が ダイバージェンス の名前で考察されており，その勾配流として種々の最適化手法が理解できる．\n\n\n4.3.3 Bayes に繋げる数学\n通常の頻度論的手法は，うまくいくことが先であり，理論が後付けされる．そしてその理論もどこか ad-hoc というべきであり，worst-case で漸近論的である．\nこれらに Bayes 的な解釈を与えることで，暗黙のうちにどのような仮定を課しているモデリング手法に相等するのか明確にされる．特に，非漸近論的な知見を与えてくれる数少ないの道の一つである．\n\n\n\n4.4 ベイズ推論とみる美点\nベイズ推論自体への理解だけでなく，種々の頻度論的手法を（特定の環境下での）ベイズ推論の近似として理解することは，新たなアルゴリズムの開発に有用であるという合意が形成されつつあるようである．26\n最適化に基づく手法の計算効率性は，正確なベイズ推論に勝る場面も多い．ここで注意すべきは，ベイズ推論の実行が肝要であり，その実装は最適化に依ろうと，積分近似に依ろうと大した違いではないのである．\n「ベイズ推論は多くの最尤法に基づく手法よりも，自然な正則化がなされるために過学習の問題がない．」と説明されるが本来は逆である．多くの最適化に基づく手法は，目的関数の選択に恣意性があり，その選択を誤り続けているために過学習という問題が生じている，という方が，後世の教科書に載る表現なのではないかと筆者は考えている．\nそこで，種々の既存手法のベイズ流の解釈を探究することは，より良い推論アルゴリズムの開発に資すると考えられている．\nこの方向の近年の発展をいくつか紹介したい．\n\n4.4.1 ベイズ学習規則\n現状の機械学習は，統計学，連続最適化，計算機科学の知識を総動員して開発された種々の推論手法によって支えられている．\nその性能は驚異的なスピードで向上しているが，それぞれの手法がどのような仮定をモデリングの段階で課しているかが不明瞭であり，どの手法を使うべきかの統一的な枠組みは得られていない．\nこの現状の抜本的な改善が，それぞれの手法のベイズ流の解釈を探究することで得られると考えられる．\nその枠組みの一つが ベイズ学習規則 (Khan and Rue, 2023) である．\n(Khan and Rue, 2023, p. 4) では，ベイズ流の解釈を持つ種々の手法が他より優れている理由として，目的関数に現れるエントロピー項が 自然勾配 の概念を通じて自然な正則化を与えることが，ベイズ学習規則という新たな理論的枠組みの中で示されている．\n\n\n4.4.2 例：強化学習\n強化学習でも，モデルベースのアプローチが取り入れられつつあり (Deisenroth and Rasmussen, 2011)，さらに学習と制御をベイズ推論と見ることが，アルゴリズムの設計において有用であることが提唱されつつある：\n\nCrucially, in the framework of PGMs, it is sufficient to write down the model and pose the question, and the objectives for learning and inference emerge automatically. (Levine, 2018)"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#bayes-機械学習の例",
    "href": "posts/2024/AI/BAI.html#bayes-機械学習の例",
    "title": "これからはじめるベイズ機械学習",
    "section": "5 Bayes 機械学習の例",
    "text": "5 Bayes 機械学習の例\n\n\n\n\n\n\n要約\n\n\n\n深層学習モデルにより教師あり学習は十分に発展し，多くの訓練データが得られる場面では驚異的な性能を発揮するようになった．\nこの発展は，モデリングの仮定に捉われずに純粋にアルゴリズムの開発に集中することが出来るという頻度論的な枠組みの利点を有効活用する形で達成された．\nしかし，殆どの実世界応用では，不確実性のモデリングが必要不可欠である．この点を後回しにして性能を追求することで得た栄華である．だからこそ，極めて高い性能を誇るモデルを，実世界応用の場面で有効活用する手段を我々はまだ知らないのである．\nその鍵はベイズにある．安全性，信頼性，柔軟性……．これらの21世紀の社会の要請に応えるためには，ベイズ機械学習手法の発展と，既存の手法のベイズ流の理解とが追いつくことが，第一歩である．\n\n\n既存の深層学習モデルは，「教師あり学習」という枠組みや，画像の分類タスクや自然言語処理のタスクなど，広く周知された問題設定とデータセットが存在する．\n一方で，ベイズ機械学習における対応物はまだ十分に周知されていないようである．\nベイズ機械学習では「損失を最小化する」という枠組みの中でなるべく性能の良い推論手法を探す，というわかりやすい枠組みがある訳ではないようである．\nそこで，本章ではベイズ機械学習の近年の発展を概観することを試みる．\n\n5.1 Bayes 深層学習\nニューラルネットワークモデルは，隠れ素子数が無限大になる極限において，Gauss 過程モデルに漸近することが知られている (Neal, 1996)．Gauss 過程とはノンパラメトリックなベイズ機械学習手法の代表である．この対応を通じて，深層学習のベイズ流の解釈が進められている．\nこの稿の執筆後，本稿をまとめるかのようなアブストラクトを持ったポジションペーパー (Papamarkou et al., 2024) が公開された\n\nIn the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. (Papamarkou et al., 2024)\n\n深層学習をベイズ化することで，上にあげた\n\n不確実性の自然な定量化\n継続学習への柔軟な接続\n科学的営みの促進\n\nなどが目指せる．特に，現状の大規模な基盤モデルをベイズ化する悲願を真っ向から論じている．\n\n\n5.2 確率的グラフィカルモデル\n歴史的に，（確率的）モデリングは，主に（確率的）グラフィカルモデルを通じて機械学習の分野に導入された．\nそのため，20世紀に入ったばかりの頃は，Bayes 機械学習の唯一の例は確率的グラフィカルモデルなのであった．27\nだが，確率的グラフィカルモデルは，極めて普遍的で，従来の因果推論・階層モデル・欠測モデル・潜在変数モデル・構造方程式モデルなどの発展を包含する統一的な枠組みであることをより広く認識すべきである．\n\n5.2.1 ベイジアンネットワーク\nBayesian Network は Markov 圏上の図式であり，方向関係のある変数間の関係をモデリングする最も直接的な方法である．\n\n\n5.2.2 構造的因果モデル\n\n\n5.2.3 階層モデル\n階層モデルとは，ベイズの枠組みでは，観測変数・潜在変数の区別なく，モデルを自由に結合出来る点を利用したモデリング手法である．\n\n\n5.2.4 モデルの属人化\n大きなデータも，属人化医療や推薦システムなど多くの文脈では小さなデータの寄せ集めであり，そうでなくともその構造を正しく捉え，全ての不確実性を取り入れた柔軟なモデリングをすることで，さらに密接な形で社会に取り入れることができる．28\n\n\n\n5.3 確率的プログラミング\n\n5.3.1 アルゴリズムのプログラミングから，モデルのプログラミングへ\nベイズ流の解釈では，解析者の恣意的な選択はモデリングの段階に集中しており，モデルが決定すれば推論手法は自動的に従う．\nこのパラダイムでは，推論手法は背後に隠し，解析者はモデリングに集中するための新たなプログラミング言語があっても良いはずである．\nこのような言語を 確率的プログラミング (Probabilistic Programming) 言語と呼ぶ．\n\n\n5.3.2 確率的プログラミングはグラフィカルモデルの拡張である\n確率的グラフィカルモデルをどのようにプログラムに落とし込むかというと，確率核をシミュレーターとして実装するのである．\n逆に，シミュレーションが可能な限りどのようなモデルも実装できるので，確率的グラフィカルモデルの真の拡張であると言える．29\n\n\n5.3.3 Simulation-based Inference\n上述の通り，シミュレーターがあればモデルが定義でき，モデルがあれば推論ができる．さらに，棄却法，重点サンプリング法，MCMC，SMC などの Monte Carlo 法のレパートリーにより，殆どあらゆるシミュレーションと推論が統一的に実行できる．これが Bayes 推論の強みである．\n\n\n\n5.4 Bayes 最適化\nベイズはシステムの一部として自然に組み込まれると論じたが（第 2.1 節），現状その最先端をいくのがベイズ最適化の分野である．\nベイズ最適化は最も簡単な形では，未知の関数 \\(f:X\\to\\mathbb{R}\\) の最大値点を求める問題を，逐次意思決定問題 として解く手法である．\nベイズ数値計算 (O’Hagan, 1991) の現代的な再解釈とも捉えられる．30\nこの際，未知の関数 \\(f\\) を Gauss 過程などでモデリングし，不確実性の高い点からサンプル \\(f(x_1),f(x_2),\\cdots\\) を取って最も効率の良い方法で最大化していくことを目指す．\nベイズ最適化は多腕バンディット問題と関係が深く，２つの問題は共に一方向のエージェント・環境相互作用しか仮定していないという形での強化学習への入り口である．\n\n\n5.5 確率的データ圧縮\n殆どの（可逆）データ圧縮アルゴリズムは，シンボルの列に対する確率的モデリングと等価である．31 そしてモデルの予測精度が良いほど，データの圧縮率は高い．\nしたがって，より良いベイズ（ノンパラメトリック）モデルの開発と，より幅広いデータに対するデータ圧縮技術の発展とは両輪である．32\n\n\n5.6 モデルの自動発見\n機械学習の精神の一つに，データからの知識獲得をなるべく自動化したいというものがある．\nベイズの方から，統計解析自体を自動化する Automatic Statistician (Lloyd et al., 2014) という試みがある．これはデータを説明するモデルを自動発見し，結果を自然言語でまとめてくれる上に，モデルに含まれる不確実性に関しても報告してくれる．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#footnotes",
    "href": "posts/2024/AI/BAI.html#footnotes",
    "title": "これからはじめるベイズ機械学習",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこれは Human-AI interaction におけるガイドライン (Amershi et al., 2019), (Bansal et al., 2019) でも明確にされている点である．この方向への試みの代表がベイズ機械学習，というわけではないが，筆者はベイズ機械学習の興隆は信頼のおける AI システムの構築にための極めて盤石な土台になるだろうと論じる．↩︎\n本稿執筆後に，ほとんど同じ論調を，深層学習や基盤モデルを中心に，遥かに明瞭に述べた論文 (Papamarkou et al., 2024) を見つけたので，賢明な読者はぜひこちらを参考にしていただきたい．↩︎\n(Broderick et al., 2023, p. 2) など．↩︎\nこの違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” (Ghahramani, 2015, p. 454)．↩︎\n“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” (Ghahramani, 2015, p. 454)．このように，その用いる手法も鮮やかに対照的に見えるが，積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．↩︎\n(Broderick et al., 2023) が極めて説得的にこの点を指摘している．↩︎\n合理的な信念の度合い (degree of belief) は確率の公理を満たす必要がある，という主張は Cox の名前でも呼ばれる．この点から，Bayes の定理は，帰納的推論の確率論的な拡張だとも捉えられる．“This justifies the use of subjective Bayesian probabilistic representations in artificial intelligence.” “Probabilistic modelling also has some conceptual advantages over alternatives because it is a normative theory for learning in artificially intelligent systems.” (Ghahramani, 2015, p. 453)．↩︎\n現状，日本にてベイズ機械学習を専業として研究を進めている人は Emtiyaz Khan に限ると思われる．(Ghahramani, 2015, p. 452) でも “Probabilistic approaches have only recently become a mainstream approach to artificial intellifence, robotics, and machine learning.” と述べられている．↩︎\n“The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars.” (Chen et al., 2023)．↩︎\nモデルの予測結果に不確実性の定量化が伴われていたならば，モデルを信用出来ない場面で意思決定者がこれを信用したため責任があるのか，使用者には非難可能性がないのか，モデル設計者に過失があったと言えるのかの議論に，足場を与えることが出来るだろう．↩︎\n(Gal and Ghahramani, 2016) も参照．↩︎\n心理学においては「再現性問題が大きく注目される大きな契機となった「超能力論文」が出版されたのが 2011 年である」 (平石界 and 中村大輝, 2022) ようである．計量経済学における 信頼性革命 (Angrist and Pischke, 2010) は，再現性の危機の，もう一つの革新的な解決法である．↩︎\n「それでは，信頼区間は不確実性の正しい定量化を与えないではないか！」ということになるが，その通りなのである．\\(P\\)-値を計算する過程とは，帰無仮説で条件付けているだけであり，データの関数でもある．\\(P\\)-値の確率変数としての分散が大きいほど，何回か同じ実験を繰り返せばすぐに小さな \\(P\\)-値が得られることになる．これは 基準確率の誤謬 と似ている．↩︎\n“Confidence intervals suffer from an inverse inference problem that is not very different from that suffered by the NHSTP. In the NHSTP, the problem is in traversing the distance from the probability of the finding, given the null hypothesis, to the probability of the null hypothesis, given the finding.” (Trafimow and Marks, 2015)↩︎\n(Nuzzo, 2014) には，Fisher が最初に用いてから，Neyman-Pearson 理論がこれを排除したものの，コミュニティが \\(P\\)-値を誤解して都合の良いように利用するようになるまでに至った歴史が説明されている．↩︎\n(Murphy, 2022, p. 201) の議論も参照．↩︎\n(Efron, 1986) も示唆深い．↩︎\n(Mohri and Hashimoto, 2024), (Papamarkou et al., 2024, p. 3) 2.1節 なども指摘している．↩︎\n(Novello et al., 2024) では out-of-distribution detection, (Mohri and Hashimoto, 2024) は LLM の hallucination への応用．↩︎\n「筆者は，conformal prediction などの post-hoc な手法は，便利かも知れないが，「信頼区間」や「\\(P\\)-値」のような側面（第 2.3 節）も併せ持つのではないかと危惧しながら見ている．」と当初は書いていたが，どうもそう簡単な話ではないようである．(Papamarkou et al., 2024) を読んで思った．↩︎\n(Papamarkou et al., 2024, p. 5) 3.4節．↩︎\n(Wang et al., 2024) が最新のサーベイであるようだ．↩︎\nPhilipp Hennig Probabilistic ML - Lecture 1 - Introduction “Statistical Learning Theory is about Bayesian Reasoning when you don’t say out aloud what the prior is.”↩︎\nこれを 証拠 (model evidence) ともいう．↩︎\n事前分布として非有限な測度を用いた場合など，例外もある．↩︎\n“most conventional optimization-based machine-learning approaches have probabilistic analogues that handle uncertainty in a more principled manner.” (Ghahramani, 2015, p. 458)．↩︎\n(Neal and Hinton, 1998) など．↩︎\n(Ghahramani, 2015, p. 458) はこれを モデルの属人化 (personalization of models) と呼んでいる．↩︎\n(Ghahramani, 2015, p. 453) “probabilistic programming offers an elegant way of generalizing graphical models, allowing a much richer representation of models.”．↩︎\n“More generally, Bayesian optimization is a special case of Bayesian numerical computation, which is re-emerging as a very active area of research, and includes topics such as solving ordinary differential equations and numerical integration.” (Ghahramani, 2015, p. 456)．↩︎\n“All commonly used lossless data compression algorithms (for example, gzip) can be viewed as probabilistic models of sequences of symbols.” (Ghahramani, 2015, p. 456)．↩︎\n(Steinruecken et al., 2015) は記号列に対するノンパラメトリックモデルを改良することで，データ圧縮アルゴリズム PPM を改良した良い例である．↩︎"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html",
    "href": "posts/2024/Kernels/Deep2.html",
    "title": "トランスフォーマー",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#トランスフォーマー",
    "href": "posts/2024/Kernels/Deep2.html#トランスフォーマー",
    "title": "トランスフォーマー",
    "section": "1 トランスフォーマー",
    "text": "1 トランスフォーマー\n\n1.1 名前の由来と背景\nトランスフォーマー (Vaswani et al., 2017) は，注意 (attension) という機構を通じて，時系列データの依存関係を効率的に学習することの出来るモデルである．この「変換器」という名前は，後述の内部表現ベクトル \\(Y\\) を，入力 \\(X\\) から次元を変えずにより良いものに「変換する」というところから名前が付けられている．\n初めは自然言語処理（特に機械翻訳）の文脈で導入されたデコーダーとエンコーダーの組からなるモデルであるが，そのエンコーダー部分だけで言語，画像，動画などあらゆる系列データのモデリング全体で抜群の性能を発揮する上に，これら複数ドメインのデータを組み合わせてモデリングすることもできる（第 4 節）．\nさらに，トランスフォーマーはアーキテクチャとして（CNN や RNN などに比べると）シンプルであり，大規模なデータセットで大規模なモデルを訓練することが出来るスケーラビリティが魅力である．また，モデルの大きさに対して性能が単調に改善するというスケーリング則 (Hestness et al., 2017), (Kaplan et al., 2020) が成り立つことが示されており，大規模な資源を投下して大規模なモデルを作る経営判断も下しやすかった．\n\n\n\nScaling Laws (Kaplan et al., 2020)\n\n\nその後すぐに，一度大規模なモデルを訓練してしまえば，少しの修正を施すのみで種々の下流タスクに適用することが可能であることが発覚した．これを 基盤モデル という（第 3.2 節）．\n\n\n1.2 注意機構\nトランスフォーマーの核はその注意機構にある．とはいっても，注意機構自体はトランスフォーマー以前から存在した技術である．\n元々機械翻訳に用いられていたエンコーダー・デコーダー型の RNN の性能を向上させる機構として提案された (Bahdanau et al., 2015)．その後，(Vaswani et al., 2017) の Attention is All You Need とは，注意機構のみが重要で，RNN としての構造（や画像では畳み込みの構造）を排してシンプルにした方が更に性能が向上する，という報告である．\n時系列データの解析では，そして自然言語処理ではとりわけ，文脈というものが重要である．しかし文脈は長期の依存関係になることもしばしばあり，従来の RNN ではこのモデリングに苦労していた (bottleneck problem)．\n注意機構は，遠く離れた２つのトークンも直接に相互作用を持つアーキテクチャになっており，この点を抜本的に解決したものである．その結果，元の RNN のアーキテクチャも不要とするくらいのモデリング能力を，自然言語のみでなく，画像や動画に対しても示したのである．\n注意機構は自己注意と交差注意に分けられる．\n\n1.2.1 枠組み\nトランスフォーマーに入力する系列を \\(\\{x^n\\}_{n=1}^N\\subset\\mathbb{R}^D\\) で表す．生のデータをそのままモデルに入れるわけではないので，別の言葉で呼び変える．\n慣習として，特に言語データの場合は各 \\(x^n\\) を トークン (token) という．画像では パッチ (patch) ともいう．\n以降，\\(X:=(x^n)_{n=1}^N\\in M_{ND}(\\mathbb{R})\\) とも表す．\n\n\n1.2.2 自己注意機構のプロトタイプ\n自己注意機構とは，\\(Y=AX\\) によって定まる \\(M_{ND}(\\mathbb{R})\\) 上の線型変換 \\(X\\mapsto Y\\) のことである： \\[\ny^n=\\sum_{m=1}^N a^n_mx^m,\n\\tag{1}\\] \\[\na^n_m=\\frac{e^{(x^n)^\\top x^m}}{\\sum_{k=1}^Ne^{(x^n)^\\top x^k}}.\n\\tag{2}\\] ここで，\\(A=(a^n_m)_{n,m\\in[N]}\\in M_N(\\mathbb{R})\\) は 確率行列 をなし，その成分を 注意荷重 (attention weight) という．\nこの変換において，同じ \\(x^m\\) の値を，３回別々の意味で使われていることに注意する：\n\n式 1 における \\(x^m\\) は，新たな表現 \\(y^n\\) を作るためのプロトタイプにような働きをしている．これを 値 (value) という．\n式 2 において，内積が用いられており，\\(x^n\\) と \\(x^m\\) の類似度が測られている．\n\n\\(x^m\\) を，\\(x^m\\) が提供出来る情報を要約した量としての働きをし，鍵 (key) という．\n\\(x^n\\) は，\\(x^n\\) と関連すべき情報を要求する役割を果たし，クエリ (query) という．\n\n最終的に，鍵とクエリの類似度・マッチ度を，ソフトマックス関数 を通じて確率分布として表現し，値の空間 \\(\\{x^m\\}_{m=1}^N\\) 上の確率質量関数 \\(\\{a^n_m\\}_{m=1}^N\\) を得ている．これに関して 平均する ことで，鍵 \\(y^n\\) を得る．\n\n\n\n1.2.3 内積による自己注意機構\n３つの別々の役割を果たしている以上，それぞれ固有の表現を持っていても良いはずである．そこで，値，鍵，クエリに，それぞれにニューラルネットワーク \\(W_{(\\Lambda)}\\in M_{DD_{(\\Lambda)}}(\\mathbb{R})\\;(\\Lambda\\in\\{V,K,Q\\})\\) を与えて固有の表現 \\[\nx_{(\\Lambda)}^n:=XW_{(\\Lambda)}\n\\] を持たせ，この \\(W_{(\\Lambda)}\\) を誤差逆伝播法により同時に学習することとする．\nこうして得るのが，内積による自己注意機構 (dot-product self-attention mechanism) である．このとき，\\(D_{(K)}=D_{(Q)}\\) は必要だが，\\(y^n\\in\\mathbb{R}^{D_{(V)}}\\) は，元の次元 \\(D\\) と異なっても良いことに注意．\n最後に，ソフトマックス関数の適用において，勾配消失を回避するために，次元 \\(D_{(K)}\\) に応じたスケーリングを介して \\[\na^n_m=\\frac{e^{\\frac{\\left(x^n_{(Q)}\\right)^\\top x^m_{(K)}}{\\sqrt{D_K}}}}{\\sum_{k=1}^Ne^{\\frac{\\left(x^n_{(Q)}\\right)^\\top x^k_{(K)}}{\\sqrt{D_K}}}}\n\\] とする．これを最終的な 自己注意機構 (scaled dot-product self-attention mechanism) という．\n\n\n1.2.4 交差注意\nデコーダーとエンコーダーの接続部に用いられる 交差注意 (cross attention) については，ここでは触れない．\n\n\n1.2.5 マスキング\n実際に学習するとき，注意荷重 \\(A\\) は上三角部分が \\(-\\infty\\) になったものを用いる．\nこれは，次のトークンを予測するにあたって，そのトークンより後のトークンを見ないようにするためである．\n\n\n\n1.3 トランスフォーマーの全体\n注意機構に加えて，次の３要素を含め，典型的には 20 から 24 層を成した深層ニューラルネットワークがトランスフォーマーの全てである．1\n\n\n\nTransformer Architecture (Vaswani et al., 2017)\n\n\n\n1.3.1 多頭注意\n以上の自己注意機構を１単位として，これを複数独立に訓練し，最終的にはこれらの線型結合を採用する仕組みを 多頭注意 (multi-head attention) という．\nこれにより，種々の文脈をより頑健に読み取ることが出来るようである．\n\n\n1.3.2 残差結合と正規化\n更に勾配消失を回避するために，残差結合 を導入し，訓練の高速化のために正規化 (Ba et al., 2016) が導入される．\nそして，モデルを大規模化していくには，この「多頭注意＋残差結合と正規化」のブロックを積み重ねる．\n\n\n1.3.3 多層パーセプトロン\n注意機構は線型性が高いため，多頭注意の層の間に，通常の Feedforward ネットワークもスタックして，ネットワークの表現能力を保つ工夫もされる．\n\n\n1.3.4 正規化レイヤーについての補足\nレイヤー正則化 (layer normalization) (Ba et al., 2016) は，バッチ正規化 (batch normalization) (Ioffe and Szegedy, 2015) が RNN にも適するようにした修正として提案された．\nバッチ正規化は，ニューラルネットワークの内部層の学習が，手前の層のパラメータが時事刻々と変化するために安定した学習が出来ないという 内部共変量シフト (internal covariate shift) にあると突き止め，これをモデルアーキテクチャに正規化層を取り入れることで解決するものである．\n正規化層は，ニューラルネットワークへの入力を，平均が零で分散が \\(1\\) になるように変換する．元々，ニューラルネットワークの入力を正規化してから学習させることで学習が効率化されることは知られていた (LeCun et al., 2012) が，バッチ正規化は，これをバッチごとに，かつ，モデルの内部にも取り込んだものである．\nバッチ正規化は精度の上昇と訓練の加速をもたらす．これはバッチ正規化により大きな学習率で訓練しても活性化が発散せず，これにより訓練時間の短縮と，局所解に囚われにくく汎化性能の向上がもたらされているようである (Bjorck et al., 2018)．\n\n\n\n1.4 なぜトランスフォーマーはうまく行くのか？\n注意機構は全体として線型変換になっている．これをカーネル法などを用いて非線型にする試みは多くあるが，これは成功していない．2\nその代わり，トランスフォーマーのパラメータ数のほとんどは FF 層（ Section 1.3.3 ）によるものであり，この層が大きな表現能力を持っていることが，トランスフォーマーの性能を支えていると考えられている．3\n注意機構は，遠く離れた２つのトークンを直接相互作用可能にすることに妙がある．実際，注意機構は，荷重行列を入力から学習するような，荷重平均プーリング (weighted mean pooling) ともみなせる．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#言語トランスフォーマー",
    "href": "posts/2024/Kernels/Deep2.html#言語トランスフォーマー",
    "title": "トランスフォーマー",
    "section": "2 言語トランスフォーマー",
    "text": "2 言語トランスフォーマー\nトランスフォーマーの訓練は，後述するように事前学習と事後調整からなる．事後調整は Section 3 で述べる．ここでは，事前学習を，言語を例に取って説明する．\nトランスフォーマーの事前学習とは レトロニム であり，トークン（≒単語）上の確率分布をモデリングをすることに他ならない．\n古典的には \\(n\\)-gram Section 2.2.1 などのモデルが用いられていたが，これをニューラルネットワークによって作ることはトランスフォーマー以前から試みられていた (Bengio et al., 2000)．\nその後，トランスフォーマーの登場まで，これには RNN Section 2.2.2 が主に用いられていた．しかし，RNN は長い系列に対しては勾配消失とボトルネック問題が起こりやすく，また，訓練の並列化が難しいという問題があった．\n\n2.1 言語の取り扱い\n\n2.1.1 単語の分散表現\n言語をそのまま扱うのではなく，トークン \\(x^n\\in\\mathbb{R}^D\\) の形に符号化する必要がある．\n言語には他にも改行や数式，コンピューターコードがあるが，まずは単語の表現を考える．\n単語を Euclid 空間内に埋め込んだものを 分散表現 (distributed representation) という．これを２層のニューラルネットワークで行う技術が word2vec である (Tomas Mikolov et al., 2013)．\nその訓練法には２つあり，窓の幅を \\(M=5\\) などとすると，\n\nCBOW (Continuous Bag of Words)：前後 \\(M\\) 語のみを見せて，中央の語を予測する．\nContinuous Skip-gram：中央の語を見せて，前後 \\(M\\) 語を予測する．\n\nという，いずれも教師なしの方法によって学習される．\n\n\n2.1.2 トークン化\nバイトペア符号化 (BPE: Byte Pair Encoding) (Sennrich et al., 2016) は，データ圧縮の手法であるが，単語に限らず種々のデータを含んだ文字列を符号化するのに用いられる．\n\n\n2.1.3 位置情報符号化\nトランスフォーマーはそのままではトークンの順番を考慮しないため，トークンの順番の情報も符号化時に含める必要がある．これを 位置情報符号化 (positional encoding) という (Dufter et al., 2021)．\nこのようにして，位置情報はトランスフォーマーのモデル構造を修正して組み込むのではなく，符号化の段階で組み込み，トランスフォーマーはそのまま使うのである．\nこれは，位置情報をトークンと同じ空間に埋め込んだ表現 \\(r^n\\) を学習し， \\[\n\\widetilde{x}^n:=x^n+r^n\n\\] を新たな符号とする．4\n\n\n\n2.2 従来の言語モデル\n文章をトークン列 \\(\\{x^n\\}_{n=1}^N\\subset\\mathbb{R}^D\\) に置き換えたあとに，この上の結合分布 \\(p(x^1,\\cdots,x^N)\\) をモデリングすることが，言語モデル の目標である．\n\n2.2.1 \\(n\\)-gram\n\\(n\\ge1\\) とし，\\(x_i=0\\;(i\\le0)\\) として， \\[\np(x_1,\\cdots,x_N)=\\prod_{i=1}^Np_{\\theta_i}(x_i|x_{i-n},\\cdots,x_{i-1})\n\\] という形で \\(p\\) をモデリングする．\nこれを \\(n\\)-gram モデルと呼ぶが，文章の長さ \\(N\\) が大きくなると，必要なパラメータ \\(\\theta_n\\) の数が増加する．\nこれに対処する方法としては，隠れ Markov モデル を用いることが考えられる．\nニューラルネットワーク (Bengio et al., 2000) を用いることも出来る．しかし，依存の長さ \\(n\\ge1\\) が固定されていることはやはり問題である．\n\n\n2.2.2 RNN\n長さ制限のない長期的な依存関係を Feedback network によって表現することが，(T. Mikolov et al., 2010) によって試みられた．\nこれは通常のニューラルネットワーク (FFN: Feedforward Network と呼ばれる) に，出力の一部を次の入力に使うという回帰的な流れを追加することで，隠れ Markov モデルのように次に持ち越される内部状態を持つことを可能にしたモデルである．\nしかしこれは学習が困難であることと，結局長期的な依存関係は効率的に学習されないという２つの問題があった．\n誤差の逆伝播を時間に対しても逆方向に繰り返す必要がある (Backpropagation through time) ので，長い系列に対しては逆伝播しなければいけない距離が長く，勾配消失・爆発が起こりやすい．これは長期的な依存関係を学習しにくいということももたらす．5 また，並列化も難しく，大規模なモデルの学習は難しい．\nこれに対処するために，モデルの構造を変えて過去の情報を流用しやすくする方法も種々提案された．LSTM (Long short-term memory) (Hochreiter and Schmidhuber, 1997) や GRU (Gated Recurrent Unit) (Cho et al., 2014) などがその例である．\n\n\n\n2.3 トランスフォーマーによる言語モデルとその訓練\nトランスフォーマーによる言語モデルの最大の美点は，自己教師あり学習による言語モデルの学習が可能である点である．これによりインターネットに蓄積していた大量のデータが利用可能になる．\nパラメータを自己教師あり学習により初期化することで，言語モデルの性能が大幅に改善できることは (Dai and Le, 2015) が LSTM 入りの RRN における実験を通じて最初に指摘したようである．\nラベルデータを必要とするならば，これは本当の意味でスケーラブルではなかったであろう．\n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) は，双方向エンコーダーである．\nGPT (Generative Pre-trained Transformer) (Radford et al., 2018) は，単方向デコーダーである．\nBART (Bidirectional and Auto-Regressive Transformer) (M. Lewis et al., 2020) は，双方向エンコーダーと単方向デコーダーの両方を持つ．\n\n\n2.3.1 デコーダーのみの言語モデル\nGPT などの生成モデルは，デコーダー部分のトランスフォーマーの機能を主に用いている．\nこれはまず，\n\nトークン列 \\((x^n)_{n=1}^{N-1}\\) を入力し，条件付き分布 \\(p(x^N|x_1,\\cdots,x^{N-1})\\) を得る．\n分布 \\(p(x^N|x_1,\\cdots,x^{N-1})\\) からサンプリングをする．\n\nの２段階で行われる．こうして \\((x^n)_{n=1}^N\\) を得たら，次は \\(x^{N+1}\\) を生成し，文章が終わるまでこれを続けることで，最終的な生成を完遂する．\n\n2.3.1.1 条件付き分布の表現\n大規模なデータセットの上で，文章を途中まで読み，次のトークンを推測する，という自己教師あり学習を行うことで，トークン上の条件付き分布を学習する．\nこの際に，先のトークンの情報は使わないように，注意機構を工夫 (masked / causal attention) して訓練する．\n\n\n2.3.1.2 条件付き分布からのサンプリング\n仮に最も確率の高いトークンを毎回選択する場合，出力は決定論的であり，同じ表現を繰り返すことが多くみられる．\n実は，より人間らしい表現は，確率の低いトークンもかなら頻繁に採用される (Holtzman et al., 2020)．\nかと言って，純粋なサンプリングをしたのでは，文章全体から見て意味をなさない場合も多い．\nこれを解決したのが top-\\(p\\) sampling / nucleus sampling (Holtzman et al., 2020) である．\nGPT-2 にも実装されている ようである．\n\n\n\n2.3.2 エンコーダーのみの言語モデル\nBERT (bidirectional encoder representations from transformers) (Devlin et al., 2019) などの言語理解モデルは，エンコーダ部分のトランスフォーマーの機能を主に用いている．その結果，生成は出来ない．\n訓練は，データセットから単語を確率的に脱落させ，これを補完するように訓練する．結果として，文章の前後両方 (bidirectional) の文脈を考慮するようになるのである．\n実際に使う際は，例えば感情の判別などでは，文章の冒頭に [class] などの特殊なトークンを置き，これをエンコーダーに通してトークンが何に置き換わるかを見ることで，判別を実行することができる．\n\n\n2.3.3 エンコーダー・デコーダーの言語モデル\nトランスフォーマーは原論文 (Vaswani et al., 2017) では，エンコーダーとデコーダーがセットになったモデルとして提案された．\nこれは機械翻訳を念頭に置いていたため，RNN の構造を引き継いだ形で提案されたためである．この場合，次のようにしてモデルは使われる\n\n入力 \\(X\\) をエンコーダーに通し，内部表現 \\(Z\\) を得る．\nこの内部表現 \\(Z\\) を元に，デコードした結果 \\(Y\\) を出力する．\n唯一，\\(Z\\) をデコーダーに渡す部分での注意機構層では，鍵と値としては \\(Z\\) を使うが，クエリとしては \\(Y\\) を使う．\n\n３の機構を エンコーダー・デコーダーの注意機構 (encoder-decoder / corss attention mechanism) といい，これによって \\(Z\\) と \\(Y\\) のトークンの間の類似度をモデルに取り入れる．\n\n\n\n2.4 近年の進展\n\n2.4.1 分布外データに対するロバスト性\nGPT-4 などの大規模言語モデルが，コンテクスト内学習 (in-context learning) が可能であることが，興味深い現象として解析されている．\nこの文脈内学習とは，パラメータをそのタスクに対して事後調整した訳でもないのに優れた性能を見せること を指す (Garg et al., 2022)．\n\n\n\nAn Example of In-context Learning (Bubeck et al., 2023)\n\n\nこれの理論的な解明が進んでいる．\nトランスフォーマーの注意機構をデータが通過する過程が，勾配降下を通じて局所モデルを学習する過程と等価になっている見方が指摘されている (von Oswald et al., 2023)．すなわち，トランスフォーマーを通過すること自体が，汎用的な学習そのものになっている (Akyürek et al., 2023), (Bai et al., 2023), (Guo et al., 2024), (Kim and Suzuki, 2024)．\n\n\n2.4.2 状態空間モデルによる依存構造モデリング\nトランスフォーマーの注意機構はデータ内の長期的な依存関係をモデリングできる点が革新的なのであった．\nしかし，そのシークエンスの長さに対して計算複雑性が非線型に増加してしまう点を改良すべく，種々の代替的なアーキテクチャが試みられており，中でも状態空間モデルを中間層に用いるモデルが注目されている．\nしかし，まだまだ長期的な依存関係の処理を苦手とするため，言語においては注意機構ほど性能が出ず，トランスフォーマーに比べて並列計算が難しいために実行速度が遅くなる，という問題がある．\nしかし，これらは解決可能で，トランスフォーマーの性能を凌駕する可能性があるとされている (Gu et al., 2022), (Fu et al., 2023)．まず，状態空間モデルのパラメータを入力から決めることで依存関係のモデリングを豊かにし（ 選択的状態空間モデル (Selective SSM)），並列可能なアルゴリズムも提案されている．\n実際に，選択的状態空間モデルを注意機構と多層パーセプトロン層の代わりに取り入れた Mamba (Gu and Dao, 2024) は同じサイズのトランスフォーマーの性能を凌駕する．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#sec-fine-tuning",
    "href": "posts/2024/Kernels/Deep2.html#sec-fine-tuning",
    "title": "トランスフォーマー",
    "section": "3 基盤モデル",
    "text": "3 基盤モデル\n大規模なトランスフォーマーを，インターネットに蓄積していた大量のデータを用いて訓練することにより得るモデルは，チャットボットや感情分析，要約など種々の下流タスクに少しの事後調整を施すだけで抜群の性能を発揮することが発見された．\nこれを 基盤モデル という．\n\n3.1 大規模言語モデル\n\n3.1.1 名前の由来と背景\n自然言語処理にトランスフォーマーを応用した例は大きな成功を見ている．GPT (Radford et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023b) のシリーズはその代表であり，特に GPT-4 はその文脈内学習能力（第 2.4.1 節）の高さから AGI の実現に向けた重要な一歩とも評されている (Bubeck et al., 2023)．\nその成功は，アーキテクチャとして優れているという点よりもむしろ，並列化が可能であり GPU などの計算資源を効率的に使える (Weng and Brockman, 2022) という点にあり，アーキテクチャの改良よりも計算資源の増強が最終的に大きな進歩をもたらすという側面が大きい，という認識が優勢になっている (R. Sutton, 2019)．これはスケーリング則として理論的にも理解が試みられている (Kaplan et al., 2020)．\nこの観点から，トランスフォーマーを用いた事前学習済みの言語モデルが，種々のタスクをほとんど例示なし (few-shot / zero-shot) で解ける能力を創発する程度に大きい場合，その規模が意味を持つことを強調して，大規模言語モデル (LLM: Large Language Model) とも呼ぶ (Zhao et al., 2023)．\n\n\n3.1.2 最適なモデルサイズ\n従来の LLM は，訓練データに対してモデルが大規模すぎる 可能性があることが (Hoffmann et al., 2022) で指摘された．\n\nSize of LLMs\n\n\n\n\n\n\n\nModel\nParameters\nTraining Tokens\n\n\n\n\nLaMDA (Thoppilan et al., 2022)\n137B\n168B\n\n\nGPT-3 (Brown et al., 2020)\n175B\n300B\n\n\nJurassic (Lieber et al., 2021)\n280B\n300B\n\n\nGopher (Rae et al., 2021)\n280B\n300B\n\n\nChinchilla\n70B\n1.4T\n\n\n\n最適なパラメータ-学習データサイズの比を考慮して設計された Chinchilla (Hoffmann et al., 2022) は，モデルのサイズは最も小さいにも拘らず，種々の下流タスクに対して，他のモデルを凌駕することが (Hoffmann et al., 2022) で報告されている．\n\n\n\nNumber of Training Tokens BabyLM Challenge\n\n\n\n\n\n\n\n\n種々の LLM とマルチモーダル化\n\n\n\n\n\nGoogle の GShard (Lepikhin et al., 2021)，PaML (Chowdhery et al., 2022)，M4 (Aharoni et al., 2019)，Google Brain の Switch Transformers (Fedus et al., 2022)，Google DeepMind の Gopher (Rae et al., 2021) などがある．\n最近のものでは，\n\nGoogle の LaMDA (Thoppilan et al., 2022) は会話に特化した LLM である．\nGoogle から 12/6/2023 に Gemini (Team et al., 2023) が発表され，2/16/2024 には Gemini 1.5 が発表された．\n\nこれに伴い，Bard と Duet AI はいずれも Gemini に名称変更された．\nGShard (Lepikhin et al., 2021) 同様，トランスフォーマーに加えて，新しいアーキテクチャである Sparsely-Gated MoE (Shazeer et al., 2017) が用いられている．これはモデルのパラメータを分割し（それぞれを専門家 expert という），１つの入力にはその一部分しか使わないようにすることでメモリを節約し並列化を可能にする手法である．\n文書から高精度にテキストを抽出する LMDX (Language Model-based Document Information Extraction and Localization) (Perot et al., 2023) も用いられている．\n\nOpenAI から 9/5/2023 に GPT-4V (OpenAI, 2023c) が発表され，ChatGPT にも実装された．\n\nMicrosoft の研究者も，GPT-4V の出来ること関する考察 (Yang et al., 2023) を発表している．\n\n\n\n\n\n\n\n3.1.3 訓練の並列化\nここまで大規模なモデルだと，訓練時の GPU の並行計算を適切に計画することが肝心になる．\n\n\n\nfour types of parallelism (Weng and Brockman, 2022)\n\n\nByteDance の MegaScale (Jiang et al., 2024) は，12,288 の GPU を用いながら，55.2 % の Model FLOPs Utilization を引き出した．\n更なる LLM の訓練の効率化には，GPU による並列計算におけるボトルネックである メモリ帯域幅 を克服するために，分散型訓練手法を採用することが提案されている．\n\n\n\n3.2 「基盤モデル」と事後調整\nGPT の P とは Pre-trained である．自己教師あり学習によって 事前学習 をしたあと，その後のタスクに応じて，教師あり学習によって 事後調整 (fine-tune) を行う．6\n事後調整では，目的関数に KL 乖離度を入れるなどして，元のモデルから遠く離れすぎないように工夫されている．\n事後調整を行う前の大規模言語モデルのことを，種々の応用や下流タスク (downstream task) の基礎となるモデルであることと，そのものでは未完成であることとを強調して，基盤モデル (foundation model) とも呼ばれる (Bommasani et al., 2021)．\n事後調整では，モデルの全体では規模が大きすぎるため，出力層の後に新しいニューラルネットを付加したり，最後の数層のみを追加で教師あり学習をしたりする方法が一般的である．または，LoRA (Low-Rank Adaptation) (E. J. Hu et al., 2021) では，トランスフォーマーの各層に新たな層を挿入し，これを学習する．\nこれは，事後調整に有効な内的次元は実際には小さく (Aghajanyan et al., 2021)，これに有効にアクセスし，効率的な事後調整を行うことが出来るという．さらに (Zhou et al., 2023) によると，事後調整に必要なラベル付きデータは，量よりも質が重要であり，LLaMA 5.1 に対しても多くて 1000 データで十分であるようである．\n事後調整には，他にも，ChatGPT のようなサービスを展開するために必要なユーザー体験の改善を目的としたものも含まれる．これは アラインメント とも呼ばれ，強化学習が用いられることが多い．実際，GPT-4 では 人間のフィードバックによる強化学習 (RLHF: Reinforcement Learning through Human Feedback) (Christiano et al., 2017) が用いられている (OpenAI, 2023b, p. 2)．\nこれについては第 3.5 節で改めて論じる．．\n\n\n3.3 プロンプトエンジニアリング\n基盤モデルには世界と人間に対する膨大な知識が含まれているが，使い方によって大きく性能が変わる．正しい条件付けを行うことで，内部に存在する知識をうまく引き出すことができる．これを大規模言語モデルでは prompt engineering (Liu et al., 2023) という．プロンプトの送り方によって性能がどう変わるかを調べる新たな分野である．\nその結果，プロンプト内で新たなタスクを定義するだけで，またはいくつか例を与えるだけで，これが解けてしまうこともわかっており，これを zero-shot または few-shot learning という．\n\n\n3.4 RAG\nLLM は世界に関する正確な知識を持っており，知識ベースとしての利用も期待されている (Petroni et al., 2019)．7 しかし，知識を正確に，そして信頼出来る形で引き出すことが難しいのであった．\n特に，出典を示すことや，最新の知識のアップデートなどが難題として待っている．\nそこで，LLM に（自由に外部情報を探索できるという意味で）ノンパラメトリックな知識ベースを接続することで解決するのが RAG (Retrieval-Augmented Generation) モデル (P. Lewis et al., 2020) である．\nDPR (Dense Passage Retriever) (Karpukhin et al., 2020) は文書を密に符号化する手法を開発し，これを用いて文書検索をすることで Q&A タスクを効率的に解く手法を提案した．このような文書の符号化器は 検索器 (retriever) と呼ばれる．\nRAG (P. Lewis et al., 2020) はこの検索器を BART (M. Lewis et al., 2020) に接続した．\nREALM (Retrieval-Augmented Language Model) (Guu et al., 2020) も同時期に提案されている．\nMeta での研究 (Yasunaga et al., 2023) はこの検索器を Text-to-Image トランスフォーマー である CM3 (Aghajanyan et al., 2022) と結合することで，初めて言語と画像の両方を扱える RAG モデル RA-CM3 (retrieval-augmented CM3) を構成した．\nWebGPT (Nakano et al., 2022) は，RAG や REAML が文書検索をしているところを，Web 検索を実行できるようにした GPT-3 (Brown et al., 2020) の事後調整である．\n\n\n3.5 アラインメント\nLLM などの機械学習モデルを訓練する際の目的関数は，そのままでは人間社会が要請するものとずれがあることが多い．これを修正するような試みを アラインメント (alignment) という．\n\nFor example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. (Ouyang et al., 2022)\n\n加えて，人間の選好は，0-1 損失関数で表現できるものではないことが多い．そこで，強化学習を用いることが考えられた．しかし，一々人間がフィードバックを与える方法はスケーラビリティに深刻な問題があるため，「人間の選好」をモデリングするニューラルネットワークを 代理モデル (surrogate model) として構築することも考える．\nその代表的な手法が 人間のフィードバックによる強化学習 (RLHF: Reinforcement Learning through Human Feedback) (Christiano et al., 2017) である．\nInstructGPT (Ouyang et al., 2022) は OpenAI API を通じて寄せられたフィードバックを用いて，PPO (Proximal Policy Optimization) アルゴリズム (Schulman et al., 2017) による強化学習により事後調整をしたものである．\n近接ポリシー最適化 (PPO: Proximal Policy Optimization) アルゴリズム (Schulman et al., 2017) は 信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) (Schulman et al., 2015) の洗練化として提案されたもので，現在の RLHF においても最も広く使われている手法である (Zheng et al., 2023)．\nInstructGPT が ChatGPT の前身となっている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#sec-multimodal-transformer",
    "href": "posts/2024/Kernels/Deep2.html#sec-multimodal-transformer",
    "title": "トランスフォーマー",
    "section": "4 多相トランスフォーマー",
    "text": "4 多相トランスフォーマー\nトランスフォーマーは自然言語処理の文脈で開発されたが，画像や動画，音声 (Radford et al., 2023a)，さらにはプログラミング言語 (Chen et al., 2021) にも適用されている．\n動画はまだしも画像には，直感的には時系列構造がないように思えるが，トランスフォーマーはもはや汎用のニューラルネットワークアーキテクチャとして使用できることが解りつつある．\nそれぞれの応用分野で モデルの構造は殆ど差異がなく，トークン化の手法などに差異があるのみのように見受けられる．8\n\n4.1 画像認識トランスフォーマー (ViT)\n画像の分類問題を解くためのエンコーダ・トランスフォーマーは ViT (Vision Transformer) (Dosovitskiy et al., 2021) と呼ばれており，ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) では未だ ResNet 系のモデルが優勢であった 2021 年に，これを超える性能を示した．\n実はモデルは殆どトランスフォーマーそのままであり，肝要であったのは画像をトークン化である．ピクセルをそのまま用いるのではなく，ある程度大きなピクセルの集合である パッチ (patch) を用いることで，計算量を下げる．(Dosovitskiy et al., 2021) では \\(16\\times16\\) サイズなどが採用された．\n一方で，画像を恣意的に系列化しているため，幾何学的な構造は１から学ぶ必要があり，最初からモデルに組み込まれている CNN よりは一般に多くの訓練データを必要とする．だが，これにより帰納バイアスが弱いということでもある．9\nViT はその後，動画も扱える ViViT (Arnab et al., 2021), あらゆるアスペクト比に対応する NaViT (Native Resolution ViT) (Dehghani et al., 2023) などの拡張が続いた．\nDeepMind の Perceiver (Jaegle et al., 2021) は画像，動画，音声のいずれのメディアの分類問題にも対応可能な，非対称な注意機構を持つトランスフォーマーである．\n\n\n4.2 画像生成トランスフォーマー\n一方でデコーダートランスフォーマーを用いて，画像の生成モデリングを行った最初の例は ImageGPT (Chen et al., 2020) である．\nなお，自己回帰的な生成モデルを通じて画像の生成を試みることは，CNN (van den Oord, Kalchbrenner, Vinyals, et al., 2016) や RNN (van den Oord, Kalchbrenner, and Kavukcuoglu, 2016) でも行われていた．\nこの際に判明したことには，画像の分類タスクでは連続表現が役に立っても，生成タスクでは高い解像度を持った画像の生成が難しく，離散表現が有効であることが知られている．しかしこれではデータ量が増えてしまうため，画像の ベクトル量子化 が行われることが多い．\nそこで，ImageGPT (Chen et al., 2020) でも \\(K\\)-平均法によるクラスタリングが行われており，さらに VQ-VAE を用いたデータ圧縮も行われている．\nImageGPT (Chen et al., 2020) では最終的に各ピクセルを one-hot 表現にまで落とし込み，これを GPT-2 モデル (Radford et al., 2019) につなげている．\nMUSE (Chang et al., 2023) も，トランスフォーマーを用いた画像生成モデルの例である．\n\n4.2.1 拡散モデルとの邂逅\n潜在拡散モデル で U-Net (Ronneberger et al., 2015) を用いていたところをトランスフォーマーに置換した 拡散トランスフォーマー (DiT: Diffusion Transformer) (Peebles and Xie, 2023) が発表された．\nその後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) (Ma et al., 2024) が発表された．\n\n\n\n4.3 Text-to-Image トランスフォーマー\nこの分野は (Reed et al., 2016) 以来，初めは GAN によるアプローチが試みられていた．\nGPT-3 (Brown et al., 2020) と ImageGPT (Chen et al., 2020) とは殆ど同じモデルを用いている．これらを組み合わせたデコーダー型のトランスフォーマーが DALL-E (Ramesh et al., 2021) である．10\nトークン化して仕舞えば，言語も画像も等価に扱えるというのである．Google の Parti (J. Yu et al., 2022) も同様のアプローチである．\nMeta の CM3 (Aghajanyan et al., 2022) と CM3leon (L. Yu et al., 2023) は画像と言語を両方含んだ HTML ドキュメントから学習している．\nGoogle DeepMind の Flamingo (Alayrac et al., 2022) は画像から言語を生成する．\n\n\n4.4 Image-to-Text トランスフォーマー\nOpenAI の CLIP (Contrastive Language-Image Pre-training) (Radford et al., 2021) は画像の表現学習をする視覚モデルである．これは DALL-E (Ramesh et al., 2021) と同時に開発された重要な構成要素である．\n一方で DALL-E2 (Ramesh et al., 2022) では，CLIP により画像を潜在空間にエンコードし，拡散モデルによってデコードする．\nDALL-E3 (OpenAI, 2023a) もその改良である．\n\n\n4.5 動画生成トランスフォーマー\n動画を画像の連続と見てトランスフォーマーを応用するアプローチは Latte (Latent Diffusion Transformer) (Rakhimov et al., 2020) に始まる．\nVideoGPT (Yan et al., 2021) では動画を 3D の CNN でデータ圧縮，VQ-VAE で量子化して離散的な潜在表現を得た後，GPT と殆ど似たトランスフォーマーに通して学習する．\nWayve の GAIA-1 (Generative AI for Autonomy) (A. Hu et al., 2023) も同様の手法で動画を生成しているが，その動画を用いて自動運転の強化学習に応用する点が画期的である．\nOpenAI は 2/15/2024 に Sora (Brooks et al., 2024) を発表した．これも 潜在拡散モデル (Rombach et al., 2022) 同様，自己符号化器による動画の潜在表現を得た上でパッチに分割し，この上で拡散トランスフォーマー (Peebles and Xie, 2023) の学習を行う．\n\n\n4.6 世界モデルとしてのトランスフォーマー\nトランスフォーマーを世界モデルとして用いて，シミュレーションを行い動画を生成し，これをモデルベースの強化学習 (R. S. Sutton and Barto, 2018) の材料とすることが広く提案されている．これは learning in imagination (Racanière et al., 2017) と呼ばれる．11\nIRIS (Imagination with auto-Regression over an Inner Speech) (Micheli et al., 2023) はこれに初めてトランスフォーマーを用いた世界モデルから動画生成をした．\nGAIA-1 (Generative AI for Autonomy) (A. Hu et al., 2023) は自動運転に特化した世界モデルを，トランスフォーマーを用いて構築している．\n他にも，動画生成を強化学習に応用する例としては，OpenAI による VPT (Video Pre-Training) (Baker et al., 2022) がある．\n\n\n4.7 音声生成トランスフォーマー\nOpenAI の Jukebox (Dhariwal et al., 2020) は，VQ-VAE を用いて音声データを圧縮・量子化し，トランスフォーマーに通したものである．\nこのトランスフォーマーは Sparse Transformer (Child et al., 2019) という，注意機構の計算効率を改良したモデルを用いている．\n\n\n4.8 Text-to-Speech トランスフォーマー\nMicrosoft Research の VALL-E (C. Wang et al., 2023) は，音声データをベクトル量子化によって言語データと全く同等に扱うことで，トランスフォーマーを用いて音声生成を行っている．\n\n\n4.9 Speach to Text トランスフォーマー\nOpenAI の Whisper (Radford et al., 2023b) は encoder-decoder 型のトランスフォーマーを用いている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#近年の動向",
    "href": "posts/2024/Kernels/Deep2.html#近年の動向",
    "title": "トランスフォーマー",
    "section": "5 近年の動向",
    "text": "5 近年の動向\n\n5.1 LLaMA の一般公開とその影響\nMeta AI が 7/18/2023 に LLM LLaMA (Touvron et al., 2023) を公開した．そして API を通じて利用する形ではなく，そのモデルのウェイトが公開されたため，Stanford 大学の Alpaca など，モデルの改良と研究が促進されている．\n特に事後調整のための公開データセットの整備が進んでおり，Alpaca では Self-Instruct (Y. Wang et al., 2023) による効率的な alignment 技術が採用されている．\n産業界でも影響は大きい．ELYZA は 12/27/2023 に日本語に特化した LLM である ELYZA-japanese-Llama-2-13b を公開している．Stockmark も 10/27/2023 に Stockmark-13b を公開している．\nいずれも，開発費と開発時間が大幅に圧縮されたという．12\nIBM は 9/12/2023 に LLM Granite を発表している．加えて，プラットフォーム watsonx も提供しており，その上で RAG など独自の事後調整を可能にしている．\nIBM と Meta の２社が発起人となり，12/5/2023 に AI Alliance が発足し，オープンイノベーションを推進している．\nStable Diffusion (Rombach et al., 2022) もソースコードとウェイトが 一般公開 されている．\n\n\n5.2 LLM の経済的影響\n(Tamkin et al., 2021) は早い段階での OpenAI と Stanford 大学 HAI (Human-centered AI) との対談録である．\nOpenAI はコード生成能力の経済的な影響を重要なアジェンダとしている (Manning et al., 2022)．\nOpen AI の Codex (Chen et al., 2021) はプログラム言語を扱うトランスフォーマーであり，GitHub Copilot の元となっている．これが社会に与える影響も，新たな評価フレームワークと共に提案されている (Khlaaf et al., 2022)．\nLLM の労働市場へのインパクトも推定している (Eloundou et al., 2023)．これによると，アメリカの労働者の 80% が，LLM の導入により少なくとも仕事の 10% に影響が生じるとしている．さらに全体の 20% は仕事の半分以上が影響を受けるとしている．\n\n\n5.3 世界モデルとしての基盤モデル\n\n5.3.1 社会行動シミュレーターとしての LLM\n社会的なシミュレーションを LLM 内で行うことで，社会科学やビジネスの場面での意思決定を支援することが期待されている．\nLLM は人間の心の理論を理解し，その心情・意図を（ある程度）シミュレートすることが出来るようである (Andreas, 2022)．\nLLM でのシミュレーションを通じて，社会科学的な知識を引き出そうとする試みもある (Leng and Yuan, 2023)．\n\n\n\n5.4 LLM と経済安全保障\n\n5.4.1 幻覚の防止\nLLM が事実と異なる物語を生成することを 幻覚 (hallucination) と呼び，一部の応用では問題になることがある．\nこれを解決するにあたって，等角推測 (conformal prediction) と組み合わせ，出力の不確実性を評価することで幻覚を防止する手法が提案されている (Mohri and Hashimoto, 2024)．\n一般に意思決定の場面において AI を活用するには，不確実性の定量化が必要不可欠である．\nGPT-3 を Bayesian にし，自身の確証度合いを言表するように事後調整する研究が OpenAI で行われている (Lin et al., 2022)．\n\n\n5.4.2 ウォーターマーク\nウォーターマークを開発することで，LLM から出力された文章であることを高確率で検出できるようにする方法が，統計的仮説検定の技術を応用して提案されている (Kuditipudi et al., 2023)．\n\n\n5.4.3 偽情報対策\n生成 AI は，一国の政府が特定のプロパガンダを流布するための効果的な手段として選ばれることになる．その際の考え得る使用例と，それに対する対策が考えられてる (Goldstein et al., 2023)．\n\n\n5.4.4 開発規制\n(Anderljung et al., 2023) は先端的な AI を Frontier AI と呼び，これの開発過程におけるあるべき規制を模索している．監督当局に執行権を付与することやフロンティアAIモデルのライセンス制度などが議論されている．\n(Shoker et al., 2023) は LLM と国家安全保障との関係を議論している．信頼構築措置 (CBMs: Confidence-Building Measures) とは，国家間の敵意を減少させることで，衝突のリスクを減らす措置の全般をいう．元々は冷戦時代に提案された概念であるが，これを LLM 開発に適用することが具体的に提案されている．\n\n\n5.4.5 生物学的脅威\nLLM の登場により個人がエンパワーメントを受けており，生物学的脅威を作る障壁が低下していることは間違いない．\n(Patwardhan et al., 2024) では，生物学的リスクに焦点を当てて，AI による安全リスク評価の手法と事前警鐘システムを模索している．この研究では，LLM によりリスクが増加するという統計的に有意義な証拠は得られていないが，この方面の研究の草分けとなっている．\n\n\n\n5.5 アラインメント問題\nDALL-E2 では訓練前の緩和策も取られている (Nichol, 2022)．\n\n5.5.1 プログラムの支援\n遺伝的プログラムの改良の過程を模倣できる (Lehman et al., 2024) として，ソフトウェア開発やロボット開発分野での，プログラムの漸次的改良への応用が考えられている．\n困難なタスクに対して AI がアシストするという研究もある (Saunders et al., 2022)．これは最終的に，AI のアラインメントにおいても重要な技術になるとしている．\n\n\n5.5.2 超アラインメント問題\nこれは，OpenAI のアラインメント研究が次の３本の柱であることが背景にある (Leike et al., 2022)\n\n人間のフィードバックによる AI の強化学習\nAI の支援を通じて人間のフィードバックを正確にする\nAI を通じてアラインメントの研究を促進する\n\nの３つである．\n第３の柱として，GPT-4 (OpenAI, 2023b) によるシミュレーションを通じて，特定のニューロンがどのような出力に対応しているかを解明する手法を提案している (Leike et al., 2023)．これにより，人間が直接調べる行為が自動化され，アラインメントの研究が効率化され，スケーラブルな手法になるということである．\nさらに，将来的なアラインメントは RLHF では出来なくなっていく．人智を超えた超知能 (superintelligence) をアラインメントすることを，超アラインメントと呼び，OpenAI は 2023 年の暮れに 超アラインメントチーム を創設し，人間が「弱い監督者」となってしまった状況でもどのように超アラインメントを実行すれば良いかを研究するとしている．\n\n\n5.5.3 AI エージェントへの道\nOpenAI は能動的 AI システム (Agentic AI system) の構築に向けて，安全な運用と責任のある管理を目指す白書 (Shavit et al., 2023) を発表した．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep2.html#footnotes",
    "href": "posts/2024/Kernels/Deep2.html#footnotes",
    "title": "トランスフォーマー",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n層の数などの表現は (Hashimoto, 2024) から．↩︎\n(Hashimoto, 2024) で聞きました．↩︎\n(Hashimoto, 2024) で聞きました．↩︎\nもちろん結合することも考えられているが，加算を行うことが現状の多数派であるようである (Hashimoto, 2024)．↩︎\n勾配の爆発に対しては gradient clipping などの対症療法が用いられる．↩︎\n後述のアラインメントも事後調整の一つであるが，これと区別して，教師ありの事後調整 (supervised fine-tuning) とも呼ばれる．↩︎\nパラメトリックな知識ベースとしての利用については (Raffel et al., 2020)，(Roberts et al., 2020) など．一方で (Marcus, 2020) などは，hallucination などの欠点を補う形で，古典的な知識ベースと連結したハイブリット型での使用を提案している．↩︎\nモデルの比較は (Raffel et al., 2020) などが行っている．↩︎\nトークン化に小規模な CNN を用いてデータ圧縮を行うこともある．↩︎\nすなわち，DALL-E は GPT-3 のマルチモーダルな実装である (Tamkin et al., 2021, p. 4)．↩︎\n(Ha and Schmidhuber, 2018) は RNN により世界モデルを構築している．(Kaiser et al., 2020) は動画から Atari を学習している．(Hafner et al., 2021) はさらに性能が良い．↩︎\n日経新聞 (2/19/2024)↩︎"
  },
  {
    "objectID": "static/PartialCategories.html#sampling",
    "href": "static/PartialCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\n生成＝サンプリング＝シミュレーション＝ Monte Carlo 法が人類にもたらしたもの\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Notations.html",
    "href": "static/Notations.html",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "",
    "text": "Do not laugh at notations; invent them, they are powerful. In fact, mathematics is, to a large extent, invention of better notations. — The Feynman Lectures on Physics, Vol.1, Ch.17, Sec. 5."
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion1.html",
    "href": "posts/2024/Samplers/Diffusion1.html",
    "title": "雑音除去拡散過程",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion1.html#sec-DDPS",
    "href": "posts/2024/Samplers/Diffusion1.html#sec-DDPS",
    "title": "雑音除去拡散過程",
    "section": "1 雑音除去拡散による事後分布サンプリング (DDPS)",
    "text": "1 雑音除去拡散による事後分布サンプリング (DDPS)\n潜在空間 \\(\\mathcal{X}\\) 上の事前分布 \\(\\mu\\) と，尤度が確率核 \\(\\mathcal{X}\\to\\mathcal{Y}\\) \\[\nx\\mapsto g(y|x)\\,dy\n\\] の形で与えられているとする．\n\n1.1 雑音除去拡散 (DD)\n拡散模型 は，次で定まる OU 過程によってデータ分布を \\(\\mathrm{N}_d(0,I_d)\\) にまで破壊しているとみなせる (Y. Song et al., 2021)： \\[\ndX_t=-\\frac{1}{2}X_t\\,dt+dB_t,\\qquad X_0\\sim p(x|y).\n\\]\nただし，この過程は指数エルゴード性を持つと言っても，完全に \\(\\mathrm{N}_d(0,I_d)\\) に従うようになるのは \\(t\\to\\infty\\) の極限においてである．この極限においては，\\(p(x|y)\\) はもやは \\(y\\) に依らなくなる．\nこの \\((X_t)\\) の有限時区間 \\([0,T]\\) における時間反転は，\\((X_t)\\) の密度を \\(p_t(x_t|y)\\) で表すと， \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log p_{T-t}(Z_t|y)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T|y),\n\\tag{1}\\] の弱解になる (Anderson, 1982), (Haussmann and Pardoux, 1986)．この \\((Z_t)_{t\\in[0,T]}\\) を 雑音除去拡散 (Denoising Diffusion) という．\n\n\n1.2 \\((Z_t)\\) からのサンプリング\nすると残りの問題は，拡散過程 \\((Z_t)_{t\\in[0,T]}\\) からのサンプリングになるが，これは \\(\\log p_{T-t}(Z_t|y)\\) という項の評価と \\(p_T(x_T|y)\\) からのサンプリングが必要である．\n\\((Z_t)\\) を \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+s_{T-t}^\\theta(Z_t,y)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] で近似することが (Y. Song et al., 2021) の方法である．思い切って \\(\\mathrm{N}_d(0,I_d)\\approx p_T(x_T|y)\\) としてしまい，\\(s_t^\\theta(x_t,y)\\) のモデリングに特化するのである．\nこの過程 \\((Z_t)\\) が定める測度を \\(\\mathbb{Q}_y^\\theta\\in\\mathcal{P}(C([0,T];\\mathcal{X}))\\) と表すと，訓練目標は KL 乖離度の期待値 \\[\\begin{align*}\n    \\mathcal{L}(\\theta)&:=2\\operatorname{E}\\biggl[\\operatorname{KL}\\biggr(\\mathbb{P}_Y,\\mathbb{Q}_Y^\\theta\\biggl)\\biggr]\\\\\n    &=\\int^T_0\\operatorname{E}\\biggl[\\left\\|s^\\theta_t(X_t,Y)-\\nabla_x\\log p_{t|0}(X_t|X_0)\\right\\|^2\\biggr]\\,dt+\\mathrm{const.}\n\\end{align*}\\] が考えられる．ただし，\\(\\mathbb{P}_Y\\) は \\((X_t)\\) の分布，\\(p_{t|0}\\) は \\((X_t)\\) の遷移密度を表す．この損失は DSM (Vincent, 2011) で与えられたものに等しい．\n\\[\n(X_0,Y)\\sim p(x,y)=g(y|x)\\mu(x)\n\\] からのシミュレーションが可能であるならば，この目的関数は確率的最適化アルゴリズムによって最適化できる．\nこうして，雑音除去拡散サンプラー (DDPS: Denoising Diffusion Posterior Sampler) を得る．\n\n\n1.3 近似ベイズ計算への応用\n事前分布と尤度 \\(g(y|x)\\) からのサンプリングが可能な状況は，生成モデリングの他に Simulation-based Inference などの近似推論でもあり得る．\n実際，この DDPS は従来の ABC (Approximate Bayesian Computation) 法の代替になり得る．\nさらに，拡散模型の加速法 （Progressive Distillation (Salimans and Ho, 2022) など）が DDPS にも応用可能である．\n\n\n1.4 逆問題への応用\nサンプルが画像だとしても，画像修復 (inpainting) や高解像度化 (super-resolution) などの逆問題応用が豊富に存在する．\nこのような，単一の \\(Y=y\\) を固定した状況で潜在変数 \\(X_T\\) からサンプリングをしたい場合では，\\(\\log p_t(x_t|y)\\) を一緒くたに \\(s^\\theta_{t}(x_t,y)\\) に取り替えてしまうのではなく，次の事前分布と尤度への分解に基づいて扱うこともできる：\n\\[\n\\nabla_x\\log p_t(x_t|y)=\\nabla_x\\log\\mu_t(x_t)+\\nabla_x\\log g_t(y|x_t),\n\\] \\[\n\\mu_t(x_t):=\\int_\\mathcal{X}\\mu(x_0)p_{t|0}(x_t|x_0)\\,dx_0,\\qquad g_t(y|x_t):=\\int_\\mathcal{X}g(y|x_0)p_{0|t}(x_0,x_t)\\,dx_0.\n\\]\nこの第一項は \\(s_t^\\theta(x_t)\\) により統一的にモデリングでき，同様に \\(X_0\\sim\\mu(x)\\) から始まる雑音化過程 \\((X_t)\\) の分布を \\(\\mathbb{P}\\) として \\(\\operatorname{KL}(\\mathbb{P},\\mathbb{Q}^\\theta)\\) 最小化問題として処理できる．\n\\(g_t(y|x_t)\\) の項も近似可能である．(Chung et al., 2023) では条件付き誘導が，(J. Song et al., 2023) では Monte Carlo 法が用いられている．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion1.html#sec-DSB-PS",
    "href": "posts/2024/Samplers/Diffusion1.html#sec-DSB-PS",
    "title": "雑音除去拡散過程",
    "section": "2 Schrödinger 橋による事後分布サンプリング (DSB-PS)",
    "text": "2 Schrödinger 橋による事後分布サンプリング (DSB-PS)\n\n2.1 導入\n\\(p_T(x_T|y)\\approx\\mathrm{N}_d(0,I_d)\\) の近似を成り立たせるために \\(T\\) を十分大きく取る必要がある問題は，OU 過程の代わりに Schrödinger 橋を用いることで解決できることが (Shi et al., 2022) で提案された．\nSchrödinger 橋自体は，(De Bortoli et al., 2021) などから拡散模型への応用は議論されていた．\n\n\n2.2 定義\nSchrödinger 橋 (SB) とは， \\[\n\\Pi^*:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}_0}\\operatorname{KL}(\\Pi,\\mathbb{P}),\n\\] \\[\n\\mathcal{P}_0:=\\biggl\\{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))\\,\\bigg|\\,\\Pi_0(x_0,y_0)=p(x_0,y_0),\\Pi_T(x_T,y_T)=\\mathrm{N}_d(0,I_d)p(y_T)\\biggr\\},\n\\] によって定まる確率分布に従う確率過程をいう．ただし，\\(\\mathbb{P}:=\\mathbb{P}_{y_0}\\otimes\\delta_{p(y)}\\) とした．\\(\\delta_{p(y)}\\) は次で定まる確率分布である： \\[\ndY_t=0,\\qquad Y_0\\sim p(y).\n\\]\nこれは表示 \\[\n\\Pi^*=\\mathbb{P}^*_{y_0}\\otimes\\delta_{p(y)}\n\\] を持つから，\\(Z_0\\sim\\mathrm{N}_d(0,I_d)\\) に従う過程 \\((Z_t)\\) をシミュレーションすることで， \\[\nZ_T\\sim\\Pi^*_0(x|y)=p(x_0|y)\\qquad p(y)\\text{-a.s.}\n\\] が成り立つ．\n\n\n2.3 SB のシミュレーション\nSB 問題の解 \\(\\Pi\\) は 逐次的比例フィッティング (IPF: Iterative Proportional Fitting) により得られる．\n\n2.3.1 IPF とは\nIPF アルゴリズムは離散的な形で (Deming and Stephan, 1940) が分割表データ解析の研究で提案している．その手続きw (Ireland and Kullback, 1968) が距離の最小化として特徴付け，(Kullback, 1968) が確率密度に対しても一般化した．ただし，この確率密度に対するアルゴリズムは (Fortet, 1940) が Schrödinger 方程式の研究ですでに提案しているものである．\nIPF は元々，指定した２つの確率ベクトル \\(r\\in(0,\\infty)^{d_r},c\\in(0,\\infty)^{d_c}\\) を周辺分布に持つ結合分布（カップリング）のうち，指定の行列 \\(W\\in M_{d_rd_c}(\\mathbb{R}_+)\\) に最も近い KL 乖離度を持つカップリングを見つけるための逐次アルゴリズムである (Kurras, 2015)．\n種々の分野で再発見され，複数の名前を持っているようである．例：Sheleikhovskii 法，Kruithof アルゴリズム，Furness 法，Sinkhorn-Knopp アルゴリズム，RAS 法など (Kurras, 2015)．\n\\(W\\) の成分が正である場合は，(Sinkhorn, 1967) がアルゴリズムの収束と解の一意性を示している．1\nしかし，\\(W\\) の成分が零を含む場合，零成分の位置に依存してアルゴリズムは収束しないことがあり得ることを，(Sinkhorn and Knopp, 1967) が \\(d_r=d_c=1\\) の場合について示している．\n\n\n2.3.2 アルゴリズム\nIPF アルゴリズムは，観念的には，２つの周辺分布のうち片方を制約に課しながら，KL 距離を最小にする射影を返していく：\n\\[\n\\Pi^{2n+1}:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))}\\biggl\\{\\operatorname{KL}(\\Pi,\\Pi^{2n})\\,\\bigg|\\,\\Pi_T=\\mathrm{N}_d(0,I_d)\\otimes p(y_T)dy_T\\biggr\\},\n\\] \\[\n\\Pi^{2n+2}:=\\operatorname*{argmin}_{\\Pi\\in\\mathcal{P}(C([0,T];\\mathcal{X}\\times\\mathcal{Y}))}\\biggl\\{\\operatorname{KL}(\\Pi,\\Pi^{2n+1})\\,\\bigg|\\,\\Pi_0(x_0,y_0)=p(x_0,y_0)\\biggr\\}.\n\\]\n今回の場合， \\[\n\\Pi^{2n+1}=\\mathbb{P}_{y_T}^{2n+1}\\otimes\\delta_{p(y)},\\qquad\\Pi^{2n+2}=\\mathbb{P}^{2n+2}_{y_0}\\otimes\\delta_{p(y)},\n\\] と分解される．ただし，\\(\\mathbb{P}_{y_T}^{2n+1}\\) は次で定まる \\((Z_t)\\) の時間反転 \\[\ndZ_t=f_{T-t}^{2n+1}(Z_t,y_T)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),f_t^{2n+1}(x_t,y):=-f_t^{2n}(x_t,y)+\\nabla_{x}\\log\\Pi^{2n}_t(x_t|y),\n\\] \\(\\mathbb{P}_{y_0}^{2n+2}\\) は次で定まる \\((X_t)\\) の経路測度となる： \\[\ndX_t=f^{2n+2}_t(X_t,y_0)\\,dt+dB_t,\\qquad X_0\\sim p(x|y_0)\\,dx,f_t^{2n+2}(x_t,y):=-f_t^{2n+1}(x_t,y)+\\nabla_x\\log\\Pi_t^{2n+1}(x_t|y).\n\\] ただし，\\(f^0_t(x_t)=-x_t/2\\)．\n\n\n2.3.3 DDPS との関係\n最初のイテレーション \\(n=0\\) における \\(\\mathbb{P}^1_y\\) が雑音除去拡散 (1) に対応する．\nしかし，IPF アルゴリズムのイテレーションを繰り返していくごとに，\\(T&gt;0\\) が十分に大きくない場合でも正確に \\(\\mathrm{N}_d(0,I_d)\\) にデータ分布を還元する SB が得られるようになっていく．\n\n\n2.3.4 DSB-PS\nこの際，スコア \\(\\nabla_z\\log p_{T-t}(Z_t|y)\\) から始まり，\\(f^{n}_t\\;(n\\ge2)\\) の推定も逐次的に行なわなければならない点については，mean-matching (De Bortoli et al., 2021), (Shi et al., 2022) という方法が考えられている．\nこの方法を用いて，IPF アルゴリズムが収束するまで実行して最終的に得るサンプラーを Schrödinger 橋サンプラー (DSB-PS: Diffusion Schrödinger Bridge Posterior Sampling) という．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion1.html#footnotes",
    "href": "posts/2024/Samplers/Diffusion1.html#footnotes",
    "title": "雑音除去拡散過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし，(Deming and Stephan, 1940) にも (Fortet, 1940) にも言及しておらず，Markov 連鎖の遷移確率の推定という文脈で研究している．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion2.html",
    "href": "posts/2024/Samplers/Diffusion2.html",
    "title": "雑音除去拡散サンプラー",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion2.html#導入",
    "href": "posts/2024/Samplers/Diffusion2.html#導入",
    "title": "雑音除去拡散サンプラー",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 拡散モデルをサンプラーに使う\n前稿で扱った雑音除去拡散モデル (Denoising Diffusion Models) はそのままでは一般の確率分布からのサンプリングには使えない：\n\n\n\n\n\n\n\n\n\n拡散模型（深層生成モデル６）\n\n\n\n\n\n\n\n雑音除去拡散過程\n\n\n\n\n\n一方で本稿では，正規化定数が不明な分布 \\[\n\\pi(x)=\\frac{\\gamma(x)}{Z},\\qquad Z:=\\int_\\mathcal{X}\\gamma(x)\\,dx\n\\] に対しても使える汎用サンプラー DDS (Denoising Diffusion Sampler) (Vargas et al., 2023) とその Schrödinger 橋による改良を扱う：\n\n\n\n\n\n\n\n\n\n名称\n正規化定数の不明な分布に使えるか？\nIPF が必要か？\n\n\n\n\n雑音除去拡散モデル\n\n\n\n\nSchrödinger 橋による改良\n\n\n\n\nDDS（第 2 節）\n\n\n\n\nDSBS （第 3 節）\n\n\n\n\n\n\n\n\n1.2 DDS の現状\nMCMC, SMC そして ABC の代替手法ともくされているが，理論が未発達である．例えば (Bortoli, 2022) などの既存の理論は，スコア関数の推定誤差の言葉で収束を論じており，この推定誤差は実践上では確認が難しいものであると言える (Heng et al., 2024)．\n加えて，拡散模型は確率的局所化の考え方と関係が深いことが知られており，近似メッセージ伝搬を取り入れることで，定量的な収束保証をつけることもできる (Montanari and Wu, 2023)．\n\n\n1.3 AIS (Neal, 2001) との比較\n焼きなまし重点サンプリング (AIS: Annealed Importance Sampling) (Neal, 2001) では，目標分布 \\(\\pi\\) に至る列 \\(\\pi_0,\\cdots,\\pi_p=\\pi\\) が得られており，\\(\\pi_0\\) からのサンプリングは可能である場合の重点サンプリング法のテクニックである．\nAIS とは，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，1 拡張された空間 \\(\\mathcal{X}^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(\\pi_0\\otimes P_1\\otimes P_2\\otimes\\cdots\\otimes P_p\\) を提案分布に用いたとして荷重荷重を計算する．2 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw:=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．3\n従って，本当は \\(\\mathcal{X}^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion2.html#sec-DDGS",
    "href": "posts/2024/Samplers/Diffusion2.html#sec-DDGS",
    "title": "雑音除去拡散サンプラー",
    "section": "2 雑音除去拡散によるサンプリング (DDGS)",
    "text": "2 雑音除去拡散によるサンプリング (DDGS)\n\n2.1 導入\n(Vargas et al., 2023) は次の２点を克服するサンプラーを提案した．\n\n2.1.1 条件付き生成\nDDPS と DSB-PS は，\\(y\\in\\mathcal{Y}\\) について一様に均してしまった 償却推論 を行っている．\n特殊な \\(y\\in\\mathcal{Y}\\) に対しても，これにフィットしたモデルを作りたい状況がある．\n\n\n2.1.2 正規化定数のわからない分布\n同時に DDPS と DSB-PS は，正規化定数の不明な分布などからのサンプリングには使えない．\nここでは， \\[\np(x)=\\frac{\\gamma(x)}{Z},\\qquad Z:=\\int_\\mathcal{X}\\gamma(x)\\,dx\n\\] という形で，\\(\\gamma\\) のみを与えられた場合を考える．\nDDPS において考えたように，\\((X_0,Y)\\) からのサンプルが得られないため，\\(\\nabla_x\\log p_t(x_t)\\) の項の近似に関しては別のアプローチを考える必要がある．\n\n\n\n2.2 \\(h\\)-変換としての表示\n雑音除去拡散 \\[\ndZ_t=\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log p_{T-t}(Z_t|y)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T|y),\n\\] の \\(\\nabla_x\\log p_t(x_t)\\) の表示が消えるような変数変換を考える．\nまず，OU 過程 \\((X_t)\\) を定常分布 \\(X_0\\sim\\mathrm{N}_d(0,I_d)\\) から始めた場合の分布を \\(\\mathbb{M}\\) とすると，この逆は \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] である．この過程の \\(\\mathbb{M}\\) の下での \\(h\\)-変換は， \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+\\nabla_z\\log h_{T-t}(Z_t)\\,dt+dW_t,\\qquad Z_0\\sim p_T(x_T),\n\\] \\[\nh_t(x_t):=\\int_\\mathcal{X}\\Phi(x_0)m_{T|T-t}(x_0|x_t)\\,dx_0,\\qquad \\Phi(x_0):=\\frac{p(x_0)}{\\phi_d(x_0;0,I_d)}\n\\] と表せる．ただし \\(m\\) は OU 過程 \\((X_t)\\) の遷移密度とした．\nこの表示に対するパラメトリックな近似 \\[\ndZ_t=-\\frac{1}{2}Z_t\\,dt+u^\\theta_{T-t}(Z_t)\\,dt+dW_t,\\qquad Z_0\\sim\\mathrm{N}_d(0,I_d),\n\\] の分布を \\(\\mathbb{Q}^\\theta\\) で表し，\\(\\operatorname{KL}(\\mathbb{P},\\mathbb{Q}^\\theta)\\) を最小化することが最初に思いつくが，これでは \\(\\mathbb{P}\\) からのサンプル，従って \\(p\\) からのサンプルを必要としてしまう．\n\n\n2.3 逆 KL-乖離度の最適制御\n\\(h\\)-変換をした理由は，\\(\\operatorname{KL}(\\mathbb{Q}^\\theta,\\mathbb{P})\\) ならば計算できる点にある．\n\\[\n\\mathcal{L}(\\theta):=\\operatorname{KL}(\\mathbb{Q}^\\theta,\\mathbb{P})=\\operatorname{E}_{\\mathbb{Q}^\\theta}\\left[\\frac{1}{2}\\int^T_0\\|u^\\theta_{T-t}(Z_t)\\|^2\\,dt-\\log\\Phi(Z_T)\\right]\n\\] については，\\(\\log\\Phi(Z_T)\\) には \\(\\theta\\) が出現しないため，第一項のみに集中すれば良い．\nそうすると，これは KL 最適制御問題として解くことができる．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion2.html#sec-DSB-GS",
    "href": "posts/2024/Samplers/Diffusion2.html#sec-DSB-GS",
    "title": "雑音除去拡散サンプラー",
    "section": "3 Schrödinger 橋によるサンプリング (DSB-GS)",
    "text": "3 Schrödinger 橋によるサンプリング (DSB-GS)\n\n3.1 導入\n全く同様にして，Schrödinger 橋としての見方を導入することにより，DDGS の効率はさらに上げられる．\n加えて，無雑音極限において，Schrödinger 橋問題は，エントロピー正則化を持つ最適輸送問題と Monge-Kantorovich 問題と関連がある (De Bortoli et al., 2021, p. 3.1節)．\nこの場合も，\\(T\\to\\infty\\) の極限において，DDGS は Schrödinger 橋の近似を与える．\n\n\n3.2 Schrödiger-Föllmer サンプラー\n\\(\\mathbb{M}\\) を OU 過程と取る代わりに，\\(\\Pi_T(x_T)\\) を Dirac 測度として Brown 橋を取ることもできる．これが (Föllmer, 1985) 以来のアプローチである．\nこのアプローチでは，IPF は２回のイテレーションで収束するという美点がある．このための数値的方法も広い分野で提案されている：(Barr et al., 2020), (Zhang et al., 2021)．\n終端の測度を Dirac 測度としていることの綻びが数値的な不安定性に現れやすいことが (Vargas et al., 2023) で述べられている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html",
    "href": "posts/2024/Kernels/Deep4.html",
    "title": "VAE：変分自己符号化器",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#sec-AE",
    "href": "posts/2024/Kernels/Deep4.html#sec-AE",
    "title": "VAE：変分自己符号化器",
    "section": "1 自己符号化器 (AE)",
    "text": "1 自己符号化器 (AE)\n\n1.1 導入\n主成分分析 (PCA) とは，データを線型変換により低次元の線型空間に変換することで，データの良い要約を得ようとする多変量解析手法である．\nこの手法をカーネル法により非線型化することで，データの隠れた構造をよりよく表現することができる (Lawrence, 2005)．全く同様にニューラルネットワークを使って PCA を非線型化することもでき (Cottrell and Munro, 1988)，これが 自己符号化器 と呼ばれるアーキテクチャである．1\n自己符号化器は，VAE と対比する場合には 決定論的 自己符号化器 (deterministic autoencoder) とも呼ばれる．\n\n\n\n\n\n\nAE と VAE の違い\n\n\n\n\nエンコーダー \\(q\\) は VAE では確率核であるが，AE では決定論的な関数である．2\n訓練時の目的関数が違う．AE では復元誤差のみであるが，VAE では潜在表現が事前に設定した分布 \\(p(z)dz=\\mathrm{N}(0,I_d)\\) と近いことを要請する KL-分離度の項が追加される．3\n\n\n\nその結果，AE は基本的には生成モデルとしての使い方は出来ない．事前分布からのサンプル \\(Z\\sim\\mathrm{N}(0,I_d)\\) は全く想定されていない．\n一方で，データ内の画像の復元は AE の方が上手であり，\\(\\beta\\)-VAE の \\(\\beta\\) が大きいほど画像にはもやがかかるようになる．\n\n\n\n\n\n\n\n\n\nテストデータ\n\n\n\n\n\n\n\nVAE による復元\n\n\n\n\n\n\n\n1.2 NN による PCA\nそもそも PCA は，３層からなる自己符号化器の自乗復元誤差の最小化と（学習された基底が正規直交化されていないことを除いて）等価になる (Bourlard and Kamp, 1988), (Baldi and Hornik, 1989), (Karhunen and Joutsensalo, 1995)．\nしかし，３層のままでは非線型な活性化を加えても非線型な次元削減が出来ない (Bourlard and Kamp, 1988) が，４層以上では話が違い，PCA の真の非線型化による拡張になっている (Japkowicz et al., 2000)．\n\n\n\n５層の自己符号化器の例 from (Murphy, 2023, p. 635)\n\n\nしかしこれにより目的関数は２次関数とは限らず，凸最適化の範疇を逸脱するので，大域的最適解が必ず見つかるなどの理論保証ができる世界からは逸脱してしまう．\n\n\n1.3 罰則による潜在表現獲得\n上述の NN は砂時計型をしており，中央の中間層を細くすることで低次元の潜在表現を獲得しようとするものである．\nこのようにアーキテクチャによって潜在表現獲得を制御するのではなく，明示的に目的関数に含めることで潜在表現をすることができる．\n例えば，元の目的関数 \\(E\\) に対して LASSO 様の罰則項 \\[\n\\widetilde{E}(w)=E(w)+\\lambda\\sum_{k=1}^K\\lvert z_k\\rvert\n\\] を加えることで，スパースな潜在表現の獲得を促すことが考えられる．この正則化項を activity regularization という．4\n\n\n1.4 ニューロンによるスパース表現\n罰則を課す代わりに，rectifying neuron \\(f(x)=x\\lor0\\) を用いることも，スパースな潜在表現を獲得することにつながる (Glorot et al., 2011)．\nこのように獲得された潜在表現は，\\(l^1\\)-罰則による場合よりも，「常にゼロ」になる素子が少ない．このことは脳の活動により近い (Beyeler, 2019) ため，好ましいと考えられている．\n\n\n1.5 Denoising Autoencoder (DAE)\nデータベクトル \\(x_n\\) にノイズを加えたもの \\(\\widetilde{x}_n\\) を元のデータに復元することを \\[\nE(w)=\\sum_{n=1}^N\\|y_w(\\widetilde{x}_n)-x_n\\|^2\n\\] などの目的関数で学習することで，ノイズにロバストな潜在表現を獲得することができる．\nこれは denoising autoencoder (DAE) (Vincent et al., 2008), (Vincent et al., 2010) として提案され，直ちにあるエネルギーベースモデルをスコアマッチングにより推定していることと等価であること (Vincent, 2011) が自覚された．\n(Vincent et al., 2008) の問題意識は，深層モデルの初期値を設定する層ごとの教師なし事前学習がなぜ成功しているか？ にあった．その結果，この denoising autoencoder のような目的関数が，深層モデルの学習を成功させるような初期値を与えることに成功していた要因であることを示唆している．\nDAE の成功は，これがスコアベクトル場を学習しているためだと言える． \\[\n\\widetilde{x_i}=x_i+\\sigma\\epsilon\\qquad\\epsilon\\sim\\mathrm{N}_1(0,1)\n\\] によってノイズを印加し， \\[\n\\ell(x,r(\\widetilde{x}_i))=\\|r(\\widetilde{x})-x\\|_2^2\n\\] を損失関数として DAE を学習したとすると，一定の条件の下で \\[\nr(\\widetilde{x})-x\\approx\\nabla\\log p(x)\\qquad(\\sigma\\to0)\n\\] が成り立つという．すなわち，少し摂動が与えられたデータが与えられても，データの真の多様体上に射影して（ノイズを除去して）これを返すことができる．5\n\n\n1.6 Contractive Autoencoder (CAE) (Rifai et al., 2011)\n元の目的関数 \\(E\\) に対して，エンコーダー \\(f\\) の Jacobian の Frobenius ノルムに対して罰則を課すことを考える： \\[\n\\widetilde{E}(w)=E(w)+\\lambda\\|J_f(x)\\|_2\n\\] これにより，エンコーダー \\(f\\) は Jacobian が縮小的になるものが学習されるため，データがなす部分多様体から外れた入力に対してこれを部分多様体内に押し込める形の \\(f\\) が学習される．\nこれを縮小的自己符号化器という．\\(J_f\\) を計算するために，訓練は減速される．\n\n\n1.7 マスキングによる潜在表現獲得\nBERT (Devlin et al., 2019) はランダムにデータを脱落させ（マスキング），これを予測することで言語に対する極めて豊かな潜在表現を獲得した．\nmasked autoencoder (K. He et al., 2022) では，ノイズ印加の代わりに，データの脱落を行って AE を訓練する．\nこの方法は ViT の事前訓練として使われる．言語と違って画像ではより多くの部分を脱落させることで，より豊かな潜在表現を獲得することができる．6\nするとマスキングがほとんどデータの軽量化になっており，大規模なトランスフォーマーの事前訓練としてよく選択される．この場合，デコーダーはエンコーダーより軽量な非対称な構造をしている場合が多い．\n加えて，ひとたび訓練が終わればデコーダーは取り外し，種々のタスクに対して調整されたデコーダーを改めて訓練して使われることが多い．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#sec-VAE",
    "href": "posts/2024/Kernels/Deep4.html#sec-VAE",
    "title": "VAE：変分自己符号化器",
    "section": "2 変分自己符号化器 (VAE) (Kingma and Welling, 2014)",
    "text": "2 変分自己符号化器 (VAE) (Kingma and Welling, 2014)\n\n\n\nSamples from VQ-VAE-2 Taken from Figure 6 (Razavi et al., 2019, p. 8)\n\n\n\n2.1 導入\nVAE (Variational Auto-Encoder) (Kingma and Welling, 2014), (Rezende et al., 2014) も GAN と同じく，深層生成モデル \\(p_\\theta\\) にもう１つの深層ニューラルネットワーク \\(q_\\phi\\) を対置する．\n一方でこのニューラルネット \\(q_\\phi\\) は GAN のように判別をするのではなく，近似推論によってデータ生成分布（の拡張分布）を \\(q_\\phi(x,z)p(z)\\) の形で再構成しようとする 認識モデル (recognition model) である．7\nこのスキームを変分ベイズの文脈では償却推論 2.4 ともいう．\\(q_\\phi\\) を エンコーダー，\\(p\\) を 事前分布 ともいう．\n\n\n2.2 エンコーダーによる表現獲得\nすなわち，VAE ではエンコーダーは（少なくとも形式的な意味で）「推論」するように設計された自己符号化器である．この際のベイズ推論は変分推論によって達成されるが，reparametrization trick によって \\(q_\\phi\\) の変分推論をデコーダー \\(p_\\phi\\) と同時に SGD によって実行できる点が革新的である．\nエンコーダー \\(q_\\phi\\) は \\[\nq_\\phi(x,z)\\,dz=\\mathrm{N}\\biggr(\\mu_\\phi(z),\\mathrm{diag}_\\phi(\\sigma^2(z))\\biggl)\n\\] という形を仮定し，平均 \\(\\mu_\\phi\\) と分散 \\(\\sigma^2_\\phi\\) の関数形をニューラルネットワークで表現する．\n一方でデコーダー \\(p_\\phi(z,x)\\) はこの潜在表現からデータを再構成することを目指し，ひとたび学習されれば \\(p(z)p_\\phi(z,x)\\) の形でデータ生成ができるというわけである．\n学習は深層生成モデル \\(p_\\theta\\) のデータとの乖離度の最小化と，データで条件づけた潜在変数 \\(Z\\) の事後分布 \\(q_\\phi\\) の近似推論器とを，確率勾配降下法によって同時に実行する．\nVAE 自体は拡散モデルの登場以降，画像生成モデルとしては下火になったが，エンコーダー \\(q_\\phi\\) は Sora (Brooks et al., 2024) における動画データの圧縮表現の学習など，その他の下流タスクの構成要素としても用いられる（VQ-VAE 3 も参照）．\n\n\n2.3 デコーダーの変分ベイズ学習\nデータ \\(X\\) の生成過程 \\(Z\\to X\\) に，モデル \\(p_\\theta(z)p_\\theta(x|z)\\) を考える．これがニューラルネットワークによるモデルであるとすると，周辺尤度 \\[\np_\\theta(x)=\\int_\\mathcal{Z}p_\\theta(z)p_\\theta(x|z)\\,dz\n\\] の評価は容易でない．\nこのとき，対数周辺尤度は次のように下から評価できるのであった：8 \\[\n\\begin{align*}\n    \\log p_\\theta(x)&=\\log\\int_\\mathcal{Z}p_\\theta(x,z)\\,dz\\\\\n    &=\\log\\int_\\mathcal{Z}q_\\phi(z)\\frac{p_\\theta(x,z)}{q_\\phi(z)}\\,dz\\\\\n    &\\ge\\int_\\mathcal{Z}q_\\phi(z)\\log\\frac{p_\\theta(x|z)p_\\theta(z)}{q_\\phi(z)}\\,dz\\\\\n    &=-\\operatorname{KL}(q_\\phi,p_\\theta)+\\int_\\mathcal{Z}q_\\phi(z)\\log p_\\theta(x|z)\\,dz\\\\\n    &=:F(\\theta,\\phi)\n\\end{align*}\n\\]\nこの \\(F\\) を 変分下界 （機械学習では ELBO）といい，\\(\\theta,\\phi\\) に関して逐次的に最大化する（＝\\(\\operatorname{KL}(q,p)\\) を最小化する）ことによって，\\(\\log p_\\theta\\) を直接評価することなく最大化する \\(\\theta\\) を見つけるのが変分 Bayes の枠組みである．\nこれを一般のモデルについて実行するためには \\(q_\\phi\\) に平均場近似などの追加の仮定や \\(E\\)-ステップの近似が必要であるが，ここでは \\(q_\\phi\\) は NN からなる認識モデルとし，\\(F\\) の勾配 \\(D_\\phi F\\) の推定量を用いて，\\(p_\\theta,q_\\phi\\) を同時に学習することが出来るというのである．\n\n\n2.4 償却推論 (amortized inference)\nデータ \\(x_1,\\cdots,x_n\\) が互いに独立で，潜在変数 \\(z_1,\\cdots,z_n\\) も同じ数だけ用意し，互いに独立であるとする．実際，VAE では \\(z\\sim\\mathrm{N}_n(\\mu,\\Sigma)\\) とし，\\(\\Sigma\\) は対角行列とする．\nこのとき，変分下界は \\[\nF(\\theta,\\phi)=\\sum_{i=1}^n\\int_\\mathcal{Z}q_\\phi(z_i)\\log\\frac{p_\\theta(x_i|z)p_\\theta(z_i)}{q_\\phi(z_i)}\\,dz_i\n\\tag{1}\\] と表せる．さらに \\(p_\\theta(x_i|z)=p_\\theta(x_i|z_i)\\) と仮定すると， \\[\nq_\\phi(z_i)=p(z_i|x_i)=\\frac{p(x_i|z_i)p(z_i)}{p(x_i)}\n\\] と取った場合が \\(F\\) を最大化する．\n償却推論 (Gershman and Goodman, 2014), (Rezende et al., 2014) では，\\(i\\in[n]\\) ごとにフィッティングするのではなく，確率核 \\(p(x_i,z_i)\\,dz_i\\) を \\(i\\in[n]\\) に依らずに単一のニューラルネットワーク \\(q_\\phi\\) でモデリングする確率的変分推論法をいう．9\n\nVAE では，EM アルゴリズムのように \\(\\theta,\\phi\\) を交互に更新していくわけではなく，両方 NN であることを利用して同時に SGD によって最適化する．換言すれば，EM アルゴリズムの様に本当にデータを最もよく説明する変分推論を実行したいという様な目的関数にはなっておらず，あくまで生成と表現学習が目的である．\n\n\n2.5 確率的勾配変分近似 (SGVB)\n\n式 (1) はデータ点ごとに \\[\nF(\\theta,\\phi)=\\sum_{i=1}^n\\int_\\mathcal{Z}q_\\phi(z_i)\\log p_\\theta(x_i|z_i)\\,dz_i-\\operatorname{KL}(q_\\phi,p_\\theta)\n\\tag{2}\\] と表示できる．事前分布 \\(p_\\theta\\) もエンコーダー \\(q_\\phi\\) も正規分布族としたので，第二項は簡単に計算できる： \\[\n\\operatorname{KL}\\biggr(q_\\phi(z_i|x_i),p_\\theta(z_i)\\biggl)=\\frac{1}{2}\\sum_{j=1}^m\\biggr(1+\\log\\sigma^2_j(x_i)-\\mu^2_j(x_i)-\\sigma^2_j(x_i)\\biggl).\n\\]\nそこで第１項が問題である．勾配 \\(D_\\phi F,D_\\theta F\\) 自体は計算不可能でも，不偏な推定量は得られないだろうか？\nしかも，単に \\(q_\\phi(z|x)\\) からのサンプルを用いた crude Monte Carlo \\[\n\\int_\\mathcal{Z}q_\\phi(z_i|x_i)\\log p_\\theta(x_i|z_i)\\,dz_i\\approx\\frac{1}{N}\\sum_{n=1}^N\\log p_\\theta(x_i|z_i^{(n)})\n\\] では，分散が非常に大きくなってしまう (Paisley et al., 2012) ため，効率的な不偏推定量である必要もある．また，\\(\\theta\\) に関する勾配は数値的に計算できても，ここから \\(D_\\phi F\\) を得ることが困難である．\nこれを 重点サンプリングの考え方により解決した のが \\(D_\\phi F,D_\\theta F\\) に対する SGVB 推定量である．10 (Kingma and Welling, 2014) では reparameterization trick と呼んでいる．\n\n\n\n\n\n\n一般的な設定での SGVB 推定量\n\n\n\n\n\nある分布 \\(P\\in\\mathcal{P}(E)\\) と可微分同相 \\(g_\\phi:E\\times\\mathcal{X}\\to\\mathcal{Z}\\) であって \\[\ng_\\phi(\\epsilon,x)\\sim q_\\phi(z,x)\\quad(\\epsilon\\sim P)\n\\] を満たすものを見つけることができるとき，この \\(P\\) を提案分布とする重点サンプリング推定量 \\[\n\\begin{align*}\n    \\operatorname{E}_{q_\\phi}[f(Z)]&=\\operatorname{E}_{P}[f(g_\\phi(\\epsilon,x))]\\\\\n    &\\simeq\\frac{1}{M}\\sum_{i=1}^Mf(g_\\phi(\\epsilon^i,x))\n\\end{align*}\n\\] により，Monte Carlo 推定量の分散を減らすことができる．\\(f=F\\) と取ることで SGVB 推定量を得る．\n\n\n\nエンコーダー \\(q_\\phi\\) から直接サンプル \\(z_i\\) を得るわけではなく， \\[\nz_i=\\sigma_\\phi(x_i)\\epsilon+\\mu_\\phi(x_i),\\qquad\\epsilon\\sim\\mathrm{N}_1(0,1)\n\\] によって Monte Carlo サンプルを得れば，これはサンプリングと \\(\\phi\\) に関する微分が分離されている．\n加えて，元の方法よりも Monte Carlo 分散が低減される．\n\n\n2.6 目的関数\n以上を総じて，目的関数は \\[\n\\mathcal{L}=\\sum_{i=1}^n\\left(\\operatorname{KL}\\biggr(q_\\phi(z_i|x_i),p_\\theta(z_i)\\biggl)+\\frac{1}{N}\\sum_{n=1}^N\\log p_\\theta(x_i|z_i^{(n)})\\right)\n\\] となる．Monte Carlo サンプルは \\(N=1\\) が採用され，SGD と組み合わせるとこの設定が良い効率を与えるという．11\n\n\n\n\n\n\n訓練過程\n\n\n\n\nデータをエンコーダーを前方向に伝播させ，\\(\\mu,\\sigma\\) の値を得て，そこからサンプルする．\nこの Monte Carlo サンプルをデコーダーに入れて伝播させ，変分下界 \\(F\\)（の推定量）を評価する．\n自動微分により \\(\\theta,\\phi\\) に関する \\(\\mathcal{L}\\) の勾配を計算する．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#sec-VQ-VAE",
    "href": "posts/2024/Kernels/Deep4.html#sec-VQ-VAE",
    "title": "VAE：変分自己符号化器",
    "section": "3 ベクトル量子化変分自己符号化器 (VQ-VAE)",
    "text": "3 ベクトル量子化変分自己符号化器 (VQ-VAE)\nVQ-VAE は，VAE を特に表現学習に用いるために，潜在表現層を離散変数とした変種である．この際の潜在表現は符号帳 (codebook) とも呼ばれる．\n加えて，(van den Oord et al., 2017) では，事後分布 \\(q_\\phi(z|x)\\) が事前分布 \\(p(z)\\) に十分近くない場合には，事前分布を使ってサンプルを生成するのではなく，\\(q_\\phi(z|x)\\) を改めて Pixel-CNN などを用いて推論してそこからサンプルを得ることを提案している．\n(van den Oord et al., 2017) では CNN が使われていたが，近年はトランスフォーマーによるデコーダーが用いられることも多い．\n\n3.1 ベクトル量子化\n一般に，画像・音声・動画などの複雑なデータに対しては，背後の構造をよく掴んだ低次元な潜在表現を得ることを重要なステップとして含むため，データの潜在表現を得る汎用手法は価値が高い．このようなタスクを 表現学習 という (Bengio, Courville, et al., 2013)．\nVAE の主な応用先に画像データがある．その際は，デコーダーを通じた画像生成モデルとして用いるだけでなく，エンコーダーを用いてデータ圧縮をすることも重要な用途である (Ballé et al., 2017)．\nその際，潜在空間を離散空間にすることで，連続データである画像を離散化することができる．これを ベクトル量子化 と結びつけたのが VQ-VAE である．ベクトル量子化は DALL-E (Ramesh et al., 2021) など，より大きな画像生成モデルの構成要素としても利用される．\n\n\n3.2 分布崩壊 (Posterior collapse)\nVAE を表現学習に使う際の最大の問題は 分布崩壊 である (J. He et al., 2019)．これはデコーダーが強力すぎる場合，ほとんどデコーダー層のみでデータの生成に成功してしまい，潜在表現が十分組織されないまま最適化が完了され，潜在表現が縮退してしまうことをいう\nVQ-VAE は潜在表現を離散変数にすることでこれが解決できるとし，連続潜在変数による VAE とデータの復元力を変えず，同時に良い潜在表現も獲得できるという．\n実際，(van den Oord et al., 2017) が，言語が離散的であることに首肯するならば，人間は言表によって画像や動画の概要を掴めるように，画像や動画の有効な潜在表現は離散変数で十分であるはずという議論は十分説得的である．\n\n\n3.3 表現学習をする VAE\n\n3.3.1 \\(\\beta\\)-VAE (Higgins et al., 2017)\n変分下界 (2) の KL 乖離度の項に新たなハイパーパラメータ \\(\\beta&gt;0\\) を追加する： \\[\n\\int_\\mathcal{Z}q_\\phi(z_i)\\log p_\\theta(x_i|z_i)\\,dz_i-\\beta\\operatorname{KL}(q_\\phi,p_\\theta).\n\\] \\(\\beta=0\\) の場合が決定論的な AE，\\(\\beta=1\\) の場合が元々の VAE に当たる．\nこの \\(\\beta\\) を適切なスケジュールで \\(0\\) から \\(1\\) に段階的に引き上げることによって，分布崩壊が防げる．これを KL アニーリング という (Bowman et al., 2016)．\n一般に \\(\\beta\\) は潜在表現の圧縮度合いを意味しており，\\(\\beta&lt;1\\) では画像の復元が得意になり，\\(\\beta&gt;1\\) ではデータの圧縮が得意になる (Higgins et al., 2017)．\n特に，データの潜在表現の disentanglement が得意になるとして，表現学習に重要な応用を持つ (Locatello et al., 2019)．\n\n\n3.3.2 Variational Lossy Autoencoder (Chen et al., 2017)\nデコーダー \\(p(x|z)\\) と事前分布 \\(p(x)\\) を自己回帰モデルにし，VAE のスキームを純粋なエンコーダー \\(q(z|x)\\) の訓練に用いた．\nその際，用いる自己回帰モデルの予測性能の強さを制御することで，どのような潜在表現を生成するかの制御が可能になることを論じている (Chen et al., 2017)．\n\n\n\n3.4 VQ-VAE\nVQ-VAE (van den Oord et al., 2017), VQ-VAE-2 (Razavi et al., 2019) は，自己符号化器の中間表現に ベクトル量子化 を施し，JPEG (Wallace, 1992) のような画像データの圧縮を行うことで，不要な情報のモデリングを回避している．\nすなわち，エンコーダーの出力 \\(z\\in\\mathbb{R}^{H\\times W\\times K}\\) は最終的に符号帳 \\(\\{e_k\\}_{k=1}^K\\subset\\mathbb{R}^L\\) と見比べて最近傍点の符号 \\(k\\in[K]\\) のみが記録される．デコーダーには符号帳の要素 \\(\\{e_k\\}_{k=1}^K\\) のみが入力される．これにより，デコーダーに対して元データの 30 分の 1 以下のサイズで学習を行うことができるのも美点である．\n符号帳も同時に学習され，そのための項が目的関数に追加される．\n一つの技術的な難点に，離散化のステップが途中に含まれるために勾配の計算が困難になることがあるが，stright-through 推定量 (Bengio, Léonard, et al., 2013) の利用によって解決している．\nGAN は元データのうち，尤度が低い部分が無視され，サンプルの多様性が失われがちであったが，VQ-VAE はこの問題を解決している．また，GAN にはないようなモデル評価の指標が複数提案されている．\n\n\n3.5 連続緩和\nVQ-VAE ではコードブックへの対応はハードな帰属をしている．すなわち，全ての出力はどれか１つのエントリー \\(e_k\\) を選んで \\(k\\) のみが記録されるが，これをソフトな帰属に変更し，連続な表現を許すことが考えられる．12\nこの際には，元々の reparametrization trick 2.5 が離散変数には直ちに一般化できないところが，新たな方法が見つかり引き続き勾配による最適化が可能という美点もある．\n標準正規分布 \\(\\mathrm{N}(0,1)\\) の代わりに，質的変数のサンプリングにおいて，Gumbel 分布を提案分布として用いることが有効であり，この reparametrization trick を Gumbel Max Trick (Chris J. Maddison et al., 2014), (Jang et al., 2017) という．\nConcrete (Continuous Relaxatino of Discrete) (Chris J. Maddison et al., 2017) はこれを連続分布に拡張し，reparametrization trick に応用したものである．\nこれらの手法は VAE だけでなく，DALL-E (Ramesh et al., 2021) の訓練にも応用されている．\n\n\n3.6 VQ-VAE-2 (Razavi et al., 2019)\nVQ-VAE-2 は，VQ-VAE から潜在空間に階層構造を持たせた，エンコード・デコードを各２回以上繰り返したものである．\n\n\n3.7 Codebook collapse\nVQ-VAE は符号帳 (codebook) に冗長性が生まれ，符号帳の一部が使われなくなるという問題がある．これを解決するためには，符号帳への対応を softmax 関数を用いて軟化することが dVAE (Ramesh et al., 2021) として考えられている．\nしかしこの dVAE も codebook collapse から完全に解放されるわけではない．これは softmax 関数の性質によると考えられ，実際，Dirichlet 事前分布を導入した Bayes モデルによって緩和される (Baykal et al., 2023)．\nこのような技術を エビデンス付き深層学習 (EDL: Evidential Deep Learning) (Sensoy et al., 2018), (Amini et al., 2020) という．13\n\n\n3.8 GAN との比較\nVAE は GAN よりも画像生成時の解像度が劣るという問題がある．\n\n3.8.1 Wasserstein VAE (Tolstikhin et al., 2018)\nこれを，目的関数を Wasserstein 距離に基づいて再定式化することで解決できるというのが Wasserstein Auto-encoder (Tolstikhin et al., 2018) である．\n\n\n3.8.2 VQ-GAN (Esser et al., 2021)\n一方で，目的関数に \\(L^2\\)-損失を用いている点自体が難点であるとして，ベクトル量子化の考え方を GAN に移植した VQ-GAN が提案された．\nVQ-GAN では潜在空間上の事前分布の学習にトランスフォーマーが用いられた．なお，この次回作が生成を VAE 内の潜在空間で行うものを 潜在拡散モデル (latent diffusion model) (Rombach et al., 2022) であり，Stable Diffusion の元となっている．\n一方，VIM (Vector-quantized Image Modeling) (Yu et al., 2022) では，VAE でも GAN でもなく，エンコーダーもデコーダーもトランスフォーマーにすることで更なる精度が出ることが報告されている．"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#参考文献",
    "href": "posts/2024/Kernels/Deep4.html#参考文献",
    "title": "VAE：変分自己符号化器",
    "section": "4 参考文献",
    "text": "4 参考文献\n\n決定論的な自己符号化器の解説は (Bishop and Bishop, 2024) 19.1 節に詳しい．\nAE と VAE を比較した実験は，こちら の (Murphy, 2023) の Jupyeter Notebook で見れる．\nVAE の簡単な実装は次の稿も参照：\n\n    \n        \n            \n            \n                VAE：変分自己符号化器\n                PyTorch によるハンズオン"
  },
  {
    "objectID": "posts/2024/Kernels/Deep4.html#footnotes",
    "href": "posts/2024/Kernels/Deep4.html#footnotes",
    "title": "VAE：変分自己符号化器",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nauto-associative NN ともいう (Bishop and Bishop, 2024, p. 563)．↩︎\nVAE では正規分布族の２つのパラメータ \\(\\operatorname{E}[Z|X],\\mathrm{V}[Z|X]\\) をいずれもモデリングするが，AE では前者のみをモデリングする．↩︎\nこの項に係数 \\(\\beta\\) をつけたものを \\(\\beta\\)-VAE と言い，\\(\\beta=0.5\\) とすると，AE と VAE の中間的な性格を持つようになる．↩︎\n(Murphy, 2023, p. 681) 20.3.4も参照．↩︎\n(Murphy, 2023, p. 681) 20.3.2も参照．↩︎\nBERT では文章の 15% であるが，ViT では 75% 近くがマスキングされるという (Bishop and Bishop, 2024, p. 568)．↩︎\n(Kingma and Welling, 2019, p. 321) の用語に倣った．↩︎\n変分ベイズの稿 も参照．↩︎\n(Murphy, 2023, p. 438) 10.1.5 節も参照．↩︎\n最適化の文脈において，目的関数の評価が困難であるとき，Monte Carlo 推定量でこれを代替する際，重点サンプリングを用いると良いことは従来提案されている (Geyer, 1996)．(Robert and Casella, 2004, p. 203) も参照．↩︎\n(Bishop and Bishop, 2024, p. 576) も参照．↩︎\n\\(k\\)-平均クラスタリング のソフトとハードに似ている．↩︎\nPresent Square 記事，GIGAZINE 記事 もある．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion2.html#footnotes",
    "href": "posts/2024/Samplers/Diffusion2.html#footnotes",
    "title": "雑音除去拡散サンプラー",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(\\pi_p(x_p)/\\pi_0P_1P_2\\cdots P_p\\) は多くの場合計算不能である．↩︎\nただし，\\(P_i^{-1}\\) とは，\\[ P_i(x_{i-1},x_i)\\pi_{i-1}(x_i-1)=\\pi_i(x_i)P_i^{-1}(x_{i-1},x_i) \\] で定まる確率核とした．\\(\\otimes\\) の記法はこちらも参照．↩︎\nこのウェイトの表示は，\\(P_i^{-1}/P_i=\\pi_{i-1}/\\pi_i\\) が成り立つことから直ちに従う．↩︎"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "href": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．"
  }
]