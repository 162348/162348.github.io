[
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#mc-markov-chain-vs.-mp-markov-process",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#mc-markov-chain-vs.-mp-markov-process",
    "title": "動き出す次世代サンプラー",
    "section": "1.1 MC (Markov Chain) vs. MP (Markov Process)",
    "text": "1.1 MC (Markov Chain) vs. MP (Markov Process)\n「拡散過程じゃない連続過程」"
  },
  {
    "objectID": "posts/2024/Computation/VI.html",
    "href": "posts/2024/Computation/VI.html",
    "title": "変分推論１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nより図が見やすい PDF 版は こちら．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#導入",
    "href": "posts/2024/Computation/VI.html#導入",
    "title": "変分推論１",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 歴史\nハード \\(K\\)-平均法はモデルフリーのクラスタリングアルゴリズムである．Voronoi 分割による競争学習の一形態とも見れる．1\n一方で原論文 (Lloyd, 1982) では，パルス符号変調 の文脈で，アナログ信号の量子化の方法として提案している．2\n実際，\\(K\\)-平均法は（非可逆）データ圧縮にも用いられる．クラスター中心での画像の値と，それ以外では帰属先のクラスター番号のみを保存すれば良いというのである．このようなアプローチを ベクトル量子化 (vector quantization) という．3\nソフト \\(K\\)-平均法とは，このようなデータ点のクラスターへの一意な割り当てを，ソフトマックス関数を用いて軟化したアルゴリズムであり，多少アルゴリズムとしての振る舞いは改善するとされている．\n\n\n1.2 最適化アルゴリズムとしての見方\n\\(N\\) 個のデータ \\(\\{x_n\\}_{n=1}^N\\) の \\(K\\) クラスへの \\(K\\)-平均クラスタリングアルゴリズムは，ハードとソフトの二種類存在するが，いずれも \\[\nJ:=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\|x_n-\\mu_k\\|^2\n\\] という損失関数の逐次最小化アルゴリズムとみなせる．\nこの見方は，EM アルゴリズム への一般化の軸となる．\nしかしこの目的関数 \\(J\\) が非凸関数であることが，第 5 節で示す実験結果で見る通り，アルゴリズムに強い初期値依存性をもたらす．\nこれをクラスタがより互いに離れるように，クラス割り当て法を修正することで対策した 最遠点クラスタリング (Gonzalez, 1985) が提案された．これは k-means++ (Arthur and Vassilvitskii, 2007) とも呼ばれており，データ圧縮法として用いた場合は復元誤差が \\(O(\\log K)\\) でバウンド出来ることも示されている．\n\n\n1.3 用いるデータ\n実際のコードとデータを用いて \\(K\\)-平均法を解説する．\nまずは，解説にために作られた，次のような３つのクラスタからなる２次元のデータを考え，これの正しいクラスタリングを目指す．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#sec-hard-k-means",
    "href": "posts/2024/Computation/VI.html#sec-hard-k-means",
    "title": "変分推論１",
    "section": "2 ハード \\(K\\)-平均法",
    "text": "2 ハード \\(K\\)-平均法\n\n2.1 アルゴリズムの説明\nhead \\(K\\)-means algorithm はデータ \\(\\{x^{(n)}\\}_{n=1}^N\\subset\\mathbb{R}^I\\) とクラスタ数 \\(K\\in\\mathbb{N}^+\\)，そして初期クラスター中心 \\((m^{(k)})_{k=1}^K\\in(\\mathbb{R}^I)^K\\) の３組をパラメータに持つ．\nsoft \\(K\\)-means algorithm 3.1 はさらに硬度パラメータ \\(\\beta\\in\\mathbb{R}_+\\) を持つ．\nnumpy の提供する行列積を利用して，これを Python により実装した例を以下に示す．ソフト \\(K\\)-平均法の実装と対比できるように，負担率を通じた実装を意識した例である．\nアノテーションを付してあるので，該当箇所（右端の丸囲み数字）をクリックすることで適宜解説が読めるようになっている．\ndef hkmeans_2d(data, K, init, max_iter=100):\n    \"\"\"\n    ２次元データに対するハード K-平均法の実装例．\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n\n    Returns:\n    - clusters: (N,)-numpy.ndarray クラスター番号\n    \"\"\"\n\n1    N = data.shape[0]\n2    I = data.shape[1]\n3    m = init\n4    r = np.zeros((K, N), dtype=float)\n\n    for _ in range(max_iter):\n        # Assignment Step\n        for i in range(N):\n5            distances = np.array([d(data[i], m[:,k]) for k in range(K)])\n6            k_hat = np.argmin(distances)\n7            r[:,i] = 0\n            r[k_hat,i] = 1\n        \n        # Update Step\n8        new_m = np.zeros_like(m, dtype=float)\n9        numerator = np.dot(r, data)\n10        denominator = np.sum(r, axis=1)\n11        for k in range(K):\n            if denominator[k] &gt; 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = m[:,k]\n\n12        if np.allclose(m, new_m):\n            break\n        m = new_m\n    \n13    return np.argmax(r, axis=0)\n\n1\n\nデータ数を取得している．\n\n2\n\nデータの次元を取得している．今回はすべて２次元データを用いる．\n\n3\n\nクラスター中心に引数として受け取った初期値を代入. \\(2×K\\)-行列であることに注意．\n\n4\n\n負担率を \\(K×N\\)-行列として格納している．その理由は後ほど行列積を通じた計算を行うためである．dtype=float の理由は後述．\n\n5\n\nこの distances 変数は (K,)-numpy.ndarray になる．すなわち，第 \\(k\\) 成分が，第 \\(k\\) クラスター中心との距離となっているようなベクトルである．ただし，d は Euclid 距離を計算する関数として定義済みとした．\n\n6\n\n距離が最小となるクラスター番号 \\(\\hat{k}:=[\\operatorname*{argmin}_{k\\in[K]}d(m_k,x_i)]\\) を，\\(i\\in[N]\\) 番目のデータについて求める．\n\n7\n\n\\(\\hat{k}\\) に基づいて負担率を更新するが，ループ内で前回の結果をリセットする必要があることに注意．\n\n8\n\nここで dtype=float と指定しないと，初め引数 init が整数のみで構成されていた場合に，Python の自動型付機能が int 型だと判定し，クラスター中心 m の値が整数に限られてしまう．すると，アルゴリズムがすぐに手頃な格子点に収束してしまう．\n\n9\n\nnumpy の行列積を計算する関数 np.dot を使用している．更新式 \\[\nm^{(k)}\\gets\\frac{\\sum_{n=1}^Nr^{(n)}_kx^{(n)}}{\\sum_{n=1}^Nr^{(n)}_k}\n\\] の分子を行列積と見たのである．\n\n10\n\n分母 (denominator) は \\((K,N)\\)-行列 r の行和として得られる．\n\n11\n\nゼロによる除算が起こらないように場合わけをしている．\n\n12\n\nクラスター中心がもはや変わらない場合はアルゴリズムを終了する．\n\n13\n\n負担率の最も大きいクラスター番号を返す．今回は hat_k の列をそのまま返せば良いが，soft \\(K\\)-means アルゴリズムにも通じる形で実装した．\n\n\n\n\n\n\n\n\n注：実際に用いる実装\n\n\n\n\n\nただし，本記事の背後では次の実装を用いる．\nクラスター中心の推移のヒストリーを保存して図示に利用したり，負担率 r の中身を見たりすることが出来るようにするため，assignment step と update step とに分けてクラスメソッドとして実装し，run メソッドでそれらを呼び出すようにしている．これに fetch_cluster と fetch_history メソッドを加えることで，クラスター番号とクラスター中心の推移を取得することが出来る．フィールド .r から（最終的な）負担率を見ることもできる．\n\n\nCode\nclass kmeans_2d:\n    \"\"\"\n    ２次元データに対するソフト K-平均法の実装．\n\n    Usage:\n        kmeans = kmeans_2d(data, K, init, beta)\n        kmeans.run()\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n    - beta: float 硬度パラメータ\n    \"\"\"\n\n    def __init__(self, data, K, init, beta, max_iter=100):\n        self.data = np.array(data, dtype=float)\n        self.K = K\n        self.init = np.array(init, dtype=float)\n        self.beta = float(beta)\n        self.max_iter = max_iter\n        self.N = data.shape[0]  # データ数\n        self.I = data.shape[1]  # 次元数 今回は２\n        self.m = init  # クラスター中心の初期化．2×K行列．\n        self.r = np.zeros((K, self.N), dtype=float)  # 負担率．K×N行列．\n        self.history = [init.copy()] # クラスター中心の履歴．2×K行列．\n    \n    def soft_assigment(self):\n        \"\"\"soft K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) ** 2 for j in range(self.K)]) # (N,)-numpy.ndarray\n            denominator_ = np.sum(np.exp(-self.beta * distances))  # 分母\n            self.r[:,i] = np.exp(- self.beta * distances) / denominator_\n\n    def hard_assigment(self):\n        \"\"\"hard K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray\n            k_hat = np.argmin(distances)  # 最小距離のクラスター番号\n            self.r[:,i] = 0  # 前のループの結果をリセット\n            self.r[k_hat,i] = 1\n    \n    def update(self):\n        \"\"\"クラスター中心の更新\"\"\"\n        new_m = np.zeros_like(self.m, dtype=float) # ここで float にしないと，クラスター中心が整数に限られてしまう．\n        numerator = np.dot(self.r, self.data)  # (K,2)-numpy.ndarray\n        denominator = np.sum(self.r, axis=1)  # 各クラスターの負担率の和\n        for k in range(self.K):\n            if denominator[k] &gt; 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = self.m[:,k]\n        self.m = new_m\n\n    def fetch_cluster(self):\n        \"\"\"最終的なクラスター番号を格納した (N,)-array を返す\"\"\"\n        return np.argmax(self.r, axis=0)\n    \n    def fetch_history(self):\n        \"\"\"クラスター中心の履歴を格納したリストを，３次元の np.array に変換して返す\"\"\"\n        return np.stack(self.history, axis=0)\n\n    def run_soft(self):\n        \"\"\"soft K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.soft_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n    \n    def run_hard(self):\n        \"\"\"hard K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.hard_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n\n\nなお，この実装は \\(\\beta\\ge500\\) などの場合にオーバーフローが起こることに注意．これへの対処は logsumexp の使用などが考えられる．\n\n\n\n\n\n2.2 初期値依存性\n次の２つの初期値を与えてみる． \\[\nm_1:=\\begin{pmatrix}4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3=\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] と，\\(m_2,m_3\\) は変えずに \\(m_1\\) の \\(y\\)-座標を \\(1\\) だけ下げたもの \\[\nm_1':=\\begin{pmatrix}4\\\\-1\\end{pmatrix}\n\\] とを初期値として与えてみる．\n\n\n\n\n\n\n\n\n図 1: ハード K-平均法によるクラスタリングの結果．初期値は \\((m_1,m_2,m_3)=\\left(\\begin{pmatrix}4\\\\0\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．\n\n\n\n\n\n正解数: 51     正解率: 56.7 %     反復数: 9 回\n\n\n別の初期値を与えてみる（右下の点 \\(m_1\\) を \\(1\\) だけ下に下げただけ）： \\[\n\\begin{pmatrix}4\\\\0\\end{pmatrix}=m_1\\mapsto m_1':=\\begin{pmatrix}4\\\\-1\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n図 2: ハード K-平均法によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 7 回\n\n\n結果が全く変わり，\\((m_1',m_2,m_3)\\) を与えた方が，大きく正解に近づいている．具体的には，右下の初期値 \\(m_1\\) は右上の島に行くが，\\(m_1'\\) は左下の島に行ってくれる．\nハード \\(K\\)-平均アルゴリズムは初期値に敏感である ことがよく分かる．\n\n\n2.3 局所解への収束\n直前の結果2ではクラスター２と３の境界線で４つのミスを犯しており，これを修正できないか試したい．\nそこで，答えに近いように， \\[\nm_1\\gets\\begin{pmatrix}2.5\\\\2\\end{pmatrix},\\;\\; m_2\\gets\\begin{pmatrix}-1\\\\-1\\end{pmatrix},\\;\\; m_3\\gets\\begin{pmatrix}1\\\\-2\\end{pmatrix},\n\\] を初期値として与えてみて，正答率の変化を観察する．\n\n\n\n\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 5 回\n\n\nもはや初期値から殆ど動いていないが，目標のクラスター３に分類された３つの点が，相変わらず３のままであり，加えてクラスター２の中心がこれらから逃げているようにも見えるので，クラスター２の初期値をよりクラスター３に近いように誘導し，クラスター３の中心をより右側から開始する：\n\\[\nm_2:\\begin{pmatrix}-1\\\\-1\\end{pmatrix}\\mapsto\\begin{pmatrix}0\\\\-2\\end{pmatrix}\\;\\; m_3:\\begin{pmatrix}1\\\\-2\\end{pmatrix}\\mapsto\\begin{pmatrix}2\\\\-2\\end{pmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n正解数: 85     正解率: 94.4 %     反復数: 6 回\n\n\nこんなに誘導をしても，正しく分類してくれない．\n実は，以上２つの初期値では，最終的に３つのクラスター中心は同じ値に収束している．よって，これ以上どのように初期値を変更しても，正答率は上がらないシナリオが考えられる．\n以上の観察から，ハード \\(K\\)-平均法はある種の 局所解に収束する ようなアルゴリズムであると考えられる．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#ソフト-k-平均法",
    "href": "posts/2024/Computation/VI.html#ソフト-k-平均法",
    "title": "変分推論１",
    "section": "3 ソフト \\(K\\)-平均法",
    "text": "3 ソフト \\(K\\)-平均法\n\n3.1 アルゴリズムの説明\nハード \\(K\\)-平均法2では，負担率 \\[\nr_{kn}\\gets\\delta_{k}(\\operatorname*{argmax}_{i\\in[k]}d(m_i,x_n))\n\\] は \\(0,1\\) のいずれかの値しか取らなかった．この振る舞いを， \\[\n\\sigma(z;e)_i:=\\frac{e^{z_i}}{\\sum_{j=1}^Ke^{e_j}}\\quad(i\\in[K])\n\\] で定まる ソフトマックス関数 \\(\\sigma:\\mathbb{R}^K\\to(0,1)^K\\) を用いて，「軟化」する．\nここでは，\\(\\beta\\ge0\\) として， \\[\n\\sigma(z;e^{-\\beta})_i=\\frac{e^{-\\beta z_i}}{\\sum_{j=1}^Ke^{-\\beta e_j}}\n\\] の形で用い，\\(\\operatorname*{argmax}\\) の代わりに \\[\n\\begin{align*}\n    r_{kn}&\\gets\\sigma(d(-,x_n)^2\\circ m;e^{-\\beta})_k\\\\\n    &=\\frac{e^{-\\beta d(m_k,x_n)^2}}{\\sum_{j=1}^K e^{-\\beta d(m_j,x_n)^2}}\n\\end{align*}\n\\] とする．ただし，\\(d\\) は \\(\\mathbb{R}^2\\) 上の Euclid 距離とした．\n\n3.1.1 硬度パラメータ\n\\(\\beta\\) は 硬度 (stiffness) または逆温度と呼ぶ．4 \\(\\sigma:=\\beta^{-1/2}\\) は距離の次元を持つ．\n\\(\\beta=0\\) のときは温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる \\(\\beta\\to\\infty\\) の極限が hard \\(K\\)-means アルゴリズムに相当する．\n逆温度 \\(\\beta\\) を連続的に変化させることで，クラスタ数に分岐が起こる，ある種の相転移現象を見ることができる．5\n\n\n3.1.2 実装\n実装は例えば hard \\(K\\)-means アルゴリズム2から，負担率計算の部分のみを変更すれば良い：\nfor i in range(N):\n1        distances = np.array([d(data[i], m[:,k]) for k in range(K)])\n2        denominator_ = np.sum(np.exp(-beta * distances))\n3        r[:,i] = np.exp(-beta * distances) / denominator_\n\n1\n\nデータ \\(x_i\\) とクラスター中心 \\((m_k)_{k=1}^K\\) との距離を計算し，ベクトル \\((d(x_n,m_k))_{k=1}^K\\) を distances に格納している．\n\n2\n\n負担率の計算 \\[\nr_{ik}=\\frac{\\exp(-\\beta d(m_k,x_i))}{\\sum_{j=1}^K\\exp(-\\beta d(m_j,x_i))}\n\\] を２段階に分けて行なっており，分母を先に計算して変数 denominator_ に格納している．\n\n3\n\nすでに計算してある分母 denominator_ を用いてデータ \\(x_i\\) の負担率 \\((r_{ki})_{k=1}^K\\) を計算し，\\((K,N)\\)-行列 r の各列に格納している．\n\n\n\n\n\n3.2 挙動の変化の観察\n逆温度をはじめに \\(\\beta=0.3\\) としてみる．図 1 と全く同様な初期値 \\[\nm_1:=\\begin{pmatrix}4\\\\0\\end{pmatrix},\\quad m_2:=\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3=\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] を与えてみると，次の通りの結果を得る：\n\n\n\n\n\n\n\n\n図 3: 左がソフト K-平均法（\\(\\beta=1\\)），右がハード K-平均法によるクラスタリングの結果（図２の左と全く同じもの）．初期値は \\((m_1,m_2,m_3)=\\left(\\begin{pmatrix}4\\\\0\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．\n\n\n\n\n\n正解数: 44 vs. 51     正解率: 48.9 % vs. 56.7 %     反復数: 28 回 vs. 9 回\n\n\nクラスターの境界が変化しており，正解率は悪化している．さらに，反復数が９回であったところから，３倍に増えている（28回）．\nまた，右上の２つのクラスター中心の収束先は，微妙にずれているが ほとんど一致している 点も注目に値する．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\ncenters = history[-1, :, :]\ndf = pd.DataFrame(centers, columns=['Cluster1', 'Cluster2', 'Cluster3'])\nprint(df)\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.397456  2.397535 -0.036071\ny  2.047565  2.047580 -1.448288\n\n\n\n\n\n図 2 で与えた初期値 \\((m_1',m_2,m_3)\\) も与えてみる．\n\n\n\n\n\n\n\n\n図 4: ソフト K-平均法（\\(\\beta=1\\)）によるクラスタリングの結果，右がハード K-平均法によるクラスタリングの結果（図２の右と全く同じもの）．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 59 回 vs. 7 回\n\n\nクラスター境界と正答率は変わらないが，反復数がやはり７回から大きく増えている．\n結果はやはり 図 3 とは大きく異なっており，ハード \\(K\\)-平均法で観察された初期値鋭敏性が，変わらず残っている．\n加えてこの場合も 図 3 のクラスター１と２と同様に，クラスター２と３の中心がほぼ一致している．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.466833 -0.369537  0.447958\ny  2.124961 -1.076874 -1.543758\n\n\n\n\n\n\\(\\beta=0.3\\) の場合のソフト \\(K\\)-平均法は，この例では クラスター中心が融合する傾向にある ようである．\n一般に，\\(\\beta\\) が小さく，温度が大きいほど，エネルギーランドスケープに極小点が少なくなり，クラスターは同じ場所へ収束しやすくなると予想される．\n\n\n3.3 高温になるほどクラスター数は減少する\n初期値を直前で用いた \\[\nm_1\\gets\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\quad m_2\\gets\\begin{pmatrix}1\\\\4\\end{pmatrix},\\quad m_3\\gets\\begin{pmatrix}-1\\\\1\\end{pmatrix},\n\\] で固定とし，さらに温度を上げて，逆温度を \\(\\beta=0.1\\) としてみる．\n\n\n\n\n\n\n\n\n図 5: ソフト K-平均法（左\\(\\beta=0.1\\)，右\\(\\beta=1\\)）によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 68 vs. 85     正解率: 75.6 % vs. 94.4 %     反復数: 101 回 vs. 59 回\n\n\n反復数はさらに増加し，全てがほとんど同じクラスターに属する結果となってしまった．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n\n\n   Cluster1  Cluster2  Cluster3\nx  1.715903  0.862511  1.329066\ny  1.012398 -0.099845  0.511186\n\n\n\n\n\n温度が大変に高い状態では，全てが乱雑で，３つのクラスターが一様・公平に負担率を持つようになった．そのため，第一歩からほとんど全体の中心へと移動し，反復数が減る．\n次に，温度を少し下げて，逆温度を \\(\\beta=2\\) としてみる．\n\n\n\n\n\n\n\n\n図 6: ソフト K-平均法（左\\(\\beta=10\\)，右\\(\\beta=1\\)）によるクラスタリングの結果．初期値は \\((m_1',m_2,m_3)=\\left(\\begin{pmatrix}4\\\\-1\\end{pmatrix},\\begin{pmatrix}1\\\\4\\end{pmatrix},\\begin{pmatrix}-1\\\\1\\end{pmatrix}\\right)\\)．\n\n\n\n\n\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 17 回 vs. 59 回\n\n\n初めて soft \\(K\\)-means アルゴリズムを用いた場合で，３つのクラスター中心がはっきりと別れた．反復回数は，\\(\\beta=0.3\\) の場合と比べればやはり落ち着いている．\nしかし，正解率は head \\(K\\)-means の場合（ 図 2 など）と全く同じである．実は，最終的なクラスター中心も 図 2 の最終的なクラスター中心とほとんど同じになっている．\n\n\n\n\n\n\n参考：最終的なクラスター中心の座標\n\n\n\n\n\n今回のソフト \\(K\\)-平均法の最終的なクラスター中心\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.416113  0.881629 -1.338782\ny  2.086327 -1.934090 -0.816316\n\n\n図 2 のハード \\(K\\)-平均法の最終的なクラスター中心\n\n\n   Cluster1  Cluster2  Cluster3\nx  2.426102  0.868333 -1.323353\ny  2.091429 -1.948458 -0.765176\n\n\n\n\n\n以上より，ソフト \\(K\\)-平均法は温度を上げるほどクラスター数が少なくなり，温度を下げるほどクラスター数は上がり，十分に温度を下げるとハード \\(K\\)-平均法に挙動が似通う．\n\n\n3.4 最適な硬度の選択\n\\(\\beta=0.2\\) ではクラスターが２つに縮退し，\\(\\beta=1\\) では hard \\(K\\)-means アルゴリズムの結果とほとんど変わらなくなる．その中間では次のように挙動が変わる：\n\n\n\n\n\n\n\n\n図 7: ソフト K-平均法によるクラスタリングの結果の比較（\\(\\beta=0.2\\) vs. \\(\\beta=0.25\\)）．\n\n\n\n\n\n正解数: 50 vs. 86\n正解率: 55.6 % vs. 95.6 %\n\n\n\n\n\n\n\n\n\n\n図 8: ソフト K-平均法によるクラスタリングの結果の比較（\\(\\beta=0.3\\) vs. \\(\\beta=0.5\\)）．\n\n\n\n\n\n正解数: 85 vs. 85\n正解率: 94.4 % vs. 94.4 %\n\n\nやはり，温度が高い場合はクラスター中心が合流・融合してしまいやすいが，冷却することでクラスター数は大きい状態で安定する，と言えるだろう．"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#本番データセットでの実験",
    "href": "posts/2024/Computation/VI.html#本番データセットでの実験",
    "title": "変分推論１",
    "section": "4 本番データセットでの実験",
    "text": "4 本番データセットでの実験\n今まで使っていたデータ1.3はクラスターのオーバーラップはなかったため，いわば優しいデータであった．ここからはよりデータ生成過程が複雑なデータを用いて，ソフト \\(K\\)-平均法の挙動を観察する．\n\n4.1 データの概観\n今度は，次の４クラスのデータを用いる．\n\n\n\n\n\n\n\n\n\n実は，これは４つの Gauss 分布から生成されたデータである．\n\n\n4.2 最適な温度の選択\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正解数: 377 vs. 366     正解率: 83.8 % vs. 81.3 %     反復数: 49 回 vs. 62 回\n正解数: 386 vs. 375     正解率: 85.8 % vs. 83.3 %     反復数: 101 回 vs. 70 回\n正解数: 378 vs. 378     正解率: 84.0 % vs. 84.0 %     反復数: 39 回 vs. 14 回"
  },
  {
    "objectID": "posts/2024/Computation/VI.html#sec-result",
    "href": "posts/2024/Computation/VI.html#sec-result",
    "title": "変分推論１",
    "section": "5 実験結果まとめ",
    "text": "5 実験結果まとめ\n\n\n\n\n\n\n結論\n\n\n\n\nデータ 1.3 に対して，（初期値 \\((m'_1,m_2,m_3)\\) で）ソフト \\(K\\)-平均法を適用すると，\n\n\\(\\beta\\ge2\\) の場合で結果はハード \\(K\\)-平均法と変わらなくなる．\n\\(\\beta=1\\) の場合で結果はクラスターがほとんど２つになり，\\(\\beta\\le0.5\\) では計算機上では実際に２つになってしまう．\n正答率は \\(1\\le\\beta\\le1.1\\) で最大であった．\n\\(\\beta\\) を大きくするほど，反復回数は減少していった．\n\nデータ 4.1 に対しても，以上の４点について同様の傾向が確認できた．\n\n\n\nこうしてソフト \\(K\\)-平均法とハード \\(K\\)-平均法の性質は分かった．主に\n\n初期値依存性\nクラスタ数 \\(K\\) の選択法\n\nの問題が未解決であり，恣意性が残る．\nこれを，ベイズ混合モデリングにより解決する方法を次稿で紹介する："
  },
  {
    "objectID": "posts/2024/Computation/VI.html#footnotes",
    "href": "posts/2024/Computation/VI.html#footnotes",
    "title": "変分推論１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(MacKay, 2003, p. 285)．↩︎\nLloyd は 1957 年には発表していたが，論文の形になったのが 1982 である．\\(K\\)-means という名前の初出は (MacQueen, 1967) とされている．↩︎\n(MacKay, 2003, p. 284)，(Bishop, 2006, p. 429)，(Murphy, 2022, p. 722) 21.3.3節．クラスター中心は 符号表ベクトル または 代表ベクトル (code-book vector) という．↩︎\nstiffness の用語は (MacKay, 2003, p. 289) から．実は各クラスターに Gauss モデルを置いた場合の分散 \\(\\sigma^2\\) に対して，\\(\\beta=\\frac{1}{2\\sigma^2}\\) の関係がある．次稿 参照．↩︎\n(MacKay, 2003, p. 291)．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html",
    "href": "posts/2024/Computation/VI2.html",
    "title": "変分推論２",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#最尤推定",
    "href": "posts/2024/Computation/VI2.html#最尤推定",
    "title": "変分推論２",
    "section": "1 最尤推定",
    "text": "1 最尤推定\nクラスタリングを一度さっぱり忘れて，最尤推定を思い出してみる．\n\n\n\n\n\n\n定義：最尤推定量1 (Fisher, 1912)\n\n\n\n\\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\subset\\mathcal{P}(\\mathcal{X})\\) を統計モデルで，ある共通の \\(\\sigma\\)-有限測度 \\(\\mu\\in\\mathcal{P}(\\mathcal{X})\\) に関して密度 \\(\\{p_\\theta\\}_{\\theta\\in\\Theta}\\) を持つとする．\n独立な観測 \\(X_1,\\cdots,X_n\\) の 最尤推定量 とは，モデルの対数尤度 \\[\n\\log p_\\theta\n\\] を通じて定まる次の目的関数 \\(\\ell_n:\\Theta\\to(-\\infty,0)\\) を最大化するような \\(M\\)-推定量 をいう： \\[\n\\ell_n(\\theta;X_1,\\cdots,X_n):=\\sum_{i=1}^n\\log p_\\theta(X_i),\\qquad\\theta\\in\\Theta.\n\\]\n\n\n\n1.1 最尤推定と最適化\nすなわち，最尤推定量とは最適化問題の解として定式化されるのである．\n最大値点であるということは，停留点である必要があるから，微分が零になるという条件を通じて解析的に求まることもある．この \\(Z\\)-推定量としての特徴付けは (Carmer, 1946, p. 498) による．\nまた，計算機的な方法では，Iterative Propertional Fittting や勾配に基づく最適化手法を用いることも考えられる (Robbins and Monro, 1951), (Fletcher, 1987)．\n最尤推定量が解析的に求まらない場面には，代表的には欠測モデルなどがある．欠測モデルは，観測される確率変数 \\(X\\) の他に，観測されない確率変数 \\(Z\\) も想定し，その同時分布を考えるモデルである．これにより，\\((X,Z)\\) 全体には単純な仮定しか置かずとも，\\(X\\) に対して複雑な分布を想定することが可能になるのである．\nこの場合には，モデルの構造を利用して最尤推定量を求めるための MM アルゴリズム (Sun et al., 2016) の例がある．これが EM アルゴリズム (Dempster et al., 1977) である．\n現在でも，その他の MM アルゴリズムが，種々の最適化問題に対する “problem-driven algorithm” であり続けている (T. T. Wu and Lange, 2010)．\n\n\n1.2 最尤推定と Bayes 推定\n最尤推定は，一様事前分布をおいた場合の MAP 推定 とみなせる．この意味で，Bayes 推定の特殊な場合である．\nBayes 推定は MCMC や SMC などのサンプリング法によって統一的に行えるが，殊に MAP 推定に対しては，効率的な最適化法として EM アルゴリズムが使える，ということである．\nより一般の Bayes 推定に対応できるような EM アルゴリズムの一般化が，近似アルゴリズムとして存在する．これが次稿で紹介する 変分推論 である．"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#sec-EM0",
    "href": "posts/2024/Computation/VI2.html#sec-EM0",
    "title": "変分推論２",
    "section": "2 EM アルゴリズム",
    "text": "2 EM アルゴリズム\nEM アルゴリズムは，混合モデルに対する最尤推定アルゴリズムである．一般に，目的関数が \\[\nh(x)=\\operatorname{E}[H(x,Z)]\n\\] と表せる場合に対する特殊な MM アルゴリズムである．2\n\n2.1 欠測データと混合モデル\n欠測データ (incomplete data) とは，２つの確率変数 \\((Z,X)\\) について次の図式が成り立つ際の，\\(Z\\) を潜在変数として，\\(X\\) からの観測とみなせるデータをいう (Dempster et al., 1977, p. 1)：\n\n\n\n\n\n\n図 1: Missing Data Model / Latent State Model / Completed Model for \\(X\\)\n\n\n\nこれは， \\[\np(x|\\theta)=\\int_{\\mathcal{Z}}p(x,z|\\theta)\\,dz\n\\tag{1}\\] という形の尤度を持つモデルである．3\nこれは潜在変数 \\(Z\\) を持つモデルの最も単純な例ともみなせる．特に \\(Z\\) が離散変数である場合，\\(X\\) に対する混合モデルともいう．隠れ Markov モデル はこの発展例である．4\nこのように，\\(X\\) の分布を，潜在変数 \\(Z\\) を追加して理解することを，モデルの 完備化 (completion) または 脱周辺化 (demarginalization)，またはデータの拡張 (data augmentation) ともいう．5\n\n\n2.2 EM アルゴリズム\n値域 \\(\\mathcal{Z}\\) を持つ潜在変数 \\(Z\\) とパラメータ \\(\\theta\\in\\Theta\\) に関して Equation 1 で表せる尤度関数 \\(p(x|\\theta)\\) に関して，Jensen の不等式より，任意の \\(x,\\theta\\) で添字づけられた確率密度関数 \\(q:\\mathcal{Z}\\to\\mathbb{R}_+\\)6 とパラメータ \\(\\theta\\in\\Theta\\) について次の評価が成り立つ：\n\\[\n\\begin{align*}\n    \\log p(x|\\theta)&=\\log\\int_{\\mathcal{Z}}p(x,z|\\theta)\\,dz\\\\\n    &\\ge\\int_\\mathcal{Z}q(z|x,\\varphi)\\log\\frac{p(x,z|\\theta)}{q(z|x,\\varphi)}\\,dz\\\\\n    &=:F(q,\\theta).\n\\end{align*}\n\\tag{2}\\]\nこの事実に基づき，\\(F\\) を代理関数として，これを２つの変数 \\(q,\\theta\\) について交互に最大化するという手続きを，EM アルゴリズム という．7\n\n\\(E\\)-ステップ：\\(F\\) を \\(q\\) について最大化する． \\[\n\\begin{align*}\nF(q,\\theta)&=\\int_\\mathcal{Z}q(z|x,\\varphi)\\log\\frac{p(z|x,\\theta)p(x|\\theta)}{q(z|x,\\varphi)}\\,dz\\\\\n&=\\log p(x|\\theta)-\\operatorname{KL}(q_\\varphi,p_\\theta).\n\\end{align*}\n\\] より，\\(q(z|x,\\varphi)=p(z|x,\\theta)\\) で最大化される．8\n\\(M\\)-ステップ：\\(F\\) を \\(\\theta\\) について最大化する． \\[\n\\begin{align*}\nF(q,\\theta)&=\\int_{\\mathcal{Z}}q(z|x,\\varphi)\\log p(x,z|\\theta)\\,dz\\\\\n&\\qquad-\\int_\\mathcal{Z}q(z|x,\\varphi)\\log q(z|x,\\varphi)\\,dz\\\\\n&=\\underbrace{(q_\\varphi dz\\,|\\log p_\\theta)}_{=:Q(\\theta|\\varphi,x)}+H(q_\\varphi)\n\\end{align*}\n\\] より，\\(Q\\) の停留点で最大化される．\n\n総じて，EM アルゴリズムは \\(p,q\\) の KL 乖離度を逐次的に最小化している．\n\n\n2.3 \\(E\\)-ステップの変形\n\\(M\\)-ステップにおける \\(F\\) の \\(\\theta\\) における最大化は \\(Q\\) の \\(\\theta\\) による最大化に等価であるから，\\(E\\)-ステップは結局，事後分布 \\(p(z|x,\\theta)\\) を計算し，これに関する積分である \\[\nQ(\\theta|\\varphi,x)=\\int_\\mathcal{Z}p(z|x,\\theta)\\log p(x,z|\\theta)\\,dz\n\\] を計算する，というステップになる．\nモデル \\(\\{p_\\theta\\}\\) を複雑にしすぎた場合，この \\(Q\\) の計算は困難で実行不可能になってしまう．解析的に \\(E\\)-ステップを実行したい場合，典型的には指数型分布族を仮定する．\nそこで，\\(Q\\) を Monte Carlo 推定量で代替して，それを最大化した場合の EM アルゴリズムを MCEM (Monte Carlo EM) という (Wei and Tanner, 1990a), (Wei and Tanner, 1990b)．典型的には Metropolis-Hastings アルゴリズムを用いることになり (Chau et al., 2021)，これがスケーラビリティ問題を産む．9\nまた，この \\(E\\)-ステップで必ずしも完全な最大化を達成する必要はない (Neal and Hinton, 1998), (Bishop, 2006, p. 454)．従って，\\(p(z|x,\\theta)\\) が複雑すぎる場合，十分近い \\(q\\) を選択してこれに関する積分として \\(Q\\) を近似することが考えられる．特に \\(p\\) を 変分近似 した場合，変分 EM アルゴリズムという (Wainwright and Jordan, 2008, p. 154)．\n\n\n2.4 \\(M\\)-ステップの変形\n\\(Q\\) の停留点を探すにあたって，典型的には微分が消える点を探す．\nしかしこれが難しい場合，厳密な最大化は行わず，代わりにせめて「現状よりは大きくする」ことを実行するアルゴリズムを用いた場合，これを 一般化 EM アルゴリズム (GEM: Generalized EM) ともいう (Bishop, 2006, p. 454), (Hastie et al., 2009, p. 277)．\n例えば，大域的最大化の代わりに条件付き最大化を行うこととする方法 ECM (Expectation Conditional Maximization) などがその例である (Meng and Rubin, 1991), (Meng and Rubin, 1993)．(Christian P. Robert and Casella, 2004, p. 200) も参照．\n\n\n2.5 EM アルゴリズムの有効性\n\n\n\n\n\n\n命題：尤度は単調減少する (Dempster et al., 1977)10\n\n\n\n\\(\\{\\widehat{\\theta}_{(j)}\\}\\subset\\Theta\\) を EM アルゴリズムの \\(M\\)-ステップでの出力列とする．このとき，\n\\[\nL(\\widehat{\\theta}_{(j+1)}|x)\\ge L(\\widehat{\\theta}_{(j)}|x).\n\\] 等号成立は \\[\nQ(\\widehat{\\theta}_{(j+1)}|\\widehat{\\theta}_{(j)},x)=Q(\\widehat{\\theta}_{(j)}|\\widehat{\\theta}_{(j)},x)\n\\] の場合のみ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n命題：局所解への収束 (Boyles, 1983)-(C. F. J. Wu, 1983)11\n\n\n\n\\[\nQ(\\theta|\\theta_0,x):=\\int_\\mathcal{Z}p(z|\\theta,x)\\log p(\\theta|x,z)\\,dz\n\\] は \\(\\theta,\\theta_0\\in\\Theta\\) について連続であるとする．このとき，EM アルゴリズムの出力 \\(\\{\\widehat{\\theta}_(j)\\}\\) は尤度 \\(p(\\theta|x)\\) の停留点に単調に収束する．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\nよって，EM アルゴリズムは局所解には収束する．\nしかし，常に尤度が単調増加するという性質上，局所解に囚われてしまった場合，そこから逃れることはないことになる．\n大域解に収束することを保証したい場合は，異なる初期値で複数回 EM アルゴリズムを実行するか，擬似除冷 (simulated annealing)12 などの別の手法を用いることを考える必要がある (Finch et al., 1989)．"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#em-アルゴリズムの実装gauss-有限混合モデルの場合",
    "href": "posts/2024/Computation/VI2.html#em-アルゴリズムの実装gauss-有限混合モデルの場合",
    "title": "変分推論２",
    "section": "3 EM アルゴリズムの実装（Gauss 有限混合モデルの場合）",
    "text": "3 EM アルゴリズムの実装（Gauss 有限混合モデルの場合）\n負担率に確率モデルを置いた場合，ソフト \\(K\\)-平均アルゴリズム は EM アルゴリズムになる．\nEM アルゴリズムは一般に多峰性に弱いことをここで示す．13\n\n3.1 Guass 有限混合モデル\nここでは，以下の，有限な混合モデルで，さらに混合される分布は正規であるものを考える：\n\n\n\n\n\n\n定義 (Gaussian finite mixture model)\n\n\n\n集合 \\([K]\\) 上に値を取る隠れ変数 \\(Z\\) の確率質量関数を \\((p_k)_{k=1}^K\\) とする． \\[\np(x;(\\mu_k),(\\Sigma_k),(p_k))=\\sum_{k=1}^K p_k\\phi(x;\\mu_k,\\Sigma_k)\n\\tag{3}\\] として定まるモデル \\((p_{(\\mu_k),(\\Sigma_k),(p_k)})\\) を \\(\\mathbb{R}^d\\) 上の Gauss 有限混合モデル という．\nただし，\\(\\phi(x\\); \\(\\mu\\), \\(\\sigma)\\) は \\(\\mathrm{N}_d(\\mu,\\sigma^2)\\) の密度とした．\n\n\nEquation 3 は \\((X,Z)\\) 上の結合分布の族を表しており，そのパラメータは \\(\\theta:=((\\mu_k),(\\Sigma_k),(p_k))\\) である．さらに，\\(\\theta_k:=(\\mu_k,\\Sigma_k)\\) と定める．\n\n\n3.2 Gauss 有限混合モデルでの EM アルゴリズム\nSection 2.2 での議論を，今回の Gauss 有限混合モデルに当てはめてみる．\n対数周辺尤度は\n\\[\n\\begin{align*}\n    \\log p(x|\\theta)&=\\log\\left(\\sum_{k=1}^K q_k\\frac{p_k\\phi(x|\\theta_k)}{q_k}\\right)\\\\\n    &\\ge\\sum_{k=1}^Kq_k\\log\\left(\\frac{p_k\\phi(x|\\theta_k)}{q_k}\\right)\\\\\n    &=:F(q_k,\\theta).\n\\end{align*}\n\\]\nという下界を持つ．\nこれに基づき，観測 \\(\\{x^{(n)}\\}_{n=1}^N\\) と混合 Gauss モデル 3.1 に対する EM アルゴリズムは次の２段階を繰り返す：\n\n\\(E\\)-ステップ： \\[\n\\begin{align*}\n     r_k^{(n)}&\\gets\\operatorname{P}[Z=k|x^{(n)},\\theta]\\\\\n     &=\\frac{p_k\\phi(x^{(n)}|\\theta_k)}{\\sum_{j=1}^Kp_j\\phi(x^{(n)}|\\theta_j)}\n\\end{align*}\n\\] を計算して，\\(F\\) に代入する．\n\\(M\\)-ステップ：\\(F\\) を \\(\\theta\\) について最大化する．これは，次の値を計算することに等しい：\n\n\\[\n\\mu_k\\gets\\frac{\\sum_{n=1}^Nr_k^{(n)}x^{(n)}}{\\sum_{n=1}^Nr_k^{(n)}}\n\\]\n\\[\n\\Sigma_k\\gets\\frac{\\sum_{n=1}^Nr_{k}^{(n)}(x^{(n)}-\\mu_k)(x^{(n)}-\\mu_k)^\\top}{\\sum_{n=1}^Nr_{k}^{(n)}}\n\\]\n\\[\np_k\\gets\\frac{\\sum_{n=1}^Nr_{k}^{(n)}}{N}\n\\]\n\n\n\n\n\n\n\n\n\\(E\\)-ステップの導出\n\n\n\n\n\n最初のステップは Bayes の定理から，\n\\[\n\\begin{align*}\n    \\operatorname{P}[Z=k|x,\\theta]&=\\frac{p(x|z=k,\\theta)p(z=k|\\theta)}{p(x|\\theta)}\\\\\n    &=\\frac{p_k\\phi(x|\\theta_k)}{\\sum_{j=1}^Kp_j\\phi(x|\\theta_j)}.\n\\end{align*}\n\\] の計算に帰着する．\\(\\{p_k\\}\\) が一様で，\\(\\sigma_k=\\sigma\\) も一様であるとき，これはソフト \\(K\\)-平均法における 負担率 \\(r_{kn}\\) に他ならない．このとき， \\[\n\\beta=\\frac{1}{2\\sigma^2}.\n\\]\nこうして，負担率とは，「データ点がそのクラスターに属するという事後確率」としての意味も持つことが判った．\n\n\n\n\n\n\n\n\n\n\\(M\\)-ステップの導出14\n\n\n\n\n\n2,3,4 はそれぞれ条件 \\[\n\\frac{\\partial }{\\partial \\mu_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] \\[\n\\frac{\\partial }{\\partial \\Sigma_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] \\[\n\\frac{\\partial }{\\partial p_k}\\sum_{n=1}^N\\log p(x_n|\\theta)=0\n\\] から出る．\n最大化に Newton-Raphson 法を用いたとも捉えられる．15\n\n\n\n\n\n3.3 \\(K\\)-平均アルゴリズムとの対応\n\\(E\\)-ステップが assignment ステップ，\\(M\\)-ステップが update ステップに対応する．\nハード \\(K\\)-平均法は，歪み尺度 (distortion measure) \\[\nJ(r,\\mu):=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\|x_n-\\mu_k\\|^2\n\\] を \\(r,\\mu\\) のそれぞれについて逐次的に最小化する手法とも見れる．16\n\n\n3.4 Gauss 混合モデルの場合17\n\\(K=2\\) での Gauss 混合分布 \\[\np\\mathrm{N}(\\mu_1,\\sigma^2)+(1-p)\\mathrm{N}(\\mu_2,\\sigma^2),\n\\tag{4}\\] \\[\np=0.7,\\quad\\sigma=1,\n\\] を考える．未知パラメータは \\(\\theta:=(\\mu_1,\\mu_2)\\) である．\n実は，混合モデルでは，ここまで単純な例でさえ，尤度は多峰性を持つ．\n試しに，\\((\\mu_1,\\mu_2)=(0,3.1)\\) として 500 個のデータを生成し，モデル 4 が定める尤度をプロットしてみると，次の通りになる：\n\n\n&lt;&gt;:42: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:43: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:44: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:42: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:43: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:44: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_74977/1951455018.py:42: SyntaxWarning: invalid escape sequence '\\m'\n  plt.title('Log Likelihood for 500 Observations with $(\\mu_1,\\mu_2)=(0,3.1)$')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_74977/1951455018.py:43: SyntaxWarning: invalid escape sequence '\\m'\n  plt.xlabel('$\\mu_1$')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_74977/1951455018.py:44: SyntaxWarning: invalid escape sequence '\\m'\n  plt.ylabel('$\\mu_2$')\n\n\n\n\n\n\n\n\n\n真値 \\((\\mu_1,\\mu_2)=(0,3.1)\\) で確かに最大になるが，\\((\\mu_1,\\mu_2)=(2,-0.5)\\) 付近で極大値を取っていることがわかる．\n\n\n3.5 EM アルゴリズムの初期値依存性\nEM アルゴリズムはその初期値依存性からランダムな初期値から複数回実行してみる必要がある．モデル 4 の場合，その結果は次のようになる：\n\n\nCode\nclass EM_1d:\n    \"\"\"\n    Gauss 有限混合モデルに対する EM アルゴリズム\n\n    Parameters:\n    - K (int): 混合成分の数．デフォルトは2．\n    - max_iter (int): アルゴリズムの最大反復回数．デフォルトは100．\n    - tol (float): 収束の閾値．連続する反復での対数尤度の差がこの値以下になった場合，アルゴリズムは収束したと見なされる．デフォルトは1e-4．\n    \"\"\"\n\n    def __init__(self, K=2, init=None, max_iter=100, tol=1e-4):\n        self.K = K\n        self.max_iter = max_iter\n        self.tol = tol\n\n        self.means = None\n        self.variances = None\n        self.mixing_coefficients = None\n        self.log_likelihood_history = []\n        self.mean_history = []\n        self.initial_value = init\n\n    def expectation(self, X):\n        \"\"\"\n        E ステップ\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        N = X.shape[0]\n        r = np.zeros((N, self.K))\n        for k in range(self.K):\n            pdf = norm.pdf(X, self.means[k], np.sqrt(self.variances[k]))\n            r[:, k] = self.mixing_coefficients[k] * pdf\n        r /= r.sum(axis=1, keepdims=True)\n        return r\n\n    def maximization(self, X, r):\n        \"\"\"\n        M ステップ\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        - r (ndarray): 負担率．\n        \"\"\"\n        N = X.shape[0]\n        Nk = r.sum(axis=0)\n        self.means = (X.T @ r / Nk).T\n        self.variances = np.zeros(self.K)\n        for k in range(self.K):\n            diff = X - self.means[k]\n            self.variances[k] = (r[:, k] @ (diff ** 2)) / Nk[k]\n        self.mixing_coefficients = Nk / N\n\n    def compute_log_likelihood(self, X):\n        \"\"\"\n        対数尤度の計算\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        log_likelihood = 0\n        for x in X:\n            log_likelihood += np.log(np.sum([self.mixing_coefficients[k] * norm.pdf(x, self.means[k], np.sqrt(self.variances[k])) for k in range(self.K)]))\n        return log_likelihood\n    \n    def fit(self, X):\n        \"\"\"\n        EM アルゴリズムの実行\n\n        Parameters:\n        - X (ndarray): 観測データ．\n        \"\"\"\n        N = X.shape[0]\n        np.random.seed(42)\n\n        if self.initial_value is None:\n            random_indeces = np.random.choice(N, self.K, replace=False)\n            self.initial_value = X[random_indeces]\n        self.means = self.initial_value\n        self.initial_value = self.means\n        self.variances = np.ones(self.K)\n        self.mixing_coefficients = np.ones(self.K) / self.K\n\n        # 反復\n        for _ in range(self.max_iter):\n            r = self.expectation(X)\n            self.maximization(X, r)\n            log_likelihood = self.compute_log_likelihood(X)\n            self.log_likelihood_history.append(log_likelihood)\n            self.mean_history.append(self.means)\n\n            if len(self.log_likelihood_history) &gt;= 2 and np.abs(self.log_likelihood_history[-1] - self.log_likelihood_history[-2]) &lt; self.tol:\n                break\n        \n        return self\n\n\n\n\n&lt;&gt;:32: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:33: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:34: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:32: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:33: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:34: SyntaxWarning: invalid escape sequence '\\m'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_74977/163584773.py:32: SyntaxWarning: invalid escape sequence '\\m'\n  axs[1].set_title('Mean Values Progress ($\\mu_1,\\mu_2$)')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_74977/163584773.py:33: SyntaxWarning: invalid escape sequence '\\m'\n  axs[1].set_xlabel('$\\mu_1$')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_74977/163584773.py:34: SyntaxWarning: invalid escape sequence '\\m'\n  axs[1].set_ylabel('$\\mu_2$')"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#sec-data-augmentation",
    "href": "posts/2024/Computation/VI2.html#sec-data-augmentation",
    "title": "変分推論２",
    "section": "4 Monte Carlo 法による解決",
    "text": "4 Monte Carlo 法による解決\nEquation 1 の逆向きの関係 \\[\n\\begin{align*}\n    p(\\theta|x)&=\\int_\\mathcal{Z}p(z,\\theta|x)\\,dz\\\\\n    &=\\int_\\mathcal{Z}p(\\theta|z,x)p(z|x)\\,dz\n\\end{align*}\n\\] も成り立つという 階層構造 (hierarchical structure) を持つモデルにおいて，Bayes 推論が Gibbs サンプラーによって実行できる (C. P. Robert, 1996)．18\nこのような欠測モデルの文脈で Gibbs サンプラーを用いる手法は，データ拡張 の名前でも知られる (Tanner and Wong, 1987)．\n加えて，初期値依存性や局所解へのトラップが懸念されるという EM アルゴリズムの問題点を，MCMC はいずれも持ち合わせていない．\nさらに，混合数 \\(K\\) に関する検定も構成できる (Mengersen and Robert, 1996) など，Gibbs サンプラーひとつで確率モデルに関する種々の情報を取り出せる．\n最尤推定の代わりに Bayes 推定を行なっているため，データ数が少なくとも，過学習の問題が起こりにくいという利点もある．\nBayes 階層モデルは複雑なモデルに対する表現力が高く，地球科学をはじめとして多くの応用分野で使われている (Hrafnkelsson, 2023)．\n\n4.1 Gibbs サンプリング\n高次元な確率変数 \\((U_1,\\cdots,U_K)\\) のシミュレーションを行いたい場合，直接行うのではなく，条件付き分布 \\(p(u_k|u_{-k})\\) からのサンプリングを繰り返すことでこれを行うことが出来る．19\n\n任意の初期値 \\(U_1^{(0)},\\cdots,U_K^{(0)}\\) を与える．\n各 \\(k\\in[K]\\) について， \\[\nU_k^{(t)}\\sim p(u_k|U_{-k}^{(t-1)})\n\\] をサンプリングする．\n十分時間が経過した際，アルゴリズムの出力 \\((U^{(t)}_1,\\cdots,U^{(t)}_K)\\) は \\((U_1,\\cdots,U_K)\\) と同分布になる．\n\n実際，\\(\\{(U_1^{(t)},\\cdots,U_K^{(t)})\\}_{t\\in\\mathbb{N}}\\) はエルゴード的な Markov 連鎖を定め，定常分布 \\(p(U_1,\\cdots,U_K)\\) を持つ．\n\n\n\n\n\n\n命題 (Jean Diebolt and Robert, 1994)20\n\n\n\n\\(p(u_1|u_2)\\) が正，または \\(p(u_2|u_1)\\) が正ならば，Markov 連鎖 \\(\\{U_1^{(t)}\\},\\{U_2^{(t)}\\}\\) はいずれもエルゴード的で，不変分布 \\(p(u_1|u_2),p(u_2|u_1)\\) を持つ．\n\n\n\n\n4.2 確率的 EM アルゴリズム\nGibbs サンプリングアルゴリズムは，EM アルゴリズム2.2の変形とみなせる：\n\n\\(E\\)-ステップ：EM アルゴリズムでは \\[\nQ(\\theta|\\vartheta,x):=(p_\\vartheta dz\\,|\\log p_\\theta)\n\\] を評価するところであったが，Gibbs サンプリングでは，\\(p(z|x,\\vartheta)\\) のサンプリングを行う．\n\\(M\\)-ステップ：EM アルゴリズムでは \\[\n\\operatorname*{argmax}_{\\vartheta\\in\\Theta}Q(\\vartheta|\\theta,x)=(p_\\theta dz\\,|\\log p_\\vartheta)\n\\] を求めるところであったが，Gibbs サンプリングでは，\\(p(\\theta|z,x)\\) のサンプリングを行う．\n\nこれは \\(E\\)-ステップでの \\(Q\\) 関数の評価が困難であるとき，\\(p(z|x,\\theta)\\) からのサンプリングでこれを回避できるという美点もある．\n\n4.2.1 EM アルゴリズムへの部分的な適用：\\(E\\)-ステップ\nまたこの美点のみを用いて，\\(p(z|x,\\theta)\\) からサンプリングをして \\(Q\\) の Monte Carlo 推定量 \\[\nQ(\\theta)=\\frac{1}{M}\\sum_{m=1}^M\\log p(x,z^{(m)}|\\theta)\n\\] を計算し，\\(M\\)-ステップとしてこれを最大化して \\(\\{\\widehat{\\theta}_j\\}\\) を得るという 確率的 EM アルゴリズム (Stochastic EM) も考えられる (Celeux and Diebolt, 1985)．21\nこの場合，\\(\\{\\widehat{\\theta}_j\\}\\) は多くの場合エルゴード的な Markov 連鎖を定めるが，これがどこに収束するかの特定が難しい (J. Diebolt and Ip, 1996)．\n\n\n4.2.2 EM アルゴリズムへの部分的な適用：\\(M\\)-ステップ\nGibbs サンプリングの考え方を \\(M\\)-ステップにのみ導入し，\\(M\\)-ステップを完全に最大化するのではなく「条件付き最大化」に置き換えても，EM アルゴリズム本来の収束性は保たれる．\\(\\theta=(\\theta_1,\\theta_2)\\) と分解できる際に，いずれか片方ずつのみを最大化する，などである．これを ECM (Expectation Conditional Maximization) アルゴリズムという (Meng and Rubin, 1991), (Meng and Rubin, 1993)．\n\\(M\\)-ステップのみを確率的にすることで，EM アルゴリズムの局所解へのトラップを改善することができる．そのような手法の例に，SAME (State Augmentation for Marginal Estimation) (Doucet et al., 2002) などがある．\n\n\n\n4.3 諸言\n欠測モデル2.1のように，一般に グラフィカルモデル として知られる，局所的な関係のみから指定されるモデルや潜在変数を持つモデルでは，Gibbs サンプリングにより効率的に結合分布からサンプリングができる．\nMCMC はグラフィカルモデルを用いた Bayes 推論の，強力な武器である．22"
  },
  {
    "objectID": "posts/2024/Computation/VI2.html#footnotes",
    "href": "posts/2024/Computation/VI2.html#footnotes",
    "title": "変分推論２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Carmer, 1946, p. 498) によると，(Fisher, 1912) が初出であるが，以前に Gauss がその特別な形を用いていた．また，(Carmer, 1946, p. 499) での定義はこことは違い，尤度関数の停留点（＝尤度方程式の解）と定義している．↩︎\n(Christian P. Robert and Casella, 2004, p. 174) 式(5.8)．↩︎\n(Christian P. Robert and Casella, 2004, p. 174) 式(5.7)．\\(p\\) を完備化された尤度 (completed likelihood) ともいう．\\(X\\) を incomplete data, \\((X,Z)\\) を complete data ともいう (Bishop, 2006, pp. 433, p.440)．↩︎\n特に，隠れ Markov モデルの文脈では，EM アルゴリズムは Baum-Welch アルゴリズム とも呼ばれる (Chopin and Papaspiliopoulos, 2020, p. 70)．↩︎\nそれぞれ，(Christian P. Robert, 2007, p. 330)，(Christian P. Robert and Casella, 2004, p. 176)，(Hastie et al., 2009, p. 276)，↩︎\n正確には確率核 \\(Q:\\mathcal{X}\\times\\Theta\\to\\mathcal{Z}\\)．↩︎\nこの \\(F\\) は多く \\(Q\\) とも表され，\\(Q\\)-関数ともいう．\\(p(x|\\theta)\\) やその対数は 証拠 (evidence) ともいうので，\\(F\\) は 証拠下界 (ELBO: Evidence Lower BOund) ともいう．↩︎\n式変形は (Bishop, 2006, p. 450) も参照．この \\(p(z|x,\\theta)\\) は観測 \\(x\\) の下での，潜在変数 \\(z\\) の条件付き分布である．しかし，このように双方を最大化ステップと見る変分法的な見方が出来るのである (Wainwright and Jordan, 2008, pp. 153–154), (Neal and Hinton, 1998), (Hastie et al., 2009, p. 277)．よって，この \\(E\\)-ステップも，GEM のように，必ずしも完全な最大化を達成する必要はないことがわかる (Neal and Hinton, 1998), (Bishop, 2006, p. 454)．例えば変分近似を行った場合，変分 EM アルゴリズムができあがる (Wainwright and Jordan, 2008, p. 154)．↩︎\n(Johnston et al., 2024) にも言及あり．↩︎\n(Christian P. Robert and Casella, 2004, p. 177) 定理5.15，(Christian P. Robert, 2007, p. 334) 演習6.52．↩︎\n(Christian P. Robert and Casella, 2004, p. 178) 定理5.16．↩︎\nこの用語は (甘利俊一, 1989, p. 141) の 模擬除冷 の表現に触発された．↩︎\n(Christian P. Robert and Casella, 2004) も参照．↩︎\n(Bishop, 2006, pp. 436–439) も参照．↩︎\n(MacKay, 2003, p. 303)．↩︎\n(Bishop, 2006, p. 424)．↩︎\n(Christian P. Robert and Casella, 2004, pp. 181–182) 例5.19 も参照．↩︎\n(Christian P. Robert, 2007, p. 307) も参照．↩︎\n\\(u_{-k}:=u_{1:(k-1),(k+1):K}\\) とした．↩︎\n(Christian P. Robert, 2007, p. 309) 補題6.3.6，(鎌谷研吾, 2020, p. 139) 定理5.7．↩︎\n(Christian P. Robert and Casella, 2004, p. 200) 5.5.1 節も参照．↩︎\n(Christian P. Robert, 2007, p. 318) も参照．↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes for Self Study",
    "section": "",
    "text": "サーベイ | レビュー | カテゴリ\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMasked Diffusion Models\n\n\nA New Light to Discrete Data Modeling\n\n\n\nDenoising Model\n\n\n\nMasked diffusion models are conceptually based upon an absorbing forward process and its reverse denoising process. However, their roles are intricately intertwined, in that all three aspects of training, sampling, and modeling are involved. To develop our understanding, we give two toy examples, 1d and 2d, without training a neural network, to showcase how absorbing processes behave. We identify core questions which should be investigated to expand our understanding. \n\n\n\n\n\n9/15/2025\n\n\nHirofumi Shiba\n\n\nCTDDM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nシンガポール研究滞在記\n\n\n\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n6/25/2025\n\n\n司馬博文\n\n\nNUS.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nBayesComp2025 １日目\n\n\n\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nBayesComp2025 の１日目の参加記． 聞いた発表の概要と考えたことを書く． \n\n\n\n\n\n6/16/2025\n\n\n司馬 博文\n\n\nBayesComp1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC と PDMP の概観\n\n\n\n\n\n\nLife\n\n\nSurvey\n\n\nPDMP\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n5/31/2025\n\n\n司馬 博文\n\n\nOverview.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n博士課程での研究テーマ\n\n\n所信表明：残り３年をどう過ごすか\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n5/24/2025\n\n\n司馬 博文\n\n\nThemes.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP サンプラーの高次元 Gauss での挙動\n\n\n確率過程によるアプローチ\n\n\n\nPDMP\n\n\nMCMC\n\n\nProcess\n\n\n\n\n\n\n\n\n\n4/28/2025\n\n\n司馬 博文\n\n\nHDG.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nA Function that is Continuous on a Closed Subset\n\n\n\n\n\n\nTopology\n\n\n\nThe set of all continuity points of a function is a \\(G_\\delta\\) set. Conversely, every \\(G_\\delta\\) set is the set of all continuity points of some function. We construct an example of a function \\(f:\\mathbb{R}\\to\\mathbb{R}\\) that is continuous on \\((-\\infty,0]\\) but discontinuous on \\((0,\\infty)\\). \n\n\n\n\n\n4/17/2025\n\n\nHirofumi Shiba\n\n\nContiunitySet.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる\n\n\n\n\n\n\nPDMP\n\n\n\n第19回日本統計学会春季集会でのポスター発表の予稿です．PDF 版はこちら \n\n\n\n\n\n2/25/2025\n\n\n司馬博文\n\n\n第19回日本統計学会春季集会.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nPDMP\n\n\n\n\n\n\n\n\n\n12/31/2024\n\n\nHirofumi Shiba\n\n\nDetails.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nノンパラメトリック回帰分析\n\n\n\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n\n\n\n\n\n12/30/2024\n\n\n司馬 博文\n\n\nNRegression.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n重回帰モデルにおける OLS 推定量は，部分回帰推定量としての解釈を持つ． この性質を用いた手法が媒介分析や操作変数法である． OLS 推定量は不均一分散の場合でも不偏性・一致性・漸近正規性を持ち得るが，漸近有効性は失われる． これを回復するには，誤差の分散を推定して重み付けを行う必要がある． このような方法は一般化最小二乗法と呼ばれる． さらに相関を持つデータを分析するために，より一般の共分散構造を持ったモデルに対してこの手法が拡張されている． 疫学では一般化推定方程式，さらに一般には計量経済学で一般化モーメント法と呼ばれる方法である． これらの方法は作業共分散の選択により，セミパラメトリック漸近最適な分散を達成したり，バイアスを小さくしたりできるが， いずれもトレードオフの範疇にある． \n\n\n\n\n\n12/29/2024\n\n\n司馬博文\n\n\nRegression.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\n\n\n\nProbability\n\n\nStatistics\n\n\n\nWe examine how to find & formally determine the likelihood function of hierarchical models. As a real-world example, we consider the ideal point model, also known as the 2-parameter logistic item response model. \n\n\n\n\n\n12/23/2024\n\n\nHirofumi Shiba\n\n\nlikelihood.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\nIdeal point models are 2-parameter item response model, tailored to the purpose of visualizing / measuring the ideological positions of the legislators / judges. (Bafumi et al., 2005) introduced a hierarchical structure to the model to deal with the problem of identifiability. In this article, we re-examine the model and show that the posterior distribution of the parameters (ideal points) is still bimodal, indicating its weak identifiability. \n\n\n\n\n\n12/22/2024\n\n\nHirofumi Shiba\n\n\nBafumi.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\nBayesian\n\n\nStatistics\n\n\nPDMP\n\n\n\n\n\n\n\n\n\n12/21/2024\n\n\n司馬 博文\n\n\nBayesSticky.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n\n\n\n\n\n12/21/2024\n\n\n司馬 博文\n\n\nBayesTraverse.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n12/16/2024\n\n\n司馬 博文\n\n\nBayesNonparametrics.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n\n\n\n\n\n12/15/2024\n\n\n司馬 博文\n\n\nIdealPoint3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n項目反応モデルとは，被験者と項目のそれぞれが独自のパラメータを持った一般化線型混合効果モデルである． 被験者ごとの特性の違いや，項目ごとの性質の違いが視覚化できるが， 本稿では能力・難易度パラメータに更なる階層構造を考える． これにより能力パラメータを変化させている背後の要因や，項目特性と個人特性の交絡効果（特異項目機能）を解析することが可能になる． brms パッケージは極めて直感的な方法でモデルのフィッティングから事後分布の推論までを実行できる． \n\n\n\n\n\n12/14/2024\n\n\n司馬博文\n\n\nBayesGLMM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\nロジスティック回帰分析は離散的な応答データを扱うことのできる一般化線型モデルである． 他にも，高度に非線型な関係が予期される場合，ノンパラメトリック手法に移る前の簡単な非線型解析としても活躍する． 本稿では BMI と LDL の非線型関係に関する探索的手法として，順序ロジスティック回帰分析を実行する． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\nBayesGLM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nデータが自然な階層構造を持つ場合，これを取り入れた自然な事前分布を，一つ上の階層に回帰モデルを付け加えることで構成できる． このようなモデルをベイズ階層モデルという． 本稿ではベイズ階層モデルの縮小効果を概観する． 事前知識を構造に関する知識としてモデルに取り入れることでデータによりフィットする尤度構造を獲得することは，データ解析の一つの目標として，（線型）回帰モデルの自然な拡張と理解できる． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\nBDA3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬博文\n\n\nFixedRandom.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析８\n\n\n正規グラフィカルモデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬 博文\n\n\nBDA4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n点推定における変数選択法は，正則化項の追加によることが多い． これはベイズ推論では \\(0\\) 近傍に大きな確率を持った事前分布を仮定していることに等しい． ベイズの観点から適切な縮小事前分布を用意することで，大きな効果を持つ回帰係数は変えずに， 効果の小さい変数を排除することができる． 一般に LASSO よりも絞って選択してくれることが多い．\nまたベイズ変数選択では，\\(0\\) にアトムを持つ事前分布を用いることで，当該の変数がモデルに含まれる事後確率 (PIP: Posterior Inclusion Probability) を算出することができる． この方法ではモデルの空間を効率的に探索するサンプラーの開発が重要であるが， 近年では効率的なサンプラーが複数提案されている． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\nBayesSelection.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\nベイズ重回帰分析は解析者のデータへの理解を促進する強力な探索的データ解析手法である． このことを brms パッケージと BMI データを用いて例証する． １変数の場合から始め，変数を追加して挙動が変わるのを解釈・検証（残差プロット・事後予測プロット）しながら慎重に進んでいく． 交差検証による事後予測スコア elpd を用いて，データの非線型変換を利用することで，非線型な関係を見出す方法を扱う． ここまで行えば，データの階層化やノンパラメトリックな手法の採用などの次のステップが自然と見えてくるだろう． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\nBayesRegression.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する． 分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる． また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\nBDA1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n通常の回帰モデルは応答変数が連続であることが暗黙の仮定となっている． この節では，応答変数が質的変数である場合のモデリングを扱う． 質的変数は順序変数であるか名目変数であるか（順序の構造があるかないか）の峻別が重要である． いずれの場合でも多くのモデルが利用可能であり，その多くが一般化線型モデルの枠組みで統一的に扱うことができる． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\nBDA2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n12/01/2024\n\n\n司馬博文\n\n\nUCL.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n理想点解析とは，政治学において国会議員のイデオロギーを定量化・視覚化する方法論である．この手法は多くの側面を持ち，項目反応モデルであると同時に多次元展開法 (MDU: Multidimensional Unfolding)でもある． \n\n\n\n\n\n11/22/2024\n\n\n司馬 博文\n\n\nIdealPoint2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\nJulia に存在する粒子フィルター関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n10/26/2024\n\n\n司馬 博文\n\n\nAdvancedPS.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\nPDMP / 連続時間 MCMC とは 2018 年に以降活発に研究が進んでいる新たな MCMC アルゴリズムである． 実用化を遅らせていた要因として，種々のモデルに統一的な実装が難しく，モデルごとにコードを書き直す必要があったことが挙げられたが， この問題は自動微分の技術と，(Corbella et al., 2022), (Sutton and Fearnhead, 2023) らの適応的で効率的な Poisson 点過程のシミュレーションの研究によって解決されつつある． ここでは (Andral and Kamatani, 2024) の Python パッケージ pdmp_jax とこれに基づく Julia パッケージ PDMPFlux.jl を紹介する． \n\n\n\n\n\n10/17/2024\n\n\n司馬博文\n\n\nPDMPFlux.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程に駆動される SDE のエルゴード性\n\n\nカップリング法／最適輸送距離による証明\n\n\n\nProcess\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n10/14/2024\n\n\n司馬 博文\n\n\nPureJump1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\nSchrödinger 橋は\n\n\n\n\n\n10/06/2024\n\n\n司馬 博文\n\n\nSF1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n(Vargas et al., 2023) の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である．今回は 公式の実装 を吟味する． \n\n\n\n\n\n10/06/2024\n\n\n司馬博文\n\n\nSB2-HandsOn.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\nLorenz’ 63, Lorenz’ 96 とはそれぞれ (Lorenz, 1963), (Lorenz, 1995) によって導入された大気モデルである． 前者はバタフライ効果の語源ともなった，最初に特定されたカオス力学系でもある． Navier-Stokes 方程式は流体の運動を記述する方程式である． これらはいずれもデータ同化・軌道推定技術のベンチマークとして用いられている． ここでそれぞれのモデルの数学的性質と Julia を通じたシミュレーションの方法をまとめる． \n\n\n\n\n\n10/05/2024\n\n\n司馬博文\n\n\nLorenz95.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n10/03/2024\n\n\n司馬 博文\n\n\nTrans1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n政治学における理想点解析とは，項目反応モデルを用いて裁判官や国会議員などの価値判断基準や「イデオロギー」を定量化・視覚化する方法である． ここでは既存のパッケージを用いて簡単に理想点解析を行う方法から始め， 自分で Stan コードを書いてモデルを推定する方法を紹介する． その際に最も重要な理想点モデルの性質として，識別可能性 の議論がある． これが保たれていないと，モデルの事後分布は多峰性を持ってしまい，推定をするたびに結果が異なったり，統計量の長期間平均が \\(0\\) になってしまったりしてしまう． \n\n\n\n\n\n10/02/2024\n\n\n司馬博文\n\n\nIdealPoint1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n10/01/2024\n\n\n司馬 博文\n\n\nTrans2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられてきた． しかし，古典的な ANOVA 手法である F-検定や t-検定は，データの一側面に着目した手法である．\nベイズ的な解析手法は，これを補完する多くの探索的な手法を提供してくれる． 特に，データに潜む極めて微妙な消息も捉えることが可能になることをここでは強調したい． このような微妙な消息を最初から想定することは難しく，ベイズの探索的な性格が真に可能にするデータ解析事例があると言えるかもしれない．\nそこで本稿では (van den Bergh et al., 2020) に基づいて，「社会的なロボット」に関する心理学実験のデータに対するベイズ ANOVA (Gelman, 2005), (Rouder et al., 2012) 解析のモデルケースを紹介する． 少しずつデータの構造が見えてくる過程が，読者にうまく提示できることを願う． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\nBayesANOVA.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n応募法 (voluntary sampling) や多くのウェブアンケートは，確率標本抽出に該当しない．このような場合でも母集団に関する補助情報がある限り，バイアスを軽減し推定精度を高めることができる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\nSurvey4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n標本調査において欠測はつきものである．観測単位が欠測している場合 (unit nonresponse)，call-back や follow-up などの調査を行うか，それができない場合は 荷重校正 (calibration weighting) が可能である．一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\nSurvey3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． これらの問題点を解決策としてベイズの方法を導入し，ベイズ ANOVA，ベイズ推論とモデル比較が ANOVA の発展として得られることをみる． この拡張は，ANOVA の線型モデルとしての解釈を通じてなされ，ANOVA の「同じ係数を共有するクタスタ構造の特定手法」というより広い理解へ導かれる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\nSurvey1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n人間を対象にする介入の研究では，介入の前後で変化があったかが争点となる． この変化の量を表す平均処置効果 (ATE) を，なるべくモデルを仮定せずどこまで識別できるかが多くの場合論点になる． この際の枠組みが潜在結果モデルである． したがって，操作変数法などの交絡統制法がある一方で，ATE の推定にはモデルの誤特定に強いセミパラメトリックな手法が要請される． 一般化推定方程式，一般化モーメント法，経験尤度法などの方法がある． 本稿ではこれらの推定量を同一の枠組みの下でまとめる． 推定量の分散を求めるためには漸近論のほかにブートストラップ法も用いられる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\nSurvey2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n9/22/2024\n\n\n司馬博文\n\n\nBayesTrans.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能ですが，本稿では特に R からの利用方法をまとめます．\n\n\n\n\n\n9/19/2024\n\n\n司馬博文\n\n\nStan2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n医療技術評価における生存解析では，打ち切りデータを最もよく外挿できるハザードモデルが探索される． そこでモデルの選択が重要な課題になるが，ベイズの方法だとモデル平均というアイデアが使える． これを polyhazard model で実行するためのベイズ階層モデルとモデル平均法を紹介する． キーとなる記述は Zig-Zag サンプラーである． \n\n\n\n\n\n9/12/2024\n\n\n司馬 博文\n\n\nSurvivalAnalysis1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n本稿では生存時間解析の代表的なモデルを概観する． 特に医療技術評価への応用では，打ち切りデータを最もよく外挿できるハザードモデルが探索され，ベイズ推定が有効な方法としてよく選択される． 本稿では特に表現力の高い競合リスクモデルとして polyhazard model を紹介し，ベイズ推定の困難さを議論する．\n次稿ではこのモデルを Zig-Zag サンプラーでベイズ推定する方法を紹介する． \n\n\n\n\n\n9/12/2024\n\n\n司馬博文\n\n\nSurvivalAnalysis.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n最適輸送問題は変分法の黎明期に提案された変分問題の１つであるが，その発展は確率論の成熟を待つ必要があった．現代では多くの非正則な空間上に幾何学的な量を定義する普遍的な手法として理解されてから，多くのフィールズ賞受賞者を輩出する最も活発な分野の１つとなっている．ここまでの発展の歴史を本記事では概観したい．\n\n\n\n\n\n9/03/2024\n\n\n司馬博文\n\n\nOT.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．特に OU 過程の時間反転は雑音除去過程 (Denoising Diffusion) といい，サンプリングに利用されている．デノイジングスコアマッチングでは，時間反転に Hyvärinen スコアが出現することを利用してデータ分布のスコアを推定する．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．\n\n\n\n\n\n8/26/2024\n\n\n司馬博文\n\n\nDD1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\nSkilling-Hutchinson の跡推定量は，跡の計算 \\(O(d^2)\\) を \\(O(d)\\) に落とすことができる Monte Carlo 法である．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\nTrace.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\nNF4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\nHierarchicalModel.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n行列の特異値分解とは，正方行列の直交対角化を一般の行列に拡張したものである．特異値を大きいものから \\(r\\) 個選ぶことで，Hilbert-Schmidt ノルムの意味で最適な \\(r\\)-階数近似が構成できる．このことは主成分分析に応用を持つ．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\nSVD.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n生物情報学への応用を念頭に，tSNE と Diffusion Map について詳しく扱う．\n\n\n\n\n\n8/11/2024\n\n\n司馬 博文\n\n\nDiffusionMap.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\nカーネル法とは，半正定値カーネルを用いてデータを Hilbert 空間内に埋め込むことで，非線型な変換を行う統一的な手法である．再生核 Hilbert 空間の理論により，写した先における内積は，半正定値カーネルの評価を通じて効率的に計算できるため，無限次元空間上での表現に対する tractable な手段を提供する．適切な半正定値カーネルを用いることで，データの「類似度」を定義することができる．本稿では半正定値カーネルの理論と距離学習法を扱う．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\nKernel.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散模型は拡張性にも優れており，条件付けが容易である．現状は誘導付き拡散によってこれが実現されるが，連続的な条件付き生成のために，フローマッチングなる方法も提案された．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\nNF3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上の拡散確率モデル\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n画像と動画に関してだけでなく，言語，化学分子の構造生成など，拡散模型が応用されるドメインは拡大を続けている．これは連続空間上にとどまらず，言語やグラフなどの離散空間上でも拡散模型が拡張理解され始めたことも大きい．本稿では，離散データを連続潜在空間に埋め込むことなく，直接離散空間上に拡散模型をデザインする方法をまとめる．その利点はドメインごとに過程を設計できる柔軟性にあると言える．\n\n\n\n\n\n8/09/2024\n\n\n司馬博文\n\n\nDiscreteDiffusion.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n\n\n\n\n\n8/06/2024\n\n\n司馬博文\n\n\nDDPM1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフロー学習手法は，画像と動画に関して 2024 年時点で最良の性能を誇る． これは統計的に言えば事後分布からの近似的サンプリングを実行していることに相当する． 近似的ではなく，正確に２つの分布を補間するような拡散過程を推定するためには Schrödinger 橋がある． Schrödinger 橋については 次稿 に譲るとし，本稿ではサンプラーとしての拡散モデルを復習する． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nSB0.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルは「データ過程をノイズに還元する Langevin ダイナミクスを時間反転する」という発想に基づいており，画像と動画の生成・条件付き生成タスクに関して 2024 年時点で最良の方法の１つである． この発想を正確なサンプリング法に昇華するためには，(Deming and Stephan, 1940) の Iterative Proportional Fitting アルゴリズムを用いることができる． この方法は拡散モデルによる条件付き生成の加速法として (Shi et al., 2022) によって提案された． こうして得る拡散過程は Schrödinger Bridge とも呼ばれ，エントロピー最適輸送と深い関わりを持つ． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nSB1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n(Vargas et al., 2023) の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である． 本記事では Schrödinger 橋を用いて DDS を正確にすることを考える． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\nSB3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n(Vargas et al., 2023) の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\nSB2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフローによるサンプリング法は，画像と動画に関して 2024 年時点で最良の方法の１つである．本稿ではこれを統計に応用することを考える．\n生成モデリングを２つの密度の補間問題と捉え，Schrödinger 橋を用いた正確なサンプリング法を考える．この観点から展開されるブリッジマッチング（橋照合？）はフローマッチング，確率的補間，Rectified Flow などを綜合する枠組みとなる． \n\n\n\n\n\n8/03/2024\n\n\n司馬 博文\n\n\nTransportMethods.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nEBM2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は normflows を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\nNF2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて，GAN の実装の概要を見る．\n\n\n\n\n\n8/02/2024\n\n\n司馬 博文\n\n\nGAN.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\nEBM1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて， Ho et. al. [NeurIPS 33(2020)] による DDPM (Denoising Diffusion Probabilistic Model) の実装の概要を見る．DDPM は拡散模型の最初の例の１つであり，ノイズからデータ分布まで到達するフローを定める拡散過程（雑音除去過程）を，データをノイズにする拡散過程の時間反転として学習する方法である．画像や動画だけでなく，離散空間上でタンパク質などの構造生成でも state of the art の性能を示すモデルである．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\nDDPM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬博文\n\n\nManifold.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n表現学習，非線型独立成分分析など，「生成」以外の潜在変数模型の応用法を横断してレビューする．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．\n\n\n\n\n\n7/29/2024\n\n\n司馬博文\n\n\nNCL.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\nVAE.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\nBayes3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\nサンプリング，または Monte Carlo 法は，現代の統計学と機械学習において必要不可欠な道具となっている．それは一体どうしてだろうか？初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\nSampling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\nLogistic2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\nZigZagSubsampling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．初めに政治学における理想点解析の目的と役割を概観し，続いて多次元展開法と項目反応理論の２つの観点から理想点解析を眺める． \n\n\n\n\n\n7/16/2024\n\n\n司馬博文\n\n\nIdealPoint.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\nLogistic.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\nLangevin.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\nJulia に存在する MCMC 関連のパッケージをまとめ，多くの MCMC のパッケージを支える，Turing ecosystem の基盤となる抽象的なフレームワーク MCMCChains と AbstractMCMC を概観する．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\nMCMCwithJulia.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する Metropolis-Hastings 法と MALA 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\nMALAwithJulia.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nJulia に存在する HMC 関連のパッケージの実装と，その使い方をまとめる．\n\n\n\n\n\n7/03/2024\n\n\n司馬 博文\n\n\nHMCwithJulia.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\nZigZag.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\nLevy.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\nECMC (Event-chain Monte Carlo) 法は，平衡分布の直接的な評価を一度もすることなく，平衡分布からのサンプリングを達成する新たなモンテカルロ法である．非対称性をもち，従来手法より高い効率を持つ．実際，Metropolis 法の開発以来の興味の対象であった２次元剛体円板系の液相転移のシミュレーションに，約 60 年越しに成功している．\n\n\n\n\n\n6/29/2024\n\n\n司馬 博文\n\n\nStatisticalMechanics4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\nStatisticalMechanics3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\nPoisson.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\nBayes2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\nBayes1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\ncalculus.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of Applied Probability 53(2016) 410-20] は Metropolis-Hastings アルゴリズムの計算複雑性を論じたもの \n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\nRoberts-Rosenthal2016.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\nOrnstein-Uhlenbeck 過程は唯一の非自明な定常 Gauss-Markov 過程である．また，連続時間の自己回帰模型を与える重要な拡散過程である．加えて，その遷移半群は解析的な表示を持ち，Malliavin 解析でも基本的な意味を持つ．したがって，直感的な理解を涵養しておくことは非常に見返りが大きいことだろう．そこで，YUIMA パッケージを通じてシミュレーションを行いながら，Ornstein-Uhlenbeck のパラメータの意味と，遷移半群・生成作用素の直感的な理解の醸成を目指す．\n\n\n\n\n\n6/05/2024\n\n\n司馬 博文\n\n\nOU1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫制博士課程（正確には，総合研究大学院大学統計科学コース）を紹介します．同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷のような研究環境が整っていると言えるでしょう．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\nSOKENDAI.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\nMCMC.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Statistical Science 16(2001) 351-67] は Metropolis-Hastings 法の最適スケーリングに関する結果をまとめ，実際の実装にその知見をどのように活かせば良いかを例示したレビュー論文である． \n\n\n\n\n\n5/21/2024\n\n\n司馬 博文\n\n\nRoberts-Rosenthal2001.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はこのような確率過程に対する統計推測を実行する方法を紹介します．yuima は従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．ほとんどの手法が，\\(N\\to\\infty,\\Delta_n\\to0\\) の極限で得られるデータ（高頻度データ）にも応用可能な手法となっています．\n\n\n\n\n\n5/18/2024\n\n\n司馬 博文\n\n\nYUIMA2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はそのような確率過程の汎函数の漸近展開に基づく計算方法を紹介します．確率変数の期待値を近似するのに Monte Carlo 法は普遍的な方法ですが，漸近展開が用いられる場合，その計算時間は比較にならないほど速くなります．\n\n\n\n\n\n5/18/2024\n\n\n司馬博文\n\n\nYUIMA1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．本稿では Stan 言語の基本をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\nStan1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．鋭意開発中のパッケージですが，すでに広範なクラスの確率微分方程式のシミュレーションが可能です．本稿では基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\nYUIMA.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\nR パッケージ YUIMA を用いた SDE のベイズ推定に，バックエンドとして Stan による HMC の実装を用いる方法を模索する．Stan は C++ を用いる独立した確率プログラミング言語で移植性は高いが，それ故 YUIMA からこれを用いる際に，専用のインターフェイスを考える必要が生じる．\n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\nadastan.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行う R パッケージである． 基本的な線型回帰から固定・変量効果の追加まで極めて簡単に実行できる，大変実用的なパッケージである． 本稿では，brms の基本的な使い方とその実装を紹介する． その中で混合効果モデルについてレビューをする． ランダム効果の追加は縮小推定などの自動的な正則化を可能とする美点がある一方で，係数の不偏推定やロバスト推定に拘る場合はこれを避ける判断もあり得る． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\nbrms.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\nAppliedMath.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n5/07/2024\n\n\n司馬 博文\n\n\nMeasureTheory.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\nRoberts-Tweedie1996.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの． \n\n\n\n\n\n4/22/2024\n\n\n司馬 博文\n\n\nRoberts-Rosenthal1998.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における (Parisi and Wu, 1981) の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\nDuane+1987.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\nTartero-Krauth2023.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の (Hastings, 1970) による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\nMetropolis+1953.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n粒子法とは空間や分布を多数の粒子の集合として離散化して表現・計算する技術の総称である．シミュレーションからデータ同化まで幅広い応用を持つ．この記事ではこれらの技術を「粒子」という軸でひとつの記事にまとめることを試みる． \n\n\n\n\n\n4/07/2024\n\n\n司馬 博文\n\n\nParticleMethods.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\nStatisticalMechanics2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\nStatisticalMechanics1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\nAboutSimulation.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である (Nemeth and Fearnhead, 2021)． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\nPeters-deWith2012.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\nButkovsky-Veretennikov2013.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n論文メモ\n\n\n\nReview\n\n\n\n(Dai et al., 2019) は有限混合で表される分布からのサンプリング法（Fusion 問題）に関する最初の理論解析である． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\nDai+2019.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n(Fearnhead et al., 2017) は拡散過程を離散化誤差なしにシミュレーションする手法を提案している．逐次重点サンプリング（SIS）の連続時間極限を考えることで，提案過程と重点荷重との組がPDMPとなり，効率的なシミュレーションが可能になる． \n\n\n\n\n\n4/01/2024\n\n\n司馬 博文\n\n\nFearnhead+2017.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬博文\n\n\nEBM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\n\n\n\nProcess\n\n\n\nMarkov 過程のエルゴード性の証明は，カップリングの概念を用いれば極めて明瞭に見渡せる．\n\n\n\n\n\n3/25/2024\n\n\n司馬 博文\n\n\nCoupling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\nResidualWaitingTime.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論，確率論，さらにはデータ解析の中心に据えられるべき中心概念であると言えるかもしれない．例えば，カーネル法とは確率核に沿った埋め込みである．MCMC の性質も，本質的に確率核の性質が決定する．また確率核は，確率空間の圏の射となる．このように，多くのデータ解析手法の中核に位置する数学的本体たる「確率核」への入門を目指すのが本記事である．\n\n\n\n\n\n3/24/2024\n\n\n司馬博文\n\n\nKernel.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの微細化技術をレビューする．\n\n\n\n\n\n3/23/2024\n\n\n司馬 博文\n\n\nSemiconductor2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\nBAI.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\nKernel1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n\n\n\n\n\n3/14/2024\n\n\n司馬 博文\n\n\nLLM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\nPython で最適輸送写像を計算する方法を解説する． 直接最適輸送問題を POT (Python Optimal Transport) で解く．この方法は原子の数 \\(N\\) に対して \\(O(N^3\\log N)\\) の複雑性を持つ． 一方で，エントロピー正則化項 \\(\\epsilon\\operatorname{Ent}(\\pi)\\) を導入したエントロピー最適輸送問題は Sinkhorn アルゴリズムで高速に解くことができる． これには OTT-JAX パッケージを用いる． \\(\\epsilon\\to0\\) の極限で元の最適輸送問題の解を得る． \n\n\n\n\n\n3/13/2024\n\n\n司馬 博文\n\n\nOT1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\nState vs Loomis 判決を題材に，アルゴリズムと公平性を議論する．\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理___.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n転移学習とは\n\n\n\n\n\n3/10/2024\n\n\n司馬 博文\n\n\nTheory4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\n3/08/2024\n\n\nHirofumi Shiba\n\n\nPF.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬博文\n\n\nGNN.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．\n\n\n\n\n\n3/03/2024\n\n\n司馬 博文\n\n\nTheory3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\nPAC-Bayes は現実的に有用な鋭い PAC bound を得る新たな技術である．最適化の問題に帰着する点が研究を盛り上げている．Vapnik-Chervonenkis 理論の一般化であり，推定量上の確率分布を返すようなより一般的なアルゴリズムに対しても適用できる．\n\n\n\n\n\n3/02/2024\n\n\n司馬 博文\n\n\nTheory2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\nSemiconductor.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，「刑法入門」の内容を扱う．\n\n\n\n\n\n2/21/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理7.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\nDeep2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\nDeep4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n深層学習の学習における確率最適化アルゴリズムに関して概説する．\n\n\n\n\n\n2/16/2024\n\n\n司馬 博文\n\n\nOptimization.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\nNF1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\nDiffusion.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\nNF.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n数学者のために，深層生成モデルを概観する．\n\n\n\n\n\n2/13/2024\n\n\n司馬 博文\n\n\nBAI1_Dropout.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\nVI3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nGP.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nGP2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nDeep.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\nDeep3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\nVI2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n法律家のための統計数理6.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\nRL2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\n\n\n\nAI\n\n\n\n強化学習の考え方を数学的に理解する．\n\n\n\n\n\n2/06/2024\n\n\n司馬 博文\n\n\nRL.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次稿 で，\\(K\\)-平均アルゴリズムの model-aware な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す． \n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\nVI.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\nPureJump.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/29/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理_.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n法律家のための統計数理5.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n粒子フィルターを拡散過程に対して適用することを考える．拡散過程の Euler-Maruyama 離散化に対して構成された粒子フィルターの，タイムステップを \\(0\\) にする極限 \\(\\Delta\\searrow0\\) での振る舞いを議論する．\n\n\n\n\n\n1/23/2024\n\n\n司馬 博文\n\n\nContinuousLimit.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\n\n\n\nProcess\n\n\n\nマルチンゲール問題とは何か？\n\n\n\n\n\n1/20/2024\n\n\nDraft Draft\n\n\nMartingaleProblem.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\nPGM2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n粒子フィルターは，リサンプリングを取り入れた逐次重点サンプリングと見れる．リサンプリングにより荷重の退化を防げるが本質的な問題は回避できないことが多い．本稿では，リサンプリングのアルゴリズムを複数紹介し比較する．\n\n\n\n\n\n1/14/2024\n\n\n司馬 博文\n\n\nresampling.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n法律家のための統計数理4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\nTheory.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\n\n\n\nProcess\n\n\n\n確率過程の離散化に関する漸近論的な結果を，Brown 運動を例に取り示す．\n\n\n\n\n\n1/09/2024\n\n\nDraft Draft\n\n\nDiscretization.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\n1/05/2024\n\n\nHirofumi Shiba\n\n\nMinkowskiSum.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\nRadonMeasures.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，\n\n\n\n\n\n1/02/2024\n\n\n司馬 博文\n\n\n法律家のための統計数理__.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\nBranchingProcesses.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\nVSCode での LaTeX 環境構築に関するページ．\n\n\n\n\n\n12/22/2023\n\n\n司馬博文\n\n\nLaTeXwithVSCode.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n法律家のための統計数理3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\nPGM1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nMCMC\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\nSMCSamplers.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\nParticleFilter.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n法律家のための統計数理2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\nBayesianComp.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\nMentalHealth.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n条件付き期待値の問題.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\nBoundedMeasure.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\n12/01/2023\n\n\nHirofumi Shiba\n\n\nBookRecommendation.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\nParticleFilter.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\nBeta-Gamma.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n書き起こし.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n法律家のための統計数理1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n独立性.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\n\n\n\nKernel\n\n\n\n数学者のために，SVMによるデータ解析が何をやっているのかを抽象的に説明する．\n\n\n\n\n\n11/18/2023\n\n\nDraft Draft\n\n\nSVM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n条件付き正規分布からのシミュレーション法.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\nMarkovCategory.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\nDelMoral2013.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\nFeynman-Kac モデルという物理モデルを定義し，逐次モンテカルロ法（粒子フィルター）をその Monte Carlo シミュレーション法として位置付けて解説した書籍である． 例として挙げられるトピックも物理学のものが多く，書籍のスタイルも物理学書のそれである． ここでは 1.1 節 “On the Origins of Feynman-Kac and Particle Models” の抄訳を通じて内容を概観したい． \n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\nDelMoral2004.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\nKernelMethods4Mathematicians.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\nSSM.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\nChangedMyLife.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．公式の ギャラリー も参照．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\nQuartoBasics.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia のSymbol型とExpr型，そしてExpr型からExpr型への関数であるマクロを用いたメタプログラミングについて解説する．\n\n\n\n\n\n1/23/2022\n\n\n司馬 博文\n\n\nJulia6.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nPython の import について\n\n\n\n\n\n\nPython\n\n\n\nPython の import について\n\n\n\n\n\n5/23/2021\n\n\n司馬 博文\n\n\nPython-import.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\nR0.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\nR1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\nR3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\nR2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\nR5.qmd\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．\n\n\n\n\n\n5/07/2021\n\n\n司馬 博文\n\n\nR4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）パッケージ作成とモジュール\n\n\nモジュールとパッケージ\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia でパッケージを作成する際の基礎知識をまとめる．\n\n\n\n\n\n9/10/2020\n\n\n司馬 博文\n\n\nJulia5.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は階層関係を明示的に宣言する必要のある名前的型付け言語であり，既存の型から自由な構成が可能なパラメトリック型付け言語である．\n\n\n\n\n\n9/09/2020\n\n\n司馬 博文\n\n\nJulia4.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia において，関数はメソッドの貼り合わせである．スクリプト言語のように，気軽に関数定義を行うこともできれば（単一メソッドによる関数と解す），多重ディスパッチによる実装も可能である．\n\n\n\n\n\n9/08/2020\n\n\n司馬 博文\n\n\nJulia3.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は 2012 年に公開された科学計算向きの動的型付け言語である．\n\n\n\n\n\n9/07/2020\n\n\n司馬 博文\n\n\nJulia2.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia は動的型付け言語で，型宣言 :: を省略すると全てのオブジェクトとマッチする Any 型と解釈される．一方で静的型付け言語のような豊かな型システムも持つ．これにより関数をメソッドのディスパッチにより実装するのが Julia の根幹思想である．メソッドのディスパッチについては次稿に譲り，ここでは基本的なデータ型について述べる．\n\n\n\n\n\n9/06/2020\n\n\n司馬 博文\n\n\nJulia1.qmd\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\nJulia0.qmd\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReferences\n\nAndral, C., and Kamatani, K. (2024). Automated techniques for efficient sampling of piecewise-deterministic markov processes.\n\n\nBafumi, J., Gelman, A., Park, D. K., and Kaplan, N. (2005). Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation. Political Analysis, 13(2), 171–187.\n\n\nCorbella, A., Spencer, S. E. F., and Roberts, G. O. (2022). Automatic Zig-Zag Sampling in Practice. Statistics and Computing, 32(6), 107.\n\n\nDai, H., Pollock, M., and Roberts, G. (2019). Monte Carlo Fusion. Journal of Applied Probability, 56(1), 174–191.\n\n\nDeming, W. E., and Stephan, F. F. (1940). On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. The Annals of Mathematical Statistics, 11(4), 427–444.\n\n\nFearnhead, P., Latuszynski, K., Roberts, G. O., and Sermaidis, G. (2017). Continious-time importance sampling: Monte carlo methods which avoid time-discretisation error.\n\n\nGelman, A. (2005). Analysis of variance—why it is more important than ever. The Annals of Statistics, 33(1), 1–53.\n\n\nHastings, W. K. (1970). Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika, 57(1), 97–109.\n\n\nLorenz, E. N. (1963). Deterministic nonperiodic flow. Journal of Atmospheric Sciences, 20(2), 130–141.\n\n\nLorenz, E. N. (1995). Predictability: A problem partly solved. In Seminar on predictability, 4-8 september 1995. ECMWF.\n\n\nNemeth, C., and Fearnhead, P. (2021). Stochastic gradient markov chain monte carlo. Journal of the American Statistical Association, 116(533), 433–450.\n\n\nParisi, G., and Wu, Y. (1981). PERTURBATION THEORY WITHOUT GAUGE FIXING. Scientia Sinica, 24(4), 483–.\n\n\nRouder, J. N., Morey, R. D., Speckman, P. L., and Province, J. M. (2012). Default bayes factors for ANOVA designs. Journal of Mathematical Psychology, 56(5), 356–374.\n\n\nShi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. (2022). Conditional simulation using diffusion Schrödinger bridges. In J. Cussens and K. Zhang, editors, Proceedings of the thirty-eighth conference on uncertainty in artificial intelligence,Vol. 180, pages 1792–1802. PMLR.\n\n\nSutton, M., and Fearnhead, P. (2023). Concave-Convex PDMP-based Sampling. Journal of Computational and Graphical Statistics, 32(4), 1425–1435.\n\n\nvan den Bergh, D., Doorn, J. van, Marsman, M., Draws, T., Kesteren, E.-J. van, Derks, K., … Wagenmakers, E.-J. (2020). A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP. L’Année Psychologique, 120, 73–96.\n\n\nVargas, F., Grathwohl, W. S., and Doucet, A. (2023). Denoising Diffusion Samplers. In The eleventh international conference on learning representations."
  },
  {
    "objectID": "static/Slides.html",
    "href": "static/Slides.html",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法のスケーリング解析\n\n\nSlides are here．\n\n\n\n2025-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Survey and a Gap in the Theory\n\n\nRevealjs slides are here．\n\n\n\n2025-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n区分確定的モンテカルロ\n\n\nスライドはこちら． 予稿はこちら．\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here.\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA MCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#upcomings-newests",
    "href": "static/Slides.html#upcomings-newests",
    "title": "Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法のスケーリング解析\n\n\nSlides are here．\n\n\n\n2025-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Survey and a Gap in the Theory\n\n\nRevealjs slides are here．\n\n\n\n2025-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n区分確定的モンテカルロ\n\n\nスライドはこちら． 予稿はこちら．\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\nPresented at D314, ISM, Tokyo. Get your own copy of the slides here.\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA MCMC Game-Changer\n\n\nSlides are available here.\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびきを超える MCMC\n\n\nスライドはこちら．\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学による統一的アプローチ\n\n\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n現代社会の「魔素」が見えるように\n\n\n井形研 RA 半導体読書会 駒場IIキャンパス４号館\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#history",
    "href": "static/Slides.html#history",
    "title": "Slides",
    "section": "History",
    "text": "History\n\n\n\n  \n    \n      9/22/2025.\n      \n        13:00-13:30.\n      \n      司馬博文 .\n      メトロポリスを超えた枠組みで我々はどこまで行けるか？\n      : イベント連鎖モンテカルロ法のスケーリング解析.\n      \n        \n          機械学習若手の会 (YAML) 2025.\n        \n      \n      \n        ハートピア熱海.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      7/15/2025.\n      \n        17:30-18:00.\n      \n      Hirofumi Shiba.\n      Understanding Discrete Denoising Diffusion Models\n      : A Survey and a Gap in the Theory.\n      \n        \n          \n            Workshop on Opportunity Week 2025, Stochastische Geometrie und räumliche Statistik.\n          \n        \n      \n      \n        Universität Ulm.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      2/06/2025.\n      \n        16:50-17:10.\n      \n      司馬博文 .\n      動き出す次世代サンプラー\n      : 区分確定的モンテカルロ.\n      \n        \n          学生研究発表会（統数研）.\n        \n      \n      \n        統数研２階大会議室.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n        \n           PDF\n        \n      \n    \n  \n    \n      9/10/2024.\n      \n      Hirofumi Shiba | 司馬博文.\n      Zig-Zag Sampler\n      : A MCMC Game-Changer.\n      \n        \n          Talk.\n        \n      \n      \n        Seoul National University, Gwanak (관악) campus, South Korea.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      7/25/2024.\n      \n      司馬博文 .\n      Zig-Zag サンプラー\n      : 物理のくびきを超える MCMC.\n      \n        \n          ベイズ会.\n        \n      \n      \n        本郷キャンパス小島ホール第二セミナー室.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#probability",
    "href": "static/PartialCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\nProbability\n\n\nStatistics\n\n\n\n\n2024-12-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#process",
    "href": "static/PartialCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#functional-analysis",
    "href": "static/PartialCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\nFunctional Analysis\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#geometry",
    "href": "static/PartialCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mathcalpx",
    "href": "static/PartialCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#nature",
    "href": "static/PartialCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#mcmc",
    "href": "static/PartialCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#foundation",
    "href": "static/PartialCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#information",
    "href": "static/PartialCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#statistics",
    "href": "static/PartialCategories.html#statistics",
    "title": "Categories",
    "section": "2.5 Statistics",
    "text": "2.5 Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\nProbability\n\n\nStatistics\n\n\n\n\n2024-12-23\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#computation-2",
    "href": "static/PartialCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#sampling",
    "href": "static/PartialCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#python",
    "href": "static/PartialCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#julia",
    "href": "static/PartialCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nPDMP\n\n\n\n\n2024-12-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#yuima",
    "href": "static/PartialCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#stan",
    "href": "static/PartialCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#r",
    "href": "static/PartialCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#bayesian",
    "href": "static/PartialCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#particles",
    "href": "static/PartialCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#kernels",
    "href": "static/PartialCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#ai",
    "href": "static/PartialCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#deep-learning",
    "href": "static/PartialCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#posters",
    "href": "static/PartialCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ変数選択の計算的解決\n\n\nPDMP による非可逆ジャンプの達成\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-02-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#reviews",
    "href": "static/PartialCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#surveys",
    "href": "static/PartialCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#slides",
    "href": "static/PartialCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総研大５年一貫博士課程・中間評価\n\n\n\nSlide\n\n\n\n\n2025-01-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズとは何か\n\n\n数学による統一的アプローチ\n\n\n\nSlide\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nSlide\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#papers",
    "href": "static/PartialCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/PartialCategories.html#opinion",
    "href": "static/PartialCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#life",
    "href": "static/PartialCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n2024-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#lifestyle",
    "href": "static/PartialCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#法律家のための統計数理",
    "href": "static/PartialCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/PartialCategories.html#俺のための-julia-入門",
    "href": "static/PartialCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/NotationList.html#sec-set",
    "href": "static/NotationList.html#sec-set",
    "title": "Notation & Subject Index of This Website",
    "section": "1 Set",
    "text": "1 Set\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nHere, we define all mathematical concepts as sets under ZFC axioms.1 The symbol \\(:=\\) is a shorthand for the statement “the right-hand side defines the left-hand side, and the equality holds as a result”.2\n\n1.1 Sets\n\n空集合 を \\[\n\\emptyset:=\\{x\\mid x\\ne x\\}\n\\] で表す．3\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．4\n\\(A,B\\subset X\\) の 差 を \\[\nA\\setminus B:=\\left\\{a\\in A\\mid a\\notin B\\right\\}\n\\] で表す．\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cup B\\) と同じ数学的対象であるが，同時に \\(A\\cap B\\) という事実も主張するものとする．5\n対称差 を \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\] で表す．6\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．7 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．8\n例えば，\\(X\\in\\mathcal{L}(\\Omega)\\) を実確率変数，\\(A\\in\\mathcal{B}(\\mathbb{R})\\) を Borel 集合とすると， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] という略記を用いる．\n\n\n\n1.2 構成\n\n自然数 を \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\},\n\\] によって帰納的に定義する．9\n自然数の集合を表すため，次の記法を用意する：10 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，11 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す： \\[\n\\mathbb{R}_+=[0,\\infty),\\quad\\mathbb{R}^+=(0,\\infty).\n\\]\n部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．12\n実数 \\(x\\in\\mathbb{R}\\) に対して，その整数部分を \\[\n\\lfloor x\\rfloor:=\\max\\{n\\in\\mathbb{Z}\\mid n\\le x\\}\n\\] と表す．13\n次の演算規則を約束する：14 \\[\n\\prod_\\emptyset=1,\\quad\\sum_{\\emptyset}=0.\n\\]\n\\(0!=1\\) とする．\n\\(n\\)-組 を次のように帰納的に定める：15 \\[\n(x_1,x_2):=\\{\\{x_1\\},\\{x_1,x_2\\}\\},\n\\] \\[\n(x_1,\\cdots,x_n):=(x_1,(x_2,\\cdots,x_n)).\n\\]\n自然数の組を表すため，次の記法を用意する：16 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組を \\[\nX_{1:N}:=(X_1,\\cdots,X_N)\n\\] と表す．17\n\n\n\n1.3 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n写像 \\(f\\) の 値域 を \\[\\mathrm{Im}\\,f:=f(X)\\] で表す．\n\\(A\\subset X\\) への 制限 を \\(f|_A:A\\to Y\\) と表す．18\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．19\n\\(f_*\\) は部分集合 \\(A\\subset X\\) を像 \\(f(A)\\subset Y\\) に対応させる写像 \\[\nf_*:P(X)\\to P(Y)\\] と定義する．\n同様に写像 \\(f^*:P(Y)\\to P(X)\\) を定める： \\[\nf^*(B)=f^{-1}(B)\\quad(B\\subset Y).\n\\]\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．20\n部分集合 \\(A\\subset X\\) の 指示関数 といった場合は \\[\n\\sigma_A:=0\\cdot1_A+\\infty\\cdot1_{A^\\complement}\n\\] を表す．21\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．22\n写像 \\(f:X\\to Y\\) のうち，有限個の元を除いて \\(f(x)=0\\) を満たすものがなす全体を \\[\nY^{(X)}:=\\left\\{f\\in Y^X\\mid f=0\\;\\;\\text{f.e.}\\right\\}\n\\] と表す．23\n\\(P(X)\\) を \\(2^X\\) と同一視する．特に，\\(X\\) の有限部分集合の全体を \\[\n2^{(X)}=\\left\\{A\\in P(X)\\:\\middle|\\: A\\overset{\\text{finite}}{\\subset}X\\right\\}\n\\] と表す．24\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．25\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\[\n\\mathrm{pr}_i:\\prod_{i\\in I}X_i\\twoheadrightarrow X_i\n\\] で表す．26\n\\(x\\in X\\) での 評価写像 を \\[\n\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\n\\] で表す．27\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．\nしかしこの写像の値域も 族 と呼び，この場合は \\[\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\] と表す．28\n特に \\(I=\\mathbb{N}\\) のときは 列 ともいう．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．29\n\n\n\n1.4 圏\n\n集合の圏 を \\(\\mathrm{Set}\\) で表す．\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X)\n\\] を表す．30\n確率空間と確率核の圏を \\(\\mathrm{Stoch}\\) で表す．31\n圏 \\(C\\) の対象 \\(X,Y\\in C\\) の間の 射 の全体を \\(\\mathrm{Hom}_C(X,Y)\\) で表す．32\n特に \\[\nY^X=\\mathrm{Map}(X,Y)=\\mathrm{Hom}_\\mathrm{Set}(X,Y).\n\\]\n圏 \\(C\\) の対象 \\(X\\in C\\) の自己射の全体を \\[\n\\mathrm{End}_C(X):=\\mathrm{Hom}_C(X,X)\n\\] で表す．\nそのうち可逆なもののなす部分集合を \\(\\mathrm{Aut}_C(X)\\) で表す．集合 \\([n]\\) の 置換群 は \\(\\mathrm{Aut}_\\mathrm{Set}([n])\\) と表せる．\n\n\n\n1.5 関数\n\n\\(R\\) を環とする．\\(f_1,f_2\\in R^X\\) に対して， \\[\n(f_1+f_2)(x):=f_1(x)+f_2(x),\n\\] \\[\n(f_1f_2)(x):=f_1(x)f_2(x)\n\\] で定める演算により \\(R^X\\) も環とみなし，定値関数 \\(f\\equiv a\\in R\\) を \\(R\\) の元と同一視する．33\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を \\[\na\\lor b:=\\sup\\{a,b\\},\n\\] \\[\na\\land b:=\\inf\\{a,b\\},\n\\] で表す．34\n\\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right)\n\\] と表す．35\n\\(0\\) を持つ束においては次の略記を使う：36 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序集合 \\(Y\\) に値を取る関数 \\(f,g\\in\\mathrm{Map}(X,Y)\\) について，\\(f\\le g\\) とは \\[\n\\forall_{x\\in X}\\;f(x)\\le g(x)\n\\] の略記とする．\n同じ条件を，一階の量化記号 \\(\\forall\\) を省略して \\[\nf(x)\\le g(x)\\quad(x\\in X)\n\\] または \\(f\\le g\\) とも略記する．\n\\(Y\\) が束であるとき，この順序により関数の空間 \\(\\mathrm{Map}(X,Y)\\) は束となり，演算 \\(\\land,\\lor\\) が定まる．\n関数の列 \\(\\{f_n\\}\\subset Y^X\\) について，\\(f_n\\nearrow f\\) とは，収束 \\(f_n\\to f\\) だけでなく，\\(\\{f_n\\}\\) が単調増加であることも含意する．37\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\[\nO(g(x))\\;(x\\to x_0)\\] とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．38\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の意味でも使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0\n\\] を満たすこととする．"
  },
  {
    "objectID": "static/NotationList.html#sec-space",
    "href": "static/NotationList.html#sec-space",
    "title": "Notation & Subject Index of This Website",
    "section": "2 Space",
    "text": "2 Space\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．39\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．40\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．\n閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}\n\\] で表す．\n\n\n\n2.2 線型空間\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．41\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．42 \\(S_n(\\mathbb{R})_+\\) で半正定値なものの全体を表す．43\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}\n\\] とも表す．44\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，45 共役転置を \\(A^*\\) で表す．46 \\(\\mathbb{F}=\\mathbb{C}\\) の場合は \\(A^\\top=A^*\\)．\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．47\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について， \\[\n\\begin{align*}\n  A&+B\\\\\n  &\\quad:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n  \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\},\n\\end{align*}\n\\] と表す．48\n集合 \\(A\\subset X\\) の 凸包 を \\(\\operatorname{Conv}(A)\\) で表す．49\n集合 \\(A\\subset X\\) が生成する部分空間を \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x\n\\] で表す．50\n内積を \\((-|-)\\) で表す．51\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を52 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\operatorname{Tr}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}\n\\] で表す．53\n\n\n\n2.3 Banach 空間\n\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．54 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n特に \\(J\\) が有限であるとき， \\[\n  \\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n  \\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．55\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．56\n距離空間 \\((T,d)\\) の 開球 を \\[\n\\begin{align*}\n  U_\\epsilon(t)&:=U(t;\\epsilon)\\\\\n  &:=\\left\\{s\\in T\\mid d(s,t)&lt;\\epsilon\\right\\}\n\\end{align*}\n\\] で表す．57\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．58\n単位閉球を \\(B:=B(0;1)\\) で表す．\n\\(\\mathbb{R}^n\\) のものである場合は特に \\(B^n\\) とも表す．59\n\\(\\mathbb{R}^n\\) の標準基底を \\[\ne_i=(0,\\cdots,0,1,0,\\cdots,0)\n\\] と表す．60\nBanach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．61\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}\n\\] で表す．62\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) として理解できる数学的対象の別記法と捉えられるように設計する．63\n\n\n2.4 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．64\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．65\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．66\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．67\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．68\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．69\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．70\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．71\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．72\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．73 \\(\\gamma_n:=\\operatorname{N}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n2.5 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．74 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．75\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．76\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．77\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．78\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．79\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．80\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．81\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．82\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．83"
  },
  {
    "objectID": "static/NotationList.html#sec-kernel",
    "href": "static/NotationList.html#sec-kernel",
    "title": "Notation & Subject Index of This Website",
    "section": "3 Kernel",
    "text": "3 Kernel\nNow that spaces are defined, we need their morphisms next.\nIn this section, \\((E,\\mathcal{E})\\) is always a measurable space.84\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 85\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 86\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．87 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．88\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．89\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．90\n\n\n\n3.2 確率分布\n\nThe set of all probability measures on \\((E,\\mathcal{E})\\) is denoted by \\(\\mathcal{P}(E,\\mathcal{E})\\).\nWhen \\(E\\) is a topological space\n\nan abbreviation \\(\\mathcal{P}(E):=\\mathcal{P}(E,\\mathcal{B}(E))\\) is used for the set of Borel probability measures.91\nAmong them, the set of Radon probability measures on \\((E,\\mathcal{B}(E))\\) is denoted by92 \\[\nP(E)\\subset\\mathcal{P}(E).\n\\]\n\nWhen \\((E,d)\\) is a metric space, the \\(p\\)-th Wasserstein space is denoted by \\[\n\\mathcal{P}_p(E):=\\mathcal{P}_p(E,d):=\\left\\{\\mu\\in\\mathcal{P}(E)\\,\\middle|\\,\\exists_{x_0\\in E}\\;\\int_Ed(x_0,x)^p\\mu(dx)&lt;\\infty\\right\\}.\n\\]\nThe set of couplings between probability measures \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) is denoted by93 \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\]\n\\(\\operatorname{N}_d(\\mu,\\Sigma)\\in P(\\mathbb{R}^d)\\) is the \\(d\\)-dimensional normal distribution．94 Its density is represented by \\[\n  \\phi_d(x;\\mu,\\Sigma):=\\frac{d \\operatorname{N}_d(\\mu,\\Sigma)}{d \\ell_d}(x).\n  \\]\n\\(\\mathrm{U}(A)\\in P(\\mathbb{R}^d)\\) is the uniform distribution on the set \\(A\\subset\\mathbb{R}^d\\)．\nThe Delta measure on the point \\(x\\in E\\) is denoted by \\(\\delta_x\\)．95\nThe distribution function of a random variable \\(X\\sim\\nu\\in P(\\mathbb{R}^d)\\) is denoted by \\[\nF_X(a):=F_\\nu(a):=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d],\\quad(a=a_{1:d}\\in\\mathbb{R}^d).\n\\]\nWhen \\(d=1\\), the generalized inverse of the distribution function is denoted by96 \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\},\\quad(u\\in(0,1)).\n\\]\n\n\n\n3.3 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：97\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．98 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．99\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 100\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．101\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．102\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．103\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．104\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．105\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.4 関数の空間\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．106\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．107\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．108\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．109 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．110\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．111\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．112\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．113\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．114\n\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．115\n\n\n3.5 作用素\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とし，\\(X,Y\\) をノルム空間とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．116\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．117\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．118\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．119\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．120\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．121\n\n作用素 \\(T:X\\to Y\\) と言ったとき，線型写像 \\(T:X\\to Y\\) を指すこととする．122\n\\(X\\) 内の作用素 \\(T:X\\supset\\mathcal{D}(T)\\to Y\\) と言ったとき，ある \\(X\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to Y\\) を指すこととする．123\n有界作用素の全体を \\(B(X,Y)\\) で表す．124 \\(B(X):=B(X,X)\\) とする．\n連続作用素の全体を \\(L(X,Y)\\) で表す．125"
  },
  {
    "objectID": "static/NotationList.html#sec-analysis",
    "href": "static/NotationList.html#sec-analysis",
    "title": "Notation & Subject Index of This Website",
    "section": "4 Operator",
    "text": "4 Operator\nKernels, now flourishes in machine learning and computational statistics, originate from the theory of integral operators.\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．126\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．127\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．128\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．129 \\(m=1\\) のとき， \\[\n\\operatorname{grad}u:=\\nabla u:=(Du)^\\top=\\begin{pmatrix}\\frac{\\partial u}{\\partial x_1}\\\\\\vdots\\\\\\frac{\\partial u}{\\partial x_n}\\end{pmatrix}\n\\] とも表す．\n発散 を \\[\n\\operatorname{div}u:=\\nabla\\cdot u:=\\operatorname{Tr}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．130\n\\(u\\) が正方行列 \\(M_n(\\mathbb{R})\\)-値であった場合，行成分毎の適用 \\[\n  \\operatorname{div}u:=\\begin{pmatrix}\\operatorname{div}(u_{1-})\\\\\\vdots\\\\\\operatorname{div}(u_{n-})\\end{pmatrix}\n  \\] と解する．\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n  \\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n  \\] と同一視する．131\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n  \\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\operatorname{Tr}(D^2u)\n  \\] で定める．\n\n\n\n4.2 Fourier 変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．132\n符号関数 を \\[\n\\operatorname{sgn}(x):=2H(x)-1\n\\] で定める．133\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 Generalized Functions\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．134 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．135\n\n\n\n4.4 Stochastic Analysis\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．136\n\n\n4.5 Operators\n\nThe domain of the operator \\(T\\) is denoted by \\(\\mathcal{D}(T)\\).\nThe graph of the operator \\(T:X\\to Y\\) is denoted by \\[\n\\mathcal{G}(T):=\\left\\{x\\oplus Tx\\in X\\oplus Y\\mid x\\in\\mathcal{D}(T)\\right\\}.\n\\]"
  },
  {
    "objectID": "static/NotationList.html#sec-process",
    "href": "static/NotationList.html#sec-process",
    "title": "Notation & Subject Index of This Website",
    "section": "5 Process",
    "text": "5 Process\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率変数の収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．137\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．138\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n\n\n\n5.2 確率基底\n（確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．139\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\(E^T\\) に定める写像 \\[\nX_-:\\Omega\\to E^T\n\\] を 転置 と呼ぶ．140\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいう．141\n\nこのような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．\n特に \\(T=\\mathbb{R}_+\\) の場合は \\(D(\\mathcal{X}):=D(\\mathbb{R}_+;\\mathcal{X})\\) と略記する．\n\n\\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．142 ただし，\\(x(0-)=x(0)\\) とする．143\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の 情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系 \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．144\n加えて， \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\] と表す．145\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) とその上の情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) からなる 4-組 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) を 確率基底 という．\n確率基底が 完備 であるとは， \\[\n\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\n\\] を満たすことをいう．146\n\n\n\n5.3 可測性\n\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}}\\) がフィルトレーション \\((\\mathcal{F}_t)\\) に 適合的 であるとは， \\[\nX_t\\in\\mathcal{F}_t\\quad(t\\in\\mathbb{R})\n\\] を満たすことをいう．\n第一種不連続な見本道を持つ適合的な過程の全体を \\(\\mathbb{D}\\) で表す．一方で，càglàd な見本道を持つ適合的な過程の全体を \\(\\mathbb{L}\\) で表す．147\n\n\n\n5.4 停止時148\n\n確率基底 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) 上の 停止時 とは，同じ確率空間 \\(\\Omega\\) 上の可測関数 \\(T:\\Omega\\to[0,\\infty]\\) であって， \\[\n\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t,\\qquad t\\in\\mathbb{R}_+,\n\\] も満たすものをいう．149\n停止時 \\(T\\) までの 情報 とは， \\[\n\\mathcal{F}_T:=\\left\\{A\\in\\mathcal{F}_\\infty\\mid\\forall_{t\\in\\mathbb{R}_+}\\;A\\cap\\left\\{T\\le t\\right\\}\\in\\mathcal{F}_t\\right\\}\n\\] で定まる \\(\\sigma\\)-代数をいう．"
  },
  {
    "objectID": "static/NotationList.html#footnotes",
    "href": "static/NotationList.html#footnotes",
    "title": "Notation & Subject Index of This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe category of sets \\(\\mathrm{Set}\\) is very suitable for the foundation of mathematics nLab.↩︎\n(Del Moral and Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) agree with this definition. \\(\\equiv,\\overset{\\text{def}}{=}\\) are also commonly used. (Crisan and Doucet, 2002), (Smith, 2010) use \\(\\overset{\\triangle}{=}\\). Here, we avoid these symmetric symbols. Some people also use \\(=_{\\text{df}}\\) (Quine and Szczotka, 1994).↩︎\n(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 2) の定め方に一致する．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie and Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie and Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod and Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod and Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod and Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Jacod and Protter, 2012) では \\([x]\\) で表される．↩︎\n(Del Moral and Penev, 2014, p. xlviii), (Del Moral, 2004, p. 10) の定義に一致する．これは \\(\\prod_{i\\in\\emptyset}X_i\\) が一点集合で，\\(\\coprod_{i\\in I}X_i\\) が空集合である消息の一般化と見れる．なお，集合 \\(X\\) の部分集合の空な族 \\((X_i)_{i\\in\\emptyset}\\) は存在し，それは \\(\\mathrm{Map}(\\emptyset,X_i)\\) のただ一つの元である．↩︎\n(Kuratowski, 1921) による定義である．(Shoenfield, 1967, p. 243), (新井敏康, 2011, p. 118), (斎藤毅, 2009, pp. 定義1.3.1 p.15) の定め方に一致する．また \\(n\\)-組を英語では tuple と呼ぶが，全く同じ対象をリスト (list) とも呼ぶ nLab Concept with an Attitude．↩︎\n(Chopin and Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(新井敏康, 2011, p. 119) などでは，\\(f\\restriction_A\\) とも表す．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\n(Brezis, 2011, p. 14) の使い分けに倣った．支持関数は英語で indicator function という (Beck, 2017, p. 14) 例2.1，(寒野善博，土谷隆, 2014, p. 110)，(Ekeland and Témam, 1999, p. 8)．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné and Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(斎藤毅, 2007, pp. 例1.4.7 p.20) に従った．また f.e. とは with a finite number of exceptions の略で，「有限個の例外を除いて成り立つ」という意味である (伊藤清, 1991, p. 124)．↩︎\n(斎藤毅, 2009, p. 179) では \\(F(X)\\) と表記している．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(Billingsley, 1999), (Ethier and Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n(Fritz, 2020, p. 19), (Perrone, 2024) など．Markov圏の稿 も参照↩︎\nnLab の記法に一致する．(斎藤毅, 2020, p. 7) では \\(\\mathrm{Mor}_C(X,Y)\\) と表す．↩︎\n(Del Moral, 2004, p. 7) も参照．↩︎\n(Del Moral and Penev, 2014, p. xlvii), (Bogachev, 2007, p. 277) 4.1.(i) に一致する．(Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie and Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod and Protter, 2012), (Del Moral and Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(Bogachev and Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Rogers and Williams, 2000, p. 110) V.1.3 では \\(S_n^+\\) の記法が用いられている．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．特に，古典力学や有限要素法の文献においては，二項積 の間の演算である二重点乗積を \\(:\\) で表したことから，この記法が用いられる．二項積については (Abraham et al., 1988, p. 341) も参照．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi), (Bakry et al., 2014, p. xv) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné and Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné and Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart and Wellner, 2023, p. 6) では 外確率 という．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Bogachev and Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(Bogachev, 2007, p. 23) に倣った．(Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie and Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (Bogachev, 2007, p. 189) 参照．↩︎\n(Giné and Nickl, 2021, p. 16), (Bogachev and Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 8) に倣った．(Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n(Nualart and Nualart, 2018) に倣った．(Giné and Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．(Giné and Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral and Penev, 2014), (Dellacherie and Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné and Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart and Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain and Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral and Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné and Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral and Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral and Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) denote an arbitrary measurable space by \\((E,\\mathcal{E})\\).↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay and Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford and Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford and Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné and Nickl, 2021, p. 2), (Villani, 2009) に一致する．(Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral and Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie and Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan and Doucet, 2002) に一致する．(Dellacherie and Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Jacod and Shiryaev, 2003, p. 347), (Crisan and Doucet, 2002), (Ethier and Kurtz, 1986, p. 96), (Bogachev, 2007, p. 228) agree with this definition. (Kechris, 1995, p. 109), (Villani, 2009) use italic \\(P(E)\\) for this set.↩︎\nThis notation is derived from (Pedersen, 1989, p. 72). (Section 7.2 Bogachev, 2007, p. 76) denotes \\(\\mathcal{P}_r(X)\\) for this set. Radon measure is a Borel measure satisfying the following internal regularity, or tightness, condition: \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] (Bogachev, 2007, pp. 68–69) Definition 7.1.1, 7.1.4.↩︎\nNote that a coupling is a Radon measure, following (Section 8.10(viii) Bogachev, 2007, p. 235) and (Remark 6.5 Villani, 2009, p. 95). (Kulik, 2018) uses \\(\\mathcal{C}\\) for this set. (Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) use \\(\\Pi(\\mu,\\nu)\\), (Ethier and Kurtz, 1986, p. 96) uses \\(\\mathcal{M}(\\mu,\\nu)\\), (Dudley, 2002, p. 420) 11.8节 uses \\(M(\\mu,\\nu)\\), and (Figalli and Glaudo, 2023) uses \\(\\Gamma(\\mu,\\nu)\\).↩︎\n(竹村彰道, 2020) agree with this notation.↩︎\nAlso called Dirac measure. (Jacod and Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) use \\(\\epsilon_x\\)．(Protter, 2005, p. 299) uses \\(\\delta_x\\) for the Dirac function.↩︎\n(Gerber et al., 2019) agrees with this notation. (竹村彰道, 2020, p. 16) calls it 分位点関数 (quantile function)，(森口繁一, 1995) calls it 確率表現関数 (probability representation function). (Dudley, 2002, p. 283) denotes it \\(X_F\\)．↩︎\n(Revuz and Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho and Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod and Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan and Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal and van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin and Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie and Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas and Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal and van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf and Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．(Heng et al., 2024) では \\(T=\\mu\\) という定値核の場合も同様の記法 \\(\\mu\\otimes S\\) を定義している．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) や (Protter, 2005, p. 52) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie and Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (Bogachev, 2007, p. 262) 4.4節．(Dunford and Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral and Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg and Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier and Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné and Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie and Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\nこのような使い分けは (Nummelin, 1984, p. 1) に一致する．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart and Nualart, 2018, p. 31) に一致する．↩︎\n(Nualart and Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod and Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod and Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod and Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie and Meyer, 1978, p. 114), (Revuz and Yor, 1999, p. 42) に倣った．↩︎\n右連続性と完備性を併せて，フィルトレーション付き確率空間 \\((\\Omega,\\mathcal{F},(\\mathcal{F}_t),\\operatorname{P})\\) の 通常の条件 ともいう．(Protter, 2005, p. 3) など参照．↩︎\n(Protter, 2005, p. 56)．↩︎\n(Dellacherie and Meyer, 1978) 49 115-IV では 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n(Jacod and Shiryaev, 2003, p. 4) 1.11，(Protter, 2005, p. 3) に従った．↩︎"
  },
  {
    "objectID": "static/Japanese.html",
    "href": "static/Japanese.html",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学５年一貫博士課程（統計科学コース）３年．\n統計や機械学習の手法を実現するアルゴリズム，特にモンテカルロ法や確率的最適化法のダイナミクスに興味を持って研究しています．\nPDMPFlux.jl や YUIMA などのパッケージ開発，ヘルスケア・ものづくり企業へのデータ解析コンサルティングも行っています．"
  },
  {
    "objectID": "static/Japanese.html#研究",
    "href": "static/Japanese.html#研究",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "研究",
    "text": "研究\n博士課程では，統計数理研究所 鎌谷研吾 先生の下，モンテカルロ法や拡散モデルなどのサンプリング手法の理論解析に取り組んでいます．\n統計と機械学習で用いられるアルゴリズムには幅広く興味があり，いずれも統計力学などの自然現象と深く関連していることが特に面白いと感じています．\nこれらが確率過程の収束や確率測度の空間上の幾何などの数理的枠組みで統一的に理解できた際には，学習や情報処理など広く統計的な現象に対する人類の理解を大きく深めてくれると信じています．\n研究成果の実装も大好きで，連続時間 MCMC を用いた事後分布サンプリングのための Julia パッケージ PDMPFlux.jl や，確率過程の統計推測のための R パッケージ YUIMA の開発にも取り組んでいます．"
  },
  {
    "objectID": "static/Japanese.html#キーワード",
    "href": "static/Japanese.html#キーワード",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "キーワード",
    "text": "キーワード\n\nモンテカルロ計算 MCMC，SMC，拡散モデルなどのベイズ計算法．\n最適化 ニューラルネットワーク・カーネル法などの計算法．\nモデリング 政治学・疫学・惑星地球科学などへの応用．"
  },
  {
    "objectID": "static/Japanese.html#経歴",
    "href": "static/Japanese.html#経歴",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "経歴",
    "text": "経歴\n\n 特別研究員 (JST BOOST)\n2025.4 – 2028.3 総合研究大学院大学 SOKENDAI\n\n\n データサイエンティスト\n2024.9 – 2025.9 株式会社プリメディカ"
  },
  {
    "objectID": "static/Japanese.html#学位",
    "href": "static/Japanese.html#学位",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "学位",
    "text": "学位\n\n 2028.3 博士（統計科学）（見込み）\n総合研究大学院大学．主指導教員：鎌谷研吾．\n\n\n 2023.3 学士（理学）\n東京大学理学部数学科．指導教員：吉田朋広．"
  },
  {
    "objectID": "static/CV/cv.html#profile-and-skills",
    "href": "static/CV/cv.html#profile-and-skills",
    "title": "Hirofumi Shiba",
    "section": "Profile and Skills",
    "text": "Profile and Skills\nHirofumi is currently a Ph.D. candidate at the Institute of Statistical Mathematics, working under the supervision of Prof. Kengo Kamatani and Keisuke Yano."
  },
  {
    "objectID": "static/CV/cv.html#research-interests",
    "href": "static/CV/cv.html#research-interests",
    "title": "Hirofumi Shiba",
    "section": "Research Interests",
    "text": "Research Interests\nComputational Statistics & Machine Learning. Especially, Markov Chain Monte Carlo, Piecewise Deterministic Markov Process, Denoising Diffusion Models, Stochastic Gradient Descent, Evolution Strategies, and Bayesian Learning."
  },
  {
    "objectID": "static/CV/cv.html#work-experience",
    "href": "static/CV/cv.html#work-experience",
    "title": "Hirofumi Shiba",
    "section": "Work Experience",
    "text": "Work Experience\n\nResearch Fellow. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2025.4 – today\nSupported by JST BOOST, Japan Grant Number JPMJBS2412.\nData Scientist. PreMedica, Inc., Tokyo, Japan. 2024.9 – today\nProvided Bayesian data analysis solutions to healthcare applications.\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – 2025.3\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference."
  },
  {
    "objectID": "static/CV/cv.html#education",
    "href": "static/CV/cv.html#education",
    "title": "Hirofumi Shiba",
    "section": "Education",
    "text": "Education\n\nPh.D. in Statistical Science. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2023.4 – 2028.3\nSuperivsor: Kengo Kamatani\nB.A. in Mathematics. The University of Tokyo, Japan. 2019.4 – 2023.3\nSupervisor: Nakahiro Yoshida"
  },
  {
    "objectID": "static/CV/cv.html#research-stay",
    "href": "static/CV/cv.html#research-stay",
    "title": "Hirofumi Shiba",
    "section": "Research Stay",
    "text": "Research Stay\n\nDepartment of Statistical Science, University College London，United Kingdom．2024.11.4 – 2024.12.2\nSupervisor: Alexandros Beskos\nDepartment of Statistics & Data Science, National University of Singapore. 2025.6.1 – 2024.6.30\nSupervisor: Alexandre Thiéry\nInstitute of Stochastics, Ulm University，Germany．2025.7.13 – 2025.7.26\nSupervisor: Evgeny Spodarev"
  },
  {
    "objectID": "static/Bio.html",
    "href": "static/Bio.html",
    "title": "生い立ち",
    "section": "",
    "text": "1999 年に横浜市磯子区に生まれる． \n物心がついてきた頃，当時は「高校生クイズ」なる番組で開成高校が大活躍しており，その世界に入りたいと思うようになる．高校生たちが紙とペンだけから，たった5分で「宇宙の年齢を推定せよ」という問題にも「このヒエログリフ碑文を読解せよ」という問題にも糸口を見つけて正解に辿りつく姿には大変憧れた． \n小学３年生で東京に引っ越し，無事その後開成中学に入学．憧れていたクイズに打ち込み，自身も高校生クイズに出場することが叶うが，アメリカ進出手前で敗退．番組も学問的な内容から方針を変えてしまっており，「憧れの舞台」というものがすでに失くなってしまっていたことも少し残念であった．\nしかし高校の頃はよく本を読むことは心がけた．期末試験前で授業がなくなるタイミングなどは絶賛の好機であった．統計を通じて「履歴書が厚手で紙質が良いほど合格が出やすい」などという人間行動の不思議が暴けることに驚き，統計科学の研究者に憧れたものだった．\nそこで部活引退後は学問を志すようになる．数学で自然の音が聴こえるようになりたいと思い，浪人期は物理に熱中した．原島力学 や ファインマン力学 が面白く，自分も物理学がやりたいと思うようになるが，肝心の「自然現象にはあまり興味がない」という難点があった．物理学以外の応用数学分野も知らなかったので，ひとまず大学入学後は理学部物理学科に進もうと思っていた．\n\n入学後，計算機が世間の潮流だと知り，ここに「新時代の応用数理」が眠っているのではないかと思うようになる．ということで計算機関連の数学がやりたいと思うようになるが，東京大学の 前期教養学部での数学の授業 が始まると「これだ」と直覚する．自分はずっと数学の応用が好きだが，最初の「数学の応用」は数学の内部で起こることに気づき，学部は絶対数学科だと進学を決意．\n５月の数学科のガイダンスで，「応用に興味ある者にとって，数学科と他の学科との最大の違いは，ここでなら『数学に軸足を置いた応用』ができる」との言葉が背中を押したことも大きい．応用先が曖昧であった自分にとって，「軸足」の語がしっくり来た．入学１ヶ月の時点でもう堅く決意していたため，教養の統計学の授業も一切取らず，数学科目に集中した．数学原論 の出版に伴い開講された 圏と層 に巡り合い，より一層のめり込んでいく．\n\n\n  \n    数学科に進むかどうかは点数の高低ではなく来年の夏に死がどれぐらい怖いかによって決定される— Hirofumi Shiba (@ano2math5)\n    May 11, 2019\n  \n\n\n無事数学科に進学したものの，家庭環境の急変から経済的にも学業的にも休めない状況が続き，大学３年の５月にパニック障害を患う．追試をくらった代数と幾何が半ばトラウマになり，自分は確率統計で食べていくのだと思うようになる．しかし治療に専念していた間でも，Pedersen の関数解析の本 だけは読めた．ずっと 基礎論 や 計算機科学 が好きであったが，解析が面白いと思ったのはこれが初めてだった．\n４年の講究では Nualart の Malliavin 解析の本 を読んだ．３年次は人生を立て直すので精一杯で授業が取れず，確率論は全て独学であったから確率過程の概念に大変難儀したが，確率解析と Malliavin 解析は関数解析の知識が非常に役に立ち，大変に面白かった．\nしかし自分はやはり「応用先としての数学」には何も興味を持てず，かといって前例の少ない進学をするための準備時間も取れなかった．そんな中で，指導教員から鎌谷先生の存在と計算統計学の分野を知り，統数研の受験を決意する．たまたまこの年は統数研の改組があったため冬の受験のみであり，その頃には病は快方に向かっていたためよく対策できたのである．\n統数研では初年度から多くの学会に出させてもらったことがかけがえのない経験になった．MCMC の概念をなんとなく把握できた程度だった夏の ICIAM の ４日目 のセッションで，「粒子系を輸送する」という最適輸送の見方を取り入れた最適化ベースのサンプリング法を知り衝撃を受けた．サンプリングは確率分布の空間 \\(\\mathcal{P}(X)\\) 上の 力学系と見れる と気づいて大変興味を持った．\nサンプリング法も輸送もとびきりに面白かった．これで機械学習というものを一気に身近に感じて本格的に興味を持ち，年度末に機械学習分野最大のサマースクール MLSS2024 に参加するため，なんとかポスター発表を用意した．今思うとこれがその後の研究の方向性を決定付けた．誘ってもらった清水さんには感謝が尽きない．\nサマースクールでは全く知らない・興味のない分野の授業も聞くことになり，これが思いもしなかった出会いに導いてくれるのが美点であった．Francesco Orabona 氏の最適化の授業と Marco Cuturi 氏の最適輸送の授業とは当時から面白かったが，グラフニューラルネットワークと群論による位相的機械学習の授業も後から効いてきた．「数学は一つ」という思いを強めるばかりであった．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html",
    "href": "posts/Surveys/SMCSamplers.html",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nSMC の文脈で，目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) が複雑であるとき，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをする，というアイデアを 調温 (tempering) という（粒子フィルターの稿 も参照）．\nこの tempering という考え方は本質的に逐次的な発想を持っているが，元々は SMC の文脈とは全く独立に，MCMC を多峰性を持つ複雑な分布に対しても使えるように拡張する研究で提案された．これらの手法が自然と SMC へと接続する様子を population Monte Carlo (Iba, 2001b), (Ajay Jasra et al., 2007) というキーワードで理解されている．\nまずはその歴史を概観する．いずれも，目標分布 \\(\\pi_p\\) が多峰性をもち，MCMC がうまく峰の間を遷移できずに正しいサンプリングができない（収束が遅くなる）問題を解決する文脈の中で捉えられる．\n\n\nこれは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3\n\n\n\nMCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15\n\n\n\n\n\n\ntempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nこの方法は混合モデルにおいて事後分布が多峰性を持つなどして Gibbs サンプラーがうまく収束しない場合でも，有効な MCMC サンプラーとなる (A. Jasra et al., 2005)．\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，18 拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．19 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw(X_{1:p}):=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．20\n従って，本当は \\(E^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．21\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．\n\n\n\n\n\n\n\n簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．\n\n\n\n\n\n\n\n\n目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-SA",
    "href": "posts/Surveys/SMCSamplers.html#sec-SA",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "これは MCMC とは関係がなく，もはやシミュレーション法でさえなく最適化手法であるが，「調温」の考え方を一気にポピュラーにした手法であった．1 汎用最適化手法として，半導体製造を通じて，電子工学・コンピュータ産業にも大きな影響を与えた手法である．\nそもそも 焼きなまし (annealing) とは，物性物理の用語であり，鉄などの固体を極めて高音にして溶解させたのちに徐々に冷却することで，基底状態の構造を得るのに使われる技術であった．2\n分布列を \\(\\pi_n\\,\\propto\\,e^{-\\frac{h(x)}{T_n}}\\,dx\\) \\[\nT_1&gt;T_2&gt;\\cdots&gt;T_n\\searrow 0\n\\] と構成することで， \\[\n\\pi_n\\xrightarrow{n\\to\\infty}1_{\\operatorname*{argmin}h}(x)\\,dx\n\\] であることを利用して，関数 \\(h\\) の最小値を見つけることができる．3"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-population",
    "href": "posts/Surveys/SMCSamplers.html#sec-population",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "MCMC を複数同時に実行する手法を 拡張アンサンブル法 という (Iba, 2001a)．これは正準集団などの物理的根拠のあるアンサンブルを用いるのではなく，人工のアンサンブルを導入してサンプリング効率を向上させると捉えられるために呼ぶ．4\nmultilevel sampling とも呼ばれる．5\n一方で，次節 1.3 で扱う相関粒子法も含めて，複数のサンプルを用いる手法はとして population-based method とも呼ばれる (Iba, 2001b), (Ajay Jasra et al., 2007)．\n\n\n(Torrie and Valleau, 1977) では系のポテンシャルに傘ポテンシャルと呼ばれる追加項を足すことで，本来なら到達できない状態からもサンプリングすることを可能にするアイデアであり，拡張アンサンブル法の最初の萌芽と捉えられる．\n\nこの傘ポテンシャルとして，上述の意味でのテンパリング分布をとることも提案されており，後述の種々のテンパリング法の先駆けともみなせるのである．6\n\n\n\n積空間 \\(\\otimes_{n=1}^pE\\) 上で \\(\\pi_1\\otimes\\cdots\\otimes\\pi_p\\) を目標分布として MCMC を実行することを考えるのが MC3 (Metropolis-Coupled MCMC) (Geyer, 1991) である．\n時折，不変分布を変えないような Metropolis 核による提案に従って，MCMC 鎖の位置を交換することで収束を加速する．\nこの手法は parallel tempering7 または exchange Monte Carlo (Hukushima and Nemoto, 1996) という名前による独立な提案に伴って 交換モンテカルロ または レプリカ交換法，8 さらには population-based MCMC9 とも呼ばれる．\n\n特に，その分子動力学法版（REMD）(Sugita and Okamoto, 1999) が開発されてからは，分子シミュレーションの分野に広く受け入れられ，AMBER, CHARMM, GROMACS, NAMD などの汎用プログラムにも REMD が組み込まれた．(岡本祐幸, 2010)\n\nマルチカノニカル法 1.2.5 や模擬テンパリング 1.2.4 では荷重を決定するために試行が必要であるが，並行テンパリングでは荷重は Boltzmann 因子であるため，このような予備試行は必要ない．10\nしかしながら，全てのテンパリング手法に共通するように，交換の棄却率が高まりすぎないようにするためには隣り合う \\(\\pi_n,\\pi_{n+1}\\) を十分近く取る必要があり，すると必要な MCMC 鎖の数が極めて大きくなってしまうこともある．11\npopulation-based (Iba, 2001b) というのは，\\(p\\) 個の粒子を展開して高温状態でも探索してもらい，定期的に粒子を交換することでその情報を互いに伝え合うメカニズムのように思えるために言う．12 この観点から見ると，「鎖の間の交換」とは，粒子の間の相互作用としては極めてナイーブなもので，粒子フィルターに見られるような遺伝的なアルゴリズムの導入でより効率化できるのではないか？という発想が出てくる．\n\n\n\n並行テンパリングに加えて，種々の population-based method が提案された．(Ajay Jasra et al., 2007) によるレビューも参照．\nまずは Adaptive direction sampling (Gilks et al., 1994) がある．これは複数の粒子 \\(\\boldsymbol{x}:=\\{x_t^n\\}_{n=1}^p\\) を，\n\nある \\(x_t^a\\in\\boldsymbol{x}\\) を選んで，ここからアンカーポイント \\(y\\in E\\) を何かしらの方法で定める．\n\\(x_t^c\\in\\boldsymbol{x}\\setminus\\{x_t^a\\}\\) を選んで，1 で定めた \\(y\\in E\\) の方向にランダムに動かす．\n\nの繰り返しによって発展させていくことによりサンプリングする手法である．\nこのような手続きを，遺伝的アルゴリズムの考え方を取り入れてさらに推し進め，実際に MCMC としての収束レートを速めたのが 進化モンテカルロ (Liang and Wong, 2000), (Liang and Wong, 2001) である．\n\n\n\n最適化手法である 焼きなまし法（または模擬アニーリング） (Kirkpartick et al., 1983) のサンプリングへの変形として提案されたのが 焼き戻し法，または 模擬テンパリング (simulated tempering) (Marinari and Parisi, 1992) である．13\n模擬アニーリングでは温度は下がる一方であったのが，模擬テンパリングでは温度もある周辺分布に従って遷移する．模擬アニーリングは最終的にサンプルが最小値点の周りに集積して最適化問題を解くことが目的であったが，模擬テンパリングは高温状態においては多峰性分布が軟化され，峰の間を遷移しやすくなることを利用し，多峰性分布からの効率的なサンプリングを目指す．\n模擬テンパリングは状態空間を \\(E\\times [p]\\) に拡大して，その上でサンプリングを行うものともみなせる．14 \\(E\\times[p]\\) 上の標的分布を \\[\nX|N=n\\sim\\pi_n\n\\] を満たすようにし，\\(N|X\\) は適宜架橋分布 \\(\\{\\pi_n\\}\\) を往来するよう設計することで，MC3 が \\(p\\) 本の MCMC を用いて実現していたことを，\\(E\\times [p]\\) 上の MCMC 1つで効率的に実行する．\nまた，MCMC の収束を大幅に加速する手法としても，遺伝学における複雑な事後分布からのサンプリングへの応用を念頭に独立に提案された (Geyer and Thompson, 1995)．\n\n\n\nマルチカノニカル法 (Berg and Neuhaus, 1991) もポテンシャルを人工的に変更する方法であり，この点で傘サンプリングの発展ともみなせ，Adaptive umberlla sampling とも呼ばれる (Iba, 2001a)．\n物性物理学の分野から提案され，スピングラスの問題などでも大きな成果を挙げた．15"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "href": "posts/Surveys/SMCSamplers.html#sec-IPM",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "tempered transitions では，架橋列 \\(\\{\\pi_n\\}\\) をそれぞれの \\(\\pi_n\\) を不変分布に持つ Markov 核を通じて１往復して探索し，その結果を元に \\(\\pi_p\\) を効率的に探索するような MCMC の提案を構成する．16\nこの方法は混合モデルにおいて事後分布が多峰性を持つなどして Gibbs サンプラーがうまく収束しない場合でも，有効な MCMC サンプラーとなる (A. Jasra et al., 2005)．\nまた， \\[\n\\pi_n(x)\\,\\propto\\,\\pi_0(x)e^{-\\beta_nh(x)}\n\\] と表せる際，架橋分布 \\(\\{\\pi_n\\}\\) は温度比 \\(\\beta_n/\\beta_{n+1}\\) が一定になるように 幾何的に 取ることを提案しており，現在でも一般的な基準であるようである (Behrens et al., 2012)．\n\n\n\nここで初めて SMC の文脈にもテンパリングが輸入された．17 (Neal, 2001) は重点サンプリングによってあらゆる温度 \\(\\{\\pi_n\\}\\) からの提案を効率的に採用する方法を模索した．\nAIS は，各 \\(\\pi_i\\) を不変分布とする MCMC 核 \\(P_i\\) について，\\(\\pi_0P_1P_2\\cdots P_p\\) を重点サンプリング法における提案分布に用いる方法である．\nしかし，そのまま重点荷重を計算するのではなく，18 拡張された空間 \\(E^{p+1}\\) 上の目標分布 \\[\n\\pi_p\\otimes P_p^{-1}\\otimes\\cdots\\otimes P_1^{-1}\n\\] に対して \\(P_p\\otimes P_{p-1}\\otimes\\cdots\\otimes P_1\\otimes\\pi_0\\) を提案分布に用いたとして荷重荷重を計算する．19 実際には， \\[\nX_p\\sim P_{p}(X_{p-1},-),\\quad X_{p-1}\\sim P_{p-1}(X_{p-2},-),\\quad \\cdots\\quad X_1\\sim P_1(X_0,-),\\quad X_0\\sim \\pi_0\n\\] というように \\(X_0\\sim\\pi_0\\) を MCMC 核 \\(P_1,\\cdots,P_p\\) で順に流し，最後にウェイト \\[\nw(X_{1:p}):=\\frac{\\pi_p(X_p)}{\\pi_{p-1}(X_{p})}\\frac{\\pi_{p-1}(X_{p-1})}{\\pi_{p-2}(X_{p-1})}\\cdots\\frac{\\pi_2(X_2)}{\\pi_1(X_2)}\\frac{\\pi_1(X_1)}{\\pi_0(X_1)}\n\\] を計算する．20\n従って，本当は \\(E^{p+1}\\) 上で重点サンプリングを行っているが，\\(x_p\\) の成分のみに注目することで周辺分布では \\(\\pi_p\\) に対する効率的な重点サンプリングが実現されている．\nテンパリング遷移の後半のアルゴリズムを発展させた形とも見れる．\n同様の手法は自由エネルギーの推定の文脈で統計物理学で独立に提案されている (Jarzynski, 1997b), (Jarzynski, 1997a), (Crooks, 1998)．21\n\n\n\nこちらは模擬テンパリングを基にし，他の温度からの提案を保持しておく機構を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "href": "posts/Surveys/SMCSamplers.html#デノイジング拡散過程と最適架橋",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "簡単な分布からサンプリングをし，データの分布まで輸送するという発想は，生成モデリング，特に拡散過程のそれと同一である．\nここでは，近年の拡散過程とスコアマッチングの研究と SMC の交差について調べる．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#その他の手法",
    "href": "posts/Surveys/SMCSamplers.html#その他の手法",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "",
    "text": "目標分布の峰を特定するタスクを MCMC から分離して，BFGS 法 に基づく最適化法によって先に解いてしまう手法が (Pompe and Łatuszyński, 2020) によって提案されている．\nこれにより探索した峰の全体を \\(\\mathcal{I}:=\\{1,\\cdots,I\\}\\) に格納し，拡大した状態空間 \\(E\\times\\mathcal{I}\\) 上で \\(\\widetilde{\\pi}\\) を対象とした MCMC を実行するが，この \\(\\widetilde{\\pi}\\) をさらに適応的に更新する Auxiliary Variable Adaptive MCMC を提案している．"
  },
  {
    "objectID": "posts/Surveys/SMCSamplers.html#footnotes",
    "href": "posts/Surveys/SMCSamplers.html#footnotes",
    "title": "粒子フィルターを用いたサンプリング | About SMC Samplers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの前にも，Umbrella sampling (Torrie and Valleau, 1977) が本質的には密度の調温のアイデアを用いていた．(Liu, 2004, pp. 206–) Section 10.1 も参照．↩︎\n分子動力学 (molecular dynamics) などの文脈では Metropolis 法はちょうど分子運動のシミュレーションになっていることを踏まえれば，これを simulated annealing と呼ぶのは極めて鮮やかなアナロジーとなっている．焼きなまし法自体も，シミュレーション可能になったのである．↩︎\n(Geman and Geman, 1984) によると，各 \\(\\pi_n\\) における MCMC move の回数を \\(N_n\\) とした場合，\\(O(\\log(N_1+\\cdots+N_n))\\) のオーダーで \\(T_n\\) を（十分遅く）変化させれば，この手法はほとんど確実に \\(\\operatorname*{argmin}h\\) 内に収束する．(Liu, 2004, pp. 209–) 10.2 節も参照．↩︎\n(岡本祐幸, 2010), (Iba, 2001a) など．↩︎\n(Liu, 2004, pp. 205–) Chapter 10. Multilevel Sampling and Optimization Methods も参照．↩︎\n(Liu, 2004, p. 207) も参照．↩︎\n(Chopin et al., 2023), (Liu, 2004, p. 4) でも (Geyer, 1991) を引用して PT と呼んでいる．一方で物理学の分野では (Hukushima and Nemoto, 1996) の exchange Monte Carlo や (Swendsen and Wang, 1986) などの文献もある．前者は (Liu, 2004, p. 4) が “is reminiscent of parallel tempering (Geyer, 1991)” と指摘しており，後者は (Bouchard-Côté et al., 2012) などが引用している．↩︎\n最終講義 スピングラスと計算物性物理 p.34 も参照．温度の違う熱浴につけたレプリカをシミュレートして，時々交換する，という見方ができるためにこう呼ぶ．↩︎\n(Ajay Jasra et al., 2007) は (Geyer, 1991) を指して population-based MCMC と呼んでおり，SMC も含めて population-based simulation と呼んでいる．population-based という言葉自体は (Iba, 2001b) からとったという．“we define a population-based simulation method as one which, instead of sampling a single (independent/dependent) sample, generates a collection of samples in parallel” と定義しており，大きく MCMC によるものと逐次重点サンプリングベースのものの２流儀あるとしている．(Liu, 2004, pp. 225–) 第11章なども参照．↩︎\n(岡本祐幸, 2010) など．↩︎\n(Behrens et al., 2012, p. 66) も参照．↩︎\n(Iba, 2001a) が良い解説を与えていると (Ajay Jasra et al., 2007) でも言及されている．ただし，(Iba, 2001a) はこの並行テンパリングだけでなく，模擬テンパリング，multicanonical Monte Carlo (Berg and Neuhaus, 1991) / Adaptive Umbrella Sampling (Torrie and Valleau, 1977) を総称して 拡張アンサンブル法 (Extended Ensemble Monte Carlo) と呼んでサーベイしていることに注意．↩︎\n(Lyubartsev et al., 1992) が引用されることもある．(酒井佑士, 2017), (岡本祐幸, 2010) など．method of expanded ensemble とも呼ばれる (岡本祐幸, 2010), (Iba, 2001a)．↩︎\n記法 \\([p]=\\{1,\\cdots,p\\}\\) は 本サイトの数学記法一覧 を参照↩︎\nその後すぐに分子シミュレーションの分野にも導入された．(岡本祐幸, 2010) も参照．↩︎\n(Behrens et al., 2012) も参照．↩︎\n(Chopin and Papaspiliopoulos, 2020, p. 33) で，SMC を調温に初めて応用した論文として紹介されている．p.352 では “An early version of SMC tempering (without resampling)” としている．↩︎\n\\(\\pi_p(x_p)/\\pi_0P_1P_2\\cdots P_p\\) は多くの場合計算不能である．↩︎\nただし，\\(P_i^{-1}\\) とは，\\[ P_i(x_{i-1},x_i)\\pi_{i-1}(x_i-1)=\\pi_i(x_i)P_i^{-1}(x_{i-1},x_i) \\] で定まる確率核とした．\\(\\otimes\\) の記法はこちらも参照．↩︎\nこのウェイトの表示は，\\(P_i^{-1}/P_i=\\pi_{i-1}/\\pi_i\\) が成り立つことから直ちに従う．↩︎\n(Doucet et al., 2022) も参照．↩︎"
  },
  {
    "objectID": "posts/2025/Slides/学生研究発表会.html",
    "href": "posts/2025/Slides/学生研究発表会.html",
    "title": "動き出す次世代サンプラー・区分確定的モンテカルロ",
    "section": "",
    "text": "本研究はより現実的な設定でのサンプラーの挙動を解析することで，実際的な応用への示唆を得たり，さらには深層学習のように「ベイズ計算を促進するモデルへのパラメータの付け方」への洞察を得たりすることを目指す研究の先駆けとも位置付けられる．"
  },
  {
    "objectID": "posts/2025/Slides/学生研究発表会.html#計算技術による学際的統計解析ワークショップ-isact2025",
    "href": "posts/2025/Slides/学生研究発表会.html#計算技術による学際的統計解析ワークショップ-isact2025",
    "title": "動き出す次世代サンプラー・区分確定的モンテカルロ",
    "section": "1 計算技術による学際的統計解析ワークショップ (ISACT2025)",
    "text": "1 計算技術による学際的統計解析ワークショップ (ISACT2025)\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nFeb. 17-18th, 2025\nISM D305"
  },
  {
    "objectID": "posts/2025/Posters/Sticky.html",
    "href": "posts/2025/Posters/Sticky.html",
    "title": "ベイズ変数選択の計算的解決",
    "section": "",
    "text": "Date\nLocation\n\n\n\n\nFeb. 17th, 15:30-16:3018th, 11:20-12:00\nISM D305No. 11\n\n\n\n\n\n\n\nクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/Sticky.html#計算技術による学際的統計解析ワークショップ-isact2025",
    "href": "posts/2025/Posters/Sticky.html#計算技術による学際的統計解析ワークショップ-isact2025",
    "title": "ベイズ変数選択の計算的解決",
    "section": "",
    "text": "Date\nLocation\n\n\n\n\nFeb. 17th, 15:30-16:3018th, 11:20-12:00\nISM D305No. 11\n\n\n\n\n\n\n\nクリックで PDF を表示"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html",
    "href": "posts/2024/Process/ZigZag.html",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-過程",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "1 Zig-Zag 過程",
    "text": "1 Zig-Zag 過程\n\n1.1 はじめに\n１次元の Zig-Zag 過程は元々，Curie-Weiss 模型 における Glauber 動力学を lifting により非可逆化して得る Markov 連鎖の，スケーリング極限として特定された Feller-Dynkin 過程である (Bierkens and Roberts, 2017)．\n区分確定的 Markov 過程（PDMP）といい，ランダムな時刻にランダムな動きをする以外は，決定論的な動きをする過程である．\n\n\n\n\\(\\mathbb{R}^2\\) 上の Gauss 分布に収束する Zig-Zag 過程の軌跡\n\n\nPDMP の一般論については次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\nただし Zig-Zag 過程は，(Goldstein, 1951) で電信方程式と関連して，同様の過程が扱われた歴史もある．\n\n\n1.2 設定\nZig-Zag 過程 \\(Z=(X,\\Theta)\\) の状態空間は，力学的な立場に立って \\(E=\\mathbb{R}^d\\times\\{\\pm1\\}^d\\) と見ることが多い．\n力学における相空間と同様，\\(X\\in\\mathbb{R}^d\\) が位置を，\\(\\theta\\in\\{\\pm1\\}^d\\) が速度を表すと解する．すなわち，Zig-Zag 過程は全座標系と \\(45\\) 度をなす方向に，常に一定の単位速さで \\(\\mathbb{R}^d\\) 上を運動する粒子とみなせる．\nすなわち，\\((x,\\theta)\\in E\\) から出発する Zig-Zag 過程は，次の微分方程式系で定まる決定論的なフロー \\(\\phi_{(x,\\theta)}:\\mathbb{R}\\to\\mathbb{R}^d\\) に従って運動する粒子とみなせる： \\[\n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\\qquad \\frac{d \\Theta_t}{d t}=0.\n\\]\n\n\n1.3 アルゴリズム\n\n1.3.1 全体像\nZig-Zag 過程 \\(Z\\) は次のようにしてシミュレーションできる：\n\n\n\n\n\n\nZig-Zag 過程のシミュレーション\n\n\n\n\nレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) から定まる強度関数 \\[\nm_i(t):=\\lambda_i(x+\\theta t,\\theta),\\qquad i\\in[d],\n\\] を持つ，\\(d\\) 個の独立な \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程の，最初の到着時刻 \\(T_1,\\cdots,T_d\\) をシミュレーションする．\n最初に到着した座標番号 \\(j:=\\operatorname*{argmin}_{i\\in[d]}T_i\\) について，時刻 \\(T_j\\) に速度成分 \\(\\theta_j\\) の符号を反転させる．すなわち，関数 \\[\nF_j(\\theta)_i:=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n\\] に従ってジャンプする．\n１に \\(t=T_j\\) として戻って，くり返す．\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の跳躍測度\n\n\n\n\n\nもう一つ，MCMC の文脈で自然な見方は，状態空間を \\[\nE=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\}\n\\] と取る見方である．これは (Davis, 1993) による 一般の PDMP の設定 と対応する．\nこの \\(E\\) 上で，レート関数 \\[\n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n\\] が定める強度 \\[\nM(t):=\\lambda(x+t\\theta,\\theta)\n\\] を持った \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程に従ってジャンプが訪れる．\nこの点過程に対して，確率核 \\[\nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\\] に 印付けられた点過程 が，\\(Z\\) の跳躍測度である．\n\n\n\n\n\n\n\n\n\n証明（２つの定義の等価性）\n\n\n\n\n\nZig-Zag 過程に対する２つの定義を与えた．これら２つが同分布の過程を定めることは (Corbella et al., 2022), (Hardcastle et al., 2024) などさまざまなところで触れられているが，証明が必要である．\nまず，\\(\\min_{i\\in[d]}T_i\\) が，強度関数 \\(M\\) が定める到着時刻に同分布であることを示す．\n各 \\(T_i\\) の密度は \\[\np_i(t)=m_i(t)e^{-M_i(t)}1_{(0,\\infty)}(t)\n\\] で与えられ，\\(T_i\\) は互いに独立だから，\\((T_1,\\cdots,T_d)\\) の結合密度もわかる．\n\\(T_1,\\cdots,T_d\\) を昇順に並べた順序統計量を \\[\nT_{(1)}\\le\\cdots\\le T_{(d)}\n\\] で表すとする．この \\(d\\) 次元確率ベクトルの密度 \\(p\\) は， \\[\np(t_1,\\cdots,t_d)=1_{\\left\\{t_1\\le\\cdots\\le t_d\\right\\}}(t_1,\\cdots,t_d)\\left(\\sum_{\\sigma\\in\\mathfrak{S}_d}\\prod_{i=1}^dm_i(t_{\\sigma(i)})e^{-M_i(t_{\\sigma(i)})}\\right)\n\\] と計算できる．\nこの \\(p\\) を \\(t_2,\\cdots,t_d\\) に関して積分することで，\\(T_1\\) の密度が得られる：1 \\[\\begin{align*}\n    p_{(1)}(t)&=\\int_{(0,\\infty)^{d-1}}p(t_1,\\cdots,t_d)\\,dt_2\\cdots dt_d\\\\\n    &=\\biggr(\\sum_{i=1}^dm_i(t_1)\\biggl)\\exp\\left(-\\sum_{i=1}^dM_i(t_1)\\right)=m(t_1)e^{-M(t_1)}.\n\\end{align*}\\]\nこれは確かに，強度関数 \\(m\\) が定める到着時刻の密度である．\n続いて，\\(j=\\operatorname*{argmin}_{i\\in[d]}T_i\\) の，\\(\\min_{i\\in[d]}T_i\\) に関する条件付き確率質量関数が \\[\nq(i|t)=\\frac{m_i(t)}{\\sum_{i=1}^dm_i(t)}\n\\] であることを示す．\nそのためには，任意の \\(i\\in[d]\\) と \\(A\\in\\mathcal{B}(\\mathbb{R}^+)\\) とに関して \\(\\left\\{T_{(1)}\\in A,T_{(1)}=T_i\\right\\}\\) という形の事象を計算し，密度が積の形で与えられることを見れば良い．\n\\[\\begin{align*}\n    &\\qquad\\operatorname{P}[T_{(1)}\\in A,T_{(1)}=T_i]\\\\\n    &=\\operatorname{P}[T_i\\in A,\\forall_{j\\ne i}\\;T_i\\le T_j]\\\\\n    &=\\int_Ap_i(t_i)\\,dt_i\\left(\\sum_{\\sigma\\in\\mathrm{Aut}([d]\\setminus\\{i\\})}\\int^\\infty_{t_i}p_{\\sigma(1)}(t_{\\sigma(1)})\\,dt_{\\sigma(1)}\\int^\\infty_{t_{\\sigma(1)}}p_{\\sigma(2)}(t_{\\sigma(2)})\\,dt_{\\sigma(2)}\\cdots\\int^\\infty_{t_{\\sigma(d-1)}}p_{\\sigma(d)}(t_{\\sigma(d)})\\,dt_{\\sigma(d)}\\right)\\\\\n    &=\\int_Am_i(t_i)\\exp\\left(-\\sum_{i=1}^dm_i(t_i)\\right)\\,dt_i\\\\\n    &=\\int_A\\frac{m_i(t_i)}{m(t_i)}m(t_i)e^{-M(t_i)}\\,dt_i.\n\\end{align*}\\]\nよって，\\(\\min_{i\\in[d]}T_i\\) と \\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) とに関する結合密度は，2 \\[\nq(i|t)p_{(1)}(t)\n\\] という積の形で与えられることがわかった．\n\n\n\n\n\n\n\n\n\nまとめ\n\n\n\n\n前述の定義は，\\(\\min_{i\\in[d]}T_i\\) の形で密度 \\(p_{(1)}\\) からシミュレーションし，\\(\\operatorname*{argmin}_{i\\in[d]}T_i\\) の形で \\(q\\) からシミュレーションしている．\n後述の定義は，\\(p_{(1)}(t)\\) から直接シミュレーションし，再び \\(q(i|t)\\) から直接シミュレーションをする．\n\n１が２に等価であることがわかった．\n\n\n\n\n1.3.2 到着時刻 \\(T_i\\) のシミュレーション方法\nZig-Zag 過程のシミュレーションは，ほとんど強度\n\\[\nM_i(t):=\\int^t_0m_i(s)\\,ds\n\\] を持つ非一様 Poisson 点過程のシミュレーションに帰着される．\n実はこれは，指数分布確率変数 \\(E_i\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{Exp}(1)\\) について \\[\nT_i\\overset{\\text{d}}{=}M_i^{-1}(E_i)\n\\] と求まる．\n\n\n1.3.3 Poisson 剪定\n仮にこの逆関数 \\(M_i^{-1}\\) が得られない場合でも，剪定 (Lewis and Shedler, 1979) によって \\(T_i\\) は正確なシミュレーションが可能である．\nこの方法は，\\(M_i^{-1}\\) を数値的に計算するよりも遥かに速い．これは \\(M_i\\) の定義に積分が存在し，これが多くの場合高次元になるためである．\n\n\n\n1.4 レート関数の条件\nZig-Zag 過程 \\(Z\\) がどのような分布に従うかは，全てレート関数 \\(\\lambda_1,\\cdots,\\lambda_d\\) に委ねられている．\n\n\n\n\n\n\nZig-Zag 過程のレート関数 \\(\\lambda_1,\\cdots,\\lambda_d:E\\to\\mathbb{R}_+\\) は，負の対数密度 \\(U\\in C^1(\\mathbb{R}^d)\\) に対して， \\[\n\\lambda_i(x,\\theta):=(\\theta_i\\partial_iU(x))_++\\gamma_i(x,\\theta_{-i})\\quad(i\\in[d])\n\\] と定める．\nただし，次を仮定する：\n\n\\(\\gamma_i:E\\to\\mathbb{R}_+\\) は，\\(\\theta_i\\) のみには依らない任意の非負連続関数3 \\[\n  \\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta)).\n  \\]\n\\(e^{-U}\\in\\mathcal{L}^1(\\mathbb{R}^d)\\) が成り立ち，\\(\\pi(dx)\\,\\propto\\,e^{-U(x)}dx\\) が確率測度を定める．\n\\(M_i\\) は \\(t\\to\\infty\\) の極限で発散する： \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\]\n\n\n\n\n\n\n\n\n\n\n注（細かい条件たちについて）\n\n\n\n\n\nまた， \\[\nM_i(t):=\\int^t_0\\lambda_i(x+t\\theta,\\theta)\\,dt\n\\] は \\(t\\to\\infty\\) の極限で発散する必要がある．\nさもなくば，\\(M_i:(0,L)\\to(0,\\infty)\\;(L\\in(0,\\infty])\\) の形で定まらず，\\(M_i\\) がこのような可微分同相を与えない場合は \\[\nT_i:=M_i^{-1}(E_i),\\qquad E_i\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{Exp}(1),\n\\] によるシミュレーションも不正確になる．\n\n\n\n\n\n\n\n\n\n(Bierkens et al., 2019, pp. 1294 定理2.2)\n\n\n\n上述のリフレッシュレート \\(\\lambda_1,\\cdots,\\lambda_d\\) に対して，定義 1.3 で定まる Zig-Zag 過程 \\(Z\\) は次の分布 \\(\\widetilde{\\pi}=\\pi\\otimes\\mathrm{U}(\\{\\pm1\\}^{d})\\in\\mathcal{P}(E)\\) を不変にする： \\[\n\\widetilde{\\pi}(dxd\\theta)=\\frac{1}{2^d}\\frac{e^{-U(x)}}{\\mathcal{Z}}\\,dxd\\theta\n\\]\n\n\n\n\n\n\n\n\n注（拡張の可能性について）\n\n\n\n\n\n\\(\\{\\pm1\\}^d\\) 上の周辺分布が一様分布になっていること，勾配ベクトル \\(DU\\) の情報のみを使っており，座標に沿った方向しか見ていないため \\(U\\) の異方性に大きく左右されること，これらが「必ずしもそうある必要はない」拡張可能な点である．\n\n\n\n\n\n1.5 エルゴード性の条件\n\\(\\pi\\) が不変分布になるための十分条件 1.4 は極めて緩かったが，MCMC として使えるためにはエルゴード性が成り立つ必要がある．\n\n\n\n\n\n\n(Bierkens and Roberts, 2017 定理５)\n\n\n\n\\(d=1\\) で，レート関数 \\(\\lambda:E\\to\\mathbb{R}_+\\) はある \\(x_0&gt;0\\) が存在して次を満たすとする： \\[\n\\inf_{x\\ge x_0}\\lambda(x,1)&gt;\\sup_{x\\ge x_0}\\lambda(x,-1),\n\\] \\[\n\\inf_{x\\le-x_0}\\lambda(x,-1)&gt;\\sup_{x\\le-x_0}\\lambda(x,1).\n\\]\nこのとき，ある関数 \\(f:E\\to[1,\\infty)\\) が存在して \\(f(x,\\theta)\\to\\infty\\;(\\lvert x\\rvert\\to\\infty)\\) が成り立ち，かつ \\[\n\\left\\|P^t\\left((x,\\theta),-\\right)-\\pi\\right\\|_\\mathrm{TV}\\le\\kappa f(x,\\theta)\\rho^t,\\qquad(x,\\theta)\\in E,t\\ge0,\\rho\\in(0,1).\n\\]\n\n\n\n\n1.6 Subsampling Technology\nZig-Zag 過程はレート関数 \\(\\lambda\\) の設計に大きな自由度があった（第 1.4 節）．\nこれにより，Zig-Zag 過程ではバイアスを導入しない subsampling が可能であり，これを通じて データサイズに依らない一定効率での事後分布サンプリングが可能になる という super-efficiency (Bierkens et al., 2019) と呼ばれる性質を持つ．\nこの性質が実用上は最も重要である．詳しくは，次の記事を参照：\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#シミュレーション",
    "href": "posts/2024/Process/ZigZag.html#シミュレーション",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "2 シミュレーション",
    "text": "2 シミュレーション\n\n2.1 １次元での例\nZigZag サンプラーは非対称なダイナミクスを持っており，その点が MALA (Metropolis-adjusted Langevin Algorithm) や HMC (Hamiltonian Monte Carlo) などの従来手法と異なる．\n１次元でその違いを確認するために，Cauchy 分布という裾の重い分布を用いる．Cauchy 分布 \\(\\mathrm{C}(\\mu,\\sigma)\\) は次のような密度を持つ： \\[\nf(x)=\\frac{1}{\\pi\\sigma}\\frac{1}{1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n\\]\nその裾の重さ故，平均も分散も存在しない（発散する）．\nこのとき，次のような観察が得られる：\n\n\n\n\n\n\n\nZigZag サンプラーは Cauchy 分布に対して，最頻値から十分遠くから開始しても，高速に最頻値に戻ってくる．\nMALA は diffusive behaviour が見られ，最頻値に戻るまでに時間がかかる．\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する ZigZag サンプラーの軌道\n\n\nMALA と比較すると，その再帰の速さが歴然としている：4\n\n\nCode\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing Distributions\nusing Random\n\nRandom.seed!(2)\n\n# Define the target distribution (1D Cauchy) using the `LogDensityProblem` interface\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Choose initial parameter value for 1D\ninitial_θ = [500.0]\n\n# Use automatic differentiation to compute gradients\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensityCauchy(0.0, 1.0))\n\n# Set up the sampler with a multivariate Gaussian proposal.\nσ² = 100\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\n# Sample from the posterior.\nchain = sample(model_with_ad, spl, 2000; initial_params=initial_θ, chain_type=StructArray, param_names=[\"θ\"])\n\n# plot\nθ_vector = chain.θ\nsample_values = zip(1:length(θ_vector), θ_vector)\np2 = plot_1dtraj(sample_values, title=\"1D MALA Sampler (Cauchy Distribution)\", markersize=0, ylim=(-30, 750), label=\"MALA_1D\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する MALA サンプラーの軌道\n\n\n２つを並べて比較すると，Langevin ダイナミクスの方は，少し diffusive な動き（random walk property と呼ばれる）が見られることがわかる．\n\n\n\n\n\n\n例（NUTS サンプラーの動き）\n\n\n\n\n\nNUTS サンプラーはステップサイズを極めて大きくするため，プロットによるダイナミクスの比較があまり意味を持たなくなってくる．\n実際見てみると恐ろしいものである：\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC(0,1) に対する NUTS サンプラーの軌道\n\n\n\n\n\n\n\n2.2 ２次元での例\n\n\n\n\n\n\n\n勾配 \\(-\\nabla\\log\\pi\\) を計算し，∇ϕ(x, i, Γ) の形で定義する．\n\n\n\n\n\\[\n\\Sigma^{-1}=\\begin{pmatrix}2&-1\\\\-1&2\\end{pmatrix}\n\\]\nで定まる分散共分散行列 \\(\\Sigma\\) を持った中心化された正規分布 \\(\\pi(x)dx=\\mathrm{N}_2(0,\\Sigma)(dx)\\) に対しては，対数尤度は\n\\[\\begin{align*}\n    \\log\\pi(x)&=-\\log\\left((2\\pi)^{d/2}(\\det\\Sigma)^{1/2}\\right)\\\\\n    &\\qquad\\quad-\\frac{1}{2}x^\\top\\Sigma^{-1}x\n\\end{align*}\\]\nであるから，\\(\\phi:=-\\log\\pi\\) の第 \\(i\\) 成分に関する微分は\n\\[\\begin{align*}\n    \\partial_i\\phi(x)&=\\frac{\\partial }{\\partial x_i}\\biggr(\\frac{1}{2}x^\\top\\Sigma^{-1}x\\biggl)\\\\\n    &=\\Sigma^{-1}x.\n\\end{align*}\\]\n\nusing ZigZagBoomerang\nusing SparseArrays\n\nd = 2\n\n# 対数尤度関数 ϕ の第 i 成分に関する微分を計算\n1Γ = sparse([1,1,2,2], [1,2,1,2], [2.0,-1.0,-1.0,2.0])\n2∇ϕ(x, i, Γ) = ZigZagBoomerang.idot(Γ, i, x)\n\n# 初期値\nt0 = 0.0\nx0 = randn(d)\nθ0 = rand([-1.0,1.0], d)\n\n# Rejection bounds\nc = 1.0 * ones(length(x0))\n\n# ZigZag 過程をインスタンス化\nZ = ZigZag(Γ, x0*0)\n\n# シミュレーション実行\nT = 20.0\nzigzag_trace, (tT, xT, θT), (acc, num) = spdmp(∇ϕ, t0, x0, θ0, T, c, Z, Γ; adapt=true)\n\n# 軌跡を離散化\ntraj = collect(zigzag_trace)\n\n\n1\n\n勾配関数∇ϕの計算のためには，共分散行列の逆（精度行列ともいう）をSparseMatrixCSC型で指定する必要があることに注意．idotの実装 も参照．\n\n2\n\nidotは，疎行列Γの第i列と，疎ベクトルxとの内積を高速に計算する関数．\n\n\n\n\n\n\n\n\n\n\nidotの定義\n\n\n\n\n\nidot(A,j,u)は，疎行列Aの第j列と，疎ベクトルuとの内積を高速に計算する関数である．\n1function idot(A::SparseMatrixCSC, j, x)\n2    rows = rowvals(A)\n3    vals = nonzeros(A)\n    s = zero(eltype(x))\n4    @inbounds for i in nzrange(A, j)\n5        s += vals[i]'*x[rows[i]][2]\n    end\n    s\nend\n\n1\n\nパッケージ内部で，位置 \\(x\\in\\mathbb{R}^d\\) は全て SparseSate 型に統一されている？\n\n2\n\n疎行列 A の行インデックスを取得．rowvals(A)はベクトルであり，第１列から順番に，非零要素のある行番号が格納されている．\n\n3\n\n非零要素の値が格納されている．\n\n4\n\n@inbounds は，範囲外アクセスを許容するマクロ．高速化のためだろう．nzrange は，A の第 j 列に非零要素がある範囲を，第 \\(1\\) 列から累積して何番目かで返す．すなわち，rows[i]で正確に第j列の非零要素の行番号を狙い撃ちしてイテレーションできる．\n\n5\n\nxの非零要素がある行番号 rows[i] における成分の値 u[rows[i]][2] はこのような表記になる．これと，A の非零要素 vals[i] との内積を計算．\n\n\nなお，通常の行列に対しては，次のように実装されている：\nidot(A, j, x) = dot((@view A[:, j]), x)"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "href": "posts/2024/Process/ZigZag.html#zig-zag-サンプラーの実装",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "3 Zig-Zag サンプラーの実装",
    "text": "3 Zig-Zag サンプラーの実装\n\nZigZagBoomerang の実装を紹介する．\nJulia の MCMC パッケージ一般については次の稿を参照：\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#文献紹介",
    "href": "posts/2024/Process/ZigZag.html#文献紹介",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\nZig-Zag Sampler を導入したのは (Bierkens et al., 2019) であるが，ざっと仕組みを把握をしたいならば (Corbella et al., 2022) の第二章がよっぽどわかりやすいだろう．"
  },
  {
    "objectID": "posts/2024/Process/ZigZag.html#footnotes",
    "href": "posts/2024/Process/ZigZag.html#footnotes",
    "title": "Zig-Zag 過程によるサンプリング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n計算過程は省略したが，\\(d=2\\) の場合と，\\(d=3\\) の場合を少しやってみると良い．↩︎\n参照測度は，\\([d]\\) 上のものは計数測度 \\(\\#\\) をとっている．↩︎\n従って，レート関数 \\(\\lambda\\) は連続とする．この関数 \\(\\gamma_i\\) は，\\(U\\) の情報には依らない追加のリフレッシュ動作を仮定に加える．実際，\\(\\lambda_i(x,\\theta)-\\lambda_i(x,F_i(\\theta))=\\theta_i\\partial_iU(x)\\) である限り，\\(\\theta\\) と \\(F_i(\\theta)\\) の往来には影響を与えず釣り合っているため，どのような \\(\\gamma_i\\) をとっても，平衡分布には影響を与えない．しかし，高くするごとにアルゴリズムの対称性が上がるため，\\(\\gamma\\equiv0\\) とすることが Monte Carlo 推定量の漸近分散を最小にするという (Andrieu and Livingstone, 2021)．(Bierkens et al., 2021) でも同様の洞察がなされている．↩︎\n(Bierkens et al., 2019) にある提示の仕方である．Zig-Zag の 2000 単位時間を単純に MALA と比較はできないと筆者も考えるが，ダイナミクスに注目していただきたい．実際，自分で実装してみると，シード値をいじらないと，Zig-Zag は必ずしも 500 単位時間前後でモード \\(0\\) に戻るわけではない．↩︎\n実は６つ持つ．他の初期値は σ=(Vector(diag(Γ))).^(-0.5); λref=0.0, ρ=0.0↩︎"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html",
    "href": "posts/2024/Process/ResidualWaitingTime.html",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#導入",
    "href": "posts/2024/Process/ResidualWaitingTime.html#導入",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 Markov 過程のエルゴード性\n空間 \\(E\\) 上の Markov連鎖は，\\(E\\) 上の確率測度の空間 \\(\\mathcal{P}(E)\\) 上に力学系 \\(((P^*)^n\\mu)_{n\\in\\mathbb{N}}\\) を定める．その不動点 \\(P^*\\mu=\\mu P=\\mu\\) が不変確率分布（平衡分布）である．\nこれは，Markov 連鎖の 確率核 \\(P\\) の 左作用 \\(P:\\mathcal{L}_b(E)\\to\\mathcal{L}_b(E)\\) の随伴作用素 \\(P^*:\\mathcal{P}(E)\\to\\mathcal{P}(E)\\) が \\(\\mathcal{P}(E)\\) に作用して得られる力学系ともみれる．\n\n\n\n\n\n\nこの力学系 \\((\\mathcal{P}(E),P^*)\\) は不動点を持つか？持つならば，どのようなノルムについてどのくらいの速さで収束するか？\n\n\n\nこれが Markov 連鎖のエルゴード性の議論である．\n通常，Markov 過程のエルゴード性は全変動ノルムについて考慮されるが，近年は弱位相に関する議論も進んでいる．\n\n\n1.2 再起過程\n\n\n\n\n\n\n定義 (renewal process, counting process)1\n\n\n\n非負確率変数の独立同分布 \\(\\{T_n\\}\\subset\\mathcal{L}(\\Omega)_+\\) について，\n\n\\(\\{T_n\\}\\) が定めるランダムウォーク \\[S_0=0,\\qquad S_n:=T_1+\\cdots+T_n,\\qquad n\\ge1,\\] を 再起過程 または再生過程という．2 \\(\\{T_n\\}\\) を待ち時間 (interarrival times)，\\(S_n\\) を \\(n\\) 回目到着時刻という．\n再起過程 \\(\\{S_n\\}\\) の 再起回数過程 とは， \\[N_t:=\\sum_{n=0}^\\infty1_{[0,t]}(S_n)=\\sup_{n\\in\\mathbb{N}\\mid S_n\\le t}\\] をいう．\\(t\\mapsto\\operatorname{E}[N_t]\\) を再起関数という．\n\n\n\n再起過程 \\(\\{S_n\\}\\) は通常，繰り返し起こる事象の発生時間をモデル化するために用いられる．\nその代表的なものが，待ち時間 \\(\\{T_n\\}\\) を指数分布に取った場合である Poisson 過程である．\n再起過程は OR を中心として，多くの応用先を持つ：\n\n\n\n再起過程の応用例 by Claude 3 Opus\n\n\n\n\n1.3 付随する待ち時間の Markov 過程\n再起過程 \\(\\{S_n\\}\\) は，ある Markov 過程 \\(\\{X_t\\}\\) が原点に戻る時刻の列と捉えることで，エルゴード性を議論することができる．\nこのときの Markov 過程 \\(\\{X_t\\}\\) を，本稿では 待ち時間の Markov 過程 と呼ぶことにする．\n\n\n\n待ち時間の Markov 過程 \\((X_t)\\) のアニメーション．原点は左端としている．\n\n\n以下，第 2 節では離散時間で状態空間も離散 \\(\\mathbb{N}\\) の場合，第 3 節では連続時間で状態空間も連続 \\(\\mathbb{R}_+\\) の場合について，この待ち時間の Markov 過程 \\(X\\) のエルゴード性を調べる．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-chain",
    "href": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-chain",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "2 待ち時間の Markov 連鎖",
    "text": "2 待ち時間の Markov 連鎖\nまず，待ち時間 \\(T_n\\) は非負整数 \\(\\mathbb{N}=\\{0,1,\\cdots\\}\\) 値とし，その分布を \\((p_i)\\sim\\mathcal{P}(\\mathbb{N})\\) とする．\nこれが定める再生過程 \\(\\{S_n\\}\\) は，次の遷移確率 \\((p_{ij})\\) を持つ \\(\\mathbb{N}\\) 上の Markov 連鎖 \\(X\\) が原点 \\(0\\) に到着する時刻の列と同分布である： \\[\np_{(i+1)i}=1,\\qquad p_{0i}=p_i,\\qquad i\\in\\mathbb{N}.\n\\]\nこのとき，（無限次元の）確率行列 \\(P\\) は Frobenius の同伴行列 の転置の形をしている．\n\n2.1 エルゴード定理\n\n\n\n\n\n\n命題\n\n\n\n上で定義した Markov 連鎖 \\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\) について，\n\n任意の状態 \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) が成り立つとする．このとき，\\(X\\) は既約で非周期的であり，再帰的である．\nさらに，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) も満たすとき，\\(X\\) は不変確率測度 \\[\\mu_i=\\frac{\\sum_{j=i}^\\infty p_j}{1+\\sum_{j=1}^\\infty jp_j}\\] をもち，正に再帰的である．そうでないときは零再帰的である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n分布 \\((p_i)\\) が偶数の上にしか台を持たないなど，\\(\\mathrm{supp}\\;(p)\\) に周期がある場合は \\(X\\) は周期的になってしまうが，任意の \\(i\\in\\mathbb{N}\\) に関して \\(p_i&gt;0\\) ならば，任意の状態 \\(i\\in\\mathbb{N}\\) は本質的であり，互いに行き来できるため既約であり，周期も持たない．必ず有限時間内に原点に戻ってくるため，再帰的でもある．\n原点 \\(0\\) に初めて帰ってくる時刻を \\(T_0\\) とすると， \\[\\begin{align*}\n\\operatorname{E}_0[T_0]&=\\sum_{j=0}^\\infty(j+1)p_j\\\\\n&=1+\\sum_{j=0}^\\infty jp_j\\\\\n&=1+\\sum_{j=1}^\\infty jp_j.\n\\end{align*}\\] よって，正に再帰的であること \\(\\operatorname{E}_0[T_0]&lt;\\infty\\) は，\\(\\sum_{j=1}^\\infty jp_j&lt;\\infty\\) に同値． このとき，離散エルゴード定理より，ただ一つの不変測度 \\((\\mu_n)\\in\\mathcal{P}(\\mathbb{N})\\) を持ち， \\[\\mu_i=\\frac{1}{\\operatorname{E}_i[T_i]},\\qquad i\\in\\mathbb{N},\\] と表せる．これにより \\(i=0\\) の場合はすぐに計算できるが，\\(i&gt;0\\) の場合は少し計算の見通しが良くない．そこで，必要条件 \\[\\mu_i=\\mu_{i+1}+\\mu_0p_i,\\qquad i\\in\\mathbb{N},\\] に注目すると，これを再帰的に適用することで， \\[\\begin{align*}\n\\mu_{i-1}&=\\mu_{i-1}-\\mu_0p_{i-1}\\\\\n&=\\mu_{i-2}-\\mu_0p_{i-2}-\\mu_0p_{i-1}\\\\\n&=\\cdots\\\\\n&=\\mu_0-\\mu_0\\sum_{j=0}^{i-1}p_j\\\\\n&=\\mu_0\\sum_{j=i}^\\infty p_j.\n\\end{align*}\\]\n\n\n\n\nMarkov 連鎖の概念は次節で解説しているので，証明を読む前にぜひチェックしてください．\n\n\n2.2 離散 Markov 連鎖の概念\nまず，離散状態空間 \\(E\\) 上の Markov 連鎖は，各状態 \\(i\\in E\\) の分類から始まる．\n\n\n\n\n\n\n定義：状態の再帰性 (recurrent, transient, positive recurrent, null recurrent)\n\n\n\n\\(E\\) を可算集合，\\(\\{X_n\\}\\) を \\(E\\) 上の Markov 連鎖とする．状態 \\(i\\in E\\) について， \\[\n\\tau_i:=\\inf\\{n\\ge1\\mid X_n=i\\}\n\\] を到着時刻とする．\n\n\\(i\\) が 再帰的 な状態であるとは，Markov 連鎖 \\(\\{X_n\\}\\) が \\(i\\in E\\) からスタートした場合，必ずいずれ戻ってくることをいう： \\[\n\\operatorname{P}_i[\\tau_i&lt;\\infty]=1.\n\\] そうでない場合，\\(i\\in E\\) は 推移的 であるという．\n再帰的な状態 \\(i\\in E\\) がさらに 正に再帰的 であるとは，帰ってくる時刻の期待値が有限であることをいう： \\[\n\\operatorname{E}_i[\\tau_i]&lt;\\infty.\n\\] そうでない場合は 零再帰的 であるという．\n\n\n\n続いて，この状態 \\(i\\in E\\) 毎に定義した性質が，Markov 連鎖 \\(\\{X_n\\}\\) 全体の性質に直接に影響するためには，次の「既約性」の条件が必要である．\n状態 \\(i\\in E\\) から \\(j\\in E\\) へ 到達可能 であるとは，ある \\(n\\in\\mathbb{N}\\) が存在して \\(p_{ij}^n&gt;0\\) を満たすことをいう．これを \\(i\\to j\\) と表す．\n\n\n\n\n\n\n定義：既約性と非周期性\n\n\n\n\\(E\\) を可算集合，\\(\\{X_n\\}\\) を \\(E\\) 上の Markov 連鎖とし，その遷移確率を \\(p_{ij}^n=\\operatorname{P}_i[X_n=j]\\) と表す．\n\n状態 \\(i\\in E\\) が 本質的 であるとは，任意の到達可能な状態 \\(i\\to j\\) に対して，\\(j\\to i\\) でもあることをいう．\nMarkov 連鎖 \\(X\\) が 既約 であるとは，任意の本質的な状態 \\(i,j\\in E\\) が互いに到達可能であることをいう：\\(i\\leftrightarrow j\\)．\n\n\n\n\\(X\\) の遷移確率 \\(p\\) は，\\(E\\) の本質的な状態 \\(E_\\mathrm{ess}\\) 上に，互いに到達可能であるという関係 \\(\\leftrightarrow\\) を通じて同値類 \\(E_\\mathrm{ess}/\\leftrightarrow\\) を定めることが示せる．この同値類が１つに縮退することを，既約というのである．\nまた，周期 \\[\nd(i):=\\gcd\\{n\\ge1\\mid p_{ii}^n&gt;0\\}\n\\] は，先述の同値類 \\(E_\\mathrm{ess}/\\leftrightarrow\\) 上に関数を定める．この関数 \\(d:(E_\\mathrm{ess}/\\leftrightarrow)\\to\\mathbb{N}^+\\) が 定値関数 \\(1\\) となるとき，\\(X\\) を 非周期的 という．\n\n\n\n\n\n\n証明\n\n\n\n\n\n任意の \\(i,j,k\\in E\\) について，必ず \\[\np^{n+m}_{ik}\\ge p^n_{ij}p^m_{jk}\n\\] が成り立つ．\\(i\\to j\\) かつ \\(j\\to k\\) であるとき，ある \\(n,m\\ge1\\) が存在して \\(p^n_{ij}&gt;0\\) かつ \\(p^m_{jk}&gt;0\\) であるから，\\(p^{n+m}_{ik}&gt;0\\) である．よって，\\(i\\to j\\)．これより \\(\\leftrightarrow\\) は推移的である．反射性は \\(p_{ii}^0=1&gt;0\\) であるため，定義上成り立つ．対称性も成り立つ．\n続いて，\\(i\\leftrightarrow j\\) ならば，\\(d(i)=d(j)\\) を示す． \\[\nN_i:=\\gcd\\{n\\ge1\\mid p_{ii}^n&gt;0\\}\n\\] と表すと，\\(i\\leftrightarrow j\\) ならば \\(N_i\\ne\\emptyset\\) である．任意の \\(s\\in N_i\\) を取ると，仮定 \\(i\\leftrightarrow j\\) より，先ほどの議論と同様にして，ある \\(n,m\\ge1\\) が存在して， \\[\np^{n+m+ks}_{jj}\\ge p^m_{ji}p^s_{ii}p^n_{ij}&gt;0,\\qquad k=1,2,\\cdots.\n\\] よって，\\(d(j)|s\\) が必要であるから，\\(d(j)\\le d(i)\\) が結論づけられる．逆も全く同様に議論できるから，\\(d(j)=d(i)\\)．\n\n\n\n\n\n\n\n\n\n命題：既約な Markov 連鎖の再帰性\n\n\n\n状態 \\(i,j\\in E\\) は互いに到達可能であるとする：\\(i\\leftrightarrow j\\)．このとき，\\(i,j\\) の推移性・零再帰性・正再帰性は一致する．特に，Markov 連鎖 \\(X\\) が既約ならば，全ての状態が同じ再帰性を持つ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nひとまず (Hairer, 2021, p. 2) 参照．\n\n\n\nこうして，既約な Markov 連鎖 \\(P\\) の再帰性が議論できるようになる．推移的であるか，零再帰的であるか，正に再帰的であるかのいずれかである．\n\n\n2.3 離散エルゴード定理\nMarkov 連鎖 \\(X\\) が再帰的であるためには，既約性と非周期性が十分条件である．加えて，極限が零測度でなければ，正に再帰的である．\n\n\n\n\n\n\n離散エルゴード定理3\n\n\n\n\\(X=\\{X_n\\}_{n\\in\\mathbb{N}}\\subset L(\\Omega;E)\\) をMarkov連鎖，\\(E\\) を可算集合とする． \\(X\\) が既約で非周期的ならば，次が成り立つ：\n\n任意の本質的な状態 \\(i\\in E\\) について，次が成り立つ： \\[\np^n_{ij}\\xrightarrow{n\\to\\infty}\\mu_j=\\frac{1}{\\operatorname{E}_j[\\tau_j]},\\qquad j\\in E.\n\\] 特に，任意の開始地点 \\(i\\in E\\) について，\\((p_{i-}^n)\\) は \\(\\mu\\) に全変動収束する．\n加えて \\(X\\) が正に再帰的であるならば，\\(\\mu:=\\{\\mu_i\\}_{i\\in\\mathcal{X}}\\) は \\(X\\) のただ一つの不変確率測度である．\n\\(X\\) が零再帰的である場合は \\(\\mu_i\\equiv0\\) であり，\\(X\\) の不変確率測度は存在しない．\n\n\n\n状態空間 \\(E\\) が有限である場合，正に再帰的＝エルゴード的ならば，必ず収束は（全変動ノルムに関して）指数速度で起こる．\nしかし，\\(E\\) が可算無限である場合，速度は様々である．\n\\(\\mathbb{N}\\) 上の待ち時間の Markov 連鎖が，その良い例となっている．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-process",
    "href": "posts/2024/Process/ResidualWaitingTime.html#sec-waiting-markov-process",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "3 待ち時間の Markov 過程",
    "text": "3 待ち時間の Markov 過程\n\n3.1 過程の定義\n待ち時間の分布 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) は非零な１次の積率を持つとする．\n\\(\\nu\\) が定める再生過程を作り出す Markov 過程 \\(\\{X_t\\}\\) とは，離散時間の場合（第 2 節）と同様，\n\n\\(\\mathbb{R}^+\\) 上で \\(\\dot{X}_t=-1\\)．\n\\(X_t=0\\) のとき，次の瞬間 \\(\\nu\\) に従って選択されたある正の値にジャンプする．\n\nこのとき，次が成り立つ：\n\n\n\n\n\n\n命題\n\n\n\n上で定義した Markov 過程 \\(\\{X_t\\}\\) について，\n\n生成作用素は次で定まる： \\[\nLf(x)=-\\frac{d f(x)}{d x},\\qquad f\\in\\mathcal{D}(L),\n\\] \\[\n\\mathcal{D}(L):=\\left\\{f\\in\\mathcal{L}^1(\\nu)\\,\\middle|\\,f(0)=\\int^\\infty_0f(x)\\nu(dx)\\right\\}.\n\\]\n次で定まる確率分布 \\(\\mu_*\\in\\mathcal{P}(\\mathbb{R}_+)\\) は \\(\\{X_t\\}\\) に関して不変である： \\[\n\\mu_*(dx)=c\\nu([x,\\infty])dx,\n\\] \\[\nc:=\\int^\\infty_0y\\nu(dy)\\in(0,\\infty).\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n収束 \\[\n\\frac{P_tf(x)-f(x)}{t}\\to-\\frac{d f(x)}{d x}\\qquad t\\to\\infty,\n\\] は，\\(x\\ne0\\) の場合直ちに成り立つ．\nこれが \\(x=0\\) の場合も含めて一様に成り立つことと，\\((\\nu|f)=f(0)\\) は同値である．\n任意の \\(f\\in C_c^1(\\mathbb{R}_+)\\cap\\mathcal{D}(L)\\) について，次のようにして \\((\\mu_*|Lf)=0\\) が示せるためである： \\[\\begin{align*}\n(\\mu_*|Lf)&=-\\int^\\infty_0f'(x)\\mu_*(dx)=-c\\int^\\infty_0f'(x)\\nu([x,\\infty))\\,dx\\\\\n&=-c\\biggl[f(x)\\nu([x,\\infty))\\biggr]^\\infty_0-c\\int^\\infty_0f(x)\\nu(dx)=cf(0)-cf(0)=0.\n\\end{align*}\\]\n\n\n\n\n\n\n3.2 多項式エルゴード性\n\n\n\n\n\n\n定理（待ち時間の Markov 連鎖の多項式エルゴード性）\n\n\n\n待ち時間の分布は \\(\\nu\\ll\\ell_1\\) で密度 \\(p\\) をもち，ある \\(\\zeta&gt;2\\) が存在して \\(p\\) は \\(x^{-\\zeta}\\) のレートを持つとする： \\[\n\\frac{c_-}{x^\\zeta}\\le p(x)\\le\\frac{c_+}{x^\\zeta}\n\\] このとき，次の多項式エルゴード性が成り立つ： \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le C\\frac{x^\\alpha}{t^{\\alpha-1}},\n\\] \\[\nt\\ge0,x\\in\\mathbb{R}_+,\\alpha\\in(0,\\zeta-1).\n\\] 加えて，次が成り立つ： \\[\n\\lim_{t\\to\\infty}\\frac{\\log\\|P_t(x,-)-\\mu_*\\|_\\mathrm{TV}}{\\log t}=2-\\xi.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(V(x):=x^{\\alpha}\\;\\mathrm{on}\\;[1,\\infty)\\;(\\alpha&gt;0)\\) という形の関数であって，\\(V\\in\\mathcal{D}(L)\\) を満たすものが存在する．\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\alpha\\in(0,\\zeta-1)\\) を満たすように取れば， \\[\n\\int^\\infty_0V(x)\\nu(dx)=\\int^1_0V(x)p(x)dx+c_+\\int_1^\\infty x^{\\alpha-\\xi}dx&lt;\\infty\n\\] より，この値を \\(V(0)\\) とし，\\((0,\\infty)\\) 上で \\(C^1\\)-級になるように繋げば良い．\n\n\n\nこのとき， \\[\nLV(x)=-\\alpha x^{\\alpha-1}=-\\alpha V(x)^{1-\\frac{1}{\\alpha}}\\qquad\\mathrm{on}\\;[1,\\infty)\n\\] が成り立つ．即ち，Lyapunov関数 \\(\\varphi(x):=\\alpha x^{1-\\frac{1}{\\alpha}}\\) に関する劣線型ドリフト条件を満たす．スケルトンの議論を通じて，多項式エルゴード定理より，\\(\\frac{(1-1/\\alpha)}{1-(1-1/\\alpha)}=\\alpha-1\\) のレートで収束する： \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le C\\frac{\\lvert x\\rvert^\\alpha}{t^{\\alpha-1}}.\n\\] ここで \\(\\alpha\\in(0,\\zeta-1)\\) は任意の値であったから， \\[\n\\log\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\le\\log C+\\alpha\\log x-(\\alpha-1)\\log t\n\\] \\[\n\\therefore\\qquad\\limsup_{t\\to\\infty}\\frac{\\log\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}}{\\log t}\\le\\limsup_{\\alpha&lt;\\zeta-1}-(\\alpha-1)=2-\\zeta.\n\\]\n最後の主張を示す．まず，\\(LV\\) は上に有界であるから， \\[\n\\frac{d }{d t}P_tV(x)=P_tLV(x)\\le C\n\\] \\[\n\\therefore\\qquad P_tV(x)\\le Ct+x^\\alpha=:g(x,t).\n\\] 続いて，\\(\\frac{c_-}{x^\\zeta}\\le p(x)\\) より，\\(\\mu_*\\) の密度は下から評価できる： \\[\n\\frac{\\mu_*(dx)}{dx}=\\int^\\infty_xp(y)\\,dy\\ge\\int^\\infty_x\\frac{c_-}{y^\\zeta}\\,dy=cx^{1-\\zeta}.\n\\] これより， \\[\n\\mu_*[V&gt;R]=\\mu_*[x&gt;R^{1/\\alpha}]\\ge CR^{-\\frac{\\zeta-2}{\\alpha}}=:f(R)\n\\] を得る．以上の評価とから， \\[\n\\frac{1}{2}\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\ge f(R)-\\frac{g(x,t)}{R}=CR^{-\\frac{\\zeta-2}{\\alpha}}-\\frac{\\lvert x\\rvert^\\alpha+Ct}{R}\n\\] \\(R&gt;0\\) について最適化することで， \\[\n\\|P^t(x,-)-\\mu_*\\|_\\mathrm{TV}\\ge C\\biggr(\\lvert x\\rvert^\\alpha+Ct\\biggl)^{\\frac{\\zeta-2}{\\zeta-2-\\alpha}}.\n\\] 同様にして，\\(\\alpha\\nearrow\\zeta-1\\) を考えることで結論が従う．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#参考文献",
    "href": "posts/2024/Process/ResidualWaitingTime.html#参考文献",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "4 参考文献",
    "text": "4 参考文献\n\n離散時間の場合は (Kulik, 2018, p. 22) 例 1.3.6, (Feller, 1967, p. 381) 例 XV.2.(k)，連続時間の場合は (Hairer, 2021, pp. 35–36) を参考にした．"
  },
  {
    "objectID": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "href": "posts/2024/Process/ResidualWaitingTime.html#footnotes",
    "title": "待ち時間の Markov 過程のエルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Resnick, 2002, p. 174), (Mitov and Omey, 2014, p. 1), (Nummelin, 1984, p. 49) 定義4.2 など参照．計数過程の用語については (Aalen, 1978, p. 701) の解説も参照．↩︎\n一般には和訳「再生過程」が定着しているだろう．だが regeneration process ではなく，renewal process なのである．生死というよりは，再起というべきだと考えるため，ここでは再起過程と呼ぶこととする．最大の欠点は「再帰」と音が同じことである．↩︎\n(Kulik, 2018, p. 16) 定理1.2.5，(Robert and Casella, 2004, p. 224) を参照．↩︎"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html",
    "href": "posts/2024/Process/MartingaleProblem.html",
    "title": "マルチンゲール問題",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "href": "posts/2024/Process/MartingaleProblem.html#footnotes",
    "title": "マルチンゲール問題",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Hoh, 1998, p. 28), (Criens et al., 2023)↩︎"
  },
  {
    "objectID": "posts/2024/Process/OU1.html",
    "href": "posts/2024/Process/OU1.html",
    "title": "Ornstein-Uhlenbeck 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nYUIMAについては次の記事も参照：\n\n  \n    \n      \n      \n        YUIMA 入門\n        確率微分方程式のシミュレーションと推測のためのパッケージ`yuima`の構造と使い方をまとめます．"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html",
    "href": "posts/2024/Process/Poisson.html",
    "title": "Poisson 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html#poisson-過程",
    "href": "posts/2024/Process/Poisson.html#poisson-過程",
    "title": "Poisson 過程を見てみよう",
    "section": "1 Poisson 過程",
    "text": "1 Poisson 過程\n\n1.1 はじめに\nPoisson 過程と言った場合，Poisson 点過程 \\(\\eta\\) と Poisson 計数過程 \\(N\\) の２つを峻別する必要がある．1\n\n\n\n\n\n\n\\(E=[0,T]\\)，\\(\\lambda&gt;0\\) をレートとした場合，\n\n\\(E\\) 上の（一様な） Poisson 点過程 \\(\\eta\\) とは，\\(X_k\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{U}([0,T])\\) を一様確率変数，\\(\\kappa\\sim\\mathrm{Pois}(\\lambda T)\\) を Poisson 確率変数として， \\[\n  \\eta=\\sum_{k=1}^\\kappa\\delta_{X_k}\n  \\] で表せる 確率核 \\(\\Omega\\to E\\) をいう．\n\\(E\\) 上の（一様な） Poisson 計数過程 \\(N\\) とは，点過程 \\(\\eta\\) の \\([0,t]\\) 内の点の数 \\[\n  N_t:=\\#\\left\\{n\\in\\mathbb{N}\\mid X_n\\le t\\right\\}=\\sum_{n=1}^\\infty1_{\\left\\{X_n\\le t\\right\\}}\n  \\] で定まる 確率過程 \\(\\{N_t\\}\\subset\\mathcal{L}(\\Omega)\\) をいう．\n\n\n\n\nこのとき， \\[\n\\eta([0,t])\\sim\\mathrm{Pois}(\\lambda t),\n\\] \\[\nN_t\\sim\\mathrm{Pois}(\\lambda t),\n\\] が成り立ち，\\(N\\) は Lévy 過程になる．\n\\(N\\) は Lévy 過程の中でも，大きさ１の跳躍のみで増加するものとして特徴付けられる．2\n\\(N\\) は，ランダム測度 \\(\\eta\\) が定める \\[\nN_t(\\omega)=\\eta(\\omega,[0,t])=\\int_0^t\\eta(\\omega,ds)\n\\] とも理解できる．\n\n\n1.2 点過程の定義\n\n\n\n\n\n\n定義 (point process)3\n\n\n\n\\((E,\\mathcal{E})\\) を測度空間とする．\\(E\\) 上の 点過程 とは，次の条件を満たす確率核 \\(\\eta:\\Omega\\to E\\) をいう： \\[\n\\eta(\\omega,B)\\in\\mathbb{N}\\cup\\{\\infty\\},\\qquad\\omega\\in\\Omega,B\\in\\mathcal{E}.\n\\]\n\n\n従って，任意の点過程 \\(\\eta\\) に対して，積分 \\[\n(\\eta|f)(\\omega):=\\int_E\\eta(\\omega,dx)f(x)\\in[-\\infty,\\infty]\n\\] が定まる．\n点過程 \\(\\eta\\) が 真の点過程 であるとは，ある \\(E\\)-値確率変数の列 \\(X_1,X_2,\\cdots\\) と，\\(\\mathbb{N}\\cup\\{\\infty\\}\\)-値確率変数 \\(\\kappa\\) が存在して， \\[\n\\eta=\\sum_{n=1}^\\kappa \\delta_{X_n}\\;\\;\\text{a.s.}\n\\] と表せることとする．\n\n\n\n\n\n\n真の点過程になるための条件\n\n\n\n\n\n(Revuz and Yor, 1999, p. 471) 定義XII.1.1 は違う定義を与えている：\n\n\\(E\\) 上の点過程とは，\\(E_\\delta:=E\\cup\\{\\delta\\}\\) 上の確率過程 \\(\\{e_t\\}_{t\\in\\mathbb{R}_+}\\subset\\mathcal{L}(\\Omega;E_\\delta)\\) であって，次の２条件を満たすものをいう：\n\n\\((t,\\omega)\\mapsto e_t(\\omega)\\) は \\(\\mathcal{B}(\\mathbb{R}^+)\\otimes\\mathcal{F}\\)-可測である．\n\\(D_\\omega:=\\left\\{t\\in\\mathbb{R}_+\\mid e_t(\\omega)\\ne\\delta\\right\\}\\) は殆ど確実に可算である．\n\n\n(Kingman, 2006) の定義は「\\(E\\) 内のランダムに決まる可算集合」というもので，これと等価な定義である．この定義は，上で与えた確率核による定義よりも強い（範囲が狭い）ものである．\n確率核の意味での点過程が，追加で次の条件を満たすとき，真の点過程になる．4 特に，(Revuz and Yor, 1999) と (Kingman, 2006) の意味でも点過程である．\n\nある可算な分割 \\(\\sqcup_{n\\in\\mathbb{N}}B_n=E\\) が存在して， \\[\n\\operatorname{P}[\\eta(B_n)&lt;\\infty]=1\n\\] である．\n\n\n\\(E\\) が Polish 空間で，\\(\\eta(\\omega,-)\\) が（殆ど確実に）任意の有界集合上で有限ならば，これを満たす．\n\\(\\sigma\\)-有限な強度測度を持つ Poisson 過程もこれを満たす（第 1.4 節）．\n\n\n\n\n\n\n\n\n\n\n例 (Cox, 1955 過程)5\n\n\n\n\n\n\\(\\xi\\) を確率核 \\(\\Omega\\to E\\) とする．すなわち，可測写像 \\(\\Omega\\to\\mathcal{P}(E)\\) であるとする．このような確率核 \\(\\xi\\) をランダム測度ともいう．\nランダム測度 \\(\\xi\\) によって定まる強度測度を持った Poisson 過程 \\(\\eta\\) を，Cox 過程 または二重確率 Poisson 過程という．\n式で表すと，強度測度 \\(\\lambda\\) を持つ Poisson 点過程の分布を \\(\\Pi_\\lambda\\in\\mathcal{P}(E)\\) とすると， \\[\n\\eta|\\xi\\overset{\\text{d}}{=}\\Pi_\\xi\n\\] を満たす点過程 \\(\\eta:\\Omega\\to E\\) をいう．\n例えば，Poisson 過程のレート関数を Gauss 過程によってモデリングした場合，これは Cox 過程によりモデリングしていることになる．6\n\n\n\n\n\n1.3 強度測度\n点過程には「各集合 \\(B\\in\\mathcal{E}\\) に平均何個の点が入るか」を表す 強度測度 \\(\\lambda\\) が定まる．平均測度 とも呼ばれる．\nこの強度測度 \\(\\lambda\\) は \\(\\operatorname{E}[d\\eta]\\) のようなものであり，Fubini の定理のような性質 \\[\n\\operatorname{E}\\left[\\int_E u\\,d\\eta\\right]=\\int_Eu\\operatorname{E}[d\\eta]\n\\] が成り立つ．これを Campbell の公式 という．\n\n\n\n\n\n\n定義 (intensity / mean measure)7\n\n\n\n\\(\\eta\\) を可測空間 \\((E,\\mathcal{E})\\) 上の点過程とする．\\(\\eta\\) の 強度測度 とは，次で定まる測度 \\(\\lambda:\\mathcal{E}\\to[0,\\infty]\\) をいう： \\[\n\\lambda(B):=\\operatorname{E}[\\eta(B)],\\qquad B\\in\\mathcal{E}.\n\\]\n\n\n\n\n\n\n\n\n命題 (Campbell, 1909 の公式)8\n\n\n\n\\(\\eta\\)　を点過程，\\(\\lambda\\) をその強度測度とする．任意の \\(u\\in\\mathcal{L}(E)\\) について，\\((\\eta|u)\\) は確率変数であり，加えて \\(u\\in\\mathcal{L}^1(\\lambda)\\) または \\(u\\ge0\\) である場合，次が成り立つ： \\[\n\\operatorname{E}\\left[\\int_Eu(x)\\eta(dx)\\right]=\\int_Eu(x)\\lambda(dx).\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(u\\) がある \\(B\\in\\mathcal{E}\\) の特性関数 \\(u=1_B\\) である場合， \\[\n\\int_Eu(x)\\eta(dx)=\\eta(B)\n\\] であり，点過程 \\(\\eta\\) の定義からこれは確率変数である．\n期待値の線型性より一般の単関数，単調収束定理より一般の可測関数についても成り立つ．\n\\(u\\in\\mathcal{L}^1(\\lambda)\\) であるとき，\\(u\\) に収束する単関数列 \\[\nu_n:=\\sum_{i=1}^{m_n}a_i^{(n)}1_{B_i^{(n)}}\\nearrow u\n\\] を取ると，Lebesgue の優収束定理より次のように結論づけられる： \\[\\begin{align*}\n    \\operatorname{E}\\left[\\int_Eu(x)\\eta(dx)\\right]&=\\lim_{n\\to\\infty}\\operatorname{E}\\left[\\int_Eu_n(x)\\eta(dx)\\right]\\\\\n    &=\\lim_{n\\to\\infty}\\operatorname{E}\\left[\\sum_{i=1}^{m_n}a_i^{(n)}\\eta(B_i^{(n)})\\right]\\\\\n    &=\\lim_{n\\to\\infty}\\sum_{i=1}^{m_n}a_i^{(n)}\\lambda(B_i^{(n)})\\\\\n    &=\\int_Eu(x)\\lambda(dx)=(\\lambda|u)&lt;\\infty.\n\\end{align*}\\]\n\\(u\\ge0\\) の場合も同様で，Lebesgue の優収束定理が単調収束定理に変わるのみである．\n\n\n\n\n\n1.4 Poisson 点過程の定義\n\n\n\n\n\n\n定義 (Poisson (point) process / random measure)9\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間，\\(\\lambda:\\mathcal{E}\\to[0,\\infty]\\) を測度とする．強度（測度） \\(\\lambda\\) を持つ Poisson 過程 とは，次の２条件を満たす点過程 \\(\\eta\\) をいう：\n\n任意の \\(B\\in\\mathcal{E}\\) に対して，\\(\\eta(B)\\sim\\mathrm{Pois}(\\lambda(B))\\)．\n任意の \\(m\\in\\mathbb{N}\\) と互いに素な可測集合 \\(B_1,\\cdots,B_m\\in\\mathcal{E}\\) について，\\(\\eta(B_1),\\cdots,\\eta(B_m)\\) は独立である．\n\n\n\n２つの強度測度 \\(\\lambda,\\lambda'\\) は \\(\\sigma\\)-有限であるとする．\nこのとき，\\(\\lambda=\\lambda'\\) ならば，これを強度とする Poisson 過程は分布同等である．10\n加えて，\\(\\sigma\\)-有限な強度測度を持つ Poisson 過程は，真の点過程（と同分布）である：\n\n\n\n\n\n\n命題（Poisson 過程が真の点過程となるとき）11\n\n\n\n\\(\\lambda\\) を \\(\\sigma\\)-有限とする．このとき，ある確率変数の列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) と \\(\\mathbb{N}\\cup\\{\\infty\\}\\)-値確率変数 \\(\\kappa\\) が存在して， \\[\n\\eta:=\\sum_{n=1}^\\kappa\\delta_{X_n}\n\\] は強度 \\(\\lambda\\) の Poisson 過程となる．\n\n\n実は Poisson 点過程は，Poisson 分布と同様に，可算な範囲で再生性がある：\\(\\eta^{(1)},\\eta^{(2)},\\cdots\\) を独立な Poisson 点過程とすると， \\[\n\\eta:=\\sum_{k=1}^\\infty\\eta^{(k)}\n\\] も Poisson 点過程であり，強度測度は \\(\\nu:=\\sum_{k=1}^\\infty\\lambda^{(k)}\\) となる．12\n従って，実際は \\(\\sigma\\)-有限な強度測度を持つ Poisson 過程よりもさらに一般的な設定で上の定理が成り立つ．\n\n\n1.5 例\n\n\n\n\n\n\n\n強度測度が有限であるとき，複合二項過程として極めて明瞭な理解ができる．\n区間上の Poisson 過程は，一様分布に関する複合二項過程になる．点の間の間隔は指数分布に従う．13\n\n\n\n\n\n1.5.1 複合二項過程\n\\(\\mu\\in\\mathcal{P}(E),\\pi\\in\\mathcal{P}(\\mathbb{N})\\) とする．\\(X_k\\overset{\\text{i.i.d.}}{\\sim}\\mu\\) と \\(\\kappa\\sim\\pi\\) について， \\[\n\\eta:=\\sum_{k=1}^\\kappa\\delta_{X_k}\n\\] は点過程を定める．これを サンプリング分布 \\(\\mu\\) を持った \\(\\pi\\) による 複合二項過程 という．14\nある \\(\\gamma&gt;0\\) に関して \\(\\pi=\\mathrm{Pois}(\\gamma)\\) と取った場合，\\(\\eta\\) は強度 \\(\\gamma\\mu\\) を持った Poisson 過程となる．\n複合二項過程について，次が成り立つ：\n\n\n\n\n\n\n命題（有限な強度を持つ Poisson 過程の特徴付け）15\n\n\n\n\\(\\lambda:\\mathcal{E}\\to(0,\\infty)\\) を有限測度とする．このとき，点過程 \\(\\eta\\) について，次の２条件は同値である：\n\n\\(\\eta\\) は \\(\\lambda\\) を強度測度とする Poisson 過程である．\nサンプリング分布 \\(\\mu:=\\lambda(E)^{-1}\\lambda\\) を持つ \\(\\mathrm{Pois}(\\lambda(E))\\)-複合二項過程である．\n\n特に，\\(\\eta(E)=m\\) で条件づけた分布は，\\(\\delta_{X_1}+\\cdots+\\delta_{X_m}\\) に等しい．\n\n\n\n\n1.5.2 \\(\\mathbb{R}_+\\) 上の Poisson 過程\n\\(\\mathbb{R}_+\\) の Poisson 過程で，強度が \\(\\mathbb{R}_+\\) 上の Lebesgue 測度の定数倍であるものを 一様 Poisson 過程 といい，\\(\\lambda&gt;0\\) を レート ともいう．\n\n\n\n\n\n\n命題（\\(\\mathbb{R}_+\\) 上の Poisson 点過程の特徴付け）16\n\n\n\n\\(\\eta\\) を \\(\\mathbb{R}_+\\) 上の点過程とする．このとき，\\(\\lambda&gt;0\\) に関して次は同値：\n\n\\(\\eta\\) はレート \\(\\lambda\\) を持った一様 Poisson 過程である．\n到着時刻 \\[\nT_n(\\omega):=\\inf\\left\\{t\\ge 0\\mid \\eta(\\omega,[0,t])\\ge n\\right\\}\n\\] の間隔は \\(\\operatorname{Exp}(\\lambda)\\) の独立同分布である．\n\n\n\n\n\n\n1.6 印付き過程\n\n\n\n\n\n\n定義 (\\(K\\)-marked process)17\n\n\n\n\\(\\eta\\) を \\(E\\) 上の真の点過程，\\(K:E\\to F\\) を確率核，\\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega;F)\\) を確率変数列とする．加えて，任意の \\(m\\in\\mathbb{N}\\cup\\{\\infty\\}\\) について，\\(\\kappa=m\\) と \\(X_1,\\cdots,X_m\\) の値で条件づけた \\(Y_i\\) の分布は，もはや \\(X_i\\) の値にしか依らずに独立に定まるとする： \\[\n(Y_n)_{n\\le m}\\,|\\,\\kappa=m,(X_n)_{n\\le m}\\overset{\\text{d}}{=}\\otimes_{n=1}^mK(X_n,-)\n\\] このとき，\\(E\\times F\\) 上の真の点過程 \\[\n\\xi:=\\sum_{n=1}^\\kappa\\delta_{(X_n,Y_n)}\n\\] を \\(X\\) の \\(K\\) で印付けられた過程 または \\(K\\)-付印過程という．\n\n\n\\(\\eta\\) を \\(\\sigma\\)-有限な強度 \\(\\lambda\\) を持つ Poisson 過程とするとき，\\(\\eta\\) の任意の可測写像 \\(T:E\\to F\\) による像は \\(T_*\\lambda\\) を強度とする Poisson 過程となり，\\(\\eta\\) の任意の \\(K\\)-印付き過程もやはり Poisson 過程になる．18\n\\(K:E\\to\\mathcal{P}(F)\\) が定値関数になる場合（すなわち \\(\\{Y_n\\}\\) が \\(X_n\\) の値に依らずに独立同分布に定まる場合），これを 独立 \\(K\\)-付印 という．\n\n\n\n\n\n\n例（降水量の時系列）\n\n\n\n\n\n雨がふるという事象が Poisson 点過程に従い，その際の降水量がランダムに決まるため，降水量の時系列は古くから独立付印 Poisson 過程としてモデル化されている (Todorovic and Yevjevich, 1969)．19\n\n\n\n\n\n\n\n\n\n例（待ち行列過程）\n\n\n\n\n\n顧客が特定のタイミング \\(T_n\\) でのみ到着するとした場合，その際の人数 \\(N_n\\) との組 \\((T_n,N_n)\\) は付印 Poisson 過程としてモデル化できる (Brémaud, 1981, pp. 233 例3)．\n\n\n\n\n\n\n\n\n\n例（ニューロン発火の時系列）\n\n\n\n\n\n(R. Shibue and Komaki, 2017), (F. Shibue Ryohei AND Komaki, 2020) にて，ニューロン集団の発火特性の推定に応用されている．\n\n\n\n\n\n\n\n\n\n命題（一様 Poisson 過程の独立付印過程の特徴付け）\n\n\n\n\\[\n\\xi=\\sum_{n=1}^\\infty1_{\\left\\{T_n&lt;\\infty\\right\\}}\\delta_{(T_n,Y_n)}\n\\] を \\(\\mathbb{R}_+\\times E\\) 上の点過程， \\[\n\\eta=\\sum_{n=1}^\\infty1_{\\left\\{T_n&lt;\\infty\\right\\}}\\delta_{T_n}\n\\] を \\(\\mathbb{R}_+\\) 上の点過程とする．\n\\(\\gamma&gt;0\\) と \\(\\mu\\in\\mathcal{P}(E)\\) について，次は同値：\n\n\\(\\xi\\) はレート \\(\\gamma\\) を持つ一様 Poisson 過程 \\(\\eta\\) の \\(\\mu\\)-独立付印である．\n\\(T_1,Y_1,T_2-T_1,Y_2,\\cdots\\) は独立で，\\(T_n-T_{n-1}\\sim\\operatorname{Exp}(\\gamma)\\) かつ \\(Y_n\\overset{\\text{i.i.d.}}{\\sim}\\mu\\) である．\n\n\n\n\n\n1.7 剪定\n\n\n\n\n\n\n定義 (\\(p\\)-thinning)20\n\n\n\n\\(p:E\\to[0,1]\\) を可測関数とし，確率核 \\(K:E\\to2\\) を次で定める： \\[\nK_p(x,-):=\\biggr(1-p(x)\\biggl)\\delta_0+p(x)\\delta_1.\n\\]\n\\(E\\) 上の真の点過程 \\(X\\) の，この確率核に関する \\(E\\times2\\) 上の \\(K\\)-印付き過程 \\(M\\) に対して， \\[\n(\\omega,B)\\mapsto M(\\omega,B\\times\\{1\\})\n\\] で定まる \\(E\\) 上の真の点過程を \\(X\\) の \\(p\\)-剪定 という．\n\n\nすなわち，点 \\(x\\in E\\) に定まる所定の確率 \\(p(x)\\) に関して，確率 \\(p(X_n)\\) で点 \\(X_n\\) を脱落させて得る点過程を，\\(p\\)-剪定という．\n\\(p\\)-剪定は強度 \\(p(x)\\lambda(dx)\\) を持つ Poisson 過程となる．加えて，\\(1-p\\)-剪定と違いに独立になる．21\n\n\n\n\n\n\n例（非一様な Poisson 過程のシミュレーション）\n\n\n\n\n\nこの剪定によって，連続なレート関数 \\(\\lambda\\) を持つ非一様な Poisson 過程をシミュレーションする方法が (Lewis and Shedler, 1979) で提案され，(Ogata, 1981) が一般の点過程に拡張した．\n実際，yuimaでもこの方法が採用されている（第 1.9.2 節）．\nアルゴリズムは次のとおりである：\n\n\\(\\lambda\\le\\lambda^*\\) を満たす２つのレート関数を持つ Poisson 過程 \\(\\eta,\\eta^*\\) を考える．\\(\\eta^*([0,x_0])\\) 内の点 \\[\nX_1^*,X_2^*,\\cdots,X_{\\eta^*([0,x_0])}^*\n\\] を生成し，それぞれの点を確率 \\(1-\\frac{\\lambda(X_i^*)}{\\lambda^*(X_i^*)}\\) で取り除く．すると，残った点は \\[\n\\frac{\\lambda(X_i^*)}{\\lambda^*(X_i^*)}\\lambda^*(dx)=\\lambda(dx)\n\\] をレート関数にもつ Poisson 過程の分布，すなわち \\(\\eta\\) の分布に従う．\n\n最も簡単な場合としては，\\(\\lambda^*:=\\max_{t\\in[0,x_0]}\\lambda(t)\\) などと取れば良いが，\\(\\lambda\\) が変動が激しい関数である場合，より効率が良い方法があるかもしれない．\nすると，定値なレート関数を持つ Poisson 過程 \\(\\eta^*\\) は，指数分布に従う確率変数をシミュレーションすることなどによって得られる（命題 1.5.2）．\n\n\n\n\n\n1.8 Poisson 過程に関する積分\n\n1.8.1 直接の積分\n\n\n\n\n\n\n命題（Poisson 点過程との積分が定まるための条件）22\n\n\n\n\\((E,\\mathcal{E},\\lambda)\\) を \\(\\sigma\\)-有限な測度空間，\\(\\eta\\) を \\(E\\) 上の強度測度 \\(\\lambda\\) を持つ Poisson 点過程とする．\nこのとき，\\(f\\in\\mathcal{L}(E)_+\\) について，次は同値：\n\n\\(\\operatorname{P}[(\\eta|f)&lt;\\infty]=1\\)．\n次が成り立つ： \\[\n\\int_E(f\\land1)d\\lambda&lt;\\infty.\n\\]\n\nまた，これらの条件が成り立たないとき，\\(\\operatorname{P}[(\\eta|f)=\\infty]=1\\) が成り立つ．\n\n\n従って，\\(f\\in\\mathcal{L}^1(\\lambda)\\) に関して，\\((\\eta|f)\\) は殆ど確実に有限になる．\n\n\n1.8.2 補過程に関する積分\n加えて，中心化された積分 \\[\nI(f):=(\\eta|f)-(\\lambda|f)\\in L^2(\\operatorname{P})\n\\] は \\(L^1(\\lambda)\\cap L^2(\\lambda)\\) 上の等長作用素で，\\(L^2(\\lambda)\\) 上に有界延長する．23\n\n\n1.8.3 Poisson 積分の分布\n\n\n\n\n\n\n命題（Poisson 積分の分布）24\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間，\\(\\eta\\) を有限な強度測度 \\(\\lambda\\) を持つ Poisson 点過程とする．このとき，\\(\\eta\\) に関する \\(h\\in\\mathcal{L}^1(\\lambda;\\mathbb{R}^d)\\) の積分 \\[\n(\\eta|h)(\\omega)=\\int_Eh(z)\\eta(\\omega,dz)\n\\] の第二キュムラント母関数は，次のように表される： \\[\\begin{align*}\n    \\psi(u)&=\\int_E\\biggr(e^{i(u|h(z))}-1\\biggl)\\lambda(dz)\\\\\n    &=\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|x)}-1\\biggl)(h_*\\lambda)(dx).\n\\end{align*}\\] すなわち，\\((\\eta|h)\\) は複合 Poisson 分布 \\(\\mathrm{CP}(\\|h_*\\lambda\\|_\\mathrm{TV},\\|h_*\\lambda\\|_\\mathrm{TV}^{-1}h_*\\lambda)\\) に従う，平均 \\((\\lambda|h)\\)，分散 \\((\\lambda|h^2)\\) の確率変数である．\n\n\n\n\n\n1.9 Poisson 計数過程のシミュレーション\nyuimaパッケージでは，Poisson 計数過程は複合 Poisson 計数過程の特別な場合として扱うため，シミュレーション法は第 2.5 節で扱い，ここでは結果のみを示す．\n\n1.9.1 一様な Poisson 計数過程\n強度 \\(\\lambda&gt;0\\) を持つ（一様な） Poisson 計数過程とは，\\(\\mathbb{R}_+\\) 上のレート \\(\\lambda&gt;0\\) を持つ一様な Poisson 点過程（第 1.5.2 節）\\(\\eta\\) に対して， \\[\nN_t(\\omega):=\\eta(\\omega,[0,t])\n\\] で定まる Lévy 過程である．\nレート \\(\\lambda&gt;0\\) はジャンプの頻度を表している：\n\n\n\n\n\n\n\n\n\n\n\n1.9.2 非一様な Poisson 計数過程\n強度関数 \\(\\lambda:\\mathbb{R}_+\\to\\mathbb{R}^+\\) を持つ 非一様な Poisson 計数過程 とは，全く同様な定義 \\[\nN_t(\\omega):=\\eta(\\omega,[0,t])\n\\] をし，ただ \\(\\eta\\) の強度測度を \\(\\lambda(t)\\ell_+(dt)\\) に置き換えたものである．\n例えば，強度関数 \\[\n\\lambda(t)=10e^{-\\frac{t}{5}}\n\\] を持つ非一様な Poisson 過程は次のような見本道を持つ：\n\n\n\n\n\n\n\n\n\n時間が経つごとに強度関数 \\(\\lambda\\) の値（赤線）が小さくなり，それに伴ってジャンプの頻度が減少していくことがわかる．"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html#複合-poisson-過程",
    "href": "posts/2024/Process/Poisson.html#複合-poisson-過程",
    "title": "Poisson 過程を見てみよう",
    "section": "2 複合 Poisson 過程",
    "text": "2 複合 Poisson 過程\n\n2.1 はじめに\n点過程としての複合 Poisson 過程は，印付けられた Poisson 点過程（第 1.6 節）から構成される．\n\n2.1.1 計数過程として\nPoisson 過程 \\(N\\) は，大きさ１の跳躍のみで増加する Lévy 過程として特徴付けられるのであった．\nこの跳躍の大きさを任意の確率分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R})\\) に従ったものに変更したもの \\[\\begin{align*}\n    M_t&:=\\sum_{n=1}^{N_t}Y_n\\\\\n    &=\\sum_{k=1}^\\kappa Y_k1_{\\left\\{X_k\\le t\\right\\}}\n\\end{align*}\\] が 複合 Poisson （計数）過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) である．25\n\\(\\mu=\\delta_1\\) の場合が Poisson 過程に当たる．26\n\n\n\n\n\n\n注（従属操作による見方）27\n\n\n\n\n\n\\(Y_n\\overset{\\text{i.i.d.}}{\\sim}\\mu\\) について，これが定めるランダムウォーク \\[\nS_n:=\\sum_{k=1}^nY_k\n\\] を 再起過程 (renewal process) という．28\n\\(\\mu=\\operatorname{Exp}(\\lambda)\\) の場合の再起過程が定める計数過程 \\[\nN_t:=\\sum_{n=0}^\\infty1_{[0,t]}(S_n)\n\\] が Poisson 過程なのであった．\nこれに対して，\\(N_t\\) を従属過程とした従属 \\(M_t:=S_{N_t}\\) が複合 Poisson 過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) と理解できる．\n\n\n\n\n\n2.1.2 点過程として\nこの複合 Poisson 過程は，印付けられた Poisson 過程 \\[\n\\eta:=\\sum_{n=1}^\\kappa\\delta_{(X_n,Y_n)}\n\\] が \\(E\\times\\mathbb{R}\\) 上の強度測度 \\(\\mu\\otimes\\lambda\\ell\\) を持つ Poisson 点過程であり，各 \\(X_n\\) に \\(Y_n\\) の重みをつけて足し合わせた点過程 \\[\n\\xi(\\omega,B):=\\int_{B\\times\\mathbb{R}}r\\,\\eta(\\omega,dydr)\n\\] が基になっている．\nこれが複合 Poisson 点過程であり，\\(\\xi\\) に関する積分として \\(M\\) が理解できる（第 1.8.1 節参照）： \\[\nM_t(\\omega)=\\xi(\\omega,[0,t])=\\int^t_0\\xi(\\omega,ds).\n\\]\n\n\n\n\n\n\n注（Lévy 過程としての複合 Poisson 過程）29\n\n\n\n\n\n一様 Poisson 過程は定数の Lévy 測度 \\(\\lambda&gt;0\\) を持つ，特性量 \\((0,0,\\lambda\\delta_1)\\) が定める Lévy 過程である．\n一方で，一様な複合 Poisson 過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) は特性量 \\((0,0,\\lambda\\mu)\\) を持つ Lévy 過程である．\n従って，Lévy 測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}\\) はジャンプの頻度，\\(\\|\\nu\\|_\\mathrm{TV}^{-1}\\nu\\) がジャンプの分布を表すと言える．\nしかし，一般に Lévy 測度は，\\(\\left\\{z\\in\\mathbb{R}^d\\mid\\lvert z\\rvert&gt;\\epsilon\\right\\}\\) 上では有限であっても，\\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上で有限とは限らない．\\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上でも有限である場合，跳躍部分は定義 2.4.1 の意味でも複合 Poisson 過程である．\nすなわち，有限時区間内では（殆ど確実に）有限回しかジャンプしない純粋跳躍 Lévy 過程は，全て一様な複合 Poisson 計数過程である．30\n一般の Lévy 測度を持つ Lévy 過程は，一様な複合 Poisson 部分を跳躍部分にもつ Lévy 過程の，広義 \\(\\mathbb{H}^2\\)-収束極限として表される．31\n\n\n\n\n\n2.1.3 複合 Poisson 点過程の普遍性\n違いに素な \\(A,B\\in\\mathcal{E}\\) に対して \\(\\xi(A)\\perp\\!\\!\\!\\perp\\xi(B)\\) を満たすようなランダム測度 \\(\\xi\\) は，固定した原子を持たないならば，ある決定論的な測度と複合 Poisson 点過程の和として表せる．32\n\n\n2.1.4 点過程と計数過程の峻別\nここでも \\(\\xi\\) と \\(M\\) は全く異なる数学的対称であり，区別を要する．\n特に，\\(\\xi\\) は確率核，\\(M\\) は Lévy 過程である．\nしかし，Lévy 過程に関する確率積分を定義する際，２つの概念は密接に関連する．\n\n\n\n\n\n\nLévy 過程が駆動する確率積分\n\n\n\n\n\nというのも，\\(\\xi\\) の背後には，拡張された空間 \\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程 \\(\\eta\\) があり，\\(M\\) と \\(\\eta\\) が密接に関連するのである．\n\\(M\\) に関する確率積分は \\[\ndM_t=\\eta(dzdt)\n\\] と理解でき，\\(\\eta\\) のランダム測度としての理解が活躍する．\n\\(\\eta\\) の平均測度は \\(\\nu\\otimes\\ell_+\\) と表され，この \\(\\nu\\) を Lévy 測度という．\nすなわち，Lévy 過程に関する確率積分とは，Lévy 過程に付随する Poisson 点過程に関する確率積分に他ならない．\n\n\n\n\n\n\n2.2 複合 Poisson 点過程の定義\n\n2.2.1 一般的な定義\n\n\n\n\n\n\n定義 (compound Poisson process)33\n\n\n\n\\((E,\\mathcal{E})\\) を可測空間とする．\\(E\\) 上の 複合 Poisson 分布 とは，\\(E\\times(\\mathbb{R}\\setminus\\{0\\})\\) 上の Poisson 過程 \\(\\eta\\) を用いて \\[\n\\xi(\\omega,B)=\\int_{B\\times\\mathbb{R}\\setminus\\{0\\}}r\\,\\eta(\\omega,dydr),\n\\] \\[\nB\\in\\mathcal{E},\n\\] と表せる確率核 \\(\\xi:\\Omega\\to E\\) をいう．\n\n\n\\(E\\times\\mathbb{R}\\setminus\\{0\\}\\) 上の Poisson 点過程 \\(\\eta\\) の第二成分が，\\(E\\) 上のランダム点の「重み」のようなもので，\\(\\xi\\) は重みをつけて点を積分したものと理解できる．これは一般化された Poisson 複合の手続きに思える．\n\n\n\n\n\n\n\\(\\mathbb{R}^d\\) 上の複合 Poisson 分布\n\n\n\n\n\n上述の定義を，\\(\\mathbb{R}^d\\) 上の分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の Poisson 複合 \\(\\mathrm{CP}(\\lambda,\\mu)\\in\\mathcal{P}(\\mathbb{R}^d)\\) と比較してみよう．\nこれは \\[\nX:=\\sum_{n=1}^NZ_n,\\qquad Z_n\\overset{\\text{i.i.d.}}{\\sim}\\mu,N\\sim\\mathrm{Pois}(\\lambda)\n\\] で定まる \\(X\\) の分布であり，特性関数は \\[\n\\varphi_X(u)=\\exp\\left(\\lambda\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|y)}-1\\biggl)\\mu(dy)\\right)\n\\] で与えられる．\n複合 Poisson 過程はこれを過程に一般化したものであり，Poisson 複合の部分は \\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程の積分として一般化されている．\n\n\n\n\n\n2.2.2 ジャンプ時刻とジャンプ幅が独立に決まる場合\n第 1.6 節で扱ったような，（一様とは限らない）Poisson 点過程の独立付印の場合が特に重要なクラスである．\nこれは，Lévy 過程のジャンプ測度が，このクラスの複合 Poisson 点過程になるためである．34\n\n\n\n\n\n\n命題 (\\(\\rho_0\\)-symmetric compound Poisson process with Lévy measure \\(\\nu\\))35\n\n\n\n\\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程 \\(\\eta\\) の強度測度が \\(\\lambda=\\rho_0\\otimes\\nu\\) で表せ，さらに次を満たすとする：\n\n\\(\\rho_0\\in\\mathcal{P}(E)\\) は \\(\\sigma\\)-有限．\n\\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) は \\[\n\\int_0^\\infty(r\\land1)\\nu(dr)&lt;\\infty\n\\] を満たす．\n\nこのとき，次が成り立つ：\n\n\\(\\xi\\) は \\(\\rho_0\\)-対称である：\\(\\rho_0(B)=\\rho_0(B')\\) ならば，\\(\\xi(B)\\overset{\\text{d}}{=}\\xi(B')\\)．\n任意の \\(\\epsilon&gt;0\\) について \\(\\nu([\\epsilon,\\infty))&lt;\\infty\\)．\n\n\n\n\n\n\n\n\n\n例（Lévy 過程の跳躍測度）36\n\n\n\n\n\nLévy 測度 \\(\\nu\\) を持つ Lévy 過程 \\((L_t)\\) を考える．\nこのとき \\(\\nu\\) は，\\(\\nu(\\{0\\})=0\\) で， \\[\n\\int_\\mathbb{R}(1\\land\\lvert z\\rvert^2)\\nu(dz)&lt;\\infty\n\\] を満たすとされる．\n\\[\n\\mathcal{A}_\\nu:=\\left\\{A\\in\\mathcal{B}(\\mathbb{R}\\setminus\\{0\\})\\mid \\nu(A)&lt;\\infty\\right\\}\n\\] に対して， \\[\nN([s,t]\\times A):=\\sum_{s\\le r\\le t}1_A(\\Delta L_r)\n\\] とすると，時刻 \\([s,t]\\) 間の，幅が \\(A\\in\\mathcal{A}_\\nu\\) に入るジャンプの数を表す確率変数となる．\nこれは \\(\\mathbb{R}_+\\times\\mathbb{R}\\setminus\\{0\\}\\) 上の Poisson 点過程に延長し，強度 \\(\\ell_+\\otimes\\nu\\) を持つ．\nすなわち，\\(N\\) は Lévy 測度 \\(\\nu\\) を持つ \\(\\ell_+\\)-対称な複合 Poisson 点過程である．この \\(N\\) を 跳躍測度 (jump measure) ともいう．37\nLévy 測度 \\(\\nu\\) は，単位時間あたり（\\(\\ell_+\\) で測って測度 \\(1\\) の集合あたり）の平均ジャンプ数を表す： \\[\n\\nu(A)=\\operatorname{E}[N([0,1]\\times A)].\n\\]\nなお，一般の加法過程（非一様 Lévy 過程）に対しても，同様に跳躍測度は \\((\\mathbb{R}^d\\setminus\\{0\\})\\times\\mathbb{R}^+\\) 上の Poisson 点過程になる．38\n\n\n\n\n\n\n2.3 複合 Poisson 点過程に関する積分\n\\(E\\) 上の複合 Poisson 点過程 \\(\\xi\\) とは，\\(E\\times\\mathbb{R}^+\\) 上の Poisson 点過程 \\(\\eta\\) に他ならないため，Poisson 点過程に関する積分を通じて，\\(f\\in\\mathcal{L}(E)\\) に関して \\[\n\\int_Ef(z)\\,\\xi(dz)=\\int_Erf(y)\\,\\eta(dydr)\n\\] と定義できる．39\nCampbell の定理 1.3 より，\\((r,z)\\mapsto rf(z)\\) が \\(\\eta\\) の強度測度 \\(\\lambda\\) に関して可積分ならば，右辺は可積分であるから，\\((\\xi|f)\\) も可積分な確率変数を定める．\n\n\n\n\n\n\n例（Lévy 過程の Lévy-Itô 分解）40\n\n\n\n\n\nLévy 過程（一般に加法過程）は，跳躍部分と連続部分との独立和に分解でき，これが Lévy-Itô 分解である．\n跳躍部分は，複合 Poisson 点過程 \\(\\xi\\) （跳躍測度）に関する積分として表せる．\nしかし，Lévy 測度は \\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上で有限であるのみで，\\(0\\) の近傍で発散する場合がある．これにより，小さなジャンプを繰り返し，純粋なジャンプの和は発散することがある．そのために，\\(0\\) の近傍での跳躍部分については，連続部分から項を借りて，中心化された Poisson 積分（第 1.8 節）に関して表示する必要がある．41\nこれを実際に見てみよう．\\(L\\) を特性量 \\((\\beta,\\alpha^2,\\nu)\\) を持つ Lévy 過程，\\(\\eta\\) を跳躍測度とする．\n\n強度測度 \\(\\lambda\\) について， \\[\n   \\int_{\\mathbb{R}^d}z\\,\\lambda(dz)\n   \\] は \\(0\\) の近傍で発散し得るため，単に \\[\n   \\int^t_0\\int_{\\mathbb{R}^d}z\\,\\eta(dsdz)\n   \\] として跳躍部分を表そうとしても，well-defined とは限らない．\nしかし Lévy 測度は，\\(B^d\\subset\\mathbb{R}^d\\) を単位閉球として， \\[\n     \\int_{B^d}z^2\\,\\nu(dz)&lt;\\infty\n     \\] は満たすから，中心化された Poisson 積分（第 1.8.2 節） \\[\n     \\int^t_0\\int_{B^d}z\\,\\widehat{\\eta}(dsdz)&lt;\\infty\n     \\] は定義され，\\(\\mathcal{L}^2(\\operatorname{P})\\) の元である：補過程に関する Poisson 積分の等長性より，42 \\[\\begin{align*}\n         \\operatorname{E}\\left[\\left(\\int^t_0\\int_{B^d}z\\,\\widehat{\\eta}(dsdz)\\right)^2\\right]&=\\int^t_0\\int_{B^d}z^2\\,dt\\lambda(dz)\\\\\n         &=t\\int_{B^d}z^2\\,\\lambda(dz)&lt;\\infty.\n     \\end{align*}\\]\n一方で \\(\\mathbb{R}^d\\setminus B^d\\) 上では， \\[\n   \\int_{\\mathbb{R}^d\\setminus B^d}(z\\land1)\\,\\nu(dz)=\\nu(\\mathbb{R}^d\\setminus B^d)&lt;\\infty\n   \\] であるから，単に \\(\\eta\\) に関して積分しただけでも，殆ど確実に有限な値を持つ（第 1.8.1 節）．\n以上より，任意の Lévy 過程 \\(L\\) は次の分解として常に表示できる： \\[\n     L_t=\\beta t+\\alpha B_t+\\int^t_0\\int_{B^d}z\\widehat{\\eta}(dsdz)+\\int^t_0\\int_{\\left\\{\\lvert z\\rvert&gt;1\\right\\}}z\\eta(dsdz).\n     \\] このとき，\\(B\\perp\\!\\!\\!\\perp N\\) であり，最後の項は有限和としても表せる： \\[\n     \\int^t_0\\int_{\\mathbb{R}^d\\setminus B^d}zN(dsdz)=\\sum_{0\\le r\\le t}\\Delta L_r1_{\\mathbb{R}^d\\setminus B^d}(\\Delta L_r).\n     \\] この項が有限であることは，Lévy 測度 \\(\\nu\\) が \\(\\mathbb{R}^d\\setminus B^d\\) 上で有限であることによる．\n追加で Lévy 測度 \\(\\nu\\) が \\[\n   \\int_{B^d}\\lvert z\\rvert\\,\\nu(dz)&lt;\\infty\n   \\] も満たすような Lévy 過程（一般には加法過程）\\(L\\) については，跳躍部分は \\[\n   L^{\\text{jump}}_t:=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}z\\,\\eta(dsdz)\n   \\] と表せ，\\(L-L^{\\text{jump}}\\) は連続過程で，\\(L^{\\text{jump}}\\) と独立である．43 このような分解ができるとき，\\(L^{\\text{jump}}\\) を 跳躍部分，残りを 連続部分 という．\n\nすなわち，Lévy 過程の中には，無数の小さな跳躍部分が連続部分が相殺しているために収束しているものがあり，そのような場合は明確に跳躍部分と連続部分に分離できないものがある．\n\n\n\n\n\n\n\n\n\n例（ショット雑音）44\n\n\n\n\n\nある \\(\\widetilde{k}\\in\\mathcal{L}^1(\\ell_d)\\) が定める核関数 \\[\nk(x,y):=\\widetilde{k}(x-y)\n\\] と，ある平均を持つ測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R})\\) が定める強度測度 \\(\\lambda:=\\ell_d\\otimes\\nu\\) を持つ \\(\\mathbb{R}^d\\times\\mathbb{R}\\) 上の Poisson 点過程 \\(\\eta\\) に関して， \\[\nY_x:=\\int_{\\mathbb{R}^{d}\\times\\mathbb{R}}r\\widetilde{k}(x-y)\\eta(dydr)\n\\] で定まる確率場 \\(\\{Y_x\\}_{x\\in\\mathbb{R}^d}\\) を，Poisson 過程 \\(\\eta\\) が駆動する ショット雑音 という．\n\n\n\n\n\n2.4 複合 Poisson 計数過程の定義\n単に複合 Poisson 過程といった場合，通常，ここでいう一様な複合 Poisson 計数過程を指すことが多い．\n\n2.4.1 一様な場合\n\n\n\n\n\n\n定義 (compound Poisson processes)45\n\n\n\n\\(\\lambda&gt;0\\) を正数，\\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) を確率分布とする．一様な 複合 Poisson 過程 \\(\\mathrm{CP}(\\lambda,\\mu)\\) とは，強度 \\(\\lambda\\) を持つ（一様な） Poisson 過程 \\(N\\) が定める Lévy 過程 \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) をいう： \\[\nX_t:=\\sum_{n=1}^{N_t}Y_n,\n\\] \\[\nY_n\\overset{\\text{i.i.d.}}{\\sim}\\mu.\n\\]\n\n\nこの \\(X_t\\) は Lévy 過程になっており，\\(X_t\\) の特性関数は \\[\n\\varphi(u)=\\exp\\left(\\lambda t\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|y)}-1\\biggl)\\mu(dy)\\right)\n\\] で表される．46 すなわち，\\(X_t\\) は複合 Poisson 分布 \\(\\mathrm{CP}(\\lambda t,\\mu)\\) に従う．\n\n\n2.4.2 非一様・非有限な場合\nすなわち，複合 Poisson 分布が一様であるとは，背後にある \\(\\mathbb{R}^d\\times\\mathbb{R}_+\\) 上の Poisson 点過程が，\\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程（第 1.5.2 節）の独立付印過程になっていることをいう（第 1.6 節）．\nさらに，Lévy 測度が \\(\\lambda\\mu\\) という有限測度になるという重要な仮定も含まれている．\n一方で，一般の Lévy 分布の跳躍測度は，\\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程に基づくとは限らない（第 2.2.2 節）上に，\\(\\mathbb{R}^d\\setminus\\{0\\}\\) 上有限とも限らない．\nこのような場合でも，一般の複合 Poisson 点過程 \\(\\xi\\) を通じて，複合 Poisson 計数過程が \\[\nM_t(\\omega):=\\xi(\\omega,[0,t])\n\\] と定義できる．\n\\(\\xi\\) が \\(\\mathbb{R}_+\\times\\mathbb{R}^d\\) 上の Poisson 点過程 \\(\\eta\\) が定めるものであるとすると， \\[\nM_t(\\omega)=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}r\\,\\eta(\\omega,dsdr)\n\\] と表せる．\n\n\n\n2.5 シミュレーション\n\n2.5.1 複合 Poisson 過程のシミュレーション\nyuimaパッケージには，複合 Poisson 分布専用のコンストラクタsetPoissonが用意されている．このコンストラクタは２つの引数を持つ：\n\n\n\n\n\n\nsetPoissonの引数47\n\n\n\n\nintensity：強度（関数） \\(\\lambda&gt;0\\) として用いる変数名を文字列で指定する．\ndf：跳躍測度 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) として用いる分布名を，文字列のリストで指定する．\nscale=1：\\(\\|\\nu\\|_\\mathrm{TV}\\) の値のこと．\ndimension=1：\\(d\\) の値．\n\n\n\n\n\n\n\n\n\n例（一様 Poisson 過程の例）48\n\n\n\n\n\nPoisson 過程はジャンプの大きさが定数１の複合 Poisson 過程であるから，dfとしては \\(\\delta_1\\) を意味するdconst(z,1)を指定する．\n\nlibrary(yuima)\n\nmod1 &lt;- setPoisson(intensity=\"lambda\", df=list(\"dconst(z,1)\"))\n\n\nTerminal &lt;- 30\nsamp &lt;- setSampling(Terminal=Terminal, n=3000)\nset.seed(123)\npoisson1 &lt;- simulate(mod1, true.par=list(lambda=1), sampling=samp)\n\n\nplot(poisson1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n例（正規分布に従う跳躍を持つ複合 Poisson 過程）49\n\n\n\n\n\ndfとして正規分布族dnorm(z,mu,sigma)を指定すれば良い．\n\nmod2 &lt;- setPoisson(intensity=\"lambda\", df=list(\"dnorm(z,mu,sigma)\"))\npoisson2 &lt;- simulate(mod2, sampling=samp, true.par=list(lambda=1, mu=0, sigma=2))\n\n跳躍分布が \\(\\mu=\\operatorname{N}_1(0,2)\\) である場合の複合 Poisson 過程をシミュレーションは次のとおり：\n\nplot(poisson2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 複合 Poisson 過程が駆動する Lévy 過程のシミュレーション\nこの場合はsetModel内でモデルを定義する．\n\n\n\n\n\n\nsetModelの引数50\n\n\n\n\njump.coeff：\\(dM_t\\) 項の係数を，文字列のベクトルで指定する．\nmeasure.type：CPと指定することで，measure引数が複合 Poisson 過程 \\(M\\) に付随する Poisson 点過程 \\(\\eta\\) の強度測度と解釈される．\nmeasure：\\(\\eta\\) の強度測度 \\(\\lambda\\otimes\\nu\\) を，intensityとdfのリストとして指定する．\n\n\n\n\n\n\n\n\n\n例（複合 Poisson 跳躍を持つ Lévy 過程）51\n\n\n\n\n\n複合 Poisson 過程 \\(M\\sim\\mathrm{CP}(\\lambda,\\operatorname{N}(2,0.1))\\) が定める SDE \\[\ndX_t=-\\theta X_tdt+\\sigma dB_t+\\left(\\gamma+\\frac{X_{t-}}{\\sqrt{1+X^2_{t-}}}\\right)dM_t\n\\] \\[\nX_0=0\n\\] は次のように定義する：\n\nmodJump &lt;- setModel(drift = c(\"-theta*x\"), diffusion = \"sigma\", jump.coeff=c(\"gamma+x/sqrt(1+x^2)\"), measure = list(intensity=\"lambda\", df=list(\"dnorm(z, -3, 1)\")), measure.type=\"CP\", solve.variable=\"x\")\n\nsamp &lt;- setSampling(n=10000,Terminal=10)\n\nX &lt;- simulate(modJump, xinit=2, sampling=samp, true.par= list(theta=2, sigma=0.5,gamma=0.3,lambda=0.5))\nplot(X)"
  },
  {
    "objectID": "posts/2024/Process/Poisson.html#footnotes",
    "href": "posts/2024/Process/Poisson.html#footnotes",
    "title": "Poisson 過程を見てみよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Nualart and Nualart, 2018, p. 159) 例9.1.3，(Revuz and Yor, 1999, p. 471) 定義XII.1.3 などは，Poisson 計数過程の方を Poisson 過程と呼んでおり，(Last_Penrose2017?) などは Poisson 点過程の方を Poisson 過程と呼んでいる．(Eberle, 2012, p. 18) では，ここでは点過程とランダム測度を混用しているが，この２つを使い分けている．(Vasdekis, 2021) も，ここでいう計数過程を点過程と呼んでいる．↩︎\n(Revuz and Yor, 1999, p. 472) 命題XII.1.4など．↩︎\n(Kallenberg, 2017, p. 49)，(Last_Penrose2017?) 定義2.1 に倣った．全く同じ概念を (伊藤清, 1991, p. 298) は 偶然配置 と呼ぶ．↩︎\n(Last_Penrose2017?) 系6.5 を参照．↩︎\n(Last_Penrose2017?) 定義13.5 に倣った．↩︎\n(Murphy, 2023, p. 696) 18.4.3 節も参照．↩︎\n(伊藤清, 1991, p. 298) に倣った．↩︎\n(Last_Penrose2017?) 命題2.7 に倣った．(Campbell, 1909) は元々真空管内の shot noise の研究をしていた．結果の一部は G. H. Hardy にもよるという．↩︎\n(Nualart and Nualart, 2018, p. 160) 定義9.2.1，(Sato, 2013, p. 119) 第4章19節，(Resnick, 2002, p. 303)，(Sato, 2013, p. 119) 定義19.1，(Last_Penrose2017?) 定義3.1 に倣った．↩︎\n(Revuz and Yor, 1999, p. 476) 命題 XII.1.12，(Last_Penrose2017?) 命題3.2 はより一般的な状況で示している．↩︎\n(Nualart and Nualart, 2018, p. 160) 定理9.2.2 では，\\(\\lambda\\) はアトムを持たないとし，\\(E\\) を Polish 空間として示している．(Last_Penrose2017?) 系3.7 はここよりも一般的な設定で示している．↩︎\n(Eberle, 2012, p. 19) 定理1.6も参照．↩︎\n逆に，指数分布のシミュレーションにより，一様分布の順序統計量が効率的にシミュレーションできる．このことは粒子フィルターにおけるリサンプリングに応用できる．(Chopin and Papaspiliopoulos, 2020, p. 113) 命題9.1も参照．↩︎\n\\(\\pi=\\delta_m\\) であるとき，\\(\\eta\\) を単に二項（点）過程という．これは \\(\\eta(B)\\sim\\mathrm{Bin}(m,\\mu(B))\\) が成り立つためである．(Last_Penrose2017?) 定義3.4も参照．↩︎\n(Last_Penrose2017?) 命題3.8も参照．↩︎\n(Last_Penrose2017?) 定理7.2参照．↩︎\n(Last_Penrose2017?) 定義5.3 に倣った．(Kingman, 1992, p. 55) 5.2節や (Brémaud, 1981, p. 233) も参照．↩︎\n例えば (Kingman, 1992, p. 55)，(Last_Penrose2017?) 命題5.5 を参照．↩︎\n日本語文献としては，(西村克己 and 江藤剛治, 1981) も参照．↩︎\n(Last_Penrose2017?) 定義5.7 に従った．↩︎\n(Last_Penrose2017?) 系5.9 参照．↩︎\n(Last_Penrose2017?) 命題12.1 参考．↩︎\n(Nualart and Nualart, 2018, p. 163) 9.3節，(Last_Penrose2017?) 命題12.4 など．↩︎\n(Sato, 2013, p. 123) 定理19.5 や (Eberle, 2012, p. 20) 定理1.7 は強度測度 \\(\\lambda\\) が有限な場合について述べている，(Nualart and Nualart, 2018, p. 164) は補過程に関する積分に関して述べており，そのために追加で\\(-iu\\int_Eh(z)\\lambda(dz)\\) の項を持つ．．↩︎\n(Nualart and Nualart, 2018, p. 159) 例9.1.4，(Last_Penrose2017?) 例15.5 など．↩︎\n(Applebaum, 2009, p. 50) も参照．↩︎\n(Iacus and Yoshida, 2018, p. 171) 4.8.1 節も参照．↩︎\n(Resnick, 2002, p. 174)，(Mitov and Omey, 2014, p. 1)，(Nummelin, 1984, p. 49) 定義4.2 などに倣った．↩︎\n(Nualart and Nualart, 2018, pp. 159–160) も参照．↩︎\n\\(\\left\\{z\\in\\mathbb{R}^d\\mid\\lvert z\\rvert&gt;\\epsilon\\right\\}\\) 上で有限でないと，これを Lévy 測度にもつ Lévy 過程は存在しない．(Eberle, 2012, p. 20) 定理1.7も参照．↩︎\n(Eberle, 2012, p. 26) 定理1.10 も参照．↩︎\n(Kingman, 1992, p. 79) 第８章，(Last_Penrose2017?) 第 15.3 節参照．↩︎\n(Last_Penrose2017?) に倣った．↩︎\n一様じゃない Lévy 過程，すなわち加法過程の場合は，\\(\\lambda((0,t]\\times B)=\\nu_t(B)\\) という関係にある．これはもはや独立付印ではないが，やはり印付きの Poisson 過程が肝心であることは変わらない．↩︎\n(Last_Penrose2017?) も参照，↩︎\n(Nualart and Nualart, 2018, p. 162) 例9.2.4 を参考．↩︎\n(Nualart and Nualart, 2018, p. 162) 例9.2.4 による用語法．\\(N\\) をランダム測度として跳躍測度と呼んでいる．↩︎\n(Sato, 2013, p. 120) 定理19.2(i) を参照．↩︎\n(Nualart and Nualart, 2018, p. 164) 例9.3.1，(Last_Penrose2017?) 15.4節なども参照．↩︎\n(Nualart and Nualart, 2018, p. 164) 例9.3.1，(Sato, 2013, p. 120) 定理19.2，(Protter, 2005, p. 31) 定理42 など参照．↩︎\n(Sato, 2013, p. 119) が指摘するように，まるで Cauchy の主値積分である．↩︎\nただし，\\(\\widehat{\\eta}([0,t]\\times A):=\\eta([0,t]\\times A)-t\\nu(A)\\) を中心化した Poisson 過程とした．これを Poisson 補過程 (compensated Poisson process) ともいう．↩︎\n(Sato, 2013, p. 121) 定理19.3 も参照．↩︎\n(Last_Penrose2017?) 例15.14 を参考にした．↩︎\n(Iacus and Yoshida, 2018, p. 137), p.158，(Sato, 2013, p. 18) 定理4.3，(Protter, 2005, p. 33) 例2，(Baudoin, 2014, p. 90) 演習3.44，(Applebaum, 2009, p. 49) 命題1.3.11 に倣った．↩︎\n(Sato, 2013, p. 20) 命題4.5など参照．↩︎\n(Iacus and Yoshida, 2018, p. 137) 参照．↩︎\n(Iacus and Yoshida, 2018, pp. 137–138) も参照．↩︎\n(Iacus and Yoshida, 2018, p. 139) も参照．↩︎\nLévy 過程が駆動する SDE モデルの定義方法は (Iacus and Yoshida, 2018, p. 191) 4.11.3 節を参照．↩︎\n(Iacus and Yoshida, 2018, p. 191) 参照．↩︎"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/SVD.html",
    "href": "posts/2024/FunctionalAnalysis/SVD.html",
    "title": "特異値分解",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/SVD.html#特異値分解",
    "href": "posts/2024/FunctionalAnalysis/SVD.html#特異値分解",
    "title": "特異値分解",
    "section": "1 特異値分解",
    "text": "1 特異値分解\n\n\n\n\n\n\n命題（特異値分解）1\n\n\n\n任意の行列 \\(A\\in M_{n,p}(\\mathbb{R}),r:=\\operatorname{rank}(A)\\) について，直交行列 \\(U\\in O_n(\\mathbb{R}),V\\in O_p(\\mathbb{R})\\) と非負実数 \\(\\sigma_1\\ge\\cdots\\ge\\sigma_r&gt;0\\) が存在して，次が成り立つ： \\[\nA=U\\Sigma V^\\top,\\qquad\\Sigma:=\\begin{bmatrix}D&O_{r,p-r}\\\\O_{n-r,r}&O_{n-r,p-r}\\end{bmatrix},\\quad D:=\\mathrm{diag}(\\sigma_1,\\cdots,\\sigma_r).\n\\] \\(D\\) を 特異値行列，の対角要素を 特異値 と呼ぶ．2\n\n\n(Sylvester, 1889) は特異値を正準乗数 (canonical multipliers) と呼んでいた．Sylvester は特異値分解を独立に再発見した一人で，歴史上最初の特異値分解は (Beltrami, 1873) が与えたものだとされている．より詳しい歴史については (Stewart, 1993) 参照．\n\n\n\n\n\n\n証明\n\n\n\n\n\n\nまず \\(v_1,\\cdots,v_r\\in\\mathbb{R}^n\\) を，\\(A^\\top A\\) の固有ベクトルからなる正規直交系として取る． このとき \\(v_1,\\cdots,v_r\\) は \\(\\mathrm{Im}\\,(A^\\top)\\) の像の基底である． \\(v_1,\\cdots,v_r\\) が \\(A^\\top A\\) の固有ベクトルであることが必要であることは， \\[\nA^\\top A=(U\\Sigma V^\\top)^\\top(U\\Sigma V^\\top)=V\\Sigma^\\top U^\\top U\\Sigma V^\\top=V(\\Sigma^\\top\\Sigma)V\n\\] となることからわかり，\\(\\sigma_1^2,\\cdots,\\sigma_r^2\\) が \\(A^\\top A\\) の固有値である． （従って \\(AA^\\top\\) の固有値でもある）．\n続いて，条件 \\(Av_i=\\sigma_iu_i\\;(i\\in[r])\\) によって，\\(u_1,\\cdots,u_r\\) を定める．するとこれらは直交し， \\(\\mathrm{Im}\\,(A)\\) の基底をなす．さらに，\\(AA^\\top\\) の固有ベクトルでもある．\nこのことは次のように示せる：\n\\[\\begin{align*}\nu_i^\\top u_j&=\\left(\\frac{Av_i}{\\sigma_i}\\right)^\\top\\left(\\frac{Av_j}{\\sigma_j}\\right)=\\frac{v_i^\\top A^\\top Av_j}{\\sigma_i\\sigma_j}=\\frac{\\sigma_j^2}{\\sigma_i\\sigma_j}v_i^\\top v_j=0,\\qquad i\\ne j.\n\\end{align*}\\]\n\\(\\langle u_1,\\cdots,u_r\\rangle\\subset\\mathrm{Im}\\,(A)\\) であることと，正規直交することから線型独立でもあり，これらが \\(\\mathrm{Im}\\,(A)\\) の基底であることがわかる． さらに，任意の \\(i\\in[r]\\) について， \\[\nAA^\\top u_i=\\frac{AA^\\top Au_i}{\\sigma_i}=\\sigma_iAv_i=\\sigma_i^2u_i,\\qquad i\\in[r].\n\\] なお，\\(Av_i=\\sigma_iu_i\\) が必要であることは，\\(v_1,\\cdots,v_r\\) の正規直交性から， \\[\nAv_i=\\biggr(u_1\\sigma_1v_1^\\top+\\cdots+u_r\\sigma_rv_r^\\top\\biggl)v_i=u_i\\sigma_i\n\\] からわかる．\n\\(v_1,\\cdots,v_r\\) の正規直交な延長であって，\\(v_{r+1},\\cdots,v_n\\) が \\(\\mathrm{Ker}\\;(A)\\) の基底になるもの，\\(u_1,\\cdots,u_r\\) の正規直交な延長であって，\\(u_{r+1},\\cdots,u_m\\) が \\(\\mathrm{Ker}\\;(A^\\top)\\) の基底になるものが取れる． これは，核と余像，像と余核が直交するためである． これについて，\\(A=U\\Sigma V^\\top\\) が成り立つ．\n\n\n\n\n\n\n\n\n\n\n系\n\n\n\n以上の証明から，次のこともわかる：\n\n\\(A\\) の特異値は，\\(A^\\top A\\) の固有値の正の平方根に等しい．\n\\(V\\) の列ベクトルは \\(A^\\top A\\) の固有ベクトルであり，\\(U\\) の列ベクトルは \\(AA^\\top\\) の固有ベクトルになる．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/SVD.html#低階数近似",
    "href": "posts/2024/FunctionalAnalysis/SVD.html#低階数近似",
    "title": "特異値分解",
    "section": "2 低階数近似",
    "text": "2 低階数近似\n\\((n,p)\\)-行列の全体 \\(M_{n,p}(\\mathbb{C})\\) は Hilbert-Schmidt 内積 \\[\n(B \\,|\\,A)_\\mathrm{HS}:=\\operatorname{Tr}(A^*B)=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\] に関して Hilbert 空間をなす．\\(M_{n,p}(\\mathbb{R})\\) はこの閉部分空間をなす．\n\n\n\n\n\n\n命題 (Eckart and Young, 1936)\n\n\n\n\\(A\\in M_{n,p}(\\mathbb{R})\\) を行列，\\(0\\le r\\le n\\lor p\\) を自然数とする．ランク \\(r\\) の行列 \\(\\widetilde{A}\\in M_{n,p}(\\mathbb{R})\\) のうち，\\(A\\) に最も近いものは \\[\n\\widetilde{A}:=U\\Sigma_{1:r}V^\\top=\\operatorname*{argmin}_{\\operatorname{rank}(\\widetilde{A})=r}\\|A-\\widetilde{A}\\|_\\mathrm{HS}\n\\] が与える．ただし，\\(\\Sigma_{1:r}=\\mathrm{diag}(\\sigma_1,\\cdots,\\sigma_r,0,\\cdots,0)\\) とした．\n\n\nまたこのときの残差は，残った特異値のうち最大のもの \\[\n\\|A-\\widetilde{A}\\|_\\mathrm{HS}=\\sigma_{r+1}\n\\] が与える．3"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/SVD.html#一般化逆行列",
    "href": "posts/2024/FunctionalAnalysis/SVD.html#一般化逆行列",
    "title": "特異値分解",
    "section": "3 一般化逆行列",
    "text": "3 一般化逆行列\n\n\n\n\n\n\n命題 (Moore, 1920)-(Penrose, 1955)4\n\n\n\n\\(A\\in M_{mn}(\\mathbb{R})\\) について，次の３条件を満たす行列 \\(A^+\\in M_{nm}(\\mathbb{R})\\) は一意的に定まる：\n\n反射型一般可逆行列：\\(AA^+A=A,A^+AA^+=A^+\\)\n最小ノルム型：\\(A^+A\\) は自己共役である：\\((A^+A)^\\top=A^+A\\)\n最小誤差型：\\(AA^+\\) も自己共役である：\\((AA^+)^\\top=AA^+\\)\n\nこれを Moore-Penrose の一般化逆行列 という．\n\n\n任意の行列 \\(A\\in M_{n,p}(\\mathbb{R})\\) の一般化逆行列は，直交行列 \\(V,U\\) で座標変換を施したところで逆行列を取り，これを再び \\(V,U\\) で変換し直したものに等しい：\n\n\n\n\n\n\n命題（一般化逆の特徴付け）5\n\n\n\n\\(A\\in M_{n,p}(\\mathbb{R})\\) の一般化逆行列は次のように表せる： \\[\nA^+=V\\Sigma^{-1}U^\\top,\\qquad \\Sigma^{-1}=\\begin{bmatrix}D^{-1}&O_{r,p-r}\\\\O_{n-r,r}&O_{n-r,p-r}\\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/SVD.html#関連ページ",
    "href": "posts/2024/FunctionalAnalysis/SVD.html#関連ページ",
    "title": "特異値分解",
    "section": "関連ページ",
    "text": "関連ページ\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/SVD.html#footnotes",
    "href": "posts/2024/FunctionalAnalysis/SVD.html#footnotes",
    "title": "特異値分解",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Eckart and Young, 1936, p. 213 Theorem 1), (Strang, 2016, p. 372) など．↩︎\n(Eckart and Young, 1936, p. 214) によると，以前はにより正準乗数 (canonical multipliers) と呼ばれていた．↩︎\n\\(r\\ge\\operatorname{rank}(A)\\) であるとき，\\(\\sigma_{r+1}=0\\) とする．(Strang, 2016, p. 394) も参照．↩︎\n(柳井晴夫，竹内啓, 1983) も参照．↩︎\n(柳井晴夫，竹内啓, 1983, p. 定理5.6), (Strang, 2016, p. 395) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html",
    "href": "posts/2024/Survey/BDA3.html",
    "title": "ベイズデータ解析７",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#関連記事",
    "href": "posts/2024/Survey/BDA3.html#関連記事",
    "title": "ベイズデータ解析７",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#はじめに",
    "href": "posts/2024/Survey/BDA3.html#はじめに",
    "title": "ベイズデータ解析７",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 階層モデル\nデータが自然な 階層構造 を持つとする．例えば標本がクラスター抽出された場合，パネルデータや経時的繰り返し観測による標本，共変量統制とマッチングなど，自然な階層構造を持つデータは多い．\nこのようなデータに対して，個々のサブグループに対して全く同じ回帰モデルを繰り返し適用し，結果のプロットを小窓に分割して並べることで，グループごとの効果の違いを比較することができる．\nこの方法はナイーブながらも有力で，(10.9 節 Gelman et al., 2020, p. 148) では “secret weapon” と呼んでいる．\nこの別々の回帰モデルはベイズ的に統合して推論することができる．このグループごとに変動する係数を許したモデルが（ベイズ）階層モデル である．\n\n\n1.2 いつ使うか？\n線型回帰の枠組みで交差項を入れることで，グループごとに緩やかに異なる回帰係数を許すことができる．\nさらには一般化線型モデルを用いることで，連続・離散データの簡単な非線型関係まで扱うことができる．\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nそれでも階層モデルに進む動機として次のような点が挙げられる：\n\n\n\n\n\n\n階層モデルの美点1\n\n\n\n\n説明変数が多すぎる場合は馬蹄事前分布などの構造を持った事前分布が使用可能である．しかしデータが階層構造を持つ場合は，その構造に関する事前知識を用いて，自然な事前分布を組織的に導入することができる．これが階層モデリングに他ならないとも見れる．その結果大量の説明変数がある場合でも，その関連度をモデルに知らせて適切に推定する方法にもなる2．\n単一のグループではデータ数が小さい場合に，グループレベルの回帰係数 \\(\\alpha_{j[i]}\\) を安定して推定する手法とも理解できる．他のグループの回帰モデルと組み合わせた大きな階層モデルをベイズすることで，確度が低い（が関連する）モデルも捨てずに推定に利用するため (partial pooling)，より好ましい統計的推定を実行できる．これは 縮小推定 のキーワードの下でも追及されており，階層モデルは自然な縮小推定が実行される典型的な設定である．3\nグループ毎の回帰係数の違いに興味がある場合，グループレベルの変動の分散 \\(\\sigma_j^2\\) を推定する最も自然なモデルである．実際，一階層の回帰分析は \\(\\sigma_j=0\\) の場合，グループ毎に別々の回帰分析を実行する場合は \\(\\sigma_j=\\infty\\) という（場合によっては非現実的な）仮定を置いた縮退した階層モデルと見れる．\n\n\n\n全体の係数のみに興味がある場合，グループレベルの変動やデータの階層構造は局外構造である．このような状況では，誤差が相関を持つ場合にも頑健な点推定手法として，一般化推定方程式法なども用いることができる．4"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#線型変動係数モデル",
    "href": "posts/2024/Survey/BDA3.html#線型変動係数モデル",
    "title": "ベイズデータ解析７",
    "section": "2 線型変動係数モデル",
    "text": "2 線型変動係数モデル\n\n2.1 変量効果モデル\n階層モデル \\[\ny_i=\\alpha_{j[i]}+\\beta x_i+\\epsilon_i,\\qquad\\epsilon_i\\sim\\operatorname{N}(0,\\sigma^2_y),\n\\tag{1}\\] \\[\n\\alpha_j=\\mu_\\alpha+\\eta_j,\\qquad\\eta_j\\sim\\operatorname{N}(0,\\sigma^2_\\alpha),\n\\tag{2}\\] は２つの別の階層にある回帰モデルが，１つに統合された形をしている．5\n(1), (2) は 変量効果モデル，または NER (Nest Error Regression) model (Battese et al., 1988) と呼ばれる．6\n\n\n2.2 中庸としての階層モデル\n全く等価だが \\(D_i\\) を \\(i\\) 番目の成分のみが \\(1\\) のベクトル，\\(\\alpha=(\\alpha_1,\\cdots,\\alpha_J)^\\top\\) として \\[\ny_i=D_i\\alpha+\\beta x_i+\\epsilon_i\n\\] と表すことができる．\nこうみると \\(J\\) 個のクラスそれぞれに関する指示変数 \\(D_1,\\cdots,D_J\\) で定数項を置き換えた単一の回帰モデルと見ることもできる．仮に複数の変動係数が存在しても共線型性が問題にならないのは，\\(\\alpha\\) の事前分布に構造が入っているためである．\n階層モデル (1), (2) において \\(\\sigma^2_\\alpha\\to0\\) の極限を考えると，グループ毎に変動がないという単一の線型回帰モデルに帰着する．\n他方で実は \\(\\sigma^2_\\alpha\\to\\infty\\) の極限を考えると，古典的な点推定の文脈では，\\(J\\) グループのそれぞれに別々の線型回帰を実行するというモデルに帰着する．\nこれは「グループ毎に全く別々で互いに関係がない」というモデルであり，真実は２つの中庸にあると思われる．\nグループ毎の変動と，その間の緩い関係性の双方を許し，１つのモデルに取り込んだものが 階層モデル である．グループレベルの変動 \\(\\sigma^2_\\alpha\\) を推定するための自然なモデルでもある．\n\n\n2.3 BLUE\n線型最良不偏推定量 (BLUE) は (Henderson, 1950) によって提案された，\\(\\beta,\\alpha_{j[i]}\\) の推定量である．\n\\(\\beta\\) の推定量は計量経済学の文脈で変量効果推定量と呼ばれる GLS 推定量に他ならない．7\n\\(\\alpha_{j[i]}\\) の BLUE は，モデル (1), (2) においては \\[\n\\widehat{\\alpha}_j=\\frac{\\frac{n_j}{\\alpha_y^2}}{\\frac{n_j}{\\alpha_y^2}+\\frac{1}{\\sigma^2_\\alpha}}(\\overline{y}_j-\\beta\\overline{x}_j)+\\frac{\\frac{1}{\\sigma^2_\\alpha}}{\\frac{n_j}{\\alpha_y^2}+\\frac{1}{\\sigma^2_\\alpha}}\\mu_\\alpha\n\\] と表せる．8\n\n\n\n2.4 分散成分モデル\n\n2.4.1 一般的な定義\n(Henderson, 1950) に始まる分散成分モデルは，一般的には \\[\ny_{ij}=\\beta x_i+\\alpha_{j_1[i]}+\\cdots+\\alpha_{j_k[i]}+\\epsilon_{ij}\n\\] \\[\n\\alpha_{j_l}\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,\\tau_l^2),\n\\] と表されるモデルと理解される (Sugasawa and Kubokawa, 2023, p. 73)．\\(k=1\\) の場合は (Fay and Herriot, 1979) モデルともいう．\n変量効果モデルにおいて誤差には完全な階層関係があったが，分散成分モデルはその non-nested な場合への拡張とみなせる．\n\n\n2.4.2 分散成分モデルの例\nこの場合データ \\(y_{ij}\\) の変動を成分ごとに分解しようという志向が，分散分析の発展と見れる．例えば \\[\ny_{ijk}=\\mu+\\mu_{1i}+\\mu_{2j}+\\epsilon_{ijk}\n\\] という形の場合は特に 二元分類モデル，\\(\\mu_{3ij}\\) も加えた場合は二元交差モデルと呼ばれる．\nさらにグループ階層のモデルに関して，分散成分モデルではグループレベル変数 \\(\\alpha_{j_l}\\) は互いに独立であるとしているが，空間モデルでは互いに相関を持つと仮定することもある．\nさらにこの階層は潜在変数の階層とみるとコピュラモデルや理想点モデルに近くなる．項目反応モデルは，個人毎の変動と項目毎の変動の，non-nested な２つの変動を持ったロジスティック階層モデル 3 になる．\n\n\n2.4.3 分散成分の点推定法\n変量効果モデルや分散成分モデルにも適用可能な，一般の階層モデルに適用可能な分散成分の推定法として，一般化推定方程式法がある．\nその特別な荷重を取った場合に REML (Restricted Maximum Likelihood) 推定量がある (2.3 節 Sugasawa and Kubokawa, 2023, p. 13)．\nこうして得た分散成分，特に分散比 \\(\\rho:=\\sigma^2_v/\\sigma^2_e\\) の推定量を得たあと，これを BLUE \\(\\widehat{\\beta}\\) に代入して得る２段階推定量を 経験 BLUE という (久保川達也, 2006, p. 143)．\nその縮小効果については次稿も参照：\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n2.5 ベイズ推定\nベイズ階層モデルの基礎は (Lindley and Smith, 1972) が敷いたと言われる．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#sec-multilevel-logistic",
    "href": "posts/2024/Survey/BDA3.html#sec-multilevel-logistic",
    "title": "ベイズデータ解析７",
    "section": "3 階層ロジスティックモデル",
    "text": "3 階層ロジスティックモデル\n\n3.1 項目応答モデル\n\n3.1.1 １母数ロジットモデル\n\\[\ng(x):=\\operatorname{logit}(x)=\\log\\frac{x}{1-x}\n\\] \\[\ng(\\mu_{i})=\\alpha_{j[i]}-\\beta_{k[i]},\\qquad\\mu_{i}=\\operatorname{P}[Y_{ik}=1],\n\\] というモデルを １母数応答モデル または (Rasch, 1960) モデルという．\nこのモデルは \\((\\alpha_j,\\beta_k)\\) 上の平行移動に関して識別不可能であるため，点推定の文脈では追加の制約条件が導入される．しかしベイズの文脈では不適な事前分布を避けることで自然に回避される．\n階層ベイズモデルでは，自然に関心が能力パラメータ \\(\\alpha_j\\) と難易度パラメータ \\(\\beta_k\\) の分散に向けられる： \\[\n\\alpha_j=\\mu_\\alpha+\\gamma_\\alpha X^\\alpha_j+\\epsilon_j,\\qquad\\epsilon_j\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,\\sigma^2_\\alpha),\n\\] \\[\n\\beta_k=\\mu_\\beta+X^\\beta_k+\\epsilon_k,\\qquad\\epsilon_k\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,\\sigma^2_\\beta).\n\\]\n\n\n3.1.2 ２母数ロジットモデル\nさらに項目毎の 識別力母数 (discrimination parameter) \\(\\gamma_k\\) を導入したモデル \\[\ng(\\mu_i)=\\gamma_{k[i]}\\biggr(\\alpha_{j[i]}-\\beta_{k[i]}\\biggl),\n\\] を ２母数ロジットモデル という．\n\n\n3.1.3 多次元の潜在空間\n読解力と数学力の別々の能力を要求するテストなどのデータに対して，多次元の潜在空間を持つモデルを考えたい．\n両方が要求される AND の論理の場合は \\[\n\\mu_i=g^{-1}\\biggr(\\gamma^{(1)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(1)}_{k[i]})\\biggl)g^{-1}\\biggr(\\gamma^{(2)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(2)}_{k[i]})\\biggl)\n\\] というモデルが考えられるかもしれない．OR の論理の各因数を \\(1-\\mu_i, 1-g^{-1}(-)\\) で置き換えれば良い．\n一方で潜在空間からの関数を設定しても良いだろう．最も直感的には能力値の和で \\[\n\\mu_i=g^{-1}\\biggr(\\gamma^{(1)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(1)}_{k[i]})+\\gamma^{(2)}_{k[i]}(\\alpha_{j[i]}-\\beta^{(2)}_{k[i]})\\biggl)\n\\] と表すものである．\n\n\n3.1.4 応答曲線\nリンク関数 \\(g\\) の選択は，潜在変数 \\(\\alpha_j,\\beta_k,\\gamma_k\\) の変化が応答確率の変化にどう関係するかを規定する．\n\\(g\\) のプロットは ICC (Item Characteristic Curve) または trace line と呼ばれる (Fox, 2010, p. 6)．\nこれは \\(g(0.5)\\) を中心に対称になっているが，この対称性が不適切な場合も多い．\n(Bafumi et al., 2005) などは \\[\n\\mu_i=\\pi_1+(1-\\pi_0-\\pi_1)g^{-1}\\biggr(\\gamma_k(\\alpha_j-\\beta_k)\\biggl)\n\\] として，最低限の \\(y_i=0,1\\) への反応確率 \\(\\pi_0,\\pi_1\\) をあらかじめ設定し，\\([\\pi_0,\\pi_1]\\subset[0,1]\\) の範囲だけに応答曲線をフィッティングすることを考えた．\n\\(\\pi_0,\\pi_1\\) も推定すべきパラメータとする．\n\n\n\n3.2 選択モデル\n\n3.2.1 はじめに\nロジスティックモデルは１次元の潜在空間を持つ 選択モデルと見ることができる．\nこれをさらに精緻化し，価値関数や効用関数などを通じてアクターの意思決定と行動をより詳細にモデリングすることも考え得る．\nこのようなモデルはミクロ計量経済学で歴史的に考えられたほか，政治科学における 理想点モデルはこのような選択モデルを通じて生まれた．\n\n\n3.2.2 ロジットモデルになる場合\n個人 \\(i\\in[N]\\) ごとのパラメータ \\(a_i,b_i,c_i\\) が存在して， \\[\n\\operatorname{P}[Y_i=1]=\\operatorname{P}[a_i&gt;b_i+c_iX_i]=\\operatorname{P}\\left[\\frac{a_i-b_i}{c_i}&gt;X_i\\right]\n\\] によって意思決定が決まるとした場合，\\(d_i:=\\frac{a_i-b_i}{c_i}\\) と潜在変数 \\(x_i\\) との大小によって応答が変わると要約できる．\nすると \\(d_i\\) の分布関数 \\(F_i\\) がロジスティックであるか正規分布であるか \\(t\\)-分布であるかに依って，ロジスティック回帰・プロビット回帰・ロビット回帰というモデルが得られる．\n\n\n3.2.3 Tobit モデル\nさらに \\(y_i\\) の観測値が \\(x_i\\) の値に依存するというモデルが (Tobin, 1958) によって提案されている： \\[\nY=Y^*\\lor0.\n\\] \\[\nY^*_i=\\beta X_i+\\epsilon_i,\n\\]\nこのモデルは（１型）トービットモデルの他に，打ち切り回帰 (censored regression) とも呼ばれる．\nある閾値を超えた場合にプライバシーの問題で公開を制限する top-coding データなどに用いられる．\n\n\n3.2.4 Heckman モデル\n(Heckman, 1979) は標本に入るかどうかのメカニズムをモデリングする，選択バイアスのためのモデル \\[\nY=\\begin{cases}Y^*&S=1\\\\\\text{missing}&S=0\\end{cases}\n\\] \\[\nY^*=\\beta X+\\epsilon,\n\\] \\[\nS=1_{\\mathbb{R}_+}(S^*),\\qquad S^*=\\gamma Z+\\eta.\n\\] を提案した．これは２型トービットモデルとも呼ばれる．\nそしてこの共分散構造 \\[\n\\begin{pmatrix}\\epsilon\\\\\\eta\\end{pmatrix}\\sim\\operatorname{N}\\biggr(0,\\begin{pmatrix}\\sigma^2_\\epsilon&\\sigma_{\\epsilon\\eta}\\\\\\sigma_{\\epsilon\\eta}&1\\end{pmatrix}\\biggl)\n\\] を局外母数として \\(\\beta\\) を一致推定する．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#文献紹介",
    "href": "posts/2024/Survey/BDA3.html#文献紹介",
    "title": "ベイズデータ解析７",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n(12 章 Gelman and Hill, 2006) が筆者の知る限り最も丁寧な階層モデルへの導入である．\n変動係数モデルに関して (Sugasawa and Kubokawa, 2023) が極めて見通しが良い．小地域推定を主なテーマとしている．\n項目応答モデルと理想点モデルは (14.3 Gelman and Hill, 2006) において一般化線型階層モデルの文脈で扱われている．\n選択モデルは (27 章 Hansen, 2022) を参照．"
  },
  {
    "objectID": "posts/2024/Survey/BDA3.html#footnotes",
    "href": "posts/2024/Survey/BDA3.html#footnotes",
    "title": "ベイズデータ解析７",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(11.5 節 Gelman and Hill, 2006, p. 246) も参照．↩︎\n一般に階層モデルは，全ての説明変数を組み込んだモデルから，特定の係数の部分集合を括り出し，その部分集合に super-population model を仮定したものと見れる．(13.6 節 Gelman and Hill, 2006, p. 296) の議論も参照．↩︎\n例えば小地域推定においては極めて自然なモデルである (久保川達也, 2006), (Sugasawa and Kubokawa, 2023) も参照．↩︎\nこの点については (11.6 節 Gelman and Hill, 2006, p. 248)，(Carlin et al., 2001) などを参照．↩︎\nこの第二階層の構造を \\(\\alpha_j\\sim\\operatorname{N}(\\mu_\\alpha,\\sigma^2_\\alpha)\\) という事前分布とみて，この未知パラメータの推定から始める点推定法を経験ベイズという (Sugasawa and Kubokawa, 2023, pp. 11–12)．↩︎\n(Sugasawa and Kubokawa, 2023) では，\\(\\beta\\) が \\(j[i]\\in[J]\\) 毎に異なるとき，これを変量係数モデルと呼んでいる．ここでは (Gelman, 2005) などに倣って，全てまとめて変動係数 (varying coefficient) モデルと呼ぶことにする．↩︎\n(Sugasawa and Kubokawa, 2023, p. 10) に GLS 推定量が BLUE であることの証明がある．変量効果推定量については (17.6 Hansen, 2022, pp. 601–603) を参照した．↩︎\n正規性の仮定の下では一般に経験ベイズ推定量と一致することが (Sugasawa and Kubokawa, 2023, pp. 11–12) で示されている．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html",
    "href": "posts/2024/Survey/Survey3.html",
    "title": "ベイズデータ解析３",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#関連記事",
    "href": "posts/2024/Survey/Survey3.html#関連記事",
    "title": "ベイズデータ解析３",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#有限標本論の概要",
    "href": "posts/2024/Survey/Survey3.html#有限標本論の概要",
    "title": "ベイズデータ解析３",
    "section": "1 有限標本論の概要",
    "text": "1 有限標本論の概要\n\n1.1 設定\n\\([N]\\) を母集団とする．\n\\([N]\\) の部分集合の全体 \\(P([N])\\) 上の確率分布を 抽出計画 (sampling design) といい，ある既知の抽出分布に従って得られる標本 \\(S\\subset[N]\\) を 確率標本 (probability sample) という．日本語では 無作為抽出標本 などとも呼ばれる．\n\n\n\n\n\n\n抽出計画の例\n\n\n\n\n\n抽出計画 \\(\\mathcal{L}[S]\\) には，\n\n単純無作為抽出 (SRS: Simple Random Sampling)\n系統無作為抽出 (Systematic Random Sampling)\n層別抽出 (Stratified Sampling)：母集団を層別し，各層の間では独立な抽出を行う．層ごとに抽出計画は異なっても良い．条件付きランダム化 (conditional randomization) ともいう (Section 2.2 Hernán and Robins, 2020, p. 17)．\nクラスター抽出 (Cluster Random Sampling)：クラスターがまずランダム抽出され，そのクラスター内の全構成員が標本に加わる．クラスターのことを PSU (Primary Sampling Unit) ともいう．クラスター内でもランダム抽出が行われた場合，２段階クラスターサンプリング という．\n\nなどの方法が存在する．\n例えば，日本の国勢調査は２段階の層別抽出である．\n\n\n\n確率標本 \\(S\\) では（１次の）包含確率 \\[\n\\pi_i:=\\operatorname{P}[i\\in S]=\\operatorname{P}[I=1],\\qquad I:=1_{i\\in S}.\n\\] が定まる．\n\n\n\n\n\n\n狭義の「確率標本」\n\n\n\n\n\n先ほど，\\(\\mathcal{P}([N])\\) 上の確率変数を確率標本と呼ぶとしたが，正確に \\(S\\) が 確率標本 と呼ばれるためには，\\(\\pi_i&gt;0\\) が母集団 \\(i\\in[N]\\) の全域で成り立つことが必要である (Kim, 2024, p. 12)．\n\n\n\n\n\n1.2 Horvitz-Thompson 推定量\n確率標本 \\(S\\in\\mathcal{L}(\\Omega;\\mathcal{P}([N]))\\) に対しては，ある量 \\(y\\) についての母集団の総和 \\[\nY:=\\sum_{i=1}^Ny_i\n\\] が \\[\n\\widehat{Y}_\\mathrm{HT}:=\\sum_{i\\in S}\\frac{y_i}{\\pi_i}\n\\] により不偏推定できる．\n\\(\\widehat{Y}_\\mathrm{HT}\\) は (Horvitz and Thompson, 1952) 推定量と呼ばれる．1\n\n\n\n\n\n\n(Sen, 1953)-(Yates and Grundy, 1953)\n\n\n\nHorvitz-Thompson 推定量の分散は次で与えられる：\n\\[\n\\mathrm{V}[\\widehat{Y}_\\mathrm{HT}]=\\sum_{i,j=1}^N\\biggr(\\pi_{ij}-\\pi_i\\pi_j\\biggl)\\frac{y_iy_j}{\\pi_i\\pi_j}.\n\\] ただし， \\[\n\\pi_{ij}:=\\operatorname{P}[i\\in S,j\\in S]=\\operatorname{P}[I=1=J].\n\\] を２次の包含確率という．\n\n\nHorvitz-Thompson 推定量の要点には，「計画した欠損ならば，重みづけによって不偏推定量を得ることができる」という点にある．\nそこで抽出計画が不明な場合もこれを推定し，バイアスを補正しようとするアプローチを傾向スコアの方法，または 擬似ランダム化 (pseudo-randomization) の方法という．\n\n\n1.3 「校正」：効率の改善に向けて\nHT 推定量は確率標本 \\(S\\) の分布，すなわち抽出計画に依らずに不偏性を持つ．\nこれを計画不偏性 (design-unbiasedness) というが，この性質を持つ線型な推定量は HT に限られる．\nしかし，HT 推定量はいつでも分散が最小というわけではない．\n計画不偏性は bias-variance trade-off の観点からは欠点でもあり，それゆえ抽出計画に関する情報を用いて分散を低減することも考えられる．\n特に，HT 推定量の荷重 \\((\\pi_i^{-1})\\) を，補助変数 \\(x_i\\) に関する 外部一致性 \\[\n\\sum_{i\\in S}w_ix_i=\\overline{x}\n\\] を保ちながら新しいもの \\((w_i)\\) に変更するものが多く考えられた．\n\n\n\n\n\n\n「外部一致性」の別名\n\n\n\n有限標本論は普遍的な統計推測の基礎であると言える．\n実際，この外部一致性の条件は多くの分野で考慮されており，種々の名前が与えられている．\n\n有限標本論：校正条件 (calibration condition / benchmarking property) （第 2.3 節）\n欠測データ解析：共変量バランシング (covariate balancing) (Imai and Ratkovic, 2014)\n機械学習（継続学習）：共変量シフト (covariate shift) (Shimodaira, 2000)\n\n\n\nこのアプローチを 荷重校正 (calibration weighting) という．\n次章にてこれ以降，種々の荷重校正推定量を紹介する．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#回帰推定量",
    "href": "posts/2024/Survey/Survey3.html#回帰推定量",
    "title": "ベイズデータ解析３",
    "section": "2 回帰推定量",
    "text": "2 回帰推定量\n\n2.1 はじめに\n前述の通り，補助変数 \\(x\\) が母集団上で知られている場合に，ここから抽出計画に対する追加情報を抽出して推定量に組み込むことで，計画的に欠測させられたデータ（＝確率標本）に対する不偏推定量 (Horvitz and Thompson, 1952) （以降 HT 推定量という）の効率を改善することを考える．\n以降，補助変数 \\(x\\in\\mathbb{R}^p\\) は母集団上で既知であるとし，その総和を \\[\nX:=\\sum_{i\\in[N]}x_i\n\\] で表す．\n\n\n2.2 比による校正\n補助変数の次元が \\(p=1\\) のとき，最も安直には \\(X\\) の HT 推定量から，真の値 \\(X\\) との「ズレ方」を用いて，\\(Y\\) の推定量を「校正」することができる．\n\\[\n\\widehat{Y}_{\\mathrm{R}}:=\\widehat{Y}_\\mathrm{HT}\\frac{X}{\\widehat{X}_\\mathrm{HT}}\n\\] とできるだろう．\nこの推定量は ratio estimator などと呼ばれ，性能の代わりにバイアスが生じてしまう．\n一般に，\\(X,Y\\) が正の相関を持つとき大きな分散低減が得られる (Deng and Wu, 1987), (Kim, 2024, p. 92)．\n\\(x_i=1\\) と取った場合を Hajék 推定量ともいう．Hajék 推定量が HT 推定量よりも推奨される状況が (Särndal et al., 1992, p. 182) にリストされている．\nこの推定量は昔は計算の簡単さから使われていたが，一般の次の回帰推定量の方が MSE が小さいことが知られている (Deng and Wu, 1987)．\n\n\n2.3 回帰推定量\n超母集団模型 \\[\nY=X^\\top\\beta+\\epsilon,\\qquad\\epsilon\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2)\n\\] を想定し，得られている標本のみから \\(\\widehat{\\beta}\\) を推定する．こうして得られる \\[\n\\widehat{y}_i:=x_i^\\top\\widehat{\\beta},\\qquad\\widehat{\\beta}:=\\left(\\sum_{i\\in S}\\pi_i^{-1}x_ix_i^\\top\\right)^{-1}\\sum_{i\\in S}\\pi_i^{-1}x_iy_i\n\\] の総和が，\\(Y\\) に対する 回帰推定量 (regression estimator) と呼ばれる．2\nこれは \\((y_i)\\in\\mathbb{R}^n\\) に関する線型推定量になっている．加えて，外部一致性 \\[\n\\sum_{i\\in S}w_ix_i=\\overline{x}\n\\tag{1}\\] を満たす荷重 \\[\nw_i:=\\overline{X}^\\top\\left(\\sum_{i\\in S}\\pi_i^{-1}x_ix_i^\\top\\right)^{-1}\\pi_i^{-1}x_i\n\\] に関して， \\[\n\\widehat{Y}_{\\mathrm{reg}}=\\sum_{i\\in S}w_iy_i\n\\] という形の線型推定量になっている．\n式 (1) を 外部一致性 (external consistency)，または 校正条件 (calibration / benchmarking property) (Deville and Särndal, 1992) という．\n回帰推定量は \\(X,Y\\) の関係に依らず一致性を持ち，\\(X,Y\\) の間の相関の絶対値が大きいほど分散低減効果が高くなる (Kim, 2024, p. 95)．3\n\n\n2.4 事後層別化\n事後層別化 (post-stratification / stratification after selection) は標本抽出の結果を見て標本を層別化する手法であるが，回帰推定量の特別な場合と見れる．\n母集団が \\(G\\) 個の層に分けられるとする：\\(N=N_1+\\cdots+N_G\\)．\nこのとき，\\(i\\in[N]\\) 番目の単位が層 \\(g\\in[G]\\) に属するかどうかの指示変数 \\(x_{ig}\\in2\\) のベクトル \\(x_i:=(x_{i1},\\cdots,x_{iG})^\\top\\in2^G\\) に関する回帰推定量 \\[\\begin{align*}\n  \\widehat{Y}_{\\mathrm{post}}&:=\\sum_{i=1}^Nx_i^\\top\\left(\\sum_{i\\in S}\\pi_i^{-1}x_ix_i^\\top\\right)^{-1}\\sum_{i\\in S}\\pi_i^{-1}x_iy_i\\\\\n  &=\\sum_{g=1}^G\\sum_{i\\in S_g}\\pi_i^{-1}\\frac{N_g}{\\widehat{N}_g}y_i,\\qquad\\widehat{N}_g:=\\sum_{i\\in S}\\pi_i^{-1}x_{ig}.\n\\end{align*}\\] を事後層別化推定量という．\nMRP (Multilevel Regression and Post-stratification) (Gelman and Little, 1997), (Gelman, 2014) は事後層別化の階層モデル・縮小推定版である．\n\n\n2.5 ランキング法／繰り返し比例的フィッティング法\n(Deming and Stephan, 1940) では 1940 年の国勢調査の結果の分析を考えていた．\n特に，基本的な情報は全数調査されるが，詳細な情報は標本調査でしか得られない状況下で，母集団の \\(I\\times J\\) 分割表の各セル \\(U_{ij}\\) の値 \\(N_{ij}\\) の推定を考えていた．\nただし，周辺和 \\(N_{i-},N_{-j}\\) は全数調査で得られているとする．\nこのとき，\\(N_{ij}\\) の推定量の候補として \\[\n\\frac{n_{ij}}{n_{i-}}N_i,\\quad\\frac{n_{ij}}{n_{-j}}N_{-j},\\quad\\frac{n_{ij}}{n}N\n\\] の３つが考えられる．３番目が良いと考えるかもしれないが，その結果得られる分割表は周辺和を保存しない．\nこの問題は次のような形でも現れる：指示変数 \\[\nx_k=(x_{1-k},\\cdots,x_{I-k},x_{-1k},\\cdots,x_{-Jk}),\\qquad x_{ijk}:=1_{U_{ij}}(k),\n\\] に基づく事後層別化推定量 \\[\n\\widehat{Y}_{\\mathrm{post}}=\\sum_{i\\in S}\\pi_i^{-1}g_i(S)y_i,\\qquad g_i(S):=\\left(\\sum_{k=1}^Nx_k\\right)^\\top\\left(\\sum_{k\\in S}\\pi_k^{-1}x_kx_k^\\top\\right)^{-1}x_i\n\\] を考えたいが，これが \\(\\operatorname{rank}\\left(\\sum_{k\\in S}\\pi_k^{-1}x_kx_k^\\top\\right)=I+J-1\\) であるため，一意な表示を持たない．\n\\(g_i(S)\\) の候補のうち，次を満たす \\(g_i\\) を選ぶことが目標である： \\[\n\\sum_{k\\in S}\\frac{g_k}{\\pi_k}x_{i-k}=\\sum_{k=1}^Nx_{i-k}=N_{i-},\n\\tag{2}\\] \\[\n\\sum_{k\\in S}\\frac{g_k}{\\pi_k}x_{-jk}=\\sum_{k=1}^Nx_{-jk}=N_{-j}.\n\\tag{3}\\]\n\n\n\n\n\n\n(Iterative Proportional Fitting / Ranking algorithm Deming and Stephan, 1940)\n\n\n\n\n\\(g^{(0)}_k\\gets1\\) と初期化する．\n\\(x_{i-k}=1\\) すなわち \\(k\\in U_{i-}\\) であるとき， \\[\n  g^{(t+1)}_k\\gets g_k^{(t)}\\frac{\\sum_{k=1}^Nx_{i-k}}{\\sum_{k\\in S}\\frac{g^{(t)}_k}{\\pi_k}x_{i-k}}.\n  \\] これにより条件 (2) が満たされる．\n\\(z_{-jk}=1\\) すなわち \\(k\\in U_{-j}\\) であるとき， \\[\n  g^{(t+2)}_k\\gets g_k^{(t+1)}\\frac{\\sum_{k=1}^Nx_{-jk}}{\\sum_{k\\in S}\\frac{g^{(t+1)}_k}{\\pi_k}x_{-jk}}.\n  \\] これにより条件 (3) が満たされる．\n収束するまで繰り返す．\n\n\n\nこれは特定の目的関数を最小化することに等しい．(Deming and Stephan, 1940, p. 428), (Zieschang, 1990), (Jean-Claude Deville and Sautory, 1993) も参照．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#荷重校正推定量",
    "href": "posts/2024/Survey/Survey3.html#荷重校正推定量",
    "title": "ベイズデータ解析３",
    "section": "3 荷重校正推定量",
    "text": "3 荷重校正推定量\n\n3.1 はじめに\n回帰推定量は \\(X\\) から \\(Y\\) に関する情報を抽出することで，HT 推定量の効率を改善することができる方法である．\nしかし，HT のもう一つの魅力的な性質であった 計画一致性 (design consistency) が失われている．\n回帰推定量の性質である 外部一致性 (external consistency) を保ちながら，別の解を見つけることで，回帰推定量を一般化する形で計画一致性を持つ効率的な推定量を構成することを考える．\n実はこの方法は，モデリングの観点からは \\(X,Y\\) の間のモデルを，標本レベルから母集団レベルに一般化することに相当する．こうして考えられる超母集団モデルを 一般化回帰モデル (GREG: Generalized Regression) という．\nこのような方法で HT 推定量を改善した計画一致性を持つ推定量を model-assisted estimator，特に特定の制約下最適化問題の解として与えられるものを 校正推定量 (calibrated estimator) という．\n校正推定量は計画一致性を持つために，傾向スコアの推定に成功していれば不偏性が保証される．この性質は二重頑健推定量の構成の基礎となる．\n\n\n3.2 差分推定量\n補助的な量 \\(y_i^{(0)}\\) が母集団全体で観測されている場合， \\[\n\\widehat{Y}_{\\mathrm{diff}}:=\\sum_{i=1}^Ny_i^{(0)}+\\sum_{i\\in S}\\pi_i^{-1}\\left(y_i-y_i^{(0)}\\right)\n\\] は 差分推定量 (difference estimator) と呼ばれる．\nHT 推定量同様不偏であるが，分散の値は変化し，特に \\(y_i^{(0)}\\) が \\(y_i\\) の良い近似であるほど分散が小さくなる (Kim, 2024, p. 99)．\nこの \\(y_i\\) の proxy とも言える量 \\(y_i^{(0)}\\) を，他の共変量 \\(x_i\\) から回帰により構成することで，回帰推定量（第 2.3 節）よりも複雑な \\(x_i,y_i\\) 関係もうまく取り込んだ分散低減が可能になる．\nこのように（暗黙裡にでも）モデルを用いており，加えて モデルの特定が成功しているかに依らず HT 推定量を改善できる 方法を model-assisted estimation といい，校正推定量の基本的な考え方である．\n\n\n3.3 一般化回帰モデルと射影推定量\nまず母集団 \\([N]\\) に応用 \\(Y\\) のモデルを当てはめる： \\[\ny_i=x_i^\\top\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,c_i(x_i)\\sigma^2).\n\\tag{4}\\] このように母集団に置かれるモデルを 超母集団モデル (superpopulation model) (Isaki and Fuller, 1982) という．\n特に式 (4) の Gauss-Markov 型の超母集団モデルを 一般化回帰モデル (GREG: Generalized Regression) ともいう．\nこれを解いて得る推定量 \\(\\widehat{y}_i=x_i^\\top\\widehat{\\beta}_c\\) の総和として得られる推定量 \\[\n\\widehat{Y}_{\\mathrm{P}}:=\\sum_{i=1}^N\\widehat{y}_i\n\\] を（モデルベースの） 射影推定量 (projection estimator) という．\n射影推定量は計画一致性を持つとは限らない．\n仮に GREG モデルで \\[\n\\frac{c_i}{\\pi_i}\\parallel x_i\n\\] が成り立つならば，内部バイアス校正 (IBC: Internally Biased Calibration) (Firth and Bennett, 1998) 条件 \\[\n\\sum_{i\\in S}\\frac{1}{\\pi_i}(y_i-\\widehat{y}_i)=0\n\\] が成り立つ．\nこの IBC が，射影推定量が抽出計画に依らずに一致性を持つための十分条件である (補題9.1 Kim, 2024, p. 100)．\n\n\n3.4 一般化最小二乗法 (GLS)\n当然 GREG モデルが IBC 条件を満たすとは限らない．\nそのような場合でも計画一致性を持つような推定量を考えたい．実は， \\[\n\\widehat{Y}_{\\mathrm{GREG}}:=\\widehat{Y}_\\mathrm{HT}+\\biggr(X-\\widehat{X}_\\mathrm{HT}\\biggl)^\\top\\widehat{\\beta}_c\n\\] は計画一致性を持つ．\nこれは 一般化回帰推定量 (GREG: Generalized Regression Estimator) または計量経済学において GLS (Generalized Least Squares) (Aitken, 1936) と呼ばれる．4\n一般化回帰推定量は次の最適化による特徴付けがある： \\[\n\\widehat{Y}_{\\mathrm{GREG}}=\\sum_{i\\in S}\\widehat{\\omega}_iy_i,\\qquad\\widehat{\\omega}_i:=\\pi_i^{-1}+\\left(X-\\widehat{X}_\\mathrm{HT}\\right)^\\top\\left(\\sum_{i\\in S}\\frac{1}{c_i}x_ix_i^\\top\\right)^{-1}\\frac{x_i}{c_i}.\n\\] この荷重 \\(\\widehat{\\omega}_i\\) は，校正条件 (calibration constraint) （式 (1) との違いに注意）を満たすものの中で \\[\nQ(\\omega):=\\sum_{i\\in S}(\\omega_i-d_i)^2c_i,\\qquad d_i:=\\pi_i^{-1},\\quad\\operatorname{subject to}\\sum_{i\\in S}\\omega_ix_i=\\sum_{i=1}^Nx_i.\n\\] を最小にするものとも特徴付けられる (Kim, 2024, p. 102)．\n特に，\\(\\widehat{w}_i\\xrightarrow[n\\to\\infty]{\\mathrm{p}}d_i\\)．\n\n\n3.5 校正推定量\n一般に，校正条件制約を満たす \\((\\omega_i)\\) のうち，凸関数 \\(G\\) が定める目的関数 \\[\nQ(\\omega):=\\sum_{i\\in S}d_iG\\left(\\frac{\\omega_i}{d_i}\\right)c_i\n\\] を最小にするものを 校正荷重 (calibration weight)，校正荷重に関する線型推定量を 校正推定量 (calibration estimator) という (Deville and Särndal, 1992), (Kim, 2024, p. 103)．\nほとんどの校正推定量は漸近的に GREG 推定量に一致する．\n一般に，有限母集団に対する確率標本からの一様最小分散不偏推定量 (UMVUE) は存在しない (Godambe and Joshi, 1965) が，GREG 推定量は「期待漸近分散」の下界を達成する (Isaki and Fuller, 1982)．\n\n\n3.6 最適校正推定量\n特に， \\[\nQ(\\omega)=\\sum_{i\\in S}\\omega_i^2c_i\n\\] を最小化するものは 最適校正推定量 (optimal calibrated estimator) と呼ばれる (Kim, 2024, p. 110)．\nこれはモデルの視点からは \\(x\\) を拡張して人工的に IBC 条件を満たすようにした射影推定量（第 3.3 節）とも見れる．\n最適校正推定量は超母集団モデル (4) が誤特定されている場合に GREG 推定量より良い性能を示す (Kim, 2024, p. 112)．\nGREG モデルより一般的な超母集団モデルに対しての同様の手続きは モデル校正 (model calibration) (Wu and Sitter, 2001) と呼ばれている．この方法では \\(X,Y\\) の関係を推定し，\\(Y\\) の線型推定量を \\(m(X)\\) の形で構成してから，最適構成推定量の議論に還元する．\n\n\n3.7 一般化エントロピー法\n最適構成推定量の構成に倣い， \\[\nQ(\\omega):=\\sum_{i\\in S}G(\\omega_i)c_i\\qquad\\operatorname{subject to}\\sum_{i\\in S}\\omega_ig(d_i)c_i=\\sum_{i=1}^Ng(d_i)c_i\n\\] の最小化により校正荷重を構成する方法を 一般化エントロピー法 (generalized entropy method) (Kwon et al., 2024) という．\nこれは目的関数には計画荷重 \\(d_i=\\pi_i^{-1}\\) が入っていないが，制約条件に入っていることで計画一致性を達成している．\n超母集団モデルである GREG モデルが正しく特定されているならば (Godambe and Joshi, 1965) の下界を達成するが，そうでなくとも一致性は保たれる上に，一般の校正推定量（第 3.5 節）よりも分散は小さいである (Kwon et al., 2024)．5"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#欠測データの扱い",
    "href": "posts/2024/Survey/Survey3.html#欠測データの扱い",
    "title": "ベイズデータ解析３",
    "section": "4 欠測データの扱い",
    "text": "4 欠測データの扱い\n\n4.1 はじめに\n観測単位が欠測している場合 (unit nonresponse)，call-back / follow-up 調査を行うか，それができない場合は次の２つの対処が可能である：\n\n\n\n\n\n\n単位欠測の扱い\n\n\n\n\n欠測メカニズムを抑える共変量は見えている場合（MAR 条件），傾向スコア推定量が利用可能（第 4.2 節）．これは欠測メカニズムのモデリングに基づく．\n一般の校正推定量に対しても，\n\n\n\n単位欠測の場合は，２段階の標本抽出と状況が似ているのである．さらには，非確率標本（調査観察データ，ビッグデータなど）の扱いとも似通う．これについては次稿も参照．\n一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる．6\n現状は多重代入法（第 5.2 節）が主流であると言える (Buuren, 2018)．\n\n\n4.2 傾向スコア推定量\n標本の観測 \\(Y_i\\) は，\\(\\delta_i=0\\) のとき欠損しているとする．\n\n4.2.1 MAR 条件：欠測のメカニズムを抑える共変量が観測できている\n加えて，標本全体についてある変数 \\(X\\) が観測できており，これについて次の条件が成り立つとする：\n\n\n\n\n\n\n(MAR condition Rubin, 1976)7\n\n\n\n欠測の指示変数 \\(\\delta\\) について， \\[\n\\operatorname{P}[\\delta=1|X,Y]=\\operatorname{P}[\\delta=1|X]=:p(X)\n\\] が成り立つ．\n\n\nこれは条件付き独立性 \\(\\delta\\perp\\!\\!\\!\\perp Y\\mid X\\) よりも弱い条件で，MAR (Missing At Random) の条件と呼ばれる．8\n\n\n4.2.2 欠測メカニズムの推定\n欠測確率 \\(p(x):=\\operatorname{P}[\\delta=1|X=x]\\) にノンパラメトリックなモデル \\(p_\\phi(x)\\) を課したとする．\nこのとき，パラメータ \\(\\phi\\) は擬似最尤推定量 \\(\\widehat{\\phi}\\) により一致推定をすることができる．\n\n\n4.2.3 傾向スコア推定量\n仮に母平均 \\[\nY:=\\sum_{i=1}^Ny_i\n\\] が推定対象であったとしよう．\nこのとき，推定された \\(\\widehat{\\phi}\\) を元に，次の推定量が構成できる：\n\\[\n\\widehat{Y}_\\mathrm{PS}:=\\sum_{i\\in\\delta^{-1}(1)}\\frac{1}{\\pi_i}\\frac{y_i}{p_{\\widehat{\\phi}}(x_i)}.\n\\]\n\n\n\n\n\n\n命題（傾向スコア推定量の一致性）9\n\n\n\n欠測確率 \\(p\\) のモデル \\(p_\\phi(x)\\) の特定に成功しているとき，ある正則性に関する条件が満たされる限り，傾向スコア推定量 \\(\\widehat{Y}_\\mathrm{PS}\\) は一致推定量に \\(n^{-1}\\) のオーダーで漸近する．\n\n\n\n\n\n4.3 校正推定量\nある校正荷重 \\((d_i)\\) に関して，計画一致性を持つ推定量 \\[\n\\widehat{Y}=\\sum_{i\\in S}d_iy_i\n\\] を考えているが，単位欠測により特定の \\(y_i\\) が得られず，計算できないものとする．\nこの場合でも，応答があった部分標本 \\[\nS_R:=\\delta^{-1}(1)\n\\] 上の校正推定量 \\[\n\\widehat{Y}_\\omega:=\\sum_{i\\in S_R}\\omega_iy_i\n\\] であって，欠測メカニズム \\(p(x)\\) の特定か，または超母集団モデル \\[\ny_i=x_i^\\top\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,c_i\\sigma^2)\n\\] の特定に成功すれば一致性を持つ，二重頑健なものを構成できる (Kim and Haziza, 2014)．\n\n\n4.4 代入法とその不偏性条件\n項目非反応がある場合，代入値を \\(y_i^*\\) として \\[\n\\widehat{Y}_{\\mathrm{I}}:=\\sum_{i\\in S}\\frac{1}{\\pi_i}\\biggr(\\delta_iy_i+(1-\\delta_i)y_i^*\\biggl)\n\\] による推定が試みられる．\n代入 \\(y_i^*\\) を行うことでリストワイズの削除をするよりも推定の効率を上げることができる．\n\n\n\n\n\n\n(代入推定量の不偏性 Kim, 2024, p. 162)\n\n\n\n\\[\n\\operatorname{E}[Y^*|\\delta=0]=\\operatorname{E}[Y|\\delta=1]\n\\] が成り立つならば，\\(\\widehat{Y}_\\mathrm{I}\\) は不偏推定量である．\n\n\nこの条件は，標本内で MAR 条件（第 4.2.1 節）が成り立つとき： \\[\nY|(X,\\delta=1)=Y|(X,\\delta=0),\n\\tag{5}\\] \\(Y^*\\) を \\(Y|(X,\\delta)\\) からのサンプリングで代入すれば達成される．\nさらに強い条件 \\[\n\\delta\\perp\\!\\!\\!\\perp Y\\mid X\n\\] が成り立つとき，標本内の MAR 条件が成り立つ．\n換言すれば代入法において，欠測の原因 \\(X\\) を突き止め，欠測したグループにおける \\(Y\\) の値 \\(Y|(X,\\delta=1)\\) にモデル (outcome model) を立て，そこからサンプリングをすることを目指す．\n\n\n4.5 回帰による代入\n仮に共変量 \\(X\\) が \\(Y\\) と強い相関を持つとする．このように線型回帰模型を背後に想定することが適切な場合は，よく次のような手続きで代入がされる．\nまず共変量により母集団を \\([N]=N_1+\\cdots+N_G\\) 個に層別化し，それぞれの層で \\[\nY_i=X_i^\\top\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2)\n\\tag{6}\\] というセミパラメトリック回帰モデルを考える．\n次に推定されたモデルを用いて，\\(\\epsilon_i^*\\sim(0,\\sigma^2)\\) を残差 \\[\n\\widehat{\\epsilon}_i:=y_i-x_i^\\top\\widehat{\\beta}\n\\] の分布から（リ）サンプリングし， \\[\ny_i^*\\gets x_i^\\top\\widehat{\\beta}+\\epsilon_i^*\n\\] を代入値とする．\n以上の手続きは 確率的回帰代入法 (stochastic regression imputation) と呼ばれる．平均を代入する場合は単に回帰代入法または条件付き平均代入法 (conditional mean imputation) という．\n\\(Y\\) と強い相関を持つ補助変数 \\(X\\) がいつでも見つかるとは限らない．\nその場合は Gauss-Markov モデル (6) を一般の統計モデルに一般化すれば良い．\n\n\n4.6 マッチングによる代入\n層の中の他のセルをランダムに選んでその値を代入する hot deck imputation や，セルの加重平均を代入する fractional hot deck も同様の考え方に基づく (Fuller and Kim, 2005)．\nこのような手法は マッチング と呼ばれ，カーネル法と関連が深い (Cheng, 1994)．加重平均は対象のセルとの関連度を「距離」によって測り，距離を計算するのに使われる変数は キー ともいう (高井啓二 et al., 2016, p. 110)．傾向スコアマッチングでは傾向スコアがキーである．\n最も単純には同一データセット内の最も似ている単位を持ち出してその値を代入するのがマッチングであるが，最も洗練された方法としては類似度に依存して関連度を自動的に重みづけて，データセット全体で加重平均をとっても良いわけである．\n他の標本の値を参考にする場合は cold deck imputation という．\nなどの Least squares method も同様の考え方に基づく (Little, 1992)．\n\n\n4.7 母集団モデルによる代入法\n一方で，母集団上での \\(Y,X\\) の関係についてモデルを立てて \\(Y|X\\) からサンプリングをすることも考えられる．\n\n\n\n\n\n\n注（無情報サンプリング条件）\n\n\n\n\n\n母集団の分布と標本の分布が一致するとき，無情報サンプリング (noninformative sampling) が実施されたという．そうでない場合は informative sampling という．\nサンプリングが無情報であるための十分条件には \\[\n\\operatorname{P}[I=1|X,Y]=\\operatorname{P}[I=1|X]\n\\] が挙げられる．(Sugden and Smith, 1984) はこれを無情報サンプリング条件という．\nこの下では母集団のモデルと標本のモデルとは一致するが，一般にはこの２つは厳密に峻別しなければ混乱の源である．\n\n\n\n標本内の MAR 条件 (5) だけでなく，母集団上で MAR 条件が成り立つ場合は，\\(Y|X\\) の尤度を \\(f_\\theta(y|x)\\) としてモデリングをし，これを \\[\n\\ell(\\theta):=\\sum_{i\\in S}w_i\\delta_i\\log f_\\theta(y_i|x_i)\n\\] の最大化によって \\(M\\)-推定することが考えられる．10\nただし，\\(w_i\\) は \\(Y\\) の計画一致性を持つ校正推定量を定める校正荷重であるとする．\\(w_i\\) の存在は標本と母集団のズレに起因する．\n最終的に学習されたモデル \\(f_\\theta(y|x_i)\\) からのサンプリングによって代入値 \\(y_i^*\\) を生成する．\nこのモデル \\(f_\\theta(y|x_i)\\) を当てはまりの度合いを見ながらベイズ推論によって得る方法もよく取られるようになっている (C. K. Enders et al., 2020)．\n母集団上の MAR 条件が成り立たない場合は \\(Y|(X,\\delta=0)\\) のモデリングを考える必要がある．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#多重代入法",
    "href": "posts/2024/Survey/Survey3.html#多重代入法",
    "title": "ベイズデータ解析３",
    "section": "5 多重代入法",
    "text": "5 多重代入法\n\n5.1 はじめに\nベイズの観点からは，欠測データとパラメータとは違いがない (Chapter 18 Gelman et al., 2014, p. 449)．\nベイズ事後分布は欠測データとパラメータの上に同時に定まり，欠測データに関して積分をすることで最終的な推論が実行される．\nこれを模倣する形で提案されたのが 多重代入法 (MI: Multiple Imputation) (Rubin, 1978), (Rubin, 1987) である．\n多重代入法ではベイズ事後分布から補完値を複数生成し，複数の擬似完全データに関して同じ解析を実行し，最後に結果を平均する．\n擬似完全データに対する解析が一貫したベイズ推論であった場合，この一連の手続きによって（近似的な）ベイズ推論が実行されることになる．\nしかしデータの補完とその後の擬似完全データ解析は 融和性 (congeniality) を保つ限り別の方法を用いても良いように拡張された (Meng, 1994)．\nこのことにより多重代入法は広く使われるようになっている．\n\n\n5.2 多重代入法\n多重代入法では，モデルベースの代入法（第 4.7 節）をさらに推し進める．\n本来の推定量 \\[\n\\widehat{Y}=\\sum_{i\\in S}w_iy_i\n\\] を代入推定量 \\[\n\\widehat{Y}_\\mathrm{I}=\\sum_{i\\in S}w_i\\biggr(\\delta_iy_i+(1-\\delta_i)y_i^*\\biggl),\\qquad y_i^*\\sim f_\\theta(y_i|x_i)\n\\] で模倣する際，ベイズ事後予測分布で \\[\ny_i^*\\sim f(y_i|y_{\\text{obs}})\n\\] によって補間することが理想的である．\n\n\n\n\n\n\n(Multiple Imputation Rubin, 1978)\n\n\n\n\n事後予測分布から補間値を \\(M\\) 個生成する： \\[\n  y_i^{(j)}\\sim f(y_i|y_{\\text{obs}}),\\qquad j\\in[M].\n  \\]\nそれぞれの補間値について推定量 \\(\\widehat{Y}^{(j)}\\) を計算し，その平均を最終的な推定値とする： \\[\\newcommand{\\MI}{\\mathrm{MI}}\n  \\widehat{Y}_\\MI:=\\frac{1}{M}\\sum_{j=1}^M\\widehat{Y}^{(j)}.\n  \\]\n\n\n\n(Royston and White, 2011) は \\(M\\approx10^3\\) を推奨している．\n\n\n5.3 連鎖方程式による多重代入\n多重代入法において事後予測分布から補間値を生成することは，\\(Y\\) に関してモデルを立てる必要があるためネックになりがちである．\n相互条件付き識別性 (FCS: Fully Conditional Specification) (Stef Van Buuren and Rubin, 2006) が成り立つモデルについては，モデルの具体的な形に依らない Gibbs サンプラーによるサンプリングが可能になる．\nこれを 連鎖方程式による多重代入 (MICE: Multiple Imputation by Chained Equations) (Buuren and Groothuis-Oudshoorn, 2011) といい，R 言語 mice パッケージで実装されている．\n\nその実用性も相まってか，近年の Lancet 誌，New England Journal of Medicine 誌のレビューでは，欠測データの取り扱いに最も多く用いられている手法は MICE であるという報告もある(Hayati Rezvan et al., 2015)． (久史, 2017, p. 75)\n\n\n\n5.4 その他の代入法\nランダムな欠損ではなく，計画された大規模な欠損がある場合は，two-phase sampling の考え方を応用することができる (Kim, 2024, p. 173)．\nなお，全ての代入法はモデル \\(Y|(X,\\delta)\\) の特定を間違えると，\\(\\widehat{Y}\\) の不偏性が失われることに注意 (Hayati Rezvan et al., 2015)．\n\n\n5.5 代入をしない\n代入をせず，欠測しているなら欠測したままで最尤推定を実行することも考えられる．\nこのアプローチは 完全情報最尤推定 (FIML: Full Information Maximum Likelihood)，より最近では　pairwise likelihood estimation とも呼ばれる．11\n欠測が \\(Y\\) に依存しない場合，この「最尤推定量」は MAR の下で一致性と漸近正規性を持つ．12\nただし，推定されたモデルから，欠測値を代入してから結果を出してももちろん良い．ベイズの観点からは，モデルの平均を取ってから予測することに当たる．13\n(1.6節 Buuren, 2018) も参照．\n\n\n5.6 欠測値をどう扱うべきか？\nいつでも多重代入法を使えば良いというものではない．\n例えば \\((X,Y)\\) の関数関係が知りたい回帰分析の状況下で被説明変数 \\(Y\\) の欠損は，これを無視してリストワイズ消去をした complete-case analysis が代入法と等価になる．\n他にも complete-case analysis や代入をしない方がむしろ適切な場合は多い (2.7節 Buuren, 2018)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#終わりに",
    "href": "posts/2024/Survey/Survey3.html#終わりに",
    "title": "ベイズデータ解析３",
    "section": "6 終わりに",
    "text": "6 終わりに"
  },
  {
    "objectID": "posts/2024/Survey/Survey3.html#footnotes",
    "href": "posts/2024/Survey/Survey3.html#footnotes",
    "title": "ベイズデータ解析３",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInverse probability weighting estimator ともいう (Hernán and Robins, 2020, p. 22)．↩︎\n結果的に，Weighted Least Squares と同じ形になっている．WLS は誤差分散が既知の形 \\(W^{-1}:=\\mathrm{diag}(\\sigma_1^2,\\cdots,\\sigma_n^2)\\) をしている場合の最良線型不偏推定量 (BLUE) である．一般に最小二乗法は広い設定で BLUE を与え続け，一般の既知の分散 \\(V(X)\\) を持つ場合は GLS (Generalized Least Squares) と呼ばれる．\\(V(X)\\) が既知である場合などなく，一般にはこれの推定から始める必要があり，これは Feasible GLS と呼ばれる (Hayashi, 2000, p. 59)．↩︎\nこの抽出計画に依らない性質を以て，(Särndal et al., 1992) は model-assisted 推定量と呼んでいる．model-dependent 推定量とは対照的である．↩︎\nこの２つの類似性は (Zieschang, 1990) が指摘している．一般の回帰分析の設定下では “GLS is more efficient than OLS under heteroscedasticity (also spelled heteroskedasticity) or autocorrelation” などと説明される．↩︎\nただし，余分な項があるために，正しく特定されている下では校正推定量よりもやや分散が大きい．↩︎\n総務省統計局では，Imputation の訳語として「補定」を用いる．↩︎\n最も古典的な形のものであり，母集団上の条件であることから，population MAR とも呼ばれる．母集団上の MAR と抽出計画の無視可能性 (Sugden and Smith, 1984) との２条件が成り立つとき，標本の MAR が成り立つ (Berg et al., 2016)．↩︎\n\\(Y\\to X\\to\\delta\\) が Markov 連鎖をなす，とも換言できる．↩︎\n(Kim, 2024, p. 154) 定理12.1も参照．↩︎\n一方で，重み付き推定方程式の解として定まる \\(Z\\)-推定量として構成することもできる．(5.2節 高井啓二 et al., 2016, p. 163)．↩︎\n完全情報最尤推定の言葉は初期の構造方程式モデリングプログラム AMOS に組み込まれて有名になっていた (Craig K. Enders and Bandalos, 2001)．直接尤度 (direct likelihood) または観測尤度 (observed likelihood) の方法ともいう (狩野裕, 2019)．完全尤度 (full likelihood) の用語は (高井啓二 et al., 2016) など．↩︎\n(狩野裕, 2019) に素晴らしい解説がある．日本語の文献としては (高井啓二 et al., 2016) もあり，第５章で推定方程式の観点から解説されている．↩︎\nそういえば Bayes 的な integral out に関して doubly robust という考え方はないのか？doubly robust の Bayesian counterpart はなんだろう？↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html",
    "href": "posts/2024/Survey/Survey1.html",
    "title": "ベイズデータ解析１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#関連記事",
    "href": "posts/2024/Survey/Survey1.html#関連記事",
    "title": "ベイズデータ解析１",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#分散分析",
    "href": "posts/2024/Survey/Survey1.html#分散分析",
    "title": "ベイズデータ解析１",
    "section": "1 分散分析",
    "text": "1 分散分析\n\n1.1 はじめに\n分散分析 (ANOVA: Analysis of Variance) は標本がある因子 \\(A,B,\\cdots\\) によって層別されている場合に，層間の平均効果 \\(\\mu_i\\) に差があるかどうかを検定する手法である： \\[\nH_0:\\mu_1=\\cdots=\\mu_p\\quad\\text{v.s.}\\quad H_1:\\exists_{i,j\\in[p]}\\;\\mu_i\\ne\\mu_j.\n\\]\n因子 \\(A,B,\\cdots\\) の総数に応じて，\\(A\\) のみの場合を一元配置分散分析，\\(A,B\\) の場合を二元配置分散分析などという．\n多くの場合，観測のモデルには 線型 Gauss の仮定が置かれる．例えば一元配置である場合， \\[\nY_{ij}=\\mu_i+\\epsilon_{ij},\\qquad i\\in[p],j\\in[n_i],\\epsilon_{ij}\\sim\\operatorname{N}(0,\\sigma^2),\n\\tag{1}\\] というモデルを仮定し，パラメータ \\(\\mu_i\\) に関して検定を行う．\n二元配置では \\[\nY_{ij}=\\mu+\\alpha_i+\\beta_j+\\epsilon_{ij}\n\\] となり，集団は２つの軸 \\(A,B\\) で層別されており（分割表の状態），それぞれの因子からの効果 \\(\\alpha_i,\\beta_j\\) が説明変数に加法的に加えられる．\n\n\n1.2 分散分析の抽象的な説明\n分散分析では観測 \\(Y_{ij}\\) の変動のうち，\\(H_0\\) の仮定の下で説明されなかった部分（残差） \\(R_1^2\\) と，そもそも線型 Gauss 模型 (1) では説明しきれない部分 \\(R_0^2\\) とに関して， \\[\nF:=\\frac{(R^2_1-R^2_0)/q}{R^2_0/(n-r)}\n\\] を考える．ただし，\\(r:=\\operatorname{rank}(X)\\) はデータの自由度とした．\nこの \\(F\\) は，各群への所属表すダミー変数を用いた回帰分析の残差 \\(R_0\\) と，各群への所属を考慮しない回帰分析による残差 \\(R_1\\) とを，自由度を考慮して比を取った形をしている．\nこの \\(F\\) は一般に非心 F-分布に従い，仮定 \\(H_0\\) が成り立つときのみ 中心 F-分布 \\(\\mathrm{F}(q,n-r)\\) に従う．これは各群への所属情報に何の情報量もない場合には，\\(F\\) が同じ平均を持つ正規確率変数の自乗和の比になるためである．\n換言すれば，非心パラメータ \\[\n\\delta:=\\frac{1}{\\sigma^2}\\sum_{i=1}^pn_i(\\mu_i-\\overline{\\mu})^2\n\\] に関して \\(H_0:\\delta=0\\) を検定することに等しい (Bertolino et al., 1990), (Solari et al., 2008)．\nこの \\(F\\) （または等価な \\(t,\\delta\\)）を検定統計量として仮設検定を実行するのが (repeated measures) ANOVA である．\n\n\n1.3 Gauss-Markov の仮定\n「標本が正規分布に従う母集団からの独立標本である」という帰無仮説を持つ検定に，(Shapiro and Wilk, 1965) の検定などがある．\n探索的な方法には Q-Q plot などがある．(van den Bergh et al., 2020) も参照．\n等分散の仮定をチェックする検定には (Mauchly, 1940) の検定や (Brown and Forsythe, 1974) の検定などがある．\n仮にこれらの仮定が破られた場合は，(Kruskal and Wallis, 1952) 検定などのランクベースの ANOVA 手法を用いることができる．1\nただし，検定はデータの一側面しか伝えていない．例えば，データが帰無仮説をどれほど支持しているかの尺度は検定では得られない．\nそれゆえ，ANOVA などのモデルの仮定を確認するためには，検定だけでなく他の探索的手法と組み合わせることが推奨される．(Tijmstra, 2018) も参照．\n\n\n1.4 「検定」に対する注意喚起\n一方で ANOVA を含めた検定は一面のみを強調する構造となっており，一連の統計解析の中で自然な位置付けを持つものでない．\n特に，\\(p\\)-値による仮設検定は「データが不十分であることにより判断ができない」ことと，「データと帰無仮説は激しく矛盾する」こととを区別できない．この意味でも限定的な情報量しか持たない．\n例えば \\(p\\)-値が小さく帰無仮説が棄却されたとしても，\\(p\\)-値はモデルの変化に対して頑健ではないかもしれず，実際はほとんど帰無仮説が成り立つことが真実かもしれない．\nこのような全貌を探索的に捉えるためには，検定を金科玉条とするのではなく，広くモデル比較・モデル選択の観点からアプローチすることが大切である．同様のことが (Rouder et al., 2016) でも論じられている．\nANOVA は，特に大規模なものが，現在でも実験心理学などの分野で広く用いられている．これは心理学では多くの因子 \\(A,B,C,\\cdots\\) が存在し，それぞれが複雑な関係にあるためである．\nしかし推定法も従来の \\(F\\)-検定を用いたのではその力を十分に発揮できない．解決は丁寧な階層モデリングとベイズによる探索的な解析にある．2\n\n”if you have a complicated data structure and are trying to set up a model, it can help to use multilevel modeling”—not just a simple unitswithin-groups structure but a more general approach with crossed factors where appropriate. This is the way that researchers in psychology use ANOVA, but they are often ill-served by the classical framework of mean squares and F-tests. We hope that our estimation-oriented approach will allow the powerful tools of Bayesian modeling to be used for the applied goals of inference about large numbers of structured parameters. (Gelman, 2005, p. 53)"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ分散分析",
    "href": "posts/2024/Survey/Survey1.html#ベイズ分散分析",
    "title": "ベイズデータ解析１",
    "section": "2 ベイズ分散分析",
    "text": "2 ベイズ分散分析\n\n2.1 はじめに\nベイズ分散分析 (Rouder et al., 2012), (Rouder et al., 2016) では，(1) などの線型モデルに対して，パラメータが零ベクトルであることに対する検定の代わりに，帰無仮説を表現するモデルに対する Bayes 因子を用いたモデル比較を行う．\nつまり，ベイズ分散分析と言った場合，「分散分析」の概念は完全に線型回帰モデルのベイズ推論に回収される．より正確には，階層モデルのベイズ推論は「分散分析」の正統な後継である (Gelman, 2005)．\n\nIn this sense, ANOVA is indeed a special case of linear regression, but only if hierarchical models are used. (Gelman, 2005, p. 2)\n\n(Rouder et al., 2012) はその際の標準的な事前分布の選択について議論している（第 2.4 節）．\n(Rouder et al., 2016) が指摘するように，分散分析がベイズ化される過程で，検定がモデル比較に置き換わっている．\n\n\n2.2 JZS 因子\nベイズ的な仮設検定は (Jeffreys, 1961) に源流を持つ．一般に，位置母数の事前分布に Cauchy 分布を用いることは (5.3節 Jeffreys, 1961) に源流を持つ．このことについては (Robert et al., 2009) も参照．\n同一の単位を繰り返し測定し， \\[\nY_i\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(\\mu,\\sigma^2),\\qquad i\\in[n],\n\\] に従って何らかの処置効果 \\(Y_i\\) 観測するとし，帰無仮説 \\(\\mu=0\\) の妥当性を議論したいとする．\nこの際，まずは効果サイズ (effect size) (Cohen, 1988) \\(\\delta:=\\mu/\\sigma\\) という無次元量にパラメータを変換し，これを Cauchy 分布と比較することを考える： \\[\nM_0:\\delta=0\\quad\\text{v.s.}\\quad M_1:\\delta\\sim\\mathrm{C}(0,1)\n\\]\n実際，この Cauchy 分布というのは変換 \\(J(\\nu,\\sigma):=\\frac{\\mu^2}{\\sigma^2}\\) を通じて \\(\\mathbb{R}\\) 上の Jeffreys 事前分布（この場合は Lebesgue 測度に一致）に（ほとんど）等価になる．\nこの２つのモデル \\(M_0,M_1\\) の残りの仮定は共通の Jeffreys の事前分布 \\(p(\\sigma^2)\\,\\propto\\,\\sigma^{-2}\\) で共通とし，Bayes 因子を算出する．\nこの値を検定統計量のように扱うとき，これを Jeffreys の名前に (Zellner and Siow, 1980) を加えて JZS 因子 (Bayarri and García-Donato, 2007) という．あるいは上述の事前分布の選び方を JZS 事前分布という．\nJZS 因子は，事前のモデル確率 \\(p(M_0),p(M_1)\\) に依らずに算出できる．\n\n\n2.3 \\(p\\)-値との違い\nJZS 因子は \\(p\\)-値と比較して，サンプルサイズが大きいほど保守的になる傾向がある．(Wetzels et al., 2011) は 2007 年に特定の実験心理学雑誌に投稿された 855 件の t-検定に対して，JZS 因子と \\(p\\)-値との値を報告してこれを観察している．\n(van den Bergh et al., 2023) は実例を用いて，特に複雑な心理学実験において，２つの解析手法はときに全く違う結論を導くことを例証している．\nまた JZS 因子は \\(N\\to\\infty\\) の極限で，\\(\\delta\\ne0\\) であった場合は \\(\\infty\\) に発散し，\\(\\delta=0\\) であった場合は \\(1\\) に収束するという性質を持つ．\n\n\n2.4 １元配置 ANOVA の線型モデル解釈\n１元配置 ANOVA のモデルを次のように表す： \\[\n\\boldsymbol{Y}=\\mu\\boldsymbol{1}_n+\\sigma\\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon},\\qquad\\boldsymbol{\\epsilon}|\\sigma^2\\sim\\operatorname{N}(\\boldsymbol{0},\\sigma^2I_n).\n\\tag{2}\\] 各水準 \\(i\\in[p]\\) に属するデータの数を \\(n_i\\) とすると \\[\n\\boldsymbol{X}=\\begin{pmatrix}\n\\boldsymbol{1}_{n_1} & \\boldsymbol{0} & \\cdots & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{1}_{n_2} & \\cdots & \\boldsymbol{0}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\boldsymbol{0} & \\boldsymbol{0} & \\cdots & \\boldsymbol{1}_{n_p}\n\\end{pmatrix},\\qquad\\boldsymbol{\\theta}=\\begin{pmatrix}\n\\mu_1\\\\\n\\mu_2\\\\\n\\vdots\\\\\n\\mu_p\n\\end{pmatrix}.\n\\] となる．\nこのとき，定数項 \\(\\mu\\) をすでに括り出しているので，\\(\\boldsymbol{\\theta}=0\\) の場合のモデルが帰無仮説に対応する．\n対立仮説としては，\\(\\boldsymbol{\\theta}\\) 上に次の \\(g\\)-prior を定める： \\[\n\\boldsymbol{\\theta}\\sim\\operatorname{N}(\\boldsymbol{0},G),\\qquad G=\\mathrm{diag}(g_1,\\cdots,g_p),\\qquad g_i\\overset{\\text{i.i.d.}}{\\sim}\\chi^{-2}(1).\n\\] これは各 \\(\\mu_i\\) に対して独立な Cauchy 事前分布を仮定している．\n(Zellner and Siow, 1980) の事前分布 \\[\n\\boldsymbol{\\theta}|g\\sim\\operatorname{N}(\\boldsymbol{0},g(\\boldsymbol{X}^\\top\\boldsymbol{X}/n)^{-1}I_p)\n\\] や一般の \\(g\\)-prior との違いとして，スケール因子 \\((\\boldsymbol{X}^\\top\\boldsymbol{X}/n)^{-1}\\) がない形であるのは，ANOVA の説明変数 \\(\\boldsymbol{X}\\) が離散変数であるためである．\n\n\n2.5 ANOVA でのベイズ因子\n以上のモデルを，帰無仮説に対立させる「デフォルトモデル」として提案したのが (Rouder et al., 2012) である．\n特に \\(G=gI_p\\) の場合は，結果として得られるベイズ因子は１次元の積分のみを含むため，簡単な数値積分アルゴリズムによって計算可能である．\n\n\n2.6 多元配置ベイズ ANOVA\n多くの場合，被説明変数に関連すると予期される因子は複数存在する．ここでは２元配置の場合を考える．それぞれの因子が \\(a,b\\) 水準を持つとき，フルモデルは \\[\n\\boldsymbol{Y}=\\mu\\boldsymbol{1}+\\sigma\\biggr(\\boldsymbol{X}_\\alpha\\alpha+\\boldsymbol{X}_\\beta\\beta+\\boldsymbol{X}_{\\gamma}\\gamma\\biggl)+\\boldsymbol{\\epsilon},\n\\] と表せる．\n詳しくは (Section 8 Rouder et al., 2012) を参照．\n\n\n2.7 「ベイズ因子」に関する注意喚起\nベイズ因子を含め，周辺尤度 \\(p(\\theta|y)\\) （エビデンスともいう）を用いた指標は，モデルの仮定に対して感度が高い (Robert, 2016), (Kamary et al., 2018)．\nそのため「モデルのデータへの当てはまりの良さを１つの指標にまとめた値」としては少し心許ない．\n加えて，帰無仮説に対立させる仮説を構成して，二項対立の構造に持ち込むことは，自然なデータ解析のワークフローに必ずしも入ってこない．\nベイズ推論の仮定で得られる事後分布から，特定の仮説 \\(H:\\theta=\\theta_0\\) がまともかを評価する方法の方が，探索的データ解析の観点からは含意が多いことも多い．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ推論に基づく方法",
    "href": "posts/2024/Survey/Survey1.html#ベイズ推論に基づく方法",
    "title": "ベイズデータ解析１",
    "section": "3 ベイズ推論に基づく方法",
    "text": "3 ベイズ推論に基づく方法\n\n3.1 はじめに\nANOVA とベイズ ANOVA の究極的な目標は，平均が一致する \\[\nH_0:\\mu_1=\\cdots=\\mu_p\n\\] という仮説がデータからどれほど支持されるか／されないかを評価することにある．\n最も直接的な方法は，パラメータ空間上に描かれる事後分布を見ることである．信用区間を報告し，帰無仮説がそのどこに位置するかを見ても良いだろう．\n\n\n3.2 事後予測によるモデル検証\nこのように，ベイズ事後分布やそのサンプルを通じたモデル検証方法は posterior predictive check (Gelman and Shalizi, 2013) と呼ばれ，これを多角的に行うことが一つの理想形とされている (Gelman et al., 2020)．\n同様に (Kruschke, 2015) では，モデル (2) の形を一般化線型モデルの特別な場合と見て推定し，ANOVA をモデル比較の観点から適切に実行する方法を論じている．\nこのように，ANOVA を適切に扱うには，階層モデルとしての取り扱いが欠かせない．同様の議論は (Gelman, 2005) でも展開されている．\nここでは，以下の節で帰無仮説 \\(H_0\\) の妥当性を詳細に評価するための方法を見ていく．\n\n\n3.3 ベイズ事後 \\(p\\)-値\nBayes 因子の他に，検定統計量に対するベイズ事後予測分布を導出し，その裾確率を評価して \\(p\\)-値の代替とする方法もある．\nこれは 事後予測 \\(p\\)-値 (posterior predictive \\(p\\)-value) (Gelman et al., 2014, p. 146) と呼ばれる．\n\n\n3.4 ROPE (Region of Practical Equivalence)\nROPE は帰無仮説 \\(H_0\\) と区別がつかないとする区間である．\n帰無仮説が \\(H_0:\\theta=\\theta_0\\) という形をしていた場合，ほとんどの場合 \\(\\theta=\\theta_0+0.1\\) でも事実上変化はない．\nこのように，帰無仮説と同一視してしまう範囲を ROPE (Kruschke, 2015, p. 336) という．\n\n3.4.1 HDR の使用\nこの ROPE が 95% 最高密度信用領域 (HDR: Highest Density Region) と互いに素になるときに，「棄却」されたとする．\nただし，最高密度信用領域とは 95% 信用区間＝95% の事後確率を持つ領域のうち，面積が最も小さいもののことを言う．\nこの方法では，逆に HDR が ROPE を完全に含む場合，帰無仮説を「採択」するという積極的な意思決定も可能である．\nROPE と同様の考え方は，ベイズによる実験計画法でも range of equivalence (Freedman et al., 1984), (Spiegelhalter et al., 1994) の名前で用いられてきた歴史がある．\n\n\n3.4.2 ROPE の確率\nまたは，事後確率分布が ROPE 内にどれほどの確率を与えるかを見ることもできる (Kruschke, 2018)．\nROPE の応用と実装は (Kelter, 2022) も参照．\n\n\n\n3.5 混合モデリングによる方法\nベイズの方法の特徴は，検定・モデル比較と推論とに区別がないことである．\n加えて純粋に検定・モデル比較のための手続きを作るより（ベイズ因子など），推定の一種として扱った方がより多くの情報を引き出せるため，ワークフローとしては好ましい (Kruschke, 2011)．\n(Robert, 2016), (Kamary et al., 2018) では検定の対象となっているモデル \\[\nM_1:x\\sim f_1(x|\\theta_1)\\quad\\text{v.s.}\\quad M_2:x\\sim f_2(x|\\theta_2)\n\\] を，混合モデル \\[\nM_\\alpha:x\\sim\\alpha f_1(x|\\theta_1)+(1-\\alpha)f_2(x|\\theta_2)\n\\] の成分の１つとみなし，その荷重 \\(\\alpha\\) の事後分布を推定し，これを検定に用いるという方法が提案されている．\n\n\n3.6 ハイパーモデル上の推論による解決\n同様の発想により，ベイズ因子の計算と推論によるモデル比較とを，より大きなハイパーモデルを立てることで同時に実行することもできる．\n(Kruschke, 2011, p. 308) や (O’Neill and Kypraios, 2016) などで考えられている．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#ベイズ統計解析に関する文献案内",
    "href": "posts/2024/Survey/Survey1.html#ベイズ統計解析に関する文献案内",
    "title": "ベイズデータ解析１",
    "section": "4 ベイズ統計解析に関する文献案内",
    "text": "4 ベイズ統計解析に関する文献案内\n応用分野の研究者に対する「なぜベイズを使うのか？」に対する端的な回答として，「統計的有意性」などの「わかりやすい」指標に飛びついた結果，真のデータの声を聞かずに自分の見たいものを見始めてしまうと言うことが少なく，「自己欺瞞に陥りにくい」という美点があることは，ベイズ機械学習の稿 でも触れた．\n\\(p\\)-値はそのような欺瞞を生む代用例であり，使用を禁止すべきとの声 (Blakeley B. McShane and Tackett, 2019) もある．その論拠は大まかに次のとおりである．\nそもそも \\(p\\)-値とは，「帰無仮説を採用したモデルはデータへの当てはまりの度合いが悪い」ということを言っているだけであり，\\(p\\)-値が十分に低ければそれ以上の情報は引き出せない．\n当然 \\(p\\)-値が \\(0.01\\) であることと \\(0.00001\\) であることは質的に全く変わらない (Gelman et al., 2014, p. 150)．\nそのことに加え \\(p\\)-値は必ずしも頑健な指標ではなく，帰無仮説を少し摂動させただけで \\(p\\) 値が大きくなってしまうかもしれない．そのような場合は結局ほとんど帰無的であり，「統計的有意性」はほとんど無意味になってしまう．同様の議論が (Gelman and Stern, 2006) で展開されている．\nこのような現状への対処として，応用分野の研究者にもベイズ統計学は根本的な解決法として広く推奨される (Dienes and Mclatchie, 2018)．(Wagenmakers et al., 2016) はその旨を２つの実例を通じて簡潔に実証しており，同時にベイズ統計学の考え方に対する洗練された導入を行なっている．\n(van den Bergh et al., 2020) は分散分析をベイズの方法によって実行する手引きを，特に JASP を用いて実演している．\nJASP のベイズ ANOVA のエンジンは R パッケージ BayesFactor (CRAN / GitHub) を用いている．BayesFactor では大規模な \\(M\\)-元配置 ANOVA モデルにおいても Bayes 因子を用いたモデル比較を行うことができる．\nベイズ ANOVA の R パッケージとしては bayesanova (CRAN / GitHub) (Kelter, 2022) もある．これは検定に似た行為を根本的に排除して Gauss 混合モデルとして Gibbs サンプラーによるベイズ推定を実行し，ROPE (Region of Practical Equivalence) (Kruschke, 2015, p. 336) (Kruschke, 2018) を用いたモデル比較を行う．\nもちろんこのような完全なモデリングを行うことが理想かもしれないが，従来の ANOVA になれきっている研究者にとっては，Bayesian ANOVA に手を伸ばしてみることが次のステップとして大変良いだろう．\nまた別の角度からの「ベイズを使うべき理由」としての説得的な議論としては，ベイズ階層モデリングは ANOVA の正統進化という理解 (Gelman, 2005) ができるという向きもある．\n以上の立場は (Gelman et al., 2014) や (Kruschke, 2015) などの標準的なベイズデータ解析の教科書でも一貫している．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#その他の文献案内",
    "href": "posts/2024/Survey/Survey1.html#その他の文献案内",
    "title": "ベイズデータ解析１",
    "section": "5 その他の文献案内",
    "text": "5 その他の文献案内\n\nF-検定については (吉田朋広, 2006) と (Solari et al., 2008) を参考にした．\nANOVA の歴史については (Tweney, 2014) を参照．(repeated measures) ANOVA は多重線型回帰のうち説明変数が離散変数である場合に相当するという理解は，一般化線型モデルの発展と普及に伴って理解が広がった．"
  },
  {
    "objectID": "posts/2024/Survey/Survey1.html#footnotes",
    "href": "posts/2024/Survey/Survey1.html#footnotes",
    "title": "ベイズデータ解析１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nもちろん，Stan などの確率的プログラミング言語を用いた完全なベイズモデリングはいつでも実行可能である．↩︎\nそして因子分析を通じて，記述統計学の正統進化であるということもできる！？ ANOVA の歴史については (Tweney, 2014) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html",
    "href": "posts/2024/Survey/Survey4.html",
    "title": "ベイズデータ解析４",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#関連記事",
    "href": "posts/2024/Survey/Survey4.html#関連記事",
    "title": "ベイズデータ解析４",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#非確率標本とは何か",
    "href": "posts/2024/Survey/Survey4.html#非確率標本とは何か",
    "title": "ベイズデータ解析４",
    "section": "1 非確率標本とは何か？",
    "text": "1 非確率標本とは何か？\n\nGenerally speaking, these designs have not been explored in detail by survey researchers even though they are frequently used in other applied research ﬁelds. (Baker et al., 2013, p. 91)\n\n母集団 \\([N]\\) から部分集合 \\(S\\subset[N]\\) が標本として抽出されたとする．\nこの抽出計画 (sampling design / mechanism) が未知である場合，これを 非確率標本 (nonprobability sample) という．\n\n1.1 非確率標本\n確率抽出 (probability sampling) とは \\([N]\\) の部分集合の全体 \\(P([N])\\) 上の既知の確率分布に従っているとみなせる標本で，さらに何人も標本に選ばれる確率が零でないもの \\[\n\\pi_i:=\\operatorname{P}[i\\in S]&gt;0\n\\] をいう．詳しくは，前稿 も参照．\nすなわち非確率標本とは，\\(S\\in\\mathcal{L}(\\Omega;P([N]))\\) の従う分布が未知であったり，抽出計画上絶対に標本に入り得ない単位が存在する場合をいう．\n\n\n1.2 例\n母集団 \\([N]\\) を国民全体だとした場合，確率抽出は国勢調査規模の営為によってのみしか達成し得ない．\n多くの科学分野で実施されるような，特定の学校の学生や特定の地域の構成員を対象としたサンプル　便宜的標本 (convenience sample) は全て非確率標本に分類されることになる．\nまた多くのウェブアンケート代行業者は，事前にアンケートに協力することを約束したユーザーのプールからランダムに抽出して実行する．このような，自主的な応募によって得られたパネルを opt-in panel / panel of volunteers といい，ここからのサンプルもまた便宜的（二段階抽出）標本である．\n以上の理由から，多くの「ビッグデータ」と呼ばれるデータは非確率標本である (Meng, 2018), (Jae-Kwang Kim and Tam, 2021)．\nそのほかの非確率的標本の例については，(Section 3 AAOR, 2013) を参照．\n\n\n1.3 自己選択バイアスの問題\nこのような非確率標本では，特定のクラスの単位を包摂できていない問題 (frame undercoverage) や，自ら進んで応募して標本に入ることで生じる交絡とバイアス (self-selection bias) が問題になる．1\n端的に言えば，ランダムな欠測 (MAR: Missing At Random) (Rubin, 1976) の仮定が成り立たず，多くの欠測データ手法はそのままでは適用できないことが問題になる．\n\n\n1.4 データ統合\n非確率標本単体では出来ることが限られているかもしれないが，補助情報と組み合わせてモデルを立てることで統計的推論を試みることができる．\n\n\n典型的なデータの例\n\n\n\n\n\n\n\n\n\nData\nDesign\nRepresentative?\nX\nY\n\n\n\n\nA\nProbability\nYes\nX\nmissing\n\n\nB\nNonprobability\nNo\nX\nY\n\n\n\n\n確率標本 A をビッグデータ B と紐づけられるという状況はかなら理想的であるが，仮にこのような dual frame estimation (Hartley, 1962), (Skinner and Rao, 1996) の一部として非確率標本を扱えるときは，B を A の補助情報とみることで従来の校正荷重による推定の理論が利用可能になる．校正推定量については前稿も参照．\n例えば A を実験データ，B を観察データとしたデータ統合の試みは計量経済学においても進んでいる (Athey et al., 2019), (Athey et al., 2020), (Park and Sasaki, 2024)．B をオルタナティブデータと呼ぶ向きもある．\n実はこれから見るように，非確率標本の過小包摂性 (under coverage) は，単純ランダム抽出ではない抽出計画による確率標本のバイアス補正の議論に帰着し，自己選択バイアス (self-selection bias) の補正は欠測データの議論に帰着する (Jae-Kwang Kim and Tam, 2021)．\n\n\n1.5 データ統合の方法\n大きく分けて次の３通りが考えられる (Salvatore et al., 2024)：\n\n\n\n\n\n\n\n荷重校正による方法 (Elliot, 2009), (Robbins et al., 2020)\n\n非確率標本はあくまで確率標本の補助情報とし，荷重校正を実施する．\n\n擬似ランダム化による方法 (Elliott and Valliant, 2017)\n\n自然によるランダム化が行われたとし，これを推定するステップを追加することで確率標本の議論に帰着させる．\n\n大量代入 (mass imputation) による方法 (Jae Kwang Kim et al., 2021)\n\n\n\n\n\n\n1.6 バイアス低減\n各単位 \\(i\\in[N]\\) が標本に包含される確率 \\[\n\\pi_i:=\\operatorname{P}[i\\in S],\\qquad i\\in [N],\n\\] が未知である場合でも，母集団 \\([N]\\) 上で \\[\n\\pi_i^{-1}\\,\\propto\\,x_i^\\top\\lambda,\\qquad i\\in[N],\n\\] を満たす補助変数 \\(x_i\\;(i\\in[N])\\) が利用可能ならば，推定のバイアスを低減することが可能である．\n\n\n1.7 傾向スコア\nしたがって \\(\\pi_i\\) を推定することが問題になる．\n\\(\\delta_i:=1_S(i)\\) が \\(\\delta_i=1\\) を満たすときのみ \\(y_i\\) が観測されるとすると，\n\\[\n\\pi(x):=\\operatorname{P}[\\delta=1|X=x]\n\\] を 包含確率 または 傾向スコア (propensity score) (Rosenbaum and Rubin, 1983) という．2\n「未知のランダム化メカニズム \\(\\pi\\)」を想定し，これを推定することで確率標本の議論に帰着させるというアプローチは quasi-randomization approach とも呼ばれる (Elliott and Valliant, 2017), (Beresovsky et al., 2024)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#校正推定量",
    "href": "posts/2024/Survey/Survey4.html#校正推定量",
    "title": "ベイズデータ解析４",
    "section": "2 校正推定量",
    "text": "2 校正推定量\n\n2.1 確率標本に対する校正推定量\nGREG モデルと呼ばれる超母集団模型 \\[\ny_i=x_i^\\top\\beta+e_i,\\qquad e_i\\overset{\\text{i.i.d.}}{\\sim}(0,c_i(x_i)\\sigma^2),\n\\tag{1}\\] を仮定する．校正条件 \\[\n\\sum_{i\\in S}\\omega_ix_i=\\sum_{i=1}^Nx_i\n\\tag{2}\\] を満たす荷重 \\((\\omega_i)\\) を用いた線型推定量 \\[\n\\widehat{Y}_{\\mathrm{cal}}:=\\sum_{i\\in S}\\omega_iy_i\n\\] を 校正推定量 (calibration estimator) といい，抽出計画が 無視可能 (ignorable) である限り \\(Y\\) の不偏推定量になる．\nここまでは 前稿 で見た通りである．\n\n\n2.2 非確率標本に対する校正推定量\nこうなると \\(\\sum_{i=1}^Nx_i\\) が判明・推定すれば良いので，校正推定量に関しては 欠測データに対する対処 と同様に，傾向スコアの推定を通じて非確率標本に対応することができる．\nこれには超母集団模型 (1) に加えて，傾向スコア \\[\n\\operatorname{P}[\\delta=1|X=x]=:\\pi(x)\n\\] に対してもモデル \\((\\pi_\\phi)\\) をおく必要がある．\nこのとき，\\(G\\in C^2(\\mathbb{R})\\) を強凸関数，\\(g:=G'\\) として \\[\nQ(\\omega):=\\sum_{i\\in S}G(\\omega_i)c_i(x_i)\n\\] を，校正条件 (2) と完全情報の下で最尤推定された \\(\\widehat{\\phi}\\) を用いて推定した傾向スコア \\(\\widehat{\\pi}_i:=\\pi(\\widehat{\\phi}(x_i))\\) に関して \\[\n\\sum_{i\\in S}\\omega_ig(\\widehat{\\pi}_i^{-1})c_i=\\sum_{i=1}^Ng(\\widehat{\\pi}_i^{-1})c_i(x_i)\n\\tag{3}\\] を満たす中で最小化する荷重 \\((\\omega_i)\\) を用いた校正推定量は，二重頑健性を持つ．\n制約 (3) は選択バイアスを抑える役割を持ち，脱偏倚制約 (de-biasing constraint) とも呼ばれる (Jae Kwang Kim, 2024, p. 198)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#文献案内",
    "href": "posts/2024/Survey/Survey4.html#文献案内",
    "title": "ベイズデータ解析４",
    "section": "3 文献案内",
    "text": "3 文献案内\n\n(Jae Kwang Kim, 2024) を最も参考にした．他によく読んだものは (AAOR, 2013), (Elliott and Valliant, 2017)．\nセミパラメトリック推定に関する日本語文献は (逸見昌之, 2014)．\n非確率標本の確率標本と組み合わせた利用については，計量経済学の文献を除いても (Lohr and Raghunathan, 2017), (Meng, 2018), (Hand, 2018), (Robbins et al., 2020), (Rao, 2021), (Beaumont and Rao, 2021), (Angelopoulos et al., 2023), (Golini and Righi, 2024), (Salvatore et al., 2024) などがあり，大変盛り上がってきている印象がある．"
  },
  {
    "objectID": "posts/2024/Survey/Survey4.html#footnotes",
    "href": "posts/2024/Survey/Survey4.html#footnotes",
    "title": "ベイズデータ解析４",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nすごく大雑把には，収入が高い人ほど収入に関するアンケートに参加しやすい，ウェブに関心のある人ほどウェブアンケートを受けやすい，など．↩︎\n包含確率の用語は標本調査論による．傾向スコアは欠測データ解析による．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BayesNonparametrics.html",
    "href": "posts/2024/Survey/BayesNonparametrics.html",
    "title": "brms を用いたノンパラメトリック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nformula_nonlin &lt;- bf(\n  obesity ~ t2(z_sqrt_LAB, z_log_LDL),\n  family = cumulative(link = \"logit\")\n)\nfit_nonlin &lt;- brm(\n  formula_nonlin,\n  data = df_double,\n  chains = 4, cores = 4\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html",
    "href": "posts/2024/Survey/BayesANOVA.html",
    "title": "ベイズ分散分析のモデル解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#はじめに",
    "href": "posts/2024/Survey/BayesANOVA.html#はじめに",
    "title": "ベイズ分散分析のモデル解析",
    "section": "1 はじめに",
    "text": "1 はじめに\n分散分析では，質的変数 \\(A\\) の各水準 \\(A=a_1,a_2,\\cdots\\) について，水準内の変動と，水準間の変動を比較することで検定を構成する．\n因子 \\(A\\) がデータに何の影響も及ぼさない場合（＋データが正規分布に従う場合），分散の比は中心 \\(F\\)-分布に従うはずであり，これに基づいて帰無仮説を検定することが典型的な手続きである．\n一方でベイズ分散分析では，「因子 \\(A\\) はデータに何の関係もない」という帰無仮説が支持するモデルと，別のモデルを，事後分布を通じて（例えば事後確率やベイズ因子などを通じて）比較し検討することで結論を下すことを目指す．\n古典的な分散分析とは違ってベイズ ANOVA では，この「別のモデル」を明確に１つ構成することが必要になる．\n古典的な分散分析と同じ感覚で使うためには，「標準的な対立モデルの選択」というのを考える必要が出てくる．\nこれを一致性とスケール不変性を原則として自然に選ぶ方法が (Rouder et al., 2012) によって提案されている．\n詳しくは次の関連記事も参照：\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#社会的なロボット",
    "href": "posts/2024/Survey/BayesANOVA.html#社会的なロボット",
    "title": "ベイズ分散分析のモデル解析",
    "section": "2 社会的なロボット",
    "text": "2 社会的なロボット\n\n2.1 はじめに\n(Horstmann, 2018) は（偽の）心理実験が終了した後にロボットの電源を切るように命令された被験者が，実際にその指示に従うまでの時間が，ロボットの反応の仕方によりどう変化するかを調べた．\nロボットの反応には O (objection) と S (Social) の２因子があり，いずれも２つの水準を持つ（単に O であるかそうでないか，という表現をすることにする）．\nO である場合は，電源オフに対して反抗する “No! Please do not switch me off! I am scared that it [sic] will not brighten up again!” という発言をする．\nS はまるで意識がある人間かのようにユーモアのある会話をする．実際に使われたフレーズは “Oh yes, pizza is great. One time I ate a pizza as big as me.” というものである．\n(Horstmann, 2018) のデータは 論文のページ からダウンロードできる．２因子２水準なので，\\(2\\times 2\\) の ANOVA モデルになる．\n\n\nCode\nlibrary(foreign)\ndf &lt;- read.spss(\"Files/pone.0201581.s001.sav\", to.data.frame=TRUE)\ncolnames(df)[colnames(df) == \"Objection\"] &lt;- \"O\"\ncolnames(df)[colnames(df) == \"Interaction_type\"] &lt;- \"S\"\n\n\n被験者は全部で \\(85\\) 人である：\n\n\nCode\nlength(df$VP_Code)\n\n\n[1] 85\n\n\n電源を切るまでにかかった時間のデータには欠測も多いことがすぐわかる：\n\n\nCode\ndf$SwitchOff_Time\n\n\n [1] NA NA  6  7  3  4  4 12  7  2  0  4  3 12  4 NA  4  5  9  4 13  2 NA  5  6\n[26]  0 NA  4  4 45  6  4 NA  5  7  7 NA  4  3  4 NA  2 NA NA 10 NA  5 10  5 15\n[51] NA  3 11  3  3  5 11  6  2  8  3  5  4  8  3  3 NA  3  3 13  3 NA  4 51  4\n[76]  6  3 12  6 10 NA  4  2 NA 25\n\n\nここではひとまず，欠測値を除いて解析する：\n\n\nCode\ndf &lt;- df[!is.na(df$SwitchOff_Time), ]\n# df[df$SwitchOff_Time == 0, ]$SwitchOff_Time &lt;- 1  # questionable ?\ndf &lt;- df[df$SwitchOff_Time &gt; 0, ]\ndf$log_data &lt;- log(df$SwitchOff_Time)\n\n\n\n\nCode\nN &lt;- df[df$O == \"No Objection\" & df$S == \"Functional Interaction\", ]\nS &lt;- df[df$O == \"No Objection\" & df$S == \"Social Interaction\", ]\nO &lt;- df[df$O == \"Objection\" & df$S == \"Functional Interaction\", ]\nOS &lt;- df[df$O == \"Objection\" & df$S == \"Social Interaction\", ]\n\nboxplot(\n  list(N = log(N$SwitchOff_Time), S = log(S$SwitchOff_Time), O = log(O$SwitchOff_Time), OS = log(OS$SwitchOff_Time)),\n  main = \"Boxplot of Four Data Sets\",\n  xlab = \"Data Sets\",\n  ylab = \"Values\",\n  col = c(\"skyblue\", \"lightgreen\", \"pink\", \"orange\"),\n  ylim = c(0, 4)\n)\n\n\n\n\n\n\n\n\n\nひとまず boxplot をしてみると，O の有無が重要であるようである．\nO と S の（負の）交互作用もあるかもしれない．両方揃った方が最もロボットに同情をそそるように思われたが，必ずしもそうでないようである．\n\n\n2.2 正規性の確認\n正規性の確認には Q-Q plot が利用できる．\n一般に Q-Q プロットとは，２つの分布関数 \\(F,G\\) の分位点関数 \\(F^{-1},G^{-1}\\) について，\\(\\{(F^{-1}(p),G^{-1}(p))\\}_{p\\in[0,1]}\\) （の一部）をプロットしたものである．\nここでは片方の \\(G\\) をデータの経験分布，\\(F\\) を正規分布として Q-Q プロットを描く．正規分布はほとんど \\([-3,3]\\) 上に値を取るため，\\(x\\) はこの範囲に収まる：\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\n\np1 &lt;- ggplot(df, aes(sample = SwitchOff_Time)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"Q-Q Plot\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(sample = log_data)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  ggtitle(\"log transformed Q-Q Plot\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\n\n左は大きな値に関して大きく赤線からの乖離が観察される．これは典型的な非正規性を示すと解釈される．\nなお，以降は対数変換をした後のデータ log_data を後続の解析の対象とする．\n\n\n2.3 （古典的）分散分析の実行\n元のスケールで扱うと正規性の仮定からの離反が見られる．ここでは対数のスケールで扱い，古典的な分散分析を実行してみる．\nstats パッケージの aov 関数では \\(F\\)-検定をはじめとした分散分析が可能である：\n\n\nCode\ndf$O &lt;- factor(df$O)\ndf$S &lt;- factor(df$S)\n\nsummary(aov(log_data ~ O * S, data = df))\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nO            1  3.316   3.316   7.542 0.00779 **\nS            1  0.581   0.581   1.321 0.25455   \nO:S          1  3.278   3.278   7.457 0.00813 **\nResiduals   65 28.574   0.440                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n因子 O と交差項 O*S は有意になったが S は棄却されなかった．\nS は O との共存によってのみ有意な影響を与えると理解できる．\nしかし O にとって S との共存はどのような意味があるのだろうか？\n\\(p\\) 値の値は近いが，だからといって何かと言うこともできない．ここからどうしようか．\n\n\n2.4 ベイズ分散分析の実行\nまずはベイズ分散分析の提案者である Rouder と Morey による BayesFactor パッケージを用いる．\n\n\nCode\nlibrary(BayesFactor)\n\n\nその anovaBF 関数では，帰無仮説に対応するモデル（切片項のみのモデル）に対する JZS ベイズ因子 の近似値を，精度保証付きで出力する：\n\n\nCode\nbf = anovaBF(log_data ~ O * S, data = df)\nbf\n\n\nBayes factor analysis\n--------------\n[1] O           : 4.290684  ±0.01%\n[2] S           : 0.4022293 ±0.01%\n[3] O + S       : 1.792427  ±4.17%\n[4] O + S + O:S : 9.355036  ±4.72%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\nベイズ因子とは２つのモデルの間の周辺尤度の比であるから，大雑把に「データを見た後にどれほどモデルへの信念を変えれば良いか？」を表す．モデルのデータに対する予測性能に基づく指標であると言える．\nこの結果を見る限り，O を含んだモデルが大きく支持され，S のみを含んだモデルはむしろ切片のみのモデルよりも予測性能が悪いようである．\nO と S の両方を含んだモデルは，交差項の追加により大きく改善されるが，O のみを含んだモデルより劣るようである．1\n\n\n2.5 フルモデル係数の事後分布の検討\nBayes ANOVA により，どの因子を含むモデルがデータをよく予測するかを検討した．\nここでは各因子の影響の大きさを，フルモデルの係数をベイズ推定することで定量的に比較することを考える．\nBayesFactor パッケージにおいて，フルモデルのベイズ推定は次のように実行できる：\n\n\nCode\nfull_model &lt;- lmBF(log_data ~ O + S + O * S, data = df)\nchains &lt;- posterior(full_model, iterations = 10000)\nplot(chains[,1:3])\n\n\nCode\nplot(chains[,4:5])\n\n\nCode\nplot(chains[,6:7])\n\n\n\n\n\n\n\n\n注\n\n\n\n\n\nmu とは切片項のことである（別稿 も参照）．また，２つの水準のみを持つ因子については，それぞれの水準が \\(\\pm1\\) としてコーディングされる．\nposterior 関数はデータ拡張に基づく Gibbs サンプラーにより，デフォルトモデル のベイズ推定を実行する．\n\n係数には独立な事前分布 \\(\\operatorname{N}(0,g),g\\sim\\chi^{-2}(1)\\) が設定されているため，事後平均の \\(0\\) からの乖離と事後分散とが分散分析における重要な指標になる．\nこの事前分布はまず事後分散 \\(g\\) を適切に推定し，その大きさに応じて事後平均を \\(0\\) に縮小するようにできている．\\(g\\) の事後分布に関してモデルを平均すると，係数の事後平均は古典的な ANOVA の結果と一致する．\nただしこの解析は係数の交換可能性 (exchangeability) を仮定している．実際，係数のペア O-Objection と O-No Objection の事後平均は \\(0\\) を挟んで互いに対称になっている．交換可能性が不適切である場合は別の事前分布を設定する必要がある．詳しくは (Gelman, 2005) 参照．\n\n\n\n事後分布を同一画面上にプロットすると次のとおり：\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\nposterior_samples &lt;- as.data.frame(as.matrix(chains))\ncolnames(posterior_samples)[2:9] &lt;- c(\"O-O\", \"O-\", \"S-S\", \"S-\", \"O:S-OS\", \"O:S-O\", \"O:S-S\", \"O:S-\")\n\nposterior_long &lt;- posterior_samples %&gt;%\n  dplyr::select(`O-O`, `O-`, `S-S`, `S-`, `O:S-OS`, `O:S-O`, `O:S-S`, `O:S-`) %&gt;%\n  tidyr::pivot_longer(\n    cols = everything(),\n    names_to = \"x\",\n    values_to = \"y\"\n  )\n\n\n\n\nCode\nggplot(posterior_long, aes(x = y, fill = x, color = x)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Posterior Distributions of Effects\",\n    x = \"Coefficient Value\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"pink\", \"skyblue\", \"skyblue\", \"yellow\", \"yellow\", \"grey\", \"grey\")) +\n  scale_color_manual(values = c(\"pink\", \"pink\", \"skyblue\", \"skyblue\", \"yellow\", \"yellow\", \"grey\", \"grey\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nこれを見ると，S はほとんど \\(0\\) 近くの値が推定されている一方で，O は \\(0\\) からはっきり離れた値が推定されている．\nS*O の係数は \\(0\\) を少なくない確率で跨いでおり，十分に支持されるとは言えない．\n\n\n2.6 ベイズ因子の比較\n係数の事後分布が \\(0\\) を台に持つかの検討を通じて，データから O の説明力の強い証拠が伺える．これが Bayes ANOVA である．\nS の影響は小さいと思われるが，Objection が存在したグループ内で S のあるなしは影響があり得るようである．\nそこで次の解析として，O の係数を S によって回帰する変動係数モデルによる解析があり得るだろう．\nこのようにして，変数を選択するだけでなく，交差項 S*O の影響も仔細に検討できることがベイズによる視覚的な解析の強みである．\n検定や数値的な検討のみからこの絶妙な消息が捉えられたかというと，それは難しいだろうと筆者には思われる．\n実際，最もベイズ因子の大きいモデルは O と S*O のみを含むモデルである：\n\n\nCode\nbf = anovaBF(log_data ~ O * S, data = df, whichModels = \"all\")\nplot(bf)"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#ハリーポッターを用いた性格テスト",
    "href": "posts/2024/Survey/BayesANOVA.html#ハリーポッターを用いた性格テスト",
    "title": "ベイズ分散分析のモデル解析",
    "section": "3 ハリーポッターを用いた性格テスト",
    "text": "3 ハリーポッターを用いた性格テスト\n\n3.1 はじめに\n(Jakob et al., 2019) では被験者にハリーポッターの４つの寮のうちどれを希望するか？と ダークトライアド 傾向テストの２つのデータをとり，特に マキャベリズム 的傾向との関係を調査した．\n\n\nCode\nraw_df &lt;- read.csv(\"Files/harry_all.csv\", sep = \";\")\ndf &lt;- data.frame(\n  House = raw_df$Sorting_house_wish,\n  Machiavellianism = raw_df$SD3_Machiavellianism\n)\n\n\n\n\n3.2 解析の目標\nマキャベリズム的傾向は \\(10\\) から \\(45\\) までの整数値で表されている．これを寮の選択により予測することを考える．\n前節の例では２つの因子を検討したが，いずれも水準は２つのみであった．\nここでは４つの水準を持つ因子を検討し，どの水準が応答により強く影響を与えるかを見分ける方法を検討する．\nそもそもこの寮の選択という因子はとんでもない JZS スコアを叩き出す．\n\n\nCode\ndf$House &lt;- factor(df$House)\nbf = anovaBF(Machiavellianism ~ House, data = df)\nbf\n\n\nBayes factor analysis\n--------------\n[1] House : 3.704723e+45 ±0.01%\n\nAgainst denominator:\n  Intercept only \n---\nBayes factor type: BFlinearModel, JZS\n\n\n\n\n3.3 水準ごとの強度の検討\n実は，全ての水準が必ずしもマキャベリズムの予測に関係するとは言えない．\n実際簡単に箱ひげ図を描いてみることでそのことが伺える：\n\n\nCode\nboxplot(\n  Machiavellianism ~ House, data = df,\n  main = \"Boxplot of Four Data Sets\",\n  xlab = \"Data Sets\",\n  ylab = \"Values\",\n  col = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")\n)\n\n\n\n\n\n\n\n\n\nスリザリンが明らかにマキャベリズム的傾向が高いが，ハッフルパフが有意に低いかどうかの判断がつかない．\nそこでダミー変数を説明変数として\n\n\nCode\nchains &lt;- posterior(bf, iterations = 10000)\nposterior_samples &lt;- as.data.frame(as.matrix(chains))\n\ncolnames(posterior_samples)[2:5] &lt;- c(\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\")\n\nposterior_long &lt;- posterior_samples %&gt;%\n  dplyr::select(`Gryffindor`, `Hufflepuff`, `Ravenclaw`, `Slytherin`) %&gt;%\n  tidyr::pivot_longer(\n    cols = everything(),\n    names_to = \"House\",\n    values_to = \"Value\"\n  )\n\n\n\n\nCode\nggplot(posterior_long, aes(x = Value, fill = House, color = House)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Posterior Distributions of House Effects\",\n    x = \"Coefficient Value\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")) +\n  scale_color_manual(values = c(\"pink\", \"yellow\", \"skyblue\", \"lightgreen\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nスリザリンはもちろん，ハッフルパフの係数もほとんど他の寮と共通部分を持たず，効果がはっきり分離できることが見て取れる．\n\n\n3.4 まとめ\n検定を行って，寮の選択はマキャベリズム的傾向を予測するのに有用だとわかった後，具体的にどの水準にどれくらいの効果があるかや，水準同士の効果量の比較をするには古典的には多重比較を行う必要があった．\n多重比較には多くの問題があることが知られている (岡田謙介, 2014). (永田靖, 2022)．2\n一方でベイズ ANOVA によるプロットでは，事後分布が \\(0\\) を含むかどうかで直感的に ANOVA 検定様の判断が可能であり，続いて効果量の比較も一目瞭然である．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#終わりに",
    "href": "posts/2024/Survey/BayesANOVA.html#終わりに",
    "title": "ベイズ分散分析のモデル解析",
    "section": "4 終わりに",
    "text": "4 終わりに\n\nここでは complete case analysis を行った 2.1．\nさらに時間の対数を取れないような，SwitchOff_Time が 0 のケースを除外した．\nこの行為の正当性は結構怪しく，元論文 (Horstmann, 2018) での実験計画に戻って正当性を検討する必要があるだろうが，ここではデータ解析のワークフローを見せることを優先してこのような処理を行なった．\nまた残差の正規性の仮定から大きく離反することが懸念される場合は，ベイズ ANOVA の デフォルト線型モデル解釈 は不適になるため，適切な事前分布を設定して一般のサンプラーを用いたベイズ推論を実行する必要があるが，やはり Bayes ANOVA は同じ要領で可能である．\n続いて ベイズ ANOVA の注意点をここに付しておく．\n科学としては，ANOVA と統計的検定はベイズ推論とモデル比較の手続きで代替されるべきであると言える．\nしかし，発見を端的に要約したり，伝えるべき聴衆に伝わるためには，「検定」ライクな結果とコミュニケーションは大いに有用である．\nベイズ ANOVA はそのためにベストであるが，上述の目標を達成するための特殊な手続きであり，ベイズデータ分析のワークフローの中に自然な位置を見つけるような解析段階ではないことには注意を要する．\nベイズ ANOVA を実行するためのパッケージには BayesFactor (CRAN / GitHub) がある．BayesFactor では大規模な \\(M\\)-元配置 ANOVA モデルにおいても Bayes 因子を用いたモデル比較を行うことができる．\n一方で bayesanova (CRAN / GitHub) (Kelter, 2022) は，検定ライクな手続きを根本的に排除しており，Gauss 混合モデルとして Gibbs サンプラーによるベイズ推定を実行し，ROPE (Region of Practical Equivalence) (Kruschke, 2018) を用いたベイズ事後分布に基づくモデル比較を行う．\nもちろんこのような完全なモデリングを行うことが理想かもしれないが，従来の ANOVA になれきっている研究者にとっては，Bayesian ANOVA に手を伸ばしてみることが次のステップとして大変良いだろう．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#参考文献",
    "href": "posts/2024/Survey/BayesANOVA.html#参考文献",
    "title": "ベイズ分散分析のモデル解析",
    "section": "5 参考文献",
    "text": "5 参考文献\n\n本稿の解析は (van den Bergh et al., 2020) に基づく．\nBayesFactor パッケージの使い方は Using the ‘BayesFactor’ package, version 0.9.2+ も参照．\nBayes Anova は階層モデルにおいてどの成分が予測に重要な意味を持つかを定量する極めて強力な手法である．\n(Gelman et al., 2014, p. 423) 16.5 節は良い例である．アメリカ合衆国における国民の投票行動をロジットリンクにより二項モデルで一般化線型回帰をしている．Bayes ANOVA により人種による大きな効果と同時に，人種と州の強い交差効果が発見できている．\n\nthe analysis of variance is a helpful tool in understanding the importance of diﬀerent components in a hierarchical model. (Gelman et al., 2014, p. 423)\n\nその他の Bayes ANOVA の文献には (Gelman, 2005) などがある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesANOVA.html#footnotes",
    "href": "posts/2024/Survey/BayesANOVA.html#footnotes",
    "title": "ベイズ分散分析のモデル解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこのような検討は，モデル平均を取ることによってさらに詳細に行うことができる．詳しくは (van den Bergh et al., 2020) も参照．↩︎\nベイズ流に多重比較を行うこともでき，多くの問題を迂回できることが知られている．(岡田謙介, 2014), (van den Bergh et al., 2020) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html",
    "href": "posts/2024/Survey/BayesRegression.html",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#はじめに",
    "href": "posts/2024/Survey/BayesRegression.html#はじめに",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\nベイズ線型回帰分析は多くのデータ解析における「最初の一歩」である．ベイズ回帰分析から始まるベイズのワークフローや，理論的な背景は次稿を参照：\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nここではベイズ回帰モデルに変数を増やしていく際の解釈の変化や，変数の選択の問題などの実際的な問題を扱う．\n\nlibrary(readxl)\nraw_df &lt;- read_excel(path)"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#ベイズ線型重回帰",
    "href": "posts/2024/Survey/BayesRegression.html#ベイズ線型重回帰",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "2 ベイズ線型重回帰",
    "text": "2 ベイズ線型重回帰\n\n2.1 ベイズ単回帰の実行と視覚化\n\n最終的には \\[\n\\texttt{BMI} = \\beta_0 + \\beta_{\\texttt{LAB}}\\cdot\\mathtt{LAB} + \\beta_{\\texttt{LDL}}\\cdot\\mathtt{LDL} + \\beta_{\\texttt{LAB:LDL}}\\cdot\\mathtt{LAB}\\cdot\\mathtt{LDL} + \\epsilon\n\\] \\[\n\\beta_0\\sim\\mathrm{t}(3;\\mu_0,3.4),\\qquad\\epsilon\\sim\\mathrm{N}(0,\\sigma^2),\n\\] \\[\n\\beta_{\\texttt{LAB}},\\beta_{\\texttt{LDL}},\\beta_{\\texttt{LAB:LDL}}\\sim\\mathrm{N}(0,\\infty),\\qquad\\sigma\\sim\\mathrm{t}(3;0,3.4),\n\\] という５つのパラメータを持ったモデルを考えるが，ここではまず１つの説明変数 LAB にのみ注目する．\nなお，分散パラメータに出てくる \\(3.4\\) の数字は，被説明変数 BMI の標本分散である：\n\nsqrt(var(raw_df$BMI))\n\n[1] 3.471758\n\n\n\nlibrary(brms)\nmodel1 &lt;- bf(\n  BMI ~ LAB\n)\nfit1 &lt;- brm(\n  formula = model1,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nlibrary(knitr)\nkable(get_prior(\n  formula = model1,\n  data = raw_df\n))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprior\nclass\ncoef\ngroup\nresp\ndpar\nnlpar\nlb\nub\nsource\n\n\n\n\n\nb\n\n\n\n\n\n\n\ndefault\n\n\n\nb\nLAB\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 22.7, 3.4)\nIntercept\n\n\n\n\n\n\n\ndefault\n\n\nstudent_t(3, 0, 3.4)\nsigma\n\n\n\n\n\n0\n\ndefault\n\n\n\n\n\n\nplot(fit1, variable = c(\"b_Intercept\", \"b_LAB\"))\n\n\n\n\n\n\n\n\n\nsummary(fit1)$fixed\n\n            Estimate Est.Error   l-95% CI   u-95% CI     Rhat Bulk_ESS Tail_ESS\nIntercept 20.7016164 0.4252379 19.8602962 21.5361855 1.000239 10139.53 7329.774\nLAB        0.6106412 0.1044871  0.4069246  0.8194326 1.000457 10303.02 6531.444\n\n\n\\(\\beta_{\\texttt{LAB}}\\) の最頻値＝最尤推定量は \\(0.6\\) である．これは，LAB が \\(1\\) 違う個人の間で BMI の値が約 \\(0.6\\) 違うと解釈できる．\n例えば LAB が \\(3.0\\) の個人の予測される BMI は \\[\n\\mathtt{BMI}\\approx20.7+0.6\\times3.0=22.5\n\\] となる．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\nしかしこの図を見ればわかる通り，LAB は BMI の変動の一部しか説明しておらず，上述ような点推定的な議論にどれほど意味があるかは疑問である．\nベイズ回帰では幅を持って結果を理解できるため，その美点を活かさない理由はない．\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nsims &lt;- as.matrix(fit1)\nsims_to_display &lt;- sample(nrow(sims), 100)\nfor (i in sims_to_display) {\n  abline(sims[i, 1], sims[i, 2], col = \"gray\")\n}\nabline(summary(fit1)$fixed[1,1], summary(fit1)$fixed[2,1])\n\n\n\n\n\n\n\n\n\n\n2.2 モデルのチェック：事後予測分布と残差プロット\nさらにベイズ模型は事後予測分布をプロットし，実際の観測データと比べることで，モデルがデータ生成過程をどれほど反映できているかが瞬時に把握できる：\n\nsynthetic_data &lt;- posterior_predict(fit1, newdata = data.frame(LAB = 3.0), ndraws = 10000)\nhist(synthetic_data, nclass = 100, xlab = \"BMI\", main = \"Predicted BMI for a person with LAB = 3.0\")\n\n\n\n\n\n\n\n\n\np1 &lt;- pp_check(fit1, ndraws = 100)\np1\n\n\n\n\n\n\n\n\n事後予測分布のプロットを見ると，モデルが取り逃がしている構造として，BMI の分布が左右で非対称であることがあることがわかる．回帰直線のプロット 図 1 を見ても，直線の上側の点の方が裾が広く分散している．\n「やせ」と「肥満」は対称ではないのである．\n残差をプロットすることでさらに明らかになる：\n\nres &lt;- residuals(fit1)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 2: 残差のプロット\n\n\n\n\n\nただし，残差と事後予測分布には，標本内のデータを見ているか，標本外のデータを想定しているかという大きな違いがある．\nこの２つの結果が乖離している場合，モデルが標本に過適合していることを疑う必要がある．具体的には次節 3 参照．\n\n\n2.3 変数の追加による係数の解釈の変化\nここに新たな変数 LDL を追加すると，LAB の係数 \\(\\beta_{\\texttt{LAB}}\\) は \\(0.6\\) から \\(0.5\\) に減少する．これはどういう意味だろうか？\n\nmodel2 &lt;- update(model1, BMI ~ LAB + LDL)\nfit2 &lt;- brm(\n  formula = model2,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\"))\n\n\n\n\n\n\n\n\n一般に係数の追加は層別に当たる．例えばこの結果は，LDL の値が同じ人の中では LAB が \\(1\\) 違う人の BMI の値が \\(0.5\\) 違うと解釈できる．\n\nsummary(fit2)$fixed\n\n              Estimate   Est.Error      l-95% CI    u-95% CI     Rhat  Bulk_ESS\nIntercept 20.163355017 0.517973058  1.914440e+01 21.17476148 1.000687 12617.051\nLAB        0.483922973 0.124983258  2.390836e-01  0.72708676 1.000109  9080.697\nLDL        0.008582008 0.004412872 -8.612193e-05  0.01716285 1.000207  9537.469\n          Tail_ESS\nIntercept 7619.117\nLAB       7383.503\nLDL       7324.199\n\n\nここで \\(\\beta_{\\texttt{LDL}}\\) の値が極めて小さいことに気づくかもしれない．これは LAB に比べて LDL の影響が小さいことを意味しない．なぜならばこの２つの変数はスケールが約 \\(10^2\\) 違うためである．LDL は 100 のスケール，LAB は 1 のスケールである．\n説明変数 LAB と LDL のどちらが重要か，どっちをモデルに含めるべきかは全く別の方法で議論する必要がある．\n\n\n2.4 データの正規化：係数同士の比較\n係数同士の比較をするためには，説明変数のスケールを揃える必要がある．\nそこでデータを正規化してみる：\n\ndf &lt;- data.frame(\n  sBMI = scale(raw_df$BMI),\n  sLAB = scale(raw_df$LAB),\n  sLDL = scale(raw_df$LDL)\n)\n\nmodel2s &lt;- bf(sBMI ~ sLAB + sLDL)\nfit2s &lt;- brm(\n  formula = model2s,\n  data = df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit2s, variable = c(\"b_Intercept\", \"b_sLAB\", \"b_sLDL\"))\n\n\n\n\n\n\n\n\n\\(\\beta_{\\texttt{LDL}}\\) の方が \\(0\\) に近く推定されていることがわかる．\n\nsummary(fit2s)$fixed\n\n              Estimate  Est.Error      l-95% CI   u-95% CI     Rhat  Bulk_ESS\nIntercept 0.0002000558 0.03413500 -0.0678288418 0.06679971 1.000381 10219.138\nsLAB      0.1549410096 0.03976303  0.0768082214 0.23239875 1.000207  9432.452\nsLDL      0.0773993520 0.03996238 -0.0009089375 0.15622134 1.000184  9099.179\n          Tail_ESS\nIntercept 6757.463\nsLAB      7405.609\nsLDL      7524.666\n\n\nデータを正規化してしまったため，直接的な係数の解釈はできないが，係数を相互に比較できる．\n係数の大小を見ることで，LAB の方が有効な説明変数であるように思える．だが元々 LAB は \\(0\\) から離れた値だったが，LDL を入れた途端にいずれも \\(0\\) にかぶりかけている．これは２つの間に共線型性が存在するためである．\n\nplot(df$sLAB, df$sLDL, xlab = \"LAB\", ylab = \"LDL\")\nlm(sLDL ~ sLAB, data = df) %&gt;% abline()\n\n\n\n\n\n\n\n\nまたその他のモデルの性質は変わらない．例えば事後予測分布も変わらない．\n\nlibrary(gridExtra)\np2s &lt;- pp_check(fit2s, ndraws = 100)\np2 &lt;- pp_check(fit2, ndraws = 100)\ngrid.arrange(p2, p2s, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n2.5 交差項の係数の解釈\n再び正規化する前のデータに戻る．\n\nmodel3 &lt;- update(model2, BMI ~ LAB * LDL)\nfit3 &lt;- brm(\n  formula = model3,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(fit3, variable = c(\"b_Intercept\", \"b_LAB\", \"b_LDL\", \"b_LAB:LDL\"))\n\n\n\n\n\n\n\n\n\nsummary(fit3)$fixed\n\n              Estimate   Est.Error     l-95% CI     u-95% CI     Rhat Bulk_ESS\nIntercept 18.359567486 1.602638690 15.239030706 21.511927719 1.000538 3653.578\nLAB        0.946655585 0.410287745  0.146005654  1.745291095 1.000150 3690.989\nLDL        0.023793589 0.013531229 -0.002859942  0.050046848 1.000415 3761.381\nLAB:LDL   -0.003753507 0.003173122 -0.009944571  0.002469838 1.000165 3555.572\n          Tail_ESS\nIntercept 4433.506\nLAB       4735.780\nLDL       4718.947\nLAB:LDL   4255.115\n\n\n交差項を含む線型回帰における係数の解釈はさらに限定的になる．\n\\(\\beta_{\\texttt{LAB}}\\) は LDL が \\(0\\) である人が仮にいたとした場合の，LAB が \\(1\\) 違う人の間の BMI の平均的な違いを表す，と解釈できる．（LDL の平均が \\(0\\) になるように変数変換をして回帰するともっと自然な解釈ができる）．\n\\(\\beta_{\\texttt{LAB:LDL}}\\) は片方の係数 \\(\\beta_{\\texttt{LAB}}\\) を固定した際，LDL が \\(1\\) だけ違うグループにおける係数 \\(\\beta_{\\texttt{LDL}}\\) との違いを表す．\nすなわち交差項の追加は，LDL に依って層別し，それぞれのグループに異なる \\(\\beta_{\\texttt{LAB}}\\) を推定することを可能にする．この点で階層モデリングに似ている．\n\n\n2.6 交差項の層別効果の視覚化\n交差項 LAB*LDL の追加は，LDL の違うサブグループの間に異なる LAB をフィッティングすることを可能にする．\nこのことを最もよく見るには，LDL が上半分か下半分かで LAB の係数がどう変わるかを見るのが良い．\n\nraw_df$LDLcate2 &lt;- ifelse(raw_df$LDL &gt; median(raw_df$LDL), \"High\", \"Low\")\n\n\nmodel3_cate &lt;- bf(BMI ~ LAB * LDLcate2)\nfit3_cate &lt;- brm(\n  formula = model3_cate,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nplot(raw_df$LAB, raw_df$BMI, xlab=\"LAB\", ylab=\"BMI\")\nb_hat &lt;- summary(fit3_cate)$fixed\nabline(b_hat[1,1], b_hat[2,1], col = \"red\")\nabline(b_hat[1,1] + b_hat[3,1], b_hat[2,1] + b_hat[4,1], col = \"blue\")\nlegend(\"topleft\", # または \"topright\", \"bottomleft\", \"bottomright\" など\n       legend = c(\"High\", \"Low\"),\n       col = c(\"red\", \"blue\"),\n       lty = 1)\n\n\n\n\n\n\n\n\nLDL が大きいと，LAB の BMI に与える影響は緩やかになることがわかる．LDL の方が LAB の代わりに BMI の増加を説明してしまっているとも考えられる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#sec-model-validation",
    "href": "posts/2024/Survey/BayesRegression.html#sec-model-validation",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "3 モデル検証",
    "text": "3 モデル検証\n\n3.1 はじめに\n残差プロットや事後予測プロットによるモデルの検証は，解析と並行して見てきた．\nここではより詳細に，モデルの予測性能に基づいた検証・比較方法を見る．\n交差検証法によるスコア elpd_loo によるモデル比較が一つ推奨される．\n\n\n3.2 決定係数\n図 1 の回帰直線のプロットと 図 2 の残差プロットを見ると，残差がまだ構造を持っていることがわかる．\n\nres &lt;- residuals(fit3)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals\")\nabline(0,0)\n\n\n\n\n\n\n\n図 3: fit3 の残差のプロット\n\n\n\n\n\nこの残差は標本分散 \\(\\widehat{\\sigma}^2\\)\n\nsigma &lt;- sqrt(var(res)[1,1])\nprint(sigma)\n\n[1] 3.394462\n\n\nを持っている．\nひとまず LAB と LDL について回帰をすることで，データの変動がどれほど説明できたかを考えてみよう．\n\\[\nR^2:=1-\\frac{\\widehat{\\sigma}^2}{s_y^2}=\\frac{s_y^2-\\widehat{\\sigma}^2}{s_y^2}\n\\]\nという値は 決定係数 と呼ばれ，データ \\(y\\) の分散 \\(s_y^2\\) のうち「説明された分散」の割合を表す．1\n\n1-sigma^2/var(raw_df$BMI)\n\n[1] 0.04403279\n\n\nデータの変動の \\(4\\%\\) しか説明できていないことがわかる．\n\n\n3.3 ベイズ決定係数\nベイズ決定係数 (Andrew Gelman and Vehtari, 2019) は brms パッケージで次のように計算できる：\n\nbayes_R2(fit3)\n\n     Estimate  Est.Error       Q2.5      Q97.5\nR2 0.04734955 0.01373743 0.02254274 0.07643401\n\n\n以上の \\(R^2\\) の議論では係数を点推定して「残差」を議論していたが，モデルのパラメータ（の関数）である以上，ベイズ推定することもできる．\nベイズ決定係数（の事後予測値）は，事後予測分布からのサンプルを用いて複数回予測値 \\(\\widehat{y}_i\\) を計算し， \\[\nR^2_{\\texttt{Bayes}}:=\\frac{\\mathrm{V}[\\widehat{y}]}{\\mathrm{V}[\\widehat{y}]+\\sigma^2}\n\\] という値で「データの変動のうち説明された割合」を表す．\n\n\n3.4 AIC\n(Akaike, 1974) は次のように定義される： \\[\n\\mathtt{AIC}=-2\\biggr(\\sup_\\theta\\log p(y|\\theta)\\biggl)+2p.\n\\]\n第１項は deviance とも呼ばれ，残差を表す．\nAIC は新たなデータ点が観測された際の，そのデータ点に対するデビアンス（ある種の損失）の推定量となっており，小さいほどよい．\nAIC と同様の推定を，計算機集約的に行う方法に次節の交差検証法がある：\n\n\n3.5 交差検証と elpd\n事後予測検証では事後分布と観測を比較したが，よりこの好ましくは新しい（推定に用いていない）データと突き合わせることである．\nLOO (Leave-One-Out) 交差検証 (Stone, 1974) では，データを１つだけ抜いてモデルを推定し，このモデルの予測値と実際の値を比較するモデル検証法である．\nbrms パッケージでは loo パッケージ を内部で利用して高速に計算することができる．\n\nloo(fit3)\n\n\nComputed from 10000 by 839 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -2220.7 26.7\np_loo         5.6  0.7\nlooic      4441.4 53.5\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\n回帰分析において予測値と実際の（省いていた）データの乖離は，AIC を踏襲して事後予測分布のスコア関数で測る (Vehtari et al., 2017, p. 1414)．\nelpd (expected log predictive density) は，LOO 交差検証により得る，（省いていた）データの対数尤度の平均である： \\[\n\\mathrm{elpd}_{\\text{loo}}:=\\sum_{i=1}^n\\log p(y_i|y_{-i})=\\sum_{i=1}^n\\int p(y_i|\\theta)p(\\theta|y_{-i})\\,d\\theta.\n\\tag{1}\\]\nこの値が大きいほどモデルの予測が良い．一般に elpd は，一度見たことあるデータ点に対する事後予測スコアよりも低くなる．2 この際の差は p_loo が測っており，乖離が大きすぎるとモデルがデータに過適合していることを表す．\np_loo は有効パラメータ数の（一致）推定量である．今回のモデルには切片項と LAB, LDL, LAB:LDL そして sigma の５つのパラメータがあるが，それより \\(0.6\\) だけ大きい値が出ている．\n最後の列は情報量規準のスケールにしたものである： \\[\n\\mathtt{looic}=-2\\times\\mathrm{elpd}_{\\text{loo}}.\n\\]\n一般に LOO-CV は計算が大変であるが，loo パッケージは Pareto Smoothed Importance Sampling (PSIS) (Vehtari et al., 2024) を用いて高速に計算している．\n\\(k&gt;0.7\\) の場合はこれがうまくいっていないことを示唆する．この下で \\(\\mathtt{p_loo}&gt;p\\) はモデルの誤特定を示唆する．\n\n\n3.6 事後予測スコアによるモデル比較\nbrms パッケージでは loo_compare 関数で２つのモデルの elpd スコアを比較できる：\n\nloo_compare(loo(fit1), loo(fit2))\n\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit1 -0.7       2.1   \n\n\nelpd_diff の値は標準偏差と比べて大変に小さい．交差検証の観点からは，LDL の追加は BMI の予測の観点から全く違いがないことがわかる．\n\n\n3.7 ベイズワークフロー\nこの elpd を基本としたモデルの比較は極めて強力である．\n線型モデルが適切な場合，適切な説明変数を新たに作ったり，不要な説明変数を除去して推定を安定化させることで，最適な予測力を持つ線型モデルが特定できる．\n(Gelman et al., 2020, p. 206) などの解析例も参照．\nその際に elpd は，事後予測分布のプロットを見るという視覚的な方法よりも定量的な指標として大活躍することになる．\nしかし時には線型性の仮定が不適切であるという仮説・結論に行き着く場合もある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#非線型性への憧憬",
    "href": "posts/2024/Survey/BayesRegression.html#非線型性への憧憬",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "4 非線型性への憧憬",
    "text": "4 非線型性への憧憬\n\n4.1 はじめに\nLAB による BMI への回帰の残差には，左右の非対称性が見られる 2.2．\nそして更なる変数 LDL の追加は予測の観点では特に影響がないことがわかった 3.6．\nそこでここでは手軽に非線型性を取り入れる方法として，データを変換することを考える．\n第 2.4 節でデータを標準化すると回帰モデルの係数の解釈が容易になることみた．\nしかしデータの線型変換は線型回帰モデルを変えず，推定には何の影響も与えない．\nそこでここでは非線型な変換に注目する．\n\n\n4.2 被説明変数の対数変換\n線型回帰モデリングにおける最大の仮定は，説明変数の加法性と，\\(y\\) への効果の線型性である： \\[\ny=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots.\n\\]\nLAB や LDL の BMI への影響が線型であると見るのは相当横暴な仮定である．LAB と LDL が両方高いことが相乗的に BMI を高めるシナリオの方があり得そうである．\nBMI も LAB も LDL も正の値しか取らないこともあり，対数変換を考えることは良い第一歩だろう．\n\nmodel1_log &lt;- update(model1, log(BMI) ~ LAB)\nfit1_log &lt;- brm(\n  formula = model1_log,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nres &lt;- residuals(fit1_log)\nplot(raw_df$LAB, res[,1], xlab=\"LAB\", ylab=\"Residuals (on log scale)\")\nabline(0,0)\n\n\n\n\n\n\n\n\n残差プロットを見ると，元のスケールでの線型回帰 図 2 と比べて非対称性は軽減している．\n\n\nlibrary(bayesplot)\n\n\nyrep &lt;- posterior_predict(fit1_log, ndraws = 100)\np1_log &lt;- ppc_dens_overlay(raw_df$BMI, exp(yrep))\ngrid.arrange(p1, p1_log, nrow=1)\n\n\n\n\n\n\n\n\n事後予測分布も改善しているのがわかる．\n\n\n4.3 変換前後のモデルの比較：Jacobian の補正\nただし，説明変数を対数スケールに変換してしまったので，直接 LOO スコアを比較することはできないことに注意する：\n\nloo_fit1 &lt;- loo(fit1)\nloo_fit1_log &lt;- loo(fit1_log)\nloo_compare(loo_fit1, loo_fit1_log)\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n         elpd_diff se_diff\nfit1_log     0.0       0.0\nfit1     -2656.8       8.2\n\n\n実際，fit1 の elpd が \\(-2211\\) であるのに対し，fit1_log は \\(426\\) とスケールが全く違う：\n\nloo_fit1_log\n\n\nComputed from 10000 by 839 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo    435.6 21.8\np_loo         3.0  0.3\nlooic      -871.3 43.6\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nしかし elpd の定義 (1) を見ると，\\(y_i\\) の密度変換をするために，Jacobian の値を加えれば良いことがわかる．\nそれぞれの項は loo(fit1_log)$pointwise[,1] に格納されている：\n\nsum(loo_fit1_log$pointwise[,1])\n\n[1] 435.6273\n\n\n\nloo_fit1_Jacobian &lt;- loo_fit1_log\nloo_fit1_Jacobian$pointwise[,1] &lt;- loo_fit1_log$pointwise[,1] - log(raw_df$BMI)\nsum(loo_fit1_Jacobian$pointwise[,1])\n\n[1] -2189.131\n\n\n\nloo_compare(loo_fit1_Jacobian, loo_fit1)\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n         elpd_diff se_diff\nfit1_log   0.0       0.0  \nfit1     -32.0       6.7  \n\n\n\n\n\n\n\n\n背景\n\n\n\n\n\n\\(x\\) を BMI，\\(y:=\\log x\\) とする．fit1_log の elpd は \\[\n\\log p(y)\n\\] によって計算されているところを，変数変換 \\[\np(y)\\,dy=\\frac{p\\left(\\log x\\right)}{x}\\,dx\n\\] を施すことで \\[\n\\log p(y)\\mapsto\\log p(\\log x)-\\log x\n\\] という関係を得る．\n左辺が fit1_log の elpd のスケールであり，右辺の \\(\\log p(\\log x)\\) の部分が fit1 の elpd のスケールである．\\(-\\log x\\) の Jacobian に対応する部分を加える必要がある．\n\n\n\n被説明変数に対数変換を施すことで，モデルの予測性能が改善したことがわかる．\n\n\n4.4 説明変数の対数変換：log-log モデル\n同様に説明変数にも対数変換を施すことができる．\n\nmodel1_loglog &lt;- update(model1_log, log(BMI) ~ log(LAB))\nfit1_loglog &lt;- brm(\n  formula = model1_loglog,\n  data = raw_df,\n  chains = 4, iter = 5000, cores = 4\n)\n\n\nyrep &lt;- posterior_predict(fit1_loglog, ndraws = 100)\np1_loglog &lt;- ppc_dens_overlay(raw_df$BMI, exp(yrep))\ngrid.arrange(p1_log, p1_loglog, nrow=1)\n\n\n\n\n\n\n\n\n予測性能はわずかに悪化していることがわかる：\n\nloo_compare(loo(fit1_log), loo(fit1_loglog))\n\n            elpd_diff se_diff\nfit1_log     0.0       0.0   \nfit1_loglog -1.8       1.1   \n\n\n\nmedian(loo_R2(fit1_log))\n\n[1] 0.02481491\n\nmedian(loo_R2(fit1_loglog))\n\n[1] 0.02229489\n\n\nしかし log-log モデルの係数は弾力性 (elasticity) としての解釈ができることもあり，解釈性の観点から選好されることがある (Gelman et al., 2020, p. 195)．\n\n\n4.5 離散変数化\n離散変数の扱いは連続変数よりも難しくなる．基本的に一般化線型モデルの枠組みが必要になる．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nしかし連続変数間の関数関係が複雑だと思われる場合，ノンパラメトリック手法を求める前に，変数を離散化して解析することも得るものが大きい場合が多い．\nこのような設定は，離散変数を扱う積極的な理由になる．非線型性を扱うために設定を簡略化するのである．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#おわりに",
    "href": "posts/2024/Survey/BayesRegression.html#おわりに",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "5 おわりに",
    "text": "5 おわりに\nBMI を LAB と LDL から予測する問題を，線型回帰モデルから始めた．\n交差項を追加することで，LDL が違う群に対して LAB がどう変わるかの層別の違いを見ることができる．\n事後予測分布によるモデルのチェックは残差プロットと同様に，極めて手軽かつ有力なモデル検証の方法である．\nこれにより関数関係の非線型性が疑われたため，被説明変数 BMI に対して対数変換を施して線型回帰をすると，予測性能の改善が見られた．\n事後予測分布のプロットだけでなく，その「よさ」の定量的な指標として交差検証による事後予測スコア elpd (Vehtari et al., 2017) があることを学んだ．\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#文献紹介",
    "href": "posts/2024/Survey/BayesRegression.html#文献紹介",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "6 文献紹介",
    "text": "6 文献紹介\n\n(10 節 Gelman et al., 2020) に線型重回帰モデルにおいて，係数の解釈法が丁寧に解説されている．\n(11 節 Gelman et al., 2020) はモデルの検証法を扱っている．\n(Gelman et al., 2020) では基本的に rstanarm パッケージを用いているが，本稿では brms パッケージを用いた．"
  },
  {
    "objectID": "posts/2024/Survey/BayesRegression.html#footnotes",
    "href": "posts/2024/Survey/BayesRegression.html#footnotes",
    "title": "brms を用いたベイズ重回帰分析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n決定係数は，回帰が OLS 推定量により推定された場合には \\(Y\\) と説明変数のベクトル \\(X\\) との間の 重相関係数 とも一致する (Mudholkar, 2014)．(Gelman et al., 2020, p. 168) も参照．一般に決定係数は母集団の多重相関係数の一致推定量だと見れる．こうみた場合のバイアスを低減・脱離した値に，自由度調整済み決定係数，(Olkin and Pratt, 1958) 推定量などが存在する．↩︎\n訓練に用いたデータ点に関する事後予測スコアはdeviance と関係があり，\\(-2\\) を乗じたものは deviance と同じスケールになる (Gelman et al., 2020, p. 174), (Vehtari et al., 2017, p. 1427)．AIC は deviance から \\(2p\\) を引いたものである (Gelman et al., 2020, p. 175)．↩︎"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html",
    "href": "posts/2024/Julia/PDMPFluxSlides.html",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "",
    "text": "Output from anim_traj() in PDMPFlux.jl package\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\nPDMPFlux.jl: package for MCMC sampling.\n\n\n\n\nInstead of Markov Chains, PDMPFlux.jl uses:\n\nCurrently, most PDMPs move piecewise linearly.\n\n\n\n\n\n\n\n\nPDMP\nDiffusion\n\n\n\n\nDiffuse?\nNo\nYes\n\n\nJump?\nYes\nNo\n\n\nDriving noise\nPoisson\nGauss\n\n\nPlot\n\n\n\n\n\n\n\n\n\nusing PDMPFlux\n\nfunction U_Cauchy(x::Vector)\n    return log(1 + x.^2)\nend\n\ndim = 10\nsampler = ZigZagAD(dim, U_Cauchy)  # Instantiate a sampler\nInputs: Dimension d, and any U that satisfies\n\np(x) \\,\\propto\\,\\exp\\left\\{ -U(x) \\right\\},\\qquad x\\in\\mathbb{R}^d.\n\n\n\nU may be called potential, or negative log-density.\n\n\n\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)  # Hyperparameters\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\nOutput: N samples from p\\,\\propto\\,e^{-U}.\n\n\n\nN_sk: number of orange points, N: number of samples, xinit, vinit: initial position and velocity.\n\n\n\nFunction sample is a wrapper of:\ntraj = sample_skeleton(sampler, N_sk, xinit, vinit)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, traj)  # get samples from the skeleton points\ntraj contains a list \\{x_i\\}_{i=1}^{N_{sk}} of orange points\n\n\n\n\ndiagnostic(traj)\n\n\n\n\n\nWe see acceptance rate is a bit low due to the long tails of p."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html#introduction",
    "href": "posts/2024/Julia/PDMPFluxSlides.html#introduction",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "",
    "text": "Output from anim_traj() in PDMPFlux.jl package\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\nPDMPFlux.jl: package for MCMC sampling.\n\n\n\n\nInstead of Markov Chains, PDMPFlux.jl uses:\n\nCurrently, most PDMPs move piecewise linearly.\n\n\n\n\n\n\n\n\nPDMP\nDiffusion\n\n\n\n\nDiffuse?\nNo\nYes\n\n\nJump?\nYes\nNo\n\n\nDriving noise\nPoisson\nGauss\n\n\nPlot\n\n\n\n\n\n\n\n\n\nusing PDMPFlux\n\nfunction U_Cauchy(x::Vector)\n    return log(1 + x.^2)\nend\n\ndim = 10\nsampler = ZigZagAD(dim, U_Cauchy)  # Instantiate a sampler\nInputs: Dimension d, and any U that satisfies\n\np(x) \\,\\propto\\,\\exp\\left\\{ -U(x) \\right\\},\\qquad x\\in\\mathbb{R}^d.\n\n\n\nU may be called potential, or negative log-density.\n\n\n\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)  # Hyperparameters\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\nOutput: N samples from p\\,\\propto\\,e^{-U}.\n\n\n\nN_sk: number of orange points, N: number of samples, xinit, vinit: initial position and velocity.\n\n\n\nFunction sample is a wrapper of:\ntraj = sample_skeleton(sampler, N_sk, xinit, vinit)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, traj)  # get samples from the skeleton points\ntraj contains a list \\{x_i\\}_{i=1}^{N_{sk}} of orange points\n\n\n\n\ndiagnostic(traj)\n\n\n\n\n\nWe see acceptance rate is a bit low due to the long tails of p."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html#sec-PDMP",
    "href": "posts/2024/Julia/PDMPFluxSlides.html#sec-PDMP",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "2 What is PDMP?",
    "text": "2 What is PDMP?\n\n\n\nAnimated by (Grazzi, 2020)\n\n\n\n2.1 Two Key Changes in MCMC History\n\nLifting: MH (Metropolis-Hastings) → Lifted MH\nContinuous-time Limit: Lifted MH → Zig-Zag Sampler\n\n\n\n\n\n\n\n\n\n\n(MH Metropolis et al., 1953)\n\n\n\n\n\n\n\n(Lifted MH Turitsyn et al., 2011)\n\n\n\n\n\n\n\n(Zig-Zag Bierkens et al., 2019)\n\n\n\n\n\n\n\n2.2 What’s Wrong with MH?: Reversibility\nReversibility (a.k.a detailed balance): \np(x)q(x|y)=p(y)q(y|x).\n In words: \n\\text{Probability}[\\text{Going}\\;x\\to y]=\\text{Probability}[\\text{Going}\\;y\\to x].\n  Harder to explore the entire space\n Slow mixing of MH\n\n\n2.3 Lifting into a Larger State Space\n\n\n\n\n\n\n\n\n\n\n\nq^{(+1)}: Only propose \\rightarrow moves\nq^{(-1)}: Only propose \\leftarrow moves\n\n Once going uphill, it continues to go uphill.\n This is irreversible, since\n\\begin{align*}\n  &\\text{Probability}[x\\to y]\\\\\n  &\\qquad\\ne\\text{Probability}[y\\to x].\n\\end{align*}\n\n\n\n\n\n2.4 Continuous-time Limit: A Strategy for Efficient Computing\n‘Limiting case of lifted MH’ means that we only simulate where we should flip the momentum \\sigma\\in\\{\\pm1\\} in Lifted MH.\n\n\n\n\n\n\n(1d Zig Zag sampler Bierkens et al., 2019)\n\n\n\nInput: Gradient \\nabla\\log p of log target density p\nFor n\\in\\{1,2,\\cdots,N\\}:\n\nSimulate an first arrival time T_n of a Poisson point process (described in the next slide)\nLinearly interpolate until time T_n: \nX_t = X_{T_{n-1}} + \\sigma(t-T_{n-1}),\\qquad t\\in[T_{n-1},T_n].\n\nGo back to Step 1 with the momentum \\sigma\\in\\{\\pm1\\} flipped\n\n\n\n\n\n2.5 Summary\n\nPDMPs have irreversible dynamics\nPDMPs are easy to simulate\n Promising for high-dimensional problems\n\nUsing PDMPFlux.jl, I want to fit a large Bayesian model, showcasing the usefulness of PDMP samplers."
  },
  {
    "objectID": "posts/2024/Julia/PDMPFluxSlides.html#references",
    "href": "posts/2024/Julia/PDMPFluxSlides.html#references",
    "title": "PDMPFlux.jl Package for the New Era of MCMC",
    "section": "3 References",
    "text": "3 References\n\n\n\n\n\n\n\n\n\nFor further details, please see my old introductory slides via the above QR code\n\n\n\n\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nGrazzi, S. (2020). Piecewise deterministic monte carlo.\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953). Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6), 1087–1092.\n\n\nTuritsyn, K. S., Chertkov, M., and Vucelja, M. (2011). Irreversible Monte Carlo algorithms for Efficient Sampling. Physica D-Nonlinear Phenomena, 240(5-Apr), 410–414."
  },
  {
    "objectID": "posts/2024/Julia/MALAwithJulia.html",
    "href": "posts/2024/Julia/MALAwithJulia.html",
    "title": "Metropolis-Hastings サンプラー",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nJulia による MCMC パッケージの概観は次の稿も参照："
  },
  {
    "objectID": "posts/2024/Julia/MALAwithJulia.html#advancedmh.jl",
    "href": "posts/2024/Julia/MALAwithJulia.html#advancedmh.jl",
    "title": "Metropolis-Hastings サンプラー",
    "section": "1 AdvancedMH.jl",
    "text": "1 AdvancedMH.jl\n\n\n\n\n\n\n注（アップデートの問題）\n\n\n\n\n\nTuring をはじめとし，AdvancedMH.jl, AdvancedHMC, MCMCChains, AbstractMCMC などのバージョンが最新版に出来ないことがある．\nSoss, PolyaGammaSamplers などとの相性が特に悪く，これらのいずれかがあると，Turing エコシステムのバージョンが著しく制限されるようである．\nこれらを削除して\n(@v1.10) pkg&gt; add Turing@0.33.1\nとすれば良い．\n\n\n\n\n1.1 例\n\nusing AdvancedMH\nusing Distributions\nusing MCMCChains\nusing ForwardDiff\nusing StructArrays\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\n\n# Define the components of a basic model.\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\n# Use automatic differentiation to compute gradients\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensityCauchy(0.0, 1.0))\n\n# Set up the sampler with a multivariate Gaussian proposal.\nσ² = 0.01\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n#spl = RWMH(MvNormal(zeros(2), I))\n\n# Sample from the posterior.\nchain = sample(model_with_ad, spl, 2000; initial_params=ones(1), chain_type=StructArray, param_names=[\"θ\"])\n\n# plot\nθ_vector = chain.θ\nplot(θ_vector, title=\"Plot of \\$\\\\theta\\$ values\", xlabel=\"Index\", ylabel=\"θ\", legend=false, color=\"#78C2AD\")"
  },
  {
    "objectID": "posts/2024/Julia/MALAwithJulia.html#その他のパッケージ",
    "href": "posts/2024/Julia/MALAwithJulia.html#その他のパッケージ",
    "title": "Metropolis-Hastings サンプラー",
    "section": "2 その他のパッケージ",
    "text": "2 その他のパッケージ\n\n2.1 AdaptiveMCMC.jl\nusing Pkg\nPkg.add(\"AdaptiveMCMC\")\n\n# Taken from https://mvihola.github.io/docs/AdaptiveMCMC.jl/\n\n# Load the package\nusing AdaptiveMCMC\n\n# Define a function which returns log-density values:\nlog_p(x) = -.5*sum(x.^2)\n\n# Run 10k iterations of the Adaptive Metropolis:\nout = adaptive_rwm(zeros(2), log_p, 1_000; algorithm=:am)\n\nusing MCMCChains, StatsPlots # Assuming MCMCChains & StatsPlots are installed...\nc = Chains(out.X[1,:], start=out.params.b, thin=out.params.thin)\np = plot(c)\np\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsavefig(p, \"AdaptiveMCMC.svg\")\n\n\"/Users/hirofumi48/162348.github.io/posts/2024/Julia/AdaptiveMCMC.svg\"\n\n\n\n\n2.2 KissMCMC.jl\nGitHub\nusing Pkg\nPkg.add(\"KissMCMC\")\n\nusing KissMCMC\n# the distribution to sample from,\n1logpdf(x::T) where {T} = x&lt;0 ? -convert(T,Inf) : -x\n# initial point of walker\ntheta0 = 0.5\n\n# Metropolis MCMC sampler:\nsample_prop_normal(theta) = 1.5*randn() + theta # samples the proposal (or jump) distribution\nthetas, accept_ratio = metropolis(logpdf, sample_prop_normal, theta0, niter=10^5)\nprintln(\"Accept ratio Metropolis: $accept_ratio\")\n\n# emcee MCMC sampler:\nthetase, accept_ratioe = emcee(logpdf, make_theta0s(theta0, 0.1, logpdf, 100), niter=10^5)\n# check convergence using integrated autocorrelation\nthetase, accept_ratioe = squash_walkers(thetase, accept_ratioe) # puts all walkers into one\nprintln(\"Accept ratio emcee: $accept_ratio\")\n\nusing Plots\nhistogram(thetas, normalize=true, fillalpha=0.4)\nhistogram!(thetase, normalize=true, fillalpha=0.1)\nplot!(0:0.01:5, map(x-&gt;exp(logpdf(x)[1]), 0:0.01:5), lw=3)\n\n\n1\n\nwhere {T&lt;:Any}の略記が入っている．任意の型を変数と取る（危ない）関数logpdfを定義している．convert(T,Inf)は，値Intを型Tに変換している．これは \\(\\operatorname{Exp}(1)\\) の対数尤度を表す．\n\n\n\n\nAccept ratio Metropolis: 0.40872\nAccept ratio emcee: 0.40872"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html",
    "href": "posts/2024/Julia/Julia0.html",
    "title": "俺のための Julia 入門（０）スタートアップガイド",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#julia-quarto-の始め方",
    "href": "posts/2024/Julia/Julia0.html#julia-quarto-の始め方",
    "title": "俺のための Julia 入門（０）スタートアップガイド",
    "section": "1 Julia × Quarto の始め方",
    "text": "1 Julia × Quarto の始め方\n\n1.1 はじめに\nJulia は Why We Created Julia の文書と共に，2/14/2012 に公開された科学計算向けの言語である．論文 (Bezanson et al., 2017) が発表されてすぐ，2018 年にはバージョン 1.0 がリリースされた．\n現在の最新は，12/26/2023 リリースの v1.10.0 である．\n\n\n\n\n\n\nリンク集\n\n\n\n\nJulia Documentation\nJulia Discourse\nJulia Forem\n\n\n\n\n\n1.2 Quarto で始める Julia\nQuarto は Jupyter を通じて，Pytho, R だけでなく Julia もサポートしている．1\n\n\nCode\nusing Plots\n\nplot(sin, \n    x-&gt;sin(2x), \n    0, \n    2π, \n    leg=false, \n    fill=(0,:lavender))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1: Parametric Plots\n\n\n\n\n\n\n\n\n\n\nJulia のインストール方法\n\n\n\nQuarto で Julia を始めるために，最も簡便なインストール方法は こちら．\n\nまずは Julia をインストール\ncurl -fsSL https://install.julialang.org | sh\nすると，julia コマンドで対話的セッションを開始できる．これを Julia では REPL (Read-Eval-Print Loop) と呼ぶ．\n続いて，Quarto で julia ブロックを動かすには，IJulia パッケージをインストールする．次を REPL で実行する：\nusing Pkg\nPkg.add(\"IJulia\")\nusing IJulia\nnotebook()\nこれで，Jupyter Notebook で Julia を使うことができる．2\nRevise.jl も追加すると，Julia セッションを再起動するためが省ける\nusing Pkg\nPkg.add(\"Revise\")\nJupyter Cache も追加すると，ソースが変わらない限りその出力がキャッシュされ，再実行が控えられる．\nusing Conda\nConda.add(\"jupyter-cache\")\n\nGetting Started\n\n\n\n\n\n\n\n\nJulia （カーネル）のアップデート方法\n\n\n\njuliaup を用いれば極めて簡単にできる．\njuliaup update\nその後，\njupyter kernelspec list\nで表示されるカーネルもアップデートする必要がある．アップデートは再インストールによる：\nusing IJulia\ninstallkernel(\"Julia\")\n\n\n\n\n1.3 パッケージ管理\nJulia を始めるにあたって最も心強いのは，Julia 独自のパッケージマネジャー Pkg の存在である．\njuliaキーワードで立ち上がる対話的実行環境 REPL（第 2.2 節）において，] キーでパッケージモードに入り，add でパッケージを追加する．\n(@v1.10) pkg&gt; status  # または st\nでパッケージの情報が表示される．\n(@v1.10) pkg&gt; update  # または up\nでアップデートが可能である．\n\n\n\n\n\n\nトラブルシューティング\n\n\n\n\n\n(@v1.10) pkg&gt; status --outdated -m\nにより，アップデートがあるパッケージの一覧を得ることができる．\nものによっては，アップデートを阻害している依存関係が見れる．\n(@v1.10) pkg&gt; rm Example\n\n\n\n\n\n1.4 プロジェクト\nパッケージ管理システム Pkg が提供する環境分離システムの単位を プロジェクト という．3\ngenerateは新たなディレクトリと，Project.toml と src/MyPackage.jl を作成する．\n(@v1.10) pkg&gt; generate MyProject\nactivate . はパスを引数に取り，そのディレクトリにある Project.toml を有効化する．\n(@v1.10) pkg&gt; activate MyProject\n引数なしで activate を実行することで，デフォルトの環境に戻る．\n(@v1.10) pkg&gt; instantiate\nは，working directory にプロジェクトファイル（Project.toml と Manifest.toml）を作成する．\n\n\n1.5 数学表記\n何よりコードの見た目の特徴に，LaTeX コマンドと数学記号が使える ことがある．\nlaw tex を打って tab を押すと処理されるのである！\n\nα = 2  # \\alpha と打って tab を押すと処理される．\nβ = 3\nγ = α + β\n\nprintln(\"α + β = $γ\")\n\nα + β = 5\n\n\n加えて，プロットのタイトルにも使える：\n\n\nCode\nusing Plots\n\nplot(1:10, (1:10).^2, title=\"Plot of \\$y = x^2\\$\")\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこれと合わせ，変数名にはUnicodeが許される．÷, ^ などの使用が直感的に行える．\n\n÷\n\ndiv (generic function with 57 methods)\n\n\n\nx = 2^3  # 2の3乗\nprintln(\"2^3 = $x\")\n\n2^3 = 8\n\n\nまた，変数の前に数値をつけると暗黙に乗算と解釈する．\nそのほかにも関数定義など，数学的にも直感的に読める，文芸的プログラミングの精神が込められている言語である．\n\n\n1.6 Shell コマンド\npwd()\ncd()\nなどのコマンドが使える．"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#概要",
    "href": "posts/2024/Julia/Julia0.html#概要",
    "title": "俺のための Julia 入門（０）スタートアップガイド",
    "section": "2 概要",
    "text": "2 概要\n\n\n2.1 クラス構造\n\n\n\n\n\n\n\n組み込み型，組み込み関数はいずれも Base クラスに属する．\n\n\n\n\n\n\n2.2 Read-Eval-Print Loop\njulia コマンドで起動される\n\n—project\n\n—project=@. の略記．\n現在のディレクトリ . にある Project.toml によるプロジェクトを有効化する，\nPERL における activate コマンドに対応する．\n環境変数 JULIA_PROJECT を @. に変更することと等価．\n\nCtrl+D か exit() で終了\nans：直近の evaluated term が格納されて居る．\n\n５つのモードがある．\n\n\n\n\n\n\nREPL ５つのモード\n\n\n\n\nJuliaモード\nhelp：? で発動．backspace で戻る．使いやすい！\npackage：] で発動．標準では GitHub のリポジトリ上の任意のモジュールがインストール可能．\nshell：; で発動\nsearch"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#julia-パッケージについて",
    "href": "posts/2024/Julia/Julia0.html#julia-パッケージについて",
    "title": "俺のための Julia 入門（０）スタートアップガイド",
    "section": "3 Julia パッケージについて",
    "text": "3 Julia パッケージについて\n\n3.1 Pkg\n]で package mode に入る．\npkg&gt; add &lt;package name&gt;  # 追加\nusing &lt;filename without extension&gt;  # 使用\nhelp  # 使用可能なコマンドの一覧\n\nst (= statusの alias)\n\n現在のプロジェクトがどの config file によるかの確認\n\nデフォルトは Status ~/.julia/environments/v1.5/Project.toml\nこういうデフォルトプロジェクトはユーザー固有になる（ホームディレクトリの下にあるから）\n\n現在インストールされている外部パッケージの状態\n\nrm (= removeの alias)\n\ninstall した package を消去する\n\nup (= updateの alias)\n\n引数無しで用いると，install している全ての package の更新\n\ngc (garbage collection)\n\npackage の追加・削除・消去を繰り返すと不要なデータが蓄積するらしい．これを自動的に削除してくれる．\n\n\n\n\n3.2 パッケージの構造\nConfig file としては TOML (Tom’s Obvious, Minimal Language) 形式を採用している．可読性も高い．\n\n\n\n\n\n\nTOML の構文\n\n\n\n\nディクショナリ構造に明確にマッピングされるように設計されている\nTOML の構文は，大部分がキーと値の組\n\nkey = \"value\"\n[テーブル名]\n# コメント の3種類からなる．\n\n\n\n\n\n\n3.3 Project.toml と Manifest.toml\n\nProject.toml\n\n現在の Project が依存している package を管理する．\n\npackage としてダウンロードした directory にある Project.toml を読んで，依存パッケージもダウンロードするようになっている．\n\nプロジェクト自体のメタデータや依存パッケージの一覧が収められているテーブル．\n開発者も書き足せる．\n\nproject 名など．\n\n[deps] テーブルには，(package 名) = “UUID” が納められている．\n\nこういうのは人間が書くものではない．UUID 複雑．これをするのが Add command の実装に他ならない．\n\n\nManifest.toml\n\n実際に Julia の実行時に使うべきパッケージの正確な version やインストール場所を管理するテーブル．\nこのファイルは，package maneger が Project.toml を参照しながら依存解決して生成する．\n\n\n\n\n\n\n\n\nプロジェクト起動のコマンド\n\n\n\n\nactivate .\n\nProject.toml を読み込んで，新たなプロジェクトを実行する．\nPrompt 名が新たなプロジェクト名に更新される．\ninstall されている package も刷新される．\n\nこれはうまくできているなぁ！\n\nREPL じゃない時は—project=@.オプションで julia から実行する．\n\n\n\n\n\n\n3.4 Package 作成\n基本的には PkgTemplates.jl パッケージ を使うのが良い．次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）パッケージ作成とモジュール\n\n\nモジュールとパッケージ\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nJulia パッケージに必要な要素\n\n\n\n\nREADME.md\nLICENSE\nProject.toml\nsrc\n\nsource code\nPackage名と同じ名前の.jlファイル．\n\ntest\n\ntest code\n\ndocs\n\ndocumentation\n\ndeps\n\nパッケージの依存ライブラリやビルドスクリプトを収める\n継続的インテグレーションのための設定ファイル\n\nManifest.toml は開発環境に特異的で，バージョン管理ツール（GitHub）で共有しないのが一般的．\n\n\n\n\n\n\n\n\n\nパッケージ作成のコマンド\n\n\n\n\ngenerate MyPackage\n\nMyPackage ディレクトリを作り，その中に Project.toml と src/MyPackage.jl を作る．\nProject.toml には，UUID を毎回ランダムに生成し，version や name や author が登録される．\n\nこうしてつくった MyPackage はすぐに using MyPackage と MyPackage.greet() で呼べる．"
  },
  {
    "objectID": "posts/2024/Julia/Julia0.html#footnotes",
    "href": "posts/2024/Julia/Julia0.html#footnotes",
    "title": "俺のための Julia 入門（０）スタートアップガイド",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuarto – Using Julia↩︎\nJupyter Notebook をインストールしていない場合，install Jupyter via Conda, y/n? [y]: n の確認がなされる．これに y で応えると，Conda.jl パッケージを通じて，Miniconda から最小限の Jupyter 環境がインストールされ，グローバル環境は変わらない．↩︎\n正確には，Project.toml（とManifest.toml）により定義される写像がプロジェクトである．↩︎"
  },
  {
    "objectID": "posts/2024/Julia/Julia3.html",
    "href": "posts/2024/Julia/Julia3.html",
    "title": "俺のための Julia 入門（３）関数",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$ %%% 汎用コード列\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Julia/Julia3.html#関数定義",
    "href": "posts/2024/Julia/Julia3.html#関数定義",
    "title": "俺のための Julia 入門（３）関数",
    "section": "1 関数定義",
    "text": "1 関数定義\n\n\n\n\n\n\n\n複数行記述\nfunction name(arg, [arg2, …])\n    methods\nend\n１行記述：極めて数学的に直感的な定義法\nname(arg, [arg2, …]) = return_value\n型の明示\nname(arg::DataType, [arg2::DataType, …])::DataType = return_value\n\n\n\n\n\n型の指定は全て任意．\n戻り値にも型を指定した場合は，暗黙のうちにそれに変換して返される．変換できない場合はERROE: InexactError\n\n\n1.1 返り値について\n\nreturnの省略\n\n戻り値がreturn文で指定されなかった場合は，endの直前の１行で評価された値が自動的に返り値となる．\n\nreturnとだけ書くと，return nothingの略記とみなす．\n複数返り値\n\ntupleを返せば良い．\n\n\n\n\n1.2 引数について\n\n可変長引数\n\n… objectを用いてarg…またはarg::DataType…とすれば，tuple型で受け取れる．\n受け取ったargに対してlnegth(arg)としてforを回せる．\n関数の最後の引数のみに定義可能．\n\noptional引数\n\nname=valueとして宣言すると，optionalになる．\n全てのoptional引数は，そうでない引数より前に来ることができない．\npositionalである．あまりに多い場合はkeyword引数にすると良い．\n\nkeyword引数\n\noptional引数の記法において，コンマ”,”の代わりにセミコロン”;”で区切ると，以降は名前付きで呼び出さなきゃいけないkeyword引数となる．\n呼び出すときはコンマ”,”で良い．\n名前付きで呼び出せるので\n\n多重ディスパッチ\n\n引数の型注釈が違えば，同じ名前の関数をいくつも定義できる．\n呼ぶ時はJuliaが暗黙のうちに正しいものを呼んでくれる．\n\n\n\n\n1.3 匿名関数：-&gt;\n-&gt;がconstructorで，大体代入してから使う．map, filter 関数の引数として用いられる．\n\n１行記述 name = arg -&gt; value 左辺のnameは関数objectに，右辺は匿名関数である．\n複数行記述 name = arg -&gt; begin suite end\n\n\nデータ型はvar”#1#2”となる．なぜだ？"
  },
  {
    "objectID": "posts/2024/Julia/Julia3.html#メソッド",
    "href": "posts/2024/Julia/Julia3.html#メソッド",
    "title": "俺のための Julia 入門（３）関数",
    "section": "2 メソッド",
    "text": "2 メソッド\n\n2.1 導入\n+など，Julia における関数 (generic function) はメソッドの貼り合わせとして実装される．1\n\nfirst(methods(+), 5)\n\n5-element Vector{Method}: +(a::Pkg.Resolve.FieldValue, b::Pkg.Resolve.FieldValue) in Pkg.Resolve at /Users/hirofumi48/.julia/juliaup/julia-1.10.4+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/Pkg/src/Resolve/fieldvalues.jl:43 +(x::Bool, z::Complex{Bool}) in Base at complex.jl:305 +(x::Bool, y::Bool) in Base at bool.jl:166 +(x::Bool) in Base at bool.jl:163 +(x::Bool, z::Complex) in Base at complex.jl:312\n\n\n\nThus, the overall behavior of a function is a patchwork of the behaviors of its various method definitions. Docs.\n\n関数が呼び出された際に，どのメソッドを実行するかの選択機構を ディスパッチ という．2 Julia は引数の型だけでなく，引数の数も見てディスパッチを行う．従来のように，最初の引数の型を見るディスパッチと区別して，これを 多重ディスパッチ という．3\nここでは，今までの関数定義とは違い，:: signature による明示的な型制約を設けた定義が行われる．\n\n\n2.2 パラメトリックメソッド\n\nsame_type(x::T, y::T) where {T} = true\nsame_type(x,y) = false\nsame_type(1, 2.0)\nsame_type(1, 2)\nmethods(same_type)\n\n# 2 methods for generic function same_type from \u001b[35mMain\u001b[39m: same_type(x::T, y::T) where T in Main at In[3]:1  same_type(x, y) in Main at In[3]:2 \n\n\nT は返り値としても使える：\n\nmytypeof(x::T) where {T} = T\nmytypeof(nothing)\n\nNothing\n\n\n\n\n2.3 呼び出し可能オブジェクト"
  },
  {
    "objectID": "posts/2024/Julia/Julia3.html#組み込み関数",
    "href": "posts/2024/Julia/Julia3.html#組み込み関数",
    "title": "俺のための Julia 入門（３）関数",
    "section": "3 組み込み関数",
    "text": "3 組み込み関数\n\n3.1 Base.\n\nnextind(str::AbstractString, i::Integer, n::Integer=1) -&gt; n-th_index::Int\n\nn==1の時，iでindexされているstrの要素の，次の要素の開始indexの値を返す．\nn&gt;1の時，n個次の要素の開始indexの値を返す．\nn==0の時，iでindexされているstrの要素が存在する場合，iを返す．\n\nprevind()\nrand([rng=GLOBAL_RNG], [S=Float64], [dims…]) -&gt;\n\n可変長引数dimをとって，(n,m,…)だとn×m×…のランダム行列を返す．\n\nmap(f, c…) -&gt; collection\n\ncollection型であるcの各要素にfを施す．\n積写像である．\n\nfilter(f,a) -&gt; a’\n\naはcollection, fは真理値関数．\ncollection aのcopyであって，fがfalseと判定したものを返す．\n\n\n\n3.1.1 Base.Iterators.\n\nenumerate(iter) -&gt; (i,x)::iterator\n\niは1から始まるindex，xはiterのi番目の要素．\niterable object xだけで無く，カウンターiも同時に得られるところが便利．それに常にx = iter[i]とは限らない．\n\nこの返り値objectの型はBase.Iterators.Enumerate{NTuple{n,T}}．\n\n\n\n\n\n3.1.2 Base.MainInclude.\n\neval(expr)\n\nEvaluate an expression in the global scope of the containing module.\n\n\n\n\n\n3.2 Core.\n\neval(m::Module, expr)\nisa(x, type) —&gt; Bool\n\n中置記法(infix)でx isa typeともかける．\nxがtypeのinstandeならtrueを返す．"
  },
  {
    "objectID": "posts/2024/Julia/Julia3.html#footnotes",
    "href": "posts/2024/Julia/Julia3.html#footnotes",
    "title": "俺のための Julia 入門（３）関数",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Thus far, we have presented only examples of functions defined with a single method, applicable to all types of arguments. However, the signatures of method definitions can be annotated to indicate the types of arguments in addition to their number, and more than a single method definition may be provided.” Docs.↩︎\n“The choice of which method to execute when a function is applied is called dispatch.” Docs.↩︎\n“Multiple dispatch is particularly useful for mathematical code, where it makes little sense to artificially deem the operations to”belong” to one argument more than any of the others: does the addition operation in x + y belong to x any more than it does to y?” Docs.↩︎"
  },
  {
    "objectID": "posts/2024/Julia/Julia6.html",
    "href": "posts/2024/Julia/Julia6.html",
    "title": "俺のための Julia 入門（６）メタプログラミング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$ %%% 汎用コード列\n%%% 演算子 \n%%% 線型代数学\n%%% 複素解析学 %%% 集合と位相 \n%%% 形式言語理論 %%% Graph Theory \n%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学\n%%% 函数解析\n%%% 積分論\n%%% Fourier解析 %%% 数値解析 \n%%% 確率論\n%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス \n%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 \n%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計\n%%% 計量経済学 \n%%% 無限次元統計模型の理論\n%%% Banach Lattices \n%%% 圏 %代数の圏 %Metric space & Contraction maps %確率空間とMarkov核の圏 %Sober space & continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 \n%%% SMC\n%%% 括弧類\n%%% 予約語 \n%%% 略記 \n%%% 矢印類 $$"
  },
  {
    "objectID": "posts/2024/Julia/Julia6.html#メタプログラミング",
    "href": "posts/2024/Julia/Julia6.html#メタプログラミング",
    "title": "俺のための Julia 入門（６）メタプログラミング",
    "section": "1 メタプログラミング",
    "text": "1 メタプログラミング\nプログラムにプログラミングをさせること．\nこれはプログラミング言語を抽象構文木として捉えることで，一切の「プログラムの実行」に関連した意味論を排除することができることによって可能となる．\n他の「プログラムの実行」関連の処理以前に，parse とセットでマクロの展開が行われる．まさに，「プログラミングのプログラミング」である．\n\nLispのマクロ\nPythonのデコレータ\n\n\nマクロ\n\n正規表現Regexなどのnon-standard string literal\n\nじゃあVERSIONに格納されているVsersionNumberも？\n\n\n\n\n1.1 Symbol 型：処理系内部の名前\nidentifierとも呼べる．\n２通りのコンストラクタがある．\n\nx = :foo  # シンボル :foo を作成\ny = Symbol(\"foo\")  # シンボル :foo を作成\n\nprintln(x == y)  # true\n\ntrue\n\n\n\ntypeof(:foo)\n\nSymbol\n\n\nfooという変数がソースコード内で使用されている時，処理系内ではfooという名前のシンボルが作成され，それらのテーブルを保持する．\n\n\n1.2 Expr：AST が型を持って Julia で操作可能なオブジェクトとして登場！\n構文論における「文 expression」とは，木である．これを AST (Abstract Syntax Tree) ともいう．\n\n1.2.1 コンストラクタとdump\n:() による引用 (quoting)，または quote-end ブロックの２通りがある．\n\nexpr = :( 2+3 )\ndump(expr)\n\nExpr\n  head: Symbol call\n  args: Array{Any}((3,))\n    1: Symbol +\n    2: Int64 2\n    3: Int64 3\n\n\ncode を parse までして実行はせず，抽象構文木 object として保持する．処理系に parse された後の抽象構文木も object として扱えるのが Julia！\n\nprintln(expr)\n\n2 + 3\n\n\n\n\n1.2.2 eval関数\nparse 以降の実行工程を，現在の Module でテーブルを作って 実行する．\n必ずその Module の global テーブルで実行される．関数定義のなかで eval されていても，global table でなされる．\n\neval(expr)\n\n5\n\n\n\n\n1.2.3 ２つのフィールドを持つ\n２つの field head と args の繰り返しからなる．head は構文木の種類，args が各要素．\n\nprintln(\"head: \", expr.head)\nprintln(\"args: \", expr.args)\n\nhead: call\nargs: Any[:+, 2, 3]\n\n\n\n\n1.2.4 補間演算子$( )\n\n# String 型オブジェクトの補間（比較のため）\nname = \"Julia\"\ngreeting = \"Hello, $name\" * '!'\nprintln(greeting)\n\nHello, Julia!\n\n\n\n# Expr 型オブジェクトの補間\nex = :x\nexpr = :(2 * x + $(ex))\nprintln(expr)\n\n2x + x\n\n\nこの時exがSymbol型オブジェクトでも，剥がされて，String型として補間される．1\n\n# 評価してから補間\nx = 2\ny = 3\nresult = \"The sum of $x and $y is $(x + y).\"\nprintln(result)  # 出力: The sum of 2 and 3 is 5.\n\nThe sum of 2 and 3 is 5.\n\n\nSymbol型を保持したい場合は，さらにexをQuoteNoteでクオートする：\n\nexpr = :(2 * x + $(QuoteNode(ex)))\nprintln(expr)\n\n2x + :x"
  },
  {
    "objectID": "posts/2024/Julia/Julia6.html#マクロ",
    "href": "posts/2024/Julia/Julia6.html#マクロ",
    "title": "俺のための Julia 入門（６）メタプログラミング",
    "section": "2 マクロ",
    "text": "2 マクロ\n\n2.1 導入：与えたコードを別のコードにして評価する高次の仕組み\n「マクロの展開」という言い方をする．\nマクロの展開は，parse のすぐ次の段階で行われるので，一番速い．これが，高級言語か……．\nマクロの引数は，Shell command のようにスペースで区切って与える．\n一方，関数のように @macro(x,y,...) と与えることもできるが，慣習に逆らうという．\nこの２つは構文解析のされ方が違う．\n:(@macro x + y) == :(@macro(x+y)) -&gt; true\n\n:(@macro x +) == :(@macro(x,+) -&gt; true\n\n\n2.2 標準マクロ\n\n(macroexpand?)\n\n与えられた式にあるマクロを展開して得る表現のExprオブジェクトを返す．\n全てこれで展開してみれば，マクロの挙動がわかる．\n\n(eval?, mod,) ex\n\neval()関数と同じ．しかし，自分でquoteしてExpr型にする必要はない．\nEvaluate an expression with values interpolated into it using eval. If two arguments are provided, the first is the module to evaluate in.\n\n(assert?) cond [text]\n\n条件式condがfalseならばAssertionErrorを投げる．text::AbstractStringを指定すれば，エラーメッセージとしてそれを表示する．\n\n(enum?)\n\nC言語のenum関数の継承．\n\n(view?)\n\n配列についてのview関数のマクロ化．\n\n\n\n\n\n\n\n\n例（開発補助）\n\n\n\n\n\nREPLで主に使われる，Shell commandに似てる．\n\n(less?)\n\n関数呼び出しの式から，呼び出されるmethodのソースコードを表示する．コマンドのlessか．\n\n(time?)\n\n処理を受け取り，その実行にかかった時間やメモリ使用量を表示する．\n\n(code_typed?)\n\n関数呼び出しの式を受け取って，コンパイラによる型推論の結果を表示する．\n\n\n\n\n\n\n\n\n\n\n\n例（コンパイラへのヒント）\n\n\n\n\n\n構文木に特殊な情報を差し込むことで，最適化が進む．\n\n(inbounds?)\n\n配列要素の参照が配列の有効な範囲に収まる確信があるので，コンパイラはチェックしなくていいよ．\n\n(inline?)\n\n関数を積極的にインライン化するべき．\n\n(fastmath?)\n\n不動小数点演算について，IEEE 754の制約を超えて最適化することを許可する．\n\n\n\n\n\n\n\n\n\n\n\n例（非標準文字列リテラル）\n\n\n\n\n\nassertは部分的にそうであったが，String型のみを受け取るマクロのことを特に「非標準文字列リテラル」と呼ぶ．\nマクロ名は_strで終わり，文字列の前にマクロ名から_strを除いたものを接続しても呼び出せる．これは冠頭演算子っぽい，いや，タグっぽいかもしれない．\n殆どが「特殊なリテラル」として使われるために，こう呼ぶ．従って，リテラルを持つ特殊な型のconstructorだと思えばいい．\n\n(r_str?) / r”…”　-&gt; Regex\n\nRegex型のリテラルの定義に用いられる．\n\n(v_str?)\n\nVersionNumberリテラル．\n\n(b_str?)\n\nCreate an immutable byte (UInt8) vector using string syntax.\n\n(s_str?) -&gt; SubstitutionString\n(big_str?)\n\nParse a string into a BigInt or BigFloat, and throw an ArgumentError if the string is not a valid number. For integers _ is allowed in the string as a separator.\n例：big”43”，big”3.1415926535”\n\n\n\n\n\n\n\n2.3 マクロ定義\nマクロとは，Expr上の，Exprを返す関数であり，keyword が macro になるだけで，関数定義と同じ．\n\nmacro sayhello(name)\n    return :(println(\"Hello, \", $name))\nend\n\n@sayhello(\"world\")  # 展開されるコードは println(\"Hello, world\")\n\nHello, world\n\n\n\n引数は構文木やリテラルになるから，構文木の補間 $(arg1) が頻出することになる．\nしかし，マクロの展開は，マクロが定義されたmodule内でのscopeでなされるので，想定外の動作をすることがある．\n従って，メタプログラミング特有の「エスケーピング」が必要になる．\n\nesc(ex)\n\n構文木にある識別子を別の識別子に置き換えはせず，そのままにする．\nOnly valid in the context of an Expr returned from a macro. Prevents the macro hygiene pass from turning embedded variables into gensym variables.\n\n例：macro plus1(ex) :($(esc(ex)) + 1 )end\n\n\n識別子の変換規則 引数ex::ExprもJuliaコードであるから，マクロ定義内の文章と衝突することがあり得る，メタプログラミング故の悩みの種である．\n識別子の変換が，マクロが定義されたmodule内の大域テーブルでなされるのが原則だが，次は例外である． 1. global宣言なしで代入された時 2. local宣言がある時 3. 関数定義の引数である時\nこれら３条件を満たすためにローカル変数と解釈された識別子は，マクロ展開時に新しい変数に置き換えられる（#10#nameなど）．これはマクロ呼び出し側にある別の識別子との衝突を避けるためである．このマクロ展開の仕方を【hygene macro】という．\nまとめ マクロが返す構文木やリテラルに含まれる識別子は，次のいずれかの経路を辿ったものである． 1. esc関数でエスケープされていれば，識別子は変換されずそのまま維持される． 2. 代入，local宣言，関数引数のいずれかであれば，新しいローカル変数が生成される． 3. いずれでもない場合は，マクロを定義したmoduleのglobal変数に変換される．\nメタプログラミングの例\n\n規則のある（algorithmablic）変数定義自体をメタプログラミングで回す． for (i, name) in enumerate([:A, :B, :C]) eval(:(const $(Symbol(:FLAG_,name)) = \\((UInt16(1) &lt;&lt; (i-1) )))\nend\n1. (i, name) = (1, :A), (2, :B), (3, :C)で３周回す．\n2. const FLAG_A = 1 &lt;&lt; 0const FLAG_B = 1 &lt;&lt; 1const FLAG_C = 1 &lt;&lt; 2と書いたのと同じ．\n3. bit-shift演算なので，それぞれ，数値と解すれば2^0, 2^1, 2^2になる．つまり，UInt16型では，0x0001, 0x0002, 0x0004となる．\n4. Symbl(:FLAG_, A)で，Symbol objectである:FLAG_Aを生成している．ここで，FLAG_と書くと変数名と解されて，UndefVarError: FLAG_1 not definedが返ってくる．\n5. そしてそれを\\)()演算子で補間して，結局String型として:()でquoteさせている．"
  },
  {
    "objectID": "posts/2024/Julia/Julia6.html#footnotes",
    "href": "posts/2024/Julia/Julia6.html#footnotes",
    "title": "俺のための Julia 入門（６）メタプログラミング",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSymbol型の即値と解さない方をデフォルトとした方が利便性が高いからである．↩︎"
  },
  {
    "objectID": "posts/2024/Julia/Julia4.html",
    "href": "posts/2024/Julia/Julia4.html",
    "title": "俺のための Julia 入門（４）型宣言",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/Julia4.html#型の構成",
    "href": "posts/2024/Julia/Julia4.html#型の構成",
    "title": "俺のための Julia 入門（４）型宣言",
    "section": "1 型の構成",
    "text": "1 型の構成\n\n1.1 抽象型\n\n\n\n\n\n\nAnyの子の定義：supertype を指定しない場合は Any::DataType が supertype となる．\n\nabstract type Name end\n\ntypeof(Name)\n\nDataType\n\n\n子の定義\n\nabstract type SubtypeName &lt;: Name end\n\n\n\n\n\n\n\n\n\n\n型情報の表示方法\n\n\n\n\n\n\nsupertype(Name)\n\nsubtypes(Name)\n\nfieldnames(Int)\n\nmethodswith(Name)\n\n@show Name\n\nName = Name\n\n\nName\n\n\n\n\n\n&lt;:, &gt;: は arity 2 の論理演算子にもなる．\n\nInt &gt;: Int\n\ntrue\n\n\n関数宣言の際の型注釈にて，冠頭演算子としても使う．型変数に対するslicingである．従って，木構造の上でのslicingとなる（第 1.5.2 節）．\n\n\n1.2 複合型 Composite Types\n複数の名前付き field をまとめて扱うことができる．\nJulia で複合型を立てる時は，まるで構造付きの空間を作るようである．Euclid空間のように．\n従って，データと method を統合する（＝関数が複合型に所属する）OOP とは少し違い，複合型と，その上の関数を別々に作る．従って，Julia の関数は常 にfirst-class object として扱われる．1\n\n\n\n\n\n\nimutable な複合型の変数はstruct-endブロックで宣言する．\n\nstruct MyType\n    x::Int\n    y::Int\nend\n\nfieldnames(MyType)\n\n(:x, :y)\n\n\n複合型を宣言すると，暗黙のうちに constructor が生成される．これも OOP と同じ．\n\nz = MyType(1, 2)\nprintln(z.x)\n\n1\n\n\nmutable な複合型はmutable struct-endブロックで宣言する．\n\nmutable struct MutableType\n    x::Int\n    y::Int\nend\n\nm = MutableType(3, 4)\nm.x = 10  # フィールドxの値を変更可能\n\n10\n\n\n\n\n\n\n\n\n\n\n\nカスタムコンストラクタの作成\n\n\n\n\n\n\nMyType(x) = MyType(x, 0)\n\nMyType\n\n\n\n\n\n\n\n1.3 Union型：直和\n直和型を表す，代数的データ型の簡単な場合．他の言語では Nullable, Option 型と呼ばれる．\nUnion{Int,String}というように，タグUnionをつけて表す．\n\nfunction process(x::Union{Int, String})\n    if x isa Int\n        println(\"The integer is $x\")\n    elseif x isa String\n        println(\"The string is $x\")\n    end\nend\n\nprocess(3)\n\nThe integer is 3\n\n\nUnion{Int, Nothing}とすると，部分関数の値域として使える．\n\n\n1.4 Parametric型\n他の言語では generics や template と呼ばれるものである．2\nそもそもタグ付という表示方法自体が，タグを関数名と見れば，点を表しているように見える．その調子で，タグを固定し，型変数を導入すればいい．\n\n\n\n\n\n\nParametric 型は，{}オブジェクトで定義する．試しに，Pointというパラメトリック型を定義する：\n\nstruct Point{T, U}\n    x::T\n    y::U\nend\n\nインスタンス化する際は，型変数 T, U にも代入をする．\n\np1 = Point{Int, Float64}(3, 4.5)  # 明示的に型を指定\np2 = Point(3, 4.5)  # 型推論により自動的に型が決まる\np2\n\nPoint{Int64, Float64}(3, 4.5)\n\n\nパラメトリック型のデータに対して関数を定義する際は，型変数を使用した後に，whereを使って型変数を指定する．3\n\nfunction distance(p::Point{T, U}) where {T, U}\n    sqrt(p.x^2 + p.y^2)\nend\n\ndistance(p2)\n\n5.408326913195984\n\n\n\n\n\n\n\n\n\n\n\n注（多重ディスパッチの精神）\n\n\n\n\n\nただし，この時に，変数Tに関する条件節でif文を分けるのはよくない．\nJulia の多重ディスパッチの精神に従って，関数を分けて定義するのが良い．Julia は自動で引数の型に最も適したメソッドを呼ぶ．\n即ち，型は parametric や直積にしても，関数は直積にはしない．\n\nstruct Rectangle{T}\n    width::T\n    height::T\nend\n\n# 型推論によるインスタンス生成\nr1 = Rectangle(3.0, 4.0)\n\n# 型指定によるインスタンス生成\nr2 = Rectangle{Int}(3, 4)\n\nRectangle{Int64}(3, 4)\n\n\n\n# 多重ディスパッチの実践\n\nfunction area(r::Rectangle{T}) where T\n    r.width * r.height\nend\n\nprintln(area(r1))  # 出力: 12.0\nprintln(area(r2))  # 出力: 12\n\n12.0\n12\n\n\n\n\n\nPoint{T}型に対して，型変数Tに代入を行って得る型Point{Int}は concrete 型になり，Point{T}という抽象型を親型に持つ．\n従って，多重ディスパッチにおいてはPoint{Int}はPoint{T}にも match する．4\n\n\n1.5 型階層\n\n1.5.1 パラメトリック型の位置\n型変数を{}演算子により持たせた Parametric 型は，持たせていない型の下流にある．\n\nPoint{Int} &lt;: Point\n\ntrue\n\n\n「持たせていない型」はUnionAllという型になる．5\n\ntypeof(Point)\n\nUnionAll\n\n\nJulia では，型変数の間に包含関係があっても，一度 parametric になると包含関係は消える【不変】．これは主に実行効率性の理由による．\n\nPoint{Int} &lt;: Point{Float64}\n\nfalse\n\n\n\n\n1.5.2 パラメトリック型の sclicing\nUnionAll型はスライシングができる．\n\nfunction distance(p::Point{&lt;:Number})\n    sqrt(p.x^2 + p.y^2)\nend\n\nfunction distance(p::Point{T}) where T &lt;: Number\n    sqrt(p.x^2 + p.y^2)\nend\n\ndistance (generic function with 2 methods)\n\n\n\n\n1.5.3 抽象型と具体型\nNumber は抽象型であり，Point{Number}という表現はできない．\nPoint{&lt;:Number}には，Point{Int}もマッチする：\n\np1 = Point(3, 4)  # Point{Int}\ndistance(p1)  # 呼び出し成功\n\n5.0\n\n\n\n\n1.5.4 SingletonType：たった一つの型を含む自明な Parametric Type\nキーワード Type によって，Type{T}の形で定義される．元の型 T のインスタンスとなる．\n\nisa(Float64, Type{Float64})  # Float64 は Type{Float64} 型のインスタンス\nisa(Real, Type{Float64})\n\nfalse"
  },
  {
    "objectID": "posts/2024/Julia/Julia4.html#組み込み型とその関数",
    "href": "posts/2024/Julia/Julia4.html#組み込み型とその関数",
    "title": "俺のための Julia 入門（４）型宣言",
    "section": "2 組み込み型とその関数",
    "text": "2 組み込み型とその関数\n\n2.1 Collection 型：直積\n更なる構造付きのものはDataStructure.jlにある．\n各直積型に，tagをつけてその性質を明示する．\nAny直下の型 0. (a,b,…)::Tuple{T1,T2,…} 0. immutableである 0. Array型に対するsize関数の返り値はTuple型． 0. 可変長引数もTuple型のobjectとして受けられる． 0. 上記から観察されるように，入れ物であって，代数的構造を持たせることを意図していない．その場合はArrayを使う． 0. named tuple：Typeの直積． 0. 元々NamedTuple.jlという独立したpackageだったが，0.7から統合． 0. 要素に数字以外のaliasでタグ付できる．immutable． 0. (a=1, b=2)::NamedTuple{(:a, :b), Tuple(Int, Int)}などの記法で定義される． 0. つまり，値のTupleと，Symbol型のオブジェクトのTupleとの組で表される． ⁃ この時の射影が，keys関数，values関数として実装されている． 0. keyは.演算子でアクセスできる． • t.a 0. Symbolをindexの代わりに使える． • t[:a] 0. 一時的に使うのが普通で，本格的にはstruct, mutable structとして第一級の居住権を与えるのが良い． 0. List：Array{T,1}としての実装．Vector{T}というエイリアスもある． 0. 追加や削除などの順序的構造が重視されるCollection型． 0. スタック，キュー，両端キューはDataStructure.jlへ． ⁃ push!(List, object) -&gt; List’ ⁃ 要素の末尾追加 ⁃ pushfirst!(List, object) -&gt; List’ ⁃ 要素の先頭追加 ⁃ pop!(List) -&gt; object ⁃ 要素の末尾摘出 ⁃ popfirst!(List) ⁃ insert!(List, n, object) -&gt; List’ ⁃ n番目の位置に追加 ⁃ deleteat!(List, n) -&gt; List’ ⁃ n番目の要素を削除． 0. 辞書：Dict{K, V}という直積型 0. constructorは • d = Dict{String, Int}() • d[“a”] = 1 • d = Dict(key =&gt; value, key =&gt; value, …) 0. constructorの{}内の要素を１つ，または全て省略するとAnyとしたのと等価． ⁃ haskey(Dict, key) -&gt; Bool ⁃ Dict型objectに，所定のkeyが含まれているかを判定する． 0. built-inにIdDict型とWeakKeyDict型がある． 0. 集合：Set{T} 0. constructorは • s = Set([1,2]) 0. 即ち，１次元Arrayから生成される．というより，１次元Arrayにタグをつけたものである． 0. 実装は「keyのみを保持する辞書」というべきもので，辞書と同様，値の重複を無視する． ⁃ push!(set, object) ⁃ 値の追加． ⁃ union(set, List) ⁃ intersect(set, List) ⁃ issubset(List, List) -&gt; ::Bool ⁃ List ⊂ List -&gt; ::Bool ⁃ Set型のinstanceを生成することなく集合演算ができる． 0. 多次元配列：Array{T,n}．Matrix{T}はArray{T,2}のエイリアスである． 0. MATLABを踏襲している．NumPyとは所々違う． 0. NumPyは0からindexingし，row-major orderである．これは，行列のindexingにとって，辞書式順序になる． 0. しかしJuliaは1からindexingし，column-major orderである．後者は行列のindexingに沿って，第一要素のストライドが１になる． 0. 従って，Juliaは同じ行の要素の列挙が得意．線型代数のものの見方である． 0. 内部実装は結局一次元配列（List）であることを意識すると良い．\n⁃   [] (constructor)\n⁃   [ a b c; d e f; h i j ]\n⁃   または改行を入れてもいい．\n•   []\n•   要素へアクセスする作用素．\n⁃   isempty(collection) -&gt; Bool\n⁃   empty!(collection) -&gt; collection\n⁃   空にする\n⁃   length(collection) -&gt; Int\n⁃   eltype(collection) -&gt; Type\n•   関数名の最後に!\n•   引数の一部を変更・破棄する関数\n•   !のつかない関数は，引数に対する破壊的変更はないので安心して使用できる，という慣習．\n•   例：push!，insert!\n•   sortは新たなobjectを返すが，sort!は引数そのものを変更してしまう．\n•   for文に使う構文はPythonと同じ使用感．\n•   しかし，直積型を意識．\n•   for (key, value) in d::Dict\n•   実装は，iterable型オブジェクトを介して行われる．つまり，Juliaは次のように変換して処理される．速度の問題？\n•   next = iterate(collection)while next !== nothing　　(x,state) = next　　#suite　　next = iterate(collection, state)end\n•   Juliaはiterable型は無く，Tuple{Int, Int}である．\n•   第一要素は，「次の要素」で，第二要素は「その次の要素」のindex（やそれに値するもの）を表す整数またはnothing．\n•   従って，自作のcollection型にもfor文iterationを実現させるためには，iterate関数をディスパッチすれば良い．"
  },
  {
    "objectID": "posts/2024/Julia/Julia4.html#array型の関数",
    "href": "posts/2024/Julia/Julia4.html#array型の関数",
    "title": "俺のための Julia 入門（４）型宣言",
    "section": "3 Array型の関数",
    "text": "3 Array型の関数\n作成 0. constructor 0. Array{T}(undef, dims…) 0. 値が初期化されていないことに注意． 0. collect(reshape(1:9, 3, 3)) 0. collectionから，要素を奪って配列に仕立て直す． 0. zeros([T,] dims…) 0. ones([T,] dims…) 0. fill(x, dims…) 0. 行列xI 0. fill!(A,x) 0. 配列AをxIにする． 0. rand([T,] dims…) 0. 一様分布でランダムに初期化した配列 0. 型を指定しないとFloat64で． 0. randn([T,] dims…) 0. 正規分布でランダムに初期化した配列 0. similar(A,[T,dims…]) 0. 配列Aと類似した配列を返す． 0. reshape(A, dims…) -&gt; AbstractArray 0. 切り取る，または形を変える． 0. 並びはcolumn-major orderのままである． 0. AにはUnitRange型も許容されるのがすごい． 0. copy(A) 0. deepcopy(A) 0. Aの要素も再帰的にコピーする．\n0.  [A B]\n0.  数学的記法の感覚で使える行列の接続．\n0.  view(A, n, m)  -&gt;  view(::Array{T,n}, m, i)\n0.  n, mはindexingまたはslicing．\n0.  Aから部分配列を抜き出す．\n0.  返るobjectはAへの参照とindexの情報を持っているので，Aを変更するとその部分配列も変わる．\n0.  この実装は，Aが巨大すぎる場合への配慮を感じる．\n0.  「ただし，現在の計算機による配列のコピー操作は，一般に非常に高速であるため，巨大な配列を扱うのではない限り，サブ配列を作成するよりも，通常のindexingで新たな配列を作成してしまう方が高速であることも多い．この辺りは，実際に計測してパフォーマンスを確かめてみるのが良いだろう．」\n属性 0. eltype(A) 0. length(A) 0. ndims(A) 0. size(A) 0. size(A,n) 0. n番目の次元におけるAのサイズ 0. size(A)で返ってくるtupleの第n要素． 0. strides(A) 0. Aのストライド． 0. 第一要素は必ず１になる． 0. 要素同士が，一番浅い意味での一次元配列上でどれくらい離れているか． 0. stride(A,n) 0. n番目の次元におけるAのストライド\n0.  [i, j, k, …]または[n:m, i:j, k:l, …]でindexingできる．\n代数的構造 0. * 0. 行列乗算 dot付演算子：broadcasting．broadcast(+, A, B)のエイリアスである．broadcast!(+, A, B)とするとAが変更される． 0. component-wiseの演算． 0. A .+ c 0. A + cIと同じ． 0. column-wiseの演算． 0. A .+ B 0. BがAの繰り返し単位になっている場合のみ． 0. つまり，size(B)とsize(A)を各要素ごとに見比べたとき，次の２条件のいずれかを満たすとき． 0. 値が同じ 0. どちらかの値が１ 0. ２つ目の条件として，pr_i(size(B)) | pr_i(size(A))だったらもっと使いやすかったがね． 0. broadcast関数 0. broadcast(f, args…)が定義． 0. f.(args…)とも記述できる． 0. ただし，f=+などの時には使えない．fは関数が想定されている． 0. sigmoid.(A)やexp.(A)などである． 0. argsはArrayに限らず，tupleでも良い．\n位相的構造 0. map(f, c::collection…) -&gt; collection 0. collection cにelement-wiseにfを適用させる． 0. broadcastingやdot演算と似ているが，fが匿名関数でない場合はdot演算を使うのが良い？ 0. reduce(op, itr; [init]) -&gt; obj 0. Aをitrableと見做して，要素ごとにopに突っ込んでいく． 0. 従って，次元が１段階下がる． 0. filter(f, a::collection) -&gt; a’ 0. aを要素ごとにfに突っ込んで，fがfalseを返すものについては脱落させる．"
  },
  {
    "objectID": "posts/2024/Julia/Julia4.html#footnotes",
    "href": "posts/2024/Julia/Julia4.html#footnotes",
    "title": "俺のための Julia 入門（４）型宣言",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの言葉は 1960 年代に Christopher Strachey によって “functions as first-class citizens” という文脈で初めて使われた。全ての機能を享受できる citizen 的な？ ↩︎\nJava の generics は Julia に似て不変であるが，C# の generics は in/out キーワードにより共変性をサポートしている．↩︎\nwhere T &lt;:Any の略記↩︎\nしかし，Julia の仕様は，多重ディスパッチにおいては concrete 型に近いものが先に適用されるようになっている．↩︎\n意味論的にはそう，全ての型の和集合で，以降はTに代入するたびに同値類のいずれかを得る．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#政治学における理想点解析",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#政治学における理想点解析",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "1 政治学における理想点解析",
    "text": "1 政治学における理想点解析\n\n1.1 はじめに\n理想点推定 は (Poole and Rosenthal, 1991) 以来，政治学において各アクターのイデオロギーを定量化・可視化する方法論として用いられている．\n理想点推定は，政治学における 空間モデリング の重要な一環であり，現代ではイデオロギーの「近さ」を定量化する際の多次元展開法の一種として，理想点推定を位置付けることができる．\n\n\n\n\n\n\n理想点推定の展開\n\n\n\n理想点推定は 態度測定 (飽戸弘, 1966) や 議員尺度化 (legislative scaling) (Jackman, 2001)，または 空間分析 (spatial analysis) (Davis et al., 1970), (岡田謙介 and 加藤淳子, 2016) とも呼ばれる．\n\n\n政治過程とは合意形成の過程である．これを各アクターが政策空間上に選好分布を持つとしてモデリングし，その上でのアクターの行動を分析することで政治・立法・司法過程の理解を試みることを 空間モデリング (Davis et al., 1970), (Enelow and Hinich, 1984) という．\n空間モデリングの政治学的な理論的根拠として 空間（競争）理論 (Downs, 1957) が源流にある．\n政治学における空間理論とは，イデオロギーの「近さ」が影響力を持つとする枠組みであり，はじめは１次元空間上での選挙と投票行動の公理的な分析に用いられた．\n\n\n1.2 空間競争理論 (Downs, 1957)\n空間理論はもともと，ゲーム理論における交渉理論 (bargaining theory) において (Hotelling, 1929) が雑貨店の立地の情報を考慮に入れたことから始まった．\n政治学，特に選挙競争において (Black, 1948) が空間競争理論，特に一次元の政策空間を導入し，公理的な議論を行なった：\n\n\n\n\n\n\n(中位投票者定理 Black, 1948)1\n\n\n\n一次元の政策空間上に投票者が単峰性の選好分布を持つ際，中位政策が Condorcet 勝者となる．\n\n\n(Downs, 1957) は (Black, 1948) が用意した政策空間とゲーム理論を合流させ，選挙競争と投票行動の分析に応用した．\n\n\n\n\n\n\n(Downs, 1957; Hotelling, 1929)2\n\n\n\n１次元の政策空間上の２政党競争において，いくつかの仮定の下で，ナッシュ均衡は両政党が中位政策を採用することである．\n\n\n\n\n1.3 空間競争モデルと理想点\n(Hotelling, 1929)-(Downs, 1957) のアプローチは政治的競争のモデルの出発点となり，政治的競争を人工的な空間上でモデリングする手法が広がった．\n例えば多くの選挙結果を分析する際，政策空間内での中位政策の位置の特定や，実際の政党の政策の中位政策からのズレが重要な意味を持つようになった．\n\n\n\n\n\n\n例：赤い州と青い州の問題\n\n\n\n\n\n米国での投票行動において，個人レベルでの選考と州レベルでの選考とが食い違うという問題が 21 世紀以来有名になっている (Gelman, 2014)．\n端的に言えば，政策的にはリベラルに位置する民主党は貧困層の見方であるが，その主な得票源は富裕層の多い州からのものである．\nこれは中位投票者が中産階級に位置するためであるという見方が一つ説明のつく仮説である (浅古泰史, 2016, p. 78)．\n一方で，ベイズ階層モデリングによる解析によって，各州の投票行動が大きく違うことが判明し，New York や California のように裕福な州では収入が投票行動に全く影響しないこともわかっている (Gelman, 2014)．\n\n\n\nこのように政策空間上にアクターをマッピングし，その上で競争をモデリングする手法は 空間モデル (spatial model) とも呼ばれる．\n特にアクターが政策空間上に持つ選好分布の最頻値を 理想点 (ideal point) という．\nさらには多次元に拡張された理論が多くの経済分析に応用されており，価格などの一次元的な尺度に限らずより一般的な選好を考慮した交渉の議論が可能になっている．3\n\none way to try to account for political choices is to imagine that each chooser occupies a fixed position in a space of one or more dimensions, and to suppose that every choice presented to him is a choice between two or more points in that space. (MacRae, 1958)\n\n\n\n1.4 理想点解析の発展\n現代では空間理論と空間モデルは，投票などの政治過程，そして議会などにおける立法過程の研究に応用される．広く交渉における空間理論については (林光, 2016) も参照．\nさらには純粋にイデオロギーという概念を定量化することにも用いられる．\n古くイデオロギーとは一見バラバラに見える政治的問題の相互の繋がりに関する信念体系である (Converse, 2006)．\n特にリベラル - 中道 - 保守，左 - 右などといった空間的な理解は長らく用いられているものであるが，これは本人が既存のイデオロギーに倣って行動しているというより，よく見られる一貫した行動パターンに名前をつけたものというべきである (Hinich and Pollard, 1981)．\n一貫した行動パターンの分類，その分類がどれほど行動の予測に有用であるか，これらの尺度は統計学の本領というべきである．\n\n\n1.5 点呼投票データ\n理想点解析で最もよく使われるデータとして，各政治家が審議期間にて表明した投票記録，特に 点呼投票 (Roll Call Voting) 記録が用いられる．\n点呼投票データを扱う展開法 (roll-call scaling method) として初めに提案されたものが NOMINATE (nominal, three-step estimation) (Poole and Rosenthal, 1985) であり，次の３段階からなる：\n\n議員の理想点の推定\n法案に対する応答が対応する点の推定\n議員の効用関数のパラメータ推定\n\n\n\n\n\n\n\nNOMINATE の発展\n\n\n\n\nD-NOMINATE (Poole and Rosenthal, 1991)\n点呼投票データの時系列構造も取り込めるようにした拡張．D は dynamic の略である．\nW-NOMINATE (Poole and Rosenthal, 1997)．\nW は weighted の略であり，パソコン上でも動くように設計されたアルゴリズム．現在は R パッケージ wnominate (Poole et al., 2011) で利用可能．\n(Heckman and Snyder, 1997) は同様の手法を因子分析の言葉で定式化している．\nDW-NOMINATE (McCarty et al., 1997)\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n1.6 項目反応モデルとしての理想点解析 (Clinton et al., 2004)\nNOMINATE の方法には政策次元が \\(K=1\\) などの隠匿された仮定があり，これらの仮定を緩めることが必ずしも簡単ではなく，モデル比較の議論となるとほとんど十分な理論的根拠を持たなかった．\n理想点推定を統計モデル，特に 項目反応モデル（第 3 節）とみなし，従来は局外母数とみなされた項目毎の母数も，ベイズの枠組みで同時に推論・モデル比較を行うことが (Jackman, 2000), (Jackman, 2001), (Clinton et al., 2004) によって提案された．4\nここでは (Imai et al., 2016) で「標準的な理想点モデル」とされている BIRT (Bayesian Item Response Theory) (Clinton et al., 2004) の定式化を紹介する．\n\n\n\n\n\n\n標準的な理想点モデル\n\n\n\n\n\\(i\\in[N]\\) 番目の議員が \\(j\\in[J]\\) 番目の法案に対して賛成ならば \\(y_{ij}=1\\)，反対ならば \\(y_{ij}=0\\) のデータが得られているとする．\nこのとき \\(i\\in[N]\\) 番目の議員の理想点 \\(x_i\\in\\mathbb{R}^K\\) は，\\(y_{ij}\\) を次のように予測する潜在変数とする： \\[\\begin{align*}\n  y_{ij}&=1_{\\mathbb{R}^+}(y^*_{ij})\\\\\n  y^*_{ij}&=\\alpha_j+x_i^\\top\\beta_j+\\epsilon_{ij},\\qquad\\epsilon_{ij}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1)\\\\\n  &=:\\widetilde{x}_i^\\top\\widetilde{\\beta}_j+\\epsilon_{ij}.\n\\end{align*}\\]\nすなわち \\(K\\)-母数のロジット項目反応モデル 3.4 において，議員ごとの母数である \\(x_i\\) を 理想点 と呼ぶ．項目識別母数 \\(\\beta_j\\) は法案ごとの性質の違いを表しているものと考える．\n換言すれば，次のプロビットモデルが想定されたことになる： \\[\n\\operatorname{P}[y_{ij}=1]=\\Phi(\\widetilde{x}_i^\\top\\widetilde{\\beta}_j).\n\\]\n\n\nこのモデルは潜在変数 \\(Y^*\\) とパラメータ \\((x_i)_{i=1}^N\\in\\mathbb{R}^{KN},(\\beta_j)_{j=1}^J\\in\\mathbb{R}^{KJ}\\) を持つ．\nプロビット項目反応モデル 3.4 は，項目反応モデルの文脈でデータ拡張に基づく Gibbs サンプリングによるベイズ推定が古くから議論されていた (Albert, 1992)．\n(Patz and Junker, 1999) はロジスティックモデルに対して Metropolis-Hastings within Gibbs アルゴリズムを提案している．5\n\n\n\n\n\n\n注：ランダム効用理論との離別\n\n\n\n\n\nここでは (Clinton et al., 2004) を踏襲した (Imai et al., 2016) の定式化に従った．\n(Clinton et al., 2004) の定式化は ランダム効用理論 (random utility framework) (McFadden, 1976) に従い，議員 \\(i\\in[N]\\) の効用関数を用いていたという点で NOMINATE (Poole and Rosenthal, 1985) を踏襲していた．\nだが，(Jackman, 2001) ではこれを統計モデル（項目反応モデル）として解釈する際に \\(U_i\\) を排している．\n(Clinton et al., 2004) の設定では，理想点 \\(x_i\\in\\mathbb{R}^K\\) は次の効用関数 \\(U_i:\\{\\zeta_j,\\psi_j\\}_{j=1}^J\\to\\mathbb{R}\\) を通じて意思決定に影響するとした： \\[\nU_i(\\zeta_j)=-\\lvert x_i-\\zeta_j\\rvert^2+\\eta_{ij},\n\\] \\[\nU_i(\\psi_j)=-\\lvert x_i-\\psi_j\\rvert^2+\\nu_{ij}.\n\\] ただし，\\(\\eta_{ij},\\nu_{ij}\\) は互いに独立な Gauss 誤差とし，\\(\\eta_{ij}-\\nu_{ij}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma_j^2)\\) とする．\n最終的に議員 \\(i\\in[N]\\) は \\[\ny_{ij}=1_{\\left\\{U_i(\\zeta_j)&gt;U_i(\\psi_j)\\right\\}}\n\\] として投票行動を表現する．\nこの統計モデルは \\[\n\\operatorname{P}[y_{ij}=1]=\\Phi(\\beta_j^\\top x_i-\\alpha_j)\n\\] \\[\n\\beta_j:=\\frac{2(\\zeta_j-\\psi_j)}{\\sigma_j},\\qquad\\alpha_j=\\frac{\\zeta_j^\\top\\zeta_j-\\psi_j^\\top\\psi_j}{\\sigma_j}\n\\] という probit モデルに等価になる．6\nなお誤差 \\(\\eta_{ij},\\nu_{ij}\\) を極値分布に従うとすると logit モデルを得る．logit モデルは NOMINATE (Poole and Rosenthal, 1985) 発表前に計算資源が少なかった時代に用いられていたが，その後は誤差の正規性の仮定が優先された (Poole and Rosenthal, 2001)．\n効用関数 \\(U\\) は NOMINATE では Gauss 密度，(Heckman and Snyder, 1997) と (Clinton et al., 2004) では二次関数を用いていた．\n(Clinton et al., 2004) では \\(x_i,\\widetilde{\\beta}_j\\) に独立な共役事前分布 \\[\np(x_1,\\cdots,x_N)=\\prod_{i=1}^N\\phi_K(x_i;\\mu_x,\\Sigma_x)\n\\] \\[\np(\\widetilde{\\beta}_1,\\cdots,\\widetilde{\\beta}_J)=\\prod_{j=1}^J\\phi_{K+1}(\\widetilde{\\beta}_j;\\mu_{\\widetilde{\\beta}},\\Sigma_{\\widetilde{\\beta}})\n\\] を仮定した．\\(\\phi_d\\) は \\(d\\) 次元の Gauss 密度である．\n(Clinton et al., 2004) ではこのモデルの２パラメータの項目反応モデルとの対応に基づいて，データ拡大に基づく Gibbs サンプラーによる推定が WinBUGS (Lunn et al., 2000) によりなされ，R パッケージに実装され，現在も pscl (Political Science Computational Laboratory) パッケージ (Zeileis et al., 2008) に実装されている．\n\n\n\n\n\n\n\n\n\n単一指標モデルとの関係\n\n\n\n\n\n被説明変数が \\(y_{ij}\\in\\{0,1\\}=2\\) であるこのモデルは，計量経済学では 二項選択モデル (binary choice model) として知られている (Chapter 25 Hansen, 2022, p. 801)．\n計量経済学ではプロビット，ロジットモデルの他に，リンク関数 \\(G\\) の関数系を局外母数としたセミパラメトリックモデルである 単一指標モデル (single-index model) \\[\n\\operatorname{P}[y_{ij}=1]=G(x^\\top_i\\beta_j)+\\epsilon_{ij}\n\\] が考えられる．\n特に \\[\ny_{ij}=1_{\\mathbb{R}^+}\\biggr(\\widetilde{x}_i^\\top\\widetilde{\\beta}_j+\\epsilon_{ij}\\biggl)\n\\] という（潜在変数）モデルでは，\\(\\epsilon_{ij}\\) の分布関数を \\(F\\) とすると， \\[\nY_{ij}\\sim\\mathrm{Ber}(F(X^\\top_i\\beta_j))\n\\] \\[\n\\operatorname{P}[y_{ij}=1]=F(x^\\top_i\\beta_j)\n\\] というモデルと等価になり，単一指標モデルに一致する (Section 25.4 Hansen, 2022, p. 804)．\nこの設定で \\(F\\) を未知のままでも \\(\\beta_j\\) に関してセミパラメトリック推定ができる (Klein and Spady, 1993)．\nただし，理想点推定の場合のように \\(X\\) に定数項があると識別可能性が失われるため，追加の制約が必要である．また \\(\\beta\\) も定数倍を除いて識別される．\n\n\n\n\n\n1.7 ベイズ計算の問題\n理想点推定にベイズモデルを立てて MCMC により推定する方法は動的なモデル (Clinton and Meirowitz, 2001), (Martin and Quinn, 2002)，戦略的投票 (Clinton and Meirowitz, 2017), 階層モデリング (Bafumi et al., 2005) へ拡張され，主流の方法となった．\nしかし (Martin and Quinn, 2002) では 47 年の米国最高裁データの分析に５日間かかっている．特に pscl (Zeileis et al., 2008) による Gibbs サンプリングがデータの不均衡性によって収束に苦しんでいる可能性がある．\nそこでベイズの方法で理想点解析をやりたいが，理想点推定はモデルが大規模になるために効率的な計算手法が必要となっている．\n\n\n1.8 変分 EM アルゴリズム\n(Imai et al., 2016) は高速なベイズ推論のために変分 EM アルゴリズムを提案し，emIRT パッケージに実装している．\n種々のタイプのモデル（多値反応モデル，動的モデル，階層モデル，テキストデータ）を考察しているので，種々の理想点解析モデルのレビューとしても有用である．\nその共通するアプローチは \\(Y^*\\) を欠測データと扱い，\\(\\widetilde{x}_i,\\widetilde{\\beta}_j\\) を同時に EM アルゴリズムにより推定し，特定の基準に基づいてアルゴリズムを停止することである．その途中で変分近似を用いる．\nベイズ的な不確実性の可視化を得るために NOMINATE のようにパラメトリックブートストラップ (Carroll et al., 2009), (Lewis and Poole, 2004) を行う．\n(Imai et al., 2016) の変分 EM アルゴリズムにより \\(d=1\\) 次元空間上の理想点を推定した結果が (三輪洋文, 2017) で公開されている：\n\n\n\nTwitter データとプロビットモデルによる理想点推定 (三輪洋文, 2017, p. 51)\n\n\n\n\n1.9 その他のデータ源の探索\n点呼投票データには，政党規律や 票取引 (logrolling) などの戦略的投票行動がある際には，必ずしも個人の政治的信条を反映しないという欠点がある．\nそこで点呼投票データの他に有用なデータ源の探索とそれを用いた理想点推定の方法が模索されており，データ統合が最終的な目標として目指されている．\n特に日本では政党規律が強く，点呼投票データが適さないため，政治家へのサーベイや質問，専門家調査 (加藤淳子, 2021) によってデータが収集されることが多いという (三輪洋文, 2017), (Miwa and Taniguchi, 2017)．\nこのテキストベースのアプローチは，政党が公開しているマニフェストなどの客観的なデータも取り入れることが可能であるという点に美点がある (岡田謙介 and 加藤淳子, 2016)．\nまた近年では，Twitter が政治家の政策と信条の空間的位置について多くの情報を含んでいる情報源として注目されている (Barberá, 2015), (三輪洋文, 2017)．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#多次元展開法としての理想点解析",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#多次元展開法としての理想点解析",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "2 多次元展開法としての理想点解析",
    "text": "2 多次元展開法としての理想点解析\n\n2.1 Twitter データと項目反応モデルを通じた多次元尺度展開\n(Barberá, 2015) は特に Twitter において誰が誰をフォローしているかのデータに注目した．\n\\((y_{ij})\\in M_n(2)\\) を，ユーザー \\(i\\) がユーザー \\(j\\) をフォローしているかを２値で表した \\(0,1\\) 成分行列とし，この関係が政策空間 \\(\\mathbb{R}^d\\) におけるユーザー \\(i,j\\) の距離の近さによって決定されているとする．\n\\(\\theta_i:[n]\\to\\mathbb{R}^d\\) をユーザーの政策空間への埋め込みとすると，\\(g\\) をリンク関数として \\[\ng\\biggr(\\operatorname{P}[Y_{ij}=1\\,|\\,\\alpha_j,\\beta_i,\\theta]\\biggl)=\\alpha_i+\\beta_j-d(\\theta_i,\\theta_j)\n\\] とするのである．\nただし，\\(\\alpha_j\\) は知名度，\\(\\beta_i\\) は政治的関心を表す説明変数とした．\nこれにより Gibbs サンプラーにより \\(\\alpha,\\beta,\\theta\\) の推定が可能になるが，この方法では推定が遅く，また大規模なデータや偏りのあるデータに弱い．\nこの問題点は Zig-Zag サンプラーによって解決され，さらに推定が高速になる．詳しくは次の稿も参照：\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.2 多次元展開法としての展開\n(Bakker and Poole, 2013) は理想点解析を多次元尺度法と見て，ベイズ化の方法を提案している．\n多次元空間への多次元尺度構成法は，非線型次元縮約法，多様体学習法，埋め込み法などといった種々の名前の下で考察されている．\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\nNo matching items\n\n\n逆に言えば，これらの他手法と比較したり，長所と短所を洗い出すことで，個々の手法に対する理解が深まるかもしれない．\n(Escolar et al., 2023) では特許のデータを用い，各企業を技術空間 \\(\\mathbb{R}^{430}\\) 内に埋め込んだ後，mapper (Singh et al., 2007) によりグラフ化したところ，企業の独自戦略が可視化されたという．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#sec-IRT",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#sec-IRT",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "3 項目反応理論",
    "text": "3 項目反応理論\n\n3.1 はじめに\n(Jackman, 2001), (Clinton et al., 2004) でも自覚されているように，理想点解析は多次元尺度構成法であると同時に，点呼投票という２値応答に特化した項目反応理論とも見れる．\n項目反応理論 (IRT: Item Response Theory) は 現代テスト理論 とも呼ばる．\n因子分析に基づいた古典テスト理論とは異なり，特定の項目に被験者がどのように応答するかを左右する種々の潜在変数を柔軟に取り入れることを可能にする モデルベース の枠組みである．7\nその柔軟性のため，コンピュータを通じた適応的なテスト などの現代的な設定における心理測定・行動計量の基礎を支えている．\n\n\n3.2 項目反応理論の歴史\n項目反応理論の初まりは (F. M. Lord et al., 1968) と ETS における実践・セミナーと目されている．\n書籍 (F. M. Lord et al., 1968) はテストに対して真に統計的でモデルベースな扱いを創始したと評されている (Embretson and Reise, 2000)．\nただし，同様の取り扱いはデンマークにて (Rasch, 1960) により早くから用いられており，この２つが IRT の源流とされている (Embretson and Reise, 2000)．\n(Rasch, 1960) のモデルは２値応答の確率を，個人と項目とのそれぞれ１母数の関数としてモデリングする最も単純なものであった．\n長らくこの研究はヨーロッパを出ず，(Fischer, 1973) がこれを拡張し翌年に教科書も書いたが，ドイツ語であったので世界的には広まらなかった．\n最終的に２つの流れが邂逅したのは Benjamin Wright を介してであった．\n1960 年に Rasch が Wright を訪問して以来，Rasch モデルの客観的測定 (objective measurement properties) の重要性を評価し，その推定方法を FORTRAN により実装した (Wright and Panchapakesan, 1969)．\nその後 Wright の下で学んだ多くの学生が (Rasch, 1960) のモデルに関して基礎的な研究を行なった．8\n\n\n3.3 項目反応モデルの応用\n項目反応モデルは個々人レベルの応答変数に基づいて，個人ごとに違う潜在変数 \\(\\theta_i\\) と項目ごとに違うパラメータ \\(\\xi_j\\) の推定を実行する際に広く用いられる．\n\\(\\theta_i\\) は典型的には個々人の「能力」といった概念構成を表すパラメータで 能力母数 (ability parameter) とも呼ばれる (Fox, 2010, p. 6)．一方 \\(\\xi_j\\) は難易度パラメータ (difficulty parameter) ともいう．\n項目反応モデルの用途は主に潜在変数の測定 (measurement) と多次元尺度構成 (scaling) との２つに分けられる．\n理想点解析は後者の用途に属する．これはパラメータ \\(\\theta_i\\) がテストの種類などの測定方法に依存せず，モデルが同一ならば一定した尺度を持つという項目反応モデルの美点に基づく．この普遍性を Rasch は 固有客観性 (specific objectivity) と呼んだ (井澤廣行, 2008, p. 51)．\nまた複数の項目反応モデルの結果の間で尺度を統一することを，特にテスト分析の分野では リンキング または 等化 (equating) という．9\n\n\n3.4 ２値反応の項目反応モデル\n項目反応モデルでは \\(\\theta_i\\) は応答確率を変化させるとする： \\[\n\\operatorname{P}[Y_{ij}=1]=g_j(\\theta_i),\\qquad i\\in[N],j\\in[J].\n\\] このリンク関数 \\(g_j\\) は 項目特性曲線 (ICC: Item Characteristic Curve / Trace Line) と呼ばれる．\n加えて \\(\\theta_i\\) の値で条件付けたとき，異なる項目への応用は互いに独立であると仮定する（局所独立性 という）：10 \\[\n\\operatorname{P}[Y_{i1}=1,\\cdots,Y_{iJ}=1]=\\prod_{j=1}^J\\operatorname{P}[Y_{ij}=1].\n\\]\n\n\n\n\n\n\n(Rasch, 1960) モデル\n\n\n\n１母数応答モデル (1PLM: one-parameter logistic model) または Rasch モデル とは，個人の母数 \\(\\theta_i\\) と項目の母数 \\(b_j\\) とが定めるロジスティックモデル \\[\n\\operatorname{P}[Y_{ij}=1]=\\biggr(1+e^{b_j-\\theta_i}\\biggl)^{-1}\n\\] である．\n\\(b_j-\\theta_i\\) は \\(b_j,\\theta_i\\) 双方の十分統計量であり，\\(\\theta_i\\) のみを条件付き最尤推定可能である．\n\n\n\\(b_j,\\theta_i\\) は同じ空間 \\(\\mathbb{R}\\) 上にプロットでき，同じ尺度を持つことに注意．\\(\\theta_i\\) が \\(b_j\\) からみて左右のどちらにあるかに依って，応答確率が \\(1/2\\) より大きいか小さいかが決まる．\n\n\n\n\n\n\n２母数ロジットモデル\n\n\n\n２母数ロジットモデル (2PLM: two-parameter logistic model) とは，項目 \\(j\\in[J]\\) が２つの母数 \\(a_j,b_j\\) でパラメータ付けられたロジスティックモデル \\[\n\\operatorname{P}[Y_{ij}=1]=\\biggr(1+e^{b_j-a_j\\theta_i}\\biggl)^{-1}\n\\] である．\\(a_j\\) は 項目識別力母数 (item discrimination parameter) ともいう．11\nもはや条件付き最尤推定は不可能であるが，(Bock and Lieberman, 1970) は能力母数を局外母数として項目母数を推定する方法を数値積分法によって与えた．\nEM アルゴリズムによる周辺最尤推定法 (Bock and Aitkin, 1981) は能力母数を局外母数と扱う教育の分野において現在でも標準的な方法の１つである．\n\n\nプロビットモデルも \\(n\\)-PNM (\\(n\\)-Parameter Normal ogive Model) (F. M. Lord et al., 1968, pp. 365–384) として古くから考えられていたが，Gibbs サンプリングの都合上ロジスティックモデルが好まれた．\nロジスティックモデルで推定された空間上で \\(d=1.7\\) のスケーリングの違いを除いて [-3,3] 上ではほとんど一致することが知られている (Hambleton, 1991, p. 15)．\n\n\n3.5 多値項目反応モデル\n正解・誤答の２値以外にも，部分点があるなどの多値項目 (polytomous item) に対する拡張が考えられている．\n\n\n\n\n\n\n(部分得点モデル Masters, 1982)\n\n\n\n部分得点モデル (PCM: Partical Credit Model) (Masters, 1982) とは，項目 \\(j\\in[J]\\) の応答がカテゴリ \\(c\\in[C_j]\\) に当たる確率を \\[\n\\operatorname{P}[Y_{ij}=c]=\\frac{e^{\\sum_{l=1}^c(\\theta_i-\\kappa_{kl})}}{\\sum_{r=1}^{C_j}e^{\\sum_{l=1}^r(\\theta_i-\\kappa_{kl})}}\n\\] で与える．\n\\(\\kappa_{kl}\\) は項目 \\(j\\in[J]\\) の step 難易度パラメータという．\n\n\n(Muraki, 1992) はこれを一般化し，EM アルゴリズムによる推定方法を与えている．\n\n\n\n\n\n\n(段階反応モデル Samejima, 1997)\n\n\n\n段階反応モデル (GRM: Graded Response Model) (Samejima, 1997) では，項目 \\(j\\in[J]\\) の応答確率の分布関数をモデリングし，カテゴリ \\(c\\in[C_j]\\) に当たる確率は \\[\n\\operatorname{P}[Y_{ij}=c]=\\biggr(e^{\\kappa_{j,c-1}-a_j\\theta_i}\\biggl)-\\biggr(e^{\\kappa_{j,c}-a_j\\theta_i}\\biggl)\n\\] で与えられる．\nただし，難易度パラメータには \\[\n-\\infty=\\kappa_{j,0}&lt;\\kappa_{j,1}&lt;\\cdots&lt;\\kappa_{j,C_j}=\\infty\n\\] という順序制約が必要になる．\n\n\n\n\n3.6 多次元の項目反応モデル\n\n\n\n\n\n\n多次元項目反応モデル\n\n\n\n\\(b_j-a_j^\\top\\theta_j\\) という指標を多次元化することで，(Rasch, 1960) のモデルを多次元化することができる： \\[\n\\operatorname{P}[Y_{ij}=1]=\\operatorname{expit}\\biggr(-b_j+a_j^\\top\\theta_i\\biggl).\n\\]\n\n\n空間理論（第 1.2 節）の端緒からして，単なる１次元の左-右といった軸ではなく，多次元の潜在空間上に各政治家の理想点を写像したい，という悲願がある (岡田謙介 and 加藤淳子, 2016)．\nこのように新たな次元も考慮に入れることで，リベラル - 保守といった概念への理解が進むことが期待される上に，予測などの下流タスクの精度の大きな向上も望めるだろう．\n一般に複雑な構成概念の精緻な検証が可能になる (坂本佑太朗 and 柴山直, 2017) ため，多次元項目反応モデルは近年注目されており，これを実現する統計計算法が必要とされている．\n特に識別可能性の問題が深刻になるが，それがベイズのアプローチでは，\\(\\ell_2\\)-ノルムベースであったところを \\(\\ell_1\\)-ノルムベースにすることで，推定の安定性と効率性が向上することなどが考えられている (Lim et al., 2024)．\n\n\n3.7 理想点解析の認知モデリングとしての展開\n従来の理想点解析における参照軸は，純粋に複雑な政治的現象を理解するための構成概念として利用された．\n一方で理想点解析と項目反応理論との類似性に気付いた以上，応答過程に認知科学的変数も取り入れることは自然な拡張の１つとして試みられてきた (Lee, 2001)．\n例えば個々人の認知過程の違い (Embretson (Whitely), 1984) (DIF: Differential Item Functioning) (Frederic M. Lord, 1980, p. 212) や発達段階の違い (Wilson, 1984) も変数に取り入れることが考えられている．\nそこで近年，理想点推定が出力する「次元」に対する人間の空間的認知との関係を明示的に取り入れたモデリングをしようという試みが，行動計量学との接点で考えられている (岡田謙介 and 加藤淳子, 2016)．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#文献紹介",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#文献紹介",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "4 文献紹介",
    "text": "4 文献紹介"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/IdealPoint.html#footnotes",
    "title": "理想点解析・多次元展開法・項目応答理論",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(浅古泰史, 2016, p. 69) を参考．↩︎\n(浅古泰史, 2016, p. 75) を参考．↩︎\nこの交渉理論におけるコンテクストから，理想点 というのである．各主体が理想とする点，という意味である．↩︎\n“In short, the goal that Bayesian methods make plausible is a transformation of roll call analysis, from a technical scaling or measurement problem best left to psychometricians (witness the canonical status of NOMINATE scores) to something that scholars motivated primarily by substantive concerns can do for themselves.” (Jackman, 2001, p. 240)．↩︎\nさらに詳しくは (Fox, 2010, pp. 71–) も参照．↩︎\nprobit とは (Bliss, 1934) が probability unit から名付けた．↩︎\n例えば消費者の購買行動をモデリングする際は，選択疲れをした消費者は中間的な商品を選びやすいという 妥協効果 (compromise effect) (Simonson, 1989) などの文脈効果もモデルに入れる必要がある (加藤拓巳, 2021)．↩︎\nだが，Bock も Wright の教え子も主に教育学で活躍しており，最終的に心理学者に心理測定の基本として古典テスト理論を IRT が代替したのは 2000 年代になってからだったという (Embretson and Reise, 2000, p. 7)．↩︎\n２つは厳密には，等化は一番強い仮定のもとで行われるリンキングの一つである (宇佐美慧 et al., 2018)．例えば集団の基礎学力が違った場合，同一の困難度を測定するためでも別のバージョンのテストを作成する必要がある．等化は ICC が affine 合同である場合に affine 変換により可能である (宇佐美慧 et al., 2018)．↩︎\nこれが成り立つように，１つの設問で問われる能力は１つになるように設計することが原則である (宇佐美慧 et al., 2018)．↩︎\n３母数ロジットモデルにおいて加わる母数は当て推量母数／下方漸近パラメータとも呼ばれる (宇佐美慧 et al., 2018)．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html",
    "title": "ベイズ生存時間解析",
    "section": "",
    "text": "Zig-Zag サンプラーによるモデル平均法\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#次稿",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#次稿",
    "title": "ベイズ生存時間解析",
    "section": "",
    "text": "Zig-Zag サンプラーによるモデル平均法\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#生存時間と競合リスクの解析",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#生存時間と競合リスクの解析",
    "title": "ベイズ生存時間解析",
    "section": "1 生存時間と競合リスクの解析",
    "text": "1 生存時間と競合リスクの解析\n\n1.1 はじめに\n医療経済学 などにおける 医療技術評価 (HTA: Health Technology Assessment) とは，新たな医療技術を臨床試験で評価し，リスクやコストを勘案して新技術の既存法との効果や安全性を評価・比較する 決定理論的な枠組み である．\nこの枠組みでは 生存時間 と呼ばれる非負確率変数 \\(Y\\) に対して，例えば次のような平均の計算が必要になる： \\[\n\\operatorname{E}[Y]=\\int^\\infty_0S_Y(y)\\,dy,\\qquad S_Y(y):=\\operatorname{P}[Y\\ge y].\n\\tag{1}\\]\n医療技術評価の文脈において 生存時間 \\(Y\\) とは被験者のイベント（発症・死亡など）までの時間 (time-to-event) を表す確率変数で，\\(S_Y\\) は \\(Y\\) の 生存（割合）関数 といい，\\(Y\\) の分布関数 \\(F_Y\\) と次の関係を持つ： \\[\nS_Y(y)=1-F_Y(y).\n\\]\n平均生存時間 (1) は英国の医療技術評価機構 (NICE) が，医療技術評価の上で推奨する指標の一つである (Hardcastle et al., 2025)．\nただし，平均生存時間 (1) を計算するためには，観測の打ち切りを乗り越えて \\(S_Y\\) を \\([0,\\infty)\\) 全域で推定する必要がある．\n多くの臨床試験ではコストの関係上期限があり，これを超えた時点に発生するイベントは観測が打ち切られる (censoring)．しかし 2018 年以来の NICE の癌治療法査定の半分以上は，多くのイベントが打ち切りの後に発生してしまうデータを用いている (Gibbons and Latimer, 2025)．\nこれらのデータから，打ち切りを超えた外挿をし，加えて平均生存時間 (1) のような量を推定するためには，例えば Sheffield elicitation framework などを用いた専門家からの聞き取りによる事前情報を明示的に扱うことができるベイズ手法 (Mikkola et al., 2024) が有望視されている（第 1.5 節）．\n\n\n1.2 生存時間解析とは\nイベントまでの時間 \\(Y\\) の生存関数 \\(S_Y\\) を推定する問題は 生存時間解析 (survival analysis) または信頼性解析として知られ，ほとんど統計学の起源と同時に始まる長い歴史を持つ．1\n共変量 \\(X\\) の生存時間 \\(Y\\) への影響を調べる際には，生存関数 \\(S_Y\\) の代わりに ハザード関数 （または瞬間故障率，死亡率） \\[\nh(y):=\\frac{f(y)}{S(t)}=-\\frac{d \\log S(y)}{d t},\\qquad f(y)=F'(y)=-S'(y),\n\\] もモデリングの対象になる．ただし \\(f\\) は \\(Y\\) の確率密度関数とした．\nハザード関数 \\(h\\) は，被験者の生存割合が \\(S(y)\\) である段階での，次の単位時間でのイベント発生率を表す．ハザード関数が判れば，生存関数は \\[\nS(y)=\\exp\\left(-\\int^y_0h(t)\\,dt\\right)\n\\] によって復元される．\nさらに究極の目標として，生存時間 \\(Y\\) に影響を与える共変量 \\(X\\) の成分を特定することがある．この際に用いるモデルを 競合リスクモデル という（第 1.7 節）．\n競合する \\(K\\) 個のリスク要因がそれぞれハザード \\(h_1,\\cdots,h_K\\) を持つとき，イベント発生時刻 \\(Y\\) のハザードは和 \\[\nh_Y(y)=\\sum_{j=1}^Kh_j(y)\n\\] で与えられる．これを 多ハザード模型 (Polyhazard model)（第 1.6 節），\\(h_i\\) を 原因別ハザード という．\n\n\n1.3 生存関数推定\n\n1.3.1 欠測の問題\n生存時間解析における最大の問題は観測の 打ち切り (censoring) である．\n換言すれば，ほとんどの生存時間データでは種々の理由で被験者が脱落し，追跡終了時点以降はイベントの発生を確認できないのである．\nそこで，５割生存時間 (MST: Median Survival Time) を代わりに推定対象としたり，ある打ち切り時刻 \\(T&gt;0\\) までの区間のみに限って \\(S:[0,T]\\to[0,1]\\) を推定することが考えられた．\n\n\n1.3.2 パラメトリックモデル\n\\(S_Y\\) をパラメトリックな分布族の中から推定することが考えられる．\nこの場合，形状母数 \\(\\nu&gt;0\\) によってハザード関数の増加・減少を柔軟にモデリングできる Weibull 分布 \\(\\mathrm{W}(\\nu,\\alpha)\\) または (Rosin and Rammler, 1933) の分布 \\[\nf(x;\\nu,\\alpha)=\\frac{\\nu}{\\alpha}\\left(\\frac{x}{\\alpha}\\right)^{\\nu-1}\\exp\\left(-\\left(\\frac{x}{\\alpha}\\right)^\\nu\\right)1_{\\left\\{x&gt;0\\right\\}},\\qquad\\alpha&gt;0,\\nu&gt;0,\n\\] \\[\nh(t)=\\frac{\\nu}{\\alpha}\\left(\\frac{t}{\\alpha}\\right)^{\\nu-1}1_{\\left\\{t&gt;0\\right\\}}\n\\] をはじめとして，Gamma 分布，対数正規分布，対数ロジスティック分布分布，パレート分布など，極値分布の指数変換様の分布族が用いられる．\n\n\n1.3.3 ノンパラメトリックモデル\nこれには (Kaplan and Meier, 1958) や (Cutler and Ederer, 1958) の方法が古来より有名である．\n\n\n\n1.4 回帰\n\n1.4.1 ハザードの回帰\n生存時間解析の主な目的は，生存曲線の正確な描画というより，生存時間を決定する要因の特定にある．\n(Cox, 1972) はベースとなる生存関数 \\(S_0\\) (baseline survival curve) を局外母数として，\\(S\\) と \\(S_0\\) の関係をパラメトリックにモデリングする．\nより正確には，ハザード関数 \\(h\\) のベース \\(h_0\\) からの比の対数を，線型な予測子 \\[\n\\log \\frac{h(y)}{h_0(y)}=X^\\top\\beta+\\epsilon\n\\] によってモデリングする．\n一般にハザードの比を \\[\nh(y|x)=h_0(y)c(X^\\top\\beta)\n\\] とモデリングするものを 乗法的ハザードモデル といい，特に \\(c=\\exp\\) と取った場合を Cox の 比例ハザードモデル (Cox’s proportional hazard model) とも呼ばれる．\nこの方法は，打ち切りデータへの対処が簡便になることが美点である．\n\n\n1.4.2 生存時間の回帰\n生存時間 \\(Y\\) の対数を直接 \\[\n\\log Y=\\mu+\\beta^\\top X+\\epsilon\n\\] によってモデリングする方法は 加速故障時間モデル (AFT: Accelerated Failure Time Model) (Wei, 1992) と呼ばれる．\nこのモデルはハザード関数に，\\(h_0\\) を \\(x=0\\) の場合のハザード関数として \\[\nh(y|x)=e^{-\\beta^\\top x}h_0(ye^{-\\beta^\\top x}),\n\\] という乗法的な仮定をおいていることに相当する．\n\n\n\n1.5 生存関数推定再論\n一方で医療行為の社会的な影響も考える HTA の目標を達成するためには，式 (1) のような量を計算する必要がある．\nそのためには，生存曲線の推定と同時に打ち切り時点以降の外挿もできるようなモデルを考える必要があるが，Kaplan-Meier 法などのノンパラメトリック法は（現状）この用途には用いることができない．\n表現力が高いパラメトリックモデルをベイズ推定することが，非常に魅力的な解決策として考えられ，実際 NICE のガイドラインでも推奨されている (Latimer, 2011)．\nその際の魅力的なモデルに polyhazard model (Berger and Sun, 1993), (Louzada-Neto, 1999) がある．\n(Latimer, 2013) では現状の HTA 分析では，生存時間モデルに対してモデル検証・モデル選択が不十分であることに警鐘が鳴らされている．\npolyhazard model のような階層モデルを効率的にベイズ推定・モデル平均化ができるような MCMC 法が開発されることは，このモデル検証の手続きを自動化したり，より手軽にするために非常に重要である．\n\n\n1.6 多ハザードモデルの表現力\nPolyhazard model もハザード関数をモデリングするが， \\[\nh_Y(y)=\\sum_{j=1}^Kh_j(y)\n\\] という形でモデリングし，個々の \\(h_j\\) にパラメトリックな仮定をおく．\n仮に \\(h_j\\) として，形状母数 \\(\\nu&gt;0\\) と位置母数 \\(\\mu:=\\alpha^{-\\nu}&gt;0\\) を持つ Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) のハザード関数2 \\[\nh_{\\mathrm{W}}(y):=\\mu\\nu y^{\\nu-1}\n\\] と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) のハザード関数 \\[\nh_{\\mathrm{LL}}(y):=\\frac{\\left(\\frac{\\nu}{\\mu}\\right)\\left(\\frac{y}{\\mu}\\right)^{\\nu-1}}{1+\\left(\\frac{y}{\\mu}\\right)^\\nu}\n\\] の２つのみを考えたとしても，複数のパラメトリックモデルを足し合わせることで驚異的な表現力を達成することができる．\n\n\n\n(Hardcastle et al., 2024, p. 5) より．\n\n\n\n\n1.7 競合リスク解析\n\n1.7.1 モデルの解釈\npolyhazard モデルでは各 \\(h_k\\) の前に係数がついていない点には注意が必要である．\n各 \\(h_k\\) は実在の原因のハザードを表しており，各被験者は同時に \\(K\\) 個のリスクに晒されているというモデルである（第 1.2 節）．\nこのようなモデルを 競合リスクモデル (competing risk model) ともいう．\n例えば，心臓の移植後のハザード曲線はバスタブ曲線の形を持ち，少なくとも２つの別々の競合するリスク要因が存在することが窺える (Demiris et al., 2015)．\n\n\n1.7.2 競合リスクモデリング\nしかし，リスク因子はほとんどの場合観測できず，潜在変数となる．\\(K\\) の数も不確実である．\nこのような識別不可能なモデルは，階層モデルとしてベイズ推論を実行することが向いていると言えるかも知れない．\nさらに発展的なモデルにはマルチステートモデルもある (齋藤哲雄 and 室谷健太, 2023), (Saito and Murotani, 2024)．\n\n\n\n1.8 ベイズ階層多ハザードモデル\n(Hardcastle et al., 2024) では HTA への応用を念頭に，完全なベイズ階層多ハザードモデルの推定を試みている．ここではそのモデルの詳細を紹介する．\n\n1.8.1 第１階層\n各個別要因 \\(k\\in[K]\\) の形状母数 \\(\\nu_k\\) と位置母数 \\(\\mu_k\\) に階層構造 \\[\n\\log(\\nu_k)=\\alpha_k\\sim\\operatorname{N}(0,\\sigma_\\alpha^2)\n\\tag{2}\\] \\[\n\\log(\\mu_k)=\\beta_{k,0}+\\sum_{j\\in\\left\\{j\\in[p]\\mid\\gamma_{kj}=1\\right\\}}x_j\\beta_{k,j},\\qquad\\beta_{k,0}\\sim\\operatorname{N}(0,\\sigma_{\\beta_0}^2)\n\\tag{3}\\] を考える．ただし，\\(\\gamma_{k,j}\\in2\\) は共変量 \\(x_j\\) が \\(k\\in[K]\\) 番目の部分モデルに参加するかどうかを決める指示変数とする．\n残っているパラメータ \\(\\beta_{k,j}\\;(j\\in[p])\\) には \\[\n\\beta_{k,j}\\sim(1-\\omega)\\delta_0+\\omega\\operatorname{N}(0,\\sigma_\\beta^2)\n\\] と spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定し，変数選択を促進する．\n以降，\\(\\theta_k=(\\nu_k,\\mu_k)\\) とし，\\((K,\\gamma,\\theta)\\) を本モデルのパラメータとする．\n\n\n1.8.2 第二階層\n\\(\\sigma_\\alpha^2=2,\\sigma_{\\beta_0}^2=5\\) は固定してしまうと，\\(\\phi:=(\\omega,\\sigma_\\beta)\\) がハイパーパラメータとして残っている．これには \\[\n\\omega\\sim\\operatorname{Beta}(a,b)\n\\] \\[\n\\sigma_\\beta\\sim\\operatorname{HalfCauchy}(0,1)\n\\] という事前分布をおく．\n前者はモデルのサイズについて Beta-二項分布を仮定することに等価である (3.1 節 Ley and Steel, 2009)．後者は (Gelman, 2006), (Polson and Scott, 2012) の推奨の通りである．\n\n\n1.8.3 \\(\\mathcal{P}(E)\\) 上の事前分布\n実はまだ第一階層のパラメータが残っている．ハザードの数 \\(K\\) と \\(h_k\\) の関数形をどうするかである．\nここでは Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) の２つ \\[\nD=\\{\\mathrm{W}(\\nu,\\mu),\\mathrm{LL}(\\nu,\\mu)\\}\n\\] から等確率で \\[\nK\\sim\\mathrm{Pois}_{&gt;0}(\\xi)\n\\] 個選ぶこととする．\nハイパーパラメータ \\(\\xi\\) については，(Hardcastle et al., 2024) では \\(\\xi=2\\) としている．この根拠は \\[\n\\operatorname{P}[K&gt;4]\\approx0.061\n\\] であることとしている．\n\n\n1.8.4 部分的な階層モデル\n元々部分的に階層化された多ハザードモデルは広く考えられていた．\n(Demiris et al., 2015) は \\(K\\in\\{1,2,3,4\\}\\) の poly-Weibull モデルを推定し，その適合具合を比較した．\n(Benaglia et al., 2015) では \\(K=2\\) に限ったが，bi-Weibull モデルと bi-Gompertz モデルを推定し，視覚化手法によりモデルの適合具合を比較した．\nこのように個別のモデルを推定してベストのものを選び出す方法は，モデルの仮定を緩和して多くの変数を動かすようにすればするほど難しくなっていく．\n例えば分布族も種々のものを考え，\\(K\\in\\{1,2,3,4\\}\\) のいずれも考えるとなると，推定すべき個別のモデルは乗法的に増加していく．\nそこで (Negrín et al., 2017) は，モデルを「比較」して「選択」するよりもむしろ，ベイズモデル平均 (BMA: Bayesian Model Averaging) を用いることでモデルの不確実性を考慮しつつ最終的なモデルを得ることを提案した．\nしかし，そのモデルは \\(K=2\\) の Weibull 分布族にのみ限っていた．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#pdmp-によるベイズ競合リスク分析",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#pdmp-によるベイズ競合リスク分析",
    "title": "ベイズ生存時間解析",
    "section": "2 PDMP によるベイズ競合リスク分析",
    "text": "2 PDMP によるベイズ競合リスク分析\n(Hardcastle et al., 2024) では完全なベイズ階層 polyhazard モデルに対するサンプラーを Zig-Zag サンプラーに基づいて構成している．ここではこれを紹介する．\n\n2.1 課題\n完全なベイズ階層多ハザードモデルが考えられていなかったことは，次の３つの問題によると考えられる：\n\n\n\n\n\n\nZig-Zag サンプラーが挑む課題\n\n\n\n\n次元の変化 (trans-dimensionality)\n\nラベル \\(k\\in[K]\\) が変わる毎に共変量の数が違い，従って \\(\\beta_k\\) の次元が変わる．従ってサンプラーは，次元の異なる空間の間を往復する必要がある．\n\n尤度の変化\n\n加えて関数空間上にも事前分布をおいているため，\\(h_k\\) の関数形も途中で変わる（第 1.8.3 節）．これに伴い，サンプリングするべき尤度の形も変わってしまう．\n\n識別不可能性\n\nこのモデルは潜在的な \\(K\\) 個のリスクを仮定しており，全てに同等な同等な仮定をおいているため当然識別不可能である．その上事後分布が \\(K!\\) 個の峰を持つ対称性を持ってしまう．このことは MCMC の収束を遅くする上に，最終的に得られる推定量を変に平均化してしまう恐れがある．\n\n\nそこで局外母数 \\((K,D,\\gamma,\\phi)\\) と推定対象 \\(\\theta\\in\\mathbb{R}^{2K+\\lvert\\gamma\\rvert_1}\\) のサンプリングに，多峰性分布につよい Zig-Zag サンプラーを用いる．\n\n\n2.2 \\(\\theta\\) の Zig-Zag サンプリング\nZig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\nモデルが複雑であることに加えて尤度が変動することもあり，Poisson 点過程に対する解析的に上界が見つかるはずもないため，Automatic Zig-Zag (Corbella et al., 2022) を Concave-Convex PDMP (Sutton and Fearnhead, 2023) で修正して用いる．\n続きは次稿参照：\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#文献紹介",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#文献紹介",
    "title": "ベイズ生存時間解析",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n生存時間解析について (武冨奈菜美 and 山本和嬉, 2023) は網羅的で入門的な日本語文献である．\n(西川正子, 2008) は競合リスクモデルに焦点を当てた日本語文献である．(齋藤哲雄 and 室谷健太, 2023), (Saito and Murotani, 2024) はマルチステートモデルも扱っている．(森満, 2016) は医学者による説明が与えられている．\n本項は (Hardcastle et al., 2024) を大きく参考にした．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis.html#footnotes",
    "title": "ベイズ生存時間解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(武冨奈菜美 and 山本和嬉, 2023) は Halley の生命表や Daniel Bernoulli の競合リスク解析の例を挙げている．生命表については ベイズ計算の稿 も参照．↩︎\n位置母数 \\(\\mu:=\\alpha^{-\\nu}&gt;0\\) の変換により第 1.3.2 節のハザード関数と見た目が異なることに注意．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#はじめに",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 ベイズ変数選択\nベイズ変数選択には多くの方法がある．次の記事を参照：\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本記事ではパラメータ推定とベイズ変数選択を同時に行う方法としては State-of-the-art の１つだと思われる Sticky PDMP によるベイズ変数選択を紹介する．\nこの方法はパラメータを PDMP によりサンプリングすると同時に，そのダイナミクスを活かすことで，パラメータ選択の離散空間上で非可逆なジャンプを達成する．\n\n\n1.2 他手法との比較\n\n\n\n\n\n\n\n(3) のような非絶対連続分布からのサンプリングは難しい．そのため spike-and-slab 事前分布 (1) の \\(\\delta_0\\) を分散の小さな正規分布などに軟化した絶対連続な分布に置き換えて，これに対して HMC などの勾配ベースの MCMC 法を適用することもあり得る (Goldman et al., 2022)．\nあるいは初めから Laplace 分布や馬蹄事前分布などの絶対連続なスパース誘導事前分布を用い，事後分布は Gibbs サンプラーなどの勾配情報を用いないサンプラーで行う (Griffin and Brown, 2021)，というアプローチもあり得る．\n\n\n\n\nしかし (3) のようなアトムを持った分布に直接適用できる Sticky PDMP によるアプローチは，２の方法と違って非可逆なモデル間ジャンプを達成する効率的なサンプラーである．\nさらにその上，Reversible-Jump MCMC の拡張と見れる通り，特定の部分空間にトラップされていた総時間を計算することで，ベイズ因子を計算せずに事後包含確率 (PIP: Posterior Inclusion Probability) を，１の方法と違って誤差なく計算できるという美点がある．\n\n\n1.3 Sticky PDMP の設定\nパラメータ \\(x\\in\\mathbb{R}^d\\) 上に spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) \\[\np(dx)=\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{1}\\] を導入して，ベイズ変数選択を行うとしよう．\nこのとき，モデルの対数尤度を \\(\\ell(x):=\\log p(y|x)\\) とすると，事後分布は \\[\np(x|y)\\,dx\\,\\propto\\,p(y|x)p(dx)=e^{\\ell(x)}\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{2}\\] と表せる．\nそこでこの設定を少し抽象化して，ポテンシャル \\(\\Psi:\\mathbb{R}^d\\to\\mathbb{R}\\) を通じて \\[\n\\mu(dx)=Ce^{-\\Psi(x)}\\prod_{i=1}^d\\left(dx_i+\\frac{1}{\\kappa_i}\\delta_0(dx_i)\\right)\n\\tag{3}\\] と表せる分布 \\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) からサンプリングする問題を考える．\\(\\kappa_i\\) が小さいほど \\(0\\) に大きな重みがかかる．\n事後分布 (2) は \\[\n\\kappa_i=\\frac{\\omega_i}{1-\\omega_i}p_i(0)(&gt;0)\n\\] と取った場合にあたる．\nこの \\(\\mu\\) (3) は Lebesgue 測度に関して絶対連続でないため密度を持たず，通常の勾配を用いた MCMC 法を直接は適用できない．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の理論",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の理論",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "2 Sticky PDMP の理論",
    "text": "2 Sticky PDMP の理論\n\n2.1 アルゴリズム\n\n\n\n\n\n\nThe Sticky Zig-Zag Sampler (Bierkens et al., 2023)\n\n\n\n\n基本的には \\[\n\\mathbb{R}^d\\times\\{\\pm1\\}^d\n\\]\n上を動く Zig-Zag サンプラーである．\n任意の座標成分が \\(0\\) になったとき，すなわち\n\\[\n\\left\\{(x,v)\\in\\mathbb{R}^d\\times\\{\\pm 1\\}^d\\mid\\exists i\\in[d]\\;x_i=0\\right\\}\n\\] を通過したとき，該当する座標成分 \\(x_i\\) は \\(0\\) に固定される．\n固定された座標 \\(x_i\\) は，強度 \\(\\kappa_i\\lvert v_i\\rvert\\) を持つ Poisson 過程が到着するまでそのままである．\n\n\n\nこれは一見 Markov 過程にならないが，状態空間を拡張して \\[\nE_0:=\\mathbb{R}_{0}^d\\times\\{\\pm1\\}^d,\\qquad\\mathbb{R}_{0}:=(-\\infty,0^-]\\sqcup[0^+,\\infty)\n\\] とし，この上の領域 \\[\n\\partial E_i:=\\left\\{(x,v)\\in E_0\\mid (x_i,v_i)=(0^-,-1)\\text{ or }(0^+,1)\\right\\},\\qquad i\\in[d],\n\\] を吸収的だとすると，この \\(E_{0}\\) 上の Markov 過程になる．\n\n\n2.2 設定\n\\(\\partial E_i\\) に入った瞬間にその地点から動かなくなり，\\(v_i\\) は必ずしも「速さ」を表さなくなる，と解する．代わりに \\[\n\\alpha(x,v):=\\left\\{i\\in[d]\\mid(x,v)\\notin \\partial E_i\\right\\}\n\\] を「アクティブな座標番号」の全体とする．\nすると例えば Julia では alpha::Vector{Bool} として，alpha .* v が真の速度ベクトルとなり，簡単に実装できる．これを数式上は \\[\nv_\\alpha:=1_{\\alpha(x,v)}\\cdot v\n\\] で表すこととする．\n続いて \\(T_i:\\partial E_i\\to E_0\\) を，\\(i\\) 番目の座標・速度成分 \\((x_i,v_i)\\) に関して \\[\n(0^+,-1)\\mapsto(0^-,-1),\\qquad(0^-,1)\\mapsto(0^+,1)\n\\] と変換する写像とする．\n\n\n2.3 生成作用素\nZig-Zag サンプラーは各成分ごとに完全に分離したダイナミクスを持つために，生成作用素も \\(\\widehat{A}=\\sum_{i=1}^d\\widehat{A}_i\\) と表せる．各 \\(\\widehat{A}_i\\) は \\[\n\\widehat{A}_if(x,v)=1_{\\partial E_i^\\complement}(x,v)\\left(v_i\\partial_{x_i}f(x,v)+(v_i\\partial_{x_i}\\Phi(x))_+\\biggr(F_i^*f(x,v)-f(x,v)\\biggl)\\right)\n\\] \\[\n+1_{\\partial E_i}(x,v)\\kappa_i\\biggr(T_i^*f(x,v)-f(x,v)\\biggl).\n\\] と表せ，定義域は \\[\n\\mathcal{D}(\\widehat{A})=\\left\\{f\\in L(E_0)\\mid f\\circ\\varphi(-,x,v)\\;\\text{は絶対連続で}\\;\\partial E_i\\;\\text{上にも連続延長する}\\right\\}\n\\] となる．\n\n\n2.4 エルゴード性\nこうして構成された Zig-Zag サンプラーは，分布 \\(\\mu\\) の裾が重すぎない場合，具体的にはある \\(c&gt;d,c'\\in\\mathbb{R}\\) が存在して \\[\n\\Psi(x)&gt;c\\log\\lvert x\\rvert-c',\\qquad x\\in\\mathbb{R}^d,\n\\] を満たすとき（全変動ノルムに関して）エルゴード的である (Prop. A.9 Bierkens et al., 2023, p. 20)．\n\n\n\n\n\n\n(\\(0\\) への再起時刻 Bierkens et al., 2023, p. 6)\n\n\n\n\\(\\mathbb{R}^d\\) の原点への再起時刻 \\(T_0\\) の期待値は \\[\n\\operatorname{E}[T_0]=\\frac{1-\\mu(\\{0\\})}{d\\kappa\\mu(\\{0\\})}.\n\\]\n\n\n\n\n2.5 極限\nSticky Zig-Zag サンプラーは，spike-and-slab 事前分布 (1) を，十分小さい標準偏差 \\(c_i&gt;0\\) を持つ正規分布で近似した \\[\n\\widetilde{p}(dx)=\\sum_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_1+(1-\\omega_i)\\mathrm{N}(0,c_i^2)\\biggl)\n\\] に対する Zig-Zag サンプラーの，\\(c_i\\to0\\) の極限の場合と見れる (Chevallier et al., 2023, p. 2917)．\n\n\n\n(Chevallier et al., 2023, p. 2918)\n\n\n\n\n2.6 一般化\n一般に２つの空間 \\(E_i,E_j\\) の間のジャンプをデザインするとする．\n任意に部分集合 \\(\\Gamma_{i,j}\\subset E_i\\) を定めて，ここを通過する度に確率 \\(p_{i,j}&gt;0\\) でジャンプするとすると，ここから戻る Poisson レート \\(\\beta_{i,j}:E_j\\to\\mathbb{R}_+\\) は \\[\n\\beta_{i,j}(z):=p_{i,j}\\frac{}{}\n\\]"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の実装",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#sticky-pdmp-の実装",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "3 Sticky PDMP の実装",
    "text": "3 Sticky PDMP の実装\nSticky Zig-Zag サンプラーは筆者の開発したパッケージ PDMPFlux.jl に実装されている．詳細は次も参照：\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nPDMP\n\n\n\n\n2024-12-31\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSticky.html#実装",
    "href": "posts/2024/TransDimensionalModels/BayesSticky.html#実装",
    "title": "Sticky PDMP によるベイズ変数選択",
    "section": "4 実装",
    "text": "4 実装\n\n\nStickyZigZag.jl として実装したが，任意の PDMP サンプラーを受け取って Sticky 化する，というようなコードは書けないか？"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#introduction",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#introduction",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n1.1 The 2PL Model\nSuppose we have a binary response variable \\(Y_{i,j}\\) for the \\(i\\in[N]\\)-th judge and the \\(j\\in[J]\\)-th case.\n2-parameter logistic model (2PL) can be written as follows: \\[\nY_{i,j}\\sim\\operatorname{Bernoulli}(\\mu_{i,j}),\n\\tag{1}\\] \\[\n\\operatorname{logit}(\\mu_{i,j})=\\alpha_j+\\beta_j X_i=:\\beta_j\\biggr(\\widetilde{\\alpha}_j+X_i\\biggl).\n\\tag{2}\\]\nAlthough the model is called the 2PL model, we have three parameters \\(\\alpha_j,\\beta_j,X_i\\) in total, two for the cases \\(j\\in[J]\\) and one for the judges \\(i\\in[N]\\).\nIn IRT (Item Response Theory) vocabulary, we call \\(\\alpha_j\\) the difficulty, and \\(\\beta_j\\) the discrimination parameter of the \\(j\\)-th ‘item’.\n\\(X_i\\) may be called the latent trait or ability parameter of the \\(i\\)-th ‘unit’ in that context, but here we call it the ideal point of the \\(i\\)-th judge.\n\n\n1.2 The Problem of Identifiability\nAs (Section 2 Bafumi ほか, 2005) nicely categorized, the above model has three sources of non-identifiability:\n\n\n\n\n\n\nSources of non-identifiability\n\n\n\n\nAdditive aliasing / base point indeterminancy\nFor any \\(c\\in\\mathbb{R}\\), the transformation \\[\n(\\beta_j,\\widetilde{\\alpha}_j,X_i)\\mapsto(\\beta_j-c,\\widetilde{\\alpha}_j+c,X_i+c),\\qquad c\\in\\mathbb{R},\n\\] does not change the likelihood.\nMultiplicative aliasing / scaling indeterminancy\nSame applies to the following transformation: \\[\n(\\beta_j,\\widetilde{\\alpha}_j,X_i)\\mapsto(c^{-1}\\beta_j,c\\widetilde{\\alpha}_j,cX_i),\\qquad c&gt;0.\n\\]\nReflection invariance / sign indeterminacy\n\\[\n(\\beta_j,\\widetilde{\\alpha}_j,X_i)\\mapsto(-\\beta_j,-\\widetilde{\\alpha}_j,-X_i)\n\\]\n\n\n\nAlthough all of the three problems may be settled by setting informative prior distributions to one of the \\(\\alpha_j,\\beta_j,X_i\\)’s, e.g., \\(X_i\\sim N(0,1)\\), the authors (Bafumi ほか, 2005) propose a different approach to the thrid problem, reflection invariance.\n\n\n1.3 Resolution by Hierarchical Structure\n(Bafumi ほか, 2005) proceed to introduce a person(/judge)-level predictor \\(Z_i\\) to the model, i.e., \\[\nX_i\\sim\\mathrm{N}(\\delta+\\gamma Z_i,\\sigma^2)\n\\] to indirectly inform the model of the correct sign of the ideal points.\nSpecifically, \\(Z_i\\in\\{\\pm1\\}\\) corresponds to the party of the nominating president of \\(i\\)-th judge; \\(Z_i=+1\\) corresponds to the Republican party, and \\(Z_i=-1\\) corresponds to the Democratic party.\nIn this way, (Bafumi ほか, 2005) tried to guide the model & likelihood to have only one mode, where liberal judges would be on the left and conservative judges would be on the right on the \\(X_i\\in\\mathbb{R}\\) axis.\n\n\n1.4 The Problem Remains …\nLet us consider the data from the 1994-2004 terms of the U.S. Supreme Court here, although (Bafumi ほか, 2005) used the data from the 1954-2000 terms. The data is available as Rehnquist via the MCMCpack package (Martin ほか, 2011) in R.\n\nlibrary(MCMCpack)\ndata(Rehnquist)\nkable(head(Rehnquist))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRehnquist\nStevens\nO.Connor\nScalia\nKennedy\nSouter\nThomas\nGinsburg\nBreyer\nterm\ntime\n\n\n\n\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\nNA\n0\n1994\n1\n\n\n0\n1\n0\n1\n1\n1\n0\n1\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1994\n1\n\n\n\n\n\nLet us first see the ‘correct’ output from the model. The precise meaning of ‘correct’ will be clarified later in the next セクション 2, along with the Stan codes.\n\n\n\nA ‘correct’ output from bafumi_normal.stan\n\n\nWe see four judges classified as liberal, namely Stevens, Souter, Ginsburg & Breyer. The last two judges were nominated by Bill Clinton, a Democrat, while the first two were nominated by Republican presidents.\nThis result aligns with the common understanding of the Supreme Court justices. For instance, we quote a sentence from the wikipedia page of John Paul Stevens:\n\nDespite being a registered Republican who throughout his life identified as a conservative, Stevens was considered to have been on the liberal side of the Court at the time of his retirement.\n\nThe similar situation applies to David Souter.\nHere we notice that two liberal judges have \\(Z_i=+1\\), while the other two have \\(Z_i=-1\\). The predictive ability of the covariate \\(Z_i\\) is (presumably) weak during the 1994-2004 term.\nTherefore, the information from \\(Z_i\\) about the sign of the ideal points may not be strong enough to resolve the identifiability problem. In that case, the posterior distribution would be bimodal. Indeed, this is the case, as we will see next."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#sec-bafumi-stan",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#sec-bafumi-stan",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "2 Estimation by Stan",
    "text": "2 Estimation by Stan\nThe 2PL model (1), (2) can be written in Stan as follows:\n\n\nbafumi_normal.stan\n\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  vector[N] Z;  // covariates for judges\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points for judges\n  vector[J] alpha;\n  vector[J] beta;\n\n  real delta;\n  real gamma;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(delta);\n  lprior += std_normal_lpdf(gamma);\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  X ~ normal(delta + Z * exp(gamma), 1);\n\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\nUsing this Stan code, we run the following experiment, where we run 4 chains in parallel, each with 4000 iterations, 3000 of which are used for warmup. The chains are initialized randomly.\n\n\nFiles/experiment.r\n\nfor (i in 1:100) {\n  fit &lt;- stan(\"bafumi_normal.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n\n  all_samples &lt;- extract(fit, pars = \"X\")$X\n  last_1000_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n  mean &lt;- apply(last_1000_samples, 2, mean)\n  if (mean[9] &gt; 0.5) {\n    count &lt;- count + 1\n  }\n}\nprint(count)\n\n\n\n[1] 52\n\n\nThe following plots are some (first 9) of the results from the experiment.\n\n\n\n\n\n\n\n\n\nElapsed time: 16.99 seconds\n\n\n\n\n\n\n\nElapsed time: 17.01 seconds\n\n\n\n\n\n\n\nElapsed time: 16.30 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 15.00 seconds\n\n\n\n\n\n\n\nElapsed time: 14.98 seconds\n\n\n\n\n\n\n\nElapsed time: 15.06 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.42 seconds\n\n\n\n\n\n\n\nElapsed time: 16.57 seconds\n\n\n\n\n\n\n\nElapsed time: 17.18 seconds\n\n\n\n\n\nWe see that the result has two patterns and they are in symmetry with each other.\nThis phenomenon proves that the posterior distribution of the ideal points is bimodal, indicating the weak identifiability of the model."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Bafumi.html#conclusion",
    "href": "posts/2024/TransDimensionalModels/Bafumi.html#conclusion",
    "title": "On the Identifiability of the Bafumi et. al. Ideal Point Model",
    "section": "3 Conclusion",
    "text": "3 Conclusion\n(Bafumi ほか, 2005)’s hierarchical resolution of the identifiability problem by the covariate \\(Z_i\\) will fail if the covariate \\(Z_i\\) is not informative enough.\nIn that case, the posterior distribution of the ideal points will be bimodal."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html",
    "title": "階層ベイズ理想点解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html#前稿",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html#前稿",
    "title": "階層ベイズ理想点解析",
    "section": "1 前稿",
    "text": "1 前稿\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html#モデル",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html#モデル",
    "title": "階層ベイズ理想点解析",
    "section": "2 モデル",
    "text": "2 モデル\n\n2.1 構造\n議会での点呼投票データ \\(\\{Y_{i,j}\\}_{i\\in[N],j\\in[J]}\\) を考える．２母数ロジットモデル \\[\n\\operatorname{P}[Y_{i,j}=1]=\\Phi(\\alpha_j+\\beta_jX_i)\n\\tag{1}\\] によって各議員 \\(i\\in[N]\\) の理想点 \\(X_i\\in\\mathbb{R}\\) を推定することを考える．さらにここに階層モデル \\[\nX_i=Z_i^\\top\\gamma_{g(i)}+\\epsilon_i,\\qquad\\epsilon_i\\sim\\mathrm{N}(0,\\sigma^2),\n\\tag{2}\\] を考える．ただし \\(Z_i\\in\\mathbb{R}^p\\) は議員ごとの共変量， \\[\ng:[N]\\to[G]\n\\] は議員の項目応答特性のクラスタリングとする．\n\n\n2.2 事前分布\n第一階層 (1) の２母数ロジットモデルには \\[\n\\alpha_j\\sim\\mathrm{N}(0,\\sigma_\\alpha^2),\\quad\\beta_j\\sim\\mathrm{N}(0,\\sigma_\\beta^2)\n\\] という正規事前分布を仮定する．\n第二階層 (2) の共変量にはスパース性を促進する spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定する： \\[\np(d\\gamma_{g(i)})=\\omega_{g(i)}\\delta_0(d\\gamma_{g(i)})+(1-\\omega_{g(i)})p_0(d\\gamma_{g(i)}).\n\\] \\[\n\\omega_{g(i)}\\sim\\operatorname{Beta}(a,b).\n\\] ただし \\(p_0\\) は多様な理想点を促進するために t-分布とする．\n\\(\\sigma\\) には half-Cauchy 事前分布を仮定する： \\[\n\\sigma\\sim\\text{Half-Cauchy}(0,1).\n\\]\n最後にグループ所属 \\(g\\) には，最大クラスタ数 \\(G_\\max=10\\) を仮定し， \\[\n\\operatorname{P}[g(i)=g]=\\frac{1}{G},\\qquad G\\sim U([G_\\max])\n\\] とする．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint3.html#計算",
    "href": "posts/2024/TransDimensionalModels/IdealPoint3.html#計算",
    "title": "階層ベイズ理想点解析",
    "section": "3 計算",
    "text": "3 計算\nモデル (1), (2) の尤度 \\[\np(y_{ij}|z_i,\\alpha_j,\\beta_j,\\gamma_g,g,\\sigma)\n\\] は \\(g\\) に関してのみ微分可能でない．\nそこで基本的には \\(\\gamma\\in\\mathbb{R}^p\\) と \\(\\alpha_j,\\beta_j\\in\\mathbb{R},\\sigma\\in\\mathbb{R}_+\\) のサンプリングには Sticky PDMP サンプラー (Chevallier et al., 2023), (Bierkens et al., 2023) を用い，\\(g\\in [G]^{[N]}\\) のサンプリングには次の２つの時計を追加して行う： \\[\n\\Lambda^S(t):=\\Lambda_0^S\\left(1\\land\\frac{p(g')}{p(g)}\\right),\n\\] \\[\n\\Lambda^K(t)=\\Lambda_0^K.\n\\]\n\\(\\Lambda^S\\) により候補 \\(g'\\in [G]^{[N]}\\) への遷移を行い，\\(\\Lambda^K\\) により候補 \\(g'\\in [G]^{[N]}\\) の更新をある確率核 \\(q_g(g,-)\\) に従って行う．\n広大な離散空間 \\([G]^{[N]}\\) 上を Poisson 跳躍により歩き回る．これは Zig-Zag within Gibbs (Sachs et al., 2023), (Hardcastle et al., 2024) の考え方である．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html",
    "title": "ベイズ変数選択",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#はじめに",
    "title": "ベイズ変数選択",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 （復習）ベイズデータ解析の第一歩\nデータの非線型変換も取り入れたベイズ線型重回帰分析は，多くの場合，データを理解するための最初の解析手法として選択される．\nその方法を brms パッケージを用いて実践したのが次の記事である：\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n前稿では BMI を LAB と LDL から予測する問題を，線型回帰モデルから始めた．\n交差項を追加することで，LDL が違う群に対して LAB がどう変わるかの層別の違いを見ることができる．\n事後予測分布によるモデルのチェックは残差プロットと同様に，極めて手軽かつ有力なモデル検証の方法である．\nこれにより関数関係の非線型性が疑われたため，被説明変数 BMI に対して対数変換を施して線型回帰をすると，予測性能の改善が見られた．\n事後予測分布のプロットだけでなく，その「よさ」の定量的な指標として交差検証による事後予測スコア elpd (Vehtari et al., 2017) があることを学んだ．\n\n\n1.2 ベイズから見た変数選択\nこうして予測力を基にモデル選択をする方法は得たわけであるが，純粋にベイズ的な観点から変数選択を行う方法が大きく分けて２つある．\n\n1.2.1 縮小事前分布による方法\n１つ目が「モデルに含まれる変数は少ないはずである」という信念を表現した事前分布を用いる方法である（第 2 節）．\nこれは馬蹄事前分布 (Carvalho et al., 2009), (Carvalho et al., 2010)，Laplace 事前分布 / Bayesian Lasso (Park and Casella, 2008) などの global-local shrinkage prior を用いる方法である．\nこの方法は点推定や頻度論的な方法ではほとんど唯一の変数選択の方法であり，正則化またはスパース性 のキーワードの下で盛んに研究されている (Hastie et al., 2015)．\n\n\n1.2.2 ベイズ変数選択\n２つ目が spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) という \\(0\\) にマスを持つ事前分布を用いる方法である： \\[\np(dx)=\\prod_{i=1}^d\\biggr(\\omega_i\\phi_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{1}\\]\nこの方法では当該変数の 事後包含確率 (PIP: Posterior Inclusion Probability) を導出することができる．実際 (1) は混合分布の形をしており，spike \\(\\delta_0\\) と slab \\(\\phi_i\\) のどちらからサンプリングされるかを表す潜在変数 \\(\\gamma_i\\in\\{0,1\\}\\) を導入すれば，\\(\\operatorname{P}[\\gamma=0|\\mathcal{D}]\\) という事後確率こそが変数 \\(x_i\\) がモデルに入る事後確率である．\nPIP を用いることで「当該変数がモデルに含まれるか？」という問題に直接ベイズ的に答えることができる．これを ベイズ変数選択 という 3．\n一方で前述の global-local shrinkage prior でも，post-processing を通じて同様に PIP を近似的に算出することができる (Hahn and Carvalho, 2015)．\n\n\n\n1.3 ベイズモデル平均を見据えて\nこのようにベイズ変数選択 1.2.2 では，変数選択も統計的推論の問題として解く．\nこの方法は最適なレートで縮小する効果を持ち (Castillo et al., 2015)，また予測力にも優れる (Porwal and Raftery, 2022)．\n最終的には，適切に構造と事前分布が設定されたベイズモデルを用いて，ベイズ推論により変数の関連度を自動で判断して結果を出すことが理想である．その意味では全ての変数を（適切に）入れたモデルを用いることが好ましい．1\nベイズ変数選択はこの最終目標に向かうまでの探索的な中途解析と見ることもできる．\n実際，ベイズ変数選択により得た事後包含確率 PIP は，ベイズモデル平均 (BMA: Bayesian Model Averaging) (Hoeting et al., 1999) に用いることができる．\n変数選択・モデル選択を実行し，選ばれた単一のモデルで推論・予測を実行するよりも，尤度が必ずしも最も高いわけではないモデルも捨てずに推論に用いることで精度を上げることができる．\nこれがベイズモデル平均の考え方であり，ベイズの美点をフルに発揮する枠組みであると言える．実際，(Porwal and Raftery, 2022) では線型回帰モデルの変数選択において，３つの適応的 BMA 手法が全てのタスクでベストな予測性能を示したことを報告している．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-regularization",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-regularization",
    "title": "ベイズ変数選択",
    "section": "2 縮小事前分布による方法",
    "text": "2 縮小事前分布による方法\n\n2.1 多くの説明変数が存在する場合の事前分布\nstan_glm では回帰係数には適切な分散を持った独立な正規分布（\\(g\\)-prior）をデフォルトの事前分布としている．\nbrms では一様事前分布である．\n仮に説明変数が極めて多い場合，このデフォルト事前分布を採用し続けることは適切ではない．\n実際，独立な正規・一様分布に従う説明変数が大量にある場合，これは「ベイズ（事後平均）推定量の分散が大きい」という事前分布を採用していることに含意してしまう．\n仮に \\(\\sigma\\) にも同様の分散の大きい事前分布をおいているのならば辻褄は合うが，そうでないならばベイズ決定係数 \\(R^2\\) にほとんど \\(1\\) 近くの事前分布をおいていることに等価である．\nすなわち過学習されたモデルに強い事前分布をおいていることになる (Gelman et al., 2020, p. 208)．これは我々の信念と食い違うだろう．そもそも弱情報であるべきデフォルト事前分布としては相応しくない．\n\n\n2.2 縮小事前分布\nまずは各変数の正規事前分布の分散を十分小さくして，誤差 \\(\\epsilon\\) の分散 \\(\\sigma^2\\) のスケールと同一にすることが考えられる．\nこの際 \\(R^2\\) にはほとんど無情報な事前分布が仮定されるのと同一である．\nさらに，仮に「多くの説明変数のうち，一部しか重要なものはなく，他の大部分はほとんど無関係である」と思っている，あるいは思いたいとする．変数選択を行いたい場合がこれにあたる．\nこの信念を正確に表現する事前分布の一つに馬蹄事前分布 (horseshoe prior) (Carvalho et al., 2009), (Carvalho et al., 2010) とその正則化バージョン (Piironen and Vehtari, 2017b) がある．\nこれらの分布は global-local shrinkage prior と呼ばれ，\\(R^2\\) 上の事前分布に，\\(0\\) 上にスパイクを生じさせる．モデルに支持されない説明変数の係数を \\(0\\) に向かって縮小する効果があり，結果としてシンプルなモデルを選好することになる．\nStan においては prior=hs によって指定できる (Gelman et al., 2020, p. 209)．\n\n\n2.3 Local Scale Mixture\n多くの正則化事前分布は次のような正規分布の local scale mixture (West, 1987) の形をしている：2 \\[\n\\pi(\\beta_j|\\lambda)=\\int_\\mathbb{R}\\phi(\\beta_j|0,\\lambda^2\\lambda^2_j)\\pi(\\lambda_j^2)\\,d\\lambda_j.\n\\]\n\\(\\pi\\) が２点のみに台を持つ場合が spike-and-slab (1) であった (Polson and Scott, 2011)．SSVS （第 3.2 節）も含む．\\(\\pi\\) が絶対連続である場合も次のような例を持つ (Polson and Scott, 2012)：\n\n\n\n\n\n\n\n二重指数分布 (double exponential) / Bayesian Lasso (Park and Casella, 2008), (Hans, 2009)\n馬蹄事前分布 (horseshoe prior) (Carvalho et al., 2009), (Carvalho et al., 2010)\nBayesian elastic net (Hans, 2011)\n\n\n\n\n(Ishwaran and Rao, 2005) はこれらの研究より早い段階で，\\(\\pi\\) に \\(0\\) の近くと \\(0\\) から大きく離れた二峰を持つ分布を用意している．\n\n\n2.4 ベイズ縮小の効果\nスケールパラメータ \\(\\lambda\\) は，LASSO (Tibshirani, 1996) では CV などの基準により選択することになるが，\\(\\lambda\\) を推定してモデル平均を行うことでより高い推定精度を得ることができる (Hans, 2009)．\n同様にして事後平均推定量により推定精度は改善されるが，ほとんど確実にこれはスパースではない．従って推定量のスパース性と推定精度はトレードオフの関係にあり，完全にベイジアンに Bayesian LASSO を実行すると本末転倒に陥るという一面もある．\nこのためベイズ縮小事前分布を用いた場合，自動的にスパース性に基づいたモデル選択ができるというわけではなく，事後モデル確率 (posterior model probability) を最大にするものを見つけるという post-processing が必要になる (Section 1.5 Hahn and Carvalho, 2015, p. 438), (Piironen et al., 2020), (Jim E. Griffin, 2024)．\nしかし以上のベイズモデル選択の手続きを踏むことによって，事後モデル確率を最大にするものという統計的・決定理論的に根拠を持ったモデル選択を実行することができる．\nLASSO はベイズモデル選択の結果よりも予測性能が必ずしも高いわけではないにも拘らず，より多くの変数をモデルに残しがちであることも報告されている (Porwal and Raftery, 2022, p. 3)．これは馬蹄事前分布などの最新の縮小事前分布は，回帰係数の効果量やモデルの大きさなどに応じて適応的に正則化の強さを加減しているためだとも言える (Li et al., 2023)．\n\n\n2.5 馬蹄事前分布\n馬蹄事前分布 \\[\n\\beta_j|\\lambda_j,\\tau\\sim\\mathrm{N}(0,\\lambda_j^2\\tau^2),\\qquad\\lambda_j\\sim\\operatorname{half-Cauchy}(0,1),\n\\] は global-local shrinkage prior の１つである．\nというのも，hyperparameter \\(\\tau\\) で決まる大域的な縮小効果がある一方で，半 Cauchy 分布による混合の構造が局所的なスケールパラメータ \\(\\lambda_j\\) を調整し，縮小される変数とされない変数とにコントラストをつけてくれる．\nつまり，馬蹄事前分布は「少数の変数のみが大きなスケールを持つ」という信念を，階層的な構造で表現したものと理解できる．\n最後の問題はハイパーパラメータ \\(\\tau\\) の調整である．\n交差検証法や周辺尤度の値，情報量規準により \\(\\tau\\) の値を選んで推定を実行することもできる（経験ベイズ）．(Polson and Scott, 2011) では \\[\n\\tau|\\sigma\\sim\\operatorname{half-Cauchy}(0,\\sigma^2)\n\\] という hyperprior を推奨している．ただし \\(\\sigma^2\\) は誤差の分散と共通とする．一方で (Piironen and Vehtari, 2017a) は事前の信念から非零の係数を持つべき変数の数 \\(p_0\\) に対して \\[\n\\tau|\\sigma\\sim\\operatorname{half-Cauchy}(0,\\tau^2_0),\\qquad\\tau_0:=\\frac{p_0}{p-p_0}\\frac{\\sigma^2}{\\sqrt{n}},\n\\] により hyperprior を置くことを推奨している．\n\n\n2.6 正則化\n馬蹄事前分布はモデルに支持される変数の係数はほとんど縮小させないように設計されている．これは美点である一方で，正則化の効果を弱めてしまい，縮小事前分布であるはずが推定の安定化が望めない場合がある．\n例えばロジスティック回帰に馬蹄事前分布をおくと，分離 などが起こって識別性が弱い場合に Cauchy 分布と同様の裾を持つために事後平均推定量が存在しなくなってしまう (Ghosh et al., 2018)．\nまた従来の馬蹄事前分布では，ときに事後分布が漏斗型を持ってしまい，収束が劇的に遅くなるという現象も観測されていた (Piironen and Vehtari, 2015)．\nこれらの問題を解決するために spike-and-slab 事前分布の slab width と同様の正則化ハイパーパラメータ \\(c&gt;0\\) を導入した 正則化馬蹄事前分布 (regularized horseshoe prior) (Piironen and Vehtari, 2017b) が提案されている： \\[\n\\beta_j|\\lambda_j,\\tau,c\\sim\\mathrm{N}(0,\\tau^2\\widetilde{\\lambda}_j^2),\\qquad\\widetilde{\\lambda}_j:=\\frac{c^2\\lambda_j^2}{c^2+\\tau^2\\lambda_j^2},\\lambda_j\\sim\\operatorname{half-Cauchy}(0,1).\n\\] \\(c\\to\\infty\\) の極限では馬蹄事前分布に一致する．この \\(c\\) には次の hyperprior を推奨している： \\[\nc^2\\sim\\operatorname{Inv-Gamma}(\\nu/2,\\nu s^2/2).\n\\] これにより最大の係数に対して \\(t_\\nu(0,s^2)\\) の事前分布を置くことに等価になる ((2.11) Piironen and Vehtari, 2017b, p. 5025)．\n\n\n2.7 rstanarm での利用\nrstanarm では hs (hierarchical shrinkage) 事前分布\nhs(df = 1, global_df = 1, global_scale = 0.01, slab_df = 4, slab_scale = 2.5)\nが利用可能である．これは \\[\n\\tau|\\sigma\\sim\\operatorname{half-t}(\\textcolor{purple}{\\mathtt{global_df}}),\\qquad\\lambda_j\\sim\\operatorname{half-t}(\\textcolor{purple}{\\mathtt{df}})\n\\] というものであるから，df=1, global_df=1 が正則化馬蹄事前分布に対応する．\nglobal_scale が \\(\\tau_0\\) に，slab_scale が \\(s\\)，slab_df が \\(\\nu\\) に対応する．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-variable-selection",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#sec-Bayesian-variable-selection",
    "title": "ベイズ変数選択",
    "section": "3 ベイズ変数選択",
    "text": "3 ベイズ変数選択\n\n3.1 はじめに：階層モデリング\nベイズ変数選択 では，説明変数 \\(\\{x_i\\}_{i=1}^p\\) のそれぞれがモデルに含まれるかを意味する潜在変数 \\(\\{\\gamma_i\\}_{i=1}^p\\in\\{0,1\\}^p=:\\Gamma\\) の事後分布を算出して，特定の変数がモデルに含まれる確率を算出する．\n最終的にこの確率分布は，ベイズモデル平均 (BMA: Bayesian Model Averaging) と言って，それぞれのモデルの事後予測を平均するためのプライヤーとして用いることもできる．\nこの方法では \\[\np(\\gamma)=\\prod_{i=1}^p\\omega_i^{\\gamma_i}(1-\\omega_i)^{1-\\gamma_i},\\qquad p(\\sigma^2)\\,\\propto\\,\\sigma^{-2}\n\\] \\[\np(\\beta|\\sigma,\\gamma)\\,d\\beta=\\mathrm{N}_p(0,\\Sigma(\\sigma,\\gamma))\n\\] という階層構造を通じて，回帰モデルに潜在変数 \\(\\gamma\\) を導入する．\\(\\Sigma\\) は (X. Liang et al., 2022) では独立，(George and McCulloch, 1997) では \\(g\\)-prior とする： \\[\n\\Sigma(\\sigma,\\gamma):=g\\sigma^2(X^\\top_\\gamma X_\\gamma)^{-1}.\n\\] \\(g\\) は global scale parameter と呼ばれ，これにさらに hyperprior を設定することもある (F. Liang et al., 2008), (Ley and Steel, 2009)．\nこのアプローチは (George and McCulloch, 1997) らによって創始された．（仮に \\(p\\) が比例的に増えるとしても） \\(n\\to\\infty\\) の極限で PIP は正しいモデル上の Delta 測度に収束する (Shang and Clayton, 2011)．\n\n\n3.2 確率的探索法\n特に (George and McCulloch, 1993) では \\[\n\\beta_i|\\gamma_i\\sim(1-\\gamma_i)\\mathrm{N}(0,\\sigma_i^2)+\\gamma_i\\mathrm{N}(0,c_i^2\\sigma_i^2)\n\\tag{2}\\] という構造を設定し，データ拡張に基づく Gibbs サンプラーによって推定することを提案した．\nこの方法は 確率的探索法 (SSVS: Stochastic Search Variable Selection) と呼ばれる．3\nしかし (2) は spike-and-slab (1) の近似になっているため，\\(\\gamma_i=1\\) の事後確率は正確に PIP になっているわけではない．\nこの近似は Gibbs サンプラーを高速にするという利点はあったかもしれないが，現代では spike-and-slab (1) に直接適用できる高速なサンプラーが多数開発されている．\n\n\n3.3 Add-Delete-Swap による探索\n計量化学 (chemometrics) では \\(p\\) が特に高次元になり得る．(Brown et al., 1998) は近赤外線分光法で得られたデータから，予測に有用な波長を選択する問題に対処するためにベイズ変数選択の方法を用いることを考えた．\nそのためにまず第 3.1 節の階層モデルを多次元化し，推定には乱歩 MH 法を用いた．\n(Brown et al., 1998) の乱歩 MH 法の提案核は，Add-Delete-Swap の動きをするものであった：\n\n\n\n\n\n\n次の２つの動きを，それぞれ確率 \\(\\phi,1-\\phi\\) で行う；\n\nAdding or Deleting\n新たな変数 \\(x_i\\) をランダムに選び，まだモデルに入っていない場合は入れ，すでにモデルに含まれている場合は取り除く．\nSwapping\nモデルに含まれていない変数 \\(x_i\\) と含まれている変数 \\(x_j\\) をそれぞれランダムに選び，入れ替える．\n\n\n\n\n(Yang et al., 2016) は MH 法の計算複雑性を解析し，\\(p\\) が大きい場合にも計算量が線型にしか増加しない乱歩 MH 法を提案した．\n\n\n3.4 超次元 MCMC による PIP 算出\n事後包含確率を出すにあたって，Reversible-Jump MCMC (Green, 1995) などの超次元手法を用いることも考えられる．\n超次元 MCMC とは，一般に複数のモデルから同時にサンプリングするための用いられ，ベイズ変数選択法は分解可能なグラフィカルモデルに対する超次元 MCMC 法の特別な場合と見れる (Godsill, 2001, p. 232)．\n詳しくは別稿で取り上げるが，特に Sticky PDMP (Bierkens et al., 2023) は有力な PIP 算出法になる．\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.5 計算の問題\nベイズ変数選択とはモデル空間 \\(\\Gamma=\\{0,1\\}^p\\) 上の事後分布を計算することであるが，これを効率的に行う MCMC を構成することが中心的な問題になる．\nここまで叙述してきた Gibbs サンプラー（例えば確率的探索法 3.2 など）は，\\(p=2\\) の極めて簡単な設定で簡単に崩壊する．というのも，２つの同等な説明力を持つ確率変数が強い相関を持つ場合，\\((\\beta,\\gamma)\\) の同時分布は強い二峰性を持つ．\nその結果ただナイーブに Gibbs サンプラーを適用しただけでは片方の峰しか見つけることができず，PIP について偏った結果を出してしまう (Section 5.1 Zanella and Roberts, 2019)．\n推定したいモデルが階層モデルである限り，多峰性の問題は常に付きものである．\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\\(p=2\\) の時でさえ深刻になり得る多峰性の問題に加えて，共変量の数 \\(p\\) が大きい現代的な問題に対処する必要もあることを思えば，ベイズ変数選択の問題は，多峰性に強い効率的なベイズ計算法を開発するという普遍的な課題に回収されるのである．\n\n\n3.6 Tempered Gibbs サンプラー\n目標分布の条件付き分布 \\(f(x_i|x_{-i})\\) が多峰性を持つ場合，何らかの軟化 \\(g(x_i|x_{-i})\\) を考えることがあり得る．これに対して \\[\np_i(x):=\\frac{g(x_i|x_{-i})}{f(x_i|x_{-i})}\n\\] と定め，まず \\((p_1(x),\\cdots,p_p(x))\\) に従って \\(i\\in[p]\\) を選び，続いて \\(x_i\\sim g(x_i|x_{-i})\\) をサンプリングする random scan Gibbs サンプラーを考えると，これはやはり \\(f\\) を不変分布にもつ．\nこの方法では \\(p_i(x)\\) で各説明変数 \\(x_i\\) に傾斜をつけているために，特に PIP の高い \\(i\\in[p]\\) から優先的にサンプリングすることができる．この方法は後述 3.7 節の informed MCMC の先駆けとなった．\n\n\n\n3.7 Locally Informed MH Samplers\nAdd-Delete-Swap による乱歩 MH 法 3.3 の設計は，見通しの良い素朴な構成であるが，効率的な動きである保証は全くない．さらには \\(\\phi\\) やそれぞれの候補を持ってくる確率など，ユーザーが調整する必要があるハイパーパラメータも多い．\n対称かつ局所的なサンプラーの中では，採択率が高いほど効率が良い (Peskun, 1973), (Tierney, 1998)．そこで離散空間上の乱歩 MH 法の採択率を上げるために，現在位置の周囲の点の情報を収集して次の動きを決めるサンプラーが Informed MCMC の名前の下で開発されている (X. Liang et al., 2022, p. 84)．\n多くの locally informed MCMC (Zanella, 2020) では，通常の乱歩 MH 核 \\(Q\\) を基底核 (base kernel) として，これを近傍での事後分布の様子を要約した 釣り合い関数 (balancing function) \\(g\\) を用いて修正することで効率的な動きを達成する： \\[\nq_g(\\gamma,\\gamma')\\,\\propto\\,g\\left(\\frac{\\pi(\\gamma')}{\\pi(\\gamma)}\\right)q(\\gamma,\\gamma').\n\\]\nこの中でも LIT (Locally Informed and Thresholded proposal) (Zhou et al., 2022) は釣り合い関数として閾値関数 (threshold function) \\[\ng(t)=p^L\\land(p^l\\lor t),\\qquad-\\infty&lt;l&lt;L&lt;\\infty.\n\\] を用い，さらに提案核 \\(Q\\) を単なる一様分布ではなく第 3.3 節で考えられた Add-Delete-Swap 核 (Brown et al., 1998) にとることで，一定の条件の下で 次元 \\(p\\) に依存しない収束速度 を達成することを示した．\n\n\n3.8 適応的なサンプラー\nLIT は固定した基底核 \\(Q\\) を取り，そこから \\(g\\) で修正することを基本戦略としていた．一方でそもそも基底核 \\(Q\\) を適応的に調整していくメカニズムを導入することができる．\nASI (Adaptively Scaled Individual adaptation) (J. E. Griffin et al., 2021) では，(Brown et al., 1998) の Add-Delete-Swap 核 3.3 \\[\nq_\\eta(\\gamma,\\gamma')=\\prod_{j=1}^pq_{\\eta,j}(\\gamma_j,\\gamma_j'),\\qquad \\eta=(A_1,\\cdots,A_p,D_1,\\cdots,D_p)\\in(0,1)^{2p},\n\\] \\[\nq_{\\eta,j}(0,1)=\\eta_j=A_j,\\qquad q_{\\eta,j}(1,0)=\\eta_{p+j}=D_j,\n\\] を元にして，\\(\\eta=(A,D)\\) を適応的に更新していくことを考える．その際の目安は，\\(x_j\\) の PIP \\(\\pi_j\\) から定まる \\[\nA_j^\\mathrm{opt}:=1\\land\\frac{\\pi_j}{1-\\pi_j},\\qquad D_j^\\mathrm{opt}:=1\\land\\frac{1-\\pi_j}{\\pi_j},\n\\] である．これの推定量 \\(\\eta^{(i)}\\) を各段階で構成した上で，総じた採択率を調整する学習率のようなパラメータ \\(\\zeta^{(i)}\\) も導入し，(Robbins and Monro, 1951) の方法で更新していく．\n実は ASI は (X. Liang et al., 2023) がいう 確率近傍サンプラー (random neighbourhood sampler) の例になっている．これは Add-Delete-Swap (Brown et al., 1998) のように提案される近傍が，補助的な離散確率変数 \\(k\\in[K]\\) によって定まるような乱歩 MH 法をいう．\n(X. Liang et al., 2023) では ASI によって構成される確率的近傍の中から，さらに locally informed に次の動きを選ぶことを提案し， 適応的確率近傍 (ARNI: Adaptive Random Neighbourhood Informed) サンプラーと呼んでいる．これにより ASI で上がりきらなかった採択率を押し上げつつ，計算量を抑えつつも良い近傍を提案するメカニズムを取り入れることができる．\nこれにより \\(p\\) に依らない収束だけでなく，計算複雑性も \\(p\\) の次元に対してスケールするものが得られると期待される．\n\n\n3.9 非可逆なサンプラー\nInformed MCMC とは，連続空間上で言えば，MALA などの勾配情報を利用したサンプラーである．このような乱歩 MH 法の修正として得られる効率的なサンプラーを (X. Liang et al., 2022) は 近傍サンプラー (neighbourhood sampler) と呼んでいる．\nMALA などの Langevin 拡散を元にした乱歩 MH 法は革新的であったが，現在最も効率的なサンプラーは，局所的な動きを廃した HMC 法と，非可逆な動きを達成する PDMP (Piecewise Deterministic Markov Process) / ECMC (Event-Chain Monte Carlo) である．\n離散空間上でもこれらのサンプラーに対応するものは高い効率を示すだろうと思われる．\n一般の離散空間上でも局所性の打開には (Nishimura et al., 2020)，可逆性の打開には (Koskela, 2022) などの試みがあるが，殊に変数選択に関しては Sticky PDMP (Bierkens et al., 2023) という画期的な手法が開発されている．\nこれに関しては次稿で詳しく取り上げる：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#文献紹介",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#文献紹介",
    "title": "ベイズ変数選択",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n変数選択のための事前分布とその \\(R^2\\) 上に定める事前分布については (12.7 節 Gelman et al., 2020) で丁寧に議論されている．\nこの第一のアプローチ・縮小事前分布については (Bhadra et al., 2019) のレビューがある．他には (Jim E. Griffin and Brown, 2021), (Jim E. Griffin and Brown, 2017), (Hahn and Carvalho, 2015) が詳しい．\n(George and McCulloch, 1993) による変数選択法が (Chapter 9 Hoff, 2009) で取り上げられている．\nベイズ変数選択手法の概観は (X. Liang et al., 2023) や (Jim E. Griffin, 2024) のイントロに圧倒されるほどまとまっている．(Jim E. Griffin, 2024) ではモデルの空間上に得られた事後分布の情報を効果的に表示するための「信用区間」の構成法を提案している．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesSelection.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/BayesSelection.html#footnotes",
    "title": "ベイズ変数選択",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n例えば (Barr et al., 2013) では検証的仮説検定の設定で，どこまでランダム効果をモデルに入れるかを議論しており，「全部入れるべき」という結論を一定の前提の下で導いている．↩︎\n(Hahn and Carvalho, 2015, p. 436) は local scale mixture と呼んでいる．他にこの観点は (Jim E. Griffin and Brown, 2017), (Polson and Scott, 2012) でも議論されている．↩︎\n変数減少法やステップワイズ法などのヒューリスティックな方法に対しての「確率的探索法」という名称だったのだと思われる．特に (George and McCulloch, 1993) ではデータ拡張に基づく Gibbs サンプラーを用いており，その様子が「確率的探索」に見えるのだと思われる．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans2.html",
    "href": "posts/2024/TransDimensionalModels/Trans2.html",
    "title": "超次元 Zig-Zag サンプラー",
    "section": "",
    "text": "点呼投票データに対しては (Clinton et al., 2004) がベイズ的なアプローチを創始した．\nその際は政策空間の適切な次元 \\(K\\) に対しての（ドメインエキスパートによる）事前知識を自由に取り入れられる点が利点とされた．\nここでは政策空間の適切な事件 \\(K\\) も推論の対象としたベイズモデル選択法を，１度の MCMC サンプリングで実行することを考える．\n\n\n二項選択モデルで，説明変数の次元が大きく，説明変数間の交互作用が強く，また事前分布の裾が重い場合，特に困難な事後分布を定める．\nこのような設定はベイズ計算手法のベンチマークに適している (Chopin and Ridgway, 2017)．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans2.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/Trans2.html#はじめに",
    "title": "超次元 Zig-Zag サンプラー",
    "section": "",
    "text": "点呼投票データに対しては (Clinton et al., 2004) がベイズ的なアプローチを創始した．\nその際は政策空間の適切な次元 \\(K\\) に対しての（ドメインエキスパートによる）事前知識を自由に取り入れられる点が利点とされた．\nここでは政策空間の適切な事件 \\(K\\) も推論の対象としたベイズモデル選択法を，１度の MCMC サンプリングで実行することを考える．\n\n\n二項選択モデルで，説明変数の次元が大きく，説明変数間の交互作用が強く，また事前分布の裾が重い場合，特に困難な事後分布を定める．\nこのような設定はベイズ計算手法のベンチマークに適している (Chopin and Ridgway, 2017)．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html",
    "href": "posts/2024/Samplers/EBM.html",
    "title": "エネルギーベースモデル",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#エネルギーベースモデル",
    "href": "posts/2024/Samplers/EBM.html#エネルギーベースモデル",
    "title": "エネルギーベースモデル",
    "section": "1 エネルギーベースモデル",
    "text": "1 エネルギーベースモデル\n\n1.1 はじめに\nエネルギーベースのモデル (EBM: Energy-based Model) とは，密度が \\[\np(x,z)\\,\\propto\\,e^{-H(x,z)}\n\\] の形で与えられるモデルをいう．1\nこれは統計物理学的な系との対応が意識されているように，入力 \\(z\\) と出力 \\(x\\) の整合性 \\(k(x,z)\\) をエネルギー \\(H(x,z)\\) の言葉で与えているモデルであると見ることができ，このエネルギー \\(H\\) をニューラルネットワークによってモデル化するのである．\nすると EBM の最尤推定とは，訓練データ \\(\\{x_i\\}_{i=1}^n\\) に対して最も低いエネルギーを割り当てるエネルギー関数 \\(H_\\theta\\) を探すという基底状態探索の問題に対応する (LeCun et al., 2007)．\nこれで EBM の概要は終わりであるが，このような極めて一般的な設定で有用な一般論が得られることは驚くべきことである．\n\n\n\n1.2 例\n\n1.2.1 有向グラフを EBM とみなす\nVAE や 正規化流 などの生成モデルや，トランスフォーマー などの自己回帰モデルも EBM とみなせる．\n特にこれらの生成モデルを事後調整しようと思うと，自然に EBM の構造が出てくる．例えば，LLM における RLHF などの事後調整は，生成モデル \\(p(x|z)\\) とある目的関数 \\(\\phi\\) に関して \\[\np'(x|z):=\\frac{1}{Z(z)}p(x|z)\\phi(z)\n\\] によって分布を調整する営みであると見ると，やはり正規化定数を除いた分布族が定まる (Zhao et al., 2024)．2\n\n\n1.2.2 無向グラフと階層モデル\nMarkov 確率場などの無向グラフィカルモデルで定義される分布族や，正規化定数を除いて定義された確率分布族は，自然に EBM とみなせる．3\n初期の研究 (Zhu and Mumford, 1998), (Zhu et al., 1998) では画像への応用を見据えて，浅い EBM を MCMC により推定している．\n(Hinton and Teh, 2001) で論じられている通り，音声や画像などのデータに対する独立成分分析では，DAG などの有向グラフによるモデリングでは正確な推論ができないため，無向グラフによるより良いモデリング法が 積スパートモデル (PoE: Product of Experts) などを通じて探求されていた．\n\n\n1.2.3 Boltzmann 機械\n例えば Ising 模型の一種でもある Hopfield ネットワーク (Hopfield, 1982) と Boltzmann 機械 (Ackley et al., 1985) は最も有名な EBM の１つである．4\nしかしこの場合，分配関数 \\(Z_\\theta\\) の計算が難しく（ほとんどの統計力学模型の中心問題である），EBM を訓練することが難しかった．\nこのようなモデルに対する最初の実用的な訓練手法（というより SGD の目的関数）は，Boltzmann 機械に対して与えられた Contrastive Divergence (Hinton, 2002) であった．\nこれ以降正規化定数が不明である模型も効率的に訓練可能であることが周知され，ICA に応用した論文 (Teh et al., 2003) で “Energy-based Model” の名前が造語された．\n\n\n1.2.4 深層化\n制約付き Boltzmann 機械に対する greedy, layer-by-layer の事前学習を取り入れることで，深層化しても訓練が効率的に行われるようになった (Hinton et al., 2006), (Hinton and Salakhutdinov, 2006)．\nしかしこの方法により訓練されたモデルは深層信念機械というべきものであり，一般的な Boltmzmann 機械とは違った．深層 Boltzmann 機械の訓練法は (Salakhutdinov and Hinton, 2009) で確立された．\n画像生成の分野においても，生成 CNN (generative ConvNet) (Xie et al., 2016) 以降，\\(H_\\theta\\) のモデリングには深層の CNN が用いられるようになっていった．\n\n\n\n1.3 分配関数\nエネルギーベースモデルでは，分布は正規化定数を除いて定まっており， \\[\nZ_\\theta:=\\int e^{-H(x,\\theta)}dx\n\\] も既知ではないとする．この点が EBM の表現力を支える自由度の高さであるが，5 EBM の訓練にあたっては \\(Z_\\theta\\) の推定というタスクが増える．\nこれが 自己回帰モデル や フローベースモデル との最大の違いである．\nまた，GAN や VAE も含めた生成モデルでは，生成のタスクが念頭にあるためにモデルを有向グラフによって表現するが，EBM はより一般の状況も考える広いクラスだと言える．\n\\(Z_\\theta\\) の推定という追加のタスクには，典型的には MCMC が用いられ，最尤推定 2 が完遂させられるが，近年スコアマッチング（第 3 節）や ノイズ対照学習 などの新たな手法が提案されている．\n\n\n1.4 エネルギー\n仮に，２つの学習済みモデル \\(p_1(x),p_2(x)\\) が EBM の形で得られており，それぞれのエネルギーが \\(H_1,H_2\\) で与えられるとする．この際，\\(H:=H_1+H_2\\) をエネルギーにもつ EBM は \\(p\\,\\propto\\,p_1p_2\\) であり，このモデルは \\(p_1\\) でも \\(p_2\\) でも高確率であるような \\(x\\) を高く評価することになる．\nこのようにして得るモデル \\(p\\) を 積スパートモデル (PoE: Product of Experts) (Hinton, 2002) という．6 (Hinton, 2002) は引き続き，対照分離度 (contrast divergence) 2.3 による訓練法を提案している．\n例えば (Murphy, 2023, p. 840) では，タンパク質構造の生成モデルを作りたいとして，\\(p_1\\) を「常温で安定であるタンパク質」の生成モデル，\\(p_2\\) を「COVID-19 のスパイクタンパク質に結合するタンパク質」の生成モデルとして説明している．\n従ってこの \\(H\\) は，データの好ましさを表すパラメータと考えられ，他モデルへの移転にも使えることが期待される．\\(H\\) は，contrast function, value function, 負の対数尤度などとも呼ぶ (LeCun et al., 2007, p. 193)．\n\n\n1.5 応用\n２次元の密度推定に対して，FCE (Flow Contrastive Estimation, 第 4.5 節) により学習させた EBM は，正規化流 よりも遥かに小さいネットワークサイズで高い性能を示した (Gao et al., 2020)．\n\n\n1.6 推定手法\nContrastive Divergence 2.3 は漸近的に消えないバイアスを持つ (Carreira-Perpiñán and Hinton, 2005) が，SM 3 と NCE 4 は一致推定量である．\n\n1.6.1 CD\nCD 2.3 は最尤推定を SGD によって実行するために建てられた代理目標である．\n近似が入っているために消えないバイアスがある．\n\n\n1.6.2 スコアマッチング\nスコアマッチング（第 3 節）のアイデアは，データ分布の密度 \\(p(x)\\) とモデル \\(p_\\theta(x)\\) とのスコア関数をマッチングさせることを狙う： \\[\n\\nabla_x\\log p(x)=\\nabla_x\\log p_\\theta(x).\n\\] このとき，両辺を積分すると，正規化条件から \\(p(x)=p_\\theta(x)\\) が従う．\nこの考え方は EBM だけでなく，尤度の評価は困難でも対数尤度の評価は可能である文脈で普遍的に有効である．\n例えば 自己符号化器 においても用いられるアイデアである (Vincent, 2011), (Swersky et al., 2011)．\n\n\n1.6.3 NCE\nNCE 4 は，表現学習や深層距離学習で用いられる 対照学習 を，特にノイズと対照させることで最尤推定に応用したものである．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#sec-ML",
    "href": "posts/2024/Samplers/EBM.html#sec-ML",
    "title": "エネルギーベースモデル",
    "section": "2 最尤推定",
    "text": "2 最尤推定\n\n2.1 はじめに\nEBM は，データの分布との KL 乖離度，または等価なことだが対数尤度の期待値 \\[\n\\operatorname{E}[\\log p_\\theta]\n\\] を最小化することによって学習することが考えられる (LeCun et al., 2007)．\nしかし尤度の評価は正規化定数 \\(Z_\\theta:=\\int e^{-H_\\theta(x)}\\,dx\\) の評価が必要であるため，一般の設定では実行できないが，勾配 \\[\n\\nabla_\\theta\\log p_\\theta(x)=-\\nabla_\\theta H_\\theta(x)-\\nabla_\\theta\\log Z_\\theta\n\\tag{1}\\] は近似できる．\\(\\nabla_\\theta H_\\theta(x)\\) はニューラルネットワークの自動微分で計算することができ，２項目は \\[\n\\nabla_\\theta\\log Z_\\theta=-(p_\\theta|\\nabla_\\theta H_\\theta)\n\\] の関係を用いて，\\(p_\\theta\\) からのサンプルを用いた Monte Carlo 推定量で評価できる (Younes, 1999)．\n正規化定数の不明な密度 \\(p_\\theta\\) からのサンプリングといえば，MCMC と SMC である．この Monte Carlo 近似を通じて，確率的勾配降下法によって最尤推定が実行できる．\n\n\n2.2 確率勾配 Langevin 動力学 (SGLD) (Welling and Teh, 2011)\nこの際の Monte Carlo 推定には，多少バイアスがあっても高速に収束してくれる MCMC が欲しい．そこで提案されたのが 確率勾配 Langevin 動力学 である．\nこれは (Hyvärinen, 2005) のスコア関数7 \\[\n\\nabla_x\\log p_\\theta(x)=-\\nabla_x H_\\theta(x)\n\\tag{2}\\] の値から情報を得て，\\(x\\) の空間上で効率的な Markov 連鎖のダイナミクスを構成する方法である．\n\\(H_\\theta\\) の勾配が急であればあるほど高速に収束するが，Zig-Zag サンプラー などの PDMP 手法の方が高速に収束する．\n\n\n2.3 対照分離度 (CD)\nSGD の各ステップに Monte Carlo 推定が必要であるが，毎度 MCMC を十分な数だけ回して，十分に分散が小さい勾配の推定量を得る必要はない．\nこのように，系統的に MCMC を打ち切って，手早く計算された勾配の推定量を通じて SGD により最尤推定を行う方法 (short-run MCMC) の代表的なものに，対照分離度 (CD: Contrastive Divergence) (Hinton, 2002) がある．\nCD による訓練では，バッチごとに提供された訓練データ \\(x_n\\) を開始地点として，一定の回数 \\(T\\) だけ MCMC を回す．多くの場合 \\(T=1\\) でさえある (CD-1 algorithm)．\n\n2.3.1 RBM での CD の例\n(Hinton, 2002) は Gibbs サンプリングが可能な潜在変数を持つ EBM モデルである制限付き Boltzmann マシン (RBM) について CD を提案した．\n特に簡単な binary RBM は，次のエネルギーが定める Markov 確率場である： \\[\nH_w(x,z)=\\sum_{d\\in[D],k\\in[K]}x_dz_kW_{dk}+\\sum_{d=1}^Dx_db_d+\\sum_{k=1}^KZ_kx_k.\n\\] 観測 \\(x_d\\) を入力して学習させたとき，\\(z_k\\) も似たような値だった場合，それが強化されるように \\(W_{dk}\\) が更新されるというように，Hebb 則に則った学習が行われる．\nこのとき， \\[\n\\frac{\\partial }{\\partial w_{dk}} H_w(x,z)=z_dz_k\n\\] より，対数尤度の勾配 (1) の期待値は \\[\n\\operatorname{E}[\\nabla_w\\log p_w(x)]=-\\operatorname{E}[xz^\\top]-(p_\\theta(x)|xz^\\top).\n\\]\n第一項はデータ \\(x_n\\) に対して \\(x_n\\operatorname{E}[z|x_n,W]^\\top\\) によって，第二項は \\(x_n\\) を初期値として \\(T\\) 回 MCMC を回して得られたサンプル \\(x_n'\\) を用いて \\(x_n'\\operatorname{E}[z|x_n',W]^\\top\\) によって推定される．\nこの手続きは，\\(p_0\\) をデータ \\(\\{x_i\\}_{i=1}^n\\) の分布として， \\[\\newcommand{\\CD}{\\operatorname{CD}}\n\\CD_T:=\\operatorname{KL}(p_0,p_\\infty)-\\operatorname{KL}(p_T,p_\\infty)\n\\] を最小化することに相当している．\n\n\n\n2.3.2 PCD：効率的な不偏推定を目指して\nデータ点を取り替えるごとに \\(p_0\\) を取り替えるのではなく，\\(p_0\\) を以前の MCMC の終わり値から定めた場合の CD の変形を PCD (Persistent Contrastive Divergence) (Tieleman, 2008), (Tieleman and Hinton, 2009) という．\n確かにバッチごとに \\(\\theta\\) がアップデートされるため，MCMC の目標分布 \\(p_\\theta\\) は取り替える必要があるから，CD-\\(T\\) のように毎回新たな MCMC を回す必要があるように思えるかもしれない．しかし，\\(\\theta\\) の更新は総じて大変小さなものであるとすると，真のモデル分布 \\(p_\\theta\\) からはずれていくかもしれないが，１つの収束した MCMC からサンプリングし続けた方が良い可能性がある．\n更に，完全に同じ MCMC を走らせ続けるというところから，リサンプリングを取り入れて \\(\\theta\\) のアップデートに収束性を保ちながら対応することで，GAN に匹敵する性能と，分布の峰を正確に再現できるという GAN にはない美点を獲得できるという (Du and Mordatch, 2019)．\n\n\n2.3.3 MCMC による生成\n一方で，正しい \\(p_\\theta\\) によく収束する short-run MCMC が CD 法により訓練できたならば，これは効率的な生成モデルとなるかもしれない．\n(Nijkamp et al., 2019) は，MCMC が EBM モデルに対置されている analysis by synthesis スキーム (Grenander and Miller, 1994) と見て，この short-run MCMC をよく学ぶことに特化したアプローチ Short-Run MCMC as Generator or Flow Model を提案した．\nこのアプローチでは，MCMC は毎回同じ初期分布（ノイズの分布）からスタートさせ，\\(T\\) の値も固定する．このようなスキームで学習された EBM は全く良い性能を持たないが，EBM から返ってくる Hyvärinen スコアを持った勾配 MCMC 法は，生成モデルとして良い性能を持つという (Nijkamp et al., 2019)．\n\n\n2.3.4 安定した CD 訓練に向けて\n(Du et al., 2021) は，この MCMC に情報を与える Hyvärinen スコアの変化が CD 訓練の重要な要素であり，これを正確に扱うことがを安定化させることを報告した．\n特にこれは，従来 CD フレームワークの目的関数が捨象していた「第三項」 \\[\n\\frac{\\partial q_\\theta}{\\partial \\theta}\\frac{\\partial \\operatorname{KL}(q_\\theta,p_\\theta)}{\\partial q_\\theta}\n\\] を目的関数に含めることとして捉えられる (Du et al., 2021)．ただし，\\(q_\\theta\\) は真のデータ分布を初期分布として \\(T\\) ステップ MCMC を実行して得られる分布とした．\n\n\n2.3.5 adversarial CD\n尤度 \\(p_\\theta\\) の評価を迂回するため，GAN にヒントを得た (Finn et al., 2016)，２つのネットワークを対置させて行う敵対的な学習も考えられている (Kim and Bengio, 2016)．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#sec-SM",
    "href": "posts/2024/Samplers/EBM.html#sec-SM",
    "title": "エネルギーベースモデル",
    "section": "3 スコアマッチング (SM, Hyvärinen, 2005)",
    "text": "3 スコアマッチング (SM, Hyvärinen, 2005)\n\n3.1 はじめに\nEBM のスコア関数 (2) を \\[\ns_\\theta(x):=\\nabla_x\\log p_\\theta(x)=-\\nabla_xH_\\theta(x)\\tag{2}\n\\] と定め，データ分布 \\(p\\) との Fisher 乖離度を最小化するスキームが (Hyvärinen, 2005) の スコアマッチング 目的関数である： \\[\nD_F(p,p_\\theta)=\\frac{\\|s-s_\\theta\\|_{L^2(p)}^2}{2}=\\operatorname{E}\\left[\\frac{1}{2}\\biggl|\\nabla_x\\log p(X)-\\nabla_x\\log p_\\theta(X)\\biggr|^2\\right].\n\\tag{3}\\]\nこの際，データ分布のスコア \\(s(x)=\\nabla_x\\log p(x)\\) の計算を回避することが焦点になる．\n\n\n3.2 部分積分 (Hyvärinen, 2005)\n次が成り立つことがあり得る： \\[\\begin{align*}\n    D_F(p,p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\lvert s_\\theta(X)\\rvert^2+\\operatorname{div}(s_\\theta(X))\\right]+(\\theta\\;\\text{に依らない定数})\\\\\n    &=\\operatorname{E}\\left[\\frac{1}{2}\\lvert\\nabla H_\\theta(X)\\rvert^2-\\mathop{}\\!\\mathbin\\bigtriangleup H_\\theta(X)\\right]+(\\theta\\;\\text{に依らない定数})\n\\end{align*}\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    D_F(p,p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\biggl|\\nabla_x\\log p(x)-\\nabla_x\\log p_\\theta(x)\\biggr|^2\\right]\\\\\n    &=\\frac{1}{2}\\int_{\\mathbb{R}^d}\\sum_{i=1}^d\\left(\\frac{1}{p(x)}\\frac{\\partial p}{\\partial x_i}(x)-s_\\theta(x)_i\\right)^2p(x)\\,dx\\\\\n    &=\\frac{1}{2}\\int_{\\mathbb{R}^d}\\frac{\\lvert Dp\\rvert^2}{p}\\,dx-\\sum_{i=1}^d\\int_{\\mathbb{R}^d}s_\\theta(x)_i\\frac{\\partial p}{\\partial x_i}(x)+\\frac{1}{2}\\int_{\\mathbb{R}^d}\\lvert s_\\theta(x)\\rvert^2p(x)\\,dx\\\\\n    &=\\mathrm{const.}+\\int_{\\mathbb{R}^d}\\operatorname{Tr}(Ds_\\theta)p\\,dx+\\operatorname{E}\\left[\\frac{1}{2}\\lvert s_\\theta(x)\\rvert^2\\right]\\\\\n    &=\\operatorname{E}\\left[\\frac{1}{2}\\lvert s_\\theta(x)\\rvert^2+\\operatorname{div}(s_\\theta(x))\\right]+\\mathrm{const.}\n\\end{align*}\\]\n\n\n\nこの右辺はデータのスコアを含まないので，\\(p_\\theta\\) の２階微分が計算可能ならば計算できるが，データの次元 \\(d\\) に関するスケールは悪い．\n加えて，\\(D_F(p,p_\\theta)\\) をここから \\(\\theta\\) に関して微分することが困難である．独立成分分析モデルを除いて，(Köster and Hyvärinen, 2007), (Köster et al., 2009) などのニューラルネットワークモデルにも応用されたが，画像などの実データに直接適用することは困難であった．特に，\\(H_\\theta\\) の勾配と Laplacian を解析的に計算してから実装していた．\n\n\n3.3 正則化スコアマッチング (RSM) (Kingma and LeCun, 2010)\n画像データなどの量子化されたデータの密度 \\(p(x)\\) は可微分ではない上に，有限な台を持ってしまうためスコア \\(s\\) は well-defined ではない．\nこのような量子化されたデータ \\(x\\) に対して，Gauss ノイズを加えたもの \\[\n\\widetilde{x}=x+\\epsilon,\\qquad\\epsilon\\sim\\operatorname{N}(0,\\sigma^2I)\n\\] は連続なデータに変貌する（Gauss 核が軟化子として働く）．\nこの \\(\\epsilon\\) だけ摂動されたデータの分布 \\(\\widetilde{p}\\) に対して，スコアマッチング目的関数 (3) は \\[\\begin{align*}\n    D_F(\\widetilde{p},p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\lvert\\nabla H_\\theta\\rvert^2-\\mathop{}\\!\\mathbin\\bigtriangleup H_\\theta+\\frac{\\sigma^2}{2}\\|D^2H_\\theta\\|^2_2\\right]+O(\\epsilon^2)\\\\\n    &=D_F(p,p_\\theta)+\\frac{\\sigma^2}{2}\\operatorname{E}\\biggl[\\|D^2H_\\theta\\|_2^2\\biggr]+O(\\epsilon^2)\n\\end{align*}\\] と表せる (Kingma and LeCun, 2010)．\nこの \\(D_F\\) は，Hessian \\(D^2H\\) の非対角項を対角項で近似し，結局は元の目的関数に対して正則化項 \\[\n\\lambda\\mathop{}\\!\\mathbin\\bigtriangleup H_\\theta,\\qquad\\lambda\\approx\\frac{\\sigma^2}{2}\n\\] を加えたもので近似できる．\nこれを目的関数の用いる方法が 正則化スコアマッチング (RSM: Regularized Score Matching) (Kingma and LeCun, 2010) である．\nさらに (Kingma and LeCun, 2010) は，従来のように，解析的に微分が計算可能な \\(H_\\theta\\) を採用するのではなく，誤差逆伝播を２回行う Double Backpropagation (Drucker and Le Cun, 1992) によってこの目的関数の勾配 \\(\\frac{d D_F(p,p_\\theta)}{d \\theta}\\) を自動微分で計算する方法を提案した．\n\n\n3.4 Denoising スコアマッチング (DSM) (Vincent, 2011)\nRSM の場合と違い，次のような表示もできる (Vincent, 2011)： \\[\nD_F(\\widetilde{p},p_\\theta)=\\frac{1}{2}\\operatorname{E}\\left[\\left\\|s_\\theta(\\widetilde{X})-\\frac{X-\\widetilde{X}}{\\sigma^2}\\right\\|^2_2\\right]+\\mathrm{const.}\n\\]\nすなわち，ノイズ消去方向のベクトル \\(\\frac{x-\\widetilde{x}}{\\sigma^2}\\) に一致するようにモデルのスコア \\(s_\\theta\\) を学習することが考えられる．\nこうすることで，\\(D^2H_\\theta\\) の計算を回避することができる．\nただし，RSM と DSM に共通することであるが，あくまで \\(D_F(\\widetilde{p},p_\\theta)\\) はノイズの入ったデータ分布を学習してしまうのであり，\\(\\sigma\\to0\\) が志向されるが，\\(\\sigma\\) が小さいほど \\(D_F(\\widetilde{p},p_\\theta)\\) に関する推定は不安定になる．8\n\n\n3.5 スライススコアマッチング (SSM) (Song et al., 2019)\nSSM (Sliced Score Matching) (Song et al., 2019) ではスコアマッチング目的関数 (3) 自体を，sliced Fisher 乖離度 \\[\\begin{align*}\n    D_{SF}(p,p_\\theta)&=\\operatorname{E}\\left[\\frac{1}{2}\\biggr(V^\\top s(X)-V^\\top s_\\theta(X)\\biggl)^2\\right]\\\\\n    &=\\operatorname{E}\\left[\\frac{1}{2}\\sum_{i=1}^d\\left(\\frac{\\partial H_\\theta(X)}{\\partial x_i}V_i\\right)^2+\\sum_{i,j=1}^d\\frac{\\partial ^2H_\\theta(X)}{\\partial x_i\\partial x_j}V_iV_j\\right]+\\mathrm{const.}\n\\end{align*}\\] で Monte Carlo 近似する．ただし，\\(V\\) はランダムな \\(\\mathbb{R}^d\\) 上のベクトルである．9\nやはり \\(D^2H_\\theta\\) が登場しているように思えるが， \\[\n\\sum_{i,j=1}^d\\frac{\\partial ^2H_\\theta}{\\partial x_i\\partial x_j}V_iV_j=\\sum_{i=1}^d\\frac{\\partial }{\\partial x_i}V_i\\sum_{j=1}^d\\frac{\\partial H_\\theta}{\\partial x_j}V_j=\\sum_{i=1}^d\\frac{\\partial }{\\partial x_i}V_i(DH_\\theta|V)\n\\] の表示により，一度内積 \\((DH_\\theta|V)\\) を計算してしまえば，以降，この項は自動微分を通じて \\(O(d)\\) のオーダーで計算できる．\nなお，\\(V\\sim\\operatorname{N}_d(0,I_d)\\) と取った際の目的関数 \\(D_{SF}\\) は，元の Fisher 乖離度による目的関数に対して，Hutchinson の跡推定量（正規化流でも出てきた）により Jacobian \\(Ds_\\theta\\) を推定したものとも同一視できる．\nSSM の最大の美点は，ノイズが印加された分布 \\(\\widetilde{p}\\) を学習してしまう DSM と違って真の分布 \\(p\\) を学習できることである．一方で，自動微分の分だけ，4倍ほど計算量が必要になる．\n\n\n3.6 SM 目的関数の理論\nスコアマッチングの目的関数 \\(D_F(p,p_\\theta)\\) は，Contrastive Divergence のある極限に一致する (Hyvarinen, 2007)．\nステップサイズ \\(\\epsilon&gt;0\\) の Langevin Monte Carlo 法により \\(p_\\theta\\) からサンプリングした場合の対数尤度の期待値は \\[\n\\operatorname{E}[\\nabla_\\theta\\log p_\\theta]=\\frac{\\epsilon^2}{2}\\nabla_\\theta D_F(p,p_\\theta)+o(\\epsilon^2)\n\\] と Monte Carlo 近似されるという (Hyvarinen, 2007)．\nここで de Bruijin の関係 \\[\n\\frac{d }{d t}\\operatorname{KL}(\\widetilde{p}_t,\\widetilde{p}_{\\theta,t})=-\\frac{1}{2}D_F(\\widetilde{p}_t,\\widetilde{p}_{\\theta,t})\n\\] に似た消息が生じている (Lyu, 2009)．なお，\\(\\widetilde{p}\\) とは，\\(p,p_\\theta\\) の分散 \\(t^2\\) を持った Gaussian i.i.d. ノイズによる摂動とする．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#sec-NCE",
    "href": "posts/2024/Samplers/EBM.html#sec-NCE",
    "title": "エネルギーベースモデル",
    "section": "4 ノイズ対照学習 (NCE, M. Gutmann and Hyvärinen, 2010)",
    "text": "4 ノイズ対照学習 (NCE, M. Gutmann and Hyvärinen, 2010)\n\n4.1 はじめに\n既知の密度 \\(p_n\\) と対置させ，これをノイズとしてデータ分布 \\(p\\) との識別を繰り返すことで学習を行う．\n\\(Y\\sim\\mathrm{Ber}\\left(\\frac{\\nu}{1+\\nu}\\right)\\) に関する混合を \\[\n\\widetilde{X}:=YX+(1-Y)N,\\qquad N\\sim p_n,X\\sim p,\n\\] と定め，これに対して混合分布族 \\[\np_{n,\\theta}:=\\frac{1}{1+\\nu}p_n+\\frac{\\nu}{1+\\nu}p_\\theta\n\\] を KL-乖離度の意味でマッチさせることを目指す．\n\n\n4.2 ノイズ \\(p_n\\) の選び方\n重点サンプリング法の提案分布のように，\\(p_n\\) は真のデータ分布 \\(p\\) に近ければ近いほどよい (M. U. Gutmann and Hirayama, 2011)．\n従って多くの方法では，\\(p_n\\) を適応的に選ぶことが考えられる．\n\n\n4.3 スコアマッチングによる解釈\n\\(p_n\\) を \\(p\\) の摂動 \\[\np_n(x):=p(x-v)\n\\] とした場合，\\(\\|v\\|_2\\to0\\) の極限において，\\(V\\) に関するスライススコアマッチングの目的関数 3.5 に一致する (M. U. Gutmann and Hirayama, 2011), (Song et al., 2019)．\n\n\n4.4 応用\nノイズ対照学習は，距離学習や埋め込み，特に word2vec (Mikolov, Chen, et al., 2013), (Mikolov, Sutskever, et al., 2013) などの言語の埋め込みに応用される．\n\n\n4.5 フロー対照推定 (FCE, Gao et al., 2020)\nノイズ対照推定のノイズ分布を，正規化流 を用いて敵対的に高難易度にしていく．この状況で，EBM とフローベースモデルを同時に学習することを考える手法である．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#その他の訓練方法",
    "href": "posts/2024/Samplers/EBM.html#その他の訓練方法",
    "title": "エネルギーベースモデル",
    "section": "5 その他の訓練方法",
    "text": "5 その他の訓練方法\n\n5.1 Stein 乖離度\nStein 乖離度は，Fisher 乖離度の１つの \\(s_\\theta\\) を動かして得るダイバージェンスである： \\[\nD_S(p,p_\\theta):=\\sup_{f\\in\\mathcal{F}}\\operatorname{E}\\left[s_\\theta(X)^\\top f(X)+\\operatorname{div}(f(X))\\right].\n\\]\nFisher 乖離度の難点は発散項の計算が \\(O(d^2)\\) の計算量を持ってしまうことであった．Stein 乖離度はこれをカーネル法の方法で迂回することができる．\n\\(\\mathcal{F}\\) がある RKHS の単位閉球であった場合，発散項は定数になる (Chwialkowski et al., 2016), (Q. Liu et al., 2016)．\n\n\n5.2 敵対的な訓練\n(Murphy, 2023) 24.5.3 も参照．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#文献",
    "href": "posts/2024/Samplers/EBM.html#文献",
    "title": "エネルギーベースモデル",
    "section": "6 文献",
    "text": "6 文献\n\nEnergy-based model は独立成分分析の研究 (Teh et al., 2003) において命名され，スコアマッチングの (Hyvärinen, 2005) も非線型独立成分分析の大家である．\nAwesome-EBM レポジトリは，種々の EBM のリストを与えている．\n系統的なイントロダクションには (Bishop and Bishop, 2024) 14.3節，(Murphy, 2023) 24章が良い．\n訓練方法について (Song and Kingma, 2021) が詳細なサーベイを与えている．上の２つの教科書の記述も多くはこのサーベイに基づいている．\n(Gao et al., 2020)"
  },
  {
    "objectID": "posts/2024/Samplers/EBM.html#footnotes",
    "href": "posts/2024/Samplers/EBM.html#footnotes",
    "title": "エネルギーベースモデル",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nnon-normalized probabilistic model ともいう (Song and Kingma, 2021)．この形の分布族を 正準分布 または Gibbs 分布 (Koller and Friedman, 2009, p. 108), (Friedli and Velenik, 2017, p. 25)，または Boltzmann 分布 (Kim and Bengio, 2016), (Mézard and Montanari, 2009, p. 23), (Chewi, 2024) ともいう．物理の用語では \\(e^{H(z)}\\) を Boltzmann 因子と呼ぶのみであるようである (田崎晴明, 2008, p. 107)．(J. S. Liu, 2004, p. 7) ではどちらも掲載している．正準集団は，NVT 一定集団ともいう．↩︎\n今回の表示は特に SMC と相性が良く，(Zhao et al., 2024) では \\(p'(x|z)\\) を段階的に近似する列を通じた twisted SMC という手法を提案している．↩︎\nだが，正規化定数 \\(Z\\) が評価できないという前提で EBM の理論は進むため，有向グラフで定義される VAE などのモデルを EBM とみる積極的な理由はない．↩︎\nなお，2つの Hamiltonian \\(H\\) の形は同じで温度が違うのみである．加えて，用途も違う：Hopfield ネットワークは連想記憶のモデル，Boltzmann 機械はデータ分布の（生成）モデリングに用いられる (Carbone, 2024, p. 4)．↩︎\n例えば \\(H\\) のモデリングには GNN，CNN などが自由に使える．↩︎\nMoE (Mixture of Expert) (Jacobs et al., 1991) と並んで使う用語である．↩︎\n通常，スコア関数といったとき，微分はパラメータ \\(\\theta\\) について取ることに注意．↩︎\n\\(D_F(\\widetilde{p},p_\\theta)\\) を近似する Monte Carlo 推定量の分散が大きくなる．↩︎\n(Song et al., 2019) では \\(V\\sim\\operatorname{N}_d(0,I_d)\\) とすることで Monte Carlo 近似による追加の誤差を回避している．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html",
    "href": "posts/2024/Samplers/NF.html",
    "title": "正規化流",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#概観",
    "href": "posts/2024/Samplers/NF.html#概観",
    "title": "正規化流",
    "section": "1 概観",
    "text": "1 概観\n正規化流 (NF: Normalizing Flow) とは (Tabak and Vanden-Eijnden, 2010), (Tabak and Turner, 2013) が提案・造語した生成モデリングアプローチである．\nニューラルネットワークとしては画像生成を念頭に置いた密度推定モデルである NICE (Dinh et al., 2015)（のちに Real NVP (Dinh et al., 2017)）が初期のものであり，正則化流を有名にした．\nまた変分推論のサブルーチンとして正則化流を使えば，効率的な reparametrization trick を可能にするというアイデア (Rezende and Mohamed, 2015) も普及に一役買ったという (Kobyzev et al., 2021)．\nアイデアとしては，ニューラルネットワークの一層一層を可逆に設計するという制約を課すのみである．\n\n1.1 原理\n\n1.1.1 基底分布の押し出しとしての生成モデリング\nGAN，VAE，拡散モデル など，深層生成モデルは，潜在空間 \\(\\mathcal{Z}\\) 上の基底分布 \\(p(z)dz\\) を，パラメータ \\(w\\in\\mathcal{W}\\) を持つ深層ニューラルネットによる変換 \\(f_w:\\mathcal{Z}\\times\\mathcal{W}\\to\\mathcal{X}\\) を通じて，押し出し \\[\np_w(x):=(f_w)_*p(x)\n\\] により \\(\\mathcal{X}\\) 上の分布をモデリングする．\nこのモデル \\(\\{p_w\\}_{w\\in\\mathcal{W}}\\) の尤度は解析的に表示できない．そこで，GAN (Goodfellow et al., 2014) は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものであり，VAE (Kingma and Welling, 2014) は変分下界を通じて尤度を近似するというものであった．\n\n\n1.1.2 逆変換の利用\n正規化流 (normalizing flow / flow-based models) では，拡散モデル に似て，「逆変換」を利用することを考える．\nすなわち，\\(\\{f_w\\}\\subset\\mathcal{L}(\\mathcal{Z},\\mathcal{X})\\) が可逆であるように設計するのである．逆関数を \\(g_w:=f_w^{-1}\\) と表すと，\\(p_w(x)dx\\) は \\(p(z)dz\\) の \\(g_w\\) による引き戻しの関係になっているから，変数変換 を通じて， \\[\np_w(x)=p(g_w(x))\\lvert\\det J_{g_w}(x)\\rvert\\;\\mathrm{a.e.}\\;\n\\] が成立する．\nすると， \\[\n\\log p_w(x)=\\log p(g_w(x))+\\log\\lvert\\det J_{g_w}(x)\\rvert\n\\] を通じて，尤度の評価とパラメータの最尤推定が可能である．\n\n\n\n1.2 実装\n\n従って，可逆なニューラルネットワーク \\(\\{f_w\\}\\subset\\mathcal{L}(\\mathcal{Z},\\mathcal{X})\\) を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．1\nこのとき，行列式 \\(\\det:\\mathrm{GL}_n(\\mathbb{R})\\to\\mathbb{R}^\\times\\) は群準同型であるから，\\(g_w\\) のヤコビアンは，各層のヤコビアンの積として得られ，その対数は \\[\n\\log\\left|\\det J_{g_w}(x)\\right|=\\sum_{i=1}^n\\log\\left|\\det J_{g_i}(z_i)\\right|,\\qquad z_i:=f_i\\circ\\cdots\\circ f_1(z),\n\\] と得られる．\nこの条件はたしかにモデルに仮定を置いている．\\(p(z)dz\\) は典型的に正規で，\\(f_w\\) は可逆である．\nしかしそれでも，深層ニューラルネットワーク \\(\\{f_w\\}\\) の表現力は十分高いため，多様な密度のモデリングにも使うことが出来る (Papamakarios et al., 2021)．\n\n\n1.3 アーキテクチャ\n次節 2 で紹介するカップリング流は大変親しみやすい．\nその特別な場合として自己回帰流（第 3 節）があり，複雑な成分間の条件付き構造をモデリングできるが，その分高次元データに対する密度評価とサンプリングは遅くなる．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#sec-coupling-flow",
    "href": "posts/2024/Samplers/NF.html#sec-coupling-flow",
    "title": "正規化流",
    "section": "2 カップリング流",
    "text": "2 カップリング流\n\n2.1 概要\nある可微分同相 \\(f_\\theta:\\mathbb{R}^m\\to\\mathbb{R}^m\\) と任意の関数 \\(\\Theta:\\mathbb{R}^{n-m}\\to\\mathbb{R}^{n-m}\\) について， \\[\n(z^{(1)},z^{(2)})\\mapsto (f_{\\Theta(z^{(2)})}(z^{(1)}),z^{(2)})\n\\] で定まる変換を カップリング層，\\(f\\) をカップリング関数，\\(\\Theta\\) を conditioner という．\n(Kingma and Dhariwal, 2018) では，conditioner には ResNet を使っている．\nこの逆変換は \\[\n(x^{(1)},x^{(2)})\\mapsto (f^{-1}_{\\Theta(x^{(2)})}(x^{(1)}),x^{(2)})\n\\] で与えられる．\n加えて，後半の成分 \\((-)^{(2)}\\) には変換を施していないので，カップリング層の Jacobian は \\(f_\\theta\\) の Jacobian に一致する．\n\\(f_\\theta\\) は各成分が可逆になるように設計することで \\(f_\\theta^{-1}\\) が計算しやすくされることが多い．\n\n\n2.2 Real NVP\nカップリング流を最初に提案したのが NICE (Non-linear Independent Component Estimation) (Dinh et al., 2015) であり，これを画像データに応用したのが (Dinh et al., 2017) の RealNVP (Real-valued Non-Volume-Preserving) である．\nNICE では affine カップリング \\[\nf_\\theta(x)=\\theta_1x+\\theta_2\n\\] を考えていたが，RealNVP ではさらに \\[\nf_{\\Theta(z^{(2)})}(z^{(1)})=e^{f_{\\Theta(z^{(2)})}^{(1)}} \\odot z^{(1)}+f_{\\Theta(z^{(2)})}^{(2)}(z^{(1)}),\n\\] という形を与えている．ただし \\(\\odot\\) はベクトルの成分ごとの積とした．\n実際に訓練する変換は \\(f^{(1)}_\\theta,f^{(2)}_\\theta\\) のみであり，これを可逆に制約するということはなく，実装も簡単になっている．\nこうして得たカップリング層を，分割 \\(z\\mapsto(z^{(1)},z^{(2)})\\) を取り替えながら（permutation layer と呼ばれる） 32 層重ねるのみで，効率的な密度モデリングができる：\n\n\n\n画像をタップしてコードを見る\n\n\n\n\n2.3 GLOW (Kingma and Dhariwal, 2018)\nReal NVP では簡単な置換 (permutation) が用いられていたところを，(Kingma and Dhariwal, 2018) では，\\(1\\times 1\\) の畳み込みに基づく，一般化されたカップリング・アーキテクチャを提案した．\nこのことによる計算量の増加は，LU 分解の利用によって回避している．\nOpenAI release に発表されているように，画像生成タスクに応用された．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#sec-autoregressive-flow",
    "href": "posts/2024/Samplers/NF.html#sec-autoregressive-flow",
    "title": "正規化流",
    "section": "3 自己回帰流",
    "text": "3 自己回帰流\n\n3.1 概要\n自己回帰流 (autoregressive flow) とは，カップリング流のカップリング関数 \\(f_\\theta\\) を極限まで押し進めた立場である．\n入力 \\(z\\in\\mathbb{R}^n\\) を形式上時系列と見做し，ある可微分関数 \\(f_\\theta:\\mathbb{R}\\to\\mathbb{R}\\) と任意の関数列 \\(\\Theta_i\\) について， \\[\nx_i=f_{\\Theta_i(x_{1:i-1})}(z_i)\n\\] と再帰的に定義していく変換 \\(z\\mapsto x\\) を 自己回帰流 という．\nこの逆は \\[\nz_i=f_{\\Theta_i(x_{1:i-1})}^{-1}(x_i)\n\\] で与えられる．\n自己回帰流の Jacobi 行列は上三角行列になるので，Jacobian は効率的に計算できる．\n\n\n3.2 マスク付き自己回帰流 (MAF)\n\\(\\Theta_i\\) を単一の自己回帰型ニューラルネットワークを用いてモデリングするためには，\\(\\Theta_i\\) が \\(x_{1:i-1}\\) のみに依存するようにマスク (Germain et al., 2015) をするとよい．\nこうすることで \\(x_1,x_2,\\cdots\\) と順々に生成する必要はなくなり，並列で計算することができる．（しかし逆の計算は並列化はできない）．\nこの方法を採用したのが MAF (Masked Autoregressive Flow) (Papamakarios et al., 2017) であった．\nMAF では \\(f\\) は affine 関数としている．\n\n\n3.3 逆自己回帰流 (IAF)\n\\(f\\) と \\(f^{-1}\\) を取り替え \\[\nx_i=f_{\\Theta_i(z_{1:i-1})}(z_i)\n\\] としてモデリングする方法を IAF (Inverse Autoregressive Flow) (Kingma et al., 2016) という．\nこれにより \\(x_i\\) の生成のために \\(x_1,\\cdots,x_{i-1}\\) を先に評価する必要がなく，並列で生成することが可能になり，サンプリングが高速になる．\n従って IAF はサンプリング用途に使われるが，その分 MAF より密度評価が遅くなってしまう (Papamakarios et al., 2017)．\n\n\n3.4 Parallel WaveNet でのサンプリング加速\nParallel WaveNet (Oord et al., 2018) では，WaveNet モデル \\(p_t\\) からのサンプリングを加速させる方法として IAF \\(p_s\\) を用いている．\n\\(p_t\\) の密度評価は高速であったため，\\(\\operatorname{KL}(p_s,p_t)\\) の値は，\\(p_t\\) の評価と IAF \\(p_s\\) からのサンプリングによって効率的に計算できる．\nこの KL 乖離度を最適化することで \\(p_t\\) のモデルを \\(p_s\\) に移すことで，サンプルの質を保ちながらサンプリングを加速することに成功した．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#変数関数の積によるフロー",
    "href": "posts/2024/Samplers/NF.html#変数関数の積によるフロー",
    "title": "正規化流",
    "section": "4 １変数関数の積によるフロー",
    "text": "4 １変数関数の積によるフロー\n\n4.1 はじめに\nカップリング流と自己回帰流はいずれも変換 \\(f_\\theta:\\mathbb{R}^m\\to\\mathbb{R}^m\\) に基づいており，次元 \\(m\\le n\\) が違うに過ぎない．\nこの \\(f_\\theta\\) の表現力を高めることも重要である．\nそのためには，1次元の写像 \\(f^i_\\theta:\\mathbb{R}\\to\\mathbb{R}\\) の積 \\(f=(f^i)\\) としてデザインすることもできる．\nこのアプローチを element-wise flow ともいう (Kobyzev et al., 2021)．\n\n\n4.2 スプライン\n指定した点 \\(\\{(x_i,y_i)\\}\\) をなめらかに補間する曲線を一般に スプライン という．\n単調関数によって補間すると約束すれば，可逆な変換 \\(f^i\\) を得る．\nスプラインを，2つの二次関数の商 (rational-quadratic spline) によって補間する方法を提案したのが，Neural Spline Flow (Durkan et al., 2019) である．\nこの方法では，指定した点 \\((x_i,y_i)\\) における微分係数が学習すべきパラメータとなる．\n例えば von Mises 分布が次のようにモデリングできる：\n\n\n\n画像をタップでコードを見る"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#sec-residual-flow",
    "href": "posts/2024/Samplers/NF.html#sec-residual-flow",
    "title": "正規化流",
    "section": "5 残差フロー",
    "text": "5 残差フロー\n\n5.1 はじめに\n残差接続 \\[\nu\\mapsto u+F(u)\n\\] において，残差ブロック \\(F\\) が縮小写像になるようにすれば，全体として可逆な層となる．\niResNet (Invertible Residual Networks) (Behrmann et al., 2019) では，縮小写像 \\(F\\) のモデリングに CNN を用いた．\nこの方法のボトルネックは Jacobian の計算にある．愚直に計算すると入力の次元 \\(d\\) に関して \\(O(d^3)\\) の計算量が必要になるため，なんらかの方法で効率化が必要である．\nResidual Flow (R. T. Q. Chen et al., 2019) では，Jacobian の推定に Monte Carlo 推定量を用いた．\n\n\n5.2 Jacobian が計算可能な場合\n\\(J_F\\) のランクが低い場合は，\\(I+J_F\\) の形の行列の Jacobian は効率的に計算できる．\nPlanar flow (Rezende and Mohamed, 2015) では隠れ素子数１の残差接続層を用いたものである： \\[\nf(u)=u+v\\sigma\\biggr(w^\\top u+b\\biggl).\n\\] この設定では Jacobian が次のように計算できる： \\[\n\\det J_f(u)=1+w^\\top v\\sigma'\\biggr(w^\\top u+b\\biggl).\n\\]\nただし非線型な活性化関数 \\(\\sigma\\) は全単射になるように選ぶ必要がある．\n\n\n\n\n\n\n証明\n\n\n\n\n\nこの Jacobian を導出する際には，次の Matrix determinant lemma を用いる．\n\\[\n\\det(A+uv^\\top)=\\det(A)\\biggr(1+v^\\top A^{-1}u\\biggl).\n\\]\nこれは行列式の多重歪線型性を用いて簡単に示すことができる．\n\n\n\nPlanar flow の欠点は逆の計算が難しい点であり，現在広く使われるわけではない (Kobyzev et al., 2021)．\n他に，circular flow (Rezende and Mohamed, 2015) や Sylvester flow (Berg et al., 2019) も同様な解析的な Jacobian の表示を与えるアーキテクチャである．\nSylvester flow (Berg et al., 2019) は \\(\\sigma\\) を多次元化したものである．この際の Jacobian は Sylvester の行列式の補題 により導かれることから名前がついた： \\[\n\\det(I_n+AB)=\\det(I_m+BA).\n\\]\n\n\n5.3 Jacobian を推定する方法\n仮に変換 \\(F\\) に制約を課さずとも，Jacobian は \\[\n\\log\\lvert\\det(I+J_F)\\rvert=\\sum_{k=1}^\\infty\\frac{(-1)^{k+1}}{k}\\operatorname{Tr}(J_F^k)\n\\] を通じて，\\(\\operatorname{Tr}(J_F)\\) が (Skilling, 1989)-(Hutchinson, 1990) の跡推定量 により，その無限和が Russian-roulette 推定量により不偏推定できる (R. T. Q. Chen et al., 2019)．\nRussian-roulette 推定量では，無限和を最初の \\(N\\) 項のみの有限和で近似するとバイアスが入るので，\\(N\\) を確率変数とすることで不偏推定が達成される．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#応用",
    "href": "posts/2024/Samplers/NF.html#応用",
    "title": "正規化流",
    "section": "6 応用",
    "text": "6 応用\n\n6.1 変分推論のサブルーチンとして\n変分推論における \\(E\\)-ステップ（変分分布について期待値を取るステップ）などにおいて，複雑な分布からのサンプラーとしても用いられる (Gao et al., 2020)．\nこの使い方を初めて提唱し，フローベースのアプローチを有名にしたのが (Rezende and Mohamed, 2015) であった．\nこのような変分ベイズ推論，特にベイズ深層学習に向けたフローベースモデルが盛んに提案されている．\nHouseholder flow (Tomczak and Welling, 2017) は VAE の改良のために考えられたものである．直交行列で表現できる層を，直行行列の Householder 行列の積として表現することで学習することを目指す．\n他にも IAF (Kingma et al., 2016), 乗法的正規化流 (Louizos and Welling, 2017), Sylvester flow (Berg et al., 2019) など．\n\n\n6.2 ベイズ計算\nNeural Importance Sampling (Müller et al., 2019) とは，困難な分布からの重点サンプリングの際に，提案分布を正規化流で近似する方法である．この際に \\(f_\\theta(x)\\) にはスプラインを用いている．\nBoltzmann Generator (Noé et al., 2019) は名前の通り，多体系の平衡分布から正規化流でサンプリングをするという手法である．\n(Hoffman et al., 2019) は IAF を用いて目標分布を学習し，学習された密度 \\(q\\) で変換後の分布から MCMC サンプリングをすることで効率がはるかに改善することを報告した．実際，フローによる変換を受けた後は対象分布は正規分布に近くなることから，MCMC サンプリングを減速させる要因の多くが消滅していることが期待される．\n\n\n6.3 密度推定\n目標の分布 \\(p_w\\) を Guass 分布 \\(p\\) からの写像 \\((f_w)_*p\\) として捉える発想は，まずなんといっても密度推定に用いられた (S. Chen and Gopinath, 2000)．この時点では Gaussianization と呼ばれていた．\nこの \\(f_w\\) のモデリングにニューラルネットワークを用いるという発想は (Rippel and Adams, 2013) の Deep Density Model 以来であるようだ．\nその後同様の発想は非線型独立成分分析 (Dinh et al., 2015)，引き続き密度推定 (Dinh et al., 2017) に用いられた．\n現在は MAF 3.2 の性能が圧倒的である．\n\n\n6.4 表現学習\n通常の VAE などは，あくまで \\(p(x|z)\\) を学習する形をとるが，正規化流を用いて結合分布 \\(p(x,z)\\) を学習することで，双方を対等にモデリングすることができる．\nこれを flow-based hybrid model (Nalisnick et al., 2019) という．これは予測と生成のタスクでも良い性能を見せるが，分布外検知などの応用も示唆している．\n異常検知 (Zhang et al., 2020)，不確実性定量化 (Charpentier et al., 2020) のような種々の下流タスクに用いられた場合は，VAE など NN を２つ用いる手法よりもモデルが軽量で，順方向での１度の使用で足りるなどの美点があるという．\n\n\n6.5 生成\n画像生成への応用が多い：GLOW (Kingma and Dhariwal, 2018) (OpenAI release), 残差フロー (R. T. Q. Chen et al., 2019) など．\n動画に対する応用も提案されている：VideoFlow (Kumar et al., 2020)．\n言語に対する応用もある (Tran et al., 2019)．言語は離散データであることが難点であるが，潜在空間上でフローを使うことも提案されている (Ziegler and Rush, 2019)．\n\n\n6.6 蒸留\n純粋な生成モデリングの他に，IAF 3.3 は (Oord et al., 2018) において音声生成モデルの蒸留に用いられた．\nWaveFLOW (Prenger et al., 2018), FloWaveNet (Kim et al., 2019) などもカップリング層を取り入れて WaveNet の高速化に成功している．\n\n\n6.7 SBI\nモデルの尤度は隠されており，入力 \\(\\theta\\) に対して，\\(p(x|\\theta)\\) からのサンプルのみが利用可能であるとする．このような状況でベイズ推論を行う問題を simulation-based inference (SBI) という．\nこれはデータの分布のサンプルから学習して，似たようなデータを増やすという生成モデルのタスクに似ており，正則化流との相性が極めて良い (Cranmer et al., 2020)．\nこの際，任意の分布 \\(p(\\theta)\\) に対して，結合分布 \\[\np(x,\\theta)=p(x|\\theta)p(\\theta)\n\\] を，シミュレータから得られるサンプルのみからフローベースモデルにより学習してしまうことで，ベイズ償却推論が可能になる (Papamakarios et al., 2019)．\nこの方法はさらに，事後分布推定に特化することで，SMC-ABC などの従来の近似ベイズ計算技法の性能を超えていくようである (Greenberg et al., 2019)．\nBayesFlow (Radev et al., 2022) は観測データからモデルパラメータへのフローを，償却推論によって学習する．一度このフローが学習されれば，新たなデータの到着に対しても極めて安価な限界費用で推論を更新できる．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#文献",
    "href": "posts/2024/Samplers/NF.html#文献",
    "title": "正規化流",
    "section": "7 文献",
    "text": "7 文献\n\nJanosh Riebesell のリポジトリ awesome-normalizing-flows に実装がまとめられている．\n(Murphy, 2023) 第23章は入門に良い．詳細はサーベイ (Kobyzev et al., 2021), (Papamakarios et al., 2021) に譲られている．\n(S. Chen and Gopinath, 2000) の時点では同様のアイデアは Gaussianization と呼ばれていた．\n(Tabak and Vanden-Eijnden, 2010), (Tabak and Turner, 2013) が (S. Chen and Gopinath, 2000) のアイデアを推し進めて，確率測度のフローによる押し出しとしての正規化流の提案と命名を行った．このフローの学習は最尤推定によって行なっていた．"
  },
  {
    "objectID": "posts/2024/Samplers/NF.html#footnotes",
    "href": "posts/2024/Samplers/NF.html#footnotes",
    "title": "正規化流",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの考え方は，VAE ではデコーダーをエンコーダーを用いていたものを，１つの可逆な NN で済ましているようにみなせる．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html",
    "href": "posts/2024/Samplers/Diffusion.html",
    "title": "拡散模型",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#sec-DDPM",
    "href": "posts/2024/Samplers/Diffusion.html#sec-DDPM",
    "title": "拡散模型",
    "section": "1 デノイジング拡散模型 (DDPM)",
    "text": "1 デノイジング拡散模型 (DDPM)\n\n1.1 はじめに\n拡散モデルによる画像生成は，初め (Sohl-Dickstein et al., 2015) で提案され，(Ho et al., 2020) で DDPM (Denoising Diffusion Probabilistic Model) として拡張された．\n\n\nDDPM では訓練データを，完全な Gauss 分布になるような変換を拡散過程によって行う．これを複数段階に分けて実行し，エンコーダー \\[\nQ(x_0,-)=\\int\\cdots\\int Q^1(x_0,dx_1)\\cdots Q^T(x_{T-1},-)=\\operatorname{N}_d(0,I_d)\n\\] を得る．\n続いて，この逆過程 \\(p_\\theta(x_t,x_{t-1})\\) を VAE 様の方法で変分推論により学習しようというのである．\nこの方法は，Langevin サンプラーをデータ分布に誘導するスコア場を，種々のノイズレベル \\(\\sigma_t&gt;0\\) で スコアマッチング により学習し，アニーリングをした Langevin サンプラーによりサンプリングをしていることに等価になる．\nこの SGM (Score-based Generative Model) としての見方（第 2 節）からは，サンプリングに特化した EBM ともみなせる．\nこの２つの見方はノイズスケジュールの違いを除いて等価な目的関数を定める (Huang et al., 2021)．また，違う SDE が定める異なる拡散課程を用いて，ノイズ分布をデータ分布に輸送しているともみれる (Y. Song, Sohl-Dickstein, et al., 2021)．現状２つの定式化は完全に等価とみなされ，単に 拡散模型 と言った場合 DDPM と SGM の双方を指す．\n拡散模型は現在，連続時間ベースのフローベースモデルとして，連続時間正規化流 (CNF) と同一の枠組みで捉えられる (Albergo et al., 2023)．\n\n\n1.2 エンコーダーの設定\n前述の通り，DDPM はエンコーダー \\(q\\) とデコーダー \\(p_\\theta\\) がそれぞれ複数層からなるような，階層的 VAE ともみなせる．\n\nただし潜在空間上の事前分布は \\(\\operatorname{N}_d(0,I_d)\\) で固定し，エンコーダー \\(q\\) は \\[\nQ(x_0,dx_T)=q(x_0,x_T)\\,dx_T=\\operatorname{N}_d(0,I_d)\n\\] を満たすように，ノイズスケジュール \\(\\{\\beta_t\\}\\) の自由度のみをハイパーパラメータとして残して \\[\nQ^t(x_{t-1},dx_t):=\\operatorname{N}_d\\left(\\sqrt{1-\\beta_t}x_{t-1},\\beta_tI_d\\right),\\qquad\\beta_t\\in(0,1),\n\\] で固定し，学習すべきパラメータは入れない．\nこのとき \\[\nQ^{1:t}(x_0,dx_t)=\\operatorname{N}_d\\left(\\sqrt{\\alpha_t}x_0,(1-\\alpha_t)I_d\\right),\\qquad\\alpha_t:=\\prod_{s=1}^t(1-\\beta_s),\n\\] であり，後述の通り，これは OU 過程をシミュレーションしていることにあたる（第 3.2 節）．\n\n\n1.3 デコーダーの設定\n核 \\(Q^t(x_{t-1},dx_t)\\) の逆 \\[\nq(x_{t-1}|x_t)\\,dx_{t-1}dx_t=Q^t(x_{t-1},dx_t)\\,dx_{t-1}\n\\] を考えたい．この際，\\(x_0\\) で条件づけると，次のような表示を得る：\n\n\n\n\n\n\n命題\n\n\n\n\\[\nq(x_{t-1}|x_t,x_0)\\,dx_{t-1}=\\operatorname{N}_d\\left(\\widetilde{\\mu}_t(x_t,x_0),\\widetilde{\\beta}_tI_d\\right).\n\\] ただし，\\(\\widetilde{\\mu}_t,\\widetilde{\\beta}_t\\) は次のように定めた： \\[\n\\widetilde{\\mu}_t(x_t,x_0):=\\frac{(1-\\alpha_{t-1})\\sqrt{1-\\beta_t}x_t+\\sqrt{\\alpha_{t-1}}\\beta_tx_0}{1-\\alpha_t},\\qquad\\widetilde{\\beta}_t:=\\frac{1-\\alpha_{t-1}}{1-\\alpha_t}\\beta_t.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\nBayes の定理から， \\[\\begin{align*}\n    q\\biggr(x_{t-1},(x_t,x_0)\\biggl)&=\\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\\\\\n    &=\\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_t|x_0)}.\n\\end{align*}\\]\nいま， \\[\nq(x_t|x_{t-1})=\\frac{1}{(2\\pi\\beta_t)^{d/2}}\\exp\\left(-\\frac{\\lvert x_t-\\sqrt{1-\\beta_t}x_{t-1}\\rvert^2}{2\\beta_t}\\right)\n\\] \\[\nq(x_{t-1}|x_0)=\\frac{1}{(2\\pi(1-\\alpha_{t-1}))^{d/2}}\\exp\\left(-\\frac{\\lvert x_{t-1}-\\sqrt{\\alpha_{t-1}}x_0\\rvert^2}{2(1-\\alpha_{t-1})}\\right)\n\\] \\[\nq(x_t|x_0)=\\frac{1}{(2\\pi(1-\\alpha_t))^{d/2}}\\exp\\left(-\\frac{\\lvert x_t-\\sqrt{\\alpha_t}x_0\\rvert^2}{2(1-\\alpha_t)}\\right)\n\\] であるから，これを代入して結論を得る．\n\n\n\nこのことに基づいて， \\[\nP^t_\\theta(x_t,dx_{t-1})=\\operatorname{N}_d\\biggr(\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t)\\biggl),\\qquad\\Sigma_\\theta(x_t,t):=\\sigma_t^2I_d,\n\\tag{1}\\] とモデリングする．\n総じて，データ分布を \\[\np_\\theta(x_0)\\,dx_0:=\\int_{\\mathbb{R}^{dT}}p(x_T)P^T_\\theta(x_T,dx_{T-1})\\cdots P^1_\\theta(x_1,dx_0)dx_T\n\\tag{2}\\] としてモデリングすることになる．ただし，\\(p(x_T)\\,dx_T=\\operatorname{N}_d(0,I_d)\\)．\n\n\n1.4 DDPM の実装\nDDPM (Ho et al., 2020) では，平均のモデリングにのみパラメータを入れて，分散は \\[\n\\Sigma_\\theta(x_t,t):=\\sigma_t^2I_d,\\qquad\\sigma^2_t\\in\\{\\beta_t,\\widetilde{\\beta}_t\\}\n\\] としてしまう．\n\\(\\beta_t\\) の選択は Taylor 展開を通じた一次近似として正当化されるが，これにもニューラルネットワークを導入してより高次な近似をし，精度を上げる方法が (Nichol and Dhariwal, 2021) で議論されている（第 4.1.2 節）．\n加えて，\\(x_t,x_0\\) で条件付けた \\(x_{t-1}\\) の平均 \\(\\widetilde{\\mu}_t(x_t,x_0)\\) は， \\[\nx_t=\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon_t\\quad\\Leftrightarrow\\quad x_0=\\frac{1}{\\sqrt{\\alpha_t}}x_t-\\frac{\\sqrt{1-\\alpha_t}}{\\sqrt{\\alpha_t}}\\epsilon_t,\\qquad\\epsilon_t\\sim\\operatorname{N}_d(0,I_d)\n\\] を通じてパラメータ変換を施スト \\[\\begin{align*}\n    \\widetilde{\\mu}_t(x_t,\\epsilon_t)&=\\frac{1}{\\sqrt{1-\\beta_t}}\\left(\\frac{1-\\alpha_{t-1}}{1-\\alpha_t}(1-\\beta_t)x_t+\\frac{\\beta_t}{1-\\alpha_t}x_t-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}}\\epsilon_t\\right)\\\\\n    &=\\frac{1}{\\sqrt{1-\\beta_t}}\\left(x_t-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}}\\epsilon_t\\right)\n\\end{align*}\\] を得る．\n従って，\\(x_t\\) から \\(x_{t-1}\\) を予測する代わりに，\\(x_t\\) から「どれくらいデノイジングするか？」を代わりに予測することもできる．すなわち， \\[\n\\widetilde{\\mu}_\\theta(x_t,t)=\\frac{1}{\\sqrt{1-\\beta_t}}\\left(x_t-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}}\\widetilde{\\epsilon}_\\theta(x_t,t)\\right)\n\\] というパラメータ変換により，\\(\\widetilde{\\mu}_t(x_t,t)\\) の代わりに \\(\\widetilde{\\epsilon}_\\theta(x_t,t)\\) を予測する．\nまた \\(x_t\\) は常に \\(d\\) 次元で一定であるため，アーキテクチャは \\(U\\)-net や ConvNet などが用いられる．\n\n\n\nDDPM の出力の例．画像をタップしてコードを見る\n\n\n\n\n1.5 訓練\nこのモデルの対数尤度 (2) は \\(\\mathbb{R}^{dT}\\) 上の積分を含む．そこで VAE 同様，最初の選択が変分推論となる．\n次のように \\(\\mathcal{L}(\\theta,q)\\) によって下から評価できる：\n\\[\\begin{align*}\n    \\log p_\\theta(x_0)&=\\log\\int_{\\mathbb{R}^{dT}}p_\\theta(x_{0:T})\\,dx_{1:T}\\\\\n    &=\\log\\left(\\int_{\\mathbb{R}^{dT}}\\frac{p_\\theta(x_{0:T})}{q(x_{1:T}|x_0)}q(x_{1:T}|x_0)\\,dx_{1:T}\\right)\\\\\n    &\\ge\\int_{\\mathbb{R}^{dT}}\\log\\left(p(x_T)\\prod_{t=1}^T\\frac{p_\\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\\right)q(x_{1:T}|x_0)\\,dx_{1:T}\\\\\n    &=:\\mathcal{L}(\\theta,q).\n\\end{align*}\\]\nただし，\\(\\mathcal{L}(\\theta,q)\\) は次のように変形できる：\n\\[\\begin{align*}\n    \\mathcal{L}(\\theta,q)&=-\\operatorname{KL}\\biggr(q(x_T|x_0),p(x_T)\\biggl)\\\\\n    &\\qquad-\\sum_{t=2}^T\\int_{\\mathbb{R}^d}\\operatorname{KL}\\biggr(q(x_{t-1}|x_t,x_0),p_\\theta(x_{t-1}|x_t)\\biggl)q(x_t|x_0)\\,dx_t\\\\\n    &\\qquad+\\int_{\\mathbb{R}^d}\\log p_\\theta(x_0|x_1)q(x_1|x_0)\\,dx_1\\\\\n    &=:\\mathcal{L}_1(q)+\\mathcal{L}_2(\\theta,q)+\\mathcal{L}_3(\\theta,q).\n\\end{align*}\\]\nこの式は VAE の変分下界 から，KL 乖離度の項が \\(T-1\\) 個増えた形になっている．\nこの変分下界 \\(\\mathcal{L}(\\theta,q)\\) の無限次元変数 \\(q\\) の方は，\\(x\\) からスタートするエンコーダー過程 1.2 の確率核に固定してしまう．\nすると，第二項 \\(\\mathcal{L}_2(\\theta)\\) に登場する密度は全て正規密度であるため，解析的に計算できる： \\[\n\\operatorname{KL}\\biggr(q(x_{t-1}|x_t,x_0),p_\\theta(x_{t-1}|x_t)\\biggl)=\\frac{\\lvert\\widetilde{\\mu}_t(x_t,x_0)-\\mu_\\theta(x_t,t)\\rvert^2}{2\\beta_t}.\n\\] 第三項 \\(\\mathcal{L}_3(\\theta)\\) は \\(q(x_1|x_0)\\,dx_1=\\operatorname{N}_d(\\sqrt{1-\\beta_1}x_0,\\beta_1I_d)\\) からのサンプルにより Monte Carlo 近似できる．\n\n\n1.6 代理目標\nDDPM (Ho et al., 2020) では，\\(\\widetilde{\\mu}_\\theta(x_t,t)\\) の代わりに，\\(\\widetilde{\\epsilon}_\\theta(x_t,t)\\) を予測するのであった（第 1.4 節）．\nこの際，第三項は次の変更を受ける： \\[\n\\operatorname{KL}\\biggr(q(x_{t-1}|x_t,x_0),p_\\theta(x_{t-1}|x_t)\\biggl)=\\frac{\\beta_t}{2(1-\\alpha_t)(1-\\beta_t)}\\biggl|\\widetilde{\\epsilon}_\\theta\\biggr(\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon_t,t\\biggl)-\\epsilon_t\\biggr|^2+\\mathrm{const.}\n\\] 第二項も全く同じ表記になり，\\(t=1\\) の場合に当たるから，総じて \\[\n\\mathcal{L}(\\theta)=-\\sum_{t=1}^T\\frac{\\beta_t}{2(1-\\alpha_t)(1-\\beta_t)}\\biggl|\\widetilde{\\epsilon}_\\theta\\biggr(\\sqrt{\\alpha_t}x_0+\\sqrt{1-\\alpha_t}\\epsilon_t,t\\biggl)-\\epsilon_t\\biggr|^2+\\mathrm{const.}\n\\]\nしかし，DDPM (Ho et al., 2020) では，正確な変分推論を実行するのではなく，この係数を全て \\(1\\) にした \\[\n\\mathcal{L}'=\\lvert\\epsilon_\\tau-\\widetilde{\\epsilon}_\\theta(X_\\tau,\\tau)\\rvert^2,\\qquad X_\\tau=\\sqrt{\\alpha_\\tau}X_0+\\sqrt{1-\\alpha_\\tau}\\epsilon_\\tau,\\tau\\sim\\mathrm{U}([T]),X_0\\sim p(x_0)\\,dx_0,\\epsilon_\\tau\\sim\\operatorname{N}_d(0,I_d),\n\\] を代理目標として用いた．ただし，\\(p(x_0)\\,dx_0\\) はデータ分布である．\nこれは純粋に，時刻 \\(t\\) までに実際に印加されるノイズ \\(\\epsilon_t\\) とモデルが予測するノイズ \\(\\widetilde{\\epsilon}_\\theta\\) の二乗差を最小化することに等しい．\nこの目的関数の変更は，\\(t\\) が小さい場合の誤差を小さく評価し，\\(t\\) が大きい場合の誤差を大きく評価するため，大きい \\(t\\) でのデノイジングというより難しいタスクを強調して学習する効果を持つという (Ho et al., 2020)．\n(Choi et al., 2022) この代理目標の議論をさらに進めている．またこのような代理目標は，データ拡張の下では真の ELBO と一致する (Kingma and Gao, 2023) ため，全く無根拠な変更というわけではない．\n\n\n1.7 最尤推定\nVDM (Variational Diffusion Model) (Kingma et al., 2021) では正確に \\(\\mathcal{L}(\\theta)\\) を最小化することで最尤推定を実行する．\nこの方法では DDPM では固定されていたノイズスケジュール \\(\\widetilde{\\beta}_t\\) の真の変分推論が実行できる．\n\\[\nQ^{1:t}(x_0,dx_t)=\\operatorname{N}_d\\biggr(\\sqrt{\\alpha_t}x_0,(1-\\alpha_t)I_d\\biggl)\n\\] の変数 \\(\\alpha_t\\) をパラメータ変換した SNR (Signal-to-Noise Ratio) \\[\nR(t)=\\frac{\\alpha_t}{1-\\alpha_t}=:e^{-\\gamma_\\phi(t)}\n\\] に注目し，\\(\\gamma_\\phi\\) をニューラルネットワークでモデリングする．\nこの \\(R(t)\\) を用いれば，正確な目的関数 \\(\\mathcal{L}(\\theta)\\) はより簡単な表示を持つことを発見した (Kingma et al., 2021) は，さらにこれを訓練する際に QMC によるより分散の小さい Monte Carlo 推定量を用いることで，DDPM と同じくらい効率的に最尤推定が実行できることを示した．\n(Kingma et al., 2021), (Huang et al., 2021) は無限層の VAE という DDPM の観点からの貢献であるが，SDE の観点からの最尤推定法を (Y. Song, Durkan, et al., 2021), (Vahdat et al., 2021) が提案している（第 2.6 節）．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#sec-SGM",
    "href": "posts/2024/Samplers/Diffusion.html#sec-SGM",
    "title": "拡散模型",
    "section": "2 スコアベースの生成模型 (SGM)",
    "text": "2 スコアベースの生成模型 (SGM)\n\n2.1 はじめに\nスコアマッチングはもともと，エネルギーベースモデル (EBM) の学習手法として (Hyvärinen, 2005) により提案された．\nそもそも EBM も優秀な生成モデルとして知られている (Du and Mordatch, 2019), (Gao et al., 2020) が，スコアマッチングによるスコアの学習はもっと直接的に生成モデルを定める：\n\n\n\n\n\n\nスコア（または Stein (Liu et al., 2016) / (Hyvärinen, 2005) スコア）と呼ばれるベクトル場 \\[\n\\nabla_x\\log p(x)\n\\] 自体を学習できれば，これが駆動する OU 過程を用いて \\(p(x)\\) からサンプリングを行うことが出来る．\n\n\n\nこれが SGM (Score-based Generative Model) (Y. Song and Ermon, 2019) のアイデアであった．1\nこのアイデアは既存の DSM (Denoising Score Matching) の改良とも見れる．\nDSM はデータに印加するノイズ \\(\\sigma&gt;0\\) が強いほど推定が安定するが，推定対象であるノイズが印加されたスコアは元のデータ分布から乖離してしまうというトレードオフがあった．これをアニーリングによって解決する方法とも見れる．\n\n\n2.2 SGM の課題\nしかしこのスキームの課題は，\\(\\nabla_x\\log p(x)\\) の値は \\(p\\) を一意的に定めないということである．\n例えば， \\[\np=\\pi p_0+(1-\\pi)p_1,\\qquad\\pi\\in(0,1)\n\\] という関係があり，\\(p_0,p_1\\) の台が互いに素であったとき，\\(\\nabla_x\\log p\\) からは \\(\\pi\\in(0,1)\\) を定めるための情報が完全に消えてしまう．\nすなわち，データが下図のように部分多様体上にのみ台を持つ場合は，モデルが識別可能でない．\n\n\n\nスコアマッチングの例：データが青，学習されたスコア場が緑．\n\n\nこれにより間違ったスコアが学習され，多峰性を持ってしまった場合，OU 過程によるサンプリングが失敗する (Y. Song and Ermon, 2019)．\n従って，各ノイズレベル \\(\\sigma\\) に対応したスコア場を学習し，Langevin サンプラーをノイズが大きく分布が単純な領域からスタートさせ，徐々にデータ分布に近づけて収束を促す（アニーリング）．\nすると SGM は，最初の発想は全く違えど，最終的に得られるモデルは，DDPM と等価になる．\n\n\n2.3 デノイジングによる解決\nSGM (Y. Song and Ermon, 2019), (Y. Song and Ermon, 2020) ではデータにさまざまな強度 \\(\\sigma\\) の Gauss ノイズを印加したもののスコア場を，スコアマッチングで学習することを考える．\nノイズレベル \\(\\sigma\\) も入力に含めることで，単一のニューラルネットワーク \\(s_\\theta(x,\\sigma)\\) で学習をする．このアーキテクチャを NCSN (Noise Conditional Score Network) (Y. Song and Ermon, 2019) または単に スコアネットワーク (Y. Song and Ermon, 2020) という．\nノイズを印加することは，Gauss 核との畳み込みにより分布を軟化していくことに相当し，アニーリングと同じ効果を持つ．\nその結果，\\(\\sigma\\) が十分大きい際は多峰性が消失してスコアマッチングが正確になる．加えて MCMC によるサンプリングも容易になる．\nはじめ，OU 過程を十分に大きい \\(\\sigma&gt;0\\) に対応するスコアで駆動させ，徐々に \\(\\sigma\\to0\\) としていくことで，データ分布からサンプリングすることを目指す．2\n\n\n2.4 目的関数\n\nスコアのスケール \\(\\sigma\\) の Gauss ノイズにより軟化したデータのスコア場を \\(s_\\theta(x,\\sigma)\\) と表し，デノイジングスコアマッチング (DNS) の目的関数の列を考える：3 \\[\n\\mathcal{L}(\\theta;\\sigma)=\\frac{1}{2}\\operatorname{E}\\left[\\left\\|s_\\theta(\\widetilde{X},\\sigma)+\\frac{\\widetilde{X}-X}{\\sigma^2}\\right\\|^2_2\\right].\n\\]\nこの DNS 目的関数の，ある等比数列 \\(\\sigma_1&gt;\\cdots&gt;\\sigma_T&gt;0\\) に関する線型和 \\[\n\\mathcal{L}(\\theta;\\sigma_{1:T})=\\sum_{t=1}^T\\lambda_t\\mathcal{L}(\\theta;\\sigma_t),\\qquad\\lambda_t&gt;0,\n\\tag{3}\\] を最終的な目的関数とする．\n数列 \\((\\sigma_i)\\) の設定の仕方，\\(\\sigma_1\\) をどこまで大きくすれば良いか，などは (Y. Song and Ermon, 2020) の理論解析を参照．ステップ数 \\(T\\) は大きければ大きいほどよい．\n\n\n2.5 DDPM との対応\nこの SGM の目的関数 (3) は，ノイズスケジュール \\(\\lambda_t\\) が異なるのみで DDPM 1 と同じ目的関数となっている (Ho et al., 2020), (Huang et al., 2021)．\n実際， \\[\n\\mathcal{L}(\\theta;\\sigma_t^2)=\\operatorname{E}\\left[\\frac{\\lambda_\\tau}{\\sigma_\\tau^2}\\lvert\\epsilon_\\tau-\\widetilde{\\epsilon}_\\theta(X_\\tau,\\tau)\\rvert^2\\right]\n\\] と表せるから，ハイパーパラメータ \\(\\lambda_t\\) を \\(\\lambda_t=\\sigma_t^2\\) と取ることで，DDPM の代理目的関数 1.6 に一致する．\n\n\n2.6 最尤推定\n近似が入っている DDPM の訓練目標と対応していることから判るように，スコアマッチングは正確な最尤推定を実行していない．\nかといって，正確な変分推論 1.7 を実行するのは高くつく．\n(Y. Song, Durkan, et al., 2021) はスコアマッチングと同じくらい効率的な最尤推定法が，目的関数 (3) の重みづけ \\((\\lambda_t)\\) を特定の値 likelihood weighting にとることで得られることを示している．4\n\n\n\n(Y. Song, Durkan, et al., 2021, p. 5)"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#連続時間極限",
    "href": "posts/2024/Samplers/Diffusion.html#連続時間極限",
    "title": "拡散模型",
    "section": "3 連続時間極限",
    "text": "3 連続時間極限\n\n3.1 はじめに\nDDPM 1 と SGM 2 は，いずれも OU 過程とその時間反転の離散化として統一的に理解できる．\nこの連続時間極限に関する知見が，モデルのデザインに関して示唆を与えてくれる (Tzen and Raginsky, 2019), (Y. Song, Sohl-Dickstein, et al., 2021)．\nタイムステップ \\(t\\) の取り方は適応的にできるし，5 SDE (4) と等価な輸送を定める ODE を考え，サンプリングを加速することも考えられる．6\n\n\n3.2 前向き拡散過程\nデータ分布を正規分布に還元する際に DDPM 1 で用いた拡散過程は，パラメータ \\(\\beta(t)\\) を持った \\(0\\) に回帰的な OU 過程である (Y. Song, Sohl-Dickstein, et al., 2021, p. 5)： \\[\ndX_t=-\\frac{\\beta(t)}{2}X_tdt+\\sqrt{\\beta(t)}dB_t,\\qquad\\beta\\left(\\frac{t}{T}\\right)=T\\beta_t.\n\\tag{4}\\]\nこれを (Y. Song, Sohl-Dickstein, et al., 2021) では 分散保存過程 (variance-preserving process) と呼んでいる．\n一方で，SGM 2 では \\[\ndX_t=\\sqrt{\\frac{d }{d t}\\sigma(t)^2}dB_t\n\\] で定まる拡散過程を用いる．これを (Y. Song, Sohl-Dickstein, et al., 2021) では 分散爆発過程 (variance-exploding process) と呼んでいる．\n\n\n3.3 後ろ向き拡散\n(Anderson, 1982) によると，一般に \\[\ndX_t=f_t(X_t)\\,dt+\\sigma_t\\,dB_t\n\\] という SDE の時間反転 は， \\[\ndY_t=\\biggr(f_t(Y_t)-\\sigma_t^2\\nabla_x\\log q_t(Y_t)\\biggl)\\,dt+\\sigma_t\\,dB_{-t}\n\\] が定める．ここにスコア場 \\(\\nabla_x\\log q_t\\) が出てくるのである．\n特に，OU 過程 (4) の時間反転は \\[\ndY_t=\\biggr(-\\frac{\\beta_t}{2}Y_t-\\beta_t\\nabla_{x}\\log q_t(Y_t)\\biggl)\\,dt+\\sqrt{\\beta_t}\\,dB_{-t}\n\\] と表せる．\nこのスコア関数 \\(\\nabla_x\\log q_t(X_t)\\) を DSM によって推定した \\[\ndY_t=-\\frac{\\beta_t}{2}\\biggr(Y_t+2s_\\theta(Y_t,t)\\biggl)\\,dt+\\sqrt{\\beta_t}\\,dB_{-t}\n\\] でデータ分布からサンプリングすることができるのである．なお，拡散過程のサンプリングは難しい問題であり，最も直接的には Euler-Maruyama 離散化を通じれば良い．\n\n\n3.4 等価な輸送を定める確定的ダイナミクス\n代わりに ODE \\[\n\\frac{d x_t}{d t}=f(x,t)-\\frac{g(t)^2}{2}\\nabla_x\\log p_t(x)\n\\] で定まる確定的ダイナミクス \\(\\{x_t\\}\\) を用いても，同様にデータ分布は \\(\\operatorname{N}_d(0,I_d)\\) に還元される．\nこれを (Y. Song, Sohl-Dickstein, et al., 2021 Sec D.3) は probability flow ODE と呼ぶ．\n\n\n3.5 後ろ向き ODE\nこの ODE を推定した \\[\n\\frac{d y_t}{d t}=f(y,t)-\\frac{g(t)^2}{2}s_\\theta(y_t,t)\n\\] を Euler 法，またはより高次な Heun 法などによって逆から解くことができる (Karras et al., 2022)．\nこれは連続時間正規化流，特に Neural ODE と等価なモデリングをすることになる．\n実は SDE は \\[\ndY_t=\\biggr(f(y,t)-\\frac{g(t)^2}{2}s_\\theta(y_t,t)\\biggl)-\\frac{\\beta_t}{2}s_\\theta(y_t,t)dt+\\sqrt{\\beta_t}\\,dB_t\n\\] と，上記の確率的フロー ODE に OU 過程の項を加えた形になっており，ODE によるアプローチはこの追加の OU 過程項を消去することに等しい．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#応用",
    "href": "posts/2024/Samplers/Diffusion.html#応用",
    "title": "拡散模型",
    "section": "4 応用",
    "text": "4 応用\n\n4.1 サンプリング加速法\nODE を用いることで拡散模型のサンプリング速度が向上する．(Nichol and Dhariwal, 2021) も参照．\n\n4.1.1 Denoising Diffusion Implicit Model (DDIM) (J. Song et al., 2021)\nDDIM (J. Song et al., 2021) は，スタート地点 \\(x_0\\) に条件付けられた確率核 \\[\nq_\\sigma(x_{t-1}|x_t,x_0)\\,dx_{t-1}=\\operatorname{N}\\biggr(\\sqrt{\\alpha_{t-1}}x_0+\\sqrt{1-\\alpha_{t-1}-\\sigma_t^2}\\frac{x_t-\\sqrt{\\alpha_t}x_0}{\\sqrt{1-\\alpha_t}},\\sigma_t^2I_d\\biggl)\n\\tag{5}\\] を考える．7 これを \\(x_0\\) に関して積分すると元の確率核 \\(q_\\sigma(x_{t-1}|x_t)\\) を得る．\n\\(\\sigma_t\\) は任意としており， \\[\n\\sigma_t=\\sqrt{\\frac{1-\\alpha_{t-1}}{1-\\alpha_t}}\\sqrt{1-\\frac{\\alpha_t}{\\alpha_{t-1}}}\n\\] と取った場合，\\(q_\\sigma(x_{t-1}|x_t,x_0)\\) は \\(x_0\\) にもはや依らず，DDPM の設定を回復する．\n\\(q_\\sigma(x_{t-1}|x_t,x_0)\\) の時間反転は次のように与えられる： \\[\np_\\theta(x_{t-1}|x_t)\\,dx_{t-1}=\\operatorname{N}\\biggr(\\sqrt{\\alpha_{t-1}}\\widehat{x}_0+\\sqrt{1-\\alpha_{t-1}-\\sigma_t^2}\\frac{x_t-\\sqrt{\\alpha_t}\\widehat{x}_0}{\\sqrt{1-\\alpha_t}},\\sigma_t^2I_d\\biggl).\n\\]\nただし，\\(x_0\\) は事前にはわからないから，これをニューラルネットワークによりモデリングするとする： \\[\n\\widehat{x}_0=\\frac{x_t-\\sqrt{1-\\alpha_t}\\epsilon_\\theta(x_t,t)}{\\sqrt{\\alpha_t}}.\n\\]\nすると，この際の目的関数は DDPM と全く変わらない．すなわち，既存の DDPM の訓練は特別な \\(\\sigma_t\\) に関する DDIM (5) の訓練とみなせるのである．\n\\(\\sigma_t\\) が自由となると，\\(\\sigma_t\\equiv0\\) と取っても良いはずである．この場合，サンプリングの過程は確定的になる．\n総じて，すでに訓練された DDPM の \\(\\epsilon_\\theta(x_t,t)\\) があれば，この \\(\\sigma_t\\equiv0\\) の場合のサンプリング過程を採用すれば，DDPM 1 より 10 倍から 50 倍速いサンプリングが実現できる．\n\n\n4.1.2 逐次サンプリング問題としての解決\nデコーダーに用いるモデル (2) の表現力を上げることで，ステップサイズを大きくしてもサンプリングの性能を悪化させないようにできる．\n(Nichol and Dhariwal, 2021) は \\(\\Sigma_\\theta(x_t,t)\\) もニューラルネットワークで学習することで，サンプリングのステップサイズを大きくしてもサンプルの質を保つことができると報告している．\n(Kingma et al., 2021) も，ノイズスケジュールも学習しようとしている点で似ている．\n(Gao et al., 2021) は，ノイズ分布からデータ分布までのアニーリング列を，EBM の列として捉えてフィッティングをしている．同様のスキームで，(Xiao et al., 2022) は GAN を用いている．\n\n\n4.1.3 蒸留\n(Salimans and Ho, 2022) は Progressive Distillation と呼ばれる拡散過程の蒸留手法を提案している．\n学習済みの拡散モデル（(Salimans and Ho, 2022) では DDIM）から，徐々にステップ数を減らした蒸留モデルを作成していく．\n\n\n4.1.4 潜在拡散模型 (LDM) (Rombach et al., 2022)\nまず画像データを VAE などで学習した潜在空間に変換し，その上で拡散模型でモデリングをする．\nLatent Score-based Generative Model (LSGM) (Vahdat et al., 2021) では，VAE と拡散模型を同時に訓練することを考えている．\nこの手法は，潜在表現さえ適切に見つければ，複数のドメインのデータを同時に扱えるという美点がある．\nまた，(Pandey et al., 2022) では，まず VAE による生成モデルと作成し，その精度が足りない最終的な出力を高画質にするステップにのみ拡散模型を用いるスキームを提案している．\n\n\n4.1.5 SDE から ODE へ (DEIS)\nDEIS (Diffusion Exponential Integrator Sampler) (Zhang and Chen, 2023) では，付随する ODE 3.5 を Euler 法ではなく指数積分法で解くことで，離散化誤差を低減しつつサンプリングを加速する．\n\n\n\n4.2 拡散モデルの例\n\n4.2.1 ADM (Dhariwal and Nichol, 2021)\nADM (Ablated Diffusion Model) (Dhariwal and Nichol, 2021) と 分類器による誘導 は ImageNet データの判別において当時の最先端であった BigGAN (Brock et al., 2019) の性能を凌駕した．\nそのアーキテクチャには U-Net (Ronneberger et al., 2015) が用いられた．\n\n\n4.2.2 GLIDE (Nichol et al., 2022)\nOpenAI の GLIDE (Guided Language to Image Diffusion for Generation and Editing) (Nichol et al., 2022) は，CLIP (Contrastive Language-Image Pre-training) というトランスフォーマーベースの対照学習による画像テキスト同時表現学習器と組み合わされた，テキスト誘導付き拡散モデルである．\n\n\n4.2.3 Imagen (Saharia, Chan, Saxena, et al., 2022)\nGoogle も Imagen (Saharia, Chan, Saxena, et al., 2022) という Cascaded Generation (Ho et al., 2022) に基づいた誘導付き拡散モデルを開発している．\nT5-XXL (Raffel et al., 2020) に基づく言語モデルを通じて言語と画像を同等の潜在空間にのせ，U-Net アーキテクチャを持った VDM 1.7 でモデリングすることで，高精度な text-to-image を実現している．\nPalette (Saharia, Chan, Chang, et al., 2022) は同様の仕組みで image-to-image を実現している．\n\n\n\n4.2.4 潜在拡散模型\nVAE や GAN と違い，１つのニューラルネットワークしか用いないため，学習が安定しやすい．\n一方で，生成時には逆変換を何度も繰り返す必要があるため，計算量が大きい．これを回避するために，生成を VAE 内の潜在空間で行うものを 潜在拡散モデル (latent diffusion model) (Rombach et al., 2022) という．これが Stable Diffusion の元となっている．\n\n\n4.2.5 トランスフォーマーとの邂逅\n並列化が容易であり，スケーラブルな手法であるため，トランスフォーマーと組み合わせて画像と動画の生成に使われる．\n潜在拡散モデルで U-Net (Ronneberger et al., 2015) を用いていたところをトランスフォーマーに置換した 拡散トランスフォーマー (DiT: Diffusion Transformer) (Peebles and Xie, 2023) が発表された．\nその後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) (Ma et al., 2024) が発表された．\n\n\n4.2.6 Discrete Denoising Diffusion Probabilistic Models (D3PM) (Austin et al., 2021)\nImagen にように言語を連続な潜在空間に埋め込む他に，直接離散空間上にも拡散模型を用いる事ができる．\n実はこのように設計された拡散模型は，BERT (Lewis et al., 2020) などのマスク付き言語モデルと等価になる．\nMaskGIT (Masked Generative Image Transformer) (Chang et al., 2022) はこの枠組みに，画像をベクトル量子化して載せる．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#参考文献",
    "href": "posts/2024/Samplers/Diffusion.html#参考文献",
    "title": "拡散模型",
    "section": "5 参考文献",
    "text": "5 参考文献\n\nAwesome-Diffusion-Models，What’s the score?．\n良いサーベイには次がある：(Luo, 2022)，(McAllester, 2023)．古くからあり，すでに出版されているものには，(Yang et al., 2023), (Cao et al., 2024)．\nCVPR のチュートリアルが (Kreis et al., 2022), (J. Song et al., 2023)．Web サイトには (Dieleman, 2023)，(Yuan, 2024)，(Duan, 2023)，(Das, 2024)．\nまた，入門的な内容が (Nakkiran et al., 2024) で扱われている．\n拡散モデルのサンプリングを加速する手法に関する論文には (Nichol and Dhariwal, 2021), (Croitoru et al., 2023) など．"
  },
  {
    "objectID": "posts/2024/Samplers/Diffusion.html#footnotes",
    "href": "posts/2024/Samplers/Diffusion.html#footnotes",
    "title": "拡散模型",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nエネルギー関数とスコア関数，どっちを学習の中心に据えるかについては (Salimans and Ho, 2021) も参照．↩︎\n“Our sampling strategy is inspired by simulated annealing (Kirkpartick et al., 1983), (Neal, 2001) which heuristically improves optimization for multimodal landscapes.” (Y. Song and Ermon, 2019, p. 2) より．↩︎\nDNS も SSM も使用可能であるが，ここでは Langevin サンプラーのアニーリングのために，ノイズが印加されたベクトル場 \\(s_\\theta(x,\\sigma)\\) の学習が必要であるため，DNS の選択が自然である (Y. Song and Ermon, 2019)．↩︎\nこれが可能であることは (Ho et al., 2020, p. 8) でも触れられている．↩︎\nちょうど，Neural ODE がタイムステップの取り方を適応的にしてくれたように．↩︎\nちょうど，MCMC がベクトル場に関する決定論的なフローで効率化させられるように．↩︎\n(J. Song et al., 2021) 内では \\(q_\\sigma(x_t|x_{t-1},x_0)\\) の形を与えていないが，Bayes の定理を通じてわかる：\\[q_\\sigma(x_t|x_{t-1},x_0)=\\frac{q_\\sigma(x_{t-1}|x_t,x_0)q_\\sigma(x_t|x_0)}{q_\\sigma(x_{t-1}|x_0)}.\\]↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/NF3.html",
    "href": "posts/2024/Samplers/NF3.html",
    "title": "フローベース模型による条件付き生成",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF3.html#誘導",
    "href": "posts/2024/Samplers/NF3.html#誘導",
    "title": "フローベース模型による条件付き生成",
    "section": "1 誘導",
    "text": "1 誘導\n拡散模型の美点には，条件付けが可能で拡張性に優れているという点もある．\n実際，拡散模型の出現後，Conditional VAE (Kingma et al., 2014) などの従来手法を凌駕する条件付き生成が可能であることが直ちに理解された．\n\\(C\\) がクラスラベルなどの離散変数である場合，「誘導」による条件付き生成が初めに考えられた．\n\n1.1 はじめに\n「誘導」ではまず，DDPM (Ho et al., 2020) でタイムステップ \\(t\\) を positional encoding したようにして，プロンプト \\(c\\) をデータに埋め込む．1\nそしてデータ \\(X\\) とそのラベル \\(C\\) に対して，条件付き分布 \\(\\mathcal{L}[X|C]\\) をモデリングする．\nしかしこのアプローチの問題は，ラベル \\(C\\) が不確実な場合などは，この情報を無視して普通の \\(X\\) が生成されてしまいがちであることである．\nそこで目的関数に，条件付き分布 \\(X|C\\) の正確性を期すような追加のデザインをする．これが「誘導」である．\n\n\n1.2 条件付きスコア場\n条件付き分布 \\(p(x|c)\\) を学習することを考える．\nこのとき \\(p(x|c)\\) のスコアは，Bayes の定理から次のように表せる： \\[\n\\log p(x|c)=\\log p(c|x)+\\log p(x)-\\log p(c),\n\\] \\[\n\\therefore\\qquad\\nabla_x\\log p(x|c)=\\nabla_x\\log p(x)+\\nabla_x\\log p(c|x).\n\\tag{1}\\]\nすなわち，条件付き確率 \\(p(x|c)\\) のスコア場は，条件なしのスコア場 \\(\\nabla_x\\log p(x)\\) と，分類器のスコア場 \\(\\nabla_x\\log p(c|x)\\) の重ね合わせになる．\n\n\n1.3 分類器による誘導 (CG)\n式 (1) から，\\(\\nabla_x\\log p(x|c)\\) が計算できる分類器 \\(p(c|x)\\) を新たに訓練すれば，既存のモデル \\(\\nabla_x\\log p(x)\\) から，サンプリング方法を変えるだけで条件付き生成ができる．\nこれを CG: Classifier Guidance (Dhariwal and Nichol, 2021) といい，サンプリング中に各ステップで少しずつ \\(x_t\\) が \\(p(x_t|c)\\) に近づくように「誘導」されていく．\nさらに，\\(c\\) が無視されがちな場合も見越して，誘導スケール (guidance scale) という新たなハイパーパラメータ \\(\\lambda\\ge0\\) を導入し，次のスコア \\[\n\\nabla_x\\log p(x)+\\lambda\\nabla_x\\log p(c|x).\n\\tag{2}\\] からサンプリングすることも考えられる．\n\\(\\lambda&gt;1\\) としどんどん大きくしていくと，クラスラベル \\(c\\) に「典型的な」サンプルが生成される傾向にある．\n\n\n1.4 分類器なしの誘導\nCG はいわばアドホックな方法であり，外部の分類器 \\(p(c|x)\\) に頼らない方法を考えたい．\nそのためには，式 (2) から \\(p(c|x)\\) を消去して \\[\n\\lambda\\nabla_x\\log p(x|c)+(1-\\lambda)\\nabla_x\\log p(x)\n\\tag{3}\\] とみて，\\(p(x|c),p(x)\\) のいずれもデータから学ぶ．\nこのアプローチを Classifier-Free Diffusion Guidance (Ho and Salimans, 2021) という．\nその際は，新たなクラスラベル \\(\\emptyset\\) を導入して \\[\np(x)=p(x|\\emptyset)\n\\] とみなすことで，\\(p(x|c),p(x)\\) を同一の スコアネットワーク でモデリングする．\nデータセット内にランダムに1から2割の画像をクラスラベル \\(\\emptyset\\) と設定することで，これを実現する．\n同様の方法を，スコアマッチングではなくフローマッチングを行うことを (Dao et al., 2023), (Q. Zheng et al., 2023) が提案している．\nこの方法は，追加の分類器の訓練が必要ないだけでなく，サンプリングのクオリティも向上する (Nichol et al., 2022), (Saharia, Chan, Chang, et al., 2022)．これは分類タスクで訓練されたスコア \\(\\log p(c|x)\\) はどう訓練してもスコアネットワークで学習したスコア (3) に匹敵する「良い」勾配が得られないためである．\n\n\n1.5 高解像度画像生成への応用\n\n1.5.1 Cascaded Generation\n条件付き生成の技術はそのままで，最終的なクオリティを向上させるためには，Cascading (Ho et al., 2022) が使用可能である．\nこれは，画像生成は \\(x\\) の解像度が低い状態で行い，この低解像度画像を次の条件付き拡散モデルの条件付け \\(c\\) として，条件付き生成を 高解像度化 (super-resolution) に用いるものである (Saharia et al., 2023)．\nこの方法の美点は，条件付き生成器をたくさんスタックしたのちに，拡散模型間の段階でも Gauss ノイズや blur を印加することで，さらに最終的なクオリティが上げられるという (Ho et al., 2022)．これを conditioning augmentation と呼んでいる．\nこの方法は最初から高解像度での生成を目指して大規模な単一の拡散模型を設計するよりも大きく計算コストを削減できる．\nGoogle も Imagen (Saharia, Chan, Saxena, et al., 2022) でこのアーキテクチャを用いている．\n\n\n1.5.2 Self-Conditioning (T. Chen et al., 2023)\n拡散モデルを自己再帰的に用い，自身の前回の出力を今回の入力として逐次的にサンプリングを繰り返すことで，サンプリングのクオリティをさらに向上する自己条件づけが (T. Chen et al., 2023) で提案された．\nこの方法は RoseTTAFold Diffusion (Watson et al., 2023) によるたんぱく質構造生成でも用いられている：\n\n\n\nRFdiffusion generating a novel protein that binds to the insulin receptor. Taken from Baker Lab HP\n\n\n\n\n\n1.6 逆問題への応用\n一方で単一の \\(Y=y\\) を想定した状況では，非償却的な方法を採用することでさらに精度を上げることが考えられる．\n\\(\\log p_t(x_t|y)\\) を一緒くたに \\(s^\\theta_{t}(x_t,y)\\) に取り替えてしまうのではなく，まず第一項 \\(\\nabla_x\\log p_t(x_t|y)\\) を \\(s_t^\\theta(x_t)\\) により統一的にモデリングする．\nそして \\(\\nabla_x\\log p_t(y|x_t)\\) の項は Tweedie の推定量 \\[\n\\widehat{x}_0:=\\operatorname{E}[x_0|x_t]=\\frac{1}{\\sqrt{\\overline{\\alpha}_t}}\\biggr(x_t+(1-\\overline{\\alpha}_t)\\nabla_{x_t}\\log p_t(x_t)\\biggl)\n\\tag{4}\\] を通じて \\[\np(y|x_t)\\approx p(y|\\widehat{x}_0)\n\\] によって近似する．式 (4) の \\(\\nabla_{x_t}\\log p_t(x_t)\\) に事前に訓練したスコアネットワーク \\(s_t^\\theta(x_t)\\) を用いる．\n(Chung et al., 2023) はこの方法を Computer Vision における非線型逆問題に適用している．\n(Song et al., 2023) では Monte Carlo 法が用いられている．\n拡散模型の一般の事後分布サンプリングのための応用については次稿も参照：\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Samplers/NF3.html#sec-2",
    "href": "posts/2024/Samplers/NF3.html#sec-2",
    "title": "フローベース模型による条件付き生成",
    "section": "2 フローマッチングによる連続な条件付け",
    "text": "2 フローマッチングによる連続な条件付け\n\n2.1 連続な条件付き生成\n連続な変数に対する条件付き確率からの生成は CcGAN (Ding et al., 2021) などでも試みられていた．\nAlphaFold 3 (Abramson et al., 2024) や RoseTTAFold Diffusion (Watson et al., 2023), (Krishna et al., 2024) など，たんぱく質構造生成模型において拡散モデルが用いられている理由も，高精度な条件付き生成が可能であることが大きいという．\nこのことに加えて連続な変数に対する条件付けを可能にすることは，拡散モデルの拡張性をさらに高めることになる．\nそもそも拡散モデルは 連続時間正規化流 (CNF) と合流し，フローマッチング（第 2.2 節）によりノイズ分布 \\(P_0\\) をデータ分布 \\(P_1\\) に変換する曲線 \\(\\{P_t\\}_{t\\in[0,1]}\\subset\\mathcal{P}(\\mathbb{R}^d)\\) を直接学習するように発展した．\nこの方法では，新たな条件付け変数 \\(c\\in[0,1]^k\\) に対して，連続写像 \\[\nP_{t,c}:[0,1]\\times[0,1]^k\\to\\mathcal{P}(\\mathbb{R}^d)\n\\] を学習するようにフローマッチングを拡張できれば，連続な条件付き生成が可能になることになる．\nこれを行列値ベクトル場の理論を通じて達成するのが 拡張フローマッチング (EFM: Extended Flow Matching) (Isobe et al., 2024) である．\nこのようなフローマッチングの拡張は (R. T. Q. Chen and Lipman, 2024) でも考えられている．\n\n\n2.2 フローマッチング (FM)\n２つの確率分布 \\(P_0,P_1\\in\\mathcal{P}(\\mathbb{R}^d)\\) を結ぶ曲線を \\[\n(P_t)=((\\phi_t)_*P_0)_{t\\in[0,1]}\\in\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\n\\] の形で学習することを考える．\nそのための１つのアプローチとして，連続方程式 というPDE \\[\n\\frac{\\partial p_t}{\\partial t}+\\operatorname{div}(F_tp_t)=0.\n\\tag{5}\\] を満たすベクトル場 \\(F_t\\) を学習し，これが定めるフローを \\((\\phi_t)\\) とすることがある：\n\\[\n\\frac{\\partial \\phi_t(x)}{\\partial t}=F_t(\\phi_t(x)).\n\\]\nこのような \\(F_t\\) が１つ既知であり，\\(p_t\\) から自由にサンプリングできる場合は， \\[\n\\mathcal{L}_{\\mathrm{FM}}(\\theta)=\\operatorname{E}\\biggl[\\biggl|F_\\theta(X_T,T)-F_T(X_T)\\biggr|^2\\biggr],\\qquad T\\sim\\mathrm{U}([0,1]),X_T\\sim p_T,\n\\tag{6}\\] の最小化によってベクトル場 \\(F_t\\) が学習できる．これを フローマッチング (FM: Flow Matching) の目的関数という．\n\n\n2.3 条件付きフローマッチング (CFM)\n仮に \\(p_t\\) が \\[\np_t(x)=\\int_\\Omega p_t(x|c)q(c)\\,dc,\\qquad\\Omega\\subset\\mathbb{R}^k,\n\\] という \\(p_{t,c}(x):=p_t(x|c)\\) の \\(q\\)-混合としての展開を通じて得られているとする．\nこの場合，\\((p_{t,c})\\) を生成するベクトル場 \\(F_t(x|c)\\) が特定できれば， \\[\nF_t(x):=\\operatorname{E}\\left[\\frac{F_t(x|U)p_t(x|U)}{p_t(x)}\\right]\n\\tag{7}\\] が \\((p_t)\\) を生成する (定理1 Lipman et al., 2023), (定理3.1 Tong et al., 2024)．\n従って，\\(F_t\\) を学習するには FM 目的関数 (6) の代わりに \\[\n\\mathcal{L}_{\\mathrm{CFM}}(\\theta)=\\operatorname{E}\\biggl[\\biggl|F_\\theta(X_T,T)-F_T(X|C)\\biggr|^2\\biggr],\\qquad C\\sim q,\n\\tag{8}\\] の最小化によっても \\(F_t(x|c)\\) が学習できる．これを 条件付きフローマッチング (CFM: Conditional Flow Matching) の目的関数という．\n\n\n\n\n\n\n\\(P_0\\) が Gauss 分布である場合 (Lipman et al., 2023)\n\n\n\n\n\n\\(P_0=\\operatorname{N}_d(0,I_d)\\) をノイズ分布，\\(P_1\\) を一般のデータ分布とする．\nただし，誤差を許して，\\(P_1*\\operatorname{N}_d(0,\\sigma^2I_d)\\) を改めて真のデータ分布とする．\nこのように定式化することで，各データ点 \\(c\\in\\{x_i\\}_{i=1}^n\\) で条件づければ， \\[\nP_{0,c}:=P_0(-|c)=\\operatorname{N}_d(0,I_d),\\qquad P_{1,c}:=P_1(-|c)=\\operatorname{N}_d(0,\\sigma^2I_d),\n\\] の間を結ぶ曲線 \\((P_{t,c})_{t\\in[0,1],c\\in\\{x_i\\}_{i=1}^n}\\) を学習する問題となる．\n実は \\(P_{0,c},P_{1,c}\\) が Gauss 分布であることにより，この問題はすでに (McCann, 1997, p. 159) によって解かれており，最適輸送は \\[\nP_{t,c}=\\operatorname{N}_d\\biggr(tc,(t\\sigma-t+1)^2I_d\\biggl),\\qquad F_t(x|c)=\\frac{c-(1-\\sigma)x}{1-(1-\\sigma)t},\n\\] によって与えられる．\n\n\n\nしかし，各 \\((P_{t,c})_{t\\in[0,1]}\\) が最適輸送になっていても，式 (7) で定まる \\((P_t)_{t\\in[0,1]}\\) が最適輸送になるとは限らない．\n\n\n2.4 最適輸送 CFM (OT-CFM)\nここで形式的に，条件付ける変数 \\(c\\) は カップリング \\(\\pi\\in C(P_0,P_1)\\) に従う \\(C\\sim\\pi\\) とする： \\[\nC(P_0,P_1):=\\left\\{\\pi\\in\\mathcal{P}(\\mathbb{R}^d\\times\\mathbb{R}^d)\\:\\middle|\\:\\begin{array}{l}(\\mathrm{pr}_1)_*\\pi=P_0,\\\\(\\mathrm{pr}_2)_*\\pi=P_1\\end{array}\\right\\}.\n\\]\n\n\n\n\n\n\n\\(P_0\\) も一般の分布である場合 (I-CFM Tong et al., 2024)\n\n\n\n\n\n\\(Q_0,Q_1\\) は未知のデータ分布で， \\[\nP_1=Q_1*\\operatorname{N}_d(0,\\sigma^2I_d),\\qquad P_0=Q_0*\\operatorname{N}_d(0,\\sigma^2I_d),\n\\] の間を架橋したいとする．このとき， \\[\n\\pi:=Q_0\\otimes Q_1\n\\] と定めると， \\[\nP_{t,c}=\\operatorname{N}_d\\biggr(tc_1+(1-t)c_0,\\sigma^2I_d\\biggl),\\qquad F_t(x|c)=c_1-c_0,\n\\] が \\(P_0,P_1\\) の間の輸送を定める (命題3.3 Tong et al., 2024)．\n加えて，\\(\\sigma\\to0\\) の極限において，学習される輸送 \\((P_t)\\) は \\(Q_0,Q_1\\) の間の輸送になる．\nこれは (Lipman et al., 2023) の例の，\\(P_0,P_1\\) を対称に扱った拡張と見れる．\nまた，\\(P_{t,c}\\) が \\(\\sigma\\to0\\) とした Delta 測度である場合が Rectified Flow (Liu et al., 2023) に当たる．\nこの方法を拡張し，例えば平均を線型関数 \\(m(t)=tc_1+(1-t)c_0\\) の代わりに \\[\nm(t)=\\cos\\left(\\frac{\\pi t}{2}\\right)c_0+\\sin\\left(\\frac{\\pi t}{2}\\right)c_1\n\\] とした場合が Stochastic Interpolant (Albergo and Vanden-Eijnden, 2023) に当たる．\n\n\n\nその中でも特に，\\(\\pi\\) を 2-Wasserstein 距離に関する最適輸送計画 \\[\n\\pi:=\\operatorname*{argmin}_{\\pi\\in C(P_0,P_1)}\\operatorname{E}[\\lvert X-Y\\rvert^2]\n\\] であるとする．\nこのとき， \\[\nP_{t,c}=\\operatorname{N}_d\\biggr(tc_1+(1-t)c_0,\\sigma^2I_d\\biggl),\\qquad F_t(x|c)=c_1-c_0,\n\\] を \\(C\\sim\\pi\\) に関して周辺化した輸送 \\((P_t)\\in\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\) は，\\(\\sigma\\to0\\) の極限で（動的な）最適輸送になる (命題3.4 Tong et al., 2024)．\n\n\n\n\n\n\nSchrödinger Bridge のシミュレーション (SB-CFM Tong et al., 2024)\n\n\n\n\n\n\\[\n\\pi_{2\\sigma^2}:=\\operatorname*{argmin}_{\\pi\\in C(P_0,P_1)}\\biggr(\\operatorname{E}[\\lvert X-Y\\rvert^2]+2\\sigma^2H(\\pi)\\biggl)\n\\] を，エントロピー正則化項 \\(2\\sigma^2\\) を持ったエントロピー最適輸送計画とする．\nこのとき，各点を結んだ Broanian bridge \\[\nP_{t,c}:=\\operatorname{N}\\biggr(tc_1+(1-t)c_0,t(1-t)\\sigma^2I_d\\biggl),\n\\] \\[\nF_t(x|c):=\\frac{1-2t}{2t(1-t)}\\biggr(x-(tc_1+(1-t)c_0)\\biggl)+(c_1-c_0),\n\\] の周辺化 \\((P_t)\\in\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\) は，標準 Brown 運動を \\(\\sigma\\) だけスケールした分布 \\(W\\) に対する Schrödinger bridge \\[\n\\pi^*:=\\operatorname*{argmin}_{\\substack{\\mu_0=P_0\\\\\\mu_1=P_1}}\\operatorname{KL}(\\mu,W)\n\\] と分布同等になる (定理3.5 Tong et al., 2024)．\n\\(\\sigma\\to0\\) の極限が OT-CFM であり，\\(\\sigma\\to\\infty\\) の極限が I-CFM である．\n\n\n\n訓練時は，CFM の目的関数 (8) を計算するために \\((X_0,X_1)\\sim\\pi\\) というサンプリングが必要になる．データサイズが大きい場合には，これにミニバッチ最適輸送 (Fatras et al., 2021) を用いることができる．\nこのように，２つの分布 \\(P_0,P_1\\) を単に独立カップリングと見るのではなく，依存関係があった場合にはそれも考慮してなるべくダイナミクスが直線になるように誘導する方法 Multisample Flow Matching として (Pooladian et al., 2023) も考えている．\n\n\n2.5 \\(\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\) 上の最適化としての見方\n実は OT-CFM は，２つの確率密度 \\(p_0,p_1\\) を結ぶ曲線 \\((p_t)\\in\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\) の中で，Dirichlet エネルギー \\[\nD(p):=\\inf_{(p,F)}\\frac{1}{2}\\int_{[0,1]\\times\\mathbb{R}^d}\\lvert F_t(x)\\rvert^2p_t(x)\\,dxdt\n\\] を最小化する曲線 \\((p_t)\\) を学習していると見れる (Isobe et al., 2024)．ただし，\\((p,F)\\) は連続方程式 (5) を満たす密度とベクトル場の組とした．\n条件付きフローマッチングでは，このような曲線 \\((p_t)\\) を次の方法で構成していた．\n\n\n\n\n\n\n\nある決定論的なダイナミクス \\(\\psi_c:[0,1]\\to\\mathbb{R}^d\\) を定める．2\n\\(Q\\in\\mathcal{P}(C^1([0,1];\\mathbb{R}^d))\\) を確率測度とする．\n\\(\\psi,Q\\) から， \\[\nP^Q:=\\operatorname{E}_{\\psi\\sim Q}[\\delta_\\psi]\n\\] によって確率測度の曲線 \\((P^Q_t)\\in\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\) を定める．\n\n\n\n\n実は Dirichlet 汎函数 \\(D:\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\to\\mathbb{R}_+\\) が凸であるために，このように構成される \\((p_t)\\) の中での最適解は，\\(Q\\in\\mathcal{P}(C^1([0,1];\\mathbb{R}^d))\\) の全体で探す必要はなく，線型なダイナミクス \\[\n\\psi_c(t)=tc_1+(1-t)c_0,\\qquad c=(c_0,c_1)\\in\\mathbb{R}^d\\times\\mathbb{R}^d,\n\\] の重ね合わせの形でのみ探せば良い (Brenier, 2003)．\n従って，\\((X_0,X_1)\\) の分布の全体 \\(C(P_0,P_1)\\) のみについてパラメータづけをして探せば良い．さらにこの場合， \\[\nF_t(x|c)=\\frac{\\partial \\psi_c(t)}{\\partial t}=c_1-c_0\n\\] であるから，\\(D(P)=2W_2(P_0,P_1)^2\\) の最小化は \\(P_0,P_1\\) の 2-Wasserstein 最適な輸送計画 \\(\\pi^*\\) の探索に等価になる．\nこれが OT-CFM の \\(\\mathcal{P}(\\mathbb{R}^d)^{[0,1]}\\) 上の最適化としての解釈である．同時に，条件付きフローマッチングの目的関数 (8) の他に，DSM 様の目的関数 \\[\n\\operatorname{E}\\biggl[\\biggl|F_T(\\psi(T))-\\partial_t\\psi_C(T)\\biggr|^2\\biggr],\\qquad T\\sim\\mathrm{U}([0,1]),C\\sim\\pi^*,\n\\] の最小化点としてもベクトル場 \\(F_t\\) が学習できる．\n\n\n2.6 拡張フローマッチング (GFM)\n前節での観察は次のように要約できる：\n\n\n\n\n\n\nOT-CFM の Dirichlet 汎函数最小化としての特徴付け\n\n\n\nOT-CFM は，条件付きフローマッチングに対して，Dirichlet エネルギーの言葉で帰納バイアスを導入することで，最適輸送を学習するための方法論である．\n\n\nこう考えると，Dirichlet エネルギーの言葉で他の帰納バイアスを導入することが考えられる．\nここで条件付けの議論（第 2.1 節）に戻ってくる．最適輸送のための \\(c=(c_0,c_1)\\in\\mathbb{R}^{2d}\\) に限らず，一般の \\(c\\in\\mathbb{R}^k\\) に対して連続に条件付けされるように拡張したい．\nこれは，\\((F_t),(p_t)\\) の添字を \\(t\\in[0,1]\\) から \\(\\xi\\in[0,1]\\times\\mathbb{R}^k\\) に拡張することで達成される．\nこれは新たな \\((F_\\xi),(p_\\xi)\\) を \\(M_{dk}(\\mathbb{R})\\)-値の行列値ベクトル場 \\((F_t)\\) とベクトル値密度 \\((p_t)\\) と見ることに等価である．すると，一般化連続方程式 (Brenier, 2003), (Lavenant, 2019) \\[\n\\nabla_\\xi p_\\xi(x)+\\operatorname{div}_x(p_\\xi u_\\xi)=0\n\\] の理論を用いれば，全く同様の枠組みで可能になる (命題1 Isobe et al., 2024)．\nこれが 拡張フローマッチング (EFM: Extended Flow Matching) (Isobe et al., 2024) である．\n\n\n2.7 GFM の無限次元最適化\nただし，拡張 Dirichlet エネルギー (Lavenant, 2019) \\[\nD(P):=\\inf_{(p,F)}\\frac{1}{2}\\int_{[0,1]\\times\\mathbb{R}^k\\times\\mathbb{R}^d}\\lvert F_\\xi(x)\\rvert^2p_\\xi(x)\\,dxd\\xi\n\\] の第 2.5 節の形での最小化点は，もはや線型なダイナミクスの重ね合わせとは限らない．\nすると無限次元最適化になってしまうため，適切な RKHS \\(\\mathcal{F}\\subset\\mathrm{Map}([0,1]\\times\\mathbb{R}^k;\\mathbb{R}^d)\\) 内で探すことが必要である： \\[\n\\psi=\\phi_{x_{\\partial\\Xi}}\\in\\operatorname*{argmin}_{f\\in\\mathcal{F}}\\sum_{\\xi\\in\\partial\\Xi}\\lvert f(\\xi)-x_\\xi\\rvert^2.\n\\] ただし，\\(\\partial\\Xi\\overset{\\text{finite}}{\\subset}[0,1]\\times\\mathbb{R}^k\\) は境界条件が与えられる点の有限集合で，\\(x_\\xi\\in\\mathbb{R}^d\\) はその点での値である．\n\\((\\mathbb{R}^d)^{\\lvert\\partial\\Xi\\rvert}\\) 上での結合分布 \\(\\pi\\) が与えられたならば， \\[\n\\inf_{Q\\in\\mathcal{P}(C^1([0,1]\\times\\mathbb{R}^k;\\mathbb{R}^d))}D(P^Q)\\le\\inf_\\pi\\int_{(\\mathbb{R}^d)^{\\lvert\\partial\\Xi\\rvert}}\\lvert\\nabla_\\xi\\phi_{x_{\\partial\\Xi}}\\rvert^2\\pi(dx_\\xi)\n\\] という評価が得られるが，この右辺は最適輸送の形になっており，最小値が適切な周辺分布とコスト関数 \\[\nc(x_{\\partial\\Xi}):=\\int_{[0,1]\\times\\mathbb{R}^k}\\lvert\\nabla_\\xi\\phi_{x_{\\partial\\Xi}}(\\xi)\\rvert^2\\,d\\xi\n\\] が定める輸送計画問題になっている．\nこの解 \\(\\pi^*\\) をミニバッチ最適輸送で解きながら，目的関数 \\[\n\\operatorname{E}\\biggl[\\biggl|F_T(\\psi(T))-\\nabla_\\xi\\phi_{x_{\\partial\\Xi}}\\biggr|^2\\biggr],\\qquad T\\sim\\mathrm{U}([0,1]),x_{\\partial\\Xi}\\sim\\pi^*,\n\\] の最小化点としてベクトル場 \\(F_t\\) を学習することができる (定理4 Isobe et al., 2024)．\nこれを (Isobe et al., 2024) は MMOT-EFM と呼んでいる．"
  },
  {
    "objectID": "posts/2024/Samplers/NF3.html#文献紹介",
    "href": "posts/2024/Samplers/NF3.html#文献紹介",
    "title": "フローベース模型による条件付き生成",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n本記事の後半第 2 節は，(Tong et al., 2024), (Isobe et al., 2024) の解説である．\n前半の内容に関して，メンダコ氏によるブログ記事 AlphaFold の進化史 は AlphaFold3 が丁寧に解説されている．\n当該ブログは丁寧に書かれており，大変おすすめできる．\n\nAlphafold3とは長大な条件付けネットワークを備えた全原子拡散生成モデルであると前述したとおり、Alphafold3では必須入力としてタンパク質配列を、任意入力として核酸配列、SMILES形式で表現された低分子リガンド、金属イオンなどを長大な条件付けネットワークに入力することで、拡散モデルへの条件付けベクトルを作成します。\n\n\nDeepLearningで大規模分子の構造分布を予測するなんて数年前には考えられませんでしたが、拡散モデルによってすでに現実になりつつあります。一例として Distributional GraphormerというMicrosoft Researchの研究 (S. Zheng et al., 2024) を紹介します。\n\n続きはぜひ，メンダコ氏のブログでお読みください．\n(Dao et al., 2023) のプロジェクトページは こちら．"
  },
  {
    "objectID": "posts/2024/Samplers/NF3.html#footnotes",
    "href": "posts/2024/Samplers/NF3.html#footnotes",
    "title": "フローベース模型による条件付き生成",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(c\\) が \\(x_t\\) と同じ画像である場合は，(Ho et al., 2022) のように \\(x_t\\) にそのまま連結することも考えられる．↩︎\nすべての \\((P_{t,c})_{t\\in[0,1]}\\) は \\(\\sigma\\to0\\) の極限で決定論的なダイナミクスを定めていた．これを \\(\\psi_c(t)\\) と表すこととする．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html",
    "href": "posts/2024/Samplers/DD1.html",
    "title": "雑音除去過程",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html#命題",
    "href": "posts/2024/Samplers/DD1.html#命題",
    "title": "雑音除去過程",
    "section": "1 命題",
    "text": "1 命題\n\n\n\n\n\n\n(命題 Haussmann and Pardoux, 1986)\n\n\n\nBrown 運動 \\(\\{B_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) と可測関数 \\(b:[0,1]\\times\\mathbb{R}^d\\to\\mathbb{R}^d,\\sigma:[0,1]\\times\\mathbb{R}^d\\to M_d(\\mathbb{R})\\) に関して， \\[\ndX_t=b_t(X_t)\\,dt+\\sigma_t(X_t)\\,dB_t,\\qquad t\\in[0,1],\n\\] を Markov 過程とする．さらに次の３条件を仮定する：\n\n\\(b_t,\\sigma_t\\) は \\(\\mathbb{R}^d\\) 上局所 Lipschitz 連続で，線型増大条件を満たす： \\[\n  \\lvert b_t(x)\\rvert+\\lvert\\sigma_t(x)\\rvert\\le K(1+\\lvert x\\rvert),\\qquad x\\in\\mathbb{R}^d,K&gt;0.\n  \\]\n\\(X_0\\) は密度 \\(p_0\\) をもち，ある \\(\\lambda&lt;0\\) について次を満たす： \\[\n  p_0\\in L^2((1+\\lvert x\\rvert^2)^\\lambda\\,dx).\n  \\]\n\\(a:=\\sigma\\sigma^\\top\\) は一様に正定値である \\[\n  a_t(x)\\ge\\alpha I_d\n  \\] であるか，\\(\\alpha^{ij}_{x_ix_j}\\in L^\\infty((0,1)\\times\\mathbb{R}^d)\\) である．\n\nこのとき，時間反転 \\(\\overline{X}_t:=X_{1-t}\\) の分布は次の SDE の解である： \\[\nd\\overline{X}_t=\\overline{b}_t(\\overline{X}_t)\\,dt+\\overline{\\sigma}_t(\\overline{X}_t)\\,d\\overline{B}_t,\\qquad t\\in[0,1].\n\\] ただし，\\((B_t)\\) も Brown 運動で， \\[\n\\overline{b}^i_t(x)=-b_{1-t}^i(x)+\\sum_{j=1}^d\\frac{\\biggr(a^{ij}_{1-t}(x)p_{1-t}(x)\\biggl)_{x_j}}{p_{1-t}(x)},\n\\] \\[\n\\overline{a}^{ij}_t(x)=a^{ij}_{1-t}(x),\\qquad\\overline{\\sigma}^{ij}_t(x)=\\sigma^{ij}_{1-t}(x).\n\\]\n\n\n\n\n\n\n\n\n系\n\n\n\n前の命題の３条件を満たす，ドリフト係数 \\(\\sigma\\) が \\(x\\in\\mathbb{R}^d\\) に依らない SDE \\[\ndX_t=b_t(X_t)\\,dt+\\sigma_t\\,dB_t,\\qquad t\\in[0,1],\n\\] を考える．この \\((X)_{t\\in[0,1]}\\) の時間反転は，\\(a_t:=\\sigma_t\\sigma^\\top_t\\) に関して \\[\nd\\overline{X}_t=\\biggr(-b_{1-t}(\\overline{X}_t)+a_{1-t}\\nabla_x\\log p_{1-t}(\\overline{X}_t)\\biggl)\\,dt+\\sigma_{1-t}\\,d\\overline{B}_t,\\qquad\\overline{X}_0=X_1,\n\\] と分布同等になる．\n\n\nSGM (Song and Ermon, 2019), (Song et al., 2021) は， \\[\nb_t(x)=-x,\\qquad\\sigma_t=\\sqrt{2},\n\\tag{1}\\] と設定し，\\((X_t)\\) を OU 過程とした．これは \\(\\operatorname{N}_d(0,I_d)\\) に全変動距離・Wasserstein 距離・\\(\\chi^2\\)-距離で指数収束する．\n従って，この時間反転を \\(\\operatorname{N}_d(0,I_d)\\) からスタートさせることで，データ分布 \\(p_0\\) からのサンプリングが可能になる．\nしかしこのアイデアを実行するためには，スコア \\(\\nabla_x\\log p_{1-t}(X_t)\\) の項を何らかの方法で推定する方法が必要である．"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html#スコアマッチングへの応用",
    "href": "posts/2024/Samplers/DD1.html#スコアマッチングへの応用",
    "title": "雑音除去過程",
    "section": "2 スコアマッチングへの応用",
    "text": "2 スコアマッチングへの応用\n\n2.1 はじめに\nDenoising Score Matching (Vincent, 2011) を初めとして，Generalized Flow Matching (Isobe et al., 2024) や Functional Flow Matching (Kerrigan et al., 2024) は，次のような目的関数を持っている： \\[\n\\mathcal{L}(\\theta)=\\operatorname{E}\\biggl[\\biggl|s_\\theta(\\widetilde{X})-\\frac{X-\\widetilde{X}}{\\sigma^2}\\biggr|^2\\biggr],\\qquad X\\sim p_0,\\widetilde{X}\\sim p_0*\\operatorname{N}(0,\\sigma^2I_d).\n\\tag{2}\\]\nこれはデータ \\(X\\sim p_0\\) と，それに独立な Gauss ノイズを印加したもの \\(\\widetilde{X}\\) との差分を目標としてスコアネットワーク \\(s_\\theta\\) を学習している．\n\n\n2.2 デノイジング過程としての見方\nデータにノイズを印加する過程は，\\(b_t=0,\\sigma_t=I_d\\) とした SDE \\[\ndX_t=dB_t,\\qquad t\\in[0,1],X_0\\sim p_0,\n\\] で \\(t=0\\) から \\(t=\\sigma^2\\) まで輸送することにあたる： \\[\nX_\\sigma^2=X_0+(B_{\\sigma^2}-B_0)\\overset{\\text{d}}{=}\\widetilde{X}.\n\\] この時間反転は \\[\nd\\overline{X}_t=\\nabla_x\\log p_{1-t}(\\overline{X}_t)\\,dt+d\\overline{B}_t,\\qquad\\overline{X}_0=X_1,\n\\] と分布同等になる．\nこの時間反転過程 \\((\\overline{X}_t)\\) は \\(\\overline{X}_{1-\\sigma^2}=\\widetilde{X}\\) を \\(\\overline{X}_1=X\\) まで運ぶが，この際に \\(\\sigma\\ll1\\) ならば次の関係を示唆する： \\[\\begin{align*}\n    X&\\overset{\\text{d}}{=}\\widetilde{X}+\\int^1_{1-\\sigma^2}\\nabla_x\\log p_{1-t}(\\overline{X}_t)\\,dt+\\epsilon\\\\\n    &\\approx\\widetilde{X}+\\sigma^2\\nabla_x\\log p_0(X)+\\epsilon,\\qquad\\epsilon\\sim\\operatorname{N}(0,\\sigma^2I_d).\n\\end{align*}\\] \\(\\sigma\\to0\\) の極限で次の等号が成り立つ： \\[\n\\lim_{\\sigma\\to0}\\frac{X-\\widetilde{X}}{\\sigma^2}\\overset{\\text{d}}{=}\\nabla_x\\log p_0(X).\n\\]\n\n\n2.3 Tweedie の式\n実は同様の式は，\\(\\sigma\\to0\\) の極限で漸近的にではなく正の \\(\\sigma^2&gt;0\\) に関しても，次の意味で成り立つ：\n\n\n\n\n\n\n命題\n\n\n\n\\(X\\sim p_0,\\widetilde{X}\\sim p_0*\\operatorname{N}(0,\\sigma^2I_d)=:\\widetilde{p}_0\\) とする．このとき，次が成り立つ： \\[\n\\operatorname{E}[X|\\widetilde{X}=\\widetilde{x}]=\\widetilde{x}+\\sigma^2\\nabla_{\\widetilde{x}}\\log \\widetilde{p}_0(\\widetilde{x}).\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n一般に，\\(\\phi_\\sigma\\) を \\(\\operatorname{N}_d(0,\\sigma^2I_d)\\) の密度関数とすると， \\[\n\\operatorname{E}[X|\\widetilde{X}=\\widetilde{x}]=\\int_{\\mathbb{R}^d}xp_0(x)\\phi_\\sigma(\\widetilde{x}-x)\\,dx\n\\] より， \\[\n\\operatorname{E}[X|\\widetilde{X}=\\widetilde{x}]-\\widetilde{x}=\\int_{\\mathbb{R}^d}(x-\\widetilde{x})p_0(x)\\phi_\\sigma(\\widetilde{x}-x)\\,dx.\n\\]\n一方で， \\[\\begin{align*}\n    &\\qquad\\nabla_{\\widetilde{x}}\\log\\left(\\int_{\\mathbb{R}^d}p_0(x)\\phi_\\sigma(\\widetilde{x}-x)\\,dx\\right)\\\\\n    &=\\frac{\\int_{\\mathbb{R}^d}p_0(x)\\nabla_{\\widetilde{x}}\\phi_\\sigma(\\widetilde{x}-x)\\,dx}{\\int_{\\mathbb{R}^d}p_0(x)\\phi_\\sigma(\\widetilde{x}-x)\\,dx}\\\\\n    &=\\int_{\\mathbb{R}^d}p_0(x)\\phi_\\sigma(\\widetilde{x}-x)\\frac{x-\\widetilde{x}}{\\sigma^2}\\,dx\\\\\n    &=\\frac{\\operatorname{E}[X|\\widetilde{X}=\\widetilde{x}]-\\widetilde{x}}{\\sigma^2}.\n\\end{align*}\\]\n\n\n\nすなわち，\\(\\widetilde{X}\\) から \\(X\\) を不偏推定しようとすることで，スコア \\(\\nabla_{\\widetilde{x}}\\log\\widetilde{p}(\\widetilde{x})\\) を学習することができるのである．\nただし，学習されるスコアは，データ分布 \\(p_0\\) のものではなく，ノイズ分布 \\(\\widetilde{p}_0\\) のものであることに注意．\nこれが，デノイジングスコアマッチングの目的関数 (2) の背後にある動機付けである．"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html#確率的局所化",
    "href": "posts/2024/Samplers/DD1.html#確率的局所化",
    "title": "雑音除去過程",
    "section": "3 確率的局所化",
    "text": "3 確率的局所化\n\n3.1 OU 過程による SGM\nOU 過程の例 (1) に戻ろう．OU 過程 \\[\ndX_t=-X_t\\,dt+\\sqrt{2}\\,dB_t\n\\] の時間反転は次と分布同等である： \\[\nd\\overline{X}_t=\\biggr(\\overline{X}_t+2\\nabla_x\\log p_{1-t}(\\overline{X}_t)\\biggl)\\,dt+\\sqrt{2}\\,d\\overline{B}_t,\\qquad\\overline{X}_0=X_1.\n\\tag{3}\\]\nOU 過程 \\((X_t)\\) は \\[\nX_t\\overset{\\text{d}}{=}e^{-t}X_0+\\sqrt{1-e^{-2t}}\\epsilon,\\qquad X_0\\sim p_0,\\epsilon\\sim\\operatorname{N}_d(0,I_d)\n\\] という遷移半群を持っているため，\\(\\mathcal{L}[X_t]=\\mathcal{L}[e^{-t}X_0]*\\operatorname{N}_d(0,1-e^{-2t})\\) であることから，Tweedie の式 2.3 より \\[\n\\nabla_x\\log p_t(x_t)=\\frac{\\operatorname{E}[e^{-t}X_0|X_t=x_t]-x_t}{1-e^{-2t}}\n\\] を得る．従ってこのスコアを式 (3) に代入し， \\[\nm_t(x_t):=\\operatorname{E}[X_0|tX_0+\\sqrt{t}\\epsilon=x_t],\\qquad X_0\\sim p_0,\\epsilon\\sim\\operatorname{N}_d(0,I_d),\n\\] とおき， \\[\n\\tau(t)=\\frac{1}{e^{2t}-1}\n\\] の変数変換を施すと OU 過程の時間反転 (3) は次のように書き直せる： \\[\nd\\overline{Y}_\\tau=\\biggr(-\\frac{1+\\tau}{\\tau(1+\\tau)}\\overline{Y}_\\tau+\\frac{1}{\\sqrt{\\tau(1+\\tau)}}m_\\tau\\left(\\sqrt{\\tau(1+\\tau)}\\overline{Y}_\\tau\\right)\\biggl)\\,d\\tau+\\frac{1}{\\sqrt{\\tau(1+\\tau)}}\\,d\\overline{B}_\\tau.\n\\tag{4}\\]\nこれが denoising diffusion である．\n\n\n3.2 もう一つのサンプリング法\n確率的局所化 (stochastic localization) は初め，(Eldan, 2013) が高次元空間内の等方的な凸体上での等周不等式を示すために構成した半マルチンゲールが基になっている．\n確率的局所化においては，\\(p_0\\) からのあるサンプル \\(x_0\\) に対して，その 観測過程 \\((Y_t)\\) と呼ばれる \\(x_0\\) のノイズ付きの観測を考える．ただし，\\(Y_t\\) は時間が進むごとに \\(x_0\\) に関する情報量が増えるとする．1\n例えば \\[\nY_t=tx_0+B_t,\\qquad t\\in\\mathbb{R}_+,\n\\] という場合である．\\(B_t\\) というノイズは印加されているが，\\(x_0\\) というメッセージの内容がどんどん大きくなるため，Signal-to-noise 比は増大していく．\nこの場合については，\\(p_0\\) が有限な二次の積率を持つならば， \\[\ndY_\\tau=m_\\tau(Y_\\tau)\\,d\\tau+dB'_\\tau\n\\] という SDE の解と分布同等である (Liptser and Shiryaev, 2001)．\nこれは式 (4) で与えた OU 過程の時間反転 \\((\\overline{Y}_\\tau)\\) に関して，\\(Y_\\tau=\\sqrt{\\tau(1+\\tau)}\\overline{Y}_\\tau\\) の関係を持つ．\n\n\n3.3 確率的局所化\n実は \\((Y_t)\\) は， \\[\n\\mu_t:=\\mathcal{L}[X_0|Y_t]\n\\] として定まる \\(\\operatorname{P}(\\mathbb{R}^d)\\)-値の確率過程 \\(\\{\\mu_t\\}\\subset\\mathcal{L}(\\Omega;\\mathcal{P}(\\mathbb{R}^d))\\) について，次の性質を持つ： \\[\n\\mu_0=\\mathcal{L}[X_0]=p_0\\,d\\ell_d,\\qquad\\mu_t\\Rightarrow\\delta_{x_0}\\qquad t\\to\\infty,\n\\]\n実は上述のサンプリング法は，このような \\(p_0\\,d\\ell_d\\) から \\(\\delta_{X_0}\\;(X_0\\sim p_0)\\) への確率過程が与えられるごとに構成できる．\n実際，最も簡単には，重心 \\[\nM_t:=\\int_{\\mathbb{R}^d}x\\,\\mu_t(dx)\n\\] を計算すれば，\\(M_t\\) は \\(t\\to\\infty\\) の極限で \\(\\mathcal{L}[X_0]\\) に収束する．\nこれが 確率的局所化 である．\n確率的局所化に基づいたサンプラーは (Alaoui et al., 2022) により Sherrington-Kirkpatrick 模型 の Gibbs 分布からのサンプリングに適用され，(Montanari and Wu, 2023) でさらにベイズ統計への応用のために拡張されている．\nまた，最良の雑音除去拡散モデルの収束証明は確率的局所化に基づいた証明によって与えられている (Benton et al., 2024)．"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html#関連ページ",
    "href": "posts/2024/Samplers/DD1.html#関連ページ",
    "title": "雑音除去過程",
    "section": "関連ページ",
    "text": "関連ページ\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html#文献紹介",
    "href": "posts/2024/Samplers/DD1.html#文献紹介",
    "title": "雑音除去過程",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n(Anderson, 1982) では Fokker-Planck 方程式の解に対する条件の言葉で時間反転命題を与えている．また，時間反転も，元の Brown 運動 \\(B_t\\) と独立な Brown 運動 \\(\\overline{B}_t\\) に関する SDE ではなく，その時間反転 \\(\\widetilde{B}_t:=B_{1-t}\\) に関する SDE で与えている．\nAleandre Thiéry のブログ記事 や 鈴木大慈氏のスライド，Montanari の講義資料 を参照した．\nTweedie の式は (Robbins, 1956) によって命名されている．(Efron, 2011) では選択バイアスが存在する状況における経験ベイズ法に応用している．\n確率的局所化については (Montanari, 2023) を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/DD1.html#footnotes",
    "href": "posts/2024/Samplers/DD1.html#footnotes",
    "title": "雑音除去過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n例えば，\\(x_*,Y_{t_2},Y_{t_1}\\) が長さ３の Markov 連鎖をなす，などの意味で．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html",
    "href": "posts/2024/Samplers/EBM2.html",
    "title": "エネルギーベースモデルのノイズ対照学習",
    "section": "",
    "text": "深層生成モデル５\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJAX によるハンズオン\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半正定値カーネルから距離学習まで\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html#モデル定義",
    "href": "posts/2024/Samplers/EBM2.html#モデル定義",
    "title": "エネルギーベースモデルのノイズ対照学習",
    "section": "1 モデル定義",
    "text": "1 モデル定義\n\nimport torch\nimport torch.nn as nn\n\n# ------------------------------\n# ENERGY-BASED MODEL\n# ------------------------------\nclass EBM(nn.Module):\n    def __init__(self, dim=2):\n        super(EBM, self).__init__()\n        # The normalizing constant logZ(θ)        \n        self.c = nn.Parameter(torch.tensor([1.], requires_grad=True))\n\n        self.f = nn.Sequential(\n            nn.Linear(dim, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(128, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(128, 1),\n            )\n\n    def forward(self, x):\n        log_p = - self.f(x) - self.c\n        return log_p"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html#事前準備",
    "href": "posts/2024/Samplers/EBM2.html#事前準備",
    "title": "エネルギーベースモデルのノイズ対照学習",
    "section": "2 事前準備",
    "text": "2 事前準備\n\n\nCode\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\ndef value(energy, noise, x, gen):\n    logp_x = energy(x)  # logp(x)\n    logq_x = noise.log_prob(x).unsqueeze(1)  # logq(x)\n    logp_gen = energy(gen)  # logp(x̃)\n    logq_gen = noise.log_prob(gen).unsqueeze(1)  # logq(x̃)\n\n    value_data = logp_x - torch.logsumexp(torch.cat([logp_x, logq_x], dim=1), dim=1, keepdim=True)  # log[p(x)/(p(x) + q(x))]\n    value_gen = logq_gen - torch.logsumexp(torch.cat([logp_gen, logq_gen], dim=1), dim=1, keepdim=True)  # log[q(x̃)/(p(x̃) + q(x̃))]\n\n    v = value_data.mean() + value_gen.mean()\n\n    r_x = torch.sigmoid(logp_x - logq_x)\n    r_gen = torch.sigmoid(logq_gen - logp_gen)\n\n    acc = ((r_x &gt; 1/2).sum() + (r_gen &gt; 1/2).sum()).cpu().numpy() / (len(x) + len(gen))\n\n    return -v,  acc\n\n\n#-------------------------------------------\n# DATA\n#-------------------------------------------\ndef get_data(args):\n    dataset = sample_2d_data(dataset=args.dataset, n_samples=args.samples)\n    dataloader  = DataLoader(dataset, batch_size=args.batch, shuffle=True)\n    return dataset, dataloader\n\ndef sample_2d_data(dataset='8gaussians', n_samples=50000):\n    \n    z = torch.randn(n_samples, 2)\n\n    if dataset == '8gaussians':\n        scale = 4\n        sq2 = 1/math.sqrt(2)\n        centers = [(1,0), (-1,0), (0,1), (0,-1), (sq2,sq2), (-sq2,sq2), (sq2,-sq2), (-sq2,-sq2)]\n        centers = torch.tensor([(scale * x, scale * y) for x,y in centers])\n        return sq2 * (0.5 * z + centers[torch.randint(len(centers), size=(n_samples,))])\n\n    elif dataset == '2spirals':\n        n = torch.sqrt(torch.rand(n_samples // 2)) * 540 * (2 * math.pi) / 360\n        d1x = - torch.cos(n) * n + torch.rand(n_samples // 2) * 0.5\n        d1y =   torch.sin(n) * n + torch.rand(n_samples // 2) * 0.5\n        x = torch.cat([torch.stack([ d1x,  d1y], dim=1),\n                       torch.stack([-d1x, -d1y], dim=1)], dim=0) / 3\n        return x + 0.1*z\n\n    elif dataset == 'checkerboard':\n        x1 = torch.rand(n_samples) * 4 - 2\n        x2_ = torch.rand(n_samples) - torch.randint(0, 2, (n_samples,), dtype=torch.float) * 2\n        x2 = x2_ + x1.floor() % 2\n        return torch.stack([x1, x2], dim=1) * 2\n\n    elif dataset == 'rings':\n        n_samples4 = n_samples3 = n_samples2 = n_samples // 4\n        n_samples1 = n_samples - n_samples4 - n_samples3 - n_samples2\n\n        # so as not to have the first point = last point, set endpoint=False in np; here shifted by one\n        linspace4 = torch.linspace(0, 2 * math.pi, n_samples4 + 1)[:-1]\n        linspace3 = torch.linspace(0, 2 * math.pi, n_samples3 + 1)[:-1]\n        linspace2 = torch.linspace(0, 2 * math.pi, n_samples2 + 1)[:-1]\n        linspace1 = torch.linspace(0, 2 * math.pi, n_samples1 + 1)[:-1]\n\n        circ4_x = torch.cos(linspace4)\n        circ4_y = torch.sin(linspace4)\n        circ3_x = torch.cos(linspace4) * 0.75\n        circ3_y = torch.sin(linspace3) * 0.75\n        circ2_x = torch.cos(linspace2) * 0.5\n        circ2_y = torch.sin(linspace2) * 0.5\n        circ1_x = torch.cos(linspace1) * 0.25\n        circ1_y = torch.sin(linspace1) * 0.25\n\n        x = torch.stack([torch.cat([circ4_x, circ3_x, circ2_x, circ1_x]),\n                         torch.cat([circ4_y, circ3_y, circ2_y, circ1_y])], dim=1) * 3.0\n\n        # random sample\n        x = x[torch.randint(0, n_samples, size=(n_samples,))]\n\n        # Add noise\n        return x + torch.normal(mean=torch.zeros_like(x), std=0.08*torch.ones_like(x))\n\n    elif dataset == \"pinwheel\":\n        rng = np.random.RandomState()\n        radial_std = 0.3\n        tangential_std = 0.1\n        num_classes = 5\n        num_per_class = n_samples // 5\n        rate = 0.25\n        rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)\n\n        features = rng.randn(num_classes*num_per_class, 2) \\\n            * np.array([radial_std, tangential_std])\n        features[:, 0] += 1.\n        labels = np.repeat(np.arange(num_classes), num_per_class)\n\n        angles = rads[labels] + rate * np.exp(features[:, 0])\n        rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n        rotations = np.reshape(rotations.T, (-1, 2, 2))\n        \n        data = 2 * rng.permutation(np.einsum(\"ti,tij-&gt;tj\", features, rotations))\n        return torch.as_tensor(data, dtype=torch.float32)\n\n    else:\n        raise RuntimeError('Invalid `dataset` to sample from.')\n\n\n\n\nCode\n# --------------------\n# Plotting\n# --------------------\n\n@torch.no_grad()\ndef plot(dataset, energy, noise, epoch, device):\n    n_pts = 1000\n    range_lim = 4\n\n    # construct test points\n    test_grid = setup_grid(range_lim, n_pts, device)\n\n    # plot\n    # fig, axs = plt.subplots(1, 3, figsize=(12,4.3), subplot_kw={'aspect': 'equal'})\n    # plot_samples(dataset, axs[0], range_lim, n_pts)\n    # plot_noise(noise, axs[1], test_grid, n_pts)\n    fig, ax = plt.subplots(1, 1, figsize=(4,4), subplot_kw={'aspect': 'equal'})\n    plot_energy(energy, ax, test_grid, n_pts)\n\n    # format\n    for ax in plt.gcf().axes: format_ax(ax, range_lim)\n    plt.tight_layout()\n\n    # save\n    print('Saving image to images/....')\n    plt.savefig('images/epoch_{}.png'.format(epoch))\n    plt.close()\n\ndef setup_grid(range_lim, n_pts, device):\n    x = torch.linspace(-range_lim, range_lim, n_pts)\n    xx, yy = torch.meshgrid((x, x), indexing='ij')\n    zz = torch.stack((xx.flatten(), yy.flatten()), dim=1)\n    return xx, yy, zz.to(device)\n\ndef plot_samples(dataset, ax, range_lim, n_pts):\n    samples = dataset.numpy()\n    ax.hist2d(samples[:,0], samples[:,1], range=[[-range_lim, range_lim], [-range_lim, range_lim]], bins=n_pts, cmap=plt.cm.jet)\n    ax.set_title('Target samples')\n\ndef plot_energy(energy, ax, test_grid, n_pts):\n    xx, yy, zz = test_grid\n    log_prob = energy(zz)\n    prob = log_prob.exp().cpu()\n    # plot\n    ax.pcolormesh(xx.numpy(), yy.numpy(), prob.view(n_pts,n_pts).numpy(), cmap=plt.cm.jet)\n    ax.set_facecolor(plt.cm.jet(0.))\n    ax.set_title('Energy density')\n\ndef plot_noise(noise, ax, test_grid, n_pts):\n    xx, yy, zz = test_grid\n    log_prob = noise.log_prob(zz)\n    prob = log_prob.exp().cpu()\n    # plot\n    ax.pcolormesh(xx.numpy(), yy.numpy(), prob.view(n_pts,n_pts).numpy(), cmap=plt.cm.jet)\n    ax.set_facecolor(plt.cm.jet(0.))\n    ax.set_title('Noise density')\n\ndef format_ax(ax, range_lim):\n    ax.set_xlim(-range_lim, range_lim)\n    ax.set_ylim(-range_lim, range_lim)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax.invert_yaxis()"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html#訓練",
    "href": "posts/2024/Samplers/EBM2.html#訓練",
    "title": "エネルギーベースモデルのノイズ対照学習",
    "section": "3 訓練",
    "text": "3 訓練\n\nimport argparse\nimport os\nimport torch\nimport torch.distributions as D\n\nd = 'cpu'\n# if torch.cuda.is_available():\n#     d = 'cuda'\n# elif torch.backends.mps.is_available():\n#     d = 'mps'\ndevice = torch.device(d)\n\nclass Args:\n    def __init__(self):\n        self.epoch = 50\n        self.batch = 100\n        self.dataset = '8gaussians'\n        self.samples = 10000\n        self.lr = 1e-3\n        self.b1 = 0.9\n        self.b2 = 0.999\n        self.resume = False\n\nargs = Args()\n\n# ------------------------------\n# I. MODELS\n# ------------------------------\nenergy = EBM(dim=2).to(device)\nnoise = D.MultivariateNormal(torch.zeros(2).to(device), 4.*torch.eye(2).to(device))\n# ------------------------------\n# II. OPTIMIZERS\n# ------------------------------\noptim_energy = torch.optim.Adam(energy.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n# ------------------------------\n# III. DATA LOADER\n# ------------------------------\ndataset, dataloader = get_data(args)\n# ------------------------------\n# IV. TRAINING\n# ------------------------------\ndef main(args):\n    start_epoch = 0\n# ----------------------------------------------------------------- #\n    if args.resume:\n        print('Resuming from checkpoint at ckpts/nce.pth.tar...')\n        checkpoint = torch.load('ckpts/nce.pth.tar')\n        energy.load_state_dict(checkpoint['energy'])\n        start_epoch = checkpoint['epoch'] + 1\n# ----------------------------------------------------------------- #\n    for epoch in range(start_epoch, start_epoch + args.epoch):\n        for i, x in enumerate(dataloader):           \n            x = x.to(device)\n            # -----------------------------\n            #  Generate samples from noise\n            # -----------------------------\n            gen = noise.sample((args.batch,))\n            # -----------------------------\n            #  Train Energy-Based Model\n            # -----------------------------\n            optim_energy.zero_grad()\n\n            loss_energy, acc = value(energy, noise, x, gen)\n\n            loss_energy.backward()\n            optim_energy.step()  \n\n            print(\n                \"[Epoch %d/%d] [Batch %d/%d] [Value: %f] [Accuracy:%f]\"\n                % (epoch, start_epoch + args.epoch, i, len(dataloader), loss_energy.item(), acc)\n            )\n\n        # Save checkpoint\n        print('Saving models...')\n        state = {\n        'energy': energy.state_dict(),\n        'value': loss_energy,\n        'epoch': epoch,\n        }\n        os.makedirs('ckpts', exist_ok=True)\n        torch.save(state, 'ckpts/nce.pth.tar')\n\n        # visualization\n        plot(dataset, energy, noise, epoch, device)\n\n\nif __name__ == '__main__':\n    print(args)\n    main(args)\n\n大変軽量で，cpu でも５分ほどで学習できる（そのうちほとんどは画像の保存にかかる時間である）．しかし，mps では次のエラーを得る．\nNotImplementedError: The operator 'aten::linalg_cholesky_ex.L' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n\n他のデータに関しても，次のように学習できる：\n\n\n\npinwheel はそれぞれの羽の尾の部分が消えてしまっているように見える．\n\n\n\n\n\nrings に関しては結構学習に苦労しているようだ．\n\n\n\n\n\ncheckerboard も四角い形までは 50 epoch では再現が難しいのかもしれない．\n\n\n\n\n\n2spirals は結構トポロジーを間違えている！\n\n\n\n\n\nrings ロングバージョン．epoch=150 としたが，内側２輪しか再現できていない．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM2.html#文献",
    "href": "posts/2024/Samplers/EBM2.html#文献",
    "title": "エネルギーベースモデルのノイズ対照学習",
    "section": "4 文献",
    "text": "4 文献\n\n李飞氏 による 実装 を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html",
    "href": "posts/2024/Samplers/DDPM1.html",
    "title": "Neural Network 訓練の加速",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html#問題点と改善したいこと",
    "href": "posts/2024/Samplers/DDPM1.html#問題点と改善したいこと",
    "title": "Neural Network 訓練の加速",
    "section": "1 問題点と改善したいこと",
    "text": "1 問題点と改善したいこと\nデータセットの読み込みの段階において，次のコードがある：\nkwargs = {'num_workers': 5, 'pin_memory': True, 'prefetch_factor': 2}\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False,  **kwargs)\nこれはデータセット（今回はMNIST）を読み込み，iterable 型としての仕様を可能にするためのコードである．\n上述の通りのコードだとエポック 18 で RuntimeError: Shared memory manager connection has timed out を得たが，num_workers=0 とするとエラーが発生しなかった．\nしかし，num_workers=0 （デフォルト設定）とすると，デフォルトの単一プロセス処理が実行されるため，並列による高速化の恩恵を受けられない．その結果，１エポック 12 分以上なので，40 時間以上をかける必要が出てきた（寝てる間もディスプレイをオフにするだけでスリープさせず，回し続ける）．\n\n\n\n\n\n\n今回の目標\n\n\n\nうまく並列処理をするようなコードに書き直すことで，ローカル環境でも１日以内で実行できるようにしたい．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html#dataloader-の引数について",
    "href": "posts/2024/Samplers/DDPM1.html#dataloader-の引数について",
    "title": "Neural Network 訓練の加速",
    "section": "2 DataLoader の引数について",
    "text": "2 DataLoader の引数について\nDataLoader メソッドのドキュメント を参照すると，\n\n2.1 num_workers\nは正整数に設定されると，その数だけ並列に動く ‘worker’ が起動され，マルチプロセス処理が実行される．\nしかし，子プロセスも同等のメモリを占めるため，値が大きすぎるとランタイムエラーが発生する（issue #13246 参照）．\nさらに，この際の並列処理は Python の multiprocessing パッケージによるもので，Windows と MacOS では（Unix 系のような fork() ではなく） spawn() が呼ばれる．これは別のインタープリターを開始するため，コードの大部分を if __name__ == \"__main__\": で囲まない限り，同じコードを何回も実行することとなり，ランタイムエラーが出現することとなる．\n\n\n2.2 pin_memeory\nしかし，CUDA 上のテンソルオブジェクトを並列処理で共有することは非推奨であり，その際は自動メモリ固定 (automatic memory pinning) を行う必要がある．\npinned memory とは page-locked メモリとも呼ばれ，通常の pageable メモリより転送速度が速いという．\nさて，paging とはなんだろうか？（一旦後回し）\n\n\n2.3 prefetch_factor\nは各 worker が取ってきてストックしておくバッチの数である．\nすなわち，num_workers * prefetch_factor だけデータをメモリに読み込んでおくことになる．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html#高速化法",
    "href": "posts/2024/Samplers/DDPM1.html#高速化法",
    "title": "Neural Network 訓練の加速",
    "section": "3 高速化法",
    "text": "3 高速化法\n\n3.1 Google Colab の利用\n結局この方法でトレーニングをし，前稿 を完成させたのであった．\n\n\n\nA100（8/6/2024 時点）\n\n\nA100 が税込 1,494,000 円であったが，これを利用すると１エポック 22 秒で実行できた．\n\n\n3.2 torch.nn.DataParallel の使用\n自分のローカルマシンは CUDA がないため利用できないが，ある場合は PyTorch のモジュールで並列処理が可能である．1"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html#mps-で本当に高速になっているのか",
    "href": "posts/2024/Samplers/DDPM1.html#mps-で本当に高速になっているのか",
    "title": "Neural Network 訓練の加速",
    "section": "4 mps で本当に高速になっているのか？",
    "text": "4 mps で本当に高速になっているのか？\nアップルは Metal という計算 API を提供しており，これが Apple Silicon で利用できる．\n\nimport torch\nDEVICE = torch.device(\"mps\")\ntrain_batch_size = 128\nepochs = 1\n\nとし，１エポックにかかる時間を比較する．その他の設定は前節と同様．\n\nprint(\"Start training DDPMs...\")\nmodel.train()\n\nimport time\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    noise_prediction_loss = 0\n    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        optimizer.zero_grad()\n\n        x = x.to(DEVICE)\n        \n        noisy_input, epsilon, pred_epsilon = diffusion(x)\n        loss = denoising_loss(pred_epsilon, epsilon)\n        \n        noise_prediction_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n    \ntotal_time = time.time() - start_time\n\nprint(\"Finish!! Total time: \", total_time)\n\n12:58 であった．一方で，CPU でも訓練してみる．\n\nDEVICE = torch.device(\"cpu\")\nmodel = Denoiser(image_resolution=img_size,\n                    hidden_dims=hidden_dims, \n                    diffusion_time_embedding_dim=timestep_embedding_dim, \n                    n_times=n_timesteps).to(DEVICE)\ndiffusion = Diffusion(model, image_resolution=img_size, n_times=n_timesteps, beta_minmax=beta_minmax, device=DEVICE).to(DEVICE)\n\nprint(\"Start training DDPMs...\")\nmodel.train()\n\nimport time\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    noise_prediction_loss = 0\n    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        optimizer.zero_grad()\n\n        x = x.to(DEVICE)\n        \n        noisy_input, epsilon, pred_epsilon = diffusion(x)\n        loss = denoising_loss(pred_epsilon, epsilon)\n        \n        noise_prediction_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n    \ntotal_time = time.time() - start_time\n\nprint(\"Finish!! Total time: \", total_time)\n\n１時間越え！"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html#終わりに",
    "href": "posts/2024/Samplers/DDPM1.html#終わりに",
    "title": "Neural Network 訓練の加速",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nあまりに時間がかかるので，本記事は eval: false としておく．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM1.html#footnotes",
    "href": "posts/2024/Samplers/DDPM1.html#footnotes",
    "title": "Neural Network 訓練の加速",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPyTorchでGPUを並列で使えるようにするtorch.nn.DataParallelのメモ などを参照した．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal2001.html",
    "href": "posts/2024/Review/Roberts-Rosenthal2001.html",
    "title": "Roberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal2001.html#概要",
    "href": "posts/2024/Review/Roberts-Rosenthal2001.html#概要",
    "title": "Roberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms",
    "section": "概要",
    "text": "概要\n(Gareth O. Roberts and Rosenthal, 2001) は (G. O. Roberts et al., 1997), (Gareth O. Roberts and Rosenthal, 1998) などの最適スケーリングの結果をまとめたサーベイ論文である．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal2001.html#導入",
    "href": "posts/2024/Review/Roberts-Rosenthal2001.html#導入",
    "title": "Roberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms",
    "section": "1 導入",
    "text": "1 導入\nasymptotic overall acceptance rate が，MCMC アルゴリズムの効率性の指標に使える，という点を上手に導入している．\nひょっとしたら，(Chopin et al., 2022) が目指したい方向性は，同様の指標を SMC について見出すことだったのかもしれない．\n\nOur results provide theoretical justification for a commonly used strategy for implementing the multivariate random-walk Metropolis algorithm. which dates back at least as far as (Tierney, 1994).\n\n\nThis demonstrates that MALA algorithms asymptotically mix considerably faster than do RWM algorithms."
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal2001.html#乱歩-mh-法の提案分散",
    "href": "posts/2024/Review/Roberts-Rosenthal2001.html#乱歩-mh-法の提案分散",
    "title": "Roberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms",
    "section": "2 乱歩 MH 法の提案分散",
    "text": "2 乱歩 MH 法の提案分散\n\n2.1 高次元極限"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal1998.html",
    "href": "posts/2024/Review/Roberts-Rosenthal1998.html",
    "title": "Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal1998.html#概要",
    "href": "posts/2024/Review/Roberts-Rosenthal1998.html#概要",
    "title": "Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions",
    "section": "概要",
    "text": "概要\nMALA は (Besag, 1994) で提案され，(Gareth O. Roberts and Tweedie, 1996) で指数エルゴード性が示されている．(Gareth O. Roberts and Rosenthal, 1998) では最適スケーリングが論じられている．\n\n  \n    \n      \n      \n        Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n        Roberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの．\n      \n    \n  \n\nなお，本論文 (Gareth O. Roberts and Rosenthal, 1998) では MALA を “Hastings-Metropolis algorithms derived from Langevin diffusions” と呼んでいる．(Neal, 1993) では Langevin Monte Carlo を Hybrid Monte Carlo の特別な場合として取り上げている．\nBrown 運動の離散化を Monte Carlo 法に用いるというアイデアは (Rossky et al., 1978) でもすでに見られていた．\n(Fearnhead et al., 2018) において，MALA は BPS と比較されている．1 モデルは AR(1) を用いており，低次元ではほとんど変わらないが，高次元では BPS の方が自己相関時間が５倍良いという結論が得られている．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal1998.html#導入",
    "href": "posts/2024/Review/Roberts-Rosenthal1998.html#導入",
    "title": "Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 MALA について\n乱歩 MH は目標分布に依存せず同一の実装をもつ点が利点であるが，それ故に目標分布が複雑である場合は収束が遅いことがある．\n一方で，Langevin アルゴリズムは目標分布の勾配を利用した対称的な手法で，\\(\\pi\\) が増加する方向への提案を増やすことで収束を速めた MH 法である．\n\nLangevin algorithms use local problem-specific information and are therefore often almost as easy to implement.\n\n(Gareth O. Roberts and Tweedie, 1996) では Langevin アルゴリズムはいつでも指数エルゴード性を持つ訳ではないことがわかったが，それでも，特に高次元の設定で，乱歩 MH 法より速い収束が確認されている．本論文の結果を通じて，このことを理論的に確認することもできる！\n\n\n1.2 最適スケーリングについて\n(G. O. Roberts et al., 1997) では乱歩 MH の最適スケーリングが調べられ，漸近的な採択率を \\(0.234\\) とするのが良いこと，そして提案分散は次元 \\(n\\) に対して \\(n^{-1}\\) のオーダーで漸近的に分散させるのが良いことが示されている．\nいずれも \\(n\\to\\infty\\) の漸近論的な結果であるが，(Gelman et al., 1996) で確認されたように，比較的低次元でもこの指針は有効である．\nこれと同じように，Langevin アルゴリズムにおいても最適スケーリングを論じたい．\nMALA は目標分布の情報を用いた提案をするため，最適な採択率はより高い水準で調整されるべきであるだろう，という結果の予測が立つ．\n\n\n1.3 主結果の概要\n\n最適な漸近的スケーリングは，採択率を \\(0.574\\) としたものである．これは (Mountain and Thirumalai, 1994) でも実験的に検証されていた．\n提案分散は \\(n^{-1/3}\\) のスケーリングを持つべきである．これは (Kennedy and Pendleton, 1991) でも実験的に観察されていた．\n従って，アルゴリズムが収束するには \\(O(n^{1/3})\\) のステップが必要であり，ランダムウォーク MH 法の \\(O(n)\\) よりもよっぽど良い（スピード測度 1.4 を踏まえた議論）．\n\n\nFurthermore, the proposal variance should scale as \\(n^{-1/3}\\), and thus \\(O(n^{1/3})\\) steps are required for the Langevin algorithm to converge.\n\n\n\n1.4 スピード測度\nspeed measure が MCMC の効率性を測るにあたって極めて重要な指標であることを説明している．\nまず第一に，ベイズ計算法としては，Markov 連鎖 \\(X\\) と関数 \\(f:E\\to\\mathbb{R}\\) に対して，その Monte Carlo 推定量の漸近分散の逆数 \\[\ne_f:=\\left(\\lim_{n\\to\\infty}n\\mathrm{V}\\left[\\frac{1}{n}\\sum_{i=1}^nf(X_i)\\right]\\right)^{-1}\n\\] が最重要指標の１つとして挙げられる．\nしかし拡散極限では，\\(e_f\\) は \\(f\\) に依存せず，スピード測度のみの関数になるのである．\nこれにより，高次元極限 \\(n\\to\\infty\\) においては MCMC アルゴリズムの性能比較が理論的に行える．\n\nAll other measures of efficiency are equivalent (up to a normalization constant), including those described above.\n\nまた，\\(0.574\\) がスピード速度 \\(h\\) の最大値点であるが，\\([0.4,0.8]\\) の区間ならばほとんど変わらない．\n\n\n1.5 設定\n目標分布は，ある密度 \\(f\\) に従う確率変数の独立同分布観測であるとし， \\[\n\\pi_n(x):=\\prod_{i=1}^nf(x_i)=:\\prod_{i=1}^ne^{g(x_i)}\n\\] であると仮定して議論する．\n\\(\\pi_n\\) に対する分散パラメータ \\(\\sigma^2\\) を持った 可逆な Langevin 拡散 (reversible Langevin diffusion) とは， \\[\nd\\Lambda_t=\\sigma dB_t+\\frac{\\sigma^2}{2}\\nabla\\log(\\pi_n(\\Lambda_t))dt\n\\] をいう．\nこの step variance \\(\\{\\sigma_n^2\\}\\) による離散化を \\[\n\\widetilde{\\Lambda}_{t+1}=\\widetilde{\\Lambda}_t+\\sigma_nZ_{t+1}+\\frac{\\sigma^2_n}{2}\\nabla\\log(\\pi_n(\\widetilde{\\Lambda}_t))\n\\] とする．これは \\(\\sigma_n^2\\) をどんなに小さく取っても収束しない可能性がある．\n\\(\\widetilde{\\Lambda}\\) の不変分布はそのままでは \\(\\pi_n\\) ではないから，\\(\\widetilde{\\Lambda}\\) の MH 法による修正を考える． \\[\nY_{t+1}:=X_t+\\sigma_nZ_{t+1}+\\frac{\\sigma^2_n}{2}\\nabla\\log(\\pi_n(X_t))\n\\] とし， \\[\nX_{t+1}:=\\begin{cases}Y_{t+1}&\\text{確率}\\;\\alpha_n(X_t,Y_{t+1})\\\\X_t&\\text{確率}1-\\alpha_n(X_t,Y_{t+1})\\end{cases}\n\\] \\[\n\\alpha_n(X_t,Y_{t+1}):=\\frac{\\pi_n(Y_{t+1})q_n(Y_{t+1},Y_t)}{\\pi_n(X_t)q_n(X_t,Y_{t+1})}\\land1\n\\] \\[\\begin{align*}\n  &q_n(x,y):=\\\\\n  &\\frac{1}{(2\\pi\\sigma^2_n)^{n/2}}\\exp\\left(-\\frac{1}{2\\sigma^2_n}\\left\\|y-x-\\frac{\\sigma^2_n}{2}\\nabla\\log(\\pi_n(x))\\right\\|^2_2\\right)\\\\\n  &=:\\prod_{i=1}^nq(x_i^n,y_i).\n\\end{align*}\\]\nステップサイズ \\(\\sigma_n^2\\) はちょうど乱歩 MH のステップサイズのような役割を果たす．大きくすると，一歩で動く幅が大きくなるが，採択率が低くなりすぎると逆効果である．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal1998.html#主結果",
    "href": "posts/2024/Review/Roberts-Rosenthal1998.html#主結果",
    "title": "Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions",
    "section": "2 主結果",
    "text": "2 主結果\nMALA \\(\\{X_t\\}\\) については，定常性を仮定する：\\(X_0\\sim\\pi_n\\)．\n\\(\\pi_n\\) は任意次の積率を持ち，\\(g\\in C^8_p(\\mathbb{R}^n)\\) で，ある多項式 \\(M_0\\) について， \\[\n\\lvert g^{(i)}(x)\\rvert\\le M_0(x)\n\\] \\[\nx\\in\\mathbb{R}^n,i=0,1,\\cdots,8,\n\\] とする．加えて，SDE の議論のために \\(g'\\in\\mathrm{Lip}(\\mathbb{R}^n)\\) も仮定する．\n\\(\\{J_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^n)\\) をレート \\(n^{1/3}\\) を持つ Poisson 過程で，\\(\\sigma^2_n:=l^2n^{-1/3}\\;(l&gt;0)\\) に対して \\[\n\\Gamma^n_t:=X_{J_t}\n\\] で定まる \\(\\{\\Gamma^n_t\\}\\) を純粋跳躍過程とする．\n\\[\n\\alpha_n(l):=\\iint_{\\mathbb{R}^n}\\pi_n(x)q_n(x,y)\\alpha_n(x,y)dxdy=\\operatorname{E}\\left[\\frac{\\pi_n(Y)q_n(Y,X)}{\\pi_n(X)q_n(X,Y)}\\land 1\\right]\n\\] を \\(\\Gamma\\) を定める採択率の \\(\\pi_n\\)-平均とする．\n\n\n\n\n\n\n定理１（平均採択率の極限）\n\n\n\n\\[\n\\lim_{n\\to\\infty}a_n(l)=a(l):=2\\Phi\\left(-\\frac{Kl^3}{2}\\right)\n\\] \\[\nK:=\\sqrt{\\operatorname{E}_{\\pi_1}\\left[\\frac{5g'''(X)^2-3g''(X)^3}{48}\\right]}&gt;0.\n\\]\n\n\n\n\n\n\n\n\n定理２（拡散極限）\n\n\n\n\\(\\{U^n\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R})\\) を \\(\\Gamma^n\\) の第一成分とする．このとき，\\(\\{U^n\\}\\) は Skorokhod 位相について次の拡散過程 \\(U\\) に弱収束する： \\[\ndU_t=\\sqrt{h(l)}dB_t+\\frac{1}{2}h(l)\\frac{d }{d x}\\log(\\pi_1(U_t))dt.\n\\] \\[\nh(l):=2l^2\\Phi\\left(-\\frac{Kl^3}{2}\\right).\n\\]\n\n\nこの \\(h\\) は \\(a(l)=0.574\\) を満たす点 \\(l\\) で最大化される．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal1998.html#footnotes",
    "href": "posts/2024/Review/Roberts-Rosenthal1998.html#footnotes",
    "title": "Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nその際なぜか (Gareth O. Roberts and Rosenthal, 1998) を引用．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html",
    "href": "posts/2024/Review/Tartero-Krauth2023.html",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html#背景",
    "href": "posts/2024/Review/Tartero-Krauth2023.html#背景",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "1 背景",
    "text": "1 背景\nKrauth は (Bernard et al., 2009) において event-chain Monte Carlo アルゴリズムを提案した．\nその後この手法は一般の連続系に適用できる形に拡張され，連続スピン系などにも適用されている (酒井佑士, 2017, p. 24)．"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html#本論",
    "href": "posts/2024/Review/Tartero-Krauth2023.html#本論",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "2 本論",
    "text": "2 本論\n１粒子が次の one-dimensional anharmonic potential \\[\nU_{24}(x)=\\frac{x^2}{2}+\\frac{x^4}{4}\n\\] に従って運動する場合を考える．\n\n\n\nポテンシャル \\(U\\) のプロット\n\n\nこのポテンシャルに関する Boltzmann-Gibbs 分布 \\(\\pi_{24}\\) は次の通り：\n\n\n\nポテンシャル \\(U\\) が定める Botlzmann-Gibbs 分布のプロット\n\n\n\\[\nZ(\\beta)=\\int^\\infty_{-\\infty}\\pi_{24}(x)\\,dx=\\frac{e^{\\frac{\\beta}{8}}}{\\sqrt{2}}K_{1/4}\\left(\\frac{\\beta}{8}\\right)\n\\]\n１粒子非調和振動子は，孤立系としては決定論的な力学系であるが，熱浴に接続すると区分確定的な系になる (Davis, 1984)．この系からは，Newton 力学と熱浴との相互作用の MD モデリングによって，\\(\\pi_{24}\\) からサンプリングすることができる．\nこの系を「熱浴に接続する」とは，速度を交換出来る仕組みを導入すれば良い．例えば振動中心 \\(x=0\\) に半透性の弾性的な物体（thermostat）を固定し，振動子が \\(x=0\\) を通る度に確率 \\(\\frac{1}{2}\\) で弾性衝突して速度を交換する系などとして考えられる．この半透性で弾性的な物体は無視できる幅で振動しながら熱浴と接続されており，温度が一定に保たれているとする．1\nこの系を十分に放置すると，粒子の位置 \\(x\\) は Boltzmann-Gibbs 分布 \\(\\pi_{24}\\,\\propto\\,e^{-\\beta U_{24}}\\) に従う．\n\nこれを，粒子の位置 \\(x\\) を力学に基づいて追跡することで \\(\\pi_{24}\\) からサンプリングすることも考えられる．これを MD 法という．\n一方で Monte Carlo 法によりサンプリングすることが出来る．ここでは Gauss 分布の方が裾が重いので，これを提案分布とした棄却法によりサンプリングできる．積分を実行する場合も重点サンプリング法の考え方で実行できる．\n(Metropolis et al., 1953) などではこの提案分布と対象分布の距離が離れすぎていることが問題なのであった．そこで Markov 連鎖を用いるのである．Metropolis 法とは，正方形の範囲への一様ランダムウォークから，採択-棄却のフィルターを通じて Markov 核を構成する普遍的手続きだと言える．\n目標分布が因子分解可能であるとき，別のフィルター factorized Metropolis filter を通じても詳細釣り合い条件から Markov 核が構成できる．これは独立な乱数を生成して，全員可決したときに採択する，というよりスピーディーな棄却手続きが可能で consensus と呼ばれている．\n(Chen et al., 1999), (Diaconis et al., 2000) によって詳細釣り合い条件を破る方法 lifting が提案された．これは補助変数法により，同じ方向に進み続けるように設計された Markov 連鎖である．2\nlifting を通じて，アルゴリズムを連続時間ベースにできる．これは，いちいち細かいステップサイズで「提案」するのではなく，次に棄却される位置と時刻をサンプリングすれば良い，というのである．event-driven にすることで連続時間ベースのシミュレーションが可能になるのである．\nこの event-driven なバージョンでも，factorized filter と consensus を応用できる．\n\\(U\\) の評価が高価である場合，これよりも採択率が下がるような bounding potential \\(\\widehat{U}\\) を用いることができる．これをフィルターを狭めるという感覚で thining (Lewis and Shedler, 1979) という．棄却された場合に，本格的に \\(U\\) を評価する．"
  },
  {
    "objectID": "posts/2024/Review/Tartero-Krauth2023.html#footnotes",
    "href": "posts/2024/Review/Tartero-Krauth2023.html#footnotes",
    "title": "Tartero and Krauth (2023) Concepts in Monte Carlo Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこのとき，thermostat は Maxwell 分布に従うわけではない点に注意．熱浴内の粒子と違って，thermostat は \\(x=0\\) に固定されているため（例えば無視できる幅で振動しているとする），Maxwell boundary condition と呼ばれる分布に従い，比較的に簡単にサンプリングできる．↩︎\n元の状態空間を拡張した上で詳細釣り合い条件の破れを導入する手法を総称して lifting と呼ぶ (酒井佑士, 2017)．特に２値空間との席をとってリフティングをする手法は (Turitsyn et al., 2011) による方法で，(酒井佑士, 2017, pp. 24–) で詳細な解析が与えられている．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#概要",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#概要",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "1 概要",
    "text": "1 概要\n\nマルチンゲールによる導出\nLyapunov 型の条件が簡単に十分条件を与えるような，再帰時刻による取り扱い\n\n(Kulik, 2018, pp. 45–) では，安定性の定理の証明に，マルチンゲールを用いた議論を用いている．エルゴード性を持つ Markov 連鎖は必ず\n\n局所的な集合上で良い攪拌性を持ち，\nその他の点に行ってしまった場合でも，「十分早く」その局所的な集合に戻ってくる\n\nという２つのモードを持つ．これを別々に解析する見通しの良い議論を与えてくれるのがマルチンゲールによる議論であるとしている (Kulik, 2018, p. 71) が，似たような議論をしているのが本論文 (Butkovsky and Veretennikov, 2013) である．1"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#背景",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#背景",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "2 背景",
    "text": "2 背景\n\n一様エルゴード性を strongly ergodic とも呼んでいる： \\[\n\\sup_{x\\in E}\\|P^n(x,-)-\\pi\\|_\\mathrm{TV}\\le Ce^{-\\lambda n}.\n\\]\n一方で，各点 \\(x\\in E\\) で \\[\n\\|P^n(x,-)-\\pi\\|_\\mathrm{TV}\\to0\n\\] が成り立つことを weakly ergodic と呼んでいる．\n\n本論文では，\\(\\lambda\\) を推定する (Diaconis and Stroock, 1991) 理論を，weakly ergodic の場合と非対称な場合に拡張する．\nすると，(Diaconis and Stroock, 1991) 理論では遷移確率核 \\(P\\) のスペクトルギャップであった \\(\\lambda\\) は，一般の設定の下でもある一般化した半群生成作用素のスペクトル半径に関係することがわかった．\n\n2.1 (Diaconis and Stroock, 1991) 理論\n一様エルゴード性の収束速度 \\(\\lambda\\) を定量化するアプローチの１つ．\n\n\n\n\n\n\n定理\n\n\n\n有限状態空間 \\(E\\) 上の \\(P\\)-一様 Markov 連鎖は，既約かつ対称ならば， \\[\n\\lambda&lt;\\log\\operatorname{Gap}(P)\n\\] \\[\n\\operatorname{Gap}(P):=\\max\\left\\{\\lvert\\lambda\\rvert\\in\\mathbb{R}_+\\mid 1&gt;\\lambda\\in\\mathrm{Sp}(P)\\right\\}\n\\]\n\n\n\n対称ならば \\(P=P^*\\)．\nスペクトルギャップは一般の正作用素に定義できる．\n\n\n\n2.2 (Vaserstein, 1969) による最適カップリングの構成\nこれを一般化したという．\n最適な Markov カップリングよりも，カップリング確率が高いカップリングがあるらしい（しかし Markov にならない）．この稿 に書いた．\n\n\n2.3 Lyapunov 型の条件\n(Douc et al., 2004), (Kalashinikov, 1973), (Lamperti, 1960), (Rosenthal, 2002), (Tweedie, 1981) による．"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#本論",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#本論",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "3 本論",
    "text": "3 本論\n\n3.1 カップリング補題\n本論に入る前に，次の結果を準備している：\n\n\n\n\n\n\n補題（カップリング不等式の評価）2\n\n\n\n\\(X_1,X_2\\) を確率核 \\(P\\) を持つ Markov 連鎖，\\(Z=(X_1,X_2)\\) をその最適 Markov カップリングを \\[\nX_n^1=:\\xi_n1_{\\left\\{\\zeta_n=0\\right\\}}+\\eta_n^11_{\\left\\{\\zeta_n=1\\right\\}}\n\\] \\[\nX_n^2=:\\xi_n1_{\\left\\{\\zeta_n=0\\right\\}}+\\eta_n^21_{\\left\\{\\zeta_n=1\\right\\}}\n\\] とする．このとき， \\[\n\\operatorname{P}[X_n^1\\ne X_n^2]\\le\\biggr(1-p_0\\biggl)\\operatorname{E}\\left[\\prod_{k=0}^{n-1}\\biggr(1-p(\\eta_k^1,\\eta_k^2)\\biggl)\\right]\n\\] \\[\np(x_1,x_2):=1-\\frac{1}{2}\\|P(x_1,-)-P(x_2,-)\\|_\\mathrm{TV}\n\\] \\[\np_0:=1-\\frac{1}{2}\\|\\operatorname{P}^{X_0^1}-\\operatorname{P}^{X_0^2}\\|_\\mathrm{TV}=\\operatorname{P}[X_0^1=X_0^2]\n\\] が成り立つ．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n基本的な考え方は \\[\n\\begin{align*}\n    \\operatorname{P}[X^1_n\\ne X_n^2]&\\le\\operatorname{P}[\\zeta_0=1,\\zeta_1=1,\\cdots,\\zeta_{n}=1]\\\\\n    &=\\operatorname{E}[1_{\\left\\{\\zeta_0=1\\right\\}}1_{\\left\\{\\zeta_1=1\\right\\}}\\cdots1_{\\left\\{\\zeta_{n}=1\\right\\}}]\n\\end{align*}\n\\] である．これは，\\(X_n^1\\ne X_n^2\\) ならば \\(\\zeta_n=1\\) が必要であるため， \\[\n\\begin{align*}\n    \\left\\{X_n^1\\ne X_n^2\\right\\}&\\subset\\left\\{\\zeta_n=1\\right\\}\\\\\n    &=\\left\\{\\zeta_0=1,\\zeta_1=1,\\cdots,\\zeta_{n}=1\\right\\}\n\\end{align*}\n\\] であるが，逆は必ずしも成り立たないためである．\nこれに，\\(\\mathcal{F}_n:=\\sigma[X_n^1,X_n^2]\\) について， \\[\n\\begin{align*}\n    &\\operatorname{E}\\left[\\prod_{i=k}^n1_{\\left\\{\\zeta_i=1\\right\\}}\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}\\left[\\prod_{i=k-1}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta_i^2)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]1_{\\left\\{\\zeta_{k-1}=1\\right\\}}\n\\end{align*}\n\\] の \\(k=1\\) の場合を併せて結論を得る．この等式自体は降下法により示す．\n\\(k=n\\) の場合の式 \\[\n\\operatorname{E}[1_{\\left\\{\\zeta_n=1\\right\\}}\\,|\\,\\mathcal{F}_{n-1}]=\\biggr(1-p(\\eta_{n-1}^1,\\eta_{n-1}^2)\\biggl)1_{\\left\\{\\zeta_{n-1}=1\\right\\}}\n\\] は明らかである．\\(k&lt;n\\) の場合，帰納法の仮定と，\\(\\mathcal{F}_{k-1}\\) の下で \\(\\zeta_k\\) と \\((\\eta_k^1,\\eta_k^2)\\) は条件付き独立であるから，次のように式変形できる： \\[\n\\begin{align*}\n    &\\quad\\operatorname{E}\\left[\\prod_{i=k}^n1_{\\left\\{\\zeta_i=1\\right\\}}\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}\\left[1_{\\left\\{\\zeta_k=1\\right\\}}\\operatorname{E}\\left[\\prod_{i=k+1}^n1_{\\left\\{\\zeta_i=1\\right\\}}\\,\\middle|\\,\\mathcal{F}_k\\right]\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}\\left[1_{\\left\\{\\zeta_k=1\\right\\}}\\operatorname{E}\\left[\\prod_{i=k}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_k\\right]\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=\\operatorname{E}[1_{\\left\\{\\zeta_k=1\\right\\}}\\,|\\,\\mathcal{F}_{k-1}]\\operatorname{E}\\left[\\prod_{i=k}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=1_{\\left\\{\\zeta_{k-1}=1\\right\\}}\\biggr(1-p(\\eta^1_{k-1},\\eta^2_{k-1})\\biggl)\\operatorname{E}\\left[\\prod_{i=k}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n    &=1_{\\left\\{\\zeta_{k-1}=1\\right\\}}\\operatorname{E}\\left[\\prod_{i=k-1}^{n-1}\\biggr(1-p(\\eta^1_i,\\eta^2_i)\\biggl)\\,\\middle|\\,\\mathcal{F}_{k-1}\\right]\\\\\n\\end{align*}\n\\]\n\n\n\nこの補題により，確率核 \\(P\\) を共有する２つの Markov 過程 \\((X_n^1),(X_n^2)\\) が与えられたとき，これらのカップリング \\((\\widetilde{X}^1_n),(\\widetilde{X}^2_n)\\) を同一の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上に構成し，\\((\\widetilde{X}_n^1)\\overset{\\text{d}}{=}(X_n^1),(\\widetilde{X}_n^2)\\overset{\\text{d}}{=}(X_n^2)\\) であるが，全変動距離を \\(\\operatorname{P}\\) によって評価できるようになる．\n\n\n3.2 スペクトルギャップによる一様エルゴード速度\n次の積分作用素 \\(A:\\mathcal{L}_b(E^2)\\to\\mathcal{L}_b(E^2)\\) を考える： \\[\nAf(x):=\\biggr(1-p(x)\\biggl)\\operatorname{E}[f(\\eta_1)\\,|\\,\\eta_0=x]\n\\] \\[\n\\eta_i=\\begin{pmatrix}\\eta_i^1\\\\\\eta_i^2\\end{pmatrix},\\quad x=\\begin{pmatrix}x^1\\\\x^2\\end{pmatrix}.\n\\] このスペクトル半径 \\[\nr(A):=\\limsup_{n\\to\\infty}\\sqrt[n]{\\|A^n\\|}\n\\] が一様エルゴード性を引き起こすのである．\n\n\n\n\n\n\n証明の概略\n\n\n\n\n\nなお，この定義式は，後述の \\(r(A)\\le1\\) と併せると，任意の \\(\\epsilon&gt;0\\) に対して，ある \\(C&gt;0\\) が存在して，\\(r(A)\\le r(A)^{1-\\epsilon}\\) であるから， \\[\n\\begin{align*}\n    \\|A^n\\|&\\le C\\left(r(A)^{1-\\epsilon}\\right)^n\\\\\n    &=Ce^{n(1-\\epsilon)\\log r(A)}\\\\\n    &=Ce^{-n(1-\\epsilon)\\log\\lvert r(A)\\rvert}\n\\end{align*}\n\\] を含意することに注意．\n実は次の定理の証明は，次の不等式を導いているのみである： \\[\n\\frac{1}{2}\\|P^n(x,-)-P^n(y,-)\\|_\\mathrm{TV}\\le\\|A^{n}\\|.\n\\]\nこの２式より，直ちに証明が完成する．\n\n\n\nただし，作用素ノルム \\(\\|A^n\\|\\) は任意の \\(\\|1\\|=1\\) を満たす関数ノルムに関して構成して良い．\\(C_b(E^2)\\) を考えることも有用である．\nいずれにしろ， \\[\n\\|A\\|_\\infty=1-\\inf_{x\\in E^2}p(x)\\le1\n\\] という条件式は変わらない．作用素ノルムの劣乗法性から \\[\nr(A)=\\limsup_{n\\to\\infty}\\sqrt[n]{\\|A^n\\|_\\infty}\\le\\|A\\|_\\infty\\le1\n\\] であることに注意．\n\n\n\n\n\n\n定理3\n\n\n\n\n\\(A:\\mathcal{L}_b(E^2)\\to\\mathcal{L}_b(E^2)\\) は有界作用素であり，\\(r(A)\\le1\\) を満たす．\n\\(r(A)&lt;1\\) ならば，\\(X\\) はただ一つの不変確率測度を持ち，任意の \\(\\epsilon&gt;0\\) に対して，初期分布 \\(X_0^1,X_0^2\\) に依らないある \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le C(1-p_0)e^{-n\\lvert\\log r(A)\\rvert(1-\\epsilon)}.\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n一般に，任意の \\(f\\in\\mathcal{L}_b(E^2)\\) に対して，\\((\\eta_k)\\) のMarkov性より， \\[\n\\begin{align*}\n    \\operatorname{E}\\left[f(\\eta_n)\\prod_{i=0}^{n-1}\\biggr(1-p(\\eta_i)\\biggl)\\right]&=\\operatorname{E}\\left[\\operatorname{E}\\left[f(\\eta_n)\\prod_{i=0}^{n-1}\\biggr(1-p(\\eta_i)\\biggl)\\,\\middle|\\,\\eta_0,\\cdots,\\eta_{n-1}\\right]\\right]\\\\\n    &=\\operatorname{E}\\left[\\prod_{i=0}^{n-2}\\biggr(1-p(\\eta_i)\\biggl)\\cdot\\biggr(1-p(\\eta_{n-1})\\biggl)\\operatorname{E}[f(\\eta_n)\\,|\\,\\eta_{n-1}]\\right]\\\\\n    &=\\operatorname{E}\\left[\\prod_{i=0}^{n-2}\\biggr(1-p(\\eta_i)\\biggl)\\cdot Af(\\eta_{n-1})\\right]=\\operatorname{E}[A^nf(\\eta_0)].\n\\end{align*}\n\\] これより，\\(\\left\\{\\eta_n=0\\right\\}\\supset\\left\\{\\prod_{k=0}^{n-1}\\biggr(1-p(Z_k)\\biggl)=0\\right\\}\\) に注意すれば，次の評価を得る： \\[\n\\begin{align*}\n    \\|P^n(x,-)-P^n(y,-)\\|_\\mathrm{TV}&\\le2\\operatorname{P}_{(x,y)}^Q[X_n\\ne Y_n]\\le2\\operatorname{E}_{(x,y)}^{Q_\\perp}\\left[\\prod_{k=0}^{n-1}\\biggr(1-p(Z_k)\\biggl)\\right]\\\\\n    &=2\\operatorname{E}_{(x,y)}^{Q_\\perp}\\left[\\delta_1(\\eta_n)\\prod_{k=0}^{n-1}\\biggr(1-p(Z_k)\\biggl)\\right]=2\\operatorname{E}_{(x,y)}^{Q_\\perp}[A^{n-1}\\delta_1(\\eta_1)](1-p(\\eta_0))\\\\\n    &\\le2\\delta_{x,y}\\|A^{n-1}\\|_\\infty\n\\end{align*}\n\\] よって，\\(\\frac{1}{2}\\|P^n(x,-)-P^n(y,-)\\|\\) はちょうど作用素ノルム \\(\\|A^{n-1}\\|\\) を評価する問題に帰着する．\n\n\n\n\n\n3.3 再帰性と非一様エルゴード速度\n\\[\n\\|A\\|_\\infty=1-\\inf_{x\\in E^2}p(x)=1\n\\] の場合に当たるものであるが， \\[\nK(\\epsilon):=\\left\\{x\\in E^2\\mid p(x)\\ge\\epsilon\\right\\}\n\\] に無限回再帰するとき，その頻度に依存して，指数的か多項式的か決まる．4 この頻度は，次の帰着時間 \\(\\tau^B,T^B\\) の積率条件で記述される： \\[\n\\tau^B:=\\inf\\left\\{n\\ge 1\\mid\\eta_n\\in B\\right\\}\n\\] \\[\nT^B:=\\inf\\left\\{n\\ge 1\\mid(X_n^1,X_n^2)\\in B\\right\\}\n\\] ただし，\\(B\\in\\mathcal{E}^{\\otimes2}\\)．\n\n\n\n\n\n\n命題2.1\n\n\n\nある \\(\\epsilon,\\lambda,M&gt;0\\) と \\(\\mathcal{E}^{\\otimes2}\\ni B\\subset K(\\epsilon)\\) について，次が成り立つとする：\n\n\\(Q:=\\operatorname{E}[e^{\\lambda\\tau^B}]&lt;\\infty\\)．\n任意の \\(x\\in B\\) について，\\(\\operatorname{E}_x[e^{\\lambda\\tau^B}]\\le M\\)．\n\nこのとき，\\(X\\) はただ一つの不変確率測度 \\(\\pi\\) をもち，またある初期分布 \\((X_0^1,X_0^2)\\) に依らない \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le CQe^{-n\\theta}\n\\] \\[\n\\theta:=\\frac{\\lvert\\log(1-\\epsilon)\\rvert\\lambda}{\\log M+\\lvert\\log(1-\\epsilon)\\rvert}\n\\]\n\n\n\n\n\n\n\n\n定理2.2\n\n\n\nある \\(\\epsilon&gt;0,\\lambda&gt;0,M&gt;0,\\mathcal{E}^{\\otimes2}\\ni B\\subset K(\\epsilon)\\) について，次が成り立つとする：\n\n\\(Q_2:=\\operatorname{E}[e^{\\lambda T^B}]&lt;\\infty\\)．\n任意の \\(x\\in B\\setminus K(1)\\) について，\\(\\operatorname{E}_x[e^{\\lambda T^B}]&lt;M\\)．\n\nこのとき，過程 \\(X\\) はただ一つの不変確率分布 \\(\\pi\\) をもち，ある初期分布 \\((X_0^1,X_0^2)\\) に依存しない定数 \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le CQ_2e^{-n\\theta_1}\n\\] \\[\n\\theta_1:=\\frac{\\lvert\\log(1-\\epsilon)\\rvert\\lambda}{\\log M+3\\lvert\\log(1-\\epsilon)\\rvert}.\n\\]\n\n\n\n\n\n\n\n\n定理2.3\n\n\n\nある \\(\\epsilon&gt;0,\\lambda\\ge1,M&gt;0,\\mathcal{E}^{\\otimes2}\\ni B\\subset K(\\epsilon)\\) について，次が成り立つとする：\n\n\\(Q_3:=\\operatorname{E}[(T^B)^\\lambda]&lt;\\infty\\)．\n任意の \\(x\\in B\\setminus K(1)\\) に対して，\\(\\operatorname{E}_x[(T^B)^\\lambda]&lt;M\\)．\n\nこのとき，過程 \\(X\\) はただ一つの不変確率分布 \\(\\pi\\) をもち，任意の \\(\\lambda_1\\in(0,\\lambda)\\) に対して，ある初期分布 \\((X_0^1,X_0^2)\\) に依存しない定数 \\(C&gt;0\\) が存在して， \\[\n\\|\\operatorname{P}^{X_n^1}-\\operatorname{P}^{X_n^2}\\|_\\mathrm{TV}\\le CQ_3n^{-\\lambda_1}\n\\]"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#例",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#例",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "4 例",
    "text": "4 例\nあ"
  },
  {
    "objectID": "posts/2024/Review/Butkovsky-Veretennikov2013.html#footnotes",
    "href": "posts/2024/Review/Butkovsky-Veretennikov2013.html#footnotes",
    "title": "Butkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nそれと (Durmus et al., 2016) を挙げている．↩︎\n(Butkovsky and Veretennikov, 2013, pp. 3521–22) 補題2.2↩︎\n(Butkovsky and Veretennikov, 2013) 定理2.1．↩︎\n前節から判る通り，局所Dobrushin条件を満たす集合（small set ともいう）上では常に指数収束である．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Dai+2019.html",
    "href": "posts/2024/Review/Dai+2019.html",
    "title": "Dai+ (2019) Monte Carlo Fusion",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\nReferences\n\nDai, H., Pollock, M., and Roberts, G. (2019). Monte Carlo Fusion. Journal of Applied Probability, 56(1), 174–191."
  },
  {
    "objectID": "posts/2024/AI/RL.html",
    "href": "posts/2024/AI/RL.html",
    "title": "強化学習",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/AI/RL.html#導入",
    "href": "posts/2024/AI/RL.html#導入",
    "title": "強化学習",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 他の機械学習手法との違い\n強化学習は機械学習の３分類の１つとして「他の２類型（教師あり・教師なし学習）とは大きく文脈が違う」という注記と共によく紹介される．それもその通り．強化学習は，エージェントが世界の中でどのように行動するかを，環境との相互作用を通じて自律的に学ぶ，という人工知能分野の問題設定から生じた学問である (Sutton and Barto, 2018)．\n強化学習の設定にはいくつか特徴がある．\n\n試行錯誤の中で学ぶこと：教師あり学習のように，報酬を最大化するように学んでいくが，学習データというものはなく，試行錯誤の中で学ぶ必要がある．\n報酬は遅れてくるものもあること：行動の結果がすぐに報酬として返ってくるわけではなく，以前の行動が未来の報酬に影響を与えることがあり，それを踏まえて学習をすることが求められる．\n\nすると，強化学習は専ら動的計画法の議論が中心となる．\n\n\n1.2 強化学習の応用\n強化学習は，部分的に観測されている Markov 決定過程の最適制御として理解される．\n在庫管理 (Van Roy et al., 1997)，動的なチャンネルの割り当て (Singh and Bertsekas, 1996)，エレベータ制御 (Crites and Barto, 1998)，テーブルゲーム (Silver et al., 2018)，気候変動対策 (Rolnick et al., 2022) などにも用いられている．\nまた，深層学習と組み合わせることで，DeepMind の AlphaGo (Silver et al., 2016) と AlphaGoZero (Silver et al., 2017) は囲碁において人類の追随を許さない実力をつけた．\n今後も，人間のフィードバックによる強化学習 (RLHF: Reinforcement Learning through Human Feedback) (Christiano et al., 2017) や GNN のトレーニングなど，他の機械学習手法と組み合わせることでより大きな AI システムを作るにあたって，強化学習は必要不可欠な立場を占めていくだろう．\n\n\n1.3 歴史"
  },
  {
    "objectID": "posts/2024/AI/RL.html#markov-決定過程",
    "href": "posts/2024/AI/RL.html#markov-決定過程",
    "title": "強化学習",
    "section": "2 Markov 決定過程",
    "text": "2 Markov 決定過程\n\n\n\n\n\n\n定義 (Markov decision process, policy)1\n\n\n\n\n状態空間 \\(\\mathcal{X}\\)，行動空間 \\(\\mathcal{A}\\) という２つの可測空間と，確率核 \\[\nP:\\mathcal{X}\\times\\mathcal{A}\\to\\mathcal{X}\\times\\mathbb{R}\n\\] との3-組を Markov 決定過程 という．\n\\(\\mathcal{X},\\mathcal{A}\\) がいずれも有限集合であるとき，MDP \\((\\mathcal{X},\\mathcal{A},P)\\) を 有限 であるという．\n確率核 \\(\\pi:\\mathcal{X}\\to\\mathcal{A}\\) を 方策 という．\n\n\n\n多くの場合，\\(P(x,a)\\in\\mathcal{P}(\\mathcal{X}\\times\\mathbb{R})\\) は直積 \\[\nP(x,a)=P_{\\mathcal{X}}(x,a)\\otimes P_{\\mathcal{R}}(x,a)\n\\] で与えられるとする．"
  },
  {
    "objectID": "posts/2024/AI/RL.html#q-学習",
    "href": "posts/2024/AI/RL.html#q-学習",
    "title": "強化学習",
    "section": "3 \\(Q\\)-学習",
    "text": "3 \\(Q\\)-学習\n強化学習の最も標準的かつ古典的なアルゴリズムが，\\(Q\\)-学習 (Watkins, 1989), (Watkins and Dayan, 1992) と，その派生アルゴリズムである SARSA である (Powell, 2011, p. 122)．\n\n3.1 TD 学習\n推移確率が未知である場合，これをシミュレーションによって推定することができる．この設定では 時間差分学習 (Temporal Difference Learning) とも呼ばれる (Sutton, 1988)．"
  },
  {
    "objectID": "posts/2024/AI/RL.html#footnotes",
    "href": "posts/2024/AI/RL.html#footnotes",
    "title": "強化学習",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bellemare et al., 2023, p. 15), (Sutton and Barto, 2018, p. 48)．↩︎"
  },
  {
    "objectID": "posts/2024/AI/LLM.html",
    "href": "posts/2024/AI/LLM.html",
    "title": "大規模言語モデル",
    "section": "",
    "text": "Mistral AI は 2023 年に Google DeepMind の研究者１人と Meta Platform の元研究者２人によって設立されたフランス企業で，オープンソースでの大規模言語モデルの開発を行っている．\nLLama 2 70B モデルより性能が良いとされているが，ヨーロッパ系の言語５言語のみに特化しているモデルである．\nMistral Cookbook ではコミュニティによる Mistral AI の言語モデルの利用事例が公開されている．このいくつかを本記事では見て遊んでいく．\nAPI の利用には 登録が必要 であるが，サブスクリプションではなくて利用量に応じた課金方式である．"
  },
  {
    "objectID": "posts/2024/AI/LLM.html#rag",
    "href": "posts/2024/AI/LLM.html#rag",
    "title": "大規模言語モデル",
    "section": "0.1 RAG",
    "text": "0.1 RAG\nRAG (Retrieval-Augmented Generation) (Lewis et al., 2020) は，言語モデルと情報検索を次のように組み合わせることで，質問応答などのタスクでの性能を上げる手法である：\n\n情報検索を行い，関連があると思われる情報を知識ベースから抽出する．\n抽出した情報をプロンプトに含めて言語モデルに入力する．\n\nこれを Mistral を用いて実装してみる．\n\n0.1.1 Import needed packages\nThe first step is to install the needed packages mistralai and faiss-cpu and import the needed packages:\n! pip install faiss-cpu==1.7.4 mistralai==0.0.12\n\n\nCode\nfrom mistralai.client import MistralClient, ChatMessage\nimport requests\nimport numpy as np\nimport faiss\nimport os\nfrom getpass import getpass\n\napi_key= getpass(\"Type your API Key\")\nclient = MistralClient(api_key=api_key)\n\n\n\n\n0.1.2 外部データの下処理\nPaul Graham のエッセイを知識ベースとして用いることを考える．\n\n\nCode\nresponse = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt')\ntext = response.text\nlen(text)\n\n\nf = open('essay.txt', 'w')\nf.write(text)\nf.close()\n関連する情報の抽出を効率的に行うため，外部情報を小さなチャンクに分割することを考える．\nIn a RAG system, it is crucial to split the document into smaller chunks so that it’s more effective to identify and retrieve the most relevant information in the retrieval process later. In this example, we simply split our text by character, combine 2048 characters into each chunk, and we get 37 chunks.\n\n\nCode\nchunk_size = 2048\nchunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n\n\n\n\nCode\nlen(chunks)\n\n\n\n0.1.2.1 Considerations:\n\nChunk size: Depending on your specific use case, it may be necessary to customize or experiment with different chunk sizes and chunk overlap to achieve optimal performance in RAG. For example, smaller chunks can be more beneficial in retrieval processes, as larger text chunks often contain filler text that can obscure the semantic representation. As such, using smaller text chunks in the retrieval process can enable the RAG system to identify and extract relevant information more effectively and accurately. However, it’s worth considering the trade-offs that come with using smaller chunks, such as increasing processing time and computational resources.\nHow to split: While the simplest method is to split the text by character, there are other options depending on the use case and document structure. For example, to avoid exceeding token limits in API calls, it may be necessary to split the text by tokens. To maintain the cohesiveness of the chunks, it can be useful to split the text by sentences, paragraphs, or HTML headers. If working with code, it’s often recommended to split by meaningful code chunks for example using an Abstract Syntax Tree (AST) parser.\n\n\n\n\n0.1.3 Create embeddings for each text chunk\nFor each text chunk, we then need to create text embeddings, which are numeric representations of the text in the vector space. Words with similar meanings are expected to be in closer proximity or have a shorter distance in the vector space. To create an embedding, use Mistral’s embeddings API endpoint and the embedding model mistral-embed. We create a get_text_embedding to get the embedding from a single text chunk and then we use list comprehension to get text embeddings for all text chunks.\n\n\nCode\ndef get_text_embedding(input):\n    embeddings_batch_response = client.embeddings(\n          model=\"mistral-embed\",\n          input=input\n      )\n    return embeddings_batch_response.data[0].embedding\n\n\n\n\nCode\ntext_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n\n\n\n\nCode\ntext_embeddings.shape\n\n\n\n\nCode\ntext_embeddings\n\n\n\n\n0.1.4 Load into a vector database\nOnce we get the text embeddings, a common practice is to store them in a vector database for efficient processing and retrieval. There are several vector database to choose from. In our simple example, we are using an open-source vector database Faiss, which allows for efficient similarity search.\nWith Faiss, we instantiate an instance of the Index class, which defines the indexing structure of the vector database. We then add the text embeddings to this indexing structure.\n\n\nCode\nd = text_embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(text_embeddings)\n\n\n\n0.1.4.1 Considerations:\n\nVector database: When selecting a vector database, there are several factors to consider including speed, scalability, cloud management, advanced filtering, and open-source vs. closed-source.\n\n\n\n\n0.1.5 Create embeddings for a question\nWhenever users ask a question, we also need to create embeddings for this question using the same embedding models as before.\n\n\nCode\nquestion = \"What were the two main things the author worked on before college?\"\nquestion_embeddings = np.array([get_text_embedding(question)])\nquestion_embeddings.shape\n\n\n\n\nCode\nquestion_embeddings\n\n\n\n0.1.5.1 Considerations:\n\nHypothetical Document Embeddings (HyDE): In some cases, the user’s question might not be the most relevant query to use for identifying the relevant context. Instead, it maybe more effective to generate a hypothetical answer or a hypothetical document based on the user’s query and use the embeddings of the generated text to retrieve similar text chunks.\n\n\n\n\n0.1.6 Retrieve similar chunks from the vector database\nWe can perform a search on the vector database with index.search, which takes two arguments: the first is the vector of the question embeddings, and the second is the number of similar vectors to retrieve. This function returns the distances and the indices of the most similar vectors to the question vector in the vector database. Then based on the returned indices, we can retrieve the actual relevant text chunks that correspond to those indices.\n\n\nCode\nD, I = index.search(question_embeddings, k=2)\nprint(I)\n\n\n\n\nCode\nretrieved_chunk = [chunks[i] for i in I.tolist()[0]]\nprint(retrieved_chunk)\n\n\n\n0.1.6.1 Considerations:\n\nRetrieval methods: There are a lot different retrieval strategies. In our example, we are showing a simple similarity search with embeddings. Sometimes when there is metadata available for the data, it’s better to filter the data based on the metadata first before performing similarity search. There are also other statistical retrieval methods like TF-IDF and BM25 that use frequency and distribution of terms in the document to identify relevant text chunks.\nRetrieved document: Do we always retrieve individual text chunk as it is? Not always.\n\nSometimes, we would like to include more context around the actual retrieved text chunk. We call the actual retrieve text chunk “child chunk” and our goal is to retrieve a larger “parent chunk” that the “child chunk” belongs to.\nOn occasion, we might also want to provide weights to our retrieve documents. For example, a time-weighted approach would help us retrieve the most recent document.\nOne common issue in the retrieval process is the “lost in the middle” problem where the information in the middle of a long context gets lost. Our models have tried to mitigate this issue. For example, in the passkey task, our models have demonstrated the ability to find a “needle in a haystack” by retrieving a randomly inserted passkey within a long prompt, up to 32k context length. However, it is worth considering experimenting with reordering the document to determine if placing the most relevant chunks at the beginning and end leads to improved results.\n\n\n\n\n\n0.1.7 Combine context and question in a prompt and generate response\nFinally, we can offer the retrieved text chunks as the context information within the prompt. Here is a prompt template where we can include both the retrieved text and user question in the prompt.\n\n\nCode\nprompt = f\"\"\"\nContext information is below.\n---------------------\n{retrieved_chunk}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {question}\nAnswer:\n\"\"\"\n\n\n\n\nCode\ndef run_mistral(user_message, model=\"mistral-medium-latest\"):\n    messages = [\n        ChatMessage(role=\"user\", content=user_message)\n    ]\n    chat_response = client.chat(\n        model=model,\n        messages=messages\n    )\n    return (chat_response.choices[0].message.content)\n\n\n\n\nCode\nrun_mistral(prompt)\n\n\n\n0.1.7.1 Considerations:\n\nPrompting techniques: Most of the prompting techniques can be used in developing a RAG system as well. For example, we can use few-shot learning to guide the model’s answers by providing a few examples. Additionally, we can explicitly instruct the model to format answers in a certain way.\n\nIn the next sections, we are going to show you how to do a similar basic RAG with some of the popular RAG frameworks. We will start with LlamaIndex and add other frameworks in the future."
  },
  {
    "objectID": "posts/2024/AI/LLM.html#langchain",
    "href": "posts/2024/AI/LLM.html#langchain",
    "title": "大規模言語モデル",
    "section": "0.2 LangChain",
    "text": "0.2 LangChain\n\n\nCode\n!pip install langchain langchain-mistralai==0.0.4\n\n\n\n\nCode\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_mistralai.chat_models import ChatMistralAI\nfrom langchain_mistralai.embeddings import MistralAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\n\n# Load data\nloader = TextLoader(\"essay.txt\")\ndocs = loader.load()\n# Split text into chunks\ntext_splitter = RecursiveCharacterTextSplitter()\ndocuments = text_splitter.split_documents(docs)\n# Define the embedding model\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n# Create the vector store\nvector = FAISS.from_documents(documents, embeddings)\n# Define a retriever interface\nretriever = vector.as_retriever()\n# Define LLM\nmodel = ChatMistralAI(mistral_api_key=api_key)\n# Define prompt template\nprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n\n&lt;context&gt;\n{context}\n&lt;/context&gt;\n\nQuestion: {input}\"\"\")\n\n# Create a retrieval chain to answer questions\ndocument_chain = create_stuff_documents_chain(model, prompt)\nretrieval_chain = create_retrieval_chain(retriever, document_chain)\nresponse = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\nprint(response[\"answer\"])"
  },
  {
    "objectID": "posts/2024/AI/LLM.html#llamaindex",
    "href": "posts/2024/AI/LLM.html#llamaindex",
    "title": "大規模言語モデル",
    "section": "0.3 LlamaIndex",
    "text": "0.3 LlamaIndex\n\n\nCode\n!pip install llama-index==0.10.13 llama-index-llms-mistralai==0.1.4 llama-index-embeddings-mistralai==0.1.3\n\n\n\n\nCode\nimport os\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.llms.mistralai import MistralAI\nfrom llama_index.embeddings.mistralai import MistralAIEmbedding\n\n# Load data\nreader = SimpleDirectoryReader(input_files=[\"essay.txt\"])\ndocuments = reader.load_data()\n# Define LLM and embedding model\nSettings.llm = MistralAI(model=\"mistral-medium\")\nSettings.embed_model = MistralAIEmbedding(model_name='mistral-embed')\n# Create vector store index\nindex = VectorStoreIndex.from_documents(documents)\n# Create query engine\nquery_engine = index.as_query_engine(similarity_top_k=2)\nresponse = query_engine.query(\n    \"What were the two main things the author worked on before college?\"\n)\nprint(str(response))"
  },
  {
    "objectID": "posts/2024/AI/BAI1_Dropout.html",
    "href": "posts/2024/AI/BAI1_Dropout.html",
    "title": "ベイズ機械学習１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n(Hinton et al., 2012), (Srivastava et al., 2014) による，ミニバッチごとに確率的に使わない結合を決定するという正則化の技法である．1"
  },
  {
    "objectID": "posts/2024/AI/BAI1_Dropout.html#dropout-による正則化",
    "href": "posts/2024/AI/BAI1_Dropout.html#dropout-による正則化",
    "title": "ベイズ機械学習１",
    "section": "1 Dropout による正則化",
    "text": "1 Dropout による正則化\n\n1.1 Bayes からの説明\nDropout による正則化は，Gauss 過程による近似とも見れ，Bayes 手法の持つ正則化効果と相通ずることが指摘されている (Gal and Ghahramani, 2016)．\n\n\n1.2 Monte Carlo Dropout\n(Gal and Ghahramani, 2016)"
  },
  {
    "objectID": "posts/2024/AI/BAI1_Dropout.html#footnotes",
    "href": "posts/2024/AI/BAI1_Dropout.html#footnotes",
    "title": "ベイズ機械学習１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGAN の数値実験 (Goodfellow et al., 2014, p. 6) にも，判別器を訓練するのに用いられている．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory.html",
    "href": "posts/2024/AI/Theory.html",
    "title": "統計的学習理論１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n機械学習と統計学に別を設けるならば，いずれもデータから構造を発見することを目標とするとしても，前者は明示的なプログラムを伴わない「自動化」を念頭におくものであると言える．この人間による介入をなるべく少なくしたいという志向が「学習」の名前に表れている．\nそれ故，機械学習の理論としては，通常の統計的決定理論の枠組みよりも，汎化性能に力点を置いたものとなっている．これを，径数模型の教師あり学習の場合に関して述べる．"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#機械学習の形式化",
    "href": "posts/2024/AI/Theory.html#機械学習の形式化",
    "title": "統計的学習理論１",
    "section": "1 機械学習の形式化",
    "text": "1 機械学習の形式化\n\n1.1 記法と用語1\n\nデータサイズを \\(n\\in\\mathbb{N}^+\\) で表す．\n訓練データ (sample) の全体を \\(S_n=\\{z_i\\}_{i=1}^n\\subset\\mathcal{X}\\times\\mathcal{Y}\\) と表す．2 \\(\\mathcal{X}\\) を入力空間，\\(\\mathcal{Y}\\) を出力空間と呼ぶ．3\n\\(\\mathcal{X},\\mathcal{Y}\\) はいずれも可測空間とし，可測関数 \\(h\\in\\mathcal{L}(\\mathcal{X},\\mathcal{Y})\\) を 推定量，部分集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) を 仮説集合 (hypothesis set) という．4\n\\(l(\\Delta_\\mathcal{Y})=\\{0\\}\\) を満たす関数 \\(l:\\mathcal{Y}^2\\to\\mathbb{R}_+\\) を 損失関数 という．5\n写像 \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) を（機械学習） アルゴリズム または学習者という．\n\n以降，データはある真の分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に従うものとし，\\((X,Y)\\sim\\mathbb{P}\\) と表す．サンプル \\(S_n=\\{z_i\\}_{i=1}^n\\) は \\((X,Y)\\sim\\mathbb{P}\\) の独立同分布な複製と仮定する．6\n\n\n\n\n\n\n損失関数の例\n\n\n\n\n\n\n\\(l:=1_{\\Delta_\\mathcal{Y}^\\complement}\\) は 0-1損失 と呼ばれ，主に分類問題で使われる．これについて，汎化誤差とは，7 \\[\nR(h)=\\mathbb{E}[l(h(X),Y)]=\\mathbb{P}[h(X)\\ne Y]\n\\]\n\\(\\mathcal{Y}=\\mathbb{R}^d\\) とし，\\(l(y_1,y_2)=\\|y_1-y_2\\|^2_2\\) とした場合を 二乗損失 といい，主に回帰問題の最小二乗法などで用いられる．\n\n\n\n\n\n\n1.2 汎化ギャップ\n\n\n\n\n\n\n(generalization) error, training error8\n\n\n\n\n定義 1 \\(l:\\mathcal{Y}^2\\to\\mathbb{R}_+\\) を損失関数とする．\n\n仮説 \\(h\\in\\mathcal{H}\\) の （汎化）誤差 または 危険 または 予測損失 とは， \\[\nR(h):=\\mathbb{E}[l(h(X),Y)]\n\\] をいう．\n仮説 \\(h\\in\\mathcal{H}\\) のサンプル \\(S_n=\\{(x_i,y_i)\\}_{i=1}^n\\) に関する 訓練誤差 または 経験損失 とは， \\[\n\\widehat{R}_n(h):=\\frac{1}{n}\\sum_{i=1}^n l(h(x_i),y_i)\n\\] をいう．\n差 \\(\\widehat{R}_n(h)-R(h)\\) を 汎化ギャップ という．\n\n\n\n\nアルゴリズム \\(A\\) にとって，汎化誤差は不可知であるが，訓練誤差は計算可能である．データが独立同分布に従うとする場合，経験損失は予測損失の不偏推定量であり，9 \\(n\\to\\infty\\) の漸近論もすでに準備が出来ている．\n従って，不可知である予測損失の最小化の代わりに，経験損失を最小化する予測器 \\[\n\\operatorname{ERM}_\\mathcal{H}(S_n)\\in\\operatorname*{argmin}_{h\\in\\mathcal{H}}R_n(h)\n\\] を構成すれば良い，という指針があり得る．この枠組みを 経験リスク最小化 (Empirical Risk Minimization) といい，PAC 学習は，この ERM の枠組みがどれほどの意味で正しいかの定量的な検証になっている．\n\n\n1.3 経験リスク最小化の問題\nERM は一見，過学習の問題を孕んでいるように思える．\nそこで，あらかじめ学習者 \\(A\\) の値域 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) を制限することを考える．これを 帰納バイアス といい，正則化などの方法によって達成される．\nしかし，この漸近論が提供してくれない消息は複数ある．\n\n機械学習においては，仮説 \\(h\\) 自体がデータから決まる確率変数 \\(h_{S_n}:\\Omega\\to\\mathcal{H}\\) である場合が多い．これを考慮した収束が欲しい．\n\\(n\\) が有限の場合に非漸近論的消息が欲しい．\n\nそこで以降は，アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) を通じて，\\(h_{S_n}:=A(S_n)\\) と定まるとし，\\(h_{S_n}\\) を単に \\(h\\) ともかき，これをデータの関数とする．\nこの下で，\\(\\widehat{R}(h_{S_n})\\) と \\(R(h_{S_n})\\) の関係を考える．\n\n\n\n\n\n\n損失と誤差の区別\n\n\n\n\n\n(金森敬文, 2015, p. 13) では，（決定論的な）仮説 \\(h\\in\\mathcal{H}\\) に関して，\\(R(h)\\) を損失，データから決まる仮説 \\(h_{S_n}=A(S_n)\\) に関して，\\(\\operatorname{E}[R(h_{S_n})]\\) をリスクと呼び分けている．\n損失のうち，特に 0-1損失 \\[\nl=1_{\\Delta_\\mathcal{Y}^\\complement}\n\\]\nに関するものを誤差といい，この２語は殆ど交換可能な形で使う．その期待値をリスクと言う，という使い分けは一つ筋が通りそうである．\nただし，(Alquier, 2024), (Bousquet and Elisseeff, 2002), (Shalev-Shwartz and Ben-David, 2014) はいずれもリスクと誤差を交換可能な概念としている．\n\n\n\n\n\n1.4 PAC 学習\n機械学習を形式化する数理的枠組みのうち，PAC 学習 とは，\n\nProbably Approximately Correct Learning\n\nの略であり，(Valiant, 1984) によって提案されたものである．\n機械学習における哲学的な問題として，「そもそも不可知なリスク \\(R(h)\\) を最小化できるのか？」「できるとしたら，どのような場合においてか？」というものがあった．10\n\n\n\n\n\n\nagnostically PAC Learnable11\n\n\n\n\n定義 2 集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が（不可知論的な意味で） PAC 学習可能 であるとは，ある関数 \\[\nm_\\mathcal{H}:(0,1)^2\\to\\mathbb{N}\n\\] とアルゴリズム \\[\nA:(\\mathcal{X}\\times\\mathcal{Y})^{&lt;\\omega}\\to\\mathcal{H}\n\\] が存在して，任意の \\(\\epsilon,\\delta\\in(0,1)\\) と \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に対して，\\(m_\\mathcal{H}(\\epsilon,\\delta)\\) よりも多くの i.i.d. サンプルが存在すれば，\\(1-\\delta\\) 以上の確率で， \\[\nR(A(S_m))\\le\\min_{h\\in\\mathcal{H}}R(h)+\\epsilon\\quad(m\\ge m_\\mathcal{H}(\\epsilon,\\delta))\n\\] が成り立つことをいう．\n\n\n\nPAC 学習とは，分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に依らない真の誤差の評価を，確率論的に与えることを目的としており，Probably Approximately Correct の名前はその様子を端的に表現している．\n(Valiant, 1984) による PAC 学習可能性の定義には，計算量と計算時間の制約も入っていた．12 (Haussler and Warmuth, 1993, p. 292) によれば，PAC 学習の枠組みにより，計算効率性の研究者が，機械学習のアルゴリズムにも目を向け，協業を始めるきっかけになったとしている．\n\n\n\n\n\n\nagnostically PAC Learnable13\n\n\n\n\n定理 1 仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が有限ならば，（不可知論的な意味で）PAC 学習可能である．\n\n\n\n\n\n1.5 定理 1 の証明\n仮説集合 \\(\\mathcal{H}\\) が 一様収束性 を持つことを示せば良い，というように議論する．\nPAC 学習可能性（ 定義 2 ）は純粋に真の誤差の議論であるが，訓練誤差との関係に注目して示すのである．\n\n\n\n\n\n\n\\(\\epsilon\\)-representative, uniform convergence property14\n\n\n\n\n定義 3  \n\n訓練データ \\(S_n=\\{x_i\\}_{i=1}^n\\) が \\(\\epsilon\\)-代表的 であるとは，次を満たすことをいう： \\[\n\\lvert\\widehat{R}_n(h)-R(h)\\rvert\\le\\epsilon\\quad(h\\in\\mathcal{H}).\n\\]\n仮説集合 \\(\\mathcal{H}\\) が 一様収束性 を持つとは，任意の \\(\\epsilon,\\delta\\in(0,1)\\) と \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) について，十分大きな訓練データ \\(S_m\\) を取れば，\\(1-\\delta\\) 以上の確率で \\(S_m\\) は \\(\\epsilon\\)-代表的であることをいう．\nこのときのサンプル数の増加の速さを \\(m_\\mathcal{H}^{\\mathrm{UC}}(\\epsilon,\\delta)\\) と書く．\n\n\n\n補題 1 訓練データ \\(S_n\\) が \\(\\epsilon/2\\)-代表的ならば，任意の経験リスク最小化学習器 \\[\nh_{S_n}\\in\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\widehat{R}_n(h)\n\\] は \\[\nR(h_{S_n})\\le\\min_{h\\in\\mathcal{H}}R(h)+\\epsilon\n\\] を満たす．\n特に，\\(m_\\mathcal{H}^{\\mathrm{UC}}(\\epsilon,\\delta)\\) に関して一様収束性を持つならば，\\(m_\\mathcal{H}(\\epsilon/2,\\delta)\\le m_\\mathcal{H}^{\\mathrm{UC}}(\\epsilon,\\delta)\\) に関して（不可知論的な意味で）PAC 学習可能である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(h_{S_n}\\) の最小性に注意して，\n\\[\n\\begin{align*}\n    R(h_{S_n})&\\le\\widehat{R}_n(h_{S_n})+\\frac{\\epsilon}{2}\\\\\n    &\\le\\widehat{R}_n(h)+\\frac{\\epsilon}{2}\\\\\n    &\\le R(h)+\\epsilon.\n\\end{align*}\n\\]\n\n\n\nこうして，PAC 学習の枠組みは，（今回のケースでは）ERM の枠組みを肯定する結果を導いている．\nこれは，一様収束性が成り立つ仮説集合 \\(\\mathcal{H}\\) については，経験リスクは真のリスクに十分近いことを意味している．このような \\(\\mathcal{H}\\) を Glivenko-Cantelli クラス (Glivenko, 1933), (Cantelli, 1933) ともいう．15\nこうして，\n\n\n1.6 PAC 学習の基本定理\n実は，分類問題においては，一様収束性は，PAC 学習可能性を特徴付ける．\nこの証明は，VC 次元 (V. N. Vapnik and Chervonenkis, 1971) の概念による．\nしかし，一般の学習問題においても同じ状況というわけではない (Shalev-Shwartz et al., 2010)．多クラス分類でさえ同値性は崩れる (Daniely et al., 2011)．\n\n\n\n\n\n\nFundamental Theorem of Statistical Machine Learning16\n\n\n\n\n定理 2 分類問題 \\(\\mathcal{Y}=2\\) を 0-1 損失 \\(l=1_{\\Delta_\\mathcal{Y}^\\complement}\\) で考えるとする．仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) について，次は同値：\n\n\\(\\mathcal{H}\\) は一様収束性を持つ．\n\\(\\mathcal{H}\\) は（不可知論的な意味で）PAC 学習可能である．\n\\(\\mathcal{H}\\) は有限な VC 次元を持つ．\n\n\n\n\n\n\n1.7 Bayes ルール\n\n\n\n\n\n\nBayes error, Bayes rule17, excess risk / regret\n\n\n\n\n定義 4 損失関数 \\(l\\) に対して， \\[\n\\begin{align*}\n    R^*&:=\\inf_{h\\in\\mathcal{L}(\\mathcal{X};\\mathcal{Y})}R(h)\\\\\n    &=\\inf_{h\\in\\mathcal{L}(\\mathcal{X};\\mathcal{Y})}\\mathbb{E}[l(h(X),Y)]\n\\end{align*}\n\\] を Bayes 誤差 という．仮に右辺の下限が達成される \\(h^*\\in\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が存在するとき，これを Bayes 最適学習則 またはベイズルール という． \\[\n\\mathcal{E}(h):=R(h)-R^*\n\\] を 超過損失 という．\n\n\n\n\n\n\n\n\n\nBayes 規則の例18\n\n\n\n\n\n\\(\\mathcal{Y}=2\\) の場合，任意の \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に対して， \\[\nh^*(x):=\\begin{cases}\n1&\\mathbb{P}[Y=1\\,|\\,X=x]\\ge\\frac{1}{2},\\\\\n0&\\mathrm{otherwise}\n\\end{cases}\n\\] は Bayes 最適学習則である．\n\n\n\n\\[\n\\begin{align*}\n    \\mathcal{E}(\\widehat{h}_S)&=R(\\widehat{h}_S)-R(h^*)\\\\\n    &=\\biggr(R(\\widehat{h}_S)-\\inf_{h\\in\\mathcal{H}}R(h)\\biggl)+\\biggr(\\inf_{h\\in\\mathcal{H}}R(h)-R(h^*)\\biggl).\n\\end{align*}\n\\] 第一項を 推定誤差，第二項を 近似誤差 という．19\nここから，\\(\\overline{h}\\) を \\(\\inf_{h\\in\\mathcal{H}}R(h)\\) を達成する oracle machine とすると，推定誤差はさらに２項に分解して評価できる： \\[\n\\begin{align*}\n    &R(\\widehat{h}_n)-\\inf_{h\\in\\mathcal{H}}R(H)\\\\\n    &=R(\\widehat{h}_n)-R(\\overline{h})\\\\\n    &=\\underbrace{\\widehat{R}_n(\\widehat{h}_n)-\\widehat{R}_n(\\overline{h}_n)}_{\\le0}+R(\\widehat{h}_n)-\\widehat{R}_n(\\widehat{h}_n)+\\widehat{R}_n(\\overline{h})-R(\\overline{h})\\\\\n    &\\le\\biggl|\\widehat{R}_n(\\widehat{h}_n)-R(\\widehat{h}_n)\\biggr|+\\biggl|\\widehat{R}_n(\\overline{h})-R(\\overline{h})\\biggr|.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#統計的決定理論",
    "href": "posts/2024/AI/Theory.html#統計的決定理論",
    "title": "統計的学習理論１",
    "section": "2 統計的決定理論",
    "text": "2 統計的決定理論\nPAC 学習の枠組みを相対的に理解するため，統計的決定理論の目線から，同じ形式を見直してみる．\n\n2.1 枠組み\n最大の違いは，データ生成分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) にパラメトリックな仮定をおく点である．このとき，組 \\((\\mathcal{X}\\times\\mathcal{Y},(\\mathbb{P}_\\theta)_{\\theta\\in\\Theta})\\) を 統計的実験 ともいう．\n損失関数 \\(l:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}_+\\) は，より一般には，決定空間 \\(\\mathcal{Z}\\) に対して， \\[\nl:\\mathcal{Y}\\times\\mathcal{Z}\\to\\mathbb{R}_+\n\\] と定まるものである．\n\n\n2.2 一様最強力検定\n学習ではなく，検定の文脈では，PAC 同様全てのデータ生成分布 \\(\\mathbb{P}\\in\\mathcal{P}()\\) を考えるが，リスクが小さいことを要請する．"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#pac-bound",
    "href": "posts/2024/AI/Theory.html#pac-bound",
    "title": "統計的学習理論１",
    "section": "3 PAC bound",
    "text": "3 PAC bound\n\n3.1 定理\n\n\n\n\n\n\nPAC bound20\n\n\n\n\n定理 3 仮説集合 \\(\\mathcal{H}\\) が有限であるとする：\\(\\#\\mathcal{H}=:M&lt;\\infty\\)． このとき，任意の \\(\\epsilon\\in(0,1)\\) について， \\[\n\\mathbb{P}\\left[\\forall_{h\\in\\mathcal{H}}\\;R(h)-\\widehat{R}(h)\\le C\\sqrt{\\frac{\\log\\frac{M}{\\epsilon}}{2n}}\\right]\\ge1-\\epsilon.\n\\]\n\n\n\n仮説 \\(\\mathcal{H}\\) の数 \\(M\\) を増やすごとに，訓練データ数 \\(n\\) は \\(\\log M\\) のオーダーで増やす必要がある，ということになる．\n\n\n3.2 \\(\\biggl|\\widehat{R}_n(\\widehat{h}_n)-R(\\widehat{h}_n)\\biggr|\\) の評価\n\n\n3.3 \\(\\biggl|\\widehat{R}_n(\\overline{h})-R(\\overline{h})\\biggr|\\) の評価\n\n\n3.4 定理の一般化21\n\n一般の \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) への拡張は，VC次元の理論を用いて行われる（ 定理 2 など）．\nバウンドの変形に，Rademacher 複雑性も使われる．\n現実との乖離：現代の深層学習では \\(M\\) が極めて大きくなり，PAC 不等式はほとんど意味をなさない．これを包括できる理論が試みられている．"
  },
  {
    "objectID": "posts/2024/AI/Theory.html#footnotes",
    "href": "posts/2024/AI/Theory.html#footnotes",
    "title": "統計的学習理論１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Mohri et al., 2018, pp. 9–10) と (Shalev-Shwartz and Ben-David, 2014, pp. 13–14), (Alquier, 2024) を参考にした．↩︎\nこれは訓練セット (training set) ともいう (Shalev-Shwartz and Ben-David, 2014, p. 14)．↩︎\n(Bousquet and Elisseeff, 2002) の用語に一致する．(Alquier, 2024, p. 2) では，\\(\\mathcal{X}\\) を object set，\\(\\mathcal{Y}\\) を label set と呼んでいる．(Shalev-Shwartz and Ben-David, 2014, pp. 13–14) は \\(\\mathcal{X}\\) を domain set，\\(\\mathcal{Y}\\) を label set と呼ぶ．↩︎\n(Valiant, 1984) では，\\(\\mathcal{Y}=2\\) の場合，元 \\(h\\in\\mathcal{H}\\) を 概念 (concept) ともいう．その他の場合を predicate とも呼んでいる．(Shalev-Shwartz and Ben-David, 2014, p. 14) では predictor, predictino rule, classifier とも呼ぶとしている．↩︎\n(Alquier, 2024, p. 177) を参考にした．\\(\\Delta_\\mathcal{Y}:=\\left\\{(y',y)\\in\\mathcal{Y}^2\\mid y=y'\\right\\}\\) を対角集合とした．↩︎\n(Alquier, 2024) 第4章ではこの i.i.d. 仮定を外している．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 24) などでは，\\(\\mathcal{Y}=2\\) として分類問題を考えていることもあり，専らこの損失を考えている．↩︎\n(Alquier, 2024, p. 4), (Mohri et al., 2018, p. 10)，(金森敬文, 2015, p. 7) を参考にした．(Shalev-Shwartz and Ben-David, 2014, p. 14) でも，generalization error, risk, error，さらには loss のいずれの名前でも呼ぶし，training error と empirical error /risk とも交換可能に使う，としている．↩︎\n(Mohri et al., 2018, pp. 10–11), (金森敬文, 2015, p. 8) など．↩︎\n(Haussler and Warmuth, 1993, p. 263) にある Valiant 本人による解説に，その哲学的なモチベーションがよく表れている．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 25) 定義3.3，(Mohri et al., 2018, p. 22) 定義2.14 など．元々の (Valiant, 1984) の定義では，\\(m\\) と計算時間の増加レートは \\(1/\\ep,1/\\delta\\) の多項式以下であるという制限もあった．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 28) も参照．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 34) 系4.6 など．↩︎\n(Shalev-Shwartz and Ben-David, 2014, pp. 31–32) 定義3.1 と 定義3.3．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 35) など．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 48) 定理6.7 など．↩︎\n(Mohri et al., 2018, p. 22) 定義2.15，(金森敬文, 2015, p. 9) を参考．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 25) など．↩︎\n(金森敬文, 2015, p. 17) を参考．↩︎\n(Alquier, 2024, p. 7) 定理1.2 など．↩︎\n(Devroye et al., 1996) 第11, 12章 参照．(Vladimir N. Vapnik, 1998)．↩︎"
  },
  {
    "objectID": "posts/2024/AI/BAI.html",
    "href": "posts/2024/AI/BAI.html",
    "title": "これからはじめるベイズ機械学習",
    "section": "",
    "text": "現在，産業界における “AI” というと専ら，いくつかの限られた巨大 IT 企業が，巨大ニューラルネットワークを最尤推定で学習させ，これを基盤モデルとして公開し，我々一般庶民はそれを有効活用して下流タスクを安価に解くことだけ考えるという営みを指す．\nその産業や生活への破壊的な影響を憂慮しながらも，雨乞いをする日々である．\nAI はそんなものではない．AI はこれにかぎるものではない．\nAI が真に我々の友となり，我々の日常をほんとうに豊かにするは，AI の進歩だけが必要なのではなく，人間との協業が得意になる必要がある．\nそのための第一歩はすでに明らかである．不確実性の定量化 である．\nつまり，「その AI には何が出来て何が出来ないか」「AI の出力がいつ信頼にたるもので，いつ人間の介入が必要であるのか」がわかりやすい形で伝わるコミュニケーション様式をそなえている必要があるのである．1\n筆者の知る限り，ここにある全てのナラティブは現時点では全く広く語られているものではなく，筆者も最初の１年の研究生活を通じて朧げながら見えて来たばかりのものである．\n不確実性の定量化は，機械学習モデルを民主化し，我々の民芸に取り込むための重要な一歩である（のではないだろうか？）．\n本稿はこの発見を共有するために書いた．筆者の反芻不足から，冗長な部分も多いだろうが，少しでも，琴線に触れるものがあれば幸いである．2"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#ベイズ機械学習のすすめ",
    "href": "posts/2024/AI/BAI.html#ベイズ機械学習のすすめ",
    "title": "これからはじめるベイズ機械学習",
    "section": "1 ベイズ機械学習のすすめ",
    "text": "1 ベイズ機械学習のすすめ\n我々が AI をより信頼するためには，何が必要だろうか？\n筆者の考えでは，信頼への第一歩は 不確実性の定量化 が出来るようになることのはずである．\nそしてそのためには ベイズ機械学習 (Bayesian Machine Learning) の発展による本質的解決が必要不可欠である．本稿はこの点を説明するために執筆されたものである．\n筆者に言わせれば，ベイズ機械学習が，今後数年間で AI が経験すべき進展の方向である．この山を越えれば，今まででさえ思っても見なかった未来がひらけてくるだろう．\n\nAlthough considerable challenges remain, the coming decade promises substantial advances in artificial intelligence and machine learning based on the probabilistic framework. (Ghahramani, 2015, p. 452)\n\n\n1.1 ベイズとは何か？\n機械学習において，確率論的なモデリングに基づいたアプローチを ベイズ機械学習 ともいう．典型的には，モデルの全変数上の結合分布をモデリングし，ベイズ規則によりパラメータのベイズ推定を行う，という手続きからなる．そのため，確率論的アプローチ や モデルベースアプローチ も同義語として用いられる．3\n一方で，頻度論的 という言葉は，よく非ベイズ的アプローチを示す接頭辞として用いられる．典型的には，損失関数を設定し，これを最小化するパラメータを探索することによって実行される．\nこの２つのアプローチは互いに対照的であり，統計学の始まりから基本的な二項対立の図式をなしてきた．\n\n\nContrast of the two main approachs to Machine Learning\n\n\n\n\n\n\n\n\nBayesian\nFrequentist\n\n\n\n\nInference is4\nMarginalization\nApproximation\n\n\nComputational Idea5\nIntegration\nOptimization\n\n\nObjective\nUncertainty Quantification\nRecovery of True Value\n\n\nEmphasis\nModelling\nInference\n\n\n\n\nしかし，機械学習の時代においては，互いの弱みを補間し合う形で発展していくと筆者は考える．特に，現状の推論偏重でモデリング軽視の風潮が，重要な実世界応用の多くを阻んでしまっている．機械学習の世界樹は実は２本あるのである．\n\n\n1.2 ベイズと頻度論との違い\nベイズと頻度論では，確率の解釈も異なるかも知れないが，数学的枠組みとしてはベイズの方が一般的な枠組みであり，また手続き上は，モデリングを重視するか，推論を重視するかの違いでしかない．\n実際，殆どの場合，頻度論的手法はある特定の事前分布を持ったベイズ手法とみなせ，逆も然りである．\nデータから推論を行うには，何らかの仮定が必ず必要であり，それを明示的にモデルに組み込むのがベイズで，推論アルゴリズムにより自動化する精神を持つのが頻度論的手法である．\nその結果，優秀な推論アルゴリズムが日夜驚異的なスピードで提案され，今や機械学習手法は教師あり学習・教師なし学習・強化学習の全てで目覚ましい発展を見た．\nしかし，ベイズと頻度論の２つの柱のバランスを欠いた発展はここまでである．今や，頻度論的な手法を採用した際に，自分たちがどのような仮定を置いたのか全く明瞭な知識を欠いてしまっている．一方で，現実のビッグで複雑なデータを扱うためには，もはや確率的なモデリングを避けては通れない．6\n極めて本質的で強大な敵に対面しつつあるのである．\nだが，現状の病理は明らかであり，頻度論とベイズの手法の間に対応をつけ，足並みを揃えることで次の前進が約束されてる．この意味で，２つの世界樹が必要なのである．\nさらに，ベイズ推論は帰納的推論の確率論的拡張と見れるため，エージェントの合理的な学習と意思決定の最良のモデル（の一つ）と信じられている．7\nしたがって，ベイズ流解釈により手法を理解し，最適化流解釈により手法を実装する．これがあるべき機械学習の未来であると筆者は考える．\n\n\n1.3 ２つの世界樹\n今こそ，この２つの手法は根底では繋がっていることをよく周知し，この２つの視座を往来しながら適材適所に使うことが大事だと筆者は考える．\nしかしそのためには，ベイズ機械学習の発展が遅れている現状を鑑みて，ベイズの手法のより一層の発展と理解の深化が必要である．8\n本章「ベイズ機械学習のすすめ」は，ベイズの手法の特に肝心と思われる３つの側面を指摘して終わる．以下３章を通じて，\n\n第 2 節 ベイズは不確実性を定量化する\nBayes の方が不確実性の定量化が得意であるため，そのような応用先では頻度論的な手法よりも，Bayes バージョンの手法を用いることが出来ると便利である．\n第 3 節 ベイズは分布という共通言語を与える\nBayes による統一的な扱いが理論的に有用である場面が増えている．その際に，Bayes による理論解析と最適化による実際の推論という適材適所の協業が未来の方向であるかも知れない．\n第 4 節 ベイズは理解を促進する\nベイズの手法が敬遠されていた理由も，換言すれば，「事前分布」という得体の知れないものを通じて，理論的深淵と直結するためである．ベイズ手法の研究が理論的な解明を要請する．だからこそ，数学者の魂を持った者がこの途を通ることは人類に大きく資すると筆者は考える．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-uncertainty-quantification",
    "href": "posts/2024/AI/BAI.html#sec-uncertainty-quantification",
    "title": "これからはじめるベイズ機械学習",
    "section": "2 ベイズは不確実性を定量化する",
    "text": "2 ベイズは不確実性を定量化する\n\n2.1 不確実性の定量化の必要性\n機械学習と統計学が単なる道具ではなく，人間のより大きなシステムの一環を単独で担う場面が増えてきた．例えば，\n\n金融・経営・政策決定などの分野で，意思決定に繋げるデータ解析をするとき\n科学において，発見や仮説を検証するためのデータ解析をするとき\nロボットや自動車などの自動化をし，社会に実装するとき9\n医療診断や裁判などの場面で，専門家を補助するシステムを作るとき\n\nこれらのいずれの例でも，システムの一部を担うにあたって，不確実性を定量化しておくことが欠かせない．その出力を用いるのが人間である場合も勿論，別の機械学習モデルである場合は尚更である．\nつまり，人間社会で優秀であるだけでなくホウレンソウと信頼獲得も重要であるように，機械学習モデルも性能の高さと正確さだけでなく，いつその結果を信頼して良いのかを「どの程度」という指標と共に知らせてくれることが信頼関係の基本となるだろう．\n実際，殆どの場面で，データから高い確証度で言えることと，そうではないことでは全く違う意味を持つ．それぞれの場面での例には，次のようなものがあるだろう：\n\nデータから高い確証度で言えることと，意思決定者による采配が必要な部分を分離できない限り，意思決定プロセスの一部として組み込むことが難しく，結局機械学習手法が全く採用されないということもあり得る．\n結果の再現可能性が科学の基本的な要請である以上，その結果の不確実性を実験結果に付記することは基本的な科学的態度である．後述（第 2.3.1 節）するが，\\(p\\)-値や信頼区間などの統計量はこれに応えるものではない．\nロボットや自動車の自動化 AI システムは，いくつかのモデルを組み合わせて作ることになるだろう．個々が十分な性能を持っていても，小さな誤差が累積してシステムとしての性能を著しく低下させることがある．これを防ぐために，統一した方法での不確実性の取り扱いが必要である．\n個々人の権利と法益が衝突する場面にも AI が利用されより良い生活が実現されるには，法的な解釈可能性が担保される必要があることが，実は大きな難関として我々を待っている．その第一歩は，不確実性の可視化になるだろう．10\n\n以上の内容は，結果の 解釈可能性 でも全く同じことが言えるだろう．\n\n\n2.2 信頼のおける AI システム\n上述の点をまとめると，機械学習手法と人間社会がよりよく共生していくには AI の 信頼性 (trustworthyness) が必要とされているのである．不確実性の定量化と解釈可能性は，AI が人間社会で信頼を獲得するにあたって根本的な要素になるだろう．\n現状の手法の延長でこの信頼性の問題は扱えず，新たな手法が必要とされている．Bayesian approach や probabilistic approach と呼ばれている試みは，まさにこれに応えるものであり，近年急速に発展している．\n\n\n2.3 不確実性を扱うには Bayes が必要である\n実装は頻度論的な手法の方が簡単で高速であることが多いが，不確実性の定量化には向かない．\nこのような場面では，頻度論的手法を頻度論的に改善する，という方向は筋が悪いと思われる．このようなときこそ，もう一つの世界樹であるベイズの方法を用いるべきである．\nこれを，科学における再現性の危機を例にとって確認したい．11\n\n2.3.1 再現性の危機\n多くの実験科学では不確実性の定量化が必要不可欠である (Krzywinski and Altman, 2013)．\n\nIt is necessary and true that all of the things we say in science, all of the conclusions, are uncertain … (Feynman, 1998)\n\n再現性の危機 (replication crisis) とは，多くの実験において報告されている統計的有意性が，再現実験において得られないことが多いという問題を指し，2010年代の初めから多くの科学分野において問題として取り上げられてきた．12\nその理由は明白である．信頼区間は集合値の推定量であるため，「分散」が十分大きいならば，データセットを変えて何回も計算することでいずれは非自明なものを得ることが出来るのである．そのため，信頼区間や \\(P\\)-値を報告するだけでは，結果の信頼性については何も保証されないのである．\nその結果多くの科学分野では Bayes 統計学による不確実性の定量化に移行しつつある (Herzog and Ostwald, 2013), (Trafimow and Marks, 2015), (Nuzzo, 2014)．\n信頼区間と信用区間の違いに注目して，その違いを解説する．\n\n\n2.3.2 信頼区間と信用区間\n「95 % の信頼区間」と言ったとき，「95 % の確率で真の値がその範囲に含まれるような区間」だと思いがちであるが，これはどちらかというと信用区間の説明であり，信頼区間は計算するごとに値が変わってしまう確率変数である ことを見落としがちである．13\nつまり，信頼区間は頻度論的な概念であり，「真の値」がまず存在し，区間自体が変動し，95 % の確率で被覆するというのである．今回見ている信頼区間が，別のデータセットで計算した場合にどう変わるかについては全く未知である．\nこのことは，信頼区間は「真のパラメータの値」で条件づけて得るものであるが，信用区間はデータによって条件づけて得るものであるという点で違う，とまとめられる．この２つの混同は「何で条件づけているか？」を意識することで回避することができる．14\n誤解を恐れず言うならば，再現性の危機とは，信頼区間というサイコロの出目によって科学が踊らされていたということに他ならない (Nuzzo, 2014)．15\n\n\n2.3.3 なぜベイズを用いれば良いのか？\nこれは，信頼区間や \\(P\\)-値などの頻度論的な手法は，しばしば尤度原理に違反するためである．16\n換言すれば，何らかのモデルと事前分布に関するベイズ手法と等価である，すなわち，Bayesianly justifiable (Rubin, 1984) とみなせない手法は，何らかの意味でデータを十分に反映できていない可能性が高くなる．\n従って，ベイズの手法が原理的に最も適切である場面が多い．一方でその計算の困難さや，全てのステップをモデリング段階に組み込む点を回避するために，種々の頻度論的な実装は考え得て，頻度論的な手法はそのような運用においては健全であるとの指標にもなる．17\nBayes により手法を理解し，頻度論的に手法を実装することが，あるべき姿勢であると思われる．\n\nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice. (Rubin, 1984)\n\n\n\n\n2.4 ベイズ深層学習という夢\n深層モデルはその性能の高さから，最も実世界応用が期待されるモデルであるが，パラメータが極めて多いため，特にベイズ化することが難しいと言われている．\n例えば，ハルシネーション (hallucination) として，LLM が「事実に基づかない」情報を生成してしまうことが問題とされているが，これも不確実性の定量化の問題に他ならない．18\nその他の場面でも，不確実性の定量化には conformal prediction などの事後的な手法が試みられている．19 これらはどのようなブラックボックスに対しても適用可能である一方で，対症療法というべきものであり，ベイズ流の解釈をすることで直接的に事後分布を求めるという根本的な解決にも，もっと注力されるべきである．\nベイズによる不確実性の定量化は，自然であるだけでなく，より有用な不確実性の定量化を与えるものだと予想している．20\n加えて，事前分布を変えることで，種々の帰納バイアスを加えるという「プロンプトエンジニアリング」ならぬ「プライヤーエンジニアリング」の理論が樹立できるかもしれない．すでに，公平性，同変性，スパース性，共変量シフトへの頑健性などを達成するための事前分布が考えられている．21\n\n\n2.5 分野全体の動向\n現状の機械学習モデルと実応用との乖離は，他の側面でも生じている．\nまず，訓練データが実際の運用環境を十分に反映できていないということは極めて頻繁に起こるだろう．この現象を 分布シフト といい，機械学種モデルの予測性能だけで無く，分布外汎化 (out-of-distribution generalization) 能力も重視するという潮流が生じている．\nさらに，一度訓練したモデルを，分布シフト自体が移り変わっていく環境で，微調整のみによって繰り返し使い続けるという使用を想定した 継続学習 (continual learning) という考え方もある．22\n章を変えて別の角度から議論を続けよう．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-distributional-representation",
    "href": "posts/2024/AI/BAI.html#sec-distributional-representation",
    "title": "これからはじめるベイズ機械学習",
    "section": "3 ベイズは分布という共通言語を与える",
    "text": "3 ベイズは分布という共通言語を与える\n\n3.1 継続学習という発想\n継続学習は，機械学習モデルをより動的で実際的な環境でも使えるようにするための新たな枠組みである．そこまで，教師あり学習モデルがすでに実用的な性能を獲得したということでもある．\nつまり，単に「教師あり」「教師なし」の１タスクを解く営みは爛熟しつつあり，機械学習の理論と応用の最先端は，より深い森に分け入りつつあるのである．\nここにおいて，ベイズ流の接近が統一的な取り扱いを与えるという美点が，さらに重要でもはや必要不可欠な役割を果たすものと思われる．\n\n3.1.1 ベイズ推論が与える統一的枠組み\nベイズ推論とは，事前分布 というものを設定して，これをデータによって更新するという営みである（その更新規則は Bayes の公式が与える）．\n事前分布をどう設定すれば良いか？の問題は，ベイズ推論の初期からの問題であった．極めて自由度が高いことが，逆にベイズ推論が実際のデータ解析の場面において敬遠される一因ともなっていた．\n\n\n3.1.2 ベイズと最適化との協業\nしかし，継続学習が当たり前になった社会において，全てのパラメータ値を事前分布と事後分布とみなし，全ての学習過程をベイズの公式という統一的な方法で更新すると捉えられることは，極めて大きな利点になり得る．\nというのも，継続学習においては，学習を繰り返すうちに過去に学んだ内容を忘れ去ってしまうという 壊滅的忘却 (catastrophically forgetting) が最大の困難である．\n理論的には，分布のベイズ更新の繰り返しとして見る方が極めて見通しが良い．一方で，事後分布の近似が十分でない場合，実際にベイズ更新を行うことは性能に悪影響を与える．\nそこで，理論解析や設計をベイズの観点から行い，実際の推論は最適化ベースで行うという適材適所により，壊滅的忘却を緩和できる可能性がある (Farquhar and Gal, 2019)．\n\n\n\n3.2 例：強化学習への分布によるアプローチ\n\nwe believe the value distribution has a central role to play in reinforcement learning. (Bellemare et al., 2017)"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#sec-inductive-bias",
    "href": "posts/2024/AI/BAI.html#sec-inductive-bias",
    "title": "これからはじめるベイズ機械学習",
    "section": "4 ベイズは理解を促進する",
    "text": "4 ベイズは理解を促進する\n我々はもはや機械学習を通じて，自分たちが何をやっているのかわかっていない．この愚かさを AI に継がせてはならない．\n\n4.1 なぜベイズ法の発展が遅れたか？\nベイズ法の採用は，自分たちが何をやっているかへの理解と解釈可能性を刺激するという側面がある．\nその理由は簡単である．ベイズ推論は，モデルとその上の事前分布を定めれば，あとはベイズ更新規則をどう計算するかの問題となり，近似手法は様々あれど，もはや推論手法に選択の余地はない．\n換言すれば，その分解析者がモデルと事前分布の特定を全てこなす必要があるのであり，解析者に確率モデリングへの理解を強要するところがある．\nしかしこれは「面倒なことは全てアルゴリズムにやってほしい」という精神とは対立するため，ベイズの美点であると同時に，ベイズの発展を阻害してきた遠因の一つでもあった．\nこれを指して「事前分布の選択に恣意性が入る」という通り文句がよく使われるが，実際は，頻度論的手法における「どのような目的関数をどのように最適化すれば良いか？」という恣意性に変換されているのみであり，問題を先送りにして，「ベイズ法 対 頻度論的手法」という虚構の対立を作り上げているのみである．\n機械学習のポテンシャルが具現化したいまこそ，この困難に立ち向かう必要があるが，この問題は最適化や頻度論的な立場から見るより，ベイズの立場から見た方が，理論的な見通しが良いようである（第 4.4 節）．\n\n\n4.2 帰納バイアスの明確化の必要性\n機械学習の真の理解のためには，各モデルの帰納バイアスを明確化する必要がある．\n\n4.2.1 帰納バイアスとは何か？\n現状の AI システムは大量のラベル付きデータが必要であり，多くの現実的に有用なタスクでこのような教師データが用意できるわけではない．\n一方で，人間は遥かに少ないデータから効率的に学習することができる．\n\n\n\nNumber of Training Tokens BabyLM Challenge\n\n\nその違いは，進化が我々生物に授けた 帰納バイアス にあると考えられている．\n我々には遺伝的に継がれている生まれ持った学習特性があり，より効率的に学習出来るのかも知れない．\n事実，一度事前学習をした LLM は，極めて少ないデータにより新しいタスクを学習することができるがわかりつつある (Zhou et al., 2023)．LLM の事後調整に関する稿 も参照．\n\n\n4.2.2 事前分布に向き合わずにやり過ごしてきた\n現状，多くの機械学習手法は確率的な方法を取っていない．これは事前分布を明示せずに（ひょっとしたら明後日の方向に向かって）行われる Bayes 学習手法であるとみなせる．\n現状の機械学習の成功は，事前分布に関する知識なしに到達されたものであり，それ故の限界がある．例えば，現状のままではモデルにどのような帰納バイアスが組み込まれているか不明瞭である．23\n\n\n4.2.3 帰納バイアスに対するベイズ的視点\nデータの空間 \\(\\mathcal{X}\\) 上の任意のモデル \\(\\mathcal{M}\\) の周辺尤度 \\(p(x|\\mathcal{M})\\) は，24 ベイズ流には事後確率として捉えられ，全てのデータ \\(x\\in\\mathcal{X}\\) 上に有限な測度を定める．25\nよって，全てのモデルは，あるデータを得意とするならば他のデータについては不得意であることを免れない．これは no free lunch 定理と呼ばれる定理の一群により推測されており，分類問題などの簡単なタスクを除いて完全な形式的表現はまだ持たない作業仮設である．\n\n\n\nA Probabilistic Perspective of Genelization (Wilson and Izmailov, 2020)\n\n\n例えば，基盤モデル とは，インターネット上のデータから最大限人間の言語というものに関する帰納バイアスを取り込んだ，パラメータ上の初期設定であると見れる．\nこれは，あるパラメータ空間上の理想的な事前分布からのサンプリングであるかも知れない．それ故，種々の下流タスクに対して，小さなモデル変更のみにより適応することが出来る．\n大規模言語モデルの能力創発現象は，帰納バイアスを十分取り込むことにより自然に解かれるタスクであったのかもしれない．\n\n\n4.2.4 worst-case analysis からの脱皮\n帰納バイアスを明確にせず，やり過ごしてきたつけが，特に学習理論においても現れている．\n現状の統計的学習理論は全て，worst-case analysis であるが，実用上は全くそうではない．「動くモデル」には暗黙の帰納バイアスが入っており，これに明るくなる必要があるのである．\n2024 年に生きる我々は，worst-case analysis からの脱皮を迫られている．\n\n\n\n4.3 数学者の哲学\nBayes の見方は，機械学習モデルを底流する数理的枠組みになっている．仮に次の Mac Lane の言葉が数学者のあるべき態度の１つであるとするならば，この意味での数学者には Bayes の立場から機械学習を研究することを特におすすめする．\n\nHowever, I persisted in the position that as mathematicians we must know whereof we speak, be it a homotopy group or an adjoint functor. (Mac Lane, 1983, p. 55)\n\n数理統計学に始まり，数学者の統計や機械学習分野への参入は，推論手法の解析が想像されるかも知れない．\nしかし，真の数学的理解は，手法の数学的な機械仕掛けを紐解くだけでなく，それぞれの手法がモデルとしてどのような仮定の下で成り立っているかを，モデリングの観点から理解することにもあると筆者には思われる．\n現状，後者の視点が大変に不足しており，数理的な知識に支えられた大局観というものがない．個々の数学的な道具に捉われず，大局的な構造を捉える数理的枠組みが必要である．\nこれに応えるのがベイズの枠組みであると筆者は信じる．\n推論とモデリングという双対的な営みは深い数理的な構造を持っていることが明らかになりつつある．この大局的構造の解明と理論構築には，ベイズの観点から光を照らしてくれるような，Mac Lane の意味での数学者的な魂が必要とされているのである．\n\n4.3.1 Bayes の数学\nBayes 流の解釈では，どんなにモデルが複雑で巨大になろうとも，推論とは積分に他ならない．\n\n\n\nBayes’ Theorem (density form)\n\n\n全ての（尤度原理に則った）推論は，事後分布の関数としてなされる（べきである）．\n実際の実装は，その近似として実行される（べきである）．\nよって，実装とモデリングの段階を明確に分離する枠組みを提供している上に，極めて普遍的な枠組みである．\nというのも，Bayes 流のモデリングは，Markov 圏 上の図式と見ることができ（第 5.2 節），普遍的である上に，数学的にも最も直接的で直感的な表現であると思われる．\n圏として持つ代数的性質は，モデルの結合・分解が自由に出来るということに繋がり，モジュール性 が高いということになる．\n\nI basically know of two principles for treating complicated systems in simple ways: the ﬁrst is the principle of modularity and the second is the principle of abstraction. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. Michael I. Jordan excerpted from (Frey, 1998)\n\n分布を明示的に用いた 確率核 を通じてのモデリングは，なぜだか数学的に極めて自然なアプローチを提供してくれるようである．\n\n\n4.3.2 ベイズの代数・幾何・解析\n上述したように，ベイズのモデリング法と学習規則は本質的に代数的なところがある．\n加えて，分布を基本言語とするために，ベイズ推論においては空間 \\(\\mathcal{P}(\\mathcal{X})\\subset\\mathcal{M}^1(\\mathcal{X})\\) が極めて基本的な役割を果たす．\nサンプリングは \\(\\mathcal{P}(\\mathcal{X})\\) 上の幾何学に関係が深く，情報幾何学や最適輸送などの発展が見られている．\n一方で最適化は \\(\\mathcal{P}(\\mathcal{X})\\) 上の解析学に関係が深く，古くから機械学習分野では \\(\\mathcal{P}(\\mathcal{X})\\) 上の様々な汎函数が ダイバージェンス の名前で考察されており，その勾配流として種々の最適化手法が理解できる．\n\n\n4.3.3 Bayes に繋げる数学\n通常の頻度論的手法は，うまくいくことが先であり，理論が後付けされる．そしてその理論もどこか ad-hoc というべきであり，worst-case で漸近論的である．\nこれらに Bayes 的な解釈を与えることで，暗黙のうちにどのような仮定を課しているモデリング手法に相等するのか明確にされる．特に，非漸近論的な知見を与えてくれる数少ないの道の一つである．\n\n\n\n4.4 ベイズ推論とみる美点\nベイズ推論自体への理解だけでなく，種々の頻度論的手法を（特定の環境下での）ベイズ推論の近似として理解することは，新たなアルゴリズムの開発に有用であるという合意が形成されつつあるようである．26\n最適化に基づく手法の計算効率性は，正確なベイズ推論に勝る場面も多い．ここで注意すべきは，ベイズ推論の実行が肝要であり，その実装は最適化に依ろうと，積分近似に依ろうと大した違いではないのである．\n「ベイズ推論は多くの最尤法に基づく手法よりも，自然な正則化がなされるために過学習の問題がない．」と説明されるが本来は逆である．多くの最適化に基づく手法は，目的関数の選択に恣意性があり，その選択を誤り続けているために過学習という問題が生じている，という方が，後世の教科書に載る表現なのではないかと筆者は考えている．\nそこで，種々の既存手法のベイズ流の解釈を探究することは，より良い推論アルゴリズムの開発に資すると考えられている．\nこの方向の近年の発展をいくつか紹介したい．\n\n4.4.1 ベイズ学習規則\n現状の機械学習は，統計学，連続最適化，計算機科学の知識を総動員して開発された種々の推論手法によって支えられている．\nその性能は驚異的なスピードで向上しているが，それぞれの手法がどのような仮定をモデリングの段階で課しているかが不明瞭であり，どの手法を使うべきかの統一的な枠組みは得られていない．\nこの現状の抜本的な改善が，それぞれの手法のベイズ流の解釈を探究することで得られると考えられる．\nその枠組みの一つが ベイズ学習規則 (Khan and Rue, 2023) である．\n(Khan and Rue, 2023, p. 4) では，ベイズ流の解釈を持つ種々の手法が他より優れている理由として，目的関数に現れるエントロピー項が 自然勾配 の概念を通じて自然な正則化を与えることが，ベイズ学習規則という新たな理論的枠組みの中で示されている．\n\n\n4.4.2 例：強化学習\n強化学習でも，モデルベースのアプローチが取り入れられつつあり (Deisenroth and Rasmussen, 2011)，さらに学習と制御をベイズ推論と見ることが，アルゴリズムの設計において有用であることが提唱されつつある：\n\nCrucially, in the framework of PGMs, it is sufficient to write down the model and pose the question, and the objectives for learning and inference emerge automatically. (Levine, 2018)"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#bayes-機械学習の例",
    "href": "posts/2024/AI/BAI.html#bayes-機械学習の例",
    "title": "これからはじめるベイズ機械学習",
    "section": "5 Bayes 機械学習の例",
    "text": "5 Bayes 機械学習の例\n\n\n\n\n\n\n要約\n\n\n\n深層学習モデルにより教師あり学習は十分に発展し，多くの訓練データが得られる場面では驚異的な性能を発揮するようになった．\nこの発展は，モデリングの仮定に捉われずに純粋にアルゴリズムの開発に集中することが出来るという頻度論的な枠組みの利点を有効活用する形で達成された．\nしかし，殆どの実世界応用では，不確実性のモデリングが必要不可欠である．この点を後回しにして性能を追求することで得た栄華である．だからこそ，極めて高い性能を誇るモデルを，実世界応用の場面で有効活用する手段を我々はまだ知らないのである．\nその鍵はベイズにある．安全性，信頼性，柔軟性……．これらの21世紀の社会の要請に応えるためには，ベイズ機械学習手法の発展と，既存の手法のベイズ流の理解とが追いつくことが，第一歩である．\n\n\n既存の深層学習モデルは，「教師あり学習」という枠組みや，画像の分類タスクや自然言語処理のタスクなど，広く周知された問題設定とデータセットが存在する．\n一方で，ベイズ機械学習における対応物はまだ十分に周知されていないようである．\nベイズ機械学習では「損失を最小化する」という枠組みの中でなるべく性能の良い推論手法を探す，というわかりやすい枠組みがある訳ではないようである．\nそこで，本章ではベイズ機械学習の近年の発展を概観することを試みる．\n\n5.1 Bayes 深層学習\nニューラルネットワークモデルは，隠れ素子数が無限大になる極限において，Gauss 過程モデルに漸近することが知られている (Neal, 1996)．Gauss 過程とはノンパラメトリックなベイズ機械学習手法の代表である．この対応を通じて，深層学習のベイズ流の解釈が進められている．\nこの稿の執筆後，本稿をまとめるかのようなアブストラクトを持ったポジションペーパー (Papamarkou et al., 2024) が公開された\n\nIn the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. (Papamarkou et al., 2024)\n\n深層学習をベイズ化することで，上にあげた\n\n不確実性の自然な定量化\n継続学習への柔軟な接続\n科学的営みの促進\n\nなどが目指せる．特に，現状の大規模な基盤モデルをベイズ化する悲願を真っ向から論じている．\n\n\n5.2 確率的グラフィカルモデル\n歴史的に，（確率的）モデリングは，主に（確率的）グラフィカルモデルを通じて機械学習の分野に導入された．\nそのため，20世紀に入ったばかりの頃は，Bayes 機械学習の唯一の例は確率的グラフィカルモデルなのであった．27\nだが，確率的グラフィカルモデルは，極めて普遍的で，従来の因果推論・階層モデル・欠測モデル・潜在変数モデル・構造方程式モデルなどの発展を包含する統一的な枠組みであることをより広く認識すべきである．\n\n5.2.1 ベイジアンネットワーク\nBayesian Network は Markov 圏上の図式であり，方向関係のある変数間の関係をモデリングする最も直接的な方法である．\n\n\n5.2.2 構造的因果モデル\n\n\n5.2.3 階層モデル\n階層モデルとは，ベイズの枠組みでは，観測変数・潜在変数の区別なく，モデルを自由に結合出来る点を利用したモデリング手法である．\n\n\n5.2.4 モデルの属人化\n大きなデータも，属人化医療や推薦システムなど多くの文脈では小さなデータの寄せ集めであり，そうでなくともその構造を正しく捉え，全ての不確実性を取り入れた柔軟なモデリングをすることで，さらに密接な形で社会に取り入れることができる．28\n\n\n\n5.3 確率的プログラミング\n\n5.3.1 アルゴリズムのプログラミングから，モデルのプログラミングへ\nベイズ流の解釈では，解析者の恣意的な選択はモデリングの段階に集中しており，モデルが決定すれば推論手法は自動的に従う．\nこのパラダイムでは，推論手法は背後に隠し，解析者はモデリングに集中するための新たなプログラミング言語があっても良いはずである．\nこのような言語を 確率的プログラミング (Probabilistic Programming) 言語と呼ぶ．\n\n\n5.3.2 確率的プログラミングはグラフィカルモデルの拡張である\n確率的グラフィカルモデルをどのようにプログラムに落とし込むかというと，確率核をシミュレーターとして実装するのである．\n逆に，シミュレーションが可能な限りどのようなモデルも実装できるので，確率的グラフィカルモデルの真の拡張であると言える．29\n\n\n5.3.3 Simulation-based Inference\n上述の通り，シミュレーターがあればモデルが定義でき，モデルがあれば推論ができる．さらに，棄却法，重点サンプリング法，MCMC，SMC などの Monte Carlo 法のレパートリーにより，殆どあらゆるシミュレーションと推論が統一的に実行できる．これが Bayes 推論の強みである．\n\n\n\n5.4 Bayes 最適化\nベイズはシステムの一部として自然に組み込まれると論じたが（第 2.1 節），現状その最先端をいくのがベイズ最適化の分野である．\nベイズ最適化は最も簡単な形では，未知の関数 \\(f:X\\to\\mathbb{R}\\) の最大値点を求める問題を，逐次意思決定問題 として解く手法である．\nベイズ数値計算 (O’Hagan, 1991) の現代的な再解釈とも捉えられる．30\nこの際，未知の関数 \\(f\\) を Gauss 過程などでモデリングし，不確実性の高い点からサンプル \\(f(x_1),f(x_2),\\cdots\\) を取って最も効率の良い方法で最大化していくことを目指す．\nベイズ最適化は多腕バンディット問題と関係が深く，２つの問題は共に一方向のエージェント・環境相互作用しか仮定していないという形での強化学習への入り口である．\n\n\n5.5 確率的データ圧縮\n殆どの（可逆）データ圧縮アルゴリズムは，シンボルの列に対する確率的モデリングと等価である．31 そしてモデルの予測精度が良いほど，データの圧縮率は高い．\nしたがって，より良いベイズ（ノンパラメトリック）モデルの開発と，より幅広いデータに対するデータ圧縮技術の発展とは両輪である．32\n\n\n5.6 モデルの自動発見\n機械学習の精神の一つに，データからの知識獲得をなるべく自動化したいというものがある．\nベイズの方から，統計解析自体を自動化する Automatic Statistician (Lloyd et al., 2014) という試みがある．これはデータを説明するモデルを自動発見し，結果を自然言語でまとめてくれる上に，モデルに含まれる不確実性に関しても報告してくれる．"
  },
  {
    "objectID": "posts/2024/AI/BAI.html#footnotes",
    "href": "posts/2024/AI/BAI.html#footnotes",
    "title": "これからはじめるベイズ機械学習",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこれは Human-AI interaction におけるガイドライン (Amershi et al., 2019), (Bansal et al., 2019) でも明確にされている点である．この方向への試みの代表がベイズ機械学習，というわけではないが，筆者はベイズ機械学習の興隆は信頼のおける AI システムの構築にための極めて盤石な土台になるだろうと論じる．↩︎\n本稿執筆後に，ほとんど同じ論調を，深層学習や基盤モデルを中心に，遥かに明瞭に述べた論文 (Papamarkou et al., 2024) を見つけたので，賢明な読者はぜひこちらを参考にしていただきたい．↩︎\n(Broderick et al., 2023, p. 2) など．↩︎\nこの違いが「過学習」という現象に見舞われるかの違いでもある．“Fortunately, Bayesian approaches are not prone to this kind of overfitting since they average over, rather than fit, the parameters” (Ghahramani, 2015, p. 454)．↩︎\n“for Bayesian researchers the main computational problem is integration, whereas for much of the rest of the community the focus is on optimization of model parameters.” (Ghahramani, 2015, p. 454)．このように，その用いる手法も鮮やかに対照的に見えるが，積分は変分近似を通じて最適化問題としても解けるし，Lengevin 法や HMC などの最適化手法は積分問題を解ける．↩︎\n(Broderick et al., 2023) が極めて説得的にこの点を指摘している．↩︎\n合理的な信念の度合い (degree of belief) は確率の公理を満たす必要がある，という主張は Cox の名前でも呼ばれる．この点から，Bayes の定理は，帰納的推論の確率論的な拡張だとも捉えられる．“This justifies the use of subjective Bayesian probabilistic representations in artificial intelligence.” “Probabilistic modelling also has some conceptual advantages over alternatives because it is a normative theory for learning in artificially intelligent systems.” (Ghahramani, 2015, p. 453)．↩︎\n現状，日本にてベイズ機械学習を専業として研究を進めている人は Emtiyaz Khan に限ると思われる．(Ghahramani, 2015, p. 452) でも “Probabilistic approaches have only recently become a mainstream approach to artificial intellifence, robotics, and machine learning.” と述べられている．↩︎\n“The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars.” (Chen et al., 2023)．↩︎\nモデルの予測結果に不確実性の定量化が伴われていたならば，モデルを信用出来ない場面で意思決定者がこれを信用したため責任があるのか，使用者には非難可能性がないのか，モデル設計者に過失があったと言えるのかの議論に，足場を与えることが出来るだろう．↩︎\n(Gal and Ghahramani, 2016) も参照．↩︎\n心理学においては「再現性問題が大きく注目される大きな契機となった「超能力論文」が出版されたのが 2011 年である」 (平石界 and 中村大輝, 2022) ようである．計量経済学における 信頼性革命 (Angrist and Pischke, 2010) は，再現性の危機の，もう一つの革新的な解決法である．↩︎\n「それでは，信頼区間は不確実性の正しい定量化を与えないではないか！」ということになるが，その通りなのである．\\(P\\)-値を計算する過程とは，帰無仮説で条件付けているだけであり，データの関数でもある．\\(P\\)-値の確率変数としての分散が大きいほど，何回か同じ実験を繰り返せばすぐに小さな \\(P\\)-値が得られることになる．これは 基準確率の誤謬 と似ている．↩︎\n“Confidence intervals suffer from an inverse inference problem that is not very different from that suffered by the NHSTP. In the NHSTP, the problem is in traversing the distance from the probability of the finding, given the null hypothesis, to the probability of the null hypothesis, given the finding.” (Trafimow and Marks, 2015)↩︎\n(Nuzzo, 2014) には，Fisher が最初に用いてから，Neyman-Pearson 理論がこれを排除したものの，コミュニティが \\(P\\)-値を誤解して都合の良いように利用するようになるまでに至った歴史が説明されている．↩︎\n(Murphy, 2022, p. 201) の議論も参照．↩︎\n(Efron, 1986) も示唆深い．↩︎\n(Mohri and Hashimoto, 2024), (Papamarkou et al., 2024, p. 3) 2.1節 なども指摘している．↩︎\n(Novello et al., 2024) では out-of-distribution detection, (Mohri and Hashimoto, 2024) は LLM の hallucination への応用．↩︎\n「筆者は，conformal prediction などの post-hoc な手法は，便利かも知れないが，「信頼区間」や「\\(P\\)-値」のような側面（第 2.3 節）も併せ持つのではないかと危惧しながら見ている．」と当初は書いていたが，どうもそう簡単な話ではないようである．(Papamarkou et al., 2024) を読んで思った．↩︎\n(Papamarkou et al., 2024, p. 5) 3.4節．↩︎\n(Wang et al., 2024) が最新のサーベイであるようだ．↩︎\nPhilipp Hennig Probabilistic ML - Lecture 1 - Introduction “Statistical Learning Theory is about Bayesian Reasoning when you don’t say out aloud what the prior is.”↩︎\nこれを 証拠 (model evidence) ともいう．↩︎\n事前分布として非有限な測度を用いた場合など，例外もある．↩︎\n“most conventional optimization-based machine-learning approaches have probabilistic analogues that handle uncertainty in a more principled manner.” (Ghahramani, 2015, p. 458)．↩︎\n(Neal and Hinton, 1998) など．↩︎\n(Ghahramani, 2015, p. 458) はこれを モデルの属人化 (personalization of models) と呼んでいる．↩︎\n(Ghahramani, 2015, p. 453) “probabilistic programming offers an elegant way of generalizing graphical models, allowing a much richer representation of models.”．↩︎\n“More generally, Bayesian optimization is a special case of Bayesian numerical computation, which is re-emerging as a very active area of research, and includes topics such as solving ordinary differential equations and numerical integration.” (Ghahramani, 2015, p. 456)．↩︎\n“All commonly used lossless data compression algorithms (for example, gzip) can be viewed as probabilistic models of sequences of symbols.” (Ghahramani, 2015, p. 456)．↩︎\n(Steinruecken et al., 2015) は記号列に対するノンパラメトリックモデルを改良することで，データ圧縮アルゴリズム PPM を改良した良い例である．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory3.html",
    "href": "posts/2024/AI/Theory3.html",
    "title": "統計的学習理論３",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/AI/Theory3.html#正則化と汎化の関係",
    "href": "posts/2024/AI/Theory3.html#正則化と汎化の関係",
    "title": "統計的学習理論３",
    "section": "1 正則化と汎化の関係",
    "text": "1 正則化と汎化の関係\n\n1.1 非一様性1\nPAC 学習 の枠組みは，分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に依らない一様な評価を要請している．\nその結果，分類問題でも，PAC 学習可能であるためには仮設集合 \\(\\mathcal{H}\\) には VC 次元の有限性が必要十分となるのであった（統計的機械学習の基本定理）．\nこれは多くの場合強すぎる．\nそこで，必要な訓練データ数が，分布 \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) に依存することも許すことを考える．この緩めた学習可能性は，\\(\\mathcal{H}\\) が有限 VC 次元集合の有限合併であることに同値になる．\n\n\n\n\n\n\nnonuniformly learnable2\n\n\n\n\n定義 1 仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) が 非一様に学習可能 であるとは，あるアルゴリズム \\(A\\) とある \\(h\\in\\mathcal{H}\\) に依存しても良い関数 \\[\nm^{\\mathrm{NUL}}:(0,1)^2\\times\\mathcal{H}\\to\\mathbb{N}\n\\] が存在して，任意の \\(\\epsilon,\\delta\\in(0,1)\\) と \\(h\\in\\mathcal{H}\\) と \\(\\mathbb{P}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) について，\\(m\\ge m^{\\mathrm{NUL}}(\\epsilon,\\delta,h)\\) ならば確率 \\(1-\\delta\\) 以上で \\[\nR(A(S_m))\\le R(h)+\\epsilon\n\\] を満たすことをいう．\n\n\n\n論理的な構造は PAC 学習可能性 と非常に似ているが，確かに真に弱い条件になっている．\n\n\n\n\n\n\nCharacterization of Nonuniform Learnability3\n\n\n\n\n定理 1 分類問題 \\(\\mathcal{Y}=2\\) を 0-1 損失 \\(l=1_{\\Delta_\\mathcal{Y}^\\complement}\\) で考えるとする．仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) について，次は同値：\n\n\\(\\mathcal{H}\\) は非一様に学習可能である．\n\\(\\mathcal{H}\\) は有限 VC 次元集合の可算合併で表せる．\n\n\n\n\n\n\n1.2 構造的リスク最小化\nPAC バウンド の証明からも判る通り，推定誤差と近似誤差のトレードオフが存在する．\nすなわち，仮説の複雑性には代償がある．\nそこで，仮説集合 \\(\\mathcal{H}\\) を小さくする代わりに，アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) が探索する範囲を小さいものにし，実質的な仮説空間のサイズを抑えることも考えられる．これを 正則化 という．\n実際，深層学習も暗黙的正則化によりよい汎化性能を出していることが徐々に明らかになりつつある．4\nすなわち，真に汎化性能を上げたい場合は，経験誤差を最小化するだけでは十分ではなく，経験誤差を小さくしながら，関数が滑らかになるように帰納バイアスを入れる必要がある．\nこの枠組みを経験リスク最小化の代わりに，構造リスク最小化 といい，(Vapnik and Chervonenkis, 1974) により提案された．\nこの方向の研究の源流は，(Bousquet and Elisseeff, 2002) らの 安定性 の理論であった．これは「実質的な仮説空間」という考え方を導入することで，機械学習モデルの予測精度理論の精緻化も生んだ．\n\n\n1.3 枠組み：アルゴリズムに目を向ける\nリスクを \\[\nR(A,S):=\\operatorname{E}[l(A(S)(X),YT)]\n\\] 経験リスクと \\[\n\\widehat{R}(A,S_n):=\\frac{1}{n}\\sum_{i=1}^nl(A(S_n)(x_i),y_i)\n\\] として，アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) の関数とみる．5\nこの場合，仮説空間 \\(\\mathcal{H}\\) 上の一様な評価は，そもそも目指さない．\n\n\n1.4 安定性\n\n\n\n\n\n\nTip\n\n\n\n\n定義 2 (安定性6) アルゴリズム \\(A:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{H}\\) が，損失関数 \\(l\\) に関して \\(\\beta\\in(0,1)\\)-安定 であるとは，任意の \\(S\\subset(\\mathcal{X}\\times\\mathcal{Y})^n\\) に対して， \\[\n\\begin{align*}\n    &\\max_{i\\in[n]}\\operatorname{E}\\biggl[\\biggl|l(A(S)(x_i),y_i)\\\\\n    &\\qquad-l(A(S\\setminus\\{z_i\\})(x_i),y_i)\\biggr|\\biggr]\\le\\beta\n\\end{align*}\n\\] が成り立つことをいう．\n\n\n\nすなわち，学習データを１つ減らしたときの損失の変化が，ある一定以下であることをいう．\nこれは感度分析的な考え方であるが，実は正則化により，アルゴリズムは安定的な挙動をするようになり，安定性が汎化誤差の上界を与える！\n\n\n1.5 主結果\n\n\n\n\n\n\nTip\n\n\n\n\n定理 2 (安定なアルゴリズムに対する汎化バウンド7) \\(A\\) を \\(\\beta_1\\)-安定で，損失関数 \\(l\\) は上界 \\(M&gt;0\\) を持つとする．このとき，\\(1-\\delta\\) の確率で \\[\nR(A,S)\\le\\widehat{R}(A,S_n)+2\\beta+(4n\\beta+M)\\sqrt{\\frac{\\log1/\\delta}{2n}}.\n\\]\n\n\n\n\n\n1.6 アルゴリズムの安定性\n一方で，アルゴリズムの安定性を示すことは難しく，通常 admissibility と Bregman divergence を通じて議論されるようである．8"
  },
  {
    "objectID": "posts/2024/AI/Theory3.html#footnotes",
    "href": "posts/2024/AI/Theory3.html#footnotes",
    "title": "統計的学習理論３",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Shalev-Shwartz and Ben-David, 2014, p. 58) 第７章 も参照．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 59) 定義7.1．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 59) 定理7.2 など．↩︎\n(Murphy, 2022, p. 455) も参照．↩︎\n(Bousquet and Elisseeff, 2002, p. 502)．↩︎\n(Bousquet and Elisseeff, 2002, p. 503)．↩︎\n(Bousquet and Elisseeff, 2002, p. 507) Theorem 12．↩︎\n(Bousquet and Elisseeff, 2002) 第５節．↩︎"
  },
  {
    "objectID": "posts/2024/AI/RL2.html",
    "href": "posts/2024/AI/RL2.html",
    "title": "強化学習",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) (Schulman et al., 2015) から PPO Algorithm (Schulman et al., 2017)\nModel-based RL\n\n\n\n\nReferences\n\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy optimization.\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms."
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html",
    "href": "posts/2024/Stat/Bayes3.html",
    "title": "信念伝搬アルゴリズム",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html#導入",
    "href": "posts/2024/Stat/Bayes3.html#導入",
    "title": "信念伝搬アルゴリズム",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 はじめに\nMP は情報理論では低密度パリティ検査符号の文脈で (Gallager, 1962) が，因果推論の文脈で (Pearl, 1982) が独立に開発している．\n信念伝搬法は変分手法とは違い，ある種のモデル（木やランダムグラフなど）では正確な推論が可能になる上に，一般に速い．1 当然 Monte Carlo 法よりも速い．\nそのため，変分推論の改良の源泉が，メッセージ伝搬を通じて探求されている (Winn and Bishop, 2005)．\n\n\n1.2 ランダムグラフ上のスピン系の分配関数\n頂点数 \\(N\\)，辺数 \\(E\\) の Erdös-Renyi ランダムグラフ \\(\\mathcal{G}(N,E)\\) 上のスピン \\((\\sigma_i)_{i\\in[N]}\\) と相互作用 \\((\\psi_{ij})_{(i,j)\\in E}\\) を考える： \\[\n\\psi_{ij}(\\sigma_i,\\sigma_j)=\\exp\\biggr(-\\beta J_{ij}\\sigma_i\\sigma_j\\biggl).\n\\]\nランダムグラフでは平均的なループの長さは \\(\\log N\\) であり，局所的に木の構造を持つため，このことを利用した近似を使って信念伝搬を行う．\n\\(Z_{i\\to j}(\\sigma_i)\\) を，\\(i\\) を根とする木から \\(j\\) を根とする木を除去し，\\(i\\) におけるスピンの値が \\(\\sigma_i\\) であると条件付けた場合の部分木上の分配関数，\\(Z_i(\\sigma_i)\\) を \\(i\\) におけるスピンの値が \\(\\sigma_i\\) であると条件付けた場合の木全体の分配関数とすると，次の関係がある： \\[\nZ_i(\\sigma_i)=\\prod_{j\\in\\partial i}\\left(\\sum_{\\sigma_j}Z_{j\\to i}(\\sigma_j)\\psi_{ij}(\\sigma_i,\\sigma_j)\\right).\n\\]\nするとこのとき， \\[\\begin{align*}\n    \\eta_i(\\sigma_i):=\\frac{Z_i(\\sigma_i)}{\\sum_{\\sigma'}Z_i(\\sigma')}&=\\frac{1}{\\sum_{\\sigma'}Z_i(\\sigma')}\\prod_{j\\in\\partial i}\\left(\\sum_{\\sigma_j}Z_{j\\to i}(\\sigma_j)\\psi_{ij}(\\sigma_i,\\sigma_j)\\right)\\\\\n    &=:\\frac{1}{z_i}\\prod_{j\\in\\partial i}\\left(\\sum_{\\sigma_j}\\eta_{j\\to i}(\\sigma_j)\\psi_{ij}(\\sigma_i,\\sigma_j)\\right).\n\\end{align*}\\] は Boltzmann-Gibbs 分布の周辺分布になっている： \\[\nm_i=\\langle\\sigma_i\\rangle=\\sum_{\\sigma}\\eta_i(\\sigma)\\sigma.\n\\]\nこのときの正規化定数は \\[\\begin{align*}\n    z_i&=\\sum_{\\sigma_i}\\prod_{j\\in\\partial i}\\left(\\sum_{\\sigma_j}\\eta_{j\\to i}(\\sigma_j)\\psi_{ij}(\\sigma_i,\\sigma_j)\\right)\n    =\\sum_{\\sigma_i}\\prod_{j\\in\\partial i}\\left(\\sum_{\\sigma_j}\\frac{Z_{j\\to i}(\\sigma_j)}{\\sum_{\\sigma'}Z_{j\\to i}(\\sigma')}\\psi_{ij}(\\sigma_i,\\sigma_j)\\right)\\\\\n    &=\\sum_{\\sigma_i}\\frac{Z_i(\\sigma_i)}{\\prod_{j\\in\\partial i}\\sum_{\\sigma_j}Z_{j\\to i}(\\sigma_j)}=\\frac{Z}{\\prod_{j\\in\\partial i}\\sum_{\\sigma_j}Z_{j\\to i}(\\sigma_j)},\\\\\n    z_{j\\to i}&=\\frac{\\sum_{\\sigma_j}Z_{j\\to i}(\\sigma_j)}{\\prod_{k\\in\\partial j\\setminus i}\\sum_{\\sigma_k}Z_{k\\to j}(\\sigma_k)}.\n\\end{align*}\\]\nと表せる．\n\\(z_i\\) の分母には \\(Z\\) が現れていることを用いると \\(Z\\) は，任意の位置 \\(i\\in[N]\\) を起点として，次のような展開表示ができる： \\[\\begin{align*}\n    Z&=\\sum_{\\sigma_i}Z_i(\\sigma_i)=z_i\\prod_{j\\in\\partial i}\\left(\\sum_{\\sigma_j}Z_{j\\to i}(\\sigma_j)\\right)=z_i\\prod_{j\\in\\partial i}\\left(z_{j\\to i}\\prod_{k\\in\\partial j\\setminus i}\\sum_{\\sigma_k}Z_{k\\to j}(\\sigma_k)\\right).\n\\end{align*}\\] ここで，最右辺の \\(\\sum_{\\sigma_k}Z_{k\\to j}(\\sigma_k)\\) は \\(z_{j\\to i}\\) を計算するのと同じ要領で計算されることになり，最終的に木の葉まで再帰的に計算することで，公式 \\[\nZ=z_i\\prod_{j\\in\\partial i}\\left(z_{j\\to i}\\prod_{k\\in\\partial j\\setminus i}z_{k\\to j}\\cdots\\right)=z_i\\prod_{j\\in\\partial i}\\left(\\frac{z_j}{z_{ij}}\\prod_{k\\in\\partial j\\setminus i}\\frac{z_k}{z_{jk}}\\cdots\\right)=\\frac{\\prod_{i\\in[N]}z_i}{\\prod_{(i,j)\\in E}z_{ij}}.\n\\]\nよって，この再帰的計算により，自由エネルギー \\[\nF=-T\\log Z=\\sum_{i\\in[N]}\\biggr(-T\\log z_i\\biggl)-\\sum_{(i,j)\\in E}\\biggr(-T\\log z_{ij}\\biggl).\n\\] も得られることになる．\n\n\n1.3 無限レンジ極限での高温解の安定性\n(Thouless et al., 1977) は TAP 方程式により，(Sherrington and Kirkpatrick, 1975) 模型を解いた．\nその際の結果が，同じく無限サイズのグラフである Bethe 格子 上でならば，信念伝搬法により議論でき，有効 cavity 場が次のように与えられるという： \\[\n\\beta h_0=\\sum_{i=1}^k\\operatorname{atanh}\\biggr(\\tanh(\\beta J_{ij})\\tanh(\\beta h_i)\\biggl)+H.\n\\]\nこのモデルでもスピングラス相が出現することが，信念伝搬法が失敗する（局所不安定になる）こととして現れる．これはグラフの木構造を破壊するような長距離の相関が出現することによる．スピングラス相と常磁性相の境界は Almeida-Thouless 線 (Thouless, 1986) という．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html#共同体抽出",
    "href": "posts/2024/Stat/Bayes3.html#共同体抽出",
    "title": "信念伝搬アルゴリズム",
    "section": "2 共同体抽出",
    "text": "2 共同体抽出\n\n2.1 はじめに\nグラフの頂点をクラスタリングした際のクラスターに当たる概念を 共同体 (community) という．\n最初におもつくような方法は，横断する辺の数が最小になるようなクラスター境界を決定する方法であるが，これは NP 困難である．\nこれを少し修正して，各辺に対して edge centrality を計算し，これを順に脱落させて edge centrality を計算しなおすというような反復を行う Girvan-Newman アルゴリズム (Girvan and Newman, 2002) が最も有名なものである（引用数２万！）．\n\n\n2.2 modularity 最大化による方法\nGirvan-Newman アルゴリズムの中では，分割の「良さ」の指標として modularity \\[\nQ=\\frac{1}{2m}\\sum_{ij}\\left(A_{ij}-\\frac{d_id_j}{2N}\\right)\\delta_{s_i}\\delta_{s_j}\n\\] を定義し，アルゴリズムの停止の指標としていた．\n現在では，逆にこの modularity を最適化することでクラスタリングを達成する手法が主流の１つである．\n辺を全て消去した状況から１つずつ追加していき，この modularity の値を最大化する階層的クラスタリングアルゴリズムとして (Newman, 2004), (Clauset et al., 2004) が提案された．\n最適化問題として定式化された以上，模擬アニーリングが用いられることもある (Guimerà et al., 2004)．\n\n\n2.3 スピングラス対応\nじきに (Reichardt and Bornholdt, 2006) によって，共同体抽出の問題は Potts 模型の基底状態探索と等価であることが示された．\n実際，各頂点に割り当てられた未知の Potts スピン \\(\\sigma_i\\) に対して，同じ状態を持つ頂点同士は繋がろうとし，違う頂点を持つ場合は結合は疎になる傾向があるという状況は \\[\nH(\\sigma)=-\\sum_{i&lt;j}J_{ij}\\delta(\\sigma_i,\\sigma_j),\n\\] \\[\nJ_{ij}:=J\\biggr(A_{ij}-\\gamma p_{ij}\\biggl)\n\\] というハミルトニアンで表現できる．\\(A_{ij}\\) は隣接行列 \\(A=(A_{ij})\\in M_n(2)\\) の成分，\\(p_{ij}\\) は辺の数の期待値，\\(\\gamma\\) は推定されるクラスタ数を決定するハイパーパラメータ (resolution parameter とも表現するらしい) である．\nこう捉えると，たしかに模擬アニーリングは１つの選択肢だ．\n\n\n2.4 信念伝搬による復号\nPotts モデルではないが，(Hastings, 2006) は同様にスピングラスモデルと同一視をし，こちらでは信念伝搬により基底状態探索を行った．\n一方で (Newman and Leicht, 2007) は混合モデルとみなし，EM アルゴリズムによる方法を見出している．ここまで来ると確率的ブロックモデル (Snijders and Nowicki, 1997) による方法に非常に似通っており，(Decelle et al., 2011) はまさにこの見方をしている．\nそもそも，EM アルゴリズムと西森ラインは深い関係がある．2 実際，EM アルゴリズムも，確率的ブロックモデルにおけるクラスの割り当ても，\\(k\\)-平均法のクラスタ数も，相転移を起こす．背後に何か物理過程と対応がつくものが存在するのかもしれない．\nブロックモデル (blockmodel / image graph) とはグラフの分割に関するモデルで，クラスタ数 \\(q\\)，\\([q]\\) 上の確率分布（頂点数の分布）\\(\\{n_\\alpha\\}_{\\alpha\\in[q]}\\)，グループ間に辺が存在する確率を表す行列 \\((p_{ab})_{a,b\\in[q]}\\) のパラメータからなる．3\n\n\n2.5 スペクトルを通じた方法\nモジュラリティ最適化と対照的な手法として，スペクトルを用いた方法がある．有名なサーベイには (Luxburg, 2007) がある．\n最も直接的には，Laplacian 行列 の固有空間分解を通じて頂点集合を別の潜在空間に埋め込み，そこで \\(\\mathbb{R}^N\\)-クラスタリングを行うという (Donath and Hoffman, 1973) 以来の方法である．この文献はグラフの分割を問題にしているが，(Donetti and Muñoz, 2004) はコミュニティ抽出に集中している．\nこちらの方が数学的にグラフの構造を捉えられそうなものであるが，現状，物理学的な背景を持った最適化・ベイズ推論に基づいた手法の方が性能が良いようである．4"
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html#圧縮センシング",
    "href": "posts/2024/Stat/Bayes3.html#圧縮センシング",
    "title": "信念伝搬アルゴリズム",
    "section": "3 圧縮センシング",
    "text": "3 圧縮センシング\n\n3.1 データ圧縮との違い\n(Krzakala et al., 2015, p. 30) には大変魅力的な導入がなされている．\n例えば JPEG 2000 ではデータ圧縮の技術が使われており，これは画像データが wavelet 基底表現では大変スパースなデータになることを用いている．\n圧縮センシングは最初から観測がスパースであるとし，データの復元の代わりに観測がなんだったかを推定することを考える．この技術は MRI に応用される (Lustig et al., 2007) ことで有名である．\n即ち，「データは正しい基底に関してはスパースになるはずである」という事前情報の下， \\[\n\\vec{y}=G\\vec{s},\\qquad G\\in M_{MN}(\\mathbb{R}),M&lt;N,\n\\] という計画行列 \\(G\\) と低次元の観測のみを通じて，真の観測 \\(\\vec{s}\\) を，ある行列 \\(A\\) に関して \\[\n\\vec{s}=A\\vec{x}\n\\] として得られる \\(\\vec{x}\\) はスパースになるという追加情報を通じて推定することが，圧縮センシングの問題になる．\n\n\n3.2 スパースなデータの少数観測からの復元\n\\((A,\\vec{x})\\) の組について最適化を行い，最もスパースな \\(\\vec{x}\\) を決定するという問題 \\[\n\\operatorname*{argmin}_{\\vec{x}}\\|\\vec{y}-F\\vec{x}\\|_{\\ell_0}\n\\] は NP-困難であるが，統計力学の方法により \\(M/N=:\\rho\\) を一定にした比例的高次元極限 \\(N\\to\\infty\\) に関する漸近論が得られる．\n結論として，\\(\\vec{s}\\) の長さ \\(N\\) に対して，実際は \\[\nM=O\\left(N^{1/4}\\log^{5/2}N\\right)\n\\] の固定された (nonadaptive) 観測があれば十分な復元が可能であるという．\n具体的には，特定の条件の下では，（適切な基底に関する）スパースな \\(l^p\\)-展開係数が大きい順に \\(n\\) 個わかれば，\\(l^2\\)-誤差が　\\(O(n^{1/2-1/p})\\) のオーダーに従う (Pinkus, 1985)．\nそして，\\(M=O\\left(n\\log N\\right)\\) の観測があれば，この \\(n\\) 個の重要な係数が十分な精度で復元できる，というカラクリである．そのためのアルゴリズムには Basis Persuit (Chen et al., 1998) という凸最適化を通じた極めて効率的なアルゴリズムが利用可能である．\n\n\n3.3 \\(\\ell_p\\)-ノルム最適化の方法\n代わりに \\[\n\\operatorname*{argmin}_{\\vec{x}}\\|\\vec{y}-F\\vec{x}\\|_{\\ell_p}\n\\] という最適化問題を考えるとこれは凸最適化問題であるため，突然線型計画法により多項式時間で解ける問題に変貌する．\n\\(p=1\\) と取ることが実用上スパースな結果をうまく出してくれるようである．実際，一定の条件の下では \\(p=0\\) の場合（本当に解きたかった問題）の解に一致する (Candes and Tao, 2006)．\nこうして LASSO 様のアルゴリズムにたどり着くが，このアプローチでは観測数の比 \\(\\rho\\) は真の限界より大きく取る必要がある．一方で，後述の信念伝搬の方法だと復号限界ギリギリまで通用することが多い．\n\n\n3.4 スピングラス対応\n観測モデルが線型 Gauss であるとするとき，ランダムに観測されたスパース符号の復元の問題は，事前分布 \\(p(x)=(1-\\rho)\\delta(x)+\\rho\\phi(x)\\) が定める事後分布 \\[\np(x|F,y)=\\frac{1}{Z}\\prod_{i=1}^N\\biggr((1-\\rho)\\delta(x_i)+\\rho\\phi(x_i)\\biggl)\\prod_{\\mu=1}^M\\frac{1}{\\sqrt{2\\pi\\Delta_\\mu}}\\exp\\left(-\\frac{\\lvert y_\\mu-F_{\\mu-}\\cdot x\\rvert^2}{2\\Delta_\\mu}\\right)\n\\] を通じた Bayes 推論として定式化できる．するとこれは負の対数尤度 \\[\nH(x)=-\\sum_{i=1}^N\\log\\biggr((1-\\rho)\\delta(x_i)+\\rho\\phi(x_i)\\biggl)+\\sum_{\\mu=1}^M\\frac{1}{2\\Delta_\\mu}\\lvert y_\\mu-F_{\\mu-}\\cdot x\\rvert^2\n\\] が定める Boltzmann 分布と見ることができるが，これは長距離無秩序を持つスピングラス系の Hamiltonian となっており，この系の基底状態の特定として問題が捉え直せる．\n\n\n3.5 平均場変分ベイズによる復号（一般論）\n一般の事後分布が \\(p(x|y)\\,\\propto\\,p(y|x)p(x)\\) の形で与えられているとし，分布 \\(q\\) に関するこの系の自由エネルギーは \\[\n\\mathcal{L}=[\\log p(y|x)p(x)]_q-\\int_\\Omega q(x)\\log q(x)\\,dx=[-E(y,x)]_q-\\int_\\Omega q(x)\\log q(x)\\,dx\n\\tag{1}\\] と表せる．ただし，\\(E:=-\\log p(y|x)p(x)\\) は内部エネルギーとした．\nすると，この \\(\\mathcal{L}\\) を最小化する \\(q\\) が平衡分布となるわけである．これで自由エネルギーに関する変分原理に問題が定式化し直された．\n平均場近似とは，この \\(q\\) に関して \\[\nq(x)=\\prod_{i=1}^Nq_i(x_i)\n\\] という積の形を仮定した場合の表現 \\[\n\\mathcal{L}_{\\mathrm{MF}}=[-E(y,x)]_q-\\sum_{i=1}^N\\int q_i(x_i)\\log q_i(x_i)\\,dx\n\\] に関して最小化を考えることをいう．\n実はこの場合の \\(\\mathcal{L}_{\\mathrm{MF}}\\) は KL-乖離度となっており，停留条件は \\[\nq_i(x_i)=\\frac{e^{[-E(y,x)]_q}}{Z_i}\n\\] で得られ，これは平均場方程式と呼ばれるものである．5\nこのことから，\\(q\\) が一般の分布族の場合でも，KL 乖離度を最小化することによって推論をするという変分推論の指針が示唆される．\n\n\n3.6 平均場変分ベイズによる復号（圧縮センシングの場合）\nこの場合，\\(R_i,\\Sigma^2_i\\) を用いれば \\[\nQ_i(x_i)=\\frac{1}{Z_i}p(x_i)\\frac{1}{\\sqrt{2\\pi\\Sigma_i^2}}e^{-\\frac{(x_i-R)^2}{2\\Sigma_i^2}}\n\\] というように，事前分布と Gauss 分布の積の形を持ち，\\(\\Sigma_i^2,R_i\\) の値も順次決定される．\nするとその結果を用いれば逐次的に計算することができる．実際，これを \\[\n\\sum_{\\mu}F^2_{\\mu i}=1\n\\] の仮定の下で行ったものが，Iterative Threshouding (Maleki and Donoho, 2010) である．\n\n\n3.7 信念伝搬による復号\n式 (1) は \\[\n\\mathcal{L}=[\\log p(y|x)]_q-\\operatorname{KL}(q,p)\n\\] とも表せる．第一項は人工的な分布 \\(q\\) に関する平均であるから計算できるものとすると，\\(\\operatorname{KL}(q,p)\\) も，平均場近似の下では，各 \\(x_i\\) は独立と仮定しているから各項ごとに計算できる： \\[\n\\operatorname{KL}(q_i,p_i)=-\\log\\widetilde{Z}_i-\\frac{c_i+(a_i-R_i)^2}{2\\Sigma^2_i}.\n\\] 右辺はノルム，\\(q_i\\) による \\(x_i\\) の平均と分散によって計算できる．\n実際，メッセージ \\[\nm_{\\mu\\to i}(x_i)=\\frac{1}{Z^{\\mu\\to i}}\\int\\prod_{j\\ne i}dx_je^{-\\frac{1}{2\\Delta_\\mu}\\left(\\sum_{j\\ne i}F_{\\mu j}x_j+F_{\\mu i}x_i-y_\\mu\\right)^2}\\prod_{j\\ne i}m_{j\\to\\mu}(x_j)\n\\] \\[\nm_{i\\to\\mu}(x_i)=\\frac{1}{Z^{i\\to\\mu}}\\biggr((1-\\rho)\\delta(x_i)+\\rho\\phi(x_i)\\biggl)\\prod_{\\gamma\\ne\\mu}m_{\\gamma\\to i}(x_i)\n\\] の伝播により計算できるところを，\\(N\\to\\infty\\) の極限では，密度 \\(m_{\\mu\\to i},m_{i\\to\\mu}\\) そのものではなく平均 \\(a_{i\\to\\mu}\\) と分散 \\(v_{i\\to\\mu}\\)，その局所的な近似 \\(a_i,v_i\\)（「信念」）を伝えるのみで近似計算が可能である．\nその結果，とりわけ圧縮センシングの問題に関しては，正確性とスピードの面で極めて効率的な信念伝搬法が得られる (Kschischang et al., 2001), (Yedidia et al., 2003)．アルゴリズムが停止したと判断されたのち，\\(a_i\\) によって真の信号 \\(x_i^*\\) の推定とし，\\(v_i\\) は推定の不確実性を表す．\\(N\\to\\infty\\) の極限において一致性を持つ．\nBP はあるクラスの分布（混合 Gauss など）に従う観測 \\(x\\) について，理論的な復号限界ピッタリまで復元可能である．\n\n\n3.8 近似メッセージ伝搬 (AMP)\nこの信念伝搬方では \\(2M\\times N\\) のメッセージが送信されるが，ある仮定の下では \\(N+M\\) のメッセージで十分である (David L. Donoho et al., 2009)．\nこれは物理的には TAP 方程式 (Thouless et al., 1977) に基づく消息であり，\\(N\\to\\infty\\) の極限で信念伝搬法に一致する．\nその後，AMP は必ずしもスパースな観測に限らずとも，一般の設定における復号（特に \\(M\\)-推定）においても有用であることが報告されている (D. Donoho and Montanari, 2016)．\n低ランク行列の推定問題に対する変種 (Antenucci, Krzakala, et al., 2019) も提案されている．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html#低ランク行列分解",
    "href": "posts/2024/Stat/Bayes3.html#低ランク行列分解",
    "title": "信念伝搬アルゴリズム",
    "section": "4 低ランク行列分解",
    "text": "4 低ランク行列分解\n低ランク行列を分解するという推定問題においては，情報理論的な復号限界の手前に，困難相 (hard phase) が生じ，そこではメッセージ伝搬法が無効になる．\n低ランク行列分解における困難相は，AMP が適用可能な範囲にも出現し，そこでは AMP の効率が大きく落ちてしまうという (Antenucci, Franz, et al., 2019)．\nこの困難相は物理的にはスピングラス相に由来するものであるから，そのことを踏まえて AMP を修正した ASP (Approximate Survey Propagation) が提案されている (Antenucci, Krzakala, et al., 2019) が，それでも性能は改善しないという！\nこの困難相というものは，何か背後の物理的な理由（はたまたもっと深遠な理由？）によって，効率的なアルゴリズムは存在しないということが本当に保証されている領域なのかもしれない：\n\nThis result provides further evidence that the hard phase in inference problems is algorithmically impenetrable for some deep computational reasons that remain to be uncovered. (Antenucci, Franz, et al., 2019)"
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html#参考文献",
    "href": "posts/2024/Stat/Bayes3.html#参考文献",
    "title": "信念伝搬アルゴリズム",
    "section": "5 参考文献",
    "text": "5 参考文献"
  },
  {
    "objectID": "posts/2024/Stat/Bayes3.html#footnotes",
    "href": "posts/2024/Stat/Bayes3.html#footnotes",
    "title": "信念伝搬アルゴリズム",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Zdeborová and Krzakala, 2016, p. 471)↩︎\n(Krzakala et al., 2015, p. 23)，ベイズ統計学とスピングラスの稿 も参照．↩︎\n(Krzakala et al., 2015, p. 21) 4.1節，(Fortunato, 2010, p. 124) 9.2節．↩︎\n(Krzakala et al., 2015, p. 29) “In fact previous methods like spectral methods are not so good as the algorithm proposed in this section.”↩︎\n同様の事実を (池田思朗 et al., 2004, p. 396) 定理１は情報幾何学の言葉で述べている．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html",
    "href": "posts/2024/Stat/Regression.html",
    "title": "セミパラメトリック重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n回帰モデルの点推定で最も広く用いられるアルゴリズムは OLS である： \\[\n\\widehat{\\beta}:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\lvert Y-Xb\\rvert^2_2.\n\\]\nその他の推定法は別稿で扱う：\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\nOLS は応答 \\(Y\\) を最もよく復元する推定量 \\(X\\widehat{\\beta}\\) を構成する．損失を \\(Y\\) のなす Euclid 空間 \\(\\mathbb{R}^n\\) 内の距離とした \\(M\\)-推定量である．\n尤度を使わない推定法であるが，Gauss-Markov モデル（均一誤差モデル） \\[\n\\operatorname{E}[\\epsilon|X]=0,\\qquad\\mathrm{C}[\\epsilon|X]=\\sigma^2I_n\n\\] に関しては，誤差 \\(\\epsilon\\) の分布に依らず，（セミパラメトリック）漸近有効性を持つ．しかも \\(\\epsilon_i\\) は i.i.d. とは限らない．\nその構成から予期される通り，OLS 推定量は極めて良い線型代数的な性質を持つ．実際， \\[\n\\widehat{\\beta}=(X^\\top X)^{-1}X^\\top Y\n\\] という表示をもち，\\(X\\widehat{\\beta}\\) は \\(Y\\) の \\(X\\) の列ベクトルの貼る空間への線型射影である．\nここでは重回帰モデルにおける OLS 推定量の性質を調べる．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#序",
    "href": "posts/2024/Stat/Regression.html#序",
    "title": "セミパラメトリック重回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n回帰モデルの点推定で最も広く用いられるアルゴリズムは OLS である： \\[\n\\widehat{\\beta}:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\lvert Y-Xb\\rvert^2_2.\n\\]\nその他の推定法は別稿で扱う：\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\nNo matching items\n\n\nOLS は応答 \\(Y\\) を最もよく復元する推定量 \\(X\\widehat{\\beta}\\) を構成する．損失を \\(Y\\) のなす Euclid 空間 \\(\\mathbb{R}^n\\) 内の距離とした \\(M\\)-推定量である．\n尤度を使わない推定法であるが，Gauss-Markov モデル（均一誤差モデル） \\[\n\\operatorname{E}[\\epsilon|X]=0,\\qquad\\mathrm{C}[\\epsilon|X]=\\sigma^2I_n\n\\] に関しては，誤差 \\(\\epsilon\\) の分布に依らず，（セミパラメトリック）漸近有効性を持つ．しかも \\(\\epsilon_i\\) は i.i.d. とは限らない．\nその構成から予期される通り，OLS 推定量は極めて良い線型代数的な性質を持つ．実際， \\[\n\\widehat{\\beta}=(X^\\top X)^{-1}X^\\top Y\n\\] という表示をもち，\\(X\\widehat{\\beta}\\) は \\(Y\\) の \\(X\\) の列ベクトルの貼る空間への線型射影である．\nここでは重回帰モデルにおける OLS 推定量の性質を調べる．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#重回帰",
    "href": "posts/2024/Stat/Regression.html#重回帰",
    "title": "セミパラメトリック重回帰分析",
    "section": "2 重回帰",
    "text": "2 重回帰\n\n2.1 設定\n計画行列 \\(X=(X_1\\;X_2)\\) に関して，回帰モデル \\[\nY=X\\widehat{\\beta}+\\widehat{\\epsilon}\n\\tag{1}\\] に対して，\\(X_1\\) を入れなかった場合 \\[\nY=X_2\\widetilde{\\beta}_2+\\widetilde{\\epsilon}\n\\tag{2}\\] を考える．\n部分モデル (2) の回帰係数 \\(\\widetilde{\\beta}_2\\) は \\[\n\\widetilde{\\beta}_2=(X_2^\\top X_2)^{-1}X_2^\\top Y\n\\] で得られる．\n\n\n2.2 Frisch-Waugh-Lovell の定理\n次の結果は少なくとも (Yule, 1907) から知られていたが，計量経済学では (Frisch and Waugh, 1933) と (Lovell, 1963) の名前で知られる．\n\n\n\n\n\n\nFrisch-Waugh-Lovell の定理\n\n\n\n\\(X_1\\) の列ベクトルが貼る空間の \\(\\mathbb{R}^n\\) 上の補空間への射影を \\[\nH_2:=I_n-H_1,\\qquad H_1:=X_1(X_1^\\top X_1)^{-1}X_1^\\top\n\\] で表すと， \\[\n\\widehat{\\beta}_2=(\\widetilde{X}_2^\\top\\widetilde{X}_2)^{-1}\\widetilde{X}_2^\\top\\widetilde{Y},\\qquad\\widetilde{X}_2:=H_2X_2,\\widetilde{Y}:=H_2Y.\n\\]\n\n\nすなわち，\\(X_1,X_2\\) で回帰した (1) の係数 \\(\\widehat{\\beta}_2\\) は，まず \\(X_2\\) を \\(X_1\\) を説明変数で回帰した後に，(2) の代わりに \\(Y\\) をその残差 \\(\\widetilde{X}_2\\) で回帰して得る係数に等しい．\nこれを重回帰係数の 部分回帰係数 としての解釈とも呼ぶ (Ding, 2024, p. 60)．\n\n\n2.3 Cochran の公式\n\n\n\n\n\n\n(Cochran, 1938)1\n\n\n\n\\(X_1\\) を \\(X_2\\) で回帰した際の係数を \\(\\widehat{\\delta}\\) とする： \\[\nX_1=X_2\\widehat{\\delta}+\\widehat{U}.\n\\] このとき， \\[\n\\widetilde{\\beta}_2=\\widehat{\\beta}_2+\\widehat{\\delta}\\widehat{\\beta}_1.\n\\]\n\n\nこれは \\(X_2\\) の \\(Y\\) への影響のうち，\\(X_1\\) を通じたもの \\(\\widehat{\\delta}\\widehat{\\beta}_1\\) とそうでないものとを分解していると見れる．\\(\\widehat{\\beta}_2\\) は \\(\\widehat{\\beta}_1\\) の方向に縮小するとも見れる．\n\\(\\widehat{\\delta}\\widehat{\\beta}_1\\) の符号によっては，\\(\\widetilde{\\beta}_2,\\widehat{\\beta}_2\\) の符号が異なることがある．このような現象は (Simpson, 1951) のパラドックスともいう．2\n計量経済学では \\(\\widehat{\\delta}\\widehat{\\beta}_1\\) の項を 欠落変数バイアス (omitted variable bias) とも呼ぶ． \\[\n\\widehat{\\delta}=\\frac{\\mathrm{C}[X_1,X_2]}{\\sqrt{\\mathrm{V}[X_1]\\mathrm{V}[X_2]}}\n\\] であるから，\\(X_1,X_2\\) が無相関であった場合はこの項は零になる．\nすなわち，誤差 \\(\\epsilon\\) が外生性の仮定 \\(\\operatorname{E}[\\epsilon|X]=0\\) を満たすまでに十分多くの説明変数を回帰モデルに入れないと，OLS 推定量はバイアスを持ってしまう．軽量経済学において，\\(X\\) が \\(\\epsilon\\) と相関を持つことを 内生性 (endogeneity) という (B. E. Hansen, 2022, p. 335), (Hayashi, 2000, p. 64)．3\n(Baron and Kenny, 1986) の媒介分析はこのように OLS 推定を複数の回帰モデルに対して実行し，直接効果と間接効果の量を推定する．この手続きは (Wright, 1918) のパス分析と深い関係がある．\n\n\n2.4 交絡と共変量統制\n具体的に，処置変数を \\(Z_i\\in\\{0,1\\}\\) とした回帰分析 \\[\nY_i=\\widetilde{\\beta}_0+\\widetilde{\\beta}_1Z_i+\\widetilde{\\beta}_2^\\top X_i+\\widetilde{\\epsilon}_i\n\\] を考える．この際，欠落した説明変数 \\(U_i\\) であって，処置変数 \\(Z_i\\) と相関を持つものを 交絡因子 という．4\nフルモデル \\[\nY_i=\\widehat{\\beta}_0+\\widehat{\\beta}_1Z_i+\\widehat{\\beta}_2^\\top X_i+\\widehat{\\beta}_3^\\top U_i+\\widehat{\\epsilon}_i\n\\] に関して，Cochran の公式によれば，\\(Z_i\\) を \\(U_i\\) に関して回帰した際の \\(U_i\\) の係数を \\(\\widehat{\\delta}\\) とすると， \\[\n\\widetilde{\\beta}_1-\\widehat{\\beta}_1=\\widehat{\\beta}_3\\widehat{\\delta}\n\\] が成り立つ．加えて，\\(U_i\\) を \\(X_i\\) に関して回帰して得る残差を \\(e_i\\) とすると，\\(\\widehat{\\delta}\\) の値はこの \\(e_i\\) の値のグループ間差に等しい： \\[\n\\widehat{\\delta}=\\overline{e}^{(1)}-\\overline{e}^{(0)}.\n\\]\nすなわち，\\(X_i\\) で説明される分を除いて，\\(U_i\\) の値が処置群と管理群とで平均的に大きな差があるほど，交絡によるバイアスは大きいものとなる．\n\n\n2.5 leverage score\n射影行列 \\[\nH:=X(X^\\top X)^{-1}X^\\top\n\\] は鍵となる値で，この対角成分は leverage score と呼ばれ，次を満たす5 \\[\n\\operatorname{tr}(H)=\\operatorname{rank}(H)=p.\n\\]\n\n\n2.6 VIF\n\n\n\n\n\n\n命題6\n\n\n\n\\(\\widehat{\\beta}_j\\) を \\(Y\\) を \\((1_n,X_1,\\cdots,X_q)\\) に関して回帰した際の係数とする．真のモデルがある関数 \\(f\\) に関して \\(y_i=f(x_i)+\\epsilon_i\\) で \\(\\epsilon_i\\sim(0,\\sigma^2)\\) が互いに相関を持たない場合，次が成り立つ： \\[\n\\mathrm{V}[\\widehat{\\beta}_j]=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_{ij}-\\overline{x}_j)^2}\\frac{1}{1-R^2_j}.\n\\] ただし \\(R_j^2\\) とは \\(X_j\\) を \\((1_n,X_1,\\cdots,X_{j-1},X_{j+1},\\cdots,X_q)\\) に関して回帰した際の決定係数とした．\n\n\nこの際，最初の因子は \\(Y\\) を \\((1_n,X_j)\\) に関して回帰した際の係数 \\(\\widetilde{\\beta}_j\\) の分散に一致する．従って次の因子 \\[\n\\operatorname{VIF}_j:=1/(1-R_j^2)\n\\] は，他の説明変数 \\(X_1,\\cdots,X_{j-1},X_{j+1},\\cdots,X_q\\) を加えたことによる，\\(X_j\\) の推定係数の増大具合を表す．\nこれを 分散拡大係数 (VIF: Variance Inflation Factor) と呼ぶ．\n\n\n2.7 Bias-Variance Tradeoff\n一般に全ての関連する説明変数を入れた方が現実に近く，推定・予測精度は高くなると考えられる．\nしかし VIF の命題から，説明変数を増やすたびに OLS 推定量の分散は増大することがわかる．\nこのようなトレードオフを バイアス-分散トレードオフ (Bias-Variance Tradeoff) という．\n\n\n2.8 操作変数\n仮に \\(X_2\\) が内生性を持つとする： \\[\n\\operatorname{E}[\\epsilon|X_1]=0,\\qquad\\operatorname{E}[\\epsilon|X_2]\\ne0.\n\\]\n\n\n\n\n\n\nこのとき \\(U\\) であって次を満たすものを 操作変数 という：\n\n（広義）外生性 \\[\n\\mathrm{C}[U,\\epsilon]=0\n\\]\n関連性 \\[\n\\mathrm{C}[U,X_2]\\ne0\n\\]\n\n\n\n\n操作変数 \\(U\\) を用いれば，回帰モデル (1) の両辺の \\(U\\) との相関を考えると，外生性から \\[\n\\mathrm{C}[U,V]=\\mathrm{C}[U,X]\\widehat{\\beta}\n\\] が成り立ち，これを通じて \\(\\widehat{\\beta}\\) を推定できる．これを IV 推定量 という．\n\\(\\epsilon,\\epsilon_2\\) の相関を測ることで，内生性の強さを定量化することもできる (Chan and Tobias, 2020, p. 14)．\n\n\n2.9 ２段階 OLS\n以上の手続きは，ここまで議論してきた方法の特別な場合である．\n実際，\\(X_2\\) を \\(U\\) に関して回帰することを考える： \\[\nX_2=U\\delta+\\epsilon_2.\n\\]\nこの回帰により得る推定値 \\(\\widehat{X}_2=U\\widehat{\\delta}\\) は \\(\\epsilon\\) と相関を持たない．相関が取り除かれた成分を射影によって取り出していると見れる．\n続いて \\(X_2\\) を \\(\\widehat{X}_2\\) に取り替えて，\\(Y\\) に向かって回帰することで得る推定量を TSLS (Two-Stage Least Squares) 推定量 という．\n\\(U\\) が２値変数であるときは (Wald, 1940) 推定量ともいう．\\(X_2\\) の次元と \\(U\\) の次元が一致するとき，TSLS 推定量は IV 推定量と一致する．\n一般に TSLS も一致性と漸近正規性を持つ (B. E. Hansen, 2022, pp. 351–352)．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#不均一分散",
    "href": "posts/2024/Stat/Regression.html#不均一分散",
    "title": "セミパラメトリック重回帰分析",
    "section": "3 不均一分散",
    "text": "3 不均一分散\n\n3.1 OLS の漸近正規性\nGuass-Markov モデルの線型モデルとしての最大の仮定は，均一の分散 \\(\\sigma^2\\) を仮定していたことである．\nしかしこの仮定を外しても，OLS 推定量は不偏性・一致性・漸近正規性を持つ（この順に追加の条件が厳しくなる）．\n\n\n\n\n\n\n命題7\n\n\n\n\\[\nY_i=X_i\\beta+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma_i^2),\n\\] に関して，計画行列 \\(X\\) は最大階数であるとする．このとき，次が成り立つ：\n\nOLS 推定量 \\(\\widehat{\\beta}\\) は不偏性を持つ．\n次の \\(B_n\\) が可逆な極限 \\(B_n\\to B\\) を持つとき，一致性も持つ： \\[\nB_n:=\\frac{1}{n}\\sum_{i=1}^nX_i^\\top X_i\\in\\mathrm{GL}_{p\\land n}(\\mathbb{R}).\n\\]\n２の条件に加えて，\\(x_i,\\epsilon_i\\) が３次のモーメントを持つとき，漸近正規性も成り立つ：\n\n\n\n\n\n\n\n\n\n反例\n\n\n\n\n\n一致性（と漸近正規性）の成立のために追加の条件が入っていることがわかる．不偏性さえあれば，「極限での不偏性」とも思える一致性が成り立って然るべきな気がする．\nこの追加の条件は，有限個の \\(Y_i\\) の説明にしか参加しない予測子を排除するためにある．\n例えばある分布 \\(P(\\mu,\\sigma^2)\\) に関して \\(Y_i\\overset{\\text{i.i.d.}}{\\sim}P(\\mu_i,\\sigma_i^2)\\) とする． \\[\nY_i=\\beta_1X_i^{(1)}+\\beta_2X_i^{(2)}+\\epsilon_i,\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}P(0,\\sigma_i^2)\n\\] \\[\nX_i^{(1)}=1_{\\left\\{1\\right\\}}(i),\\qquad X_i^{(2)}=1_{\\left\\{2,\\cdots,n\\right\\}}(i)\n\\] とモデルすると，計画行列はフルランクであるが，OLS 推定量は \\(\\widehat{\\beta}_1=\\epsilon_1\\sim P(0,\\sigma_1^2)\\) となる．\nこれは標本サイズ \\(n\\) に依らない値であり，\\(n\\to\\infty\\) を考えても \\(\\widehat{\\beta}_1\\) は一致性はもたず，漸近正規分布もしない．一方で \\(\\widehat{\\beta}_2\\) はする．\n\n\n\n\n\n3.2 EHW 頑健標準誤差\nこの漸近正規性に基づく分散推定量 \\[\n\\widehat{V}:=n^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^nX_i^\\top X_i\\right)^{-1}\\left(\\frac{1}{n}\\sum_{i=1}^n\\epsilon_i^2X_i^\\top X_i\\right)\\left(\\frac{1}{n}\\sum_{i=1}^nX_i^\\top X_i\\right)^{-1}\n\\] は，誤差分布が不均一な場合でも頑健な分散推定量となる．\nこれを計量経済学では (White, 1980) の推定量と呼ぶが，初めに提案したのは (Eicker, 1967) と (Huber, 1967) であるようである．\n\n\n3.3 有効性\nでは OLS は何を失うのか？\n\\(\\sigma^2\\) が不均一になった場合，観測によってノイズの大きさが違うわけである．\nしたがって特に情報量が大きい観測と，ノイズが大きくてあまり意味をなさない観測というものが相対的に出てくる．\nこれを峻別して適切に観測に重み付けることが必要である．\nこれができない OLS は有効性を失う．代わりに重み付けを行った OLS は有効性を持つ．\n\n\n\n\n\n\nBLUE\n\n\n\n既知の正定値行列 \\(\\Sigma\\) に関して， \\[\n\\operatorname{E}[\\epsilon]=0,\\qquad\\mathrm{C}[\\epsilon]=\\sigma^2\\Sigma,\n\\] を満たすとする．このとき，BLUE は次のように表せる： \\[\n\\widehat{\\beta}_\\Sigma=(X^\\top\\Sigma^{-1}X)^{-1}X^\\top\\Sigma^{-1}Y.\n\\]\n\n\n\n\n3.4 WLS\n第 3.1 節で考えた不均一分散の設定は \\[\n\\Sigma=\\mathrm{diag}(\\sigma_1^2,\\cdots,\\sigma_n^2)=:\\mathrm{diag}(w_1^{-1},\\cdots,w_n^{-1})\n\\] の場合に当たる．このときの BLUE は次の最適化条件でも特徴付けられる： \\[\n\\widehat{\\beta}_w:=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}(Y-Xb)^\\top\\Sigma^{-1}(Y-Xb)=\\operatorname*{argmin}_{b\\in\\mathbb{R}^p}\\sum_{i=1}^nw_i\\lvert Y_i-X_ib\\rvert^2_2.\n\\tag{3}\\] これを WLS (Weighted Least Squares) 推定量 という．\n一般には解析を始める前に \\(\\Sigma\\) の形は未知であるから，これの推定から始める．その手続きを計量経済学では FGLS (Feasible Generalized Least Squares) と呼ぶ．\nこの重み付けの考え方は標本抽出の際にも重要であり，(Horvitz and Thompson, 1952) の逆確率重み付け法とも呼ばれる：\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.5 局所線型回帰\n局所線型回帰 (local linear regression) はカーネル法を用いてデータ点を適切に重み付けることで，非線型な回帰を達成する方法である．\n具体的には，基準点 \\(x_0\\) の近傍でのベストな線型近似 \\[\ny(x)=\\alpha+\\beta(x-x_0)\n\\] を得るために，あるカーネル \\(K\\) と帯域幅 \\(h&gt;0\\) を通じて \\[\n(\\widehat{\\alpha},\\widehat{\\beta}):=\\operatorname*{argmin}_{a,b}\\sum_{i=1}^nw_i\\biggl|y_i-a-b(x_i-x_0)\\biggr|^2_2,\\qquad w_i:=K\\left(\\frac{x_i-x_0}{h}\\right),\n\\] によって定める．\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.6 一般化線型モデルの解放\n一般化線型モデル \\[\n\\operatorname{E}[Y_i|X_i]=\\mu(X_i\\beta)\n\\] は基本的に分布が特定されたパラメトリックモデルである．これについても，EHW 頑健標準偏差推定量 3.2 に当たる，分布の誤特定に頑健な標準偏差推定量が存在する．\nその肝となる事実は，あらゆる関数 \\(\\widetilde{\\sigma}^2(x,\\beta)\\) に関して， \\[\n\\operatorname{E}\\left[\\sum_{i=1}^n\\frac{Y_i-\\mu(X_i\\beta)}{\\widetilde{\\sigma}^2(X_i,\\beta)}\\frac{\\partial \\mu(X_i\\beta)}{\\partial \\beta}\\right]=0\n\\tag{4}\\] が真値 \\(\\beta\\) に関して成り立ち続けることである．\nしたがって \\(\\widetilde{\\sigma}^2\\) を何らかの方法で決定し，これに関して式 (4) を通じた \\(M\\)-推定量 \\(\\widehat{\\beta}\\) が構成できる．\\(\\widetilde{\\sigma}^2(x_i,\\beta)=\\mathrm{V}[Y_i|X_i=x_i]\\) と正しく特定できた場合，これは最尤推定量になる．\n多くの場合 \\(\\widetilde{\\sigma}^2\\) は一般化線型モデルの仮定に基づいて算出するが，語特定されているものとしている場合が多く，作業分散 ともいう．\n\\((X_i,Y_i)\\) が独立同分布に従うとするとき，第 3.1 節のような漸近正規性の結果は，一般の \\(M\\)-推定量に関する次の結果から導かれる：\n\n\n\n\n\n\nRestricted Mean Model に対する \\(M\\)-推定8\n\n\n\n\\((X_i,Y_i)\\) が独立同分布に従うとする．このとき， \\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta)\\Rightarrow\\mathrm{N}(0,B^{-1}MB^{-1}),\n\\] \\[\nB:=\\operatorname{E}\\left[\\frac{1}{\\widetilde{\\sigma}^2(x,\\beta)}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta^\\top}\\right],\\qquad\nM:=\\operatorname{E}\\left[\\frac{\\sigma^2(x)}{\\widetilde{\\sigma}^2(x,\\beta)^2}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta}\\frac{\\partial \\mu(X\\beta)}{\\partial \\beta^\\top}\\right].\n\\]\n\n\nこの結果を用いれば，指数型分布族などのパラメトリックモデルに依らずとも，漸近論に基礎付けられた点推定が達成できる．\nなお指数分布族の仮定の下で \\(\\widetilde{\\sigma}^2\\) がが正しく特定されていた場合，\\(B=M\\) は Fisher 情報行列となる．\n\n\n3.7 相関の考慮\nここまでの議論をまとめよう．OLS の漸近正規性 3.1 は，誤差分布が不均一であるばかりでなく，\\(Y_i\\) が相関を持つ場合（\\(\\Sigma\\) の非対角成分が非零の場合）でも成り立つ．\nGLS (Generalized Least Squares) はこの相関を持つ場合でもセミパラメトリック漸近最適性を達成する．\nこの結果を任意の逆リンク \\(\\mu\\) に関して \\[\n\\operatorname{E}[Y_i|X_i]=\\mu(X_i\\beta)\n\\] という非線型な回帰モデルにも拡張することを考えたいが，前節ではまだ \\(Y_i\\) が i.i.d. であるという仮定を置いていた．\n最後にこの仮定を取り払い，一般の誤差分布 \\(\\mathrm{C}[\\epsilon|X]=\\Sigma\\) を考えたい．\nこのために開発されたのが 一般化推定方程式 (GEE: Generalized Estimating Equations) (Liang and Zeger, 1986) である．\n\n\n3.8 一般化推定方程式\n\\((X_i,Y_i)\\) を i.i.d. とした場合の推定方程式 (4) を拡張した推定方程式 \\[\n\\sum_{i=1}^n\\frac{\\partial \\mu(X_i\\beta)}{\\partial \\beta}\\widetilde{\\Sigma}^{-1}(X_i,\\beta)\\biggr(Y_i-\\mu(X_i\\beta)\\biggl)=0\n\\tag{5}\\] を一般化推定方程式といい，\\(\\widetilde{\\Sigma}(X_i,\\beta)\\) を 作業共分散行列 という．\nこの式は今までで最も一般的な形をしており，最適化条件 (3) で推定を実行する GLS に対して，１次の最適性条件に基づいて導出する方法ということができる．それ故，逆リンク \\(\\mu\\) の一般性も許容できている．\n実際，一般化推定方程式 (5) は，最適化条件 (3) を \\(b\\) に関して微分して得る一次の最適性条件に見える．\n一般化推定方程式 (5) による推定も，i.i.d. とは限らない場合の \\(M\\)-推定の理論から，漸近正規性が導ける．この漸近論から得られる EHW 推定量の一般化は，\\(\\mu\\) の特定さえ正しければ，\\(\\widetilde{\\Sigma}\\) の誤特定に頑健な分散推定量となる (Liang and Zeger, 1986), (Altonji and Segal, 1996)．\n\n\n3.9 GEE の仮定\nしかし GEE には重要な仮定 \\[\n\\operatorname{E}[Y_{it}|X_i]=\\operatorname{E}[Y_{it}|X_{it}],\\qquad t\\in[n_i],\n\\] が存在する．これは \\((X_{it},Y_{it})\\) が状態空間モデルに従うことを意味する．\nしかし \\(\\widetilde{\\Sigma}\\) の非対角成分が零になる，独立作業共分散行列を用いた場合は，この仮定が成り立たない場合でも，\\(\\mu\\) の特定が正しければやはり一致性が成り立つが，推定量の分散は少し膨らむ．\nまた関数関係 \\(\\mu\\) が \\(t\\in[n_i]\\) に依存しないという仮定も含まれている．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#終わりに",
    "href": "posts/2024/Stat/Regression.html#終わりに",
    "title": "セミパラメトリック重回帰分析",
    "section": "4 終わりに",
    "text": "4 終わりに\n\n以上の枠組みは全て 一般化モーメント法 (GMM: Generalized Method of Moments) (L. P. Hansen, 1982) の枠組みの中に位置する．\nGMM という名前は，OLS 推定の１次の最適性条件として得る直交条件 \\[\n\\operatorname{E}[X(Y-\\mu(X\\beta))]=0\n\\] が，モーメント法の課す条件と似ており，どれも \\[\n\\operatorname{E}[g(\\beta)]=0\n\\] という形をしているという点から来る．\nさらには経験尤度法 (Owen, 1988), (Qin and Lawless, 1994) も漸近正規性を持つノンパラメトリック手法であり，GMM の後釜として期待されている．特に直交条件の数が多い GMM よりバイアスが少ない．\nしかし GMM の方が分散が大きいことがあり，bias-variance のトレードオフがある (Newey and Smith, 2004)．"
  },
  {
    "objectID": "posts/2024/Stat/Regression.html#footnotes",
    "href": "posts/2024/Stat/Regression.html#footnotes",
    "title": "セミパラメトリック重回帰分析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ding, 2024, p. 81) 定理9.1．The proof of Theorem 9.1 is very simple. However, it is one of the most insightful formulas in statistics.↩︎\n生態学的誤謬 (ecological fallacy) ともいう．↩︎\nこの意味での「内生性」は，「外生的じゃない」こととも意味がズレてしまう．\\(\\operatorname{E}[\\epsilon|X]=0\\) を満たすならば \\(\\mathrm{C}[\\epsilon,X]=0\\) が必要であるから，「内生的ならば外生的でない」は成り立つ．ここでは内生的じゃないことを 広義外生性 と呼ぼう．また多くの場合他の経済学の文脈では，「モデル内で決定される変数」程度の意味で内生変数と呼ぶことも多い．↩︎\n処置変数と相関を持たないということは，非交絡性 \\(Y_i\\perp\\!\\!\\!\\perp Z_i\\mid U_i\\) よりは弱い条件である．なお，この「非交絡性」は疫学の言い方であり，計量経済学では 無視可能性 または \\(U_i\\) が観測可能である場合は selection on observables などとも呼ぶ．逆に言えば，交絡とは selection on unobservables のことである．↩︎\n(Ding, 2024, p. 95) も参照．↩︎\n(Ding, 2024, p. 130) 定理13.1．↩︎\n(Ding, 2024, p. 44) 定理6.1．↩︎\n(Ding, 2024, p. 268) 定理24.2．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html",
    "href": "posts/2024/Stat/Logistic2.html",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n前稿はこちら："
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#重点サブサンプリングを取り入れた-zig-zag-サンプラー",
    "href": "posts/2024/Stat/Logistic2.html#重点サブサンプリングを取り入れた-zig-zag-サンプラー",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "1 重点サブサンプリングを取り入れた Zig-Zag サンプラー",
    "text": "1 重点サブサンプリングを取り入れた Zig-Zag サンプラー\n\n1.1 はじめに\n説明変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p)\\) と回帰係数 \\(\\xi\\in\\mathcal{L}(\\Omega;\\mathbb{R}^p)\\) に関するロジスティックモデル\n\\[\n\\operatorname{P}[Y=1\\,|\\,X,\\xi]=F(X^\\top\\xi)=\\frac{\\exp(X^\\top\\xi)}{1+\\exp(X^\\top\\xi)}\\tag{1}\n\\]\nにおいて，\\(\\{y^i\\}\\) または \\(\\{x^i_j\\}\\) が不均衡であった場合は，前節で見たように Gibbs サンプラーによる方法ではスケールしない．\nよく調整されたランダムウォーク Metropolis-Hastings 法を用いることが解決の１つとしてあり得るが，やはり本当に大きな \\(n\\) や \\(p\\) に関してスケールしないことが問題である (Chopin and Ridgway, 2017)．\nそこで Zig-Zag サンプラーを用いることを考える．Zig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n1.2 事後分布の特徴\n実はロジスティック回帰ではポテンシャル \\(U\\) の勾配が有界になり，簡単なサブサンプリングが可能である．\n事後分布の負の対数密度は，定数の違いを除いて次のように表せる： \\[\\begin{align*}\n    U(\\xi)&:=-\\log p_0(\\xi)-\\sum_{i=1}^n\\log\\left(\\frac{\\exp(y^i(x^i)^\\top\\xi)}{1+\\exp((x^i)^\\top\\xi)}\\right)\\\\\n    &=:U_0(\\xi)+U_1(\\xi).\n\\end{align*}\\]\n\\(U_1\\) は次のように１つのサンプルのみに依存する関数 \\(U^j_1\\) の平均として表せる： \\[\nU_1(\\xi)=\\frac{1}{n}\\sum_{j=1}^nU^j_1(\\xi)\n\\] \\[\nU_1^j(\\xi)=-n\\log\\left(\\frac{\\exp\\left(y^j(x^j)^\\top\\xi\\right)}{1+\\exp\\left((x^j)^\\top\\xi\\right)}\\right)\n\\]\n各 \\(U^j_1\\) の勾配は次のように計算でき，有界である： \\[\n\\partial_iU^j_1(\\xi)=n\\frac{x^j_i\\exp\\left((x^j)^\\top\\xi\\right)}{1+\\exp\\left((x^j)^\\top\\xi\\right)}-ny^jx^j_i&lt;nx^j_i(1-y^j).\n\\]\n\n\n1.3 定数強度 Poisson 点過程からの剪定\n特に全 \\(\\partial_iU_1(\\xi)\\) に共通する上界として \\[\n\\lvert\\partial_iU^j_1(\\xi)\\rvert\\le n\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\qquad i\\in[d]\n\\tag{1}\\] を得るから，強度 \\[\n\\lambda_i(\\xi,\\theta)=\\biggr(\\theta_i\\partial_iU(\\xi)\\biggl)_+\\le\\biggr(n\\theta_i\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\biggl)_+\n\\] をもった Poisson 点過程を元に剪定 (thinning) (Lewis and Shedler, 1979) を実行できる．\n\n\nサブサンプリングなしの Zig-Zag 過程のシミュレーションをする関数 ZZ1d() を定義\nusing ZigZagBoomerang\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing Statistics  # just for sure\nusing StatsFuns\n\n\"\"\"\n    ∇U(i,j,ξ,x,y)\n        i ∈ [d]: 次元を表すインデックス\n        j ∈ [n]: サンプル番号を表すインデックス\n        ξ: パラメータ空間 R^d 上の位置\n        他，観測 (x,y) を引数にとる．\n    この関数を実装する際，log の中身をそのまま計算しようとすると大変大きくなり，数値的に不安定になる（除算の後は 1 近くになるはずだが，Inf になってしまう）\n\"\"\"\n∇U(i::Int64, j::Int64, ξ, x::Matrix{Float64}, y::Vector{Float64}) = length(y) * x[i,j] * (logistic(dot(x[:,j],ξ)) - y[j])\n\n\"\"\"\n    ∇U(i,ξ,x,y)：∇U(i,j,ξ,x,y) を全データ j ∈ [n] について足し合わせたもの\n        i ∈ [d]: 次元を表すインデックス\n        ξ: パラメータ空間 R^d 上の位置\n        他，観測 (x,y) を引数にとる．\n\"\"\"\nfunction ∇U(i::Int64, ξ, x::Matrix{Float64}, y::Vector{Float64})\n    n = length(y)\n    U_list = []\n    for j in 1:n\n        push!(U_list, ∇U(i, j, ξ, x, y))\n    end\n    return mean(U_list)\nend\n\nfunction  ∇U(ξ, x::Matrix{Float64}, y::Vector{Float64})  # 1次元の場合のショートカット\n    return ∇U(1, ξ, x, y)\nend\n\npos(x) = max(zero(x), x)\n\n\"\"\"\n    λ(i, ξ, θ, ∇U, x, y)：第 i ∈ [d] 次元のレート関数\n        i ∈ [d]: 次元を表すインデックス\n        (ξ,θ): E 上の座標\n        ∇U\n        (x,y): 観測\n\"\"\"\nλ(i::Int64, ξ, θ, ∇U, x, y) = pos(θ[i] * ∇U(i, ξ, x, y))\nλ(ξ, θ, ∇U, x, y) = pos(θ * ∇U(ξ, x, y))  # 1次元の場合のショートカット\n\n\"\"\"\n    λ(τ, a, b)：代理レート関数の時刻 τ における値\n        τ: 時間\n        a,b: 1次関数の係数\n\"\"\"\nλ_bar(τ, a, b) = pos(a + b*τ)\n\n\"\"\"\n`x`: current location, `θ`: current velocity, `t`: current time,\n\"\"\"\nfunction move_forward(τ, t, ξ, θ, ::ZigZag1d)\n    τ + t, ξ + θ*τ , θ\nend\n\n\"\"\"\n    ZZ1d(∇U, ξ, θ, T, x, y, Flow; rng=Random.GLOBAL_RNG, ab=ab_Global)：ZigZag sampler without subsampling\n        `∇U`: gradient of the negative log-density\n        `(ξ,θ)`: initial state\n        `T`: Time Horizon\n        `(x,y)`: observation\n        `Flow`: continuous dynamics\n\n        `a+bt`: computational bound for intensity m(t)\n\n        `num`: ポアソン時刻に到着した回数\n        `acc`: 受容回数．`acc/num` は acceptance rate\n\"\"\"\nfunction ZZ1d(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_Global)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        l, lb = λ(ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n                println(l)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\na_Global(ξ, θ, x, y) = length(y) * maximum(abs.(vec(x)))\nb_Global(ξ, θ, x, y) = 0\n\nab_Global(ξ, θ, x, y, ::ZigZag1d) = (a_Global(ξ, θ, x, y), b_Global(ξ, θ, x, y))\n\n\n\n1.4 Affine 強度 Poisson 点過程からの剪定\nさらにタイトに，次の affine 関数による評価を考える： \\[\n\\overline{m}_i(t):=a_i+b_it,\\qquad i\\in[d],\n\\] \\[\na_i:=(\\theta_i\\partial_iU(\\xi_*))_++C_i\\lvert\\xi-\\xi_*\\rvert\n\\] \\[\nb_i:=C_i\\sqrt{d},\\qquad C_i:=\\frac{n}{4}\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\lvert x^j\\rvert.\n\\]\n\n\n観測を生成\nusing StatsFuns\nusing Distributions\n\nξ0 = [1.0] # True value\nn_list = [10, 100, 1000]  # 実験で用いるサンプルサイズの列\n\nΣ = [2]\nx = rand(MvNormal(ξ0, Σ), n_list[end])\ny = rand.(Bernoulli.(logistic.(ξ0*x)))  # BitVector になってしまう\ny = Float64.(vec(y))  # Vector{Float64} に変換\n\n\n\nusing Statistics\nusing LinearAlgebra\n\n\"\"\"\n    U(ξ, x, y)：ポテンシャル関数\n        ξ: パラメータ空間上の点\n        (x,y): 観測\n\"\"\"\nfunction U(ξ, x, y)\n    n = length(y)\n    U_list = []\n    for j in 1:n\n        push!(U_list, U(j, ξ, x, y))\n    end\n    return mean(U_list)\nend\nfunction U(j, ξ, x, y)\n    n = length(y)\n    product = dot(x[:,j],ξ)\n    return -n * log(exp(y[j] * product) / (1 + exp(product)))\nend\n\nusing Optim\n\nresult = optimize(ξ -&gt; U(ξ, x, y), [0.0], LBFGS())\nξ_star = Optim.minimizer(result)\n\nfunction C(ξ, θ, x, y)\n    n = length(y)\n    max_value = maximum(x.^2)\n    return n * max_value / 4\nend\n\na_Affine(ξ, θ, x, y) = pos(θ * ∇U(ξ_star,x,y)) + C(ξ, θ, x, y) * abs(ξ - ξ_star[1])\nb_Affine(ξ, θ, x, y) = C(ξ, θ, x, y)\n\n# computational bounds for intensity m(t)\nab_Affine(ξ, θ, x, y, ::ZigZag1d) = (a_Affine(ξ, θ, x, y), b_Affine(ξ, θ, x, y))\n\n\n\n1.5 サブサンプリング：確率的な点過程からの剪定\nPoisson 過程の強度関数を確率化し，\\(K\\sim\\mathrm{U}([n])\\) に対して \\[\nm_i^K(t):=\\biggr(\\theta_i E^K_i(x+\\theta t)\\biggl)_+\n\\] \\[\nE^K_i(x):=\\partial_iU(\\xi_*)+\\partial_iU^K(\\xi)-\\partial_iU^K(\\xi_*)\n\\] としても，引き続き同様の上界を持つ．\nここで，\\(m_i^K\\) の評価は \\(m_i\\) より \\(n\\) 倍軽量になっていることに注意．\n\n\nサブサンプリングありの Zig-Zag 過程のシミュレーションをする関数 ZZ1d_SS() と ZZ1d_CV() を定義\nusing ZigZagBoomerang\nusing Distributions\nusing Random\nusing LinearAlgebra\nusing Statistics  # just for sure\nusing StatsFuns\n\nfunction λj_Global(j::Int64, ξ, θ, ∇U, x, y)\n    Eʲ = ∇U(1, j, ξ, x, y)\n    return pos(θ * Eʲ)\nend\n\nfunction ZZ1d_SS(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_Global)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        j = rand(1:length(y))\n        l, lb = λj_Global(j, ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\nfunction λj(j::Int64, ξ, θ, ∇U, x, y)\n    Eʲ = ∇U(ξ_star, x, y) + ∇U(1, j, ξ, x, y) - ∇U(1, j, ξ_star, x, y)\n    return pos(θ * Eʲ)\nend\n\nfunction ZZ1d_CV(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_Affine)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        j = rand(1:length(y))\n        l, lb = λj(j, ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n1.6 数値実験による性能比較\n\n\n４つの Zig-Zag サンプラーの実装\n\n\n\n\n\n\n\n剪定元の Poisson 過程の強度 \\ サブサンプリング\nなし\nあり\n\n\n\n\n定数\nZZ (Global) 1.3\nZZ-SS 1.5\n\n\nAffine\nZZ (Affine) 1.4\nZZ-CV 1.5\n\n\n\n\n\n\nサブサンプリングなしの実験を実行する関数 experiment_ZZ() を定義\nusing Statistics\n\nfunction ESS(samples::Vector{Float64}, T, dt)\n    B = T / dt\n    V = (dt / T) * sum(samples.^2) - ((dt / T) * sum(samples))^2\n    Y = samples .* sqrt(T / B)\n    ESS = T * V / var(Y)\n    return ESS\nend\n\nfunction getESSperEpoch(ab, T ,dt, x, y; ξ0=0.0, θ0=1.0)\n    trace, epochs, acc = ZZ1d(∇U, ξ0, θ0, T, x, y, ZigZag1d(); ab=ab)\n    traj = discretize(trace, ZigZag1d(), dt)\n    return ESS(traj.x, T, dt) / epochs[end]\nend\n\nN = 10\nT = 500.0\ndt = 0.1\n\nfunction experiment_ZZ(N, T, dt; ξ0=0.0, θ0=1.0, n_list=[10, 100, 1000])  # サブサンプリングなしの ZZ() に関して N 回実験\n    ESSs_sum_Affine = zero(n_list)\n    ESSs_sum_Global = zero(n_list)\n\n    for _ in 1:N\n        ESSs_Affine = []\n        ESSs_Global = []\n        for n in n_list\n            push!(ESSs_Affine, getESSperEpoch(ab_Affine, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_Global, getESSperEpoch(ab_Global, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n        end\n        ESSs_sum_Affine = [ESSs_sum_Affine ESSs_Affine]\n        ESSs_sum_Global = [ESSs_sum_Global ESSs_Global]\n    end\n    return mean(ESSs_sum_Affine, dims=2), var(ESSs_sum_Affine, dims=2), mean(ESSs_sum_Global, dims=2), var(ESSs_sum_Global, dims=2)\nend\n\n# ESS_Affine, var_ESS_Affine, ESS_Global, var_ESS_Global = experiment_ZZ(2, T, dt; ξ0=0.0, θ0=1.0, n_list=n_list)\n\n\n\n\n実験には 10分 かかるので，保持した実行結果を用いる\nusing JLD2\n\n@load \"Logistic2_Experiment1.jld2\" ESS_Affine var_ESS_Affine ESS_Global var_ESS_Global\n\n\n\n\nサブサンプリング付きの実験を実行する関数 experiment_ZZ() を定義\nusing Statistics\n\n# function ESS(samples::Vector{Float64}, T, dt)\n#     B = T / dt\n#     V = (dt / T) * sum(samples.^2) - ((dt / T) * sum(samples))^2\n#     Y = samples .* sqrt(T / B)\n#     ESS = T * V / var(Y)\n#     return ESS\n# end\n\nfunction getESSperEpoch_SS(ab, ZZ, T ,dt, x, y; ξ0=0.0, θ0=1.0)\n    trace, epochs, acc = ZZ(∇U, ξ0, θ0, T, x, y, ZigZag1d(); ab=ab)\n    traj = discretize(trace, ZigZag1d(), dt)\n    return ESS(traj.x, T, dt) * length(y) / epochs[end]  # サブサンプリングをしているので length(y) で補正する必要あり\nend\n\nN = 10\nT = 500.0\ndt = 0.1\n\nfunction experiment_ZZ_SS(N, T, dt; ξ0=0.0, θ0=1.0, n_list=[10, 100, 1000])  # サブサンプリングなしの ZZ() に関して N 回実験\n    ESSs_sum_CV = zero(n_list)\n    ESSs_sum_SS = zero(n_list)\n\n    for _ in 1:N\n        ESSs_CV = []\n        ESSs_SS = []\n        for n in n_list\n            push!(ESSs_CV, getESSperEpoch_SS(ab_Affine, ZZ1d_CV, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_SS, getESSperEpoch_SS(ab_Global, ZZ1d_SS, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n        end\n        ESSs_sum_CV = [ESSs_sum_CV ESSs_CV]\n        ESSs_sum_SS = [ESSs_sum_SS ESSs_SS]\n    end\n    return mean(ESSs_sum_CV, dims=2), var(ESSs_sum_CV, dims=2), mean(ESSs_sum_SS, dims=2), var(ESSs_sum_SS, dims=2)\nend\n\n# ESS_CV, var_ESS_CV, ESS_SS, var_ESS_SS = experiment_ZZ_SS(2, T, dt; ξ0=0.0, θ0=1.0, n_list=n_list)\n\n\n\n\n結果をプロット\nq = addPlot(q, n_list, ESS_CV, sqrt.(var_ESS_CV); label=\"ZZ-CV\", color=\"darkorange\")\nq = addPlot(q, n_list, ESS_SS, sqrt.(var_ESS_SS); label=\"ZZ-SS\", color=\"blue\")\nq = plot!(q, legend=:bottomleft)\ndisplay(q)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n実際に実験に用いたコードはこちら．\n\n\n1.7 比較対象：MALA\n\n\nMALA によるサンプリングを実行\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing LinearAlgebra\n\nstruct LogTargetDensity\n    x::Matrix{Float64}\n    y::Vector{Float64}\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensity, ξ) = -U(ξ, p.x, p.y)\nLogDensityProblems.dimension(p::LogTargetDensity) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensity}) = LogDensityProblems.LogDensityOrder{0}()\n\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensity(x, y))\n\nσ² = 0.0001\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\nchain = sample(model_with_ad, spl, 2000; initial_params=ξ0, chain_type=StructArray, param_names=[\"ξ\"], stats=true)\n\ntraj_MALA = chain.ξ\n\n\n\n\n1.8 有効サンプル数について\n時区間 \\([0,T]\\) における ZigZag 過程の，関数 \\(h\\in\\mathcal{L}^2(\\mathbb{R}^d)\\) に関する 有効サンプル数 (ESS) とは \\[\n\\widehat{\\operatorname{ESS}}:=T\\frac{\\widehat{\\mathrm{V}_\\pi[h]}}{\\widehat{\\sigma^2_h}}\n\\] \\[\n\\widehat{\\mathrm{V}_\\pi[h]}:=\\frac{1}{T}\\int^T_0h(X_s)^2\\,ds-\\left(\\frac{1}{T}\\int^T_0h(X_s)\\,ds\\right)^2,\n\\] \\[\n\\widehat{\\sigma^2_h}:=\\frac{1}{B-1}\\sum_{i=1}^B(Y_i-\\overline{Y})^2,\\quad Y_i:=\\sqrt{\\frac{B}{T}}\\int^{\\frac{iT}{B}}_{\\frac{(i-1)T}{B}}h(X_s)\\,ds\n\\] で定まる値である．\n例えば次のようにして計算できる：\nfunction ESS(samples::Vector{Float64}, T, dt)\n    V = (dt / T) * sum(samples.^2) - ((dt / T) * sum(samples))^2\n    Y = samples .* sqrt(T / B)\n    ESS = T * V / var(Y)\n    return ESS\nend\n\n\n1.9 重点サブサンプリング (Sen et al., 2020)\n一様でないサブサンプリングを導入することで，Zig-Zag サンプラーを不均衡データにも強くすると同時に，サブサンプリングの効率を上げることもできる．\nサブサンプリングのために定義したランダムな強度関数（第 1.5 節） \\[\nm_i^K(t)=\\biggr(\\theta_iE^K_i(x+\\theta t)\\biggl)_+\n\\] は， \\[\n\\operatorname{E}\\biggl[E^K_i(\\xi)\\biggr]=\\partial_iU(\\xi)\n\\] を満たす限り，\\(K\\sim\\mathrm{U}([n])\\) に限る必要はなかったのである．\nすなわち，\\((p_x)\\) をある \\([n]\\) 上の分布 \\(\\nu\\in\\mathcal{P}([n])\\) の質量関数として \\[\n\\partial_iV_1^J(\\xi):=\\frac{1}{p_J}\\partial_iU^J(\\xi)\\qquad J\\sim\\nu\n\\] と定めると， \\[\n\\lvert\\partial_iV_i^j(\\xi)\\rvert\\le\\max_{j\\in[n]}\\frac{\\lvert x_i^j\\rvert}{p_j}\n\\] が成り立つ．式 (1) は \\(p_j\\equiv1/n\\) の場合であったのである．\n換言すれば， \\[\np_j\\,\\propto\\,\\lvert x^j_i\\rvert\n\\] と定めることで，Poisson 強度関数 \\(m^j_i\\) の上界をタイトにすることができ，その結果剪定の効率が上がる．\n\na_IS(ξ, θ, x, y) = sum(abs.(x))\nb_IS(ξ, θ, x, y) = 0\n\nab_IS(ξ, θ, x, y, ::ZigZag1d) = (a_IS(ξ, θ, x, y), b_IS(ξ, θ, x, y))\n\n\n\n重点サブサンプリングによる ZigZag サンプラー ZZ1d_IS() を定義\nusing StatsBase\n\nfunction λj_IS(j::Int64, ξ, θ, ∇U, x, y)\n    pj = abs(x[1,j]) / sum(abs.(x))  # x がスパースだと 0 になりやすいことに注意\n    Eʲ = ∇U(1, j, ξ, x, y) / (length(y) * pj)\n    return pos(θ * Eʲ)\nend\n\nfunction ZZ1d_IS(∇U, ξ, θ, T::Float64, x::Matrix{Float64}, y::Vector{Float64}, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_IS)\n    t = zero(T)\n    Ξ = [(t, ξ, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(ξ, θ, x, y, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n    n = length(y)\n\n    while t &lt; T\n        τ = t′ - t\n        t, ξ, θ = move_forward(τ, t, ξ, θ, Flow)\n        j = sample(1:n, Weights(abs.(vec(x))))\n        l, lb = λj_IS(j, ξ, θ, ∇U, x, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, ξ, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(ξ, θ, x, y, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n実験を実行する関数 experiment_ZZ_IS() を定義\nfunction experiment_ZZ_IS(N, T, dt; ξ0=0.0, θ0=1.0, n_list=[10, 100, 1000])  # 重点サブサンプリング ZZ1d_IS() に関して N 回実験\n    ESSs_sum_IS = zero(n_list)\n\n    for _ in 1:N\n        ESSs_IS = []\n        for n in n_list\n            push!(ESSs_IS, getESSperEpoch_SS(ab_IS, ZZ1d_IS, T, dt, x[:,1:n], y[1:n]; ξ0=ξ0, θ0=θ0))\n        end\n        ESSs_sum_IS = [ESSs_sum_IS ESSs_IS]\n    end\n    return mean(ESSs_sum_IS, dims=2), var(ESSs_sum_IS, dims=2)\nend\n\nESS_IS, var_ESS_IS = experiment_ZZ_IS(2, 500.0, 1.0; ξ0=0.0, θ0=1.0, n_list=n_list)\n\n\n\n\n結果をプロット\nr = addPlot(q, n_list, ESS_IS, sqrt.(var_ESS_IS); label=\"ZZ-IS\", color=\"green\")\ndisplay(r)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n制御変数による方法 ZZ-CV （オレンジ色）は \\(n\\to\\infty\\) の漸近論に基づいているので，観測数が増えるほど効率は上がっていく．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#zig-zag-サンプラーの大規模不均衡データへの適用",
    "href": "posts/2024/Stat/Logistic2.html#zig-zag-サンプラーの大規模不均衡データへの適用",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "2 Zig-Zag サンプラーの大規模不均衡データへの適用",
    "text": "2 Zig-Zag サンプラーの大規模不均衡データへの適用\n\n2.1 大規模不均衡データ\n大規模不均衡データでは，事後分布が十分な集中性を持たないために制御変数による方法 ZZ-CV が十分な効率改善を示さないが，重点サブサンプリングによれば Poisson 強度関数のタイトな上界を引き続き構成できる．\nここでは，\\(\\xi_0=1\\) を真値とし，次のような１次元データを考える： \\[\nX^j\\overset{\\text{iid}}{\\sim}(1-\\alpha)\\delta_0+\\alpha\\mathrm{N}(1,2)\n\\] \\[\n\\operatorname{P}[Y^j=1]=\\frac{1}{1+e^{-X^j}}\n\\]\n\nξ0 = [1.0] # True value\nΣ = [2]\nn = 1000\n\nfunction sample_SparseData(n::Int64, α::Float64; ρ=MvNormal(ξ0, Σ))\n    x = []\n    while length(x) &lt; n\n        rand() &lt; α ? push!(x, rand(ρ)[1]) : push!(x, 0.0)\n    end\n    x = Float64.(reshape(x,1,:))\n    y = rand.(Bernoulli.(logistic.(ξ0*x)))\n    y = Float64.(vec(y))\n    return x, y\nend\n\nα_list = [1, 0.1, 0.01]\n\nx_Sparse, y_Sparse = [], []\n\nfor α in α_list\n    x, y = sample_SparseData(n, α)\n    push!(x_Sparse, x)\n    push!(y_Sparse, y)\nend\n\n\nusing Optim\n\na_Sparse(ξ_star, ξ, θ, x, y) = pos(θ * ∇U(ξ_star,x,y)) + C(ξ, θ, x, y) * abs(ξ - ξ_star[1])\nξ_star_list = []\nα_list = [1, 0.1, 0.01]\n\nfor α in 1:length(α_list)\n    result = optimize(ξ -&gt; U(ξ, x_Sparse[α], y_Sparse[α]), [0.0], LBFGS())\n    ξ = Optim.minimizer(result)\n    push!(ξ_star_list, ξ)\nend\n\nfunction experiment_Sparse(N, T, dt; ξ0=0.0, θ0=1.0, α_list=[1, 0.1, 0.01, 0.001])\n    # ESSs_sum_CV = zero(α_list)\n    ESSs_sum_SS = zero(α_list)\n    ESSs_sum_IS = zero(α_list)\n\n    for _ in 1:N\n        # ESSs_CV = []\n        ESSs_SS = []\n        ESSs_IS = []\n        for i in 1:length(α_list)\n            # ab_Sparse(ξ, θ, x, y, ::ZigZag1d) = (a_Sparse(ξ_star_list[i], ξ, θ, x, y), b_Affine(ξ, θ, x, y))\n            # push!(ESSs_CV, getESSperEpoch_SS(ab_Sparse, ZZ1d_CV, T, dt, x_Sparse[i], y_Sparse[i]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_SS, getESSperEpoch_SS(ab_Global, ZZ1d_SS, T, dt, x_Sparse[i], y_Sparse[i]; ξ0=ξ0, θ0=θ0))\n            push!(ESSs_IS, getESSperEpoch_SS(ab_IS, ZZ1d_IS, T, dt, x_Sparse[i], y_Sparse[i]; ξ0=ξ0, θ0=θ0))\n        end\n        # ESSs_sum_CV = [ESSs_sum_CV ESSs_CV]\n        ESSs_sum_SS = [ESSs_sum_SS ESSs_SS]\n        ESSs_sum_IS = [ESSs_sum_IS ESSs_IS]\n    end\n    # return mean(ESSs_sum_CV, dims=2), var(ESSs_sum_CV, dims=2), mean(ESSs_sum_SS, dims=2), var(ESSs_sum_SS, dims=2), mean(ESSs_sum_IS, dims=2), var(ESSs_sum_IS, dims=2)\n    return mean(ESSs_sum_SS, dims=2), var(ESSs_sum_SS, dims=2), mean(ESSs_sum_IS, dims=2), var(ESSs_sum_IS, dims=2)\nend\n\nN = 2\nT = 500.0\ndt = 0.1\n\nESS_SS, var_ESS_SS, ESS_IS, var_ESS_IS = experiment_Sparse(N, T, dt; ξ0=0.0, θ0=1.0, α_list=α_list)\n\n\n\nプロット\nusing LaTeXStrings\np = startPlot(α_list, ESS_SS, sqrt.(var_ESS_SS); label=\"ZZ-SS\", xlabel=L\"Sparsity $\\alpha$\", color=\"blue\"#, background_color=true\n)\np = addPlot(p, α_list, ESS_IS, sqrt.(var_ESS_IS); label=\"ZZ-IS\", color=\"green\")\np = addPlot(p, α_list, ESS_CV, sqrt.(var_ESS_CV); label=\"ZZ-CV\", color=\"darkorange\")\ndisplay(p)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nここでは問題にしないが，圧倒的に実行時間が重点サブサンプリングの方が短い．\n\n\n\n\n\n\n注\n\n\n\n\n\n\\(\\alpha&lt;10^{-3}\\) の領域では動作が不安定になる．論文 (Sen et al., 2020) でもこの領域は触れられていない．しかし， \\[\n\\#\\left\\{i\\in[n]\\mid y^i=1\\right\\}\\approx500\n\\] であるため，特に理由は見つからない．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic2.html#高次元へのスケーリング",
    "href": "posts/2024/Stat/Logistic2.html#高次元へのスケーリング",
    "title": "大規模な不均衡データに対するロジスティック回帰（後編）",
    "section": "3 高次元へのスケーリング",
    "text": "3 高次元へのスケーリング\nさらに Sticky Zig-Zag サンプラー (Bierkens et al., 2023) により，スパースデータに対する効率化が可能である．\n\nusing StatsFuns\nusing Distributions\n\n\"\"\"\n    U(ξ, x, y)：ポテンシャル関数\n        ξ: パラメータ空間上の点\n        (x,y): 観測\n\"\"\"\nfunction U(ξ, x, y)\n    n = length(y)\n    U_list = []\n    for j in 1:n\n        push!(U_list, U(j, ξ, x, y))\n    end\n    return mean(U_list)\nend\nfunction U(j, ξ, x, y)\n    n = length(y)\n    product = dot(x[:,j],ξ)\n    return -n * exp(y[j] * product) / (1 + exp(product))\nend\n\nξ0 = [1,2]  # True value\nn_list = [10, 100, 1000, 10000]  # 実験で用いるサンプルサイズの列\n\nΣ = [2 0; 0 2]\nx = rand(MvNormal(ξ0, Σ), n_list[end])\ny = rand.(Bernoulli.(logistic.(ξ0'*x)))  # BitVector になってしまう\ny = Float64.(vec(y))  #  Vector{Float64} に変換\n\n\nusing Optim\n\nresult = optimize(ξ -&gt; U(ξ,x,y), [0.0,0.0], LBFGS())\nξ_star = Optim.minimizer(result)"
  },
  {
    "objectID": "posts/2024/Stat/DiffusionMap.html",
    "href": "posts/2024/Stat/DiffusionMap.html",
    "title": "拡散埋め込み | Diffusion Map",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Stat/DiffusionMap.html#文献紹介",
    "href": "posts/2024/Stat/DiffusionMap.html#文献紹介",
    "title": "拡散埋め込み | Diffusion Map",
    "section": "1 文献紹介",
    "text": "1 文献紹介\n\n\n生物学データの次元削減・可視化手法PHATEを使ってみる は大変丁寧に書かれた記事である．"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html",
    "href": "posts/2024/Computation/PGM1.html",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html#歴史と導入",
    "href": "posts/2024/Computation/PGM1.html#歴史と導入",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "1 歴史と導入",
    "text": "1 歴史と導入\n\n1.1 例\n\n\n\n\n\n\nベイジアンネットワークの例1\n\n\n\n\n階層モデル や構造方程式モデル，状態空間モデル\nニューラルネットワーク (Rumelhart et al., 1987)\n決定木 や influence diagram (R. A. Howard and Matheson, 1981)\nシステム工学におけるブロック線図\n構造的因果モデル (Pearl et al., 2016)2\n\n\n\nまた，ベイジアンネットワークは，Markov 圏 上の図式のうち，特定のグラフ理論的な条件 2.2 を満たすものと見れる．\n\n\n\n\n\n\nマルコフネットワークの例3\n\n\n\n\n（２次元以下の）Ising 模型 や Potts 模型 はマルコフネットワークの例である．\n画像データはマルコフネットワークとみなされ，デノイジングなどに用いられる．\n\n\n\nファクターグラフは，マルコフネットワークと，その上に局所的に定義されたポテンシャルに関する情報との組である．\n以上，グラフを利用したモデリング法は全て，確率的グラフィカルモデルとも呼ばれる．\n\n\n1.2 はじめに\nモデルに変数が多く含まれるほど，モデリングの作業は難しくなっていく．\nその中でも，最も基本的な事前知識は「どの変数の間に関係があり，どの変数は互いに独立か」というタイプのものであり，変数間のグラフを描くことで特にわかりやすくなる．\nグラフィカルモデル とは，このような変数間の依存性・独立性を表現したグラフに，特定の分布族を対応させる数学的枠組みである．4\n\n\n1.3 諸科学での知識表現の歴史\n多くの科学分野において，「知識表現」の「知識」とは，特に因果関係に関する知識のことを指すようである．これを捉えるために，グラフを用いることは自然な発想であり，計算機の登場以前にも，純粋に人間が理解を深めるための用途に，歴史上極めて早い時期から用いられていた．\n\n\n\n\n\n\nグラフ表現の歴史5\n\n\n\n\n\n高次元分布において，成分間の独立性をグラフを用いて表現しようという発想は，その計算機との親和性が見つかる前に，種々の科学分野で試みられていた．\n\n(Gibbs, 1902) が統計力学の文脈で，相関粒子系の分布をグラフで表現した．\n(Wright, 1918) は骨格測定のデータを用いた因子分析で，（遺伝的な意味での）依存関係を，パス図と呼ばれる有向グラフを用いて表した．\n(Herman Wold, 1954) とその教え子との (K. G. Jöreskog and Wold, 1982)，さらに (Blalock Jr., 1971) が社会学において，因果をグラフを用いて表す因子分析法を 構造方程式モデル (SEM: Structural Equation Model) の名前の下に普及させた．6\n経済学では特に 操作変数法 として独自の発展を遂げている．\nその後 (H. Wold and Strotz, 1960) は (Pearl, 2009) などの do-calculus に繋がっている．これはパス解析や構造方程式モデルのノンパラメトリックな拡張とも見れる．7\n統計学でも (Bartlett, 1935) が分割表分析において変数同士の相関の研究をしたが，界隈が本格的に受け入れたのはやっと 1960 年代以降である．\n\n\n\n\n\n\n1.4 人工知能分野での確率的モデリングの採用\n人工知能分野が確率的手法を採用したのは，エキスパートシステムの構築が志向された 1960 年代であった．8\n医療診断や油源探索における専門家に匹敵する判断力を持つアルゴリズムを構築する途上で，不確実性の度合いの定量化が必要となり，naive Bayes model （第 2.1 節）と呼ばれる確率的モデルが採用された．特に (de Dombal et al., 1972) は限られた分野であるが人間を凌駕する診断正答率を示した．\nだがこの確率的アプローチは，主にその計算複雑性から 1970 年代では冬の時代を経験することとなり，エキスパートシステムも production rule framework や ファジー論理 (Zadeh, 1989) など，確率論に代わって他のアーキテクチャが試みられるようになっていった．\n\n\n\n\n\n\nBayesian network とエキスパートシステム\n\n\n\n\n\nグラフィカルモデリングは初め，自立して推論・意思決定を行うエキスパート・システムの構築のために人工知能分野で用いられ始めた．9\nベイズ流のアプローチでは，事前分布を定める上で，系に対する客観的な事前知識と主観的な事前知識とを分けることが重要であるが，グラフィカルモデルや階層モデリングはこの分離を自然に行うことができる (C. P. Robert, 2007, pp. 457–458)．\n世界に対する知識には不確実性がつきものであり，実世界応用ではこの「不確実性」を反映したモデリング手法が有効であることが多い．そこで，近年の機械学習へのベイズ流アプローチでも，グラフィカルモデルは盛んに応用されている（第 1.4 節）．10\n\nI have approximate answers and possible beliefs with different degrees of uncertainty about different things, but I am not absolutely sure of anything. – Richard Feynman\n\n加えてグラフという表現方法は，人間にとって視覚的にわかりやすいだけでなく，周辺化，極値計算，条件付き確率の計算を高速化するという点で計算機的にも利点がある．11\n\n\n\n\n\n1.5 Bayesian Network と確率的モデリングの登場\nこれを打開したのが\n\n\n\n\n\n\n\n(Pearl, 1988) による Bayesian network framework と，(Lauritzen and Spiegelhalter, 1988) による効率的な推論手法という理論的発展．\n(Heckerman et al., 1992), (Heckerman and Nathwani, 1992) が Bayesian network を病理学標本に応用して大きな成功を挙げたこと．\n\n\n\n\nの２つである．\nこれにより，確率的グラフィカルモデル，また一般に確率的アプローチが広く受け入れられるようになった．\n\n\n1.6 Markov Network の登場と MCMC の普及\nMCMC が真に統計界隈に輸入されるきっかけとなった (Gelfand and Smith, 1990) は Gibbs サンプラーに関するものであった．(Geman and Geman, 1984) による Gibbs サンプラーの提案も，画像分析，広く空間統計学においてなされたものであった．\n統計物理学における Ising モデルが，空間統計学において Markov random field として広く受け入れられ (Besag, 1974)，これにより統計界隈に階層モデルと Gibbs サンプラーが広く受け入れられるようになったのである．\n特に，Markov random field が，Gibbs 分布の局所的な条件付き分布からの特徴付けを与える（Hammersley-Clifford の定理3.5）という認識を導き，このことが MCMC を一般の Bayes 計算手法たらしめたと強調している (Besag and Green, 1993)．12\n\nit is no coincidence that the original concept and the early development of MCMC in Bayesian inference should take place exclusivelyin the spatial statistics literature. (Besag and Green, 1993, p. 26)"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html#sec-BN",
    "href": "posts/2024/Computation/PGM1.html#sec-BN",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "2 Bayesian Network",
    "text": "2 Bayesian Network\n\n2.1 例：ナイーブ Bayes モデル\nnaive Bayes model は Idiot Bayes model とも呼ばれる Bayesian Network の簡単な例である．\nこれは クラス と呼ばれる離散潜在変数 \\(C\\in\\{c^1,\\cdots,c^k\\}\\) を持つ次のようなモデルである．\n\n\n\nnaive Bayes model\n\n\nこの際，グラフィカルモデルに共通する用語を確認する．\n\nクラスの実現値 \\(c^i\\) を インスタンス と呼ぶ．\n潜在変数の実現値が確定することを，観測 の他に インスタンス化 ともいう．\nインスタンス化されたときに取る値は エビデンス とも呼ばれる．\n\n観測値 \\(X_1,\\cdots,X_n\\) は 特徴 (features) と呼ばれ，これは互いに辺で結ばれていないため，クラスを与えた下で互いに条件付き独立であるとする： \\[\n(X_i\\perp\\!\\!\\!\\perp\\boldsymbol{X}_{-i}\\mid C)\\;(i\\in[n]),\n\\] \\[\n\\boldsymbol{X}_{-i}:=(X_{1:i-1},X_{i+1:n}).\n\\]\nこうして得る階層モデルを naive Bayes model という．13 その結合密度は \\[\np(c,x_1,\\cdots,x_n)=p(c)\\prod_{i=1}^np(x_i|c)\n\\] と表せる．\n\n\n2.2 DAG\n\n\n\n\n\n\n定義14 （Bayesian Network structure）\n\n\n\n\n\n確率変数 \\(\\boldsymbol{X}:=(X_1,\\cdots,X_n)\\) に関して，成分の全体 \\(\\mathcal{X}:=\\{\\mathcal{X}_1,\\cdots,\\mathcal{X}_n\\}\\) を節集合とした 有向非循環グラフ (directed acyclic graph, DAG) \\(\\mathcal{G}=(\\mathcal{X},\\mathcal{E})\\) を Bayesian Network 構造 といい，これが親ノードから子ノードへの確率核 \\(P_i:\\pi(\\mathcal{X}_i)\\to\\mathcal{X}_i\\) を通じて定める \\[\n\\prod_{\\mathcal{X}_i\\in\\mathcal{X}}P_i\n\\] という形の分布の全体を Bayesian Network または 有向グラフィカルモデル という．\n\n\n\nBayesian network は belief network とも呼ばれる．15 決定分析で用いられる influence diagram / decision network はその一般化である．\n\n\n\n\n\n\n記法（親ノード，子孫ノード，非子孫ノード）\n\n\n\n\n\nDAG \\(\\mathcal{G}\\) において，\n\n節 \\(\\mathcal{X}_i\\) からその親節の全体への対応を \\[\n\\mathcal{X}_i\\mapsto\\pi(\\mathcal{X}_i)\n\\] で表す．\n節 \\(\\mathcal{X}_i\\) からその子節の全体への対応を添字について表現したものを \\[\n\\mathcal{X}_i\\mapsto\\mathop{\\mathrm{des}}(\\mathcal{X}_i)\n\\] で表す．\n次の対応を 非子孫ノード という： \\[\n\\mathop{\\mathrm{nd}}(\\mathcal{X}_i):=\\mathcal{X}\\setminus(\\{\\mathcal{X}_i\\}\\cup\\mathop{\\mathrm{des}}(\\mathcal{X}_i)).\n\\]\n\n「子孫ノードである」という関係は DAG 上に順序を定める．\n\n\n\n\n\n2.3 DAG が表現する局所独立構造\n\n\n\n\n\n\n定義16 （Directed Local Markov Independence）\n\n\n\n\n\nDAG \\(\\mathcal{G}\\) が表現する条件付き独立性の全体を \\(\\mathcal{I}(\\mathcal{G})\\) で表す．そのうち，特に \\[\nX_i\\perp\\!\\!\\!\\perp(X_j)_{j\\in\\mathop{\\mathrm{nd}}(i)}\\mid (X_j)_{j\\in\\pi(i)}\n\\] という形をしたものを 局所独立性 といい，その（論理式の）全体を \\(\\mathcal{I}_l(\\mathcal{G})\\) で表す．\n\n\n\nBayesian Network が視覚的表現・記号論で，その表現する所の局所依存性が意味論であると言える．\n\n\n\n\n\n\n定義17 （Independence Assertions）\n\n\n\n\n\n\\(P\\in\\mathcal{P}(\\mathcal{X})\\) をノードの集合 \\(\\mathcal{X}=\\{X_1,\\cdots,X_n\\}\\) 上の確率分布とする．\\((X_i)_{i=1}^n\\sim P\\) に関して成立する条件付き独立性の主張 \\[\n(X_i)_{i\\in I}\\perp\\!\\!\\!\\perp(X_j)_{j\\in J}\\mid (X_k)_{k\\in K}\n\\] \\[\nI\\sqcup J\\sqcup K\\subset[n]\n\\] の（論理式の）全体を \\(P\\) が含意する条件付き独立性 といい， \\(\\mathcal{I}(P)\\) で表す．\n\n\n\n\n\n\n\n\n\n定義18 （\\(I\\)-Map）\n\n\n\n\n\n\\(\\mathcal{I}\\) を確率変数 \\((X_1,\\cdots,X_n)\\) の成分間の条件付き独立性に関する論理式の全体，\\(\\mathcal{K}\\) を DAG とする．\\(\\mathcal{K}\\) が \\(\\mathcal{I}\\) の \\(I\\)-map であるとは， \\[\n\\mathcal{I}_l(\\mathcal{K})\\subset\\mathcal{I}\n\\] を満たすことをいう．すなわち，DAG \\(\\mathcal{K}\\) が表す局所的な独立性関係が \\(\\mathcal{I}\\) に含まれていることをいう．\n\n\n\n\n\n2.4 Bayesian Network の特徴付け\n\n\n\n\n\n\n定義19 （factorize, chain rule, local probabilistic model, Bayesian Network）\n\n\n\n\n\n\\(\\mathcal{G}\\) を確率変数 \\((X_1,\\cdots,X_n)\\) に関する Bayesian Network 構造とする．\n\n分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) が \\(\\mathcal{G}\\) に従って 分解する とは，\\((X_1,\\cdots,X_n)\\sim P\\) と仮定したとき，次が成り立つことをいう： \\[\n\\mathcal{L}[X_1,\\cdots,X_n]=\\bigotimes^n_{i=1}\\mathcal{L}[X_i|(X_j)_{j\\in\\pi(i)}].\n\\]\nこの式を Bayesian Network \\(\\mathcal{G}\\) の 連鎖律 といい，右辺の因子 \\(\\mathcal{L}[X_i|(X_j)_{j\\in\\pi(i)}]\\) の全体を 条件付き確率分布族 または 局所モデル という．\nBayesian Network 構造 \\(\\mathcal{G}\\) とこれに沿って分解する分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) との組 \\((\\mathcal{G},P)\\) を，Bayesian Network または 有向確率モデル という．\n\n\n\n\n\n\n\n\n\n\n命題20 （Bayesian Network の特徴付け）\n\n\n\n\n\n\\(\\mathcal{G}\\) を確率変数 \\((X_1,\\cdots,X_n)\\) に関する Bayesian Network 構造，\\(P\\in\\mathcal{P}(\\mathcal{X})\\) を確率分布とする．このとき，次は同値：\n\n\\(\\mathcal{G}\\) が \\(\\mathcal{I}(P)\\) の \\(I\\)-map である．\n\\(P\\) は \\(\\mathcal{G}\\) に従って分解する．\n\n\n\n\n\n\n2.5 ３節グラフの分離性\n節が３つ \\(X,Y,Z\\) の場合の DAG は大別して３通り存在する．この場合で「分離性」の概念を説明する．\n３つの成分 \\((X,Y,Z)\\) が依存関係にある状態で，\\(Z\\) が観測された（インスタンス化された）とする．\nその場合に，\\(X,Y\\) 間の因果関係がどう変化するか？を考える．元々因果関係があったところから，21 これが解消されるとき，\\(X,Y\\) は \\(Z\\) を介して \\(d\\)-分離 であるという．22\n\n\n\n\n\n\n逐次結合の場合\n\n\n\n\n\n次のような逐次結合の場合，節 \\(X,Y\\) は，節 \\(Z\\) がインスタンス化されたとき \\(d\\)-分離 である，という．\n\n\n\n\n\n\n\n\nCausalTrail\n\n\n\nX\n\nX\n\n\n\nZ\n\nZ\n\n\n\nX-&gt;Z\n\n\n\n\n\nY\n\nY\n\n\n\nZ-&gt;Y\n\n\n\n\n\n\n\n\n図 1: 逐次結合 (Causal Trail)\n\n\n\n\n\n\\(X\\) を勉強量，\\(Z\\) を素点，\\(Y\\) を GPA とするとき，\\(Z\\) が観測されたならば，もはや勉強量は GPA に影響を与えない．ただし，相関は存在するだろうが．\n\n\n\n\n\n\n\n\n\n分岐結合の場合\n\n\n\n\n\n次のような分岐結合の場合，節 \\(X,Y\\) は，節 \\(Z\\) がインスタンス化されたとき \\(d\\)-分離 である，という．\n\n\n\n\n\n\n\n\nCausalTrail\n\n\n\nZ\n\nZ\n\n\n\nX\n\nX\n\n\n\nZ-&gt;X\n\n\n\n\n\nY\n\nY\n\n\n\nZ-&gt;Y\n\n\n\n\n\n\n\n\n図 2: 分岐結合 (Common Cause)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n合流結合の場合\n\n\n\n\n\n次のような合流結合の場合，節 \\(X,Y\\) は，節 \\(Z\\) またはその子孫節がインスタンス化されなければ，節 \\(Z\\) を介して \\(d\\)-分離 である，という．\n\n\n\n\n\n\n\n\nCausalTrail\n\n\n\nX\n\nX\n\n\n\nZ\n\nZ\n\n\n\nX-&gt;Z\n\n\n\n\n\nY\n\nY\n\n\n\nY-&gt;Z\n\n\n\n\n\n\n\n\n図 3: 合流結合 (Common Effect)\n\n\n\n\n\nこの構造は \\(v\\)-構造ともいう．23 この場合，\\(Z\\) が観測されたならば，\\(X,Y\\) は因果関係を持つようになる．\n\\(Z\\) が事象の有無で，\\(X,Y\\) のいずれかが起こった時に \\(Z\\) も起こるとしよう．いま \\(Z\\) が起こったこと \\(Z=1\\) が判明したとすると，\\(X,Y\\) のいずれか一方も起こっている必要がある．従って，\\(X=0\\) は \\(Y=1\\) を要請するという因果関係が生じる．\n\n\n\n\n\n2.6 一般の DAG の分離性\n\n\n\n\n\n\n定義24 （active, \\(d\\)-Separated, Directed Global Markov Independencies）\n\n\n\n\n\n\\(\\mathcal{G}\\) を Bayesian Network 構造，\\(\\boldsymbol{Z}\\subset\\mathcal{X}\\) を観測された節とする．\n\n非有向道 \\(X_1\\rightleftharpoons\\cdots\\rightleftharpoons X_n\\) が \\(\\boldsymbol{Z}\\) の下でも active であるとは， 次の２条件を満たすことをいう：\n\n\\(\\{X_i\\}_{i=1}^n\\cap\\boldsymbol{Z}=\\emptyset\\)．\n任意の無向道内の合流結合 \\(X_{i-1}\\rightarrow X_i\\leftarrow X_{i+1}\\) について，\\(X_i\\) またはその子孫に \\(\\boldsymbol{Z}\\) の元が存在する．\n\n\\(\\boldsymbol{X}\\sqcup\\boldsymbol{Y}\\sqcup\\boldsymbol{Z}\\subset\\mathcal{X}\\) を節の集合とする．\\(\\boldsymbol{X},\\boldsymbol{Y}\\) が \\(\\boldsymbol{Z}\\) に関して \\(d\\)-分離 であるとは，任意の \\(X\\in\\boldsymbol{X}\\) と \\(Y\\in\\boldsymbol{Y}\\) と，\\(X,Y\\) を結ぶ無向道が，\\(\\boldsymbol{Z}\\) の下で active でないことをいう．このことを \\(\\mathop{\\mathrm{d-sep}}_\\mathcal{G}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\) と表す．25\n\\(\\mathcal{G}\\) 内の \\(d\\)-分離な組 \\((\\boldsymbol{X},\\boldsymbol{Y},\\boldsymbol{Z})\\) が表す条件付き独立性の条件式の全体を \\[\n\\mathcal{I}(\\mathcal{G}):=\\left\\{(\\boldsymbol{X}\\perp\\!\\!\\!\\perp\\boldsymbol{Y}|\\boldsymbol{Z})\\mid\\mathop{\\mathrm{d-sep}}_\\mathcal{G}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\right\\}.\n\\] この元を 大域的独立性 ともいう．\n\n\n\n\n局所依存性（ Section 2.2 ）は \\(d\\)-分離性の特別な場合であり，\\(\\mathcal{I}_l(\\mathcal{G})\\subset\\mathcal{I}(\\mathcal{G})\\) である．\n\n\n2.7 例\n\n\n\n\\(d\\)-分離になるのはいつか？\n\n\nこの Bayesian Network 構造は，いつ \\(d\\)-分離になり，いつ \\(d\\)-分離ではないか？\n\n\n\n\n\n\n答え\n\n\n\n\n\n\nいずれも観測されない場合は \\(d\\)-分離である．\n\\(Z\\) が観測された場合，\\(A,B\\) のいずれかも観測されていれば，やはり \\(d\\)-分離である．\n\n\n\n\n\n\n2.8 分離性の特徴付け\n\n\n\n\n\n\n命題26 （\\(d\\)-分離性の特徴付け）\n\n\n\n\n\n\\(\\mathcal{G}\\) を Bayesian Network 構造，\\(P\\in\\mathcal{P}(\\mathcal{X})\\) を確率分布とする．\n\n\\(P\\) が \\(\\mathcal{G}\\) に沿って分解するならば，\\(\\mathcal{I}(\\mathcal{G})\\subset\\mathcal{I}(P)\\)．\n\\(\\mathcal{H}\\) に沿って分解する殆ど全ての \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して，上の逆も成り立ち，特に等号が成立する．\n\n\n\n\n\\(\\mathcal{G}\\) が定める分布族について，殆ど全ての分布が共通して持つ条件付き独立性の構造を，\\(\\mathcal{G}\\) から読み取れる \\(d\\)-分離性によって発見できるということになる．\nさらには，分布 \\(P\\) の独立性の情報を知りたい場合，この背後にあるグラフ \\(\\mathcal{G}\\) を探し出して，\\(d\\)-分離性を調べれば良い，ということでもであるのである．27\n\n\n2.9 \\(I\\)-同値性\n\n\n\n\n\n\n定義28 （\\(I\\)-Equivalence）\n\n\n\n\n\n２つの Bayesian Network 構造 \\(\\mathcal{G},\\mathcal{G}'\\) が \\(I\\)-同値 であるとは，\\(\\mathcal{I}(\\mathcal{G})=\\mathcal{I}(\\mathcal{G}')\\) が成り立つことをいう．\n\n\n\n\\(I\\) は写像であるから，この関係は確かに Bayesian Network 構造の全体（果てには有向グラフの全体）に同値関係を定める．\n\n\n\n\n\n\n命題29 （\\(I\\)-同値性の十分条件）\n\n\n\n\n\n２つの Bayesian Network 構造 \\(\\mathcal{G},\\mathcal{G}'\\) が\n\n同じスケルトンを持ち，30\n同じ \\(v\\)-構造を持つ\n\nならば，\\(I\\)-同値である．\n\n\n\n有向グラフ \\(\\mathcal{G}=(\\mathcal{X},\\mathcal{E})\\) の辺 \\((X,Y)\\in\\mathcal{E}\\) が 被覆されている とは， \\[\n\\pi(Y)=\\pi(X)\\cup\\{X\\}\n\\] を満たすことをいう．\n合流結合 \\(X\\rightarrow Z\\leftarrow Y\\) において，辺 \\(X\\to Z\\) は被覆されていない．\n\n\n\n\n\n\n命題31 （\\(I\\)-同値性の特徴付け）\n\n\n\n\n\n２つの Bayesian Network 構造 \\(\\mathcal{G},\\mathcal{G}'\\) について，次は同値：\n\n\\(\\mathcal{G},\\mathcal{G}'\\) は \\(I\\)-同値である．\n\\(\\mathcal{G}\\) に \\(I\\)-同値なグラフの列 \\(\\mathcal{G}=\\mathcal{G}_0,\\cdots,\\mathcal{G}_m=\\mathcal{G}'\\) であって，隣り合うグラフ \\(\\mathcal{G}_i,\\mathcal{G}_{i+1}\\;(i\\in m)\\) 同士は，被覆されている辺の向きの反転しか違わないものが存在する．"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html#markov-network",
    "href": "posts/2024/Computation/PGM1.html#markov-network",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "3 Markov Network",
    "text": "3 Markov Network\n\n3.1 グラフ理論の準備\n\\(A\\) を集合とする． \\[\n[A]^k:=\\left\\{B\\in P(A)\\mid\\# B=k\\right\\}\n\\] とする．無向グラフとは集合 \\(V\\) と \\(E\\subset[V]^2\\) の組 \\(G:=(V,E)\\) のことをいう．32\nMarkov Network 構造 とは，任意の無向グラフをいう．\n２つの節 \\(x,y\\in V\\) が 隣接する (adjacent / neighbours) とは，\\(\\{x,y\\}\\in E\\) が成り立つことをいう．\n無向グラフ \\(G\\) が 完備 (complete) であるとは，任意の \\(x,y\\in V\\) について \\(\\{x,y\\}\\in E\\) が成り立つことをいう．このとき，頂点集合 \\(V\\) は クリーク (clique) であるという．位数 \\(n\\) の完備グラフは \\(K^n\\) で表される．33\n\\(K^r\\subset G\\) を満たす最大の数 \\[\n\\omega(G):=\\left\\{r\\in\\mathbb{N}\\mid K^r\\subset G\\right\\}\n\\] を クリーク数 といい，グラフの不変量となる．34\n弦グラフ (chordal / triangulated graph) とは，任意の長さ４以上のサイクルが弦を持つグラフを言う．35 弦グラフが，Bayesian Network と Markov Network の双方により表現可能であるグラフのクラスに一致する．\n\n\n3.2 Markov Network と Markov Random Field\nマルコフネットワークは，２次元のマルコフ確率場に等価である．36\n後者は Ising モデル の一般化である．37\n\n\n3.3 はじめに\nMarkov Network は相互作用に自然な双方向性がない場合でもモデリングを可能とする．\n例えば，集合 \\(\\{A,B,C,D\\}\\) 上の条件付き独立関係 \\[\n\\mathcal{I}:=\\left\\{\\substack{A\\perp\\!\\!\\!\\perp C|(B,D),\\\\B\\perp\\!\\!\\!\\perp D|(A,C)}\\right\\}\n\\] に関して，\\(\\mathcal{I}(\\mathcal{G})=\\mathcal{I}\\) を満たす Bayesian Network 構造 \\(\\mathcal{G}\\) は存在しない．\n一方で，分岐結合と合流結合とを区別できないため，因果性のような方向を持った依存関係は表現できない．\nMarkov Network では，節の間に自然な順序構造がないため，分布の表示が難しくなり，より純粋にグラフの分解に頼ることになる．それゆえ，データからの構造学習も遥かに難しくなる．38\nBayesian Network では条件付き確率密度のみで十分だったところを，これを一般化する概念である factor と呼ばれる概念によって達成する．\n条件付き確率密度 \\(p(x_1,\\cdots,x_m|y_1,\\cdots,y_k)\\) とは，形式的には，積空間 \\(\\prod_{i=1}^m\\mathrm{Im}\\,(X_i)\\times\\prod_{j=1}^k\\mathrm{Im}\\,(Y_j)\\) 上の（正規化された）関数である．一般に，確率変数の値域の積上の（正規化されているとは限らない）関数を ファクター と言う．\n\n\n3.4 ファクター\n確率変数の組 \\(\\boldsymbol{X}=(X_1,\\cdots,X_n)\\) 上の ファクター とは，ある部分集合 \\(\\{n_1,\\cdots,n_D\\}\\subset[n]\\) に対して，関数 \\((X_{n_1},\\cdots,X_{n_D})\\) の値域上に定義された関数 \\[\n\\phi:\\prod_{i=1}^D\\mathrm{Im}\\,(X_{n_i})\\to\\mathbb{R}\n\\] を言う．この定義域を スコープ と言う．39\n定義域 \\(a,b\\subset[n]\\) がかぶる２つのファクター \\(\\phi_1,\\phi_2,a\\cap b\\ne\\emptyset\\) が存在する場合，これらを接続して，\\(\\prod_{i\\in a\\cup b}\\mathrm{Im}\\,(X_i)\\) 上に定義された新たなファクターを作ることが出来る：40 \\[\n\\phi_1\\times\\phi_2(X_{a\\cup b}):=\\phi_1(X_a)\\phi_2(X_b).\n\\]\n\n\n\n\n\n\n定義41 （Gibbs distribution, factorization）\n\n\n\n\n\n\n離散確率変数の組 \\(\\boldsymbol{X}=(X_1,\\cdots,X_n)\\) とその上のファクター \\[\n\\Phi:=(\\phi_1(\\boldsymbol{D}_1),\\cdots,\\phi_m(\\boldsymbol{D}_m))\n\\] \\[\n\\boldsymbol{D}_j\\subset\\{X_i\\}_{i=1}^n\\quad(j\\in[m])\n\\] とが定める \\(\\prod_{i=1}^n\\mathrm{Im}\\,(X_i)\\) 上の Gibbs 分布 とは，密度 \\[\np_\\Phi(\\boldsymbol{x})=\\frac{1}{Z}\\prod_{j=1}^m\\phi_j(\\boldsymbol{D}_j)\n\\] が定める分布をいう．ここで \\(Z\\) は正規化定数であり，歴史的には 分配関数 と言う．42\nGibbs 分布 \\(p_\\Phi\\) が Markov network \\(\\mathcal{H}=(\\{X_i\\}_{i=1}^n,\\mathcal{E})\\) 上で 分解する とは，任意の \\(\\mathcal{D}_j\\subset\\{X_i\\}_{i=1}^n\\;(j\\in[m])\\) が \\(\\mathcal{H}\\) のクリークであることをいう．このとき，各ファクター \\(\\phi_1,\\cdots,\\phi_m\\) を clique potential という．\n\n\n\n\n\n\n3.5 Markov Network の分離性\n\n\n\n\n\n\n定義43 （Global Markov Independence）\n\n\n\n\n\n\\(\\mathcal{H}\\) を Markov network 構造とする．\n\n道 \\(X_1\\rightleftharpoons\\cdots\\rightleftharpoons X_n\\) が \\(\\boldsymbol{Z}\\subset\\{X_i\\}_{i=1}^n\\) が観測された下でも active であるとは，\\(\\{X_i\\}_{i=1}^n\\cap\\boldsymbol{Z}=\\emptyset\\) を満たすことをいう．\n節集合 \\(\\boldsymbol{X},\\boldsymbol{Y},\\boldsymbol{Z}\\) について，\\(\\boldsymbol{Z}\\) が \\(\\boldsymbol{X},\\boldsymbol{Y}\\) を 分離 するとは，任意の \\(X\\in\\boldsymbol{X}\\) と \\(Y\\in\\boldsymbol{Y}\\) と，\\(X,Y\\) を結ぶ道が，\\(\\boldsymbol{Z}\\) の下で active でないことをいう．このことを \\(\\mathop{\\mathrm{sep}}_\\mathcal{H}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\) と表す．\n\n\n\\(\\mathcal{H}\\) 内の分離的な組 \\((\\boldsymbol{X},\\boldsymbol{Y},\\boldsymbol{Z})\\) が表す条件付き独立性の条件式の全体を \\[\n\\mathcal{I}(\\mathcal{H}):=\\left\\{(\\boldsymbol{X}\\perp\\!\\!\\!\\perp\\boldsymbol{Y}|\\boldsymbol{Z})\\mid\\mathop{\\mathrm{sep}}_\\mathcal{H}(\\boldsymbol{X};\\boldsymbol{Y}|\\boldsymbol{Z})\\right\\}\n\\] で表す．この元を 大域的独立性 ともいう．\n\n\n\n\n\n\n\n\n\n\n定理44 (Hammersley and Clifford, 1971)\n\n\n\n\n\n\\(P\\in\\mathcal{P}(\\mathcal{X})\\) をノードの集合 \\(\\mathcal{X}=\\{X_1,\\cdots,X_n\\}\\) 上の確率分布，\\(\\mathcal{H}\\) を \\(\\mathcal{X}\\) 上の Markov network 構造とする．このとき 1. \\(\\Rightarrow\\) 2. が成り立ち，\\(P\\) が \\(\\mathcal{X}\\) 全域を台に持つとき次は同値：\n\n\\(\\mathcal{H}\\) は \\(P\\) の \\(I\\)-map である：\\(\\mathcal{I}(\\mathcal{H})\\subset\\mathcal{I}(P)\\)．\n\\(P\\) は \\(\\mathcal{H}\\) に従って分解する Gibbs 分布である．\n\n\n\n\n(Besag, 1974) はこの定理に別証明を付し，植物生態学における空間統計モデルに応用している．45\nMarkov 確率場の結合分布を，条件付き分布の系から得ることは困難であるが，結局結合分布も Gibbs 分布になることが (Hammersley and Clifford, 1971) の定理からわかるので，Gibbs 分布を通じて計算することができる．\nこの「条件付き分布から結合分布が復元できる」という知見が Gibbs sampling の基礎となった．46 また統計的画像解析の基礎ともなった (Grenander, 1983)．\nまた (Geman and Geman, 1984) は，Markov 確率場でモデリングをし，その最大事後確率 MAP (Maximum a Posteriori) を目的関数として最適化を行う，という MAP-MRF アプローチを創始した (Stan Z. Li, 2009, p. 2)．\nさらに統計計算法の進展により，画像の低レイヤーな特徴を表現する（画像修復，物体発見など）だけでなく，高レイヤーな特徴（物体認識やマッチングなど）をも扱えることがわかっている (Gidas, 1989), (S. Ziqing Li, 1991)．\n\n\n\n\n\n\n命題47 （分離性の特徴付け）\n\n\n\n\n\n\\(\\mathcal{H}\\) を Markov network 構造，\\(\\{X\\}\\sqcup\\{Y\\}\\sqcup\\boldsymbol{Z}\\subset\\mathcal{X}\\) を節の集合とする．このとき，次が成り立つ：\n\n\\(\\mathcal{H}\\) 内で \\(X,Y\\) は \\(\\boldsymbol{Z}\\) によって分離されないならば，ある \\(\\mathcal{H}\\) に沿って分解する分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) について，\\(X\\perp\\!\\!\\!\\perp Y|\\boldsymbol{Z}\\) が成り立つ．\n\\(\\mathcal{H}\\) に沿って分解する殆ど全ての \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して，\\(\\mathcal{I}(\\mathcal{H})=\\mathcal{I}(P)\\) が成り立つ．\n\n\n\n\nBeysian Network （ Section 2.8 ）の場合と違い，1. の主張が，\\(\\mathcal{H}\\) に沿って分解する全ての分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して成り立つとは限らない．\nしかし，殆ど全ての \\(\\mathcal{H}\\) に沿って分解する分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) に関して成り立つ条件付き独立性は，グラフの構造から読み取れる．\n\n\n3.6 局所依存性\nBayesian Network の \\(d\\)-分離性に対応する分離性の概念を導入し，大域的独立性の概念を定義した．\nしかし，Bayesian Network の場合では有向グラフとしての構造からすぐに読み取れた局所依存性の概念は，Markov Network の場合では，グラフの構造からは読み取れない．\nそして２通りの定義が考え得る．局所依存性は，大域的依存性のサブセットであることに注意．そして，台を全体 \\(\\mathcal{X}\\) に持つ分布については，大域的依存性も含めて３つの定義は全て同値である．48"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html#sec-Factor-Graph",
    "href": "posts/2024/Computation/PGM1.html#sec-Factor-Graph",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "4 Factor Graph",
    "text": "4 Factor Graph\nMarkov network は Gibbs 分布の依存性を十分に表現できているわけではなかった（第 3.5 節）．これは特に，クリーク間の大小関係を把握できていないことに因る．\n\n4.1 定義\n\n\n\n\n\n\n定義49 （Factor Graph）\n\n\n\n\n\nMarkov newtork から，ファクターを表す節を（四角形で囲うなどして区別した形で）追加し，ファクターをそのスコープに入る変数と隣接するようにし，一方で変数を表す（元々の）節とファクターを表す節とが隣接しないように修正した ２部グラフ \\(\\mathcal{F}\\) を 因子グラフ という．\n分布 \\(P\\in\\mathcal{P}(\\mathcal{X})\\) が \\(\\mathcal{F}\\) に関して 分解する とは，\\(\\mathcal{F}\\) が定める確率変数の組とその上のファクターが定める Gibbs 分布であることをいう．"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html#終わりに",
    "href": "posts/2024/Computation/PGM1.html#終わりに",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nMCMC，特に Gibbs サンプラーは，Markov network の形で与えられる局所的な情報を利用したダイナミクスを持つ．\nそれ故，デザインから，大域的な探索が不得手であると言える．"
  },
  {
    "objectID": "posts/2024/Computation/PGM1.html#footnotes",
    "href": "posts/2024/Computation/PGM1.html#footnotes",
    "title": "数学者のための確率的グラフィカルモデル１",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Taroni et al., 2014, p. 35)，(Sucar, 2021, p. x), (Clark, 2018) Graphical Models．(Jordan et al., 1999, p. 191) は 3.2節で Neural Networks as Graphical Models を扱っている．↩︎\nこの文脈では，ベイジアンネットワークのことは DAG とも，汎函数因果モデル (Schölkopf, 2022) とも呼ぶ．(Murphy, 2023, p. 211) 4.7節．例えば，医療診断では，複数の症状や検査結果，医学的指標との関連・相関・因果に関する知識を Bayesian Network （Section 2） で表現する．↩︎\n(Mézard and Montanari, 2009, p. 177) 9.1.2 節に，ファクターグラフの例が挙げられている．↩︎\n(Koller and Friedman, 2009, p. 3) 1.2.1，(Murphy, 2023, p. 143) 第4章．(Balgi et al., 2024) “As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships.”↩︎\n(Koller and Friedman, 2009, pp. 12–14) 1.4節 など．↩︎\n一般に，SEM は (Karl Gustav Jöreskog, 1970) が発祥と見られており，潜在変数モデルにもパス解析を拡張したもの，と説明される (Clark, 2018)．↩︎\n(黒木学 and 小林史明, 2012) など．↩︎\n(Koller and Friedman, 2009, pp. 12–14) 1.4節．↩︎\n(Koller and Friedman, 2009, p. 1) 1.1 Motivation．これはこの分野が不確実性を定量的に扱う必要があり，それ故確率的モデリングを必要としたためである．一般に，特定のタスクに特化しながら，汎用性も持つエキスパートシステムを構築するためには，宣言型の知識表現 が良い接近として用いられる (Koller and Friedman, 2009, p. 1) 1.1 Motivation．declarative representation の他に model-based approach ともいう．これは対象となるシステムの構造に関する知識を，計算機が理解可能な形で表現するモデルベースな接近であり，「知識」と「推論」という異なるタスクを分離する点に妙がある．↩︎\n(Koller and Friedman, 2009, p. 2) 1.1 Motivation．Probabilistic models allow us to make this fact (= many systems cannot be specified deterministically.) explicit, and therefore often provide a model which is more faithful to reality.↩︎\n(Theodoridis, 2020, p. 772) なども参照．用いるアルゴリズムの計算複雑性も，グラフ理論の言葉で記述できることが多い (Wainwright and Jordan, 2008, p. 4)．↩︎\nその最重要文献として，(Grenander, 1983)，特に画像分析への Bayesian アプローチを取り扱った 4-6 章を挙げている．Gibbs サンプラーの語を導入したのは (Geman and Geman, 1984) であるが，すでに (Grenander, 1983) において極めて重要な Bayes 計算手法として扱われていた．↩︎\n(Bishop, 2006, p. 46) などでも紹介されている．↩︎\n(Koller and Friedman, 2009, p. 57)，(Mézard and Montanari, 2009, p. 269)．↩︎\n(須山敦志, 2019, p. 4), (Stan Z. Li, 2009, p. 48)．Wikipedia も参照．↩︎\n(Koller and Friedman, 2009, p. 57)↩︎\n(Koller and Friedman, 2009, p. 60)↩︎\n(Koller and Friedman, 2009, p. 60)↩︎\n(Koller and Friedman, 2009, p. 62)↩︎\n(Koller and Friedman, 2009, p. 62) 定理3.1，定理3.2 p.63．(Ronald A. Howard and Matheson, 1984) による．↩︎\nこれを trail が active である，ともいう．(Koller and Friedman, 2009, p. 71)．↩︎\nこの語は directed separation の略であり (Koller and Friedman, 2009, p. 71)，和語では 有向分離 ともいう．↩︎\n(Koller and Friedman, 2009, p. 71)↩︎\n(Koller and Friedman, 2009, pp. 71–72) 定義3.6, 3.7．↩︎\n\\(I(\\boldsymbol{X},\\boldsymbol{Y}|\\boldsymbol{Z})_\\mathcal{G}\\) と表すこともある．↩︎\n(Koller and Friedman, 2009, pp. 72–73) 定理3.3, 3.5．↩︎\n(Koller and Friedman, 2009, p. 78) 3.4節 の内容．↩︎\n(Koller and Friedman, 2009, p. 76) 定義3.9．↩︎\n(Koller and Friedman, 2009, p. 77) 定理3.7．↩︎\n有向グラフの スケルトン とは，同じ辺を持つ無向グラフのことである．↩︎\n(Koller and Friedman, 2009, p. 77) 定理3.8．↩︎\n(Diestel, 2017, pp. 1–2) 参照．↩︎\n(Diestel, 2017, p. 3) 参照．↩︎\n(Diestel, 2017, p. 135) 参照．↩︎\nすなわち，三角形以外の 誘導部分グラフ を部分グラフに持たないグラフをいう．(Diestel, 2017, p. 135) 参照．↩︎\n(Sucar, 2021, p. 94) も参照．(Stan Z. Li, 2009, p. 47) は，pairwise なマルコフ確率場もマルコフネットワークと見れることを指摘している．pairwise とは非零なポテンシャルを持つクリークが二点集合になるマルコフ確率場をいう．↩︎\n(Kindermann and Snell, 1980, p. 1)↩︎\n(Koller and Friedman, 2009, p. 106) 4.2節．↩︎\n(Koller and Friedman, 2009, p. 104) 定義4.1．↩︎\nただし，\\(\\phi_1(X_a)\\) とは \\(\\phi_1((X_i)_{i\\in a})\\) の略とした．↩︎\n(Koller and Friedman, 2009, p. 108) 定義4.3．↩︎\n(Koller and Friedman, 2009, p. 105) によると，当初統計物理学の分野の Markov 確率場の概念でこの用語が用いられたことが始まりとなっている．↩︎\n(Koller and Friedman, 2009, pp. 114–115) 定義4.8, 9．↩︎\n(Koller and Friedman, 2009, pp. 116–117) 定理4.1，定理4.2．↩︎\n(Stan Z. Li, 2009) の Rama Chellappa による foreword に “A big impetus to theoretical and practical considerations of 2D spatial interaction models, of which MRF’s form a subclass, was given by the seminal works of Julian Besag.” とある．“Labeling is also a natural representation for the study of MRF’s (Besag 1974).” は (Stan Z. Li, 2009, p. 3)．↩︎\n(C. Robert and Casella, 2011) (Stan Z. Li, 2009, p. 1) も参照．↩︎\n(Koller and Friedman, 2009, p. 117) 定理4.3．↩︎\nこれは台が縮退している場合は，自明な（決定論的な）独立性が生じてしまうためである．↩︎\n(Koller and Friedman, 2009, p. 123) 4.4.1.1，(Mézard and Montanari, 2009, p. 175) 9.1.1 節．↩︎"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html",
    "href": "posts/2024/Slides/ZigZagPoliSci.html",
    "title": "Zig-Zag Sampler",
    "section": "",
    "text": "A continuous-time variant of MCMC algorithms\n\n\n\nTrajectory for Zig-Zag Sampler. Please attribute Hirofumi Shiba.  \n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\nPDMP (Piecewise Deterministic1 Markov Process2) (Davis, 1984)\n\nMostly deterministic with the exception of random jumps happens at random times\nContinuous-time, instead of discrete-time processes\n\n Plays a complementary role to SDEs / Diffusions\n\n\n\n\n\n\n\n\n\nProperty\nPDMP\nSDE\n\n\n\n\nExactly simulatable?\n\n\n\n\nSubject to discretization errors?\n\n\n\n\nDriving noise\nPoisson\nGauss\n\n\n\n\n\n\n\n\n\n\nHistory of PDMP Applications\n\n\n\n\nFirst applications: control theory, operations research, etc. (Davis, 1993)\nSecond applications: Monte Carlo simulation in material sciences (Peters and de With, 2012)\nThird applications: Bayesian statistics (Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n\nWe will concentrate on Zig-Zag sampler (Bierkens, Fearnhead, et al., 2019)\nOther PDMPs: Bouncy sampler (Bouchard-Côté et al., 2018) , Boomerang sampler (Bierkens et al., 2020)\n\n\n\n\nThe most famous three PDMPs. Animated by (Grazzi, 2020)\n\n\n\n\n\n\n\n\n\n\n\nWhat We’ve Learned\n\n\n\nThe new algorithm ‘Zig-Zag Sampler’ is based on comtinuous-time process called PDMP.\n\n\n\n\n\n\n\n\nWhat We’ll Learn in the Rest of this Section 1\n\n\n\nWe will review 3 instances of the standard (discrete-time) MCMC algorithm: MH, Lifted MH, and MALA.\n\nReview: MH (Metropolis-Hastings) algorithm\nReview: Lifted MH, A method bridging MH and Zig-Zag\nComparison: MH vs. Lifted MH vs. Zig-Zag\nReview: MALA (Metropolis Adjusted Langevin Algorithm)\nComparison: Zig-Zag vs. MALA\n\n\n\n\n\n\n\n\n\n\n\n\n(Metropolis et al., 1953)-(Hastings, 1970)\n\n\n\nInput: Target distribution p, (symmetric) proposal distribution q\n\nDraw a X_t\\sim q(-|X_{t-1})\nCompute \n  \\alpha(X_{t-1}, X_t) = \\frac{p(X_t)}{p(X_{t-1})}\n  \nDraw a uniform random number U\\sim\\mathrm{U}([0,1]).\nIf \\alpha(X_{t-1},X_t)\\le U, then X_t\\gets X_{t-1}. Do nothing otherwise.\nReturn to Step 1.\n\n\n\n\n\nMH algorithm works even without p’s normalizing constant. Hence, its ubiquity.\n\n\n\n\nAlternative View: MH is a generic procedure to turn a simple q-Markov chain into a Markov chain converging to p.\n\n\n\n\n\n\n\nThe Choise of Proposal q\n\n\n\n\nRandom Walk Metropolis (Metropolis et al., 1953): Uniform / Gaussian \nq(y|x) = q(y-x) \\in\\left\\{ \\frac{d \\mathrm{U}([0,1])}{d \\lambda}(y-x),\\frac{d \\mathrm{N}(0,\\Sigma)}{d \\lambda}(y-x)\\right\\}\n\nHybrid / Hamiltonian Monte Carlo (Duane et al., 1987): Hamiltonian dynamics \nq(y|x) = \\delta_{x + \\epsilon\\rho},\\qquad\\epsilon&gt;0,\\;\\rho\\;\\text{: momentum defined via Hamiltonian}\n\nMetropolis-adjusted Langevin algorithm (MALA) (Besag, 1994): Langevin diffusion \nq(-|X_t):=\\text{ the transition probability of } X_t \\text{ where } dX_t=\\nabla\\log p(X_t)\\,dt+\\sqrt{2\\beta^{-1}}dB_t.\n\n\n\n\n\n\n\n\nReversibility (a.k.a detailed balance): \np(x)q(x|y)=p(y)q(y|x).\n In words: \n\\text{Probability}[\\text{Going}\\;x\\to y]=\\text{Probability}[\\text{Going}\\;y\\to x].\n  Harder to explore the entire space\n Slow mixing of MH\n\n\nFrom the beginning of 21th century, many efforts have been made to make MH irreversible.\n\n\n\n\nLifting: A method to make MH’s dynamics irreversible\nHow?: By adding an auxiliary variable \\sigma\\in\\{\\pm1\\}, called momentum\n\n\n\n\n\n\n\nLifted MH (Turitsyn et al., 2011)\n\n\n\nInput: Target p, two proposals q^{(+1)},q^{(-1)}, and momentum \\sigma\\in\\{\\pm1\\}\n\nDraw X_t from q^{(\\sigma)}\nDo a MH step\nIf accepted, go back to Step 1.\nIf rejected, flip the momentum and go back to Step 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nq^{(+1)}: Only propose \\rightarrow moves\nq^{(-1)}: Only propose \\leftarrow moves\n\n Once going uphill, it continues to go uphill.\n This is irreversible, since\n\\begin{align*}\n  &\\text{Probability}[x\\to y]\\\\\n  &\\qquad\\ne\\text{Probability}[y\\to x].\n\\end{align*}\n\n\n\n\n\n\nReversible dynamic of MH has ‘irreversified’\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nScale is different in the vertical axis!\n\n\n\nLifted MH successfully explores the edges of the target distribution.\n\n\n\n\n\n\n*Irreversibility actually improves the efficiency of MCMC, as we observe in two slides later.\n\n\n\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\nZig-Zag corresponds to the limiting case of lifted MH as the step size of proposal q goes to zero, as we’ll learn later.\n Zig-Zag has a maximum irreversibility.\n\n\n\nIrreversibility actually improves the efficiency of MCMC.\nFaster decay of autocorrelation \\rho_t\\approx\\mathrm{Corr}[X_0,X_t] implies\n\nfaster mixing of MCMC\nlower variance of Monte Carlo estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin diffusion: A diffusion process defined by the following SDE:\n\ndX_t=\\nabla\\log p(X_t)\\,dt+\\sqrt{2\\beta^{-1}}dB_t.\n\nLangevin diffusion itself converges to the target distribution p in the sense that 3 \n\\|p_t-p\\|_{L^1}\\to0,\\qquad t\\to\\infty.\n\n\n\n\nTwo MCMC algorithms derived from Langevin diffusion:\n\nULA (Unadjusted Langevin Algorithm) \\quad Use the discretization of (X_t). Discretization errors accumulate.\nMALA (Metropolis Adjusted Langevin Algorithm) \\quad Use ULA as a proposal in MH, erasing the errors by MH steps.\n\n\n\n\nHow fast do they go back to high-probability regions? 4\n\n\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\n\n\nMALA\n\n\n\n\n\nIrreversibility of Zig-Zag accelerates its convergence.\n\n\n\n\n\n\n\n\n\n\n\n\nMALA trajectory\n\n\n\n\n\n\n\n\n\n\nCaution: Fake Continuity\n\n\n\nThe left plot looks continuous, but it actually is not.\n\n\n\n\n\nMH, including MALA, is actually a discrete-time process.\nThe plot is obtained by connecting the points by line segments.\n\n\n\nMonte Carlo estimation is also done differently:\n\n\n\n\n\n\n\n\n\nMALA outputs (X_n)_{n\\in[N]} defines\n\n\\frac{1}{N}\\sum_{n=1}^Nf(X_n)\\xrightarrow{N\\to\\infty}\\int_{\\mathbb{R}^d} f(x)p(x)\\,dx.\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag outputs (X_t)_{t\\in[0,T]} defines\n\n\\int^T_0f(X_t)\\,dt\\xrightarrow{T\\to\\infty}\\int_{\\mathbb{R}^d} f(x)p(x)\\,dx.\n\n\n\n\n\n\n\n\nZig-Zag Sampler’s trajectory is a PDMP.\nPDMP, by design, has maximum irreversibility.\nIrreversibility leads to faster convergence of Zig-Zag in comparisons against MH, Lifted MH, and especially MALA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html#sec-Zig-Zag",
    "href": "posts/2024/Slides/ZigZagPoliSci.html#sec-Zig-Zag",
    "title": "Zig-Zag Sampler",
    "section": "",
    "text": "A continuous-time variant of MCMC algorithms\n\n\n\nTrajectory for Zig-Zag Sampler. Please attribute Hirofumi Shiba.  \n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\nPDMP (Piecewise Deterministic1 Markov Process2) (Davis, 1984)\n\nMostly deterministic with the exception of random jumps happens at random times\nContinuous-time, instead of discrete-time processes\n\n Plays a complementary role to SDEs / Diffusions\n\n\n\n\n\n\n\n\n\nProperty\nPDMP\nSDE\n\n\n\n\nExactly simulatable?\n\n\n\n\nSubject to discretization errors?\n\n\n\n\nDriving noise\nPoisson\nGauss\n\n\n\n\n\n\n\n\n\n\nHistory of PDMP Applications\n\n\n\n\nFirst applications: control theory, operations research, etc. (Davis, 1993)\nSecond applications: Monte Carlo simulation in material sciences (Peters and de With, 2012)\nThird applications: Bayesian statistics (Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n\nWe will concentrate on Zig-Zag sampler (Bierkens, Fearnhead, et al., 2019)\nOther PDMPs: Bouncy sampler (Bouchard-Côté et al., 2018) , Boomerang sampler (Bierkens et al., 2020)\n\n\n\n\nThe most famous three PDMPs. Animated by (Grazzi, 2020)\n\n\n\n\n\n\n\n\n\n\n\nWhat We’ve Learned\n\n\n\nThe new algorithm ‘Zig-Zag Sampler’ is based on comtinuous-time process called PDMP.\n\n\n\n\n\n\n\n\nWhat We’ll Learn in the Rest of this Section 1\n\n\n\nWe will review 3 instances of the standard (discrete-time) MCMC algorithm: MH, Lifted MH, and MALA.\n\nReview: MH (Metropolis-Hastings) algorithm\nReview: Lifted MH, A method bridging MH and Zig-Zag\nComparison: MH vs. Lifted MH vs. Zig-Zag\nReview: MALA (Metropolis Adjusted Langevin Algorithm)\nComparison: Zig-Zag vs. MALA\n\n\n\n\n\n\n\n\n\n\n\n\n(Metropolis et al., 1953)-(Hastings, 1970)\n\n\n\nInput: Target distribution p, (symmetric) proposal distribution q\n\nDraw a X_t\\sim q(-|X_{t-1})\nCompute \n  \\alpha(X_{t-1}, X_t) = \\frac{p(X_t)}{p(X_{t-1})}\n  \nDraw a uniform random number U\\sim\\mathrm{U}([0,1]).\nIf \\alpha(X_{t-1},X_t)\\le U, then X_t\\gets X_{t-1}. Do nothing otherwise.\nReturn to Step 1.\n\n\n\n\n\nMH algorithm works even without p’s normalizing constant. Hence, its ubiquity.\n\n\n\n\nAlternative View: MH is a generic procedure to turn a simple q-Markov chain into a Markov chain converging to p.\n\n\n\n\n\n\n\nThe Choise of Proposal q\n\n\n\n\nRandom Walk Metropolis (Metropolis et al., 1953): Uniform / Gaussian \nq(y|x) = q(y-x) \\in\\left\\{ \\frac{d \\mathrm{U}([0,1])}{d \\lambda}(y-x),\\frac{d \\mathrm{N}(0,\\Sigma)}{d \\lambda}(y-x)\\right\\}\n\nHybrid / Hamiltonian Monte Carlo (Duane et al., 1987): Hamiltonian dynamics \nq(y|x) = \\delta_{x + \\epsilon\\rho},\\qquad\\epsilon&gt;0,\\;\\rho\\;\\text{: momentum defined via Hamiltonian}\n\nMetropolis-adjusted Langevin algorithm (MALA) (Besag, 1994): Langevin diffusion \nq(-|X_t):=\\text{ the transition probability of } X_t \\text{ where } dX_t=\\nabla\\log p(X_t)\\,dt+\\sqrt{2\\beta^{-1}}dB_t.\n\n\n\n\n\n\n\n\nReversibility (a.k.a detailed balance): \np(x)q(x|y)=p(y)q(y|x).\n In words: \n\\text{Probability}[\\text{Going}\\;x\\to y]=\\text{Probability}[\\text{Going}\\;y\\to x].\n  Harder to explore the entire space\n Slow mixing of MH\n\n\nFrom the beginning of 21th century, many efforts have been made to make MH irreversible.\n\n\n\n\nLifting: A method to make MH’s dynamics irreversible\nHow?: By adding an auxiliary variable \\sigma\\in\\{\\pm1\\}, called momentum\n\n\n\n\n\n\n\nLifted MH (Turitsyn et al., 2011)\n\n\n\nInput: Target p, two proposals q^{(+1)},q^{(-1)}, and momentum \\sigma\\in\\{\\pm1\\}\n\nDraw X_t from q^{(\\sigma)}\nDo a MH step\nIf accepted, go back to Step 1.\nIf rejected, flip the momentum and go back to Step 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nq^{(+1)}: Only propose \\rightarrow moves\nq^{(-1)}: Only propose \\leftarrow moves\n\n Once going uphill, it continues to go uphill.\n This is irreversible, since\n\\begin{align*}\n  &\\text{Probability}[x\\to y]\\\\\n  &\\qquad\\ne\\text{Probability}[y\\to x].\n\\end{align*}\n\n\n\n\n\n\nReversible dynamic of MH has ‘irreversified’\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nScale is different in the vertical axis!\n\n\n\nLifted MH successfully explores the edges of the target distribution.\n\n\n\n\n\n\n*Irreversibility actually improves the efficiency of MCMC, as we observe in two slides later.\n\n\n\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\nZig-Zag corresponds to the limiting case of lifted MH as the step size of proposal q goes to zero, as we’ll learn later.\n Zig-Zag has a maximum irreversibility.\n\n\n\nIrreversibility actually improves the efficiency of MCMC.\nFaster decay of autocorrelation \\rho_t\\approx\\mathrm{Corr}[X_0,X_t] implies\n\nfaster mixing of MCMC\nlower variance of Monte Carlo estimates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin diffusion: A diffusion process defined by the following SDE:\n\ndX_t=\\nabla\\log p(X_t)\\,dt+\\sqrt{2\\beta^{-1}}dB_t.\n\nLangevin diffusion itself converges to the target distribution p in the sense that 3 \n\\|p_t-p\\|_{L^1}\\to0,\\qquad t\\to\\infty.\n\n\n\n\nTwo MCMC algorithms derived from Langevin diffusion:\n\nULA (Unadjusted Langevin Algorithm) \\quad Use the discretization of (X_t). Discretization errors accumulate.\nMALA (Metropolis Adjusted Langevin Algorithm) \\quad Use ULA as a proposal in MH, erasing the errors by MH steps.\n\n\n\n\nHow fast do they go back to high-probability regions? 4\n\n\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\n\n\nMALA\n\n\n\n\n\nIrreversibility of Zig-Zag accelerates its convergence.\n\n\n\n\n\n\n\n\n\n\n\n\nMALA trajectory\n\n\n\n\n\n\n\n\n\n\nCaution: Fake Continuity\n\n\n\nThe left plot looks continuous, but it actually is not.\n\n\n\n\n\nMH, including MALA, is actually a discrete-time process.\nThe plot is obtained by connecting the points by line segments.\n\n\n\nMonte Carlo estimation is also done differently:\n\n\n\n\n\n\n\n\n\nMALA outputs (X_n)_{n\\in[N]} defines\n\n\\frac{1}{N}\\sum_{n=1}^Nf(X_n)\\xrightarrow{N\\to\\infty}\\int_{\\mathbb{R}^d} f(x)p(x)\\,dx.\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag outputs (X_t)_{t\\in[0,T]} defines\n\n\\int^T_0f(X_t)\\,dt\\xrightarrow{T\\to\\infty}\\int_{\\mathbb{R}^d} f(x)p(x)\\,dx.\n\n\n\n\n\n\n\n\nZig-Zag Sampler’s trajectory is a PDMP.\nPDMP, by design, has maximum irreversibility.\nIrreversibility leads to faster convergence of Zig-Zag in comparisons against MH, Lifted MH, and especially MALA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html#sec-Algorithm",
    "href": "posts/2024/Slides/ZigZagPoliSci.html#sec-Algorithm",
    "title": "Zig-Zag Sampler",
    "section": "2 The Algorithm: How to Use It?",
    "text": "2 The Algorithm: How to Use It?\nFast and exact simulation of continuous trajectory.\n\n2.1 Review: MH vs. LMH vs. Zig-Zag (1/2)\nAs we’ve learned before, Zig-Zag corresponds to the limiting case of lifted MH as the step size of proposal q goes to zero.\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\n\n\n2.2 Review: MH vs. LMH vs. Zig-Zag (2/2)\n‘Limiting case of lifted MH’ means that we only simulate where we should flip the momentum \\sigma\\in\\{\\pm1\\} in Lifted MH.\n\n\n\n\n\n\n\n\n\nMH\n\n\n\n\n\n\n\nLifted MH\n\n\n\n\n\n\n\nZig-Zag\n\n\n\n\n\n\n\n2.3 Algorithm (1/2)\n‘Limiting case of lifted MH’ means that we only simulate where we should flip the momentum \\sigma\\in\\{\\pm1\\} in Lifted MH.\n\n\n\n\n\n\n(1d 5 Zig Zag sampler Bierkens, Fearnhead, et al., 2019)\n\n\n\nInput: Gradient \\nabla\\log p of log target density p\nFor n\\in\\{1,2,\\cdots,N\\}:\n\nSimulate an first arrival time T_n of a Poisson point process (described in the next slide)\nLinearly interpolate until time T_n: \nX_t = X_{T_{n-1}} + \\sigma(t-T_{n-1}),\\qquad t\\in[T_{n-1},T_n].\n\nGo back to Step 1 with the momentum \\sigma\\in\\{\\pm1\\} flipped\n\n\n\n\n\n2.4 Algorithm (2/2)\n\n\n\n\n\n\n(Fundamental Property of Zig-Zag Sampler (1d) Bierkens, Fearnhead, et al., 2019)\n\n\n\nLet U(x):=-\\log p(x). Simluating a Poisson point process with a rate function \n\\lambda(x,\\sigma):=\\biggr(\\sigma U'(x)\\biggl)_++\\;\\gamma(x)\n ensures the Zig-Zag sampler converges to the target p, where \\gamma is an arbitrary non-negative function.\n\n\nIts ergodicity is ensured as long as there exists c,C&gt;0 such that6 \np(x)\\le C\\lvert x\\rvert^{-c}.\n\n\n\n2.5 Core of the Algorithm\nGiven a rate function \n\\lambda(x,\\sigma):=\\biggr(\\sigma U'(x)\\biggl)_++\\;\\gamma(x)\n how to simulate a corresponding Poisson point process?\n\n\n\n\n\n\nWhat We’ll Learn in the Rest of this Section 2\n\n\n\n\nWhat is Poisson Point Process?\nHow to Simulate It?\nCore Technique: Poisson Thinning\n\nTake Away: Zig-Zag sampling reduces to Poisson Thinning.\n\n\n\n\n2.6 Simulating Poisson Point Process (1/2)\n\n\n\n\n\n\nWhat is a Poisson Point Process with rate \\lambda?\n\n\n\nThe number of points in [0,t] follows a Poisson distribution with mean \\int^t_0\\lambda(x_s,\\sigma_s)\\,ds: \nN([0,t])\\sim\\mathrm{Pois}\\left(M(t)\\right),\\qquad M(t):=\\int^t_0\\lambda(x_s,\\sigma_s)\\,ds.\n We want to know when the first point T_1 falls on [0,\\infty).\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen \\displaystyle\\lambda(x,\\sigma)\\equiv c\\;(\\text{constant}),\n\nblue line: Poisson Process\nred dots: Poisson Point Process\n\nsatisfying \\displaystyle\\textcolor{#0096FF}{N_t}=\\textcolor{#E95420}{N([0,t])}\\sim\\mathrm{Pois}(ct).\n\n\n\n\n\n\n2.7 Simulating Poisson Point Process (2/2)\n\n\n\n\n\n\nProposition (Simulation of Poisson Point Process)\n\n\n\nThe first arrival time T_1 of a Poisson Point Process with rate \\lambda can be simulated by \nT_1\\overset{\\text{d}}{=}M^{-1}(E),\\qquad E\\sim\\operatorname{Exp}(1),M(t):=\\int^t_0\\lambda(x_s,\\sigma_s)\\,ds,\n where \\operatorname{Exp}(1) denotes the exponential distribution with parameter 1.\n\n\nSince \\displaystyle\\lambda(x,\\sigma):=\\biggr(\\sigma U'(x)\\biggl)_++\\;\\gamma(x), M can be quite complicated.\n Inverting M can be impossible.\n We need more general techniques: Poisson Thinning.\n\n\n\n2.8 Poisson Thinning (1/2)\n\n\n\n\n\n\n(Lewis and Shedler, 1979)\n\n\n\nTo obtain the first arrival time T_1 of a Poisson Point Process with rate \\lambda,\n\nFind a bound M that satisfies \nm(t):=\\int^t_0\\lambda(x_s,\\sigma_s)\\,ds\\le M(t).\n\nSimulate a point T from the Poisson Point Process with intensity M.\nAccept T with probability \\frac{m(T)}{M(T)}.\n\n\n\n\n\nm(t): Defined via \\displaystyle\\lambda(x,\\sigma):=\\biggr(\\sigma U'(x)\\biggl)_++\\;\\gamma(x).\nM(t): Simple upper bound m\\le M, such that M^{-1} is analytically tractable.\n\n\n\n\n2.9 Poisson Thinning (2/2)\nIn order to simulate a Poisson Point Process with rate \n\\lambda(x,\\sigma):=\\biggr(\\sigma U'(x)\\biggl)_++\\;\\gamma(x),\n we find a invertible upper bound M that satisfies \n\\int^t_0\\lambda(x_s,\\sigma_s)\\,ds=m(t)\\le\\textcolor{#0096FF}{M}(t).\n for all possible Zig-Zag trajectories \\{(x_s,\\sigma_s)\\}_{s\\in[0,T]}.\n\n\n2.10 Recap of Section 2\n\nContinuous-time MCMC, based on PDMP, has an entirely different algorithm and strategy.\nTo simulate PDMP is to simulate Poisson Point Process.\nThe core technology to simulate Poisson Point Process is Poisson Thinning.\nPoisson Thinning is about finding an upper bound M, with tractable inverse M^{-1}; Typically a polynomial function.\nThe upper bound M has to be given on a case-by-case basis."
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html#sec-ProofOfConcept",
    "href": "posts/2024/Slides/ZigZagPoliSci.html#sec-ProofOfConcept",
    "title": "Zig-Zag Sampler",
    "section": "3 Proof of Concept: How Good Is It?",
    "text": "3 Proof of Concept: How Good Is It?\nQuick demonstration of the state-of-the-art performance on a toy example.\n\n3.1 Review: The 3 Steps of Zig-Zag Sampling\nGiven a target p,\n\nCalculate the negative log-likelihood U(x):=-\\log p(x)\nFix a refresh rate \\gamma(x) and compute the rate function \n  \\lambda(x,\\sigma):=\\biggr(\\sigma U'(x)\\biggl)_++\\;\\gamma(x).\n  \nFind an invertible upper bound M that satisfies \n  \\int^t_0\\lambda(x_s,\\sigma_s)\\,ds=:m(t)\\le\\textcolor{#0096FF}{M}(t).\n  \n\n\n\n3.2 Model: 1d Gaussian Mean Reconstruction\n\n\n\n\n\n\n\n\n\n\n\n\nSetting\n\n\n\n\nData: y_1,\\cdots,y_n\\in\\mathbb{R} aquired by \ny_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(x_0,\\sigma^2),\\qquad i\\in[n],\n with \\sigma&gt;0 known, x_0\\in\\mathbb{R} unknown.\nPrior: \\mathrm{N}(0,\\rho^2) with known \\rho&gt;0.\nGoal: Sampling from the posterior \np(x)\\,\\propto\\,\\left(\\prod_{i=1}^n\\phi(x|y_i,\\sigma^2)\\right)\\phi(x|0,\\rho^2),\n where \\phi(x|y,\\sigma^2) is the \\mathrm{N}(y,\\sigma^2) density.\n\n\n\n\n\nThe negative log-likelihood: \\begin{align*}\n  U(x)&=-\\log p(x)\\\\\n  &=\\frac{x^2}{2\\rho^2}+\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x-y_i)^2+\\mathrm{const.},\\\\\n  U'(x)&=\\frac{x}{\\rho^2}+\\frac{1}{\\sigma^2}\\sum_{i=1}^n(x-y_i),\\\\\n  U''(x)&=\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}.\n\\end{align*}\n\n\n\n\n\n3.3 Menu\nIn the rest of this Section 3, we’ll learn:\n\nEven a simple Zig-Zag Sampler with \\gamma\\equiv0 surpasses MALA.\nIncorporating sub-sampling, Zig-Zag with Control Variates further improves the efficiency.\n\n\n\n\n\n\n\n\n3.4 Simple Zig-Zag Sampler with \\gamma\\equiv0 (1/2)\nFixing \\gamma\\equiv0, we obtain the upper bound M \\begin{align*}\n  m(t)&=\\int^t_0\\lambda(x_s,\\sigma_s)\\,ds=\\int^t_0\\biggr(\\sigma U'(x_s)\\biggl)_+\\,ds\\\\\n  &\\le\\left(\\frac{\\sigma x}{\\rho^2}+\\frac{\\sigma}{\\sigma^2}\\sum_{i=1}^n(x-y_i)+t\\left(\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}\\right)\\right)_+\\\\\n  &=:(a+bt)_+=\\textcolor{#0096FF}{M}(t),\n\\end{align*}\nwhere \na=\\frac{\\sigma x}{\\rho^2}+\\frac{\\sigma}{\\sigma^2}\\sum_{i=1}^n(x-y_i),\\quad b=\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}.\n\n\n\n3.5 Result: 1d Gaussian Mean Reconstruction\nWe generated 100 samples from \\mathrm{N}(x_0,\\sigma^2) with x_0=1.\n\n\n\n\n\n\n\n3.6 MSE per Epoch: The Vertical Axis\nMSE (Mean Squared Error) of \\{X_i\\}_{i=1}^n is defined as \n\\frac{1}{n}\\sum_{i=1}^n(X_i-x_0)^2.\n Epoch: Unit computational cost.\n\n\n\n\n\n\nThe following is considered as one epoch:\n\n\n\n\nOne evaluation of a likelihood ratio \n\\frac{p(X_{n+1})}{p(X_n)}.\n\nOne evaluation of a Poisson Point Process.\n\n\n\n\n\n3.7 Good News!\nCase-by-case construction of an upper bound M is too complicated / demanding.\nTherefore, we are trying to automate the whole procedure.\n\n\n\n\n\n\nAutomatic Zig-Zag\n\n\n\n\nAutomatic Zig-Zag (Corbella et al., 2022)\nConcave-Convex PDMP (Sutton and Fearnhead, 2023)\nNuZZ (numerical Zig-Zag) (Pagani et al., 2024)"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html#references",
    "href": "posts/2024/Slides/ZigZagPoliSci.html#references",
    "title": "Zig-Zag Sampler",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\n\nSlides and codes are available here\n\n\n\n\n\n\nBesag, J. E. (1994). Comments on “Representations of Knowledge in Complex Systems” by U. Grenander and M. I. Miller. Journal of the Royal Statistical Society. Series B (Methodological), 56(4), 591–592.\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nBierkens, J., Grazzi, S., Kamatani, K., and Roberts, G. O. (2020). The boomerang sampler. Proceedings of the 37th International Conference on Machine Learning, 119, 908–918.\n\n\nBierkens, J., Roberts, G. O., and Zitt, P.-A. (2019). Ergodicity of the zigzag process. The Annals of Applied Probability, 29(4), 2266–2301.\n\n\nBouchard-Côté, A., Vollmer, S. J., and Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. Journal of the American Statistical Association, 113(522), 855–867.\n\n\nCorbella, A., Spencer, S. E. F., and Roberts, G. O. (2022). Automatic Zig-Zag Sampling in Practice. Statistics and Computing, 32(6), 107.\n\n\nDai, H., Pollock, M., and Roberts, G. (2019). Monte Carlo Fusion. Journal of Applied Probability, 56(1), 174–191.\n\n\nDavis, M. H. A. (1984). Piecewise-deterministic markov processes: A general class of non-diffusion stochastic models. Journal of the Royal Statistical Society. Series B (Methodological), 46(3), 353–388.\n\n\nDavis, M. H. A. (1993). Markov models and optimization,Vol. 49. Chapman & Hall.\n\n\nDuane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987). Hybrid monte carlo. Physics Letters B, 195(2), 216–222.\n\n\nFearnhead, P., Grazzi, S., Nemeth, C., and Roberts, G. O. (2024). Stochastic gradient piecewise deterministic monte carlo samplers.\n\n\nGrazzi, S. (2020). Piecewise deterministic monte carlo.\n\n\nHastings, W. K. (1970). Monte carlo sampling methods using markov chains and their applications. Biometrika, 57(1), 97–109.\n\n\nLewis, P. A. W., and Shedler, G. S. (1979). Simulation of nonhomogeneous poisson processes by thinning. Naval Research Logistics Quarterly, 26(3), 403–413.\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953). Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6), 1087–1092.\n\n\nPagani, F., Chevallier, A., Power, S., House, T., and Cotter, S. (2024). NuZZ: Numerical zig-zag for general models. Statistics and Computing, 34(1), 61.\n\n\nPeters, E. A. J. F., and de With, G. (2012). Rejection-free monte carlo sampling for general potentials. Physical Review E, 85(2).\n\n\nScott, S. L., Blocker, A. W., Bonassi, F. V., Chipman, H. A., George, E. I., and McCulloch, R. E. (2016). Bayes and big data: The consensus monte carlo algorithm. International Journal of Management Science and Engineering Management, 11, 78–88.\n\n\nSrivastava, S., Cevher, V., Dinh, Q., and Dunson, D. (2015). WASP: Scalable Bayes via barycenters of subset posteriors. In G. Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the eighteenth international conference on artificial intelligence and statistics,Vol. 38, pages 912–920. San Diego, California, USA: PMLR.\n\n\nSutton, M., and Fearnhead, P. (2023). Concave-Convex PDMP-based Sampling. Journal of Computational and Graphical Statistics, 32(4), 1425–1435.\n\n\nTuritsyn, K. S., Chertkov, M., and Vucelja, M. (2011). Irreversible Monte Carlo algorithms for Efficient Sampling. Physica D-Nonlinear Phenomena, 240(5-Apr), 410–414.\n\n\nWelling, M., and Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on international conference on machine learning, pages 681–688. Madison, WI, USA: Omnipress."
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html#appendix-scalability-by-subsampling",
    "href": "posts/2024/Slides/ZigZagPoliSci.html#appendix-scalability-by-subsampling",
    "title": "Zig-Zag Sampler",
    "section": "Appendix: Scalability by Subsampling",
    "text": "Appendix: Scalability by Subsampling\nConstruction of ZZ-CV (Zig-Zag with Control Variates).\n\n3.8 Review: 1d Gaussian Mean Reconstruction\nU' has an alternative form:\n\\begin{align*}\n  U'(x)&=\\frac{x}{\\rho^2}+\\frac{1}{\\sigma^2}\\sum_{i=1}^n(x-y_i)=:\\frac{1}{n}\\sum_{i=1}^nU'_i(x),\n\\end{align*} where \nU'_i(x)=\\frac{x}{\\rho^2}+\\frac{n}{\\sigma^2}(x-y_i).\n\n We only need one sample y_i to evaluate U'_i.\n\n\n3.9 Randomized Rate Function\n\nInstead of \n\\lambda_{\\textcolor{#78C2AD}{\\text{ZZ}}}(x,\\sigma)=\\biggr(\\sigma U'(x)\\biggl)_+\n we use \n\\lambda_{\\textcolor{#E95420}{\\text{ZZ-CV}}}(x,\\sigma)=\\biggr(\\sigma U'_I(x)\\biggl)_+,\\qquad I\\sim\\mathrm{U}([n]).\n Then, the latter is an unbiased estimator of the former: \n\\operatorname{E}_{I\\sim\\mathrm{U}([n])}\\biggl[\\lambda_{\\textcolor{#E95420}{\\text{ZZ-CV}}}(x,\\sigma)\\biggr]=\\lambda_{\\textcolor{#78C2AD}{\\text{ZZ}}}(x,\\sigma).\n\n\n\n\n3.10 Last Step: Poisson Thinning\nFind an invertible upper bound M that satisfies \n\\int^t_0\\lambda_{\\textcolor{#E95420}{\\text{ZZ-CV}}}(x_s,\\sigma_s)\\,ds=:m_I(t)\\le\\textcolor{#0096FF}{M}(t),\\qquad I\\sim\\mathrm{U}([n]).\n It is harder to bound \\lambda_{\\textcolor{#E95420}{\\text{ZZ-CV}}}, since it is now an estimator (random function).\n\n\n3.11 Upper Bound M with Control Variates\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing (once and for all)\n\n\n\n\nFind \n  x_*:=\\operatorname*{argmin}_{x\\in\\mathbb{R}}U(x)\n  \nCompute \n  U'(x_*)=\\frac{x_*}{\\rho^2}+\\frac{1}{\\sigma^2}\\sum_{i=1}^n(x_*-y_i).\n  \n\n\n\n\n\nThen, with a re-parameterization of m_i, \nm_i(t)\\le M(t):=a+bt,\n\n\n\n\nwhere \na=(\\sigma U'(x_*))_++\\|U'\\|_\\mathrm{Lip}\\|x-x_*\\|_p,\\qquad b:=\\|U'\\|_\\mathrm{Lip}.\n And m_i is redefined as \nm_i(t)=U'(x_*)+U'_i(x)-U'_i(x_*).\n\n\n\n3.12 Subsampling with Control Variates\nZig-Zag sampler with the random rate function \n\\lambda_{\\textcolor{#E95420}{\\text{ZZ-CV}}}(x,\\sigma)=\\biggr(\\sigma U'_I(x)\\biggl)_+,\\qquad I\\sim\\mathrm{U}([n]).\n and the upper bound \nM(t)=a+bt\n is called Zig-Zag with Control Variates (Bierkens, Fearnhead, et al., 2019).\n\n\n3.13 Zig-Zag with Control Variates\n\nhas O(1) efficiency as the sample size n grows.7\nis exact (no bias).\n\n\n\n\n\n\n\n\n3.14 Scalability (1/3)\nThere are currently two main approaches to scaling up MCMC for large data.\n\nDevide-and-conquer\nDevide the data into smaller chunks and run MCMC on each chunk.\nSubsampling\nUse a subsampling estimate of the likelihood, which does not require the entire data.\n\n\n\n3.15 Scalability (2/3) by Devide-and-conquer\nDevide the data into smaller chunks and run MCMC on each chunk.\n\n\n\n\n\n\n\n\n\nUnbiased?\nMethod\nReference\n\n\n\n\n\nWASP\n(Srivastava et al., 2015)\n\n\n\nConsensus Monte Carlo\n(Scott et al., 2016)\n\n\n\nMonte Carlo Fusion\n(Dai et al., 2019)\n\n\n\n\n\n\n3.16 Scalability (3/3) by Subsampling\nUse a subsampling estimate of the likelihood, which does not require the entire data.\n\n\n\n\n\n\n\n\n\nUnbiased?\nMethod\nReference\n\n\n\n\n\nStochastic Gadient MCMC\n(Welling and Teh, 2011)\n\n\n\nZig-Zag with Subsampling\n(Bierkens, Fearnhead, et al., 2019)\n\n\n\nStochastic Gradient PDMP\n(Fearnhead et al., 2024)"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagPoliSci.html#footnotes",
    "href": "posts/2024/Slides/ZigZagPoliSci.html#footnotes",
    "title": "Zig-Zag Sampler",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMostly deterministic with the exception of random jumps happens at random times↩︎\nContinuous-time, instead of discrete-time processes↩︎\nunder fairly general conditions on p.↩︎\nThe target here is the standard Cauchy distribution \\mathrm{C}(0,1), equivalent to \\mathrm{t}(1) distribution. Its heavy tails hinder the convergence of MCMC.↩︎\nMultidimensional extension is straightforward, but we won’t cover it today.↩︎\nWith some regularity conditions on U. (See Bierkens, Roberts, et al., 2019).↩︎\nAs long as the preprocessing step is properly done.↩︎"
  },
  {
    "objectID": "posts/2024/Computation/brms.html",
    "href": "posts/2024/Computation/brms.html",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brms-リンク集",
    "href": "posts/2024/Computation/brms.html#brms-リンク集",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "brms リンク集",
    "text": "brms リンク集\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nbrms: Bayesian Regression Models using ‘Stan’ リンク集\n\n\n\n\nr-project\nDocumentation\nGitHub\ndiscourse\n(Bürkner, 2017), (Bürkner, 2018), (Bürkner, 2021)\n\n\n\nダウンロードは：\ninstall.packages(\"brms\")"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#カウントデータで学ぶ-brms-の使い方",
    "href": "posts/2024/Computation/brms.html#カウントデータで学ぶ-brms-の使い方",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "1 カウントデータで学ぶ brms の使い方",
    "text": "1 カウントデータで学ぶ brms の使い方\n\n1.1 モデルの概要\nDocumentation で紹介されている Epilepsy Seizures Data (Leppik et al., 1987)，(Thall and Vail, 1990) を用いた 例 を実行してみる：\n\nlibrary(brms)\nformula1 &lt;- bf(count ~ zAge + zBase * Trt + (1|patient))\nfit1 &lt;- brm(formula = formula1, data = epilepsy, family = poisson())\n\n\n1.1.1 formula について\nてんかん (epilepsy) 患者の発作回数 count を被説明変数とし，処置の有無を表す説明変数 Trt と患者毎のランダム誤差を表す切片項 (1|patient)，及び標準化された説明変数 zAge, zBase への依存構造を調べたい．\n\n\n\n\n\n\n説明変数\n\n\n\n\nzAge：標準化された年齢\nzBase：ベースの発作回数\nTrt：治療の有無を表す２値変数\n(1|patient)：患者ごとに異なるとした変動切片項\n\nzBase * Trtという記法は，この２つの交互作用もモデルに含めることを意味する．この項の追加により，モデルは zBase の違いに応じて Trt の効果が変わる度合い \\(\\beta_4\\) を取り入れることができる．\n\n\nこのような処置効果 \\(\\beta_3\\) を調べるモデルでは，回帰係数を（因果）効果 (effect) とも呼ぶことに注意．\n\nkable(head(epilepsy))\n\n\n\n\nAge\nBase\nTrt\npatient\nvisit\ncount\nobs\nzAge\nzBase\n\n\n\n\n31\n11\n0\n1\n1\n5\n1\n0.4249950\n-0.7571728\n\n\n30\n11\n0\n2\n1\n3\n2\n0.2652835\n-0.7571728\n\n\n25\n6\n0\n3\n1\n2\n3\n-0.5332740\n-0.9444033\n\n\n36\n8\n0\n4\n1\n4\n4\n1.2235525\n-0.8695111\n\n\n22\n66\n0\n5\n1\n7\n5\n-1.0124085\n1.3023626\n\n\n29\n27\n0\n6\n1\n5\n6\n0.1055720\n-0.1580352\n\n\n\n\n\n\n\n\n\n\n\nデータの詳細\n\n\n\nepilepsyは59 人の患者に関して，４回の入院時の発作回数を記録した，全 236 データからなる．patientが患者を識別する ID であり，(1|patient)は患者ごとのランダム効果ということになる．\n\n\n\n\n1.1.2 family=poisson() について\n被説明変数 count は離散変数であり，Poisson 分布に従うと仮定する．過分散への対応を次の段階で考慮する．\n従って本モデルはzAge, zBase, Trt, Trt*zBaseという固定効果（係数），(1|patient)というランダム効果を取り入れた（一般化線型）混合効果モデル である．回帰式は次の通り： \\[\ny_{it} = \\beta_1 \\cdot\\texttt{zAge}_i+ \\beta_2 \\cdot \\texttt{zBase}_i + \\beta_3 \\cdot \\texttt{Trt}_i\n\\] \\[\n+ \\beta_4 \\cdot (\\texttt{zBase}_i \\cdot \\texttt{Trt}_i) + \\alpha_i +\\epsilon_{it}.\n\\] ただし，\\(\\texttt{count}_{it}\\) の Poisson 母数を \\(\\lambda_{it}\\) として，\\(y_{it}:=\\log(\\lambda_{it})\\) とした．\nfamily=poisson() は次の略記である：\nfamily = brmsfamily(family = \"&lt;family&gt;\", link = \"&lt;link&gt;\")\n多くの場合 link 引数は省略可能である．この２つの情報を通じて，一般化線型モデルを取り扱うことができる．\n\n\n\n1.2 モデルの推定と結果の解釈\nbrm() 関数による推定結果は，返り値として渡される brmsfit オブジェクトに対して summary() メソッドを適用して見ることができる：\n\nsummary(fit1)\n\n Family: poisson \n  Links: mu = log \nFormula: count ~ zAge + zBase * Trt + (1 | patient) \n   Data: epilepsy (Number of observations: 236) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~patient (Number of levels: 59) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.58      0.07     0.46     0.75 1.00     1016     1779\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.78      0.12     1.54     2.02 1.01      959     1321\nzAge           0.09      0.09    -0.08     0.26 1.01      765     1399\nzBase          0.70      0.12     0.47     0.93 1.01      813     1369\nTrt1          -0.27      0.17    -0.61     0.06 1.00      817     1205\nzBase:Trt1     0.05      0.16    -0.26     0.36 1.01      819     1692\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n基本的な解析の前提がまず出力され，推定結果はグループレベル変数（今回は患者ごとの変量効果 \\(\\alpha_i\\)，コードだと (1|patient)）から表示される．\n後半に固定効果の係数，特にユニットレベルの回帰係数と切片項の推定結果が表示される．\n結果の解釈をしてみる．治療効果Trtの係数は負で，平均的に処置効果はある可能性があるが，95% 信頼区間は \\(0\\) を跨いでいるという意味では，有意とは言えない．\nまた交差項zBase*Trtの係数は小さく，交互効果の存在を示す証拠はないと思われる．\n\\(\\widehat{R}\\) (Gelman and Rubin, 1992) とは MCMC の収束に関する指標で，１より大きい場合，MCMC が収束していない可能性を意味する (Vehtari et al., 2021)．通説には \\(\\widehat{R}\\le1.1\\) などの基準がある．\n\n\n1.3 プロット\n変数を指定して，事後分布と MCMC の軌跡をプロットできる：\n\nplot(fit1, variable = c(\"b_Trt1\", \"b_zBase\", \"b_zBase:Trt1\"))\n\n\n\n\n\n\n\n\nより詳しく見るにはconditional_effects関数を用いることもできる．\n\nconditional_effects(fit1, effects = \"zBase:Trt\")\n\n\n\n\n\n\n\n図 1: 交差項の効果（ベースレートの違いによる処置効果の違い）\n\n\n\n\n\n処置群 Trt=1 の方がカウントは減っているのは見えるが，モデルの不確実性に比べてその減少量は十分に大きいとは言えない．\nベースレート zBase が大きいほどカウントは大きい．しかしベースレートが大きいほど処置効果も大きい（交互効果がある）ようには見えない．\n\n\n1.4 モデルによる予測\nfit したモデル fit1 を用いて，平均年齢と平均ベースレートを持つ患者に対する治療効果を予測する：\n\nnewdata &lt;- data.frame(Trt = c(0, 1), zAge = 0, zBase = 0)\npredict(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error Q2.5  Q97.5\n[1,]   5.9710  2.530960    2 11.025\n[2,]   4.5425  2.178275    1  9.000\n\n\n関数predict() は事後予測分布からのサンプリングを１回行う．一方で，関数fitted() は事後予測分布の平均を返す．1\n\nfitted(fit1, newdata = newdata, re_formula = NA)\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 5.953824  0.712907 4.682468 7.518386\n[2,] 4.525703  0.545437 3.518021 5.614800\n\n\n\n\n1.5 モデルの比較\nモデルfit1で行った Poisson 回帰分析は，fit1に含めた説明変数の違いを除けば，個々の観測が独立になる，という仮定の上に成り立っている（第 4.3 節）．\nこの仮定が破れているとき＝全ての説明変数をモデルに含めきれていないとき，Poisson 分布の性質 \\[\n\\operatorname{E}[X]=\\mathrm{V}[X]=\\lambda\\qquad (X\\sim\\mathrm{Pois}(\\lambda))\n\\] からの離反として現れ，この現象は 過分散（overdispersion）とも呼ばれる．\n\n1.5.1 観測レベルランダム効果\nということで，他の未観測の説明変数が存在した場合を想定して， Poisson 分布族ではなく，分散が平均よりも大きいような別の分布族を用いて，フィット度合いを比較してみることを考えたい．\nそこで追加の変動をモデルに追加するべく，モデルfit1に観測ごとの切片項 \\(\\eta_{it}\\) を追加してみる（この手法は観測レベルランダム効果と呼ばれる．第 3.3 節参照）．\n\nfit2 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient) + (1|obs),\n            data = epilepsy, family = poisson())\n\nこうして得た２つのモデル fit1,fit2 を比較する．\nLLO (Leave-One-Out) 交差検証 (cross-validation) が関数 loo によって実行できる：\nloo(fit1, fit2)\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit1, fit2)\n\nWarning: Found 8 observations with a pareto_k &gt; 0.7 in model 'fit1'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 68 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit1':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -672.1 37.0\np_loo        94.9 14.9\nlooic      1344.2 74.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.2]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     228   96.6%   140     \n   (0.7, 1]   (bad)        7    3.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   1    0.4%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -596.2 13.9\np_loo       108.4  7.1\nlooic      1192.4 27.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.7]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     168   71.2%   201     \n   (0.7, 1]   (bad)       62   26.3%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   6    2.5%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2   0.0       0.0  \nfit1 -75.9      27.6  \n\n\n\n\n\nelpd_diff は expected log posterior density の差異を表す．fit2 の方が大きく当てはまりが良いことが見て取れる．\nまた WAIC (Watanabe-Akaike Information Criterion) も実装されている：\nprint(waic(fit1))\n\n\n\n\n\n\nWAIC の結果\n\n\n\n\n\n\nprint(waic(fit1))\n\nWarning: \n51 (21.6%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -669.0 36.9\np_waic        91.8 14.8\nwaic        1338.0 73.8\n\n51 (21.6%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\nprint(waic(fit2))\n\nWarning: \n68 (28.8%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -573.0 12.0\np_waic        85.3  5.1\nwaic        1146.0 24.0\n\n68 (28.8%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\n\n\n\n他にも，reloo, kfold などの関数もある．\n\n\n\n\n\n\n他の関数一覧\n\n\n\n\n\n\nmethods(class=\"brmsfit\")\n\n [1] add_criterion           add_ic                  as_draws_array         \n [4] as_draws_df             as_draws_list           as_draws_matrix        \n [7] as_draws_rvars          as_draws                as.array               \n[10] as.data.frame           as.matrix               as.mcmc                \n[13] autocor                 bayes_factor            bayes_R2               \n[16] bridge_sampler          coef                    conditional_effects    \n[19] conditional_smooths     control_params          default_prior          \n[22] expose_functions        family                  fitted                 \n[25] fixef                   formula                 getCall                \n[28] hypothesis              kfold                   log_lik                \n[31] log_posterior           logLik                  loo_compare            \n[34] loo_linpred             loo_model_weights       loo_moment_match       \n[37] loo_predict             loo_predictive_interval loo_R2                 \n[40] loo_subsample           loo                     LOO                    \n[43] marginal_effects        marginal_smooths        mcmc_plot              \n[46] model_weights           model.frame             nchains                \n[49] ndraws                  neff_ratio              ngrps                  \n[52] niterations             nobs                    nsamples               \n[55] nuts_params             nvariables              pairs                  \n[58] parnames                plot                    post_prob              \n[61] posterior_average       posterior_epred         posterior_interval     \n[64] posterior_linpred       posterior_predict       posterior_samples      \n[67] posterior_smooths       posterior_summary       pp_average             \n[70] pp_check                pp_mixture              predict                \n[73] predictive_error        predictive_interval     prepare_predictions    \n[76] print                   prior_draws             prior_summary          \n[79] psis                    ranef                   reloo                  \n[82] residuals               restructure             rhat                   \n[85] stancode                standata                stanplot               \n[88] summary                 update                  VarCorr                \n[91] variables               vcov                    waic                   \n[94] WAIC                   \nsee '?methods' for accessing help and source code\n\n\n\n\n\n\n\n1.5.2 患者内の相関構造のモデリング\nまた fit1 において，同一患者の異なる訪問の間には全く相関がないと仮定されており，これは全く非現実的な仮定をおいてしまっていると言える．2\n患者内の相関構造は，brm() 関数の autocor 引数で指定できる（第 4.3.2 節）．\n例えば，全く構造を仮定しない場合は unstrを指定する：\n\nfit3 &lt;- brm(count ~ zAge + zBase * Trt + (1|patient),\n            autocor = ~unstr(time=visit, gr=patient),\n            data = epilepsy, family = poisson())\n\nこのモデルも fit1 より遥かに当てはまりが良く，fit2 とほとんど同じ当てはまりの良さが見られる：\n\n\n\n\n\n\nLOO-CV の結果\n\n\n\n\n\n\nloo(fit2,fit3)\n\nWarning: Found 68 observations with a pareto_k &gt; 0.7 in model 'fit2'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nWarning: Found 57 observations with a pareto_k &gt; 0.7 in model 'fit3'. We\nrecommend to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\n\nOutput of model 'fit2':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -596.2 13.9\np_loo       108.4  7.1\nlooic      1192.4 27.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.7]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     168   71.2%   201     \n   (0.7, 1]   (bad)       62   26.3%   &lt;NA&gt;    \n   (1, Inf)   (very bad)   6    2.5%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nOutput of model 'fit3':\n\nComputed from 4000 by 236 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -600.8 15.0\np_loo       112.1  8.2\nlooic      1201.6 29.9\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.5]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     179   75.8%   196     \n   (0.7, 1]   (bad)       47   19.9%   &lt;NA&gt;    \n   (1, Inf)   (very bad)  10    4.2%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\nModel comparisons:\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit3 -4.6       2.9   \n\n\n\n\n\n思ったよりも fit2 の当てはまりが良いため，Poisson-対数正規混合モデリングを本格的に実施してみることが次の選択肢になり得る（第 3.3 節参照）．\n\n\n\n1.6 その他の例\nSebastian Weber らにより，新薬の治験における実際の解析事例をまとめたウェブサイト が公開されている．3\n特にその 13 章 では同様の経時的繰り返し観測データを扱っているが，ここではカウントデータではなく連続な応用変数が扱われている．\nbrms は他にもスプラインや Gauss 過程を用いた一般化加法モデルの推論も可能である (Bürkner, 2018)．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "href": "posts/2024/Computation/brms.html#ランダム効果モデルの正しい使い方",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "2 ランダム効果モデルの正しい使い方",
    "text": "2 ランダム効果モデルの正しい使い方\n\n\n\n\n\n\n概要\n\n\n\n\nランダム効果モデル とは，グループ毎に異なる切片項 \\(\\alpha_{s[i]}\\) を追加し，これにも誤差を仮定してモデルに入れて得る階層モデルである（狭義の用法）．この意味では変動切片モデルともいう．\nしかしランダム効果 \\(\\alpha_{s[i]}\\) が（ユニットレベルの）説明変数 \\(x_i\\) と相関を持つ場合，推定量の一致性が失われる．これを回避するために，\\(x_i\\) の係数 \\(\\beta\\) にのみ関心がある場合は固定効果推定量や一般化推定方程式 (GEE) が用いられることも多い．\nだが，ランダム効果モデルにおいても未観測の説明変数が存在する場合でも，簡単なトリック（ランダム効果 \\(\\alpha_{s[i]}\\) のグループレベル説明変数に \\(\\overline{x}_s\\) を追加すること）で，推定量の一致性を回復することができる．\nこのトリックを取り入れたランダム効果モデルは，\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がない場合は固定効果モデルと等価な \\(\\beta\\) の推定量を与え，相関がある場合でも \\(\\beta\\) を一致推定し，各変動切片項 \\(\\alpha_{s[i]}\\) の構造にも洞察を与えてくれる．さらには内生性の度合いを推定するといった構造的な洞察も可能である (Chan and Tobias, 2020, p. 14)．\n以上のランダム効果モデルや固定効果モデルは，異なるモデルに対するベイズ推定と見れる．どのモデルを採用するのが良いか，実経計画の時点からは判らない場合，ベイズ階層モデルの方法で探索的に実行することも可能である．\n\n\n\nランダム効果モデルは「通常の固定効果のみを含んだ線型回帰モデルにランダム項を導入したもの」という意味でも用いられる．この場合は 線型混合モデル (Linear Mixed Model; LMM) の別名と理解できる．\n\n2.1 ランダム効果モデル\nランダム効果 は，変動する切片項 (Gelman, 2005) (Bafumi and Gelman, 2007) という別名も提案されているように，サブグループ毎に異なる切片項のことである．4\nユニット（個人などの最小単位）レベルの回帰式を書き下すと，グループ選択関数 \\(s:[n]\\to[S]\\;(S\\le n)\\) を通じて， \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\n\\tag{1}\\] というようになる．\nこれは，確率変数 \\(\\alpha_{s[i]}\\) の平均を \\(\\alpha_0\\) とすると，グループレベルの回帰式 \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad s\\in[S]\n\\tag{2}\\] が背後にある 階層モデル (multilevel / hierarchical model) だとみなすこともできる．\n\n\n2.2 説明変数との相関の問題\n\n2.2.1 問題の所在\nランダム効果では，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s}\\) が相関を持たないという仮定が Gauss-Markov の定理の仮定に等価になるため，これが違反されると \\(\\beta\\) の OLS 推定量の不偏性・一致性が約束されず，推定量の分散も大きくなる．5\n\n\n\n\n\n\nGauss-Markov の仮定\n\n\n\nユニットレベル回帰式 \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] において，ユニットレベルの説明変数 \\(x_i\\) と変動切片項 \\(\\alpha_{s[i]}\\) が相関を持たないこと．\n\n\n実際，ランダム効果モデルの階層構造を，(2) を (1) に代入することで一つの式にまとめると \\[\ny_i=\\alpha_0+\\beta x_i+\\underbrace{\\epsilon_i'}_{\\epsilon_i+\\eta_{s[i]}}\n\\tag{3}\\] を得る．\n\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) に相関がある場合，\\(x_i\\) と \\(\\eta_s\\) にも相関があるため，結果として (3) では説明変数と誤差 \\(\\epsilon_i'\\) に相関が生じてしまう．これは計量経済学では 内生性 (endogeneity) の問題と呼ばれているものに他ならない．\n\n\n2.2.2 代替モデル１：母数効果モデル\nそのため，ランダム効果モデルは避けられる傾向にあり，切片項 \\(\\alpha_{s[i]}\\equiv\\alpha_0\\) は変動しないとし，グループレベルの効果を無視してモデリングすることも多い： \\[\ny_i=\\alpha_0+\\beta x_i+\\epsilon_i.\n\\] このことを 完全プーリングモデル (complete pooling model) または母数効果モデルと呼び，ランダム効果モデルを 部分プーリングモデル (partial pooling model) と呼んで対比させることがある．6\n周辺モデル (marginal model) や 母平均モデル (population-average model) とも呼ばれる (Gardiner et al., 2009, p. 228)．\n実際，これ以上の仮定を置かず，ランダム効果は局外母数として（母数効果ともいう）一般化推定方程式の方法（第 2.6 節）によれば，\\(\\beta\\) の不偏推定が可能である．\nリンク関数 \\(g\\) を通じた非線型モデル \\[\ng(\\operatorname{E}[y_i|x_i])=\\beta x_i\n\\] であっても，指数型分布族を仮定すれば（すなわち一般化線型モデルについては），\\(\\beta\\) の一致推定が可能である．\nだが，切片項の変動を消してしまうことで，回帰係数 \\(\\beta\\) の推定に対する縮小効果（第 3.2 節）が得られないという欠点もあり，小地域推定などにおいては \\(\\alpha_{s[i]}\\) を確率変数とみなす積極的理由もある．この点については (久保川達也, 2006), (Sugasawa and Kubokawa, 2023) も参照．\n\n\n2.2.3 代替モデル２：固定効果モデル\n問題を起こさずに，しかしながらグループレベルの効果をモデリングしたい場合， \\[\ny_i=\\alpha_{s[i]}^{\\text{unmodeled}}+\\beta x_i+\\epsilon_i\n\\] として，グループ毎に変動する切片項 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を許すが，この変数自体にモデルは仮定しない，とすることもできる．\nしたがってグループ毎に別々の回帰分析を実行し，別々の切片 \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) を得て，\\(\\beta\\) の値はこれらのグループの間で適切に重みづけて最終的な推定値としているに等しい．\nすなわち，グループの数だけ，グループへの所属を表す２値変数 \\(1_{\\left\\{s[i]=s\\right\\}}\\) を導入し，\\(S\\) 個の項 \\(\\sum_{s=1}^S1_{\\left\\{s[i]=s\\right\\}}\\alpha_{s[i]}^{\\text{unmodeled}}\\) を説明変数に加えて回帰分析を行うことに等しい．\nまた群内平均を引いた値 \\(y_i-\\overline{y}_{s[i]}\\) を目的変数として，説明変数 \\(x_i-\\overline{x}_{s[i]}\\) により回帰分析を行うこととも等価である．この変換により \\(\\alpha_{s[i]}^{\\text{unmodeled}}\\) が消去されると考えられるのである．\n\n\n\n\n\n\n固定効果モデルの別名\n\n\n\n\n(Hansen, 2022) をはじめ，計量経済学では fixed effects model と呼ばれる．7\n(Bafumi and Gelman, 2007) は unmodeled varying intercept と呼んでいる．\nleast squares dummy variable regression とも呼べる．8\n\n\n\n\n\n2.2.4 固定効果 vs. 変量効果\n\n\n\n\n\n\n固定効果モデルの利点\n\n\n\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持ち得る場合も，固定効果モデルでは問題が生じない．9\n\n\n\n\n\n\n\n\n変量効果モデルの利点\n\n\n\n固定効果モデルでは異なるグループのデータが相互作用する機構がランダム効果モデルに比べて貧しい．\n一方でランダム効果モデルを用いた場合，外れ値グループが存在するなどノイズの大きなデータに対しても，\\(\\eta_s\\) を通じて緩やかに情報が伝達され，\\(\\beta\\) の値は平均へ縮小されて推定される（第 3.2 節）．これは Stein 効果とも呼ばれる (Hoff, 2009, p. 146)．固定効果モデルではそのような頑健性を持たない (Bafumi and Gelman, 2007, pp. 4–5)．\nさらには変量効果モデルにおいては \\(\\epsilon_i\\) と \\(\\eta_s\\) の相関を推定することができ，これは内生性の強さの尺度として使える (Chan and Tobias, 2020, p. 14)．\n\n\n固定効果モデルは \\(\\beta\\) （のみ）に関心がある場合，\\(\\alpha_{s[i]}\\) と \\(x_i\\) の相関の存在に対してロバストな推定法として有用であり，その理由で計量経済学（特に線型パネルデータ）では主流の推定手法となっている．10\n実際，\\(\\alpha_{s[i]}\\) と \\(x_i\\) が無相関であるとき，変量効果モデルと固定効果モデルは \\(\\beta\\) に関しては等価な推定量を与える．\n\nCurrent econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estmimator for linear panel data models. (Hansen, 2022, p. 624)\n\n逆に言えば，固定効果モデルは \\(x_i\\) と \\(\\alpha_{s[i]}\\) の構造のモデリングを放棄したモデリング法であり，各 \\(\\alpha_{s[i]}\\) の値にも興味がある場合，または \\(\\beta\\) のより精度の高い推定が実行したい場合には，やはり \\(\\alpha_{s[i]}\\) の誤差と相関構造もモデルに取り入れたランダム効果モデルを用いたいということになる．\n\n\n\n2.3 階層モデルによる総合\nランダム効果モデルは， \\[\n\\alpha_s=\\alpha_0+\\eta_s,\\qquad \\eta_s\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,\\sigma^2),\n\\] というグループレベルの回帰モデルの想定された階層モデルと見れるのであった（第 2.1 節）．\nすると完全プーリングモデル（第 2.2.2 節）は \\(\\sigma^2\\to0\\) の場合，固定効果モデル（第 2.2.3 節）は \\(\\sigma^2\\to\\infty\\) の場合の，特定の点推定法と見れる．\n換言すれば，improper な一様事前分布 \\[\n\\alpha_s^{\\text{unmodeled}}\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(\\alpha_0,\\infty)\n\\] を仮定した場合が固定効果モデルであると理解される (Gelman et al., 2014, p. 383)．\nこの点については次稿も参照：\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.4 ランダム効果モデルにおける相関のモデリング\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) が相関を持つ場合に，効果 \\(\\beta\\) の OLS 推定量の一致性が保証されないことがランダム効果モデルの欠陥だと述べたが，実はこれは簡単な方法で解決できる．\n\\(x_i\\) と \\(\\alpha_{s[i]}\\) との相関は，欠落変数（未観測の交絡因子）が存在するため，と考えることができる．\nランダム効果モデリングではこの欠落変数に対する操作変数を人工的に作り出すことができる．\nというのも，説明変数の平均 \\(\\overline{x}_s\\) を変動する切片項 \\(\\alpha_s\\) の説明変数として追加することで除去できる：11\n\\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i\n\\] \\[\n\\alpha_s=\\alpha_0+\\alpha_1\\overline{x}_s+\\eta_s\n\\tag{4}\\]\nこれにより，Gauss-Markov の仮定（外生性）が回復される．\n(Bafumi and Gelman, 2007, pp. 7–9) ではこの効果をシミュレーションによって検証している．\n\nPractitioners can get around this problem by taking advantage of the multilevel structure of their regression equation. (Bafumi and Gelman, 2007, p. 12)\n\n\n\n2.5 第三の名前：混合効果モデル\n以上，解説してきたランダム効果モデル／変量効果モデルであるが，混合効果モデル とも呼ばれる．12\n何を言っているのかわからないかもしれないが，式 (1) \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] において，\\(\\alpha_{s[i]}\\) がランダム効果であるが，回帰係数 \\(\\beta\\) を 固定効果 とも呼ぶことがあるのである．\nそしてこう見ると全体として固定効果と変量効果が同居した 混合（効果）モデル とも呼べそうである．\nこれは変動切片項だけを変量効果と呼ぶのではなく，一般の回帰係数 \\(\\beta x\\) もグループ \\(s\\in[S]\\) ごとに異なるものを許す場合は広義の変量効果と呼べることから生じる．\n\n\n\n\n\n\n\n\n線型混合モデルの別名\n\n\n\n式 (1) \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] で定義されるモデルは，(Chung et al., 2013) によると次のような複数の名前を持つ：\n\n線型混合モデル (linear mixed models) (Kincaid, 2005)\n階層モデル (hierarchical models)\nマルチレベル線型モデル (multilevel linear models)\n混合効果モデル (mixed-effects models) (Chung et al., 2015)\nランダム効果モデル (random effects model) (Hubbard et al., 2010) や (Bafumi and Gelman, 2007)．\n分散成分モデル (variance component model)13\n\n\n\n詳しくは (第6章 Gelman, 2005, pp. 20–) も参照．\n\n\n2.6 GEE との違い\n一般化推定方程式 (GEE: Generalized Estimating Equation) では，ランダム効果モデルにおける階層的な議論を全て「局外母数」として捨象し，母数 \\(\\beta\\) の推定に集中する見方をする．\nなお，GEE そのものはあらゆる一般化されたスコア方程式を指し得る．そのため分散成分の推定にも応用可能であろう．例えば (Sugasawa and Kubokawa, 2023, p. 13) など．\n\n\n\n\n\n\nGEE との違い\n\n\n\n\n回帰式が違う\nGEE の文脈ではよくモデル式 \\[\n   Y_{it}=\\alpha+\\beta_1x_{1,i,t}+\\cdots+\\beta_px_{p,i,t}\n   \\] にはランダムな切片項というものは見当たらない．その代わり，グループ間の依存関係は相関係数行列としてモデル化を行う．ランダム効果モデルでは，この相関構造をランダムな切片項を追加することで表現し，回帰式を複数立てることでモデルを表現するのと対照的である．\n推定目標が違う\nGEE は population average model でよく用いられる (Hubbard et al., 2010) ように，あくまで応答 \\(Y_{it}\\) の平均 \\(\\beta\\) の不偏推定が目標であり，共分散構造はいわば局外母数である．一方混合効果モデルは，その階層モデルとしての性質の通り，平均構造と分散構造のいずれも推定するという志向性がある．\n推定方法が違う\n頻度論的には，混合効果モデルは主に最尤法により推定される (Hubbard et al., 2010)．一方で GEE はモーメント法により推定され，最尤法ベースではない． \n\n\n\nGEE にとって相関構造は局外母数であり，正確な特定は目的に含まれない．この意味で GEE の相関係数⾏列におく仮定は「間違えていてもよい便宜的な仮定」であるため，作業相関係数行列 (working correlation coefficient matrix) とも呼ばれる．相関構造を誤特定していても，平均構造は一致推定が可能であり，ロバストである．両方の特定に成功した場合はセミパラメトリック有効性が達成される．\n一方で混合効果モデルは，階層モデルとして平均構造と分散構造のいずれにも明示的な仮定をおくため，片方（例えば共分散構造）の特定を間違えていた場合，もう片方の解釈性が失われる，というリスクがあると論じることができる．特に (Hubbard et al., 2010) に見られる論調である．\nしかし小地域推定や地域ごとのばらつきに注目した研究など，ユニットの平均効果ではなく個別効果に注目したい場合には混合効果モデルの方が適していることになる (Gardiner et al., 2009)．実際，モデルの特定に成功していれば，いずれのパラメータも最尤推定されるため一致性を持つ．\n\n\n\n\n2.7 ベイズ混合効果モデルという光\n第 2.3 節で見た通り，ベイズ統計学の立場からは，変量効果モデル・固定効果モデル・完全プーリングモデルはいずれもモデルの違いとして理解できる．\nそれぞれに自然な点推定法は違うかもしれないが，それだって特定の事前分布に関するベイズ推論の特殊な場合に過ぎない．\nそれぞれのモデルに関してベイズ推論をし，周辺化をして平均構造に関する marginal estimator を構成すれば GEE や固定効果推定量の代用になる上に，どのような構造的な仮定を置いてしまっているか反省する契機にもなる．\n計算機の性能と，計算統計手法の発展が目まぐるしい現代にて，過去の議論を踏襲しすぎることは，問題の本質を誤るということもあるのだろう．\n\nこの節はこれで終わり．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#混合効果モデリングのテクニック集",
    "href": "posts/2024/Computation/brms.html#混合効果モデリングのテクニック集",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "3 混合効果モデリングのテクニック集",
    "text": "3 混合効果モデリングのテクニック集\n\n\n\n\n\n\n概要\n\n\n\n\n混合効果モデルの最尤推定・ベイズ推定において，グループレベル変動 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になり得る．特に，グループ数 \\(S\\) が小さい場合に顕著である．\n分散成分を推定したあと，これを係数 \\(\\beta\\) の推定量に代入することで，縮小効果を持った効率的な推定量を得ることができる．\nカウントデータの Poisson モデルでは，「観測レベルのランダム効果」を追加することで，実質的に Poisson-対数正規混合モデリングを実行できる．\n\n\n\n\n3.1 グループレベル分散の推定\n\n3.1.1 問題\n変量効果モデル \\[\ny_i=\\alpha_{s[i]}+\\beta x_i+\\epsilon_i,\\qquad i\\in[n],\\tag{1}\n\\] の推定において，特にグループ数 \\(S\\) が小さい場合，グループレベルの変動切片項 \\(\\alpha_{s[i]}\\) の共分散行列 \\(\\mathrm{V}[\\eta_s]\\) の推定が不安定になったり，分散が負の値をとったりするという問題点が古くからある (Harville, 1977)．14\n変量効果 \\(\\eta_s\\) を \\(\\eta_s\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2_s)\\)，誤差を \\(\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}(0,\\sigma^2_e)\\) とすると，この \\(\\mathrm{V}[\\eta_s]\\) は次の形をもち，グループ間の相関構造のモデリングを一手に引き受けている： \\[\n\\mathrm{V}[\\eta_{s}]=\\sigma^2_sJ_{n_s}+\\sigma_e^2I_{n_s},\\qquad J_{n_s}:=\\boldsymbol{1}_{n_s}\\boldsymbol{1}_{n_s}^\\top.\n\\]\nEM アルゴリズムが提案されたばかりの頃 (Laird and Ware, 1982) では，共分散構造にパラメトリックな仮定をおいていたが，現代ではこれを取り去った最尤推定法・ベイズ推定法が主流である．\n\n\n3.1.2 退化しない共分散行列推定\nしかし，最尤推定法と，一定の事前分布を仮定したベイズ MAP 推定法では，推定された共分散行列が退化してしまったり，分散が負の値を取ってしまうことがある．\n打ち切り推定量 (Kubokawa and Srivastava, 1999), (Kubokawa, 2000) なども提案されているが，ベイズでは Wishart 事前分布を仮定することでこれが回避される (Chung et al., 2015)．15 これは最尤法の文脈では，penalized likelihood と等価になる (Chung et al., 2013)．\nモデルのサイズによっては，完全なベイズ推定を実行することが難しく，一部は等価な頻度論的な方法や近似を用いることもある．その際，最適化ソルバーの収束を速めるために，共分散構造に（データや計画とは無関係に）パラメトリックモデルを仮定してしまうこともある (Kincaid, 2005)．\n\n\n\n3.2 係数の縮小推定\n\n3.2.1 係数の２段階推定\n分散 \\(\\mathrm{V}[\\eta_s]\\) を推定して分散比 \\(\\rho:=\\sigma_v^2/\\sigma_e^2\\) の推定量 \\(\\widehat{\\rho}\\) を得て，これを最良線型不偏推定量 (BLUE) \\(\\widehat{\\beta}\\) に代入して得られる，グループごとの \\(y_s\\) の推定量に \\[\n\\widehat{y}_s:=\\frac{\\widehat{\\rho}n_s}{1+\\widehat{\\rho}n_s}\\overline{y}_s+\\frac{1}{1+\\widehat{\\rho}n_s}\\overline{x}_s^\\top\\widetilde{\\beta}(\\widehat{\\rho})\n\\] というものがあり，これを 経験 BLUE という (久保川達也, 2006, p. 143)．\nこれは，各グループ \\(s\\in[S]\\) における値 \\(y_s\\) を，単なる経験平均 \\(\\overline{y}_s\\) ではなく，全データプールから得られる推定量 \\(\\overline{x}_s^\\top\\widetilde{\\beta}(\\widehat{\\rho})\\) で補正した推定量になっている．\nこのことにより，各グループ \\(s\\in[S]\\) のデータ数が少なく，経験平均 \\(\\overline{y}_s\\) では分散が大きくなってしまう場合でも，安定した推定量を得ることができる．\n縮小推定は小地域推定 (Battese et al., 1988) に応用を持つ．例えば \\(s\\in[S]\\) をアメリカ合衆国の各州とし，投票行動のデータに応用した例が (Gelman, 2014) にある．\nこのように，変量効果 \\(\\alpha_{s[i]}\\) を追加したモデリングを実行することにより，グループごとの被説明変数を縮小推定することができる．\n\n\n3.2.2 経験ベイズ\n縮小推定の効用は初め，経験ベイズの枠組みで説明された．\n\n以上の考え方は，経験ベイズの枠組みで (Efron and Morris, 1975) の一連の論文の中で示されてきたものであり，ベイズ的アプローチの現実的な有用性は基本的には上述の考え方に基づいている．\n\nそもそも１元配置混合線型モデルは \\[\ny_{ij}=\\theta_{ij}+e_{ij},\\qquad \\theta_{ij}=x_{ij}^\\top\\beta+v_i\n\\] とも理解できる．これは階層モデル \\[\ny_{ij}\\sim\\operatorname{N}(\\theta_{ij},\\sigma^2_e),\\qquad\\theta_{ij}\\sim\\operatorname{N}(x_{ij}^\\top\\beta,\\sigma_v^2)\n\\] とも見れる．\n\\(\\beta,\\sigma^2_v,\\sigma^2_e\\) を未知母数として扱った場合を 経験ベイズモデル，変量として扱って更なる分布を仮定した場合を（狭義の） 階層ベイズ ともいう (久保川達也, 2006, p. 155)．\n\n\n\n3.3 カウントデータ過分散へのお手軽対処法\nこれはカウントデータのモデリング限定のテクニックである．\nカウントデータも，一般化線型（混合）モデルの範疇で扱うことができるため，リンク関数 \\(g\\) を通じてほとんど同等の扱いが可能である．\n\n3.3.1 負の二項分布によるモデリング\nカウントデータの基本は Poisson 分布であろうが，過分散を考慮するために負の二項分布でモデリングすることもできる．(17.2節 Gelman et al., 2014) なども参照．\n負の二項分布は例えばマーケティングにおいて，顧客の購買回数をモデル化する際に用いられる (森岡毅，今西聖貴, 2016)．\nこの行為は，Poisson 分布の Gamma 分布による混合分布族を用いた，混合モデリングを行っているとみなせる：\n\n\n\n\n\n\n命題\n\n\n\nPoisson 分布 \\(\\mathrm{Pois}(\\theta)\\) の \\(\\mathrm{Gamma}(\\alpha,\\nu)\\)-混合は負の二項分布 \\(\\mathrm{NB}\\left(\\nu,\\frac{\\alpha}{\\alpha+1}\\right)\\) になる．\nただし，負の二項分布 \\(\\mathrm{NB}(\\nu,p)\\) は，次の確率質量関数 \\(p(x;\\nu,p)\\) が定める \\(\\mathbb{N}\\) 上の確率分布である： \\[\np(x;\\nu,p)=\\begin{pmatrix}x+\\nu-1\\\\x\\end{pmatrix}p^\\nu(1-p)^x.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n確率分布の変換則より，次のように計算できる：\n\\[\\begin{align*}\n  p(x)&=\\int_{\\mathbb{R}_+}\\frac{\\theta^x}{x!}e^{-\\theta}\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu\\theta^{\\nu-1}e^{-\\alpha\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\int_{\\mathbb{R}_+}\\theta^{x+\\nu-1}e^{-(\\alpha+1)\\theta}d\\theta\\\\\n  &=\\frac{\\alpha^\\nu}{x!\\Gamma(\\nu)}\\frac{\\Gamma(x+\\nu)}{(\\alpha+1)^{x+\\nu}}\\\\\n  &=\\begin{pmatrix}\\nu+x-1\\\\x\\end{pmatrix}\\left(\\frac{1}{\\alpha+1}\\right)^x\\left(\\frac{\\alpha}{\\alpha+1}\\right)^\\nu.\n\\end{align*}\\]\nこの最右辺は，たしかに負の二項分布の質量関数である．\nこの証明方法と，Gamma 分布については次の記事を参照：\n\n  \n    \n      \n      \n        確率測度の変換則\n        Gamma 分布とBeta 分布を例に\n      \n    \n  \n\n\n\n\nこれは \\[\ny_{it}\\sim\\mathrm{Pois}(\\theta)\n\\] \\[\n\\theta\\sim\\mathrm{Gamma}(\\alpha,\\nu)\n\\] という Gamma 分布を仮定した経験ベイズモデル（第 3.2.2 節）に当たる．\nGamma 分布は Poisson 分布の共役事前分布であるため計算が容易であり，早くから質病地図の作成などにも用いられていた (Clayton and Kaldor, 1987), (丹後俊郎, 1988)．\n\n\n3.3.2 Poisson-対数正規混合によるモデリング\nPoisson 回帰\n\\[\n\\begin{align*} y_{it} & \\sim \\operatorname{Pois}(\\lambda_{s[i]}) \\\\ \\log(\\lambda_{s[i]}) & = \\alpha_i + \\eta_{it} \\\\ \\eta_{it} & \\sim \\operatorname{N}(0, \\sigma). \\end{align*}\n\\]\nを考えると，各 \\(y_{it}\\) を，（グループ毎に条件付ければ）Poisson 分布の対数正規分布による混合分布を用いてモデル化していることにあたる．\nこの，Poisson-対数正規分布族は，(Bulmer, 1974) により生物種の個体数分布のモデリングで，過分散を説明するために用いられている．\nすなわち，第 1.1 節のモデルの比較 1.5 で扱った，観測レベルランダム効果 (OLRE: Observation-level Random Effects) の方法は，観測毎に \\(\\eta_{it}\\) というランダム切片項を追加するだけで，本質的には Poisson-対数正規混合モデリングを実施する という，いわばハックのような使い方である．16\n今回はモデル比較の結果が良かったため，本格的に対数正規混合を実施してみるのも良いかもしれない．\n\n\n\n3.4 変量係数モデルによる非線型モデリング\n\n\n\n\n\n\n混合モデルの種々の拡張\n\n\n\n前節 3.3 では，カウントデータに適用するための一般化線型混合モデルをみた．\n(久保川達也, 2006) では，ここまで考慮した１元配置混合線型モデルの拡張をいくつか紹介している：\n\n各グループ \\(s\\in[S]\\) の中でもいくつかのクラスターに分けられる場合，２元配置混合モデル が考えられる： \\[\ny_{ijk}=x_{ijk}^\\top\\beta+v_i+u_{ij}+e_{ijk}.\n\\]\n誤差分散が一定であるという仮定が怪しい場合，変動分散モデル が考えられる．これは，グループ内の分散を \\(e_{ij}|\\sigma_i^2\\sim\\operatorname{N}(0,\\sigma_i^2)\\) とし，\\(\\sigma_i\\) をグループ内で同一の分布に従う i.i.d. と仮定した階層モデルをいう．\n係数 \\(\\beta\\) にもモデルを仮定した階層モデルは 変量係数モデル ともいう： \\[\n\\beta_i=W_i\\alpha+v_i.\n\\] 州ごとの，収入因子が投票行動に与える影響の差を突き止めた (Gelman, 2014) ではこの変量係数モデルを用いている．\n\n\n\n\n3.4.1 例：投票行動の州ごとの違い\n(Gelman, 2014) では州ごとの投票行動の違いを説明するために，まずは次のロジスティック混合モデルを考えている： \\[\n\\operatorname{Pr}(y_i=1)=\\operatorname{logit}^{-1}(\\alpha_{s[i]}+x_i\\beta)\n\\] \\[\n\\alpha_s=W_s^\\top\\gamma+\\epsilon_s,\\qquad\\epsilon_s\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,\\sigma^2_\\alpha).\n\\]\n\n\n\n\n\n\n各変数の説明\n\n\n\n\n\n\n\\(y_i\\in\\{0,1\\}\\) は共和党に投票したか，民主党に投票したかを表す２値変数．\n\\(x_i\\in\\{\\pm2,\\pm1,0\\}\\) は収入のレベルを５段階で表す離散変数．\n\\(W_j\\) は各州の共変量のベクトル．\n\n\n\n\nしかしこのままではモデルの当てはまりが良くなかった．これは州ごとに収入が投票に与える影響が異なるためであった．これを考慮するために，(Gelman, 2014) は変量係数モデルを用いた．\n\n\n3.4.2 混合モデルの変量係数モデル化\n\\(\\beta\\) を州ごとに変化させ，これに \\[\n\\beta_s=W_s^\\top\\gamma'+\\epsilon'_s,\\qquad \\epsilon'_s\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,\\sigma^2_\\beta),\n\\] というモデルをおく．\nこれにより，州ごとに変化する収入-投票関係をモデリングできる．\n\n\n3.4.3 非線型モデル化\nこれに加えて，\\(\\beta_s\\) を収入カテゴリのアイテム \\(x_i\\in\\{\\pm2,\\pm1,0\\}\\) ごとに変化させることも考えられる．\nこれは値も持つダミー変数 \\[\n\\boldsymbol{x}_i^j=(j-3)1_{\\left\\{x_i=j\\right\\}},\\qquad j\\in\\{1,2,3,4,5\\},\n\\] を成分にもつ \\(\\boldsymbol{x}_i\\in\\mathbb{Z}^5\\) を用いて， \\[\n\\operatorname{Pr}(y_i=1)=\\operatorname{logit}^{-1}(\\alpha_{s[i]}+\\boldsymbol{x}_i^\\top\\boldsymbol{\\beta}_{s[i]})\n\\tag{5}\\] というモデルを考えることにあたる．\nこの小さな変更により，非線型な関係もモデリングできるようになる．\n\n\n3.4.4 多重共線型性の霧消\nこのようなトリックが可能な理由は，ベイズ回帰においては多重線型性が問題にならないためである．\nモデル (5) では，３通りで収入が説明変数に入っている：\n\n各収入カテゴリのダミー変数 \\(1_{\\left\\{x_i=j\\right\\}}\\) として\n収入カテゴリの値 \\(\\boldsymbol{x}_i^j\\) として．\n州ごとの収入として \\(W_s\\) にも入っている．\n\nこのことに気づけただろうか？\n頻度論的に回帰分析を実行していたならば，このような多重共線性は問題になっていただろうが，階層ベイズモデリングにおいては有用なトリックとして積極的に活用することができる．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#brmsの実装",
    "href": "posts/2024/Computation/brms.html#brmsの実装",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "4 brmsの実装",
    "text": "4 brmsの実装\nbrm 関数（コードは こちら）の実装を調べる．\n\n\n\n\n\n\n\nbrms\n\nStan コードを扱っている関数は .stancode() であった．最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\n\n.standata\n\n\n\n\n\n4.1 事前分布\nbrm 関数 では，デフォルトでは無情報事前分布が用いられる．\n\nDefault priors are chosen to be non or very weakly informative so that their influence on the results will be negligible and you usually don’t have to worry about them. However, after getting more familiar with Bayesian statistics, I recommend you to start thinking about reasonable informative priors for your model parameters: Nearly always, there is at least some prior information available that can be used to improve your inference.brm(): Fit Bayesian Generalized (Non-)Linear Multivariate Multilevel Models\n\n具体的には，ユニットレベルの回帰係数（クラス b）には一様分布が置かれる．\n\n\n4.2 回帰式\nbrm() 関数の第一引数 formula は，validate_formula 関数に渡される．\nこの関数は S3 のメソッドのディスパッチを用いて実装されており，brmsformula オブジェクトに対しては，validate_formula.brmsformula 関数が呼び出される．\nここでは autocor 引数が引かれている場合，出力の formula 属性に追加される：17\n\nfit3$formula\n\ncount ~ zAge + zBase * Trt + (1 | patient) \nautocor ~ unstr(time = visit, gr = patient)\n\n\nなお，brmsformula オブジェクトのコンストラクタは brmsformula() 関数 である．これは，R の formula オブジェクトを通じて，階層モデルを定義できるようになっている（実装はリスト）．\n\n\n4.3 共分散構造\n共分散構造は２つの観点から，brmsformula オブジェクトから自動的に指定される．\n１つ目がグルーピング構造（共分散行列のブロック構造）であり，これはgr関数 が使用される．\n２つ目がグループ内の相関構造であり，これは brm() 関数の autocor 引数を用いる．\n\n4.3.1 gr 関数\nこの関数は brm 関数の第一引数として与えられたモデル定義式から，暗黙のうちに内部で呼び出される．\n例えば，回帰式に (1|patient) が含まれていた場合， gr(patient) が呼び出される．\n共分散構造におく仮定について，重要なデフォルト設定が２つある：\n\n\n\n\n\n\n\nグループ間の相関構造は想定されている：cor=True．\n\nIf TRUE (the default), group-level terms will be modelled as correlated.gr(): Set up basic grouping terms in brms\n\n一方で，グループ内の相関構造は想定されておらず，独立とされている．具体的に指定したい場合は引数 cov を用いる．\n\nBy default, levels of the same grouping factor are modeled as independent of each other.gr(): Set up basic grouping terms in brms\n\n\nすなわち，\\(\\mathrm{V}[\\eta_s]\\) には一切仮定が置かれておらず（第 3.1 節），一方で \\(\\{\\epsilon_{it}\\}_{t=1}^T\\) は互いに独立とされている．\n\n\n\nまた，この二階層目の分布族（第 2.1 節での \\(\\alpha_i\\) と \\(\\eta_{it}\\)）は，分散共分散行列 \\(\\mathrm{V}[\\eta_s]\\) を持った正規分布がデフォルトで，現状他の分布族は指定できないでいる．\n\ndist: Name of the distribution of the group-level effects. Currently “gaussian” is the only option.gr(): Set up basic grouping terms in brms\n\n\n\n4.3.2 autocor 引数\nbrm() 関数には，autocor 引数 が用意されている．\ngr() のデフォルト値では独立とされていたグループ内の相関構造を，具体的に指定するのに用いられる．\n\n\n\n\n\n\n\nunstr：一才の仮定を置かない．\nAR：一次の自己相関構造．\n\n\n\n\n\n\n\n4.4 推論エンジン\nbrm 関数 は，Stan による MCMC サンプリングを通じて，事後分布を計算する．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#文献紹介",
    "href": "posts/2024/Computation/brms.html#文献紹介",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "5 文献紹介",
    "text": "5 文献紹介\n\nここでは計量経済学の呼称に従い，固定効果モデルと変量効果モデルと呼んだが，同じモデルを母数モデル (fixed effect model) と変量モデル (random effect model) と呼んだりもする (足立浩平, 2000)．"
  },
  {
    "objectID": "posts/2024/Computation/brms.html#acknowledgements",
    "href": "posts/2024/Computation/brms.html#acknowledgements",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\n\nI would like to extend my gratitude to Robert Long, who kindly shared me the knowledge about the covariance structure implicitly defined via brms formula on this Cross Validated post. His insights were instrumental in enhancing this work."
  },
  {
    "objectID": "posts/2024/Computation/brms.html#footnotes",
    "href": "posts/2024/Computation/brms.html#footnotes",
    "title": "brms によるベイズ混合モデリング入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの区別については こちら も参照．↩︎\n通常は時間的に離れている観測は相関が薄いとしても，直近の観測と関連性が高いだろう．↩︎\nStatistical Modeling, Causal Inference, and Social Science における こちらのエントリ も参照．↩︎\nすなわち，ある super population を想定して，その確率分布の従う項と考えており，変量効果 とも呼ばれる．一方で未知母数とみなす場合は 母数効果 ともいう (久保川達也, 2006)．↩︎\n(Hansen, 2022, p. 333) 第12.3節，(Bafumi and Gelman, 2007, p. 3), (Hansen, 2022, p. 604)，(Gardiner et al., 2009, p. 228)．↩︎\n(Bafumi and Gelman, 2007, p. 5) や (久保川達也, 2006, p. 141), (Gelman et al., 2020) も参照．(Cunningham, 2021) は pooled OLS と呼んでいる．↩︎\n特にパネルデータの文脈では within estimator ともいう (Cunningham, 2021)．↩︎\n(Bafumi and Gelman, 2007, p. 5)，(Hansen, 2022, p. 609) 17.11節 など．狭義では，fixed effects model は within transformation を行い，グループ間の影響を引いたあとに回帰を実行する……という手続きを指すこともあるが，２つは等価な結果を生む．詳しくは (Cunningham, 2021) なども参照．↩︎\n(Hansen, 2022, p. 624) 17.25節．↩︎\n(Hansen, 2022, p. 624)，(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Bafumi and Gelman, 2007, p. 6)．↩︎\n(Hubbard et al., 2010) では両方の名前で呼んでいる．↩︎\n\\(\\mathrm{V}[\\eta_s]\\) はブロック行列の構造を持つためこう呼ばれる．(久保川達也, 2006, p. 141) でも LMM と併記されている．↩︎\n(Laird and Ware, 1982)，(Chung et al., 2013)，(Chung et al., 2015)，Statistical Modeling, Causal Inference, and Social Science ブログ 6/2/2023．↩︎\n逆 Wishart ではないらしい (Chung et al., 2015)．↩︎\nSolomon Kurtz (2021) による解説，RPubs も参照．↩︎\nLine 1363．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html",
    "href": "posts/2024/Computation/PGM2.html",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html#統計力学概観",
    "href": "posts/2024/Computation/PGM2.html#統計力学概観",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "1 統計力学概観",
    "text": "1 統計力学概観\n\n1.1 興りと運動論的方法\n統計力学は，マクロ観測量をミクロ情報のみから予測することを最大の目的とする．その成果の１つとして，マクロ観測量の揺らぎについても，より細かい分析を可能とする．1 特に物理的過程を確率過程としてもモデリングする非平衡統計力学は，粗視化と確率化が対応していることを示唆する．2\n\nThe aim of statistical mechanics is to predict the relations between the observable macroscopic properties of the system, given only a knowledge of the microscopic forces between the components. (Baxter, 1982, p. 1)\n\n(Boltzmann, 1872) による方程式 \\[\nS=k\\log W\n\\] は Max Planck が 1900 年前後にまとめたものである．3\n\n統計力学はこの１行から生まれたといってもよかろう． (戸田盛和 et al., 2011, p. vi)\n\nこれは希薄気体の熱力学を，ミクロの対象に関する密度関数の時間変化を記述することで基礎付けたもので，この接近を一般に 運動論的方法 (kinetic method) という．4\nこの手法がそのまま統計熱力学一般の基礎付けとはならず，その抽象的な枠組みは，後に (Gibbs, 1902) によって与えられた．5\nなお，運動論的方法は，統計物理学の端緒であると同時に，現在でも十分に理解されているとは言い難く，機械学習などとも結びつきながら現在でも活発に研究が進められている分野である (Villani, 2002)．\n\n\n1.2 磁性体の物理\n多くの物質は，外部磁場 \\(H&gt;0\\) を印加すると磁化する：\\(M(H)&gt;0\\)．これを 常磁性体 (paramagnetism) という．これは相互作用が無視できる要素から出来ているためと言える．そのため，理想体系 とも呼ばれる．6\n一方で鉄などの強磁性体では，外部磁場を印加したのち，これを取り除いても磁性が残り，これを 自発磁化 という：\\(M_0:=M(0)&gt;0\\)． 7\n一方で，磁場を反転させたとき，逆向きに磁荷されるから，\\(M(-H)=-M(H)\\) というべきである．すなわち，強磁性体は \\(H=0\\) において相転移を起こす．8\n強磁性体には (Weiss, 1907) が一定の理解を与え，これが理論が与えられた最初の秩序無秩序問題である．9 その後多くの模型が提案されているが，Ising 模型を初め，１次元の場合と２次元の特別な例のいくつかを除いて，相転移の厳密な理論は存在しない．10\nしかし強磁性体も，一定の温度以上では自発磁化を失い，常磁性体と同じように振る舞う．すなわち，\\(M\\) は \\(H=0\\) でも連続になる．これを Curie温度 といい，\\(T_C\\) で表す．11\n理論的には，\\(T=T_C\\) の場合，\\(H=0\\) では連続であるが傾きが発散して可微分ではないとする．\n物体の磁化 \\(H\\) を \\(T,H\\) の２変数関数と見做した場合，線分 \\([0,T_C]\\) を除いた領域で \\(H\\) は滑らかである，とも解釈できる．点 \\((T_C,0)\\) を 臨界点 (critical point) という．\n\\[\nM_0(T)=\\lim_{H\\to 0+}M(H,T).\n\\]\n\n\n1.3 磁化率\n磁化率を \\[\n\\chi(H,T):=\\frac{\\partial M(H,T)}{\\partial H}\n\\] と定義する．変数変換 \\(t=\\frac{T-T_C}{T_C}\\) も用いる．すると臨界点は \\((H,t)=(0,0)\\) にある．\n磁化率 \\(\\chi\\) は \\((H,t)=(0,0)\\) に，近づき方によって異なる非整数位数の極 \\[\n\\frac{\\chi(0,T)}{t^{-\\gamma}}=O(1)\\quad(t\\to0+),\n\\] \\[\n\\frac{\\chi(0,T)}{(-t)^{-\\gamma'}}=O(1)\\quad(t\\to0-)\n\\] を持つことが知られており，その指数 \\(\\gamma,\\gamma'\\) を 臨界指数 (critical exponents) という．12\nこの臨界指数は，一部のモデルでは斉次になる：\\(\\gamma=\\gamma'\\) ことを，(Griffiths, 1967) は２次と３次の Ising モデルで例を構成した．\nまた，臨界指数は具体的な Hamiltonian \\(H\\) の関数形には殆ど依らないと考えられている (Fisher, 1966)．これを 普遍性 という．13\n\n\n1.4 最近傍 Ising モデル\nIsing モデルは強磁性体の磁性体への相転移を記述するために (Lenz, 1920) によって導入された．命名は Lenz の指導の下で完成を見た Ising の博士論文 (Ising, 1925) から (Peieris, 1936) がつけたものであるが，Ising 本人は Lenz-Ising model としていた．14\nIsing モデルは格子点 \\(\\Lambda\\subset\\mathbb{Z}^d\\) 上の無向グラフ \\(G=(\\Lambda,\\mathcal{E})\\) 上に定義される．多くの場合 \\[\n\\Lambda:=B(n):=\\{-n,\\dots,n\\}^d,\n\\] \\[\n\\mathcal{E}:=\\biggl\\{\\{x,y\\}\\subset\\Lambda\\:\\bigg|\\:\\|x-y\\|_1=1\\biggr\\},\n\\] とする．\nIsing 模型最大の単純化として，各頂点を２値変数と同一視する．すなわち，ミクロ状態の全体を \\[\n\\Omega:=\\{\\pm1\\}^\\Lambda\\simeq_\\mathrm{Set}P(\\Lambda)\n\\] とし，配置集合 (configuration) ともいう．15\nこの上に Hamiltonian を \\[\n\\begin{align*}\n    H_{\\Lambda,h}(\\omega)&:=H_0(\\omega)+H_1(\\omega)\\\\\n    &:=-\\sum_{(i,j)\\in\\mathcal{E}}J_{ij}\\omega_i\\omega_j-h\\sum_{i\\in\\Lambda}\\omega_i\n\\end{align*}\n\\] と定義する．16 \\(h\\) は外部磁場である．Hamiltonian の値をエネルギーともいう．\nこの Hamiltonian を 最近傍 Ising 模型 (nearest-neighbor Ising model) という．相互作用を表す偶関数 \\(H_0\\) の関数系をもっと一般的に取ることで，より一般的な Ising 模型も構成される．17\n\\(J&gt;0\\) の場合，隣り合うスピンが揃っている方が Hamiltonian は小さいため，これは強磁性体の模型になっていると言える．18\nこの最近傍 Ising 模型についても，３次元以上の場合と，２次元で \\(h\\ne0\\) の場合はまだ解かれていない．19\nこの Hamiltonian \\(H_{\\Lambda,h}\\) が \\(\\Omega\\) 上に定める Gibbs 分布 とは，20 \\[\n\\mu_{\\Lambda;\\beta,h}(\\omega)\\propto e^{-\\beta H_{\\Lambda;h}(\\omega)}\n\\] をいう．規格化定数は \\[\nZ_{\\Lambda}(\\beta,h):=\\sum_{\\omega\\in\\Omega}e^{-\\beta H_{\\Lambda;h}(\\omega)}\n\\] \\[\n\\beta:=\\frac{1}{kT}\n\\] などと表され，分配関数 と呼ばれる．\nこの最も簡単とも思われる設定の下で，全磁化 \\[\nM_\\Lambda(\\omega):=\\sum_{i\\in\\Lambda}\\omega_i\n\\] または磁化密度 \\[\n\\frac{M_\\Lambda(\\omega)}{N},\\quad N:=\\lvert\\Lambda\\rvert\n\\] を確率変数と見て，Gibbs 分布の上で調べるのである．21\nIsing 模型は流体のモデルとしても用いられ，Ising 模型に従うと仮想される気体を 格子気体 (lattice gas) という．22 \\(J_{ij}\\) には Lennard-Jones ポテンシャルなどが用いられる．\n\n\n1.5 平衡統計力学の枠組み\nこのように，全ミクロ状態 \\(\\Omega\\) とその上の Hamiltonian \\(H:\\Omega\\to\\mathbb{R}\\) を考え，Gibbs 分布 \\(\\mu\\) と分配関数 \\[\nZ:=\\sum_{s\\in\\Omega}e^{-\\frac{H(s)}{kT}}\n\\] を考えるというのが，(Gibbs, 1902) が創始した枠組みである．23\nこうして，平衡統計力学の議論の中心は，Gibbs 分布に関する平均の計算である．24 得られる平均は，絶対温度 \\(T\\) と，\\(H\\) 内の変数（Ising 模型では外部磁場 \\(h\\)）に関する関数になる．\n同時に平衡統計力学の中心問題が明らかになる．それは分配関数 \\(Z\\) の計算が多くの模型では極めて難しいということである．\nこうして，Isign 模型のような簡単な模型から議論するか，平均の値を近似する手法を考えることになるが，特に臨界点付近では後者の方法は行き詰まる．25\n前者も，多くの正確に解かれた模型は，Ising 模型の例と見れる上に，殆どが２次元以下である．26\n\n\n1.6 Curie-Weiss 模型\n一方で，(Weiss, 1907) の理論は，Ising 模型の 平均場近似 に基づいていた．27\n３次元以上では解けていない Ising 模型を，平均場近似で近似的に解くことが出来る．ただし，１次元の場合でも相転移が起こるという都合の悪い面も出てくる．28\n\\(J_{ij}\\equiv J\\) とした最近傍 Ising 模型において， \\[\n-J\\sum_{(i,j)\\in\\mathcal{E}}\\omega_i\\omega_j=-(2dJ\\omega_i)\\cdot\\frac{1}{2d}\\sum_{(i,j)\\in\\mathcal{E}}\\omega_j\n\\] と見て，第２の因子 \\(\\frac{1}{2d}\\sum_{(i,j)\\in\\mathcal{E}}\\omega_j\\) を \\(\\omega_i\\) の周囲の局所的な平均磁化とみなす．\\(2d\\) はちょうど最近傍格子点の数であることに注意．\nこの値を，大域的な磁化密度 \\(\\frac{1}{N}\\sum_{j=1}^N\\omega_j\\)，\\(N:=\\lvert\\Lambda\\rvert\\) に置き換えることを Weiss近似 といい，平均場近似の歴史上最初の例であった．29\nこうして，次の Curie-Weiss Hamiltonian を得る： \\[\nH_{N;J,h}(\\omega):=-\\frac{dJ}{N}\\sum_{i,j=1}^N\\omega_i\\omega_j-h\\sum_{i=1}^N\\omega_i.\n\\]\n最近傍 Ising 模型と違って，積 \\(\\omega_i\\omega_j\\) を取る際の添字 \\(i,j\\) に制約はないから，大域的な相互作用も考えていることになる．ただし，この相互作用には一様性を課しており，粒子の個々の位置関係などは完全に捨象されている．実際，\\(N\\) 位の完全グラフ \\(K^n\\) 上の最近傍 Ising 模型とも見れる．\nそれ故，強相関電子系 など揺らぎの大きな系では，平均場近似では正確な結果は得られない．\n\n\n1.7 バンド理論\nバンド計算における 一電子近似 も平均場近似の例である．30\n固体内の電子は，まず化学結合に寄与する価電子 (valence electron) と原子核に束縛された核電子 (core electron) に分けられる．第一近似として，原子核とその核電子と，価電子とを，固体の独立な２つの構成要素と考えることが多い．31\nさらにはバンド理論では，電子と正孔の間の相互作用を捨象している．32\n\n\n1.8 半導体\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．33\nこのような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．34 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．\n(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，35 その後20世紀に入るとラジオに応用された．Braun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html#markov-確率場",
    "href": "posts/2024/Computation/PGM2.html#markov-確率場",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "2 Markov 確率場",
    "text": "2 Markov 確率場\n(Li, 2009) Chapter 2 も参照．"
  },
  {
    "objectID": "posts/2024/Computation/PGM2.html#footnotes",
    "href": "posts/2024/Computation/PGM2.html#footnotes",
    "title": "数学者のための確率的グラフィカルモデル２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Baxter, 1982, p. 1) の他に，(Friedli and Velenik, 2017, p. 18) にも同様の記述がある．↩︎\n(戸田盛和 et al., 2011, p. ix) 「物理的過程を確立的に捉えるとき，根元的なミクロのレベルから出発して，マクロのレベルに到達するには，ものの見方の粗さ (coarse graining) のさまざまな段階がある．それぞれの段階でインフォメーションが失われ，それに応じた確率化が行われる．」↩︎\n(戸田盛和 et al., 2011, p. vi), Eric Weisstein’s World of Physics．これは方程式というよりは，エントロピー（左辺）の数学的定義と読むべきである (Villani and Mouhot, 2015, p. 6)．↩︎\n(戸田盛和 et al., 2011, p. vi)．「ミクロの対象」と言ったが，Maxwell がこの理論を展開した当時 (Maxwell, 1867), (Maxwell, 1878) は原子論もまだ仮説の一つに過ぎなかった (Villani and Mouhot, 2015, p. 3)．↩︎\nstatisitcal ensemble の概念もこのときに導入された．Wikipedia↩︎\n同様の例として Boyle-Charles の法則に従う気体も挙げられている (戸田盛和 et al., 2011, p. 129)↩︎\n(Kittel, 2018, p. 323), (Baxter, 1982, p. 1)．↩︎\n(Baxter, 1982, p. 2)．「相転移を起こす系は一般に構成要素間に相互作用のある系であり，協力系 とも呼ばれ，相転移は 協力現象 と言われることもある」 (戸田盛和 et al., 2011, p. 129)．↩︎\n(戸田盛和 et al., 2011, p. viii)．↩︎\n(戸田盛和 et al., 2011, p. viii), (Baxter, 1982, p. v)．↩︎\n(Kittel, 2018, p. 323), (Huebener, 2019, pp. 176–177)．よく \\(T_C\\) で表される．一方で反磁性体では Néel温度 という．↩︎\n(戸田盛和 et al., 2011, p. 170) も参照．↩︎\n(Baxter, 1982, p. 7)．↩︎\n(Friedli and Velenik, 2017, p. 40)↩︎\n(Preston, 1974, p. 1) も参照．\\(B(n)\\) の \\(n\\to\\infty\\) の場合などが，熱力学的極限の例である． (Friedli and Velenik, 2017, p. 43)．↩︎\n(Friedli and Velenik, 2017, p. 42), (Baxter, 1982, p. 15) など参照．↩︎\n(Baxter, 1982) の1.7節が一般 Ising 模型，1.8節が最近傍 Ising 模型．↩︎\n(Friedli and Velenik, 2017, p. 42), (Baxter, 1982, p. 21) 参照．↩︎\n(Baxter, 1982, p. 21), (Friedli and Velenik, 2017, p. 60)．↩︎\n\\(\\Omega\\) 上の分布は 状態 ともいう (Preston, 1974, p. 2)．↩︎\n(Baxter, 1982, p. 17) も参照．↩︎\n(Baxter, 1982, p. 24) 1.9節, (戸田盛和 et al., 2011, p. 131)．↩︎\n(Baxter, 1982, p. 8)．↩︎\n(Baxter, 1982, p. 9)．↩︎\n(Baxter, 1982, p. 11)↩︎\n(Baxter, 1982, p. 14)．↩︎\n現在でこそ平均場近似と呼ばれるが，(Weiss, 1907) は分子場近似と呼んでいた．平均場近似の最初の例である．↩︎\n(戸田盛和 et al., 2011, p. 167) 参照．↩︎\n(戸田盛和 et al., 2011, p. 162), (Friedli and Velenik, 2017, pp. 60–61) も参照．二元合金においては Bragg-Williams 近似ともいう．↩︎\n(Madelung, 1978, p. 10)．↩︎\n(Madelung, 1978, p. 6)．↩︎\n(Madelung, 1978, p. 118)．↩︎\n(Böer and Pohl, 2018, p. 4), (Huebener, 2019, p. 73) Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う (Madelung, 1978, p. 17)．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である (Madelung, 1978, p. 211)．↩︎\n(Huebener, 2019, p. 75)．↩︎\n(Huebener, 2019, p. 73) 特に伝導体と半導体の境界部分で強く見られた．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html",
    "href": "posts/2024/Stat/Logistic.html",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#二項回帰モデル",
    "href": "posts/2024/Stat/Logistic.html#二項回帰モデル",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "1 二項回帰モデル",
    "text": "1 二項回帰モデル\n\n1.1 はじめに\n独立標本 \\([n]:=\\{1,\\cdots,n\\}\\) 上に，２値データ \\(y_i\\in\\{0,1\\}=2\\) が観測されているとする．\\(y_i\\) は例えば症状の有無などのように，２つのカテゴリーのみを持つデータである．\n\\(x_i\\in\\mathbb{R}^p\\) を \\(p\\) 次元の実ベクトルとし，これを説明変数として \\(y_i\\) をよく予測するモデルを作りたいとする．\n予測確率を最大限上げることよりも，\\(x_i\\) の \\(p\\) 個の成分のうちどの成分が \\(y_i\\) に影響を与えているかを知りたい場合，機械学習的な方法よりも，モデルを立てて推定する統計的な方法が適している (Breiman, 2001)\nよく使われる統計モデルは，\\(y_i=1\\) となる確率を予測するという 二項回帰モデル (binary regression model) である： \\[\n\\operatorname{P}[Y=1|X=x]=F(\\xi^\\top x).\n\\tag{1}\\]\nただし，\\(F\\) は分布関数とする．\nこれは \\(Y\\sim\\mathrm{Ber}(\\mu)\\) というモデルの平均パラメータ \\(\\mu\\) に対して，リンク関数 \\(G:=F^{-1}\\) を通じて線型予測をする 一般化線型モデル の一種である．\n\n\n\n\n\n\n二項モデルの尤度\n\n\n\n尤度は次のように表せる： \\[\np(\\{x_i,y_i\\}|\\xi)=\\prod_{i=1}^n\\biggr(y_iF(\\xi^\\top x_i)+(1-y_i)\\left(1-F(\\xi^\\top x_i)\\right)\\biggl).\n\\]\n仮に \\(y_i\\in\\{\\pm1\\}\\) であった場合は，次のような簡単な表記もできる： \\[\np(\\{x_i,y_i\\}|\\xi)=\\prod_{i=1}^nF(y_i\\xi^\\top x_i).\n\\]\n\n\n\n\n1.2 潜在変数モデルとしての解釈\n二項回帰模型 (1) は，\\(F\\) を分布関数にもつ誤差項 \\(\\epsilon\\) を通じて， \\[\nY=1_{\\left\\{Y^*&gt;0\\right\\}},\\qquad Y^*=\\xi^\\top X+\\epsilon,\\;\\epsilon\\sim F,\n\\tag{2}\\] によって \\(Y\\) が観測されていると解釈することもできる．\nこの解釈は計量経済学において 二項選択モデル とも呼ばれる (Hansen, 2022, p. 804)．\nこの \\(Y^*\\) という潜在変数を考慮することはベイズ計算上も大変有意義である．\n\\((\\xi,Y^*)\\) 上の拡張されたパラメータ空間上で事後分布推定をし，その後 \\(\\xi\\) 上の周辺分布を求める方法は データ拡張 と呼ばれる．\n問題の次元を上げているため一見非効率に思えるが，式 (2) が人間にとって読みやすいように，その分条件付き確率の構造が単純になっており，Gibbs サンプラーの構成が可能になる．\n\n\n1.3 ロジットモデル\n\\(F\\) を標準ロジスティック分布関数 \\[\nF(x)=\\frac{1}{1+e^{-x}},\\qquad G(x)=F^{-1}(x)=\\log\\frac{x}{1-x},\n\\] と取った場合，\\(G\\) を ロジット，このモデルは ロジットモデル と呼ばれる： \\[\n\\operatorname{P}[Y=1\\,|\\,X=x]=F(\\xi^\\top x)=\\frac{\\exp(\\xi^\\top x)}{1+\\exp(\\xi^\\top x)}.\n\\tag{3}\\]\nロジット \\(G(\\mu)\\) は \\(\\mu\\) の 対数オッズ ともいい，ロジットモデルは対数オッズに線型モデルをおいているとも解釈される．\nこのとき，\\(e^0=1\\) であることを利用すれば尤度は次のように表示できる： \\[\np(\\{x_i,y_i\\}|\\xi)=\\prod_{i=1}^n\\frac{\\exp(y_i\\xi^\\top x_i)}{1+\\exp(\\xi^\\top x_i)}.\n\\]\n\n\n\n\n\n\n(Rasch, 1960) のモデル\n\n\n\n\\[\n\\operatorname{P}[Y=1\\,|\\,X=x,B=b,A=a]=F(ax-b)=\\biggr(1+e^{b-ax}\\biggl)^{-1}\n\\] とした場合を ２母数応答モデル (2PLM: two-parameter logistic model) という．\nこのようなモデルは項目反応理論で考えられる．\nロジットモデルとの違いは，\\(x\\) も所与ではなく推定するパラメータである点である．\\(x\\) を能力パラメータ，\\(a\\) を項目識別パラメータ，\\(b\\) を難易度パラメータなどという．詳しくは次稿参照：\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#ロジットモデルのベイズ推定法",
    "href": "posts/2024/Stat/Logistic.html#ロジットモデルのベイズ推定法",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "2 ロジットモデルのベイズ推定法",
    "text": "2 ロジットモデルのベイズ推定法\n\n2.1 はじめに\nロジットモデルは比較的単純であるが，多くの深い性質を持っている．\nまず \\(p\\ll n\\) の仮定が満たされない場合，最尤推定量はバイアスを持つ (Sur and Candès, 2019)．\n比例的高次元極限では従来の漸近論は成り立たず，尤度比検定も実行不可能になる．\nこのように \\(p\\) の数が大きい場合などはベイズ推論が志向される理由がある (Firth, 1993), (Gelman et al., 2008)．\n\n\n2.2 ロジットモデルの事後分布\nロジットモデルではパラメータ \\(\\xi\\) をベイズ推定することを考える．\n即ち，データ \\(\\{(y_i,x_i)\\}_{i=1}^n\\subset2\\times\\mathbb{R}^p\\) と事前分布 \\(p(\\xi)d\\xi\\in\\mathcal{P}(\\mathbb{R}^p)\\) を通じて，事後分布 \\[\\begin{align*}\n  p(\\xi|\\{y_i,x_i\\})&\\,\\propto\\,p(\\xi)p(\\{y_i,x_i\\}_{i=1}^n|\\xi)\\\\\n  &= p(\\xi)\\prod_{i=1}^n\\frac{\\exp(y_i(x_i)^\\top\\xi)}{1+\\exp((x_i)^\\top\\xi)}=:e^{-U(\\xi)}\n\\end{align*}\\] を計算することを考える．\n事後分布は正規化定数を除いて \\[\np(\\xi|\\{y_i,x_i\\})\\,\\propto\\,e^{-U(\\xi)}\n\\] と得られたことになる．\n\\(U\\) は ポテンシャル と呼び，次のように表せる： \\[\\begin{align*}\n    U(\\xi)&:=-\\log p_0(\\xi)-\\sum_{i=1}^n\\log\\left(\\frac{\\exp(y_i(x_i)^\\top\\xi)}{1+\\exp((x_i)^\\top\\xi)}\\right)\\\\\n    &=:U_0(\\xi)+U_1(\\xi).\n\\end{align*}\\]\n\n\n2.3 ロジットモデルの事後分布サンプラー\nロジットリンク \\(G\\) による変換が複雑であるため，ロジスティック回帰は（完全な）ベイズ推定を実行することが難しいモデルとして知られてきた．\n一方でリンク関数 \\(g\\) を標準正規分布 \\(\\mathrm{N}(0,1)\\) の分布関数 \\(\\Phi\\) の逆関数に取り替えた プロビットモデル にはデータ拡張に基づく Gibbs サンプラーが早くから提案されており (Albert and Chib, 1993)，これにより効率的なベイズ推論が可能となっていた．\nプロビットモデルはロジットモデルに極めて似ており，ただ裾の重さが違うのみであると言って良い (Gelman et al., 2014, p. 407)．そのこともあり，プロビットモデルのベイズ推論は計量経済学や政治科学で広く使われている手法となったが，ロジットモデルのベイズ推論の応用は遅れていた (Polson et al., 2013)．\nしかし実はロジットモデルの事後分布 \\(\\pi\\) も正規分布の Pólya-Gamma 混合として表すことができ，データ拡張によって効率的な Gibbs サンプラーを構成することができる (Polson et al., 2013)．現在ではこのデータ拡張 Gibbs サンプラーが，標準的な事後分布サンプラーとなっている．\n\n\n2.4 Gibbs サンプラーの課題：不均衡データ\nデータもモデルも大規模になっていく現代では，このようなデータ拡張に基づく Gibbs サンプラーは特定の条件が揃うと極めて収束が遅くなる場面が少なくないことが明らかになってきている．\nそのうちの１つのパターンが大規模な 不均衡データ (Johndrow et al., 2019)，すなわち，特定のラベルが極めて稀少なカテゴリカルデータである．\nこのようなデータに対しては，プロビットモデルやロジットモデルに限らず，ほとんど全てのデータ拡張に基づく Gibbs サンプラーが低速化することが報告されている：\n\nWe have found that this behavior occurs routinely, essentially regardless of the type and complexity of the statistical model, if the data are large and imbalanced. (Johndrow et al., 2019, p. 1395)\n\n\n\n2.5 実験：不均衡データでの収束鈍化\n\\(p=1\\) 次元のプロビットモデル \\[\n\\sum_{i=1}^n y_i\\,\\bigg|\\,(n,\\theta)\\sim\\mathrm{Bin}(n,F(\\theta)),\\qquad\\theta\\sim\\mathrm{N}_1(0,B),B=100,\n\\] しかも説明変数なしの切片項のみの状況を考える： \\[\nx_i\\equiv1,\\qquad p_0(\\xi)d\\xi=\\mathrm{N}(a,B),\\qquad a=0,\n\\]\nこの場合，ポテンシャルは次のように表される： \\[\n-U(\\xi)=\\xi\\sum_{i=1}^ny_i-n\\log(1+e^{\\xi})-\\frac{(\\xi-a)^2}{2B}-\\frac{1}{2}\\log2\\pi B.\n\\]\nここまで単純化した設定でも，観測 \\(y_i\\) のカテゴリが不均衡ならば，前述の Gibbs サンプラーの収束鈍化が見られることを検証する： \\[\n\\sum_{i=1}^ny_i=1.\n\\]\n\n\n\n\n\n\n展開してコードを見る\n\n\n\n\n\nMetropolis-Hastings 法は，Turing Institute による Julia の AdvancedMH.jl パッケージなどを通じて実装することができる：\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nusing AdvancedMH\nusing Distributions\nusing MCMCChains\nusing ForwardDiff\nusing StructArrays\nusing LinearAlgebra\nusing LogDensityProblems\nusing LogDensityProblemsAD\n\n(a,B) = (0,100.0)\n\n# Define the components of a basic model.\nstruct LogTargetDensity_Logistic\n    a::Float64\n    B::Float64\n    n::Int64\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensity_Logistic, ξ) = -log(2π * p.B) - (ξ[1] - p.a)^2/(2 * p.B) + ξ[1] - p.n * log(1 + exp(ξ[1]))\nLogDensityProblems.dimension(p::LogTargetDensity_Logistic) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensity_Logistic}) = LogDensityProblems.LogDensityOrder{0}()\n\nfunction MHSampler(n::Int64; discard_initial=30000)\n\n    model_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensity_Logistic(a, B, n))\n\n    spl = RWMH(MvNormal(zeros(1), I))\n\n    chain = sample(model_with_ad, spl, 50000; chain_type=Chains, param_names=[\"ξ\"])\n\n    return chain\nend\n\n# ξ_vector = MHSampler(10000)\n# plot(ξ_vector, title=\"Plot of \\$\\\\xi\\$ values\", xlabel=\"Index\", ylabel=\"ξ\", legend=false, color=\"#78C2AD\")\n\nMHSampler (generic function with 1 method)\n\n\n\nusing DataFrames\nusing Plots\n\nn_list = [10, 100, 1000, 10000]\n\nelapsed_time_Metropolis = @elapsed begin\n    chains = [MHSampler(n) for n in n_list]\nend\n\nautos = [DataFrame(autocor(chain, lags=1:100)) for chain in chains]\n\nMHChain = chains\n\ncombined_df = vcat(autos..., source=:chain)\n\nlag_columns = names(combined_df)[2:101]\nlags = 1:100\n\np_Metropolis = plot(\n    title = \"Metropolis\",\n    xlabel = \"Lag\",\n    ylabel = \"Autocorrelation\",\n    legend = :topright,\n    #background_color = \"#F0F1EB\"\n)\n\nfor (i, n) in zip(1:4, n_list)\n    plot!(\n        p_Metropolis,\n        lags,\n        Array(combined_df[i, lag_columns]),\n        label = \"n = $n\",\n        linewidth = 2\n    )\nend\n\nパッケージ PolyaGammaSamplers は現在，過去のバージョンの依存関係を必要とするので，グローバルの環境とは分離しておくのが良い．\nここでは，Pólya-Gamma 分布のサンプラーの実装 PolyaGammaSamplers を参考にして，直接次のように定義する．\n\nusing Random\nusing StatsFuns\n\nstruct PolyaGammaPSWSampler{T &lt;: Real} &lt;: Sampleable{Univariate, Continuous}\n    b::Int\n    z::T\nend\n\nstruct JStarPSWSampler{T &lt;: Real} &lt;: Sampleable{Univariate, Continuous}\n    z::T\nend\n\nfunction Base.rand(rng::AbstractRNG, s::PolyaGammaPSWSampler)\n    out = 0.0\n    s_aux = JStarPSWSampler(s.z / 2)\n    for _ in 1:s.b\n        out += rand(rng, s_aux) / 4\n    end\n    return out\nend\n\nfunction Base.rand(rng::AbstractRNG, s::JStarPSWSampler)\n    z = abs(s.z)  # modified to avoid negative z\n    t = 0.64\n    μ = 1 / z\n    k = π^2 / 8 + z^2 / 2\n    p = (π / 2 / k) * exp(- k * t) \n    q = 2 * exp( - z) * cdf(InverseGaussian(μ, 1.0), t)\n    while true\n        # Simulate a candidate x\n        u = rand(rng)\n        v = rand(rng)\n        if (u &lt; p / (p + q))\n            # (Truncated Exponential)\n            e = randexp(rng)\n            x = t + e / k\n        else\n            # (Truncated Inverse Gaussian)\n            x = randtigauss(rng, z, t)\n        end\n        # Evaluate if the candidate should be accepted\n        s = a_xnt(x, 0, t)\n        y = v * s\n        n = 0\n        while true\n            n += 1\n            if (n % 2 == 1)\n                s += a_xnt(x, n, t)\n                y &gt; s && break\n            else\n                s -= a_xnt(x, n, t)\n                y &lt; s && return x\n            end\n        end\n    end\nend\n\n# Return ``a_n(x)`` for a given t, see [1], eqs. (12)-(13)\n# Equations (12)-(13) in [1]\n# Note: \n# This is a literal transcription from the article's formula\n# except for the letter case\nfunction a_xnt(x::Real, n::Int, t::Real)\n    x ≤ t ? a_xnt_left(x, n, t) : a_xnt_right(x, n, t)\nend\n\n# Return ``a_n(x)^L`` for a given t\n# Equation (12) in [1]\n# Note: \n# This is a literal transcription from the article's formula\n# except for the letter case\nfunction a_xnt_left(x::Real, n::Int, t::Real)\n    π * (n + 0.5) * (2 / π / x)^(3 / 2) * exp(- 2 * (n + 0.5)^2 / x)\nend\n\n# Return ``a_n(x)^R`` for a given t, see [1], eq. (13)\n# Equation (13) in [1]\n# Note: \n# This is a literal transcription from the article's formula\n# except for the letter case\nfunction a_xnt_right(x::Real, n::Int, t::Real)\n    π * (n + 0.5) * exp(- (n + 0.5)^2 * π^2 * x / 2)\nend\n\n# Simulate from an IG(μ, 1) distribution\n# Algorithms 2-3 in [1]'s supplementary material\n# Note: \n# This is a literal transcription from the article's pseudo code\n# except for the letter case\nfunction randtigauss(rng::AbstractRNG, z::Real, t::Real)\n    1 / z &gt; t ? randtigauss_v1(rng, z, t) : randtigauss_v2(rng, z, t)\nend\n\n# Simulate from an IG(μ, 1) distribution, for μ := 1 / z &gt; t;\n# Algorithms 2 in [1]'s supplementary material\n# Note:\n# This is a literal transcription from the article's pseudo code\n# except for the letter case and one little a detail: the \n# original condition  `x &gt; R` must be replaced by `x &gt; t`\nfunction randtigauss_v1(rng::AbstractRNG, z::Real, t::Real)\n    x = t + one(t)\n    α = zero(t)\n    while rand(rng) &gt; α\n        e = randexp(rng) # In [1]: E \n        é = randexp(rng) # In [1]: E'\n        while e^2 &gt; (2 * é / t)\n            e = randexp(rng)\n            é = randexp(rng)\n        end\n        x = t / (1 + t * e)^2 \n        α = exp(- z^2 * x / 2)\n    end\n    return x\nend\n\n# Simulate from an IG(μ, 1) distribution, for μ := 1 / z ≤ t\n# Algorithms 3 in [1]'s supplementary material\n# Note: This is a literal transcription from the article's pseudo code\nfunction randtigauss_v2(rng::AbstractRNG, z::Real, t::Real)\n    x = t + one(t)\n    μ = 1 / z\n    while x &gt; t \n        y = randn(rng)^2\n        x = μ + μ^2 * y / 2 - μ * √(4 * μ * y + (μ * y)^2) / 2\n        if rand(rng) &gt; μ / (μ + x)\n            x = μ^2 / x\n        end\n    end\n    return x\nend\n\nrandtigauss_v2 (generic function with 1 method)\n\n\n\n# using PolyaGammaSamplers\n\nfunction PGSampler(n::Int64; discard_initial=30000, iter_number=50000, initial_ξ=0.0, B=100)\n\n    λ = 1 - n/2\n\n    ξ_list = [initial_ξ]\n    ω_list = []\n\n    while length(ξ_list) &lt; iter_number\n        ξ = ξ_list[end]\n        ω_sampler = PolyaGammaPSWSampler(n, ξ)\n        ω_new = rand(ω_sampler)\n        push!(ω_list, ω_new)\n        ξ_sampler = Normal((ω_new + B^(-1))^(-1) * λ, (ω_new + B^(-1))^(-1))\n        ξ_new = rand(ξ_sampler)\n        push!(ξ_list, ξ_new)\n    end\n\n    return Chains(ξ_list[discard_initial+1:end])\nend\n\nfunction Distributions.mean(s::PolyaGammaPSWSampler)\n    s.b * inv(2.0 * s.z) * tanh(s.z / 2.0)\nend\n\nfunction Distributions.var(s::PolyaGammaPSWSampler)\n    s.b * inv(4 * s.z^3) * (sinh(s.z) - s.z) * (sech(s.z / 2)^2)\nend\n\n\nelapsed_time_PolyaGamma = @elapsed begin\n    chains = [PGSampler(n) for n in n_list]\nend\nautos = [DataFrame(autocor(chain, lags=1:100)) for chain in chains]\n\nPGChain = chains\n\ncombined_df = vcat(autos..., source=:chain)\n\nlag_columns = names(combined_df)[2:101]\nlags = 1:100\n\np_PolyaGamma = plot(\n    title = \"Pólya-Gamma\",\n    xlabel = \"Lag\",\n    ylabel = \"Autocorrelation\",\n    legend = (0.65, 0.35),\n    #background_color = \"#F0F1EB\"\n)\n\nfor (i, n) in zip(1:4, n_list)\n    plot!(\n        p_PolyaGamma,\n        lags,\n        Array(combined_df[i, lag_columns]),\n        label = \"n = $n\",\n        linewidth = 2,\n    )\nend\n\n\nprintln(\"Elapsed time: $elapsed_time_Metropolis seconds v.s. $elapsed_time_PolyaGamma seconds\")\n\nElapsed time: 2.0604135 seconds v.s. 79.162613875 seconds\n\n\nPG サンプラーは MH 法に比べ恐ろしいほどに時間がかかる．これは，Turing のパッケージの最適化が優秀であるのか，Pólya-Gamma サンプラーの宿命であるのか，引き続き調べる必要がある．\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6 理論：収束鈍化の理由\n前節の \\(p=1\\) の不均衡プロビットモデルの下での計算複雑性のオーダーは，\n\nMetropolis 法では最悪で \\((\\log n)^3\\)\nGibbs サンプラーでは最高でも \\(n^{3/2}(\\log n)^{2.5}\\)\n\nになることが示されている (Section 4 Johndrow et al., 2019)．\nその理由は，提案分布と対象分布のズレに由来することも (Johndrow et al., 2019) は明らかにしている．\n\\(\\sum_{i=1}^ny_i\\) の値を固定して \\(n\\to\\infty\\) の極限を取った場合，事後分布は次のように負方向にスライドしながら，幅が狭まっていく．\nその幅の縮小レートは \\(n^{-1/2}\\) ではなく，約 \\((\\log n)^{-1}\\) になる．\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n一方で提案分布は \\(\\xi_t\\) をモードとした場合，\\(\\xi_{t+1}\\) もモードの周りに幅 \\(\\frac{(\\log n)^{3/2}}{n^{1/2}}\\) で集中してしまう．すなわち，提案のステップサイズが事後分布のスケールに比べて極めて小さくなってしまう．\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMH 法でサンプルした事後分布に比べて，より鋭くなっていることがわかるだろう（\\(y\\) 軸のスケールに注目）．\nGibbs サンプラーではステップサイズがあまりに小さくなってしまい，分布の十分な探索が阻害され，サンプル間の自己相関が高くなってしまうという問題が起こるようである．"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#zig-zag-サンプラーによる解決",
    "href": "posts/2024/Stat/Logistic.html#zig-zag-サンプラーによる解決",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "3 Zig-Zag サンプラーによる解決",
    "text": "3 Zig-Zag サンプラーによる解決\n後編に続く：\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Stat/Logistic.html#文献紹介",
    "href": "posts/2024/Stat/Logistic.html#文献紹介",
    "title": "大規模な不均衡データに対するロジスティック回帰（前編）",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\nロジスティック回帰やプロビット回帰などの２項回帰モデルは，広くベイズ計算手法のベンチマークとして用いられるモデルである (Chopin and Ridgway, 2017)．\n二項回帰モデルにおいて観測 \\(y_{i}\\) の値が激しく偏っていた場合，Gibbs サンプラーの収束に問題が起きることは (Johndrow et al., 2019) が指摘している．\nこの問題が Zig-Zag サンプラーを用いれば，重点荷重＋ミニバッチサブサンプリングのアイデアで比較的簡単に解決できることが (Sen et al., 2020) で実証された．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html",
    "href": "posts/2024/Stat/ZigZagSubsampling.html",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nMCMC の計算複雑性のボトルネックは，尤度の評価にある．各ステップで全てのデータを用いて尤度を計算する必要がある点が，MCMC を深層学習などの大規模データの設定への応用を難しくしている (Murphy, 2023, p. 647)．\nサブサンプリングが可能であることと，複数の効率的なサブサンプリング法の提案により，Zig-Zag 過程は次世代のサンプラーとして圧倒的なスケーラビリティ（Super Efficient Bayesian Inference (Bierkens et al., 2019)）を示すのではないかと期待されている．1"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#対数尤度の勾配を不変推定する",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#対数尤度の勾配を不変推定する",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "1 対数尤度の勾配を不変推定する",
    "text": "1 対数尤度の勾配を不変推定する\n\\(p(x)\\) を事前分布，\\(p(y|x)\\) を観測のモデル（または尤度）とし，データ \\(y_1,\\cdots,y_n\\) は互いに独立であるとする．\nこのとき，事後分布 \\(\\pi(x):=p(x|y)\\) と Hamiltonian \\(U\\) は次のように表せる： \\[\n\\pi(x)\\,\\propto\\,\\left(\\prod_{k=1}^n p(y_k|x)\\right)p(x)\n\\] \\[\\begin{align*}\n   U(x)&=-\\sum_{k=1}^n\\log p(y_k|x)-\\log p(x)\\\\\n   &=\\frac{1}{n}\\sum_{k=1}^n\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl)=:\\frac{1}{n}\\sum_{k=1}^nU^k(x).\n\\end{align*}\\]\nこのとき，\\(U\\) の導関数 \\(\\partial_i U(x)\\) は，独立な観測 \\(y_1,\\cdots,y_n\\) について項別微分をして平均をとったものに等しい： \\[\n\\partial_iU(x)=\\frac{1}{n}\\sum_{k=1}^nE^k_i(x),\n\\tag{1}\\] \\[\nE^k_i(x):=\\partial_iU^k(x)=\\frac{\\partial }{\\partial x_i}\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl).\n\\]\nよって，精度は劣るかもしれないが，一様に選んだ \\(K\\sim\\mathrm{U}([n])\\) から定まる \\(E^K_i\\) の値は \\(\\partial_i U(x)\\) の不偏推定量となっている．この発想により，ZZ-SS という新たなアルゴリズムを構成できる．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#サブサンプリングを取り入れた-zig-zag-サンプラー",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#サブサンプリングを取り入れた-zig-zag-サンプラー",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "2 サブサンプリングを取り入れた Zig-Zag サンプラー",
    "text": "2 サブサンプリングを取り入れた Zig-Zag サンプラー\nこの各 \\(E^K_i\\) が定める強度関数 \\[\nm^K_i(t):=\\biggr(\\theta E^K_i(x+\\theta t)\\biggl)_+=\\biggr(\\theta\\partial_iU^K(x+\\theta t)\\biggl)_+\n\\] を用いた Zig-Zag サンプラーを (Bierkens et al., 2019) では ZZ-SS (Zig-Zag with Sub-Sampling) と呼んでいる．\n\\[\n\\max_{k\\in[n]}m^k_i\\le M_i\n\\] を満たす連続関数 \\(M_i\\) を用いて次のようにシミュレーションすることができる：\n\n\n\n\n\n\n(Bierkens et al., 2019, p. 1303 アルゴリズム３)\n\n\n\n\n代理強度関数 \\(M_1,\\cdots,M_d\\) を持つ互いに独立な \\(\\mathbb{R}_+\\) 上の非一様 Poisson 点過程の到着時刻 \\(T_1,\\cdots,T_d\\) をシミュレーションする．\n最初に到着した座標番号 \\(j:=\\operatorname*{argmin}_{i\\in[d]}T_i\\) について，確率 \\[\n\\frac{m^K_j(T_j)}{M_j(T_j)},\\qquad K\\sim\\mathrm{U}([n]),\n\\] で時刻 \\(T_j\\) に速度成分 \\(\\theta_j\\) の符号を反転させる．\n１に \\(t=T_j\\) として戻って，繰り返す．\n\n\n\n\n\n\n\n\n\n部分サンプリングにより不変分布が変わらないことの証明2\n\n\n\n\n\nZZ-SS によってシミュレートされる過程は，レート関数 \\[\n\\lambda_i(x,\\theta)=\\operatorname{E}[(\\theta E^K_i(x))_+]=\\frac{1}{n}\\sum_{k=1}^n(\\theta E^k_i(x))_+\n\\] を持った Zig-Zag 過程に等しい\nこれは，元々のレート関数に対して， \\[\n\\gamma_i(x,\\theta):=\\frac{1}{n}\\sum_{k=1}^n(\\theta_iE^k_i(x))_+-\\left(\\theta_i\\frac{1}{n}\\sum_{k=1}^nE^k_i(x)\\right)_+\n\\] という項を加えて得る Zig-Zag サンプラーともみなすことができる．非負性は関数 \\((x)_+:=x\\lor0\\) の凸性から従う．最後に \\(\\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta))\\) を確認すれば良い．\nこれは \\[\\begin{align*}\n  &\\qquad\\frac{1}{n}\\sum_{k=1}^n\\biggr(\\theta_iE_i^k(x)\\biggl)_+-\\frac{1}{n}\\sum_{k=1}^n\\biggr(-\\theta_iE_i^k(x)\\biggl)_+\\\\\n  &=\\frac{1}{n}\\sum_{k=1}^n\\left((\\theta_iE_i^k(x))_+-(-\\theta_iE_i^k(x))_+\\right)=\\frac{1}{n}\\sum_{k=1}^n\\theta_iE_i^k(x)\n\\end{align*}\\] であることから従う．\nこうして，サブサンプリングの実行による精度の劣化が，(Andrieu and Livingstone, 2021) の枠組みで捉えられる，ということでもある（レート関数が増加したので，スイッチングイベントが増え，diffusive な動きが増加する）．\n\n\n\n例えば \\(p(y_k|x)\\) が Cauchy 密度であるなど \\(\\partial_iU\\) が有界であるとき，\\(M_i:=\\max_{x\\in\\mathbb{R}^d}\\partial_iU(x)\\) などと選ぶことができる．\\(M_i\\) をより \\(\\partial_iU\\) に近く選ぶほど剪定の効率は上がるが，\\(M_i\\) を複雑にしすぎると今度は \\(M_i\\) を強度とする Poisson 点過程のシミュレーションが困難になる．\nそのため，ZZ-SS では代理レート関数 \\(M_i\\) は大きく取る必要があり，尤度関数の評価の回数が増える．そのため，アルゴリズムの計算複雑性は上がっていることに注意 (Bierkens et al., 2019, p. 1302 第４節)．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#sec-ZZ-CV",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#sec-ZZ-CV",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "3 制御変数による分散低減",
    "text": "3 制御変数による分散低減\n\\(\\partial_iU(x)\\) が Lipschitz 連続であるとき，\\(E_i^k\\) をある参照点 \\(\\partial_iU(x_*)\\) とそこからの乖離と取ることで \\(n\\to\\infty\\) の極限で分散が抑えられる．\nこうすることで，\\(M_i\\) を１次関数としたまま，より小さく \\(E_i^k\\) にフィットするように取ることができる．\n\n\n\n\n\n\n命題\n\n\n\n任意の \\(i\\in[d]\\) について，ある \\(C_i&gt;0\\) が存在して， \\[\n\\lvert\\partial_iU(x)-\\partial_iU(y)\\rvert\\le C_i\\lvert x-y\\rvert,\\quad(x,y)\\in\\mathbb{R}^{2d},\n\\] が成り立つとする．このとき， \\[\nM_i(t):=a_i+b_it\n\\] \\[\na_i:=(\\theta_i\\partial_iU(x_*))_++C_i\\|x-x_*\\|_p,\\quad b_i:=C_id^{1/p}\n\\] と定めれば， \\[\nm_i^k\\le M_i\n\\] が成り立つ．ただし， \\[\n\\partial_iU(x)=\\frac{1}{n}\\sum_{k=1}^nE^k_i(x),\\tag{1}\n\\] \\[\nE^k_i(x):=\\partial_iU(x_*)+\\partial_iU^k(x)-\\partial_iU^k(x_*),\n\\] \\[\nm^k_i(t):=\\biggr(\\theta E_i^k(x+\\theta t)\\biggl)_+,\n\\] とした．\n\n\nこの仮定は例えば \\(\\partial_iU\\) が有界な導関数を持つならば成り立つ．\\(p(y_k|x)\\) が Gauss 密度であるやさらに裾が重いときは成り立つ．\n次のようにして参照点 \\(x_*\\) を選ぶ事前処理を行うことで，データのサイズに依存しない計算複雑性で事後分布からの正確なサンプリングが可能になる．\n\n\n\n\n\n\npreprocessing for ZZ-CV\n\n\n\n\n\\(x_*:=\\operatorname*{argmin}_{x\\in\\mathbb{R}^d}U(x)\\) を探索する．\n\\(\\partial_iU(x_*),\\partial_iU^k(x_*)\\) を計算する．\n\nこの２つはいずれも \\(O(n)\\) の複雑性で実行できる．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#zz-cv-のスケーリング",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#zz-cv-のスケーリング",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "4 ZZ-CV のスケーリング",
    "text": "4 ZZ-CV のスケーリング\nこのとき，\\(x_*\\) を定める事前処理が，\\(\\widehat{x}\\) を最尤推定量として， \\[\n\\|x_*-\\widehat{x}\\|_p=O(n^{-1/2})\\quad(n\\to\\infty)\n\\] 程度の正確性があれば，事後分布の最尤推定量周りの漸近展開 (Johnson, 1970) を通じて， \\[\n\\|x-x_*\\|_p=O_p(n^{-1/2})\\quad(n\\to\\infty)\n\\] \\[\n\\partial_iU(x_*)=O_p(n^{1/2})\\quad(n\\to\\infty)\n\\] が成り立つ．\n\n\n\n\n\n\nZig-Zag 過程のスケーリング3\n\n\n\n\n\n事後分布に対する Zig-Zag 過程は，\\(\\sqrt{n}\\) だけ時間を加速したものが \\(\\mathrm{N}_d(0,i(x_0))\\) を標的にする Zig-Zag 過程に収束するから，\\(O(n^{-1/2})\\) のタイムステップで区切ってサンプルとすることができる．\nしかし \\[\n\\max_{k\\in[n]}\\biggr(\\theta_i\\partial_iU^k(x+\\theta t)\\biggl)_+\\le M_i\n\\] を満たす \\(M_i\\) は \\(O(n^{\\alpha})\\;(\\alpha\\ge1/2)\\) のスケールで増大していく．\n各スイッチングイベントにおいて，全データにアクセスする \\(O(n)\\) の計算複雑性が必要であるから，総じて \\(O(n^{\\alpha+1/2})\\) の計算複雑性となる．\n\n\n\nZZ-CV が平衡に至っている場合は \\(x\\) はほとんど \\(x_*\\) に集積するため， \\[\n\\lvert E^k_i(x)\\rvert=\\biggl|\\partial_iU(x_*)+\\partial_iU^k(x)-\\partial_iU^k(x_*)\\biggr|=O(n^{1/2})\n\\] が成り立つ．よってこれを抑える \\(M_i\\) も \\(O(n^{1/2})\\) で済み，必要以上に大きい代理レート関数を用意して剪定する必要がない．\n全データにアクセスする \\(O(n)\\) のステップもないために，事前処理 3 と十分平衡に至っているとみなせるまでの burn-in を除いて，\\(O(1)\\) の計算複雑性でサンプリングが可能である．このことを (Bierkens et al., 2019) は super-efficiency と呼ぶ．\n\n\n\n\n\n\n更なるスケーラブル手法の可能性（事後分布が集中する場合のみ？）\n\n\n\n\n\n他に，事後分布の集中領域でうまくスイッチング回数が抑えられる \\(\\lambda_i\\) が構成できたならば，低い計算複雑性を達成できるだろう．\nZZ-CV では，これに事後分布の Gauss 近似を用いたことになる．\nまた，\\(U\\) の２階微分が有界でない場合，この枠組みが使えない．実際，(Bierkens et al., 2019, pp. 1315 第6.5節) ではこの場合での数値実験の結果が示されており，事後分布が集積しないために super-efficiency は得られていない．\n参照点 \\(x_*\\) を複数取る拡張なども (Bierkens et al., 2019, p. 第7節) で考えられている．"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#数値実験mse-の比較",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#数値実験mse-の比較",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "5 数値実験：MSE の比較",
    "text": "5 数値実験：MSE の比較\nある Gauss 分布に従うデータを生成する： \\[\nY^j\\overset{\\text{iid}}{\\sim}\\mathrm{N}(x_0,\\sigma^2),\\qquad j\\in[n],\n\\] 分散 \\(\\sigma^2\\) は既知として，位置母数 \\(x\\in\\mathbb{R}\\) を推定する問題を考える．\n事前分布を \\(\\mathrm{N}(0,\\rho^2)\\) とすると，定数の違いを除いて \\[\\begin{align*}\n    U(x)&=\\frac{x^2}{2\\rho^2}+\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x-y^j)^2\\\\\n    &=\\frac{1}{n}\\sum_{j=1}^n\\left(\\frac{x^2}{2\\rho^2}+\\frac{n}{2\\sigma^2}(x-y^j)^2\\right)=:\\frac{1}{n}\\sum_{j=1}^nU^j(x)\n\\end{align*}\\] であるから， \\[\\begin{align*}\n    U'(x)&=\\frac{x}{\\rho^2}+\\frac{1}{\\sigma^2}\\sum_{j=1}^n(x-y^j)\\\\\n    &=\\frac{x}{\\rho^2}+\\frac{n}{\\sigma^2}(x-\\overline{y}),\n\\end{align*}\\] \\[\nU''(x)=\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}.\n\\]\n従って，Zig-Zag 過程のイベントの強度関数は \\[\\begin{align*}\n    m(t)&=\\biggr(\\theta U'(x+\\theta t)\\biggl)_+\\\\\n    &=\\left(\\frac{\\theta(x+\\theta t)}{\\rho^2}+\\frac{\\theta}{\\sigma^2}\\sum_{j=1}^n(x+\\theta t-y^j)\\right)_+\\\\\n    &=\\left(\\frac{\\theta x}{\\rho^2}+\\frac{\\theta}{\\sigma^2}\\sum_{j=1}^n(x-y^j)+t\\left(\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}\\right)\\right)_+\n\\end{align*}\\] と表せ，これは１次関数 \\((a+bt)_+\\) の形であるから直接のシミュレーションが可能である．4\n\n\nサブサンプリングなしの Zig-Zag 過程のシミュレーションをする関数 ZZ() を定義\nusing ZigZagBoomerang\nusing Distributions\nusing Random\n\nλ(∇U, x, θ, F::ZigZag1d) = pos(θ*∇U(x)) # rate function on E\nλ_bar(τ, a, b) = pos(a + b*τ)  # affine proxy\n\n\"\"\"\n`x`: current location, `θ`: current velocity, `t`: current time,\n\"\"\"\nfunction move_forward(τ, t, x, θ, ::ZigZag1d)\n    τ + t, x + θ*τ , θ\nend\n\n\"\"\"\n    `∇U`: gradient of the negative log-density\n    `(x,θ)`: initial state\n    `T`: Time Horizon    \n    `a+bt`: computational bound for intensity m(t)\n\n    `num`: ポアソン時刻に到着した回数\n    `acc`: 受容回数．`acc/num` は acceptance rate\n\"\"\"\nfunction ZZ(∇U, x::Float64, θ::Float64, T::Float64, y, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_ZZ)\n    t = zero(T)\n    Ξ = [(t, x, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(x, θ, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, x, θ = move_forward(τ, t, x, θ, Flow)\n        l, lb = λ(∇U, x, θ, Flow), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, x, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(x, θ, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n今回の設定に応じたレート関数 (a+bt)+ を用意\npos(x) = max(zero(x), x)  # positive part\na(x, θ, ρ, σ, y) = θ * x / ρ^2 + (θ/σ^2) * sum(x .- y)\nb(x, θ, ρ, σ, y) = ρ^(-2) + length(y)/σ^2\n\nρ, σ, x0, θ0 = 1.0, 1.0, 1.0, 1.0\nn1, n2 = 100, 10^4\nTrueDistribution = Normal(x0, σ)\ny1 = rand(TrueDistribution, n1)\ny2 = rand(TrueDistribution, n2)\n\n# computational bounds for intensity m(t)\nab_ZZ_n1(x, θ, ::ZigZag1d) = (a(x, θ, ρ, σ, y1), b(x, θ, ρ, σ, y1))\nab_ZZ_n2(x, θ, ::ZigZag1d) = (a(x, θ, ρ, σ, y2), b(x, θ, ρ, σ, y2))\n\n∇U1(x) = x/ρ^2 + (length(y1)/σ^2) * (x - mean(y1)) \n∇U2(x) = x/ρ^2 + (length(y2)/σ^2) * (x - mean(y2)) \n\n# T = 2500.0\n# trace_ZZ1, epochs_ZZ1, acc_ZZ1 = ZZ(∇U1, x0, θ0, T, ZigZag1d(); ab=ab_ZZ_n1)\n# trace_ZZ2, num_ZZ2, acc_ZZ2 = ZZ(∇U2, x0, θ0, T, ZigZag1d(); ab=ab_ZZ_n2)\n# dt = 0.01\n# traj_ZZ1 = discretize(trace_ZZ1, ZigZag1d(), dt)\n# traj_ZZ2 = discretize(trace_ZZ2, ZigZag1d(), dt)\n\n\n\n\nN 回 ZZ() を実行して，その事後平均の MSE を計算する関数 experiment() を定義\nfunction SquaredError(sample::Vector{Float64}, y)\n    True_Posterior_Mean = sum(y) / (length(y) + 1)\n    return (mean(sample) - True_Posterior_Mean)^2\nend\n\n\"\"\"\n    epoch_list: 注目するエポック数のリスト\n    N: 実験回数\n\"\"\"\nfunction experiment(epoch_list, T, dt, N, ∇U, x0, θ0, y, Sampler; ab=ab_ZZ_n1)\n    SE_sum = zero(epoch_list)\n    acc_list = []\n    for _ in 1:N\n        trace_ZZ1, epochs_ZZ1, acc_ZZ1 = Sampler(∇U, x0, θ0, T, y, ZigZag1d(); ab=ab)\n        push!(acc_list, acc_ZZ1)\n        traj_ZZ1 = discretize(trace_ZZ1, ZigZag1d(), dt)\n        SE_list = []\n        for T in epoch_list\n            epoch = findfirst(x -&gt; x &gt; T, epochs_ZZ1) - 1\n            t = findfirst(x -&gt; x &gt; trace_ZZ1[epoch][1], traj_ZZ1.t) - 1\n            SE = SquaredError(traj_ZZ1.x[1:t], y)\n            push!(SE_list, SE)\n        end\n        SE_sum += SE_list\n    end\n    return SE_sum ./ N, mean(acc_list)\nend\n\n\n\n\n実験の実行\nusing Plots\n\nT = 3000.0\nepoch_list = [10.0, 100.0, 1000.0, 10000.0]\ndt = 0.01\nN = 11\n\nMSE_ZZ1, acc = experiment(epoch_list, T, dt, N, ∇U1, x0, θ0, y1, ZZ; ab=ab_ZZ_n1)\np = plot(#epoch_list, MSE_ZZ1,\n    xscale=:log10,\n    yscale=:log10,\n    xlabel=\"epochs\",\n    ylabel=\"MSE\"\n    # ,background_color = \"#F0F1EB\"\n    )\nscatter!(p, epoch_list, MSE_ZZ1,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"#78C2AD\",\n    label=nothing\n    )\n\nusing GLM, DataFrames\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_ZZ1))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(p, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:solid,\n    linewidth=2,\n    color=\"#78C2AD\",\n    label=\"ZZ\"\n    )\n\n# display(p)\n\nprintln(\"Average acceptance rate: $acc\")\n\n\nAverage acceptance rate: 1.0\n\n\nより，たしかに剪定なしの正確なシミュレーションができている．\n一方で， \\[\nU^j(x):=\\frac{x^2}{2\\rho^2}+\\frac{n}{2\\sigma^2}(x-y^j)^2,\n\\] \\[\n\\lambda^j(x,\\theta):=\\biggr(\\theta(U^j)'(x)\\biggl)_+\n\\] としてサブサンプリングを取り入れることを考えるが，これを同じ \\((a+bt)_+\\) ではバウンド出来ない：\n\n\nZZ-SS (ZigZag with Subsampling) の定義\nλj(j,x,θ,y) = pos(θ * (x/ρ^2 + length(y)/σ^2 * (x - y[j])))\n\nfunction ZZ_SS(∇U, x::Float64, θ::Float64, T::Float64, y, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_ZZ)\n    t = zero(T)\n    Ξ = [(t, x, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(x, θ, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, x, θ = move_forward(τ, t, x, θ, Flow)\n        j = rand(1:length(y))\n        l, lb = λj(j, x, θ, y), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            if l &gt; lb + 0.01\n                # println(l-lb)\n                acc += 1  #  overflow を数えるように変更済み！注意！\n            end\n            θ = -θ\n            push!(Ξ, (t, x, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(x, θ, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n実験の実行\nusing LaTeXStrings\n\nMSE_ZZ_SS, acc = experiment(epoch_list, T, dt, N, ∇U1, x0, θ0, y1, ZZ_SS; ab=ab_ZZ_n1)\nprintln(L\"上界 $(a+bt)_+$ を超えてしまう平均的割合: \", \"$acc\")\n\n\n上界 $(a+bt)_+$ を超えてしまう平均的割合: 0.49500662479838825\n\n\nしかし，ZZ-CV アルゴリズムではこのようなことは起こらない．実際，次の等式が成り立つ： \\[\nU'(x_*)+(U^j)'(x)-(U^j)'(x_*)=U'(x),\\qquad x,x_*\\in\\mathbb{R}.\n\\]\nこのモデルにおける MAP 推定量は \\[\n\\widehat{x}:=\\frac{\\overline{y}}{1+\\frac{\\sigma^2}{n\\rho^2}}\n\\] である．\n\n\nZZ-CV (ZigZag with Control Variates) の定義\nx_star = mean(y1) / (1 + σ^2/(length(y1) * ρ^2))\n\nC(ρ, σ, y) = ρ^(-2) + length(y)/σ^2\na(x, θ, ρ, σ, y) = pos(θ*∇U1(x_star)) + C(ρ, σ, y) * abs(x - x_star)\nb(x, θ, ρ, σ, y) = C(ρ, σ, y)\n\n# New Computational Bounds for ZZ-CV\nab_ZZ_CV(x, θ, ::ZigZag1d) = (a(x, θ, ρ, σ, y1), b(x, θ, ρ, σ, y1))\n\nfunction ZZ_CV(∇U, x::Float64, θ::Float64, T::Float64, y, Flow::ZigZagBoomerang.ContinuousDynamics; rng=Random.GLOBAL_RNG, ab=ab_ZZ_CV)\n    t = zero(T)\n    Ξ = [(t, x, θ)]\n    num = acc = 0\n    epoch_list = [num]\n    a, b = ab(x, θ, Flow)\n    t′ =  t + poisson_time(a, b, rand())  # イベントは a,b が定める affine proxy に従って生成する\n\n    while t &lt; T\n        τ = t′ - t\n        t, x, θ = move_forward(τ, t, x, θ, Flow)\n        # j = rand(1:length(y))  # 今回はたまたま要らない\n        l, lb =λ(∇U, x, θ, Flow), λ_bar(τ, a, b)  # λ が真のレート, λ_bar が affine proxy\n        num += 1\n        if rand()*lb &lt; l\n            acc += 1\n            if l &gt; lb + 0.01\n                println(l-lb)\n            end\n            θ = -θ\n            push!(Ξ, (t, x, θ))\n            push!(epoch_list, num)\n        end\n        a, b = ab(x, θ, Flow)\n        t′ = t + poisson_time(a, b, rand())\n    end\n\n    return Ξ, epoch_list, acc/num\nend\n\n\n\n\n実験の実行\nMSE_ZZ_CV, acc = experiment(epoch_list, T, dt, N, ∇U1, x0, θ0, y1, ZZ_CV; ab=ab_ZZ_CV)\n\nq = scatter(p, epoch_list, MSE_ZZ_CV,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"#E95420\",\n    label=nothing\n    )\n\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_ZZ_CV))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(q, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:dash,\n    linewidth=2,\n    color=\"#E95420\",\n    label=\"ZZ-CV (without amendment)\"\n    )\n\ndisplay(q)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n一見すると ZZ-CV が負けているように見える．しかし，点線で描いているのは，横軸が epoch であることを正しく考慮していない間違ったプロットであるためである．\n(Bierkens et al., 2019, p. 1310) において epoch とは，計算量の１単位分としており，ZZ における１回の到着時刻のシミュレーションは，ZZ-CV の \\(n\\) 回分に当たる．これを考慮に入れてプロットし直すと次の通りになる：\n\n\n実験の実行\nT_SuperEfficient = 300000.0\nepoch_list_SuperEfficient = [1000.0, 10000.0, 100000.0, 1000000.0]\n\n@time MSE_ZZ_CV, acc = experiment(epoch_list_SuperEfficient, T_SuperEfficient, dt, N, ∇U1, x0, θ0, y1, ZZ_CV; ab=ab_ZZ_CV)\n\nscatter!(p, epoch_list, MSE_ZZ_CV,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"#E95420\",\n    label=nothing\n    )\n\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_ZZ_CV))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(p, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:solid,\n    linewidth=2,\n    color=\"#E95420\",\n    label=\"ZZ-CV\"\n    )\n\ndisplay(p)\n\n\n 48.490704 seconds (2.48 G allocations: 46.290 GiB, 7.66% gc time)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nこれは換言すれば横軸が「ズルい」ということでもあるが，同時に \\(n\\to\\infty\\) の極限では，圧倒的に ZZ-CV が効率的になるということでもある．5"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#mala-との比較",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#mala-との比較",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "6 MALA との比較",
    "text": "6 MALA との比較\n\n\nMALA のセットアップ\nusing AdvancedHMC, AdvancedMH, ForwardDiff\nusing LogDensityProblems\nusing LogDensityProblemsAD\nusing StructArrays\nusing LinearAlgebra\n\nstruct LogTargetDensity\n    y::Vector{Float64}\nend\n\nfunction U(i, x, y)\n    x[1] * x[1] / (2 * ρ * ρ) + length(y) * (x[1] - y[i]) * (x[1] - y[i]) / (2 * σ * σ)  # 自動微分のために x は長さ1のベクトルと扱う必要がある\nend\n\nfunction U(x, y)\n    vec = [U(i, x, y) for i in 1:length(y)]\n    return mean(vec)\nend\n\nLogDensityProblems.logdensity(p::LogTargetDensity, x) = U(x, p.y)\nLogDensityProblems.dimension(p::LogTargetDensity) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensity}) = LogDensityProblems.LogDensityOrder{0}()\n\nmodel_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensity(y1))\n\n# σ² = 0.1 # ほぼ横ばい\n# σ² = 0.5 # １回小さいエポック10で効率勝った＋全く横ばいになった\nσ² = 0.2  # すごく良い感じ\nspl = MALA(x -&gt; MvNormal((σ² / 2) .* x, σ² * I))\n\n\n\n\n実験の実行\nfunction experiment_MALA(epoch_list, N, y)\n    SE_sum = zero(epoch_list)\n    for _ in 1:N\n        chain = sample(model_with_ad, spl, Int64(epoch_list[end]); initial_params=[x0], chain_type=StructArray, param_names=[\"x\"], stats=true)\n        traj_MALA = Vector{Float64}(chain.x)\n        SE_list = []\n        for T in epoch_list\n            SE = SquaredError(traj_MALA[1:T], y)\n            push!(SE_list, SE)\n        end\n        SE_sum += SE_list\n    end\n    return SE_sum ./ N\nend\n\nMSE_MALA = experiment_MALA(Vector{Int64}(epoch_list), N, y1)\n\n\n\n\n結果のプロット\nscatter!(p, epoch_list, MSE_MALA,\n    marker=:circle,\n    markersize=5,\n    markeralpha=0.6,\n    color=\"blue\",\n    label=nothing\n    )\n\ndf = DataFrame(X = log10.(epoch_list), Y = log10.(MSE_MALA))\nmodel = lm(@formula(Y ~ X), df)\nX_pred = range(minimum(df.X), maximum(df.X), length=100)\nY_pred = predict(model, DataFrame(X = X_pred))\nplot!(p, 10 .^ X_pred, 10 .^ Y_pred,\n    line=:solid,\n    linewidth=2,\n    color=\"blue\",\n    label=\"MALA\"\n    )\n\ndisplay(p)"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#非一様な部分サンプリング",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#非一様な部分サンプリング",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "7 非一様な部分サンプリング",
    "text": "7 非一様な部分サンプリング\n当然，必ずしも一様な分解 \\[\nU(x)=\\frac{1}{n}\\sum_{j=1}^nU^j(x)\n\\] に基づいた一様なサブサンプリング \\(K\\sim\\mathrm{U}([n])\\) を行う必要はない．\n剪定の手続きを棄却法とみると，重点サンプリングのアイデアを導入することで制御変数に依らない分散低減が狙える (Sen et al., 2020 importance subsampling strategy)．\n特に，比例的高次元極限や，不均衡データに対するロジスティック回帰では，事後分布が十分な集中性を持たないために制御変数の方法 3 が十分な効率改善を示さないが，この重点サブサンプリングによれば効率の改善が見込める．\n詳しくは，次稿参照：\n\n    \n        \n            \n            \n                大規模な不均衡データに対するロジスティック回帰（後編）\n                離散時間 MCMC から連続時間 MCMC へ"
  },
  {
    "objectID": "posts/2024/Stat/ZigZagSubsampling.html#footnotes",
    "href": "posts/2024/Stat/ZigZagSubsampling.html#footnotes",
    "title": "Zig-Zag サンプラーのサブサンプリングによるスケーラビリティ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの２点が両方肝心である．効率的なサブサンプリング推定量の開発が (Fearnhead et al., 2018) 以来議論の焦点になっている．↩︎\n(Vasdekis, 2021, p. 25) や (Bierkens et al., 2019, pp. 1302 定理4.1) も参照．↩︎\n(Bierkens et al., 2019, pp. 1306–) 第5.1節参照．↩︎\n実装は ZigZagBoomerang パッケージの zigzagboom1d.jl を参考にした．↩︎\nただし，例えば今回も計算時間で言えば長くなっていることに注意．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html",
    "href": "posts/2024/Stat/Bayes1.html",
    "title": "ベイズ統計学と統計物理学",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n本稿では全ての結果に数学的に厳密な証明をつけることを優先し，簡単なモデルを取り上げた．\n一般のモデルと，スピングラス理論との大局観は，次稿を参照："
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#bayes-推定",
    "href": "posts/2024/Stat/Bayes1.html#bayes-推定",
    "title": "ベイズ統計学と統計物理学",
    "section": "1 Bayes 推定",
    "text": "1 Bayes 推定\n符号の誤り訂正，または denoising の文脈で Bayes 推定を考える．\n雑音が加わる通信路から受け取った観測から真の信号を推定するというデノイジング問題は，Bayes 推定が自然に選好される格好の設定である．\n\n1.1 デノイジング問題としての設定\n情報源は無記憶で，確率分布 \\(p(x)dx\\) に従うとし，通信路は，確率核 \\(p(y|x)dy\\) に従うとする．\n送信符号は１つの実数 \\(x^*\\sim p(x)dx\\) であったとし，この単一の入力 \\(x^*\\) を \\(n\\) 回独立に観測する： \\[\ny_1,\\cdots,y_n\\overset{\\text{iid}}{\\sim}p(y|x^*)dy.\n\\]\nこの観測を経たあと，送信符号 \\(x^*\\) はいったい何だったのかを推定することを考えると，極めて自然に Bayes 推定が選択される．\n\n\n\n\n\n\n純粋な解釈を持つ Bayes 推定\n\n\n\n\n\nまず，何の観測もない場合，\\(x^*\\) の確率分布は \\(p(x)dx\\) である（設定上）．\nしかし，すでに観測 \\(y_1,\\cdots,y_n\\) を経ている．よってこの事象の上での \\(x^*\\) の条件付き分布を計算すれば良いことになる： \\[\np(x|y_1,\\cdots,y_n) = \\frac{p(\\boldsymbol{y}|x)p(x)}{p(\\boldsymbol{y})}\n\\] \\[\n= \\frac{\\displaystyle p(y_1|x)\\cdots p(y_n|x)p(x)}{\\displaystyle\\int_\\mathbb{R}p(y_1|x)\\cdots p(y_n|x)p(x)\\,dx}\n\\tag{1}\\]\nこうして，誤り訂正符号の文脈では，Bayes 事前確率・事後確率が純粋に確率（または頻度）としての解釈を持つ．通常の統計的枠組みよりも仮定が多いが，その仮定の形式が Bayes 推定の形式にピッタリなのである．\n信号処理と Bayesian formalism の類似性は (Iba, 1999, p. 3876)，(Zdeborová and Krzakala, 2016, p. 456) でも指摘されている．\nまた，統計力学をベイズ推定の特殊な場合とみなせるという見方は，(Jaynes, 1957) から始まる．1\n\n\n\n\n\n1.2 例：Delta-Gauss 混合\n入力は \\(1/2\\) の確率で \\(x=0\\) だが，もう \\(1/2\\) の確率で標準正規分布 \\(\\mathrm{N}(0,1)\\) に従うとする： \\[\np(x)dx=\\frac{1}{2}\\delta_0(dx)+\\frac{1}{2}\\mathrm{N}(0,1)(dx).\n\\] 通信路は加法的 Gauss 型であるとする： \\[\np(x,y)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-y)^2}{2}\\right).\n\\] では真値を \\(x^*=1\\) として，\\(n=10\\) 回の観測データを生成してみる：\n\nusing Random\n\nRandom.seed!(123)\ndata = 1 .+ randn(10)\nprint(data)\n\n[1.8082879284649667, -0.12207250811417336, -0.10463610232929588, 0.5830073648350667, 1.2875879806238557, 1.2298186980518677, 0.5782313356003073, -0.355590621101197, -0.08821851393628699, 1.7037583257923017]\n\n\n愚直な計算から，\\(p(\\boldsymbol{y})\\) は次の積分から得られる：\n\n\\[\\begin{align*}\n    p(\\boldsymbol{y})&=\\int_\\mathbb{R}p(\\boldsymbol{y}|x)p(x)dx\\\\\n    &=\\int_\\mathbb{R}\\prod_{i=1}^n p(y_i|x)p(x)dx\\\\\n    &=\\frac{1}{2}\\frac{1}{(2\\pi)^{n+1}}\\int_\\mathbb{R}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-x)^2+x^2\\right)dx\\\\\n    &\\qquad+\\frac{1}{2}\\frac{1}{(2\\pi)^n}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-x)^2\\right)dx.\n\\end{align*}\\]\n\nこれを実際に計算した結果は (Krazakala and Zdeborová, 2024, p. 117) で与えられている．\nここでは，邪教のような計算を啓示するのではなく，Julia と Turing を通じて計算する方法を紹介する (Storopoli, 2021)．\n\nusing Turing, Distributions\n\nprior = MixtureModel([Normal(0.0, 1.0), Normal(0.0, 0.005)], [0.5, 0.5])\n\n@model function mixed_normal_model(data)\n    μ ~ prior\n    for i in 1:length(data)\n        data[i] ~ Normal(μ, 1)\n    end\nend\n\nmodel = mixed_normal_model(data)\n\n\nchain = sample(model, NUTS(), 100000)\n\n\nusing MCMCChains, Plots, StatsPlots\n\nplot(chain[:μ], seriestype = :density, xlabel = \"x*\", ylabel = \"Density\", title = \"Posterior Distribution\", color = \"#78C2AD\")\n\nvline!([1], color = :pink, linewidth = 2, label = \"x^* = 1\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n注（\\(x^*=0\\) でのアトムについて）\n\n\n\n\n\n\\(x^*=0\\) に原子が存在する．\nここでは密度関数は滑らかに見えるが，実際には \\(x^*=0\\) で不連続である．\nこれは，計算の都合上，\\(\\delta_0\\) を \\(\\mathrm{N}(0,0.005)\\) で代用したためである．\n\n\n\n\n\n1.3 MAP 推定量\n\n\n\n\n\n\n定義 (maximum a posterior estimator)\n\n\n\n\\[\n\\widehat{x}:=\\operatorname*{argmax}_{x\\in\\mathbb{R}}\\;p(x|y_1,\\cdots,y_n)\n\\] で定まる推定量 \\(\\mathbb{R}^n\\to\\mathbb{R}\\) を MAP 推定量 という．\n\n\n例 1.2 などをはじめ，ほとんどの場面では良い推定量を与え，多くの場合の最初のチョイスとして適しているかもしれない．\nしかし，例 1.2 をさらに変形することで，次のような事後分布が得られる状況は容易に想像がつく：\n\n\nCode\nfunction f(x)\n    if -1.8 &lt; x &lt; -1.6\n        return 2.5\n    elseif 0 &lt; x &lt; 1.5\n        return 1\n    else \n        return 0\n    end\nend\n\nplot(f, -2, 2, size=(600, 400), legend=false, color=\"#78C2AD\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 1\n\n\n\n\n\n\n1.4 ベイズ最適推定量\n図 1 の状況でも，たしかに１点のみを選ぶならば MAP 推定量で良いかもしれないが，\\(x\\le0\\) である確率は \\(x\\ge0\\) である確率よりも低いため，この意味では，\\(x\\in[0,3/2]\\) の範囲に推定量が収まっていた方が好ましいかもしれない．\n推定量 \\(\\widehat{x}_n:\\mathbb{R}^n\\to\\mathbb{R}\\) を評価するには，何らかのアプリオリな 損失 の概念が必要である．これを損失関数 \\(L:\\mathbb{R}^2\\to\\mathbb{R}\\) という形で与える．\nすると，損失の期待値が計算可能になり，これを 危険 という： \\[\nR(\\widehat{x}_n):=\\operatorname{E}[L(\\widehat{x}_n(\\boldsymbol{Y}),X)].\n\\]\nこのリスクを最小化する推定量を ベイズ最適推定量，その際のリスクを ベイズリスク という．2\n\n1.4.1 \\(l^2\\)-ベイズ最適推定量\n\n\n\n\n\n\n命題\n\n\n\n\\(L(x,y):=(x-y)^2\\) と定める．このとき，事後平均 がベイズリスクを達成する： \\[\n\\widehat{x}(\\boldsymbol{y}):=\\int_\\mathbb{R}xp(x|\\boldsymbol{y})\\,\ndx.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n図 1 の場合では，事後平均は約 \\(0.1\\) で，かろうじて正になる．\n\n\n1.4.2 \\(l^1\\)-ベイズ最適推定量\n\n\n\n\n\n\n命題\n\n\n\n\\(L(x,y):=\\lvert x-y\\rvert\\) と定める．このとき，事後中央値 がベイズリスクを達成する．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n図 1 の場合では，事後中央値は \\(0.5\\) となる．大変頑健な推定だと言えるだろう．\n\n\n1.4.3 最適決定規則\n今回の誤り訂正符号の文脈の目標は，\\(x^*\\) の復元であることを思い出せば，今回の損失は \\(L(x,y)=\\delta_0(x-y)\\) ととり，復号が成功する確率を最大とする推定量が「最良」と言うべきであろう： \\[\nR(\\widehat{x}_n)=\\operatorname{P}[\\widehat{x}_n(\\boldsymbol{Y})=X]\n\\]\nこれは結局 MAP 推定量 \\[\n\\widehat{x}_n:=\\operatorname*{argmax}_{x\\in\\mathbb{R}}p(x|\\boldsymbol{y})\n\\] で与えられることになる．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#統計物理からの視点",
    "href": "posts/2024/Stat/Bayes1.html#統計物理からの視点",
    "title": "ベイズ統計学と統計物理学",
    "section": "2 統計物理からの視点",
    "text": "2 統計物理からの視点\n真のシグナル \\(x^*\\in\\mathbb{R}\\) を，Bayes 事後平均によって点推定する問題を，統計力学の観点から考察する．\n次節 3 で，スパースな一般次元のベクトル \\(x^*\\in\\mathbb{R}^d\\) を復元する問題に拡張する．\n\n2.1 事後分布をある物理系の平衡分布と見る\nベイズの公式 (1) が与える事後分布 \\(p(x|\\boldsymbol{y})\\) は，次の Boltzmann-Gibbs 分布として理解できる： \\[\np(x|\\boldsymbol{y})=\\frac{e^{\\log p(\\boldsymbol{y}|x)p(x)}}{p(\\boldsymbol{y})}\n\\] \\[\n=\\frac{e^{-H(x,\\boldsymbol{y})}}{Z(\\boldsymbol{y})},\n\\] \\[\nH(x,\\boldsymbol{y}):=-\\log p(\\boldsymbol{y}|x)-\\log p(x),\n\\] \\[\nZ(\\boldsymbol{y}):=p(y).\n\\]\nすなわち，Bayes 事後分布 \\(p(x|\\boldsymbol{y})\\) は，\\(\\mathbb{R}\\times\\mathbb{R}^n\\) を配位空間に持ち，Hamiltonian \\(H(x,\\boldsymbol{y})\\) を持つ正準集団の平衡分布と捉えることができる．\n従って，Bayes 事後平均とは，この系に関する熱平均になる．加えて，MAP 推定量とは，この系に関する基底状態となる．\nしかし，この系の，物理的な意義どころか，統計的な意義も定かではない．\n\n\n2.2 もう一つの見方\n今回，通信路は加法的に Gauss ノイズを加えるものとしたのであった： \\[\np(\\boldsymbol{y}|x)d\\boldsymbol{y}=\\mathrm{N}(x,\\sigma^2)^{\\otimes n}(d\\boldsymbol{y})\n\\] この場合，前節 2.1 でみた方法の他に，事後分布 \\(p(x|\\boldsymbol{y})\\) を次のように理解することもできる： \\[\np(x|\\boldsymbol{y})=\\frac{e^{-H(x,\\boldsymbol{y})}}{Z(\\boldsymbol{y})},\n\\] \\[\nH(x,\\boldsymbol{y}):=\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)-\\log p(x),\n\\] \\[\nZ(\\boldsymbol{y}):=\\left(\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\frac{e^{-\\frac{\\lvert\\boldsymbol{y}\\rvert^2}{2\\sigma^2}}}{p(\\boldsymbol{y})}\\right)^{-1}.\n\\]\nこの Hamiltonian \\(H\\) により定まる系の分配関数 \\(Z(\\boldsymbol{y})\\) は，情報源 \\(p(x)\\) と Gauss 型通信路 \\(p(\\boldsymbol{y}|x)\\) で与えられた観測 \\(\\boldsymbol{y}\\) に関する周辺モデル \\(p(\\boldsymbol{y})\\) と，完全にランダムなホワイトノイズ \\(\\mathrm{N}(0,\\sigma^2)^{\\otimes n}\\) との尤度比，または ベイズ因子 になっている．\n\n\n\n\n\n\n式変形\n\n\n\n\n\n\\[\\begin{align*}\n    p(x|\\boldsymbol{y})&=p(\\boldsymbol{y}|x)\\frac{p(x)}{p(\\boldsymbol{y})}\\\\\n    &=\\frac{p(x)}{p(\\boldsymbol{y})}\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x-y_i)^2}\\\\\n    &=\\frac{p(x)}{p(\\boldsymbol{y})}\\frac{e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^ny_i^2}}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)}\\\\\n    &=\\frac{p(x)e^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)}}{Z(\\boldsymbol{y})}\\\\\n    &=\\frac{1}{Z(\\boldsymbol{y})}\\exp\\left(\\log p(x)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x^2-2xy_i)\\right)\n\\end{align*}\\]\n\n\n\nさらに，この系 \\(H\\) における自由エントロピーは，\\(p(\\boldsymbol{y})\\) と \\(\\mathrm{N}(0,\\sigma^2)^{\\otimes n}\\) との間の KL 乖離度となっている： \\[\nF_n:=\\int_{\\mathbb{R}^n}\\log\\frac{p(\\boldsymbol{y})}{q(\\boldsymbol{y})}p(\\boldsymbol{y})\\,d\\boldsymbol{y}=\\operatorname{KL}(p,q).\n\\] ただし，\\(q\\) は \\(\\mathrm{N}(0,\\sigma^2)^{\\otimes n}\\) の密度とした．\nこの系の他の物理量も，自由エネルギーと定数倍違うのみとなっている：\n\n\n\n\n\n\n命題（エントロピーと相互情報量）3\n\n\n\nこの Hamiltonian \\(H\\) を持つ系について，\n\nエントロピー \\(H\\) は次で与えられる：\n\\[\\begin{align*}\n     H(Y)&:=-\\int_{\\mathbb{R}^n}(\\log p(\\boldsymbol{y}))p(\\boldsymbol{y})\\,d\\boldsymbol{y}\\\\\n     &=\\frac{n}{2}\\log(2\\pi\\sigma^2)\\\\\n     &\\qquad+\\frac{1}{2\\sigma^2}\\int_{\\mathbb{R}^n}\\lvert\\boldsymbol{y}\\rvert^2p(\\boldsymbol{y})\\,d\\boldsymbol{y}-F_n.\n\\end{align*}\\]\n相互情報量 \\(I\\) は次で与えられる：\n\\[\\begin{align*}\n     I(X,Y)&:=\\operatorname{KL}(p(x,y),p(x)p(y))\\\\\\\\\n     &=\\frac{n}{2\\sigma^2}\\int_{\\mathbb{R}}x^2p(x)\\,dx-F_n.\n\\end{align*}\\]\n\n特に，いずれも自由エネルギーの定数倍である\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\nの式変形は次のとおり： \\[\\begin{align*}\n    H(Y)&:=-\\int_{\\mathbb{R}^n}(\\log p(\\boldsymbol{y}))p(\\boldsymbol{y})\\,d\\boldsymbol{y}\\\\\n    &=-\\int_{\\mathbb{R}^n}p(\\boldsymbol{y})d\\boldsymbol{y}\\log q(\\boldsymbol{y})-F_n\\\\\n    &=-\\int_{\\mathbb{R}^n}\\biggr(-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{\\lvert\\boldsymbol{y}\\rvert^2}{2\\sigma^2}\\biggl)p(\\boldsymbol{y})d\\boldsymbol{y}-F_n\\\\\n    &=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{n}{2\\sigma^2}\\int_{\\mathbb{R}}y_i^2p(y_i)\\,dy_i-F_n.\n\\end{align*}\\]\n次の関係式を用いる： \\[\nI(X,Y)=H(Y)-H(Y|X)\n\\] \\(H(Y)\\) は 1. から判明しており，\\(H(Y|X)\\) は次のように計算できる：\n\\[\\begin{align*}\nH(Y|X)&=-\\int_{\\mathbb{R}^{n+1}}\\log p(\\boldsymbol{y}|x)p(\\boldsymbol{y}|x)p(x)\\,dxd\\boldsymbol{y}\\\\\n&=-\\int_{\\mathbb{R}^{n+1}}\\log\\left(\\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-x)^2\\right)\\right)p(\\boldsymbol{y}|x)p(x)\\,dxd\\boldsymbol{y}\\\\\n&=\\int_{\\mathbb{R}^{n+1}}\\left(\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{\\sum_{i=1}^n(x-y_i)^2}{2\\sigma^2}\\right)p(\\boldsymbol{y}|x)p(x)\\,dxd\\boldsymbol{y}\\\\\n&=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\int_\\mathbb{R}\\left(\\int_{\\mathbb{R}^n}\\sum_{i=1}^n(x-y_i)^2p(\\boldsymbol{y}|x)\\,d\\boldsymbol{y}\\right)\\,p(x)dx\\\\\n&=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{n\\sigma^2}{2\\sigma^2}\\int_\\mathbb{R}p(x)\\,dx\\\\\n&=\\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{n}{2}.\n\\end{align*}\\]\n\nこれと，\\(y_i\\) には \\(x_i\\) とこれと独立な分散 \\(\\sigma^2\\) のノイズが加わって得られる値であることから，次のように計算できる： \\[\\begin{align*}\n    I(X,Y)&=H(Y)-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{n}{2}\\\\\n    &=-F_n-\\frac{n}{2}+\\frac{n}{2\\sigma^2}\\int_\\mathbb{R}y_i^2p(y_i)\\,dy_i\\\\\n    &=-F_n-\\frac{n}{2}+\\frac{n}{2\\sigma^2}\\left(\\sigma^2+\\int_\\mathbb{R}x^2p(x)\\,dx\\right)\\\\\n    &=-F_n+\\frac{n}{2\\sigma^2}\\int_\\mathbb{R}x^2p(x)\\,dx.\n\\end{align*}\\]\n\n\n\n\n\n2.3 西森対称性\n\n\n\n\n\n\n命題 (Nishimori, 1980)4\n\n\n\n\\(P:\\mathbb{R}\\to\\mathbb{R}^n\\) を確率核，\\(X^*\\in\\mathcal{L}(\\Omega)\\) は分布 \\(\\nu\\in\\mathcal{P}(\\mathbb{R})\\) を持ち，\\(Y\\in\\mathcal{L}(\\Omega;\\mathbb{R}^n)\\) の分布は \\[\n\\mu(dy)=\\int_\\mathbb{R}\\nu(dx)P(x,dy)\n\\] で定まるとする．ここで，\\(Y\\) で条件づけた \\(X^*\\) の確率分布を \\(P^{X|Y}\\) とする： \\[\n\\nu(dx)=\\int_{\\mathbb{R}^n}\\mu(dy)P^{X|Y}(y,dx).\n\\] \\[\nX^{(1)},\\cdots,X^{(k)}\\overset{\\text{iid}}{\\sim}P^{X|Y}\n\\] を独立な確率変数列とすると，次が成り立つ：任意の \\(f\\in\\mathcal{L}_b(\\mathbb{R}^{n+k})\\) について，\n\\[\\begin{align*}\n    &\\quad\\operatorname{E}\\biggl[f(Y,X^{(1)},\\cdots,X^{(k)})\\biggr]\\\\\n    &=\\operatorname{E}\\left[f(Y,X^{(1)},\\cdots,X^{(k-1)},X^*)\\right].\n\\end{align*}\\]\n\n\n\\(X^{(1)},\\cdots,X^{(k)}\\) で表した確率変数の \\(P^{X|Y}\\) に関する積分を Boltzmann 積分または熱平均と呼び，観測を作り出す過程 \\((X^*,Y)\\) に関する積分を無秩序積分 (disorder expectation) または quenched average という．5\n\n\n\n\n\n\n証明\n\n\n\n\n\nこの設定の下で，\\((X^*,Y)\\) の結合分布が次の２通りで表せていることに注意： \\[\n\\nu(dx)P(x,dy)=\\mu(dy)P^{X|Y}(y,dx).\n\\] 従って， \\[\\begin{align*}\n    &\\quad\\nu(dx)P(x,dy)P^{X|Y}(y,dx^{(1)},\\cdots,dx^{(k)})\\\\\n    &=\\mu(dy)P^{X|Y}(y,dx)P^{X|Y}(y,dx^{(1)},\\cdots,dx^{(k)})\n\\end{align*}\\] に関して \\(f(Y,X^{(1)},\\cdots,X^{(k)})\\) の期待値を取ると，次のように計算できる： \\[\\begin{align*}\n    &\\quad\\operatorname{E}\\biggl[f(Y,X^{(1)},\\cdots,X^{(k)})\\biggr]\\\\\n    &=\\int_{\\mathbb{R}^{n+k}}f(y,x^{(1)},\\cdots ,x^{(k-1)},x^{(k)})P^{X|Y}(y,dx^{(1)})\\cdots P^{X|Y}(y,dx^{(k)})\\mu(dy)\\\\\n    &=\\int_{\\mathbb{R}^{n+k}}f(y,x^{(1)},\\cdots,x^{(k-1)},x)P^{X|Y}(y,dx^{(1)})\\cdots P^{X|Y}(y,dx^{(k-1)})\\nu(dx)P(x,dy)\\\\\n    &=\\operatorname{E}\\left[f(Y,X^{(1)},\\cdots,X^{(k-1)},X^*)\\right].\n\\end{align*}\\]\n確率核 \\(P\\) にまつわる記法は次の記事も参照：\n\n    \n        \n            \n            \n                数学記法一覧\n                本サイトで用いる記法と規約\n            \n        \n    \n\n\n\n\n\n\n\n\n\n\n物理的な解釈\n\n\n\n\n\n\\(P^{X|Y}\\) に関する積分を \\(\\langle-\\rangle\\) で表すことで，何をどのように物理的に解釈しているかが明確になる： \\[\n\\langle X\\rangle=\\operatorname{E}[X|Y].\n\\]\nこの見方を採用すると，期待値を \\[\n\\operatorname{E}\\biggl[f(Y,X^{(1)},\\cdots,X^{(k)})\\biggr]=\\operatorname{E}^Y\\left[\\left\\langle f(Y,X^{(1)},\\cdots,X^{(k)})\\right\\rangle\\right]\n\\] と二段階で捉えていることになる．右辺の外側の期待値は単に \\(Y\\) のみに関してとっていることになる．\n第 2.2 節で考えたモデル \\(H\\) における Boltzmann 分布が \\(P^{X|Y}\\) となり，平均 \\(\\langle-\\rangle\\) はこれに関する平均となる．\n一方で，Hamiltonian \\(H\\) にもランダム性が残っているのであり，これに関する平均が \\((X^*,Y)\\) に関する平均に当たる．\nこうして，ベイズ統計モデルはスピングラス系（特にランダムエネルギーモデル (Derrida, 1980)）と同一視できるようになる．\nだが同時に，スピングラスのサンプリングを困難にする多谷構造も，ベイズ統計に輸入されるのである……．\nスピングラスについては，次の記事も参照：\n\n    \n        \n            \n            \n                数学者のための統計力学１\n                Ising 模型とスピングラス\n            \n        \n    \n\n\n\n\n\n\n2.4 最小自乗誤差推定量\n第 1.4.1 節で扱った最小自乗誤差推定量の自乗誤差は次のように計算できる：\n\n\n\n\n\n\n命題（最小自乗誤差の表示）\n\n\n\n\\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) と表す．事後平均推定量の自乗誤差は次のように表せる： \\[\\begin{align*}\n    \\DeclareMathOperator{\\MMSE}{MMSE}\n    \\MMSE&:=\\operatorname{E}\\biggl[\\biggr(X-\\operatorname{E}[X|Y]\\biggl)^2\\biggr]\\\\\n    &=\\operatorname{E}[X^2]-\\operatorname{E}\\biggl[\\langle X\\rangle^2\\biggr].\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    \\operatorname{E}[\\mathrm{V}[X|Y]]&=\\operatorname{E}\\biggl[\\biggr(\\operatorname{E}[X|Y]-X\\biggl)^2\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2-2X\\operatorname{E}[X|Y]+X^2\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2\\biggr]-2\\operatorname{E}\\biggl[X\\operatorname{E}[X|Y]\\biggr]+\\operatorname{E}[X^2]\\\\\n    &=\\operatorname{E}[X^2]-\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2\\biggr].\n\\end{align*}\\]\nこの変形では，繰り返し期待値の法則 \\[\\begin{align*}\n    \\operatorname{E}\\biggl[X\\operatorname{E}[X|Y]\\biggr]&=\\operatorname{E}\\biggl[\\biggl[X\\operatorname{E}[X|Y]\\,\\bigg|\\,Y\\biggr]\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\operatorname{E}[X|Y]^2\\biggr]\n\\end{align*}\\] を西森の対称性の代わりに用いたことになる．\n換言すれば，西森の対称性を証明したのと同様の方法を本命題にも適用したため，直接命題の適用は回避したことになる．\n\n\n\n\n\n2.5 KL 乖離度の微分としての分散\n次の定理は，相互情報量 \\(I\\) と MMSE を結びつける定理であるため，(Guo et al., 2005) 以来，I-MMSE 定理 と呼ばれている．\n\n\n\n\n\n\n命題（自由エネルギーによる特徴付け） (Guo et al., 2005)6\n\n\n\n簡単のため，\\(n=1\\) とする．このとき，\\(\\beta:=\\sigma^{-2}\\) に関して，次の式が成り立つ：\n\n自由エネルギー \\(F_1\\) について， \\[\n\\frac{\\partial F_1}{\\partial \\beta}=\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}.\n\\]\n相互情報量 \\(I(X,Y)\\) について，\n\\[\\begin{align*}\n    \\frac{\\partial I(X,Y)}{\\partial \\beta}&=\\frac{\\operatorname{E}[X^2]-\\operatorname{E}[\\langle X\\rangle^2]}{2}\\\\\n    &=\\frac{\\MMSE}{2}\n\\end{align*}\\]\n\nただし，\\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) と定めた．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n    \\log\\frac{p(y)}{q(y)}&=\\frac{\\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb{R}e^{-\\frac{(x-y)^2}{2\\sigma^2}}p(x)\\,dx}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{y^2}{2\\sigma^2}}}\\\\\n    &=\\int_\\mathbb{R}e^{-\\frac{x^2-2xy}{2\\sigma^2}}p(x)\\,dx.\n\\end{align*}\\]\nこの \\(p(y)dy\\) に関する積分を \\(\\beta\\) で微分すれば良いのであるが，\\(p(y)\\) 自体が \\(\\sigma\\) に依存し，そのまま計算したのでは大変煩雑になる．\nそこで，\\(y=\\sigma z+x^*\\) と変数変換をすることで，標準 Gauss 確率変数 \\(Z\\sim\\gamma_1\\) と真値 \\(X^*\\) は独立で，\\(\\sigma\\) も含まないから，次のように計算ができる： \\[\\begin{align*}\n    F_1(\\beta)&=\\int_\\mathbb{R}\\log\\frac{p(y)}{q(y)}p(y)\\,dy\\\\\n    &=\\operatorname{E}\\biggl[\\log\\left(\\int_\\mathbb{R}e^{-\\frac{x^2-2xY}{2\\sigma^2}}p(x)\\,dx\\right)\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\log\\left(\\int_\\mathbb{R}\\exp\\left(-\\frac{x^2}{2\\sigma^2}+\\frac{xZ}{\\sigma}+\\frac{xX^*}{\\sigma^2}\\right)p(x)\\,dx\\right)\\biggr]\\\\\n    &=\\operatorname{E}\\biggl[\\log\\left(\\int_\\mathbb{R}\\exp\\left(-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta\\right)p(x)\\,dx\\right)\\biggr].\n\\end{align*}\\]\nこうして，最右辺の期待値に関する密度はもう \\(\\beta\\) に依存しないから，次のように微分が計算できる．ただし， \\[\n\\exp\\left(-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta\\right)p(x)\\,dx\n\\] は \\(p(x|y)\\) の定数倍，すなわち Gibbs 測度 \\(\\frac{e^{-H(x,y)}}{Z(y)}dx\\) の定数倍であることに注意して，これらに関する平均を引き続き \\(\\langle-\\rangle:=\\operatorname{E}[-|Y]\\) で表すこととすると，\n\\[\\begin{align*}\n    \\frac{\\partial F_1(\\beta)}{\\partial \\beta}&=\n    \\operatorname{E}\\left[\\frac{\\displaystyle\\int_\\mathbb{R}\\biggr(-\\frac{x^2}{2}+\\frac{xZ}{2\\sqrt{\\beta}}+xX^*\\biggl)e^{-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta}p(x)dx}{\\displaystyle\\int_\\mathbb{R}\\exp\\left(-\\frac{x^2\\beta}{2}+xZ\\sqrt{\\beta}+xX^*\\beta\\right)p(x)\\,dx}\\right]\\\\\n    &=-\\frac{\\operatorname{E}[\\langle X^2\\rangle]}{2}+\\frac{1}{2\\sqrt{\\beta}}\\operatorname{E}[\\langle X\\rangle Z]+\\operatorname{E}[\\langle X\\rangle X^*].\n\\end{align*}\\]\n\\(\\langle X\\rangle\\) が中に入っているのは，\\(\\sigma[X^*,Z]\\) に関する条件付き期待値を \\(\\operatorname{E}\\) 内で取ったためと捉えられる．\nまず，第二項は，Stein の補題 2.6 の系から， \\[\\begin{align*}\n    \\frac{1}{2\\sqrt{\\beta}}\\operatorname{E}[\\langle X\\rangle Z]&=\\frac{1}{2\\sqrt{\\beta}}\\sqrt{\\beta}\\biggr(\\operatorname{E}[\\langle X^2\\rangle]-\\operatorname{E}[\\langle X\\rangle^2]\\biggl)\\\\\n    &=\\frac{\\operatorname{E}[\\langle X^2\\rangle]}{2}-\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}\n\\end{align*}\\]\n第三項は，西森の対称性から， \\[\\begin{align*}\n    \\operatorname{E}[\\langle X\\rangle X^*]=\\operatorname{E}[\\langle X\\rangle^2].\n\\end{align*}\\]\n以上を併せると， \\[\n\\frac{\\partial F_1(\\beta)}{\\partial \\beta}=\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}.\n\\]\n\nの主張も，\\(n=1\\) のときは \\[\nI(X,Y)=\\frac{\\beta}{2}\\int_\\mathbb{R}x^2p(x)\\,dx-F_1\n\\] であったために従う： \\[\\begin{align*}\n\\frac{\\partial I(X,Y)}{\\partial \\beta}&=\\frac{\\operatorname{E}[X^2]}{2}-\\frac{\\operatorname{E}[\\langle X\\rangle^2]}{2}\\\\\n&=\\frac{\\MMSE}{2}.\n\\end{align*}\\]\n\n\n\n\n\\(\\operatorname{E}[\\langle X\\rangle^2]\\) は \\(q\\) とも表され，スピングラスの 秩序パラメータ ともいう．7\n\\(I\\) の \\(\\beta\\) に関する二回微分を計算することより，\\(\\beta\\) に関する凸関数であることもわかる．\n\n\n2.6 Stein の補題\n\n\n\n\n\n\n命題 (Stein, 1972)\n\n\n\n可積分な確率変数 \\(X\\in\\mathcal{L}^1(\\Omega)\\) について，次は同値：\n\n\\(X\\) の分布は標準Gaussである：\\(X\\sim\\gamma_1\\)．\n任意の \\(f\\in C^1_b(\\mathbb{R})\\) について， \\[\n\\operatorname{E}[f'(X)]=\\operatorname{E}[Xf(X)]&lt;\\infty.\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n(1)\\(\\Rightarrow\\)(2)\n\\(f,f'\\) はいずれも有界としたから， \\[\n  \\operatorname{E}[f'(X)],\\operatorname{E}[f(X)]&lt;\\infty\n  \\] が成り立つ．\n\\(\\gamma_1\\) の密度 \\(\\phi\\) は \\(\\phi'(x)=-x\\phi(x)\\) を満たすことに注意すれば，部分積分により， \\[\\begin{align*}\n      \\operatorname{E}[f'(X)]&=-\\int_\\mathbb{R}f(x)\\phi'(x)dx\\\\\n      &=\\int_\\mathbb{R}f(x)x\\phi(x)dx=\\operatorname{E}[f(X)X].\n  \\end{align*}\\]\n(2)\\(\\Rightarrow\\)(1)\n\\(X\\) は可積分としたから，特性関数 \\(\\varphi(u):=\\operatorname{E}[e^{iuX}]\\) は少なくとも \\(C^1(\\mathbb{R})\\)-級で，その微分は \\(\\phi(x):=e^{iux}\\) に関する仮定 \\(\\operatorname{E}[\\phi'(X)]=\\operatorname{E}[X\\phi(X)]\\) を通じて \\[\\begin{align*}\n      \\varphi'(u)&=i\\operatorname{E}[Xe^{iuX}]=-u\\operatorname{E}[e^{iuX}]\\\\\n      &=-u\\varphi(u),\\qquad u\\in\\mathbb{R},\n  \\end{align*}\\] と計算できる．\nこの微分方程式は規格化条件 \\(\\varphi(0)=1\\) の下で一意な解 \\(\\varphi(u)=e^{-\\frac{u^2}{2}}\\) を持つ．\n\n\n\n\n\n\n\n\n\n\n系\n\n\n\n\\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) とこれと独立な \\(Z\\sim\\gamma_1\\) に関して，次が成り立つ：\n\\[\\begin{align*}\n    \\operatorname{E}[\\langle X\\rangle Z]&=\\operatorname{E}\\left[\\frac{\\partial \\langle X\\rangle}{\\partial Z}\\right]\\\\\n    &=\\sqrt{\\beta}\\biggr(\\operatorname{E}[\\langle X^2\\rangle]-\\operatorname{E}[\\langle X\\rangle^2]\\biggl)\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n最初の等号 \\[\n\\operatorname{E}[\\langle X\\rangle Z]=\\operatorname{E}\\left[\\frac{\\partial \\langle X\\rangle}{\\partial Z}\\right]\n\\] は Stein の補題による．\n続いて， \\[\n\\langle X\\rangle=\\int_\\mathbb{R}xp(x|Y)\\,dx\n\\] は \\(Y=\\sigma Z+X^*\\) を通じてのみ \\(Z\\) に依存するから， \\[\n\\frac{\\partial \\langle X\\rangle}{\\partial Z}=\\frac{\\partial \\langle X\\rangle}{\\partial Y}\\frac{d Y}{d Z}=\\sigma\\frac{\\partial \\langle X\\rangle}{\\partial Y}\n\\tag{2}\\] が成り立つ．あとは \\(\\langle X\\rangle=\\operatorname{E}[X|Y]\\) の \\(Y\\) に関する微分を計算すれば良い．\n\\[\n\\frac{\\partial p(y|x)}{\\partial y}=-\\frac{y-x}{\\sigma^2}p(y|x).\n\\] また， \\[\np(y)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb{R}e^{-\\frac{(x-y)^2}{2\\sigma^2}}p(x)\\,dx\n\\] であったから， \\[\\begin{align*}\n    p'(y)&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_\\mathbb{R}-\\frac{y-x}{\\sigma^2}e^{-\\frac{(x-y)^2}{2\\sigma^2}}p(x)\\,dx\\\\\n    &=-\\int_\\mathbb{R}\\frac{y-x}{\\sigma^2}p(y|x)p(x)\\,dx\\\\\n    &=-\\int_\\mathbb{R}\\frac{y-x}{\\sigma^2}p(x|y)p(y)\\,dx\\\\\n    &=-\\frac{y-\\langle X\\rangle}{\\sigma^2}p(y).\n\\end{align*}\\] \\[\n\\therefore\\quad\\frac{p'(Y)}{p(Y)}=\\frac{\\langle X\\rangle-Y}{\\sigma^2}\n\\]\nに注意すれば，次のように計算できる：\n\\[\\begin{align*}\n    \\frac{\\partial \\langle X\\rangle}{\\partial Y}&=\\frac{\\partial }{\\partial Y}\\int_\\mathbb{R}xp(x|Y)\\,dx\\\\\n    &=\\frac{\\partial }{\\partial Y}\\int_\\mathbb{R}x\\frac{p(Y|x)p(x)}{p(Y)}\\,dx\\\\\n    &=\\int_\\mathbb{R}x\\frac{\\partial }{\\partial Y}\\frac{p(Y|x)p(x)}{p(Y)}\\,dx\\\\\n    &=\\int_\\mathbb{R}x\\frac{\\partial p(Y|x)}{\\partial Y}\\frac{p(x)}{p(Y)}\\,dx\\\\\n    &\\qquad+\\int_\\mathbb{R}x\\left(\\frac{\\partial }{\\partial Y}\\frac{1}{p(Y)}\\right)p(Y|x)p(x)\\,dx\\\\\n    &=-\\int_\\mathbb{R}x\\frac{Y-x}{\\sigma^2}p(x|Y)\\,dx\\\\\n    &\\qquad-\\frac{p'(Y)}{p(Y)}\\int_\\mathbb{R}xp(x|Y)\\,dx\\\\\n    &=-\\frac{Y}{\\sigma^2}\\langle X\\rangle+\\frac{\\langle X^2\\rangle}{\\sigma^2}\\\\\n    &\\qquad-\\langle X\\rangle\\frac{\\langle X\\rangle-Y}{\\sigma^2}\\\\\n    &=\\frac{1}{\\sigma^2}\\biggr(\\langle X^2\\rangle-\\langle X\\rangle^2\\biggl).\n\\end{align*}\\]\n最後，式 (\\(\\ref{eq-chain-rule}\\)) より， \\[\\begin{align*}\n    \\frac{\\partial \\langle X\\rangle}{\\partial Z}&=\\sigma\\frac{\\partial \\langle X\\rangle}{\\partial Y}\\\\\n    &=\\sqrt{\\beta}\\biggr(\\langle X^2\\rangle-\\langle X\\rangle^2\\biggl).\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#sec-sparse-vector-denoising",
    "href": "posts/2024/Stat/Bayes1.html#sec-sparse-vector-denoising",
    "title": "ベイズ統計学と統計物理学",
    "section": "3 スパースベクトルの復号",
    "text": "3 スパースベクトルの復号\nスパースベクトルの信号推定問題を考える．ここまで，\\(x,x^*\\) は \\(\\mathbb{R}\\) の点としてきたが，本節では，\\(\\mathbb{R}^d,d=2^N\\) の one-hot ベクトルであるとする．\n\n3.1 設定\n真の信号 \\(x^*\\in\\mathbb{R}^d\\) は，\\(d\\) 次元の one-hot ベクトルであるとする．すなわち，次の集合 \\(\\Delta_d\\) の元であるとする： \\[\n\\Delta_d:=\\left\\{x\\in\\mathbb{Z}^d\\mid\\|x\\|_1=1\\right\\}.\n\\]\n加えて，\\(\\Delta_d\\) 上の一様分布に従うとする：\\(x^*\\sim\\mathrm{U}(\\Delta_d)\\)．\nこれを分散 \\(\\frac{\\sigma^2}{N}\\) を持った Gauss ノイズを通じて観測する： \\[\nY\\sim\\mathrm{N}_d\\left(x^*,\\frac{\\sigma^2}{N}I_d\\right).\n\\]\n事前分布を \\(p(x)\\)，\\(Y\\) の分布を \\(p(y)\\) とすると，Bayes の定理より， \\[\\begin{align*}\n    p(x|y)&=\\frac{p(x)}{p(y)}\\prod_{i=1}^d\\frac{e^{-\\frac{(y_i-x_i)^2}{2\\sigma^2/N}}}{\\sqrt{2\\pi\\sigma^2/N}}\\\\\n    &=\\frac{\\prod_{i=1}^d\\phi(y_i;0,\\sigma^2/N)}{p(y)}\\\\\n    &\\qquad\\times \\frac{1}{2^N}\\prod_{i=1}^d\\exp\\left(-\\frac{x_i^2-2x_iy_i}{2\\sigma^2/N}\\right)\n\\end{align*}\\]\nただし，\\(\\phi(-;\\mu,\\sigma):=\\frac{d \\mathrm{N}_1(\\mu,\\sigma)}{d \\ell_1}\\) を正規密度とした．\n\n\n3.2 スピングラス系との同一視\nこの事後分布 \\(p(x|y)\\) は，次の Hamiltonian \\(H\\) に関する，逆温度 \\(\\beta=\\sigma^{-2}\\) での Boltzmann 分布とみなせる：\n\\[\\begin{align*}\n    p(x|y)&=\\frac{1}{\\mathcal{Z}}e^{-\\beta H(x,y)}\\\\\n    H(x,y)&:=-N\\sigma^2\\log2\\\\\n    &\\qquad-\\frac{N}{2}\\sum_{i,j=1}^dx_i^*x_j^*\\sqrt{(1-2y_i)(1-2y_j)}\n\\end{align*}\\]\n\n\n\n\n\n\n注\n\n\n\n\n\nこの Hamiltonian は非物理的なものであり，planted ensemble とも呼ばれる．(Murphy, 2023, p. 843) での clamped という表現と同じニュアンスである．詳しくは次項参照：\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\n\n\n2024-06-23\n\n\n\n\n\n\nNo matching items\n\n\nまた，\\(H\\) の表示については，\\(x^*\\in\\Delta_d\\) であるから，\\(i,j\\in[d]\\) と２つの和をとっているように見えるが，二つの添え字が一致している場合しか非零な値は取らず，結局 \\[\nH(x,y)=-\\frac{N}{2}\\sum_{j=1}^d(2y_j-1)x_j+N\\sigma^2\\log2\n\\] であることに注意．\n\n\n\nなお，分配関数は \\[\n\\mathcal{Z}:=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(\\frac{N}{2\\sigma^2}(2y_i-1)\\right),\n\\] と表される．\n\n\n3.3 自由エネルギー密度の計算\n系が用意されたら，統計力学はまず，代表的な物理量の熱力学極限を計算する．特に自由エネルギー密度は，熱力学極限 \\(N\\to\\infty\\) において自己平均性（確率論では集中性という性質）を示すことが期待される．\n自由エネルギー密度 \\(\\Phi\\) は，熱力学極限を通じて \\[\n\\Phi(\\beta):=\\lim_{N\\to\\infty}\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\n\\] と定まる．\nI-MMSE 定理（第 2.5 節）はこの多次元の \\(X\\) に関しても有効であり， \\[\n\\Phi_N(\\beta):=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\n\\] に関して， \\[\n\\frac{\\partial \\Phi_N(\\beta)}{\\partial \\beta}=\\frac{\\operatorname{E}\\biggl[\\lvert\\langle X\\rangle\\rvert^2\\biggr]}{2}\n\\] が成り立つ．\n\n\n\n\n\n\n証明\n\n\n\n\n\nまず有限の \\(N\\in\\mathbb{N}^+\\) で計算する．\n\\[\n\\frac{\\partial \\Phi_N(\\beta)}{\\partial \\beta}=\\frac{1}{N}\\operatorname{E}\\left[\\frac{1}{\\mathcal{Z}}\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}\\right]\n\\] の右辺を求めれば良い．\n\\[\nY_i=X_i^*+\\frac{\\sigma}{\\sqrt{N}}Z_i\n\\] であり，one-hot ベクトル \\(x\\in\\Delta_d\\) については \\[\nx_j=x_j^2,\\quad\\lvert x\\rvert^2=1\n\\] でもあることに注意すれば，\n\\[\\begin{align*}\n    \\mathcal{Z}&=\\sum_{i=1}^de^{-H(x^{(i)},Y)}\\\\\n    &=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(-\\frac{N\\beta}{2}\\lvert x^{(i)}\\rvert^2+N\\beta(Y|x^{(i)})\\right)\\\\\n    &=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(-\\frac{N\\beta}{2}\\lvert x^{(i)}\\rvert^2+N(X^*|x^{(i)})\\beta+(Z|x^{(i)})\\sqrt{N\\beta}\\right)\n\\end{align*}\\] \\[\n\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}=\\sum_{i}^d\\biggr(-\\frac{N}{2}\\lvert x^{(i)}\\rvert^2+N(X^*|x^{(i)})+\\frac{\\sqrt{N}}{2\\sqrt{\\beta}}(Z|x^{(i)})\\biggl)e^{-H(x^{(i)},Y)}\n\\] \\[\n\\frac{1}{\\mathcal{Z}}\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}=-\\frac{N}{2}\\left\\langle\\lvert X\\rvert^2\\right\\rangle+N\\biggl\\langle(X^*|X)\\biggr\\rangle+\\frac{\\sqrt{N}}{2\\sqrt{\\beta}}\\biggl\\langle(Z|X)\\biggr\\rangle\n\\] \\[\n\\frac{1}{N}\\operatorname{E}\\left[\\frac{1}{\\mathcal{Z}}\\frac{\\partial \\mathcal{Z}}{\\partial \\beta}\\right]=\\operatorname{E}\\left[\\left\\langle-\\frac{\\langle\\lvert X\\rvert^2\\rangle}{2}+\\biggl\\langle(X^*|X)\\biggr\\rangle+\\frac{\\biggl\\langle(Z|X)\\biggr\\rangle}{2\\sqrt{N\\beta}}\\right\\rangle\\right]\n\\] これは１次元の場合の計算（第 2.5 節）と同様に， \\[\n\\frac{\\partial \\Phi_N(\\beta)}{\\partial \\beta}=\\frac{\\operatorname{E}[\\lvert\\langle X\\rangle\\rvert^2]}{2}\n\\] と計算できる．\n\n\n\nよって，\\(N\\to\\infty\\) の極限でも右辺が定数に収束し（右辺は \\(2^N\\) 項和を含む），この種の関係式が成り立ち続けるならば，\\(\\Phi\\) は \\(\\beta\\) の一次関数の形であるはずである．実際，次が示せる：\n\n\n\n\n\n\n命題8\n\n\n\n\\(\\Delta:=\\sigma^2\\) の関数として，自由エネルギー密度 \\(\\Phi\\) は次で定まる関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}_+\\) に一致する： \\[\nf(\\Delta):=\n\\begin{cases}\n\\frac{1}{2\\Delta}-\\log 2&\\Delta\\le\\Delta_c,\\\\\n0&\\Delta\\ge\\Delta_c.\n\\end{cases}\n\\] \\[\n\\Delta_c:=\\frac{1}{2\\log 2}\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n次節 3.4 では \\(\\Phi_N\\) を \\(f\\) によって上下から評価することで，数学的に厳密な証明を与える．ここでは，本ベイズモデルをランダムエネルギーモデルとみなし，レプリカ法によって計算した場合の証明の概略を付す．\n\\(n\\) 個のレプリカを複製した際の，それぞれの配置 \\((i_1,\\cdots,i_n)\\in[d]^n\\) について足し合わせることで，\\(Z^n\\) の disorder average を取る．\n最終的に次の表示を得る： \\[\ne^{nN\\left(\\log 2+\\frac{\\beta}{2}\\right)}\\approx\\int dQdM\\,e^{Ns(Q,M)+N\\beta\\left(\\sum_{a=1}^nM_a+\\frac{1}{2}\\sum_{a,b=1}^nQ_{a,b}\\right)}\n\\] \\[\n=:\\int dQdM\\,e^{Ng(\\Delta,Q,M)}.\n\\tag{3}\\] ただし，\\(M_a:=\\delta_{i_a,1}\\in2\\) は磁化のベクトル，\\(Q_{ab}:=\\delta_{i_a,i_b}\\) は overlap matrix と呼ばれる．\n積分はこの \\(M_a,Q_{ab}\\) の全体について実行され，同じ \\(M_a,Q_{ab}\\) の値を取る配置の数を \\[\n\\#(Q,M)=:e^{Ns(Q,M)}\n\\] と表した．\n\\(N\\to\\infty\\) の極限では，式 (3) に対して Laplace 近似を実行すれば良い．\n従って，\\(Q,M\\) の構造のうち，特に支配的なものの特定に成功すれば，解が求まることになる．\n最初の仮定として，レプリカ \\(i_1,\\cdots,i_d\\) は交換可能で見分けがつかないはずだろう，という replica symmetric ansatz が考えられる．\nこのレプリカ対称性を仮定すると，次の３通りまでシナリオが絞られる：\n\n任意のレプリカは同一の状態にある：\\(i_a\\equiv i\\in[d]\\)．だが，正しいレプリカではない \\(i\\ne1\\)．\nこのとき，\\(Q_{ab}\\equiv 1,M_a\\equiv0\\) となり，\\(s(Q,M)=\\log2\\)，かつ \\(g(\\beta,Q)=\\log2+n^2/\\Delta\\)．\nこれは \\(N\\to\\infty\\) の極限で \\(n\\) に関して線型ではなく，物理的な解が得られるとは思えず，レプリカ法も解析接続に失敗する．\n任意のレプリカは全て正しい状態にある：\\(i_a\\equiv1\\)．\n\\(Q_{ab}=M_a\\equiv1\\) となり，\\(s(Q,M)=0,g(\\beta,Q)=n/\\Delta+n^2/2\\Delta\\)．\nレプリカ内に少なくとも２つの違う状態がある：\nこのとき，\\(g(\\beta,Q)=n/2\\Delta+n\\log2\\)．\n\n２と３の場合から，次の２つが解の候補として回収できた： \\[\n\\operatorname{E}[Z^n]=e^{-nN\\left(\\log 2-\\frac{\\beta}{2}\\right)},\n\\] \\[\n\\operatorname{E}[Z^n]=0.\n\\]\nこの２つは，次節 3.4 から導かれる厳密な結果に一致する．\n最後，自由エネルギーの \\(\\Delta\\) に関する凸性と解析性から，結論を得る．\n\n\n\n\n\n3.4 自由エネルギーの上下評価\nレプリカ法を回避し，数学的に厳密な証明を与えるには，次のように上下から評価することになる．\n\n\n\n\n\n\n補題（下界）9\n\n\n\n\\[\n\\Phi_N(\\Delta)\\ge f(\\Delta)\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n一般に下からの評価は，\\(Z_i\\) に関する項をまとめて無視して良いために，簡単である．\n\\[\nY_i=X_i^*+\\frac{\\sigma}{\\sqrt{N}}Z_i,\n\\] \\[\nZ_i\\overset{\\text{iid}}{\\sim}\\mathrm{N}_1(0,1),\n\\]\nであることから，\\(i^*\\) を \\(X^*_{i^*}=1\\) を満たす添字 \\(i^*\\in[d]\\) とすると，\n\\[\\begin{align*}\n    \\mathcal{Z}&=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(\\frac{NY_i}{\\sigma^2}-\\frac{N}{2\\sigma^2}\\right)\\\\\n    &=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(\\frac{NX^*_i}{\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_i-\\frac{N}{2\\sigma^2}\\right)\\\\\n    &\\ge\\frac{1}{2^N}\\exp\\left(\\frac{N}{\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_{i^*}-\\frac{N}{2\\sigma^2}\\right)\\\\\n    &=\\frac{1}{2^N}\\exp\\left(\\frac{N}{2\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_{i^*}\\right)\n\\end{align*}\\]\nと評価できる．\n\\[\n\\log\\mathcal{Z}\\ge\\frac{N}{2\\sigma^2}+\\frac{\\sqrt{N}}{\\sigma}Z_{i^*}-N\\log 2.\n\\] \\[\\begin{align*}\n    \\Phi_N(\\Delta)&=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\\\\\n    &\\ge\\frac{1}{N}\\left(\\frac{N}{2\\sigma^2}-N\\log 2\\right)\\\\\n    &=\\frac{1}{2\\Delta}-\\log2.\n\\end{align*}\\]\nこれより，低温領域に於ては， \\[\n\\Phi_N\\ge f\\;\\mathrm{on}\\;(0,\\Delta_c)\n\\] が確認できた．\n続いて， \\[\\begin{align*}\n    \\frac{\\partial \\Phi_N(\\Delta)}{\\partial \\Delta}&=\\frac{\\partial \\Phi_N(\\Delta)}{\\partial \\beta}\\frac{d \\beta}{d \\Delta}\\\\\n    &=\\frac{\\operatorname{E}[\\lvert\\langle X\\rangle\\rvert^2]}{2}\\left(-\\frac{1}{\\Delta^2}\\right)\\le0\n\\end{align*}\\] であることと， \\[\n\\lim_{\\Delta\\to\\infty}\\Phi_N(\\Delta)=0\n\\] であることから，\\([\\Delta_c,\\infty)\\) 上においても非負であることがわかる．\n以上より， \\[\n\\Phi_N\\ge f\\;\\mathrm{on}\\;\\mathbb{R}_+.\n\\]\n\n\n\n\n\n\n\n\n\n補題（上界）10\n\n\n\n\\[\n\\Phi_N(\\Delta)\\le f(\\Delta)+o(1)\\quad(N\\to\\infty)\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\Phi_N\\) の上界は，Boltzmann 分布 \\(p(x|y)\\) に関する期待値に対して，Jensen の不等式を用いることで導かれる．このような不等式は annealed bound とも呼ばれる．\n真値 \\(x^*\\) とノイズ \\(z\\) が確定している（従って，\\(x^*_{i^*}=1\\) を満たす \\(i^*\\in[d]\\) も確定している）とすると， \\[\\begin{align*}\n    \\mathcal{Z}&=\\frac{1}{2^N}\\sum_{i=1}^d\\exp\\left(-\\frac{N\\beta}{2}+Nx_i^*\\beta+z_i\\sqrt{N\\beta}\\right)\\\\\n    &=\\frac{1}{2^N}\\left(\\exp\\left(\\frac{N\\beta}{2}+z_{i^*}\\sqrt{N\\beta}\\right)+\\sum_{i\\ne i^*}\\exp\\left(-\\frac{N\\beta}{2}+z_i\\sqrt{N\\beta}\\right)\\right)\n\\end{align*}\\] \\[\n\\log\\mathcal{Z}=\\log\\left(\\exp\\left(\\frac{N\\beta}{2}+z_{i^*}\\sqrt{N\\beta}-N\\log 2\\right)+\\sum_{i\\ne i^*}\\frac{1}{2^N}\\exp\\left(-\\frac{N\\beta}{2}*Z_i\\sqrt{N\\beta}\\right)\\right)\n\\] と計算できる．凹関数 \\(\\log\\) に対する Jensen の不等式より， \\[\\begin{align*}\n    \\operatorname{E}[\\log\\mathcal{Z}]&\\le\\operatorname{E}\\biggl[\\log\\biggr(\\exp\\biggr(\\frac{N\\beta}{2}+Z_{i^*}\\sqrt{N\\beta}-N\\log 2\\biggl)\\\\\n    &\\qquad\\quad\\qquad+\\sum_{i\\ne i^*}\\operatorname{E}_{Z_{-i^*}}\\biggl[\\frac{1}{2^N}\\exp\\biggr(-\\frac{N\\beta}{2}+Z_i\\sqrt{N\\beta}\\biggl)\\biggr]\\biggl)\\biggr]\\\\\n    &=\\operatorname{E}\\left[\\log\\left(\\exp\\left(\\frac{N\\beta}{2}+Z_{i^*}\\sqrt{N\\beta}-N\\log 2\\right)+\\frac{2^{N-1}}{2^N}\\right)\\right]\\\\\n    &\\le\\operatorname{E}\\left[\\log\\left(\\exp\\left(\\frac{N\\beta}{2}+Z_{i^*}\\sqrt{N\\beta}-N\\log 2\\right)+1\\right)\\right]\\\\\n    &=\\operatorname{E}\\left[\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}e^{Z_{i^*}\\sqrt{N\\beta}}+1\\biggl)\\right].\n\\end{align*}\\]\n最初の等号にて，\\(\\xi\\sim\\mathrm{N}(\\mu,\\sigma^2)\\) の積率母関数が \\[\n\\operatorname{E}[e^{t\\xi}]=\\exp\\left(\\mu t+\\frac{\\sigma^2t^2}{2}\\right)\n\\] で表せることを用いた．\nこの \\(Z_{i^*}\\) という確率変数は複雑に定まっており，これを直接議論することは後回しにする．\n本補題の主張は，純粋に関数 \\[\ng(z):=\\exp\\left(N\\left(\\frac{\\beta}{2}-\\log 2\\right)+z\\sqrt{N\\beta}\\right)+1\n\\] の性質を考察するだけで従う．この関数 \\(g\\) を用いると， \\[\\begin{align*}\n    \\Phi_N(\\beta)&=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\\\\\n    &\\le\\frac{1}{N}\\operatorname{E}\\biggl[\\log(g(Z_{i^*}))\\biggr]\n\\end{align*}\\] と表せる．\\(g\\) も \\(\\log g\\) も凸関数であるから， \\[\\begin{align*}\n    \\log g(\\lvert z\\rvert)&\\le\\log g(0)+\\lvert z\\rvert\\frac{d }{d z}\\log g(\\lvert z\\rvert)\\\\\n    &=\\log g(0)+\\frac{g'(\\lvert z\\rvert)}{g(\\lvert z\\rvert)}\\\\\n    &\\le\\log g(0)+\\lvert z\\rvert\\sqrt{N\\beta}\\\\\n    &=\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}+1\\biggl)+\\lvert z\\rvert\\sqrt{N\\beta}.\n\\end{align*}\\]\nこれより， \\[\\begin{align*}\n    \\Phi_N(\\beta)&\\le\\frac{1}{N}\\operatorname{E}\\biggl[\\log(g(\\lvert Z_{i^*}\\rvert))\\biggr]\\\\\n    &\\le\\frac{1}{N}\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}+1\\biggl)+\\sqrt{\\frac{\\beta}{N}}\\operatorname{E}[\\lvert Z_{i^*}\\rvert].\n\\end{align*}\\]\nよって，\\(Z_{i^*}\\) が可積分であることさえ認めれば，\\(N\\to\\infty\\) のとき， \\[\n\\Delta\\ge\\Delta_c\\quad\\Leftrightarrow\\quad\\frac{\\beta}{2}\\le\\log2\n\\] のとき，\\(\\log\\biggr(e^{N\\left(\\frac{\\beta}{2}-\\log 2\\right)}+1\\biggl)\\ge0\\) であり，そうでない場合は \\[\n\\log(1+e^x)\\le x\\quad x\\in\\mathbb{R}_+\n\\] より， \\[\n\\Phi_N(\\Delta)\\le\\frac{\\beta}{2}-\\log2+o(1)\\quad(N\\to\\infty)\n\\] を得る．\n\n\n\n\n\n3.5 解釈\n\\(N\\to\\infty\\) の極限において，高温領域 \\(\\Delta&gt;\\Delta_c\\) において，最小自乗誤差（MMSE）は \\(1\\) であり，何をどうしても復号することはできない．Gauss ノイズ \\(\\Delta=\\sigma^2\\) が大きすぎるのである．\n一方で，低温領域 \\(\\Delta&lt;\\Delta_c\\) において，\\(\\partial_\\beta f=\\frac{1}{2}\\) であり，従って MMSE は \\(0\\) になる．よって完全な誤りのない復号が可能であるはずである．\n\n\n\n\n\n\n設定（第 3.1 節）が与えられたならば，立ち所に，\\(\\Delta\\) が大きいほど復号が難しいことは予想がつく．しかし，ある閾値 \\(\\Delta_c\\) に依存して，少なくとも \\(N\\to\\infty\\) の極限では，\n\n２つの領域で全く異なる振る舞いをすること\n２つの振る舞いのみに分類される（ある一定以上の確率で復号が可能である，などの中間状態がない）こと\n\nは驚きである．\n\n\n\n実は，高温領域 \\(\\Delta&gt;\\Delta_c\\) では，自由エネルギー \\(F_N\\) が \\(0\\) に指数収束する．\\(F_N\\) とは，完全な Gauss ノイズ \\[\nq(y)dy:=\\mathrm{N}_d\\biggr(0,\\frac{\\Delta}{N}I_d\\biggl)\n\\] と \\(p(y)\\) との KL 距離距離であったから，メッセージ \\(x^*\\) を完全な雑音と見分けることが加速度的に難しくなっていくのである．\n\n\n3.6 自由エネルギーのさらに鋭い評価\n\n\n\n\n\n\n命題（KL 乖離度は指数収束する）11\n\n\n\n高温領域 \\(\\Delta&gt;\\Delta_c\\) において，ある \\(C&gt;0\\) が存在して，任意の \\(N\\in\\mathbb{N}^+\\) について次が成り立つ：\n\\[\\begin{align*}\n    F_N(\\Delta)=\\operatorname{KL}\\biggr(p(y)\\,\\bigg|\\,q(y)\\biggl)\\le e^{-CN}\n\\end{align*}\\]\nただし，\\(p\\) は観測信号 \\(Y\\sim\\mathrm{N}_d\\left(X^*,\\frac{\\Delta}{N}I_d\\right)\\) の密度，\\(q\\) は \\(\\mathrm{N}_d\\left(0,\\frac{\\Delta}{N}I_d\\right)\\) の密度とした．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n第 3.4 節で示した \\(\\Phi_N\\) の上界評価では， \\[\\begin{align*}\n    \\Phi_N(\\beta)&=\\frac{\\operatorname{E}[\\log\\mathcal{Z}]}{N}\\\\\n    &\\le\\frac{1}{N}\\operatorname{E}\\left[\\log\\left(\\exp\\left(N\\left(\\frac{\\beta}{2}-\\log 2\\right)+Z_i^*\\sqrt{N\\beta}\\right)+1\\right)\\right]\n\\end{align*}\\] を得て，\\(Z_i^*\\) の評価を回避したのであった．\nこれに正面から取り組むことで，高温領域 \\(\\Delta&gt;\\Delta_c\\) での \\(\\Phi_N\\) の指数収束を示すことができる．\n高温領域 \\(\\Delta&gt;\\Delta_c\\) では， \\[\nf:=-\\frac{\\beta}{2}+\\log2&gt;0\n\\] が成り立つ．\n\\(Z_{i^*}\\) は，まず \\(X^*\\) によって条件付ければ \\(Z_{i^*}|X^*\\sim\\mathrm{N}(0,1)\\) であるから， \\[\\begin{align*}\n    N\\Phi_N(\\beta)&\\le\\operatorname{E}\\left[\\log\\biggr(e^{-fN+Z_{i^*}\\sqrt{N\\beta}}+1\\biggl)\\right]\\\\\n    &=\\int_\\mathbb{R}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-fN+z\\sqrt{N\\beta}}+1\\biggl)\\,dz\\\\\n    &=\\left(\\int^{R}_{-\\infty}+\\int_{R}^\\infty\\right)\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-fN+z\\sqrt{N\\beta}}+1\\biggl)\\,dz\n\\end{align*}\\] と分解すると，まず \\((-\\infty,R)\\) 上の積分は \\(N\\to\\infty\\) に関して指数収束する．\n実際，\\(R&gt;0\\) に対して \\(N\\in\\mathbb{N}^+\\) を十分大きく取ることで，ある \\(\\epsilon&gt;0\\) が存在して \\[\n-fN+z\\sqrt{N\\beta}\\le-\\epsilon fN\n\\] を満たすようにできるから，\n\\[\\begin{align*}\n    &\\quad\\int^{R}_{-\\infty}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-fN+z\\sqrt{N\\beta}}+1\\biggl)\\,dz\\\\\n    &&lt;\\int_\\mathbb{R}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\biggr(e^{-\\epsilon fN}+1\\biggl)\\,dz\\\\\n    &=\\log\\left(1+e^{-\\epsilon fN}\\right)\\le e^{-\\epsilon fN}.\n\\end{align*}\\] 最後の不等式は \\[\n\\log(1+x)\\le x\\quad x\\in\\mathbb{R}\n\\] による．\n従って，あとは \\((R,\\infty)\\) 上の積分が指数収束することを示せば良いが，再び \\(\\log(1+x)\\le x\\) に注意して \\[\\begin{align*}\n    &\\quad\\int^\\infty_\\mathbb{R}\\frac{e^{-\\frac{z^2}{2}}}{\\sqrt{2\\pi}}\\log\\left(e^{-fN+z\\sqrt{N\\beta}}+1\\right)\\,dz\\\\\n    &\\le\\int^\\infty_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}}e^{-fN+z\\sqrt{N\\beta}-\\frac{z^2}{2}}\\,dz\\\\\n    &=e^{\\frac{N\\beta}{2}-fN}\\int^\\infty_\\mathbb{R}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(z-\\sqrt{N\\beta})^2}{2}}\\,dz\\\\\n    &=e^{\\frac{N\\beta}{2}-fN}\\operatorname{P}\\biggl[Z&gt;R-\\sqrt{N\\beta}\\biggr]\n\\end{align*}\\] と評価できるから，あとは \\(Z\\sim\\gamma_1\\) の尾部確率が指数収束するかどうか（正確には劣 Gauss 性を持つかどうか）の問題に帰着する．\n実はこれは yes である．中心化された確率変数の積率母関数が，ある \\(\\sigma&gt;0\\) に関して \\[\n\\operatorname{E}[e^{\\lambda\\xi}]\\le e^{\\frac{\\lambda^2\\sigma^2}{2}}\\quad\\lambda\\in\\mathbb{R}\n\\] を満たすことは，ある \\(\\kappa&gt;0\\) が存在して \\[\n\\operatorname{P}[\\lvert\\xi\\rvert\\ge t]\\le 2e^{-\\frac{t^2}{2\\kappa^2}}\n\\] を満たすことに同値になる．特に，\\(\\Rightarrow\\) 方向には \\(\\kappa=\\sigma\\) ととれる．12 \\(\\Phi_N\\) の上界評価（第 3.4 節）で触れたように，（中心化された）Gauss 確率変数はこれを満たす．\nよって， \\[\\begin{align*}\n    &\\quad e^{\\frac{N\\beta}{2}-fN}\\operatorname{P}\\biggl[Z&gt;R-\\sqrt{N\\beta}\\biggr]\\\\\n    &\\le e^{\\frac{N\\beta}{2}-fN}e^{-\\frac{(R-\\sqrt{N\\beta})^2}{2}}\\\\\n    &=e^{\\frac{N\\beta}{2}-fN}e^{-\\frac{R^2}{2}+\\sqrt{N\\beta}R-\\frac{N\\beta}{2}}\\\\\n    &=\\exp\\left(-\\frac{R^2}{2}+\\sqrt{N\\beta}R-Nf\\right).\n\\end{align*}\\]\nこの最右辺，ある定数 \\(C&gt;0\\) が存在して，\\(e^{-CN}\\) で抑えられる．\n\nあるいは，任意の \\(N\\in\\mathbb{N}^+\\) に対して \\[\nR:=\\sqrt{\\frac{N}{\\beta}}f-\\delta\\quad\\delta&gt;0\n\\] と取ることで，\\((-\\infty,R),(R,\\infty)\\) 上の積分のそれぞれについて，同様の評価を得る．\n\n以上より， \\[\\begin{align*}\n    F_N(\\Delta)&=N\\Phi_N\\\\\n    &\\le e^{-KN}.\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n命題（劣 Gauss 確率変数の特徴付け）13\n\n\n\n\\(X\\in L(\\Omega)\\) を確率変数とする．このとき，次の４条件は同値で，さらに \\(\\{K_1,\\cdots,K_5\\}\\subset\\mathbb{R}^+\\) はある \\(C\\in\\mathbb{R}\\) が存在して，\\(\\forall_{i,j\\in[5]}\\;K_j\\le CK_i\\) を満たすように取れる．\n\n尾部確率の評価：ある \\(K_1&gt;0\\) が存在して，14 \\[\n\\mathrm{P}[\\lvert X\\rvert\\ge t]\\le 2e^{-\\frac{t^2}{K_1^2}},\\qquad t\\ge0.\n\\]\n\\(L^p\\)-ノルムの評価：ある \\(K_2&gt;0\\) が存在して， \\[\n\\|X\\|_{L^p}\\le K_2\\sqrt{p},\\qquad p\\ge1.\n\\]\n\\(X^2\\) の積率母関数の \\(0\\) の近傍での評価：ある \\(K_3&gt;0\\) が存在して， \\[\n\\lvert\\lambda\\rvert\\le\\frac{1}{K_3}\\quad\\Rightarrow\\quad \\mathrm{E}[e^{\\lambda^2 X^2}]\\le e^{K_3^2\\lambda^2}.\n\\]\n\\(X^2\\) の積率母関数のある1点での値：ある \\(K_4&gt;0\\) が存在して， \\[\n\\operatorname{E}\\left[e^{\\frac{X^2}{K_4^2}}\\right]\\le2.\n\\]\nさらに，\\(X\\) が中心化されている場合，次とも同値：ある \\(K_5&gt;0\\) が存在して， \\[\n\\operatorname{E}[e^{\\lambda X}]\\le e^{K^2_5\\lambda^2},\\qquad\\lambda\\in\\mathbb{R}.\n\\]\n\n以上の同値な条件を満たす確率変数 \\(X\\) を 劣 Gauss (sub-Gaussian) という．\n\n\n\n\n3.7 まとめ\n大変単純化された設定 toy model であったが，比例的高次元極限 \\(N\\to\\infty\\) において，厳密に示せる相転移を示す模型である．\nすなわち，ただ一つの非零成分 \\(1\\) に対して，ノイズの分散 \\(\\sigma^2\\) が \\((2\\log2)^{-1}\\) より大きいかどうかで，これが復号可能かどうかが決まる．\nこの \\(2\\log2\\) という値は (Donoho and Johnstone, 1994) が universal threshold と呼ぶ値の例であり，ランダムエネルギーモデルのスピングラス相転移境界と対応する．15\n一般のモデルでは，この臨界温度の値は不明である上に，\\(\\Delta&gt;\\Delta_c\\) の高温領域での効率的な推定法が見つかっていない場合も多い．\nこのような，高次元統計推測の問題においては，統計物理学，特にスピングラス理論の知見が活発に応用されて，漸近極限における相図の解明，統計計算手法の開発が目指されている．\n\n\n\nSchematic representation of a typical high-dimensional inference problem from (Zdeborová and Krzakala, 2016, p. 466)\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\n\n\n2024-06-23\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#終わりに",
    "href": "posts/2024/Stat/Bayes1.html#終わりに",
    "title": "ベイズ統計学と統計物理学",
    "section": "4 終わりに",
    "text": "4 終わりに\n\n第 3 節において，統計力学の知見は，レプリカ法などの計算手法を通じて，\\(N\\to\\infty\\) における漸近極限として，「大規模な確率分布から平均値を計算するための”ツールボックス”」(樺島祥介, 2003) として働いている．\n\n大自由度系では，次元に関して計算量が指数約に爆発してしまう．平衡統計力学の歴史はこの問題との戦いの歴史である，と言っても過言ではない．(樺島祥介 and 杉浦正康, 2008, p. 22)"
  },
  {
    "objectID": "posts/2024/Stat/Bayes1.html#footnotes",
    "href": "posts/2024/Stat/Bayes1.html#footnotes",
    "title": "ベイズ統計学と統計物理学",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Zdeborová and Krzakala, 2016, p. 467) も参照．↩︎\n(Shalev-Shwartz and Ben-David, 2014, p. 25) や (Krazakala and Zdeborová, 2024, p. 119) に倣った．↩︎\n(Krazakala and Zdeborová, 2024, p. 122)．↩︎\n(Zdeborová and Krzakala, 2016, p. 464)，(Krazakala and Zdeborová, 2024, p. 123) 定理13，(Iba, 1999, pp. 3876–3877) などで扱われている．西森ライン上のみで見られる性質であるため，西森対称性と呼ぶ．西森ラインについては 次項 も参照．↩︎\n(Mézard and Montanari, 2009, p. 249) や (Iba, 1999, p. 3876) などでは thermal average と quenched average の用語が採用されている．↩︎\n(Krazakala and Zdeborová, 2024, p. 124) 定理14．↩︎\n(西森秀稔, 2005, p. 123)なども参照．↩︎\n(Krazakala and Zdeborová, 2024, p. 125) 定理15，(樺島祥介 and 杉浦正康, 2008, p. 14) なども参照．証明は (Krazakala and Zdeborová, 2024, p. 133) 7.B 節 を参考にした．↩︎\n(Krazakala and Zdeborová, 2024, p. 125)補題８．↩︎\n(Krazakala and Zdeborová, 2024, p. 126)補題９．↩︎\n(Krazakala and Zdeborová, 2024, p. 132)補題10．↩︎\n(Vershynin, 2018, p. 25)命題2.5.2など．↩︎\n(Vershynin, 2018, p. 25)命題2.5.2．↩︎\nこの定数2は，1よりも真に大きい定数ならばなんでも良い．(Vershynin, 2018, p. 27)注2.5.3 も参照．↩︎\n(Krzakala et al., 2015, p. 8) また p.16 も参照すべし．↩︎"
  },
  {
    "objectID": "posts/2024/Stat/Bayes2.html",
    "href": "posts/2024/Stat/Bayes2.html",
    "title": "ベイズ統計学とスピングラス",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n上掲稿で扱ったスパース符号の復元の問題をさらに押し進め，より実用的な誤り訂正符号の設定を考える．\nモデルにハイパーパラメータが増え，厳密に証明できる事項は大きく減る．（逆）温度パラメータ \\(\\beta&gt;0\\) が特定の値を取る際，厳密解が計算可能であり，これを 西森温度 と呼ぶ．1 これは，ハイパーパラメータ \\(\\beta&gt;0\\) が指定する分布族が，真の事後分布を含む条件と等価になる．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes2.html#誤り訂正符号",
    "href": "posts/2024/Stat/Bayes2.html#誤り訂正符号",
    "title": "ベイズ統計学とスピングラス",
    "section": "1 誤り訂正符号",
    "text": "1 誤り訂正符号\n\n1.1 線型符号\n有限体 \\(\\mathbb{F}_p\\) 上の固定符号長 \\(n\\) を持つ 線型符号 とは，\\(\\mathbb{Z}\\)-部分加群 \\(C\\subset\\mathbb{F}_p^n\\) のことである： \\[\na(x+y)=ax+ay\\quad(a\\in\\mathbb{Z},x,y\\in C).\n\\]\nこのクラスの符号については，Hamming 距離 \\(d\\) の最小値 \\[\n\\min_{\\substack{x,y\\in C\\\\ x\\ne y}}d(x,y)\n\\] が線型時間で計算可能であるために，誤り訂正符号の例として重宝される．\n\n\n\n\n\n\n例（最も単純な線型パリティ検査符号）2\n\n\n\n\n\n\\[\nC:=\\left\\{\\mathtt{000},\\mathtt{011},\\mathtt{101},\\mathtt{110}\\right\\}\n\\] とすると，\\(\\mathbb{F}_2^3\\) 上の \\(\\mathbb{Z}\\)-部分加群となっている．\n加えて，この線型符号は次の構造を持つ：任意の符号語 \\(x_1x_2x_3\\in C\\) について， \\[\nx_1+x_2\\equiv x_3\\mod 2.\n\\] すなわち，真の符号は前半２ビット（情報ビット）に含まれており，最後の符号は パリティ検査ビット と呼ばれる．\n１ビットの誤りまでなら，\\(x_3\\) の偶奇が変わるために，誤りを検出できる．このことを，\\(C\\) は単一パリティ検査符号であるという．\n\n\n\n\n\n1.2 パリティ検査\n\n\n\n\n\n\n定義（パリティ検査方程式）\n\n\n\n\\(n&gt;k\\in\\mathbb{N}\\) に対して，\\((n,k)\\)-線型符号 とは，ある行列 \\(H\\in M_{n,n-k}(\\mathbb{F}_2)\\) を用いて \\[\nC=\\mathrm{Ker}\\;H\n\\] と定義される符号 \\(C\\subset\\mathbb{F}_2^n\\) のことをいう．このとき，\\(H\\) を パリティ検査行列，符号語 \\(x\\in C\\) が満たす性質 \\[\nHx=0\n\\] を パリティ検査方程式 という．\n\n\n\n\n\n\n\n\n例（組織符号）3\n\n\n\n\n\n前掲の例（第 1.1 節）のように，情報ビットと組織ビットが完全に分離している符号を 組織符号 という．\nこれは，ある行列 \\(G\\in M_{k,n-k}(\\mathbb{F}_2)\\) が存在して，白文 \\(u\\in\\mathbb{F}_2^k\\) に対する符号が \\[\nx=(I_k\\;G)^\\top u\n\\] と表せる場合に当たる．\nこのとき，パリティ検査行列 \\(H\\in M_{n,n-k}(\\mathbb{F}_2)\\) を \\[\nH:=(G\\;I_{n-k})\n\\] で定めると，パリティ検査方程式が成り立つ．\n実際， \\[\nHx=(G\\;I_{n-k})(I_k\\;G)^\\top u=Ou=0.\n\\]\nこうして定まる \\(H\\) はパリティ検査行列の標準形ともいう．\n\n\n\nパリティ検査行列における誤り訂正では，次の逆問題を考えることになる．\n\n\n\n\n\n\nシンドローム復号\n\n\n\n対称通信路が加法的ノイズを印加するならば，受信符号 \\(y\\) はあるベクトル \\(e\\in\\mathbb{F}_2^n\\) に対して \\[\ny=x+e\n\\] の形で表される．\nすると，パリティ検査方程式より， \\[\ns:=Hy=He\n\\] が必要である．この値 \\(s\\) を シンドローム という．\nシンドロームは誤りベクトル \\(e\\) のみの関数であるから，良い設定下では誤り \\(e\\) を推定することができる．\n\n\n\n\n\n\n\n\n例（低密度パリティ検査）4\n\n\n\n\n\n一般に，パリティ検査方程式 \\[\ns=He\n\\] から \\(e\\) を推定する問題は計算量的に困難になる．\nしかし，例えばパリティ検査行列が \\[\nH=(C_1\\;C_2)\n\\] \\[\nC_1\\in M_{n-k,k}(\\mathbb{F}_2),\\quad C_2\\in M_{n-k,n-k}(\\mathbb{F}_2)\n\\] と２つの疎行列の結合として表される場合（Gallager 符号 (Gallager, 1960) という），Bethe 近似による効率的な復号アルゴリズムが存在する．\n\n\n\n\n\n\n1.3 スピングラスとの類似性\nスピングラスとの類似性を最初に指摘したのは (Sourlas, 1989) であり，すぐに西森ラインとの関連性も知られた (Hidetoshi Nishimori, 2001)．\n現在ではこの対応は誤り訂正符号に限らず，極めて広範な統計的推定問題に渡っていることが認識されている．\n実際，スピングラス理論で開発されたレプリカ法，cavity method, 信念伝搬法は盛んに統計的推定問題に応用されている (Zdeborová and Krzakala, 2016)．\n\nPerhaps the most important message we want to pass in this review is that many calculations that physics uses to understand the world can be translated into algorithms that machine learning uses to understand data. (Zdeborová and Krzakala, 2016, p. 457)"
  },
  {
    "objectID": "posts/2024/Stat/Bayes2.html#スピングラスと西森温度",
    "href": "posts/2024/Stat/Bayes2.html#スピングラスと西森温度",
    "title": "ベイズ統計学とスピングラス",
    "section": "2 スピングラスと西森温度",
    "text": "2 スピングラスと西森温度\nスピングラス系にベイズ推定を紐づけることができる．\nその際，伝統的にスピングラス理論で扱われた quenched ensemble とも annealed ensemble とも違う第三のアンサンブル planted ensemble を扱うことになる．\n\n2.1 設定：２点相互作用のみを考えた Ising モデル\n配置空間を \\(x\\in\\{\\pm1\\}^N\\) とし，\\(N\\) 個の頂点を持ったグラフ \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\) が定める Ising モデル \\[\nE(x,y)=-\\sum_{(i,j)\\in\\mathcal{E}}y_{ij}x_ix_j\n\\tag{1}\\] を考える．\n\\(y_{ij}&gt;0\\) の場合 \\(x_i,x_j\\) は揃い，\\(y_{ij}&lt;0\\) の場合 \\(x_i,x_j\\) は反対方向を向く方がエネルギー \\(E\\) が下がる．\n\n\n\n\n\n\nフラストレーションについて\n\n\n\n\n\nこの系を絶対零度まで冷却した際，\\((y_{ij})\\) に指定された通りのルールを満たす配置 \\(x\\in\\{\\pm1\\}^N\\) があるならばその配置が出現するはずである．この場合，系はフラストレーションを持たないという．\nそうでない場合でも，ファクターグラフ \\((\\mathcal{V},\\mathcal{E},(y_{ij}))\\) が定めるフラストレーションを最小にした基底状態が見つかるはずである．\nなお，あるファクターグラフがフラストレーションを持たないかどうかは，クラス P に属する問題である．すなわち，多項式時間で判定可能である．5\n\n\n\n相互作用項 \\(y_{ij}\\) が確率的に与えられる場合，その系を スピングラス という．\nスピングラス理論で代表的な (Edwards and Anderson, 1975) モデルでは，\\(\\{y_{ij}\\}\\) はある簡単な分布の独立同分布確率変数だとして，系の振る舞いを分析する．6\n一方でここでは，確率変数 \\(y_{ij}\\) は次に説明するように定まると考えることで，スピングラス系とベイズ推定問題との対応が明らかになる．7\n\n\n2.2 スピングラス系のベイズからの解釈\n統計的な手続きは，スピングラス系を特殊な方法で生成し，その基底状態を探る逆問題として理解できる．8 こうして生成されたスピングラスを planted ensemble と呼ぶ．\n具体的には，前稿 で扱った信号推定の問題で，情報源の分布 \\(p(x)\\) と通信路の分布 \\(p(y|x)\\) が，あるパラメータ \\(\\alpha,\\beta\\) の分だけ一般化し，次の過程を考える：\n\n\n\n\n\n\nplanted ensemble9\n\n\n\n\n配置 \\(x\\in\\{\\pm1\\}^N\\) をある分布 \\(p_\\alpha(x)\\) に従って選ぶ．10\n相互作用項 \\(y\\) を，この \\(x\\) と式 (1) で定まる Hamiltonian \\(E\\) が定める Boltzmann 分布 \\(p_\\beta(y|x)\\) からサンプリングする： \\[\np_\\beta(y|x):=\\frac{e^{-\\beta E(x,y)}}{\\mathcal{Z}_\\beta}\n\\] \\[\n\\mathcal{Z}_\\beta:=\\sum_{y\\in\\mathcal{E}}e^{-\\beta E(x,y)}\n\\]\n観測者は \\(y\\) のみを見て，\\(x\\) がなんだったかを考える．\\(y\\) を生成するのに用いたパラメータの真値 \\(\\alpha^*,\\beta^*\\) も未知とする．11\n\n\n\n最初に設定される Ising スピンの配置 \\(x\\in\\{\\pm1\\}^N\\) を信号とみなすと，ノイズが加わった観測とは coupling \\(y_{ij}\\) のみを見ることに対応する．\nそして信号推定とは，この \\(y_{ij}\\) が定める Hamiltonian \\(E(x,y)\\) の基底状態を探索することに他ならない．信号 \\(x\\) が，planted ensemble の基底状態に埋め込まれているわけである．\n\n\n\n\n\n\n例（planted ensemble と同一視できる系）12\n\n\n\n\n\n二元対称通信路における誤り訂正符号のスピングラス的解釈（第 1.3 節）はこの例の１つに過ぎない．\nさらに，\\(x\\) を一般の潜在変数とし，事前分布 \\(p(x)\\) と確率核 \\(p(y|x)\\) が定める確率的グラフィカルモデルは，すべてこの枠組みに収まる．\n以上，Bayes 推定の文脈でスピングラスの planted ensemble を定義したが，同様にしてスピングラス系の中に情報を隠すことは，よく見られるモデリング法である．\n\nHopfield モデル (Hopfield, 1982) はランダムな配置を記憶させたスピングラス系であり，脳科学において連想記憶のモデルとしても用いられる．\nタンパク質の折りたたみ構造をモデリングするとき，その native configuration を基底状態として埋め込むことがある (Bryngelson and Wolynes, 1987)．\nグラフ内の大規模なクリークを見つける問題の難易度を，対応する planted ensemble における MCMC の収束の遅さとして捉えることができる (Mark Jerrum, 1992)．\n同様に，グラフの２分割問題と擬似アニーリングとにも対応が存在する (M. Jerrum and Sorkin, 1993)．\n充足性問題（四色問題など）の計算複雑性も，planted ensemble に対応させて示された (Achlioptas and Coja-Oghlan, 2008)．\n\n\nThe authors find that making the connection between planting and inference very explicitly brings a lot of insight into a generic inference problem and for this reason we built the present review around this connection. (Zdeborová and Krzakala, 2016, p. 469)\n\n\n\n\n\n\n\n\n\n\n比例的高次元漸近論\n\n\n\n\n\n\\(M:=\\lvert\\mathcal{E}\\rvert\\) が観測 \\(y\\) のサイズとなる．\n従来の統計学における漸近理論では，パラメータの次元 \\(N\\) は固定とし，\\(M\\to\\infty\\) の極限での理論が展開された．\n現代では，モデルのサイズも大きくする \\(N\\to\\infty\\) の極限を考える需要が高まっている．\nしかしこの場合でも，統計力学のツールボックスはこれを可能にする．\n特に，比 \\(\\alpha:=\\frac{M}{N}\\) を保ったまま \\(N,M\\to\\infty\\) の極限を考えるスキームは 比例的高次元 と呼ばれる．\n一般に，\\(\\alpha\\) が十分大きい場合は，モデルの複雑性に対して観測も多いので，意味のある情報が引き出せるはずであり，\\(\\alpha\\) が小さいほど困難になっていくはずである．\n実際に，比例的高次元極限では，前稿でみた閾値現象が普遍的に生じることが知られている．これが，スピングラスの相転移に対応するのである．\n\n\n\n\n\n2.3 西森ライン\n前節 2.2 の解釈では，観測 \\(y\\) を経たあとの信号 \\(x\\) の事後分布は \\[\\begin{align*}\n    p_{\\alpha,\\beta}(x|y)&\\,\\propto\\,p_\\beta(y|x)p_\\alpha(x)\\\\\n    &=\\frac{e^{-\\beta E(x,y)}p_\\alpha(x)}{\\mathcal{Z}_{\\alpha,\\beta}}\n\\end{align*}\\] \\[\n\\mathcal{Z}_{\\alpha,\\beta}:=\\sum_{x\\in\\mathcal{E}}e^{-\\beta E(x,y)}p_\\alpha(x)\n\\] で与えられる．\nただし，ここで，観測者の立てたモデルにおいて，パラメータ \\(\\alpha,\\beta\\) は真の値 \\(\\alpha^*,\\beta^*\\) とは異なり得るとする．\n仮に観測者の用いたパラメータが真のパラメータと一致していた \\((\\alpha,\\beta)=(\\alpha^*,\\beta^*)\\) の場合，種々の魅力的な性質が成り立つ．これを西森条件という．13\n中でも特に，次の性質は 西森対称性 と呼ばれている：14 \\[\n\\operatorname{E}[f(X_1,X_2)]=\\operatorname{E}[f(X^*,X_2)]\n\\] ただし，\\(X_1,X_2\\) は事後分布からのランダムサンプル（平衡状態にあるレプリカ）であり，\\(X^*\\) は \\(p(x)\\) に従う真の信号とした．\nこの西森対称性を通じて，自由エネルギー（＝ KL 乖離度）をはじめとした厳密解が求まるのである．そのことを簡単なモデルで確認したのが 前稿 である．\n\n\n\n\n\n\n西森対称性の含意\n\n\n\n\n\n西森対称性とは，planted ensemble において，元々の配置 \\(x^*\\) は，Boltzmann 分布の平衡状態からのサンプル \\(X\\) と平均的に（マクロ的には）見分けがつかないという性質である．\n西森対称性を用いた厳密解は，前稿で多く紹介している：\n\n    \n        \n            \n            \n                ベイズ統計学と統計物理学\n                スパース符号の復元を題材として\n            \n        \n    \n\n前稿では信号推定の問題を扱ったが，情報源分布 \\(p(x)\\) と通信路 \\(p(y|x)\\) のモデルがすでにわかっていることは，西森条件に相当する．\nすなわち，信号推定の問題は，自然に西森ライン上で議論している場合に当たるのである．\n一般に，事後平均推定量 \\(\\widehat{X}_n\\) が平均自乗誤差を最低にするのであった：\n\\[\n\\operatorname{E}[(\\widehat{X}_n-X^*)^2]=\\min_{\\widehat{X}_n}\\operatorname{E}[(\\widehat{X}_n-X^*)^2]\n\\]\n一般にこの値は不可知であるが，西森ライン上では＝モデルの特定が成功している場合，\\(\\widehat{X}_n\\) の分散に平均的に（＝\\(\\operatorname{E}\\) 内で）一致することになる．\nこれが 命題 \\[\n\\DeclareMathOperator{\\MMSE}{MMSE}\n\\MMSE=\\operatorname{E}[X^2]-\\operatorname{E}\\biggl[\\langle X\\rangle^2\\biggr]\n\\] の意味であるとも見れる．\n\n\n\n換言すれば，Boltzmann 分布のうち，\\(\\beta\\) が真値 \\(\\beta^*\\) に一致する場合のみ，Bayes 推定の観点からは特別な意味を持つことになる．これを 西森温度 というのである．\n\n\n\n2.4 Planted Ensemble の新規性\nスピングラスでは，ランダムに定まる相互作用項 \\(Y_{ij}\\) と，これが定める Boltzmann 分布 \\(\\frac{e^{-\\beta H}}{\\mathcal{Z}}\\) の２つの平均が存在する．\n後者に関する平均は \\(\\langle X\\rangle:=\\operatorname{E}[X|Y]\\) で表す一方で，前者に関する平均はよく \\([-]\\) で表される．15\n従来のスピングラス理論では，disorder \\(Y_{ij}\\) に関する quenched average \\([-]\\) を先に取ることで，従来の Ising 模型と同様の議論に持ち込む戦略が取られる．\n\n\n\n\n\n\nスピングラス理論における２つの平均16\n\n\n\n\n\n系が大きくなるほど，\\(\\frac{\\log Z_N}{N}\\) の値はその quenched average に集中していく．17\n従ってこの quenched average を求めることが中心問題とされる： \\[\nf=\\lim_{N\\to\\infty}-\\frac{1}{\\beta N}[\\log Z].\n\\] これは，自由エネルギー密度の quenched average と呼ばれる．\nしかしこれに解決が難しく，値を計算するヒューリスティックとしてレプリカトリックが用いられる： \\[\nf\\approx-\\lim_{N\\to\\infty}\\frac{1}{\\beta N}\\lim_{n\\to\\infty}\\frac{[Z^n]-1}{n}.\n\\]\n一方で，次の値は計算が簡単であり，quenched average の下からの評価として用いられることがある： \\[\nf_{\\text{annealed}}:=-\\frac{1}{\\beta N}\\log[Z].\n\\] これは annealed average と呼ばれる．\nこうして，quenched ensemble, average ensemble の両方が用いられることがあるが，この両方に共通していたのが \\(Y_{ij}\\) に関する平均を先に取る という点である．\nしかし，planted ensemble ではこのアプローチをとっていない．\n\nThis notion of averaging over disorder is so well engraved into a physicist’s mind that going away from this framework and starting to use the related (replica and cavity) calculations for a single realization of the disorder has lead to a major paradigm shift and to the development of revolutionary algorithms such as the survey propagation (M. Mézard et al., 2002).\n\nなお，この要約伝搬法は Bethe 近似に関係がある (Kabashima, 2005)．\n\n\n\n一方で，planted ensemble では，スピンの配置を決めてから，\\(Y_{ij}\\) を生成している．当然，実際の物理過程とは違うものであり，Monte Carlo 法の真逆をやっているものであるが，ベイズ統計学との関連の結節点になる．\nそれだけでなく，スピングラス転移などの物理現象に関しても示唆を与える対象だとわかりつつある．18\n\n\n2.5 レプリカ対称性の破れ\nレプリカ対称性は大雑把に，平衡状態のダイナミクスの緩和過程によって表現できる．\n事後分布 \\(p(x|y)\\) について\n\n\n\n\n\n\nレプリカ法によるレプリカ対称性の破れパターン19\n\n\n\n\nレプリカ対称 であるとは，この平衡分布からスタートする MCMC が，系が大きくなる極限 \\(N\\to\\infty\\) で，どの平衡状態も一様に線型の複雑性でサンプリング可能であることをいう．20\nd1RSB (dynamical one-step of replica symmetry breaking) とは，系が大きくなる極限 \\(N\\to\\infty\\) に対して線型の複雑性でサンプリング可能な平衡状態が，指数減少していくことを言う．\n1RSB (static one-step RSB) とは，平衡分布からスタートする MCMC がサンプリング可能な平衡状態が，全体の一部に留まることをいう．また，任意の２つの平衡状態の間の距離は，\\(N\\to\\infty\\) の極限で殆ど確実に，２つの値のいずれかのみを取ることを言う．\nFRSB (full-step RSB) とは，任意の２つの平衡状態の間の距離が，ある絶対連続分布に従うことをいう．\n\n\n\nRSB 下では，\\(N\\to\\infty\\) の極限で，Boltzmann 分布の自己平均化＝測度の集中が起こらない．\n\n\n\n\n\n\n例（自発磁化密度）21\n\n\n\n\n\n平衡状態から開始する Glauber 動力学を考えると，例えば \\(d\\ge2\\) 次元での外場を持たない強磁性 Ising 模型では，臨界温度以下である限り， \\[\n\\lim_{t\\to\\infty}\\lim_{N\\to\\infty}\\langle\\sigma_i(t)\\rangle_G\\ne\\langle\\sigma_i\\rangle\n\\] が成り立つ．ただし，\\(\\langle-\\rangle_G\\) は特定の初期状態を持つ Glauber 動力学に関する平均とした．\n特に，ある自発磁化密度 \\(M(\\beta)&gt;0\\) が存在して， \\[\n\\lim_{t\\to\\infty}\\lim_{N\\to\\infty}\\langle\\sigma_i(t)\\rangle_G\\in\\{\\pm M(\\beta)\\}\n\\] である．\nこの「準平衡状態」と言える状態にとらわれる現象は普遍的であるが，この「準平衡状態」が，初期位置の関数として，時間にも空間にも不変性が存在しないときが，スピングラスである．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes2.html#a-planted-spin-glass-model-of-error-correcting-codes",
    "href": "posts/2024/Stat/Bayes2.html#a-planted-spin-glass-model-of-error-correcting-codes",
    "title": "ベイズ統計学とスピングラス",
    "section": "3 A Planted Spin Glass Model of Error Correcting Codes",
    "text": "3 A Planted Spin Glass Model of Error Correcting Codes\n誤り訂正符号を例に取り，censored block model (Abbe et al., 2014) とも呼ばれる具体的な planted ensemble を考える．22\n\n3.1 設定\n\n\n\n\n\n\nplanted spin glass of error correcting code\n\n\n\n\n\\(p_\\alpha(x)\\) を \\(\\{\\pm1\\}^N\\) 上の 一様分布とする．\n辺の集合 \\(\\mathcal{E}\\) は，大きさ \\(M\\) で，ランダムに生成されるものとする．23\n\\(p_\\rho(y|x)\\) は，ある確率 \\(\\rho\\in(0,1)\\) に関して， \\[\n  p(y_{ij}|x_i^*,x_j^*)=\\rho\\delta_{y_{ij},x_i^*x_j^*}+(1-\\rho)\\delta_{y_{ij},-x_i^*x_j^*}\n  \\] の直積として定まるとする．24\n\n\n\nこの設定下では，事後分布は \\[\\begin{align*}\n    p(x|y)&=\\frac{p(y|x)p(x)}{p(y)}\\\\\n    &=\\frac{\\prod_{(i,j)\\in E}e^{\\beta^*y_{ij}x_ix_j}}{2^N(2\\cosh\\beta^*)^Mp(y)}\n\\end{align*}\\] と表せる．ただし， \\[\n\\beta^*:=\\beta^*(\\rho):=\\frac{1}{2}\\log\\frac{\\rho}{1-\\rho}\n\\tag{2}\\] とした．\nこれは，Hamiltonian \\[\nH(x,y):=-\\sum_{(i,j)\\in\\mathcal{E}}y_{ij}x_ix_j\n\\tag{3}\\] が定める Boltzmann 分布に他ならない．\n\n\n\n\n\n\n証明25\n\n\n\n\n\nこの設定下では， \\[\n\\rho=\\frac{e^{\\beta^*}}{2\\cosh\\beta^*},\\qquad\\beta^*:=\\frac{1}{2}\\log\\frac{\\rho}{1-\\rho}\n\\] が成り立ち， \\[\np(y_{ij}|x_i^*,x_j^*)=\\frac{e^{\\beta^*y_{ij}x^*_ix^*_j}}{2\\cosh\\beta^*}\n\\] と表せるためである．\n\n\n\n\n\n\n\n\n\n解釈\n\n\n\n\n\n\nまず，二元対称通信路における誤り訂正符号としての見方ができる (Iba, 1999, pp. 3879 3節)\n\\((i,j)\\in\\mathcal{E}\\) に関して， \\[\n  y_{ij}^{\\text{in}}:=x_ix_j\n  \\] が送信符号であるとし，確率 \\(1-\\rho\\) で誤りが生じるとする： \\[\n  y_{ij}=\\begin{cases}\n      y_{ij}^{\\text{in}} & \\text{with probability }\\rho,\\\\\n      -y_{ij}^{\\text{in}} & \\text{with probability }1-\\rho.\n  \\end{cases}\n  \\]\n次に，teacher-student sinario としての見方もできる (Zdeborová and Krzakala, 2016, pp. 473–2.1節)\n\\(N\\) 人が，\\(\\pm1\\) のいずれかが書かれたカードを持っているとし，教師はこれを知っているとする．\n生徒は，人物 \\(x_i,x_j\\) が持っているカードが一致しているならば \\(y_{ij}=1\\)，そうでないならば \\(y_{ij}=-\\) と知らされるが，正しく知らされる確率は \\(\\rho\\) であるとする．\n\n\n\n\n\n\n3.2 ベイズ推定からの解釈\nそこで，前節の計算に基づき，事後分布を分布族 \\[\np(x|y)\\,\\propto\\,e^{-\\beta H(x,y)},\\qquad\\beta&gt;0,\n\\tag{4}\\] によりモデリングしたとすると，\\(\\beta=\\beta^*(\\beta)\\) の場合が真の事後分布に当たる．\nただし，この真のハイパーパラメータの値 \\(\\beta^*(\\rho)\\) はわからないとする．\nこのとき例えば，平均正解個数を最大化した MMO (Maximum Mean Overlap) 復号をしたいならば，26 \\[\n\\widehat{x}_i=\\mathrm{sign}(\\langle x_i\\rangle)\n\\tag{5}\\] というように，planted spin glass 系の平衡状態における磁化密度を計算することで達成できる．汎用的には，Monte Carlo 法を通じて解けることになる．27\nこうして，推定の問題が，スピングラスの planted ensemble の平衡状態のシミュレーションや基底状態の探索の問題に翻訳されたことになる．\n特に，\\(\\beta=\\beta^*(\\rho)\\) に設定した場合のみ，西森条件が成り立ち，これを満たす値 \\((\\beta,\\rho)\\) の集合を西森ラインという．\n\n\n3.3 西森ラインの発見\n西森ラインは元々，純粋に Hamiltonian \\(H\\) と Boltzmann 分布 (4) を持つ Edwards-Anderson モデルの解析の中で発見されたものであり，ベイズ統計学との関連はその約 10 年後になってから見出された．\n\n3.3.1 ゲージ変換\n(H. Nishimori, 1980) では，この Hamiltonian (3) が，任意の点 \\(\\widetilde{x}\\in\\{\\pm1\\}^N\\) が定める次の変換に対して不変であることが利用された： \\[\nx_i\\mapsto x_i\\widetilde{x}_i,\n\\] \\[\ny_{ij}\\mapsto y_{ij}\\widetilde{x}_i\\widetilde{x}_j.\n\\]\nこの変換を真の配置 \\(\\widetilde{x}=x^*\\in\\{\\pm1\\}^N\\) について考える．真の配置 \\(x^*\\) がこのゲージ変換を受けると，全てのスピンが \\(1\\) に揃った状態に写される．\n続いて，相互作用項 \\(y_{ij}\\) は，確率 \\(\\rho\\) で \\(y_{ij}=1\\)，確率 \\(1-\\rho\\) で \\(y_{ij}=-1\\) を満たす独立同分布列となる．\nこうして，今回の設定 3.1 も Edwards-Anderson モデルに帰着され，この設定の中で全てのスピンが揃った強磁性状態を探索する問題に変換される．\nこの \\(\\{y_{ij}\\}\\) に関する Edwards-Anderson モデル \\(H\\) において，\\(\\rho\\) と \\(\\beta\\) の値の関係が系の振る舞いを決定し，その相図は元々の planted ensemble の性質と同一視できる．28\n\n\n3.3.2 西森ライン上での西森条件\nこのとき，Edwards-Anderson モデルは \\(\\beta=\\beta^*\\) （式 2）の場合に限り，内部エネルギーや比熱の上限が厳密解を持つ：29 \\[\n\\langle E\\rangle=-M\\tanh\\beta^*.\n\\tag{6}\\]\nこの条件 \\(\\beta=\\beta^*(\\rho)\\) を満たす組 \\((\\beta,\\rho)\\) を，\\(T\\)-\\(p\\) 相図上で見たとして，西森ライン と呼んだのであった．30\n\n\n\n平均次数 \\(3\\) を持つランダムグラフ上の \\(\\pm1\\) Edwards-Anderson モデルの \\(T\\)-\\(p\\) 相図．(Zdeborová and Krzakala, 2016, p. 477 Figure. 2) から．\n\n\n西森ラインは，相転移点 \\(\\beta_c\\) を通るにも拘らず，内部エネルギーは常に特異性を示さない．特異部が消えているのである．31\nそして何より，途中でスピングラス相を通過しない．32\n\n\n\n3.4 西森ラインの意味\n一般の模型に対してこのようなゲージ変換が見つかるわけでもなければ，その変換の物理的な意味が定かでない．\n\n3.4.1 ベイズによる解釈と一般化\nしかし，西森ラインとは，今回の設定 3.1，または一般の planted spin glass model （第 2.2 節）において，モデルが正しく特定されていること \\(\\beta=\\beta^*\\) の特別な場合と理解できる (Iba, 1999)．\n西森ラインの存在は長らく謎であったが，ベイズ推論の文脈では明確な意味を持つのである．特に，ゲージ対称性が存在しないモデルであっても，第 2.2 節の teacher-student scenario に当てはまる統計問題である限り，対応するスピングラス系に西森ラインは考えられる．33\n西森条件とは，モデルが正しく特定されている場合に成り立つ魅力的な性質の数々であると理解できる．\n\n\n3.4.2 西森ライン上でのベイズ推論\nモデルの特定が正しかったとする：\\(\\beta=\\beta^*\\)．\n高温領域 \\(\\beta^*&lt;\\beta_c,\\rho&lt;\\rho_c\\) では，\\(\\rho\\) が \\(1/2\\) に近いことに対応する．対応する Edwards-Anderson モデルは常磁性相 P にある．もはやこの系は planted information \\(x^*\\) を憶えておらず，真値 \\(x^*\\) の推定が（情報理論的に）不可能である．\n一方で，低温領域 \\(\\beta^*&gt;\\beta_c,\\rho&gt;\\rho_c\\) では，Monte Carlo 法は高速に強磁性状態へ収束する．Monte Carlo 法により熱平均 (5) を計算することで，真値 \\(x^*\\) を最適に推定できる．\nこれら２つの領域の中間に「推定可能だが，計算が困難」という状態（スピングラス相）は存在せず，２つの領域の間には二次の相転移が見られる．34\n\\(T_c\\) の値は，ランダムグラフ \\(\\mathcal{G}\\) の平均次数が大きいほど大きくなる．即ち，観測の数が多いほど，真値 \\(x^*\\) を推定することが可能な範囲が広がる．\n\n\n\n3.5 誤特定の場合にはスピングラス相が現れる\nモデルの真のハイパーパラメータ \\(\\beta^*\\) を知らなかった場合を考える．\n\\(\\rho&lt;\\rho_c\\) であるとき，推定は不可能である．系を低温にしていくとスピングラス相が現れ，Monte Carlo 法の収束は圧倒的に遅くなる．加えて，推定は不可能であるはずなのに，\\(\\beta\\to\\infty\\) の極限でも磁化が残る．これは過適応・過学習の現象と対応する．\n\\(\\rho&gt;\\rho_c\\) であるとき，推定は可能であるが，系が低温過ぎると強磁性相が存在しない可能性があるどころか，スピングラス相が同居する相が出現する可能性さえある．従って，ハイパーパラメータ \\(\\beta\\) を大きくしすぎないことが大事である．\nスピングラス相，過学習，いずれの現象も，西森ライン上では見られない．スピングラス系の不可知な性質は，真のモデルが不明な場合の推定問題の困難さと同根なのである．\n\n\n3.6 系の温度はハイパーパラメータに対応する\n以上の，Edwards-Anderson 模型の相図は，統計推測において「ハイパーパラメータ \\(\\beta^*\\) の選択が大事」であることを示唆してくれる．\n実際，EM アルゴリズム (Dempster et al., 1977) はこれを考慮した MAP 推定と捉えられる．\n\n    \n        \n            \n            \n                変分推論２\n                EM アルゴリズム\n            \n        \n    \n\nEM アルゴリズムは，エビデンス \\(p_\\beta(x|y)\\) を最大化するように \\(\\beta\\) を調整することでハイパーパラメータの真値 \\(\\beta^*\\) と MAP 推定量 \\(x^*\\) を同時に探索する．\n具体的には，温度 \\(\\beta\\) における自由エネルギーを計算し（\\(E\\)-ステップ），その最大化を図る（\\(M\\)-ステップ）ことを繰り返す．\nスピングラスで西森ラインから外れた状態では物理量の正確な計算が困難であるように，EM アルゴリズムでも一般に \\(E\\)-ステップは intractable であることが知られている．\nまた，自由エネルギーの代わりに内部エネルギーを用い，最大値点 \\(\\beta^*\\) で満たすべき条件である西森条件 (6) を用いて \\(M\\)-ステップを実行することも考えられる．35\n即ち，西森条件を「成り立ってほしい条件」として，ハイパーパラメータの最大化に用いる方法が EM アルゴリズムであるとも捉えられるのである．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes2.html#終わりに",
    "href": "posts/2024/Stat/Bayes2.html#終わりに",
    "title": "ベイズ統計学とスピングラス",
    "section": "4 終わりに",
    "text": "4 終わりに\n\nモデルの誤特定 \\(\\beta\\ne\\beta^*\\) が起こった場合，統計解析は泥沼に陥る．\nこの「泥沼」とは，物理的に正確な意味で，スピングラスであったわけだ．\n逆に，モデルを正しく特定できているとき．通信の問題において，encoding と channel のモデルが正しく理解されているとき，復号難易度は大きく下がるはずである．これが，西森ライン上で物理量の計算が一気に簡単になる理由である．"
  },
  {
    "objectID": "posts/2024/Stat/Bayes2.html#footnotes",
    "href": "posts/2024/Stat/Bayes2.html#footnotes",
    "title": "ベイズ統計学とスピングラス",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n西森ライン ともいう．(Iba, 1999)，(西森秀稔, 1999, p. 55)など参照．↩︎\n(横尾英俊, 2004, pp. 101–102)も参照．↩︎\n(横尾英俊, 2004, p. 106) も参照．↩︎\n(樺島祥介 and 杉浦正康, 2008, p. 28) も参照．↩︎\n充足可能性問題と違い，３項以上の相互作用を考えてもクラス P のままである．(Marc Mézard and Montanari, 2009, p. 246) や (樺島祥介 and 杉浦正康, 2008) も参照．↩︎\n例えば (Talagrand, 2011) など．↩︎\n従って，planted ensemble における \\(\\{y_{ij}\\}\\) は，同一の真値 \\(x^*\\) で繋がっているため，強い相関を持つ．この相関から \\(x^*\\) を読み解けるか？が逆に問題となっているのである．↩︎\n特に，信号処理などの情報科学的な設定で，この対応がつけやすい．というのも，一般の統計的な問題では，「スピングラスを生成する」部分に相当するような，データ生成過程に対する事前知識を持っていることが稀であり，モデル選択も重要なトピックに入るためである．一方で，スピングラス系と対応づけるとき，モデル選択は一般に射程には入らない．(Zdeborová and Krzakala, 2016, p. 456) も参照．一般の統計的問題を，どのようにしてスピングラスの planted ensemble と対応づけられるかについては，(Zdeborová and Krzakala, 2016, p. 468) 1.3.2 節を参照．↩︎\nこうして生成されたスピングラス系を，planted ensemble と呼び，このシナリオを機械学習の訓練過程に準えて teacher-student scenario とも呼ぶのが (Zdeborová and Krzakala, 2016, p. 462) の用語である．これを (Iba, 1999, p. 3881) は符号理論に準えて，encoding-decoding scenario と呼ぶべき設定で解説している．(Marc Mézard and Montanari, 2009, p. 249) にも同様の通信に準えた記述がある．なお，最後の２つの文献では，事前分布 \\(p(x)\\) を一様分布に限っている．↩︎\n前稿 では，この \\(x\\) を \\(x^*\\) と表した．(Zdeborová and Krzakala, 2016) のように，これを lanted configuration または ground truth と呼ぶとわかりやすい．↩︎\nこの \\(y\\) は quenched disorder ともいう．quenched disorder が，ある信号 \\(x\\) とその通信 \\(p(y|x)\\) によって生成された系を，planted ensemble と考えるのである．そこで，(Zdeborová and Krzakala, 2016, p. 468) は planted disorder と呼んでいる．大変良い呼び名である．↩︎\n(Zdeborová and Krzakala, 2016, p. 468) に多くの関連文献がまとめられている．↩︎\n(Zdeborová and Krzakala, 2016, p. 476) も参照．↩︎\n(Zdeborová and Krzakala, 2016, p. 475) 2.1 節も参照．(Krzakala et al., 2015, p. 13) の \\([Z^n]_{\\text{planted}}=\\frac{[Z^{n+1}]_{\\text{quenched}}}{[Z]_{\\text{quenched}}}\\) も関係するかもしれない．↩︎\n(Marc Mézard and Montanari, 2009, p. 249) では \\([-]\\) ではなく \\(\\mathbb{E}\\) で表されている．これを quenched average という．↩︎\n(Zdeborová and Krzakala, 2016, pp. 480–2.5節)，(Marc Mézard and Montanari, 2009, pp. 102 5.4節) が詳しい．↩︎\n数学的な説明としては (Talagrand, 2011, p. 7) も参照．↩︎\nこのことは物理学にも進展を与えており，従来考えられなかった解析が進むことがあるという． (Krzakala et al., 2015, pp. 10 2.3節) や (Zdeborová and Krzakala, 2016, p. 483) 2.5.3 節 Quiet Planing を参照ください．↩︎\n(Zdeborová and Krzakala, 2016, p. 483) 2.6 節，(Marc Mézard and Montanari, 2009, p. 253) 12.3.3 節．↩︎\n(Zdeborová and Krzakala, 2016, p. 483) 2.6 節の興味深い説明方法．↩︎\n(Marc Mézard and Montanari, 2009, p. 250) 12.3.1 節参照．↩︎\n(Mattis, 1976) のモデルとも深い関連があるという．(Zdeborová and Krzakala, 2016, p. 474) も参照．ここでは planted SG model と呼ばれている．↩︎\nすなわち，\\(G(N,M)\\)-Erdős–Rényi ランダムグラフ とする．↩︎\n従って，各 \\(y_{ij}\\) は \\(x^*\\) で条件付けると独立である．↩︎\n(Zdeborová and Krzakala, 2016, p. 475)，(Iba, 1999, p. 3879)．↩︎\n(Krzakala et al., 2015, p. 6) では MARG (Minimal Error Assignments Estimator) と呼ばれている．↩︎\nまた，平均場の設定では cavity method によって解ける．(Zdeborová and Krzakala, 2016, p. 475) も参照．↩︎\nこの同一視は，Nishimori ensemble と planted ensemble との同一視と論じることもできる (Krzakala et al., 2015, p. 12) 2.6 節．↩︎\n(H. Nishimori, 1980) の報告である．(西森秀稔, 1999, pp. 53–)，(Iba, 1999, p. 3879) 式 (25)，(Marc Mézard and Montanari, 2009, p. 248) 式 (12.15) も参照．平均次数 \\(c\\) を持つランダムグラフ上の Edwards-Anderson モデルでは，\\(M:=\\lvert\\mathcal{E}\\rvert=c/2\\) となる．↩︎\n\\(\\beta^*\\) は \\(\\rho\\) の関数であることに注意．式 (2) 参照．↩︎\n(西森秀稔, 1999, p. 55) も参照．↩︎\nこれは静的な RSB 相が存在しないことを言う．１次の相転移，動的な one-step RSB (d1RSB) 相などは見られることがある (Zdeborová and Krzakala, 2016, pp. 484 2.7節)．↩︎\n(Iba, 1999, p. 3882) の時点で示唆されていた考え方である．↩︎\n(Zdeborová and Krzakala, 2016, pp. 478 2.3節) も参照．↩︎\n(Zdeborová and Krzakala, 2016, p. 479) 2.4節による素晴らしい洞察！↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory2.html",
    "href": "posts/2024/AI/Theory2.html",
    "title": "統計的学習理論２",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/AI/Theory2.html#pac-bayes",
    "href": "posts/2024/AI/Theory2.html#pac-bayes",
    "title": "統計的学習理論２",
    "section": "1 PAC-Bayes",
    "text": "1 PAC-Bayes\n通常の機械学習の枠組みでは，仮説集合 \\(\\mathcal{H}\\subset\\mathcal{L}(\\mathcal{X};\\mathcal{Y})\\) を固定し，この中で最適な推定量 \\(\\overline{h}\\in\\mathcal{H}\\) を探すことに集中する．\n一方で，PAC-Bayes では，仮説集合 \\(\\mathcal{H}\\) 上の確率分布を学習し，最終的に投票 (vote) などの確率的な操作によって決めることを考え，これにも対応する理論を構築する．1\nこれは (Shawe-Taylor and Williamson, 1997) によって創始され， (McAllester, 1999) によって最初の定理が示された．(Seeger, 2002), (Catoni, 2007) も金字塔であり，後者は情報統計力学との関連を推し進めている．\n\n1.1 枠組み\nデータにより決まる確率測度 \\[\n\\widehat{\\rho}:(\\mathcal{X}\\times\\mathcal{Y})^n\\to\\mathcal{P}(\\mathcal{H})\n\\] を考え，推定量をランダムに \\(\\widetilde{h}\\sim\\widehat{\\rho}\\) とサンプリングする．これを ランダム推定量 (randomized estimator) という．\n例えば \\(\\mathcal{Y}=2\\) においては，Gibbs 判別器と呼ばれる．2\nまた，最終的な推定量を積分により \\[\nh_{\\widehat{\\rho}}:=(\\widehat{\\rho}|h)\n\\] と決定しても良い．これを 集合推定量 (aggregated predictor) という．\nこれらの\n\n経験バウンド (empirical bound)：\\(R(\\widehat{h})-\\widehat{R}_n(h^*)\\)\n超過リスクバウンド (excess risk / oracle PAC bound)：\\(R(h_{\\widehat{\\rho}})-R(h^*)\\)\n\nを調べるのが PAC-Bayes である．\n\n\n1.2 KL-乖離度\nすると，\\(\\log M\\) の項に KL-乖離度が現れる．\n\n\n\n\n\n\nTip\n\n\n\n\n定義 1 (Kullback-Leibler divergence) \\(\\mu,\\nu\\in\\mathcal{P}(\\mathcal{H})\\) の Kullback-Leibler 乖離度 とは， \\[\n\\operatorname{KL}(\\mu|\\nu):=\\begin{cases}\n\\int_\\mathcal{H}\\log\\left(\\frac{d \\mu}{d \\nu}(\\theta)\\right)\\mu(d\\theta)&\\mu\\ll\\nu,\\\\\n\\infty&\\mathrm{otherwise}.\n\\end{cases}\n\\] をいう．\n\n\n\n\n\n1.3 McAllester バウンド\n\n1.3.1 応用\nSGD で訓練されたニューラルネットワークに対しても適用されている (Clerico et al., 2023)．\n\n\n\nPAC-Bayes による汎化バウンド (Dziugaite and Roy, 2017)\n\n\n事後分布からサンプリングをすることで鋭い評価を得ている (Ujváry et al., 2023)．"
  },
  {
    "objectID": "posts/2024/AI/Theory2.html#footnotes",
    "href": "posts/2024/AI/Theory2.html#footnotes",
    "title": "統計的学習理論２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Alquier, 2024) Introduction より．↩︎\n(Schölkopf and Smola, 2002, p. 381) 定義12.23．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Theory4.html",
    "href": "posts/2024/AI/Theory4.html",
    "title": "統計的学習理論４",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n機械学習モデルの社会実装が進むにつけて，経験リスク最小化 の枠組みでは足りず，さらに汎化性能を重要視した枠組みが必要になってくる．\n構造的リスク最小化 もその例であるが，基盤モデル の台頭を見た現代では，分布外リスク最小化 (IRM: Invariant Risk Minimization) という新たな枠組みが (Arjovsky et al., 2020) により提案されている．"
  },
  {
    "objectID": "posts/2024/AI/Theory4.html#ドメイン汎化-wang2023-domain",
    "href": "posts/2024/AI/Theory4.html#ドメイン汎化-wang2023-domain",
    "title": "統計的学習理論４",
    "section": "1 ドメイン汎化 (J. Wang et al., 2023)",
    "text": "1 ドメイン汎化 (J. Wang et al., 2023)\n現状の多くの理論と手法は，訓練データとテストデータは同じ標本を分割したものにすぎず，同じ分布に従うことを前提としている．しかし新たに生じた多くの応用の場面では，新たな分布に対しての汎化性能が特に肝要である．\nこれに対処するために，複数のドメインを用意し，未知のドメインに対する汎化性能を高めたいとする問題を ドメイン汎化 (domain generalization) または 分布外汎化 (out-of-distribution generalization) と呼ぶ．1\n\n1.1 枠組み\nドメイン汎化では，入力空間 \\(\\mathcal{X}\\) と出力空間 \\(\\mathcal{Y}\\) は固定されている．\nある分布 \\(\\mathbb{P}\\sim\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Y})\\) からの独立同分布列 \\(\\mathcal{S}=\\{(x_i,y_i)\\}_{i=1}^N\\subset\\mathcal{X}\\times\\mathcal{Y}\\) を ドメイン という．\nドメイン汎化は，複数のドメイン \\(\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_K\\) が与えられた状態から，まだ見ぬドメイン \\(\\mathcal{S}\\) におけるリスクの最小化を目指す問題である．\n\n\n1.2 関連する話題\n\nマルチタスク学習 (Caruana, 1997)\n複数のタスクにおいて同時に良い性能を出すモデルを学習する枠組み．複数のドメイン \\(\\mathcal{S}_1,\\cdots,\\mathcal{S}_K\\) において平均的に良い性能を出すことを目指す，などの問題も含む．\n転移学習 (Zhuang et al., 2021)\n始域タスクと終域タスクが異なる場合の学習を指す．終域タスクが既知であるという前提があり，事前学習-事後調整 (pretraining-finetuning) という手法が最も一般的である．\nドメイン適応 (M. Wang and Deng, 2018)\n特にドメインが異なる場合の転移学習を指す．終域ドメインが既知であるという点がドメイン汎化と異なる．\nメタ学習 (Vanschoren, 2018), (Hospedales et al., 2022)\n新たなタスクに対して「学習法を学習する」というメタ的な学習を目指す．ドメイン汎化は同じタスクでドメインを変えたものに対する汎化を目指すため，メタ学習はドメイン汎化における有力な手法の一つということになる．\n継続学習 (continual / lifelong learning) (Biesialska et al., 2020)\n例示なし学習 (zero-shot learning)\n例示なしで新たなクラスに対する分類を行う問題．ドメイン汎化はクラスは同じで分布のみが異なる．\n\n\n\n1.3 ドメイン汎化の手法\n大きく分けて次の３通りの手法が存在する．\n\n表現学習 (representation learning)\n最も主要なアプローチは，ドメイン汎化に適した特徴空間をデザインすることである．主に次の２つの接近がある．\n\nドメイン不変な表現学習 (domain-invariant representation learning) を行うことを考える．主な手法には 分布外リスク最小化 による学習や，敵対的学習による方法 (Ganin et al., 2016) などがある．\n特徴分離 (feature disentanglement) により，ドメインに依存しない特徴とドメイン依存の特徴とを分離する．\n\nデータ操作 (data manipulation)\n同じくドメイン汎化に適した特徴空間をデザインするのが目的であるが，これを データ拡張 やデータの生成によって達成することを目指すこともできる．\n学習枠組み (learning paradigm)\n集合学習 (ensemble learning) や メタ学習 などのように，学習のアプローチから変えることも考えられる．\n\n\n1.3.1 分布外リスク最小化\nドメイン汎化が失敗する理由の一つに，因果関係がないが相関関係がある要素（擬似相関）を学習して予測に使ってしまうことがある．\nこの問題は分布外リスク最小化 (Arjovsky et al., 2020) によって対処できることが実験的に示されており，近年理論的な解明 (Toyota and Fukumizu, 2024) も進んでいる．"
  },
  {
    "objectID": "posts/2024/AI/Theory4.html#footnotes",
    "href": "posts/2024/AI/Theory4.html#footnotes",
    "title": "統計的学習理論４",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(J. Wang et al., 2023) がドメイン汎化に対する最初のサーベイである．(Jindong Wang and Chen, 2023, p. 175) 11章 も参照．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html",
    "href": "posts/2024/AI/Bayes.html",
    "title": "ベイズとは何か",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n多くの応用を持つが，原理は同一である．\nベイズ深層学習，ベイズ最適化，……"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#始まりは区間推定の問題であった",
    "href": "posts/2024/AI/Bayes.html#始まりは区間推定の問題であった",
    "title": "ベイズとは何か",
    "section": "1.1 始まりは区間推定の問題であった",
    "text": "1.1 始まりは区間推定の問題であった\n\n\n\n\n\n\n\n\nベイズが取り組んだ問題（現代語訳）1\n\n\n\n２値の確率変数は \\(Y_i\\in\\{0,1\\}\\) はある確率 \\(\\theta\\in(0,1)\\) で \\(1\\) になるとする： \\[\nY_i=\\begin{cases}\n1&\\text{確率 }\\theta\\text{ で}\\\\\n0&\\text{残りの確率} 1-\\theta\\text{ で}\n\\end{cases}\n\\] このような確率変数の独立な観測 \\(y_1,\\cdots,y_n\\) から，ある区間 \\((a,b)\\subset[0,1]\\) に \\(\\theta\\) が入っているという確率を計算するにはどうすれば良いか？\n\n\n\n\n決定的特徴：未知のパラメータ \\(\\theta\\) に対する確率分布を考えている．\n与えられている観測のモデル \\(p(y|\\theta)\\) に対して，逆の条件付き確率 \\(p(\\theta|y)\\) を考えれば良い．\nそのための計算公式として「ベイズの定理」を導いた (Bayes, 1763)．"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#ベイズの定理",
    "href": "posts/2024/AI/Bayes.html#ベイズの定理",
    "title": "ベイズとは何か",
    "section": "2.1 ベイズの定理",
    "text": "2.1 ベイズの定理\n\n\n\n\n\n\nベイズの定理2\n\n\n\n任意の可積分関数 \\(g\\)，確率変数 \\(\\Theta\\sim\\operatorname{P}^\\Theta\\)，部分 \\(\\sigma\\)-代数 \\(\\mathcal{G}\\) について， \\[\n\\operatorname{E}[g(\\Theta)|\\mathcal{G}](\\omega)=\\frac{\\int_\\mathbb{R}g(\\theta)p(\\omega|\\theta)\\operatorname{P}^{\\Theta}(d\\theta)}{\\int_\\mathbb{R}p(\\omega|\\theta)\\operatorname{P}^\\Theta(d\\theta)}\\;\\;\\text{a.s.}\\,\\omega\n\\]\n\n\n一般には次の形で使う： \\[\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{\\int_\\Theta p(x|\\theta)p(\\theta)\\,d\\theta}\n\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n確率空間を \\((\\Omega,\\mathcal{F},\\operatorname{P})\\)，確率変数 \\(\\Theta\\) は可測関数 \\(\\Omega\\to\\mathcal{X}\\)，可積分関数は \\(g\\in\\mathcal{L}(\\mathcal{X})\\) とし，定理の式は確率測度 \\(\\operatorname{P}\\) に関して確率 \\(1\\) で成り立つという意味であるとした．\n可測空間 \\((\\Omega,\\mathcal{G})\\) 上の測度 \\(\\operatorname{Q}\\) を \\[\n\\operatorname{Q}(B):=\\int_B g(\\theta(\\omega))\\operatorname{P}(d\\omega),\\qquad B\\in\\mathcal{G}\n\\] と定めると， \\[\n\\operatorname{E}[g(\\Theta)|\\mathcal{G}]=\\frac{d \\operatorname{Q}}{d \\operatorname{P}}.\n\\] なお，この定理は暗黙に条件付き期待値 \\(\\operatorname{P}[B|\\Theta]\\) は正則で，\\((\\Omega,\\mathcal{G})\\) 上の \\(\\sigma\\)-有限な参照測度 \\(\\lambda\\) に対して次の密度を持つことを仮定した： \\[\n\\operatorname{P}[B|\\Theta=\\theta]=\\int_B p(\\omega|\\theta)\\lambda(d\\omega).\n\\] この下では，Fubini の定理から \\[\n\\begin{align*}\n  \\operatorname{P}[B]&=\\int_\\mathbb{R}\\operatorname{P}[B|\\Theta=\\theta]\\operatorname{P}^\\Theta(d\\theta)\\\\\n  &=\\int_B\\int_\\mathbb{R}p(\\omega|\\theta)\\operatorname{P}^\\Theta(d\\theta)\\lambda(d\\omega)\n\\end{align*}\n\\] \\[\n\\begin{align*}\n  \\operatorname{Q}[B]&=\\operatorname{E}[g(\\Theta)\\operatorname{E}[1_B|\\sigma[\\Theta]]]\\\\\n  &=\\int_\\mathbb{R}g(\\theta)\\operatorname{P}[B|\\Theta=\\theta]\\operatorname{P}^\\Theta(d\\theta)\\\\\n  &=\\int_B\\int_\\mathbb{R}g(\\theta)p(\\omega|\\theta)\\operatorname{P}^\\Theta(d\\theta)\\lambda(d\\omega).\n\\end{align*}\n\\] よってあとは \\[\n\\frac{d \\operatorname{Q}}{d \\operatorname{P}}=\\frac{d \\operatorname{Q}/d\\lambda}{d \\operatorname{P}/d\\lambda}\\;\\operatorname{P}\\text{-a.s.}\n\\] を示せば良い．これは (Shiryaev, 2016, p. 273) に譲る．"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#ベイズ推論のもう一つのピース事前分布",
    "href": "posts/2024/AI/Bayes.html#ベイズ推論のもう一つのピース事前分布",
    "title": "ベイズとは何か",
    "section": "2.2 ベイズ推論のもう一つのピース「事前分布」",
    "text": "2.2 ベイズ推論のもう一つのピース「事前分布」"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#帰納的推論の確率的拡張としてのベイズ推論",
    "href": "posts/2024/AI/Bayes.html#帰納的推論の確率的拡張としてのベイズ推論",
    "title": "ベイズとは何か",
    "section": "2.3 帰納的推論の確率的拡張としてのベイズ推論",
    "text": "2.3 帰納的推論の確率的拡張としてのベイズ推論"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#生物の不確実性の下での推論のモデルとしてのベイズ推論",
    "href": "posts/2024/AI/Bayes.html#生物の不確実性の下での推論のモデルとしてのベイズ推論",
    "title": "ベイズとは何か",
    "section": "2.4 生物の不確実性の下での推論のモデルとしてのベイズ推論",
    "text": "2.4 生物の不確実性の下での推論のモデルとしてのベイズ推論\n\n脳の平時の活動は経験的事前分布を表現していると解釈できる (Berkes et al., 2011)\n脳の神経回路はベイズ推論（正確には，事後分布からのサンプリング）を行っている可能性がある (Terada and Toyoizumi, 2024)"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#ベイズ計算という分野",
    "href": "posts/2024/AI/Bayes.html#ベイズ計算という分野",
    "title": "ベイズとは何か",
    "section": "3.1 「ベイズ計算」という分野",
    "text": "3.1 「ベイズ計算」という分野\n\\[\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{\\int_\\Theta p(x|\\theta)p(\\theta)\\,d\\theta}\n\\]\n\nベイズの定理で終わりじゃない．\n→「どう実際に計算するか？」（特に分母の積分が問題）\nベイズ統計，ベイズ機械学習…… はすべてベイズの定理を使っている．\n→効率的で汎用的な計算方法を１つ見つければ，多くの応用分野に資する．"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#ベイズ計算の問題意識",
    "href": "posts/2024/AI/Bayes.html#ベイズ計算の問題意識",
    "title": "ベイズとは何か",
    "section": "3.2 「ベイズ計算」の問題意識",
    "text": "3.2 「ベイズ計算」の問題意識\n\n受験問題で出題される積分問題は，解析的に解ける異例中の異例\n加えて，「解析的に解ける」もののみを扱うのでは，モデリングの幅が狭すぎる\n\nどんな関数 \\(p(x|\\theta),p(\\theta)\\) に対しても積分 \\[\n\\int_\\Theta p(x|\\theta)p(\\theta)\\,d\\theta\n\\] が計算できる方法が欲しい．"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#積分はどう計算すれば良いか",
    "href": "posts/2024/AI/Bayes.html#積分はどう計算すれば良いか",
    "title": "ベイズとは何か",
    "section": "3.3 積分はどう計算すれば良いか？",
    "text": "3.3 積分はどう計算すれば良いか？\n\n数値積分（グリッド法）\n→ Riemann 積分の定義を地で行く計算法\n→ ３次元以上でもう現実的には計算量が爆発する\nモンテカルロ積分法\n→ 確定的なグリッドを用いるのではなく，乱数を用いる\n\n\nIt is evidently impractical to carry out a several hundred-dimensional integral by the usual numerical methods, so we resort to the Monte Carlo method. (Metropolis et al., 1953, p. 1088)"
  },
  {
    "objectID": "posts/2024/AI/Bayes.html#footnotes",
    "href": "posts/2024/AI/Bayes.html#footnotes",
    "title": "ベイズとは何か",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bayes, 1763)↩︎\n(Shiryaev, 2016, p. 272) (34) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html",
    "href": "posts/2024/AI/Semiconductor.html",
    "title": "半導体入門",
    "section": "",
    "text": "スライドを全画面で開くにはこちら"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#半導体入門",
    "href": "posts/2024/AI/Semiconductor.html#半導体入門",
    "title": "半導体入門",
    "section": "1 半導体入門",
    "text": "1 半導体入門\n\n1.1 半導体とは？\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．1\nこのような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．2 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．\n\n\n1.2 半導体発見の歴史\n(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，3 その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える (Sze and Lee, 2012, p. 1)．4\nBraun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．\n(Round, 1907) はダイオードが電界を印加することで発行することがある (electroluminescence) ことを発見した．\n\n\n1.3 基本用語のまとめ\n半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した ウエハ (wafer) 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．\n\n\n\nチップの断面構造\n\n\n一つのウエハから多数のチップが作成され，その各単位を ダイ (die) ともいう．\nしかし，普段我々が目にする IC チップ は パッケージ されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．\n32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．5\n\n\n1.4 集積回路が出来るまで\nまず回路を設計し，原版（マスター）を作る．これを フォトマスク (photomask) または レティクル (reticle) という．\nこれをウエハに転写するには，フォトリソグラフィ (Photolithography) を用いる．シリコンウエハの形成は，Czochralski 法 (Czochralski, 1918) による．6\nリソグラフィ自体は 1798 年からあり，Niépce が 歴青 が感光剤の役割を果たすことを発見し，カメラの発明と同時に発見された．\nエッチングに耐性のある感光剤を使えば，半導体デバイスの製造に応用できると気づいたのは (Andrus, 1957) である．この技術は半導体製造コストの 35 %を占めており，半導体市場の急成長はほとんどこの技術の進歩と両輪であると言う者も多い．7\nシリコン表面に酸化被膜を形成することで不純物原子の移動を阻止できることは (Frosch and Derick, 1957) が発見した．\n以上の技術を用いて，最初の IC は (Kilby, 1959) が作った．Jack Kilby はその後 2000 年にノーベル物理学賞を受賞する．\nしかし真に大量生産可能にし，半導体産業を大きくしたのは (Noyce, 1959) の発明であった．これは，現在主流の製法の基である Planar Process (Hoerni, 1960) で作られた．\n実際に，どのように半導体チップを製造するかについては次節 2 で詳しく解説する．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#sec-manufacturing",
    "href": "posts/2024/AI/Semiconductor.html#sec-manufacturing",
    "title": "半導体入門",
    "section": "2 半導体の製造",
    "text": "2 半導体の製造\n半導体の製造段階は，典型的には次の３つに大別される．\n\n設計\n\n回路・レイアウト作成 (design)\nフォトマスク (photomask) 作成\n\n前工程：次の３工程を繰り返ことで何層もの膜を積層する\n\n成膜 (deposition)\nパターン転写 (exposure)\n食刻 (etching)\n\n後工程\n\n角切り (dicing)\n封入 (packaging)\n品質検査 (inspection)\n\n\n日立の解説 も参照．\n\n2.1 設計工程\n18に分類されることもある．\n\n2.1.1 回路・レイアウト作成\nEDA (Eleotron Design Automation) を用いて設計を行う．\n\n\n\n2.2 前工程\n\n2.2.1 基本３工程\n成膜，パターン転写，食刻の３工程を繰り返すことでウエハ上に構造を作っていくのであるが，その目的は大きく５つに分類出来る．\n\n素子分離領域形成\n酸化被膜により，素子間の絶縁を形成する．\nwell 形成\nトランジスタの基盤となる領域に，食刻の代わりにイオンの添加する．\nトランジスタ形成\nウエハ基盤上にトランジスタ素子を形成する．\n電極形成\nシリコン基盤上のトランジスタに届くように，すでに形成された層に穴 (contact hole) をあけ，導体を埋め込む．\n配線層形成\n基本３工程を繰り返すことで，トランジスタ層上を分厚い配線層で覆う．8\n\n\n\n2.2.2 異物検査と洗浄\nほとんどの工程間に，異物検査と洗浄の工程が必要になる．\n日立の製品例，ウェーハ欠陥検査\n\n\n2.2.3 表面酸化\n熱酸化法では，酸素や高温のスチームを当てることで，表面に SiO2 の酸化被膜を形成し，絶縁体として用いる．\n\n\n2.2.4 成膜\n\n\n2.2.5 パターン転写\nフォトマスク上から紫外線を当てることで，フォトレジストを感光させる．次の工程で感光部分のみを食刻することで，パターン該当部分のみに酸化被膜を残すことが出来る．\n\n\n2.2.6 食刻\n現像後は，寸法計測を行う (ADI: After Development Inspection)．これは走査性電子顕微鏡 (SEM: Scanning Electron Microscope) である CD-SEM などを用いて行う（日立の製品例）\nこれにより，正しくパターンが転写されていることが確認されたのち，食刻を行い，再び寸法計測を行う（AEI: After Etch Inspection などと呼び分ける）．\n最後に，残ったフォトレジストはオゾンやプラズマにより灰化 (ashing) により除去する．\nコンダクターエンチング（日立の例）\n\n\n2.2.7 イオン添加\nイオンを注入するとアモルファスとなるため，一度再加熱をして (annealing) 再結晶化する．\n\n\n\n2.3 後工程"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#半導体デバイス",
    "href": "posts/2024/AI/Semiconductor.html#半導体デバイス",
    "title": "半導体入門",
    "section": "3 半導体デバイス",
    "text": "3 半導体デバイス\n\n3.1 構成要素\n整流作用を示す接合には，ショットキー接合と pn 接合がある．\n\n\n3.2 双極から MOS へ\nMOS (Metal-Oxide-Semiconductor) の３層構造を用いたトランジスタが採用される前は，双極トランジスタ (bipolar transistor) (Shockley, 1949) を用いた TTL (Transistor-Transistor Logic) 回路が主流であった．\nしかしこれは，トランジスタの間の絶縁が難しく，密度を上げることが難しかった．そのような中で MOSFET (Metal-Oxide-Silicon Field-Effect Transistor) が開発された (Kahng and Atalla, 1960)．MOSFET 技術は現代の半導体市場の 95% に関連する．9\nMOSFET は自己絶縁構造を持つため，これ以上の絶縁処理を必要とせず，双極トランジスタの 10 %の面積で済んだ．\n\n\n3.3 CMOS (Complementary Metal-Oxide-Semiconductor)\nMOS が初め採用した設計論理のは NMOS や PMOS であったが，常に直流電流を消費する必要があった．\nしかし現代では，P 型と N 型の MOSFET を相補的に用いる CMOS が集積回路における支配的な技術である．\nその秘訣は消費電力の少なさにあり，トランジスタの \\(0,1\\) の切り替えの際に生じる電力（動的エネルギー）のみが消費される．10\n\nCMOS技術は、当初アメリカの半導体業界では、当時より高性能だったNMOSを優先して見過ごされていた。しかし、CMOSは低消費電力であることから日本の半導体メーカーにいち早く採用され、さらに進化し、日本の半導体産業の隆盛につながった。CMOS\n\n\n\n3.4 現代に汎在する半導体\n計算機を構成する要素は多いが，Moore の法則により，多くが同一のチップの上に載ってしまい，不可視化が進んでいる．11\n\n3.4.1 プロセッサ\nコンピュータの CPU と言ったときに，１枚のチップを意味するようになったのは 1971 年の Intel 4004 が初めてである．12\n3mm × 4mm のチップ上に 2300 の MOSFET を備え，大きな机ほどの CPU を備えた IBM コンピュータに匹敵する処理能力を持っていた．13\n\n\n3.4.2 半導体メモリ\n現代でメモリといえば RAM (Random Access Memory) を指す．本来はアクセスする順番に制約があった SAM (Sequential Access Memory) に対して作られた言葉であったが，現代では ROM (Read Only Memory) との対義語として理解されることが多いようである．\nSRAM (Static RAM) は半導体メモリの一種であり，DRAM (Dynamic RAM) と比べて高速である．１ビットあたり６から８のトランジスタを使用したフリップフロップ回路により情報を記憶するため，定期的なリフレッシュが不要で高速な読み書きが可能であるが，集積率を上げることが出来ず，大容量メモリには向かない．14\nDRAM (Dynamic RAM) (Dennard, 1967) はチップ内にコンデンサを備えており，１つのコンデンサで１ビットを表現する．これを読み出すのに１つのトランジスタ MOSFET を使うのみであるから，SRAM に比べて安価であるが，電荷は時間と共に散逸するため，定期的にリフレッシュする必要があり，消費電力は大きい．15\n\n\n\nFrom (Patterson and Hennessy, 2014, p. 380)\n\n\nDRAM と SRAM はいずれも揮発性である．不揮発性の半導体メモリには フィラッシュメモリ がある．これには 蜉蝣ゲートMOSFET という素子で捉えた電子により情報を記憶することで不揮発性を実現している．\nフラッシュメモリには NAND 型と NOR 型があり，前者が主流である．\n\n\n\nMemory Revenue May, 2022. Source: Omedia\n\n\n\n\n3.4.3 Graphics Processing Unit\n描画を扱うチップは従来 VGA コントローラーと呼ばれていたが，1999 年には１つのチップで描画タスクの殆どをこなせるようになり，特に NVIDIA GeForce 256 は GPU という名称で売り出された．\nこうして GPU は元来の 3D グラフィクスに特化した存在から，徐々に CPU を補完する多様なタスクに柔軟に対応できるように，プログラム可能で，大量のコアを持って並列計算可能なものに進化していった．近年の CPU はマルチコアのものが多いが，現在の GPU は 1000 コアを超えるものも多い．\n\n\n3.4.4 Language Processing Unit\nLPU は，Google で TSU (Tensor Processing Unit) のプロジェクトに初期から従事していたエンジニア Jonathan Ross が 2016 年に創業したスタートアップ Groq の 登録商標 である．\nGroq が Samsung と協力して 実現した LPU (Abts et al., 2022) は eDRAM を持つ ASIC (Application Specific Integrated Circuit) であり，メモリのバンド幅と計算密度を増やし，逐次処理に特化することで特に言語処理に特化している．\n2/20/2024 に デモ を公開した．\nLPU は推論に，GPU は学習に特化しており，相補的な役割を演じながら AI の進化を支えていく可能性がある．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#物理",
    "href": "posts/2024/AI/Semiconductor.html#物理",
    "title": "半導体入門",
    "section": "4 物理",
    "text": "4 物理\n\n4.1 単体の半導体"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#文献レビュー",
    "href": "posts/2024/AI/Semiconductor.html#文献レビュー",
    "title": "半導体入門",
    "section": "5 文献レビュー",
    "text": "5 文献レビュー\n(Rudan et al., 2023) は辞典として使える．(Van Rossum, 2005) は凝縮系物理学のハンドブック内でのエントリ．\n\n5.1 産業\n(Miller, 2022) は Tufts 大学の Chris Miller による書籍．\n和書としては，(菊池正典, 2023) は NEC 社員による半導体業界の解説書．\n\n\n5.2 教科書\n(Sze and Lee, 2012) は浮遊ゲート MOSFET の発明者でもある Simon Min Sze（施敏）の著作で，半導体分野で最も多く引用される教科書とされている．第２版なら 和訳 もある．\n(May and Spanos, 2006) は California 大学 Davis 校の現学長 Gary May と California 大学 Berkeley 校の Costas Spanos による書籍．\n(May and Sze, 2003) もある．さらに発展的なものは (Pierret, 2003)．\n\n\n5.3 理論\n(Kittel, 2018) が固体物理学の標準的な入門書とされている．(Böer and Pohl, 2018) が特に半導体物理学の専門書になる．(Huebener, 2019) は前２つの橋渡しの役割をするが，重点は超伝導にある．\n\n\n5.4 その他\n(Richard, 2023) は SignalFire という VC に所属する著者による書．\n(Lau, 2021) は AMS Pacific Technology の 技術アドバイザー による書籍．\n(Evstigneev, 2022) はカナダ Memorial 大学 の 凝縮系物理学者 が書いた．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor.html#footnotes",
    "href": "posts/2024/AI/Semiconductor.html#footnotes",
    "title": "半導体入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Böer and Pohl, 2018, p. 4), (Huebener, 2019, p. 73) Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う (Madelung, 1978, p. 17)．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である (Madelung, 1978, p. 211)．↩︎\n(Huebener, 2019, p. 75)．↩︎\n(Huebener, 2019, p. 73) 特に伝導体と半導体の境界部分で強く見られた．↩︎\n現代では，このような接合を金属-半導体接合 (metal-semiconductor contact) または Schottky 接合 といい，Ohmic 接合と対比する．↩︎\n(Patterson and Hennessy, 2014, p. 27) も参照．↩︎\n一方で GaAs の形成は Bridgman 法 (Bridgman, 1925) による．最も，この化合物が半導体であると発見されたのは (Welker, 1952) になってようやくのことである．(Sze and Lee, 2012, p. 6) も参照．↩︎\n(Sze and Lee, 2012, p. 6) など．↩︎\n今日の IC ではトランジスタ層は１層のみで，絶縁層で仕切ることで２〜８層の金属導体の配線層をその上に設ける． (Patterson and Hennessy, 2014, p. 26)．↩︎\n(Sze and Lee, 2012, p. 4) など．↩︎\n(Patterson and Hennessy, 2014, p. 41)．↩︎\n(Patterson and Hennessy, 2014, p. 379)．↩︎\n(Hoff et al., 1996) が開発者自ら歴史を振り返っている．当時は Intel も出来て３年しか経っていない新興企業であった．↩︎\n(Sze and Lee, 2012, p. 8) など．↩︎\n(Patterson and Hennessy, 2014, p. 379)．↩︎\n(Sze and Lee, 2012, p. 8)，(Patterson and Hennessy, 2014, p. 379) 参照．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html",
    "href": "posts/2024/AI/Semiconductor_slides.html",
    "title": "半導体入門",
    "section": "",
    "text": "半導体とは？なぜ重要なのか？\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n21世紀に入った時点で，大きく分けて 18 の半導体デバイスが存在する (Ng, 2002)．\n細かいものを含めると 140．\n全てトランジスタの組み合わせからなる．\n\n\n\n\n\n世界の半導体出荷額は直近 10 年で２倍になっている\n\n\n\n\n\n\n\n\n\n物質としての半導体\n\n\n\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．1\n\n\n\n\n\nこのような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．2 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．\n\n\n\n(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，3 その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える (Sze and Lee, 2012, p. 1)．4\nBraun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．\n\n\n\n半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した ウエハ (wafer) 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．\n\n\n\nチップの断面構造\n\n\n一つのウエハから多数のチップが作成され，その各単位を ダイ (die) ともいう．\nしかし，普段我々が目にする IC チップ は パッケージ されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．\n32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．5"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体",
    "title": "半導体入門",
    "section": "",
    "text": "21世紀に入った時点で，大きく分けて 18 の半導体デバイスが存在する (Ng, 2002)．\n細かいものを含めると 140．\n全てトランジスタの組み合わせからなる．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体産業",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体産業",
    "title": "半導体入門",
    "section": "",
    "text": "世界の半導体出荷額は直近 10 年で２倍になっている"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体とは",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体とは",
    "title": "半導体入門",
    "section": "",
    "text": "物質としての半導体\n\n\n\n価電子帯と伝導帯の間の 禁制帯 (band gap) が十分に小さくて遷移を制御することが可能で，基底状態では価電子帯は完全に埋まっているものの伝導帯は空いているような物質を 半導体 という．1"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体の物理",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体の物理",
    "title": "半導体入門",
    "section": "",
    "text": "このような半導体では，熱や光，また外部電磁場などにより価電子が励起され，伝導帯に移る．この電子に加えて，価電子帯に生じた正孔も導電性に寄与する．2 この 正孔 (hole) を擬似的に粒子と扱い，正孔の波動方程式を議論したのが (Heisenberg, 1931) である．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#半導体発見の歴史",
    "href": "posts/2024/AI/Semiconductor_slides.html#半導体発見の歴史",
    "title": "半導体入門",
    "section": "",
    "text": "(Faraday, 1833) は，通常金属では温度の上昇と共に電気抵抗が増すが，硫化銀 Ag2S を初めとしたいくつかの物質では逆に電気抵抗が減少することを報告している．\n(Braun, 1874) は 方鉛鉱 PbS に電流を流そうとしても，単一方向にしか電流が流れない整流作用を示すことを発見し，3 その後20世紀に入るとラジオに応用された．これが人類が初めて出会った半導体デバイスだったと言える (Sze and Lee, 2012, p. 1)．4\nBraun はその後ブラウン管を発明し，こちらの業績により 1909 年にノーベル物理学賞を受賞する．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#基本用語のまとめ",
    "href": "posts/2024/AI/Semiconductor_slides.html#基本用語のまとめ",
    "title": "半導体入門",
    "section": "",
    "text": "半導体素子には，トランジスタやダイオードなどがある．これらを配線によって相互接続したものが IC チップである．IC チップはシリコンのインゴットを円板状に切り出した ウエハ (wafer) 上に構築する．IC チップは平面的な印象を受けるが，実際は層に分けて構成されている，高度に立体的な構造物である．\n\n\n\nチップの断面構造\n\n\n一つのウエハから多数のチップが作成され，その各単位を ダイ (die) ともいう．\nしかし，普段我々が目にする IC チップ は パッケージ されたもの，で．IC チップそのもの（ダイそのもの）を目にすることはない．\n32nm などというときは，ダイの大きさではなく，ダイ上の最小のトランジスタのサイズをいう．5"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#設計",
    "href": "posts/2024/AI/Semiconductor_slides.html#設計",
    "title": "半導体入門",
    "section": "2.1 設計",
    "text": "2.1 設計\nまず回路を設計し，原版（マスター）を作る．これを フォトマスク (photomask) または レティクル (reticle) という．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィ",
    "href": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィ",
    "title": "半導体入門",
    "section": "2.2 フォトリソグラフィ",
    "text": "2.2 フォトリソグラフィ\nこれをウエハに転写するには，フォトリソグラフィ (Photolithography) を用いる．シリコンウエハの形成は，Czochralski 法 (Czochralski, 1918) による．6"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィの歴史",
    "href": "posts/2024/AI/Semiconductor_slides.html#フォトリソグラフィの歴史",
    "title": "半導体入門",
    "section": "2.3 フォトリソグラフィの歴史",
    "text": "2.3 フォトリソグラフィの歴史\nリソグラフィ自体は 1798 年からあり，Niépce が 歴青 が感光剤の役割を果たすことを発見し，カメラの発明と同時に発見された．\n\n\n\ncamera obscura"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#フォトリトグラフィの産業応用",
    "href": "posts/2024/AI/Semiconductor_slides.html#フォトリトグラフィの産業応用",
    "title": "半導体入門",
    "section": "2.4 フォトリトグラフィの産業応用",
    "text": "2.4 フォトリトグラフィの産業応用\nエッチングに耐性のある感光剤を使えば，半導体デバイスの製造に応用できると気づいたのは (Andrus, 1957) である．この技術は半導体製造コストの 35 %を占めており，半導体市場の急成長はほとんどこの技術の進歩と両輪であると言う者も多い．7\nシリコン表面に酸化被膜を形成することで不純物原子の移動を阻止できることは (Frosch and Derick, 1957) が発見した．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#最初の-ic",
    "href": "posts/2024/AI/Semiconductor_slides.html#最初の-ic",
    "title": "半導体入門",
    "section": "2.5 最初の IC",
    "text": "2.5 最初の IC\n以上の技術を用いて，最初の IC は Texas Instruments の Jack Kilby によって作られた．\n\n\n\nMiniaturized electronic circuits. U.S. Patent 3,138,743A (Kilby, 1959)\n\n\nJack Kilby はその後 2000 年にノーベル物理学賞を受賞する．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#最初の-ic-1",
    "href": "posts/2024/AI/Semiconductor_slides.html#最初の-ic-1",
    "title": "半導体入門",
    "section": "2.6 最初の IC",
    "text": "2.6 最初の IC\n\n\n\n設計図\n\n\n\n双極トランジスタ が１つ\n抵抗器３つ\nコンデンサ１つ\n全てゲルマニウムからなる．\n回路は導線"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#次の-ic",
    "href": "posts/2024/AI/Semiconductor_slides.html#次の-ic",
    "title": "半導体入門",
    "section": "2.7 次の IC",
    "text": "2.7 次の IC\nFairchild Semiconductor の Robert Noyce によって作られた．\n\n\n\nSemiconductor Device-and-Lead Structure. U.S. Patent 2,981,877A (Noyce, 1959)\n\n\nRobert Noyce はその後 Gordon Moore と Intel も創業し，the Mayor of Silicon Valley と呼ばれる．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#次の-ic-1",
    "href": "posts/2024/AI/Semiconductor_slides.html#次の-ic-1",
    "title": "半導体入門",
    "section": "2.8 次の IC",
    "text": "2.8 次の IC\n\n\n\n設計図\n\n\n\nリトグラフによりアルミニウムの配線を形成\nPlanar Process (Hoerni, 1960) （現在主流の製法）で製造\n１つのシリコン基盤上に作った\n→ monolithic で大量生産が可能\n\n1961 年から 1965 年は，NASA の Appolo 計画からの特需もあり，半導体産業は大きく成長した．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#設計-design",
    "href": "posts/2024/AI/Semiconductor_slides.html#設計-design",
    "title": "半導体入門",
    "section": "3.1 設計 (design)",
    "text": "3.1 設計 (design)\n\n\n\n世界初の１チップ CPU の設計図 from (Hoff et al., 1996, p. 11)"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor_slides.html#footnotes",
    "href": "posts/2024/AI/Semiconductor_slides.html#footnotes",
    "title": "半導体入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Böer and Pohl, 2018, p. 4), (Huebener, 2019, p. 73) Chapter 6．金属が電気を通すのは，伝導帯が部分的に電子によって占められているためである．半導体は，（例えば温度を上げることなどにより）価電子帯の電子を簡単に伝導帯に移すことができるため，思い通りに金属のような振る舞いも，絶縁体のような振る舞いも引き出すことができる．しかし，半導体の自由電子は，金属に比べて極めて少なく．Boltzmann 統計に従い，金属の自由電子は Fermi 統計に従う (Madelung, 1978, p. 17)．一方で，金属の導電性は電子の密度とは関係がなく，金属内の電子密度は温度により一定である (Madelung, 1978, p. 211)．↩︎\n(Huebener, 2019, p. 75)．↩︎\n(Huebener, 2019, p. 73) 特に伝導体と半導体の境界部分で強く見られた．↩︎\n現代では，このような接合を金属-半導体接合 (metal-semiconductor contact) または Schottky 接合 といい，Ohmic 接合と対比する．↩︎\n(Patterson and Hennessy, 2014, p. 27) も参照．↩︎\n一方で GaAs の形成は Bridgman 法 (Bridgman, 1925) による．最も，この化合物が半導体であると発見されたのは (Welker, 1952) になってようやくのことである．(Sze and Lee, 2012, p. 6) も参照．↩︎\n(Sze and Lee, 2012, p. 6) など．↩︎"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor2.html",
    "href": "posts/2024/AI/Semiconductor2.html",
    "title": "半導体の微細化技術",
    "section": "",
    "text": "(内山貴之, 2015) より．"
  },
  {
    "objectID": "posts/2024/AI/Semiconductor2.html#euv-リトグラフィ",
    "href": "posts/2024/AI/Semiconductor2.html#euv-リトグラフィ",
    "title": "半導体の微細化技術",
    "section": "1 EUV リトグラフィ",
    "text": "1 EUV リトグラフィ\n極端紫外線リソグラフィ (Extreme Ultraviolet Lithography) は波長 13.5 nm で露光する技術である．\nオランダの ASML 社が唯一 EUV 露光装置を製造している．\n\nChina asks Netherlands, with world-leading chipmaking equipment, to not decouple\nここまで短い波長では光を透過するレンズはなく，反射鏡で光学系を構成する必要がある．13.5 nm という波長はミラーの反射率の極大点の１つをとって決定された． \\(0.68^{12}=0.98\\) というように重ねて用いる．\n\n\n\n(内山貴之, 2015)　より．"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html",
    "href": "posts/2024/Review/Duane+1987.html",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html#概要と背景",
    "href": "posts/2024/Review/Duane+1987.html#概要と背景",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "1 概要と背景",
    "text": "1 概要と背景\n\n1.1 HMC とは\nhybrid Monte Carlo とは，MD と MC（＝Metropolis 法） の融合を指す．\nすなわち，粒子を動かして，これを棄却手続きによって正準集団を作る MCMC 法を指すが，粒子の動かし方＝提案核を運動論から構成するというのである．1\nそもそも著者は (Simon Duane, 1985) において，場の量子論をシミュレーションするための “hybrid algorithm” を提案していた．これは第三の量子化と呼ばれる (Parisi and Wu, 1981) の確率過程量子化に基づく Langevin algorithm と 小正準法（量子系に対する MD 法）のいいとこ取りをする確率的アルゴリズムである．\nここからさらに Monte Carlo を導入し，「MD を提案分布にする」という発想の転換がある．正確には，任意の Hamilton 力学系を提案分布にとっても，Metropolis 法が使えるということを示唆したのである．この Hamilton 力学系を正確に取ると，これは hybrid algorithm になる（古典系に対しては MD に一致するだろう）が，必ずしも正確に取る必要はないのである．\n\n\n1.2 概要\n(Simon Duane et al., 1987) は格子上の場の理論における数値シミュレーション法として提案している．格子ゲージ理論は量子色力学で扱われる模型である．\n\n大きなステップサイズを用いても離散化誤差がない\nフェルミオン自由度を含む量子色力学系のシミュレーションに有効\n\nである点が abstract で触れられている．\n\n\n1.3 はじめに\nフェルミオン自由度がある系では，“Grassmann nature of the fermions” を除去するためにまず積分をして有効作用のみを取り出し，残りのボゾンのみを考えるが，このときに非常に遠距離な（非局所的な）相互作用になってしまう．\n従来法には次の２つがある：\n\nexact / entire Monte Carlo：ボゾンの局所的なアップデートは系の全体の状態をシミュレーションしないとわからないから，nested Monte Carlo ともいうべきサブルーチンを回す必要がある．pseudofermion を導入して，有効作用の変化を効率的に計算し，これを用いて元々のボゾン場をアップデートする．要は棄却のステップが大変に高価ということだろうか？\n運動方程式の計算：MD に対応する方法である．小さいステップサイズで系全体を運動方程式に沿ってアップデートしていくことで，非局所的な有効作用というものは考えなくて済む．しかし，運動方程式の決定論的計算に伴う truncation error が導入される．\n\n後者の計算効率性と，前者の正確性を両取りすることを考える．"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html#本論",
    "href": "posts/2024/Review/Duane+1987.html#本論",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "2 本論",
    "text": "2 本論\nHMC は結局完全に Metropolis 法 (Metropolis et al., 1953) の枠内であり，詳細釣り合い条件を満たしにいくことを考える．ただし，この枠組みの中で最も優秀な方法を考える，というのである．\n採択関数は \\[\n\\alpha(x,y)=1\\land\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}\n\\] で与えられるから，2 \\(q(x,y)\\) の計算が速いだけでなく，\\(q(y,x)\\) も得られやすい理論的に都合の良い提案核 \\(Q\\) を探すことを考える．\n\n2.1 先行研究\nこの考えは molecular dynamics と Langevin algorithm をハイブリッドするアルゴリズム (Simon Duane, 1985), (S. Duane and Kogut, 1986) に基づく．\n場の理論において，確率過程量子化 (stochastic quantization) (Parisi and Wu, 1981) に基づく Langevin 方程式の方法と，小正準集団の方法（MD に近い，QCD の熱力学のシミュレーションにも使われる）という２つの方法が，特に力学的フェルミオンを含んだ系の数値シミュレーションにおいて魅力的な代替理論になっている．\nこの２つのシミュレーションのいいとこ取りをする hybrid algorithm が (Simon Duane, 1985) で提案されており，(S. Duane and Kogut, 1986) で理論的な解析が進められた．これは，確率 \\(p\\Delta\\) によって，Langevin 法を用いるか，小正準法を決めるというアルゴリズムである．これは系をある熱浴に接続するという物理的な解釈を持つ．加えて，このアルゴリズムの軌跡は，ある古典的な運動方程式に従った奇跡ともみなせる．\n\n\n2.2 アイデア\nParisi-Wu の確率過程量子化では，仮想的な時間 \\(t\\) を導入して，場の量 \\(\\phi_i(x)\\) が Langevin 方程式に従って発展するとする．こうして定まる確率過程が \\(t\\to\\infty\\) の極限で場の量子論を与えるというのである．\nこれに倣い，仮想的な時間パラメータ \\(\\tau\\) と Hamilton 力学系を導入する．ここで補助変数として，共役運動量 \\(\\pi(t)\\) が導入される．\nもし Hamiltonian \\(H\\) を正確に対象の系と同様に取れば，採択率は \\(1\\) になり，これが hybrid algorithm (Simon Duane, 1985) に他ならない．しかし，ずらしても良いのである．\n\n\n2.3 検証\n詳細釣り合い条件を満たすことを示している．詳細釣り合い条件が満たされる主な理由は Hamilton 力学系が可逆であることによる．"
  },
  {
    "objectID": "posts/2024/Review/Duane+1987.html#footnotes",
    "href": "posts/2024/Review/Duane+1987.html#footnotes",
    "title": "Duane+ (1987) Hybrid Monte Carlo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“a form of the Metropolis algorithm in which candidate states are found by means of dynamical simulation.” (Neal, 1996, p. 55)↩︎\n(鎌谷研吾, 2020) より．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Fearnhead+2017.html",
    "href": "posts/2024/Review/Fearnhead+2017.html",
    "title": "Fearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\nReferences\n\nFearnhead, P., Latuszynski, K., Roberts, G. O., and Sermaidis, G. (2017). Continious-time importance sampling: Monte carlo methods which avoid time-discretisation error."
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html",
    "href": "posts/2024/Review/Peters-deWith2012.html",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#概要",
    "href": "posts/2024/Review/Peters-deWith2012.html#概要",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "1 概要",
    "text": "1 概要\n統計用語でいう Bouncy Particle Sampler を，Metropolis-Hastings 法の連続時間極限として初めて提案した論文であるが，これが (Bouchard-Côté et al., 2018) に発見されるには６年の時間を要した．\nこれを正準分布からのサンプリング法として導入した (Peters and de With, 2012) では，この手法を event-driven rejection-free Monte Carlo 法と呼ぶ．\n連続なポテンシャル \\(E\\) を用いることが可能なことが特徴．event-driven アプローチだけでなく，event-chain アプローチ (Bernard et al., 2009) での実装も扱った．Lennard-Jones ポテンシャルを持った流体のシミュレーションで検証した．\n\n\n\n\n\n\n注\n\n\n\n\n\n\nここでは本質的には連続時間極限をとっているが，直接議論されているのはポテンシャルの細分化の連続極限である．\n\n\\(U(x(t))\\) があったとき，\\(\\Delta U\\to0\\) の極限を考えているために，実質的に \\(\\Delta t\\to0\\) を議論している．\n\n多粒子系のなすカノニカル分布からのサンプリングを考えているため，分布が積の形に分解できることを前提としている．\nSEC (Bernard et al., 2009) からの改良と読める．\n\n\n\n\n\n\n\n\n\n\n関連文献\n\n\n\n\n\nMetropolis 法については\n\n    \n        \n            \n            \n                Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n                Metropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の (Hastings, 1970) による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる．\n            \n        \n    \n\n\n    \n        \n            \n            \n                分子動力学法\n                物理に寄り添った Monte Carlo 法"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#導入",
    "href": "posts/2024/Review/Peters-deWith2012.html#導入",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "2 導入",
    "text": "2 導入\n\n2.1 time-driven から event-driven へ\n平衡統計力学に従って粒子系をシミュレーションする際には，MD (molecular dynamics) と MC (Monte Carlo methods) の２つのアプローチがあり得る．\nしかし現状，いずれも (Metropolis et al., 1953) の採択-棄却の枠組みで実行されるのが一般的である．\n棄却を取り入れた MD 法を，剛体球の系に適用すると，\n\n高濃度領域では，タイムステップは小さく取らないと，すぐに他の粒子と重なってしまいやすく，棄却率が高くなる．かといってタイムステップを小さくすると計算量が増大する．\n低濃度領域では，タイムスケールは分子の衝突過程に依存するので，シミュレーション時間のほとんどは無駄に使うことになる．\n\nという難点がある．\nそこで元来 MD 法では，event-driven と呼ばれるアプローチが早期に提案された (Alder and Wainwright, 1959)．\n実際，このアプローチは，(Metropolis et al., 1953) では叶わなかった，剛体円板系の液相転移のシミュレーションに成功している (Alder and Wainwright, 1962)．1\n従って，Monte Carlo 法の方にも，event-driven なアプローチを取り入れると，大きな効率改善が望めるだろう．\n\n\n2.2 ED-MD から改良された点\nかといって event-driven MD にも難点がある．まず (Alder and Wainwright, 1962) がシミュレーションに成功したのは剛体円板系であり，ポテンシャルは単関数である．\n悪いことに，ED-MD はポテンシャルが単関数である場合にまでしか一般化できない．単関数でないと，いつイベントが起こるかが予測できないのである．\nしかし本論文で提案する手法は，イベント発生時刻を Poisson 過程でシミュレーションすることにしたので，一般のポテンシャルに適用できる（分子動力学の稿 も参照）．\n\n\n2.3 衝突のモデリングには自由度が残る\n提案手法では，Metropolis 的な棄却を実施するのではなく，衝突によって向きを変えるというイベントが起こる．\nこの衝突のモデリングは，MD 法のように，Newton 力学から衝突の様子をシミュレーションしても良い．\nしかし，event-chain Monte Carlo 法 (Bernard et al., 2009), (Bernard and Krauth, 2011) のように，衝突をしたら，衝突を受けた粒子が運動を引き継ぐ，というルールにしても良い．（Monte Carlo 法だから，背後の物理過程に忠実である必要はない．）\n\n\n2.4 他のアルゴリズムとの比較\nアルゴリズム的には kinetic / dynamic MC (Fichthorn and Weinberg, 1991) \\(n\\)-fold way MC simulation (Bortz et al., 1975) に似て Poisson 過程のシミュレーションに帰着する．\nだが上述の手法は，Poisson イベント間は何も起こらないとしている．\n一方で本手法は，Poisson イベントをシミュレーションしたのちに，そのイベントの間の粒子系の動きは線形に補間される．"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#本論",
    "href": "posts/2024/Review/Peters-deWith2012.html#本論",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "3 本論",
    "text": "3 本論\nMetropolis scheme のように提案と棄却によって詳細釣り合い条件を満たすのではなく，棄却するところを衝突にすることによって詳細釣り合い条件を達成することを考える．\nこの状態から連続極限を考えていく．\nまず，\\(U\\) が階段関数であるとし，\\(q(x,y)\\) が提案されたとすると，登った段数の分だけ独立な採択-棄却を実行することで，1回の採択-棄却の手続きを代用できる（ Tartero and Krauth (2023) の consensus 方式）．すなわち，最終的な採択確率は \\[\n\\begin{align*}\n    P_{\\text{no-coll}}(x,y)&=\\prod_i1\\land e^{-\\beta\\Delta U_i}\\\\\n    &=\\exp\\left(-\\beta\\sum_{i}(\\Delta U_i)_+\\right)\n\\end{align*}\n\\]\nこうして，連続なポテンシャルを単関数近似して，同様の手続きを実行するアイデアが考えられるが，ここではより洗練された手法を考える．\nポテンシャルのステップ \\(\\Delta U\\) が \\(0\\) に近づくという連続極限を取ることで，「どの時点まで衝突せずに直進できるか」を計算することに帰着する．例えば時刻 \\([s,s_0]\\) の間に衝突しない確率は \\[\nP_{\\text{no-coll}}(s)=\\exp\\left(-\\beta\\int_s^{s_0}(D_t U(x(t)))_+dt\\right)\n\\] となる．\n実際に，衝突する時刻を求めるには，採択-棄却手続きのための一様変数 \\(u\\sim\\mathrm{U}([0,1])\\) を取り，これに対して \\(u=P_{\\text{no-coll}}(s)\\) を満たす時刻 \\(s\\) を計算すれば良いのである： \\[\n-k_BT\\log u=\\int^s_{s_0}(D_tU(x(t)))_+dt.\n\\]\n衝突のモデルが，例えば新しい速度 \\(v\\) を「対称」に決めるようなものであったならば，この手法は対称な手法で詳細釣り合い条件を満たす．ここら辺も evnet-chain Monte Carlo (Bernard et al., 2009) に似ている．"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#多粒子系での実装",
    "href": "posts/2024/Review/Peters-deWith2012.html#多粒子系での実装",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "4 多粒子系での実装",
    "text": "4 多粒子系での実装\nポテンシャル \\(U=\\sum_{\\alpha\\in\\Lambda}U_\\alpha\\) を持つ多粒子系を考え，衝突規則とその実装の例を提示している．\nこのとき，粒子の速度 \\(v\\) を何かしらの分布に従って refresh するとしているが，これは ECMC (Bernard et al., 2009) の名残だろう．\n実際，Appendix にて，動力学の不変分布が Boltzmann-Gibbs 分布になっていることが示されており，refresh は必要ないことが注記されている．\nまた，ここで例示されているアルゴリズムは対称な MCMC を定めるとしているのも懸念点の１つである．"
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#議論",
    "href": "posts/2024/Review/Peters-deWith2012.html#議論",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "5 議論",
    "text": "5 議論\n３粒子以上が関与するポテンシャルに関しても自然に拡張でき，これが SEC (Bernard et al., 2009) にはない美点である．\nMD よりも効率的であることは理論的には示していないが，実験的にはそう予想される．\n\nIt remains to be seen if the application of the method is suited for niche applications only or if it can rival with MD and Metropolis-MC for general purpose molecular simulations."
  },
  {
    "objectID": "posts/2024/Review/Peters-deWith2012.html#footnotes",
    "href": "posts/2024/Review/Peters-deWith2012.html#footnotes",
    "title": "Peters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nそしてこの発見は，(Kosterlitz and Thouless, 1973) による液相転移の「２段階モデル」の理論が生まれるきっかけとなった． (Faulkner and Livingstone, 2024, p. 15) 4.3 節も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html",
    "href": "posts/2024/Review/Metropolis+1953.html",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#概要",
    "href": "posts/2024/Review/Metropolis+1953.html#概要",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "1 概要",
    "text": "1 概要\n相互作用する分子系からなる物質の状態方程式などの性質を調べるために使える汎用手法（ fast computing machine にぴったり！）を提案する．\nこの手法は配置空間上の，提案分布を正確にした「修正版 Monte Carlo 積分法」だと捉えられる，としている．\n現代的には，１度に一つの粒子ずつを動かすことで，棄却率の低下を防いだ Metropolis-within-Gibbs サンプラーと理解できる．1\nMANIAC を用いて，２次元剛体円板の液相転移のシミュレーション結果を付した．その結果を，自由体積状態方程式と，４項ビリアル係数展開と比較した．2\n一方，(Wood and Parker, 1957) ではこの手法を３次元の Lennard-Jones ポテンシャルに適用しており，相転移の痕跡を掴んでいる．\nこの手法を Ising 模型に応用した (Glauber, 1963) は (random scan) Gibbs サンプラーの先駆けとみなされている．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#背景",
    "href": "posts/2024/Review/Metropolis+1953.html#背景",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "2 背景",
    "text": "2 背景\n\n2.1 サンプリング法としては見られていない\nむしろ cell method などの計算手法との類似で捉えており，(Hastings, 1970) に取り上げられるまで全く別個の，広く抽象的に応用されるポテンシャルを持つサンプリング法であるという抽象的な観点とは離れた泥臭い探索を感じる．\nまだ Markov 連鎖という単語も使われていない．\n\n\n2.2 分子シミュレーションの延長としての Monte Carlo 法\n話の展開は，分子系のシミュレーションの問題を，Gibbs 分布に関する積分計算の問題に帰着し，Monte Carlo 法を用いるところまで還元するが，そこで不適当な提案分布による重点サンプリングを実行するよりかは，系の Gibbs 分布を直接シミュレーションすれば良い，というものである．\nよってここでいう修正 Monte Carlo 法とは，Monte Carlo 法（ここでは重点サンプリングと同義，（一様）乱数を用いた数値計算，くらいの意味）による乱数生成過程を，少しだけ MD 法の考え方を取り入れて，より Gibbs 分布に則した乱数生成をする，くらいの提案である．\n後世では次のようにも説明されている始末である：\n\nThe Monte Carlo method addresses the sampling problem more abstractly than molecular dynamics, as it samples (obtains samples \\(x\\) from) the distribution \\(\\pi_{24}(x)\\) without simulating a physical process. (Tartero and Krauth, 2023)\n\nもはや，愚直な Monte Carlo 法が MD 法で，運動方程式を解かない MD 法が Monte Carlo 法，という具合である．事実，気体の状態方程式は，気体分子の運動の力学には無関係に成り立つので，必ずしも正しい力学に従ったシミュレーションが必要というわけではないのである．3\nなお，Monte Carlo 法の提案者として Joseph Edward Mayer と Stanisław Ulam の名前を挙げている．\nここで Berni Alder の名前が上がっており，やはり MCMC の開発は MD と極めて深い関係にあることが伺える．\n\nThis method has been proposed independently by J. E. Mayer and by S. Ulam. Mayer suggested the method as a tool to deal with the problem of the liquid state, while Ulam proposed it as a procedure of general usefulness. B. Alder, J. Kirkwood, S. Frankel, and V. Lewinson discussed an application very similar to ours.\n\n\n\n2.3 Monte Carlo 法の起源について\nJohn von Neumann は Edward Teller と同郷であり，戦時中の ENIAC の開発にも関わっていたことから，偉い人たちを説得して最初の ENIAC のテストに熱核融合反応の計算問題を用いることにした．\n計算可能なモデルの構築をしたのが Metropolis である．これが完成し，実際にテストが行われたのは戦後の 1946 年であったが．\nそのお披露目会に居合わせた Stanislaw Ulam が，統計的サンプリング技術を電子計算機で復活させることを提案し，Johnny がすぐさまその重要性を理解した．これがモンテカルロ法の始まりとなった，という (Metropolis, 1987)．\nMetropolis は Ulam が着想を得た理由として，数学的な背景を持っていたために，統計的サンプリング技術が計算の難しさのために歴史に埋没したことを知っており，ENIAC のポテンシャルを見てこれと関連づけることに成功したのではないかと示唆している．\nそして Johnny の熱の入り用が周りも刺激した．1947 年には統計的サンプリングの中でも特に中性子の拡散問題を取り上げて当時の Los Alamos の理論部リーダーであった Robert Richtmyer に手紙を送った (Eckhardt, 1987)．こうして周りを巻き込んで大ごとになっていった．Monte Carlo 法という命名も 1947 年だったという\n\nIt was at that time that I suggested an obvious name for the statistical method—a suggestion not unrelated to the fact that Stan had an uncle who would borrow money from relatives because he “just had to go to Monte Carlo.”\n\n\nOn a less grand scale these events brought about a renascence of a mathematical technique known to the old guard as statistical sampling; in its new surroundings and owing to its nature, there was no denying its new name of the Monte Carlo method. (Metropolis, 1987)\n\n特に戦時中の関心もあり，核分裂時の 中性子の拡散 のシミュレーションが問題であった．Monte Carlo 法と呼んでいるが本質的に MD 法チックであり，ここの中性子の散乱・吸収・分裂の系譜をシミュレートすることで全体の統計的性質がわかる，というだけの話であった．\nその後 1952 年には後続機の MANIAC が開発され，nucleaer cascade と状態方程式も射程に入った．\nこの状態方程式を取り扱う際に (Metropolis et al., 1953) がさらに効率的な「モンテカルロ法」を発明したのである．これは Gibbs 分布を直接シミュレーションできるというブレイクスルーであり，「一般の確率分布からサンプリングできる」という今の理解とは大きく異なる文脈の中で発見されたと言うべきである．\n\n\n2.4 Monte Carlo 法とはなんだろうか\n思うに，「ランダムな方法を使って計算する」というのは外道に思えるかもしれない．\nだが，実はランダムな系の \\(\\mathcal{P}(E)\\) 上のダイナミクスの決定論的な計算になっているのかもしれない．\nそう思わせるだけの透徹性が測度論にはある．\nただ，(Metropolis, 1987) は Monte Carlo 法を 実験数学 (experimental mathematics) と呼んでおり，極めて物理学的な見方で評している：\n\nAt long last, mathematics achieved a certain parity–the twofold aspect of experiment and theory–that all other sciences enjoy.\n\n\n\n2.5 他のコメント\n\nNote that (Metropolis et al., 1953) move one particle at a time, rather than moving all of them together, which makes the initial algorithm appear a primitive kind of Gibbs sampler! (Robert and Casella, 2011)\n\nこの点は，Metropolis 法と MD 法の違いとも通じる．\nMetropolis の方法は１つの粒子を順番に動かすが，MD 法では全ての粒子を同時に動かす．\n加えて，Metropolis 法は平衡状態のシミュレーションのみを想定しているという点で，本質的にはサンプリング法であるが，MD 法は非平衡系の時間発展も同等に扱うシミュレーション法である．4\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法である．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#本論",
    "href": "posts/2024/Review/Metropolis+1953.html#本論",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "3 本論",
    "text": "3 本論\n\n3.1 設定\n古典統計を仮定し，２体間相互作用のみを考え，ポテンシャルは球対称であるとする（流体力学では通常の仮定である）．だが，温度や密度には全く仮定を置かない．5\n実際の計算のために，粒子数 \\(N\\) は several hundred に取る．そして正方形の中にいれ，境界条件を最小化するために同様の系が２次元に無限に連なっているとする．２つの粒子 \\(A\\) の他の粒子 \\(B\\) との最短距離を \\(d_{A,B}\\) とし，これのみが粒子 \\(A\\) にかかる主な力になるとする．\n仮に \\(N=1\\) だとしたら，これは cell method と呼ばれるモデルでもある．こうして粒子を増やすことで，単一相のシステムに対するより良いモデルになるだろうが，二相以上のシステムには限界がある．\n以上の仮定から，系のエネルギーが次のように与えられる： \\[\nE=\\frac{1}{2}\\sum_{i\\ne j\\in[N]}V(d_{ij}).\n\\]\nこの系の平衡状態の性質を計算するには，Gibbs の正準分布を利用し，計算したい物理量 \\(F\\) に対して \\[\n\\overline{F}=\\frac{\\int Fe^{-\\frac{E}{kT}}d^{2N}pd^{2N}q}{\\int e^{-\\frac{E}{kT}}d^{2N}pd^{2N}q}\n\\] を計算すれば良い．ただし，\\(d^{2n}pd^{2n}q\\) は \\(4N\\) 次元相空間上の体積要素である．\n加えて，ここではポテンシャル \\(V\\) は位置のみの引数としているから，\\(2N\\) 次元上でのみ計算すれば良い．\nこのような数百次元上での積分を数値的方法で実行するのは明らかに実行可能でないから，Monte Carlo 法に頼らざるを得ない．と言っても，決定論的な点で値を計算する代わりに，ランダムに点をうつ，というだけの違いではある．\n最も簡単な実装としては，ランダムに \\(N\\) 粒子を配置してエネルギーを計算し（重点荷重），これにウェイトをつけて足していくということが考えられる（重点サンプリングだ！）．しかし，高エネルギーの配置もたくさん生成してしまうから，これによる効率の低減が避けられない（提案分布が悪いのだ！）．\nそこで我々は modified Monte Carlo method を考える．そもそも確率 \\(e^{-\\frac{E}{kT}}\\) からサンプルを生成し，荷重を一様にすることを目指す．\n\n\n3.2 アルゴリズムの記述\nこれは次のようにする．まず適当に初期分布を決める（格子点上に \\(N\\) 粒子を配置するなど）．そしてこれをアップデートしていく： \\[\nX\\mapsto X+\\alpha\\xi_1\n\\] \\[\nY\\mapsto Y+\\alpha\\xi_2\n\\] \\(\\alpha&gt;0\\) は一度にどれくらい動かすかを調節するパラメータであり，\\(\\xi_1,\\xi_2\\in(0,1)\\) は一様乱数とする．6\nすなわち，\\((X,Y)\\) を中心とした一辺 \\(2\\alpha\\) の正方形の中で，新たな位置をランダムに決めるのである．7\nこの動きによるエネルギーの変化量 \\(\\Delta E\\) を計算し，\\(\\Delta E&lt;0\\) ならばこれを実行するが，\\(\\Delta E&gt;0\\) ならば確率 \\(e^{-\\frac{\\Delta E}{kT}}\\) によって採択する．\n仮に棄却されたとしても，そのポジションから新たな Monte Carlo 標本を取り，最終的に \\[\n\\overline{F}=\\frac{1}{M}\\sum_{j=1}^MF_j\n\\] を推定量とする．\n\n\n3.3 アルゴリズムの有効性の検証\nまずこの系はエルゴードであると主張しているが，その論証は「任意の粒子が任意の位置に行くポテンシャルがあるため，この手法はエルゴード的である」で終わっている．エルゴードという単語を「任意の状態からもう一つの任意の状態に遷移可能である」という意味で使っている．これは現代的には既約性という．8\n続いて，この系をたくさんコピーしてアンサンブルを考えたとき，状態 \\(r\\) にいるアンサンブルの数 \\(\\nu_r\\) は \\[\n\\nu_r\\,\\propto\\,e^{-\\frac{E_r}{kT}}\n\\] を満たすことを示したい．その論証は，上の比率から崩れていたら，平衡に至る方向へ移動が起こるということを具体的に議論している．\n以上の２点から，提案されたアルゴリズムは正準分布に収束することの根拠としている．\nこのアンサンブルによる考え方は極めて直感的に訴える．実際，この語彙を用いて，棄却された場合は元々の状態を Monte Carlo サンプルとしてダブルカウントすべきであることを説明している．これをしなければ，低エネルギー状態のアンサンブルの数を不当に低く評価してしまう，という説明である．\nただし，このアンサンブルによる考え方は自然に我々の思考を詳細釣り合い条件に絞っている．\n\n\n3.4 附言\n収束の速さについて注意喚起しているのみで，ステップサイズ \\(\\alpha&gt;0\\) は大きすぎても棄却率が高まり，小さすぎても攪拌が遅くなるということ以外具体的なことは触れていない．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#実験",
    "href": "posts/2024/Review/Metropolis+1953.html#実験",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "4 実験",
    "text": "4 実験\n\nIn the case of two-dimensional rigid spheres, runs made with 56 particles and with 224 particles agreed within statistical error. For a computing time of a few hours with presently available electronic computers, it seems possible to obtain the pressure for a given volume and temperature to an accuracy of a few percent. In the case of two-dimensional rigid spheres our results are in agreement with the free volume approximation for \\(A/A_0&lt; 1.8\\) and with a five-term virial expansion for \\(A/A_0&gt; 2.5\\). There is no indication of a phase transition.\n\n16 step を焼き入れとし，48-64 いてレーションを実行するのに，MANIAC で 4-5時間かかったという．"
  },
  {
    "objectID": "posts/2024/Review/Metropolis+1953.html#footnotes",
    "href": "posts/2024/Review/Metropolis+1953.html#footnotes",
    "title": "Metropolis+ (1953) Equation of State Calculations by Fast Computing Machines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Faulkner and Livingstone, 2024, p. 13) 4.1 節も参照．↩︎\nこのシミュレーションでは相転移は全く観察されなかった．これはこの方法が極めて収束が遅いことによる．その理由は (Peters and de With, 2012) が導入部で説明する通りである．↩︎\n(戸田盛和 et al., 2011, p. 14)↩︎\n(Faulkner and Livingstone, 2024, p. 15) 4.3 節も参照．↩︎\nさらに，Lennard-Jones ポテンシャルについての２次元のケースも考えており，次のレポートで報告される予定であるという．↩︎\nここで乱数生成法に関して注記されている．これは middle square process で生成する，としている．↩︎\nなお，periodic assumption をしているため（長方形の系の外には同様の系が無数に並んでいるとしたため），境界の外に出ようとした場合は衝突するのではなく，反対側の辺から入ってくるものとする．↩︎\n(Robert and Casella, 2011) も指摘している．だが，(戸田盛和 et al., 2011, p. 5) にも同じ意味で「エルゴード的」という単語を使っている記述がある．↩︎"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal2016.html",
    "href": "posts/2024/Review/Roberts-Rosenthal2016.html",
    "title": "Roberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Rosenthal2016.html#概要",
    "href": "posts/2024/Review/Roberts-Rosenthal2016.html#概要",
    "title": "Roberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits",
    "section": "概要",
    "text": "概要\n(Gareth O. Roberts and Rosenthal, 2016) は (G. O. Roberts et al., 1997), (Gareth O. Roberts and Rosenthal, 1998) で試みられたような最適スケーリングの結果を，アルゴリズムの複雑性に関する結果に換言する方法を議論したもの．\n\n  \n    \n      \n      \n        Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n        Roberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの．\n      \n    \n  \n\n最適スケーリングについては (Gareth O. Roberts and Rosenthal, 2001) も参照．\n\n  \n    \n      \n      \n        Roberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n        Roberts and Rosenthal [Statistical Science 16(2001) 351-67] は Metropolis-Hastings 法の最適スケーリングに関する結果をまとめ，実際の実装にその知見をどのように活かせば良いかを例示したレビュー論文である．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#概要",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#概要",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "概要",
    "text": "概要\n\n研究の立ち位置\nMALA は (Besag, 1994) で提案され，(Gareth O. Roberts and Tweedie, 1996) で指数エルゴード性の必要十分条件が示されている．後の研究 (Gareth O. Roberts and Rosenthal, 1998) で最適スケーリングが論じられる．\n\n  \n    \n      \n      \n        Roberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n        Roberts and Rosenthal [Journal of the Royal Statistical Society. Series B 60(1998) 255-268] は MALA (Metropolis-Adjusted Langevin Algorithm) の最適スケーリングを論じたもの．\n      \n    \n  \n\n(Gareth O. Roberts and Tweedie, 1996) は (G. O. Roberts and Tweedie, 1996) とセットである．後者はランダムウォーク MH に対象を限定して，指数エルゴード性がどのような含意を持つかを検証している（積率も指数収束するための条件，中心極限定理など）．\n\n\nMALA とは\nMALA は，提案核を対象分布 \\(\\pi\\) から Langevin 拡散の形で構成する，problem-specific な Metropolis-Hastings 法であり，ランダムウォーク MH よりも速く収束することが期待される．(Corenflos and Finke, 2024) でも state-of-the-art と呼ばれているサンプリング法である．\n(Fearnhead et al., 2018) において，MALA は BPS と比較されている．モデルは AR(1) を用いており，低次元ではほとんど変わらないが，高次元では BPS の方が自己相関時間が５倍良いという結論が得られている．\nMALA の他に，(Neal, 1994) などの hybrid-Monte Carlo アルゴリズムも MALA と「提案核をよく考えている」という点で関係が深いが，本論文では扱わない．\n\n\nMALA のエルゴード性について\nLangevin 拡散 \\[\ndL_t=dB_t+\\frac{1}{2}\\nabla\\log\\pi(L_t)dt\n\\] は，例えば１次元では，尾部確率が指数減衰するならば，指数エルゴード性を持つ．つまり，\\(\\pi(x)\\,\\propto\\,e^{-\\gamma\\lvert x\\rvert^\\beta}\\;(\\beta\\ge1)\\) など．\nこのように，確率分布の裾の重さに影響されるという現象が普遍的である．\n続いて，Langevin 拡散の離散化が，元の Langevin 拡散に収束するかどうかも別問題である．\n最後に，Langevin 拡散の離散化を Metropolis 法によって修正したアルゴリズムである MALA が指数収束する条件を示す．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#導入",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#導入",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 MCMC 手法の指数エルゴード性について\n(Gelfand and Smith, 1990) 以来，MH 法の利用が爆発している\n\nThere has recently been a real explosion in the use of the Hastings and Metropolis algorithms\n\n乱歩 MH などは対象分布に依らずに実装できる汎用アルゴリズムであるが，対象分布の情報を取り入れて収束を高速にすることが考えられる．その例が MALA である．\nLangevin 動力学は極めて優秀なモデリング手法であるが，シミュレーションのための離散化が入ることで，収束は遅くなるどころかそもそも収束性が失われることさえある．\n(Neal, 1994) を引用しながら，hybrid Monte Carlo が Langevin アルゴリズムに似ていると触れながら，話が対称性と非対称性の話に変わる．\n\nHowever, it is worth remarking that often methods induced by non-reversible methods can be shown to converge more quickly than their reversible counterparts (see, for example, (Sheu, 1991)).\n\nしかし (Sheu, 1991) は \\(P_t\\) の評価を上下からしているのみで，非対称性どころか収束の議論は全くしていない．議論に飛躍があるか，引用が間違っているかが考えられる．\n\n\n1.2 Langevin 拡散について\n本論文が研究する MH 法は，提案を Langevin 拡散により取り入れたものである．\n密度 \\(\\pi\\) が可微分であるとき，\\(\\nabla\\log\\pi\\) が考えられる．これに対して，Langevin 拡散 \\(\\{L_t\\}\\subset\\mathbb{R}^k\\) とは \\[\ndL_t=dW_t+\\frac{1}{2}\\nabla\\log\\pi(L_t)dt\n\\] と定義される．\n\\(\\pi\\) が十分滑らかであるとき，これはエルゴード性を持つ： \\[\n\\|P^t_L(x,-)-\\pi\\|_\\mathrm{TV}\\to0.\n\\]\n本論文では，この収束が指数レートで起こる場合，そして同様の積率収束も起こる場合に興味がある．\n\n\n1.3 指数収束する例\n\n1.3.1 １次元クラス \\(\\mathcal{E}(\\beta,\\gamma)\\)\n\\(k=1\\) とする．\\(\\pi\\in\\mathcal{E}(\\beta,\\gamma)\\subset C^\\infty(\\mathbb{R})\\) であるとは，可微分であり，加えてある \\(x_0\\in\\mathbb{R}\\) と \\(\\gamma,\\beta&gt;0\\) が存在して， \\[\n\\pi(x)\\,\\propto\\,e^{-\\gamma\\lvert x\\rvert^\\beta},\\quad\\lvert x\\rvert\\ge x_0,\n\\] が成り立つことをいう．このとき， \\[\n\\nabla\\log\\pi(x)=-\\gamma\\beta\\operatorname{sgn}(x)\\lvert x\\rvert^{\\beta-1},\\quad\\lvert x\\rvert&gt;x_0.\n\\]\n2.3 節にて，指数収束するための必要十分条件は \\(\\beta\\ge1\\) であることを見る．この結論は乱歩 MH アルゴリズムと全く同様である (Mengersen and Robert, 1996, p. 定理3.5)．\n\n\n1.3.2 多次元指数分布族 \\(\\mathcal{P}_m\\)\n(G. O. Roberts and Tweedie, 1996) で乱歩 MH 法について考えたように，多次元の統計モデルとしては指数分布族を考えることとする．特に，次の場合を考える： \\[\n\\pi(x)\\,\\propto\\,e^{-p(x)}\\quad \\lvert x\\rvert\\ge x_0,\n\\] ただし \\(p\\) は \\(m\\) 次の多項式で，次の表示を持つ： \\[\np=p_m+q_{m-1}.\n\\] \\(\\pi\\in\\mathcal{P}_m\\subset C^\\infty(\\mathbb{R}^k)\\) とは，\\(p_m\\xrightarrow{\\lvert x\\rvert\\to\\infty}\\infty\\) を満たすことをいう．特に，\\(m\\ge2\\) であることに注意．\n\n\n\n1.4 Langevin 拡散 \\(L_t\\) の離散近似\n実際の実装で \\(L_t\\) をそのまま用いることは出来ず，これを離散化することが必要である．\n\n1.4.1 ULA (unadjusted Langevin algorithm)\n\nThe unadjusted Langevin algorithm (ULA) is a discrete-time Markov chain \\(U_n\\) which is the natural discretization of ordinary Langevin diffusion \\(L_t\\).\n\n(Parisi, 1980), (Grenander and Miller, 1994) で考えられたものである．\n\\[\nU_n\\sim\\mathrm{N}_k\\left(U_{n-1}+\\frac{h}{2}\\nabla\\log\\pi(U_{n-1}),hI_k\\right)\n\\] によって構成できる． (Besag, 1994) が指摘したように，元の \\(L_t\\) と似た（しかしステップサイズ \\(h\\) に依存する変換を受けた）不変分布を持つ．\n\n\n\n\n\n\n例\n\n\n\n\n\n\\(\\pi=\\mathrm{N}_1(0,1)\\) であり，ステップサイズを \\(h=2\\) とすると，\\(\\nabla\\log\\pi(x)\\)=-x$ であるから， \\[\nU_{n-1}+\\frac{h}{2}\\nabla\\log\\pi(U_{n-1})=U_{n-1}-\\frac{2}{2}\\frac{U_{n-1}}{1}=0\n\\] より，\\(U_n\\sim\\mathrm{N}_1(0,2)\\) となる．\n\n\n\n\n\n1.4.2 MALA (Metropolis-Adjusted Langevin Algorithm)\n(Besag, 1994) に従い，修正を加える．\\(U_n\\) を提案として，Metropolis-Hastings 法を実行するのである．\n\\[\nq(M_{n-1},-):=\\mathrm{N}_k\\left(M_{n-1}+\\frac{1}{2}h\\nabla\\log\\pi(M_{n-1}),hI_k\\right)\n\\] を提案核とし， \\[\n\\alpha(M_{n-1},U_n):=1\\land\\frac{\\pi(U_n)q(U_n,M_{n-1})}{\\pi(M_{n-1})q(M_{n-1},U_n)}\n\\] を採択確率とする．\nこの場合，必ずエルゴード性が成り立つ．(G. O. Roberts and Tweedie, 1996) の結論と同様，\\(\\ell_k\\)-既約かつ周期的だからである．1 特に，殆ど至る所の \\(x\\in\\mathbb{R}^k\\) について， \\[\n\\|P^n_M(x,-)-\\pi\\|_\\mathrm{TV}\\to0.\n\\] また，本論文が提示する指数収束の条件の下では，任意の \\(x\\in\\mathbb{R}^k\\) で起こる．\nULA が推移的であるとき，MALA も指数エルゴード性を持たないことが判る．だが，これは尾部確率が指数よりも重い場合にしか起こらない．\n\n\n1.4.3 MALTA (Metropolis-Adjusted Langevin Truncated Algorithm)\n\\[\nT_n\\sim\\mathrm{N}_k\\biggr(M_{n-1}+R(M_{n-1}),hI_k\\biggl)\n\\] \\[\nR(x):=\\frac{D\\nabla\\log\\pi(x)}{2(D\\lor\\lvert\\nabla\\log\\pi(x)\\rvert)}.\n\\] を提案とし，MH 法を適用する．\nMALTA は大変ロバストで，指数収束が起こりやすい．"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#langevin-拡散アルゴリズムの指数収束",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#langevin-拡散アルゴリズムの指数収束",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "2 Langevin 拡散アルゴリズムの指数収束",
    "text": "2 Langevin 拡散アルゴリズムの指数収束\n\n2.1 一般の収束結果\n\\[\ndL_t=dW_t+\\frac{1}{2}\\nabla\\log\\pi(L_t)dt\n\\]\n\n\n\n\n\n\n定理2.1（Langevin 拡散のエルゴード性）\n\n\n\n\\(\\nabla\\log\\pi\\) は \\(C^1(\\mathbb{R}^k)\\)-級で，2 ある \\(N,a,b&gt;0\\) が存在して \\[\n\\nabla\\log\\pi(x)\\cdot x\\le a\\lvert x\\rvert^2+b,\\qquad\\lvert x\\rvert&gt;N,\n\\] を満たすとする．このとき，Langevin 拡散 \\(\\{L_t\\}\\) について次が成り立つ：\n\n\\(L\\) は爆発的でなく，\\(\\ell_k\\)-既約で，周期的で，強 Feller であり，任意のコンパクト集合は小集合である．\n\\(L\\) は \\(\\pi\\)-不変であり，任意の \\(x\\in\\mathbb{R}^k\\) について \\[\n\\|P^t_L(x,-)-\\pi\\|_\\mathrm{TV}\\to0.\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\\(\\lvert L_t\\rvert\\) と OU 過程を比較することで，\\(\\lvert L_t\\rvert\\) が爆発しないことが判る．\n続いて，ドリフト係数が局所有界であるため，(Bhattacharya, 1978) と同様の議論により強 Feller である．加えて，\\(\\ell_k\\)-既約であることも判る．3\n\nTheorem 2.1. If, in addition to the hypothesis (A), \\(a_{ij}(-)\\) and \\(b_i(-)\\) are bounded on \\(\\mathbb{R}^k\\), then for each \\(x\\) in \\(\\mathbb{R}^k\\) there exists a unique probability measure \\(P_x\\) on \\((\\Omega',\\mathcal{M}')\\) such that (i) \\(P_x(X_0=x)=1\\), (ii) for every bounded real \\(f\\) on \\(\\mathbb{R}^k\\) having bounded continuous first and second order derivatives, the process \\(f(X_t)-\\int^t_0Lf(X_s)ds\\) is a martingale under \\(P_x\\). Further, (a) \\(X\\) is strong Markov and strong Feller, and (b) support of \\(P_x\\) is \\(\\Omega_x':=\\left\\{\\omega\\in\\Omega\\mid\\omega(0)=x\\right\\}\\). (Bhattacharya, 1978)\n\n任意の \\(\\ell_k\\)-正集合 \\(A\\subset\\mathbb{R}^k\\) に対して， \\[\n   \\widetilde{A}:=\\left\\{\\omega\\in\\Omega_x'\\mid \\omega(t)\\in A\\right\\}\n   \\] は \\(P_x\\)-正集合である．実際，\\(A\\) が \\(\\mathbb{R}^k\\) 上の開集合を含むために \\(\\widetilde{A}\\) は \\(\\Omega_x'\\) 上の開集合を含むので，\\(P_x\\)-零であったら \\(P_x\\) の台が \\(\\Omega_x'\\) 全体であることに矛盾する．これより，\\(L_t\\) は \\(A\\) に到達可能である．よって \\(L_t\\) は \\(\\ell_k\\)-既約．\n（強）Feller かつ \\(\\ell_k\\)-既約ならば，任意のコンパクト集合は小集合である (Tweedie, 1994, p. 定理5.1)．4\n非周期性は，任意のスケルトンも \\(\\ell_k\\)-既約であることから従う．5\n(Ikeda and Watanabe, 1981, p. 5.4節) により，\\(\\pi\\) は \\(L\\) の不変分布であることが，生成作用素 \\[\n   A_Lf(x)=\\left(\\frac{1}{2}\\nabla\\log\\pi(x)\\right)\\cdot\\nabla f(x)+\\frac{1}{2}\\mathop{}\\!\\mathbin\\bigtriangleup f(x)\n   \\] \\[\n   f\\in C^2(\\mathbb{R}^k)\n   \\] に関して，\\((\\pi|A_Lf)=0\\;(f\\in C^2(\\mathbb{R}^k))\\) を満たすために従う．\\(C^2(\\mathbb{R}^k)\\) は，爆発しない拡散過程の生成作用素の核をなす．6\n\\(\\ell_k\\)-既約な Makrov 過程であって不変確率分布を持つから，再帰的ではある．(S. Meyn and Tweedie, 2009, p. 172)定理8.0.1．これは，\\(\\psi\\)-既約な Markov 連鎖は再帰的であるか推移的であるかの２択だからである(Tweedie, 1994, p. 定理2.3)．\nまた連続な過程であるから，これは Harris 再帰性を意味する．\n不変確率分布を持つ非周期的な Harris 再帰的な Markov 連鎖であるから，全変動収束が従う (S. P. Meyn and Tweedie, 1993, p. 定理6.1)．\n\nProposition 6.1. Suppose that \\(\\Phi\\) is positive Harris recurrent, and that some skeleton chain is irreducible. If \\(C\\) is petite, then there exists a constant \\(T&gt;0\\) and a non-trivial measure \\(\\mu\\) such that \\(P^s(x,-)\\ge\\mu(-)\\;(s\\ge T,x\\in C)\\). (S. P. Meyn and Tweedie, 1993)_\n\n\n\n\n\n\n\n2.2 \\(L_t\\) の指数エルゴード性\n\n2.2.1 指数エルゴード性の必要条件\n\\(V:\\mathbb{R}^k\\to[1,\\infty)\\) に対して，\\(V\\)-一様エルゴード的 であるとは，任意の \\(x\\in\\mathbb{R}^k\\) に対して，ある \\(R&gt;0,\\rho\\in(0,1)\\) が存在して \\[\n\\|P^t_L(x,-)-\\pi\\|_V\\le V(x)R\\rho^t,\\qquad t\\ge0,\n\\] が成り立つことをいう．ただし， \\[\n\\|A\\|_V:=\\sup_{\\lvert f\\rvert\\le V}\\int_{\\mathbb{R}^k}f(x)A(dx).\n\\]\n(S. Meyn and Tweedie, 2009, pp. 336–337) 第 14 章にいう \\(f\\)-エルゴード性に当たる概念であることに注意．\\(V\\)-一様エルゴード性は第 16 章で別の意味に使われている．\n\n\n\n\n\n\n定理2.2（Langevin 拡散の指数エルゴード性）\n\n\n\n\\(\\nabla\\log\\pi\\) は \\(C^1(\\mathbb{R}^k)\\)-級で，ある \\(N,a,b&gt;0\\) が存在して \\[\n\\nabla\\log\\pi(x)\\cdot x\\le a\\lvert x\\rvert^2+b,\\qquad\\lvert x\\rvert&gt;N,\n\\] を満たすとする．このとき，Langevin 拡散 \\(\\{L_t\\}\\) について次が成り立つ：\n\n任意の \\(V\\in C^2(\\mathbb{R}^k)\\) であって，\\(V\\ge1\\) かつ，ある \\(b,c&gt;0\\) と非空コンパクト集合 \\(C\\overset{\\textrm{cpt}}{\\subset}\\mathbb{R}^k\\) とについて \\[\nA_LV\\le-cV+b1_C\n\\] を満たすものについて，\\(V\\)-一様エルゴード的である．\n\\(L\\) が指数エルゴード的ならば，ある非空コンパクト集合 \\(C\\overset{\\textrm{cpt}}{\\subset}\\mathbb{R}^k\\) について，ある \\(\\kappa&gt;1,\\delta&gt;0\\) が存在して，任意のスタート地点 \\(x\\in \\mathbb{R}^k\\) について， \\[\n\\sup_{x\\in C}\\operatorname{E}[\\kappa^{\\tau_C^\\delta}]&lt;\\infty,\n\\] \\[\n\\tau^\\delta_C:=\\inf\\left\\{t\\ge\\delta\\mid L_t\\in C\\right\\}.\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n非空コンパクト集合 \\(C\\overset{\\textrm{cpt}}{\\subset}\\mathbb{R}^k\\) は小集合であるから，これは成り立つ．(S. Meyn and Tweedie, 2009, pp. 336–337) 定理 14.0.1 が Markov 連鎖に関して与えている消息である．\n任意の \\(\\delta&gt;0\\) について，\\(\\delta\\)-スケルトンも幾何エルゴード的である．この \\(\\delta\\)-スケルトンに関する \\(C\\) への到着時刻は \\(\\tau^\\delta_C\\) よりも必ず大きくなる．\n\\(L_t\\) は強 Feller であるから，\\(C\\) は \\(\\delta\\)-スケルトンに関しても小集合である．あとは，Markov 連鎖に関する幾何エルゴード定理より従う．7\n\n\n\n\n\n\n2.2.2 指数エルゴード性の十分条件\n\\(-\\nabla\\log\\pi=O(\\lvert x\\rvert^{2k-1})\\;(\\lvert x\\rvert\\to\\infty)\\) としたとき，後述の定理 2.4 の条件である \\(k&lt;1/2\\) を満たさない限り，\\(k\\ge1/2\\) まで指数エルゴード性は成り立ち続ける．8\n\n\n\n\n\n\n定理2.3（Langevin 拡散の指数エルゴード性）\n\n\n\nある \\(S&gt;0\\) が存在して， \\(\\lvert x\\rvert\\ge S\\) 上で \\(\\pi\\) は有界であるとする．加えて，ある \\(d\\in(0,1)\\) が存在して \\[\n\\liminf_{\\lvert x\\rvert\\to\\infty}(1-d)\\lvert\\nabla\\log\\pi(x)\\rvert^2+\\nabla^2\\log\\pi(x)&gt;0\n\\] を満たすならば，\\(\\{L_t\\}\\) は指数エルゴード的である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n上の定理の 1. のドリフト条件を示せば良い．\\(V=\\pi^{-d}\\;(0&lt;d&lt;1)\\) と取れば良い．このとき， \\[\n2A_LV=V\\biggr(\\lvert\\nabla\\log\\pi\\rvert^2(d^2-d)-d\\nabla^2\\log\\pi\\biggl).\n\\]\n\n\n\n\n\n2.2.3 指数エルゴード性が成り立たないとき\n指数エルゴード性が成り立たない場合は，返ってこない時，棄却率が高すぎる時，急に動く場合の３つが考えられる．\n\n\n\n\n\n\n定理2.4（Langevin 拡散が指数エルゴード性を失うとき）\n\n\n\n\\(\\lvert\\nabla\\log\\pi(x)\\rvert\\to0\\) ならば，\\(\\{L_t\\}\\) は指数エルゴード的でない．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n上の定理の 2. と矛盾させれば良い．幾何エルゴード的だと仮定し，コンパクトな Kendall 集合 \\(C\\overset{\\textrm{cpt}}{\\subset}\\mathbb{R}^k\\) が存在するとする： \\[\n\\sup_{x\\in C}\\operatorname{E}[\\kappa^{\\tau^\\delta_C}]&lt;\\infty.\n\\] ある \\(R&gt;0\\) が存在して， \\[\n\\lvert\\nabla\\log\\pi(x)\\rvert\\le2(\\log\\kappa)^{1/2}\\quad(\\lvert x\\rvert\\ge R).\n\\] このとき，\\(S\\ge\\sup_{x\\in C}\\lvert x\\rvert\\lor R\\) を取り，\\(\\lvert y\\rvert\\ge2S\\) を満たす地点 \\(y\\in\\mathbb{R}^k\\) からスタートしたとすると，\\(Z_t:=\\lvert L_t\\rvert\\) は，\\(a(x)&gt;-(\\log\\kappa)^{1/2}\\;(\\lvert x\\rvert&gt;S)\\) を満たす係数について \\[\ndZ_t=dW_t+a(L_t)dt\n\\] を満たす．\\(B_t:=W_t-(\\log\\kappa)^{1/2}t\\) とし，それぞれの過程の \\(S\\) への到着時刻 \\(\\sigma(Z),\\sigma(B)\\) を考えると，\\(\\sigma(Z)\\ge\\sigma(B)\\;\\;\\text{a.s.}\\) が成り立つ．このとき， \\[\\begin{align*}\n  \\operatorname{P}_y[\\tau_C&gt;t]&\\ge\\operatorname{P}_y[\\sigma(Z)&gt;t]\\\\\n  &\\ge\\operatorname{P}_y[\\sigma(B)&gt;t]\\\\\n  &\\ge\\Phi\\left(\\frac{-S+t(\\log\\kappa)^{1/2}}{\\sqrt{t}}\\right)-e^{2S(\\log\\kappa)^{1/2}}\\Phi\\left(\\frac{-S-t(\\log\\kappa)^{1/2}}{\\sqrt{t}}\\right).\n\\end{align*}\\] 最後の式は Bachelier-Lévy の公式による．\\(\\sigma(B)\\) の密度を \\(f\\) とすると， \\[\n\\frac{\\log f(t)}{t}\\to-\\frac{\\log\\kappa}{2}.\n\\] これは矛盾を起こすらしい．\n\n\n\n\n\n\n2.3 指数収束する例\n\n2.3.1 １次元クラス \\(\\mathcal{E}(\\beta,\\gamma)\\)\n\n\n2.3.2 多次元指数分布族 \\(\\mathcal{P}_m\\)"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#ula-アルゴリズム",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#ula-アルゴリズム",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "3 ULA アルゴリズム",
    "text": "3 ULA アルゴリズム"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#mala-アルゴリズム",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#mala-アルゴリズム",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "4 MALA アルゴリズム",
    "text": "4 MALA アルゴリズム"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#結論",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#結論",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "5 結論",
    "text": "5 結論"
  },
  {
    "objectID": "posts/2024/Review/Roberts-Tweedie1996.html#footnotes",
    "href": "posts/2024/Review/Roberts-Tweedie1996.html#footnotes",
    "title": "Roberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDoob の定理から，で良くないか？↩︎\n\\(\\log\\pi\\) が \\(C^1\\)-級，の間違いでは？↩︎\nこれは，Langevin 拡散の分布は \\(\\left\\{\\omega\\in C(\\mathbb{R}_+;\\mathbb{R}^k)\\mid\\omega(0)=x\\right\\}\\) 全体を台に持つから，\\(\\ell_k\\) と同値な分布を持つことになることが(Bhattacharya, 1978) 定理2.1で触れられている．すると，\\(\\ell_k\\)-非零集合は必ず到達可能である．↩︎\n(S. Meyn and Tweedie, 2009, p. 124) 第６章がこのテーマを扱っている．定理6.0.1の(iii)は \\(\\psi\\)-既約な Feller Markov 過程が \\((\\mathrm{supp}\\;\\psi)^\\circ\\ne\\emptyset\\) であるとき，\\(T\\)-連鎖であること，(ii)は \\(\\psi\\)-既約な \\(T\\)-連鎖は任意のコンパクト集合を petite にすることを述べている．(Tweedie, 1994, p. 定理5.1) は連続過程について述べているが結局「(S. Meyn and Tweedie, 2009)と同様に示せる」としか言っていない．最後に，既約かつ非周期的ならば，任意の petite 集合は小さい．↩︎\n\\(\\ell_k\\) と \\(L_t\\) の分布が同値であるから，任意の \\(\\ell_k\\)-正集合には，任意のスケルトンが到達可能であるはずである．このことは周期性を破る．↩︎\n分布を定める (distribution-determining class)という言い方をしている．↩︎\n例えば (S. Meyn and Tweedie, 2009, p. 定理15.0.1)．↩︎\n(Hairer, 2021, p. 34) など．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/NF4.html",
    "href": "posts/2024/Samplers/NF4.html",
    "title": "ニューラル常微分方程式",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF4.html#事前準備",
    "href": "posts/2024/Samplers/NF4.html#事前準備",
    "title": "ニューラル常微分方程式",
    "section": "1 事前準備",
    "text": "1 事前準備\n\n\nCode\nimport math\nimport numpy as np\nfrom IPython.display import clear_output\nfrom tqdm import tqdm_notebook as tqdm\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.color_palette(\"bright\")\nimport matplotlib as mpl\nimport matplotlib.cm as cm\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.nn  import functional as F\nfrom torch.autograd import Variable\n\nuse_cuda = torch.cuda.is_available()\n\n\nまずは ODE ソルバーを用意する．これはどのようなものでも NODE のサブルーチンとして使うことができる．\n\ndef ode_solve(z0, t0, t1, f):\n    \"\"\"\n    Simplest Euler ODE initial value solver\n    \"\"\"\n    h_max = 0.05\n    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n\n    h = (t1 - t0)/n_steps\n    t = t0\n    z = z0\n\n    for i_step in range(n_steps):\n        z = z + h * f(z, t)\n        t = t + h\n    return z\n\nNODE では，\\(D_{x}L_t\\) と \\(D_\\theta L_t\\) とは随伴状態 \\(a(t)\\) に関する ODE で得られる．\nこの ODE の係数を事前に自動微分を通じて計算しておくための親クラスを定義する：\n\nclass ODEF(nn.Module):\n    def forward_with_grad(self, z, t, grad_outputs):\n        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n        batch_size = z.shape[0]\n\n        out = self.forward(z, t)\n\n        a = grad_outputs\n        adfdz, adfdt, *adfdp = torch.autograd.grad(\n            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n            allow_unused=True, retain_graph=True\n        )\n        # grad method automatically sums gradients for batch items, we have to expand them back\n        if adfdp is not None:\n            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n            adfdp = adfdp.expand(batch_size, -1) / batch_size\n        if adfdt is not None:\n            adfdt = adfdt.expand(batch_size, 1) / batch_size\n        return out, adfdz, adfdt, adfdp\n\n    def flatten_parameters(self):\n        p_shapes = []\n        flat_parameters = []\n        for p in self.parameters():\n            p_shapes.append(p.size())\n            flat_parameters.append(p.flatten())\n        return torch.cat(flat_parameters)"
  },
  {
    "objectID": "posts/2024/Samplers/NF4.html#neural-ode-の実装",
    "href": "posts/2024/Samplers/NF4.html#neural-ode-の実装",
    "title": "ニューラル常微分方程式",
    "section": "2 Neural ODE の実装",
    "text": "2 Neural ODE の実装\nNeural ODE では誤差逆伝播の代わりに随伴感度法を用いる．\nこれは torch.nn.Module を継承したクラスとしては定義できないため，torch.autograd.Function を継承したクラスとして定義する：\n\nclass ODEAdjoint(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, z0, t, flat_parameters, func):\n        assert isinstance(func, ODEF)\n        bs, *z_shape = z0.size()\n        time_len = t.size(0)\n\n        with torch.no_grad():\n            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n            z[0] = z0\n            for i_t in range(time_len - 1):\n                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n                z[i_t+1] = z0\n\n        ctx.func = func\n        ctx.save_for_backward(t, z.clone(), flat_parameters)\n        return z\n\n    @staticmethod\n    def backward(ctx, dLdz):\n        \"\"\"\n        dLdz shape: time_len, batch_size, *z_shape\n        \"\"\"\n        func = ctx.func\n        t, z, flat_parameters = ctx.saved_tensors\n        time_len, bs, *z_shape = z.size()\n        n_dim = np.prod(z_shape)\n        n_params = flat_parameters.size(0)\n\n        # Dynamics of augmented system to be calculated backwards in time\n        def augmented_dynamics(aug_z_i, t_i):\n            \"\"\"\n            tensors here are temporal slices\n            t_i - is tensor with size: bs, 1\n            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n            \"\"\"\n            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n\n            # Unflatten z and a\n            z_i = z_i.view(bs, *z_shape)\n            a = a.view(bs, *z_shape)\n            with torch.set_grad_enabled(True):\n                t_i = t_i.detach().requires_grad_(True)\n                z_i = z_i.detach().requires_grad_(True)\n                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n\n            # Flatten f and adfdz\n            func_eval = func_eval.view(bs, n_dim)\n            adfdz = adfdz.view(bs, n_dim)\n            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n\n        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n        with torch.no_grad():\n            ## Create placeholders for output gradients\n            # Prev computed backwards adjoints to be adjusted by direct gradients\n            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n            adj_p = torch.zeros(bs, n_params).to(dLdz)\n            # In contrast to z and p we need to return gradients for all times\n            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n\n            for i_t in range(time_len-1, 0, -1):\n                z_i = z[i_t]\n                t_i = t[i_t]\n                f_i = func(z_i, t_i).view(bs, n_dim)\n\n                # Compute direct gradients\n                dLdz_i = dLdz[i_t]\n                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n\n                # Adjusting adjoints with direct gradients\n                adj_z += dLdz_i\n                adj_t[i_t] = adj_t[i_t] - dLdt_i\n\n                # Pack augmented variable\n                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n\n                # Solve augmented system backwards\n                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n\n                # Unpack solved backwards augmented system\n                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n\n                del aug_z, aug_ans\n\n            ## Adjust 0 time adjoint with direct gradients\n            # Compute direct gradients\n            dLdz_0 = dLdz[0]\n            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n\n            # Adjust adjoints\n            adj_z += dLdz_0\n            adj_t[0] = adj_t[0] - dLdt_0\n        return adj_z.view(bs, *z_shape), adj_t, adj_p, None\n\nこれを nn.Module クラスとしてラップすることで，準備完了である：\n\nclass NeuralODE(nn.Module):\n    def __init__(self, func):\n        super(NeuralODE, self).__init__()\n        assert isinstance(func, ODEF)\n        self.func = func\n\n    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n        t = t.to(z0)\n        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n        if return_whole_sequence:\n            return z\n        else:\n            return z[-1]"
  },
  {
    "objectID": "posts/2024/Samplers/NF4.html#ダイナミクスの再現",
    "href": "posts/2024/Samplers/NF4.html#ダイナミクスの再現",
    "title": "ニューラル常微分方程式",
    "section": "3 ダイナミクスの再現",
    "text": "3 ダイナミクスの再現\n\n3.1 線型ダイナミクス\n簡単な線型ダイナミクスを，線型なダイナミクスで学習する．\n\nclass LinearODEF(ODEF):\n    def __init__(self, W):\n        super(LinearODEF, self).__init__()\n        self.lin = nn.Linear(2, 2, bias=False)\n        self.lin.weight = nn.Parameter(W)\n\n    def forward(self, x, t):\n        return self.lin(x)\n\nclass SpiralFunctionExample(LinearODEF):\n    def __init__(self):\n        super(SpiralFunctionExample, self).__init__(Tensor([[-0.1, -1.], [1., -0.1]]))\n\nclass RandomLinearODEF(LinearODEF):\n    def __init__(self):\n        # super(RandomLinearODEF, self).__init__(torch.randn(2, 2)/2.)\n        super(RandomLinearODEF, self).__init__(Tensor([[0.1, -0.1], [0.1, -0.1]]))\n\ndef to_np(x):\n    return x.detach().cpu().numpy()\n\n\ndef plot_trajectories(obs=None, times=None, trajs=None, save=None, figsize=(16, 8)):\n    plt.figure(figsize=figsize)\n    if obs is not None:\n        if times is None:\n            times = [None] * len(obs)\n        for o, t in zip(obs, times):\n            o, t = to_np(o), to_np(t)\n            for b_i in range(o.shape[1]):\n                plt.scatter(o[:, b_i, 0], o[:, b_i, 1], c=t[:, b_i, 0], cmap=cm.plasma)\n\n    if trajs is not None:\n        for z in trajs:\n            z = to_np(z)\n            plt.plot(z[:, 0, 0], z[:, 0, 1], lw=1.5)\n        if save is not None:\n            plt.savefig(save)\n    plt.show()\n\ndef conduct_experiment(ode_true, ode_trained, n_steps, name, plot_freq=10, lr=0.01):\n    # Create data\n    z0 = Variable(torch.Tensor([[0.6, 0.3]]))\n\n    t_max = 6.29*5\n    n_points = 200\n\n    index_np = np.arange(0, n_points, 1, dtype=np.int64)\n    index_np = np.hstack([index_np[:, None]])\n    times_np = np.linspace(0, t_max, num=n_points)\n    times_np = np.hstack([times_np[:, None]])\n\n    times = torch.from_numpy(times_np[:, :, None]).to(z0)\n    obs = ode_true(z0, times, return_whole_sequence=True).detach()\n    obs = obs + torch.randn_like(obs) * 0.01\n\n    # Get trajectory of random timespan\n    min_delta_time = 1.0\n    max_delta_time = 5.0\n    max_points_num = 32\n    def create_batch():\n        t0 = np.random.uniform(0, t_max - max_delta_time)\n        t1 = t0 + np.random.uniform(min_delta_time, max_delta_time)\n\n        idx = sorted(np.random.permutation(index_np[(times_np &gt; t0) & (times_np &lt; t1)])[:max_points_num])\n\n        obs_ = obs[idx]\n        ts_ = times[idx]\n        return obs_, ts_\n\n    # Train Neural ODE\n    optimizer = torch.optim.Adam(ode_trained.parameters(), lr=lr)\n    for i in range(n_steps):\n        obs_, ts_ = create_batch()\n\n        z_ = ode_trained(obs_[0], ts_, return_whole_sequence=True)\n        loss = F.mse_loss(z_, obs_.detach())\n\n        optimizer.zero_grad()\n        loss.backward(retain_graph=True)\n        optimizer.step()\n\n        if i % plot_freq == 0:\n            z_p = ode_trained(z0, times, return_whole_sequence=True)\n\n            plot_trajectories(obs=[obs], times=[times], trajs=[z_p], save=f\"Files/{name}/{i//plot_freq}.png\")\n            clear_output(wait=True)\n\n\node_true = NeuralODE(SpiralFunctionExample())\node_trained = NeuralODE(RandomLinearODEF())\nconduct_experiment(ode_true, ode_trained, 500, \"linear\")\n\nImageMagick により git 生成した結果は次の通り：\nconvert -delay 10 -loop 0 $(for i in {0..49}; do echo $i.png; done) output.gif\n\n\n\n3.2 非線型ダイナミクス\n今回は非線型のダイナミクスを，ELU を備えた一層のニューラルネットワークで学習する：\n\nclass TestODEF(ODEF):\n    def __init__(self, A, B, x0):\n        super(TestODEF, self).__init__()\n        self.A = nn.Linear(2, 2, bias=False)\n        self.A.weight = nn.Parameter(A)\n        self.B = nn.Linear(2, 2, bias=False)\n        self.B.weight = nn.Parameter(B)\n        self.x0 = nn.Parameter(x0)\n\n    def forward(self, x, t):\n        xTx0 = torch.sum(x*self.x0, dim=1)\n        dxdt = torch.sigmoid(xTx0) * self.A(x - self.x0) + torch.sigmoid(-xTx0) * self.B(x + self.x0)\n        return dxdt\n\nclass NNODEF(ODEF):\n    def __init__(self, in_dim, hid_dim, time_invariant=False):\n        super(NNODEF, self).__init__()\n        self.time_invariant = time_invariant\n\n        if time_invariant:\n            self.lin1 = nn.Linear(in_dim, hid_dim)\n        else:\n            self.lin1 = nn.Linear(in_dim+1, hid_dim)\n        self.lin2 = nn.Linear(hid_dim, hid_dim)\n        self.lin3 = nn.Linear(hid_dim, in_dim)\n        self.elu = nn.ELU(inplace=True)\n\n    def forward(self, x, t):\n        if not self.time_invariant:\n            x = torch.cat((x, t), dim=-1)\n\n        h = self.elu(self.lin1(x))\n        h = self.elu(self.lin2(h))\n        out = self.lin3(h)\n        return out\n\n\nfunc = TestODEF(Tensor([[-0.1, -0.5], [0.5, -0.1]]), Tensor([[0.2, 1.], [-1, 0.2]]), Tensor([[-1., 0.]]))\node_true = NeuralODE(func)\n\nfunc = NNODEF(2, 16, time_invariant=True)\node_trained = NeuralODE(func)\n\nconduct_experiment(ode_true, ode_trained, 3000, \"nonlinear\", plot_freq=30, lr=0.001)\n\n逡巡を繰り返して学習する様子がよく伺える．学習率を lr=0.001 としているが，lr=0.01 でも lr=0.005 でも，学習が非常に良い線まで行ってもすぐに初期値よりもカオスなダイナミクスに戻ってしまう挙動がよく見られた．"
  },
  {
    "objectID": "posts/2024/Samplers/NF4.html#文献紹介",
    "href": "posts/2024/Samplers/NF4.html#文献紹介",
    "title": "ニューラル常微分方程式",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\nMikhail Surtsukov 氏によるチュートリアルが，このレポジトリで公開されている．\nFFJORD (Grathwohl et al., 2019) の実装は，このレポジトリで公開されている．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM1.html",
    "href": "posts/2024/Samplers/EBM1.html",
    "title": "スコアマッチング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/EBM1.html#sgm-score-based-generative-model",
    "href": "posts/2024/Samplers/EBM1.html#sgm-score-based-generative-model",
    "title": "スコアマッチング",
    "section": "1 SGM (Score-based Generative Model)",
    "text": "1 SGM (Score-based Generative Model)\n\n1.1 目標のデータ\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_swiss_roll\n\nimport jax\nimport jax.numpy as jnp\n\ntry:\n    from flax import linen as nn  # The Linen API\nexcept ModuleNotFoundError:\n    %pip install -qq flax\n    from flax import linen as nn  # The Linen API\nfrom flax.training import train_state  # Useful dataclass to keep train state\n\ntry:\n    import optax  # Optimizers\nexcept ModuleNotFoundError:\n    %pip install -qq optax\n    import optax  # Optimizers\n\nfrom functools import partial\n\nfrom IPython.display import clear_output\n\n\ndef sample_batch(size, noise=1.0):\n    x, _ = make_swiss_roll(size, noise=noise)\n    x = x[:, [0, 2]] / 10.0\n    return np.array(x)\n\n\n# plt.figure(figsize=[16, 16])\nplt.scatter(*sample_batch(10**4).T, alpha=0.1)\nplt.axis(\"off\")\nplt.tight_layout()\n# plt.savefig(\"swiss_roll.png\")\n\n\n\n\n\n\n\n\n\n\n1.2 モデルの定義\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n\n        x = nn.Dense(128)(x)\n        x = nn.softplus(x)\n        x = nn.Dense(128)(x)\n        x = nn.softplus(x)\n        x = nn.Dense(2)(x)\n\n        return x\n\n\n\n1.3 モデルの訓練\n\n@jax.jit\ndef compute_loss(params, inputs):\n    #  a function that computes jacobian by forward mode differentiation\n    jacobian = jax.jacfwd(Model().apply, argnums=-1)\n\n    # we use jax.vmap to vectorize jacobian function along batch dimension\n    batch_jacobian = jax.vmap(partial(jacobian, {\"params\": params}))(inputs)  # [batch, dim, dim]\n\n    trace_jacobian = jnp.trace(batch_jacobian, axis1=1, axis2=2)\n    output_norm_sq = jnp.square(Model().apply({\"params\": params}, inputs)).sum(axis=1)\n\n    return jnp.mean(trace_jacobian + 1 / 2 * output_norm_sq)\n\n\n@jax.jit\ndef train_step(state, batch, key):\n    \"\"\"Train for a single step.\"\"\"\n    loss = compute_loss(state.params, batch)\n    grads = jax.grad(compute_loss, argnums=0)(state.params, batch)\n    state = state.apply_gradients(grads=grads)\n    return state, loss\n\ndef create_train_state(rng, learning_rate):\n    \"\"\"Creates initial `TrainState`.\"\"\"\n    net = Model()\n    params = net.init(rng, jnp.ones([128, 2]))[\"params\"]\n    tx = optax.adam(learning_rate)\n    return train_state.TrainState.create(apply_fn=net.apply, params=params, tx=tx)\n\ndef train_loop(key, train_step, nsteps):\n    key, subkey = jax.random.split(key)\n    state = create_train_state(subkey, 1e-3)\n    del subkey  # Must not be used anymore.\n    loss_history = []\n    for i in range(nsteps):\n        x = sample_batch(size=128)\n        key, subkey = jax.random.split(key)\n        state, loss = train_step(state, x, subkey)\n        loss_history.append(loss.item())\n\n        if i % 200 == 0:\n            clear_output(True)\n            plt.figure(figsize=[16, 8])\n            plt.subplot(1, 2, 1)\n            plt.title(\"mean loss = %.3f\" % jnp.mean(jnp.array(loss_history[-32:])))\n            plt.scatter(jnp.arange(len(loss_history)), loss_history)\n            plt.grid()\n\n            plt.subplot(1, 2, 2)\n            xx = jnp.stack(jnp.meshgrid(jnp.linspace(-1.5, 2.0, 50), jnp.linspace(-1.5, 2.0, 50)), axis=-1).reshape(\n                -1, 2\n            )\n            scores = Model().apply({\"params\": state.params}, xx)\n            scores_norm = jnp.linalg.norm(scores, axis=-1, ord=2, keepdims=True)\n            scores_log1p = scores / (scores_norm + 1e-9) * jnp.log1p(scores_norm)\n\n            plt.quiver(*xx.T, *scores_log1p.T, width=0.002, color=\"green\")\n            plt.xlim(-1.5, 2.0)\n            plt.ylim(-1.5, 2.0)\n            plt.show()\n\n    return state\n\nstate = train_loop(jax.random.PRNGKey(seed=42), train_step, 10000)\n\n\n\n\n\n\n\n\n\n\n1.4 学習されたスコア\n\n# plt.figure(figsize=[16, 16])\n\nxx = jnp.stack(jnp.meshgrid(jnp.linspace(-1.5, 1.5, 50), jnp.linspace(-1.5, 1.5, 50)), axis=-1).reshape(-1, 2)\nscores = Model().apply({\"params\": state.params}, xx)\nscores_norm = jnp.linalg.norm(scores, axis=-1, ord=2, keepdims=True)\nscores_log1p = scores / (scores_norm + 1e-9) * jnp.log1p(scores_norm)\n\nplt.quiver(*xx.T, *scores_log1p.T, width=0.002, color=\"green\")\nplt.scatter(*sample_batch(10_000).T, alpha=0.1)\nplt.axis(\"off\")\nplt.tight_layout()\n# plt.savefig(\"score_matching_swiss_roll.png\")\n\n\n\n\n\n\n\n図 1"
  },
  {
    "objectID": "posts/2024/Samplers/EBM1.html#ssm-sliced-score-matching",
    "href": "posts/2024/Samplers/EBM1.html#ssm-sliced-score-matching",
    "title": "スコアマッチング",
    "section": "2 SSM (Sliced Score Matching)",
    "text": "2 SSM (Sliced Score Matching)"
  },
  {
    "objectID": "posts/2024/Samplers/EBM1.html#jax-framework",
    "href": "posts/2024/Samplers/EBM1.html#jax-framework",
    "title": "スコアマッチング",
    "section": "3 JAX framework",
    "text": "3 JAX framework\n\n3.1 はじめに\nGoogle の JAX (GitHub) とは，科学計算と機械学習のためのフレームワークである．\nAutograd (GitHub) を用いて，Python のビルトイン関数や NumPy 関数を自動微分することができる．\n今回は JAX エコシステムの一つである，深層学習のためのフレームワークである Flax，特に Linen モジュール (docs / GitHub) を用いた．"
  },
  {
    "objectID": "posts/2024/Samplers/EBM1.html#文献",
    "href": "posts/2024/Samplers/EBM1.html#文献",
    "title": "スコアマッチング",
    "section": "4 文献",
    "text": "4 文献\n\nコードは (Murphy, 2023) の こちら を参考にした．\nYang Song のコードも参考．\n(Song et al., 2019) のコードは このレポジトリ で公開されている．\nUvA DL Tutorial も参照．"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html",
    "href": "posts/2024/Samplers/NF2.html",
    "title": "正規化流",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html#real-nvp-dinh2017",
    "href": "posts/2024/Samplers/NF2.html#real-nvp-dinh2017",
    "title": "正規化流",
    "section": "1 Real NVP (Dinh et al., 2017)",
    "text": "1 Real NVP (Dinh et al., 2017)\n\n# Import required packages\nimport torch\nimport torchvision as tv\nimport numpy as np\nimport normflows as nf\n\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\nfrom tqdm import tqdm\n\n\n# Set up model\n\n# Define 2D Gaussian base distribution\nbase = nf.distributions.base.DiagGaussian(2)\n\n# Define list of flows\nnum_layers = 32\nflows = []\nfor i in range(num_layers):\n    # Neural network with two hidden layers having 64 units each\n    # Last layer is initialized by zeros making training more stable\n    param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True)\n    # Add flow layer\n    flows.append(nf.flows.AffineCouplingBlock(param_map))\n    # Swap dimensions\n    flows.append(nf.flows.Permute(2, mode='swap'))\n    \n# Construct flow model\nmodel = nf.NormalizingFlow(base, flows)\ndevice = torch.device(\"mps\")\nmodel = model.to(device)\n\n\n# Define target distribution\ntarget = nf.distributions.TwoMoons()\n\n# Plot target distribution\ngrid_size = 200\nxx, yy = torch.meshgrid(torch.linspace(-3, 3, grid_size), torch.linspace(-3, 3, grid_size))\nzz = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\nzz = zz.to(device)\n\nlog_prob = target.log_prob(zz).to('cpu').view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\n# plt.figure(figsize=(15, 15))\nplt.pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\nplt.gca().set_aspect('equal', 'box')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot initial flow distribution\nmodel.eval()\nlog_prob = model.log_prob(zz).to('cpu').view(*xx.shape)\nmodel.train()\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\n# plt.figure(figsize=(15, 15))\nplt.pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\nplt.gca().set_aspect('equal', 'box')\nplt.show()\n\n\n\n\n\n\n\n\n# Train model\nmax_iter = 4000\nnum_samples = 2 ** 9\nshow_iter = 500\n\n\nloss_hist = np.array([])\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n\nfor it in tqdm(range(max_iter)#, disable=True\n):\n    optimizer.zero_grad()\n    \n    # Get training samples\n    x = target.sample(num_samples).to(device)\n    \n    # Compute loss\n    loss = model.forward_kld(x)\n    \n    # Do backprop and optimizer step\n    if ~(torch.isnan(loss) | torch.isinf(loss)):\n        loss.backward()\n        optimizer.step()\n    \n    # Log loss\n    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n    \n    # Plot learned distribution\n    if (it + 1) % show_iter == 0:\n        model.eval()\n        log_prob = model.log_prob(zz)\n        model.train()\n        prob = torch.exp(log_prob.to('cpu').view(*xx.shape))\n        prob[torch.isnan(prob)] = 0\n\n        # plt.figure(figsize=(15, 15))\n        plt.pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n        plt.gca().set_aspect('equal', 'box')\n        plt.show()\nnp.save('loss_history.npy', loss_hist)\n\n\n\n\n# Plot loss\n# plt.figure(figsize=(10, 10))\nloss_hist = np.load('Files/loss_history.npy')\nplt.plot(loss_hist, label='loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot target distribution\nf, ax = plt.subplots(1, 2, sharey=True, figsize=(15, 7))\n\nlog_prob = target.log_prob(zz).to('cpu').view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nax[0].pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n\nax[0].set_aspect('equal', 'box')\nax[0].set_axis_off()\nax[0].set_title('Target', fontsize=24)\n\n# Plot learned distribution\nmodel.eval()\nlog_prob = model.log_prob(zz).to('cpu').view(*xx.shape)\nmodel.train()\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nax[1].pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n\nax[1].set_aspect('equal', 'box')\nax[1].set_axis_off()\nax[1].set_title('Real NVP', fontsize=24)\n# plt.savefig(\"./Files/NF2.png\")\nplt.subplots_adjust(wspace=0.1)\nplt.show()"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html#neural-spline-flow-durkan2019",
    "href": "posts/2024/Samplers/NF2.html#neural-spline-flow-durkan2019",
    "title": "正規化流",
    "section": "2 Neural Spline Flow (Durkan et al., 2019)",
    "text": "2 Neural Spline Flow (Durkan et al., 2019)\n円周 \\(S^1\\) 上の確率分布として，wrapped Normal 分布や von Mises 分布がある．\n今回は後者を採用し，\\(\\mathbb{R}^2\\) 上で密度モデリングを試みる：\n\n# Set up target\nclass GaussianVonMises(nf.distributions.Target):\n    def __init__(self):\n        super().__init__(prop_scale=torch.tensor(2 * np.pi), \n                         prop_shift=torch.tensor(-np.pi))\n        self.n_dims = 2\n        self.max_log_prob = -1.99\n        self.log_const = -1.5 * np.log(2 * np.pi) - np.log(np.i0(1))\n    \n    def log_prob(self, x):\n        return -0.5 * x[:, 0] ** 2 + torch.cos(x[:, 1] - 3 * x[:, 0]) + self.log_const\n\ntarget = GaussianVonMises()\n\n# Plot target\ngrid_size = 300\nxx, yy = torch.meshgrid(torch.linspace(-2.5, 2.5, grid_size), torch.linspace(-np.pi, np.pi, grid_size))\nzz = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\n\nlog_prob = target.log_prob(zz).view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nplt.figure(figsize=(15, 15))\nplt.pcolormesh(yy, xx, prob.data.numpy(), cmap='coolwarm')\nplt.gca().set_aspect('equal', 'box')\nplt.show()\n\n\n\n\n\n\n\n\n今回は 12 層の Neural Spline Flow を採用し，２次元の Gaussian 分布に基底として採用する．\n\nbase = nf.distributions.UniformGaussian(2, [1], torch.tensor([1., 2 * np.pi]))\n\nK = 12\n\nflow_layers = []\nfor i in range(K):\n    flow_layers += [nf.flows.CircularAutoregressiveRationalQuadraticSpline(2, 1, 512, [1], num_bins=10,\n                                                                           tail_bound=torch.tensor([5., np.pi]),\n                                                                           permute_mask=True)]\n\nmodel = nf.NormalizingFlow(base, flow_layers, target)\n\n# Move model on GPU if available\ndevice = torch.device(\"mps\")\nmodel = model.to(device)\n\n\n# Plot model\nlog_prob = model.log_prob(zz.to(device)).to('cpu').view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nplt.figure()\nplt.pcolormesh(yy, xx, prob.data.numpy(), cmap='coolwarm')\nplt.gca().set_aspect('equal', 'box')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train model\nmax_iter = 10000\nnum_samples = 2 ** 14\nshow_iter = 2500\n\n\nloss_hist = np.array([])\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iter)\n\nfor it in tqdm(range(max_iter)):\n    optimizer.zero_grad()\n    \n    # Compute loss\n    loss = model.reverse_kld(num_samples)\n    \n    # Do backprop and optimizer step\n    if ~(torch.isnan(loss) | torch.isinf(loss)):\n        loss.backward()\n        optimizer.step()\n    \n    # Log loss\n    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n    \n    # Plot learned model\n    if (it + 1) % show_iter == 0:\n        model.eval()\n        with torch.no_grad():\n            log_prob = model.log_prob(zz.to(device)).to('cpu').view(*xx.shape)\n        model.train()\n        prob = torch.exp(log_prob)\n        prob[torch.isnan(prob)] = 0\n\n        plt.figure(figsize=(15, 15))\n        plt.pcolormesh(yy, xx, prob.data.numpy(), cmap='coolwarm')\n        plt.gca().set_aspect('equal', 'box')\n        plt.show()\n    \n    # Iterate scheduler\n    scheduler.step()\n\n# Plot loss\nplt.figure(figsize=(10, 10))\nplt.plot(loss_hist, label='loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n訓練は L4 で約１時間であった．\n\n\n# 2D plot\nf, ax = plt.subplots(1, 2, sharey=True, figsize=(15, 7))\n\nlog_prob = target.log_prob(zz).view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nax[0].pcolormesh(yy, xx, prob.data.numpy(), cmap='coolwarm')\nax[0].set_aspect('equal', 'box')\n\nax[0].set_xticks(ticks=[-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\nax[0].set_xticklabels(['$-\\pi$', r'$-\\frac{\\pi}{2}$', '$0$', r'$\\frac{\\pi}{2}$', '$\\pi$'],\n                      fontsize=20)\nax[0].set_yticks(ticks=[-2, -1, 0, 1, 2])\nax[0].set_yticklabels(['$-2$', '$-1$', '$0$', '$1$', '$2$'],\n                      fontsize=20)\nax[0].set_xlabel('$\\phi$', fontsize=24)\nax[0].set_ylabel('$x$', fontsize=24)\n\nax[0].set_title('Target', fontsize=24)\n\nlog_prob = model.log_prob(zz.to(device)).to('cpu').view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nax[1].pcolormesh(yy, xx, prob.data.numpy(), cmap='coolwarm')\nax[1].set_aspect('equal', 'box')\n\nax[1].set_xticks(ticks=[-np.pi, -np.pi/2, 0, np.pi/2, np.pi])\nax[1].set_xticklabels(['$-\\pi$', r'$-\\frac{\\pi}{2}$', '$0$', r'$\\frac{\\pi}{2}$', '$\\pi$'],\n                      fontsize=20)\nax[1].set_xlabel('$\\phi$', fontsize=24)\n\nax[1].set_title('Neural Spline Flow', fontsize=24)\n\nplt.subplots_adjust(wspace=0.1)\n\nplt.show()\n\n\n\n# 3D plot\nfig = plt.figure(figsize=(15, 7))\nax1 = fig.add_subplot(1, 2, 1, projection='3d')\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\nphi = np.linspace(-np.pi, np.pi, grid_size)\nz = np.linspace(-2.5, 2.5, grid_size)\n\n# create the surface\nx = np.outer(np.ones(grid_size), np.cos(phi))\ny = np.outer(np.ones(grid_size), np.sin(phi))\nz = np.outer(z, np.ones(grid_size))\n\n# Target\nlog_prob = target.log_prob(zz).view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nprob_vis = prob / torch.max(prob)\nmyheatmap = prob_vis.data.numpy()\n\nax1._axis3don = False\nax1.plot_surface(x, y, z, cstride=1, rstride=1, facecolors=cm.coolwarm(myheatmap), shade=False)\n\nax1.set_title('Target', fontsize=24, y=0.97, pad=0)\n\n# Model\nlog_prob = model.log_prob(zz.to(device)).to('cpu').view(*xx.shape)\nprob = torch.exp(log_prob)\nprob[torch.isnan(prob)] = 0\n\nprob_vis = prob / torch.max(prob)\nmyheatmap = prob_vis.data.numpy()\n\nax2._axis3don = False\nax2.plot_surface(x, y, z, cstride=1, rstride=1, facecolors=cm.coolwarm(myheatmap), shade=False)\n\nt = ax2.set_title('Neural Spline Flow', fontsize=24, y=0.97, pad=0)\n\nplt.show()"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html#glow-kingma-dhariwal2018",
    "href": "posts/2024/Samplers/NF2.html#glow-kingma-dhariwal2018",
    "title": "正規化流",
    "section": "3 Glow (Kingma and Dhariwal, 2018)",
    "text": "3 Glow (Kingma and Dhariwal, 2018)\n今回は CIFAR-10 という手描き文字画像データセットを学習し，画像の生成を目指す．\nこの際には，(Dinh et al., 2017) の multiscale architecture を採用し，基底分布も成分ごとにスケールが違う正規分布を用いる．\n\n# Set up model\n\n# Define flows\nL = 3\nK = 16\ntorch.manual_seed(0)\n\ninput_shape = (3, 32, 32)\nn_dims = np.prod(input_shape)\nchannels = 3\nhidden_channels = 256\nsplit_mode = 'channel'\nscale = True\nnum_classes = 10\n\n# Set up flows, distributions and merge operations\nq0 = []\nmerges = []\nflows = []\nfor i in range(L):\n    flows_ = []\n    for j in range(K):\n        flows_ += [nf.flows.GlowBlock(channels * 2 ** (L + 1 - i), hidden_channels,\n                                     split_mode=split_mode, scale=scale)]\n    flows_ += [nf.flows.Squeeze()]\n    flows += [flows_]\n    if i &gt; 0:\n        merges += [nf.flows.Merge()]\n        latent_shape = (input_shape[0] * 2 ** (L - i), input_shape[1] // 2 ** (L - i),\n                        input_shape[2] // 2 ** (L - i))\n    else:\n        latent_shape = (input_shape[0] * 2 ** (L + 1), input_shape[1] // 2 ** L,\n                        input_shape[2] // 2 ** L)\n    q0 += [nf.distributions.ClassCondDiagGaussian(latent_shape, num_classes)]\n\n\n# Construct flow model with the multiscale architecture\nmodel = nf.MultiscaleFlow(q0, flows, merges)\nmodel = model.to(device)\n\n\n# Prepare training data\nbatch_size = 128\n\ntransform = tv.transforms.Compose([tv.transforms.ToTensor(), nf.utils.Scale(255. / 256.), nf.utils.Jitter(1 / 256.)])\ntrain_data = tv.datasets.CIFAR10('datasets/', train=True,\n                                 download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,\n                                           drop_last=True)\n\ntest_data = tv.datasets.CIFAR10('datasets/', train=False,\n                                download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n\ntrain_iter = iter(train_loader)\n\n\n# Train model\nmax_iter = 20000\n\nloss_hist = np.array([])\n\noptimizer = torch.optim.Adamax(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\nfor i in tqdm(range(max_iter)):\n    try:\n        x, y = next(train_iter)\n    except StopIteration:\n        train_iter = iter(train_loader)\n        x, y = next(train_iter)\n    optimizer.zero_grad()\n    loss = model.forward_kld(x.to(device), y.to(device))\n\n    if ~(torch.isnan(loss) | torch.isinf(loss)):\n        loss.backward()\n        optimizer.step()\n\n    loss_hist = np.append(loss_hist, loss.detach().to('cpu').numpy())\n\n\nplt.figure(figsize=(10, 10))\nplt.plot(loss_hist, label='loss')\nplt.legend()\nplt.savefig('fig1.png')\nplt.show()\n\n\n2万イテレーションで1時間10分を要したが，cutting-edge な性能を出すには遥かに大きいモデルを 100 万イテレーションほどする必要があるという．\n\n# Model samples\nnum_sample = 10\n\nwith torch.no_grad():\n    y = torch.arange(num_classes).repeat(num_sample).to(device)\n    x, _ = model.sample(y=y)\n    x_ = torch.clamp(x, 0, 1)\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.transpose(tv.utils.make_grid(x_, nrow=num_classes).cpu().numpy(), (1, 2, 0)))\n    plt.savefig('fig2.png')\n    plt.show()"
  },
  {
    "objectID": "posts/2024/Samplers/NF2.html#文献",
    "href": "posts/2024/Samplers/NF2.html#文献",
    "title": "正規化流",
    "section": "4 文献",
    "text": "4 文献\n\nEric Jang 氏 による チュートリアル （やその他のチュートリアル）は，TensorFlow 1 を用いており，特に tfb.Affine はもうサポートされていない（対応表）．\nnormflows というパッケージ (Stimper et al., 2023) は PyTorch ベースの実装を提供しており，これを代わりに用いた．Real NVP, Neural Spline Flow, Glow などのデモを公開している．"
  },
  {
    "objectID": "posts/2024/Samplers/NF1.html",
    "href": "posts/2024/Samplers/NF1.html",
    "title": "ニューラル常微分方程式",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/NF1.html#sec-CNF",
    "href": "posts/2024/Samplers/NF1.html#sec-CNF",
    "title": "ニューラル常微分方程式",
    "section": "1 ニューラル常微分方程式 (NODE)",
    "text": "1 ニューラル常微分方程式 (NODE)\n\n1.1 ベクトル場のモデリング\nベクトル場 \\(F:\\mathbb{R}^d\\times[0,T]\\to\\mathbb{R}^d\\) に関して ODE \\[\n\\frac{d x_t}{d t}=F(x_t,t)\n\\tag{1}\\] を満たす曲線 \\((x_t)\\) を 積分曲線 という．\nCNF (Continuous Normalizing Flow) では，\\((x_t)\\) はデータ分布とノイズ分布を結ぶダイナミクスとする．すなわち，フロー \\((f_t)\\)，特に \\(f_1\\) を輸送写像としてノイズからのデータの生成を目指す．\nこの積分曲線をモデリングするために，ベクトル場 \\(F\\) をニューラルネットによってモデリングするが，CNF では離散化誤差を入れずに，連続なままモデリングする方法を考える．\n\n\n\n1.2 Neural ODE\n\\(F\\) が得られたならば，Euler の方法により積分曲線 \\((x_t)\\) を数値計算できる： \\[\nx_{t+\\epsilon}=x_t+\\epsilon F(x_t,t),\\qquad\\epsilon&gt;0.\n\\]\nこの式の形から，\\(F\\) が定める ODE を \\(\\epsilon F(x_t,t)\\) が定める \\(T/\\epsilon\\) 層の残差ネットワークによってモデリングすることもあり得たが，それではタイムステップ \\(\\epsilon&gt;0\\) を自由に設定することができない．\n連続時間アプローチではこの出力 \\(x_T\\) を得る手続きは，完全にネットワーク外の ODE ソルバーに任せてしまう．一方で，\\(\\epsilon&gt;0\\) を自由に取れるように，連続なままダイナミクスをモデリングする．これが NODE (Neural ODE) (Chen et al., 2018) である．\n従って NODE ではその強みを活かし，\\(\\epsilon&gt;0\\) を必ずしも等間隔ではなく，適応的な設定が追求される．\n\n\n1.3 訓練\n\\(F(x_t,t)\\) を何度も使う NODE のスキームは，\\(x_0,x_T\\) のみに依存した損失関数に関する誤差逆伝播法に向いていない．\n(Chen et al., 2018) では，最適制御の分野で知られていた (Pontryagin et al., 1962) の 随伴感度法 (adjoint sensitivity method) を用いた誤差逆伝播法の連続時間への拡張を提案している．\n時刻 \\(t\\) での損失 \\(L_t(x_t)\\) はパス \\((x_t)\\) の全体に依存する汎函数であるとする： \\[\nL_t(x_{t}):=L\\left(x_0+\\int^t_0F_\\theta(x_t,t)\\,dt\\right).\n\\]\n\\(\\frac{d L_t}{d \\theta}\\) を計算するためには，まずは次の 随伴（状態） を考える： \\[\na(t):=\\frac{d L_t(x_t)}{d x_t}.\n\\]\n出力 \\(x_T\\) が得られているとするならば，この随伴は (Pontryagin et al., 1962) の定理から次の ODE を後ろ向きに解けば良いため，実際に \\(x_t,L_t\\) を計算して微分する必要はない： \\[\n\\frac{d a(t)}{d t}=-a(t)^\\top\\frac{\\partial F_\\theta(x_t,t)}{\\partial x_t}.\n\\tag{2}\\]\nこの ODE にも \\(x_t\\) の項が表れているが，ODE (1) と同時に解けば良い．こうして \\(a(t)\\) を得たのちは，\n\\[\n\\frac{d L_t}{d \\theta}=-\\int^t_0a(s)^\\top\\frac{\\partial F_\\theta(x_s,s)}{\\partial \\theta}\\,ds\n\\tag{3}\\] によって最終的な勾配を得る．\n\n\n\n\n\n\n勾配の計算法\n\n\n\n\n誤差逆伝播により \\(D_xF_\\theta,D_\\theta F_\\theta\\) を得る．\nODE (1) と (2) を解いて随伴 \\(a(t)\\) を得る．\n勾配の計算 (3) により勾配 \\(\\frac{d L_t}{d \\theta}\\) を得る．\n\n\n\n実際には，(1), (2), (3) は同時に1つの ODE ソルバーへの関数呼び出しで解くことができる．\n\n\n1.4 Jacobian の計算\nNODE を連続な正則化流として用いるためには，損失 \\(L\\) に尤度 \\(p_T\\) を登場させる必要がある： \\[\n\\log p_t(x_t)=\\log p(x_0)-\\log\\lvert\\det J_{f_t}(x_t)\\rvert.\n\\]\nそして尤度の評価のためにはフロー \\((f_t)\\) の Jacobian \\(J_{f_t}(x_t)\\) が必要である．\n残差ネットワークによる正規化流 においては，Hutchinson の跡推定量 を用いたり，残差接続の関数形を単純にして Jacobian を解析的に計算可能にしたりという方法で，Jacobian の計算 \\(O(d^3)\\) を効率化していた．\nNODE では，Jacobian \\(J_{f_t}(x_t)\\) は \\[\n\\frac{d \\log p_t(x_t)}{d t}=-\\frac{d \\log\\lvert\\det J_{f_t}(x_t)\\rvert}{d t}=-\\operatorname{Tr}\\biggr(J_{F_t}(x_t)\\biggl)\n\\] を利用することで，\\(J_{F_t}\\) の跡から得ることができる (Chen et al., 2018, p. 定理1)： \\[\n\\log p_t(x_t)=\\log p(x_0)-\\int^t_0\\operatorname{Tr}\\biggr(J_{F_s}(x_s)\\biggl)\\,ds.\n\\]\n\\(\\det J_{f_t}(x_t)\\) の行列評価が \\(O(d^3)\\) であるところを，\\(\\operatorname{Tr}(J_{F_t}(x_t))\\) の計算は \\(O(d^2)\\) で済む．\nこうして，勾配 \\(D_\\theta L_t\\) と Jacobian \\(J_{f_t}\\) の計算が，いずれも \\(F_\\theta\\) の微分係数が定める ODE の数値解を求めることに帰着される．\n\n\n1.5 Hutchinson の跡推定量による更なる軽量化\nFFJORD (Free-Form Jacobian of Reversible Dynamics) (Grathwohl et al., 2019) では，\\(\\operatorname{Tr}(J_{F_t}(x_t))\\) の計算に Hutchinson の跡推定量 を用いる： \\[\n\\log p_t(x_t)=\\log p(x_0)-\\operatorname{E}\\left[\\int^t_0\\epsilon^\\top J_{F_s}(x_s)\\epsilon\\,ds\\right].\n\\]\nこれにより最終的に \\(O(d)\\) の計算量が達成される上に，Glow (Kingma and Dhariwal, 2018) の 2% 以下のパラメータ数に抑えられる．\n\n\n\n(Grathwohl et al., 2019, p. 5)\n\n\n\n\n1.6 Neural SDE\nNeural SDE (Tzen and Raginsky, 2019), (Peluchetti and Favaro, 2020) は ODE を SDE に拡張することでモデリングの柔軟性をさらに高めた．\nこれらの方法では SDE の係数をニューラルネットワークでモデリングし，サンプリングは SDE ソルバーによって行う．\nこれについては 拡散モデル の稿を参照：\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Samplers/NF1.html#フローマッチング-fm",
    "href": "posts/2024/Samplers/NF1.html#フローマッチング-fm",
    "title": "ニューラル常微分方程式",
    "section": "2 フローマッチング (FM)",
    "text": "2 フローマッチング (FM)\n\n2.1 はじめに\n拡散模型をスコアマッチングと見ることでさらに効率的な訓練が可能になったように，NODE を フローマッチング (FM: Flow Matching) (Lipman et al., 2023) と見ることでよりスケーラブルな代替訓練方法が与えられる．\nFM ではベクトル場 \\(F_\\theta(x_t,t)\\) を直接目標 \\(F(x_t,t)\\) に向けて回帰する．この目標 \\(F(x_t,t)\\) はノイズ分布とデータ分布を結ぶ輸送を定めるように決定される．\n直接的な目標 \\(F(x_t,t)\\) を特定することで，随伴感度法と ODE ソルバーを通じた連続な誤差逆伝播よりも直接的な訓練目標 (5) が得られ，効率的でスケーラブルな訓練手法が得られる．\n拡散過程 も，与えられた SDE と等価な輸送を行う ODE (Y. Song et al., 2021), (Maoutsa et al., 2020) を通じてベクトル場のモデリングに議論を帰着できるから，FM により Neural ODE を効率的に訓練できたら，拡散模型のサンプリングの遅さの問題も解決できる．しかし，フローマッチングの美点はそれにとどまらない．\n始点と終点がノイズ分布とデータ分布である限り，輸送はそもそも拡散過程に基づいたものである必要はない．フローマッチングでは，\\(F(x_t,t)\\) をそのように選ぶことで，最適輸送経路に沿った輸送を行うベクトル場を直接学習することを考えることもできる (Lipman et al., 2023)．\n当時，Denoising Score Matching が使える拡散模型と違い，CNF にはスケーラブルな訓練手法が存在しなかった．この拡散模型の加速の問題を，CNF の訓練の加速の問題と同時に解いたのが FM (Lipman et al., 2023) である．\nその発想の鍵は，\\(\\mathcal{P}(E)\\) 上の経路のみに注目することで，SDE と ODE を「輸送 \\((p_t)\\) を定めるフロー \\((\\phi_t)\\) を構成する道具」として相対化することであった．同様のアイデアは，同時期に確率的補間 (Michael Samuel Albergo and Vanden-Eijnden, 2023) と rectified flow (Liu et al., 2023) と２つ提出されている．\n\n\n2.2 フローマッチング (FM)\nFlow Matching (Lipman et al., 2023) のアイデアは，ノイズ分布とデータ分布を結ぶ輸送 \\(p_t:=(f_t)_*p_0\\) を定めるベクトル場 \\(F(x_t,t)\\) に関して， \\[\n\\overline{\\mathcal{L}}(\\theta):=\\lvert F_\\theta(X_\\tau,\\tau)-F(X_\\tau,\\tau)\\rvert^2,\\qquad \\tau\\sim\\mathrm{U}([0,T]),X_\\tau\\sim p_t(x_\\tau)\n\\] を目的関数とするというものである．\nすなわち，ベクトル場ネットワーク \\(F_\\theta\\) を準備し，目的の \\(F\\) に向かって最小二乗法を用いて訓練する．\nアイデアは大変シンプルであり，拡散模型とスコアマッチングのアナロジーに沿うものであるが，条件を満たす \\(F\\) は複数ある上，データ分布 \\(p_T\\) が不明であるために \\(F\\) の解析的な表示も不明である．\nこれは Monte Carlo 法により解決できる．\n\n\n2.3 条件付きフローマッチング (CFM)\nまずデータ分布 \\(p_T\\) を，経験分布 \\[\nq(y)\\,dy=\\frac{1}{n}\\sum_{i=1}^n\\delta_{x_i}\n\\] と Gauss 核の畳み込みで近似する： \\[\np_T(x)=\\int_{\\mathbb{R}^d} p_T(x|y)q(y)\\,dy,\\qquad p_T(-|y)=\\operatorname{N}_d(y,\\sigma^2I_d).\n\\]\nすると，各データ点 \\(y\\in\\{x_i\\}_{i=1}^n\\) に関して輸送 \\((p_t(x|y)\\,dx)_t\\) を \\[\np_T(-|y)=\\operatorname{N}_d(y,\\sigma^2I_d),\\qquad p_0(-|y)=\\operatorname{N}(0,I_d),\n\\tag{4}\\] を満たすように学習すれば，全体としてノイズ分布をデータ分布に輸送する道 \\((p_t)\\) が得られる．\nこれを，\\(p_T(-|y)\\) を生成する 条件付きベクトル場 (Conditional Vector Field) \\(F_t(-|y)\\) として学習し，最終的に次のように混合することで所望のベクトル場 \\(F_t\\) を得る (定理1 Lipman et al., 2023)： \\[\nF_t(x)=\\int_{\\mathbb{R}^d}F_t(x|y)\\frac{p_t(x|y)q(y)}{p_t(x)}\\,dy.\n\\]\nこのことに基づき，次の代理目標が得られる： \\[\n\\mathcal{L}(\\theta):=\\lvert F_\\theta(X_\\tau,\\tau)-F_t(X_\\tau|Y)\\rvert^2,\\qquad \\tau\\sim\\mathrm{U}([0,T]),Y\\sim q(y)\\,dy,X_\\tau\\sim p_\\tau(x|y)\\,dx.\n\\tag{5}\\]\n実は，単に代理目標となっているだけでなく，\\(\\overline{\\mathcal{L}}\\) と \\(\\mathcal{L}\\) の \\(\\theta\\) に関する勾配は一致する (定理2 Lipman et al., 2023)．\n条件付き変数 \\(y\\) が離散的でない場合もこれは成り立つ (定理3.1 Tong et al., 2024)．さらに，\\(p_0(-)\\) が Gauss でない場合にも CFM は拡張できる (Tong et al., 2024)．\n\n\n2.4 架橋の選択\n最後に，条件付き確率の (4) を満たす輸送 \\((p_t(-|y))\\) を，どのようにパラメータ付けして学習するかを考える．\n式 (4) の始点と終点が Gauss 分布であることを見れば，Gauss 空間内での輸送 \\[\np_t(x|y)\\,dx=\\operatorname{N}\\biggr(\\mu_t(y),\\sigma_t(y)^2I_d\\biggl),\\qquad t\\in[0,T],\n\\] が候補に上がる．ただし，\\(\\sigma_t\\) は単調減少とし， \\[\n(\\mu_0(y),\\mu_T(y))=(0,y),\\qquad(\\sigma_0(y),\\sigma_T(y))=(1,\\sigma_\\min).\n\\]\n\\(p_0(x|y)\\,dx=\\operatorname{N}_d(0,I_d)\\) をこのような分布に押し出す写像で最も簡単なものは \\[\n\\psi_t(x):=\\sigma_t(y)x+\\mu_t(y)\n\\tag{6}\\] である．したがって，フロー \\((\\psi_t)\\) を定めるベクトル場 \\[\nF_t(x|y):=\\frac{\\sigma_t'(y)}{\\sigma_t(y)}\\biggr(x-\\mu_t(y)\\biggl)+\\mu_t'(y)\n\\tag{7}\\] を目標として学習される．これが CFM (Conditional Flow Matching) (Lipman et al., 2023) である．\n\n\n\n\n\n\nフローマッチングの例としての拡散模型\n\n\n\n\n\n拡散模型は２種の SDE でデータ分布をノイズ分布に還元しているとみれる．\nDDPM が対応する分散保存過程の逆は，輸送 \\[\np_t(x|y)\\,dx=\\operatorname{N}(y,\\sigma^2_{T-t}I_d)\n\\] を定める．これは \\[\n\\mu_t(y)=y,\\qquad\\sigma_t(y)=\\sigma_{T-t},\n\\] の場合に当たる．\nSGM が対応する分散爆発過程の逆は，輸送 \\[\np_t(x|y)\\,dx=\\operatorname{N}\\biggr(\\alpha_{T-t}y,(1-\\alpha_{T-t}^2)I_d\\biggl)\n\\] を定める．これは \\[\n\\mu_t(y)=\\alpha_{T-t}y,\\qquad\\sigma_t(y)=\\sqrt{1-\\alpha_{T-t}^2},\n\\] の場合に当たる．\nこの見地から，拡散模型も，フローマッチングによりより効率的に訓練することができる．\n\n\n\n\n\n\n\n\n\n最適輸送になる場合\n\n\n\n\n\n\\(\\mu_t,\\sigma_t\\) を線型関数 \\[\n\\mu_t(y)=ty,\\qquad\\sigma_t(y)=1-(1-\\sigma_\\min)t,\n\\] に設定する．\nこの場合に対応するフロー \\[\n\\psi_t(x)=\\biggr(1-(1-\\sigma_\\min)t\\biggl)x+ty\n\\] は，最適輸送になっている (McCann, 1997, p. 159)．\n\n\n\n\n\n2.5 モデルから輸送へ\nFM の貢献は，所望のフロー \\((p_t(x|y))\\) に対して，ベクトル場 \\(F_t(x|y)\\) をどのように定めれば良いかの必要条件を与えたところにもあるが，何より生成モデリングの問題を，モデリングから輸送へ中心を据え変えた点が大きい．\nVAE と DDPM は確率モデルとして考案されたが，生成モデリングのために確率モデルを考える必要はなかったのである．1\n重要なのは，「事前分布 \\(p_0\\) をどのように \\(p_1\\) に移すか」という輸送の問題だけであり，我々はずっと確率モデルと尤度という偽の目標に囚われていたのである．\nGAN や VAE, NF は最尤推定が目標であった．拡散模型において，確率モデルの最尤推定の見方と輸送計画のスコア場を通じた学習としての見方の２つが出揃ったが，SGM はまだ輸送の問題を SDE の言葉で暗に捉えているのみであった．\nフローとその \\(\\mathcal{P}(E)\\) 上への押し出しが生成モデリングの本体であることがやっと明らかになったのは，拡散モデルと連続時間正規化流との関係が自覚されてからようやくのことである．2\n\n\n\n\n\n\n輸送問題3\n\n\n\n任意の２つの分布 \\(p_0,p_1\\in\\mathcal{P}(\\mathbb{R}^d)\\) から，これを結ぶ輸送 \\((p_t)_{t\\in[0,1]}\\) を定めるフロー \\(\\phi_t:\\mathbb{R}^d\\to\\mathbb{R}^d\\) \\[\n(\\phi_1)_*p_0=p_1\n\\] を学習することを考える．4\n\n\n\n\n2.6 ODE により輸送問題を解く\nFM, Rectified Flow, 確率的補間はいずれも，ODE を通じて輸送問題を解く．\n所望の輸送 \\((p_t)\\) があった場合，これを定めるフローを生成するベクトル場 \\(F_t\\) は次のように特定される：\n\n\n\n\n\n\n指針5\n\n\n\n\\((p_t)\\) はベクトル場 \\(F_t\\) に関する次の ODE を満たすとする： \\[\n\\frac{\\partial p_t}{\\partial t}+\\nabla\\cdot(F_tp_t)=0.\n\\] このとき，ベクトル場 \\(F_t\\) が定めるフロー \\((\\phi_t)\\) \\[\n\\frac{\\partial \\phi_t(x)}{\\partial t}=F_t(\\phi_t(x))\n\\] は，輸送 \\((p_t)\\) を定める： \\[\np_t=(\\phi_t)_*p_0.\n\\]\n\n\nこの ODE を 連続方程式 という．この連続方程式を解くベクトル場 \\(F_t\\) を学習することで，所望の輸送が学習できる．\n\n\n2.7 FM 再論\nFM では各データ点 \\(y\\in\\{x_i\\}_{i=1}^n\\) に関する条件付きベクトル場 \\(F_t(x|y)\\) を \\(y\\) に関して積分したもの \\(F_t(x)\\) が解の１つであることを特定した（(Lipman et al., 2023) 定理１，第 2.3 節）．\nこの \\(F_t(x|y)\\) を CFM 目的関数 (5) の最適化により学習することで \\(F_t(x)\\)，果てには \\((\\phi_t)\\) を学習することを目指した．\n\\(F_t(x|y)\\) の具体的な形に関してはほとんど留保しており，(7) の形の条件付きベクトル場が使えるとし，例を２つ挙げたのみである（第 2.4 節）．\nCFM は訓練可能な代理目標を定める非常に有用な方法であるが，条件付きベクトル場 \\(F_t(x|y)\\) を最適輸送に学習したからといって，最終的なベクトル場 \\(F_t(x)\\) が最適輸送を定めるとは限らない．これは条件付け変数 \\(y\\) を工夫して得る手法 OT-CFM (Tong et al., 2024) により乗り越えられる．\n以降，FM の例（とみなせる手法）を３つ見る：\n\n\n\n\n\n\n\nRectified Flow (Liu et al., 2023) は，ベクトル場でないダイナミクスを対象に FM 様の学習する．FM の理論から逸脱するが，その有用性は最適輸送の枠組みに支えられる．6\nIADB (Heitz et al., 2023) は FM 様の目的関数の他に，等価な非確率的アルゴリズムも提案している．学習される軌道は DDIM (J. Song et al., 2021) のものと一致する．\n確率的補間 (Michael Samuel Albergo and Vanden-Eijnden, 2023) では，ベクトル場ではなく，輸送計画 \\(I_t\\) を直接回帰するための，等価な二次の目的関数を提案している．7\n\n\n\n\n\n\n2.8 Rectified Flow\nRectified Flow (Liu et al., 2023) はこの最適輸送を定めるベクトル場 \\(F_\\theta(x_t,t)\\) を，なるべく直線で行けるところまで行くような，区分線形なダイナミクスとして学習することを目指す．\nこれは，\\(p_0,p_1\\) からのサンプリングが可能な状況下では，目的関数 \\[\n\\mathcal{L}(\\theta)=\\lvert(X_1-X_0)-F_\\theta(X_\\tau,\\tau)\\rvert^2,\n\\tag{8}\\] \\[\n\\tau\\sim\\mathrm{U}([0,1]),X_0\\sim p_0,X_1\\sim p_1,X_\\tau:=\\tau X_1+(1-\\tau)X_0,\n\\] を最小化するように学習することで達成できるという．\nこれは任意のサンプル \\(X_0,X_1\\) を線型に繋ぐようなダイナミクスを目標として誤差を最小化しているが，このようなダイナミクスは ODE が定めるものではない．実際，簡単に交差してしまい，ODE の解の一意性に違反する．\n従って recrified flow の有用性は実証的に認められなければならないが，(Liu et al., 2023) は FID と recall に関する SOTA を CIFAR-10 で達成している．\nこれは，rectified flow は繰り返すことができることによる．目的関数 (8) を最小化するベクトル場 \\(F(x,t)\\) について， \\[\ndZ_t:=F(Z_t,t)\\,dt,\\qquad Z_0\\sim p_0,\n\\] は \\(Z_1\\sim p_1\\) を満たす．次に \\((Z_0,Z_1)\\) に関して再び rectified flow を適用して得るダイナミクスは，元の \\(F\\) よりも直線的で (定理D.7 Liu et al., 2023, p. 26)，輸送コストが落ちたものになる (定理D.5 Liu et al., 2023, p. 24)．\n\n\n\n(Liu et al., 2023, p. 6) より．\n\n\n\\(X_t=tX_1+(1-t)X_0\\) という直線的なダイナミクスを変更して， \\[\nX_t=\\alpha_tX_1+\\beta_tX_0,\\qquad\\alpha_1=\\beta_0=1,\\alpha_0=\\beta_1=0,\n\\] という非線型なダイナミクスを考えた場合，これは probabilistic flow ODE (Y. Song et al., 2021) や DDIM (J. Song et al., 2021) に等価になる．\nRectified Flow は Stable Diffusion 3 のアーキテクチャ (Esser et al., 2024) に採用されており，従来の拡散モデルの方法より画像生成用途に優れていると結論付けている．8\n\n\n2.9 繰り返し \\(\\alpha\\)-ブレンディング (IADB)\nIADB (Iterative \\(\\alpha\\)-(de)Blending) (Heitz et al., 2023) は，どうやら直線にはならないようであるが，効率的な決定論的ダイナミクス \\((\\phi_t)\\) が，次の逐次サンプリングによって得られることを報告している：\n\n\n\n\n\n\n\\(N\\in\\mathbb{N}\\) を繰り返し数として，\\(n\\in[N]\\) に関して \\(\\alpha_n:=n/N\\) として次をループする：\n\nblending \\[X_{\\alpha_n}^{(n)}:=(1-\\alpha_n)X_0^{(n)}+\\alpha_n X_1^{(n)}.\\]\ndeblending \\[X_\\alpha^{(n)}=(1-\\alpha_n)X_0^{(n+1)}+\\alpha_n X_1^{(n+1)}\\] を満たすように \\[\nX_0^{(n+1)}\\sim p_0,\\qquad X_1^{(n+1)}\\sim p_1,\n\\] を取り直す．\n\\[X_{\\alpha_{n+1}}^{(n+1)}:=(1-\\alpha_{n+1})X_0^{(n+1)}+\\alpha_{n+1}X_1^{(n+1)}\\] として繰り返す．\n\n\n\n\n\\(N\\to\\infty\\) の極限で，\\((X_{\\alpha_n})_{n\\in[N]}\\) はある確定的なダイナミクスに収束する (定理 Heitz et al., 2023, p. 3)．\n\n\n\n(Fig.6 Heitz et al., 2023, p. 3) より．\n\n\n同時に，ニューラルネットワークによりこの軌道を訓練するための，Rectified Flow 様の目的関数 (8) も導入している．\n実は，この結果学習される軌道は，DDIM (J. Song et al., 2021) のものと一致するため，拡散モデルの決定論的な代替として機能する．\n\\(\\alpha\\)-ブレンド (Porter and Duff, 1984) の名前の通り，コンピュータグラフィクスへの応用も意識して議論されている．\n\n\n2.10 確率的補間\n(Michael Samuel Albergo and Vanden-Eijnden, 2023) により提案されたもので，SiT (Scalable Interpolant Transformer) (Ma et al., 2024) でも用いられている技術である．\nこの方法では \\(p_0,p_1\\) からのサンプル \\(x_0,x_1\\) に対する決定論的輸送 \\((I_t(x_0,x_1))\\) に注目し，ベクトル場 \\(F_t(x)\\) を \\[\n\\operatorname*{argmin}\\mathcal{L}(F):=\\operatorname*{argmin}\\operatorname{E}\\biggl[\\biggl|F_T(I_T(X_0,X_1))-2\\partial_tI_T(X_0,X_1)\\cdot F_T(I_T(X_0,X_1))\\biggr|\\biggr]\n\\] のただ一つの解として探索する．この解が輸送 \\((p_t)\\) を定めることが，連続方程式を通じて (命題1 Michael Samuel Albergo and Vanden-Eijnden, 2023, p. 4) で示されている2．\nこの方法では，目標 \\(p_1\\) と学習された輸送 \\((\\phi_1)_*p_0\\) との誤差を 2-Wasserstein 距離で測ることもできる．\nまた，学習されたダイナミクスは，ある Langevin 過程の時間変換に等しくなる (命題4 Michael Samuel Albergo and Vanden-Eijnden, 2023, p. 7)．\n\n\n2.11 軌道推定\nさて，輸送問題 2.5 は，確率過程 \\((X_t)_{t\\in[0,1]}\\) を，２つの時点 \\(X_0,X_1\\) に関する観測から推定する問題とも等価である．\nこうみると，拡散模型とフローマッチングの違いは，２つの \\(X_0,X_1\\) を繋ぐダイナミクス \\((X_t)_{t\\in[0,1]}\\) を，どのような帰納バイアスの下で推定するかの違いに他ならない．\nすると拡散模型は，これを Langevin 拡散により内挿する問題，そして FM をはじめとして Rectified Flow や \\(\\alpha\\)-ブレンディングは，これを線型などの単純なダイナミクスにより内挿する問題として理解できる．\n一般に，確率過程 \\((X_t)_{t\\in[0,1]}\\) の独立なコピー \\(X^1,X^2,\\cdots,X^n\\) から，ランダムな時点 \\(t\\in[0,1]\\) での観測 \\[\nX^1_{t_1},X^2_{t_2},\\cdots,X^n_{t_n},\\qquad\\{t_i\\}_{i=1}^n\\subset[0,1],\n\\] をもとに，\\((X_t)\\) を推定する問題を 軌道推定 (trajectory inference) (Lavenant et al., 2024) という．9\n作用マッチング (Action Matching) (Neklyudov et al., 2023) では，生成モデリングを軌道推定の問題として解く．\nただし，軌道はある作用 \\(s_t^*:\\mathbb{R}^d\\to\\mathbb{R}\\) に関する勾配ベクトル場 \\(\\nabla_xs_t^*\\) が定めるフローであると仮定した下で，この場を \\[\n\\widetilde{\\mathcal{L}}(\\theta)=\\frac{1}{2}\\operatorname{E}\\biggl[\\biggl|\\nabla s_T(X_T)-\\nabla s^*_T(X_T)\\biggr|\\biggr],\\qquad T\\sim\\mathrm{U}([0,1]),X_T\\sim p_T,\n\\] の最小化によって学習することを考える．\nこの \\(\\widetilde{\\mathcal{L}}\\) は真の場 \\(s^*\\) を含むが，次の目的関数と定数を除いて一致するため，次の目的関数により訓練可能である： \\[\n\\mathcal{L}(\\theta):=\\operatorname{E}\\biggl[s_0(X_0)-s_1(X_1)+\\frac{1}{2}\\lvert\\nabla s_T(X_T)\\rvert^2+\\partial_ts_T(x_T)\\biggr].\n\\]\n一方で Neural Lagrangian Schrödinger 橋 (Koshizuka and Sato, 2023) では，同様にラグランジアンの言葉で帰納バイアスを導入しながら，拡散過程のダイナミクスを学習する．"
  },
  {
    "objectID": "posts/2024/Samplers/NF1.html#文献紹介",
    "href": "posts/2024/Samplers/NF1.html#文献紹介",
    "title": "ニューラル常微分方程式",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n(Lettermann et al., 2024) は NODE に触れつつ，随伴感度法を用いた複雑系のモデリングとパラメータ推定の方法を解説したチュートリアルである．\n(Michael S. Albergo et al., 2023) は確率的補間の観点をさらに推し進め，CNF と Diffusion モデルを統一的な観点から提示している．\nCambridge MLG による An Introduction to Flow Matching の web ページ (Fjelde et al., 2024) も参照．\n軌道推定の見方は新しいようで古い．はじめ TrajectoryNet (Tong et al., 2020) という CNF 手法は軌道推定に用いられており，のちに OT-CFM として生成モデリングにも使えることが自覚されたのである．\nTrajectoryNet では，OT によりより直線的な軌道が学習されるような帰納バイアスを導入することが主眼であった．"
  },
  {
    "objectID": "posts/2024/Samplers/NF1.html#footnotes",
    "href": "posts/2024/Samplers/NF1.html#footnotes",
    "title": "ニューラル常微分方程式",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n生成モデルとしての VAE の学習された潜在変数が，何らかの現実を意味していると仮定して解釈を試みることはないだろう．↩︎\nフローをナイーブに確率モデルとしてみると無限層のニューラルネットワークと見る．これを打開した NODE のアイデアが，尤度原理という蒙昧の打開に必要であったのかもしれない．「拡散過程はサンプリングが遅い」というのは，この発想の転換の最後の離陸の段階であったのだろう．これは拡散過程の正確なシミュレーションが困難である一方で，区分確定的過程のシミュレーションが容易であることに対応する．↩︎\n(Liu et al., 2023) と (Michael Samuel Albergo and Vanden-Eijnden, 2023), (Heitz et al., 2023) の問題設定に従った．↩︎\n(Liu et al., 2023) はこのような輸送の問題として，GAN や VAE をはじめとした生成モデリングと，ドメイン転移の問題をみた．↩︎\n(Michael Samuel Albergo and Vanden-Eijnden, 2023, p. 2) など．正確なステートメントは，(Ambrosio et al., 2008, p. 183) 定理8.3.1 など参照．↩︎\nさらに，\\(\\sigma\\to0\\) の極限をとっており，CFM がまだ確率的であるのに対して，Rectified Flow では \\(p_t(-|y)\\) は完全に Delta 分布になる．↩︎\nさらに，Rectified Flow が直線を考えているのに対して，\\(I_t(x_0,x_1):=\\cos(\\pi t/2)x_0+\\sin(\\pi t/2)x_1\\) という回転様の補間を考えている．↩︎\nDDPM (Ho et al., 2020) と同様，正確な訓練目標ではなく，困難なデノイジングでの成功を強調する uniform reweighting した訓練目標を用いている点に注意．↩︎\n(Hashimoto et al., 2016), (Koshizuka and Sato, 2023) などは scRNA-seq データへの応用を念頭に population dynamics と呼んでいる．古典的な横断面データ (cross-sectional data) の設定に似ている．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/Sampling.html",
    "href": "posts/2024/Samplers/Sampling.html",
    "title": "サンプリングとは何か",
    "section": "",
    "text": "計算機の登場が人類にもたらしたものは，計算手 (computer) を代替する圧倒的計算資源だけではない．\n1946 年に Los Alamos でお披露目された最初期の計算機 ENIAC の登場により，科学者たちは「一様乱数をシミュレーションする力」を手に入れた．\nこの力は何を意味するだろうか？\n直ちにこの「力」の破壊的威力を理解したのは Stanislaw Ulam とその話を聴いた John von Neumann であったという．1\nその後，この「力」は Monte Carlo 法の名前を得て，(Nicholas Metropolis and Ulam, 1949) では Boltzmann 方程式や Schrödinger 方程式をシミュレーションすることに応用され，最終的に Los Alamos での研究成果は原爆として結実した．\nこの衝撃的な登場をした「力」は，その後平和が訪れた世界において，どのように使われているのであろうか？\n本章では，Monte Carlo 法はその枠にとどまらず，「生成」「シミュレーション」「サンプリング」など種々の名前で，現代の生活を下支えしていると論じる．\n\n\n\nGAN，VAE，正規化流，拡散模型 などは，いずれも潜在変数モデルを学習していることに等しい．\n\n\n\nMissing Data Model / Latent State Model / Completed Model for \\(X\\)\n\n\n数学的には \\(Z\\to X\\) とは 確率核 であり，Markov 圏 における射とも見れる．\n\n\n欠測データモデル，潜在変数モデルは全て 階層モデル の特別な場合とみなすことができる（Concept with an attitude）．そこで本稿では全て「階層モデル」と呼んでしまうこととする．\n階層モデルには，これがひとたび推定されたのちは，そのモデルからデータをサンプリングすることが出来るという特徴がある．2\n\n\n\nベイズ統計学の文脈では，こうして得たデータをモデルの正確さの検証に用いることが出来る点が，生成モデルの美点としてみなされている．3\nこれはベイズ統計学では，階層モデルは物理的過程や社会的過程や因果など，何らかの現実を抽象化したものとみなされるためである．\n\n\n\nfrom McElreath’s Statistical Rethinking 2023 Lecture\n\n\n\n\n\n一方で生成モデルの文脈では，純粋に「データ分布からの複製」を目標とするため，階層モデルはモデリングの道具ではなく，ひとえに正確なデータ分布を学習するための装置とみなされる．\nこのように，統計と機械学習とでは階層モデルに対する態度と姿勢が違うのみで，生成モデルとベイズモデリングは数理的には同じ営為であると言える．\nこのことは表現学習の初期から意識されていたことであり (Kersten et al., 2004), (Yuille and Kersten, 2006)，実際データの背後にある因果構造を正しく特定できている形の階層モデルを推定した際は，その階層的な構造がデータの背後にある因果的な変動要因をうまく分解して陽の下に晒してくれるはずである (Chen et al., 2017)．\n\n\n階層モデルの使い方の違い\n\n\n\n\n\n\n\n\nベイズ統計\n生成モデル\n\n\n\n\n目標\n正しいモデルを得る\n高精度の生成をする\n\n\n生成\nおまけ\n主目的\n\n\n価値観\n不偏性は嬉しい\n精度が重要\n\n\n推定法\nMCMC などの Monte Carlo 法\n変分推論などの最適化\n\n\n\n\n\n\n\n\n\n\n現代の科学技術のミトコンドリア・イブは Newton 力学だとする言説に，異論は少ないだろう．\nNewton 力学から始まる全ての物理学における理論は，観測データを説明するために作られた．\n統計モデルのように尤度を最適化するわけではないが，最もデータをよく説明する模型のみが残ってきたのは物理学も同じである．\nNewton 力学の大きなモチベーションに Kepler の三法則の説明があり，この法則は Tycho Brahe の観測から見出されたものである．\n\nティコ・ブラーエは彼のためにオラネンブルグに建てられたよく整備された「天文台」の中で，同じ一様な製図の上に惑星の位置を書き記すだけでなく，前もって印刷した同じ星図の上に惑星の位置を書き記すだけでなく，前もって印刷した同じ星図をヨーロッパ中の天文学者に送ってそこへ書き記すように求めて，彼らからの観測結果をも集めた． 彼（ティコ・ブラーエのこと）の精神が突然に変化を遂げたのでは無い．彼の目が突然に古い偏見から自由になるのでは無い．以前の誰よりも夏の空を注意深く観測している訳でも無い．彼は，夏の空＋自分の観測＋同僚の観測＋コペルニクスの書物＋プトレマイオスの『アルマゲスト』の多くの版とを一望のもとに見て考えた最初の人物である．長いネットワークの始点と終点に座り，不変で結合可能な可動物を私が呼ぶものを生み出した最初の人物である．(ブルーノ・ラトゥール, 1999)\n\n\n\n\nまもなくして実験が物理学を駆動する方法論となり，長い間物理学においては，実験と理論の２者が相補的な関係にあった．\n\nIf it disagrees with experiment, it’s wrong. In that simple statement, is the key to science. (Feynman, 1964)\n\nだが，多くの物理理論は，特定の「この数値を測定すれば良い」というようなところまで提供し，実験はこれを確認する役割を果たすことが多かった．\nこのようなスキームでは，実験から集中される観察データ自体が複雑な構造を持つということは稀だったと言えるだろう．\n\n\n\nしかし，統計力学の扱う対象などは，例えば Ising 模型など，環境を整えて実験的に再現するということは難しい．その結果，計算機内でのシミュレーションが実験の代わりを果たすようになっている．３次元の Ising 模型の臨界指数の conformal bootstrap による計算 (El-Showk et al., 2012) などはその良い例である．\n\nThe computer becomes the virtual laboratory in which a system is studied - a numerical experiment. (Rapaport, 2004, p. 3)\n\nまた，天体観測の時代とは違い，データ自体が複雑な構造を持つ一つの宇宙のような様相を呈しているのが計算機時代である．例えば，高次元における測度の集中の現象 (Carlen et al., 2017) が発見されたように，高次元データにおける多様体仮説も広く観察されている (本武陽一, 2017)．\n現代では，そのようなデータを料理すること自体が一つの重要な実験技法と言えるかもしれない．\n\n\n\n「数値実験」「シミュレーション」という新たな「実験」技術が，真の意味で社会科学を「科学」たらしめるかもしれない．\n\nSocial science is an example of a science which is not a science.Feynman on the social sciences\n\n結局，このような「社会科学は科学か？」という論争も，自然科学の理論の実験による検証よりも，社会科学の理論の実験による検証の方が，はるかに実験技術の発達を必要とした，というだけに過ぎないかもしれない．\n\n\n\n大規模言語モデル（LLM）は，物理学的なシミュレーションと相補的な世界モデルを提供しつつある．産業界では，初めて言語を生成できるモデルとして，会話やディスカッションのシミュレーションツールとしても使われている．\n加えて，計算科学の意味での数値実験によるシミュレーションだけでなく，生成モデルを汎用物理シミュレーターとして用いる考え方が，Sora (Brooks et al., 2024) の出現以降広まった．\n実際，動画の生成・自動運転のシミュレーション・自律的エージェントのシミュレーションなどは，物理法則のシミュレーションとタスクとして一致するところが多い (Zhu et al., 2024)．"
  },
  {
    "objectID": "posts/2024/Samplers/Sampling.html#サンプリングとは何か",
    "href": "posts/2024/Samplers/Sampling.html#サンプリングとは何か",
    "title": "サンプリングとは何か",
    "section": "",
    "text": "計算機の登場が人類にもたらしたものは，計算手 (computer) を代替する圧倒的計算資源だけではない．\n1946 年に Los Alamos でお披露目された最初期の計算機 ENIAC の登場により，科学者たちは「一様乱数をシミュレーションする力」を手に入れた．\nこの力は何を意味するだろうか？\n直ちにこの「力」の破壊的威力を理解したのは Stanislaw Ulam とその話を聴いた John von Neumann であったという．1\nその後，この「力」は Monte Carlo 法の名前を得て，(Nicholas Metropolis and Ulam, 1949) では Boltzmann 方程式や Schrödinger 方程式をシミュレーションすることに応用され，最終的に Los Alamos での研究成果は原爆として結実した．\nこの衝撃的な登場をした「力」は，その後平和が訪れた世界において，どのように使われているのであろうか？\n本章では，Monte Carlo 法はその枠にとどまらず，「生成」「シミュレーション」「サンプリング」など種々の名前で，現代の生活を下支えしていると論じる．\n\n\n\nGAN，VAE，正規化流，拡散模型 などは，いずれも潜在変数モデルを学習していることに等しい．\n\n\n\nMissing Data Model / Latent State Model / Completed Model for \\(X\\)\n\n\n数学的には \\(Z\\to X\\) とは 確率核 であり，Markov 圏 における射とも見れる．\n\n\n欠測データモデル，潜在変数モデルは全て 階層モデル の特別な場合とみなすことができる（Concept with an attitude）．そこで本稿では全て「階層モデル」と呼んでしまうこととする．\n階層モデルには，これがひとたび推定されたのちは，そのモデルからデータをサンプリングすることが出来るという特徴がある．2\n\n\n\nベイズ統計学の文脈では，こうして得たデータをモデルの正確さの検証に用いることが出来る点が，生成モデルの美点としてみなされている．3\nこれはベイズ統計学では，階層モデルは物理的過程や社会的過程や因果など，何らかの現実を抽象化したものとみなされるためである．\n\n\n\nfrom McElreath’s Statistical Rethinking 2023 Lecture\n\n\n\n\n\n一方で生成モデルの文脈では，純粋に「データ分布からの複製」を目標とするため，階層モデルはモデリングの道具ではなく，ひとえに正確なデータ分布を学習するための装置とみなされる．\nこのように，統計と機械学習とでは階層モデルに対する態度と姿勢が違うのみで，生成モデルとベイズモデリングは数理的には同じ営為であると言える．\nこのことは表現学習の初期から意識されていたことであり (Kersten et al., 2004), (Yuille and Kersten, 2006)，実際データの背後にある因果構造を正しく特定できている形の階層モデルを推定した際は，その階層的な構造がデータの背後にある因果的な変動要因をうまく分解して陽の下に晒してくれるはずである (Chen et al., 2017)．\n\n\n階層モデルの使い方の違い\n\n\n\n\n\n\n\n\nベイズ統計\n生成モデル\n\n\n\n\n目標\n正しいモデルを得る\n高精度の生成をする\n\n\n生成\nおまけ\n主目的\n\n\n価値観\n不偏性は嬉しい\n精度が重要\n\n\n推定法\nMCMC などの Monte Carlo 法\n変分推論などの最適化\n\n\n\n\n\n\n\n\n\n\n現代の科学技術のミトコンドリア・イブは Newton 力学だとする言説に，異論は少ないだろう．\nNewton 力学から始まる全ての物理学における理論は，観測データを説明するために作られた．\n統計モデルのように尤度を最適化するわけではないが，最もデータをよく説明する模型のみが残ってきたのは物理学も同じである．\nNewton 力学の大きなモチベーションに Kepler の三法則の説明があり，この法則は Tycho Brahe の観測から見出されたものである．\n\nティコ・ブラーエは彼のためにオラネンブルグに建てられたよく整備された「天文台」の中で，同じ一様な製図の上に惑星の位置を書き記すだけでなく，前もって印刷した同じ星図の上に惑星の位置を書き記すだけでなく，前もって印刷した同じ星図をヨーロッパ中の天文学者に送ってそこへ書き記すように求めて，彼らからの観測結果をも集めた． 彼（ティコ・ブラーエのこと）の精神が突然に変化を遂げたのでは無い．彼の目が突然に古い偏見から自由になるのでは無い．以前の誰よりも夏の空を注意深く観測している訳でも無い．彼は，夏の空＋自分の観測＋同僚の観測＋コペルニクスの書物＋プトレマイオスの『アルマゲスト』の多くの版とを一望のもとに見て考えた最初の人物である．長いネットワークの始点と終点に座り，不変で結合可能な可動物を私が呼ぶものを生み出した最初の人物である．(ブルーノ・ラトゥール, 1999)\n\n\n\n\nまもなくして実験が物理学を駆動する方法論となり，長い間物理学においては，実験と理論の２者が相補的な関係にあった．\n\nIf it disagrees with experiment, it’s wrong. In that simple statement, is the key to science. (Feynman, 1964)\n\nだが，多くの物理理論は，特定の「この数値を測定すれば良い」というようなところまで提供し，実験はこれを確認する役割を果たすことが多かった．\nこのようなスキームでは，実験から集中される観察データ自体が複雑な構造を持つということは稀だったと言えるだろう．\n\n\n\nしかし，統計力学の扱う対象などは，例えば Ising 模型など，環境を整えて実験的に再現するということは難しい．その結果，計算機内でのシミュレーションが実験の代わりを果たすようになっている．３次元の Ising 模型の臨界指数の conformal bootstrap による計算 (El-Showk et al., 2012) などはその良い例である．\n\nThe computer becomes the virtual laboratory in which a system is studied - a numerical experiment. (Rapaport, 2004, p. 3)\n\nまた，天体観測の時代とは違い，データ自体が複雑な構造を持つ一つの宇宙のような様相を呈しているのが計算機時代である．例えば，高次元における測度の集中の現象 (Carlen et al., 2017) が発見されたように，高次元データにおける多様体仮説も広く観察されている (本武陽一, 2017)．\n現代では，そのようなデータを料理すること自体が一つの重要な実験技法と言えるかもしれない．\n\n\n\n「数値実験」「シミュレーション」という新たな「実験」技術が，真の意味で社会科学を「科学」たらしめるかもしれない．\n\nSocial science is an example of a science which is not a science.Feynman on the social sciences\n\n結局，このような「社会科学は科学か？」という論争も，自然科学の理論の実験による検証よりも，社会科学の理論の実験による検証の方が，はるかに実験技術の発達を必要とした，というだけに過ぎないかもしれない．\n\n\n\n大規模言語モデル（LLM）は，物理学的なシミュレーションと相補的な世界モデルを提供しつつある．産業界では，初めて言語を生成できるモデルとして，会話やディスカッションのシミュレーションツールとしても使われている．\n加えて，計算科学の意味での数値実験によるシミュレーションだけでなく，生成モデルを汎用物理シミュレーターとして用いる考え方が，Sora (Brooks et al., 2024) の出現以降広まった．\n実際，動画の生成・自動運転のシミュレーション・自律的エージェントのシミュレーションなどは，物理法則のシミュレーションとタスクとして一致するところが多い (Zhu et al., 2024)．"
  },
  {
    "objectID": "posts/2024/Samplers/Sampling.html#サンプリングと最適化の双対性",
    "href": "posts/2024/Samplers/Sampling.html#サンプリングと最適化の双対性",
    "title": "サンプリングとは何か",
    "section": "2 サンプリングと最適化の双対性",
    "text": "2 サンプリングと最適化の双対性\n「生成」「シミュレーション」などの「隠れたサンプリング」が多いのと同様，隠れた最適化も多い．ほとんどの物理法則は変分法的解釈を持つ．\n\n2.1 サンプリングによる最適化\n統計力学的な系は自由エネルギー最小化原理に従って基底状態に緩和する．\nこのことにより，最適化にサンプリングが利用できる．模擬アニーリング (Kirkpartick et al., 1983) などはその例と言えよう．\n一般に，サンプリングが勾配フリーの最適化ソルバーとして利用可能であることが理解されてきている (Andrieu et al., 2024)．\n\n\n2.2 最適化によるサンプリング\n生成モデルは，その生成の精度が重視されるために，多くの場合は最適化によってベイズ推論問題が解かれる．"
  },
  {
    "objectID": "posts/2024/Samplers/Sampling.html#あとがき",
    "href": "posts/2024/Samplers/Sampling.html#あとがき",
    "title": "サンプリングとは何か",
    "section": "3 あとがき",
    "text": "3 あとがき\n\n本稿は，下掲の稿からの思索の発展を記したものである：\n\n    \n        \n            \n            \n                計算とは何か\n                計算とサンプリングのはざまにある Monte Carlo 法\n            \n        \n    \n\n概要で提起した問題意識\n\n初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？\n\nへの回答は「原爆」であると同時に，「平和」でもあると応えたい．そのような研究がしていきたいと思う．"
  },
  {
    "objectID": "posts/2024/Samplers/Sampling.html#footnotes",
    "href": "posts/2024/Samplers/Sampling.html#footnotes",
    "title": "サンプリングとは何か",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n当時の歴史は (N. Metropolis, 1987) に詳しい．↩︎\n圏論的には，射の列を終対象から辿ることにあたる：\\(\\{*\\}\\to Z\\to X\\)．↩︎\n特に Richard McElreath の統計観に強く現れている．↩︎"
  },
  {
    "objectID": "posts/2024/Samplers/DiscreteDiffusion.html",
    "href": "posts/2024/Samplers/DiscreteDiffusion.html",
    "title": "離散空間上のフローベース模型",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/DiscreteDiffusion.html#sec-D3PM",
    "href": "posts/2024/Samplers/DiscreteDiffusion.html#sec-D3PM",
    "title": "離散空間上のフローベース模型",
    "section": "1 離散雑音除去拡散模型 (D3PM) (Austin et al., 2021)",
    "text": "1 離散雑音除去拡散模型 (D3PM) (Austin et al., 2021)\n\n\n\nMinimal Implementation of a D3PM by Simo Ryu (Ryu, 2024) (Tap to image to visit his repository)\n\n\n\n1.1 はじめに\n離散データ上のフローベースのサンプリング法として，Argmax Flows と Multinomial Diffusion が (Hoogeboom et al., 2021) により提案された．\nD3PM (Austin et al., 2021) はこの拡張として提案されたものである．\nその結果，D3PM は BERT (Lewis et al., 2020) などのマスク付き言語モデルと等価になる．\n\n\n1.2 ノイズ過程\n\n1.2.1 設計意図\n効率的な訓練のために，\n\n\\(q(x_t|x_0)\\) からシミュレーション可能\n\\(q(x_{t-1}|x_t,x_0)\\) が評価可能\n\nであるとする．これにより， \\[\nL_{t-1}(x_0):=\\int_\\mathcal{X}\\operatorname{KL}\\biggr(q(x_{t-1}|x_t,x_0),p_\\theta(x_{t-1}|x_t)\\biggl)\\,q(x_t|x_0)\\,dx_t\n\\]\nの Monte Carlo 近似が可能になる．\n\\(p(x_T)=q(x_T|x_0)\\) を一様分布など，簡単にシミュレーション可能な分布とする．\n\n\n1.2.2 実装\n\\(x_0\\in\\mathcal{X}\\) は，\\([K]\\)-値の離散ベクトル \\(x_0^{(i)}\\) が \\(D\\) 個集まったものとする．ただし，\\(x_0^{(i)}\\) は one-hot encoding による横ベクトルとする．\nすると，ある確率行列 \\(Q_t\\) に関して， \\[\nQ(-|x_{t-1})=x_{t-1}Q_t=\\cdots=x_0Q_1\\cdots Q_t\n\\] と表せる．右辺の第 \\(i\\) 行は，次 \\(k\\in[K]\\) の状態に至る確率を表す確率ベクトルとなっている．\nするとこの逆は，ベイズの定理より \\[\nq(x_{t-1}|x_t,x_0)=\\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\n\\] \\[\nQ(-|x_t,x_0)=\n\\]\n\n\n1.2.3 核 \\(Q\\) の取り方\n\\[\nQ_t:=(1-\\beta_t)I_K+\\frac{\\beta_t}{K}\n\\] と取った場合を一様核という．\nまたは，\\(Q_t\\) として 脱落核 を取ることもできる．これは１つの点 \\(m\\in[K]\\) を吸収点とする方法である： \\[\n(Q_t)_{ij}:=\\begin{cases}1&i=j=m,\\\\\n1-\\beta_t&i=j\\in[K]\\setminus\\{m\\}\\\\\n\\beta_t&\\mathrm{otherwise}\n\\end{cases}\n\\]\n\n\n\n1.3 除去過程\n\\(p_\\theta(x_{t-1}|x_t)\\) をモデリングするのではなく，\\(\\widetilde{p}_\\theta(x_0|x_t)\\) をモデリングし， \\[\np_\\theta(x_{t-1}|x_t)\\,\\propto\\,\\sum_{\\widetilde{x}_0\\in[K]}q(x_{t-1}|x_t,\\widetilde{x}_0)\\widetilde{p}_\\theta(\\widetilde{x}_0|x_t)\n\\] は間接的にモデリングする．\nこれにより，ステップ数を小さく取った場合でも，\\(k\\) ステップをまとめて \\(p_\\theta(x_{t-k}|x_t)\\) をいきなりサンプリングするということも十分に可能になるためである．\n\n\n1.4 BERT (Devlin et al., 2019) との対応\n\\(Q_t\\) として，一様核と脱落核を重ね合わせたとする．\nすなわち，各トークンを各ステップで \\(\\alpha=10\\%\\) でマスクし，\\(\\beta=5\\%\\) で一様にリサンプリングし，これを元に戻す逆過程を学習する．\nこれは BERT (Devlin et al., 2019) と全く同じ目的関数を定める．\nMaskGIT (Masked Generative Image Transformer) (Chang et al., 2022) も，画像をベクトル量子化した後に，全く同様の要領でマスク・リサンプリングをし，これを回復しようとする．これはトランスフォーマーなどの自己回帰的モデルを用いて逐次的に生成するより，サンプリングがはるかに速くなるという．"
  },
  {
    "objectID": "posts/2024/Samplers/DiscreteDiffusion.html#参考文献",
    "href": "posts/2024/Samplers/DiscreteDiffusion.html#参考文献",
    "title": "離散空間上のフローベース模型",
    "section": "2 参考文献",
    "text": "2 参考文献\n\n(Ryu, 2024) に素晴らしい教育的リポジトリがある．D3PM の 425 行での PyTorch での実装を提供している．\n(Campbell et al., 2024) は最新の論文の一つである．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html",
    "href": "posts/2024/Samplers/DDPM.html",
    "title": "拡散模型の実装",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#ハイパーパラメーターの設定",
    "href": "posts/2024/Samplers/DDPM.html#ハイパーパラメーターの設定",
    "title": "拡散模型の実装",
    "section": "1 ハイパーパラメーターの設定",
    "text": "1 ハイパーパラメーターの設定\n\\(\\beta_0=10^{-4}\\) から \\(\\beta_T=0.02\\) までを，n_timesteps = 1000 等分し，その間のダイナミクスを hidden_dim = 256 次元の CNN ８層で学習する．\n\n\n必要なパッケージの読み込み\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image, make_grid\nfrom tqdm import tqdm\nfrom torch.optim import Adam\n\nimport math\n\ndataset_path = '~/hirofumi/datasets'\n\n\n\nDEVICE = torch.device(\"mps\")  # MacOS 上で実行しました\n\ndataset = 'MNIST'\nimg_size = (32, 32, 3)   if dataset == \"CIFAR10\" else (28, 28, 1) # (width, height, channels)\n\ntimestep_embedding_dim = 256\nn_layers = 8\nhidden_dim = 256\nn_timesteps = 1000\nbeta_minmax=[1e-4, 2e-2]\n\ntrain_batch_size = 128\ninference_batch_size = 64\nlr = 5e-5\nepochs = 100\n\nseed = 1234\n\nhidden_dims = [hidden_dim for _ in range(n_layers)]\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n\n\nデータセットの読み込み\nfrom torchvision.datasets import MNIST, CIFAR10\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n])\n\nkwargs = {'num_workers': 0, 'pin_memory': True}  # 今回は軽量だし worker number は 0 にする\n\nif dataset == 'CIFAR10':\n    train_dataset = CIFAR10(dataset_path, transform=transform, train=True, download=True)\n    test_dataset  = CIFAR10(dataset_path, transform=transform, train=False, download=True)\nelse:\n    train_dataset = MNIST(dataset_path, transform=transform, train=True, download=True)\n    test_dataset  = MNIST(dataset_path, transform=transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False,  **kwargs)"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#モデル定義",
    "href": "posts/2024/Samplers/DDPM.html#モデル定義",
    "title": "拡散模型の実装",
    "section": "2 モデル定義",
    "text": "2 モデル定義\n\n2.1 タイムステップ \\(t\\) の位置埋め込み\n(Ho et al., 2020) ではトランスフォーマー (Vaswani et al., 2017) 同様の sinusoidal positional encoding を用いて timestep_embedding_dim = 256 次元の潜在表現を得て，タイムステップ \\(t\\) の情報をデータに統合する．\nそのタイムステップが統合されたデータが，同一に CNN に与えられ，その CNN のパラメータが学習される．\nこうすることで n_timesteps = 1000 の別々の NN を訓練するより遥かに効率的な学習が可能である．\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\n\n\n2.2 ニューラルネットワークの構成\n(Ho et al., 2020) では U-Net (Ronneberger et al., 2015) アーキテクチャを用いているが，ここでは同じ次元の CNN を n_layers=8 層重ねて作ることとする．\n\nclass ConvBlock(nn.Conv2d):\n    \"\"\"\n        Conv2D Block\n            Args:\n                x: (N, C_in, H, W)\n            Returns:\n                y: (N, C_out, H, W)\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, activation_fn=None, drop_rate=0.,\n                    stride=1, padding='same', dilation=1, groups=1, bias=True, gn=False, gn_groups=8):\n        \n        if padding == 'same':\n            padding = kernel_size // 2 * dilation\n\n        super(ConvBlock, self).__init__(in_channels, out_channels, kernel_size,\n                                            stride=stride, padding=padding, dilation=dilation,\n                                            groups=groups, bias=bias)\n\n        self.activation_fn = nn.SiLU() if activation_fn else None\n        self.group_norm = nn.GroupNorm(gn_groups, out_channels) if gn else None\n        \n    def forward(self, x, time_embedding=None, residual=False):\n        \n        if residual:\n            # in the paper, diffusion timestep embedding was only applied to residual blocks of U-Net\n            x = x + time_embedding\n            y = x\n            x = super(ConvBlock, self).forward(x)\n            y = y + x\n        else:\n            y = super(ConvBlock, self).forward(x)\n        y = self.group_norm(y) if self.group_norm is not None else y\n        y = self.activation_fn(y) if self.activation_fn is not None else y\n        \n        return y\n\n\n\n2.3 デコーダーの定義\n\nclass Denoiser(nn.Module):\n    \n    def __init__(self, image_resolution, hidden_dims=[256, 256], diffusion_time_embedding_dim = 256, n_times=1000):\n        super(Denoiser, self).__init__()\n        \n        _, _, img_C = image_resolution\n        \n        self.time_embedding = SinusoidalPosEmb(diffusion_time_embedding_dim)\n        \n        self.in_project = ConvBlock(img_C, hidden_dims[0], kernel_size=7)\n        \n        self.time_project = nn.Sequential(ConvBlock(diffusion_time_embedding_dim, hidden_dims[0], kernel_size=1, activation_fn=True),ConvBlock(hidden_dims[0], hidden_dims[0], kernel_size=1))\n        \n        self.convs = nn.ModuleList([ConvBlock(in_channels=hidden_dims[0], out_channels=hidden_dims[0], kernel_size=3)])\n        \n        for idx in range(1, len(hidden_dims)):\n            self.convs.append(ConvBlock(hidden_dims[idx-1], hidden_dims[idx], kernel_size=3, dilation=3**((idx-1)//2),activation_fn=True, gn=True, gn_groups=8))                                \n\n        self.out_project = ConvBlock(hidden_dims[-1], out_channels=img_C, kernel_size=3)\n        \n        \n    def forward(self, perturbed_x, diffusion_timestep):\n        y = perturbed_x\n        \n        diffusion_embedding = self.time_embedding(diffusion_timestep)\n        diffusion_embedding = self.time_project(diffusion_embedding.unsqueeze(-1).unsqueeze(-2))\n        \n        y = self.in_project(y)\n        \n        for i in range(len(self.convs)):\n            y = self.convs[i](y, diffusion_embedding, residual = True)\n            \n        y = self.out_project(y)\n            \n        return y\n    \nmodel = Denoiser(image_resolution=img_size,\n                 hidden_dims=hidden_dims, \n                 diffusion_time_embedding_dim=timestep_embedding_dim, \n                 n_times=n_timesteps).to(DEVICE)\n\n\n\n2.4 エンコーダーの定義\n\nclass Diffusion(nn.Module):\n    def __init__(self, model, image_resolution=[32, 32, 3], n_times=1000, beta_minmax=[1e-4, 2e-2], device='cuda'):\n    \n        super(Diffusion, self).__init__()\n    \n        self.n_times = n_times\n        self.img_H, self.img_W, self.img_C = image_resolution\n\n        self.model = model\n        \n        # define linear variance schedule(betas)\n        beta_1, beta_T = beta_minmax\n        betas = torch.linspace(start=beta_1, end=beta_T, steps=n_times).to(device) # follows DDPM paper\n        self.sqrt_betas = torch.sqrt(betas)\n                                     \n        # define alpha for forward diffusion kernel\n        self.alphas = 1 - betas\n        self.sqrt_alphas = torch.sqrt(self.alphas)\n        alpha_bars = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n        \n        self.device = device\n    \n    def extract(self, a, t, x_shape):\n        \"\"\"\n            from lucidrains' implementation\n                https://github.com/lucidrains/denoising-diffusion-pytorch/blob/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L376\n        \"\"\"\n        b, *_ = t.shape\n        out = a.gather(-1, t)\n        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n    \n    def scale_to_minus_one_to_one(self, x):\n        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n        return x * 2 - 1\n    \n    def reverse_scale_to_zero_to_one(self, x):\n        return (x + 1) * 0.5\n    \n    def make_noisy(self, x_zeros, t): \n        # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n        epsilon = torch.randn_like(x_zeros).to(self.device)\n        \n        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars, t, x_zeros.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, t, x_zeros.shape)\n        \n        # Let's make noisy sample!: i.e., Forward process with fixed variance schedule\n        #      i.e., sqrt(alpha_bar_t) * x_zero + sqrt(1-alpha_bar_t) * epsilon\n        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n    \n        return noisy_sample.detach(), epsilon\n    \n    \n    def forward(self, x_zeros):\n        x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n        \n        B, _, _, _ = x_zeros.shape\n        \n        # (1) randomly choose diffusion time-step\n        t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(self.device)\n        \n        # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n        perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n        \n        # (3) predict epsilon(noise) given perturbed data at diffusion-timestep t.\n        pred_epsilon = self.model(perturbed_images, t)\n        \n        return perturbed_images, epsilon, pred_epsilon\n    \n    \n    def denoise_at_t(self, x_t, timestep, t):\n        B, _, _, _ = x_t.shape\n        if t &gt; 1:\n            z = torch.randn_like(x_t).to(self.device)\n        else:\n            z = torch.zeros_like(x_t).to(self.device)\n        \n        # at inference, we use predicted noise(epsilon) to restore perturbed data sample.\n        epsilon_pred = self.model(x_t, timestep)\n        \n        alpha = self.extract(self.alphas, timestep, x_t.shape)\n        sqrt_alpha = self.extract(self.sqrt_alphas, timestep, x_t.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, timestep, x_t.shape)\n        sqrt_beta = self.extract(self.sqrt_betas, timestep, x_t.shape)\n        \n        # denoise at time t, utilizing predicted noise\n        x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n        \n        return x_t_minus_1.clamp(-1., 1)\n                \n    def sample(self, N):\n        # start from random noise vector, x_0 (for simplicity, x_T declared as x_t instead of x_T)\n        x_t = torch.randn((N, self.img_C, self.img_H, self.img_W)).to(self.device)\n        \n        # autoregressively denoise from x_T to x_0\n        #     i.e., generate image from noise, x_T\n        for t in range(self.n_times-1, -1, -1):\n            timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(self.device)\n            x_t = self.denoise_at_t(x_t, timestep, t)\n        \n        # denormalize x_0 into 0 ~ 1 ranged values.\n        x_0 = self.reverse_scale_to_zero_to_one(x_t)\n        \n        return x_0\n    \n    \ndiffusion = Diffusion(model, image_resolution=img_size, n_times=n_timesteps, \n                      beta_minmax=beta_minmax, device=DEVICE).to(DEVICE)\n\noptimizer = Adam(diffusion.parameters(), lr=lr)\ndenoising_loss = nn.MSELoss()\n\n\n\n2.5 エンコーディングの様子\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"Number of model parameters: \", count_parameters(diffusion))\n\nNumber of model parameters:  4870913\n\n\n\nmodel.eval()\nfor batch_idx, (x, _) in enumerate(test_loader):\n    x = x.to(DEVICE)\n    perturbed_images, epsilon, pred_epsilon = diffusion(x)\n    perturbed_images = diffusion.reverse_scale_to_zero_to_one(perturbed_images)\n    break\n\ndef show_image(x, idx):\n    fig = plt.figure()\n    plt.imshow(x[idx].transpose(0, 1).transpose(1, 2).detach().cpu().numpy())\n\nshow_image(perturbed_images, idx=0)\nshow_image(perturbed_images, idx=1)\nshow_image(perturbed_images, idx=2)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n図 1"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#モデル訓練",
    "href": "posts/2024/Samplers/DDPM.html#モデル訓練",
    "title": "拡散模型の実装",
    "section": "3 モデル訓練",
    "text": "3 モデル訓練\n\nprint(\"Start training DDPMs...\")\nmodel.train()\n\nimport time\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    noise_prediction_loss = 0\n    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        optimizer.zero_grad()\n\n        x = x.to(DEVICE)\n        \n        noisy_input, epsilon, pred_epsilon = diffusion(x)\n        loss = denoising_loss(pred_epsilon, epsilon)\n        \n        noise_prediction_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n    \ntotal_time = time.time() - start_time\nprint(\"Finish!! Total time: \", total_time)\n\n\n\n\n\n\n\n注\n\n\n\n\n\nこの訓練コードも，前述の train_loader の定義が if __name__ == '__main__': と同じ if ブロックに入れる必要がある．\nさもなくば，並列処理が実行できず，訓練には大変な時間がかかる．1\nそこでここでは別の Python ファイル (Google Colab) で実行して，結果を読み込むこととする．\n\nmodel.eval()\n\ngenerated_images = torch.load(\"Files/generated_images1.pt\", map_location=torch.device('cpu'))\n\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_6810/3728020930.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  generated_images = torch.load(\"Files/generated_images1.pt\", map_location=torch.device('cpu'))\n\n\nGPU に乗った状態で読み込まれることに注意．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#データ生成",
    "href": "posts/2024/Samplers/DDPM.html#データ生成",
    "title": "拡散模型の実装",
    "section": "4 データ生成",
    "text": "4 データ生成\n\nmodel.eval()\n\nwith torch.no_grad():\n    generated_images = diffusion.sample(N=inference_batch_size)\n\nfor i in range(4):\n    show_image(generated_images, idx=i)\n\n\n\n\n\n\n\n\n\n\n\n(a) 生成された画像\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n図 2\n\n\n\nfor i in range(4, 7):\n    show_image(generated_images, idx=i)\n\n\n\n\n\n\n\n\n\n\n\n(a) 生成された画像\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n図 3\n\n\n\n\nshow_image(generated_images, idx=7)\n\n\n\n\n\n\n\n図 4: 生成された画像"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#モデル評価",
    "href": "posts/2024/Samplers/DDPM.html#モデル評価",
    "title": "拡散模型の実装",
    "section": "5 モデル評価",
    "text": "5 モデル評価\n\ndef draw_sample_image(x, postfix):\n  \n    plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    plt.title(\"Visualization of {}\".format(postfix))\n    plt.imshow(np.transpose(make_grid(x.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))\n\n\ndraw_sample_image(perturbed_images, \"Perturbed Images\")\n\n\n\n\n\n\n\n\n\ndraw_sample_image(generated_images, \"Generated Images\")\n\n\n\n\n\n\n\n\n\ndraw_sample_image(x[:inference_batch_size], \"Ground-truth Images\")"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#参考",
    "href": "posts/2024/Samplers/DDPM.html#参考",
    "title": "拡散模型の実装",
    "section": "6 参考",
    "text": "6 参考\n\n本稿は，Minsu Jackson Kang 氏 による チュートリアル を参考にした．"
  },
  {
    "objectID": "posts/2024/Samplers/DDPM.html#footnotes",
    "href": "posts/2024/Samplers/DDPM.html#footnotes",
    "title": "拡散模型の実装",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nkwargs = {'num_workers': 5, 'pin_memory': True, 'prefetch_factor': 2} でも１エポック 12 分以上なので，40 時間以上はかかる．さらにこの場合エポック 18 で RuntimeError: Shared memory manager connection has timed out を得たため，num_workers=0 とせざるを得なかった．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/Trans1.html",
    "href": "posts/2024/TransDimensionalModels/Trans1.html",
    "title": "超次元 Zig-Zag サンプラー",
    "section": "",
    "text": "理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html",
    "title": "ベイズ階層多ハザードモデル",
    "section": "",
    "text": "生存曲線のベイズ階層モデルによる外挿\n\n\n\n\n\n\n\nNo matching items\n\n\nZig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#前稿",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#前稿",
    "title": "ベイズ階層多ハザードモデル",
    "section": "",
    "text": "生存曲線のベイズ階層モデルによる外挿\n\n\n\n\n\n\n\nNo matching items\n\n\nZig-Zag サンプラーについては次の記事も参照：\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#ベイズ階層多ハザードモデル",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#ベイズ階層多ハザードモデル",
    "title": "ベイズ階層多ハザードモデル",
    "section": "1 ベイズ階層多ハザードモデル",
    "text": "1 ベイズ階層多ハザードモデル\n\n1.1 多ハザードモデルの表現力\nPolyhazard model もハザード関数をモデリングするが， \\[\nh_Y(y)=\\sum_{j=1}^Kh_j(y)\n\\] という形でモデリングし，個々の \\(h_j\\) にパラメトリックな仮定をおく．\n仮に \\(h_j\\) として，形状母数 \\(\\nu&gt;0\\) と位置母数 \\(\\mu:=\\alpha^{-\\nu}&gt;0\\) を持つ Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) のハザード関数 \\[\nh_{\\mathrm{W}}(y):=\\mu\\nu y^{\\nu-1}\n\\] と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) のハザード関数 \\[\nh_{\\mathrm{LL}}(y):=\\frac{\\left(\\frac{\\nu}{\\mu}\\right)\\left(\\frac{y}{\\mu}\\right)^{\\nu-1}}{1+\\left(\\frac{y}{\\mu}\\right)^\\nu}\n\\] の２つのみを考えたとしても，複数のパラメトリックモデルを足し合わせることで驚異的な表現力を達成することができる．\n\n\n\n(Hardcastle et al., 2024, p. 5) より．\n\n\n\n\n1.2 ベイズ階層多ハザードモデル\n(Hardcastle et al., 2024) では HTA への応用を念頭に，完全なベイズ階層多ハザードモデルの推定を試みている．\n\n1.2.1 第１階層\n各個別要因 \\(k\\in[K]\\) の形状母数 \\(\\nu_k\\) と位置母数 \\(\\mu_k\\) に階層構造 \\[\n\\log(\\nu_k)=\\alpha_k\\sim\\mathrm{N}(0,\\sigma_\\alpha^2)\n\\tag{1}\\] \\[\n\\log(\\mu_k)=\\beta_{k,0}+\\sum_{j\\in\\left\\{j\\in[p]\\mid\\gamma_{kj}=1\\right\\}}x_j\\beta_{k,j},\\qquad\\beta_{k,0}\\sim\\mathrm{N}(0,\\sigma_{\\beta_0}^2)\n\\tag{2}\\] を考える．ただし，\\(\\gamma_{k,j}\\in2\\) は共変量 \\(x_j\\) が \\(k\\in[K]\\) 番目の部分モデルに参加するかどうかを決める指示変数とする．\n式 (2) で残っているパラメータ \\(\\beta_{k,j}\\;(j\\in[p])\\) には \\[\n\\beta_{k,j}\\sim(1-\\omega)\\delta_0+\\omega\\operatorname{N}(0,\\sigma_\\beta^2)\n\\] と spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定し，変数選択を促進する．選択された変数については \\(\\gamma_{k,j}=1\\) とする．\n以降，\\(\\theta_k=(\\nu_k,\\mu_k)\\) とし，\\((K,\\gamma,\\theta)\\) を本モデルのパラメータと理解する（\\(K\\) の事前分布は後述 1.2.3）．\n\n\n1.2.2 第２階層\n\\(\\sigma_\\alpha^2=2,\\sigma_{\\beta_0}^2=5\\) は固定してしまうと，\\(\\phi:=(\\omega,\\sigma_\\beta)\\) がハイパーパラメータとして残っている．これには \\[\n\\omega\\sim\\operatorname{Beta}(a,b)\n\\] \\[\n\\sigma_\\beta\\sim\\operatorname{HalfCauchy}(0,1)\n\\] という事前分布をおき，\\(a=b=4\\) と固定する．\n前者はモデルのサイズについて Beta-二項分布を仮定することに等価である (3.1 節 Ley and Steel, 2009)．後者は (Gelman, 2006), (Polson and Scott, 2012) の推奨の通りである．\n\n\n1.2.3 \\(\\mathcal{P}(E)\\) 上の事前分布\n実はまだ第一階層のパラメータが残っている．ハザードの数 \\(K\\) と \\(h_k\\) の関数形をどうするかである．\nここでは Weibull 分布 \\(\\mathrm{W}(\\nu,\\mu)\\) と対数ロジスティック分布 \\(\\mathrm{LL}(\\nu,\\mu)\\) の２つ \\[\nD=\\{\\mathrm{W}(\\nu,\\mu),\\mathrm{LL}(\\nu,\\mu)\\}\n\\] から等確率で \\[\nK\\sim\\mathrm{Pois}_{&gt;0}(\\xi)\n\\] 個選ぶこととする．\nハイパーパラメータ \\(\\xi\\) については，(Hardcastle et al., 2024) では \\(\\xi=2\\) としている．その根拠の１つに \\[\n\\operatorname{P}[K\\ge5]\\approx0.061\n\\] を満たす性質が，実際の応用で５つ以上の競合リスクが存在する場面は稀であることに整合することを挙げている："
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#pdmp-によるベイズ競合リスク分析",
    "href": "posts/2024/TransDimensionalModels/SurvivalAnalysis1.html#pdmp-によるベイズ競合リスク分析",
    "title": "ベイズ階層多ハザードモデル",
    "section": "2 PDMP によるベイズ競合リスク分析",
    "text": "2 PDMP によるベイズ競合リスク分析\n\n2.1 はじめに\n前節で定義されたモデルのパラメータ空間 \\[\n(K,D,\\gamma,\\phi,\\theta)\n\\] 上での事後分布サンプリングを行うことを考える．\\(\\phi=(\\omega,\\sigma_\\beta)\\) はハイパーパラメータである．\n基本的には \\(\\theta=(\\theta_k)=(\\nu_k,\\mu_k)\\) の事後分布サンプリングを考えるのであるが，\\(K,\\gamma\\) の如何によって \\(\\theta\\) の次元が変化する．\n加えて \\(D\\) の如何によって尤度が変化するから，Poisson 点過程の強度関数に解析的な上界が見つかるはずもないため，Automatic Zig-Zag (Corbella et al., 2022) と Concave-Convex PDMP (Sutton and Fearnhead, 2023) を組み合わせて用いる．\n\n\n2.2 \\(\\theta\\) の Zig-Zag サンプリング\npolyhazard model の標準的なサンプラーには Gibbs サンプラーや NUTS サンプラーがあり得るが，いずれも複数の次元の間を飛び回れるように拡張するには，空間の間の跳躍をうまく設計する必要がある．\nこの点で Zig-Zag サンプラーは従来のサンプリング法と対等であるが，今回の設定には多峰性の懸念も存在する．\nというのも，\\(D\\) によって指定される分布は互いに交換可能であるため，混合モデリングにおける label switching problem (Jasra et al., 2005) 同様に事後分布は必然的に多峰性（対称性）を帯びるはずである．\n多峰性への対処という点では，Zig-Zag サンプラーに軍配が上がるはずである (Andrieu and Livingstone, 2021)．\n\n\n2.3 ハイパーパラメータ \\(\\phi\\) のサンプリング\n\\((\\theta,\\phi)\\) 上から結合分布を Zig-Zag サンプリングすることも可能であるが，\\(\\theta\\) は \\(\\phi\\) に依存するため，事後分布は強い相関構造を持つと予想される．\n一方でこの条件付き構造は Gibbs サンプラーの発想で有効に利用したいものである．これには Zig-Zag within Gibbs (Sachs et al., 2023) を用いることができる．\nこの方法によれば，ハイパーパラメータ \\(\\phi=(\\omega,\\sigma_\\beta)\\) を固定した下で他の変数をサンプリングすることになる．この設定では次に論じるように変数選択の指示変数 \\(\\gamma\\) に対する効率的なシミュレーション法も導く．\n\n\n2.4 変数選択 \\(\\gamma\\) のサンプリング\n一般にベイズ変数選択は \\(\\gamma\\) のような指示変数のサンプリングに問題を帰着させる方法 (Zanella, 2020) が state-of-the-art とされる．\nしかし各 \\(\\gamma\\in 2^{Kp}\\) に対応するモデルの（周辺）尤度の計算が困難な場面は多く，特に今回の生存モデルはその一例である (Liang et al., 2023)．\nしかし Zig-Zag サンプラーでは \\(\\{\\beta_{k,j}=0\\}\\) のなす部分空間を通過する際に一定の確率でランダムな時間ここに囚われることで，自然な形で trans-dimensional なサンプリングが可能になることが (Chevallier et al., 2023), (Bierkens et al., 2023) により同時に提案されている．\nこの際 \\(\\{\\beta_{k,j}=0\\}\\) から脱出する確率は時刻や位置に依らず（ハイパーパラメータ \\(\\omega\\) の下で）一定であり，一様な Poisson 点過程のシミュレーションにより実現される．この意味で Zig-Zag サンプラーによる変数選択は，周辺尤度の計算を回避した効率的な計算法を提供する．\n\n\n2.5 \\(K,D\\) のサンプリング\nここでは birth-death-swap 過程を用いる．というのも，３つの Poisson 点過程 \\(\\Lambda^b,\\Lambda^d,\\Lambda^s\\) を用いて，これが到着するごとに確率核 \\(q_b,q_d,q_s\\) に従って新たなハザード関数の追加・消去・取り替えが起こる．\n取り替えは必ずしも必要がないが，これの追加により多峰性の問題 2.2 が解決され，サンプラーの収束が改善されるという．\nbirth-death 過程は次の詳細釣り合い条件を満たせば良い： \\[\n\\Lambda^b(t)\\pi(\\theta,D,K)q_b(u)=\\Lambda^d(t)\\pi(\\theta',D',K')q_d(u').\n\\]\nこの方法は Zig-Zag within Gibbs (Sachs et al., 2023) の Gibbs 核を，(Green, 1995) の超次元跳躍核に取り替えていることに等しい．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html",
    "title": "ベイズ理想点解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#理想点解析と項目反応モデル",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#理想点解析と項目反応モデル",
    "title": "ベイズ理想点解析",
    "section": "1 理想点解析と項目反応モデル",
    "text": "1 理想点解析と項目反応モデル\n理想点解析とは，項目反応モデルを利用した各議員（立法府の構成員など）の行動様式・イデオロギーの定量化・視覚化の手法である．\n詳しくは次項を参照：\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本稿では理想点解析を目標として，項目反応モデル，特に二項選択モデルをベイズ推定する方法を考える．\nその中でも，理想点 \\(x_i\\in\\mathbb{R}^d\\) の次元 \\(d\\) に関してモデル選択を行うことを考える．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#変数選択と-sticky-pdmp-サンプラー",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#変数選択と-sticky-pdmp-サンプラー",
    "title": "ベイズ理想点解析",
    "section": "2 変数選択と Sticky PDMP サンプラー",
    "text": "2 変数選択と Sticky PDMP サンプラー\n\n2.1 はじめに\nまずは次の標準的な理想点モデルを考える (Imai et al., 2016, p. 633) 参照：\n\n\n\n\n\n\n標準的な理想点モデル\n\n\n\n\\(i\\in[N]\\) 番目の議員が \\(j\\in[J]\\) 番目の法案に対して賛成ならば \\(y^i_j=1\\)，反対ならば \\(y^i_j=0\\) のデータ \\(y\\in M_{N,J}(2)\\) が得られているとする．\nこのとき \\(i\\in[N]\\) 番目の議員の理想点 \\(x_i\\in\\mathbb{R}^d\\) は，\\(y^i\\) を次のように予測する潜在変数とする： \\[\\begin{align*}\n  y^i_j&=1_{\\mathbb{R}^+}(\\widetilde{y}^i_j)\\\\\n  \\widetilde{y}^i_j&=x_i^\\top\\beta_j+\\epsilon^i_j,\\qquad\\epsilon^i_j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1).\n\\end{align*}\\]\nすなわち \\(d\\)-母数のロジット項目反応モデルにおいて，議員ごとの母数である \\(x_i\\in\\mathbb{R}^d\\) を 理想点 と呼ぶ．項目識別母数 \\(\\beta_j\\in\\mathbb{R}^d\\) は法案ごとの性質の違いを表しているものと考える．\n換言すれば，\\(\\Phi\\) を \\(\\mathrm{N}(0,1)\\) の分布関数として，次のプロビットモデルが想定されたことになる： \\[\n\\operatorname{P}[y^i_j=1]=\\Phi(x_i^\\top\\beta_j).\n\\]\n\n\nこのモデルの潜在変数 \\(\\widetilde{y}\\) とパラメータ \\((x_i)_{i=1}^N\\in M_{dN}(\\mathbb{R}),(\\beta_j)_{j=1}^J\\in M_{dJ}(\\mathbb{R})\\) の事後分布を推定すると同時に，理想点の次元 \\(d\\) についてモデル選択を行うことを考える．\n\n\n2.2 事後分布の表示\n\\((x_i),(\\beta_j)\\)\n\\[\\begin{align*}\n  p(\\widetilde{y},x,\\beta|y)&=\n\\end{align*}\\]\nこのモデルはテーブルデータ \\(y^i_j\\) の対称性の崩れに全ての情報が委ねられている．その性質上強い多峰性を持った事後分布になるはずであり，本質的に MCMC のサンプリングが困難である．\nそこでここでは PDMP サンプラーを用いた事後分布サンプリングを試みる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#モデルの拡張",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#モデルの拡張",
    "title": "ベイズ理想点解析",
    "section": "3 モデルの拡張",
    "text": "3 モデルの拡張\n\n3.1 時系列データへの適用\n(Martin and Quinn, 2002) は最高裁判事の理想点の経時変化を解析し，その MCMC を通じた推定法が MCMCpack パッケージで実装されている (Martin et al., 2011)．\nしかしこの方法は極めて時間がかかることで知られており，(Imai et al., 2016, p. 643) は代わりに変分ベイズ推論による推定法を提案し，５日の推定時間が４秒に短縮されたとしている．\n\n\n3.2 階層化\n(Bafumi et al., 2005) では従来のモデル \\[\ny^*_l=\\alpha_{j[l]}+x_{i[l]}^\\top\\beta_{j[l]}+\\epsilon_l,\\qquad\\epsilon_l\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1)\n\\] に次の階層構造を加えたモデルを考察している： \\[\nx_{i[l]}=\\gamma_{g[i[l]]}^\\top z_{i[l]}+\\eta_{i[l]},\\qquad\\eta_{i[l]}\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,\\sigma^2_{g[i[l]]}).\n\\]\nこれにより議員 \\(i[l]\\in [N]\\) ごとに異なる共変量 \\(z_{i[l]}\\in\\mathbb{R}^M\\) をモデルに加味することが可能になっている．\n\\(g:[N]\\to\\mathbb{N}\\) は議員のグループを表している．\n\n\n\n\n\n\n線型トレンド時系列モデルとしての解釈\n\n\n\n\n\n\\(g:[N]\\to\\mathbb{N}\\) を議員番号とし，\\(i[l]\\) を当該議員の累計議員経験年数とする（○○議員３期目，など）．\n加えて \\(z_{i[l]}\\) を累計議員経験年数と取ると，議員ごとに係数 \\(\\gamma_{g[i[l]]}\\) を持った線型トレンドをモデリングすることに相当する．\nこのモデルは DW-NOMINATE (Poole and Rosenthal, 1997) が用いたのちに (Bailey, 2007) などでも継承されている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#クラスタリングと-zigzag-within-gibbs-サンプラー",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#クラスタリングと-zigzag-within-gibbs-サンプラー",
    "title": "ベイズ理想点解析",
    "section": "4 クラスタリングと ZigZag-within-Gibbs サンプラー",
    "text": "4 クラスタリングと ZigZag-within-Gibbs サンプラー"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#注",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#注",
    "title": "ベイズ理想点解析",
    "section": "5 注",
    "text": "5 注\n\n本稿は (Imai et al., 2016) と (Hardcastle et al., 2024) に触発されて書かれた．(Imai et al., 2016) では種々の理想点モデルを，最も純粋なものから真に興味深い複雑なものまで，変分ベイズ推論の観点で統一的に扱っている．(Hardcastle et al., 2024) では大規模な polyhazard モデルとそのモデル選択を，単一の PDMP サンプラーで行っている．\n(Imai et al., 2016) では変分 EM アルゴリズムとパラメトリックブートストラップが一貫して用いられているが，本稿はあくまで MCMC の正統進化としての PDMP サンプラーを主軸に据えることを提案する．政治科学におけるモデルは不確実性が大きく，これを直接に取り扱えるベイズの方法が望ましいと信じるためである（最終的な目標にモデル平均による推定がある）．\nベイズ計算手法としては，PDMP の採用により正確でバイアスのない推定を，従来よりもはるかに高速に推定できることを示す．加えて PDMP サンプラーでは次のような追加の利点がある\n\n(Imai et al., 2016) のようなパラメトリックブートストラップをせずとも自然な不確実性の定量化を与える．\nモデル選択と平均を同時に行うことができる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint2.html#acknowledgement",
    "href": "posts/2024/TransDimensionalModels/IdealPoint2.html#acknowledgement",
    "title": "ベイズ理想点解析",
    "section": "6 Acknowledgement",
    "text": "6 Acknowledgement\n\n\nThis work was supported in part by The Graduate University for Advanced Studies, SOKENDAI.\n\n\nThe author(s) would like to thank the Isaac Newton Institute for Mathematical Sciences, Cambridge, for support and hospitality during the programme Monte Carlo sampling: beyond the diffusive regime, where work on this paper was undertaken. This work was supported by EPSRC grant EP/Z000580/1."
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html",
    "title": "理想点解析のハンズオン",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#関連記事",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#関連記事",
    "title": "理想点解析のハンズオン",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-12-15\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本稿では実際に理想点モデルの推定を，Martin-Quinn により公開されている連邦最高裁判所の 1937 年から 2022 年までのデータを（MCMCpack パッケージを通じて）用いて行う．\n\nlibrary(MCMCpack)\ndata(Rehnquist)  # MCMCpackに含まれる U.S. Supreme Court（連邦最高裁）のデータ\nkable(head(Rehnquist))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRehnquist\nStevens\nO.Connor\nScalia\nKennedy\nSouter\nThomas\nGinsburg\nBreyer\nterm\ntime\n\n\n\n\n0\n1\n0\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\nNA\n0\n1994\n1\n\n\n0\n1\n0\n1\n1\n1\n0\n1\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1994\n1\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1994\n1\n\n\n\n\n\nこのデータは保守的な判断をする場合が \\(y_i=1\\)，リベラルな判断をする場合が \\(y_i=0\\) の２値データとなっている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点モデルとは何か",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点モデルとは何か",
    "title": "理想点解析のハンズオン",
    "section": "1 理想点モデルとは何か？",
    "text": "1 理想点モデルとは何か？\n\n1.1 MCMCpack パッケージ\nはじめに，理想点モデルではどのようなことができるかをみるために，MCMCpack パッケージを通じて理想点推定を簡単に実行する方法を見る．\n理想点モデルでは識別性が一つの論点になる（第 2.5 節）が，ここでは簡単に，Stevens 判事と Thomas 判事の位置を固定する方法を用いてみよう．\nMCMCpack パッケージでは，時系列理想点モデルの推定に MCMCdynamicIRT1d() 関数が用意されている．\n\n# 初期値の設定\ntheta.start &lt;- rep(0, 9)  # 9人の裁判官の初期値\ntheta.start[2] &lt;- -3      # Stevens裁判官の初期値\ntheta.start[7] &lt;- 2       # Thomas裁判官の初期値\n\n# MCMCの実行\nout &lt;- MCMCdynamicIRT1d(\n    t(Rehnquist[,1:9]),           # データ行列（転置して裁判官×案件の形に）\n    item.time.map=Rehnquist$time, # 各案件の時期情報\n    theta.start=theta.start,      # 初期値\n    mcmc=2000,                   # MCMCの反復回数\n    burnin=2000,                 # バーンイン期間\n    thin=5,                       # 間引き数\n    verbose=500,                  # 進捗表示間隔\n    tau2.start=rep(0.1, 9),      # τ²の初期値\n    e0=0, E0=1,                  # θの事前分布パラメータ\n    a0=0, A0=1,                  # αの事前分布パラメータ\n    b0=0, B0=1,                  # βの事前分布パラメータ\n    c0=-1, d0=-1,               # τ²の事前分布パラメータ\n    store.item=FALSE,            # アイテムパラメータを保存しない\n    theta.constraints=list(Stevens=\"-\", Thomas=\"+\")  # 識別制約\n)\n\ntheta_cols &lt;- grep(\"theta\", colnames(out), value=TRUE)\ntheta_mcmc &lt;- out[, theta_cols]\n\n# library(coda)\n# summary(theta_mcmc)  # codaのsummary関数で要約\nplot(theta_mcmc)\n\n\n\n\n\n\n\n\n1.2 理想点の推定\n出力は各最高裁判事の理想点の事後分布である．\n\ntheta_means &lt;- colMeans(theta_mcmc)\npattern &lt;- \"t11\"\ncol_names &lt;- colnames(theta_mcmc)\nselected_cols &lt;- grep(pattern, col_names)  # 正規表現にマッチする列のインデックスを取得\nselected_theta_mcmc &lt;- theta_mcmc[, selected_cols]\n\nquantiles_2_5 &lt;- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.025))\nquantiles_97_5 &lt;- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.975))\n\nggplot(data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = unname(theta_means[selected_cols]),\n  lower = unname(quantiles_2_5),\n  upper = unname(quantiles_97_5)\n), aes(x = mean, y = legislator)) + \n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Estimated Ideal Points (11th term)\", x = \"Ideal Point\", y = \"Legislator\")\n\n\n\n\n\n\n\n\n\n\n1.3 時系列化\n(Martin and Quinn, 2002) ではこの理想点の時系列的な変化を調べた．\ntime_points &lt;- unique(Rehnquist$time)\nn_subjects &lt;- 9  # 裁判官の数\n\n# 各裁判官の軌跡をプロット\nplot(time_points, theta_means[1:length(time_points)], \n     type=\"l\", ylim=range(theta_means),\n     xlab=\"Time\", ylab=\"Ideal Point\",\n     main=\"Estimated Ideal Points Over Time\")\n\n# 各裁判官を異なる色で追加\ncolors &lt;- rainbow(n_subjects)\ncolors[9] &lt;- \"blue\"\nfor(i in c(1,8,9)) {\n    lines(time_points, \n          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],\n          col=colors[i],\n          lwd=3)\n}\n\n# 凡例を追加\nlegend(\"topright\", \n       legend=unique(colnames(Rehnquist)[c(1,8,9)]),  # 裁判官の名前\n       col=colors[c(1,8,9)], \n       lty=1)\n\n\n\n\n\nたしかに William Rehnquist は共和党，Ruth Bader Ginsburg と Stephen Breyer は民主党である．\n\n\n\n\n\n\\(0\\) の上に位置している Anthony Kennedy や Sandra Day O’Connor はほとんど中道的だが，やや保守党寄りである． Antonin Scalia は特に保守的な立場であることが知られている．\n\\(0\\) よりも下に位置するもう一人は David Souter であるが，彼はもともと保守系と目されていたが，後年リベラルな傾向を示したとされる．1"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#母数ロジットモデルの推定",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#母数ロジットモデルの推定",
    "title": "理想点解析のハンズオン",
    "section": "2 ２母数ロジットモデルの推定",
    "text": "2 ２母数ロジットモデルの推定\n\n2.1 はじめに\nMCMCpack パッケージで理想点推定の出力がつかめたいま，より詳しくモデルを見ていく．\n本節では rstan パッケージを用いて，項目反応モデルとして具体的な手順を踏んで推定してみる．\n\nlibrary(tidyverse)\ndf &lt;- Rehnquist %&gt;%\n  # データを長形式に変換\n  pivot_longer(cols = -c(term, time), names_to = \"name\", values_to = \"y\") %&gt;%\n  # ケース ID を追加\n  mutate(case = (row_number() - 1) %/% 9 + 1)\n\n\n\n2.2 １母数モデル（brms パッケージ）\nまずは最も簡単な項目反応モデルとして，\\(g\\) を logit リンクとして， \\[\ng(\\operatorname{P}[Y_{ij}=1])=\\alpha_0+\\alpha_j-x_i\n\\] というモデルを推定することを考えよう．\nbrms パッケージを用いれば，他の R パッケージと同様のインターフェイスで推定を行うことができる．\n\nlibrary(brms)\nformula &lt;- bf(\n  y ~ 1 + (1 | case) + (1 | name)\n)\nfit_1PL &lt;- brm(\n  formula,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ 1 + (1 | case) + (1 | name) \n   Data: df (Number of observations: 4343) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~case (Number of levels: 485) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.05      0.06     0.93     1.18 1.00     2016     2773\n\n~name (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.62      0.47     0.99     2.75 1.01      875     1534\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.11      0.54    -1.15     1.00 1.00      652     1263\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n簡単なモデルであるが切片項の ESS が低く，すでに暗雲が立ち込めている．\n\nplot(fit_1PL)\n\n\n\n\n\n\n\n\nここには変動係数（我々の欲しい潜在変数）はパラメータとみなされておらず，推定値が表示されないので次のようにしてプロットする必要がある：\n\nranef_legislator &lt;- ranef(fit_1PL)$name\nposterior_means &lt;- ranef_legislator[,1,\"Intercept\"]\nlower_bounds &lt;- ranef_legislator[,3,\"Intercept\"]\nupper_bounds &lt;- ranef_legislator[,4,\"Intercept\"]\nplot_legislator &lt;- data.frame(\n  legislator = rownames(ranef_legislator),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_1PL &lt;- ggplot(plot_legislator, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"1PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Legislator\")\np_1PL\n\n\n\n\n\n\n\n\nThomas や Scalia，そして Stevens が極端であることはとらえているが，Stevens や Ginsburg らリベラルな判事は左側に来て欲しいのであった．\n誘導が成功しておらず，片方の峯からサンプリングしてしまっている．\n\nprior_summary(fit_1PL)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n student_t(3, 0, 2.5)        sd            case                  0   \n student_t(3, 0, 2.5)        sd Intercept  case                  0   \n student_t(3, 0, 2.5)        sd            name                  0   \n student_t(3, 0, 2.5)        sd Intercept  name                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n\n\n\n\n2.3 １母数モデル（rstan パッケージ）\nbrms パッケージ内で生成される Stan コードを参考にして，自分で Stan コードを書いて推定することもできる．\n\n\n\n\n\n\nStan コードの出力\n\n\n\n\n\nstancode(fit_1PL)\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n\n\n\nlibrary(rstan)\nstan_code &lt;- \"\ndata {\n  int&lt;lower=1&gt; n;  // data size: n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  real alpha_zero;  // intercepts\n  vector[J] alpha;  // item effects\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(alpha_zero | 3, 0, 2.5);\n  lprior += student_t_lpdf(alpha | 3, 0, 2.5);\n  lprior += student_t_lpdf(X | 3, 0, 2.5);\n}\nmodel {\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] - X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu + alpha_zero);\n  target += lprior;\n}\n\"\ncase_number &lt;- as.integer(nrow(df) / 9)\nindicator_i &lt;- rep(1:9, times = case_number)\nindicator_j &lt;- rep(1:case_number, each = 9)\ndf$i &lt;- indicator_i\ndf$j &lt;- indicator_j\n\ndf_NA &lt;- df %&gt;% filter(!is.na(y))\n\ndata &lt;- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit &lt;- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\nall_samples &lt;- extract(fit, pars = \"X\")$X\nx_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe &lt;- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\np &lt;- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"1PL Model (RStan)\")\n\nbrms による推定結果と，左右が逆になっている．これはモデル式を alpha[j[k]] - X[i[k]] と Stan コードに記入しており，brms の設定と（たまたま）逆になったためである．\n余談であるが，\\(\\alpha_0\\) を固定された切片項でなくて，変動させる（変量効果にする）ことで，事後分散は大きく縮まる．\n係数 \\(\\alpha_j,x_i\\) の事前分布を正規分布に変更したりしても結果はほとんど変わらない．\n\n\n2.4 ２母数モデル\n(Bafumi et al., 2005) など，多くの理想点モデルでは２母数ロジットモデルが用いられる： \\[\ng(x):=\\operatorname{logit}(x)=\\log\\frac{x}{1-x},\n\\] \\[\ng(\\mu_{i,j})=\\alpha_j+\\beta_j x_i=\\beta_j\\biggr(\\widetilde{\\alpha}_j+x_i\\biggl).\n\\] この際 \\(x_i\\) は \\(i\\) 番目の判事の 理想点 といい，\\(\\alpha_j,\\beta_j\\) は \\(j\\) 番目の事件の性質を表すパラメータである．\nものによっては判事の立場が関係ない事件もあるため，\\(\\beta_j\\) が用意されている．\n基本的にこの識別パラメータが正になるように調整したいが，明示的にそうすることはしない．\n次節で説明する方法により，理想点 \\(x_i\\) が大きい場合は保守的な判断を下しやすいものと解釈できるように設計することができる（\\(x_i\\) を数直線上にプロットした際に，リベラルな場合に左に，保守的な場合に右に来るようにする）が，ここではストレートに実装してみよう．\nlibrary(rstan)\nstan_code &lt;- \"\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  vector[J] alpha;  // item effects\n  vector[J] beta;  // item discremination\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\"\ncase_number &lt;- as.integer(nrow(df) / 9)\nindicator_i &lt;- rep(1:9, times = case_number)\nindicator_j &lt;- rep(1:case_number, each = 9)\ndf$i &lt;- indicator_i\ndf$j &lt;- indicator_j\n\ndf_NA &lt;- df %&gt;% filter(!is.na(y))\n\ndata &lt;- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit &lt;- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\nall_samples &lt;- extract(fit, pars = \"X\")$X\nx_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe &lt;- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\nggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"Ideal Points of Rehnquist\")\n\n２つを並べてみると\n\n\n\n\n\n\n\n\n\n1PL Model\n\n\n\n\n\n\n\n2PL Model\n\n\n\n\n\nモデルの自由度が上がったことにより，事後分散が大幅に小さくなっていることがわかる．\nまた，1PL の場合と再び左右が反転し，最後のリベラルな判事の２人 Ginsburg と Breyer が右に来てしまっている．\nこれはモデル式を mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]]; と足し算に戻したからだろうか？\n実はそうではない．実際，何度か MCMC を回し直すと，ちゃんとリベラルな判事が左に来てくれることもある．\n即ち，事後分布が多峰性を持つ のである！\n\n\n\n2.5 識別可能性\n実は２母数ロジットモデルでは３つ識別不可能性を引き起こす対称性がある：\n\n\n\n\n\n\n(Section 2 Bafumi et al., 2005)\n\n\n\n\n加法的別名 (additive aliasing)\n\\(\\widetilde{\\alpha}_j,x_i\\) に同じ数を足した場合と \\(\\beta_j\\) と \\((\\widetilde{\\alpha}_j,x_i)\\) に同じ数を乗じた／除した場合，全く等価なモデルが得られる．すなわちスケールを定める必要がある．\n乗法的別名 (additive aliasing)\n\\(\\beta_j\\) を \\(a&gt;0\\) 倍し，\\(\\widetilde{\\alpha}_j,x_i\\) を \\(1/a\\) 倍すると等価な尤度を定める．\n反転対称性 (reflection invariance)\n最後に \\(\\beta_j\\) の符号の問題があるためである．\\(\\beta_j\\) を \\(-1\\) 倍させることで \\(\\widetilde{\\alpha}_j,x_i\\) の役割を \\(-1\\) 倍させることができる．このまま推定すると事後分布は \\(0\\) に関して対称な形を持つことになる．\n\n\n\n\n\\(\\beta_j\\) の符号を制約したり，特定の判事の \\(x_i\\) を固定して参照点とするなどの方法があるかもしれないが，ここでは (2.2.3 節 Bafumi et al., 2005, p. 178) に倣って，階層モデルの方法により，構造的なやり方でモデルに情報を伝える．\nというのも，理想点 \\(x_i\\) に次の階層構造を入れるのである： \\[\nx_i=\\delta+\\gamma z_i+\\epsilon_i\\qquad\\epsilon_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(0,1).\n\\]\n\\(z_i\\) は当該判事を示した大統領の所属政党を表す２値変数で，共和党ならば \\(z_i=1\\) とする．そして \\(\\gamma\\) に \\(\\mathbb{R}_+\\) 上に台を持つ事前分布を置く．\n\n\n\n\n\n\n(Bafumi et al., 2005) による理想点モデルの階層化\n\n\n\nこのように共変量を適切な階層に追加することは，モデルに自然な形で正則化情報を伝えることに繋がり，モデルの識別やより現実的な推定値の獲得に繋がる．\n\n\n\n\n2.6 階層ベイズ推定\n\n2.6.1 指名大統領の政党属性\n続いて 2.5 で検討した，(Bafumi et al., 2005) による階層ベイズモデルにより緩やかに情報を伝えることで識別可能性を保つ方法を検討する（第 2.6.2 節）．\n\ndf &lt;- df %&gt;%\n  mutate(\n    nominator = case_when(\n      name %in% c(\"Rehnquist\", \"Stevens\") ~ \"Nixon\",\n      name %in% c(\"O.Connor\", \"Scalia\", \"Kennedy\") ~ \"Reagan\",\n      name %in% c(\"Souter\", \"Thomas\") ~ \"Bush\",\n      name %in% c(\"Breyer\", \"Ginsburg\") ~ \"Clinton\"\n    )\n  )\ndf$x &lt;- ifelse(\n  df$nominator %in% c(\"Nixon\", \"Reagan\", \"Bush\", \"Trump\"),\n  1, -1)\n\n\n\n2.6.2 階層２母数モデル\nx の情報を階層的に伝えるには，もはや brms パッケージでは実行できないようである．\n\n\n\n\n\n\nfit_2PL::brmsfit で用いた Stan コードの出力\n\n\n\n\n\nbrms パッケージでは stancode() 関数を用いて Stan コードを出力できる．\nstancode(fit_2PL)\n結果は以下の通りになる：\n// generated with brms 2.21.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  vector[N] Z_2_2;\n  int&lt;lower=1&gt; NC_2;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  matrix[M_2, N_2] z_2;  // standardized group-level effects\n  cholesky_factor_corr[M_2] L_2;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  matrix[N_2, M_2] r_2;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_2] r_2_1;\n  vector[N_2] r_2_2;\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  // compute actual group-level effects\n  r_2 = scale_r_cor(z_2, sd_2, L_2);\n  r_2_1 = r_2[, 1];\n  r_2_2 = r_2[, 2];\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(to_vector(z_2));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n  vector&lt;lower=-1,upper=1&gt;[NC_2] cor_2;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_2) {\n    for (j in 1:(k - 1)) {\n      cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n    }\n  }\n}\nfunction ブロックでは scale_r_cor() 関数が定義されている．標準化された項目変数 \\((\\alpha_j,\\beta_j)\\) を z として，その各次元の標準偏差を含む２次元ベクトルを sd_2，Cholesky 因子を L_2 として，行列積 sd_2*L2 にベクトル z を掛けて返している．これは \\((\\alpha_j,\\beta_j)\\) を３つの要素（対角行列・Cholesky 因子・標準化されたベクトル）に因数分解して計算していることに起因する．\ndata ブロックでデータのコーディングを定めている．Y は \\(NJ\\) 行列である．case が ID2 で name が ID1 に対応する．判事 \\(i\\in[n]\\) は N_1 人，件数 \\(j\\in[J]\\) は N_2 件．このデータから M_1=1 次元の理想点を持った M_2=2 パラメータモデルを推定する．Z_1_1 が判事を表す標示変数で，項目を表す標示変数は Z_2_1 と Z_2_2 である．\nparameter ブロックでは標準化された理想点 z_1 と項目パラメータ z_2，それぞれの標準偏差 sd_1, sd_2 を宣言している．z_1 だけ N_1 ベクトル，z_2 は N_2 行 M_2 列の行列であることに注意．\ntransformed_parameters で真のスケールに戻す．r_1_1=(sd_1[1]*(z_1[1])) は理想点 \\(x_1\\) にあたり，r_2=scale_r_cor(z_2,sd_2,L_2) は項目パラメータ \\((\\alpha_j,\\beta_j)\\) にあたる．その後 r_2_1=r_2[,1] と r_2_2=r_2[,2] でそれぞれ \\(\\alpha_j\\) と \\(\\beta_j\\) に分解している．最後に sd_1, sd_2 に t-分布，L_2 に Cholesky 分布を事前分布として定義している．\nmodel で尤度を定義している．mu はこのブロックでしか使われない線型予測子の格納変数である．\nmu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\nにより \\(\\alpha_j+\\beta_j x_i\\) が計算されている．\\(j\\) は J_2[n], \\(i\\) は J_1[n] で表されている．最後に bernoulli_logit_lpmf(Y | mu) で尤度が定義される．\ngenerated quantities ブロックでは相関行列 cor_2 を計算している．\n\n\n\n\n\nFiles/bafumi.stan\n\ndata {\n  int&lt;lower=1&gt; n;  // n = N * J - #(NA responses)\n  int&lt;lower=1&gt; N;  // number of judges\n  int&lt;lower=1&gt; J;  // number of cases\n\n  array[n] int&lt;lower=0, upper=1&gt; Y;  // response variable\n  vector[N] Z;  // covariates for judges\n  array[n] int&lt;lower=1, upper=N&gt; i;  // indicator for judges i in [N]\n  array[n] int&lt;lower=1, upper=J&gt; j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points for judges\n  vector[J] alpha;\n  vector[J] beta;\n\n  real delta;\n  real gamma;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(delta | 3, 0, 2.5);\n  lprior += student_t_lpdf(gamma | 3, 0, 2.5);\n  lprior += student_t_lpdf(alpha | 3, 0, 2.5);\n  lprior += student_t_lpdf(beta | 3, 0, 2.5);\n  lprior += std_normal_lpdf(X);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  X ~ normal(delta + Z * exp(gamma), sigma);\n\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\n\n\n\n\n階層モデルの推定結果\n\n\n\n\n\n\n\n\n\n\n\n今回の推定結果\n\n\n\n\n\n\n\n２母数モデル 2.4 の推定結果\n\n\n\n\n\n\n\n\n2.7 識別性の影響\n実は階層的に緩やかにしか情報を伝えていないために，今回の \\(\\gamma\\) への事前分布の設定では，まだ \\(0\\) の近くに密集することでもう一つの峰を作り出してしまうようである．\n例えば，Stevens, Souter, Ginsburg, Breyer らリベラルな判事の初期位置を右側の \\(1\\) に，Thomas, Scalia ら保守派の判事の初期位置を \\(-1\\) に，そのほか中道的な判事の初期位置を \\(0\\) にしてサンプリングをすると，ほぼ確実に左右が逆になった結果が出てくる．\nその際は \\(\\gamma\\) の事後分布が大きく違う．\ninit_values &lt;- list(X = c(-1.0,1.0,1.0,-1.0,-1.0,0.0,0.0,1.0,1.0), alpha = rep(0.0, case_number), beta = rep(0.0, case_number), delta = 0.0, gamma = 0.0, sigma = 1.0)\nfit &lt;- stan(\"Files/bafumi.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000, init = rep(list(init_values), 4))\n\n\n\n\n\n\n\n\n\n\n理想点の事後平均の位置が正しい場合の回帰係数 \\(\\gamma\\) の事後分布\n\n\n\n\n\n\n\n理想点の事後平均の位置が間違っている場合の回帰係数 \\(\\gamma\\) の事後分布\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点の事後平均の位置が正しい場合の回帰係数 \\(\\delta\\) の事後分布\n\n\n\n\n\n\n\n理想点の事後平均の位置が間違っている場合の回帰係数 \\(\\delta\\) の事後分布\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点の事後平均の位置が正しい場合の理想点の事後分布\n\n\n\n\n\n\n\n理想点の事後平均の位置が間違っている場合の理想点の事後分布\n\n\n\n\n\n\n\n2.8 事後分布の２峰性\n\nしかし \\(\\alpha_j,\\beta_j,\\delta,\\gamma\\) の事前分布はあまり大きな影響はないようである．\\(\\gamma\\) の中心も少しズラしたくらいでは関係ない．\\(\\gamma\\) を分散 \\(1\\) で平均 \\(10\\) の正規分布などにすると，\\(\\gamma\\) の事後分布は右に引っ張れる．\n\n\n\n\n\n\n\n\n\n元々の事前分布\n\n\n\n\n\n\n\n\\(\\gamma\\) を右に引っ張った場合の事後分布\n\n\n\n\n\nしかし理想点の結果自体は（左右の別を除いて）ほとんど変わらない：\n\n\n\n\n\n\n\n\n\n元々のモデルの結果\n\n\n\n\n\n\n\n\\(\\gamma\\) の事後分布を右に引っ張った場合\n\n\n\n\n\n次は初期値をランダムとして９回実行して得る結果である：\n\n\nexperiment.r\n\nfor (i in 1:9) {\n  execution_time &lt;- system.time({\n    fit &lt;- stan(\"Files/bafumi_normal.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n  })['elapsed']\n  all_samples &lt;- extract(fit, pars = \"X\")$X\n  last_1000_samples &lt;- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n  plot_dataframe &lt;- data.frame(\n      legislator = colnames(Rehnquist)[1:9],\n      mean = apply(last_1000_samples, 2, mean),\n      lower = apply(last_1000_samples, 2, quantile, probs = 0.025),\n      upper = apply(last_1000_samples, 2, quantile, probs = 0.975)\n  )\n  title &lt;- paste0(\"Elapsed time: \", execution_time, \" seconds\")\n  p &lt;- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n      geom_point() +\n      geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n      labs(x = \"Mean\", y = \"Legislator\", title = title)\n  ggsave(paste0(\"Files/experiment_\", i, \".png\"), p)\n}\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.99 seconds\n\n\n\n\n\n\n\nElapsed time: 17.01 seconds\n\n\n\n\n\n\n\nElapsed time: 16.30 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 15.00 seconds\n\n\n\n\n\n\n\nElapsed time: 14.98 seconds\n\n\n\n\n\n\n\nElapsed time: 15.06 seconds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElapsed time: 16.42 seconds\n\n\n\n\n\n\n\nElapsed time: 16.57 seconds\n\n\n\n\n\n\n\nElapsed time: 17.18 seconds\n\n\n\n\n\nなんと，緩やかな情報伝達に拘らず，判事の理想点はほぼ完全に識別されているが，方向が違う！\nサンプリングを繰り返すことで結果がよく移り変わる．即ち，この事後分布は２峰あるようである．\nそして burn-in 後の 1000 回のサンプリング期間では，異なる峰の間を移り変わることはほとんどないようである．\nこの点については次の稿も参照：\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点解析パッケージ一覧",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#理想点解析パッケージ一覧",
    "title": "理想点解析のハンズオン",
    "section": "3 理想点解析パッケージ一覧",
    "text": "3 理想点解析パッケージ一覧\n本節では次の３つのパッケージを紹介する：\n\n\n\n\n\n\n\npscl (Zeileis et al., 2008)：GitHub, CRAN．(Arnold, 2018) も参照．\nMCMCpack (Martin et al., 2011)：GitHub, CRAN\nemIRT (Imai et al., 2016)：GitHub, CRAN\n\n\n\n\n\n3.1 pscl パッケージ\ninstall.packages(\"pscl\")\n\n3.1.1 voteview データ\nこのパッケージでは，Keith T. Poole と Howard Rosenthal が 1995 年から運営しているサイト voteview.com のデータを利用するための関数 readKH() が提供されている．\n例えば連邦議会 (U.S. Congress) 117 議会期 (Congress) 2021.1.3-2023.1.3 の上院 (Senate) の点呼投票データを読み込むには以下のようにする：2\n\nlibrary(pscl)\ns117 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S117_votes.ord\",\n                desc=\"117th U.S. Senate\")\n\ns117 は rollcall オブジェクト，８つのフィールドを持った配列である．\ns117$votes データは \\(n=104\\) 議員の計 \\(m=949\\) 回の投票からなる \\(10\\)-値の行列である．\n\nsummary(s117)\n\n\nSummary of rollcall object s117 \n\nDescription:     117th U.S. Senate \nSource:      https://voteview.com/static/data/out/votes/S117_votes.ord \n\nNumber of Legislators:       104\nNumber of Roll Call Votes:   949\n\n\nUsing the following codes to represent roll call votes:\nYea:         1 2 3 \nNay:         4 5 6 \nAbstentions:     7 8 9 \nNot In Legislature:  0 \n\nParty Composition:\n    D Indep     R \n   50     2    52 \n\nVote Summary:\n               Count Percent\n0 (notInLegis)  3544     3.6\n1 (yea)        55542    56.3\n6 (nay)        35995    36.5\n7 (missing)        5     0.0\n9 (missing)     3610     3.7\n\nUse summary(s117,verbose=TRUE) for more detailed information.\n\n\n\n\n3.1.2 点呼投票データ\n点呼投票データとは \\(n\\times m\\) の行列で，そのエントリーは２値変数である（今回は \\(1\\) か \\(6\\)）．\nしかし実際には種々の欠測により，\\(0,7,9\\) も使われる．\nこれをヒートマップで可視化してみる．\n\nlibrary(tidyverse)\n\nvotes_df &lt;- as.data.frame(s117$votes[1:15, 1:15]) %&gt;% rownames_to_column(\"Legislator\")  # 投票データをデータフレームに変換し、行名を列として追加\n\nvotes_long &lt;- votes_df %&gt;% pivot_longer(cols = -Legislator, names_to = \"Vote\", values_to = \"value\")  # データを長形式に変換\n\n\nggplot(votes_long, aes(x = Vote, y = Legislator, fill = value)) + geom_tile() + scale_fill_gradient(low = \"white\", high = \"red\") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Votes\", y = \"Legislators\", title = \"Voting Patterns\")  # ヒートマップを作成\n\n\n\n\n\n\n\n\n\n\n3.1.3 政党毎の賛成率\n政党でソートし，賛成率を最初の 15 法案についてプロットしたものは次の通り：\n\n\nCode\nlibrary(dplyr)\n\n# 政党ごとの賛成票の割合を計算\nparty_votes &lt;- s117$votes %&gt;%\n  as.data.frame() %&gt;%\n  mutate(party = s117$legis.data$party) %&gt;%\n  group_by(party) %&gt;%\n  summarise(across(everything(), ~mean(. == 1, na.rm = TRUE)))\n\n# データを長形式に変換\nparty_votes_long &lt;- party_votes %&gt;% pivot_longer(cols = -party, names_to = \"Vote\", values_to = \"value\")\n\n# DとRのデータのみを抽出\nparty_votes_d &lt;- party_votes_long %&gt;% filter(party == \"D\")\nparty_votes_r &lt;- party_votes_long %&gt;% filter(party == \"R\")\n\n# Democrats (D) のデータのみをプロット\nggplot(party_votes_d, aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value)) +\n  geom_line(color = \"blue\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\",\n       title = \"Proportion of Yea votes for Democrats\")\n\n\n\n# Democrats (D) と Republicans (R) のデータを同じプロットに追加\nggplot() +\n  geom_line(data = party_votes_d[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Democrat\"), linewidth = 0.5) +\n  geom_line(data = party_votes_r[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Republican\"), linewidth = 0.5) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\", color = \"Party\",\n       title = \"Proportion of Yea votes by Party\")\n\n\n\n\n\n\n\n\n民主党の 0-1 がはっきりした投票行動が見られる．\n\ns109 &lt;- readKH(\"https://voteview.com/static/data/out/votes/S109_votes.ord\",\n                desc=\"109th U.S. Senate\")\n\nAttempting to read file in Keith Poole/Howard Rosenthal (KH) format.\nAttempting to create roll call object\n109th U.S. Senate \n102 legislators and 645 roll calls\nFrequency counts for vote types:\nrollCallMatrix\n    0     1     6     7     9 \n  645 40207 22650     1  2287 \n\n\n\n\n3.1.4 ベイズ推定\npscl パッケージでは，rollcall オブジェクトに対して ideal() 関数を用いてデータ拡張に基づく Gibbs サンプラーを通じた理想点解析を行うことができる．\nideal() 関数のマニュアル に記載された例では maxiter=260E3, burnin=10E3, thin=100 での実行が例示されているが，ここでは簡単に実行してみる．\n\nn &lt;- dim(s117$legis.data)[1]\nx0 &lt;- rep(0,n)\nx0[s117$legis.data$party==\"D\"] &lt;- -1\nx0[s117$legis.data$party==\"R\"] &lt;- 1\n\nlibrary(tictoc)\ntic(\"ideal() fitting\")\n\nid1 &lt;- ideal(s117,\n             d=1,\n             startvals=list(x=x0),\n             normalize=TRUE,\n             store.item=TRUE,\n             maxiter=10000,  # MCMCの反復回数\n             burnin=5000,\n             thin=50,  # 間引き間隔\n             verbose=TRUE)\ntoc()\n\nideal() fitting: 43.938 sec elapsed であった．\n\nplot(id1)\n\nLooking up legislator names and party affiliations\nin rollcall object s117 \n\n\n\n\n\n\n\n\n\nplot.ideal() 関数のマニュアル にある通り，shoALLNames = FALSE がデフォルトになっている．\n\nsummary(id1)  # 全議員の正確な推定値が見れる．\n\nもっとも保守的な議員として Trump，５番目にリベラルな議員として Biden の名前がみえる．Harris は中道である．\n\n\n\n3.2 MCMCpack パッケージ\n\n3.2.1 ロジットモデルの推定\n\nlibrary(MCMCpack)\n# データの生成\nx1 &lt;- rnorm(1000)  # 説明変数1\nx2 &lt;- rnorm(1000)  # 説明変数2\nXdata &lt;- cbind(1, x1, x2)  # デザイン行列\n\n# 真のパラメータ\ntrue_beta &lt;- c(0.5, -1, 1)\n\n# 応答変数の生成\np &lt;- exp(Xdata %*% true_beta) / (1 + exp(Xdata %*% true_beta))\ny &lt;- rbinom(1000, 1, p)\n\n# MCMClogitでサンプリング\nposterior &lt;- MCMClogit(y ~ x1 + x2,    # モデル式\n                      burnin = 1000,    # バーンイン期間\n                      mcmc = 10000,     # MCMCの反復回数\n                      thin = 1,         # 間引き数\n                      verbose = 1000)   # 進捗表示間隔\n\n\n# 結果の確認\nsummary(posterior)\n\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean      SD  Naive SE Time-series SE\n(Intercept)  0.4464 0.07613 0.0007613       0.002471\nx1          -0.8935 0.08239 0.0008239       0.002677\nx2           0.9617 0.08706 0.0008706       0.002880\n\n2. Quantiles for each variable:\n\n               2.5%     25%     50%     75%   97.5%\n(Intercept)  0.2962  0.3953  0.4464  0.4972  0.6027\nx1          -1.0560 -0.9485 -0.8931 -0.8391 -0.7310\nx2           0.7910  0.9017  0.9608  1.0206  1.1369\n\nplot(posterior)\n\n\n\n\n\n\n\n\n\n\n3.2.2 変化点解析\n(Chib, 1998) に基づく変化点モデルのベイズ推定の関数 MCMCpoissonChange() も実装されている．詳しくは (Martin et al., 2011) 第4節参照．\n\n\n\n3.3 emIRT パッケージ\ninstall.packages(\"emIRT\")\nこのパッケージには備え付けの 80-110 議会期の上院における点呼投票データ dwnom がある．\nこのデータに対して，階層モデルを用いた理想点解析を行う関数 hierIRT() がある．\n\nlibrary(emIRT)\ndata(dwnom)\n\n## This takes about 10 minutes to run on 8 threads\n## You may need to reduce threads depending on what your machine can support\nlout &lt;- hierIRT(.data = dwnom$data.in,\n                    .starts = dwnom$cur,\n                    .priors = dwnom$priors,\n                    .control = {list(\n                    threads = 8,\n                    verbose = TRUE,\n                    thresh = 1e-4,\n                    maxit=200,\n                    checkfreq=1\n                        )})\n\n## Bind ideal point estimates back to legislator data\nfinal &lt;- cbind(dwnom$legis, idealpt.hier=lout$means$x_implied)\n\n## These are estimates from DW-NOMINATE as given on the Voteview example\n## From file \"SL80110C21.DAT\"\nnomres &lt;- dwnom$nomres\n\n## Merge the DW-NOMINATE estimates to model results by legislator ID\n## Check correlation between hierIRT() and DW-NOMINATE scores\nres &lt;- merge(final, nomres, by=c(\"senate\",\"id\"),all.x=TRUE,all.y=FALSE)\ncor(res$idealpt.hier, res$dwnom1d)"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/IdealPoint1.html#footnotes",
    "href": "posts/2024/TransDimensionalModels/IdealPoint1.html#footnotes",
    "title": "理想点解析のハンズオン",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Souter was nominated to the Supreme Court without a significant”paper trail” but was expected to be a conservative justice. Within a few years of his appointment, Souter moved towards the ideological center. He eventually came to vote reliably with the Court’s liberal wing.” Wikipedia より引用．↩︎\n１つの議会期 (Congress) は２つの会期 (Session)，第１会期と第２会期から構成される．↩︎"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#はじめに",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "1 はじめに",
    "text": "1 はじめに\nZig-Zag サンプラーなどの PDMP サンプラーは \\(\\mathbb{R}^d\\) またはその領域上の確率分布からサンプリングするための，連続時間アルゴリズムである．\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n一方でこれらのサンプラーは離散空間上では使えない．\nその際は連続時間で動く PDMP と，Metropolis-Hastings 法などの従来の MCMC 手法を統合して動かす必要がある．\nこのように離散空間と連続空間の合併上で動くサンプラー（の一部）を (Tierney, 1994) は hybrid サンプラーと呼んでいる．\nしかしこの名前は hybrid Monte Carlo (Duane et al., 1987) と紛らわしいから，(Green, 1995, p. 714) から “traverse” sampler とここでは呼ぶことにする．\n(Sachs et al., 2023) は Zig-Zag サンプラーともう１つの離散時間 MCMC を，Gibbs 様の考え方で組み合わせた GZZ (Gibbs Zig-Zag) サンプラーを提案した（第 2 節）．\n一方で (Hardcastle et al., 2024) では，旧来は点過程からのサンプリングに用いられていた技術であった Birth-Death 過程を用いて統合する方法が提案されている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#sec-GZZ",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#sec-GZZ",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "2 Gibbs Zig-Zag サンプラー",
    "text": "2 Gibbs Zig-Zag サンプラー\n\n2.1 はじめに\n\\(\\zeta\\in\\mathbb{R}^d\\) からのサンプリングを， \\[\n\\zeta=(\\xi,\\alpha)\\in\\mathbb{R}^p\\times\\mathbb{R}^r,\\qquad p+r=d,\n\\] というように分解して考え，\\(\\xi\\in\\mathbb{R}^p\\) には Zig-Zag サンプラーを適用するが，\\(\\alpha\\in\\mathbb{R}^r\\) にはしないとする．\nこのような例は階層モデル \\[\nX_i\\text{i.i.d.}p(x|\\xi),\\qquad\\xi|\\alpha\\sim p(\\xi|\\alpha),\\qquad\\alpha\\sim p(\\alpha),\n\\] の文脈で自然に現れる．実際，ポテンシャル（負の対数尤度関数）は \\[\nU(\\zeta)=U^0(\\xi,\\alpha)+\\sum_{i=1}^NU^i(\\xi),\n\\] \\[\nU^0(\\xi,\\alpha)=-\\log p(\\xi|\\alpha)-\\log p(\\alpha),\\qquad U^i(\\xi)=-\\log p(x_i|\\xi),\n\\] と表せる．\n\n\n2.2 サンプラーの設計\n\\(\\xi\\) の Zig-Zag サンプラーの生成作用素を \\(L_\\xi\\) とする．\\(\\alpha\\) からサンプリングをする MCMC の確率核を \\(Q\\) とし，ある定数 \\(\\eta&gt;0\\) をパラメータにもつ Poisson 点過程が到着するたびに \\(Q\\) により \\(\\alpha\\) の値を動かすとする．\nすると全体としてのサンプラーの生成作用素は次のように表せる： \\[\nL=L_\\xi+\\eta L_\\alpha,\n\\] \\[\nL_\\alpha f((\\xi,\\theta),\\alpha)=\\int_{\\mathbb{R}^r}\\biggr(f((\\xi,\\theta),\\alpha')-f((\\xi,\\theta),\\alpha)\\biggl)Q(\\alpha,d\\alpha').\n\\]"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#traverse-サンプラー",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#traverse-サンプラー",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "3 Traverse サンプラー",
    "text": "3 Traverse サンプラー\n\n3.1 はじめに\n(Koskela, 2022) により，任意の可算空間 \\(\\mathbb{F}\\) に対して，\\(\\mathbb{F}\\) と連続空間との合併からサンプリングをするサンプラーが提案されている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTraverse.html#関連記事",
    "href": "posts/2024/TransDimensionalModels/BayesTraverse.html#関連記事",
    "title": "連続・離散を往来する MCMC サンプラー",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html",
    "title": "超次元 MCMC",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#関連ページ",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#関連ページ",
    "title": "超次元 MCMC",
    "section": "関連ページ",
    "text": "関連ページ\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#はじめに",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#はじめに",
    "title": "超次元 MCMC",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 問題設定\n本稿では，状態空間が \\[\nE=\\bigcup_{k\\in[K]}E_k,\\qquad E_k:=\\{k\\}\\times\\mathbb{R}^{n_k},\n\\tag{1}\\] により定義される場合を考える．\n空間の次元 \\(n_k\\) は一定とは限らないため，このような状態空間 \\(E\\) 上に適用される MCMC を 超次元 (trans-dimensional) MCMC と呼ぶ．\n\nこのような設定はまずモデル選択において自然に生じる．超次元 MCMC により，モデル（の添え字）の空間 \\[\n[K]:=\\{1,\\cdots,K\\}\n\\] 上に定まる周辺事後分布を通じて，モデル選択をすることができる．\n\n\n1.2 例\n最初に超次元 MCMC を考えた (Green, 1995) では炭鉱事故データに Poisson モデルを適用し，そのレート関数の変化点解析に応用している．\n変化点の存在も数 \\(k\\in\\mathbb{N}\\) も不明であるとするとき，\\(k\\) 個の変化点の位置と，これによって生じる \\(k+1\\) 個の区間上のレートの値で，モデルの次元 \\[\nn_k=2k+1\n\\] が変化する設定が自然に生じる．\n続いて２次元の変化点解析として，画像の区分け (segmentation) や物体認識の問題が Voronoi 分割を通じて扱えることが説明されている (Green, 1995, p. 724)．\n似た発想として，節点の個数と位置を不定として曲線のフィッティングをする問題にも (Denison et al., 1998) で応用されている．\n(Richardson and Green, 1997) では多峰性のある酵素活性データに対して正規混合モデルを適用しており，そのサンプリングに混合比に応じてパラメータ空間の間をジャンプする超次元 MCMC を応用している．\n現代では混合数 \\(K\\) も不定としてデータから学ぶベイズ推定は当然のものとなっているが，初の分析 (Richardson and Green, 1997) は超次元 MCMC の開発 (Green, 1995) を待って初めて可能になったものである．\n\n\n1.3 滞在時間比によるモデル事後確率の推定\nモデル \\(k\\in[K]\\) に関する 周辺尤度 (marginal likelihood) とは，尤度をパラメータ \\(\\theta\\) の事前分布に関して平均をとったもの \\[\np(D|k)=\\int_{E_k}p(D|k,\\theta_k)p(\\theta_k|k)\\,d\\theta_k\n\\] をいう．\\(D\\) はデータとした．よく \\(m_k(D)\\) とも表記される．\n周辺尤度の比 \\[\nB_{k',k}:=\\frac{p(D|k')}{p(D|k)}\n\\] は Bayes 因子 (Kass and Raftery, 1995) と呼ばれる．\n超次元 MCMC によるモデル選択の美点の１つとして，事後モデル確率 \\[\np(k|D)\\,\\propto\\,p(D|k)p(k)\n\\] が，MCMC のモデル \\(k\\in[K]\\) への滞在時間の割合によって簡単に計算できることが挙げられる．この方法によるモデル選択は (Carlin and Polson, 1991) などの極めて初期の論文でも提案されている．\nただし，全ての周辺尤度 \\(p(D|k)\\;k\\in[K]\\) が得られればベイズ因子も事後モデル確率も計算できるため，これらの値を得るためだけならば必ずしも MCMC による必要はないことに注意する．または MCMC の出力を使ってより効率的に推定することができる (Chib, 1995)．\n(Han and Carlin, 2001) は簡便性と得られる情報量とについて，超次元 MCMC と直接周辺尤度を推定する方法とはトレードオフの関係にあるとしている．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#可逆な方法",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#可逆な方法",
    "title": "超次元 MCMC",
    "section": "2 可逆な方法",
    "text": "2 可逆な方法\n\n2.1 はじめに\n本稿ではまず，詳細釣り合い条件を満たすような MCMC のみに焦点を絞って，次元を超える MCMC をどのように構成できるかを議論する．\n一般論（第 2.2 節）が (Green, 1995) で議論されている．結局こうして定義されるクラスは，より一般の空間への MH アルゴリズムの拡張となっている：\n\nIn brief, reversible jump MCMC is a random sweep Metropolis± Hastings method adapted for general state spaces. (Richardson and Green, 1997, p. 736)\n\n\n\n2.2 一般論\n目標分布 \\(\\pi\\in\\mathcal{P}(E)\\) に対して提案核が \\[\nq(x,-)=\\sum_{m=1}^Mq_m(x,-)\n\\] という形で用意されている場合に，棄却率 \\(\\alpha_m(x,x')\\) をどう設定すれば所望の \\(\\pi\\) に収束する Markov 連鎖が得られるかを考える．\n\n\n\n\n\n\n(Green, 1995, p. 715)\n\n\n\n\\(\\pi\\otimes q_m\\) がある \\(E^2\\) 上の対称な測度に関して密度 \\(f_m\\) を持つとする．このとき， \\[\n\\alpha_m(x,x')f_m(x,x')=\\alpha_m(x',x)f_m(x',x)\n\\] を満たすならば，Metropolis-Hastings サンプラーは \\(\\pi\\) を不変分布にもつ．\n\n\n(Peskun, 1973) の含意として受容確率はなるべく高い方が良いから，結局は \\[\n\\alpha_m(x,x')=1\\land\\frac{\\pi(dx')q_m(x',dx)}{\\pi(dx)q_m(x,dx')}.\n\\] と取ることにあたる．\nこの命題の前提条件である，「\\(\\pi\\otimes q_m\\) がある \\(E^2\\) 上の対称な測度に関して密度 \\(f_m\\) を持つ」という条件は dimension-matching requirement と呼ばれる．\n仮に \\(q_m\\) はモデル間のジャンプのみを提案する核であるとする．簡単のために次元は下がるとしよう．その際，ジャンプ先の次元と同じ次元を持った部分多様体上でしか \\(q_m\\) によるジャンプは起こってはいけないことが要求される．\n\n\n2.3 ジャンプによる方法\n\n2.3.1 Jump-Diffusion を用いた方法\n次元の違う空間をジャンプによって往来するという発想自体は (Grenander and Miller, 1994) が画像認識の分野で提案している．この論文では birth-death の代わりに creation-annihilation と呼ばれている．\nジャンプは Poisson レートに従って起こり，それ以外の間は Langevin 拡散によって平衡分布からのサンプリングを行う．\n後にこれを一般のモデル選択に応用したものが (Phillips and Smith, 1995) で議論されており，(Green, 1995) の可逆超次元 MCMC 2.2 の特別な場合に当たる (Green, 1995, p. 716)．\nなお，MALA (Metropolis-adjusted Langevin algorithm) はこの論文 (Grenander and Miller, 1994) へのコメント (J. E. Besag, 1994) で提案されたものである．\n\n\n2.3.2 PDMP への Jump の導入\nJump-Diffusion と全く同様の発想で，PDMP に可逆なジャンプを導入することを提案したのが (Chevallier et al., 2023) である．\nただし (Grenander and Miller, 1994) の提案のように，特定のレートに従ってジャンプをするのではなく，特定の active boundary に到達したら一定の確率でジャンプをするというようにトリガーを決める．\nこうすることで超次元ジャンプを起こすために追加の Poisson 過程をシミュレートする必要がなくなり，計算コストが落ちる．(Grenander and Miller, 1994) の方法は，その意味で大変計算コストが高いものだったのである．\nそして (Chevallier et al., 2023) の Reversible Jump PDMP では，ジャンプする度に「いつ元のモデルに戻ってくるか」を決める指数到着時間をシミュレートし，それまでの間別のモデルに滞在する，という動きをする．\n\n\n\n2.4 混合モデルへの応用\n\n2.4.1 Split-Merge の導入\n前述の通り (Richardson and Green, 1997) は混合モデルからのサンプリングに超次元 MCMC を応用している．\nここでは超次元跳躍は，新たな混合成分の導入／削除に用いられる．(Richardson and Green, 1997) は birth-death と呼んでいる．\nそして混合成分が追加／削除される前後に，既存の成分の分割／合併 (split-merge) を行うことで，混合モデルからのサンプリングを行う．\nこの際 (Richardson and Green, 1997) は２次までの積率を変えないように split-merge を行うことを提案しており，これを 積率マッチング (moment matching) ともいう (Fan et al., 2024, p. 10)．\n\n\n2.4.2 Birth-Death 点過程によるサンプリング\n(Stephens, 2000) は混合モデルのパラメータを，複数の \\(\\Theta\\) 上の点を重み付きで足し合わせたものと見ることで，\\(\\Theta\\) 上の事後分布を \\(\\Theta\\) 上の \\([0,1]\\)-印付き点過程をシミュレーションすることを通じて得られることを示した．\n実はこの見方は，第 2.2 節のサンプラーの極限として得られるものである (Cappé et al., 2003)．\n同様のメカニズムを，総リスク数不明の競合リスクモデルに PDMP サンプラーと組み合わせて，ベイズモデル平均に適用したのが (Hardcastle et al., 2024) である．\n\n\n\n2.5 積空間上の方法\n\n2.5.1 ベイズモデルとしての吸収\n超次元 MCMC と同じタイミング (Carlin and Chib, 1995) で，比較したい２つのモデルがあった場合，これらを結合し，足りない specification には “psudo-prior” を設定することで一つの巨大なモデルとみなし，Gibbs サンプラーによって推論するという発想があった．\nこの考え方は現代もモデル平均につながるが，比較したいモデルが多かったり，\\(K\\) を無限大にしたい場合は pseudo-prior の設定が煩雑である．\nということでこの方法は必ずしもスケールしないが，超次元 MCMC を包含する広いクラスの手法を特別な場合として含む (Godsill, 2001)．\n\n\n2.5.2 population-based method\npopulation-based methods の考え方を導入することで，超次元 MCMC の収束を加速することができる．"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#非可逆な方法",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#非可逆な方法",
    "title": "超次元 MCMC",
    "section": "3 非可逆な方法",
    "text": "3 非可逆な方法\n\n3.1 リフティングを用いる方法\nリフティングは状態空間を拡張し，MH 法を非可逆にする方法である (Diaconis et al., 2000), (Chen et al., 1999)．\nこの方法をモデル間のジャンプに応用したものが (Gagnon and Doucet, 2020) で議論されている．\n\n\n3.2 非可逆なサンプラーを用いる方法\n連続時間ベースの MCMC 法は，その非可逆なダイナミクスから従来の MCMC よりも良い収束性を持つ (Andrieu and Livingstone, 2021)．\n連続時間 MCMC は従来とは全く異なるアルゴリズムをもち，Poisson 過程の到着によりランダムなジャンプをし，それまでは決定論的な動きを続ける（Zig-Zag サンプラーや BPS サンプラーでは直進）．\nこのようなサンプラーにモデル間の移動を導入するには，新たなタイマー（Poisson 過程）を導入して，その到着のたびにジャンプをすれば良い．\nこの考え方を推し進めることで，非可逆なモデル間ジャンプをデザインすることができる．\n\n\n3.3 Sticky PDMP\nモデル選択の文脈では，\\(E_k\\) の間に自然な包含関係がある場合が多い．特に飽和モデルを \\(x\\in\\mathbb{R}^p\\) として，この係数に spike-and-slab 事前分布 (Mitchell and Beauchamp, 1988) を仮定して変数選択をする状況を考える： \\[\np(dx)=\\prod_{i=1}^p\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl).\n\\]\n\\(\\mathbb{R}^p\\) 上でサンプリングを開始し，特定の部分空間に到達する（＝どれかの座標成分が \\(0\\) になる）たびに，その部分モデルにどれくらいの時間とどまるかを決める「タイマー」を開始し，その間部分空間内のみを探索する，と設計する．\nタイマーが鳴った際は止めていた（速度成分を \\(0\\) にしていた）座標成分を，タイマーが開始された状況と同じ速度で動かし始める．\nこうして得られるサンプラーは Sticky PDMP (Bierkens et al., 2023) と呼ばれ，非可逆なサンプラーダイナミクスに依存してタイマーが発動するために，モデル間の非可逆なジャンプを達成することになる．\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.4 境界の導入による方法\nモデル選択の文脈では \\(E_k\\) の間に自然な包含関係があった (nested models) が，合祖木の空間やグラフの空間など，従来から Monte Carlo シミュレーションが困難な離散構造は多く知られている．\nそこで一般的な設定を考えたいが，その際に使える Sticky PDMP の一般化のような方法が (Koskela, 2022) で提案されている．\nこの方法では \\(\\mathbb{F}\\) を一般的な可算集合とし， \\[\n\\mathbb{F}\\leftarrow\\bigsqcup_{k\\in\\mathbb{F}}\\Omega_m=:\\Omega,\\qquad\\Omega_m\\overset{\\mathrm{open}}{\\subset}\\mathbb{R}^d,\n\\] をその上のファイバー束とする．各被覆の境界 \\[\n\\partial\\Omega:=\\bigsqcup_{k\\in\\mathbb{F}}\\partial\\Omega_k\n\\] からサンプラーが出ようとするときに，ある核に従って \\(\\mathbb{F}\\) をジャンプするとするのである．\nこれにより \\(\\mathbb{F}\\) 上でも非可逆な動きをするサンプラーが，連続変数の空間と離散変数の空間 \\(\\mathbb{F}\\) の合併上で構成できる．\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/TransDimensionalModels/BayesTrans.html#文献案内",
    "href": "posts/2024/TransDimensionalModels/BayesTrans.html#文献案内",
    "title": "超次元 MCMC",
    "section": "4 文献案内",
    "text": "4 文献案内\n\nTrans-dimensional MCMC のトピックは，MCMC の黎明期の歴史に深く関わっている．\n(J. Besag and Green, 1993)\nSticky PDMP の間は極めて画期的なアイデアになり，今後数年で PDMP サンプラーをベイズ推論の workhorse algorithm に押し上げるポテンシャルがあるものと筆者は考えている．\n実際，連続時間 MCMC アルゴリズムは従来法と大きく違い，収束が多少早い程度ではコミュニティへの浸透が遅いと思われていたが，モデル選択や高次元・多峰性分布への推論に特に優れた応用を見せ始めた今，その重要性が高まっていると言えるだろう．"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html",
    "href": "posts/2024/Julia/MCMCwithJulia.html",
    "title": "Julia による MCMC サンプリング",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html#導入",
    "href": "posts/2024/Julia/MCMCwithJulia.html#導入",
    "title": "Julia による MCMC サンプリング",
    "section": "1 導入",
    "text": "1 導入\n具体的なサンプラーは次の稿も参照：\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n1.1 依存パッケージ一覧\n\nAbstractMCMC.jl (GitHub / Juliapackages / HP) は MCMC サンプリングのための抽象的なインターフェースを提供するパッケージ．後述の Turing.jl エコシステムの基盤となる．\nLogDensityProblems.jl：対数密度を扱うフレームワーク．\n\n\n\n1.2 MCMC パッケージ一覧\n\nAdaptiveMCMC.jl (Juliapackages / Docs) は適応的な乱歩 MH アルゴリズムを提供するパッケージ．\n\n関連に AdaptiveParticleMCMC.jl (GitHub) がある．これは SequentialMonteCarlo.jl (GitHub) に基づいている．\n\nMamba.jl (Docs / GitHub) Markov chain Monte Carlo (MCMC) for Bayesian analysis in julia\nKissMCMC (Juliapackages / GitHub) は ‘Keep it simple, stupid, MCMC’ ということで，計量な MCMC を提供するパッケージ．\nDynamicHMC.jl (GitHub) は NUTS サンプラーを提供するパッケージ．Tamás K. Papp による．\nSGMCMC.jl (GitHub) は Oxford CSML チームによる，Bayesian deep learning に向けたサンプラーを提供するパッケージ．\n\n\n\n1.3 連続時間 MCMC パッケージ一覧\n\n\n\n\n\n\n連続時間 MCMC\n\n\n\n\nZigZagBoomerang.jl (GitHub / Juliapackages)\nPDMP.jl (GitHub / Docs) は 2018 年まで Alan Turing Institute によって開発されていたパッケージ．\nPiecewiseDeterministicMarkovProcesses.jl (GitHub / Docs / (Veltz, 2015) / HP of Dr. Veltz / Discource) は細胞生物学におけるモデリング手法としての PDMP を提供するパッケージである．\n\n\n\n\n\n1.4 確率的プログラミング\n代表的なものは次の２つである：\n\n\n\n\n\n\nベイズ推論のためのパッケージ\n\n\n\n\nTuring.jl (HP / GitHub / Juliapackages / (Ge et al., 2018))\nSoss.jl (GitHub / Docs)\n\n\n\nただし，現状の Soss.jl v0.21.2 (6/22/2022) は FillArrays のバージョンを 0.13.11 以下に制限してしまうため（最新は 1.11.0），これが Turing.jl の最新バージョン v0.33.1 と衝突してしまう．\nインストールは１つの環境にどちらか１つのみにすることが推奨される．\n\n\n1.5 Turing ecosystem\n\n\n\n\n\n\nTuring ecosystem 一覧\n\n\n\n\nAdvancedPS (GitHub / Docs) は Turing.jl による粒子フィルターベースのサンプラーを提供するパッケージ．\nAdvancedHMC (GitHub)\nAdvancedMH (GitHub)\nAdvancedVI (GitHub) 変分推論を提供するパッケージ．\nBijectors.jl (GitHub) 正規化流などによる分布の変換を提供するパッケージ．\n\n\n\n(Storopoli, 2021) も参照．"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html#abstractmcmc.jl-の枠組み",
    "href": "posts/2024/Julia/MCMCwithJulia.html#abstractmcmc.jl-の枠組み",
    "title": "Julia による MCMC サンプリング",
    "section": "2 AbstractMCMC.jl の枠組み",
    "text": "2 AbstractMCMC.jl の枠組み\nAbstractMCMC.jl は，DensityProblem をはじめとした AbstractModel と AbstractSampler のデータ構造を提供する，Turing エコシステムの根幹部分を支えるパッケージである．\n\n2.1 LogDensityProblem\nAbstractMCMC.jl は，Tamás K. Papp による LogDensityProblem.jl の wrapper を提供している．\nsample 関数なども，AbstractModel の他に，longdensity オブジェクトに対するメソッドも定義されている．"
  },
  {
    "objectID": "posts/2024/Julia/MCMCwithJulia.html#mcmcchains.jl-と-abstractmcmc.jl",
    "href": "posts/2024/Julia/MCMCwithJulia.html#mcmcchains.jl-と-abstractmcmc.jl",
    "title": "Julia による MCMC サンプリング",
    "section": "3 MCMCChains.jl と AbstractMCMC.jl",
    "text": "3 MCMCChains.jl と AbstractMCMC.jl\nTuring によるエコシステムは，この MCMC のデザインパターンを利用している．\n\nusing MCMCChains\nusing StatsPlots\n\n# Define the experiment\nn_iter = 100\nn_name = 3\nn_chain = 2\n\n# experiment results\nval = randn(n_iter, n_name, n_chain) .+ [1, 2, 3]'\nval = hcat(val, rand(1:2, n_iter, 1, n_chain))\n\n# construct a Chains object\nchn = Chains(val, [:A, :B, :C, :D])\n\n# visualize the MCMC simulation results\nplot(chn; size=(840, 600))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1 sample 関数\nsample 関数は，MCMCCahins での実装 と AbstractMCMC での実装 との２つがある．\n\n3.1.1 使い方\nAbstractModel に対するデフォルトは次のように始まる：\nfunction mcmcsample(\n    rng::Random.AbstractRNG,\n    model::AbstractModel,\n    sampler::AbstractSampler,\n    N::Integer;\n    progress=PROGRESS[],\n    progressname=\"Sampling\",\n    callback=nothing,\n    discard_initial=0,\n    thinning=1,\n    chain_type::Type=Any,\n    initial_state=nothing,\n    kwargs...,\n)\ninisital_params を指定した場合：\nfunction mcmcsample(\n    rng::Random.AbstractRNG,\n    model::AbstractModel,\n    sampler::AbstractSampler,\n    ::MCMCThreads,\n    N::Integer,\n    nchains::Integer;\n    progress=PROGRESS[],\n    progressname=\"Sampling ($(min(nchains, Threads.nthreads())) threads)\",\n    initial_params=nothing,\n    initial_state=nothing,\n    kwargs...,\n)\n\n\n\n3.2 autocor 関数\nChains オブジェクトに対する autocor 関数が，次のように定義されている：\nfunction autocor(\n    chains::Chains;\n    append_chains = true,\n    demean::Bool = true,\n    lags::AbstractVector{&lt;:Integer} = _default_lags(chains, append_chains),\n    kwargs...\n)\n    funs = Function[]\n    func_names = @. Symbol(\"lag \", lags)\n    for i in lags\n        push!(funs, x -&gt; autocor(x, [i], demean=demean)[1])\n    end\n\n    return summarize(\n        chains, funs...;\n        func_names = func_names,\n        append_chains = append_chains,\n        name = \"Autocorrelation\",\n        kwargs...\n    )\nend"
  },
  {
    "objectID": "posts/2024/Julia/Julia5.html",
    "href": "posts/2024/Julia/Julia5.html",
    "title": "俺のための Julia 入門（５）パッケージ作成とモジュール",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/Julia5.html#モジュール",
    "href": "posts/2024/Julia/Julia5.html#モジュール",
    "title": "俺のための Julia 入門（５）パッケージ作成とモジュール",
    "section": "1 モジュール",
    "text": "1 モジュール\n\n\n\n\n\n\n\nCode Loading\n\n\n\n\n\n1.1 はじめに\nModule は，Julia において名前空間を提供する．\ndocumentation における Base.map などである．現在 Module 内では大域的な名前を，外部から参照不可能にする方法はない． 現在の Module は @__MODULE__ マクロで確認できる．\n\nprintln(@__MODULE__)\n\nMain\n\n\nこの時の ”method” の語は，OOP に似ている使い方であるが，やはり多重ディスパッチが意識されている．Julia の多重ディスパッチにおいて，関数とは method の張り合わせである．\n\n\n1.2 module文とexport文：Module 定義\nmodule MyModule\n\nexport my_function\n\nconst global_var = 42\n\nfunction my_function(x)\n    return x * global_var\nend\n\nend  # module MyModule\n\n\n\n\n\n\n例（スコープの変化）\n\n\n\nブロック内では@__MODULE__マクロの値は更新される．\n\nmodule MyModule\n\nexport my_function\n\nconst global_var = 42\n\nfunction my_function(x)\n    return x * global_var\nend\n\nprintln(@__MODULE__)\n\nend  # module MyModule\n\nprintln(@__MODULE__)\n\nMain.MyModule\nMain\n\n\n\n\nmy_functionはMyModule.my_functionの外部からアクセス可能な関数になる．\nexport文は，この module をusingした時に，何が取り込まれるかを指定する．\n\n\n\n\n\n\n例（外部からの参照）\n\n\n\nexportしていなかったグローバル変数global_varはモジュール内部では直接参照できても，外部からはMyModule.global_varとしてアクセスしないといけない．\n\nusing .MyModule\n\nresult = my_function(2)\nprintln(result)\n\n84\n\n\n\nprintln(MyModule.global_var)\n\n42\n\n\n\nprintln(global_var)\n\nLoadError: UndefVarError: `global_var` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nUndefVarError: `global_var` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\n\nStacktrace:\n [1] top-level scope\n   @ In[6]:1\n\n\n\n\nmodule は入れ子式にできる．その際はA.B.fooというように参照する．\n\n\n1.3 using文：既存の外部 Module から名前を輸入する\n\n1.3.1 :演算子：個別の名前だけを選んで取り込む\nusing Statistics: mean, std\nusing Statistics: Statistics\nとすると，Module 内部の変数・関数名は一切入れずに，Module 名だけ取り込む．\nすると，Statistics.mean という形で利用できるようになる．using じゃ届かない import されていない名前も，全てこの方法でアクセスできる．1\n\n\n1.3.2 using .MyModule\n名前はLOAD_PATH変数に沿って検索される．\nREPL で定義した直後の module は Main.Module になっており，LOAD_PATH が通っていないので，using Module と言われてもわからない．\nこの場合は，相対 path で読み込む必要がある．.Moduleでアクセスできる．..は親の子モジュール，...は祖父母の子モジュールにアクセスする．ShellScript と同じ要領である．\n\n\n\n1.4 include文：ファイルの分割\ninclude(String)\nStringを path として評価し，そのファイルを見つけ出し，Julia 文として評価する．\npath の構文は / を用いる．2\ninclude 関数を REPL で使うと，相対 path は working directory からのものと解釈される．\n\n\n1.5 Import文：拡張を許す取り込み\nまたusing文と違って，exportに制御された暗黙の取り込みがない．\nimport Base: length\n\nlength(v::MyType) = 3\nimportを使わない方法：\nBase.length(v::MyType) = 3\n構文と:演算子はusingと同じ．"
  },
  {
    "objectID": "posts/2024/Julia/Julia5.html#パッケージ",
    "href": "posts/2024/Julia/Julia5.html#パッケージ",
    "title": "俺のための Julia 入門（５）パッケージ作成とモジュール",
    "section": "2 パッケージ",
    "text": "2 パッケージ\n\n\n\n\n\n\n\nHow to develop a Julia package\nプロジェクト環境のドキュメンテーション\n関数のドキュメンテーションの書き方\nPkgTemplates.jl (GitHub / Docs)3\nDocumenter.jl (GitHub / Docs)\n\n\n\n\n\n2.1 プロジェクトの作成\nディレクトリごと作成する場合は：\n(@v1.10) pkg&gt; generate MyPackage\n既存のディレクトリを用いる場合は：\n(@v1.10) pkg&gt; activate .\n(@v1.10) pkg&gt; instantiate\n既存のプロジェクトを有効にして REPL を起動する場合は\n$ julia --project\nこの状態で\n(MyPackage.jl) pkg&gt; add Example\nとすると，Project.tomlに依存関係が追記され，パッケージ内でusing Exampleとすることができるようになる．\n\n\n2.2 プロジェクトの定義\nプロジェクト（環境）とは，Project.tomlとManifest.toml（任意）を備えたディレクトリのことをいう．\nProject.tomlはプロジェクトが読み込む名前空間と識別子を定義する．\nこれらのファイルは抽象的には次の３つの写像を定めている：\n\n\n\n\n\n\n\nroots : name::Symbol \\(\\Longrightarrow\\) uuid:UUID\nパッケージ名nameに，一意なuuidを割り当てる．\n環境内でimport Xという構文を見つけた際，Julia は識別子 roots[:X] を検索する．\ngraph : context::UUID \\(\\Longrightarrow\\) name::Symbol \\(\\Longrightarrow\\) uuid::UUID\nroots とは違って，コンテクストによって変わり得る名前とUUIDの対応を定める．\npaths : uuid::UUID \\(\\times\\) name::Symbol \\(\\Longrightarrow\\) path::String\nuuidとnameの組に，パッケージのインストールされた場所を定める．\n\n\n\n\n写像rootsはProject.tomlの[deps]セクションによって定義される．\n\n\n2.3 PkgTemplates.jl を用いたパッケージの作成\n\n\n\n\n\n\n\nPkgTemplates User Guide\n寺崎敏志氏のブログ：PkgTemplates による Julia パッケージの作り方（前半）\n\n\n\n\n例えば次のようにして使う：\nusing PkgTemplates\n\nt = Template(;\nuser=\"162348\",\ndir=\"~/Desktop/NurtureOfStatistics\",\nauthors=\"Hirofumi Shiba\",\nplugins=[\n  License(; name=\"MIT\"),\n  Git(; manifest=false, ssh=true),\n  GitHubActions(),\n  Documenter{GitHubActions}(),\n  Codecov(),\n  Develop(),\n],\n)\n\nt(\"PDMPFlux\")\n\n\n2.4 パッケージのリリース\n\n\n\n\n\n\n\nCreating a new release\nReleases and Bots\n\n\n\n\n\n\n\n\n\n\nワークフロー\n\n\n\n\nProject.toml の versions を更新する．\ngit commit する．\ngit tag -a v0.3.0 -m \"Releasing version 0.3.0\" でタグをつける．\ngit push で GitHub にプッシュする．git push origin v0.3.0 や git push origin --tags でタグもプッシュ．\nGitHub のリリースページでリリースを作成する．\n\n\n\n\n\n2.5 Documenter.jl を用いたドキュメンテーションの作成\n\n\n\n\n\n\nワークフロー\n\n\n\nPackage Guide も参照．\n\ndocs/ ディレクトリ内に新たな環境を作成し，その環境に Documenter.jl をインストールする：\n(MyPackage) pkg&gt; activate docs/\n(docs) pkg&gt; add Documenter\ndocs/make.jl を実行する：\ncd docs/\njulia --project make.jl\nこれにより，docs/build/ ディレクトリにドキュメントが生成されるが，その内容はコミットしない．GitHub 上では documentation.yml によって gh-pages ブランチにのみプッシュされる．\n\n\n\n\n2.5.1 *.md ファイル\nデフォルトでは index.md のみがあるが，自分で追加することもできる．\n\n\n2.5.2 LaTeX 構文\n\n\n\n2.6 Debugger パッケージ\n\n2.6.1 はじめに\nコード内に @bp でブレークポイントを設定し，デバッグしたい関数呼び出しに対して Debugger.@enter でデバッグを開始する．c でブレークポイントまで実行する．\n基本的には，現在のスコープ内で次の行に行く n と，関数呼び出しに呼び込む s とを使って移動していく．s の取り消しは so でできる．\n\n\n2.6.2 基本的なステッピングの仕方\n関数呼び出しの際，f(arg1, arg2, ...) という構文を評価するのに，実際に f が呼び出されるのは最後になったりする．このような場合は sl (step last) を使う．一般に f(g(h())) の形がある場合に，f に step in することになる．\n出力されるコード脇の &gt; は次のステップで実行しようとしている行を意味する．\nw add expr によって watch list に変数を追加し，追加した変数の値は w で確認できる．"
  },
  {
    "objectID": "posts/2024/Julia/Julia5.html#footnotes",
    "href": "posts/2024/Julia/Julia5.html#footnotes",
    "title": "俺のための Julia 入門（５）パッケージ作成とモジュール",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこれで指定しないと何を取り込んだのかの制御が外部にあるままなので（module 定義内のexport文），共同開発の時は指定するのが良い．↩︎\nWindows のように，Julia は/を使う．↩︎\n日本語解説記事もある：Qiita (2023.9)．↩︎"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html",
    "href": "posts/2024/Julia/Julia2.html",
    "title": "俺のための Julia 入門（２）制御",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#概要",
    "href": "posts/2024/Julia/Julia2.html#概要",
    "title": "俺のための Julia 入門（２）制御",
    "section": "1 概要",
    "text": "1 概要\n\n\n\n\n\n\n６つの制御機能1\n\n\n\n\n複合表現\n\n\n\n\n1.1 scope 内での注意\nwhile, for block 内で global 変数を参照するときは自由でいいが，編集する際は\nglobal i += 1\nなどとする必要がある．\n一方で，if-elseif-else-endブロックでは local scope は導入されない．"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#複合表現",
    "href": "posts/2024/Julia/Julia2.html#複合表現",
    "title": "俺のための Julia 入門（２）制御",
    "section": "2 複合表現",
    "text": "2 複合表現\n複数の subexpression を順に評価し，最後の subexpression の値を返す expression を compound expression という．\n\n2.1 begin-endブロック\n\nz = begin\n    x = 1\n    y = 2\n    x + y\nend\n\n3\n\n\n\n\n2.2 ;演算子\n\n    z = (x = 1; y = 2; x + y)\n\n3"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#条件評価",
    "href": "posts/2024/Julia/Julia2.html#条件評価",
    "title": "俺のための Julia 入門（２）制御",
    "section": "3 条件評価",
    "text": "3 条件評価\n条件評価では，条件が一致する場合のみ subexpression の評価が行われ，最後に評価された subexpression の値が返される．\n\n3.1 if-elseif-else-endブロック\n\nx = 1; y = 2\n\nif x &lt; y\n    println(\"x is less than y\")\nelseif x &gt; y\n    println(\"x is greater than y\")\nelse\n    println(\"x is equal to y\")\nend\n\nx is less than y\n\n\n\n\n\n\n\n\n注\n\n\n\n\n\n\nif 1\n    println(\"true\")\nend\n\nLoadError: TypeError: non-boolean (Int64) used in boolean context\n\n\n\n\n\n\n\n3.2 ?,:演算子\na ? b : cとした場合，aがtrueならbを，falseならcを評価した結果を返す．\n\nprintln(x &lt; y ? \"x is less than y\" : \"x is greater than or equal to y\")\n\nx is less than y"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#短絡評価",
    "href": "posts/2024/Julia/Julia2.html#短絡評価",
    "title": "俺のための Julia 入門（２）制御",
    "section": "4 短絡評価",
    "text": "4 短絡評価\n\n短絡評価される演算子&&, ||を利用する． a && b aがtrueならばbを実行する．そうでないならfalseが返る． 例： n ≥ 0 && error(“n must be negative”)"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#ループ",
    "href": "posts/2024/Julia/Julia2.html#ループ",
    "title": "俺のための Julia 入門（２）制御",
    "section": "5 ループ",
    "text": "5 ループ\n\nwhile block while condition suite end\nfor block for condition suite end\n\n\nconditionの書き方は，Range型objectを用いる．\n\ni=1:5\ni in 1:5"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#例外処理",
    "href": "posts/2024/Julia/Julia2.html#例外処理",
    "title": "俺のための Julia 入門（２）制御",
    "section": "6 例外処理",
    "text": "6 例外処理\n\ntry [/ catch / finally]：try内のsuiteを処理している間に例外が生じたら，中断してcatchに移る．この２つが全て終わるとfinallyをやる． try suite catch suite #例外処理．失敗した場合の後始末． finally suite #ファイルを閉じるなど． end"
  },
  {
    "objectID": "posts/2024/Julia/Julia2.html#footnotes",
    "href": "posts/2024/Julia/Julia2.html#footnotes",
    "title": "俺のための Julia 入門（２）制御",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia Manual↩︎"
  },
  {
    "objectID": "posts/2024/Julia/Julia1.html",
    "href": "posts/2024/Julia/Julia1.html",
    "title": "俺のための Julia 入門（１）データ型",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Julia/Julia1.html#built-in-typebaseboot.jl",
    "href": "posts/2024/Julia/Julia1.html#built-in-typebaseboot.jl",
    "title": "俺のための Julia 入門（１）データ型",
    "section": "1 Built-in Type@base/boot.jl",
    "text": "1 Built-in Type@base/boot.jl\n\n\n\n\n\n\nTypes (Julia Manual)\n\n\n\n\n\n\n組み込み型一覧\n\n\n\nJulia の型木 (type tree) において，具体型はすべて葉になり，他の型の親にはならない．\nこの機能は Julia の多重ディスパッチと併せて，「構造は受け継がないが機能は継承される」型システムが実現される．"
  },
  {
    "objectID": "posts/2024/Julia/Julia1.html#原始型",
    "href": "posts/2024/Julia/Julia1.html#原始型",
    "title": "俺のための Julia 入門（１）データ型",
    "section": "2 原始型",
    "text": "2 原始型\n\nNothing型 &lt;: Any\n\nただ一つの instance であるnothingを持つ．\nsubtypes を訊くと Type[]と返ってきた．\nnothing objectはREPLでは何も表示されない．return [nothing]と同じ原理．新たな空の概念ですね，記号論が出来る．\n\nisnothing(x) -&gt; ::Bool\n整数ℤ\n\nInt8, UInt8, Int16, UInt16, Int32, UInt32, Int64, Uint64, Int128, Uint128\nInt, UInt型：システムのデフォルトSys.WORD_SIZEに応じて，上記のいずれかのエイリアスとして設定されている．僕のMacは64bitなので64．\n符号なし整数型はprefix”0x”を付けて十六進法で表す．\n÷/2\n\n整数除算\n%の共役というか．\n\n\n真理値TV={true, false}\n\nBool\n!\n\n論理否定\n\n~\n\nbit not\n\n&\n\nbit and\n\n\n\nbit or\n\n⊻ (+ Tab)\n\nbit xor\n\n\n\n\n\n\n\n\n\n右論理シフト\n\n\n\n\n\n\n\n右算術シフト\n\n&lt;&lt;\n\n左論理／算術シフト ＊論理演算はassembly言語の包みと考えるのが良い． 論理演算：word(charやintなど)では届かない，個々のbitに対する演算に対するinterfaceを実装するためにISAや言語に追加された命令操作．初期のコンピュータは「語」単位でデータを扱っていたが，そんなの粒度大きくて仕方がない，という歴史的経緯で後から追加された群．C言語では1bitまで小さいfieldを取れる．（こういう”field”の使い方を，”bit field”という．）\n\nshift : 語中の全てのbitを左や右にずらして，空いた部分には０を詰める．n bitの左/右シフトは，2^(±n)を乗ずることに等価．だから，実は配列のindexからbyte addressingに換算してどれくらいbaseからとぶかの計算は，左２シフトである．CやJavaの&lt;&lt;に当たる．\n\nAND : 以前はmaskと呼んでいた，隠すからである．maskはAND論理演算のことだったのか！高級言語の&\nOR, NOR(=ORのNOT) : 左から作用すると考えればちょうどNOT◦OR．それぞれ高級言語の:, ~(これは本来ORの実装)にあたる．\n\nNOTはNORを使われて，ISAには等価なものはない．\n\n\n===\n\nメモリ上のobjectとしての同一性．\nこんな意味論数学上にはない．\n\n\n小数\n\nFloat16, Float32, Float64\n通常は64で解釈され，32はf0やf-4のsuffixを付けて表す．\nそれぞれのサイズに対して，値Inf16, Inf32, Infが無限大を表す値として用意されている．\n0/0などの「未定義」値として，NaN16, NaN32, NaNが用意されている．\n/2\n\nx/yの共役演算\n\n\n複素数ℂ\n\n虚数単位はim\nreal(x)\nimag(x)\nconj(x)\nabs(x)\n\n任意精度演算\n\n任意精度整数：BigInt，任意精度小数：BigFloat\n\n定数\n\n宣言constをつけてから定義する．\n組込定数\n\npi\nVERSION"
  },
  {
    "objectID": "posts/2024/Julia/Julia1.html#collection型直積",
    "href": "posts/2024/Julia/Julia1.html#collection型直積",
    "title": "俺のための Julia 入門（１）データ型",
    "section": "3 Collection型：直積",
    "text": "3 Collection型：直積\n更なる構造付きのものはDataStructure.jlにある． 各直積型に，tagをつけてその性質を明示する．\n\n3.1 Collection 型\n全てのobjectに，indexing, slicingの操作が施せる．\nindexing - [n] - 最初から数えて第n byteのobjectを返す． - 日本語は3byte表現されていることに注意． - [end] - 最後の要素のindexing．長さがわからないときに使う． - [end-n] - 末尾からのindexing slicing * n:m * UnitRange{Int}型object． * for文にも使える． - [n:m] - n番目からm番目までの閉区間を切り取る． - n=mの時，indexingとは違ってString型を返す．\n\n文字列\n\nChar, String\n\nUTF-8符号化を用いているので，Unicode文字列がサポートされている．\n\nUTF-8は可変長の符号化なので，indexingについては注意しなきゃいけない．\n\n\n“ … “ #constructor\n\nString型を作る．\n\n‘ … ‘ #constructor\n\nChar型を作る．\n\n\n\n連結演算子\n\n\nstring(x[,y,…])\n\n連結コンストラクタ．\n\n$\n\n文字列補間(interpolation)演算子\n$(obj)で，objに格納されたString型オブジェクトに置換される．\n$(1+2)は評価されてから文字列に変換される．\n\nVector{Char::DataType}(s::String) -&gt; Array\n\nCharと指定したデータの配列にデータ型を変換する．するとindexingが直感的にやりやすくなる．\nこれはconstructorに(s)で作用させているのか．\n\nlength(s)\nrepeat(s,n::int)\nreplace(str, s =&gt; t)\n\n論理でいえばstr[s::=t]\n\nsplit(str, delimitar) -&gt; Array{SubString{String}, n}\n\ndelimitarの部分で分割し，n要素のString-配列を返す．\n\nstartswith(str1, str2) -&gt; TV::Bool\n\nstr1がstr2の文字列を始切片として持つ場合trueを返す．\n\nendswith(str1, str2)\njoin([str1, str2]::Array{String}, delimitar} -&gt; String\n\ndelimitarで区切りながら連結\n\nfindfirst(str1, str2) -&gt; slicing object::UnitRange{Int64}\n\nstr1の先頭から，str2にマッチする部分を検索し，見つかったらその最初の要素の範囲を閉区間で返す．\n\n\n正規表現object::Regex\n\n文字列型データの前にrをつけることで表す．\nPCRE (Perl compatible regular expressions)ライブラリでサポートされている．\nmatch(regex::Regex, string) -&gt; RegexMatch(“substring”::Substring{String})::RegexMatch\n\nmatchingがなかった場合はnothing::Nothingが返ってくる．\n\n\nRegexMatch\n\nm.match::Substring{String}：matchした文字列が格納されている．\nm.offset::Int：マッチした位置を表すindexが格納されている．\n\n配列\n\n\n\n3.2 Any直下の型\n\n(a,b,…)::Tuple{T1,T2,…}\n\nimmutableである\nArray型に対するsize関数の返り値はTuple型．\n可変長引数もTuple型のobjectとして受けられる．\n上記から観察されるように，入れ物であって，代数的構造を持たせることを意図していない．その場合はArrayを使う．\n\nnamed tuple：Typeの直積．\n\n元々NamedTuple.jlという独立したpackageだったが，0.7から統合．\n要素に数字以外のaliasでタグ付できる．immutable．\n(a=1, b=2)::NamedTuple{(:a, :b), Tuple(Int, Int)}などの記法で定義される．\nつまり，値のTupleと，Symbol型のオブジェクトのTupleとの組で表される．\n\n\nこの時の射影が，keys関数，values関数として実装されている．\n\n\nkeyは.演算子でアクセスできる．\n\nt.a\n\nSymbolをindexの代わりに使える．\n\nt[:a]\n\n一時的に使うのが普通で，本格的にはstruct, mutable structとして第一級の居住権を与えるのが良い．\n\nList：Array{T,1}としての実装．Vector{T}というエイリアスもある．\n\n追加や削除などの順序的構造が重視されるCollection型．\nスタック，キュー，両端キューはDataStructure.jlへ．\n\n\npush!(List, object) -&gt; List’\n\n要素の末尾追加\n\npushfirst!(List, object) -&gt; List’\n\n要素の先頭追加\n\npop!(List) -&gt; object\n\n要素の末尾摘出\n\npopfirst!(List)\ninsert!(List, n, object) -&gt; List’\n\nn番目の位置に追加\n\ndeleteat!(List, n) -&gt; List’\n\nn番目の要素を削除．\n\n\n辞書：Dict{K, V}という直積型\n\nconstructorは\n\nd = Dict{String, Int}()\nd[“a”] = 1\nd = Dict(key =&gt; value, key =&gt; value, …)\n\nconstructorの{}内の要素を１つ，または全て省略するとAnyとしたのと等価．\n\n\nhaskey(Dict, key) -&gt; Bool\n\nDict型objectに，所定のkeyが含まれているかを判定する．\n\n\n\nbuilt-inにIdDict型とWeakKeyDict型がある．\n\n集合：Set{T}\n\nconstructorは\n\ns = Set([1,2])\n\n即ち，１次元Arrayから生成される．というより，１次元Arrayにタグをつけたものである．\n実装は「keyのみを保持する辞書」というべきもので，辞書と同様，値の重複を無視する．\n\n\npush!(set, object)\n\n値の追加．\n\nunion(set, List)\nintersect(set, List)\nissubset(List, List) -&gt; ::Bool\nList ⊂ List -&gt; ::Bool\n\nSet型のinstanceを生成することなく集合演算ができる．\n\n\n多次元配列：Array{T,n}．Matrix{T}はArray{T,2}のエイリアスである．\n\nMATLABを踏襲している．NumPyとは所々違う．\n\nNumPyは0からindexingし，row-major orderである．これは，行列のindexingにとって，辞書式順序になる．\nしかしJuliaは1からindexingし，column-major orderである．後者は行列のindexingに沿って，第一要素のストライドが１になる．\n\n従って，Juliaは同じ行の要素の列挙が得意．線型代数のものの見方である．\n\n\n内部実装は結局一次元配列（List）であることを意識すると良い．\n\n\n\n[] (constructor)\n\n[ a b c; d e f; h i j ]\nまたは改行を入れてもいい．\n\n[]\n\n要素へアクセスする作用素．\n\nisempty(collection) -&gt; Bool\nempty!(collection) -&gt; collection\n\n空にする\n\nlength(collection) -&gt; Int\neltype(collection) -&gt; Type\n関数名の最後に!\n\n引数の一部を変更・破棄する関数\n!のつかない関数は，引数に対する破壊的変更はないので安心して使用できる，という慣習．\n例：push!，insert!\nsortは新たなobjectを返すが，sort!は引数そのものを変更してしまう．\n\nfor文に使う構文はPythonと同じ使用感．\n\nしかし，直積型を意識．\n\nfor (key, value) in d::Dict\n\n実装は，iterable型オブジェクトを介して行われる．つまり，Juliaは次のように変換して処理される．速度の問題？\n\nnext = iterate(collection)while next !== nothing　　(x,state) = next　　#suite　　next = iterate(collection, state)end\n\nJuliaはiterable型は無く，Tuple{Int, Int}である．\n\n第一要素は，「次の要素」で，第二要素は「その次の要素」のindex（やそれに値するもの）を表す整数またはnothing．\n\n従って，自作のcollection型にもfor文iterationを実現させるためには，iterate関数をディスパッチすれば良い．"
  },
  {
    "objectID": "posts/2024/Julia/Julia1.html#array型の関数",
    "href": "posts/2024/Julia/Julia1.html#array型の関数",
    "title": "俺のための Julia 入門（１）データ型",
    "section": "4 Array型の関数",
    "text": "4 Array型の関数\n\n4.1 作成\n\nconstructor\n\nArray{T}(undef, dims…)\n\n値が初期化されていないことに注意．\n\ncollect(reshape(1:9, 3, 3))\n\ncollectionから，要素を奪って配列に仕立て直す．\n\n\nzeros([T,] dims…)\nones([T,] dims…)\nfill(x, dims…)\n\n行列xI\n\nfill!(A,x)\n\n配列AをxIにする．\n\nrand([T,] dims…)\n\n一様分布でランダムに初期化した配列\n型を指定しないとFloat64で．\n\nrandn([T,] dims…)\n\n正規分布でランダムに初期化した配列\n\nsimilar(A,[T,dims…])\n\n配列Aと類似した配列を返す．\n\nreshape(A, dims…) -&gt; AbstractArray\n\n切り取る，または形を変える．\n並びはcolumn-major orderのままである．\nAにはUnitRange型も許容されるのがすごい．\n\ncopy(A)\ndeepcopy(A)\n\nAの要素も再帰的にコピーする．\n\n[A B]\n\n数学的記法の感覚で使える行列の接続．\n\nview(A, n, m) -&gt; view(::Array{T,n}, m, i)\n\nn, mはindexingまたはslicing．\nAから部分配列を抜き出す．\n返るobjectはAへの参照とindexの情報を持っているので，Aを変更するとその部分配列も変わる．\n\nこの実装は，Aが巨大すぎる場合への配慮を感じる．\n「ただし，現在の計算機による配列のコピー操作は，一般に非常に高速であるため，巨大な配列を扱うのではない限り，サブ配列を作成するよりも，通常のindexingで新たな配列を作成してしまう方が高速であることも多い．この辺りは，実際に計測してパフォーマンスを確かめてみるのが良いだろう．」\n\n\n\n\n\n4.2 属性\n\neltype(A)\nlength(A)\nndims(A)\nsize(A)\nsize(A,n)\n\nn番目の次元におけるAのサイズ\nsize(A)で返ってくるtupleの第n要素．\n\nstrides(A)\n\nAのストライド．\n第一要素は必ず１になる．\n要素同士が，一番浅い意味での一次元配列上でどれくらい離れているか．\n\nstride(A,n)\n\nn番目の次元におけるAのストライド\n\n[i, j, k, …]または[n:m, i:j, k:l, …]でindexingできる．\n\n\n\n4.3 代数的構造\n\n\n\n行列乗算\n\n\n\ndot付演算子：broadcasting．broadcast(+, A, B)のエイリアスである．broadcast!(+, A, B)とするとAが変更される．\n\ncomponent-wiseの演算．\n\nA .+ c\n\nA + cIと同じ．\n\n\ncolumn-wiseの演算．\n\nA .+ B\n\nBがAの繰り返し単位になっている場合のみ．\nつまり，size(B)とsize(A)を各要素ごとに見比べたとき，次の２条件のいずれかを満たすとき．\n\n値が同じ\nどちらかの値が１\n\n２つ目の条件として，pr_i(size(B)) | pr_i(size(A))だったらもっと使いやすかったがね．\n\n\nbroadcast関数\n\nbroadcast(f, args…)が定義．\nf.(args…)とも記述できる．\n\nただし，f=+などの時には使えない．fは関数が想定されている．\nsigmoid.(A)やexp.(A)などである．\n\nargsはArrayに限らず，tupleでも良い．\n\n\n\n\n4.4 位相的構造\n\nmap(f, c::collection…) -&gt; collection\n\ncollection cにelement-wiseにfを適用させる．\nbroadcastingやdot演算と似ているが，fが匿名関数でない場合はdot演算を使うのが良い？\n\nreduce(op, itr; [init]) -&gt; obj\n\nAをitrableと見做して，要素ごとにopに突っ込んでいく．\n従って，次元が１段階下がる．\n\nfilter(f, a::collection) -&gt; a’\n\naを要素ごとにfに突っ込んで，fがfalseを返すものについては脱落させる．"
  },
  {
    "objectID": "posts/2024/Julia/Julia1.html#footnotes",
    "href": "posts/2024/Julia/Julia1.html#footnotes",
    "title": "俺のための Julia 入門（１）データ型",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia でメソッドというと関数の多重ディスパッチのことであり，特定のクラスへの所属を含意しない．この意味で，Julia はオブジェクト志向というより関数志向・プロセス志向である．Stefan Karpinski (2019) The Unreasonable Effectiveness of Multiple Dispatch も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html",
    "href": "posts/2024/Julia/PDMPFlux.html",
    "title": "PDMPFlux.jl パッケージ",
    "section": "",
    "text": "PDMP (Piecewise Deterministic Markov Process) または 連続時間 MCMC とは，名前の通り MCMC のサンプリングを連続時間で行うアルゴリズムである：\n\n\n\nZig-Zag サンプラーが２次元の正規分布からサンプリングを実行している様子\n\n\n\n\n\n\n\n\nPDMP サンプラーの特徴\n\n\n\n\n方向転換をするオレンジ色の点を一定の法則（Poisson 点過程）に従って定めることで，軌跡全体が目標の分布に従う．\n目標の関数を緑色の軌跡上で線積分をすれば期待値が得られる．一定のステップサイズで切り出してサンプルとし，従来の MCMC output と同様に使っても良い．\n非可逆 なダイナミクスをもち，HMC や MALA などの可逆なサンプラーよりも高次元分布や多様性に強いと期待されている．\n\n\n\n詳しくは次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nusing PDMPFlux\n\n\n\n\n\n\n自動微分を使う場合（推奨）\n\n\n\n\nまずポテンシャル＝負の対数尤度を（定数倍の違いを除いて）自分で定義する：\nfunction U_Gauss(x::Vector)\n    return sum(x.^2) / 2\nend\n標準正規分布 \\(\\mathrm{N}_d(0,I_d)\\) だと，対数尤度は \\[\n\\log\\phi(x) = - \\frac{1}{2}\\sum_{i=1}^d x_i^2 - \\frac{d}{2}\\log(2\\pi)\n\\] 第二項は定数であるから無視して良い．\\(U(x) = -\\log\\phi(x)\\) とすべき点に注意（先頭のマイナス符号は落とす）．\n次のようにしてサンプラーをインスタンス化する：\ndim = 10\nsampler = ZigZagAD(dim, U_Gauss)\nハイパーパラメータを与えてサンプリングを実行する．\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\n結果の可視化を行う．\njointplot(samples)\n\n\n\n\n\n\n\n\n\n\n自動微分を使わず，ポテンシャルの勾配を手動で与える場合\n\n\n\n\n\nsampler = ZigZag(dim, grad_U, grid_size=grid_size)  # initialize your Zig-Zag sampler\noutput = sample_skeleton(sampler, N_sk, xinit, vinit, verbose = true)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, output)  # get samples from the skeleton points\nとできる．\noutput::PDMPHistory にはサンプラーの挙動を確認するための多くのメソッドが定義されている：\nplot_traj(output, 10000)\ndiagnostic(output)"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#速習-pdmp",
    "href": "posts/2024/Julia/PDMPFlux.html#速習-pdmp",
    "title": "PDMPFlux.jl パッケージ",
    "section": "",
    "text": "PDMP (Piecewise Deterministic Markov Process) または 連続時間 MCMC とは，名前の通り MCMC のサンプリングを連続時間で行うアルゴリズムである：\n\n\n\nZig-Zag サンプラーが２次元の正規分布からサンプリングを実行している様子\n\n\n\n\n\n\n\n\nPDMP サンプラーの特徴\n\n\n\n\n方向転換をするオレンジ色の点を一定の法則（Poisson 点過程）に従って定めることで，軌跡全体が目標の分布に従う．\n目標の関数を緑色の軌跡上で線積分をすれば期待値が得られる．一定のステップサイズで切り出してサンプルとし，従来の MCMC output と同様に使っても良い．\n非可逆 なダイナミクスをもち，HMC や MALA などの可逆なサンプラーよりも高次元分布や多様性に強いと期待されている．\n\n\n\n詳しくは次の記事も参照：\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nusing PDMPFlux\n\n\n\n\n\n\n自動微分を使う場合（推奨）\n\n\n\n\nまずポテンシャル＝負の対数尤度を（定数倍の違いを除いて）自分で定義する：\nfunction U_Gauss(x::Vector)\n    return sum(x.^2) / 2\nend\n標準正規分布 \\(\\mathrm{N}_d(0,I_d)\\) だと，対数尤度は \\[\n\\log\\phi(x) = - \\frac{1}{2}\\sum_{i=1}^d x_i^2 - \\frac{d}{2}\\log(2\\pi)\n\\] 第二項は定数であるから無視して良い．\\(U(x) = -\\log\\phi(x)\\) とすべき点に注意（先頭のマイナス符号は落とす）．\n次のようにしてサンプラーをインスタンス化する：\ndim = 10\nsampler = ZigZagAD(dim, U_Gauss)\nハイパーパラメータを与えてサンプリングを実行する．\nN_sk, N, xinit, vinit = 1_000_000, 1_000_000, zeros(dim), ones(dim)\nsamples = sample(sampler, N_sk, N, xinit, vinit, seed=2024)\n結果の可視化を行う．\njointplot(samples)\n\n\n\n\n\n\n\n\n\n\n自動微分を使わず，ポテンシャルの勾配を手動で与える場合\n\n\n\n\n\nsampler = ZigZag(dim, grad_U, grid_size=grid_size)  # initialize your Zig-Zag sampler\noutput = sample_skeleton(sampler, N_sk, xinit, vinit, verbose = true)  # simulate skeleton points\nsamples = sample_from_skeleton(sampler, N, output)  # get samples from the skeleton points\nとできる．\noutput::PDMPHistory にはサンプラーの挙動を確認するための多くのメソッドが定義されている：\nplot_traj(output, 10000)\ndiagnostic(output)"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#pdmp_jax-パッケージ",
    "href": "posts/2024/Julia/PDMPFlux.html#pdmp_jax-パッケージ",
    "title": "PDMPFlux.jl パッケージ",
    "section": "2 pdmp_jax パッケージ",
    "text": "2 pdmp_jax パッケージ\n本節では PDMP のシミュレーションに自動微分を応用した (Andral and Kamatani, 2024) のアルゴリズムを紹介する．\n\n2.1 デモ\n(Neal, 2003) が slice sampling のデモ用に定義した 漏斗分布 を考える： \\[\np(y,x)=\\phi(y;0,3)\\prod_{i=1}^9\\phi(x_i;0,e^{y/2}),\\qquad y\\in\\mathbb{R},x\\in\\mathbb{R}^9.\n\\]\nimport jax\nfrom jax.scipy.stats import multivariate_normal\nfrom jax.scipy.stats import norm\n\nimport jax.numpy as jnp\n\ndef funnel(d=10, sig=3, clip_y=11):\n  \"\"\"Funnel distribution for testing. Returns energy and sample functions.\"\"\"\n\n  def unbatched(x):\n    y = x[0]\n    log_density_y = - y**2 / 6\n\n    variance_other = jnp.exp(y/2)\n\n    log_density_other = - jnp.sum(x[1:]**2) / (2 * variance_other)\n\n    return - log_density_y - log_density_other\n\n  def sample_data(n_samples):\n    # sample from Nd funnel distribution\n    y = (sig * jnp.array(np.random.randn(n_samples, 1))).clip(-clip_y, clip_y)\n    x = jnp.array(np.random.randn(n_samples, d - 1)) * jnp.exp(-y / 2)\n    return jnp.concatenate((y, x), axis=1)\n\n  return unbatched, sample_data\nimport pdmp_jax as pdmp\ndim = 10\nU, _ = funnel(d=dim)\ngrad_U = jax.grad(U)\nseed = 8\nxinit = jnp.ones((dim,)) # initial position\nvinit = jnp.ones((dim,))  # initial velocity\ngrid_size = 0\nN_sk = 100000 # number of skeleton points\nN = 100000 # number of samples\nsampler = pdmp.ZigZag(dim, grad_U, grid_size)\n# sample the skeleton of the process\nout = sampler.sample_skeleton(N_sk, xinit, vinit, seed, verbose = True)  # takes only 3 seconds on my M1 Mac\n# sample from the skeleton\nsample = sampler.sample_from_skeleton(N,out)\nimport seaborn as sns\nsns.jointplot(x = sample[:,0],y = sample[:,1])\nplt.show()\n\n\nnumber of error bound : 46817\n\n\n2.2 サンプリングループの構造\n\n\n\n\n\n\ngraph TD;\n  A[\"one_step()\"] --&gt;|state.indicator=false| B[\"one_step_while()\"]\n  B --&gt; C{tp &lt;= state.horizon}\n    C --&gt;|No| D[move_to_horizon]\n    C --&gt;|Yes| E[move_before_horizon]\n    E --&gt;|state.accept=false| G[inner_while]\n    G --&gt; I{ar &lt;= 1.0}\n    I --&gt;|No| J[error_acceptance]\n    I --&gt;|Yes| K[ok_acceptance]\n    K --&gt;|reject| L[\"if_reject()\"]\n    L --&gt;|場合によっては move_to_horizon2| N[\"inner_while() か one_step_while() まで戻る\"]\n    K --&gt;|accept| O[\"if_accept()\n    state.indicator=true\n    \"]\n    D --&gt; B\n    J --&gt;|horizon を縮める| N\n\n\n\n\n\n\n\nar=lambda_t/state.lambda_bar によって Poisson thinning を行う．\nただし，lambda_bar とは近似的な上界であり，「最も近い直前の grid 上の点での値」でしかない．当然 lambda_t を超過し得る．そのような場合に error_acceptance() に入る．\nerror_acceptance() に入った場合，horizon を縮めてより慎重に同じ区間を Poisson thinning しなおす．adaptive=true の場合はこのタイミングで horizon を恒久的に縮める．\n最後 if_reject() に入った場合，horizon に到達したら one_step_while() まで戻るが，そうでない場合は inner_while() まで戻る実装がなされている．\n\n\n\n\n2.3 適応的なステップサイズ"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#pdmpflux.jl-パッケージ",
    "href": "posts/2024/Julia/PDMPFlux.html#pdmpflux.jl-パッケージ",
    "title": "PDMPFlux.jl パッケージ",
    "section": "3 PDMPFlux.jl パッケージ",
    "text": "3 PDMPFlux.jl パッケージ\n\n3.1 デモ\n\nusing PDMPFlux\n\nusing Random, Distributions, Plots, LaTeXStrings, Zygote, LinearAlgebra\n\n\"\"\"\n    Funnel distribution for testing. Returns energy and sample functions.\n    For reference, see Neal, R. M. (2003). Slice sampling. The Annals of Statistics, 31(3), 705–767.\n\"\"\"\nfunction funnel(d::Int=10, σ::Float64=3.0, clip_y::Int=11)\n\n    function neg_energy(x::Vector{Float64})\n        v = x[1]\n        log_density_v = logpdf(Normal(0.0, 3.0), v)\n        variance_other = exp(v)\n        other_dim = d - 1\n        cov_other = I * variance_other\n        mean_other = zeros(other_dim)\n        log_density_other = logpdf(MvNormal(mean_other, cov_other), x[2:end])\n        return - log_density_v - log_density_other\n    end\n\n    function sample_data(n_samples::Int)\n        # sample from Nd funnel distribution\n        y = clamp.(σ * randn(n_samples, 1), -clip_y, clip_y)\n        x = randn(n_samples, d - 1) .* exp.(-y / 2)\n        return hcat(y, x)\n    end\n\n    return neg_energy, sample_data\nend\n\nfunction plot_funnel(d::Int=10, n_samples::Int=10000)\n    _, sample_data = funnel(d)\n    data = sample_data(n_samples)\n\n    # 最初の2次元を抽出（yとx1）\n    y = data[:, 1]\n    x1 = data[:, 2]\n\n    # 散布図をプロット\n    scatter(y, x1, alpha=0.5, markersize=1, xlabel=L\"y\", ylabel=L\"x_1\", \n            title=\"Funnel Distribution (First Two Dimensions' Ground Truth)\", grid=true, legend=false, color=\"#78C2AD\")\n\n    # xlim と ylim を追加\n    xlims!(-8, 8)  # x軸の範囲を -8 から 8 に設定\n    ylims!(-7, 7)  # y軸の範囲を -7 から 7 に設定\nend\nplot_funnel()\n\nfunction run_ZigZag_on_funnel(N_sk::Int=100_000, N::Int=100_000, d::Int=10, verbose::Bool=false)\n    U, _ = funnel(d)\n    grad_U(x::Vector{Float64}) = gradient(U, x)[1]\n    xinit = ones(d)\n    vinit = ones(d)\n    seed = 2024\n    grid_size = 0  # constant bounds\n    sampler = ZigZag(d, grad_U, grid_size=grid_size)\n    out = sample_skeleton(sampler, N_sk, xinit, vinit, seed=seed, verbose = verbose)\n    samples = sample_from_skeleton(sampler, N, out)\n    return out, samples\nend\noutput, samples = run_ZigZag_on_funnel()  # ４分かかる\n\njointplot(samples)\n\n\n\nこのデモコードは Zygote.jl による自動微分を用いると 5:29 かかっていたところが，ForwardDiff.jl による自動微分を用いると 0:21 に短縮された．\n\n\n3.2 Zygote.jl と ForwardDiff.jl による自動微分\n\n\n\n\n\n\n\nJuliaDiff：Differentiation tools in Julia\nZygote (Docs / GitHub)\nForwardDiff (Docs / GitHub)\n\n\n\n\nZygote.jl は FluxML が開発する Julia の自動微分パッケージである．\n\nusing Zygote\n@time Zygote.gradient(x -&gt; 3x^2 + 2x + 1, 5)\n\n  0.830714 seconds (3.45 M allocations: 168.328 MiB, 3.95% gc time, 99.98% compilation time)\n\n\n(32.0,)\n\n\n\nf(x::Vector{Float64}) = 3x[1]^2 + 2x[2] + 1\ng(x) = Zygote.gradient(f,x)\ng([1.0,2.0])\n\n([6.0, 2.0],)\n\n\n大変柔軟な実装を持っており，広い Julia 関数を微分できる．\nForwardDiff.jl (Revels et al., 2016) は Zygote.jl よりも高速な自動微分を特徴としている．\n\nusing ForwardDiff\n@time ForwardDiff.derivative(x -&gt; 3x^2 + 2x + 1, 5)\n\n  0.068804 seconds (260.59 k allocations: 12.725 MiB, 16.89% gc time, 99.92% compilation time)\n\n\n32\n\n\nしかし定義域の次元が \\(100\\) 以上の場合は ReverseDiff.jl の方が高速になる．\n\n\n3.3 Brent の最適化\nOptim.jl は Julia の最適化パッケージであり，デフォルトで Brent の最適化アルゴリズムを提供する．\n\nusing Optim\nf(x) = (x-1)^2\nresult = optimize(f, 0.0, 1.0)\nresult.minimizer\n\n0.999999984947842\n\n\n\n\n3.4 StatsPlots.jl による可視化\nStatsPlots は現在 Plots.jl に統合されている．\nまた PDMPFlux.jl は marginalhist を wrap した jointplot() 関数を提供する．\n\n\n3.5 ProgressBars.jl による進捗表示\nProgressBars.jl は tqdm の Julia wrapper を提供する．PDMPFlux.jl ではこちらを採用して，サンプリングの実行進捗を表示する．\nなお ProgressMeter.jl も同様の機能を提供しており，有名な別の PDMP パッケージである ZigZagBoomerang.jl ではこちらを採用している．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#終わりに",
    "href": "posts/2024/Julia/PDMPFlux.html#終わりに",
    "title": "PDMPFlux.jl パッケージ",
    "section": "4 終わりに",
    "text": "4 終わりに\n\n今後の確率的プログラミングの１つの焦点は自動微分かもしれない．\n今回のパッケージ開発で，少なくとも v0.2.0 の時点では，プログラムに与える U_grad は多くの場合（10 次元の多変量 Gauss，50 次元の Banana など） Zygote.jl が少し速い（Funnel 分布では ForwardDiff.jl が速い）．\nしかし上界を構成する際の func の微分は ForwardDiff.jl の方が圧倒的に速い．大変に不可思議である．\nだから現在の実装は Zygote.jl と ForwardDiff.jl の両方を用いている．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#todo",
    "href": "posts/2024/Julia/PDMPFlux.html#todo",
    "title": "PDMPFlux.jl パッケージ",
    "section": "5 ToDo",
    "text": "5 ToDo\n\n\nZig-Zag 以外のサンプラーの実装\nZigZag(dim) は自動で知ってほしい\nTry clause 内の else を用いているので Julia 1.8 以上が必要．\nMCMCChains のような plot.jl を完成させる．\nPDMPFlux.jl のドキュメントを整備する．\nZigZagBoomerang.jl を見習って統合したり API をつけたり？\nTuring エコシステムと統合できたりしないか？\nRng を指定できるようにする？\npdmp-jax では 37 秒前後かかる Banana density の例が，PDMPFlux.jl では 2 分前後かかる．\n\nしかし，Julia の方が数値誤差が少ないのか，banana potential の対称性がうまく結果に出る．尾が消えたりしない．\n→ ForwardDiff.jl を採用したところ，02:05 から 10 分以上に変化した．ReverseDiff.jl を採用したところ 4:44 になった．50 次元というのが微妙なところなのかもしれない．\n\nFunnel 分布で試したところ，PDMPFlux.jl の棄却率が極めて高い．"
  },
  {
    "objectID": "posts/2024/Julia/PDMPFlux.html#付録他に模索した可能性",
    "href": "posts/2024/Julia/PDMPFlux.html#付録他に模索した可能性",
    "title": "PDMPFlux.jl パッケージ",
    "section": "6 付録：他に模索した可能性",
    "text": "6 付録：他に模索した可能性"
  },
  {
    "objectID": "posts/2024/Julia/Details.html",
    "href": "posts/2024/Julia/Details.html",
    "title": "Implementation Details of PDMPFlux.jl",
    "section": "",
    "text": "In this introduction, we quickly give an overview of how the PDMPFlux.jl package works, through a standard example.\nTake the Sticky Zig-Zag Sampler (Bierkens et al., 2023) as an example.\n\nsampler = StickyZigZag(dim, ∇U)\nThis instantiates the sampler, an object of an AbstractPDMP subtype.\noutput = sample_skeleton(sampler, N_sk, xinit, vinit)\nThis takes a sampler object, and returns a PDMPHistory object, by transforming the PDMPState object while pushing its snapshots into the PDMPHistory object."
  },
  {
    "objectID": "posts/2024/Julia/Details.html#introduction",
    "href": "posts/2024/Julia/Details.html#introduction",
    "title": "Implementation Details of PDMPFlux.jl",
    "section": "",
    "text": "In this introduction, we quickly give an overview of how the PDMPFlux.jl package works, through a standard example.\nTake the Sticky Zig-Zag Sampler (Bierkens et al., 2023) as an example.\n\nsampler = StickyZigZag(dim, ∇U)\nThis instantiates the sampler, an object of an AbstractPDMP subtype.\noutput = sample_skeleton(sampler, N_sk, xinit, vinit)\nThis takes a sampler object, and returns a PDMPHistory object, by transforming the PDMPState object while pushing its snapshots into the PDMPHistory object."
  },
  {
    "objectID": "posts/2024/Julia/Details.html#directory-structure-of-pdmpflux.jlsrc",
    "href": "posts/2024/Julia/Details.html#directory-structure-of-pdmpflux.jlsrc",
    "title": "Implementation Details of PDMPFlux.jl",
    "section": "2 Directory Structure of PDMPFlux.jl/src",
    "text": "2 Directory Structure of PDMPFlux.jl/src\n\n2.1 Composites.jl\n\n\n\n\n\n\ncomposites / constructs such as BoundBox, PDMPState, PDMPHistory are defined.\n\nBoundBox is basically a grid, together with the values on it, tailored to perform Poisson thinning. 3 constructors are defined in UpperBound.jl Section 2.4.\n\nboundbox.grid is the grid points. Let the dimension be n_grid, which is a field of any AbstractPDMP subtype.\nThe grid is equi-spaced, with the step size step_size.\nboundbox.box_max has n_grid - 1 elements \\[\n\\textcolor{purple}{\\mathtt{box\\_max[i]}} = \\max_{t \\in [\\textcolor{purple}{\\mathtt{grid[i]}}, \\textcolor{purple}{\\mathtt{grid[i+1]}}]} \\lambda(t),\\qquad i\\in[\\textcolor{purple}{\\mathtt{n\\_grid-1}}].\n\\]\nboundbox.cum_sum has n_grid elements, and \\[\n\\textcolor{purple}{\\mathtt{cum\\_sum[i]}} = \\sum_{j=1}^{i-1} \\textcolor{purple}{\\mathtt{box\\_max[j]}}\\times\\textcolor{purple}{\\mathtt{step\\_size}},\\qquad i\\in[\\textcolor{purple}{\\mathtt{n\\_grid}}].\n\\] Note that it is not the cumulative sum of box_max, but the cumulative sum of the piecewise constant (upper bound) function represented by box_max’s: \\[\n\\textcolor{purple}{\\mathtt{cum\\_sum[i]}} = \\int^i_0\\sum_{j=1}^{\\textcolor{purple}{\\mathtt{n\\_grid}-1}}1_{[\\textcolor{purple}{\\mathtt{grid[j]}}, \\textcolor{purple}{\\mathtt{grid[j+1]}})}(t)\\cdot\\textcolor{purple}{\\mathtt{box\\_max[j]}}\\,dt,\\qquad i\\in[\\textcolor{purple}{\\mathtt{n\\_grid}}].\n\\]\n\nPDMPState is used to keep the position of the sampler, together with caracteristics and diagnostic information. The whole SamplingLoop.jl Section 2.3 is implemented as a collection of functions that modify the PDMPState object in place. In addition to the \\((x,v,t)\\) tuple, the fields include\n\nmethods specific to the sampler.\n\nstate.rate and state.rate_vect are the rate function and its vectorized version. These slots are determined after checking the field pdmp.signed_bound in init_state() in AbstractPDMP.jl Section 3.2.\nstate.upper_bound_func() takes \\((x,v,t)\\) and returns the BoundBox object. Basically, state.upper_bound_func() is the only field that differs among samplers.\nstate.velocity_jump() takes \\((x,v,t)\\) and returns the velocity vector after the PDMP’s jump.\nstate.flow() takes \\((x_0,v_0,t)\\) and returns \\((x_t,v_t)\\), which indicates the PDMP’s position after the time \\(t\\) from \\((x_0,v_0)\\).\n\nfields to facilitate the sampling loop.\n\nboolean flags to indicate whether Poisson thinning proposal is accepted or not, which is used within ac_step_with_proxy() in the SamplingLoop.jl Section 2.3.\nproposed jump time tp and time already spent ts. They are using in SamplingLoop.jl Section 2.3 to conduct poisson thinning, where tp is the proposed jump time, and is added to ts when rejected.\nhorizon is passed to upper-bounding functions to create a BoundBox object.\n\nstatistics to diagnose the sampler dynamic.\n\n\n\n\n\n\n\n2.2 sample.jl\nsample.jl contains functions, called by users, to start MCMC sampling. The ProgressBar package is used to be friendly to users.\n\n\n\n\n\n\nsample.jl contains 2 functions, and sample() that calls them in sequel.\n\nsample_skeleton() -&gt; PDMPHistory initializes the progress bar and PDMPState & PDMPHistory objects, using init_state() and PDMPHistory() constructor respectively. Then call get_event_state() in SamplingLoop.jl Section 2.3 for iter::Int times.\nsample_from_skeleton() -&gt; Matrix{Float64} is a function that generates samples, which might subsequently be called by the plotting functions in plot.jl.\n\n\n\n\n\n\n2.3 SamplingLoop.jl\nThis module contains 10 functions. Some of them are summarized in the following figure:\n\n\n\n\n\n\n\n\n\n\n\nThe main loop for sampling is implemented here.\n\nget_event_state() -&gt; PDMPState is the entering point from sample.jl.\n\nThere is only one thing in get_event_state(), i.e., callling one_step_of_thinning() until state.accept becomes true.\nget_event_state() returns a PDMPState object with the field \\((x,v,t)\\) indicating where an accepted event happens, which is pushed to PDMPHistory back in sample_skeleton() in sample.jl Section 2.2.\n\none_step_of_thinning() returns where simulation has completed up to, with state.accept = false if any proposal isnot accepted yet.\n\nFirstly, it proposes the next jump event, by simulating exp_rv and calling next_event(upper_bound, exp_rv) -&gt; (tp, lambda_bar).\n\nupper_bound is the BoundBox object, which is calculated by state.upper_bound_func() and the current state (x,v,t).\ntp is the proposed jump time.\nlambda_bar is a upper bound, with error, for the rate function \\(\\lambda\\).\n\nSecondly, it checks whether tp &lt;= state.horizon.\n\nIf not, it calls move_to_horizon() and continues the loop one_step_of_thinning() inside get_event_state().\nIf yes, it proceeds to moves_until_horizon(), where another loop, calling ac_step(), is performed until one of the following 2 happens\n\nwhen state.accept becomes true, it gets out of the both loops.\nwhen tp &gt; state.horizon, it calls move_to_horizon2() to get out of ac_step() loop, and continues the loop one_step_of_thinning() inside get_event_state().\n\n\n\nac_step(), standing for acceptance-rejection step, is the core of Poisson thinning.\n\ncalculating the acceptance rate ar, which might exceed \\(1.0\\).\nIn that case, erroneous_acceptance_rate() is called to shrink the horizon by half, and then continues the ac_step() loop in moves_until_horizon() function. Shrinking horizon leads to a finer grid, since n_grid is fixed.\nOtherwise, it performs the Poisson thinning using the proxy rate lambda_bar, in ac_step_with_proxy() call.\n\nWithin ac_step_with_proxy(), either if_accept() or if_reject() is called depending on the result of Poisson thinning, followed by move_to_horizon2() if accepted. Whether accepted or not is informed by state.accept flag, which will lead you out of all the loops.\nIn if_accept(), the flags are updated as state.accept = true & state.accept = true, getting out of both ac_step() loop and one_step_of_thinning() loop, with the correct \\((x,v,t)\\) stored in appropriate state’s fields, which is pushed to PDMPHistory back in sample_skeleton() in sample.jl Section 2.2.\nif_reject() being called, we are still in the ac_step() loop in moves_until_horizon(), until acceptance or tp &gt; state.horizon.\n\n\n\n\n\n\n\n\n2.4 UpperBound.jl\nThis module contains next_event() function and 3 constructors for BoundBox object, with the name of upper_bound_**().\n\n\n\n\n\n\n3 constructors for BoundBox object\n\n\n\nupper_bound_**(func::Function, start::Float64, horizon::Float64) -&gt; BoundBox\n\nupper_bound_constant() computes the constant upper bound for the function func over the interval [start, horizon].\nTechnically, it returns the BoundBox object, with just 2 grid points, start and horizon. The maximum value is searched by the Brent’s algorithm via Optim.jl package.\nupper_bound_grid() computes a piecewise constant upper bound, using the n_grid grid points.\nThe box_max[i] field is the maximum value of the three points in the interval [grid[i], grid[i+1]], which are the values on the two edges func(grid[i]), func(grid[i+1]), and the intersection point of the two tangents on the two edges of the interval.\nupper_bound_grid_vect() is a LinearAlgebra implementation of upper_bound_grid().\n\n\n\n\n\n\n\n\n\nnext_event(boundbox, exp_rv) takes boundbox to propose a next event time\n\n\n\nindex = searchsortedfirst(boundbox.cum_sum, exp_rv)\nis performed to find the index that satisfies\n\\[\n\\textcolor{purple}{\\mathtt{exp\\_rv}} \\in \\left[\\textcolor{purple}{\\mathtt{cum\\_sum[index-1]}}, \\textcolor{purple}{\\mathtt{cum\\_sum[index]}}\\right).\n\\]\nFinally, t_prop that satisfies\n\\[\n\\int^\\textcolor{purple}{\\mathtt{t\\_prop}}_0 \\overline{\\lambda}(t)\\,dt = \\textcolor{purple}{\\mathtt{exp\\_rv}},\n\\] where \\(\\overline{\\lambda}(t)\\) is the piecewise constant upper-bounding function defined by \\[\n\\overline{\\lambda}(t) = \\sum_{j=1}^{\\textcolor{purple}{\\mathtt{n\\_grid}-1}}1_{[\\textcolor{purple}{\\mathtt{grid[j]}}, \\textcolor{purple}{\\mathtt{grid[j+1]}})}(t)\\cdot\\textcolor{purple}{\\mathtt{box\\_max[j]}}.\n\\]\nnext_event() returns the jump time t_prop and the corresponding upper bound value upper_bound, i.e., \\[\n\\textcolor{purple}{\\mathtt{upper\\_bound}} = \\overline{\\lambda}(\\textcolor{purple}{\\mathtt{t\\_prop}}) = \\textcolor{purple}{\\mathtt{box\\_max[index-1]}}.\n\\]\n\n\n\n\n2.5 diagnostic.jl\nfunction anim_traj(\n    history::PDMPHistory, \n    N_max::Int; \n    N_start::Int=1, \n    plot_start::Int=1, \n    filename::Union{String, Nothing}=nothing, \n    plot_type=\"2D\", \n    color=\"#78C2AD\", \n    background=\"#FFF\", \n    coordinate_numbers=[1,2,3], \n    dt::Float64=0.1, \n    verbose::Bool=true, \n    fps::Int=60, \n    frame_upper_limit::Int=10000, \n    linewidth=2, \n    dynamic_range::Bool=false\n)\n\n\n\n\n\n\narguments of anim_traj()\n\n\n\n\nhistory takes the output of sample_skeleton().\nN_start, N_max are the indeces of history to be plotted. Namely, from N_start th event, including reflection, refreshing, and thawing events, to N_max th event.\nplot_start is the starting index of the animation. The points of history.x[1:plot_start] will be already there in the first frame of the animation.\nframe_upper_limit is the maximum number of frames to be plotted.\nfps is the frame rate of the animation.\ndynamic_range is a boolean flag to determine whether to use the dynamic range for the animation.\n\n\n\n\n\n2.6 plot.jl\n\n\n\n\n\n\nThis file contains functions to plot the samples generated by sample.jl."
  },
  {
    "objectID": "posts/2024/Julia/Details.html#implementation-of-the-samplers",
    "href": "posts/2024/Julia/Details.html#implementation-of-the-samplers",
    "title": "Implementation Details of PDMPFlux.jl",
    "section": "3 Implementation of the Samplers",
    "text": "3 Implementation of the Samplers\n\n3.1 Introduction\nAll samplers are defined as subtypes of AbstractPDMP in Samplers/AbstractPDMP.jl Section 3.2.\nDifferent samplers have four different fields in PDMPState object, which are upper_bound_func, rate, rate_vect, and velocity_jump, as we learned in Section 2.1.\nThe four special fields are initialized in the constructor defined in the respective Samplers/&lt;Name&gt;.jl module.\n\n\n3.2 Samplers/AbstractPDMP.jl\nThis module contains one line\nabstract type AbstractPDMP end\nand one function, whose signature is\ninit_state(pdmp::AbstractPDMP, xinit::Array{Float64}, vinit::Array{Float64}, seed::Int) -&gt; PDMPState\nThis init_state() is basically a constructor for PDMPState, called in sample_skeleton() Section 2.2.\nIt is defined in AbstractPDMP.jl because the composite PDMPState must have different field values depending on the type of the argument pdmp.\n\n\n\n\n\n\ninit_state() mainly does two things\n\n\n\n\nCheck the field pdmp.signed_bound and modify the 3 fields rate, rate_vect, refresh_rate accordingly, which were already initialized in the respective constructor of pdmp.\nCheck the field pdmp.grid_size & pdmp.vectorized_bound and initialize the field upper_bound_func as one of the functions in the UpperBound.jl Section 2.4 module accordingly.\n\n\n\nThe remaining special fields, velocity_jump and flow are defined, together with initialization of rate and rate_vect, in the sampler specific modules, to which we will turnin the following sections.\n\n\n3.3 Samplers/ZigZagSamplers.jl\nIn this module, the declaration\nmutable struct ZigZag &lt;: AbstractPDMP\nis followed by the 2 constructors, ZigZag() and ZigZagAD(), whose signatures are\nfunction ZigZag(dim::Int, ∇U::Function; refresh_rate::Float64=0.0, grid_size::Int=10, tmax::Union{Float64, Int}=2.0, \n                    vectorized_bound::Bool=true, signed_bound::Bool=true, adaptive::Bool=true)\nfunction ZigZagAD(dim::Int, U::Function; refresh_rate::Float64=0.0, grid_size::Int=10, tmax::Union{Float64, Int}=2.0, \n                    vectorized_bound::Bool=true, signed_bound::Bool=true, adaptive::Bool=true, AD_backend::String=\"Zygote\")\nNotice the difference in ∇U and U in the arguments.\n\n\n\n\n\n\nIn ZigZag() and ZigZagAD(), two fields are initialized\n\n\n\n\nflow is defined as\n\n\\[\n  \\textcolor{purple}{\\mathtt{flow}}\\,(x_0,v_0,t) =(x_0+v_0t, v_0)\n  \\]\n\nrate_vect is defined component-wisely as\n\\[\n\\textcolor{purple}{\\mathtt{rate\\_vect[i]}}(x_0,v_0,t) = \\max(0, \\nabla U_i(x_0+v_0t)v_0),\n\\] where \\(\\nabla U=(\\nabla U_1,\\dots,\\nabla U_d)^\\top\\) is the gradient \\(\\nabla U:\\mathbb{R}^d\\to\\mathbb{R}^d\\).\nrate is the sum of rate_vect, i.e.,\n\\[\n\\textcolor{purple}{\\mathtt{rate}}(x_0,v_0,t) = \\sum_{i=1}^d \\textcolor{purple}{\\mathtt{rate\\_vect[i]}}(x_0,v_0,t).\n\\]\nsigned_rate_vect is a version of rate_vect without taking the max with \\(0\\).\n\n\n\nTwo flags signed_bound and vectorized_bound are true in default, in which case signed_rate_vect is used.\nThis is called the signed strategy, detailed in (Section 4.4.2 Andral and Kamatani, 2024).\nThe vectorized_bound is also special to the Zig-Zag samplers.\n\n\n3.4 Samplers/BouncyParticleSamplers.jl\nSimilar to the Zig-Zag samplers, mutable struct BPS and 2 constructors BPS() and BPSAD() are defined.\nDifference is that, in BPS, vectorization is not used, therefore vectorized_bound=false no matter what the user specifies.\nNote that typically Bouncy Particle Samplers need nonzero refresh rate, therefore refresh_rate=0.0 would result in erronous samples.\n\n\n3.5 Samplers/ForwardEventChainMonteCarlo.jl\nForward ECMC (Event Chain Monte Carlo) is a generalizatione of the Bouncy Particle Sampler, being free from the need of refreshing, substituting it with a ‘informed’ velocity jump.\nRegarding the implementation, however, note there is an error in the pseudo-code of (Michel et al., 2020)’s paper, and in the implementation of the pdmp_jax package.\nTo sum it up, the velocity jump is implemented separately on \\(\\mathbb{R}\\nabla U\\) and \\((\\mathbb{R}\\nabla U)^\\perp\\). To the former, the velocity is newly sampled from the invariant distribution directly, while to the latter, occasionally (tuned by mix_p) only two dimensions of them are changed. (If ran_p=false, they are swaped.)\nAs a result, the sampler loses ergodicity and confined to a certain subspace when, for example, \\(U\\) is completely isotropic and the initial velocity is proportional to its contours.\n\n\n3.6 Samplers/StickyZigZagSamplers.jl\nThis module implements the Sticky Zig-Zag sampler, for variable selection with the spike-and-slab prior.\nThe sampler takes additional argument κ, which has to be positive Float64 or Inf, determining the ‘stickyness’ of the sampler.\n\n\n3.7 StickySamplingLoops.jl\nStickyPDMP samplers have their own sampling loop, implemented in StickySamplingLoops.jl using multiple dispatch.\n\n\n\n\n\n\n\nIn one step of one_step_of_thinning_or_sticking_or_thawing(), it is first checked whether the sampler crosses any axes by \\(\\min(\\textcolor{purple}{\\mathtt{tp}}, \\textcolor{purple}{\\mathtt{tt}}, \\textcolor{purple}{\\mathtt{horizon}})0\\).\n\nIf it does cross, move_to_axes_and_stick() is called.\nElse, \\(\\min(\\textcolor{purple}{\\mathtt{tp}}, \\textcolor{purple}{\\mathtt{tt}}) &gt; \\textcolor{purple}{\\mathtt{horizon}}\\) is checked.\n\nIf it is true, move_to_horizon() is called.\n\nElse, moves_until_horizon_or_axes() is called."
  },
  {
    "objectID": "posts/2024/Julia/HMCwithJulia.html",
    "href": "posts/2024/Julia/HMCwithJulia.html",
    "title": "Hamiltonian Monte Carlo 法",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nJulia による MCMC パッケージの概観は次の稿も参照："
  },
  {
    "objectID": "posts/2024/Julia/HMCwithJulia.html#advancedhmc.jl",
    "href": "posts/2024/Julia/HMCwithJulia.html#advancedhmc.jl",
    "title": "Hamiltonian Monte Carlo 法",
    "section": "1 AdvancedHMC.jl",
    "text": "1 AdvancedHMC.jl\nCauchy 分布に HMC を適用するとぶっ壊れる！？\n\nusing AdvancedHMC, ForwardDiff\nusing LogDensityProblems\nusing LinearAlgebra\nusing Plots\n\nstruct LogTargetDensityCauchy\n    loc::Float64\n    scale::Float64\nend\n\n# Define the target distribution (1D Cauchy) using the `LogDensityProblem` interface\n\nLogDensityProblems.logdensity(p::LogTargetDensityCauchy, θ) = -log(π) - log(p.scale) - log(1 + ((θ[1] - p.loc)/p.scale)^2)\nLogDensityProblems.dimension(p::LogTargetDensityCauchy) = 1\nLogDensityProblems.capabilities(::Type{LogTargetDensityCauchy}) = LogDensityProblems.LogDensityOrder{0}()\n\nfunction HMC_sample(initial_θ)\n\n    # Choose initial parameter value for 1D\n    initial_θ = [initial_θ]\n\n    # Define the Cauchy distribution with location and scale\n    loc, scale = 0.0, 1.0\n    ℓπ = LogTargetDensityCauchy(loc, scale)\n\n    # Set the number of samples to draw and warmup iterations\n    n_samples, n_adapts = 2_000, 1\n\n    # Define a Hamiltonian system\n    metric = DiagEuclideanMetric(1)\n    hamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n\n    # Define a leapfrog solver, with the initial step size chosen heuristically\n    initial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\n    integrator = Leapfrog(initial_ϵ)\n\n    # Define an HMC sampler with the following components\n    #   - multinomial sampling scheme,\n    #   - generalised No-U-Turn criteria, and\n    #   - windowed adaption for step-size and diagonal mass matrix\n    kernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))\n    adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n\n    # Run the sampler to draw samples from the specified Cauchy distribution, where\n    #   - `samples` will store the samples\n    #   - `stats` will store diagnostic statistics for each sample\n    samples, stats = sample(hamiltonian, kernel, initial_θ, n_samples, adaptor, n_adapts; progress=true)\n\n    # Print the results\n    sample_values = [s[1] for s in samples]\n\n    p = plot(1:length(samples), sample_values,\n                label=\"HMC trajectory\",\n                title=\"1D HMC Sampler (Cauchy distribution)\",\n                xlabel=\"t\",\n                ylabel=\"X\",\n                linewidth=2,\n                marker=:circle,\n                markersize=2,\n                markeralpha=0.6,\n                color=\"#78C2AD\")\nend\n\nHMC_sample(50.0)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter ~/.julia/packages/ProgressMeter/gA66f/src/ProgressMeter.jl:590\nSampling 100%|███████████████████████████████| Time: 0:00:00\n  iterations:                                   2000\n  ratio_divergent_transitions:                  0.02\n  ratio_divergent_transitions_during_adaption:  0.0\n  n_steps:                                      3\n  is_accept:                                    true\n  acceptance_rate:                              4.073791219423035e-7\n  log_density:                                  -10.758856435114664\n  hamiltonian_energy:                           13.458401323991495\n  hamiltonian_energy_error:                     0.0\n  max_hamiltonian_energy_error:                 811.5344241268535\n  tree_depth:                                   2\n  numerical_error:                              false\n  step_size:                                    324.39138009045865\n  nom_step_size:                                324.39138009045865\n  is_adapt:                                     false\n  mass_matrix:                                  DiagEuclideanMetric([1.0])\n┌ Info: Finished 2000 sampling steps for 1 chains in 1.282884958 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=324.0), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 0.30445073109268245\n└   average_acceptance_rate = 0.22613549485908152\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 50 にした場合\n\n\n\nHMC_sample(0.0)\n\n┌ Info: Finished 2000 sampling steps for 1 chains in 0.044786417 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=14.7), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 2.018632634147994\n└   average_acceptance_rate = 0.00031706816850295914\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 0 にした場合\n\n\n\nHMC_sample(500.0)\n\n┌ Info: Finished 2000 sampling steps for 1 chains in 0.056288833 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=1670.0), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 0.2517834551566722\n└   average_acceptance_rate = 0.20632568153387063\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 500 にした場合\n\n\n\nHMC_sample(100.0)\n\n┌ Info: Finished 2000 sampling steps for 1 chains in 0.043402167 (s)\n│   h = Hamiltonian(metric=DiagEuclideanMetric([1.0]), kinetic=GaussianKinetic())\n│   κ = HMCKernel{AdvancedHMC.FullMomentumRefreshment, Trajectory{MultinomialTS, Leapfrog{Float64}, GeneralisedNoUTurn{Float64}}}(AdvancedHMC.FullMomentumRefreshment(), Trajectory{MultinomialTS}(integrator=Leapfrog(ϵ=956.0), tc=GeneralisedNoUTurn{Float64}(10, 1000.0)))\n│   EBFMI_est = 1.998017004885298\n└   average_acceptance_rate = 6.0828891145860786e-9\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスタート地点を 100 にした場合"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html",
    "href": "posts/2024/Survey/BayesGLM.html",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#はじめに",
    "href": "posts/2024/Survey/BayesGLM.html#はじめに",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "1 はじめに",
    "text": "1 はじめに\n多くの社会的なデータは非数値的である．しかしその背後には潜在的な連続変数を想定することが多い．\n加えて，線型回帰分析の結果複雑な非線型関係が予期された際，本格的なノンパラメトリック推論に移る前に，離散変数の設定に換言して非線型性を扱いやすくするなど，離散変数を扱う積極的理由もある．\n本稿ではロジスティック回帰を主に扱う．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#多項ロジスティック回帰",
    "href": "posts/2024/Survey/BayesGLM.html#多項ロジスティック回帰",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "2 多項ロジスティック回帰",
    "text": "2 多項ロジスティック回帰\n\n2.1 はじめに\nここでは BMI と LDL コレステロールの関係を見る．\n\n2.1.1 BMI と LDL の関係\n一般に HDL コレステロールは BMI と正の相関がある（特に２型糖尿病患者では (Hussain et al., 2019)）．\n\ncor(raw_df$BMI, raw_df$HDL)\n\n[1] -0.3689261\n\n\n一方で LDL コレステロールと BMI の相関は弱い：\n\ncor(raw_df$BMI, raw_df$LDL)\n\n[1] 0.158966\n\n\nしかし全く無関係ではないように見える：\n\nboxplot(\n  raw_df$LDL ~ raw_df$obesity,\n  col = c(\"pink\", \"lightgreen\", \"skyblue\"),\n  main = \"LDL classified by BMI\",\n  xlab = \"BMI\",\n  ylab = \"LDL\"\n)\n\n\n\n\n\n\n\n\nこれは LDL と BMI の関係は非線型性が高く，その非線型関係が男女，さらに年齢で違うためかもしれない (Li et al., 2021)．\nこの関係を詳しく見ていくことで何か発見があるかもしれないだろう．\n\n\n2.1.2 LAB\nLAB (Inoue et al., 2010) は酸化変性した LDL のことで，別名超悪玉コレステロールと知られる．LDL コレステロールよりも動脈硬化リスクを（特に残余リスクとして）反映する新しいバイオマーカーになり得ると期待されている (Okamura et al., 2013)．\nLAB と LDL の相関は高くない：\n\ncor(raw_df$BMI, raw_df$LAB)\n\n[1] 0.1952133\n\n\n\nboxplot(\n  raw_df$LAB ~ raw_df$obesity,\n  col = c(\"gray\", \"pink\", \"skyblue\"),\n  main = \"LAB classified by BMI\",\n  xlab = \"BMI\",\n  ylab = \"LAB\"\n)\n\n\n\n\n\n\n\n\nここでは LAB と LDL の BMI への影響を比較したい．\n\n\n2.1.3 BMI の離散化\nBMI と LDL の関数関係は非線型性が予期される (Li et al., 2021)．\nそこで BMI を直接被説明変数とするのではなく，離散化した順序変数 obesity を導入する：\nraw_df &lt;- raw_df %&gt;%\n  mutate(obesity = case_when(\n  BMI &lt; 18.5 ~ 1,  # underweight\n  BMI &lt; 25 ~ 2,    # normal\n  BMI &gt;= 25 ~ 3    # obese\n  ))\n\n\n\n2.2 LDL の予測力\nLDL を直接用いて推定すると係数が極めて小さくなるため，対数変換によりスケールを変換して説明変数に入れる：\n\\[\n\\operatorname{P}[\\texttt{obesity}&gt;1]=g^{-1}(\\beta_{\\texttt{LDL}}\\cdot\\log(\\texttt{LDL})-c_1)\n\\] \\[\n\\operatorname{P}[\\texttt{obesity}&gt;2]=g^{-1}(\\beta_{\\texttt{LDL}}\\cdot\\log(\\texttt{LDL})-c_2)\n\\]\n\nformula_LDL &lt;- bf(\n  obesity ~ log(LDL),\n  family = cumulative(link = \"logit\")\n)\n# prior_LDL &lt;- prior(normal(0,0.1), class = b, coef = \"logLDL\")\nfit_LDL &lt;- brm(\n  formula_LDL,\n  data = raw_df,\n  chains = 4, cores = 4\n#  prior = prior_LDL\n)\n\n\nsummary(fit_LDL)\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: obesity ~ log(LDL) \n   Data: raw_df (Number of observations: 839) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     2.05      1.26    -0.42     4.52 1.00     2671     2228\nIntercept[2]     5.62      1.27     3.12     8.12 1.00     2606     2280\nlogLDL           0.97      0.27     0.46     1.50 1.00     2622     2226\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLDL が上がるごとに，normal や underweight から obese に移る確率が上がるのが見える．\n\nconditional_effects(fit_LDL, \"LDL\", categorical = TRUE)\n\n\n\n\n\n\n\n\n\nprior_summary(fit_LDL)\n\n                prior     class   coef group resp dpar nlpar lb ub       source\n               (flat)         b                                         default\n               (flat)         b logLDL                             (vectorized)\n student_t(3, 0, 2.5) Intercept                                         default\n student_t(3, 0, 2.5) Intercept      1                             (vectorized)\n student_t(3, 0, 2.5) Intercept      2                             (vectorized)\n\n\n\\(c_1,c_2\\) には \\(0\\) を中心とした \\(t\\)-事前分布が置かれているため，識別性は保たれると考えて良い．\n\n\n2.3 LAB との比較\n実は LDL よりも LAB の方が少し予測力が高い．しかも LAB は \\([1,10]\\) に値を取るので，対数変換をするよりも平方根変換をする方が自然である：\n\nformula_LAB &lt;- bf(\n  obesity ~ sqrt(LAB),\n  family = cumulative(link = \"logit\")\n)\nfit_LAB &lt;- brm(\n  formula_LAB,\n  data = raw_df,\n  chains = 4, cores = 4\n)\n\n\nloo_compare(loo(fit_LDL), loo(fit_LAB))\n\n        elpd_diff se_diff\nfit_LAB  0.0       0.0   \nfit_LDL -2.2       3.9   \n\n\n\nconditional_effects(fit_LAB, \"LAB\", categorical = TRUE)\n\n\n\n\n\n\n\n\nLAB が上昇すると underweight, normal から obese に移る確率がグンと上がるのが見える．\n\n\n2.4 双方の採用\n前節で LAB と LDL を比較すると，前者のみを用いたモデルの方が予測力が高いことを見た．\nでは両方をモデルに入れてベイズ推論をすることで，２つの情報を統合したより良いモデルができるだろうか？\n\ndf_double &lt;- data.frame(\n  obesity = raw_df$obesity,\n  z_sqrt_LAB = scale(sqrt(raw_df$LAB)),\n  z_log_LDL = scale(log(raw_df$LDL))\n)\nformula_double &lt;- bf(\n  obesity ~ z_sqrt_LAB + z_log_LDL,\n  family = cumulative(link = \"logit\")\n)\nfit_double &lt;- brm(\n  formula_double,\n  data = df_double,\n  chains = 4, cores = 4\n)\n\n実際予測性能の面では両方入れたモデルの方が良いようである：\n\nloo_compare(loo(fit_LAB), loo(fit_double))\n\n           elpd_diff se_diff\nfit_double  0.0       0.0   \nfit_LAB    -0.3       1.7   \n\n\n\nplot(fit_double, variable = c(\"b_z_sqrt_LAB\", \"b_z_log_LDL\"))\n\n\n\n\n\n\n\n\nLDL の方が僅かに縮小されて推定されていることがわかる．\nさらには LAB と LDL がモデルに入っている確率を出す方法は別稿で追求する：\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.5 交絡の存在\n\nformula_double_confound &lt;- bf(\n  obesity ~ z_sqrt_LAB * z_log_LDL,\n  family = cumulative(link = \"logit\")\n)\nfit_double_confound &lt;- brm(\n  formula_double_confound,\n  data = df_double,\n  chains = 4, cores = 4\n)\n\n\nplot(fit_double_confound, variable = c(\"b_z_sqrt_LAB\", \"b_z_log_LDL\", \"b_z_sqrt_LAB:z_log_LDL\"))\n\n\n\n\n\n\n\n\n若干の交絡の存在が疑われる．LDL が大きいほど，LAB と BMI との関係は減少していき，逆もまた然りである．\nこれは共線型性が強いので当然とも思われる：\n\ncor(df_double$z_sqrt_LAB, df_double$z_log_LDL)\n\n[1] 0.5204453\n\n\nしかし必ずしも条件数が大きいわけではない．\n\nX &lt;- model.matrix(~ z_sqrt_LAB + z_log_LDL, data = df_double)\nkappa(X)\n\n[1] 1.879479\n\n\n\n\n2.6 名目モデルを使ってしまったら？\nunderweight, normal, obese 間の順序構造を無視して，カテゴリカル分布を通じてモデリングをしても，実は当てはまりは必ずしも悪くない．\n\nformula_nominal &lt;- bf(\n  obesity ~ LAB,\n  family = categorical(link = \"logit\")\n)\nfit_nominal &lt;- brm(\n  formula_nominal,\n  data = raw_df,\n  chains = 4, cores = 4\n)\n\n\nwaic(fit_LAB)\n\n\nComputed from 4000 by 839 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -686.0 18.5\np_waic         2.8  0.1\nwaic        1372.0 37.0\n\nwaic(fit_nominal)\n\n\nComputed from 4000 by 839 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -685.8 18.6\np_waic         3.7  0.2\nwaic        1371.6 37.2\n\n\nほとんど質的には一致した結果を得る：\n\nconditional_effects(fit_nominal, \"LAB\", categorical = TRUE)\n\n\n\n\n\n\n\n\nひょっとしたら，BMI を離散化したという点で順序変数に思えるかもしれないが，これは単なる思い込みで，「痩せている」ことと「太っている」ことに順序関係を仮定することはむしろノイズになっているのかもしれない．\n\n\n\n2.7 非線型関係の追求\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-16\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.8 共変量の追加と効果の観察\n\nraw_df$sqrt_LAB &lt;- sqrt(raw_df$LAB)\nraw_df$log_Age &lt;- log(raw_df$Age)\nformula_LAB_cov &lt;- bf(\n  obesity ~ log_Age + SEX + log_Age:SEX + (0 + SEX | sqrt_LAB),\n  family = categorical(link = \"logit\")\n)\nfit_LAB_cov &lt;- brm(\n  formula_LAB_cov,\n  data = raw_df,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_LAB_cov)\n\n Family: categorical \n  Links: mu2 = logit; mu3 = logit \nFormula: obesity ~ log_Age + SEX + log_Age:SEX + (0 + SEX | sqrt_LAB) \n   Data: raw_df (Number of observations: 839) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~sqrt_LAB (Number of levels: 56) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(mu2_SEXfemale)                  0.20      0.14     0.01     0.54 1.00\nsd(mu2_SEXmale)                    0.20      0.14     0.01     0.53 1.00\nsd(mu3_SEXfemale)                  0.48      0.23     0.05     0.94 1.01\nsd(mu3_SEXmale)                    0.28      0.19     0.01     0.70 1.00\ncor(mu2_SEXfemale,mu2_SEXmale)    -0.12      0.57    -0.97     0.92 1.00\ncor(mu3_SEXfemale,mu3_SEXmale)    -0.10      0.54    -0.96     0.90 1.00\n                               Bulk_ESS Tail_ESS\nsd(mu2_SEXfemale)                  1288     1735\nsd(mu2_SEXmale)                    1118     1308\nsd(mu3_SEXfemale)                   792      963\nsd(mu3_SEXmale)                    1027     1614\ncor(mu2_SEXfemale,mu2_SEXmale)     2097     2182\ncor(mu3_SEXfemale,mu3_SEXmale)     1638     1708\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmu2_Intercept          -4.06      1.92    -7.89    -0.26 1.00     2108     2674\nmu3_Intercept          -6.39      2.35   -11.05    -1.71 1.00     2049     2334\nmu2_log_Age             1.55      0.50     0.55     2.56 1.00     2069     2656\nmu2_SEXmale            12.79      5.40     2.66    24.16 1.00      994     1566\nmu2_log_Age:SEXmale    -2.98      1.34    -5.79    -0.43 1.00     1004     1534\nmu3_log_Age             1.81      0.61     0.63     3.02 1.00     2028     2315\nmu3_SEXmale            14.91      5.62     4.64    26.55 1.00      933     1490\nmu3_log_Age:SEXmale    -3.31      1.40    -6.16    -0.73 1.00      945     1525\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(fit_LAB_cov, \"SEX\", categorical = TRUE)\n\n\n\n\n\n\n\n\n一般に男性の方が太っている確率が高くなる．これはよく知られているようである．\n\nconditional_effects(fit_LAB_cov, \"log_Age\", conditions = list(SEX = \"male\"), categorical = TRUE)\n\n\n\n\n\n\n\nconditional_effects(fit_LAB_cov, \"log_Age\", conditions = list(SEX = \"female\"), categorical = TRUE)\n\n\n\n\n\n\n\n\n女性は年齢とともに太る傾向が見えるが，男性はそうでもない（むしろ逆である）ようである．\nそして共変量を追加したことでモデルの予測力が大きく良くなっている：\n\nloo_compare(loo(fit_LAB), loo(fit_LAB_cov))\n\nWarning: Not all models have the same y variable. ('yhash' attributes do not\nmatch)\n\n\n            elpd_diff se_diff\nfit_LAB_cov   0.0       0.0  \nfit_LAB     -11.2       8.1  \n\n\n\npp_check(fit_LAB_cov)\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n\n\n\n\n2.9 その他の共変量の探索\nBMI と LDL は，骨ミネラル密度 (BMD: Bone Mineral Density) に因果的な影響を与えることが知られている (Wu, 2024)．\nP1NP は骨形成マーカー，ALP は骨の形成や骨疾患の評価に役立つ酵素である．\ndeoxypyridinolin (DPD) は骨を構成するⅠ型コラーゲンを束ねる蛋白質で、骨吸収の指標になる．\n他に Ca, Mg などの値もある．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLM.html#階層ロジットモデル",
    "href": "posts/2024/Survey/BayesGLM.html#階層ロジットモデル",
    "title": "brms を用いたベイズロジスティック回帰分析",
    "section": "3 階層ロジットモデル",
    "text": "3 階層ロジットモデル\n\n3.1 はじめに\nデータの不均衡性に着想を得て，「太っているかいないか？」の後に「痩せ気味かどうか？」という２つのロジット回帰の連続とみることを考える．\n\nraw_df &lt;- raw_df %&gt;%\n  mutate(obesity_former = case_when(\n  BMI &lt; 25 ~ 0,    # normal\n  BMI &gt;= 25 ~ 1    # obese\n  ))\nraw_df &lt;- raw_df %&gt;%\n  mutate(obesity_latter = case_when(\n  BMI &lt; 18.5 ~ 0,    # underweight\n  BMI &gt;= 18.5 ~ 1    # normal\n  ))\n\n\nformula_former &lt;- bf(\n  obesity_former ~ LAB,\n  family = bernoulli(link = \"logit\")\n)\nfit_former &lt;- brm(\n  formula_former,\n  data = raw_df,\n  chains = 4, cores = 4\n)\nformula_latter &lt;- bf(\n  obesity_latter ~ LAB,\n  family = bernoulli(link = \"logit\")\n)\nfit_latter &lt;- brm(\n  formula_latter,\n  data = raw_df,\n  chains = 4, cores = 4\n)"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html",
    "href": "posts/2024/Survey/BayesGLMM.html",
    "title": "brms を用いたベイズ混合ロジスティック回帰分析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html#項目応答モデル",
    "href": "posts/2024/Survey/BayesGLMM.html#項目応答モデル",
    "title": "brms を用いたベイズ混合ロジスティック回帰分析",
    "section": "1 項目応答モデル",
    "text": "1 項目応答モデル\n\n1.1 データの概観\n(Vansteelandt, 2001), (Boeck and Wilson, 2004) による「怒るかどうか？」のデータ VerbAgg を用いる．混合モデルの点推定のためのパッケージ lme4 (Bates et al., 2015) で利用可能になっている．\n\nlibrary(lme4)\ndata(\"VerbAgg\", package = \"lme4\")\ndf &lt;- VerbAgg\n\n質問票は「自分が意思表示をしたのにバスが止まってくれなかったので悪態をついた」などのもので，同意できるかを３段階 “yes”, “perhaps”, “no” で評価する (Boeck and Wilson, 2004, pp. 7–8)．\n応答は３段階の順序応答 resp とこれを２段階にしたもの r2 である．\n\nkable(head(df))\n\n\n\n\nAnger\nGender\nitem\nresp\nid\nbtype\nsitu\nmode\nr2\n\n\n\n\n20\nM\nS1WantCurse\nno\n1\ncurse\nother\nwant\nN\n\n\n11\nM\nS1WantCurse\nno\n2\ncurse\nother\nwant\nN\n\n\n17\nF\nS1WantCurse\nperhaps\n3\ncurse\nother\nwant\nY\n\n\n21\nF\nS1WantCurse\nperhaps\n4\ncurse\nother\nwant\nY\n\n\n17\nF\nS1WantCurse\nperhaps\n5\ncurse\nother\nwant\nY\n\n\n21\nF\nS1WantCurse\nyes\n6\ncurse\nother\nwant\nY\n\n\n\n\n\n\n\n1.2 固定効果１母数モデル\n通常の１母数モデルに，過分散を説明するための固定効果の項 \\(\\alpha_0\\) を加えたモデルを考える：\n\\[\ng(\\operatorname{P}[Y_{ik}=1])=\\alpha_{j[i]}-\\beta_{k[i]}+\\alpha_0,\\qquad\\alpha_0\\sim\\mathrm{t}(3;0,2.5),\n\\] \\[\n\\alpha_j\\sim\\mathrm{N}(\\mu_\\alpha,\\sigma_\\alpha^2),\\quad\\mu_\\alpha\\sim\\mathrm{N}(0,3),\\quad\\sigma_\\alpha\\sim\\mathrm{N}(0,3),\n\\] \\[\n\\beta_k\\sim\\mathrm{N}(\\mu_\\beta,\\sigma_\\beta^2),\\quad\\mu_\\beta\\sim\\mathrm{N}(0,3),\\quad\\sigma_\\beta\\sim\\mathrm{N}(0,3).\n\\]\nsd というクラスはグループレベル変数の標準偏差を意味する．\n\\(\\alpha_j,\\beta_k\\) の定数の違いに関する識別不可能性は，いずれも \\(0\\) を中心とした\n\nformula_1PL &lt;- bf(r2 ~ 1 + (1|item) + (1|id))\nprior_1PL &lt;-  prior(\"normal(0,3)\", class=\"sd\", group = \"id\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\")\nfit_1PL &lt;- brm(\n  formula_1PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nprior_summary(fit_1PL)\n\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n          normal(0,3)        sd              id                  0   \n          normal(0,3)        sd Intercept    id                  0   \n          normal(0,3)        sd            item                  0   \n          normal(0,3)        sd Intercept  item                  0   \n       source\n      default\n      default\n         user\n (vectorized)\n         user\n (vectorized)\n\n\nvectorized というのは，下記 Stan コード内で尤度は for 文で構成されるが，このループに入れなくて良いものがある場合をいう．\n\n\n\n\n\n\nStan コードの表示\n\n\n\n\n\nstancode(fit_1PL)\nによって推定に用いられた Stan コードが表示できる．\n次を見る限り，確かに意図したモデルになっている：\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += normal_lpdf(sd_1 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n  lprior += normal_lpdf(sd_2 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n-1*normal_lccdf(0|0,3) というのは定数であり，推定には全く影響を与えないが，後続の bridgesampling パッケージ (Gronau et al., 2020) によるモデル比較の API 構築のために付けられたものである (Bürkner, 2021, p. 21)．\n\n\n\n\n\nsummary(fit_1PL)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ 1 + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.39      0.07     1.25     1.54 1.00     1004     1840\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.22      0.19     0.91     1.64 1.00      471     1009\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.16      0.24    -0.63     0.31 1.00      355      812\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n低い ESS から変動効果の項 \\(\\epsilon_i\\) の推定に苦労していることがわかる．\n\nplot(fit_1PL)\n\n\n\n\n\n\n\n\nここにはグローバルなパラメータしか表示されておらず，ランダム効果の結果は次のように見る必要がある：\n\nlibrary(ggplot2)\nranef_item &lt;- ranef(fit_1PL)$item\nposterior_means &lt;- ranef_item[,1,1]\nlower_bounds &lt;- ranef_item[,3,1]\nupper_bounds &lt;- ranef_item[,4,1]\nplot_df_item &lt;- data.frame(\n  item = rownames(ranef_item),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n\n\np_PL1 &lt;- ggplot(plot_df_item, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Items\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\np_PL1\n\n\n\n\n\n\n\n\n多くの参加者にとって腹立たしい例とそうでない例が区別できているようである．\n\nplot_df_id &lt;- plot_df_id %&gt;% arrange(mean) %&gt;% mutate(rank = row_number())\np_PL1_id &lt;- ggplot(plot_df_id, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Individuals\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\np_PL1_id\n\n\n\n\n\n\n\n\nこうして怒りやすかった人を並べることができる．\nしかしガタガタしている区分定数的な模様が見れる．実はこれは item の分だけある．というのも，「何個の項目に Yes と答えたか」だけが \\(\\alpha_j\\) を決める要因になってしまっているためである．\nこれが項目識別のできない１母数モデルの限界である．\n\n\n1.3 固定効果２母数モデル\n項目識別力母数 \\(\\gamma_k\\) を導入する： \\[\ng(\\mu_i)=\\gamma_{k[i]}\\biggr(\\alpha_{j[i]}-\\beta_{k[i]}\\biggl),\n\\]\nすると追加の制約が必要になる．ここでは理想点モデルの場合と違い，研究のデザインから \\(\\gamma_{k[i]}\\) は正として良いだろう．\nこれを変数変換 \\(\\gamma_k=\\exp(\\log\\gamma_k)\\) によってモデルに知らせることとする．\n\nformula_2PL &lt;- bf(\n  r2 ~ exp(loggamma) * eta,\n  loggamma ~ 1 + (1|i|item),\n  eta ~ 1 + (1|i|item) + (1|id),\n  nl = TRUE\n)\n\n\\(g(\\mu_i)\\) の右辺はもはや \\(\\log\\gamma_k\\) の線型関数ではないので，これを nl=TRUE によって知らせる必要がある．\n|i| によって，\\(\\log\\gamma_k\\) と \\(\\eta_{jk}\\) 内の項 \\(\\beta_k\\) には相関があることを知らせている (Bürkner, 2018, p. 397)．項目難易度 \\(\\beta_k\\) が低いほど識別力 \\(\\log\\gamma_k\\) は低いとしているのである．\n\nprior_2PL &lt;-  prior(\"normal(0,5)\", class=\"b\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"b\", nlpar = \"loggamma\") +\n  prior(\"constant(1)\", class=\"sd\", group = \"id\", nlpar = \"eta\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"sd\", group = \"item\", nlpar = \"loggamma\")\n\nfit_2PL &lt;- brm(\n  formula = formula_2PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_2PL,\n  chains = 4, cores = 4\n)\n\nついに Stan が２分ほどかかるようになった上に，収束に苦労しており，ESS が低くなっている．\n\nsummary(fit_2PL)\n\nWarning: There were 1 divergent transitions after warmup. Increasing\nadapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ exp(loggamma) * eta \n         loggamma ~ 1 + (1 | i | item)\n         eta ~ 1 + (1 | i | item) + (1 | id)\n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~item (Number of levels: 24) \n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(loggamma_Intercept)                    0.12      0.06     0.01     0.24 1.01\nsd(eta_Intercept)                         0.93      0.16     0.68     1.28 1.00\ncor(loggamma_Intercept,eta_Intercept)     0.31      0.36    -0.46     0.90 1.01\n                                      Bulk_ESS Tail_ESS\nsd(loggamma_Intercept)                     712      874\nsd(eta_Intercept)                         1076     1826\ncor(loggamma_Intercept,eta_Intercept)      264      645\n\n~id (Number of levels: 316) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(eta_Intercept)     1.00      0.00     1.00     1.00   NA       NA       NA\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nloggamma_Intercept     0.32      0.06     0.20     0.44 1.00     1193     1895\neta_Intercept         -0.14      0.20    -0.52     0.25 1.00     1186     1676\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nranef_item2 &lt;- ranef(fit_2PL)$item\nposterior_means &lt;- ranef_item2[,1,\"eta_Intercept\"]\nlower_bounds &lt;- ranef_item2[,3,\"eta_Intercept\"]\nupper_bounds &lt;- ranef_item2[,4,\"eta_Intercept\"]\nplot_df_item2 &lt;- data.frame(\n  item = rownames(ranef_item2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_PL2 &lt;- ggplot(plot_df_item2, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\ngrid.arrange(p_PL1, p_PL2, nrow = 1)\n\n\n\n\n\n\n\n\n識別力パラメータ \\(\\gamma_k\\) が \\(1\\) より大きい値をとっており，これが変動を吸収しているため，\\(\\alpha_j\\) は \\(0\\) に縮小されて推定されるようになっている．\n\nranef_id2 &lt;- ranef(fit_2PL)$id\nposterior_means &lt;- ranef_id2[,1,\"eta_Intercept\"]\nlower_bounds &lt;- ranef_id2[,3,\"eta_Intercept\"]\nupper_bounds &lt;- ranef_id2[,4,\"eta_Intercept\"]\nplot_df_id2 &lt;- data.frame(\n  id = rownames(ranef_id2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n\n\nplot_df_id2 &lt;- plot_df_id2 %&gt;% arrange(mean) %&gt;% mutate(rank = row_number())\np_PL2_id &lt;- ggplot(plot_df_id2, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\ngrid.arrange(p_PL1_id, p_PL2_id, nrow = 1)\n\n\n\n\n\n\n\n\n少し滑らかになっている．\n\ncor(ranef_id[,1,\"Intercept\"], ranef_id2[,1,\"eta_Intercept\"])\n\n[1] 0.9994873\n\n\nしかし線型の相関になっており，軟化以上の変化は導入されなかったことがわかる．\nそれもそうである．モデルの表現力はあげたから解像度は高くなったが，モデルに新しい情報を入れたわけではないのである．\n\n\n1.4 共変量の追加\n理想点モデルなど多くの項目応答モデルは，\\(\\alpha_j,\\beta_k\\) の推定に終始してきたが，本当のリサーチクエスチョンはその先にある．\n個人レベルの共変量を追加した階層モデルを構築して，\\(\\alpha_j\\) の位置や応答の傾向への影響を調べることが真の目標であった．\n\n1.4.1 項目共変量の追加\n本データにおいて項目は \\(2\\times2\\times3\\) の split-plot デザインがなされている．\nmode とは「悪態をつきたい」と「咄嗟についてしまう」という２種の行動を区別するためのものである．この２つの行動容態は，本人の抑制的な意識が実際に働いたかどうかにおいて全く質的に異なる．モデルにこれを教えたらどうなるだろうか？\nsitu とはシチュエーションであり，自分に責任があるか（「店に入ろうとした瞬間閉店時間になった」など）他人に責任があるか（「バスが止まってくれなかった」など）の２項目がある．\nbtype は行動様式であり，「悪態をつく」「叱る」「怒鳴りつける」の３項目がある．後に行くほど他人への攻撃性が強い．\n最初に考えられるモデル\nr2 ~ btype + situ + mode + (1|item) + (1 + mode|id)\nは，元々の１母数モデルに変動切片項を３つ追加した上に，mode の係数を個人ごとに変えることを許したものである．これは mode の効果が個人ごとに異なるだろうという信念による．\nしかしこのモデルに至る前に，1 を 0 にすることで modedo と modewant 双方の標準偏差を推定することを考える（1 の場合は modewant の標準偏差の代わりに Intercept の標準偏差を推定する）．\n\nformula_1PL_cov &lt;- bf(\n  r2 ~ btype + situ + mode + (1|item) + (0 + mode|id)\n)\nfit_1PL_cov &lt;- brm(\n  formula = formula_1PL_cov,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n\n\nsummary(fit_1PL_cov)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ btype + situ + mode + (1 | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.47      0.09     1.30     1.65 1.00     1898\nsd(modedo)               1.67      0.10     1.48     1.88 1.00     1932\ncor(modewant,modedo)     0.77      0.04     0.69     0.84 1.00     1674\n                     Tail_ESS\nsd(modewant)             3005\nsd(modedo)               2826\ncor(modewant,modedo)     2675\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.46      0.09     0.32     0.68 1.00     1643     2370\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.88      0.23     1.42     2.32 1.00     1923     2640\nbtypescold    -1.12      0.24    -1.60    -0.63 1.00     2004     2515\nbtypeshout    -2.23      0.25    -2.73    -1.74 1.00     1962     2482\nsituself      -1.12      0.21    -1.53    -0.70 1.00     1941     2500\nmodedo        -0.77      0.21    -1.18    -0.33 1.00     2114     2523\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nmodedo の係数が負になっており，悪態をつきたくなっても，実際にする人の割合は下がることがわかる．\nだが係数の -0.77 が大きいかどうかがわからない．これには対数オッズ比のスケールから元のスケールに戻す便利な関数がある：\n\nconditional_effects(fit_1PL_cov, \"mode\")\n\n\n\n\n\n\n\n\n確率としての減少は軽微だがあることがわかる．次に気づくことは do の方がエラーバーが長いことである．２つの係数は相関しているので，頻度論的な検定は難しいかもしれないが，２つの標準偏差の差の事後分布を見ることでチェックすることができる：\n\nhyp &lt;- \"modedo - modewant &gt; 0\"\nhypothesis(fit_1PL_cov, hyp, class = \"sd\", group = \"id\")\n\nHypothesis Tests for class sd_id:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (modedo-modewant) &gt; 0      0.2      0.12     0.01     0.39       23.1\n  Post.Prob Star\n1      0.96    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n0.96 の確率で modedo の標準偏差の方が大きいことがわかるが，その差も 0.2 ほどで，対数オッズ比としては大したことがないと思われる．\n\n\n1.4.2 個人共変量の追加\nTrait Anger スコア (Spielberger, 2010) が個人ごとに算出されており（Anger 変数），そのスコアによってどのように項目への反応が違うかを調べる．こうするとどんどん心理学の研究っぽくなる．\n\nformula_1PL_cov_id &lt;- bf(\n  r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0+Gender|item) + (0+mode|id)\n)\nfit_1PL_cov_id &lt;- brm(\n  formula = formula_1PL_cov_id,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4,\n  iter = 3000  # これ以上大きくすると GitHub にあげられない\n)\n\n\nsummary(fit_1PL_cov_id)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0 + Gender | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.49      0.09     1.32     1.67 1.00     1998\nsd(modedo)               1.58      0.10     1.40     1.78 1.00     1715\ncor(modewant,modedo)     0.78      0.04     0.70     0.85 1.00     1529\n                     Tail_ESS\nsd(modewant)             2740\nsd(modedo)               2531\ncor(modewant,modedo)     2380\n\n~item (Number of levels: 24) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(GenderF)              0.52      0.11     0.35     0.77 1.01     1342\nsd(GenderM)              0.34      0.11     0.16     0.57 1.00     1659\ncor(GenderF,GenderM)     0.78      0.18     0.32     0.99 1.00     2178\n                     Tail_ESS\nsd(GenderF)              2491\nsd(GenderM)              2553\ncor(GenderF,GenderM)     1911\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          0.74      0.44    -0.14     1.59 1.00     1293     2072\nAnger              0.06      0.02     0.02     0.09 1.00     1274     2101\nGenderM           -0.10      0.24    -0.56     0.38 1.00      931     1839\nbtypescold        -1.03      0.22    -1.46    -0.60 1.00     1931     2144\nbtypeshout        -2.43      0.25    -2.90    -1.94 1.00     1472     1396\nsituself          -1.04      0.18    -1.38    -0.68 1.00     1919     2346\nmodedo            -0.98      0.23    -1.43    -0.51 1.00     1738     2315\nGenderM:modedo     0.89      0.24     0.40     1.35 1.00     2525     2932\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nconditional_effects(fit_1PL_cov_id, \"Anger\")\n\n\n\n\n個人共変量の効果と，特異項目機能\n\n\n\n\n\nconditional_effects(fit_1PL_cov_id, \"mode:Gender\")\n\n\n\n\n\n\n\n\n\nAnger の値が大きいほど悪態をつく確率が綺麗に上がっていく様子がわかる．\n加えて，女性の方が悪態を吐こうと思っても，実際に行動に移すには大きな壁があることがわかる．こうして mode と Gender の間の交絡が陽の下に明らかになった．\nこのような，項目共変量と個人共変量の間の交絡は 特異項目機能 (DIF: Differential Item Functioning) と呼ばれる．項目の特性が，被験者のグループによって違った機能を示すことは，例えばテスト理論では個人の潜在特性を推定する際の重大なノイズ要因となっており，これを統制することが重要な課題になる．\n\n\n\n1.5 特異項目機能の解析\nこの特異項目機能を，項目の特性ごとにさらに詳しく見ていく．\n特に怒鳴りつける行動様式を除き，悪態をつく行為と叱る行為は，男性と女性において違う機能を持っているのではないか？という仮説を検証してみる．\n女性が実際に悪態をつく／叱る行為にだけマークをつけるダミー変数 dif を用意する：\n\ndf$dif &lt;- as.numeric(with(\n  df,\n  Gender == \"F\" & mode == \"do\" & btype %in% c(\"curse\", \"scold\")\n))\n\n\nformula_1PL_dif &lt;- bf(\n  r2 ~ Gender + dif + (1|item) + (1|id)\n)\n\n\nfit_1PL_dif &lt;- brm(\n  formula = formula_1PL_dif,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 3, cores = 3,\n  # iter = 3000  # これ以上大きくすると GitHub にあげられない\n)\n\n\nsummary(fit_1PL_dif)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Gender + dif + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.40      0.07     1.26     1.54 1.00      793     1557\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.34      0.21     1.00     1.84 1.01      684     1088\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.12      0.31    -0.51     0.72 1.00      304      502\nGenderM      -0.01      0.21    -0.45     0.41 1.00      573      864\ndif          -0.95      0.14    -1.23    -0.67 1.00     2875     1778\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\ndif の係数 -0.94 を見ることで，殊に「女性」と「実際に悪態を吐いたり叱ったりする」という組み合わせは特異な項目機能を持っていることがわかる．"
  },
  {
    "objectID": "posts/2024/Survey/BayesGLMM.html#文献案内",
    "href": "posts/2024/Survey/BayesGLMM.html#文献案内",
    "title": "brms を用いたベイズ混合ロジスティック回帰分析",
    "section": "2 文献案内",
    "text": "2 文献案内\n\n(Bürkner, 2021) に項目応答モデルのベイズ的な扱いが取り上げられている．特にパッケージ brms を用いた例が３つある．\nDIF に関する日本語文献に (龍一, 2012) がある．"
  },
  {
    "objectID": "posts/2024/Survey/BDA4.html",
    "href": "posts/2024/Survey/BDA4.html",
    "title": "ベイズデータ解析８",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA4.html#関連記事",
    "href": "posts/2024/Survey/BDA4.html#関連記事",
    "title": "ベイズデータ解析８",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html",
    "href": "posts/2024/Survey/BDA1.html",
    "title": "ベイズデータ解析５",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#関連記事",
    "href": "posts/2024/Survey/BDA1.html#関連記事",
    "title": "ベイズデータ解析５",
    "section": "関連記事",
    "text": "関連記事\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#回帰分析の一般事項",
    "href": "posts/2024/Survey/BDA1.html#回帰分析の一般事項",
    "title": "ベイズデータ解析５",
    "section": "1 回帰分析の一般事項",
    "text": "1 回帰分析の一般事項\n\n1.1 はじめに：正規線型回帰\n回帰分析とは，２つの確率変数 \\(X,Y\\) の実現と見られるデータが得られている際に，\\(Y\\) に対して（より正確にはその条件付き期待値 \\(\\operatorname{E}[Y|X]\\) に対して），\\(x\\in\\mathcal{X}\\) に依存する関数としてモデル \\[\n\\mathcal{X}\\ni x\\mapsto P_{\\theta,x}\\in\\mathcal{P}(\\mathcal{Y})\n\\] を考えることをいう．\n例えば \\[\nP_{\\theta,x}=\\operatorname{N}(\\beta^\\top x,\\sigma^2),\\qquad\\theta=(\\beta,\\sigma),\n\\] とした場合を 正規線型回帰モデル (normal linear model) という．\n\\(Y\\) のモデリングに集中しており，\\(X\\) には分布の仮定を置いていない点が回帰モデルの特徴である．主に \\(X\\) を用いた \\(Y\\) の予測や，\\(X\\) の \\(Y\\) への（因果）効果を推定するために用いられる．1\n\n\n1.2 ベイズ回帰分析\n\nベイズ回帰分析では，\\(Y\\) の条件付き構造 \\(Y|X\\) をモデリングするにあたって，パラメータ \\(\\theta\\) との結合分布 \\((Y,\\Theta)|X\\) をモデリングする．\n実際，ベイズ回帰分析では事前分布 \\(P_\\varphi\\) も併せて次の２つのモデルが想定される： \\[\nY|(\\Theta,X)\\sim P_{\\theta,x},\\qquad \\Theta\\sim P_\\varphi.\n\\] \\(\\varphi\\) はハイパーパラメータとも呼ばれる．\nパラメータ \\(\\theta\\) の空間（または \\((\\theta,\\varphi)\\) ）上からの確率核 \\[\n\\theta\\mapsto P_{\\theta,x}(dy)\\in\\mathcal{P}(\\mathcal{Y})\n\\] を 尤度 という．よく密度 \\(p(y|x,\\theta)\\) の形で表される．\n尤度によりデータ \\(y\\) の空間 \\(\\mathcal{Y}\\) 上の確率分布に関する問題を，パラメータ \\(\\theta\\) の空間上に移送し，事前分布 \\(P_\\varphi\\) からの変化を見ることが （ベイズ）推論 である．\nベイズ回帰分析とは，尤度がパラメータ \\(\\theta\\) の他にデータ \\(x\\) の関数でもある場合のベイズモデリングに過ぎない．換言すれば，\\(Y|X\\) の依存構造に焦点がある際に行うベイズ推論である．\n\n\n\n1.3 強線型性\n仮に２つの説明変数に完全な線型関係がある場合，複数のパラメータ値が同一のモデルを表現するため，パラメータ推定が複数の解を持つ（＝識別不可能）．\nこの場合 OLS 推定は数値的に不安定になる危険性がある．\n一方でベイズ推定は事前分布から与えられる情報によりこのような不安定性が回避でき，多くのデフォルト事前分布はそのように設計されている．(8.4 節 Gelman, Hill, et al., 2020, p. 109) も参照．2\nこの美点は階層モデリングにおいても引き継がれる．\\(J\\) 個の指示変数（ダミー変数）を用いた回帰においては，切片項を除けば \\(J-1\\) 個の指示変数しか追加してはならない．共線型性をもつためである．\nしかし階層モデリングにおいては係数に超階層モデルが想定されるため，ここから伝ってくる事前情報が自然に正則化を行い，適切にベイズ推定が行われる (Gelman and Hill, 2006, p. 393)．さらに縮小推定が働き，ほとんどの場合より推定の効率が上がる 2.5．\n\n\n1.4 ベイズ回帰分析ワークフロー\n\n\\((X,Y)\\) の依存構造が単純（線型）になるような変数変換を行う（一般化線型モデルの利用を含む）．\n\\((\\Theta,\\Phi)\\) の事前分布を設定する（初めは一様分布やデフォルトの無情報事前分布で良い）．\n事後分布を計算し，事後予測分布を見てデータが再現できているかを基にモデルを検証する．\n\nその後，十分に階層化をして，パラメータの空間上の事前分布がほとんど情報を持たなくて良いようにする，完全ベイズ推論が一つの悲願とされる．3\nモデルの挙動がもはや事前分布に依存しなくなった際，モデルの階層構造や尤度の構造が十分にデータを反映できていると思われるためである．4\n\n推定すべきものはモデルの尤度であってパラメターの値ではないというのが赤池氏の主張です．いいかえると，推定すべきは確率構造であってパラメターではないというのです．(田邉國士, 2010)\n\n\n\n\n\n\n\nこの階層化と尤度の推定を探索的に実行できる点がベイズの真の美点だと筆者は考える．そして一度モデルの構造・尤度が明瞭化された際は，もはやベイズである必要はない場合が多い．5\n\n\n\n\n\n1.5 ベイズ線型回帰からの脱出\n前節の立場にたてば，最初の解析は常に（弱い情報を持った事前分布による）ベイズ回帰分析であるべきである．6\nこれは若干の正則化を加えたロバスト最尤推定に，不確実性の定量化を加えたものと等価であるが，これを MCMC を回すことで一度に実行できる点が美点である．\n多くのデフォルト事前分布が開発されており，ほとんど自動的に最初のベイズ回帰分析が実行できる．共線型性が懸念される場合や，小さなデータセットに大きなモデルをフィッティングしようとしている場合などの識別不可能性が生じる状況でも安定した推定値が得られる．\n事後分布は豊富な情報を持っており，何より事後予測分布を計算することで予測モデルとしての妥当性を即時に確認できる (PPC: Posterior Predictive Check)．\n同時に解析の目標は，適切な関数関係や階層関係を持った階層モデルの発見と，これに適合する（ベイズだろうと点推定だろうと）パラメータ推定法の構成による，ナイーブなベイズ線型回帰からの脱出である．\nそれにあたって，MCMC の収束鈍化も大きな情報である (Bürkner, 2021, p. 32)．\nThis is the game we (should) play."
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#階層モデル",
    "href": "posts/2024/Survey/BDA1.html#階層モデル",
    "title": "ベイズデータ解析５",
    "section": "2 階層モデル",
    "text": "2 階層モデル\n\n2.1 はじめに\n階層モデルは複雑なモデルを構築するための強力なツールであり，ベイズのワークフローにおいて基本的な要素になる．\n層別抽出やクラスター抽出をはじめとして，多くの場合階層別に知識が存在し，これらを系統的に組み込んだ形でモデルを構築できる．\nしかし同時に計算が困難になり，第一近似として正規性が仮定される場合が多い．\n\n\n2.2 混合効果モデル\n標本を \\(J\\) 個のグループにわけ，これへの所属を表す２値変数 \\(x_i\\in\\mathbb{R}^J\\) を説明変数に追加するとする．\nこのような所属変数 \\(x_i\\) の回帰係数 \\(\\beta\\in\\mathbb{R}^J\\) に対して階層モデルが考えられる場合が多い．\n例えば \\(\\alpha\\in\\mathbb{R},\\sigma_\\beta&gt;0\\) をスカラーとして \\[\n\\beta\\sim\\operatorname{N}_J(\\alpha\\boldsymbol{1},\\sigma^2_\\beta I_J),\n\\] という正規モデルを想定した場合，\\(x_{ij}\\) の係数 \\(\\beta_j\\) は各グループ \\(j\\in[J]\\) 固有の切片項であり，変量効果 (random effect) と呼ばれる．\n変量効果の追加は，同一グループ内の \\(y_i\\) に相関を生じさせるが，グループが違う場合は相変わらず独立のままとする効果がある．\nさらに \\(\\beta_j\\) の分散 \\(\\sigma^2_{\\beta_j}\\) を \\(\\infty\\) とした場合，すなわち（もはや確率分布ではないが）一様分布を仮定した場合を 固定効果 (fixed effect) という．7\nベイズの立場からは，「変量」と「固定」の名称は歴史的なもので，実質的な違いは「次の階層で回帰モデルを仮定するか，モデルを持たない最終階層の変数と扱い一様事前分布に従うとするか」という仮定の違いにすぎない．詳しくは次節：\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\nNo matching items\n\n\nこの２種の取り扱いをする回帰係数を混在させた場合は 混合モデル (mixed model) という (Gelman et al., 2014, p. 383)．\n\n\n2.3 階層モデルから見た分散分析\nベイズのワークフローにおいて，複数の説明変数間の階層関係の特定や「どのグループの回帰係数を共通とするか」の見極めが極めて重要である 1.4．\n特に，膨大な説明変数の中から「因子」（性別・教育水準・出身地など）とその「水準」（女性・大学院生・山形県民など）とを峻別することが重要であり，どのクラスに独自の回帰係数 \\(\\beta^{(m)}\\sim\\operatorname{N}_{J_m}(\\alpha_{m}\\boldsymbol{1}_{J_m},\\sigma^2_{m}I_{J_m})\\) を与えるかの決定が，モデルの尤度の改善において大きな効果を持つ (Gelman, 2005)．\n\\(M\\) 元配置の分散分析において，各因子 \\(m\\in[M]\\) に対応する回帰係数は，\\(J_m\\) 個の水準ごとに次のように決まるバラバラの変動係数を持つとする： \\[\n\\beta^{(m)}_j\\sim\\operatorname{N}(0,\\sigma^2_m),\\qquad j\\in[J_m].\n\\] この変量効果としての解釈により，ANOVA は階層モデルの推定とみなせる (3.2節 Gelman, 2005, p. 9)．8\nこの際の \\(\\sigma^2_m\\) に対する共役（超）事前分布は逆 \\(\\chi^2\\)-分布である (Gelman et al., 2014, p. 396) \\[\n\\sigma^2_{m}\\sim\\chi^{-2}(\\mu_m,\\sigma^2_{0m})\n\\] であり，\\(\\nu_m=-1,\\sigma^2_{0m}=0\\) とすることで一様事前分布を得る．\n\nAnalysis of variance (Anova) represents a key idea in statistical modeling of complex data structures. (Gelman et al., 2014, p. 395)\n\nこうして設定された各因子の各水準ごとの係数 \\(\\beta^{(m)}_j\\) の事後分布を見ることで「分散分析」を実行することになる．これがベイズによる分散分析の再解釈である．\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.4 水準ごとの分散\nこのように分散分析を階層モデルのベイズ推定と再解釈することで，膨大な数の水準の組み合わせに関して，その効果量を定量的に比較することができる．\nここからさらに，\\(M\\) 個の因子ごとに全ての水準で共通した分散 \\(\\sigma_1^2,\\cdots,\\sigma_M^2\\) の \\(M\\) 個のみを考えるのではなく， \\[\n\\beta^{(m)}\\sim\\operatorname{N}_{J_m}(\\alpha_m\\boldsymbol{1}_{J_m},\\mathrm{diag}(\\sigma^2_{m1},\\cdots,\\sigma^2_{mJ_m})),\\qquad m\\in[M],\n\\] として，水準ごとにも異なる \\(\\sum_{m=1}^M J_m\\) 個の分散を考えることもできる (15.7 節 Gelman et al., 2014, p. 397)．\nこの際， \\[\n\\operatorname{Cauchy}(0,A),\\qquad A\\sim U(\\mathbb{R}_+),\n\\] という形の半 Cauchy 分布が \\(\\sigma^2_{mj}\\) の事前分布として用いられる (Gelman et al., 2014, p. 399)．これは一部の水準にのみ大きな分散を認め，その他の水準にはほとんど分散への寄与がないという仮定を表している．\n\n\n2.5 縮小推定\n階層モデルでは，自然に他のグループの情報が共有され，各グループの平均が全体の平均に向けて「縮小」されて推定される．これを (Stein, 1956) から Stein 効果ともいう (Hoff, 2009, p. 146)．9"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#一般化線型モデル",
    "href": "posts/2024/Survey/BDA1.html#一般化線型モデル",
    "title": "ベイズデータ解析５",
    "section": "3 一般化線型モデル",
    "text": "3 一般化線型モデル\n\n3.1 線型 Gauss 性からの乖離\n正規線型モデルから，次の２つの自由度を追加したモデルを 一般化線型モデル (Nelder and Wedderburn, 1972) という：\n\n\n\n\n\n\n\nリンク関数 \\(g\\)\n\n\\[\n  g\\circ\\operatorname{E}[Y|X]=\\beta^\\top X\n  \\]\n\n（正規分布以外の）分布族 \\(P\\)\n\n\\[\n  Y|X\\sim P(\\operatorname{E}[Y|X],\\phi)\n  \\]\n\n\n\nその結果質的データ解析にも応用可能な広いクラスのモデルを得る．\n\n\n\n\n\n\nカウントデータに対する Poisson モデル\n\n\n\n自然数値のデータに対して，\\(y_i\\sim\\mathrm{Pois}(\\lambda_i)\\) の \\(\\lambda_i\\) を，正準リンク関数 \\(g=\\log\\) を通じて \\[\n\\log\\lambda_i=X_i^\\top\\beta\n\\] とモデリングすることが考えられる．このモデルを Poisson 回帰 ともいう．\n\n\n\n\n\n\n\n\n成功回数データに対する二項モデル\n\n\n\n２値データや成功回数を表すデータの場合，\\(y_i\\sim\\mathrm{Bin}(n_i,\\mu_i)\\) の \\(\\mu_i\\) を \\(\\operatorname{P}[Y_i=1|X_i]\\) の形でモデリングすることを考えることができる．\nこの場合，正準リンク関数は \\[\ng(\\mu)=\\log\\frac{\\mu}{1-\\mu}\n\\] という logit 変換で与えられる．より効率的な推論を促進するために probit リンクや，非対称性を導入する complementary log-log リンク \\[\ng(\\mu)=\\log(-\\log\\mu)\n\\] が用いられることもある (Gelman et al., 2014, p. 407)．\n\n\n\n\n3.2 指数分布族\nなお正準リンクとは，Poisson 分布族や二項分布族を指数分布族とみなした際のリンク関数のことである．\n例えば二項分布族 \\(\\{\\mathrm{Bin}(n,\\mu)\\}_{\\mu\\in(0,1)}\\) は，計数測度 \\(\\nu\\) に対して， \\[\n\\frac{d \\mathrm{Bin}(n,\\mu)}{d \\nu}(x)=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\exp\\left(x\\log\\frac{\\mu}{1-\\mu}+n\\log(1-\\mu)\\right)\n\\] と表せる．\\(g(\\mu)\\) を 自然な十分統計量 ともいう．\n指数分布族と正準リンク関数を用いた一般化線型モデリングは，パラメトリック分布族の十分統計量を代理の応答変数として線型回帰を行なっているものとみなせる．\n\n\n\n\n\n\n変換の意味\n\n\n\nこのような数理統計学的な理由とは別に，\\(g(\\mu)\\) の意味を直接解釈することもできる．\n\nPoisson 回帰はオフセット \\(\\log y_i\\) を作ることで， \\[\n\\log\\frac{\\lambda_i}{y_i}=X_i^\\top\\beta\n\\] と見ることもできる．\\(\\lambda_i/y_i\\) は観測カウント \\(y_i\\) に対する平均 \\(\\lambda_i\\) の 率比 (rate ratio) と呼ばれ，\\(g(\\lambda_i/y_i)\\) は対数率比と解釈できる．\n\\(\\frac{\\mu}{1-\\mu}\\) という値は成功確率 \\(\\mu\\) に対するオッズ比と呼ばれ，\\(g(\\mu)\\) は 対数オッズ比 (log odds ratio) と呼ばれる．\n\n\n\n\n\n3.3 分散分析\n線型モデルにおいて分散分析は，第一義的には帰無モデルの検定であった．後続の多重比較による解析は，説明変数ごとの効果量の比較を行う．\nしかし線型モデルにおいてその方法は分散の分解に基づいており，この一般化線型モデルへの拡張は自明ではない．\n一般化線型モデルにおいても残差を定義し，これに基づいてモデルの検証を行うことはできる (Davison and Tsai, 1992)．"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#終わりに",
    "href": "posts/2024/Survey/BDA1.html#終わりに",
    "title": "ベイズデータ解析５",
    "section": "4 終わりに",
    "text": "4 終わりに"
  },
  {
    "objectID": "posts/2024/Survey/BDA1.html#footnotes",
    "href": "posts/2024/Survey/BDA1.html#footnotes",
    "title": "ベイズデータ解析５",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし回帰モデルを「因果効果」の推定に用いる際には，通常とは異なる，仮定に対する精査が必要になる．最も安全には，\\(x\\) が変化した際の \\(y\\) の変化量を単なる「比較」の文脈で説明することである．この「違い」が \\(x\\) の変化により生み出されたとは限らないためである．(6.4 節 Gelman, Hill, et al., 2020, p. 85) も参照．↩︎\nimproper な一様事前分布を用いた場合，引き続き不安定なままな可能性はある．だが多くのデフォルト事前分布は，weakly informative というように，軽微な情報を加えることで正則化が働くように設定されている裾の（適度に）広い事前分布であることが多い．↩︎\n従来は事前分布の経験ベイズ推定と呼ばれていた考え方である (Gelman, Vehtari, et al., 2020, p. 6)．↩︎\n一方で多くの頻度論的な手法は，無情報事前分布を仮定したベイズ推論とみなせる．そこでベイズの，有効な頻度論的モデルを探索するための方法としての美点が見えてくるのである．↩︎\nこの点については (Gelman, 2014) も参照．大統領選における有権者の行動のモデリングを，ベイズ階層モデルに基づいて探索的に実行しており，“multilevel Bayesian modeling can be considered as an elaborate form of exploratory data analysis” と結論している．↩︎\nもちろん重要な事前情報や予備解析が存在する場合は，これを事前分布としてどう更新されるかをみるのが良い．(1.6 節 Gelman, Hill, et al., 2020, p. 16) に簡潔な概観的議論がある．↩︎\n(Bafumi and Gelman, 2007) では unmodeled varying intercept と呼んでいる．↩︎\nすると「自由度」とは変動係数の数に他ならない．↩︎\n同様の縮小効果を得るための点推定手続きが，経験ベイズ の名称で研究されている (Efron and Morris, 1973), (Efron and Morris, 1975)．これについては (久保川達也, 2006) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html",
    "href": "posts/2024/Survey/BDA2.html",
    "title": "ベイズデータ解析６",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#関連記事",
    "href": "posts/2024/Survey/BDA2.html#関連記事",
    "title": "ベイズデータ解析６",
    "section": "関連記事",
    "text": "関連記事\n回帰モデルや分散分析では，暗黙裡に応答変数が連続であることが仮定されている．\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n本節では質的な変数，特に順序構造がある離散変数のモデリングを考える．このようなものを 順序応答 (ordinal response) という．\nこのようなデータには，多くの場合背後に 連続潜在変数 を仮定してモデリングを行う．\nその結果，潜在変数の存在が Gibbs サンプラーの構成を容易にする．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#項モデル",
    "href": "posts/2024/Survey/BDA2.html#項モデル",
    "title": "ベイズデータ解析６",
    "section": "1 ２項モデル",
    "text": "1 ２項モデル\n\n1.1 はじめに\n\\(0,1\\) の確率変数の分布は，Bernoulli 分布 \\(\\mathrm{Ber}(p)=\\mathrm{Bin}(1,p)\\) によって記述できる．\nこの際，パラメータ \\(p\\) を代理の応答変数として回帰分析がなされることになる．\n多くの場合，リンク関数 \\(g\\) に関して，\\(g(p)\\) を線型の予測子 \\(\\beta^\\top X\\) で回帰する．Bernoulli 分布は指数型分布族の例であるため，このようなモデルは 一般化線型モデル の例になる．\n\n\n1.2 ロジスティックモデル\n仮に変数 \\(y_i\\sim\\mathrm{Ber}(p)\\) に応じて， \\[\nX_i|Y_i=i\\sim\\operatorname{N}(\\mu_i,\\sigma^2),\\qquad i\\in\\{0,1\\}\n\\] として説明変数 \\(X_i\\) が決まっていたとする．\nこのような状況で，\\(X_i\\) はある \\(\\alpha,\\beta\\) に関して次のロジットモデルを満たす：1 \\[\n\\operatorname{logit}(p_i)=\\alpha+\\beta x_i.\n\\]\n\n\n1.3 潜在変数解釈\n\\(P(\\mu,\\sigma^2)\\) を正規分布やロジスティック分布などの対称な分布，\\(F\\) を \\(P(0,1)\\) の分布関数とする．このとき，プロビットまたはロジスティックモデル \\[\n\\operatorname{P}[Y_i=1|X_i]=F(X_i^\\top\\beta)\n\\] は，潜在変数 \\(Y_i^*\\) が存在して \\[\nY_i=1_{\\mathbb{R}_+}(Y_i^*),\\qquad Y_i^*\\sim P(X_i^\\top\\beta,1)\n\\tag{1}\\] として結果が決まっているものとも解釈できる．これは \\[\n\\operatorname{P}[Y_i=1|X_i]=\\operatorname{P}[Y_i^*&gt;0|X_i]=\\operatorname{P}[-U\\le X_i^\\top\\beta],\\qquad U\\sim P(0,1),\n\\] が成り立つためである．\nこの潜在変数 \\(Y_i^*\\) は単位 \\(i\\in[N]\\) の潜在的な尺度として解釈できる．これを用いて因子分析様の解析を行う例には 理想点推定 もある．\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\n一方で \\(Y_i^*\\) の存在は Gibbs サンプラーの構成を容易にする (Albert and Chib, 1993)．\n\\(F\\) を \\(t\\)-分布とした場合，これを robit モデル (Liu, 2004) という (15.6 節 Gelman et al., 2020)．\\(t\\) 分布の自由度を無限大にした場合が probit モデルに相当する．\n第 2.3 節でまた別の，効用関数による潜在変数表現を扱う．\n\n\n1.4 順序応答\n前節 1.3 の潜在変数解釈は容易に２値応答以外の場合に拡張できる．その結果多項モデルを用いたソフトマックス回帰 2.2 などとは別のアプローチが，多値順序応答に対して可能になる．\n\\(y\\in n+1:=\\{0,1,\\cdots,n\\}\\) という順序を持った順序応答の場合でも \\[\n\\chi(u)=1_{(c_0,\\infty)}(u)+1_{(c_1,\\infty)}(u)+\\cdots+1_{(c_n,\\infty)}(u)\n\\] という階段関数を用いて \\[\nY_i=\\chi(Y_i^*),\\qquad Y_i^*\\sim P(X_i^\\top\\beta,1)\n\\] というデータ生成過程を想定できる．\nこれはそのまま 順序多項回帰 (ordered multinomial regression) (Walker and Duncan, 1967) に相当する．\nこの場合 \\(c_0=0\\) とし，それに応じて \\(Y_i^*\\sim P(X_i^\\top\\beta-c_0,1)\\) とする径数付けを採用する場合も多い．\nベイズ推論は \\(\\chi\\) を特徴付ける点 \\((c_0,c_1,\\cdots,c_n)\\) 上に事前分布を置くことで実行できるが，順位尤度 (rank likelihood) を通じた議論により，\\(g\\) を特定せずとも Gibbs サンプラーによる推論が可能である (Section 12.1.2 Hoff, 2009)．\n\n\n1.5 識別可能性と分離\n線型回帰において，共線型性 があると識別可能性が失われる．ロジスティック回帰にはもう一つ識別不可能性の典型的な原因がある．\n多くの点推定手法において，説明変数の線型変換が極めて強力な説明変数になる場合，やはり「正解」が何個も存在する状況が現れるため，モデルの非識別性が暗黙のうちに問題になる．これを 分離 (separation) という (Gelman et al., 2014, p. 412)．\n特に最尤推定法，一様事前分布を持ったベイズ推定は不安定になるが，このような場合でも裾の重い事前分布を採用することでベイズ推論が安定的に実行可能である（同様にして正則化を加えた最尤推定も可能である） (Gelman and Hill, 2006, p. 104)．\n特に係数に（互いに独立な） \\(t\\)-分布 \\(t(\\nu,0,s)\\) を仮定する，ロバスト性を意識した設定がデフォルトと理解されている (Gelman et al., 2014, p. 412)．\nただし，\\(\\nu,s\\) は説明変数のスケールに合わせてなるべく無情報になるように設定される．\\(s\\to\\infty\\) の極限が一様分布になり，分離の問題にセンシティブになっていく．\n\\(g(\\mu)=:\\theta\\) のそれぞれの値に関して，成功試行と失敗試行が同数現れる尤度は \\(\\frac{e^{\\theta/2}}{1+e^\\theta}\\) となり，これは \\(t(7,0,2.5)\\) でよく近似される．\n\\(t(1,0,2.5)\\) から \\(t(7,0,2.5)\\) はいずれも \\([-5,5]\\) に多くの重みを持つが，\\(\\nu=1\\) に近いほど裾が重くなる．\\(g(\\mu)\\) のスケールで \\(+5\\) をすることは，確率を \\(0.01\\) から \\(0.5\\) にすることに等しいため，切片項への \\(t\\)-事前分布は \\(\\mu\\in[0.01,0.99]\\) を強く仮定していることは含意する．\n\n\n1.6 ベイズ計算\n一般に二項回帰モデルはベイズ計算法の良いベンチマークになる．\n大規模なモデルでは事前調整ありの期待伝搬と乱歩 MH が強いが，Gibbs サンプラーや SMC サンプラーも十分良い性能を示す一方で，NUTS などの HMC ベースの手法は苦しむという (Chopin and Ridgway, 2017)．\n特に説明変数の次元 \\(p\\) が大きい場合の事後分布サンプリングはまだまだ難しいことが知られている．\n\n\n1.7 分散分析\n(Gelman et al., 2014, p. 423) 16.5 節は良い例である．アメリカ合衆国における国民の投票行動をロジットリンクにより二項モデルで一般化線型回帰をしている．Bayes ANOVA により人種による大きな効果と同時に，人種と州の強い交差効果が発見できている．\n\n\n1.8 選択モデル\n計量経済学の分野で古くから使われている潜在変数表現 1.3 の拡張として，選択モデル (choice model) がある．\nこれに関しては 階層ロジスティックモデルの稿 で詳しく扱う．\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#多項モデル",
    "href": "posts/2024/Survey/BDA2.html#多項モデル",
    "title": "ベイズデータ解析６",
    "section": "2 多項モデル",
    "text": "2 多項モデル\n\n2.1 はじめに\n順序応答 \\(y\\in\\{1,\\cdots,k\\}\\) の場合，\\(y\\) がカウントを表す場合は Poisson モデル，複数回繰り返される独立試行の成功回数である場合は二項モデルが考えられる．\n一方で多項モデル（正確にはカテゴリカルモデル） \\[\nY\\sim\\operatorname{Cat}(\\alpha_1,\\cdots,\\alpha_k),\\qquad\\sum_{j=1}^k\\alpha_j=1,\n\\] も想定できる．\\(y\\) が全く構造を持たない名目 (nominal) 応答である場合，ほぼ唯一の選択である．\n\n\n2.2 名目応答に対する多項モデル\n名目応答が \\(k\\) 次元ベクトル \\(y_i\\in\\mathbb{N}^k\\) の形で与えられている場合， \\[\ny_i\\sim\\mathrm{Mult}(n_i;\\alpha_{i1},\\cdots,\\alpha_{ik}),\\qquad\\sum_{j=1}^k\\alpha_{ij}=1,\n\\] というモデルを想定できる．\\(n_i\\equiv1\\) の場合，応答を one-hot 表現にしたカテゴリカルモデルに対応する．\nリンク関数 \\(g\\) は，特定のカテゴリ \\(j=1\\) を 基準 (reference) として \\(g(\\alpha):=\\log\\frac{\\alpha}{\\alpha_{i1}}\\) と定めることが多い： \\[\n\\log\\frac{\\alpha_{ij}}{\\alpha_{i1}}=X_i\\beta^{(j)}.\n\\] これを 多項ロジスティック回帰 (multinomial logistic regression) (Ding, 2024, p. 228) または ソフトマックス回帰 (softmax regression) (Kruschke, 2015, p. 650) という： \\[\n\\alpha_{ij}=\\operatorname{softmax}(X_i\\beta^{(j)})=\\frac{e^{X_i\\beta^{(j)}}}{\\sum_{l=1}^ke^{X_i\\beta^{(l)}}},\\qquad \\beta^{(1)}=0.\n\\]\n係数は，説明変数が \\(1\\) 単位増加した際の，参照カテゴリ \\(j=1\\) に対する対数オッズ比の変化を表す．\n一方で条件付きロジスティック回帰 (conditional logistic regression) なる方法もある (22.2節 Kruschke, 2015, pp. 655–)．\n\n\n2.3 効用による表現\n多項ロジスティックモデルも潜在変数解釈 1.3 が可能である．\n\\(k\\) 次元の標準 Gumbel 分布に従う誤差 \\(\\epsilon_{i-}\\in\\mathbb{R}^k\\) に関して， \\[\nU_{il}:=\\beta_l^\\top X_l+\\epsilon_{il},\\qquad l\\in[k],\n\\] を潜在的な効用とし， \\[\nY_i:=\\operatorname*{argmax}_{l\\in[k]}U_{il}\n\\] によって応答が定まると見ることができる (McFadden, 1974)．\nただしこの表示からはっきりわかるように，各選択肢を選ぶ効用は他の選択肢とは独立に決まることが仮定されている．これを IIA (Independence of Irrelevant Alternatives) ともいう．\nこのような仮定は，文脈効果（「選択肢セットの配置によって消費者の選好する選択肢が容易に変化する」 (竹内真登 and 猪狩良介, 2022) 現象）を主な関心とするマーケティングの分野では非常に不適当なものになる．\n\n\n2.4 順序応答に対する多項モデル\n応答 \\(y_i\\in\\{1,\\cdots,k\\}\\) に自然な順序がある場合，カテゴリ確率 \\(\\alpha_{i1},\\cdots,\\alpha_{ik}\\) の代わりに累積確率 \\[\n\\pi_{ij}:=\\sum_{l\\le j}\\alpha_{il}=\\operatorname{P}[Y_i\\le j]\n\\] を考え，モデルにその順序構造を自然に伝えることができる．\nこの場合リンク関数はロジットやプロビットが使える： \\[\n\\log\\frac{\\pi_{ij}}{1-\\pi_{ij}}=X_i\\beta^{(j)}.\n\\] \\(\\beta^{(j)}\\equiv\\beta\\) と取ることも多い．\nこのモデルを，\\(g\\) がロジットである場合順序ロジスティック回帰 (ordinal / ordered logistic regression) (McCullagh, 1980) といい (Kruschke, 2015, p. 671)，\\(g\\) がプロビットである場合順序プロビット回帰 (ordered probit regression) という．\n異なるパラメータ付け \\[\n\\operatorname{logit}(\\pi_{ij})=\\beta_0^{(j)}-X_i\\beta\n\\] を用いた場合，これを 比例オッズロジットモデル (proportional odds logit model) ともいう (Ding, 2024, p. 232)．\n\n\n2.5 Poisson モデル\n応答が \\(\\{1,\\cdots,k\\}\\) カテゴリのカウント \\(y=(y_1,\\cdots,y_k)\\) である場合， \\[\ny_i\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{Pois}(\\lambda_i)\n\\] というモデルを想定できる．\nカウント総数 \\(n:=\\sum_{i=1}^ky_i\\) が既知である場合，これに関して条件付けると \\[\ny\\bigg|\\sum_{i=1}^ky_i=n\\sim\\mathrm{Mult}(n;\\alpha_1,\\cdots,\\alpha_k),\\qquad\\alpha_i:=\\frac{\\lambda_i}{\\sum_{j=1}^k\\lambda_j}\n\\] という周辺モデルを想定したことになる．\nリンク関数には対数関数 \\(g=\\log\\) を用いることが多い．\n\n\n2.6 トーナメントデータ\n一度に２人の単位が勝負をし，どちらが勝利したかのデータに対する標準的なモデルに，(Bradley and Terry, 1952) モデルがある．国際チェス連盟や欧州囲碁連盟で選手のランクづけにも採用されている (Hastie and Tibshirani, 1998)．\nこのモデルでは各プレイヤーに能力パラメータ \\(\\alpha_i\\) を与え，能力の差のロジスティック関数 \\[\n\\operatorname{P}[i\\;\\text{defeats}\\;j]=\\frac{e^{\\alpha_i-\\alpha_j}}{1+e^{\\alpha_i-\\alpha_j}}\n\\] という確率で勝敗が決まるとする．\\(\\lambda_i:=e^{\\alpha_i}\\) というパラメータづけもよく用いられる．\nこのモデルは「勝利」「引き分け」「敗北」の３応答に対する確率モデルを調節することで，引き分け (Rao and Kupper, 1967) や先手有利 (Davidson and Beaver, 1977), (Agresti, 2012) などの情報も取り入れられるように簡単に拡張できる．\nこのように２値応答ではなく多値応答とみても，前節の Poisson モデルの定式化に帰着させることで，一般化線型モデリングの枠組みに合流させることができる (Gelman et al., 2014, pp. 427–428)．\n\n\n2.7 順位データ\nランキングデータは，トーナメントのような常に１対１比較のみを通じて情報が得られるわけではない．多者比較 (multiple comparison) も取り入れた Bradley-Terry モデルより一般的なものが (Plackett, 1975)-(Luce, 1959) モデルである．\nPlackett-Luce モデルでは，参加者 \\([K]:=\\left\\{1,\\cdots,K\\right\\}\\) の強さに当たる潜在変数 \\(\\lambda_k&gt;0\\) を導入し，これに比例した「優勝」確率を持つとする．そして順位の決定は，参加者のプールから「勝ち抜け」の１人を決定する独立試行の繰り返しと見て， \\[\n\\operatorname{P}[Y=y|\\lambda]=\\prod_{j=1}^{K-1}\\frac{\\lambda_{y_j}}{\\sum_{l=j}^K\\lambda_{y_l}}\n\\] としてモデリングする．\nこのモデルは，「陽の目を見る瞬間」 \\[\nW_k\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{Exp}(\\lambda_k)\n\\] という到着時刻があり，この時刻の早さで順位が決まる \\[\n\\operatorname{P}[W_{y_1}&lt;\\cdots&lt;W_{y_K}|\\lambda]=\\operatorname{P}[Y=y|\\lambda]=\\prod_{j=1}^{K-1}\\frac{\\lambda_{y_j}}{\\sum_{l=j}^K\\lambda_{y_l}}\n\\] という潜在変数表現を持つ．これは Thurstone 表現 (Thurstone, 1927)，またはランダム効用モデルと呼ばれる (Chapter 9 Diaconis, 1988, p. 167)．\nこのような潜在変数表現を元にした MM アルゴリズム (Hunter, 2004) や Gibbs サンプラー (Caron and Doucet, 2012) に基づく推論法が存在する．PlackettLuce パッケージ (Turner et al., 2020) も参照．\nさらに Plackett-Luce モデルで引き分けを許すように拡張したものが Gibbs サンプラーとともに (Henderson, 2024) で提案されている．順位データとレーティング（点数付け）のデータを同時に扱うことができるような拡張が (Pearce and Erosheva, 2025) で提案されている．\n\n\n2.8 対数線型モデル\n\\(y\\) も \\(x\\) も名目応答である場合，これは分割表解析の問題になる．\nこの際には 対数線型モデル (log-linear model) も考えられる．\nそれぞれのセルに Poisson モデルをおき，そのパラメータを代理応答変数として，対数リンクにより一般化線型回帰を行うものである．\nこのモデルは，サンプルサイズ \\(N\\) が既知の場合などの下では，周辺分布に多項モデルをおくことに等価である (Yates, 1934)．\n対数線型モデルは分割表解析だけでなく，多重代入法 などの欠測データ解析にも応用される．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#コピュラモデル",
    "href": "posts/2024/Survey/BDA2.html#コピュラモデル",
    "title": "ベイズデータ解析６",
    "section": "3 コピュラモデル",
    "text": "3 コピュラモデル\n\n3.1 はじめに\n以上の質的変数のモデルは，いずれも潜在的な連続変数の離散化としてデータを理解するものであった．\n仮にこの潜在変数により興味がある場合，この潜在変数をより具体的に，特にその相関構造をモデリングしたいということになる．\n(Lopez-Paz et al., 2013)"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#文献紹介",
    "href": "posts/2024/Survey/BDA2.html#文献紹介",
    "title": "ベイズデータ解析６",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n(Gelman et al., 2014) の 16 章で一般化線型モデルが扱われている．(Kruschke, 2015) はさらに詳しく，22 章で名目応答，23 章で順序応答，24 章でカウントデータを扱っている．\n(12 章 Hoff, 2009) にて正規コピュラモデルが ordinal probit モデルの，潜在変数を多次元に拡張した場合として導入されている．\n(Chib and Winkelmann, 2001) はリンクを対数関数とし，Poisson 周辺分布を持つカウントデータのコピュラモデリングを実行している．\n(Quinn, 2017) は連続確率変数に対してコピュラモデリングを実行している．"
  },
  {
    "objectID": "posts/2024/Survey/BDA2.html#footnotes",
    "href": "posts/2024/Survey/BDA2.html#footnotes",
    "title": "ベイズデータ解析６",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ding, 2024, p. 222) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html",
    "href": "posts/2024/Survey/Survey2.html",
    "title": "ベイズデータ解析２",
    "section": "",
    "text": "分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#関連記事",
    "href": "posts/2024/Survey/Survey2.html#関連記事",
    "title": "ベイズデータ解析２",
    "section": "",
    "text": "分散分析\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n標本調査データと欠測データの扱い\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンケートデータとデータ統合\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#はじめに",
    "href": "posts/2024/Survey/Survey2.html#はじめに",
    "title": "ベイズデータ解析２",
    "section": "1 はじめに",
    "text": "1 はじめに\n\n1.1 概観\n現代の因果推論は平均処置効果 (ATE) \\[\n\\tau:=\\operatorname{E}[Y_i^1]-\\operatorname{E}[Y_i^0]\n\\] と関連する推定対象 (estimand / target parameter) に集中している．\n\n1.1.1 潜在結果モデル\nこのように 因果効果 と呼ばれる推定対象を設定し，良い実験計画を構築してこれを推定するという枠組みは (Neyman et al., 1990) から始まるもので，(Rubin, 1974) の因果モデルや 潜在結果モデル (potential outcome model)，または 反実仮想モデル (counterfactual model) とも呼ばれる．\n\n\n\n\n\n\n潜在結果モデルを用いた因果推論\n\n\n\n潜在結果モデルは特定の介入の因果効果を推定する枠組みであり，十分に考慮された実験計画が必要であるが，それにより不必要なモデリングの議論を避けることができる．\nこのために，極めて多くの人間を対象とする科学分野で潜在結果モデルが用いられている．\n\n計量経済学\n医学・疫学・生物統計学\n社会学 (Morgan and Winship, 2014)\n\n\n\nこの方法では，ランダム化された実験，あるいは欠測メカニズムが推定しやすいように工夫された「擬似実験」を行なうことで，ほとんどモデリングの議論を表に出さずとも ATE やその他の実験科学者が設定する量を不偏推定可能にする，というアプローチをとる．1\n最終的に ATE の推定においては，各個体 \\(i\\in[n]\\) に対して処置を行った場合の結果 \\(Y_i^1\\) と行わなかった場合の結果 \\(Y_i^0\\) とのいずれかは必ず欠測するということである．2\n端的に言えば，統計学のサンプリング論と科学の実験計画論との邂逅である (Ding, 2024a)．\n\n\n1.1.2 構造的因果モデル\nしかし他の多くの科学では実験的介入が難しかったり，実験計画だけでは背後の交絡要因が統制しきれない状況がある．\nまたはそもそも，科学的な興味の対象が「因果効果」だけでなくデータの背後にある「モデル」にもある場合も多い．\nこのような場合には，変数同士の関係を丁寧にモデリングし，加えて識別可能性を確保するなどの理論的な配慮が欠かせない．\nこれを可能にするのが 構造的因果モデル (SCM: Structural Causal Model) またはノンパラメトリック構造方程式モデルの枠組みである (Bongers et al., 2021)．\n多くはグラフィカルモデルと計算機的な方法を組み合わせることで，推定可能な高次元モデルを構築する．このモデルに対する「変換」として，介入操作と因果効果を定義する．\n\n\n\n\n\n\n構造的因果モデルを用いた因果推論\n\n\n\n自然を対象にする科学分野を中心として，介入できない状況下での因果推論が構造的因果モデルの枠組みで行われる．\n\n地球科学 (Runge et al., 2023)\n機械学習：エキスパートシステム (Pearl, 1988)，継続学習 (Cui and Athey, 2022)，反実仮想機械学習 (M. Huber, 2023)\n医学・疫学・生物統計学：標的学習 (targeted learning) (Laan and Rose, 2011)\n\nモデリングが必要不可欠であるため，計算的にも困難な問題となる．それゆえ計算機科学や機械学習の分野で盛んに研究されている．\n\n\n\n\n1.1.3 まとめ\n\n\n\n\n\n\n\n潜在結果アプローチ：実験を行うことでモデリングを回避する．\n構造的因果モデル：モデリングを行い，モデルの変換として因果効果を定義する．特定の条件下 (Rubin, 1980) で離散変数に実験処置介入を行った場合として，潜在結果アプローチを含むとも見れる．3\n\n\n\n\n歴史的には，どちらかというと構造的因果モデル → 潜在結果モデルという順に注目された．\nこの現象は特に経済学で顕著に起こった．Cowles 委員会のイニシアティブの下で，初めは構造的なアプローチを取っていた経済学が，実験事実との乖離が激しいことの自覚から，実験と統計的推論を取り入れるように生まれ変わった現象は 信頼性革命 と呼ばれている．\n本章では以降，各分野における因果推論の歴史を議論する．\n\n\n\n1.2 経済学における因果推論の歴史\n経済学において，(Haavelmo, 1943) は「構造推定」の枠組みで政策介入の因果効果を推定しようとした．4\n構造推定では「同時方程式」により統計モデルを定義するが，その際には識別可能性が問題になる．\n当時の計算資源では十分な推定を実行することができず，加えて，マクロなモデルに対する「介入」の定義を正しく与えていなかった (Lucas, 1976)．5\n時代が下ると，このアプローチでの経済学は大きな批判に晒され，より実験的なアプローチを採用するように変化を余儀なくされた．これが信頼性革命である．\n\n\n1.3 計量経済学における信頼性革命\n(Leamer, 1983) は計量経済学の手法と古典的な実験科学とを比較し，計量経済学の信頼性は感度解析とロバストを取り入れることによって（のみ）回復されるだろうと論じた．\n(LaLonde, 1986) は職業訓練の効果に関する観察研究と実験研究の結果が大きく異なることを示し，当時の計量経済学が抱えていた体質に抜本的改革を迫った．\n(Leamer, 1983) が「よく計画された実験を超える統計的手法など出てこないかもしれない」と論じていた通り，その後の信頼性革命は主に実験計画を改善することと擬似ランダム化の適切な取り扱いによって達成された (Angrist and Pischke, 2010)．\nそのキーワードは「自然実験」や「擬似実験」と呼ばれており，サーベイ手法での「擬似ランダム化」アプローチに相当する．擬似ランダム化については次稿も参照．\n\n\n1.4 媒介分析\n媒介分析 (mediation analysis) (Robins and Greenland, 1992) においては，因果の流れが複数あり得る場合に，媒介因子 \\(Z\\) を経由した 間接効果 の量を総合効果の中から識別することを目標とする．\n\n\n\n媒介変数 \\(Z\\) を表す図式\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\nNo matching items\n\n\n一方でモデリングに基づいた方法も可能である (Pearl, 2012), (Nguyen et al., 2021)．6\nはじめ社会学や社会心理学においてはモデルによる媒介分析が試みられていた (Alwin and Hauser, 1975), (Baron and Kenny, 1986)．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#交絡調整法",
    "href": "posts/2024/Survey/Survey2.html#交絡調整法",
    "title": "ベイズデータ解析２",
    "section": "2 交絡調整法",
    "text": "2 交絡調整法\n\n2.1 はじめに\n因果推論において，実験計画による工夫に限界がある際は，条件 \\[\n(Y^0_i,Y^1_i)\\perp\\!\\!\\!\\perp A_i|X_i\n\\tag{1}\\] を満たす共変量 \\(X_i\\) の特定を目指す．\nこの \\(X_i\\) を 交絡因子 (cofounders) といい，条件 (1) を 非交絡性 (unconfoundedness) または 無視可能性 (ignorability) という．\n\n\n2.2 操作変数法\n操作変数 (instrumental variable) とは，処置変数（または説明変数）をよく予測するような補助変数であり，補助変数と処置変数の間の関係を推定することで擬似的に層別サンプリングが行われたとみなせるようなものである．7\n未観測の交絡因子が予期される場合でも，操作変数が利用可能である場合はこれを調整することができる．\n\n\n\n\n\n\n計量経済学的な説明8\n\n\n\n\n\n回帰モデル \\[\nY=X_1^\\top\\beta_1+X_2^\\top\\beta_2+\\epsilon\n\\] において，\\(X_1\\) は外生性を持つが，\\(X_2\\) は内生性を持ってしまうとする： \\[\n\\operatorname{E}[X_1\\epsilon]=0,\\quad\\operatorname{E}[X_2\\epsilon]\\ne0.\n\\] このとき，次の３条件を満たす \\(X\\) と同次元の \\(Z\\) を操作変数という：\n\n外生性：\\(\\operatorname{E}[Z\\epsilon]=0\\)．\n多重線型性の非存在：\\(\\operatorname{rank}\\operatorname{E}[ZZ^\\top]&gt;0\\)．\n関連性：\\(\\operatorname{rank}\\operatorname{E}[ZX^\\top]=\\operatorname{rank}\\operatorname{E}[X]\\)．\n\n操作変数 \\(Z_2\\) を \\(X_2\\) の代わりに説明変数に用いることで内生性の問題が除去される．このため \\(Z_2\\) は 排除されていた外生変数 (excluded exogenous variable) ともいう．\n\n\n\n操作変数が存在するとき，遵守者の平均処置効果，すなわち 局所平均処置効果 (LATE: Local Average Treatment Effect) が識別可能になる (Imbens and Angrist, 1994)．\n\n\n2.3 回帰非連続デザイン\n回帰不連続デザイン (RDD: Regression Discontinuity Design) では，割り当ての閾値の近傍では擬似ランダム化が行われていると仮定できる状況において，閾値の近傍に位置した部分標本を用いて，その部分標本での処置効果を推定する．\n\n\n2.4 差の差法\n差分の差法 (DID: Difference-in-Differences) は，被曝群と比較群それぞれの処置前後の差分に現れる差分を，処置効果の近似とみなす方法である．\n被曝群と比較群をマッチングすることで共変量を統制することが期待されるが，処置の有無と関係を持つ未統制の共変量の調整が問題となる (Bertrand et al., 2004)．\n\n\n2.5 周辺構造モデル\n周辺構造モデル (marginal structural model) は平均処置効果をパラメータに持つモデルであり (Robins, 2000)，潜在結果変数の（周辺）平均構造をモデリングする： \\[\ng(\\operatorname{E}[Y^a_i|L_i])=\\psi_0+\\psi_1a+\\psi_2L_i+\\psi_3L_ia.\n\\]\nさらに \\(Y^a|L,A\\) の分布に指数型分布族を仮定したものは 一般化線型モデル (GLM) と呼ばれるが，周辺（構造）モデルは GLM から分布の仮定を除去した一般化とも見れる．9\n統計ソフトの充実によりよく使われるようになったが，後述の構造的ネストモデルと \\(G\\)-推定の方が一般的であり，より効率的である (Vansteelandt and Joffe, 2014) との見解もある．\n\n\n2.6 構造的平均モデルと \\(G\\)-推定\n\\(G\\)-推定 (Robins et al., 2000) は不服従など処置変数 \\(D\\) に依存した交絡を調整するために，構造的平均モデル，パラメトリック \\(G\\)-公式 (Robins, 1986)，構造的ネストモデル (structural nested model) (Robins et al., 1992) と同時に提案された推定手法である．\n構造的平均モデル (SMM: Structural Mean Model) では，リンク関数 \\(g\\) の自由度を残して \\[\ng(\\operatorname{E}[Y^a|L=l,A=a])-g(\\operatorname{E}[Y^0|L=l,A=a])=\\gamma^*(l,a;\\psi^*)\n\\] により処置 \\(A=a\\) の平均因果効果にパラメトリックな仮定をおく．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#誤差分布に仮定を置かない推定手法",
    "href": "posts/2024/Survey/Survey2.html#誤差分布に仮定を置かない推定手法",
    "title": "ベイズデータ解析２",
    "section": "3 誤差分布に仮定を置かない推定手法",
    "text": "3 誤差分布に仮定を置かない推定手法\n\n3.1 はじめに\n疫学や医療統計では一般化推定方程式，さらに一般的な枠組みとして計量経済学では一般化モーメント法など，モデルを全面に押し出さずに推定目標を定義し，これを最尤推定にヒントをもらった方法で推定する手法が用いられる．\nこのように関心のある母数以外の 局外母数 (nuisance parameter) にはモデルを明示的に想定しない手法を セミパラメトリック法 (semi-parametric method) という．\nこのような手法では，興味のあるパラメータがはっきりしているため，それ以外のモデルの仮定にはひとまず興味がなく，誤特定の下でも効率的な推論ができるロバスト性が重視される．10\n\n\n3.2 共通の枠組み\nある関数 \\(g\\) に関して， \\[\n\\operatorname{E}[g(\\beta,X,Y)]=0\n\\tag{2}\\] によって推定対象 \\(\\beta\\) を特徴付ける場面は多い．11\n条件 (2) によって推定対象 \\(\\beta\\) が定義されているとき，標本上の対応する方程式 \\[\n\\frac{1}{n}\\sum_{i=1}^ng(\\beta,X_i,Y_i)=0\n\\tag{3}\\] の解として推定量を構成することが自然な発想になる．\nこの枠組みは種々の名前で呼ばれる．\n\n\n\n\n\n\n名前一覧\n\n\n\n\n数理統計学・頑健統計では極値点として定義される \\(M\\)-推定量（第 3.3 節）と区別して \\(Z\\)-推定量 とも呼ばれる (van der Vaart, 1998)．\n疫学では \\(g\\) を 推定関数 (Godambe, 1997)，式 (2) を推定関数の 不偏性，式 (3) を（一般化） 推定方程式 という．\n計量経済学では 一般化モーメント法 (GMM: Generalized Method of Moments) (L. P. Hansen, 1982) として知られる．式 (3) を モーメント方程式 という．\n\n\n\n一般化モーメント法は，\\(Y\\) の分布に関する議論を伴わないリサーチクエスチョンに応えるために格好の枠組みである．\n例えば線型回帰係数に対する OLS 推定量は \\(g(\\beta,X)=X(Y-X^\\top\\beta)\\) によって定まる一般化モーメント推定量である．\n操作変数推定量は \\(g(\\beta,X,Y,Z)=Z(Y-X^\\top\\beta)\\) によって定まる一般化モーメント推定量である．\nこのクラスの推定量はモデルに依存しない方法を与える上に，漸近正規性などの好ましい性質を持つ (L. P. Hansen, 1982) ため，これを通じて推定量の分散を推定することもできる．\n\n\n3.3 \\(M\\)-推定量\nなお，前述の推定量は \\(M\\)-推定量と呼ばれてしまうこともあるが，数理統計学では，最尤推定量のように，特定の目的関数 \\[\nM_n(\\theta):=\\frac{1}{n}\\sum_{i=1}^nm_\\theta(X_i)\n\\] を最大化する点として定義される推定量 \\(\\widehat{\\theta}\\) は \\(M\\)-推定量 と呼ばれる．\n同時に最尤推定量は，スコア関数の零点としても特徴付けられる (Carmer, 1946)．こう見た場合を \\(Z\\)-推定量と呼び分けることにする．\n大変大雑把に言えば，モーメント法 → 最尤推定量 → \\(Z\\)-推定量という歴史的な流れがある (Le Cam, 1952)．\n\n\n\n\n\n\n\\(M\\)-推定量の別名一覧\n\n\n\n\n頑健統計において \\(M\\)-推定量 (P. J. Huber, 1964) と呼ばれる．12\n数理統計において最小コントラスト推定量 (Pfanzagl, 1969) と呼ばれる．\n\n\n\n頑健統計に起源を持つ \\(M\\)-推定量，一般に \\(Z\\)-推定量は，分布の誤特定の下でも極めて安定して一致性と漸近正規性をもつ推定量のクラスとして導入された．\n推定方程式 (3) を近似的に解いても大丈夫だし，推定関数の不偏性 (2) が漸近的にしか成り立たなくてもほとんど問題がない．\n以下では，特別な \\(Z\\)-推定量を見ていく．\n\n\n3.4 一般化推定方程式\n一般化推定方程式は，一般化線型モデル (Nelder and Wedderburn, 1972) に基づいて推定した \\(V_i\\) を代入した擬似スコア \\[\nU(Y_j,\\beta|X_j)=(\\partial_\\beta\\mu(X_j^\\top\\beta))^\\top V_i^{-1}(Y_i-\\mu(X_i^\\top\\beta))\n\\] を推定関数に用いた一般化モーメント法をいう．\n\\(U\\) を擬似スコアと呼んだのは，誤差の分散 \\(V_i\\) の計算に用いたモデルは仮定として認めているわけではなく，主に推定効率や所望の推定量の性質を得るために「作業仮設として」設定されたものであるためである．13\n\\(V_i\\) は作業共分散⾏列ともいう．\\(V_i\\) を代入した尤度を 擬似尤度 (quasi-likelihood) (Wedderburn, 1974) ともいう．\nこの方法では，真の誤差分布が相関を持つようなものであった場合でも，平均構造に関する逆リンク関数 \\(\\mu\\) の特定にさえ成功すれば，\\(\\beta\\) に関して一致推定を可能にする．\n誤差分布を特定せずとも実行できる最尤推定の一般化とも見れる．\n\\(V_i\\) の特定に成功した場合は，セミパラメトリック最適な推定量を与える．\n二次のモーメントに関しても関心がある場合は，混合効果モデルなどを通じたモデル化が必要になる．\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.5 経験尤度法\nデータ \\(\\{x_i\\}_{i=1}^n\\subset\\mathbb{R}^d\\) に関する 経験尤度 (empirical likelihood) (Owen, 1988) とは，分布関数の汎函数 \\[\nL(F):=\\prod_{i=1}^nF(X_i)-F(X_i-)=\\prod_{i=1}^n\\operatorname{P}[X=x_i]\n\\] をいう．\nこの観点から，経験分布関数 \\(F_n\\) は経験尤度を最大にするノンパラメトリック推定量である．\n最尤法はモデルの全ての母数を特定化しない限り実行できないが，経験尤度の最大化ならば可能である．14\n\\[\n\\sum_{i=1}^np_ig(x_i,\\theta)=0\n\\] を満たす中で経験尤度を最大化する \\(\\theta\\) を 最大経験尤度推定量 (MELE: maximum empirical likelihood estimator) (Qin and Lawless, 1994) という．\nこの推定量は漸近正規性を持ち，一般化推定方程式や一般化モーメント法の代替手法として期待されている．"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#文献案内",
    "href": "posts/2024/Survey/Survey2.html#文献案内",
    "title": "ベイズデータ解析２",
    "section": "4 文献案内",
    "text": "4 文献案内"
  },
  {
    "objectID": "posts/2024/Survey/Survey2.html#footnotes",
    "href": "posts/2024/Survey/Survey2.html#footnotes",
    "title": "ベイズデータ解析２",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n科学的な興味の対象は多くの場合モデル全体ではなく，特定の１つのパラメータであり，そのような場合は実験計画を工夫することでモデルに関係なく推定可能になるという発想は (Heckman, 2010) により “Marschak’s Maxim” と呼ばれる．詳しくは (Laan and Rose, 2011) のPearl による foreword も参照．↩︎\n因果推論の根本問題 (Holland, 1986) とも呼ばれる．↩︎\n(学, 2016) も参照．↩︎\n(川口康平 and 澤田真行, 2024), (Pearl, 2015) も参照．↩︎\n詳しくは ルーカス批判 (Wikipedia) も参照．↩︎\n(Laan and Rose, 2011) の Pearl による Foreword も参照．(Ryosuke Fujii and Suzuki, 2022) の オンラインページも参照．↩︎\nただし当然結果に依存してはいけない．(Section 2 Imbens and Angrist, 1994, p. 468) や (Section 3.1 Hernán and Robins, 2020, p. 28) も参照．↩︎\n(Section 12.5 B. E. Hansen, 2022, p. 335) も参照．↩︎\n(Ding, 2024b, p. 278) も参照．↩︎\nそれゆえ最尤法やベイズ法のように，一旦はモデルの想定が必要な手法が忌避されるところがある．↩︎\n\\(X,Y\\) を任意の定数とした際に，\\(\\beta\\) に関して一意な解を持つとき，モーメント条件は 識別可能 であるという．↩︎\n他には \\(L\\)-, \\(R\\)-推定量があった (Section 3.2 P. J. Huber, 1981, p. 43)．↩︎\n\\(V_i\\) の特定に成功している場合，そのパラメトリックモデルに関するスコア関数に一致する．↩︎\n一般化推定方程式のように，作業的な値を代入して得る尤度は 擬似尤度 (quasi-likelihood) という．↩︎"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html",
    "title": "Measurability of the Minkowski Sum of Two Sets",
    "section": "",
    "text": "This entry has grown out of a question I answered on MathOverflow. I will try to explain the question and my answer in a more leisurely manner here."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#introduction",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#introduction",
    "title": "Measurability of the Minkowski Sum of Two Sets",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n\n\n\n\n\nQuestion\n\n\n\nFor Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), Minkowski sum is defined as \\[\nA+B:=\\left\\{a+b\\in\\mathbb{R}^n\\mid a\\in A,b\\in B\\right\\}.\n\\] We are interested in the following questions:\n\nUnder what conditions is \\(A+B\\) Borel?\nFor what \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\) is \\(A+B\\) not Borel?\n\n\n\n\\(A+B\\) being an image of a continuous mapping \\(+:\\mathbb{R}^n\\times\\mathbb{R}^n\\to\\mathbb{R}^n\\), \\(A+B\\) is an analytic (a.k.a. Souslin) set. Therefore, \\(A+B\\) is Lebesgue measurable, given all Souslin sets are universally measurable.1\nA statistician or probability theorist may come across this problem when considering isoperimetric inequalities. For example, the one by (Borell, 1975) and (Sudakov and Tsirel’son, 1974) goes as follows:\n\n\n\n\n\n\nTheorem2 (Gaussian isoperimetric inequality)\n\n\n\nLet \\(\\gamma_n:=\\mathrm{N}_n(0,I_n)\\) be the standard Gaussian measure on \\(\\mathbb{R}^n\\), \\(u\\in\\partial B^n\\subset\\mathbb{R}^n\\) a unit vector, \\(A\\in\\mathcal{B}(\\mathbb{R}^n)\\) be a Borel measurable set, and \\[\nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\quad a\\in\\mathbb{R},\n\\] be a affine half-space satisfying \\(\\gamma_n(H_a)=\\gamma_n(A)\\). Then, the following inequality holds for all \\(\\epsilon&gt;0\\): \\[\n\\gamma_n(H_{a+\\epsilon})=\\gamma_n(A_\\epsilon),\\qquad\\epsilon&gt;0,\n\\] where \\(B^n\\overset{\\textrm{closed}}{\\subset}\\mathbb{R}^n\\) is a closed unit ball centered at the origin, and \\[\nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\:\\middle|\\:\\inf_{y\\in A}\\|x-y\\|_2\\le\\epsilon\\right\\}\n\\] is the closed \\(\\epsilon\\)-neighborhood of \\(A\\).\n\n\nHere, \\[\nA_\\epsilon=A+\\epsilon B^n\n\\] so the Borel measurability of \\(A+B^n(0,\\epsilon)\\) matters.\nOf course, \\(A_\\epsilon\\) is \\(\\gamma_n\\)-measurable, meaning that there exist Borel sets \\(B_1,B_2\\in\\mathcal{B}(\\mathbb{R}^n)\\) such that \\[\nB_1\\subset A_\\epsilon\\subset B_2,\n\\] \\[\n\\mu(B_2\\setminus B_1)=0.\n\\] Thus, the above theorem can be understood as implicitly assuming the Borel probability measure \\(\\gamma_n\\) to be completed in the Lebesgue sense.\nAs it turns out in Section 3.2, the Borel measurability of \\(A+B^n(0,\\epsilon)\\) is not guaranteed, despite the fact \\(B^n(0,\\epsilon)\\) is a closed and compact subset."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#conditions-assuring-to-be-borel",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#conditions-assuring-to-be-borel",
    "title": "Measurability of the Minkowski Sum of Two Sets",
    "section": "2 Conditions assuring to be Borel",
    "text": "2 Conditions assuring to be Borel\n\n\n\n\n\n\nProposition\n\n\n\nLet \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\) be Borel sets.\n\nIf either \\(A\\) or \\(B\\) is open, then \\(A+B\\) is open.\nEven when \\(A\\) and \\(B\\) are closed, \\(A+B\\) may not be closed.\nAdditionally imposing either \\(A\\) or \\(B\\) to be compact, then \\(A+B\\) is closed.\n\nAll of the above statements remain valid when an arbitrary topological vector space is considered in place of \\(\\mathbb{R}^n\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nGiven \\[\nA+B=\\bigcup_{b\\in B}(A+b),\n\\] we see that \\(A+B\\) is open if either \\(A\\) or \\(B\\) is open. Note \\(\\bullet+b:\\mathbb{R}^n\\to\\mathbb{R}^n\\) is a homeomorphism, so \\(A+b\\) is open.\nLet \\(n=1\\) and \\[\nA:=\\mathbb{N}^+,\n\\] \\[\nB:=\\left\\{-n+\\frac{1}{n}\\in\\mathbb{R}\\:\\middle|\\:n=2,3,\\cdots\\right\\}.\n\\] Both sets \\(A,B\\) are discrete subsets of \\(\\mathbb{R}\\), hence closed. However, \\(A+B\\) is not closed. Indeed, \\(\\{1/n\\}_{n\\ge2}\\subset A+B\\) but its limit point \\(0\\notin A+B\\).\nLet us assume \\(A\\) to be compact and take an arbitrary sequence \\(\\{a_n+b_n\\}\\subset A+B\\) converging to a limit, denoted \\(x\\in\\overline{A+B}\\). Compactnes of \\(A\\) implies the existence of a convergent subsequence \\(\\{a_{n_k}\\}\\subset\\{a_n\\}\\) converging to some \\(a\\in A\\). Then, \\(\\{b_{n_k}\\}\\subset B\\) also converges and its limit is \\(x-a\\in B\\), which belongs to \\(B\\) because \\(B\\) is closed. Hence, \\(x=a+(x-a)\\in A+B\\), giving a sufficient condition for \\(A+B\\) to be closed."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#counterexamples",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#counterexamples",
    "title": "Measurability of the Minkowski Sum of Two Sets",
    "section": "3 Counterexamples",
    "text": "3 Counterexamples\n\n3.1 Using subgroups of \\(\\mathbb{R}\\)\n(Erdös and Stone, 1969) gives a counterexample for \\(n\\ge2\\). Astonishingly, for the case \\(n=1\\), the counterexample consists of \\(A\\) being a Cantor, hence compact, set and \\(B\\) being a \\(G_\\delta\\) set.\n\n\n3.2 Using a non-Borel Souslin set of \\(\\mathbb{R}^2\\).\nFor every uncountable Polish space, there exists a non-Borel Souslin set,3 i.e., \\(\\mathcal{B}(X)\\subsetneq\\Sigma^1_1(X)\\), where \\(\\Sigma^1_1(X)\\) represents the class of all Souslin sets of \\(X\\).\nTaking \\(X=[-1,1]\\), we can construct a non-Borel Souslin set \\(A_1'\\in\\Sigma^1_1(X)\\setminus\\mathcal{B}(X)\\), and using this \\(A_1'\\) we are going to construct a counterexample for \\(n\\ge3\\).\nHere, we are in need of the following characterization of Souslin sets:\n\n\n\n\n\n\nTheorem4\n\n\n\nLet \\(X\\) be a Souslin space, a Souslin set which is also Hausdorff, and let \\(A\\subset X\\) its subset. The following are equivalent:\n\n\\(A\\) is a Souslin set;\n\\(A\\) can be represented as \\(A=\\mathrm{pr}_1(F)\\), where \\(F\\overset{\\textrm{closed}}{\\subset}X\\times\\mathbb{N}^\\infty\\);\n\\(A\\) can be represented as \\(A=\\mathrm{pr}_1(B)\\), where \\(B\\subset X\\times\\mathbb{R}\\) is Borel measurable.\n\n\n\nHere we take \\(X:=[-1,1]\\) and \\(A:=A_1'\\), we can find a Borel measurable subset \\(A'\\subset[-1,1]^2\\) such that \\(A_1'=\\mathrm{pr}_1(A')\\).5\nThe next step is crucial, where we map the Borel subset \\(A'\\) to a cylinder \\[\nC:=\\left\\{(x_1,x_2,x_3)\\in\\mathbb{R}^3\\mid x_2^2+x_3^2=1\\right\\},\n\\] using a homeomorphism \\(\\psi:\\mathbb{R}^2\\to\\mathbb{R}^2\\) which satisfies \\[\n\\begin{align*}\n    &\\psi([-1,1]\\times\\{0\\})\\\\\n    &\\qquad\\subset\\left\\{(x_1,x_2)\\in\\mathbb{R}^2\\mid x_1^2+x_2^2=1\\right\\}.\n\\end{align*}\n\\] Such a homeomorphism \\(\\psi\\) takes the segment \\([-1,1]\\) on the \\(x_1\\)-axis into the unit circumference \\(S^1\\) in the \\((x_1,x_2)\\)-plane.\nUsing \\(\\psi\\) as a building block, we constract a homeomorphism \\(\\Psi:\\mathbb{R}^3\\to\\mathbb{R}^3\\) by \\[\n\\Psi(x_1,x_2,x_3):=(x_1,\\psi(x_2,x_3)).\n\\] Such a homeomorphism \\(\\Psi\\) pastes the set \\(A'\\) onto the surface of the cylinder \\(C\\): \\(A:=\\Psi(A')\\subset C\\). Given that \\(\\Psi\\) is a homeomorphism, \\(A\\) is a Borel measurable set.6\nThus, \\(A\\subset C\\subset\\mathbb{R}^3\\) now satisfies the following properties: \\[\n\\biggr(A+B(0,1)\\biggl)\\cap\\biggr(\\mathbb{R}\\times\\{0\\}^2\\biggl)=A_1',\n\\] where \\(A_1'\\notin\\mathcal{B}(X)\\) is non-Borel and \\(B(0,1)\\subset\\mathbb{R}^3\\) is a closed unit ball centered at the origin.7 This scenario is impossible if \\(A+B(0,1)\\) is Borel measurable, since \\(\\mathbb{R}\\times\\{0\\}^2\\) is Borel measurable.\nThis idea is stimulated from (Luiro et al., 2014), which has an arXiv version, Example 2.4."
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#footnotes",
    "href": "posts/2024/FunctionalAnalysis/MinkowskiSum.html#footnotes",
    "title": "Measurability of the Minkowski Sum of Two Sets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Dudley, 2002, p. 497) Theorem 13.2.6, (Kechris, 1995, p. 155) Theorem 21.20.↩︎\n(Borell, 1975), (Sudakov and Tsirel’son, 1974), (Giné and Nickl, 2021, p. 31) Theorem 2.2.3.↩︎\n(Kechris, 1995, p. 85) Theorem 14.2.↩︎\n(Bogachev, 2007, p. 24) Theorem 6.7.2, (Kechris, 1995, p. 86) 14.3.↩︎\nFrom the theorem, we can find a Borel measurable subset \\(B\\subset[-1,1]\\times\\mathbb{R}\\) such that \\(A_1'=\\mathrm{pr}_1(B)\\). Then, we define \\(A':=B\\cap[-1,1]^2\\), which is Bore measurable and still has the property \\(A_1'=\\mathrm{pr}_1(A')\\).↩︎\nFor a complete separable metric space \\(X,Y\\), an image of a Borel set via a Borel measurable injection is again Borel measurable. (Bogachev, 2007, p. 30) Theorem 6.8.6.↩︎\nFor other notations, please consult this post.↩︎"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#有界測度の基本概念",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#有界測度の基本概念",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "1 有界測度の基本概念",
    "text": "1 有界測度の基本概念\n\n\n\n\n\n\n定義1 (\\(\\mu\\)-inner regular, Radon, tight, regular)\n\n\n\n\\(X\\) を位相空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界な符号付き Borel 測度とする．2\n\nBorel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-内部正則 であるとは， \\[\n\\lvert\\mu\\rvert(B)=\\sup_{K\\overset{\\textrm{cpt}}{\\subset}B}\\lvert\\mu\\rvert(K)\n\\] を満たすことをいう．すなわち，任意の \\(\\epsilon&gt;0\\) に対して，あるコンパクト部分集合 \\(K\\overset{\\textrm{cpt}}{\\subset}B\\) が存在して， \\[\n\\lvert\\mu\\rvert(B\\setminus K)&lt;\\epsilon\n\\] を満たすことをいう．3\n\\(\\mu\\) が Radon であるとは，任意の Borel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-内部正則であることをいう．4\n\\(\\mu\\) が 緊密 であるとは，全体集合 \\(X\\) が \\(\\mu\\)-内部正則であることをいう．5\n\\(\\mu\\) が 正則 であるとは，任意の \\(\\epsilon&gt;0\\) に対して，ある閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) が存在して，\\(F\\subset B\\) かつ \\[\nB\\setminus F\\in\\mathcal{B}(X),\\quad\\lvert\\mu\\rvert(B\\setminus F)&lt;\\epsilon\n\\] を満たすことをいう．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#riesz-正則性",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#riesz-正則性",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "2 Riesz 正則性",
    "text": "2 Riesz 正則性\n\n\n\n\n\n\n変種\n\n\n\n\\(X\\) を位相空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界な符号付き Borel 測度とする．\n\nBorel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-外部正則 であるとは， \\[\n\\lvert\\mu\\rvert(B)=\\inf_{B\\subset U\\overset{\\mathrm{open}}{\\subset}X}\\lvert\\mu\\rvert(U)\n\\] を満たすことをいう．6\n\\(\\mu\\) が Riesz 正則 であるとは，任意の Borel 集合 \\(B\\in\\mathcal{B}(X)\\) が \\(\\mu\\)-外部正則かつ \\(\\mu\\)-内部正則であることをいう．\n\nこの Riesz 正則という語用法は筆者限りのものである．(Halmos, 1950, p. 224), (Dunford and Schwartz, 1958, p. 137), (Folland, 1984, p. 205), (Lang, 1993, p. 265), (Conway, 2007, p. 380) などでは単にこれを regular と呼ぶ．\\(X\\) が局所コンパクト Hausdorff 空間であるとき，このような語用法の方が一般的である．\nさらに，上述のうち４文献で共通するように，非有界な符号付き測度 \\(\\mu\\in\\mathcal{S}(X)\\) を考える際は，最低限次の条件を課し，これも regular であるための条件に入れる：\n\n\\(\\mu\\) が 局所有界 であるとは，任意の \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) 上で有限値であることをいう．\n\n\n\n\n\n\n\n\n\n命題7 ：内部と外部の正則性\n\n\n\n\\(X\\) を局所コンパクト Hausdorff 空間，\\(\\mu\\in\\mathcal{S}^1(X)\\) を有界 Borel 測度とする．次は同値：\n\n\\(\\mu\\) は Riesz 正則である．\n任意のコンパクト集合は \\(\\mu\\)-外部正則である．\n任意の有界な開集合は \\(\\mu\\)-内部正則である．\n\nまた，\\(X\\) 上の任意の Baire 測度は Riesz 正則である．\n\n\n\n\n\n\n\n\n定理8 ：Riesz 正則測度の延長 (Alexandroff, 1940, p. 590)\n\n\n\n\\(X\\) をコンパクト空間，\\(\\mathcal{A}\\subset P(X)\\) を集合体，\\(\\mu:\\mathcal{A}\\to\\mathbb{C}\\) を Riesz 正則で有界な有限加法的関数とする．このとき，\\(\\mu\\) は \\(\\sigma\\)-加法的である．特に，\\(\\sigma(\\mathcal{A})\\) 上へのただ一つの \\(\\sigma\\)-加法的な延長を持ち，引き続き Riesz 正則である．"
  },
  {
    "objectID": "posts/2024/FunctionalAnalysis/RadonMeasures.html#footnotes",
    "href": "posts/2024/FunctionalAnalysis/RadonMeasures.html#footnotes",
    "title": "測度の正則性 | Regularities of Measures on Topological Spaces",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Bogachev, 2007, pp. 68–69) 定義7.1.1, 7,1,4 と (Dudley, 2002, p. 224) に倣った．↩︎\n有界でない一般の符号付き測度に関しては，任意の \\(X\\) のコンパクト集合上で有限値であることを Borel 測度たる条件に加えることもある，例えば (Halmos, 1950, p. 223) 52節．↩︎\n(Dudley, 2002, p. 224) では単に regular と呼んでいるが，(Halmos, 1950, p. 224) に従って inner regular と呼ぶことにした．↩︎\n(Halmos, 1950, p. 224) ではこの条件を満たす \\(\\mu\\) を 正則 と呼んだ．↩︎\n(Dudley, 2002, p. 434) によると，最初の tight の定義は (Le Cam, 1957) による uniformly tight の定義であったようである．一点集合 \\(\\{P\\}\\) が一様に緊密であることと \\(P\\) が緊密であることとは同値になる．↩︎\n(Halmos, 1950, p. 224) に倣った．↩︎\n(Halmos, 1950, p. 228) 定理X.52.F, G．↩︎\n(Dunford and Schwartz, 1958, p. 138) 定理III.5.13, 定理III.5.14↩︎"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html",
    "href": "posts/2024/Process/PureJump.html",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#sec-pure-jump",
    "href": "posts/2024/Process/PureJump.html#sec-pure-jump",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "1 純粋跳躍過程",
    "text": "1 純粋跳躍過程\n\n1.1 導入\n\n\n\n\n\n\n次の生成作用素はどのような Markov 過程に対応するか？\n\n\n\n\\(E\\) を距離空間，\\(\\mu:E\\times\\mathcal{B}(E)\\to[0,1]\\) を確率核，\\(\\lambda\\in\\mathcal{L}_b(E)_+\\) を有界可測関数とする． \\[\nAf(x):=\\lambda(x)\\int_E\\biggr(f(y)-f(x)\\biggl)\\mu(x,dy),\\quad f\\in\\mathcal{L}_b(E),\n\\tag{1}\\] は有界作用素 \\(A\\in B(\\mathcal{L}_b(E))\\) を定め，一様連続半群 \\[\n\\{T_t:=e^{tA}\\}_{t\\in\\mathbb{R}_+}\\subset B(\\mathcal{L}_b(E))\n\\] を生成する．1 これに対応する Markov 過程はどのようなものだろうか？\n\n\n\n\n\n\n\n\n端的な回答\n\n\n\n\n\n第 1.4 節をご覧ください．\n\n\n\n初期分布を \\(\\nu\\in\\mathcal{P}(E)\\) とする．\n\n\n1.2 構成１\n累積するジャンプを足し合わせた値として，畳み込み半群 \\(\\{\\mu^{\\otimes k}\\}_{k\\in\\mathbb{N}}\\) に対応する初期分布 \\(\\nu\\) の Markov 連鎖 \\(\\{Y_k\\}_{k=0}^\\infty\\) を用意する．\nこれと独立な指数確率変数列 \\(\\Delta_k\\overset{\\text{iid}}{\\sim}\\operatorname{Exp}(1)\\) を用いて， \\[\nX_t:=\\begin{cases}\nY_0&0\\le t&lt;\\frac{\\Delta_0}{\\lambda(Y_0)}\\\\\nY_k&\\sum_{j=0}^{k-1}\\frac{\\Delta_j}{\\lambda(Y_j)}\\le t&lt;\\sum_{j=0}^{k}\\frac{\\Delta_j}{\\lambda(Y_j)}\n\\end{cases}\n\\] と構成した過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\) が，\\(\\{e^{tA}\\}\\) に対応する Markov 過程になる．2\nなお，\\(\\lambda(x)=0\\) の場合は，ジャンプは起きないもの \\(\\frac{\\Delta}{\\lambda(x)}=\\infty\\) と解する．この場合は零過程である．一般に関数 \\(\\lambda\\in\\mathcal{L}_b(E)\\) は位置 \\(x\\in E\\) からのジャンプの起こりやすさを表していると思える．\n\n\n1.3 構成２\n\\(\\lambda=0\\) の場合は零過程であるから， \\[\n\\lambda:=\\sup_{x\\in E}\\lambda(x)&gt;0\n\\] とし，新たな確率核 \\(\\mu':E\\to E\\)を \\[\n\\mu'(x,\\Gamma):=\\left(1-\\frac{\\lambda(x)}{\\lambda}\\right)\\delta_x(\\Gamma)+\\frac{\\lambda(x)}{\\lambda}\\mu(x,\\Gamma)\n\\] と定めると，生成作用素 \\(A\\) は \\[\nAf(x)=\\lambda\\int_E\\biggr(f(y)-f(x)\\biggl)\\mu'(x,dy)\n\\] とも表せる．\nこのとき，畳み込み半群 \\(\\{\\mu'^{\\otimes k}\\}_{k\\in\\mathbb{N}}\\) に対応する初期分布 \\(\\nu\\) の Markov 連鎖 \\(\\{Y_k'\\}_{k=0}^\\infty\\) は \\(\\{Y_k\\}\\) とは分布同等でない．\nだが，この \\(\\{Y_k'\\}\\) に対しては，独立な Poisson 過程 \\(\\{V_t\\}_{t\\in\\mathbb{R}_+}\\) に対して \\[\nX'_t:=Y'_{V_t}\\quad t\\in\\mathbb{R}_+\n\\] と構成される過程 \\(\\{X_t'\\}_{t\\in\\mathbb{R}_+}\\) はやはり \\(\\{e^{tA}\\}\\) に対応する Markov 過程である．\n\n\n\n\n\n\n命題3\n\n\n\n\n\\(\\{X'_t\\}\\) は \\(\\{X_t\\}\\) に分布同等である．\n\\(\\{Y'_k\\}\\) は Markov 性 \\[\n\\operatorname{E}[f(Y'_{k+V_t})|\\mathcal{F}_t]=P^kf(X'_t)\n\\] を満たす．ただし，\\(P\\) は \\(\\mu'\\) が定める作用，\\(\\mathcal{F}_t:=\\mathcal{F}_t^V\\lor\\mathcal{F}^{X'}_t\\) とした．\n\\(X'\\) は \\(\\{T_t\\}\\) に対応する Markov 過程である： \\[\n\\operatorname{E}[f(X'_{t+s})|\\mathcal{F}_t]=T_sf(X'_t).\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n\n\n\n\n\n1.4 まとめ\n\nすなわち \\(X\\) は非一様な複合 Poisson 過程になる．\n各点 \\(x\\in E\\) に於て，レート \\(\\lambda(x)\\) で，分布 \\(\\mu(x,dy)\\) に従う点へとジャンプをする．4\n具体的には，次のように整理できる：\n強度測度 \\(\\lambda(x)\\mu(x,dy)dx\\) を持つ \\(E^2\\) 上の Poisson 点過程 \\(\\eta\\) に対して， \\[\n\\xi(A)=\\int_{A\\times E}y\\,\\eta(dxdy)\n\\] で定まる非一様な複合 Poisson 点過程 \\(\\xi\\) に対応する．\n例えば \\(E=\\mathbb{R}\\) のとき， \\[\nM_t:=\\xi([0,t])=\\int_0^t\\xi(ds)\n\\] が，所定の生成作用素 \\(\\{T_t\\}\\) を持つ加法過程になる．\n\\(\\lambda\\) を有界としているから，有界区間上でのジャンプ数は有限である．Gamma 過程 のように，\\(\\mathbb{R}\\) の稠密部分集合上でジャンプを繰り返す，というようなことは起こり得ない．\nPoisson 過程については次の稿も参照：\n\n   \n      \n         \n         \n            Poisson 過程を見てみよう\n            YUIMA パッケージを用いたシミュレーションを通じて"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#区分的確定的-markov-過程",
    "href": "posts/2024/Process/PureJump.html#区分的確定的-markov-過程",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "2 区分的確定的 Markov 過程",
    "text": "2 区分的確定的 Markov 過程\n\n2.1 導入\n前節で調べた純粋跳躍過程に，決定論的な動きを加えた Markov 過程のクラスを，(Davis, 1984) 以来 区分的確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) と呼ぶ．\nPDMP は拡散項を持たない Lévy 過程とも理解できる．5\n実際，(Davis, 1984) は拡散過程に相補的なクラスとして導入し，(Davis, 1993) は PDMP が待ち行列，ポートフォリオ最適化，確率的スケジューリング，標的追跡，保険数理，資源最適化など，広く確率的モデリングと最適化において重要な役割を演じることを見事に描き出した．6\n\nthe class of “piecewise-deterministic” Markov processes, newly introduced here, provides a general family of models covering virtually all non-diffusion applications. (Davis, 1984)\n\n実際，PDMP を用いた MCMC である Piecewise Deterministic Monte Carlo または連続時間 MCMC は，高次元データと大規模データセットに対する効率的なサンプリング法開発の鍵と目されている．\n次の記事も参照：\n\n   \n      \n         \n         \n            新時代の MCMC を迎えるために\n            連続時間アルゴリズムへの進化\n         \n      \n   \n\n\n\n2.2 設定\n\n2.2.1 空間\nPDMP は動きのモードが移り変わっても良い．これを表すパラメータ \\(v\\in K\\) を導入する．7\nこのパラメータ空間上に，PDMP が動く範囲の次元を表す関数 \\(d:K\\to\\mathbb{N}^+\\) を導入し，パラメータが \\(v\\in K\\) である際は，\\(M_v\\subset\\mathbb{R}^{d(v)}\\) として，積空間 \\(M_v\\times\\{v\\}\\) 上を運動するものとする．\n総合して，PDMP の状態空間を \\(M_v\\) の直和 \\[\nE:=\\bigcup_{v\\in K}M_v\\times\\{v\\}\n\\] と定める．8\n\n\n2.2.2 フロー\n\\(\\mathfrak{X}_v\\) を \\(M_v\\) 上のベクトル場とし，積分曲線 \\[\n\\frac{d \\phi_v(x,t)}{d t}=\\mathfrak{X}_v(\\phi_v(x,t)),\n\\] \\[\n\\phi_v(x,0)=x\n\\] が存在するとする．9\n\\(\\lambda:E\\to\\mathbb{R}_+\\) は \\(s\\mapsto\\lambda(\\phi_v(x,s),v)\\) が局所可積分であるような関数とし， \\[\n\\Lambda(t):=\\int^t_0\\lambda(\\phi(s,x))ds\n\\] で表す．\n\n\n2.2.3 境界\n\\(M_v\\subset\\mathbb{R}^{d(v)}\\) の境界のうち，積分曲線が到達し得る部分を \\[\n\\partial^*M_v:=\\left\\{y\\in\\partial M_v\\,\\middle|\\,\\exists_{t&gt;0}\\;\\exists_{x\\in M_v}\\;\\phi_v(x,t)=y\\right\\}\n\\] で表し，その直和を \\(\\Gamma^*:=\\bigcup_{v\\in K}\\partial^*M_v\\times\\{v\\}\\) とする．\nさらにそのうち，正な確率を持って到達可能な部分を \\[\n\\Gamma:=\\left\\{(y,v)\\in\\Gamma^*\\mid\\lim_{x\\to y}\\operatorname{P}_{(x,v)}[T_1=t^*(x,v)]=1\\right\\}\n\\]\n点 \\((x,v)\\in E\\) からフローに従って運動した場合の境界 \\(\\Gamma^*\\) への到達時刻を \\[\nt^*(x,v):=\\inf\\left\\{t\\ge 0\\mid\\phi_v(x,t)\\in\\partial^*M_v\\right\\}\n\\] とする．\n\n\n\n2.3 定義\n\n\n\n\n\n\n定義 (PDMP: Piecewise Deterministic Markov Process)10\n\n\n\n\\((\\phi(t,-))\\) をフロー，\\(\\lambda:E\\to\\mathbb{R}_+\\) をレート関数，\\(Q:(E\\cup\\Gamma^*)\\times\\mathcal{E}\\to[0,1]\\) を確率核とする．\n３組 \\((\\phi,\\lambda,Q)\\) が定める PDMP \\(Z\\) とは，次のように帰納的に定まる確率過程をいう：\n\nあるスタート地点 \\(z\\in E\\) から，一様乱数 \\(U_1\\sim\\mathrm{U}([0,1])\\) とフロー \\(\\phi\\) が定めるジャンプ時刻 \\[\nT_1:=\\inf\\left\\{t\\ge 0\\,\\middle|\\,e^{-\\Lambda(t)}\\le U_1\\right\\}\\lor t^*(x,v)\n\\] まで，フローの通りに移動するとする： \\[\nZ_t=\\phi(t,x),\\quad t&lt; T_1,\n\\]\n時刻 \\(T_1\\) での位置は確率核 \\(Q\\) に従って決まるとする： \\[\nZ_{T_1}\\sim Q\\biggr((\\phi(T_1,x),v),-\\biggl).\n\\]\nこの手続きを，\\(Z_{T_1}\\) を次のスタート地点として繰り返すのが特性量 \\((\\phi,\\lambda,Q)\\) が定める PDMP \\(Z\\) である．\n\n\n\n\\(\\lambda\\) は局所有限であるから，有限時区間上でのジャンプ数は有限になる．境界への到達によるジャンプ数も同様の性質を満たすと仮定する．11\nこうして得た PDMP は，Feller-Dynkin 過程であるとは限らないにも拘らず，時間的に一様な強 Markov 過程である．12\n\n\n2.4 PDMP の拡張生成作用素\nPDMP の拡張生成作用素は，ちょうど（有限な Lévy 測度を持つ）純粋跳躍過程の生成作用素 1 にドリフト項を加えたものになる．\n\n\n\n\n\n\n命題（PDMP の拡張生成作用素の定義域の特徴付け）13\n\n\n\n\n\n\\(X\\) を PDMP とする．このとき，\\(X\\) の拡張生成作用素 \\(\\widehat{A}\\) について，可測関数 \\(f\\in\\mathcal{L}(E\\cup\\Gamma)\\) が \\(f\\in\\mathcal{D}(\\widehat{A})\\) であるとは，次と同値：\n\n絶対連続性：任意の \\(x\\in E\\) について， \\[\nt\\mapsto f(\\phi(t,x))\n\\] は \\[\ns_*(x):=\\inf\\left\\{t\\ge 0\\mid\\operatorname{P}_x[T_1&gt;t]=0\\right\\}\n\\] に関して，\\(t\\in[0,s_*(x))\\) 上で絶対連続である．\n境界条件： \\[\n\\Gamma:=\\left\\{x\\in E\\mid\\exists_{t&gt;0}\\;\\exists_{z\\in E}\\;x=\\phi(t,z)\\right\\}\n\\] 上では \\[\nf(x)=\\int_Ef(y)Q(x,dy),\\quad x\\in\\Gamma\n\\] を満たす．\n跳躍の局所可積分性：\\(E\\times\\mathbb{R}_+\\) 上のランダム関数 \\[\n(x,s)\\mapsto Bf(x,s):=f(x_s)-f(X_{s-})\n\\] は，\\(X\\) の跳躍が定める \\(E\\times\\mathbb{R}_+\\) 上の Poisson 点過程 \\(\\eta\\) について，ある発散する停止時の増加列 \\(\\tau_n\\) が存在して， \\[\n\\operatorname{E}\\left[\\int_0^{\\tau_n}\\int_E\\lvert Bf(x,s)\\rvert\\eta(dxds)\\right]&lt;\\infty\n\\] \\[\nn=1,2,\\cdots\n\\] が成り立つ．\n\n\n\n\n\n\n\n\n\n\n命題（PDMP の拡張生成作用素の特徴付け）14\n\n\n\n\\(X\\) を PDMP とする．このとき，\\(X\\) の拡張生成作用素 \\(\\widehat{A}\\) について，\\(\\mathcal{D}(\\widehat{A})\\) の全域で次が成り立つ： \\[\n\\widehat{A}f(x)=\\mathfrak{X}f(x)+\\lambda(x)\\int_E\\biggr(f(y)-f(x)\\biggl)Q(x,dy).\n\\]\n\n\n\n\n\n\n\n\n注（第一項の well-definedness）15\n\n\n\n\n\n\\(\\mathfrak{X}\\) の標準座標 \\(\\frac{\\partial }{\\partial x_1},\\cdots,\\frac{\\partial }{\\partial x_d}\\) に関する成分表示を \\(g^i\\) とすると，\\(\\mathfrak{X}f(x)\\) は \\((g(x)|\\nabla f(x))\\) とも表せる．\nこの \\(\\nabla f\\) は，\\(f\\in C^1(M_v)\\) である場合は問題がないが，一般に単に絶対連続である場合は問題になる可能性がある．\nしかしそれでも，次を満たす関数 \\(\\nabla f\\) として用意することができる： \\[\nf(\\phi_v(x,t),v)-f(x,v)=\\int^t_0\\nabla f(\\phi_v(x,s),v)ds.\n\\]\n\n\n\n\n\n2.5 PDMP が Feller-Dynkin 過程であるとき\n\n\n\n\n\n\n命題（PDMP の Feller 性の特徴付け）16\n\n\n\n\\(X\\) を PDMP とする．次の全てが成り立つならば，\\(X\\) は Feller-Dynkin 過程である：\n\n任意の \\(x\\in E\\) について， \\[\nt_*(x):=\\inf\\left\\{t&gt;0\\mid\\phi(t,x)\\in\\partial E\\right\\}=\\infty.\n\\]\n\\(\\lambda\\in C_b(E)\\)．\n\\(Q\\) も Feller 性を持つ：\\(Q(C_b(E))\\subset C(E)\\)．\n\n\n\n\n\n\n\n\n\n注（right process としての PDMP）\n\n\n\n\n\nしかし，PDMP は常に (Meyer, 1966) にいう right process ではある．すなわち，次の４条件を満たす：\n\n\\(E\\) は Lusin 空間である．\n\\(P_t(\\mathcal{L}_b(E))\\subset\\mathcal{L}_b(E)\\)．\n\\(X\\) は殆ど確実に右連続な見本道を持つ．\n\\(f\\) が \\(\\alpha\\)-excessive function ならば，\\(t\\mapsto f\\circ X_t\\) も殆ど確実に右連続な見本道を持つ．\n\n\n\n\n\n\n2.6 PDMP のシミュレーション\n\n2.6.1 到着時刻のシミュレーション\nレート関数が \\(\\lambda&gt;0\\) であるとき，Poisson イベントのシミュレーションは次のように，指数分布に従う確率変数のシミュレーションによって行える．\n累積関数を \\[\n\\Lambda(t):=\\int^t_0\\lambda(s)ds\n\\] とすると，\\(N_t\\sim\\mathrm{Pois}(\\Lambda(t))\\) であるから，最初のイベントの到着時刻 \\(T_1\\) は，次の生存関数によって特徴付けられる： \\[\n\\operatorname{P}[T_1\\ge t]=\\operatorname{P}[N_t=0]=\\exp\\left(-\\Lambda(t)\\right).\n\\]\nここで，\\(E\\sim\\operatorname{Exp}(1)\\) とすると， \\[\n\\operatorname{P}[\\Lambda^{-1}(E)\\ge t]=\\operatorname{P}\\left[E\\ge\\Lambda(t)\\right]=e^{-\\Lambda(t)}.\n\\] 従って，\\(T_1\\overset{\\text{d}}{=}f^{-1}(E)\\) である．\nすなわち，\\(E\\) を通じて \\[\nT_1':=\\inf\\left\\{t\\ge 0\\,\\middle|\\,\\int^t_0\\lambda(s)ds=E\\right\\}\n\\] を計算すれば，最初の到着時刻が計算できる．17\n\n\n2.6.2 PDMP のためのパッケージ\nJoris Bierkens ら開発の R パッケージ RZigZag (GitHub / CRAN) を通じて実行してみる．\ninstall.packages(\"Rcpp\")\ninstall.packages(\"RcppEigen\")\ninstall.packages(\"RZigZag\")"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#zig-zag-sampler-bierkens2019",
    "href": "posts/2024/Process/PureJump.html#zig-zag-sampler-bierkens2019",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "3 Zig-Zag Sampler (Bierkens et al., 2019)",
    "text": "3 Zig-Zag Sampler (Bierkens et al., 2019)\n\n3.1 導入\n１次元の Zig-Zag 過程は元々，Curie-Weiss 模型 における Glauber 動力学を lifting により非可逆化して得る Markov 連鎖の，スケーリング極限として特定された Feller-Dynkin 過程である (Bierkens and Roberts, 2017)．\nただし，(Goldstein, 1951) で電信方程式と関連して，同様の過程が扱われた歴史もある．\n\n\n3.2 設定\nZig-Zag 過程 \\(Z=(X,\\Theta)\\) の状態空間は \\(E=\\mathbb{R}^d\\times\\{\\pm1\\}^d\\) と見ることが多い．\n\\(\\theta\\in\\{\\pm1\\}^d\\) は速度を表す．すなわち，全座標系と \\(45\\) 度をなす方向に，常に一定の単位速度で \\(\\mathbb{R}^d\\) 上を運動するとする．\n換言すれば，決定論的なフローは次のように定める： \\[\n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\n\\] \\[\n\\frac{d \\Theta}{d t}=0,\\qquad\\Theta_0=0.\n\\]\n\n\n3.3 アルゴリズム\n後述の関数 \\(\\lambda\\) に対して，各座標 \\(i\\in[d]\\) におけるレートを \\[\nm_i(t):=\\lambda(x+\\theta t,\\theta)\n\\] で定めた \\(\\mathbb{R}^d\\) 上の Poisson 過程を考える．\n多次元の Poisson 過程の各成分の跳躍は独立だから，18 それぞれの成分ごとに Poisson 到着時刻 \\(T_i\\;(i\\in[d])\\) をシミュレーションし，最初に到着したものを \\(T_j\\) とすると，関数 \\[\nF_j(\\theta)_i=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n\\] に従ってジャンプすると考えて良い．\n\n\n\n\n\n\n注（第 2.2 節の設定との対応）\n\n\n\n\n\n状態空間は \\[\nE=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\}\n\\] とみなすところから始める必要がある．\n加えて，レート関数は \\[\n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n\\] であり，跳躍測度は点過程 \\[\nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\\] とみなした場合に当たる．\n\n\n\n\n\n3.4 レート関数\nPDMP の妙は全てレート関数に宿っている．\nレート \\(\\lambda:E\\to\\mathbb{R}_+\\) は，負の対数密度 \\(U\\in C^1(\\mathbb{R}^d)\\) が定める目標分布 \\(\\pi(dx)\\,\\propto\\,e^{-U(x)}dx\\) に対して， \\[\n\\lambda_i(x,\\theta):=(\\theta_i\\partial_iU(x))_++\\gamma_i(x,\\theta_{-i})\\quad(i\\in[d])\n\\] と定める．ただし，\\(\\gamma_i\\) は，\\(\\theta_i\\) のみには依らない任意の連続関数 \\(\\gamma_i:E\\to\\mathbb{R}_+\\) とした．19\n\n\n\n\n\n\n(Bierkens et al., 2019, pp. 1294 定理2.2)\n\n\n\n\\(U\\in C^1(\\mathbb{R}^d)\\) は \\(e^{-U}\\in\\mathcal{L}^1(\\mathbb{R}^d)\\) を満たすとする．このとき，\\(Z\\) は次の分布 \\(\\mu=\\pi\\otimes\\mathrm{U}(\\{\\pm1\\}^{d})\\in\\mathcal{P}(E)\\) を不変にする： \\[\n\\mu(dxd\\theta)=\\frac{1}{2^d\\mathcal{Z}}e^{-U(x)}\\,dxd\\theta\n\\]\n\n\n\n\n3.5 部分サンプリング\nMCMC の計算複雑性のボトルネックは，尤度の評価にある．各ステップで全てのデータを用いて尤度を計算する必要がある点が，MCMC を深層学習などの大規模データの設定への応用を難しくしている (Murphy, 2023, p. 647)．\n\\(p(x)\\) を事前分布，\\(p(y|x)\\) を観測のモデル（または尤度）とし，データ \\(y_1,\\cdots,y_n\\) は互いに独立であるとする．このとき，事後分布 \\(\\pi(x):=p(x|y)\\) は \\[\n\\pi(x)\\,\\propto\\,\\left(\\prod_{k=1}^n p(y_k|x)\\right)p(x)\n\\] より，Hamiltonian \\(U\\) は \\[\\begin{align*}\n   U(x)&=-\\sum_{k=1}^n\\log p(y_k|x)-\\log p(x)\\\\\n   &=\\frac{1}{n}\\sum_{k=1}^n\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl).\n\\end{align*}\\] と表せる．\nすなわち，各微分係数 \\(\\partial_i U(x)\\) は，独立な観測 \\(y_1,\\cdots,y_n\\) が定める統計量 \\[\nE^i_k(x):=\\frac{\\partial }{\\partial x_i}\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl)\n\\] の平均により推定される値となっている．\nよって，精度は劣るかもしれないが，一様に選んだ \\(K\\sim\\mathrm{U}([n])\\) から定まる \\(E^i_K\\) の値は \\(\\partial_i U(x)\\) の不偏推定量となっている．20\n\n\n\n\n\n\n部分サンプリングにより不変分布が変わらないことの証明\n\n\n\n\n\nこれは，元々のレート関数 \\[\n\\lambda_i(x,\\theta)=\\frac{1}{n}\\sum_{k=1}^n(\\theta E^i_k(x))_+\n\\] に対して， \\[\n\\gamma_i(x,\\theta):=\\frac{1}{n}\\sum_{k=1}^n(\\theta_iE^i_k(x))_+-\\left(\\theta_i\\frac{1}{n}\\sum_{k=1}^nE^i_k(x)\\right)_+\n\\] という項を加えて得る Zig-Zag サンプラーとみなせるためである．\nこうして定義された \\(\\gamma_i\\) が非負関数である限り，平衡分布は \\(\\mu\\) のままであるが，これは関数 \\((-)_+\\) の凸性から従う．\nこうして，サブサンプリングの実行による精度の劣化が，(Andrieu and Livingstone, 2021) の枠組みで捉えられる，ということでもある．\n加えて，レート関数が大きくなっており，したがってそもそも尤度関数の評価の回数が増えていることにも注意．\n\n\n\n\n\n3.6 制御変数による分散低減\n上述の \\(\\partial_iU(x)\\) の不偏推定量の分散は，制御変数の方法を用いて低減できる．これにより，事前処理の部分を除けば，データのサイズに依存しない計算複雑性で事後分布からの正確なサンプリングが可能になる．\n\n\n3.7 シミュレーション\n\n\nCode\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#78C2AD\"), plot.title=element_text(color=\"#78C2AD\"))\n\n\n\n\n\n\\(\\mathrm{N}_2\\left(\\begin{pmatrix}2\\\\2\\end{pmatrix},\\begin{pmatrix}3&1\\\\1&3\\end{pmatrix}\\right)\\) に対する Zig-Zag 過程\n\n\n\n\n\n\nCode\nset.seed(123)\ndim &lt;- 2\ndof &lt;- 1\nresult &lt;- ZigZagStudentT(dof, dim, n_iter=1000, sphericallySymmetric = TRUE)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#78C2AD\"), plot.title=element_text(color=\"#78C2AD\"))\n\n\n\n\n\nCauchy 分布 \\(\\mathrm{C}(0,1)=\\mathrm{t}(1)\\) に対する Zig-Zag 過程\n\n\n\n\n続きは次の稿も参照：\n\n   \n      \n         \n         \n            Zig-Zag サンプラー\n            ジャンプと確定的な動きによる新たな MCMC 手法"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#bouncy-particle-sampler-bouchard-cote2018-bps",
    "href": "posts/2024/Process/PureJump.html#bouncy-particle-sampler-bouchard-cote2018-bps",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "4 Bouncy Particle Sampler (Bouchard-Côté et al., 2018)",
    "text": "4 Bouncy Particle Sampler (Bouchard-Côté et al., 2018)\n\n4.0.1 部分サンプリング\nZig-Zag サンプラーに対応するサブサンプリングの技術を (Pakman et al., 2017) が提案している．\n\n\n4.0.2 シミュレーション\n\n\nCode\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#78C2AD\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#78C2AD\"), plot.title=element_text(color=\"#78C2AD\"))\n\n\n\n\n\n\\(\\mathrm{N}_2\\left(\\begin{pmatrix}2\\\\2\\end{pmatrix},\\begin{pmatrix}3&1\\\\1&3\\end{pmatrix}\\right)\\) に対する Zig-Zag 過程"
  },
  {
    "objectID": "posts/2024/Process/PureJump.html#footnotes",
    "href": "posts/2024/Process/PureJump.html#footnotes",
    "title": "純粋跳躍過程の生成作用素と区分的確定的 Markov 過程",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Pazy, 1983, p. 2) 参照．↩︎\nこの事実は第 1.3 節で２つ目の構成と同時に証明される．↩︎\n(Ethier and Kurtz, 1986, pp. 163–164) 参照．↩︎\n\\(f(y+x)-f(x)\\) ではなくて \\(f(y)-f(x)\\) であるので，\\(\\mu(x,dy)\\) は必ずしも現在地点 \\(x\\) からみた変位の分布ではないことに注意．↩︎\nただし，ジャンプが頻繁すぎないことも必要である．(Davis, 1993, p. 60) 仮定24.4 では，有界区間上のジャンプは有限回であると過程している：\\(\\operatorname{E}[N_t]&lt;\\infty\\;(t\\in\\mathbb{R}_+)\\)．すなわち，A 型までの加法過程を PDMP というのであって，B 型と C 型は，シミュレーションが容易な連続時間過程であるという美点を逃してしまう．加法過程の分類については，この 稿 も参照．↩︎\n当時は連続時間確率過程といえば拡散過程であり，SDE によるモデリングが興隆した時代であった．(Davis, 1993) では，必ずしも SDE を使うことが自然なモデリング方法でないにも拘らず，無理やり SDE の枠組みに落とし込もうとする当時の慣行を批判し，PDMP はこのギャップを埋めるために開発した，としている．なお，(Davis, 1993) では PDMP ではなく PDP と呼んでいる．↩︎\n(Davis, 1993, p. 57) では \\(K\\) は可算という仮定をおいている．↩︎\n積空間としての \\(\\sigma\\)-代数を導入する．↩︎\n(Davis, 1993) では，\\(\\mathfrak{X}_v\\) は局所 Lipcthiz 連続である上に，（暗黙のうちに）完備で，\\(\\phi_v\\) は任意の \\(t\\in\\mathbb{R}\\) について定義されるとしていることに注意．(G. Vasdekis and Roberts, 2023) では違う．↩︎\n(Davis, 1993, p. 58) と (Georgios Vasdekis, 2021, pp. 15–16) に倣った．↩︎\nこのことを (Davis, 1993, p. 60) 仮定24.4としている．↩︎\n(Davis, 1984)，(Davis, 1993, p. 64) 定理25.5も参照．↩︎\n(Davis, 1993, p. 69) 定理26.14．↩︎\n(Davis, 1993, p. 69) 定理26.14 の後半．↩︎\n(Georgios Vasdekis, 2021, p. 18) 系2.3.4 も参照．↩︎\n(Davis, 1993, p. 77) 定理27.6．↩︎\n簡単な対象分布では，\\(\\Lambda(t)=E\\) の解が解析的に求まる事が多い．これが intractable である場合は，剪定 を用い，\\(\\lambda^*\\) としては affine 関数を用いる事が多いが，一般に３次の多項式までならば解の公式があるために，\\(\\Lambda(t)=E\\) を解析的に解く方法から効率的にシミュレーションできる．どれくらい \\(\\lambda^*\\) として \\(\\lambda\\) に近いものを選べば良いかは不明．(Georgios Vasdekis, 2021, p. 13) 命題2.2.2も参照．↩︎\n(Revuz and Yor, 1999, p. 473) 命題XII.1.7．↩︎\n従って，レート関数 \\(\\lambda\\) は連続とする．この関数 \\(\\gamma_i\\) は，\\(U\\) の情報には依らない追加のリフレッシュ動作を仮定に加える．実際，\\(\\lambda_i(x,\\theta)-\\lambda_i(x,F_i(\\theta))=\\theta_i\\partial_iU(x)\\) である限り，\\(\\theta\\) と \\(F_i(\\theta)\\) の往来には影響を与えず釣り合っているため，どのような \\(\\gamma_i\\) をとっても，平衡分布には影響を与えない．しかし，高くするごとにアルゴリズムの対称性が上がるため，\\(\\gamma\\equiv0\\) とすることが Monte Carlo 推定量の漸近分散を最小にするという (Andrieu and Livingstone, 2021)．↩︎\nこのとき，必ずしも \\(K\\sim\\mathrm{U}([n])\\) とする必要はなく，特定の観測に重みをおいても良い (Sen et al., 2020)．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Langevin.html",
    "href": "posts/2024/Process/Langevin.html",
    "title": "Langevin Dynamics の多項式エルゴード性",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\\(\\mathbb{R}^n\\) 上の Langevin 拡散を考える： \\[\ndX_t=-\\nabla V(X_t)\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t,\\qquad X_0=x.\n\\tag{1}\\] ただし， \\[\nV(x)=O(\\lvert x\\rvert^{2k})\\qquad(\\lvert x\\rvert\\to\\infty)\n\\] の仮定をおく．\\(k\\ge1/2\\) の場合，指数エルゴード的であるが，\\(k&lt;1/2\\) の場合はそうではない．\n\\(k\\in(0,1/2)\\) の設定で，次の ergodic lower bound を示したい： \\[\nC_1\\exp\\left(c_1V(x)-c_2t^{\\frac{k}{1-k}}\\right)\\le\\|P_t(x,-)-\\mu_*\\|_\\mathrm{TV}\n\\tag{2}\\] \\[\n\\mu_*(dx)\\,\\propto\\,e^{-\\beta V(x)}dx\n\\] この lower bound から，\\(k\\in(0,1/2)\\) の場合，Langevin 過程 \\(X\\) が指数エルゴード的たり得ないことが従う．\n式 (2) を示すためには，\\(G(x):=e^{\\kappa V(x)}\\;(\\kappa\\in\\mathbb{R})\\) に対して， \\[\n\\operatorname{E}_x[G(X_t)]\\le g(x,t)\n\\] を満たす関数 \\(g\\) を見つける必要がある (Hairer, 2021, pp. 34–35)．\nこれは次の３ステップを辿る"
  },
  {
    "objectID": "posts/2024/Process/Langevin.html#sec-integrability",
    "href": "posts/2024/Process/Langevin.html#sec-integrability",
    "title": "Langevin Dynamics の多項式エルゴード性",
    "section": "1 \\(G=e^{\\kappa V}\\) の可積分性について",
    "text": "1 \\(G=e^{\\kappa V}\\) の可積分性について\n\n\n\n\n\n\n次元 \\(n=1\\) で考えてみる．\n\\(V(x)=\\frac{x^2}{2}\\) とした場合，\\(X\\) は OU 過程になり，\n\\[\n\\operatorname{E}_x[G(X_t)]&lt;\\infty\\quad\\Leftrightarrow\\quad t&lt;-\\frac{1}{2}\\log\\left(1-\\frac{\\beta}{\\kappa}\\right).\n\\]\n\\(V(x)=\\log x\\) とした場合，\\(X\\) は Bessel 過程になり， \\[\n\\operatorname{E}_x[G(X_t)]&lt;\\infty\\qquad(\\forall_{t&gt;0}).\n\\]\n\\(k\\in(0,1/2)\\) の場合，\\(\\nabla V\\) が有界であることに注目すれば，Bessel 過程の場合と同様に \\[\n\\operatorname{E}_x[G(X_t)]&lt;\\infty\\qquad(\\forall_{t&gt;0}).\n\\]\n\n\n\n\n1.1 はじめに\nMarkov 過程 \\(X\\) に関するドリフト条件 \\[\n\\widehat{L}V\\le-C\\varphi\\circ V\\qquad\\mathrm{on}\\;\\mathbb{R}^n\\setminus K\n\\] からは \\(V:E\\to\\mathbb{R}_+\\) の可積分性が出る： \\[\n\\operatorname{E}_x[V(X_t)]&lt;\\infty\\qquad t\\ge0.\n\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n上のドリフト条件を，(Hairer, 2021) の最も弱い意味で解釈すると \\[\nM_t:=V(X_t)+C\\int^t_0\\varphi\\circ V(X_s)\\,ds\n\\] が任意の \\(x\\in E\\) に関して \\(\\operatorname{P}_x\\)-局所優マルチンゲールである，ということになる．\nこれだけの仮定でも，\\(V\\) が下に有界であるために \\(M_t\\) も下に有界であり，下に有界な局所優マルチンゲールは（真の）優マルチンゲールであることから， \\[\n\\operatorname{E}_x\\left[V(X_t)+C\\int^t_0\\varphi\\circ V(X_s)\\,ds\\right]\\le V(x).\n\\]\n加えて左辺が下に有界であることから，\\(\\operatorname{E}_x[V(X_t)]&lt;\\infty\\) でないと矛盾が起こる．\n\n\n\nしかし，lower bound を得たい場合， \\[\n\\widehat{L}V\\le C\\varphi\\circ G\\qquad\\mathrm{on}\\;\\mathbb{R}^n\n\\tag{3}\\] という情報のみから， \\[\n\\operatorname{E}_x[G(X_t)]\\le g(x,t)\\;(&lt;\\infty)\n\\] という評価を得る必要が出てくる．この場合，\\(\\operatorname{E}_x[G(X_t)]&lt;\\infty\\) は非自明で，ケースバイケースの議論がである．\n\n\n1.2 OU 過程の場合\nAn overdamped Langevin dynamics on \\(\\mathbb{R}\\) is defined as the solution to the following SDE: \\[\ndX_t=-\\nabla V(X_t)\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t,\\qquad X_0=x_0.\n\\]\nIf \\(V(x)=\\frac{x^2}{2}\\), \\(X\\) becomes an Ornstein-Uhlenbeck process. Transforming via \\(f(t,x)=xe^t\\) and using Itô’s formula, we get \\[\nX_t=x_0e^{-t}+\\sqrt{2\\beta^{-1}}\\int^t_0e^{-(t-s)}\\,dB_s.\n\\] Hence, \\(X\\) is a Gaussian process with \\(X_t\\sim\\operatorname{N}\\left(x_0e^{-t},\\beta^{-1}(1-e^{-2t})\\right)\\).\nIn this case, expectation with respect to \\(G(y)=e^{\\kappa V(y)}=e^{\\frac{\\kappa y^2}{2}}\\;(\\kappa\\in\\mathbb{R})\\) is given by\n\\[\\begin{align*}\n    \\operatorname{E}_x[G(X_t)]&=\\int_{\\mathbb{R}} G(y)\\frac{1}{\\sqrt{2\\pi\\beta^{-1}(1-e^{-2t})}}\\exp\\left(-\\frac{(y-xe^{-t})^2}{2\\beta^{-1}(1-e^{-2t})}\\right)\\,dy\\\\\n    &=\\frac{1}{\\sqrt{2\\pi\\beta^{-1}(1-e^{-2t})}}\\int_{\\mathbb{R}}\\exp\\left(\\frac{\\kappa\\beta^{-1}(1-e^{-2t})y^2-(y-xe^{-t})^2}{2\\beta^{-1}(1-e^{-2t})}\\right)\\,dy.\n\\end{align*}\\]\nTaking a closer look at the numerator inside \\(\\exp\\),\n\\[\\begin{align*}\n    &\\qquad\\kappa\\beta^{-1}(1-e^{-2t})y^2-(y-xe^{-t})^2\\\\\n    &=y^2\\biggr(\\kappa\\beta^{-1}(1-e^{-2t})-1\\biggl)-2xe^{-t}y+x^2e^{-2t}.\n\\end{align*}\\]\nTherefore, we conclude \\[\n\\operatorname{E}_x[G(X_t)]&lt;\\infty\\quad\\Leftrightarrow\\quad\\kappa\\beta^{-1}(1-e^{-2t})&lt;1.\n\\] In other words, \\(P_tG(x)\\) is finite as long as \\[\nt&lt;-\\frac{1}{2}\\log\\left(1-\\frac{\\beta}{\\kappa}\\right).\n\\]\n\n\n1.3 Bessel 過程の場合\n\\(V=a\\log x\\) ととると，\\(V'(x)=\\frac{a}{x}\\) であるから，これに関する Langevin 動力学は，\\(\\beta=1\\) のとき， \\[\ndX_t=-\\frac{a}{X_t}\\,dt+dB_t\n\\] と，母数 \\(a\\) を持つ Bessel 過程になる．ただし，\\(0\\) への到着時刻 \\(T_0\\) で止めたもの \\(X^{T_0}\\) を考える．\n\n\n\n\n\n\n(Lawler, 2019, pp. 10 命題2.5)\n\n\n\n母数 \\(a\\) を持つ Bessel 過程 \\(X^{T_0}\\) の密度を \\(q_t(x,y;a)\\) で表す．このとき， \\[\nq_t(x,y;1-a)=\\left(\\frac{y}{x}\\right)^{1-2a}q_t(x,y;a)\n\\] \\[\nq_t(x,y;a)=q_t(y,x;a)\\left(\\frac{y}{x}\\right)^{2a}\n\\] \\[\nq_{r^2t}(rx,ry;a)=\\frac{1}{r}q_t(x,y;a)\n\\]\n加えて \\(a\\ge\\frac{1}{2}\\) でもあるとき， \\[\nq_1(x,y;a)=y^{2a}\\exp\\left(-\\frac{x^2+y^2}{2}\\right)h_a(xy),\n\\] \\[\nh_a(x)\\sim\\frac{1}{\\sqrt{2\\pi}}x^{-a}e^x\\qquad(\\lvert x\\rvert\\to\\infty)\n\\]\n\n\nこの結果は (Lawler, 2019, p. 59) をみる限り，修正 Bessel 関数と，Bessel 過程の Fokker-Planck 方程式との考察によって証明されている．\n\\[\nG(y)=e^{\\kappa V(y)}=e^{a\\kappa\\log(y)}=y^{a\\kappa}\n\\] であるから，密度 \\(q_t(x,y;a)\\) に対してはどうやっても可積分である．\n\n\n1.4 \\(k&lt;1/2\\) の場合の尾部確率\n\\(k&lt;1/2\\) で最も大きく変わる点は， \\[\n\\nabla V(x)=O(\\lvert x\\rvert^{2k-1})\\qquad(\\lvert x\\rvert\\to\\infty)\n\\] であるために，\\(\\nabla V\\) が \\(\\mathbb{R}^n\\) 上で有界になることである．\nこのため，一般に SDE \\[\ndZ_t=b(Z_t)\\,dt+\\sigma(X_t)\\,dB_t\n\\] の密度が，任意の \\(T&gt;0\\) に対して，ある \\(A_T,a_T&gt;a\\) と \\(y\\in\\mathbb{R}\\) が存在して \\[\n\\frac{1}{A_T\\sqrt{2\\pi t}}e^{-\\frac{a_T\\lvert y-x\\rvert^2}{2t}}\\le p_t(x,y)\\le\\frac{A_T}{\\sqrt{2\\pi t}}e^{-\\frac{\\lvert y-x\\rvert^2}{2a_Tt}}\n\\] \\[\nt\\in(0,T]\n\\] が成り立つことが使える．1\nこれによれば， \\[\nG(x)=e^{\\kappa V(x)}=O(e^{\\kappa\\lvert x\\rvert^{2k}})\\quad(\\lvert x\\rvert\\to\\infty)\n\\] に対して \\(p_t\\) の尾部が勝つため，\\(P_tG(x)&lt;\\infty\\) である．\n\n\n1.5 \\(k&lt;1/2\\) の場合の \\(G\\) の可積分性\n\\(k&lt;1/2\\) の場合，式 (1) のドリフト係数が有界になる．このことから，\\(G\\) の可積分性が，\\(X_t\\) の密度の考察に依らず次のように導ける．\n\\[\nM:=\\max_{x\\in\\mathbb{R}^n}\\nabla V(x)\n\\] と定める．\\(V(x)=O(\\lvert x\\rvert^{2k})\\;(\\lvert x\\rvert\\to\\infty)\\) より，ある \\(C&gt;0\\) が存在して， \\[\nV(x)\\le C\\lvert x\\rvert^{2k}\\qquad\\mathrm{on}\\;\\mathbb{R}^n.\n\\] \\[\\begin{align*}\n    \\lvert X_t\\rvert&\\le\\int^t_0\\lvert\\nabla V(X_t)\\rvert\\,dt+\\sqrt{2\\beta^{-1}}\\lvert B_t\\rvert\\\\\n    &\\le Mt+\\sqrt{2\\beta^{-1}}\\lvert B_t\\rvert\n\\end{align*}\\] より， \\[\\begin{align*}\n    \\operatorname{E}_x[\\lvert G(X_t)\\rvert]&\\le\\operatorname{E}_x\\left[e^{\\kappa V(\\lvert X_t\\rvert)}\\right]\\\\\n    &\\le\\operatorname{E}_x\\left[\\exp\\biggr(\\kappa V(M_t+\\sqrt{2\\beta^{-1}\\lvert B_t\\rvert})\\biggl)\\right]\\\\\n    &\\le e^{\\kappa\\lvert Mt\\rvert^{2k}}\\operatorname{E}_x\\left[e^{\\kappa 2^k\\beta^{-k}\\lvert B_t\\rvert^{2k}}\\right]&lt;\\infty.\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2024/Process/Langevin.html#sec-extended-generator",
    "href": "posts/2024/Process/Langevin.html#sec-extended-generator",
    "title": "Langevin Dynamics の多項式エルゴード性",
    "section": "2 微分と拡張生成作用素の関係",
    "text": "2 微分と拡張生成作用素の関係\n\n\\((X_t)\\) を \\(E=\\mathbb{R}^n\\) 上の Feller-Dynkin 過程，\\((P_t)\\) をその遷移半群，\\(\\widehat{L}\\) をその拡張生成作用素とする．\n\n\n\n\n\n\n命題 2\n\n\n\n\\(G\\in\\mathcal{D}(\\widehat{L})\\) とする．すなわち， \\[\nt\\mapsto M_t:=G(X_t)-\\int^t_0\\widehat{L}G(X_s)ds\n\\] は任意の \\(x\\in E\\) について \\(\\operatorname{P}_x\\)-局所マルチンゲールである．\nこのとき，さらに \\(G\\) について次の条件を仮定する：\n\n\\(\\operatorname{E}_x[\\lvert G(X_t)\\rvert]&lt;\\infty\\;(x\\in E,t\\in\\mathbb{R}_+)\\)．すなわち，\\(P_tG:E\\to\\mathbb{R}\\) が定まる．\n同様に \\(\\operatorname{E}_x[\\lvert\\widehat{L}(G)(X_t)\\rvert]&lt;\\infty\\;(x\\in E,t\\in\\mathbb{R}_+)\\)．すなわち，\\(\\widehat{L}P_tG:E\\to\\mathbb{R}\\) も定まる．2\n\\(t\\mapsto P_t\\widehat{L}G(x)\\) は局所有界．\n\nこのとき，\\(P_tG(x)\\) は \\(t\\) で微分可能であり，次が導ける： \\[\n\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_t)]=\\operatorname{E}_x[\\widehat{L}G(X_t)].\n\\]\n\n\nこれは，通常の意味での生成作用素 \\(L\\) の性質 \\[\n\\frac{\\partial }{\\partial t}P_tG=P_t(LG)\n\\] が，可積分性の条件の下で，拡張生成作用素 \\(\\widehat{L}\\) にも引き継がれると理解できる．\n\n\n\n\n\n\n証明\n\n\n\n\n仮定より，停止時の列 \\(\\tau_n\\nearrow\\infty\\;\\;\\text{a.s.}\\) が存在し，任意の \\(n\\in\\mathbb{N}\\) について，\\(M^{\\tau_n}\\) はマルチンゲールで， \\[\n\\operatorname{E}_x\\left[G(X_{t\\land\\tau_n})-\\int^{t\\land\\tau_n}_0\\widehat{L}G(X_s)ds\\right]=G(x),\\qquad  t\\ge0,x\\in E.\n\\tag{4}\\]\n仮定１より \\(\\operatorname{E}_x[\\lvert G(X_{t\\land\\tau_n})\\rvert]&lt;\\infty\\) であるから， \\[\n\\operatorname{E}_x\\left[\\left|\\int^{t\\land\\tau_n}_0\\widehat{L}G(X_s)ds\\right|\\right]&lt;\\infty.\n\\] でもある．従って Fubini-Tonelli の定理から \\[\n\\operatorname{E}_x\\left[\\left|\\int^{t\\land\\tau_n}_0\\widehat{L}G(X_s)ds\\right|\\right]=\\int^t_0\\operatorname{E}_x\\biggl[1_{[0,\\tau_n]}(s)\\widehat{L}G(X_s)\\biggr]\\,ds\n\\] と書き換えられる．\nよって，式 (4) は \\[\n\\operatorname{E}_x\\biggl[G(X_{t\\land\\tau_n})\\biggr]=G(x)+\\int^{t}_0\\operatorname{E}_x\\biggl[1_{[0,\\tau_n]}(s)\\widehat{L}G(X_s)\\biggr]\\,ds\n\\] とも表せる．右辺が \\(t\\) について微分可能であるから，左辺も微分可能である： \\[\n\\frac{\\partial }{\\partial t}\\operatorname{E}_x\\biggl[G(X_{t\\land\\tau_n})\\biggr]=\\operatorname{E}_x\\biggl[1_{[0,\\tau_n]}(t)\\widehat{L}G(X_t)\\biggr].\n\\] \n両辺の \\(n\\to\\infty\\) に関する極限を取ると，右辺は \\(\\lvert\\widehat{L}G(X_t)\\rvert\\) が \\(\\operatorname{P}_x\\)-可積分であるから（仮定２），Lebesgue の優収束定理より， \\[\n\\lim_{n\\to\\infty}\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_{t\\land\\tau_n})]=\\lim_{n\\to\\infty}\\operatorname{E}_x\\biggl[1_{[0,\\tau_n]}(t)\\widehat{L}G(X_t)\\biggr]=\\operatorname{E}_x[\\widehat{L}G(X_t)],\\qquad x\\in E,t\\in(0,\\infty).\n\\]\n加えてこの収束は，\\(t\\in(0,\\infty)\\) に関して広義一様に起こる．実際，Hölder の不等式より，3 \\[\\begin{align*}\n&\\qquad\\sup_{t\\in[0,T]}\\left|\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_{t\\land\\tau_n})]-\\operatorname{E}_x[\\widehat{L}G(X_t)]\\right|\\\\\n&=\\sup_{t\\in[0,T]}\\biggl|\\operatorname{E}_x[1_{[0,\\tau_n]}(t)\\widehat{L}G(X_t)]-\\operatorname{E}_x[\\widehat{L}G(X_t)]\\biggr|\\\\\n&=\\sup_{t\\in[0,T]}\\biggl|\\operatorname{E}_x\\biggl[(1-1_{[0,\\tau_n]}(t))\\widehat{L}G(X_t)\\biggr]\\biggr|\\\\\n&\\le\\sup_{t\\in[0,T]}\\operatorname{E}_x\\biggl[(1-1_{[0,\\tau_n]}(T))\\lvert\\widehat{L}G(X_t)\\rvert\\biggr]\\\\\n&\\le\\|1-1_{[0,\\tau_n]}(T)\\|_{L^\\infty(\\Omega)}\\sup_{t\\in[0,T]}\\operatorname{E}_x\\left[\\lvert\\widehat{L}G(X_t)\\rvert\\right]\\xrightarrow{n\\to\\infty}0.\n\\end{align*}\\] 最後の不等式にて，仮定３による局所有界性 \\[\n\\sup_{t\\in[0,T]}\\operatorname{E}_x\\left[\\lvert\\widehat{L}G(X_t)\\rvert\\right]&lt;\\infty\n\\] を用いた．\nこの導関数の一様収束と，Lebesgue の優収束定理による各点収束 \\[\n\\operatorname{E}_x[G(X_{t\\land\\tau_n})]\\xrightarrow{n\\to\\infty}\\operatorname{E}_x[G(X_t)]\n\\] を併せると，\\(\\operatorname{E}_x[G(X_t)]\\) も可微分で，その導関数は極限 \\[\n\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_t)]=\\lim_{n\\to\\infty}\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_{t\\land\\tau_n})]=\\operatorname{E}_x[\\widehat{L}G(X_t)]\n\\] として得られることが結論づけられる．\n\n\n\n\n\n\n\n\n(Rudin, 1976, pp. 152 定理7.17)4\n\n\n\n\\(f_n:[a,b]\\to\\mathbb{R}\\) を可微分な関数列とし，ある関数 \\(f:[a,b]\\to\\mathbb{R}\\) に各点収束するものとする．\n仮に，導関数列 \\(\\{f'_n\\}\\) が一様位相に関して Cauchy 列ならば，\\(f_n\\to f\\) も一様収束し，加えて \\(f\\) も可微分で， \\[\n\\lim_{n\\to\\infty}f'_n(x)=f'(x)\n\\] が成り立つ．"
  },
  {
    "objectID": "posts/2024/Process/Langevin.html#sec-Gronwall",
    "href": "posts/2024/Process/Langevin.html#sec-Gronwall",
    "title": "Langevin Dynamics の多項式エルゴード性",
    "section": "3 下界の導出",
    "text": "3 下界の導出\n元来の目的である下界の導出のためには， \\[\n\\operatorname{E}_x[G(X_t)]\\le CG(x)\\exp\\left(ct^{\\frac{k}{1-k}}\\right)\n\\] という評価を得る必要がある．Gronwall の不等式を用いれば，導関数に関する不等式 \\[\n\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_t)]\\le\\operatorname{E}_x[\\widehat{L}G(X_t)]\\le C\\operatorname{E}_x[\\varphi\\circ G(X_t)]\n\\] があれば十分である．この導関数に関する不等式は，命題 2 とドリフト条件 (3) \\[\n\\widehat{L}G\\le C\\varphi\\circ G\n\\] を併せることで， \\[\n\\frac{\\partial }{\\partial t}\\operatorname{E}_x[G(X_t)]=\\operatorname{E}_x[\\widehat{L}G(X_t)]\\le C\\operatorname{E}_x[\\varphi\\circ G(X_t)]\n\\] より得る．"
  },
  {
    "objectID": "posts/2024/Process/Langevin.html#footnotes",
    "href": "posts/2024/Process/Langevin.html#footnotes",
    "title": "Langevin Dynamics の多項式エルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Kohatsu-Higa et al., 2022) で最初に知った．特に (Kohatsu-Higa, 2003) は詳しく扱っており，上からの評価は Malliavin 解析から得られる (Taniguchi, 1985)．同様にして熱方程式の基本解としても捉えられるが．↩︎\n元々はある正の定数 \\(C&gt;0\\) が存在して，\\(\\widehat{L}G\\le CG\\)．ある凹関数 \\(\\varphi\\) について \\(\\widehat{L}G\\le\\varphi\\circ G\\) が成り立つならばこの仮定は満たされることに注意，としていた．↩︎\n\\(\\sup_{t\\in[0,T]}\\widehat{L}G(X_t)\\) は可積分とは限らないため，\\(\\sup\\) を期待値の中に入れることはできない．Hölder の不等式により，これを迂回できる．↩︎\n(杉浦光夫, 1980, p. 311) 定理13.7系では，\\(f_n\\) に \\(C^1\\)-級の仮定を置いて，この場合は \\(f\\) が \\(C^1\\)-級になることを導いている．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Levy.html",
    "href": "posts/2024/Process/Levy.html",
    "title": "Lévy 過程を見てみよう",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nYUIMAについては次の記事も参照：\nPoisson 過程と複合 Poisson 過程については次の記事を参照："
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "href": "posts/2024/Process/Levy.html#lévy-itô-分解",
    "title": "Lévy 過程を見てみよう",
    "section": "1 Lévy-Itô 分解",
    "text": "1 Lévy-Itô 分解\n\n1.1 加法過程の定義\n\n\n\n\n\n\n定義 (additive process, Lévy process)1\n\n\n\n確率過程 \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 加法過程 であるとは，最初の４条件を満たすことをいう．５条件全てを満たすとき，Lévy 過程 であるという．\n\n\\(X_0\\overset{\\text{a.s.}}{=}0\\)．\nある充満集合 \\(\\Omega_0\\subset\\Omega\\) が存在し，\\(t\\mapsto X_t(\\omega)\\) は càdlàg である．\n独立増分：任意の \\(0\\le t_0&lt;\\cdots&lt;t_n\\) について， \\[\n  X_{t_0},X_{t_1}-X_{t_0},X_{t_2}-X_{t_1},\\cdots,X_{t_n}-X_{t_{n-1}}\n  \\] は独立である．\n確率連続：任意の \\(\\epsilon&gt;0\\) と \\(t\\ge0\\) について， \\[\n  \\lim_{s\\to t}\\operatorname{P}[\\lvert X_s-X_t\\rvert&gt;\\epsilon]=0\n  \\] が成り立つ．\n定常増分：\\(X_{s+t}-X_s\\) の分布は \\(s\\ge0\\) に依らない．\n\n\n\n\n\n1.2 特性量\n加法過程 \\(\\{X_t\\}\\) について，\\(X_t\\) の分布は必ず \\(\\mathbb{R}^d\\) 上の無限可分分布になる．2\n加えて，加法過程の分布は１次元の有限次元分布族が特徴付ける．\n\n\n\n\n\n\n定理（加法過程の分布）3\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を加法過程とする．\n\n\\(\\mu_{s,t}\\) を \\(X_t-X_s\\) の分布とすると，これは無限可分であり，次を満たす： \\[\n\\mu_{s,t}*\\mu_{t,u}=\\mu_{s,u},\n\\] \\[\n\\mu_{s,s}=\\delta_0,\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\quad(s\\nearrow t),\n\\] \\[\n\\mu_{s,t}\\to\\delta_0\\qquad(t\\searrow s).\n\\]\n確率分布の族 \\(\\{\\mu_{s,t}\\}\\subset\\mathcal{P}(\\mathbb{R}^d)\\) が上の４式を満たすならば，これはある加法過程 \\(\\{X_t\\}\\) が定める分布である．\n２つの加法過程 \\(X,X'\\) が， \\[\nX_t\\overset{\\text{d}}{=}X'_t\\quad t\\in\\mathbb{R}_+\n\\] を満たすならば，\\(X\\overset{\\text{d}}{=}X'\\) が成り立つ．\n\n\n\nこのことにより，加法過程 \\(X\\) の分布は，各 \\(X_t\\) の無限可分分布を特徴付ける特性量 \\((A_t,\\nu_t,\\gamma(t))\\) によって特徴付けられる．\n\n\n\n\n\n\n無限可分分布の特徴付け (Khinchin and Lévy, 1936)4\n\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) について，次は同値：\n\n\\(f\\) は無限可分である．\n(Lévy) ある \\[\n  \\nu(\\{0\\})=0,\\qquad\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n  \\] を満たす測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}^d)\\) と対称な半正定値行列 \\(A\\in S_n(\\mathbb{R})_+\\) と \\(\\gamma\\in\\mathbb{R}^d\\) が一意的に存在して，次のように表せる： \\[\n  f(z)=\\exp\\left(-\\frac{1}{2}(z|Az)+i(\\gamma|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\\biggl)\\nu(dx)\\right).\n  \\]\n(Khinchin) ある有限測度 \\(\\Psi\\in\\mathcal{M}^1(\\mathbb{R}^d)\\) と \\(\\alpha\\in\\mathbb{R}^d\\) が存在して，次のように表せる： \\[\nf(u)=\\exp\\left(i(\\alpha|u)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(u|x)}-1-\\frac{i(u|x)}{1+\\lvert x\\rvert^2}\\biggl)\\frac{1+\\lvert x\\rvert^2}{\\lvert x\\rvert^2}\\Psi(dx)\\right).\n\\]\n\n\\(A\\) を Gauss 共分散，\\(\\nu\\) を Lévy 測度，\\(\\Psi\\) を Khintchine測度 という．5\n加えて，任意の半正定値行列 \\(A\\)，\\(\\gamma\\in\\mathbb{R}^d\\)，Lévy 測度 \\(\\nu\\) であって \\(\\nu(\\{0\\})=0\\) かつ \\[\n\\int_{\\mathbb{R}^d}(\\lvert x\\rvert^2\\land1)\\nu(dx)&lt;\\infty\n\\] を満たすものに対して，\\((A,\\nu,\\gamma)\\) を特性量にもつ無限可分分布が存在する．\nKhintchin の表示はより直接的である上に，Khintchin 測度は有限になる．さらに，\\((\\alpha,\\Psi)\\) の収束が過程の収束にも対応する！6 だが，確率論的な意味付けに欠けるために，Lévy の表示の方をここでは用いる．\nLévy の表示の被積分関数 \\[\ne^{i(z|x)}-1-i(z|x)1_{\\left\\{B^d\\right\\}}(x)\n\\] は大変複雑であるが，こうしないと \\(\\nu\\)-可積分にならないのである．\n\\(\\nu\\) は \\(O(\\lvert x\\rvert^2)\\) 関数に関してならば \\(0\\) の近傍でも可積分であるから，\\(e^{i(z|x)}\\) から１次以下の項を \\(0\\) の近傍から取り去ることで可積分にしているのである．そのため，最後の項は \\(1_{\\left\\{B^d\\right\\}}\\) でなくとも， \\[\nc(x)=1+o(\\lvert x\\rvert)\\quad(\\lvert x\\rvert\\to0)\n\\] \\[\nc(x)=O(\\lvert x\\rvert^{-1})\\quad(\\lvert x\\rvert\\to\\infty)\n\\] の２条件を満たすものならばなんでも良い．だが，取り替える度に１次の項 \\(\\gamma\\in\\mathbb{R}^d\\) を変更する必要がある．\n一般に \\(\\gamma\\) はドリフトと呼んではいけない． \\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] を満たす場合のみ， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_0|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right)\n\\] と表示でき，この際の \\(\\gamma_0\\in\\mathbb{R}^d\\) を ドリフト と呼ぶ．\n逆に， \\[\n\\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\n\\] が成り立つとき， \\[\nf(z)=\\exp\\left(-\\frac{(z|Az)}{2}+i(\\gamma_1|z)+\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1-i(z|x)\\biggl)\\nu(dx)\\right)\n\\] と表示でき，\\(\\gamma_1\\) は \\(f\\) が定める確率分布の平均に一致する．7\n\n\n\nLévy 過程は，\\(A:=A_1,\\nu:=\\nu_1,\\gamma:=\\gamma(1)\\) について， \\[\nA_t=tA,\\quad\\nu_t=t\\nu,\\quad\\gamma_t=t\\gamma\n\\] と表せる場合に当たる．\n\n\n1.3 強度測度との関係\n\\(\\{(A_t,\\nu_t,\\gamma_t)\\}_{t\\in\\mathbb{R}_+}\\) を加法過程の特性量とする．\nこのとき， \\[\n\\widetilde{\\nu}([0,t]\\times B):=\\nu_t(B),\\qquad t\\ge0,B\\in\\mathcal{B}(\\mathbb{R}^d)\n\\] は \\(\\mathbb{R}_+\\times\\mathbb{R}^d\\) 上に測度を定める．\n\n\n\n\n\n\n命題（強度測度と特性測度の関係）8\n\n\n\n測度の族 \\(\\{\\nu_t\\}\\subset\\mathcal{M}(\\mathbb{R}^d)\\) と測度 \\(\\nu\\in\\mathcal{M}(\\mathbb{R}_+\\times\\mathbb{R}^d)\\) について，次は同値：\n\n\\(\\widetilde{\\nu}\\) は次の２条件を満たす： \\[\n\\widetilde{\\nu}(\\{t\\}\\times\\mathbb{R}^d)=0,\n\\] \\[\n\\int_{[0,t]\\times\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\widetilde{\\nu}(dsdx)&lt;\\infty.\n\\]\n\\(\\{\\nu_t\\}\\) はある加法過程の特性測度である．\n\n\n\nよって，任意の加法過程について， \\[\n\\int_{\\mathbb{R}^d}(1\\land\\lvert x\\rvert^2)\\nu_t(dx)&lt;\\infty\n\\] が必要である．\nLévy 過程であるとき，定常増分であることが必要であるため，跳躍時刻は \\(\\mathbb{R}_+\\) 上の一様な Poisson 点過程に従う必要がある．これより， \\[\n\\widetilde{\\nu}=\\ell_+\\otimes\\nu\n\\] と分解できる必要があり，この特性測度 \\(\\nu\\) が Lévy 測度である．このとき，\\(\\nu_t=t\\nu\\) かつ \\(\\widetilde{\\nu}(dsdx)=ds\\nu(dx)\\)．\n\n\n1.4 一般の分解\n\n\n\n\n\n\n定理 (Ito, 1941)9\n\n\n\n\\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) を特性量 \\(\\{(A_t,\\nu_t,\\gamma(t))\\}\\) を持つ加法過程とする． \\[\n\\eta(\\omega,B):=\\#\\left\\{t\\in\\mathbb{R}_+\\,\\middle|\\,\\begin{pmatrix}t\\\\X_t(\\omega)-X_{t-}(\\omega)\\end{pmatrix}\\in B\\right\\}\n\\] を \\(\\omega\\in\\Omega_0\\) 上で \\(B\\in\\mathcal{B}(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\})\\) に関して定める．\n\n\\(\\eta\\) は \\(\\mathbb{R}^+\\times\\mathbb{R}^d\\setminus\\{0\\}\\) 上の強度測度 \\(\\widetilde{\\nu}\\) を持った Poisson 点過程である．\nある充満集合 \\(\\Omega_1\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\\begin{align*}\n     X^1_t(\\omega)&:=\\lim_{\\epsilon\\searrow0}\\int_0^t\\int_{\\left\\{\\epsilon&lt;\\lvert x\\rvert\\le 1\\right\\}}x\\,\\widetilde{\\nu}(\\omega,dsdx)\\\\\n     &\\qquad+\\int_0^t\\int_{\\mathbb{R}^d\\setminus B^d}x\\,\\eta(\\omega,dsdx)\n   \\end{align*}\\] 収束は \\(t\\in\\mathbb{R}_+\\) に関して広義一様であり，\\(X^1\\) は特性量 \\(\\{(0,\\nu_t,0)\\}_{t\\in\\mathbb{R}_+}\\) が定める加法過程である．\n\\(X^2_t:=X_t-X_t^1\\) は殆ど確実に連続な見本道を持ち，特性量 \\(\\{(A_t,0,\\gamma(t))\\}\\) が定める加法過程である．\n\\(X^1\\perp\\!\\!\\!\\perp X^2\\) が成り立つ．\n\n\n\n\n\n1.5 B 型の場合\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] を満たす場合，Poisson 補過程によらない，より簡潔な表示を持つ．\n\n\n\n\n\n\n定理\n\n\n\n\\[\n\\int_{B^d}\\lvert x\\rvert\\,\\nu_t(dx)&lt;\\infty,\\quad t&gt;0\n\\] が成り立つ場合，次が成り立つ：\n\nある充満集合 \\(\\Omega_3\\subset\\Omega\\) が存在して，この上で次が定まる： \\[\n   X^3_t(\\omega):=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}x\\,\\eta(\\omega,dsdx).\n   \\] このとき，\\(X_t^3\\) の分布は複合 Poisson である： \\[\n   \\operatorname{E}[e^{i(z|X_t^3)}]=\\exp\\left(\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu_t(dx)\\right).\n   \\]\n\\(X^4_t:=X_t-X_t^3\\) は殆ど確実に連続な見本道を持ち，Gauss 過程を定める： \\[\n   \\operatorname{E}[e^{i(z|X_t^4)}]=\\exp\\left(-\\frac{1}{2}(z|A_tz)+i(\\gamma_0(t)|z)\\right).\n   \\]\n\\(X^3\\perp\\!\\!\\!\\perp X^4\\) が成り立つ．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#lévy-測度",
    "href": "posts/2024/Process/Levy.html#lévy-測度",
    "title": "Lévy 過程を見てみよう",
    "section": "2 Lévy 測度",
    "text": "2 Lévy 測度\n\n2.1 はじめに\n本節の目的は，Lévy 過程の次の３分類の見本道の違いを理解することである：10\n\n\n\n\n\n\n特性量 \\((A,\\nu,\\gamma)\\) を持つ Lévy 過程について，\n\nA 型：\\(A=0\\) かつ \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\nB 型：\\(A=0\\) かつ \\(\\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty\\) であるが，A 型ではない．\nC 型：それ以外．\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) 型は拡散項を持たず，確定的な動きと複合 Poisson 過程の和で表現される．ジャンプは離散的に起こる．\n\\(B\\) 型も拡散項を持たないが，\\(\\mathbb{R}_+\\) 上稠密な可算集合上でジャンプを繰り返す．Gamma 過程（第 3.6 節）がその例である．\n\\(A,B\\) は殆ど確実に任意の有界区間上で有界変動な見本道を持つが，\\(C\\) 型は有界変動ではない．11 Brown 運動と Cauchy 過程（第 4.4 節）がその例である．\n\n\n\n\n\n\n2.2 Lévy 測度が零ならば，Gauss 過程である\n\n\n\n\n\n\n命題（連続な Lévy 過程の特徴付け）12\n\n\n\nLévy 過程 \\(X\\) について，次の２条件は同値：\n\n\\(X\\) は殆ど確実に連続な見本道を持つ．\n\\(\\nu=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n時刻 \\(t&gt;0\\) までの跳躍回数を表す Poisson 過程 \\[\nN_t:=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}\\eta(dsdx)\\in[0,\\infty]\n\\] を考えると， \\[\n\\operatorname{E}[N_t]=\\int_0^t\\int_{\\mathbb{R}^d\\setminus\\{0\\}}ds\\nu(dx)=0.\n\\] すなわち，\\(N_t=0\\;\\;\\text{a.s.}\\)\n\n\n\n\n\n2.3 区分定数ならば，A 型である．\n\n\n\n\n\n\n命題（A 型の見本道の特徴付け）13\n\n\n\nLévy 過程 \\(X\\) ついて，次の３条件は同値：\n\n\\(X\\) の見本道は，殆ど確実に区分的定数であり，有界区間上では有限回のジャンプしか起こらない．\n\\(X\\) は複合 Poisson 分布であるか，零であるかのいずれかである．\n\\(X\\) は A 型で，かつ \\(\\gamma_0=0\\) である．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n1 \\(\\Rightarrow\\) 2\n任意の有限区間内でのジャンプ回数は有限回であるため，ジャンプ回数の Poisson 過程 \\(N\\) について，\\(N_t\\sim\\mathrm{Pois}(t\\nu(\\mathbb{R}^d))\\) の母数は有限である必要がある．特に \\(\\nu(\\mathbb{R}^d)&lt;\\infty\\)．\\(X\\) に連続部分がないことを併せると，定理 1.5 より， \\[\n\\operatorname{E}[e^{i(z|X_t)}]=\\exp\\left(t\\int_{\\mathbb{R}^d}\\biggr(e^{i(z|x)}-1\\biggl)\\nu(dx)\\right).\n\\] これは \\(\\mathrm{CP}(t,\\nu)\\) の特性関数である．\n2 \\(\\Rightarrow\\) 1\nこちらは省略する．\n\n\n\n\n純粋跳躍確率過程であっても，B 型ならば，見本道は区分的定数にはならない．Gamma 過程（第 3.6 節）がその例である．\n\n\n2.4 B 型の跳躍時刻\nLévy 過程の見本道は右連続であるから，\\(\\mathbb{R}_+\\) 上トータルの跳躍回数は殆ど確実に可算回である．\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) の場合は，有限区間上での跳躍回数も無限になる．\nさらに，次のことが言える：\n\n\n\n\n\n\n命題（B 型 Lévy 過程のジャンプ時刻）14\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) とする．このとき，跳躍時刻は殆ど確実に \\(\\mathbb{R}_+\\) 上稠密である．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\(\\nu(\\mathbb{R}^d)=\\infty\\) のとき， \\[\nT_\\epsilon(\\omega):=\\inf\\left\\{t\\ge 0\\mid\\lvert X_t(\\omega)-X_{t-}(\\omega)\\rvert&gt;\\epsilon\\right\\}\n\\] とすると， \\[\n\\lim_{\\epsilon\\searrow0}\\operatorname{P}[T_\\epsilon\\le t]=1.\n\\] よって， \\[\n\\lim_{\\epsilon\\searrow0}T_\\epsilon=0\\;\\;\\text{a.s.}\n\\] これは，ある充満集合 \\(\\Omega_0\\subset\\Omega\\) の上で，\\(0\\) が \\(X\\) の跳躍時刻の触点になることを含意している．\nこれと同様の議論を任意の \\(s\\in\\mathbb{Q}\\cap\\mathbb{R}_+\\) について繰り返すことで，ある充満集合 \\[\n\\bigcap_{s\\in\\mathbb{Q}\\cap\\mathbb{R}_+}\\Omega_s\n\\] 上で，\\(X\\) の跳躍時刻の閉包が \\(\\mathbb{R}_+\\) 上で稠密になることがわかる．\n\n\n\n\n\n2.5 従属過程ならば B 型である\n\\(d=1\\) で，殆ど確実に単調増加な見本道を持つ Lévy 過程を 従属過程 (subordinator) という．15\n\n\n\n\n\n\n命題（単調増加性の特徴付け）16\n\n\n\n\\(d=1\\) とし，\\(X\\) を Lévy 過程とする．このとき，次は同値：\n\n\\(X\\) は従属過程である．\n\\(A=0,\\nu((-\\infty,0))=0\\) かつ \\[\n\\int_{[0,1)}x\\,\\nu(dx)&lt;\\infty\n\\] 加えて \\(\\gamma_0\\ge0\\) である．\n\n\n\n仮に \\(A=0,\\nu((-\\infty,0))=0\\) だが， \\[\n\\int_0^1x\\,\\nu(dx)=\\infty\n\\] であったとする．\nこのとき，正なジャンプとドリフトしか持たないはずであるから，場合によっては単調増加過程になっても良さそうなものである．\nしかし，このような過程が発散せずに well-defined であるということは，負の方向に無限に強いドリフトを持っており，これが正なジャンプを打ち消していることが必要である．\nそれ故，ジャンプの隙間では負方向のドリフトが競り勝ち，全体としては単調増加にならない．特に，任意の区間において単調増加にならない．17\n\n\n2.6 C 型ならば非有界変動である\n\n\n\n\n\n\n命題（見本道の変動）18\n\n\n\nLevy 過程 \\(X\\) について，\n\nA 型または B 型ならば，有界変動過程である．すなわち，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動である．\nC 型ならば，殆ど確実に，任意の \\(t&gt;0\\) について，\\([0,t]\\) 上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "href": "posts/2024/Process/Levy.html#従属過程と-gamma-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "3 従属過程と Gamma 過程",
    "text": "3 従属過程と Gamma 過程\n\n3.1 はじめに\nGamma 過程は，拡散項もドリフト \\(\\gamma_0\\) も持たない，純粋跳躍な従属過程である．\nしかし，正のジャンプのみをもち，ジャンプだけで増加していく過程だからと言って，その見本道は区分的に定数ではない．\nその Lévy 測度は \\(\\nu((0,\\infty])=\\infty\\) を満たし，B 型に分類される．従って，\\(\\mathbb{R}_+\\) の稠密部分集合上でジャンプしており，見本道は殆ど確実に，任意の点 \\(t\\in\\mathbb{R}_+\\) で非連続である．\nGamma 過程は元々，(Moran, 1959) によりダムの貯水量のモデルとして導入された．\n\n\n\n\n\n\n証明\n\n\n\n\n\n見本道 \\(X_\\bullet(\\omega)\\) は，\\(\\mathbb{R}_+\\) のある稠密部分集合 \\(A\\subset\\mathbb{R}_+\\) 上でジャンプしているとする：\\(\\overline{A}=\\mathbb{R}_+\\)．\nこのとき，\\(\\mathbb{R}_+\\) の任意の点で \\(X_\\bullet(\\omega)\\) は非連続である．\n実際，任意の \\(t&gt;0\\) を取り，ここで連続であるとすると，任意の \\(t\\) への収束列 \\(\\{t_n\\}\\subset\\mathbb{R}_+\\) について，\\(X_{t_n}(\\omega)\\to X_t(\\omega)\\) が成り立つ必要があるが，\\(t\\) は \\(A\\) の触点でもあるので，これに収束する \\(A\\) の点列 \\(\\{t_n\\}\\subset A\\) が取れる．これを特に，下から単調に収束するように取る：\\(t_n\\searrow t\\)．\n\n\n\nしかし，\\(\\nu\\) は平均を持つために有界変動ではあり，実際シミュレーションによって得る見本道を見ても，殆どのジャンプは目に見えない．\n\n\n3.2 Gamma 分布\n\\(\\mathbb{R}\\) 上の Gamma 分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\) とは，密度関数 \\[\ng(x;\\alpha,\\nu):=\\frac{\\alpha^\\nu}{\\Gamma(\\nu)}x^{\\nu-1}e^{-\\alpha x}1_{\\mathbb{R}^+}(x)\n\\] が定める分布をいう．\\(\\alpha\\) をレート，\\(\\nu\\) を形状パラメータというのであった．\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n3.3 Gamma 点過程\n\\(\\sigma\\)-有限測度 \\(\\rho_0\\in\\mathcal{P}(E)\\) と Lévy 測度 \\(\\nu:=\\mathrm{Gamma}(\\alpha,0)\\)，すなわち \\[\n\\nu(dr):=\\frac{e^{-\\alpha r}}{r}1_{\\mathbb{R}^+}(r)\\,dr\n\\] について，\\(\\lambda:=\\rho_0\\otimes\\nu\\) で定まる強度測度を持つ \\(E\\times\\mathbb{R}_+\\) 上の Poisson 点過程 \\(\\xi\\) を Gamma 点過程 という．19\nこれは \\[\n\\xi(B)\\sim\\mathrm{Gamma}(\\alpha,\\rho_0(B))\n\\] を満たす複合 Poisson 点過程である．\\(\\rho_0\\) のことを形状測度ともいう．\n\n\n\n\n\n\nDirichlet 過程 (Ferguson, 1973) との関係20\n\n\n\n\n\n\\[\n\\Delta_n:=\\left\\{\\begin{pmatrix}p_1\\\\\\vdots\\\\p_n\\end{pmatrix}\\in[0,1]^n\\,\\middle|\\,\\sum_{i=1}^np_i=1\\right\\}\n\\] を \\(n-1\\)-単体とする．21 この上に台を持つ，パラメータ \\(\\alpha\\in(0,\\infty)^n\\) で定まる密度 \\[\nf(x)=\\frac{\\Gamma(\\alpha_1+\\cdots+\\alpha_n)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_n)}x_1^{\\alpha_1-1}\\cdots x_n^{\\alpha_n-1}1_{\\Delta_n}(x)\n\\] が定める分布 \\(\\mathrm{Dirichlet}(n,\\alpha)\\in\\mathcal{P}(\\Delta_n)\\) を Dirichlet 分布 という．\nここで，\\(E\\) 上の Gamma 点過程 \\(\\xi\\) は \\(\\rho_0(E)&lt;\\infty\\) を満たすとする．このとき，\\(E\\) の分割 \\[\nE=B_1\\sqcup\\cdots\\sqcup B_n\n\\] \\[\n\\rho_0(B_i)&gt;0\n\\] に対して， \\[\n(\\zeta(B_1),\\cdots,\\zeta(B_n))\\sim\\mathrm{Dirichlet}(n,\\alpha)\n\\] \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] が成り立ち，これは \\(\\xi(E)\\) と独立である．\nこのことをふまえて，\\(\\rho_0\\) が有限であるとき，ランダム確率測度 \\[\n\\zeta(-):=\\frac{\\xi(-)}{\\xi(E)}\n\\] を Dirichlet 過程 という．22\n\n\n\n\n\n3.4 Gamma 点過程の Lévy 測度は \\(0\\) の近傍で発散する\nしかし，\\(\\mathrm{Gamma}(\\alpha,0)\\) などという分布はなく， \\[\n\\nu(\\mathbb{R})=\\int^\\infty_0r^{-1}e^{-\\alpha r}dr=\\infty.\n\\]\nこのとき，任意の \\(\\rho_0\\) で測って正の測度を持つ集合 \\(\\rho_0(B)&gt;0\\;(B\\in\\mathcal{E})\\) に対して，\\(\\xi\\) は殆ど確実に無限個の点を \\(B\\) 内にもつ．23\nしかし，\\(\\rho_0(B)&lt;\\infty\\) ならば \\(\\xi(B)&lt;\\infty\\) ではある．すなわち，ジャンプ幅も含めて足し合わせると，収束する．これは，\\(\\nu\\) が平均を持つことによる： \\[\n\\int_0^\\infty r\\,\\nu(dr)=\\alpha^{-1}.\n\\]\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{align*}\n  \\xi(B)&=\\int^\\infty_0\\int_Br\\,\\eta(dsdr)\\\\\n  &=\\int_B\\rho_0(ds)\\int^\\infty_0r\\,\\nu(dr)\\\\\n  &=\\rho_0(B)\\alpha^{-1}\n\\end{align*}\\]\n\n\n\n\n\n3.5 従属過程\n一般に，\\(\\xi\\) を \\(\\mathbb{R}^+\\) 上の Lévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を持つ一様な複合 Poisson 点過程，すなわち \\(\\ell_+\\otimes\\nu\\) を強度測度とする \\(\\mathbb{R}_+\\times\\mathbb{R}^+\\) 上の Poisson 点過程とすると， \\[\nY_t(\\omega):=\\xi(\\omega,[0,t])\n\\] で定まる過程 \\(Y\\) は，一般に Lévy 測度 \\(\\nu\\) を持つ 従属過程 (subordinator) という．24\n\n\n3.6 Gamma 計数過程\nLévy 測度 \\(\\nu\\in\\mathcal{P}(\\mathbb{R}^+)\\) を \\[\n\\nu(dr):=\\delta\\frac{e^{-\\gamma r}}{r}dr\n\\] \\[\n\\delta,\\gamma&gt;0,\n\\] で与えたとき，付随する従属過程 \\(\\{Y_t\\}\\) を Gamma 過程 といい，\\(\\mathrm{Gamma}(\\delta,\\gamma)\\) で表す．25\nこれは \\(Y_t\\sim\\mathrm{Gamma}(\\gamma,\\delta t)\\) を満たす Lévy 過程である．\n\n\n\n\n\n\n\n\n\n目視できないジャンプが無数に存在することが窺える．\n\n\n\n3.7 分散 Gamma 過程\n２つの独立な Gamma 過程 \\[\nX^+\\sim\\mathrm{Gamma}(\\delta,\\gamma^-),X^-\\sim\\mathrm{Gamma}(\\delta,\\gamma^+)\n\\] に対して， \\[\nX^0_t=X^+_t-X^-_t\n\\] と表せる Lévy 過程 \\(X^0\\) を 分散 Gamma 過程 という．26\n\n\n\n\n\n\n\n\n\n分散 Gamma 過程は，オプション価格の対数のモデルとして，Brown 運動より柔軟なモデルとしても用いられる (Madan et al., 1998)．\nこれは，Brown 運動の分散が Gamma 分布に従うとして得る過程であるとも見れる．実際，Brown 運動の時間を，Gamma 過程によって変換したものが分散 Gamma 過程である．\n実際，Brown 運動 \\(B\\) とこれと独立な Gamma 過程 \\(T\\) について， \\[\nX^0_t=B_{T_t}\n\\] と表せる．27"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "href": "posts/2024/Process/Levy.html#安定過程と-cauchy-過程",
    "title": "Lévy 過程を見てみよう",
    "section": "4 安定過程と Cauchy 過程",
    "text": "4 安定過程と Cauchy 過程\n\n4.1 安定分布\n\n4.1.1 定義\n\n\n\n\n\n\n定義 (stable)28\n\n\n\n\n特性関数 \\(f:\\mathbb{R}^d\\to\\mathbb{C}\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して \\[\nf(t)^n=f(a_nt)e^{ib_nt}\n\\] が成り立つ無限可分分布の特性関数をいう．\n確率変数 \\(Y\\in\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) が 安定 であるとは，任意の \\(n\\in\\mathbb{N}^+\\) に対して，ある \\(a_n&gt;0,b_n\\in\\mathbb{R}^d\\) が存在して， \\[\nY_1+\\cdots+Y_n\\overset{\\text{d}}{=}a_nY+b_n\n\\] を満たすことをいう．\n\n\n\nすなわち，安定分布とは， \\[\nZ_n:=\\frac{\\sum_{i=1}^nY_i-b_n}{a_n}\n\\] という形の，独立同分布確率変数の正規化された和の列 \\(\\{Z_n\\}\\) の分布収束極限として現れ得る分布の全体を指すことになる．29\nまた，\\(a_n\\) は \\(a_n=n^{1/\\alpha}\\) という形に限り，この \\(\\alpha\\in(0,2]\\) を 安定指数 という．\n\n\n4.1.2 Lévy 測度の有限性\n安定指数 \\(\\alpha\\in(0,2)\\) を持つ安定分布の Lévy 測度は非有限であり，平均も持たない．\n\n\n\n\n\n\n命題（Lévy 測度の平均）30\n\n\n\n\\(\\mu\\in\\mathcal{P}(\\mathbb{R}^d)\\) を \\(\\alpha\\)-安定分布とする．このとき，その Lévy 測度 \\(\\nu\\) について，次は同値：\n\n\\(\\alpha\\in(0,1)\\) である．\n\\(\\nu\\) は \\(B^d\\) 上で平均を持つ： \\[\n  \\int_{B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n次も同値：\n\n\\(\\alpha\\in(1,2)\\) である．\n\\(\\nu\\) は \\(\\mathbb{R}^d\\setminus B^d\\) 上で平均を持つ： \\[\n  \\int_{\\mathbb{R}^d\\setminus B^d}\\lvert x\\rvert\\,\\nu(dx)&lt;\\infty.\n  \\]\n\n\n\n\n\n\n4.2 回転対称な安定分布\n\n4.2.1 特性関数の表示\n安定分布は無限可分であるため，Lévy-Khintchin 分解を通じた特性関数の形が特徴付けられる．\n中でも，（回転）対称な安定分布は特に簡単な表示を持つ：\n\n\n\n\n\n\n命題 (Lévy-Khinchin 表示)31\n\n\n\n\\(P\\in\\mathcal{P}(\\mathbb{R}^d)\\) は回転対称であるとする．このとき，その特性関数 \\(\\varphi\\) について次は同値：\n\n\\(\\varphi\\) は安定である．\nある \\(c&gt;0\\) と \\(\\alpha\\in(0,2]\\) が存在して， \\[\n\\varphi(u)=e^{-c\\lvert u\\rvert^\\alpha}.\n\\]\n\nこの \\(\\alpha\\) を 安定指数 という．\n\n\n\n\n\n\n\n\n例（対称な安定分布）\n\n\n\n\n\\(\\alpha=2\\) の対称安定分布とは，中心化された正規分布である．\n\\(\\alpha=1\\) の対称安定分布とは，中心化された Cauchy 分布である．\n\n\n\n\n\n\n\n\n\n中心極限定理のスケーリングレートとしての安定指数\n\n\n\n\n\n\\(a_n\\) は従って，中心極限定理を実現するために必要なスケーリングレートを表す．\nこのことは，一般のエルゴード的な定常過程に対して一般化できる：32\n\\(\\{X_n\\}\\) を \\(\\alpha\\)-撹拌的な定常過程，\\(\\{a_n\\}\\subset\\mathbb{R}^+\\) を発散列とし， \\[\n\\frac{1}{a_n}\\sum_{j=1}^nX_j-b_n\n\\] は弱収束するとする．この極限分布は安定分布になり，安定指数を \\(\\alpha\\) とする．\nこのとき，(Karamata, 1933) の意味で緩変動な関数 \\(h\\) に対して，\\(a_n=n^{1/\\alpha}h(n)\\) と表せる： \\[\n\\lim_{n\\to\\infty}\\frac{h(tn)}{h(n)}=1\\quad(t&gt;0).\n\\]\n\n\n\n\n\n4.2.2 自己相似性\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) は自己相似性を持つ．\n一般に，Hurst 指数 \\(H&gt;0\\) に関して自己相似的 (self-similar) であるとは，任意の \\(a&gt;0\\) について \\[\n(Y_{at})\\overset{\\text{d}}{=}(a^HY_t)\n\\] を満たすことをいう．\n安定指数 \\(\\alpha\\) を持つ回転対称な安定分布 \\(Y\\) については，\\(H=\\alpha^{-1}\\) と取れる．\nBrown 運動は \\(H=1/2\\) について自己相似である．\nまた，自己相似な Lévy 過程は，狭義の安定過程に限る．33\n\n\n\n4.3 安定従属過程\n\\(\\alpha\\in(0,1)\\) の安定指数を持つ安定過程は，従属過程になる．34\n\n\n\n\n\n\n例（Lévy 従属過程）35\n\n\n\n\n\nLévy 分布 \\(\\mathrm{Levy}(c):=\\mathrm{IG}(c^{1/2},0)\\) とは，密度 \\[\nf(x;c):=\\sqrt{\\frac{c}{2\\pi}}x^{-\\frac{3}{2}}e^{-\\frac{c}{2x}}1_{\\mathbb{R}^+}(x)\n\\] を持つ \\(\\mathbb{R}\\) 上の分布をいう．\nこれは次の特性関数を持ち，安定指数 \\(\\alpha=1/2\\) を持つ非対称な安定分布である： \\[\n\\varphi(u)=\\exp\\left(-\\sqrt{c\\lvert u\\rvert}\\biggr(1-i\\operatorname{sgn}(u)\\biggl)\\right).\n\\]\n安定指数 \\(1/2\\) の安定従属過程 \\(T\\) は Lévy 従属過程 とも呼ばれ， \\[\nT_t\\sim\\mathrm{Levy}(t^2/2)\n\\] を満たす．\nこれは，１次元 Brown 運動の到達時刻 \\[\nT_t:=\\inf\\left\\{s&gt;0\\mid B_s=\\frac{t}{\\sqrt{2}}\\right\\}\n\\] の過程として現れる．\nLévy 過程は逆正規過程の特殊な場合であり，これは一般の Gauss 過程の到達時刻の過程として現れる．36\n\n\n\n\n\n\n\n\n\n例（安定従属過程による従属操作）37\n\n\n\n\n\n\\(\\{\\tau_t\\}_{t\\in\\mathbb{R}_+}\\) を安定指数 \\(\\alpha\\in(0,1)\\) を持つ安定従属過程とする．\nこれと独立な Lévy 過程 \\(X\\) に対して，従属化 \\[\nt\\mapsto X_{\\tau_t}\n\\] は再び Lévy 過程である．\n特に，\\(X\\) を Brown 運動 \\(B\\) とすると，\\(B_{\\tau}\\) は安定指数 \\(2\\alpha\\) を持つ安定過程になる．\n例えば \\(\\tau_a\\) として \\[\nT_a:=\\inf\\left\\{t\\in\\mathbb{R}_+\\mid B_t=a\\right\\}\n\\] と取ると，これは安定指数 \\(1/2\\) を持つ安定従属過程（Lévy 従属過程）の修正である．38\nこれより，各 \\(a\\in\\mathbb{R}_+\\) への到達時刻で止めた Brown 運動の過程 \\(a\\mapsto B_{T_{a+}}\\) は対称な Cauchy 過程になる．\n\n\n\n\n\n4.4 Cauchy 過程\nCauchy 過程は安定指数 \\(\\alpha=1\\) を持つ狭義の対称安定過程である．39\n拡散項を持たないが，Lévy 測度は平均を持たず（命題 4.1.2），C 型の Lévy 過程である．\nすなわち，殆ど確実に，任意の区間上で有界変動でない．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#終わりに",
    "href": "posts/2024/Process/Levy.html#終わりに",
    "title": "Lévy 過程を見てみよう",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nLévy 測度 \\[\n\\int_{\\mathbb{R}^d}(\\lvert u\\rvert^2\\land1)\\,\\nu(du)&lt;\\infty\n\\] に関して最も興味深いのは，跳躍測度の焦点になるのは，裾の重さではなくて極小の跳躍の量であるということである．\n裾とは別に，極小の跳躍の和が発散するかどうかが B 型と C 型を分ける．"
  },
  {
    "objectID": "posts/2024/Process/Levy.html#footnotes",
    "href": "posts/2024/Process/Levy.html#footnotes",
    "title": "Lévy 過程を見てみよう",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Nualart and Nualart, 2018, p. 158) 定義9.1.1，(Sato, 2013, p. 3) 定義1.6，(Rocha-Arteaga and Sato, 2019, pp. 12–13) 定義1.31に倣った．(Protter, 2005, p. 20) では(2)の\\(D\\)-過程という部分がないのみで，定理30 (Protter, 2005, p. 21) で常に\\(D\\)-修正が取れることを示している．(Le Gall, 2016, p. 175) 6.5.2節も同様の取り扱いである．(伊藤清, 1991, p. 306) は時間的一様性を所与のものとはせず，(1), (2), (3), (5)のみをLévy過程の定義としており，さらに(4)も満たすものを 一様Lévy過程 という．(Baudoin, 2014, pp. 89–90) 定義3.40では(5)がない．(Böttcher et al., 2013, p. 14) 例1.17では(1),(2)がない．(Osswald, 2012, pp. 258–259) は(1), (3), (4)を定義としている．(Applebaum, 2009, p. 43) は(1), (3), (4), (5)を定義としている．(佐藤健一, 1990) では全く同じものを加法過程と呼ぶが，(佐藤健一, 2011) は完全に一致する語用法をする（加法過程に確率連続性を課している点を除く）．↩︎\n(Sato, 2013, p. 47) 定理9.1 参照．↩︎\n(Sato, 2013, p. 51) 定理9.7参照．↩︎\n(Dudley, 2002, p. 327) 定理9.8.3，(Sato, 2013, p. 37) 定理8.1，(Rocha-Arteaga and Sato, 2019, p. 11) 定理1.28，(Baudoin, 2014, p. 91) 定理3.46，(Applebaum, 2009, p. 29) 定理1.2.14 など参照．↩︎\nGauss 共分散の用語は (Sato, 2013, p. 38) 定義8.2．Khintchine 測度は (Loéve, 1977, p. 343)，(Applebaum, 2009, p. 31)，(Böttcher et al., 2013, p. 33)，(Baudoin, 2014, p. 92) など．↩︎\n(Loéve, 1977, p. 343)↩︎\n(Sato, 2013, p. 39) 注8.4．↩︎\n特性測度の名前は (Revuz and Yor, 1999, p. 478) 演習 XII.1.18 など．命題は (Sato, 2013, p. 53) 注9.9も参照．↩︎\n(Sato, 2013, p. 120) 定理19.2より．(Protter, 2005, p. 31) 定理42 は Lévy 過程に限って示している．(1)は (伊藤清, 1991, p. 313) 補題5.3でも解説されている．(Protter, 2005, p. 26)定理35も参照．↩︎\nこの分類は (Sato, 2013, p. 65) 定義11.9に倣った．↩︎\n(Sato, 2013, p. 140) 定理21.9 参照．↩︎\n(Sato, 2013, p. 135) 定理21.1．↩︎\n(Lowther, 2011) 定理１，(Sato, 2013, p. 135) 定理21.2．↩︎\n(Sato, 2013, p. 136) 定理21.3．↩︎\n(Applebaum, 2009, p. 52)，(Baudoin, 2014, p. 95) 定義3.50，(Sato, 2013, p. 137) 定義21.4，(Iacus and Yoshida, 2018, p. 171) に倣った．(Kingman, 1992, p. 88) 8.4節，(Last_Penrose2017?) 例15.7 は命題の条件2の方を定義に用いている．↩︎\n(Sato, 2013, p. 137) 定理21.5．↩︎\n(Sato, 2013, p. 138) も参照．↩︎\n(Lowther, 2011) 定理２，(Sato, 2013, p. 140) 定理21.9．↩︎\n定義は (Last_Penrose2017?) 例15.6 に倣った．↩︎\n(Ghosal-vanderVaart17-NonparametricBayes?) 命題G.2.(i)，(Last_Penrose2017?) 演習15.1，(Kingman, 1992, pp. 92–) 9.2節．↩︎\n\\(n=2\\) を取ると１単体（線分），\\(n=3\\) と取ると２単体（三角形）を得る．↩︎\n(Kingman, 1992, p. 93)，(Ghosal-vanderVaart17-NonparametricBayes?) 定義4.1．↩︎\n\\(\\lambda(B)=\\rho_0(B)\\nu(\\mathbb{R})=\\infty\\) となるためである．(Last_Penrose2017?) 演習15.2も参照．↩︎\n(Kingman, 1992, p. 88) 8.4節，(Last_Penrose2017?) 例15.7 などの用語法．一般に subordinator とは，単調増加な Lévy 過程をいう (Sato, 2013, p. 137) 定義21.4，(Baudoin, 2014, p. 95) 定義3.50，(Iacus and Yoshida, 2018, p. 171)．これは，時間変数に関する変数変換を subordination と呼び，その際の変数変換に使えるためである．↩︎\n記法は (Iacus and Yoshida, 2018) による．(Applebaum, 2009, pp. 54–55) 例1.3.22，(Protter, 2005, p. 33) 例４も参照．↩︎\n(Iacus and Yoshida, 2018, p. 160) に倣った．↩︎\n(Lowther, 2011)，(Applebaum, 2009, p. 59) 例1.3.31 も参照．↩︎\n(Revuz and Yor, 1999, p. 116) 定義III.4.1，(Sato, 2013, p. 69) 定義13.1，(Shiryaev, 2016, p. 416) 定義3.6.2，(Loéve, 1977, p. 338)．↩︎\n(Shiryaev, 2016, p. 416) 定理3.6.3 も参照．↩︎\n(Sato, 2013, p. 80) 命題14.5．↩︎\n(Sato, 2013, p. 86) 定理14.14．(Shiryaev, 2016, p. 419) 定理3.6.4，(Loéve, 1977, p. 339)，(Dudley, 2002, p. 328) 定理9.8.4 は \\(d=1\\) の場合．↩︎\n(Ibragimov and Linnik, 1971, p. 316) 定理18.1.1 も参照．↩︎\n狭義の安定過程とは，\\(b_n\\equiv0\\) と取れることをいう (Sato, 2013, p. 69) 定義13.1．(Embrechts and Maejima, 2002)，(Applebaum, 2009, p. 51) 例1.3.14 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Sato, 2013, p. 138) 例21.7，(Applebaum, 2009, p. 53) 例1.3.18 も参照．↩︎\n(Applebaum, 2009 @/53) 例1.3.19 も参照．↩︎\n(Applebaum, 2009, p. 54) 例1.3.21 も参照．↩︎\n(Revuz and Yor, 1999, p. 116)，(Rogers and Williams, 2000, p. 133) も参照．↩︎\n(Revuz and Yor, 1999, p. 107) 命題III.3.9 も参照．↩︎\n(Sato, 2013, p. 87) 例14.17．↩︎"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html",
    "href": "posts/2024/Process/Discretization.html",
    "title": "確率過程の離散化",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n(Jacod and Protter, 2012) 第１章参考．\n参照過程は Brown 運動 \\((W_t)_{t\\in\\mathbb{R}_+}\\) のスケーリング \\[\nX=\\sigma W,\\quad(\\sigma&gt;0)\n\\] であるとする．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "1 正規化汎函数 \\(V^{'n}(f,X)\\)",
    "text": "1 正規化汎函数 \\(V^{'n}(f,X)\\)\n\n1.1 \\(t\\in\\mathbb{R}_+\\) 毎の収束\n\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は独立同分布であるが，正規化を施したことにより， \\[\n\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}=\\frac{X_{i\\Delta_n}-X_{(i-1)\\Delta_n}}{\\sqrt{\\Delta_n}}\\sim\\mathrm{N}(0,c)\n\\] も離散化の段階 \\(n=0,1,\\cdots\\) に依らず独立同分布である．よって， \\[\nf\\left(\\frac{\\Delta^n_iX}{\\sqrt{\\Delta_n}}\\right)\\sim(\\rho_c(f),\\rho_c(f^2)-\\rho_c(f)^2)\n\\] を踏まえて，独立同分布列に対する０次と１次の漸近定理から \\[\nV^{'n}(f,X)_t\\overset{\\text{p}}{\\to}t\\rho_c(f)\n\\] \\[\n\\frac{V^{'n}(f,X)_t-t\\rho_c(f)}{\\sqrt{\\Delta_n}}\\overset{\\text{d}}{\\to}\\mathrm{N}\\biggr(0,t(\\rho_c(f^2)-\\rho_c(f)^2)\\biggl)\n\\] が言えそうである．\n\n０次の漸近論で概収束は示せない．\n\n\n\n1.2 \\(\\mathbb{R}_+\\) 上の過程としての収束\n\\(\\mathbb{R}_+\\) で添字付けられた過程として，\\(D(\\mathbb{R}_+)\\) 上の Skorohod 位相について確率収束する．すなわち，任意の \\(t\\in[0,T]\\) に対して， \\[\n\\sup_{s\\le t}\\lvert Z^n_s-Z_s\\rvert\\overset{\\text{p}}{\\to}0.\n\\] 加えて，汎函数中心極限定理から， \\[\n\\left(\\frac{1}{\\sqrt{\\Delta_n}}(V^{'n}(f,X)_t-t\\rho_c(f))\\right)_{t\\ge0}\\overset{\\text{d}}{\\to}\\sqrt{\\rho_c(f^2)-\\rho_c(f)^2}B.\n\\] が Skorohod 位相に関して成り立つ．これはさらに安定収束もするのである．"
  },
  {
    "objectID": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "href": "posts/2024/Process/Discretization.html#非正規化汎函数-vnfx",
    "title": "確率過程の離散化",
    "section": "2 非正規化汎函数 \\(V^n(f,X)\\)",
    "text": "2 非正規化汎函数 \\(V^n(f,X)\\)\n正規化を施さないために，\\(\\Delta^n_iX\\;(i=1,2,\\cdots)\\) は \\(0\\) に漸近していき，関数 \\(f\\) の \\(0\\) での局所的な振る舞いが収束に影響を与えるようになる．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html",
    "href": "posts/2024/Process/PureJump1.html",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "",
    "text": "YUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#関連記事",
    "href": "posts/2024/Process/PureJump1.html#関連記事",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "",
    "text": "YUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#はじめに",
    "href": "posts/2024/Process/PureJump1.html#はじめに",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "1 はじめに",
    "text": "1 はじめに\n本稿では Lévy 過程 \\(\\{Z_t\\}\\) に駆動された SDE \\[\ndX_t=a(X_t)\\,dt+b(X_{t-})\\,dZ_t=a(X_t)\\,dt+\\sigma(X_t)\\,dW_t+\\int_{\\left\\{\\lvert u\\rvert\\le 1\\right\\}}c(X_{t-},u)\\,\\widetilde{N}(dtdu)+\\int_{\\left\\{\\lvert u\\rvert&gt;1\\right\\}}c(X_{t-},u)\\,N(dtdu)\n\\tag{1}\\] で駆動される確率過程 \\(\\{X_t\\}\\) のエルゴード性を議論する．\nLévy 過程 \\(\\{Z_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^m)\\) は拡散項を持たない \\(\\sigma\\equiv0\\) とし，跳躍係数 \\(c:\\mathbb{R}^m\\times\\mathbb{R}^m\\to M_m(\\mathbb{R})\\) も跳躍幅 \\(u\\) に関して線型 \\(c(x,u)=b(x)u\\) で，次の Ito-Lévy 分解を持つとする： \\[\nZ_t=\\int^t_0\\int_{\\lvert u\\rvert\\le1}u\\widetilde{N}(ds,du)+\\int^t_0\\int_{\\lvert u\\rvert&gt;1}uN(ds,du),\n\\] \\[\nN(ds,du):=ds\\,\\nu(du),\\qquad\\widetilde{N}(ds,du):=N(ds,du)-ds\\,\\nu(du),\\qquad\\int_{\\mathbb{R}^m}\\lvert u\\rvert^2\\land1\\nu(du)&lt;\\infty.\n\\] ただし \\(N\\) を強度測度 \\(ds\\,\\nu(du)\\) を持つ跳躍を表す Poisson 点過程とした．\nBorel 可測関数 \\(b:\\mathbb{R}^m\\to M_{m}(\\mathbb{R})\\) と \\(a:\\mathbb{R}^m\\to\\mathbb{R}^m\\) は局所 Lipschitz 連続で，線型増大条件 \\[\n\\lvert a(x)\\rvert^2+\\int_{\\lvert u\\rvert\\le1}\\lvert b(x)u\\rvert^2\\nu(du)\\le K(1+\\lvert x\\rvert^2),\\qquad x\\in\\mathbb{R}^m,\n\\] を満たすとする．このとき，SDE (1) には一意な強解 \\(\\{X_t\\}\\) が存在し，\\(X\\) は càdlàg な Markov 過程である．1\n加えて，\\(b\\) が有界であるという条件も引き続き課すこととする．\n伊藤の公式より，拡張生成作用素 \\[\n\\widehat{L}f(x):=\\left(Df(x)\\,|\\,a(x)\\right)+\\int_{\\mathbb{R}^m}\\biggr(f\\biggr(x+b(x)u\\biggl)-f(x)-1_{B^m}\\biggr((Df(x)|b(x)u)\\biggl)\\biggl)\\nu(du),\\qquad f\\in C^2(\\mathbb{R}^m),\n\\tag{2}\\] に関して \\(M_t^f:=f(X_t)-f(x)-\\int^t_0\\widehat{L}f(X_s)ds\\) で定まる càdlàg 過程 \\(\\{M^f_t\\}\\) は任意の \\(x\\in\\mathbb{R}^m\\) に関して局所 \\(\\operatorname{P}_x\\)-マルチンゲールである．\n拡散過程，例えば Langevin 動力学のエルゴード性証明\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\nNo matching items\n\n\nとの最大の違いは，ドリフト関数 \\(V\\in C^2(\\mathbb{R}^m)\\) に Lévy 測度 \\(\\nu\\) に関する可積分条件が加わることにある．そもそも \\(\\widehat{L}V\\) が well-defined であるためには，式 (2) の積分が発散してはならないのである．\nこのように \\(\\widehat{L}f(x)\\) の値が \\(f\\) の \\(x\\in\\mathbb{R}^m\\) 以外での値にも依存する性質を 非局所性 という．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#文献紹介",
    "href": "posts/2024/Process/PureJump1.html#文献紹介",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "2 文献紹介",
    "text": "2 文献紹介\n\nLévy 過程のエルゴード性の結果については，(3.4節 Kulik, 2018) によくまとまっている．"
  },
  {
    "objectID": "posts/2024/Process/PureJump1.html#footnotes",
    "href": "posts/2024/Process/PureJump1.html#footnotes",
    "title": "Lévy 過程に駆動される SDE のエルゴード性",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Ikeda and Watanabe, 1981, p. 245) 定理9.1．↩︎"
  },
  {
    "objectID": "posts/2025/Publications/Sticky.html",
    "href": "posts/2025/Publications/Sticky.html",
    "title": "動き出す次世代サンプラー・区分確定的モンテカルロ",
    "section": "",
    "text": "この研究の着想は極めて自然なものだと感じていた．実際 (Chevallier ほか, 2023) でも，Sticky PDMP ではないが，可逆ジャンプ PDMP が「極限として得られる」ことのシミュレーション上の言及がある：\n\n\n\n(Chevallier ほか, 2023, p. 2918)\n\n\nSticky PDMP (Bierkens ほか, 2023) や可逆ジャンプ PDMP がある種の極限過程としてみなせることは誰もが直感的に捉えていたことであろう．\n新年に入り，指導教員と初めての研究ディスカッションをやってみようという初回であった１月15日のことであった．すでに Forward ECMC (Michel ほか, 2020) の解析について１時間半ほど議論し，方針が見えてきたところだったが，「研究として面白いだろうが，計算に時間がかかるだろう．より手早くできるテーマをもう一つ考えた方が良いかもしれない」という言葉をもらった．\nほとんど終わりかけの雑談のようなつもりで，Sticky PDMP は明らかに連続 spike-and-slab 分布の slab 幅に関する収束極限なのであるから，これを厳密に導出したいと前から思っていたことを話した．\nすると思いがけず，これが Multiscale Analysis (Pavliotis と Stuart, 2008) の例であることが瞬時に指摘された．\nその集は特別研究員や海外派遣プログラム，学生研究発表会などたくさんの申請・書類提出で忙しかったが，すぐに Sticky PDMP は必ずしも極限にならないことが理解された．極限ならば可逆になるはずである．\n\n\n\n\n参考文献\n\nBierkens, J., Grazzi, S., Meulen, F. van der, と Schauer, M. (2023). Sticky PDMP Samplers for Sparse and Local Inference Problems. Statistics and Computing, 33(1), 8.\n\n\nChevallier, A., Fearnhead, P., と Sutton, M. (2023). Reversible Jump PDMP Samplers for Variable Selection. Journal of the American Statistical Association, 118(544), 2915–2927.\n\n\nMichel, M., Durmus, A., と Sénécal, S. (2020). Forward Event-Chain Monte Carlo: Fast Sampling by Randomness Control in Irreversible Markov Chains. Journal of Computational and Graphical Statistics, 29(4), 689–702.\n\n\nPavliotis, G. A., と Stuart, A. M. (2008). Multiscale Methods: Averaging and Homogenization. Springer New York.\n\n引用BibTeX@online{博文2025,\n  author = {博文, 司馬},\n  title = {動き出す次世代サンプラー・区分確定的モンテカルロ},\n  date = {2025-02-17},\n  url = {https://162348.github.io//posts/2025/Publications/Sticky.html},\n  langid = {ja}\n}\n引用方法\n博文司馬. (2025, February 17). 動き出す次世代サンプラー・区分確定的モンテカルロ."
  },
  {
    "objectID": "posts/2025/Nonparametrics/NRegression.html",
    "href": "posts/2025/Nonparametrics/NRegression.html",
    "title": "ノンパラメトリック回帰分析",
    "section": "",
    "text": "自乗残差最小化の視点から\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n回帰モデルの概観\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI データを題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/Nonparametrics/NRegression.html#ノンパラメトリック回帰",
    "href": "posts/2025/Nonparametrics/NRegression.html#ノンパラメトリック回帰",
    "title": "ノンパラメトリック回帰分析",
    "section": "1 ノンパラメトリック回帰",
    "text": "1 ノンパラメトリック回帰\n\n1.1 カーネル密度推定量 (KDE)\nデータ \\(\\{x_n\\}\\subset\\mathcal{X}\\) と半正定値核 \\(K\\) に対して， \\[\np(x|\\{x_n\\})=\\frac{1}{N}\\sum_{n=1}^NK_\\ell(x,x_n)\n\\] は再び半正定値核である．これを Parzen 窓推定量 または カーネル密度推定量 という．\nこれはデータの経験分布と確率核 \\(K\\) との畳み込みになっている．\\(K\\) として Gauss 核を用いると，これはデータ分布の軟化として使え，デノイジングスコアマッチングなどに応用を持つ．\nただし，\\(\\ell\\) は 幅 (bandwidth) とよばれるハイパーパラメータである．例えば \\(K\\) が動径 \\(r\\) の関数であるとき， \\[\nK_\\ell(r):=\\frac{1}{\\ell}K\\left(\\frac{r}{\\ell}\\right)\n\\] などと導入できる．\n\n\n1.2 カーネル回帰\nデータが \\(\\mathcal{D}=\\{(x_i,y_i)\\}_{i=1}^n\\) という形で与えられ，平均 \\(\\operatorname{E}[Y|X,\\mathcal{D}]\\) を推定することを考える．\nこの際，まず結合密度を次の形で推定する： \\[\np(y,x|\\mathcal{D})=\\frac{1}{n}\\sum_{i=1}^nK_\\ell(x,x_i)K_\\ell(y,y_i)\n\\] これを用いると，次のように平均が推定できる： \\[\n\\operatorname{E}[Y|X,\\mathcal{D}]=\\int_{\\mathcal{Y}} yp(y|X,\\mathcal{D})\\,dy=\\sum_{i=1}^ny_iw_i(x),\\qquad w_i(x):=\\frac{K_\\ell(x,x_i)}{\\sum_{j=1}^nK_\\ell(x,x_j)}.\n\\]\nこの手続きを，カーネル回帰 / カーネル平滑化，または回帰関数に関する (Nadaraya, 1964)-(Watson, 1964) 推定量という．\n\n\n1.3 局所線型回帰 (LLR)\nカーネル回帰では \\(\\operatorname{E}[Y|X,\\mathcal{D}]\\) を，\\(\\{y_i\\}\\) の適切な線型和として予測していた．実は \\[\n\\sum_{i=1}^ny_iw_i(x)=\\min_\\beta\\sum_{i=1}^n(y_i-\\beta)^2K_\\ell(x,x_i)\n\\] の解として特徴付けられる．\n代わりに， \\[\n\\mu(x):=\\min_{\\beta}\\sum_{i=1}^n\\biggr(y_i-\\beta^\\top\\phi(x_i)\\biggl)^2K_\\ell(x,x_i)\n\\] によって \\(\\operatorname{E}[Y|X,\\mathcal{D}]\\) を予測することを，局所線型回帰 (LLR: locally linear regression) または LOWESS (Locally Weighted Scatterplot Smoothing) (Cleveland, 1979), (Cleveland and Devlin, 1988)，または Savitsky-Golay フィルター (Savitzky and Golay, 1964) という．"
  },
  {
    "objectID": "static/AllCategories.html#probability",
    "href": "static/AllCategories.html#probability",
    "title": "Categories",
    "section": "1.1 Probability",
    "text": "1.1 Probability\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\nProbability\n\n\nStatistics\n\n\n\n\n2024-12-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\nProbability\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n2023-11-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\nProbability\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#process",
    "href": "static/AllCategories.html#process",
    "title": "Categories",
    "section": "1.2 Process",
    "text": "1.2 Process\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程に駆動される SDE のエルゴード性\n\n\nカップリング法／最適輸送距離による証明\n\n\n\nProcess\n\n\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n\n2024-07-05\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度のカップリング\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n\n2024-03-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\nマルチンゲール問題\n\n\n\nProcess\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n確率過程の離散化\n\n\n\nProcess\n\n\n\n\n2024-01-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\nProcess\n\n\n\n\n2023-12-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#functional-analysis",
    "href": "static/AllCategories.html#functional-analysis",
    "title": "Categories",
    "section": "1.3 Functional Analysis",
    "text": "1.3 Functional Analysis\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\nFunctional Analysis\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\nFunctional Analysis\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\nFunctional Analysis\n\n\n\n\n2023-12-02\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#geometry",
    "href": "static/AllCategories.html#geometry",
    "title": "Categories",
    "section": "1.4 Geometry",
    "text": "1.4 Geometry\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n最適化手法\n\n\n確率的最適化\n\n\n\nGeometry\n\n\n\n\n2024-02-16\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mathcalpx",
    "href": "static/AllCategories.html#mathcalpx",
    "title": "Categories",
    "section": "1.5 \\(\\mathcal{P}(X)\\)",
    "text": "1.5 \\(\\mathcal{P}(X)\\)\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#nature",
    "href": "static/AllCategories.html#nature",
    "title": "Categories",
    "section": "2.1 Nature",
    "text": "2.1 Nature\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#mcmc",
    "href": "static/AllCategories.html#mcmc",
    "title": "Categories",
    "section": "2.2 MCMC",
    "text": "2.2 MCMC\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-12-15\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#foundation",
    "href": "static/AllCategories.html#foundation",
    "title": "Categories",
    "section": "2.3 Foundation",
    "text": "2.3 Foundation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論３\n\n\n構造的リスク最小化\n\n\n\nFoundation\n\n\n\n\n2024-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論２\n\n\nPAC-Bayes\n\n\n\nFoundation\n\n\n\n\n2024-03-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n\n2024-01-10\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\nProbability\n\n\nFoundation\n\n\n\n\n2023-11-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#information",
    "href": "static/AllCategories.html#information",
    "title": "Categories",
    "section": "2.4 Information",
    "text": "2.4 Information\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#statistics",
    "href": "static/AllCategories.html#statistics",
    "title": "Categories",
    "section": "2.5 Statistics",
    "text": "2.5 Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nノンパラメトリック回帰分析\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n\n2024-12-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\nProbability\n\n\nStatistics\n\n\n\n\n2024-12-23\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\nBayesian\n\n\nStatistics\n\n\nPDMP\n\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-16\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-12-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析８\n\n\n正規グラフィカルモデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#computation-2",
    "href": "static/AllCategories.html#computation-2",
    "title": "Categories",
    "section": "3.1 Computation",
    "text": "3.1 Computation\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-12-15\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nイベント連鎖モンテカルロ法\n\n\n数学者のための統計力学４：物理過程から離陸した Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#sampling",
    "href": "static/AllCategories.html#sampling",
    "title": "Categories",
    "section": "3.2 Sampling",
    "text": "3.2 Sampling\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger-Föllmer サンプラーとは何か？\n\n\nSchrödinger 橋をサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchrödinger 橋によるサンプリング\n\n\n拡散モデルによるベイズ計算\n\n\n\nSampling\n\n\nProcess\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n分布道の学習としての生成モデリング\n\n\nDenoising Diffusion から Schrödinger Bridge へ\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n2023-11-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#python",
    "href": "static/AllCategories.html#python",
    "title": "Categories",
    "section": "3.3 Python",
    "text": "3.3 Python\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n\n2024-10-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とそのエントロピー緩和\n\n\nIterative Proportional Fitting / Sinkhorn-Knopp Algorithm\n\n\n\nComputation\n\n\nP(X)\n\n\nPython\n\n\n\n\n2024-03-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\nPython の import について\n\n\n\nPython\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#julia",
    "href": "static/AllCategories.html#julia",
    "title": "Categories",
    "section": "3.4 Julia",
    "text": "3.4 Julia\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nPDMP\n\n\n\n\n2024-12-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-10-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\n\n2024-10-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\nHamiltonian Monte Carlo 法\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis-Hastings サンプラー\n\n\nJulia と Turing エコシステムを用いて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）パッケージ作成とモジュール\n\n\nモジュールとパッケージ\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#yuima",
    "href": "static/AllCategories.html#yuima",
    "title": "Categories",
    "section": "3.5 YUIMA",
    "text": "3.5 YUIMA\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#stan",
    "href": "static/AllCategories.html#stan",
    "title": "Categories",
    "section": "3.6 Stan",
    "text": "3.6 Stan\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\nOrnstein-Uhlenbeck 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#r",
    "href": "static/AllCategories.html#r",
    "title": "Categories",
    "section": "3.7 R",
    "text": "3.7 R\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\n\n2024-06-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（１）基本文法\n\n\n基本パッケージとその文法\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR（５）統計処理\n\n\n\nComputation\n\n\nR\n\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#bayesian",
    "href": "static/AllCategories.html#bayesian",
    "title": "Categories",
    "section": "4.1 Bayesian",
    "text": "4.1 Bayesian\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-12-22\n\n\n\n\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\nBayesian\n\n\nStatistics\n\n\nPDMP\n\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n連続・離散を往来する MCMC サンプラー\n\n\nZig-Zag within Gibbs という考え方\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたノンパラメトリック回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-16\n\n\n\n\n\n\n\n\n\n\n\n\n階層ベイズ理想点解析\n\n\nPDMP サンプラーによる特異項目機能を取り込んだ大規模ベイズ推定\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-12-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析８\n\n\n正規グラフィカルモデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-12-05\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nComputation\n\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n大規模モデル選択のための非可逆 MCMC 法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 Zig-Zag サンプラー\n\n\n点呼投票データでのハンズオン\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-10-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-09-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ階層多ハザードモデル\n\n\nZig-Zag サンプラーによるモデル平均法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n\n2024-06-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ機械学習１\n\n\nドロップアウト\n\n\n\nBayesian\n\n\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n\n2024-02-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n\n2024-01-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#particles",
    "href": "static/AllCategories.html#particles",
    "title": "Categories",
    "section": "4.2 Particles",
    "text": "4.2 Particles\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nAdvancedPS.jl パッケージ\n\n\nTuring エコシステムにおける粒子フィルター\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-10-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの連続極限\n\n\nどんな過程が現れるか？\n\n\n\nParticles\n\n\nProcess\n\n\n\n\n2024-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装\n\n\nリサンプリング編\n\n\n\nParticles\n\n\nJulia\n\n\n\n\n2024-01-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#kernels",
    "href": "static/AllCategories.html#kernels",
    "title": "Categories",
    "section": "4.3 Kernels",
    "text": "4.3 Kernels\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n\n2024-08-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法１\n\n\nカーネル平均埋め込み\n\n\n\nKernel\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための Support Vector Machine 概観\n\n\n\nKernel\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#ai",
    "href": "static/AllCategories.html#ai",
    "title": "Categories",
    "section": "4.4 AI",
    "text": "4.4 AI\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論４\n\n\nドメイン汎化と転移学習\n\n\n\nAI\n\n\nFoundation\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n強化学習\n\n\n\nAI\n\n\n\n\n2024-02-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#deep-learning",
    "href": "static/AllCategories.html#deep-learning",
    "title": "Categories",
    "section": "4.5 Deep Learning",
    "text": "4.5 Deep Learning\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-20\n\n\n\n\n\n\n\n\n\n\n\n\n拡散埋め込み | Diffusion Map\n\n\nこれからの多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\n\n\n2024-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-08-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上のフローベース模型\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\nGAN の実装\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n\n2024-07-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n\n2024-07-29\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n\n2024-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\n\n\n\n\n大規模言語モデル\n\n\nMistral AI を用いた\n\n\n\nDeep\n\n\nPython\n\n\nAI\n\n\n\n\n2024-03-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\n\n2024-03-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n\n2024-02-20\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#posters",
    "href": "static/AllCategories.html#posters",
    "title": "Categories",
    "section": "5.1 Posters",
    "text": "5.1 Posters\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nベイズ変数選択の計算的解決\n\n\nPDMP による非可逆ジャンプの達成\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-02-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#reviews",
    "href": "static/AllCategories.html#reviews",
    "title": "Categories",
    "section": "5.2 Reviews",
    "text": "5.2 Reviews\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2016) Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-06-05\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (2001) Optimal Scaling for Various Metropolis-Hastings Algorithms\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-05-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Rosenthal (1998) Optimal Scaling of Discrete Approximations to Langevin Diffusions\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\n\n2024-04-04\n\n\n\n\n\n\n\n\n\n\n\n\nDai+ (2019) Monte Carlo Fusion\n\n\n論文メモ\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\nFearnhead+ (2017) Continuous-time Importance Sampling: Monte Carlo Methods which Avoid Time-Discretization Error\n\n\n連続時間重点サンプリング：時間離散化誤差を伴わないモンテカルロ法\n\n\n\nReview\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\nReview\n\n\n\n\n2023-11-09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\nReview\n\n\n\n\n2023-11-08\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#surveys",
    "href": "static/AllCategories.html#surveys",
    "title": "Categories",
    "section": "5.3 Surveys",
    "text": "5.3 Surveys\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n\n2024-09-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\nParticles\n\n\nSurvey\n\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体の微細化技術\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-03-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n\n2024-02-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n\n2024-02-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n\n2023-11-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#slides",
    "href": "static/AllCategories.html#slides",
    "title": "Categories",
    "section": "5.4 Slides",
    "text": "5.4 Slides\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総研大５年一貫博士課程・中間評価\n\n\n\nSlide\n\n\n\n\n2025-01-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl Package for the New Era of MCMC\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag Sampler\n\n\nA MCMC Game-Changer\n\n\n\nSlide\n\n\nMCMC\n\n\n\n\n2024-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー\n\n\n物理のくびきを超える MCMC\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\nSurvey\n\n\n\n\n2024-07-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nベイズとは何か\n\n\n数学による統一的アプローチ\n\n\n\nSlide\n\n\n\n\n2024-04-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nSlide\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#papers",
    "href": "static/AllCategories.html#papers",
    "title": "Categories",
    "section": "5.5 Papers",
    "text": "5.5 Papers"
  },
  {
    "objectID": "static/AllCategories.html#opinion",
    "href": "static/AllCategories.html#opinion",
    "title": "Categories",
    "section": "6.1 Opinion",
    "text": "6.1 Opinion\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n2024-12-11\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\nUnreasonable Effectiveness of Measure Theory\n\n\n\nOpinion\n\n\n\n\n2024-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n\n2024-04-06\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n2024-03-20\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\nParticles\n\n\nOpinion\n\n\n\n\n2023-11-06\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#life",
    "href": "static/AllCategories.html#life",
    "title": "Categories",
    "section": "6.2 Life",
    "text": "6.2 Life\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n2024-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n\n2024-05-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\nLife\n\n\n\n\n2023-12-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\n\nLife\n\n\n\n\n2023-12-01\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\nLife\n\n\n\n\n2023-11-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#lifestyle",
    "href": "static/AllCategories.html#lifestyle",
    "title": "Categories",
    "section": "6.3 Lifestyle",
    "text": "6.3 Lifestyle\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\n\n2023-12-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\nLifestyle\n\n\nPython\n\n\n\n\n2023-11-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\nLifestyle\n\n\n\n\n2023-11-04\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#法律家のための統計数理",
    "href": "static/AllCategories.html#法律家のための統計数理",
    "title": "Categories",
    "section": "6.4 法律家のための統計数理",
    "text": "6.4 法律家のための統計数理\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n法律家のための統計数理（？）AI の信頼性\n\n\nアルゴリズムと公平性\n\n\n\n草野数理法務\n\n\nAI\n\n\n\n\n2024-03-10\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（７）刑法入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）多変量解析の基礎\n\n\n教科書第３章第５節から第８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-29\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（？）数理ファイナンス入門\n\n\n教科書第４章 (pp. )\n\n\n\n草野数理法務\n\n\n\n\n2024-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/AllCategories.html#俺のための-julia-入門",
    "href": "static/AllCategories.html#俺のための-julia-入門",
    "title": "Categories",
    "section": "6.5 俺のための Julia 入門",
    "text": "6.5 俺のための Julia 入門\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nJulia による MCMC サンプリング\n\n\n新時代の確率的プログラミング環境の構築に向けて\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n俺のためのJulia入門\n\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（６）メタプログラミング\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2022-01-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（５）パッケージ作成とモジュール\n\n\nモジュールとパッケージ\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-10\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（４）型定義\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-09\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（３）関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-08\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（２）制御\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-07\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（１）データ型\n\n\nデータ型とその上の原始関数\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\n\n2020-09-05\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#現在",
    "href": "static/CV/cv_Japanese.html#現在",
    "title": "司馬博文（しばひろふみ）",
    "section": "1/11/2025 現在",
    "text": "1/11/2025 現在\n総合研究大学院大学（統計科学コース）５年一貫博士課程２年目．\n\n日本語，中国語が母語で，英語も話せる（TOEFL iBT 100点）．Python, R でのコーディング経験３年以上，Julia １年以上．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#研究分野",
    "href": "static/CV/cv_Japanese.html#研究分野",
    "title": "司馬博文（しばひろふみ）",
    "section": "研究分野",
    "text": "研究分野\n\n統計計算と機械学習：MCMC，SMC，PDMC などのモンテカルロ法と，SGD，Evolution Strategies などの最適化法．\n機械学習：拡散模型，強化学習など．\nベイズモデリング：特に政治学・疫学・惑星地球科学などへの応用．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#学歴",
    "href": "static/CV/cv_Japanese.html#学歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "学歴",
    "text": "学歴\n\n博士（統計科学） 総合研究大学院大学先端学術院統計科学コース. 2023.4 – 2028.3\n指導教員：鎌谷研吾 教授\n学士（理学） 東京大学理学部数学科. 2019.4 – 2023.3\n指導教員：吉田朋広 教授"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#職歴",
    "href": "static/CV/cv_Japanese.html#職歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "職歴",
    "text": "職歴\n\n特別研究員. 総合研究大学院大学. 2025.4 – 現在\nJST 国家戦略分野の若⼿研究者及び博⼠後期課程学⽣の育成事業（博⼠後期課程学⽣⽀援） JPMJBS2412\n研究課題名「次世代の「AI 信頼性」に向けた抜本的解決」\nデータ・サイエンティスト. プリメディカ株式会社. 2024.9 – 現在\n予防医療サービス向上のための，ベイズデータ解析手法の開発と応用．\nリサーチ・アシスタント. 統計数理研究所. 2023.7 – 現在\n確率過程の統計推測のための R パッケージである YUIMA の開発，モンテカルロ法の応用．"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#研究滞在",
    "href": "static/CV/cv_Japanese.html#研究滞在",
    "title": "司馬博文（しばひろふみ）",
    "section": "研究滞在",
    "text": "研究滞在\n\nユニバーシティ・カレッジ・ロンドン，イギリス．2024.11.4 – 2024.12.2\n受入教員：Alexandros Beskos 教授\nシンガポール国立大学，シンガポール．2025.6.1 – 2025.6.30\n受入教員：Alexandre Thiéry 准教授\nウルム大学，ドイツ．2025.7.13 – 2025.7.26\n受入教員：Evgeny Spodarev 教授"
  },
  {
    "objectID": "static/English.html",
    "href": "static/English.html",
    "title": "Recent Posts",
    "section": "",
    "text": "Notations | Categories | All Posts\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nImplementation Details of PDMPFlux.jl\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\n\n\n\n\n\nDec 31, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood of Hierarchical Models\n\n\n\n\n\nWe examine how to find & formally determine the likelihood function of hierarchical models. As a real-world example, we consider the ideal point model, also known as the 2-parameter logistic item response model. \n\n\n\n\n\nDec 23, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Identifiability of the Bafumi et. al. Ideal Point Model\n\n\nRethinking of the Hierarchical Model of Bafumi et. al. (2005)\n\n\nIdeal point models are 2-parameter item response model, tailored to the purpose of visualizing / measuring the ideological positions of the legislators / judges. (Bafumi et al., 2005) introduced a hierarchical structure to the model to deal with the problem of identifiability. In this article, we re-examine the model and show that the posterior distribution of the parameters (ideal points) is still bimodal, indicating its weak identifiability. \n\n\n\n\n\nDec 22, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nA Recent Development of Particle Methods\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\nRecently developments in continuous-time MCMC algorithms have emerged as a promising direction for scalable Bayesian computation. This poster explores their SMC counterparts. A new finding about a continuous-time limit of particle filter is discussed.\n\n\n\n\n\nMar 8, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurability of the Minkowski Sum of Two Sets\n\n\n\n\n\nFor two Borel sets \\(A,B\\in\\mathcal{B}(\\mathbb{R}^n)\\), we cannot expect \\(A+B\\) to be always Borel. We give sufficient conditions for the Minkowski sum \\(A+B\\) to be Borel, and also give a concrete counterexample for the case \\(n\\ge3\\).\n\n\n\n\n\nJan 5, 2024\n\n\nHirofumi Shiba\n\n\n\n\n\n\n\n\n\n\n\n\nInfluential Books Which Paved My Path into Mathematics\n\n\nBook Recommendations\n\n\nI will explore how a few books inspired me and paved my way into Mathematics.\n\n\n\n\n\nDec 1, 2023\n\n\nHirofumi Shiba\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nReferences\n\nBafumi, J., Gelman, A., Park, D. K., and Kaplan, N. (2005). Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation. Political Analysis, 13(2), 171–187."
  },
  {
    "objectID": "static/Materials.html",
    "href": "static/Materials.html",
    "title": "Materials",
    "section": "",
    "text": "PDMP と従来の MCMC はどう違い，どう違わないか？\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-08-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非絶対連続分布からのサンプリング\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非可逆ジャンプの達成\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-02-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\nNews\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nNews\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n  \n    \n      9/22/2025.\n      \n        15:00-16:30.\n      \n      司馬博文 .\n      メトロポリスを超えた枠組みで我々はどこまで行けるか？\n      : PDMP と従来の MCMC はどう違い，どう違わないか？.\n      \n        \n          \n            機械学習若手の会 (YAML) 2025.\n          \n        \n      \n      \n        ハートピア熱海.\n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      8/28/2025.\n      \n        17:20-19:00.\n      \n      司馬博文 .\n      サンプリングにおける加速と安定性\n      \n      \n        \n          \n            連続最適化および関連分野に関する夏季学校.\n          \n        \n      \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        13:40-14:20.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非絶対連続分布からのサンプリング.\n      \n        \n          \n            統計数理研究所オープンハウス.\n          \n        \n      \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      3/08/2025.\n      \n        12:00-12:50.\n      \n      司馬博文 .\n      PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能\n      \n      \n        \n          \n            第19回日本統計学会春季集会.\n          \n        \n      \n      \n      \n        （優秀発表賞受賞）\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n        \n           Preprint\n        \n      \n    \n  \n    \n      2/17/2025.\n      \n        15:30-16:30.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非可逆ジャンプの達成.\n      \n        \n          \n            計算技術による学際的統計解析ワークショップ.\n          \n        \n      \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非絶対連続分布からのサンプリング\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#upcomings-newests",
    "href": "static/Materials.html#upcomings-newests",
    "title": "Materials",
    "section": "",
    "text": "PDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-08-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非絶対連続分布からのサンプリング\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非可逆ジャンプの達成\n\n\n\nPDMP\n\n\nPoster\n\n\nNews\n\n\n\n\n2025-02-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\nNews\n\n\nPoster\n\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nParticles\n\n\nComputation\n\n\nPoster\n\n\n\n\n2024-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\nInquiry towards a Continuous Time Limit and Scalability\n\n\n\nNews\n\n\nPoster\n\n\n\n\n2024-02-25\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#presentation-history",
    "href": "static/Materials.html#presentation-history",
    "title": "Materials",
    "section": "",
    "text": "8/28/2025.\n      \n        17:20-19:00.\n      \n      司馬博文 .\n      サンプリングにおける加速と安定性\n      \n      \n        \n          \n            連続最適化および関連分野に関する夏季学校.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        13:40-14:20.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非絶対連続分布からのサンプリング.\n      \n        \n          \n            統計数理研究所オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      3/08/2025.\n      \n        12:00-12:50.\n      \n      司馬博文 .\n      PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能\n      \n      \n        \n          \n            第19回日本統計学会春季集会.\n          \n        \n      \n      \n        （優秀発表賞受賞）\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n        \n           Preprint\n        \n      \n    \n  \n    \n      2/17/2025.\n      \n        15:30-16:30.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非可逆ジャンプの達成.\n      \n        \n          \n            計算技術による学際的統計解析ワークショップ.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/24/2024.\n      \n        10:30-11:10.\n      \n      司馬博文 .\n      新時代の MCMC を迎えるために\n      : 連続時間アルゴリズムへの進化.\n      \n        \n          \n            統数研オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#sec-法律家のための統計数理",
    "href": "static/Materials.html#sec-法律家のための統計数理",
    "title": "Materials",
    "section": "法律家のための統計数理",
    "text": "法律家のための統計数理\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n数学と法学，双方からの交流と理解を図ります．\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#empirical-process-theory",
    "href": "static/Materials.html#empirical-process-theory",
    "title": "Materials",
    "section": "Empirical Process Theory",
    "text": "Empirical Process Theory\n\n\n\n\n\n\n\n\n\n\n\nPeriod\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\nTextbook：Kengo Kato Empirical Process Theory (Lecture Note)\n\n\n\n担当分の発表資料"
  },
  {
    "objectID": "static/Materials.html#総研大特別研究員",
    "href": "static/Materials.html#総研大特別研究員",
    "title": "Materials",
    "section": "総研大特別研究員",
    "text": "総研大特別研究員\n\n\n\n申請書（最終版）"
  },
  {
    "objectID": "static/Materials.html#学振-dc1",
    "href": "static/Materials.html#学振-dc1",
    "title": "Materials",
    "section": "学振 DC1",
    "text": "学振 DC1\n\n\n\n\n\n\n\n\n\n\nPeriod\nApplication Category\nSmall Category\n結果\n\n\n\n\nSpring, 2024\n解析学、応用数学およびその関連分野\n12040 応用数学および統計数学関連\n不採択 AT スコア 2.639（上位３割）\n\n\n\n\n本書類審査セットにおける 2024 年度の採択率は 11.6% でした．\n\n\n\n申請書（最終版，５月19日）\n\n\n\n\n\n参考：申請書（バージョン１，４月３日）\n\n\n\n\n評点結果\n\n\n\n\n\n\n\n着想およびオリジナリティ\n研究者としての資質\n総合評価\n\n\n\n\n3.50\n3.17\n3.17"
  },
  {
    "objectID": "static/Notations.html#sec-set",
    "href": "static/Notations.html#sec-set",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "1 基本的対象",
    "text": "1 基本的対象\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\nここでは，あらゆる数学概念は，ZFC公理系 の下で集合として定義する．1 記号 \\(:=\\) は「右辺によって左辺を定義し，その結果等号が成り立つ」という主張の略記である．2\n\n1.1 集合\n\n\n集合 \\(X\\) の 冪集合 を \\(P(X)\\) で表す．3\n\n\n\n全体集合 \\(X\\) が明白であるとき，補集合を \\(A^\\complement:=X\\setminus A\\) とも表す．\n非交和 \\(A\\sqcup B\\) とは，\\(A\\cap B=\\emptyset\\) が成り立つ際に，直和 \\(A\\coprod B\\) を \\(A\\cup B\\) と同一視したものをいう．4\n対称差 を次で表す：5 \\[\nA\\triangle B:=(A\\setminus B)\\sqcup(B\\setminus A)\n\\]\n有限集合 \\(X\\) の元の数を \\(\\lvert X\\rvert\\) または \\(\\#X\\) で表す．6 即ち，\\(\\#:P(X)\\to[0,\\infty]\\) を 計数測度 とする．\n\\(X\\) の部分集合 \\(A\\) が有限であることを \\(A\\overset{\\text{finite}}{\\subset}X\\) とも略記する．\n特に全体集合 \\(\\Omega\\) が確率空間をなすとき，条件 \\(P\\) を満たすという 事象 \\[\nA:=\\left\\{\\omega\\in\\Omega\\mid P(\\omega)\\right\\}\n\\] を \\(\\left\\{P\\right\\}\\) とも表す．7 例えば関数 \\(X:\\Omega\\to\\mathbb{R}\\) に関して \\(\\{X&lt;a\\}\\) や， \\[\n\\left\\{X\\in A\\right\\}=\\left\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\right\\}\n\\] などの略記を用いる．\n自然数 をその集合論的構成の意味でも使う：8 \\[\n0:=\\emptyset,\\quad 1:=\\{0\\}=0\\cup\\{0\\},\n\\] \\[\n2:=\\{0,1\\}=1\\cup\\{1\\},\n\\] \\[\nn+1:=n\\cup\\{n\\}.\n\\]\n正整数のなす部分集合を表すため，次の記法を用意する：9 \\[\n[n]:=\\{1,\\cdots,n\\}=n+1\\setminus1.\n\\]\n\\(\\mathbb{R}_+\\) で 非負実数 の全体，10 \\(\\mathbb{R}^+\\) で 正実数 の全体がなす集合を表す．部分集合 \\(\\mathbb{Z},\\mathbb{Q}\\subset\\mathbb{R}\\) や \\(\\overline{\\mathbb{R}}:=[-\\infty,\\infty]\\) についても同様．特に \\(\\mathbb{N}:=\\mathbb{Z}_+\\)．11\n\n\n\n\n\n自然数の組を表すため，次の記法を用意する：12 \\[\n1:N:=(1,\\cdots,N).\n\\]\n数学的対象 \\(X_1,\\cdots,X_N\\) の組も次のように略記する：13 \\[\nX_{1:N}:=(X_1,\\cdots,X_N).\n\\]\n\n\n\n1.2 写像\n\\(X,Y\\) を集合，\\(f:X\\to Y\\) を写像とする．\n\n引数のプレイスホルダーとして \\(-\\) や \\(\\cdot\\) を用い，\\(f(-),f(\\cdot)\\) などと表す．\n\\(A\\subset X\\) の 像 を \\(f(A)\\) で表し，これが集合であることを特に明示する際は \\(f_*(A)\\) とも表す．14\n\n\n\n\n部分集合 \\(A\\subset X\\) の 特性関数 を \\(1_A:X\\to2\\) で表す．15\n部分集合 \\(A\\subset X\\) の 指標関数 といった場合は次を表す：16 \\[\n\\sigma_A:=0\\cdot1_A+\\infty\\cdot1_{A^\\complement}.\n\\]\n全射 を \\(f:X\\twoheadrightarrow Y\\)，単射 を \\(f:X\\hookrightarrow Y\\) で強調して表すことがある．17\n全単射 が特に特定の圏での 同型射 でもある場合 \\(f:X\\overset{\\sim}{\\to}Y\\) と強調して表すことがある．\n写像 \\(I\\ni i\\mapsto X_i\\) を 族 とも呼び，\\((X_i)_{i\\in I}\\) と表す．しかしこの写像の値域も 族 と呼び，この場合は \\(\\{X_i\\}_{i\\in I}:=\\mathrm{Im}\\,(X_i)_{i\\in I}\\) と表す．18\n特に \\(I=\\mathbb{N}\\) のときは 列 とも呼ぶ．\\(I\\overset{\\text{finite}}{\\subset}\\mathbb{N}\\) のときは組と同一視する．19\n\n\n\n\n多くの場合，一階の量化記号 \\(\\forall\\) を省略して次のような略記を用いる： \\[\nf(x)\\le g(x)\\quad(x\\in X).\n\\]\n\n\n\n関数 \\(g:\\mathbb{R}\\to\\mathbb{R}\\) に対して \\(O(g(x))\\;(x\\to x_0)\\) とは，条件 \\[\n\\limsup_{x\\to x_0}\\left|\\frac{f(x)}{g(x)}\\right|&lt;\\infty\n\\] を満たす関数 \\(f:\\mathbb{R}^+\\to\\mathbb{R}\\) の全体とする．20\nただし，\\(O(g)\\) はその任意の元を表すとして， \\[\nf(x)=O(g(x))\\quad(x\\to x_0)\n\\] を \\(f(x)\\in O(g(x))\\;(x\\to x_0)\\) の略式で使う．\n同様にして，\\(f(x)=o(g(x))\\;(x\\to x_0)\\) を次の意味でも使う： \\[\n\\lim_{x\\to x_0}\\frac{f(x)}{g(x)}\\to0.\n\\]\n\n\n\n1.3 構成\n\n写像 \\(f:X\\to Y\\) の全体がなす集合を \\(Y^X\\) または \\(\\mathrm{Map}(X,Y)\\) で表す．21\n\n\n\n\n積空間 \\(\\prod_{i\\in I}X_i\\) からの 第 \\(i\\) 射影 を \\(\\mathrm{pr}_i\\) で表す．22\n\\(x\\in X\\) での 評価写像 を \\(\\mathrm{ev}_x:Y^X\\twoheadrightarrow Y\\) で表す．23\n\n\n\n\\(\\mathrm{id}_X\\) で集合 \\(X\\) 上の 恒等写像 を表す：24 \\[\n\\mathrm{id}_X(x)=x\\quad(x\\in X).\n\\]"
  },
  {
    "objectID": "static/Notations.html#sec-space",
    "href": "static/Notations.html#sec-space",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "2 空間",
    "text": "2 空間\n本サイトでの主な舞台は，Banach 空間としての線型・距離・位相構造と，測度空間の構造とを持った空間である．\n\n2.1 位相・順序・距離\n\\((X,\\mathrm{Op}(X))\\) を 位相空間 とする．25\n\n点 \\(x\\in X\\) の（開集合とは限らない） 近傍 のフィルター を \\(\\mathcal{O}(x)\\) で表す．26\n集合 \\(A\\subset X\\) について，\\(A^\\circ\\) で 内部，\\(\\overline{A}\\) で 閉包，\\(\\partial A:=\\overline{A}\\setminus A^\\circ\\) で 境界 を表す．\n\\(U\\in\\mathrm{Op}(X)\\) を \\(U\\overset{\\mathrm{open}}{\\subset}X\\) とも表す．閉集合 \\(F\\overset{\\textrm{closed}}{\\subset}X\\) とコンパクト集合 \\(K\\overset{\\textrm{cpt}}{\\subset}X\\) も同様の略記を用いる．\n\\(n\\)-単体 を次で表す： \\[\n\\Delta^n:=\\left\\{x\\in(\\mathbb{R}_+)^{n+1}\\:\\middle|\\:\\sum_{i=0}^nx^i=1\\right\\}.\n\\]\n束 \\(L\\) の元 \\(a,b\\) に対して，上限と下限を次でも表す．27 \\[\na\\lor b:=\\sup\\{a,b\\},\\quad\na\\land b:=\\inf\\{a,b\\}.\n\\] 例えば \\(\\{\\mathcal{F}_i\\}_{i\\in I}\\) を 集合 \\(X\\) の元がなす \\(\\sigma\\)-代数の族とすると，これらの合併が生成する \\(\\sigma\\)-代数を次で表す：28 \\[\n\\bigvee_{i\\in I}\\mathcal{F}_i:=\\sigma\\left(\\bigcup_{i\\in I}\\mathcal{F}_i\\right).\n\\]\n\\(0\\) を持つ束においては次の略記を使う：29 \\[a_+:=a\\lor0,\\] \\[a_-:=-(a\\land 0).\\]\n順序の入った位相空間の元の列 \\((x_n)\\) について，\\(x_n\\nearrow f\\) とは，収束 \\(x_n\\to f\\) だけでなく，列 \\((x_n)\\) が単調増加であることも含意する．30\n距離空間 \\((T,d)\\) の 開球 を \\(U_\\epsilon(t)=U(t;\\epsilon)\\) で表す．31\n閉球 を \\(B_\\epsilon(t)=B(t;\\epsilon)\\) で表す．32 特に 単位閉球 は \\(B:=B(0;1)\\) で表し，\\(\\mathbb{R}^n\\) のものである場合は \\(B^n\\) とも表す．33 Banach空間 \\(X\\) の双対空間 \\(X^*\\) のものは \\(B^*\\) とも表す．34\n集合 \\(A\\subset T\\) と \\(\\epsilon&gt;0\\) に対して，その \\(\\epsilon\\)-開近傍を次で表す．35 \\[\nA_\\epsilon:=\\left\\{x\\in T\\mid d(x,A)&lt;\\epsilon\\right\\}.\n\\]\n\n以降も，ある記号 \\(\\mathcal{F}\\) に関して \\(\\mathcal{F}(x;y)\\) と表される記法は， \\(\\mathcal{F}_y(x)\\) の別記法と捉える．36\n\n\n2.2 線型空間・ノルム・内積\n\n\\(\\mathbb{F}\\)-線型空間 \\(X\\) の部分集合 \\(A,B\\subset X\\) と数 \\(\\lambda\\in\\mathbb{F}\\) について，次の記法を用いる：37 \\[\n\\begin{align*}\n    A&+B:=\\left\\{a+b\\in X\\mid a\\in A,b\\in B\\right\\},\\\\\n    \\lambda &A:=\\left\\{\\lambda a\\in X\\mid a\\in A\\right\\}.\n\\end{align*}\n\\]\n\n集合 \\(A\\subset X\\) の 凸包 を \\(\\operatorname{Conv}(A)\\) で表す．38\n集合 \\(A\\subset X\\) が生成する部分空間を次で表す：39 \\[\n\\langle A\\rangle:=\\sum_{x\\in A}\\mathbb{F}x.\n\\]\n\n体 \\(\\mathbb{F}\\) の元を成分に持つ \\((m,n)\\)-行列の全体を \\(M_{mn}(\\mathbb{F})\\) で表す．40\n\n\\(n\\)-次の実対称行列の全体を \\(S_n(\\mathbb{R})\\) で表す．41 \\(S_n(\\mathbb{R})_+\\) で半正定値なものの全体を表す．42\n対称行列 \\(A,B\\in S_n(\\mathbb{C})\\) に関して，\\(A\\ge B\\) とは，\\(A-B\\) が半正定値であることとする．43\n\n対角成分に \\(d_1,\\cdots,d_n\\) を持つ \\(n\\)-次正方行列を \\(\\mathrm{diag}(d_1,\\cdots,d_n)\\) で表す：44 \\[\n\\mathrm{diag}(d_1,\\cdots,d_n):=\\begin{pmatrix}d_1&\\cdots&0\\\\\n\\vdots&\\ddots&\\vdots\\\\0&\\cdots&d_n\\end{pmatrix}.\n\\]\n行列 \\(A\\in M_{mn}(\\mathbb{F})\\) の転置を \\(A^\\top\\) で表し，45 共役転置を \\(A^*\\) で表す．46 \\(\\mathbb{F}=\\mathbb{R}\\) の場合は \\(A^\\top=A^*\\)．\n一般に内積を \\((-|-)\\) で表す．47\n行列 \\(A,B\\in M_{mn}(\\mathbb{C})\\) の Hilbert-Schmidt 内積を48 \\[\n\\begin{align*}\n  (B \\,|\\,A)_\\mathrm{HS}&:=\\operatorname{Tr}(A^*B)\\\\\n  &=\\sum_{i=1}^m\\sum_{j=1}^na_{ij}b_{ij}\n\\end{align*}\n\\] Hilbert-Schmidt ノルム を次でも表す．49 \\[\n\\|A\\|_{\\mathrm{HS}}:=\\lvert A\\rvert:=\\sqrt{(A|A)_\\mathrm{HS}}.\n\\]\n任意の集合 \\(J\\) に関して，\\(\\mathbb{R}\\) の Banach 空間としての \\(l^p\\)-直和 を \\(l^p(J)\\) で表し，ノルムを \\(\\|-\\|_p\\) で表す．50 \\(J=\\mathbb{N}\\) のとき，単に \\(l^p\\) とも表す．\n\n特に \\(J\\) が有限であるとき， \\[\n  \\|x\\|_p=\\left(\\sum_{j\\in J}\\lvert x_j\\rvert^p\\right)^{1/p}\\quad(x\\in\\mathbb{R}^{\\lvert J\\rvert})\n  \\] となり，\\(p=2\\) の場合は \\(\\lvert x\\rvert:=\\|x\\|_2\\) とも表す．51\n特に，\\(l^\\infty(J)\\) 上で \\(J\\) 上の有界な関数全体の集合を表す．52\n\\(\\mathbb{R}^n\\) の標準基底を \\(e_1,\\cdots,e_n\\) で表す．53\n\n\n\n\n2.3 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．54\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して，外測度 を次で定める：55 \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\,\\bigg|\\,\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}.\n\\end{align*}\n\\]\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．56\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．57\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2_\\mu:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．58\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．59\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を次で表す：60 \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right).\n\\]\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．61 \\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\(\\mathcal{B}(X):=\\sigma(\\mathcal{O})\\) で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．62\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．63 \\(\\gamma_n:=\\operatorname{N}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．"
  },
  {
    "objectID": "static/Notations.html#sec-kernel",
    "href": "static/Notations.html#sec-kernel",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 核",
    "text": "3 核\n空間を導入した次は，その射を定義せねばなるまい．\n本節では，\\((E,\\mathcal{E})\\) を 可測空間 とする．64\n\n3.1 測度\n\n符号付き測度 とは，可算加法的な関数 \\[\\mu:\\mathcal{E}\\to[-\\infty,\\infty]\\] であって， \\[\\{\\pm\\infty\\}\\subset\\mathrm{Im}\\,(\\mu)\\] が起こらないものをいう．この全体を \\(\\mathcal{S}(E)\\) で表す． 65\n有界な符号付き測度の全体を \\[\n\\mathcal{S}^1(E)=\\left\\{\\mu\\in\\mathcal{S}(E)\\mid\\|\\mu\\|_\\mathrm{TV}&lt;\\infty\\right\\}\n\\] で表す． 66\n測度 の全体を \\(\\mathcal{M}(E):=\\mathcal{S}(E)_+\\) で表す．67 有界な測度の全体を \\(\\mathcal{M}^1(E):=\\mathcal{S}^1(E)_+\\) で表す．\n\\(S^1(E),M^1(E)\\) などとイタリック体を用いた場合，\\(\\mathcal{S}^1(E),\\mathcal{M}^1(E)\\) のうち Radon 測度のなす部分空間を表す．68\n\\(E\\) を位相空間とする．有界な符号付き Borel 測度の列 \\(\\{\\mu_i\\}\\subset\\mathcal{S}^1(E,\\mathcal{B}(E))\\) の 弱収束 を，\\(\\mu_i\\Rightarrow\\mu\\) とも表す．69\nこの弱位相に関する ペアリング \\((-|-):\\mathcal{S}^1(E,\\mathcal{B}(E))\\times C_b(E)\\to\\mathbb{R}\\) を \\[\n(\\mu|f):=\\int_Ef(x)\\mu(dx)\n\\] または単に \\(\\mu f\\) で表す．70\n可測空間 \\((E,\\mathcal{E})\\) 上の 確率測度 の全体を \\(\\mathcal{P}(E,\\mathcal{E})\\) と書く．\\(E\\) が位相空間であるとき，Borel 確率測度の全体を \\(\\mathcal{P}(E)\\) と略記する．71\n\\(E\\) を位相空間とする．\\((E,\\mathcal{B}(E))\\) 上の Radon 確率測度 の全体を \\[P(E)\\subset\\mathcal{P}(E)\\] で表す．72\n２つの確率分布 \\(\\mu,\\nu\\in\\mathcal{P}(E)\\) の カップリング の全体を \\[\nC(\\mu,\\nu):=\\left\\{\\pi\\in P(E^2)\\:\\middle|\\:\\substack{(\\mathrm{pr}_1)_*\\pi=\\mu,\\\\(\\mathrm{pr}_2)_*\\pi=\\nu.}\\right\\}\n\\] で表す．73\n\\(d\\)-次元 正規分布 を \\[\\operatorname{N}_d(\\mu,\\Sigma)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．74 その密度は \\[\n  \\phi_d(x;\\mu,\\Sigma):=\\frac{d \\operatorname{N}_d(\\mu,\\Sigma)}{d \\ell_d}(x)\n  \\] で表す．\n集合 \\(A\\subset\\mathbb{R}^d\\) 上の 一様分布 を \\[\\mathrm{U}(A)\\in\\mathcal{P}(\\mathbb{R}^d)\\] で表す．\n点 \\(x\\in E\\) 上の Delta 測度 を \\(\\delta_x\\) で表す．75\n確率変数 \\(X\\sim\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) の 分布関数 を \\[\n\\begin{align*}\n  F_X(a)&:=F_\\nu(a)\\\\\n  &:=\\operatorname{P}[X_1\\le a_1,\\cdots,X_d\\le a_d]\\\\\n  &\\quad(a=a_{1:d}\\in\\mathbb{R}^d)\n\\end{align*}\n\\] で表す．\n\\(d=1\\) のとき，その一般化逆を \\[\nF^-_\\nu(u):=\\inf\\left\\{x\\in\\mathbb{R}\\mid F_\\nu(x)\\ge u\\right\\}\n\\] \\[\n(u\\in(0,1)^d)\n\\] で表す．76\n\n\n\n3.2 確率核\n確率核 は可測空間の射となる基本的な対象である．\\((E,\\mathcal{E}),(F,\\mathcal{F})\\) を可測空間とする．\n\n核 \\(T:E\\to F\\) とは，次の2条件を満たす写像 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) をいう：77\n\n\\(\\{T(x,-)\\}_{x\\in E}\\subset\\mathcal{M}(F)\\)．\n\\(\\{T(-,A)\\}_{A\\in\\mathcal{F}}\\subset\\mathcal{L}(E)\\)．\n\n核 \\(T:E\\times\\mathcal{F}\\to[0,\\infty]\\) が 有界 であるとは， \\[\n\\sup_{x\\in E}\\lvert P(x,F)\\rvert&lt;\\infty\n\\] を満たすことをいう．78 すなわち，写像 \\(E\\to M^1(F)\\) が有界な像を持つことをいう．79\n\\(\\{P(x,F)\\}_{x\\in E}=\\{1\\}\\) を満たす有界核 \\(P\\) を 確率核 または Markov核 という． 80\n\\(F\\) が 可分距離空間上の確率空間であるとき，確率核 \\(P:E\\to F\\) とは可測写像 \\(T:E\\to\\mathcal{P}(F)\\) に等価である．ただし，\\(\\mathcal{P}(F)\\) は弱収束の位相による Borel 可測空間と考える．81\n核 \\(T\\) の符号付き測度の空間 \\(\\mathcal{S}(E)\\) への右作用 \\(\\cdot T:\\mathcal{S}(E)\\to\\mathcal{S}(F)\\) を \\[\n\\begin{align*}\n  &(\\mu T)(A)\\\\\n  &\\qquad:=\\int_E\\mu(dx)T(x,A),\\\\\n  &\\qquad\\qquad(A\\in\\mathcal{F}),\n\\end{align*}\n\\] で定める．\n核 \\(T\\) の可測関数の空間 \\(\\mathcal{L}(F)\\) への左作用 \\(T\\cdot:\\mathcal{L}(F)\\to\\mathcal{L}(E)\\) を \\[\n\\begin{align*}\n  &(Tf)(x)\\\\\n  &\\qquad:=\\int_FT(x,dy)f(y),\\\\\n  &\\qquad\\qquad (x\\in E),\n\\end{align*}\n\\] で定める．82\n核 \\(T:E\\to F,S:F\\to G\\) の 合成 \\(T\\otimes S:E\\to F\\times G\\) を \\[\n\\begin{align*}\n  &(T\\otimes S)(x,A\\times B)\\\\\n  &\\qquad:=\\int_AT(x,dy)S(y,B),\\\\\n  &\\qquad\\qquad(x\\in E,A\\in\\mathcal{F},B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．83\n核 \\(T:E\\to F,S:F\\to G\\) の 積 \\(TS:E\\to G\\) を \\[\n\\begin{align*}\n  (TS)(x,B)&:=(T\\otimes S)(x,F\\times B)\\\\\n  &=\\int_FT(x,dy)S(y,B)\\\\\n  &\\qquad(x\\in E,B\\in\\mathcal{G}),\n\\end{align*}\n\\] で定める．84\n\n\n\n\n\n\n\n確率核の概念\n\n\n\n\n\n確率核は積に関して結合的で，\\(I(x,A):=\\delta_x(A)\\) を単位元に持ち，可測空間と確率核の圏 \\(\\mathrm{Stoch}\\) をなす．これは \\((1,2)\\) を 終対象 とする Markov圏 である．\n可測空間 \\((1,2)\\) からの確率核 \\((1,2)\\to(E,\\mathcal{E})\\) は \\(\\mathcal{P}(E)\\) の元に等価である．85\nグラフィカルモデルは，圏 \\(\\mathrm{Stoch}\\) における図式として理解できる．この立場から本ブログでは階層モデルや生成モデルを確率核 \\(\\mathcal{Z}\\to\\mathcal{X}\\) でも表す．\n\n\n\n\n\n3.3 確率変数\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．86 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．87\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．88\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．89\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．90\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．91\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．92\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．93\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．94\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．95\n関数・確率変数と言った場合，断りがない限り \\(\\mathbb{R}\\)-値のものを考える．\n\n可測空間 \\((E,\\mathcal{E})\\) 上の 可測関数 の全体を \\(\\mathcal{L}(E)=\\mathcal{L}(E,\\mathcal{E})\\) と書く．96\n\\((E,\\mathcal{E})\\) の Lebesgue 完備化 \\(\\mathcal{E}_\\mu\\) に関して可測な関数を \\(\\mu\\)-可測関数 といい，その全体を \\(\\mathcal{L}(\\mu)=\\mathcal{L}(E,\\mathcal{E}_\\mu)\\) と書く．97\n部分 \\(\\sigma\\)-代数 \\(\\mathcal{F}\\subset\\mathcal{E}\\) について，\\(\\mathcal{F}\\)-可測なもののなす部分集合を \\(\\mathcal{L}_\\mathcal{F}(E)=\\mathcal{L}(E,\\mathcal{F})\\) と表す．\n測度空間 \\((E,\\mathcal{E},\\mu)\\) において，\\(\\mu\\) に関して殆ど至る所で等しい関数を同一視して得る商空間を \\(L(\\mu)=L(E,\\mathcal{E},\\mu)\\) と書く．98\nこの規則は任意の Lebesgue 空間 \\(L^p(\\mu)\\) で同じである．\n\\(p\\in[1,\\infty]\\) に関して，\\(L^p(E)\\) のノルム を \\(\\|-\\|_p\\) で表す．"
  },
  {
    "objectID": "static/Notations.html#sec-analysis",
    "href": "static/Notations.html#sec-analysis",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 解析",
    "text": "4 解析\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．126\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．127\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．128\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．129 \\(m=1\\) のとき， \\[\n\\operatorname{grad}u:=\\nabla u:=(Du)^\\top=\\begin{pmatrix}\\frac{\\partial u}{\\partial x_1}\\\\\\vdots\\\\\\frac{\\partial u}{\\partial x_n}\\end{pmatrix}\n\\] とも表す．\n発散 を \\[\n\\operatorname{div}u:=\\nabla\\cdot u:=\\operatorname{Tr}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．130\n\\(u\\) が正方行列 \\(M_n(\\mathbb{R})\\)-値であった場合，行成分毎の適用 \\[\n  \\operatorname{div}u:=\\begin{pmatrix}\\operatorname{div}(u_{1-})\\\\\\vdots\\\\\\operatorname{div}(u_{n-})\\end{pmatrix}\n  \\] と解する．\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n  \\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n  \\] と同一視する．131\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n  \\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\operatorname{Tr}(D^2u)\n  \\] で定める．\n\n\n\n4.2 Fourier 変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．132\n符号関数 を \\[\n\\operatorname{sgn}(x):=2H(x)-1\n\\] で定める．133\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\n\n\n4.3 超関数\n\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．134 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．135\n\n\n\n4.4 確率解析\n\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．136"
  },
  {
    "objectID": "static/Notations.html#sec-process",
    "href": "static/Notations.html#sec-process",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "5 過程",
    "text": "5 過程\n確率過程の概念は初め解析学と深く結びついて発展した．その後，確率論と統計学，そして物理学などの自然科学や社会科学の分野で，重要なモデリングの道具としても広く使われるようになった．\n\n5.1 確率基底\n確率過程という対象は写像として種々の見方があり，極めて定義がしにくい．その数学的本体はむしろ確率基底（徐々に \\(\\sigma\\)-代数が拡大していく確率分布）にあるというべきである．\nここでは （確率）過程 と言ったとき，共通の確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) を定義域に持ち，値域 \\(E\\) も共通とする確率変数の族 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) を指すこととする．127\n\n確率過程 \\(\\{X_t\\}_{t\\in T}\\subset\\mathcal{L}(\\Omega;E)\\) が積空間 \\((E^T,\\otimes_{t\\in T}\\mathcal{E})\\) に定める写像 \\(X_-:\\Omega\\to E^T\\) を 転置 と呼ぶ．128\n関数 \\(f:\\mathbb{R}\\supset T\\to\\mathcal{X}\\) が 第一種不連続 であるとは，常に左極限を持つ右連続関数であることをいい，このような関数の全体を \\(D(T;\\mathcal{X})\\) で表す．129\n第一種不連続関数 \\(x\\in D_E(T)\\) について，左極限を \\[\nx(t-):=\\lim_{s\\nearrow t}x(s)\n\\] と表し，跳躍の大きさを \\[\n\\Delta x(t):=x(t)-x(t-)\n\\] で表す．130 ただし，\\(x(0-)=x(0)\\) と約束する．131\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) 上の 情報系 \\((\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) とは，右連続性 \\[\n\\mathcal{F}_t=\\mathcal{F}_{t+}:=\\bigcap_{s&gt;t}\\mathcal{F}_s\n\\] を満たす増大系（フィルトレーション） \\(\\mathcal{F}_s\\subset\\mathcal{F}_t\\;(s\\le t)\\) をいう．132 これを \\(\\mathbb{F}:=(\\mathcal{F}_t)\\) などとも表す．\nフィルトレーションの左極限を次のようにも表す：133 \\[\n\\mathcal{F}_{t-}:=\\bigvee_{s&lt;t}\\mathcal{F}_s,\\quad(t\\in\\overline{\\mathbb{R}}_+),\n\\]\n確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) とその上のフィルトレーション \\(\\mathbb{F}:=(\\mathcal{F}_t)_{t\\in\\mathbb{R}_+}\\) からなる 4-組 \\((\\Omega,\\mathcal{F},\\mathbb{F},\\operatorname{P})\\) を 確率基底 という．\n確率基底 \\((\\Omega,\\mathcal{F},\\mathbb{F},\\operatorname{P})\\) の情報系 \\((\\mathcal{F}_t)\\) が 完備 であるとは，\\(\\mathcal{N}(\\operatorname{P})\\subset\\mathcal{F}_0\\) を満たすことをいう．このとき確率空間 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) も完備であることが必要であることに注意．確率基底が完備な情報系を備えることを 通常の条件 を満たすとも言う．134\n過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}}\\) がフィルトレーション \\((\\mathcal{F}_t)\\) に 適合的 であるとは，次を満たすことをいう： \\[\nX_t\\in\\mathcal{F}_t,\\quad(t\\in\\mathbb{R}).\n\\]\n\\(T\\times\\Omega\\) 上の 可予測／随意／発展的可測／可測／適合 \\(\\sigma\\)-代数を \\(\\mathcal{P},\\mathcal{O},\\mathcal{A},\\mathcal{M},\\mathcal{A}_0\\) で表す．135\n\n\n\n\n5.2 マルチンゲール136\nマルチンゲールは「確率的定数関数」とも呼ぶべき基本的な対象である．「無情報性」「予測不可能性」を表すようなクラスで，多くの過程（特別な半マルチンゲールに属する過程）は局所マルチンゲール部分を除けば，可予測な有界変動部分しか残らない．多くの概念の確率的な拡張は，\\(A=B\\) を「 \\(A-B\\) はマルチンゲールである」と言い換えることで得られる．137\n\nある部分集合 \\(I\\subset\\mathbb{R}_+\\) に値を取る停止時の全体を \\(\\mathbb{T}_I\\)，そのうち特に値域が有限部分集合になるものの部分集合を \\(\\mathbb{T}_I^f\\) で表す．138\n\\(\\mathbb{R}_+\\) で添字付けられた実マルチンゲールのうち，\n\n一様可積分なものを \\(\\mathbb{M}\\) で表す．139\n\\(L^p(\\operatorname{P})\\)-可積分なものを \\(\\mathbb{M}^p\\) で表す．\n\\(L^p(\\operatorname{P})\\)-有界なものを \\(\\mathbb{M}^p_\\infty\\) で表す．\\(p=2\\) の場合の元を特に \\(L^2\\)-有界マルチンゲールと呼ぶ．140\n上限が \\(L^p(\\operatorname{P})\\)-可積分なものを \\(\\mathbb{H}^p\\) で表す．\n\\(0\\) から始まる局所マルチンゲールの全体を \\(\\mathbb{L}\\) で表す．141\n\n過程 \\(A\\) が有界変動であるとは，任意の閉区間 \\([0,t]\\) 上で見本道が殆ど確実に有界変動になることをいう．142 その全体を \\(\\mathbb{W}^0\\) で表す．143\n\nこのときの変動の過程を \\(\\mathrm{Var}[A]\\) で表す．144\nさらに \\(\\{\\mathrm{Var}[A]_t\\}\\subset\\mathcal{L}^p(\\Omega)\\) を満たすもののなす部分空間を \\(\\mathbb{W}^p\\) で表す．\nさらに可積分変動を持つ過程 \\(\\operatorname{E}[\\mathrm{Var}[A]_\\infty]&lt;\\infty\\) のなす部分空間を \\(\\mathbb{W}^p_\\infty\\) で表す．\n特に単調増加過程の全体を \\(\\mathbb{W}_+^0\\) で表す．\n\n\\(D\\)-修正を持つクラス(LD)の擬似マルチンゲールの全体を \\(\\mathbb{Q}\\)，劣マルチンゲールの全体を \\(\\mathbb{Q}_+\\) で表す．145 このとき，\\(\\mathbb{Q}_\\mathrm{loc}\\) が特別な半マルチンゲールの全体を表す．\n\n\n\n5.3 収束\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega;E)\\) が，\\(X\\in\\mathcal{L}(\\Omega;E)\\) に\n\n確率収束することを \\(X_n\\overset{\\text{p}}{\\to}X\\) と表す．\n法則収束することを \\(X_n\\overset{\\text{d}}{\\to}X\\) または \\(X_n\\Rightarrow X\\) で表す．146\n\n確率変数列 \\(\\{X_n\\}\\subset\\mathcal{L}(\\Omega)\\) が 一様に緊密 であることを \\[\nX_n=O_p(1)\n\\] とも表す．147\nさらに確率変数列 \\(\\{R_n\\}\\subset\\mathcal{L}(\\Omega)\\) について， \\[\nX_n=O_P(R_n)\n\\] であるとは，ある一様に緊密な列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が \\[\nX_n=Y_nR_n\n\\] と表せることをいう．\n同様にして， \\[\nX_n= o_P(R_n)\n\\] であるとは，ある \\(0\\) に確率収束する列 \\(\\{Y_n\\}\\subset\\mathcal{L}(\\Omega)\\) が存在して \\[\nX_n=Y_nR_n\n\\] と表せることをいう．"
  },
  {
    "objectID": "static/Notations.html#終わりに",
    "href": "static/Notations.html#終わりに",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "終わりに",
    "text": "終わりに\n\n本サイトの記法で筆者が最も注意することは，あらゆる記法を背後の数学的消息と調和するように定義するということであった．\nこれにあたり，あらゆる 数学的対象 を集合から構成する立場を取る一方で，理解するにあたっては 集合と写像（または関手）とを厳密に峻別する ということを徹底することを大事にした．\n例えば集合の合併と共通部分に \\(\\cap,\\cup\\) を用いること，直和と直積に \\(\\coprod,\\prod\\) を用いることは，圏論的な双対性を視覚的に認識しながら数学的議論を進めるためである．(斎藤毅, 2009, p. 37) にも詳しく解説されている．\n記法の開発は数学の重要な一部であると筆者は信じているのである．"
  },
  {
    "objectID": "static/Notations.html#footnotes",
    "href": "static/Notations.html#footnotes",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n集合のなす圏 \\(\\mathrm{Set}\\) は数学の基礎付けとして採用するのに極めて良い性質を持つ nLab．↩︎\n(Del Moral and Penev, 2014), (Helemskii, 2006), (MacKay, 2003, p. 600) に一致する．\\(\\equiv,\\overset{\\text{def}}{=}\\) などもよく用いられる．(Crisan and Doucet, 2002), (Smith, 2010) では \\(\\overset{\\triangle}{=}\\) も用いられる．ここでは，これらの左右対称な記号は避けた．また，\\(=_{\\text{df}}\\) などを使うものもある (Quine and Szczotka, 1994)．↩︎\n(斎藤毅, 2009, p. 13) の記法に一致する．この定義と存在は公理から直ちに従う nLab．このときの \\(P\\) も関手である．関手が，対象 \\(S\\) に作用していると読めるように設計された記法である nLab．↩︎\n(斎藤毅, 2009, p. 10) は \\(A\\coprod B\\) と表す．(伊藤清三, 1963) は \\(A+B\\) と表す．↩︎\n(Dellacherie and Meyer, 1978) に一致．↩︎\n有限集合については \\(\\mathrm{Card}\\,(X)\\) とも混用される，(Gerber et al., 2019) など．↩︎\n(Dellacherie and Meyer, 1978) に倣った．一般に測度論において \\(\\left\\{f&lt;c\\right\\}:=\\left\\{x\\in X\\mid f(x)&lt;c\\right\\}\\) などのように略記される．このような集合 \\(A\\) の存在自体は分出公理により導かれ，分出公理は通常ZF公理系の置換公理から導かれる (新井敏康, 2011, p. 119)．通常 \\(\\left\\{\\omega\\mid P(\\omega)\\right\\}\\) によって定義される数学的対象をクラスと呼び，集合を定めるとは限らないとして区別される：ラッセルの逆理 が例を与える (新井敏康, 2011, p. 117)．↩︎\n(von Neumann, 1923) による定義である．(斎藤毅, 2009, pp. 15–16), Wikipedia とも一致する．↩︎\n(Chopin et al., 2022), (Srinivasan, 2001) なども採用している．↩︎\n(Jacod and Protter, 2012), (Le Gall, 2016), (鎌谷研吾, 2020, p. 106), (Helemskii, 2006, p. 2), (Jacob, 2001) の記法に一致する．(Evans, 2010, p. 698) では同じ記法で正実数の全体を意味する．↩︎\nこの運用は (Jacod and Protter, 2012) に一致する．記法 \\(\\mathbb{N}\\) は (Villani, 2009), (Jacob, 2001) などでは正整数の全体 \\(\\mathbb{N}=\\left\\{1,2,3,\\cdots\\right\\}\\) と定められている．(Jacod and Shiryaev, 2003) も \\(\\overline{\\mathbb{R}}_+=[0,\\infty]\\) としている．↩︎\n(Chopin and Papaspiliopoulos, 2020), (Chopin et al., 2022) などが採用している．↩︎\nこれは組 \\((X_1,\\cdots,X_N)\\) が定める \\(X:[N]\\ni i\\mapsto X_i\\) という写像があった際，この写像の積 \\(\\prod_{i\\in[N]}X\\) による \\(1:N\\) の像を \\(X_{1:N}\\) と略記する，という意味である．↩︎\n(斎藤毅, 2009, p. 43), (斎藤毅, 2020, p. 12) に従った．対応 \\(f\\mapsto f_*\\) は共変関手 \\(P_*:\\mathrm{Set}\\to\\mathrm{Set}\\) を定める．↩︎\n(斎藤毅, 2009, p. 25), (Evans, 2010, p. 700) などでは \\(\\chi_A\\) と表す．↩︎\n(Brezis, 2011, p. 14) の使い分けに倣った．支持関数は英語で indicator function という (Beck, 2017, p. 14) 例2.1，(寒野善博，土谷隆, 2014, p. 110)，(Ekeland and Témam, 1999, p. 8)．↩︎\nnLab に倣った．本来はエピ射とモノ射を表す記法であるが，ここでは集合の圏 \\(\\mathrm{Set}\\) に限ることとする．↩︎\n(斎藤毅, 2009, p. 26) に倣った．この混用については p.35 で触れられている．これが集合をなすのは，ZF公理系のうちの置換公理による (新井敏康, 2011, p. 118)．↩︎\n(斎藤毅, 2009, p. 37) にも詳しく解説されている．このような態度は concept with an attitude という．↩︎\nnLab に従った．\\(O\\) は写像 \\(\\mathbb{R}^\\mathbb{R}\\to P(\\mathbb{R}^\\mathbb{R})\\) を定める．(Carmer, 1946, p. 122), (Jacod and Protter, 2012), (Del Moral and Penev, 2014, p. xlvii), (Evans, 2010, p. 704) に一致．↩︎\nこれは配置集合とも言う．\\(Y^X\\) は (松坂和夫, 1968, p. 38), (Giné and Nickl, 2021) に，\\(\\mathrm{Map}(X,Y)\\) は (斎藤毅, 2009, p. 26) に倣った．(新井敏康, 2011, p. 120) は \\({}^XY\\)と表す．↩︎\n(Billingsley, 1999), (Ethier and Kurtz, 1986), (Jacob, 2001) などは \\(\\pi_i\\) で表す．↩︎\n(斎藤毅, 2009, p. 27) では値写像と訳している．↩︎\n(斎藤毅, 2009, p. 25), (Jacob, 2001) に倣った．(Villani, 2009) では \\(\\mathrm{Id}\\) で表す．↩︎\n\\(\\mathrm{Op}:\\mathrm{Top}\\to\\mathrm{Cat}\\) は関手とみれる．(斎藤毅, 2020) 定義4.2.1 p.106, 定義7.1.1 p.192，category of open subsets．↩︎\n(Pedersen, 1989, p. 8) 1.2.4 に倣った．(Bogachev and Smolyanov, 2017) は \\(\\Phi_\\tau^x\\) で表す．↩︎\n(Del Moral and Penev, 2014, p. xlvii), (Bogachev, 2007, p. 277) 4.1.(i) に一致する．(Bogachev, 2007, p. 277) では lattice を structure ともいう．↩︎\n(Dellacherie and Meyer, 1978), (伊藤清, 1991, p. 137) に倣った．↩︎\n(Jacob, 2001) など，\\(a^+,a^-\\) を用いる流儀もある．↩︎\n(Jacob, 2001) に一致．↩︎\n(斎藤毅, 2009, p. 75) に従った．(Rudin, 1991, p. 4), (Jacob, 2001) では \\(B_r(t)\\) で表す．↩︎\n(Pedersen, 1989, p. 44), (Evans, 2010, p. 699) に倣った．↩︎\n(Pedersen, 1989, p. 41) など．↩︎\n(Pedersen, 1989, pp. 2.5.1 p.70) など．↩︎\n(Boucheron et al., 2013) に倣った．ここでは \\(t\\)-blowup と呼んでいる．(Giné and Nickl, 2021, p. 27) では \\(d(x,A)\\le\\epsilon\\) と定義しているが，我々は同じものを \\(\\overline{A_\\epsilon}\\) で表すこととする．(Dudley, 2002, p. 393), (Bogachev, 2007, p. 192) では \\(A^\\epsilon\\) で表し，(Dudley, 2002, p. 407) は閉集合バージョンを \\(A^{\\delta]}\\) で表す．↩︎\nすなわち， \\(\\mathcal{F}(x;y)\\) という記法は，\\(y\\) は写像（あるいは関手） \\(\\mathcal{F}\\) のパラメータ付けをする添字として理解する数学的対象，\\(x\\) は写像（あるいは関手）の引数として理解する数学的対象として峻別する態度がある．↩︎\nMinkowski 和などとも呼ばれる．「(Jacob, 2001) などが触れている．↩︎\n(Pedersen, 1989, p. 67) は \\(\\operatorname{conv}(A)\\) で表す．(Conway, 2007, p. 101), (寒野善博，土谷隆, 2014) は \\(\\operatorname{co}(A)\\) と表す．↩︎\n(斎藤毅, 2007, p. 33) に倣った．(Jacob, 2001) などは \\(\\operatorname{lin}(A)\\) で表す．↩︎\n(斎藤毅, 2009, p. 86), (斎藤毅, 2007, p. 13), (Villani, 2009) に従った．(Evans, 2010, p. 697) では \\(\\mathbb{M}^{m\\times n}\\) で表す．↩︎\n(斎藤毅, 2007, p. 19) に一致する．(Evans, 2010, p. 697) では \\(\\mathbb{S}^n\\) と表す．↩︎\n(Rogers and Williams, 2000, p. 110) V.1.3 では \\(S_n^+\\) の記法が用いられている．↩︎\n(Evans, 2010, p. 698) に一致する．↩︎\n(Evans, 2010, p. 697) に一致する．↩︎\n(MacKay, 2003, p. 599) に一致する．(吉田朋広, 2006) などは転置を \\(A'\\) で表す．(斎藤毅, 2009, p. 86) では \\({}^t\\!A\\) と表す．(Evans, 2010, p. 697) は \\(A^T\\)．↩︎\n随伴行列ともいう (斎藤毅, 2009, p. 87)．↩︎\n(Pedersen, 1989, p. 80) に倣った．(Conway, 2007, p. 2) では \\(\\langle x,y\\rangle\\) で表されるが，(Lang, 1995, p. 343) によるとこれは von Neumann の 1950 年代のセミナーでの記法であったという．↩︎\n(Pedersen, 1989, p. 119) は \\((-|-)_{\\text{tr}}\\) で，(Evans, 2010, p. 697) は \\(A:B\\) で表す．特に，古典力学や有限要素法の文献においては，二項積 の間の演算である二重点乗積を \\(:\\) で表したことから，この記法が用いられる．二項積については (Abraham et al., 1988, p. 341) も参照．↩︎\n\\(\\|A\\|_\\mathrm{HS}\\) は (Villani, 2009, p. XVII) に，\\(\\lvert A\\rvert\\) は (Evans, 2010, p. 697) に倣った．これは Frobenius ノルムともいう．Hilbert-Schmidt ノルムは，一般の Hilbert 空間上の有界作用素に関して定義される．(Pedersen, 1989, p. 119) は \\(\\|-\\|_2\\) で表す．↩︎\n(Pedersen, 1989, p. 50) に倣った．↩︎\n(Evans, 2010, p. 699), (Jacob, 2001, p. xvi), (Bakry et al., 2014, p. xv) に倣った．↩︎\n(Pedersen, 1989, p. 50) に一致する．(Giné and Nickl, 2021, p. 17) は \\(\\ell_\\infty(J)\\) で表す．↩︎\n(Evans, 2010, p. 698) に一致．↩︎\n(Billingsley, 1999) は \\(\\sigma[\\mathcal{A}]\\) や \\(\\sigma[\\pi_t:t\\in T]\\) とも表す．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Dudley, 2002, p. 89) に倣った．(A. W. van der Vaart and Wellner, 2023, p. 6) では 外確率 という．↩︎\n(Bogachev, 2007, p. 17) 定義1.5.1, (Bogachev and Smolyanov, 2020, p. 64) に倣った．この \\(\\mathcal{A}_\\mu\\) は \\(\\mathcal{A}\\lor\\mathcal{N}(\\mu)\\) と \\(\\mathcal{L}_\\mu:=\\left\\{A\\subset X\\mid\\exists_{A_1,A_2\\in\\mathcal{A}}\\;A_1\\subset A\\subset A\\right\\}\\) に一致する上，\\(\\mu\\) が \\(\\sigma\\)-有限ならば \\(\\mathfrak{M}_{\\mu^*}:=\\left\\{A\\subset X\\:\\middle|\\:\\substack{\\forall_{A_0\\subset X}\\;\\mu^*(A\\cap A_0)+\\\\\\mu^*(A_0\\setminus A)=\\mu^*(A_0)}\\right\\}\\) にも一致する (Bogachev, 2007, p. 129) 1.12.129, (Dudley, 2002, p. 102) 3.2.2-3．↩︎\nfull set の和訳として選んだ． (Bogachev, 2007, p. 110) では a set of full measure と表現している．\\(\\mathcal{N}(\\mu)\\) の記法は (Dudley, 2002, p. 101) に倣った．↩︎\n(伊藤清, 1991, p. 137) に従った．↩︎\n(Bogachev, 2007, p. 23) に倣った．(Bogachev, 2007, p. 56) 1.12(iv) では \\(\\mathcal{A}_A\\) とも表し，trace \\(\\sigma\\)-algebra とも呼ぶという．(Dellacherie and Meyer, 1978) では \\(\\mu|_A\\) の定義域を \\(\\mathcal{A}|_A\\) で表す．↩︎\n(Bogachev, 2007, p. 188), (Lang, 1993, p. 158) に従った．↩︎\nこのような一般的な場合の定義は (Bogachev, 2007, p. 189) 参照．↩︎\n(Giné and Nickl, 2021, p. 16), (Bogachev and Smolyanov, 2020, p. 171) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 8) に倣った．(Bogachev, 2007, p. 26), (Gerber et al., 2019) などは \\(\\lambda_d\\) と表す．(Jacob, 2001, p. xv) は \\(\\lambda^{(n)}\\) で表す．↩︎\n可測空間を \\((E,\\mathcal{E})\\) で表すのは，(Revuz, 1984)，(Le Gall, 2016), (Del Moral, 2004) に倣った．↩︎\n\\(\\mathcal{S}\\) は (Nihat Ay and Schwachhöfe, 2017, pp. 第3.1節 p.121) の記法に倣った．(Bogachev, 2007), (Villani, 2009) などはこれに \\(M(E)\\) を用いる．符号付測度の定義は (Dunford and Schwartz, 1958, p. 95) III.1.1, (Dudley, 2002, p. 178) 5.6，(藤田宏，吉田耕作, 1991, p. 383) 定義7.1, (Halmos, 1950, p. 118) に一致する．↩︎\n(Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) と表し，(Lang, 1993, p. 199) では \\(M^1\\)，(Revuz, 1984) では \\(\\mathrm{b}\\mathcal{M}(\\mathcal{E})\\)，(Dunford and Schwartz, 1958) では \\(ca(E,\\mathcal{E})\\) と表す．我々も，添字 \\({}^1\\) を全変動が有限であることの象徴として採用する．実際，\\(\\mu\\)-連続な測度 \\(\\nu\\) について，\\(\\|\\nu\\|_\\mathrm{TV}=\\left\\|\\frac{d \\nu}{d \\mu}\\right\\|_1\\) である (Lang, 1993, p. 200) 定理3.3．有界かつRadonな符号付き測度を (Pedersen, 1989, p. 252) 6.5.8 は \\(M(E)\\) と表す．実は有限次元 Banach 空間 \\(B\\) について，\\(B\\)-値であることと有界であることは同値になる：「有界」測度と「有限」測度 を参照．\\(\\mathcal{S}(E;B)\\) の表記は，有界性はひとまず不問として \\(B\\)-値測度を表す際に使うこととする．全変動ノルムの記法は (Giné and Nickl, 2021, p. 2), (Villani, 2009) に一致する．(Bogachev, 2007) は \\(\\|-\\|\\) で表す．↩︎\n(Del Moral and Penev, 2014, p. xli), (Del Moral, 2004, p. 7) では \\(\\mathcal{M}(E)\\) を有界な符号付き測度に用いている．(Jacob, 2001, p. xv) では \\(\\mathcal{M}^+(E)\\) を測度の全体としている．↩︎\n(Bogachev, 2007, p. 76) では \\(\\mathcal{M}_r(E)\\) で表す．(Dellacherie and Meyer, 1978) では，有界な Radon 測度の全体を \\(\\mathcal{M}_b^+(E)\\) で表す．↩︎\n(Bogachev, 2007, p. 175) 定義8.1.1 に倣った．↩︎\n(Crisan and Doucet, 2002) に一致する．(Dellacherie and Meyer, 1978) は \\(\\mu(f),\\langle\\mu,f\\rangle\\) のいずれも用いるとしている．↩︎\n(Jacod and Shiryaev, 2003, p. 347), (Crisan and Doucet, 2002), (Ethier and Kurtz, 1986, p. 96), (Bogachev, 2007, p. 228) に一致する．(Kechris, 1995, p. 109), (Villani, 2009) はイタリックで \\(P(E)\\) と表す．↩︎\n(Pedersen, 1989, p. 72) に倣った．(Bogachev, 2007, p. 76) 第7.2節 では \\(\\mathcal{P}_r(X)\\) で表す．Radon 測度とは，内部正則性（＝緊密性） \\[\\forall_{B\\in\\mathcal{B}(E)}\\;\\forall_{\\epsilon&gt;0}\\;\\exists_{K\\overset{\\textrm{cpt}}{\\subset}B}\\;\\mu(B\\setminus K)&lt;\\epsilon\\] を満たす Borel 測度をいう (Bogachev, 2007, pp. 68–69) 定義7.1.1, 7.1.4．↩︎\n(Kulik, 2018) が \\(\\mathcal{C}\\) で表すのに倣った．(Bogachev, 2018, p. 105), (Villani, 2009, p. XXI) では \\(\\Pi(\\mu,\\nu)\\) で，(Ethier and Kurtz, 1986, p. 96) では \\(\\mathcal{M}(\\mu,\\nu)\\) で，(Dudley, 2002, p. 420) 11.8節 は \\(M(\\mu,\\nu)\\)，(Figalli and Glaudo, 2023) では \\(\\Gamma(\\mu,\\nu)\\) で表す．(Bogachev, 2007, p. 235) 8.10(viii)節と (Villani, 2009, p. 95) 注6.5 に倣い，カップリングの元は Radon なものに限っている点に注意．↩︎\n(竹村彰道, 2020) の記法に一致する．↩︎\nDirac 測度とも言う．(Jacod and Shiryaev, 2003, p. 68), (Protter, 2005, p. 299), (Jacob, 2001) などは \\(\\epsilon_x\\) で表す．(Protter, 2005, p. 299) は Dirac 関数を \\(\\delta_x\\) で表す．↩︎\n(Gerber et al., 2019) の記法に一致．分位点関数 (quantile function) (竹村彰道, 2020, p. 16)，確率表現関数 (森口繁一, 1995) などともいう．(Dudley, 2002, p. 283) は \\(X_F\\) とも表している．↩︎\n(Revuz and Yor, 1999, p. 79) 定義III.1.1.1，(Revuz, 1984, p. 8) 定義1.1.1.1，(Kallenberg, 2017, p. 16), (Bass, 2011, p. 154) 定義19.2, (Cho and Jacobs, 2019, p. 962) 例7.2 では kernel，(Jacod and Shiryaev, 2003, p. 65)，(Kolokoltsov, 2011, p. 110) 3.5節, (Klenke, 2020, p. 204) 8.3節 では transition kernel と呼んでいる．↩︎\n(Kolokoltsov, 2011, p. 110) 3.5節 に倣った．(Del Moral, 2004, p. 9) は (bounded) integral operator と呼ぶ．↩︎\n実は有界核は，可測写像 \\(E\\to M^1(F)\\) と同一視出来る (Kallenberg, 2017, p. 30) 補題1.14．ただし，\\(M^1(F)\\) には \\(\\mathcal{L}_b(F)\\) が生成する最小の \\(\\sigma\\)-代数を考える．↩︎\n(Crisan and Doucet, 2002, p. 737) では Markov transition kernel，(Del Moral, 2004, p. 9), (Ghosal and van der Vaart, 2017, p. 6), (Fritz, 2020) では Markov kernel，(Kolokoltsov, 2011, p. 110) 3.5節 では transition probability kernel or simply probability kernel と呼び，(Chopin and Papaspiliopoulos, 2020, p. 36) 定義4.1, (Bremaud, 2020, p. 135) 3.3.3節 では propability kernel，(Kulik, 2018, p. 25) では probability kernel としてさらに半群性も満たす族を transition probability kernels と呼ぶ．(Le Gall, 2016, pp. 151–152) は Markovian transition kernel と transition semigroup と呼ぶ．(Dellacherie and Meyer, 1988, p. 2) は Markovian kernel．(Kallenberg, 2017, p. 29) と (Hairer, 2021) では可測関数 \\(E\\to\\mathcal{P}(F)\\) と定義しており，transition kernel と呼んでしまう．(Bertsekas and Shreve, 1996, p. 134) 定義7.12 は stochastic kernel，(Giry, 1982), (Neveu, 1970) は transition probability, (Lawvere, 1962) は probabilistic mapping と呼んでいた．↩︎\n(Ghosal and van der Vaart, 2017, p. 510)，(Kallenberg, 2017) 補題1.14 p.30，(Hairer, 2021), (Ambrosio et al., 2008, p. 121)．この事実により，\\(E\\) 上の（局所有限な） ランダム測度 とは，確率空間からの核 \\(\\Omega\\to E\\) に等しい (Kolokoltsov, 2010)．↩︎\nこれにより，積分核も核であり，一般的に 積分核 (Conway, 2007, p. 29) または 核関数 (Schölkopf and Smola, 2002) などといったときは \\(T\\) が \\(F\\) 上で密度を持つ特別な場合であったことがわかる．nLab も参照．↩︎\n(Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 直積 と呼ばれており，p.76 定理II.4.1 でその存在が示されている．(Heng et al., 2024) では \\(T=\\mu\\) という定値核の場合も同様の記法 \\(\\mu\\otimes S\\) を定義している．↩︎\nこちらも，行列積の一般化であることを踏まえて (Kallenberg, 2017, p. 16) の呼び方に従った．(Gikhman and Skorokhod, 2004, p. 79) では 畳み込み と呼ばれている．この式は Chapman-Kolmogorov 方程式 と呼ばれるものである．そこで，Chapman-Kolmogorov 方程式は，Markov 核の族 \\(\\{P_t\\}_{t\\in\\mathbb{R}_+}\\) が，この積という演算について半群性を満たす，という形の条件でよく登場する．↩︎\nこれより，確率核 \\(T:E\\to F\\) は，確率測度 \\((1,2)\\to(E,\\mathcal{E})\\) を \\((1,2)\\to(F,\\mathcal{F})\\) に「遷移」させているようにも思えるのである．↩︎\n(Nualart and Nualart, 2018) に倣った．(Giné and Nickl, 2021), (Dudley, 2002) では \\(\\mathrm{Pr}\\) と表している．(Villani, 2009) などは \\(\\mathbb{P}\\) で表す．↩︎\n標準 Borel 空間 ともいう．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．(Giné and Nickl, 2021) ではイタリック体で \\(E\\) と表している．(Del Moral and Penev, 2014), (Dellacherie and Meyer, 1978) では \\(\\mathbb{E}\\) を用いる．(MacKay, 2003, p. 599) では \\(\\mathcal{E}\\) を用いる．\\(\\langle-\\rangle\\) で表すこともある．↩︎\n(吉田朋広, 2006, p. 5) に倣った．筆者は \\(\\operatorname{E},\\operatorname{P}\\) のいずれも作用素と見る立場に立つためである．(Giné and Nickl, 2021) は \\(E[X],\\mathrm{Pr}\\{X\\in A\\}\\) と表す．(Nualart and Nualart, 2018), (伊藤清, 1991) はいずれも丸括弧である．(鎌谷研吾, 2020), (Bain and Crisan, 2009) では \\(\\mathbb{P}(-),\\mathbb{E}[-]\\) を用いている．(Del Moral and Penev, 2014) では \\(\\mathbb{E}(-),\\mathbb{P}(-)\\) を用いる．↩︎\n\\(V\\) は (伊藤清, 1991) に，\\(C\\) は (Giné and Nickl, 2021, p. 66) に倣った，いずれもイタリック体を用いていたが．(吉田朋広, 2006, p. 23), (鎌谷研吾, 2020), (Del Moral and Penev, 2014, p. xlvii) は代わりに \\(\\mathrm{Var},\\mathrm{Cov}\\) を用いている．↩︎\n(伊藤清, 1991, p. 125) に従った．ここでは 像測度 と 確率法則 と呼んでいる．像測度の呼び名は (Bogachev, 2007, p. 190) 3.6節, (Kechris, 1995, p. 103), (Villani, 2009) にも一致する．(Bogachev, 2007, p. 190) では \\(\\operatorname{P}\\circ X^{-1}\\)，(Villani, 2009) では \\(X_\\#\\operatorname{P}\\) と表す．nLab も参照．↩︎\n(Villani, 2009) は \\(\\mathrm{law}\\,(X)\\) で表す．↩︎\n(Nair et al., 2022, p. 246) に一致．↩︎\nこれは (Dawid, 1979) が先駆けであり， Dawid notation と呼ばれる．(Del Moral and Penev, 2014, p. xlvii) は \\(\\perp\\) を用いる．↩︎\nnLab (Concept with an Attitude) も参照．↩︎\n(Pedersen, 1989, pp. 2.1.15 p.48) に倣った．(Dudley, 2002, p. 119) や (Protter, 2005, p. 52) では \\(\\mathcal{L}^0(E,\\mathcal{E};\\mathbb{R})\\) と表す．(Dellacherie and Meyer, 1978) では \\(\\mathcal{M}(E)\\) と表し，\\(\\mathcal{L}_b(\\mathcal{E})\\) を \\(b(\\mathcal{E})\\) と表す．↩︎\nすなわち，完備化 \\(\\mathcal{E}_\\mu\\) について可測な関数の全体をいう．(Bogachev, 2007, p. 108) 定義2.1.10 では殆ど至る所定義された \\(\\mu\\)-可測な関数の全体を \\(\\mathcal{L}^0(\\mu)\\) と表す．\\(\\mathcal{L}(E)\\) と \\(\\mathcal{L}(\\mu)\\) の区別は，完備化 \\(L(E)\\) をしたあとはなくなる．↩︎\n(Dudley, 2002) では \\(L^0(E,\\mathcal{E};\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) と表す．とは言えども，\\(L(E)\\) の元を，その \\(\\mathcal{L}(\\mu)\\) の元である代表元と同一視することも多い (Bogachev, 2007, p. 262) 4.4節．(Dunford and Schwartz, 1958, p. 121) III.3.4 では関数の全体を \\(L^0_p\\)，同値類を \\(L_p\\) で表す．↩︎\n(Pedersen, 1989, p. 51) は \\(\\mathrm{lip}^\\gamma(T)\\)，(Rudin, 1987, p. 113) は \\(\\mathrm{Lip}\\gamma\\) と表す．\\(\\gamma=1\\) の場合，(Del Moral and Penev, 2014, p. xliv) の記法に一致する．↩︎\n(Evans, 2010, p. 254) では \\([f]_{C^{0,\\gamma}(T)}\\)，(Gilbarg and Trudinger, 2001, p. 52) では \\([f]_{\\gamma;T}\\)，(Pedersen, 1989, p. 51) 演習2.1.10 では \\(L(f)\\)，(Dudley, 2002, p. 390) 11.2節 では \\(\\|-\\|_L\\)，(Rudin, 1987, p. 113) 演習11 では \\(M_f\\) と表している．また，\\(\\gamma=1\\)のとき， (Evans, 2010, p. 700) では \\(\\mathrm{Lip}[f]\\) と表す．↩︎\n(Bogachev, 2007, p. 191) 8.3節 に倣った．↩︎\n(Bogachev, 2007, p. 192) 8.3節, (Dudley, 2002, p. 390) 11.2節に従った．これにより \\(\\mathrm{Lip}_b(T,d)\\) が Banach 代数をなすことが命題11.2.1で示されている．(Pedersen, 1989, p. 51) 演習2.1.10 によると，このノルムは \\(I=[a,b]\\) が区間のとき，\\({\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert f \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}:=\\|f\\|_\\mathrm{Lip}+\\lvert f(a)\\rvert\\) に同値．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Evans, 2010, p. 255) に従った．↩︎\n(Baudoin, 2014, p. 69) 定理3.9，(Nualart and Nualart, 2018, p. 31) に一致する．↩︎\n例えば，コンパクト空間 \\(K\\) について，Radon 確率測度全体の集合 \\(P(X)\\) は \\(C(X)^*\\) の \\(w^*\\)-コンパクトな凸部分集合である (Pedersen, 1989, pp. 72–73) 命題2.5.7．↩︎\n\\(\\mathcal{F}_\\mathcal{X}(E)\\) という表記は (Ethier and Kurtz, 1986, p. 95) に倣った．↩︎\n(Jacob, 2001) などは，コンパクト台を持つ連続関数の空間に \\(C_0(E)\\) を用いる．↩︎\n(Giné and Nickl, 2021, p. 17), (Jacob, 2001) に倣った．↩︎\n(Nualart and Nualart, 2018, p. 1) に倣った．↩︎\n(Pedersen, 1989, p. 222) と (Revuz, 1984) に倣った．(Dellacherie and Meyer, 1978) は \\(\\mathcal{F}(E)^+\\) で表す．↩︎\nこのような使い分けは (Nummelin, 1984, p. 1) に一致する．↩︎\n(Helemskii, 2006, p. 3) に一致する．↩︎\n(藤田宏 et al., 1991, p. 103) などとは態度が違う．↩︎\n(Pedersen, 1989, p. 44), (Jacob, 2001, p. xvii) に倣った．(藤田宏 et al., 1991, p. 106) では \\(\\mathcal{L}(X,Y)\\) と表す．↩︎\n(Lang, 1993, p. 65), (吉田耕作, 1995, p. 110) に倣った．↩︎\n\\(u_{x_i}\\) は (Evans, 2010, p. 701)，\\(\\partial_iu\\) は (吉田朋広, 2006, p. 232) などに一致する．↩︎\n(Evans, 2010, p. 701) に一致する．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(Evans, 2010, pp. 701–703) に倣った．↩︎\n(Evans, 2010, p. 703) に倣った．↩︎\n(Evans, 2010, p. 701) に倣った．↩︎\n(木田良才, 2020, p. 98) 例9.5 に一致する．神経の数理モデルの文脈では，しきい関数 (threshold function) とも呼ばれる (麻生英樹 et al., 2015, p. 10)．↩︎\n(木田良才, 2020, p. 131) 例12.21 に一致する．(Le Gall, 2016, p. 161) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0]}\\)，(Evans, 2010, p. 700), (Jacob, 2001) では \\(\\operatorname{sgn}=1_{(0,\\infty)}-1_{(-\\infty,0)}\\) と定めている（\\(0\\)での値が違う）．↩︎\nこの記法は Laurent Schwartz 以来慣習的に残り続けているので，ここでもそれに従う．(Hörmander, 2003, p. 34)．↩︎\n(Jacob, 2001)．↩︎\n積空間 \\((\\mathcal{X}^T,\\mathcal{C})\\) に値を取る \\(\\mathcal{X}^T\\)-値確率変数とみなすことに同値になる nLab．積の普遍性が成り立つためである (Kallenberg, 2021, p. 15) 補題1.9．だが \\(\\mathcal{X}\\) が位相空間であるとき，\\(\\mathcal{X}^T\\) の Borel \\(\\sigma\\)-代数に \\(\\mathcal{B}(\\mathcal{X}^T)\\) ついても可測になるとは限らない．\\(X_t\\) の終域 \\(\\mathcal{X}\\) が 可分距離空間で，かつ \\(T\\) が可算集合であるときは，\\(\\mathcal{B}(\\mathcal{X}^T)=\\mathcal{C}\\) であるため，\\(\\mathcal{B}(\\mathcal{X}^T)/\\mathcal{F}\\)-可測であることとも同値になる (Kallenberg, 2021, p. 11) 補題1.2．↩︎\n筆者が考案した名称．族 \\((X_t)_{t\\in T}:T\\to\\mathcal{L}(\\Omega)\\) としての見方と転置の関係になっているところから．(伊藤清, 1991, p. 232) は 見本過程（関数） と呼び，記法 \\(X_\\bullet\\) を採用している．(Baudoin, 2014, p. 9) は application，(Métivier, 1982, p. 4) は the random function of the process と呼んでいる．↩︎\n「第一種不連続」とは (伊藤清, 1991, p. 227) の用語．(Le Gall, 2016, p. 168) では \\(\\mathbb{D}(\\mathcal{X})\\), (Jacod and Shiryaev, 2003, p. 325) では \\(\\mathbb{D}(\\mathcal{X})\\) と表す．↩︎\n(Jacod and Shiryaev, 2003, p. 325), (Protter, 2005, p. 25) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 3) に倣った．この結果，\\(\\Delta x(0)=0\\) であることに注意．↩︎\nフィルトレーションと言ったときに右連続性も課すのは (Jacod and Shiryaev, 2003), (Protter, 2005) に倣った．記法は (伊藤清, 1991, p. 239) に倣った．↩︎\n(Jacod and Shiryaev, 2003, p. 2) 定義1.2, (Bass, 2011, p. 1), (Dellacherie and Meyer, 1978, p. 114), (Revuz and Yor, 1999, p. 42), (Métivier, 1982, p. 3) に倣った．↩︎\n(Métivier, 1982, p. 3) (Protter, 2005, p. 3) (Dellacherie and Meyer, 1978, p. 114) など参照．↩︎\n(Métivier, 1982, p. 4) では適合過程，発展的可測過程を定める \\(\\sigma\\)-代数を \\(\\mathcal{M}_0,\\mathcal{M}_1\\) としている．さらに (Métivier, 1982, p. 11) では可予測矩形を \\(\\mathcal{R}\\)，これが生成する（有限）集合体を \\(\\mathcal{A}\\) としている．↩︎\n(Dellacherie and Meyer, 1978) 49 115-IV では停止時のことを 随意時刻 (optional time) とも呼んでおり，stopping time を older terminology ともしている．筆者も optional time の語がしっかり普及すれば良かったのにと思う．↩︎\n半マルチンゲールの特性量 (Jacod and Shiryaev, 2003, p. 75)，拡張生成作用素 (Kulik, 2018) など．↩︎\n(Métivier, 1982, p. 80) 定義13.1．↩︎\n(Jacod and Shiryaev, 2003, p. 11) 1.40 では \\(\\mathcal{M}\\) で表される．↩︎\n(Jacod and Shiryaev, 2003, pp. 12 定義1.41) では \\(\\mathcal{H}^2\\) で表され，square-integrable martingale と呼ばれる．↩︎\n(Jacod and Shiryaev, 2003, p. 43) 4.20 では \\(\\mathcal{L}\\) で表される．↩︎\n(Jacod and Shiryaev, 2003, pp. 31 定義I.3.1), (Métivier, 1982, p. 94) に一致．↩︎\n(Métivier, 1982, p. 149) では \\(\\mathcal{W}^0\\) で表す．(Jacod and Shiryaev, 2003, pp. 27 I.3.1) では \\(\\mathcal{V}\\) で有界変動かつ適合\\(D\\)-過程で \\(A_0=0\\) を満たすものの全体を表す．↩︎\n(Jacod and Shiryaev, 2003, p. 27) は \\(\\mathrm{Var}[A]\\) で，(Métivier, 1982, p. 149) は \\(\\lvert A\\rvert\\) で表す．↩︎\n(Métivier, 1982, p. 149) に倣った．↩︎\n(Jacod and Shiryaev, 2003), (Nualart and Nualart, 2018) などでは \\(\\xrightarrow{\\mathcal{L}}\\) でも表される．↩︎\n(A. van der Vaart, 1998, p. 12) 2.2 に倣った．一様緊密性は (Le Cam, 1957) による概念である．↩︎"
  },
  {
    "objectID": "static/ResearchJP.html",
    "href": "static/ResearchJP.html",
    "title": "研究紹介",
    "section": "",
    "text": "モンテカルロ法は統計物理学において系の平衡状態のシミュレーションに用いられますが，ベイズ統計においても事後分布を「未知のハミルトニアンを持った系の平衡分布」とみなすことでモンテカルロ法によりシミュレーションすることができます．\nこれを実行するアルゴリズムを，統計学では マルコフ連鎖モンテカルロ法 (MCMC: Markov Chain Monte Carlo) と呼びます．\n多くの既存の MCMC は，平衡分布からのサンプリングに平衡分布にある物理系のダイナミクスを模倣して行います．Langevin Monte Carlo などはその代表であり，ダイナミクスは拡散過程になります．\nしかし，自然が平衡状態に至るのは必ずしも速いでしょうか？コーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みを取り入れることで，既存手法から効率性をさらにあげることができます．\nそのキーワードが 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to My Research\n\n\n\n2025-09-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMCMC は物理学，計算機科学，統計学で広く使われます．初めは基本的に，ランダムウォークを提案核とする Metropolis アルゴリズムが用いられていました．\n空間の広さが \\(n\\) であるとき，通常のランダムウォークは収束までに \\(n^2\\) のステップを必要としますが，特定の場合に，決定論的なダイナミクスを導入することで \\(n\\) や \\(\\log n\\) で収束するように加速することが可能でした (Chung et al., 1987)．\nのちに (Diaconis et al., 2000), (Chen et al., 1999), (Neal, 2004), (Diaconis, 2013) らにより，このような加速トリックがいずれも 非対称性 を導入することが加速の原因であると理解されていきました．\n区分確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) はこれを推し進めたマルコフ過程で，決定論的で非対称なダイナミクスをもちます．\nMetropolis アルゴリズムなどの従来の MCMC とアルゴリズムは全く違うものになりましたが，より速い収束が可能である上に，大規模なデータへの応用も可能です．\n\n\n\n\n\n\n\n\n\n\n\n\n2025-08-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非絶対連続分布からのサンプリング\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#mcmc-ダイナミクスの非対称化",
    "href": "static/ResearchJP.html#mcmc-ダイナミクスの非対称化",
    "title": "研究紹介",
    "section": "",
    "text": "モンテカルロ法は統計物理学において系の平衡状態のシミュレーションに用いられますが，ベイズ統計においても事後分布を「未知のハミルトニアンを持った系の平衡分布」とみなすことでモンテカルロ法によりシミュレーションすることができます．\nこれを実行するアルゴリズムを，統計学では マルコフ連鎖モンテカルロ法 (MCMC: Markov Chain Monte Carlo) と呼びます．\n多くの既存の MCMC は，平衡分布からのサンプリングに平衡分布にある物理系のダイナミクスを模倣して行います．Langevin Monte Carlo などはその代表であり，ダイナミクスは拡散過程になります．\nしかし，自然が平衡状態に至るのは必ずしも速いでしょうか？コーヒーに砂糖を溶かす際，我々は砂糖粒子の拡散にまかせるのではなく，スプーンで混ぜます．同様の仕組みを取り入れることで，既存手法から効率性をさらにあげることができます．\nそのキーワードが 非対称性 であり，この性質をもつアルゴリズムの提案と計算複雑性の解析，そして大規模で複雑なデータへの応用を行なっています．\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to My Research\n\n\n\n2025-09-02\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMCMC は物理学，計算機科学，統計学で広く使われます．初めは基本的に，ランダムウォークを提案核とする Metropolis アルゴリズムが用いられていました．\n空間の広さが \\(n\\) であるとき，通常のランダムウォークは収束までに \\(n^2\\) のステップを必要としますが，特定の場合に，決定論的なダイナミクスを導入することで \\(n\\) や \\(\\log n\\) で収束するように加速することが可能でした (Chung et al., 1987)．\nのちに (Diaconis et al., 2000), (Chen et al., 1999), (Neal, 2004), (Diaconis, 2013) らにより，このような加速トリックがいずれも 非対称性 を導入することが加速の原因であると理解されていきました．\n区分確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) はこれを推し進めたマルコフ過程で，決定論的で非対称なダイナミクスをもちます．\nMetropolis アルゴリズムなどの従来の MCMC とアルゴリズムは全く違うものになりましたが，より速い収束が可能である上に，大規模なデータへの応用も可能です．\n\n\n\n\n\n\n\n\n\n\n\n\n2025-08-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP による非絶対連続分布からのサンプリング\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-03-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\n2024-01-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#階層モデルへの応用",
    "href": "static/ResearchJP.html#階層モデルへの応用",
    "title": "研究紹介",
    "section": "階層モデルへの応用",
    "text": "階層モデルへの応用\nベイズ統計が最も力を発揮する設定は，階層モデリングやノンパラメトリックモデリングなどの複雑なモデルです．\n\n項目反応モデル\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\n2024-12-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\n2024-10-02\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ理想点解析\n\n\nPDMP サンプラーによる変数選択と共に\n\n\n\n2024-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#粒子輸送によるサンプリング",
    "href": "static/ResearchJP.html#粒子輸送によるサンプリング",
    "title": "研究紹介",
    "section": "粒子輸送によるサンプリング",
    "text": "粒子輸送によるサンプリング\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nデノイジング・ディフュージョンによるベイズ計算\n\n\n\n2024-08-03\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/ResearchJP.html#smc-による軌道推定",
    "href": "static/ResearchJP.html#smc-による軌道推定",
    "title": "研究紹介",
    "section": "SMC による軌道推定",
    "text": "SMC による軌道推定\n\n\n\n\n\n\n\n\n\n\n粒子法の概観\n\n\n分子動力学法から SMC サンプラーまで\n\n\n\n2024-04-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\n2023-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\n2023-12-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Software.html",
    "href": "static/Software.html",
    "title": "Software",
    "section": "",
    "text": "PDMPFlux.jl is a Julia package that provides a fast and efficient implementation of Piecewise Deterministic Markov Process (PDMP) samplers, using a grid-based Poisson thinning approach proposed in (Andral and Kamatani, 2024).\nBy the means of the automatic differentiation engines, PDMPFlux.jl only requires dim and U, which is the negative log density of the target distribution (e.g., posterior). \\[ U(x) = -\\log p(x) + \\text{const}. \\]\n\n\n\n\n\n\n\n\n\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nPDMP\n\n\n\n\n2024-12-31\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Software.html#pdmpflux.jl",
    "href": "static/Software.html#pdmpflux.jl",
    "title": "Software",
    "section": "",
    "text": "PDMPFlux.jl is a Julia package that provides a fast and efficient implementation of Piecewise Deterministic Markov Process (PDMP) samplers, using a grid-based Poisson thinning approach proposed in (Andral and Kamatani, 2024).\nBy the means of the automatic differentiation engines, PDMPFlux.jl only requires dim and U, which is the negative log density of the target distribution (e.g., posterior). \\[ U(x) = -\\log p(x) + \\text{const}. \\]\n\n\n\n\n\n\n\n\n\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC 環境に向けて：PDMPFlux.jl\n\n\n\nSlide\n\n\nMCMC\n\n\nJulia\n\n\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating PDMPs with Automatic Differentiation\n\n\n\nJulia\n\n\nPDMP\n\n\n\n\n2024-12-31\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Software.html#yuima",
    "href": "static/Software.html#yuima",
    "title": "Software",
    "section": "YUIMA",
    "text": "YUIMA\nR package for simulating and estimating SDEs.\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Notes",
    "section": "",
    "text": "Notations\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nシンガポール研究滞在記\n\n\n\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n6/25/2025\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる\n\n\n\n\n\n\nPDMP\n\n\n\n第19回日本統計学会春季集会でのポスター発表の予稿です．PDF 版はこちら \n\n\n\n\n\n2/25/2025\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nセミパラメトリック重回帰分析\n\n\n自乗残差最小化の視点から\n\n\n\nStatistics\n\n\nNonparametrics\n\n\n\n重回帰モデルにおける OLS 推定量は，部分回帰推定量としての解釈を持つ． この性質を用いた手法が媒介分析や操作変数法である． OLS 推定量は不均一分散の場合でも不偏性・一致性・漸近正規性を持ち得るが，漸近有効性は失われる． これを回復するには，誤差の分散を推定して重み付けを行う必要がある． このような方法は一般化最小二乗法と呼ばれる． さらに相関を持つデータを分析するために，より一般の共分散構造を持ったモデルに対してこの手法が拡張されている． 疫学では一般化推定方程式，さらに一般には計量経済学で一般化モーメント法と呼ばれる方法である． これらの方法は作業共分散の選択により，セミパラメトリック漸近最適な分散を達成したり，バイアスを小さくしたりできるが， いずれもトレードオフの範疇にある． \n\n\n\n\n\n12/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ混合ロジスティック回帰分析\n\n\n項目応答モデルと特異項目機能を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\n項目反応モデルとは，被験者と項目のそれぞれが独自のパラメータを持った一般化線型混合効果モデルである． 被験者ごとの特性の違いや，項目ごとの性質の違いが視覚化できるが， 本稿では能力・難易度パラメータに更なる階層構造を考える． これにより能力パラメータを変化させている背後の要因や，項目特性と個人特性の交絡効果（特異項目機能）を解析することが可能になる． brms パッケージは極めて直感的な方法でモデルのフィッティングから事後分布の推論までを実行できる． \n\n\n\n\n\n12/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズロジスティック回帰分析\n\n\nBMI データと順序ロジスティック回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\nStan\n\n\n\nロジスティック回帰分析は離散的な応答データを扱うことのできる一般化線型モデルである． 他にも，高度に非線型な関係が予期される場合，ノンパラメトリック手法に移る前の簡単な非線型解析としても活躍する． 本稿では BMI と LDL の非線型関係に関する探索的手法として，順序ロジスティック回帰分析を実行する． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析７\n\n\nベイズ階層モデル\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nデータが自然な階層構造を持つ場合，これを取り入れた自然な事前分布を，一つ上の階層に回帰モデルを付け加えることで構成できる． このようなモデルをベイズ階層モデルという． 本稿ではベイズ階層モデルの縮小効果を概観する． 事前知識を構造に関する知識としてモデルに取り入れることでデータによりフィットする尤度構造を獲得することは，データ解析の一つの目標として，（線型）回帰モデルの自然な拡張と理解できる． \n\n\n\n\n\n12/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変量効果と固定効果\n\n\n統一的見解を目指して\n\n\n\nOpinion\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n12/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n点推定における変数選択法は，正則化項の追加によることが多い． これはベイズ推論では \\(0\\) 近傍に大きな確率を持った事前分布を仮定していることに等しい． ベイズの観点から適切な縮小事前分布を用意することで，大きな効果を持つ回帰係数は変えずに， 効果の小さい変数を排除することができる． 一般に LASSO よりも絞って選択してくれることが多い．\nまたベイズ変数選択では，\\(0\\) にアトムを持つ事前分布を用いることで，当該の変数がモデルに含まれる事後確率 (PIP: Posterior Inclusion Probability) を算出することができる． この方法ではモデルの空間を効率的に探索するサンプラーの開発が重要であるが， 近年では効率的なサンプラーが複数提案されている． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms を用いたベイズ重回帰分析\n\n\nBMI データを題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\nベイズ重回帰分析は解析者のデータへの理解を促進する強力な探索的データ解析手法である． このことを brms パッケージと BMI データを用いて例証する． １変数の場合から始め，変数を追加して挙動が変わるのを解釈・検証（残差プロット・事後予測プロット）しながら慎重に進んでいく． 交差検証による事後予測スコア elpd を用いて，データの非線型変換を利用することで，非線型な関係を見出す方法を扱う． ここまで行えば，データの階層化やノンパラメトリックな手法の採用などの次のステップが自然と見えてくるだろう． \n\n\n\n\n\n12/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析５\n\n\n回帰モデルの概観\n\n\n\nBayesian\n\n\nStatistics\n\n\n\nベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する． 分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる． また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析６\n\n\n応答が質的変数の場合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n通常の回帰モデルは応答変数が連続であることが暗黙の仮定となっている． この節では，応答変数が質的変数である場合のモデリングを扱う． 質的変数は順序変数であるか名目変数であるか（順序の構造があるかないか）の峻別が重要である． いずれの場合でも多くのモデルが利用可能であり，その多くが一般化線型モデルの枠組みで統一的に扱うことができる． \n\n\n\n\n\n12/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n英国研究滞在記\n\n\nUniversity College London 訪問と Isaac Newton Institute ワークショップ\n\n\n\nLife\n\n\n\n\n\n\n\n\n\n12/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\nPDMP / 連続時間 MCMC とは 2018 年に以降活発に研究が進んでいる新たな MCMC アルゴリズムである． 実用化を遅らせていた要因として，種々のモデルに統一的な実装が難しく，モデルごとにコードを書き直す必要があったことが挙げられたが， この問題は自動微分の技術と，(Corbella et al., 2022), (Sutton and Fearnhead, 2023) らの適応的で効率的な Poisson 点過程のシミュレーションの研究によって解決されつつある． ここでは (Andral and Kamatani, 2024) の Python パッケージ pdmp_jax とこれに基づく Julia パッケージ PDMPFlux.jl を紹介する． \n\n\n\n\n\n10/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去拡散サンプラー\n\n\nPython によるハンズ・オン\n\n\n\nSampling\n\n\nProcess\n\n\nPython\n\n\n\n(Vargas et al., 2023) の DDS (Denoising Diffusion Sampler) は変分推論のように逆 KL 乖離度を最小化することを通じて，一般の確率分布からのサンプリングを可能にする方法である．今回は 公式の実装 を吟味する． \n\n\n\n\n\n10/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n流体モデル概観\n\n\n大気の Lorenz 96 モデル，流体の Navier-Stokes モデル\n\n\n\nNature\n\n\nJulia\n\n\n\nLorenz’ 63, Lorenz’ 96 とはそれぞれ (Lorenz, 1963), (Lorenz, 1995) によって導入された大気モデルである． 前者はバタフライ効果の語源ともなった，最初に特定されたカオス力学系でもある． Navier-Stokes 方程式は流体の運動を記述する方程式である． これらはいずれもデータ同化・軌道推定技術のベンチマークとして用いられている． ここでそれぞれのモデルの数学的性質と Julia を通じたシミュレーションの方法をまとめる． \n\n\n\n\n\n10/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析のハンズオン\n\n\nMCMCpack パッケージとオリジナル Stan コードを使って\n\n\n\nBayesian\n\n\nStatistics\n\n\nMCMC\n\n\nR\n\n\n\n政治学における理想点解析とは，項目反応モデルを用いて裁判官や国会議員などの価値判断基準や「イデオロギー」を定量化・視覚化する方法である． ここでは既存のパッケージを用いて簡単に理想点解析を行う方法から始め， 自分で Stan コードを書いてモデルを推定する方法を紹介する． その際に最も重要な理想点モデルの性質として，識別可能性 の議論がある． これが保たれていないと，モデルの事後分布は多峰性を持ってしまい，推定をするたびに結果が異なったり，統計量の長期間平均が \\(0\\) になってしまったりしてしまう． \n\n\n\n\n\n10/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ分散分析のモデル解析\n\n\n心理学実験を題材として\n\n\n\nBayesian\n\n\nStatistics\n\n\nR\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられてきた． しかし，古典的な ANOVA 手法である F-検定や t-検定は，データの一側面に着目した手法である．\nベイズ的な解析手法は，これを補完する多くの探索的な手法を提供してくれる． 特に，データに潜む極めて微妙な消息も捉えることが可能になることをここでは強調したい． このような微妙な消息を最初から想定することは難しく，ベイズの探索的な性格が真に可能にするデータ解析事例があると言えるかもしれない．\nそこで本稿では (van den Bergh et al., 2020) に基づいて，「社会的なロボット」に関する心理学実験のデータに対するベイズ ANOVA (Gelman, 2005), (Rouder et al., 2012) 解析のモデルケースを紹介する． 少しずつデータの構造が見えてくる過程が，読者にうまく提示できることを願う． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析４\n\n\nアンケートデータとデータ統合\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n応募法 (voluntary sampling) や多くのウェブアンケートは，確率標本抽出に該当しない．このような場合でも母集団に関する補助情報がある限り，バイアスを軽減し推定精度を高めることができる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析３\n\n\n標本調査データと欠測データの扱い\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n標本調査において欠測はつきものである．観測単位が欠測している場合 (unit nonresponse)，call-back や follow-up などの調査を行うか，それができない場合は 荷重校正 (calibration weighting) が可能である．一方で，項目が欠測している場合 (item nonresponse)，代入法 (imputation) が用いられる． \n\n\n\n\n\n9/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析１\n\n\n分散分析\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる． しかし古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない． これらの問題点を解決策としてベイズの方法を導入し，ベイズ ANOVA，ベイズ推論とモデル比較が ANOVA の発展として得られることをみる． この拡張は，ANOVA の線型モデルとしての解釈を通じてなされ，ANOVA の「同じ係数を共有するクタスタ構造の特定手法」というより広い理解へ導かれる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズデータ解析２\n\n\n平均処置効果の推定とセミパラメトリック法\n\n\n\nStatistics\n\n\n\n人間を対象にする介入の研究では，介入の前後で変化があったかが争点となる． この変化の量を表す平均処置効果 (ATE) を，なるべくモデルを仮定せずどこまで識別できるかが多くの場合論点になる． この際の枠組みが潜在結果モデルである． したがって，操作変数法などの交絡統制法がある一方で，ATE の推定にはモデルの誤特定に強いセミパラメトリックな手法が要請される． 一般化推定方程式，一般化モーメント法，経験尤度法などの方法がある． 本稿ではこれらの推定量を同一の枠組みの下でまとめる． 推定量の分散を求めるためには漸近論のほかにブートストラップ法も用いられる． \n\n\n\n\n\n9/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\nベイズ統計におけるモデル選択／モデル平均のためには，異なる次元を持つパラメータ空間を往来するような MCMC サンプラーが必要になる． \n\n\n\n\n\n9/22/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\nR\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能ですが，本稿では特に R からの利用方法をまとめます．\n\n\n\n\n\n9/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ生存時間解析\n\n\n生存曲線のベイズ階層モデルによる外挿\n\n\n\nBayesian\n\n\nMCMC\n\n\nStatistics\n\n\n\n本稿では生存時間解析の代表的なモデルを概観する． 特に医療技術評価への応用では，打ち切りデータを最もよく外挿できるハザードモデルが探索され，ベイズ推定が有効な方法としてよく選択される． 本稿では特に表現力の高い競合リスクモデルとして polyhazard model を紹介し，ベイズ推定の困難さを議論する．\n次稿ではこのモデルを Zig-Zag サンプラーでベイズ推定する方法を紹介する． \n\n\n\n\n\n9/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n最適輸送とは何か？\n\n\n歴史と概観\n\n\n\nP(X)\n\n\nSurvey\n\n\n\n最適輸送問題は変分法の黎明期に提案された変分問題の１つであるが，その発展は確率論の成熟を待つ必要があった．現代では多くの非正則な空間上に幾何学的な量を定義する普遍的な手法として理解されてから，多くのフィールズ賞受賞者を輩出する最も活発な分野の１つとなっている．ここまでの発展の歴史を本記事では概観したい．\n\n\n\n\n\n9/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n雑音除去過程\n\n\nOrnstein-Uhlenbeck 過程の時間反転\n\n\n\nProcess\n\n\nSampling\n\n\n\n拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．特に OU 過程の時間反転は雑音除去過程 (Denoising Diffusion) といい，サンプリングに利用されている．デノイジングスコアマッチングでは，時間反転に Hyvärinen スコアが出現することを利用してデータ分布のスコアを推定する．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．\n\n\n\n\n\n8/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSkilling-Hutchinson の跡推定量\n\n\n\n\n\n\nProbability\n\n\nFunctional Analysis\n\n\n\nSkilling-Hutchinson の跡推定量は，跡の計算 \\(O(d^2)\\) を \\(O(d)\\) に落とすことができる Monte Carlo 法である．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．今回は PyTorch を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n階層モデル再論\n\n\n多変量解析から機械学習へ\n\n\n\nStatistics\n\n\nKernel\n\n\nProbability\n\n\nBayesian\n\n\n\n本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n特異値分解\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n行列の特異値分解とは，正方行列の直交対角化を一般の行列に拡張したものである．特異値を大きいものから \\(r\\) 個選ぶことで，Hilbert-Schmidt ノルムの意味で最適な \\(r\\)-階数近似が構成できる．このことは主成分分析に応用を持つ．\n\n\n\n\n\n8/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nカーネル法の概観\n\n\n半正定値カーネルから距離学習まで\n\n\n\nKernel\n\n\n\nカーネル法とは，半正定値カーネルを用いてデータを Hilbert 空間内に埋め込むことで，非線型な変換を行う統一的な手法である．再生核 Hilbert 空間の理論により，写した先における内積は，半正定値カーネルの評価を通じて効率的に計算できるため，無限次元空間上での表現に対する tractable な手段を提供する．適切な半正定値カーネルを用いることで，データの「類似度」を定義することができる．本稿では半正定値カーネルの理論と距離学習法を扱う．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nフローベース模型による条件付き生成\n\n\n誘導からフローマッチングへ\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散模型は拡張性にも優れており，条件付けが容易である．現状は誘導付き拡散によってこれが実現されるが，連続的な条件付き生成のために，フローマッチングなる方法も提案された．\n\n\n\n\n\n8/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n離散空間上の拡散確率モデル\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\nDeep\n\n\nSampling\n\n\nNature\n\n\n\n画像と動画に関してだけでなく，言語，化学分子の構造生成など，拡散模型が応用されるドメインは拡大を続けている．これは連続空間上にとどまらず，言語やグラフなどの離散空間上でも拡散模型が拡張理解され始めたことも大きい．本稿では，離散データを連続潜在空間に埋め込むことなく，直接離散空間上に拡散模型をデザインする方法をまとめる．その利点はドメインごとに過程を設計できる柔軟性にあると言える．\n\n\n\n\n\n8/09/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network 訓練の加速\n\n\nPyTorch について調べたこと\n\n\n\nDeep\n\n\nPython\n\n\n\n前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n\n\n\n\n\n8/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルによる事後分布サンプリング\n\n\nLangevin 拡散の時間反転を用いたシミュレーションベースのサンプリング法\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルから始まるフロー学習手法は，画像と動画に関して 2024 年時点で最良の性能を誇る． これは統計的に言えば事後分布からの近似的サンプリングを実行していることに相当する． 近似的ではなく，正確に２つの分布を補間するような拡散過程を推定するためには Schrödinger 橋がある． Schrödinger 橋については 次稿 に譲るとし，本稿ではサンプラーとしての拡散モデルを復習する． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散モデルからシュレディンガー橋へ\n\n\nIterative Proportional Fitting アルゴリズムについて\n\n\n\nProcess\n\n\nSampling\n\n\nP(X)\n\n\n\n拡散モデルは「データ過程をノイズに還元する Langevin ダイナミクスを時間反転する」という発想に基づいており，画像と動画の生成・条件付き生成タスクに関して 2024 年時点で最良の方法の１つである． この発想を正確なサンプリング法に昇華するためには，(Deming and Stephan, 1940) の Iterative Proportional Fitting アルゴリズムを用いることができる． この方法は拡散モデルによる条件付き生成の加速法として (Shi et al., 2022) によって提案された． こうして得る拡散過程は Schrödinger Bridge とも呼ばれ，エントロピー最適輸送と深い関わりを持つ． \n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデルのノイズ対照学習\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は PyTorch を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\nnormflows によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．今回は normflows を用いて，正規化流の実装の概要を見る．\n\n\n\n\n\n8/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nスコアマッチング\n\n\nJAX によるハンズオン\n\n\n\nDeep\n\n\nPython\n\n\n\nスコアマッチングとは，データ分布のスコアを学習すること中心に据えた新たな生成モデリングへのアプローチである．ここでは，JAX を用いた実装を取り扱う．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n今回は PyTorch を用いて， Ho et. al. [NeurIPS 33(2020)] による DDPM (Denoising Diffusion Probabilistic Model) の実装の概要を見る．DDPM は拡散模型の最初の例の１つであり，ノイズからデータ分布まで到達するフローを定める拡散過程（雑音除去過程）を，データをノイズにする拡散過程の時間反転として学習する方法である．画像や動画だけでなく，離散空間上でタンパク質などの構造生成でも state of the art の性能を示すモデルである．\n\n\n\n\n\n8/02/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n非線型な次元縮約法の概観\n\n\n最古にして最難のタスクと多様体学習\n\n\n\nDeep\n\n\nNature\n\n\nStatistics\n\n\nGeometry\n\n\n\n生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．\n\n\n\n\n\n7/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n表現学習と非線型独立成分分析\n\n\n「データ理解」に向けた深層潜在変数モデル\n\n\n\nDeep\n\n\n\n表現学習，非線型独立成分分析など，「生成」以外の潜在変数模型の応用法を横断してレビューする．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．\n\n\n\n\n\n7/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\nPyTorch によるハンズオン\n\n\n\nDeep\n\n\nSampling\n\n\nPython\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n7/28/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n信念伝搬アルゴリズム\n\n\n変分平均場近似\n\n\n\nBayesian\n\n\nNature\n\n\nComputation\n\n\n\n信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる． \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nサンプリングとは何か\n\n\nMonte Carlo 法が人類にもたらした「力」\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\nサンプリング，または Monte Carlo 法は，現代の統計学と機械学習において必要不可欠な道具となっている．それは一体どうしてだろうか？初まりは Los Alamos 研究所にて，確率変数をシミュレーションすることが可能になったことは，人類に何をもたらしただろうか？ \n\n\n\n\n\n7/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（後編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．前編ではこの現象がなぜ起こるかに関して考察した．ここでは代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラーのサブサンプリングによるスケーラビリティ\n\n\n大規模モデル・大規模データに対する MCMC を目指して\n\n\n\nMCMC\n\n\nComputation\n\n\nJulia\n\n\nSampling\n\n\n\nZig-Zag サンプラーは，その非対称なダイナミクスにより，収束が速くなることが期待されている MCMC 手法である．それだけでなく，対数尤度の勾配に対する不偏推定量をサブサンプリングにより構成することで，ベイズ推論においてサンプルサイズに依らない一定のコストで効率的な事後分布からのサンプリングが可能である．\n\n\n\n\n\n7/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n理想点解析・多次元展開法・項目応答理論\n\n\n空間モデルの特定を目指して\n\n\n\nBayesian\n\n\nStatistics\n\n\n\n理想点解析とは，政治学においてイデオロギーを定量化する方法論である．この手法は多くの側面を持ち，多次元展開法 (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．初めに政治学における理想点解析の目的と役割を概観し，続いて多次元展開法と項目反応理論の２つの観点から理想点解析を眺める． \n\n\n\n\n\n7/16/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n大規模な不均衡データに対するロジスティック回帰（前編）\n\n\n離散時間 MCMC から連続時間 MCMC へ\n\n\n\nBayesian\n\n\nComputation\n\n\nJulia\n\n\nMCMC\n\n\nStatistics\n\n\n\nロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが 大規模な不均衡データ である．この記事では，この現象がなぜ起こるかに関する考察を与え，次稿で代替手法として Zig-Zag サンプラーがうまくいくことをみる．\n\n\n\n\n\n7/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLangevin Dynamics の多項式エルゴード性\n\n\nErgodic Lower Bounds\n\n\n\nProcess\n\n\n\n目標分布の裾が重ければ重いほど，Langevin 拡散過程の収束は遅くなる．本記事ではその様子を，平衡分布との全変動距離について，定量的に評価する．\n\n\n\n\n\n7/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程によるサンプリング\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nJulia\n\n\nMCMC\n\n\n\nZig-Zag サンプラー定義とエルゴード性を解説する．続いて，Zig-Zag サンプラーは非対称なダイナミクスを持つために，従来の MCMC よりも速い収束が期待されることを，MALA との比較でみる．最後に，Zig-Zag サンプラーの実装に用いたパッケージとその利用方法を示す．\n\n\n\n\n\n7/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nLévy 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nLévy 過程は独立定常増分な Feller-Dynkin 過程のことである．このクラスの過程は，Brown 運動と純粋跳躍過程の独立和として表現される．これが Lévy-Ito 分解であるが，純粋跳躍過程の全てが複合 Poisson 過程かといえばそうではない．Gamma 過程は任意の区間上で無限回跳躍するが，有界変動である（B 型の Lévy 過程）．Cauchy 過程は有界変動ではなく，跳躍部分は発散するが，無限に強いドリフトによってこれを打ち消している（C 型の Lévy 過程）．これらの過程を例とし，YUIMA パッケージを通じてシミュレーションを行いながら，Lévy の特性量 \\((A,\\nu,\\gamma)\\) の変化が，Lévy 過程の見本道にどのような変化をもたらすかの直感的理解を試みる．\n\n\n\n\n\n7/01/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分子動力学法\n\n\n数学者のための統計力学３：物理に寄り添った Monte Carlo 法\n\n\n\nNature\n\n\nComputation\n\n\n\n本質的に Metropolis 法がサンプリング法であるならば，MD 法は \\(N\\)-体問題に対する数値解法であると言える．しかし，Hamiltonian Monte Carlo は元々 Monte Carlo 法と MD 法との融合を目指したものであること，Event-Chain Monte Carlo 法も MD 法における古典的手法の輸入と理解できること，Langevin 動力学も正準集団に対する MD 法と捉えられることを考えると，尽きぬ計算テクニックの源泉であると言える．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson 過程を見てみよう\n\n\nYUIMA パッケージを用いたシミュレーションを通じて\n\n\n\nProcess\n\n\nSampling\n\n\nStan\n\n\nYUIMA\n\n\nR\n\n\n\nPoisson 点過程とは，各集合内に入る点の数が Poisson 分布によって定まるランダムな点からなる測度である．これを一般化した複合 Poisson 点過程のクラスは，互いに素な集合に入る点の個数が独立に決まるようなランダム測度を網羅するクラスになる．Lévy 過程のジャンプ測度は複合 Poisson 点過程になる．\n\n\n\n\n\n6/29/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学とスピングラス\n\n\n誤り訂正符号を題材にして\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\n広い範囲の設定の下では，種々のベイズ推定は，スピングラスの planted ensemble における基底状態探索や平衡物理量の計算と同一視できる．この対応が歴史上最初に発見されたのが，誤り訂正符号の設定においてであった．特にこの対応の下で，ハイパーパラメータの正確な特定に成功したベイズ最適な推定とは，西森ライン上のスピングラス系の熱力学として捉えられる．西森ライン上ではスピングラス相は出現せず，数々の魅力的な性質が成り立つ．EM アルゴリズムはこれを利用してハイパーパラメータの真値と MAP 推定を同時に行うアルゴリズムと見れる．\n\n\n\n\n\n6/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ統計学と統計物理学\n\n\nスパース符号の復元を題材として\n\n\n\nBayesian\n\n\nNature\n\n\nInformation\n\n\n\nノイズ付きで観測された情報を復元するデノイジング問題は，ベイズ推定問題として扱える．これを統計力学の観点からランダムエネルギーモデルとして解析することで，データ数無限大の極限における振る舞いを理解できる．一般に，ベイズ統計モデルはスピングラスモデルと同一視することができ，その漸近論（特に比例的高次元極限）に閾値現象が出現することはスピングラス系の常磁性相とスピングラス相の相転移と深い対応を持つ．\n\n\n\n\n\n6/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR による記号微分入門\n\n\ncalculus パッケージ入門\n\n\n\nR\n\n\nYUIMA\n\n\n\ncalculus は c++ を通じて数値微分・数値積分を高速に実行するパッケージである．同時に，ほとんどの演算を，純粋に記号操作により実行する機能も持つ．一般の多変数関数を，記号のまま微分，Taylor 展開することができる． \n\n\n\n\n\n6/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n総合研究大学院大学５年一貫博士課程のすすめ\n\n\n統計科学コース（統計数理研究所）\n\n\n\nOpinion\n\n\nLife\n\n\n\n統数研での五年一貫制博士課程（正確には，総合研究大学院大学統計科学コース）を紹介します．同期が居ないこと（がありえること）が最も人を選ぶ点でしょう．しかし，そのことが気にならない場合は，まさに理想郷のような研究環境が整っていると言えるでしょう．\n\n\n\n\n\n5/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\nMCMC\n\n\nSampling\n\n\nPoster\n\n\n\n物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n\n\n\n5/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．広範なクラスの確率微分方程式のシミュレーションが可能です．今回はそのような確率過程の汎函数の漸近展開に基づく計算方法を紹介します．確率変数の期待値を近似するのに Monte Carlo 法は普遍的な方法ですが，漸近展開が用いられる場合，その計算時間は比較にならないほど速くなります．\n\n\n\n\n\n5/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nStan\n\n\n\nStan は MCMC や変分推論などのベイズ推論エンジンを備えた，統計モデリングのための確率的プログラミング言語です．CLI，Python，Julia，R など，主要な言語からパッケージを通じて利用可能です．本稿では Stan 言語の基本をまとめます．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n\n\n\nStan\n\n\nR\n\n\nYUIMA\n\n\nProcess\n\n\n\nR パッケージ yuima は確率過程のモデリングとその統計推測を可能にするフレームワークです．従来の i.i.d. 仮定の下での統計推測から，一般の確率過程の統計推測への橋渡しを目標としています．鋭意開発中のパッケージですが，すでに広範なクラスの確率微分方程式のシミュレーションが可能です．本稿では基本的な使い方を紹介します．\n\n\n\n\n\n5/17/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nSDE のベイズ推定入門\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\nProcess\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nYUIMA\n\n\nBayesian\n\n\n\nR パッケージ YUIMA を用いた SDE のベイズ推定に，バックエンドとして Stan による HMC の実装を用いる方法を模索する．Stan は C++ を用いる独立した確率プログラミング言語で移植性は高いが，それ故 YUIMA からこれを用いる際に，専用のインターフェイスを考える必要が生じる．\n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\nBayesian\n\n\nMCMC\n\n\nR\n\n\nStan\n\n\nStatistics\n\n\n\nbrms はベイズ階層モデリングを，確率的プログラミング言語 Stan をエンジンとして行う R パッケージである． 基本的な線型回帰から固定・変量効果の追加まで極めて簡単に実行できる，大変実用的なパッケージである． 本稿では，brms の基本的な使い方とその実装を紹介する． その中で混合効果モデルについてレビューをする． ランダム効果の追加は縮小推定などの自動的な正則化を可能とする美点がある一方で，係数の不偏推定やロバスト推定に拘る場合はこれを避ける判断もあり得る． \n\n\n\n\n\n5/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n志学・応用数学\n\n\n統計的推論のダイナミクスとその変分原理\n\n\n\nOpinion\n\n\nLife\n\n\n\n現代の統計・機械学習を確率的ダイナミクスとして理解し，同時にこれを説明する変分原理を明らかにすることが，これからの応用数学の１つの有望な方向だと考える．統計や機械学習のモデルに物理学的な解釈を付加したり，ベイズ推論としての解釈や事前分布を明瞭化したりすることで，双方に資すると同時に，共通理解の足場となる数学を目指したいものである．\n\n\n\n\n\n5/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nRoberts and Tweedie (1996) Exponential Convergence of Langevin Distributions and Their Discrete Approximations\n\n\n論文メモ\n\n\n\nReview\n\n\n\nRoberts and Tweedie [Bernoulli 2(1996) 341-363] は MALA (Metropolis-Adjusted Langevin Algorithm) の指数エルゴード性を議論したもの． \n\n\n\n\n\n4/23/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nDuane+ (1987) Hybrid Monte Carlo\n\n\n論文メモ\n\n\n\nReview\n\n\n\nDuane et al. [Phys. B 195(1987) 216-222] は Hamiltonian Monte Carlo 法の提案論文と目されているが，その実は全く違う文脈の中で提案された．場の量子論における (Parisi and Wu, 1981) の確率過程量子化や小正準法にように，正確に物理的過程をシミュレーションする必要はないのである．これを Metropolis 法の提案核に使うことを提案した論文である． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nTartero and Krauth (2023) Concepts in Monte Carlo Sampling\n\n\n論文メモ\n\n\n\nReview\n\n\n\nTartero and Krauth [arXiv (2023)] は１次元の非調和振動子を題材に，分子動力学法，Metropolis 法，consensus，lifting，連続時間 MCMC，thining などの計算手法と計算技術を，疑似コード付きで解説している． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis+ (1953) Equation of State Calculations by Fast Computing Machines\n\n\n論文メモ\n\n\n\nReview\n\n\n\nMetropolis et. al. [The Journal of Chemical Physics 21(1953) 1087-1092] は初の MCMC（乱歩 Metropolis 法）を，対称分布を Gibbs の正準分布として，“modified Monte Carlo scheme” という名前の下で提案し，剛円板モデルのシミュレーションに応用した論文である．重点サンプリングを “Monte Carlo method” と呼び，「目標分布から直接サンプルを生成できるために提案分布と目標分布とのズレによる性能劣化がない」ことを美点として挙げている．この手法は後の (Hastings, 1970) による改良と併せて，Metropolis-Hastings 法と呼ばれるようになる． \n\n\n\n\n\n4/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nアンサンブルと熱力学極限\n\n\n数学者のための統計力学２：小正準集団・正準集団・大正準集団\n\n\n\nNature\n\n\n\n統計力学の理論で用いられる３つのアンサンブルと，熱力学極限の概念を定義し，これらが熱力学極限において同等な理論を与えることを見る．統計力学の中心的トピックの１つである相転移も，熱力学極限における物理量の解析性の喪失として定義される．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計力学における基本的な模型の総覧\n\n\n数学者のための統計力学１：Ising 模型とスピングラス\n\n\n\nNature\n\n\nDeep\n\n\n\n統計力学の場面設定を数学的に理解することを試みる．統計力学の代表的なモデルを，古典粒子系と格子系とに分けて紹介する．現代の計算科学の最前線は，剛円板モデルや \\(XY\\) モデルをはじめとした，２次元のモデルであると言える．\n\n\n\n\n\n4/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n計算とは何か\n\n\n計算とサンプリングのはざまにある Monte Carlo 法\n\n\n\nComputation\n\n\nSampling\n\n\nOpinion\n\n\n\n数値実験と LLM とはいずれもシミュレーションに使えるが，用いる形式が違う（数字と文字）．これにより，物理的な用途と社会的な用途とに別れている．この形式の違いを超克するのが機械学習の悲願であるとするならば，計算とはなんだろうか？ Monte Carlo 法とはシミュレーションと計算を架橋する存在であるならば，今後どのような貢献ができるのであろうか？ \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nPeters and de With (2012) Rejection-Free Monte Carlo Sampling for General Potentials\n\n\n論文メモ\n\n\n\nReview\n\n\n\nPeters and de With [Phys. E 85(2012) 026703] は Metropolis 法による棄却-採択の代わりに，衝突により方向を変える粒子を想定することで，効率的な Monte Carlo 法を実行することを目指した．ただの event-driven な molecular dynamics と違い，一般の滑らかなポテンシャルに適用可能である点が革新的である．しかし，粒子系のポテンシャルは常に和の形で表されるように，一般の PDMP に基づいた連続時間 MCMC 手法も，適用可能なモデルの範囲が限定されている点が難点である (Nemeth and Fearnhead, 2021)． \n\n\n\n\n\n4/06/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nButkovsky and Veretennikov (2013) On Asymptotics for Vaserstein Coupling of Markov Chains\n\n\n論文メモ\n\n\n\nReview\n\n\nKernel\n\n\n\nButkovsky and Veretennikov [Stochastic Processes and Their Applications 123(2013) 3518-3541] は対称とは限らないエルゴード的な Markov 連鎖の収束レートを，カップリングの方法を用いて導出した仕事． \n\n\n\n\n\n4/04/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\nDeep\n\n\nNature\n\n\nSampling\n\n\n\n確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．\n\n\n\n\n\n3/30/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n待ち時間の Markov 過程のエルゴード性\n\n\nRecurrent Events and Residual Waiting Time\n\n\n\nProcess\n\n\n\n繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．待ち時間の分布が一次の積率を持つとき，過程はエルゴード的であり，全変動距離は多項式速度で収束する．待ち時間の分布の裾が重いほど，収束は遅くなる．\n\n\n\n\n\n3/25/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率核という概念\n\n\nデータ解析の営みを確率空間の圏上で理解する\n\n\n\nProbability\n\n\nKernel\n\n\nProcess\n\n\nFunctional Analysis\n\n\nP(X)\n\n\n\n確率核という概念は現状あまりポピュラーではないと思われるが，数学的にいえば，Markov 過程論，確率論，さらにはデータ解析の中心に据えられるべき中心概念であると言えるかもしれない．例えば，カーネル法とは確率核に沿った埋め込みである．MCMC の性質も，本質的に確率核の性質が決定する．また確率核は，確率空間の圏の射となる．このように，多くのデータ解析手法の中核に位置する数学的本体たる「確率核」への入門を目指すのが本記事である．\n\n\n\n\n\n3/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nこれからはじめるベイズ機械学習\n\n\n所信表明を兼ねて\n\n\n\nBayesian\n\n\nAI\n\n\nOpinion\n\n\n\n\n\n\n\n\n\n3/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nグラフニューラルネットワーク\n\n\n位相的データ解析の旗手\n\n\n\nDeep\n\n\n\nグラフニューラルネットワークは CNN や Transformer などの従来のニューラルネットワークアーキテクチャを拡張したクラスである．\n\n\n\n\n\n3/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n半導体入門\n\n\n現代社会の「魔素」が見えるように\n\n\n\nNature\n\n\nSurvey\n\n\n\n半導体デバイスの基本原理と製造方法を物理から理解することを目指す．\n\n\n\n\n\n2/26/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nトランスフォーマー\n\n\n深層生成モデル１\n\n\n\nDeep\n\n\nAI\n\n\n\n2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．\n\n\n\n\n\n2/20/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVAE：変分自己符号化器\n\n\n深層生成モデル３\n\n\n\nDeep\n\n\nSampling\n\n\n\n変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである． 従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである． 学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある． \n\n\n\n\n\n2/18/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\nDeep\n\n\nSampling\n\n\nP(X)\n\n\n\nGauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\nDeep\n\n\nProcess\n\n\nSampling\n\n\n\n拡散模型はノイズからデータ分布まで到達するフローを生成する拡散過程を，データをノイズにする拡散過程の時間反転として学習する方法である．大規模なニューラルネットワークを用いて学習した場合，画像と動画に関しては 2024 年時点で最良の性能を誇る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\nDeep\n\n\nSampling\n\n\n\n確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．\n\n\n\n\n\n2/14/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論３\n\n\n変分ベイズ推論\n\n\n\nBayesian\n\n\nComputation\n\n\nPython\n\n\n\n確率的グラフィカルモデルの汎用推論手法である変分 Bayes アルゴリズムを解説する．変分 Bayes 推論とは，事後分布を指定した分布族の中で，KL-距離が最も小さくなるように近似する手法をいう．この分布族として，種々のパラメトリック分布を仮定したり，平均場近似を採用したりすることで，種々の変分 Bayes アルゴリズムが得られる．\n\n\n\n\n\n2/12/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いた統計解析\n\n\n実践編（回帰と分類）\n\n\n\nBayesian\n\n\nKernel\n\n\nPython\n\n\n\n数学者のために，Gauss 過程を用いた統計解析を，回帰と分類の２例紹介する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGauss 過程を用いたベイズ推論\n\n\n理論編\n\n\n\nBayesian\n\n\nKernel\n\n\nProcess\n\n\n\nGauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための深層学習概観\n\n\n歴史と導入\n\n\n\nDeep\n\n\nSurvey\n\n\n\n数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nGAN：敵対的生成ネットワーク\n\n\n深層生成モデル２\n\n\n\nDeep\n\n\nSampling\n\n\n\n数学者のために，深層生成モデルの先駆けである GAN を概観する．\n\n\n\n\n\n2/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論２\n\n\nEM アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，第２回は変分 Bayes アルゴリズムの特殊な場合とみれる EM アルゴリズムに注目する．\n\n\n\n\n\n2/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n今回は番外編と称し，ChatGPT の元となる大規模言語モデルである GPT の概要を解説する．\n\n\n\n\n\n2/07/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n変分推論１\n\n\nK-平均アルゴリズム\n\n\n\nComputation\n\n\nPython\n\n\n\n本稿では，\\(K\\)-平均アルゴリズム によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次稿 で，\\(K\\)-平均アルゴリズムの model-aware な一般化として EM アルゴリズム を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す． \n\n\n\n\n\n2/03/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n純粋跳躍過程の生成作用素と区分的確定的 Markov 過程\n\n\nジャンプと確定的な動きによる新たな MCMC 手法\n\n\n\nProcess\n\n\nSampling\n\n\nR\n\n\n\nPDMP は，A 型の Lévy 過程を含む，複合 Poisson 点過程が定めるジャンプと決定論的なドリフトのみからなる確率過程のクラスをいう．この性質をよく理解するために，まずは，有界なレートを持つ純粋に跳躍のみで動く過程の生成作用素を調べる．確率核 \\(\\mu\\) とレート \\(\\lambda\\) という２つのパラメータは，それぞれ各地点からのジャンプ先を定める確率核と，ジャンプの起こりやすさを表す．最後に，現状もっとも活発に研究されている２つの PDMP である Zig-Zag Sampler と Bouncy Particle Sampler とを紹介する．\n\n\n\n\n\n1/31/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第５節から第８節 (pp. 96-126) を通じ，統計学検定への入門も兼ねて，推測統計学のうち統計的仮説検定の基礎を学ぶ．\n\n\n\n\n\n1/24/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル２\n\n\n統計力学の観点から\n\n\n\nBayesian\n\n\nComputation\n\n\nNature\n\n\n\n数学者のために，マルコフネットワークの古典的な例と，統計力学の考え方を概観する．\n\n\n\n\n\n1/19/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n教科書第３章第１節から第４節 (pp. 73-96) を通じ，統計学検定への入門も兼ねて，推測統計学の基礎を学ぶ．\n\n\n\n\n\n1/11/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n統計的学習理論１\n\n\nPAC 学習\n\n\n\nFoundation\n\n\n\n統計的機械学習には，「汎化」に価値を置く独特の決定理論的な枠組みが存在する．特に，第一義的には経験リスクを最小化すること，より正確には経験リスク最小化と正則化とをバランスよく目指す「構造的リスク最小化」が広く機械学習のモデリング指針として採用されている．\n\n\n\n\n\n1/10/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n測度の正則性 | Regularities of Measures on Topological Spaces\n\n\n\n\n\n\nFunctional Analysis\n\n\n\n位相空間上の測度の正則性に関連する概念をまとめる．\n\n\n\n\n\n1/05/2024\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n分岐過程\n\n\n\n\n\n\nProcess\n\n\n\n分岐過程の定義と歴史，性質についてまとめる．\n\n\n\n\n\n12/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode での執筆環境\n\n\nLaTeX, Overleaf, Quarto, Julia, R, Python, … etc.\n\n\n\nLifestyle\n\n\n\nVSCode での LaTeX 環境構築に関するページ．\n\n\n\n\n\n12/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n教科書第2章第4節 (pp. 42-72)を通じ，決定木を用いた意思決定分析の方法を学んだ．機械学習では，不確実性の下での意思決定支援をするエキスパートシステム作成を目指した，確率的グラフィカルモデルという分野が絶賛発展中である．決定木からベイジアンネットワークへの進化を遂げた現代の技術の広がりを，世界銀行報告書，内閣府日本経済白書，そして法科学への応用事例を通じて学んだ．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のための確率的グラフィカルモデル１\n\n\nベイジアンネットワークとマルコフネットワーク\n\n\n\nBayesian\n\n\nComputation\n\n\n\nPGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．\n\n\n\n\n\n12/20/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターを用いたサンプリング | About SMC Samplers\n\n\nテンパリングを通じたもう一つの万能サンプラー\n\n\n\nParticles\n\n\nMCMC\n\n\nSurvey\n\n\n\n粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n\n\n\n\n\n12/14/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターの実装 | Particles Package\n\n\nNumPy と SciPy で粒子フィルターを実装する\n\n\n\nParticles\n\n\nPython\n\n\n\nPythonを用いて粒子フィルターを実装する方法を，Nicolas Chopinによるparticlesパッケージを参考に解説する．\n\n\n\n\n\n12/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n教科書第１章第２〜３節 (pp. 14-30) までの内容を自分たちで一から解いた．特に，第３節の内容で，Bayes の定理を自分たちの手だけで，公理のみから導出した．加えて，Bayes 統計学と筆者の専門である Bayes 計算の分野紹介をした．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ計算とは何か | About Bayesian Computation\n\n\n\n\n\n\nBayesian\n\n\nComputation\n\n\nSampling\n\n\nSurvey\n\n\n\n「ベイズ統計学」は一大トピックであるが，「ベイズ計算」という分野があることはそれほど周知のことではない．しかし，ベイズ統計学は常に「計算が困難で実行が難しい」という問題と共にあってきたのであり，ここ30年のベイズ統計学の興隆は計算機の普及と効率的なベイズ計算法の発明に因る．モデル・データがいずれも大規模で複雑になっていく現代において，ベイズの枠組みも柔軟に取り入れた更なる統計計算法の発展が欠かせない．\n\n\n\n\n\n12/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Mental Health Issues\n\n\n\n\n\n\nLife\n\n\n\nメンタルヘルスの世界を知らざるを得なくなった人と，「自分は今後どうなるのか」という不安に苛まれている人へ．\n\n\n\n\n\n12/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き期待値の測度論的基礎付け\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n「有界」測度と「有限」測度 | Between ‘Bounded’ Measures and ‘Finite’ Measures\n\n\n\n\n\n\nFunctional Analysis\n\n\n\nThey are the same mathematical object. Let’s step back to view the big picture.\n\n\n\n\n\n12/02/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n粒子フィルターとは何か\n\n\n非線型フィルタリング手法としての粒子フィルタ\n\n\n\nParticles\n\n\nSurvey\n\n\nComputation\n\n\n\n粒子フィルターは今年で誕生30周年を迎える「万能」非線型フィルタリング手法である．相関を持つ粒子系によって分布を逐次的に近似する遺伝的アルゴリズムであり，多くの科学分野にまたがる応用を持つと同時に，数理的対象としても豊かな構造を持つ．その発明の歴史と今後の研究方向を紹介する．\n\n\n\n\n\n11/25/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n確率測度の変換則\n\n\nGamma 分布と Beta 分布を例に\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/24/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nWhispter API を通じて日本語音声を書き起こす方法\n\n\n\n\n\n\nLifestyle\n\n\nPython\n\n\n\nWhispter API は25MBまでの音声ファイルしか書き起こししてくれないので，長時間の音声ファイルを一度に書き起こしてもらうには工夫が必要．\n\n\n\n\n\n11/23/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n教科書第1章第1節(pp.1-14)までの内容を，確率論の公理と数学の考え方を補足しながら，自分の言葉で導出しなおした．\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n正規標本の標本平均と標本分散が独立であることの証明\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/22/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n条件付き正規分布からのシミュレーション法\n\n\n\n\n\n\nSampling\n\n\nProbability\n\n\n\n\n\n\n\n\n\n11/17/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Category (nLab) | 紹介\n\n\n\n\n\n\nProbability\n\n\nFoundation\n\n\n\n「総合的確率論」アプローチの基本概念に Markov 圏の概念がある．これは可測空間を対象とし，確率核を射として得る圏のことである．nLab の Markov category のページを翻訳して紹介する．\n\n\n\n\n\n11/11/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2013) Mean field simulation for Monte Carlo integration\n\n\n\n\n\n\nReview\n\n\n\n前文を翻訳\n\n\n\n\n\n11/09/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n書籍紹介 Del Moral (2004) Feynman-Kac Formulae\n\n\n\n\n\n\nReview\n\n\n\nFeynman-Kac モデルという物理モデルを定義し，逐次モンテカルロ法（粒子フィルター）をその Monte Carlo シミュレーション法として位置付けて解説した書籍である． 例として挙げられるトピックも物理学のものが多く，書籍のスタイルも物理学書のそれである． ここでは 1.1 節 “On the Origins of Feynman-Kac and Particle Models” の抄訳を通じて内容を概観したい． \n\n\n\n\n\n11/08/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n数学者のためのカーネル法概観\n\n\nカーネル PCA と SVM を例として\n\n\n\nKernel\n\n\n\n数学者のために，カーネル法によるデータ解析が何をやっているのかを抽象的に説明する．カーネルとは対称な２変数関数であり，これを用いてデータ点を，データ空間上の関数に変換することで非線型変換を獲得するための道具である．\n\n\n\n\n\n11/07/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n相関粒子系の社会実装\n\n\n\n\n\n\nParticles\n\n\nOpinion\n\n\n\n相関粒子系がどのように社会で活躍出来るか？という問いに対する１つの案として，「ビジネスモデルのモデル」が提示される．ここでは「状態空間モデル」の構造を人間社会に見つけることが肝要になる．\n\n\n\n\n\n11/06/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺の人生を変えたもの Top5\n\n\n\n\n\n\nLife\n\n\n\n10月以前と10月以降で過ごし方が大きく変わった その要因のうち最も大きいと思われるもの５つを紹介\n\n\n\n\n\n11/05/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto はじめて良かったこと\n\n\n\n\n\n\nLifestyle\n\n\n\nQuarto は TeX のような使用感で，数式とコードが併存する文章を書き，１つのソースファイルから PDF, HTML, Word, Reveal.js, PowerPoint などの多様な形式に出力できる次世代の執筆環境である．TeX, RStudio, Jupyter Notebook のいずれかに慣れている人であれば，極めて手軽に Quarto を使うことができる．筆者が用意した テンプレート から簡単に始めることができる．公式の ギャラリー も参照．\n\n\n\n\n\n11/04/2023\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR の概観\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR は統計計算のための言語です．その基本的なデータ型と，「属性」を通じた実装，そしてオブジェクト志向の構造について解説します．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（３）リスト\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\nR におけるリストは，独自の index $ を持った構造体であり，Python の dictionary， Perl の hash table に似ている．$ は S3 の機能で，S4 は @ である．これはリストが本質的に R の実装の深いところに存在するデータ型だからである．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\nR（２）ベクトル\n\n\n\n\n\n\nComputation\n\n\nR\n\n\n\n統計言語 R において，ベクトルは極めて基本的なデータ構造であり，行列・配列・リストはいずれも追加の属性を持ったベクトルと理解できる．本稿では，ベクトルの構成法，単項演算，二項演算，indexing などを解説する．\n\n\n\n\n\n5/07/2021\n\n\n司馬博文\n\n\n\n\n\n\n\n\n\n\n\n\n俺のための Julia 入門（０）スタートアップガイド\n\n\nインストール・特徴・パッケージ管理\n\n\n\nJulia\n\n\n俺のためのJulia入門\n\n\n\nJulia はスクリプト言語とコンパイル言語の良いとこどりを目指して開発された言語である．Matlab のような数学的な記述ができ，C のような実行速度を保ち，Python のような汎用性を持ち，Shell のようなモジュール性を持つ．\n\n\n\n\n\n9/05/2020\n\n\n司馬博文\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReferences\n\nAndral, C., and Kamatani, K. (2024). Automated techniques for efficient sampling of piecewise-deterministic markov processes.\n\n\nCorbella, A., Spencer, S. E. F., and Roberts, G. O. (2022). Automatic Zig-Zag Sampling in Practice. Statistics and Computing, 32(6), 107.\n\n\nDeming, W. E., and Stephan, F. F. (1940). On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. The Annals of Mathematical Statistics, 11(4), 427–444.\n\n\nGelman, A. (2005). Analysis of variance—why it is more important than ever. The Annals of Statistics, 33(1), 1–53.\n\n\nHastings, W. K. (1970). Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika, 57(1), 97–109.\n\n\nLorenz, E. N. (1963). Deterministic nonperiodic flow. Journal of Atmospheric Sciences, 20(2), 130–141.\n\n\nLorenz, E. N. (1995). Predictability: A problem partly solved. In Seminar on predictability, 4-8 september 1995. ECMWF.\n\n\nNemeth, C., and Fearnhead, P. (2021). Stochastic gradient markov chain monte carlo. Journal of the American Statistical Association, 116(533), 433–450.\n\n\nParisi, G., and Wu, Y. (1981). PERTURBATION THEORY WITHOUT GAUGE FIXING. Scientia Sinica, 24(4), 483–.\n\n\nRouder, J. N., Morey, R. D., Speckman, P. L., and Province, J. M. (2012). Default bayes factors for ANOVA designs. Journal of Mathematical Psychology, 56(5), 356–374.\n\n\nShi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. (2022). Conditional simulation using diffusion Schrödinger bridges. In J. Cussens and K. Zhang, editors, Proceedings of the thirty-eighth conference on uncertainty in artificial intelligence,Vol. 180, pages 1792–1802. PMLR.\n\n\nSutton, M., and Fearnhead, P. (2023). Concave-Convex PDMP-based Sampling. Journal of Computational and Graphical Statistics, 32(4), 1425–1435.\n\n\nvan den Bergh, D., Doorn, J. van, Marsman, M., Draws, T., Kesteren, E.-J. van, Derks, K., … Wagenmakers, E.-J. (2020). A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP. L’Année Psychologique, 120, 73–96.\n\n\nVargas, F., Grathwohl, W. S., and Doucet, A. (2023). Denoising Diffusion Samplers. In The eleventh international conference on learning representations."
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-piecewise-deterministic-markov-process",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-piecewise-deterministic-markov-process",
    "title": "動き出す次世代サンプラー",
    "section": "1.1 PDMP: Piecewise Deterministic Markov Process",
    "text": "1.1 PDMP: Piecewise Deterministic Markov Process"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#piecewise-deterministic-markov-process",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#piecewise-deterministic-markov-process",
    "title": "動き出す次世代サンプラー",
    "section": "1.2 Piecewise Deterministic Markov Process",
    "text": "1.2 Piecewise Deterministic Markov Process"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.2 例（申請者開発のパッケージより）",
    "text": "1.2 例（申請者開発のパッケージより）\n\n\n\n\n\n(Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n(Bierkens et al., 2019)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-13申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-13申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.2 例 1/3（申請者開発のパッケージより）",
    "text": "1.2 例 1/3（申請者開発のパッケージより）\n\n\n aaa bbb ccc\n\n\n\n\n(Bierkens et al., 2019)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-13-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-13-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.9 例 1/3 （申請者開発のパッケージより）",
    "text": "1.9 例 1/3 （申請者開発のパッケージより）\n\n\n\n\n\n(Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n(Bierkens et al., 2019)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-23-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-23-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.10 例 2/3 （申請者開発のパッケージより）",
    "text": "1.10 例 2/3 （申請者開発のパッケージより）\n\n\n\n\n\n(Michel et al., 2020)\n\n\n\n\n\n\n(Vasdekis and Roberts, 2023)\n\n\n\n\n\n\n(Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-33-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-33-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.11 例 3/3 （申請者開発のパッケージより）",
    "text": "1.11 例 3/3 （申請者開発のパッケージより）\n\n\n\n\n\n(Vasdekis and Roberts, 2023)\n\n\n\n\n\n\n(Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#モンテカルロ法小史-12",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#モンテカルロ法小史-12",
    "title": "動き出す次世代サンプラー",
    "section": "1.3 モンテカルロ法小史 1/2",
    "text": "1.3 モンテカルロ法小史 1/2"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#モンテカルロ法小史-22",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#モンテカルロ法小史-22",
    "title": "動き出す次世代サンプラー",
    "section": "1.4 モンテカルロ法小史 2/2",
    "text": "1.4 モンテカルロ法小史 2/2"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#langevin-monte-carlo-の何がダメなのか",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#langevin-monte-carlo-の何がダメなのか",
    "title": "動き出す次世代サンプラー",
    "section": "1.4 Langevin Monte Carlo の何がダメなのか？",
    "text": "1.4 Langevin Monte Carlo の何がダメなのか？\n\n\n\n\n\n\n\n\n物理のくびき\n\n\n\n拡散過程は孤立系の平衡ダイナミクス．\n必ずしもこれを模倣する必要はない．\nコーヒーに砂糖を入れたあと孤立させておく人はいない．\n\n\n\n\n\n\n\n\n\nアルゴリズムのくびき\n\n\n\n拡散過程は正確にシミュレートできない．\n 離散化をし，数値誤差を Metropolis-Hastings ステップで補正する．\n アルゴリズムが重くなる．"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の何が良いのか",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の何が良いのか",
    "title": "動き出す次世代サンプラー",
    "section": "1.6 PDMP の何が良いのか？",
    "text": "1.6 PDMP の何が良いのか？\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 収束が速い (Diaconis, 2013), (Andrieu and Livingstone, 2021)\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#hmc-との関係",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#hmc-との関係",
    "title": "動き出す次世代サンプラー",
    "section": "HMC との関係",
    "text": "HMC との関係\n\nMetropolis-Hastings ステップでは，尤度の比が 1 ならば棄却されない．\nそこで尤度の等高線をなぞることを考える．\n運動量をランダムにサンプリングすることでエルゴード性を担保する．\n\nただし尤度の等高線をなぞることは数値計算の問題になり難しいが，ハイパーパラメータをうまくチューニングすることでほとんど独立なサンプルを得ることができる．\n従来的には MCMC の１つとみれるが，ダイナミクスを複雑にした PDMP と見るべきかもしれない．\n\n尤度の等高線をなぞるダイナミクス\n\n→ 尤度の幾何情報を自然に取り入れた動きが可能\n\n運動量をリフレッシュするタイミングについての示唆？\n\n\n\n\n\n学生研究発表会 司馬博文 「動き出す次世代サンプラー：区分確定的モンテカルロ」"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#参考文献",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#参考文献",
    "title": "動き出す次世代サンプラー",
    "section": "3.1 参考文献",
    "text": "3.1 参考文献\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nBierkens, J., Grazzi, S., Kamatani, K., and Roberts, G. O. (2020). The boomerang sampler. Proceedings of the 37th International Conference on Machine Learning, 119, 908–918.\n\n\nBierkens, J., Grazzi, S., Meulen, F. van der, and Schauer, M. (2023). Sticky PDMP Samplers for Sparse and Local Inference Problems. Statistics and Computing, 33(1), 8.\n\n\nBouchard-Côté, A., Vollmer, S. J., and Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. Journal of the American Statistical Association, 113(522), 855–867.\n\n\nMichel, M., Durmus, A., and Sénécal, S. (2020). Forward event-chain monte carlo: Fast sampling by randomness control in irreversible markov chains. Journal of Computational and Graphical Statistics, 29(4), 689–702.\n\n\nVasdekis, G., and Roberts, G. O. (2023). Speed up Zig-Zag. The Annals of Applied Probability, 33(6A), 4693–4746."
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#拡散過程の何がダメなのか",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#拡散過程の何がダメなのか",
    "title": "動き出す次世代サンプラー",
    "section": "1.5 拡散過程の何がダメなのか？",
    "text": "1.5 拡散過程の何がダメなのか？\n\n\n\n\n\n\n\n\n物理のくびき\n\n\n\n拡散過程は孤立系の平衡ダイナミクス\n例：コーヒーの中に入れた砂糖粒子\n必ずしもこれを模倣する必要はない．\n例：コーヒーに砂糖を入れたあと混ぜずに見つめている人はいない\n\n\n\n\n\n\n\n\n\nアルゴリズムのくびき\n\n\n\n拡散過程は正確にシミュレートできない．\n 離散化をし，数値誤差を Metropolis-Hastings ステップで補正する．\n アルゴリズムが重くなる．"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の何が難しいのか",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の何が難しいのか",
    "title": "動き出す次世代サンプラー",
    "section": "1.7 PDMP の何が難しいのか？",
    "text": "1.7 PDMP の何が難しいのか？\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 問題ごとにアルゴリズムを設計する必要  汎用パッケージがない\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#初の-pdmp-汎用パッケージ",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#初の-pdmp-汎用パッケージ",
    "title": "動き出す次世代サンプラー",
    "section": "1.8 初の PDMP 汎用パッケージ",
    "text": "1.8 初の PDMP 汎用パッケージ\n\n\nPython パッケージ（Charly Andral） \npip install pdmp-jax\n\nJulia パッケージ（申請者開発） \n] add PDMPFlux"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html",
    "href": "posts/2024/Slides/IRT-ZigZag.html",
    "title": "動き出す次世代サンプラー",
    "section": "",
    "text": "モンテカルロ法に用いられる PDMP の例．発表者開発の PDMPFlux.jl パッケージからの出力．\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n\nMarkov 連鎖\n\n\n\n\n\n\n拡散過程\n\n\n\n\n\n\nPDMP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびき\n\n\n\n\n拡散過程は孤立系の平衡ダイナミクス\n例：コーヒーの中に入れた砂糖粒子\n必ずしもこれを模倣する必要はない．\n例：コーヒーに砂糖を入れたあと混ぜずに見つめている人はいない\n\n\n\n\n\n\n\n\n\nアルゴリズムのくびき\n\n\n\n\n拡散過程は正確にシミュレートできない．\n 離散化をし，数値誤差を Metropolis-Hastings ステップで補正する．\n アルゴリズムが重くなる．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 収束が速い (Diaconis, 2013), (Andrieu and Livingstone, 2021)\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 問題ごとにアルゴリズムを設計する必要  汎用パッケージがない\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない\n\n\n\n\n\n\n\n\n\n\nPython パッケージ（Charly Andral） \npip install pdmp-jax\n\nJulia パッケージ（申請者開発） \n] add PDMPFlux\n\n\n\n\n\n\n\n\n\n\n(Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n(Bierkens et al., 2019)\n\n\n\n\n\n\n(Bierkens et al., 2020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Michel et al., 2020)\n\n\n\n\n\n\n(Vasdekis and Roberts, 2023)\n\n\n\n\n\n\n(Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#区分確定的マルコフ過程-pdmp",
    "href": "posts/2024/Slides/IRT-ZigZag.html#区分確定的マルコフ過程-pdmp",
    "title": "動き出す次世代サンプラー",
    "section": "",
    "text": "モンテカルロ法に用いられる PDMP の例．発表者開発の PDMPFlux.jl パッケージからの出力．\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n\nMarkov 連鎖\n\n\n\n\n\n\n拡散過程\n\n\n\n\n\n\nPDMP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理のくびき\n\n\n\n\n拡散過程は孤立系の平衡ダイナミクス\n例：コーヒーの中に入れた砂糖粒子\n必ずしもこれを模倣する必要はない．\n例：コーヒーに砂糖を入れたあと混ぜずに見つめている人はいない\n\n\n\n\n\n\n\n\n\nアルゴリズムのくびき\n\n\n\n\n拡散過程は正確にシミュレートできない．\n 離散化をし，数値誤差を Metropolis-Hastings ステップで補正する．\n アルゴリズムが重くなる．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 収束が速い (Diaconis, 2013), (Andrieu and Livingstone, 2021)\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 問題ごとにアルゴリズムを設計する必要  汎用パッケージがない\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない\n\n\n\n\n\n\n\n\n\n\nPython パッケージ（Charly Andral） \npip install pdmp-jax\n\nJulia パッケージ（申請者開発） \n] add PDMPFlux\n\n\n\n\n\n\n\n\n\n\n(Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n(Bierkens et al., 2019)\n\n\n\n\n\n\n(Bierkens et al., 2020)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Michel et al., 2020)\n\n\n\n\n\n\n(Vasdekis and Roberts, 2023)\n\n\n\n\n\n\n(Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#pdmp-のシミュレーション",
    "href": "posts/2024/Slides/IRT-ZigZag.html#pdmp-のシミュレーション",
    "title": "動き出す次世代サンプラー",
    "section": "2 PDMP のシミュレーション",
    "text": "2 PDMP のシミュレーション"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#参考",
    "href": "posts/2024/Slides/IRT-ZigZag.html#参考",
    "title": "動き出す次世代サンプラー",
    "section": "参考",
    "text": "参考\n\n\nAndral, C., and Kamatani, K. (2024). Automated techniques for efficient sampling of piecewise-deterministic markov processes.\n\n\nAndrieu, C., and Livingstone, S. (2021). Peskun–Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario. The Annals of Statistics, 49(4), 1958–1981.\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nBierkens, J., Grazzi, S., Kamatani, K., and Roberts, G. O. (2020). The boomerang sampler. Proceedings of the 37th International Conference on Machine Learning, 119, 908–918.\n\n\nBierkens, J., Grazzi, S., Meulen, F. van der, and Schauer, M. (2023). Sticky PDMP Samplers for Sparse and Local Inference Problems. Statistics and Computing, 33(1), 8.\n\n\nBouchard-Côté, A., Vollmer, S. J., and Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. Journal of the American Statistical Association, 113(522), 855–867.\n\n\nCorbella, A., Spencer, S. E. F., and Roberts, G. O. (2022). Automatic Zig-Zag Sampling in Practice. Statistics and Computing, 32(6), 107.\n\n\nDiaconis, P. (2013). Some things we’ve learned (about Markov chain Monte Carlo). Bernoulli, 19(4), 1294–1305.\n\n\nLewis, P. A. W., and Shedler, G. S. (1979). Simulation of nonhomogeneous poisson processes by thinning. Naval Research Logistics Quarterly, 26(3), 403–413.\n\n\nMichel, M., Durmus, A., and Sénécal, S. (2020). Forward event-chain monte carlo: Fast sampling by randomness control in irreversible markov chains. Journal of Computational and Graphical Statistics, 29(4), 689–702.\n\n\nSen, D., Sachs, M., Lu, J., and Dunson, D. B. (2020). Efficient posterior sampling for high-dimensional imbalanced logistic regression. Biometrika, 107(4), 1005–1012.\n\n\nVasdekis, G., and Roberts, G. O. (2023). Speed up Zig-Zag. The Annals of Applied Probability, 33(6A), 4693–4746.\n\n\n\nHMC との関係\n\nMetropolis-Hastings ステップで，尤度の比が 1 ならば棄却されない．\nそこで尤度の等高線をなぞることを考える．\n運動量をランダムにサンプリングすることでエルゴード性を担保する．\n\nただし尤度の等高線をなぞることは数値計算の問題になり難しいが，ハイパーパラメータをうまくチューニングすることでほとんど独立なサンプルを得る．\n\nPDMP は HMC の一般化：運動量がリフレッシュされるまでは決定論的に動く\nHMC は PDMP の洗練：尤度の等高線をなぞることで多峰性に強い"
  },
  {
    "objectID": "posts/2024/Slides/Master.html",
    "href": "posts/2024/Slides/Master.html",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "",
    "text": "2023 年度は (Giné and Nickl, 2021) の第２章を扱った\n\n\nここでは Gauss 過程の上限に関する集中不等式を取り上げる．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n体積測度 \\mu が等しい可測集合のうち，球が最小の周長を持つ．\n\n\n\n\n\n\n\nBorel 可測集合 A\\subset\\mathbb{R}^n に関して，\\epsilon-閉近傍 を次のように定める： \nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\mid d(x,A)\\le t\\right\\},\\qquad t&gt;0.\n\n実は A_\\epsilon は Borel 可測とは限らないが，Lebesgue 可測ではある．\n\n\n\n\n\n\n\n\n\n\n古典的等周不等式 (Schmidt, 1948)-(Lévy, 1951)\n\n\n\nn-次元球面 S^n\\subset\\mathbb{R}^{n+1} に関して，A\\subset S^n を Borel 可測，C を同体積の（測地）球とすると， \n\\mu(C_\\epsilon)\\le\\overline{\\mu}(A_\\epsilon),\\qquad\\epsilon&gt;0.\n\n\n\n\n\n\n\n\n\n\n\n\n(Giné and Nickl, 2021, p. 31) 定理 2.2.3\n\n\n\n\\gamma_n を \\mathbb{R}^n 上の標準正規分布とする．A\\subset\\mathbb{R}^n を Borel 可測， \nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\qquad a\\in\\mathbb{R},u\\in\\mathbb{R}^n\\setminus\\{0\\},\n を同体積の affine 半空間 とすると， \n\\gamma_n(H_a+\\epsilon B^n)\\le\\overline{\\gamma_n}(A_\\epsilon+\\epsilon B^n),\\qquad\\epsilon&gt;0.\n\n\n\n\\mathbb{R}^n だけでなく \\mathbb{R}^\\infty 上でも成り立つ．半径 \\sqrt{m} の n+m 次元球面 S^{n+m} 上の一様分布の，最初の n 次元周辺分布は，m\\to\\infty の極限で正規分布に収束する (Poincaré, 1912)： \n(\\mathrm{pr}_{1:n})_*\\mathrm{U}{\\sqrt{m}S^{n+m}}\\Rightarrow\\operatorname{N}_n(0,I_n).\n\n\n\n\n\n\n\n\n\n\n(Borell, 1975)-(Sudakov and Tsirel’son, 1974)\n\n\n\n\\{X_t\\}_{t\\in T} を可分な中心 Gauss 過程で，ほとんど確実に上限 \\|X\\|_\\infty が有限であるとする．このとき，\\|X\\|_\\infty の中央値 M に関して，1 \n\\operatorname{P}\\biggl[\\biggl|\\|X\\|_\\infty-M\\biggr|&gt;u\\biggr]\\le\\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right),\\qquad u&gt;0,\\sigma^2:=\\sup_{t\\in T}\\mathrm{V}[X_t].\n\n\n\n同様の命題を平均値の周りに関しても示せる．係数 2 が前につくものは (Gross, 1975) による正規分布に関する対数 Sobolev 不等式から導ける．\n\n\n\n\n\n\n\n\n\n(Chatterjee, 2007)\n\n\n\nCurie-Weiss 模型の Hamiltonian \nH^n(x):=-\\frac{1}{2n}\\sum_{i,j=1}^nx_ix_j-h\\sum_{i=1}^nx_i,\\qquad x\\in\\{\\pm1\\}^n,h\\in\\mathbb{R},\n が定める Boltzmann-Gibbs 分布 \n\\pi^n(x)\\,\\propto\\,e^{-\\beta H^n(x)},\\qquad\\beta&gt;0,\n と 磁化密度 m^n(x):=\\overline{x} に関して， \n\\pi^n\\left[\\biggl|m^n-m^*\\biggr|\\le\\frac{\\beta}{n}+\\frac{t}{\\sqrt{n}}\\right]\\le2\\exp\\left(-\\frac{t^2}{4(1+\\beta)}\\right).\n\n\n\n証明は Stein の方法による．\n\n\n\n\n\n\n\n\n\n\n\n磁化密度のサンプリング\n\n\n\nHamiltonian H^n は磁化密度 m^n の二次関数 \nH^n(m)=-n\\left(\\frac{1}{2}m^2+hm\\right),\n \n\\pi^n(m)\\propto e^{-\\beta H^n(m)}.\n\n配置空間 \\Omega:=\\{\\pm1\\}^n 上の一様な酔歩が（中心化された）磁化の空間 (\\mathbb{R},\\overline{\\pi}^n) 上に定める MH 法は，高温領域では Gauss 分布に対する Langevin 拡散に n\\to\\infty で弱収束する： \ndY_t=-2l(h,\\beta)Y_t\\,dt+\\sigma(h,\\beta)\\,dB_t.\n\n\n\n\n\n\n\n\n\n\n(Bierkens and Roberts, 2017)\n\n\n\nMH 法の収束は O(n) のスケーリングで動く．すなわち生成作用素 \nL^nf:=n\\biggr(P^nf-f\\biggl),%\\qquad f\\in\\cD(L^n),\n が Langevin 拡散に収束する．\n一方で Lifted MH 法は O(n^{1/2}) のスケーリングで収束する： \nL^nf:=\\sqrt{n}\\biggr(P^nf-f\\biggl),%\\qquad f\\in\\cD(L^n).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Lefted MH Turitsyn et al., 2011)\n\n\n\n状態空間を２つに分け，目標分布を等分配する： \n\\widetilde{\\mathbb{R}}:=\\mathbb{R}\\times\\{\\pm1\\},\\quad\\widetilde{\\pi}:=\\pi\\otimes\\frac{1}{2}.\n \\mathbb{R} 上の１つの遷移核 Q から，\\mathbb{R}\\times\\{+1\\} 上と \\mathbb{R}\\times\\{-1\\} 上とで異なる遷移核 \\widetilde{Q}^\\pm を作る構成を リフティング という．\nこのとき 歪釣り合い条件 を満たすように作る： \n\\pi(x)\\widetilde{Q}^+(x,y)\\propto\\pi(y)\\widetilde{Q}^-(y,x).\n\n\n\n\n\n\n\n\n\n\n(Bierkens and Roberts, 2017)\n\n\n\n\\widetilde{Q}^+ では磁化の増加方向，\\widetilde{Q}^- では減少方向にのみ提案するとする．\nこのとき Lifted MH 法は O(n^{1/2}) のスケーリングで Zig-Zag 過程 に収束する： \nLf(m,\\theta):=\\alpha(h,\\beta)\\theta f'(m,\\theta)+\n \n(\\theta l(h,\\beta)m)_+\\biggr(f(m,-\\theta)-f(m,\\theta)\\biggl).\n\n\n\n\nスケーリングが落ちている\n拡散項がない（定常状態からの逸脱）  アルゴリズムの高速化が伺える\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散過程に対する粒子フィルターが，タイムステップ \\Delta\\to0 の極限でジャンプ付き過程になる様子を調べた (Chopin et al., 2022) について発表．\n同様の内容で学振申請し，不採択A．"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#sec-Nonparametrics",
    "href": "posts/2024/Slides/Master.html#sec-Nonparametrics",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "",
    "text": "2023 年度は (Giné and Nickl, 2021) の第２章を扱った\n\n\nここでは Gauss 過程の上限に関する集中不等式を取り上げる．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n体積測度 \\mu が等しい可測集合のうち，球が最小の周長を持つ．\n\n\n\n\n\n\n\nBorel 可測集合 A\\subset\\mathbb{R}^n に関して，\\epsilon-閉近傍 を次のように定める： \nA_\\epsilon:=\\left\\{x\\in\\mathbb{R}^n\\mid d(x,A)\\le t\\right\\},\\qquad t&gt;0.\n\n実は A_\\epsilon は Borel 可測とは限らないが，Lebesgue 可測ではある．\n\n\n\n\n\n\n\n\n\n\n古典的等周不等式 (Schmidt, 1948)-(Lévy, 1951)\n\n\n\nn-次元球面 S^n\\subset\\mathbb{R}^{n+1} に関して，A\\subset S^n を Borel 可測，C を同体積の（測地）球とすると， \n\\mu(C_\\epsilon)\\le\\overline{\\mu}(A_\\epsilon),\\qquad\\epsilon&gt;0.\n\n\n\n\n\n\n\n\n\n\n\n\n(Giné and Nickl, 2021, p. 31) 定理 2.2.3\n\n\n\n\\gamma_n を \\mathbb{R}^n 上の標準正規分布とする．A\\subset\\mathbb{R}^n を Borel 可測， \nH_a:=\\left\\{x\\in\\mathbb{R}^n\\mid(x|u)\\le a\\right\\},\\qquad a\\in\\mathbb{R},u\\in\\mathbb{R}^n\\setminus\\{0\\},\n を同体積の affine 半空間 とすると， \n\\gamma_n(H_a+\\epsilon B^n)\\le\\overline{\\gamma_n}(A_\\epsilon+\\epsilon B^n),\\qquad\\epsilon&gt;0.\n\n\n\n\\mathbb{R}^n だけでなく \\mathbb{R}^\\infty 上でも成り立つ．半径 \\sqrt{m} の n+m 次元球面 S^{n+m} 上の一様分布の，最初の n 次元周辺分布は，m\\to\\infty の極限で正規分布に収束する (Poincaré, 1912)： \n(\\mathrm{pr}_{1:n})_*\\mathrm{U}{\\sqrt{m}S^{n+m}}\\Rightarrow\\operatorname{N}_n(0,I_n).\n\n\n\n\n\n\n\n\n\n\n(Borell, 1975)-(Sudakov and Tsirel’son, 1974)\n\n\n\n\\{X_t\\}_{t\\in T} を可分な中心 Gauss 過程で，ほとんど確実に上限 \\|X\\|_\\infty が有限であるとする．このとき，\\|X\\|_\\infty の中央値 M に関して，1 \n\\operatorname{P}\\biggl[\\biggl|\\|X\\|_\\infty-M\\biggr|&gt;u\\biggr]\\le\\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right),\\qquad u&gt;0,\\sigma^2:=\\sup_{t\\in T}\\mathrm{V}[X_t].\n\n\n\n同様の命題を平均値の周りに関しても示せる．係数 2 が前につくものは (Gross, 1975) による正規分布に関する対数 Sobolev 不等式から導ける．\n\n\n\n\n\n\n\n\n\n(Chatterjee, 2007)\n\n\n\nCurie-Weiss 模型の Hamiltonian \nH^n(x):=-\\frac{1}{2n}\\sum_{i,j=1}^nx_ix_j-h\\sum_{i=1}^nx_i,\\qquad x\\in\\{\\pm1\\}^n,h\\in\\mathbb{R},\n が定める Boltzmann-Gibbs 分布 \n\\pi^n(x)\\,\\propto\\,e^{-\\beta H^n(x)},\\qquad\\beta&gt;0,\n と 磁化密度 m^n(x):=\\overline{x} に関して， \n\\pi^n\\left[\\biggl|m^n-m^*\\biggr|\\le\\frac{\\beta}{n}+\\frac{t}{\\sqrt{n}}\\right]\\le2\\exp\\left(-\\frac{t^2}{4(1+\\beta)}\\right).\n\n\n\n証明は Stein の方法による．\n\n\n\n\n\n\n\n\n\n\n\n磁化密度のサンプリング\n\n\n\nHamiltonian H^n は磁化密度 m^n の二次関数 \nH^n(m)=-n\\left(\\frac{1}{2}m^2+hm\\right),\n \n\\pi^n(m)\\propto e^{-\\beta H^n(m)}.\n\n配置空間 \\Omega:=\\{\\pm1\\}^n 上の一様な酔歩が（中心化された）磁化の空間 (\\mathbb{R},\\overline{\\pi}^n) 上に定める MH 法は，高温領域では Gauss 分布に対する Langevin 拡散に n\\to\\infty で弱収束する： \ndY_t=-2l(h,\\beta)Y_t\\,dt+\\sigma(h,\\beta)\\,dB_t.\n\n\n\n\n\n\n\n\n\n\n(Bierkens and Roberts, 2017)\n\n\n\nMH 法の収束は O(n) のスケーリングで動く．すなわち生成作用素 \nL^nf:=n\\biggr(P^nf-f\\biggl),%\\qquad f\\in\\cD(L^n),\n が Langevin 拡散に収束する．\n一方で Lifted MH 法は O(n^{1/2}) のスケーリングで収束する： \nL^nf:=\\sqrt{n}\\biggr(P^nf-f\\biggl),%\\qquad f\\in\\cD(L^n).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Lefted MH Turitsyn et al., 2011)\n\n\n\n状態空間を２つに分け，目標分布を等分配する： \n\\widetilde{\\mathbb{R}}:=\\mathbb{R}\\times\\{\\pm1\\},\\quad\\widetilde{\\pi}:=\\pi\\otimes\\frac{1}{2}.\n \\mathbb{R} 上の１つの遷移核 Q から，\\mathbb{R}\\times\\{+1\\} 上と \\mathbb{R}\\times\\{-1\\} 上とで異なる遷移核 \\widetilde{Q}^\\pm を作る構成を リフティング という．\nこのとき 歪釣り合い条件 を満たすように作る： \n\\pi(x)\\widetilde{Q}^+(x,y)\\propto\\pi(y)\\widetilde{Q}^-(y,x).\n\n\n\n\n\n\n\n\n\n\n(Bierkens and Roberts, 2017)\n\n\n\n\\widetilde{Q}^+ では磁化の増加方向，\\widetilde{Q}^- では減少方向にのみ提案するとする．\nこのとき Lifted MH 法は O(n^{1/2}) のスケーリングで Zig-Zag 過程 に収束する： \nLf(m,\\theta):=\\alpha(h,\\beta)\\theta f'(m,\\theta)+\n \n(\\theta l(h,\\beta)m)_+\\biggr(f(m,-\\theta)-f(m,\\theta)\\biggl).\n\n\n\n\nスケーリングが落ちている\n拡散項がない（定常状態からの逸脱）  アルゴリズムの高速化が伺える\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散過程に対する粒子フィルターが，タイムステップ \\Delta\\to0 の極限でジャンプ付き過程になる様子を調べた (Chopin et al., 2022) について発表．\n同様の内容で学振申請し，不採択A．"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#sec-Convergence",
    "href": "posts/2024/Slides/Master.html#sec-Convergence",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "2 2024 年度前期：Markov 過程の収束",
    "text": "2 2024 年度前期：Markov 過程の収束\n\n\n\n2024 年度前期は (Kulik, 2018) の第３章を扱った\n\n\nここでは連続時間 Markov 過程の劣指数エルゴード性のドリフト条件による特徴付けを取り上げる．\n\n2.1 離散時間のエルゴード定理\n\n\n\n\n\n\n指数エルゴード定理\n\n\n\nMarkov 連鎖 X の確率核 P:E\\to E は次の２条件を満たすとする：\n\nドリフト条件：ある関数 V:E\\to[1,\\infty) と定数 \\gamma\\in(0,1),K\\ge0 が存在して \n  PV\\le\\gamma V+K.\n  \n局所 Dobrushin 条件：任意の c&gt;0 に対して， \n  \\sup_{(x,y)\\in V^{-1}([1,c])^2}\\|P(x,-)-P(y,-)\\|_\\mathrm{TV}&lt;2.\n  \n\nこのとき P は指数エルゴード的：ただ一つの確率分布 \\mu\\in\\mathcal{P}(E) が存在し \n\\|P^n(x,-)-\\mu\\|_\\mathrm{TV}\\le C_1e^{-C_2n}\\biggr(V(x)+(\\mu|V)\\biggl),\\qquad x\\in E,n\\ge1.\n\n\n\n\n\n2.2 連続時間の劣指数エルゴード定理\n\n\n\n\n\n\nステートメント\n\n\n\nE を Polish距離空間，X を Feller-Dynkin 過程とする．連続関数 V:E\\to[1,\\infty) が存在して 後述の２条件 を満たすならば，任意の T&gt;0 に対して定数 C&gt;0 が存在して次が成り立つ： \n\\|P^t(x,-)-P^t(y,-)\\|_\\mathrm{TV}\\le C\\frac{V(x)+V(y)}{\\lambda(t)},\\qquad x,y\\in E,t\\ge T.\n \n\\lambda(t):=\\Phi^{-1}(t),\\Phi(u):=\\int^u_1\\frac{ds}{\\phi(s)}.\n\n\n\nこの V は ドリフト関数 ともいい，エルゴード性証明の鍵を握る．\n証明法は (Kulik, 2018) が扱う skelton 連鎖 X_n:=X_{hn}\\;(h&gt;0,n=1,2,\\cdots) に帰着する方法と，再起過程 (regeneration process) を用いた (Hairer, 2021) による直接的方法がある．\n\n\n2.3 成立条件\n\n\n\n\n\n\n\n条件１：ドリフト条件\nある K\\in\\mathbb{R} と全射かつ単調増加な狭義凹関数 \\phi:\\mathbb{R}_+\\to\\mathbb{R}_+ が存在して \nV(X_t)-Kt+\\int^t_0\\phi(V(X_s))\\,ds\n は任意の x\\in E に関して \\operatorname{P}_x-優マルチンゲールである．\n条件２：局所 Dobrushin 条件\n任意の c\\ge1 に関して下部集合 V^{-1}([1,c]) はコンパクトで，ある h&gt;0 が存在して P^h は V^{-1}([1,c]) 上局所 Dobrushin である： \n\\sup_{(x,y)\\in B_c}\\|P^h(x,-)-P^h(y,-)\\|_\\mathrm{TV}&lt;2,\n \nB_c:=\\left\\{(x,y)\\in E^2\\mid V(x)+V(y)\\le c\\right\\}.\n\n\n\n\n\n\n\n2.4 多項式収束域での比較\n\n\n\n\n\n\n\n\n(Hairer, 2021)\n\n\n\nポテンシャル U\\in C_p^\\infty(\\mathbb{R}^d) が定める Langevin 拡散過程 \ndX_t=-\\nabla U(X_t)\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t\n\n次が \\kappa\\ge0 について成り立つならば指数エルゴード的：2 \n\\limsup_{\\lvert x\\rvert\\to\\infty}\\left(\\nabla U(x)\\,\\middle|\\,\\frac{x}{\\lvert x\\rvert^{\\kappa+1}}\\right)&lt;0.\n \\kappa\\in(-1,0) で劣指数エルゴード的： \n\\|P_t(x,-)-\\mu\\|_\\mathrm{TV}\\le Ce^{\\alpha U(x)}e^{-ct^{\\frac{k}{1-k}}}.\n \\kappa=-1 で多項式エルゴード的．\n\n\n\n\n\n\n\n\n\n(Vasdekis and Roberts, 2022)\n\n\n\nリフレッシュなしの Zig-Zag サンプラー は目標分布の負の対数尤度 U があるコンパクト集合の外で \n\\lvert\\nabla U(x)\\rvert\\ge\\frac{1+\\nu}{\\lvert x\\rvert},\\quad\\nu&gt;0,\n を満たすならば多項式エルゴード的．\nt-分布もこれを満たす： \n\\pi(x)=\\frac{1}{\\pi}\\frac{1}{1+\\lvert x\\rvert^2}\n\n\n\nこの際のレートは Langevin アルゴリズムのもの (Jarner and Tweedie, 2003) よりも速い"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#sec-OT",
    "href": "posts/2024/Slides/Master.html#sec-OT",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "3 2024 年度後期：最適輸送",
    "text": "3 2024 年度後期：最適輸送\n\n\n\n2024 年度後期は (Figalli and Glaudo, 2023) の第２章を扱った\n\n\nここでは (Brenier, 2003) による Euclid コストに関する最適輸送写像の表示を取り上げる．\n\n3.1 最適輸送写像の見つけ方\n\n\n\n\n\n\n\n\n(Monge, 1781) の最適輸送問題\n\n\n\n２つの確率分布 \\mu,\\nu\\in\\mathcal{P}(\\mathbb{R}^d) の間の輸送 T:\\mathbb{R}^d\\to\\mathbb{R}^d \nT_*\\mu=\\nu\n のうち，総じた輸送「コスト」 \n\\min_{T\\in\\mathcal{L}(\\mathbb{R}^d;\\mathbb{R}^d)}\\left(\\int_{\\mathbb{R}^d}\\lvert x-T(x)\\rvert^2\\,d\\mu\\right)^{1/2}.\n を最小にする T を 最適輸送 という．\n\n\nこの T の総輸送コストを 2-Wasserstein 距離 W_2 という．\n\n\n\n\n\n\n\n(Brenier, 1987) の定理\n\n\n\n最適輸送 T:\\mathbb{R}^d\\to\\mathbb{R}^d は次の２条件を満たせば，ただ一つ存在する：\n\n\\mu,\\nu は２次の絶対積率を持つ\nスタートの分布 \\mu は絶対連続\n\n\n\n加えて，ある真凸関数 \\varphi:\\mathbb{R}^d\\to\\mathbb{R}\\cup\\{\\infty\\} に関する勾配として表せる： \nT=\\nabla\\varphi.\n さらにある \\varphi に関して この条件を満たす輸送 T_*\\mu=\\nu は最適輸送のみ．\n\n\n\n\n3.2 Wasserstein 勾配流へ\n実はポテンシャル U に関する Langevin 拡散は，KL 乖離度 \n\\operatorname{KL}(\\rho,e^{-V})=\\int_{\\mathbb{R}^d}\\rho(x)\\log\\frac{\\rho(x)}{e^{-U(x)}}\\,dx\n に関する「最適輸送の繰り返し」と見れる．\n 数学的には Wasserstein 距離空間 (\\mathcal{P}_2(\\mathbb{R}^d),W_2) 上の勾配流として定式化することができる．\n\n\n\n\n\n\nSampling as Optimization\n\n\n\n(Dalalyan, 2017), (Wibisono, 2018), (Korba and Salim, 2022) らにより自覚された，サンプリング法一般を，\\mathcal{P}(\\mathbb{R}^d) 上の様々な幾何構造に関する最適化手法として捉え，これを通じて比較することができるのではないか？という考え方．\n\n\n Markov 過程のエルゴード性に対する新たな視点になり得る．\n\n\n3.3 Sampling as Optimization\nエルゴード性の様子を \\mathcal{P}(\\mathbb{R}^d) 上の幾何で調べることは，実用的な含意も多い．\n\n\n\n\n\n\n\n\n(Wibisono, 2018)\n\n\n\nLangevin Monte Carlo は KL 乖離度の Wasserstein 勾配に関する最急降下法と見れる．\n\n\n\n\n\n\n\n\n(Chopin et al., 2023)\n\n\n\nTempering SMC は，逆向き KL 乖離度に対する鏡像降下法と見れる．\n\n\n\n\n\n\n\n\n\n(Chehab et al., 2024)\n\n\n\n正規化定数を計算する重点サンプリング法において，最適なテンパリング道は \n(1-\\alpha)\\pi_0+\\alpha\\pi,\\qquad\\alpha\\in(0,1)\n の \\{\\alpha_i\\}_{i=1}^n のスケジュールを，Wasserstein 距離に関して等幅になるようにしたものである．\n\n\n\n\n全変動距離は一番強い位相でカップリングとの相性が良い． しかし Wasserstein 距離を初めとした他のカップリング距離に関するアプローチも可能 (Kulik, 2018)．\nこの話は面白すぎる．"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#sec-PDMC-Flux",
    "href": "posts/2024/Slides/Master.html#sec-PDMC-Flux",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "4 現在の取り組み：PDMC パッケージの開発",
    "text": "4 現在の取り組み：PDMC パッケージの開発\n\n\n\n開発したパッケージ PDMPFlux.jl からの出力\n\n\nPDMC は 区分確定的モンテカルロ (Piecewise Deterministic Monte Carlo) の略．MCMC の後継と期待されている．\n\n4.1 興味３分野\n\n\n\n\n\n\n\n\n\n\n学んだ順番\n研究に取り掛かれる順番（予想）\n\n\n\n\nノンパラメトリクス\nSection 1\n２\n\n\nMonte Carlo 法\nSection 2\n１\n\n\nSampling as Optimization\nSection 3\n３\n\n\n\n\n\n\n\n現状：１つ目に取り掛かり，２つ目を勉強中．\n１つ目の研究を始める初手として，パッケージ PDMPFlux.jl を作成．\nできれば学生のうちに３つ目のテーマの勉強までやりたい．\n\n\n\n\n\n\n4.2 Zig-Zag 過程のシミュレーション\n\n\n\n\n\n\n\n\n\n\n\n\nPDMP のシミュレーション\n\n\n\n\n方向転換イベント がいつ起こるかをシミュレート\nイベント発生地点の列を出力3\nイベント間は決定論的に補間\n\n\n\n\n\n\n\n\n\n\n\n\n難点\n\n\n\n方向転換イベント のシミュレーション＝非一様な Poisson 過程の到着時刻のシミュレーション．\nサンプラーのレート関数 \n\\lambda(x,\\theta)=(\\theta|\\nabla U(x))_+,\\qquad i\\in[d],\n から次の生存関数を持つ乱数 T_{n+1} を発生させる必要がある： \n\\operatorname{P}[T_{n+1}\\ge t]=\\exp\\left(-\\int^t_0\\lambda(\\phi_s(Z_{S_n}))\\,ds\\right).\n 右辺の逆関数が必要だが，積分を数値誤差なく実行するのは不可能．\n 問題に合わせた棄却法が使われていた．\n\n\n\n\n\n\n\n4.3 剪定の自動化 (Andral and Kamatani, 2024)\n\n\n\n\n\n\n\n\n(Poisson Thinning Lewis and Shedler, 1979)\n\n\n\nPoisson 過程に対する棄却法． \nm(t)\\le M(t),\\qquad t\\ge0,\n を満たす２つの強度関数 m,M に対して，M から得られた Poisson イベントを確率 \n\\frac{m(t)}{M(t)}\n で採択したものは m から得られた Poisson イベントと同分布である．\n 多項式の M を見つければ良い．\n\n\n\nサンプラーの強度関数 \nm(t):=\\int^t_0\\lambda(\\phi_s(Z_{S_n}))\\,ds\n の上界 M(t) を，前回のイベント位置 Z_{S_n} に応じて自動的に見つける戦略が必要．\n\n\n\n\n\n\n(Andral and Kamatani, 2024) のアルゴリズム\n\n\n\n\nm を自動微分し，[0,t_{\\text{max}}] のグリッド上で勾配を計算\n各グリッド上では両端点と，両端点の接線の交点との３点でバウンド\nt_{\\text{max}}\\to0 の極限で正確な上界になるため，t_{\\text{max}} を適応的に小さく選ぶ\n\n\n\n\n\n\n\n4.4 PDMPFlux.jl パッケージ\nusing PDMPFlux\n\nfunction U_Gauss(x::Vector)  # ポテンシャルを与えるだけで良い\n  return sum(x.^2) / 2\nend\n\ndim = 10  # 次元は与える必要あり\nN_sk, N, xinit, vinit = 100_000, 100_000, zeros(dim), ones(dim)  # ハイパーパラメータ\n\nsampler = ZigZagAD(dim, U_Gauss)  # サンプラーのインスタンス化, AD は自動微分のこと\noutput = sample_skeleton(sampler, N_sk, xinit, vinit)  # PDMP のシミュレーション\nsamples = sample_from_skeleton(sampler, N, output)  # サンプルの抽出\n\ndiagnostic(output)  # サンプラーの診断\njointplot(samples)  # サンプルの可視化"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#sec-perspective",
    "href": "posts/2024/Slides/Master.html#sec-perspective",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "5 これから",
    "text": "5 これから\nパッケージ開発は今後の研究のため．応用もしたいが，更なるサンプラー開発や基礎的な研究の足がかりにしたい．\n\n5.1 現状の PDMC アルゴリズム\n基本的には拡張された空間上で動き，新たな変数 \\theta を 速度 や momentum という： \n(x,\\theta)\\in\\mathbb{R}^d\\times\\Theta,\\qquad\\Theta\\subset\\mathbb{R}^d.\n\n\n\n\n\n\n\n\n\n(Bouncy Particle Sampler Bouchard-Côté et al., 2018)\n\n\n\n\\Theta=S^{d-1} と単位球面上に取る．\n\nイベントが起こったら，速度をサンプリングし直す： \n\\Theta_{S_{n+1}}\\sim\\mathrm{U}(S^{d-1}).\n\nエルゴード性を保つためには定期的な向きの変更が必要（リフレッシュ）． \n\n\n\n\n\n\n\n\n\n\n(Zig-Zag Sampler Bierkens et al., 2019)\n\n\n\n\\Theta=\\{\\pm1\\}^d と「向きの空間」上に取る．\n\nイベントが起こったら，イベントを起こした座標の向きを反転させる： \n\\Theta_{S_{n+1}}^i\\gets(-1)\\times\\Theta_{S_n}^i.\n\n移動方向を 2^d 通りに制限することでエルゴード性が得られる． \n\n\n\n\n\nポテンシャル（負の対数尤度） U の構造を on the fly で学ぶメカニズムが必要．\n\n\n5.2 PDMC アルゴリズムへの基礎的貢献\n\n\n\n\n\n\nPDMC の悲願\n\n\n\nU の構造のみを尊重したアルゴリズムを作る\n 詳細釣り合い条件，歪釣り合い条件などの人工的な対称性を導入しない．\n\n\n\nすると速くなるはず\n対称な MCMC で最速は HMC．本質的に非可逆であり，遠くまで行くメカニズムを持つが，最後に採択-棄却のステップが必要で，backtracking を導入しがち．\n棄却をしない枠組みはどこまで出来るか？\nPDMC も状態空間を拡張する点は HMC に似ているが，Poisson 過程をシミュレーションするために採択-棄却のステップを使うことで，人工的な対称性の導入を回避している．\nいわばスプーンでかき混ぜるギミックの再現．しかしこの枠組みでどこまで人工的な対称性を排除したダイナミクスを作れるかは不明．\n\n 以上の直観を，数理的な知識に変換したい．\n\n\n5.3 離散空間上の MCMC\nPDMC のパッケージもあるので，応用していきたい．\n\n\n\n\n\n\n\n\nZig-Zag within Gibbs (Sachs et al., 2023)\n\n\n\nモデルパラメータ \\beta\\in\\mathbb{R}^p に加えて離散パラメータ \\gamma\\in\\Gamma が存在する場合， \n\\beta|\\gamma,\\qquad\\gamma|\\beta\n の前者を PDMP でサンプリングし，後者の離散更新はある強度 \\eta&gt;0 を持った Poisson 点過程に従って混ぜる．\n\n\nクラスタリングの問題では \\Gamma=[K]^n となる．「すべてのラベルの割り当て方の全体」\\Gamma 上で MCMC をやるアプローチ？\n\n\n\n\n\n\n\n適応的確率近傍＋informed MCMC (Liang et al., 2023)\n\n\n\n回帰における変数選択の場合，潜在指示変数とすれば \n\\Gamma:=2^p=\\{0,1\\}^p\n 上のサンプリングを考えることになる．\n\n適応的かつランダム的に「近傍」を決定\n近傍の中から採択率の高いものを提案\n\nの２段階 MH 法が SOTA．\n\n\n\n\n\n\n5.4 ノンパラメトリクスへ\n\nより現実的な尤度へ\n複雑な尤度に対するサンプラーを考えるのが一番面白い． 階層化も面白いが，Gauss 過程やカーネル法などのノンパラメトリクスは特に面白いモデリングが可能． 計算の部分で貢献できることがあるのでは？\nベイズ逆問題\nOT を逆に解く問題も (Stuart and Wolfram, 2020), (Chiu et al., 2022) で出てきた． \n\\Phi:T\\mapsto\\{(x_i,T(x_i))\\}_{i=1}^n\n 逆像を推定する，というのはフローマッチングと同じ定式化になる (Lipman et al., 2023), (Tong et al., 2024)． その学習は \\mathcal{P}(\\mathbb{R}^d)^{[0,1]} 上の Dirichlet 積分の最適化と等価になる (Isobe et al., 2024)． サンプラーにより \\mathcal{P}(\\mathbb{R}^d) 上の最適化をする方法もある？Feynman-Kac flow と関係するなら SMC でも可能？\n\n確率過程＋関数解析が自分の強みと好みを最も活かせる？"
  },
  {
    "objectID": "posts/2024/Slides/Master.html#footnotes",
    "href": "posts/2024/Slides/Master.html#footnotes",
    "title": "総研大５年一貫博士課程・中間評価",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nこの設定では \\|X\\|_\\infty は連続分布をもち，M は一意に定まる．↩︎\nDU(x)=O(\\lvert x\\rvert^{\\kappa}) よりも遅い減衰で，内側を向いていれば良い．↩︎\nこのため物理学では Event-chain Monte Carlo (Bernard et al., 2009) と呼ばれる．↩︎"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html",
    "href": "posts/2024/Slides/ZigZagSampler.html",
    "title": "Zig-Zag サンプラー",
    "section": "",
    "text": "区分確定的 Makrov 過程を用いた，連続時間 MCMC 手法の１つ\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nPDMP または 連続時間 MCMC と呼ばれる手法群の一つ\nランダムな時刻にランダムな動きをする以外は，確定的な動き\n\n\n\n\nZig-Zag サンプラーの軌跡\n\n\n\n\n\nPDMP （Piecewise Deterministic Markov Process，区分確定的マルコフ過程）\n\n\n\n代表的な３つの PDMP. Animated by (Grazzi, 2020)\n\n\n今回は Zig-Zag サンプラー（中央）に注目する．\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の軌道\n\n\n\n\n\n正確なシミュレーションが可能な連続過程である．\nシミュレーションが簡単で，次の２つだけ考えれば良い：\n\nランダムな時刻 （非一様 Poisson 点過程）\nランダムな変化 （Zig-Zag の場合はこれも無し）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の軌道\n\n\n\n\n\n軌跡自体が目標分布 \\pi に従う\n\n軌跡上で線積分する or 好きな間隔で切り出してサンプルとする\n\n非対称 Metropolis-Hastings 法の scaling limit として導かれる\n\nZig-Zag は Curie-Weiss 模型の Lifted MH 法から (Bierkens and Roberts, 2017)\nBPS は分子動力学法の模型から (Peters and de With, 2012)\n\n\n\n\n\n\n\n\n\nシミュレーションが簡単（本節の残りで解説）\nPDMP：離散化誤差なしで簡単にシミュレーションできる稀有な連続過程\nダイナミクスが良い（第 4 節）\n前スライドで見た通り，非対称なダイナミクスが作れる\nスケーラブルなサンプリング手法（第 2 節）\n全データにアクセスする必要はなく，一部で良い（サブサンプリング）\nバイアスが入らないサブサンプリングが可能 (Bierkens, Fearnhead, et al., 2019)\n\n\n\n\n\n\n\n\n\n\n定義（Zig-Zag 過程）\n\n\n\nZig-Zag 過程は状態空間 E=\\mathbb{R}^d\\times\\{\\pm1\\}^d 上に定義される過程 Z=(X,\\Theta) である．\n\\Theta は速度，X は位置と解し，第一成分 X が \\mathbb{R}^d 上の目標分布 \\pi に従うように構成される．\nランダムな時刻におけるランダムな変化（次スライド）を除いては，速度 \\theta\\in\\{\\pm1\\}^d の等速直線運動をする．\n\n\n\nすなわち，(x,\\theta)\\in E から出発する Zig-Zag 過程は，次の微分方程式系で定まる決定論的なフロー \\phi_{(x,\\theta)}:\\mathbb{R}\\to\\mathbb{R}^d に従って運動する粒子とみなせる： \n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\\qquad \\frac{d \\Theta_t}{d t}=0.\n\n\n\n\n\n\n\n\n\n\n\n\n状態空間 E 上のレート関数 \\lambda_1,\\cdots,\\lambda_d （次スライド）から定まる強度関数 \nm_i(t):=\\lambda_i(x+\\theta t,\\theta),\\qquad i\\in[d],\n を持つ，d 個の独立な \\mathbb{R}_+ 上の非一様 Poisson 点過程の，最初の到着時刻 T_1,\\cdots,T_d をシミュレーションする．\n最初に到着した成分の番号 j:=\\operatorname*{argmin}_{i\\in[d]}T_i について，時刻 T_j に速度成分 \\theta_j の符号を反転させる．すなわち，関数 \nF_j(\\theta)_i:=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n に従ってジャンプする．\nt=T_j までを線型に補間し，1. に戻ってくり返す．\n\n\n\n\n\n\n\n\n\n\n\n\n\n命題（Zig-Zag 過程の不変分布）\n\n\n\n目標の分布が \\pi(dx)\\,\\propto\\,e^{-U(x)}\\,dx と表せるとする．このとき，レート関数 \\lambda_1,\\cdots,\\lambda_d:E\\to\\mathbb{R}_+ が，ある \\theta_i のみには依らない非負連続関数 \\gamma_i:E\\to\\mathbb{R}_+ を用いて \n\\lambda_i(x,\\theta)=\\biggr(\\theta_i\\partial_iU(x)\\biggl)_++\\gamma_i(x,\\theta_{-i})\n と表されるならば，Zig-Zag 過程 Z=(X,\\Theta) の位置成分 X は \\pi を不変分布に持つ．\n\n\nあとはエルゴード性が成り立てば，\\pi に対する MCMC として使える．\n\n\n\n\n\n\n\n\n\n定理 (Bierkens, Roberts, et al., 2019)\n\n\n\nU\\in C^3(\\mathbb{R}^d) はある定数 c&gt;d,c'\\in\\mathbb{R} に関して \nU(x)\\ge c\\log\\lvert x\\rvert-c'\\qquad x\\in\\mathbb{R}^d\n を満たすならば，全変動距離に関してエルゴード性を持つ：1 \n\\left\\|P^t((x,\\theta),-)-\\pi\\right\\|_\\mathrm{TV}\\xrightarrow{t\\to\\infty}0.\n\n\n\n\n\\pi(x)\\,\\propto\\,e^{-U(x)}\\le C'\\lvert x\\rvert^c,\\qquad C'&gt;0,\n ということだから，ほとんどの場合成り立つ．\n\n\n\n\n\\lambda_i(x,\\theta)=\\biggr(\\theta_i\\partial_iU(x)\\biggl)_++\\gamma_i(x,\\theta_{-i})\n\n\n\\gamma_i は \\theta_i に依らず，成分 \\theta_i を変化させる頻度を定める．\n\\gamma_i\\equiv0 と取った方が，Monte Carlo 推定量の漸近分散は最小になる (Andrieu and Livingstone, 2021)\n\\gamma_i だけの自由度があることが，Zig-Zag サンプラーのスケーラビリティを支えている（次節 2 参照）\n\n\n\n\n\n\n\n\n\n\n命題\n\n\n\n強度関数 m_i を持つ Poisson 点過程の，最初の到着時刻 T_i は， \nM_i(t):=\\int^t_0m_i(s)\\,ds\n と指数分布する確率変数 E_i\\sim\\operatorname{Exp}(1) を用いて， \nT_i\\overset{\\text{d}}{=}M_i^{-1}(E_i)\n とシミュレーションできる．\n\n\nしかしこの方法は，m_i が多項式の場合しか使えない．\n\n\n\n\n\n\n\n\n\n命題\n\n\n\nm\\le M を２つの強度関数とする．M を強度に持つ Poisson 点過程 \\eta の点 \nX_1,X_2,\\cdots,X_{\\eta([0,T])}\n をシミュレーションし，それぞれを確率 1-\\frac{m(X_i)}{M(X_i)} で取り除く（剪定）．\nすると，残った点は強度 m の Poisson 点過程に従う．\n\n\n\nZig-Zag 過程のシミュレーションの問題は，強度関数 \nm_i(t)=\\lambda_i(x+t\\theta,\\theta)=\\biggr(\\theta_i\\partial_iU(x+t\\theta)\\biggl)_++\\gamma_i(x+t\\theta,\\theta_{-i})\n の多項式による上界 M_i を見つける問題に帰着される．"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html#zig-zag-サンプラー",
    "href": "posts/2024/Slides/ZigZagSampler.html#zig-zag-サンプラー",
    "title": "Zig-Zag サンプラー",
    "section": "",
    "text": "区分確定的 Makrov 過程を用いた，連続時間 MCMC 手法の１つ\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\nPDMP または 連続時間 MCMC と呼ばれる手法群の一つ\nランダムな時刻にランダムな動きをする以外は，確定的な動き\n\n\n\n\nZig-Zag サンプラーの軌跡\n\n\n\n\n\nPDMP （Piecewise Deterministic Markov Process，区分確定的マルコフ過程）\n\n\n\n代表的な３つの PDMP. Animated by (Grazzi, 2020)\n\n\n今回は Zig-Zag サンプラー（中央）に注目する．\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の軌道\n\n\n\n\n\n正確なシミュレーションが可能な連続過程である．\nシミュレーションが簡単で，次の２つだけ考えれば良い：\n\nランダムな時刻 （非一様 Poisson 点過程）\nランダムな変化 （Zig-Zag の場合はこれも無し）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZig-Zag 過程の軌道\n\n\n\n\n\n軌跡自体が目標分布 \\pi に従う\n\n軌跡上で線積分する or 好きな間隔で切り出してサンプルとする\n\n非対称 Metropolis-Hastings 法の scaling limit として導かれる\n\nZig-Zag は Curie-Weiss 模型の Lifted MH 法から (Bierkens and Roberts, 2017)\nBPS は分子動力学法の模型から (Peters and de With, 2012)\n\n\n\n\n\n\n\n\n\nシミュレーションが簡単（本節の残りで解説）\nPDMP：離散化誤差なしで簡単にシミュレーションできる稀有な連続過程\nダイナミクスが良い（第 4 節）\n前スライドで見た通り，非対称なダイナミクスが作れる\nスケーラブルなサンプリング手法（第 2 節）\n全データにアクセスする必要はなく，一部で良い（サブサンプリング）\nバイアスが入らないサブサンプリングが可能 (Bierkens, Fearnhead, et al., 2019)\n\n\n\n\n\n\n\n\n\n\n定義（Zig-Zag 過程）\n\n\n\nZig-Zag 過程は状態空間 E=\\mathbb{R}^d\\times\\{\\pm1\\}^d 上に定義される過程 Z=(X,\\Theta) である．\n\\Theta は速度，X は位置と解し，第一成分 X が \\mathbb{R}^d 上の目標分布 \\pi に従うように構成される．\nランダムな時刻におけるランダムな変化（次スライド）を除いては，速度 \\theta\\in\\{\\pm1\\}^d の等速直線運動をする．\n\n\n\nすなわち，(x,\\theta)\\in E から出発する Zig-Zag 過程は，次の微分方程式系で定まる決定論的なフロー \\phi_{(x,\\theta)}:\\mathbb{R}\\to\\mathbb{R}^d に従って運動する粒子とみなせる： \n\\frac{d \\phi_{(x,\\theta)}(t)}{d t}=\\theta,\\qquad \\frac{d \\Theta_t}{d t}=0.\n\n\n\n\n\n\n\n\n\n\n\n\n状態空間 E 上のレート関数 \\lambda_1,\\cdots,\\lambda_d （次スライド）から定まる強度関数 \nm_i(t):=\\lambda_i(x+\\theta t,\\theta),\\qquad i\\in[d],\n を持つ，d 個の独立な \\mathbb{R}_+ 上の非一様 Poisson 点過程の，最初の到着時刻 T_1,\\cdots,T_d をシミュレーションする．\n最初に到着した成分の番号 j:=\\operatorname*{argmin}_{i\\in[d]}T_i について，時刻 T_j に速度成分 \\theta_j の符号を反転させる．すなわち，関数 \nF_j(\\theta)_i:=\\begin{cases}-\\theta_i&i=j\\\\\\theta_i&i\\ne j\\end{cases}\n に従ってジャンプする．\nt=T_j までを線型に補間し，1. に戻ってくり返す．\n\n\n\n\n\n\n\n\n\n\n\n\n\n命題（Zig-Zag 過程の不変分布）\n\n\n\n目標の分布が \\pi(dx)\\,\\propto\\,e^{-U(x)}\\,dx と表せるとする．このとき，レート関数 \\lambda_1,\\cdots,\\lambda_d:E\\to\\mathbb{R}_+ が，ある \\theta_i のみには依らない非負連続関数 \\gamma_i:E\\to\\mathbb{R}_+ を用いて \n\\lambda_i(x,\\theta)=\\biggr(\\theta_i\\partial_iU(x)\\biggl)_++\\gamma_i(x,\\theta_{-i})\n と表されるならば，Zig-Zag 過程 Z=(X,\\Theta) の位置成分 X は \\pi を不変分布に持つ．\n\n\nあとはエルゴード性が成り立てば，\\pi に対する MCMC として使える．\n\n\n\n\n\n\n\n\n\n定理 (Bierkens, Roberts, et al., 2019)\n\n\n\nU\\in C^3(\\mathbb{R}^d) はある定数 c&gt;d,c'\\in\\mathbb{R} に関して \nU(x)\\ge c\\log\\lvert x\\rvert-c'\\qquad x\\in\\mathbb{R}^d\n を満たすならば，全変動距離に関してエルゴード性を持つ：1 \n\\left\\|P^t((x,\\theta),-)-\\pi\\right\\|_\\mathrm{TV}\\xrightarrow{t\\to\\infty}0.\n\n\n\n\n\\pi(x)\\,\\propto\\,e^{-U(x)}\\le C'\\lvert x\\rvert^c,\\qquad C'&gt;0,\n ということだから，ほとんどの場合成り立つ．\n\n\n\n\n\\lambda_i(x,\\theta)=\\biggr(\\theta_i\\partial_iU(x)\\biggl)_++\\gamma_i(x,\\theta_{-i})\n\n\n\\gamma_i は \\theta_i に依らず，成分 \\theta_i を変化させる頻度を定める．\n\\gamma_i\\equiv0 と取った方が，Monte Carlo 推定量の漸近分散は最小になる (Andrieu and Livingstone, 2021)\n\\gamma_i だけの自由度があることが，Zig-Zag サンプラーのスケーラビリティを支えている（次節 2 参照）\n\n\n\n\n\n\n\n\n\n\n命題\n\n\n\n強度関数 m_i を持つ Poisson 点過程の，最初の到着時刻 T_i は， \nM_i(t):=\\int^t_0m_i(s)\\,ds\n と指数分布する確率変数 E_i\\sim\\operatorname{Exp}(1) を用いて， \nT_i\\overset{\\text{d}}{=}M_i^{-1}(E_i)\n とシミュレーションできる．\n\n\nしかしこの方法は，m_i が多項式の場合しか使えない．\n\n\n\n\n\n\n\n\n\n命題\n\n\n\nm\\le M を２つの強度関数とする．M を強度に持つ Poisson 点過程 \\eta の点 \nX_1,X_2,\\cdots,X_{\\eta([0,T])}\n をシミュレーションし，それぞれを確率 1-\\frac{m(X_i)}{M(X_i)} で取り除く（剪定）．\nすると，残った点は強度 m の Poisson 点過程に従う．\n\n\n\nZig-Zag 過程のシミュレーションの問題は，強度関数 \nm_i(t)=\\lambda_i(x+t\\theta,\\theta)=\\biggr(\\theta_i\\partial_iU(x+t\\theta)\\biggl)_++\\gamma_i(x+t\\theta,\\theta_{-i})\n の多項式による上界 M_i を見つける問題に帰着される．"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html#sec-scalable-inference",
    "href": "posts/2024/Slides/ZigZagSampler.html#sec-scalable-inference",
    "title": "Zig-Zag サンプラー",
    "section": "2 大規模データに対するベイズ推論",
    "text": "2 大規模データに対するベイズ推論\nZig-Zag サンプラーは，不偏なサブサンプリングにより，スケーラブルな Monte Carlo 法として使える．\n\n2.1 従来の MCMC は大規模データに弱い．\n\nMetropolis-Hastings 法\n 採択-棄却のステップでは，毎度全データにアクセスする必要があり，計算量が大きい．\nデータの一部のみを使って尤度を不変推定する2\n 目標分布に収束しなくなる（バイアスが導入される）\n MCMC を使う利点が失われる\n\n\n\n2.2 大規模データに対する２つのアプローチ\n1. Devide-and-conquer\nデータを小さなチャンクに分割し，それぞれで MCMC を回し，あとから結果を総合する．\n\n\n\n\n\n\n\n\n\n不偏性\n手法名\n提案文献\n\n\n\n\n\nWASP\n(Srivastava et al., 2015)\n\n\n\nConsensus Monte Carlo\n(Scott et al., 2016)\n\n\n\nMonte Carlo Fusion\n(Dai et al., 2019)\n\n\n\n\n\n\n2.3 大規模データに対する２つのアプローチ\n2. Subsampling\n尤度評価（全データが必要）を，リサンプリングに基づく不偏推定量で代用する．\n\n\n\n\n\n\n\n\n\n不偏性\n手法名\n提案文献\n\n\n\n\n\nStochastic Gadient MCMC\n(Welling and Teh, 2011)\n\n\n\nZig-Zag with Subsampling\n(Bierkens, Fearnhead, et al., 2019)\n\n\n\nStochastic Gradient PDMP\n(Fearnhead et al., 2024)\n\n\n\n\n\n\n2.4 Zig-Zag サンプラーではバイアスを導入しないサブサンプリングが可能\n\nZig-Zag サンプラーは，ジャンプ強度で尤度の情報を使う：U=-\\log\\pi \n\\lambda_i(x,\\theta)=\\biggr(\\theta_i\\partial_iU(x)\\biggl)_++\\gamma_i(x,\\theta_{-i}),\\qquad i\\in[d],\n 仮に，単一のサンプル k\\in[n] から計算できる量 E_k^1(x) に関して \n\\partial_iU(x)=\\frac{1}{n}\\sum_{k=1}^nE_i^k(x)\n が成り立つ場合，\\partial_iU(x) を，[n]:=\\{1,\\cdots,n\\} 上の一様サンプル K を用いて \nE_i^K(x),\\qquad K\\sim\\mathrm{U}([n])\n で不偏推定できる．\n\n\n\n2.5 ZZ-SS (Zig-Zag Sampler with Sub-Sampling)\nBayes 推論の文脈では，負の対数尤度 U(x)=-\\log\\pi(x|\\boldsymbol{y}) は \nU(x)=-\\sum_{k=1}^n\\log p(y_k|x)-\\log p(x),\\qquad\\pi(x)\\,\\propto\\,\\left(\\prod_{k=1}^np(y_k|x)\\right)p(x),\n という表示を持つ．p(x) は事前分布，y_1,\\cdots,y_k がデータ． \nE_i^k(x):=\\frac{\\partial }{\\partial x_i}\\biggr(-n\\log p(y_k|x)-\\log p(x)\\biggl)\n と定めると， \n\\partial_iU(x)=\\mathbb{E}[E_i^K(x)]\\qquad K\\sim\\mathrm{U}([n]).\n\n\n\n2.6 ZZ-SS (Zig-Zag Sampler with Sub-Sampling)\nそこで，ランダムに定まる強度関数 \nm^K_i(t):=\\biggr(\\theta\\cdot E^K_i(x+\\theta t)\\biggl)_+,\\qquad K\\sim\\mathrm{U}([n]),\n を用いて Zig-Zag 過程をシミュレーションすることを考える．\nただし， \nm_i^k(t)\\le M_i(t)\n を満たす上界 M_i が存在すると仮定する．\n\n\n2.7 ZZ-SS (Zig-Zag Sampler with Sub-Sampling)\n\n\n\n\n\n\nZZ-SS アルゴリズム\n\n\n\n\n代理強度関数 M_1,\\cdots,M_d を持つ互いに独立な \\mathbb{R}_+ 上の非一様 Poisson 点過程の到着時刻 T_1,\\cdots,T_d をシミュレーションする．\n最初に到着した座標番号 j:=\\operatorname*{argmin}_{i\\in[d]}T_i について，確率 \n\\frac{m^K_j(T_j)}{M_j(T_j)},\\qquad K\\sim\\mathrm{U}([n]),\n で時刻 T_j に速度成分 \\theta_j の符号を反転させる．\n１に t=T_j として戻って，繰り返す．\n\n\n\n\n\n2.8 ZZ-SS (Zig-Zag Sampler with Sub-Sampling)\n\n\n\n\n\n\n部分サンプリングにより不変分布が変わらないことの証明\n\n\n\nZZ-SS によってシミュレートされる過程は，レート関数 \n\\lambda_i(x,\\theta)=\\mathbb{E}\\biggl[\\biggr(\\theta_iE^K_i(x)\\biggl)_+\\biggr]=\\frac{1}{n}\\sum_{k=1}^n(\\theta_iE^k_i(x))_+\n を持った Zig-Zag 過程に等しい\nこれは，元々のレート関数 (\\theta\\partial_iU(x))_+ に対して， \n\\gamma_i(x,\\theta):=\\frac{1}{n}\\sum_{k=1}^n(\\theta_iE^k_i(x))_+-\\left(\\frac{\\theta_i}{n}\\sum_{k=1}^nE^k_i(x)\\right)_+\\ge0.\n という項を加えて得る Zig-Zag サンプラーともみなすことができる．非負性は関数 (x)_+:=x\\lor0 の凸性から従う．最後に \\gamma_i(x,\\theta)=\\gamma_i(x,F_i(\\theta)) を確認すれば良い．\nこれは \\begin{align*}\n  &\\qquad\\frac{1}{n}\\sum_{k=1}^n\\biggr(\\theta_iE_i^k(x)\\biggl)_+-\\frac{1}{n}\\sum_{k=1}^n\\biggr(-\\theta_iE_i^k(x)\\biggl)_+\\\\\n  &=\\frac{1}{n}\\sum_{k=1}^n\\left((\\theta_iE_i^k(x))_+-(-\\theta_iE_i^k(x))_+\\right)=\\frac{1}{n}\\sum_{k=1}^n\\theta_iE_i^k(x)\n\\end{align*} であることから従う．\n\n\n\n\n2.9 上界 M_i をどう見つけるか？\n\nランダムに定まる強度関数 \nm^K_i(t):=\\biggr(\\theta_iE^K_i(x+\\theta t)\\biggl)_+\n に対して， \n\\max_{k\\in[n]}m^k_i(t)\\le M_i(t)\n を満たす多項式関数 M_i を見つけないと，剪定ができず，事実上シミュレーションが不可能．\nまた，タイトな上界でないと，ほとんどの提案が棄却され，無駄な計算量が増してしまう．\n\n\n\n2.10 ZZ-CV (Zig-Zag Sampler with Control Variates)\n\n\n\n\n\n\n命題 (Bierkens, Fearnhead, et al., 2019) （制御変数による上界の構成）\n\n\n\n\\partial_iU(x) は Lipschitz 定数 C_i を持って Lipschitz 連続であるとする．このとき， \nM_i(t):=a_i+b_it\n \na_i:=(\\theta_i\\partial_iU(x_*))_++C_i\\|x-x_*\\|_p,\\quad b_i:=C_id^{1/p}\n と定めれば，m_i^k\\le M_i が成り立つ．ただし，ランダムな強度関数 m_i^k は次のように定めた： \nm^k_i(t):=\\biggr(\\theta E_i^k(x+\\theta t)\\biggl)_+,\\qquad x_*:=\\operatorname*{argmin}_{x\\in\\mathbb{R}^d}U(x),\n \nE^k_i(x):=\\partial_iU(x_*)+\\partial_iU^k(x)-\\partial_iU^k(x_*).\n\n\n\n\n\n2.11 ZZ-CV (Zig-Zag Sampler with Control Variates)\n事前処理により，事後最頻値 \\widehat{x} に十分近いように参照点 x_* を選ぶ\n その後は データのサイズに依存しない O(1) 計算複雑性で事後分布からの正確なサンプリングが可能\n\n\n\n\n\n\npreprocessing for ZZ-CV\n\n\n\n\nx_*:=\\operatorname*{argmin}_{x\\in\\mathbb{R}^d}U(x) を探索する．\n\\partial_iU(x_*),\\partial_iU^k(x_*) を計算する．\n\n\n\nこの２つはいずれも O(n) の複雑性で実行できる．\n\n\n2.12 例：正規標本の平均推定\n\nデータは１次元で，分散 \\sigma^2 が既知な正規分布に従うとする： \nY^j\\overset{\\text{i.i.d.}}{\\sim}\\mathrm{N}(x_0,\\sigma^2),\\qquad j\\in[n].\n 事前分布を \\mathrm{N}(0,\\rho^2) とすると，定数の違いを除いて \\begin{align*}\n    U'(x)&=\\frac{x}{\\rho^2}+\\frac{1}{\\sigma^2}\\sum_{j=1}^n(x-y^j)=\\frac{x}{\\rho^2}+\\frac{n}{\\sigma^2}(x-\\overline{y}),\n\\end{align*} であるから，U' は非有界であり，簡単な上界が見つからない．しかし， \nU''(x)=\\frac{1}{\\rho^2}+\\frac{n}{\\sigma^2}.\n は有界だから，U' は \\|U''\\|_\\infty を Lipschitz 定数として Lipscthiz 連続である．\n\n\n\n2.13 数値実験：ZZ（サブサンプリングなし）と ZZ-CV（制御変数を用いたサブサンプリング）の比較\n\n同じ長さの軌跡＋同数のサンプルを用いて，事後平均推定量により x_0 を推定した場合の平均自乗誤差は次の通り：\n\n\n\n\n横軸：単位計算量，縦軸：平均自乗誤差（点線は不正確！）\n\n\n\n\n2.14 数値実験：ZZ と ZZ-CV の比較\n単位計算量 (epoch)：「データ n を通じた勾配の計算」を１単位とする 揃えて比較すると，たしかに効率が改善されている：\n\n\n\n横軸：単位計算量，縦軸：平均自乗誤差\n\n\n\n\n2.15 ZZ-CV がうまくいく理由\n\n データ数 n\\to\\infty の極限で，事後分布は最頻値 \\widehat{x} の周りに集中するため\n事前処理により参照点 x_* を \n\\|x_*-\\widehat{x}\\|_p=O(n^{-1/2})\\quad(n\\to\\infty)\n 程度の正確性で得られたならば， \n\\|x-x_*\\|_p=O_p(n^{-1/2}),\\quad\\partial_iU(x_*)=O_p(n^{1/2})\\quad(n\\to\\infty)\n が成り立つ．このことより， \nM_i(t)=a_i+b_it=O_p(n^{1/2})\\quad(n\\to\\infty)\n であるが，Zig-Zag 過程は O(n^{-1/2}) のタイムステップで区切って独立なサンプルとみなせるため，総じて独立なサンプルを得るための計算量は O(1)．"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html#ロジスティック回帰と大規模不均衡データの問題",
    "href": "posts/2024/Slides/ZigZagSampler.html#ロジスティック回帰と大規模不均衡データの問題",
    "title": "Zig-Zag サンプラー",
    "section": "3 ロジスティック回帰と大規模不均衡データの問題",
    "text": "3 ロジスティック回帰と大規模不均衡データの問題\n不均衡データに対する Gibbs サンプラーは収束が無際限に遅くなるが，Zig-Zag サンプラーでは簡単なトリックで克服できる．\n\n3.1 ロジスティック回帰\n\n\\operatorname{P}[Y=1\\,|\\,X,\\xi]=\\frac{1}{1+\\exp(-X^\\top\\xi)}\n\nの，事前分布 p_0(\\xi)d\\xi とデータ \\{(x^i,y^i)\\}_{i=1}^n に対する事後分布 \\pi は次のように表せる：\n\n\\pi(\\xi)\\,\\propto\\,p_0(\\xi)\\prod_{i=1}^n\\frac{\\exp(y^i(x^i)^\\top\\xi)}{1+\\exp((x^i)^\\top\\xi)}.\n\n\\pi は正規分布の Pólya-Gamma 複合としての構造を持つ  データ拡張による Gibbs サンプラー (Polson et al., 2013) によりサンプリング可能．\n\n\n3.2 ロジットモデルはポテンシャルの勾配が有界になる\n\\begin{align*}\n    U(\\xi)&:=-\\log p_0(\\xi)-\\sum_{i=1}^n\\log\\left(\\frac{\\exp(y^i(x^i)^\\top\\xi)}{1+\\exp((x^i)^\\top\\xi)}\\right)=:U_0(\\xi)+U_1(\\xi)\n\\end{align*}\n\nU_1(\\xi)=\\frac{1}{n}\\sum_{j=1}^nU^j_1(\\xi),\\qquad U_1^j(\\xi)=-n\\log\\left(\\frac{\\exp\\left(y^j(x^j)^\\top\\xi\\right)}{1+\\exp\\left((x^j)^\\top\\xi\\right)}\\right),\n \n\\partial_iU^j_1(\\xi)=n\\frac{x^j_i\\exp\\left((x^j)^\\top\\xi\\right)}{1+\\exp\\left((x^j)^\\top\\xi\\right)}-ny^jx^j_i&lt;nx^j_i(1-y^j).\n \n\\therefore\\qquad\\lvert\\partial_iU^j_1(\\xi)\\rvert\\le n\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\qquad i\\in[d].\n\n\n\n3.3 Poisson 剪定のための上界\n\n\n前スライドの評価 \\lvert\\partial_iU^j_1(\\xi)\\rvert\\le n\\max_{j\\in[n]}\\lvert x^j_i\\rvert を通じて，定数のバウンド \n  m_i(t)=\\biggr(\\theta_i\\partial_iU(\\xi+\\theta_it)\\biggl)_+\\le\\left(n\\theta_i\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\right)_+\\equiv:M_i\n   によって Poisson 剪定をする ZZ (Global bound)．\n事後分布の最頻値周りへの集中を通じた，m_i の１次関数によるバウンド \n  M_i(t):=a_i+b_it,\\qquad a_i:=(\\theta_i\\partial_iU(\\xi_*))_++C_i\\lvert\\xi-\\xi_*\\rvert,\n   \n  b_i:=C_i\\sqrt{d},\\qquad C_i:=\\frac{n}{4}\\max_{j\\in[n]}\\lvert x^j_i\\rvert\\lvert x^j\\rvert.\n   によって Poisson 剪定をすることも可能である ZZ (Affine bound)．\n\n同じ上界を用いたサブサンプリング ZZ-SS と ZZ-CV も考えられる．\n\n\n\n3.4 性能比較\n\n\n\nロジスティック回帰のサブサンプリングによる ESS の比較\n\n\n\n\n3.5 有効サンプル数について\n\nZig-Zag 過程 (Z_t)_{t\\in[0,T]} から B 個のサンプル X_1,\\cdots,X_B を生成して，関数 h\\in\\mathcal{L}^2(\\pi) の期待値 \\displaystyle(\\pi|h)=\\int_0^Th(x)\\,\\pi(dx) を推定したとする．この際 \n\\widehat{\\operatorname{ESS}}:=T\\frac{\\widehat{\\mathrm{V}_\\pi[h]}}{\\widehat{\\sigma^2_h}}\n \n\\widehat{\\mathrm{V}_\\pi[h]}:=\\frac{1}{T}\\int^T_0h(X_s)^2\\,ds-\\left(\\frac{1}{T}\\int^T_0h(X_s)\\,ds\\right)^2,\n \n\\widehat{\\sigma^2_h}:=\\frac{1}{B-1}\\sum_{i=1}^B(Y_i-\\overline{Y})^2,\\qquad Y_i:=\\sqrt{\\frac{B}{T}}\\int^{\\frac{iT}{B}}_{\\frac{(i-1)T}{B}}h(X_s)\\,ds.\n によって推定できる．\n\n\n\n3.6 ロジスティック回帰における「大規模不均衡データ」の問題\nロジスティック回帰において， \n\\sum_{j=1}^ny^j\\ll n\n が成り立つ状況下では，Pólya-Gamma 複合に基づく Gibbs サンプラーの収束が，n\\to\\infty の極限で際限なく遅くなる．\nこれは事後分布が最頻値の周りに集中する速度が，不均衡な n\\to\\infty 極限では変化するためである．\n\n\n3.7 「大規模不均衡データ」に対する Gibbs サンプラーの失敗\n\n\n\n自己相関関数の比較（ランダムウォーク・メトロポリス vs Gibbs サンプラー）\n\n\n\n\n3.8 「大規模不均衡データ」に対する Gibbs サンプラーの失敗\n\n\n\nサンプラーの動きと事後分布（ランダムウォーク・メトロポリスの場合）\n\n\n\n\n3.9 「大規模不均衡データ」に対する Gibbs サンプラーの失敗\n\n\n\nサンプラーの動きと事後分布（Pólya-Gamma 複合に基づくGibbs サンプラーの場合）\n\n\n\n\n3.10 不均衡極限では測度の集中レートが違う (Johndrow et al., 2019)\n\n\n\n\n\n\n\n\n\n\n通常の極限\n不均衡極限\n\n\n\n\n事後分布\nn^{-1/2}\n(\\log n)^{-1}\n\n\n提案分布\nn^{-1/2}\nn^{-1/2}\n\n\n\n\n\n\\sum_{i=1}^ny^i=1,\\qquad n\\to\\infty,\n の「不均衡極限」または Infinitely Imbalanced Limit (Owen, 2007) において，集中のオーダーが変わってしまう．\n\n\n3.11 事後分布の集中不足の影響\n\nPólya-Gamma 複合に基づく Gibbs サンプラー\n提案のステップサイズが，事後分布のスケールに比べて小さすぎる．\n制御変数に基づく Zig-Zag サンプラー (ZZ-CV)\nレート関数 m_i の上界 M_i の評価がズレることで効率が下がっていく．\n\n後者は単に M_i の設計不良の問題で，挽回可能！3\n\n\n3.12 重点サブサンプリング (Sen et al., 2020)\n\n\n\n\n\n\n一様でないサブサンプリング を導入することで，Zig-Zag サンプラーを不均衡データにも強くできる．（そもそも汎用的な効率化手法である．）\n\n\n\n\nサブサンプリングによりランダム化された強度関数 \nm_i^K(t)=\\biggr(\\theta_iE^K_i(x+\\theta t)\\biggl)_+\n は，真の勾配 \\partial_iU(\\xi) に対する不偏性 \n\\mathbb{E}\\biggl[E^K_i(\\xi)\\biggr]=\\partial_iU(\\xi)\n を満たす限り，一様なサブサンプリング K\\sim\\mathrm{U}([n]) に限る必要はなかったのである．\n\n\n\n3.13 重点サブサンプリング (Sen et al., 2020)\n\n(p_x) をある [n] 上の分布 \\nu\\in\\mathcal{P}([n]) の質量関数として \nE^j_i(\\xi):=\\frac{1}{p_j}\\partial_iU^j(\\xi)\\qquad j\\in[n]\n と定めると， \n\\partial_iU(\\xi)=\\sum_{j=1}^np_jE_i^j(\\xi)=\\mathbb{E}[E_i^J].\n このリスケーリングした勾配の推定量 E_i^1,\\cdots,E_i^n に対して，大域的な上界は \n\\lvert E_i^j(\\xi)\\rvert\\le\\max_{j\\in[n]}\\frac{\\lvert x_i^j\\rvert}{p_j}\n に変わる．以前の評価 \\displaystyle\\partial_iU^j\\le n\\max_{j\\in[n]}\\lvert x^j_i\\rvert (3.2) は，p_j\\equiv1/n の場合に当たる．\n\n\n\n3.14 重点サブサンプリング (Sen et al., 2020)\nこの一般化により， \n\\lvert E_i^j(\\xi)\\rvert\\le\\max_{j\\in[n]}\\frac{\\lvert x_i^j\\rvert}{p_j}\n の右辺を確率分布 \\nu=(p_j)\\in\\mathcal{P}([n]) に関して最適化することで，ZZ-SS よりタイトな上界を得る．具体的には \np_j\\,\\propto\\,\\lvert x^j_i\\rvert\n と取れば良い (ZZ-IS)．\n\n\n3.15 性能比較（通常のデータの場合）\n\n\n\n新たに追加された ZZ-IS（重点サブサンプリング）は緑色の線．\n\n\n\n\n3.16 「大規模不均衡データ」に対する Zig-Zag サンプラー\n\\xi_0=1 を真値とし，次のように生成した１次元データを考える： \nX^j\\overset{\\text{i.i.d.}}{\\sim}(1-\\alpha)\\delta_0+\\alpha\\mathrm{N}(1,2),\n \n\\mathbb{P}[Y^j=1]=\\frac{1}{1+e^{-X^j}}.\n\n\n\n\n\n\n\n予想\n\n\n\n\n\\alpha が小さくなるにつれて，ZZ-CV の効率は悪化する\nZZ-IS は重要なデータにうまく注目することで効率悪化を防げる\n\n\n\n\n\n3.17 「大規模不均衡データ」に対する Zig-Zag サンプラー\n\n\n\n左にいく（\\alpha が小さい）ほどスパース性は大きい．ZZ-CV は性能が下がるが，ZZ-IS では逆に上がっている．n=1000．\n\n\n\n\\alpha が小さい領域で ZZ-IS の効率が上がっていることは，\\partial_iV^j_i(\\xi)=0 の場合は上界も 0 になり，そもそも提案もされないため，スパースになるほど効率が上がるためだと思われる．"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html#sec-ergodicity",
    "href": "posts/2024/Slides/ZigZagSampler.html#sec-ergodicity",
    "title": "Zig-Zag サンプラー",
    "section": "4 ダイナミクスとエルゴード性",
    "text": "4 ダイナミクスとエルゴード性\n拡散様の振る舞い (diffusive behaviour) がないために，従来法より収束レートが改善する．\n\n4.1 Zig-Zag サンプラーがうまく行った理由\n従来法に比べ，\n\n確定的な動きが対称性を壊す\n 状態空間のより効率的な探索が可能．\nバイアスのない部分サンプリングが可能\n p そのものではなく，\\partial_i\\log p の値のみ使うため．\n 効率的な Poisson 剪定アルゴリズムさえ見つかれば，（理論的には）どんな U=-\\log\\pi に対しても効率的な実行が可能\n\n２を今まで詳しく見てきた．本節では１を考察する．\n\n\n4.2 連続かつ非対称なダイナミクス\n対象分布：標準 Cauchy 分布 f(x)=\\frac{1}{\\pi\\sigma}\\frac{1}{1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー (Bierkens et al., 2018)\n\n\n\n\n\n\n\nMetropolis-adjusted Langevin Algorithm (Roberts and Tweedie, 1996)\n\n\n\n\n\n\n\n4.3 忘却：エルゴード性の必要条件\n忘却＝スタート地点をすぐに忘れるかどうか： \n\\|P^t(x,-)-P^t(y,-)\\|_\\mathrm{TV}\\xrightarrow{t\\to\\infty}0.\n\n\n\n\n\n\n\n\n\n\nZig-Zag サンプラー (Bierkens et al., 2018)\n\n\n\n\n\n\n\nMetropolis-adjusted Langevin Algorithm (Roberts and Tweedie, 1996)\n\n\n\n\n\nDiffusive Behaviour\n\n\n4.4 t-分布に対する収束レート\n自由度 \\nu の t-分布 \\mathrm{t}(\\nu) に対して，\n\nZig-Zag Sampler (Giorgos Vasdekis and Roberts, 2022) \n\\left\\|P^t\\left((x,\\theta),-\\right)-\\mathrm{t}(\\nu)\\right\\|_\\mathrm{TV}\\le\\frac{C_1V_1(x)}{t^k},\\qquad k&lt;\\nu.\n\nMetropolis-adjusted Langevin Algorithm (Jarner and Tweedie, 2003) \n\\left\\|P^t\\left(x,-\\right)-\\mathrm{t}(\\nu)\\right\\|_\\mathrm{TV}\\le\\frac{C_2V_2(x)}{t^k},\\qquad k&lt;\\frac{\\nu}{2}.\n 参考：\\mathrm{t}(\\nu)\\Rightarrow\\mathrm{N}(0,1)\\;(\\nu\\to\\infty)．\n\n\n\n4.5 Zig-Zag 過程の特徴付け\n\n\n\n\n\n\n状態空間を E'=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\} と取ると，Zig-Zag 過程のジャンプは，レート関数 \n\\lambda(x,\\theta):=\\sum_{i=1}^d\\lambda_i(x,\\theta)\n が定める強度 \nM(t):=\\lambda(x+t\\theta,\\theta)\n を持った \\mathbb{R}_+ 上の非一様 Poisson 点過程に従う．\n\n\n\n\nこの E' 上の点過程上で，次の確率核 Q に従ってジャンプするとみれる： \nQ((x,\\theta),-):=\\sum_{i=1}^d\\frac{\\lambda_i(x,\\theta)}{\\lambda(x,\\theta)}\\delta_{(x,F_i(\\theta))}(-)\n\n\n\n\n\n\n\n\n証明（２つの定義の等価性）\n\n\n\n\n\nZig-Zag 過程に対する２つの定義を与えたが，これら２つが同分布の過程を定めることは証明が必要である．\nまず，\\min_{i\\in[d]}T_i が，強度関数 M が定める到着時刻に同分布であることを示す．\n各 T_i の密度は \np_i(t)=m_i(t)e^{-M_i(t)}1_{(0,\\infty)}(t)\n で与えられ，T_i は互いに独立だから，(T_1,\\cdots,T_d) の結合密度もわかる．\nT_1,\\cdots,T_d を昇順に並べた順序統計量を \nT_{(1)}\\le\\cdots\\le T_{(d)}\n で表すとする．この d 次元確率ベクトルの密度 p は， \np(t_1,\\cdots,t_d)=1_{\\left\\{t_1\\le\\cdots\\le t_d\\right\\}}(t_1,\\cdots,t_d)\\left(\\sum_{\\sigma\\in\\mathfrak{S}_d}\\prod_{i=1}^dm_i(t_{\\sigma(i)})e^{-M_i(t_{\\sigma(i)})}\\right)\n と計算できる．\nこの p を t_2,\\cdots,t_d に関して積分することで，T_1 の密度が得られる：4 \\begin{align*}\n    p_{(1)}(t)&=\\int_{(0,\\infty)^{d-1}}p(t_1,\\cdots,t_d)\\,dt_2\\cdots dt_d\\\\\n    &=\\biggr(\\sum_{i=1}^dm_i(t_1)\\biggl)\\exp\\left(-\\sum_{i=1}^dM_i(t_1)\\right)=m(t_1)e^{-M(t_1)}.\n\\end{align*}\nこれは確かに，強度関数 m が定める到着時刻の密度である．\n続いて，j=\\operatorname*{argmin}_{i\\in[d]}T_i の，\\min_{i\\in[d]}T_i に関する条件付き確率質量関数が \nq(i|t)=\\frac{m_i(t)}{\\sum_{i=1}^dm_i(t)}\n であることを示す．\nそのためには，任意の i\\in[d] と A\\in\\mathcal{B}(\\mathbb{R}^+) とに関して \\left\\{T_{(1)}\\in A,T_{(1)}=T_i\\right\\} という形の事象を計算し，密度が積の形で与えられることを見れば良い．\n\\begin{align*}\n    &\\qquad\\operatorname{P}[T_{(1)}\\in A,T_{(1)}=T_i]\\\\\n    &=\\operatorname{P}[T_i\\in A,\\forall_{j\\ne i}\\;T_i\\le T_j]\\\\\n    &=\\int_Ap_i(t_i)\\,dt_i\\left(\\sum_{\\sigma\\in\\mathrm{Aut}([d]\\setminus\\{i\\})}\\int^\\infty_{t_i}p_{\\sigma(1)}(t_{\\sigma(1)})\\,dt_{\\sigma(1)}\\int^\\infty_{t_{\\sigma(1)}}p_{\\sigma(2)}(t_{\\sigma(2)})\\,dt_{\\sigma(2)}\\cdots\\int^\\infty_{t_{\\sigma(d-1)}}p_{\\sigma(d)}(t_{\\sigma(d)})\\,dt_{\\sigma(d)}\\right)\\\\\n    &=\\int_Am_i(t_i)\\exp\\left(-\\sum_{i=1}^dm_i(t_i)\\right)\\,dt_i\\\\\n    &=\\int_A\\frac{m_i(t_i)}{m(t_i)}m(t_i)e^{-M(t_i)}\\,dt_i.\n\\end{align*}\nよって，\\min_{i\\in[d]}T_i と \\operatorname*{argmin}_{i\\in[d]}T_i とに関する結合密度は，5 \nq(i|t)p_{(1)}(t)\n という積の形で与えられることがわかった．\n\n\n\n\n\n\nまとめ\n\n\n\n\n前述の定義は，\\min_{i\\in[d]}T_i の形で密度 p_{(1)} からシミュレーションし，\\operatorname*{argmin}_{i\\in[d]}T_i の形で q からシミュレーションしている．\n後述の定義は，p_{(1)}(t) から直接シミュレーションし，再び q(i|t) から直接シミュレーションをする．\n\n１が２に等価であることがわかった．\n\n\n\n\n\n\n\n4.6 数学的な見通しの良さ\n\n\n\n\n\n\n\n運動：速度 \\theta\\in\\{\\pm1\\}^d による等速直線運動．\nジャンプ：E'=\\bigcup_{\\theta\\in\\{\\pm1\\}^d}\\mathbb{R}^d\\times\\{\\theta\\} 上の Q-付印 された非一様 Poisson 点過程．\n\n\n\n\nこの定式化の下では，(Davis, 1993) により 拡張生成作用素の形 が一般的な設定で調べられている：C^1(E)\\subset\\mathcal{D}(\\widehat{L}) かつ \n\\widehat{L}f(x,\\theta)=\\theta\\cdot D_xf(x,\\theta)+\\lambda(x,\\theta)\\biggr(f(x,-\\theta)-f(x,\\theta)\\biggl).\n\n 拡散項がジャンプ項になっただけで，従来の MCMC の解析に用いたエルゴード定理がそのまま使える．\n\n\n4.7 指数エルゴード性\n\n\n\n\n\n\n命題 (Bierkens, Roberts, et al., 2019)\n\n\n\nU\\in C^3(\\mathbb{R}^d) は次を満たすとする：\n\n\\displaystyle\\lim_{\\lvert x\\rvert\\to\\infty}\\frac{\\lvert D^2U(x)\\rvert\\lor1}{\\lvert DU(x)\\rvert}=0,\\qquad\\lim_{\\lvert x\\rvert\\to\\infty}\\frac{\\lvert DU(x)\\rvert}{U(x)}=0.\n\\gamma_1,\\cdots,\\gamma_d は有界である．\n\nこのとき，Zig-Zag 過程は指数エルゴード性を持つ： \n\\left\\|P^t((x,\\theta),-)-\\pi\\right\\|_\\mathrm{TV}\\le V(x,\\theta)e^{-ct},\\qquad t\\ge0,(x,\\theta)\\in E.\n\n\n\n\n\\pi の分布の裾が指数よりも重い場合などは条件１を満たさない． 例 ： \\displaystyle\\qquad\\quad U(x)=x^\\alpha\\qquad(0&lt;\\alpha&lt;1)\n\n\n\n4.8 対象分布の裾が重いと指数エルゴード性が破れる\n\n\n\n\n\n\n命題（確率測度の差の全変動の表示）\n\n\n\n任意の可測空間 (E,\\mathcal{E}) 上の任意の確率測度 \\mu,\\nu\\in\\mathcal{P}(E) について， \n\\|\\mu-\\nu\\|_\\mathrm{TV}=2\\max_{A\\in\\mathcal{E}}\\biggr(\\mu(A)-\\nu(A)\\biggl).\n\n\n\n\nたとえば A を半径 t\\sqrt{d} の閉球 B_{t\\sqrt{d}} の補集合 B_{t\\sqrt{d}}^\\complement と取ると，時刻 t に B_{t\\sqrt{d}}^\\complement に到着する確率は 0 だから， \n\\left\\|P^t\\left((x,\\theta),-\\right)-\\pi\\right\\|_\\mathrm{TV}\\ge\\pi(B_{t\\sqrt{d}}^\\complement),\\qquad t\\ge0.\n \\pi の裾確率が指数よりも遅い場合，これは指数エルゴード性に矛盾する．"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html#まとめと展望",
    "href": "posts/2024/Slides/ZigZagSampler.html#まとめと展望",
    "title": "Zig-Zag サンプラー",
    "section": "5 まとめと展望",
    "text": "5 まとめと展望\n\n\nダイナミクス 多様な動きを取り入れることで，高次元における収束を加速する\n\nBoomerang Sampler (Bierkens et al., 2020)\nSUZZ (Speed-Up Zig-Zag) (G. Vasdekis and Roberts, 2023)\n\nシミュレーション Poisson 剪定さえ効率的に出来れば実行可能  一般的な処方箋を求める方向に研究が進んでいる\n\nAutomatic Zig-Zag (Corbella et al., 2022)\nConcave-Convex PDMP (Sutton and Fearnhead, 2023)\nNuZZ (Numerical Zig-Zag) (Pagani et al., 2024)\n\n\n\n\n参考文献\n\n\n\n\n\n\n\n\n\nスライドとコード\n\n\n\n\n\n\nAndrieu, C., and Livingstone, S. (2021). Peskun–Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario. The Annals of Statistics, 49(4), 1958–1981.\n\n\nAndrieu, C., and Roberts, G. O. (2009). The pseudo-marginal approach for efficient monte carlo computations. The Annals of Statistics, 37(2), 697–725.\n\n\nBierkens, J., Bouchard-Côté, A., Doucet, A., Duncan, A. B., Fearnhead, P., Lienart, T., … Vollmer, S. J. (2018). Piecewise deterministic markov processes for scalable monte carlo on restricted domains. Statistics & Probability Letters, 136, 148–154.\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nBierkens, J., Grazzi, S., Kamatani, K., and Roberts, G. O. (2020). The boomerang sampler. Proceedings of the 37th International Conference on Machine Learning, 119, 908–918.\n\n\nBierkens, J., and Roberts, G. (2017). A Piecewise Deterministic Scaling Limit of Lifted Metropolis-Hastings in the Curie-Weiss Model. The Annals of Applied Probability, 27(2), 846–882.\n\n\nBierkens, J., Roberts, G. O., and Zitt, P.-A. (2019). Ergodicity of the zigzag process. The Annals of Applied Probability, 29(4), 2266–2301.\n\n\nCorbella, A., Spencer, S. E. F., and Roberts, G. O. (2022). Automatic Zig-Zag Sampling in Practice. Statistics and Computing, 32(6), 107.\n\n\nDai, H., Pollock, M., and Roberts, G. (2019). Monte Carlo Fusion. Journal of Applied Probability, 56(1), 174–191.\n\n\nDavis, M. H. A. (1993). Markov models and optimization,Vol. 49. Chapman & Hall.\n\n\nFearnhead, P., Grazzi, S., Nemeth, C., and Roberts, G. O. (2024). Stochastic gradient piecewise deterministic monte carlo samplers.\n\n\nGrazzi, S. (2020). Piecewise deterministic monte carlo.\n\n\nJarner, S. F., and Tweedie, R. L. (2003). Necessary conditions for geometric and polynomial ergodicity of random-walk-type markov chains. Bernoulli, 9(4), 559–578.\n\n\nJohndrow, J. E., Smith, A., Pillai, N., and Dunson, D. B. (2019). MCMC for imbalanced categorical data. Journal of the American Statistical Association, 114(527), 1394–1403.\n\n\nLewis, P. A. W., and Shedler, G. S. (1979). Simulation of nonhomogeneous poisson processes by thinning. Naval Research Logistics Quarterly, 26(3), 403–413.\n\n\nOwen, A. B. (2007). Infinitely imbalanced logistic regression. Journal of Machine Learning Research, 8(27), 761–773.\n\n\nPagani, F., Chevallier, A., Power, S., House, T., and Cotter, S. (2024). NuZZ: Numerical zig-zag for general models. Statistics and Computing, 34(1), 61.\n\n\nPeters, E. A. J. F., and de With, G. (2012). Rejection-free monte carlo sampling for general potentials. Physical Review E, 85(2).\n\n\nPolson, N. G., Scott, J. G., and Windle, J. (2013). Bayesian inference for logistic models using pólya–gamma latent variables. Journal of the American Statistical Association, 108(504), 1339–1349.\n\n\nRoberts, G. O., and Tweedie, R. L. (1996). Exponential convergence of langevin distributions and their discrete approximations. Bernoulli, 2(4), 341–363.\n\n\nScott, S. L., Blocker, A. W., Bonassi, F. V., Chipman, H. A., George, E. I., and McCulloch, R. E. (2016). Bayes and big data: The consensus monte carlo algorithm. International Journal of Management Science and Engineering Management, 11, 78–88.\n\n\nSen, D., Sachs, M., Lu, J., and Dunson, D. B. (2020). Efficient posterior sampling for high-dimensional imbalanced logistic regression. Biometrika, 107(4), 1005–1012.\n\n\nSrivastava, S., Cevher, V., Dinh, Q., and Dunson, D. (2015). WASP: Scalable Bayes via barycenters of subset posteriors. In G. Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the eighteenth international conference on artificial intelligence and statistics,Vol. 38, pages 912–920. San Diego, California, USA: PMLR.\n\n\nSutton, M., and Fearnhead, P. (2023). Concave-Convex PDMP-based Sampling. Journal of Computational and Graphical Statistics, 32(4), 1425–1435.\n\n\nVasdekis, Giorgos, and Roberts, G. O. (2022). A note on the polynomial ergodicity of the one-dimensional zig-zag process. Journal of Applied Probability, 59(3), 895–903.\n\n\nVasdekis, G., and Roberts, G. O. (2023). Speed up Zig-Zag. The Annals of Applied Probability, 33(6A), 4693–4746.\n\n\nWelling, M., and Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on international conference on machine learning, pages 681–688. Madison, WI, USA: Omnipress.\n\n\n\n\n\n\n\n\n関連ページ\n\n\nZig-Zag 過程\n\n０．PDMP 全般\n１．基本とエルゴード性\n２．サブサンプリング法について\nZig-Zag 過程の由来について詳しいポスター発表\n\nPoisson 点過程について\nロジスティック回帰\n\n前編：不均衡極限で何が起こるか？\n後編：Zig-Zag サンプラーによる解決\n\nエルゴード性\n\nLangevin Dynamics の多項式エルゴード性"
  },
  {
    "objectID": "posts/2024/Slides/ZigZagSampler.html#footnotes",
    "href": "posts/2024/Slides/ZigZagSampler.html#footnotes",
    "title": "Zig-Zag サンプラー",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n実は同じ条件の下で，多項式エルゴード性が示せる（第 4 節参照）．↩︎\nPseudo-marginal Metropolis-Hastings (Andrieu and Roberts, 2009) など．↩︎\n最初から Zig-Zag サンプラーでは上界 M_i の設計が永遠の課題．↩︎\n計算過程は省略したが，d=2 の場合と，d=3 の場合を少しやってみると良い．↩︎\n参照測度は，[d] 上のものは計数測度 \\# をとっている．↩︎"
  },
  {
    "objectID": "index.html#hello-im-hiro.",
    "href": "index.html#hello-im-hiro.",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "",
    "text": "Ph.D. Student Institute of Statistical Mathematics, Tokyo, Japan\n\n\n\nMonte Carlo methods e.g., MCMC, SMC & PDMP\nBayesian Statistical Modelling especially in Political Science & Biostatistics\nBayesian Neural Networks, Nonparametrics & Kernel Methods\n\n\n\n\nI maintain the Julia package PDMPFlux.jl, and also contribute to the R package YUIMA."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Interests",
    "text": "Interests\n\nMonte Carlo methods e.g., MCMC, SMC & PDMP\nBayesian Statistical Modelling especially in Political Science & Biostatistics\nBayesian Neural Networks, Nonparametrics & Kernel Methods"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Research",
    "text": "Research\nMy research interests revolve around developing and analyzing random algorithms, by leveraging insights from their (scaling / continuous-time / homogenization) limit dynamics. From these perspectives, algorithms reveal their intrinsic properties, and my work aims to unify the understanding of various sampling algorithms under a common mathematical framework, i.e., flows on the space of probability measures \\(\\mathcal{P}(X)\\)."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Education",
    "text": "Education\n\n Ph.D. in Statistical Science, 2028 (expected)\nGraduate University for Advanced Studies, SOKENDAI, Japan\n\n\n B.Sc. in Mathematics, 2023\nUniversity of Tokyo, Japan"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Experience",
    "text": "Experience\n\n Cooperative Researcher, 2023.4 – present\nRCAST, the University of Tokyo \n\n\n Data Scientist, 2023.4 – present\nPreMedica Inc., Tokyo, Japan"
  },
  {
    "objectID": "posts/2024/Probability/Trace.html",
    "href": "posts/2024/Probability/Trace.html",
    "title": "Skilling-Hutchinson の跡推定量",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Probability/Trace.html#命題",
    "href": "posts/2024/Probability/Trace.html#命題",
    "title": "Skilling-Hutchinson の跡推定量",
    "section": "1 命題",
    "text": "1 命題\n\n\n\n\n\n\n(Skilling, 1989)-(Hutchinson, 1990)\n\n\n\n任意の正方行列 \\(A\\in M_n(\\mathbb{R})\\) と，\\(\\mathrm{V}[X]=I_n\\) を満たす確率ベクトル \\(X\\in\\mathcal{L}(\\Omega;\\mathbb{R}^n)\\) について，\n\\[\n\\operatorname{Tr}(A)=\\operatorname{E}[X^\\top AX].\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\nX^\\top AX=\\operatorname{Tr}(AXX^\\top)\n\\] に注意する．これは，一般に \\(x,y\\in\\mathbb{R}^n\\) に対して \\[\nyx^\\top=\\begin{pmatrix}y_1\\\\\\vdots\\\\y_n\\end{pmatrix}(x_1\\;\\cdots\\;x_n)=\\begin{pmatrix}y_1x_1&\\cdots&y_1x_n\\\\\\vdots&\\ddots&\\vdots\\\\y_nx_1&\\cdots&y_nx_n\\end{pmatrix}\n\\] \\[\n\\therefore\\qquad\\operatorname{Tr}(yx^\\top)=x^\\top y\n\\] が成り立つためである．\nよって，次のように計算できる：\n\\[\\begin{align*}\n    \\operatorname{E}[X^\\top AX]&=\\operatorname{E}[\\operatorname{Tr}(AXX^\\top)]\\\\\n    &=\\operatorname{Tr}(\\operatorname{E}[AXX^\\top])\\\\\n    &=\\operatorname{Tr}(A\\operatorname{E}[XX^\\top])=\\operatorname{Tr}(A).\n\\end{align*}\\]\n\n\n\n(Hutchinson, 1990) では \\(A\\) を対称行列に，\\(X\\) を中心化された確率変数に限って示されている．\n(Skilling, 1989) では (Hutchinson, 1990) のように命題の形では提示していないが，同様の推定量を提案しており，これと一般化跡 (generalized trace) と Chebyshev 多項式の議論を通じて，\\(A\\) のスペクトルのベイズ推定を議論している．"
  },
  {
    "objectID": "posts/2024/Probability/Trace.html#推定量の性質",
    "href": "posts/2024/Probability/Trace.html#推定量の性質",
    "title": "Skilling-Hutchinson の跡推定量",
    "section": "2 推定量の性質",
    "text": "2 推定量の性質\n実用上，\\(X\\) の分布は標準 Gauss や Rademacher 分布などが用いられる．\n\n\n\n\n\n\n命題（推定量の分散）\n\n\n\n\\(A\\in S_n(\\mathbb{R})\\) を対称行列とする．\n\n\\(X\\sim\\mathrm{N}(0,I_n)\\) のとき， \\[\n\\mathrm{V}[X^\\top AX]=2\\operatorname{Tr}(A^2)=2\\|A\\|^2_\\mathrm{HS}.\n\\]\n\\(X\\sim\\mathrm{Rad}^{\\otimes n}\\) のとき， \\[\n\\mathrm{V}[X^\\top AX]=2\\sum_{i\\ne j}a_{ij}^2.\n\\]\n\n\n\n\n\n\n\n\n\n証明1\n\n\n\n\n\n\n\\(A\\in S_n(\\mathbb{R})\\) が正定値対称であるとき，ある直交行列 \\(U\\in O_n(\\mathbb{R})\\) と対角行列 \\(\\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)\\) が存在して， \\[\nA=U\\Lambda U^\\top.\n\\] \\(Y:=U^\\top X\\) と定めるとやはり \\(Y\\sim\\mathrm{N}(0,I_n)\\) であり， \\[\nX^\\top AX=X^\\top U\\Lambda U^\\top X=Y^\\top\\Lambda Y,\n\\] \\[\n\\therefore\\qquad\\mathrm{V}[X^\\top AX]=2\\sum_{i=1}^n\\lambda_i^2=2\\operatorname{Tr}(\\Lambda^2)=2\\operatorname{Tr}(A^2).\n\\]\n一般の \\(X\\in\\mathcal{L}(\\Omega;\\mathbb{R}^n)\\) に関して， \\[\n\\mathrm{V}[X^\\top AX]=\\sum_{i,j,k,l=1}^na_{ij}a_{kl}\\biggr(\\operatorname{E}[X_iX_jX_kX_l]-\\operatorname{E}[X_iX_j]\\operatorname{E}[X_kX_l]\\biggl).\n\\]\n\n\n\n\n\n\n\n\n\n\n系\n\n\n\n\\(A\\) が対称行列で \\(\\operatorname{E}[X]=0\\) であるとき，\\(X\\) は Rademacher とした場合が最小分散不偏推定量を定める (Hutchinson, 1990, p. 命題1)．"
  },
  {
    "objectID": "posts/2024/Probability/Trace.html#応用",
    "href": "posts/2024/Probability/Trace.html#応用",
    "title": "Skilling-Hutchinson の跡推定量",
    "section": "3 応用",
    "text": "3 応用\n\n\n\n\n\n\n\n\n\n\n正規化流\n\n\n深層生成モデル４\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nニューラル常微分方程式\n\n\nシミュレーションなしの拡散モデルとしての連続正規化流\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nエネルギーベースモデル\n\n\n深層生成モデル５\n\n\n\n2024-03-30\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n残差フロー (residual flow) では Jacobian の推定が焦点になる．これに Skilling-Hutchinson の跡推定量を用いることができる．\nNeural ODE において，Jacobian の跡 \\(\\operatorname{Tr}(J_{F_t}(x_t))\\) の計算は Skilling-Hutchinson の跡推定量を用いれば \\(O(d)\\) で済む (Grathwohl et al., 2019)．\nSliced Score Matching の目的関数は，Skilling-Hutchinson の跡推定量により Jacobian \\(Ds_\\theta\\) を推定したスコアマッチングと解釈できる．"
  },
  {
    "objectID": "posts/2024/Probability/Trace.html#文献紹介",
    "href": "posts/2024/Probability/Trace.html#文献紹介",
    "title": "Skilling-Hutchinson の跡推定量",
    "section": "4 文献紹介",
    "text": "4 文献紹介\n\n\n(Adams et al., 2018) では (Skilling, 1989) の研究を踏襲し，大規模行列のスペクトル（密度）推定に向けて，Skilling-Hutchinson の跡推定量の拡張が議論されている．\n(Meyer et al., n.d.) では Skilling-Hutchinson の跡推定量を改良したアルゴリズムが提案されている．"
  },
  {
    "objectID": "posts/2024/Probability/Trace.html#footnotes",
    "href": "posts/2024/Probability/Trace.html#footnotes",
    "title": "Skilling-Hutchinson の跡推定量",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Hutchinson, 1990, p. 437), (Avron and Toledo, 2011, p. 補題9), (Adams et al., 2018, p. 命題4.2) も参照．↩︎"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#モンテカルロ法３分類",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#モンテカルロ法３分類",
    "title": "動き出す次世代サンプラー",
    "section": "1.1 モンテカルロ法３分類",
    "text": "1.1 モンテカルロ法３分類\n\n\n\n\n\nMarkov 連鎖\n\n\n\n\n\n\n拡散過程\n\n\n\n\n\n\nPDMP"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#monte-carlo-法３分類",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#monte-carlo-法３分類",
    "title": "動き出す次世代サンプラー",
    "section": "1.1 Monte Carlo 法３分類",
    "text": "1.1 Monte Carlo 法３分類\n\n\n\n\n\nMarkov 連鎖\n\n\n\n\n\n\n拡散過程\n\n\n\n\n\n\nPDMP"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#monte-carlo-法小史-12",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#monte-carlo-法小史-12",
    "title": "動き出す次世代サンプラー",
    "section": "1.3 Monte Carlo 法小史 1/2",
    "text": "1.3 Monte Carlo 法小史 1/2"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#monte-carlo-法小史-22",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#monte-carlo-法小史-22",
    "title": "動き出す次世代サンプラー",
    "section": "1.4 Monte Carlo 法小史 2/2",
    "text": "1.4 Monte Carlo 法小史 2/2"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-12-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-12-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.9 例 1/2 （申請者開発のパッケージより）",
    "text": "1.9 例 1/2 （申請者開発のパッケージより）\n\n\n\n\n\n(Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n(Bierkens et al., 2019)\n\n\n\n\n\n\n(Bierkens et al., 2020)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-22-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#例-22-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.10 例 2/2 （申請者開発のパッケージより）",
    "text": "1.10 例 2/2 （申請者開発のパッケージより）\n\n\n\n\n\n(Michel et al., 2020)\n\n\n\n\n\n\n(Vasdekis and Roberts, 2023)\n\n\n\n\n\n\n(Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の例-12-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の例-12-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.9 PDMP の例 1/2 （申請者開発のパッケージより）",
    "text": "1.9 PDMP の例 1/2 （申請者開発のパッケージより）\n\n\n\n\n\n(Bouchard-Côté et al., 2018)\n\n\n\n\n\n\n(Bierkens et al., 2019)\n\n\n\n\n\n\n(Bierkens et al., 2020)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の例-22-申請者開発のパッケージより",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の例-22-申請者開発のパッケージより",
    "title": "動き出す次世代サンプラー",
    "section": "1.10 PDMP の例 2/2 （申請者開発のパッケージより）",
    "text": "1.10 PDMP の例 2/2 （申請者開発のパッケージより）\n\n\n\n\n\n(Michel et al., 2020)\n\n\n\n\n\n\n(Vasdekis and Roberts, 2023)\n\n\n\n\n\n\n(Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#剪定-lewis-shedler1979",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#剪定-lewis-shedler1979",
    "title": "動き出す次世代サンプラー",
    "section": "2.3 剪定 (Lewis and Shedler, 1979)",
    "text": "2.3 剪定 (Lewis and Shedler, 1979)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#問題複雑な強度を持った非一様-poisson-点過程のシミュレーション",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#問題複雑な強度を持った非一様-poisson-点過程のシミュレーション",
    "title": "動き出す次世代サンプラー",
    "section": "2.1 問題：複雑な強度を持った非一様 Poisson 点過程のシミュレーション",
    "text": "2.1 問題：複雑な強度を持った非一様 Poisson 点過程のシミュレーション"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程のシミュレーションの自動化",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程のシミュレーションの自動化",
    "title": "動き出す次世代サンプラー",
    "section": "2.1 課題：非一様 Poisson 点過程のシミュレーションの自動化",
    "text": "2.1 課題：非一様 Poisson 点過程のシミュレーションの自動化"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程シミュレーションの自動化",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程シミュレーションの自動化",
    "title": "動き出す次世代サンプラー",
    "section": "2.2 課題：非一様 Poisson 点過程シミュレーションの自動化",
    "text": "2.2 課題：非一様 Poisson 点過程シミュレーションの自動化"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程のシミュレーション",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程のシミュレーション",
    "title": "動き出す次世代サンプラー",
    "section": "2.1 課題：非一様 Poisson 点過程のシミュレーション",
    "text": "2.1 課題：非一様 Poisson 点過程のシミュレーション\n\n\n\n\n\n\n\n\nPDMP シミュレーションの原則\n\n\n方向転換地点 \\textcolor{#78C2AD}{x_1,x_2,\\cdots} を決めれば良い\n＊これが難しい\n\n\n\n\n\n\n\n\n\\textcolor{#78C2AD}{x_1,x_2,\\cdots} の決め方\n\n\n確率密度 \\pi に収束させたい場合，強度 \nm^{(i)}(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を持つ非一様 Poisson 点過程をシミュレートする．\n 最初の到着点を \\textcolor{#78C2AD}{x_i}=\\textcolor{#78C2AD}{x_{i-1}}+vT_1^{(i)} とする．"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程の到着時刻の計算",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題非一様-poisson-点過程の到着時刻の計算",
    "title": "動き出す次世代サンプラー",
    "section": "2.3 課題：非一様 Poisson 点過程の到着時刻の計算",
    "text": "2.3 課題：非一様 Poisson 点過程の到着時刻の計算"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#脱線スケーラビリティ-bierkens2019",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#脱線スケーラビリティ-bierkens2019",
    "title": "動き出す次世代サンプラー",
    "section": "2.4 【脱線】スケーラビリティ (Bierkens et al., 2019)",
    "text": "2.4 【脱線】スケーラビリティ (Bierkens et al., 2019)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#確率的剪定-sen2020",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#確率的剪定-sen2020",
    "title": "動き出す次世代サンプラー",
    "section": "2.4 確率的剪定 (Sen et al., 2020)",
    "text": "2.4 確率的剪定 (Sen et al., 2020)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題",
    "title": "動き出す次世代サンプラー",
    "section": "2.5 課題",
    "text": "2.5 課題\n\nm(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を強度に持つ非一様 Poisson 点過程をシミュレートする．\n\n\n\n\n\n\nスケーラビリティの達成\n\n\n\n\n\n\n\n\n\n\n\n\n汎用的なアルゴリズムが存在しない"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題汎用的な上界-m-の見つけ方が存在しない",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#課題汎用的な上界-m-の見つけ方が存在しない",
    "title": "動き出す次世代サンプラー",
    "section": "2.6 課題：汎用的な上界 M の見つけ方が存在しない",
    "text": "2.6 課題：汎用的な上界 M の見つけ方が存在しない\n\nm(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を強度に持つ非一様 Poisson 点過程をシミュレートする．\n\n\n\n\n\n\n○ スケーラビリティが達成可能\n\n\n m(t) の評価＝\\pi の評価\n　 全データが必要\n○ m(t) の不偏推定量 \\mu(t)\n　 データの一部を見るだけで計算可能\n\n\n\n\n 既存研究：確率的剪定 (Bierkens et al., 2019), (Sen et al., 2020)\n\n\n\n\n\n 汎用的なアルゴリズムが存在しない\n\n\n\n\\mu(t)\\le M\\;\\;\\text{a.s.}\n を達成する上界 M をどう見つければ良いのか？\n　 もっぱらロジスティック回帰くらい 　　　にしか使えなかった\n\n\n\n\n 今回の貢献：適応的剪定(Andral and Kamatani, 2024)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#適応的剪定従来法corbella2022",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#適応的剪定従来法corbella2022",
    "title": "動き出す次世代サンプラー",
    "section": "3.1 適応的剪定（従来法）(Corbella et al., 2022)",
    "text": "3.1 適応的剪定（従来法）(Corbella et al., 2022)\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\\nabla\\log\\pi は 自動微分 で計算\nホライゾン t_{\\text{max}} を決めて \n  M:=\\max\\{\\textcolor{#E95420}{\\text{３つの赤点}}\\}\n  \n\\|M-m(t)\\|_\\infty が大き過ぎたら t_{\\text{max}} を縮める\n\n\n\n\n\n \\displaystyle\\max_{t\\in[0,t_{\\text{max}}]}m(t) の計算に最適化が必要\n t_{\\text{max}} の調整アルゴリズム？"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の美点スケーラビリティが達成可能",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#pdmp-の美点スケーラビリティが達成可能",
    "title": "動き出す次世代サンプラー",
    "section": "2.6 PDMP の美点：スケーラビリティが達成可能",
    "text": "2.6 PDMP の美点：スケーラビリティが達成可能\n\nm(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を強度に持つ非一様 Poisson 点過程をシミュレートする．\n\n\n\n\n\n\n○ スケーラビリティが達成可能\n\n\n m(t),\\pi の評価：全データが必要\n○ m(t) の不偏推定量 \\mu(t)\n　 データの一部を見るだけで計算可能\n　○ 収束先が変わらない\n\n\n\n\n 既存研究：確率的剪定 (Bierkens et al., 2019), (Sen et al., 2020)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#適応的剪定提案法andral-kamatani2024",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#適応的剪定提案法andral-kamatani2024",
    "title": "動き出す次世代サンプラー",
    "section": "3.2 適応的剪定（提案法）(Andral and Kamatani, 2024)",
    "text": "3.2 適応的剪定（提案法）(Andral and Kamatani, 2024)\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\\nabla\\log\\pi とグリッド点上での接線は 自動微分 で計算\nグリッド数 N_{\\text{max}} を決めて，\\textcolor{#0096FF}{M(t)} を構成．\n\\textcolor{#0096FF}{M(t)}&lt;m(t) が起こったらグリッド数 N_{\\text{max}} を増やす．\n\n\n\n\n\n○ 最適化フリー＋自動微分のベクトライズで高速化可能○ 自動的かつ汎用的なアルゴリズム"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#解決法１剪定-lewis-shedler1979",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#解決法１剪定-lewis-shedler1979",
    "title": "動き出す次世代サンプラー",
    "section": "2.4 解決法１：剪定 (Lewis and Shedler, 1979)",
    "text": "2.4 解決法１：剪定 (Lewis and Shedler, 1979)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#解決法２確率的剪定-sen2020",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#解決法２確率的剪定-sen2020",
    "title": "動き出す次世代サンプラー",
    "section": "2.5 解決法２：確率的剪定 (Sen et al., 2020)",
    "text": "2.5 解決法２：確率的剪定 (Sen et al., 2020)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#新課題汎用的な上界-m-の見つけ方が存在しない",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#新課題汎用的な上界-m-の見つけ方が存在しない",
    "title": "動き出す次世代サンプラー",
    "section": "2.7 新課題：汎用的な上界 M の見つけ方が存在しない",
    "text": "2.7 新課題：汎用的な上界 M の見つけ方が存在しない\n\nm(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を強度に持つ非一様 Poisson 点過程をシミュレートする．\n\n\n\n\n\n\n○ スケーラビリティが達成可能\n\n\n m(t),\\pi の評価：全データが必要\n○ m(t) の不偏推定量 \\mu(t)\n　 データの一部を見るだけで計算可能\n　○ 収束先が変わらない\n\n\n\n\n 既存研究：確率的剪定 (Bierkens et al., 2019), (Sen et al., 2020)\n\n\n\n\n\n 汎用的なアルゴリズムが存在しない\n\n\n\n\\mu(t)\\le M\\;\\;\\text{a.s.}\n を達成する上界 M をどう見つければ良いのか？\n　 もっぱらロジスティック回帰くらい 　　　にしか使えなかった\n\n\n\n\n 今回の貢献：適応的剪定(Andral and Kamatani, 2024)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#直感pdmp-の強度関数",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#直感pdmp-の強度関数",
    "title": "動き出す次世代サンプラー",
    "section": "2.2 直感：PDMP の強度関数",
    "text": "2.2 直感：PDMP の強度関数\n\n１次元の Gauss 確率密度 \\pi を考える．\\textcolor{#78C2AD}{x_0} から，次の方向転換 \\textcolor{#78C2AD}{x_1} はどう決まるか？\n\n\n\n\\pi(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}\n \n\\therefore\\quad U(x)=-\\log\\pi(x)=\\frac{x^2}{2\\sigma^2}+\\mathrm{const.}\n\n\n\n\n\n\\textcolor{#78C2AD}{x_1} の打ち方\n\n\n\nm^{(1)}(t)=(v_0\\cdot U'(\\textcolor{#78C2AD}{x_0}+v_0t))_+=\\frac{v_0}{2\\sigma^2}1_{[0,\\infty)}(\\textcolor{#78C2AD}{x_0}+v_0t).\n を強度関数にもつ 非一様 Poisson 点過程 の最初の到着時刻 T_1^{(1)} について， \n\\textcolor{#78C2AD}{x_1}=\\textcolor{#78C2AD}{x_0}+v_0T_1^{(1)}\n とする．"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#直観１次元の場合",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#直観１次元の場合",
    "title": "動き出す次世代サンプラー",
    "section": "2.2 直観：１次元の場合",
    "text": "2.2 直観：１次元の場合\n\n\n\n\n１次元の Gauss 分布を考える：\n\n\\pi(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}\n \n\\therefore\\quad U(x):=-\\log\\pi(x)=\\frac{x^2}{2\\sigma^2}+\\mathrm{const.}\n\n負の対数尤度 U=-\\log\\pi は ポテンシャル ともいう．\n\n\n\n\n\n\n\\begin{align*}\n  m^{(1)}(t)&=(v_0\\cdot U'(\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}t))_+\\\\\n  &=(\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}t)\\cdot1{\\left\\{\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}t&gt;0\\right\\}},\\quad t\\ge0,\n\\end{align*} を強度にもつ 非一様 Poisson 点過程 の最初の到着時刻 T_1^{(1)} について，\\textcolor{#78C2AD}{x_1}=\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}T_1^{(1)} とすれば良い．"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#このあとの内容",
    "href": "posts/2024/Slides/IRT-ZigZag.html#このあとの内容",
    "title": "動き出す次世代サンプラー",
    "section": "このあとの内容",
    "text": "このあとの内容\n\n第１節：区分確定的マルコフ過程 PDMP （終わった）\n\nPDMPFlux パッケージの紹介\n\n第２節：PDMP のシミュレーションと課題  これから\n\n○ データの一部を見るだけで実行可能（スケーラビリティ）\n 自動化・汎用化することが難しい\n\n第３節：本パッケージの実装\n\n鍵となる技術：自動微分のベクトライズ"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#pdmp-のシミュレーションと課題",
    "href": "posts/2024/Slides/IRT-ZigZag.html#pdmp-のシミュレーションと課題",
    "title": "動き出す次世代サンプラー",
    "section": "2 PDMP のシミュレーションと課題",
    "text": "2 PDMP のシミュレーションと課題\n\n\n\nPDMP のシミュレーション＝緑色の点をどう打つか？\n\n\n\n2.1 課題：非一様 Poisson 点過程のシミュレーション\n\n\n\n\n\n\n\n\n\n\nPDMP シミュレーションの原則\n\n\n\n方向転換地点 \\textcolor{#78C2AD}{x_1,x_2,\\cdots} を決めれば良い\n＊これが難しい\n\n\n\n\n\n\n\n\n\\textcolor{#78C2AD}{x_1,x_2,\\cdots} の決め方\n\n\n\n確率密度 \\pi に収束させたい場合，強度 \nm^{(i)}(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を持つ非一様 Poisson 点過程をシミュレートする．\n 最初の到着点を \\textcolor{#78C2AD}{x_i}=\\textcolor{#78C2AD}{x_{i-1}}+vT_1^{(i)} とする．\n\n\n\n\n\n\n2.2 直観：１次元の場合\n\n\n\n\n１次元の Gauss 分布を考える：\n\n\\pi(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}\n \n\\therefore\\quad U(x):=-\\log\\pi(x)=\\frac{x^2}{2\\sigma^2}+\\mathrm{const.}\n\n負の対数尤度 U=-\\log\\pi は ポテンシャル ともいう．\n\n\n\n\n\n\n\\begin{align*}\n  m^{(1)}(t)&=(v_0\\cdot U'(\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}t))_+\\\\\n  &=(\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}t)\\cdot1{\\left\\{\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}t&gt;0\\right\\}},\\quad t\\ge0,\n\\end{align*} を強度にもつ 非一様 Poisson 点過程 の最初の到着時刻 T_1^{(1)} について，\\textcolor{#78C2AD}{x_1}=\\textcolor{#78C2AD}{x_0}+\\textcolor{#E95420}{v_0}T_1^{(1)} とすれば良い．\n\n\n\n\n2.3 課題：非一様 Poisson 点過程の到着時刻の計算\n\n\n\n2.4 解決法：剪定 (Lewis and Shedler, 1979)\n\n\n\n2.5 拡張：確率的剪定 (Sen et al., 2020)\n\n\n\n2.6 PDMP の美点：スケーラビリティが達成可能\n\nm(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を強度に持つ非一様 Poisson 点過程をシミュレートする．\n\n\n\n\n\n\n\n\n○ スケーラビリティが達成可能\n\n\n\n m(t),\\pi の評価：全データが必要\n○ m(t) の不偏推定量 \\mu(t)\n　 データの一部を見るだけで計算可能\n　○ 収束先が変わらない\n\n\n 既存研究：確率的剪定 (Bierkens et al., 2019), (Sen et al., 2020)\n\n\n\n\n\n\n2.7 新課題：汎用的な上界 M の見つけ方が存在しない\n\nm(t)=\\biggr(-v\\cdot\\nabla\\log\\pi(\\textcolor{#78C2AD}{x_{i-1}}+tv)\\biggl)_+,\\quad t\\ge0,\n を強度に持つ非一様 Poisson 点過程をシミュレートする．\n\n\n\n\n\n\n\n\n○ スケーラビリティが達成可能\n\n\n\n m(t),\\pi の評価：全データが必要\n○ m(t) の不偏推定量 \\mu(t)\n　 データの一部を見るだけで計算可能\n　○ 収束先が変わらない\n\n\n 既存研究：確率的剪定 (Bierkens et al., 2019), (Sen et al., 2020)\n\n\n\n\n\n\n\n 汎用的なアルゴリズムが存在しない\n\n\n\n\n\\mu(t)\\le M\\;\\;\\text{a.s.}\n を達成する上界 M をどう見つければ良いのか？\n　 もっぱらロジスティック回帰くらい 　　　にしか使えなかった\n\n\n 今回の貢献：適応的剪定(Andral and Kamatani, 2024)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#pdmp-のシミュレーションの自動化今回の貢献",
    "href": "posts/2024/Slides/IRT-ZigZag.html#pdmp-のシミュレーションの自動化今回の貢献",
    "title": "動き出す次世代サンプラー",
    "section": "3 PDMP のシミュレーションの自動化（今回の貢献）",
    "text": "3 PDMP のシミュレーションの自動化（今回の貢献）\n\n\n3.1 適応的剪定（従来法）(Corbella et al., 2022)\n\n\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\n\\nabla\\log\\pi は 自動微分 で計算\nホライゾン t_{\\text{max}} を決めて \n  M:=\\max\\{\\textcolor{#E95420}{\\text{３つの赤点}}\\}\n  \n\\|M-m(t)\\|_\\infty が大き過ぎたら t_{\\text{max}} を縮める\n\n\n\n \\displaystyle\\max_{t\\in[0,t_{\\text{max}}]}m(t) の計算に最適化が必要\n t_{\\text{max}} の調整アルゴリズム？\n\n\n\n\n3.2 パッケージの実装 (Andral and Kamatani, 2024)\n\n\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\n\\nabla\\log\\pi とグリッド点上での接線は 自動微分 で計算\nグリッド数 N_{\\text{max}} を決めて，\\textcolor{#0096FF}{M(t)} を構成．\n\\textcolor{#0096FF}{M(t)}&lt;m(t) が起こったらグリッド数 N_{\\text{max}} を増やす．\n\n\n\n○ 最適化フリー＋自動微分のベクトライズで高速化可能○ 自動的かつ汎用的なアルゴリズム"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#本パッケージの実装",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#本パッケージの実装",
    "title": "動き出す次世代サンプラー",
    "section": "3.2 本パッケージの実装",
    "text": "3.2 本パッケージの実装\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\\nabla\\log\\pi とグリッド点上での接線は 自動微分 で計算\nグリッド数 N_{\\text{max}} を決めて，\\textcolor{#0096FF}{M(t)} を構成．\n\\textcolor{#0096FF}{M(t)}&lt;m(t) が起こったらグリッド数 N_{\\text{max}} を増やす．\n\n\n\n\n\n○ 最適化フリー＋自動微分のベクトライズで高速化可能○ 自動的かつ汎用的なアルゴリズム"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#解決法剪定-lewis-shedler1979",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#解決法剪定-lewis-shedler1979",
    "title": "動き出す次世代サンプラー",
    "section": "2.4 解決法：剪定 (Lewis and Shedler, 1979)",
    "text": "2.4 解決法：剪定 (Lewis and Shedler, 1979)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#拡張確率的剪定-sen2020",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#拡張確率的剪定-sen2020",
    "title": "動き出す次世代サンプラー",
    "section": "2.5 拡張：確率的剪定 (Sen et al., 2020)",
    "text": "2.5 拡張：確率的剪定 (Sen et al., 2020)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#まとめ",
    "href": "posts/2024/Slides/IRT-ZigZag.html#まとめ",
    "title": "動き出す次世代サンプラー",
    "section": "まとめ",
    "text": "まとめ\n\n\n\n\n\n\n\n\n① PDMP は新たなモンテカルロ法の枠組み\n　もう武器は MCMC だけじゃない．\n② PDMP でしか出来ないことも多い\n　* バイアスを導入しないサブサンプリング\n　* \\delta_x 部分を持った非絶対連続分布からのサンプリング\n③ 以上の「良さ」を保ったまま，汎用パッケージ化できる\n\n\n\n 2025年，機械学習・統計でも動き出す……？\n\n\n\n\n非絶対連続分布からのサンプリングも可能 (Bierkens et al., 2023)"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#参考文献",
    "href": "posts/2024/Slides/IRT-ZigZag.html#参考文献",
    "title": "動き出す次世代サンプラー",
    "section": "参考文献",
    "text": "参考文献\n\n\nAndral, C., and Kamatani, K. (2024). Automated techniques for efficient sampling of piecewise-deterministic markov processes.\n\n\nAndrieu, C., and Livingstone, S. (2021). Peskun–Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario. The Annals of Statistics, 49(4), 1958–1981.\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nBierkens, J., Grazzi, S., Kamatani, K., and Roberts, G. O. (2020). The boomerang sampler. Proceedings of the 37th International Conference on Machine Learning, 119, 908–918.\n\n\nBierkens, J., Grazzi, S., Meulen, F. van der, and Schauer, M. (2023). Sticky PDMP Samplers for Sparse and Local Inference Problems. Statistics and Computing, 33(1), 8.\n\n\nBouchard-Côté, A., Vollmer, S. J., and Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. Journal of the American Statistical Association, 113(522), 855–867.\n\n\nCorbella, A., Spencer, S. E. F., and Roberts, G. O. (2022). Automatic Zig-Zag Sampling in Practice. Statistics and Computing, 32(6), 107.\n\n\nDiaconis, P. (2013). Some things we’ve learned (about Markov chain Monte Carlo). Bernoulli, 19(4), 1294–1305.\n\n\nLewis, P. A. W., and Shedler, G. S. (1979). Simulation of nonhomogeneous poisson processes by thinning. Naval Research Logistics Quarterly, 26(3), 403–413.\n\n\nMichel, M., Durmus, A., and Sénécal, S. (2020). Forward event-chain monte carlo: Fast sampling by randomness control in irreversible markov chains. Journal of Computational and Graphical Statistics, 29(4), 689–702.\n\n\nSen, D., Sachs, M., Lu, J., and Dunson, D. B. (2020). Efficient posterior sampling for high-dimensional imbalanced logistic regression. Biometrika, 107(4), 1005–1012.\n\n\nVasdekis, G., and Roberts, G. O. (2023). Speed up Zig-Zag. The Annals of Applied Probability, 33(6A), 4693–4746.\n\n\n\nHMC との関係\n\nMetropolis-Hastings ステップでは，尤度の比が 1 ならば棄却されない．\nそこで尤度の等高線をなぞることを考える．\n運動量をランダムにサンプリングすることでエルゴード性を担保する．\n\nただし尤度の等高線をなぞることは数値計算の問題になり難しいが，ハイパーパラメータをうまくチューニングすることでほとんど独立なサンプルを得ることができる．\n従来的には MCMC の１つとみれるが，ダイナミクスを複雑にした PDMP と見るべきかもしれない．\n\n尤度の等高線をなぞるダイナミクス\n\n→ 尤度の幾何情報を自然に取り入れた動きが可能\n\n運動量をリフレッシュするタイミングについての示唆？"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#パッケージの実装-andral-kamatani2024",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#パッケージの実装-andral-kamatani2024",
    "title": "動き出す次世代サンプラー",
    "section": "3.2 パッケージの実装 (Andral and Kamatani, 2024)",
    "text": "3.2 パッケージの実装 (Andral and Kamatani, 2024)\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\\nabla\\log\\pi とグリッド点上での接線は 自動微分 で計算\nグリッド数 N_{\\text{max}} を決めて，\\textcolor{#0096FF}{M(t)} を構成．\n\\textcolor{#0096FF}{M(t)}&lt;m(t) が起こったらグリッド数 N_{\\text{max}} を増やす．\n\n\n\n\n\n○ 最適化フリー＋自動微分のベクトライズで高速化可能○ 自動的かつ汎用的なアルゴリズム"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag.html#新パッケージの実装",
    "href": "posts/2024/Slides/IRT-ZigZag.html#新パッケージの実装",
    "title": "動き出す次世代サンプラー",
    "section": "3 新パッケージの実装",
    "text": "3 新パッケージの実装\n\n\n3.1 適応的剪定（従来法）(Corbella et al., 2022)\n\n\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\n\\nabla\\log\\pi は 自動微分 で計算\nホライゾン t_{\\text{max}} を決めて \n  M:=\\max\\{\\textcolor{#E95420}{\\text{３つの赤点}}\\}\n  \n\\|M-m(t)\\|_\\infty が大き過ぎたら t_{\\text{max}} を縮める\n\n\n\n \\displaystyle\\max_{t\\in[0,t_{\\text{max}}]}m(t) の計算に最適化が必要\n t_{\\text{max}} の調整アルゴリズム？\n\n\n\n\n3.2 新パッケージの実装 (Andral and Kamatani, 2024)\n\n\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\n\\nabla\\log\\pi とグリッド点上での接線は 自動微分 で計算\nグリッド数 N_{\\text{max}} を決めて，\\textcolor{#0096FF}{M(t)} を構成．\n\\textcolor{#0096FF}{M(t)}&lt;m(t) が起こったらグリッド数 N_{\\text{max}} を増やす．\n\n\n\n○ 最適化フリー＋自動微分のベクトライズで高速化可能○ 自動的かつ汎用的なアルゴリズム"
  },
  {
    "objectID": "posts/2024/Slides/IRT-ZigZag_Slides.html#新パッケージの実装-andral-kamatani2024",
    "href": "posts/2024/Slides/IRT-ZigZag_Slides.html#新パッケージの実装-andral-kamatani2024",
    "title": "動き出す次世代サンプラー",
    "section": "3.2 新パッケージの実装 (Andral and Kamatani, 2024)",
    "text": "3.2 新パッケージの実装 (Andral and Kamatani, 2024)\n\n\n\n\n\n\n\n\nアルゴリズム\n\n\n\n\\nabla\\log\\pi とグリッド点上での接線は 自動微分 で計算\nグリッド数 N_{\\text{max}} を決めて，\\textcolor{#0096FF}{M(t)} を構成．\n\\textcolor{#0096FF}{M(t)}&lt;m(t) が起こったらグリッド数 N_{\\text{max}} を増やす．\n\n\n\n\n\n○ 最適化フリー＋自動微分のベクトライズで高速化可能○ 自動的かつ汎用的なアルゴリズム"
  },
  {
    "objectID": "posts/2025/Posters/Sticky.html#参考ページ",
    "href": "posts/2025/Posters/Sticky.html#参考ページ",
    "title": "ベイズ変数選択の計算的解決",
    "section": "参考ページ",
    "text": "参考ページ\nPDMP とそのシミュレーションに関しては，次のスライドにわかりやすく解説されています：\n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\nNo matching items\n\n\n図は全て発表者開発のパッケージ PDMPFlux.jl によるものです．\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\nNo matching items\n\n\nSticky PDMP (Bierkens et al., 2023) に関するさらに詳しい内容，またはベイズ変数選択一般については，次の記事にまとめています：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025/Posters/Sticky.html#参考ページ集",
    "href": "posts/2025/Posters/Sticky.html#参考ページ集",
    "title": "ベイズ変数選択の計算的解決",
    "section": "参考ページ集",
    "text": "参考ページ集\nPDMP とそのシミュレーションに関しては，次のスライドにわかりやすく解説されています：\n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n一致なし\n\n\n図は全て発表者開発のパッケージ PDMPFlux.jl によるものです．\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n一致なし\n\n\nSticky PDMP (Bierkens ほか, 2023) に関するさらに詳しい内容，またはベイズ変数選択一般については，次の記事にまとめています：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "posts/2025/Posters/NewRegime.html",
    "href": "posts/2025/Posters/NewRegime.html",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能",
    "section": "",
    "text": "Date\nLocation\n\n\n\n\nMar. 8th, 202512:00-12:50\n筑波大学東京キャンパス文京校舎1階学生ラウンジ（正面玄関横）\n\n\n\n\n\n\n\n画像をクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/NewRegime.html#計算技術による学際的統計解析ワークショップ-isact2025",
    "href": "posts/2025/Posters/NewRegime.html#計算技術による学際的統計解析ワークショップ-isact2025",
    "title": "ベイズ変数選択の計算的解決",
    "section": "",
    "text": "Date\nLocation\n\n\n\n\nFeb. 17-18th, 2025\nISM D305\n\n\n\n\n\n\n\nクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/NewRegime.html#参考ページ集",
    "href": "posts/2025/Posters/NewRegime.html#参考ページ集",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能",
    "section": "参考ページ集",
    "text": "参考ページ集\nPDMP とそのシミュレーションに関しては，次のスライドにわかりやすく解説されています：\n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n一致なし\n\n\n図は全て発表者開発のパッケージ PDMPFlux.jl によるものです．\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n一致なし\n\n\nSticky PDMP (Bierkens ほか, 2023) に関するさらに詳しい内容，またはベイズ変数選択一般については，次の記事にまとめています：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "posts/2025/Posters/NewRegime.html#第19回日本統計学会春季集会-jss-2025-spring",
    "href": "posts/2025/Posters/NewRegime.html#第19回日本統計学会春季集会-jss-2025-spring",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能",
    "section": "",
    "text": "Date\nLocation\n\n\n\n\nMar. 8th, 202512:00-12:50\n筑波大学東京キャンパス文京校舎1階学生ラウンジ（正面玄関横）\n\n\n\n\n\n\n\n画像をクリックで PDF を表示"
  },
  {
    "objectID": "posts/2024/Posters/OH.html",
    "href": "posts/2024/Posters/OH.html",
    "title": "新時代の MCMC を迎えるために",
    "section": "",
    "text": "発表ポスター「新時代の MCMC を迎えるために」のページへ\n\n\n\n概要：物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC はどのような手法なのか？従来法を超えるのか？どのような場面で使えるのか？……等々疑問は尽きません．この新たな手法を正しく受け止めるために，現状の MCMC への理解から，新手法がどのように生まれたかの軌跡を辿り，現状の理解を確かめます．\n\n\n統数研オープンハウス\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 24th, 2024\nISM and online (Hybrid)\n\n\n\n\n\n\n\nオープンハウス案内"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#数学的骨格",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#数学的骨格",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "1 数学的骨格",
    "text": "1 数学的骨格\n詳しくは 本サイトの数学記法一覧 を参照．\n\n1.1 確率空間\n\n\n\n\n\n\n定義（確率空間）\n\n\n\n\n任意の集合 \\(\\Omega\\) に対して，確率の公理 [P1], [P2], [P3] を満たす関数 \\[\n\\operatorname{P}:\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\to[0,1]\n\\] が定義されているとき，組 \\((\\Omega,\\operatorname{P})\\) を 確率空間 (probability space) という．\n確率空間上の実数値の関数 \\[\nX:\\Omega\\to\\mathbb{R}\n\\] を 確率変数 (random variable) という．1\n確率変数 \\(X:\\Omega\\to\\mathbb{R}\\) に対して， \\[\n\\operatorname{P}^X[A]:=\\operatorname{P}[X\\in A]\n\\] で定まる実数 \\(\\mathbb{R}\\) 上の分布を，\\(X\\) の 確率分布 (probability distribution) という．\n\n\n\n集合 \\(\\Omega\\) というのを自由にとって良いというのが，確率論の懐の広さであり，統計学で出会う多種多様な問題に対応できる所以である．\nサイコロの出目を考える場合は \\(\\Omega=\\{1,2,3,4,5,6\\}\\) ととってその上の確率空間に関する理論を借りれば良い．殆どの場合は \\(\\Omega=\\mathbb{R}\\) と取ることになる．\n\n\n\n\n\n\n注（厳密な定義）\n\n\n\n\n\n本当は確率空間は３組 \\((\\Omega,\\mathcal{F},\\operatorname{P})\\) である．新たに加わった \\(\\mathcal{F}\\) とは何かというと，\\(\\operatorname{P}\\) の定義域であり，上の定義で \\[\n\\operatorname{P}:\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\to[0,1]\n\\] としていたところを \\[\n\\operatorname{P}:\\mathcal{F}\\to[0,1]\n\\] と，定義域を制限するのが厳密な定義である．\nしたがって，標本空間 \\(\\Omega\\) の部分集合はなんでも事象と呼んでいいかというと，数学的にはそうではなく，事象の全体 \\(\\mathcal{F}\\) は一定の（代数的な）規則を満たす必要がある（完全加法性 という）．\nこれは 測度論 (measure theory) と呼ばれる数学分野から得られる知見である．\nなぜ制限しなければいけないのか？は，そうしなければ数学的な矛盾が起こるからなのであるが，普通に統計学の目的で確率論を用いる範囲でこの矛盾に遭遇することは滅多にないので，ここでは触れない．\n\n\n\n\n\n1.2 確率変数の概念\n確率変数の概念は，確率論において最も重要なものである．David Mumford というフィールズ賞も受賞した世界的な数学者（専門が確率論というわけではない）も，次のように述べている：\n\nThe basic object of study in probability is the random variable and I will argue that it should be treated as a basic construct, like spaces, groups and functions, and it is artificial and unnatural to define it in terms of measure theory. (Mumford, 2000, p. 108)\n\n確率変数が重要な理由は，それは確率分布と違うということを徹底的に教えてくれることにある．換言すれば，日常的な感覚で確率を議論するとなかなかモヤモヤが解消せずに解った気になれない理由は，確率変数と確率分布という本来別々の存在を人間は混同してしまいがちだからからである，と教えてくれるのが現代の確率論なのである．\n中高の数学での「場合の数と確率」は特に混同の傾向が強い．三角関数がどのように社会の役に立つか不思議に思ったことがあるならば，あそこで習った初頭的な議論がどう統計学に応用されてどうして AI が生まれるに至ったのかたいへん不思議であろう．中高での離散的な議論を連続な場合にも通用するようにするためには，確率分布と確率変数を峻別することが肝要 である．\n確率変数は，「変数」の概念の確率化 である．変数は，高校数学などでも \\(x,y,z,\\cdots\\) と小文字で表したが，確率変数は \\(X,Y,Z,\\cdots\\) と大文字で表す． \\(\\Omega=\\{*\\}\\) と標本空間を一点集合と取った場合，確率変数は通常の決定論的な変数と同義になる．\n\n\n\n\n\n\n発展（確率過程）\n\n\n\n\n\n一方で，高校数学などでも扱う「関数」の概念の確率化は　確率過程 という．確率過程は名前は仰々しいかも知れないが，定義自体はなんてことはない，確率変数の集合のこと である．\n例えば，日付 \\(n\\) の株価 \\(X_n\\) の列 \\(X_1,X_2,X_3,\\cdots\\) は確率過程である．決定論的な関数 \\(n\\mapsto x_n\\) の確率化である．\n\n\n\n\n\n1.3 分布の押し出し\nでは実際に，確率分布と確率変数がどう違うかを説明する．\n確率分布は確率空間に宿るもので，確率変数は確率空間を繋ぐものである．\nサイコロを２回振った出目の全体を標本空間とするならば， \\[\n\\Omega:=[6]\\times[6]=\\left\\{(1,1),(1,2),\\cdots,(1.6),(2,1),\\cdots\\right\\}\n\\] という集合の上に，一様分布\n\n\n\n\n\n\n\n\n\nを定義して得る確率空間を考えるのが一つの良い方法であろう．\nこれが確率分布である．\n一方で，確率変数は，標本空間上の関数の全てである．例えば，出た目の和は確率変数である．\n最も重要なことは，確率変数は分布を押し出す ということである．\n実際，出た目の和は，確率分布を押し出して，次のような確率分布を定める："
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#今回の内容",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "2 今回の内容",
    "text": "2 今回の内容\n数学的骨格を理解した状態で，今回範囲の内容 (草野耕一, 2016, pp. 73–96) を整理する．\n\n2.1 母集団と標本 (pp.73-75)\n\n2.1.1 母集団\n標本調査が行われるとき，調査対象となる全体集団を 母集団 (population) という．全人口を精査することは困難であるため，ここから無作為に一部分を選ぶことになる．これを 標本調査 (survey sampling) といい，得られたデータを 標本 (sample) という．\nすなわち，データとは確率変数 \\(X_1,\\cdots,X_n\\) であり，これらの積が定める確率変数 \\[\nX=(X_1,\\cdots,X_n):\\Omega\\to\\mathbb{R}^n\n\\] を考え，母集団を \\(\\Omega\\) とし，確率変数 \\(X_1,\\cdots,X_n\\) を標本とするのである．\n\\(X_1,\\cdots,X_n\\) が，母集団となる確率空間 \\((\\Omega,\\operatorname{P})\\) に関する情報をなるべく効率よく伝えてくれるように設計するのが重要である．実際，現代の標本調査では，無作為抽出が基本であり，一昔前では電話番号台帳の下一桁を無作為に選び，電話を掛けるという方法が用いられた．当然この場合，電話を持っていない標本 \\(\\omega\\in\\Omega\\) についての情報は得られないので，その点に関する補正が必要になる，という具合である．\nこのように，「無作為」と言っても具体的にどのように選べば良いか？を考える分野を 標本調査法 (sampling theory) という．2 大統領選挙を通じての標本抽出法の発展の例は Section 3.2 に付した．\n\n\n2.1.2 統計量の例\n標本の関数を 統計量 (statistic) という．\n\n\n\n\n\n\n定義（３つの標本統計量）\n\n\n\n\\(x_1,\\cdots,x_n\\) を標本とする．\n\n次を 標本平均 という： \\[\n\\overline{x}:=\\frac{x_1+\\cdots+x_n}{n}.\n\\]\n次を 標本分散 という： \\[\ns^2:=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2.\n\\]\n標本分散の非負の平方根 \\(s:=\\sqrt{s^2}\\) を 標本標準偏差 という．\n次を 不偏分散 という： \\[\nu^2:=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\overline{x})^2.\n\\]\n\n\n\n\n\n\n\n\n\n命題（分散公式）\n\n\n\n標本分散 \\(s^2\\) と標本平均 \\(\\overline{x}^2\\) の間には次の関係が成り立つ： \\[\ns^2=\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2.\n\\] \\(\\frac{1}{n}\\sum_{i=1}^nx_i^2\\) という量を 標本の２次の絶対積率 という．\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\n\\begin{align*}\n    s^2&=\\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^n(x_i^2-2x_i\\overline{x}+\\overline{x}^2)\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^nx_i^2-2\\overline{x}\\frac{1}{n}\\sum_{i=1}^nx_i+\\overline{x}^2\\\\\n    &=\\frac{1}{n}\\sum_{i=1}^nx_i^2-\\overline{x}^2.\n\\end{align*}\n\\]\n\n\n\n\n\n2.1.3 母数とは何か？\n本書 (草野耕一, 2016, p. 75) に\n\n標本に統計量があるように母集団にもその特性を表す数値が備わっているはずであり，そのような数値のことを 母数 という．\n\nとあるが，この文脈では母数ではなく 特性値 という (竹村彰道, 2020)．特性値を母数と設定することが多いが，それはあくまで統計解析者の裁量である．\n母数とは次に示すように，推定対象として解析者が設定する母集団の特性値 である．標本統計量は実際に計算できるが，特性値と母数は未知である．\nよって，ほとんどの場合，推測統計の問題とは母数推定の問題に他ならない．\n\n\n\n\n\n\n定義（統計モデル，母数）\n\n\n\n\n母集団上の確率分布の族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を 統計モデル という．\n統計モデルを添字付ける「番号」 \\(\\theta\\in\\Theta\\) を 母数 という．\n\n\n\n真の分布を \\(\\operatorname{P}\\) としたとき，これを近似すると思われる分布族 \\(\\{P_\\theta\\}_{\\theta\\in\\Theta}\\) を統計解析者が設定するのである．腕の見せ所である．\nあなたが保険数理士だとして，重大事故の発生確率を推定する際， \\[\n\\Omega=\\left\\{0,1,2,\\cdots\\right\\}\n\\] を一定期間内に起こる重大事故件数とすると，これに対する分布族は Poisson 分布族 \\(\\{\\mathrm{Pois}(\\lambda)\\}_{\\lambda&gt;0}\\) を取ると近似精度が良いことが知られている．Poisson 分布の母数 \\(\\lambda&gt;0\\) は 到着率 や 強度 と呼ばれる．\n\n\n\n2.2 統計推測の技法(1) (pp.75-85)\n本書 (草野耕一, 2016, pp. 75–85) の重大な特徴に，確率変数と確率分布を区別していないという問題がある．\n確率分布とは 第１回講義 で導入した，３つの公理を満たす集合関数 \\(\\operatorname{P}:P(\\Omega)\\to[0,1]\\) である．3 このとき，ペア \\((\\Omega,\\operatorname{P})\\) を，確率が定義された集合という意味で 確率空間 という．\n確率変数 とは，確率空間 \\((\\Omega,\\operatorname{P})\\) 上に定義された関数 \\(X:\\Omega\\to\\mathbb{R}\\) のことである．特に，値域が \\(\\mathbb{N}\\subset\\mathbb{R}\\) に限る場合を 離散変数 という．\n\n2.2.1 離散の場合\n確率変数 \\(X\\) の取り得る値が \\(\\mathbb{N}=\\{0,1,2,\\cdots\\}\\) に限る場合が離散の場合である．\n\n\n\n\n\n\n定義（期待値）\n\n\n\n\n非負値の関数 \\(f:\\mathbb{N}\\to[0,1]\\) であって次を満たすものを 確率（質量）関数 という： \\[\nf(n)=\\operatorname{P}[X=n]\n\\]\n確率関数 \\(f:\\mathbb{N}\\to[0,1]\\) に従う確率変数 \\(X:\\Omega\\to\\mathbb{N}\\) に対して，次の量 \\(\\operatorname{E}[X]\\) を 期待値 という： \\[\n\\operatorname{E}[X]:=\\sum_{n=1}^\\infty nf(n)=\\sum_{n=1}^\\infty n\\operatorname{P}[X=n].\n\\]\n次の量 \\(\\mathrm{V}[X]\\) を \\(X\\) の 分散 という： \\[\n\\mathrm{V}[X]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])^2\\biggr].\n\\]\n\n\n\n\n\n2.2.2 連続の場合\n\n\n\n\n\n\n定義（期待値）\n\n\n\n\\((\\Omega,\\operatorname{P})\\) を確率空間，\\(X:\\Omega\\to\\mathbb{R}\\) をその上の確率変数とする．\n\n非負値の関数 \\(F:\\mathbb{R}\\to[0,1]\\) であって次を満たすものを，\\(X\\) の （累積）分布関数 という： \\[\nF(x):=\\operatorname{P}[X\\le x].\n\\]\n非負値の関数 \\(p:\\mathbb{R}\\to\\mathbb{R}_+\\) であって次を満たすものが存在するならば，これを \\(X\\) の （確率）密度関数 という： \\[\n\\operatorname{P}[X\\in A]=\\int_Ap(x)\\,dx.\n\\]\n確率密度 \\(p\\) に従う確率変数 \\(X:\\Omega\\to\\mathbb{R}\\) に対して，次の量 \\(\\operatorname{E}[X]\\) が存在するならば，これを \\(X\\) の 期待値 という： \\[\n\\operatorname{E}[X]:=\\int_\\mathbb{R}xp(x)\\,dx.\n\\]\n次の量 \\(\\mathrm{V}[X]\\) が存在するならば，これを \\(X\\) の 分散 という： \\[\n\\mathrm{V}[X]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])^2\\biggr].\n\\]\n\n\n\n\n\n2.2.3 期待値の性質\n\n\n\n\n\n\n命題（期待値の性質）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n（期待値の線型性）任意の \\(a,b\\in\\mathbb{R}\\) について， \\[\n\\operatorname{E}[aX+bY]=a\\operatorname{E}[X]+b\\operatorname{E}[Y].\n\\]\n（分散公式）次が成り立つ： \\[\n\\mathrm{V}[X]=\\operatorname{E}[X^2]-(\\operatorname{E}[X])^2.\n\\]\n（分散の斉次性）任意の \\(a,b\\in\\mathbb{R}\\) について， \\[\n\\mathrm{V}[aX+b]=a^2\\mathrm{V}[X].\n\\]\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\n積分の線型性から従う．\n\nの期待値の線型性のみから従う．\n\n\n\n\n\n\n\n2.2.4 独立性と共分散\n\n\n\n\n\n\n定義（確率変数の独立性）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n\\(X,Y\\) が互いに 独立 であるとは，任意の事象 \\(A,B\\subset\\Omega\\) について， \\[\n\\operatorname{P}[X\\in A,Y\\in B]=\\operatorname{P}[X\\in A]\\operatorname{P}[Y\\in B]\n\\] を満たすことをいう．\n\\(X,Y\\) の 共分散 とは， \\[\n\\mathrm{Cov}[X,Y]:=\\operatorname{E}\\biggl[(X-\\operatorname{E}[X])(Y-\\operatorname{E}[Y])\\biggr]\n\\] をいう．\n\n\n\n\n\n\n\n\n\n命題（独立確率変数の性質）\n\n\n\n\\(X,Y:\\Omega\\to\\mathbb{R}\\) を確率変数とする．\n\n\\(X,Y\\) が独立ならば，次が成り立つ： \\[\n\\operatorname{E}[XY]=\\operatorname{E}[X]\\operatorname{E}[Y].\n\\]\n次が成り立つ： \\[\n\\mathrm{V}[X+Y]=\\mathrm{V}[X]+2\\mathrm{Cov}[X,Y]+\\mathrm{V}[Y].\n\\]\n（独立ならば無相関）\\(X,Y\\) が独立ならば， \\[\n\\mathrm{Cov}[X,Y]=0.\n\\] 特に，\\(X,Y\\) が独立ならば，\\(\\mathrm{V}\\) は加法を保存する．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\n\n\nLebesgue 積分論の議論が必要なので省略する．\n\nのみから従う．式変形は次の通り： \\[\n\\begin{align*}\n\\mathrm{V}[X+Y]&=\\operatorname{E}[(X+Y)^2]-\\biggr(\\operatorname{E}[X]+\\operatorname{E}[Y]\\biggl)^2\\\\\n&=\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n2.3 統計推測の技法(2) (pp.85-94)\n\n2.3.1 正規分布\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider\nimport seaborn as sns\n\n# 正規分布のグラフを描画する関数\ndef plot_normal_distribution(variance):\n    mean = 0  # 平均値\n    sigma = np.sqrt(variance)  # 標準偏差（分散の平方根）\n    \n    # 正規分布のデータを生成\n    x = np.linspace(-10, 10, 1000)\n    y = (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (x - mean)**2 / (2 * sigma**2))\n    \n    # グラフを描画\n    plt.figure(figsize=(3, 2))\n    sns.lineplot(x=x, y=y)\n    plt.title(f'Normal Distribution with Variance {variance}')\n    plt.xlabel('Value')\n    plt.ylabel('Probability Density')\n    plt.show()\n\n# インタラクティブなウィジェットを作成\ninteract(plot_normal_distribution, variance=FloatSlider(value=1, min=0.1, max=5, step=0.1))\n\n\n\n\n\n&lt;function __main__.plot_normal_distribution(variance)&gt;\n\n\n\n\n2.3.2 Bernoulli分布と二項分布\n\n\n2.3.3 Poisson分布\n\n\n\n2.4 統計推測の技法(3) (pp.94-97)\nここで重要なトピックは不偏分散である．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#補足",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#補足",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "3 補足",
    "text": "3 補足\n\n3.1 第1節：母集団と標本\n\n3.1.1 「推測統計学」とは何か？\n本書 (草野耕一, 2016, p. 73) でも次のような注がなされている：\n\n推測統計学に対して，証拠から得られた情報をいかに効率的かつ明確に表現するかを研究する統計学の分野を記述統計学という (草野耕一, 2016, p. 73)．\n\n現代では「統計学」と言った際にほとんど推測統計学を指すと言っても過言ではない．4 つまり，現代では殆ど形骸化した区別である．この名称の本当の意味を理解するためには，歴史的な情緒を持った文脈が必要である．\n一言で言えば，推測統計学は Ronald A. Fisher の理論が出て来た際に，それ以前の Quetelet からの統計学との断絶を強調するために用いられた語であった．\n\n推計学 stochastic は推計と計画のための科学であり，その建設は主として英国の農学者 R. A. Fisher （現在 Cambridge 大学教授）の構想に懸る．–(増山元三郎, 1950, p. 3)\n\n\n\n\n\n\n\n戦後当時の理解\n\n\n\n\n\nその態度の違いは，次の例が鮮明に示している．\n\n例えば算術平均 mean という概念について：旧来の考え方に従えば平均 average は集団の ‘代表’ 値であると定義されている．5 従って，算術平均が該集団をよく ‘代表’ しうると考えられる場合にはこれを採用し，しからざる場合には度数分布の形を眺めた上で，その他の平均値，例えば並数 mode なり幾何平均 geometrical mean なりをもって，これに代えるのである．だから ‘代表値’ として如何なる平均値を選ぶかは，この際，全く個々人の常識に委ねられてしまう．これに反して推計学が算術平均を採用するのは，それが分布関数として表現せられた母集団の或る常数（即ち母数）の適切な平均値となりうる場合のみに限られる．従来統計学を定義して ‘平均の学である’ となす立場があるけれども（例えば A. L. Bowley）このような考え方こそ統計学の記述的性格を遺憾なく露呈するものであろう．統計学がこのような原理に立つ限り，それは爾後の行動に関し，形式以上に何ら指針を与える力をもちうるものではない．行動の正しい指針を与えないような学問は実は科学の名に値しないものというべきであろう．これにも拘らず推計学は吾国では最近に到るまで，科学・技術の分野でさえ仲々受け入れられず政治・経済の領域では殆ど問題にもされなかったのである．これに反し，米国などでは，推計学が社会・自然のあらゆる分野に進出し，第２次大戦の遂行にあたっても大きな貢献をなしたのであった．–(増山元三郎, 1950, p. 4)\n\n\n\n\n前者の記述統計学的な動機は，現代では「データサイエンス」のような分野に引き継がれている (Hoaglin et al., 2006)．\n\n\n\n3.2 Gallup事件\n\n3.2.1 1936年大統領選挙とクオータ抽出の重要性\n\n\n\n\n\n\nRoosevelt v.s. Landon (1936)\n\n\n\n\n背景には1929年10月24日の「暗黒の木曜日」に端を発した世界大恐慌があった．\n\n民主党 Franklin D. Roosevelt は再選を目指し，共和党の Alfred Landon が立ち向かった．\nRoosevelt の保守的な姿勢は大恐慌を食い止めるには力不足と思われ，再選の見込みは低いという意見も強く，The Literary Digest は237万人6 を対象に回収した調査結果から，57% の得票で Landon が勝つだろうと予測した．\n\n一方で Gallup 率いる the American Institute of Public Opinion は3000人の標本から Roosevelt が 55.7% の得票を得て当選するだろう，と予測した．7\n\n結果，Roosevelt が 60% の得票を得て，48州中46州を手にした．\nなぜ The Literary Digest は予測を誤ったのか？その原因は不適切な標本抽出法にあった．\n\nThe Literary Digest は自誌の購読者（大恐慌の最中でも購読した層）を対象に，そして自動車保有者と電話利用者の名簿を使って約1000万人に郵送し，回収された237万人の回答を用いた．\n過去5回の大統領選挙で的中させていたのは，経済的な状況があまり投票結果に影響しない時勢だったためと思われる．\n一方，Gallup は母集団を層別してサンプルサイズを割り当て，そのクオータに沿って標本を集める非確率的抽出法を用いていた．なお，Gallup は4ヶ月前から，The Literary Digest の予測は外れるだろうと新聞のコラム上で予言していた．\n\n\n\nこのエピソードは 不適切なデザイン下で収集された大量データよりも良いデザイン下で収集された少量のデータのほうがずっと役に立つ ということの好例として強調されることとなった．\nしかし，話はここでは終わらない．その Gallup も，後の大統領選挙で予測を大きく外している．\n\n\n3.2.2 1948年大統領選挙と無作為抽出の重要性\n\n\n\n\n\n\nTruman v.s. Dewwey (1948)\n\n\n\n\n1948年の選挙では，民主党は在任中に斃れた Roosevelt の後を継いでいた Harry Truman，共和党は4年前に Roosevelt に負けた Thomas Dewey が戦った．\nこの年の背景には公民権問題があり，共和党が20年ぶりに政権を奪還すると予想されており，Gallup もその例にもれなかった．\n\n結果，Truman が僅差で Dewey を破って当選した．\nなぜクオータ法を用いたサンプリングで実績を出した Gallup は，今回は予測を大きく誤ったのか？\n\n今回 Gallup も予想に失敗して世論調査そのものに懐疑の目が向けられたことを重く見て，検討委員会が設置された．\nそこで論点となったのが，当時 Gallup が用いていた割当法では，層内の個々の対象者の決定が調査員の個人的判断に委ねられていたことが多きなバイアスの原因となっていると予想された．\nその結果，今日では 無作為抽出 が大原則として一層強調されるエピソードとなっている (総務省統計局, 2023年4月21日確認)．\nだがこれ自体が原因だとは言えない．事実，調査担当員もそのことは自覚しており，予測を修正する調整技術を独自に開発して用いていたという (佐藤寧, 2020)．どんな標本調査法にも偏りがあり，これを修正するための予測モデルと併用するという営みは現在の無作為抽出法でも同様であり，これ自体が問題ではない．\n当時（現在も）広く用いられている電話調査という手法が，1948年代当時では裕福な有権者（電話を購入することができ，また不変の住所を維持していた）に偏ったサンプル抽出に導いたという議論もある．8\n\n\n\n\n\n\n(佐藤寧, 2020, p. 15) より\n\n\n1947年時点での GHQ による日本への統計指導でも，すでに無作為抽出法（当時は「任意見本法」）による調査が指導されている (佐藤寧, 2020)．よって，当時からクオータ法の問題は認識されており，これに必要な対策を打つ形で運用されていたと解すのが妥当であろう．\nなお，日本側のエピソードとして，統計数理研究所第７代所長も務めた 林知己夫 のオーラルヒストリーに次のような一節がある：\n\nそんな時に，CIEの担当官は，日本の新聞社を集めて，「アメリカではクォータサンプルでやっているけれど，そんなのはサンプリングじゃない」と，トルーマン，デューイの大統領選挙の予測を持ち出してきてですね，「これはクォータサンプリングでやったから間違えたんだ．こんなもの夢夢やるんじゃないぞ」と．そうしてみんな肝に銘じたんですよね．サンプリングは厳正にやらなきゃいけないって教わったわけです (高橋正樹, 2004)．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理4.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理4.html#footnotes",
    "title": "法律家のための統計数理（４）推測統計学",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nただし，\\(\\mathbb{R}\\) とは実数の全体からなる集合とした．↩︎\n(Wu and Thompson, 2020) など．↩︎\n数学では \\(P(\\Omega):=\\left\\{\\Omega\\;\\text{の部分集合の全体}\\right\\}\\) と表す．これを 冪集合 (power set) という．その頭文字の \\(P\\) である．↩︎\nなお，(増山元三郎, 1950) の序によると，推測統計学 という語は北川敏男によるものであり，増山は 推計学 (stochastics) と呼んでいる．↩︎\n\\(\\phi(y,y,\\cdots)=\\phi(x_1,x_2,\\cdots)\\) なる関係が成り立つとき，\\(y\\) を \\(x_1,x_2,\\cdots\\) の平均という．参照：(Jevons, 1879, p. 391)↩︎\n(中山健夫, 2003)↩︎\n(鈴木督久, 2021) によると，実際はこれは史実の誤解であるようだ．Gallup が「Digest は Landon が 56% だとして予測を誤るだろう」というコラムを新聞社に送付するのに用いた標本が3000なのであって，Gallup 自身の選挙予測調査の標本サイズは30万人であったという．なお，その際の抽出法については歴史的な文献が欠けており，知る由がないという．とは言え，それでも，「標本は量より質」という教訓になる，という意味では，象徴的なエピソードであることは間違いない．↩︎\nWikipedia の記述．↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "",
    "text": "統計的検定の考え方と，その科学的な態度については，(大塚淳, 2020, pp. 97–106) が大変含蓄が深い．\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#仮設検定",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#仮設検定",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "1 仮設検定",
    "text": "1 仮設検定\n\n1.1 二項モデルでの検定\n\n\n\n\n\n\n問題\n\n\n\n冠動脈バイパス手術を受けた20歳の青年が３日後に死亡した．\n\n同病院では過去３年の30件のうち10人が術後１週間以内に死亡している．\n一般に術後１週間以内に死亡する確率は0.2である．\n\n不審だと言えるだろうか？言えるとしたらどのような意味で？\n\n\n「正常な範囲内の事象である」とする帰無仮説の下で，当該事象が起こる確率を計算する．これが5%以下だったら「不審だと思うに足る」と言えるだろう．\n本問題は死亡率 \\(p\\in[0,1]\\) という母数に関する検定問題と捉えることができ，すると帰無仮説は \\[\nH_0:p=0.2\n\\] というシンプルな表示を得る．\nこの下で，\\(n=30\\) として，確率変数 \\(X\\) を「術後１週間以内に死亡する人数」とすると，\\(X\\) は二項分布 \\(\\mathrm{Bin}(30,0.2)\\) に従う（二項分布の定義は Section 2.1 ）．\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n\n# Parameters for the binomial distribution\nn = 30  # number of trials\np = 0.2 # probability of success in each trial\n\n# Generating data for the binomial distribution\nx = np.arange(0, n+1)\ny = binom.pmf(x, n, p)\n\n# Plotting the binomial distribution\nplt.figure(figsize=(3.5, 3))\nplt.plot(x, y, 'bo', markersize=5)\nplt.vlines(x, 0, y, colors='b', lw=5)\nplt.title('Binomial Distribution - n=30, p=0.2')\nplt.xlabel('Number of Deaths')\nplt.ylabel('Mass')\nplt.grid(False)\nplt.show()\n\n\n\n\n\n\n\n\n\nこの図からも，10人以上になる確率は極めて小さいことが判るだろう．実際に計算してみると，\n\n\nCode\nprob_10_or_more = 1 - binom.cdf(9, n, p) - binom.pmf(10, n, p)/2\nprint(f\"p-value is {prob_10_or_more:.4f}\")\n\n\np-value is 0.0434\n\n\nとなる．この「帰無仮説 \\(H_0\\) （今回は「１週間死亡率は20%」）を仮定した下で，実際に観測した事象（今回は「30人のうち10人が一週間死亡」）が起こる条件付き確率」を \\(p\\)-値 と呼ぶ．1\n\n\n\n\n\n\n注（検定統計量の選び方）\n\n\n\n\n\n上で叙述したのは，「死亡者数」という離散確率変数を検定統計量に用いた場合である．しかし，本書 (草野耕一, 2016, p. 99) では，別の離散検定統計量に対して，正規近似を通じて計算している．これは \\(z\\)-検定と呼ばれるものである（ Wikipedia も参照）．\n計算機が得意ならば，直接計算で出した方が近似誤差がないため，好ましいだろう．実際，書籍で得られた値は \\(p=0.0344\\) であり，過小評価している．その論拠は「\\(pn,(1-p)n\\) のいずれも \\(5\\) 以上であれば正規分布と同一視して良いことが知られている」という点である．\n一方で，上の議論では \\(p=0.0611\\) と5%の水準を超えている．一方で，(Lancaster, 1961) の mid-\\(P\\) value と呼ばれる補正法を用いると \\(p=0.0434\\) となり，再び有意になる．\nこのことをどう評価するべきか……．技術的・専門的すぎてとても人口に膾炙するものではないと思うと同時に，非常に本来的ではない議論になっていると感ずる（ Section 1.5 ）．\n\n\n\n\n\n1.2 誤り\n帰無仮説を間違えて棄却してしまうことを，第一種の過誤 という．これは有意水準 \\(\\alpha\\) の値に一致する．\n統計的仮説検定の理論は，初めは Neyman と Pearson によって科学的発見の文脈で考えられたものであるため，帰無仮説の棄却は「科学的発見の萌芽」と同義と解すことが多い（(大塚淳, 2020, pp. 97–106) も参照）．その場合，第一種の誤りとは「本当はなんでもないのに大発見だと思い上がりってしまう確率」である．これを犯す確率を最も制限したい，という志向を持つ．\n次に，第一種の誤りの可能性 \\(\\alpha\\) を制限した上で，本当は帰無仮説が誤りなのに棄却できないリスク＝発見を検出できないリスクをなるべく下げることを二次的目標として考える．これを 第二種の過誤 という．その確率 \\(1-\\beta\\) に対して，\\(\\beta\\) を 検出力 (power) と呼ぶ．\nこれは 第二回で扱った検察官の誤謬 に通じる語用法である．\n\n\n1.3 独立性の検定\n\n\n\n\n\n\n問題\n\n\n\n次の条件を持つ学習教材が「必ず英語の成績が上がる」と言えるだろうか？\n\n全国共通模試で，英語の全国平均点は58点であった．\n当該教材を用いて勉強した者80名の平均は70点であり，標準偏差は15であった．\n\n\n\nこれは，当該教材を用いた群と用いていない群という「２つの標本」の間に差がないこと，今回では「平均が同じであること」を帰無仮説として，目下の証拠からこれを棄却出来るかを検定する問題として捉えることができる．\n\n\n1.4 区間推定\n\n\n1.5 ベイズ統計学\n\n伝統的統計学は客観確率を用いているので，母数の確からしさを１つの数値として示すことができない．伝統的統計学が示しうるものは，「母数がある範囲内にあればこの証拠が現れる確率はいくらであるか」でしかないのである．この矛盾をいかに克服するかは法律家と統計学者が共同して取り組むべき今後の課題であるが，１つの可能性として考えうることは伝統的統計学に代えてベイズ統計学の手法を用いることである．(草野耕一, 2016, p. 119)"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#定義集",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#定義集",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "2 定義集",
    "text": "2 定義集\n\\[\n[a,b]:=\\left\\{x\\in\\mathbb{R}\\mid a\\le x\\le b\\right\\}\n\\] は実数の区間を表す． \\[\n\\mathbb{N}^+:=\\left\\{1,2,3,\\cdots\\right\\}\n\\] は正の整数全体の集合を表す．詳しくは 本サイトの数学記法一覧 を参照．\n\n2.1 二項分布\n(草野耕一, 2016, p. 92) も参照．\n\n\n\n\n\n\n定義（二項分布，Bernoulli分布）\n\n\n\n\nパラメータ \\(n\\in\\mathbb{N}^+\\) と \\(p\\in[0,1]\\) に関する 二項分布 \\(\\mathrm{Bin}(n,p)\\) とは，集合 \\(\\{0,1,\\cdots,n\\}\\) 上の離散確率分布で， 確率質量関数 \\[\nb(x;n,p):=\\begin{pmatrix}n\\\\x\\end{pmatrix}p^x(1-p)^{n-x},\n\\] \\[\nx=0,1,\\cdots,n,\n\\] が定めるものをいう．\nパラメータ \\(p\\in[0,1]\\) に関する Bernoulli分布 \\(\\mathrm{Ber}(p)\\) とは，\\(n=1\\) の場合の二項分布 \\[\n\\mathrm{Ber}(p):=\\mathrm{Bin}(1,p)\n\\] をいう．\n\n\n\nただし，二項係数 \\(\\begin{pmatrix}n\\\\x\\end{pmatrix}\\) は高校数学では \\({}_nC_x\\) と表す場合が多い．前者の記法の美点は，関係式 \\[\n\\begin{align*}\n    \\begin{pmatrix}n\\\\x\\end{pmatrix}&=\\frac{n!}{x!(n-x)!}\\\\\n    &=\\frac{n}{x}\\frac{(n-1)!}{(x-1)!(n-x)!}\\\\\n    &=\\frac{n}{x}\\begin{pmatrix}n-1\\\\x-1\\end{pmatrix}\n\\end{align*}\n\\] が直感的に表せる点にある．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理5.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理5.html#footnotes",
    "title": "法律家のための統計数理（５）統計的仮説検定入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(大塚淳, 2020, p. 105)，(草野耕一, 2016, p. 100)↩︎"
  },
  {
    "objectID": "posts/2025/Posters/第19回日本統計学会春季集会.html",
    "href": "posts/2025/Posters/第19回日本統計学会春季集会.html",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Posters/RecentDevelopment.html",
    "href": "posts/2024/Posters/RecentDevelopment.html",
    "title": "A Recent Development of Particle Methods",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Posters/RecentDevelopment.html#sec-MLSS2024",
    "href": "posts/2024/Posters/RecentDevelopment.html#sec-MLSS2024",
    "title": "A Recent Development of Particle Methods",
    "section": "MLSS2024",
    "text": "MLSS2024\n\n\n\n\n\n\n\n\nDates\nLocation\n\n\n\n\nMar. 4-15, 2024\nOIST, Okinawa, Japan\n\n\n\n\n\n\n\nGroup Photo at MLSS2024"
  },
  {
    "objectID": "posts/2025/Posters/第19回日本統計学会春季集会.html#導入",
    "href": "posts/2025/Posters/第19回日本統計学会春季集会.html#導入",
    "title": "正確なベイズ変数選択の計算的解決",
    "section": "1 導入",
    "text": "1 導入"
  },
  {
    "objectID": "posts/2025/Posters/BayesComp.html",
    "href": "posts/2025/Posters/BayesComp.html",
    "title": "PDMP’s Advantage in Spiky Distribution Regimes",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/Posters/BayesComp.html#導入",
    "href": "posts/2025/Posters/BayesComp.html#導入",
    "title": "PDMP’s Advantage in Spiky Distribution Regimes",
    "section": "1 導入",
    "text": "1 導入"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Software",
    "text": "Software\nI maintain the Julia package PDMPFlux.jl, and also contribute to the R package YUIMA."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hirofumi Shiba",
    "section": "",
    "text": "Interests\n\nSampling, e.g., MCMC, SMC, diffusion models, etc.\nOptimization, e.g. SGD, evolution strategies, etc.\nModelling, especially in political science & biology\n\nMy primary interest lies in understanding the dynamics of these algorithms, especially their parallels with natural processes.\nI approach these questions through stochastic processes and functional analysis, fields in which I was trained at the University of Tokyo before beginning my Ph.D.\n\n\n\n\nMy surname Shiba (司馬) has different kanji from the Shiba (柴) dog, although they are equally charming.\n\n\n\n\n\nSoftware\nI maintain the Julia package PDMPFlux.jl, and also contribute to the R package YUIMA."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Research Interests",
    "text": "Research Interests\n\nMonte Carlo methods e.g., MCMC, SMC & PDMP\nBayesian Statistical Modelling especially in Political Science & Biostatistics\nBayesian Neural Networks, Nonparametrics & Kernel Methods"
  },
  {
    "objectID": "static/about.html#hello-im-hiro.",
    "href": "static/about.html#hello-im-hiro.",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Hello! I’m Hiro.",
    "text": "Hello! I’m Hiro.\nHirofumi Shiba (Hiro) is a Ph.D. student supervised by Kengo Kamatani at the Institute of Statistical Mathematics (ISM), Tokyo, Japan.\nMy research focuses on deepening our understanding of learning algorithms through the lens of their (scaling / continuous-time) limit dynamics. From these perspectives, algorithms reveal their intrinsic properties, and my work aims to provide a unified understanding of various algorithms under a common mathematical framework, i.e., flows on the space of probability measures."
  },
  {
    "objectID": "static/about.html#education",
    "href": "static/about.html#education",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Education",
    "text": "Education\n\n Ph.D. in Statistical Science, 2028 (expected)\nGraduate University for Advanced Studies, SOKENDAI, Japan\n\n\n B.Sc. in Mathematics, 2023\nUniversity of Tokyo, Japan"
  },
  {
    "objectID": "static/about.html#experience",
    "href": "static/about.html#experience",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Experience",
    "text": "Experience\n\n Cooperative Researcher, 2023.4 – present\nRCAST, the University of Tokyo \n\n\n Data Scientist, 2023.4 – present\nPreMedica Inc., Tokyo, Japan"
  },
  {
    "objectID": "static/about.html#industrial-activities",
    "href": "static/about.html#industrial-activities",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Industrial Activities",
    "text": "Industrial Activities\n\n Research Assistant\nResearch Organization of Information and Systems, Tokyo, Japan\n\n\n Data Scientist, 2023.4 – present\nPreMedica Inc., Tokyo, Japan Consulting in the field of medical data analysis."
  },
  {
    "objectID": "static/about.html#qualifications",
    "href": "static/about.html#qualifications",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Qualifications",
    "text": "Qualifications\n\n Ph.D. in Statistical Science, 2028 (expected)\nGraduate University for Advanced Studies, SOKENDAI, Japan\n\n\n B.Sc. in Mathematics, 2023\nUniversity of Tokyo, Japan"
  },
  {
    "objectID": "posts/2025/Posters/第19回日本統計学会春季集会.html#導入新たなモンテカルロ法の出現",
    "href": "posts/2025/Posters/第19回日本統計学会春季集会.html#導入新たなモンテカルロ法の出現",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる",
    "section": "1 導入：新たなモンテカルロ法の出現",
    "text": "1 導入：新たなモンテカルロ法の出現\nマルコフ連鎖モンテカルロ法 (MCMC: Markov Chain Monte Carlo) や逐次モンテカルロ法 (SMC: Sequential Monte Carlo) などのモンテカルロ法は， 21世紀に入って計算機が普及してのち，ベイズ統計の応用範囲を爆発的に拡大する立役者となった (Gilks et al., 1996), (Doucet et al., 2001), (Martin et al., 2024)． 近年提案された区分確定的マルコフ過程 (PDMP: Piecewise Deterministic Markov Process) を利用した新たなモンテカルロ法も，この系列に続くものと理解できる．\nMCMC と SMC はいずれも離散時間のマルコフ過程 \\((X_n)_{n=1}^\\infty\\) のシミュレーションに基づく． ここでは特にMCMCに焦点を当てる． ベイズ統計では事後分布 \\(\\pi\\) を導くことで推論を実行するが，事前分布や尤度の選択範囲を大きく制限しない限り，\\(\\pi\\) は既知の分布とはならない． たとえ \\(\\pi\\) の関数形がわかっていても，事後分布 \\(\\pi\\) の平均を求めるなど，これを後続の統計解析に供することは必ずしも簡単ではない． そんな中でも，事後分布 \\(\\pi\\) を平衡分布にもつエルゴード的なマルコフ連鎖 \\((X_n)\\) を設計することができれば，大数の強法則 \\[\n\\frac{1}{N}\\sum_{n=1}^Nf(X_n)\\xrightarrow{N\\to\\infty}\\int f(x)\\pi(dx)\\;\\;\\text{a.s.}\\qquad f\\in\\mathcal{L}^1(\\pi)\n\\tag{1}\\] が成り立つ (Kulik, 2018, pp. 175–176)．この事実を用いて事後分布 \\(\\pi\\) を近似する枠組みが MCMC であり， 条件を満たす \\((X_n)\\) の構成法に Gibbs サンプラー (Geman and Geman, 1984) やランダムウォーク Metropolis-Hastings法 (Hastings, 1970) などがある．\n多くの MCMC 法は物理学に由来する．例えば Gibbs サンプラーは Ising モデル (Glauber, 1963)，Metropolis-Hastings 法は状態方程式 (Metropolis et al., 1953) のシミュレーションにおいて最初に提案された． MCMC という名前は (Geman and Geman, 1984) や (Hastings, 1970) などで統計学における確率分布からのサンプリングの問題に応用されて初めて付いたものである．\n連続時間のマルコフ過程である PDMP を用いた新しいモンテカルロ法も，やはり物理学における研究 (Bernard et al., 2009), (Peters and de With, 2012) で最初に使われ始め，初の氷の液相転移の全原子シミュレーションも成功させている (Faulkner et al., 2018)． これに倣い統計でもBouncy Particle Sampler (BPS) (Bouchard-Côté et al., 2018) や Zig-Zag Sampler (Bierkens et al., 2019)，Forward PDMC (Michel et al., 2020) などの汎用アルゴリズムが提案され， この手法群に PDMP に由来する区分確定的モンテカルロ法 (PDMC: Piecewise Deterministic Monte Carlo) の名前がついた．\n\n1.1 ポスターと本稿の構成\n本ポスターでは紙面を３つにわける． 本稿もこれに対応する３つの節を用意し，それぞれを詳説する． 初めに PDMC と MCMC（従来法）の違いを，シミュレートされるマルコフ過程の違いとアルゴリズムの違いの両面からみる（第 2 節）． 続いて Poisson 剪定をはじめとする PDMP のシミュレーションの自動化法を解説し， 筆者が開発したパッケージ PDMPFlux.jl (Shiba, 2025) ではどのようなアプローチを取っているかを解説する（第 3 節）． 最後に MCMC では不可能であったが PDMC で可能になることとして，デルタ部分を持つ非絶対連続分布 \\[\np(dx)=\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\delta_0(dx_i)\\biggl)\n\\tag{2}\\] からのサンプリング法を紹介し，ベイズ変数選択への応用に触れる（第 4 節）．"
  },
  {
    "objectID": "posts/2025/Posters/第19回日本統計学会春季集会.html#sec-1",
    "href": "posts/2025/Posters/第19回日本統計学会春季集会.html#sec-1",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる",
    "section": "2 PDMCとMCMCの違い",
    "text": "2 PDMCとMCMCの違い\n既存の PDMC 法はいずれも， \\[\n\\pi(x)\\,\\propto\\,e^{-U(x)},\\qquad U\\in C^1(\\mathbb{R}^d),\n\\tag{3}\\] という表示を持つ確率分布を対象に扱う．定数の差を除いて負の対数密度 \\(-\\log\\pi\\) にあたる \\(U\\) をポテンシャルとも呼び，PDMC アルゴリズムはその勾配 \\(\\nabla U\\) のみを入力に取る．さらに \\(\\nabla U(x)\\) の計算コストが高い場合は，\\(\\nabla U(x)\\) の不偏推定量を代わりに使うだけでも，Poisson 剪定（命題 \\(\\ref{prop-thinning}\\)）を通じて正確に \\(\\pi\\) に収束する PDMP のシミュレーションが可能である (Bierkens et al., 2019)．MCMC では各イテレーションでデータの一部のみを用いたバッチ実行を，収束先 \\(\\pi\\) を変えずに行うことが難しかったために，PDMC は特に大規模データを用いたベイズ推論への応用が期待されている．\nPDMC と MCMC の最大の違いはアルゴリズムであるが，これはシミュレートしようとしているマルコフ過程の性質の違いに起因する． 例えば最も広く使われている MCMC 法の１つである Metropolis-adjusted Langevin 法 (MALA, Besag, 1994) は， ポテンシャル \\(U\\) が定める Langevin 拡散過程 \\[\ndL_t=-\\nabla U(L_t)\\,dt+\\sqrt{2}\\,dB_t,\n\\tag{4}\\] に基づくが，\\((L_t)\\) は正確なシミュレーションが困難であり，Euler-Maruyama 法などの離散化が必要になる． この段階で数値誤差が入り，このままでは大数の法則 (1) が成り立たなくなる． これを防ぐためには Metropolis-Hastings 様の棄却法を用いる必要がある． こうして，最終的にシミュレートされるマルコフ過程は離散時間過程になり，さらに人工的な対称性が導入され，収束が遅くなる．\nPDMP とは式 (4) のような拡散過程から，拡散項 \\(dB_t\\) を除いた代わりに，ランダムなジャンプ項を加えたものを言う． このクラスのマルコフ過程は，ジャンプの時刻を棄却法によりシミュレートすることで，離散化誤差を伴わない正確なシミュレーションが可能である． その結果人工的な対称性が少なく，一般にその分収束を速くすることができる (Andrieu and Livingstone, 2021)．\n式 (3) で与えられる確率分布 \\(\\pi\\) を平衡分布にもつ PDMP \\((X_t,V_t)\\) は，次の２ステップの繰り返しでシミュレートできる． まず， \\[\nm(t)=\\biggr(v\\cdot\\nabla U(x_0+tv_0)\\biggl),\\qquad i=1,2,\\cdots\n\\tag{5}\\] を強度関数に持つ非一様 Poisson 点過程の最初の到着時刻 \\(T_1\\) をシミュレートし，時刻 \\(T_1\\) まで初期速度 \\(v_0\\) で決定論的に運動する． このイベント発生地点を新しい \\(x_1:=x_0+T_1v_0\\) とし，新たな速度 \\(v_1\\) をサンプリングして同じことを繰り返す． 前述の BPS，Zig-Zag Sampler，Forward PDMC などの既存手法はいずれもこの枠組みの下で設計されており，新たな速度 \\(v_1\\) の決め方のみが違う．\n式 (5) の強度を持つ非一様 Poisson 点過程のシミュレーションでは，Poisson 剪定 (thinning, Lewis and Shedler, 1979) と呼ばれる棄却法が主流であるが，その実行には \\(m\\) の上界 \\(M\\) を構成する必要がある（次の命題参照）．ポテンシャル \\(U\\) はモデルの尤度を含み，大変複雑である．従って \\(\\nabla U\\) が有界になるロジスティックモデルなど，PDMC の応用先は限られると考えられていた．"
  },
  {
    "objectID": "posts/2025/Posters/第19回日本統計学会春季集会.html#sec-2",
    "href": "posts/2025/Posters/第19回日本統計学会春季集会.html#sec-2",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる",
    "section": "3 自動Poisson剪定と PDMPFlux パッケージの紹介",
    "text": "3 自動Poisson剪定と PDMPFlux パッケージの紹介\n\n\n\n\n\n\n命題 (Poisson 剪定)\n\n\n\n\\(m\\le M\\) を関数とする．\\(M\\) を強度にもつ Poisson 点過程の点 \\(T_1,T_2,\\cdots\\) のそれぞれについて，確率 \\(m(T_1)/M(T_1),m(T_2)/M(T_2),\\cdots\\) のみで採択し，他を取り除いて残る点群は，\\(m\\) を強度にもつ Poisson 点過程に従う．\n\n\n\\([0,t_{\\text{max}}]\\) 上の点と \\([t_{\\text{max}},2t_{\\text{max}}]\\) 上の点とは互いに独立に決まるため，\\(t_{\\text{max}}&gt;0\\) はアルゴリズムのハイパーパラメータとし，最初の点 \\(T_1\\) が採択されるまで幅 \\(t_{\\text{max}}\\) の区間上での剪定を繰り返せば良い．(Andral and Kamatani, 2024) にて，一般の関数 \\(m\\) に対して，その \\([0,t_{\\text{max}}]\\) 上での上界を自動的に求めながら剪定を実行する方法が提案された．その方法とは次の通りである．まずグリッドの数 \\(n\\) を固定し，\\([0,t_{\\text{max}}]\\) の \\(n\\) 等分点 \\(\\{0=t_0&lt;t_1&lt;\\cdots&lt;t_n=t_{\\text{max}}\\}\\) を考え，各点 \\(t_i\\) 上での接線を自動微分 (Baydin et al., 2017) を用いて計算する．続いてそれらの接線同士の交点を求め，その \\(y\\) 座標と両端点 \\(m(t_i),m(t_{i+1})\\) の３点の最大値を，小区間 \\([t_i,t_{i+1}]\\) 上での \\(m\\) の上界とする．\nこうして得られた \\([0,t_{\\text{max}}]\\) 上の区分定数関数 \\(M\\) は，\\(t_{\\text{max}}\\to0\\) または \\(n\\to\\infty\\) の極限で \\(m\\) の上界を与えるが，\\(m\\) が \\([0,t_{\\text{max}}]\\) 上で激しく変化する場合，\\(m\\) の上界になっていない場合がある．そこで (Andral and Kamatani, 2024) は，Poisson 剪定の最中に評価される比 \\(m(t)/M(t)&gt;1\\) が１を超えた場合，\\(t_{\\text{max}}\\) の値を縮めて \\([0,t_{\\text{max}}]\\) 上での剪定をやり直す．Andral は Python パッケージ pdmp_jax にこのアルゴリズムを実装し，BPS, Zig-Zag Sampler, Forward PDMC を含む５つのサンプラーを利用可能にした．\nしかし，この方法は \\(M(t)&lt;m(t)\\) が発生してしまっている点 \\(t\\) を検出できずに，\\(t&lt;T_1\\) を満たす到着時刻 \\(T_1\\) を提案してしまった場合に，シミュレートしている PDMP に漸近的に消えないバイアスを導入してしまう．従って \\(U\\) の性質に対してグリッド数 \\(n\\) を小さく，または \\(t_{\\text{max}}\\) を大きく取りすぎた場合，\\(M\\) の誤りを検出できず，使用者が知らないうちに大数の法則 (1) が成り立たなくなってしまっている可能性がある．\n筆者の開発したパッケージ PDMPFlux (Shiba, 2025) ではこの問題の解決を図った．\\(M(t)&lt;m(t)\\) が小区間 \\(t\\in[t_i,t_{i+1}]\\) で発生するためには，\\(m\\) が \\([t_i,t_{i+1}]\\) 上で１つ以上の変曲点を持つ必要があることに注目する．そこでグリッド \\(\\{t_i\\}_{i=0}^N\\) 上で接線だけでなく２階微分係数も計算し，\\(m\\) が \\([0,t_{\\text{max}}]\\) 上で高々１つしか変曲点を持たないように \\(t_{\\text{max}}\\) を調整することを提案した．このことにより，\\(U\\) の Lipscthiz 係数に関する一定の仮定の下で，\\(t_{\\text{max}}\\to0,N\\to\\infty\\) の極限を考えずとも，\\(m&lt;M\\) の成立を保証できるようになる．\nこうしてパッケージ PDMPFlux はポテンシャル \\(U\\) を与えるだけで \\(\\pi\\) からの正確な PDMP サンプリングを実行することのできるほとんど唯一のパッケージとなった．加えて，第 4 節で後述する Sticky Zig-Zag を加えた６つのアルゴリズムが利用可能である．MCMC ベースのベイズ推論エンジンである Stan に替わる，新たなバックエンドとしての利用も可能である．"
  },
  {
    "objectID": "posts/2025/Posters/第19回日本統計学会春季集会.html#sec-3",
    "href": "posts/2025/Posters/第19回日本統計学会春季集会.html#sec-3",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる",
    "section": "4 スパイク付きの非絶対連続分布からのサンプリング",
    "text": "4 スパイク付きの非絶対連続分布からのサンプリング\nMCMC では難しく，PDMC で真に新たに可能になったこととして，式 (2) で与えられるような，\\(\\delta\\)-部分を持った非絶対連続確率分布 \\(p\\) からのサンプリングが挙げられる．この事実は次のようにも理解できる．式 (2) で与えられるような \\(\\delta_0\\)-部分を持った分布 \\(p\\) は，\\(\\delta_0\\)-部分を分散が小さい正規分布 \\(\\operatorname{N}(dx_i;0,\\epsilon^2)\\) に近似した \\[\np_\\epsilon(dx)=\\prod_{i=1}^d\\biggr(\\omega_ip_i(x_i)\\,dx_i+(1-\\omega_i)\\operatorname{N}(dx_i;0,\\epsilon^2)\\biggl)\n\\tag{6}\\] という絶対連続分布の，分散が小さくなる極限 \\(\\epsilon\\to0\\) と理解できる．多くの標準的な MCMC は \\(p_\\epsilon\\) からのサンプリングは可能でも，\\(\\epsilon\\to0\\) の極限で誤った分布 \\(\\otimes_{i=1}^dp_i\\ne p\\) に収束するマルコフ過程に収束してしまう（未発表原稿）．さらに \\(\\epsilon&gt;0\\) が小さいほど，長時間の相関が強くなり，非現実的なほど長い時間アルゴリズムを実行しない限り，大きなバイアスを持ってしまう．\n一方で標準的な PDMP は \\(\\epsilon\\to0\\) の極限でもまた別の PDMP になり，これは正確に \\(p\\) に収束する．この極限過程を直接シミュレーションすることで，\\(p\\) からのサンプリングを効率的に実現する手法 Sticky PDMP が (Bierkens et al., 2023) により提案された．\nSticky PDMP はベイズ変数選択に大きな応用を持つ．(Mitchell and Beauchamp, 1988) で提案された spike-and-slab 事前分布はまさに式 (2) で与えられる形をしているが，従来の MCMC では直接の事後分布サンプリングが難しかったため，モデルの空間 \\(\\{0,1\\}^d\\) 上で動く Gibbs サンプラーと組み合わせてサンプリングをしたり (George and McCulloch, 1993)，連続近似 (6) を代わりに用いたりされることが多かった．しかし，連続近似をベイズ変数選択に用いた場合は，説明変数 \\(x_i\\) がモデルに含まれる事後確率 (PIP: Posterior Inclusion Probability) を正確に計算するために追加の処理が必要になる (Hahn and Carvalho, 2015)．\n筆者は Sticky PDMP のダイナミクスを分析すると同時に，Sticky Zig-Zag アルゴリズムを PDMPFlux に実装することで，\\(\\delta\\)-部分を持った非絶対連続分布 \\(p\\) からのサンプリングが可能であることの，ベイズ変数選択以外への応用も模索している．例えば，特定の密度推定の問題においてミニマックス最適なベイズ事後密度推定量を与える事前分布は離散分布になる (Gangopadhyay and Mukherjee, 2021) ように，非絶対連続事前分布を用いることで正確に計算可能なベイズ推定量の幅が広がる可能性がある．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html",
    "href": "posts/2023/Surveys/BayesianComp.html",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "",
    "text": "History of Bayesian Computation (Martin+2023-history?)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズまでの統計学の黎明",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズまでの統計学の黎明",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.1 ベイズまでの統計学の黎明",
    "text": "1.1 ベイズまでの統計学の黎明\n統計学の黎明を要請したものは，社会への不安であった．筆者に言わせれば，この社会への不安を直視したのがドイツ，数で解決しようとしたのがイギリスで，解決への筋道を確率論で基礎づけたのがフランスである．\n\nプロシヤにおける国勢学，イギリスにおける政治算術，フランスにおける古典確率論–統計学はこれら３つの異った厳選を持つと言われる (増山元三郎, 1950, p. 6)．\n\n17世紀初頭から何度も流行を繰り返し，遂に1665年にはロンドンの人口の1/4を死に至らしめた ペストの大流行 は恐怖の対象であった．パンデミックは現代でも恐怖の対象であるが，当時はその全貌の把握が難しく，これが第一に切望された．月の運行による健康被害，国王の統治が疫病を引き起こす，などの俗見が流布していた時代である．しかし，「数」という解決手段は極めて功を奏した．\n数による解決が他でもないイギリスから生まれたことは，Francis Bacon 1561-1626 と Thomas Hobbes 1588-1679 に象徴される自然科学の風土，「Aristotelesの三段論法を通じて，経験的に因果関係を発見することで，我々は自然を理解できる」という希望が当時のイギリスには存在したことが挙げられる．\n\n海へ行け，きっと獲物があるぞという先輩が Bacon であった．漁獲法一般の講義をする先生が，例えば後世の J. S. Mill の帰納論理学に相当するのである．統計学を作った漁師たちは，Mill 先生の帰納法の論理学の講義などは，上の空で聞いた．そして各自の漁獲法を自らの浜で覚えたのである． (北川敏男, 1949, p. 12)\n\n\n新大陸の発見，東洋への海路の開拓により，世界商業の中心は砂漠の商隊と地中海の商人とを介して栄えていたイタリアから，スペイン・ポルトガル・オランダを経て16世紀の終り頃には，すでにイギリスに移っていたのである．ここに興隆の一路を辿る市民社会・殷盛を極める海上貿易・繁栄する英都の商業・封建制の崩壊を示す Cromwell 革命 (1649) 後のイギリス社会に，市民科学としての政治算術が起ったことは敢えて異とするに足りないであろう (増山元三郎, 1950, p. 10)．\n\n\n1.1.1 John Grauntの死亡表\nペスト流行の激しさの判定に寄与する人口状況を，最初に数によって理解しようとしたのが John Graunt 1620-1674 であった．\n当時の英国王立理学協会1 は，封建的な諸関係の崩壊解消と同時に，商品生産・貨幣による売買の全面支配によって貨幣的表現が富の大部分に侵入したことにより新たに誕生した市民階級が勢力を占めており，Graunt もこのような商人階級の出身であった．\nそのような身分の Graunt が英国王の推薦を受けて王立協会員の名誉を勝ち取った論文 (Graunt, 1662) は，ギルド発行の死亡統計 Bills of Mortality と教会に蓄積していた統計資料2 から統計的な処理を通じて世界初の「死亡表」を作成し，次の内容を初めて結論づけた．\n\n36%の幼児は６歳未満で死亡する．\n洗礼数をみると，男女比は16:15くらいである．\n都市の死亡率は地方より高い．\nLondonの城外では死亡率は３倍である．\n\n加えてLondonの世帯数を3通りの方法で推算し，世帯数は5万であろうと結論づけた．なお，当時の俗見ではLondon人口は100万と言われていた．\nその後このような「生命表」は精緻化の一途を辿り，イギリスのギルド的な共助制度の土壌の上で，生命保険の成立という実を結んだ．\n\n\n1.1.2 統計学への期待と希望\nこのイギリスの数を使った解決は，政治算術学派 と呼ばれ，海外への輸出が進んだ．\nドイツの牧師 Johann Peter Süβmilch 1707-1767 は Graunt に倣って，教会に蓄積していた統計資料を用い，出生率の性別比が長期的には女性1,000対男性1,050に収束することを発見した．\n中でも特に，「たくさんのデータを集めると何かが見えてくる」ことに大きな希望を持ち，Graunt が教会の資料に注目したことを Columbus の新大陸発見になぞらえている．そう，歴史上最初の統計分析は，教会の資料によるものであったのである．\n\n若し我々が家を一軒一軒数えていくならば，ある家では娘だけに，またある家では息子だけに，あるいはそうでなくとも，非常に不釣り合いな両者の配合にでくわすであろう．小さな社会や村落でも秩序的なものを認めることは，容易ではない．（中略）．かかる場合に，誰が，能く規則と秩序とに想達し得るだろう．所で，教会の記録はこの秩序の確認のための大きな手段である．それは教会用及び世俗用のためにすでに数世紀前から取られ，とくに宗教改革後はかなり正確にとられてきた．誰がそれを利用したか？その発見はアメリカ発見と同時に可能であったのだ．（中略）それをGrauntがなし得たのである． –Süβmilch (1741) 『神の秩序』 (Göttliche Ordnung) 訳文は (北川敏男, 1949) より．\n\nこのように Süβmilch は男児の出生率の方が高いことを神の存在証明と見なしたのであった．この宗教的な外被を取り去るには，確率論の登場をまたねばならなかったが，これにはさらにフランスの学派が合流するのを待つ必要があり，それには100年を要したのであった（ Section 1.7 も参照）．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズが取り組んだ問題",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズが取り組んだ問題",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.2 ベイズが取り組んだ問題",
    "text": "1.2 ベイズが取り組んだ問題\nというわけで，\\(i=1,\\cdots,n\\) 番目の世帯の新生児が，男児である \\(y_i=1\\) か女児である \\(y_i=0\\) かのデータなどから，人口・疫病・国家動態に役立つ知識を引き出すことが当時の重要な問題意識であることをわかっていただけただろう．\nイギリスの牧師 Thomas Bayes 1701-1761 は，より抽象的な設定で統計的推定の問題を研究していた．Bayes は就中，次のような区間推定の問題を考えていた．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\nベイズが取り組んだ問題（現代語訳）\n\n\n\n2値のデータ \\(Y_i\\in\\{0,1\\}\\) は，ある未知の「成功率」 \\(\\theta\\in(0,1)\\) に従って， \\[\nY_i=\\begin{cases}\n1&\\text{確率 }\\theta\\text{ で}\\\\\n0&\\text{残りの確率} 1-\\theta\\text{ で}\n\\end{cases}\n\\] という値を取るとする．3 このようなデータの独立観測標本 \\(\\boldsymbol{y}:=(y_1,\\cdots,y_n)^\\top\\) から．神のみぞ知る，このデータ \\(\\boldsymbol{y}\\) を生み出した真の成功率 \\(\\theta\\) が，区間 \\((a,b)\\subset(0,1)\\) に入っているという確率 \\(\\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]\\) をどう見積もれば良いか？\n\n\nここでは引き続き \\(Y_i\\) は性別で，\\(\\theta\\) は男児が生まれる確率 \\(\\theta=\\operatorname{P}[Y_i=1]\\) だと解釈する．Bayes 自身は「ある未知の位置に白線が引かれたテーブル上にボールを \\(n\\) 個転がし，それぞれの領域に幾つのボールが入ったかの情報のみから，白線の位置を推定する」という表現によって問題を定式化した (Bayes, 1763)．これは後世ではビリヤード台の問題とも呼ばれた．こちらのサイトも参照．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズのアイデア",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズのアイデア",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.3 ベイズのアイデア",
    "text": "1.3 ベイズのアイデア\n彼の発想は極めてシンプルであり，次の3段階によって推定を試みた：\n\n事前分布 \\(p(\\theta)\\) と呼ばれる，最初の \\(\\theta\\in(0,1)\\) に対する予想 \\(p(\\theta)\\) を自由に表現する．4\n事前分布のデータ \\(\\boldsymbol{y}\\) を観測した下での条件付き分布 \\[p(\\theta|\\boldsymbol{y}):=\\frac{p(\\theta,\\boldsymbol{y})}{p(\\boldsymbol{y})}\\] を計算する．これを事後分布という．\nこの事後分布 \\(p(\\theta|\\boldsymbol{y})\\) の形から区間推定を実行する．\n\nこの 3.の部分は，Bayes が特に区間推定に拘ったためのものであり，点推定でも良ければ次期予測でも良い．推定対象 3.を目的に応じて自由に入れ替えても，1.と 2.の部分が同じように動作するということ，これがベイズ統計学の枠組みである．\nそれだけに事後分布というものが表現力に富んでいるのである．また，以下の例で納得していただけるかもしれないが，ベイズ統計学の手続きは「眼前のデータは，事前の信念を変えるのにどれほど説得的であるか？」という観点からも見れ，定量的であると同時に定性的な判断も可能にする． Section 3.6 で紹介するように，この特徴は意思決定への応用おいても重要である．\n\n\n\n\n\n\n問題に対する (Bayes, 1763) の解決\n\n\n\n\nまず，事前分布 \\(p(\\theta)\\) を設定する．Bayes は前述のビリヤードの問題を考えていたこともあり， 「\\(\\theta\\in(0,1)\\) は全く予想がつかない」「どんな \\(\\theta\\) も同様にあり得る」という立場を取った．横軸を \\(\\theta\\in(0,1)\\) の値，縦軸を「主観的にあり得ると思う度合い」として図で表すと次の通りである：\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the range for x-axis\nx = np.linspace(0, 1, 1000)\n\n# Uniform distribution density function is constant\ny = np.ones_like(x)\n\n# Plot the graph\nplt.figure(figsize=(3, 2)) # Size suitable for a smartphone screen\nplt.plot(x, y, label='Uniform Distribution (0,1)', color=(0.35, 0.71, 0.73, 1))\nplt.fill_between(x, y, color=(0.35, 0.71, 0.73, 0.3))\nplt.xlabel('x')\nplt.ylim(0, 1.5)\nplt.ylabel('Density')\nplt.title('Posterior Distribution')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nこの図が表す \\((0,1)\\) 上の確率分布を一様分布という．このように，一様分布とは「どのような \\(\\theta\\) の値も同様に確からしい」という予想の表現である．\n\n次に，データ \\(\\boldsymbol{y}=(y_1,\\cdots,y_n)^\\top\\) が観測された後の条件付き分布 \\(p(\\theta|\\boldsymbol{y})\\) を計算することで，本データ \\(\\boldsymbol{y}\\) が事前の信念 \\(p(\\theta)\\) をどのように変えてしまうかを観る．簡単な確率論の結果として，条件付き分布は次の公式によって計算できる（講義ノートも参照）：5 \\[\np(\\theta|\\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|\\theta)p(\\theta)}{\\int_\\Theta p(\\boldsymbol{y}|\\theta)p(\\theta)\\,d\\theta}\n\\tag{1}\\]\n例えば 日本の2021年の出生児性別のデータ を用いると次のようになる．\n\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# パラメータの設定\nn = 811622\nmale = 415903\nfemale = n - male\n\n# ベータ分布のPDFを計算\nx = np.linspace(0, 1, 1000)\ny = beta.pdf(x, 1+male, 1+female)\n\n# プロット\nplt.figure(figsize=(3, 2))\nplt.plot(x, y, label=f'Beta({1+male}, {1+female})', color=(0.35, 0.71, 0.73, 1))\nplt.fill_between(x, y, color=(0.35, 0.71, 0.73, 0.3))\nplt.xlabel('p')\nplt.xlim(0.4, 0.6)\nplt.ylabel('Probability Density')\nplt.title('Bayesian Posterior Distribution')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nこうして極めて鋭い事後分布が出来た．事前に設定した分布 \\(p(\\theta)\\) は極めて平坦な一様分布であったのに，それをデータで条件付けた \\(p(\\theta|\\boldsymbol{y})\\) には極めて鋭いスパイクが現れたのである．Equation 1 を認めるならば，この図は「男児の方が女児よりも生まれる確率が高い」ことの証拠として，極めて説得的ではないだろうか？\n\nでは区間推定の例として，\\((a,b)=(0.5,1.0)\\) として，「男児の方が女児よりも多い確率」を推定しよう．これは次を計算することになる： \\[\n\\begin{align*}\n&\\operatorname{P}\\left[\\frac{1}{2}&lt;\\theta&lt;1\\right]\\\\\n&=\\int^1_{\\frac{1}{2}}p(\\boldsymbol{y}|\\theta)\\,d\\theta.\n\\end{align*}\n\\]\n\n\n\nCode\nprint(sum(y[500:600])/1000)\n\n\n1.0030977682960893\n\n\nもはや丸め誤差により \\(1\\) を越してしまっている．ほとんど確実に「男児の方が生まれる確率が高い」と結論づけて良いだろう．\n\n\nこの (Bayes, 1763) が実行したように，事後分布 \\(p(\\theta|\\boldsymbol{y})\\) をみて \\(\\theta\\) に関する推論をする，という立場からの統計的営み全体をベイズ統計学．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-fundamental-problem-of-Bayes",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-fundamental-problem-of-Bayes",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.4 ベイズ統計学の基本問題",
    "text": "1.4 ベイズ統計学の基本問題\n事後分布 \\(p(\\theta|\\boldsymbol{y})\\) を導く際に用いた条件付き確率の公式である Equation 1 \\[\np(\\theta|\\boldsymbol{y})=\\frac{p(\\boldsymbol{y}|\\theta)p(\\theta)}{\\int_\\Theta p(\\boldsymbol{y}|\\theta)p(\\theta)\\,d\\theta}\\quad\\text{(1)}\n\\] は Bayesの公式 と呼ばれるようになった．今回の場合では，Pythonコードをご覧になった方はわかったかもしれないが，事後分布は \\[\np(\\theta|\\boldsymbol{y})=\\frac{\\theta^m(1-\\theta)^{n-m}}{B(m+1,n-m+1)}\n\\] となり，これはパラメータの空間 \\((0,1)\\) 上の Beta分布 と呼ばれるものである．\n現代のベイズ統計学の多くの統計量は，ある可積分関数 \\(g:\\Theta\\to\\mathcal{X}\\) を用いて \\[\n\\operatorname{E}[g(\\theta)|\\boldsymbol{y}]=\\int_{\\Theta}g(\\theta)p(\\theta|\\boldsymbol{y})\\,d\\theta\n\\tag{2}\\] と表される．先ほどの Bayes の区間推定の例では \\(g=1_{(a,b)}\\) と取った場合に当たる．6 実は，この積分は，この最も簡単と思われる \\(p(\\theta),p(\\boldsymbol{y}|\\theta)\\) の設定でも，殆ど計算できないのである．\n鮮やかな解決法を提示したかと思えば，結局実行出来ないのでは全く本末転倒である！そのこともあってか，論文 (Bayes, 1763) は実は Bayes の死後に Richard Price によって投稿されたものであり，生前に自ら投稿・発表した訳ではなかった．7 当然，発表当時は全く注目を受けなかった (Stigler, 1990)．\n\nHence, despite the analytical availability of \\(p(\\theta|\\boldsymbol{y})\\) via (2)–“Bayes’ rule” as it is now known-—the quantity that was of interest to Bayes needed to be estimated, or computed. The quest for a computational solution to a Bayesian problem was thus born. (Martin+2023-history?)\n\n\n\n\n\n\n\nまとめ：ベイズ統計学の基本問題\n\n\n\nベイズの提示した統一的な統計推測の枠組み\n\n推定したい値 \\(\\theta\\) の空間上に事前分布 \\(p(\\theta)\\) を設定する．\n事前分布 \\(p(\\theta)\\) のデータ \\(\\boldsymbol{y}\\) に関する条件付き分布として事後分布 \\(p(\\theta|\\boldsymbol{y})\\) を得る．\n\nは非常に自然で，特に確率分布 \\(p(\\theta),p(\\theta|\\boldsymbol{y})\\) を簡単に視覚化できる現代では「データ \\(\\boldsymbol{y}\\) は，パラメータ \\(\\theta\\) に対する事前の信念をどれほど変えるに値するか？」を定量的にも定性的にも実感出来るという美点がある．\nしかしながら，モデル \\(p(\\theta),p(\\boldsymbol{y}|\\theta)\\) の設定をいくら簡単にしても根本的に計算が困難で実行不可能なのである．これを解決する分野をベイズ計算という．ベイズの論文 (Bayes, 1763) でも，計算法の開発が約半分を占めた．8 このように，Bayes統計学は当初からBayes計算の問題を懐胎していたのである．\n\nIn short, the implementation of all forms of Bayesian analysis relies heavily on numerical computation. (Martin+2023-history?)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-Laplace",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-Laplace",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.5 Laplaceの近似",
    "text": "1.5 Laplaceの近似\nフランスの数学者 Laplace は25歳時の初めての統計に関する著作 (Laplace, 1774) を発表した．この中で，Bayes が解こうとしたものと全く同じ\n\\[\\begin{align*}\n    &\\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]\\\\\n    &\\quad=\\frac{\\int^b_a\\theta^m(1-\\theta)^{n-m}\\,d\\theta}{B(m+1,n-m+1)}\n\\end{align*} \\tag{3}\\]\nという積分計算の問題を，被積分関数を \\[\nf(\\theta):=\\frac{\\log p(\\theta|\\boldsymbol{y})}{n}\n\\] を用いて指数関数の形に表すことで解いた：9 \\[\n\\begin{align*}\n    \\operatorname{P}[a&lt;\\theta&lt;b|\\boldsymbol{y}]&=\\int^b_ap(\\theta|\\boldsymbol{y})\\,d\\theta\\\\\n    &=\\int^b_ae^{nf(\\theta)}\\,d\\theta\n\\end{align*}\n\\] この形に変形することがどのように役立つかは，次の定理が説明してくれる：\n\n\n\n\n\n\n定理（Laplace近似）\n\n\n\n10 関数 \\(f:[a,b]\\to\\mathbb{R}\\) はただ一つの最大値を \\(x_0\\in(a,b)\\) で取り，\\(f''(x_0)&gt;0\\) を満たすとする．このとき \\(n\\to\\infty\\) の極限について， \\[\n\\int^b_ae^{nf(x)}\\,dx\\sim\\sqrt{-\\frac{2\\pi}{nf''(x_0)}}e^{nf(x_0)}\n\\]\n\n\nこれを \\(f\\) の二次近似について適用することで，あらゆる確率分布 \\(p(\\theta|\\boldsymbol{y})\\,d\\theta\\) に関する積分 Equation 3 を，その正規近似に関する積分で近似できるのである．\nこの手法は現在のBayes計算手法のアイデアの源泉であり続けている (Rue et al., 2009), (MacKay, 2003)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズ統計学の長く苦しい時代",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズ統計学の長く苦しい時代",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.6 ベイズ統計学の長く苦しい時代",
    "text": "1.6 ベイズ統計学の長く苦しい時代\n「ベイズの枠組みは理念的に好ましかろうと，実際には実行不可能である」というベイズ統計学の基本問題は，Laplace が普遍的な近似計算法を開発したこと（ Section 1.5 ）を除いて，次の進展を見るには計算機の発明と普及を待つ必要があった．その間実に2世紀超えである．\nまた，Laplace の近似手法は普遍的であり，Bayes の最初の設定のような簡単な設定の \\(p(\\theta|\\boldsymbol{y}),g\\) に限らずとも使えるという，ベイズ統計学に大きく資する特徴も備えていたが，パラメータ \\(\\theta\\in(0,1)\\) の次元が1ではなくなると途端に使えなくなるという欠点がある．\n\n（前略）ベイズ統計学の有用性は以前から理解されていたが，この問題の抜本的な解決は1980年代まで待たざるを得なかった．それ以前は，ベイズの定理自体は18世紀に早々に発見されたにもかかわらず，長い間，確率の解釈，事前分布の設定，事後分布の計算の困難さのために哲学的議論に終始し，実用化にはほど遠かったのである．実用化の扉の鍵となったのは，一つは計算機の急速な発達，もう一つは計算集約的な画期的アルゴリズムの提案である． (樋口知之, 2014, p. 17)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-France",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-France",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.7 フランスでの確率論の歴史",
    "text": "1.7 フランスでの確率論の歴史\nこのように，Laplace が，ベイズ統計学暗黒の時代の中で唯一の小さな前進を生んだ．それだけでなく，Laplace は後の1812年に当時の確率論最大の集大成と言える大著『確率論の解析理論』を産んでおり，これが他でもないフランスから生まれたことにも相応の理由があった．\nまず第一に，賭博の流行により，確率というものの理解と征服が嘱望された．\n\nその（確率論の）発展の動きを与えたものは，交易を賭ける商業資本家が占星術よりも確実な指導をこの学術に求めるという様な社会が基盤となって存在したことである．例えば，17世紀中葉のPascalとFermatの間の往復文書に取り扱われたカード遊びの数学的問題が，広く人々の関心を呼び起こした事情の裏には，至富の途を確実に求める商人たちの渇望が学問が外の世界にあったことを忘れてはならない． (北川敏男, 1949)\n\n第二に，統計的現象を神学的な畏怖の対象と見るのではなく，自然科学による自然の理解と征服の文脈の最先端として理解する土壌がフランスにあったことが指摘できる．\n\n17, 18世紀の啓蒙的合理主義は，偶然的な事象に対しても数学的な取り扱いを行うことに特別の興味を持った．思想的にはこの時代精神こと確率論を発展させた最大の動力であった．その駆使する数学解析の多彩と合理主義の徹底とに於て，Laplace の大著はよくこの時代を代表するものと言うべきであろう．\n\n当時の財務総監 Jacques Turgot を通じてパリ造幣局の監査官も務めた Nicolas de Condorcet 1743-94 は Laplace の確率論を積極的に社会分析に応用した．「社会数学」と呼んだこの運動は社会学の源流ともみなされる． 当然後進も Laplace に続いた．ベルギーの数学者 Adolphe Quetelet 1796-74 は Laplace の確率論を社会に応用することを目指し「社会物理学」なる分野を創始し，BMIの別名「ケトレー指数」にも名を残している．11\n\n古典確率論の一応の完成は典雅に見えるであろう．だが人は，確率論のもった政治的，社会的意義を忘れてはならない．理知を一切の尺度として「代数学の炉火によって倫理学及び政治学を照さん」(Condorcet) という時代精神，神の啓示に代らんとする確率論，それはフランス革命の思想的基礎に連関することを見失ってはならないのである． (北川敏男, 1949)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#quetelet-による綜合",
    "href": "posts/2023/Surveys/BayesianComp.html#quetelet-による綜合",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "1.8 Quetelet による綜合",
    "text": "1.8 Quetelet による綜合\nQuetelet は応用統計学の祖とも呼ばれる．\n\n［政治算術学派から人口理論 (Malthus, 1798) をはじめとする数多くの統計的法則の発見など，多くの成果があったにも拘らず，］18世紀の後半は理論的成果の観点からは全く空白の時代であった．吾々はその原因を方法論が進歩しなかった点に見出しうる．先験的対数法則を中枢とする古典確率論の方法がこれと合流するに至るまで，統計学の理論的分野は足踏みを続けざるを得なかったのである．後述 Quetelet の手による，この合流の着手は，応に近代的意味における統計学の発足を示すものであるとともに，記述統計学の定礎を意味するものでなければならなかった．(増山元三郎, 1950, p. 12)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-MCMC",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-MCMC",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.1 マルコフ連鎖によるモンテカルロ法の発明",
    "text": "2.1 マルコフ連鎖によるモンテカルロ法の発明\n乱数のシミュレーションを用いた確率的なアルゴリズムをモンテカルロ法と総称する．これは Metropolis が同僚 Ulam のポーカー好きから，モナコの首都 Monte Carlo にちなんで名付けたものである (Nicholas Metropolis and Ulam, 1949)．このようなアルゴリズムが最初に生まれたのが，第二次世界大戦中の Los Alamos研究所 で進行中だった原爆開発計画である Manhattan計画 においてである．\n当時の問題は，原子爆弾着火時における Schödinger 作用素の基底状態のエネルギーを計算することにあった．抽象的には，\\(p\\) を \\(N\\) 個の粒子が従う Boltzmann 分布として，積分 Equation 2 を計算することにあった：\n\\[\n\\operatorname{E}[g(\\boldsymbol{\\theta})]=\\int_\\Theta g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\\quad\\text{(2)}\n\\]\nただし，\n\n積分領域 \\(\\Theta\\) が \\(2N\\) 次元というとてつもない高次元空間上であること\n分布 \\(p\\) は定数倍を除いてしか計算できない\n\nという，2つの大きな制約があった．1.のために通常の数値積分法が使えず，また 2.により \\(p\\) からの直接の乱数シミュレーションが出来ないので，\\(p\\) からの乱数 \\(X_1,\\cdots,X_M\\) を十分多く生成することで積分 Equation 2 を \\[\n\\frac{1}{M}\\sum_{i=1}^Mg(X_i)\n\\] によって近似するという通常の Monte Carlo 積分法を実行することも出来ない．そこで，Metropolis ら当時の Los Alamos に集まった物理学者たちは新しい方法を考える必要があった．\n最終的な解決 (N. Metropolis et al., 1953) は，Monte Carlo 法の中でもとりわけ画期的な発想によるものであった．それは，Markov 連鎖を用いるということである．Markov連鎖とは（ある一定の条件を満たす）確率過程のクラスであり，\\(p\\) から直接のシミュレーションが出来ない状況でも，\\(p\\) に収束するようなMarkov 連鎖を構成することは可能だったのである．\n制約 1.と 2.は広く物理学とベイズ統計学の至る所で見られる障壁であり，これをものともしない汎用アルゴリズムの発明は極めて大きなブレイクスルーであった．(Dongarra and Sullivan, 2000) は Metropolis アルゴリズムを理学・工学分野に20世紀最大の影響を与えたアルゴリズムの1つとしている．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-importance-sampling",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-importance-sampling",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.2 重点サンプリング法の発明",
    "text": "2.2 重点サンプリング法の発明\n実は Manhattan 計画に最中に，もう一つのサンプリング技法が生まれていた．厚い壁で中性子線とガンマ線がどのように吸収されるかに取り組んでいたグループにて，Herman Kahn らが中心となり，Equation 2 の分布 \\(p\\) に関する積分が \\[\n\\begin{align*}\n    \\operatorname{E}[g(\\boldsymbol{\\theta})]&=\\int_\\Theta g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\\\\\n    &=\\int_\\Theta\\frac{g(\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p^*(\\boldsymbol{\\theta})}p^*(\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta}\n\\end{align*}\n\\] という式変形により，別の分布 \\(p^*\\) からのサンプリングを通じて計算できる，という技法が利用された．彼らはこれに重点サンプリング法という名前をつけた．これは Gerald Goertzel による命名である可能性が高い (Andral, 2022)．\nなお，当時は \\(p\\) からのサンプリングを回避できるという点よりも，\\(p^*\\) をうまく選ぶことにより元々の \\(p\\) を用いた Monte Carlo 積分法を適用するよりも近似の精度をあげることが出来るという点の方が注目された (Hammersley and Handscomb, 1964)．\n前節の Metropolis 法がMCMCの先駆けであるとしたら，この2つの美点を持った重点サンプリング法は，SMC（粒子フィルター） の先駆けであった．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#mcmcの普及とギブスサンプラー",
    "href": "posts/2023/Surveys/BayesianComp.html#mcmcの普及とギブスサンプラー",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "2.3 MCMCの普及とギブスサンプラー",
    "text": "2.3 MCMCの普及とギブスサンプラー\nMetropolis 法の発明から，すぐにMCMCの画期性が広く認識された訳ではなかった．特に，元々物理学の文脈で発明されたこともあり，統計学の文脈への応用が始まるには (Hastings, 1970) の仕事を待つ必要があった．\nしかし1970年代とはマイクロプロセッサが開発されたばかりの時代であり，12 MCMCが実際の統計解析の現場で採用可能な計算手法になるとは（そもそも現代のように小型なコンピュータを個人が所有するようになるとは）夢にも思われなかった時代であったが，ここからたったの20年で現代人の生活とベイズ統計学は大きく変わることになる．\n\n各人が安価に高性能なコンピュータを所有するようになった．\n高次元分布からのサンプリングを可能にするアルゴリズムが発見された．\n\nの2点が最後に加わることで，MCMCがベイズ計算法不動の金科玉条となった．\nこの 2.は計算機の性能の問題だけでなく，統計的画像処理 の分野から Gibbsサンプラー という新たなアルゴリズムが生まれた (Geman and Geman, 1984) ことによって実現された．13 これは，パラメータが \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)^\\top\\) と表されるとき，適切に定めた初期値 \\(\\theta_2^{(0)}\\) から初めて，条件付き分布からのサンプリング \\[\n\\theta_1^{(i)}\\sim p_1(\\theta_1^{(i)}|\\theta_2^{(i-1)},\\boldsymbol{y}),\n\\] \\[\n\\theta_2^{(i)}\\sim p_2(\\theta_2^{(i)}|\\theta_1^{(i)},\\boldsymbol{y}),\n\\] を繰り返すことで，最終的に \\(\\boldsymbol{\\theta}^{(i)}:=(\\theta_1^{(i)},\\theta_2^{(i)})^\\top\\) は全体として \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) に従うように収束する，という技法である．\nGibbs 法により，パラメータ \\(\\boldsymbol{\\theta}\\) の次元が大きく，直接のサンプリングが難しい場合や，条件付き分布の系はわかっているが結合分布がわからない場合14 でも，\\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,\\cdots)\\) というように低次元変数の結合と理解することで，あるいは補助変数を追加してわざと問題を高次元化してでもそのような状況をうまく作り出すことで (Tanner and Wong, 1987) ，部分的な低次元サンプリングから組み上げることが出来るようになった．これを データ拡張 ともいう．さらにその後も，このアイデアが (Roberts and Rosenthal, 1999) のスライスサンプラーにつながっている．\nこの点をはっきり強調して示し，ベイズ統計学がすでに実行可能なものになっており，ベイズ統計学の基本問題（ Section 1.4 ）もすでに過去の遺物となっているということを，統計学界隈に広く知らしめたのが (Gelfand and Smith, 1990) であった．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#擬似周辺尤度法",
    "href": "posts/2023/Surveys/BayesianComp.html#擬似周辺尤度法",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.1 擬似周辺尤度法",
    "text": "3.1 擬似周辺尤度法\n実は尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) が解析的に得られない場合や計算が極めて困難になる場合でも，この不偏推定量があればMCMCを実行して事後分布を得るのに十分である (Andrieu and Roberts, 2009)．この尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) の不偏推定量を得るのに粒子フィルターを用いた場合を，特に粒子MCMCという (Andrieu et al., 2010)．\nこのときの不偏推定量の性能が最終的な Monte Carlo 推定量に影響する．不偏推定量の分散を改善するには，サブルーチンである粒子フィルターの反復数を増やす必要がある．すると本体であるMCMCの反復数とのトレードオフが生じる．こうしてアルゴリズムの最適な調整が課題になる．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#高次元問題に対処するmcmc",
    "href": "posts/2023/Surveys/BayesianComp.html#高次元問題に対処するmcmc",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.2 高次元問題に対処するMCMC",
    "text": "3.2 高次元問題に対処するMCMC\nほとんどのMCMC手法は，データサイズやモデルのパラメータサイズの増加に対して，計算負荷が飛躍的に上昇する次元の呪いに苦しむ．これを克服する手法はscalabilityの名の下に盛んに研究されている (鎌谷研吾, 2021, p. 394)．\n\n対象分布の探索を効率よく行う手法として，HMC (Hamiltonian Monte Carlo) 法が提案された (Neal, 2011)．他にも NUTS (No U-Turn Sampling) (Hoffman and Gelman, 2014), Metropolis-Adjusted Langevin Algorithm (Roberts and Tweedie, 1996), Stochastic Gradient MCMC (Nemeth and Fearnhead, 2021), PDMP (区分的確定なMCMC) (Bierkens et al., 2018), (Fearnhead et al., 2018) とジグザグサンプラーなどがある，\nより良い提案分布の選択法について，MH法の最適スケーリング法，適応的サンプリング，焼き戻しなどの手法がある．\n並列計算による効率化の方向性には，並列MCMC，完全サンプリングなどの手法がある．\n他の分散低減法に，Rao-Blackwell化 (Casella and Robert, 1996)，操作変数法などがある．\n\nジグザグサンプラーについては，以下の記事も参照：\n\n  \n    \n      \n      \n        新時代の MCMC を迎えるために\n        モンテカルロ法の発展とは，背後の物理現象からの離陸の歴史でした．非対称な Metropolis-Hastings アルゴリズムがどのように生まれたかと，その最先端のアイデアと言える連続時間 MCMC 法の歴史を，サンプルコード付きで紹介します．（2024 年度統計数理研究所オープンハウス）"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#近似ベイズ手法",
    "href": "posts/2023/Surveys/BayesianComp.html#近似ベイズ手法",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.3 近似ベイズ手法",
    "text": "3.3 近似ベイズ手法\n上述までの手法はいずれもシミュレーションを十分多く行えば（理論的には）任意の精度で正しい値を得ることができるが，15 その適用範囲やスケーラビリティが課題なのであった．そこで同時に，最初からある許容精度を定めた下での近似を実行することとし，代わりにより広い適用可能性と計算速度を得るための手法も探求されている．これを近似ベイズ法という．\n一つのアプローチはシミュレーションによる方法である．これにはABC (Approximation Bayesian Computation) (Tavaré et al., 1997) と BSL (Bayesian synthetic likelihood) (Price et al., 2018) の2つの手法があるが，いずれもデータ生成過程（モデル）の複雑性と高次元性という２つの障壁が併存したときでも使える手法である．ABCではまず事後分布 \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) をある低次元な要約統計量 \\(S:\\mathcal{Y}\\to\\mathbb{R}^d\\) を用いて \\(p(\\boldsymbol{\\theta}|S(\\boldsymbol{y}))\\) で近似し，さらに尤度 \\(p(\\boldsymbol{y}|\\boldsymbol{\\theta})\\) を直接評価することは回避し，シミュレーションのみを用いて \\(p(\\boldsymbol{\\theta}|S(\\boldsymbol{y}))\\) を推定する．BSLはさらに尤度 \\(p(S(\\boldsymbol{y})|\\boldsymbol{\\theta})\\) にパラメトリックな仮定をおく．\n第二に最適化による方法がある．変分ベイズ手法とは，これは大きなパラメトリックモデル \\(\\{q^*(\\boldsymbol{\\theta})\\}\\) の中から \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\) に最も近いものを選ぶ手法である．一方で INLA (integrated nested Laplace approximation) とは，Laplaceの近似（ Section 1.5 ）に最適化を組み合わせて高次元の問題にも対応する．\nABCでは逐次モンテカルロ法も大きな役割を果たしており，ABC-SMC (Sisson et al., 2007)，ABCフィルタリング (Jasra et al., 2012)，更には変分Bayes法への応用 (Tran et al., 2017) なども進んでいる．\n変分Bayesの枠組みでは，モデルの誤想定に頑健な手法の開発も試みられている (Wang and Blei, 2019)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-BayesianModeling",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-BayesianModeling",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.4 ベイズ統計モデリングが理論モデルの実証に役立つ",
    "text": "3.4 ベイズ統計モデリングが理論モデルの実証に役立つ\nベイズモデリングの有用性は，（上述のベイズ計算の問題を除けば）どんなに複雑で大規模なモデルでも，統一的な思想と方法で対応できる点にある．\n\nメカニズムを明示的に表現した数理社会学の数理モデルを，論理的に飛躍することなくダイレクトに統計モデルへと接続できるベイズ統計モデリングは，理論モデルベースの実証研究と相性のよい，たいへん便利な方法と言えるだろう． (浜田宏, 2022, p. 137)\n\nMCMCの開発とパッケージへの実装，そして安価で高性能な計算機が普及してからというもの，ベイズ統計学の興隆は目覚ましく，現在ではベイズ統計学は統計学に関する論文の1割強を占め，諸科学分野全体に浸透しつつある．経済学・心理学への応用は早かったのに比べて，政治科学・社会科学への応用は遅れ気味であり，社会学での使用はまだ稀であると言える (Lynch and Bartlett, 2019)．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズによる逆問題",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズによる逆問題",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.5 ベイズによる逆問題",
    "text": "3.5 ベイズによる逆問題\nベイズ統計学の枠組みは，逆問題の文脈においても有用である．逆問題とは，観測データ \\(\\boldsymbol{y}\\) が与えられたときに，そのデータを説明するようなモデルのパラメータ \\(\\boldsymbol{\\theta}\\) を推定する問題である．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#sec-BayesianCausalInference",
    "href": "posts/2023/Surveys/BayesianComp.html#sec-BayesianCausalInference",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.6 ベイズによる因果推論",
    "text": "3.6 ベイズによる因果推論\n前節に挙げたベイズモデリングの美点は因果推論の文脈でも全く同様である．特に因果推論の問題では推定対象が複雑であることが多いが，このような場合でも全く同じ枠組みを提供してくれるのがベイズである．頻度論的接近では設定に応じた個別具体的な議論がベイズ計算の問題に帰着する点が利点として働くことは多いようである (Li et al., 2023)．\n実際，ベイズノンパラメトリック手法は2016年の大西洋因果推論カンファレンスのコンペティションで大きな成功を見ている (Dorie et al., 2019)．加えて強い理論的な保証も得られつつあり (Ray and van der Vaart, 2020)，これにより因果推論分野で大きな注目を集めている (Linero and Antonelli, 2023), (Daniels et al., 2023)．\n加えて，「あらゆる種の不確実性に対する統一的な定量化を与える」というベイズの性質は，因果推論から意思決定までの接続を地続きにし，例えば属人化医療などの現場でのダイナミックな意思決定に活用できることが期待される．\n\n不確実性を定量化するのに、ベイズ計算では必ず『確率』を使います。一般の人から見たら、統計で確率を使うのは当たり前と思うでしょうが、じつは他の統計手法ではそうでもなく、さまざまな解釈が生まれてしまう。定量化にはすべて統一的に確率を使うベイズ計算は、非常にシンプルなので、最終的にすべての統計はベイズに行き着くしかないと思っています．鎌谷研吾\n\nしかし，ベイズの方法が因果推論の分野で普及するための障壁は，近づきやすさにあると議論できる (Li et al., 2023)．従来の頻度論的な因果推論手法の成功には，潜在反応モデルの特定を殆どしなくて良いこと（モデルフリー），実装が簡単であることが少なからず寄与しているとすれば，ベイズ的接近もこれに当たるものを提供できるようになる必要があるだろう．Stan言語 (Carpenter et al., 2017) はこの方向への大きな試みである．"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#ベイズ学習",
    "href": "posts/2023/Surveys/BayesianComp.html#ベイズ学習",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.7 ベイズ学習",
    "text": "3.7 ベイズ学習\n機械学習の手法を用いてベイズ推論を実行する営みをベイズ学習，または単に「機械学習への確率論的アプローチ」と言ってベイズの枠組みを暗に指す場合も多い (Murphy, 2022), (Ghahramani, 2013)．\n古典的な統計手法と同様，多くの既存の（頻度論的）手法にはベイズ手法の対応物が存在する．ベイズの方法だと推定の確信度合いもセットで定量化され，頻度論的対応物よりも得られる情報が多い一方で，計算は既存手法よりも難しいことが多いという構造は，機械学習においても変わらない．\n実際，現存のニューラルネットワークの訓練法を超えるベイズ計算法が今後提案されるとは考えにくいが，その最適化する所の目的関数が例えば正則化項付きの平均自乗誤差である場合は，ある正規事前分布と正規尤度に対するMAP推定量に対応する (Seitz 2022)．畢竟，多くの既存手法も「ベイズ学習を非ベイズ的な方法で実行している」と捉えられるのである（逆も然り）．\n中でもベイズ学習を採用するのが良い場面としては，モデルの大きさに対して学習に使えるデータの数が少ない場合や，モデルに事前情報を組み込みたい場合16 ，さらには医療・政策への応用など意思決定に繋げるために不確実性の定量化が肝要な場面などがあり得る．\n実際，ベイジアン・ニューラルネットワークでは計算の困難ささえ乗り越えれば，複数の適切なモデルに対し，事後分布によって平均を取って最終的なモデルとすることで，過学習を防止し (Mackay, 1995)，大きな性能改善を得ることができる (Wilson and Izmailov, 2020)．17"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#世紀の統計学とベイズの役割",
    "href": "posts/2023/Surveys/BayesianComp.html#世紀の統計学とベイズの役割",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "3.8 21世紀の統計学とベイズの役割",
    "text": "3.8 21世紀の統計学とベイズの役割\nこのように，21世紀に入ってからベイズの成功は目まぐるしく，この傾向はさらに進むと思われる．これは統計計算の手法の進化によって達成された．今後とも統計計算の手法は，シミュレーション・変分法・最適化の垣根を超えて多様化の一途を辿るだろう (Green et al., 2015, p. 857)．\nその中でも筆者は，ベイズ手法が提供する事後分布として得られる不確実性の表現・視覚化が，計算機・自然・人間の間のよきインターフェイスとなっていくことを願っている．18\n\nThe applied statistician should be Bayesian in principle and calibrated to the real world in practice-—appropriate frequency calculations help to define such a tie. (Rubin, 1984)"
  },
  {
    "objectID": "posts/2023/Surveys/BayesianComp.html#footnotes",
    "href": "posts/2023/Surveys/BayesianComp.html#footnotes",
    "title": "ベイズ計算とは何か | About Bayesian Computation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n正式名称をThe Royal Society for the Improvement of Natural Knowledge by Experimentという↩︎\n当局の人間に死亡を報告する義務は全くなかった。その代わり、それぞれの教区では2人かそれ以上の死体を調査し、死因を決定する義務を負う調査員を任命していた。「調査員」は死亡を報告する毎に遺族より少額の手数料を徴収する資格が与えられていたので、教区では任命しなければ貧困のため救貧税による支援が必要となりそうな人間を割り当てていた。（Wikipediaページより）↩︎\nこれは統計的モデルとしてBernoulli分布 \\(Y_i|\\theta\\overset{\\text{iid}}{\\sim}\\mathrm{Ber}(\\theta)\\) を仮定するということである．↩︎\nパラメータ \\(\\theta\\) は「男児が生まれる確率」であるが，これ自体にも事前分布という「確率」\\(p(\\theta)\\) を導入することに戸惑う読者も居るだろう．しかし，これがベイズ統計学の特徴である．「男児が生まれる確率 \\(\\theta\\)」だろうとなんだろうと，「わからない」「不確実性がある」と主観的に感じるあらゆる対象に，確率分布を導入して事後分布を得ることで推論を実行する，これがベイズ統計学の枠組みの普遍性であり，無差別性であり，有用性を支えている．↩︎\n各 \\(\\theta\\) の下で目の前のデータ \\(y_1,\\cdots,y_n\\) が生成される確率 \\(p(\\boldsymbol{y}|\\theta)\\) が低いということは，「その \\(\\theta\\) から生成されたデータである確率は低い」という逆の発想ができる．そこで \\(p(\\boldsymbol{y}|\\theta)\\) という条件付き確率を尤度ともいう．今回は \\(p(\\boldsymbol{y}|\\theta)=\\theta^{\\sum_{i=1}^ny_i}(1-\\theta)^{\\sum_{i=1}^n(1-y_i)}\\) である．↩︎\nさらに，\\(g(\\theta)=\\theta^p\\) と取った場合，事後積率という統計量になる．等に \\(p=1\\) の場合が事後平均である．↩︎\nなお，1763に出版されたものはPriceによる補遺も付いた短縮版であり，全文は1974年に出版された．(Stigler, 1990)↩︎\npp.376-403 がBayesの論文の本論の内容であり pp.399-403 で計算法を３つのルールにまとめているが，その導出部は一部「長すぎるから掲載を省略する」とされている．↩︎\n一方で，Bayesの逆確率の問題への言及自体は，Laplaceの後年の1781年の著作Mémoire sur les probabilitésへのCondorcetによる序文で初めて登場する (Martin+2023-history?)．↩︎\nnCatLab 参照．↩︎\n(安藤洋美, 1995) も参照．↩︎\n1970年にインテルが世界初の DRAMである Intel 1103 を発売した．Wikipediaページ参照．↩︎\n物理学では Heat Bath 法と呼ばれ古くから同様のアルゴリズムが存在したが，統計学界隈では現在でも Gibbs サンプラーと呼ばれる．↩︎\n統計的画像処理など，Markov 確率場 によってモデリングされる対象においてはよくある状況である．↩︎\nこの性質を指して，approximateの対義語としてexactという形容詞で表現される．↩︎\nfunctional Bayes (Sun et al., 2019) という手法では，希望する入力と出力の組を事前に用意するのみで，適切な事前分布を提案してくれる枠組みである．↩︎\n(Wilson and Izmailov, 2020) によると，二重降下現象も見られない．↩︎\n推定結果に自信がないときはそう表明してくれる機械は親しみやすい．↩︎"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html",
    "href": "posts/2023/Surveys/ParticleFilter.html",
    "title": "粒子フィルターとは何か",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#フィルタリング問題の歴史",
    "href": "posts/2023/Surveys/ParticleFilter.html#フィルタリング問題の歴史",
    "title": "粒子フィルターとは何か",
    "section": "1 フィルタリング問題の歴史",
    "text": "1 フィルタリング問題の歴史\n\n1.1 フィルタリング（濾波）問題とは何か？\nフィルタ（濾波器）の第一義は，液体から不純物を取り除くための装置である．そのアナロジーで「フィルタリング問題」と言った場合は，信号処理の意味で電圧や電波の信号を「濾過」してノイズを除去し本当に注目したい部分を純粋化する営みのことを指す．凡ゆる通信機器において装置の熱運動によるノイズが入ることは避けられぬ自然の摂理であり，フィルタリング問題は普遍的な課題である．1\nGauss は天体観測の経験から「誤差論」と最小二乗法を発明し，これが現代の統計的推定理論の先駆けとなった．これと同じように，通信と制御の分野では「時々刻々と受信するデータから時々刻々と変化する信号をどのようにうまく濾波するか」という独自の課題から，独自の理論が発展していった．2\n特に，デジタル回路がない時代では，「どのような電気回路のシステムとして濾波機をデザインすれば良いか？」という電気工学的な回路設計の問題としての側面も大きかった．\n\n\n1.2 最初のフィルタリング理論\nこのアナログフィルタの時代で，フィルタリングの問題を統計的技術で解くための理論3 が，まず離散時間の場合が (Kolmogorov, 1941)，続いて連続時間の場合が (Wiener, 1949) によって模索された．4\nしかし，この Kolmogorov と Wiener 理論では「信号とノイズの過程が定常である」という仮定の下で展開されており，この定常性の制約が Kolmogorov-Wiener 理論の広い応用を阻んでいた．\nだがこれは，「当時の技術（抵抗器やコンデンサーなど）で実装出来る範囲」という制約がある上で考えられた理論としては，仕方ないことでもあった．更なる理論的発展も，デジタル技術の登場を待つ必要があった．\n\n\n1.3 デジタルフィルタリングの登場\nトランジスタというデジタル技術が使われるようになり，集積回路の製造技術が発達すると，「フィルタ（濾波器）」はアナログデジタル変換器，レジスタ，メモリ，マイクロプロセッサから構成されたデジタルフィルタが主に使われるようになった．アナログフィルタから物理的な姿は全く変わり，もはや肉眼では見えない装置になってしまったのである．5\nその中で (Kalman, 1960) が，定常性の仮定が満たされない場合でも使えるアルゴリズムである カルマンフィルター を提案すると，すぐに Apollo 計画に導入されてスペースシャトルの制御へ実用化され，更には海軍の潜水艦などにも応用されていった．6\nあらゆるシステムがデジタル化されていく中で，アナログフィルタは徹底的にデジタルフィルタに代替されるようになった．これに伴い，現代でフィルタリング問題と言った場合，現在 \\(t\\) までの観測 \\(Y_1,\\cdots,Y_t\\) からノイズを除去してメッセージ部分 \\(X_t\\) をなるべく正確に推定するアルゴリズム という完全に数学的で抽象的な存在として研究が進められていくことになる．\n\n\n1.4 状態空間モデルという語彙\nKalman の論文 (Kalman, 1960) の新規性はアルゴリズムだけではなく，状態空間モデル という枠組みを導入し，どのようなシステムに適用可能かに関する共通言語を提供したことが，即時的な応用に寄与した面もあるだろう．7 例えばこの語彙に従えば，「Kalman filter は状態空間モデルが線型正規であれば最適なフィルタリング手法」ということになる．\nこの状態空間モデルの枠組みと同様なものが (Baum and Petrie, 1966) によって隠れ Markov モデルとして展開され，こちらは (Baker, 1975) により音声認識に応用された．現代でも多くの音声認識システムは隠れ Markov モデルに基づく．8\n状態空間モデルはその後，(Akaike, 1974) で時系列モデリングに応用され，再び統計学の分野と深く関わるようになる．9\n\n\n1.5 カルマンフィルターの限界\nしかし Kalman filter にも大きな制約があった．それは モデルに線型かつ正規であるという大きな制約があった ということである．一方で現実のシステムは殆どが非線型性を持つ．\nそこで NASA の Ames 研究センター ではすぐに 拡張 Kalman フィルタ と共分散行列の二乗根を保持する実装が考案され，これが本当の意味で Kalman フィルターを実用に耐えるものにした (McGee and Schmidt, 1985)．10\nだが，「拡張」の名前の通り本質的な解決とは言えず，システムの非線型性が強い場合は性能が伸びない．11 こうして，計算機や情報通信技術の発展と共に複雑化していくシステムに併せて，様々なフィルターが考案されていく必要があるのである．統計計算の時代の黎明である．\n\n\n1.6 非線型性・非正規性という強敵\n実は，1960年に開発された Kalman filter を真に超克する手法は，1990年代に入るのを待つ必要がある．他のシミュレーションに基づく ベイズ計算手法 と同じく，計算機の十分な発達を待つ必要があったのである．\nというのも，Kalman filter は，線型正規状態空間モデルの下でフィルタリング分布 \\[\n\\mathcal{L}[X_t|Y_1,\\cdots,Y_t]\n\\tag{1}\\] は再び正規分布であり，平均と共分散という２つの量のみで完全に特徴付けられるため，これらのみを考慮し，微分方程式を解いて更新規則を事前に得ておくことで，計算量を大幅に削減することが出来る，というトリックに基づく．\nこのように線型性と正規性が同在する状況，または状態空間が有限であるなどの限られた状況でない限り，フィルタリング分布 Equation 1 は有限次元の十分統計量を持たない．12\n従って，一般の状況に対応出来るフィルタには，上述のような計算量削減のトリックは絶対に存在せず，正面から分布 Equation 1 を近似する方法を考える必要がある．これには新しいアイデアが必要であると同時に，一定の性能を持つ計算機の出現を待つ必要もあったのである．\n\n\n1.7 種々の近似戦略\nフィルタリング分布 Equation 1 \\[\\mathbb{P}_t(X_t\\in dx_t):=\\mathcal{L}[X_t|Y_{0:t}]\\] は Bayes の定理を通じて再帰的な関係 \\[\n\\begin{align*}\n    &\\mathbb{P}_{t-1}(X_t\\in dx_t|Y_{0:t-1})\\\\\n    &=\\int_{x_{t-1}\\in E}\\mathbb{P}_{t-1}(X_{t-1}\\in dx_{t-1}|\\\\\n    &\\qquad Y_{0:t-1}=y_{0:t-1})P_t(x_{t-1},dx_t),\\\\\n    &\\mathbb{P}_t(X_t\\in dx_t|Y_{0:t}=y_{0:t})\\\\\n    &=\\frac{1}{p_t(y_t|y_{0:t-1})}f_t(x_t|y_t)\\\\\n    &\\qquad\\mathbb{P}_{t-1}(X_t\\in dx_t|Y_{0:t-1}=y_{0:t-1}).\n\\end{align*}\n\\] を満たすが，この積分を計算する必要がある．13\n粒子フィルターでは相関を持った粒子系により，これらの値を逐次的に近似していくが，このアルゴリズムが出来る前に提案された近似手法を総覧する．\n\n1.7.1 解析的な方法\nSection 1.5 で紹介した拡張 Kalman filter は，モデルが線型に近い場合には有効な近似手法であるが，一致推定量にはならない．この点を修正するために，線型近似の誤差を修正する IEKF (Iterated EKF) などが開発された．\n\n\n1.7.2 数値積分による方法\n状態空間 \\(E\\) の次元が3以下である場合，積分は数値積分法によっても効率的に近似できる (Kitagawa, 1987)．14\n特に状態空間が有限である場合は積分は加法に退化するため，正確に実行することができる．15 この場合が隠れMarkovモデルに当たる．\n隠れMarkovモデルに於て MAP 推定量を動的計画法に基づいて探索する Viterbi 算譜 (Forney, 1973), (Viterbi, 1982) とパラメータ推定のための EM アルゴリズムの変種 Baum-Welch 算譜 (Baum and Eagon, 1967), (Gopalakrishnan et al., 1989) とは音声認識の分野で広く使われており，また状態空間が近似的に離散である場合にも近似手法として採用される．16\n\n\n1.7.3 Gauss混合で近似する方法\nフィルタリング分布 Equation 1 を正規分布の有限混合で近似する方法は Gaussian sum filter と呼ばれる (Sorenson and Alspach, 1971)．これは多峰性を帯びる事後分布のモデリングに強く，物体追跡の分野で広く使われることとなったが，一般の設定に使える普遍的な手法ではなかった．17\n\n\n1.7.4 アンサンブルによる近似\nフィルタリング分布 Equation 1 を正規分布を表現する決定論的に設計された粒子系によって近似し，これをシステムモデルに従って伝播させる．これにより，フィルタリング分布の2次以下の積率までは性格に近似できていることになる．この手法を 無香 Kalman filter (Unscented Kalman filter) という (S. Julier et al., 2000), (S. J. Julier and Uhlmann, 2004)．\nこの手法は，宇宙から大気圏に再突入する弾道物体の追跡などの場面で，粒子フィルターよりやや正確性が劣るが計算が速く，実用的であることも知られている (Ristic et al., 2004, p. 101)．\nこの手法は本質的に (Evensen, 1994) の EnKF (Ensemble Kalman filter) によって拡張された（ Section 3.5 も参照）．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#粒子フィルターの発明",
    "href": "posts/2023/Surveys/ParticleFilter.html#粒子フィルターの発明",
    "title": "粒子フィルターとは何か",
    "section": "2 粒子フィルターの発明",
    "text": "2 粒子フィルターの発明\n線型性や正規性の仮定を 全く 必要とせず，あらゆる状態空間モデルに使えて，加えて高次元でも適用可能な夢の新フィルタリング手法は，確率的シミュレーションを利用する Monte Carlo 法の一種 であった．(Gordon et al., 1993) はこれを bootstrap filter という名前で発表し，角度観測のみを用いた物体追跡の問題 (bearings-only tracking) への応用も付した．\n実は，この bearings-only tracking は極めて非線型性が強い問題として古典的なものであり，従来の拡張 Kalman filter の方法では精度が全く伸びなかった．これに比べて bootstrap filter では圧倒的な性能改善が見られたのであった．18\nそれだけでなく，この手法はフィルタリング問題の範疇を超えて広範な応用先を見つけつつあり，MCMC と並ぶ ベイズ計算法 となっている（ Section 2.3 ）．\nなお，北川源四郎も同年（1993年）のカンファレンスにて，Monte Carlo filter の名前で同様のアルゴリズムを発表している．そのジャーナル版は (北川源四郎, 1996a)．19 日本語文献 (北川源四郎, 1996b) はウェブ上からも読める．\n\n2.1 逐次重点サンプリングの修正としての粒子フィルター\nこの手法は 重点サンプリング を繰り返すという逐次重点サンプリングの改良として開発された．逐次重点サンプリングのアイデアは古く，(Hammersley and Morton, 1954) で提案され，(Mayne, 1966), (Handschin and Mayne, 1969) で逐次推定に応用された．\nしかし，これには荷重の分散が指数増大するという致命的な欠点があった (Chopin, 2004)．特に何度か反復を経ると，１つの粒子を除いて他の粒子は全て荷重を殆ど持たなくなってしまうという現象が起こる (Del Moral and Doucet, 2003)．このように，荷重の分散が大きくなり，少数の粒子しか推定に関与しなくなる現象を 荷重の縮退 という．20\nそこで，(Rubin, 1987) のアイデアを基に，21 リサンプリング という新たな機構を取り入れることを考える．これは 遺伝的変異・選択機構 とも呼ばれ，22 尤度の高い粒子を複製する一方で尤度の低い粒子は削除するというものである\nこれにより定期的に荷重をリセットすることで分散の指数増大を抑えることができ，が保たれるのである．しかしながらこの仕組みにより粒子の間に相関が生じるために，理論的解析を困難にする．この点から粒子フィルターは （平均場）相関粒子法 ともいう．23\nリサンプリング機構の分計算負荷は上がるが，1990年代では計算機の性能はすでにこれを補って余りある段階に達していたのである．なお，(Gordon et al., 1993) はリサンプリング手法としては多項リサンプリングを採用しており，極めて実装が簡単という点も多くの応用を生んだ理由である．24\nリサンプリングの実際の実装については，粒子フィルターの実装の稿 も参照．\n\n\n2.2 粒子フィルターの応用\n(Gordon et al., 1993) による粒子フィルターの考案は，物体追跡（と防衛目的）への応用が念頭にあり，この分野では粒子フィルターが極めて有効である (Ristic et al., 2004)．これは非線型性をものともしない性質に加えて，事前情報を柔軟に取り入れやすいという粒子フィルターの性格も大きく貢献している．25\nコンピュータビジョンへの応用も早期から取り組まれており (Isard and Andrew, 1998)，これに続いてロボティクス，HCI (Human-Computer Interaction) 分野への応用もなされている (岡兼司, 2005), (Wills and Schön, 2023)．\n一方で，(北川源四郎, 1996a) は季節調整モデルなど非定常時系列への応用が念頭にあった．確率的ボラティリティモデルなど，ファイナンスで扱う時系列は非線型性・非正規性を示すと同時にデータ数も多い．逐次推定のステップ数が増えようとも誤差が蓄積しない粒子フィルターが見事に推定を実行する．26\n加えて，マクロ経済学の分野で 動学的確率的一般均衡モデル (DSGE) の推定にも応用されている．DSGE は非線型なミクロ経済学的モデルの上に構築された大規模なモデルで， 従来は MCMC を用いたベイズ推論が実行されていたが，粒子フィルターに焼戻し法や並列計算を組み合わせることでこの問題を回避でき，さらに事後分布が多峰性を持つ場合でも有用である (Herbst and Schorfheide, 2013)．27\n近年では他の社会科学分野でもベイジアンモデリングが用いられ（ベイズ計算の稿 参照），粒子フィルターも エージェント・ベースド・モデル への応用などが試みられている (Lux, 2018)．\n\n\n2.3 粒子フィルターの一般の推定問題への応用\nこうして，粒子フィルターは非正規・非線型フィルタリング問題の解決のために開発されたアルゴリズムであったが，非線型フィルタリングに限らず極めて広い問題へと応用出来ることが徐々に明らかになった．\n特にサンプラーとしても極めて有効であり（ Section 2.3.2 ），粒子フィルターは MCMC と併せて ベイズ計算 の主要トピックの１つに躍り出た (Martin et al., 2024, p. 11)．この意味で，粒子フィルターは広く 逐次 Monte Carlo 法（Sequential Monte Carlo methods 略して SMC ）とも呼ばれる．28\n\n2.3.1 ベイズ学習\n逐次的でない「静的」な設定の下での Bayes 推論に SMC を用いる方法は (Chopin, 2002) が草分け的な仕事をした．この枠組みでは，Bayes 事後分布 \\(\\pi(\\theta|y_1,\\cdots,y_N)\\) の近似において，途中の \\(\\pi(\\theta|y_1,\\cdots,y_n)\\) を経由して逐次的に近似することが，自然な計算コスト削減法として理解できる．\n\n\n2.3.2 サンプラーとしてのSMC\n複雑な分布からのサンプリングや，その正規化定数の計算という MCMC と同様の用途に SMC を使うこともできる (Del Moral et al., 2006)．\nSMC は 調音 (tempering) を通じてサンプリング問題に応用される．これは目標の分布 \\(\\pi_p\\in\\mathcal{P}(E)\\) に対して，これに至る \\(\\mathcal{P}(E)\\) 上の道 \\[\n[p]\\ni n\\mapsto\\pi_n\\in\\mathcal{P}(E)\n\\] を通じて，より簡単な分布 \\(\\pi_1,\\pi_2,\\cdots\\) から逐次的にサンプリングをするというアイデアである．\nこの媒介的な分布 \\(\\pi_n\\) を焼き戻し分布 (tempered distribution) または架橋分布 (bridging distribution) などとも呼ぶ．29\n詳しくは，SMC サンプラーの稿 を参照．\n\n  \n    \n    \n      粒子フィルターを用いたサンプリング | About SMC Samplers\n      粒子フィルターは 30 年前に「万能」非線型フィルタリング手法として開発されたが，それは粒子系を輸送するメカニズムとしての万能性も意味するのであり，汎用サンプラーとしても「万能」であるのかもしれないのである．近年，最適化や最適輸送の理論と結びつき，その真の力がますます明らかになりつつある．本稿では現在までのサンプラーとしての SMC 手法に対する理解をまとめる．\n    \n  \n\n\n\n2.3.3 最適化\nSMC を最適化へ応用することができる．\nまず，任意の確率的最適化アルゴリズムに対して，これを並列して実行し，うまくいっているものとうまくいっていないものの間に遺伝的変異・選択機構を導入することでより性能の良い発見的アルゴリズムを導出出来ることを (Aldous and Vazirani, 1994) が指摘しており，この機構を “go with the winners” と呼んでいる．\n古典的な大域的最適化法に 焼きなまし法 (simulated annealing) (Kirkpartick et al., 1983) があるが，(Schäfer, 2013) は特定の目的関数に対してこれを一般化して粒子法に基づく最適化法を提案し，特に多峰性を持つ場合に大きく性能を改善した．\n(Johansen et al., 2008) では潜在変数モデルのパラメータの最尤推定に，EM アルゴリズム (Dempster et al., 1977) の代わりに simulated annealing に基づいた手法を用いている．分布の台が最尤推定量に収束するような分布の列 \\[\n\\overline{\\pi}_{\\gamma_n}(\\theta)\\,\\propto\\,p(\\theta)p(y|\\theta)^{\\gamma_n}\n\\] を構成し，これから逐次的にサンプリングをするのである．最終的なアルゴリズムは，EM アルゴリズムや MCMC を用いる場合より，局所解に囚われることが少なく，初期値の設定に殆ど左右されないという利点がある．\nこの枠組みは一般の非凸最適化アルゴリズムになる可能性がある．30\n\n\n2.3.4 稀現象シミュレーション\nSMC によるサンプリングは，直接のシミュレーションが困難な分布 \\(\\pi_p\\) に対しても，\\(\\pi_1,\\cdots,\\pi_p\\) と逐次的に近似することでこれを可能にするというアイデアであった．\n特に，シミュレーションが困難である分布 \\(\\pi_p\\) の例として，極めて稀な事象 \\(A_p\\) に関する条件付き分布などがあり得る．これに対して事象列 \\(A_0\\supset\\cdots\\supset A_p\\) を取り， \\[\n\\pi_n(d\\theta)=\\frac{1}{L_n}1_{A_n}(\\theta)\\nu(d\\theta)\n\\] という仲介分布の列を取るのである．この問題を 稀現象シミュレーション という．\n\n\n2.3.5 確率的グラフィカルモデルの推論手法として\n変数間の統計的依存関係が無向グラフによって与えられるモデルを 確率的グラフィカルモデル という．ニューラルネットワーク (Rumelhart et al., 1987) も 状態空間モデル もその例である．\n確率的グラフィカルモデル自体多くの応用先をもち，疫学における疾病マッピング (Green and Richardson, 2002)，画像解析 (Carbonetto and Freitas, 2003) などにも応用されている．31\nこのようなモデルの尤度は極めて複雑になるが，これへ至る道を自然な方法で構成することができる (Hamze and de Freitas, 2005)．複雑なモデルでは MCMC は尤度の正規化定数を評価する方法を持たないが，SMC ではこれを自然に評価することができる．\n\n\n2.3.6 ポリマーシミュレーション\nポリマーシミュレーションにおいても重点サンプリング法と同じ発想が提案されたのは極めて早い段階であった (Rosenbluth and Rosenbluth, 1955)．加えてリサンプリングにあたる enrichment の考え方もあった (Wall and Erpenbeck, 1959)．\nなお，このような物理・化学的文脈では，状態空間を探索する主体としての意味を強調し，「粒子」の代わりに walker と呼ぶ．32\nこれら２つを組み合わせることで，長いポリマー鎖のシミュレーションを可能にする方法として，相関粒子法に基づくアルゴリズム PERM (pruned-enriched Rosenbluth algorithm) が提案されている (Grassberger, 1997)．\nこの手法はたんぱく質の折り畳み問題にも応用されている (Hansmann and Okamoto, 1999)．\n\n\n2.3.7 量子系シミュレーション\n量子多体系の基底状態のシミュレーションにおけるモンテカルロ法である QMC (Quantum Monte Carlo)33 でも， branching というリサンプリング機構を取り入れた相関粒子法が用いられている (Assaraf et al., 2000)．\nこれは大規模な疎行列の最小固有値・固有ベクトルを近似する手法として，量子系に限らない幅広い応用がある．34\n\n\n\n2.4 粒子フィルターの弱点\n\n計算量\n粒子フィルターは高い汎用性の代償として，多数の粒子による高精度な推論のためには多くの計算量を必要とすることは欠点に挙げられる．が，CPU や並列計算の発展により十分な量の粒子を用意できる場面も増えたため，その問題点も形骸化してきてると言える．35\n荷重の縮退\nまた粒子フィルターは，観測 \\(Y_t\\) の次元が大きいなど，観測から得られる情報量が多く，尤度（ポテンシャル）の尖度が高いとき，リサンプリング機構があってもやはり縮退を起こしてしまう．36 このような場合は，観測の情報を柔軟に取り入れた提案核を構築し，誘導粒子フィルターをうまく設計する必要がある．\n\n\n粒子フィルタを適用する際の課題の一つは，各粒子に割り当てられる重みが１粒子に集中する，いわゆる退化の問題を限られた数の粒子でいかに克服するかである．(上野玄太, 2019)\n\n\n高次元\n地球科学や天気予報の分野では \\(Y_t\\) は大きく（\\(10^7\\) を超えることもある），このような場合は粒子フィルターは実行可能でなくなる．加えて Kalman フィルターも逆行列の計算が不安定になり，アンサンブル Kalman フィルタという粒子法が用いられる（ Section 3.5 ）．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#今後の研究",
    "href": "posts/2023/Surveys/ParticleFilter.html#今後の研究",
    "title": "粒子フィルターとは何か",
    "section": "3 今後の研究",
    "text": "3 今後の研究\n粒子フィルターの更なる応用には次の点の研究が肝要である．\n\n3.1 Feynman-Kac 理論と粒子法の統一的理解\n粒子フィルターは状態空間モデルのフィルタリングに使えるだけでなく，Feynman-Kac 測度の確率的近似に使える汎用手法である というのがより新しい数学的理解である．37 この文脈では 相関粒子法 (interacting particle methods) と呼ばれる．38\n\nthe mathematical concepts and models are now at a point where they provide a very natural and unifying mathematical basis for a large class of Monte Carlo algorithms. (Del Moral and Doucet, 2014, p. 2)\n\nこの枠組みからならば，粒子フィルターに限らず，物理学・化学・工学で用いられている多くの 発見的手法 について，理論的な解析が可能になる可能性がある．\n\n\n3.2 提案分布の取り方\nSection 2.4 で触れた通り，特に観測の情報量が大きい場合，提案分布の選び方が粒子フィルターの精度を大きく左右する．これは粒子を高確率領域に始めから誘導するように設計する 誘導粒子フィルター によってある程度対処できる．39\n参照 Markov 核 \\(M_t\\) がより良い攪拌性を持ち，ポテンシャル \\(G_t\\) が平坦であるほど性能は良い傾向があるが，40 このときの提案分布の取り方について普遍的な指針というものが得られていない．\n\nThe key factors for a successful application of particle filters in practice are therefore a good choice of the importance density and Rao-Blackwellization if possible. (Ristic et al., 2004) Epilogue\n\n(Guarniero et al., 2017) と (Heng et al., 2020) は提案分布にパラメトリックモデルを用意し，粒子推定量の分散を最小化するようにそのモデル内で逐次的に最適化していく機構を提案している．\n(Naesseth et al., 2015) が提唱する nested SMC は，は各時刻での提案分布を近似するために，もう一つのSMCを内部に走らせる．当然計算量は二倍になるが，それでも単純な bootstrap filter から大きく性能が改善する場合が多い．\n加えて機械学習の観点からも，真の事後分布との KL 距離を適応的に最小化する汎用手法に，提案分布のパラメトリックモデルにニューラルネットワークによる大型のパラメトリックモデルを併せる手法が提案されている (Gu et al., 2015)．\n\n\n3.3 高次元性への対応\n状態空間が高次元になることと，既存のモデルを状態空間モデルに定式化することに困難が伴うことが，朧げながら共通課題のように思われるが，その現れ方と解決法は個々の事例で異なる．\n\n3.3.1 部分的な線型構造の利用\n周辺化粒子フィルター，または Rao-Blackwellized particle filter とも呼ばれる方法である．これは多くの場面で，仮定されている状態空間モデルが部分的に線型である場合に，線型の部分をKalmanフィルターによって正確に解き，残った部分のみを粒子フィルターで解くことで，精度の向上とアルゴリズムの効率化を図る方法である．\n(Ristic et al., 2004, p. 287) では，探知前追跡 (track-before-detect) の問題41において，周辺化粒子フィルタが，必要な粒子数を大きく削減してくれることを紹介している．\nその他にも，問題毎の特有の構造を利用して計算量を削減・パフォーマンスを最適化することは重要な営みである．\n\n\n\n3.4 漸近論\n\n3.4.1 粒子数に関する漸近論\n目前の問題を，所与の精度で解くために必要な粒子数は幾ばくか？という問題は実用上も有益だと思われる．42\n\n\n3.4.2 時間離散化に関する漸近論\n(Chopin et al., 2022)\n\n\n\n3.5 EnKFの数学的解析\n状態空間が高次元である場合，（どうなる？）\nモデルが正規性を持つならば，EnKF (Ensemble Kalman Filter) (Evensen, 1994) が有効であり，データ同化の分野で広く使われている．43\n一方で，その数学的な振る舞いはほとんど解明されておらず，1次元の場合から調べている状況である (Del Moral and Horton, 2023)．\n\n\n3.6 粒子フィルターの応用\nリアルタイム性と ベイズ推定の意思決定への応用との相性の良さ が，今後多くの重要な応用を見る可能性がある．\n\n3.6.1 属人化医療への応用\n筆者は属人化医療への応用が大きなモチベーションになっている．44\n病気の進行（あるいは健康）のモニタリングのために，健康診断やウェアラブルデバイス，フォローアップから得られるデータは，治療の見直しや異常の早期発見のために即時処理されることが望ましい．これに逐次モンテカルロ法でBayes的に迫る研究がある (Alvares et al., 2021) ！\nさらに，個々人の日常生活のレベルではSMCを用いているものはどうやらまだなく，運動と睡眠時間の間の関係と処置効果をMonte Carlo法を用いて推定している研究はある (Daza and Schneider, 2022)．これを逐次化することで，よりリアルタイムで自分に合った生活習慣への示唆が得られるアプリを開発できるかもしれない．\nまた，属人化医療においてはシステム生物学的なモデルに基づいた薬効推定が欠かせない．小規模な患者群と測定時間（服薬時間）に関する不確定性を考慮した手法が (Krengel et al., 2013) で考察されている．\nまた，腫瘍サンプルに含まれる体細胞の突然変異に関するデータから，SMCを用いて腫瘍の発達と進行の状態を理解する手法も提案されている (Ogundijo et al., 2019)．\n\n\n3.6.2 ゲノム解析への応用\n次世代DNAシークエンサーでは，DNAの各塩基ごとに異なる蛍光物質を結合させ，蛍光の波長と強度により塩基を読み取る仕組みであり，蛍光強度の生データからDNA配列データへ変換するベースコールと呼ばれる段階で粒子フィルターを使うことも提案されている (Shen and Vikalo, 2012)．\n\n\n3.6.3 疫学への応用\nCovid-19のようなパンデミックにおいて，疫学モデルを通じて時間変動する再生産数をリアルタイムでモニタリングをして意思決定に繋げるためのSMC手法も考えられており，実際にノルウェーで使用され有効性が実証された (Strovik et al., 2023)．"
  },
  {
    "objectID": "posts/2023/Surveys/ParticleFilter.html#footnotes",
    "href": "posts/2023/Surveys/ParticleFilter.html#footnotes",
    "title": "粒子フィルターとは何か",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Anderson and Moore, 1979) 第1.1節．↩︎\n(有本卓, 1970)．↩︎\nこのフィルタリング問題の統計的な側面を，前述の電気工学的な側面から区別して，stochastic filteringと呼んだりもする．↩︎\n(Bain and Crisan, 2009) 1.3節．↩︎\n(青山友紀, 1986) 「当初は大型コンピュータを用いたシミュレーションの技法であったディジタルフィルタが，今では超LSI 1チップで実現されるまでになった．」他にも，発表時1986年では，音声信号を中心とする低周波帯域ではデジタルフィルタがアナログフィルタを駆逐しつつあること，通信システムのデジタル化に伴ってこの勢いは完全に代替するまで進むだろうとの筆者の考えが述べられている．↩︎\n(McGee and Schmidt, 1985) がNASAからの資料．また，(Del Moral and Penev, 2014) も参照．加えて，(Kalman, 1960) からちょうど10周年の文献 (有本卓, 1970) に当時の雰囲気も感じさせる良いサーベイがある．↩︎\n(Ristic et al., 2004, p. 3) “The state-space approach is convenient for handling multivariate data and nonlinear/non-Gaussian processes and it provides a significant advantage over traditional time-series techniques for these problems.”↩︎\n(Rabiner, 1989)．(Gales and Young, 2007) には “almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs” とある．↩︎\n(Kitagawa, 1998) で指摘されている．↩︎\nまた，同様にアポロ計画の中で，飛行体に載積出来るような小規模な計算資源と短い単語長でも安定してKalman filterが動くように，共分散行列の二乗根を保持するという “square-root” formulation of the filter が 1972年に考案されたことが，summary でも触れられている (McGee and Schmidt, 1985)．↩︎\n(S. J. Julier and Uhlmann, 2004, p. 402) など．拡張 Kalman filter は遷移関数を線型近似することに基づく．よって Jacobi 行列の計算が必要であり，このため 解析的方法 とも呼ばれる (Ristic et al., 2004, p. 21)．よって遷移関数が可微分でない場合も実行不可能である．とはいっても，拡張 Kalman filter はナビゲーションシステムや GPS のデファクトスタンダードである Wikipedia．(Ristic et al., 2004) 第7章では距離のみでの追跡 (range-only tracking) では拡張 Kalman filter の性能と大差なく，計算量の問題から EKF の選択を推奨している．第8章でも弾道物体追跡の問題で，限られた設定では粒子フィルターと同等の性能を見せている．↩︎\n(Ristic et al., 2004, p. 16)．↩︎\nこの結果は (Chopin and Papaspiliopoulos, 2020) 第5章 など．記法 \\(Y_{0:t}\\) は 本サイトの数学記法一覧 を参照↩︎\n(Crisan and Doucet, 2002) に3の数字が例示されている．↩︎\n(Chopin and Papaspiliopoulos, 2020) 第6章，(Ristic et al., 2004) 2.2節．↩︎\n(Ristic et al., 2004, p. 24)．↩︎\n(Ristic et al., 2004, p. 25)．↩︎\n(Gordon et al., 1993) の結果であると同時に，(Ristic et al., 2004) 第6章でも種々の手法と比較した数値実験がなされている．一方第7章にて，距離のみでの追跡 (range-only tracking) では拡張 Kalman filter の性能と大差なく，計算量の問題から EKF の選択を推奨している．↩︎\nDel MoralのWebサイトも歴史的背景に詳しい．↩︎\nweight degeneracy (Creal, 2011) p.253，(Cappé et al., 2005) 第7章，(Robert and Casella, 2004, p. 551) 14.3.3節 など．↩︎\nSIR (Sampling/Importance Resampling) Algorithm と呼ばれるものであった．これの逐次化が粒子フィルターだとみなせる (Robert and Casella, 2004, p. 552) 14.3.4節．↩︎\n(Del Moral and Doucet, 2014) などの用語である．↩︎\n(Del Moral and Horton, 2023) では mean-field type interacting particle methods と呼んでいる．呼び方については (Iba, 2001) と (Del Moral, 2013) と 紹介記事 も参照．↩︎\n(Creal, 2011) p.256．↩︎\n事前情報というのは，自動運転の文脈では自動車が動き得る領域というのは極めて限られている，というような事前に判明しているが，うまく取り入れにくい情報のことをいう．(Ristic et al., 2004) 第6章や第9章で繰り返し種々の設定で実証されている．(Yang et al., 2023) は水中での物体追跡が縮退により従来は粒子フィルタが使えなかった問題の解決を試みている．(Kummert et al., 2021) はロボット支援を用いた手術中に物体追跡を利用する際に，尤度が低すぎるなどの要素から追跡対象を失った状態を検出してアラートを出す機構を開発している．↩︎\nファイナンスにおける確率的ボラティリティモデルなどの例が挙げられている (Creal, 2011) p.256．↩︎\n粒子フィルターの経済学での応用が増えたきっかけが Fernández-Villaverde and Rubio-Ramírez (2005, 2007) による（小規模な）DSGEモデルの推定への応用だった (Creal, 2011, p. 246)．(矢野浩一, 2014) は実物景気循環モデル (Real Business Cycles Model) への応用を解説している．↩︎\n(Crisan and Doucet, 2002) ではすでに SMC とも呼ばれることが記されている．(Chopin and Papaspiliopoulos, 2020) 第3章にSMCのフィルタリング以外の多くの応用が紹介されている．↩︎\n(Behrens et al., 2012) はいずれも用いている．(Hamze and de Freitas, 2005) に tempered distribution の語用法がある，↩︎\n(Chopin and Papaspiliopoulos, 2020, pp. 3.4節 p.30)↩︎\n疾病マッピングは疾病の空間的な分布を把握すること，画像解析とは画像データから特徴を抽出してその意味論を理解することを指す．↩︎\n(Iba, 2001), (Kremer and Binder, 1988) などが良いレビューを提供している．(Assaraf et al., 2000) が量子系のシミュレーションの文脈で．↩︎\ndiffusion Monte Carlo, projector Monte Carlo などと呼ばれる手法に等しい．Green’s function Monte Carlo もポテンシャル \\(G\\) の取り方が違うのみである (Assaraf et al., 2000)．また (Iba, 2001) 3.1節 p.282 にも言及がある．↩︎\n(Iba, 2001) 3.1節 p.281．↩︎\n(矢野浩一, 2014) p.190，(Ristic et al., 2004) 前文 p.xi．↩︎\n(Chopin and Papaspiliopoulos, 2020, pp. 19.1節 p.371), (Creal, 2011, pp. 2.5.1節 p.258)．↩︎\n(Del Moral and Doucet, 2014) など．↩︎\n(Iba, 2001) などでは population Monte Carlo と呼ばれている．↩︎\nそのアイデアは (Doucet et al., 2001, pp. 79–95) 第4章 から．↩︎\n(Crisan and Doucet, 2002, p. 739) も参照．↩︎\n探知前追跡とは，信号が弱い，またはノイズが強い環境下において，信頼のおける初期信号を頼りにせずとも，物体追跡を実行するための手法．↩︎\n(Ristic et al., 2004, p. 288) に示唆されている．↩︎\n(Del Moral and Horton, 2023) は ensemble Kalman particle filtering methodology と呼んでいる．↩︎\n過去の記事でも触れた．↩︎"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html",
    "href": "posts/2024/Computation/MCMC.html",
    "title": "新時代の MCMC を迎えるために",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#導入",
    "href": "posts/2024/Computation/MCMC.html#導入",
    "title": "新時代の MCMC を迎えるために",
    "section": "1 導入",
    "text": "1 導入\n\n1.1 MCMC 小史\n現状，HMC (Hamiltonian Monte Carlo) という約 40 年前に提案された MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．1\nこの手法はもともと (Duane ほか, 1987) が場の量子論に特化した Monte Carlo 法として提案したものであったところを，(Neal, 1994) が一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．\nということで，HMC は，オリジナルの MCMC が物理学者 (Metropolis ほか, 1953) に由るように，物理学において着想された MCMC 手法であったのです．\nそのHMC が，提案から 40 年目を迎える前に，更なる効率的な手法によって代替されようとしています．\nそのきっかけ (Peters と de With, 2012) も，やはり，物理学（正確には物質科学）からの着想でした．\n\n\n1.2 MCMC とは何か？\nMCMC とは，確率変数をシミュレーションする際に用いられる汎用的アルゴリズムです．\n一様分布や正規分布などの名前がついた分布ではない場合，どのようにすればその分布に従う確率変数をシミュレーションできるのか？は，古くからの問題でした．\n実際，「MCMC では空間を探索するマルコフ連鎖を構成し，その足跡を辿るとちょうど確率変数のシミュレーションになっている」と種明かしを聞いても，「なぜそのような回りくどい方法を使うのか？」「もっと良い方法はないのか？」と思っても当然でしょう．\nですが，MCMC を，発明された経緯を辿り，物理学の問題意識から見てみると，実は極めて自然な発想に思えてくるかもしれません．\n以降，MCMC の起源である物理系のシミュレーション（第 2 節）を例に取り，分子動力学法（第 2.1 節），Metropolis 法（第 2.2 節）を復習します．\n\n\n\n\n\n\n\n\n\n分子動力学法の出力（第 2.1 節）2\n\n\n\n\n\n\n\nMetropolis 法の出力（第 2.2 節）\n\n\n\n\n\nこれを基礎として，近年提案された非対称な MCMC 手法（第 3 章），そして最新の連続時間 MCMC 手法（第 4 章）を紹介します．\n\n\n\n\n\n\n\n\n\n非対称 MCMC 法（Lifted Metropolis 法）の出力（第 3 章）\n\n\n\n\n\n\n\n連続時間 MCMC 法（Zig-Zag サンプラー）の出力（第 4 章）\n\n\n\n\n\n\n\n1.3 自己相関・軌跡の一覧\n\n\n\n\n\n\n\n\n\nMetropolis 法の自己相関関数（第 2.2 節）\n\n\n\n\n\n\n\n非対称 MCMC 法（Lifted Metropolis 法）の自己相関関数（第 3 章）\n\n\n\n\n\n\n\n連続時間 MCMC 法（Zig-Zag サンプラー）の自己相関関数（第 4 章）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetropolis 法の軌跡（第 2.2 節）\n\n\n\n\n\n\n\n非対称 MCMC 法（Lifted Metropolis 法）の軌跡（第 3 章）\n\n\n\n\n\n\n\n連続時間 MCMC 法（Zig-Zag サンプラー）の軌跡（第 4 章）"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#sec-origin",
    "href": "posts/2024/Computation/MCMC.html#sec-origin",
    "title": "新時代の MCMC を迎えるために",
    "section": "2 MCMC の起源",
    "text": "2 MCMC の起源\n\n\n\n\n\n\nよりみち：どうして MCMC が必要だったのか？\n\n\n\n\n\n(Metropolis ほか, 1953) では，温度 \\(T\\) 一定の条件下で \\(N\\) 粒子系をシミュレートし，任意の物理量 \\(F\\) に対してその相空間上の平均 \\[\n\\langle F\\rangle=\\frac{\\int Fe^{-\\frac{E}{kT}}dp}{\\int e^{-\\frac{E}{kT}}dp}\n\\] を効率的に計算する汎用アルゴリズムが提案された．これが現在では Metropolis 法と呼ばれている．3\n(Metropolis ほか, 1953) では \\(N\\) が数百になる場合を考えており（時代を感じるスケール感），当然愚直な数値積分は現代の計算機でも実行可能ではない．そこで Monte Carlo 法を考えることになるが，当時 Monte Carlo 法といえば，一様乱数を用いた計算法の全般を指し，具体的には \\(\\langle F\\rangle\\) を重点サンプリング推定量 \\[\n\\widehat{F}=\\frac{\\sum_{n=1}^NF(\\omega)e^{-\\frac{E(\\omega)}{kT}}}{\\sum_{n=1}^Ne^{-\\frac{E(\\omega)}{kT}}}\n\\] で推定することを指した．4\nしかしこれでは，高エネルギーな状態・低エネルギーな状態を全く区別せず，状態 \\(\\omega\\in\\Omega\\) を完全に一様に生成するため，その分だけ非効率である．\nこれを低減することが出来れば Monte Carlo 法の更なる効率改善に繋がる．こうして，Gibbs 分布 \\(\\frac{1}{Z}e^{-\\frac{E}{kT}}\\) から直接的サンプリングする方法が模索されたのである．\n\n\n\n(Metropolis ほか, 1953) では，エネルギー \\(E\\) を持つ系の Boltzmann-Gibbs 分布 \\(\\frac{1}{Z}e^{-\\frac{E}{kT}}\\) から直接サンプリングする方法が探求されました．\nここでは簡単のため，１粒子が次のようなポテンシャル \\[\nU(x)=\\frac{x^2}{2}+\\frac{x^4}{4}\n\\] に従って運動する場合を考えましょう：\n\n\n\n\n\n\n図 1: ポテンシャル \\(U\\) のプロット\n\n\n\nこのポテンシャルに関する Boltzmann-Gibbs 分布 \\(\\pi\\,\\propto\\,e^{-\\beta U}\\) は次のような形になります：5\n\n\n\n\n\n\n図 2: ポテンシャル \\(U\\) が定める Botlzmann-Gibbs 分布のプロット\n\n\n\n\n２つのプロットを見比べると，低エネルギー状態ほど出現確率が高く，エネルギーが上がるにつれて急激に出現確率が下がることがわかります．以降，\\(\\beta=1\\) としましょう．6\n\n2.1 分子動力学法\n統計力学によれば，\\(\\beta=1\\) で定まる温度とポテンシャル \\(U\\) を持つ Boltzmann-Gibbs 分布 \\(e^{-U}\\) は，温度 \\(T=\\frac{1}{k_B\\beta}\\) を持つ熱浴に接している力学系を，長時間シミュレーションして時間平均を取ることでサンプリングできるはずです．\nこのように，力学に基づいて物理過程を数値シミュレーションをすることを通じてサンプリングを達成する方法を 分子動力学法 といいます．\nこれを実際にやってみます．図 1 で定めたポテンシャルを持つ粒子を考えます．7\n続いてこれを温度 \\(T=\\frac{1}{k_B\\beta}\\) を持つ熱浴と相互作用させます．例えば，ポテンシャル 1 の \\(x=0\\) の位置に半透性の壁を置き，確率 \\(1/2\\) でこの温度 \\(T\\) の壁の粒子と弾性衝突するとします．（残りの確率 \\(1/2\\) では衝突せずに通過する）．\n壁の粒子の速度は Maxwell の境界条件から与えられるものとすれば，次のようにして粒子の位置 \\(x\\) がシミュレートできます：8\n\n\nコード\nimport numpy as np\n\nnp.random.seed(2024)\n\ndef U(x):\n    return x**2/2 + x**4/4\n\ndef pi(x):\n    return np.exp(-U(x))\n\ndef md(num_samples, initial_state, initial_velocity, timestep=0.1):\n    samples = [initial_state]\n    current_state = initial_state\n    current_velocity = initial_velocity\n    current_time = 0\n\n    for _ in range(num_samples - 1):\n        proposed_state = current_state + current_velocity * timestep\n        current_time += timestep\n        if current_state * proposed_state &lt; 0 and np.random.rand() &lt; 1/2:\n            current_velocity = (-1) * np.sign(current_velocity) * np.sqrt(-2 * np.log(np.random.rand() + 1e-7))\n        else:\n            current_velocity = current_velocity - ( current_state + current_state ** 3 ) * timestep\n            current_state = proposed_state\n        samples.append(current_state)\n\n    return np.array(samples)\n\n# サンプル数と初期条件を固定\nnum_samples = 10000\ninitial_state = 0.0\ninitial_velocity = 1.0\n\nsamples_MD = md(num_samples * 10, initial_state, initial_velocity, timestep=0.01)\n\n\n\n\n\n\n\n\n\n\n図 3: 分子動力学法からのサンプル\n\n\n\n\n\nこの方法は極めて収束が遅く，イテレーション数を \\(10^6\\) 以上に取らないと目標分布 \\(e^{-U}\\) の良い近似とならないことを思い知りました（上図も \\(10^6\\) サンプルで生成しています）．なお，以降の MCMC 法ではいずれもイテレーション数は一桁少ない \\(10^5\\) としています．\n\n\n\n\n\n\n\n\n\nたしかに，目標分布 \\(e^{-U}\\) に収束しそうですね．\n\n\n2.2 Metropolis 法\nもちろん，分布 \\(e^{-U}\\) をサンプリングするために，必ずしも背景にある物理過程まで戻ってシミュレーションをする必要はありません．\nそこで，シミュレーションは簡単なランダムウォークで行い，その結果を適切に修正することで目標分布に収束させる方法が (Metropolis ほか, 1953) で考えられました．\n(Metropolis ほか, 1953) の手法は，現在では random walk Metropolis-Hastings 法と呼ばれます．\nこの背後の物理現象から離陸する一歩が，分子動力学法と MCMC 法とを分けるものでした．\n\n\nコード\ndef metropolis(num_samples, initial_state, verbose=False):\n    samples = [initial_state]\n    current_state = initial_state\n\n    accept = []\n\n    for _ in range(num_samples - 1):\n        proposed_state = current_state + np.random.uniform(-2,2)\n        acceptance_ratio = pi(proposed_state) / pi(current_state)\n        if np.random.rand() &lt; acceptance_ratio:\n            current_state = proposed_state\n            accept.append(True)\n        samples.append(current_state)\n\n    if verbose:\n        rate = len(accept) / num_samples\n        print(f'acceptance rate : {rate}')\n\n    return np.array(samples)\n\n\n\n\n\n\n\n\n\n\n図 4: Metropolis 法からのサンプル\n\n\n\n\n\nサンプル数は分子動力学法の \\(1/10\\) であるにも拘らず，目標分布 \\(e^{-U}\\) の良い近似を得ています．\n一般に，MCMC からのサンプルの質の良さは，自己相関関数 を見ることで評価できます．9\nMetropolis 法の自己相関関数を計算してみると，横軸の Lag が大きくなればなるほど Autocorrelation の値は小さくなっています．\n\n\n\n\n\n\n\n\n図 5: Metropolis 法の自己相関関数\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 6: Metropolis 法の軌跡\n\n\n\n\n\n上図は Metropolis 法で構成される Markov 連鎖の軌跡を表しています．行ったり来たりしているのがわかります．棄却率は５割弱です．10\n\n\n2.3 統計学への応用\nこうして MCMC が発明されれば，すぐにイノベーションとして理解されたかというとそうではありませんでした．\nこの Metropolis の手法が極めて賢いシミュレーション手法であることは一目瞭然でも，一般の確率分布からのサンプリングに使える汎用アルゴリズムになっているという抽象的な観点が得られるまでには時間を要しました．これを成し遂げたのが (Hastings, 1970) でした．11\nさらに，Hastings のこの結果も見過ごされたと言って良いでしょう．真にMCMC を統計学界隈に広め，現代におけるベイズ統計学の興隆の契機となったのは階層モデリングにおける Gibbs サンプリングの有用性を強調した (Gelfand と Smith, 1990) だと言われます．12\n当時，代替手法としては複雑な数値アルゴリズムしかなかったベイズ統計学において，MCMC は汎用的で実装も容易であることが周知され，ベイズ統計学が普及するきっかけとなりました．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#sec-LMH",
    "href": "posts/2024/Computation/MCMC.html#sec-LMH",
    "title": "新時代の MCMC を迎えるために",
    "section": "3 非対称化への試み",
    "text": "3 非対称化への試み\n\n3.1 対称性という制約\nここでもう一度 Metropolis 法の軌跡 図 6 を見てみましょう．\n\n\n\n図 6 Metropolis 法の軌跡\n\n\n最初の 50 サンプルしか表示していませんから，運が悪いとうまく見つからないかもしれませんが，「一度歩んだルートを，その後すぐに逆に戻ってしまう」という事象が発生しやすいことが観察できますでしょうか？\nこれを 対称性 (reversibility) または 可逆性 と言います．\nMetropolis 法は構成上，この対称性を持つことが必要ですが，対称であるが故に一箇所に長時間とどまってしまうことが多くなります．\nその結果，対象分布が複雑で多峰性を持つ場合は，もっといろんなモード（峰）からもサンプリングをしてほしいのに，長時間１つの峰から離れられずにいることがあります．\nコーヒーに砂糖を溶かすことを考えてみましょう．砂糖の粒が拡散するのに任せておくと，最終的には均一に溶けるでしょうが，莫大な時間がかかります．スプーンで混ぜるなどして，砂糖が元の場所にとどまらずに移動し続けるようにすれば，はるかに速く平衡状態に到達できるでしょう．\nこれが 非対称化 のアイデアです．数ある Metropolis 法の改良の方向の中でも，この対称性を破るという試みは特に注目されてきました．\n\n\n3.2 リフティング\nMetropolis 法を非対称化するアプローチに，リフティング (Chen ほか, 1999) と呼ばれる方法があります．\nこれは，元々の状態空間を２つの「モード」\\(+1\\) と \\(-1\\) に分裂させ，\\(+1\\) のモードではひたすら右側に，\\(-1\\) のモードではひたすら左側に移動するようする方法です．\n２つのモード \\(+1,-1\\) の間を遷移する確率を調整することで，最終的な不変分布は変わらないようにすることができます．\nこうすることで，対称性を破り，一度「この方向に行く！」と決めたら行き続けるようにしながら，収束先は変わらないように変更することが出来るのです．\n実際に Metropolis 法に適用した Lifted Metropolis-Hastings 法 (Turitsyn ほか, 2011) を実装してみましょう：\n\n\nコード\ndef lifted_metropolis(num_samples, initial_state, verbose=False):\n    samples = [initial_state]\n    current_state = initial_state\n    lifting_variable = 1\n    accept = []\n\n    for _ in range(num_samples - 1):\n        delta = np.random.uniform(0,2)\n        proposed_state = current_state + lifting_variable * delta\n        acceptance_ratio = pi(proposed_state) / pi(current_state)\n\n        if np.random.rand() &lt; acceptance_ratio:\n            current_state = proposed_state\n            accept.append(True)\n        else:\n            lifting_variable = (-1) * lifting_variable\n\n        samples.append(current_state)\n    \n    if verbose:\n        rate = len(accept) / num_samples\n        print(f'acceptance rate : {rate}')\n\n    return np.array(samples)\n\n\n新しく追加されたリフティング変数 \\(\\sigma\\in\\{\\pm1\\}\\) に依存して，\\(\\sigma=+1\\) の場合には右方向に，\\(\\sigma=-1\\) の場合は左方向にしか提案を出さない MH 法と見れます．\n\n\n\n\n\n\n\n\n図 7: 非対称 Metropolis 法からのサンプル\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 8: 非対称 Metropolis 法の自己相関関数\n\n\n\n\n\n自己相関関数を見ると，Metropolis 法よりも急速に減衰していることがわかります．むしろ，過減衰のように自己相関関数が負になっていることもあります．\nこれは，一度「この方向に行く！」と決めたら行き続けるように設計したために，正の値が出たしばらくあとは負の値が，負の値が出たしばらくあとは正の値が出やすいようになってしまっているためです．\nしたがってこれは１次元の分布を考えていることに起因するため，殊更問題とすべきではないでしょう．\n\n\n\n\n\n\n\n\n図 9: 非対称 Metropolis 法の軌跡\n\n\n\n\n\n\n\n3.3 リフティングの有用性\n今回のような単純なポテンシャル \\(U\\) （図 1） だけでなく，統計力学における磁性体のモデルである Curie-Weiss モデルのハミルトニアン\n\\[\nH_n(x)=-\\frac{d\\beta}{2n}\\sum_{i,j=1}^nx_ix_j-h\\sum_{i=1}^nx_i,\n\\] \\[\nh\\in\\mathbb{R},x\\in\\{\\pm1\\}^n,\\quad n,d=1,2,\\cdots\n\\]\nが定める Boltzmann-Gibbs 分布 \\(e^{-H}\\) に対する Lifted Metropolis-Hastings も，単純な Metropolis-Hastings 法よりも効率的であることが知られています．13\n具体的には，モデルのパラメータ数 \\(n\\) に対して，緩和時間を \\(\\sqrt{n}\\) のオーダーだけ改善することが，(Turitsyn ほか, 2011) では数値実験で，(Bierkens と Roberts, 2017) では理論的に検証されているのです．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#sec-PDMP",
    "href": "posts/2024/Computation/MCMC.html#sec-PDMP",
    "title": "新時代の MCMC を迎えるために",
    "section": "4 新たな MCMC",
    "text": "4 新たな MCMC\n\n\n4.1 背後の物理現象からの更なる離陸\n第 2 章で，分子動力学法（第 2.1 節）から，提案分布を背後の物理現象とは全く関係ないランダムウォークとすることで，Metropolis 法（第 2.2 節）は一気に効率的なサンプリング法となったことを見ました．\nしかし，Metropolis 法はまだ思考が物理に引っ張られているのかも知れません．平衡統計力学において，ミクロの状態は等価で，ミクロなダイナミクスは可逆と考えられます．その前提が，知らず知らずのうちにまだ埋め込まれたままだと言えるでしょう．\nそこで，スプーンでかき混ぜるように，遷移を非対称にすることで，より効率的なサンプリング法となることを前章 3 で見ました．\nここでは，さらに暗黙の思い込みから解き放たれようとします．それは，シミュレーションするにあたって，必ずしも離散時間ステップに囚われる必要はない ということです．\nもう一度，Lifted Metropolis-Hastings 法の軌跡を見てみましょう：\n\n\n\n図 9 非対称 Metropolis 法の軌跡\n\n\nこの軌跡から得られる情報のほとんどは，「どこで折り返したか？」です．\nですから，この軌跡をシミュレーションするにあたって，一歩一歩採択-棄却のステップを繰り返す必要はなく，「どこで折り返すか？」を先に計算できてしまえば，あとは好きなステップサイズで切り出してサンプルとすれば良いのです．\n実は，「折り返す地点だけを効率的に計算する」ことが可能であり，それが 連続時間 MCMC のアイデアです．\n\n\n4.2 連続時間 MCMC\nLifted Metropolis-Hastings の適切な連続時間極限 \\(\\Delta t\\to0\\) を考えることで，「折り返す」という事象（が起こった回数）は Poisson 過程に従うことが導けます．\nすると，「折り返す」事象が起こるまでの待ち時間 (interarrival time) は指数分布に従うことがわかります．これに基づいて，「折り返す」事象が起こる時刻を計算し，そこまでの軌跡を直線で補間すれば，Lifted Metropolis-Hastings 法（の連続時間極限）の軌跡が模倣できることになります．\n最終的に得られる過程は，ランダムな時刻に「折り返す」事象が起こり，その間は確定的な動き（等速直線運動）をするというもので，このような過程を 区分確定的 Markov 過程 (PDMP: Piecewise Deterministic Markov Process) と呼びます．\nこのような PDMP は，Lifted Metropolis-Hastings 以外にも種々の MCMC 法の極限から見つかっており，その中でも特に有名なのが次の Zig-Zag sampler です：\n\n\nコード\nimport math\n\ndef zigzag(num_samples, initial_state, step=1):\n    samples = [initial_state]\n    trajectory = [initial_state]\n    current_state = initial_state\n    lifting_variable = 1\n    t = 0\n\n    while t &lt; num_samples * step:\n        state_event = lifting_variable * np.sqrt(-1 + np.sqrt( 1 - 4 * np.log(np.random.rand()) ))\n        t_event = t + np.abs(state_event - current_state)\n        for _ in np.arange(np.ceil(t/step)*step, np.ceil(t_event/step)*step, step):\n              samples.append(current_state + lifting_variable * (_ - t))\n        current_state = state_event\n        trajectory.append(current_state)\n        lifting_variable = (-1) * lifting_variable\n        t = t_event\n\n    return np.array(samples), np.array(trajectory)\n\n\n\n\n\nコード\nsamples_zigzag, trajectory_zigzag = zigzag(num_samples, initial_state, step=2)\nplt.figure(figsize=(3.5, 3))\nplt.hist(samples_zigzag, bins=50, density=True, alpha=0.7, color=minty)\nplt.show()\n\n\n\n\n\n\n\n\n図 10: Zig-Zag サンプラーからのサンプル\n\n\n\n\n\n\n\n\n\n\n\n\n\n図 11: Zig-Zag サンプラーの自己相関関数\n\n\n\n\n\n自己相関関数は，Lifted Metropolis-Hastings 法と同様に急激に下がって負の値に突き抜けたあとは，少し振動が残っているのがわかります．\n全３サンプラーの比較は第 1.3 節をご覧ください．\n次の軌跡を見て分かる通り，モードである \\(x=0\\) を中心に激しく往復するので，直後のサンプルとは負の相関が出やすいようです．\n\n\n\n\n\n\n\n\n図 12: Zig-Zag サンプラーの軌跡\n\n\n\n\n\n連続時間極限 \\(\\Delta t\\to0\\) をとっているということは，「極めて小さいステップサイズでの random walk Metropolis 法（第 2.2 節）」に相当します．従って，一度折り返したら，原点 \\(x=0\\) を超えるまでは絶対に棄却されません．\nそのため，このように往復するような軌跡が得られます．\n\n\n4.3 連続時間 MCMC の美点\n前節では，必ずしも PDMP 法である Zig-Zag sampler が，Lifted Metropolis-Hastings 法より，自己相関関数の観点で良いとは言い切れないことを見ました．\nしかし，今回の設定は１次元という特殊な条件下であることを考慮に入れる必要があります．\n１次元なので Zig-Zag サンプラーは行き来することしか出来ていませんが，14 ２次元以上，特に高次元の場合は，Zig-Zag サンプラーは極めて効率的に状態空間を探索できることが期待されます．\n例えば，標準正規分布に対する２次元での軌跡は次の通りです：\n\n\n\n\n\n\n図 13: Zig-Zag Sampler in \\(\\mathbb{R}^2\\)\n\n\n\nそして何より，軌道が効率的な空間の探索に有利であるだけでなく，正確なサブサンプリングを取り入れることが可能です．すなわち，ほとんどの他手法と違って，バイアスを導入することなく，データの一部のみを用いてアルゴリズムを走らせることができます．\nしたがって，従来の MCMC 法が採択-棄却のステップにおいて尤度を評価する必要があり，データサイズ \\(n\\) に対して \\(O(n)\\) の計算量を要するのに対して，\\(O(n)\\) に比例する焼き入れのステップを除けば，\\(O(1)\\) の複雑でほとんど i.i.d. なサンプルを得ることができます (Bierkens ほか, 2019)．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#終わりに",
    "href": "posts/2024/Computation/MCMC.html#終わりに",
    "title": "新時代の MCMC を迎えるために",
    "section": "5 終わりに",
    "text": "5 終わりに\n\nこうみると，MCMC は物理学の問題意識から生まれた手法でありながら，背後の物理現象を模倣することから離れていくことで，計算手法としての効率を高めていく一途を辿っていることがわかります．\nそう見ると，新時代の大規模データと大規模モデルが課す MCMC の次なる脱皮は，連続時間 MCMC で間違いないような気がしてくるのですが……．まだ筆者にははっきりとは見えてきません．\nまた本稿では１つの流れしか取り上げておらず，例えば HMC とその非対称化がどのような位置づけにあるかもまだ考慮中です．\n統計力学，統計学，機械学習が交差するなんとも面白いテーマです．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#参考文献",
    "href": "posts/2024/Computation/MCMC.html#参考文献",
    "title": "新時代の MCMC を迎えるために",
    "section": "6 参考文献",
    "text": "6 参考文献\n\n本稿では，用いたコードの導出を一切触れませんでした．これについては，文献 (Tartero と Krauth, 2024) をご参照ください．非調和振動子を系にとり，正準集団とみなすことで，分子動力学法，メトロポリス法からそのリフティングまで，種々のサンプラーを同じ題材で比較するアイデアをもらいました．こんなにわかりやすい解析ができるのかと心底驚きました．\n続いて，Metropolis-Hastings 法 → 非対称 MCMC → 連続時間 MCMC という発展の過程を，背後の物理過程の模倣からの離陸という視点で統一的に捉えることが出来るということは，(Turitsyn ほか, 2011) の魅力的なイントロダクションで気付かされました．本文献はリフティングによる MCMC の非可逆化を抽象的に定式化して数値実験で検証したものであり，「ねじれ詳細釣り合い条件」を導入した点で，アイデアの宝庫といえます．\nリフティングによる Metropolis 法の非対称化について，(酒井佑士, 2017) は貴重な日本語文献です．当該文献の第３章（の第２節）にここで紹介した内容が詳しくまとめられています．\n第 4.3 節で紹介しましたように，Zig-Zag サンプラーを用いたサンプリングではそのスケーラビリティが魅力です．この点については，Zig-Zag サンプラーが提案された論文 (Bierkens ほか, 2019) でも，前面に押し出して解説されています．"
  },
  {
    "objectID": "posts/2024/Computation/MCMC.html#footnotes",
    "href": "posts/2024/Computation/MCMC.html#footnotes",
    "title": "新時代の MCMC を迎えるために",
    "section": "脚注",
    "text": "脚注\n\n\nHamiltonian Monte Carlo の名称は (Neal2011-HMC?) からで，元々は Hybrid Monte Carlo と呼ばれていました．分子動力学法 (Molecular Dynamics) と MCMC のハイブリッド，という意味でした．Stan で実装されている MCMC アルゴリズムについては こちら を参照．↩︎\n特に収束が遅く，他の手法と比べてイテレーション数を 10　倍にしています．↩︎\n統計学界隈では (Hastings, 1970) を入れて，Metropolis-Hastings 法とも呼ばれる．↩︎\nただし，配置 \\(\\omega\\in\\Omega\\) は空間内にランダム（一様）に粒子 \\(N\\) 個を配置することで生成することとする．↩︎\n１粒子系なので相互作用はなく，\\(E=U\\)．↩︎\n\\(\\beta=1\\) と約束することは，系の温度を \\(T=k_B^{-1}\\) に固定することにあたります．↩︎\n図 1 で定めたポテンシャルを持つ力学系には，代表的なものは（非調和）振動子や，あるいは \\(U\\) の形をした谷を行ったり来たりするボールを考えても構いません．↩︎\n詳しい議論は (Tartero と Krauth, 2024) をご参照ください．大変教育的な入門です．↩︎\n自己相関関数が大きいほど，その Markov 連鎖を用いて構成した Monte Carlo 推定量の漸近分散が大きくなります．加えて，自己相関関数の裾が重すぎると，例えエルゴード性を持っており大数の法則が成り立とうとも，中心極限定理が成り立たなくなります．換言すれば，\\(n^{-1/2}\\) よりも遅い収束レートになってしまいます．↩︎\n一般には，ランダムウォーク MH 法において，採択率を \\(0.2\\le\\alpha\\le0.4\\) 前後に抑えるのが良いとされています (Roberts ほか, 1997)．これは状態空間の次元が無限に漸近する設定下での，漸近論的な結果ですが，低次元の場合でも極めて良い指標になることが (Gelman ほか, 1996) で実証されています．また今回も，１次元であるにも拘らず，たしかに棄却率が半分を越さないほうが，自己相関が小さくなる傾向が確認されました．しかし今回は対象分布の裾が極めて軽いので，あまり大きなムーブは要らず，ステップサイズの最大値を \\(2\\)，採択率は \\(0.5\\) 近くにしました．他の手法，LMH と Zig-Zag もステップサイズの最大値が \\(2\\) になるように統一しました．↩︎\n“While (Metropolis ほか, 1953) proposed the use of MCMC sampling to compute particular integrals in statistical mechanics, it was the Hastings paper that elevated the concept to a general one, and introduced it to the broader statistics community.” (Martin+2023-history?) 3.5節．↩︎\n(Martin+2023-history?) 4節，(Robert と Casella, 2011, p. 102) など．↩︎\nただし，\\(e^{-H}\\) が多峰性を示す低温領域では，LMH の方が効率的であるというはっきりとした理論保証はまだありません．↩︎\n今回は対象分布の減衰が極めて激しかったために差が現れにくかったのだと考えられます．Zig-Zag サンプラーは１次元でも，（広い設定の下で）（そして特に目標分布の裾が重いときに）ランダムウォーク・メトロポリス法や Metripolis-adjusted Langevin algorithm よりも速い収束レートを持ちます (Vasdekis と Roberts, 2022)，↩︎\n提案分布の．実際の軌跡の１ステップでの移動距離とは違う．↩︎"
  },
  {
    "objectID": "posts/2025/Posters/NewRegime.html#プレプリント",
    "href": "posts/2025/Posters/NewRegime.html#プレプリント",
    "title": "PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能",
    "section": "プレプリント",
    "text": "プレプリント\n\n\n\n\n\n\n\n\n\n\nPDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能になる\n\n\n\nPDMP\n\n\n\n\n2025-02-25\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "static/CV/cv.html#academic-history",
    "href": "static/CV/cv.html#academic-history",
    "title": "Hirofumi Shiba",
    "section": "Academic History",
    "text": "Academic History\n\nPh.D. in Statistical Science. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2023.4 – 2028.3\nSuperivsor: Kengo Kamatani\nB.A. in Mathematics. The University of Tokyo, Japan. 2019.4 – 2023.3\nSupervisor: Nakahiro Yoshida"
  },
  {
    "objectID": "static/CV/cv.html#research-visit",
    "href": "static/CV/cv.html#research-visit",
    "title": "Hirofumi Shiba",
    "section": "Research Visit",
    "text": "Research Visit\n\nDepartment of Statistical Science, University College London, United Kingdom. 2024.11.4 – 2024.12.2\nSupervisor: Alexandros Beskos\nDepartment of Statistics & Data Science, National University of Singapore. 2025.6.1 – 2025.6.30\nSupervisor: Alexandre Thiéry\nInstitute of Stochastics, Ulm University, Germany. 2025.7.13 – 2025.7.26\nSupervisor: Evgeny Spodarev"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#経歴",
    "href": "static/CV/cv_Japanese.html#経歴",
    "title": "司馬博文（しばひろふみ）",
    "section": "経歴",
    "text": "経歴\n\n博士（統計科学）. 総合研究大学院大学先端学術院統計科学コース. 2023.4 – 2028.3\n指導教員：鎌谷研吾教授\n学士（理学）. 東京大学理学部数学科. 2019.4 – 2023.3\n指導教員：吉田朋広教授"
  },
  {
    "objectID": "static/CV/cv_Japanese.html#受賞",
    "href": "static/CV/cv_Japanese.html#受賞",
    "title": "司馬博文（しばひろふみ）",
    "section": "受賞",
    "text": "受賞\n\n優秀発表賞，統計検定センター長賞．第19回日本統計学会春季集会．2025.3.8"
  },
  {
    "objectID": "static/CV/cv.html#awards",
    "href": "static/CV/cv.html#awards",
    "title": "Hirofumi Shiba",
    "section": "Awards",
    "text": "Awards\n\nBest Poster Award, 19th Spring Meeting of the Japan Statistical Society. 2025.3.8\nDirector’s Award from the Japan Statistical Society Certificate. 2025.3.8"
  },
  {
    "objectID": "posts/2025/DiffusionModels/DiscreteDiffusion.html",
    "href": "posts/2025/DiffusionModels/DiscreteDiffusion.html",
    "title": "離散空間上の拡散確率モデル",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/DiffusionModels/DiscreteDiffusion.html#sec-D3PM",
    "href": "posts/2025/DiffusionModels/DiscreteDiffusion.html#sec-D3PM",
    "title": "離散空間上の拡散確率モデル",
    "section": "1 離散雑音除去拡散模型 (D3PM) (Austin et al., 2021)",
    "text": "1 離散雑音除去拡散模型 (D3PM) (Austin et al., 2021)\n\n\n\n量子化された画像データに対する離散拡散モデル．Minimal Implementation of a D3PM by Simo Ryu (Ryu, 2024) (Tap to image to visit his repository)\n\n\n\n1.1 はじめに\n拡散モデルは画像 (Ho et al., 2020), (Y. Song and Ermon, 2020) と音声 (Kong et al., 2021), (Chen et al., 2021) の分野で大成功を収めたが，そのアプローチは連続空間上の拡散過程の離散近似を実装するというものに限られていた．\n最初の拡散モデルの提案 (Sohl-Dickstein et al., 2015) では，拡散過程の他に，\\(2=\\{0,1\\}\\) 空間上での二項分布を用いたモデルも考慮されていた．\n離散データ（言語と画像 segmentation のラベル）に対するフローベースのサンプリング法として，Argmax Flows と Multinomial Diffusion が (Hoogeboom et al., 2021) により提案されたが，state-of-the-art に迫るモデルが得られるわけではなかった．\n画像や音声も本質的には離散的であるとはいえ，積極的にそのようにみなす利点があるとは思う者は少なかっただろう．\nしかし離散空間上では遷移核 \\(P_\\theta,Q\\) の分布形を Gauss 分布に限るといった制約は必要なくなり，より柔軟なノイズ過程の設計が可能になる（第 1.4 節）．\nつまり，(Hoogeboom et al., 2021) はノイズ過程を空間一様な Markov 連鎖に限っていた点で離散拡散過程のポテンシャルを見誤っていたと言える．\n(Austin et al., 2021) は例えばトークン間の類似性を考慮に入れる（第 1.5.5 節）など，遷移核の柔軟な設計を可能にするという，離散拡散過程のポテンシャルを示した．\n(Austin et al., 2021) の提案した D3PM は，特に [MASK] トークンを特別に扱うことも可能になり，その場合は離散拡散モデルは BERT (Lewis et al., 2020) などのマスク付き言語モデルと等価になることも指摘している．その後，近年は (Arriola et al., 2025) など，自己回帰型のモデルと組み合わせたブロック拡散モデルを用いた，巨大な言語モデルの作成も試みられている．\n\n\n1.2 設定\n\n1.2.1 離散状態空間 \\([K]\\)\n離散状態空間を \\([K]:=\\{1,\\cdots,K\\}\\) (Kategorie) とし，この one-hot encoding の空間 \\[\n\\mathcal{K}:=\\{x\\in\\Delta^K\\mid\\exists_{i\\in[K]}\\;x_j=\\delta_{ij}\\}\n\\] \\[\n\\mathcal{P}([K])\\simeq\\Delta^K:=\\left\\{x\\in[0,1]^K\\,\\middle|\\,\\sum_{i=1}^Kx_i=1\\right\\}\n\\] と適宜同一視する．\n\\([K]\\) は特定の位置にあるトークンの値域の集合，または特定のピクセルにおける色の値域の集合などの抽象化である．1\n\\([K]\\) 上の確率分布は全てある確率ベクトル \\(p\\in\\Delta^K\\) に関する（カテゴリカル）分布 \\(\\mathrm{Cat}(p)\\in\\mathcal{P}([K])\\) として表せる．2\n\n\n1.2.2 \\([K]^L\\) 上の Markov 連鎖\nトークン列の長さ \\(L\\ge1\\) (Length) を固定し， \\[\nx_t=\\begin{pmatrix}x_t^1\\\\\\vdots\\\\x_t^L\\end{pmatrix}\\in E:=[K]^L\n\\] 上の Markov 過程 \\(\\{X_t\\}_{t\\in[0,1]}\\) を考える．\nこの遷移核 \\(Q^{s,t}:[K]\\to\\mathcal{P}([K])\\) は，第 \\(i\\) 行を \\(x_s=i\\) だった場合の次の時刻の状態 \\(x_t\\) の確率分布とする行列 \\(Q^{s,t}\\in M_K(\\mathbb{R})\\) と同一視して， \\[\n\\biggr(X_t|X_s=x_s\\biggl)\\sim\\mathrm{Cat}(x_sQ^{s,t})\n\\] と理解できる．\n\n\n1.2.3 約束\n以降，基本的にローマ字 \\(x\\) やギリシャ文字の小文字 \\(\\rho,\\mu\\) はベクトル（成分となるスカラーは上付き添字 \\(x^j\\) で表す）を表し，大文字 \\(P,Q\\) は（確率）行列を表す．\n\\(E=[K]^L\\) 上の座標や関数は縦ベクトルで，確率測度は横ベクトルで表す．\none-hot encoding には不思議な性質があり，\\(\\mathcal{V}\\)-値確率変数 \\(X_t\\) の平均をとると，ちょうどその分布の質量関数になる： \\[\n\\operatorname{E}_\\mathbb{Q}[X_t]=(\\rho Q^{0,t})^\\top.\n\\]\n\n\n\n1.3 枠組み\n\n1.3.1 動的定式化\nデータ分布を \\(\\rho\\in\\mathcal{P}([K])\\) とする．これを \\(\\{Q^{s,t}\\}\\) が定める Markov 過程によって輸送して得る終端の分布を \\[\n\\mu:=\\rho Q^{0,1}\\in\\mathcal{P}([K])\n\\] と定める．\n離散化点 \\(0=t_0&lt;t_1&lt;\\cdots&lt;t_N=1\\) を与えた下で，確率核の族 \\((Q^{s,t})_{s&lt;t\\in[0,1]}\\) は，見本道の空間 \\(E^{N+1}\\) 上に確率測度 \\(\\mathbb{Q}\\in\\mathcal{P}(E^{N+1})\\) を定める： \\[\n\\mathbb{Q}(dx_{0:1}):=\\bigotimes_{i=0}^N\\rho Q^{0,t_i}(dx_{t_i})\n\\]\nこの測度 \\(\\mathbb{Q}\\) と同じ分布を持つが，\\(X_1\\sim\\mu\\) から始まる時間反転過程を与える確率核 \\(P^{s,t}:[K]\\to\\mathcal{P}([K])\\) を，ニューラルネットワークで係数付けられたモデル \\(\\{P_\\theta\\}\\) の最尤推定により学習することを考える．\n変分推論の枠組みの下では，次の目的関数（損失関数）を最小にすることで達成される (Section 2.4 Sohl-Dickstein et al., 2015)： \\[\n\\mathcal{L}:=\\operatorname{E}_{\\mathbb{Q}}\\left[\\sum_{i=1}^N\\operatorname{KL}\\biggr(\\mathcal{L}[X_{t_{i-1}}|X_{t_i},X_0]\\,\\bigg|\\,\\mathcal{L}[P_\\theta^{t_i,t_{i-1}}X_{t_i}]\\biggl)\\right].\n\\tag{1}\\] ただし，上式は読みやすさのために特別な約束をしているものと考える．というのも，\\(i=1\\) の場合，KL 乖離度の項は \\(X_0\\) のモデリングの良さを表す重要な項となっているはずであるが，\\(X_0\\sim\\rho,X_{t_1}\\sim\\rho Q^{0,t_1}\\) が与えられている下では次の表示を持つことに注意： \\[\n\\operatorname{KL}\\biggr(\\mathcal{L}[X_0|X_{t_1},X_0]\\,\\bigg|\\,\\mathcal{L}[P_\\theta^{t_1,0}X_{t_1}]\\biggl)=\\operatorname{KL}\\biggr(\\delta_{X_0}\\,\\bigg|\\,\\mathcal{L}[P_\\theta^{t_1,0}X_{t_1}]\\biggl)\n\\] \\(\\delta_{X_0}\\ll \\mathcal{L}[P_\\theta^{t_1,0}X_{t_1}]\\) は成り立つとは限らない．その場合この項は通常 \\(\\infty\\) と定義されるが，それでは学習できないので，KL 乖離度と定数分の違いである 交差エントロピー の値であるとする： \\[\n\\operatorname{E}_\\mathbb{Q}\\left[\\operatorname{KL}\\biggr(\\delta_{X_0}\\,\\bigg|\\,\\mathcal{L}[P_\\theta^{t_1,0}X_{t_1}]\\biggl)\\right]=\\operatorname{E}_{\\rho Q^{0,t_1}}\\left[\\mathcal{H}\\biggr(\\rho\\,\\bigg|\\,\\mathcal{L}[P_\\theta^{t_1,0}X_{t_1}]\\biggl)\\right]=\\operatorname{E}_\\mathbb{Q}\\left[\\log p_\\theta(X_0|X_{t_1})\\right].\n\\]\nただし，\\(p_\\theta(x_0|x_{t_1})\\) は \\(P_\\theta^{t_1,0}\\) の質量関数であるとした．\n\n\n1.3.2 静的定式化\n以上の枠組みは全て，Markov 過程の見本道の空間 \\(E^{N+1}\\) 上で定式化された．実際，損失関数 (1) は道測度間の距離 \\(\\operatorname{KL}(\\mathbb{Q},\\mathbb{P}_\\theta)\\) を測っていると理解できる．\n一方で，\\(N=2\\) として，両端の分布 \\(\\rho,\\mu\\) しか考えなかった場合，\\(\\mathbb{P}_\\theta\\) の特定は一般に最適輸送問題として理解される．\n拡散模型や動的 Schrödinger 問題などは，ステップ数 \\(N\\) を増やすことによって逆に問題を解きやすくしているのである．\n\n\n\n1.4 ノイズ過程の設計\n上述の理論を実践に移すためには，次の点が重要である\n\n\n\n\n\n\n\n\\(\\mu=\\rho Q^{0,1}\\) の分布が既知であること．実際に訓練された \\(\\{P_\\theta\\}\\) を用いて \\(\\rho\\) からサンプリングをするには，\\(\\mu\\) からのサンプルが必要．\n\\(\\mathbb{Q}\\) がシミュレーション可能であること．目的関数 \\(\\mathcal{L}\\) はこれに関する積分として書かれているため，最適化の実行のためにはデータ \\(X_0\\sim\\rho\\) を \\(Q^{t_{i-1},t_i}\\) で次々と変換していく必要がある．\n条件付き分布 \\(\\mathcal{L}[X_{t_{i-1}}|X_{t_i},X_0]\\) が評価可能であること．これも目的関数 \\(\\mathcal{L}\\) の KL 乖離度の項に含まれており，これを計算するには最低限でも評価が可能である必要がある．\n\n\n\n\nであるとする．条件２と３により， \\[\nL_{t-1}(x_0):=\\int_\\mathcal{X}\\operatorname{KL}\\biggr(q(x_{t-1}|x_t,x_0),p_\\theta(x_{t-1}|x_t)\\biggl)\\,q(x_t|x_0)\\,dx_t\n\\]\nの Monte Carlo 近似が可能になる．\n連続空間の場合は，遷移核 \\(P_\\theta,Q\\) を Gauss に取ることが慣例になっている．この選択により \\(q(x_{t-1}|x_t,x_0)\\) も Gauss になるため，KL 乖離度は既知である．ただしこの選択は，\\(\\mu:=\\rho Q^{0,1}\\) が近似的にしか Gauss にならない点が問題になる．\nしかし離散空間の場合，\\(\\mu\\) を一様分布などに自由に設定しても，２と３はいつでも（効率的とは限らないかもしれないが）計算できる．3 遷移行列 \\(Q^{s,t}\\) をどのように難しい分布にとっても所詮は行列である．\nまた条件付き分布は，ベイズの定理より \\[\nq(x_{t-1}|x_t,x_0)=\\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\n\\] であるから，次のような行列演算によって計算できる： \\[\n\\mathcal{L}[X_{t_{i-1}}|X_{t_i},X_0]=\\mathrm{Cat}\\left(\\frac{X_{t_i}Q^{t_i,t_{i-1}}\\odot X_0Q^{0,t_{i-1}}}{X_0Q^{0,t_i}X_{t_i}^\\top}\\right).\n\\tag{2}\\] ただし \\(\\odot\\) は成分ごとの積（Hadamard 積）とした．\\(Q^{t_i,t_{i-1}}=(Q^{t_{i-1},t_i})^\\top\\) という関係に注意．\n\n\n1.5 核 \\(Q\\) の設計\n(Section 3.1 Austin et al., 2021) では次の４種類の枠組みに触れている．\n\n1.5.1 （空間）一様核 (uniform noise)\n(Sohl-Dickstein et al., 2015) での \\(K=2\\) の場合での定式化を一般化する形で，(Hoogeboom et al., 2021) は次のような遷移核を扱った： \\[\nQ^t:=(1-\\beta_t)I_K+\\frac{\\beta_t}{K}\\boldsymbol{1}_K\\boldsymbol{1}_K^\\top.\n\\] \\[\n\\therefore\\qquad Q^t_{ij}=\\begin{cases}\n1-\\frac{K-1}{K}\\beta_t&i=j,\\\\\n\\beta_t/K&i\\ne j.\n\\end{cases}\n\\]\nこれは \\(\\{\\beta_t\\}\\) の存在により時間一様ではないが，各状態を等価に扱っており，空間一様であると言える．その意味で \\([K]\\) 上に一才の事前構造を仮定していないと言える．\n\\(Q^t\\) は対称な確率行列であり，平衡分布は一様分布である．\n\n\n1.5.2 吸着付きの核 (absorbing-state noise)\nトークンの空間 \\([J]\\) のうち１つ \\(m\\in[J]\\) (MASK) を吸着点とすることを考える．\nこのときの \\(Q_t\\) は次のような形にする： \\[\nQ_t:=(1-\\beta_t)I_K+\\beta_t\\boldsymbol{1}_Ke_m^\\top\n\\] \\[\n(Q_t)_{ij}:=\\begin{cases}1&i=m=j,\\\\\n0&i=m\\ne j,\\\\\n1-\\beta_t&m\\ne i=j,\\\\\n\\beta_t&i\\ne m=j,\n\\end{cases}\n\\]\n\\(Q_t\\) は \\(\\{\\beta_t\\}\\) のスケジュールで \\(\\rho\\) を \\(\\delta_m\\) に収束させる．この逆過程を学習することで，[MASK] トークンの infilling が学習できる．\n\n\n1.5.3 正規分布様核\n拡散過程を参考にし，状態間の離散的な距離を考慮に入れた核を構成することも当然可能である．\n\\([K]\\) を順序数の集合と見た距離を用いるため，画像の RGB 値などに適する．\n\\[\nQ^t_{ij}=\\begin{cases}\nZ^{-1}\\exp\\left(-\\frac{4\\lvert i-j\\rvert^2}{(K-1)^2\\beta_t}\\right)&i\\ne j,\\\\\n1-\\sum_{l\\ne i}Q^t_{ij}&i=j.\n\\end{cases}\\qquad Z:=\\sum_{n=-(K-1)}^{K-1}\\exp\\left(-\\frac{4n^2}{(K-1)^2\\beta_t}\\right)\n\\] という核を用いると，\\(Z\\) の存在により \\(Q^t\\) は二重確率行列になっており，平衡分布は一様分布である．\n\n\n1.5.4 帯行列核\n他に \\([K]\\) の順序数としての構造を導入した核として \\(k&gt;0\\) を定めて \\(k\\)-近傍に一様に移動する核 \\[\nQ^t_{ij}=\\begin{cases}\n\\frac{\\beta_t}{K}&0&lt;\\lvert i-j\\rvert\\le k,\\\\\n1-\\sum_{l\\ne i}Q_{il}^t&i=j,\n\\end{cases}\n\\] を考えることもできる．\n\n\n1.5.5 距離学習核\n\\([K]\\) 内の自然な距離行列が，上流タスク，例えば言語の埋め込みで考えた \\(k\\)-最近傍隣接行列 \\[\nG_{ij}=1_{\\mathcal{N}_k(i)}(j),\\qquad\\mathcal{N}_k(i):=\\left\\{\\text{トークン}\\;i\\;\\text{の}\\;k\\text{-近傍トークン}\\right\\}\n\\] によって \\[\nA:=\\frac{G+G^\\top}{2}\\in S_K(\\mathbb{R})^+\n\\] などと与えられていることがある．\nこのとき， \\[\nQ_t:=e^{\\alpha_tR}\\qquad R_{ij}:=\\begin{cases}\nA_{ij}&i\\ne j,\\\\\n-\\sum_{l\\ne i}A_{il}&i=j,\n\\end{cases}\n\\] により遷移確率行列を定めることができる．\n\\(G\\) を連結グラフに取れば，\\(Q_t\\) はやはり二重確率行列であり，平衡分布は一様分布である．\nこの設計により，\\(K\\)-トークンの空間上の \\(k\\)-近傍グラフ \\(G\\) 上の乱歩により，一様な平衡分布 \\(\\mu=\\mathrm{U}([K])\\) に至るという描像がある．\n\n\n\n1.6 エルゴード性について\n\n1.6.1 概観\n確率行列 \\(\\{Q^{s,t}\\}\\) に関する制約として，有限時間内に既知の分布 \\(\\mu\\) に収束することが望ましい．\n正確には，最後のステップに確定的な遷移を加え，これの時間反転過程も含めて学習することにしても，撹拌性（初期値 \\(X_0=x_0\\) を忘れる性質）を持つことが望ましい．\n\n\n1.6.2 平衡分布\n\\(\\{Q^{t_{i-1},t_i}\\}\\subset M_K((0,1))\\) をすべて正行列に取れば既約性と非周期性が保たれる．4 \\(E\\) は有限であるため，これで直ちに指数エルゴード性が確保される．\n仮に \\(\\{t_i\\}_{i=1}^N\\) は等間隔で，\\(\\{Q^{t_{i-1},t_i}\\}\\) を一様に \\(Q^{t_{i-1},t_i}=Q^{t_i-t_{i-1}}\\) と取った場合，平衡分布は \\(Q^\\top\\) の固有値 \\(1\\) に属するただ１つの全ての成分が正の長さ１固有ベクトルになる．\n仮に時間一様でなくとも，正行列の族 \\(Q^{t_{i-1},t_i}\\) をさらに二重確率行列に取れば，平衡分布 \\(\\mu\\) は一様分布であることが確保される．\n\n\n1.6.3 ノイズスケジュール\n収束を速めるための一般的な慣行は，分散 \\(\\beta_t\\) を時間と共に大きくしていくことである．\n(Nichol and Dhariwal, 2021) は cosine schedule を導入しており，(Hoogeboom et al., 2021) や (Austin et al., 2021) の（空間）一様核でも用いられている．\n\n\n1.6.4 相互情報量スケジュール\n(Austin et al., 2021) は他にも，\\(X_0,X_t\\) の相互情報量を線型近似した \\[\n\\beta_t:=\\biggr(1-\\frac{t}{T}\\biggl)H(x_0)\n\\] というスケジュールを提案している．\nこれは (Sohl-Dickstein et al., 2015) が \\(K=2\\) の場合に提案したスケジュールである \\(\\beta_t=(T-t+1)^{-1}\\) という設定に，吸着核を用いた場合一致する．\n\n\n\n1.7 逆向き生成過程\n１ステップの遷移確率 \\(p_\\theta(x_{t-1}|x_t)\\) （のロジット）をモデリングするのではなく，時刻 \\(0\\) への遷移確率 \\(\\widetilde{p}_\\theta(x_0|x_t)\\) （のロジット）をモデリングし， \\[\np_\\theta(x_{t-1}|x_t)\\,\\propto\\,\\sum_{\\widetilde{x}_0\\in[K]}q(x_{t-1}|x_t,\\widetilde{x}_0)\\widetilde{p}_\\theta(\\widetilde{x}_0|x_t)\n\\tag{3}\\] は間接的にモデリングするのが (Austin et al., 2021), (Hoogeboom et al., 2021) の方法である．\nこれは連続の場合の (Ho et al., 2020) と異なるパラメータの入れ方である．\nこれは (2) で与えられる条件付き分布 \\(q(x_{t-1}|x_t,x_0)\\) と \\(p_\\theta(x_{t-1}|x_t)\\) との距離を最小化するという (1) の目的関数 \\(\\mathcal{L}\\) の構造をよく反映する．\n例えば \\(Q\\) としてスパースな遷移行列を取った場合，\\(q(x_t|x_{t-1})\\) が非零である場合に限って \\(q(x_{t-1}|x_t,x_0)\\) も非零になるが，上述のパラメトライゼーションは自然に \\(p_\\theta(x_{t-1}|x_t)\\) に同様のスパース構造を導入することになる．\nまたステップ数を小さく取った場合でも，\\(k\\) ステップをまとめて \\(p_\\theta(x_{t-k}|x_t)\\) をいきなりサンプリングするということも可能になるという美点がある，\n\n\n1.8 損失関数について\nこれは随分なスキャンダルであると思うのだが，(1) の目的関数 \\(\\mathcal{L}\\) を実際に訓練に用いている場合は少ない．\nそもそも (Ho et al., 2020) は各項の重要度を変えることで大胆な簡略化を行なった \\(\\mathcal{L}_{\\text{simple}}\\) を用いており，(Nichol and Dhariwal, 2021) は \\[\n\\mathcal{L}_{\\text{hybrid}}=\\mathcal{L}_{\\text{simple}}+\\lambda\\mathcal{L}\n\\] としている．\n(Austin et al., 2021) でも，パラメトライぜーション (3) の学習を加速するために，\\(\\widetilde{p}_\\theta\\) の交差エントロピー項を追加した \\[\n\\mathcal{L}_\\lambda:=\\mathcal{L}+\\lambda\\operatorname{E}_\\mathbb{Q}\\biggl[-\\log\\widetilde{p}_\\theta(X_0|X_t)\\biggr]\n\\] というものを用いている．この追加された項も，\\(\\mathcal{L}\\) 内の KL 項と同様，\\(\\widetilde{p}_\\theta(X_0|X_t)=\\delta_{X_0}\\) を達成した場合に最小化される．"
  },
  {
    "objectID": "posts/2025/DiffusionModels/DiscreteDiffusion.html#参考文献",
    "href": "posts/2025/DiffusionModels/DiscreteDiffusion.html#参考文献",
    "title": "離散空間上の拡散確率モデル",
    "section": "3 参考文献",
    "text": "3 参考文献\n\n(Ryu, 2024) に素晴らしい教育的リポジトリがある．D3PM の 425 行での PyTorch での実装を提供している．\n(Campbell et al., 2024) は最新の論文の一つである．"
  },
  {
    "objectID": "static/about.html#activities",
    "href": "static/about.html#activities",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Activities",
    "text": "Activities\n\n Data Scientist, 2024.9 – present\nPreMedica Inc., Tokyo, Japan Consulting in the field of medical data analysis.\n\n\n Research Fellow, 2025.4 – present\nGraduate University for Advanced Studies, SOKENDAI, Japan Supported by JST BOOST, Japan Grant Number JPMJBS2412."
  },
  {
    "objectID": "static/about.html#qualifications-1",
    "href": "static/about.html#qualifications-1",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Qualifications",
    "text": "Qualifications\n\n Ph.D. in Statistical Science, 2028 (expected)\nGraduate University for Advanced Studies, SOKENDAI, Japan\n\n\n B.Sc. in Mathematics, 2023\nUniversity of Tokyo, Japan"
  },
  {
    "objectID": "static/about.html#activities-1",
    "href": "static/about.html#activities-1",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Activities",
    "text": "Activities\n\n Data Scientist, 2023.4 – present\nPreMedica Inc., Tokyo, Japan Consulting in the field of medical data analysis.\n\n\n Research Assistant, 2023.7 – 2025.3\nResearch Organization of Information and Systems, Tokyo, Japan"
  },
  {
    "objectID": "static/about.html#research-visits",
    "href": "static/about.html#research-visits",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Research Visits",
    "text": "Research Visits\n\n Ulm University, Germany, 2025.7.13–26\nEvgeny Spodarev at the Institute of Stochastics.\n\n\n National University of Singapore, 2025.6.1–30\nAlexandre Thiéry at the Department of Statistics & Data Science.\n\n\n University College London, UK, 2024.11.4–12.2\nAlexandros Beskos at the Department of Statistical Science."
  },
  {
    "objectID": "static/about.html#awards",
    "href": "static/about.html#awards",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Awards",
    "text": "Awards\n\n Best Poster Award, 2025.3\n19th Spring Meeting of the Japanese Statistical Society\n\n\n Director’s Award, 2025.3\nJapanese Statistical Society Certificate"
  },
  {
    "objectID": "static/Japanese.html#学位-1",
    "href": "static/Japanese.html#学位-1",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "総合研究大学院大学．指導教員：鎌谷研吾，矢野恵佑．\n\n\n\n東京大学理学部数学科．指導教員：吉田朋広．"
  },
  {
    "objectID": "static/Japanese.html#経歴-1",
    "href": "static/Japanese.html#経歴-1",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "",
    "text": "第19回日本統計学会春季集会．"
  },
  {
    "objectID": "static/Japanese.html#研究滞在",
    "href": "static/Japanese.html#研究滞在",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "研究滞在",
    "text": "研究滞在\n\n ウルム大学, ドイツ, 2025.7.13–26\n受入教員：Evgeny Spodarev 支援：Opportunity Week\n\n\n シンガポール国立大学, 2025.6.1–30\n受入教員：Alexandre Thiéry 支援：SOKENDAI 研究派遣プログラム\n\n\n ユニバーシティ・カレッジ・ロンドン, 英国, 2024.11.4–12.2\n受入教員：Alexandros Beskos 支援：SOKENDAI 研究派遣プログラム"
  },
  {
    "objectID": "static/Japanese.html#受賞",
    "href": "static/Japanese.html#受賞",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "受賞",
    "text": "受賞\n\n 優秀発表賞，統計検定センター長賞 2025.3\n第19回日本統計学会春季集会．"
  },
  {
    "objectID": "static/Notations.html#sec-operator",
    "href": "static/Notations.html#sec-operator",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "4 作用素",
    "text": "4 作用素\n核の概念は近年データ解析や計算統計にも広く応用されているが，元来は解析学において重要な役割を果たす．\n\n4.1 関数空間\n\n\\((T,d)\\) を距離空間，\\(\\gamma\\in(0,1]\\) とする．\\(T\\) 上の \\(\\gamma\\)-Hölder 連続関数 の全体を \\(\\mathrm{Lip}^\\gamma(T,d)\\) で表す．99 \\(\\gamma=1\\) の場合はこれを省略して単に \\(\\mathrm{Lip}(T,d)\\) と書く．\nその 半ノルム を \\[\n\\|f\\|_{\\mathrm{Lip}^\\gamma}:=\\sup_{x\\ne y}\\frac{\\lvert f(x)-f(y)\\rvert}{d(x,y)^\\gamma}\n\\] と定める．100\nLipschitz 定数が \\(c\\) 以下になる関数のなす部分集合を \\[\n\\begin{align*}\n  &\\mathrm{Lip}_c(T,d)\\\\\n  &:=\\left\\{f\\in\\mathrm{Lip}(T)\\mid\\|f\\|_\\mathrm{Lip}\\le c\\right\\}\n\\end{align*}\n\\] で表す．101\n有界 \\(\\gamma\\)-Hölder 連続関数のなす空間 \\(\\mathrm{Lip}_b^\\gamma(T,d)\\) のノルムを \\[\n\\|f\\|_{\\mathrm{Lip}_b^\\gamma}:=\\|f\\|_{\\mathrm{Lip}^\\gamma}+\\|f\\|_\\infty\n\\] で定める．\\(\\gamma=1\\) の場合，\\(\\|f\\|_\\mathrm{BL}\\) とも表す．102\n\\(T\\) を位相空間とする．\\(T\\) 上の連続関数の全体を \\(C(T)\\) で表す．\n\\(E\\) を可微分多様体とする．\\(k\\in\\mathbb{N}^+\\cup\\{\\infty\\}\\) 回連続微分可能な関数がなす \\(C(E)\\) の部分空間を， \\[\nC^k(E):=\\left\\{f\\in C^k(E)\\:\\middle|\\:\\substack{ f\\;\\text{は}\\;k\\;\\text{回微分可能}\\\\\\forall_{1\\le l\\le k}\\;f^{(l)}\\in C(E)}\\right\\}\n\\] を表す．\nさらに \\(C_b^k(E),C_c^k(E),C_p^k(E)\\) と表した場合は，その \\(k\\) 回までの導関数も同様に \\(C_b,C_c,C_p\\) に含まれるとする．103\n\\(E\\) は距離空間でもあるとする．\\(\\gamma\\in(0,1]\\) に対して，\\(k\\) 階連続微分可能で，全ての \\(k\\) 回までの導関数も有界で \\(\\gamma\\)-Hölder 連続な関数のなす \\(C^k_b(E)\\) の部分空間を \\(C^{k,\\gamma}(E)\\) で表し，ノルムを \\[\n\\begin{align*}\n  \\|u\\|_{C^{k,\\gamma}(E)}&:=\\sum_{\\lvert\\alpha\\rvert\\le k}\\|D^\\alpha u\\|_\\infty\\\\\n  &\\qquad+\\sum_{\\lvert\\alpha\\rvert=k}\\|D^\\alpha u\\|_{\\mathrm{Lip}^\\gamma}\n\\end{align*}\n\\] で定める．\\(C^{k,\\gamma}(E)\\) を Hölder 空間 と言う．104\n\\(E,F\\) を可微分多様体とする．２変数関数 \\(f:E\\times F\\to\\mathbb{R}\\) について，\n\n\\[\\begin{align*}\n    C^{1,2}(E\\times F)&:=\\bigg\\{f:E\\times F\\to\\mathbb{R}\\;\\bigg|\\:\\substack{\\forall_{y\\in F}\\;f(-,y)\\in C^1(E)\\\\\\forall_{x\\in E}\\;f(x,-)\\in C^2(F)}\\bigg\\}\n\\end{align*}\\]\nと表す．105\nイタリック体のものが Banach 空間（の部分集合）に，カリグラフィー体のものがより一般的なものになるように注意している．106\n\\(\\mathcal{F}(E)\\subset\\mathbb{R}^E\\) は \\(L(E), C(E)\\) などの関数空間の一般形とし，\\(X,Y\\) をノルム空間とする．\n\n測度空間 \\((E,\\mathcal{E},\\mu)\\) 上の関数空間 \\(\\mathcal{F}(E)\\) に対して，文脈により \\(\\mathcal{F}(\\mu)\\) とも \\(\\mathcal{F}(E,\\mathcal{E},\\mu)\\) とも表す．\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，値域の空間が \\(\\mathcal{X}\\) であるとき，これを強調して \\(\\mathcal{F}(E;\\mathcal{X})\\) または \\(\\mathcal{F}_\\mathcal{X}(E)\\) とも表す．省略する場合は \\(\\mathcal{X}=\\mathbb{R}\\) の場合に限る．107\n任意の関数空間 \\(\\mathcal{F}(E)\\) に対して，\n\n有界なもののなす部分空間を \\(\\mathcal{F}_b(E)\\) で表す．\nコンパクト台を持つもののなす部分空間を \\(\\mathcal{F}_c(E)\\) で表す．108\n有界かつ一様連続なもののなす部分空間を \\(\\mathcal{F}_u(E)\\) で表す．109\n高々多項式増大なもののなす部分空間を \\(\\mathcal{F}_p(E)\\) で表す．110\n非負値のもののなす錐を \\(\\mathcal{F}(E)_+:=\\mathcal{F}(E;\\mathbb{R}_+)\\) で表す．111\n正値なもののなす部分集合を \\(\\mathcal{F}(E)^+:=\\mathcal{F}(E;\\mathbb{R}^+)\\) で表す．112\n\n作用素 \\(T:X\\to Y\\) と言ったとき，線型写像 \\(T:X\\to Y\\) を指すこととする．113\n\\(X\\) 内の作用素 \\(T:X\\supset\\mathcal{D}(T)\\to Y\\) と言ったとき，ある \\(X\\) の部分空間 \\(\\mathcal{D}(T)\\) 上で定義された作用素 \\(T:\\mathcal{D}(T)\\to Y\\) を指すこととする．114\n有界作用素の全体を \\(B(X,Y)\\) で表す．115 \\(B(X):=B(X,X)\\) とする．\n連続作用素の全体を \\(L(X,Y)\\) で表す．116\n作用素 \\(T:X\\to Y\\) の 定義域 を \\(\\mathcal{D}(T)\\) で表す．\n作用素 \\(T:X\\to Y\\) の グラフ を \\[\n\\mathcal{G}(T):=\\left\\{x\\oplus Tx\\in X\\oplus Y\\mid x\\in\\mathcal{D}(T)\\right\\}.\n\\] で表す．\n\n\n\n4.2 微分作用素\n\\(u\\) を \\(\\mathbb{R}^n\\) のある開集合上に定義された十分滑らかな関数とする．\n\n\\(\\mathbb{R}^n\\) 上の関数 \\(u\\) の偏導関数を \\[\nu_{x_i}:=\\partial_iu:=\\frac{\\partial u}{\\partial x_i}\n\\] でも表す．117\n\\(\\mathbb{N}^n\\) の元 \\(\\alpha\\in\\mathbb{N}^n\\) を 多重指数 といい，その位数を \\[\n\\lvert\\alpha\\rvert:=\\|\\alpha\\|_1=\\alpha_1+\\cdots+\\alpha_n\n\\] で表す．118\n\\(u\\) を \\(\\mathbb{R}^m\\)-値関数とする．自然数 \\(k\\in\\mathbb{N}\\) に対して，\\(D^ku:=(D^\\alpha u)_{\\substack{\\alpha\\in\\mathbb{N}^n\\\\\\lvert\\alpha\\rvert=k}}\\) を，\\(k\\) 階の微分 \\[\nD^\\alpha u=(D^\\alpha u^1,\\cdots,D^\\alpha u^m),\n\\] \\[\nD^\\alpha u^i:=\\frac{\\partial ^{\\lvert\\alpha\\rvert}u^i}{\\partial x_1^{\\alpha_1}\\cdots\\partial x_n^{\\alpha_n}},\n\\] の族とする．119\n特に \\(k=1\\) のとき，Jacobi 行列 または 勾配行列 \\[\nDu=\\begin{pmatrix}u^1_{x_1}&\\cdots&u^1_{x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u^m_{x_1}&\\cdots&u^m_{x_n}\\end{pmatrix}\n\\] と同一視する．120 \\(m=1\\) のとき， \\[\n\\operatorname{grad}u:=\\nabla u:=(Du)^\\top=\\begin{pmatrix}\\frac{\\partial u}{\\partial x_1}\\\\\\vdots\\\\\\frac{\\partial u}{\\partial x_n}\\end{pmatrix}\n\\] とも表す．\n発散 を \\[\n\\operatorname{div}u:=\\nabla\\cdot u:=\\operatorname{Tr}(Du)=\\sum_{i=1}^n\\frac{\\partial u}{\\partial x_i}\n\\] で表す．121\n\\(u\\) が正方行列 \\(M_n(\\mathbb{R})\\)-値であった場合，行成分毎の適用 \\[\n  \\operatorname{div}u:=\\begin{pmatrix}\\operatorname{div}(u_{1-})\\\\\\vdots\\\\\\operatorname{div}(u_{n-})\\end{pmatrix}\n  \\] と解する．\n\\(k=2\\) かつ \\(m=1\\) のとき，\\(D^2u\\) を Hesse 行列 \\[\n  \\nabla^2u:=\\begin{pmatrix}u_{x_1x_1}&\\cdots&u_{x_1x_n}\\\\\\vdots&\\ddots&\\vdots\\\\u_{x_nx_1}&\\cdots&u_{x_nx_n}\\end{pmatrix}\n  \\] と同一視する．122\n\\(\\mathbb{R}^n\\) 上の Laplace 作用素 (Laplacian) を \\[\n  \\mathop{}\\!\\mathbin\\bigtriangleup u:=\\sum_{i=1}^n\\partial_i^2u=\\operatorname{Tr}(D^2u)\n  \\] で定める．\n\n\n\n4.3 Fourier 変換\n\nHeaviside の階段関数 \\(H:\\mathbb{R}\\to2\\) を \\[\nH(x):=1_{[0,\\infty]}\n\\] で表す．123\n符号関数 を \\[\n\\operatorname{sgn}(x):=2H(x)-1\n\\] で定める．124\n関数 \\(f,g\\) の 畳み込み を \\[\n(f_1*f_2)(x):=\\int_\\mathbb{R}f_1(t)f_2(x-t)\\,dt\n\\] で表す．\n\\(\\mathcal{D}(\\mathbb{R}^d):=C_c^\\infty(\\mathbb{R}^d)\\) とも表す．125 その双対空間は \\(\\mathcal{D}'(\\mathbb{R}^d)\\) と表し，その元を 超関数 という．126"
  },
  {
    "objectID": "static/Notations.html#測度",
    "href": "static/Notations.html#測度",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "3 測度",
    "text": "3 測度\n\n3.1 可測空間\n\n集合族 \\(\\mathcal{A}\\subset P(X)\\) が生成する \\(\\sigma\\)-代数を \\(\\sigma(\\mathcal{A})\\) で表す．64\n集合の族 \\(\\mathcal{A}\\subset P(X)\\) 上の関数 \\(\\mu:\\mathcal{A}\\to[0,\\infty]\\) に対して， \\[\n\\begin{align*}\n  \\mu^*(A)&:=\\inf\\biggl\\{\\sum_{n=1}^\\infty\\mu(A_n)\\in[0,\\infty]\\:\\bigg|\\\\\n  &\\qquad\\qquad\\{A_n\\}\\subset\\mathcal{A},A\\subset\\bigcup_{n=1}^\\infty A_n\\biggr\\}\n\\end{align*}\n\\] を 外測度 という．65\n測度空間 \\((X,\\mathcal{A},\\mu)\\) において，\\(\\mathcal{A}\\) の \\(\\mu\\) による Lebesgue 完備化 を \\[\n\\mathcal{A}_\\mu:=\\left\\{A\\in P(X)\\:\\middle|\\:\\substack{\\forall_{\\epsilon&gt;0}\\;\\exists_{A_\\epsilon\\in\\mathcal{A}}\\\\\\mu^*(A\\triangle A_\\epsilon)&lt;\\epsilon}\\right\\}\n\\] で表し，この元を \\(\\mu\\)-可測集合 という．66\n\\(\\mu\\)-零集合の全体を \\[\n\\mathcal{N}(\\mu):=\\left\\{N\\in P(X)\\mid \\mu^*(N)=0\\right\\}\n\\] で表し，\\(\\mu\\)-零集合の補集合を \\(\\mu\\)-充満集合 と呼ぶ．67\n\\(\\mu\\)-零集合と \\(\\mu\\)-充満集合との全体がなす \\(\\sigma\\)-代数を \\(2:=\\sigma(\\mathcal{N}(\\mu))\\) で表す．68\n\\(\\mu\\)-可測集合 \\(A\\in\\mathcal{A}_\\mu\\) に関して， \\[\n\\mathcal{A}_\\mu\\cap A:=\\left\\{B\\cap A\\in\\mathcal{A}_\\mu\\mid B\\in\\mathcal{A}_\\mu\\right\\}\n\\] 上への \\(\\mu\\) の制限を，\\(\\mu|_A:\\mathcal{A}_\\mu\\cap A\\to[0,\\infty]\\) で表す．69\n測度空間の族 \\((E_i,\\mathcal{E}_i,\\mu_i)\\) について，積集合 \\(\\prod_{i\\in I}E_i\\) 上の 積 \\(\\sigma\\)-加法族 を \\[\n\\bigotimes_{i\\in I}\\mathcal{E}_i=\\sigma\\left([\\bigcup_{i\\in I}]\\mathrm{pr}_i^*(\\mathcal{E}_i)\\right)\n\\] で表す．70\nこの上の直積測度を \\(\\bigotimes_{i\\in I}\\mu_i\\) で表す．71\n\\(\\lvert I\\rvert=n,\\mu_i=\\mu\\) の場合は \\(\\mu^{\\otimes n}\\) とも表す．\n位相空間 \\((X,\\mathcal{O})\\) 上の Borel \\(\\sigma\\)-加法族 を \\[\n\\mathcal{B}(X):=\\sigma(\\mathcal{O})\n\\] で表す．\n\\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) の積空間 \\(\\mathbb{R}^T\\) 上の積 \\(\\sigma\\)-加法族を \\(\\mathcal{C}\\) で表す．\\((\\mathbb{R}^T,\\mathcal{C})\\) 上の標準Gauss測度を \\(\\gamma\\) で表す．72\n\\(\\ell_n\\) は \\(\\mathbb{R}^n\\) 上の Lebesgue 測度 を表す．73 \\(\\gamma_n:=\\operatorname{N}(0,1)^{\\otimes n}\\) は 標準 Gauss 測度 を表す．\n\n\n\n3.2 確率空間\n\n\\((\\Omega,\\mathcal{F},\\mathrm{P})\\) を標準的な 確率空間 とする．74 よって，明示せずとも，確率変数 \\(X\\) と言ったときは \\(\\mathcal{L}(\\Omega,\\mathcal{F},\\mathrm{P})\\) の元とする．\nPolish 確率空間 と言ったとき，Polish 空間 \\(E\\) 上の Borel 可測空間 \\((E,\\mathcal{B}(E))\\) 上の確率空間を指す．75\n期待値作用素を \\[\\operatorname{E}:L(\\Omega)\\to[-\\infty,\\infty]\\] で表す．76\n期待値作用素と確率測度の引数は \\[\\operatorname{E}[X],\\quad\\operatorname{P}[X\\in A]\\] と角括弧内に記する．77\n確率変数 \\(X\\in\\mathcal{L}(\\Omega)\\) と事象 \\(A\\in\\mathcal{F}\\) に関して，次の略記を用いる： \\[\n\\operatorname{E}[X,A]:=\\operatorname{E}[X1_A]=\\int_AX(\\omega)\\operatorname{P}(d\\omega).\n\\]\n分散と共分散は \\(\\mathrm{V}[X],\\mathrm{C}[X,Y]\\) と表す．78\n確率変数 \\(X\\in\\mathcal{L}(\\Omega;\\mathcal{X})\\) による測度 \\(\\operatorname{P}\\) の 押し出し を \\[\\operatorname{P}^X:=X_*\\operatorname{P}\\in\\mathcal{P}(\\mathcal{X})\\] で表し，これを \\(X\\) の 分布 という．79\nこの関係を \\(X\\sim\\operatorname{P}^X\\) とも表す．\n確率変数 \\(X\\) の分布 \\(\\operatorname{P}^X\\) を \\(\\mathcal{L}[X]\\in\\mathcal{P}(\\mathcal{X})\\) とも表す．80\n2つの確率変数 \\(X,Y\\in\\mathcal{L}(\\Omega)\\) の分布が等しいとき，\\(X\\overset{\\text{d}}{=}Y\\) とも表す．81\n\\(X\\perp\\!\\!\\!\\perp Y\\) とは確率変数 \\(X,Y\\) が 独立 であることを表す．82\n確率変数 \\(X:\\Omega\\to\\mathcal{X},Y:\\mathcal{X}\\to\\mathcal{Y}\\) について，\\(Y(X)\\) によって合成関数 \\(Y\\circ X:\\Omega\\to\\mathcal{Y}\\) を表す．\n\nなお，確率変数，推定量，統計量とは，確率空間上の可測関数の，特定の意図を持った別名称に他ならない．83"
  },
  {
    "objectID": "posts/2025/Slides/Boost.html#今後の研究計画",
    "href": "posts/2025/Slides/Boost.html#今後の研究計画",
    "title": "SOKENDAI 特別研究員",
    "section": "2 今後の研究計画",
    "text": "2 今後の研究計画\n理論と応用の一気通貫で，\n新手法 PDMP の スケーラビリティ を検証する\n\n2.1 理論の現状\n\n\n\n\n\nZig-Zag(Bierkens et al., 2019)動きが単純すぎる\n\n\n\n\n\n\n\n現状２手法は一長一短→ 解析の簡単なものに対象を絞っているから\n\n\n\n\n\n\nBPS(Bouchard-Côté et al., 2018)反射法則が単純すぎる\n\n\n\n\n\n\n2.2 最も有望な手法は理論の射程外\n\n\n\n\n\nZig-Zag(Bierkens et al., 2019)動きが単純すぎる\n\n\n\n\n\n\n新手法 Forward 法(Michel et al., 2020)いずれも適度にランダム\n\n\n\n\n\n\nBPS(Bouchard-Côté et al., 2018)反射法則が単純すぎる\n\n\n\n\n\n\n2.3 理論研究の目標\n\n実践上の需要と理論的手腕を持ち合わせた申請者ならではの研究\n\n\n\n2.4 応用研究の着手点\nスケーラビリティの大規模統計データでの検証\n\n\n\n従来法を米国最高裁判事９人に適用した際の結果\n\n\n→ PDMP を米国議会議員４３５人に適用できるか……？\n\n\n2.5 応用研究ではデータが重要\n申請者が共同研究を開始している分野：\n\n政治科学\n４３５人の議員の背景情報（学歴，収入，etc）だけでなく，選挙区の情報（人種組成，学歴，etc）も取り入れたい．\n予防医療\n毎年の健康診断のデータ＋新規顧客の血液・腸内細菌叢・メンタルヘルスの超高次元データ\n\nいずれも大量のデータが解析されるのを待っている＆ベイズ法の適用が重要 → PDMP の検証に最適"
  },
  {
    "objectID": "posts/2025/Slides/Boost.html#キャリアパス構想",
    "href": "posts/2025/Slides/Boost.html#キャリアパス構想",
    "title": "SOKENDAI 特別研究員",
    "section": "3 キャリアパス構想",
    "text": "3 キャリアパス構想\n\n\n\n\n\n\n\n研究派遣プログラムにて，英国 UCL に１ヶ月間滞在（2024年11月）\n\n3.1 本プログラムへの期待：国際協力\n\nAI 分野は急速に進展しており，国際連携が肝心．\n申請者は TOEFL 100 点 を超える英語力を持ち，国際連携に積極的\nしかし AI 研究の中心地には物価水準の高い国が多い．\n\n\n\n\n申請者の海外への研究滞在計画\n\n\n\n継続的な協力関係を維持するため，博士期間中に複数回訪問する機会を作りたい．\n\n\n\n3.2 キャリアパス構想\n\n本プログラムを通じて活発に国際連携を行い，将来的にはイギリスやアメリカ，シンガポールなど，国を選ばずに通用する研究者を目指す\n理論と応用を一気通貫で研究する AI 人材を目指す\n\n確固とした数理力\n万人が使える形に実装するプログラミング力\nAI で解決できるイノベーションを見つける学際性"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#本プログラムへの期待",
    "href": "posts/2025/Slides/Boost_Slides.html#本プログラムへの期待",
    "title": "SOKENDAI 特別研究員",
    "section": "3.1 本プログラムへの期待",
    "text": "3.1 本プログラムへの期待\n\nAI 分野は急速に進展しており，国際連携が肝心．\n申請者は TOEFL 100 点 を超える英語力を持ち，国際連携に積極的\nしかし AI 研究の中心地には物価水準の高い国が多い．\n\n\n\n申請者の海外への研究滞在計画\n\n\n期間\n訪問先\n国\n\n\n\n\n11/4-12/2（終了）\nAlexandros Beskos 教授\nイギリス UCL\n\n\n6/1-6/30（予定）\nAlexandre Thiéry 准教授\nシンガポール国立大\n\n\n7/13-7/26（予定）\nEvgeny Spodarev 教授\nドイツ Ulm 大\n\n\n\n\n\n継続的な協力関係を維持するため，博士期間中再度訪問する機会を作りたい．"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#理論研究",
    "href": "posts/2025/Slides/Boost_Slides.html#理論研究",
    "title": "SOKENDAI 特別研究員",
    "section": "2.2 理論研究",
    "text": "2.2 理論研究\n\n\n\n\n\nZig-Zag：動きが単純すぎる\n\n\n\n\n\n\nForward 法：動きと反射法則，いずれも適度にランダム\n\n\n\n\n\n\nBPS：反射法則が単純すぎる"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#応用研究",
    "href": "posts/2025/Slides/Boost_Slides.html#応用研究",
    "title": "SOKENDAI 特別研究員",
    "section": "2.4 応用研究",
    "text": "2.4 応用研究\nスケーラビリティの大規模統計データでの検証\n\n従来法を米国最高裁判事９人に適用した際の結果→ PDMP を米国議会議員４３５人に適用できるか……？"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#研究テーマ",
    "href": "posts/2025/Slides/Boost_Slides.html#研究テーマ",
    "title": "SOKENDAI 特別研究員",
    "section": "1.1 研究テーマ",
    "text": "1.1 研究テーマ"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#これまでの研究",
    "href": "posts/2025/Slides/Boost_Slides.html#これまでの研究",
    "title": "SOKENDAI 特別研究員",
    "section": "1.3 これまでの研究",
    "text": "1.3 これまでの研究\n\n分野内唯一の汎用 PDMP パッケージ PDMPFlux を開発\n公式リポジトリに登録済み → 政治科学・予防医療での共同研究が進行中\n\n\n発表者開発 PDMPFlux は使用者が自由に指定した入力に従う PDMP を実行可能．"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#実績パッケージの実装と公開",
    "href": "posts/2025/Slides/Boost_Slides.html#実績パッケージの実装と公開",
    "title": "SOKENDAI 特別研究員",
    "section": "1.4 実績：パッケージの実装と公開",
    "text": "1.4 実績：パッケージの実装と公開\n\n\n\n\n\nポスター発表（3月8日）\n\n\n\n\n第19回日本統計学会春季集会 優秀発表賞\n統計検定センター長賞"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#背景信頼のおける-ai-に向けて",
    "href": "posts/2025/Slides/Boost_Slides.html#背景信頼のおける-ai-に向けて",
    "title": "SOKENDAI 特別研究員",
    "section": "1.1 背景：信頼のおける AI に向けて",
    "text": "1.1 背景：信頼のおける AI に向けて\n\n\nベイズ法は統計での実績がある \n\nベイズ法は AI 分野での応用も期待されている \n\n課題：ベイズ法はスケーラビリティがない （＝大規模データでは恐ろしく計算が遅い）"
  },
  {
    "objectID": "posts/2025/Slides/Boost.html",
    "href": "posts/2025/Slides/Boost.html",
    "title": "SOKENDAI 特別研究員",
    "section": "",
    "text": "モンテカルロ法に用いられる PDMP の例．発表者開発の PDMPFlux.jl パッケージからの出力．\n\n\n\n\n→ 行政，政治科学，生物学，個別化医療……への応用\n提案：ベイズ法を使う\n\n\nベイズ法：統計学では歴史が長い\n\n\n\nベイズ法による不確実性の定量化\n\n\n\nベイズ × AI が近年の問題意識\n\n\n\nベイズ法の自動運転への応用(Kendall and Gal, 2017)\n\n\n\n\n\n\n\n課題：ベイズ法は拡張性がない（＝大規模データで計算量が爆発）\n\n\n\n\n\n拡散過程（従来法）\n\n\n\n\n\n\n\n\n\n\n新手法 PDMP\n\n\n\n\n収束 速\n計算量 少\n拡張性 高\n\n\n\n以上の理論研究に従事\n\n\n\n\nPDMP（新手法）\n\n\n\n\n\n\n\n\n従来は汎用パッケージがなく，PDMP の応用研究は皆無 → 分野内唯一の汎用 PDMP パッケージを開発\n公式リポジトリに登録済み → 政治科学・予防医療での共同研究が進行中\n\n\n\n\n発表者開発 PDMPFlux は使用者が自由に指定した入力に従う PDMP を実行可能．\n\n\n\n\n\n\n\n\n\n\nポスター発表（3月8日）\n\n\n\n\n第19回日本統計学会春季集会 優秀発表賞\n統計検定センター長賞"
  },
  {
    "objectID": "posts/2025/Slides/Boost.html#これまでの研究実績",
    "href": "posts/2025/Slides/Boost.html#これまでの研究実績",
    "title": "SOKENDAI 特別研究員",
    "section": "",
    "text": "モンテカルロ法に用いられる PDMP の例．発表者開発の PDMPFlux.jl パッケージからの出力．\n\n\n\n\n→ 行政，政治科学，生物学，個別化医療……への応用\n提案：ベイズ法を使う\n\n\nベイズ法：統計学では歴史が長い\n\n\n\nベイズ法による不確実性の定量化\n\n\n\nベイズ × AI が近年の問題意識\n\n\n\nベイズ法の自動運転への応用(Kendall and Gal, 2017)\n\n\n\n\n\n\n\n課題：ベイズ法は拡張性がない（＝大規模データで計算量が爆発）\n\n\n\n\n\n拡散過程（従来法）\n\n\n\n\n\n\n\n\n\n\n新手法 PDMP\n\n\n\n\n収束 速\n計算量 少\n拡張性 高\n\n\n\n以上の理論研究に従事\n\n\n\n\nPDMP（新手法）\n\n\n\n\n\n\n\n\n従来は汎用パッケージがなく，PDMP の応用研究は皆無 → 分野内唯一の汎用 PDMP パッケージを開発\n公式リポジトリに登録済み → 政治科学・予防医療での共同研究が進行中\n\n\n\n\n発表者開発 PDMPFlux は使用者が自由に指定した入力に従う PDMP を実行可能．\n\n\n\n\n\n\n\n\n\n\nポスター発表（3月8日）\n\n\n\n\n第19回日本統計学会春季集会 優秀発表賞\n統計検定センター長賞"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#目標信頼のおける-ai-を開発する",
    "href": "posts/2025/Slides/Boost_Slides.html#目標信頼のおける-ai-を開発する",
    "title": "SOKENDAI 特別研究員",
    "section": "1.1 目標：信頼のおける AI を開発する",
    "text": "1.1 目標：信頼のおける AI を開発する\n→ 行政，政治科学，生物学，個別化医療……への応用\n提案：ベイズ法を使う\n\n\nベイズ法：統計学では歴史が長い\n\n\n\nベイズ法による不確実性の定量化\n\n\n\nベイズ × AI が近年の問題意識\n\n\n\nベイズ法の自動運転への応用(Kendall and Gal, 2017)"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#これまでの研究理論",
    "href": "posts/2025/Slides/Boost_Slides.html#これまでの研究理論",
    "title": "SOKENDAI 特別研究員",
    "section": "1.2 これまでの研究（理論）",
    "text": "1.2 これまでの研究（理論）\n課題：ベイズ法は拡張性がない（＝大規模データで計算量が爆発）\n\n\n\n\n\n拡散過程（従来法）\n\n\n\n\n\n\n\n\n新手法 PDMP\n\n\n\n収束 速\n計算量 少\n拡張性 高\n\n\n\n\n\n以上の理論研究に従事\n\n\n\n\nPDMP（新手法）"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#これまでの研究応用",
    "href": "posts/2025/Slides/Boost_Slides.html#これまでの研究応用",
    "title": "SOKENDAI 特別研究員",
    "section": "1.3 これまでの研究（応用）",
    "text": "1.3 これまでの研究（応用）\n\n分野内唯一の汎用 PDMP パッケージを開発\n公式リポジトリに登録済み → 政治科学・予防医療での共同研究が進行中\n\n\n発表者開発の PDMPFlux.jl パッケージからの出力．"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#理論研究-12",
    "href": "posts/2025/Slides/Boost_Slides.html#理論研究-12",
    "title": "SOKENDAI 特別研究員",
    "section": "2.1 理論研究 (1/2)",
    "text": "2.1 理論研究 (1/2)\n\n\n\n\n\nZig-Zag (Bierkens et al., 2019)\n\n\n\n\n\n\n現状２手法は一長一短\n\n\n\n\n\n\nBPS (Bouchard-Côté et al., 2018)"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#理論研究-22",
    "href": "posts/2025/Slides/Boost_Slides.html#理論研究-22",
    "title": "SOKENDAI 特別研究員",
    "section": "2.2 理論研究 (2/2)",
    "text": "2.2 理論研究 (2/2)\n\n\n\n\n\nZig-Zag (Bierkens et al., 2019)\n\n\n\n\n\n\nForward 法 (Michel et al., 2020)\n\n\n\n\n\n\nBPS (Bouchard-Côté et al., 2018)"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#現状理解",
    "href": "posts/2025/Slides/Boost_Slides.html#現状理解",
    "title": "SOKENDAI 特別研究員",
    "section": "2.1 現状理解",
    "text": "2.1 現状理解\n\n\n\n\n\nZig-Zag：動きが単純すぎる\n\n\n\n\n\n\n現状２手法は一長一短\n\n\n\n\n\n\nBPS：反射法則が単純すぎる"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#理論の現状",
    "href": "posts/2025/Slides/Boost_Slides.html#理論の現状",
    "title": "SOKENDAI 特別研究員",
    "section": "2.1 理論の現状",
    "text": "2.1 理論の現状\n\n\n\n\n\nZig-Zag(Bierkens et al., 2019)動きが単純すぎる\n\n\n\n\n\n\n\n現状２手法は一長一短→ 解析の簡単なものに対象を絞っているから\n\n\n\n\n\n\nBPS(Bouchard-Côté et al., 2018)反射法則が単純すぎる"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#手法の現状",
    "href": "posts/2025/Slides/Boost_Slides.html#手法の現状",
    "title": "SOKENDAI 特別研究員",
    "section": "2.2 手法の現状",
    "text": "2.2 手法の現状\n\n\n\n\n\nZig-Zag (Bierkens et al., 2019)動きが単純すぎる\n\n\n\n\n\n\n新手法 Forward 法 (Michel et al., 2020)いずれも適度にランダム\n\n\n\n\n\n\nBPS (Bouchard-Côté et al., 2018)反射法則が単純すぎる"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#理論研究の目標",
    "href": "posts/2025/Slides/Boost_Slides.html#理論研究の目標",
    "title": "SOKENDAI 特別研究員",
    "section": "2.3 理論研究の目標",
    "text": "2.3 理論研究の目標\n\n実践上の需要と理論的手腕を持ち合わせた申請者ならではの研究"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#理論研究の目標-auto-animatetrue-smaller",
    "href": "posts/2025/Slides/Boost_Slides.html#理論研究の目標-auto-animatetrue-smaller",
    "title": "SOKENDAI 特別研究員",
    "section": "2.3 理論研究の目標 {auto-animate=true smaller}",
    "text": "2.3 理論研究の目標 {auto-animate=true smaller}\n\n\n\n\n\n理論のマップ\n\n\n\n\n\n\n\nリサーチ・クエスチョン\n\n\n\nForward 法 は左図のどこに入るか？\n右上の灰色ゾーンに入れり，スケーラビリティ を達成すると言えるか？\n\n\n\n\n\n\n\n\n\n本質的問題\n\n\nシンプルなダイナミクス vs. シンプルな反射法則\n\nこのトレードオフは本分野の本質．\nForward 法 のような限界事例の解析は，PDMP の枠組みの有用性の検証に本質的"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#新手法が次々提案されている",
    "href": "posts/2025/Slides/Boost_Slides.html#新手法が次々提案されている",
    "title": "SOKENDAI 特別研究員",
    "section": "2.2 新手法が次々提案されている",
    "text": "2.2 新手法が次々提案されている\n\n\n\n\n\nZig-Zag (Bierkens et al., 2019)動きが単純すぎる\n\n\n\n\n\n\n新手法 Forward 法 (Michel et al., 2020)いずれも適度にランダム\n\n\n\n\n\n\nBPS (Bouchard-Côté et al., 2018)反射法則が単純すぎる"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#キャリアパス構想-1",
    "href": "posts/2025/Slides/Boost_Slides.html#キャリアパス構想-1",
    "title": "SOKENDAI 特別研究員",
    "section": "3.2 キャリアパス構想",
    "text": "3.2 キャリアパス構想\n\n本プログラムを通じて活発に国際連携を行い，将来的にはイギリスやアメリカ，シンガポールなど，国を選ばずに通用する研究者を目指す\n理論と応用を一気通貫で研究する AI 人材を目指す\n\n確固とした数理力\n万人が使える形に実装するプログラミング力\nAI で解決できるイノベーションを見つける学際性"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#参考文献",
    "href": "posts/2025/Slides/Boost_Slides.html#参考文献",
    "title": "SOKENDAI 特別研究員",
    "section": "参考文献",
    "text": "参考文献\n\n\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nBouchard-Côté, A., Vollmer, S. J., and Doucet, A. (2018). The bouncy particle sampler: A nonreversible rejection-free markov chain monte carlo method. Journal of the American Statistical Association, 113(522), 855–867.\n\n\nKendall, A., and Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in neural information processing systems,Vol. 30. Curran Associates, Inc.\n\n\nMichel, M., Durmus, A., and Sénécal, S. (2020). Forward event-chain monte carlo: Fast sampling by randomness control in irreversible markov chains. Journal of Computational and Graphical Statistics, 29(4), 689–702."
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#pdmp-によるベイズ法の拡張",
    "href": "posts/2025/Slides/Boost_Slides.html#pdmp-によるベイズ法の拡張",
    "title": "SOKENDAI 特別研究員",
    "section": "1.2 PDMP によるベイズ法の拡張",
    "text": "1.2 PDMP によるベイズ法の拡張\n課題：ベイズ法は拡張性がない（＝大規模データで計算量が爆発）\n\n\n\n\n\n拡散過程（従来法）\n\n\n\n\n\n\n\n\n新手法 PDMP\n\n\n\n収束 速\n計算量 少\n拡張性 高\n\n\n\n\n\n以上の理論研究に従事\n\n\n\n\nPDMP（新手法）"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#パッケージの開発",
    "href": "posts/2025/Slides/Boost_Slides.html#パッケージの開発",
    "title": "SOKENDAI 特別研究員",
    "section": "1.3 パッケージの開発",
    "text": "1.3 パッケージの開発\n\n従来は汎用パッケージがなく，PDMP の応用研究は皆無 → 分野内唯一の汎用 PDMP パッケージを開発\n公式リポジトリに登録済み → 政治科学・予防医療での共同研究が進行中\n\n\n発表者開発 PDMPFlux は使用者が自由に指定した入力に従う PDMP を実行可能．"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#実績研究発表と受賞",
    "href": "posts/2025/Slides/Boost_Slides.html#実績研究発表と受賞",
    "title": "SOKENDAI 特別研究員",
    "section": "1.4 実績：研究発表と受賞",
    "text": "1.4 実績：研究発表と受賞\n\n\n\n\n\nポスター発表（3月8日）\n\n\n\n\n第19回日本統計学会春季集会 優秀発表賞\n統計検定センター長賞"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#最も有望な手法は理論の射程外",
    "href": "posts/2025/Slides/Boost_Slides.html#最も有望な手法は理論の射程外",
    "title": "SOKENDAI 特別研究員",
    "section": "2.2 最も有望な手法は理論の射程外",
    "text": "2.2 最も有望な手法は理論の射程外\n\n\n\n\n\nZig-Zag(Bierkens et al., 2019)動きが単純すぎる\n\n\n\n\n\n\n新手法 Forward 法(Michel et al., 2020)いずれも適度にランダム\n\n\n\n\n\n\nBPS(Bouchard-Côté et al., 2018)反射法則が単純すぎる"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#応用研究ではデータが重要",
    "href": "posts/2025/Slides/Boost_Slides.html#応用研究ではデータが重要",
    "title": "SOKENDAI 特別研究員",
    "section": "2.5 応用研究ではデータが重要",
    "text": "2.5 応用研究ではデータが重要\n申請者が共同研究を開始している分野：\n\n政治科学\n４３５人の議員の背景情報（学歴，収入，etc）だけでなく，選挙区の情報（人種組成，学歴，etc）も取り入れたい．\n予防医療\n毎年の健康診断のデータ＋新規顧客の血液・腸内細菌叢・メンタルヘルスの超高次元データ\n\nいずれも大量のデータが解析されるのを待っている＆ベイズ法の適用が重要 → PDMP の検証に最適"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#本プログラムへの期待国際協力",
    "href": "posts/2025/Slides/Boost_Slides.html#本プログラムへの期待国際協力",
    "title": "SOKENDAI 特別研究員",
    "section": "3.1 本プログラムへの期待：国際協力",
    "text": "3.1 本プログラムへの期待：国際協力\n\nAI 分野は急速に進展しており，国際連携が肝心．\n申請者は TOEFL 100 点 を超える英語力を持ち，国際連携に積極的\nしかし AI 研究の中心地には物価水準の高い国が多い．\n\n\n申請者の海外への研究滞在計画\n継続的な協力関係を維持するため，博士期間中に複数回訪問する機会を作りたい．"
  },
  {
    "objectID": "posts/2025/Slides/Boost_Slides.html#応用研究の着手点",
    "href": "posts/2025/Slides/Boost_Slides.html#応用研究の着手点",
    "title": "SOKENDAI 特別研究員",
    "section": "2.4 応用研究の着手点",
    "text": "2.4 応用研究の着手点\nスケーラビリティの大規模統計データでの検証\n\n従来法を米国最高裁判事９人に適用した際の結果→ PDMP を米国議会議員４３５人に適用できるか……？"
  },
  {
    "objectID": "static/Materials.html#プレゼンテーション",
    "href": "static/Materials.html#プレゼンテーション",
    "title": "Materials",
    "section": "プレゼンテーション",
    "text": "プレゼンテーション\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統数研での学生生活\n\n\n統計数理研究所 大学院説明会\n\n\nスライドはこちら．\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOKENDAI 特別研究員\n\n\n２次審査\n\n\nスライドはこちら．\n\n\n\n2025-03-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総研大５年一貫博士課程・中間評価\n\n\nスライドはこちら．\n\n\n\n2025-01-27\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n  \n    \n      5/23/2025.\n      \n        10:00-12:00.\n      \n      司馬博文 .\n      統数研での学生生活\n      : 統計数理研究所 大学院説明会.\n      \n        \n          統数研大学院説明会.\n        \n      \n      \n        ２階大会議室 (Hybrid).\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      3/16/2025.\n      \n        15:10-15:20.\n      \n      司馬博文 .\n      SOKENDAI 特別研究員\n      : ２次審査.\n      \n        \n          学生特別研究員（総研大）.\n        \n      \n      \n        Zoom.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n        \n           PDF\n        \n      \n    \n  \n    \n      1/27/2025.\n      \n        10:00-10:50.\n      \n      司馬博文 .\n      総研大５年一貫博士課程・中間評価\n      \n      \n        \n          中間評価（総研大）.\n        \n      \n      \n        統数研会議室１（D222）.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/R/Stan2.html",
    "href": "posts/2024/R/Stan2.html",
    "title": "R 上の Stan インターフェイス",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#概観",
    "href": "posts/2024/R/Stan2.html#概観",
    "title": "R 上の Stan インターフェイス",
    "section": "概観",
    "text": "概観\nRStan は Rcpp や inline といったパッケージにより C++ を R から呼び出すことで，Stan とのインターフェイスを実現している．\n一方で CmdStanR は CmdStan という Stan のコマンドラインインターフェイスを R から呼び出すことで，Stan とのインターフェイスを実現している．"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#rstan-パッケージ",
    "href": "posts/2024/R/Stan2.html#rstan-パッケージ",
    "title": "R 上の Stan インターフェイス",
    "section": "1 RStan パッケージ",
    "text": "1 RStan パッケージ\n\n\n\n\n\n\nインストール方法\n\n\n\n\n\n詳しくは RStan Getting Started を参照ください．\nすでに存在する場合は，次を実行してからインストールします．\nremove.packages(\"rstan\")\nif (file.exists(\".RData\")) file.remove(\".RData\")\nインストールは，ほとんどの場合，次の１行で済みます：\ninstall.packages(\"rstan\", repos = \"https://cloud.r-project.org/\", dependencies = TRUE)\nRStan の利用のためには，c++ コンパイラが必要です．\nXCode コマンドラインツールをインストールすることにより，/Library/Developer/CommandLineTools/usr/bin に clang++ がインストールされます．\nclang++ -v -E -x c++ /dev/null\n現在では，macrtools を通じて C++ コンパイラを R 内でインストールすることもできます．\n次のコードが実行されれば，インストールは成功しています．\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\n\n\n\n\n\n\n\n\nコンパイラ最適化 (MacOS)\n\n\n\n\n\nRStan Getting Started の Configuring C Toolchain for Mac では，次のようなコンパイラの最適化が推奨されています：\ndotR &lt;- file.path(Sys.getenv(\"HOME\"), \".R\")\nif (!file.exists(dotR)) dir.create(dotR)\nM &lt;- file.path(dotR, \"Makevars\")\nif (!file.exists(M)) file.create(M)\narch &lt;- ifelse(R.version$arch == \"aarch64\", \"arm64\", \"x86_64\")\ncat(paste(\"\\nCXX17FLAGS += -O3 -mtune=native -arch\", arch, \"-ftemplate-depth-256\"),\n    file = M, sep = \"\\n\", append = FALSE)\nこれにより ~/.R/Makevars に次のような行が追加されます：\nCXX17FLAGS += -O3 -mtune=native -arch arm64 -ftemplate-depth-256\n\n\n\n\n1.1 stan 関数\nRStan パッケージの本体は stan 関数である：\nstan(file, model_name = \"anon_model\", model_code = \"\", fit = NA, data = list(), pars = NA, chains = 4, iter = 2000, warmup = floor(iter/2), thin = 1, init = \"random\", seed = sample.int(.Machine$integer.max, 1), algorithm = c(\"NUTS\", \"HMC\", \"Fixed_param\"), control = NULL, sample_file = NULL, diagnostic_file = NULL, save_dso = TRUE, verbose = FALSE, include = TRUE, cores = getOption(\"mc.cores\", 1L), open_progress = interactive() && !isatty(stdout()) && !identical(Sys.getenv(\"RSTUDIO\"), \"1\"), ..., boost_lib = NULL, eigen_lib = NULL)\n\n1.1.1 モデルの受け渡し\nmodel_code=\"\" が Stan モデルを定義するコードを，文字列として直接受け渡すための引数である．\n返り値はフィット済みの stanfit オブジェクトである．\n他の方法は次のとおり：\n\nfile としてファイルへのパスを渡す\nフィット済みの stanfit オブジェクトを fit 引数として渡す\n\n\n\n1.1.2 重要な引数\n\ndata：データを与える．list 型．\niter：繰り返し回数．デフォルトは 2000．\nchains：チェイン数．デフォルトは 4．\n\n\n\n1.1.3 stanfit オブジェクト\nstan 関数は Stan モデルを C++ に変換して実行し，結果を stanfit オブジェクトとして返す．\nこれに対して print, summary, plot などのメソッドが利用可能である．\nさらに，次の様にして MCMC サンプルを取り出すことができる：\n\nas.array メソッドを用いて MCMC サンプルを array 型で取り出す\nextract メソッドを用いて MCMC サンプルを list 型で取り出す\nposterior ライブラリの as_draws_df メソッドを用いて MCMC サンプルを df 型で取り出す．種々のデータ型 &lt;format&gt; に対して as_draws_&lt;format&gt; が存在する．\n\n取り出した MCMC サンプルは bayesplot パッケージの mcmc_trace, mcmc_dens などの関数を用いて可視化することができる．\n\n\n1.1.4 例１：軌道と事後分布の可視化\n\nscode &lt;- \"\nparameters {\n  array[2] real y;\n}\nmodel {\n  y[1] ~ normal(0, 1);\n  y[2] ~ double_exponential(0, 2);\n}\n\"\nfit &lt;- stan(model_code = scode, iter = 10000, chains = 4, verbose = FALSE)\n\n\nlibrary(bayesplot)\n\nmcmc_trace(as.array(fit), pars = c(\"y[1]\", \"y[2]\"))\n\n\n\n\n軌道のプロット\n\n\n\n\n\nmcmc_dens(as.array(fit), pars = c(\"y[1]\", \"y[2]\"))\n\n\n\n\n密度のプロット\n\n\n\n\n\n\n1.1.5 例２：確率過程の統計推測\nOU 過程\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nに対して，stan 関数でベイズ推定を実行してみます．\n\nlibrary(yuima)\nmodel &lt;- setModel(drift = \"theta*(mu-X)\", diffusion = \"sigma\", state.variable = \"X\")\n\nパラメータは \\[\n\\begin{pmatrix}\\theta\\\\\\mu\\\\\\sigma\\end{pmatrix}\n=\n\\begin{pmatrix}1\\\\0\\\\0.5\\end{pmatrix}\n\\tag{1}\\] として YUIMA を用いてシミュレーションをし，そのデータを与えてパラメータが復元できるかをみます．\n\nlibrary(rstan)\nexcode &lt;- \"data {\n            int N;\n            real x[N+1];\n            real h;\n          }\n\n          parameters {\n            real theta;\n            real mu;\n            real&lt;lower=0&gt; sigma;\n          }\n\n          model {\n            x[1] ~ normal(0,1);\n            for(n in 2:(N+1)){\n              x[n] ~ normal(x[n-1] + theta * (mu - x[n-1]) * h,  sqrt(h) * sigma);\n            }\n          }\"\n\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(theta = 1, mu = 0, sigma = 0.5), xinit = rnorm(1))\nsde_dat &lt;- list(N =  yuima@sampling@n,\n                  x = as.numeric(simulation@data@original.data),\n                  h=yuima@sampling@Terminal/yuima@sampling@n)\n\n\n# シミュレーション結果\nplot(simulation)\n\n\n\n\n\n\n\n\n\n# ベイズ推定\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nfit &lt;- stan(model_code=excode, data = sde_dat, iter = 1000, chains = 4)\n\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\ntheta    0.79    0.07 0.93   -0.40    0.03    0.50    1.44    2.96   175 1.02\nmu      -0.06    0.46 3.15   -6.14   -0.68   -0.40    0.10    8.62    46 1.11\nsigma    0.48    0.00 0.01    0.46    0.48    0.48    0.49    0.51   585 1.00\nlp__  3127.12    0.06 1.17 3124.36 3126.47 3127.21 3127.95 3128.86   330 1.01\n\nSamples were drawn using NUTS(diag_e) at Sun Dec 22 10:18:54 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nパラメータ (1) がよく推定できていることがわかる．特に \\(\\sigma\\) が安定して推定できている：\n\nplot(fit)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\n\nlibrary(\"bayesplot\")\nlibrary(\"rstanarm\")\nlibrary(\"ggplot2\")\n\nposterior &lt;- as.matrix(fit)\nplot_title &lt;- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\nmcmc_areas(posterior,\n           pars = c(\"theta\", \"mu\", \"sigma\"),\n           prob = 0.8) + plot_title\n\n\n\n\n\n\n\n\n\n\n\n1.2 トラブルシューティング\n\n1.2.1 cmath が見つからない\nQuitting from lines 329-343 (adastan.qmd) \n\n compileCode(f, code, language = language, verbose = verbose) でエラー: \n  using C++ compiler: ‘Apple clang version 16.0.0 (clang-1600.0.26.3)’using C++17using SDK: ‘MacOSX15.0.sdk’In file included from &lt;built-in&gt;:1:In file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:In file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:In file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found  679 | #include &lt;cmath&gt;      |          ^~~~~~~1 error generated.make: *** [file2546221168fc.o] Error 1\n 呼び出し:  .main ... cxxfunctionplus -&gt; &lt;Anonymous&gt; -&gt; cxxfunction -&gt; compileCode\n 追加情報:  警告メッセージ: \n1:  パッケージ 'rstan' はバージョン 4.3.1 の R の下で造られました  \n2:  パッケージ 'bayesplot' はバージョン 4.3.1 の R の下で造られました  \n3:  パッケージ 'rstanarm' はバージョン 4.3.1 の R の下で造られました  \n\n\nQuitting from lines 329-343 (adastan.qmd) \n sink(type = \"output\") でエラー:  コネクションが不正です \n 呼び出し:  .main ... eval -&gt; stan -&gt; stan_model -&gt; cxxfunctionplus -&gt; sink\n 実行が停止されました \n大変長く書いてあるが，要は fatal error: 'cmath' file not found である．\n筆者の場合は純粋な clang++ の問題であった：\n❯ echo '#include &lt;cmath&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    double result = std::sqrt(16.0);\n    std::cout &lt;&lt; \"The square root of 16 is \" &lt;&lt; result &lt;&lt; std::endl;\n    return 0;\n}' &gt; test.cpp\n\n\n~\n❯ clang++ -std=c++17 test.cpp -o test\n\ntest.cpp:1:10: fatal error: 'cmath' file not found\n    1 | #include &lt;cmath&gt;\n      |          ^~~~~~~\n1 error generated.\nこのような場合は，まず Xcode の再インストールをすると良い．\nsoftwareupdate --list\nの出力を用いて，次のようにする：\nsoftwareupdate -i \"Command Line Tools (macOS High Sierra version 10.13) for Xcode-10.1\"\nまたは次のようにする：\nsudo rm -rf /Library/Developer/CommandLineTools\nxcode-select --install"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#rstanarm-パッケージ",
    "href": "posts/2024/R/Stan2.html#rstanarm-パッケージ",
    "title": "R 上の Stan インターフェイス",
    "section": "2 rstanarm パッケージ",
    "text": "2 rstanarm パッケージ\n\n2.1 はじめに\nrstanarm は他の R パッケージと同様のインターフェースで推論を実行するためのパッケージであり，MCMC の実行には rstan をバックエンドで用いる．"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#cmdstanr-パッケージ",
    "href": "posts/2024/R/Stan2.html#cmdstanr-パッケージ",
    "title": "R 上の Stan インターフェイス",
    "section": "3 CmdStanR パッケージ",
    "text": "3 CmdStanR パッケージ\nCmdStanPy, CmdStanR はいずれも Stan のインターフェースである．\nCmdStanR は R6 オブジェクトを用いており，大変現代的な実装を持っている．\n\n\n\n\n\n\nインストール方法\n\n\n\n\n\nGetting Started with CmdStanR に従って実行します．\nrepos 引数を省略するとインストールできないことがあります．\ninstall.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\nCmdStanR の利用のためには，CmdStan が必要です．\nCmdStanR を直接インストールすることもできますが，CmdStanR 内部からインストールすることもできます．\nlibrary(cmdstanr)\ninstall_cmdstan(cores = 4)\n\ncmdstan_version()\n\n[1] \"2.36.0\"\n\n\n多くの場合，自動で CMDSTAN 環境変数にパスが設定されます．次のいずれかの方法で確認できます：\nSys.getenv(\"CMDSTAN\")\ncmdstan_path()\nCmdStanR の美点の一つは，install_cmdstan() により CmdStan をアップデートすることで最新の Stan を R から簡単に利用できることである．\n一方で RStan はパッケージ自体のアップデートを待つ必要がある．\n\n\n\n\n3.1 モデル定義\ncmdstan_model() 関数は，Stan 言語による記述されたモデル定義を，C++ コードにコンパイルし，その結果を R6 オブジェクトとして返す．1\ncmdstan_model(stan_file = NULL, exe_file = NULL, compile = TRUE, ...)\n返り値は CmdStanModel オブジェクトである．ただし R6 オブジェクトでもあり，R6 流のメソッドの呼び方 $ が使える．\n\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file)\n\nStan 言語による定義は次のようにして確認できる：\n\nmod$print()\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0, upper=1&gt; y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  theta ~ beta(1, 1); // uniform prior on interval 0,1\n  y ~ bernoulli(theta);\n}\n\nnames(mod$variables())\n\n[1] \"parameters\"             \"included_files\"         \"data\"                  \n[4] \"transformed_parameters\" \"generated_quantities\"  \n\nnames(mod$variables()$transformed_parameters)\n\ncharacter(0)\n\n\n元となったファイルのパスも stan_file(), exe_file() で確認できる．\n\n\n3.2 Stan コードの操作\nwrite_stan_file() 関数は Stan コードをファイルに書き出すことができる：\nwrite_stan_file(\n  code,\n  dir = getOption(\"cmdstanr_write_stan_file_dir\", tempdir()),\n  basename = NULL,\n  force_overwrite = FALSE,\n  hash_salt = \"\"\n)\nグローバル環境変数が設定されていない限り，tempdir() で一時ファイルが作成される．これは R セッションの終了とともに削除される．\nstan_file_variables &lt;- write_stan_file(\"\ndata {\n  int&lt;lower=1&gt; J;\n  vector&lt;lower=0&gt;[J] sigma;\n  vector[J] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; tau;\n  vector[J] theta_raw;\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * theta_raw;\n}\nmodel {\n  target += normal_lpdf(tau | 0, 10);\n  target += normal_lpdf(mu | 0, 10);\n  target += normal_lpdf(theta_raw | 0, 1);\n  target += normal_lpdf(y | theta, sigma);\n}\n\")\nmod_v &lt;- cmdstan_model(stan_file_variables)\nvariables &lt;- mod_v$variables()\n\n\n3.3 サンプリング\n\ndata_list &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000 # print update every 1000 iters\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.3 seconds.\n\n\n返り値 fit は CmdStanMCMC オブジェクトであり，summary() などのメソッドが使用可能である．\nsummary() メソッドは，posterior パッケージのメソッド summarise_draws() を自動で使うようになっている．\n\nfit$summary()\n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad      q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -7.30  -7.00  0.797 0.345 -8.86   -6.75   1.00    1923.    2017.\n2 theta     0.257  0.241 0.124 0.127  0.0800  0.485  1.00    1232.    1477.\n\nfit$summary(variables = c(\"theta\", \"lp__\"), \"mean\", \"sd\")\n\n# A tibble: 2 × 3\n  variable   mean    sd\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 theta     0.257 0.124\n2 lp__     -7.30  0.797\n\n\n同様にして draws() メソッドで bayesplot パッケージが呼び出される．\n\nmcmc_hist(fit$draws(\"theta\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/2024/R/Stan2.html#文献紹介",
    "href": "posts/2024/R/Stan2.html#文献紹介",
    "title": "R 上の Stan インターフェイス",
    "section": "4 文献紹介",
    "text": "4 文献紹介"
  },
  {
    "objectID": "posts/2024/R/Stan2.html#footnotes",
    "href": "posts/2024/R/Stan2.html#footnotes",
    "title": "R 上の Stan インターフェイス",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n実際には，最初に CmdStanModel オブジェクトを生成し，compile() メソッドを呼び出している．これが compile = TRUE フラッグの存在意義である．↩︎"
  },
  {
    "objectID": "posts/2024/R/YUIMA2.html",
    "href": "posts/2024/R/YUIMA2.html",
    "title": "YUIMA による確率過程の統計推測",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/R/YUIMA2.html#setyuimaコンストラクタ",
    "href": "posts/2024/R/YUIMA2.html#setyuimaコンストラクタ",
    "title": "YUIMA による確率過程の統計推測",
    "section": "1 setYuimaコンストラクタ",
    "text": "1 setYuimaコンストラクタ\nsetYuima(data, model, sampling, characteristic, functional)\ndata を yuima オブジェクトに持たせた場合，データからパラメータ推定を行うことができる．\ndata &lt;- read.csv(\"http://chart.yahoo.com/table.csv?s=IBM&g=d&x=.csv\")\nx &lt;- setYuima(data = setData(data$Close))\nstr(x@data)"
  },
  {
    "objectID": "posts/2024/R/YUIMA2.html#擬似最尤推定",
    "href": "posts/2024/R/YUIMA2.html#擬似最尤推定",
    "title": "YUIMA による確率過程の統計推測",
    "section": "2 擬似最尤推定",
    "text": "2 擬似最尤推定\n\\(r\\)-次元 Wiener 過程 \\(\\{W_t\\}\\) が定める拡散過程 \\[\ndX_t=a(X_t,\\theta_2)\\,dt+b(X_t,\\theta_1)\\,dW_t\n\\] のパラメータ \\(\\theta_1\\in\\Theta_1\\subset\\mathbb{R}^p,\\theta_2\\in\\Theta_2\\subset\\mathbb{R}^q\\) の推定を考えたい．\n\n2.1 概要\nデータ \\[\n\\boldsymbol{X}_n:=(X_{t_i})_{i=0}^n,\\qquad t_i:=i\\Delta_n\n\\] の対数尤度は次の 擬似対数尤度 を用いて近似できる： \\[\n\\ell_n(\\boldsymbol{X}_n,\\theta)=-\\frac{1}{2}\\sum_{i=1}^n\\left(\\log\\det(\\Sigma_{i-1}(\\theta_1))+\\frac{1}{\\Delta_n}\\Sigma_{i-1}^{-1}(\\theta_1)\\biggr((\\Delta X_i-\\Delta_na_{i-1}(\\theta_2))^{\\otimes 2}\\biggl)\\right)\n\\] ただし \\[\n\\Delta X_i:=X_{t_i}-X_{t_{i-1}},\\quad\\Sigma_i(\\theta_1):=\\Sigma(\\theta_1,X_{t_i})\n\\] \\[\na_i(\\theta_2):=a(X_{t_i},\\theta_2),\\quad\\Sigma:=b^{\\otimes2},\\quad A^{\\otimes2}:=AA^\\top\n\\] とした．\nこの擬似対数尤度 \\(\\ell_n(\\boldsymbol{X}_n,\\theta)\\) を用いて， \\[\n\\widehat{\\theta}:=\\operatorname*{argmax}_{\\theta}\\ell_n(\\boldsymbol{X}_n,\\theta)\n\\] と定まる \\(M\\)-推定量を 擬似最尤推定量 という．\nこれを実装する qmle 関数が， stats4 標準の mle 関数 に似せて作られている．\nqmle(yuima, start, method = \"L-BFGS-B\", fixed = list(), print = FALSE, envir = globalenv(), lower, upper, joint = FALSE, Est.Incr =\"NoIncr\", aggregation = TRUE, threshold = NULL, rcpp =FALSE, ...)\nstart は最適化を始める初期値を，名前に対応づける辞書の形で与える．yuima オブジェクトは model と data のスロットが埋められていなければならない．最適化は BFGS によって行われる．\nジャンプ過程への応用も今後予定されている．\n\n\n2.2 例\n例えば，次のモデル \\[\ndX_t=(2-\\theta_2X_t)dt+(1+X_t^2)^{\\theta_1}dW_t,\\quad X_0=1,\n\\] を考え，真値を \\(\\theta_1=0.2,\\theta_2=0.3\\) としてデータを生成する．\n\nlibrary(yuima)\nymodel &lt;- setModel(drift=\"(2-theta2*x)\", diffusion=\"(1+x^2)^theta1\")\nn &lt;- 750\nysamp &lt;- setSampling(Terminal=n^(1/3), n=n)\nyuima &lt;- setYuima(model=ymodel, sampling=ysamp)\nyuima &lt;- simulate(yuima, xinit=1, true.parameter=list(theta1=0.2, theta2=0.3))\n\nこれに対して，QMLE を実行してみる：\n\nparam.init &lt;- list(theta2=0.5, theta1=0.5)\nlow.par &lt;- list(theta1=0, theta2=0)\nupp.par &lt;- list(theta1=1, theta2=1)\nmle1 &lt;- qmle(yuima, start=param.init, lower=low.par, upper=upp.par)\nsummary(mle1)\n\nQuasi-Maximum likelihood estimation\n\nCall:\nqmle(yuima = yuima, start = param.init, lower = low.par, upper = upp.par)\n\nCoefficients:\n        Estimate Std. Error\ntheta1 0.1785256 0.01337509\ntheta2 0.8495606 0.19301758\n\n-2 log L: -708.956 \n\n\n\\(\\theta_2\\) の推定の方が圧倒的に難しいらしいことがよくわかる．\n\n\n2.3 擬似最尤推定量の性質\n実は高頻度極限 \\(\\Delta_n\\to0,n\\to\\infty\\) において，\\(\\widehat{\\theta}_1\\) は漸近的に混合正規である (Genon-Catalot and Jacod, 1993)．\n\\(\\widehat{\\theta}_2\\) も一致性を持つには，条件 \\(T=n\\Delta_n\\to\\infty\\) が必要である．これは \\(T\\) が有限であるとき \\(\\theta_2\\) の Fisher 情報量も有限であり，それゆえこの設定したでは一致推定量が存在しないためである．\n\\(T\\to\\infty\\) の下でエルゴード条件を仮定すれば，一致性だけでなく漸近正規性も達成される．\n局所 Gauss 近似を成立させるためには \\(n\\Delta_n^2\\to0\\) が必要であるが，これでは条件が強すぎる．任意の \\(p\\ge2\\) に対して条件 \\(n\\Delta_n^p\\to0\\) しか満たさない場合での適応的推定法が (Uchida and Yoshida, 2012) によって，大偏差原理による議論に基づいて提案された．\n\\(T\\) が大きくない場合， \\(\\theta_2\\) には小標本的な影響が現れる．これは適応的 Bayes 推定（第 3 節）でも議論する．\nなお適応的 Bayes 事後平均推定量も，上述の \\(\\widehat{\\theta}_1,\\widehat{\\theta}_2\\) と全く同じ漸近的性質を持つ．"
  },
  {
    "objectID": "posts/2024/R/YUIMA2.html#sec-adaBayes",
    "href": "posts/2024/R/YUIMA2.html#sec-adaBayes",
    "title": "YUIMA による確率過程の統計推測",
    "section": "3 適応的 Bayes 推論",
    "text": "3 適応的 Bayes 推論\n\n\n\n\n\n\n\nR-Forge 上の Documentation\nR-Forge 上の Source\nGitHub 上の Source\n\n\n\n\n\\(r\\)-次元モデル\n\\[\ndX_t=a(X_t,\\theta_2)dt+b(X_t,\\theta_1)dW_t\n\\]\nの擬似対数尤度\n\\[\n\\ell_n(\\boldsymbol{X}_n,\\theta)=-\\frac{1}{2}\\sum_{i=1}^n\\left(\\log\\det(\\Sigma_{i-1}(\\theta_1))+\\frac{1}{\\Delta_n}\\Sigma_{i-1}^{-1}(\\theta_1)\\biggr((\\Delta X_i-\\Delta_na_{i-1}(\\theta_2))^{\\otimes 2}\\biggl)\\right)\n\\]\nを考える．\n\n3.1 概要\nまず，任意に初期値 \\(\\theta^\\star_2\\in\\Theta_2\\) を取り，\\(\\theta_1\\) に事前分布 \\(\\pi_1\\) を導入して，これに基づいて Bayes 事後平均推定量 を \\[\n\\widetilde{\\theta}_1:=\\frac{\\int_{\\Theta_1}\\theta_1\\exp(\\ell_n(\\boldsymbol{X}_n,(\\theta_1,\\theta_2^\\star)))\\pi_1(\\theta_1)d\\theta_1}{\\int_{\\Theta_1}\\exp(\\ell_n(\\boldsymbol{X}_n,(\\theta_1,\\theta_2^\\star)))\\pi_1(\\theta_1)d\\theta_1}\n\\] と定める．\n\\(\\pi_1\\) が \\(\\Theta_1\\) 全域を台に持つならば，高頻度極限で良い漸近的性質を持つことは保証される（第 2.3 節）．\n続いて，\\(\\theta_2\\) に事前分布 \\(\\pi_2\\) を導入して，\\(\\widetilde{\\theta}_1\\) から Bayes 事後平均推定量 を \\[\n\\widetilde{\\theta}_2:=\\frac{\\int_{\\Theta_2}\\theta_1\\exp(\\ell_n(\\boldsymbol{X}_n,(\\widetilde{\\theta}_1,\\theta_2)))\\pi_2(\\theta_2)d\\theta_2}{\\int_{\\Theta_2}\\exp(\\ell_n(\\boldsymbol{X}_n,(\\widetilde{\\theta}_1,\\theta_2)))\\pi_2(\\theta_2)d\\theta_2}\n\\] と定める．\n\n\n3.2 例\n同様のモデル \\[\ndX_t=(2-\\theta_2X_t)dt+(1+X_t^2)^{\\theta_1}dW_t,\\quad X_0=1,\n\\] を考え，真値を \\(\\theta_1=0.2,\\theta_2=0.3\\) としてデータを生成する．\n\nymodel &lt;- setModel(drift=\"(2-theta2*x)\", diffusion=\"(1+x^2)^theta1\")\nn &lt;- 750\nysamp &lt;- setSampling(Terminal=n^(1/3), n=n)\nyuima &lt;- setYuima(model=ymodel, sampling=ysamp)\nyuima &lt;- simulate(yuima, xinit=1, true.parameter=list(theta1=0.2, theta2=0.3))\n\n加えて，一様事前分布を用意する．\n\nprior &lt;- list(theta2=list(measure.type=\"code\", df=\"dunif(theta2,0,1)\"),\n      theta1=list(measure.type=\"code\", df=\"dunif(theta1,0,1)\"))\nbayes1 &lt;- adaBayes(yuima, start=param.init, prior=prior, mcmc=1000)\nsummary(mle1)\n\nQuasi-Maximum likelihood estimation\n\nCall:\nqmle(yuima = yuima, start = param.init, lower = low.par, upper = upp.par)\n\nCoefficients:\n        Estimate Std. Error\ntheta1 0.1785256 0.01337509\ntheta2 0.8495606 0.19301758\n\n-2 log L: -708.956 \n\n\n\nbayes1@coef\n\n   theta1    theta2 \n0.1866106 0.4459654 \n\n\n\nstr(bayes1)\n\nFormal class 'adabayes' [package \"yuima\"] with 6 slots\n  ..@ mcmc       :List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ accept_rate:List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ coef       : Named num [1:2] 0.187 0.446\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n  ..@ call       : language adaBayes(yuima = yuima, start = param.init, prior = prior, mcmc = 1000)\n  ..@ vcov       : num [1:2, 1:2] 0 0 0 0\n  ..@ fullcoef   : Named num [1:2] 0.187 0.446\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n\n\n\n\n3.3 小標本性がドリフト推定に与えるバイアス\nドリフト係数 \\(a(X_t,\\theta_2)\\) に関する推定は，\\([0,T]\\) の長さに強い影響を受けることが理論的にも知られている．\n数値実験では \\(n=750\\) かつ \\[\nT=n^{\\frac{1}{3}}\\approx9.09\n\\] と取った．\nこれをさらに \\(n=500,T\\approx7.94\\) とすると，\n\nn &lt;- 500\nysamp &lt;- setSampling(Terminal=n^(1/3), n=n)\nyuima &lt;- setYuima(model=ymodel, sampling=ysamp)\nyuima &lt;- simulate(yuima, xinit=1, true.parameter=list(theta1=0.2, theta2=0.3))\nmle2 &lt;- qmle(yuima, start=param.init, lower=list(theta1=0, theta2=0), upper=list(theta1=1, theta2=1))\nbayes2 &lt;- adaBayes(yuima, start=param.init, prior=prior, mcmc=1000)\n\n\nsummary(mle2)\n\nQuasi-Maximum likelihood estimation\n\nCall:\nqmle(yuima = yuima, start = param.init, lower = list(theta1 = 0, \n    theta2 = 0), upper = list(theta1 = 1, theta2 = 1))\n\nCoefficients:\n        Estimate Std. Error\ntheta1 0.2030917 0.01011359\ntheta2 0.3611649 0.13888986\n\n-2 log L: -18.11409 \n\nbayes2@coef\n\n   theta1    theta2 \n0.2077571 0.3706500 \n\nstr(bayes2)\n\nFormal class 'adabayes' [package \"yuima\"] with 6 slots\n  ..@ mcmc       :List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ accept_rate:List of 1\n  .. ..$ : chr \"NULL\"\n  ..@ coef       : Named num [1:2] 0.208 0.371\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n  ..@ call       : language adaBayes(yuima = yuima, start = param.init, prior = prior, mcmc = 1000)\n  ..@ vcov       : num [1:2, 1:2] 0 0 0 0\n  ..@ fullcoef   : Named num [1:2] 0.208 0.371\n  .. ..- attr(*, \"names\")= chr [1:2] \"theta1\" \"theta2\"\n\n\n小標本でも適応的 Bayes 推定量はよく振る舞う．一方で，QMLE では劣化が激しい．\\(\\widehat{\\theta}_1\\) は小標本の影響を \\(\\widehat{\\theta}_2\\) ほどは大きく受けない．"
  },
  {
    "objectID": "posts/2024/R/YUIMA2.html#その他の機能",
    "href": "posts/2024/R/YUIMA2.html#その他の機能",
    "title": "YUIMA による確率過程の統計推測",
    "section": "4 その他の機能",
    "text": "4 その他の機能\n\n4.1 共分散推定\n２つの伊藤過程が非同期的に離散観測されたとして，２つの間の共分散を推定する (Hayashi and Yoshida, 2005) 推定量も yuima には実装されている．\n(Brouste et al., 2014, p. 第6.4節) を参照．"
  },
  {
    "objectID": "posts/2024/R/YUIMA2.html#これから",
    "href": "posts/2024/R/YUIMA2.html#これから",
    "title": "YUIMA による確率過程の統計推測",
    "section": "5 これから",
    "text": "5 これから\n\n\nsetYuima() の help ページに ‘PLEASE FINISH THIS’ が２箇所ある．\nhelp ページによく出てくる yuimadocs パッケージとは？\ncoef(summary(bayes1))はエラーを生じる．\n\nError in object$coefficients : $ operator is invalid for atomic vectors Calls: .main … eval_with_user_handlers -&gt; eval -&gt; eval -&gt; coef -&gt; coef -&gt; coef.default\n\nqmle は mle 様に summary メソッドを持っているが，adaaBayes はまだであるようである．\nadaBayes の後ろの方に必須の引数 mcmc がある．Manpage を読む限り iteration と同じ役割では？"
  },
  {
    "objectID": "posts/2024/R/Stan1.html",
    "href": "posts/2024/R/Stan1.html",
    "title": "Stan 入門",
    "section": "",
    "text": "GitHub, Documentation, Reference Manual．\n\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#リンク集",
    "href": "posts/2024/R/Stan1.html#リンク集",
    "title": "Stan 入門",
    "section": "",
    "text": "GitHub, Documentation, Reference Manual．\n\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA と Stan を用いた確率過程のベイズ推定入門\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#stan-言語の基本文法",
    "href": "posts/2024/R/Stan1.html#stan-言語の基本文法",
    "title": "Stan 入門",
    "section": "1 Stan 言語の基本文法",
    "text": "1 Stan 言語の基本文法\n\n1.1 はじめに\nStan 言語は確率モデルを３つのブロックに分けて記述する．その３つとはデータ，パラメータとモデルである．\n\n以上の３要素により，事後分布が定まる．それぞれの要素は，対応した名前を持ったスコープ {} 内で，この順番で定義される慣習がある．\n以前のスコープ内で定義された識別子は，その後のスコープでも利用可能になる．\n\n\nstan_code.stan\n\ndata {\n    int&lt;lower=0&gt; N;  // N &gt;= 0\n    array[N] int&lt;lower=0, upper=1&gt; y;  // y[n] in {0, 1}\n}\nparameters {\n    real&lt;lower=0, upper=1&gt; theta;  // theta in [0, 1]\n}\nmodel {\n    theta ~ beta(1, 1);  // uniform prior\n    y ~ bernoulli(theta);  // observation model\n}\n\nその後 Stan プログラムは事後分布の対数密度を表す C++ 関数にコンパイルされる．\n最終的にこの対数尤度を用いて，その勾配を自動微分により計算し，Hamiltonian Monte Carlo による事後分布サンプリングを実行する．\nStan エコシステムの詳説は次節 2 に回し，ここでは Stan 言語の基本に集中する．\n\n\n1.2 確率的プログラミング言語\nStan 言語のように確率モデルを記述する（より正確には事後分布の尤度を記述する高級）言語を 確率的プログラミング言語 (PPL: Probabilistic Programming Language) という．\n最初期の確率的プログラミング言語の１つに WinBUGS (Lunn et al., 2000) の実装に代表される BUGS (Bayesian analysis Using Gibbs Sampling) がある．\nBUGS では事後分布を詳細に 有向グラフィカルモデル (DAG) で記述し，Gibbs Sampling により事後分布をサンプリングするという仕組みであった．\nStan 言語はさらに柔軟に，（正規化されているとは限らない）対数尤度を記述できるようになっている．\n理想的にはあらゆる確率モデルを扱いたいものであるが，現状の Stan 言語はパラメータとしては連続変数のみを持つモデルのみが定義可能である．\n\n\n1.3 data ブロック\nStan は静的な型システムを持ち，変数宣言の際には必ず型を指定する必要がある．\n\n\n\n\n\n\nStan の代表的なデータ型\n\n\n\n\n実数 \\(x\\in\\mathbb{R}\\)\nreal x;\n実数 \\(x\\in[a,b]\\)\nreal&lt;lower=a upper=b&gt; x;\n単体 \\(x\\in[0,1]^N,\\sum_{n=1}^Nx_n=1\\)\nsimplex[N] x;\n自然数 \\(N\\in\\mathbb{N}\\)\nint&lt;lower=0&gt; N;\n配列 \\(x\\in\\mathbb{R}^N\\)：純粋なコンテナ型であり，線型代数ライブラリは適用不可．\narray[N] real x;\nreal x[N] という記法は Stan 2.26 以降使われないことに注意（docs 参照）．\n\n\n\nなお，各ブロック内において，あらゆる変数宣言は全ての非宣言的文の前に来る必要がある．\n{\n    real variable1 = 5;\n    variable1 /= 2;  // ここでエラー\n    real variable2 = exp(variable1);\n}\n\n\n1.4 transformed data ブロック\ndata ブロックでは許されないが，一般に Stan 言語では変数宣言と同時に代入もできる．\n代入を省略した場合は NaN によって初期化される．\n\n\n\n\n\n\nStan の代表的な線型代数関連のデータ型\n\n\n\n\nベクトル \\(x\\in\\mathbb{R}^5\\)：c++ の線型代数ライブラリが使える\nvector[5] x = [0, 1, 2, 3, 4];\n横ベクトル \\(y\\in(\\mathbb{R}^5)^*\\)：\\(y*x\\) が計算可能．\nrow_vector[5] y = [1, 2, 3, 4, 5]';\nx*y; // 計算可能\n行列 \\(A\\in M_{N,M}(\\mathbb{R})\\)\nmatrix[N, M] A;\n\n\n\ntransformed data ブロックでは，観測される訳でもなければパラメータでもないような，内部で使われる変数が定義される．\n\n\n1.5 parameters ブロック\nparameters ブロックも同様にして変数を宣言する．\nmodel ブロックで使われる変数は，data, parameters ブロックのいずれかで宣言されている必要がある．\n\\(x\\in[a,b]^N\\) のような制約領域を持つ変数が parameters ブロックで宣言された場合，Stan は内部でパラメータ変換を行い，\\([a,b]\\) を \\(\\mathbb{R}\\) 上に写してサンプリングを実行する．\nGamma 分布のように台が \\(\\mathbb{R}\\) の部分集合になるような事前分布を扱う場合，対応する制約領域をパラメータに定義することがサンプリングの効率を上げる．\n一方で，\\(\\mathbb{R}\\) 全体を台に持つ事前分布を持つパラメータに対して制約領域を宣言した場合，truncate をした事前分布を定義したことに等価になる．\n\n\n1.6 model ブロック\nモデルブロックでは対数密度関数の値が変数 target として保持されており，target() 関数でアクセス可能である．\n次の３つの文は等価になる：\nbeta ~ normal(0, 1);\nbeta ~ normal(beta | 0, 1);  // syntax sugar\ntarget += normal_lpdf(beta | 0, 1);  // 実際の処理に近い\n\n\n1.7 ループと制御\nfor (n in N1:N2) {\n  // Statements executed for each N1 &lt;= n &lt;= N2\n}\n\nif (condition) {\n  // Statements evaluated if condition is true\n} else {\n  // Statements evaluated if condition is false\n}\n\n\n1.8 transformed parameters / Generated Quantities ブロック\ntransformed parameters ブロックと Generated Quantities ブロックでは，parameters ブロックで宣言されたパラメータの関数として推定対象を定義する．\nこの推定対象は推論後に事後平均が取られて報告される．\n２つの違いは，transformed parameters ブロックでは model ブロックの前に配置され，model ブロックでも使えるのに対し，Generated Quantities ブロックでは model ブロックの後に配置され，純粋に事後分布の関数として処理される点である．\nなお，関数定義は functions ブロックで行う．\nfunctions {\n    real baseline(real a1, real a2, real theta) {\n        return a1 * theta + a2;\n    }\n}"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#sec-stan-ecosystem",
    "href": "posts/2024/R/Stan1.html#sec-stan-ecosystem",
    "title": "Stan 入門",
    "section": "2 Stan エコシステムの概観",
    "text": "2 Stan エコシステムの概観\n\n2.1 Stan の推論エンジン\nStan の推論エンジンとして，HMC の他に２つの C++ アルゴリズムが用意されており，それぞれ BFGS 法による点推定と変分推定を実装している．\n\n\n2.2 Stan 数学ライブラリ\nStan 言語により定義された確率モデル（対数密度関数）が実際に評価可能にするための C++ 関数のライブラリである．\n使える関数のリストは Stan Functions Reference を参照．\n基本的な数学的関数や統計的関数に加えて，自動微分が実装されており，対数密度関数の勾配や Hessian も計算可能である．\n\n\n2.3 Stan インターフェイス\n\n2.3.1 CmdStan の仕組み\nCmdStan は makefiles の集合からなる最も軽量な，コマンドラインベースのインターフェイスである．\nこれを直接 R で wrap した CmdStanR パッケージ や CmdStanPy パッケージが存在し，同時に Julia Stan.jl, Mathematica MathematicaStan, Matlab MatlabStan, Stata StataStan からも利用可能である．\nStan の Math ライブラリ，Algorithm ライブラリなどの出力をテキストファイルで出力してやり取りする．\nCmdStan は最も軽量なインターフェイスであり，Stan の性能を純粋に引き出す場合に使われる．\n\n\n2.3.2 CmdStan のインストール\nCmdStan Installation によると，conda による方法とソースからのインストールの2つの方法がある．\n一方で次稿で扱う CmdStanR を通じてインストールすることもできる：\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n2.3.3 RStan と PyStan\nR と Python という２大言語を Stan と直接繋げるインターフェイスを提供している．\nCmdStan のように一度テキストファイルに書き出すということがなく，メモリ上でやり取りされるが，それ故に CmdStan よりも追加の処理が多くなりがちである．"
  },
  {
    "objectID": "posts/2024/R/Stan1.html#文献紹介",
    "href": "posts/2024/R/Stan1.html#文献紹介",
    "title": "Stan 入門",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n手軽に概要を掴むには Michael Betancourt によるブログ記事 An introduction to Stan が良い．\nより本格的な解説論文には (Gelman et al., 2015), (Carpenter et al., 2017) がある．\n公式の文献紹介 が stan.org から出ているが，情報が古い．\nまた，Stan には 日本語のマニュアル もある：stan-ja (GitHub)．"
  },
  {
    "objectID": "posts/2024/R/adastan.html",
    "href": "posts/2024/R/adastan.html",
    "title": "SDE のベイズ推定入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nR の YUIMA パッケージに関する詳細は，次の記事も参照："
  },
  {
    "objectID": "posts/2024/R/adastan.html#問題提起",
    "href": "posts/2024/R/adastan.html#問題提起",
    "title": "SDE のベイズ推定入門",
    "section": "1 問題提起",
    "text": "1 問題提起\n\n1.1 はじめに\n確率過程の統計推測を行うための R パッケージ yuima では多次元の SDE \\[\ndX_t=b_t^\\theta(X_t)\\,dt+\\sigma_t^\\phi(X_t)\\,dW_t,\\qquad X_0\\in\\mathbb{R}^d,\n\\] に対してパラメータ \\(\\theta\\in\\mathbb{R}^{d_\\theta},\\phi\\in\\mathbb{R}^{d_\\phi}\\) の推定を実行することができる（YUIMA の記事 も参照）．\n\n\n\n\n\n\n\n\nYUIMA 入門\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による汎函数計算\n\n\n漸近展開と setFunctional()\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\n\n\n\n\nYUIMA による確率過程の統計推測\n\n\n擬似尤度推定量，一般化 Bayes 事後平均\n\n\n\n2024-05-18\n\n\n\n\n\n\n\n\nNo matching items\n\n\nYUIMA におけるパラメータ \\(\\theta,\\varphi\\) の推定には，qmle() による擬似最尤推定量を用いることだけでなく，adaBayes() を通じて，一般化ベイズによる事後平均推定を実行することも可能である．\n現状の adaBayes() 関数では，自前のランダムウォーク MH アルゴリズムや p-CN アルゴリズムを用いているが，HMC (Hamiltonian Monte Carlo) (Duane et al., 1987), (Neal, 1996) も利用可能なように拡張することを考えたい．\nここでは新たな関数 adaStan() を定義することを考える．"
  },
  {
    "objectID": "posts/2024/R/adastan.html#stan-インターフェイスの調査",
    "href": "posts/2024/R/adastan.html#stan-インターフェイスの調査",
    "title": "SDE のベイズ推定入門",
    "section": "4 Stan インターフェイスの調査",
    "text": "4 Stan インターフェイスの調査\n\n4.1 我々のインターフェイス\n前述の OU 過程 3.3\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nのモデル model に対して stan 関数でベイズ推定を実行してみる．パラメータは \\[\n\\begin{pmatrix}\\theta\\\\\\mu\\\\\\sigma\\end{pmatrix}\n=\n\\begin{pmatrix}1\\\\0\\\\0.5\\end{pmatrix}\n\\] として YUIMA を用いてシミュレーションをし，そのデータを与えてパラメータが復元できるかをみる．\n\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(theta = 1, mu = 0, sigma = 0.5), xinit = rnorm(1))\n\n\n# シミュレーション結果\nplot(simulation)\n\n\n\n\n\n\n\n\nさて，このシミュレーション結果から，adaStan() 関数でパラメータが復元できるかを確認しましょう．\n\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\nfit &lt;- adaStan(simulation)\n\n\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\ntheta    0.63    0.26 0.78   -0.30    0.01    0.30    1.11    2.47     9 1.22\nmu       0.55    0.45 6.96  -11.26   -0.30    0.22    0.67   19.25   241 1.01\nsigma    0.52    0.00 0.01    0.50    0.51    0.52    0.53    0.54   595 1.01\nlp__  3059.51    0.28 1.30 3056.61 3058.78 3059.52 3060.53 3061.47    21 1.10\n\nSamples were drawn using NUTS(diag_e) at Fri Mar 28 15:08:12 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nplot(fit)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\n\n\n\nlibrary(\"bayesplot\")\nlibrary(\"rstanarm\")\nlibrary(\"ggplot2\")\n\nposterior &lt;- as.matrix(fit)\nplot_title &lt;- ggtitle(\"Posterior distributions\",\n                      \"with medians and 80% intervals\")\n\n\nmcmc_areas(posterior,\n           pars = c(\"theta\", \"mu\", \"sigma\"),\n           prob = 0.8) + plot_title\n\n\n\n\n\n\n\n\n\\(\\sigma\\) の推定はよくできているが \\(\\mu\\) の精度はあまりよくなく，\\(\\theta\\) はバイアスがある様で，自信を持って間違えることも多い．\n\n\n4.2 brms パッケージ\nbrms や rethinking も，背後で Stan を利用している．これらが文字式をどのように取り扱っているかを調査する．\n\n\n\n\n\n\n\n\n\n\nbrms によるベイズ混合モデリング入門\n\n\nポアソン混合効果モデルを例に\n\n\n\n2024-05-12\n\n\n\n\n\n\n\n\nNo matching items\n\n\nStan コードを扱っている関数は .stancode() であった．\n最終的に，.compile_model_rstan() と .fit_model_rstan() が呼ばれるようになっている．\n最終的にはこれらの関数も，第 3.2 節で用意した我々の実装と同様の要領で gsub と paste0 を使って Stan コードを生成していた．\n\n\n\n\n\n\npaste0 と paste の違い\n\n\n\n\n\n\n# paste0() の使用例\nresult1 &lt;- paste0(\"Hello\", \"world\")\nprint(result1)  # \"Helloworld\"\n\n[1] \"Helloworld\"\n\n# paste() の使用例\nresult2 &lt;- paste(\"Hello\", \"world\")\nprint(result2)  # \"Hello world\"\n\n[1] \"Hello world\"\n\nresult3 &lt;- paste(\"Hello\", \"world\", sep = \"-\")\nprint(result3)  # \"Hello-world\"\n\n[1] \"Hello-world\"\n\n\n\n\n\n\n\n4.3 cmdstanr パッケージ\ncmdstanr パッケージは 2020 年に beta 版がリリースされた，大変新しいパッケージである．\nR6 クラスを用いた現代的な実装がなされている．\n\n\n4.4 RStanArm パッケージ\n詳しくは次稿参照：\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\nNo matching items\n\n\nRStan と RStanArm パッケージでは極めて洗練された方法を持つ．\nstanmodels.R というファイルにおいて，大変洗練された方法で，複数の Stan コードのファイルをコンパイルして，C++ コードへのポインタを格納した stanmodel オブジェクトを生成している．\nStan コードは src/stan_files/ ディレクトリに，bernoulli.stan や count.stan など，８つ保存されている．\nまず各ファイルに対して rstan::stanc() を呼び出して，Stan コードを C++ コードへのポインタに変換している．この際 allow_undefined = TRUE という引数があり，未定義の変数が存在してもエラーは出されず，すぐには実行されない．\nこのファイルでの出力は stanmodel オブジェクトのコンストラクタに渡される．stanmodels は stanmodel オブジェクトのリストである．stanmodel オブジェクトは mk_cppmodule() のスロットを持ち，ここから RCpp モジュールが呼び出せる．\nこのオブジェクトを通じて，stan_glm.fit.R で，stanmodel クラスのオブジェクト stanfit （stanmodels の適切な要素を用いる）に対して rstan::sampling() 関数を呼ぶことで推論が実行される．\nif (algorithm == \"sampling\") {\n      sampling_args &lt;- set_sampling_args(\n        object = stanfit, \n        prior = prior, \n        user_dots = list(...), \n        user_adapt_delta = adapt_delta, \n        data = standata, \n        pars = pars, \n        show_messages = FALSE)\n      stanfit &lt;- do.call(rstan::sampling, sampling_args)\n    }\nこの際の set_sampling_args 関数によって実行されている引数設定が肝心である"
  },
  {
    "objectID": "posts/2024/R/adastan.html#テンプレート操作",
    "href": "posts/2024/R/adastan.html#テンプレート操作",
    "title": "SDE のベイズ推定入門",
    "section": "5 テンプレート操作",
    "text": "5 テンプレート操作\n\n\n\n\n\n\nR の Expression について\n\n\n\n\n\nオブジェクト志向言語ではコード自体もオブジェクトであり，これを R では Expression と呼ぶ．\n\n\n\n\n\n\n\n\n\n\nR（４）メタプログラミング\n\n\nExpression について\n\n\n\n2021-05-07\n\n\n\n\n\n\n\n\nNo matching items\n\n\n１つのクラスからなるわけではなく，call, symbol, constant, pairlist の４つの型からなる．2\n次のような操作ができる3\nrlang::expr がコンストラクタである：\n\nlibrary(rlang)\nz &lt;- expr(y &lt;- x*10)\nz\n\ny &lt;- x * 10\n\n\nexpression オブジェクトは base::eval() で評価できる：\n\nx &lt;- 4\neval(z)\ny\n\n[1] 40\n\n\nexpression には list のようにアクセス可能である：4\n\nf &lt;- expr(f(x = 1, y = 2))\n\nf$z &lt;- 3\nf\n\nf(x = 1, y = 2, z = 3)\n\n\n\nf[[2]] &lt;- NULL\nf\n\nf(y = 2, z = 3)\n\n\n\n\n\n\n5.1 glue パッケージ\ninstall.packages(\"glue\")\nglue （CRAN, Docs）パッケージは文字列リテラルを扱うパッケージである．\n\nlibrary(glue)\nname &lt;- \"  Hirofumi\\n  Shiba\\n\"\nmajor &lt;- \"Mathematics\"\nglue::glue('My name is {name}. I study {major}. Nice to meet you!')  # 名前空間の衝突を避けるために :: を使う\n\nMy name is   Hirofumi\n  Shiba\n. I study Mathematics. Nice to meet you!\n\n\n\nglue::glue(\" real {param};\", param = yuima@model@parameter@all, .collapse = \"\\n\")\n\n real theta;\n real mu;\n real sigma;\n\n\nこれを用いると，adaStan() は次のように可読性が高い形で書き直すことができる：\n\nyuima_to_stan_glued &lt;- function(yuima) {\n  # パラメータの定義部分を作成\n  parameters &lt;- glue::glue(\"real {param};\", param = yuima@model@parameter@all)\n  parameters &lt;- paste(parameters, collapse = \"\\n  \")\n\n  # drift と diffusion の式内の 'x' を 'x[n-1]' に置換\n  drift &lt;- gsub(\"x\", \"x[n-1]\", yuima@model@drift)\n  diffusion &lt;- gsub(\"x\", \"x[n-1]\", yuima@model@diffusion[[1]])\n  \n  # Stanコード全体を作成\n  template &lt;- \n'data {{\n  int N;\n  array[N+1] real x;\n  real T;\n  real h;\n}}\nparameters {{\n  {parameters}\n}}\nmodel {{\n  x[1] ~ normal(0, 1);\n  for(n in 2:(N+1)) {{\n    x[n] ~ normal(x[n-1] + h * {drift}, sqrt(h) * {diffusion});\n  }}\n}}'\n  excode &lt;- glue::glue(template, .trim = FALSE)  # parameters が複数行に渡る場合でも分離して出力しない\n  \n  return(excode)\n}\n\n\nyuima_to_stan_glued(yuima)\n\ndata {\n  int N;\n  array[N+1] real x;\n  real T;\n  real h;\n}\nparameters {\n  real theta;\n  real mu;\n  real sigma;\n}\nmodel {\n  x[1] ~ normal(0, 1);\n  for(n in 2:(N+1)) {\n    x[n] ~ normal(x[n-1] + h * (theta * (mu - x[n-1])), sqrt(h) * (sigma));\n  }\n}\n\n\n\n\n5.2 whisker パッケージ\ninstall.packages(\"whisker\")\nwhisker パッケージ（CRAN, GitHub）は Web を中心に採用されているテンプレートシステム Mustache に基づく機能 whisker.render() を提供している．\nwhisker.render(template, data = parent.frame(), partials = list(),\ndebug = FALSE, strict = TRUE)\n\nlibrary(whisker)\ntemplate &lt;-\n'Hello {{name}}\nYou have just won ${{value}}!\n{{#in_ca}}\nWell, ${{taxed_value}}, after taxes.\n{{/in_ca}}'\ndata &lt;- list(name = \"Hirofumi\"\n, value = 10000\n, taxed_value = 10000 - (10000 * 0.4)\n, in_ca = TRUE\n)\nwhisker.render(template, data)\n\n[1] \"Hello Hirofumi\\nYou have just won $10000!\\nWell, $6000, after taxes.\\n\"\n\n\nMustache の記法は Manual を参照．"
  },
  {
    "objectID": "posts/2024/R/adastan.html#sec-final-form",
    "href": "posts/2024/R/adastan.html#sec-final-form",
    "title": "SDE のベイズ推定入門",
    "section": "2 最終的なコード",
    "text": "2 最終的なコード\nadaStan.R を参照．\n\nsource(\"adaStan.R\")\n\nmodel &lt;- setModel(drift = \"theta*(mu-x)\", diffusion = \"sigma\", state.variable = \"x\", solve.variable = \"x\")\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\nyuima &lt;- setYuima(model = model, sampling = sampling)\nsimulation &lt;- simulate(yuima, true.parameter = c(theta = 1, mu = 0, sigma = 0.5), xinit = rnorm(1))\n\nfit &lt;- adaStan(simulation, iter=2000, rstan=FALSE)\n\n\nfit$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median      sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     3049.    3049.     1.34   1.39   3047.    3051.     1.08    35.5 \n2 theta       0.503    0.342  0.590  0.538    -0.103    1.64   1.15    21.0 \n3 mu         -6.60    -0.525 13.3    1.56    -35.8      3.89   1.39     9.57\n4 sigma       0.528    0.529  0.0118 0.0128    0.508    0.544  1.09    30.0 \n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\nmcmc_hist(fit$draws(\"theta\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n面白い！事後分布は２峰性をもっており，2000 回繰り返すと事後平均はかろうじて正解に近づいている．"
  },
  {
    "objectID": "posts/2024/R/adastan.html#終わりに",
    "href": "posts/2024/R/adastan.html#終わりに",
    "title": "SDE のベイズ推定入門",
    "section": "6 終わりに",
    "text": "6 終わりに\n\n\nGPT o1 によるアイデア"
  },
  {
    "objectID": "posts/2024/R/adastan.html#footnotes",
    "href": "posts/2024/R/adastan.html#footnotes",
    "title": "SDE のベイズ推定入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n例えば Cox 回帰など．Cox 回帰の Stan での実装は こちらの記事 も参照．↩︎\n(Wickham, 2019) 第17章２節．↩︎\n(Wickham, 2019) 第18章↩︎\n(Wickham, 2019) 第17章２節．↩︎"
  },
  {
    "objectID": "posts/2024/R/YUIMA1.html",
    "href": "posts/2024/R/YUIMA1.html",
    "title": "YUIMA による汎函数計算",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/R/YUIMA1.html#場面設定",
    "href": "posts/2024/R/YUIMA1.html#場面設定",
    "title": "YUIMA による汎函数計算",
    "section": "1 場面設定",
    "text": "1 場面設定\n例えば，\\(d\\)-次元拡散過程 \\(X=(X^{(\\epsilon)}_t)_{t\\in[0,T]},\\epsilon\\in(0,1]\\) を次のように定める： \\[\nX^{(\\epsilon)}_t=x_0+\\int^t_0a(X_s^{(\\epsilon)},\\epsilon)ds+\\int^t_0b(X_s^{(\\epsilon)},\\epsilon)dW_s.\n\\]\nただし，\\(W_t\\) は \\(r\\)-次元 Wiener 過程とした．そして，その汎函数 \\[\nF^{(\\epsilon)}=\\sum_{\\alpha=0}^r\\int^T_0f_\\alpha(X_t^{(\\epsilon)},\\epsilon)dW_t^\\alpha+F(X_T^{(\\epsilon)},\\epsilon),\\quad W^0_t=t\n\\] を推定したいとする．\n\n\n\n\n\n\nBlack-Scholes モデルにおける Asian option の価格\n\n\n\n例えば，Black-Scholes モデル \\[\ndX_t^{(\\epsilon)}=\\mu X_t^{(\\epsilon)}dt+\\epsilon X_t^{(\\epsilon)}dW_t\n\\] において，利子率が零である場合のアジアオプションの価格は \\[\n\\operatorname{E}\\left[\\max\\left(\\frac{1}{T}\\int^T_0X_t^{(\\epsilon)}dt-K,0\\right)\\right]\n\\] と表せる．\nこれは線型汎函数である．実際， \\[\nF^{(\\epsilon)}=\\frac{1}{T}\\int^T_0X_t^{(\\epsilon)}dt,\\quad r=1\n\\] と定めた場合に相当する．\nまた，\\(F^{(\\epsilon)}=X^{(\\epsilon)}_T\\) とした場合に当たる \\[\n\\operatorname{E}[(X^{(\\epsilon)}_T-K)\\lor0]\n\\] がヨーロピアンコールオプションの価格になる．\n\n\nこのように式で表せても，特に Asian option についてはこの線型な設定においてさえ数値計算法が要請される．"
  },
  {
    "objectID": "posts/2024/R/YUIMA1.html#渡部理論",
    "href": "posts/2024/R/YUIMA1.html#渡部理論",
    "title": "YUIMA による汎函数計算",
    "section": "2 渡部理論",
    "text": "2 渡部理論\nここでは，\\(\\epsilon\\searrow0\\) の極限で系が決定論的であるとする．すなわち， \\[\nb(-,0)=0\n\\] \\[\nf_\\alpha(-,0)=0\\quad(\\alpha\\in[r])\n\\] とする．すると \\(X_t^{(0)}\\) は次の常微分方程式 \\[\n\\frac{d X_t^{(0)}}{d t}=a(X_t^{(0)},0),\\quad X_0^{(0)}=x_0\n\\] の解であるから，\\(F^{(0)}\\) も定数 \\[\nF^{(0)}=\\int^T_0f_0(X_t^{(0)},0)dt+F(X_T^{(0)},0)\n\\] で与えられる．\nさらに，\\(a,b,f_\\alpha,F\\) がしかるべき正則性条件を満たすとき，汎函数 \\(F^{(\\epsilon)}\\) にはある版が存在して \\(\\epsilon\\in[0,1)\\) に関して殆ど確実に滑らかである．特に， \\[\n\\widetilde{F}^{(\\epsilon)}:=\\frac{F^{(\\epsilon)}-F^{(0)}}{\\epsilon}\n\\] は次の確率展開を持つ： \\[\n\\widetilde{F}^{(\\epsilon)}\\sim\\widetilde{F}^{[0]}+\\epsilon\\widetilde{F}^{[1]}+\\epsilon^2\\widetilde{F}^{[2]}+\\cdots\\quad(\\epsilon\\searrow0)\n\\]\nこの展開は，Malliavin 解析の Sobolev 空間において厳密に成り立つ．これを導くのが (Watanabe, 1987) の理論である．\nこれに基づき，汎函数 \\(F^{(\\epsilon)}\\) の近似を構成する機能が yuima に実装されている．\nこの漸近展開をオプションの価格付けに応用したのが (Yoshida, 1992) である．"
  },
  {
    "objectID": "posts/2024/R/YUIMA1.html#setfunctionalコンストラクタ",
    "href": "posts/2024/R/YUIMA1.html#setfunctionalコンストラクタ",
    "title": "YUIMA による汎函数計算",
    "section": "3 setFunctionalコンストラクタ",
    "text": "3 setFunctionalコンストラクタ\nsetFunctional(model, F, f, xinit, e)\nBlack-Scholes モデル \\[\ndX_t^{(\\epsilon)}=\\mu X_t^{(\\epsilon)}dt+\\epsilon X_t^{(\\epsilon)}dW_t\n\\] が定める幾何 Brown 運動 \\((X_t)\\) のパラメータが \\(\\mu=1,x_0=1\\) を満たす場合において，Asian call option の価格は，汎函数 \\[\ng(x):=\\max\\left(F^{(0)}-K+\\epsilon x,0\\right)\n\\] を計算すれば良い．\n\\(F^{(\\epsilon)}\\) の極限 \\(F^{(0)}\\) の値は，関数 F0 を yuima.functional スロットが埋まった yuima オブジェクトに適用することで得られる．\n\nlibrary(yuima)\nmodel &lt;- setModel(drift=\"x\", diffusion=matrix(\"x*e\", 1, 1))\nK &lt;- 100\nyuima &lt;- setYuima(model=model, sampling=setSampling(Terminal=1, n=1000))\nyuima &lt;- setFunctional(yuima, f=list(expression(x/T), expression(0)), F=0, xinit=150, e=0.5)\nF0 &lt;- F0(yuima)\n\n\nprint(F0)\n\n[1] 257.6134\n\n\n\ng &lt;- function(x) {\n  tmp &lt;- (F0 - 100) + (0.5*x)\n  tmp[(0.5*x) &lt; (100-F0)] &lt;- 0\n  tmp\n}\nasymp &lt;- asymptotic_term(yuima, block=10, expression(0), g)\n\n\nstr(asymp)\n\nList of 3\n $ d0: num 159\n $ d1: num -3.93\n $ d2: num [1, 1] 4\n\n\nこれを適切な和をとれば良い．\n\ne = 0.5\nasy1 &lt;- asymp$d0 + e*asymp$d1\nasy1\n\n[1] 156.608\n\nasy2 &lt;- asymp$d0 + e*asymp$d1 + e^2 * asymp$d2\nasy2\n\n         [,1]\n[1,] 157.6082\n\n\nは Asian call price の，それぞれ１次と２次の漸近展開を与える．\nこの設定では対数正規分布に対する Edgeworth 展開によっても計算ができる (Levy, 1992)．"
  },
  {
    "objectID": "posts/2024/R/YUIMA1.html#cir-過程",
    "href": "posts/2024/R/YUIMA1.html#cir-過程",
    "title": "YUIMA による汎函数計算",
    "section": "4 CIR 過程",
    "text": "4 CIR 過程\n\\(X_t\\) が幾何 Brown 運動の場合にしか (Levy, 1992) の近似は用いることはできないが，yuima のアプローチでは可能である\n例えば利子率の期間構造のモデルである CIR 模型 (Cox et al., 1985) \\[\ndX_t=(\\alpha-\\beta X_t)\\,dt+\\sqrt{\\gamma X_t}\\,dW_t\n\\] がこの境界例になっている．"
  },
  {
    "objectID": "posts/2024/R/YUIMA1.html#例",
    "href": "posts/2024/R/YUIMA1.html#例",
    "title": "YUIMA による汎函数計算",
    "section": "5 例",
    "text": "5 例\n\\(X\\) が \\[\ndX_t=0.9X_tdt+\\epsilon\\sqrt{X_t}dW_t,\\quad X_0=1,\n\\] である場合の \\(K=10,\\epsilon=0.4\\) における European call option の価格を考える．\n\na &lt;- 0.9; e &lt;- 0.4; Terminal &lt;- 3; xinit &lt;- 1; K &lt;- 10\ndrift &lt;- \"a * x\"; diffusion &lt;- \"e * sqrt(x)\"\nmodel &lt;- setModel(drift = drift, diffusion = diffusion)\nn &lt;- 1000 * Terminal\nyuima &lt;- setYuima(model = model, sampling = setSampling(Terminal = Terminal, n = n))\nf &lt;- list(c(expression(0)), c(expression(0)))\nF &lt;- expression(x)\nyuima.ae &lt;- setFunctional(yuima, f = f, F = F, xinit = xinit, e = e)\nrho &lt;- expression(0)\nF1 &lt;- F0(yuima.ae)\n\nget_ge &lt;- function(x, epsilon, K, F0) {\n  tmp &lt;- (F0 - K) + (epsilon * x[1])\n  tmp[(epsilon * x[1]) &gt; (K - F0)] &lt;- 0\n  return(-tmp)\n}\ng &lt;- function(x) {\n  return(get_ge(x, e, K, F1))\n}\ntime1 &lt;- proc.time()\nasymp &lt;- asymptotic_term(yuima.ae, block = 100, rho, g)\ntime2 &lt;- proc.time()\n\n\nae.value0 &lt;- asymp$d0\nae.value0\n\n[1] 0.7219652\n\nae.value1 &lt;- asymp$d0 + e * asymp$d1\nae.value1\n\n[1] 0.5787545\n\nae.value2 &lt;- as.numeric(asymp$d0 + e * asymp$d1 + e^2 * asymp$d2)\nae.value2\n\n[1] 0.5617722\n\nae.time &lt;- time2 - time1\nae.time\n\n   ユーザ   システム       経過  \n     0.537      0.016      0.561 \n\n\nこの状態での European call option の価格は，２次までの漸近展開が与える値が 100 万データ数による Monte Carlo 推定量の精度に匹敵し，当然計算量は漸近展開の方が圧倒的に少ない．"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html",
    "href": "posts/2024/R/YUIMA.html",
    "title": "YUIMA 入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html#はじめにyuimaとは",
    "href": "posts/2024/R/YUIMA.html#はじめにyuimaとは",
    "title": "YUIMA 入門",
    "section": "1 はじめに：YUIMAとは？",
    "text": "1 はじめに：YUIMAとは？\nR パッケージ yuima は，Lévy 過程または分数 Brown 運動が駆動する確率微分方程式が定める過程という，極めて一般的なクラスの確率過程を扱うための基盤パッケージである．概観には (Brouste et al., 2014) が良い．\n加えて，シミュレーションと推論のためのメソッドが提供されている．\nR で通常の統計推測に用いられる formula オブジェクト のように，直感的な記法でモデル（yuima.modelオブジェクト）を定義できる．\nyuima.modelオブジェクトに対してtoLatex()関数を適用すると，モデルを LaTeX 記法で記述した文字列に変換することもできる．1\n\n\n\n\n\n\nリンク集\n\n\n\n\nR-Forge2\nStable version on CRAN.\nGitHub\nyuimaGUI package\nrdrr.io ページ3\n論文 (Brouste et al., 2014)：パッケージ開発者らによるオープンアクセス論文．\n\n\n\n\n\n\n\n\n\nインストール\n\n\n\ninstall.packages(\"yuima\", repos=\"http://R-Forge.R-project.org\")\n\nlibrary(yuima)\n\n??yuima  # Man pageを開く"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html#yuima-の構造と-setmodel",
    "href": "posts/2024/R/YUIMA.html#yuima-の構造と-setmodel",
    "title": "YUIMA 入門",
    "section": "2 YUIMA の構造と setModel()",
    "text": "2 YUIMA の構造と setModel()\n\n2.1 クラス構造\n各クラスのが持つスロット（上段）とメソッド（下段），そして継承関係は以下の通りである．\n\n\n\n\n\n\n\nClassDiagram\n\n\n\nyuima\n\nyuima\n\n+model: yuima.model\n+sampling: yuima.sampling\n+data: yuima.data\n- characteristic: yuima.characteristic\n- functional: yuima.functional\n\n+initialize()\n+show()\n+plot()\n+simulate()\n\n\n\nyuima_model\n\nyuima.model\n\n+drift: expr\n+diffusion: expr\n+hurst: num\n+time_variable: chr\n+state_variable: chr\n+solve_variable: chr\n- parameter: model_parameter\n\n+simulate()\n\n\n\nyuima-&gt;yuima_model\n\n\n\n\n\nyuima_sampling\n\nyuima.sampling\n\n+Initial: num\n+Terminal: num\n+n: num\n- delta: num\n- grid: list[num]\n\n \n\n\n\nyuima-&gt;yuima_sampling\n\n\n\n\n\nmodel_parameter\n\nmodel_parameter\n\n+all: list[chr]\n+common: chr\n+diffusion: chr\n+drift: chr\n+jump: chr\n+measure: chr\n\n \n\n\n\nyuima_model-&gt;model_parameter\n\n\n\n\n\n\n\n\n\n\nyuimaクラスは５つのスロット yuima.model, yuima.sampling, yuima.data, yuima.characteristic, yuima.functional を持ち，それぞれが別のクラスとして実装されている．\nmodel.parameter クラスは後述の setModel() コンストラクタにより自動で作成され，ユーザーは変更できない．\n\n\n\n\n\n\nクラス構造の全貌\n\n\n\n\n\n実際には，yuima.model と yuima.sampling だけでなく，yuima クラスは５つの構成要素を持つ．\n\n\n\n\n\n\n\nClassDiagram\n\n\n\nyuima\n\nyuima\n\n+model: yuima.model\n+sampling: yuima.sampling\n+data: yuima.data\n- characteristic: yuima.characteristic\n- functional: yuima.functional\n\n+initialize()\n+show()\n+plot()\n+simulate()\n\n\n\nyuima_model\n\nyuima.model\n\n+drift: expr\n+diffusion: expr\n+hurst: num\n+time_variable: chr\n+state_variable: chr\n+solve_variable: chr\n- parameter: model_parameter\n\n+simulate()\n\n\n\nyuima-&gt;yuima_model\n\n\n\n\n\nyuima_sampling\n\nyuima.sampling\n\n+Initial: num\n+Terminal: num\n+n: num\n- delta: num\n- grid: list[num]\n\n \n\n\n\nyuima-&gt;yuima_sampling\n\n\n\n\n\nyuima_data\n\nyuima.data\n\n+original_data\n+zoo_data\n\n \n\n\n\nyuima-&gt;yuima_data\n\n\n\n\n\nyuima_characteristic\n\nyuima.characteristic\n\n+equation_number: int\n+time_scale: num\n\n \n\n\n\nyuima-&gt;yuima_characteristic\n\n\n\n\n\nyuima_functional\n\nyuima.functional\n\n+F\n+f: list\n+xinit: num\n+e: num\n\n \n\n\n\nyuima-&gt;yuima_functional\n\n\n\n\n\nmodel_parameter\n\nmodel_parameter\n\n+all: list[chr]\n+common: chr\n+diffusion: chr\n+drift: chr\n+jump: chr\n+measure: chr\n\n \n\n\n\nyuima_model-&gt;model_parameter\n\n\n\n\n\n\n\n\n\n\nそれぞれのクラスについての詳細は\n?yuima.model\nisClass(\"yuima.model\")\nmethods(class = \"yuima.model\")\nなどの方法で確認可能．\n\n\n\n\n\n2.2 setModel()コンストラクタ\nyuima.model.Rにおいて次の signature を持つ，yuima.modelオブジェクトのコンストラクタである．\nsetModel &lt;- function(drift=NULL, diffusion=NULL, hurst=0.5, jump.coeff=NULL, measure=list(), measure.type=character(), state.variable=\"x\", jump.variable=\"z\", time.variable=\"t\", solve.variable, xinit=NULL){...}\nこの setModel() 関数により，最も一般的には次の形をした確率微分方程式が定義できる：\n\\[\ndX_t=a(t,X_t,\\alpha)dt+b(t,X_t,\\beta)dW_t+c(t,X_t,\\gamma)dZ_t,\\qquad X_0=x_0.\n\\]\n\\(W\\) としては Brown 運動だけでなく，一般の Hurst 指数 \\(H\\) を持った分数 Brown 運動 \\(W^H\\) を取ることができる．\n\\(c\\ne0\\) とすることでジャンプを導入したモデルを指定することもできる．"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html#例で見る-yuima-の使い方",
    "href": "posts/2024/R/YUIMA.html#例で見る-yuima-の使い方",
    "title": "YUIMA 入門",
    "section": "3 例で見る YUIMA の使い方",
    "text": "3 例で見る YUIMA の使い方\n\n3.1 1次元の場合\nOrnstein-Uhlenbeck 過程 \\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t,\\quad (\\mu,\\theta,\\sigma,X_0)=(0,1,0.5,0)\n\\] を定義してシミュレーションを実行するためのサンプルコードは次の通り．4\n\n# 確率微分方程式モデルの設定\nmodel &lt;- setModel(drift = \"theta*(mu-X)\", diffusion = \"sigma\", state.variable = \"X\")\n\n# サンプリングスキームの設定\nsampling &lt;- setSampling(Initial = 0, Terminal = 3, n = 1000)\n\n# yuimaオブジェクトの作成\nyuima &lt;- setYuima(model = model, sampling = sampling)\n\n# シミュレーションの実行\nsimulation &lt;- simulate(yuima, true.parameter = c(mu = 0,theta = 1, sigma = 0.5), xinit = 0.02)\n\n\nplot(simulation)\n\n\n\n\n\n\n\n\n\n\n3.2 多次元の場合\nまず，所望のモデルを行列の言葉で書く．例えば，2次元の場合で次のようなモデルを考える： \\[\n\\begin{pmatrix}dX_t^{(1)}\\\\dX_t^{(2)}\\end{pmatrix}=\\begin{pmatrix}-3X_t^{(1)}\\\\-X_t^{(1)}2X_t^{(2)}\\end{pmatrix}dt+\\begin{pmatrix}1&0&X_t^{(2)}\\\\X_t^{(1)}&3&0\\end{pmatrix}\\begin{pmatrix}dW_t^{(1)}\\\\dW_t^{(2)}\\\\dW_t^{(3)}\\end{pmatrix}\n\\]\nシミュレーションは次のように，各項の係数をベクトル・行列の形式で setModel へと引き渡すことで行える．5\n\nsolve_variable &lt;- c(\"X1\", \"X2\")\ndrift &lt;- c(\"-3*X1\", \"-X1-2*X2\")\ndiffusion &lt;- matrix(c(\"1\", \"X1\", \"0\", \"3\", \"X2\", \"0\"), 2, 3)\nmodel &lt;- setModel(drift=drift, diffusion=diffusion, solve.variable=solve_variable)\nsimulation &lt;- simulate(model)\n\n\nplot(simulation, plot.type=\"single\", lty=1:2)\n\n\n\n\n\n\n\n\n\n\n3.3 分数 Gauss なノイズ\n\n\n\n\n\n\n標準分数 Brown 運動 \\(W^H\\) とは，Hurst 指数 \\(H\\in(0,1)\\) に対して， \\[\n\\operatorname{E}[W^H_s,W^H_t]=\\frac{1}{2}\\biggr(\\lvert s\\rvert^{2H}+\\lvert t\\rvert^{2H}-\\lvert t-s\\rvert^{2H}\\biggl)\n\\] という共分散構造を持った中心 Gauss 過程である．\n\n\n\n\\(H\\ne1/2\\) のとき，もはや Markov 過程でもセミマルチンゲールでもない．特に \\(H&gt;1/2\\) のときに，\\(W^H\\) は長期的な依存を持った振る舞いをしこれが多くの応用を呼んでいる．\nこの \\(W^H\\) に対して，SDE \\[\ndX_t=a(X_t)dt+b(X_t)dW_t^H\n\\] で定まるモデルを定義できる．(Brouste et al., 2014, pp. 3.5節 p.15)\n例えば分数 OU 過程 \\[\ndY_t=3Y_tdt+dW_t^H\n\\] は次のように定義する：\n\nmod4A &lt;- setModel(drift=\"3*y\", diffusion=1, hurst=0.3, solve.var=\"y\")\nmod4B &lt;- setModel(drift=\"3*y\", diffusion=1, hurst=0.7, solve.var=\"y\")\nsim1 &lt;- simulate(mod4A, sampling = setSampling(n=1000))\nsim2 &lt;- simulate(mod4B, sampling = setSampling(n=1000))\npar(mfrow=c(2,1), mar=c(2,3,1,1))\n\n\nplot(sim1, main=\"H=0.3\")\n\n\n\n\n\n\n\nplot(sim2, main=\"H=0.7\")\n\n\n\n\n\n\n\n\nこのシミュレーション法は Cholesky 法と (Wood and Chan, 1994) 法から選択が可能である．simulate メソッドのキーワード引数 methodfGn=\"WoodChan\", methodfGn=\"Cholesky\" によって利用可能である．\n\n\n3.4 Lévy 過程\n複合 Poisson 過程 \\(Z_t\\) とは，Poisson 時間に特定の分布に従うサイズの跳躍が起こるという過程である．\n複合 Poisson 過程 \\(Z_t\\) を用いて， \\[\ndX_t=a(t,X_t,\\theta)dt+b(t,X_t,\\theta)dW_t+dZ_t\n\\] という SDE を通じてジャンプを持つ過程が定義できる．\nさらに \\(Z_t\\) の項に係数 \\(c\\) を持たせるには，\\(X\\) のジャンプを定めるランダム測度（≒確率核） \\[\n\\mu(dt,dz)=\\sum_{s&gt;0}1_{\\left\\{\\Delta Z_s\\ne 0\\right\\}}\\delta_{(s,\\Delta Z_s)}(dt,dz)\n\\] により，複合 Poisson 過程が \\[\nZ_t=\\int^t_0\\int_{\\lvert z\\rvert\\le1}z(\\mu(ds,dz)-\\nu(dz)ds)+\\int^t_0\\int_{\\lvert z\\rvert&gt;1}z\\mu(ds,dz)\n\\] と表せ，ジャンプ過程 \\(X\\) が，一般の関数 \\(c\\) を用いて \\[\ndX_t=a(t,X_t,\\theta)dt+b(t,X_t,\\theta)dW_t+\\int_{\\lvert z\\rvert&gt;1}c(X_{t-},z)\\mu(dt,dz)\n\\] \\[\n\\qquad+\\int_{0&lt;\\lvert z\\rvert\\le1}c(X_{t-},z)(\\mu(dt,dz)-\\nu(dz)dt)\n\\] と表せる．\n\\(\\nu\\in\\mathcal{P}(\\mathbb{R}^d)\\) が Lévy 測度 であるとは， \\(\\nu(\\{0\\})=0\\) かつ \\[\n\\int_{\\mathbb{R}^d}(1\\land\\lvert z\\rvert^2)\\nu(dz)&lt;\\infty\n\\] を満たすことをいう．\n例えば，強度 \\(\\lambda=10\\) で Gauss 分布を跳躍測度に持つ Lévy 過程は，measure.type=\"CP\" によって指定する．\n平均 \\(0\\) のジャンプ OU 過程 \\[\ndX_t=-\\theta X_tdt+\\sigma dW_t+dZ_t\n\\] は次のように定義できる：\n\nmod5 &lt;- setModel(drift=c(\"-theta*x\"), diffusion=\"sigma\", jump.coeff=\"1\",\n      measure=list(intensity=\"10\", df=list(\"dnorm(z,0,1)\")), measure.type=\"CP\",\n      solve.variable=\"x\")\nsim5 &lt;- simulate(mod5, true.p = list(theta=1, sigma=3), sampling=setSampling(n=1000))\n\n\nplot(sim5)\n\n\n\n\n\n\n\n\n一方で，逆正規分布の大きさのジャンプを持つ Lévy 測度 \\(\\nu\\) をもち，Poisson 成分を持たない OU 過程 \\[\ndX_t=-xdt+dZ_t\n\\] は次のように定義できる：\n\nmod6 &lt;- setModel(drift=\"-x\", xinit=1, jump.coeff=\"1\",\n      measure.type=\"code\", measure=list(df=\"rIG(z,1,0.1)\"))\nsim6 &lt;- simulate(mod6, sampling=setSampling(Terminal=10, n=10000))\n\n\nplot(sim6)"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html#simulate-と-setsampling",
    "href": "posts/2024/R/YUIMA.html#simulate-と-setsampling",
    "title": "YUIMA 入門",
    "section": "4 simulate() と setSampling()",
    "text": "4 simulate() と setSampling()\n\n4.1 simulate() 関数の使い方\nsimulate() は yuima または yuima.model で定められたモデルから，Euler-Maruyama 法によるシミュレーションを実行する関数．6\nソースコードは simulate.R．ポリモーフィックな実装がなされている．\nsimulate(object, nsim=1, seed=NULL, xinit, true.parameter, space.discretized = FALSE, increment.W = NULL, increment.L = NULL, method = \"euler\", hurst, methodfGn = \"WoodChan\", sampling=sampling, subsampling=subsampling, ...)\n返り値は yuima オブジェクトである．しかし，その yuima.data フィールドにはシミュレーション結果が格納されており，それに対して plot() を使う．\n\n4.1.1 時間離散化 Euler-Maruyama 法\n最も広く使われているシミュレーション法である．\n時間にグリッド \\(0=\\tau_0&lt;\\tau_1&lt;\\cdots&lt;\\tau_j&lt;\\cdots\\) を導入し，連続過程 \\(\\{X_t\\}_{t\\in\\mathbb{R}_+}\\) の離散化 \\(\\{\\widetilde{X}_{\\tau_j}\\}_{j\\in\\mathbb{N}}\\) を \\[\n\\widetilde{X}_{\\tau_{j+1}}:=\\widetilde{X}_{\\tau_j}+b(\\tau_j,\\widetilde{X}_{\\tau_j})(\\tau_{j+1}-\\tau_j)+c(\\tau_j,\\widetilde{X}_{\\tau_j})(W_{\\tau_{j+1}}-W_{\\tau_j})\n\\] と定義する．\n\\(W_{\\tau_{j+1}}-W_{\\tau_j}\\overset{\\text{iid}}{\\sim}\\mathrm{N}(0,\\tau_{j+1}-\\tau_j)\\) を利用してサンプリングできる．\n\n\n4.1.2 空間離散 Euler-Maruyama 法\nsimulate メソッドはこの方法も（ジャンプを持たない過程について）実装している．ただし，現状，駆動過程が1次元のSDEに対してのみである．\nこれは，時間離散化 \\(\\{\\tau_j\\}_{j\\in\\mathbb{N}}\\subset\\mathbb{R}\\) は次のように取る方法である： \\[\n\\tau_0:=0,\\quad\\tau_{j+1}:=\\inf\\left\\{t&gt;\\tau_j\\mid\\lvert W_t-W_{\\tau_j}\\rvert=\\epsilon\\right\\}\n\\] ただし，実装の上では \\[\n\\epsilon^2:=\\frac{T}{n}=\\Delta_n\n\\] としている．\nこの方法は通常の時間離散化法よりも3倍高速になることが経験的に知られている．\nこれは space.discretized=TRUE で指定できる．デフォルトは =FALSE となっており，時間離散化の方である．\n他にも (Iacus, 2008) のシミュレーション法が実装予定である．\n\n\n\n4.2 setSamplingコンストラクタ\nsimulate 関数は yuima.sampling オブジェクトも引数として sampling = samp と取れる．\nそしてこのサンプリングスキームは出力にそのまま引き継がれる．\n\n\nsetSampling.R\n\nsetSampling(Initial = 0, Terminal = 1, n = 100, delta, \n   grid, random = FALSE, sdelta=as.numeric(NULL), \n   sgrid=as.numeric(NULL), interpolation=\"pt\" )\n\n\n4.2.1 例\n例えば次の2次元モデルを考える\n\\[\ndX_t^{(1)}=-\\theta X^{(1)}_tdt+dW_t^{(1)}+X_t^{(2)}dW_t^{(3)}\n\\] \\[\ndX_t^{(2)}=-(X_t^{(1)}+\\gamma X_t^{(2)})dt+X_t^{(1)}dW_t^{(1)}+\\delta dW_t^{(2)}\n\\]\n時区間 \\([0,3]\\) 上のグリッドから \\(n=3000\\) の粒度で観測するサンプリングオブジェクトは次のように定義する：\n\nsol &lt;- c(\"x1\", \"x2\")\nb &lt;- c(\"-theta*x1\", \"-x1-gamma*x2\")\ns &lt;- matrix(c(\"1\", \"x1\", \"0\", \"delta\", \"x2\", \"0\"), 2, 3)\nmyModel &lt;- setModel(drift=b, diffusion=s, solve.variable=sol)\nsamp &lt;- setSampling(Terminal=3, n=3000)\n\n\nsim2 &lt;- simulate(myModel, sampling=samp)\nstr(sim2@sampling)\n\nFormal class 'yuima.sampling' [package \"yuima\"] with 11 slots\n  ..@ Initial      : num 0\n  ..@ Terminal     : num [1:2] 3 3\n  ..@ n            : int [1:2] 3000 3000\n  ..@ delta        : num 0.001\n  ..@ grid         :List of 1\n  .. ..$ : num [1:3001] 0 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 ...\n  ..@ random       : logi FALSE\n  ..@ regular      : logi TRUE\n  ..@ sdelta       : num(0) \n  ..@ sgrid        : num(0) \n  ..@ oindex       : num(0) \n  ..@ interpolation: chr \"pt\"\n\n\nこれはsampそのものであり，確かに出力に引き継がれている．\n\n\n\n4.3 subsampling関数の使い方\n２つの独立な指数分布を指定することで，Poisson 到着時間を用いてサンプリングすることができる．\nこのような yuima.sampling オブジェクトは次のように定義する：\n\nnewsamp &lt;- setSampling(random=list(rdist=c(function(x)\n      + rexp(x, rate=10), function(x) rexp(x, rate=20))))\n\nこれを用いてサブサンプリングを実行できる：\n\nnewdata &lt;- subsampling(sim2, sampling=newsamp)\nplot(sim2, plot.type=\"single\", lty=c(1,3), ylab=\"sim2\")\npoints(get.zoo.data(newdata)[[1]], col=\"red\")\npoints(get.zoo.data(newdata)[[2]], col=\"green\", pch=18)\n\n\n\n\n\n\n\n\n赤が \\(X_t^{(1)}\\)，緑が \\(X_t^{(2)}\\) の見本道でどこがサブサンプリングされたかを示している．\nこのサンプリング法はランダムに行った．これを示すフラグがregularである：\n\nstr(newsamp@regular)\n\n logi FALSE\n\n\n一方で，決定論的に，決まった周波数でサブサンプリングもできる：\n\nnewsamp2 &lt;- setSampling(delta=c(0.1, 0.2))\n\nWarning in yuima.warn(\"'Terminal' (re)defined.\"): \nYUIMA: 'Terminal' (re)defined.\n\nnewdata2 &lt;- subsampling(sim2, sampling=newsamp2)\nplot(sim2, plot.type=\"single\", lty=c(1,3), ylab=\"sim2\")\npoints(get.zoo.data(newdata2)[[1]], col=\"red\")\npoints(get.zoo.data(newdata2)[[2]], col=\"green\", pch=18)\n\n\n\n\n\n\n\n\n赤色のサブサンプリングは，緑色の２倍の頻度（半分の周波数）で行われている．\n\n\n4.4 サンプリングとサブサンプリングの組み合わせ\nシミュレーション研究で，よく高頻度のサンプリングを行った後，推定のためにより低い頻度でのデータを抽出する，ということが行われる．これは１行で表現できる：\n\nY.sub &lt;- simulate(myModel, sampling=setSampling(delta=0.001, n=1000), subsampling=setSampling(delta=0.01, n=100))\nY &lt;- simulate(myModel, sampling=setSampling(delta=0.001, n=1000))\n\n\nplot(Y.sub, plot.type=\"single\")\npoints(get.zoo.data(Y.sub)[[1]], col=\"red\")\npoints(get.zoo.data(Y.sub)[[2]], col=\"green\", pch=18)\n\n\n\n\n\n\n\n\n\n\n4.5 今後の発展\n\n4.5.1 zooとの依存関係\nサンプリングとサブサンプリングにおいて，zooというパッケージ (Zeileis and Grothendieck, 2005) が時系列データを保存するために内部で用いられている．\nより柔軟な時系列データの保存法ができたら，この dependency は脱したいと考えられているようである．"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html#これから",
    "href": "posts/2024/R/YUIMA.html#これから",
    "title": "YUIMA 入門",
    "section": "5 これから",
    "text": "5 これから\n\n\nsetYuima() の help ページに ‘PLEASE FINISH THIS’ が２箇所ある．\nhelp ページによく出てくる yuimadocs パッケージとは？"
  },
  {
    "objectID": "posts/2024/R/YUIMA.html#footnotes",
    "href": "posts/2024/R/YUIMA.html#footnotes",
    "title": "YUIMA 入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Brouste et al., 2014) 第７節も参照．↩︎\nR-Forge とは，SourceForge に似た，R 言語のパッケージ開発者向けの協力的な開発環境を提供するプラットフォーム．↩︎\nrdrr は CRAN や GitHub だけでなく，Bioconductor や R-Forge も含めて検索可能にしているサイト．↩︎\nモデルの左辺 \\(dX_t\\) は yuima.model のスロット solve.variable が state.variable に暗黙のうちに一致させているところから暗黙に読み込んでいる．↩︎\n１次元の場合と違って，左辺を明示するためにsolve.variableのベクトルとしての指定が欠かせなくなる．(Brouste et al., 2014) 第3.4節 pp.12-13．↩︎\nただし，分数 Gauss ノイズが仮定された場合は Cholesky 法または (Wood and Chan, 1994) の手法による．↩︎"
  },
  {
    "objectID": "posts/2024/R/adastan.html#背景",
    "href": "posts/2024/R/adastan.html#背景",
    "title": "SDE のベイズ推定入門",
    "section": "3 背景",
    "text": "3 背景\n\n3.1 確率的プログラミング言語 Stan との連携\nStan は Hamiltonian Monte Carlo 法を用いた事後分布サンプリングを，確率モデルを定義するだけで実行することができる言語である．\n加えて，バックグラウンドで C++ を用いているため，非常に高速な MCMC 計算が可能である．\n確率的プログラミング言語 Stan については次の記事も参照：\n\n\n\n\n\n\n\n\n\n\nStan 入門\n\n\n\n2024-05-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 上の Stan インターフェイス\n\n\nRStan, RStanArm, CmdStanR\n\n\n\n2024-09-19\n\n\n\n\n\n\n\n\nNo matching items\n\n\nRStan は確率的プログラミング言語 Stan とのインターフェイスを提供する R パッケージであり，これを用いることで R からでも Stan を通じた HMC を用いた事後分布からのサンプリングが実行できる．\nここでは YUIMA の adaBayes() 関数と同じ推定を，内部で Stan を用いて実行する関数 adaStan() の実装を試みる．\n\n\n3.2 アイデアのスケッチ\n具体的には，R において次のような関数を定義することになるだろう．\n必ずしもベストな方法ではないかもしれないが，まずは RStan を用いてスケッチをしてみる．改良版は第 2 節参照．\n\nlibrary(yuima)\nlibrary(rstan)\n\n1yuima_to_stan &lt;- function(yuima){\n  excode &lt;- 'data {\\n  int N;\\n  vector[N+1] x;\\n  real T;\\n  real h;\\n}\nparameters {\\n'\n\n2  for(i in 1:length(yuima@model@parameter@all)){\n    excode &lt;- paste(excode, \" real\", yuima@model@parameter@all[i], \";\\n\")\n  }\n\n3  excode &lt;- paste(excode,\"\\n}\")\n\n4  excode &lt;- paste(excode,'\\nmodel {\\n  x[1] ~ normal(0,1);\\n  for(n in 2:(N+1)){')\n\n  excode &lt;- paste(excode,\n5    \"\\n  x[n] ~ normal(x[n-1] + h *\", gsub(\"x\", \"x[n-1]\", yuima@model@drift), \",sqrt(h) *\", gsub(\"x\", \"x[n-1]\", yuima@model@diffusion[[1]]),\");\\n  }\")\n\n  excode &lt;- paste(excode,'\\n}\\n')\n}\n\n6adaStan &lt;- function(yuima){\n  excode &lt;- yuima_to_stan(yuima)\n\n7  sde_dat &lt;- list(N =  yuima@sampling@n,\n    x = as.numeric(yuima@data@original.data), \n    T=yuima@sampling@Terminal,\n    h=yuima@sampling@Terminal/yuima@sampling@n)\n\n  fit &lt;- stan(model_code=excode,\n    data = sde_dat, \n    iter = 1000,\n8    chains = 4)\n\n  return(fit)\n}\n\n\n1\n\nStan モデルのコード（パラメータ部分は未定）を文字列として excode 変数に格納する．\n\n2\n\nここからが adaStan 関数の本体である．Yuima モデルの全てのパラメータについてループを開始して，excode にパラメータの宣言を追加していく．\n\n3\n\nここでついに Stan モデルのパラメータの定義部分が完成する．\n\n4\n\n最後はモデルの定義部分を追加して，Stan モデルのコードが完成する．最初の観測値 x[1] は \\(\\operatorname{N}(0,1)\\) に従う．\n\n5\n\nそれ以降の観測値 x[n] は，前の観測値 x[n-1] に drift 項と diffusion 項を加えたものに従う．これを実装するために，Yuima モデルの drift 項と diffusion 項の定義文を呼び出し，x を x[n-1] に置換することで Stan モデルのコードに埋め込む．\n\n6\n\nadaStan という関数を定義する．この関数は，Yuima パッケージのオブジェクトを引数として受け取り，Stan での推定を行い，その結果を fit オブジェクトとして返す．\n\n7\n\nStan での推定を実行するために，Yuima モデルのデータを Stan モデルに渡すためのリスト sde_dat を作成する．\n\n8\n\n最後に Stan モデルをコンパイルして実行し，結果を fit オブジェクトとして返す．\n\n\n\n\nyuima オブジェクトのスロットの存在のチェックや変数名 x の表記揺れなど，細かな問題も多いだろうが，本記事では殊に Stan との接続においてより良い方法を模索したい．\n\n\n\n\n\n\n問題点\n\n\n\n関数内部で Stan コードを文字列として生成していることがダサい．\nより良いコードオブジェクトの取り扱い方や，Stan とのより安全で効率的なインターフェイスを模索したい．\n\n\nStan を使う以上，どこかで Stan モデルの情報を受け渡すことは必要になるが，できることならばもっと良い方法を考えたい．\n\n\n3.3 サンプルコードの動き\nadaStan() 関数の挙動を詳しく見るために，次の具体例を考える．\nYUIMA を通じて１次元 OU 過程\n\\[\ndX_t=\\theta(\\mu-X_t)\\,dt+\\sigma\\,dW_t\n\\]\nをシミュレーションをするためには，次のようにモデル定義をする：\n\nmodel &lt;- setModel(drift = \"theta*(mu-x)\", diffusion = \"sigma\", state.variable = \"x\", solve.variable = \"x\")\n\nこれだけで，YUIMA は勝手にパラメータを識別してくれる：\n\nmodel@drift\n\nexpression((theta * (mu - x)))\n\nmodel@diffusion[[1]]\n\nexpression((sigma))\n\n\nこれを通じて生成される Stan モデル文は\ndata {\n  int N;\n  vector[N+1] x;\n  real T;\n  real h;\n}\n\nparameters {\n  real theta;\n  real mu;\n  real sigma;\n}\n\nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){\n    x[n] ~ normal(x[n-1] + h * theta * (mu - x[n-1]),\n                  sqrt(h) * sigma);\n  }\n}\nとなるべきであるが，実際その通りになる：\n\nx &lt;- setYuima(model = model)\nstancode &lt;- yuima_to_stan(x)\ncat(stancode)\n\ndata {\n  int N;\n  vector[N+1] x;\n  real T;\n  real h;\n}\nparameters {\n  real theta ;\n  real mu ;\n  real sigma ;\n \n} \nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){ \n  x[n] ~ normal(x[n-1] + h * (theta * (mu - x[n-1])) ,sqrt(h) * (sigma) );\n  } \n}\n\n\n\n\n3.4 CmdStanR による方法\n同様のことが CmdStanR というパッケージを用いても実行可能である．\nCmdStanR では一時ファイル上に stan ファイルを作成して，それをコンパイルして実行する．\n\nlibrary(cmdstanr)\nstan_file_variables &lt;- write_stan_file(stancode)\nmod &lt;- cmdstan_model(stan_file_variables)\n\n\nmod$print()  # R6 オブジェクトなので $ を用いた method 呼び出しができる\n\ndata {\n  int N;\n  vector[N+1] x;\n  real T;\n  real h;\n}\nparameters {\n  real theta ;\n  real mu ;\n  real sigma ;\n \n} \nmodel {\n  x[1] ~ normal(0,1);\n  for(n in 2:(N+1)){ \n  x[n] ~ normal(x[n-1] + h * (theta * (mu - x[n-1])) ,sqrt(h) * (sigma) );\n  } \n}\n\n\nこの方法ではバイナリファイルを作成・操作することができる．R セッションの終了とともに一時ファイルは削除される．\n\nmod$stan_file()\n\n[1] \"/var/folders/7c/j9mzb7pn0wn1k_f9j58c8y480000gn/T/RtmpFQDHJ2/model_28b66dedb9ecfce181f60d7f27d0c76d.stan\"\n\n\n\n\n3.5 Stan コードのベクトル化\nStan モデルの問題であるが，なるべくベクトル記法を用いた方が，Stan コードの処理（特に自動微分の計算）が速くなる．\nmodel {\n  x[1] ~ normal(0, 1);  // 初期（値の事前）分布\n  x[2:(N + 1)] ~ normal(x[1:N] + h * theta * (rep_vector(mu, N) - x[1:N]), sqrt(h) * sigma);  // xの2番目からN+1番目までをベクトル化して定義\n}\n\n\n3.6 一般化ベイズへの拡張\n第 3.2 節のサンプルコードでは，ひとまず拡散過程のみにターゲットを絞って adaStan() を定義した．\nゆくゆくは遷移確率が極めて複雑になるような確率過程に対する推論も考える必要がある．\n一方で Stan では target 変数を用いて明示的に対数密度を定義することができる．\ntarget += normal_lpdf(y | mu, sigma);\nこの明示的な記法を用いることで，擬似尤度などを用いた一般化ベイズ推定も実行できる．1"
  },
  {
    "objectID": "static/about.html#languages",
    "href": "static/about.html#languages",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Languages",
    "text": "Languages\nJapanese (native), English (fluent), Mandarin Chinese (fluent), German (A2)"
  },
  {
    "objectID": "static/CV/cv.html#main-research-interests",
    "href": "static/CV/cv.html#main-research-interests",
    "title": "Hirofumi Shiba",
    "section": "Main Research Interests",
    "text": "Main Research Interests\nMonte Carlo methods, Diffusion Models, Neural Networks, and Bayesian Statistical Modelling, especially in Political Science & Biostatistics"
  },
  {
    "objectID": "static/CV/cv.html#activities",
    "href": "static/CV/cv.html#activities",
    "title": "Hirofumi Shiba",
    "section": "Activities",
    "text": "Activities\n\nData Scientist. PreMedica, Inc., Tokyo, Japan. 2024.9 – today\nProvided Bayesian data analysis solutions to healthcare applications.\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – 2025.3\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference."
  },
  {
    "objectID": "static/CV/cv.html#grants-awards",
    "href": "static/CV/cv.html#grants-awards",
    "title": "Hirofumi Shiba",
    "section": "Grants & Awards",
    "text": "Grants & Awards\n\nResearch Fellow. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2025.4 – today\nSupported by JST BOOST, Japan Grant Number JPMJBS2412: A Radical Approach to Trustworthy AI in the Coming Era. ¥ 11,700,000.\nBest Poster Award, 19th Spring Meeting of the Japan Statistical Society. 2025.3.8\nDirector’s Award from the Japan Statistical Society Certificate. 2025.3.8"
  },
  {
    "objectID": "static/CV/cv.html#collaborations",
    "href": "static/CV/cv.html#collaborations",
    "title": "Hirofumi Shiba",
    "section": "Collaborations",
    "text": "Collaborations\n\nData Scientist. PreMedica, Inc., Tokyo, Japan. 2024.9 – today\nProvided Bayesian data analysis solutions to healthcare applications.\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – 2025.3\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference."
  },
  {
    "objectID": "static/CV/cv.html#experience",
    "href": "static/CV/cv.html#experience",
    "title": "Hirofumi Shiba",
    "section": "Experience",
    "text": "Experience\n\nResearch Fellow. Graduate University for Advanced Studies, SOKENDAI, Tokyo, Japan. 2025.4 – 2028.3\nSupported by JST BOOST, Japan Grant Number JPMJBS2412.\nData Scientist. PreMedica, Inc., Tokyo, Japan. 2024.9 – today\nProvided Bayesian data analysis solutions to healthcare applications.\nResearch Assistant. The Institute of Statistical Mathematics, Tokyo, Japan. 2023.7 – 2026.3\nContributed to the R package YUIMA, an open-source project aiming to simulate and infer multidimensional stochastic differential equations, with an emphasis on Bayesian inference."
  },
  {
    "objectID": "posts/2025/Posters/OH.html",
    "href": "posts/2025/Posters/OH.html",
    "title": "ベイズ変数選択の計算的解決",
    "section": "",
    "text": "ポスター発表時刻\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 23rd, 13:00-15:00\nISM 1F\n\n\n\n\n\n\n\nクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/OH.html#統計数理研究所-オープンハウス",
    "href": "posts/2025/Posters/OH.html#統計数理研究所-オープンハウス",
    "title": "ベイズ変数選択の計算的解決",
    "section": "",
    "text": "ポスター発表時刻\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 23rd, 13:00-15:00\nISM 1F\n\n\n\n\n\n\n\nクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/OH.html#参考ページ集",
    "href": "posts/2025/Posters/OH.html#参考ページ集",
    "title": "ベイズ変数選択の計算的解決",
    "section": "参考ページ集",
    "text": "参考ページ集\nPDMP とそのシミュレーションに関しては，次のスライドにわかりやすく解説されています：\n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n一致なし\n\n\n図は全て発表者開発のパッケージ PDMPFlux.jl によるものです．\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n一致なし\n\n\nSticky PDMP (Bierkens ほか, 2023) に関するさらに詳しい内容，またはベイズ変数選択一般については，次の記事にまとめています：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "static/Materials.html#統数研オープンハウス",
    "href": "static/Materials.html#統数研オープンハウス",
    "title": "Materials",
    "section": "",
    "text": "PDMP による非絶対連続分布からのサンプリング\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#統数研オープンハウスでのポスター",
    "href": "static/Materials.html#統数研オープンハウスでのポスター",
    "title": "Materials",
    "section": "",
    "text": "PDMP による非絶対連続分布からのサンプリング\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025/Topology/ContiunitySet.html",
    "href": "posts/2025/Topology/ContiunitySet.html",
    "title": "A Function that is Continuous on a Closed Subset",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nFor the sake of simplicity, we focus on the case of \\(f:\\mathbb{R}\\to\\mathbb{R}\\).\nThis article is concerned with the continuity set of \\(f\\), defined as \\[\n\\mathrm{Cont}(f):=\\left\\{x\\in\\mathbb{R}\\mid f\\text{ is continuous at }x\\right\\}.\n\\]\nMost continuous functions are treated on open sets. We might be easily tricked to think that \\(\\mathrm{Cont}(f)\\) should be open.\nIs this conjecture true?1 As a starting point, proving that \\(\\mathrm{Cont}(f)\\) is a countable intersection of open sets is not difficult."
  },
  {
    "objectID": "posts/2025/Topology/ContiunitySet.html#mathrmcontf-is-g_delta",
    "href": "posts/2025/Topology/ContiunitySet.html#mathrmcontf-is-g_delta",
    "title": "A Function that is Continuous on a Closed Subset",
    "section": "1 \\(\\mathrm{Cont}(f)\\) is \\(G_\\delta\\)",
    "text": "1 \\(\\mathrm{Cont}(f)\\) is \\(G_\\delta\\)\nWe denote an open ball of radius \\(\\delta\\) centered at \\(x\\) by \\(U_\\delta(x)\\).\n\n\n\n\n\n\nProposition\n\n\n\n\\(\\mathrm{Cont}(f)\\) is a \\(G_\\delta\\) set, meaning that it is a countable intersection of open sets. Specifically, \\[\n\\mathrm{Cont}(f)=\\bigcap_{n=1}^\\infty\\left\\{x\\in\\mathbb{R}\\,\\middle|\\,\\exists_{\\delta&gt;0}\\;\\forall_{y,z\\in U_\\delta(x)}\\;\\lvert f(y)-f(z)\\rvert&lt;\\frac{1}{n}\\right\\}.\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt is straightforward to check that each \\(\\left\\{x\\in\\mathbb{R}\\,\\middle|\\,\\exists_{\\delta&gt;0}\\;\\forall_{y,z\\in U_\\delta(x)}\\;\\lvert f(y)-f(z)\\rvert&lt;\\frac{1}{n}\\right\\}\\) is open.\nThe easy part is the \\(\\supset\\) direction, since for every \\(\\epsilon&gt;0\\), there exists a \\(N&gt;0\\) satisfying \\(\\frac{1}{N}&lt;\\epsilon\\).\n\\(\\subset\\) direction is also straightforward, following from the definition of continuity and triangle inequality."
  },
  {
    "objectID": "posts/2025/Topology/ContiunitySet.html#example-of-fmathbbrtomathbbr-with-closed-mathrmcontf",
    "href": "posts/2025/Topology/ContiunitySet.html#example-of-fmathbbrtomathbbr-with-closed-mathrmcontf",
    "title": "A Function that is Continuous on a Closed Subset",
    "section": "3 Example of \\(f:\\mathbb{R}\\to\\mathbb{R}\\) with closed \\(\\mathrm{Cont}(f)\\)",
    "text": "3 Example of \\(f:\\mathbb{R}\\to\\mathbb{R}\\) with closed \\(\\mathrm{Cont}(f)\\)\nIn perfectly regular spaces, every closed set is a \\(G_\\delta\\) set (Tong, 1952).\nSo there should be an example on \\(\\mathbb{R}\\) that \\(\\mathrm{Cont}(f)\\) is closed.\n\n\n\n\n\n\nExample2\n\n\n\nThe function \\(f\\) defined as follows has \\(\\mathrm{Cont}(f)=(-\\infty,0]\\): \\[\nf(x):=\\begin{cases}\n0,&x\\le0,\\\\\n\\frac{1}{n},&x\\in(0,\\infty)\\cap\\mathbb{Q},\\\\\n-\\frac{1}{n},&x\\in(0,\\infty)\\setminus\\mathbb{Q}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/2025/Topology/ContiunitySet.html#g_delta-sets-are-continuity-sets",
    "href": "posts/2025/Topology/ContiunitySet.html#g_delta-sets-are-continuity-sets",
    "title": "A Function that is Continuous on a Closed Subset",
    "section": "2 \\(G_\\delta\\) sets are continuity sets",
    "text": "2 \\(G_\\delta\\) sets are continuity sets\nA fun fact is that every \\(G_\\delta\\) set is the continuity set \\(\\mathrm{Cont}(f)\\) of some function \\(f\\), proved in, for example, a almost one-page article (Kim, 1999).\n\n\n\n\n\n\n(Theorem p.258 Kim, 1999)\n\n\n\nLet \\(X\\) be a nonempty metric space without isolated points. Then every \\(G_\\delta\\) subset \\(G\\subset X\\) is the continuity set of some function \\(f:X\\to\\mathbb{R}\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThis construction can be understood as a generalization of the example given later in Section 3.\nGiven that \\(G\\) is a \\(G_\\delta\\) set, the complement \\(G^\\complement\\) is a countable union of closed sets: \\[\nG^\\complement=\\bigcup_{n=1}^\\infty F_n,\\qquad F_n\\overset{\\textrm{closed}}{\\subset}X.\n\\]\nUsing \\(\\{F_n\\}\\), we construct a function \\(g:X\\to\\mathbb{R}\\) as follows: \\[\ng(x):=\\sum_{n=1}^\\infty\\frac{1}{2^n}1_{F_n}(x).\n\\] Observe that \\(g=0\\;\\mathrm{on}\\;G\\). Building upon this, we define \\(f:X\\to\\mathbb{R}\\) by \\[\nf(x):=g(x)\\left(1_A(x)-\\frac{1}{2}\\right)\n\\] where \\(A\\subset X\\) be a set that both \\(A\\) and \\(A^\\complement\\) are dense in \\(X\\).\nThis set \\(A\\) corresponds \\(\\mathbb{Q}\\) in the example given later in Section 3."
  },
  {
    "objectID": "posts/2025/Topology/ContiunitySet.html#footnotes",
    "href": "posts/2025/Topology/ContiunitySet.html#footnotes",
    "title": "A Function that is Continuous on a Closed Subset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is not true.↩︎\nMy colleague Nakajima-san taught me this example.↩︎"
  },
  {
    "objectID": "posts/2025/Topology/ContiunitySet.html#sec-example",
    "href": "posts/2025/Topology/ContiunitySet.html#sec-example",
    "title": "A Function that is Continuous on a Closed Subset",
    "section": "3 Example of \\(f:\\mathbb{R}\\to\\mathbb{R}\\) with closed \\(\\mathrm{Cont}(f)\\)",
    "text": "3 Example of \\(f:\\mathbb{R}\\to\\mathbb{R}\\) with closed \\(\\mathrm{Cont}(f)\\)\nIn perfectly regular spaces, every closed set is a \\(G_\\delta\\) set (Tong, 1952).\nSo there should be an example on \\(\\mathbb{R}\\) that \\(\\mathrm{Cont}(f)\\) is closed.\n\n\n\n\n\n\nExample2\n\n\n\nThe function \\(f\\) defined as follows has \\(\\mathrm{Cont}(f)=(-\\infty,0]\\): \\[\nf(x):=\\begin{cases}\n0,&x\\le0,\\\\\n\\frac{1}{n},&x\\in(0,\\infty)\\cap\\mathbb{Q},\\\\\n-\\frac{1}{n},&x\\in(0,\\infty)\\setminus\\mathbb{Q}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/2025/PDMP/HDG.html",
    "href": "posts/2025/PDMP/HDG.html",
    "title": "PDMP サンプラーの高次元 Gauss での挙動",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/PDMP/HDG.html#概観",
    "href": "posts/2025/PDMP/HDG.html#概観",
    "title": "PDMP サンプラーの高次元 Gauss での挙動",
    "section": "1 概観",
    "text": "1 概観\n\n\n1.1 BPS のポテンシャル\nBPS のポテンシャルは OU 過程に収束する： \\[\ndY_t=-\\theta(\\rho)^2/4\\cdot Y_t\\,dt+\\sigma(\\rho)\\,dB_t.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 非有界な速度を持つ BPS のポテンシャル\n\\(\\mu=\\mathrm{U}(S^{d-1})\\) とした場合と \\(\\mu=\\operatorname{N}_d(0,I_d)\\) とした場合とで，全く違う動きをする．\n\n\n\n\n\n\n\n\n\n\\(\\mu=\\mathrm{U}(S^{d-1})\\)\n\n\n\n\n\n\n\n\\(\\mu=\\operatorname{N}_d(0,I_d)\\)\n\n\n\n\n\nしたがって，\\(\\mu=\\operatorname{N}_d(0,I_d)\\) と取った場合はまた違う過程に収束するものと思われる．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nいや，同じか？\n\n\n1.3 FECMC のポテンシャル\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.4 Zig-Zag のポテンシャル"
  },
  {
    "objectID": "posts/2025/PDMP/HDG.html#有限次元周辺分布の挙動",
    "href": "posts/2025/PDMP/HDG.html#有限次元周辺分布の挙動",
    "title": "PDMP サンプラーの高次元 Gauss での挙動",
    "section": "3 有限次元周辺分布の挙動",
    "text": "3 有限次元周辺分布の挙動\n次元数 \\(d\\) を少しずつ大きくしながら，最初の１または２成分のみをみていく．\n\n3.1 BPS\nBPS は速度成分の分布を変えることで，収束先が違う．\n驚くべきことに，\\(\\mu=\\operatorname{N}_d(0,I_d)\\) と取ると，Boomerang sampler (Bierkens et al., 2020) の挙動に酷似していく．\nこれはポテンシャル \\(U\\) が定める Hamiltonian フローにトラップされていくためである．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nしかし \\(\\mu\\) を \\(S^{d-1}\\subset\\mathbb{R}^d\\) 上の一様分布に取ると，次の通り，全く違った挙動になる：\n\n\n\n3.2 FECMC"
  },
  {
    "objectID": "posts/2025/PDMP/HDG.html#bps-と-fecmc-の比較",
    "href": "posts/2025/PDMP/HDG.html#bps-と-fecmc-の比較",
    "title": "PDMP サンプラーの高次元 Gauss での挙動",
    "section": "2 BPS と FECMC の比較",
    "text": "2 BPS と FECMC の比較\n\n2.1 ポテンシャルの挙動\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 動径運動量の挙動 (d=1000の場合)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3 有限次元周辺分布の挙動\n次元数 \\(d\\) を少しずつ大きくしながら，最初の１または２成分のみをみていく．\n\n2.3.1 BPS\nBPS は速度成分の分布を変えることで，収束先が違う．\n驚くべきことに，\\(\\mu=\\operatorname{N}_d(0,I_d)\\) と取ると，Boomerang sampler (Bierkens et al., 2020) の挙動に酷似していく．\nこれはポテンシャル \\(U\\) が定める Hamiltonian フローにトラップされていくためである．\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nしかし \\(\\mu\\) を \\(S^{d-1}\\subset\\mathbb{R}^d\\) 上の一様分布に取ると，次の通り，全く違った挙動になる：\n\n\n\n2.3.2 FECMC"
  },
  {
    "objectID": "posts/2025/DiffusionModels/DiscreteDiffusion.html#拡散言語モデル",
    "href": "posts/2025/DiffusionModels/DiscreteDiffusion.html#拡散言語モデル",
    "title": "離散空間上の拡散確率モデル",
    "section": "2 拡散言語モデル",
    "text": "2 拡散言語モデル\n\n2.1 はじめに\n離散拡散モデルは言語や DNA (Avdeyev et al., 2023) などの生物情報に盛んに応用されている．本稿では言語モデリングへの離散拡散モデルの応用をまとめる．\n\n\n2.2 BERT (Devlin et al., 2019) は one-step 離散拡散モデルである\n\\(N=1,t_1=1\\) とした one-step diffusion model を考え，さらに \\(Q_t\\) としては一様核と脱落核を重ね合わせたものを取る： \\[\nQ=\\alpha\\boldsymbol{1}_Ke_m^\\top+\\frac{\\beta}{K}\\boldsymbol{1}_K\\boldsymbol{1}_K^\\top+(1-\\alpha-\\beta)I_K.\n\\]\nこれにより，各トークンを各ステップで \\(\\alpha=10\\%\\) でマスクし，\\(\\beta=5\\%\\) で一様にリサンプリングし，これを元に戻す逆過程を学習することになる．\nこれが定める目的関数 \\(\\mathcal{L}\\) (1) は BERT (Devlin et al., 2019) と全く同じ目的関数になる．\nMaskGIT (Masked Generative Image Transformer) (Chang et al., 2022) も，画像をベクトル量子化した後に，全く同様の要領でマスク・リサンプリングをし，これを回復しようとする．これはトランスフォーマーなどの自己回帰的モデルを用いて逐次的に生成するより，サンプリングがはるかに速くなるという．\n\n\n2.3 自己回帰モデルは離散拡散モデルである\n\\(N\\) をトークン列の長さとし，\\(Q^{t_{i-1},t_i}\\) は各段階で \\(i\\) 番目のトークンを [MASK] にする決定論的な変換とする．\n詳細な対応は (Austin et al., 2021, pp. 5–6) を参照．\n続いて，Masked Language-Models も，[MASK] を次々に剥がしていく逆過程と見ることができ，目的関数 \\(\\mathcal{L}\\) は MLM 目的関数の reweight と見ることができるという (A.3 Austin et al., 2021)．"
  },
  {
    "objectID": "posts/2025/DiffusionModels/DiscreteDiffusion.html#footnotes",
    "href": "posts/2025/DiffusionModels/DiscreteDiffusion.html#footnotes",
    "title": "離散空間上の拡散確率モデル",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBlock diffusion (Arriola et al., 2025) では複数のトークンをまとめてブロックとして考える．↩︎\n(Section 3 Austin et al., 2021) に簡単な文献紹介がある．(Sohl-Dickstein et al., 2015) は \\(K=2\\) の場合，(Hoogeboom et al., 2021) は一様な遷移核 \\(Q_{s,t}=Q_{t-s}\\) のみを考えた．(J. Song et al., 2021) も supplementary で同様の考察をしている．↩︎\n(A.4 Austin et al., 2021) では \\(n\\) 個の行列積として定まる \\(Q^{0,t_n}\\) を効率的に計算する方法を議論している．↩︎\nこれは (Markov, 1910)-(Dobrushin, 1956) 条件という．↩︎"
  },
  {
    "objectID": "posts/2025/Posters/OH.html#概要",
    "href": "posts/2025/Posters/OH.html#概要",
    "title": "ベイズ変数選択の計算的解決",
    "section": "概要",
    "text": "概要\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n連続緩和への Zig-Zag 過程の計算複雑性\nspike 近傍から脱出するのにかかる時間は \\(O_p(1)\\) であるのに対して，到着時刻 \\(T_\\epsilon\\) は \\(O_p(\\epsilon)\\) のオーダーである．\n従って，\\(O(\\epsilon^{-1})\\) の回数だけ \\(T_\\epsilon\\) のシミュレーションを繰り返すことでやっと \\(0\\) の近傍から脱出できる．"
  },
  {
    "objectID": "posts/2025/Posters/OH.html#オープンハウス発表ポスター一覧",
    "href": "posts/2025/Posters/OH.html#オープンハウス発表ポスター一覧",
    "title": "ベイズ変数選択の計算的解決",
    "section": "オープンハウス発表ポスター一覧",
    "text": "オープンハウス発表ポスター一覧\n\n\n\n\n\n\n\n\n\n\n新時代の MCMC を迎えるために\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#確率過程の統計解析学部",
    "href": "posts/2025/Slides/ISM.html#確率過程の統計解析学部",
    "title": "統数研での学生生活",
    "section": "2 確率過程の統計解析（学部）",
    "text": "2 確率過程の統計解析（学部）"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#確率過程による統計解析大学院",
    "href": "posts/2025/Slides/ISM.html#確率過程による統計解析大学院",
    "title": "統数研での学生生活",
    "section": "3 確率過程による統計解析（大学院）",
    "text": "3 確率過程による統計解析（大学院）\n\n\n\nPDMP"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#主観統数研進学した理由",
    "href": "posts/2025/Slides/ISM.html#主観統数研進学した理由",
    "title": "統数研での学生生活",
    "section": "4 主観：統数研進学した理由",
    "text": "4 主観：統数研進学した理由"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#section",
    "href": "posts/2025/Slides/ISM.html#section",
    "title": "統数研での学生生活",
    "section": "5 ",
    "text": "5"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#司馬博文-hirofumi-shiba",
    "href": "posts/2025/Slides/ISM_Slides.html#司馬博文-hirofumi-shiba",
    "title": "統数研での学生生活",
    "section": "1.1 司馬博文 (Hirofumi Shiba)",
    "text": "1.1 司馬博文 (Hirofumi Shiba)"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#研究テーマ",
    "href": "posts/2025/Slides/ISM_Slides.html#研究テーマ",
    "title": "統数研での学生生活",
    "section": "1.2 研究テーマ",
    "text": "1.2 研究テーマ\n\n\n\n\n\n\n確率過程の統計解析（学部）\n\n\n\n\n\n拡散過程による株価のモデリング\n\n\n\ndS_t=\\mu S_t\\,dt+\\sigma S_t\\,dB_t\n\n「正しい」パラメータ \\mu,\\sigma の推定\n\n\n\n\n\n\n\n\n\n確率過程による統計解析（大学院）\n\n\n\n\n\n拡散過程によるサンプリング (Kreis et al., 2022)\n\n\n\ndX_t = -\\frac{\\beta_t}{2}X_t\\,dt+\\sqrt{\\beta_t}\\,dB_t\n\nパラメータ \\beta_t を調整して目標の分布を学習"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#学生数",
    "href": "posts/2025/Slides/ISM_Slides.html#学生数",
    "title": "統数研での学生生活",
    "section": "2.2 学生数",
    "text": "2.2 学生数\n\n学生総数は 34 名（2024 年４月１日）\n（うち社会人が 24 名）\n\n院生室にいる固定メンバーは 6 人ほど\n（うち 3 人は博士１年）．\n\n所属研究者は 81 名（私調べ）\n\n授業は基本的に少人数\n\n準必修科目でも学生は多くて 3 人ほど\n\n先生方との距離が近い"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#経済的支援独自のものに限る",
    "href": "posts/2025/Slides/ISM_Slides.html#経済的支援独自のものに限る",
    "title": "統数研での学生生活",
    "section": "2.3 経済的支援（独自のものに限る）",
    "text": "2.3 経済的支援（独自のものに限る）\n\nリサーチ・アシスタント\n月額 10 万円の雇用が７月から３月まで\n総研大特別研究員：要申請（研究計画書＋面接）\n\nSPRING：月額19万円（学振と同額）\nBOOST：AI 特化だが月額30万（来年度で終わり）\n\n総研大研究派遣プログラム\n渡航費と研究支援金（滞在費）を援助"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#section",
    "href": "posts/2025/Slides/ISM_Slides.html#section",
    "title": "統数研での学生生活",
    "section": "2.1 ",
    "text": "2.1"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#統数研のシステム",
    "href": "posts/2025/Slides/ISM_Slides.html#統数研のシステム",
    "title": "統数研での学生生活",
    "section": "2.1 統数研のシステム",
    "text": "2.1 統数研のシステム\n\n総合研究大学院大学 ← 学生としての所属はここ\n\n先端学術院\n\n先端学術専攻\n\n統計科学コース（統計数理研究所）\n情報学コース（国立情報学研究所）\n天文科学コース（国立天文台）\n極域科学コース（国立極地研究所）\n\\qquad\\qquad\\vdots"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#進学したとき",
    "href": "posts/2025/Slides/ISM_Slides.html#進学したとき",
    "title": "統数研での学生生活",
    "section": "3.1 進学したとき",
    "text": "3.1 進学したとき\n\n経済的な支援が充実している\n\nトップ層にとっては，実は他の大学の方が充実していることもある\nしかし平均的な学生にかける金額は総研大が圧倒\n\n博士を取ることは決めていた\n\nこれは大正解\n最初から研究者のたまご扱いをしてくれる\n\n学生交流が薄い\n\nその通りだが，今年からは違う\n先輩の三戸さんが空気を変えつつある\n\n\n\n\n\n\n総研大３年 司馬博文"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#想定通りだったこと",
    "href": "posts/2025/Slides/ISM_Slides.html#想定通りだったこと",
    "title": "統数研での学生生活",
    "section": "3.1 想定通りだったこと",
    "text": "3.1 想定通りだったこと\n\n博士を取ることは決めてるなら向いてる\n\nこれは大正解だった\n最初から研究者のたまご扱いをしてくれる\n\n入ってから面白いテーマに出会える\n\nこれも大正解\n入学後に統計関連分野を見渡せる稀有な場所\n指導教員との（分野的・人間的）相性，直感が大事"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#想定外だったこと",
    "href": "posts/2025/Slides/ISM_Slides.html#想定外だったこと",
    "title": "統数研での学生生活",
    "section": "3.2 想定外だったこと",
    "text": "3.2 想定外だったこと\n\n△ 経済的な支援が充実 → ○ 総合的な支援が充実\n\nトップ層にとっては，金額的には他の大学の方が充実していることもある\nしかし平均的な学生にかける金額は総研大が圧倒\n\n△ 学生交流が薄い → ○ 同年代・同分野の交流は薄い\n\n総研大で見れば学生は多く，交流の機会も作れる\n\n先輩の三戸さんが空気を変えつつある\n\n身近な問題を相談できる人は確かに少ない"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#終わりに",
    "href": "posts/2025/Slides/ISM_Slides.html#終わりに",
    "title": "統数研での学生生活",
    "section": "3.3 終わりに",
    "text": "3.3 終わりに\n\n注意喚起\n\n半強制的な交流は少ない\n\n院生室の人数が少ない\n同じ指導教員に就く同年代の学生は基本いない\n\n主体的に動かないと何も起こらない\n自分の将来を考えて動けるのは自分だけ\n\n\n\n\n\n\n総研大３年 司馬博文"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#統数研の教育システム",
    "href": "posts/2025/Slides/ISM_Slides.html#統数研の教育システム",
    "title": "統数研での学生生活",
    "section": "2.1 統数研の教育システム",
    "text": "2.1 統数研の教育システム\n\n総合研究大学院大学 ← 学生としての所属はここ\n\n先端学術院\n\n先端学術専攻\n\n統計科学コース（統計数理研究所）\n情報学コース（国立情報学研究所）\n天文科学コース（国立天文台）\n極域科学コース（国立極地研究所）\n\\qquad\\qquad\\vdots"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#終わりに注意喚起",
    "href": "posts/2025/Slides/ISM_Slides.html#終わりに注意喚起",
    "title": "統数研での学生生活",
    "section": "3.3 終わりに：注意喚起",
    "text": "3.3 終わりに：注意喚起\n\n半強制的な交流は少ない\n\n院生室の人数は少ない\n同じ指導教員につく同年代の学生は基本いない\n\n主体的に動かないと何も起こらない\n自分の将来を考えて動けるのは自分だけ\n\n博士を目指して積極的に動ける人にとっては天国"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#総研大の学生1人当たり教育経費",
    "href": "posts/2025/Slides/ISM_Slides.html#総研大の学生1人当たり教育経費",
    "title": "統数研での学生生活",
    "section": "2.4 総研大の学生1人当たり教育経費",
    "text": "2.4 総研大の学生1人当たり教育経費\n\n日経ビジュアルデータ (2024/12/25)"
  },
  {
    "objectID": "posts/2025/Slides/ISM_Slides.html#情報提供",
    "href": "posts/2025/Slides/ISM_Slides.html#情報提供",
    "title": "統数研での学生生活",
    "section": "3.4 情報提供",
    "text": "3.4 情報提供\n\n\n\n\n\n紹介記事（本スライドの前身）\n\n\n\n\n\n\nスライドの QR コード"
  },
  {
    "objectID": "posts/2025/PDMP/ClusteringSticky.html",
    "href": "posts/2025/PDMP/ClusteringSticky.html",
    "title": "Generalizing Sticky PDMP",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\nusing PlotlyJS\n\nusing Plots\n\n# ① PlotlyJS バックエンドに切り替え\nplotlyjs()\n\n# 格子生成\nus = collect(-1.0:0.05:1.0)\nvs = collect(-1.0:0.05:1.0)\n\n# 平面を Mesh3d でプロットする関数\nfunction plane_mesh(xs, ys, zs; col)\n    PlotlyJS.mesh3d(x=xs, y=ys, z=zs, opacity=0.5, color=col, showscale=false)\nend\n\n# 各平面の座標配列を作成\nx1 = hcat([[u,u,v] for u in us, v in vs]...)\nx2 = hcat([[v,u,u] for u in us, v in vs]...)\nx3 = hcat([[u,v,u] for u in us, v in vs]...)\n\n\nusing PlotlyJS\n\n# パラメータ格子\nus = collect(-1.0:0.05:1.0)\nvs = collect(-1.0:0.05:1.0)\nm, n = length(us), length(vs)\n\n# 頂点リスト（flatten した順序で）\nx = Float64[]\ny = Float64[]\nz = Float64[]\nfor u in us, v in vs\n    push!(x, u); push!(y, u); push!(z, v)   # plane x=y\nend\n\n# 三角形フェイスを作成\ni = Int[]\nj = Int[]\nk = Int[]\nfor ii in 1:m-1\n    for jj in 1:n-1\n        p1 = (ii-1)*n + jj\n        p2 = ii*n       + jj\n        p3 = (ii-1)*n + jj+1\n        p4 = ii*n       + jj+1\n        # 2 つの三角形に分割\n        push!(i, p1-1); push!(j, p2-1); push!(k, p3-1)\n        push!(i, p2-1); push!(j, p4-1); push!(k, p3-1)\n    end\nend\n\ntrace = PlotlyJS.mesh3d(\n    x=x, y=y, z=z,\n    i=i, j=j, k=k,                # ★自前の分割インデックス\n    opacity=0.5, color=\"red\",\n    showscale=false\n)\n\nmesh3d with fields color, i, j, k, opacity, showscale, type, x, y, and z\n\n\n\nusing PlotlyJS, PlotlyBase\n\nusing Plots\nplotlyjs()\n\n# (1) プロットを作成\nplt = PlotlyJS.Plot(\n    [\n      trace,\n      plane_mesh(x2[1,:], x2[2,:], x2[3,:], col=\"green\"),\n      plane_mesh(x3[1,:], x3[2,:], x3[3,:], col=\"blue\"),\n    ],\n    Layout(\n      scene = attr(\n        xaxis_title=\"x\", yaxis_title=\"y\", zaxis_title=\"z\"\n      )\n    )\n)\n\nio = IOBuffer()\nPlotlyBase.to_html(io, plt, full_html=false)\nHTML(read(io, String))  # Quartoはこの生HTMLをそのまま出力に埋め込みます\n\n\n\n\nusing Plots\nplotlyjs()                 # プロットバックエンドにPlotlyJSを指定\nPlots.plot(plt)   # 3次元メッシュもOK\n\nplt"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html",
    "href": "posts/2025/Slides/ISM.html",
    "title": "統数研での学生生活",
    "section": "",
    "text": "確率過程の統計解析（学部）\n\n\n\n\n\n\n拡散過程による株価のモデリング\n\n\n\ndS_t=\\mu S_t\\,dt+\\sigma S_t\\,dB_t\n\n「正しい」パラメータ \\mu,\\sigma の推定\n\n\n\n\n\n\n\n\n\n確率過程による統計解析（大学院）\n\n\n\n\n\n\n拡散過程によるサンプリング (Kreis et al., 2022)\n\n\n\ndX_t = -\\frac{\\beta_t}{2}X_t\\,dt+\\sqrt{\\beta_t}\\,dB_t\n\nパラメータ \\beta_t を調整して目標の分布を学習"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#自己紹介",
    "href": "posts/2025/Slides/ISM.html#自己紹介",
    "title": "統数研での学生生活",
    "section": "",
    "text": "確率過程の統計解析（学部）\n\n\n\n\n\n\n拡散過程による株価のモデリング\n\n\n\ndS_t=\\mu S_t\\,dt+\\sigma S_t\\,dB_t\n\n「正しい」パラメータ \\mu,\\sigma の推定\n\n\n\n\n\n\n\n\n\n確率過程による統計解析（大学院）\n\n\n\n\n\n\n拡散過程によるサンプリング (Kreis et al., 2022)\n\n\n\ndX_t = -\\frac{\\beta_t}{2}X_t\\,dt+\\sqrt{\\beta_t}\\,dB_t\n\nパラメータ \\beta_t を調整して目標の分布を学習"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#他の大学院との違い",
    "href": "posts/2025/Slides/ISM.html#他の大学院との違い",
    "title": "統数研での学生生活",
    "section": "2 他の大学院との違い",
    "text": "2 他の大学院との違い\n① 教育システム\n② 学生数：教員数の比\n③ 経済的支援\n\n2.1 統数研の教育システム\n\n総合研究大学院大学 ← 学生としての所属はここ\n\n先端学術院\n\n先端学術専攻\n\n統計科学コース（統計数理研究所）\n情報学コース（国立情報学研究所）\n天文科学コース（国立天文台）\n極域科学コース（国立極地研究所）\n\\qquad\\qquad\\vdots\n\n\n\n\n\n\n2.2 学生数\n\n学生総数は 34 名（2024 年４月１日）\n（うち社会人が 24 名）\n\n院生室にいる固定メンバーは 6 人ほど\n（うち 3 人は博士１年）．\n\n所属研究者は 81 名（私調べ）\n\n授業は基本的に少人数\n\n準必修科目でも学生は多くて 3 人ほど\n\n先生方との距離が近い\n\n\n\n\n2.3 経済的支援（独自のものに限る）\n\nリサーチ・アシスタント\n月額 10 万円の雇用が７月から３月まで\n総研大特別研究員：要申請（研究計画書＋面接）\n\nSPRING：月額19万円（学振と同額）\nBOOST：AI 特化だが月額30万（来年度で終わり）\n\n総研大研究派遣プログラム\n渡航費と研究支援金（滞在費）を援助\n\n\n\n2.4 総研大の学生1人当たり教育経費\n\n\n\n日経ビジュアルデータ (2024/12/25)"
  },
  {
    "objectID": "posts/2025/Slides/ISM.html#自分の場合",
    "href": "posts/2025/Slides/ISM.html#自分の場合",
    "title": "統数研での学生生活",
    "section": "3 自分の場合",
    "text": "3 自分の場合\n① 想定通りだったこと\n② 想定外だったこと\n③ まとめと注意喚起\n\n3.1 想定通りだったこと\n\n博士を取ることは決めてるなら向いてる\n\nこれは大正解だった\n最初から研究者のたまご扱いをしてくれる\n\n入ってから面白いテーマに出会える\n\nこれも大正解\n入学後に統計関連分野を見渡せる稀有な場所\n指導教員との（分野的・人間的）相性，直感が大事\n\n\n\n\n3.2 想定外だったこと\n\n△ 経済的な支援が充実 → ○ 総合的な支援が充実\n\nトップ層にとっては，金額的には他の大学の方が充実していることもある\nしかし平均的な学生にかける金額は総研大が圧倒\n\n△ 学生交流が薄い → ○ 同年代・同分野の交流は薄い\n\n総研大で見れば学生は多く，交流の機会も作れる\n\n先輩の三戸さんが空気を変えつつある\n\n身近な問題を相談できる人は確かに少ない\n\n\n\n\n3.3 終わりに：注意喚起\n\n半強制的な交流は少ない\n\n院生室の人数は少ない\n同じ指導教員につく同年代の学生は基本いない\n\n主体的に動かないと何も起こらない\n自分の将来を考えて動けるのは自分だけ\n\n博士を目指して積極的に動ける人にとっては天国\n\n\n3.4 情報提供\n\n\n\n\n\n紹介記事（本スライドの前身）\n\n\n\n\n\n\nスライドの QR コード"
  },
  {
    "objectID": "posts/2024/Life/SOKENDAI.html",
    "href": "posts/2024/Life/SOKENDAI.html",
    "title": "総合研究大学院大学５年一貫博士課程のすすめ",
    "section": "",
    "text": "総研大と鎌谷研の学生募集ですわ〜！ https://t.co/kv27wI4wtV— 総合研究大学院大学お嬢様部 (@sokenojou) June 5, 2024"
  },
  {
    "objectID": "posts/2024/Life/SOKENDAI.html#統数研での５年一貫博士課程",
    "href": "posts/2024/Life/SOKENDAI.html#統数研での５年一貫博士課程",
    "title": "総合研究大学院大学５年一貫博士課程のすすめ",
    "section": "1 統数研での５年一貫博士課程",
    "text": "1 統数研での５年一貫博士課程\n\n1.1 総合研究大学院大学について\n統計数理研究所において研究教育を受けたい場合，大学院生としては\n\n\n\n\n\n\n\n総合研究大学院大学\n\n先端学術院先端学術専攻\n\n統計科学コース\n\n\n\n\n\n\nに所属することになります．\n上位概念から解説して参ります．\n\n総合研究大学院大学（略して 総研大）\n学部を持たない国立（大学院）大学です．1 1988 年の開学以来 35 年の歴史を持ちます．\n先端学術院先端学術専攻\n総合研究大学院大学はもともと複数の専攻に分かれてしましたが，１つの「先端学術院先端学術専攻」に統合され，20 個のコースに分かれている，という扱いになりました．受験者にとって大事なことには，このことにより定員に関する融通がより効くようになりました．このこともあり，統計科学コースは定員２名となっていますが，2024 年度のように受験者数２名でも合格者が０名などの展開が十分に起こり得ます．\n統計科学コース\n全 20 コースの一覧は こちら からご覧になれます．それぞれのコースが 基盤機関 と呼ばれる研究所と対応しており，実際の教育研究はその研究所で行われます．2 統数研の他には，国立天文台，宇宙科学研究所（JAXA の一部），国立民俗学博物館，国立情報学研究所などが基盤機関としてあります．\n\n\n\n1.2 学生数について\n\n\n\n\n\n\n定員について\n\n\n\n\n\n５年一貫博士課程の定員は２名ですが，2021 年度から2023 年度は１名，2024 年度は０名でした．\n一方で，３年次編入（通常の博士課程に相当）の定員が６名で，毎年ほとんど６名の方が入学されます．例年，そのうちの \\(2/3\\) が社会人学生です．\n\n\n\n2024 年４月１日の学生総数は \\(34\\) 名で，うち社会人が \\(24\\) 名になります．大多数が社会人であることの影響で，社会人学生以外の学生を フルタイム学生 と呼ぶ伝統があります．\n社会人学生の方は基本的にリモートでの研究指導を受けることになりますから，研究所には半年に一回来れれば良い方だという方も多いです．\nそのためフルタイム学生は同期の存在が期待できないだけでなく，そもそも研究所に来てもほとんど他の学生に会えないことも多いです．\n\n\n1.3 教員陣について\n\n\n\n\n\n\nチェックすべきリンク集\n\n\n\n\n統数研の教員と専門一覧\n統数研の教員の主な教育内容\n総研大のシラバス\n\n\n\n統数研には授業を開講している専任教員だけで 81 人います．\nまた上掲の 統数研の教員と専門一覧 から分かる通り，凡そ統計に関連のある分野が理論・応用を分けずに網羅されており，それもスター研究者が揃っています．\n統計は，応用をすれば大変多くの分野に応用されますし，理論的なアプローチも数学・物理・最適化など多様ですから，普通の大学院ならば複数の研究科・専攻に散らばっているはずの教員陣が１つの研究所に揃っている ことは，統計を学生として学ぶにあたって大変な恩恵になります．\n例えば 統計数理セミナー はこのことを端的に感得する良い機会です．筆者も，統数研の教員陣の真の裾の広さを，ここで初めて実感することができました．\n\n\n\n\n\n\n統計数理セミナーについて\n\n\n\n\n\n統数研では 統計数理セミナー と呼ばれる研究発表が毎週水曜日 16:00~17:20 （40 分のセッションが２回）に開催されており，全ての教員は毎年１回は発表することになっています．\n統計数理セミナーは，学生が出席報告とレポート提出をすることで単位を取得することも出来る授業としても開講されています．そこで多くの分野の最新の研究に触れることになり，必ずや新たな発見があることでしょう．\n特に，数学と数理統計の一部しか知らずに入学した筆者にとっては，実際の統計モデリングの研究（地震データ，天文データ，統数研が実施している 国民性調査 のサーベイデータ），機械学習の理論研究，マテリアルズインフォマティクスへの応用などが，そこで出会った新しいトピックでした．\n以上のトピックは思いつく順番に挙げたまでですが，多岐に渡る研究分野の存在と，その分野でのスターが揃っていることを，入学してしばらくしてようやく理解されてきました．\n\n\n\n\n\n1.4 授業について\n統数研では教員が学生数の３倍近くいるため，授業は大変充実しています．しかしその分，多くの授業では教員と１対１になることが想定されます．\nですが，先生がメーリングリストで追加の参加者を呼びかけたり，学生側が他の学生を誘い合わせたりして，参加人数を増やして授業を盛り上げることもできます．\n開講が珍しい授業ですと，教員陣も学生として参加していることも珍しくありません．\n実際筆者も今まで７つの講義を取りましたが，まだ１対１の授業は経験していません．一方，受講者が２人だけということは３回ほど経験しています．\nまた，社会人学生が \\(2/3\\) を占めている以上，リモートやハイブリッドでの開講や日程調整については極めて柔軟に対応されます．\n筆者は統数研から遠い片道２時間弱の場所に住んでいるため，なるべく対面授業の日は１日にまとめたく，よく日程を調整していただいています．\n\n\n1.5 金銭支援について\n\n\n\n\n\n\nチェックすべきリンク集\n\n\n\n\n統数研のページ「奨学金等」\n総研大のページ「学費・各種経済支援」\n\n\n\n\nリサーチ・アシスタント\n５年の間いつでも，申請をすることで RA としての雇用を希望することができます．月額約 10 万円の給与が７月から３月まで支給されます．１年ごとの更新です．3\n総研大特別研究員\n博士後期課程（５年一貫課程の３年次以降）では，特別研究員制度へ応募することができます．これは学振の特別研究員（DC1, DC2）に似た制度です．総研大の学生は学振のものと併せて２つの特別研究員制度が利用可能ということになります．\n\n授業料免除 や 奨学金 を除いて，統計科学コース独自のものは基本この２つです．\n他にも，次の研究支援があります：\n\n研究環境整備費\n初年度に 20 万円を上限に，PC やモニターなどの研究環境を整えるために，学生に予算が配分されます．\nSOKENDAI 研究派遣プログラム\n数日〜１年前後の多岐に渡る期間について，学生の海外での研究滞在にかかる費用を 100 万円を上限として援助してもらえるというプログラムです．4\n\n\n\n1.6 修士号について\n五年一貫博士課程では，基本的には 博士（統計科学） の学位のみが授与されます．5\nしかし特例として，２年以上在籍し，特定の要件を満たした場合は，中途退学と同時に修士の学位を得ることもできます．\n詳しくは (学生便覧, 2023, p. 5) をご参照ください．\n\n\n1.7 図書室について\n統数研での研究環境について，筆者が最後に触れたいのは 図書室 の存在です．\n統数研の歴史は極めて長く，戦前の 1944 年に設立され，2004 年度までは文部省直轄の研究機関でした．\nそのこともあってか，殊に統計と数学に関しては膨大な文献が揃っています．特に古典的な文献については，筆者は今のところ参照したい書籍が統数研図書室で見当たらなかったことはほとんどありません．\n歴史的に貴重な文献が「文部省統計数理研究所」の印と共に２冊ほど所蔵されている，という経験が何度もあります．最新の書籍も和書ならば寄贈や，同研究所の先生方がすでに買っているかなどのことも多く，よく揃っています．\nなお，電子リソース（論文や e-book）も，統計・数学関連のものは統数研内のネットワークからアクセスすることで殆どが閲覧できます．自宅にいても，ssh を通じた所内ネットワークへのリモートアクセスで随時論文などの電子リソースにアクセスすることができます．"
  },
  {
    "objectID": "posts/2024/Life/SOKENDAI.html#学生生活について私の場合",
    "href": "posts/2024/Life/SOKENDAI.html#学生生活について私の場合",
    "title": "総合研究大学院大学５年一貫博士課程のすすめ",
    "section": "2 学生生活について（私の場合）",
    "text": "2 学生生活について（私の場合）\n以上が統数研で学生生活をするにあたっての基本的な情報です．\n具体的なイメージが湧きましたでしょうか？\n以降，統数研への進学を検討している方に向けて，わたしの主観的な見解も交えて，統数研での学生生活の美点と欠点について詳しい紹介をします．\n\n2.1 結論\n結論として，統数研での学生生活は 学生が少ないという欠点を除けばこの上なく理想的な環境だ と考えます．\nその理由は次の通りです：\n\n\n\n\n\n\n\n図書室が充実していて，殆どの文献をすぐに参照することができる（第 1.7 節）．\n計算機準備予算も付くので，大きなモニターと高性能な計算機で研究・勉強ができる（第 1.5 節）．\n先生方との距離が近く，研究に関する相談もしやすい（第 1.3 節）．\n事務の方との距離が近く，込み入った相談や提出物の忘れなども，時には先回りをして親切に対応していただける（第 2.3 節）．\nおよそ統計に関連する分野のエキスパートが，単一の機関に揃っている（第 2.5 節）．\n\n\n\n\n\n\n2.2 【注意喚起】同期が居ないことについて\nまず最初に，最も重要な点を述べます．\n統数研での博士課程は，通常の大学院生活とは全く違うものになることは確かです．\n最大の違いは，研究室の同期・先輩が居ない代わりに，教員・事務の先生方との距離が近い，ということです．6\n最初から「大人」としての振る舞いが求められる，とでも言うべきでしょうか．\n換言すれば，学生は研究所内では極めて少数派ですので，先生方も事務の方々も，学生が何で困っているのか，本当に想像がつかない場合が多いのです．さらに言えば，あなたが学生であることも言わなきゃわからないことが多いです．\nですから，少しでもわからないことや不安なことがあれば，先生方も事務の方にはっきりと「自分は学生である」ということと「こういうことで困っている」と言語化してコミュニケーションを自分から取りにいかないと，困った状況に陥りやすいです．このことが，最大の注意点でしょう．\n例えば研究や資料準備などがうまく行っていなくて後ろめたい気持ちがあり，大人とのコミュニケーションを避けがちな精神状態であるときは尚更ですから，普通の大学院生活よりも「時すでに遅し」という状況に陥りやすいと言えるかも知れません．\nこの点から，「基本的には，特別な理由がない限り，５年一貫博士課程は勧めない」という立場の先生方も多いです．\n「研究室内に溜まった暗黙のノウハウ」のようなものもありませんから，学振特別研究員の申請，奨学金・授業料免除などの諸々の申請，図書館カードを作ってもらう，研究費の使い方，発表用ポスターの作り方，スパコンの使い方，eduroam を使えるようにしてもらう（以降略）など，積極的に先生方と事務の方々に，時には，「すみません，一から何をすべきか分からないかもしれません」と「迷惑」をかけていく姿勢が必要になることもあるかもしれません．\nその点，研究所外でのつながりも重要です．\n\n\n\n\n\n\nさらに踏み込んだアドバイス\n\n\n\n\n\n「迷惑」と書きましたのは，実際は迷惑ではないからです．自分で調べて解決しようとしても，すでに共有された慣習やルールがあるかもしれませんから，自己流にやってミスる／手遅れになるよりかは，わからないことはわからないと聞くのが「正解」であり，想定されています．\n自分からアクションを起こさなきゃいけないのは，５年一貫制博士課程の入学者が毎年０か１かですので，先生方も事務の方も本当に忘れている場合の方が多いです．\nその意味で，学生はマイノリティですから，「自分は学生としてここにいる」という気持ちを強く持って，研究生活を歩むのが大事だと思います．\nさらに言えば，マイノリティである学生の中でも社会人学生が大半を占めます．社会人学生の方の多くは大変にコミュニケーションがうまく，先生や事務の方と本音で話をしたり，うまく協力関係を構築したりすることが大変上手な方も多いです．\nそれゆえ，学部からストレートで統数研に来た場合，右往左往してしまうことが多くなるかもしれません．\nしかし統数研は学生数が少ないため一人当たりのリソースが多く，社会人の方が共同研究を遂行するには最高の環境でしょう．学生のみなさんも，以上の理由からみすみすこの環境を逃すにはもったいないと考えます．\nそこで，本記事を用意したのでした．\n\n\n\n\n\n2.3 学生生活の支援\n先生方も事務の方も，困っている学生を助けたいと常に配慮してくださいますし，上述の通り，支援制度と研究環境は大変充実しています．\nまたとりわけ，大学院担当の事務の方々は，学生の背景もよく覚えていてくださり，申請書を書く際にいくつかわからない項目や揃わない書類があった際は親身になって相談に乗ってくださいます．\n例えば，筆者のミスで出席連絡メールが数回不着であったときは「司馬くんのことだし出席してるよね？もしかして送信先ミスってない？」と気づいてくださったりもしました．\nですから，良い環境で研究したいと主体的に思える方は，ぜひ統数研の５年一貫博士課程を検討してみて欲しい，その気持ちで筆を取っております．\n健康診断も職員と同一扱いであるため，めっちゃすごいところでの検診になります．学部の頃は移動式のバスとかだったのに（もちろんそれが劣るわけではないですが），健康診断専用の宿泊も可能な施設での検診です．すごい安心感．\n\n\n2.4 研究室の様子\n先ほど第 2.2 節で「同期がいない」と強調しました．５年一貫博士課程の場合は，通例あなたひとりになりますから，同期はいないと思って良いでしょう．\n一方で，研究室には先輩がいる可能性があります．次に述べます通り，学生がいる研究室はいくつかの研究室に集中しているためです．\n筆者の入学時，５年一貫博士課程生は筆者を入れて４名，３年次編入学生が33名でした．\n\n\n\n\n\n\n学生が３名以上居る研究室（2023 年度当時）\n\n\n\n\n野間研究室 ５名\n日野研究室 ５名\n二宮研究室 ５名\n藤澤研究室 ４名\n持橋研究室 ４名\n\n\n\n\n\n\n\n2.5 授業について\n\n\n\n\n\n\nチェックすべきリンク集\n\n\n\n\n統数研ページ「担当教員・授業科目」\n統数研ページ「終了要件」\n\n\n\n５年一貫博士課程の卒業要件では，必修授業（研究指導）を除いて 20 単位の取得が必要です (学生便覧, 2023, p. 22)．\n私は修士の１年の間に（必修を除いて）12 単位を取得しましたので，あと４授業分を４年で消化すれば良いことになります．\n12 単位を取ったと言っても，すべて「１年の間に取るべきだ」と考えた準必修の入門的なオムニバス講義でしたから，単位取得や成績の確保に追われるようなことはなく，自分の学びたいことをじっくり学べた１年でした．7\n\n\n\n\n\n\n筆者が１年目にとった授業\n\n\n\n\n\n統計科学コースには準必修と呼ぶべき「取らなきゃいけない授業」はありませんが，修士のうちに履修すべきとされる基本的な内容を扱う 基礎科目 と呼ばれる授業が全部で８つあります．\nそのうち，次の３つを筆者は履修しました．\n\n\n\n\n\n\n多変量解析\n\n\n\n\n\n疫学と医療統計の 野間先生 や，環境統計の 金藤先生，調査の 前田先生 と 朴先生，金融データの 山下先生，差分プライバシーの 南先生 と，各分野で多変量解析を実際に使って研究なさっている先生方によるオムニバス講義です．\n何よりも，各分野によって多変量解析への姿勢が少しずつ違い，教科書で読むだけでは無味乾燥であった多変量解析が極めて身近で手触りを感じました．\n以下はシラバスの抜粋です．\n\n下記の構成で授業を行う。（）内は主な担当教員である。\n\n\nガイダンス（多変量データ解析概観）（野間）\n多変量データの取得と合成変量モデルによる解析（前田）\n多変量の推測統計の初歩（多変量正規分布，多変量T2検定など）（金藤）\nカテゴリカルデータの解析（分割表の解析）（野間）\n多変量データのための回帰分析・重回帰分析 (1)（南）\n多変量データのための回帰分析・重回帰分析 (2)（南）\n離散変数と定性データの分類・統計モデル (1) （山下）\n離散変数と定性データの分類・統計モデル (2)（山下）\n一般化線型モデル(1) 一般論，推測手法（金藤）\n一般化線形モデル(2) ロジスティック回帰，ポアソン回帰（野間）\n主成分分析と関連手法（朴）\n因子分析・共分散構造分析（朴）\n正準相関分析と数量化の方法（前田）\nクラスター分析・MDS（前田）\nまとめ・発展的話題（山下・南）\n\n\n\n\n\n\n\n\n\n\n計算推論基礎\n\n\n\n\n\n指導教員の 鎌谷先生 による MCMC の授業だけでなく，データ同化を研究する 中野先生 から粒子フィルターについて，統計物理学出身の 坂田先生 からグラフィカルモデルについて，自然言語処理のエキスパートである 持橋先生 からノンパラメトリックベイズについて学べた授業です．\nどの内容に対しても先生方の背景がピッタリであることにお気づきでしょうか．\n計算推論の手法は数理的に高度ですが，どの分野から生まれた手法で，どのような問題に適用されているかをセットで学ぶことで，稀有な直感が醸成されます．\n指定の教科書を読むだけでは得られない肌感を得るのに最適な授業で，出席する前と後での理解度の差が歴然としていました．\n以下はシラバスの抜粋です．\n\nイントロダクション\n乱数の使い方：ブートストラップ法，モンテカルロ法\nＭＣＭＣ入門\nベイズ統計入門\n階層ベイズモデルとベイズ平滑化\n状態空間モデル，逐次ベイズ推定，粒子フィルタ\n粒子フィルタの応用と発展\nグラフィカルモデルの基礎\n有向グラフと無向グラフ\n様々なグラフィカルモデル\n確率伝搬法とその周辺\n混合ガウス分布とK-means法\nEMアルゴリズム\n変分ベイズEMアルゴリズムとその応用\nVariational Autoencoder (VAE)とその応用\n\n\n\n\n\n\n\n\n\n\n統計的機械学習基礎\n\n\n\n\n\n日本のカーネル法・深層学習研究の第一線を走っていらっしゃる 福水先生 から直接深層学習について学べたことが大変印象深かったものでした．\nそれだけでなく，マテリアルズインフォマティクスで機械学習がどう活かされるかを 吉田先生 と Wu 先生 先生から学び，学習理論を 日野先生 から学び，Gauss 過程法を 松井先生 から学ぶことが出来る授業です．\n以下はシラバスの抜粋です．\n\n総論（機械学習とは？　教師あり学習、教師無し学習、モデル選択）\nカーネル法　（カーネル法の基礎と方法）\nサポートベクターマシン I　（SVMの基礎）\nサポートベクターマシン　II　（SVMの基礎と発展）\nアンサンブル法（ランダムフォレスト、バギング、ブースティングなど）\n深層学習 I　（深層学習の基礎）\n深層学習 II 　（深層学習の発展）\nガウス過程I（ガウス過程の概念と基礎）\nガウス過程II（カウス過程の方法）\n様々な問題設定 I（転移学習、半教師付き学習）\n様々な問題設定II （強化学習）\n様々な問題設定III （能動学習、実験計画、応用的話題）\n統計的学習理論 I　（訓練誤差と汎化誤差の理論）\n統計的学習理論 II　（訓練誤差と汎化誤差の理論）\nまとめ\n\n\n\n\n\n\n\nまた，統数研が毎年開催している公開講座も，学生ならば無料で参加ができます．"
  },
  {
    "objectID": "posts/2024/Life/SOKENDAI.html#終わりに",
    "href": "posts/2024/Life/SOKENDAI.html#終わりに",
    "title": "総合研究大学院大学５年一貫博士課程のすすめ",
    "section": "3 終わりに",
    "text": "3 終わりに\n\nページ下部のコメントからもぜひお気軽にお問い合わせください．"
  },
  {
    "objectID": "posts/2024/Life/SOKENDAI.html#footnotes",
    "href": "posts/2024/Life/SOKENDAI.html#footnotes",
    "title": "総合研究大学院大学５年一貫博士課程のすすめ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nなお，博士課程のみの国立大学院大学としては，日本初であるという（Wikipedia）．↩︎\n総研大本部は葉山にキャンパスありますが，そこに通うことは，統合進化科学研究センターに対応する統合進化科学コースを除いてありません．↩︎\n収入の要件があります．例えば学振特別研究員はここに書いた意味での RA を利用できません．また，指導教員が別の予算を持っている場合は，その予算から追加で（二重に）雇用される場合もあります．↩︎\n国内での研究滞在にも同様の援助プログラムがあります．↩︎\n場合によっては博士（学術）も授与されます．↩︎\n現在，統数研では，複数名の学生がいる教員も５名ほど居ます．ですが，それでも同期は基本的におらず，先輩もすぐに卒業してしまったり，社会人であり殆ど連絡を取る機会がない，という場合が多いです．同期・先輩の存在を期待して進学するのは危険でしょう．↩︎\n例えば，計 20 単位を５年で消化することにして，１学期に１授業ずつ取ることも，制度としては出来ます．↩︎"
  },
  {
    "objectID": "posts/2025/PDMP/ClusteringSticky.html#section",
    "href": "posts/2025/PDMP/ClusteringSticky.html#section",
    "title": "Generalizing Sticky PDMP",
    "section": "1 ",
    "text": "1"
  },
  {
    "objectID": "posts/2025/Life/Themes.html",
    "href": "posts/2025/Life/Themes.html",
    "title": "博士課程での研究テーマ",
    "section": "",
    "text": "博士課程の標準卒業年限まで残り３年を切ったこの段階で，現時点で考えている研究の方向性を５つ書き留めておきたい．\nおおまかに，統計学・機械学習における計算手法（特にサンプリング手法）を，確率過程の技術を用いて効率化する，という方向性が共通している．\nMCMC も拡散モデルも大きな成功をおさめている手法であるが，「提案時の枠組み（Markov 連鎖と拡散過程）に縛られすぎているところがあるのではないか？」というのが私の秘められたテーマである．1"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#真に高次元で効率的な-pdmp-リフレッシュ戦略の特定",
    "href": "posts/2025/Life/Themes.html#真に高次元で効率的な-pdmp-リフレッシュ戦略の特定",
    "title": "博士課程での研究テーマ",
    "section": "1 真に高次元で効率的な PDMP リフレッシュ戦略の特定",
    "text": "1 真に高次元で効率的な PDMP リフレッシュ戦略の特定\n本研究テーマは Boost の申請書 でも扱ったもので，自分が現在中核としている研究プロジェクトである．"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#hamiltonian-monte-carlo-と測度の集中現象",
    "href": "posts/2025/Life/Themes.html#hamiltonian-monte-carlo-と測度の集中現象",
    "title": "博士課程での研究テーマ",
    "section": "2 Hamiltonian Monte Carlo と測度の集中現象",
    "text": "2 Hamiltonian Monte Carlo と測度の集中現象\n(Betancourt et al., 2017), (Betancourt, 2018)"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#非定常レジームと確定的流体極限",
    "href": "posts/2025/Life/Themes.html#非定常レジームと確定的流体極限",
    "title": "博士課程での研究テーマ",
    "section": "3 非定常レジームと確定的流体極限",
    "text": "3 非定常レジームと確定的流体極限\n(Christensen et al., 2005),"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#はじめに",
    "href": "posts/2025/Life/Themes.html#はじめに",
    "title": "博士課程での研究テーマ",
    "section": "はじめに",
    "text": "はじめに\n\nMCMC の発展\n\n\n\nモンテカルロ法の発展．筆者の ポスター からの抜粋．"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#stick-technique-で何が可能になるか",
    "href": "posts/2025/Life/Themes.html#stick-technique-で何が可能になるか",
    "title": "博士課程での研究テーマ",
    "section": "4 Stick technique で何が可能になるか？",
    "text": "4 Stick technique で何が可能になるか？\n実は本研究テーマは，筆者が人生で最初に取り組んだ研究課題であるが，恐ろしいほど難しい可能性が浮上しつつある．第19回日本統計学会春季集会での発表 でも扱ったので，そちらの 予稿 も参考にしてほしい．\n(Bierkens et al., 2023), (Chevallier et al., 2023)"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#拡散モデル",
    "href": "posts/2025/Life/Themes.html#拡散モデル",
    "title": "博士課程での研究テーマ",
    "section": "5 拡散モデル",
    "text": "5 拡散モデル"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#footnotes",
    "href": "posts/2025/Life/Themes.html#footnotes",
    "title": "博士課程での研究テーマ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n確率過程の中でも，Markov 連鎖は非負行列，拡散過程は SDE という，直感的な記法と calculus が確立された爛熟した分野であることは注記に値するだろう．しかしこれが数学的に自然なものの見方であるとは限らない．私見ではあるが，Markov 過程や確率微分方程式などの確率過程の枠組みは，確率積分がセミマルチンゲールの枠組みにまで一般化されたことを皮切りに，この１世紀でまだまだ化ける要素を秘めていると思う．それくらいに底が見えない領域であり，Lebesgue 積分の定義，Kolmogorov による確率論の公理化などに匹敵するような，分野としてもう一段階の変化をまだ隠しているのではないかと感じる．↩︎"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#背景",
    "href": "posts/2025/Life/Themes.html#背景",
    "title": "博士課程での研究テーマ",
    "section": "背景",
    "text": "背景\n\nMCMC の確立\nMCMC の手法自体は (Metropolis et al., 1953) が始まりであり，以降統計物理分野で発展を遂げていった．Gibbs サンプラーに対応する手法である熱浴法 (Glauber, 1963) もこの時点で既に現れる．\n一般の確率分布からのサンプリングを可能にする手法である，というより抽象的な理解は (Hastings, 1970) で確立された．その際に，Markov 連鎖を記述するには遷移行列 \\(P\\) を一つ構築すれば良い，そしてその必要条件は簡単な式（詳細釣り合い条件）で表せる，という数学的な手軽さが大いに役立ったと思われる．\n真に統計分野で再発見されたのはコンピュータが普及したのちである．画像処理と空間統計の分野で MCMC が採用され活発に研究され，その中で (Geman and Geman, 1984) により Gibbs サンプラーの名前がついた．これが真に一般の統計分野にまで普及したのは，統計学の一流誌で紹介した (Gelfand and Smith, 1990) からだと言われる．\nこうして統計の広い分野に普及した MCMC は，大きく応用分野での発展と手法的発展の２つに分かれた．応用分野では，Gibbs サンプラーが今での重要な位置を占めていると言える．これらのモデルの構造を利用した MCMC アルゴリズムは数多く提案されている．\n\n\n汎用 MCMC 手法の発展\n一方で，モデルによらずブラックボックスで適用できる汎用手法の探索は，独自の経路を辿ったと言える．筆者の専門も専らこちらである．まず (Grenander and Miller, 1994) のコメントにおいて，(Besag, 1994) が，分布 \\(\\pi\\) に収束する Langevin dynamics \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) \\[\ndX_t=\\nabla(\\log\\pi(X_t))\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t,\\qquad\\beta&gt;0,\n\\tag{1}\\] を提案分布に用いた Hastings 法を示唆し，これは現在でも統計・機械学習の双方で理論的に重要な位置を占めている．\n理論解析には主に２つのアプローチがある．１つはスケーリング解析であり，もう１つはスペクトル解析である．後者は筆者の専門ではないが，対数 Sobolev 不等式，等周不等式，対数凸分布，などがキーワードになる．\n前者では，空間の次元 \\(d\\to\\infty\\) の極限において，\\(\\{X_t\\}\\) の時間と空間についてのスケーリングであって，確定的な極限を持つスケールを探す．続いて，そのダイナミクスについて中心化し，Gauss 過程に収束するような中心極限定理のスケールを特定する．\n後者の中心極限定理のスケールにおける極限を調べ，その極限過程の平均自乗移動距離を比較することで，アルゴリズムの「速さ」を比較するというものである．これは，当該の過程からの出力を用いた Monte Carlo 推定量の漸近分散を比較する，という明確な意味を持つ．\n本アプローチは (Roberts et al., 1997) により創始され，(Roberts and Rosenthal, 2001) に Statistical Science 誌での解説記事がある．\n以上，MCMC が定める確率過程に対する理論解析のアプローチを２つ紹介したが，アルゴリズムとしての解析や計算複雑性の議論はまた別である．上述の２つの過程に対する解析も，計算複雑性に対する含意が可能である．例えばスケーリング解析については (Roberts and Rosenthal, 2016) など．\nアルゴリズムに目を向けると，１つ厄介な点に気づく．そもそも Langevin 拡散過程 (1) をシミュレートするには離散化が必要であり，その際の誤差によって，離散化して得る Markov 連鎖の平衡分布は元々の \\(\\pi\\) からズレてしまう．統計分野ではこのズレを補正するために Hastings の棄却法の枠組みが要請される．すると元々の Langevin 拡散 (1) と結果として得る MCMC とは，収束特性が異なる場合がある (Roberts and Rosenthal, 1998)．補正なしのものを ULA (Unadjusted Langevin Algorithm)，ありのものを MALA (Metropolis-adjusted Langevin Algorithm) と呼ぶ．\nULA と MALA では当然計算複雑性が違う．MALA では各棄却法のステップで尤度比の評価が必要になり，データ数に応じて計算量が増える．一方で ULA を Monte Carlo 法に用いると漸近的に消えないバイアスを持つ．そこで，ULA のような特性を持つ手法を低精度手法，MALA を高精度手法と呼び，前者から初めて，途中でこの２つを切り替えることで，良いところどりをすることができる．この議論は (Altschuler and Chewi, 2023) から始まり，機械学習の分野で盛んである．\n\n\nHastings の枠組みからの離陸\n以上の議論は全て，(Hastings, 1970) の枠組みの下での MCMC 手法であった．提案 \\(Q\\) には大きな自由度があり，これを補正する形で所望の遷移核 \\(P\\) を得る，という構成法である．\n離散時間の Markov 連鎖を生成することに拘らなければ，この枠組みから逸脱した Markov 過程を容易に作り出すことができる．\nその主なアイデアは，所与のダイナミクスに対して，ジャンプによって制御するというものである．\n\n\n\nモンテカルロ法の発展．筆者の ポスター からの抜粋．\n\n\nPDMP は rejection-free MCMC などのキーワードの下で，当初は Event-Chain Monte Carlo の名前で，やはり統計物理分野で提案された (Bernard et al., 2009)．\n\n\n拡散モデルの発展"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#mcmc-の背景",
    "href": "posts/2025/Life/Themes.html#mcmc-の背景",
    "title": "博士課程での研究テーマ",
    "section": "MCMC の背景",
    "text": "MCMC の背景\n\nMCMC の確立\nMCMC の手法自体は (Metropolis et al., 1953) が始まりであり，以降統計物理分野で発展を遂げていった．Gibbs サンプラーに対応する手法である熱浴法 (Glauber, 1963) もこの時点で既に現れる．\n一般の確率分布からのサンプリングを可能にする手法である，というより抽象的な理解は (Hastings, 1970) で確立された．その際に，Markov 連鎖を記述するには遷移行列 \\(P\\) を一つ構築すれば良い，そしてその必要条件は簡単な式（詳細釣り合い条件）で表せる，という数学的な手軽さが大いに役立ったと思われる．\n真に統計分野で再発見されたのはコンピュータが普及したのちである．画像処理と空間統計の分野で MCMC が採用され活発に研究され，その中で (Geman and Geman, 1984) により Gibbs サンプラーの名前がついた．これが真に一般の統計分野にまで普及したのは，統計学の一流誌で紹介した (Gelfand and Smith, 1990) からだと言われる．\nこうして統計の広い分野に普及した MCMC は，大きく応用分野での発展と手法的発展の２つに分かれた．応用分野では，Gibbs サンプラーが今での重要な位置を占めていると言える．これらのモデルの構造を利用した MCMC アルゴリズムは数多く提案されている．\n\n\n汎用 MCMC 手法の発展\n一方で，モデルによらずブラックボックスで適用できる汎用手法の探索は，独自の経路を辿ったと言える．筆者の専門も専らこちらである．まず (Grenander and Miller, 1994) のコメントにおいて，(Besag, 1994) が，分布 \\(\\pi\\) に収束する Langevin dynamics \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) \\[\ndX_t=\\nabla(\\log\\pi(X_t))\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t,\\qquad\\beta&gt;0,\n\\tag{1}\\] を提案分布に用いた Hastings 法を示唆し，これは現在でも統計・機械学習の双方で理論的に重要な位置を占めている．\n理論解析には主に２つのアプローチがある．１つはスケーリング解析であり，もう１つはスペクトル解析である．後者は筆者の専門ではないが，対数 Sobolev 不等式，等周不等式，対数凸分布，などがキーワードになる．\n前者では，空間の次元 \\(d\\to\\infty\\) の極限において，\\(\\{X_t\\}\\) の時間と空間についてのスケーリングであって，確定的な極限を持つスケールを探す．続いて，そのダイナミクスについて中心化し，Gauss 過程に収束するような中心極限定理のスケールを特定する．\n後者の中心極限定理のスケールにおける極限を調べ，その極限過程の平均自乗移動距離を比較することで，アルゴリズムの「速さ」を比較するというものである．これは，当該の過程からの出力を用いた Monte Carlo 推定量の漸近分散を比較する，という明確な意味を持つ．\n本アプローチは (Roberts et al., 1997) により創始され，(Roberts and Rosenthal, 2001) に Statistical Science 誌での解説記事がある．\n以上，MCMC が定める確率過程に対する理論解析のアプローチを２つ紹介したが，アルゴリズムとしての解析や計算複雑性の議論はまた別である．上述の２つの過程に対する解析も，計算複雑性に対する含意が可能である．例えばスケーリング解析については (Roberts and Rosenthal, 2016) など．\nアルゴリズムに目を向けると，１つ厄介な点に気づく．そもそも Langevin 拡散過程 (1) をシミュレートするには離散化が必要であり，その際の誤差によって，離散化して得る Markov 連鎖の平衡分布は元々の \\(\\pi\\) からズレてしまう．統計分野ではこのズレを補正するために Hastings の棄却法の枠組みが要請される．すると元々の Langevin 拡散 (1) と結果として得る MCMC とは，収束特性が異なる場合がある (Roberts and Rosenthal, 1998)．補正なしのものを ULA (Unadjusted Langevin Algorithm)，ありのものを MALA (Metropolis-adjusted Langevin Algorithm) と呼ぶ．\nULA と MALA では当然計算複雑性が違う．MALA では各棄却法のステップで尤度比の評価が必要になり，データ数に応じて計算量が増える．一方で ULA を Monte Carlo 法に用いると漸近的に消えないバイアスを持つ．そこで，ULA のような特性を持つ手法を低精度手法，MALA を高精度手法と呼び，前者から初めて，途中でこの２つを切り替えることで，良いところどりをすることができる．この議論は (Altschuler and Chewi, 2023) から始まり，機械学習の分野で盛んである．\n\n\nHastings の枠組みからの離陸\n以上の議論は全て，(Hastings, 1970) の枠組みの下での MCMC 手法であった．提案 \\(Q\\) には大きな自由度があり，これを補正する形で所望の遷移核 \\(P\\) を得る，という構成法である．\n離散時間の Markov 連鎖を生成することに拘らなければ，この枠組みから逸脱した Markov 過程を容易に作り出すことができる．\nその主なアイデアは，所与のダイナミクスに対して，ジャンプによって制御するというものである．\n\n\n\nモンテカルロ法の発展．筆者の ポスター からの抜粋．\n\n\nPDMP は rejection-free MCMC などのキーワードの下で，当初は Event-Chain Monte Carlo の名前で，やはり統計物理分野で提案された (Bernard et al., 2009)．\n\n\nPDMP の発展\nレビュー的な文献には (Fearnhead et al., 2018) と (Vanetti, 2019) があるが，いずれも一昔前のものであり，「適用できるモデルが限られる手法である」というような言説は現在では違うと言って良い．\nここ５年の一大課題の１つには，PDMP の適用範囲の拡大，すなわち Poisson 剪定の自動化があったが，これは現在では複数の解決法を得ている．\n\n自動微分の利用による上界の自動構成 (Corbella et al., 2022), (Andral and Kamatani, 2024)\nHMC 様の splitting scheme による近似スキームと，MH の方法による修正 (Bertazzi et al., 2023), (Chevallier et al., 2025)"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#拡散モデルの背景",
    "href": "posts/2025/Life/Themes.html#拡散モデルの背景",
    "title": "博士課程での研究テーマ",
    "section": "拡散モデルの背景",
    "text": "拡散モデルの背景\n(Austin et al., 2021)"
  },
  {
    "objectID": "posts/2025/Life/Themes.html#mcmc-と-pdmp-の背景",
    "href": "posts/2025/Life/Themes.html#mcmc-と-pdmp-の背景",
    "title": "博士課程での研究テーマ",
    "section": "MCMC と PDMP の背景",
    "text": "MCMC と PDMP の背景\n最初の４つのテーマは MCMC，特に PDMP を用いた Monte Carlo 法におけるテーマである．\nこれらの背景は別稿にまとめたのでぜひ参考にしてほしい．\n\n\n\n\n\n\n\n\n\n\nMCMC と PDMP の概観\n\n\n\n2025-05-31\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/Life/Overview.html",
    "href": "posts/2025/Life/Overview.html",
    "title": "MCMC と PDMP の概観",
    "section": "",
    "text": "本稿では，筆者の博士課程で専門に選んだ MCMC，特に PDMP を用いた Monte Carlo 法について，その歴史と重要文献を挙げながら概説する．"
  },
  {
    "objectID": "posts/2025/Life/Overview.html#mcmc",
    "href": "posts/2025/Life/Overview.html#mcmc",
    "title": "MCMC と PDMP の概観",
    "section": "1 MCMC",
    "text": "1 MCMC\n\n1.1 MCMC の確立\nMCMC の手法自体は (Metropolis et al., 1953) が始まりであり，以降統計物理分野で発展を遂げていった．Gibbs サンプラーに対応する手法である熱浴法 (Glauber, 1963) もこの時点で既に現れる．1\n一般の確率分布からのサンプリングを可能にする手法である，というより抽象的な理解は (Hastings, 1970) で確立された．これが統計学の文脈における最初の MCMC の出現である．2 同時に，PDMP が出現するまで，全ての MCMC 手法は Hastings の提示した枠組みの中に収まるのである．\nその際に Markov 連鎖の概念の簡単さは大きく役に立ったものと思われる．Markov 連鎖を構成するには遷移行列と呼ばれる確率行列 \\(P\\) を一つ構築すれば良い，そしてその必要条件は簡単な式（詳細釣り合い条件） \\[\n\\pi P=\\pi\n\\] で表せる．\nMCMC が真に統計分野で再発見されたのはコンピュータが普及したのちである．画像処理と空間統計の分野で MCMC が採用され活発に研究され，その中で (Geman and Geman, 1984) により Gibbs サンプラーの名前がついた．\nさらに空間統計以外の統計分野にも広く普及したのは，統計学の一流誌で複数の例を取り上げて紹介した (Gelfand and Smith, 1990) からだと言われる．\n(Gelfand and Smith, 1990) 以降の MCMC の文献の数は大きく増える．中でも，応用分野での発展と手法的発展の２つに分けられる．モデリングを基調とする応用分野では，Gibbs サンプラーが今でも重要な位置を占めていると言える．\n\n\n1.2 汎用 MCMC 手法の発展\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n一方で，モデルに依らず，ブラックボックスで適用できる汎用手法の探索は，独自の発展を辿った．筆者の専門も専らこちらである．\n\n1.2.1 Langevin dynamics による MCMC とその理論解析\nまず (Grenander and Miller, 1994) のコメントにおいて，(Besag, 1994) が，分布 \\(\\pi\\) に収束する Langevin dynamics \\(\\{X_t\\}\\subset\\mathcal{L}(\\Omega;\\mathbb{R}^d)\\) \\[\ndX_t=\\nabla(\\log\\pi(X_t))\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t,\\qquad\\beta&gt;0,\n\\tag{1}\\] を提案分布に用いた Hastings 法を示唆し，これは現在でも統計・機械学習の双方で理論的に重要な位置を占めている．\n理論解析には主に２つのアプローチがある．１つはスケーリング解析であり，もう１つはスペクトル解析である．後者は筆者の専門ではないが，対数 Sobolev 不等式，等周不等式，対数凸分布，などがキーワードになる．\n前者では，空間の次元 \\(d\\to\\infty\\) の極限において，\\(\\{X_t\\}\\) の時間と空間についてのスケーリングであって，確定的な極限を持つスケールを探す．続いて，そのダイナミクスについて中心化し，Gauss 過程に収束するような中心極限定理のスケールを特定する．\n後者の中心極限定理のスケールにおける極限を調べ，その極限過程の平均自乗移動距離を比較することで，アルゴリズムの「速さ」を比較するというものである．これは，当該の過程からの出力を用いた Monte Carlo 推定量の漸近分散を比較する，という明確な意味を持つ．\n本アプローチは (Roberts et al., 1997) により創始され，(Roberts and Rosenthal, 2001) に Statistical Science 誌での解説記事がある．\n\n\n1.2.2 アルゴリズムとしての計算複雑性の解析\n以上，MCMC が定める確率過程に対する理論解析のアプローチを２つ紹介したが，アルゴリズムとしての解析や計算複雑性の議論はまた別である．上述の２つの過程に対する解析も，計算複雑性に対する含意が可能である．例えばスケーリング解析については (Roberts and Rosenthal, 2016) など．\nアルゴリズムに目を向けると，１つ厄介な点に気づく．そもそも Langevin 拡散過程 (1) をシミュレートするには離散化が必要であり，その際の誤差によって，離散化して得る Markov 連鎖の平衡分布は元々の \\(\\pi\\) からズレてしまう．統計分野ではこのズレを補正するために Hastings の棄却法の枠組みが要請される．すると元々の Langevin 拡散 (1) と結果として得る MCMC とは，収束特性が異なる場合がある (Roberts and Rosenthal, 1998)．\n補正なしのものを ULA (Unadjusted Langevin Algorithm)，ありのものを MALA (Metropolis-adjusted Langevin Algorithm) と呼ぶ．\nULA と MALA では当然計算複雑性が違う．MALA では各棄却法のステップで尤度比の評価が必要になり，データ数に応じて計算量が増える．一方で ULA を Monte Carlo 法に用いると漸近的に消えないバイアスを持つ．そこで，ULA のような特性を持つ手法を低精度手法，MALA を高精度手法と呼び，前者から初めて，途中でこの２つを切り替えることで，良いところどりをすることができる．この議論は (Altschuler and Chewi, 2023) から始まり，機械学習の分野で盛んである．"
  },
  {
    "objectID": "posts/2025/Life/Overview.html#pdmp",
    "href": "posts/2025/Life/Overview.html#pdmp",
    "title": "MCMC と PDMP の概観",
    "section": "2 PDMP",
    "text": "2 PDMP\n\n2.1 Hastings の枠組みからの離陸\n以上の議論は全て，(Hastings, 1970) の枠組みの下での MCMC 手法であった．提案 \\(Q\\) には大きな自由度があり，これを補正する形で所望の遷移核 \\(P\\) を得る，という構成法である．\n離散時間の Markov 連鎖を生成することに拘らなければ，この枠組みから逸脱した Markov 過程を容易に作り出すことができる．\nその主なアイデアは，所与のダイナミクスに対して，ジャンプによって制御するというものである．\n\n\n\nモンテカルロ法の発展．筆者の ポスター からの抜粋．\n\n\nPDMP は rejection-free MCMC などのキーワードの下で，当初は Event-Chain Monte Carlo の名前で，やはり統計物理分野で提案された (Bernard et al., 2009)．\n\n\n2.2 PDMP の発展\nレビュー的な文献には (Fearnhead et al., 2018) と (P. Vanetti, 2019) があるが，いずれも一昔前のものであり，「適用できるモデルが限られる手法である」というような言説は現在では違うと言って良い．\nここ５年の一大課題の１つには，PDMP の適用範囲の拡大，すなわち Poisson 剪定の自動化があったが，これは現在では複数の解決法を得ている．\n\n自動微分の利用による上界の自動構成 (Corbella et al., 2022), (Andral and Kamatani, 2024)\nHMC 様の splitting scheme による近似スキームと，MH の方法による修正 (Bertazzi et al., 2023), (Chevallier et al., 2025)"
  },
  {
    "objectID": "posts/2025/Life/Overview.html#文献紹介",
    "href": "posts/2025/Life/Overview.html#文献紹介",
    "title": "MCMC と PDMP の概観",
    "section": "3 文献紹介",
    "text": "3 文献紹介\n\n3.1 PDMP のレビュー\nPDMP ブームの早い段階から Statistical Science 誌に掲載された (Fearnhead et al., 2018) は第３節までの約 10 ページで，簡潔に PDMP の数学と性質，応用がまとまっている．特に 3.4 節にある数値実験では，２峰性を持つ事後分布に対して，\\(d=128\\) にて最適にチューニングした MALA の性能を BPS が大きく越す様子が示されている．\n\n\n\n(Fearnhead et al., 2018, p. 395)\n\n\n(Chapter 2 P. Vanetti, 2019) は，第１章で MCMC の基礎概念も概観していることもあり，約 35 ページでより詳しく扱っている．MCMC も同時に知りたい場合は勧めたいところだが，初学者が MCMC の多様な概念（Slice sampler や lifting など）をこれだけで理解できるとは少し思えない．\nしかし，2019 年時点までの発展をほとんど全て網羅しており，PDMP の研究を本格的に視野に入れる際には「抑えておけば間違いない」文献ではある．\n特に 2.1 節で PDMP を導入した後，2.2 節は生成作用素の扱いから始まっており，数理から入りたい場合には特に向いている．PDMP が特定の分布を不変にするための条件を証明している．一方で (Section 2 Fearnhead et al., 2018) では数式は理解を促進するために使われており，証明はない．初読の際にお勧めできる．\n\n\n3.2 (Fearnhead et al., 2018) へのコメント\nということで PDMP について最初に読む文献としては (Fearnhead et al., 2018) を勧めるが，2016 年に初稿が書き上がった 10 年弱前の文献であることもあり，ここでいくらかコメントをする．\nPDMP を連続時間 MCMC として捉え，SMC の連続時間バージョンと並列して説明している点は大変喜ばしいものであるが，現在の筆者はこの２つを全く分離して理解しており，SMC の方についてはコメントできない．\nPDP (Piecewise Deterministic Process) という用語は，このクラスの過程を最初に定義した (Davis, 1984) に従ったものであると思われるが，現在は（少なくともベイズ統計では）PDMP という略語が主流になっていると言える．3\n\n3.2.1 PDMP のシミュレーションについて\n(Section 2.1 Simulating a PDP Fearnhead et al., 2018) は PDMP アルゴリズムを理解する上で最も重要な節である．PDMP のシミュレーションでは非一様なレート \\(\\Lambda_Z(s)\\) を持つ Poisson 過程の到着時刻のシミュレーションがボトルネックになる．そのためサラッと\n\nBelow we will assume that our PDP has been chosen so that \\(\\Psi(·,·)\\) is known analytically and that the proposal distribution at events, \\(q(· | ·)\\), can be easily simulated from.\n\nと書かれているように，決定論的なフローは Hamiltonian flow などの数値積分が必要なものは，ほとんど考えられていない．数少ない例外は Hamiltonian flow を用いる (Paul Vanetti et al., 2018) と楕円軌道を用いる (Bierkens et al., 2020) である．「より複雑なフローを採用し，数値計算のコストを払うことは，より効率的な PDMP をもたらすか？」というようなリサーチクエスチョンはまだ決着していないと思われる．\n(Section 2.1 Simulating a PDP Fearnhead et al., 2018) では Poisson thinning だけが紹介されている．この方向の研究としては，現在は自動微分を用いて上界を自動的に構成する方法が確立されている (Corbella et al., 2022), (Andral and Kamatani, 2024)．\n以下他に２つの方向性を紹介するが，これらは Poisson thinning と違って誤差が入ることに注意する．（これを補正する Metropolis-Hastings 法またはその拡張も考えられている (Paul Vanetti et al., 2018), (Bertazzi et al., 2023), (Chevallier et al., 2025)）．\n数値計算で直接 \\(\\Lambda_Z(s)\\) の逆関数を計算し，到着時刻を近似する方法も (Pagani et al., 2024) で考察されている．\n他にも HMC では leapfrog 法として標準的になっている splitting scheme を用いることで近似計算をする方法 (Bertazzi et al., 2023) がある．この方法はコードを極めて簡単にする．\n\n\n3.2.2 PDMP の数理について\n(Section 2.2 Analyzing a PDP Fearnhead et al., 2018) では PDMP の数理的な取り扱いが紹介されている．この章は飛ばしても何の問題もない．しかし，生成作用素などのキーとなる概念に関して，直感的で分かりやすい解説だと思う．\nPDMP は Markov 過程であり，基本的には生成作用素 \\(L\\) の言葉で理解される．PDMP が特定の分布を平衡分布に持つことの証明も，この生成作用素が特定の性質を満たすことを示すことによって行われる．\nしかしその証明はとんでもなく難しい．そもそも (Davis, 1984) が生成作用素の形を導いているのはほとんど奇跡である．(Davis, 1984) では Appendix でマルチンゲール点過程の理論を展開しており，その証明は極めてオリジナルで圧巻としか言いようがない．\nさらに \\(L\\) のコアとして，コンパクトな関数からなるクラスが取れることを示すことがキーとなるステップである．具体的な PDMP の生成作用素 \\(L\\) のコアの特定は極めて困難である．この方向に関する既存文献は (Durmus et al., 2021) と (Holderrieth, 2021) の２つがあり，後者の方は極めて短いペーパーであり，証明の見通しも良い．(Holderrieth, 2021) の議論を引用しようと思うかもしれないが，残念ながらその補題 5.1 の証明は間違っている．我々には (Durmus et al., 2021) の膨大な理論が必要なのである．"
  },
  {
    "objectID": "posts/2025/Life/Overview.html#footnotes",
    "href": "posts/2025/Life/Overview.html#footnotes",
    "title": "MCMC と PDMP の概観",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n他に重要な文献には，Metropolis 法の変種である (Barker, 1965) の提案核が挙げられる．↩︎\n次に (Peskun, 1973) の理論研究が続く．↩︎\nOR 分野では一部 PDP を現在に使う文献もあるかもしれない．↩︎"
  },
  {
    "objectID": "posts/2025/Blog/BayesComp2.html",
    "href": "posts/2025/Blog/BayesComp2.html",
    "title": "BayesComp2025 ２日目",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/Blog/BayesComp2.html#robust-gp",
    "href": "posts/2025/Blog/BayesComp2.html#robust-gp",
    "title": "BayesComp2025 ２日目",
    "section": "1 Robust GP",
    "text": "1 Robust GP\n基本的に ML の人にとっての GP って共役でやるんですね．\nGeneralized Hierarchical Models が望まれる．"
  },
  {
    "objectID": "posts/2025/Blog/BayesComp2.html#gaussian-copula-process-in-the-last-layer",
    "href": "posts/2025/Blog/BayesComp2.html#gaussian-copula-process-in-the-last-layer",
    "title": "BayesComp2025 ２日目",
    "section": "2 Gaussian Copula Process in the Last Layer",
    "text": "2 Gaussian Copula Process in the Last Layer\nESN などの時系列 DNN を Bayes 様に fine-tuning する (”Bayesian Last Layers”)．MCMC を使う程度の軽い推論で SOTA を達成する．データはオーストラリアの電力マーケット． 使ったのは Gaussian copula process であり，これにより周辺分布の calibration を向上させることができるが，このステップが大きく予測性能も改善する．加えて補助情報を加えると大きく予測分布が変わる（裾の重さなど）．これを見る限り Copula in the last layer はアリかもしれないが，何が起こっているかは要検討．"
  },
  {
    "objectID": "posts/2025/Blog/BayesComp1.html",
    "href": "posts/2025/Blog/BayesComp1.html",
    "title": "BayesComp2025 １日目",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/Blog/BayesComp1.html#robust-gp",
    "href": "posts/2025/Blog/BayesComp1.html#robust-gp",
    "title": "BayesComp2025 １日目",
    "section": "1 Robust GP",
    "text": "1 Robust GP\n基本的に ML の人にとっての GP って共役でやるんですね．\nGeneralized Hierarchical Models が望まれる．"
  },
  {
    "objectID": "posts/2025/Blog/BayesComp1.html#gaussian-copula-process-in-the-last-layer",
    "href": "posts/2025/Blog/BayesComp1.html#gaussian-copula-process-in-the-last-layer",
    "title": "BayesComp2025 １日目",
    "section": "2 Gaussian Copula Process in the Last Layer",
    "text": "2 Gaussian Copula Process in the Last Layer\nESN などの時系列 DNN を Bayes 様に fine-tuning する (”Bayesian Last Layers”)．MCMC を使う程度の軽い推論で SOTA を達成する．データはオーストラリアの電力マーケット． 使ったのは Gaussian copula process であり，これにより周辺分布の calibration を向上させることができるが，このステップが大きく予測性能も改善する．加えて補助情報を加えると大きく予測分布が変わる（裾の重さなど）．これを見る限り Copula in the last layer はアリかもしれないが，何が起こっているかは要検討．"
  },
  {
    "objectID": "static/about.html#hello",
    "href": "static/about.html#hello",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "Hello!",
    "text": "Hello!\nHirofumi is a Ph.D. student supervised by Kengo Kamatani at the Institute of Statistical Mathematics (ISM), Tokyo, Japan.\nMy research aims to deepen our understanding of learning algorithms, especially Monte Carlo and optimization methods, through the lens of their (scaling / continuous-time) limit dynamics.\nFrom these perspectives, algorithms reveal their intrinsic properties, and my work seeks to provide a unified understanding of various algorithms under a common mathematical framework, i.e., flows on the space of probability measures."
  },
  {
    "objectID": "posts/2024/Life/UCL.html",
    "href": "posts/2024/Life/UCL.html",
    "title": "英国研究滞在記",
    "section": "",
    "text": "11/4 からロンドンに１ヶ月ほど滞在します。ロンドン（広くイギリス）は自分の中ではベイズ計算の中心地の１つと目してますので大変楽しみです。 https://t.co/gaGrnnLTRl— Hirofumi Shiba (@ano2math5) September 27, 2024  \n\n11月3日から12月3日までの１ヶ月間，総研大の研究派遣プログラムの援助を受けて，イギリス・ロンドンの UCL (University College London) の Alexandros Beskos 教授を訪問した．\nAlex は拡散過程の Monte Carlo シミュレーションや MCMC (Markov Chain Monte Carlo)，さらには SMC (Sequential Monte Carlo) と幅広い守備範囲を持つベイズ計算 (Bayesian Computation) の大家である．\nUCL はロンドン大学 (University of London) に所属するカレッジの１つであり，「功利主義」の哲学で有名な Jeremy Bentham によって，世界で初めての性別・宗教・出自・肌の色に依らず入学できる大学として 19 世紀前半に創立された．\n\nこのような歴史もあり，UCL の統計の博士課程には極めて多様な背景（国籍・人種・宗教など）を持った学生が揃っている．自分はそのことが最も気に入った．Alex の学生にはイギリス出身の Chris や日本出身の井口さんがいた．他に筆者と仲良くしてくれた学生は，カナダ，ギリシャ，マレーシア，中国出身と，誰ひとり被っていなかった．\n受入教員である Alex も元はギリシャの出身であり，滞在最後に一緒にご飯を食べたときには UCL で採用に至るまでの面接の苦労を話してくれた．\nこの多様性にみんな慣れていることもあってか，初のヨーロッパ体験に借りてきた猫状態だった自分も，滞在一週間も経たずに「馴染めた」「仲間にしてもらえた」と感じた．それにはイギリスのパブ文化の影響もあっただろう．\n金曜日の夜にパブに行く文化があると事前に聞いていたから，勇気を出して Ph.D. 部屋に残っていた人を誘ってみた．５時だったが部屋にはほとんど人はいなかった．UCL はロンドンの中心にあり，その近くは大変家賃が高いため，多くの生徒は郊外に住む（Oxford から通っている学生もいる）．そのため５時には大抵の学生は帰ろうとし始めるのだ．（そして寮は博士１年生しか利用できない．）\n中でも Teresa は帰って作り置きの夕飯を食べる予定だったところを変更してまで，僕の誘いを快諾してくれた．結局メンバーは５人にまで増え，まずはピザを食べにいき，その後パブで一杯飲んだ．\nTeresa は台湾の出身であるが，カナダの McGill 大学で数学を専攻してから UCL の統計博士課程に進み，今は同時に医局で統計スタッフとして働いてもいる．自分が「イギリスどころかヨーロッパが初めてで，パブに行ってみたいが勇気が出ない」と素直に伝えると，当時の自分の境遇と重なったこともあってか，大変歓迎して友人も誘い合わせてパブへ連れて行ってくれたのである．\n日本の飲み屋は食事とアルコールを同時に提供するが，イギリスではその２つの機能はレストランとパブが別々に担っているのだと自分は理解した．するとパブが必然的に「二軒目」になっており，その頃にはすっかり仲間だという感じになって腹をわった話ができる．大変不思議な感覚である．"
  },
  {
    "objectID": "posts/2024/Life/UCL.html#はじまり",
    "href": "posts/2024/Life/UCL.html#はじまり",
    "title": "英国研究滞在記",
    "section": "",
    "text": "11/4 からロンドンに１ヶ月ほど滞在します。ロンドン（広くイギリス）は自分の中ではベイズ計算の中心地の１つと目してますので大変楽しみです。 https://t.co/gaGrnnLTRl— Hirofumi Shiba (@ano2math5) September 27, 2024  \n\n11月3日から12月3日までの１ヶ月間，総研大の研究派遣プログラムの援助を受けて，イギリス・ロンドンの UCL (University College London) の Alexandros Beskos 教授を訪問した．\nAlex は拡散過程の Monte Carlo シミュレーションや MCMC (Markov Chain Monte Carlo)，さらには SMC (Sequential Monte Carlo) と幅広い守備範囲を持つベイズ計算 (Bayesian Computation) の大家である．\nUCL はロンドン大学 (University of London) に所属するカレッジの１つであり，「功利主義」の哲学で有名な Jeremy Bentham によって，世界で初めての性別・宗教・出自・肌の色に依らず入学できる大学として 19 世紀前半に創立された．\n\nこのような歴史もあり，UCL の統計の博士課程には極めて多様な背景（国籍・人種・宗教など）を持った学生が揃っている．自分はそのことが最も気に入った．Alex の学生にはイギリス出身の Chris や日本出身の井口さんがいた．他に筆者と仲良くしてくれた学生は，カナダ，ギリシャ，マレーシア，中国出身と，誰ひとり被っていなかった．\n受入教員である Alex も元はギリシャの出身であり，滞在最後に一緒にご飯を食べたときには UCL で採用に至るまでの面接の苦労を話してくれた．\nこの多様性にみんな慣れていることもあってか，初のヨーロッパ体験に借りてきた猫状態だった自分も，滞在一週間も経たずに「馴染めた」「仲間にしてもらえた」と感じた．それにはイギリスのパブ文化の影響もあっただろう．\n金曜日の夜にパブに行く文化があると事前に聞いていたから，勇気を出して Ph.D. 部屋に残っていた人を誘ってみた．５時だったが部屋にはほとんど人はいなかった．UCL はロンドンの中心にあり，その近くは大変家賃が高いため，多くの生徒は郊外に住む（Oxford から通っている学生もいる）．そのため５時には大抵の学生は帰ろうとし始めるのだ．（そして寮は博士１年生しか利用できない．）\n中でも Teresa は帰って作り置きの夕飯を食べる予定だったところを変更してまで，僕の誘いを快諾してくれた．結局メンバーは５人にまで増え，まずはピザを食べにいき，その後パブで一杯飲んだ．\nTeresa は台湾の出身であるが，カナダの McGill 大学で数学を専攻してから UCL の統計博士課程に進み，今は同時に医局で統計スタッフとして働いてもいる．自分が「イギリスどころかヨーロッパが初めてで，パブに行ってみたいが勇気が出ない」と素直に伝えると，当時の自分の境遇と重なったこともあってか，大変歓迎して友人も誘い合わせてパブへ連れて行ってくれたのである．\n日本の飲み屋は食事とアルコールを同時に提供するが，イギリスではその２つの機能はレストランとパブが別々に担っているのだと自分は理解した．するとパブが必然的に「二軒目」になっており，その頃にはすっかり仲間だという感じになって腹をわった話ができる．大変不思議な感覚である．"
  },
  {
    "objectID": "posts/2024/Life/UCL.html#研究環境",
    "href": "posts/2024/Life/UCL.html#研究環境",
    "title": "英国研究滞在記",
    "section": "2 研究環境",
    "text": "2 研究環境\n正直学生のレベル感は ISM とほとんど差を感じない．だが「ロンドン界隈」全体としては，総じて環境は全く違うと言わざるを得ないと感じた．\nかの Thomas Bayes もロンドンがうんだように，イギリスでは現在もベイズの豊かな土壌があり，一週間のうちに必ずどこかではベイズ計算に関連するセミナーが開催されている．そして大変多くの人が集まるのである．この環境だけは世界の他のどこにもないだろう．\nさらに滞在の最後に当たる 11/25（月）から 11/29（金）までの５日間，INI (Isaac Newton Institute) にてベイズ計算に特化したワークショップ Monte Carlo Sampling: Beyond the Diffusive Regime が行われた．\n\n  I’m here! https://t.co/N4YTmmJFCo— Hirofumi Shiba (@ano2math5) November 25, 2024 \n\nこの INI というのが大変興味深い組織である．1992 年に Cambridge 大学の２つのカレッジ St. Jones と Trinity の出資により設立された数理に特化した研究施設であるが，ここにパーマネントに所属する研究者というのは原則存在せず，基本は６ヶ月ごとに入れ替わる．しかしこの６ヶ月間というのは，テーマを決めてそれに関連する研究者を世界中から呼び込み，INI という一箇所に集めての「６ヶ月」なのである．\n私が参加したこの５日間のワークショップも，Monte Carlo 法に関する研究プログラム Stochastic systems for anomalous diffusion の一環として開催されている．このワークショップの参加者も，希望すれば一定期間オフィスをもらい，ワークショップの前後に渡って滞在することができる．\n\n\n\nINI 最上階からの写真．写真左手前が Alex である．\n\n\nINI にはありとあらゆるところに黒板が用意されている．トイレの中にもある．そのことがこんなにも違いを生むとは思わなかった．参加者は年齢に依らず，休み時間になると活発に議論をした．\n\n  Even in the bathroom, we have blackboard here.Really good environment. pic.twitter.com/61nZzXMSaw— Hirofumi Shiba (@ano2math5) November 28, 2024"
  },
  {
    "objectID": "posts/2024/Life/UCL.html#monte-carlo-ワークショップ",
    "href": "posts/2024/Life/UCL.html#monte-carlo-ワークショップ",
    "title": "英国研究滞在記",
    "section": "3 Monte Carlo ワークショップ",
    "text": "3 Monte Carlo ワークショップ\nこのワークショップの最大の特徴は統計と物理の２分野の邂逅であった．Monte Carlo 法は物理学に端を発する計算技術であるが，今では統計学にも大きな研究コミュニティがあり，「ベイズ計算」はその一つである．統計と物理の研究者が語彙をすり合わせながら互いに歩み寄る極めて貴重な機会だったと言えるだろう．\n\n  Mpemba 効果（より熱い水の方が冷たい水より先に凍ることがある）への理解を深めれば，より速く収束する MCMC が作れるかもしれない……？Check out \"Anomalous thermal relaxation on dense graphs with Metropolis-Hastings dynamics\" by Marija Vuceljahttps://t.co/c9N7Eftmqx— Hirofumi Shiba (@ano2math5) November 26, 2024  \n\n最初の講演は “beyond the diffusive regime” という副題を象徴する統計物理学者である Werner Krauth 氏の発表から始まった．公式 YouTube チャンネルに アーカイブ が残っている．Werner は ECMC (Event Chain Monte Carlo) という新たな Monte Carlo 法を開発することで，水の融解の全粒子シミュレーションを世界で初めて成功させた．\n\ndiffusive regime とは熱平衡にある系が示す特徴である．したがって多峰性分布からも効率的にサンプリングができるような次世代の Monte Carlo 法をデザインするためには，diffusive regime から脱すること (= beyond the diffusive regime) がいくらか必要条件になる，ということから始まった．\n続いて Werner は diffusive regime を超越する Monte Carlo 法に，「局所性」を破る HMC (Hamiltonian / Hybrid Monte Carlo) と「対称性」を破る ECMC / PDMP (Piecewise Deterministic Markov Process) の２つがあるとして，現状のベイズ計算を概観した．（ここでも二分野の語彙の違いが出ている，統計が PDMP と呼ぶものを物理は ECMC と呼ぶ．）\nこのプレゼンはワークショップ全体のアジェンダを設定したと言えるものであった．それだけでなく，その後ワークショップの全体を通して，Werner は最も影響力を持つ存在であった．何より，５日間を通して最前列で最も活発に質問するだけでなく，筆者のような修士の学生とも議論する時間を惜しまなかった．「英語では教授は Sir をつけて呼ぶべきなのだろうか」とか考えてひたすら恐れ入っている自分に “I’m also a learner here.” と対等に議論しようとふっかけてくれたことが嬉しかった．\nWerner は７時にお腹が空いてどうしようもなくなるまで INI に残っていた．言語の壁を超えて伝わってくる人格があった．どこか現代の Feynman に出会ったような感覚であった．僕はこの仕事がますます好きになるばかりだ．\nそこで翌日も INI に残っていたらどんな人に出会うかとワクワクしていたが，気づけば 10 時過ぎまでカードゲームをしていた．Andi Q. Wang 氏は MCMC の収束を関数不等式を用いて議論する研究をしている．彼はどの学会にもスウェーデンで買ったカードゲームを持参するようで，今回は Blood on Clocktower という人狼をさらに複雑にしたようなゲームを８人でプレイした．スウェーデンでは毎年１つボードゲームが無料でもらえるらしい．\n研究の話じゃなくなると本当に英語は難しい．オランダ・ギリシャ・スロベニア・インドと，英語が母語じゃない研究者もたくさんプレイしていたはずであるが，彼らはとんでもなく英語がうまい．全く気後れしている様子がなかった．やはりインド・ヨーロッパ語族から外れた母語を持つと，ハンディキャップが大きいのだろうかなどと考えざるを得なかった．正直，ゲームは楽しいというより苦しかった．\n\n  Although some of my colleagues (might) praise me for my Englisg skills, I am useless itself when playing cardgame with people here.— Hirofumi Shiba (@ano2math5) November 27, 2024"
  },
  {
    "objectID": "posts/2024/Life/UCL.html#共同研究",
    "href": "posts/2024/Life/UCL.html#共同研究",
    "title": "英国研究滞在記",
    "section": "4 共同研究",
    "text": "4 共同研究\n今回のロンドン滞在の話は，実は最初は断るつもりであった．英語以前にも，そもそも自分は修士２年でまだ論文もなく，「滞在先の先生に何も提供できないのではないか」という懸念が後ろ髪を引いた．しかし，副指導教員の先生や家族に「失敗しても失うものはない」と大きく背中を押されたこともあり，自分の運命を受け入れることにした．\n修士２年で夏休みを終えたばかりの自分はというと，いまいち指導教員の先生がやっているような MCMC や SMC の理論解析には踏み切れずにいた．MCMC や PDMP のアルゴリズムは十分理解したつもりだったし，使う数学にも慣れてきたが，いまいち心がついてこなかった．\nそのような中で９月に奇縁でソウル大学の政治学者（かつベイジアン！）である Jong Hee Park （박종희）氏に PDMP について発表する機会があった．全く新しい MCMC であるからしばらくは戸惑っていたようであるが，話すうちに「これは使える！」という反応をもらった．\nそのアイデアは要するに「モデル選択に使えるのではないか？」ということであった．すごく意外だった．\nPDMP とは，従来の Metropolis-Hastings 法を非可逆化したアルゴリズムの，さらに提案分布の分散を小さくしていく極限を取ったものと考えることができる．その結果，割り切った (ballistic) ダイナミクスとより速いエルゴード収束レートをもつ．これにより Monte Carlo 分散を従来の MCMC より小さくできることが期待されていた．\n自分はこのことを数学的に証明できたかもしれないが，だからといって PDMP が何の役に立つかを全く知らなかったのである．\nそこで出発までの１ヶ月で PDMP サンプリングを実行するための パッケージ を作ってみた．ギリギリ動くものができた段階でロンドンに向かったのである．\n正直，本当の興味はどちらかというと SMC と最適輸送の関係にあった．こちらの話も，なるほど interesting だと共感はしてもらえたが，アイデアが未成熟で具体的に次のステップに繋げることができなかった．しかしこの PDMP パッケージの話が出ると新たな方向が見えた．Alex 自身に PDMP に関する著作はないが，昨今 Graphical Model のベイズモデル選択には大きな興味があった．\nそこで，パッケージの応用先の例として筆者がそれとなく挙げた巨大な階層モデル（理想点モデル）を PDMP で推定することと，そのモデル選択のための新種のトリックに大きく興味を持ったようだった．\nINI の３日目（中間日）は午後には講演はなく，参加者は思い思いの場所で議論に耽っていた．Alex は自分のアイデアに大変興味を持ってくれて，PDMP の大家である Samuel Livingstone とその学生である Luke も誘い合わせてミーティングを企画してくれた．Luke は，殊に PDMP の具体的な統計モデルへの応用については現時点で世界で最も詳しい人間かもしれない．\n僕が黒板に文字を書くなり，奥から Samuel が羽衣チョークを持ち出してきたときには笑った．Samuel は６ヶ月 INI に滞在しているメンバーであり，自室に羽衣チョークのストックを作っていたのである（UCL に黒板はない）．他の INI 滞在メンバーである，これまた PDMP の専門家である Georgios Vasdekis に至っては，６ヶ月の会期が終わると使い所がないからと１本プレゼントしてくれた．\n\n  Hagoromo （羽衣） chalk presented by Georgios pic.twitter.com/3x9POrgti3— Hirofumi Shiba (@ano2math5) November 28, 2024"
  },
  {
    "objectID": "posts/2024/Life/UCL.html#終わりに",
    "href": "posts/2024/Life/UCL.html#終わりに",
    "title": "英国研究滞在記",
    "section": "5 終わりに",
    "text": "5 終わりに\n３週間という長い期間 UCL に滞在でき，他の Ph.D. 学生と交流できたことと，今回の極めて自分の専門に焦点のあった INI のワークショップに参加できた意義は，計り知れないほど大きい．\n自分は海を隔てれば同志がいることを知った．また INI ワークショップの講演者は，まるで自分が読んできた論文の著者リストであった．\n論文を読むだけでなく，それについて他人とディスカッションすることは，全く別の行為であることを学んだ．ディスカッションを通じて，既存のリテラチャーに +1 をするような考え方だけでなく，遠く離れた２つのアイデアを結びつける「核」というものが特定できる気がする．\n\n実際，自分の原稿であろうと，いざ黒板の前に立って思い出せることはごく一部なのである．アイデアは論文ではなく人に宿るのである……．\n\n  いろんな人と将来コラボしたいから広い知見と深い数学が欲しい。だが自分だけのオリジナルな結果も博士で欲しい。二つのバランスをずっと考えている。— Hirofumi Shiba (@ano2math5) November 27, 2024"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html",
    "href": "posts/2025/Blog/NUS.html",
    "title": "シンガポール研究滞在記",
    "section": "",
    "text": "６月の１日から 30 日までの１ヶ月間，総研大の研究派遣プログラムの援助を受けて，シンガポール国立大学の Alexandre Thiéry 准教授を訪問した．\n昨年度のロンドン UCL の訪問 に続き，人生２度目の研究滞在である．その際は Alexandros Beskos 教授を訪問し，２度目の Alex 訪問でもある．\nいずれの Alex も逐次モンテカルロ法 (SMC)，マルコフ連鎖モンテカルロ法 (MCMC) 双方で顕著な業績をあげている，代表的なベイズ計算の研究者である．２人の間には共著も多く，いずれもすでに大変な大物であるが，年齢的にはまだまだ中堅というべきである．\nシンガポールの気候はロンドンの対極にある．まず熱くて日差しが強い．緯度１度であることを思えば不思議ではない．\nさらに蒸し暑さで悪名高い東京の夏よりも湿度が高い．だが不思議と不快感は少ない．駅に着くまでに汗ダラダラになった，という経験は１度もしていない．おそらく，町中に日陰が多いことと，アスファルトが少なく，代わりに土と街路樹（日本のような人工的な街路樹ではなくて本当に生えている）が多いからだろう．\n天気予報を見ると１週間まるまる 100% 雨の予報である週もあったが，かといってほとんど雨は降らない．どういうことかというと，当日中に雨が １度でも 降る確率は 100% であるが，それはバケツをひっくり返したような雨で 10 分も続かないのが常である．シンガポールは屋根が多いから，近場で雨宿りしてやり過ごせば，傘を持たずとも１週間過ごせる．\n\nしかしロンドンと共通するのは，人々の多様性である．ロンドンとは違いアジア人が多数を占めるとはいえ，マレー系，インド系，中華系に３分され，言語もこの３つを結構似た頻度で聞く．\nこの多様性が，僕がシンガポールを，そしてロンドンを好きな理由である．"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#シンガポール生活のはじまり",
    "href": "posts/2025/Blog/NUS.html#シンガポール生活のはじまり",
    "title": "シンガポール研究滞在記",
    "section": "",
    "text": "６月の１日から 30 日までの１ヶ月間，総研大の研究派遣プログラムの援助を受けて，シンガポール国立大学の Alexandre Thiéry 准教授を訪問した．\n昨年度のロンドン UCL の訪問 に続き，人生２度目の研究滞在である．その際は Alexandros Beskos 教授を訪問し，２度目の Alex 訪問でもある．\nいずれの Alex も逐次モンテカルロ法 (SMC)，マルコフ連鎖モンテカルロ法 (MCMC) 双方で顕著な業績をあげている，代表的なベイズ計算の研究者である．２人の間には共著も多く，いずれもすでに大変な大物であるが，年齢的にはまだまだ中堅というべきである．\nシンガポールの気候はロンドンの対極にある．まず熱くて日差しが強い．緯度１度であることを思えば不思議ではない．\nさらに蒸し暑さで悪名高い東京の夏よりも湿度が高い．だが不思議と不快感は少ない．駅に着くまでに汗ダラダラになった，という経験は１度もしていない．おそらく，町中に日陰が多いことと，アスファルトが少なく，代わりに土と街路樹（日本のような人工的な街路樹ではなくて本当に生えている）が多いからだろう．\n天気予報を見ると１週間まるまる 100% 雨の予報である週もあったが，かといってほとんど雨は降らない．どういうことかというと，当日中に雨が １度でも 降る確率は 100% であるが，それはバケツをひっくり返したような雨で 10 分も続かないのが常である．シンガポールは屋根が多いから，近場で雨宿りしてやり過ごせば，傘を持たずとも１週間過ごせる．\n\nしかしロンドンと共通するのは，人々の多様性である．ロンドンとは違いアジア人が多数を占めるとはいえ，マレー系，インド系，中華系に３分され，言語もこの３つを結構似た頻度で聞く．\nこの多様性が，僕がシンガポールを，そしてロンドンを好きな理由である．"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#俺の見てきたシンガポール",
    "href": "posts/2025/Blog/NUS.html#俺の見てきたシンガポール",
    "title": "シンガポール研究滞在記",
    "section": "5 俺の見てきたシンガポール",
    "text": "5 俺の見てきたシンガポール\n\nさらに個人的な経験を書いてしまおうと思う．\n基本的に東アジア人の顔をして中華系の料理店に入ると，最初から中国語（標準語）で接客される．コスト的には納得である．牛肉麺とかを beef noodle とか言って注文する気も起きないから，まあ理にかなっているかと思っていた．\nしかしある日タクシーに乗って中国語を話したところ，「君ら中国人はマナーが悪い」と説教を受けた．シンガポールは配車アプリが発達しており，運転手と全く会話せずに目的地に到着することも可能であるが，本人確認の都合もあり，確認したいことも多い．それにも拘らず何をいっても一言も喋ろうとしないおそろしい乗客はたいてい中華系なのだという．\nあまりにお門違いだと思ったので事情を説明したところ，すぐに通じた．あろうことか，運転手さんは東京に 30 年以上も住んでいたことがあり，自分が日本語を喋ったらすぐにわかってくれた．1 なお，この運転手さんは，マレー系のシンガポール人で，生粋のシンガポール生まれシンガポール育ちであると言っていた．\nシンガポールは極めて犯罪率が低く，麻薬使用率も極めて低い．日本で生きていると正直それがデフォルトだと感じてしまうところがあるが，ここまで多様な社会で軋轢がないはずがない．\n私は，これは資本主義のマジックだと見る．少なくとも今のシンガポールは，近隣国に比べて圧倒的な富がある．その大きな要因として，英語を公用語に残してあることがある．"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#研究環境",
    "href": "posts/2025/Blog/NUS.html#研究環境",
    "title": "シンガポール研究滞在記",
    "section": "3 研究環境",
    "text": "3 研究環境\nシンガポールでは初日から Thiéry 氏がポスドクの林氏と，David Nott のポスドクである Lucas を紹介してくれた．\n私には机も用意されており，同室にはイタリアから滞在中の Piergiacomo がいた．\n出張前には想像もしなかったことだが，私は彼らと，Linda Tan のポスドクである Abhinek も入れて，その後は毎日お昼と４時のティータイムを共にすることになる．\n大学に行く機会はあまりないかもしれないと思い，東部のエリアにホテルをとってしまったのだが，彼らとの時間が好きすぎて平日は結局毎日片道１時間かけて通ってしまった．\nこんなことならば大学の近くにホテルを借りればよかったと後悔したものであった．\n林は訪問時時系列の予測を研究しており，これに SSM-GP と呼ばれる状態空間モデルと Gauss 過程の考え方を両方取り入れた手法を用いていた．\nその新規性としては，さらに拡散モデルによる非線型な遷移を可能にしていることにある．\n月末にあった BayesComp2025 というベイズ計算最大の国際学会でポスター発表をすることに向けて，最後のシミュレーション欄を詰めていたのだがなぜかうまくいかない点があった．\nその点について Thiéry とミーティングをしていたところに，私も同居させてもらい，その日のうちに解決した．\n特に粒子フィルターのシミュレーションにおいては，これまた隣に世界的な粒子フィルターの研究者である Hai-Dang も居たにも拘らず，自分が観測ノイズが小さすぎることに問題があることを見抜けた．\n結局 (Del Moral and Murray, 2015) に関連する現象であったが，自分の友人を助けたいという気持ちが実ったようで嬉しかった．"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#bayescomp2025",
    "href": "posts/2025/Blog/NUS.html#bayescomp2025",
    "title": "シンガポール研究滞在記",
    "section": "4 BayesComp2025",
    "text": "4 BayesComp2025\nBayesComp は２年に１度開催される，計算に特化した BayesComp-ISBA 主催のベイズ特化の国際学会である．\nこの分野では最大規模であり，今回もサテライト２日と本会議３日を併せて５日間開催された．\nこの会議に出席する前に，自分は出席予定のセッションで触れられる論文のほとんど全てに目を通してから出席した．\nお陰で聞きたいことをはっきり質問できた機会も多い．さらに何より，インターバル時間に研究者の本音が聞くことによって，自分の知識が有機的に繋がり，２倍にも３倍にもなるのを感じた．\nこの会議では，自分は残念ながら研究の進捗が間に合わず，ポスター発表はできなかったが，多くの友人に「見たかった」と言ってもらった．次こそはポスターでも口頭でもぜひ発表したいものである．\n最も印象に残ったのは Emti による Bayesian Learning Rule に関する講演であった．ベイズ推論の情報理論的な特徴付け (Zellner, 1988) は自分にとって大変興味深いものであり，“Bring the science back to ML” の標語は discussion section に参加したみんなの脳裏に残ったであろう．"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#共同研究",
    "href": "posts/2025/Blog/NUS.html#共同研究",
    "title": "シンガポール研究滞在記",
    "section": "2 共同研究",
    "text": "2 共同研究\nシンガポールでは週１で滞在先の Thiéry とのミーティングを行った．初回から PDMP の離散化と既存の MCMC との比較という PDMP 研究の一大課題が共有されたため，その後のミーティングも活発にディスカッションが進んだ．\nだかそれだけということもなく，多様体上の MCMC 手法 (Au et al., 2023) や，高次元における粒子フィルター (Finke and Thiery, 2023) にも詳しいため，多くのことを学ぶことができた．\n自分が追っている研究の最新論文 (Crucinio and Pathiraja, 2025) がたまたま出たので，これについて Slack 上で紹介したところ，Thiéry も読んでくれてディスカッションまでしてくれた．\n最終日には Hamiltonian Monte Carlo の高次元漸近解析の方向性をまとめ，ドラフトにまとめることまで出来た．帰国後に継続的に取り組む予定をしている．\nさらに高次元における PDMP とそのベイズ逆問題への応用についても，その研究の後にやる見通しがついている．"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#おわりに",
    "href": "posts/2025/Blog/NUS.html#おわりに",
    "title": "シンガポール研究滞在記",
    "section": "6 おわりに",
    "text": "6 おわりに"
  },
  {
    "objectID": "posts/2025/Blog/NUS.html#footnotes",
    "href": "posts/2025/Blog/NUS.html#footnotes",
    "title": "シンガポール研究滞在記",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nここで気づくだろうが，僕の中国語は，聞き手がネイティブじゃない限り，僕がネイティブじゃないことは見抜けない完成度がある．↩︎"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#problem-setting-bayesian-generative-modeling",
    "href": "posts/2025/Slides/DDD_Slides.html#problem-setting-bayesian-generative-modeling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.1 Problem Setting: Bayesian / Generative Modeling",
    "text": "1.1 Problem Setting: Bayesian / Generative Modeling"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#two-popular-solutions",
    "href": "posts/2025/Slides/DDD_Slides.html#two-popular-solutions",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.2 Two Popular Solutions",
    "text": "1.2 Two Popular Solutions\nProblem: compute the posterior distribution: \np(\\textcolor{#E95420}{z}|\\{x_i\\}_{i=1}^n)\\propto p(\\textcolor{#E95420}{z})\\prod_{i=1}^n p(x_i|\\textcolor{#E95420}{z})=\\text{prior}\\times\\prod_{i=1}^n\\text{model likelihood of }x_i\n\n\n\n\n\n\n\n\n\n\n\nSampling-based Methods\nOptimization-based Methods\n\n\n\n\nPurpose\nGet a sample\nGet an approximation\n\n\nScalable?\nNo (Yet)\nYes\n\n\nExact?\nYes\nNo\n\n\nE.g.\nMonte Carlo\nDiffusion Models\n\n\nMainly used\nin Bayesian statistics\nin Machine Learning\n\n\n\n\n\n\nThis talk is about an optimization-based Methods"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#problem-bayesian-generative-modeling",
    "href": "posts/2025/Slides/DDD_Slides.html#problem-bayesian-generative-modeling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.1 Problem: Bayesian / Generative Modeling",
    "text": "1.1 Problem: Bayesian / Generative Modeling"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#section",
    "href": "posts/2025/Slides/DDD_Slides.html#section",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.3 ",
    "text": "1.3"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#markov-chain-monte-carlo",
    "href": "posts/2025/Slides/DDD_Slides.html#markov-chain-monte-carlo",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.3 Markov Chain Monte Carlo",
    "text": "1.3 Markov Chain Monte Carlo\n\n\n\n\n\n\n\n\nProperty of Langevin Diffusion\n\n\n\nd\\textcolor{#2780e3}{X_t}=-\\nabla\\log p(\\textcolor{#2780e3}{X_t}|\\{x_i\\}_{i=1}^n)\\,dt+dB_t\n\nconverges to p(\\textcolor{#E95420}{z}|\\{x_i\\}_{i=1}^n) as t\\to\\infty.\n\n\n\n\nThis approach is feasible because …\n\n\\text{score function}\\quad\\nabla\\log p(\\textcolor{#E95420}{z}|\\{x_i\\}_{i=1}^n)\n\nis evaluatable."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#piecewise-deterministic-monte-carlo",
    "href": "posts/2025/Slides/DDD_Slides.html#piecewise-deterministic-monte-carlo",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.4 Piecewise Deterministic Monte Carlo",
    "text": "1.4 Piecewise Deterministic Monte Carlo\n\n\n\n\n\nBetter convergence(Andrieu and Livingstone, 2021)\nBetter scalability(Bierkens et al., 2019)\nNumerical stability(Chevallier et al., 2025)\n\nAvailable in our package PDMPFlux.jl\n\n] add PDMPFlux"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder",
    "href": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.6 Variational Auto-Encoder",
    "text": "1.6 Variational Auto-Encoder\nIn generative modeling, we also have to learn p\\in\\{p_\\theta\\}_{\\theta\\in\\R^e}\n\nJointly trained to minimize the loss function \n\\mathcal{L}(\\theta,\\phi):=\\operatorname{KL}\\bigg(q_\\phi(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_\\theta(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})\\bigg)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-kingma-welling2014",
    "href": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-kingma-welling2014",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.5 Variational Auto-Encoder (Kingma and Welling, 2014)",
    "text": "1.5 Variational Auto-Encoder (Kingma and Welling, 2014)"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#variational-inference",
    "href": "posts/2025/Slides/DDD_Slides.html#variational-inference",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.5 Variational Inference",
    "text": "1.5 Variational Inference\n\n\\text{Posterior distribution:}\\qquad p(\\textcolor{#E95420}{z}|\\boldsymbol{x})\\propto p(\\textcolor{#E95420}{z})\\prod_{i=1}^n p(x_i|\\textcolor{#E95420}{z})\n is searched in a variational formulation via KL divergence: \np(\\textcolor{#E95420}{z}|\\boldsymbol{x})=\\argmin_{q\\in\\mathcal{P}(\\textcolor{#E95420}{\\mathcal{Z}})}\\operatorname{KL}\\bigg(q(\\textcolor{#E95420}{z}),p(\\textcolor{#E95420}{z}|\\boldsymbol{x})\\bigg).\n\n\n\n\n\nScalable Solution to VI\n\n\n\nConstrain the problem on q\\in\\{q_{\\textcolor{#E95420}{\\phi}}\\}_{\\textcolor{#E95420}{\\phi}\\in\\R^d},\nSolve by (stochastic) optimization, using the gradient of \n\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}),p(\\textcolor{#E95420}{z}|\\boldsymbol{x})\\bigg)=\\operatorname{E}_{\\textcolor{#E95420}{\\phi}}[\\log q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{Z})]\n-\\operatorname{E}_{\\textcolor{#E95420}{\\phi}}[\\log p(\\textcolor{#E95420}{Z},\\boldsymbol{x})]+\\text{const.}"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-1",
    "href": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.6 Variational Auto-Encoder",
    "text": "1.6 Variational Auto-Encoder\n(Kingma and Welling, 2014) found that the loss function \n\\begin{align*}\n\\mathcal{L}(\\theta,\\phi)&=\\operatorname{KL}\\bigg(q_\\phi(\\textcolor{#E95420}{z}|x),p_\\theta(\\textcolor{#E95420}{z}|x)\\bigg)\\\\\n&=\\operatorname{E}_{\\phi,x}[\\log q_\\phi(\\textcolor{#E95420}{Z}|x)]\n-\\operatorname{E}_{\\phi,x}[\\log p_\\theta(\\textcolor{#E95420}{Z},x)]+\\log p_\\theta(x)\\\\\n&=\\operatorname{KL}\\bigg(q_\\phi(\\textcolor{#E95420}{z}|x),p_\\theta(\\textcolor{#E95420}{z})\\bigg)-\\operatorname{E}_{\\phi,x}[\\log p_\\theta(x|\\textcolor{#E95420}{Z})]+\\log p_\\theta(x)\n\\end{align*}\n still lends itself to stochastic optimization.\nOnce learned, we are able to sample from \np_{\\theta^*}(\\textcolor{#2780e3}{x})=\\int_{\\mathcal{Z}}p_{\\theta^*}(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{z})p_{\\theta^*}(\\textcolor{#E95420}{z})\\,d\\textcolor{#E95420}{z}\n\n\n\nNote that now q_\\phi depends on x as well."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#denoising-diffusion-models-ddm",
    "href": "posts/2025/Slides/DDD_Slides.html#denoising-diffusion-models-ddm",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.7 Denoising Diffusion Models (DDM)",
    "text": "1.7 Denoising Diffusion Models (DDM)\nConcentrating on learning p_{\\textcolor{#2780e3}{\\theta}}, we fix \nq_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=q(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=q^{t_1}(\\textcolor{#E95420}{z_1}|\\textcolor{#2780e3}{x})\\prod_{i=1}^T q^{t_{i+1}-t_i}(\\textcolor{#E95420}{z_{i+1}}|\\textcolor{#E95420}{z_{i}}),\n as a path measure of a Langevin diffusion on \\textcolor{#E95420}{\\mathcal{Z}}=(\\R^d)^{T+1}.\n\n\n\nA common choice is an OU process: q^t(z|x)=\\operatorname{N}(z;x,t)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-vae",
    "href": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-vae",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.6 Variational Auto-Encoder (VAE)",
    "text": "1.6 Variational Auto-Encoder (VAE)\nIn generative modeling, we also have to learn p\\in\\{p_{\\textcolor{#2780e3}{\\theta}}\\}_{\\textcolor{#2780e3}{\\theta}\\in\\R^e}\n\nJointly trained to minimize the KL divergence \n\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})\\bigg)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-vae-1",
    "href": "posts/2025/Slides/DDD_Slides.html#variational-auto-encoder-vae-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.6 Variational Auto-Encoder (VAE)",
    "text": "1.6 Variational Auto-Encoder (VAE)\n(Kingma and Welling, 2014) found that a part of the KL divergence \n\\begin{align*}\n&\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})\\bigg)\\\\\n&\\qquad=\\operatorname{E}_{\\textcolor{#E95420}{\\phi},\\textcolor{#2780e3}{x}}[\\log q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{Z}|\\textcolor{#2780e3}{x})]\n-\\operatorname{E}_{\\textcolor{#E95420}{\\phi},\\textcolor{#2780e3}{x}}[\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z},\\textcolor{#2780e3}{x})]+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x})\\\\\n&\\qquad=\\underbrace{\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z})\\bigg)-\\operatorname{E}_{\\textcolor{#E95420}{\\phi},\\textcolor{#2780e3}{x}}[\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{Z})]}_{=:-\\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta},\\textcolor{#E95420}{\\phi})\\text{ : we only optimize this part}}+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x})\n\\end{align*}\n still lends itself to stochastic optimization.\nOnce \\textcolor{#2780e3}{\\theta^*} is learned, we are able to sample from \np_{\\textcolor{#2780e3}{\\theta^*}}(\\textcolor{#2780e3}{x})=\\int_{\\textcolor{#E95420}{\\mathcal{Z}}}p_{\\textcolor{#2780e3}{\\theta^*}  }(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{z})p_{\\textcolor{#2780e3}{\\theta^*}  }(\\textcolor{#E95420}{z})\\,d\\textcolor{#E95420}{z}\n\n\n\nNote that now q_{\\textcolor{#E95420}{\\phi}} depends on \\textcolor{#2780e3}{x} as well."
  },
  {
    "objectID": "static/Slides.html#プレゼンテーション",
    "href": "static/Slides.html#プレゼンテーション",
    "title": "Slides",
    "section": "プレゼンテーション",
    "text": "プレゼンテーション\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n統数研での学生生活\n\n\n統計数理研究所 大学院説明会\n\n\nスライドはこちら．\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOKENDAI 特別研究員\n\n\n２次審査\n\n\nスライドはこちら．\n\n\n\n2025-03-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総研大５年一貫博士課程・中間評価\n\n\nスライドはこちら．\n\n\n\n2025-01-27\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n  \n    \n      5/23/2025.\n      \n        10:00-12:00.\n      \n      司馬博文 .\n      統数研での学生生活\n      : 統計数理研究所 大学院説明会.\n      \n        \n          統数研大学院説明会.\n        \n      \n      \n        ２階大会議室 (Hybrid).\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      3/16/2025.\n      \n        15:10-15:20.\n      \n      司馬博文 .\n      SOKENDAI 特別研究員\n      : ２次審査.\n      \n        \n          学生特別研究員（総研大）.\n        \n      \n      \n        Zoom.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n        \n           PDF\n        \n      \n    \n  \n    \n      1/27/2025.\n      \n        10:00-10:50.\n      \n      司馬博文 .\n      総研大５年一貫博士課程・中間評価\n      \n      \n        \n          中間評価（総研大）.\n        \n      \n      \n        統数研会議室１（D222）.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Slides.html#non-research-presentations",
    "href": "static/Slides.html#non-research-presentations",
    "title": "Slides",
    "section": "Non-research Presentations",
    "text": "Non-research Presentations\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDrawing Parallels between Statistics and Nature\n\n\nAn Introduction to My Research\n\n\nThese slides are designed to introduce myself in 5 minutes, especially to those who are not familiar with statistics or machine learning. Slides can be viewed in slide mode…\n\n\n\n2025-09-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n統数研での学生生活\n\n\n統計数理研究所 大学院説明会\n\n\nスライドはこちら．\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOKENDAI 特別研究員\n\n\n２次審査\n\n\nスライドはこちら．\n\n\n\n2025-03-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n総研大５年一貫博士課程・中間評価\n\n\nスライドはこちら．\n\n\n\n2025-01-27\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n  \n    \n      9/02/2025.\n      \n        14:00-16:00.\n      \n      Hirofumi Shiba.\n      Drawing Parallels between Statistics and Nature\n      : An Introduction to My Research.\n      \n        \n          SOKENDAI 研究交流セミナー.\n        \n      \n      \n        御茶ノ水ソラシティ.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        10:00-12:00.\n      \n      司馬博文 .\n      統数研での学生生活\n      : 統計数理研究所 大学院説明会.\n      \n        \n          統数研大学院説明会.\n        \n      \n      \n        ２階大会議室 (Hybrid).\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n    \n      3/16/2025.\n      \n        15:10-15:20.\n      \n      司馬博文 .\n      SOKENDAI 特別研究員\n      : ２次審査.\n      \n        \n          学生特別研究員（総研大）.\n        \n      \n      \n        Zoom.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n        \n           PDF\n        \n      \n    \n  \n    \n      1/27/2025.\n      \n        10:00-10:50.\n      \n      司馬博文 .\n      総研大５年一貫博士課程・中間評価\n      \n      \n        \n          中間評価（総研大）.\n        \n      \n      \n        統数研会議室１（D222）.\n      \n      \n      \n        Details\n      \n      \n        \n           Slide\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#denoising-diffusion-models-ddm-1",
    "href": "posts/2025/Slides/DDD_Slides.html#denoising-diffusion-models-ddm-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.7 Denoising Diffusion Models (DDM)",
    "text": "1.7 Denoising Diffusion Models (DDM)\nAs proposed in (Sohl-Dickstein et al., 2015), the KL will reduce to \n\\begin{align*}\n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})&=\\operatorname{KL}\\bigg(q(\\textcolor{#E95420}{z_{1:T}}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z_{1:T}}|\\textcolor{#2780e3}{x})\\bigg)\\\\\n&=\\operatorname{E}[\\log q(\\textcolor{#E95420}{Z_{1:T}}|\\textcolor{#2780e3}{x})]-\\operatorname{E}[\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x},\\textcolor{#E95420}{Z_{1:T}})]+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x})\\\\\n&=:-\\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta})+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}).\n\\end{align*}\n By maximizing the \\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta}), we are still performing a form of (approximate) maximum likelihood inference since \n\\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta})\\le\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}).\n Although approximate as inference, it proved to be very effective in generating high-quality images (Ho et al., 2020)."
  },
  {
    "objectID": "posts/2025/Slides/DDD.html",
    "href": "posts/2025/Slides/DDD.html",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "",
    "text": "1 Mathematical Introduction (-2020)\n2 Developments in Continuous Diffusion Models (2021-2023)\n3 Discrete Diffusion Models (2024-)\n\n\n\n\nD3PM (Discrete Denoising Diffusion Probabilistic Model) example from (Ryu, 2024)"
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#mathematical-introduction",
    "href": "posts/2025/Slides/DDD.html#mathematical-introduction",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1 Mathematical Introduction",
    "text": "1 Mathematical Introduction\n\nProblem Setting: Generative Modeling ≒ Bayesian Modeling\nTwo main approaches:\n\nSampling-based Methods: Monte Carlo methods, etc.\nOptimization-based Methods: Diffusion Models, etc.\n\nDiffusion Models succeed by\n\nDiscarding inference\nConcentrating on learning to generate\n\n\n\n1.1 Problem: Bayesian / Generative Modeling\n\n\n\n1.2 Two Popular Solutions\nProblem: compute the posterior distribution: \np(\\textcolor{#E95420}{z}|\\{x_i\\}_{i=1}^n)\\propto p(\\textcolor{#E95420}{z})\\prod_{i=1}^n p(x_i|\\textcolor{#E95420}{z})=\\text{prior}\\times\\prod_{i=1}^n\\text{model likelihood of }x_i\n\n\n\n\n\n\n\n\n\n\n\nSampling-based Methods\nOptimization-based Methods\n\n\n\n\nPurpose\nGet a sample\nGet an approximation\n\n\nScalable?\nNo (Yet)\nYes\n\n\nExact?\nYes\nNo\n\n\nE.g.\nMonte Carlo\nDiffusion Models\n\n\nMainly used\nin Bayesian statistics\nin Machine Learning\n\n\n\n\n\n\nThis talk is about an optimization-based Methods\n\n\n1.3 Markov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\nProperty of Langevin Diffusion\n\n\n\n\nd\\textcolor{#2780e3}{X_t}=-\\nabla\\log p(\\textcolor{#2780e3}{X_t}|\\{x_i\\}_{i=1}^n)\\,dt+dB_t\n\nconverges to p(\\textcolor{#E95420}{z}|\\{x_i\\}_{i=1}^n) as t\\to\\infty.\n\n\nThis approach is feasible because …\n\n\\text{score function}\\quad\\nabla\\log p(\\textcolor{#E95420}{z}|\\{x_i\\}_{i=1}^n)\n\nis evaluatable.\n\n\n\n\n1.4 Piecewise Deterministic Monte Carlo\n\n\n\n\n\nBetter convergence(Andrieu and Livingstone, 2021)\nBetter scalability(Bierkens et al., 2019)\nNumerical stability(Chevallier et al., 2025)\n\nAvailable in our package PDMPFlux.jl\n\n] add PDMPFlux\n\n\n\n\n1.5 Variational Inference\n\n\\text{Posterior distribution:}\\qquad p(\\textcolor{#E95420}{z}|\\boldsymbol{x})\\propto p(\\textcolor{#E95420}{z})\\prod_{i=1}^n p(x_i|\\textcolor{#E95420}{z})\n is searched in a variational formulation via KL divergence: \np(\\textcolor{#E95420}{z}|\\boldsymbol{x})=\\argmin_{q\\in\\mathcal{P}(\\textcolor{#E95420}{\\mathcal{Z}})}\\operatorname{KL}\\bigg(q(\\textcolor{#E95420}{z}),p(\\textcolor{#E95420}{z}|\\boldsymbol{x})\\bigg).\n\n\n\n\n\n\n\nScalable Solution to VI\n\n\n\n\nConstrain the problem on q\\in\\{q_{\\textcolor{#E95420}{\\phi}}\\}_{\\textcolor{#E95420}{\\phi}\\in\\R^d},\nSolve by (stochastic) optimization, using the gradient of \n\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}),p(\\textcolor{#E95420}{z}|\\boldsymbol{x})\\bigg)=\\operatorname{E}_{\\textcolor{#E95420}{\\phi}}[\\log q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{Z})]\n-\\operatorname{E}_{\\textcolor{#E95420}{\\phi}}[\\log p(\\textcolor{#E95420}{Z},\\boldsymbol{x})]+\\text{const.}\n\n\n\n\n\n\n1.6 Variational Auto-Encoder (VAE)\nIn generative modeling, we also have to learn p\\in\\{p_{\\textcolor{#2780e3}{\\theta}}\\}_{\\textcolor{#2780e3}{\\theta}\\in\\R^e}\n\n\n\n\n\nJointly trained to minimize the KL divergence \n\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})\\bigg).\n\n\n\n1.6 Variational Auto-Encoder (VAE)\n(Kingma and Welling, 2014) found that a part of the KL divergence \n\\begin{align*}\n&\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})\\bigg)\\\\\n&\\qquad=\\operatorname{E}_{\\textcolor{#E95420}{\\phi},\\textcolor{#2780e3}{x}}[\\log q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{Z}|\\textcolor{#2780e3}{x})]\n-\\operatorname{E}_{\\textcolor{#E95420}{\\phi},\\textcolor{#2780e3}{x}}[\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z},\\textcolor{#2780e3}{x})]+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x})\\\\\n&\\qquad=\\underbrace{\\operatorname{KL}\\bigg(q_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z})\\bigg)-\\operatorname{E}_{\\textcolor{#E95420}{\\phi},\\textcolor{#2780e3}{x}}[\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{Z})]}_{=:-\\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta},\\textcolor{#E95420}{\\phi})\\text{ : we only optimize this part}}+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x})\n\\end{align*}\n still lends itself to stochastic optimization.\nOnce \\textcolor{#2780e3}{\\theta^*} is learned, we are able to sample from \np_{\\textcolor{#2780e3}{\\theta^*}}(\\textcolor{#2780e3}{x})=\\int_{\\textcolor{#E95420}{\\mathcal{Z}}}p_{\\textcolor{#2780e3}{\\theta^*}  }(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{z})p_{\\textcolor{#2780e3}{\\theta^*}  }(\\textcolor{#E95420}{z})\\,d\\textcolor{#E95420}{z}\n\n\n\nNote that now q_{\\textcolor{#E95420}{\\phi}} depends on \\textcolor{#2780e3}{x} as well.\n\n\n1.7 Denoising Diffusion Models (DDM)\nConcentrating on learning p_{\\textcolor{#2780e3}{\\theta}}, we fix \nq_{\\textcolor{#E95420}{\\phi}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=q(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=q^{t_1}(\\textcolor{#E95420}{z_1}|\\textcolor{#2780e3}{x})\\prod_{i=1}^T q^{t_{i+1}-t_i}(\\textcolor{#E95420}{z_{i+1}}|\\textcolor{#E95420}{z_{i}}),\n as a path measure of a Langevin diffusion on \\textcolor{#E95420}{\\mathcal{Z}}=(\\R^d)^{T+1}.\n\n\n\nA common choice is an OU process: q^t(z|x)=\\operatorname{N}(z;x,t).\n\n\n1.7 Denoising Diffusion Models (DDM)\nAs proposed in (Sohl-Dickstein et al., 2015), the KL will reduce to \n\\begin{align*}\n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})&=\\operatorname{KL}\\bigg(q(\\textcolor{#E95420}{z_{1:T}}|\\textcolor{#2780e3}{x}),p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z_{1:T}}|\\textcolor{#2780e3}{x})\\bigg)\\\\\n&=\\operatorname{E}[\\log q(\\textcolor{#E95420}{Z_{1:T}}|\\textcolor{#2780e3}{x})]-\\operatorname{E}[\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x},\\textcolor{#E95420}{Z_{1:T}})]+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x})\\\\\n&=:-\\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta})+\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}).\n\\end{align*}\n By maximizing the \\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta}), we are still performing a form of (approximate) maximum likelihood inference since \n\\operatorname{ELBO}(\\textcolor{#2780e3}{\\theta})\\le\\log p_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}).\n Although approximate as inference, it proved to be very effective in generating high-quality images (Ho et al., 2020).\n\n\n1.7 Denoising Diffusion Models (DDM)\nIt is because DDM learns how to denoise a noisy data. DDM …\n× constrains the posterior to be \\operatorname{N}(0,I_d),\n○ the whole training objective is devoted to learn the generator p_{\\textcolor{#2780e3}{\\theta}}\n\n\n\nA very famous figure from (Kreis et al., 2022)\n\n\n\n\nSummary\n\nProblem Setting: Generative Modeling ≒ Bayesian Modeling\nTwo main approaches:\n\nSampling-based Methods: MCMC, PDMC, etc.\nOptimization-based Methods: VI, VAE, DDM, etc.\n\nDDM succeeds by\n\nDiscarding modeling inference process q_{\\textcolor{#E95420}{\\phi}}\nConcentrating on learning to generate from p_{\\textcolor{#2780e3}{\\theta}}\n\n\n\n\n\n\n\n\nDevelopment 1: More Concentration on Learning to Generate\n\n\n\nIsn’t there a more suitable training objective?\n\n\n\n\n\n\n\n\nDevelopment 2: Overlooked Design Choice\n\n\n\nWas “fixing q_{\\textcolor{#E95420}{\\phi}} to be a Langevin diffusion” really a good idea?"
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#analysis-of-discrete-diffusion-models",
    "href": "posts/2025/Slides/DDD.html#analysis-of-discrete-diffusion-models",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2 Analysis of Discrete Diffusion Models",
    "text": "2 Analysis of Discrete Diffusion Models"
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#references",
    "href": "posts/2025/Slides/DDD.html#references",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "References",
    "text": "References\n\n\nAlbergo, M. S., and Vanden-Eijnden, E. (2023). Building Normalizing Flows with Stochastic Interpolants. In The eleventh international conference on learning representations.\n\n\nAnderson, B. D. O. (1982). Reverse-time diffusion equation models. Stochastic Processes and Their Applications, 12(3), 313–326.\n\n\nAndrieu, C., and Livingstone, S. (2021). Peskun–Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario. The Annals of Statistics, 49(4), 1958–1981.\n\n\nArriola, M., Sahoo, S. S., Gokaslan, A., Yang, Z., Qi, Z., Han, J., … Kuleshov, V. (2025). Block diffusion: Interpolating between autoregressive and diffusion language models. In The thirteenth international conference on learning representations.\n\n\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and Berg, R. van den. (2021). Structured Denoising Diffusion Models in Discrete State-Spaces. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in neural information processing systems,Vol. 34, pages 17981–17993. Curran Associates, Inc.\n\n\nBierkens, J., Fearnhead, P., and Roberts, G. (2019). The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data. The Annals of Statistics, 47(3), 1288–1320.\n\n\nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. (2022). A Continuous Time Framework for Discrete Denoising Models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in neural information processing systems,Vol. 35, pages 28266–28279. Curran Associates, Inc.\n\n\nCao, Y., Chen, J., Luo, Y., and ZHOU, X. (2023). Exploring the optimal choice for generative processes in diffusion models: Ordinary vs stochastic differential equations. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in neural information processing systems,Vol. 36, pages 33420–33468. Curran Associates, Inc.\n\n\nChen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in neural information processing systems,Vol. 31. Curran Associates, Inc.\n\n\nChevallier, A., Power, S., and Sutton, M. (2025). Towards practical PDMP sampling: Metropolis adjustments, locally adaptive step-sizes, and NUTS-based time lengths.\n\n\nGat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T. Q., Synnaeve, G., … Lipman, Y. (2024). Discrete flow matching. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems,Vol. 37, pages 133345–133385. Curran Associates, Inc.\n\n\nHo, J., Jain, A., and Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. In Advances in neural information processing systems,Vol. 33.\n\n\nKarras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in neural information processing systems.\n\n\nKingma, D. P., and Welling, M. (2014). Auto-encoding variational bayes. In International conference on learning representations,Vol. 2.\n\n\nKreis, K., Gao, R., and Vahdat, A. (2022). Denoising diffusion-based generative modeling: Foundations and applications.\n\n\nLiang, Y., Huang, R., Lai, L., Shroff, N., and Liang, Y. (2025). Absorb and converge: Provable convergence guarantee for absorbing discrete diffusion models.\n\n\nLipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. (2023). Flow matching for generative modeling. In The eleventh international conference on learning representations.\n\n\nLiu, S., Nam, J., Campbell, A., Stark, H., Xu, Y., Jaakkola, T., and Gomez-Bombarelli, R. (2025). Think while you generate: Discrete diffusion with planned denoising. In The thirteenth international conference on learning representations.\n\n\nLiu, X., Gong, C., and liu, qiang. (2023). Flow straight and fast: Learning to generate and transfer data with rectified flow. In The eleventh international conference on learning representations.\n\n\nLou, A., Meng, C., and Ermon, S. (2024). Discrete diffusion modeling by estimating the ratios of the data distribution. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st international conference on machine learning,Vol. 235, pages 32819–32848. PMLR.\n\n\nNeklyudov, K., Brekelmans, R., Severo, D., and Makhzani, A. (2023). Action matching: Learning stochastic dynamics from samples. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th international conference on machine learning,Vol. 202, pages 25858–25889. PMLR.\n\n\nRyu, S. (2024). Minimal implementation of a D3PM (structured denoising diffusion models in discrete state-spaces), in pytorch.\n\n\nShi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. (2024). Simplified and generalized masked diffusion for discrete data. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in neural information processing systems,Vol. 37, pages 103131–103167. Curran Associates, Inc.\n\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In F. Bach and D. Blei, editors, Proceedings of the 32nd international conference on machine learning,Vol. 37, pages 2256–2265. Lille, France: PMLR.\n\n\nSong, J., Meng, C., and Ermon, S. (2021). Denoising diffusion implicit models. In International conference on learning representations.\n\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. In International conference on learning representations.\n\n\nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. (2023). Score-based Continuous-time Discrete Diffusion Models. In The eleventh international conference on learning representations."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#denoising-diffusion-models-ddm-2",
    "href": "posts/2025/Slides/DDD_Slides.html#denoising-diffusion-models-ddm-2",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.7 Denoising Diffusion Models (DDM)",
    "text": "1.7 Denoising Diffusion Models (DDM)\nIt is because DDM learns how to denoise a noisy data. DDM …\n× constrains the posterior to be \\operatorname{N}(0,I_d),\n○ the whole training objective is devoted to learn the generator p_{\\textcolor{#2780e3}{\\theta}}\n\nA very famous figure from (Kreis et al., 2022)"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#score-based-diffusion-models",
    "href": "posts/2025/Slides/DDD_Slides.html#score-based-diffusion-models",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.8 Score-based Diffusion Models",
    "text": "1.8 Score-based Diffusion Models\n\n\n\n\nProposition about the Langevin Diffusion (Anderson, 1982)\n\n\nThe following two processes have the same distribution, but in the opposite direction:\n\n\\text{Langevin diffusion:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=U_t(\\textcolor{#E95420}{Z_t})\\,dt+dB_t\n \n\\text{Denoising diffusion:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-U_{T-t}(\\textcolor{#2780e3}{X_t})+\\underbrace{\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{T-t}(\\textcolor{#2780e3}{X_t})}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\n\n\nTo learn the path measure p_{\\textcolor{#2780e3}{\\theta}} is to learn the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#flow-matching",
    "href": "posts/2025/Slides/DDD_Slides.html#flow-matching",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.9 Flow Matching",
    "text": "1.9 Flow Matching\nThe solution to the following ODE \n\\frac{d\\textcolor{#2780e3}{X}_t}{dt}=U_t(\\textcolor{#2780e3}{X_t})-\\frac{1}{2}\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#2780e3}{X_t})\n\\tag{1} has the same 1d marginal distributions as the reverse-time Langevin diffusion \nd\\textcolor{#2780e3}{X_t}=\\bigg(-U_{T-t}(\\textcolor{#2780e3}{X_t})+\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{T-t}(\\textcolor{#2780e3}{X_t})\\bigg)\\,dt+dB'_t.\n Equation (1) provides a faster way to sample from \\{\\textcolor{#2780e3}{X_t}\\}_{t=0}^T.\n\n\nAlternatively, we can directly try to learn the RHS of (1). This approach is called flow matching, independently proposed by (Liu et al., 2023), (Albergo and Vanden-Eijnden, 2023), (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#summary",
    "href": "posts/2025/Slides/DDD_Slides.html#summary",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Summary",
    "text": "Summary\n\nProblem Setting: Generative Modeling ≒ Bayesian Modeling\nTwo main approaches:\n\nSampling-based Methods: MCMC, PDMC, etc.\nOptimization-based Methods: VI, VAE, DDM, etc.\n\nDDM succeeds by\n\nDiscarding modeling inference process q_{\\textcolor{#E95420}{\\phi}}\nConcentrating on learning to generate from p_{\\textcolor{#2780e3}{\\theta}}\n\n\n\n\n\n\nDevelopment 1: More Concentration on Learning to Generate\n\n\nIsn’t there a more suitable training objective?\n\n\n\n\n\n\n\n\nDevelopment 2: Overlooked Design Choice\n\n\nWas “fixing q_{\\textcolor{#E95420}{\\phi}} to be a Langevin diffusion” really a good idea?"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ddm-sampling",
    "href": "posts/2025/Slides/DDD_Slides.html#ddm-sampling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.10 DDM Sampling",
    "text": "1.10 DDM Sampling\nTo generate a new sample after learning the path measure p_{\\textcolor{#2780e3}{\\theta}},\nwe avoid tracing it exactly, by …\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nLoss\nScore matching\nFlow matching\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\nWe need a theory to explain these behaviours.\n\n\n\n\nE.g.\n\n\nSDE approach is more robust to the estimation error in the score\n\n\n\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#score-based-diffusion-modeling",
    "href": "posts/2025/Slides/DDD_Slides.html#score-based-diffusion-modeling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.8 Score-based Diffusion Modeling",
    "text": "1.8 Score-based Diffusion Modeling\n\n\n\n\nProposition about the Langevin Diffusion (Anderson, 1982)\n\n\nThe following two processes have the same distribution, but in the opposite direction:\n\n\\text{Langevin diffusion:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=U_t(\\textcolor{#E95420}{Z_t})\\,dt+dB_t\n \n\\text{Denoising diffusion:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-U_{T-t}(\\textcolor{#2780e3}{X_t})+\\underbrace{\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{T-t}(\\textcolor{#2780e3}{X_t})}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\n\n\nTo learn the path measure p_{\\textcolor{#2780e3}{\\theta}} is to learn the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.9 ODE Sampling",
    "text": "1.9 ODE Sampling\nThe solution to the following ODE \n\\frac{d\\textcolor{#2780e3}{X}_t}{dt}=U_t(\\textcolor{#2780e3}{X_t})-\\frac{1}{2}\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#2780e3}{X_t})\n\\tag{1} has the same 1d marginal distributions as the reverse-time Langevin diffusion \nd\\textcolor{#2780e3}{X_t}=\\bigg(-U_{T-t}(\\textcolor{#2780e3}{X_t})+\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{T-t}(\\textcolor{#2780e3}{X_t})\\bigg)\\,dt+dB'_t.\n Equation (1) provides a faster way to sample from \\{\\textcolor{#2780e3}{X_t}\\}_{t=0}^T.\n\n\nAlternatively, we can directly try to learn the RHS of (1). This approach is called flow matching, independently proposed by (Liu et al., 2023), (Albergo and Vanden-Eijnden, 2023), (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#score-based-ddm",
    "href": "posts/2025/Slides/DDD_Slides.html#score-based-ddm",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.2 Score-based DDM",
    "text": "2.2 Score-based DDM\n\n\n\n\nTheorem from (Anderson, 1982)\n\n\n(\\textcolor{#E95420}{Z}_t)_{t=0}^T and (\\textcolor{#2780e3}{X}_{T-t})_{t=0}^T have the same path measure:\n\n\\text{\\textcolor{#E95420}{Langevin diffusion}:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=b_t(\\textcolor{#E95420}{Z}_t)\\,dt+dB_t\n \n\\text{\\textcolor{#2780e3}{Denoising diffusion}:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-b_{T-t}(\\textcolor{#2780e3}{X}_t)+\\underbrace{\\nabla\\log q^{T-t}(\\textcolor{#2780e3}{X}_t)}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\n\n\nLearning (\\textcolor{#2780e3}{X}_{t}) is equivalent to learning the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log q^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.2 ODE Sampling of DDM",
    "text": "2.2 ODE Sampling of DDM\nTo generate a new sample after learning the path measure p_{\\textcolor{#2780e3}{\\theta}},\nwe avoid tracing it exactly, by …\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nLoss\nScore matching\nFlow matching\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\nWe need a theory to explain these behaviours.\n\n\n\n\nE.g.\n\n\nSDE approach is more robust to the estimation error in the score\n\n\n\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm-1",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.2 ODE Sampling of DDM",
    "text": "2.2 ODE Sampling of DDM\nTo generate a new sample after learning the path measure p_{\\textcolor{#2780e3}{\\theta}},\nwe avoid tracing it exactly, by …\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nLoss\nScore matching\nFlow matching\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\nWe need a theory to explain these behaviours.\n\n\n\n\nE.g.\n\n\nSDE approach is more robust to the estimation error in the score\n\n\n\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#discrete-diffusion-models",
    "href": "posts/2025/Slides/DDD_Slides.html#discrete-diffusion-models",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.2 Discrete Diffusion Models",
    "text": "2.2 Discrete Diffusion Models\n\n\nDiscrete state space \\textcolor{#E95420}{\\mathcal{Z}} (e.g. images, texts) offers …\n\ndirect modeling without discretization\na diverse choice of inference processes q_{\\textcolor{#E95420}{\\phi}}\nununiform weights on different states\n\n\n\n\n\n\n\n\n\n\n\nHistorical Development\n\n\n\n\n\n\n\n\\textcolor{#E95420}{\\mathcal{Z}}\nContinuous\nDiscrete\n\n\n\n\nOrigin\n(Sohl-Dickstein et al., 2015)\n(Austin et al., 2021)\n\n\nContinuous-time\n(Y. Song et al., 2021)\n(Campbell et al., 2022)\n\n\nScore-based\n(Y. Song et al., 2021)\n(Sun et al., 2023)\n\n\nFlow-based\n(Lipman et al., 2023)\n(Gat et al., 2024)"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm",
    "href": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Flow-based DDM",
    "text": "2.1 Flow-based DDM"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#summary-1",
    "href": "posts/2025/Slides/DDD_Slides.html#summary-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Summary",
    "text": "Summary\n\nStraight path\n\nLangevin path p_{\\textcolor{#2780e3}{\\theta}} requires a forward simulation during training"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm-much-flexible-framework",
    "href": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm-much-flexible-framework",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Flow-based DDM: Much Flexible Framework",
    "text": "2.1 Flow-based DDM: Much Flexible Framework"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm-a-flexible-framework",
    "href": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm-a-flexible-framework",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.6 Flow-based DDM: A Flexible Framework",
    "text": "2.6 Flow-based DDM: A Flexible Framework\nInstead of score \\nabla\\log q^t(\\textcolor{#E95420}{z}), we learn the vector field u satisfying \n(\\text{continuity equation})\\quad\\partial_tp^t+\\operatorname{div}(p^tu^t)=0.\n We learn u by a NN (t,x)\\mapsto v_{\\textcolor{#2780e3}{\\theta}}^t(x) with the loss \n\\text{Flow Matching Loss:}\\qquad\\mathcal{L}_{\\text{FM}}(\\textcolor{#2780e3}{\\theta})=\\int_0^T\\operatorname{E}\\bigg[\\bigg|v_{\\textcolor{#2780e3}{\\theta}}^t(X)-u^t(X)\\bigg|^2\\bigg]\\,dt.\n To generate a new sample, we let X_0\\sim p^0 flow along v_{\\textcolor{#2780e3}{\\theta^*}}^t.\n\n\nUsually, FM is understood as a scalable alternative to train CNFs (Chen et al., 2018). Being an alternative to score matching by learning directly the RHS of (1), this approach is called flow matching, independently proposed by (X. Liu et al., 2023), (Albergo and Vanden-Eijnden, 2023), (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm-a-flexible-framework-1",
    "href": "posts/2025/Slides/DDD_Slides.html#flow-based-ddm-a-flexible-framework-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.4 Flow-based DDM: A Flexible Framework",
    "text": "2.4 Flow-based DDM: A Flexible Framework\n\n\n\n\n\n\nDiffusion Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm-.stylepadding-bottom-0px",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm-.stylepadding-bottom-0px",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.9 ODE Sampling of DDM {.style=“padding-bottom: 0px;”}",
    "text": "1.9 ODE Sampling of DDM {.style=“padding-bottom: 0px;”}\n\n\\text{ODE:}\\qquad\\frac{d\\textcolor{#2780e3}{X}_t}{dt}=-U_t(\\textcolor{#2780e3}{X_t})+\\frac{1}{2}\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#2780e3}{X_t})=:u_t(\\textcolor{#2780e3}{X_t})\n\\tag{1} has the same 1d marginal distributions as \n\\text{Reversed Langevin:}\\quad d\\textcolor{#2780e3}{X_t}=\\bigg(-U_{t}(\\textcolor{#2780e3}{X_t})+\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{t}(\\textcolor{#2780e3}{X_t})\\bigg)\\,dt+dB_t."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm-padding-bottom-0px",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-ddm-padding-bottom-0px",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "1.9 ODE Sampling of DDM {padding-bottom: 0px;}",
    "text": "1.9 ODE Sampling of DDM {padding-bottom: 0px;}\n\n\\text{ODE:}\\qquad\\frac{d\\textcolor{#2780e3}{X}_t}{dt}=-U_t(\\textcolor{#2780e3}{X_t})+\\frac{1}{2}\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#2780e3}{X_t})=:u_t(\\textcolor{#2780e3}{X_t})\n\\tag{1} has the same 1d marginal distributions as \n\\text{Reversed Langevin:}\\quad d\\textcolor{#2780e3}{X_t}=\\bigg(-U_{t}(\\textcolor{#2780e3}{X_t})+\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{t}(\\textcolor{#2780e3}{X_t})\\bigg)\\,dt+dB_t."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#algorithmic-stability",
    "href": "posts/2025/Slides/DDD_Slides.html#algorithmic-stability",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Algorithmic Stability",
    "text": "Algorithmic Stability\n\n\n\n\nThe Score of DDM (Y. Song et al., 2021)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Vector Field of FM (Lipman et al., 2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne hidden theme was algorithmic stability, which plays a crucial role in the successful methods."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#other-training-objectives",
    "href": "posts/2025/Slides/DDD_Slides.html#other-training-objectives",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Other Training Objectives",
    "text": "Other Training Objectives\nInstead of the vector field u, we can learn its potential \nv_\\theta^t=\\nabla s_\\theta^t.\n through the Action Matching loss (Neklyudov et al., 2023) \n\\mathcal{L}_{\\text{AM}}(\\theta)=\\operatorname{E}[s^0_\\theta(X_0)-s^1_\\theta(X_1)]+\\int^1_0\\operatorname{E}\\bigg[\\frac{1}{2}|\\nabla s^t_\\theta(X)|^2+\\partial_ts^t_\\theta(X)\\bigg]\\,dt\n\n\n\n\n\nHiro Shiba"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#masked-diffusion",
    "href": "posts/2025/Slides/DDD_Slides.html#masked-diffusion",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.1 Masked Diffusion",
    "text": "3.1 Masked Diffusion\nConsider path measures \\{p_t\\}_{t=0}^T\\subset\\mathcal{P}(\\textcolor{#E95420}{\\mathcal{Z}}\\sqcup\\{m\\}) that satisfy \n\\text{1d marginal condition:}\\qquad p_t(-|\\textcolor{#2780e3}{x})=\\alpha_t\\delta_{\\textcolor{#2780e3}{x}}(-)+(1-\\alpha_t)\\delta_m(-),\n \n\\therefore\\qquad\\delta_m(-)\\xleftarrow{t\\to0}p_t(-)=\\int_{\\textcolor{#2780e3}{\\mathcal{X}}}p_t(-|\\textcolor{#2780e3}{x})p^{\\textcolor{#2780e3}{\\text{data}}}(\\textcolor{#2780e3}{x})\\,d\\textcolor{#2780e3}{x}\\xrightarrow{t\\to T}p^{\\textcolor{#2780e3}{\\text{data}}}(-).\n One reverse process that trace \\{p_t\\}_{t=0}^T is the Markov process with the rate \nR_t(m,\\textcolor{#2780e3}{x})=\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\underbrace{p_{t}(\\textcolor{#2780e3}{x}|m).}_{\\text{learn this part using NN}}"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html",
    "href": "posts/2025/DiffusionModels/CTDDM.html",
    "title": "Masked Diffusion Models",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#denoising-markov-model-再論",
    "href": "posts/2025/DiffusionModels/CTDDM.html#denoising-markov-model-再論",
    "title": "拡散モデルの離散空間・連続時間での設計",
    "section": "1 Denoising Markov Model 再論",
    "text": "1 Denoising Markov Model 再論\n\n1.1 はじめに：離散状態空間\n拡散モデルのアイデアをどこまで一般化できるか？何が最も肝心な骨組みで，何が交換可能であるかを再論したい．\n例えば，画像・言語をはじめとして，離散データのモデリングを考える際に，連続緩和するのではなく離散なまま扱いたいとしよう．\nこのような場合でも全く問題なく，一般化スコアマッチングにより従来通りの議論により，訓練目標が得られることが (Benton, Shi, et al., 2024) で示されている．\n\n\n1.2 離散拡散モデルを連続時間で考えたい理由"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#誤差解析",
    "href": "posts/2025/DiffusionModels/CTDDM.html#誤差解析",
    "title": "拡散モデルの離散空間・連続時間での設計",
    "section": "2 誤差解析",
    "text": "2 誤差解析\n\n2.1 はじめに\n\\(p\\) を前向き過程の遷移密度，\\(q\\) を逆向き過程の遷移密度としたとき， \\[\n\\operatorname{KL}(p_\\delta,q_t)\n\\] という量を評価することを考える．\n連続状態空間上の，普通の意味での拡散モデルに対しては (Benton, Bortoli, et al., 2024) が，離散状態空間上では (Ren et al., 2025) がこれを行っている．"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#in-search-of-better-path",
    "href": "posts/2025/Slides/DDD_Slides.html#in-search-of-better-path",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.5 In Search of Better Path",
    "text": "2.5 In Search of Better Path\n\n\n\n\n\nDiscrete Time Markov Chain\n\n\n\n\n\n\nDiffusion Process\n\n\n\n\n\n\nPiecewise Deterministic"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#discrete-diffusion-models-1",
    "href": "posts/2025/Slides/DDD_Slides.html#discrete-diffusion-models-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.1 Discrete Diffusion Models",
    "text": "3.1 Discrete Diffusion Models\n\n\nDiscrete state space \\textcolor{#2780e3}{\\mathcal{X}} (e.g. images, texts) offers …\n\ndirect modeling without discretization\na diverse choice of inference processes q_{\\textcolor{#E95420}{\\phi}}\nununiform weights on different states\n\n\n\n\n\n\n\n\n\n\nHistorical Development\n\n\n\n\n\n\n\n\\textcolor{#2780e3}{\\mathcal{X}}\nContinuous\nDiscrete\n\n\n\n\nTrigger\n(Ho et al., 2020)\n(Austin et al., 2021)\n\n\nContinuous-time\n(Y. Song et al., 2021)\n(Campbell et al., 2022)\n\n\nScore-based\n(Y. Song et al., 2021)\n(Sun et al., 2023)\n\n\nFlow-based\n(Lipman et al., 2023)\n(Gat et al., 2024)"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#summary-2",
    "href": "posts/2025/Slides/DDD_Slides.html#summary-2",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Summary",
    "text": "Summary\n\nStraight path\n\nLangevin path p_{\\textcolor{#2780e3}{\\theta}} requires a forward simulation during training"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#score-based-ddm-a-continuous-time-perspective",
    "href": "posts/2025/Slides/DDD_Slides.html#score-based-ddm-a-continuous-time-perspective",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Score-based DDM: A Continuous-Time Perspective",
    "text": "2.1 Score-based DDM: A Continuous-Time Perspective\n\n\n\n\nTheorem from (Anderson, 1982)\n\n\n(\\textcolor{#E95420}{Z}_t)_{t=0}^T and (\\textcolor{#2780e3}{X}_{T-t})_{t=0}^T have the same path measure:\n\n\\text{\\textcolor{#E95420}{Langevin diffusion}:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=b_t(\\textcolor{#E95420}{Z}_t)\\,dt+dB_t\n \n\\text{\\textcolor{#2780e3}{Denoising diffusion}:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-b_{T-t}(\\textcolor{#2780e3}{X}_t)+\\underbrace{\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^{T-t}(\\textcolor{#2780e3}{X}_t)}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\n\n\nLearning (\\textcolor{#2780e3}{X}_{t}) is equivalent to learning the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log p_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#score-based-ddm-a-new-objective",
    "href": "posts/2025/Slides/DDD_Slides.html#score-based-ddm-a-new-objective",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Score-based DDM: A New Objective",
    "text": "2.1 Score-based DDM: A New Objective\n\n\n\n\nTheorem from (Anderson, 1982)\n\n\n(\\textcolor{#E95420}{Z}_t)_{t=0}^T and (\\textcolor{#2780e3}{X}_{T-t})_{t=0}^T have the same path measure:\n\n\\text{\\textcolor{#E95420}{Langevin diffusion}:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=b_t(\\textcolor{#E95420}{Z}_t)\\,dt+dB_t\n \n\\text{\\textcolor{#2780e3}{Denoising diffusion}:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-b_{T-t}(\\textcolor{#2780e3}{X}_t)+\\underbrace{\\nabla\\log q^{T-t}(\\textcolor{#2780e3}{X}_t)}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\n\n\nLearning (\\textcolor{#2780e3}{X}_{t}) is equivalent to learning the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log q^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-score-based-ddm",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-score-based-ddm",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.3 ODE Sampling of Score-based DDM",
    "text": "2.3 ODE Sampling of Score-based DDM\n\n\\text{ODE:}\\qquad\\frac{d\\textcolor{#2780e3}{X}_t}{dt}=-b_t(\\textcolor{#2780e3}{X_t})+\\frac{1}{2}s_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#2780e3}{X_t})=:v^t_\\theta(\\textcolor{#2780e3}{X_t})\n\\tag{1} has the same 1d marginal distributions as \n\\text{\\textcolor{#2780e3}{Denoising diffusion} SDE:}\\quad d\\textcolor{#2780e3}{X_t}=\\bigg(-b_{t}(\\textcolor{#2780e3}{X_t})+s_{\\textcolor{#2780e3}{\\theta}}^{t}(\\textcolor{#2780e3}{X_t})\\bigg)\\,dt+dB_t."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-score-based-ddm-1",
    "href": "posts/2025/Slides/DDD_Slides.html#ode-sampling-of-score-based-ddm-1",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.2 ODE Sampling of Score-based DDM",
    "text": "2.2 ODE Sampling of Score-based DDM\nTo generate a new sample after learning the path measure p_{\\textcolor{#2780e3}{\\theta}},\nwe avoid tracing it exactly, by …\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nLoss\nScore matching\nFlow matching\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\nWe need a theory to explain these behaviours.\n\n\n\n\nE.g.\n\n\nSDE approach is more robust to the estimation error in the score\n\n\n\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#sampling-of-score-based-ddm",
    "href": "posts/2025/Slides/DDD_Slides.html#sampling-of-score-based-ddm",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.3 Sampling of Score-based DDM",
    "text": "2.3 Sampling of Score-based DDM\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nForward Path\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n\n\nBackward Path\n(p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n(?)\n\n\nLoss\nScore matching\nFlow matching\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset.\nThe SDE approach seems to be more robust to the estimation error in the score (Cao et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#the-new-lost-enables-new-sampling",
    "href": "posts/2025/Slides/DDD_Slides.html#the-new-lost-enables-new-sampling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.3 The New Lost Enables New Sampling",
    "text": "2.3 The New Lost Enables New Sampling\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nForward Path\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n\n\nBackward Path\n(p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n(?)\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset.\nThe SDE approach seems to be more robust to the estimation error in the score (Cao et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#new-loss-enables-new-sampling",
    "href": "posts/2025/Slides/DDD_Slides.html#new-loss-enables-new-sampling",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.4 New Loss Enables New Sampling",
    "text": "2.4 New Loss Enables New Sampling\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nForward Path\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n\n\nBackward Path\n(p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{z}))_{t=0}^T\n(?)\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\nProblem: “ODE Solver applied to SDE path” doesn’t make sense.\n\n→ Explore other possibilities in the forward path\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset.\nThe SDE approach seems to be more robust to the estimation error in the score (Cao et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#new-forward-path",
    "href": "posts/2025/Slides/DDD_Slides.html#new-forward-path",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.7 New Forward Path",
    "text": "2.7 New Forward Path\n\n\n\n\n\n\nDiffusion Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#limit-in-ttoinfty",
    "href": "posts/2025/Slides/DDD_Slides.html#limit-in-ttoinfty",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Limit in T\\to\\infty",
    "text": "2.1 Limit in T\\to\\infty"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#score-based-ddm-in-sde-formulation",
    "href": "posts/2025/Slides/DDD_Slides.html#score-based-ddm-in-sde-formulation",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.2 Score-based DDM in SDE formulation",
    "text": "2.2 Score-based DDM in SDE formulation\n\n\n\n\nTheorem from (Anderson, 1982)\n\n\n(\\textcolor{#E95420}{Z}_t)_{t=0}^T and (\\textcolor{#2780e3}{X}_{T-t})_{t=0}^T have the same path measure:\n\n\\text{\\textcolor{#E95420}{Langevin diffusion}:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=b_t(\\textcolor{#E95420}{Z}_t)\\,dt+dB_t\n \n\\text{\\textcolor{#2780e3}{Denoising diffusion}:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-b_{T-t}(\\textcolor{#2780e3}{X}_t)+\\underbrace{\\nabla\\log q^{T-t}(\\textcolor{#2780e3}{X}_t)}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\n\n\nLearning (\\textcolor{#2780e3}{X}_{t}) is equivalent to learning the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log q^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#limit-in-ttoinfty-to-get-sde-formulation",
    "href": "posts/2025/Slides/DDD_Slides.html#limit-in-ttoinfty-to-get-sde-formulation",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Limit in T\\to\\infty to get SDE formulation",
    "text": "2.1 Limit in T\\to\\infty to get SDE formulation"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#limit-in-ttoinfty-leads-to-sde-formulation",
    "href": "posts/2025/Slides/DDD_Slides.html#limit-in-ttoinfty-leads-to-sde-formulation",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.1 Limit in T\\to\\infty leads to SDE formulation",
    "text": "2.1 Limit in T\\to\\infty leads to SDE formulation"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#in-search-of-better-forward-path",
    "href": "posts/2025/Slides/DDD_Slides.html#in-search-of-better-forward-path",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.5 In Search of Better Forward Path",
    "text": "2.5 In Search of Better Forward Path\n\n\n\n\n\nDiscrete Time Markov Chain\n\n\n\n\n\n\nDiffusion Process\n\n\n\n\n\n\nPiecewise Deterministic"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#new-forward-path-and-corresponding-backward-ode",
    "href": "posts/2025/Slides/DDD_Slides.html#new-forward-path-and-corresponding-backward-ode",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.7 New Forward Path and corresponding backward ODE",
    "text": "2.7 New Forward Path and corresponding backward ODE\n\n\n\n\n\n\nDiffusion Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#forward-paths-and-corresponding-backward-odes",
    "href": "posts/2025/Slides/DDD_Slides.html#forward-paths-and-corresponding-backward-odes",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.7 Forward Paths and Corresponding Backward ODEs",
    "text": "2.7 Forward Paths and Corresponding Backward ODEs\n\n\n\n\n\n\nDiffusion Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation.\nFigures are from (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#new-forward-paths-and-corresponding-backward-odes",
    "href": "posts/2025/Slides/DDD_Slides.html#new-forward-paths-and-corresponding-backward-odes",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.7 New Forward Paths and Corresponding Backward ODEs",
    "text": "2.7 New Forward Paths and Corresponding Backward ODEs\n\n\n\n\n\n\nDiffusion Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation.\nFigures are from (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#summary-towards-straighter-paths",
    "href": "posts/2025/Slides/DDD_Slides.html#summary-towards-straighter-paths",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Summary: Towards Straighter Paths",
    "text": "Summary: Towards Straighter Paths\n\nSDE formulation enables faster ODE sampling\nODE sampling is possible to other choices of q_{\\textcolor{#E95420}{\\phi}} \\because\\quad Only 1d marginals matter (= Flow-based Modeling)\n\nLangevin path ← the Diffusion Model\nOptimal Transport path\nmore in discrete settings!\n\n\n\n\n\n\n\nLangevin Path\n\n\n\n\n\n\nODE Path w.r.t. Langevin Forward\n\n\n\n\n\n\nOT Path"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#masking-processes",
    "href": "posts/2025/Slides/DDD_Slides.html#masking-processes",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.1 Masking Processes",
    "text": "3.1 Masking Processes\n\nwith some rate R_t(\\texttt{mask}|x)&gt;0 of masking x\\ne\\texttt{mask}.\nThe reverse process is characterized by the rate \n\\textstyle\\hat{R}_t(x|y)=R_t(y|x)\\underbrace{\\frac{q^t(x)}{q^t(y)}.}_{\\text{learn this part using NN}}\n\n\n\nComparison of forward designs (Liang et al., 2025)\n\n\n\n\n\n\n\nForward process\nUniform\nMasking\n\n\n\n\nNumber of steps needed in backward process\n\\tilde{O}(d^2/\\epsilon)\n\\tilde{O}(d/\\epsilon)\n\n\n\n\n\n\nThis continuous-time approach starts with (Campbell et al., 2022), followed by (Sun et al., 2023) and culminates in (Lou et al., 2024)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#uniform-processes",
    "href": "posts/2025/Slides/DDD_Slides.html#uniform-processes",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.2 Uniform Processes",
    "text": "3.2 Uniform Processes"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#convergence-of-masking-processes",
    "href": "posts/2025/Slides/DDD_Slides.html#convergence-of-masking-processes",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.3 Convergence of Masking Processes",
    "text": "3.3 Convergence of Masking Processes"
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#from-path-measure-to-probability-flow",
    "href": "posts/2025/Slides/DDD_Slides.html#from-path-measure-to-probability-flow",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.7 From Path Measure to Probability Flow",
    "text": "2.7 From Path Measure to Probability Flow\n\n\n\n\n\n\nDiffusion Probability Flow\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Flow\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation.\nFigures are from (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#from-path-to-flow",
    "href": "posts/2025/Slides/DDD_Slides.html#from-path-to-flow",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2.7 From Path to Flow",
    "text": "2.7 From Path to Flow\n\n\n\n\n\n\nDiffusion Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation.\nFigures are from (Lipman et al., 2023)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#a-cutting-edge-method",
    "href": "posts/2025/Slides/DDD_Slides.html#a-cutting-edge-method",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.2 A Cutting-Edge Method",
    "text": "3.2 A Cutting-Edge Method\nConsider path measures \\{q_t\\}_{t=0}^T\\subset\\mathcal{P}(\\textcolor{#E95420}{\\mathcal{Z}}\\sqcup\\{\\texttt{mask}\\}) that satisfy \n\\text{linear interpolation:}\\qquad q_t(-|\\textcolor{#2780e3}{x})=(1-\\alpha_t)\\delta_{\\textcolor{#2780e3}{x}}(-)+\\alpha_t\\delta_{\\texttt{mask}}(-),\n  A backward sampling process is given by \n\\text{rate function:}\\qquad R_t(\\mathtt{mask},\\textcolor{#2780e3}{x})=\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\underbrace{p^t(\\textcolor{#2780e3}{x}|\\mathtt{mask}).}_{\\text{learn this part using NN}}\n  The predictor p^t(\\textcolor{#2780e3}{x}|\\mathtt{mask}) is learned by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\operatorname{E}\\bigg[\\operatorname{KL}\\bigg(p^{\\text{\\textcolor{#2780e3}{data}}},p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{X_t}|\\texttt{mask})\\bigg)\\bigg]\\,dt.\n\n\n\nThis approach culminates in (Shi et al., 2024), (S. Liu et al., 2025)."
  },
  {
    "objectID": "posts/2025/Slides/DDD_Slides.html#discrete-flow-matching",
    "href": "posts/2025/Slides/DDD_Slides.html#discrete-flow-matching",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3.2 Discrete Flow Matching",
    "text": "3.2 Discrete Flow Matching\nTargets a forward process \\{q_t\\}_{t=0}^T\\subset\\mathcal{P}(\\textcolor{#E95420}{\\mathcal{Z}}\\sqcup\\{\\texttt{mask}\\}) that satisfies \n\\text{linear interpolation:}\\qquad q_t(-|\\textcolor{#2780e3}{x})=(1-\\alpha_t)\\delta_{\\textcolor{#2780e3}{x}}(-)+\\alpha_t\\delta_{\\texttt{mask}}(-),\n  A backward sampling process is given by \n\\text{rate function:}\\qquad R_t(\\textcolor{#2780e3}{x}|\\mathtt{mask})=\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\underbrace{p^t(\\textcolor{#2780e3}{x}|\\mathtt{mask}).}_{\\text{learn this part using NN}}\n  The predictor p^t(\\textcolor{#2780e3}{x}|\\mathtt{mask}) is learned by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\operatorname{E}\\bigg[\\operatorname{KL}\\bigg(p^{\\text{\\textcolor{#2780e3}{data}}},p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{X_t}|\\texttt{mask})\\bigg)\\bigg]\\,dt.\n\nTheory lacks in this setting.\n\n\nThis approach culminates in (Shi et al., 2024), (S. Liu et al., 2025)."
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#todays-contents",
    "href": "posts/2025/Slides/DDD.html#todays-contents",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "",
    "text": "1 Mathematical Introduction (-2020)\n2 Developments in Continuous Diffusion Models (2021-2023)\n3 Discrete Diffusion Models (2024-)\n\n\n\n\nD3PM (Discrete Denoising Diffusion Probabilistic Model) example from (Ryu, 2024)"
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#developments-in-continuous-diffusion-models",
    "href": "posts/2025/Slides/DDD.html#developments-in-continuous-diffusion-models",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "2 Developments in Continuous Diffusion Models",
    "text": "2 Developments in Continuous Diffusion Models\n\n\nHistorical Development\n\n\n\n\n\n\n\nData Space \\textcolor{#2780e3}{\\mathcal{X}}\nContinuous\nDiscrete\n\n\n\n\nOrigin\n(Ho et al., 2020)\n(Austin et al., 2021)\n\n\nContinuous-time\n(Y. Song et al., 2021)\n(Campbell et al., 2022)\n\n\nScore-based\n(Y. Song et al., 2021)\n(Sun et al., 2023)\n\n\nFlow-based\n(Lipman et al., 2023)\n(Gat et al., 2024)\n\n\n\n\n\n2.1 Limit in T\\to\\infty leads to SDE formulation\n\n\n\n\n\n\n\n2.2 Score-based DDM in SDE formulation\n\n\n\n\n\n\nTheorem from (Anderson, 1982)\n\n\n\n(\\textcolor{#E95420}{Z}_t)_{t=0}^T and (\\textcolor{#2780e3}{X}_{T-t})_{t=0}^T have the same path measure:\n\n\\text{\\textcolor{#E95420}{Langevin diffusion}:}\\qquad\\qquad d\\textcolor{#E95420}{Z}_t=b_t(\\textcolor{#E95420}{Z}_t)\\,dt+dB_t\n \n\\text{\\textcolor{#2780e3}{Denoising diffusion}:}\\quad d\\textcolor{#2780e3}{X}_t=\\bigg(-b_{T-t}(\\textcolor{#2780e3}{X}_t)+\\underbrace{\\nabla\\log q^{T-t}(\\textcolor{#2780e3}{X}_t)}_{\\text{score function}}\\bigg)\\,dt+dB'_t\n\n\n\nLearning (\\textcolor{#2780e3}{X}_{t}) is equivalent to learning the score s_{\\textcolor{#2780e3}{\\theta}} by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\operatorname{E}\\bigg[\\bigg|\\nabla\\log q^t(\\textcolor{#E95420}{Z_t}|\\textcolor{#2780e3}{x})-s_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#E95420}{Z_t},t)\\bigg|^2\\bigg]\\,dt.\n\n\n\nThis is proposed by (Y. Song et al., 2021). \\mathcal{L}(\\textcolor{#2780e3}{\\theta}) is called the denoising score matching loss.\n\n\n2.3 ODE Sampling of Score-based DDM\n\n\\text{ODE:}\\qquad\\frac{d\\textcolor{#2780e3}{X}_t}{dt}=-b_t(\\textcolor{#2780e3}{X_t})+\\frac{1}{2}s_{\\textcolor{#2780e3}{\\theta}}^t(\\textcolor{#2780e3}{X_t})=:v^t_\\theta(\\textcolor{#2780e3}{X_t})\n\\tag{1} has the same 1d marginal distributions as \n\\text{\\textcolor{#2780e3}{Denoising diffusion} SDE:}\\quad d\\textcolor{#2780e3}{X_t}=\\bigg(-b_{t}(\\textcolor{#2780e3}{X_t})+s_{\\textcolor{#2780e3}{\\theta}}^{t}(\\textcolor{#2780e3}{X_t})\\bigg)\\,dt+dB_t.\n\n\n\n\n\n\n\n\n\n\n2.4 New Loss Enables New Sampling\n\n\n\n\n\n\n\n\n\n\nSDE sampling\nODE sampling\n\n\n\n\nForward Path\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n(q^t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x}))_{t=0}^T\n\n\nBackward Path\n(p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{x}|\\textcolor{#E95420}{z}))_{t=0}^T\n(?)\n\n\nSpeed\nSlow\nFast\n\n\nQuality\nHigh\nLow\n\n\n\n\nProblem: “ODE Solver applied to SDE path” doesn’t make sense.\n\n→ Explore other possibilities in the forward path\n\n\n\n(Karras et al., 2022) uses Heun’s 2nd order correction method to discretize the ODE.\nThe ODE parametrization by (J. Song et al., 2021) is favorable from its stable curvature.\nDiscretizing the SDE by adding extra noise results in higher quality in imagenet dataset.\nThe SDE approach seems to be more robust to the estimation error in the score (Cao et al., 2023).\n\n\n2.5 In Search of Better Forward Path\n\n\n\n\n\nDiscrete Time Markov Chain\n\n\n\n\n\n\nDiffusion Process\n\n\n\n\n\n\nPiecewise Deterministic\n\n\n\n\n\n\n2.6 Flow-based DDM: A Flexible Framework\nInstead of score \\nabla\\log q^t(\\textcolor{#E95420}{z}), we learn the vector field u satisfying \n(\\text{continuity equation})\\quad\\partial_tp^t+\\operatorname{div}(p^tu^t)=0.\n We learn u by a NN (t,x)\\mapsto v_{\\textcolor{#2780e3}{\\theta}}^t(x) with the loss \n\\text{Flow Matching Loss:}\\qquad\\mathcal{L}_{\\text{FM}}(\\textcolor{#2780e3}{\\theta})=\\int_0^T\\operatorname{E}\\bigg[\\bigg|v_{\\textcolor{#2780e3}{\\theta}}^t(X)-u^t(X)\\bigg|^2\\bigg]\\,dt.\n To generate a new sample, we let X_0\\sim p^0 flow along v_{\\textcolor{#2780e3}{\\theta^*}}^t.\n\n\nUsually, FM is understood as a scalable alternative to train CNFs (Chen et al., 2018). Being an alternative to score matching by learning directly the RHS of (1), this approach is called flow matching, independently proposed by (X. Liu et al., 2023), (Albergo and Vanden-Eijnden, 2023), (Lipman et al., 2023).\n\n\n2.7 From Path to Flow\n\n\n\n\n\n\n\n\nDiffusion Path\n\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(\\alpha_{1-t}\\textcolor{#2780e3}{x},(1-\\alpha_{1-t}^2)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\alpha_{1-t}}{1-\\alpha_{1-t}^2}(\\alpha_{1-t}\\textcolor{#E95420}{z}-\\textcolor{#2780e3}{x})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Transport Path\n\n\n\n\np_{\\textcolor{#2780e3}{\\theta}}^t(-|\\textcolor{#2780e3}{x})=\\operatorname{N}\\bigg(t\\textcolor{#2780e3}{x},(1-t)I_d\\bigg)\n corresponds to \nu_t(\\textcolor{#E95420}{z}|\\textcolor{#2780e3}{x})=\\frac{\\textcolor{#2780e3}{x}-\\textcolor{#E95420}{z}}{1-t}\n\n\n\n\n\n\n\n\n\n\nOT paths result in straight trajectries with constant speed, which is more suitable for stable generation.\nFigures are from (Lipman et al., 2023).\n\n\nSummary: Towards Straighter Paths\n\nSDE formulation enables faster ODE sampling\nODE sampling is possible to other choices of q_{\\textcolor{#E95420}{\\phi}} \\because\\quad Only 1d marginals matter (= Flow-based Modeling)\n\nLangevin path ← the Diffusion Model\nOptimal Transport path\nmore in discrete settings!\n\n\n\n\n\n\n\nLangevin Path\n\n\n\n\n\n\nODE Path w.r.t. Langevin Forward\n\n\n\n\n\n\nOT Path"
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#discrete-diffusion-models",
    "href": "posts/2025/Slides/DDD.html#discrete-diffusion-models",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "3 Discrete Diffusion Models",
    "text": "3 Discrete Diffusion Models\n\n\n\nBlock Diffusion proposed in (Arriola et al., 2025)\n\n\n\n3.1 Masking Processes\n\nwith some rate R_t(\\texttt{mask}|x)&gt;0 of masking x\\ne\\texttt{mask}.\nThe reverse process is characterized by the rate \n\\textstyle\\hat{R}_t(x|y)=R_t(y|x)\\underbrace{\\frac{q^t(x)}{q^t(y)}.}_{\\text{learn this part using NN}}\n\n\n\nComparison of forward designs (Liang et al., 2025)\n\n\n\n\n\n\n\nForward process\nUniform\nMasking\n\n\n\n\nNumber of steps needed in backward process\n\\tilde{O}(d^2/\\epsilon)\n\\tilde{O}(d/\\epsilon)\n\n\n\n\n\n\nThis continuous-time approach starts with (Campbell et al., 2022), followed by (Sun et al., 2023) and culminates in (Lou et al., 2024).\n\n\n3.2 Discrete Flow Matching\nTargets a forward process \\{q_t\\}_{t=0}^T\\subset\\mathcal{P}(\\textcolor{#E95420}{\\mathcal{Z}}\\sqcup\\{\\texttt{mask}\\}) that satisfies \n\\text{linear interpolation:}\\qquad q_t(-|\\textcolor{#2780e3}{x})=(1-\\alpha_t)\\delta_{\\textcolor{#2780e3}{x}}(-)+\\alpha_t\\delta_{\\texttt{mask}}(-),\n  A backward sampling process is given by \n\\text{rate function:}\\qquad R_t(\\textcolor{#2780e3}{x}|\\mathtt{mask})=\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\underbrace{p^t(\\textcolor{#2780e3}{x}|\\mathtt{mask}).}_{\\text{learn this part using NN}}\n  The predictor p^t(\\textcolor{#2780e3}{x}|\\mathtt{mask}) is learned by the loss \n\\mathcal{L}(\\textcolor{#2780e3}{\\theta})=\\int^T_0\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\operatorname{E}\\bigg[\\operatorname{KL}\\bigg(p^{\\text{\\textcolor{#2780e3}{data}}},p^t_{\\textcolor{#2780e3}{\\theta}}(\\textcolor{#2780e3}{X_t}|\\texttt{mask})\\bigg)\\bigg]\\,dt.\n\nTheory lacks in this setting.\n\n\nThis approach culminates in (Shi et al., 2024), (S. Liu et al., 2025)."
  },
  {
    "objectID": "posts/2025/Slides/DDD.html#appendix",
    "href": "posts/2025/Slides/DDD.html#appendix",
    "title": "Understanding Discrete Denoising Diffusion Models",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "static/Japanese.html#ベイズ計算",
    "href": "static/Japanese.html#ベイズ計算",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "ベイズ計算",
    "text": "ベイズ計算\n私は「ベイズ計算」と呼ばれる分野を専門に研究しています．\nベイズ推論は，統計学・逆問題・機械学習・情報科学・数値解析などの分野において確率論を応用するための指導原理を与える枠組みです．\nそこで私は，ベイズ推論に汎用的に使える計算アルゴリズムを開発することで，確率論の応用を広げることを目指しています．"
  },
  {
    "objectID": "static/Japanese.html#興味関心",
    "href": "static/Japanese.html#興味関心",
    "title": "司馬博文 | Hirofumi Shiba",
    "section": "興味・関心",
    "text": "興味・関心\n博士課程では，統計数理研究所 鎌谷研吾 先生の下，モンテカルロ法や拡散モデルなどのサンプリング手法の理論解析に取り組んでいます．\n統計と機械学習で用いられるアルゴリズムには幅広く興味があり，いずれも統計力学などの自然現象と深く関連していることが特に面白いと感じています．\nこれらが確率過程の収束や確率測度の空間上の幾何などの数理的枠組みで統一的に理解できた際には，学習や情報処理など広く統計的な現象に対する人類の理解を大きく深めてくれると信じています．\n研究成果の実装も大好きで，連続時間 MCMC を用いた事後分布サンプリングのための Julia パッケージ PDMPFlux.jl や，確率過程の統計推測のための R パッケージ YUIMA の開発にも取り組んでいます．"
  },
  {
    "objectID": "static/about.html",
    "href": "static/about.html",
    "title": "Hirofumi Shiba | 司馬博文",
    "section": "",
    "text": "Hirofumi is a Ph.D. student supervised by Kengo Kamatani at the Institute of Statistical Mathematics (ISM), Tokyo, Japan.\nMy research aims to deepen our understanding of these algorithms, especially Monte Carlo and optimization methods, through the lens of their (scaling / continuous-time) limit dynamics.\nBefore pursuing my Ph.D., I was trained in mathematics at the University of Tokyo, with an emphasis on Stochastic Processes and Functional Analysis."
  },
  {
    "objectID": "posts/2025/Slides/Bio_Slides.html#whats-wrong-with-diffusion",
    "href": "posts/2025/Slides/Bio_Slides.html#whats-wrong-with-diffusion",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "What’s Wrong with Diffusion?",
    "text": "What’s Wrong with Diffusion?\n\n\n\n\n\n\n\n\nEquilibrium ≒ Reversibility\n\n\nLangevin Diffusion represents a particle in a medium.\nE.g. A sugar particle in a coffee\n\n\n\n\n\nNature is not necessarily efficient.\nE.g. Would you wait until the sugar dissolves? To have a cup of coffee?\nIt’s difficult to simulate."
  },
  {
    "objectID": "posts/2025/Slides/Bio_Slides.html#pdmp-の何が良いのか",
    "href": "posts/2025/Slides/Bio_Slides.html#pdmp-の何が良いのか",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "PDMP の何が良いのか？",
    "text": "PDMP の何が良いのか？\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 収束が速い (Diaconis, 2013), (Andrieu and Livingstone, 2021)\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない"
  },
  {
    "objectID": "posts/2025/Slides/Bio_Slides.html#whats-new-about-pdmp",
    "href": "posts/2025/Slides/Bio_Slides.html#whats-new-about-pdmp",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "What’s New about PDMP?",
    "text": "What’s New about PDMP?\n\n\n\n\n\n\n\n\n物理：非可逆なダイナミクス\n\n\n\n「棄却」されるまで一直線に猛進\n≒ スプーンで混ぜる行為の模倣\n人工的な対称性（例：詳細釣り合い）がない\n 収束が速い (Diaconis, 2013), (Andrieu and Livingstone, 2021)\n\n\n\n\n\n\n\n\n\nアルゴリズム：棄却フリー\n\n\n\nPDMP は数値誤差なくシミュレートできる\n Metropolis-Hastings の棄却-採択の枠組みが要らない"
  },
  {
    "objectID": "posts/2025/Slides/Bio_Slides.html#whats-new-in-pdmp",
    "href": "posts/2025/Slides/Bio_Slides.html#whats-new-in-pdmp",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "What’s New in PDMP?",
    "text": "What’s New in PDMP?\n\n\n\n\n\n\n\n\nIrreversibility & Acceleration\n\n\n\nBallistic motion, up until a turn\nE.g. Stirring coffee with a spoon\nNo artificial symmetry (e.g. detailed balance)\n Fast convergence & reduced computational cost\n\n\n\n\n\nAll with a new strategy of simulation, which seems to be very efficient (ongoing research)"
  },
  {
    "objectID": "posts/2025/Posters/Acceleration.html",
    "href": "posts/2025/Posters/Acceleration.html",
    "title": "サンプリングにおける加速と安定性",
    "section": "",
    "text": "PDMP とはマルコフ連鎖，拡散過程に続いて，近年活発にモンテカルロ法に利用されつつある連続時間マルコフ過程のクラスである．より速く収束するサンプラーが構成しやすいこと，モンテカルロ推定量にバイアスを導入しないようなサブサンプリング（バッチ実行）が可能であることから，近年特に統計・機械学習の分野でも注目が高まっている．\n本ポスターではまず，サンプリングと最適化の関係を図によってまとめる．目的関数を \\(\\newcommand{\\R}{\\mathbb{R}}U:\\R^d\\to\\R\\) で表すと，その勾配流 \\(\\dot{x}=-\\nabla U(x)\\) に確率的な揺らぎを加えた過程である Langevin 拡散過程 \\[\ndX_t=-\\nabla U(X_t)\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t\n\\] による確率分布 \\(\\pi(x)\\propto e^{-U(x)}\\) からのサンプリングは，\\(\\mathcal{P}(\\R^d)\\) 上の勾配流の離散化だと見れる (Bernton, 2018), (Durmus ほか, 2019), (Wibisono, 2018), (Zhang と Nemeth, 2024)．このときの目的関数は KL 乖離度 \\(F(\\rho):=\\operatorname{KL}(\\rho,e^{-U})\\) である．\n近年 Langevin 拡散過程のような古典的なサンプラーを改善する試みが MCMC の理論・手法研究で進んでいる．キーワードの一つが 非可逆性 である．Langevin 拡散を非可逆化した代表的なダイナミクスには underdamped Langevin diffusion というものが存在する： \\[\n\\begin{cases}\ndX_t=V_t\\,dt,\\\\\ndV_t=-(\\nabla U(X_t)+\\gamma V_t)\\,dt+\\sqrt{2\\gamma}\\,dB_t.\n\\end{cases}\n\\tag{1}\\]\nこの underdamped Langevin diffusion の離散化によるサンプラーには GHMC (Generalized Hamiltonian Monte Carlo) (Horowitz, 1991) が存在し，現在の（ほとんど）SOTA である HMC を大きく改善する．が，シミュレーション技術に問題があり，アルゴリズムとしての tuning-free 化に苦しんでいる (Hoffman と Sountsov, 2022)．\nUnderdampled Langevin dynamics (1) は実は (Polyak, 1964) による Heavy-Ball 加速勾配法の連続極限と \\(dB_t\\) の項を除いて一致する．すなわち，MCMC の界隈で理解されている「非可逆化によるサンプラーの加速」は，最適化の言葉では加速法として理解できるのである．この (Ma ほか, 2021) の観察を紹介する．\n非可逆なダイナミクスを持つ MCMC サンプラーとして期待されている手法は GHMC だけでない．PDMP (Piecewise Deterministic Markov Process) というクラスの Markov 過程に基づく，新種の MCMC 手法が近年提案されている (Fearnhead ほか, 2018)．\n本ポスターでは最後に，「 \\(\\delta_x\\) 部分を持った非絶対連続確率分布からも正確なサンプリングが可能」という PDMP に基づくモンテカルロ法の第３の美点に焦点を当てる． PDMP 法と従来法との挙動の違いを調べるために，スパイク幅が \\(0\\) に収束する極限という従来考えられなかったレジームを導入する．\nこのレジームにおいて，従来法はスパイクの検出に失敗し，誤った分布からサンプリングを行ってしまう． 一方で PDMP 法はスパイクの台への到達確率が \\(0\\) でない限り，正しい分布からサンプリングを行うことができる．\n加えて極限もまた別の PDMP となり，これを直接シミュレートすることで，\\(\\delta\\) 部分を持った非絶対連続分布からの効率的なサンプリングが可能になる． なお，この極限で得られるサンプラーは Sticky PDMP (Bierkens ほか, 2023) と一致する．"
  },
  {
    "objectID": "posts/2025/Posters/Acceleration.html#連続最適化および関連分野に関する夏季学校",
    "href": "posts/2025/Posters/Acceleration.html#連続最適化および関連分野に関する夏季学校",
    "title": "サンプリングにおける加速と安定性",
    "section": "連続最適化および関連分野に関する夏季学校",
    "text": "連続最適化および関連分野に関する夏季学校\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nAug. 28th - 30th, 2025\n統数研２階大会議室\n\n\n\n\n\n\n\n画像をクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/Acceleration.html#参考ページ集",
    "href": "posts/2025/Posters/Acceleration.html#参考ページ集",
    "title": "サンプリングにおける加速と安定性",
    "section": "参考ページ集",
    "text": "参考ページ集\nPDMP とそのシミュレーションに関しては，次のスライドにわかりやすく解説されています：\n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n一致なし\n\n\n図は全て発表者開発のパッケージ PDMPFlux.jl によるものです．\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n一致なし\n\n\nSticky PDMP (Bierkens ほか, 2023) に関するさらに詳しい内容，またはベイズ変数選択一般については，次の記事にまとめています：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#distribution-key-of-new-science",
    "href": "posts/2025/Slides/Bio.html#distribution-key-of-new-science",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "2 Distribution: Key of New Science",
    "text": "2 Distribution: Key of New Science\n\n\n\n\n\nOne thing that all the keywords and fields have in common is that they use the notion of probability distribution as a key concept.\nIn the slide, I show four examples where the notion of distribution is used as a key building block,\n\nDiffusion Model, where a transformation between known and a unknown distribution is seeked,\nBoltzmann-Shannon entropy, which is one of the most important concepts to understand different distributions, identified in the field of statistical mechanics, but found a new application in the field of information theory,\nBoltmann-Gibbs distribution, which is a distribution that is used to describe the equilibrium state of a system, but found a new interpretation in the field of Bayesian inference and machine learning,\nUncertainty Quantification, which is a characteristic of Bayesian approach in statistics, but recently rediscovered as a key quality of successful methods in the field of machine learning."
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#a-computational-reinterpretation",
    "href": "posts/2025/Slides/Bio.html#a-computational-reinterpretation",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "3 A Computational Reinterpretation",
    "text": "3 A Computational Reinterpretation\n\n\n\n\n\nMost of the traditional approaches, including conjugate Bayesian models and density estimation, concentrate on aquiring the function value p(x) of the probability density function.\nHowever, after the rise of the computer, ‘learn to generate from p’ approach has been found to be more powerful and practical, especially through the development of a class of Monte Carlo methods called Markov Chain Monte Carlo (MCMC).\nThe mere fact that these two approach (learn the function p, and learn to generate) are indeed different is already surprising, which is only realized recently by the invention of Generative Modeling."
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#development-in-monte-carlo-methods",
    "href": "posts/2025/Slides/Bio.html#development-in-monte-carlo-methods",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "4 Development in Monte Carlo Methods",
    "text": "4 Development in Monte Carlo Methods\n\n\n\n\n\nMarkov Chain\n\n\n\n\n\n\nDiffusion\n\n\n\n\n\n\nJump Process\n\n\n\n\nStarting from this slide, I will talk about my research more specifically, which is about MCMC methods mentioned above.\nIn this slide, there are three stochastic processes with distinct characteristics. They are found and utilized in the MCMC methods, from the left to the right, that is, Markov Chain (discrete time) to Diffusion Process (continuous time), and the last one is a Jump Process (still in continuous time).\n\n4.1 What’s Wrong with Diffusion?\n\n\n\n\n\n\n\n\n\n\nEquilibrium ≒ Reversibility\n\n\n\nLangevin Diffusion represents a particle in a medium.\nE.g. A sugar particle in a coffee\n\n\n\nNature is not necessarily efficient.\nE.g. Would you wait until the sugar dissolves? To have a cup of coffee?\nIt’s difficult to simulate.\n\n\n\nThe Langevin diffusion constitutes one of the most effective methods in MCMC’s history, but it is not free of shortcomings.\nThere are sereval reasons technically, but from the high level, the choice of the process itself is not optimal.\nAn intuition behind this poor performance is captured by the notion of reversibility. The reversibility of the process typically implies slow exploration of the space, as the process tends to go back to where it came from.\n\n\n4.2 What’s New in PDMP?\n\n\n\n\n\n\n\n\n\n\nIrreversibility & Acceleration\n\n\n\n\nBallistic motion, up until a turn\nE.g. Stirring coffee with a spoon\nNo artificial symmetry (e.g. detailed balance)\n Fast convergence & reduced computational cost\n\n\n\nAll with a new strategy of simulation, which seems to be very efficient (ongoing research)\n\n\nTherefore, a new class of processes, called Piecewise Deterministic Markov Process (PDMP), has been proposed as an alternative to the Langevin diffusion.\nAs can be easily seen, PDMPs are characterized by the dominating deterministic move, which goes straight through the space until the point where it changes the direction randomly."
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#pdmp-package",
    "href": "posts/2025/Slides/Bio.html#pdmp-package",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "5 PDMP Package",
    "text": "5 PDMP Package\nPiecewise Deterministic Markov Process\n\n\nPython \npip install pdmp-jax\n\nJulia \n] add PDMPFlux\n\n\nMy research mainly concerns the theoretical aspects of PDMPs. The most obvious question is whether PDMPs are really efficient seen as a Monte Carlo algorithm.\nI aim to conduct convergence analysis and complexity analysis of PDMPs to answer this question, and further provide insights into PDMP and related statistical computational algorithms.\nAlongside theoretical inquiries, I have developed a package to simulate PDMPs for sampling applications, because there was not a fixed implementation that works for general targets and general PDMPs.\nThis package will accelerate my research and comparison between different implementations, which may be one of the most interests practically."
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#bringing-science-back",
    "href": "posts/2025/Slides/Bio.html#bringing-science-back",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "6 Bringing Science Back",
    "text": "6 Bringing Science Back\n\n\n\n\n\nThis line of research regarding PDMPs might occupy the green area indicated in the slide, bridging Statistical Physics with (Bayesian) Statistics, and a part of AI and Machine Learning.\nHowever, the notion of reversibility has recently found to be importance in generative modeling as well.\nThus, the Stochastic Process approach I take and the notion of probability distribution are expected to have more and more important roles in the four fields I mentioned above. Indeed, Stoachstic Process is already used as a foundation of stochastic thermodynamics, which is a new field of physics.\nThe recent development in AI and Machine Learning is sometimes said to be ‘launched bottom up’, which means that the success of the methods are not necessarily based on the theoretical insights, but rather on the engineering efforts.\nMy aim is to bring science back to the field of machine learning and statistics, and to provide a solid foundation for the future collaborative development, just as how we acquired the technology we use today."
  },
  {
    "objectID": "posts/2025/Posters/Acceleration.html#発表概要",
    "href": "posts/2025/Posters/Acceleration.html#発表概要",
    "title": "サンプリングにおける加速と安定性",
    "section": "",
    "text": "PDMP とはマルコフ連鎖，拡散過程に続いて，近年活発にモンテカルロ法に利用されつつある連続時間マルコフ過程のクラスである．より速く収束するサンプラーが構成しやすいこと，モンテカルロ推定量にバイアスを導入しないようなサブサンプリング（バッチ実行）が可能であることから，近年特に統計・機械学習の分野でも注目が高まっている．\n本ポスターではまず，サンプリングと最適化の関係を図によってまとめる．目的関数を \\(\\newcommand{\\R}{\\mathbb{R}}U:\\R^d\\to\\R\\) で表すと，その勾配流 \\(\\dot{x}=-\\nabla U(x)\\) に確率的な揺らぎを加えた過程である Langevin 拡散過程 \\[\ndX_t=-\\nabla U(X_t)\\,dt+\\sqrt{2\\beta^{-1}}\\,dB_t\n\\] による確率分布 \\(\\pi(x)\\propto e^{-U(x)}\\) からのサンプリングは，\\(\\mathcal{P}(\\R^d)\\) 上の勾配流の離散化だと見れる (Bernton, 2018), (Durmus ほか, 2019), (Wibisono, 2018), (Zhang と Nemeth, 2024)．このときの目的関数は KL 乖離度 \\(F(\\rho):=\\operatorname{KL}(\\rho,e^{-U})\\) である．\n近年 Langevin 拡散過程のような古典的なサンプラーを改善する試みが MCMC の理論・手法研究で進んでいる．キーワードの一つが 非可逆性 である．Langevin 拡散を非可逆化した代表的なダイナミクスには underdamped Langevin diffusion というものが存在する： \\[\n\\begin{cases}\ndX_t=V_t\\,dt,\\\\\ndV_t=-(\\nabla U(X_t)+\\gamma V_t)\\,dt+\\sqrt{2\\gamma}\\,dB_t.\n\\end{cases}\n\\tag{1}\\]\nこの underdamped Langevin diffusion の離散化によるサンプラーには GHMC (Generalized Hamiltonian Monte Carlo) (Horowitz, 1991) が存在し，現在の（ほとんど）SOTA である HMC を大きく改善する．が，シミュレーション技術に問題があり，アルゴリズムとしての tuning-free 化に苦しんでいる (Hoffman と Sountsov, 2022)．\nUnderdampled Langevin dynamics (1) は実は (Polyak, 1964) による Heavy-Ball 加速勾配法の連続極限と \\(dB_t\\) の項を除いて一致する．すなわち，MCMC の界隈で理解されている「非可逆化によるサンプラーの加速」は，最適化の言葉では加速法として理解できるのである．この (Ma ほか, 2021) の観察を紹介する．\n非可逆なダイナミクスを持つ MCMC サンプラーとして期待されている手法は GHMC だけでない．PDMP (Piecewise Deterministic Markov Process) というクラスの Markov 過程に基づく，新種の MCMC 手法が近年提案されている (Fearnhead ほか, 2018)．\n本ポスターでは最後に，「 \\(\\delta_x\\) 部分を持った非絶対連続確率分布からも正確なサンプリングが可能」という PDMP に基づくモンテカルロ法の第３の美点に焦点を当てる． PDMP 法と従来法との挙動の違いを調べるために，スパイク幅が \\(0\\) に収束する極限という従来考えられなかったレジームを導入する．\nこのレジームにおいて，従来法はスパイクの検出に失敗し，誤った分布からサンプリングを行ってしまう． 一方で PDMP 法はスパイクの台への到達確率が \\(0\\) でない限り，正しい分布からサンプリングを行うことができる．\n加えて極限もまた別の PDMP となり，これを直接シミュレートすることで，\\(\\delta\\) 部分を持った非絶対連続分布からの効率的なサンプリングが可能になる． なお，この極限で得られるサンプラーは Sticky PDMP (Bierkens ほか, 2023) と一致する．"
  },
  {
    "objectID": "posts/2024/Particles/PF.html",
    "href": "posts/2024/Particles/PF.html",
    "title": "A Recent Development of Particle Methods",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nThe following is a detailed version of the poster presented at MLSS2024, S3-41, March 8 (Fri) 18:00-19:30."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#what-is-particle-filter",
    "href": "posts/2024/Particles/PF.html#what-is-particle-filter",
    "title": "A Recent Development of Particle Methods",
    "section": "1 What Is Particle Filter?",
    "text": "1 What Is Particle Filter?\nParticle filters, also known as Sequential Monte Carlo methods (SMCs), were invented in (Kitagawa, 1993) and (Gordon ほか, 1993) independently as an simulation-based algorithm which performs filtering in non-Gaussian and non-linear state space models, overcoming the weeknesses of then-standard Kalman-based filtering methods.1\nIn particle-based approaches, a filtering distribution is approximated by a cloud of weighted samples, hence giving rise to the term ‘particle filter’. The samples are propagated to approximate the next distribution, leading to efficient sequential estimation in dynamic settings.\nRecent developments have highlithgted the capability of particle filters as general-purpose samplers, extending their applicability beyond the traditional realm of temporal graphical models to a broader range of statistical inference problems. This versatility has earned them the alternative name ‘SMC’, a term reminiscent of ‘MCMC’. This poster trys to be another contribution in this direction."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#mcmc-vs.-smc",
    "href": "posts/2024/Particles/PF.html#mcmc-vs.-smc",
    "title": "A Recent Development of Particle Methods",
    "section": "2 MCMC vs. SMC",
    "text": "2 MCMC vs. SMC\nPDMPs (Piecewise Deterministic Markov Processes) (Davis, 1984), a type of continuous-time Markov processes with jumps as their only random components, play a complementary role to diffusion processes in stochastic modelling.2\nIn (Peters と de With, 2012), a PDMP was identified through the continuous limit of the MCMC, Metropolis-Hastings algorithm. The PDMP was further investigated and termed Bouncy Particle Sampler (BPS) in (Bouchard-Côté ほか, 2018).\n\n\nコード\nlibrary(RZigZag)\nlibrary(ggplot2)\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nx0 &lt;- c(0,0)\nresult &lt;- BPSGaussian(V, mu, n_iter = 100, x0 = x0)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Bouncy Particle Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nAlso, other types of continuous-time MCMC algorithms have been developed, such as the Zig-Zag sampler (Bierkens ほか, 2019):\n\n\nコード\nV &lt;- matrix(c(3,1,1,3),nrow=2)\nmu &lt;- c(2,2)\nresult &lt;- ZigZagGaussian(V, mu, 100)\nggplot() +\n   geom_path(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   geom_point(aes(x=result$Positions[1,], y=result$Positions[2,]), color=\"#2F579C\") +\n   labs(x=\"\", y=\"\", title=\"Zig-Zag Sampler\") +\n   theme_void() +\n   theme(text=element_text(size=12), axis.title=element_text(color=\"#2F579C\"), plot.title=element_text(color=\"#2F579C\"))\n\n\n\n\n\n\n\n\n\nEnpirical evidence suggests that continuous-time MCMCs are more efficient than their discrete-time counterparts.\n\nInterestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as they need only access a small sub-set of data points at each iteration, and yet are still guaranteed to target the true posterior distribution. (Fearnhead ほか, 2018)"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#inquiry-for-continuous-time-smc",
    "href": "posts/2024/Particles/PF.html#inquiry-for-continuous-time-smc",
    "title": "A Recent Development of Particle Methods",
    "section": "3 Inquiry for Continuous-time SMC",
    "text": "3 Inquiry for Continuous-time SMC\nDespite the success of continuous-time MCMC, the continuous-time limit of SMC has not been fully explored. The continuous-time limit of SMC is expected to be a jump process, which is similar to PDMP, but is more diffusion-like.\nMCMC has now taken a step ahead; it is time for SMC to explore its continuous-time limit!\n\n3.1 A Generic Particle Filter: An Algorithmic Description\n\n\n\nProcedure of a generic step of a particle filter at time \\(t\\)\n\n\n\nResampling Step\nParticles with high weights are duplicated, and those with the lowest weights are discarded.\nMovement Step\nSubsequently, a MCMC move is executed from the resampled particles.\n\nThe resampling step is the key difference from sequential importance sampling methods. Particle filters incorporate a resampling step to occasionally reset the weights of the samples, while maintaining the overall distribution they represent, in order to prevent the effective number of particles participating in the estimation from becoming too small–a situation also called weight degeneracy.\n\n\n3.2 A Necessary Condition: Resampling Stability\nIn order to have a time-step \\(\\Delta\\to0\\) limit, resampling events must occur with (at most linearly) decreasing frequency as \\(\\Delta\\to0\\).\nOnly the most efficient resampling schemes satisfy this property.\n\n\n\nRoot mean squared errors of marginal likelihood estimates (Chopin ほか, 2022)"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#the-continuous-time-limit-process",
    "href": "posts/2024/Particles/PF.html#the-continuous-time-limit-process",
    "title": "A Recent Development of Particle Methods",
    "section": "4 The Continuous-time Limit Process",
    "text": "4 The Continuous-time Limit Process\nThe continuous-time limit process, if it exists, is characterized by a Feller-Dynkin process, whose infinitesimal generator is given by:\n\\[\n\\begin{align*}\n    \\mathcal{L}f(x)&=\\sum_{n=1}^N\\sum_{i=1}^db_i(x^n)\\frac{\\partial f}{\\partial x^n_i}(x)\\\\\n    &\\;\\;+\\sum_{n=1}^N\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x^n)\\frac{\\partial ^2f}{\\partial x^n_i\\partial x^n_j}(x)\\\\\n    &\\;\\;+\\sum_{a\\ne1:N}\\overline{\\iota}(V(x),a)\\biggr(f(x^{a(1:N)})-f(x^{1:N})\\biggl)\n\\end{align*}\n\\] \\[\n(f\\in C_c^2(\\mathbb{R}^{dN}),x\\in\\mathbb{R}^{dN},x^n\\in\\mathbb{R}^d)\n\\]\nwhen the latent process \\((X_t)\\) is an Itô process given by the generator:\n\\[\n\\begin{align*}\n    Lf(x)&=\\sum_{i=1}^db_i(x)\\frac{\\partial f}{\\partial x_i}(x)\\\\\n    &\\;\\;+\\frac{1}{2}\\sum_{i,j=1}^d(\\sigma\\sigma^\\top)_{ij}(x)\\frac{\\partial ^2f}{\\partial x_i\\partial x_j}(x)\n\\end{align*}\n\\] \\[\n(f\\in C_c^2(\\mathbb{R}^d),x\\in\\mathbb{R}^d)\n\\]\nFor details, please consult (Chopin ほか, 2022, p. 3206), Theorem 19."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#conclusions",
    "href": "posts/2024/Particles/PF.html#conclusions",
    "title": "A Recent Development of Particle Methods",
    "section": "5 Conclusions",
    "text": "5 Conclusions\n\n\n\n\n\n\nSummaries\n\n\n\nSMC with efficient resampling schemes possess a continuous-time limit \\(\\Delta\\to0\\), which turns out to be a Feller-Dynkin process, a diffusion process with jumps, when \\((X_t)\\) is a diffusion."
  },
  {
    "objectID": "posts/2024/Particles/PF.html#forthcoming-research",
    "href": "posts/2024/Particles/PF.html#forthcoming-research",
    "title": "A Recent Development of Particle Methods",
    "section": "6 Forthcoming Research",
    "text": "6 Forthcoming Research\n\n\n\n\n\n\nUltimate Purpose\n\n\n\nHow can we leverage the knowledge of the continuous-time limit process to design efficient Sequential Monte Carlo (SMC) samplers capable of sampling from posterior distributions of diffusions?\n\n\n\nWhat are the properties of this limit jump process, and how do they change with modifications to the underlying latent process?\nHow does the timing of resampling affect overall efficiency? Can insights be gained from the perspective of continuous-time limits?\nDoes the continuous-time limit process improve SMC efficiency when used for particle propagation?"
  },
  {
    "objectID": "posts/2024/Particles/PF.html#footnotes",
    "href": "posts/2024/Particles/PF.html#footnotes",
    "title": "A Recent Development of Particle Methods",
    "section": "脚注",
    "text": "脚注\n\n\nGood references are (Murphy, 2023) Chapter 13, and (Theodoridis, 2020, p. 881) Section 17.4.↩︎\n(Fearnhead ほか, 2018) is a great introduction to this topic.↩︎"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html",
    "href": "posts/2024/Probability/likelihood.html",
    "title": "Likelihood of Hierarchical Models",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#problem",
    "href": "posts/2024/Probability/likelihood.html#problem",
    "title": "Likelihood of Hierarchical Models",
    "section": "1 Problem",
    "text": "1 Problem\n\n1.1 Starting Point\nAs a starting point, consider a simple univariate linear regression model with 3 parameters, i.e., \\(\\alpha,\\beta,\\sigma^2\\): \\[\n\\operatorname{E}[Y|X]=\\alpha+\\beta X+\\epsilon,\\qquad\\epsilon\\sim N(0,\\sigma^2).\n\\tag{1}\\] Since (1) is just another expression for \\(Y|X\\sim N(\\alpha+\\beta X,\\sigma^2)\\), the likelihood function is given by \\[\np(y|\\alpha,\\beta,\\sigma)=\\phi(y|\\alpha+\\beta x,\\sigma^2),\n\\] given data \\(x\\in\\mathbb{R}\\), where \\(\\phi(-|\\mu,\\sigma^2)\\) represents the Gaussian density for \\(N(\\mu,\\sigma^2)\\).\n\n\n1.2 Hierarchical Model\nTo make our model (1) more realistic, let’s say, we would like to model variation in the intercept \\(\\alpha\\) by imposing another regression structure (you might call it super-population structure): \\[\n\\alpha=\\mu_\\alpha+\\epsilon_\\alpha,\\qquad\\epsilon_\\alpha\\sim N(0,\\sigma_\\alpha^2).\n\\tag{2}\\]\nNow, what does the likelihood function \\(p(y|\\mu_\\alpha,\\sigma_\\alpha,\\beta,\\sigma)\\) of this model look like?\nThe answer is normal likelihood.\n\n\n1.3 Likelihood Calculation\nFirst, rewrite (1) and (2) as \\[\nY|X,\\alpha\\sim N(\\alpha+\\beta X,\\sigma^2),\n\\] \\[\n\\alpha|\\mu_\\alpha,\\sigma_\\alpha\\sim N(\\mu_\\alpha,\\sigma_\\alpha^2).\n\\]\nUsing the formula (note this is not always true, see セクション 2.4) \\[\np(y|\\mu_\\alpha,\\sigma_\\alpha,\\beta,\\sigma)=\\int_\\mathbb{R}p(y|\\alpha,\\beta,\\sigma)p(\\alpha|\\mu_\\alpha,\\sigma_\\alpha)\\,d\\alpha,\n\\] we get a normal density \\[\np(y|\\mu_\\alpha,\\sigma_\\alpha,\\beta,\\sigma)=\\phi(y|\\widetilde{\\mu},\\widetilde{\\sigma}^2),\n\\] where \\[\n\\widetilde{\\mu}(\\beta,\\mu_\\alpha):=\\beta x+\\mu_\\alpha,\n\\] \\[\n\\widetilde{\\sigma}^2(\\sigma,\\sigma_\\alpha):=\\sigma^2+\\sigma_\\alpha^2.\n\\]\nThis also follows from the reproducing property of normal distribution: \\[\nN(\\beta X,\\sigma^2)*N(\\mu_\\alpha,\\sigma_\\alpha^2)=N(\\beta X+\\mu_\\alpha,\\sigma^2+\\sigma_\\alpha^2).\n\\]"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#background-in-probability",
    "href": "posts/2024/Probability/likelihood.html#background-in-probability",
    "title": "Likelihood of Hierarchical Models",
    "section": "2 Background in Probability",
    "text": "2 Background in Probability\n\n2.1 Core Proposition\nAbstracting away by setting \\[\n\\theta:=(\\beta,\\sigma),\\qquad\\varphi:=(\\mu_\\alpha,\\sigma_\\alpha),\n\\] the core identity was the following: \\[\np(y|\\theta,\\varphi)=\\int p(y|\\theta,\\alpha)p(\\alpha|\\varphi)\\,d\\alpha.\n\\tag{3}\\]\nHaving such a good notation, this formula might look obvious for stats people, but how actually do we prove it?\n\n\n2.2 Conditional Probability\nEssentially, 式 3 comes from the tower property of conditional expectation:\n\n\n\n\n\n\nTower Property\n\n\n\nFor sigma algebras \\(\\mathcal{G}_1,\\mathcal{G}_2\\), \\[\n\\mathcal{G}_1\\subset\\mathcal{G}_2\\quad\\Rightarrow\\quad\\operatorname{E}[X|\\mathcal{G}_1]=\\operatorname{E}[\\operatorname{E}[X|\\mathcal{G}_2]|\\mathcal{G}_1]\\quad\\operatorname{P}\\text{-a.s.}\n\\] for any random variable \\(X\\) on \\((\\Omega,\\mathcal{F},\\operatorname{P})\\), \\(\\mathcal{G}_1,\\mathcal{G}_2\\subset\\mathcal{F}\\).\n\n\nLet’s see how we apply this tower property to our problem:\n\n\n\n\n\n\n(Th’m 8.15 Kallenberg, 2021, p. 174)\n\n\n\nFor any \\(A\\in\\mathcal{F}\\), \\[\n\\operatorname{P}[A|X]=\\operatorname{E}\\biggl[\\operatorname{P}[A|X,Y]\\,\\bigg|\\,X\\biggr]\\quad\\operatorname{P}\\text{-a.s.}\n\\tag{4}\\]\n\n\nConcerning the definition of conditional probability, we first have the definition of conditional expectation, through which we define conditional probability as1 \\[\n\\operatorname{P}[A|X]:=\\operatorname{E}[1_A|X].\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\operatorname{E}[1_A|X]=\\operatorname{E}\\biggl[\\operatorname{E}[1_A|X,Y]\\,\\bigg|\\,X\\biggr]\\quad\\operatorname{P}\\text{-a.s.}\n\\] holds from tower property, noting that \\[\n\\sigma(X)\\subset\\sigma(X,Y).\n\\]\n\n\n\n\n\n2.3 Conditional Density\nThe last piece we need is the definition of conditional density.\n\n\n\n\n\n\nDef (Conditional Density)\n\n\n\nFor absolutely continuous random variables \\(X,Y\\) on \\(\\mathbb{R}^d\\), we define the conditional density of \\(Y\\) given \\(X\\) as \\[\np(y|x):=\\frac{p(x,y)}{p(x)}1_{\\left\\{p(x)&gt;0\\right\\}},\n\\] where \\(p(x,y)\\) is the joint density of \\((X,Y)\\) and \\(p(x)\\) is the marginal density of \\(X\\).\n\n\n\n\n\n\n\n\nCharacterization of Conditional Density\n\n\n\n\\[\n\\operatorname{E}[Y|X=x]=\\int_{\\mathbb{R}^d}y\\,p(y|x)\\,dy\\quad\\operatorname{P}^X\\text{-a.s.}\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSince conditional expectation is unique up to \\(\\operatorname{P}\\)-null sets, we’ll check the RHS (right-hand side) satisfies the conditions of conditional expectation.\n\nBy Fubini’s theorem, the RHS is a measurable function of \\(x\\) and is \\(\\operatorname{P}^X\\)-integrable.\nFor all \\(\\sigma(X)\\)-measurable set \\(A\\in\\sigma(X)\\),\n\\[\n\\int_A\\int_{\\mathbb{R}^d}y\\,p(y|x)\\,dy\\,p(x)\\,dx=\\int_{A\\times\\mathbb{R}^d}y\\,p(x,y)\\,dxdy=\\operatorname{E}[Y1_A].\n\\]\n\n\n\n\nPlugging in this characterization into (4), we get \\[\n\\operatorname{P}[A|X=x]=\\int_{\\mathbb{R}^d}\\operatorname{P}[A|X=x,Y=y]\\,p(y|x)\\,dy.\n\\] Denoting the density of \\(\\operatorname{P}[A|X=x]\\) as \\(p(a|x)\\), we have \\[\np(a|x)=\\int_{\\mathbb{R}^d}p(a|x,y)\\,p(y|x)\\,dy.\n\\]\n\n\n2.4 Last Piece\nReterning to the notation in (3), we have \\[\np(y|\\theta,\\varphi)=\\int p(y|\\theta,\\varphi,\\alpha)p(\\alpha|\\varphi)\\,d\\alpha.\n\\]\nThis is slightly different from (3) in that there is \\(p(y|\\theta,\\varphi,\\alpha)\\) instead of \\(p(y|\\theta,\\alpha)\\).\nSo the last piece we need is the modeling assumption in the regression setting of (1), which is conditional independence between \\(Y\\) and the hyperparameters \\((\\mu_\\alpha,\\sigma_\\alpha)\\) given \\(\\alpha\\): \\[\nY\\perp\\!\\!\\!\\perp\\varphi\\mid\\alpha.\n\\]\nUnder this assumption, we have2 \\[\np(y|\\theta,\\alpha,\\varphi)=p(y|\\theta,\\alpha).\n\\]\nIn the regression setting of (1), \\[\np(y|\\beta,\\sigma,\\alpha,\\mu_\\alpha,\\sigma_\\alpha)=p(y|\\beta,\\sigma,\\alpha).\n\\]\n\n\n\n\n\n\nWe implicitly assumed \\[\np(y|\\beta,\\sigma,\\alpha,\\mu_\\alpha,\\sigma_\\alpha)=p(y|\\beta,\\sigma,\\alpha),\n\\] since it was reasonable to assume the conditional independence between \\(Y\\) and the hyperparameters \\((\\mu_\\alpha,\\sigma_\\alpha)\\) given \\(\\alpha\\): \\[\nY\\perp\\!\\!\\!\\perp(\\mu_\\alpha,\\sigma_\\alpha)\\mid\\alpha.\n\\]"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#real-world-example-ideal-point-model",
    "href": "posts/2024/Probability/likelihood.html#real-world-example-ideal-point-model",
    "title": "Likelihood of Hierarchical Models",
    "section": "3 Real-world Example: Ideal Point Model",
    "text": "3 Real-world Example: Ideal Point Model\n\n3.1 Specification\nLet’s consider the following hierarchical model with 6 parameters, i.e., \\(\\alpha,\\beta,\\gamma,\\delta,\\sigma^2\\) and \\(X\\): \\[\nY\\sim\\operatorname{Bernoulli}(\\mu),\n\\] \\[\n\\operatorname{logit}(\\mu)=\\alpha+\\beta X,\n\\] \\[\nX=\\gamma+\\delta Z+\\epsilon,\\qquad\\epsilon\\sim N(0,\\sigma^2).\n\\]\nWe see the hierarchical structure \\[\nY|X\\sim\\operatorname{Bernoulli}(\\operatorname{logit}^{-1}(\\alpha+\\beta X)),\n\\] \\[\nX|Z\\sim N(\\gamma+\\delta Z,\\sigma^2),\n\\] just as in セクション 1.3.\nThis time, however, we cannot detour the calculation by the tower property, while in セクション 1.3 we could sanity check the result by the reproducing property of normal distribution.\n\n\n3.2 Likelihood Calculation\nThrough the core 式 3,\n\\[\\begin{align*}\n  p(y|\\alpha,\\beta,\\gamma,\\delta,\\sigma)&=\\int_\\mathbb{R}p(y|\\alpha,\\beta,x)p(x|\\gamma,\\delta,\\sigma)\\,dx\\\\\n  &=1_{\\left\\{1\\right\\}}(y)\\int_\\mathbb{R}\\operatorname{logit}^{-1}(\\alpha+\\beta x)\\phi(x|\\gamma+\\delta z,\\sigma)\\,dx\\\\\n  &\\qquad+1_{\\left\\{0\\right\\}}(y)\\int_\\mathbb{R}\\biggr(1-\\operatorname{logit}^{-1}(\\alpha+\\beta x)\\biggl)\\phi(x|\\gamma+\\delta z,\\sigma)\\,dx\\\\\n  &=\\qquad\\cdots\\cdots\n\\end{align*}\\]\nWe won’t proceed any more, observing the integral is not always tractable.\nIf the likelihood involves an intractable integral, how can we proceed maximum likelihood / Bayesian estimation?\n\n\n3.3 Data Augmentation\nWe can still find the mode of the likelihood function by the EM algorithm (Dempster ほか, 1977).\nThe case is similar in the Bayesian approach, where we enlarge the parameter space by treating \\(x\\) as a parameter (or a latent variable) and sample from \\(x\\) as well.\nThis is called data augmentation approach, initiated in the EM algorithm, later applied to Bayesian computations in (Tanner と Wong, 1987).\nAlthough \\(p(y|\\alpha,\\beta,\\gamma,\\delta,\\sigma)\\) might involve possibly intractable integrals over \\(x\\), \\(p(y|\\textcolor{red}{x},\\alpha,\\beta,\\gamma,\\delta,\\sigma)\\) does not, since it holds that \\[\np(y|x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)=p(y|x,\\alpha,\\beta)\n\\] given that \\(y\\) and \\(\\gamma,\\delta,\\sigma\\) are conditionally independent given \\(x\\).\nTherefore, the posterior distribution of \\((x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)\\) is given by\n\\[\np(x,\\alpha,\\beta,\\gamma,\\delta,\\sigma|y)\\,\\propto\\,p(y|x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)p(x,\\alpha,\\beta,\\gamma,\\delta,\\sigma)\n\\] \\[\n=p(y|x,\\alpha,\\beta)p(x|\\gamma,\\delta,\\sigma)p(\\alpha,\\beta,\\gamma,\\delta,\\sigma).\n\\tag{5}\\]\nBasically, higher level parameters \\(\\gamma,\\delta,\\sigma\\) are treated as a part of the machinery that systematically defines a prior for the lower level parameter \\(x\\). In fact, this was the motivation when the hierarchical structure is first introduced in (Lindley と Smith, 1972).\n\n\n3.4 Bayesian Computation\nUsing the decomposition (5), we can readily run MCMC algorithms to sample from the posterior distribution.\nIntroducing another latent variable, we may also run the Gibbs sampler based on Polya-Gamma decomposition as in (Polson ほか, 2013).\nAlternatively, we can run the HMC (Hamiltonian Monte Carlo) algorithm based on the gradient of the log posterior density.\nIn the following articles, we prepared you with the codes for gradient-based Bayesian estimation, including HMCs via Stan and Zig-Zag via PDMPFlux. Visit the following articles (although they are in Japanese) for more details:\n\n\n\nコード 1"
  },
  {
    "objectID": "posts/2024/Probability/likelihood.html#footnotes",
    "href": "posts/2024/Probability/likelihood.html#footnotes",
    "title": "Likelihood of Hierarchical Models",
    "section": "脚注",
    "text": "脚注\n\n\nFor this definition of conditional probability, see (Kallenberg, 2021, p. 167), (Dudley, 2002, p. 347). If we are to define conditional probability first, we need the concept of regular conditional probability, which is not discussed here.↩︎\nsee, for example, (Th’m 8.9 Kallenberg, 2021, p. 170)↩︎"
  },
  {
    "objectID": "posts/2025/Slides/Bio.html",
    "href": "posts/2025/Slides/Bio.html",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "",
    "text": "This slide contains some of my research keywords. The four words in black letters, Monte Carlo Simulation, Bayesian Inference, Generative Modeling, Uncertainty Quantification, are the keywords of my research.\nThe four fields in colors are the fields that my research is related to."
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#keywords-in-my-research",
    "href": "posts/2025/Slides/Bio.html#keywords-in-my-research",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "",
    "text": "This slide contains some of my research keywords. The four words in black letters, Monte Carlo Simulation, Bayesian Inference, Generative Modeling, Uncertainty Quantification, are the keywords of my research.\nThe four fields in colors are the fields that my research is related to."
  },
  {
    "objectID": "posts/2025/Slides/Bio.html#footnotes",
    "href": "posts/2025/Slides/Bio.html#footnotes",
    "title": "Drawing Parallels between Statistics and Nature",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMCMC stands for Markov Chain Monte Carlo methods.↩︎"
  },
  {
    "objectID": "static/Notations.html",
    "href": "static/Notations.html",
    "title": "数学記法一覧 | Mathematical Notations on This Website",
    "section": "",
    "text": "Do not laugh at notations; invent them, they are powerful.In fact, mathematics is, to a large extent, invention of better notations.— The Feynman Lectures on Physics, Vol.1, Ch.17, Sec. 5.\n\n\n\n\n\n\n最もよく使う記号にはローマン体またはイタリック体を用いた．紙や黒板に書く際にも便利だからである．\n代表的な関数の空間にはブラックボード・ボールド体を，測度の空間にはカリグラフィー体を用いる．"
  },
  {
    "objectID": "static/Materials/PosterHistory.html",
    "href": "static/Materials/PosterHistory.html",
    "title": "Posters",
    "section": "",
    "text": "8/28/2025.\n      \n        17:20-19:00.\n      \n      司馬博文 .\n      サンプリングにおける加速と安定性\n      \n      \n        \n          \n            連続最適化および関連分野に関する夏季学校.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        13:40-14:20.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非絶対連続分布からのサンプリング.\n      \n        \n          \n            統計数理研究所オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      3/08/2025.\n      \n        12:00-12:50.\n      \n      司馬博文 .\n      PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能\n      \n      \n        \n          \n            第19回日本統計学会春季集会.\n          \n        \n      \n      \n        （優秀発表賞受賞）\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n        \n           Preprint\n        \n      \n    \n  \n    \n      2/17/2025.\n      \n        15:30-16:30.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非可逆ジャンプの達成.\n      \n        \n          \n            計算技術による学際的統計解析ワークショップ.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/24/2024.\n      \n        10:30-11:10.\n      \n      司馬博文 .\n      新時代の MCMC を迎えるために\n      : 連続時間アルゴリズムへの進化.\n      \n        \n          \n            統数研オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      2/25/2024.\n      \n        18:00-19:30.\n      \n      Hirofumi Shiba.\n      A Recent Development of Particle Methods\n      : Inquiry towards a Continuous Time Limit and Scalability.\n      \n        \n          \n            MLSS2024 (OIST, Okinawa, Japan).\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#recent-5-presentations",
    "href": "static/Materials.html#recent-5-presentations",
    "title": "Materials",
    "section": "",
    "text": "8/28/2025.\n      \n        17:20-19:00.\n      \n      司馬博文 .\n      サンプリングにおける加速と安定性\n      \n      \n        \n          \n            連続最適化および関連分野に関する夏季学校.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        13:40-14:20.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非絶対連続分布からのサンプリング.\n      \n        \n          \n            統計数理研究所オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      3/08/2025.\n      \n        12:00-12:50.\n      \n      司馬博文 .\n      PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能\n      \n      \n        \n          \n            第19回日本統計学会春季集会.\n          \n        \n      \n      \n        （優秀発表賞受賞）\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n        \n           Preprint\n        \n      \n    \n  \n    \n      2/17/2025.\n      \n        15:30-16:30.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非可逆ジャンプの達成.\n      \n        \n          \n            計算技術による学際的統計解析ワークショップ.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/24/2024.\n      \n        10:30-11:10.\n      \n      司馬博文 .\n      新時代の MCMC を迎えるために\n      : 連続時間アルゴリズムへの進化.\n      \n        \n          \n            統数研オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#recent-posters-all-records",
    "href": "static/Materials.html#recent-posters-all-records",
    "title": "Materials",
    "section": "",
    "text": "8/28/2025.\n      \n        17:20-19:00.\n      \n      司馬博文 .\n      サンプリングにおける加速と安定性\n      \n      \n        \n          \n            連続最適化および関連分野に関する夏季学校.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        13:40-14:20.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非絶対連続分布からのサンプリング.\n      \n        \n          \n            統計数理研究所オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      3/08/2025.\n      \n        12:00-12:50.\n      \n      司馬博文 .\n      PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能\n      \n      \n        \n          \n            第19回日本統計学会春季集会.\n          \n        \n      \n      \n        （優秀発表賞受賞）\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n        \n           Preprint\n        \n      \n    \n  \n    \n      2/17/2025.\n      \n        15:30-16:30.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非可逆ジャンプの達成.\n      \n        \n          \n            計算技術による学際的統計解析ワークショップ.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/24/2024.\n      \n        10:30-11:10.\n      \n      司馬博文 .\n      新時代の MCMC を迎えるために\n      : 連続時間アルゴリズムへの進化.\n      \n        \n          \n            統数研オープンハウス.\n          \n        \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Materials.html#recent-posters",
    "href": "static/Materials.html#recent-posters",
    "title": "Materials",
    "section": "",
    "text": "9/22/2025.\n      \n        15:00-16:30.\n      \n      司馬博文 .\n      メトロポリスを超えた枠組みで我々はどこまで行けるか？\n      : PDMP と従来の MCMC はどう違い，どう違わないか？.\n      \n        \n          \n            機械学習若手の会 (YAML) 2025.\n          \n        \n      \n      \n        ハートピア熱海.\n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      8/28/2025.\n      \n        17:20-19:00.\n      \n      司馬博文 .\n      サンプリングにおける加速と安定性\n      \n      \n        \n          \n            連続最適化および関連分野に関する夏季学校.\n          \n        \n      \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      5/23/2025.\n      \n        13:40-14:20.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非絶対連続分布からのサンプリング.\n      \n        \n          \n            統計数理研究所オープンハウス.\n          \n        \n      \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n    \n      3/08/2025.\n      \n        12:00-12:50.\n      \n      司馬博文 .\n      PDMP によりスパイク付きの非絶対連続分布からもサンプリングが可能\n      \n      \n        \n          \n            第19回日本統計学会春季集会.\n          \n        \n      \n      \n      \n        （優秀発表賞受賞）\n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n        \n           Preprint\n        \n      \n    \n  \n    \n      2/17/2025.\n      \n        15:30-16:30.\n      \n      司馬博文 .\n      ベイズ変数選択の計算的解決\n      : PDMP による非可逆ジャンプの達成.\n      \n        \n          \n            計算技術による学際的統計解析ワークショップ.\n          \n        \n      \n      \n      \n      \n      \n        Details\n      \n      \n        \n           Poster\n        \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "static/Lectures/法律家のための統計数理.html",
    "href": "static/Lectures/法律家のための統計数理.html",
    "title": "法律家のための統計数理",
    "section": "",
    "text": "Period\nLocation\nTime\nFrequency\n\n\n\n\nFall, 2023\nSapia 8F, Tokyo\n18:00, Wed.\nBiweekly\n\n\n\n\n\n\n\nTextbook: Quantitative Analysis of Law by Koichi Kusano 草野耕一\n\n\n\n近年ベイズ統計学の発展には目覚ましいものがあり，裁判における事実の証明にベイズ統計学の手法が登場する日も遠くないかもしれない．（本書 p.123）\n\n\n\n\n\n\n\n\n\n\n\nSession\nDate\nSection\nKeywords\n\n\n\n\n1\n11/22, 2023\n第1章第1節\n確率の公理，確率の性質，条件付き確率\n\n\n2\n12/6, 2023\n第1章第2-3節\n条件付き確率，独立性，Bayesの公式，ベイズ計算\n\n\n3\n12/20, 2023\n第2章 pp. 42-72\n決定木，期待効用，ブースティング\n\n\n4\n1/11, 2024\n第3章第1-4節 pp. 73-96\n確率変数，統計的推測\n\n\n5\n1/24, 2024\n第3章第5-8節 pp. 96-126\n統計的検定，区間推定\n\n\n6\n2/7, 2024\n深層学習と GPT\n自己符号化器，word2vec\n\n\n7\n2/21, 2024\n刑法入門１\n法益，構成要件，責任\n\n\n8\n–, 2024\n刑法入門２\n詐欺，未遂\n\n\n\n\n\n全記事概観\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（６）GPT 入門\n\n\n番外編１\n\n\n\n草野数理法務\n\n\n\n\n2024-02-07\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（５）統計的仮説検定入門\n\n\n教科書第３章第５―８節 (pp. 96-126)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-24\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（４）推測統計学\n\n\n教科書第３章第１―４節 (pp. 73-96)\n\n\n\n草野数理法務\n\n\n\n\n2024-01-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（３）意思決定解析\n\n\n教科書第２章 (pp. 42-72)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（２）Bayes の定理\n\n\n教科書第１章第２―３節 (pp. 14-30)\n\n\n\n草野数理法務\n\n\n\n\n2023-12-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理（１）確率論入門\n\n\n教科書第１章第１節 (pp. 1-14)\n\n\n\n草野数理法務\n\n\n\n\n2023-11-22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#今回の内容",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n\n1.1 第2章：意思決定\n第二章は 決定木 を用いた 決定分析 に関する章である．\n\n1.1.1 第2章1節「決定の木の作り方」\n決定木の美点は，あり得るシナリオを全て書き出すことが出来ること にある (Smith, 2010, p. 29) が，それも箇条書きをするのではなく，木構造に書くことで，\n\n計算機との親和性があり，自動発見に繋げられる．\n最適なポリシーを執るための，期待値の計算が容易になる．\n計算した結果から，自身の信念の構造を反省する契機になる．\n\nなどの利点が生まれる，という極めて古典的な技法 (Raiffa and Schlaifer, 1961) である．\nこれを用いて全てのシナリオと，それに対応する金銭的利得と確率を書き出し，各意思決定毎の期待値を計算することで，意思決定に役立てることができる．\n決定木の 節 (node) には2種類ある (Taroni et al., 2014, pp. 35–36)\n\n確率節 (Chance Node)：どっちに転ぶか判らない事象（確率変数）を表す節．\n決定節 (Decision Node)：意思決定者が選択する行動を表す節．\n\n教科書 (草野耕一, 2016) では 1.を \\(\\bigcirc\\) で，2.を \\(\\square\\) で表している．\n\n\n1.1.2 第2章第2節「リスク中立的な行為者」\n\n\n\n\n\n\n問題2-2\n\n\n\n上場会社Dテレビがその報道についてE社から訴訟を受けた．2億円支払えば和解に応じるという．\n\nEがDに勝訴する確率は8割で，その場合1億円の賠償請求が命じられる．\n訴訟費用は3000万円．\nEが米国で訴訟を起こした場合，管轄権が認められる可能性は1割であり，さらにその場合には9割で5億円の賠償で2億の訴訟費用，1割で unjust enrichment の法理が適用され50億賠償で5億の訴訟費用がかかるとする．\n\n\n\nこの問題は「キャッシュフローの期待値」を決定木を通じて算出することで教科書内で解かれるが，同時に重要な問題を提起している．\nそもそも，評価基準が「キャッシュフローの期待値」であるべきとは限らない．リスクに対する評価は人それぞれである．\n事実，もし50億の賠償命令が下った場合に倒産リスクが生じる場合，この事象は「50億円」という額面以上に避けるべき事象ということになるだろう．\nそこで，金銭的利得とは別に 効用 の概念を導入し，この効用の期待値によって意思決定をするための論理基盤として (von Neumann and Morgenstern, 1944) の 期待効用理論 を紹介している．\n\n\n1.1.3 コラム2-1：「期待効用理論」\n期待効用理論極めて古いが，現在でも不確実性の下での意思決定の定量的理論の騎手である (Dentcheva and Ruszczynski, 2013)．\n教科書 (草野耕一, 2016) では「効用」という概念を金銭的利得と関連付けて説明しているが，そのためにわかりにくい提示の仕方になっている．1\nそこでここでは，抽象的な定義を提示する．効用とはここでは，「行為者にとっての好ましさの度合いを，相対的に比較できるように定量化したもの」以上の意味はないものとする．\n\n\n\n\n\n\n定義 (preference relation) (Dentcheva and Ruszczynski, 2013)\n\n\n\n\\(X\\) を位相空間とする．完備な 前順序2 \\(\\precsim\\;\\subset X^2\\) を 選好関係 と呼ぶ．3\n\n（完備性）任意の \\(x,y\\in X\\) について，4 \\[x\\precsim y\\;\\lor\\;x\\succsim y.\\]\n（連続性）任意の \\(z\\in X\\) に対して， \\[\n\\left\\{v\\in X\\mid v\\precsim z\\right\\}\\overset{\\textrm{closed}}{\\subset}X,\n\\] \\[\n\\left\\{v\\in X\\mid v\\succsim z\\right\\}\\overset{\\textrm{closed}}{\\subset}X.\n\\]\n（独立性）任意の \\(x,y,z\\in X\\) に対して，\\(x\\succsim y\\) ならば，任意の \\(\\alpha\\in(0,1)\\) について \\[\n\\alpha x+(1-\\alpha)z\\succsim\\alpha y+(1-\\alpha)z.\n\\]\n（アルキメデス性）5 任意の \\(x,y,z\\in X\\) に対して，\\(x\\succsim y\\succsim z\\) ならば，ある \\(\\alpha,\\beta\\in(0,1)\\) が存在して， \\[\n\\alpha x+(1-\\alpha)z\\succsim y\\succsim\\beta x+(1-\\beta)z.\n\\]\n\n\n\n\n\n\n\n\n\n定理 (Dentcheva and Ruszczynski, 2013)\n\n\n\n\\(X\\) を Polish 空間，\\(\\precsim\\;\\subset P(X)^2\\) を連続で独立な選好関係とする．6 このとき，ある有界連続関数 \\(u\\in C_b(\\mathbb{R})\\) が存在して， \\[\nU(\\mu):=\\int_Xu(z)\\,\\mu(dz)\\quad(\\mu\\in P(X))\n\\] は \\[\n\\mu\\succsim\\nu\\quad\\Rightarrow\\quad U(\\mu)\\ge U(\\nu)\n\\] を満たす．\n\n\n教科書 (草野耕一, 2016) にいう「期待効用定理」とはこの定理を指すものと思われる．この \\(u\\in C_b(\\mathbb{R})\\) を 効用関数 (utility function) という．\n問題2-2で用いていた金銭的利得の空間とは \\(X=\\mathbb{R}\\) の場合であり，これは定理の条件を満たす．よって任意の連続で独立な選好関係 \\(\\succsim\\) を用意することで，金銭的利得の代わりに効用 \\(u\\) の期待値 \\(U\\) を用いれば，特定のリスク選好性に対応して，同様の議論を用いて意思決定分析が可能になる．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#決定木の応用",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#決定木の応用",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "2 決定木の応用",
    "text": "2 決定木の応用\n今回決定木を用いた意思決定の形式化の仕方を学んだが，この手法は多くの人間と機械との間で広く共有されて最も輝く．\n決定木は現代では一般に （確率的）グラフィカルモデル の例として捉えられており，そのグラフとしての構造は計算機との親和性が高く，「不確実性の下での意思決定が出来る人工知能・エキスパートシステムの作成」において重要な役割を果たすと考えられている (Sucar, 2021)．\n\n2.1 人間同士のコミュニケーションツールとしての応用\n\n2.1.1 世界銀行の太陽光発電＋蓄電池システムの導入へのイニシアティブ\n世界銀行は 11月28日 に，途上国向けに大規模な太陽光発電＋蓄電システムを導入するために，どのようにプロジェクトを進めれば良いかの実践的なフレームワークを提供する報告書 (Jain et al., 2023) を公開した（プレスリリース）．\n安定したエネルギー供給源の確保も多くの途上国にとって重要な課題であるが，太陽光発電システムを導入し，化石燃料への依存度を低減させることも（特に公的債務がかさんでいる国家では）同時に重要である．これを可能にするフレームワークを提供することが，本報告書の目的であるようである．\nそのフレームワークは4段階\n\n実行可能性の評価：長期的なコスト，既存電力網への統合の方法，需要予測などの予備調査．\nビジネスモデルの選択：二部料金契約，容量契約，混合契約の3つを提示している．\nリスク配分の方法\n競争入札による調達と実行\n\nからなるが，特に 2.のビジネスモデルの選択について，多くのケーススタディを通じて得た「どの変数に応じてどのモデルが選択されるべきか」の知見を決定木の形にまとめている．\n\n\n\nDecision Tree for Selecting a Business Model Figure 3.2 from (Jain et al., 2023, p. 41)\n\n\n\nThe report’s ready-to-use planning framework, the decision-making tree, sample business models, and the PPA template aim to streamline the adoption of solar-plus-storage projects that leverage private investments in countries where fuel-dependency is putting stress on limited public resources. (Jain et al., 2023)\n\n\n報告書は、12月初旬にアラブ首長国連邦のドバイで開催される気候変動枠組み条約第28回締約国会議（COP28）で発表される。–プレスリリース\n\n\n\n2.1.2 世界銀行の気候変動対策イニシアティブ\n世界銀行は過去にも，気候変動対策の分野でも，同様の報告書と簡単な決定木（フローチャート）を発表している．\n\nNo generally accepted methodology for assessing the significance of climate risks relative to all other risks to water resources projects currently exists. This book puts forth a decision support framework in the form of a decision tree to meet this need. (Ray and Brown, 2015)\n\n\n\n\n2.2 機械とのコミュニケーションツールとしての応用\n木構造というのは計算機にとっても扱いやすい構造であり，決定木をデータから学習することで大いに我々の意思決定に活用することができる．これが現代において，機械学習技術が我々に与えてくれる希望の形である．\n\n2.2.1 決定木学習\nどう考えても，決定木は人間が書くよりも，データから学習する方が良い．これが 決定木（学習） (Breiman et al., 1984) である．\nこれは初め，回帰木 という名前で，単関数の線型和を用いたノンパラメトリック回帰手法として導入された．さらにアンサンブル法と組み合わせて精度を向上させたものが ランダムフォレスト (Breiman, 2001) と呼ばれる手法であり，現在極めて主流な手法となっている．\n\n\n2.2.2 2017年度日本経済白書\n内閣府が発表している日本経済白書の 2017年度版 の第二章では「多様化する職業キャリアの現状と課題」を扱っている．\n第3節にて，リスキリングに関して調べられており，「自主的にキャリア設計をしたい人を決める変数は何か？」を決定木学習によって調べている．7\n\n分析結果をみると（第2－3－7図（2））、正社員では、自主的に職業生活設計を考えている割合は、将来に備える目的で自己啓発を実施している人では8割となるが、実施していない人では65％と大きく下がる。だが、実施していなくても大卒・院卒であれば、職業生活設計の自主性割合は73％と増えるが、大卒・院卒以外では59％と大きく差が開く。–日本経済2017-2018\n\n\n\n\n日本経済2017-2018 第2章第3節2 図2-3-7 より\n\n\n第1節では，転職市場の流動性を調べるにあたって，決定木を用いたブースティングにより，転職による賃金上昇の説明変数で，最も有力なものは何かを調べている．\n\nまず、転職前後の賃金変化と関係の深い変数を調べるため、機械学習の手法を用いて、転職後に賃金が変化する人の特徴について整理する。ここでは機械学習の分野でよく使われている「ランダム・フォレスト」という手法を用いた。同手法は、説明変数の数が多くても対応でき、それぞれの説明変数の「重要度」を算出できることから、転職者が持つ多数の特徴のうち、どこに注目するのが適切かを把握するのに有用であると考えられる。–日本経済2017-2018\n\nなお，結果は以下の通りである．\n\n\n\n日本経済2017-2018 第2章第1節2 図2-1-7 より\n\n\n\n\n2.2.3 ランダムフォレストとは何か？\nせっかくなので，日本経済 付記2-1 に付された簡潔な説明を引用する．\n\nランダム・フォレストは、学習データから「決定木」と呼ばれるツリー構造をしたグラフを大量に作成し、作成した決定木を元に多数決で最良の結果を導き出す方法である。–日本経済2017-2018\n\nランダム・フォレストはバギングと呼ばれるアンサンブル法の一種である．アンサンブル法 とは，性能の芳しくない小さな学習器をたくさん集め，これらを効率的に組み合わせることで全体として優れた性能を発揮するように組み合わせる，一段階高次元な機械学習手法であり，バギング とは「1つ1つは信頼出来ない出力でも，全ての平均を取れば性能が良くなる」という考え方に基づく手法である．8\n特に決定木に対してバギングを使用した場合，その手法は ランダム・フォレスト と呼ばれる．これがうまくいく理由は，アンサンブルの過程にある．学習の段階では何も最適化しておらず，アンサンブルの段階で最適化されているのである．\nアンサンブル法は極めてアドホックに感じられるかも知れないが，実践の場面では重要な技術であり，実際 Kaggle などのデータコンペティションでは最も広く見られる戦略の1つである．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#グラフィカルモデルによる意思決定支援",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#グラフィカルモデルによる意思決定支援",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "3 グラフィカルモデルによる意思決定支援",
    "text": "3 グラフィカルモデルによる意思決定支援\n本書では取り上げられなかったが，意思決定の場面こそベイズ統計学の本領 と言えるものである．9\n\n3.1 グラフィカルモデルの利用\n本書で解説されている決定木による意思決定分析の先に，近年では Bayesian Network などの新しい道具が，確率的グラフィカルモデリング の分野から追加されている (Smith, 2010)．\n\nThese graphical methods help draw different aspects of a decision problem together into a coherent whole and provide frameworks where data can be used to support a Bayesian decision analysis. (Smith, 2010, p. viii)\n\nこれは，決定木によるイベントの表現では，イベントの間の依存関係を表現することが出来ない（現実を単純化し過ぎている）という欠点を，木構造を一般のグラフ構造に拡張することで解決したものである．10\n構造は多少複雑になるが，視覚的な理解も引き続き用意である同時に，近年では多くの推論手法が提案されつつある (Sucar, 2021)．\n\n\n3.2 法廷での利用について\n(Smith, 2010, p. 20) では，法廷での DNA 証拠の使用をきっかけに，「確率」を用いた議論が法廷に導入されたことに触れて，ベイズの枠組みは次の意味で親和性が高いと論じている．そのことは，本勉強会で 第一章の内容 を学んできた皆さんには首肯いただけることだろう．\n\nベイズ統計学は演繹的論理の拡張であり，厳密で論理的なフレームワークを提供してくれる．\nどうしてその意思決定に至ったかを，透明で一貫性のある方法で説明することができる．\n\n\n\n3.3 法科学への応用\nBayesian Network は法科学への応用が進んでいる．11\n\nA problem that arises in a courtroom, affecting both lawyers, witnesses and jurors, is that several pieces of evidence have to be put together before a reasoned judgement can be reached: as when motive has to be considered along with material evidence. Probability is designed to effect such combinations but the accumulation of simple rules can produce complicated procedures. Methods of handling sets of evidence have been developed: for example Bayes nets (…). There is a fascinating interplay here between the lawyer and the scientist where they can learn from each other and develop tools that significantly assist in the production of a better judicial system.–Foreword by Dennis Lindley (Aitken and Taroni, 2004, p. 24)"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理3.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理3.html#footnotes",
    "title": "法律家のための統計数理（３）意思決定解析",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n加えて，p.55 に「上記の公理からの演繹的推論によって次の結論が導き出される」とあるが，これは「効用」「単調増加」などの未定義用語を含むため，「演繹的推論によって」というのは誤謬というべきである．↩︎\nすなわち反射的 \\(x\\precsim\\) で推移的な二項関係↩︎\nさらに \\(X\\) を prospect space という．↩︎\nすなわち（反対称性を満たさないが）「全順序」であるということである．↩︎\n教科書 (草野耕一, 2016) では「連続性」とあったが，ここでは (von Neumann and Morgenstern, 1944) の原著に従う．↩︎\n記法については 数学記法一覧 参照．↩︎\n決定木学習について，付記2-1 にて「決定木による分類は、説明変数によるサンプルの分割を繰り返しながら徐々に分類目的（職業設計を自分で実施）の予測誤差を小さくしていく手法である。説明変数間の相互作用を考慮した分類が可能であり、複数の説明変数で分割していくことで職業設計を自分でしたい人の比率が高まる（低まる）樹形図（tree）が作成できる。」と述べられている．↩︎\nBootstrap aggregating の略である (Hansen, 2022, p. 927)．↩︎\n(Smith, 2010, p. 8) で，近年の決定分析の文脈でベイズの手法が興隆している理由の一つに，ベイズ計算手法の発展を挙げている．↩︎\n(Smith, 2010, p. 59) 2.8節も参照．↩︎\n(Taroni et al., 2014), (Smith, 2010, pp. 34–37) 2.3.1節↩︎"
  },
  {
    "objectID": "static/Materials.html#その他-others",
    "href": "static/Materials.html#その他-others",
    "title": "Materials",
    "section": "その他 | Others",
    "text": "その他 | Others\n\n\n\n\n\n\n\n\n\n\n法律家のための統計数理\n\n\n数学と法学，双方からの交流と理解を図ります\n\n\n\n\n\n\n\n\n\n\n\n\n\n経験過程論勉強会\n\n\n可測性の問題，再生核 Hilbert 空間\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Seminars/経験過程論.html",
    "href": "static/Seminars/経験過程論.html",
    "title": "経験過程論勉強会",
    "section": "",
    "text": "Period\nLocation\nTime\nFrequency\nDuration\n\n\n\n\nSummer, 2023\nEconomics 6F, Univ. of Tokyo\n13:00~, Wed.\nWeekly\nAug. 16 - Oct. 13\n\n\n\n\n\n\n\n担当分の発表資料"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理_.html",
    "href": "posts/2024/数理法務/法律家のための統計数理_.html",
    "title": "法律家のための統計数理（？）多変量解析の基礎",
    "section": "",
    "text": "シリーズトップページはこちら．\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "",
    "text": "A Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n君塚さん に今回用いたスライドを特別に公開して良いとの許可をいただきました．どうもありがとうございます．\n以降は，講義を通じて筆者が理解した内容の，筆者自身のためのまとめであり，発表者の見解を代弁するものではない．シリーズトップページは こちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の目的と処罰対象",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の目的と処罰対象",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "1 刑法の目的と処罰対象",
    "text": "1 刑法の目的と処罰対象\n\n1.1 刑法の目的と機能\n\n\n\n\n\n\n刑法の目的：法益保護主義\n\n\n\n刑法は 法益 (Rechtsgut) の保護 を目的とするものであり，法益を侵害または危殆化する行為 を処罰対象とする．\n\n\n法益は，これが帰属する主体に応じて，\n\n個人法益\n社会法益\n国家法益\n\nに大別される．\n\n\n\n\n\n\n例：堕胎罪\n\n\n\n\n\n次の刑法の条文は，誰のどのような法益を保護していると言えるか？\n\n第二十九章　堕胎の罪 （堕胎） 第二百十二条　妊娠中の女子が薬物を用い、又はその他の方法により、堕胎したときは、一年以下の懲役に処する。\n\nこれには，すぐに考えつく立場が２つあるだろう．\n\n胎児の生命\n母体の健康\n\n１の立場では，立場上胎児をある種の「人間」と認めていることになり，２の立場では母体の一部と認めていることになる．\n\n\n\n\n\n\n\n\n\n例：胎児性過失致死傷の最高裁判例\n\n\n\n\n\n最高裁判所第三小法廷決定昭和63年2月29日 は，行為と結果の時点が分離していることが問題の１つである．というのも，胎児の段階で被った傷害による，出生後の過失致死傷が起こったという事件である．\n裁判要旨（の一部）は次のようなものであった，\n\n業務上の過失により、胎児に病変を発生させ、これに起因して出生後その人を死亡させた場合も、人である母体の一部に病変を発生させて人を死に致したものとして、業務上過失致死罪が成立する。\n\n\n\n\n\n\n1.2 刑法の処罰根拠\n\n\n\n\n\n\n刑法の処罰根拠\n\n\n\nこれには２つの説がある．\n\n法益侵害／結果無価値 (Erfolgsunwert) 説\n規範違反／行為無価値 (Handlungsunwert) 説\n\n\n\n\n1.2.1 法益侵害説\n\n\n1.2.2 規範違反説\n\n刑法とは，刑罰という制裁を予告することにより，人間の行動を心理的にコントロールして，犯罪から遠ざけようとするシステムにほかならない． (伊藤正己 and 加藤一郎, 2005, p. 111)\n\n\n\n\n1.3 刑法と道徳\n\n刑法は，法の中でも道徳と最も密接な関係を持つものであると言われている．刑法は，一般的に，「人ヲ殺シタル者ハ死刑又ハ無期若クハ三年以上ノ懲役ニ処ス」（同法一九九条），「他人ノ財物ヲ窃取シタル者.ハ窃盗ノ罪ト為シ十年以下ノ懲役ニ処ス」（同法二三五条）というふうに，裁判官が裁判をする場合の準則すなわち裁判規範のかたちで規定されているが，その背後には当然，「人を殺してはならない」，「他人の財物を窃取してはならない」という行為規範が前提とされており，これらはそれぞれ，「殺すなかれ」，「盗むなかれ」という道徳律を裏付けるものということができよう．しかし，刑法と道徳とは，必ずしも常にこのような相即不離の関係にあるわけではない．階級対立のある社会においては，階級間の価値意識の分裂に照応して，道徳もまた，支配階級の道徳と被支配階級の道徳とに分かれている．このように複数の道徳が存在する場合に，法によって裏うちされるのは，必ず支配階級の道徳である．従って，支配階級の道徳を反映した法の一環としての刑法と，被支配階級の道徳とは，しばしばするどく対立する．例えば，資本主義社会にあっては，ストライキは被支配階級である労働者がその生存を維持するための最も強力な手段であり，従って労働者にとって組合の決定によりストライキを決行する場合に団結を固くしてこれに積極的に参加することは，当然のモラルである．しかも，資本主義国家においては，このように労働者階級のモラルに忠実な行為がしばしば刑罰の対象とされてきたし，勤労者の団体行動権を保障した日本国憲法の下でさえも，たとえば，公務員のストライキを煽り，そそのかすような行為は犯罪とされているのである． (渡辺洋三, 1993, pp. 216–217)\n\n刑法は国民に対して道徳的な規範を示していると読み取れる．だが，そのことをどう評価すべきかは，筆者はまだわからない．時代と共に変わるべきものであるということは確かである．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の体系",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html#刑法の体系",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "2 刑法の体系",
    "text": "2 刑法の体系\n\n2.1 犯罪の３成立要件\n\n\n\n\n\n\n犯罪の３成立要件\n\n\n\n\n構成要件該当性\n\n\n基本的には 条文に具体的に示されている要件に該当するか ということである．\nその主たる２大要素は 行為 と 結果 である．\nこれを 客観 と 主観 に分けて議論することも慣例である．\n\n\n違法性\n\n\n刑法の処罰根拠たる 法益への侵害と危殆化 が実現しているか（違法性阻却事由）を判断する．\n基本的には，正当行為・正当防衛・緊急避難（第35条から第37条）への該当の有無の判断である．\n法益保護の方向性を持った行為ならば，\n\n\n有責性\n\n\n法的避難可能性 の有無のこと．\n\n\n\n「構成要件」の段階に「行為」と「結果」をどこまで入れるかが，行為無価値と結果無価値．\n\n\n\n\n\n\n例：実質的違法論\n\n\n\n\n\n「貧しすぎて」は根拠条文がないが，違法性阻却事由になる．これは法益保護の方向だからである (厚生労働省, 2004)．\nこれには「当該行為の具体的状況その他諸般の事情を考慮に入れ、それが法秩序全体の見地から許容されるべきものであるか否か」を判別する作業を伴う．最高裁判所第二小法廷判決 昭和50年8月27日\n\n\n\n\n\n2.2 故意の概念\nこの概念が難しすぎる．\n\n「（構成要件的）故意」とは「犯罪構成要件の認識と認容」である．\n「責任故意」とは「違法性の認識と認容」である．\n\n\n\n\n\n\n\n例"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理7.html#令和５年司法試験問題",
    "href": "posts/2024/数理法務/法律家のための統計数理7.html#令和５年司法試験問題",
    "title": "法律家のための統計数理（７）刑法入門",
    "section": "3 令和５年司法試験問題",
    "text": "3 令和５年司法試験問題\n\n\n\n令和５年司法試験問題論文式試験問題集［刑事系科目第１問］\n\n\n\n\n\n\n\n\n事例１\n\n\n\n\n\n\n甲は、乙及び丙と共に、後記計画に基づき、常習的に高齢者から現金をだまし取っていた。その計画は、\n\n甲が資産家の名簿を見て、現金をだまし取る対象者を選定する。\n甲が警察官に成りすまして相手方に電話をかけ、「Ｘ警察署の○○です。この度、この地域を担当することになりました。今後、当署からの連絡はこの番号からかけますので、御登録をお願いします。」などとうそを言って、名前と電話番号を告げる（以下、この内容の電話を「１回目の電話」という。）。\nその翌日、甲が相手方に電話をかけ、「昨日電話した○○です。あなたの預金口座が、不正に利用されている疑いがあります。捜査のために必要なので、お持ちの預金口座に１００万円を超える残高があるようでしたら、速やかに全額を引き出して自宅に持ち帰った後、こちらに電話をください。」などとうそを言う（以下、この内容の電話を「２回目の電話」という。）。\n相手方に預金口座から現金を引き出させて、自宅にその現金を持ち帰らせる。\nその後、相手方からかかってきた電話で、甲が、相手方の現金引出しを確認した上、「これから警察官がそちらに向かいます。」とうそを言う。\nその約１時間後、乙及び丙が警察官を装って相手方の家を訪ねる。\n乙及び丙が、捜査のために必要なので現金を預けてほしい旨のうそを言い、その交付を受けて現金をだまし取る。\n\nというものであった。\n甲らは、上記計画に従い、以下の行為に及んだ。\n\n甲は、某月１日、名簿から現金をだまし取る対象者として高齢の男性Ａを選んだ。\n甲は、同日午前１０時、Ａに１回目の電話をかけた。\n甲は、同月２日午前１０時、Ａに２回目の電話をかけた。\n甲のうそを信用したＡは、預金口座から２００万円を引き出して自宅に持ち帰った。\n甲は、同日正午、Ａからかかってきた電話に出て、Ａが２００万円を引き出したことを確認した上、Ａに対し、「これから警察官がそちらに向かいます。」とうそを言った。\n乙及び丙は、甲の指示に基づき、同日午後１時、警察官を装ってＡ宅を訪ねた。しかし、乙らの姿を見て不審に思ったＡが玄関ドアを開けなかったため、乙及び丙は、捜査のために必要なので現金を預けてほしい旨のうそを言うことができないまま、Ａから現金をだまし取ることを断念した\n\n\n\n\n\n\n\n\n\n\n\n設問１\n\n\n\n\n\n【事例１】におけるＡに対する甲の罪責に関し、以下の⑴及び⑵について、答えなさい。なお、⑴及び⑵のいずれについても、自らの見解を問うものではない。\n\n甲に詐欺未遂罪の成立を認める立場から、その結論を導くために、どのような説明が考えられるか。詐欺罪が「人を欺いて財物を交付させ」るという手段・態様を限定した犯罪であるのに、その実行の着手に「現金の交付を求める文言を述べること」を要しないと考える理由に触れつつ論じなさい。\n1の説明に基づくと、上記 1~6 のうちどの時点で実行の着手を認めることになるのか。具体的事実に即して、それより前の時点との実質的相違を明らかにしつつ論じなさい。\n\n\n\n\n直観的には甲を正犯として罰するべきであると思われる．その方向性で議論するための，厳密な論拠を示すことが求められている．\n\n\n\n\n\n\n事例２\n\n\n\n\n\n【事例１】の１の事実に続けて、以下の事実があったものとする。\n\n甲は、上記計画に従い、某月５日午前１０時、名簿から現金をだまし取る対象者として高齢で一人暮らしの男性Ｂを選んだ上、Ｂに１回目の電話をかけ、さらに、同月６日午前１０時、２回目の電話をかけた。Ｂは、甲のうそを信用し、同日午前１０時３０分、預金口座から３００万円を引き出して自宅に持ち帰った。甲は、同日正午、Ｂからかかってきた電話で、Ｂが３００万円を引き出して自宅に持ち帰った旨を聞いたことから、「これから警察官がそちらに向かいます。」とうそを言い、Ｂは「分かりました。待っています。」と答えた。甲は、乙及び丙に対し、高齢で一人暮らしの男性Ｂがうそを信用し、３００万円を自宅に用意している旨を告げ、計画どおり、捜査のために必要なので現金を預けてほしい旨のうそを言って、３００万円をだまし取ってくるように指示し、乙及び丙はこれを了承した。\n乙は、甲の上記指示を受け、丙と共にＢ宅に向かうことにしたが、その道中で、Ｂを縛り上げてしまえば、より確実に現金を手に入れることができると考え、丙に対し、「ジジイをだますより、縛った方が確実に金を奪える。縛って、金を奪ってしまおうぜ。奪った３００万円を３人で分ければ問題ないだろう。」などと言い、丙はこれを了承した。そして、乙及び丙は、Ｂの手足を縛るためのロープと口を塞ぐための粘着テープを準備した上、同日午後１時、Ｂ宅へ赴き、インターホンを鳴らして警察官であることを告げ、Ｂに玄関ドアを開けさせた。乙及び丙は、直ちにＢ宅内に押し入り、Ｂの手足をそれぞれロープで縛り、口を粘着テープで塞ぎ、Ｂを床の上に倒した。そして、リビングルームに移動した乙及び丙は、Ｂが預金口座から引き出してテーブル上に置いていた上記３００万円を見付け、同日午後１時１０分、同３００万円を持ってＢ宅を出た。その後、乙及び丙は、甲に対し、いつもどおりのやり方でＢから３００万円をだまし取ってきたと虚偽の報告をし、それぞれ１００万円ずつ山分けした。\n同日午後３時、Ｂの娘ＣがＢ宅を訪れ、緊縛されたＢを発見した。Ｃから上記ロープ及び粘着テープを取り外してもらったＢは、立ち上がろうとしたものの、長時間の緊縛による足のしびれでふらついて倒れそうになった。そのため、Ｃは、Ｂを座らせ、そのままでいるように言った。Ｂは、それにもかかわらず、その１分後、Ｃがその場を離れた隙に、奪われた物の有無を確認するために立ち上がろうとした。その際、Ｂは、まだ上記足のしびれが残っていたために、転倒して床に頭を打ち付け、全治２週間を要する頭部打撲の傷害を負った。\n\n\n\n\n\n\n\n\n\n\n設問２\n\n\n\n\n\n【事例２】における甲、乙及び丙の罪責について、論じなさい（住居等侵入罪（刑法第１３０条）及び特別法違反の点は除く。）。\n\n\n\n\n\n\n\n\n\n事例３\n\n\n\n\n\n【事例２】の事実に続けて、以下の事実があったものとする。\n\nＹ警察署の警察官Ｄは、【事例２】に係る事件につき、乙に対する逮捕状を取得し、乙の逮捕に向かったところ、乙が細い路地を丁と共に歩いているのを発見した。Ｄは、逮捕のため、乙に接近しようとしたが、それに気付いた乙が走って逃げ出したため、急いで乙を追おうとした。丁は、乙が警察官に逮捕されそうになっていることを察し、乙を逃がそうと考え、怒号しながら両手を広げて立ちはだかり、道を塞いだ。そのため、Ｄは、直ちに乙を追い掛けることができず、乙を逮捕することができなかった。\nその後、Ｄは、Ｙ警察署の警察官５名に乙を追跡して逮捕するよう応援を要請した。丁は、警察官による乙の逮捕を妨害しようと考え、Ｙ警察署に電話をかけ、「Ｙ署近くの路上で、通り魔に刺されました。すぐに来てください。」などとうそを言った。そのため、上記警察官５名は、更なる通り魔事件発生への警戒等を行わざるを得なくなった結果、乙を追跡できず、乙を逮捕することができなかった。\n\n\n\n\n\n\n\n\n\n\n設問３\n\n\n\n\n\n【事例３】における前記６の事実につき、丁に業務妨害罪の成立を否定しつつ（丁による怒号などは、公務執行妨害罪における暴行・脅迫には当たらないが、業務妨害罪における威力には当たることを前提とする。）、前記７の事実につき、丁に上記警察官５名に対する業務妨害罪の成立を肯定する立場からは、その結論を導くために、どのような説明が考えられるか、論じなさい。なお、自らの見解を問うものではない。"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理___.html",
    "href": "posts/2024/数理法務/法律家のための統計数理___.html",
    "title": "法律家のための統計数理（？）AI の信頼性",
    "section": "",
    "text": "シリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理___.html#state-v.s.-loomis",
    "href": "posts/2024/数理法務/法律家のための統計数理___.html#state-v.s.-loomis",
    "title": "法律家のための統計数理（？）AI の信頼性",
    "section": "1 State v.s. Loomis",
    "text": "1 State v.s. Loomis\n(山本龍彦 and 尾崎愛美, 2018)"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "",
    "text": "(草野耕一, 2016) の勉強会第1回の補足として，確率論の数学的枠組みを紹介する．\n参考書としては (大塚淳, 2020) もおすすめ．\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#今回の内容",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n\n1.1 本書の概観\n本書は「数理法務」＝「法の数理分析」に関する発展的内容を扱った書籍で，内容は大きく次の1から3の3つからなる：\n\n法の行動分析：法律家がとるべき行動を数理を用いて分析する．\n法の統計分析：事実の推定や因果関係の推定に統計手法を応用する．\n法の財務分析：企業や金融に関わる法事象をファイナンス理論を用いて分析する．\n法の経済分析：法を経済学的な観点から分析する（本書では扱われていない）．\n\n第1回勉強会では第1章「行動分析(1)事実認定」を扱った．事実認定を，Bayes推論の枠組みで捉え直し，法律家として誤謬やバイアスに陥ってしまうことを避けるツールとして，確率論を導入しており，「法の数理分析は役に立つ」ことを端的に実感できる，導入として極めて鮮やかな章になっている．\n\n\n1.2 主観確率とBayes計算\nまず，第1章は，事実認定の文脈で妥当な確率概念は「主観確率」であり，今後「確率」とはこの意味で用いることを注意喚起する内容から始まる．\n主観確率と客観確率の詳細な定義は本書を参照願いたいが，一言で言えば，後者は「人間に不可知な真の値」というものの存在を前提とするのに対し，前者はそれを仮定しない．\n従って，主観確率の考え方は，より多くのものに「確率」を導入することを可能にし，より柔軟な議論が可能であるが，その分数理的な困難も増し，真に発展が進んだと言えるのは，計算機が十分に爛熟した21世紀になってのことであると言える．この統計学分野を Bayes計算 (Bayesian Computation) といい，筆者の研究分野である．\n\nThe development of computing algorithms especially suited for Bayesian analysis in the 1990s together with the exponential growth of computing resources enabled Bayesian nonparametrics to go beyond the simplest problems and made it a universally applicable paradigm for inference. (Ghosal and van der Vaart, 2017)\n\n\n\n1.3 Bayes統計学とは？\n大雑把に言って，客観確率に基づく統計手法を 頻度論的手法 (frequentist methods)，主観確率に対する統計手法を Bayes手法 (Bayesian methods)という．一般に後者は前者を包含する（前者は後者の特別な場合1）と考えられる．しかしこれは「確率の解釈」が違うのみであり，数学としては確率の定義は1つである．「確率の解釈」については，双方の立場の中でもそれぞれ複数の立場が乱立しており，ここでは立ち入らない．と言っても，この注記も教科書的なもので，実用上不便を生じる場面はほとんどないだろう．\n\n不確実性を定量化するのに、ベイズ計算では必ず『確率』を使います。一般の人から見たら、統計で確率を使うのは当たり前と思うでしょうが、じつは他の統計手法ではそうでもなく、さまざまな解釈が生まれてしまう。定量化にはすべて統一的に確率を使うベイズ計算は、非常にシンプルなので、最終的にすべての統計はベイズに行き着くしかないと思っています。 鎌谷研吾\n\n\n\n1.4 Bayes確率の基礎付けの試み……！？\n法律家による事実認定の文脈においても，「真実はいつも1つだからそれを推定したい」と考えても，「不確実な中でも，判断を誤らないようにしたい」と考えても，どちらから議論しても良いことは納得いただけるだろう．ただ，一般の人の素朴な「確率」の理解は，Bayes流のものに近いと言われている．2\nそのこともあり，本書で「主観確率の考え方を採用する」というのは，「確率の解釈の議論はここではしない」「主観的な確信度合いの意味で，現実から多少の乖離を許す」という程度の意味であろう．\nしかし，本書の「主観確率」の議論は中途半端な取り扱いでは終わらず，興味深いことに，One More Step 1-1 (pp.9-10) にて，数理哲学者Donald A. Gilliesによる主観確率の測定による基礎付けの議論が紹介されていた．筆者は初耳の議論であり，己の議論の正統性・基礎付けに細心の注意を施す法律家の心が現れていると筆者は見た．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#sec-2",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#sec-2",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "2 【深掘り】確率の公理",
    "text": "2 【深掘り】確率の公理\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\n2.1 「確率の公理」がなぜ重要なのか？\n本書1.1節では確率の性質が列挙されている．1.2節以降では，これらの性質が「確率の定義」として引用されるが，いまいちどれを指して「定義」と呼んでいるのか定かでない．\n数学的な議論に慣れたあとはそれでも良いかも知れないが，法学も初学の間は逐一根拠条文に戻ることが大切であるように，数学もはっきりと定義を列挙し，「それのみを根拠とすること」を徹底することが大事である．\nなお，数学では何を定義として採用するかに任意性がある場合が多いが，唯一やってはいけないことは「定義が曖昧な状態で進むこと」である．そこで，せっかくであるから，現代数学が定義する最も筋の良い定義を採用して，本書の内容を俯瞰することにする．\n\n人は，確率論のもった政治的，社会的意義を忘れてはならない．理知を一切の尺度として「代数学の炉火によって倫理学及び政治学を照さん」(Condorcet) という時代精神，神の啓示に代らんとする確率論，それはフランス革命の思想的基礎に連関することを見失ってはならないのである．(北川敏男, 1949)\n\n現代数学において，確率を特徴付けるものは「代数的性質」であり，それは次の3つのみに集約される．3\n\n\n2.2 確率の公理\n\n\n\n\n\n\n定義（確率） (Kolmogorov, 1931)\n\n\n\n集合 \\(\\Omega\\) 上の確率とは，次の3条件を満たす関数 \\(P:\\mathcal{P}(\\Omega)\\to\\mathbb{R}\\) である：4\n   [P1] \\(P(\\Omega)=1\\)．\n   [P2] \\(A\\cap B=\\emptyset\\) ならば， \\[P(A\\sqcup B)=P(A)+P(B).\\]\n   [P3] 任意の事象 \\(A\\subset\\Omega\\) について， \\[0\\le P(A).\\]5\nただし，\n\n\\(P\\) の定義域 \\(\\mathcal{P}(\\Omega)\\) は「\\(\\Omega\\) の部分集合全体の集合」のことである．これを \\(\\Omega\\) の冪集合という．\n\\(A\\sqcup B\\) とは， \\(A\\cap B=\\emptyset\\) が成り立つときの \\(A,B\\) の合併 \\(A\\cup B\\) を，\\(A\\cap B=\\emptyset\\) を強調して書き分ける記法とする．\n\n\n\nこの公理から，我々が日常的な感覚から「確率に成り立っていて欲しい性質」が全て導ける，ということが現代数学の重要な発見である．性質を見ていく前に，「定義」として，主要な概念に親しみやすい名前を付ける．そのすべての過程において，上の[P1], [P2], [P3]以外を用いていないことを確認することは，数学入門の際には非常に大事な営みである．6\n\n\n\n\n\n\n確率論に関連する用語\n\n\n\n全体集合 \\(\\Omega\\) は所与のものとする．7\n\n事象 とは，部分集合 \\(A\\subset\\Omega\\) のことをいう．\n事象 \\(A\\subset\\Omega\\) の補集合\\[A^\\complement=\\Omega\\setminus A=\\overline{A}:=\\left\\{\\omega\\in\\Omega\\mid \\omega\\notin A\\right\\}\\]を \\(A\\) の余事象という．左から順に，数学で一般によく使われる記号である．8\n2つの事象 \\(A,B\\subset\\Omega\\) が 排反 であるとは，集合として共通部分を持たないことをいう： \\(A\\cap B=\\emptyset\\)．\n\n\n\nこの3性質から，本書第1.1節にいう「確率の推論法則」が全て導出できる．\n\n\n2.3 式(1.1) p.4の証明\n\n\n\n\n\n\n式(1.1) p.4\n\n\n\n任意の事象 \\(A\\subset\\Omega\\) について，\\[0\\le P(A)\\le 1.\\]9\n\n\n\n\n\n\n\n\n式(1.2) p.5\n\n\n\n任意の事象 \\(A\\subset\\Omega\\) について， \\[\nP(A)+P(A^\\complement)=1.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(0\\le P(A)\\) は[P3]に他ならない．式(1.2)も[P2]から従う．\\(A\\subset\\Omega\\) の補集合を \\(A^\\complement:=\\Omega\\setminus A\\) で表すと， \\(P(A^\\complement)\\ge0\\) も成り立つから， \\[\n\\begin{align*}\nP(A)&\\le P(A)+P(A^\\complement)\\\\\n&=P(A\\sqcup A^\\complement)\\\\\n&=P(\\Omega)=1.\n\\end{align*}\n\\]\n\n\n\n\n2.4 式(1.3) p.5の証明\n\n\n\n\n\n\n式(1.3) p.5\n\n\n\n任意の \\(n\\ge1\\) について， \\(n\\) 個の事象 \\(A_1,\\cdots,A_n\\subset\\Omega\\) が互いに排反であるとき， \\[\n\\begin{align*}\n&P(A_1)+P(A_2)+\\cdots+P(A_n)\\\\\n&\\qquad\\qquad=P(A_1\\sqcup A_2\\sqcup\\cdots\\sqcup A_n).\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(n\\) に関する数学的帰納法による．\n\n\n\n\n2.5 式(1.4) p.6の証明\n\n\n\n\n\n\n式(1.4) p.6\n\n\n\n任意の事象 \\(A,B\\subset\\Omega\\) について， \\[\nP(A)+P(B)=P(A\\cup B)+P(A\\cap B).\n\\]\n\n\n[P2] の条件は，\\(A_1,A_2\\) が排反である場合に限定しており，その制限が邪魔であった．ここで一般の加法公式を得ることになる．\n\n\n\n\n\n\n証明\n\n\n\n\\(C:=A\\cap B\\) とおくと，3つの集合 \\(A\\setminus B,C,B\\setminus A\\) が互いに排反であることから，\n\\[\n\\begin{align*}\n&\\quad P(A)+P(B)\\\\\n&=\\biggr(P(A\\setminus B)+P(C)\\biggl)+\\biggr(P(C)+P(B\\setminus A)\\biggl)\\\\\n&=\\biggr(P(A\\setminus B)+P(C)+P(B\\setminus A)\\biggl)+P(C)\\\\\n&=P(A\\cup B)+P(A\\cap B).\n\\end{align*}\n\\]\n\n\n\n\n&lt;&gt;:12: SyntaxWarning: invalid escape sequence '\\B'\n&lt;&gt;:13: SyntaxWarning: invalid escape sequence '\\A'\n&lt;&gt;:12: SyntaxWarning: invalid escape sequence '\\B'\n&lt;&gt;:13: SyntaxWarning: invalid escape sequence '\\A'\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_2631/1376407110.py:12: SyntaxWarning: invalid escape sequence '\\B'\n  venn.get_label_by_id('10').set_text('A\\B')\n/var/folders/gx/6w78f6997l5___173r25fp3m0000gn/T/ipykernel_2631/1376407110.py:13: SyntaxWarning: invalid escape sequence '\\A'\n  venn.get_label_by_id('01').set_text('B\\A')\n\n\n\n\n\n\n\n\n\n\n\n2.6 条件付き確率の定義\n\n\n\n\n\n\n定義（条件付き確率）\n\n\n\n\\(A,B\\subset\\Omega\\) を事象とする． 事象 \\(A\\) が起こった場合の，事象 \\(B\\) の条件付き確率とは， \\[\nP(B|A):=\\begin{cases}\\frac{P(A\\cap B)}{P(A)}&P(A)\\ne0\\;\\text{のとき}\\\\0&P(A)=0\\;\\text{のとき}\\end{cases}\n\\] という値を指す．10\n\n\n\n\n2.7 式(1.7) p.7の証明\n\n\n\n\n\n\n式(1.7) p.7\n\n\n\n\\(A,B\\subset\\Omega\\) を事象，\\(P(A)&gt;0\\) とする． \\[\nP(A|B)+P(A^\\complement|B)=1.\n\\]\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\[\n\\begin{align*}\n&\\quad P(A|B)+P(A^\\complement|B)\\\\\n&=\\frac{P(A\\cap B)}{P(B)}+\\frac{P(A^\\complement\\cap B)}{P(B)}\\\\\n&\\overset{\\text{[P2]}}{=}\\frac{P((A\\cap B)\\sqcup (A^\\complement\\cap B))}{P(B)}\\\\\n&=\\frac{P(B)}{P(B)}=1.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#sec-independent",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#sec-independent",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "3 【重要概念】統計的独立性",
    "text": "3 【重要概念】統計的独立性\n\n3.1 定義\n\n\n\n\n\n\n定義（独立性）\n\n\n\n2つの事象 \\(A,B\\subset\\Omega\\) が独立であるとは，次を満たすことをいう： \\[\nP(A\\cap B)=P(A)P(B).\n\\] このとき， \\(A\\perp\\!\\!\\!\\perp B\\) と表す．\n\n\nこの式は本書p.7 (1.8)式に一致している．これを「積の公式」として導出しているが，これは実は独立性の定義とすべき性質である．その意味するところを次節で解説する．\n\n\n3.2 条件付き確率による特徴付け\nSection 2 で「数学では何を定義として採用するかに任意性がある場合が多い」と言った．今回の「独立性」概念も，2つの同値な定義がある．しかし，「唯一やってはいけないことは定義が曖昧な状態で進むことである」とも言った．従って，どちらか片方を定義とし，「定義ともう一つの条件が同値である」という命題が生まれることになる．\nこの形の命題のことを（数学概念の）特徴付け という．このことを解説するWikipediaページもある．\n\n\n\n\n\n\n命題（独立性の特徴付け）\n\n\n\n2つの事象 \\(A,B\\subset\\Omega\\) について，次の2条件は同値：\n\n\\(A,B\\) は独立である：\\(A\\perp\\!\\!\\!\\perp B\\)．\n\\(P(B|A)=P(B)\\)．\n\n\n\n\n\n\n\n\n\n証明\n\n\n\n\\(P[A]=0\\) の場合，任意の \\(B\\) について(1),(2)はいずれも常に成り立つ． あとは，\\(P[A]\\ne0\\)の場合を考える． すると，条件付き確率の定義 \\[P[A\\cap B]=P[A]P[B|A]\\] を考えれば，この右辺が\\(P[A]P[B]\\)に等しいことと，\\(P[B|A]=P[B]\\)であることとは同値．\n\n\n本書では2.の性質の方を定義としているが， \\(P(B|A)\\) という量は， \\(P(A)=0\\) の場合に定義に任意性が残る．従って，1.の方が定義として明瞭ということになる．\nさらに重要なことには，1.の方が一般個数の事象 \\(A_1,\\cdots,A_n\\) の場合に「独立性」の概念の拡張が可能であり，より本質的な定義だと思われる，ということが確率論の示唆である．実は，無限個の事象が独立であることも同様に定義する．\n\n\n\n\n\n\n定義（独立性）\n\n\n\n集合族 \\(\\{A_\\lambda\\}_{\\lambda\\in\\Lambda}\\subset\\mathcal{F}\\) が独立であるとは，任意の \\(n\\in\\mathbb{N}\\) 個の相異なる元 \\(A_{\\lambda_1},\\cdots,A_{\\lambda_n}\\) に対して， \\[P[A_{\\lambda_1}\\cap\\cdots\\cap A_{\\lambda_n}]=P[A_{\\lambda_1}]\\cdots P[A_{\\lambda_n}]\\] が成り立つことをいう．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#余談数学について",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#余談数学について",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "4 【余談】数学について",
    "text": "4 【余談】数学について\nここまでを読んだ読者の中で，「集合」「写像」の言葉に，定義が十分に提示されていないと感じたものがあるなら，あなたは極めて筋が良い．実は，これらの裏に全て厳密な定義があるのが数学であるが，今回は確率論に集中するために省いた．\n実際，確率論をKolmogorovによる確率の公理的な定義 (Kolmogorov, 1931) から始まる数学分野だとするならば，これはまだ100年の歴史もない，数学分野にしては極めて珍しい若い分野である．\n確率論の確率が遅れた理由は，「確率」の概念がつかみどころのない日常に根ざした概念であり，抽象化が本質的に難しいこともあるだろうが，第一に「集合」「写像」といった概念が十分に数学者の間で理解が深まるのを待つ必要があったということがある．\n現代の確率論では，「確率は測度の特別なものである」という態度をとっていることは本文中でも述べたが，この「測度」という概念の成立が，そもそもLebesgueによる積分論が確立される20世紀に入るのを待つ必要があった．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理1.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理1.html#footnotes",
    "title": "法律家のための統計数理（１）確率論入門",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(McElreath, 2020) 1.3節．頻度論はさらに「仮想的な反復」(imaginary resampling of data)を想定する，という性質を除けば，不確実性が観測からくるもののみである特別な場合が頻度論であると捉えられる．↩︎\n(McElreath, 2020) 1.3節，(Rubin, 1984)．↩︎\nWikipediaページ確率の公理も参照．↩︎\n関数とは，入力と出力の集合 \\(X,Y\\) の間に定まる対応であって，任意の入力 \\(x\\in X\\) に対してただ一つの出力 \\(y\\in Y\\) が対応するもののことをいう．この対応を \\(f(x)=y\\) と表す．↩︎\n後ろの2条件[P2], [P3] のみを満たす関数 \\(P\\) は「測度」という．そのため，確率は測度でもある．数学用語では「確率分布」は「確率測度」ともいう（例えばこのwikipediaページ）．↩︎\n[P1] などの P は Probability のつもりである．↩︎\n集合にも公理があり，現代数学はZFC公理系の下で展開される．が，ここでは深入りしない．↩︎\n\\(\\lnot A\\) という記法について，\\(\\lnot\\) は論理記号であるから，集合 \\(A\\) に用いることは好ましくない．↩︎\n確率は必ず\\(0\\)から\\(1\\)の値を取る，ということを主張している命題である．初学者はこれが「示すべき内容」として提示されていることに戸惑いを覚えるだろうが，現代数学では「これが示せるような必要最小限の定義が見つかった」ことに価値を見出す．↩︎\nここでは \\(P(A)=0\\) の場合は \\(0\\) としたが，実際はどんな値でも良い．↩︎"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "",
    "text": "Bayesian probability theory is sometimes called ‘common sense, ampliﬁed’. (MacKay, 2003, p. 293)\nシリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html#今回の内容",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html#今回の内容",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "1 今回の内容",
    "text": "1 今回の内容\n第一審の裁判において，事実認定が中心的な問題である．この際に起き得る誤謬を，ベイズの方法を用いてどう回避できるか？という例が３つ挙げられている．\n\n1.1 主観確率の基本的な計算方法と捜査官の誤謬\n\n\n\n\n\n\n問題1-1\n\n\n\n殺人事件の加害者はAとBのどちらか？利用可能な情報は次のみ：被疑者のA, Bは同居しており，そのうちどちらかが加害者であることはわかっているものとする．\n\n台所に左利きの包丁があった．Aが左利きである確率はいくらか？ただし，人間が左利きである確率は1割とする．\nAは左利きであること，被害者の外傷の部位や凶器の形状から加害者も左利きであったことは確定的であるとする．Aに逮捕令状を出しても良いだろうか？ただし，利き手の情報を除けば，AかBかは完全に五分五分であるとする．なお，共犯はなく，どちらか一方の単独犯であることも確実であるとする．\n\n\n\n\n\n\n\n\n\n結論\n\n\n\n\nAもBも左利きである可能性があるため，5割より少し大きい．\nAもBも左利きである可能性があるため，9割強である．95%を一つの基準にするなら，逮捕令状は出すべきではない．\n\n\n\nこの問題のポイントは「条件付き確率」への理解である．\n\n\n\n\n\n\n論証\n\n\n\n\n事象を \\[\nA:=\\left\\{\\text{Aは左利きである}\\right\\}\n\\] \\[\nB:=\\left\\{\\text{Bは左利きである}\\right\\}\n\\] と定めると， \\[\n\\begin{align*}\n&\\quad\\;\\operatorname{P}[A\\cup B]\\\\\n&=\\operatorname{P}[A]+\\operatorname{P}[B]-\\operatorname{P}[A\\cap B]\\\\\n&=\\frac{19}{100}\n\\end{align*}\n\\] これより，条件付き確率の定義から \\[\n\\begin{align*}\n\\operatorname{P}[A|A\\cup B]&=\\frac{\\operatorname{P}[A]}{\\operatorname{P}[A\\cup B]}\\\\\n&=\\frac{10}{19}\\approx52.6\\%\n\\end{align*}\n\\]\n\n\n\n今回の肝は，事象 \\(A,B,C\\) を互いに独立に設定したために，各積事象 \\(A\\cap B,B\\cap C,C\\cap A\\) が悉く計算可能なものとして得られた，という点である．これを計算しておけば，欲しい値がこれらの言葉で得られているから，答えまで一本道で辿り着ける，という訳である．\n\n\n\n\n\n\nMonty Hall問題\n\n\n\nモンティ・ホール問題 も条件付けを正しく行えない（「何が分母か」を分別せず，違う次元の話を混同する）ことによって起こるパラドックスの有名な例である．\n\n\n\n\n\n\n\n\nまとめ\n\n\n\n「捜査官の誤謬」は「条件付け」を正しく行わないことにより起こる誤謬である．これを回避するには，独立な事象 \\(A,B\\) を抽出し，これらの確率を計算し，最終的に求めたい確率が何かを正しく特定することが重要である．\n\n\n\n\n1.2 ベイズの公式と検察官の誤謬\n\n\n\n\n\n\n問題1-2\n\n\n\nドーピング検査の結果から，ある日本選手Iが金メダルを剥奪された．弁護人としては，どのような弁護の筋があるか？\n\n本ドーピング検査において，禁止薬物をを用いていない人に対して陽性の結果が出る（偽陽性）確率は1%で，逆の偽陰性も1%である．\n日本選手で，禁止薬物を用いている割合は0.1%とする．当該日本選手Iもこの割合に従うものとする（とりわけ禁止薬物を使っていそうな理由・いそうでない理由はないものとする）．\n\n\n\n各事象を \\[\nA:=\\left\\{\\text{ I は薬物を使用していた}\\right\\}\n\\] \\[\nE:=\\left\\{\\text{ I に陽性反応が出た}\\right\\}\n\\] と設定する．今回は \\(A,E\\) は独立ではないことに注意．例えば，後からわかることだが， \\[\\operatorname{P}[A\\cap E]\\ne\\operatorname{P}[A]\\operatorname{P}[E]\\] である．ここで，条件付き確率の計算の問題に分け入ることになる．今回与えられている条件はそれぞれ，\n\\[\n\\begin{align*}\n\\text{1.}&\\qquad\\operatorname{P}[\\overline{E}|A]=\\frac{1}{100},\\\\\n&\\qquad\\operatorname{P}[E|\\overline{A}]=\\frac{1}{100}\\\\\n\\text{2.}&\\qquad\\operatorname{P}[A]=\\frac{1}{1000}\n\\end{align*}\n\\]\nと表現できており，知りたい値は，今現在Iが本当に薬を使っていたという確率 \\(\\operatorname{P}[A|E]\\) である．\n実は，これは全く大きな値ではない！これは，そもそも薬物を使っている人が少なく，健常な人の方が大多数であるために，検査で陽性が出たからといってそれが本当に薬物を使っている人から出た「真の陽性」である確率が極めて小さくなってしまうという普遍的な現象である．\n\n\n\n\n\n\n証明\n\n\n\n求めたい量 \\(\\operatorname{P}[A|E]\\) は \\[\n\\begin{align*}\n&\\quad\\;\\operatorname{P}[A|E]\\\\\n&=\\frac{\\operatorname{P}[A\\cap E]}{\\operatorname{P}[E]}\\\\\n&\\overset{\\text{(2)}}{=}\\frac{\\operatorname{P}[E|A]\\operatorname{P}[A]}{\\operatorname{P}[E]}\\\\\n&\\overset{\\text{(3)}}{=}\\frac{\\operatorname{P}[E|A]\\operatorname{P}[A]}{\\operatorname{P}[E|A]\\operatorname{P}[A]+\\operatorname{P}[E|\\overline{A}]\\operatorname{P}[\\overline{A}]}\n\\end{align*}\n\\tag{1}\\] と式変形できる．この右辺は，全て既知の値で表現できている．\nなお，途中の式変形については，条件付き確率の定義から\n\\[\n\\operatorname{P}[A\\cap E]=\\operatorname{P}[E|A]\\operatorname{P}[A]\n\\tag{2}\\]\nと，全確率の法則\n\\[\n\\begin{align*}\n&\\quad\\;\\operatorname{P}[E|A]\\operatorname{P}[A]+\\operatorname{P}[E|\\overline{A}]\\operatorname{P}[\\overline{A}]\\\\\n&=\\frac{\\operatorname{P}[E\\cap A]}{\\operatorname{P}[A]}\\operatorname{P}[A]\\\\\n&\\qquad\\qquad+\\frac{\\operatorname{P}[E\\cap\\overline{A}]}{\\operatorname{P}[\\overline{A}]}\\operatorname{P}[\\overline{A}]\\\\\n&=\\operatorname{P}[E\\cap A]+\\operatorname{P}[E\\cap\\overline{A}]\\\\\n&=\\operatorname{P}[E]\n\\end{align*}\n\\tag{3}\\] とを用いた．\n\n\n実際に計算してみると， \\[\n\\operatorname{P}[A|E]=\\frac{99}{1098}\\approx9.0\\%.\n\\] 選手Iは実際は薬を使っていない可能性の方がよっぽど高いのである．\n\n\n\n\n\n\nBase rate fallacy\n\n\n\nこの検察官の誤謬は，特に不良品検出の文脈では深刻なバイアスになり，英語では基準確率の誤謬ともいう．1\n\\(n\\) 人の母集団に，ある病気の検査を行うとしよう． \\[\\begin{cases}A_i:=\\left\\{i\\text{は有病}\\right\\},\\\\B_i:=\\left\\{i\\text{は陽性}\\right\\}.\\end{cases}\\quad i\\in[n].\\] としたとき，\n\n\\(\\alpha:=P[B_i|A_i^\\complement]\\) を偽陽率・危険度という．検定一般に言う，第一種の過誤率である．\n患者が有病であるときに陽性が出る確率 \\(1-\\alpha\\) の値を 感度(sensiticity)という．\n\\(\\beta:=P[B_i^\\complement|A_i]\\) を偽陰率という．検定一般に言う，第二種の過誤率である．\n患者が無病であるときに陰性が出る確率 \\(1-\\beta\\) の値を特異度(specificity)という．2 検定一般に言う検出力(power)である．\n\n統計的検定では第一種の過誤率を重く見て，これを制限した上での第二種の過誤率の低さを指標とする．このために「検出力」が重要．一方で失病検査の際は第一種の過誤率が大変重要であり，これに「感度」という名前がついている．\nこのような一般的な設定の下で，陽性の結果を見て，患者の有病率を \\(1-\\beta\\) だと結論づけてしまう誤謬を基準確率の誤謬という．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html#ベイズ統計学",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html#ベイズ統計学",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "2 ベイズ統計学",
    "text": "2 ベイズ統計学\n\n2.1 ベイズの公式\n節 1.2 で使った式変形 式 1 の最左辺と最右辺のみに注目して公式化すると，次のようになる： \nこれを（分割 \\(\\Omega=A\\sqcup\\overline{A}\\) に関する）Bayesの公式という．\nこれは独立性の特徴付け \\[\n\\operatorname{P}[A|E]=\\operatorname{P}[A],\\quad\\operatorname{P}[E|A]=\\operatorname{P}[E]\n\\] の一般化になっているともみれる．\n\n\n2.2 ベイズ統計学\n事前に確率 \\(\\operatorname{P}[A]\\) を想定しておく．そして，\\(A\\) に関連する観測の結果 \\(\\operatorname{P}[E|A]\\) を見てから，ベイズの公式を通じて \\(\\operatorname{P}[A|E]\\) を計算し，事象 \\(A\\) に関する理解を深める営みが，ベイズ統計学の雛形である．\nこの \\(\\operatorname{P}[A]\\) を事前確率，\\(\\operatorname{P}[A|E]\\) を事後確率という．\n\n事象 \\(A\\) として何を選んでも良い．\n事前情報 \\(\\operatorname{P}[A]\\) を推論に取り込む余地がある．\n\\(A\\) と \\(E\\) に対して多様な関係を想定できる．\n\\(\\operatorname{P}[A|E]\\) は一般にグラフの形で与えられるので，（他の統計手法と比べて）情報量が多い．\nベイズの公式が全てであり，何をやっているかがわかりやすい．\n\n点がよくベイズ統計学の美点として挙げられる．\n\n\n2.3 ベイズ計算\n前節で解説した通り，ベイズ統計学はベイズの公式が全てであり，原理的には極めて明快である．では，何が難しいかというと，一般的な形のベイズの公式 \\[\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{\\int_\\Theta p(x|\\theta)p(\\theta)\\,d\\theta}\n\\] は，最も単純な場合でも，計算が不可能であるという点である．積分は現代では高校で習う数学の範囲であるが，実際に計算できる積分など応用の現場では都合よく出てこないのである．\n従って，ベイズ統計学の研究において，計算手法の研究が極めて重要な位置を占める．この分野をベイズ計算というのである．詳しくは ベイズ計算とは何か の記事を参照してほしい．"
  },
  {
    "objectID": "posts/2023/数理法務/法律家のための統計数理2.html#footnotes",
    "href": "posts/2023/数理法務/法律家のための統計数理2.html#footnotes",
    "title": "法律家のための統計数理（２）Bayes の定理",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Arias-Castro, 2022) と (Agresti, 2012, pp. 第2.1.3節 p.39), (Smith, 2010, pp. 22–23) も参照．↩︎\n(Yerushalmy, 1947) ↩︎"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理6.html",
    "href": "posts/2024/数理法務/法律家のための統計数理6.html",
    "title": "法律家のための統計数理（６）GPT 入門",
    "section": "",
    "text": "シリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理6.html#ニューラルネットワークの基礎",
    "href": "posts/2024/数理法務/法律家のための統計数理6.html#ニューラルネットワークの基礎",
    "title": "法律家のための統計数理（６）GPT 入門",
    "section": "1 ニューラルネットワークの基礎",
    "text": "1 ニューラルネットワークの基礎\n高等学校情報科「情報II」教員研修用教材 の 情報とデータサイエンス 第17章「ニューラルネットワークとその仕組み」も参照．\nまさか，この勉強会で扱った内容が，高校範囲に含まれることになるとは思わなかった．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理6.html#トランスフォーマー",
    "href": "posts/2024/数理法務/法律家のための統計数理6.html#トランスフォーマー",
    "title": "法律家のための統計数理（６）GPT 入門",
    "section": "2 トランスフォーマー",
    "text": "2 トランスフォーマー\nこちらの稿 を参照．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理__.html",
    "href": "posts/2024/数理法務/法律家のための統計数理__.html",
    "title": "法律家のための統計数理（？）数理ファイナンス入門",
    "section": "",
    "text": "シリーズトップページはこちら．"
  },
  {
    "objectID": "posts/2024/数理法務/法律家のための統計数理__.html#大王製紙cb発行事件",
    "href": "posts/2024/数理法務/法律家のための統計数理__.html#大王製紙cb発行事件",
    "title": "法律家のための統計数理（？）数理ファイナンス入門",
    "section": "1 大王製紙CB発行事件",
    "text": "1 大王製紙CB発行事件\n\n\n\n\n\n\n大王製紙CB発行事件\n\n\n\n(東京地判, 2018) は\n\n東京証券取引所市場第１部上場会社 Z の取締役 Y1 〜 Y13 に対して，\nZ の株主 X が，\n株主総会決議を経ずに「特に有利な条件」で転換社債型新株予約権付社債を発行したこと（を含め計３点）が，\n会社法 429 条 1 項に基づく損害賠償責任に該当する\n\nとして訴えを起こした事件である．\n訴えは認められず，控訴も棄却された (潘阿憲, 2020), (川島いづみ, 2021)．\n\n\n\n新株予約権の公正な価値は，一般に，現在の株価，権利行使価額（転換価額），行使期間，無リスク利子率，株価変動性（ボラティリティ）等の要素をもとにオプション評価理論に基づき算出された新株予約権の発行時点における価額であると解されており，また，オプション評価理論において，新株予約権等のオプションの価値を評価する手法としては，ブラック＝ショールズ公式，二項モデル，モンテカルロ・シミュレーションといった複数の手法があるとされているが，評価に採用する要素及びその数値が同一であれば，算出される結果は基本的に等しくなるとされている。\n\n\nもっとも，証拠（甲３３，３４，丙４９の１・２）によれば，ボラティリティ（株価変動性）や無リスク利子率については，いかなる数値を採用するかにつき一定の裁量の余地があることが認められ，その意味で新株予約権の理論価値は常に機械的・一義的に定まるものではない。"
  },
  {
    "objectID": "static/Posters/統数研OH.html",
    "href": "static/Posters/統数研OH.html",
    "title": "統数研オープンハウス",
    "section": "",
    "text": "PDMP による非絶対連続分布からのサンプリング\n\n\n\n2025-05-23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n連続時間アルゴリズムへの進化\n\n\n\n2024-05-24\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/Posters/統数研OH.html#sec-ISM-openhouse2025",
    "href": "static/Posters/統数研OH.html#sec-ISM-openhouse2025",
    "title": "統数研オープンハウス",
    "section": "2025 年オープンハウス",
    "text": "2025 年オープンハウス\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 23rd, 2025\nISM\n\n\n\n\n\n\n\nオープンハウス案内\n\n\n\n\n\nポスター"
  },
  {
    "objectID": "static/Posters/統数研OH.html#sec-ISM-openhouse2024",
    "href": "static/Posters/統数研OH.html#sec-ISM-openhouse2024",
    "title": "統数研オープンハウス",
    "section": "2024 年オープンハウス",
    "text": "2024 年オープンハウス\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nMay 24th, 2024\nISM and online (Hybrid)\n\n\n\n\n\n\n\nオープンハウス案内\n\n\n\n\n\nポスター"
  },
  {
    "objectID": "static/Seminars/SéminaireDeStatistique.html",
    "href": "static/Seminars/SéminaireDeStatistique.html",
    "title": "Séminaire de Statistique",
    "section": "",
    "text": "確率過程\nキーワード：semimartingale (Métivier, 1982), Malliavin calculus (Nualart and Nualart, 2018), convergence of processes (Jacod and Shiryaev, 2003)\n\n(Roberts et al., 1997), (Christensen et al., 2005) の証明の semi-martingale の視点からの再定式化\n(Chevallier et al., 2023), (Winkler et al., 2024) などの状態空間が変化する過程の列の収束の証明は完成されていない．semi-martingale の視点からの厳密な定式化を目指す．\n\n\\(\\mathcal{P}(\\mathbb{R}^d)\\) の幾何学\nキーワード：JKO scheme (Figalli and Glaudo, 2023), Fisher-Rao geometry (Ay et al., 2017), Wasserstein geometry (Villani, 2003)\n\ntempering path の Fisher-Rao 幾何に関する解析 (Aubin-Frankowski et al., 2022), (Chopin et al., 2023) の証明を，Wasserstein 幾何に関する勾配流の枠組みと同様に，勾配流の数学 (Ambrosio et al., 2008) を通じて定式化できないか？\n平均場 Langevin 拡散の \\(\\mathcal{P}(\\mathbb{R}^d)\\) 勾配流としての解釈．\n(Hoffman and Ma, 2020) の過程を \\(d\\to\\infty\\) の極限をとったら，変化点を持つ過程に収束する？最適化とサンプリングの中間を取れば，特定の分布族の中で KL の意味で真の分布との距離を最小にする点に収束する？\n\n\n統計的学習理論\nキーワード：empirical process (Giné and Nickl, 2021), learning theory (Bach, 2024), PAC-Bayes (Alquier, 2024)\n\n早期停止に関する研究（離散拡散モデル (Zhang et al., 2024), ロジスティック回帰 (Wu et al., 2025) ）はリスク評価を各時点（早期停止点や収束点など）に対して静的に行っているのみである．理論の確率過程化ができないか？\nSGD path を追う研究 (Glasgow, 2024), (Berthier et al., 2024) もまだ静的な・停止時を用いた解析にとどまっている．"
  },
  {
    "objectID": "static/Seminars/SéminaireDeStatistique.html#テーマ",
    "href": "static/Seminars/SéminaireDeStatistique.html#テーマ",
    "title": "Séminaire de Statistique",
    "section": "",
    "text": "確率過程\nキーワード：semimartingale (Métivier, 1982), Malliavin calculus (Nualart and Nualart, 2018), convergence of processes (Jacod and Shiryaev, 2003)\n\n(Roberts et al., 1997), (Christensen et al., 2005) の証明の semi-martingale の視点からの再定式化\n(Chevallier et al., 2023), (Winkler et al., 2024) などの状態空間が変化する過程の列の収束の証明は完成されていない．semi-martingale の視点からの厳密な定式化を目指す．\n\n\\(\\mathcal{P}(\\mathbb{R}^d)\\) の幾何学\nキーワード：JKO scheme (Figalli and Glaudo, 2023), Fisher-Rao geometry (Ay et al., 2017), Wasserstein geometry (Villani, 2003)\n\ntempering path の Fisher-Rao 幾何に関する解析 (Aubin-Frankowski et al., 2022), (Chopin et al., 2023) の証明を，Wasserstein 幾何に関する勾配流の枠組みと同様に，勾配流の数学 (Ambrosio et al., 2008) を通じて定式化できないか？\n平均場 Langevin 拡散の \\(\\mathcal{P}(\\mathbb{R}^d)\\) 勾配流としての解釈．\n(Hoffman and Ma, 2020) の過程を \\(d\\to\\infty\\) の極限をとったら，変化点を持つ過程に収束する？最適化とサンプリングの中間を取れば，特定の分布族の中で KL の意味で真の分布との距離を最小にする点に収束する？\n\n\n統計的学習理論\nキーワード：empirical process (Giné and Nickl, 2021), learning theory (Bach, 2024), PAC-Bayes (Alquier, 2024)\n\n早期停止に関する研究（離散拡散モデル (Zhang et al., 2024), ロジスティック回帰 (Wu et al., 2025) ）はリスク評価を各時点（早期停止点や収束点など）に対して静的に行っているのみである．理論の確率過程化ができないか？\nSGD path を追う研究 (Glasgow, 2024), (Berthier et al., 2024) もまだ静的な・停止時を用いた解析にとどまっている．"
  },
  {
    "objectID": "static/Materials.html#séminaire-de-statistique",
    "href": "static/Materials.html#séminaire-de-statistique",
    "title": "Materials",
    "section": "Séminaire de Statistique",
    "text": "Séminaire de Statistique"
  },
  {
    "objectID": "static/Seminars/SéminaireDeStatistique.html#夏",
    "href": "static/Seminars/SéminaireDeStatistique.html#夏",
    "title": "Séminaire de Statistique",
    "section": "2025 夏",
    "text": "2025 夏\n\n\n\n\n\n\n\n\nLocation\nTime\n\n\n\n\nISM 3F\n-"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#関連記事",
    "href": "posts/2025/DiffusionModels/CTDDM.html#関連記事",
    "title": "Masked Diffusion Models",
    "section": "5 関連記事",
    "text": "5 関連記事\n\n\n\n\n\n\n\n\n\n\n\n離散空間上の拡散確率モデル\n\n\n位相構造を取り入れた次世代の構造生成へ\n\n\n\n2024-08-09\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型の実装\n\n\nPyTorchによるハンズオン\n\n\n\n2024-08-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n拡散模型\n\n\n深層生成モデル６\n\n\n\n2024-02-14\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#demo",
    "href": "posts/2025/DiffusionModels/CTDDM.html#demo",
    "title": "Masked Diffusion Models",
    "section": "2 Demo",
    "text": "2 Demo\nWe carry out unconditional generation from a toy data distribution \\(\\pi_{\\text{data}}\\) on \\(5=\\{0,1,2,3,4\\}\\), by running the exact reverse kernel of the absorbing (masked) forward process. Therefore, no neural network training is involved.\n\n2.1 Setup\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n\n\np_data = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\n\nWe will represent the MASK as \\(-1\\). The state space is then \\(E:=5 \\cup \\{-1\\}\\).\n\nMASK = -1\n\nAn important design choice in the forward process is the noising schedule \\(\\alpha_t\\), which can be interpreted as survival probability and satisfy the following relatioship with the jump intensity \\(\\beta_t\\): \\[\n\\alpha_t=\\exp\\left(-\\int^t_0\\beta_s\\,ds\\right),\\qquad t\\in[0,1].\n\\]\nLet us keep it simple and set \\(\\alpha_t=t\\). To achive this, we need to set \\[\n\\beta_t=\\frac{1}{1-t},\\qquad t\\in[0,1),\n\\] which is clearly diverging as \\(t\\to1\\). This is to ensure the process to converge in finite time.\n\nT = 10  # number of steps\nalpha = np.linspace(1.00, 0.00, T+1)\n\n\n\n2.2 Backward Transition Kernel\nIn this setting, the backward transition kernel \\(p(x_{t-1} | x_t)\\) satisfies \\[\n\\operatorname{P}[X_{t-1}=-1|X_t=-1]=\\frac{1 - \\alpha_{t-1}}{1 - \\alpha_t}.\n\\] In the other cases, the \\(x_{t-1}\\) should be determined according to \\(\\pi_{\\text{data}}\\), which is unavailable in a real setting of course.\n\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])  # length T\n\n\ndef reverse_sample(num_samples: int, p_unmask: np.ndarray):\n    \"\"\"\n    Start from x_T = MASK for all samples, apply the exact reverse transitions down to t=0.\n    Returns x_0 samples in 5 = {0,1,...,4}.\n    \"\"\"\n    x_t = np.full(num_samples, MASK, dtype=int)\n    hist = np.empty((T+1, num_samples), dtype=int)\n    hist[0] = x_t.copy()\n    for t in range(T, 0, -1):\n        idx_mask = np.where(x_t == MASK)[0]  # masked indices\n        if idx_mask.size &gt; 0:\n            u = rng.random(idx_mask.size)\n            unmask_now = idx_mask[u &lt; p_unmask[t-1]]  # indices that are going to be unmasked\n            if unmask_now.size &gt; 0:\n                cats = rng.choice(5, size=unmask_now.size, p=p_data)\n                x_t[unmask_now] = cats\n        hist[T-t+1] = x_t.copy()\n\n    # At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,\n    # but numerically we ensure no MASK remains:\n    assert np.all(x_t != MASK), \"Some samples remained MASK at t=0, which should not happen.\"\n    return x_t, hist\n\n\n2.2.1 A Note on Alternative Sampling Strategies\nNote that we need not to obey this exact backward transition kernel to sample from the data distribution.\nFor example, remasking (Zhao et al., 2024), (Gat et al., 2024), (Wang et al., 2025), a form of predictor-corrector sampling, can be incorporated to improve sample quality, mitigating numerical errors.\nRecently, sampling time path planning (Peng et al., 2025), (Kim et al., 2025), (Rout et al., 2025) has been proposed to improve sample quality and model log-likelihood.\n\n\n\n2.3 Honest Sampling\n\nN = 100_000  # size of sample to get\nx0_samples, hist = reverse_sample(N, p_unmask)\n\nWe first make sure our implementation is correct by checking the empirical distribution of the samples agrees with the true distribution.\n\n\nCode\ncounts = np.bincount(x0_samples, minlength=5).astype(float)\np_emp = counts / counts.sum()\n\nprint(\"Toy data marginal p_data:\", p_data.round(4))\nprint(\"Empirical p after reverse sampling:\", p_emp.round(4))\n\n# ---------- Bar chart: p_data vs empirical ----------\nxs = np.arange(5)\nwidth = 0.4\nplt.figure(figsize=(6,3))\nplt.bar(xs - width/2, p_data, width=width, label=\"true p_data\")\nplt.bar(xs + width/2, p_emp, width=width, label=\"empirical (reverse)\")\nplt.title(\"Reverse samples match the data marginal\")\nplt.xlabel(\"category id\")\nplt.ylabel(\"probability\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nToy data marginal p_data: [0.4  0.3  0.18 0.1  0.02]\nEmpirical p after reverse sampling: [0.3979 0.3002 0.1828 0.0992 0.0198]\n\n\n\n\n\n\n\n\n\nMaking sure everything is working, we plot 1000 sample paths from the reverse process.\n\n\nCode\nn_samples_to_plot = min(1000, hist.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist.shape[0]), hist[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see a relatively equal number of jumps per step:\n\njump_counts = np.zeros(T)\nfor i in range(10):\n    jump_counts[i] = sum(hist[i] != hist[i+1])\nprint(jump_counts)\n\n[ 9916. 10017. 10001. 10143. 10062. 10195.  9815. 10005.  9859.  9987.]\n\n\nThis is because we set \\(\\alpha_t=1-t\\) to be linear.\n\n\nCode\n# ---------- Plot schedule α_t (survival probability) ----------\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.4 Choice of \\(\\alpha_t\\)\n\\(\\alpha_t\\) controls the convergence rate of the forward process.\nWe change \\(\\alpha_t\\) to see the impact on the sampling accuracy. (There should be no influence as long as the exact backward kernel is used.)\nLet us change \\(\\alpha_t\\) to be an exponential schedule:\n\n\nCode\nalpha_exp = np.exp(np.linspace(0.00, -10.00, T+1))\np_unmask_exp = (alpha_exp[:-1] - alpha_exp[1:]) / (1.0 - alpha_exp[1:])\n\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha_exp, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nx0_exp, hist_exp = reverse_sample(N, p_unmask_exp)\n\n\n\nCode\nn_samples_to_plot = min(1000, hist_exp.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist_exp.shape[0]), hist_exp[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see many jumps happen in the very last steps. This is certainly not what we want.\nWe spend almost half of the computational time (up to 6th step) in simulating the phantom jumps which just do not happen. The same concern was raised by (Chao et al., 2025).\nHowever, the accuracy is same, as the exact kernel is used to simulate, if the computational cost might be different.\n\n\nCode\ndef calc_l1_kl(x0_samples, split = 10):\n    chunks = np.array_split(x0_samples, split)\n    counts = np.array([np.bincount(chunk, minlength=5).astype(float) for chunk in chunks])\n\n    p_emp = counts / counts.sum(axis=1)[0]\n    l1 = np.abs(p_emp - p_data).sum(axis=1).mean()\n    kl = (np.where(p_emp &gt; 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).mean()\n    return l1, kl\n\n\n\nl1, kl = calc_l1_kl(x0_samples)\nprint(\"Linear Schedule: L1 distance:\", round(l1, 6), \"   KL(p_emp || p_data):\", round(kl, 6))\n\nl1_exp, kl_exp = calc_l1_kl(x0_exp)\nprint(\"Exponential Schedule: L1 distance:\", round(l1_exp, 6), \"   KL(p_emp || p_data):\", round(kl_exp, 6))\n\nLinear Schedule: L1 distance: 0.0147    KL(p_emp || p_data): 0.000196\nExponential Schedule: L1 distance: 0.0148    KL(p_emp || p_data): 0.000232\n\n\n\n\n2.5 Corrector Sampling\n\ndef reverse_sample_corrector(num_samples: int, p_unmask: np.ndarray):\n    \"\"\"\n    adding corrector sampling steps to reverse_sample()\n    \"\"\"\n    x_t = np.full(num_samples, MASK, dtype=int)\n    hist = np.empty((T+1, num_samples), dtype=int)\n    hist[0] = x_t.copy()\n    for t in range(T, 0, -1):\n        idx_mask = np.where(x_t == MASK)[0]  # masked indices\n        if idx_mask.size &gt; 0:\n            u = rng.random(idx_mask.size)\n            unmask_now = idx_mask[u &lt; p_unmask[t-1]]  # indices that are going to be unmasked\n            if unmask_now.size &gt; 0:\n                cats = rng.choice(5, size=unmask_now.size, p=p_data)\n                x_t[unmask_now] = cats\n        hist[T-t+1] = x_t.copy()\n\n    # At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,\n    # but numerically we ensure no MASK remains:\n    assert np.all(x_t != MASK), \"Some samples remained MASK at t=0, which should not happen.\"\n    return x_t, hist"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#introduction",
    "href": "posts/2025/DiffusionModels/CTDDM.html#introduction",
    "title": "Masked Diffusion Models",
    "section": "1 Introduction",
    "text": "1 Introduction\n\n1.1 A Time-Agnostic Learning Framework\nThe absorbing process, a.k.a. masked diffusion, has a unique characteristic as a forward process in a discrete denoising diffusion model; it offers a time-agnostic learning to unmask training framework.\nWhen the state space is \\(E^d\\) where \\(E=\\{0,\\cdots,K-1\\}\\) is finite, a current practice is to learn a neural network \\(p_\\theta\\) based on a loss given by \\[\n\\mathcal{L}(\\theta):=\\int^1_0\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\operatorname{E}\\left[\\sum_{i=1}^d\\log p_\\theta(X_0^i|X_t)\\right]\\,dt,\n\\tag{1}\\] where \\(\\alpha_t\\) is a noising schedule, determining the convergence speed of the forward process.1\nThe expectation in (1) is exactly a cross-entropy loss. Therefore, the loss (1) can be understood as a weighted cross-entropy loss, weighted by the noising schedule \\(\\alpha_t\\).\nNote that \\(p_\\theta\\) predicts the true state \\(x_0\\), based on the current state \\(x_t\\), some components of which might be masked. Hence we called this framework learning to unmask.\nNote also that \\(p_\\theta\\) doesn’t take \\(t\\) as an argument (Shi et al., 2024), (Ou et al., 2025), which we call the time-agnostic property, following (Zheng et al., 2025).\n\n\n1.2 Choice of \\(\\alpha_t\\)\nThis ‘learning to unmask’ task might be very hard when \\(t\\) is near \\(1\\), since most of the \\(x_t^i\\)’s are still masked.\nFor instance, if we choose a linear schedule \\[\n\\alpha_t=1-t\\qquad(t\\in[0,1])\n\\] the scaler \\(\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}=-t^{-1}\\) before the expectation in (1) puts less weight on large \\(t\\approx1\\), while puts much more weight on small \\(t\\approx0\\), where most of the \\(x_t^i\\)’s should be already unmasked.\nHence, selecting the schedule \\(\\alpha_t\\) to make learning easier can be very effective, for example by reducing the variance of gradient estimator in a SGD algorithm. Actually, this is a part of the technique how (Arriola et al., 2025) achieved their remarkable empirical results.\nThis flexibility of \\(\\alpha_t\\) is why the loss (1) is considered as a potential competitor against the current dominant autoregressive models. In fact, one work (Chao et al., 2025), still under review, claimed their masked diffusion model surpassed the autoregressive model on the task of language modeling, achieving an evaluation perplexity of 15.36 on the OpenWebText dataset.2\nWe briefly discuss their trick and related promising techniques to improve the model, before programming toy examples in Section 2 and Section 3 to deepen our understanding in absorbing forward process.\n\n\n1.3 A Gibbs Sampler Take\nOne problem about the loss (1) is that \\(p_\\theta\\) predicts the unmasked complete sequence in a product form: \\[\np_\\theta(x_0|x_t)=\\prod_{i=1}^d p_\\theta(x_0^i|x_t).\n\\]\nThis should cause no problem when we unmask one component at a time, since it will be a form of ancestral sampling based on disintegration property.\nHowever, when unmasking two or more components simultaneously, for example when the number of steps is less than \\(d\\), the product form assumption will simply introduce a bias, as the data distribution is by no means of product form on \\(E^d\\).\nHere, to recover correctness asymptotically, analogy with Gibbs sampling becomes very important.\nFor example, predictor-corrector technique can be readily employed to mitigate this bias, as discussed in (Lezama et al., 2023), (S. Zhao et al., 2024), (L. Zhao et al., 2024), (Gat et al., 2024), (Wang et al., 2025).\nWe demonstrate this strategy in Section 3.5.\n\n\n1.4 Intermediate States\nAs we mentioned earlier, unmasking can be a very hard task, as closely investigated in (Kim et al., 2025, sec. 3).\nTo alleviate this problem, (Chao et al., 2025) introduced intermediate states by re-encoding the token in a base-\\(2\\) encoding, such as \\[\n5\\mapsto 101.\n\\] The right-hand side needs three steps to be completely unmasked, while the left-hand side only needs one jump.\nTherefore, unmasking can be easier to learn, compared to the original token encoding.\nHowever, this is not the only advantage of intermediate states. (Chao et al., 2025) were able to construct a full predictor \\(p_\\theta\\) without the product form assumption on each token.\nThis approach might act as a block Gibbs sampler and make the convergence faster, as we will discuss later in Section 3.6.\n\n\n1.5 State-Dependent Rate\nAs a function of \\(t\\mapsto\\alpha_t\\), different choices for \\(\\alpha_t\\) seem to make little impact on the total performance of the model, as we observe in our toy example in Section 2.4.\nHowever, if we allow \\(\\alpha_t\\) to depend on the state as in (Shi et al., 2024, sec. 6), I believe the masked diffusion model will start to show its real potential over currently dominant autoregressive framework.\nA problem arises when one tries to learn \\(\\alpha_t\\) at the same time, for example, by including a corresponding term into the loss (1). This will lead to amplified variances of the gradient estimates and unstable training, as reported in (Shi et al., 2024) and (Arriola et al., 2025).\nThe idea of exploiting state-dependent rate is already very common in sampling time (Peng et al., 2025), (Liu et al., 2025), (Kim et al., 2025), (Rout et al., 2025), determining which token to unmask next during the backward sampling, a bit like Monte Carlo tree search in reinforcement learning."
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#discussion",
    "href": "posts/2025/DiffusionModels/CTDDM.html#discussion",
    "title": "Masked Diffusion Models",
    "section": "3 Discussion",
    "text": "3 Discussion\n\n3.1 A Reinforcenment Learning Take\nThe inference step and the sampling step should be decoupled, at least conceptually.\nTo tune the forward process noising schedule \\(\\alpha_t(x)\\), a reinforcement learning framework will be employed, I believe in the near future.\nThis is a variant of meta-learning and, in this way, the unmasking network \\(p_\\theta\\) will be able to efficiently learn the correct dependence structure in the data domain.\nFor example, in language modeling, there is a natural sequential structure, which is partly why autoregressive modeling has been dominant in this domain. However, by learning \\(\\alpha_t\\) in masking process, a much more efficent factorization over texts can be aquired in a data-driven way.\nI even think this \\(\\alpha_t\\) can play an important role just as a word embedding does currently.\nIn the sampling step, a sampling time path planning will greatly enhance sample quality, just as Monte Carlo tree search does in reinforcement learning.\nAs a conclusion, the flexible framework of masked diffusion models will enable a marriage with reinforcement learning and meta learning, which will be a way to overcome the autoregressive modeling framework, because the latter imposes unnecessary sequential inductive bias into the model."
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#future-works",
    "href": "posts/2025/DiffusionModels/CTDDM.html#future-works",
    "title": "Masked Diffusion Models",
    "section": "4 Future Works",
    "text": "4 Future Works\nSince the absorbing process is favoured only because of its time-agnostic property, its ability should be explained separately with the properties of the process.\n\n4.1 A Reinforcenment Learning Take\nThe inference step and the sampling step should be decoupled, at least conceptually.\nTo tune the forward process noising schedule \\(\\alpha_t(x)\\), a reinforcement learning framework will be employed, I believe in the near future.\nThis is a variant of meta-learning and, in this way, the unmasking network \\(p_\\theta\\) will be able to efficiently learn the correct dependence structure in the data domain.\nFor example, in language modeling, there is a natural sequential structure, which is partly why autoregressive modeling has been dominant in this domain. However, by learning \\(\\alpha_t\\) in masking process, a much more efficent factorization over texts can be aquired in a data-driven way.\nI even think this \\(\\alpha_t\\) can play an important role just as a word embedding does currently.\nIn the sampling step, a sampling time path planning will greatly enhance sample quality, just as Monte Carlo tree search does in reinforcement learning.\nAs a conclusion, the flexible framework of masked diffusion models will enable a marriage with reinforcement learning and meta learning, which will be a way to overcome the autoregressive modeling framework, because the latter imposes unnecessary sequential inductive bias into the model.\n\n\n4.2 A Scaling Analysis\nThere are two papers, namely (Santos et al., 2023) and (Winkler et al., 2024), which carry out a scaling analysis as \\(K\\to\\infty\\), in order to bridge the gap between discrete and continuous state spaces.\nIn (Santos et al., 2023, sec. 2.4) and its Appendix C, it is proved that a fairly large class of discrete processes will converge to Ito processes, as \\(K\\to\\infty\\).\nTheir discussion based on a formal argument. They proves that the Kramers-Moyal expansion of the Chapman-Kolmogorov equation of the discrete process converges to that of a Ito process.\nIn other words, they didn’t identify the direct correspondence, for example deriving the exact expression of limiting SDEs.\n(Winkler et al., 2024) builds on their analysis, identifying an OU process on \\(\\mathbb{R}^d\\) corresponds to a Ehrenfest process.\nThis line of research has a lot to do with thermodynamics, and might provide insights into whether images should be modeled discretely or continuously.\nAlso, the limit \\(d\\to\\infty\\) has yet to be explored.\n\n\n4.3 Concluding Remarks\nI believe that masked diffusion modeling can be viewed as a form of Gibbs sampling, with a learned transition kernel.\nMany current practices are based on (uniformly) random scan Gibbs, while the autoregressive models are a fixed scan Gibbs.\nMost recent improvements are based upon ideas from reinforcement learning and meta learning, where an optimal order to unmask components is pursued.\nThis point of view might not be only an abstract nonsense. I actually believe that this point of view will be fruitful in the future."
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#footnotes",
    "href": "posts/2025/DiffusionModels/CTDDM.html#footnotes",
    "title": "Masked Diffusion Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDiscrete Flow Matching (Campbell et al., 2024), (Gat et al., 2024), (Shaul et al., 2025) and simplified Masked diffusion (Shi et al., 2024), (Ou et al., 2025), (Zheng et al., 2025) are different frameworks with different ranges, but both lead to the same training objective (1), when applied to the forward masking process.↩︎\nIn the context of language modeling, the perplexity is defined as \\(2^{l}\\) where \\(l\\) is the average log-likelihood of the test set.↩︎\nOf course, the exact sampling would have been available, for exmple, if we learned the backward intensity as (Campbell et al., 2022). However, these methods have been marginalized due to suboptimal performance.↩︎"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#sec-Demo",
    "href": "posts/2025/DiffusionModels/CTDDM.html#sec-Demo",
    "title": "Masked Diffusion Models",
    "section": "2 Demo",
    "text": "2 Demo\nTo demonstrate what an absorbing process does, we carry out generation from a toy data distribution \\(\\pi_{\\text{data}}\\) on \\(5=\\{0,1,2,3,4\\}\\), by running an exact reverse kernel of the absorbing (masked) forward process.\nTherefore, no neural network training will be involved. A 2d example, which is much more interesting, in Section 3 will basically process in parallel.\n\n2.1 Setup\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n\n\np_data = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\n\nWe will represent the MASK as \\(-1\\). The state space is then \\(E:=5 \\cup \\{-1\\}\\).\n\nMASK = -1\n\nAn important design choice in the forward process is the noising schedule \\(\\alpha_t\\), which can be interpreted as survival probability and satisfy the following relatioship with the jump intensity \\(\\beta_t\\): \\[\n\\alpha_t=\\exp\\left(-\\int^t_0\\beta_s\\,ds\\right),\\qquad t\\in[0,1].\n\\]\nLet us keep it simple and set \\(\\alpha_t=t\\). To achive this, we need to set \\[\n\\beta_t=\\frac{1}{1-t},\\qquad t\\in[0,1),\n\\] which is clearly diverging as \\(t\\to1\\). This is to ensure the process to converge in finite time.\n\nT = 10  # number of steps\nalpha = np.linspace(1.00, 0.00, T+1)\n\n\n\n2.2 The Backward Transition Kernel\nIn this setting, the backward transition kernel \\(p(x_{t-1} | x_t)\\) satisfies \\[\n\\operatorname{P}[X_{t-1}=-1|X_t=-1]=\\frac{1 - \\alpha_{t-1}}{1 - \\alpha_t}.\n\\] In the other cases, the unmasked values \\(x_{t-1}\\) should be determined according to \\(\\pi_{\\text{data}}\\), which is unavailable in a real setting, of course.\n\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])  # length T\n\n\ndef reverse_sample(num_samples: int, p_unmask: np.ndarray):\n    \"\"\"\n    Start from x_T = MASK for all samples, apply the exact reverse transitions down to t=0.\n    Returns x_0 samples in 5 = {0,1,...,4}.\n    \"\"\"\n    x_t = np.full(num_samples, MASK, dtype=int)\n    hist = np.empty((T+1, num_samples), dtype=int)\n    hist[0] = x_t.copy()\n    for t in range(T, 0, -1):\n        idx_mask = np.where(x_t == MASK)[0]  # masked indices\n        if idx_mask.size &gt; 0:\n            u = rng.random(idx_mask.size)\n            unmask_now = idx_mask[u &lt; p_unmask[t-1]]  # indices that are going to be unmasked\n            if unmask_now.size &gt; 0:\n                cats = rng.choice(5, size=unmask_now.size, p=p_data)\n                x_t[unmask_now] = cats\n        hist[T-t+1] = x_t.copy()\n\n    # At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,\n    # but numerically we ensure no MASK remains:\n    assert np.all(x_t != MASK), \"Some samples remained MASK at t=0, which should not happen.\"\n    return x_t, hist\n\n\n2.2.1 A Note on Alternative Sampling Strategies\nNote that we need not to obey this exact backward transition kernel to sample from the data distribution.\nFor example, remasking (Lezama et al., 2023), (S. Zhao et al., 2024), (Gat et al., 2024), (Wang et al., 2025), a form of predictor-corrector sampling, can be incorporated to improve sample quality, mitigating numerical errors, as we will see in Section 3.5.\nRecently, sampling time path planning (Peng et al., 2025), (Liu et al., 2025), (Kim et al., 2025), (Rout et al., 2025) has been proposed to improve sample quality and model log-likelihood, which lay out of the scope of this post.\n\n\n\n2.3 Honest Sampling\n\nN = 100_000  # size of sample to get\nx0_samples, hist = reverse_sample(N, p_unmask)\n\nWe first make sure our implementation is correct by checking the empirical distribution of the samples generated agrees with the true distribution.\n\n\nCode (tap me)\ncounts = np.bincount(x0_samples, minlength=5).astype(float)\np_emp = counts / counts.sum()\n\nprint(\"Toy data marginal p_data:\", p_data.round(4))\nprint(\"Empirical p after reverse sampling:\", p_emp.round(4))\n\n# ---------- Bar chart: p_data vs empirical ----------\nxs = np.arange(5)\nwidth = 0.4\nplt.figure(figsize=(6,3))\nplt.bar(xs - width/2, p_data, width=width, label=\"true p_data\")\nplt.bar(xs + width/2, p_emp, width=width, label=\"empirical (reverse)\")\nplt.title(\"Reverse samples match the data marginal\")\nplt.xlabel(\"category id\")\nplt.ylabel(\"probability\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nToy data marginal p_data: [0.4  0.3  0.18 0.1  0.02]\nEmpirical p after reverse sampling: [0.3979 0.3002 0.1828 0.0992 0.0198]\n\n\n\n\n\n\n\n\n\nPerfect! Making sure everything is working, we plot 1000 sample paths from the reverse process.\n\n\nCode (tap me)\nn_samples_to_plot = min(1000, hist.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist.shape[0]), hist[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see a relatively equal number of jumps per step:\n\njump_counts = np.zeros(T)\nfor i in range(10):\n    jump_counts[i] = sum(hist[i] != hist[i+1])\nprint(jump_counts)\n\n[ 9916. 10017. 10001. 10143. 10062. 10195.  9815. 10005.  9859.  9987.]\n\n\nThis is because we set \\(\\alpha_t=1-t\\) to be linear.\n\n\nCode (tap me)\n# ---------- Plot schedule α_t (survival probability) ----------\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.4 Choice of \\(\\alpha_t\\)\n\\(\\alpha_t\\) controls the convergence rate of the forward process.\nWe change \\(\\alpha_t\\) to see the impact on the sampling accuracy. (There should be no influence as long as the exact backward kernel is used.)\nLet us change \\(\\alpha_t\\) to be an exponential schedule:\n\n\nCode (tap me)\nalpha_exp = np.exp(np.linspace(0.00, -10.00, T+1))\np_unmask_exp = (alpha_exp[:-1] - alpha_exp[1:]) / (1.0 - alpha_exp[1:])\n\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha_exp, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn this way, most of the unmasking events should occur in the very last step of the reverse process.\n\nx0_exp, hist_exp = reverse_sample(N, p_unmask_exp)\n\n\n\nCode (tap me)\nn_samples_to_plot = min(1000, hist_exp.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist_exp.shape[0]), hist_exp[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see many jumps happen in the latter half.\nPractically speaking, this is certainly not what we want.\nWe spend almost half of the computational time (up to the 6th step) in simulating the phantom jumps which just do not happen. The same concern was raised by (Chao et al., 2025).\nHowever, the accuracy is same, as the exact kernel is used to simulate, if the computational cost might be different.\n\n\nCode (tap me)\ndef calc_l1_kl(x0_samples, split = 10):\n    chunks = np.array_split(x0_samples, split)\n    counts = np.array([np.bincount(chunk, minlength=5).astype(float) for chunk in chunks])\n\n    p_emp = counts / counts.sum(axis=1)[0]\n    l1 = np.abs(p_emp - p_data).sum(axis=1).mean()\n    l1_var = np.abs(p_emp - p_data).sum(axis=1).var()\n    kl = (np.where(p_emp &gt; 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).mean()\n    kl_var = (np.where(p_emp &gt; 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).var()\n    return l1, l1_var, kl, kl_var\n\nl1, l1_var, kl, kl_var = calc_l1_kl(x0_samples)\nprint(\"Linear Schedule: L1 distance:\", round(l1, 6), \" ± \", round(l1_var, 6), \"   KL(p_emp || p_data):\", round(kl, 6), \" ± \", round(kl_var, 6))\n\nl1_exp, l1_exp_var, kl_exp, kl_exp_var = calc_l1_kl(x0_exp)\nprint(\"Exponential Schedule: L1 distance:\", round(l1_exp, 6), \" ± \", round(l1_exp_var, 6), \"   KL(p_emp || p_data):\", round(kl_exp, 6), \" ± \", round(kl_exp_var, 6))\n\n\nLinear Schedule: L1 distance: 0.0147  ±  3e-05    KL(p_emp || p_data): 0.000196  ±  0.0\nExponential Schedule: L1 distance: 0.0148  ±  4.9e-05    KL(p_emp || p_data): 0.000232  ±  0.0"
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#d-example",
    "href": "posts/2025/DiffusionModels/CTDDM.html#d-example",
    "title": "Masked Diffusion Models",
    "section": "3 2D Example",
    "text": "3 2D Example\n\n3.1 Setup\nWe consider a highly correlated distribution, whose support is degenerated on the diagonal element on \\(5^2\\).\n\n\nCode (tap me)\nK = 5\nMASK = -1\n\n# Base marginal for a single site\np_single = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\np_single /= p_single.sum()\n\n# Build correlated joint with same-parity constraint\nW = np.zeros((K, K), dtype=float)\nfor i in range(K):\n    for j in range(K):\n        if (i % 2) == (j % 2):\n            W[i, j] = p_single[i] * p_single[j]\npi_joint = W / W.sum()\npi_x = pi_joint.sum(axis=1)\npi_y = pi_joint.sum(axis=0)\n\n# Conditionals\ncond_x_given_y = np.zeros((K, K), dtype=float)  # [j, i]\ncond_y_given_x = np.zeros((K, K), dtype=float)  # [i, j]\nfor j in range(K):\n    col = pi_joint[:, j]; s = col.sum()\n    if s &gt; 0:\n        cond_x_given_y[j, :] = col / s\nfor i in range(K):\n    row = pi_joint[i, :]; s = row.sum()\n    if s &gt; 0:\n        cond_y_given_x[i, :] = row / s\n\nfig = plt.figure(figsize=(8, 3.4))\n\n# Heatmap\nax1 = plt.subplot(1, 2, 1)\nim = ax1.imshow(pi_joint, cmap='viridis', aspect='equal')\nax1.set_xlabel('Y')\nax1.set_ylabel('X')\nax1.set_title('Joint Probability Distribution (Heatmap)')\nax1.set_xticks(range(K))\nax1.set_yticks(range(K))\n\n# Value annotation\nfor i in range(K):\n    for j in range(K):\n        ax1.text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\n\nplt.colorbar(im, ax=ax1)\n\n# 3D bar plot\nax2 = plt.subplot(1, 2, 2, projection='3d')\nx = np.arange(K)\ny = np.arange(K)\nX, Y = np.meshgrid(x, y)\nZ = pi_joint\n\nax2.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), \n         0.8, 0.8, Z.ravel(), alpha=0.8, cmap='viridis')\n\nax2.set_xlabel('Y')\nax2.set_ylabel('X')\nax2.set_zlabel('Probability')\nax2.set_title('Joint Probability Distribution (3D)')\nax2.set_xticks(x)\nax2.set_yticks(y)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2 The Backward Transition Kernel\nWe will first consider, again, linear schedule:\n\nT = 10\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\n\n\nCode (tap me)\nplt.figure(figsize=(5, 3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (definition of reverse_sample_pairs)\ndef reverse_sample_pairs(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = pi_joint.ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n\n\n\n\n3.3 Honest Sampling\n\n\nCode (tap me)\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\ndef l1_kl(pairs, split=10):\n    chunks = np.array_split(pairs, split)\n    l1, kl = [], []\n    for chunk in chunks:\n        counts = np.zeros((K, K), dtype=float)\n        for a, b in chunk:\n            counts[a, b] += 1.0\n        pi_emp = counts / counts.sum()\n        \n        eps = 1e-12\n        l1.append(np.abs(pi_emp - pi_joint).sum())\n        nz = (pi_emp &gt; 0) & (pi_joint &gt; 0)\n        kl.append((pi_emp[nz] * np.log((pi_emp[nz] + eps) / pi_joint[nz])).sum())\n    return l1, kl\n\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.024162  ±  4.6e-05    KL(emp || true): 7.286359e-04 ± 7.462192e-08\n\n\nThis is a gradual unmasking time schedule.\n\n\nCode (tap me)\nnew_unmasks_per_step = []\nfor t in range(T):\n    changed1 = (h1[t] == MASK) & (h1[t+1] != MASK)\n    changed2 = (h2[t] == MASK) & (h2[t+1] != MASK)\n    new_unmasks_per_step.append(changed1.sum() + changed2.sum())\n\nplt.figure(figsize=(6, 3))\nplt.plot(range(1, T+1), new_unmasks_per_step, marker=\"o\")\nplt.title(\"Newly unmasked coordinates per step\")\nplt.xlabel(\"reverse step (t→t-1)\")\nplt.ylabel(\"#coords\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.3.1 Larger Step Size\nWhat if we employ a large step size?\nActually, the result doesn’t change. Moreover, the accuracy is higher, since it is equivalent to direct sampling from \\(\\pi_{\\text{data}}\\).\n\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\n\n\nCode (tap me)\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.021245  ±  5e-05    KL(emp || true): 6.510835e-04 ± 1.258177e-07\n\n\n\n\n\n3.4 Coordinate-wise Sampling\nThe joint distribution is unavailable, even if the learning based on the loss (1) has been done perfectly, because of the product form assumption on the neural network predictor \\(p_\\theta\\).3\nWe mock this situation by replacing the joint distribution in the exact kernel in Section 3.2 with a product of its marginals.\n\n\nCode (definition of reverse_sample_incorrect)\ndef reverse_sample_incorrect(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n\n\n\n\nCode (tap me)\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.089135  ±  1.2e-05    KL(emp || true): -4.100716e-02 ± 6.515707e-06\n\n\nThere is a small deviation from the exact kernel, where \\(\\ell^1\\) distance is \\(0.024\\pm0.00005\\).\nWe can easily see, in the empirical distribution, some cells are assinged with positive mass, although there is \\(0\\) probability for them to be sampled.\nThis effect becomes smaller when the number of steps T is large. For example, setting T=1000 gives us with almost same accuracy as the exact kernel.\n\n3.4.1 Larger Step Size\nThe situation gets worse when the step size is large.\n\n\nCode (tap me)\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.85124  ±  4.4e-05    KL(emp || true): -2.884951e-01 ± 1.114368e-05\n\n\nThe error is now significant, because the incorrect product kernel is used every time, as we set \\(T=1\\) meaning unmasking in just one step!\n\n\n\n3.5 Corrector Sampling\nOne remedy is to utilise Gibbs sampling ideas to correct the bias, at the expense of computational cost.\nTo do this, we must identify a Markov kernel that keeps the marginal distribution of \\(X_t\\) invariant for every timestep \\(t\\in[0,1]\\).\nIn our case, we simply add a remasking step, only to those pairs \\((x_t^1,x_t^2)\\)’s which are completely unmasked. As they have some probability of being unmasked simultaneously, re-masking only one of them, this time \\(x_t^1\\), will allow us a second chance to correctly arrive at a correct pair \\((x_t^{1,\\text{corrected}},x_t^2)\\).\n\n\nCode (definition of reverse_sample_correct)\ndef reverse_sample_corrector(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        # corrector step\n        q = 1.0 - p  # masking probability\n        both = (x1 != MASK) & (x2 != MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            will = rng.random(idx.size) &lt; q\n            idx_now = idx[will]\n            if idx_now.size &gt; 0:\n                x1[idx_now] = MASK\n        # predictor step\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n\n\n\n\nCode (tap me)\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_corrector(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs, 10)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.022518  ±  2e-05    KL(emp || true): 4.694469e-05 ± 8.500292e-08\n\n\nWe see a small improvement from the \\(\\ell^1\\) distance of \\(0.0241\\).\nThis is the idea behind the predictor-corrector technique proposed, for example, in (Gat et al., 2024), (S. Zhao et al., 2024), (L. Zhao et al., 2024).\nWe only included one corrector step per reverse step, although more correction will lead to more accurate results.\nThis can be regarded as a form of inference time compute scaling (Wang et al., 2025), although it is a fairly bad idea to wait for the predictor-corrector steps to converge."
  },
  {
    "objectID": "posts/2025/DiffusionModels/CTDDM.html#sec-2d-example",
    "href": "posts/2025/DiffusionModels/CTDDM.html#sec-2d-example",
    "title": "Masked Diffusion Models",
    "section": "3 2D Example",
    "text": "3 2D Example\n\n3.1 Setup\nWe consider a highly correlated distribution, whose support is degenerated on the diagonal element on \\(5^2\\).\n\n\nCode (tap me)\nK = 5\nMASK = -1\n\n# Base marginal for a single site\np_single = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\np_single /= p_single.sum()\n\n# Build correlated joint with same-parity constraint\nW = np.zeros((K, K), dtype=float)\nfor i in range(K):\n    for j in range(K):\n        if (i % 2) == (j % 2):\n            W[i, j] = p_single[i] * p_single[j]\npi_joint = W / W.sum()\npi_x = pi_joint.sum(axis=1)\npi_y = pi_joint.sum(axis=0)\n\n# Conditionals\ncond_x_given_y = np.zeros((K, K), dtype=float)  # [j, i]\ncond_y_given_x = np.zeros((K, K), dtype=float)  # [i, j]\nfor j in range(K):\n    col = pi_joint[:, j]; s = col.sum()\n    if s &gt; 0:\n        cond_x_given_y[j, :] = col / s\nfor i in range(K):\n    row = pi_joint[i, :]; s = row.sum()\n    if s &gt; 0:\n        cond_y_given_x[i, :] = row / s\n\nfig = plt.figure(figsize=(8, 3.4))\n\n# Heatmap\nax1 = plt.subplot(1, 2, 1)\nim = ax1.imshow(pi_joint, cmap='viridis', aspect='equal')\nax1.set_xlabel('Y')\nax1.set_ylabel('X')\nax1.set_title('Joint Probability Distribution (Heatmap)')\nax1.set_xticks(range(K))\nax1.set_yticks(range(K))\n\n# Value annotation\nfor i in range(K):\n    for j in range(K):\n        ax1.text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\n\nplt.colorbar(im, ax=ax1)\n\n# 3D bar plot\nax2 = plt.subplot(1, 2, 2, projection='3d')\nx = np.arange(K)\ny = np.arange(K)\nX, Y = np.meshgrid(x, y)\nZ = pi_joint\n\nax2.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), \n         0.8, 0.8, Z.ravel(), alpha=0.8, cmap='viridis')\n\nax2.set_xlabel('Y')\nax2.set_ylabel('X')\nax2.set_zlabel('Probability')\nax2.set_title('Joint Probability Distribution (3D)')\nax2.set_xticks(x)\nax2.set_yticks(y)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n3.2 The Backward Transition Kernel\nWe will first consider, again, linear schedule:\n\nT = 10\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\n\n\nCode (tap me)\nplt.figure(figsize=(5, 3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe code for backward sampling is basically the same, except for the number of if branch is now four, rather than just one.\n\n\nCode (definition of reverse_sample_pairs)\ndef reverse_sample_pairs(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = pi_joint.ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n\n\n\n\n3.3 Honest Sampling\nAgain, using the exact backward kernel, we are able to reproduce the true joint distribution.\n\n\nCode (tap me)\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\ndef l1_kl(pairs, split=10):\n    chunks = np.array_split(pairs, split)\n    l1, kl = [], []\n    for chunk in chunks:\n        counts = np.zeros((K, K), dtype=float)\n        for a, b in chunk:\n            counts[a, b] += 1.0\n        pi_emp = counts / counts.sum()\n        \n        eps = 1e-12\n        l1.append(np.abs(pi_emp - pi_joint).sum())\n        nz = (pi_emp &gt; 0) & (pi_joint &gt; 0)\n        kl.append((pi_emp[nz] * np.log((pi_emp[nz] + eps) / pi_joint[nz])).sum())\n    return l1, kl\n\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.024162  ±  4.6e-05    KL(emp || true): 7.286359e-04 ± 7.462192e-08\n\n\nNote that since the survival rate \\(\\alpha_t\\) decreases linearly, the number of newly unmasked coordinates per step will decrease exponentially.\n\n\nCode (tap me)\nnew_unmasks_per_step = []\nfor t in range(T):\n    changed1 = (h1[t] == MASK) & (h1[t+1] != MASK)\n    changed2 = (h2[t] == MASK) & (h2[t+1] != MASK)\n    new_unmasks_per_step.append(changed1.sum() + changed2.sum())\n\nplt.figure(figsize=(6, 3))\nplt.plot(range(1, T+1), new_unmasks_per_step, marker=\"o\")\nplt.title(\"Newly unmasked coordinates per step\")\nplt.xlabel(\"reverse step (t→t-1)\")\nplt.ylabel(\"#coords\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.3.1 Larger Step Size\nWhat if we employ a large step size?\nActually, the result doesn’t change. Moreover, the accuracy is higher, since it is equivalent to direct sampling from \\(\\pi_{\\text{data}}\\).\n\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\n\n\nCode (tap me)\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.021245  ±  5e-05    KL(emp || true): 6.510835e-04 ± 1.258177e-07\n\n\n\n\n\n3.4 Coordinate-wise Sampling\nThe joint distribution would be unavailable, even if the learning based on the loss (1) were perfectly done, because of the product form assumption on the neural network predictor \\(p_\\theta\\).3\nWe mock this situation by replacing the joint distribution in the exact kernel, programmed in Section 3.2, with the product of its marginals.\n\n\nCode (definition of reverse_sample_incorrect)\ndef reverse_sample_incorrect(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n\n\n\n\nCode (tap me)\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThen, this time the result is not so good.\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.089135  ±  1.2e-05    KL(emp || true): -4.100716e-02 ± 6.515707e-06\n\n\nThere is a small deviation from the exact kernel, where \\(\\ell^1\\) distance was \\(0.024\\pm0.00005\\).\nWe can easily see, in the empirical distribution, some cells are assinged with positive mass, although the true probability is \\(0\\).\nThis effect becomes smaller when the number of steps T is large. For example, setting T=1000 gives us with almost same accuracy as the exact kernel (although we don’t show it here for brevity).\n\n3.4.1 Larger Step Size\nThe situation gets worse when the step size is large, for example T=1.\n\n\nCode (tap me)\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.85124  ±  4.4e-05    KL(emp || true): -2.884951e-01 ± 1.114368e-05\n\n\nThe error is now significant, because the incorrect product kernel is used every time, as we set \\(T=1\\) meaning unmasking in just one step!\nOne way to fix this is to set T as large as possible, making sure no more than one jump occurs simultaneously.\n\n\n\n3.5 Corrector Sampling\nAnother remedy is to utilise Gibbs sampling ideas to correct the bias, again at the expense of computational cost.\nTo do this, we must identify a Markov kernel that keeps the marginal distribution of \\(X_t\\) invariant for every timestep \\(t\\in[0,1]\\).\nIn our case, let us simply add a re-masking step, only to those pairs \\((x_t^1,x_t^2)\\)’s which are completely unmasked.\nAs there is some probability of having been unmasked simultaneously, re-masking only one of them, this time \\(x_t^1\\), will allow us a second chance to arrive at a correct pair \\((x_t^{1,\\text{corrected}},x_t^2)\\).\n\n\nCode (definition of reverse_sample_correct)\ndef reverse_sample_corrector(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        # corrector step\n        q = 1.0 - p  # masking probability\n        both = (x1 != MASK) & (x2 != MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            will = rng.random(idx.size) &lt; q\n            idx_now = idx[will]\n            if idx_now.size &gt; 0:\n                x1[idx_now] = MASK\n        # predictor step\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size &gt; 0:\n            um1 = rng.random(idx.size) &lt; p\n            um2 = rng.random(idx.size) &lt; p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size &gt; 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size &gt; 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size &gt; 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size &gt; 0:\n            will = rng.random(idx_b1.size) &lt; p\n            idx_now = idx_b1[will]\n            if idx_now.size &gt; 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size &gt; 0:\n            will = rng.random(idx_b2.size) &lt; p\n            idx_now = idx_b2[will]\n            if idx_now.size &gt; 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n\n\n\n\nCode (tap me)\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_corrector(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode (tap me)\nl1, kl = l1_kl(pairs, 10)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n\n\nL1 distance: 0.022518  ±  2e-05    KL(emp || true): 4.694469e-05 ± 8.500292e-08\n\n\nWe see a small improvement from the \\(\\ell^1\\) distance of \\(0.0241\\). The difference is significant in terms of our variance of an order \\(O(10^{-5})\\) culculated from \\(10\\) repeated experiments.\nThis is the idea behind the predictor-corrector technique discussed, for example, in (Gat et al., 2024), (S. Zhao et al., 2024), (L. Zhao et al., 2024).\nWe only included one corrector step per reverse step, although more correction will increase accuracy.\nThis can be regarded as a form of inference time compute scaling (Wang et al., 2025), although it is a fairly bad idea to wait for the predictor-corrector steps to converge.\n\n\n3.6 Discussion\nLet us come back to the technique employed in (Chao et al., 2025), which we mentioned in Section 1.4.\nAs we have seen, the number of steps \\(T\\) has to be kept larger than \\(d\\), to make sure no more than one jump occurs simultaneously.\nHowever, this increases the computational cost, as more time steps must be spent in simulating phantom jumps.\nOne thing we can do is to fill this blank steps with a informed ‘half-jump’, by introducing a sub-structure in each state \\(x\\in E\\).\nThis half-jump is a bit more robust than a direct unmasking. Therefore we are able to spend the time up to \\(T\\) more meaningfully.\nThus, this strategy can be view as another way to mitigate the bias introduced by the incorrect backward kernel."
  },
  {
    "objectID": "posts/2025/Posters/FECMC.html",
    "href": "posts/2025/Posters/FECMC.html",
    "title": "メトロポリスを超えた枠組みで我々はどこまで行けるか？",
    "section": "",
    "text": "区分確定的マルコフ過程 (PDMP: Piecewise Deterministic Markov Process) に基づく Monte Carlo 法は，Metropolis-Hastings (MH) の枠組みから逸脱することでスケーラビリティを達成する MCMC 手法である．良いサーベイは (Fearnhead ほか, 2018) など．\nより速い収束 (Andrieu と Livingstone, 2021) に加え，ミニバッチから計算した勾配の不偏推定量のみを用いて実行した場合も漸近的に正確な推論が可能であることが美点である．この点，Stochastic Gradient (Ma ほか, 2015) と特に相性が良いと言える．\nしかし，momentum を用いる方法 (Horowitz, 1991) や自然勾配 (Girolami と Calderhead, 2011) を用いる方法など，従来 MH 法に対して培われた性能改善手法はそのままでは PDMP に使えない．\nその中で，PDMP の性能を持続的な運動量を保持する技術を備えた Forward Event-Chain Monte Carlo 法 (Michel ほか, 2020) という手法が，PDMP の枠組みの中で尤度の幾何的情報を上手に利用して収束を速める・計算複雑性を低減できているということを，スケーリング解析を通じて示す．"
  },
  {
    "objectID": "posts/2025/Posters/FECMC.html#発表概要",
    "href": "posts/2025/Posters/FECMC.html#発表概要",
    "title": "メトロポリスを超えた枠組みで我々はどこまで行けるか？",
    "section": "",
    "text": "区分確定的マルコフ過程 (PDMP: Piecewise Deterministic Markov Process) に基づく Monte Carlo 法は，Metropolis-Hastings (MH) の枠組みから逸脱することでスケーラビリティを達成する MCMC 手法である．良いサーベイは (Fearnhead ほか, 2018) など．\nより速い収束 (Andrieu と Livingstone, 2021) に加え，ミニバッチから計算した勾配の不偏推定量のみを用いて実行した場合も漸近的に正確な推論が可能であることが美点である．この点，Stochastic Gradient (Ma ほか, 2015) と特に相性が良いと言える．\nしかし，momentum を用いる方法 (Horowitz, 1991) や自然勾配 (Girolami と Calderhead, 2011) を用いる方法など，従来 MH 法に対して培われた性能改善手法はそのままでは PDMP に使えない．\nその中で，PDMP の性能を持続的な運動量を保持する技術を備えた Forward Event-Chain Monte Carlo 法 (Michel ほか, 2020) という手法が，PDMP の枠組みの中で尤度の幾何的情報を上手に利用して収束を速める・計算複雑性を低減できているということを，スケーリング解析を通じて示す．"
  },
  {
    "objectID": "posts/2025/Posters/FECMC.html#連続最適化および関連分野に関する夏季学校",
    "href": "posts/2025/Posters/FECMC.html#連続最適化および関連分野に関する夏季学校",
    "title": "メトロポリスを超えた枠組みで我々はどこまで行けるか？",
    "section": "連続最適化および関連分野に関する夏季学校",
    "text": "連続最適化および関連分野に関する夏季学校\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nAug. 28th - 30th, 2025\n統数研２階大会議室\n\n\n\n\n\n\n\n画像をクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Posters/FECMC.html#参考ページ集",
    "href": "posts/2025/Posters/FECMC.html#参考ページ集",
    "title": "メトロポリスを超えた枠組みで我々はどこまで行けるか？",
    "section": "参考ページ集",
    "text": "参考ページ集\nPDMP とそのシミュレーションに関しては，次のスライドにわかりやすく解説されています：\n\n\n\n\n\n\n\n\n\n\n動き出す次世代サンプラー\n\n\n区分確定的モンテカルロ\n\n\n\nSlide\n\n\nPDMP\n\n\nJulia\n\n\n\n\n2025-02-06\n\n\n\n\n\n\n\n\n一致なし\n\n\n図は全て発表者開発のパッケージ PDMPFlux.jl によるものです．\n\n\n\n\n\n\n\n\n\n\nPDMPFlux.jl パッケージ\n\n\n自動微分により全自動化された連続時間 MCMC サンプラー\n\n\n\nJulia\n\n\nMCMC\n\n\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n一致なし\n\n\nSticky PDMP (Bierkens ほか, 2023) に関するさらに詳しい内容，またはベイズ変数選択一般については，次の記事にまとめています：\n\n\n\n\n\n\n\n\nSticky PDMP によるベイズ変数選択\n\n\n非絶対連続分布からの正確なサンプリング\n\n\n\n2024-12-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n超次元 MCMC\n\n\nモデル選択のためのマルコフ連鎖モンテカルロ法\n\n\n\n2024-09-22\n\n\n\n\n\n\n\n\n\n\n\n\nベイズ変数選択\n\n\nBMI データの重線型回帰を題材として\n\n\n\n2024-12-10\n\n\n\n\n\n\n\n\n一致なし"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html",
    "href": "posts/2023/Probability/Beta-Gamma.html",
    "title": "確率測度の変換則",
    "section": "",
    "text": "Gamma 確率変数と，その変換として得る Beta 確率変数とに関する次の命題の証明を与える（第 3 節）．\n証明に先んじて，Gamma 分布と Beta 分布の性質を，それぞれ第 1 節と第 2 節で見ていく．\n最後に，確率分布の変換の計算方法をまとめる（第 4 節）．混合分布や複合分布など，特定の可微分変換で密度関数がどう変化するかの計算の基礎となる．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-Gamma",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-Gamma",
    "title": "確率測度の変換則",
    "section": "1 Gamma 分布を見る",
    "text": "1 Gamma 分布を見る\n\n1.1 定義\n\n\n\n\n\n\n定義（Gamma 分布）\n\n\n\n可測空間 \\((\\mathbb{R},\\mathcal{B}(\\mathbb{R}))\\) 上の Gamma分布 \\(\\mathrm{Gamma}(\\alpha,\\nu)\\;(\\alpha,\\nu&gt;0)\\) とは， 密度関数 \\[g(x;\\alpha,\\nu):=\\frac{1}{\\Gamma(\\nu)}\\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}1_{\\left\\{x&gt;0\\right\\}}\\] が定める分布をいう．実際，\\(t=\\alpha x\\) と変数変換すると， \\[\n\\begin{align*}\n&\\quad\\int_0^\\infty \\alpha^\\nu x^{\\nu-1}e^{-\\alpha x}dx\\\\\n&=\\alpha^\\nu\\int^\\infty_0\\left(\\frac{t}{\\alpha}\\right)^{\\nu-1}e^{-t}\\frac{dt}{\\alpha}\\\\&=\\int^\\infty_0t^{\\nu-1}e^{-t}dt=\\Gamma(\\nu).\\end{align*}\n\\]\n\n\n\n\n1.2 形状\n\\(\\alpha\\) をレートパラメータ（スケールパラメータと呼ばれるものの逆数），\\(\\nu\\) を形状パラメータともいう．レートパラメータが大きいほど突起も大きく，手前に寄る．形状パラメータ \\(\\nu\\) は分布の形状を大きく司る．\n実際，先度と歪度は形状パラメータのみに依って \\[\\gamma_1=\\frac{2}{\\sqrt{\\nu}},\\qquad\\gamma_2=3+\\frac{6}{\\nu},\\] と記述される．\nその意味するところを感得するために，scipy.statsでの実装を用いてプロットしてみる．\n\n\nコードを表示\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gamma\n\nnu = 1.5  # 形状パラメーター\n\n# Gamma分布のPDFをグリッド上で計算\nx = np.linspace(0, 8, 100)\npdf = gamma.pdf(x, nu)\n\n# プロットの実行\nplt.figure(figsize=(3.2, 4.8)) # スマホサイズに合わせる\nplt.plot(x, pdf)\nplt.title('Gamma(1,3/2) Distribution')\nplt.ylabel('Density')\nplt.xlabel('Value')\nplt.show()\n\n\n\n\n\n\n\n\n\nレートパラメータを固定し，形状パラメータを残した \\[\\chi^2(k):=\\mathrm{Gamma}(1/2,k/2)\\] を自由度 \\(k\\) のカイ自乗分布ということに注意．\n\n\n\n\n\n\n\n\n\n最後に，レートパラメータが大きいほど突起が大きくなる様子は次の通り：\n\n\n\n\n\n\n\n\n\nなお，形状パラメータが \\(\\nu=1\\) である Gamma 分布のことを指数分布という： \\[\n\\operatorname{Exp}(\\gamma):=\\mathrm{Gamma}(\\gamma,1)\\;(\\gamma&gt;0)\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-Beta",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-Beta",
    "title": "確率測度の変換則",
    "section": "2 Beta分布を見る",
    "text": "2 Beta分布を見る\n\n2.1 定義\n\n\n\n\n\n\n定義（Beta 分布）\n\n\n\n可測空間 \\(((0,1),\\mathcal{B}((0,1)))\\)上の （第１種）ベータ分布 \\(\\operatorname{Beta}(\\alpha,\\beta)\\;(\\alpha,\\beta&gt;0)\\) とは， 密度関数 \\[\\frac{1}{B(\\alpha,\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}1_{(0,1)}(x)\\] が定める分布をいう．ただし，\\[B(\\alpha,\\beta)=\\int^1_0x^{\\alpha-1}(1-x)^{\\beta-1}\\,dx.\\]\n\n\n\n\n2.2 形状\n次のような性質を持つ：1\n\n\n\n\n\n\n\n\\(\\alpha_1=\\alpha_2=1\\) のとき一様分布となり，\\(\\alpha_1=\\alpha_2&gt;1\\) の場合に左右対称な単峰性分布，\\(\\alpha_1=\\alpha_2&lt;1\\) の場合に左右対称な U 字型の二峰性分布を得る．\nいずれも \\(1\\) より大きい場合，左のパラメータが大きい場合 \\(\\alpha_1&gt;\\alpha_2&gt;1\\) 左に，右のパラメータが大きい場合 \\(\\alpha_2&gt;\\alpha_1&gt;1\\) 右に歪んだ単峰性分布を得る．\nいずれも \\(1\\) より小さい場合はその逆．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-proof",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-proof",
    "title": "確率測度の変換則",
    "section": "3 証明",
    "text": "3 証明\n\n\n\n\n\n\n証明\n\n\n\n\n\n\\[\\begin{cases}\n    X_1=\\frac{Y_1}{Y_1+Y_2},\\\\\n    X_2=Y_1+Y_2.\n\\end{cases}\\] を逆に解くことで， \\[\\begin{pmatrix}y_1\\\\y_2\\end{pmatrix}=\\begin{pmatrix}x_1x_2\\\\x_2\\end{pmatrix}=:T(x_1,x_2)\\] を得る．\\(A:=(0,1)\\times(0,\\infty),B:=(0,\\infty)^2\\) と定めると，\\(T:A\\to B\\) は可微分同相で，Jacobianは \\[DT=\\begin{pmatrix}x_2&x_1\\\\0&1\\end{pmatrix},\\qquad J_T=x_2,\\] と計算でき，\\(A\\) 上で は消えない．\nよって \\((X_1,X_2)\\) の結合分布は \\[\n\\begin{align*}\n    &p(T(x_1,x_2))J_T(x_1,x_2)dx_1dx_2\\\\\n    &=\\frac{\\alpha^{\\nu_1}}{\\Gamma(\\nu_1)}y_1^{\\nu_1-1}e^{-\\alpha y_1}\\frac{\\alpha^{\\nu_2}}{\\Gamma(\\nu_2)}y_2^{\\nu_2-1}e^{-\\alpha y_2}\\\\\n    &\\qquad\\times\\frac{x_2}{(1-x_1)^2}\\,dx_1dx_2\\\\\n    &=\\frac{\\alpha^{\\nu_1}}{\\Gamma(\\nu_1)}x_1^{\\nu_1-1}x_2^{\\nu_1-1}e^{-\\alpha x_1x_2}\\frac{\\alpha^{\\nu_2}}{\\Gamma(\\nu_2)}x_2^{\\nu_2-1}\\\\\n    &\\qquad\\times(1-x_1)^{\\nu_2-1}e^{-\\alpha x_2(1-x_1)}x_2\\,dx_1dx_2\\\\\n    &=\\underbrace{\\frac{\\Gamma(\\nu_1+\\nu_2)}{\\Gamma(\\nu_1)\\Gamma(\\nu_2)}}_{=B(\\nu_1,\\nu_2)^{-1}}x_1^{\\nu_1-1}(1-x_1)^{\\nu_2-1}\\,dx_1\\\\\n    &\\qquad\\times\\frac{\\alpha^{\\nu_1+\\nu_2}}{\\Gamma(\\nu_1+\\nu_2)}x_2^{(\\nu_1+\\nu_2)-1}e^{-\\alpha x_2}\\,dx_2.\n\\end{align*}\n\\] これは \\(X_1\\) が \\(\\operatorname{Beta}(\\nu_1,\\nu_2)\\) に，\\(X_2\\) が \\(\\mathrm{Gamma}(\\alpha,\\nu_1+\\nu_2)\\) に独立に従った場合の密度になっている．\n\n\n\n\n3.1 余談\n総合研究大学院大学統計科学コース2018年8月実施の入試問題の第三問にて，本命題を背景とした問題が出題された．2\n\n\n\n\n\n\n第３問\n\n\n\n\n数直線 \\(\\mathbb{R}\\) 上の点Pの \\(x\\) 座標 \\(X\\) は \\(\\mathrm{N}(0,1)\\) に従うとする． Pの原点からの距離の自乗の確率密度関数が \\[\\frac{1}{\\sqrt{2\\pi x}}e^{-\\frac{x}{2}},\\qquad(x&gt;0)\\] であることを示せ．\nEuclid空間 \\(\\mathbb{R}^n\\) 内の点Qの座標 \\((X_1,\\cdots,X_n)\\) は \\(\\mathrm{N}_n(0,I_n)\\) に従うとする． Qの原点からの距離の自乗の確率密度関数が \\[\\frac{1}{\\Gamma\\left(\\frac{n}{2}\\right)2^{\\frac{n}{2}}}x^{\\frac{n}{2}-1}e^{-\\frac{x}{2}},\\qquad(x&gt;0)\\] であることを示せ．\n(2)の確率密度関数を持つ分布を \\(\\chi^2(n)\\) という． 確率変数 \\(X,Y\\) は独立で \\(X\\sim\\chi^2(n),Y\\sim\\chi^2(m)\\) であるとする．このとき， \\[X+Y\\sim\\chi^2(n+m),\\] \\[\\frac{X}{X+Y}\\sim\\operatorname{Beta}(n/2,m/2),\\] であり，互いに独立であることを示せ．"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#sec-transform",
    "href": "posts/2023/Probability/Beta-Gamma.html#sec-transform",
    "title": "確率測度の変換則",
    "section": "4 確率分布の変換則",
    "text": "4 確率分布の変換則\n\\(A,B\\subset\\mathbb{R}^d\\) を連結開集合，\\(C^1\\)-微分同相 \\(T:A\\overset{\\sim}{\\to}B\\) に対して，3 \\(B\\) 上の分布 \\(\\pi\\in\\mathcal{P}(B)\\) の \\(T\\) による引き戻し \\(T^*\\pi\\) の密度 \\(p^*\\) が，\\(\\pi\\) の密度 \\(p\\) と Jacobian \\(J_T(x)\\) の絶対値との積になる： \\[\np^*(x)=p(T(x))\\lvert J_T(x)\\rvert\\;\\;\\text{a.s.}\\quad(x\\in A).\n\\]\n\n\n\nCommutative diagram discribing current situations\n\n\n\n\n\n\n\n\n定理（変数変換）4\n\n\n\n\\(A,B\\subset\\mathbb{R}^d\\) を連結開集合，\\(T:A\\overset{\\sim}{\\to}B\\) を \\(C^1\\)-微分同相，\\(f:B\\to\\mathbb{R}\\) を Lebesgue 可測関数とする．\n\n\\(f\\circ T:A\\to\\mathbb{R}\\) も Lebesgue 可測．\n\\(f\\) は非負関数とする．このとき， \\[\n\\begin{align*}\n\\int_Af(T(x))\\lvert J_T(x)\\rvert\\,dx=\\int_Bf(y)\\,dy.\n\\end{align*}\n\\]\n\n\n\nこの定理より，任意の可測集合 \\(A_0\\in\\mathcal{B}(A)\\) に対して， \\[\n\\begin{align*}\n    \\int_{A_0}p^*(x)\\,dx&=(T^*\\pi)[A_0]\\\\\n    &=\\int_{T(A_0)}p(y)\\,dy\\\\\n    &=\\int_{A_0}p(T(x))\\lvert J_T(x)\\rvert\\,dx.\n\\end{align*}\n\\] ただし，最後の等号は定理による．5 \\(A_0\\in\\mathcal{B}(A)\\) は任意だったから， \\[\np^*(x)=p(T(x))\\lvert J_T(x)\\rvert\\;\\;\\text{a.s.}\\quad(x\\in A).\n\\]"
  },
  {
    "objectID": "posts/2023/Probability/Beta-Gamma.html#footnotes",
    "href": "posts/2023/Probability/Beta-Gamma.html#footnotes",
    "title": "確率測度の変換則",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(Agresti, 2012) 第1.6.2節 Binomial Estimation: Beta and Logit-Normal Prior Distributions p.24 参照．↩︎\n過去9年分の入試問題の解答はこちらから↩︎\n記法 \\(\\overset{\\sim}{\\to}\\) は 記法一覧 参照．↩︎\n会田先生講義ノート 定理5.1 など参照．↩︎\n最後から2番目の等号は，測度の押し出し \\(T^*\\pi\\) の定義である．↩︎"
  },
  {
    "objectID": "posts/2025/Posters/FECMC.html#機械学習若手の会-yaml-2025",
    "href": "posts/2025/Posters/FECMC.html#機械学習若手の会-yaml-2025",
    "title": "メトロポリスを超えた枠組みで我々はどこまで行けるか？",
    "section": "機械学習若手の会 (YAML) 2025",
    "text": "機械学習若手の会 (YAML) 2025\n\n\n\n\n\n\n\n\nDate\nLocation\n\n\n\n\nSep. 21st - 23rd, 2025\nハートピア熱海\n\n\n\n\n\n\n\n画像をクリックで PDF を表示"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu.html",
    "href": "posts/2025/Slides/MrTakatsu.html",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "",
    "text": "0.1 Background\n\nEmergency departments (ED) face increasing patient numbers and overcrowding.\nTraditional triage systems are based on chief complaints, medical history, and vital signs, but they cannot fully capture subtle patient behaviors.\nFew studies have examined prognosis prediction based on patient’s spontaneous behaviors in the ED, and one overlooked example is the ‘request to urinate.’\nWe investigated how the probability of hospital admission changes according to the time from arrival at the ED to the patient’s request to urinate."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#background",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#background",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.1 Background",
    "text": "1.1 Background\n\nEmergency departments (ED) face increasing patient numbers and overcrowding.\nTraditional triage systems are based on chief complaints, medical history, and vital signs, but they cannot fully capture subtle patient behaviors.\nFew studies have examined prognosis prediction based on patient’s spontaneous behaviors in the ED, and one overlooked example is the ‘request to urinate.’\nWe investigated how the probability of hospital admission changes according to the time from arrival at the ED to the patient’s request to urinate."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#methods",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#methods",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.2 Methods",
    "text": "1.2 Methods\n\nFrom January 29 to July 31, we collected data on ambulance-transported patients to the ED, including sex, age, ED arrival time, time of request to urinate, presence of a urinary catheter, and outcomes (discharge, outpatient, admission to general ward/CCU/SCU/ICU1, transfer, or death).\nPatients with missing data, those who had a urinary catheter in place, and patients with caudiopulmonary arrest on arrival were excluded from the analysis.\nHierarchical Bayesian modeling was performed with PyMC 5.25.1.\n\nCCU:Cardiac Care Unit, SCU:Stroke Care Unit, ICU:Intensive Care Unit"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#patient-flow",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#patient-flow",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.3 Patient Flow",
    "text": "1.3 Patient Flow"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#table-one",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#table-one",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "4 Table One",
    "text": "4 Table One"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#table-1",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#table-1",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.4 Table 1",
    "text": "1.4 Table 1\n\n\n\n\nDischarged Home(n=1140)\nHospital Admission(n=870)\n\n\n\n\nAge\n62 (35.8-80.0)\n77 (62.0-84.0)\n\n\nFemale\n525 (46.1%)\n389 (44.7%)\n\n\nTime\n49 (23-88)\n89.5 (45-141)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#table-2",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#table-2",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "5 Table 2",
    "text": "5 Table 2\n\n\n\n\nDischarged Home (n=1140)\nHospital Admission (n=870)\n\n\n\n\nAge\n62 (35.8-80.0)\n77 (62.0-84.0)\n\n\nFemale\n525 (46.1%)\n389 (44.7%)\n\n\nTime\n49 (23-88)\n89.5 (45-141)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#table-2-comorbidity併発症",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#table-2-comorbidity併発症",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.5 Table 2: Comorbidity（併発症）",
    "text": "1.5 Table 2: Comorbidity（併発症）\n\n\n\n\nDischarged Home(n=1140)\nHospital Admission(n=870)\n\n\n\n\nNone\n102 (40.8%)\n49 (26.3%)\n\n\nHypertention\n56 (22.4%)\n64 (34.4%)\n\n\nDiabetes\n30 (12.0%)\n31 (16.7%)\n\n\nMalignancy\n22 (8.8%)\n26 (14.0%)\n\n\nNeuroligic Dis.\n22 (8.8%)\n22 (11.8%)\n\n\nCerebrovascular Dis.\n19 (7.6%)\n25 (13.4%)\n\n\nArriythmia\n25 (10.0%)\n19 (10.2%)\n\n\nDyslipidemia\n16 (6.4%)\n22 (11.8%)\n\n\nHeart Failure\n14 (5.6%)\n17 (9.1%)\n\n\nChronic Lung Dis.\n11 (4.4%)\n13 (5.7%)\n\n\nRenal Failure\n9 (3.6%)\n15 (8.1%)\n\n\nPsychiatric Dis.\n14 (5.6%)\n5 (2.7%)\n\n\nLiver Dis.\n9 (3.6%)\n5 (2.7%)\n\n\nValvular Dis.\n6 (2.4%)\n8 (4.3%)\n\n\nHematologic Dis.\n6 (2.4%)\n7 (3.8%)\n\n\nEndocrine Dis.\n3 (1.2%)\n8 (4.3$)\n\n\nPerioheral Vascular Dis.\n4 (1.6%)\n4 (2.2%)\n\n\nConnective Tissue Dis.\n4 (1.6%)\n3 (1.6%)\n\n\nPeptic Ulcer Dis.\n2 (0.8%)\n3 (1.6%)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#table-3-diagnosis-category診断",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#table-3-diagnosis-category診断",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.6 Table 3: Diagnosis Category（診断）",
    "text": "1.6 Table 3: Diagnosis Category（診断）\n\n\n\n\nDischarged Home(n=1140)\nHospital Admission(n=870)\n\n\n\n\nOrthopedics\n81 (32.4%)\n36 (19.3%)\n\n\nGastroenterology\n40 (16.0%)\n43 (23.1%)\n\n\nNeurology\n44 (17.6%)\n16 (8.6%)\n\n\nInfection\n13 (5.2%)\n34 (18.3%)\n\n\nCardiology\n21 (8.4%)\n20 (10.8%)\n\n\nUrology\n21 (8.4%)\n7 (3.8%)\n\n\nCerebrovascular\n1 (9.6%)\n26 (14.0%)\n\n\nRespiratory\n10 (13.2%)\n11 (5.9%)\n\n\nNephrology\n5 (9.6%)\n13 (7.0%)\n\n\nDermatology\n9 (9.6%)\n7 (3.8%)\n\n\nOther\n5 (2.0%)\n11 (5.9%)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#model",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#model",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.1 Model",
    "text": "2.1 Model\n\nObservations: i=1,\\cdots,n\nBinary outcome \n  y_i=0\\text{: Discharge Home / Outpatient},\\; y_i=1\\text{: \\textcolor{#E95420}{\\textbf{Admission}} / Death}\n  \nElapsed Time from ED Arrival to Request to Void \n  t_i^{\\text{raw}}\\ge0\\qquad(\\text{minutes})\n  \nIndicator m_i\\in\\{0,1\\} \n  m_i=0\\text{: No Request}, \\quad m_i=1\\text{: Request Present}.\n  \nRight-censoring was applied at 300 \n  t_i:=\\min(t_i^{\\text{raw}},300)."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#table-3-diagnosis診断",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#table-3-diagnosis診断",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "1.6 Table 3: Diagnosis（診断）",
    "text": "1.6 Table 3: Diagnosis（診断）\n\n\n\n\nDischarged Home(n=1140)\nHospital Admission(n=870)\n\n\n\n\nOrthopedics\n81 (32.4%)\n36 (19.3%)\n\n\nGastroenterology\n40 (16.0%)\n43 (23.1%)\n\n\nNeurology\n44 (17.6%)\n16 (8.6%)\n\n\nInfection\n13 (5.2%)\n34 (18.3%)\n\n\nCardiology\n21 (8.4%)\n20 (10.8%)\n\n\nUrology\n21 (8.4%)\n7 (3.8%)\n\n\nCerebrovascular\n1 (9.6%)\n26 (14.0%)\n\n\nRespiratory\n10 (13.2%)\n11 (5.9%)\n\n\nNephrology\n5 (9.6%)\n13 (7.0%)\n\n\nDermatology\n9 (9.6%)\n7 (3.8%)\n\n\nOther\n5 (2.0%)\n11 (5.9%)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#variables",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#variables",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.1 Variables",
    "text": "2.1 Variables\n\nObservations: i=1,\\cdots,n.\nBinary outcome \n  y_i=0\\text{: Discharge Home / Outpatient},\\; y_i=1\\text{: \\textcolor{#E95420}{\\textbf{Admission}} / Death}.\n  \nElapsed Time from ED Arrival to Request to Void \n  t_i^{\\text{raw}}\\ge0\\qquad(\\text{minutes}).\n  \nIndicator m_i\\in\\{0,1\\} \n  m_i=0\\text{: No Request}, \\quad m_i=1\\text{: Request Present}.\n  \nRight-censoring was applied at 300 \n  t_i:=\\min(t_i^{\\text{raw}},300)."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#regression-model",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#regression-model",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.2 Regression Model",
    "text": "2.2 Regression Model\nCovariates with missingness flags: \nx_i^\\top\\beta=\\beta_{\\text{age}}a_i+\\beta_{\\text{age-mis}}a_i^{\\text{mis}}\n+\\beta_{\\text{sex}}s_i+\\beta_{\\text{sex-mis}}s_i^{\\text{mis}}\n\n\na_i: standardized age\na_i^{\\text{mis}}: missing indicator for age\ns_i: sex (0: male, 1: female)\ns_i^{\\text{mis}}: missing indicator for sex"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#bayesian-hierarchical-logistic-regression",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#bayesian-hierarchical-logistic-regression",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.2 Bayesian Hierarchical Logistic Regression",
    "text": "2.2 Bayesian Hierarchical Logistic Regression\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n$$\n$$\n\n\ny_i\\sim\\operatorname{Bernoulli}(p_i),\\qquad i=1,\\cdots,n,\n \n\\operatorname{logit}(p_i)=(1-m_i)\\left(\\operatorname{logit}(\\rho_0)+x_i^\\top\\beta\\right)+m_i\\left(\\operatorname{logit}(\\rho_1)+\\Delta_i+x_i^\\top\\beta\\right).\n\nwith \\Delta_i (next slide) and the linear part given by: \nx_i^\\top\\beta=\\beta_{\\text{age}}a_i+\\beta_{\\text{age-mis}}a_i^{\\text{mis}}\n+\\beta_{\\text{sex}}s_i+\\beta_{\\text{sex-mis}}s_i^{\\text{mis}}.\n\n\na_i: standardized age, a_i^{\\text{mis}}: missing indicator for age\ns_i: sex (0: male, 1: female), s_i^{\\text{mis}}: missing indicator for sex"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#section",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#section",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.3 ",
    "text": "2.3 \nGaussian density \nf_0(t)=\\phi(t|\\mu_0,\\sigma_0^2),\\quad f_1(t)=\\phi(t|\\mu_1,\\sigma_1^2),\n with \n\\mu_1\\ge\\mu_0."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#effective-log-likelihood-delta_i",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#effective-log-likelihood-delta_i",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.3 Effective Log-likelihood \\Delta_i",
    "text": "2.3 Effective Log-likelihood \\Delta_i\n\n\\Delta_i:=(1-c_i)\\log\\frac{f_1(t_i)}{f_0(t_i)}+c_i\\log\\left(\\overline{\\Phi}\\left(\\frac{C-\\mu_1}{\\sigma_1}\\right)/\\overline{\\Phi}\\left(\\frac{C-\\mu_0}{\\sigma_0}\\right)\\right),\n where \\overline{\\Phi}=1-\\Phi is the Gaussian survival function, and Gaussian densities \nf_0(t)=\\phi(t|\\mu_0,\\sigma_0^2),\\quad f_1(t)=\\phi(t|\\mu_1,\\sigma_1^2),\n with \n\\mu_1\\ge\\mu_0."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#prior-specification",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#prior-specification",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.4 Prior Specification",
    "text": "2.4 Prior Specification\n\n\\operatorname{logit}(\\rho_0),\\operatorname{logit}(\\rho_1)\\sim\\operatorname{N}(0,1)\n \n\\log\\sigma_k\\sim\\operatorname{N}(\\log\\text{scale},1)\\quad(k=0,1)\n \n\\mu_0\\sim\\operatorname{TruncatedNormal}(t_{\\text{mean}},2\\cdot\\text{scale},0,300)\n \n\\mu_1=\\min(\\mu_0+\\delta_\\mu,300),\\qquad\\delta_\\mu\\sim\\operatorname{HalfNormal}(\\mathrm{scale})\n \n\\beta\\overset{\\text{i.i.d.}}{\\sim}\\operatorname{N}(0,1)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#entire-workflow",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#entire-workflow",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.5 Entire Workflow",
    "text": "2.5 Entire Workflow"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#inference-and-diagnosis",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#inference-and-diagnosis",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "2.6 Inference and Diagnosis",
    "text": "2.6 Inference and Diagnosis\nNo-U-Turn Sampler(NUTS) with 4chains:\n\nBaseline: 3,000 warm-up + 3,000 posterior draws\nCovariate: 1,500 warm-up + 1,500 posterior draws\nSettings: target_accept = 0.90, max_treedepth = 12\nJAX-Numpyro acceleration when available; otherwise PyMC, Convergence assessed via hat R, ESS, MCSE, and visual diagnosis (trace, rank, energy)."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#evaluation-metrics",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#evaluation-metrics",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.1 Evaluation Metrics",
    "text": "3.1 Evaluation Metrics\n3.1.1 Population-level fit\nFor cumulative admissions,\n\nObserved C_\\text{obs}(t)/N vs posterior mean \\operatorname{E}[\\widehat{C}(t)]/N and 95% credible bands.\nTime-integrated scalars: ABC, IAE, ISE, RMSE, KS, CvM, empirical coverage_95, avg_band_width\n\n3.1.2 Individual-level metrics\nAt landmark times, T=60,120,180,240,300, we check\n\nAUC(t)\nBrier(t)\nCalibration at (t)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#study-population",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#study-population",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.2 Study Population",
    "text": "3.2 Study Population\n\nAmbulance-transported patients (Feb-Aug2025)\nExclusions: CPA at arrival, existing urinary catheter, missing time infomation.\nFinal sample size: n=2571"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#mcmc-convergence",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#mcmc-convergence",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.3 MCMC Convergence",
    "text": "3.3 MCMC Convergence\nSampling diagnostics showed stable mixing without pathological divergences on representative patameters: \nρ0,\\;ρ1,\\;μ0,\\;μ1,\\;σ0,\\;σ1,\\;β.\n\nTrace, rank, and energy plots were visually acceptable in the combined diagnostic bundle."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#rank-plot-and-trace-plot",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#rank-plot-and-trace-plot",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.4 Rank Plot and Trace Plot",
    "text": "3.4 Rank Plot and Trace Plot"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#energy-diagnosis",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#energy-diagnosis",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.5 Energy Diagnosis",
    "text": "3.5 Energy Diagnosis"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#population-level-fit-1",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#population-level-fit-1",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.6 Population-level Fit",
    "text": "3.6 Population-level Fit"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#calibration",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#calibration",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.7 Calibration",
    "text": "3.7 Calibration"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#decision-curve-analysis",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#decision-curve-analysis",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.8 Decision Curve Analysis",
    "text": "3.8 Decision Curve Analysis"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#discussion-and-feedback",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#discussion-and-feedback",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "Discussion and Feedback",
    "text": "Discussion and Feedback\n\nWe developed an app. URL: accm.jp/urination\nIntegrate patient behaviors and clinical data into the model to enable earlier prediction of hospitalization probability.\nWe applied Bayesian regression in this study. What other approaches might be suitable for this problem?\nWe did not record whether urinary catheter were inserted before RD arrival or the exact time of insertion in the ED. Catheter uses likely affecs the expression of voiding desire and should be considered in future research."
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#robustness-to-ttu-measurement-error",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#robustness-to-ttu-measurement-error",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.9 Robustness to TTU Measurement Error",
    "text": "3.9 Robustness to TTU Measurement Error"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#individual-level-performance-12",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#individual-level-performance-12",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.10 Individual-level Performance (1/2)",
    "text": "3.10 Individual-level Performance (1/2)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#individual-level-performance-22",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#individual-level-performance-22",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.11 Individual-level Performance (2/2)",
    "text": "3.11 Individual-level Performance (2/2)"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#posterior-predictive-checks",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#posterior-predictive-checks",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.12 Posterior Predictive Checks",
    "text": "3.12 Posterior Predictive Checks"
  },
  {
    "objectID": "posts/2025/Slides/MrTakatsu_Slides.html#posterior-distribution-of-the-baseline-model",
    "href": "posts/2025/Slides/MrTakatsu_Slides.html#posterior-distribution-of-the-baseline-model",
    "title": "Association between Patient Behaviors and Clinical Significance in the ED",
    "section": "3.13 Posterior Distribution of the Baseline Model",
    "text": "3.13 Posterior Distribution of the Baseline Model"
  }
]