{
  "hash": "093726799628fd370bf4f7a19b726e60",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"`brms` を用いたベイズロジスティック回帰分析\"\nsubtitle: \"項目応答モデルと特異項目機能を題材として\"\nauthor: \"司馬博文\"\ndate: 12/12/2024\ndate-modified: 12/14/2024\ncategories: [Bayesian, Statistics, R, Stan]\nbibliography: \n    - ../../../assets/mathematics.bib\n    - ../../../assets/bib.bib\n    - ../../../assets/bib1.bib\ncsl: ../../../assets/apalike.csl\n# abstract-title: 概要\n# abstract: |\n#     心理学などの人間を対象にする研究では変数の数が多く，正しいモデルを見つけるために分散分析 (ANOVA) が広く用いられる．\n#     しかし，古典的な ANOVA 解析手法である F-検定や t-検定は，データの一側面しか伝えない．\n#     一方で，モデルの仮定を前面に出したベイズ的な解析手法は，データを探索的に吟味することができ，極めて微妙な消息も捉えることが可能になる．\n#     本稿では特にベイズ ANOVA 手法 [@Gelman2005], [@Rouder+2012] を採用して，そのモデルケースを実証する．\n# image: Files/House.png\ncode-fold: false\nexecute:\n    cache: true\nlisting: \n    -   id: lst-survey\n        type: grid\n        sort: false\n        contents:\n            - \"BDA2.qmd\"\n            - \"BayesRegression.qmd\"\n            - \"../TransDimensionalModels/IdealPoint1.qmd\"\n        date-format: iso\n        fields: [title,image,date,subtitle]\n    # -   id: lst-embedding\n    #     type: grid\n    #     grid-columns: 1\n    #     grid-item-align: center\n    #     sort: false\n    #     contents:\n    #         - \"BDA2.qmd\"\n    #     date-format: iso\n    #     fields: [title,image,date,subtitle]\n---\n\n\n::: {.hidden}\n\n::: {.content-visible when-format=\"html\"}\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n\n$$\n\n\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\BRac}[1]{\\biggl\\langle#1\\biggr\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\\newcommand{\\rN}{\\mathrm{N}}\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n\n\n\n\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n\\newcommand{\\argmax}{\\operatorname*{argmax}}\\newcommand{\\argmin}{\\operatorname*{argmin}}\n\n\\newcommand{\\pr}{\\mathrm{pr}}\\newcommand{\\Conv}{\\operatorname{Conv}}\\newcommand{\\cU}{\\mathcal{U}}\n\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n\\newcommand{\\grad}{\\operatorname{grad}}\\newcommand{\\rot}{\\operatorname{rot}}\\renewcommand{\\div}{\\operatorname{div}}\\newcommand{\\tr}{\\operatorname{tr}}\\newcommand{\\Tr}{\\operatorname{Tr}}\\newcommand{\\KL}{\\operatorname{KL}}\\newcommand{\\JS}{\\operatorname{JS}}\\newcommand{\\ESS}{\\operatorname{ESS}}\\newcommand{\\MSE}{\\operatorname{MSE}}\\newcommand{\\erf}{\\operatorname{erf}}\\newcommand{\\arctanh}{\\operatorname{arctanh}}\\newcommand{\\pl}{\\operatorname{pl}}\\newcommand{\\minimize}{\\operatorname{minimize}}\\newcommand{\\subjectto}{\\operatorname{subject to}}\\newcommand{\\sinc}{\\operatorname{sinc}}\\newcommand{\\Ent}{\\operatorname{Ent}}\\newcommand{\\Polya}{\\operatorname{Polya}}\\newcommand{\\Exp}{\\operatorname{Exp}}\\newcommand{\\codim}{\\operatorname{codim}}\\newcommand{\\sgn}{\\operatorname{sgn}}\\newcommand{\\rank}{\\operatorname{rank}}\n\n\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\n\n\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n\n\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n\n\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n\n\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n\n\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n\n\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\renewcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\renewcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n\n\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n\n\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n\n\n\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\bL}{\\mathbb{L}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n\n\\newcommand{\\bit}{\\mathrm{bit}}\n\n\\newcommand{\\err}{\\mathrm{err}}\n\n\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n\n\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n\n\n\\newcommand{\\del}{\\partial}\n\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n\n\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n\n\\newcommand{\\Ens}{\\mathrm{Ens}}\n\n\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n\n\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n\n\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n\n\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\renewcommand{\\Beta}{\\operatorname{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n\n\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n\n\n\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\n\n\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}}\n\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n\\newcommand{\\Op}{\\mathrm{Op}}\n\\newcommand{\\Sh}{\\mathrm{Sh}}\n\\newcommand{\\Diff}{\\mathrm{Diff}}\n\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n\n\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{i.i.d.}}{\\sim}}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n\n\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n\n\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n\\newcommand{\\lmd}{\\lambda}\n\\newcommand{\\Lmd}{\\Lambda}\n\\newcommand{\\cI}{\\mathcal{I}}\n\n\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n\\DeclareMathOperator{\\des}{des}\n\\DeclareMathOperator{\\nd}{nd}\n\\DeclareMathOperator{\\dsep}{d-sep}\n\\DeclareMathOperator{\\sep}{sep}\n\\newcommand{\\rLL}{\\mathrm{LL}}\\newcommand{\\HT}{\\mathrm{HT}}\\newcommand{\\PS}{\\mathrm{PS}}\\newcommand{\\rI}{\\mathrm{I}}\n$$\n\n:::\n\n:::\n\n\n\n## はじめに\n\n多くの社会的なデータは非数値的である．しかしその背後には潜在的な連続変数を想定することが多い．\n\n加えて，線型回帰分析の結果複雑な非線型関係が予期された際，本格的なノンパラメトリック推論に移る前に，離散変数の設定に換言して非線型性を扱いやすくするなど，離散変数を扱う積極的理由もある．\n\n本稿ではロジスティック回帰を主に扱う．\n\n::: {#lst-survey}\n:::\n\n## ロジスティック回帰\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## 項目応答モデル\n\n### データの概観\n\n[@Vansteelandt2001], [@Boeck-Wilson2004] による「怒るかどうか？」のデータ [`VerbAgg`](https://rdrr.io/cran/lme4/man/VerbAgg.html) を用いる．混合モデルの点推定のためのパッケージ `lme4` [@Bates+2015] で利用可能になっている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\ndata(\"VerbAgg\", package = \"lme4\")\ndf <- VerbAgg\n```\n:::\n\n\n\n質問票は「自分が意思表示をしたのにバスが止まってくれなかったので悪態をついた」などのもので，同意できるかを３段階 \"yes\", \"perhaps\", \"no\" で評価する [@Boeck-Wilson2004 pp.7-8]．\n\n応答は３段階の順序応答 `resp` とこれを２段階にしたもの `r2` である．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkable(head(df))\n```\n\n::: {.cell-output-display}\n\n\n| Anger|Gender |item        |resp    |id |btype |situ  |mode |r2 |\n|-----:|:------|:-----------|:-------|:--|:-----|:-----|:----|:--|\n|    20|M      |S1WantCurse |no      |1  |curse |other |want |N  |\n|    11|M      |S1WantCurse |no      |2  |curse |other |want |N  |\n|    17|F      |S1WantCurse |perhaps |3  |curse |other |want |Y  |\n|    21|F      |S1WantCurse |perhaps |4  |curse |other |want |Y  |\n|    17|F      |S1WantCurse |perhaps |5  |curse |other |want |Y  |\n|    21|F      |S1WantCurse |yes     |6  |curse |other |want |Y  |\n\n\n:::\n:::\n\n\n\n### 固定効果１母数モデル\n\n通常の１母数モデルに，過分散を説明するための固定効果の項 $\\al_0$ を加えたモデルを考える：\n\n$$\ng(\\P[Y_{ik}=1])=\\al_{j[i]}-\\beta_{k[i]}+\\al_0,\\qquad\\al_0\\sim\\rt(3;0,2.5),\n$$\n$$\n\\al_j\\sim\\rN(\\mu_\\al,\\sigma_\\al^2),\\quad\\mu_\\al\\sim\\rN(0,3),\\quad\\sigma_\\al\\sim\\rN(0,3),\n$$\n$$\n\\beta_k\\sim\\rN(\\mu_\\beta,\\sigma_\\beta^2),\\quad\\mu_\\beta\\sim\\rN(0,3),\\quad\\sigma_\\beta\\sim\\rN(0,3).\n$$\n\n`sd` というクラスはグループレベル変数の標準偏差を意味する．\n\n$\\al_j,\\beta_k$ の定数の違いに関する識別不可能性は，いずれも $0$ を中心とした\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula_1PL <- bf(r2 ~ 1 + (1|item) + (1|id))\nprior_1PL <-  prior(\"normal(0,3)\", class=\"sd\", group = \"id\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\")\nfit_1PL <- brm(\n  formula_1PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fit_1PL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n          normal(0,3)        sd              id                  0   \n          normal(0,3)        sd Intercept    id                  0   \n          normal(0,3)        sd            item                  0   \n          normal(0,3)        sd Intercept  item                  0   \n       source\n      default\n      default\n         user\n (vectorized)\n         user\n (vectorized)\n```\n\n\n:::\n:::\n\n\n\n`vectorized` というのは，下記 Stan コード内で尤度は for 文で構成されるが，このループに入れなくて良いものがある場合をいう．\n\n::: {.callout-tip collapse=\"true\" title=\"Stan コードの表示\"}\n\n```r\nstancode(fit_1PL)\n```\n\nによって推定に用いられた Stan コードが表示できる．\n\n次を見る限り，確かに意図したモデルになっている：\n\n```stan\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int<lower=1> N_1;  // number of grouping levels\n  int<lower=1> M_1;  // number of coefficients per level\n  array[N] int<lower=1> J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int<lower=1> N_2;  // number of grouping levels\n  int<lower=1> M_2;  // number of coefficients per level\n  array[N] int<lower=1> J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector<lower=0>[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector<lower=0>[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += normal_lpdf(sd_1 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n  lprior += normal_lpdf(sd_2 | 0,3)\n    - 1 * normal_lccdf(0 | 0,3);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n```\n\n`-1*normal_lccdf(0|0,3)` というのは定数であり，推定には全く影響を与えないが，後続の `bridgesampling` パッケージ [@Gronau-Singmann-Wagenmakers2020] によるモデル比較の API 構築のために付けられたものである [@Burkner2021 p.21]．\n\n:::\n\n<!-- \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum <- summary(fit_1PL)\nrow1 <- data.frame(sum$fix)\nrow2 <- data.frame(sum$random[1]) %>%\n  setNames(colnames(row1))\nrow3 <- data.frame(sum$random[2]) %>%\n  setNames(colnames(row1))\nkable(rbind(row1, row2, row3))\n```\n\n::: {.cell-output-display}\n\n\n|               |   Estimate| Est.Error|   l.95..CI|  u.95..CI|     Rhat| Bulk_ESS|  Tail_ESS|\n|:--------------|----------:|---------:|----------:|---------:|--------:|--------:|---------:|\n|Intercept      | -0.1593485| 0.2742444| -0.6884396| 0.3900238| 1.005955| 220.0006|  414.0010|\n|sd(Intercept)  |  1.3922165| 0.0713446|  1.2613895| 1.5378435| 1.003541| 785.6063| 1861.6443|\n|sd(Intercept)1 |  1.2098521| 0.1938848|  0.9070696| 1.6589571| 1.005652| 468.5274|  952.8326|\n\n\n:::\n:::\n\n\n-->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_1PL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ 1 + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.39      0.07     1.26     1.54 1.00      786     1862\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.21      0.19     0.91     1.66 1.01      469      953\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.16      0.27    -0.69     0.39 1.01      220      414\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n低い ESS から変動効果の項 $\\ep_i$ の推定に苦労していることがわかる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit_1PL)\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nここにはグローバルなパラメータしか表示されておらず，ランダム効果の結果は次のように見る必要がある：\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nranef_item <- ranef(fit_1PL)$item\nposterior_means <- ranef_item[,1,1]\nlower_bounds <- ranef_item[,3,1]\nupper_bounds <- ranef_item[,4,1]\nplot_df_item <- data.frame(\n  item = rownames(ranef_item),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_PL1 <- ggplot(plot_df_item, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Items\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\np_PL1\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n多くの参加者にとって腹立たしい例とそうでない例が区別できているようである．\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_df_id <- plot_df_id %>% arrange(mean) %>% mutate(rank = row_number())\np_PL1_id <- ggplot(plot_df_id, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Posterior Means and 95% Credible Intervals for Individuals\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\np_PL1_id\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nこうして怒りやすかった人を並べることができる．\n\nしかしガタガタしている区分定数的な模様が見れる．実はこれは item の分だけある．というのも，「何個の項目に Yes と答えたか」だけが $\\al_j$ を決める要因になってしまっているためである．\n\nこれが項目識別のできない１母数モデルの限界である．\n\n### 固定効果２母数モデル\n\n項目識別力母数 $\\gamma_k$ を導入する：\n$$\ng(\\mu_i)=\\gamma_{k[i]}\\Paren{\\al_{j[i]}-\\beta_{k[i]}},\n$$\n\nすると追加の制約が必要になる．ここでは理想点モデルの場合と違い，研究のデザインから $\\gamma_{k[i]}$ は正として良いだろう．\n\nこれを変数変換 $\\gamma_k=\\exp(\\log\\gamma_k)$ によってモデルに知らせることとする．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula_2PL <- bf(\n  r2 ~ exp(loggamma) * eta,\n  loggamma ~ 1 + (1|i|item),\n  eta ~ 1 + (1|i|item) + (1|id),\n  nl = TRUE\n)\n```\n:::\n\n\n\n$g(\\mu_i)$ の右辺はもはや $\\log\\gamma_k$ の線型関数ではないので，これを `nl=TRUE` によって知らせる必要がある．\n\n`|i|` によって，$\\log\\gamma_k$ と $\\eta_{jk}$ 内の項 $\\beta_k$ には相関があることを知らせている [@Burkner2018 p.397]．項目難易度 $\\beta_k$ が低いほど識別力 $\\log\\gamma_k$ は低いとしているのである．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_2PL <-  prior(\"normal(0,5)\", class=\"b\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"b\", nlpar = \"loggamma\") +\n  prior(\"constant(1)\", class=\"sd\", group = \"id\", nlpar = \"eta\") +\n  prior(\"normal(0,3)\", class=\"sd\", group = \"item\", nlpar = \"eta\") +\n  prior(\"normal(0,1)\", class=\"sd\", group = \"item\", nlpar = \"loggamma\")\n\nfit_2PL <- brm(\n  formula = formula_2PL,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_2PL,\n  chains = 4, cores = 4\n)\n```\n:::\n\n\n\nついに Stan が２分ほどかかるようになった上に，収束に苦労しており，ESS が低くなっている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_2PL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ exp(loggamma) * eta \n         loggamma ~ 1 + (1 | i | item)\n         eta ~ 1 + (1 | i | item) + (1 | id)\n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~item (Number of levels: 24) \n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(loggamma_Intercept)                    0.11      0.06     0.01     0.24 1.02\nsd(eta_Intercept)                         0.92      0.15     0.67     1.27 1.00\ncor(loggamma_Intercept,eta_Intercept)     0.34      0.38    -0.58     0.93 1.02\n                                      Bulk_ESS Tail_ESS\nsd(loggamma_Intercept)                     376      298\nsd(eta_Intercept)                         1551     2198\ncor(loggamma_Intercept,eta_Intercept)      239      196\n\n~id (Number of levels: 316) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(eta_Intercept)     1.00      0.00     1.00     1.00   NA       NA       NA\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nloggamma_Intercept     0.32      0.06     0.21     0.43 1.00     1264     2299\neta_Intercept         -0.15      0.20    -0.55     0.26 1.00     2008     2014\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nranef_item2 <- ranef(fit_2PL)$item\nposterior_means <- ranef_item2[,1,\"eta_Intercept\"]\nlower_bounds <- ranef_item2[,3,\"eta_Intercept\"]\nupper_bounds <- ranef_item2[,4,\"eta_Intercept\"]\nplot_df_item2 <- data.frame(\n  item = rownames(ranef_item2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_PL2 <- ggplot(plot_df_item2, aes(x = mean, y = item)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Item\")\ngrid.arrange(p_PL1, p_PL2, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n識別力パラメータ $\\gamma_k$ が $1$ より大きい値をとっており，これが変動を吸収しているため，$\\al_j$ は $0$ に縮小されて推定されるようになっている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranef_id2 <- ranef(fit_2PL)$id\nposterior_means <- ranef_id2[,1,\"eta_Intercept\"]\nlower_bounds <- ranef_id2[,3,\"eta_Intercept\"]\nupper_bounds <- ranef_id2[,4,\"eta_Intercept\"]\nplot_df_id2 <- data.frame(\n  id = rownames(ranef_id2),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_df_id2 <- plot_df_id2 %>% arrange(mean) %>% mutate(rank = row_number())\np_PL2_id <- ggplot(plot_df_id2, aes(x = mean, y = rank)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Individual\")\ngrid.arrange(p_PL1_id, p_PL2_id, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n少し滑らかになっている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(ranef_id[,1,\"Intercept\"], ranef_id2[,1,\"eta_Intercept\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9995785\n```\n\n\n:::\n:::\n\n\n\nしかし線型の相関になっており，軟化以上の変化は導入されなかったことがわかる．\n\nそれもそうである．モデルの表現力はあげたから解像度は高くなったが，モデルに新しい情報を入れたわけではないのである．\n\n### 共変量の追加\n\n理想点モデルなど多くの項目応答モデルは，$\\al_j,\\beta_k$ の推定に終始してきたが，本当のリサーチクエスチョンはその先にある．\n\n個人レベルの共変量を追加した階層モデルを構築して，$\\al_j$ の位置や応答の傾向への影響を調べることが真の目標であった．\n\n#### 項目共変量の追加\n\n本データにおいて項目は $2\\times2\\times3$ の split-plot デザインがなされている．\n\n`mode` とは「悪態をつきたい」と「咄嗟についてしまう」という２種の行動を区別するためのものである．この２つの行動容態は，本人の抑制的な意識が実際に働いたかどうかにおいて全く質的に異なる．モデルにこれを教えたらどうなるだろうか？\n\n`situ` とはシチュエーションであり，自分に責任があるか（「店に入ろうとした瞬間閉店時間になった」など）他人に責任があるか（「バスが止まってくれなかった」など）の２項目がある．\n\n`btype` は行動様式であり，「悪態をつく」「叱る」「怒鳴りつける」の３項目がある．後に行くほど他人への攻撃性が強い．\n\n最初に考えられるモデル\n\n```r\nr2 ~ btype + situ + mode + (1|item) + (1 + mode|id)\n```\n\nは，元々の１母数モデルに変動切片項を３つ追加した上に，`mode` の係数を個人ごとに変えることを許したものである．これは `mode` の効果が個人ごとに異なるだろうという信念による．\n\nしかしこのモデルに至る前に，`1` を `0` にすることで `modedo` と `modewant` 双方の標準偏差を推定することを考える（`1` の場合は `modewant` の標準偏差の代わりに `Intercept` の標準偏差を推定する）．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula_1PL_cov <- bf(\n  r2 ~ btype + situ + mode + (1|item) + (0 + mode|id)\n)\nfit_1PL_cov <- brm(\n  formula = formula_1PL_cov,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_1PL_cov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ btype + situ + mode + (1 | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.47      0.09     1.31     1.65 1.00     1843\nsd(modedo)               1.67      0.10     1.48     1.88 1.00     1589\ncor(modewant,modedo)     0.77      0.04     0.69     0.84 1.00     1378\n                     Tail_ESS\nsd(modewant)             3080\nsd(modedo)               2683\ncor(modewant,modedo)     2492\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.46      0.09     0.32     0.67 1.00     1324     2306\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      1.87      0.25     1.39     2.35 1.00     1832     2496\nbtypescold    -1.11      0.24    -1.61    -0.64 1.00     1960     2207\nbtypeshout    -2.22      0.25    -2.70    -1.74 1.00     2012     2028\nsituself      -1.11      0.20    -1.51    -0.72 1.00     2425     2824\nmodedo        -0.77      0.22    -1.22    -0.34 1.00     2045     2218\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n`modedo` の係数が負になっており，悪態をつきたくなっても，実際にする人の割合は下がることがわかる．\n\nだが係数の `-0.77` が大きいかどうかがわからない．これには対数オッズ比のスケールから元のスケールに戻す便利な関数がある：\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit_1PL_cov, \"mode\")\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n確率としての減少は軽微だがあることがわかる．次に気づくことは `do` の方がエラーバーが長いことである．２つの係数は相関しているので，頻度論的な検定は難しいかもしれないが，２つの標準偏差の差の事後分布を見ることでチェックすることができる：\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhyp <- \"modedo - modewant > 0\"\nhypothesis(fit_1PL_cov, hyp, class = \"sd\", group = \"id\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHypothesis Tests for class sd_id:\n             Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (modedo-modewant) > 0      0.2      0.11     0.02     0.39      26.21\n  Post.Prob Star\n1      0.96    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n```\n\n\n:::\n:::\n\n\n\n`0.96` の確率で `modedo` の標準偏差の方が大きいことがわかるが，その差も `0.2` ほどで，対数オッズ比としては大したことがないと思われる．\n\n#### 個人共変量の追加\n\nTrait Anger スコア [@Spielberger2010] が個人ごとに算出されており（`Anger` 変数），そのスコアによってどのように項目への反応が違うかを調べる．こうするとどんどん心理学の研究っぽくなる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula_1PL_cov_id <- bf(\n  r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0+Gender|item) + (0+mode|id)\n)\nfit_1PL_cov_id <- brm(\n  formula = formula_1PL_cov_id,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_1PL_cov_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Anger + Gender + btype + situ + mode + mode:Gender + (0 + Gender | item) + (0 + mode | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(modewant)             1.49      0.09     1.32     1.67 1.00     1939\nsd(modedo)               1.58      0.10     1.40     1.79 1.00     1937\ncor(modewant,modedo)     0.78      0.04     0.71     0.85 1.00     1553\n                     Tail_ESS\nsd(modewant)             2936\nsd(modedo)               2880\ncor(modewant,modedo)     2607\n\n~item (Number of levels: 24) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(GenderF)              0.52      0.11     0.35     0.77 1.00     1321\nsd(GenderM)              0.34      0.11     0.15     0.58 1.00     1835\ncor(GenderF,GenderM)     0.78      0.18     0.31     0.99 1.00     1981\n                     Tail_ESS\nsd(GenderF)              2064\nsd(GenderM)              2489\ncor(GenderF,GenderM)     2170\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          0.72      0.44    -0.14     1.57 1.00     1465     1926\nAnger              0.06      0.02     0.02     0.09 1.00     1357     2049\nGenderM           -0.09      0.24    -0.55     0.39 1.00     1581     2275\nbtypescold        -1.03      0.22    -1.46    -0.59 1.00     1932     2447\nbtypeshout        -2.43      0.25    -2.91    -1.95 1.00     1616     2465\nsituself          -1.03      0.18    -1.39    -0.69 1.00     2087     2038\nmodedo            -0.98      0.24    -1.47    -0.52 1.00     2010     2411\nGenderM:modedo     0.90      0.25     0.41     1.39 1.00     3022     3093\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(fit_1PL_cov_id, c(\"Anger\", \"mode:Gender\"))\n```\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](BayesGLM_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n\n\n\n`Anger` の値が大きいほど悪態をつく確率が綺麗に上がっていく様子がわかる．\n\n加えて，女性の方が悪態を吐こうと思っても，実際に行動に移すには大きな壁があることがわかる．こうして `mode` と `Gender` の間の交絡が陽の下に明らかになった．\n\nこのような，項目共変量と個人共変量の間の交絡は **特異項目機能** (DIF: Differential Item Functioning) と呼ばれる．項目の特性が，被験者のグループによって違った機能を示すことは，例えばテスト理論では個人の潜在特性を推定する際の重大なノイズ要因となっており，これを統制することが重要な課題になる．\n\n### 特異項目機能の解析\n\nこの特異項目機能を，項目の特性ごとにさらに詳しく見ていく．\n\n特に怒鳴りつける行動様式を除き，悪態をつく行為と叱る行為は，男性と女性において違う機能を持っているのではないか？という仮説を検証してみる．\n\n女性が実際に悪態をつく／叱る行為にだけマークをつけるダミー変数 `dif` を用意する：\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$dif <- as.numeric(with(\n  df,\n  Gender == \"F\" & mode == \"do\" & btype %in% c(\"curse\", \"scold\")\n))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nformula_1PL_dif <- bf(\n  r2 ~ Gender + dif + (1|item) + (1|id)\n)\nfit_1PL_dif <- brm(\n  formula = formula_1PL_dif,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  prior = prior_1PL,\n  chains = 4, cores = 4\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_1PL_dif)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: r2 ~ Gender + dif + (1 | item) + (1 | id) \n   Data: df (Number of observations: 7584) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 316) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.40      0.07     1.26     1.56 1.00     1200     2057\n\n~item (Number of levels: 24) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.34      0.21     1.00     1.84 1.00     1060     1885\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.08      0.29    -0.49     0.66 1.01      568     1098\nGenderM      -0.01      0.21    -0.41     0.41 1.00     1180     1825\ndif          -0.94      0.14    -1.23    -0.66 1.00     5678     3007\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n`dif` の係数 `-0.94` を見ることで，殊に「女性」と「実際に悪態を吐いたり叱ったりする」という組み合わせは特異な項目機能を持っていることがわかる．\n\n### 極大階層モデルの推論\n\n複雑な階層モデルであっても，Stan が実装する HMC サンプラーは効率的に推論を実行してくれるようである．これを見ていこう．\n\n\n## 文献案内 {.appendix}\n\n[@Burkner2021] に項目応答モデルのベイズ的な扱いが取り上げられている．特にパッケージ `brms` を用いた例が３つある．\n\nDIF に関する日本語文献に [@熊谷龍一2012] がある．",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}