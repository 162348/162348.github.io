{
  "hash": "048a8685e2d74ba84c5c800a71eed908",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"変分推論１\"\nsubtitle: \"K-平均アルゴリズム\"\nauthor: \"司馬博文\"\ndate: 2/3/2024\ndate-modified: 2/17/2024\ncategories: [Computation, Python]\ntoc: true\nnumber-sections: true\ncode-block-bg: true\ncode-overflow: wrap\ncode-fold: true\ncode-annotations: select\nbibliography: \n    - ../../../mathematics.bib\n    - ../../../bib.bib\ncsl: ../../../apa.csl\ncrossref:\n    sec-prefix: 節\n    eq-prefix: 式\n    def-prefix: 定義\n    def-title: 定義\n    thm-prefix: 定理\n    thm-title: 定理\n    fig-prefix: 図\n    fig-title: 図\nabstract-title: 概要\nabstract: 数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である $K$-平均法の説明から始める．$K$-平均法は第一義的にはモデルフリーの（確率論と関係のない）クラスタリングアルゴリズムである．\n---\n\n::: {.hidden}\n$$\n\\usepackage[all]{xy}\\usepackage{amsmath}\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n\n%%% 演算子\n\\DeclareMathOperator{\\grad}{\\mathrm{grad}}\\DeclareMathOperator{\\rot}{\\mathrm{rot}}\\DeclareMathOperator{\\divergence}{\\mathrm{div}}\\DeclareMathOperator{\\tr}{tr}\\DeclareMathOperator{\\Tr}{Tr}\\newcommand{\\pr}{\\mathrm{pr}}\n\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n\n\n%%% 線型代数学\n\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\DeclareMathOperator{\\rank}{\\mathrm{rank}}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\\DeclareMathOperator{\\sgn}{sgn}\n%%% 複素解析学\n\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n%%% 集合と位相\n\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\DeclareMathOperator{\\maj}{\\mathrm{maj}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\n\n%%% 形式言語理論\n\\newcommand{\\REGEX}{\\mathrm{REGEX}}\\newcommand{\\RE}{\\mathbf{RE}}\n%%% Graph Theory\n\\newcommand{\\SimpGph}{\\mathrm{SimpGph}}\\newcommand{\\Gph}{\\mathrm{Gph}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n\n%%% 多様体\n\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n\\DeclareMathOperator{\\Exp}{Exp}\\DeclareMathOperator{\\codim}{codim}\n%%% 代数\n\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsub}{\\triangleleft}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n%%% 代数的位相幾何学\n\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n%%% 微分幾何学\n\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n\n%%% 函数解析\n\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n%%% 積分論\n\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\E}{\\operatorname{E}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\DeclareMathOperator*{\\argmax}{arg\\,max}\\DeclareMathOperator*{\\argmin}{arg\\,min}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\newcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\newcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\DeclareMathOperator{\\Dom}{\\mathrm{Dom}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n%%% Fourier解析\n\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n%%% 数値解析\n\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n\n%%% 確率論\n\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\F}{\\mathcal{F}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n\\DeclareMathOperator{\\Ent}{Ent}\\DeclareMathOperator{\\Polya}{Polya}\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\DeclareMathOperator{\\LR}{LR}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n%%% 情報理論\n\\newcommand{\\bit}{\\mathrm{bit}}\\DeclareMathOperator{\\sinc}{sinc}\n%%% 量子論\n\\newcommand{\\err}{\\mathrm{err}}\n%%% 最適化\n\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\DeclareMathOperator{\\minimize}{minimize}\\DeclareMathOperator{\\subjectto}{subject to}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n%%% 数理ファイナンス\n\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n\n%%% 偏微分方程式\n\\DeclareMathOperator{\\div}{div}\n\\newcommand{\\del}{\\partial}\n\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n%%% 常微分方程式\n\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n%%% 統計力学\n\\newcommand{\\Ens}{\\mathrm{Ens}}\n%%% 解析力学\n\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n\n%%% 統計的因果推論\n\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n%%% 応用統計学\n\\DeclareMathOperator{\\pl}{pl}\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n%%% 数理統計\n\\DeclareMathOperator{\\arctanh}{arctanh}\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\newcommand{\\Beta}{\\mathrm{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\DeclareMathOperator{\\rN}{N}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n\\DeclareMathOperator{\\erf}{erf}\n%%% 計量経済学\n\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n\n%%% 無限次元統計模型の理論\n\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n%%% Banach Lattices\n\\newcommand{\\Slv}{\\mathrm{Slv}}\\newcommand{\\Hypo}{\\mathrm{Hypo}}\\newcommand{\\CL}{\\mathrm{CL}}\\DeclareMathOperator{\\ba}{ba}\\DeclareMathOperator{\\ca}{ca}\n\n%%% 圏\n\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\\newcommand{\\Alg}{\\mathrm{Alg}} %代数の圏\n\\newcommand{\\Met}{\\mathrm{Met}} %Metric space & Contraction maps\n\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}} %確率空間とMarkov核の圏\n\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n\\newcommand{\\Sob}{\\mathrm{Sob}} %Sober space & continuous map\n\\newcommand{\\Op}{\\mathrm{Op}} %Category of open subsets\n\\newcommand{\\Sh}{\\mathrm{Sh}} %Category of sheave\n\\newcommand{\\PSh}{\\mathrm{PSh}} %Category of presheave, PSh(C)=[C^op,set]のこと\n\\DeclareMathOperator{\\Conv}{Conv} %Convergence spaceの圏\n\\newcommand{\\Unif}{\\mathrm{Unif}} %一様空間と一様連続写像の圏\n\\newcommand{\\Frm}{\\mathrm{Frm}} %フレームとフレームの射\n\\newcommand{\\Locale}{\\mathrm{Locale}} %その反対圏\n\\newcommand{\\Diff}{\\mathrm{Diff}} %滑らかな多様体の圏\n\\newcommand{\\Quiv}{\\mathrm{Quiv}} %Quiverの圏\n\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n\n%%% SMC\n\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{iid}}{\\sim}}\\DeclareMathOperator{\\KL}{KL}\\DeclareMathOperator{\\JS}{JS}\\DeclareMathOperator{\\ESS}{ESS}\\DeclareMathOperator{\\MSE}{MSE}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n\n%%% 括弧類\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\bracket}[1]{\\langle#1\\rangle}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Bracket}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\n\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\n\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\n\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\n\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\n\n%%% 予約語\n\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n\n%%% 略記\n\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n\\newcommand{\\lmd}{\\lambda}\n\\newcommand{\\Lmd}{\\Lambda}\n\\newcommand{\\cI}{\\mathcal{I}}\n\n%%% 矢印類\n\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n$$\n:::\n\n\n\n本稿では，[$K$-平均アルゴリズム](https://ja.wikipedia.org/wiki/K%E5%B9%B3%E5%9D%87%E6%B3%95) によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次の [稿](VI2.qmd) で，$K$-平均アルゴリズムの確率的な一般化として [EM アルゴリズム](https://ja.wikipedia.org/wiki/EM%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0) を説明し，その共通の問題点「初期値依存性」と「局所解へのトラップ」の数理的な理解を目指す．\n\nより図が見やすい PDF 版は [こちら](report1.pdf)．\n\n## 導入\n\n### 歴史 {#sec-history}\n\nハード $K$-平均法はモデルフリーのクラスタリングアルゴリズムである．\n\n一方で原論文 [@Lloyd1982] では，[パルス符号変調](https://ja.wikipedia.org/wiki/%E3%83%91%E3%83%AB%E3%82%B9%E7%AC%A6%E5%8F%B7%E5%A4%89%E8%AA%BF) の文脈で，アナログ信号の量子化の方法として提案している．^[Lloyd は 1957 年には発表していたが，論文の形になったのが 1982 である．$K$-means という名前の初出は [@MacQueen1967] とされている．]\n\n実際，いまでも $K$-平均法は（非可逆）データ圧縮に用いられる．クラスター中心での画像の値と，それ以外では帰属先のクラスター番号のみを保存すれば良いというのである．このようなアプローチを **ベクトル量子化** (vector quantization) という．^[[@MacKay2003 p.284]，[@Bishop2006 p.429]．クラスター中心は **符号表ベクトル** または 代表ベクトル (code-book vector) という．]\n\nソフト $K$-平均法とは，このようなデータ点のクラスターへの一意な割り当てを，ソフトマックス関数を用いて軟化したアルゴリズムであり，多少アルゴリズムとしての振る舞いは改善するとされている．\n\n### 最適化アルゴリズムとしての見方\n\n$N$ 個のデータ $\\{x_n\\}_{n=1}^N$ の $K$ クラスへの $K$-平均クラスタリングアルゴリズムは，ハードとソフトの二種類存在するが，いずれも\n$$\nJ:=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\norm{x_n-\\mu_k}^2\n$$\nという損失関数の逐次最小化アルゴリズムとみなせる．\n\nこの見方は，[ML アルゴリズム](VI2.qmd) への一般化の軸となる．\n\n### 用いるデータ {#sec-data}\n\n実際のコードとデータを用いて $K$-平均法を解説する．\n\nまずは，解説にために作られた，次のような３つのクラスタからなる２次元のデータを考え，これの正しいクラスタリングを目指す．\n\n\n\n::: {#ecc9f199 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-3-output-1.png){width=309 height=283}\n:::\n:::\n\n\n## ハード $K$-平均法 {#sec-hard-k-means}\n\n### アルゴリズムの説明\n\nhead $K$-means algorithm はデータ $\\{x^{(n)}\\}_{n=1}^N\\subset\\R^I$ とクラスタ数 $K\\in\\N^+$，そして初期クラスター中心 $(m^{(k)})_{k=1}^K\\in(\\R^I)^K$ の３組をパラメータに持つ．\n\nsoft $K$-means algorithm [-@sec-soft-k-means] はさらに硬度パラメータ $\\beta\\in\\R_+$ を持つ．\n\n`numpy` の提供する行列積を利用して，これを Python により実装した例を以下に示す．ソフト $K$-平均法の実装と対比できるように，負担率を通じた実装を意識した例である．\n\nアノテーションを付してあるので，該当箇所（右端の丸囲み数字）をクリックすることで適宜解説が読めるようになっている．\n\n\n\n```{.python}\ndef hkmeans_2d(data, K, init, max_iter=100):\n    \"\"\"\n    ２次元データに対するハード K-平均法の実装例．\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n\n    Returns:\n    - clusters: (N,)-numpy.ndarray クラスター番号\n    \"\"\"\n\n    N = data.shape[0]  # <1>\n    I = data.shape[1]  # <2>\n    m = init  # <3>\n    r = np.zeros((K, N), dtype=float)  # <4>\n\n    for _ in range(max_iter):\n        # Assignment Step\n        for i in range(N):\n            distances = np.array([d(data[i], m[:,k]) for k in range(K)]) # <5>\n            k_hat = np.argmin(distances)  # <6>\n            r[:,i] = 0  # <7>\n            r[k_hat,i] = 1\n        \n        # Update Step\n        new_m = np.zeros_like(m, dtype=float) # <8>\n        numerator = np.dot(r, data)  # <9>\n        denominator = np.sum(r, axis=1) # <10>\n        for k in range(K): # <11>\n            if denominator[k] > 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = m[:,k]\n\n        if np.allclose(m, new_m): # <12>\n            break\n        m = new_m\n    \n    return np.argmax(r, axis=0)  # <13>\n```\n\n1. データ数を取得している．\n2. データの次元を取得している．今回はすべて２次元データを用いる．\n3. クラスター中心に引数として受け取った初期値を代入. $2×K$-行列であることに注意．\n4. 負担率を $K×N$-行列として格納している．その理由は後ほど行列積を通じた計算を行うためである．`dtype=float` の理由は後述．\n5. この `distances` 変数は `(K,)-numpy.ndarray` になる．すなわち，第 $k$ 成分が，第 $k$ クラスター中心との距離となっているようなベクトルである．ただし，`d` は Euclid 距離を計算する関数として定義済みとした．\n6. 距離が最小となるクラスター番号 $\\hat{k}:=[\\argmin_{k\\in[K]}d(m_k,x_i)]$ を，$i\\in[N]$ 番目のデータについて求める．\n7. $\\hat{k}$ に基づいて負担率を更新するが，ループ内で前回の結果をリセットする必要があることに注意．\n8. ここで `dtype=float` と指定しないと，初め引数 `init` が整数のみで構成されていた場合に，Python の自動型付機能が `int` 型だと判定し，クラスター中心 `m` の値が整数に限られてしまう．すると，アルゴリズムがすぐに手頃な格子点に収束してしまう．\n9. `numpy` の行列積を計算する関数 `np.dot` を使用している．更新式\n$$\nm^{(k)}\\gets\\frac{\\sum_{n=1}^Nr^{(n)}_kx^{(n)}}{\\sum_{n=1}^Nr^{(n)}_k}\n$$\nの分子を行列積と見たのである．\n10. 分母 (denominator) は $(K,N)$-行列 `r` の行和として得られる．\n11. ゼロによる除算が起こらないように場合わけをしている．\n12. クラスター中心がもはや変わらない場合はアルゴリズムを終了する．\n13. 負担率の最も大きいクラスター番号を返す．今回は `hat_k` の列をそのまま返せば良いが，soft $K$-means アルゴリズムにも通じる形で実装した．\n\n::: {.callout-caution icon=\"false\" title=\"注：実際に用いる実装\" collapse=\"true\"}\n\nただし，本記事の背後では次の実装を用いる．\n\nクラスター中心の推移のヒストリーを保存して図示に利用したり，負担率 `r` の中身を見たりすることが出来るようにするため，assignment step と update step とに分けてクラスメソッドとして実装し，`run` メソッドでそれらを呼び出すようにしている．これに `fetch_cluster` と `fetch_history` メソッドを加えることで，クラスター番号とクラスター中心の推移を取得することが出来る．フィールド `.r` から（最終的な）負担率を見ることもできる．\n\n::: {#5636be44 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nclass kmeans_2d:\n    \"\"\"\n    ２次元データに対するソフト K-平均法の実装．\n\n    Usage:\n        kmeans = kmeans_2d(data, K, init, beta)\n        kmeans.run()\n\n    Parameters:\n    - data: (N,2)-numpy.ndarray\n    - K: int クラスター数\n    - init: (2,K)-numpy.ndarray 初期値\n    - beta: float 硬度パラメータ\n    \"\"\"\n\n    def __init__(self, data, K, init, beta, max_iter=100):\n        self.data = np.array(data, dtype=float)\n        self.K = K\n        self.init = np.array(init, dtype=float)\n        self.beta = float(beta)\n        self.max_iter = max_iter\n        self.N = data.shape[0]  # データ数\n        self.I = data.shape[1]  # 次元数 今回は２\n        self.m = init  # クラスター中心の初期化．2×K行列．\n        self.r = np.zeros((K, self.N), dtype=float)  # 負担率．K×N行列．\n        self.history = [init.copy()] # クラスター中心の履歴．2×K行列．\n    \n    def soft_assigment(self):\n        \"\"\"soft K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) ** 2 for j in range(self.K)]) # (N,)-numpy.ndarray\n            denominator_ = np.sum(np.exp(-self.beta * distances))  # 分母\n            self.r[:,i] = np.exp(- self.beta * distances) / denominator_\n\n    def hard_assigment(self):\n        \"\"\"hard K-means の場合の負担率の更新\"\"\"\n        for i in range(self.N):\n            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray\n            k_hat = np.argmin(distances)  # 最小距離のクラスター番号\n            self.r[:,i] = 0  # 前のループの結果をリセット\n            self.r[k_hat,i] = 1\n    \n    def update(self):\n        \"\"\"クラスター中心の更新\"\"\"\n        new_m = np.zeros_like(self.m, dtype=float) # ここで float にしないと，クラスター中心が整数に限られてしまう．\n        numerator = np.dot(self.r, self.data)  # (K,2)-numpy.ndarray\n        denominator = np.sum(self.r, axis=1)  # 各クラスターの負担率の和\n        for k in range(self.K):\n            if denominator[k] > 0:\n                new_m[:,k] = numerator[k] / denominator[k]\n            else:\n                new_m[:,k] = self.m[:,k]\n        self.m = new_m\n\n    def fetch_cluster(self):\n        \"\"\"最終的なクラスター番号を格納した (N,)-array を返す\"\"\"\n        return np.argmax(self.r, axis=0)\n    \n    def fetch_history(self):\n        \"\"\"クラスター中心の履歴を格納したリストを，３次元の np.array に変換して返す\"\"\"\n        return np.stack(self.history, axis=0)\n\n    def run_soft(self):\n        \"\"\"soft K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.soft_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n    \n    def run_hard(self):\n        \"\"\"hard K-means アルゴリズムの実行\"\"\"\n        for _ in range(self.max_iter):\n            self.hard_assigment()\n            self.update()\n            self.history.append(self.m.copy())\n            if np.allclose(self.history[-1], self.history[-2]):\n                break\n```\n:::\n\n\nなお，この実装は $\\beta\\ge500$ などの場合にオーバーフローが起こることに注意．これへの対処は `logsumexp` の使用などが考えられる．\n\n:::\n\n### 初期値依存性\n\n次の２つの初期値を与えてみる．\n$$\nm_1:=\\vctr{4}{0},\\quad m_2:=\\vctr{1}{4},\\quad m_3=\\vctr{-1}{1},\n$$\nと，$m_2,m_3$ は変えずに $m_1$ の $y$-座標を $1$ だけ下げたもの\n$$\nm_1':=\\vctr{4}{-1}\n$$\nとを初期値として与えてみる．\n\n::: {#cell-fig-1 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![ハード K-平均法によるクラスタリングの結果．初期値は $(m_1,m_2,m_3)=\\paren{\\vctr{4}{0},\\vctr{1}{4},\\vctr{-1}{1}}$．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．](VI_files/figure-html/fig-1-output-1.png){#fig-1 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 51     正解率: 56.7 %     反復数: 9 回\n```\n:::\n:::\n\n\n別の初期値を与えてみる（右下の点 $m_1$ を $1$ だけ下に下げただけ）：\n$$\n\\vctr{4}{0}=m_1\\mapsto m_1':=\\vctr{4}{-1}\n$$\n\n::: {#cell-fig-2 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![ハード K-平均法によるクラスタリングの結果．初期値は $(m_1',m_2,m_3)=\\paren{\\vctr{4}{-1},\\vctr{1}{4},\\vctr{-1}{1}}$．](VI_files/figure-html/fig-2-output-1.png){#fig-2 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 85     正解率: 94.4 %     反復数: 7 回\n```\n:::\n:::\n\n\n結果が全く変わり，$(m_1',m_2,m_3)$ を与えた方が，大きく正解に近づいている．具体的には，右下の初期値 $m_1$ は右上の島に行くが，$m_1'$ は左下の島に行ってくれる．\n\n**ハード $K$-平均アルゴリズムは初期値に敏感である** ことがよく分かる．\n\n### 局所解への収束\n\n直前の結果[-@fig-2]ではクラスター２と３の境界線で４つのミスを犯しており，これを修正できないか試したい．\n\nそこで，答えに近いように，\n$$\nm_1\\gets\\vctr{2.5}{2},\\;\\; m_2\\gets\\vctr{-1}{-1},\\;\\; m_3\\gets\\vctr{1}{-2},\n$$\nを初期値として与えてみて，正答率の変化を観察する．\n\n::: {#f1b04fe1 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-8-output-1.png){width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 85     正解率: 94.4 %     反復数: 5 回\n```\n:::\n:::\n\n\nもはや初期値から殆ど動いていないが，目標のクラスター３に分類された３つの点が，相変わらず３のままであり，加えてクラスター２の中心がこれらから逃げているようにも見えるので，クラスター２の初期値をよりクラスター３に近いように誘導し，クラスター３の中心をより右側から開始する：\n\n$$\nm_2:\\vctr{-1}{-1}\\mapsto\\vctr{0}{-2}\\;\\; m_3:\\vctr{1}{-2}\\mapsto\\vctr{2}{-2}\n$$\n\n::: {#b09c948f .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-9-output-1.png){width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 85     正解率: 94.4 %     反復数: 6 回\n```\n:::\n:::\n\n\nこんなに誘導をしても，正しく分類してくれない．\n\n実は，以上２つの初期値では，最終的に３つのクラスター中心は同じ値に収束している．よって，これ以上どのように初期値を変更しても，正答率は上がらないシナリオが考えられる．\n\n以上の観察から，ハード $K$-平均法はある種の **局所解に収束する** ようなアルゴリズムであると考えられる．\n\n## ソフト $K$-平均法\n\n### アルゴリズムの説明 {#sec-soft-k-means}\n\nハード $K$-平均法[-@sec-hard-k-means]では，負担率\n$$\nr_{kn}\\gets\\delta_{k}(\\argmax_{i\\in[k]}d(m_i,x_n))\n$$\nは $0,1$ のいずれかの値しか取らなかった．この振る舞いを，\n$$\n\\sigma(z;e)_i:=\\frac{e^{z_i}}{\\sum_{j=1}^Ke^{e_j}}\\quad(i\\in[K])\n$$\nで定まる [**ソフトマックス関数**](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0) $\\sigma:\\R^K\\to(0,1)^K$ を用いて，「軟化」する．\n\nここでは，$\\beta\\ge0$ として，\n$$\n\\sigma(z;e^{-\\beta})_i=\\frac{e^{-\\beta z_i}}{\\sum_{j=1}^Ke^{-\\beta e_j}}\n$$\nの形で用い，$\\argmax$ の代わりに\n$$\n\\begin{align*}\n    r_{kn}&\\gets\\sigma(d(-,x_n)^2\\circ m;e^{-\\beta})_k\\\\\n    &=\\frac{e^{-\\beta d(m_k,x_n)^2}}{\\sum_{j=1}^K e^{-\\beta d(m_j,x_n)^2}}\n\\end{align*}\n$$\nとする．ただし，$d$ は $\\R^2$ 上の Euclid 距離とした．\n\n$\\beta$ は **硬度 (stiffness)** または逆温度と呼ぶ．^[stiffness の用語は [@MacKay2003 p.289] から．実は各クラスターに Gauss モデルを置いた場合の分散 $\\sigma^2$ に対して，$\\beta=\\frac{1}{2\\sigma^2}$ の関係がある．[次稿](VI2.qmd#sec-EM-and-K-means) 参照．] $\\beta=0$ のときは温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる $\\beta\\to\\infty$ の極限が hard $K$-means アルゴリズムに相当する．\n\n実装は例えば hard $K$-means アルゴリズム[-@sec-hard-k-means]から，負担率計算の部分のみを変更すれば良い：\n\n```{.python}\nfor i in range(N):\n        distances = np.array([d(data[i], m[:,k]) for k in range(K)]) # <1>\n        denominator_ = np.sum(np.exp(-beta * distances))  # <2>\n        r[:,i] = np.exp(-beta * distances) / denominator_ # <3>\n```\n1. データ $x_i$ とクラスター中心 $(m_k)_{k=1}^K$ との距離を計算し，ベクトル $(d(x_n,m_k))_{k=1}^K$ を `distances` に格納している．\n2. 負担率の計算\n$$\nr_{ik}=\\frac{\\exp(-\\beta d(m_k,x_i))}{\\sum_{j=1}^K\\exp(-\\beta d(m_j,x_i))}\n$$\nを２段階に分けて行なっており，分母を先に計算して変数 `denominator_` に格納している．\n3. すでに計算してある分母 `denominator_` を用いてデータ $x_i$ の負担率 $(r_{ki})_{k=1}^K$ を計算し，$(K,N)$-行列 `r` の各列に格納している．\n\n### 挙動の変化の観察\n\n逆温度をはじめに $\\beta=0.3$ としてみる．@fig-1 と全く同様な初期値\n$$\nm_1:=\\vctr{4}{0},\\quad m_2:=\\vctr{1}{4},\\quad m_3=\\vctr{-1}{1},\n$$\nを与えてみると，次の通りの結果を得る：\n\n::: {#cell-fig-3 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![左がソフト K-平均法（$\\beta=1$），右がハード K-平均法によるクラスタリングの結果（図２の左と全く同じもの）．初期値は $(m_1,m_2,m_3)=\\paren{\\vctr{4}{0},\\vctr{1}{4},\\vctr{-1}{1}}$．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．](VI_files/figure-html/fig-3-output-1.png){#fig-3 width=950 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 44 vs. 51     正解率: 48.9 % vs. 56.7 %     反復数: 28 回 vs. 9 回\n```\n:::\n:::\n\n\nクラスターの境界が変化しており，正解率は悪化している．さらに，反復数が９回であったところから，３倍に増えている（28回）．\n\nまた，右上の２つのクラスター中心の収束先は，微妙にずれているが **ほとんど一致している** 点も注目に値する．\n\n::: {.callout-note icon=\"false\" title=\"参考：最終的なクラスター中心の座標\" collapse=\"true\"}\n```{.python}\ncenters = history[-1, :, :]\ndf = pd.DataFrame(centers, columns=['Cluster1', 'Cluster2', 'Cluster3'])\nprint(df)\n```\n\n::: {#b617af1b .cell execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\n   Cluster1  Cluster2  Cluster3\nx  2.397456  2.397535 -0.036071\ny  2.047565  2.047580 -1.448288\n```\n:::\n:::\n\n\n:::\n\n@fig-2 で与えた初期値 $(m_1',m_2,m_3)$ も与えてみる．\n\n::: {#cell-fig-4 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![ソフト K-平均法（$\\beta=1$）によるクラスタリングの結果，右がハード K-平均法によるクラスタリングの結果（図２の右と全く同じもの）．初期値は $(m_1',m_2,m_3)=\\paren{\\vctr{4}{-1},\\vctr{1}{4},\\vctr{-1}{1}}$．](VI_files/figure-html/fig-4-output-1.png){#fig-4 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 59 回 vs. 7 回\n```\n:::\n:::\n\n\nクラスター境界と正答率は変わらないが，反復数がやはり７回から大きく増えている．\n\n結果はやはり @fig-3 とは大きく異なっており，ハード $K$-平均法で観察された初期値鋭敏性が，変わらず残っている．\n\n加えてこの場合も @fig-3 のクラスター１と２と同様に，クラスター２と３の中心がほぼ一致している．\n\n::: {.callout-note icon=\"false\" title=\"参考：最終的なクラスター中心の座標\" collapse=\"true\"}\n\n::: {#8e03abc7 .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\n   Cluster1  Cluster2  Cluster3\nx  2.466833 -0.369537  0.447958\ny  2.124961 -1.076874 -1.543758\n```\n:::\n:::\n\n\n:::\n\n$\\beta=0.3$ の場合のソフト $K$-平均法は，この例では **クラスター中心が融合する傾向にある** ようである．\n\n一般に，$\\beta$ が小さく，温度が大きいほど，エネルギーランドスケープに極小点が少なくなり，クラスターは同じ場所へ収束しやすくなると予想される．\n\n### 高温になるほどクラスター数は減少する\n\n初期値を直前で用いた\n$$\nm_1\\gets\\vctr{4}{-1},\\quad m_2\\gets\\vctr{1}{4},\\quad m_3\\gets\\vctr{-1}{1},\n$$\nで固定とし，さらに温度を上げて，逆温度を $\\beta=0.1$ としてみる．\n\n::: {#cell-fig-5 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![ソフト K-平均法（左$\\beta=0.1$，右$\\beta=1$）によるクラスタリングの結果．初期値は $(m_1',m_2,m_3)=\\paren{\\vctr{4}{-1},\\vctr{1}{4},\\vctr{-1}{1}}$．](VI_files/figure-html/fig-5-output-1.png){#fig-5 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 68 vs. 85     正解率: 75.6 % vs. 94.4 %     反復数: 101 回 vs. 59 回\n```\n:::\n:::\n\n\n反復数はさらに増加し，全てがほとんど同じクラスターに属する結果となってしまった．\n\n::: {.callout-note icon=\"false\" title=\"参考：最終的なクラスター中心の座標\" collapse=\"true\"}\n\n::: {#7d704597 .cell execution_count=14}\n\n::: {.cell-output .cell-output-stdout}\n```\n   Cluster1  Cluster2  Cluster3\nx  1.715903  0.862511  1.329066\ny  1.012398 -0.099845  0.511186\n```\n:::\n:::\n\n\n:::\n\n温度が大変に高い状態では，全てが乱雑で，３つのクラスターが一様・公平に負担率を持つようになった．そのため，第一歩からほとんど全体の中心へと移動し，反復数が減る．\n\n次に，温度を少し下げて，逆温度を $\\beta=2$ としてみる．\n\n::: {#cell-fig-6 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![ソフト K-平均法（左$\\beta=10$，右$\\beta=1$）によるクラスタリングの結果．初期値は $(m_1',m_2,m_3)=\\paren{\\vctr{4}{-1},\\vctr{1}{4},\\vctr{-1}{1}}$．](VI_files/figure-html/fig-6-output-1.png){#fig-6 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 85 vs. 85     正解率: 94.4 % vs. 94.4 %     反復数: 17 回 vs. 59 回\n```\n:::\n:::\n\n\n初めて soft $K$-means アルゴリズムを用いた場合で，３つのクラスター中心がはっきりと別れた．反復回数は，$\\beta=0.3$ の場合と比べればやはり落ち着いている．\n\nしかし，正解率は head $K$-means の場合（ @fig-2 など）と全く同じである．実は，最終的なクラスター中心も @fig-2 の最終的なクラスター中心とほとんど同じになっている．\n\n::: {.callout-note icon=\"false\" title=\"参考：最終的なクラスター中心の座標\" collapse=\"true\"}\n\n**今回のソフト $K$-平均法の最終的なクラスター中心**\n\n::: {#8139aa1d .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\n   Cluster1  Cluster2  Cluster3\nx  2.416113  0.881629 -1.338782\ny  2.086327 -1.934090 -0.816316\n```\n:::\n:::\n\n\n@fig-2 のハード $K$-平均法の最終的なクラスター中心\n\n::: {#7df724aa .cell execution_count=17}\n\n::: {.cell-output .cell-output-stdout}\n```\n   Cluster1  Cluster2  Cluster3\nx  2.426102  0.868333 -1.323353\ny  2.091429 -1.948458 -0.765176\n```\n:::\n:::\n\n\n:::\n\n以上より，ソフト $K$-平均法は温度を上げるほどクラスター数が少なくなり，温度を下げるほどクラスター数は上がり，**十分に温度を下げるとハード $K$-平均法に挙動が似通う**．\n\n### 最適な硬度の選択\n\n$\\beta=0.2$ ではクラスターが２つに縮退し，$\\beta=1$ では hard $K$-means アルゴリズムの結果とほとんど変わらなくなる．その中間では次のように挙動が変わる：\n\n::: {#cell-fig-7 .cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n![ソフト K-平均法によるクラスタリングの結果の比較（$\\beta=0.2$ vs. $\\beta=0.25$）．](VI_files/figure-html/fig-7-output-1.png){#fig-7 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 50 vs. 86\n正解率: 55.6 % vs. 95.6 %\n```\n:::\n:::\n\n\n::: {#cell-fig-8 .cell execution_count=19}\n\n::: {.cell-output .cell-output-display}\n![ソフト K-平均法によるクラスタリングの結果の比較（$\\beta=0.3$ vs. $\\beta=0.5$）．](VI_files/figure-html/fig-8-output-1.png){#fig-8 width=662 height=374}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 85 vs. 85\n正解率: 94.4 % vs. 94.4 %\n```\n:::\n:::\n\n\nやはり，温度が高い場合はクラスター中心が合流・融合してしまいやすいが，冷却することでクラスター数は大きい状態で安定する，と言えるだろう．\n\n## 本番データセットでの実験\n\n今まで使っていたデータ[-@sec-data]はクラスターのオーバーラップはなかったため，いわば優しいデータであった．ここからはよりデータ生成過程が複雑なデータを用いて，ソフト $K$-平均法の挙動を観察する．\n\n### データの概観 {#sec-data2}\n\n今度は，次の４クラスのデータを用いる．\n\n::: {#0930c4cc .cell execution_count=20}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-21-output-1.png){width=309 height=283}\n:::\n:::\n\n\n実は，これは４つの Gauss 分布から生成されたデータである．\n\n### 最適な温度の選択\n\n::: {#d7387483 .cell execution_count=21}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-22-output-1.png){width=662 height=374}\n:::\n:::\n\n\n::: {#2fef38f5 .cell execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-23-output-1.png){width=662 height=374}\n:::\n:::\n\n\n::: {#b814e6f5 .cell execution_count=23}\n\n::: {.cell-output .cell-output-display}\n![](VI_files/figure-html/cell-24-output-1.png){width=662 height=374}\n:::\n:::\n\n\n::: {#2ad54257 .cell execution_count=24}\n\n::: {.cell-output .cell-output-stdout}\n```\n正解数: 377 vs. 366     正解率: 83.8 % vs. 81.3 %     反復数: 49 回 vs. 62 回\n正解数: 386 vs. 375     正解率: 85.8 % vs. 83.3 %     反復数: 101 回 vs. 70 回\n正解数: 378 vs. 378     正解率: 84.0 % vs. 84.0 %     反復数: 39 回 vs. 14 回\n```\n:::\n:::\n\n\n## 実験結果まとめ\n\n::: {.callout-note icon=\"false\" title=\"結論\"}\n* データ [-@sec-data] に対して，（初期値 $(m'_1,m_2,m_3)$ で）ソフト $K$-平均法を適用すると，\n  * $\\beta\\ge2$ の場合で結果はハード $K$-平均法と変わらなくなる．\n  * $\\beta=1$ の場合で結果はクラスターがほとんど２つになり，$\\beta\\le0.5$ では計算機上では実際に２つになってしまう．\n  * 正答率は $1\\le\\beta\\le1.1$ で最大であった．\n  * $\\beta$ を大きくするほど，反復回数は減少していった．\n* データ [-@sec-data2] に対しても，以上の４点について同様の傾向が確認できた．\n:::\n\nこうしてソフト $K$-平均法とハード $K$-平均法の性質は分かった．主に\n\n1. 初期値依存性\n2. クラスタ数 $K$ の選択法\n\nの問題が未解決であり，恣意性が残る．\n\n誰がどう使ってもうまくいくようなアルゴリズムであると言うことは出来ない．\n\n",
    "supporting": [
      "VI_files"
    ],
    "filters": [],
    "includes": {}
  }
}