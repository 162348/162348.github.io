{
  "hash": "3e908c54ffd3fdc9c3479aa6ebdb299a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"VAE：変分自己符号化器\"\nsubtitle: \"PyTorch によるハンズオン\"\nauthor: \"司馬博文\"\ndate: 7/28/2024\ncategories: [Deep, Sampling, Python]\nbibliography: \n    - ../../../mathematics.bib\n    - ../../../bib.bib\ncsl: ../../../apalike.csl\nabstract-title: 概要\nabstract: |\n    変分自己符号化器 (VAE) は，データを周辺分布にもつ潜在変数モデルを変分 Bayes 推論によって学習するアルゴリズムである．\n    従来計算・近似が困難であった変分下界を，ニューラルネットワークによって近似するアプローチである．\n    学習されたベイズ潜在変数モデルからはサンプリングによって新たなデータを生成することができるため，深層生成モデルの一つに分類されることもある．\ncode-fold: false\n---\n\n:::{#5fa8dccb .cell .markdown}\n::: {.hidden}\n\n::: {.content-visible when-format=\"html\"}\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n\n$$\n\n\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\BRac}[1]{\\biggl\\langle#1\\biggr\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\\newcommand{\\rN}{\\mathrm{N}}\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n\n\n\n\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n\\newcommand{\\argmax}{\\operatorname*{argmax}}\\newcommand{\\argmin}{\\operatorname*{argmin}}\n\n\\newcommand{\\pr}{\\mathrm{pr}}\n\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n\\newcommand{\\grad}{\\operatorname{grad}}\\newcommand{\\rot}{\\operatorname{rot}}\\renewcommand{\\div}{\\operatorname{div}}\\newcommand{\\tr}{\\operatorname{tr}}\\newcommand{\\Tr}{\\operatorname{Tr}}\\newcommand{\\KL}{\\operatorname{KL}}\\newcommand{\\JS}{\\operatorname{JS}}\\newcommand{\\ESS}{\\operatorname{ESS}}\\newcommand{\\MSE}{\\operatorname{MSE}}\\newcommand{\\erf}{\\operatorname{erf}}\\newcommand{\\arctanh}{\\operatorname{arctanh}}\\newcommand{\\pl}{\\operatorname{pl}}\\newcommand{\\minimize}{\\operatorname{minimize}}\\newcommand{\\subjectto}{\\operatorname{subject to}}\\newcommand{\\sinc}{\\operatorname{sinc}}\\newcommand{\\Ent}{\\operatorname{Ent}}\\newcommand{\\Polya}{\\operatorname{Polya}}\\newcommand{\\Exp}{\\operatorname{Exp}}\\newcommand{\\codim}{\\operatorname{codim}}\\newcommand{\\sgn}{\\operatorname{sgn}}\\newcommand{\\rank}{\\operatorname{rank}}\n\n\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\n\n\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n\n\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n\n\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n\n\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n\n\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n\n\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\renewcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\renewcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n\n\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n\n\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n\n\n\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\bL}{\\mathbb{L}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n\n\\newcommand{\\bit}{\\mathrm{bit}}\n\n\\newcommand{\\err}{\\mathrm{err}}\n\n\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n\n\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n\n\n\\newcommand{\\del}{\\partial}\n\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n\n\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n\n\\newcommand{\\Ens}{\\mathrm{Ens}}\n\n\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n\n\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n\n\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n\n\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\renewcommand{\\Beta}{\\mathrm{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n\n\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n\n\n\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\n\n\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}}\n\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n\\newcommand{\\Op}{\\mathrm{Op}}\n\\newcommand{\\Sh}{\\mathrm{Sh}}\n\\newcommand{\\Diff}{\\mathrm{Diff}}\n\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n\n\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{iid}}{\\sim}}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n\n\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n\n\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n\\newcommand{\\lmd}{\\lambda}\n\\newcommand{\\Lmd}{\\Lambda}\n\\newcommand{\\cI}{\\mathcal{I}}\n\n\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n$$\n\n:::\n\n:::\n\n\n\n## VAE [@Kingma-Welling2014]\n\n### 導入\n\n`PyTorch` を用いることで詳細を省略し，VAE の構造を概観することとする．\n:::\n\n::: {#143fc28a .cell execution_count=1}\n``` {}\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image, make_grid\n```\n:::\n\n\n:::{#a1518091 .cell .markdown}\n今回は，MNIST データセットを用い，隠れ次元 400 を通じて潜在次元 200 まで圧縮する．\n:::\n\n::: {#5bbecea2 .cell execution_count=2}\n``` {}\ndataset_path = '~/hirofumi/datasets'\n\nDEVICE = torch.device(\"mps\")\n\nbatch_size = 100\n\nx_dim = 784\nhidden_dim = 400\nlatent_dim = 200\n\nlr = 1e-3\n\nepochs = 30\n```\n:::\n\n\n::: {#a3f7431e .cell execution_count=3}\n``` {}\n#| code-summary: データセットをダウンロードして読み込む\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nmnist_transform = transforms.Compose([\n        transforms.ToTensor(),\n])\n\nkwargs = {'num_workers': 0, 'pin_memory': True} \n\ntrain_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\ntest_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)\n```\n:::\n\n\n:::{#6ecf22c2 .cell .markdown}\nPyTorch の [Dataset と DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) は，訓練やテスト用のデータセットの簡単なアクセスと，それに対する iterable オブジェクトを提供する．\n\n::: {.callout-important title=\"M2 Mac 上での実行\" collapse=\"true\" icon=\"false\"}\n\nまず，次のようにして仮想環境を用意する：\n```zsh\npython3 -m venv VAE\nsource VAE/bin/activate\npip install torch\n```\n\nM2 Mac では Metal Performance Shaders (MPS) という Apple の GPU アクセラレーション技術が利用可能で，PyTorch 1.12 からはこれをサポートしている．\n:::\n\n::: {#7392664d .cell execution_count=4}\n``` {}\nimport torch\nprint(torch.__version__)\nprint(torch.backends.mps.is_available())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.4.0\nTrue\n```\n:::\n:::\n\n\n:::{#a6b1f29b .cell .markdown}\n:::\n\n::: {.callout-important title=\"DataLoader worker (pid(s) 9044) exited unexpectedly\" collapse=\"true\" icon=\"false\"}\n\n上記のエラーは，`DataLoader` が並列処理によりデータを読み込むことに失敗したことを意味する．\n\nメモリ不足も考えられるが，`num_workers=0` として単一プロセスで実行することでもエラーが抑えられる．\n\n今回は軽量な計算であるから，これで良いということである．\n\n:::\n\n### モデルの定義\n\n#### エンコーダー\n\nエンコーダーはデータを受け取り，２層の全結合隠れ層を通じて，「平均」と「対数分散」の名前がついた計 400 次元の潜在表現を得る．\n:::\n\n::: {#06397143 .cell execution_count=5}\n``` {}\nclass Encoder(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(Encoder, self).__init__()\n\n        self.FC_input = nn.Linear(input_dim, hidden_dim)  # <1>\n        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n        self.FC_var   = nn.Linear(hidden_dim, latent_dim)\n        \n        self.LeakyReLU = nn.LeakyReLU(0.2)\n        \n        self.training = True\n        \n    def forward(self, x):\n        h_       = self.LeakyReLU(self.FC_input(x))\n        h_       = self.LeakyReLU(self.FC_input2(h_))  # <2>\n        mean     = self.FC_mean(h_)\n        log_var  = self.FC_var(h_)                     #  <3>\n        \n        return mean, log_var\n```\n:::\n\n\n:::{#1e5d6b40 .cell .markdown}\n1. [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) は PyTorch による全結合層 $y=xA^\\top+b$ の実装である．\n2. ここまで２層の全結合層にデータを通して，最終的な出力`h_`を得ており，次の段階で最終的な潜在表現を得る．\n3. 最後の隠れ層の出力`h_`に関して平均と対数分散という名前のついた最終的な出力を，やはり全結合層を通じて得る（最終層なので活性化なし）．\n\n#### デコーダー\n:::\n\n::: {#a72f9562 .cell execution_count=6}\n``` {}\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, hidden_dim, output_dim):\n        super(Decoder, self).__init__()\n        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_output = nn.Linear(hidden_dim, output_dim)\n        \n        self.LeakyReLU = nn.LeakyReLU(0.2)\n        \n    def forward(self, x):\n        h     = self.LeakyReLU(self.FC_hidden(x))\n        h     = self.LeakyReLU(self.FC_hidden2(h))\n        \n        x_hat = torch.sigmoid(self.FC_output(h))  # <1>\n        return x_hat\n```\n:::\n\n\n:::{#8ca9c944 .cell .markdown}\n1. 最後の出力は，エンコーダーとは違い，シグモイド関数を通して確率分布`x_hat`とする．\n\n#### モデル\n\nVAE はエンコーダーとデコーダーを連結し，１つのニューラルネットワークとして学習する．\n:::\n\n::: {#3e86aab3 .cell execution_count=7}\n``` {}\nclass Model(nn.Module):\n    def __init__(self, Encoder, Decoder):\n        super(Model, self).__init__()\n        self.Encoder = Encoder\n        self.Decoder = Decoder\n        \n    def reparameterization(self, mean, var):\n        epsilon = torch.randn_like(var).to(DEVICE)  # <1>  \n        z = mean + var*epsilon   # <2>\n        return z\n        \n                \n    def forward(self, x):\n        mean, log_var = self.Encoder(x)  # <3>\n        z = self.reparameterization(mean, torch.exp(0.5 * log_var))  # <4>\n        x_hat            = self.Decoder(z)  # <5>\n        \n        return x_hat, mean, log_var  # <6>\n```\n:::\n\n\n:::{#4fe17013 .cell .markdown}\n1. これは **サンプリングイプシロン** と呼ばれる値である．\n2. ここで reparametrization trick を行っている．\n3. 入力 `x` があったならば，まずエンコーダーに通して `mean`, `log_var` を得る．\n4. 元々 `log_var` の名前の通り対数分散として扱うこととしていたので，２で割り指数関数に通すことで標準偏差を得る．この平均と標準偏差について reparametrization trick を実行し，デコーダーに繋ぐ．\n5. デコーダーではデータの潜在表現 `z` を受け取り，デコードしたものを `x_hat` とする．\n6. 返り値は，デコーダーの出力 `x_hat` だけでなく，潜在表現 `mean`, `log_var` も含むことに注意．\n:::\n\n::: {#0fb5d5ba .cell execution_count=8}\n``` {}\nencoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\ndecoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n\nmodel = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)  # <1>\n```\n:::\n\n\n:::{#cc5d9bf1 .cell .markdown}\n1. `.to(DEVICE)` により，モデルを M2 Mac の MPS デバイス上に移送している．\n\n### モデルの訓練 {#sec-VAE-training}\n\n最適化には Adam [@Kingma-Ba2017] を用い，バイナリ交差エントロピー（BCE）を用いる．これは [`nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss) に実装がある．\n:::\n\n::: {#e54314a0 .cell execution_count=9}\n``` {}\nfrom torch.optim import Adam\n\nBCE_loss = nn.BCELoss()\n\ndef loss_function(x, x_hat, mean, log_var):\n    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n    KLD      = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n\n    return reproduction_loss + KLD\n\n\noptimizer = Adam(model.parameters(), lr=lr)\n```\n:::\n\n\n:::{#7affc30f .cell .markdown}\nここでの損失関数は，真のデータ `x` をデコーダーが復元できているかを交差エントロピーで測った `reproduction_loss` と，潜在表現がどれだけ $\\rN_d(0,I_d),d=200$ に近いかを KL 乖離度で測った `KLD` の和で定義されている．^[なお，`mean.pow(2)` は Julia の `mean.^2` に同じ．]\n\n::: {.callout-important title=\"訓練の実行\" collapse=\"true\" icon=\"false\"}\n:::\n\n::: {#6f46ed90 .cell execution_count=10}\n``` {}\nimport time\n\nprint(\"Start training VAE...\")\nmodel.train()  # <1>\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    overall_loss = 0\n    for batch_idx, (x, _) in enumerate(train_loader):\n        x = x.view(batch_size, x_dim)  # <2>\n        x = x.to(DEVICE)  # <3>\n\n        optimizer.zero_grad()  # <4>\n\n        x_hat, mean, log_var = model(x)\n        loss = loss_function(x, x_hat, mean, log_var)\n        \n        overall_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n\ntotal_time = time.time() - start_time\nprint(\"Finish!! Total time: \", total_time)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart training VAE...\n\tEpoch 1 complete! \tAverage Loss:  173.99486194438649\n\tEpoch 2 complete! \tAverage Loss:  128.83013526776398\n\tEpoch 3 complete! \tAverage Loss:  117.38623147954925\n\tEpoch 4 complete! \tAverage Loss:  112.83824174731323\n\tEpoch 5 complete! \tAverage Loss:  110.08542848106218\n\tEpoch 6 complete! \tAverage Loss:  108.42780969584724\n\tEpoch 7 complete! \tAverage Loss:  107.20257375573873\n\tEpoch 8 complete! \tAverage Loss:  106.37051309474124\n\tEpoch 9 complete! \tAverage Loss:  105.65905170727254\n\tEpoch 10 complete! \tAverage Loss:  105.09276363926857\n\tEpoch 11 complete! \tAverage Loss:  104.53313302118113\n\tEpoch 12 complete! \tAverage Loss:  104.02758779280572\n\tEpoch 13 complete! \tAverage Loss:  103.57385199290484\n\tEpoch 14 complete! \tAverage Loss:  103.16217977227672\n\tEpoch 15 complete! \tAverage Loss:  102.86741876108618\n\tEpoch 16 complete! \tAverage Loss:  102.57010049300918\n\tEpoch 17 complete! \tAverage Loss:  102.2765659562813\n\tEpoch 18 complete! \tAverage Loss:  102.12429527728506\n\tEpoch 19 complete! \tAverage Loss:  101.87565946499375\n\tEpoch 20 complete! \tAverage Loss:  101.63949083433326\n\tEpoch 21 complete! \tAverage Loss:  101.49195385864462\n\tEpoch 22 complete! \tAverage Loss:  101.33026659015025\n\tEpoch 23 complete! \tAverage Loss:  101.21425551374686\n\tEpoch 24 complete! \tAverage Loss:  101.00954409693239\n\tEpoch 25 complete! \tAverage Loss:  100.92588200386061\n\tEpoch 26 complete! \tAverage Loss:  100.7645504225793\n\tEpoch 27 complete! \tAverage Loss:  100.63024573833994\n\tEpoch 28 complete! \tAverage Loss:  100.53897935361019\n\tEpoch 29 complete! \tAverage Loss:  100.3836006462594\n\tEpoch 30 complete! \tAverage Loss:  100.29222264972871\nFinish!! Total time:  130.202960729599\n```\n:::\n:::\n\n\n:::{#bd068bd7 .cell .markdown}\n1. `PyTorch` のモデルオブジェクトを訓練モードにするメソッド．Dropout や Batch Normalization 層がある場合は，これにより訓練時の挙動を示すようになる．\n2. 事前に定めた `batch_size` に従ってバッチを展開．\n3. データを GPU に移動．\n4. 勾配をゼロに初期化するとのこと．\n\n:::\n\n### モデルの評価\n\nテスト用データの最初のバッチについて処理し，入力データと出力データを見比べてみる．\n:::\n\n::: {#cbc77d47 .cell execution_count=11}\n``` {}\nmodel.eval()\n\nwith torch.no_grad():  # <1>\n    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n        x = x.view(batch_size, x_dim)\n        x = x.to(DEVICE)\n        \n        x_hat, _, _ = model(x)\n\n\n        break\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]\r  0%|          | 0/100 [00:00<?, ?it/s]\n```\n:::\n:::\n\n\n:::{#4d1ca3af .cell .markdown}\n1. 勾配評価を無効化するコンテクストマネージャーで，メモリの使用を節約できるという．\n:::\n\n::: {#fig-reconstruction .cell layout-ncol='2' execution_count=12}\n``` {}\n#| layout-ncol: 2\n#| label: fig-reconstruction\n#| fig-cap: 左がテストデータ，右がその VAE による復元\nimport matplotlib.pyplot as plt\n\ndef show_image(x, idx):\n    x = x.view(batch_size, 28, 28)\n\n    fig = plt.figure()\n    plt.imshow(x[idx].cpu().numpy())\n\nshow_image(x, idx=0)\nshow_image(x_hat, idx=0)\n```\n\n::: {.cell-output .cell-output-display}\n![左がテストデータ，右がその VAE による復元](VAE_files/figure-ipynb/fig-reconstruction-output-1.png){#fig-reconstruction-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](VAE_files/figure-ipynb/fig-reconstruction-output-2.png){#fig-reconstruction-2}\n:::\n:::\n\n\n:::{#0b61c005 .cell .markdown}\n左が入力で右が出力である．\n\n### データの生成\n\nここで，エンコーダを取り外してデコーダーからデータを生成する．\n\n損失関数（第 [-@sec-VAE-training] 節）には，潜在空間におけるデータを標準正規分布に近付けるための項が入っていたため，データの潜在表現は極めて標準正規分布に近いとみなすことにする．\n\nすると，潜在表現と同じ次元の正規乱数から，データセットに極めて似通ったデータが生成できるだろう．\n:::\n\n::: {#fig-generation .cell layout-ncol='2' layout-nrow='2' execution_count=13}\n``` {}\n#| layout-ncol: 2\n#| layout-nrow: 2\n#| label: fig-generation\nwith torch.no_grad():\n    noise = torch.randn(batch_size, latent_dim).to(DEVICE)\n    generated_images = decoder(noise)\n\nsave_image(generated_images.view(batch_size, 1, 28, 28), 'generated_sample.png')\nfor i in range(4):\n    show_image(generated_images, idx=i)\n```\n\n::: {.cell-output .cell-output-display}\n![](VAE_files/figure-ipynb/fig-generation-output-1.png){#fig-generation-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![](VAE_files/figure-ipynb/fig-generation-output-2.png){#fig-generation-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![](VAE_files/figure-ipynb/fig-generation-output-3.png){#fig-generation-3}\n:::\n\n::: {.cell-output .cell-output-display}\n![](VAE_files/figure-ipynb/fig-generation-output-4.png){#fig-generation-4}\n:::\n:::\n\n\n:::{#f1fea8d4 .cell .markdown}\n## VQ-VAE [@vandenOord+2017]\n\n### 導入\n:::\n\n::: {#c14ba5d5 .cell execution_count=14}\n``` {}\nDEVICE = torch.device(\"mps\")\n\nbatch_size = 128\nimg_size = (32, 32)\n\ninput_dim = 3\nhidden_dim = 512\nlatent_dim = 16\nn_embeddings= 512\noutput_dim = 3\ncommitment_beta = 0.25\n\nlr = 2e-4\n\nepochs = 50\n\nprint_step = 50\n```\n:::\n\n\n::: {#c2d9bb83 .cell execution_count=15}\n``` {}\nfrom torchvision.datasets import CIFAR10\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nmnist_transform = transforms.Compose([\n        transforms.ToTensor(),\n])\n\nkwargs = {'num_workers': 1, 'pin_memory': True} \n\ntrain_dataset = CIFAR10(dataset_path, transform=mnist_transform, train=True, download=True)\ntest_dataset  = CIFAR10(dataset_path, transform=mnist_transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFiles already downloaded and verified\nFiles already downloaded and verified\n```\n:::\n:::\n\n\n:::{#81aadedf .cell .markdown}\n### モデルの定義\n\n#### エンコーダー\n\nVQ-VAE は画像への応用を念頭に置いているため，エンコーダーには [CNN アーキテクチャ](../Kernels/Deep.qmd#sec-CNN) を採用する．\n:::\n\n::: {#efedd408 .cell execution_count=16}\n``` {}\nclass Encoder(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=(4, 4, 3, 1), stride=2):\n        super(Encoder, self).__init__()\n        \n        kernel_1, kernel_2, kernel_3, kernel_4 = kernel_size\n        \n        self.strided_conv_1 = nn.Conv2d(input_dim, hidden_dim, kernel_1, stride, padding=1)\n        self.strided_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, stride, padding=1)\n        \n        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_3, padding=1)\n        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_4, padding=0)\n        \n        self.proj = nn.Conv2d(hidden_dim, output_dim, kernel_size=1)\n        \n    def forward(self, x):\n        \n        x = self.strided_conv_1(x)\n        x = self.strided_conv_2(x)\n        \n        x = F.relu(x)\n        y = self.residual_conv_1(x)\n        y = y+x\n        \n        x = F.relu(y)\n        y = self.residual_conv_2(x)\n        y = y+x\n        \n        y = self.proj(y)\n        return y\n```\n:::\n\n\n:::{#b07b212f .cell .markdown}\n## 参考文献 {.appendix}\n\n本稿は，[Minsu Jackson Kang 氏](https://velog.io/@mskang/about) による [チュートリアル](https://github.com/Jackson-Kang/Pytorch-VAE-tutorial) を参考にした．\n\nVAE の潜在表現は [t-SNE](https://ja.wikipedia.org/wiki/T分布型確率的近傍埋め込み法) などを用いて可視化でき，[@Murphy2023 p.635] の例などでも，潜在空間において手書き数字がクラスごとによく分離されていることが確認できる．\n:::\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.4\n---\n",
    "supporting": [
      "VAE_files/figure-ipynb"
    ],
    "filters": []
  }
}