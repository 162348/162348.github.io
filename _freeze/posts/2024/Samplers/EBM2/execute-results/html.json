{
  "hash": "a5d6cfffff8ee4bb4357617d80c3e5cd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"エネルギーベースモデルのノイズ対照学習\"\nsubtitle: \"`PyTorch` によるハンズオン\"\nauthor: \"司馬博文\"\ndate: 8/3/2024\ndate-modified: 8/21/2024\ncategories: [Deep, Sampling, Python]\nimage: Files/NCL/thumb_8gaussians.gif\nbibliography: \n    - ../../../assets/mathematics.bib\n    - ../../../assets/bib.bib\n    - ../../../assets/bib1.bib\ncsl: ../../../assets/apalike.csl\nabstract-title: 概要\nabstract: 確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．今回は `PyTorch` を用いて，エネルギーベースモデルのノイズ対照学習の実装を見る．\ncode-fold: false\nlisting: \n    -   id: lst-listing\n        type: grid\n        sort: false\n        contents:\n            - \"EBM.qmd\"\n            - \"EBM1.qmd\"\n            - \"../Kernels/Kernel.qmd\"\n        date-format: iso\n        fields: [title,image,date,subtitle]\n---\n\n### 関連ページ {.unnumbered .unlisted}\n\n::: {#lst-listing}\n:::\n\n## モデル定義\n\n::: {#76e8355d .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\n# ------------------------------\n# ENERGY-BASED MODEL\n# ------------------------------\nclass EBM(nn.Module):\n    def __init__(self, dim=2):\n        super(EBM, self).__init__()\n        # The normalizing constant logZ(θ)        \n        self.c = nn.Parameter(torch.tensor([1.], requires_grad=True))\n\n        self.f = nn.Sequential(\n            nn.Linear(dim, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(128, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(128, 1),\n            )\n\n    def forward(self, x):\n        log_p = - self.f(x) - self.c\n        return log_p\n```\n:::\n\n\n## 事前準備\n\n::: {#6808abe1 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\ndef value(energy, noise, x, gen):\n    logp_x = energy(x)  # logp(x)\n    logq_x = noise.log_prob(x).unsqueeze(1)  # logq(x)\n    logp_gen = energy(gen)  # logp(x̃)\n    logq_gen = noise.log_prob(gen).unsqueeze(1)  # logq(x̃)\n\n    value_data = logp_x - torch.logsumexp(torch.cat([logp_x, logq_x], dim=1), dim=1, keepdim=True)  # log[p(x)/(p(x) + q(x))]\n    value_gen = logq_gen - torch.logsumexp(torch.cat([logp_gen, logq_gen], dim=1), dim=1, keepdim=True)  # log[q(x̃)/(p(x̃) + q(x̃))]\n\n    v = value_data.mean() + value_gen.mean()\n\n    r_x = torch.sigmoid(logp_x - logq_x)\n    r_gen = torch.sigmoid(logq_gen - logp_gen)\n\n    acc = ((r_x > 1/2).sum() + (r_gen > 1/2).sum()).cpu().numpy() / (len(x) + len(gen))\n\n    return -v,  acc\n\n\n#-------------------------------------------\n# DATA\n#-------------------------------------------\ndef get_data(args):\n    dataset = sample_2d_data(dataset=args.dataset, n_samples=args.samples)\n    dataloader  = DataLoader(dataset, batch_size=args.batch, shuffle=True)\n    return dataset, dataloader\n\ndef sample_2d_data(dataset='8gaussians', n_samples=50000):\n    \n    z = torch.randn(n_samples, 2)\n\n    if dataset == '8gaussians':\n        scale = 4\n        sq2 = 1/math.sqrt(2)\n        centers = [(1,0), (-1,0), (0,1), (0,-1), (sq2,sq2), (-sq2,sq2), (sq2,-sq2), (-sq2,-sq2)]\n        centers = torch.tensor([(scale * x, scale * y) for x,y in centers])\n        return sq2 * (0.5 * z + centers[torch.randint(len(centers), size=(n_samples,))])\n\n    elif dataset == '2spirals':\n        n = torch.sqrt(torch.rand(n_samples // 2)) * 540 * (2 * math.pi) / 360\n        d1x = - torch.cos(n) * n + torch.rand(n_samples // 2) * 0.5\n        d1y =   torch.sin(n) * n + torch.rand(n_samples // 2) * 0.5\n        x = torch.cat([torch.stack([ d1x,  d1y], dim=1),\n                       torch.stack([-d1x, -d1y], dim=1)], dim=0) / 3\n        return x + 0.1*z\n\n    elif dataset == 'checkerboard':\n        x1 = torch.rand(n_samples) * 4 - 2\n        x2_ = torch.rand(n_samples) - torch.randint(0, 2, (n_samples,), dtype=torch.float) * 2\n        x2 = x2_ + x1.floor() % 2\n        return torch.stack([x1, x2], dim=1) * 2\n\n    elif dataset == 'rings':\n        n_samples4 = n_samples3 = n_samples2 = n_samples // 4\n        n_samples1 = n_samples - n_samples4 - n_samples3 - n_samples2\n\n        # so as not to have the first point = last point, set endpoint=False in np; here shifted by one\n        linspace4 = torch.linspace(0, 2 * math.pi, n_samples4 + 1)[:-1]\n        linspace3 = torch.linspace(0, 2 * math.pi, n_samples3 + 1)[:-1]\n        linspace2 = torch.linspace(0, 2 * math.pi, n_samples2 + 1)[:-1]\n        linspace1 = torch.linspace(0, 2 * math.pi, n_samples1 + 1)[:-1]\n\n        circ4_x = torch.cos(linspace4)\n        circ4_y = torch.sin(linspace4)\n        circ3_x = torch.cos(linspace4) * 0.75\n        circ3_y = torch.sin(linspace3) * 0.75\n        circ2_x = torch.cos(linspace2) * 0.5\n        circ2_y = torch.sin(linspace2) * 0.5\n        circ1_x = torch.cos(linspace1) * 0.25\n        circ1_y = torch.sin(linspace1) * 0.25\n\n        x = torch.stack([torch.cat([circ4_x, circ3_x, circ2_x, circ1_x]),\n                         torch.cat([circ4_y, circ3_y, circ2_y, circ1_y])], dim=1) * 3.0\n\n        # random sample\n        x = x[torch.randint(0, n_samples, size=(n_samples,))]\n\n        # Add noise\n        return x + torch.normal(mean=torch.zeros_like(x), std=0.08*torch.ones_like(x))\n\n    elif dataset == \"pinwheel\":\n        rng = np.random.RandomState()\n        radial_std = 0.3\n        tangential_std = 0.1\n        num_classes = 5\n        num_per_class = n_samples // 5\n        rate = 0.25\n        rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)\n\n        features = rng.randn(num_classes*num_per_class, 2) \\\n            * np.array([radial_std, tangential_std])\n        features[:, 0] += 1.\n        labels = np.repeat(np.arange(num_classes), num_per_class)\n\n        angles = rads[labels] + rate * np.exp(features[:, 0])\n        rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n        rotations = np.reshape(rotations.T, (-1, 2, 2))\n        \n        data = 2 * rng.permutation(np.einsum(\"ti,tij->tj\", features, rotations))\n        return torch.as_tensor(data, dtype=torch.float32)\n\n    else:\n        raise RuntimeError('Invalid `dataset` to sample from.')\n```\n:::\n\n\n::: {#1a2c9ac8 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# --------------------\n# Plotting\n# --------------------\n\n@torch.no_grad()\ndef plot(dataset, energy, noise, epoch, device):\n    n_pts = 1000\n    range_lim = 4\n\n    # construct test points\n    test_grid = setup_grid(range_lim, n_pts, device)\n\n    # plot\n    # fig, axs = plt.subplots(1, 3, figsize=(12,4.3), subplot_kw={'aspect': 'equal'})\n    # plot_samples(dataset, axs[0], range_lim, n_pts)\n    # plot_noise(noise, axs[1], test_grid, n_pts)\n    fig, ax = plt.subplots(1, 1, figsize=(4,4), subplot_kw={'aspect': 'equal'})\n    plot_energy(energy, ax, test_grid, n_pts)\n\n    # format\n    for ax in plt.gcf().axes: format_ax(ax, range_lim)\n    plt.tight_layout()\n\n    # save\n    print('Saving image to images/....')\n    plt.savefig('images/epoch_{}.png'.format(epoch))\n    plt.close()\n\ndef setup_grid(range_lim, n_pts, device):\n    x = torch.linspace(-range_lim, range_lim, n_pts)\n    xx, yy = torch.meshgrid((x, x), indexing='ij')\n    zz = torch.stack((xx.flatten(), yy.flatten()), dim=1)\n    return xx, yy, zz.to(device)\n\ndef plot_samples(dataset, ax, range_lim, n_pts):\n    samples = dataset.numpy()\n    ax.hist2d(samples[:,0], samples[:,1], range=[[-range_lim, range_lim], [-range_lim, range_lim]], bins=n_pts, cmap=plt.cm.jet)\n    ax.set_title('Target samples')\n\ndef plot_energy(energy, ax, test_grid, n_pts):\n    xx, yy, zz = test_grid\n    log_prob = energy(zz)\n    prob = log_prob.exp().cpu()\n    # plot\n    ax.pcolormesh(xx.numpy(), yy.numpy(), prob.view(n_pts,n_pts).numpy(), cmap=plt.cm.jet)\n    ax.set_facecolor(plt.cm.jet(0.))\n    ax.set_title('Energy density')\n\ndef plot_noise(noise, ax, test_grid, n_pts):\n    xx, yy, zz = test_grid\n    log_prob = noise.log_prob(zz)\n    prob = log_prob.exp().cpu()\n    # plot\n    ax.pcolormesh(xx.numpy(), yy.numpy(), prob.view(n_pts,n_pts).numpy(), cmap=plt.cm.jet)\n    ax.set_facecolor(plt.cm.jet(0.))\n    ax.set_title('Noise density')\n\ndef format_ax(ax, range_lim):\n    ax.set_xlim(-range_lim, range_lim)\n    ax.set_ylim(-range_lim, range_lim)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax.invert_yaxis()\n```\n:::\n\n\n## 訓練\n\n::: {#a0d10dd7 .cell execution_count=5}\n``` {.python .cell-code}\nimport argparse\nimport os\nimport torch\nimport torch.distributions as D\n\nd = 'cpu'\n# if torch.cuda.is_available():\n#     d = 'cuda'\n# elif torch.backends.mps.is_available():\n#     d = 'mps'\ndevice = torch.device(d)\n\nclass Args:\n    def __init__(self):\n        self.epoch = 50\n        self.batch = 100\n        self.dataset = '8gaussians'\n        self.samples = 10000\n        self.lr = 1e-3\n        self.b1 = 0.9\n        self.b2 = 0.999\n        self.resume = False\n\nargs = Args()\n\n# ------------------------------\n# I. MODELS\n# ------------------------------\nenergy = EBM(dim=2).to(device)\nnoise = D.MultivariateNormal(torch.zeros(2).to(device), 4.*torch.eye(2).to(device))\n# ------------------------------\n# II. OPTIMIZERS\n# ------------------------------\noptim_energy = torch.optim.Adam(energy.parameters(), lr=args.lr, betas=(args.b1, args.b2))\n# ------------------------------\n# III. DATA LOADER\n# ------------------------------\ndataset, dataloader = get_data(args)\n# ------------------------------\n# IV. TRAINING\n# ------------------------------\ndef main(args):\n    start_epoch = 0\n# ----------------------------------------------------------------- #\n    if args.resume:\n        print('Resuming from checkpoint at ckpts/nce.pth.tar...')\n        checkpoint = torch.load('ckpts/nce.pth.tar')\n        energy.load_state_dict(checkpoint['energy'])\n        start_epoch = checkpoint['epoch'] + 1\n# ----------------------------------------------------------------- #\n    for epoch in range(start_epoch, start_epoch + args.epoch):\n        for i, x in enumerate(dataloader):           \n            x = x.to(device)\n            # -----------------------------\n            #  Generate samples from noise\n            # -----------------------------\n            gen = noise.sample((args.batch,))\n            # -----------------------------\n            #  Train Energy-Based Model\n            # -----------------------------\n            optim_energy.zero_grad()\n\n            loss_energy, acc = value(energy, noise, x, gen)\n\n            loss_energy.backward()\n            optim_energy.step()  \n\n            print(\n                \"[Epoch %d/%d] [Batch %d/%d] [Value: %f] [Accuracy:%f]\"\n                % (epoch, start_epoch + args.epoch, i, len(dataloader), loss_energy.item(), acc)\n            )\n\n        # Save checkpoint\n        print('Saving models...')\n        state = {\n        'energy': energy.state_dict(),\n        'value': loss_energy,\n        'epoch': epoch,\n        }\n        os.makedirs('ckpts', exist_ok=True)\n        torch.save(state, 'ckpts/nce.pth.tar')\n\n        # visualization\n        plot(dataset, energy, noise, epoch, device)\n\n\nif __name__ == '__main__':\n    print(args)\n    main(args)\n```\n:::\n\n\n大変軽量で，cpu でも５分ほどで学習できる（そのうちほとんどは画像の保存にかかる時間である）．しかし，mps では次のエラーを得る．\n\n```zsh\nNotImplementedError: The operator 'aten::linalg_cholesky_ex.L' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n```\n\n![](Files/NCL/output_8gaussian.gif)\n\n他のデータに関しても，次のように学習できる：\n\n![`pinwheel` はそれぞれの羽の尾の部分が消えてしまっているように見える．](Files/NCL/output_pinwheel.gif)\n\n![`rings` に関しては結構学習に苦労しているようだ．](Files/NCL/output_rings.gif)\n\n![`checkerboard` も四角い形までは 50 epoch では再現が難しいのかもしれない．](Files/NCL/output_checkerboard.gif)\n\n![`2spirals` は結構トポロジーを間違えている！](Files/NCL/output_2spirals.gif)\n\n![`rings` ロングバージョン．`epoch=150` としたが，内側２輪しか再現できていない．](Files/NCL/long_rings.gif)\n\n## 文献 {.appendix}\n\n[李飞氏](https://lifei.ai/) による [実装](https://github.com/lifeitech/nce/blob/master/model.py) を参考にした．\n\n",
    "supporting": [
      "EBM2_files"
    ],
    "filters": [],
    "includes": {}
  }
}