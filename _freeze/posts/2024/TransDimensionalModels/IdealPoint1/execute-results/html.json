{
  "hash": "a896ac2948980c1458342f7f659f570a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"理想点解析のハンズオン\"\nsubtitle: \"`MCMCpack` パッケージとオリジナル `Stan` コードを使って\"\nauthor: \"司馬 博文\"\ndate: 10/2/2024\ndate-modified: 12/22/2024\ncategories: [Bayesian, Statistics, MCMC, R]\nbibliography: \n    - ../../../assets/mathematics.bib\n    - ../../../assets/bib.bib\n    - ../../../assets/bib1.bib\n    - IdealPoint.bib\ncsl: ../../../assets/apalike.csl\nabstract-title: 概要\nabstract: |\n    政治学における理想点解析とは，項目反応モデルを用いて裁判官や国会議員などの価値判断基準や「イデオロギー」を定量化・視覚化する方法である．\n    ここでは既存のパッケージを用いて簡単に理想点解析を行う方法から始め，\n    自分で Stan コードを書いてモデルを推定する方法を紹介する．\n    その際に最も重要な理想点モデルの性質として，**識別可能性** の議論がある．\n    これが保たれていないと，モデルの事後分布は多峰性を持ってしまい，推定をするたびに結果が異なったり，統計量の長期間平均が $0$ になってしまったりしてしまう．\nimage: Images/IPE_experiment.png\ncode-fold: false\nexecute:\n    cache: true\nlisting: \n    -   id: lst-embedding\n        type: grid\n        sort: false\n        contents:\n            - \"IdealPoint.qmd\"\n            - \"IdealPoint2.qmd\"\n            - \"IdealPoint3.qmd\"\n        date-format: iso\n        fields: [title,image,date,subtitle,categories]\n    # -   id: lst-embedding1\n    #     type: grid\n    #     grid-columns: 1\n    #     contents:\n    #         - \"../Survey/BayesGLMM.qmd\"\n    #     date-format: iso\n    #     fields: [title,image,date,subtitle,categories]\n---\n\n\n::: {.hidden}\n\n::: {.content-visible when-format=\"html\"}\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n\n$$\n\n\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\BRac}[1]{\\biggl\\langle#1\\biggr\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\\newcommand{\\rN}{\\mathrm{N}}\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n\n\n\n\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n\\newcommand{\\argmax}{\\operatorname*{argmax}}\\newcommand{\\argmin}{\\operatorname*{argmin}}\n\n\\newcommand{\\pr}{\\mathrm{pr}}\\newcommand{\\Conv}{\\operatorname{Conv}}\\newcommand{\\cU}{\\mathcal{U}}\n\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n\\newcommand{\\grad}{\\operatorname{grad}}\\newcommand{\\rot}{\\operatorname{rot}}\\renewcommand{\\div}{\\operatorname{div}}\\newcommand{\\tr}{\\operatorname{tr}}\\newcommand{\\Tr}{\\operatorname{Tr}}\\newcommand{\\KL}{\\operatorname{KL}}\\newcommand{\\JS}{\\operatorname{JS}}\\newcommand{\\ESS}{\\operatorname{ESS}}\\newcommand{\\MSE}{\\operatorname{MSE}}\\newcommand{\\erf}{\\operatorname{erf}}\\newcommand{\\arctanh}{\\operatorname{arctanh}}\\newcommand{\\pl}{\\operatorname{pl}}\\newcommand{\\minimize}{\\operatorname{minimize}}\\newcommand{\\subjectto}{\\operatorname{subject to}}\\newcommand{\\sinc}{\\operatorname{sinc}}\\newcommand{\\Ent}{\\operatorname{Ent}}\\newcommand{\\Polya}{\\operatorname{Polya}}\\newcommand{\\Exp}{\\operatorname{Exp}}\\newcommand{\\codim}{\\operatorname{codim}}\\newcommand{\\sgn}{\\operatorname{sgn}}\\newcommand{\\rank}{\\operatorname{rank}}\n\n\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\n\n\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n\n\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n\n\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n\n\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n\n\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n\n\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\renewcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\renewcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n\n\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n\n\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n\n\n\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\bL}{\\mathbb{L}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n\n\\newcommand{\\bit}{\\mathrm{bit}}\n\n\\newcommand{\\err}{\\mathrm{err}}\n\n\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n\n\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n\n\n\\newcommand{\\del}{\\partial}\n\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n\n\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n\n\\newcommand{\\Ens}{\\mathrm{Ens}}\n\n\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n\n\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n\n\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n\n\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\renewcommand{\\Beta}{\\operatorname{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n\n\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n\n\n\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\n\n\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}}\n\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n\\newcommand{\\Op}{\\mathrm{Op}}\n\\newcommand{\\Sh}{\\mathrm{Sh}}\n\\newcommand{\\Diff}{\\mathrm{Diff}}\n\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n\n\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{i.i.d.}}{\\sim}}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n\n\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n\n\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n\\newcommand{\\lmd}{\\lambda}\n\\newcommand{\\Lmd}{\\Lambda}\n\\newcommand{\\cI}{\\mathcal{I}}\n\n\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n\\DeclareMathOperator{\\des}{des}\n\\DeclareMathOperator{\\nd}{nd}\n\\DeclareMathOperator{\\dsep}{d-sep}\n\\DeclareMathOperator{\\sep}{sep}\n\\newcommand{\\rLL}{\\mathrm{LL}}\\newcommand{\\HT}{\\mathrm{HT}}\\newcommand{\\PS}{\\mathrm{PS}}\\newcommand{\\rI}{\\mathrm{I}}\n$$\n\n:::\n\n:::\n\n\n\n## 関連記事 {.unnumbered .unlisted}\n\n::: {#lst-embedding}\n:::\n\n本稿では実際に理想点モデルの推定を，[Martin-Quinn](http://mqscores.wustl.edu/replication.php) により公開されている連邦最高裁判所の 1937 年から 2022 年までのデータを（`MCMCpack` パッケージを通じて）用いて行う．\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MCMCpack)\ndata(Rehnquist)  # MCMCpackに含まれる U.S. Supreme Court（連邦最高裁）のデータ\nkable(head(Rehnquist))\n```\n\n::: {.cell-output-display}\n\n\n| Rehnquist| Stevens| O.Connor| Scalia| Kennedy| Souter| Thomas| Ginsburg| Breyer| term| time|\n|---------:|-------:|--------:|------:|-------:|------:|------:|--------:|------:|----:|----:|\n|         0|       1|        0|      0|       1|      1|      0|        1|      1| 1994|    1|\n|         1|       1|        1|      0|       1|      1|      0|        1|      1| 1994|    1|\n|         0|       1|        0|      0|       0|      0|      0|       NA|      0| 1994|    1|\n|         0|       1|        0|      1|       1|      1|      0|        1|      0| 1994|    1|\n|         0|       1|        0|      0|       0|      0|      0|        0|      0| 1994|    1|\n|         0|       1|        0|      0|       0|      1|      0|        0|      0| 1994|    1|\n\n\n:::\n:::\n\n\n\nこのデータは保守的な判断をする場合が $y_i=1$，リベラルな判断をする場合が $y_i=0$ の２値データとなっている．\n\n\n## 理想点モデルとは何か？\n\n### `MCMCpack` パッケージ\n\nはじめに，理想点モデルではどのようなことができるかをみるために，`MCMCpack` パッケージを通じて理想点推定を簡単に実行する方法を見る．\n\n理想点モデルでは識別性が一つの論点になる（第 [-@sec-identification] 節）が，ここでは簡単に，Stevens 判事と Thomas 判事の位置を固定する方法を用いてみよう．\n\n`MCMCpack` パッケージでは，時系列理想点モデルの推定に [`MCMCdynamicIRT1d()`](https://github.com/cran/MCMCpack/blob/master/man/MCMCdynamicIRT1d.Rd) 関数が用意されている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 初期値の設定\ntheta.start <- rep(0, 9)  # 9人の裁判官の初期値\ntheta.start[2] <- -3      # Stevens裁判官の初期値\ntheta.start[7] <- 2       # Thomas裁判官の初期値\n\n# MCMCの実行\nout <- MCMCdynamicIRT1d(\n    t(Rehnquist[,1:9]),           # データ行列（転置して裁判官×案件の形に）\n    item.time.map=Rehnquist$time, # 各案件の時期情報\n    theta.start=theta.start,      # 初期値\n    mcmc=2000,                   # MCMCの反復回数\n    burnin=2000,                 # バーンイン期間\n    thin=5,                       # 間引き数\n    verbose=500,                  # 進捗表示間隔\n    tau2.start=rep(0.1, 9),      # τ²の初期値\n    e0=0, E0=1,                  # θの事前分布パラメータ\n    a0=0, A0=1,                  # αの事前分布パラメータ\n    b0=0, B0=1,                  # βの事前分布パラメータ\n    c0=-1, d0=-1,               # τ²の事前分布パラメータ\n    store.item=FALSE,            # アイテムパラメータを保存しない\n    theta.constraints=list(Stevens=\"-\", Thomas=\"+\")  # 識別制約\n)\n\ntheta_cols <- grep(\"theta\", colnames(out), value=TRUE)\ntheta_mcmc <- out[, theta_cols]\n\n# library(coda)\n# summary(theta_mcmc)  # codaのsummary関数で要約\nplot(theta_mcmc)\n```\n:::\n\n\n\n![](Images/plot.png){width=70% fig-align=center}\n\n### 理想点の推定\n\n出力は各最高裁判事の理想点の事後分布である．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta_means <- colMeans(theta_mcmc)\npattern <- \"t11\"\ncol_names <- colnames(theta_mcmc)\nselected_cols <- grep(pattern, col_names)  # 正規表現にマッチする列のインデックスを取得\nselected_theta_mcmc <- theta_mcmc[, selected_cols]\n\nquantiles_2_5 <- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.025))\nquantiles_97_5 <- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.975))\n\nggplot(data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = unname(theta_means[selected_cols]),\n  lower = unname(quantiles_2_5),\n  upper = unname(quantiles_97_5)\n), aes(x = mean, y = legislator)) + \n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Estimated Ideal Points (11th term)\", x = \"Ideal Point\", y = \"Legislator\")\n```\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n### 時系列化\n\n[@Martin-Quinn2002] ではこの理想点の時系列的な変化を調べた．\n\n```r\ntime_points <- unique(Rehnquist$time)\nn_subjects <- 9  # 裁判官の数\n\n# 各裁判官の軌跡をプロット\nplot(time_points, theta_means[1:length(time_points)], \n     type=\"l\", ylim=range(theta_means),\n     xlab=\"Time\", ylab=\"Ideal Point\",\n     main=\"Estimated Ideal Points Over Time\")\n\n# 各裁判官を異なる色で追加\ncolors <- rainbow(n_subjects)\ncolors[9] <- \"blue\"\nfor(i in c(1,8,9)) {\n    lines(time_points, \n          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],\n          col=colors[i],\n          lwd=3)\n}\n\n# 凡例を追加\nlegend(\"topright\", \n       legend=unique(colnames(Rehnquist)[c(1,8,9)]),  # 裁判官の名前\n       col=colors[c(1,8,9)], \n       lty=1)\n```\n\n![](Images/plot3.png){width=70% fig-align=center}\n\nたしかに [William Rehnquist](https://en.wikipedia.org/wiki/William_Rehnquist) は共和党，[Ruth Bader Ginsburg](https://en.wikipedia.org/wiki/Ruth_Bader_Ginsburg) と [Stephen Breyer](https://en.wikipedia.org/wiki/Stephen_Breyer) は民主党である．\n\n\n\n::: {.cell}\n\n:::\n\n\n\n![](Images/plot2.png){width=70% fig-align=center}\n\n$0$ の上に位置している [Anthony Kennedy](https://ja.wikipedia.org/wiki/アンソニー・ケネディ) や [Sandra Day O'Connor](https://en.wikipedia.org/wiki/Sandra_Day_O%27Connor) はほとんど中道的だが，やや保守党寄りである． [Antonin Scalia](https://en.wikipedia.org/wiki/Antonin_Scalia) は特に保守的な立場であることが知られている．\n\n$0$ よりも下に位置するもう一人は [David Souter](https://en.wikipedia.org/wiki/David_Souter) であるが，彼はもともと保守系と目されていたが，後年リベラルな傾向を示したとされる．^[\"Souter was nominated to the Supreme Court without a significant \"paper trail\" but was expected to be a conservative justice. Within a few years of his appointment, Souter moved towards the ideological center. He eventually came to vote reliably with the Court's liberal wing.\" [Wikipedia](https://en.wikipedia.org/wiki/David_Souter) より引用．]\n\n## ２母数ロジットモデルの推定\n\n### はじめに\n\n`MCMCpack` パッケージで理想点推定の出力がつかめたいま，より詳しくモデルを見ていく．\n\n本節では `rstan` パッケージを用いて，項目反応モデルとして具体的な手順を踏んで推定してみる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- Rehnquist %>%\n  # データを長形式に変換\n  pivot_longer(cols = -c(term, time), names_to = \"name\", values_to = \"y\") %>%\n  # ケース ID を追加\n  mutate(case = (row_number() - 1) %/% 9 + 1)\n```\n:::\n\n\n\n### １母数モデル（`brms` パッケージ）\n\nまずは最も簡単な項目反応モデルとして，$g$ を logit リンクとして，\n$$\ng(\\P[Y_{ij}=1])=\\al_0+\\al_j-x_i\n$$\nというモデルを推定することを考えよう．\n\n`brms` パッケージを用いれば，他の R パッケージと同様のインターフェイスで推定を行うことができる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\nformula <- bf(\n  y ~ 1 + (1 | case) + (1 | name)\n)\nfit_1PL <- brm(\n  formula,\n  data = df,\n  family = brmsfamily(\"bernoulli\", link = \"logit\"),\n  chains = 4, cores = 4\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_1PL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: bernoulli \n  Links: mu = logit \nFormula: y ~ 1 + (1 | case) + (1 | name) \n   Data: df (Number of observations: 4343) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~case (Number of levels: 485) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.05      0.06     0.93     1.18 1.00     2016     2773\n\n~name (Number of levels: 9) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.62      0.47     0.99     2.75 1.01      875     1534\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.11      0.54    -1.15     1.00 1.00      652     1263\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n簡単なモデルであるが切片項の ESS が低く，すでに暗雲が立ち込めている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit_1PL)\n```\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nここには変動係数（我々の欲しい潜在変数）はパラメータとみなされておらず，推定値が表示されないので次のようにしてプロットする必要がある：\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranef_legislator <- ranef(fit_1PL)$name\nposterior_means <- ranef_legislator[,1,\"Intercept\"]\nlower_bounds <- ranef_legislator[,3,\"Intercept\"]\nupper_bounds <- ranef_legislator[,4,\"Intercept\"]\nplot_legislator <- data.frame(\n  legislator = rownames(ranef_legislator),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_1PL <- ggplot(plot_legislator, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"1PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Legislator\")\np_1PL\n```\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nThomas や Scalia，そして Stevens が極端であることはとらえているが，Stevens や Ginsburg らリベラルな判事は左側に来て欲しいのであった．\n\n誘導が成功しておらず，片方の峯からサンプリングしてしまっている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(fit_1PL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                prior     class      coef group resp dpar nlpar lb ub\n student_t(3, 0, 2.5) Intercept                                      \n student_t(3, 0, 2.5)        sd                                  0   \n student_t(3, 0, 2.5)        sd            case                  0   \n student_t(3, 0, 2.5)        sd Intercept  case                  0   \n student_t(3, 0, 2.5)        sd            name                  0   \n student_t(3, 0, 2.5)        sd Intercept  name                  0   \n       source\n      default\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n```\n\n\n:::\n:::\n\n\n\n### １母数モデル（`rstan` パッケージ）\n\n`brms` パッケージ内で生成される Stan コードを参考にして，自分で Stan コードを書いて推定することもできる．\n\n::: {.callout-caution title=\"Stan コードの出力\" collapse=\"true\" icon=\"false\"}\n\n```r\nstancode(fit_1PL)\n```\n\n```stan\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int<lower=1> N_1;  // number of grouping levels\n  int<lower=1> M_1;  // number of coefficients per level\n  array[N] int<lower=1> J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int<lower=1> N_2;  // number of grouping levels\n  int<lower=1> M_2;  // number of coefficients per level\n  array[N] int<lower=1> J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector<lower=0>[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector<lower=0>[M_2] sd_2;  // group-level standard deviations\n  array[M_2] vector[N_2] z_2;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  vector[N_2] r_2_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  r_2_1 = (sd_2[1] * (z_2[1]));\n  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(z_2[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n```\n\n:::\n\n```r\nlibrary(rstan)\nstan_code <- \"\ndata {\n  int<lower=1> n;  // data size: n = N * J - #(NA responses)\n  int<lower=1> N;  // number of judges\n  int<lower=1> J;  // number of cases\n\n  array[n] int<lower=0, upper=1> Y;  // response variable\n  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]\n  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  real alpha_zero;  // intercepts\n  vector[J] alpha;  // item effects\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(alpha_zero | 3, 0, 2.5);\n  lprior += student_t_lpdf(alpha | 3, 0, 2.5);\n  lprior += student_t_lpdf(X | 3, 0, 2.5);\n}\nmodel {\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] - X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu + alpha_zero);\n  target += lprior;\n}\n\"\ncase_number <- as.integer(nrow(df) / 9)\nindicator_i <- rep(1:9, times = case_number)\nindicator_j <- rep(1:case_number, each = 9)\ndf$i <- indicator_i\ndf$j <- indicator_j\n\ndf_NA <- df %>% filter(!is.na(y))\n\ndata <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n```\n\n```r\nall_samples <- extract(fit, pars = \"X\")$X\nx_samples <- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe <- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\np <- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"1PL Model (RStan)\")\n```\n\n![](Images/1PL.png)\n\n`brms` による推定結果と，左右が逆になっている．これはモデル式を `alpha[j[k]] - X[i[k]]` と Stan コードに記入しており，`brms` の設定と（たまたま）逆になったためである．\n\n余談であるが，$\\al_0$ を固定された切片項でなくて，変動させる（変量効果にする）ことで，事後分散は大きく縮まる．\n\n係数 $\\al_j,x_i$ の事前分布を正規分布に変更したりしても結果はほとんど変わらない．\n\n### ２母数モデル {#sec-2PL}\n\n[@Bafumi+2005] など，多くの理想点モデルでは２母数ロジットモデルが用いられる：\n$$\ng(x):=\\operatorname{logit}(x)=\\log\\frac{x}{1-x},\n$$\n$$\ng(\\mu_{i,j})=\\al_j+\\beta_j x_i=\\beta_j\\Paren{\\wt{\\al}_j+x_i}.\n$$\nこの際 $x_i$ は $i$ 番目の判事の **理想点** といい，$\\al_j,\\beta_j$ は $j$ 番目の事件の性質を表すパラメータである．\n\nものによっては判事の立場が関係ない事件もあるため，$\\beta_j$ が用意されている．\n\n基本的にこの識別パラメータが正になるように調整したいが，明示的にそうすることはしない．\n\n次節で説明する方法により，理想点 $x_i$ が大きい場合は保守的な判断を下しやすいものと解釈できるように設計することができる（$x_i$ を数直線上にプロットした際に，リベラルな場合に左に，保守的な場合に右に来るようにする）が，ここではストレートに実装してみよう．\n\n```r\nlibrary(rstan)\nstan_code <- \"\ndata {\n  int<lower=1> n;  // n = N * J - #(NA responses)\n  int<lower=1> N;  // number of judges\n  int<lower=1> J;  // number of cases\n\n  array[n] int<lower=0, upper=1> Y;  // response variable\n  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]\n  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  vector[J] alpha;  // item effects\n  vector[J] beta;  // item discremination\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += std_normal_lpdf(alpha);\n  lprior += std_normal_lpdf(beta);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\"\ncase_number <- as.integer(nrow(df) / 9)\nindicator_i <- rep(1:9, times = case_number)\nindicator_j <- rep(1:case_number, each = 9)\ndf$i <- indicator_i\ndf$j <- indicator_j\n\ndf_NA <- df %>% filter(!is.na(y))\n\ndata <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n```\n\n```r\nall_samples <- extract(fit, pars = \"X\")$X\nx_samples <- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe <- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\nggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"Ideal Points of Rehnquist\")\n```\n\n![](Images/2PL.png)\n\n２つを並べてみると\n\n::: {layout-ncol=2}\n![1PL Model](Images/1PL.png)\n\n![2PL Model](Images/2PL.png)\n:::\n\nモデルの自由度が上がったことにより，事後分散が大幅に小さくなっていることがわかる．\n\nまた，1PL の場合と再び左右が反転し，最後のリベラルな判事の２人 Ginsburg と Breyer が右に来てしまっている．\n\nこれはモデル式を `mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];` と足し算に戻したからだろうか？\n\n実はそうではない．実際，何度か MCMC を回し直すと，ちゃんとリベラルな判事が左に来てくれることもある．\n\n即ち，**事後分布が多峰性を持つ** のである！\n\n<!-- \n\n```r\nformula_2PL <- bf(\n  y ~ 0 + (1 + name | case)\n)\nfit_2PL <- update(fit_1PL, formula = formula_2PL, cores = 4, iter = 3000)\n```\n\n```r\nsummary(fit_2PL)\n```\n\n    Family: bernoulli \n      Links: mu = logit \n    Formula: y ~ (1 | case) + (case | name) - 1 \n      Data: df (Number of observations: 4343) \n      Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n            total post-warmup draws = 4000\n\n    Multilevel Hyperparameters:\n    ~case (Number of levels: 485) \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n    sd(Intercept)     1.05      0.06     0.93     1.18 1.00     1800     2643\n\n    ~name (Number of levels: 9) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n    sd(Intercept)           1.59      0.44     0.96     2.70 1.00      767     1456\n    sd(case)                0.00      0.00     0.00     0.00 1.00     1144     2343\n    cor(Intercept,case)    -0.06      0.48    -0.87     0.87 1.00     4220     2337\n\n    Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n    and Tail_ESS are effective sample size measures, and Rhat is the potential\n    scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n```r\nplot(fit_2PL)\n```\n\n![](Images/plot_2PL.png){width=70% fig-align=center}\n\n```r\nranef_legislator <- ranef(fit_2PL)$name\nposterior_means <- ranef_legislator[,1,\"Intercept\"]\nlower_bounds <- ranef_legislator[,3,\"Intercept\"]\nupper_bounds <- ranef_legislator[,4,\"Intercept\"]\nplot_legislator <- data.frame(\n  legislator = rownames(ranef_legislator),\n  mean = posterior_means,\n  lower = lower_bounds,\n  upper = upper_bounds\n)\np_2PL <- ggplot(plot_legislator, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"2PL Model\",\n       x = \"Posterior Estimate\",\n       y = \"Legislator\")\ngrid.arrange(p_1PL, p_2PL, nrow = 1)\n```\n\n![](Images/grid_arrange.png)\n\nよりモデルの不確実性が減って，$0$ の周りに縮小されたことがわかる．これは一般の項目反応モデルで見られる：\n\n::: {#lst-embedding}\n:::\n\n左派が右に表示されてしまっていることは変わらない．\n-->\n\n### 識別可能性 {#sec-identification}\n\n実は２母数ロジットモデルでは３つ識別不可能性を引き起こす対称性がある：\n\n::: {.callout-tip appearance=\"simple\" icon=\"false\" title=\"[Section 2 @Bafumi+2005]\"}\n\n* 加法的別名 (additive aliasing)\n\n  $\\wt{\\al}_j,x_i$ に同じ数を足した場合と $\\beta_j$ と $(\\wt{\\al}_j,x_i)$ に同じ数を乗じた／除した場合，全く等価なモデルが得られる．すなわちスケールを定める必要がある．\n\n* 乗法的別名 (additive aliasing)\n\n  $\\beta_j$ を $a>0$ 倍し，$\\wt{\\al}_j,x_i$ を $1/a$ 倍すると等価な尤度を定める．\n\n* 反転対称性 (reflection invariance)\n\n  最後に $\\beta_j$ の符号の問題があるためである．$\\beta_j$ を $-1$ 倍させることで $\\wt{\\al}_j,x_i$ の役割を $-1$ 倍させることができる．このまま推定すると事後分布は $0$ に関して対称な形を持つことになる．\n\n:::\n\n<!-- \nベイズ推定では，次のような事前分布と階層構造を置くことでこの問題を回避できる：\n$$\nx_i\\iidsim\\rN(0,1),\\qquad \\al_j=\\mu_\\al+\\ep_\\al,\\qquad\\ep_\\al\\iidsim\\rN(0,\\sigma^2_\\al),\n$$\n$$\n\\beta_j=\\mu_\\beta+\\ep_\\beta,\\qquad\\ep_\\beta\\iidsim\\rN(0,\\sigma^2_\\beta).\n$$\n\n```r\nlibrary(rstan)\nstan_code <- \"\ndata {\n  int<lower=1> n;  // n = N * J - #(NA responses)\n  int<lower=1> N;  // number of judges\n  int<lower=1> J;  // number of cases\n\n  array[n] int<lower=0, upper=1> Y;  // response variable\n  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]\n  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points\n  vector[J] alpha_zero;  // item effects\n  vector[J] beta_zero;  // item discremination\n\n  real<lower=0> sigma_al;  // sd of alpha\n  real<lower=0> sigma_beta;  // sd of beta\n\n  vector[J] alpha;  // real values of alpha\n  vector[J] beta;  // real values of beta\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(alpha_zero | 3, 0, 2.5);\n  lprior += student_t_lpdf(beta_zero | 3, 0, 2.5);\n  lprior += student_t_lpdf(sigma_al | 3, 0, 2.5);\n  lprior += student_t_lpdf(sigma_beta | 3, 0, 2.5);\n  lprior += std_normal_lpdf(X);\n}\nmodel {\n  alpha ~ normal(alpha_zero, sigma_al);\n  beta ~ normal(beta_zero, sigma_beta);\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n\"\ncase_number <- as.integer(nrow(df) / 9)\nindicator_i <- rep(1:9, times = case_number)\nindicator_j <- rep(1:case_number, each = 9)\ndf$i <- indicator_i\ndf$j <- indicator_j\n\ndf_NA <- df %>% filter(!is.na(y))\n\ndata <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)\nfit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n```\n\n```r\nall_samples <- extract(fit, pars = \"X\")$X\nx_samples <- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe <- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\nggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"Ideal Points of Rehnquist\")\n```\n\n::: {layout-ncol=2}\n![2PL Model](Images/stan2.png)\n\n![Identified 2PL hierarchical model](Images/stan3.png)\n:::\n-->\n\n\n$\\beta_j$ の符号を制約したり，特定の判事の $x_i$ を固定して参照点とするなどの方法があるかもしれないが，ここでは [2.2.3 節 @Bafumi+2005 p.178] に倣って，階層モデルの方法により，構造的なやり方でモデルに情報を伝える．\n\nというのも，理想点 $x_i$ に次の階層構造を入れるのである：\n$$\nx_i=\\delta+\\gamma z_i+\\ep_i\\qquad\\ep_i\\iidsim\\rN(0,1).\n$$\n\n$z_i$ は当該判事を示した大統領の所属政党を表す２値変数で，共和党ならば $z_i=1$ とする．そして $\\gamma$ に $\\R_+$ 上に台を持つ事前分布を置く．\n\n::: {.callout-important appearance=\"simple\" icon=\"false\" title=\"[@Bafumi+2005] による理想点モデルの階層化\"}\n\nこのように共変量を適切な階層に追加することは，モデルに自然な形で正則化情報を伝えることに繋がり，モデルの識別やより現実的な推定値の獲得に繋がる．\n\n:::\n\n### 階層ベイズ推定\n\n#### 指名大統領の政党属性\n\n続いて [-@sec-identification] で検討した，[@Bafumi+2005] による階層ベイズモデルにより緩やかに情報を伝えることで識別可能性を保つ方法を検討する（第 [-@sec-Bafumi] 節）．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- df %>%\n  mutate(\n    nominator = case_when(\n      name %in% c(\"Rehnquist\", \"Stevens\") ~ \"Nixon\",\n      name %in% c(\"O.Connor\", \"Scalia\", \"Kennedy\") ~ \"Reagan\",\n      name %in% c(\"Souter\", \"Thomas\") ~ \"Bush\",\n      name %in% c(\"Breyer\", \"Ginsburg\") ~ \"Clinton\"\n    )\n  )\ndf$x <- ifelse(\n  df$nominator %in% c(\"Nixon\", \"Reagan\", \"Bush\", \"Trump\"),\n  1, -1)\n```\n:::\n\n\n\n#### 階層２母数モデル {#sec-Bafumi}\n\n`x` の情報を階層的に伝えるには，もはや `brms` パッケージでは実行できないようである．\n\n::: {.callout-caution title=\"`fit_2PL::brmsfit` で用いた Stan コードの出力\" collapse=\"true\" icon=\"false\"}\n\n`brms` パッケージでは [`stancode()`](https://paulbuerkner.com/brms/reference/stancode.html) 関数を用いて Stan コードを出力できる．\n\n```r\nstancode(fit_2PL)\n```\n\n結果は以下の通りになる：\n\n```stan\n// generated with brms 2.21.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int<lower=1> N;  // total number of observations\n  array[N] int Y;  // response variable\n  // data for group-level effects of ID 1\n  int<lower=1> N_1;  // number of grouping levels\n  int<lower=1> M_1;  // number of coefficients per level\n  array[N] int<lower=1> J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  // data for group-level effects of ID 2\n  int<lower=1> N_2;  // number of grouping levels\n  int<lower=1> M_2;  // number of coefficients per level\n  array[N] int<lower=1> J_2;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  vector[N] Z_2_2;\n  int<lower=1> NC_2;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector<lower=0>[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n  vector<lower=0>[M_2] sd_2;  // group-level standard deviations\n  matrix[M_2, N_2] z_2;  // standardized group-level effects\n  cholesky_factor_corr[M_2] L_2;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  matrix[N_2, M_2] r_2;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_2] r_2_1;\n  vector[N_2] r_2_2;\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  // compute actual group-level effects\n  r_2 = scale_r_cor(z_2, sd_2, L_2);\n  r_2_1 = r_2[, 1];\n  r_2_2 = r_2[, 2];\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)\n    - 2 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\n    }\n    target += bernoulli_logit_lpmf(Y | mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n  target += std_normal_lpdf(to_vector(z_2));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n  vector<lower=-1,upper=1>[NC_2] cor_2;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_2) {\n    for (j in 1:(k - 1)) {\n      cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n    }\n  }\n}\n```\n\n`function` ブロックでは `scale_r_cor()` 関数が定義されている．標準化された項目変数 $(\\al_j,\\beta_j)$ を `z` として，その各次元の標準偏差を含む２次元ベクトルを `sd_2`，Cholesky 因子を `L_2` として，行列積 `sd_2*L2` にベクトル `z` を掛けて返している．これは $(\\al_j,\\beta_j)$ を３つの要素（対角行列・Cholesky 因子・標準化されたベクトル）に因数分解して計算していることに起因する．\n\n`data` ブロックでデータのコーディングを定めている．`Y` は $NJ$ 行列である．`case` が `ID2` で `name` が `ID1` に対応する．判事 $i\\in[n]$ は `N_1` 人，件数 $j\\in[J]$ は `N_2` 件．このデータから `M_1=1` 次元の理想点を持った `M_2=2` パラメータモデルを推定する．`Z_1_1` が判事を表す標示変数で，項目を表す標示変数は `Z_2_1` と `Z_2_2` である．\n\n`parameter` ブロックでは標準化された理想点 `z_1` と項目パラメータ `z_2`，それぞれの標準偏差 `sd_1`, `sd_2` を宣言している．`z_1` だけ `N_1` ベクトル，`z_2` は `N_2` 行 `M_2` 列の行列であることに注意．\n\n`transformed_parameters` で真のスケールに戻す．`r_1_1=(sd_1[1]*(z_1[1]))` は理想点 $x_1$ にあたり，`r_2=scale_r_cor(z_2,sd_2,L_2)` は項目パラメータ $(\\al_j,\\beta_j)$ にあたる．その後 `r_2_1=r_2[,1]` と `r_2_2=r_2[,2]` でそれぞれ $\\al_j$ と $\\beta_j$ に分解している．最後に `sd_1`, `sd_2` に t-分布，`L_2` に Cholesky 分布を事前分布として定義している．\n\n`model` で尤度を定義している．`mu` はこのブロックでしか使われない線型予測子の格納変数である．\n```stan\nmu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];\n```\nにより $\\al_j+\\beta_j x_i$ が計算されている．$j$ は `J_2[n]`, $i$ は `J_1[n]` で表されている．最後に `bernoulli_logit_lpmf(Y | mu)` で尤度が定義される．\n\n`generated quantities` ブロックでは相関行列 `cor_2` を計算している．\n\n:::\n\n```{.r filename=\"Files/bafumi.stan\"}\ndata {\n  int<lower=1> n;  // n = N * J - #(NA responses)\n  int<lower=1> N;  // number of judges\n  int<lower=1> J;  // number of cases\n\n  array[n] int<lower=0, upper=1> Y;  // response variable\n  vector[N] Z;  // covariates for judges\n  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]\n  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]\n}\nparameters {\n  vector[N] X;  // ideal points for judges\n  vector[J] alpha;\n  vector[J] beta;\n\n  real delta;\n  real gamma;\n  real<lower=0> sigma;\n}\ntransformed parameters {\n  real lprior = 0;\n\n  lprior += student_t_lpdf(delta | 3, 0, 2.5);\n  lprior += student_t_lpdf(gamma | 3, 0, 2.5);\n  lprior += student_t_lpdf(alpha | 3, 0, 2.5);\n  lprior += student_t_lpdf(beta | 3, 0, 2.5);\n  lprior += std_normal_lpdf(X);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  X ~ normal(delta + Z * exp(gamma), sigma);\n\n  vector[n] mu = rep_vector(0, n);\n  for (k in 1:n) {\n    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];\n  }\n  target += bernoulli_logit_lpmf(Y | mu);\n  target += lprior;\n}\n```\n\n<!-- \n```r\n## fit without initial values\n\nlibrary(rstan)\ncase_number <- as.integer(nrow(df) / 9)\nindicator_i <- rep(1:9, times = case_number)\nindicator_j <- rep(1:case_number, each = 9)\ndf$i <- indicator_i\ndf$j <- indicator_j\n\ndf_NA <- df %>% filter(!is.na(y))\n\ndata <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, Z = df_NA$x[1:9], i = df_NA$i, j = df_NA$j)\n\nfit <- stan(\"Files/bafumi_modified.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n```\n\n```r\n## fit with erronous initial values\n\ninit_values <- list(X = c(-1.0,1.0,1.0,-1.0,-1.0,0.0,0.0,1.0,1.0), alpha = rep(0.0, case_number), beta = rep(0.0, case_number), delta = 0.0, gamma = 0.0, sigma = 1.0)\nfit <- stan(\"Files/bafumi.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000, init = rep(list(init_values), 4))\n```\n\n```r\n## fit with correct initial values\n\n#| output: false\ninit_values <- list(X = -1.0 * c(-1.0,1.0,1.0,-1.0,-1.0,0.0,0.0,1.0,1.0), alpha = rep(0.0, case_number), beta = rep(0.0, case_number), delta = 0.0, gamma = 0.0, sigma = 1.0)\nfit <- stan(\"Files/bafumi.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000, init = rep(list(init_values), 4))\n```\n\n```r\n## plot X (ideal points)\n\nall_samples <- extract(fit, pars = \"X\")$X\nx_samples <- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n\nplot_dataframe <- data.frame(\n  legislator = colnames(Rehnquist)[1:9],\n  mean = apply(x_samples, 2, mean),\n  lower = apply(x_samples, 2, quantile, probs = 0.025),\n  upper = apply(x_samples, 2, quantile, probs = 0.975)\n)\np <- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n  geom_point() +\n  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"Mean\", y = \"Legislator\", title = \"Ideal Points of Rehnquist\")\np\n# ggsave(\"Files/Bafumi_Model1.png\", p)\n```\n-->\n\n![階層モデルの推定結果](Images/stan4.png)\n\n::: {layout-ncol=2}\n\n![今回の推定結果](Images/stan4.png)\n\n![２母数モデル [-@sec-2PL] の推定結果](Images/2PL.png)\n\n:::\n\n### 識別性の影響\n\n実は階層的に緩やかにしか情報を伝えていないために，今回の $\\gamma$ への事前分布の設定では，まだ $0$ の近くに密集することでもう一つの峰を作り出してしまうようである．\n\n例えば，Stevens, Souter, Ginsburg, Breyer らリベラルな判事の初期位置を右側の $1$ に，Thomas, Scalia ら保守派の判事の初期位置を $-1$ に，そのほか中道的な判事の初期位置を $0$ にしてサンプリングをすると，ほぼ確実に左右が逆になった結果が出てくる．\n\nその際は $\\gamma$ の事後分布が大きく違う．\n\n```r\ninit_values <- list(X = c(-1.0,1.0,1.0,-1.0,-1.0,0.0,0.0,1.0,1.0), alpha = rep(0.0, case_number), beta = rep(0.0, case_number), delta = 0.0, gamma = 0.0, sigma = 1.0)\nfit <- stan(\"Files/bafumi.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000, init = rep(list(init_values), 4))\n```\n\n<!-- \n```r\ngamma_all_samples <- extract(fit, pars = \"gamma\")$gamma\ngamma_samples <- gamma_all_samples[(nrow(gamma_all_samples) - 999):nrow(gamma_all_samples)]\np <- ggplot(data.frame(gamma = exp(gamma_samples)), aes(x = gamma)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\") +\n  # xlim(0, 1) +\n  labs(x = \"Gamma\", y = \"Density\", title = \"Posterior Distribution of Gamma\")\n# ggsave(\"Files/Bafumi_Model_erronous_Gamma.png\", p)\np\n```\n-->\n\n\n::: {layout-ncol=2}\n\n![理想点の事後平均の位置が正しい場合の回帰係数 $\\gamma$ の事後分布](Files/Bafumi_Model1_Gamma.png)\n\n![理想点の事後平均の位置が間違っている場合の回帰係数 $\\gamma$ の事後分布](Files/Bafumi_Model_erronous_Gamma.png)\n\n:::\n\n<!--\n```r\ndelta_all_samples <- extract(fit, pars = \"delta\")$delta\ndelta_samples <- delta_all_samples[(nrow(delta_all_samples) - 999):nrow(delta_all_samples)]\np <- ggplot(data.frame(delta = delta_samples), aes(x = delta)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\") +\n  labs(x = \"Delta\", y = \"Density\", title = \"Posterior Distribution of Delta\")\n# ggsave(\"Files/Bafumi_Model_erronous_Delta.png\", p)\np\n```\n-->\n\n::: {layout-ncol=2}\n\n![理想点の事後平均の位置が正しい場合の回帰係数 $\\delta$ の事後分布](Files/Bafumi_Model1_Delta.png)\n\n![理想点の事後平均の位置が間違っている場合の回帰係数 $\\delta$ の事後分布](Files/Bafumi_Model_erronous_Delta.png)\n\n:::\n<!-- \n```r\nX_calc <- delta_samples + df_NA$x[1:9] * exp(gamma_samples)\np <- ggplot(data.frame(x = X_calc), aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\") +\n  labs(x = \"X\", y = \"Density\", title = \"Posterior Distribution of X\")\n# ggsave(\"Files/Bafumi_Model_erronous_X.png\", p)\np\n```\n-->\n::: {layout-ncol=2}\n\n![理想点の事後平均の位置が正しい場合の理想点の事後分布](Files/Bafumi_Model1_X.png)\n\n![理想点の事後平均の位置が間違っている場合の理想点の事後分布](Files/Bafumi_Model_erronous_X.png)\n\n:::\n\n### 事後分布の２峰性\n\n<!-- この階層モデルは極めて事前分布の影響を受けやすい． -->\n\nしかし $\\al_j,\\beta_j,\\delta,\\gamma$ の事前分布はあまり大きな影響はないようである．$\\gamma$ の中心も少しズラしたくらいでは関係ない．$\\gamma$ を分散 $1$ で平均 $10$ の正規分布などにすると，$\\gamma$ の事後分布は右に引っ張れる．\n\n::: {layout-ncol=2}\n\n![元々の事前分布](Files/Bafumi_Model_erronous_Gamma.png)\n\n![$\\gamma$ を右に引っ張った場合の事後分布](Files/Bafumi_Model_artificial_Gamma.png)\n\n:::\n\nしかし理想点の結果自体はほとんど変わらない：\n\n::: {layout-ncol=2}\n\n![元々のモデルの結果](Files/Bafumi_Model1.png)\n\n![$\\gamma$ の事後分布を右に引っ張った場合](Files/Bafumi_Model_artificial.png)\n\n:::\n\n次は初期値をランダムとして９回実行して得る結果である：\n\n```{.r filename=\"experiment.r\"}\nfor (i in 1:9) {\n  execution_time <- system.time({\n    fit <- stan(\"Files/bafumi_normal.stan\", data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)\n  })['elapsed']\n  all_samples <- extract(fit, pars = \"X\")$X\n  last_1000_samples <- all_samples[(nrow(all_samples) - 999):nrow(all_samples), ]\n  plot_dataframe <- data.frame(\n      legislator = colnames(Rehnquist)[1:9],\n      mean = apply(last_1000_samples, 2, mean),\n      lower = apply(last_1000_samples, 2, quantile, probs = 0.025),\n      upper = apply(last_1000_samples, 2, quantile, probs = 0.975)\n  )\n  title <- paste0(\"Elapsed time: \", execution_time, \" seconds\")\n  p <- ggplot(plot_dataframe, aes(x = mean, y = legislator)) +\n      geom_point() +\n      geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +\n      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n      labs(x = \"Mean\", y = \"Legislator\", title = title)\n  ggsave(paste0(\"Files/experiment_\", i, \".png\"), p)\n}\n```\n\n::: {layout-ncol=3}\n![Elapsed time: 16.99 seconds](Files/experiment_1.png)\n\n![Elapsed time: 17.01 seconds](Files/experiment_2.png)\n\n![Elapsed time: 16.30 seconds](Files/experiment_3.png)\n:::\n\n::: {layout-ncol=3}\n![Elapsed time: 15.00 seconds](Files/experiment_4.png)\n\n![Elapsed time: 14.98 seconds](Files/experiment_5.png)\n\n![Elapsed time: 15.06 seconds](Files/experiment_6.png)\n:::\n\n::: {layout-ncol=3}\n![Elapsed time: 16.42 seconds](Files/experiment_7.png)\n\n![Elapsed time: 16.57 seconds](Files/experiment_8.png)\n\n![Elapsed time: 17.18 seconds](Files/experiment_9.png)\n:::\n\nなんと，緩やかな情報伝達に拘らず，判事の理想点はほぼ完全に識別されているが，**方向が違う**！\n\nサンプリングを繰り返すことで結果がよく移り変わる．即ち，この事後分布は２峰あるようである．\n\nそして burn-in 後の 1000 回のサンプリング期間では，異なる峰の間を移り変わることはほとんどないようである．\n\n## 理想点解析パッケージ一覧\n\n本節では次の３つのパッケージを紹介する：\n\n::: {.callout-tip appearance=\"simple\" icon=\"false\"}\n\n* `pscl` [@Zeileis+2008]：[GitHub](https://github.com/atahk/pscl), [CRAN](https://cran.r-project.org/web/packages/pscl/index.html)．[@Arnold2018] も参照．\n* `MCMCpack` [@Martin+2011]：[GitHub](https://github.com/cran/MCMCpack), [CRAN](https://cran.r-project.org/web/packages/MCMCpack/index.html)\n* `emIRT` [@Imai+2016]：[GitHub](https://github.com/kosukeimai/emIRT), [CRAN](https://cran.r-project.org/web/packages/emIRT/index.html)\n\n:::\n\n### `pscl` パッケージ\n\n```r\ninstall.packages(\"pscl\")\n```\n\n#### `voteview` データ\n\nこのパッケージでは，Keith T. Poole と Howard Rosenthal が 1995 年から運営しているサイト [`voteview.com`](https://voteview.com/) のデータを利用するための関数 `readKH()` が提供されている．\n\n例えば連邦議会 (U.S. Congress) 117 議会期 (Congress) 2021.1.3-2023.1.3 の上院 (Senate) の点呼投票データを読み込むには以下のようにする：^[１つの議会期 (Congress) は２つの会期 (Session)，第１会期と第２会期から構成される．]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pscl)\ns117 <- readKH(\"https://voteview.com/static/data/out/votes/S117_votes.ord\",\n                desc=\"117th U.S. Senate\")\n```\n:::\n\n\n\n`s117` は `rollcall` オブジェクト，８つのフィールドを持った配列である．\n\n`s117$votes` データは $n=104$ 議員の計 $m=949$ 回の投票からなる $10$-値の行列である．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(s117)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSummary of rollcall object s117 \n\nDescription:\t 117th U.S. Senate \nSource:\t\t https://voteview.com/static/data/out/votes/S117_votes.ord \n\nNumber of Legislators:\t\t 104\nNumber of Roll Call Votes:\t 949\n\n\nUsing the following codes to represent roll call votes:\nYea:\t\t 1 2 3 \nNay:\t\t 4 5 6 \nAbstentions:\t 7 8 9 \nNot In Legislature:\t 0 \n\nParty Composition:\n    D Indep     R \n   50     2    52 \n\nVote Summary:\n               Count Percent\n0 (notInLegis)  3544     3.6\n1 (yea)        55542    56.3\n6 (nay)        35995    36.5\n7 (missing)        5     0.0\n9 (missing)     3610     3.7\n\nUse summary(s117,verbose=TRUE) for more detailed information.\n```\n\n\n:::\n:::\n\n\n\n#### 点呼投票データ\n\n点呼投票データとは $n\\times m$ の行列で，そのエントリーは２値変数である（今回は $1$ か $6$）．\n\nしかし実際には種々の欠測により，$0,7,9$ も使われる．\n\nこれをヒートマップで可視化してみる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\n\nvotes_df <- as.data.frame(s117$votes[1:15, 1:15]) %>% rownames_to_column(\"Legislator\")  # 投票データをデータフレームに変換し、行名を列として追加\n\nvotes_long <- votes_df %>% pivot_longer(cols = -Legislator, names_to = \"Vote\", values_to = \"value\")  # データを長形式に変換\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(votes_long, aes(x = Vote, y = Legislator, fill = value)) + geom_tile() + scale_fill_gradient(low = \"white\", high = \"red\") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = \"Votes\", y = \"Legislators\", title = \"Voting Patterns\")  # ヒートマップを作成\n```\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n#### 政党毎の賛成率\n\n政党でソートし，賛成率を最初の 15 法案についてプロットしたものは次の通り：\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(dplyr)\n\n# 政党ごとの賛成票の割合を計算\nparty_votes <- s117$votes %>%\n  as.data.frame() %>%\n  mutate(party = s117$legis.data$party) %>%\n  group_by(party) %>%\n  summarise(across(everything(), ~mean(. == 1, na.rm = TRUE)))\n\n# データを長形式に変換\nparty_votes_long <- party_votes %>% pivot_longer(cols = -party, names_to = \"Vote\", values_to = \"value\")\n\n# DとRのデータのみを抽出\nparty_votes_d <- party_votes_long %>% filter(party == \"D\")\nparty_votes_r <- party_votes_long %>% filter(party == \"R\")\n\n# Democrats (D) のデータのみをプロット\nggplot(party_votes_d, aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value)) +\n  geom_line(color = \"blue\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\",\n       title = \"Proportion of Yea votes for Democrats\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Democrats (D) と Republicans (R) のデータを同じプロットに追加\nggplot() +\n  geom_line(data = party_votes_d[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Democrat\"), linewidth = 0.5) +\n  geom_line(data = party_votes_r[1:15,], aes(x = as.numeric(gsub(\"Vote \", \"\", Vote)), y = value, color = \"Republican\"), linewidth = 0.5) +\n  scale_color_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  labs(x = \"Votes\", y = \"Proportion of Yea votes\", color = \"Party\",\n       title = \"Proportion of Yea votes by Party\")\n```\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n民主党の 0-1 がはっきりした投票行動が見られる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns109 <- readKH(\"https://voteview.com/static/data/out/votes/S109_votes.ord\",\n                desc=\"109th U.S. Senate\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAttempting to read file in Keith Poole/Howard Rosenthal (KH) format.\nAttempting to create roll call object\n109th U.S. Senate \n102 legislators and 645 roll calls\nFrequency counts for vote types:\nrollCallMatrix\n    0     1     6     7     9 \n  645 40207 22650     1  2287 \n```\n\n\n:::\n:::\n\n\n\n#### ベイズ推定\n\n`pscl` パッケージでは，`rollcall` オブジェクトに対して `ideal()` 関数を用いてデータ拡張に基づく Gibbs サンプラーを通じた理想点解析を行うことができる．\n\n[`ideal()` 関数のマニュアル](https://github.com/atahk/pscl/blob/master/man/ideal.Rd) に記載された例では `maxiter=260E3, burnin=10E3, thin=100` での実行が例示されているが，ここでは簡単に実行してみる．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- dim(s117$legis.data)[1]\nx0 <- rep(0,n)\nx0[s117$legis.data$party==\"D\"] <- -1\nx0[s117$legis.data$party==\"R\"] <- 1\n\nlibrary(tictoc)\ntic(\"ideal() fitting\")\n\nid1 <- ideal(s117,\n             d=1,\n             startvals=list(x=x0),\n             normalize=TRUE,\n             store.item=TRUE,\n             maxiter=10000,  # MCMCの反復回数\n             burnin=5000,\n             thin=50,  # 間引き間隔\n             verbose=TRUE)\ntoc()\n```\n:::\n\n\n\n`ideal() fitting: 43.938 sec elapsed` であった．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(id1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLooking up legislator names and party affiliations\nin rollcall object s117 \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n[`plot.ideal()` 関数のマニュアル](https://github.com/atahk/pscl/blob/master/man/plot.ideal.Rd) にある通り，`shoALLNames = FALSE` がデフォルトになっている．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(id1)  # 全議員の正確な推定値が見れる．\n```\n:::\n\n\n\nもっとも保守的な議員として Trump，５番目にリベラルな議員として Biden の名前がみえる．Harris は中道である．\n\n### `MCMCpack` パッケージ\n\n#### ロジットモデルの推定\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MCMCpack)\n# データの生成\nx1 <- rnorm(1000)  # 説明変数1\nx2 <- rnorm(1000)  # 説明変数2\nXdata <- cbind(1, x1, x2)  # デザイン行列\n\n# 真のパラメータ\ntrue_beta <- c(0.5, -1, 1)\n\n# 応答変数の生成\np <- exp(Xdata %*% true_beta) / (1 + exp(Xdata %*% true_beta))\ny <- rbinom(1000, 1, p)\n\n# MCMClogitでサンプリング\nposterior <- MCMClogit(y ~ x1 + x2,    # モデル式\n                      burnin = 1000,    # バーンイン期間\n                      mcmc = 10000,     # MCMCの反復回数\n                      thin = 1,         # 間引き数\n                      verbose = 1000)   # 進捗表示間隔\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 結果の確認\nsummary(posterior)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nIterations = 1001:11000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean      SD  Naive SE Time-series SE\n(Intercept)  0.4464 0.07613 0.0007613       0.002471\nx1          -0.8935 0.08239 0.0008239       0.002677\nx2           0.9617 0.08706 0.0008706       0.002880\n\n2. Quantiles for each variable:\n\n               2.5%     25%     50%     75%   97.5%\n(Intercept)  0.2962  0.3953  0.4464  0.4972  0.6027\nx1          -1.0560 -0.9485 -0.8931 -0.8391 -0.7310\nx2           0.7910  0.9017  0.9608  1.0206  1.1369\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(posterior)\n```\n\n::: {.cell-output-display}\n![](IdealPoint1_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\n#### 変化点解析\n\n[@Chib1998] に基づく変化点モデルのベイズ推定の関数 `MCMCpoissonChange()` も実装されている．詳しくは [@Martin+2011] 第4節参照．\n\n### `emIRT` パッケージ\n\n```r\ninstall.packages(\"emIRT\")\n```\n\nこのパッケージには備え付けの 80-110 議会期の上院における点呼投票データ `dwnom` がある．\n\nこのデータに対して，階層モデルを用いた理想点解析を行う関数 `hierIRT()` がある．\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emIRT)\ndata(dwnom)\n\n## This takes about 10 minutes to run on 8 threads\n## You may need to reduce threads depending on what your machine can support\nlout <- hierIRT(.data = dwnom$data.in,\n                    .starts = dwnom$cur,\n                    .priors = dwnom$priors,\n                    .control = {list(\n                    threads = 8,\n                    verbose = TRUE,\n                    thresh = 1e-4,\n\t\t\t\t    maxit=200,\n\t\t\t\t    checkfreq=1\n                        )})\n\n## Bind ideal point estimates back to legislator data\nfinal <- cbind(dwnom$legis, idealpt.hier=lout$means$x_implied)\n\n## These are estimates from DW-NOMINATE as given on the Voteview example\n## From file \"SL80110C21.DAT\"\nnomres <- dwnom$nomres\n\n## Merge the DW-NOMINATE estimates to model results by legislator ID\n## Check correlation between hierIRT() and DW-NOMINATE scores\nres <- merge(final, nomres, by=c(\"senate\",\"id\"),all.x=TRUE,all.y=FALSE)\ncor(res$idealpt.hier, res$dwnom1d)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}