{
  "hash": "7689ebc9fd8c72fbc8e27a861f13e036",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Masked Diffusion Models\"\nsubtitle: \"A New Light to Discrete Data Modeling\"\nauthor: \"Hirofumi Shiba\"\ndate: 9/14/2025\n# image: ../../2024/Samplers/Files/best.gif\ncategories: [Denoising Model]\nbibliography: \n    - ../../../assets/2023.bib\n    - ../../../assets/2024.bib\n    - ../../../assets/2025.bib\ncsl: ../../../assets/apalike.csl\nabstract: |\n  Masked diffusion models are conceptually based upon an absorbing forward process and its reverse denoising process. However, their roles are intricately intertwined, in that all three aspects of training, sampling, and modeling are involved.\n  To develop our understanding, we give two toy examples, 1d and 2d, without training a neural network, to showcase how absorbing processes behave.\n  We identify core questions which should be investigated to expand our understanding.\nlisting: \n    -   id: diffusion-listing\n        type: grid\n        sort: false\n        contents:\n            - \"DiscreteDiffusion.qmd\"\n            - \"../../2024/Samplers/DDPM.qmd\"\n            - \"../../2024/Samplers/Diffusion.qmd\"\n        date-format: iso\n        fields: [title,image,date,subtitle]\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n    code-summary: Code (tap me)\nexecute:\n  cache: false\n---\n\n::: {.hidden}\n\n::: {.content-visible when-format=\"html\"}\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n\n$$\n\n\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\bR}{\\mathbb{R}}\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\BRac}[1]{\\biggl\\langle#1\\biggr\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\\newcommand{\\rN}{\\operatorname{N}}\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n\n\n\n\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n\\newcommand{\\argmax}{\\operatorname*{argmax}}\\newcommand{\\argmin}{\\operatorname*{argmin}}\n\n\\newcommand{\\pr}{\\mathrm{pr}}\\newcommand{\\Conv}{\\operatorname{Conv}}\\newcommand{\\cU}{\\mathcal{U}}\n\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n\\newcommand{\\grad}{\\operatorname{grad}}\\newcommand{\\rot}{\\operatorname{rot}}\\renewcommand{\\div}{\\operatorname{div}}\\newcommand{\\tr}{\\operatorname{tr}}\\newcommand{\\Tr}{\\operatorname{Tr}}\\newcommand{\\KL}{\\operatorname{KL}}\\newcommand{\\JS}{\\operatorname{JS}}\\newcommand{\\ESS}{\\operatorname{ESS}}\\newcommand{\\MSE}{\\operatorname{MSE}}\\newcommand{\\erf}{\\operatorname{erf}}\\newcommand{\\arctanh}{\\operatorname{arctanh}}\\newcommand{\\pl}{\\operatorname{pl}}\\newcommand{\\minimize}{\\operatorname{minimize}}\\newcommand{\\subjectto}{\\operatorname{subject to}}\\newcommand{\\sinc}{\\operatorname{sinc}}\\newcommand{\\Ent}{\\operatorname{Ent}}\\newcommand{\\Polya}{\\operatorname{Polya}}\\newcommand{\\Exp}{\\operatorname{Exp}}\\newcommand{\\codim}{\\operatorname{codim}}\\newcommand{\\sgn}{\\operatorname{sgn}}\\newcommand{\\rank}{\\operatorname{rank}}\n\n\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\n\n\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n\n\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n\n\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n\n\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n\n\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n\n\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\renewcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\renewcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n\n\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n\n\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n\n\n\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\bL}{\\mathbb{L}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n\n\\newcommand{\\bit}{\\mathrm{bit}}\n\n\\newcommand{\\err}{\\mathrm{err}}\n\n\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n\n\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n\n\n\\newcommand{\\del}{\\partial}\n\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n\n\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n\n\\newcommand{\\Ens}{\\mathrm{Ens}}\n\n\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n\n\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n\n\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n\n\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\renewcommand{\\Beta}{\\operatorname{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n\n\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n\n\n\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\n\n\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}}\n\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n\\newcommand{\\Op}{\\mathrm{Op}}\n\\newcommand{\\Sh}{\\mathrm{Sh}}\n\\newcommand{\\Diff}{\\mathrm{Diff}}\n\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n\n\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{i.i.d.}}{\\sim}}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n\n\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n\n\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n\\newcommand{\\lmd}{\\lambda}\n\\newcommand{\\Lmd}{\\Lambda}\n\\newcommand{\\cI}{\\mathcal{I}}\n\n\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n\\DeclareMathOperator{\\des}{des}\n\\DeclareMathOperator{\\nd}{nd}\n\\DeclareMathOperator{\\dsep}{d-sep}\n\\DeclareMathOperator{\\sep}{sep}\n\\newcommand{\\rLL}{\\mathrm{LL}}\\newcommand{\\HT}{\\mathrm{HT}}\\newcommand{\\PS}{\\mathrm{PS}}\\newcommand{\\rI}{\\mathrm{I}}\n$$\n\n:::\n\n:::\n\n\n\n## Introduction\n\n### A Time-Agnostic Learning Framework\n\nThe absorbing process, a.k.a. masked diffusion, has a unique characteristic as a forward process in a discrete denoising diffusion model; it offers a time-agnostic *learning to unmask* training framework.\n\nWhen the state space is $E^d$ where $E=\\{0,\\cdots,K-1\\}$ is finite, a current practice is to learn a neural network $p_\\theta$ based on a loss given by\n$$\n\\cL(\\theta):=\\int^1_0\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\E\\Square{\\sum_{i=1}^d\\log p_\\theta(X_0^i|X_t)}\\,dt,\n$$ {#eq-L}\nwhere $\\alpha_t$ is a *noising schedule*, determining the convergence speed of the forward process.^[Discrete Flow Matching [@Campbell+2024], [@Gat+2024], [@Shaul+2025] and simplified Masked diffusion [@Shi+2024], [@Ou+2025], [@Zheng+2025] are different frameworks with different ranges, but both lead to the same training objective ([-@eq-L]), when applied to the forward masking process.]\n\nThe expectation in ([-@eq-L]) is exactly a cross-entropy loss. Therefore, the loss ([-@eq-L]) can be understood as a weighted cross-entropy loss, weighted by the noising schedule $\\alpha_t$.\n\nNote that $p_\\theta$ predicts the true state $x_0$, based on the current state $x_t$, some components of which might be masked. Hence we called this framework *learning to unmask*.\n\nNote also that $p_\\theta$ doesn't take $t$ as an argument [@Shi+2024], [@Ou+2025], which we call the *time-agnostic* property, following [@Zheng+2025].\n\n### Choice of $\\alpha_t$\n\nThis 'learning to unmask' task might be very hard when $t$ is near $1$, since most of the $x_t^i$'s are still masked. \n\nFor instance, if we choose a linear schedule\n$$\n\\al_t=1-t\\qquad(t\\in[0,1])\n$$\nthe scaler $\\frac{\\dot{\\al}_t}{1-\\al_t}=-t^{-1}$ before the expectation in ([-@eq-L]) puts less weight on large $t\\approx1$, while puts much more weight on small $t\\approx0$, where most of the $x_t^i$'s should be already unmasked.\n\nHence, selecting the schedule $\\al_t$ to make learning easier can be very effective, for example by reducing the variance of gradient estimator in a SGD algorithm. Actually, this is a part of the technique how [@Arriola+2025] achieved their remarkable empirical results.\n\nThis flexibility of $\\al_t$ is why the loss ([-@eq-L]) is considered as a potential competitor against the current dominant autoregressive models. In fact, one work [@Chao+2025], still under review, claimed their masked diffusion model surpassed the autoregressive model on the task of language modeling, achieving an evaluation perplexity of 15.36 on the OpenWebText dataset.^[In the context of language modeling, the perplexity is defined as $2^{l}$ where $l$ is the average log-likelihood of the test set.]\n\nWe briefly discuss their trick and related promising techniques to improve the model, before programming toy examples in @sec-Demo and @sec-2d-example to deepen our understanding in absorbing forward process.\n\n### A Gibbs Sampler Take {#sec-Gibbs-sampler-take}\n\nOne problem about the loss ([-@eq-L]) is that $p_\\theta$ predicts the unmasked complete sequence in a product form:\n$$\np_\\theta(x_0|x_t)=\\prod_{i=1}^d p_\\theta(x_0^i|x_t).\n$$\n\nThis should cause no problem when we unmask one component at a time, since it will be a form of ancestral sampling based on disintegration property.\n\nHowever, when unmasking two or more components simultaneously, for example when the number of steps is less than $d$, the product form assumption will simply introduce a bias, as the data distribution is by no means of product form on $E^d$.\n\nHere, to recover correctness asymptotically, analogy with Gibbs sampling becomes very important.\n\nFor example, predictor-corrector technique can be readily employed to mitigate this bias, as discussed in [@Lezama+2023], [@Zhao+2024], [@Zhao+2024Unified], [@Gat+2024], [@Wang+2025].\n\nWe demonstrate this strategy in @sec-Predictor-Corrector.\n\n### Intermediate States {#sec-Intermediate-States}\n\nAs we mentioned earlier, unmasking can be a very hard task, as closely investigated in [@Kim+2025 Section 3].\n\nTo alleviate this problem, [@Chao+2025] introduced intermediate states by re-encoding the token in a base-$2$ encoding, such as\n$$\n5\\mapsto 101.\n$$\nThe right-hand side needs three steps to be completely unmasked, while the left-hand side only needs one jump.\n\nTherefore, unmasking can be easier to learn, compared to the original token encoding.\n\nHowever, this is not the only advantage of intermediate states. [@Chao+2025] were able to construct a full predictor $p_\\theta$ without the product form assumption on each token.\n\nThis approach might act as a block Gibbs sampler and make the convergence faster, as we will discuss later in @sec-Chao.\n\n### State-Dependent Rate\n\nAs a function of $t\\mapsto\\alpha_t$, different choices for $\\al_t$ seem to make little impact on the total performance of the model, as we observe in our toy example in @sec-exp.\n\nHowever, if we allow $\\al_t$ to depend on the state as in [@Shi+2024 Section 6], I believe the masked diffusion model will start to show its real potential over currently dominant autoregressive framework.\n\nA problem arises when one tries to learn $\\al_t$ at the same time, for example, by including a corresponding term into the loss ([-@eq-L]). This will lead to amplified variances of the gradient estimates and unstable training, as reported in [@Shi+2024] and [@Arriola+2025].\n\nThe idea of exploiting state-dependent rate is already very common in sampling time [@Peng+2025], [@Liu+2025], [@Kim+2025], [@Rout+2025], determining which token to unmask next during the backward sampling, a bit like Monte Carlo tree search in reinforcement learning.\n\n## Demo {#sec-Demo}\n\nTo demonstrate what an absorbing process does, we carry out generation from a toy data distribution $\\pi_{\\text{data}}$ on $5=\\{0,1,2,3,4\\}$, by running an exact reverse kernel of the absorbing (masked) forward process.\n\nTherefore, no neural network training will be involved. A 2d example, which is much more interesting, in @sec-2d-example will basically process in parallel.\n\n### Setup\n\n::: {#be87021b .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n```\n:::\n\n\n::: {#95c4c33d .cell execution_count=2}\n``` {.python .cell-code}\np_data = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\n```\n:::\n\n\nWe will represent the MASK as $-1$. The state space is then $E:=5 \\cup \\{-1\\}$.\n\n::: {#ece93547 .cell execution_count=3}\n``` {.python .cell-code}\nMASK = -1\n```\n:::\n\n\nAn important design choice in the forward process is the noising schedule $\\alpha_t$, which can be interpreted as *survival probability* and satisfy the following relatioship with the jump intensity $\\beta_t$:\n$$\n\\alpha_t=\\exp\\paren{-\\int^t_0\\beta_s\\,ds},\\qquad t\\in[0,1].\n$$\n\nLet us keep it simple and set $\\alpha_t=t$. To achive this, we need to set\n$$\n\\beta_t=\\frac{1}{1-t},\\qquad t\\in[0,1),\n$$\nwhich is clearly diverging as $t\\to1$. This is to ensure the process to converge in finite time.\n\n::: {#c865e199 .cell execution_count=4}\n``` {.python .cell-code}\nT = 10  # number of steps\nalpha = np.linspace(1.00, 0.00, T+1)\n```\n:::\n\n\n### The Backward Transition Kernel\n\nIn this setting, the backward transition kernel $p(x_{t-1} | x_t)$ satisfies\n$$\n\\P[X_{t-1}=-1|X_t=-1]=\\frac{1 - \\alpha_{t-1}}{1 - \\alpha_t}.\n$$\nIn the other cases, the unmasked values $x_{t-1}$ should be determined according to $\\pi_{\\text{data}}$, which is unavailable in a real setting, of course.\n\n::: {#ece6987d .cell execution_count=5}\n``` {.python .cell-code}\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])  # length T\n```\n:::\n\n\n::: {#71030694 .cell execution_count=6}\n``` {.python .cell-code}\ndef reverse_sample(num_samples: int, p_unmask: np.ndarray):\n    \"\"\"\n    Start from x_T = MASK for all samples, apply the exact reverse transitions down to t=0.\n    Returns x_0 samples in 5 = {0,1,...,4}.\n    \"\"\"\n    x_t = np.full(num_samples, MASK, dtype=int)\n    hist = np.empty((T+1, num_samples), dtype=int)\n    hist[0] = x_t.copy()\n    for t in range(T, 0, -1):\n        idx_mask = np.where(x_t == MASK)[0]  # masked indices\n        if idx_mask.size > 0:\n            u = rng.random(idx_mask.size)\n            unmask_now = idx_mask[u < p_unmask[t-1]]  # indices that are going to be unmasked\n            if unmask_now.size > 0:\n                cats = rng.choice(5, size=unmask_now.size, p=p_data)\n                x_t[unmask_now] = cats\n        hist[T-t+1] = x_t.copy()\n\n    # At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,\n    # but numerically we ensure no MASK remains:\n    assert np.all(x_t != MASK), \"Some samples remained MASK at t=0, which should not happen.\"\n    return x_t, hist\n```\n:::\n\n\n#### A Note on Alternative Sampling Strategies\n\nNote that we need not to obey this exact backward transition kernel to sample from the data distribution.\n\nFor example, remasking [@Lezama+2023], [@Zhao+2024], [@Gat+2024], [@Wang+2025], a form of predictor-corrector sampling, can be incorporated to improve sample quality, mitigating numerical errors, as we will see in @sec-Predictor-Corrector.\n\nRecently, sampling time path planning [@Peng+2025], [@Liu+2025], [@Kim+2025], [@Rout+2025] has been proposed to improve sample quality and model log-likelihood, which lay out of the scope of this post.\n\n### Honest Sampling\n\n::: {#61b923bb .cell execution_count=7}\n``` {.python .cell-code}\nN = 100_000  # size of sample to get\nx0_samples, hist = reverse_sample(N, p_unmask)\n```\n:::\n\n\nWe first make sure our implementation is correct by checking the empirical distribution of the samples generated agrees with the true distribution.\n\n::: {#6e9e46e0 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\ncounts = np.bincount(x0_samples, minlength=5).astype(float)\np_emp = counts / counts.sum()\n\nprint(\"Toy data marginal p_data:\", p_data.round(4))\nprint(\"Empirical p after reverse sampling:\", p_emp.round(4))\n\n# ---------- Bar chart: p_data vs empirical ----------\nxs = np.arange(5)\nwidth = 0.4\nplt.figure(figsize=(6,3))\nplt.bar(xs - width/2, p_data, width=width, label=\"true p_data\")\nplt.bar(xs + width/2, p_emp, width=width, label=\"empirical (reverse)\")\nplt.title(\"Reverse samples match the data marginal\")\nplt.xlabel(\"category id\")\nplt.ylabel(\"probability\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToy data marginal p_data: [0.4  0.3  0.18 0.1  0.02]\nEmpirical p after reverse sampling: [0.3979 0.3002 0.1828 0.0992 0.0198]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-9-output-2.png){width=566 height=278}\n:::\n:::\n\n\nPerfect! Making sure everything is working, we plot 1000 sample paths from the reverse process.\n\n::: {#6f0d93db .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nn_samples_to_plot = min(1000, hist.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist.shape[0]), hist[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-10-output-1.png){width=587 height=449}\n:::\n:::\n\n\nWe see a relatively equal number of jumps per step:\n\n::: {#1c649d70 .cell execution_count=10}\n``` {.python .cell-code}\njump_counts = np.zeros(T)\nfor i in range(10):\n    jump_counts[i] = sum(hist[i] != hist[i+1])\nprint(jump_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 9916. 10017. 10001. 10143. 10062. 10195.  9815. 10005.  9859.  9987.]\n```\n:::\n:::\n\n\nThis is because we set $\\alpha_t=1-t$ to be linear.\n\n::: {#8840ee4b .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# ---------- Plot schedule α_t (survival probability) ----------\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-12-output-1.png){width=470 height=277}\n:::\n:::\n\n\n### Choice of $\\alpha_t$ {#sec-exp}\n\n$\\alpha_t$ controls the convergence rate of the forward process.\n\nWe change $\\al_t$ to see the impact on the sampling accuracy. (There should be no influence as long as the exact backward kernel is used.)\n\nLet us change $\\al_t$ to be an exponential schedule:\n\n::: {#4c7bea6e .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nalpha_exp = np.exp(np.linspace(0.00, -10.00, T+1))\np_unmask_exp = (alpha_exp[:-1] - alpha_exp[1:]) / (1.0 - alpha_exp[1:])\n\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha_exp, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-13-output-1.png){width=470 height=277}\n:::\n:::\n\n\nIn this way, most of the unmasking events should occur in the very last step of the reverse process.\n\n::: {#dc31a589 .cell execution_count=13}\n``` {.python .cell-code}\nx0_exp, hist_exp = reverse_sample(N, p_unmask_exp)\n```\n:::\n\n\n::: {#c9a9a740 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nn_samples_to_plot = min(1000, hist_exp.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist_exp.shape[0]), hist_exp[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-15-output-1.png){width=587 height=449}\n:::\n:::\n\n\nWe see many jumps happen in the latter half.\n\nPractically speaking, this is certainly not what we want.\n\nWe spend almost half of the computational time (up to the 6th step) in simulating the phantom jumps which just do not happen. The same concern was raised by [@Chao+2025].\n\nHowever, the accuracy is same, as the exact kernel is used to simulate, if the computational cost might be different.\n\n::: {#e0983a05 .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\ndef calc_l1_kl(x0_samples, split = 10):\n    chunks = np.array_split(x0_samples, split)\n    counts = np.array([np.bincount(chunk, minlength=5).astype(float) for chunk in chunks])\n\n    p_emp = counts / counts.sum(axis=1)[0]\n    l1 = np.abs(p_emp - p_data).sum(axis=1).mean()\n    l1_var = np.abs(p_emp - p_data).sum(axis=1).var()\n    kl = (np.where(p_emp > 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).mean()\n    kl_var = (np.where(p_emp > 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).var()\n    return l1, l1_var, kl, kl_var\n\nl1, l1_var, kl, kl_var = calc_l1_kl(x0_samples)\nprint(\"Linear Schedule: L1 distance:\", round(l1, 6), \" ± \", round(l1_var, 6), \"   KL(p_emp || p_data):\", round(kl, 6), \" ± \", round(kl_var, 6))\n\nl1_exp, l1_exp_var, kl_exp, kl_exp_var = calc_l1_kl(x0_exp)\nprint(\"Exponential Schedule: L1 distance:\", round(l1_exp, 6), \" ± \", round(l1_exp_var, 6), \"   KL(p_emp || p_data):\", round(kl_exp, 6), \" ± \", round(kl_exp_var, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Schedule: L1 distance: 0.0147  ±  3e-05    KL(p_emp || p_data): 0.000196  ±  0.0\nExponential Schedule: L1 distance: 0.0148  ±  4.9e-05    KL(p_emp || p_data): 0.000232  ±  0.0\n```\n:::\n:::\n\n\n## 2D Example {#sec-2d-example}\n\n### Setup\n\nWe consider a highly correlated distribution, whose support is degenerated on the diagonal element on $5^2$.\n\n::: {#486e4536 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\nK = 5\nMASK = -1\n\n# Base marginal for a single site\np_single = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\np_single /= p_single.sum()\n\n# Build correlated joint with same-parity constraint\nW = np.zeros((K, K), dtype=float)\nfor i in range(K):\n    for j in range(K):\n        if (i % 2) == (j % 2):\n            W[i, j] = p_single[i] * p_single[j]\npi_joint = W / W.sum()\npi_x = pi_joint.sum(axis=1)\npi_y = pi_joint.sum(axis=0)\n\n# Conditionals\ncond_x_given_y = np.zeros((K, K), dtype=float)  # [j, i]\ncond_y_given_x = np.zeros((K, K), dtype=float)  # [i, j]\nfor j in range(K):\n    col = pi_joint[:, j]; s = col.sum()\n    if s > 0:\n        cond_x_given_y[j, :] = col / s\nfor i in range(K):\n    row = pi_joint[i, :]; s = row.sum()\n    if s > 0:\n        cond_y_given_x[i, :] = row / s\n\nfig = plt.figure(figsize=(8, 3.4))\n\n# Heatmap\nax1 = plt.subplot(1, 2, 1)\nim = ax1.imshow(pi_joint, cmap='viridis', aspect='equal')\nax1.set_xlabel('Y')\nax1.set_ylabel('X')\nax1.set_title('Joint Probability Distribution (Heatmap)')\nax1.set_xticks(range(K))\nax1.set_yticks(range(K))\n\n# Value annotation\nfor i in range(K):\n    for j in range(K):\n        ax1.text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\n\nplt.colorbar(im, ax=ax1)\n\n# 3D bar plot\nax2 = plt.subplot(1, 2, 2, projection='3d')\nx = np.arange(K)\ny = np.arange(K)\nX, Y = np.meshgrid(x, y)\nZ = pi_joint\n\nax2.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), \n         0.8, 0.8, Z.ravel(), alpha=0.8, cmap='viridis')\n\nax2.set_xlabel('Y')\nax2.set_ylabel('X')\nax2.set_zlabel('Probability')\nax2.set_title('Joint Probability Distribution (3D)')\nax2.set_xticks(x)\nax2.set_yticks(y)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-17-output-1.png){width=717 height=334}\n:::\n:::\n\n\n### The Backward Transition Kernel {#sec-exact-kernel-2d}\n\nWe will first consider, again, linear schedule:\n\n::: {#d440566f .cell execution_count=17}\n``` {.python .cell-code}\nT = 10\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n```\n:::\n\n\n::: {#9cfadcb0 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(5, 3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-19-output-1.png){width=470 height=277}\n:::\n:::\n\n\nThe code for backward sampling is basically the same, except for the number of `if` branch is now four, rather than just one.\n\n::: {#2cb71a29 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code (definition of reverse_sample_pairs)\"}\ndef reverse_sample_pairs(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = pi_joint.ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n```\n:::\n\n\n### Honest Sampling\n\nAgain, using the exact backward kernel, we are able to reproduce the true joint distribution.\n\n::: {#dc739d61 .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-21-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#885d3f91 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\ndef l1_kl(pairs, split=10):\n    chunks = np.array_split(pairs, split)\n    l1, kl = [], []\n    for chunk in chunks:\n        counts = np.zeros((K, K), dtype=float)\n        for a, b in chunk:\n            counts[a, b] += 1.0\n        pi_emp = counts / counts.sum()\n        \n        eps = 1e-12\n        l1.append(np.abs(pi_emp - pi_joint).sum())\n        nz = (pi_emp > 0) & (pi_joint > 0)\n        kl.append((pi_emp[nz] * np.log((pi_emp[nz] + eps) / pi_joint[nz])).sum())\n    return l1, kl\n\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.024162  ±  4.6e-05    KL(emp || true): 7.286359e-04 ± 7.462192e-08\n```\n:::\n:::\n\n\nNote that since the survival rate $\\al_t$ decreases linearly, the number of newly unmasked coordinates per step will decrease exponentially.\n\n::: {#d1b0eb2a .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nnew_unmasks_per_step = []\nfor t in range(T):\n    changed1 = (h1[t] == MASK) & (h1[t+1] != MASK)\n    changed2 = (h2[t] == MASK) & (h2[t+1] != MASK)\n    new_unmasks_per_step.append(changed1.sum() + changed2.sum())\n\nplt.figure(figsize=(6, 3))\nplt.plot(range(1, T+1), new_unmasks_per_step, marker=\"o\")\nplt.title(\"Newly unmasked coordinates per step\")\nplt.xlabel(\"reverse step (t→t-1)\")\nplt.ylabel(\"#coords\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-23-output-1.png){width=566 height=278}\n:::\n:::\n\n\n#### Larger Step Size\n\nWhat if we employ a large step size?\n\nActually, the result doesn't change. Moreover, the accuracy is higher, since it is equivalent to direct sampling from $\\pi_{\\text{data}}$.\n\n::: {#1276aa86 .cell execution_count=23}\n``` {.python .cell-code}\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n```\n:::\n\n\n::: {#f0096506 .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-25-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#1647e893 .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.021245  ±  5e-05    KL(emp || true): 6.510835e-04 ± 1.258177e-07\n```\n:::\n:::\n\n\n### Coordinate-wise Sampling\n\nThe joint distribution would be unavailable, even if the learning based on the loss ([-@eq-L]) were perfectly done, because of the product form assumption on the neural network predictor $p_\\theta$.^[Of course, the exact sampling would have been available, for exmple, if we learned the backward intensity as [@Campbell+2022]. However, these methods have been marginalized due to suboptimal performance.]\n\nWe mock this situation by replacing the joint distribution in the exact kernel, programmed in @sec-exact-kernel-2d, with the product of its marginals.\n\n::: {#05bbf1c8 .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code (definition of reverse_sample_incorrect)\"}\ndef reverse_sample_incorrect(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n```\n:::\n\n\n::: {#a43ce07c .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-28-output-1.png){width=736 height=321}\n:::\n:::\n\n\nThen, this time the result is not so good.\n\n::: {#4a3901ca .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.089135  ±  1.2e-05    KL(emp || true): -4.100716e-02 ± 6.515707e-06\n```\n:::\n:::\n\n\nThere is a small deviation from the exact kernel, where $\\ell^1$ distance was $0.024\\pm0.00005$.\n\nWe can easily see, in the empirical distribution, some cells are assinged with positive mass, although the true probability is $0$.\n\nThis effect becomes smaller when the number of steps `T` is large. For example, setting `T=1000` gives us with almost same accuracy as the exact kernel (although we don't show it here for brevity).\n\n#### Larger Step Size\n\nThe situation gets worse when the step size is large, for example `T=1`.\n\n::: {#29032feb .cell execution_count=29}\n``` {.python .cell-code code-fold=\"true\"}\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-30-output-1.png){width=740 height=321}\n:::\n:::\n\n\n::: {#aadd5a6c .cell execution_count=30}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.85124  ±  4.4e-05    KL(emp || true): -2.884951e-01 ± 1.114368e-05\n```\n:::\n:::\n\n\nThe error is now significant, because the incorrect product kernel is used every time, as we set $T=1$ meaning unmasking in just one step!\n\nOne way to fix this is to set `T` as large as possible, making sure no more than one jump occurs simultaneously.\n\n### Corrector Sampling {#sec-Predictor-Corrector}\n\nAnother remedy is to utilise Gibbs sampling ideas to correct the bias, again at the expense of computational cost.\n\nTo do this, we must identify a Markov kernel that keeps the marginal distribution of $X_t$ invariant for every timestep $t\\in[0,1]$.\n\nIn our case, let us simply add a *re-masking* step, only to those pairs $(x_t^1,x_t^2)$'s which are completely unmasked.\n\nAs there is some probability of having been unmasked simultaneously, re-masking only one of them, this time $x_t^1$, will allow us a second chance to arrive at a correct pair $(x_t^{1,\\text{corrected}},x_t^2)$.\n\n::: {#891ea461 .cell execution_count=31}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code (definition of reverse_sample_correct)\"}\ndef reverse_sample_corrector(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        # corrector step\n        q = 1.0 - p  # masking probability\n        both = (x1 != MASK) & (x2 != MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            will = rng.random(idx.size) < q\n            idx_now = idx[will]\n            if idx_now.size > 0:\n                x1[idx_now] = MASK\n        # predictor step\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n```\n:::\n\n\n::: {#4f3c8d7c .cell execution_count=32}\n``` {.python .cell-code code-fold=\"true\"}\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_corrector(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-33-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#00bf15e1 .cell execution_count=33}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs, 10)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.022518  ±  2e-05    KL(emp || true): 4.694469e-05 ± 8.500292e-08\n```\n:::\n:::\n\n\nWe see a small improvement from the $\\ell^1$ distance of $0.0241$. The difference is significant in terms of our variance of an order $O(10^{-5})$ culculated from $10$ repeated experiments.\n\nThis is the idea behind the predictor-corrector technique discussed, for example, in [@Gat+2024], [@Zhao+2024], [@Zhao+2024Unified].\n\nWe only included one corrector step per reverse step, although more correction will increase accuracy.\n\nThis can be regarded as a form of inference time compute scaling [@Wang+2025], although it is a fairly bad idea to wait for the predictor-corrector steps to converge.\n\n### Discussion {#sec-Chao}\n\nLet us come back to the technique employed in [@Chao+2025], which we mentioned in @sec-Intermediate-States.\n\nAs we have seen, the number of steps $T$ has to be kept larger than $d$, to make sure no more than one jump occurs simultaneously.\n\nHowever, this increases the computational cost, as more time steps must be spent in simulating phantom jumps.\n\nOne thing we can do is to fill this blank steps with a informed 'half-jump', by introducing a sub-structure in each state $x\\in E$.\n\nThis half-jump is a bit more robust than a direct unmasking. Therefore we are able to spend the time up to $T$ more meaningfully.\n\nThus, this strategy can be view as another way to mitigate the bias introduced by the incorrect backward kernel.\n\n## Future Works\n\nSince the absorbing process is favoured only because of its time-agnostic property, its ability should be explained separately with the properties of the process.\n\n### A Reinforcenment Learning Take\n\nThe inference step and the sampling step should be decoupled, at least conceptually.\n\nTo tune the forward process noising schedule $\\alpha_t(x)$, a reinforcement learning framework will be employed, I believe in the near future.\n\nThis is a variant of meta-learning and, in this way, the unmasking network $p_\\theta$ will be able to efficiently learn the correct dependence structure in the data domain.\n\nFor example, in language modeling, there is a natural sequential structure, which is partly why autoregressive modeling has been dominant in this domain. However, by learning $\\alpha_t$ in masking process, a much more efficent factorization over texts can be aquired in a data-driven way.\n\nI even think this $\\alpha_t$ can play an important role just as a word embedding does currently.\n\nIn the sampling step, a sampling time path planning will greatly enhance sample quality, just as Monte Carlo tree search does in reinforcement learning.\n\nAs a conclusion, the flexible framework of masked diffusion models will enable a marriage with reinforcement learning and meta learning, which will be a way to overcome the autoregressive modeling framework, because the latter imposes unnecessary sequential inductive bias into the model.\n\n### A Scaling Analysis\n\nThere are two papers, namely [@Santos+2023] and [@Winkler+2024], which carry out a scaling analysis as $K\\to\\infty$, in order to bridge the gap between discrete and continuous state spaces.\n\nIn [@Santos+2023 Section 2.4] and its Appendix C, it is proved that a fairly large class of discrete processes will converge to Ito processes, as $K\\to\\infty$.\n\nTheir discussion based on a formal argument. They proves that the Kramers-Moyal expansion of the Chapman-Kolmogorov equation of the discrete process converges to that of a Ito process.\n\nIn other words, they didn't identify the direct correspondence, for example deriving the exact expression of limiting SDEs.\n\n[@Winkler+2024] builds on their analysis, identifying an OU process on $\\R^d$ corresponds to a Ehrenfest process.\n\nThis line of research has a lot to do with thermodynamics, and might provide insights into whether images should be modeled discretely or continuously.\n\nAlso, the limit $d\\to\\infty$ has yet to be explored.\n\n### Concluding Remarks\n\nI believe that masked diffusion modeling can be viewed as a form of Gibbs sampling, with a learned transition kernel.\n\nMany current practices are based on (uniformly) random scan Gibbs, while the autoregressive models are a fixed scan Gibbs.\n\nMost recent improvements are based upon ideas from reinforcement learning and meta learning, where an optimal order to unmask components is pursued.\n\nThis point of view might not be only an abstract nonsense. I actually believe that this point of view will be fruitful in the future.\n\n## 関連記事 {.appendix}\n\n::: {#diffusion-listing}\n:::\n\n",
    "supporting": [
      "CTDDM_files"
    ],
    "filters": [],
    "includes": {}
  }
}