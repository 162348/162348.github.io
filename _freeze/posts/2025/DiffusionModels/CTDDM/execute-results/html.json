{
  "hash": "88decef6a5610ef4184630f4199f3472",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Masked Diffusion Models\"\nsubtitle: \"A New Light to Discrete Data Modeling\"\nauthor: \"Hirofumi Shiba\"\ndate: 9/13/2025\n# image: ../../2024/Samplers/Files/best.gif\ncategories: [Denoising Model]\nbibliography: \n    - ../../../assets/2023.bib\n    - ../../../assets/2024.bib\n    - ../../../assets/2025.bib\ncsl: ../../../assets/apalike.csl\nabstract: |\n  Masked diffusion models are based upon absorbing / masking noising process and its reverse denoising process.\n  To develop our understanding, we give a toy example, without training a neural network, to showcase how absorbing process behaves.\n  We identify core questions which should be investigated to expand our understanding.\nlisting: \n    -   id: diffusion-listing\n        type: grid\n        sort: false\n        contents:\n            - \"DiscreteDiffusion.qmd\"\n            - \"../../2024/Samplers/DDPM.qmd\"\n            - \"../../2024/Samplers/Diffusion.qmd\"\n        date-format: iso\n        fields: [title,image,date,subtitle]\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n    code-summary: Code (tap me)\nexecute:\n  cache: false\n---\n\n::: {.hidden}\n\n::: {.content-visible when-format=\"html\"}\n\nA Blog Entry on Bayesian Computation by an Applied Mathematician\n\n$$\n\n\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\bR}{\\mathbb{R}}\\newcommand{\\F}{\\mathcal{F}}\n\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\BRac}[1]{\\biggl\\langle#1\\biggr\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\\newcommand{\\rN}{\\operatorname{N}}\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n\n\n\n\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n\\newcommand{\\argmax}{\\operatorname*{argmax}}\\newcommand{\\argmin}{\\operatorname*{argmin}}\n\n\\newcommand{\\pr}{\\mathrm{pr}}\\newcommand{\\Conv}{\\operatorname{Conv}}\\newcommand{\\cU}{\\mathcal{U}}\n\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n\\newcommand{\\grad}{\\operatorname{grad}}\\newcommand{\\rot}{\\operatorname{rot}}\\renewcommand{\\div}{\\operatorname{div}}\\newcommand{\\tr}{\\operatorname{tr}}\\newcommand{\\Tr}{\\operatorname{Tr}}\\newcommand{\\KL}{\\operatorname{KL}}\\newcommand{\\JS}{\\operatorname{JS}}\\newcommand{\\ESS}{\\operatorname{ESS}}\\newcommand{\\MSE}{\\operatorname{MSE}}\\newcommand{\\erf}{\\operatorname{erf}}\\newcommand{\\arctanh}{\\operatorname{arctanh}}\\newcommand{\\pl}{\\operatorname{pl}}\\newcommand{\\minimize}{\\operatorname{minimize}}\\newcommand{\\subjectto}{\\operatorname{subject to}}\\newcommand{\\sinc}{\\operatorname{sinc}}\\newcommand{\\Ent}{\\operatorname{Ent}}\\newcommand{\\Polya}{\\operatorname{Polya}}\\newcommand{\\Exp}{\\operatorname{Exp}}\\newcommand{\\codim}{\\operatorname{codim}}\\newcommand{\\sgn}{\\operatorname{sgn}}\\newcommand{\\rank}{\\operatorname{rank}}\n\n\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\n\n\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n\n\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n\n\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n\n\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n\n\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n\n\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n\n\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\renewcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\renewcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n\n\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n\n\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n\n\n\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\bL}{\\mathbb{L}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n\n\\newcommand{\\bit}{\\mathrm{bit}}\n\n\\newcommand{\\err}{\\mathrm{err}}\n\n\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n\n\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n\n\n\\newcommand{\\del}{\\partial}\n\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n\n\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n\n\\newcommand{\\Ens}{\\mathrm{Ens}}\n\n\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n\n\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n\n\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n\n\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\renewcommand{\\Beta}{\\operatorname{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n\n\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n\n\n\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\n\n\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}}\n\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n\\newcommand{\\Op}{\\mathrm{Op}}\n\\newcommand{\\Sh}{\\mathrm{Sh}}\n\\newcommand{\\Diff}{\\mathrm{Diff}}\n\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n\n\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{i.i.d.}}{\\sim}}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n\n\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n\n\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n\\newcommand{\\lmd}{\\lambda}\n\\newcommand{\\Lmd}{\\Lambda}\n\\newcommand{\\cI}{\\mathcal{I}}\n\n\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n\\DeclareMathOperator{\\des}{des}\n\\DeclareMathOperator{\\nd}{nd}\n\\DeclareMathOperator{\\dsep}{d-sep}\n\\DeclareMathOperator{\\sep}{sep}\n\\newcommand{\\rLL}{\\mathrm{LL}}\\newcommand{\\HT}{\\mathrm{HT}}\\newcommand{\\PS}{\\mathrm{PS}}\\newcommand{\\rI}{\\mathrm{I}}\n$$\n\n:::\n\n:::\n\n\n\n## Introduction\n\nThe absorbing process, a.k.a. masking diffusion, has a unique characteristic as a forward process in a discrete denoising diffusion model; it offers a time-agnostic *learning to unmask* training framework.\n\nWhen the state space is $E^d$ where $E$ is finite, current practice is to learn a neural network $p_\\theta$ based on a loss given by\n$$\n\\cL(\\theta):=\\int^1_0\\frac{\\dot{\\alpha}_t}{1-\\alpha_t}\\E\\Square{\\sum_{i=1}^d\\log p_\\theta(X_0^i|X_t)}\\,dt,\n$$ {#eq-L}\nwhere $\\alpha_t$ is a *noising schedule*, determining the convergence speed of the forward process.^[Discrete Flow Matching [@Campbell+2024], [@Gat+2024], [@Shaul+2025] and simplified Masked diffusion [@Shi+2024], [@Ou+2025], [@Zheng+2025] are different frameworks with different ranges, but both lead to the same training objective ([-@eq-L]), when applied to the forward masking process.]\n\nThe expectation in ([-@eq-L]) is exactly a cross-entropy loss. Therefore, the loss ([-@eq-L]) can be understood as a weighted cross-entropy loss, weighted by the noising schedule $\\alpha_t$.\n\nNote that $p_\\theta$ predicts the true state $x_0$, based on the current state $x_t$, some components of which might be masked, hence we called this framework *learning to unmask*. Note also that $p_\\theta$ doesn't take $t$ as an argument, which we call the *time-agnostic* property following [@Zheng+2025].\n\nThis 'learning to unmask' task might be a very hard task when $t$ is near $1$, since most of the $x_t^i$'s are still masked then. \n\nFor instance, if we choose a linear schedule\n$$\n\\al_t=1-t\\qquad(t\\in[0,1])\n$$\nthe scaler $\\frac{\\dot{\\al}_t}{1-\\al_t}=-t^{-1}$ before the expectation puts less weight on large $t\\approx1$, while puts much more weight on small $t\\approx0$, where most of the $x_t^i$'s should be already unmasked.\n\nHence, selecting the schedule $\\al_t$ to make learning easier can be very effective, for example by reducing the variance of gradient estimator in a SGD algorithm. Actually, this is a part of the technique how [@Arriola+2025] achieved their remarkable result.\n\nThis is why the loss ([-@eq-L]) is considered as a potential competitor against the current dominant autoregressive models. In fact, one work [@Chao+2025], still under review, claimed their masked diffusion model surpassed the autoregressive model on the task of language modeling, achieving an evaluation perplexity of 15.36 on the OpenWebText dataset.^[In the context of language modeling, the perplexity is defined as $2^{l}$ where $l$ is the average log-likelihood of the test set.]\n\nWe briefly discuss their trick and related promising techniques to improve the model, before programming a demo in @sec-Demo to deepen our understanding in absorbing forward process.\n\n### A Gibbs Sampler Take\n\nOne problem about the loss ([-@eq-L]) is that $p_\\theta$ predicts the unmasked complete sequence in a product form:\n$$\np_\\theta(x_0|x_t)=\\prod_{i=1}^d p_\\theta(x_0^i|x_t).\n$$\n\nThis should cause no problem when unmasking one component at a time, since it will be a form of ancestral sampling based on disintegration property.\n\nHowever, when unmasking two or more components at once, for example when the number of steps is less than $d$, the product form assumption simply introduces a bias, as the data distribution is by no means of product form on $E^d$.\n\nHere, to recover correctness asymptotically, analogy with Gibbs sampling becomes very important.\n\nFor example, predictor-corrector technique can be readily employed to mitigate this bias, as discussed in [@Lezama+2023], [@Zhao+2024], [@Zhao+2024Unified], [@Gat+2024], [@Wang+2025].\n\n### Intermediate States\n\nAs we mentioned earlier, unmasking can be a very hard task, as closely investigated in [@Kim+2025 Section 3].\n\nTo alleviate this problem, [@Chao+2025] introduced intermediate states by re-encoding the token in a base-$2$ encoding, such as\n$$\n5\\mapsto 101.\n$$\nThe right-hand side needs three steps to be completely unmasked, while the left-hand side only needs one jump.\n\nTherefore, unmasking can be easier to learn, compared to the original token encoding.\n\nHowever, this is not the only advantage of intermediate states. [@Chao+2025] were able to construct a full predictor $p_\\theta$ without the product form assumption on each token. This approach might act as a block Gibbs sampler and make the convergence faster.\n\n### State-Dependent Rate\n\nAs a function of $t\\mapsto\\alpha_t$, different choices for $\\al_t$ seem to make little impact on the total performance of the model, as we observe in our toy example in @sec-exp.\n\nHowever, if we allow $\\al_t$ to depend on the state as in [@Shi+2024 Section 6], I believe the masked diffusion model will start to show its real potential over currently dominant autoregressive framework.\n\nThe problem arises when one tries to learn $\\al_t$ at the same time, for example, by including a term into the loss ([-@eq-L]). This will lead to amplified variance of the gradient estimate and unstable training, as reported in [@Shi+2024] and [@Arriola+2025].\n\n## Demo {#sec-Demo}\n\nWe carry out unconditional generation from a toy data distribution $\\pi_{\\text{data}}$ on $5=\\{0,1,2,3,4\\}$, by running the exact reverse kernel of the absorbing (masked) forward process. Therefore, no neural network training is involved.\n\n### Setup\n\n::: {#56a7955c .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\n```\n:::\n\n\n::: {#3df99cf1 .cell execution_count=2}\n``` {.python .cell-code}\np_data = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\n```\n:::\n\n\nWe will represent the MASK as $-1$. The state space is then $E:=5 \\cup \\{-1\\}$.\n\n::: {#7522584d .cell execution_count=3}\n``` {.python .cell-code}\nMASK = -1\n```\n:::\n\n\nAn important design choice in the forward process is the noising schedule $\\alpha_t$, which can be interpreted as *survival probability* and satisfy the following relatioship with the jump intensity $\\beta_t$:\n$$\n\\alpha_t=\\exp\\paren{-\\int^t_0\\beta_s\\,ds},\\qquad t\\in[0,1].\n$$\n\nLet us keep it simple and set $\\alpha_t=t$. To achive this, we need to set\n$$\n\\beta_t=\\frac{1}{1-t},\\qquad t\\in[0,1),\n$$\nwhich is clearly diverging as $t\\to1$. This is to ensure the process to converge in finite time.\n\n::: {#a071294f .cell execution_count=4}\n``` {.python .cell-code}\nT = 10  # number of steps\nalpha = np.linspace(1.00, 0.00, T+1)\n```\n:::\n\n\n### The Backward Transition Kernel\n\nIn this setting, the backward transition kernel $p(x_{t-1} | x_t)$ satisfies\n$$\n\\P[X_{t-1}=-1|X_t=-1]=\\frac{1 - \\alpha_{t-1}}{1 - \\alpha_t}.\n$$\nIn the other cases, the unmasked values $x_{t-1}$ should be determined according to $\\pi_{\\text{data}}$, which is unavailable in a real setting, of course.\n\n::: {#a6eebab8 .cell execution_count=5}\n``` {.python .cell-code}\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])  # length T\n```\n:::\n\n\n::: {#1fb7da41 .cell execution_count=6}\n``` {.python .cell-code}\ndef reverse_sample(num_samples: int, p_unmask: np.ndarray):\n    \"\"\"\n    Start from x_T = MASK for all samples, apply the exact reverse transitions down to t=0.\n    Returns x_0 samples in 5 = {0,1,...,4}.\n    \"\"\"\n    x_t = np.full(num_samples, MASK, dtype=int)\n    hist = np.empty((T+1, num_samples), dtype=int)\n    hist[0] = x_t.copy()\n    for t in range(T, 0, -1):\n        idx_mask = np.where(x_t == MASK)[0]  # masked indices\n        if idx_mask.size > 0:\n            u = rng.random(idx_mask.size)\n            unmask_now = idx_mask[u < p_unmask[t-1]]  # indices that are going to be unmasked\n            if unmask_now.size > 0:\n                cats = rng.choice(5, size=unmask_now.size, p=p_data)\n                x_t[unmask_now] = cats\n        hist[T-t+1] = x_t.copy()\n\n    # At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,\n    # but numerically we ensure no MASK remains:\n    assert np.all(x_t != MASK), \"Some samples remained MASK at t=0, which should not happen.\"\n    return x_t, hist\n```\n:::\n\n\n#### A Note on Alternative Sampling Strategies\n\nNote that we need not to obey this exact backward transition kernel to sample from the data distribution.\n\nFor example, remasking [@Lezama+2023], [@Zhao+2024], [@Gat+2024], [@Wang+2025], a form of predictor-corrector sampling, can be incorporated to improve sample quality, mitigating numerical errors.\n\nRecently, sampling time path planning [@Peng+2025], [@Liu+2025], [@Kim+2025], [@Rout+2025] has been proposed to improve sample quality and model log-likelihood.\n\n### Honest Sampling\n\n::: {#c847fd29 .cell execution_count=7}\n``` {.python .cell-code}\nN = 100_000  # size of sample to get\nx0_samples, hist = reverse_sample(N, p_unmask)\n```\n:::\n\n\nWe first make sure our implementation is correct by checking the empirical distribution of the samples agrees with the true distribution.\n\n::: {#3bb67a51 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\ncounts = np.bincount(x0_samples, minlength=5).astype(float)\np_emp = counts / counts.sum()\n\nprint(\"Toy data marginal p_data:\", p_data.round(4))\nprint(\"Empirical p after reverse sampling:\", p_emp.round(4))\n\n# ---------- Bar chart: p_data vs empirical ----------\nxs = np.arange(5)\nwidth = 0.4\nplt.figure(figsize=(6,3))\nplt.bar(xs - width/2, p_data, width=width, label=\"true p_data\")\nplt.bar(xs + width/2, p_emp, width=width, label=\"empirical (reverse)\")\nplt.title(\"Reverse samples match the data marginal\")\nplt.xlabel(\"category id\")\nplt.ylabel(\"probability\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToy data marginal p_data: [0.4  0.3  0.18 0.1  0.02]\nEmpirical p after reverse sampling: [0.3979 0.3002 0.1828 0.0992 0.0198]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-9-output-2.png){width=566 height=278}\n:::\n:::\n\n\nMaking sure everything is working, we plot 1000 sample paths from the reverse process.\n\n::: {#0a820935 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nn_samples_to_plot = min(1000, hist.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist.shape[0]), hist[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-10-output-1.png){width=587 height=449}\n:::\n:::\n\n\nWe see a relatively equal number of jumps per step:\n\n::: {#c061a856 .cell execution_count=10}\n``` {.python .cell-code}\njump_counts = np.zeros(T)\nfor i in range(10):\n    jump_counts[i] = sum(hist[i] != hist[i+1])\nprint(jump_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 9916. 10017. 10001. 10143. 10062. 10195.  9815. 10005.  9859.  9987.]\n```\n:::\n:::\n\n\nThis is because we set $\\alpha_t=1-t$ to be linear.\n\n::: {#75590bee .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# ---------- Plot schedule α_t (survival probability) ----------\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-12-output-1.png){width=470 height=277}\n:::\n:::\n\n\n### Choice of $\\alpha_t$ {#sec-exp}\n\n$\\alpha_t$ controls the convergence rate of the forward process.\n\nWe change $\\al_t$ to see the impact on the sampling accuracy. (There should be no influence as long as the exact backward kernel is used.)\n\nLet us change $\\al_t$ to be an exponential schedule:\n\n::: {#8766f8b6 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nalpha_exp = np.exp(np.linspace(0.00, -10.00, T+1))\np_unmask_exp = (alpha_exp[:-1] - alpha_exp[1:]) / (1.0 - alpha_exp[1:])\n\nplt.figure(figsize=(5,3))\nplt.plot(range(T+1), alpha_exp, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-13-output-1.png){width=470 height=277}\n:::\n:::\n\n\n::: {#7fdd0e03 .cell execution_count=13}\n``` {.python .cell-code}\nx0_exp, hist_exp = reverse_sample(N, p_unmask_exp)\n```\n:::\n\n\n::: {#9b11e66d .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nn_samples_to_plot = min(1000, hist_exp.shape[1])\nplt.figure()\n\nfor i in range(n_samples_to_plot):\n    plt.plot(range(hist_exp.shape[0]), hist_exp[:, i], alpha=0.5, linewidth=0.8)\n\nplt.xlabel('Time step')\nplt.ylabel('State')\nplt.title(f'Sample trajectories (first {n_samples_to_plot} samples)')\nplt.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-15-output-1.png){width=587 height=449}\n:::\n:::\n\n\nWe see many jumps happen in the very last steps. This is certainly not what we want.\n\nWe spend almost half of the computational time (up to 6th step) in simulating the phantom jumps which just do not happen. The same concern was raised by [@Chao+2025].\n\nHowever, the accuracy is same, as the exact kernel is used to simulate, if the computational cost might be different.\n\n::: {#498fd13f .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\ndef calc_l1_kl(x0_samples, split = 10):\n    chunks = np.array_split(x0_samples, split)\n    counts = np.array([np.bincount(chunk, minlength=5).astype(float) for chunk in chunks])\n\n    p_emp = counts / counts.sum(axis=1)[0]\n    l1 = np.abs(p_emp - p_data).sum(axis=1).mean()\n    l1_var = np.abs(p_emp - p_data).sum(axis=1).var()\n    kl = (np.where(p_emp > 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).mean()\n    kl_var = (np.where(p_emp > 0, p_emp * np.log(p_emp / p_data), 0)).sum(axis=1).var()\n    return l1, l1_var, kl, kl_var\n\nl1, l1_var, kl, kl_var = calc_l1_kl(x0_samples)\nprint(\"Linear Schedule: L1 distance:\", round(l1, 6), \" ± \", round(l1_var, 6), \"   KL(p_emp || p_data):\", round(kl, 6), \" ± \", round(kl_var, 6))\n\nl1_exp, l1_exp_var, kl_exp, kl_exp_var = calc_l1_kl(x0_exp)\nprint(\"Exponential Schedule: L1 distance:\", round(l1_exp, 6), \" ± \", round(l1_exp_var, 6), \"   KL(p_emp || p_data):\", round(kl_exp, 6), \" ± \", round(kl_exp_var, 6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Schedule: L1 distance: 0.0147  ±  3e-05    KL(p_emp || p_data): 0.000196  ±  0.0\nExponential Schedule: L1 distance: 0.0148  ±  4.9e-05    KL(p_emp || p_data): 0.000232  ±  0.0\n```\n:::\n:::\n\n\n## 2D Example\n\n### Setup\n\nWe consider a highly correlated distribution, whose support is degenerated on the diagonal element on $5^2$.\n\n::: {#ff67849e .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\nK = 5\nMASK = -1\n\n# Base marginal for a single site\np_single = np.array([0.40, 0.30, 0.18, 0.10, 0.02], dtype=float)\np_single /= p_single.sum()\n\n# Build correlated joint with same-parity constraint\nW = np.zeros((K, K), dtype=float)\nfor i in range(K):\n    for j in range(K):\n        if (i % 2) == (j % 2):\n            W[i, j] = p_single[i] * p_single[j]\npi_joint = W / W.sum()\npi_x = pi_joint.sum(axis=1)\npi_y = pi_joint.sum(axis=0)\n\n# Conditionals\ncond_x_given_y = np.zeros((K, K), dtype=float)  # [j, i]\ncond_y_given_x = np.zeros((K, K), dtype=float)  # [i, j]\nfor j in range(K):\n    col = pi_joint[:, j]; s = col.sum()\n    if s > 0:\n        cond_x_given_y[j, :] = col / s\nfor i in range(K):\n    row = pi_joint[i, :]; s = row.sum()\n    if s > 0:\n        cond_y_given_x[i, :] = row / s\n\nfig = plt.figure(figsize=(8, 3.4))\n\n# Heatmap\nax1 = plt.subplot(1, 2, 1)\nim = ax1.imshow(pi_joint, cmap='viridis', aspect='equal')\nax1.set_xlabel('Y')\nax1.set_ylabel('X')\nax1.set_title('Joint Probability Distribution (Heatmap)')\nax1.set_xticks(range(K))\nax1.set_yticks(range(K))\n\n# Value annotation\nfor i in range(K):\n    for j in range(K):\n        ax1.text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\n\nplt.colorbar(im, ax=ax1)\n\n# 3D bar plot\nax2 = plt.subplot(1, 2, 2, projection='3d')\nx = np.arange(K)\ny = np.arange(K)\nX, Y = np.meshgrid(x, y)\nZ = pi_joint\n\nax2.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), \n         0.8, 0.8, Z.ravel(), alpha=0.8, cmap='viridis')\n\nax2.set_xlabel('Y')\nax2.set_ylabel('X')\nax2.set_zlabel('Probability')\nax2.set_title('Joint Probability Distribution (3D)')\nax2.set_xticks(x)\nax2.set_yticks(y)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-17-output-1.png){width=717 height=334}\n:::\n:::\n\n\n### The Backward Transition Kernel {#sec-exact-kernel-2d}\n\nWe will first consider, again, linear schedule:\n\n::: {#82898030 .cell execution_count=17}\n``` {.python .cell-code}\nT = 10\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n```\n:::\n\n\n::: {#ff9a105b .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(5, 3))\nplt.plot(range(T+1), alpha, marker=\"o\")\nplt.title(r\"Survival probability $\\alpha_t$\")\nplt.xlabel(\"t\")\nplt.ylabel(r\"$\\alpha_t$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-19-output-1.png){width=470 height=277}\n:::\n:::\n\n\n::: {#16fb0098 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code (definition of reverse_sample_pairs)\"}\ndef reverse_sample_pairs(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = pi_joint.ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n```\n:::\n\n\n### Honest Sampling\n\n::: {#8260db8e .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-21-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#55c09a30 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\ndef l1_kl(pairs, split=10):\n    chunks = np.array_split(pairs, split)\n    l1, kl = [], []\n    for chunk in chunks:\n        counts = np.zeros((K, K), dtype=float)\n        for a, b in chunk:\n            counts[a, b] += 1.0\n        pi_emp = counts / counts.sum()\n        \n        eps = 1e-12\n        l1.append(np.abs(pi_emp - pi_joint).sum())\n        nz = (pi_emp > 0) & (pi_joint > 0)\n        kl.append((pi_emp[nz] * np.log((pi_emp[nz] + eps) / pi_joint[nz])).sum())\n    return l1, kl\n\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.024162  ±  4.6e-05    KL(emp || true): 7.286359e-04 ± 7.462192e-08\n```\n:::\n:::\n\n\nThis is a gradual unmasking time schedule.\n\n::: {#b12759d7 .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nnew_unmasks_per_step = []\nfor t in range(T):\n    changed1 = (h1[t] == MASK) & (h1[t+1] != MASK)\n    changed2 = (h2[t] == MASK) & (h2[t+1] != MASK)\n    new_unmasks_per_step.append(changed1.sum() + changed2.sum())\n\nplt.figure(figsize=(6, 3))\nplt.plot(range(1, T+1), new_unmasks_per_step, marker=\"o\")\nplt.title(\"Newly unmasked coordinates per step\")\nplt.xlabel(\"reverse step (t→t-1)\")\nplt.ylabel(\"#coords\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-23-output-1.png){width=566 height=278}\n:::\n:::\n\n\n#### Larger Step Size\n\nWhat if we employ a large step size?\n\nActually, the result doesn't change. Moreover, the accuracy is higher, since it is equivalent to direct sampling from $\\pi_{\\text{data}}$.\n\n::: {#bb9e4685 .cell execution_count=23}\n``` {.python .cell-code}\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n```\n:::\n\n\n::: {#ecc25d5c .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\n# Run\nN = 100_000\npairs, h1, h2 = reverse_sample_pairs(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-25-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#0ae8b4e7 .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.021245  ±  5e-05    KL(emp || true): 6.510835e-04 ± 1.258177e-07\n```\n:::\n:::\n\n\n### Coordinate-wise Sampling\n\nThe joint distribution is unavailable, even if the learning based on the loss ([-@eq-L]) has been done perfectly, because of the product form assumption on the neural network predictor $p_\\theta$.^[Of course, the exact sampling would have been available, for exmple, if we learned the backward intensity as [@Campbell+2022]. However, these methods have been marginalized due to suboptimal performance.]\n\nWe mock this situation by replacing the joint distribution in the exact kernel in @sec-exact-kernel-2d with a product of its marginals.\n\n::: {#c396511d .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code (definition of reverse_sample_incorrect)\"}\ndef reverse_sample_incorrect(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n```\n:::\n\n\n::: {#f96d75a8 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-28-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#f26e57ce .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.089135  ±  1.2e-05    KL(emp || true): -4.100716e-02 ± 6.515707e-06\n```\n:::\n:::\n\n\nThere is a small deviation from the exact kernel, where $\\ell^1$ distance is $0.024\\pm0.00005$.\n\nWe can easily see, in the empirical distribution, some cells are assinged with positive mass, although there is $0$ probability for them to be sampled.\n\nThis effect becomes smaller when the number of steps `T` is large. For example, setting `T=1000` gives us with almost same accuracy as the exact kernel.\n\n#### Larger Step Size\n\nThe situation gets worse when the step size is large.\n\n::: {#c7975e07 .cell execution_count=29}\n``` {.python .cell-code code-fold=\"true\"}\nT = 1  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_incorrect(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-30-output-1.png){width=740 height=321}\n:::\n:::\n\n\n::: {#b1d7f339 .cell execution_count=30}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.85124  ±  4.4e-05    KL(emp || true): -2.884951e-01 ± 1.114368e-05\n```\n:::\n:::\n\n\nThe error is now significant, because the incorrect product kernel is used every time, as we set $T=1$ meaning unmasking in just one step!\n\n### Corrector Sampling\n\nOne remedy is to utilise Gibbs sampling ideas to correct the bias, at the expense of computational cost.\n\nTo do this, we must identify a Markov kernel that keeps the marginal distribution of $X_t$ invariant for every timestep $t\\in[0,1]$.\n\nIn our case, we simply add a remasking step, only to those pairs $(x_t^1,x_t^2)$'s which are completely unmasked. As they have some probability of being unmasked simultaneously, re-masking only one of them, this time $x_t^1$, will allow us a second chance to correctly arrive at a correct pair $(x_t^{1,\\text{corrected}},x_t^2)$.\n\n::: {#5272b46d .cell execution_count=31}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code (definition of reverse_sample_correct)\"}\ndef reverse_sample_corrector(num_samples: int, p_unmask: np.ndarray, T: int):\n    x1 = np.full(num_samples, MASK, dtype=int)\n    x2 = np.full(num_samples, MASK, dtype=int)\n    hist1 = np.empty((T + 1, num_samples), dtype=int); hist1[0] = x1\n    hist2 = np.empty((T + 1, num_samples), dtype=int); hist2[0] = x2\n\n    for t in range(T, 0, -1):\n        p = p_unmask[t-1]\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        # corrector step\n        q = 1.0 - p  # masking probability\n        both = (x1 != MASK) & (x2 != MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            will = rng.random(idx.size) < q\n            idx_now = idx[will]\n            if idx_now.size > 0:\n                x1[idx_now] = MASK\n        # predictor step\n\n        # both masked\n        both = (x1 == MASK) & (x2 == MASK)\n        idx = np.where(both)[0]\n        if idx.size > 0:\n            um1 = rng.random(idx.size) < p\n            um2 = rng.random(idx.size) < p\n\n            idx_both = idx[um1 & um2]\n            if idx_both.size > 0:\n                flat = np.outer(pi_x, pi_y).ravel()\n                choices = rng.choice(K*K, size=idx_both.size, p=flat)\n                xs = choices // K; ys = choices % K\n                x1[idx_both] = xs; x2[idx_both] = ys\n\n            idx_only1 = idx[um1 & (~um2)]\n            if idx_only1.size > 0:\n                x1[idx_only1] = rng.choice(K, size=idx_only1.size, p=pi_x)\n\n            idx_only2 = idx[(~um1) & um2]\n            if idx_only2.size > 0:\n                x2[idx_only2] = rng.choice(K, size=idx_only2.size, p=pi_y)\n\n        # x1 masked, x2 revealed\n        idx_b1 = np.where((x1 == MASK) & (x2 != MASK))[0]\n        if idx_b1.size > 0:\n            will = rng.random(idx_b1.size) < p\n            idx_now = idx_b1[will]\n            if idx_now.size > 0:\n                y_vals = x2[idx_now]\n                for val in np.unique(y_vals):\n                    m = (y_vals == val); n = m.sum()\n                    x1[idx_now[m]] = rng.choice(K, size=n, p=cond_x_given_y[val, :])\n\n        # x2 masked, x1 revealed\n        idx_b2 = np.where((x2 == MASK) & (x1 != MASK))[0]\n        if idx_b2.size > 0:\n            will = rng.random(idx_b2.size) < p\n            idx_now = idx_b2[will]\n            if idx_now.size > 0:\n                x_vals = x1[idx_now]\n                for val in np.unique(x_vals):\n                    m = (x_vals == val); n = m.sum()\n                    x2[idx_now[m]] = rng.choice(K, size=n, p=cond_y_given_x[val, :])\n\n        hist1[T - t + 1] = x1; hist2[T - t + 1] = x2\n\n    assert np.all(x1 != MASK) and np.all(x2 != MASK)\n    return np.stack([x1, x2], axis=1), hist1, hist2\n```\n:::\n\n\n::: {#b7559fe8 .cell execution_count=32}\n``` {.python .cell-code code-fold=\"true\"}\nT = 10  # Number of steps\nalpha = np.linspace(1.0, 0.0, T + 1)\np_unmask = (alpha[:-1] - alpha[1:]) / (1.0 - alpha[1:])\np_unmask = np.clip(p_unmask, 0.0, 1.0)\n\npairs, h1, h2 = reverse_sample_corrector(N, p_unmask, T)\n\n# Empirical joint\ncounts = np.zeros((K, K), dtype=float)\nfor a, b in pairs:\n    counts[a, b] += 1.0\npi_emp = counts / counts.sum()\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.4))\nim0 = ax[0].imshow(pi_joint, origin=\"lower\", aspect=\"equal\")\nax[0].set_title(\"True joint π_data\")\nax[0].set_xlabel(\"x2\"); ax[0].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[0].text(j, i, f'{pi_joint[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04)\n\nim1 = ax[1].imshow(pi_emp, origin=\"lower\", aspect=\"equal\")\nax[1].set_title(\"Empirical joint (reverse)\")\nax[1].set_xlabel(\"x2\"); ax[1].set_ylabel(\"x1\")\nfor i in range(K):\n    for j in range(K):\n        ax[1].text(j, i, f'{pi_emp[i, j]:.3f}', \n                ha='center', va='center', color='white', fontsize=8)\nfig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](CTDDM_files/figure-html/cell-33-output-1.png){width=736 height=321}\n:::\n:::\n\n\n::: {#21756e52 .cell execution_count=33}\n``` {.python .cell-code code-fold=\"true\"}\nl1, kl = l1_kl(pairs, 10)\n\nprint(\"L1 distance:\", round(np.mean(l1), 6), \" ± \", round(np.var(l1), 6), \"   KL(emp || true):\", f\"{np.mean(kl):.6e} ± {np.var(kl):.6e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nL1 distance: 0.022518  ±  2e-05    KL(emp || true): 4.694469e-05 ± 8.500292e-08\n```\n:::\n:::\n\n\nWe see a small improvement from the $\\ell^1$ distance of $0.0241$.\n\n## Future Works\n\nSince the absorbing process is favoured only because of its time-agnostic property, its ability should be explained separately with the properties of the process.\n\n### A Reinforcenment Learning Take\n\nThe inference step and the sampling step should be decoupled, at least conceptually.\n\nTo tune the forward process noising schedule $\\alpha_t(x)$, a reinforcement learning framework will be employed, I believe in the near future.\n\nThis is a variant of meta-learning and, in this way, the unmasking network $p_\\theta$ will be able to efficiently learn the correct dependence structure in the data domain.\n\nFor example, in language modeling, there is a natural sequential structure, which is partly why autoregressive modeling has been dominant in this domain. However, by learning $\\alpha_t$ in masking process, a much more efficent factorization over texts can be aquired in a data-driven way.\n\nI even think this $\\alpha_t$ can play an important role just as a word embedding does currently.\n\nIn the sampling step, a sampling time path planning will greatly enhance sample quality, just as Monte Carlo tree search does in reinforcement learning.\n\nAs a conclusion, the flexible framework of masked diffusion models will enable a marriage with reinforcement learning and meta learning, which will be a way to overcome the autoregressive modeling framework, because the latter imposes unnecessary sequential inductive bias into the model.\n\n### A Scaling Analysis\n\n### Another Scaling Analysis\n\n## 関連記事 {.appendix}\n\n::: {#diffusion-listing}\n:::\n\n",
    "supporting": [
      "CTDDM_files"
    ],
    "filters": [],
    "includes": {}
  }
}