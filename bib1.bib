@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
