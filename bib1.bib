@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
@inproceedings{Zhao+2024,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
	  booksubtitle    = {International Conference on Machine Learning},
      url={https://arxiv.org/abs/2404.17546}, 
}
@article{Teh+2003,
author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
title = {Energy-based models for sparse overcomplete representations},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1235â€“1260},
numpages = {26},
url = {https://www.jmlr.org/papers/v4/teh03a.html}
}

@InProceedings{Xie+2016,
  title = 	 {A Theory of Generative ConvNet},
  author = 	 {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2635--2644},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/xiec16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/xiec16.html},
  abstract = 	 {We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.}
}
