@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
@inproceedings{Zhao+2024,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
	  booksubtitle    = {International Conference on Machine Learning},
      url={https://arxiv.org/abs/2404.17546}, 
}
@article{Teh+2003,
author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
title = {Energy-based models for sparse overcomplete representations},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1235–1260},
numpages = {26},
url = {https://www.jmlr.org/papers/v4/teh03a.html}
}


@article{Teh+2006,
	author = {Yee Whye Teh, Michael I Jordan, Matthew J Beal and David M Blei},
	doi = {10.1198/016214506000000302},
	eprint = {https://doi.org/10.1198/016214506000000302},
	journal = {Journal of the American Statistical Association},
	number = {476},
	pages = {1566--1581},
	publisher = {Taylor \& Francis},
	title = {Hierarchical Dirichlet Processes},
	url = {https://doi.org/10.1198/016214506000000302},
	volume = {101},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/016214506000000302}}

@inproceedings{Wallach+2009,
author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
title = {Evaluation methods for topic models},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553515},
doi = {10.1145/1553374.1553515},
abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1105–1112},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@InProceedings{Xie+2016,
  title = 	 {A Theory of Generative ConvNet},
  author = 	 {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2635--2644},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/xiec16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/xiec16.html},
  abstract = 	 {We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.}
}
@inproceedings{He+2019,
      title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders}, 
      author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
      year={2019},
      booksubtitle    = {International Conference on Learning Representations},
      url={https://arxiv.org/abs/1901.05534}, 
}
@inproceedings{Hinton-Teh2001,
author = {Hinton, Geoffrey E. and Teh, Yee-Whye},
title = {Discovering multiple constraints that are frequently approximately satisfied},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {227–234},
numpages = {8},
location = {Seattle, Washington},
series = {UAI'01},
url             = {https://dl.acm.org/doi/abs/10.5555/2074022.2074051},
}
@misc{Finn+2016,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03852}, 
}
@inproceedings{Swersku+2011,
author = {Swersky, Kevin and Ranzato, Marc'Aurelio and Buchman, David and Marlin, Benjamin M. and Freitas, Nandode},
title = {On autoencoders and score matching for energy based models},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider estimation methods for the class of continuous-data energy based models (EBMs). Our main result shows that estimating the parameters of an EBM using score matching when the conditional distribution over the visible units is Gaussian corresponds to training a particular form of regularized autoencoder. We show how different Gaussian EBMs lead to different autoencoder architectures, providing deep links between these two families of models. We compare the score matching estimator for the mPoT model, a particular Gaussian EBM, to several other training methods on a variety of tasks including image denoising and unsupervised feature extraction. We show that the regularization function induced by score matching leads to superior classification performance relative to a standard autoencoder. We also show that score matching yields classification results that are indistinguishable from better-known stochastic approximation maximum likelihood estimators.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {1201–1208},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104633},
}

@inproceedings{Koster-Hyvarinen2007,
	abstract = {Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Hyv{\"a}rinen, Aapo},
	booktitle = {Artificial Neural Networks -- ICANN 2007},
	editor = {de S{\'a}, Joaquim Marques and Alexandre, Lu{\'\i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
	isbn = {978-3-540-74695-9},
	pages = {798--807},
	publisher = {Springer Berlin Heidelberg},
	title = {A Two-Layer ICA-Like Model Estimated by Score Matching},
	year = {2007}}

@inproceedings{Koster+2009,
	abstract = {Markov Random Field (MRF) models with potentials learned from the data have recently received attention for learning the low-level structure of natural images. A MRF provides a principled model for whole images, unlike ICA, which can in practice be estimated for small patches only. However, learning the filters in an MRF paradigm has been problematic in the past since it required computationally expensive Monte Carlo methods. Here, we show how MRF potentials can be estimated using Score Matching (SM). With this estimation method we can learn filters of size 12 {\texttimes}12 pixels, considerably larger than traditional ''hand-crafted'' MRF potentials. We analyze the tuning properties of the filters in comparison to ICA filters, and show that the optimal MRF potentials are similar to the filters from an overcomplete ICA model.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Lindgren, Jussi T. and Hyv{\"a}rinen, Aapo},
	booktitle = {Independent Component Analysis and Signal Separation},
	editor = {Adali, T{\"u}lay and Jutten, Christian and Romano, Jo{\~a}o Marcos Travassos and Barros, Allan Kardec},
	isbn = {978-3-642-00599-2},
	pages = {515--522},
	publisher = {Springer Berlin Heidelberg},
	title = {Estimating Markov Random Field Potentials for Natural Images},
	year = {2009}}

@InProceedings{Gutmann-Hyvarinen2010,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract = 	 {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}

@InProceedings{Perpinan-Hinton2005,
  title = 	 {On Contrastive Divergence Learning},
  author =       {Carreira-Perpi{\~n}\'an, Miguel \'A. and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {33--40},
  year = 	 {2005},
  editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
  volume = 	 {R5},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--08 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf},
  url = 	 {https://proceedings.mlr.press/r5/carreira-perpinan05a.html},
  note =         {Reissued by PMLR on 30 March 2021.}
}

@inproceedings{Kingma-LeCun2010,
	author = {Durk P Kingma and Yann LeCun},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
	publisher = {Curran Associates, Inc.},
	title = {Regularized estimation of image statistics by Score Matching},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},
	volume = {23},
	year = {2010},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf}}
@ARTICLE{Drucker-LeCun1992,
  author={Drucker, H. and Le Cun, Y.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Improving generalization performance using double backpropagation}, 
  year={1992},
  volume={3},
  number={6},
  pages={991-997},
  keywords={Testing;Backpropagation algorithms;Jacobian matrices;Neurons;Signal to noise ratio;Neural networks},
  doi={10.1109/72.165600}}
@inproceedings{Song+2019,
      title={{Sliced Score Matching: A Scalable Approach to Density and Score Estimation}},
      author={Yang Song and Sahaj Garg and Jiaxin Shi and Stefano Ermon},
      year={2019},
      booksubtitle    = {Uncertainty in Artificial Intelligence},
      url={https://arxiv.org/abs/1905.07088}, 
}
@ARTICLE{Hyvarinen2007,
  author={Hyvarinen, Aapo},
  journal={IEEE Transactions on Neural Networks}, 
  title={Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables}, 
  year={2007},
  volume={18},
  number={5},
  pages={1529-1531},
  keywords={Samarium;Statistical analysis;Probability density function;Parameter estimation;Monte Carlo methods;Computer science;Information technology;Normalization constant;partition function;statistical estimation},
  doi={10.1109/TNN.2007.895819}}
@inproceedings{Lyu2009,
author = {Lyu, Siwei},
title = {Interpretation and generalization of score matching},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {359–366},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@inproceedings{Gutmann-Hirayama2011,
author = {Gutmann, Michael U. and Hirayama, Jun-ichiro},
title = {Bregman divergence as general framework to estimate unnormalized statistical models},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in un-supervised learning.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {283–290},
numpages = {8},
location = {Barcelona, Spain},
series = {UAI'11}
}

@InProceedings{Chwialkowski+2016,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}

@InProceedings{Liu+2016,
  title = 	 {A Kernelized Stein Discrepancy for Goodness-of-fit Tests},
  author = 	 {Liu, Qiang and Lee, Jason and Jordan, Michael},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {276--284},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/liub16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/liub16.html},
  abstract = 	 {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.}
}
@misc{McAllester2023,
      title={On the Mathematics of Diffusion Models}, 
      author={David McAllester},
      year={2023},
      eprint={2301.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.11108}, 
}
@article{Yang+2023,
author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626235},
doi = {10.1145/3626235},
abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github:},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {39},
keywords = {stochastic differential equations, score-based generative models, diffusion models, Generative models}
}
@unpublished{Kreis+2022,
    author = {Karsten Kreis and Ruiqi Gao and Arash Vahdat},
    year   = {2022},
    title  = {Denoising Diffusion-based Generative Modeling: Foundations and Applications},
    url    = {https://cvpr2022-tutorial-diffusion-models.github.io/},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@unpublished{Song+2023Tutorial,
    author = {Jiaming Song and Chenlin Meng and Arash Vahdat},
    year   = {2023},
    title  = {Denoising Diffusion Models: A Generative Learning Big Bang},
    url    = {https://cvpr.thecvf.com/virtual/2023/tutorial/18546},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@INPROCEEDINGS{Choi+2022,
  author={Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Perception Prioritized Training of Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={11462-11471},
  keywords={Training;Visualization;Computational modeling;Noise reduction;Data models;Image restoration;Pattern recognition;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01118}}
@inproceedings{Kingma+2021,
author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
title = {Variational diffusion models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1660},
numpages = {12},
series = {NIPS '21},
url             = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
}
@inproceedings{Salimans-Ho2021,
title={Should {EBM}s model the energy or the score?},
author={Tim Salimans and Jonathan Ho},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
url={https://openreview.net/forum?id=9AS-TF2jRNb}
}

@article{Anderson1982,
	abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.},
	author = {Brian D.O. Anderson},
	doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	number = {3},
	pages = {313-326},
	title = {Reverse-time diffusion equation models},
	url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	volume = {12},
	year = {1982},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	bdsk-url-2 = {https://doi.org/10.1016/0304-4149(82)90051-5}}

@article{Haussmann-Pardoux1986,
	author = {U. G. Haussmann and E. Pardoux},
	doi = {10.1214/aop/1176992362},
	journal = {The Annals of Probability},
	keywords = {diffusion process, Kolmogorov equation, Markov process, Martingale problem, Time reversal},
	number = {4},
	pages = {1188 -- 1205},
	publisher = {Institute of Mathematical Statistics},
	title = {{Time Reversal of Diffusions}},
	url = {https://doi.org/10.1214/aop/1176992362},
	volume = {14},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176992362}}
@inproceedings{Karras+2022,
title={Elucidating the Design Space of Diffusion-Based Generative Models},
author={Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=k7FuTOWMOc7}
}
@ARTICLE {Croitoru+2023,
author = {F. Croitoru and V. Hondru and R. Ionescu and M. Shah},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Diffusion Models in Vision: A Survey},
year = {2023},
volume = {45},
number = {09},
issn = {1939-3539},
pages = {10850-10869},
abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
keywords = {computational modeling;mathematical models;noise reduction;data models;computer vision;training;task analysis},
doi = {10.1109/TPAMI.2023.3261988},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2023.3261988},
}

@ARTICLE{Cao+2024,
  author={Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Generative Diffusion Models}, 
  year={2024},
  volume={36},
  number={7},
  pages={2814-2830},
  keywords={Mathematical models;Kernel;Computational modeling;Training;Surveys;Noise reduction;Markov processes;Diffusion model;deep generative model;diffusion algorithm;diffusion applications},
  doi={10.1109/TKDE.2024.3361474}}
@inproceedings{JiamingSong+2021,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}
@inproceedings{Gao+2021,
title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P Kingma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v_1Soh8QUNc}
}
@inproceedings{Xiao+2021,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JprM0p-q0Co}
}
@inproceedings{Salimans-Ho2022,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}
@inproceedings{Vahdat+2021,
	author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {11287--11302},
	publisher = {Curran Associates, Inc.},
	title = {{Score-based Generative Modeling in Latent Space}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf}}

@article{Pandey+2022,
title={Diffuse{VAE}: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents},
author={Kushagra Pandey and Avideep Mukherjee and Piyush Rai and Abhishek Kumar},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ygoNPRiLxw},
note={}
}

@InProceedings{Nichol-Dhariwal2021,
  title = 	 {{Improved Denoising Diffusion Probabilistic Models}},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}
@inproceedings{Ho-Salimans2021,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}
@article{Ho+2022,
  author  = {Jonathan Ho and Chitwan Saharia and William Chan and David J. Fleet and Mohammad Norouzi and Tim Salimans},
  title   = {Cascaded Diffusion Models for High Fidelity Image Generation},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {47},
  pages   = {1--33},
  url     = {http://jmlr.org/papers/v23/21-0635.html}
}
@misc{Saharia+2022SIGGRAPH,
title={{Palette: Image-to-Image Diffusion Models}},
author={Chitwan Saharia and William Chan and Huiwen Chang and Chris A. Lee and Jonathan Ho and Tim Salimans and David J. Fleet and Mohammad Norouzi},
year={2022},
url={https://openreview.net/forum?id=FPGs276lUeq}
}
@ARTICLE{Saharia+2023,
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Image Super-Resolution via Iterative Refinement}, 
  year={2023},
  volume={45},
  number={4},
  pages={4713-4726},
  keywords={Noise reduction;Superresolution;Task analysis;Iterative methods;Data models;Faces;Diffusion processes;Image super-resolution;diffusion models;deep generative models},
  doi={10.1109/TPAMI.2022.3204461}}

@inproceedings{Austin+2021,
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17981--17993},
	publisher = {Curran Associates, Inc.},
	title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf}}
@InProceedings{Chang+2022,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@article{Heng+2024,
	author = {Jeremy Heng and Valentin De Bortoli and Arnaud Doucet},
	doi = {10.1214/23-STS908},
	journal = {Statistical Science},
	keywords = {Optimal transport, Schr{\"o}dinger bridge, score matching, Stochastic differential equation, Time reversal},
	number = {1},
	pages = {90 -- 99},
	publisher = {Institute of Mathematical Statistics},
	title = {{Diffusion Schr{\"o}dinger Bridges for Bayesian Computation}},
	url = {https://doi.org/10.1214/23-STS908},
	volume = {39},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1214/23-STS908}}
@inproceedings{Chung+2023,
title={Diffusion Posterior Sampling for General Noisy Inverse Problems},
author={Hyungjin Chung and Jeongsol Kim and Michael Thompson Mccann and Marc Louis Klasky and Jong Chul Ye},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=OnD9zGAGT0k}
}

@InProceedings{Song+2023,
  title = 	 {Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation},
  author =       {Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32483--32498},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23k/song23k.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23k.html},
  abstract = 	 {We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion (LGD), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a Monte Carlo method that uses multiple samples from a suitable distribution to reduce bias. Our method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and controllable motion synthesis. Notably, we show how our method can be applied to control a pretrained motion diffusion model to follow certain paths and avoid obstacles that are proven challenging to prior methods.}
}

@InProceedings{Shi+2022,
  title = 	 {Conditional simulation using diffusion {S}chr{ö}dinger bridges},
  author =       {Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1792--1802},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/shi22a/shi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/shi22a.html},
  abstract = 	 {Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr{ö}dinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schrödinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.}
}

@inproceedings{DeBortoli+2021,
	author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17695--17709},
	publisher = {Curran Associates, Inc.},
	title = {Diffusion Schr\"{o}dinger Bridge with Applications to Score-Based Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf}}

@InProceedings{Kurras2015,
  title = 	 {{Symmetric Iterative Proportional Fitting}},
  author = 	 {Kurras, Sven},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {526--534},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/kurras15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/kurras15.html},
  abstract = 	 {Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix W*. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W=W’ and r=c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of W* in terms of expansion properties of the undirected weighted graph represented by W. Finally, we show how our results contribute to recent work in machine learning.}
}
@article{Sinkhorn1967,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2314570},
 author = {Richard Sinkhorn},
 journal = {The American Mathematical Monthly},
 number = {4},
 pages = {402--405},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Diagonal Equivalence to Matrices with Prescribed Row and Column Sums},
 urldate = {2024-08-04},
 volume = {74},
 year = {1967}
}

@article{Sinkhorn-Knopp1967,
	author = {Richard Sinkhorn and Paul Knopp},
	journal = {Pacific Journal of Mathematics},
	number = {2},
	pages = {343 -- 348},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{Concerning Nonnegative Matrices and Doubly Stochastic Matrices}},
	volume = {21},
	year = {1967},
  url             = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-21/issue-2/Concerning-nonnegative-matrices-and-doubly-stochastic-matrices/pjm/1102992505.full},
}

@article{Fortet1940,
    author          = {Robert Fortet},
    year            = {1940},
    title           = {{Résolution d'un système d'équations de M. Schrödinger}},
    journal         = {Journal de Mathématiques Pures et Appliquées, Series 9},
    volume          = {19},
    number          = {1-4},
    pages           = {83-105},
    url             = {http://www.numdam.org/item/JMPA_1940_9_19_1-4_83_0/}
}
@article{Deming-Stephan1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235722},
 author = {W. Edwards Deming and Frederick F. Stephan},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {427--444},
 publisher = {Institute of Mathematical Statistics},
 title = {On a Least Squares Adjustment of a Sampled Frequency Table When the Expected Marginal Totals are Known},
 urldate = {2024-08-04},
 volume = {11},
 year = {1940}
}
@article{Kullback1968,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239692},
 author = {S. Kullback},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {1236--1243},
 publisher = {Institute of Mathematical Statistics},
 title = {Probability Densities with Given Marginals},
 urldate = {2024-08-04},
 volume = {39},
 year = {1968}
}
@article{Ireland-Kullback1968,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334462},
 abstract = {In its simplest formulation the problem considered is to estimate the cell probabilities pij of an r × c contingency table for which the marginal probabilities $p_{i\ldot}$ and $p_{\ldot j}$ are known and fixed, so as to minimize ΣΣpijln (pij/πij), where πij are the corresponding entries in a given contingency table. An iterative procedure is given for determining the estimates and it is shown that the estimates are BAN, and that the iterative procedure is convergent. A summary of results for a four-way contingency table is given. An illustrative example is given.},
 author = {C. T. Ireland and S. Kullback},
 journal = {Biometrika},
 number = {1},
 pages = {179--188},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Contingency Tables with Given Marginals},
 urldate = {2024-08-04},
 volume = {55},
 year = {1968}
}
@inproceedings{Vargas-Grathwohl-Doucet2023,
title={{Denoising Diffusion Samplers}},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@inproceedings{Follmer1985,
	abstract = {We introduce an entropy technique which allows to treat some infinite-dimensional extensions of the classical duality equations for the time reversal of diffusion processes.},
	address = {Berlin, Heidelberg},
	author = {F{\"o}llmer, H.},
	booktitle = {Stochastic Differential Systems Filtering and Control},
	editor = {Metivier, M. and Pardoux, E.},
	isbn = {978-3-540-39253-8},
	pages = {156--163},
	publisher = {Springer Berlin Heidelberg},
	title = {An entropy approach to the time reversal of diffusion processes},
	year = {1985}}

@InProceedings{Barr+2020,
  title = 	 {Quantum Ground States from Reinforcement Learning},
  author =       {Barr, Ariel and Gispen, Willem and Lamacraft, Austen},
  booktitle = 	 {Proceedings of The First Mathematical and Scientific Machine Learning Conference},
  pages = 	 {635--653},
  year = 	 {2020},
  editor = 	 {Lu, Jianfeng and Ward, Rachel},
  volume = 	 {107},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v107/barr20a/barr20a.pdf},
  url = 	 {https://proceedings.mlr.press/v107/barr20a.html},
  abstract = 	 {  Finding the ground state of a quantum mechanical system can be formulated as an optimal control problem. In this formulation, the drift of the optimally controlled process is chosen to match the distribution of paths in the Feynman–Kac (FK) representation of the solution of the imaginary time Schrödinger equation. This provides a variational principle that can be used for reinforcement learning of a neural representation of the drift. Our approach is a drop-in replacement for path integral Monte Carlo, learning an optimal importance sampler for the FK trajectories. We demonstrate the applicability of our approach to several problems of one-, two-, and many-particle physics.}
}
@inproceedings{Zhang+2021,
title={Sampling via Controlled Stochastic Dynamical Systems},
author={Benjamin Zhang and Tuhin Sahai and Youssef Marzouk},
booktitle={I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop},
year={2021},
url={https://openreview.net/forum?id=dHruzYDH719}
}
@article{DeBortoli2022,
title={Convergence of denoising diffusion models under the manifold hypothesis},
author={Valentin De Bortoli},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=MhK5aXo3gB},
note={Expert Certification}
}
@misc{Montanari-Wu2024,
      title={Posterior Sampling from the Spiked Models via Diffusion Processes}, 
      author={Andrea Montanari and Yuchen Wu},
      year={2023},
      eprint={2304.11449},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2304.11449}, 
}


@article{Crooks1998,
	abstract = {An equality has recently been shown relating the free energy difference between two equilibrium ensembles of a system and an ensemble average of the work required to switch between these two configurations. In the present paper it is shown that this result can be derived under the assumption that the system's dynamics is Markovian and microscopically reversible.},
	author = {Crooks, Gavin E. },
	date = {1998/03/01},
	date-added = {2024-08-05 13:48:11 +0900},
	date-modified = {2024-08-05 13:48:11 +0900},
	doi = {10.1023/A:1023208217925},
	id = {Crooks1998},
	isbn = {1572-9613},
	journal = {Journal of Statistical Physics},
	number = {5},
	pages = {1481--1487},
	title = {{Nonequilibrium Measurements of Free Energy Differences for Microscopically Reversible Markovian Systems}},
	url = {https://doi.org/10.1023/A:1023208217925},
	volume = {90},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1023208217925}}
@article{Jarzynski1997Equality,
  title = {{Nonequilibrium Equality for Free Energy Differences}},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. Lett.},
  volume = {78},
  issue = {14},
  pages = {2690--2693},
  numpages = {0},
  year = {1997},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.78.2690},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.78.2690}
}
@article{Jarzynski1997MasterEquation,
  title = {Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. E},
  volume = {56},
  issue = {5},
  pages = {5018--5035},
  numpages = {0},
  year = {1997},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.56.5018},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.56.5018}
}
@inproceedings{Doucet+2022,
title={Score-Based Diffusion meets Annealed Importance Sampling},
author={Arnaud Doucet and Will Sussman Grathwohl and Alexander G. D. G. Matthews and Heiko Strathmann},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=9cU2iW3bz0}
}

@inproceedings{Wu+2020,
	author = {Wu, Hao and K\"{o}hler, Jonas and Noe, Frank},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {5933--5944},
	publisher = {Curran Associates, Inc.},
	title = {{Stochastic Normalizing Flows}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf}}

@InProceedings{Thin+2021,
  title = 	 {Monte Carlo Variational Auto-Encoders},
  author =       {Thin, Achille and Kotelevskii, Nikita and Doucet, Arnaud and Durmus, Alain and Moulines, Eric and Panov, Maxim},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10247--10257},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/thin21a/thin21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/thin21a.html},
  abstract = 	 {Variational auto-encoders (VAE) are popular deep latent variable models which are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter ELBO and hence better variational approximations, it has been proposed to use importance sampling to get a lower variance estimate of the evidence. However, importance sampling is known to perform poorly in high dimensions. While it has been suggested many times in the literature to use more sophisticated algorithms such as Annealed Importance Sampling (AIS) and its Sequential Importance Sampling (SIS) extensions, the potential benefits brought by these advanced techniques have never been realized for VAE: the AIS estimate cannot be easily differentiated, while SIS requires the specification of carefully chosen backward Markov kernels. In this paper, we address both issues and demonstrate the performance of the resulting Monte Carlo VAEs on a variety of applications.}
}
@article{Tang+2024,
    author          = {Weipin Tang and Yuhang Wu and Xunyu Zhou},
    year            = {2024},
    title           = {{Discrete-Time Simulated Annealing: A Convergence Analysis via the Eyring-Kramers Law}},
    journal         = {Numerical Algebra, Control and Optimization},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://doi.org/10.3934/naco.2024015}
}

@article{Fournier-Tardif2021,
	abstract = {Using a localization procedure and the result of Holley-Kusuoka-Stroock [8] in the torus, we widely weaken the usual growth assumptions concerning the success of the continuous-time simulated annealing in Rd. Our only assumption is the existence of an invariant probability measure for a sufficiently low temperature. We also prove, in an appendix, a non-explosion criterion for a class of time-inhomogeneous diffusions.},
	author = {Nicolas Fournier and Camille Tardif},
	doi = {https://doi.org/10.1016/j.jfa.2021.109086},
	issn = {0022-1236},
	journal = {Journal of Functional Analysis},
	keywords = {Simulated annealing, Time-inhomogeneous diffusion processes, Large time behavior, Non-explosion},
	number = {5},
	pages = {109086},
	title = {On the simulated annealing in Rd},
	url = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	volume = {281},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	bdsk-url-2 = {https://doi.org/10.1016/j.jfa.2021.109086}}

@inproceedings{Chen+2018,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Ordinary Differential Equations},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf}}
@inproceedings{Grathwohl+2019,
title={{Scalable Reversible Generative Models with Free-form Continuous Dynamics}},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJxgknCcK7},
}
@article{Gortler+2019,
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  title = {A Visual Exploration of Gaussian Processes},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  doi = {10.23915/distill.00017}
}
@article{Tipping2001,
    author          = {Michael E. Tipping},
    year            = {2001},
    title           = {Sparse Bayesian Learning and the Relevance Vector Machine},
    journal         = {Journal of Machine Learning Research},
    volume          = {1},
    number          = {},
    pages           = {211-244},
    url             = {https://www.jmlr.org/papers/v1/tipping01a.html}
}
@INPROCEEDINGS{Loeliger+2016,
  author={Loeliger, Hans-Andrea and Bruderer, Lukas and Malmberg, Hampus and Wadehn, Federico and Zalmai, Nour},
  booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
  title={On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  keywords={Computational modeling;Covariance matrices;Message passing;Smoothing methods;Maximum likelihood estimation;Kalman filters},
  doi={10.1109/ITA.2016.7888168}}
@phdthesis{Gibbs1997,
    author      = {M. N. Gibbs},
    school      = {Cambridge University},
    title       = {Bayesian Gaussian Process Regression and Classification},
    year        = {1997},
    url             = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b5a0c62c8d7cf51137bfb079947b8393c00ed169},
}

@InProceedings{Heinonen+2016,
  title = 	 {Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo},
  author = 	 {Heinonen, Markus and Mannerström, Henrik and Rousu, Juho and Kaski, Samuel and Lähdesmäki, Harri},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {732--740},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/heinonen16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/heinonen16.html},
  abstract = 	 {We present a novel approach for non-stationary Gaussian process regression (GPR), where the three key parameters – noise variance, signal variance and lengthscale – can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. For inferring the full posterior distribution we use Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with the gradients. The MAP solution can also be learned with gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the non-stationary GPR is shown to give major improvement when modeling realistic input-dependent dynamics.}
}
@mastersthesis{Krige1951,
    author  = {D. G. Krige},
    school  = {University of the Witwatersrand, Faculty of Engineering},
    title   = {A Statistical Approach to Some Mine Valuation and Allied Problems on the Witwatersrand},
    year    = {1951},
    url             = {http://hdl.handle.net/10539/17975},
}

@inproceedings{Remes+2017,
	author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Non-Stationary Spectral Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf}}

@article{Kriege+2020,
	abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.},
	author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
	date = {2020/01/14},
	date-added = {2024-08-08 14:27:51 +0900},
	date-modified = {2024-08-08 14:27:51 +0900},
	doi = {10.1007/s41109-019-0195-3},
	id = {Kriege2020},
	isbn = {2364-8228},
	journal = {Applied Network Science},
	number = {1},
	pages = {6},
	title = {A survey on graph kernels},
	url = {https://doi.org/10.1007/s41109-019-0195-3},
	volume = {5},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s41109-019-0195-3}}
@article{Lodhi+2002,
    author          = {Huma Lodhi and Craig Saunders and John Shawe-Taylor and Nello Cristianini and Chris Watkins},
    year            = {2002},
    title           = {Text Classification using String Kernels},
    journal         = {Journal of Machine Learning Research},
    volume          = {2},
    number          = {},
    pages           = {419-444},
    url             = {https://www.jmlr.org/papers/v2/lodhi02a.html}
}

@inproceedings{Borgwardt+2006,
	author = {Borgwardt, Karsten and Schraudolph, Nicol and Vishwanathan, S.v.n.},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
	publisher = {MIT Press},
	title = {Fast Computation of Graph Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf},
	volume = {19},
	year = {2006},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf}}
@article{Shervashidze+2011,
  author  = {Nino Shervashidze and Pascal Schweitzer and Erik Jan van Leeuwen and Kurt Mehlhorn and Karsten M. Borgwardt},
  title   = {Weisfeiler-Lehman Graph Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {77},
  pages   = {2539-2561},
  url     = {http://jmlr.org/papers/v12/shervashidze11a.html}
}

@InProceedings{Wilson-Adams2013,
  title = 	 {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  author = 	 {Wilson, Andrew and Adams, Ryan},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1067--1075},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wilson13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wilson13.html},
  abstract = 	 {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}
@inproceedings{Sutherland-Schneider2015,
author = {Sutherland, Danica J. and Schneider, Jeff},
title = {On the error of random fourier features},
year = {2015},
isbn = {9780996643108},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Kernel methods give powerful, flexible, and theoretically grounded approaches to solving many problems in machine learning. The standard approach, however, requires pairwise evaluations of a kernel function, which can lead to scalability issues for very large datasets. Rahimi and Recht (2007) suggested a popular approach to handling this problem, known as random Fourier features. The quality of this approximation, however, is not well understood. We improve the uniform error bound of that paper, as well as giving novel understandings of the embedding's variance, approximation error, and use in some machine learning methods. We also point out that surprisingly, of the two main variants of those features, the more widely used is strictly higher-variance for the Gaussian kernel and has worse bounds.},
booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
pages = {862–871},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {UAI'15}
}


@inproceedings{Rahimi-Recht2007,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Random Features for Large-Scale Kernel Machines},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}}

@inproceedings{Rahimi-Recht2008,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf}}

@inproceedings{Yu+2016,
	author = {Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Orthogonal Random Features},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf}}
@article{Kimeldorf-Wahba1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239347},
 author = {George S. Kimeldorf and Grace Wahba},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {495--502},
 publisher = {Institute of Mathematical Statistics},
 title = {A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines},
 urldate = {2024-08-08},
 volume = {41},
 year = {1970}
}

@inproceedings{Scholkopf+2001,
	abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
	address = {Berlin, Heidelberg},
	author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
	booktitle = {Computational Learning Theory},
	editor = {Helmbold, David and Williamson, Bob},
	isbn = {978-3-540-44581-4},
	pages = {416--426},
	publisher = {Springer Berlin Heidelberg},
	title = {A Generalized Representer Theorem},
	year = {2001}}
@article{Wilkinson+2023,
  author  = {William J. Wilkinson and Simo Särkkä and Arno Solin},
  title   = {Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {83},
  pages   = {1--50},
  url     = {http://jmlr.org/papers/v24/21-1298.html}
}
@inproceedings{Wenzel+2019,
    author          = {Florian Wenzel and Théo Galy-Fajou and Christan Donner and Marius Kolft and Manfred Opper},
    year            = {2019},
    title           = {Efficient Gaussian Process Classification Using Pólya-Gamma Data Augmentation},
    booktitle       = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume          = {33},
    pages           = {},
    url             = {},
    doi             = {10.1609/aaai.v33i01.33015417},
}

@InProceedings{Galy-Fajou2020,
  title = 	 {Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models},
  author =       {Galy-Fajou, Theo and Wenzel, Florian and Opper, Manfred},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3025--3035},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/galy-fajou20a/galy-fajou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/galy-fajou20a.html},
  abstract = 	 {We propose automated augmented conjugate inference, a new inference method for non-conjugate Gaussian processes (GP) models.Our method automatically constructs an auxiliary variable augmentation that renders the GP model conditionally conjugate. Building on the conjugate structure of the augmented model, we develop two inference methods. First, a fast and scalable stochastic variational inference method that uses efficient block coordinate ascent updates, which are computed in closed form. Second, an asymptotically correct Gibbs sampler that is useful for small datasets.Our experiments show that our method is up two orders of magnitude faster and more robust than existing state-of-the-art black-box methods.}
}
@ARTICLE{Liu+2020,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  keywords={Kernel;Scalability;Sparse representation;Complexity theory;Computational modeling;Ground penetrating radar;Predictive models;Big data;Gaussian process regression (GPR);local approximations;scalability;sparse approximations},
  doi={10.1109/TNNLS.2019.2957109}}

@InProceedings{Wilson+2020,
  title = 	 {Efficiently sampling functions from {G}aussian process posteriors},
  author =       {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10292--10302},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wilson20a/wilson20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/wilson20a.html},
  abstract = 	 {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes’ statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.}
}
@INPROCEEDINGS{Hartikainen-Sarkka2010,
  author={Hartikainen, Jouni and Särkkä, Simo},
  booktitle={2010 IEEE International Workshop on Machine Learning for Signal Processing}, 
  title={Kalman filtering and smoothing solutions to temporal Gaussian process regression models}, 
  year={2010},
  volume={},
  number={},
  pages={379-384},
  keywords={Kalman filters;Approximation methods;Mathematical model;Markov processes;Computational modeling;Gaussian processes;Equations},
  doi={10.1109/MLSP.2010.5589113}}

@inproceedings{Snelson-Ghahramani2005,
	author = {Snelson, Edward and Ghahramani, Zoubin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Sparse Gaussian Processes using Pseudo-inputs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf}}

@inproceedings{Wilson+2014,
	author = {Wilson, Andrew G and Gilboa, Elad and Nehorai, Arye and Cunningham, John P},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Fast Kernel Learning for Multidimensional Pattern Extrapolation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf}}

@inproceedings{Salakhutdinov-Hinton2007,
	author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf}}

@InProceedings{Ober+2021,
  title = 	 {The promises and pitfalls of deep kernel learning},
  author =       {Ober, Sebastian W. and Rasmussen, Carl E. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1206--1216},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/ober21a/ober21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/ober21a.html},
  abstract = 	 {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.}
}
@inproceedings{Novak+2019,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}
@misc{Yang2020,
      title={Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation}, 
      author={Greg Yang},
      year={2020},
      eprint={1902.04760},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1902.04760}, 
}

@InProceedings{Damianou-Lawrence2013,
  title = 	 {Deep {G}aussian Processes},
  author = 	 {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {207--215},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/damianou13a.pdf},
  url = 	 {https://proceedings.mlr.press/v31/damianou13a.html},
  abstract = 	 {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@article{Carvalho+2010,
    author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
    title = "{The horseshoe estimator for sparse signals}",
    journal = {Biometrika},
    volume = {97},
    number = {2},
    pages = {465-480},
    year = {2010},
    month = {04},
    abstract = "{This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator’s advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator’s tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asq017},
    url = {https://doi.org/10.1093/biomet/asq017},
    eprint = {https://academic.oup.com/biomet/article-pdf/97/2/465/584621/asq017.pdf},
}


@inproceedings{Jacot+2018,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}}

@InProceedings{Woodworth+2020,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
  abstract = 	 { A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e.&nbsp;when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by \citet{chizat2018note}, we show how the \textbf{\textit{scale of the initialization}} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-$D$ linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the \emph{width}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
}

@InProceedings{Yang-Hu2021,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}

@inproceedings{Chizat+2019,
	author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On Lazy Training in Differentiable Programming},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}}
@ARTICLE{Sarkka+2013,
  author={Särkkä, Simo and Solin, Arno and Hartikainen, Jouni},
  journal={IEEE Signal Processing Magazine}, 
  title={Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering}, 
  year={2013},
  volume={30},
  number={4},
  pages={51-61},
  keywords={Machine learning;Learning systems;Gaussian processes;Bayes methods;Parametric statistics;Linear regression analysis;Kalman filters;Smoothing methods;Spatiotemporal phenomena;Kernel},
  doi={10.1109/MSP.2013.2246292}}

@InProceedings{Adam+2020,
  title = 	 {Doubly Sparse Variational Gaussian Processes},
  author =       {Adam, Vincent and Eleftheriadis, Stefanos and Artemev, Artem and Durrande, Nicolas and Hensman, James},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2874--2884},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/adam20a/adam20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/adam20a.html},
  abstract = 	 {The use of Gaussian process models is typically limited to datasets with a few tens of thousands of observations due to their complexity and memory footprint.The two most commonly used methods to overcome this limitation are 1) the variational sparse approximation which relies on inducing points and 2) the state-space equivalent formulation of Gaussian processes which can be seen as exploiting some sparsity in the precision matrix.In this work, we propose to take the best of both worlds: we show that the inducing point framework is still valid for state space models and that it can bring further computational and memory savings. Furthermore, we provide the natural gradient formulation for the proposed variational parameterisation.Finally, this work makes it possible to use the state-space formulation inside deep Gaussian process models as illustrated in one of the experiments. }
}
@article{Jona-Lasinio+2012,
 ISSN = {19326157, 19417330},
 URL = {http://www.jstor.org/stable/41713483},
 abstract = {Directional data arise in various contexts such as oceanography (wave directions) and meteorology (wind directions), as well as with measurements on a periodic scale (weekdays, hours, etc.). Our contribution is to introduce a model-based approach to handle periodic data in the case of measurements taken at spatial locations, anticipating structured dependence between these measurements. We formulate a wrapped Gaussian spatial process model for this setting, induced from a customary linear Gaussian process. We build a hierarchical model to handle this situation and show that the fitting of such a model is possible using standard Markov chain Monte Carlo methods. Our approach enables spatial interpolation (and can accommodate measurement error). We illustrate with a set of wave direction data from the Adriatic coast of Italy, generated through a complex computer model.},
 author = {Giovanna Jona-Lasinio and Alan Gelfand and Mattia Jona-Lasinio},
 journal = {The Annals of Applied Statistics},
 number = {4},
 pages = {1478--1498},
 publisher = {Institute of Mathematical Statistics},
 title = {SPATIAL ANALYSIS OF WAVE DIRECTION DATA USING WRAPPED GAUSSIAN PROCESSES},
 urldate = {2024-08-08},
 volume = {6},
 year = {2012}
}
@ARTICLE{Jacobs+1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@article{Emerson+2023,
	abstract = {Where do firms innovate? Mapping their locations and directions in technological space is challenging due to its high dimensionality. We propose a new method to characterize firms' inventive activities via topological data analysis (TDA) that represents high-dimensional data in a shape graph. Applying this method to 333 major firms' patents in 1976--2005 reveals hitherto undocumented industry dynamics: some firms remain undifferentiated; others develop unique portfolios. Firms with unique trajectories, which we define and measure graph-theoretically as ``flares'' in the Mapper graph, tend to perform better. This association is statistically and economically significant, and continues to hold after we control for portfolio size, firm survivorship, and industry classification.},
	author = {Emerson G. Escolar and Yasuaki Hiraoka and Mitsuru Igami and Yasin Ozcan},
	doi = {https://doi.org/10.1016/j.respol.2023.104821},
	issn = {0048-7333},
	journal = {Research Policy},
	keywords = {Innovation, Mapper, Patents, R&D, Topological data analysis},
	number = {8},
	pages = {104821},
	title = {Mapping firms' locations in technological space: A topological analysis of patent statistics},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	volume = {52},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	bdsk-url-2 = {https://doi.org/10.1016/j.respol.2023.104821}}
@inproceedings{Singh+2007
,
booktitle = {Eurographics Symposium on Point-Based Graphics
},
editor = {M. Botsch and R. Pajarola and B. Chen and M. Zwicker
},
title = {{Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition
}},
author = {Singh, Gurjeet and 
Memoli, Facundo and 
Carlsson, Gunnar
},
year = {2007
},
publisher = {The Eurographics Association
},
ISSN = {1811-7813
},
ISBN = {978-3-905673-51-7
},
DOI = {/10.2312/SPBG/SPBG07/091-100
}
}
@phdthesis{Roberts1963,
    author      = {Lawrence G. Roberts},
    school      = {Massachusetts Institute of Technology},
    title       = {Machine Perception of Three-Dimensional Solids},
    year        = {1963},
    url             = {http://hdl.handle.net/1721.1/11589},
}

@article{Lee-Mumford2003,
	abstract = {Traditional views of visual processing suggest that early visual neurons in areas V1 and V2 are static spatiotemporal filters that extract local features from a visual scene. The extracted information is then channeled through a feedforward chain of modules in successively higher visual areas for further analysis. Recent electrophysiological recordings from early visual neurons in awake behaving monkeys reveal that there are many levels of complexity in the information processing of the early visual cortex, as seen in the long-latency responses of its neurons. These new findings suggest that activity in the early visual cortex is tightly coupled and highly interactive with the rest of the visual system. They lead us to propose a new theoretical setting based on the mathematical framework of hierarchical Bayesian inference for reasoning about the visual system. In this framework, the recurrent feedforward/feedback loops in the cortex serve to integrate top-down contextual priors and bottom-up observations so as to implement concurrent probabilistic inference along the visual hierarchy. We suggest that the algorithms of particle filtering and Bayesian-belief propagation might model these interactive cortical computations. We review some recent neurophysiological evidences that support the plausibility of these ideas.},
	author = {Tai Sing Lee and David Mumford},
	doi = {10.1364/JOSAA.20.001434},
	journal = {J. Opt. Soc. Am. A},
	keywords = {Vision modeling ; Edge detection; Information processing; Machine vision; Neural networks; Physiology; Spatial resolution},
	month = {Jul},
	number = {7},
	pages = {1434--1448},
	publisher = {Optica Publishing Group},
	title = {Hierarchical Bayesian inference in the visual cortex},
	url = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	volume = {20},
	year = {2003},
	bdsk-url-1 = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	bdsk-url-2 = {https://doi.org/10.1364/JOSAA.20.001434}}

@article{Lake+2015,
	abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms---for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	author = {Brenden M. Lake and Ruslan Salakhutdinov and Joshua B. Tenenbaum},
	doi = {10.1126/science.aab3050},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aab3050},
	journal = {Science},
	number = {6266},
	pages = {1332-1338},
	title = {Human-level concept learning through probabilistic program induction},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	volume = {350},
	year = {2015},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	bdsk-url-2 = {https://doi.org/10.1126/science.aab3050}}
@inproceedings{Donahue+2017,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Kr{\"a}henb{\"u}hl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJtNZAFgg}
}
@inproceedings{Bao+2022,
title={{BE}iT: {BERT} Pre-Training of Image Transformers},
author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{Kivva+2021,
	author = {Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {18087--18101},
	publisher = {Curran Associates, Inc.},
	title = {Learning latent causal graphs via mixture oracles},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf}}
@inproceedings{Kivva+2022,
title={Identifiability of deep generative models under mixture priors without auxiliary information},
author={Bohdan Kivva and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022},
url={https://openreview.net/forum?id=UeG3kt_Ebg2}
}

@InProceedings{Lopez+2024,
  title = 	 {Toward the Identifiability of Comparative Deep Generative Models},
  author =       {Lopez, Romain and Huetter, Jan-Christian and Hajiramezanali, Ehsan and Pritchard, Jonathan K and Regev, Aviv},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {868--912},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/lopez24a/lopez24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/lopez24a.html},
  abstract = 	 {Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice.  Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network).  We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.}
}
@article{Locatello+2020,
  author  = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Raetsch and Sylvain Gelly and Bernhard Sch{{\"o}}lkopf and Olivier Bachem},
  title   = {A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {209},
  pages   = {1--62},
  url     = {http://jmlr.org/papers/v21/19-976.html}
}
@inproceedings{Ding+2021,
title={Cc{\{}GAN{\}}: Continuous Conditional Generative Adversarial Networks for Image Generation},
author={Xin Ding and Yongwei Wang and Zuheng Xu and William J Welch and Z. Jane Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PrzjugOsDeE}
}
@inproceedings{Hoogeboom+2021,
title={Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=6nbpPqUCIi7}
}
@misc{Simo2024,
  author={Simo Ryu},
  title={Minimal Implementation of a D3PM (Structured Denoising Diffusion Models in Discrete State-Spaces), in pytorch},
  year={2024},
  url             = {https://github.com/cloneofsimo/d3pm},
}

@inproceedings{Kingma+2014,
	author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {{Semi-supervised Learning with Deep Generative Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf}}
@inproceedings{Chen+2023AnalogBits,
title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},
author={Ting Chen and Ruixiang ZHANG and Geoffrey Hinton},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3itjR9QxFw}
}

@article{Watson+2023,
	abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods1--9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence--structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
	date = {2023/08/01},
	date-added = {2024-08-10 21:39:03 +0900},
	date-modified = {2024-08-10 21:39:03 +0900},
	doi = {10.1038/s41586-023-06415-8},
	id = {Watson2023},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7976},
	pages = {1089--1100},
	title = {De novo design of protein structure and function with RFdiffusion},
	url = {https://doi.org/10.1038/s41586-023-06415-8},
	volume = {620},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-023-06415-8}}
@article{Abramson+2024,
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	date = {2024/06/01},
	date-added = {2024-08-10 21:52:55 +0900},
	date-modified = {2024-08-10 21:52:55 +0900},
	doi = {10.1038/s41586-024-07487-w},
	id = {Abramson2024},
	isbn = {1476-4687},
	journal = {Nature},
	number = {8016},
	pages = {493--500},
	title = {Accurate structure prediction of biomolecular interactions with AlphaFold 3},
	url = {https://doi.org/10.1038/s41586-024-07487-w},
	volume = {630},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-024-07487-w}}
@article{Krishna+2024,
	abstract = {Deep-learning methods have revolutionized protein structure prediction and design but are presently limited to protein-only systems. We describe RoseTTAFold All-Atom (RFAA), which combines a residue-based representation of amino acids and DNA bases with an atomic representation of all other groups to model assemblies that contain proteins, nucleic acids, small molecules, metals, and covalent modifications, given their sequences and chemical structures. By fine-tuning on denoising tasks, we developed RFdiffusion All-Atom (RFdiffusionAA), which builds protein structures around small molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we designed and experimentally validated, through crystallography and binding measurements, proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and the light-harvesting molecule bilin. Advances in machine learning have made protein structure prediction and design much more accurate and accessible in recent years, but these tools have generally been limited to polypeptide chains. However, ligands such as small molecules, metal ions, and nucleic acids are crucial components of most proteins, both in terms of structure and biological function. Krishna et al. present a next-generation protein structure prediction and design tool, RoseTTAFold All-Atom, that can accept a wide range of ligands and covalent amino acid modifications. The authors demonstrate superior performance on protein-ligand structure prediction relative to other tools, even in the absence of an input experimental structure. They also perform de novo design of proteins to bind cofactors and small molecules and experimentally validate these designs. ---Michael A. Funk},
	author = {Rohith Krishna and Jue Wang and Woody Ahern and Pascal Sturmfels and Preetham Venkatesh and Indrek Kalvet and Gyu Rie Lee and Felix S. Morey-Burrows and Ivan Anishchenko and Ian R. Humphreys and Ryan McHugh and Dionne Vafeados and Xinting Li and George A. Sutherland and Andrew Hitchcock and C. Neil Hunter and Alex Kang and Evans Brackenbrough and Asim K. Bera and Minkyung Baek and Frank DiMaio and David Baker},
	doi = {10.1126/science.adl2528},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.adl2528},
	journal = {Science},
	number = {6693},
	pages = {eadl2528},
	title = {Generalized biomolecular modeling and design with RoseTTAFold All-Atom},
	url = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	volume = {384},
	year = {2024},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	bdsk-url-2 = {https://doi.org/10.1126/science.adl2528}}

@article{Zheng+2024,
	abstract = {Advances in deep learning have greatly improved structure prediction of molecules. However, many macroscopic observations that are important for real-world applications are not functions of a single molecular structure but rather determined from the equilibrium distribution of structures. Conventional methods for obtaining these distributions, such as molecular dynamics simulation, are computationally expensive and often intractable. Here we introduce a deep learning framework, called Distributional Graphormer (DiG), in an attempt to predict the equilibrium distribution of molecular systems. Inspired by the annealing process in thermodynamics, DiG uses deep neural networks to transform a simple distribution towards the equilibrium distribution, conditioned on a descriptor of a molecular system such as a chemical graph or a protein sequence. This framework enables the efficient generation of diverse conformations and provides estimations of state densities, orders of magnitude faster than conventional methods. We demonstrate applications of DiG on several molecular tasks, including protein conformation sampling, ligand structure sampling, catalyst--adsorbate sampling and property-guided structure generation. DiG presents a substantial advancement in methodology for statistically understanding molecular systems, opening up new research opportunities in the molecular sciences.},
	author = {Zheng, Shuxin and He, Jiyan and Liu, Chang and Shi, Yu and Lu, Ziheng and Feng, Weitao and Ju, Fusong and Wang, Jiaxi and Zhu, Jianwei and Min, Yaosen and Zhang, He and Tang, Shidi and Hao, Hongxia and Jin, Peiran and Chen, Chi and No{\'e}, Frank and Liu, Haiguang and Liu, Tie-Yan},
	date = {2024/05/01},
	date-added = {2024-08-10 22:01:13 +0900},
	date-modified = {2024-08-10 22:01:13 +0900},
	doi = {10.1038/s42256-024-00837-3},
	id = {Zheng2024},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {5},
	pages = {558--567},
	title = {Predicting equilibrium distributions for molecular systems with deep learning},
	url = {https://doi.org/10.1038/s42256-024-00837-3},
	volume = {6},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-024-00837-3}}
@unpublished{Duvenaud2014,
    author = {David Kristjanson Duvenaud},
    year   = {2014},
    title  = {The Kernel Cookbook: Advice on Covariance functions},
    url    = {https://www.cs.toronto.edu/~duvenaud/cookbook/}
}
@ARTICLE{Linsker1988,
  author={Linsker, R.},
  journal={Computer}, 
  title={{Self-Organization in a Perceptual Network}}, 
  year={1988},
  volume={21},
  number={3},
  pages={105-117},
  keywords={Intelligent networks;Biological information theory;Circuits;Biology computing;Animal structures;Neuroscience;Genetics;System testing;Neural networks;Constraint theory},
  doi={10.1109/2.36}}

@article{Torgenson1952,
	abstract = {Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.},
	author = {Torgerson, Warren S. },
	date = {1952/12/01},
	date-added = {2024-08-10 23:19:09 +0900},
	date-modified = {2024-08-10 23:19:09 +0900},
	doi = {10.1007/BF02288916},
	id = {Torgerson1952},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {401--419},
	title = {Multidimensional scaling: I. Theory and method},
	url = {https://doi.org/10.1007/BF02288916},
	volume = {17},
	year = {1952},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288916}}

@article{Kruskal1964,
	abstract = {Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.},
	author = {Kruskal, J.  B. },
	date = {1964/03/01},
	date-added = {2024-08-10 23:20:08 +0900},
	date-modified = {2024-08-10 23:20:08 +0900},
	doi = {10.1007/BF02289565},
	id = {Kruskal1964},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {1--27},
	title = {Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
	url = {https://doi.org/10.1007/BF02289565},
	volume = {29},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289565}}
@article{Poole-Rosenthal1985,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2111172},
 abstract = {A general nonlinear logit model is used to analyze political choice data. The model assumes probabilistic voting based on a spatial utility function. The parameters of the utility function and the spatial coordinates of the choices and the choosers can all be estimated on the basis of observed choices. Ordinary Guttman scaling is a degenerate case of this model. Estimation of the model is implemented in the NOMINATE program for one dimensional analysis of two alternative choices with no nonvoting. The robustness and face validity of the program outputs are evaluated on the basis of roll call voting data for the U.S. House and Senate.},
 author = {Keith T. Poole and Howard Rosenthal},
 journal = {American Journal of Political Science},
 number = {2},
 pages = {357--384},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {A Spatial Model for Legislative Roll Call Analysis},
 urldate = {2024-08-10},
 volume = {29},
 year = {1985}
}

@article{岡田謙介-加藤淳子2016,
	author = {岡田謙介 and 加藤淳子},
	doi = {10.2333/jbhmk.43.155},
	journal = {行動計量学},
	number = {2},
	pages = {155-166},
	title = {政治学における空間分析と認知空間},
	volume = {43},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.43.155}}
@book{Enelow-Hinich1984,
    author         = {James M. Enelow and Melvin J. Hinich},
    year           = {1984},
    title          = {The Spatial Theory of Voting: An Introduction},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.cambridge.org/us/universitypress/subjects/politics-international-relations/political-theory/spatial-theory-voting-introduction?format=PB&isbn=9780521275156},
    publisher      = {Cambridge University Press}
}

@article{Eckart-Young1936,
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
	author = {Eckart, Carl and Young, Gale},
	date = {1936/09/01},
	date-added = {2024-08-11 11:46:20 +0900},
	date-modified = {2024-08-11 11:46:20 +0900},
	doi = {10.1007/BF02288367},
	id = {Eckart1936},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {211--218},
	title = {The approximation of one matrix by another of lower rank},
	url = {https://doi.org/10.1007/BF02288367},
	volume = {1},
	year = {1936},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288367}}
@inbook{Easterling1987,
    author         = {D. V. Easterling},
    chapter        = {Political Scicnce: Using the Generalized Euclidian Model to Study Ideological Shifts in the U.S. Senate },
    editor         = {Robert M. Hamer and Forrest W. Young},
    pages          = {219-256},
    publisher      = {Psychology Press},
    title          = {Multidimensional Scaling: History, Theory, and Applications},
    year           = {1987},
    url             = {https://doi.org/10.4324/9780203767719},
}
@article{Coombs1950,
    author          = {C. H. Coombs},
    year            = {1950},
    title           = {{Psychological Scaling without a Unit of Measurement}},
    journal         = {Psychological Review},
    volume          = {57},
    number          = {3},
    pages           = {145-158},
    url             = {https://psycnet.apa.org/doi/10.1037/h0060984}
}

@article{足立浩平2000,
	author = {足立浩平},
	doi = {10.2333/jbhmk.27.12},
	journal = {行動計量学},
	number = {1},
	pages = {12-23},
	title = {計量多次元展開法の変量モデル},
	volume = {27},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.27.12}}
@book{MacRae1958,
    author         = {Duncan MacRae},
    year           = {1958},
    title          = {Dimensions of Congressional Voting: A Statistical Study of the House of Representatives in the Eighty-first Congress},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {University of California Press}
}
@article{Poole+2011,
 title={Scaling Roll Call Votes with wnominate in R},
 volume={42},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v042i14},
 doi={10.18637/jss.v042.i14},
 abstract={This paper presents a software package designed to estimate Poole and Rosenthal W-NOMINATE scores in R. The package uses a logistic regression model to analyze political choice data, usually (though not exclusively) from a legislative setting. In contrast to other scaling methods, W-NOMINATE explicitly assumes probabilistic voting based on a spatial utility function, where the parameters of the utility function and the spatial coordinates of the legislators and the votes can all be estimated on the basis of observed voting behavior. Building on software written by Poole in Fortran, the new &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R facilitates easier data input and manipulation, generates bootstrapped standard errors, and includes a new suite of graphics functions to display the results. We demonstrate the functionality of this package by conducting a natural experiment using roll calls -- an experiment which is greatly simplified by the data manipulation capabilities of the &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R.},
 number={14},
 journal={Journal of Statistical Software},
 author={Poole, Keith and Lewis, Jeffrey B. and Lo, James and Carroll, Royce},
 year={2011},
 pages={1–21}
}
@article{Lee2001,
	abstract = {Multidimensional scaling models of stimulus domains are widely used as a representational basis for cognitive modeling. These representations associate stimuli with points in a coordinate space that has some predetermined number of dimensions. Although the choice of dimensionality can significantly influence cognitive modeling, it is often made on the basis of unsatisfactory heuristics. To address this problem, a Bayesian approach to dimensionality determination, based on the Bayesian Information Criterion (BIC), is developed using a probabilistic formulation of multidimensional scaling. The BIC approach formalizes the trade-off between data-fit and model complexity implicit in the problem of dimensionality determination and allows for the explicit introduction of information regarding data precision. Monte Carlo simulations are presented that indicate, by using this approach, the determined dimensionality is likely to be accurate if either a significant number of stimuli are considered or a reasonable estimate of precision is available. The approach is demonstrated using an established data set involving the judged pairwise similarities between a set of geometric stimuli.},
	author = {Michael D. Lee},
	doi = {https://doi.org/10.1006/jmps.1999.1300},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	number = {1},
	pages = {149-166},
	title = {Determining the Dimensionality of Multidimensional Scaling Representations for Cognitive Modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	volume = {45},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	bdsk-url-2 = {https://doi.org/10.1006/jmps.1999.1300}}
@article{Baker-Poole2013,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/23359696},
 abstract = {In this article, we show how to apply Bayesian methods to noisy ratio scale distances for both the classical similarities problem as well as the unfolding problem. Bayesian methods produce essentially the same point estimates as the classical methods, but are superior in that they provide more accurate measures of uncertainty in the data. Identification is nontrivial for this class of problems because a configuration of points that reproduces the distances is identified only up to a choice of origin, angles of rotation, and sign flips on the dimensions. We prove that fixing the origin and rotation is sufficient to identify a configuration in the sense that the corresponding maxima/minima are inflection points with full-rank Hessians. However, an unavoidable result is multiple posterior distributions that are mirror images of one another. This poses a problem for Markov chain Monte Carlo (MCMC) methods. The approach we take is to find the optimal solution using standard optimizers. The configuration of points from the optimizers is then used to isolate a single Bayesian posterior that can then be easily analyzed with standard MCMC methods.},
 author = {Ryan Bakker and Keith T. Poole},
 journal = {Political Analysis},
 number = {1},
 pages = {125--140},
 publisher = {Oxford University Press},
 title = {Bayesian Metric Multidimensional Scaling},
 urldate = {2024-08-10},
 volume = {21},
 year = {2013}
}
@inproceedings{Lim+2024,
    author          = {Johan Lim and Sooahn Shin and Jong Hee Park},
    year            = {2024},
    title           = {$\ell^1$-Based Bayesian Ideal Point Model for Multidimensional Politics},
    booktitle       = {ISI World Statistics Congress},
    volume          = {64},
    pages           = {},
    url             = {https://www.isi-next.org/abstracts/submission/1310/view/}
}
@article{Jackman2001,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791646},
 abstract = {Vote-specific parameters are often by-products of roll call analysis, the primary goal being the measurement of legislators' ideal points. But these vote-specific parameters are more important in higher-dimensional settings: prior restrictions on vote parameters help identify the model, and researchers often have prior beliefs about the nature of the dimensions underlying the proposal space. Bayesian methods provide a straightforward and rigorous way for incorporating these prior beliefs into roll call analysis. I demonstrate this by exploiting the close connections among roll call analysis, item—response models, and "full-information" factor analysis. Vote-specific discrimination parameters are equivalent to factor loadings, and as in factor analysis, they (1) enable researchers to discern the substantive content of the recovered dimensions, (2) can be used for assessing dimensionality and model checking, and (3) are an obvious vehicle for introducing and testing researchers' prior beliefs about the dimensions. Bayesian simulation facilitates these uses of discrimination parameters, by simplifying estimation and inference for the massive number of parameters generated by roll call analysis.},
 author = {Simon Jackman},
 journal = {Political Analysis},
 number = {3},
 pages = {227--241},
 publisher = {[Oxford University Press, Society for Political Methodology]},
 title = {Multidimensional Analysis of Roll Call Data via Bayesian Simulation: Identification, Estimation, Inference, and Model Checking},
 urldate = {2024-08-11},
 volume = {9},
 year = {2001}
}
@techreport{deLeeuw1977,
    author          = {Jan {de Leeuw}},
    year            = {1977},
    title           = {Applications of Convex Analysis to Multidimensional Scaling},
    institution = {UCLA: Department of Statistics},
    url             = {https://escholarship.org/uc/item/7wg0k7xq},
}
@ARTICLE{Sammon1969,
  author={Sammon, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Nonlinear Mapping for Data Structure Analysis}, 
  year={1969},
  volume={C-18},
  number={5},
  pages={401-409},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric, pattern recognition, statistics.},
  doi={10.1109/T-C.1969.222678}}

@article{Tenenbaum+2000,
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 106 optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	author = {Joshua B. Tenenbaum and Vin de Silva and John C. Langford},
	doi = {10.1126/science.290.5500.2319},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
	journal = {Science},
	number = {5500},
	pages = {2319-2323},
	title = {{A Global Geometric Framework for Nonlinear Dimensionality Reduction}},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2319}}

@article{Balasubramanian-Schwartz2002,
	author = {Mukund Balasubramanian and Eric L. Schwartz},
	doi = {10.1126/science.295.5552.7a},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.295.5552.7a},
	journal = {Science},
	number = {5552},
	pages = {7-7},
	title = {The Isomap Algorithm and Topological Stability},
	url = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	volume = {295},
	year = {2002},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	bdsk-url-2 = {https://doi.org/10.1126/science.295.5552.7a}}

@article{Choi-Choi2007,
	abstract = {Isomap is one of widely used low-dimensional embedding methods, where geodesic distances on a weighted graph are incorporated with the classical scaling (metric multidimensional scaling). In this paper we pay our attention to two critical issues that were not considered in Isomap, such as: (1) generalization property (projection property); (2) topological stability. Then we present a robust kernel Isomap method, armed with such two properties. We present a method which relates the Isomap to Mercer kernel machines, so that the generalization property naturally emerges, through kernel principal component analysis. For topological stability, we investigate the network flow in a graph, providing a method for eliminating critical outliers. The useful behavior of the robust kernel Isomap is confirmed through numerical experiments with several data sets.},
	author = {Heeyoul Choi and Seungjin Choi},
	doi = {https://doi.org/10.1016/j.patcog.2006.04.025},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Isomap, Kernel PCA, Manifold learning, Multidimensional scaling (MDS), Nonlinear dimensionality reduction},
	number = {3},
	pages = {853-862},
	title = {Robust kernel Isomap},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	volume = {40},
	year = {2007},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2006.04.025}}
@ARTICLE{Scholkopf+1998,
  author={Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
  journal={Neural Computation}, 
  title={Nonlinear Component Analysis as a Kernel Eigenvalue Problem}, 
  year={1998},
  volume={10},
  number={5},
  pages={1299-1319},
  keywords={},
  doi={10.1162/089976698300017467}}
@inproceedings{Weinberger+2004,
author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
title = {Learning a kernel matrix for nonlinear dimensionality reduction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015345},
doi = {10.1145/1015330.1015345},
abstract = {We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {106},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{Roweis-Saul2000,
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	author = {Sam T. Roweis and Lawrence K. Saul},
	doi = {10.1126/science.290.5500.2323},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2323},
	journal = {Science},
	number = {5500},
	pages = {2323-2326},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2323}}

@inproceedings{Mikhali-Partha2001,
	author = {Belkin, Mikhail and Niyogi, Partha},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf}}
@book{Chung1997,
    author         = {Fan R. K. Chung},
    year           = {1997},
    title          = {Spectral Graph Theory},
    series         = {CBMS Regional Conference Series in Mathematics},
    volume         = {92},
    edition        = {},
    url            = {https://doi.org/10.1090/cbms/092},
    publisher      = {American Mathematical Society}
}

@inproceedings{Hinton-Roweis2002,
	author = {Hinton, Geoffrey E and Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Becker and S. Thrun and K. Obermayer},
	publisher = {MIT Press},
	title = {Stochastic Neighbor Embedding},
	url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
	volume = {15},
	year = {2002},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf}}
@article{Maaten-Hinton2008,
  author  = {Laurens {van der Maaten} and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@article{Coifman+2005,
	author = {R. R. Coifman and S. Lafon and A. B. Lee and M. Maggioni and B. Nadler and F. Warner and S. W. Zucker},
	doi = {10.1073/pnas.0500334102},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0500334102},
	journal = {Proceedings of the National Academy of Sciences},
	number = {21},
	pages = {7426-7431},
	title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	volume = {102},
	year = {2005},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0500334102}}
@inproceedings{Carreira-Perpinan2010,
author = {Carreira-Perpi\~{n}an, Miguel \'{A}.},
title = {The elastic embedding algorithm for dimensionality reduction},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a new dimensionality reduction method, the elastic embedding (EE), that optimises an intuitive, nonlinear objective function of the low-dimensional coordinates of the data. The method reveals a fundamental relation betwen a spectral method, Laplacian eigenmaps, and a nonlinear method, stochastic neighbour embedding; and shows that EE can be seen as learning both the coordinates and the affinities between data points. We give a homotopy method to train EE, characterise the critical value of the homotopy parameter, and study the method's behaviour. For a fixed homotopy parameter, we give a globally convergent iterative algorithm that is very effective and requires no user parameters. Finally, we give an extension to out-of-sample points. In standard datasets, EE obtains results as good or better than those of SNE, but more efficiently and robustly.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {167–174},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}
@inproceedings{Wu-Fischer2020,
title={Phase Transitions for the Information Bottleneck in Representation Learning},
author={Tailin Wu and Ian Fischer},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJloElBYvB}
}
@article{Wattemnerg+2016,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}
@article{vanderMaaten2014,
  author  = {Laurens van der Maaten},
  title   = {Accelerating t-SNE using Tree-Based Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {93},
  pages   = {3221--3245},
  url     = {http://jmlr.org/papers/v15/vandermaaten14a.html}
}
@article{McInnes+2018, doi = {10.21105/joss.00861}, url = {https://doi.org/10.21105/joss.00861}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {29}, pages = {861}, author = {Leland McInnes and John Healy and Nathaniel Saul and Lukas Großberger}, title = {UMAP: Uniform Manifold Approximation and Projection}, journal = {Journal of Open Source Software} } 

@inproceedings{Weinberger+2005,
	author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf}}
@article{Weinberger-Saul2009,
  author  = {Kilian Q. Weinberger and Lawrence K. Saul},
  title   = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  number  = {9},
  pages   = {207-244},
  url     = {http://jmlr.org/papers/v10/weinberger09a.html}
}

@inproceedings{Goldberger+2004,
	author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	publisher = {MIT Press},
	title = {Neighbourhood Components Analysis},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf}}

@inproceedings{Musgrave+2020,
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best. Code is available at github.com/KevinMusgrave/powerful-benchmarker.},
	address = {Cham},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58595-2},
	pages = {681--699},
	publisher = {Springer International Publishing},
	title = {A Metric Learning Reality Check},
	year = {2020}}
@article{Chopra+2005,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Sumit Chopra and Raia Hadsell and Yann LeCun},
  journal={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  year={2005},
  volume={1},
  pages={539-546},
  url={https://doi.org/10.1109/CVPR.2005.202}
}
@InProceedings{Schroff+2015,
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
title = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015},
url             = {https://openaccess.thecvf.com/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
}

@inproceedings{Sohn2016,
	author = {Sohn, Kihyuk},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf}}
@INPROCEEDINGS{Movshovitz-Attias+2017,
author = {Y. Movshovitz-Attias and A. Toshev and T. K. Leung and S. Ioffe and S. Singh},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
title = {No Fuss Distance Metric Learning Using Proxies},
year = {2017},
volume = {},
issn = {2380-7504},
pages = {360-368},
abstract = {We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship - an anchor point x is similar to a set of positive points Y , and dissimilar to a set of negative points Z, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.},
keywords = {training;measurement;computer vision;optimization;convergence;fasteners;training data},
doi = {10.1109/ICCV.2017.47},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.47},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@InProceedings{Qian+2019,
author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
title = {SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019},
url             = {https://openaccess.thecvf.com/content_ICCV_2019/html/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.html},
}

@article{Nadaraya1964,
	abstract = { A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly. },
	author = {Nadaraya, E. A.},
	doi = {10.1137/1109020},
	eprint = {https://doi.org/10.1137/1109020},
	journal = {Theory of Probability \& Its Applications},
	number = {1},
	pages = {141-142},
	title = {On Estimating Regression},
	url = {https://doi.org/10.1137/1109020},
	volume = {9},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1137/1109020}}
@article{Watson1964,
 ISSN = {0581572X},
 URL = {http://www.jstor.org/stable/25049340},
 abstract = {Few would deny that the most powerful statistical tool is graph paper. When however there are many observations (and/or many variables) graphical procedures become tedious. It seems to the author that the most characteristic problem for statisticians at the moment is the development of methods for analyzing the data poured out by electronic observing systems. The present paper gives a simple computer method for obtaining a "graph" from a large number of observations.},
 author = {Geoffrey S. Watson},
 journal = {Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)},
 number = {4},
 pages = {359--372},
 publisher = {Springer},
 title = {Smooth Regression Analysis},
 urldate = {2024-08-11},
 volume = {26},
 year = {1964}
}

@article{Cleveland-Devlin1988,
	author = {William S. Cleveland and Susan J. Devlin},
	doi = {10.1080/01621459.1988.10478639},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478639},
	journal = {Journal of the American Statistical Association},
	number = {403},
	pages = {596--610},
	publisher = {Taylor \& Francis},
	title = {Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478639},
	volume = {83},
	year = {1988},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478639},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1988.10478639}}

@article{Cleveland1979,
	author = {William S. Cleveland},
	doi = {10.1080/01621459.1979.10481038},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1979.10481038},
	journal = {Journal of the American Statistical Association},
	number = {368},
	pages = {829--836},
	publisher = {Taylor \& Francis},
	title = {Robust Locally Weighted Regression and Smoothing Scatterplots},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	volume = {74},
	year = {1979},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1979.10481038}}

@article{Savitzky-Golay1964,
	annote = {doi: 10.1021/ac60214a047},
	author = {Savitzky, Abraham. and Golay, M. J. E.},
	date = {1964/07/01},
	date-added = {2024-08-12 00:30:38 +0900},
	date-modified = {2024-08-12 00:30:38 +0900},
	doi = {10.1021/ac60214a047},
	isbn = {0003-2700},
	journal = {Analytical Chemistry},
	journal1 = {Analytical Chemistry},
	journal2 = {Anal. Chem.},
	month = {07},
	number = {8},
	pages = {1627--1639},
	publisher = {American Chemical Society},
	title = {Smoothing and Differentiation of Data by Simplified Least Squares Procedures.},
	type = {doi: 10.1021/ac60214a047},
	url = {https://doi.org/10.1021/ac60214a047},
	volume = {36},
	year = {1964},
	year1 = {1964},
	bdsk-url-1 = {https://doi.org/10.1021/ac60214a047}}

@inproceedings{Chaudhuri-DasGupta2014,
	author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Rates of Convergence for Nearest Neighbor Classification},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf}}

@article{Gonzalez1985,
	abstract = {The problem of clustering a set of points so as to minimize the maximum intercluster distance is studied. An O(kn) approximation algorithm, where n is the number of points and k is the number of clusters, that guarantees solutions with an objective function value within two times the optimal solution value is presented. This approximation algorithm succeeds as long as the set of points satisfies the triangular inequality. We also show that our approximation algorithm is best possible, with respect to the approximation bound, if P ≠ NP.},
	author = {Teofilo F. Gonzalez},
	doi = {https://doi.org/10.1016/0304-3975(85)90224-5},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Algorithms, clustering, NP-completeness, approximation algorithms, minimizing the maximum intercluster distance},
	pages = {293-306},
	title = {Clustering to minimize the maximum intercluster distance},
	url = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
	volume = {38},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
	bdsk-url-2 = {https://doi.org/10.1016/0304-3975(85)90224-5}}
@inproceedings{David-Sergei2007,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {k-means++: the advantages of careful seeding},
year = {2007},
isbn = {9780898716245},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
location = {New Orleans, Louisiana},
series = {SODA '07}
}
@inproceedings{Kaufmann-Rousseeuw1987,
    author          = {L. Kaufmann and P. Rousseeuw},
    year            = {1987},
    title           = {{Clustering by Means of Medoids}},
    booktitle       = {Proceedings of Statistical Data Analysis Based on the L1 Norm Conference},
    volume          = {},
    pages           = {405-416},
    url             = {}
}

@article{Park-Jun2009,
	abstract = {This paper proposes a new algorithm for K-medoids clustering which runs like the K-means algorithm and tests several methods for selecting initial medoids. The proposed algorithm calculates the distance matrix once and uses it for finding new medoids at every iterative step. To evaluate the proposed algorithm, we use some real and artificial data sets and compare with the results of other algorithms in terms of the adjusted Rand index. Experimental results show that the proposed algorithm takes a significantly reduced time in computation with comparable performance against the partitioning around medoids.},
	author = {Hae-Sang Park and Chi-Hyuck Jun},
	doi = {https://doi.org/10.1016/j.eswa.2008.01.039},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Clustering, K-means, K-medoids, Rand index},
	number = {2, Part 2},
	pages = {3336-3341},
	title = {A simple and fast algorithm for K-medoids clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S095741740800081X},
	volume = {36},
	year = {2009},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741740800081X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2008.01.039}}

@inproceedings{Mnih-Kavukcuoglu2013,
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
	volume = {26},
	year = {2013},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf}}

@InProceedings{Chen+2020SimCLR,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@inproceedings{Tian+2020,
	abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog'' can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.},
	address = {Cham},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58621-8},
	pages = {776--794},
	publisher = {Springer International Publishing},
	title = {Contrastive Multiview Coding},
	year = {2020}}
@misc{Faghri+2018,
      title={VSE++: Improving Visual-Semantic Embeddings with Hard Negatives}, 
      author={Fartash Faghri and David J. Fleet and Jamie Ryan Kiros and Sanja Fidler},
      year={2018},
      eprint={1707.05612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.05612}, 
}

@inproceedings{Tian+2020WhatMakes,
	author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {6827--6839},
	publisher = {Curran Associates, Inc.},
	title = {What Makes for Good Views for Contrastive Learning?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf}}

@inproceedings{Khosla+2020,
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {18661--18673},
	publisher = {Curran Associates, Inc.},
	title = {Supervised Contrastive Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf}}
@INPROCEEDINGS{Caron+2021,
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Emerging Properties in Self-Supervised Vision Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={9630-9640},
  keywords={Training;Image segmentation;Computer vision;Semantics;Layout;Image retrieval;Computer architecture;Representation learning;Recognition and classification;Transfer/Low-shot/Semi/Unsupervised Learning},
  doi={10.1109/ICCV48922.2021.00951}}

@inproceedings{Grill+2020,
	author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {21271--21284},
	publisher = {Curran Associates, Inc.},
	title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf}}

@InProceedings{Zbontar+2021,
  title = 	 {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author =       {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12310--12320},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zbontar21a.html},
  abstract = 	 {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.}
}

@inproceedings{Gretton+2007,
	author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch\"{o}lkopf, Bernhard and Smola, Alex},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {A Kernel Statistical Test of Independence},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf}}

@InProceedings{Roeder+2021,
  title = 	 {On Linear Identifiability of Learned Representations},
  author =       {Roeder, Geoffrey and Metz, Luke and Kingma, Durk},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9030--9039},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/roeder21a/roeder21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/roeder21a.html},
  abstract = 	 {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions lack identifiability in parameter space, because they are over-parameterized by design. In this paper, building on recent advances in nonlinear Independent Components Analysis, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.}
}

@inproceedings{Halva+2021,
	author = {H\"{a}lv\"{a}, Hermanni and Le Corff, Sylvain and Leh\'{e}ricy, Luc and So, Jonathan and Zhu, Yongjie and Gassiat, Elisabeth and Hyvarinen, Aapo},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {1624--1633},
	publisher = {Curran Associates, Inc.},
	title = {Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf}}
@article{Bell-Sejnowski1995,
    author = {Bell, Anthony J. and Sejnowski, Terrence J.},
    title = "{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}",
    journal = {Neural Computation},
    volume = {7},
    number = {6},
    pages = {1129-1159},
    year = {1995},
    month = {11},
    abstract = "{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1995.7.6.1129},
    url = {https://doi.org/10.1162/neco.1995.7.6.1129},
    eprint = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
}
@incollection{Barlow1961,
    author = {Barlow, H. B.},
    isbn = {9780262518420},
    title = "{Possible Principles Underlying the Transformations of Sensory Messages}",
    booktitle = "{Sensory Communication}",
    publisher = {The MIT Press},
    year = {1961},
    month = {09},
    abstract = "{This chapter is an attempt to formulate ideas about the operations performed by physiological mechanisms, and not merely a discussion of the physiological mechanisms of sensory pathways. It presents three hypotheses regarding the purpose of sensory relays. The first one is the “password” hypothesis, which posits that, since animals respond specifically to specific stimuli, their sensory pathways must possess mechanisms for detecting such stimuli and discriminating between them. The second hypothesis is the fashionable one that relays act as control points at which the flow of information is modulated according to the requirements of other parts of the nervous system. Finally, the third hypothesis theorizes that reduction of redundancy is an important principle guiding the organization of sensory messages and is carried out at relays in the sensory pathways.}",
    doi = {10.7551/mitpress/9780262518420.003.0013},
    url = {https://doi.org/10.7551/mitpress/9780262518420.003.0013},
    eprint = {https://academic.oup.com/mit-press-scholarship-online/book/0/chapter/180090664/chapter-ag-pdf/44697172/book\_20714\_section\_180090664.ag.pdf},
}

@article{Barlow1972,
	abstract = { The problem discussed is the relationship between the firing of single neurons in sensory pathways and subjectively experienced sensations. The conclusions are formulated as the following five dogmas:To understand nervous function one needs to look at interactions at a cellular level, rather than either a more macroscopic or microscopic level, because behaviour depends upon the organized pattern of these intercellular interactions.The sensory system is organized to achieve as complete a representation of the sensory stimulus as possible with the minimum number of active neurons.Trigger features of sensory neurons are matched to redundant patterns of stimulation by experience as well as by developmental processes.Perception corresponds to the activity of a small selection from the very numerous high-level neurons, each of which corresponds to a pattern of external events of the order of complexity of the events symbolized by a word.High impulse frequency in such neurons corresponds to high certainty that the trigger feature is present.The development of the concepts leading up to these speculative dogmas, their experimental basis, and some of their limitations are discussed. },
	author = {H B Barlow},
	doi = {10.1068/p010371},
	eprint = {https://doi.org/10.1068/p010371},
	journal = {Perception},
	note = {PMID: 4377168},
	number = {4},
	pages = {371-394},
	title = {Single Units and Sensation: A Neuron Doctrine for Perceptual Psychology?},
	url = {https://doi.org/10.1068/p010371},
	volume = {1},
	year = {1972},
	bdsk-url-1 = {https://doi.org/10.1068/p010371}}

@article{島崎秀昭2019,
	author = {島崎秀昭},
	doi = {10.3902/jnns.26.72},
	journal = {日本神経回路学会誌},
	number = {3},
	pages = {72-98},
	title = {ベイズ統計と熱力学から見る生物の学習と認識のダイナミクス},
	volume = {26},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3902/jnns.26.72}}

@article{Laughlin1981,
	author = {Simon Laughlin},
	doi = {doi:10.1515/znc-1981-9-1040},
	journal = {Zeitschrift f{\"u}r Naturforschung C},
	lastchecked = {2024-08-12},
	number = {9-10},
	pages = {910--912},
	url = {https://doi.org/10.1515/znc-1981-9-1040},
	volume = {36},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1515/znc-1981-9-1040},
	title     = {A Simple Coding Procedure Enhances a Neuron's Information Capacity},
	}

@article{Brenner+2000,
	abstract = {Adaptation is a widespread phenomenon in nervous systems, providing flexibility to function under varying external conditions. Here, we relate an adaptive property of a sensory system directly to its function as a carrier of information about input signals. We show that the input/output relation of a sensory system in a dynamic environment changes with the statistical properties of the environment. Specifically, when the dynamic range of inputs changes, the input/output relation rescales so as to match the dynamic range of responses to that of the inputs. We give direct evidence that the scaling of the input/output relation is set to maximize information transmission for each distribution of signals. This adaptive behavior should be particularly useful in dealing with the intermittent statistics of natural signals.},
	author = {Naama Brenner and William Bialek and Rob {de Ruyter van Steveninck}},
	doi = {https://doi.org/10.1016/S0896-6273(00)81205-2},
	issn = {0896-6273},
	journal = {Neuron},
	number = {3},
	pages = {695-702},
	title = {Adaptive Rescaling Maximizes Information Transmission},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627300812052},
	volume = {26},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0896627300812052},
	bdsk-url-2 = {https://doi.org/10.1016/S0896-6273(00)81205-2}}
@book{Doya+2006,
    author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh P.N.},
    title = "{Bayesian Brain: Probabilistic Approaches to Neural Coding }",
    publisher = {The MIT Press},
    year = {2006},
    month = {12},
    abstract = "{Experimental and theoretical neuroscientists use Bayesian approaches to analyze the brain mechanisms of perception, decision-making, and motor control.A Bayesian approach can contribute to an understanding of the brain on multiple levels, by giving normative predictions about how an ideal sensory system should combine prior knowledge and observation, by providing mechanistic interpretation of the dynamic functioning of the brain circuit, and by suggesting optimal ways of deciphering experimental data. Bayesian Brain brings together contributions from both experimental and theoretical neuroscientists that examine the brain mechanisms of perception, decision making, and motor control according to the concepts of Bayesian estimation.After an overview of the mathematical concepts, including Bayes' theorem, that are basic to understanding the approaches discussed, contributors discuss how Bayesian concepts can be used for interpretation of such neurobiological data as neural spikes and functional brain imaging. Next, contributors examine the modeling of sensory processing, including the neural coding of information about the outside world. Finally, contributors explore dynamic processes for proper behaviors, including the mathematics of the speed and accuracy of perceptual decisions and neural models of belief propagation.}",
    isbn = {9780262294188},
    doi = {10.7551/mitpress/9780262042383.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262042383.001.0001},
}
@INPROCEEDINGS{Romaszko+2017,
  author={Romaszko, Lukasz and Williams, Christopher K. I. and Moreno, Pol and Kohli, Pushmeet},
  booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Vision-as-Inverse-Graphics: Obtaining a Rich 3D Explanation of a Scene from a Single Image}, 
  year={2017},
  volume={},
  number={},
  pages={940-948},
  keywords={Cameras;Probabilistic logic;Lighting;Detectors;Object detection;Graphics;Transforms},
  doi={10.1109/ICCVW.2017.115}}
@INPROCEEDINGS{Tishby-Zaslavsky2015,
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  keywords={Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training},
  doi={10.1109/ITW.2015.7133169}}
@misc{Tishby+2000,
      title={The information bottleneck method}, 
      author={Naftali Tishby and Fernando C. Pereira and William Bialek},
      year={2000},
      eprint={physics/0004057},
      archivePrefix={arXiv},
      primaryClass={physics.data-an},
      url={https://arxiv.org/abs/physics/0004057}, 
}
@misc{Andrieu+2024,
      title={Gradient-free optimization via integration}, 
      author={Christophe Andrieu and Nicolas Chopin and Ettore Fincato and Mathieu Gerber},
      year={2024},
      eprint={2408.00888},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2408.00888}, 
}

@book{足立-山本2024,
	author         = {足立浩平 and 山本倫生},
	year           = {2024},
	title          = {主成分分析と因子分析―特異値分解を出発点として―},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {https://www.kyoritsu-pub.co.jp/book/b10085699.html},
	publisher      = {共立出版}
}

@book{Thurstone1947,
	author         = {L. L. Thurstone},
	year           = {1947},
	title          = {Multiple Factor Analysis},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {},
	publisher      = {University of Chicago Press}
}

@article{Stewart1993,
	abstract = { This paper surveys the contributions of five mathematicians---Eugenio Beltrami (1835--1899), Camille Jordan (1838--1921), James Joseph Sylvester (1814--1897), Erhard Schmidt (1876--1959), and Hermann Weyl (1885--1955)---who were responsible for establishing the existence of the singular value decomposition and developing its theory. },
	author = {Stewart, G. W.},
	doi = {10.1137/1035134},
	eprint = {https://doi.org/10.1137/1035134},
	journal = {SIAM Review},
	number = {4},
	pages = {551-566},
	title = {On the Early History of the Singular Value Decomposition},
	url = {https://doi.org/10.1137/1035134},
	volume = {35},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1137/1035134}}
@article{Sylvester1889,
	author          = {James Joseph Sylvester},
	year            = {1889},
	title           = {On The Reduction of a Bilinear Quantic of the $n$-th Order to the Form of a Sum of $n$ Products by a Double Orthogonal Substitution},
	journal         = {The Messenger of Mathematics},
	volume          = {18},
	number          = {},
	pages           = {42-46},
	url             = {https://books.google.co.jp/books?id=cxRKAQAAMAAJ&pg=PA1&hl=ja&source=gbs_toc_r&cad=2#v=onepage&q&f=false}
}
@article{Beltrami1873,
	author          = {E. Beltrami},
	year            = {1873},
	title           = {Sulle Funzioni Bilineari},
	journal         = {Giornale di Matematiche ad Uso degli Studenti Delle Universita},
	volume          = {11},
	number          = {},
	pages           = {98-106},
	url             = {https://gallica.bnf.fr/ark:/12148/bpt6k99434d/f442}
}
@article{Moore1920,
	author          = {E. H. Moore},
	year            = {1920},
	title           = {On the Reciprocal of the General Algebraic Matrix},
	journal         = {Bulletin of the American Mathematical Society},
	volume          = {26},
	number          = {9},
	pages           = {394-395},
	url             = {https://doi.org/10.1090%2FS0002-9904-1920-03322-7},
	note            = {In The fourteenth western meeting of the American Mathematical Society},
}
@article{Penrose1955, title={A generalized inverse for matrices}, volume={51}, DOI={10.1017/S0305004100030401}, number={3}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Penrose, R.}, year={1955}, pages={406–413}}

@article{Halko+2011,
	abstract = { Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m \times n\$ matrix. (i) For a dense input matrix, randomized algorithms require \$\bigO(mn \log(k))\$ floating-point operations (flops) in contrast to \$ \bigO(mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \$\bigO(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. },
	author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
	doi = {10.1137/090771806},
	eprint = {https://doi.org/10.1137/090771806},
	journal = {SIAM Review},
	number = {2},
	pages = {217-288},
	title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
	url = {https://doi.org/10.1137/090771806},
	volume = {53},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1137/090771806}}
@techreport{Murray+2023,
    Author= {Murray, Riley and Demmel, James and Mahoney, Michael W. and Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman Asif and Grigori, Laura and Luszczek, Piotr and Derezinski, Michal and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui and Dongarra, Jack},
    Title= {Randomized Numerical Linear Algebra: A Perspective on the Field With an Eye to Software},
    Year= {2023},
    Month= {Feb},
    Url= {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-19.html},
    Number= {UCB/EECS-2023-19},
}
@article{Drineas+2016,
author = {Drineas, Petros and Mahoney, Michael W.},
title = {RandNLA: randomized numerical linear algebra},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2842602},
doi = {10.1145/2842602},
abstract = {Randomization offers new benefits for large-scale linear algebra computations.},
journal = {Commun. ACM},
month = {may},
pages = {80–90},
numpages = {11}
}

@inproceedings{Roweis1997,
	author = {Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Jordan and M. Kearns and S. Solla},
	publisher = {MIT Press},
	title = {EM Algorithms for PCA and SPCA},
	url = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf},
	volume = {10},
	year = {1997},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf}}
@article{Gabriel1971,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334381},
 abstract = {Any matrix of rank two can be displayed as a biplot which consists of a vector for each row and a vector for each column, chosen so that any element of the matrix is exactly the inner product of the vectors corresponding to its row and to its column. If a matrix is of higher rank, one may display it approximately by a biplot of a matrix of rank two which approximates the original matrix. The biplot provides a useful tool of data analysis and allows the visual appraisal of the structure of large data matrices. It is especially revealing in principal component analysis, where the biplot can show inter-unit distances and indicate clustering of units as well as display variances and correlations of the variables.},
 author = {K. R. Gabriel},
 journal = {Biometrika},
 number = {3},
 pages = {453--467},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Biplot Graphic Display of Matrices with Application to Principal Component Analysis},
 urldate = {2024-08-12},
 volume = {58},
 year = {1971}
}
@article{Gower2004,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/20441132},
 abstract = {A simple geometry allows the main properties of matrix approximations used in biplot displays to be developed. It establishes orthogonal components of an analysis of variance, from which different contributions to approximations may be assessed. Particular attention is paid to approximations that share the same singular vectors, in which case the solution space is a convex cone. Two- and three-dimensional approximations are examined in detail and then the geometry is interpreted for different forms of the matrix being approximated.},
 author = {J. C. Gower},
 journal = {Biometrika},
 number = {3},
 pages = {705--714},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Geometry of Biplot Scaling},
 urldate = {2024-08-12},
 volume = {91},
 year = {2004}
}
@article{Spearman1904,
    author          = {Spearman, Charles},
    title           = {'General intelligence,' objectively determined and measured},
    year            = {1904},
    journal         = {The American Journal of Psychology},
    volume          = {15},
    number          = {2},
    pages           = {201-293},
	url             = {https://psycnet.apa.org/doi/10.2307/1412107},
}
@book{豊田秀樹1992,
	author         = {豊田秀樹},
	year           = {1992},
	title          = {SASによる共分散構造分析 },
	series         = {ＳＡＳで学ぶ統計的データ解析},
	volume         = {3},
	edition        = {},
	url            = {https://www.utp.or.jp/book/b302422.html},
	publisher      = {東京大学出版会}
}

@article{Muthen2002,
	abstract = {This article gives an overview of statistical analysis with latent variables. Using traditional structural equation modeling as a starting point, it shows how the idea of latent variables captures a wide variety of statistical concepts, including random effects, missing data, sources of variation in hierarchical data, finite mixtures. latent classes, and clusters. These latent variable applications go beyond the traditional latent variable useage in psychometrics with its focus on measurement error and hypothetical constructs measured by multiple indicators. The article argues for the value of integrating statistical and psychometric modeling ideas. Different applications are discussed in a unifying framework that brings together in one general model such different analysis types as factor models, growth curve models, multilevel models, latent class models and discrete-time survival models. Several possible combinations and extensions of these models are made clear due to the unifying framework.},
	author = {Muth{\'e}n, Bengt O. },
	date = {2002/01/01},
	date-added = {2024-08-13 12:06:06 +0900},
	date-modified = {2024-08-13 12:06:06 +0900},
	doi = {10.2333/bhmk.29.81},
	id = {Muth{\'e}n2002},
	isbn = {1349-6964},
	journal = {Behaviormetrika},
	number = {1},
	pages = {81--117},
	title = {{Beyond SEM: General Latent Variable Modeling}},
	url = {https://doi.org/10.2333/bhmk.29.81},
	volume = {29},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.2333/bhmk.29.81}}

@article{狩野裕2002,
	author = {狩野裕},
	doi = {10.2333/jbhmk.29.138},
	journal = {行動計量学},
	number = {2},
	pages = {138-159},
	title = {構造方程式モデリングは，因子分析，分散分析，パス解析の すべてにとって代わるのか？},
	volume = {29},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.29.138}}
@phdthesis{Socan2003,
	author      = {G. Socan},
	school      = {Rijksuniversiteit Groningen },
	title       = {The Incremental Value of Rank Factor Analysis},
	year        = {2003}
}

@article{足立浩平+2019,
	author = {足立浩平 and 伊藤真道 and 宇野光平},
	doi = {10.20551/jscswabun.32.1_61},
	journal = {計算機統計学},
	number = {1},
	pages = {61-77},
	title = {行列分解に基づく因子分析とその新展開},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.20551/jscswabun.32.1_61}}
@article{Lawley1942, title={XIV.—Further Investigations in Factor Estimation}, volume={61}, DOI={10.1017/S0080454100006178}, number={2}, journal={Proceedings of the Royal Society of Edinburgh. Section A. Mathematical and Physical Sciences}, author={Lawley, D. N.}, year={1942}, pages={176–185}}
@inproceedings{Anderson-Rubin1956,
	author          = {T. W. Anderson and Herman Rubin},
	year            = {1956},
	title           = {Statistical Inference in Factor Analysis},
	booktitle       = {Proceedings of the Thrid Berkeley Symposium on Mathematical Statistics and Probability},
	volume          = {5},
	pages           = {111-150},
	url             = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Statistical-Inference-in-Factor-Analysis/bsmsp/1200511860}
}

@article{Harman-Jones1966,
	abstract = {This paper is addressed to the classical problem of estimating factor loadings under the condition that the sum of squares of off-diagonal residuals be minimized. Communalities consistent with this criterion are produced as a by-product. The experimental work included several alternative algorithms before a highly efficient method was developed. The final procedure is illustrated with a numerical example. Some relationships of minres to principal-factor analysis and maximum-likelihood factor estimates are discussed, and several unresolved problems are pointed out.},
	author = {Harman, Harry H. and Jones, Wayne H.},
	date = {1966/09/01},
	date-added = {2024-08-13 13:15:59 +0900},
	date-modified = {2024-08-13 13:15:59 +0900},
	doi = {10.1007/BF02289468},
	id = {Harman1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {351--368},
	title = {Factor analysis by minimizing residuals (minres)},
	url = {https://doi.org/10.1007/BF02289468},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289468}}

@article{Harman-Fukuda1966,
	abstract = {In the course of developing the minres method of factor analysis the troublesome situation of communalities greater than one arose. This problem---referred to as the generalized Heywood case---is resolved in this paper by means of a process of minimizing the sum of squares of off-diagonal residuals. The resulting solution is superior to the otherwise very efficient original minres method without requiring additional computing time.},
	author = {Harman, Harry H. and Fukuda, Yoichiro},
	date = {1966/12/01},
	date-added = {2024-08-13 13:19:47 +0900},
	date-modified = {2024-08-13 13:19:47 +0900},
	doi = {10.1007/BF02289525},
	id = {Harman1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {563--571},
	title = {Resolution of the heywood case in the minres solution},
	url = {https://doi.org/10.1007/BF02289525},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289525}}

@article{Rubin-Thayer1982,
	abstract = {The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq ×q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation.},
	author = {Rubin, Donald B. and Thayer, Dorothy T.},
	date = {1982/03/01},
	date-added = {2024-08-13 13:21:17 +0900},
	date-modified = {2024-08-13 13:21:17 +0900},
	doi = {10.1007/BF02293851},
	id = {Rubin1982},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {69--76},
	title = {EM algorithms for ML factor analysis},
	url = {https://doi.org/10.1007/BF02293851},
	volume = {47},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293851}}
@Inbook{Jöreskog1988,
author="J{\"o}reskog, Karl G.",
editor="Nesselroade, John R.
and Cattell, Raymond B.",
title="Analysis of Covariance Structures",
bookTitle="Handbook of Multivariate Experimental Psychology",
year="1988",
publisher="Springer US",
address="Boston, MA",
pages="207--230",
abstract="Analysis of covariance structures is the common term for a number of techniques for analyzing multivariate data in order to detect and assess latent (unobserved) sources of variation and covariation in the observed measurements. The techniques of covariance structure analysis are general and flexible in that they can handle many types of covariance structures useful especially in the behavioral and social sciences. Although these techniques can be used for exploratory analysis, they have been most successfully applied to confirmatory analysis where the type of covariance structure is specified in advance. A covariance structure of a specified kind may arise because of a specified substantive theory or hypothesis, a given classificatory design for the measures, known experimental conditions, or because of results from previous studies based on extensive data. Sometimes the observed variables are ordered through time, as in longitudinal studies, or according to linear or circular patterns, as in Guttman's (1954) simplex and circumplex models, or according to a given causal scheme, as in path analysis.",
isbn="978-1-4613-0893-5",
doi="10.1007/978-1-4613-0893-5_5",
url="https://doi.org/10.1007/978-1-4613-0893-5_5"
}


@article{Bock-Bargmann1966,
	abstract = {A general method is presented for estimating variance components when the experimental design has one random way of classification and a possibly unbalanced fixed classification. The procedure operates on a sample covariance matrix in which the fixed classes play the role of variables and the random classes correspond to observations. Cases are considered which assume (i) homogeneous and (ii) nonhomogeneous error variance, and (iii) arbitrary scale factors in the measurements and homogeneous error variance. The results include maximum-likelihood estimations of the variance components and scale factors, likelihood-ratio tests of the goodness-of-fit of the model assumed for the design, and large-sample variances and covariances of the estimates. Applications to mental test data are presented. In these applications the subjects constitute the random dimension of the design, and a classification of the mental tests according to objective features of format or content constitute the fixed dimensions.},
	author = {Bock, R. Darrell and Bargmann, Rolf E.},
	date = {1966/12/01},
	date-added = {2024-08-13 13:32:44 +0900},
	date-modified = {2024-08-13 13:32:44 +0900},
	doi = {10.1007/BF02289521},
	id = {Bock1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {507--534},
	title = {Analysis of covariance structures},
	url = {https://doi.org/10.1007/BF02289521},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289521}}
@article{Joreskog1970,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334833},
 abstract = {It is assumed that observations on a set of variables have a multivariate normal distribution with a general parametric form of the mean vector and the variance-covariance matrix. Any parameter of the model may be fixed, free or constrained to be equal to other parameters. The free and constrained parameters are estimated by maximum likelihood. A wide range of models is obtained from the general model by imposing various specifications on the parametric structure of the general model. Examples are given of areas and problems, especially in the behavioural sciences, where the method may be useful.},
 author = {K. G. Jöreskog},
 journal = {Biometrika},
 number = {2},
 pages = {239--251},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A General Method for Analysis of Covariance Structures},
 urldate = {2024-08-13},
 volume = {57},
 year = {1970}
}

@article{Joreskog1978,
	abstract = {A general approach to the analysis of covariance structures is considered, in which the variances and covariances or correlations of the observed variables are directly expressed in terms of the parameters of interest. The statistical problems of identification, estimation and testing of such covariance or correlation structures are discussed.},
	author = {J{\"o}reskog, Karl G. },
	date = {1978/12/01},
	date-added = {2024-08-13 13:34:21 +0900},
	date-modified = {2024-08-13 13:34:21 +0900},
	doi = {10.1007/BF02293808},
	id = {J{\"o}reskog1978},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--477},
	title = {Structural analysis of covariance and correlation matrices},
	url = {https://doi.org/10.1007/BF02293808},
	volume = {43},
	year = {1978},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293808}}
@techreport{Fornell1985,
	author      = {Claes Fornell},
	institution = {Business, Stephen M. Ross School, University of Michigan},
	title       = {A Second generation of multivariate analysis: classification of methods and implications for marketing research},
	year        = {1985},
	url             = {https://hdl.handle.net/2027.42/35621},
}

@article{豊田秀樹1991,
	author = {豊田秀樹},
	doi = {10.5926/jjep1953.39.4_467},
	journal = {教育心理学研究},
	number = {4},
	pages = {467-478},
	title = {共分散構造分析の下位モデルとその適用例},
	volume = {39},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.5926/jjep1953.39.4_467}}
@inbook{Joreskog-Wold1982,
	author         = {K. G. Jöreskog and H. Wold},
	chapter        = {The ML and PLS Techniques for Modeling with Latent Variables: Historical and Comparative Aspects},
	editor         = {},
	pages          = {263-270},
	publisher      = {North-Holland},
	title          = {Systems under Indirect Observation: Causality, Structure, Prediction},
	year           = {1982}
}

@article{星野崇宏+2005,
	author = {星野崇宏 and 岡田謙介 and 前田忠彦},
	doi = {10.2333/jbhmk.32.209},
	journal = {行動計量学},
	number = {2},
	pages = {209-235},
	title = {構造方程式モデリングにおける適合度指標とモデル改善について : 展望とシミュレーション研究による新たな知見},
	volume = {32},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.32.209}}

@article{Joreskog1967a,
	abstract = {A new computational method for the maximum likelihood solution in factor analysis is presented. This method takes into account the fact that the likelihood function may not have a maximum in a point of the parameter space where all unique variances are positive. Instead, the maximum may be attained on the boundary of the parameter space where one or more of the unique variances are zero. It is demonstrated that suchimproper (Heywood) solutions occur more often than is usually expected. A general procedure to deal with such improper solutions is proposed. The proposed methods are illustrated using two small sets of empirical data, and results obtained from the analyses of many other sets of data are reported. These analyses verify that the new computational method converges rapidly and that the maximum likelihood solution can be determined very accurately. A by-product obtained by the method is a large sample estimate of the variance-covariance matrix of the estimated unique variances. This can be used to set up approximate confidence intervals for communalities and unique variances.},
	author = {J{\"o}reskog, K.  G. },
	date = {1967/12/01},
	date-added = {2024-08-13 14:29:19 +0900},
	date-modified = {2024-08-13 14:29:19 +0900},
	doi = {10.1007/BF02289658},
	id = {J{\"o}reskog1967},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--482},
	title = {Some contributions to maximum likelihood factor analysis},
	url = {https://doi.org/10.1007/BF02289658},
	volume = {32},
	year = {1967},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289658}}
@techreport{Joreskog1966,
	author      = {Karl G. Jöreskog},
	institution = {ETS},
	title       = {UMLFA: A Computer Program for Unrestricted Maximum Likelihood Factor Analysis},
	year        = {1966},
	url             = {https://www.ets.org/research/policy_research_reports/publications/report/1966/iazh.html},
}
@book{Grimm-Yarnold2016,
    author         = {Grimm, Laurence G. and Yarnold, Paul R.},
    title          = {研究論文を読み解くための多変量解析入門 応用篇},
    year           = {2016},
    publisher      = {北大路書房},
    series         = {},
    volume         = {},
    month          = {9},
    edition        = {},
    howpublished   = {Reading and Understanding MORE Multivariate Statistics (2020) の翻訳書}
}

@article{Sorbom1974,
	abstract = {A statistical model is developed for the study of similarities and differences in factor structure between several groups. The model assumes that the observed variables satisfy a factor analysis model in each group. A method of data analysis is presented which, in contrast to earlier work, makes use of information in the observed means as well as the observed variances and covariances to estimate the parameters in each group, i.e. factor means, factor loadings, factor variances and covariances and unique variances. Usually the units of measurement in the observed variables have no intrinsic meaning and therefore it is only meaningful to compare the relative magnitudes of the parameters for the different groups. The method estimates the parameters for all groups simultaneously and can take into account a priori information about factorial invariance of various degrees.},
	author = {S{\"o}rbom, Dag},
	doi = {https://doi.org/10.1111/j.2044-8317.1974.tb00543.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1974.tb00543.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {2},
	pages = {229-239},
	title = {{A General Method for Studying Differences in Factor Means and Factor Structure between Groups}},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1974.tb00543.x},
	volume = {27},
	year = {1974},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1974.tb00543.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.1974.tb00543.x}}
@article{清水和秋1994,
	author          = {清水和秋},
	year            = {1994},
	title           = {JöreskogとSörbomによるコンピュータ・プログラムと構造方程式モデル},
	journal         = {関西大学社会学部紀要},
	volume          = {25},
	number          = {3},
	pages           = {1-41},
	url             = {http://hdl.handle.net/10112/13345}
}
@article{清水和秋1989,
	author          = {清水和秋},
	year            = {1989},
	title           = {検証的因子分析，LISRELそしてRAMの概要},
	journal         = {関西大学社会学部紀要},
	volume          = {20},
	number          = {2},
	pages           = {61-86},
	url             = {http://hdl.handle.net/10112/13348}
}

@article{McArdle1984,
	author = {J. Jack McArdle},
	doi = {10.1080/00273171.1984.9676927},
	eprint = {https://doi.org/10.1080/00273171.1984.9676927},
	journal = {Multivariate Behavioral Research},
	note = {PMID: 26781893},
	number = {2-3},
	pages = {245--267},
	publisher = {Routledge},
	title = {On the Madness in His Method: R. B. Cattell's Contributions to Structural Equation Modeling},
	url = {https://doi.org/10.1080/00273171.1984.9676927},
	volume = {19},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1080/00273171.1984.9676927}}

@article{Bentler1980,
	author = {Bentler, P M},
	doi = {https://doi.org/10.1146/annurev.ps.31.020180.002223},
	issn = {1545-2085},
	journal = {Annual Review of Psychology},
	number = {Volume 31, 1980},
	pages = {419-456},
	publisher = {Annual Reviews},
	title = {Multivariate Analysis with Latent Variables: Causal Modeling},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.ps.31.020180.002223},
	volume = {31},
	year = {1980},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev.ps.31.020180.002223},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.ps.31.020180.002223}}
@article{白倉幸男1984,
	author          = {白倉幸男},
	year            = {1984},
	title           = {多重指標線形構造モデルとその応用 : 研究ノート },
	journal         = {大阪大学人間科学部紀要},
	volume          = {10},
	number          = {},
	pages           = {25-45},
	url             = {https://doi.org/10.18910/4301}
}
@techreport{Ghahramani-Hinton1996,
	author      = {Zoubin Ghahramani and Geoffrey E. Hinton},
	institution = {Department of Computer Science, University of Toronto},
	title       = {The EM Algorithm for Mixtures of Factor Analyzers},
	year        = {1996},
	url             = {https://www.cs.toronto.edu/~hinton/absps/tr96-1.html},
}

@article{Bernaards-Jennrich2003,
	abstract = {A loading matrix has perfect simple structure if each row has at most one nonzero element. It is shown that if there is an orthogonal rotation of an initial loading matrix that has perfect simple structure, then orthomax rotation with 0 ≤γ≤1 of the initial loading matrix will produce the perfect simple structure. In particular, varimax and quartimax will produce rotations with perfect simple structure whenever they exist.},
	author = {Bernaards, Coen A. and Jennrich, Robert I.},
	date = {2003/12/01},
	date-added = {2024-08-13 15:52:15 +0900},
	date-modified = {2024-08-13 15:52:15 +0900},
	doi = {10.1007/BF02295613},
	id = {Bernaards2003},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {585--588},
	title = {Orthomax rotation and perfect simple structure},
	url = {https://doi.org/10.1007/BF02295613},
	volume = {68},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1007/BF02295613}}
@article{Zou+2006,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/27594179},
 abstract = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.},
 author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {265--286},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Sparse Principal Component Analysis},
 urldate = {2024-08-13},
 volume = {15},
 year = {2006}
}

@article{Jolliffe+2003,
	author = {Ian T Jolliffe, Nickolay T Trendafilov and Mudassir Uddin},
	doi = {10.1198/1061860032148},
	eprint = {https://doi.org/10.1198/1061860032148},
	journal = {Journal of Computational and Graphical Statistics},
	number = {3},
	pages = {531--547},
	publisher = {Taylor \& Francis},
	title = {A Modified Principal Component Technique Based on the LASSO},
	url = {https://doi.org/10.1198/1061860032148},
	volume = {12},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1198/1061860032148}}

@inproceedings{Bishop1998,
	author = {Bishop, Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Kearns and S. Solla and D. Cohn},
	publisher = {MIT Press},
	title = {Bayesian PCA},
	url = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf},
	volume = {11},
	year = {1998},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf}}
@article{Rattray+2009,
doi = {10.1088/1742-6596/197/1/012002},
url = {https://dx.doi.org/10.1088/1742-6596/197/1/012002},
year = {2009},
month = {dec},
publisher = {},
volume = {197},
number = {1},
pages = {012002},
author = {Magnus Rattray and  Oliver Stegle and  Kevin Sharp and  John Winn},
title = {Inference algorithms and learning theory for Bayesian sparse factor analysis},
journal = {Journal of Physics: Conference Series},
abstract = {Bayesian sparse factor analysis has many applications; for example, it has been applied to the problem of inferring a sparse regulatory network from gene expression data. We describe a number of inference algorithms for Bayesian sparse factor analysis using a slab and spike mixture prior. These include well-established Markov chain Monte Carlo (MCMC) and variational Bayes (VB) algorithms as well as a novel hybrid of VB and Expectation Propagation (EP). For the case of a single latent factor we derive a theory for learning performance using the replica method. We compare the MCMC and VB/EP algorithm results with simulated data to the theoretical prediction. The results for MCMC agree closely with the theory as expected. Results for VB/EP are slightly sub-optimal but show that the new algorithm is effective for sparse inference. In large-scale problems MCMC is infeasible due to computational limitations and the VB/EP algorithm then provides a very useful computationally efficient alternative.}
}

@inproceedings{Archambeau-Bach2008,
	author = {Archambeau, C\'{e}dric and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Sparse probabilistic projections},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf}}

@inproceedings{Collins+2001,
	author = {Collins, Michael and Dasgupta, S. and Schapire, Robert E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {A Generalization of Principal Components Analysis to the Exponential Family},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf}}

@article{Joreskog-Lawley1968,
	abstract = {Until recently the main difficulty in the use of maximum-likelihood estimation in factor analysis has been the lack of satisfactory methods of obtaining numerical solutions. This defect has now been remedied, and this paper describes new rapid methods of finding maximum-likelihood estimates.},
	author = {J{\"o}reskog, K. G. and Lawley, D. N.},
	doi = {https://doi.org/10.1111/j.2044-8317.1968.tb00399.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1968.tb00399.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {1},
	pages = {85-96},
	title = {New methods in maximum likelihood factor analysis},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1968.tb00399.x},
	volume = {21},
	year = {1968},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1968.tb00399.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.1968.tb00399.x}}

@article{Joreskog-vanThillo1972,
	author = {J{\H o}reskog, Karl G. and van Thiilo, Marielle},
	doi = {https://doi.org/10.1002/j.2333-8504.1972.tb00827.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1972.tb00827.x},
	journal = {ETS Research Bulletin Series},
	number = {2},
	pages = {i-71},
	title = {{LISREL: A General Computer Program for Estimating a Linear Structural Equation System Involving Multiple Indicators of Unmeasured Variables}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1972.tb00827.x},
	volume = {1972},
	year = {1972},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1972.tb00827.x},
	bdsk-url-2 = {https://doi.org/10.1002/j.2333-8504.1972.tb00827.x}}
@inproceedings{Yu+2006,
author = {Yu, Shipeng and Yu, Kai and Tresp, Volker and Kriegel, Hans-Peter and Wu, Mingrui},
title = {Supervised probabilistic principal component analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150454},
doi = {10.1145/1150402.1150454},
abstract = {Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {464–473},
numpages = {10},
keywords = {supervised projection, semi-supervised projection, principal component analysis, dimensionality reduction},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@article{Gustafsson2001,
	annote = {doi: 10.1021/ci0003909},
	author = {Gustafsson, Mats G. },
	date = {2001/03/01},
	date-added = {2024-08-13 16:43:51 +0900},
	date-modified = {2024-08-13 16:43:51 +0900},
	doi = {10.1021/ci0003909},
	isbn = {0095-2338},
	journal = {Journal of Chemical Information and Computer Sciences},
	journal1 = {Journal of Chemical Information and Computer Sciences},
	journal2 = {J. Chem. Inf. Comput. Sci.},
	month = {03},
	number = {2},
	pages = {288--294},
	publisher = {American Chemical Society},
	title = {A Probabilistic Derivation of the Partial Least-Squares Algorithm},
	type = {doi: 10.1021/ci0003909},
	url = {https://doi.org/10.1021/ci0003909},
	volume = {41},
	year = {2001},
	year1 = {2001},
	bdsk-url-1 = {https://doi.org/10.1021/ci0003909}}
@techreport{Bach-Jordan2005,
	author      = {Francis R. Bach and Michael I. Jordan},
	institution = {University of California, Berkeley},
	title       = {A Probabilistic Interpretation of Canonical Correlation Analysis},
	year        = {2005},
	url             = {https://statistics.berkeley.edu/tech-reports/688},
}

@article{Nounou+2002,
	abstract = {Abstract Large quantities of measured data are being routinely collected in various industries and used for extracting linear models for tasks such as process control, fault diagnosis, and process monitoring. Existing linear modeling methods, however, do not fully utilize all the information contained in the measurements. A new approach for linear process modeling makes maximum use of available process data and process knowledge. Bayesian latent-variable regression (BLVR) permits extraction and incorporation of knowledge about the statistical behavior of measurements in developing linear process models. Furthermore, BLVR can handle noise in inputs and outputs, collinear variables, and incorporate prior knowledge about regression parameters and measured variables. The model is usually more accurate than those of existing methods, including OLS, PCR, and PLS. BLVR considers a univariate output and assumes the underlying variables and noise to be Gaussian, but it can be used for multivariate outputs and other distributions. An empirical Bayes approach is developed to extract the prior information from historical data or maximum-likelihood solution of available data. Examples of steady-state, dynamic and inferential modeling demonstrate the superior accuracy of BLVR over existing methods even when the assumptions of Gaussian distributions are violated. The relationship between BLVR and existing methods and opportunities for future work based on this framework are also discussed.},
	author = {Nounou, Mohamed N. and Bakshi, Bhavik R. and Goel, Prem K. and Shen, Xiaotong},
	doi = {https://doi.org/10.1002/aic.690480818},
	eprint = {https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690480818},
	journal = {AIChE Journal},
	number = {8},
	pages = {1775-1793},
	title = {Process modeling by Bayesian latent variable regression},
	url = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690480818},
	volume = {48},
	year = {2002},
	bdsk-url-1 = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690480818},
	bdsk-url-2 = {https://doi.org/10.1002/aic.690480818}}
@article{Horst1961,
author = {Horst, Paul},
title = {Generalized canonical correlations and their applications to experimental data},
journal = {Journal of Clinical Psychology},
volume = {17},
number = {4},
pages = {331-347},
doi = {https://doi.org/10.1002/1097-4679(196110)17:4<331::AID-JCLP2270170402>3.0.CO;2-D},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28196110%2917%3A4%3C331%3A%3AAID-JCLP2270170402%3E3.0.CO%3B2-D},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28196110%2917%3A4%3C331%3A%3AAID-JCLP2270170402%3E3.0.CO%3B2-D},
year = {1961}
}

@inproceedings{Klami+2010,
author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
title = {Bayesian exponential family projections for coupled data sources},
year = {2010},
isbn = {9780974903965},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Exponential family extensions of principal component analysis (EPCA) have received a considerable amount of attention in recent years, demonstrating the growing need for basic modeling tools that do not assume the squared loss or Gaussian distribution. We extend the EPCA model toolbox by presenting the first exponential family multi-view learning methods of the partial least squares and canonical correlation analysis, based on a unified representation of EPCA as matrix factorization of the natural parameters of exponential family. The models are based on a new family of priors that are generally usable for all such factorizations. We also introduce new inference strategies, and demonstrate how the methods outperform earlier ones when the Gaussianity assumption does not hold.},
booktitle = {Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
pages = {286–293},
numpages = {8},
location = {Catalina Island, CA},
series = {UAI'10}
}
@misc{Wang+2017,
title={Deep Variational Canonical Correlation Analysis},
author={Weiran Wang and Xinchen Yan and Honglak Lee and Karen Livescu},
year={2017},
url={https://openreview.net/forum?id=H1Heentlx}
}
@misc{Suzuki+2017,
title={Joint Multimodal Learning with Deep Generative Models},
author={Masahiro Suzuki and Kotaro Nakayama and Yutaka Matsuo},
year={2017},
url={https://openreview.net/forum?id=Hk8rlUqge}
}
@inproceedings{Sun+2009,
author = {Sun, Liang and Ji, Shuiwang and Yu, Shipeng and Ye, Jieping},
title = {On the equivalence between canonical correlation analysis and orthonormalized partial least squares},
year = {2009},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Canonical correlation analysis (CCA) and partial least squares (PLS) are well-known techniques for feature extraction from two sets of multi-dimensional variables. The fundamental difference between CCA and PLS is that CCA maximizes the correlation while PLS maximizes the covariance. Although both CCA and PLS have been applied successfully in various applications, the intrinsic relationship between them remains unclear. In this paper, we attempt to address this issue by showing the equivalence relationship between CCA and orthonormalized partial least squares (OPLS), a variant of PLS. We further extend the equivalence relationship to the case when regularization is employed for both sets of variables. In addition, we show that the CCA projection for one set of variables is independent of the regularization on the other set of variables. We have performed experimental studies using both synthetic and real data sets and our results confirm the established equivalence relationship. The presented analysis provides novel insights into the connection between these two existing algorithms as well as the effect of the regularization.},
booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence},
pages = {1230–1235},
numpages = {6},
location = {Pasadena, California, USA},
series = {IJCAI'09}
}

@InProceedings{Hoffman2017,
  title = 	 {Learning Deep Latent {G}aussian Models with {M}arkov Chain {M}onte {C}arlo},
  author =       {Matthew D. Hoffman},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1510--1519},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/hoffman17a.html},
  abstract = 	 {Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC’s additional computational overhead proves to be significant, but not prohibitive.}
}
@inproceedings{Canny2004,
author = {Canny, John},
title = {GaP: a factor model for discrete data},
year = {2004},
isbn = {1581138814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008992.1009016},
doi = {10.1145/1008992.1009016},
abstract = {We present a probabilistic model for a document corpus that combines many of the desirable features of previous models. The model is called "GaP" for Gamma-Poisson, the distributions of the first and last random variable. GaP is a factor model, that is it gives an approximate factorization of the document-term matrix into a product of matrices Λ and X. These factors have strictly non-negative terms. GaP is a generative probabilistic model that assigns finite probabilities to documents in a corpus. It can be computed with an efficient and simple EM recurrence. For a suitable choice of parameters, the GaP factorization maximizes independence between the factors. So it can be used as an independent-component algorithm adapted to document data. The form of the GaP model is empirically as well as analytically motivated. It gives very accurate results as a probabilistic model (measured via perplexity) and as a retrieval model. The GaP model projects documents and terms into a low-dimensional space of "themes," and models texts as "passages" of terms on the same theme.},
booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {122–129},
numpages = {8},
keywords = {probabilistic models, latent semantic analysis, EM algorithm},
location = {Sheffield, United Kingdom},
series = {SIGIR '04}
}

@article{Paatero-Tapper1994,
	abstract = {Abstract A new variant `PMF' of factor analysis is described. It is assumed that X is a matrix of observed data and σ is the known matrix of standard deviations of elements of X. Both X and σ are of dimensions n × m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n × p, F is the unknown right hand factor matrix (loadings) of dimensions p × m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by σ is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
	author = {Paatero, Pentti and Tapper, Unto},
	doi = {https://doi.org/10.1002/env.3170050203},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.3170050203},
	journal = {Environmetrics},
	keywords = {Factor analysis, Principal component analysis, Weighted least squares, Alternating regression, Error estimates, Scaling, Repetitive measurements},
	number = {2},
	pages = {111-126},
	title = {Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	volume = {5},
	year = {1994},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	bdsk-url-2 = {https://doi.org/10.1002/env.3170050203}}

@inproceedings{Buntine-Jakulin2006,
	abstract = {This article presents a unified theory for analysis of components in discrete data, and compares the methods with techniques such as independent component analysis, non-negative matrix factorisation and latent Dirichlet allocation. The main families of algorithms discussed are a variational approximation, Gibbs sampling, and Rao-Blackwellised Gibbs sampling. Applications are presented for voting records from the United States Senate for 2003, and for the Reuters-21578 newswire collection.},
	address = {Berlin, Heidelberg},
	author = {Buntine, Wray and Jakulin, Aleks},
	booktitle = {Subspace, Latent Structure and Feature Selection},
	editor = {Saunders, Craig and Grobelnik, Marko and Gunn, Steve and Shawe-Taylor, John},
	isbn = {978-3-540-34138-3},
	pages = {1--33},
	publisher = {Springer Berlin Heidelberg},
	title = {Discrete Component Analysis},
	year = {2006}}

@article{Bhattacharya-Dunson2012,
	author = {Anirban Bhattacharya and David B. Dunson},
	doi = {10.1080/01621459.2011.646934},
	eprint = {https://doi.org/10.1080/01621459.2011.646934},
	journal = {Journal of the American Statistical Association},
	note = {PMID: 23908561},
	number = {497},
	pages = {362--377},
	publisher = {Taylor \& Francis},
	title = {Simplex Factor Models for Multivariate Unordered Categorical Data},
	url = {https://doi.org/10.1080/01621459.2011.646934},
	volume = {107},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2011.646934}}

@article{Erosheva+2004,
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	author = {Elena Erosheva and Stephen Fienberg and John Lafferty},
	doi = {10.1073/pnas.0307760101},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0307760101},
	journal = {Proceedings of the National Academy of Sciences},
	number = {suppl\_1},
	pages = {5220-5227},
	title = {Mixed-membership models of scientific publications},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0307760101},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0307760101},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0307760101}}
@article{Pritchard+2000,
    author = {Pritchard, Jonathan K and Stephens, Matthew and Donnelly, Peter},
    title = "{Inference of Population Structure Using Multilocus Genotype Data}",
    journal = {Genetics},
    volume = {155},
    number = {2},
    pages = {945-959},
    year = {2000},
    month = {06},
    abstract = "{We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci—e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/~pritch/home.html.}",
    issn = {1943-2631},
    doi = {10.1093/genetics/155.2.945},
    url = {https://doi.org/10.1093/genetics/155.2.945},
    eprint = {https://academic.oup.com/genetics/article-pdf/155/2/945/42030266/genetics0945.pdf},
}


@inproceedings{Marlin2003,
	author = {Marlin, Benjamin M},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
	publisher = {MIT Press},
	title = {Modeling User Rating Profiles For Collaborative Filtering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf},
	volume = {16},
	year = {2003},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf}}


@article{Hyvarinen-Oja2000,
	abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
	author = {A. Hyv{\"a}rinen and E. Oja},
	doi = {https://doi.org/10.1016/S0893-6080(00)00026-5},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Independent component analysis, Projection pursuit, Blind signal separation, Source separation, Factor analysis, Representation},
	number = {4},
	pages = {411-430},
	title = {Independent component analysis: algorithms and applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	volume = {13},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	bdsk-url-2 = {https://doi.org/10.1016/S0893-6080(00)00026-5}}
@ARTICLE{Friedman-Tukey1974,
  author={Friedman, J.H. and Tukey, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Projection Pursuit Algorithm for Exploratory Data Analysis}, 
  year={1974},
  volume={C-23},
  number={9},
  pages={881-890},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric pattern recognition, statistics.},
  doi={10.1109/T-C.1974.224051}}

@inproceedings{Ricahrdson-Weiss2018,
	author = {Richardson, Eitan and Weiss, Yair},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On GANs and GMMs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf}}
@inproceedings{Zong+2018,
title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},
author={Bo Zong and Qi Song and Martin Renqiang Min and Wei Cheng and Cristian Lumezanu and Daeki Cho and Haifeng Chen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJJLHbb0-},
}
@inproceedings{Paisley-Carin2009,
author = {Paisley, John and Carin, Lawrence},
title = {Nonparametric factor analysis with beta process priors},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553474},
doi = {10.1145/1553374.1553474},
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {777–784},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}
@misc{Chen+2024,
      title={DiJiang: Efficient Large Language Models through Compact Kernelization}, 
      author={Hanting Chen and Zhicheng Liu and Xutao Wang and Yuchuan Tian and Yunhe Wang},
      year={2024},
      eprint={2403.19928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19928}, 
}
@INPROCEEDINGS{Zoran-Weiss2011,
  author={Zoran, Daniel and Weiss, Yair},
  booktitle={2011 International Conference on Computer Vision}, 
  title={From learning models of natural image patches to whole image restoration}, 
  year={2011},
  volume={},
  number={},
  pages={479-486},
  keywords={Image restoration;Noise reduction;Equations;Noise measurement;Image reconstruction;Mathematical model;Estimation},
  doi={10.1109/ICCV.2011.6126278}}
@ARTICLE{Papyam-Elad2016,
  author={Papyan, Vardan and Elad, Michael},
  journal={IEEE Transactions on Image Processing}, 
  title={Multi-Scale Patch-Based Image Restoration}, 
  year={2016},
  volume={25},
  number={1},
  pages={249-261},
  keywords={Noise reduction;Image restoration;Mathematical model;Closed-form solutions;Approximation methods;Context awareness;Image restoration;expected patch log likelihood (EPLL);Gaussian mixture model;multi-scale;denoising;deblurring;super-resolution;Image restoration;expected patch log likelihood (EPLL);Gaussian mixture model;multi-scale;denoising;deblurring;super-resolution},
  doi={10.1109/TIP.2015.2499698}}
@article{Deerwester+1990,
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
title = {Indexing by latent semantic analysis},
journal = {Journal of the American Society for Information Science},
volume = {41},
number = {6},
pages = {391-407},
doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
year = {1990}
}

@inproceedings{Hofmann1999,
author = {Hofmann, Thomas},
title = {Probabilistic latent semantic indexing},
year = {1999},
isbn = {1581130961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/312624.312649},
doi = {10.1145/312624.312649},
booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {50–57},
numpages = {8},
location = {Berkeley, California, USA},
series = {SIGIR '99}
}
@article{Blei+2003,
	author          = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	year            = {2003},
	title           = {{Latent Dirichlet Allocation}},
	journal         = {Journal of Machine Learning Research},
	volume          = {3},
	number          = {},
	pages           = {993-1022},
	url             = {https://www.jmlr.org/papers/v3/blei03a.html}
}
@article{Blei2012,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = {apr},
pages = {77–84},
numpages = {8}
}

@article{Church-Gale1991,
	abstract = {This paper describes a new program, CORRECT, which takes words rejected by the Unix{\textregistered}SPELL program, proposes a list of candidate corrections, and sorts them by probability score. The probability scores are the novel contribution of this work. They are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in recognition applications, especially speech recognition (Jelinek, 1985), one can often recover the intended correction,c, from a typo,t, by finding the correctionc that maximizesPr(c) Pr(t/c). The first factor,Pr(c), is a prior model of word probabilities; the second factor,Pr(t/c), is a model of the noisy channel that accounts for spelling transformations on letter sequences (insertions, deletions, substitutions and reversals). Both sets of probabilities were estimated using data collected from the Associated Press (AP) newswire over 1988 and 1989 as a training set. The AP generates about 1 million words and 500 typos per week.},
	author = {Church, Kenneth W. and Gale, William A.},
	date = {1991/12/01},
	date-added = {2024-08-13 22:12:23 +0900},
	date-modified = {2024-08-13 22:12:23 +0900},
	doi = {10.1007/BF01889984},
	id = {Church1991},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {2},
	pages = {93--103},
	title = {Probability scoring for spelling correction},
	url = {https://doi.org/10.1007/BF01889984},
	volume = {1},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1007/BF01889984}}

@InProceedings{Arova+2013,
  title = 	 {A Practical Algorithm for Topic Modeling with Provable Guarantees},
  author = 	 {Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {280--288},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/arora13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/arora13.html},
  abstract = 	 {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.}
}

@article{Griffith-Steyvers2004,
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \&amp; Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	author = {Thomas L. Griffiths and Mark Steyvers},
	doi = {10.1073/pnas.0307752101},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0307752101},
	journal = {Proceedings of the National Academy of Sciences},
	number = {suppl\_1},
	pages = {5228-5235},
	title = {Finding scientific topics},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0307752101}}
@inproceedings{Blei+2006,
author = {Blei, David M. and Lafferty, John D.},
title = {Dynamic topic models},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143859},
doi = {10.1145/1143844.1143859},
abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {113–120},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@inproceedings{Griffiths+2004,
	author = {Griffiths, Thomas and Steyvers, Mark and Blei, David and Tenenbaum, Joshua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	publisher = {MIT Press},
	title = {Integrating Topics and Syntax},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf}}
@inproceedings{Dieng+2017,
title={Topic{RNN}: A Recurrent Neural Network with Long-Range Semantic Dependency},
author={Adji B. Dieng and Chong Wang and Jianfeng Gao and John Paisley},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJbbOLcex}
}
@article{Zou-Hastie2005,
    author = {Zou, Hui and Hastie, Trevor},
    title = "{Regularization and Variable Selection Via the Elastic Net}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {67},
    number = {2},
    pages = {301-320},
    year = {2005},
    month = {03},
    abstract = "{We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.}",
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2005.00503.x},
    url = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/67/2/301/49795094/jrsssb\_67\_2\_301.pdf},
}
@book{豊田秀樹2009,
    author         = {豊田秀樹},
    title          = {共分散構造分析［実践編］},
    year           = {2009},
    publisher      = {朝倉書店},
    series         = {統計ライブラリー},
    volume         = {},
    month          = {10},
    edition        = {},
    howpublished   = {}
}
@book{豊田秀樹2007,
    author         = {豊田秀樹},
    title          = {共分散構造分析［理論編］},
    year           = {2007},
    publisher      = {朝倉書店},
    series         = {統計ライブラリー},
    volume         = {},
    month          = {10},
    edition        = {},
    howpublished   = {}
}

@book{Herbert-Simon57-ModelsOfMan,
    author         = {Herbert Simon},
    title          = {Models of man; social and rational.},
    year           = {1957},
    publisher      = {Wiley},
    series         = {},
    volume         = {},
    month          = {},
    edition        = {},
    howpublished   = {}
}

@article{岩瀬-中山2016,
	author          = {岩瀬智亮 and 中山英樹},
	year            = {2016},
	title           = {深層一般化正準相関分析},
	journal         = {情報処理学会第78回全国大会講演論文集},
	volume          = {2016},
	number          = {1},
	pages           = {183-184},
	url             = {http://id.nii.ac.jp/1001/00162588/}
}

@InProceedings{Andrew+2013,
  title = 	 {Deep Canonical Correlation Analysis},
  author = 	 {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1247--1255},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/andrew13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/andrew13.html},
  abstract = 	 {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}
}

@article{赤穂昭太郎2013,
	author = {赤穂昭太郎},
	doi = {10.3902/jnns.20.62},
	journal = {日本神経回路学会誌},
	number = {2},
	pages = {62-72},
	title = {正準相関分析入門},
	volume = {20},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.3902/jnns.20.62}}

@article{Sei-Yano2024,
	author = {Tomonari Sei and Keisuke Yano},
	doi = {10.3150/23-BEJ1687},
	journal = {Bernoulli},
	keywords = {conditional inference, copula, earthquake data, Graphical model, mixed-domain, Monte Carlo method},
	number = {4},
	pages = {2623 -- 2643},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	title = {{Minimum information dependence modeling}},
	url = {https://doi.org/10.3150/23-BEJ1687},
	volume = {30},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.3150/23-BEJ1687}}

@article{清智也2021,
	author = {清智也},
	doi = {10.11329/jjssj.51.75},
	journal = {日本統計学会誌},
	number = {1},
	pages = {75-99},
	title = {最小情報コピュラとその周辺},
	volume = {51},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.11329/jjssj.51.75}}

@article{Bedford+2016,
	abstract = {Many applications of risk analysis require us to jointly model multiple uncertain quantities. Bayesian networks and copulas are two common approaches to modeling joint uncertainties with probability distributions. This article focuses on new methodologies for copulas by developing work of Cooke, Bedford, Kurowica, and others on vines as a way of constructing higher dimensional distributions that do not suffer from some of the restrictions of alternatives such as the multivariate Gaussian copula. The article provides a fundamental approximation result, demonstrating that we can approximate any density as closely as we like using vines. It further operationalizes this result by showing how minimum information copulas can be used to provide parametric classes of copulas that have such good levels of approximation. We extend previous approaches using vines by considering nonconstant conditional dependencies, which are particularly relevant in financial risk modeling. We discuss how such models may be quantified, in terms of expert judgment or by fitting data, and illustrate the approach by modeling two financial data sets.},
	author = {Bedford, Tim and Daneshkhah, Alireza and Wilson, Kevin J.},
	doi = {https://doi.org/10.1111/risa.12471},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/risa.12471},
	journal = {Risk Analysis},
	keywords = {Copula, entropy, information, risk modeling, vine},
	number = {4},
	pages = {792-815},
	title = {Approximate Uncertainty Modeling in Risk Analysis with Vine Copulas},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12471},
	volume = {36},
	year = {2016},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12471},
	bdsk-url-2 = {https://doi.org/10.1111/risa.12471}}
@article{江口真透1999,
	author          = {江口真透},
	year            = {1999},
	title           = {概パラメトリック推測 － 柔らかなモデルの構築 －},
	journal         = {統計数理},
	volume          = {47},
	number          = {1},
	pages           = {29-48},
	url             = {http://hdl.handle.net/10787/295}
}

@article{飽戸弘1966,
	author = {飽戸弘},
	doi = {10.4992/jjpsy.37.204},
	journal = {心理学研究},
	number = {4},
	pages = {204-218},
	title = {政治的態度の構造に関する研究 I},
	volume = {37},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.4992/jjpsy.37.204}}

@article{Young-Hausholder1938,
	abstract = {Necessary and sufficient conditions are given for a set of numbers to be the mutual distances of a set of real points in Euclidean space, and matrices are found whose ranks determine the dimension of the smallest Euclidean space containing such points. Methods are indicated for determining the configuration of these points, and for approximating to them by points in a space of lower dimensionality.},
	author = {Young, Gale and Householder, A. S.},
	date = {1938/03/01},
	date-added = {2024-08-14 13:45:15 +0900},
	date-modified = {2024-08-14 13:45:15 +0900},
	doi = {10.1007/BF02287916},
	id = {Young1938},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {19--22},
	title = {Discussion of a set of points in terms of their mutual distances},
	url = {https://doi.org/10.1007/BF02287916},
	volume = {3},
	year = {1938},
	bdsk-url-1 = {https://doi.org/10.1007/BF02287916}}

@article{Unkel-Trendafilov2010,
	abstract = {Summary The classical exploratory factor analysis (EFA) finds estimates for the factor loadings matrix and the matrix of unique factor variances which give the best fit to the sample correlation matrix with respect to some goodness-of-fit criterion. Common factor scores can be obtained as a function of these estimates and the data. Alternatively to the classical EFA, the EFA model can be fitted directly to the data which yields factor loadings and common factor scores simultaneously. Recently, new algorithms were introduced for the simultaneous least squares estimation of all EFA model unknowns. The new methods are based on the numerical procedure for singular value decomposition of matrices and work equally well when the number of variables exceeds the number of observations. This paper provides an account that is intended as an expository review of methods for simultaneous parameter estimation in EFA. The methods are illustrated on Harman's five socio-economic variables data and a high-dimensional data set from genome research.},
	author = {Unkel, Steffen and Trendafilov, Nickolay T.},
	doi = {https://doi.org/10.1111/j.1751-5823.2010.00120.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2010.00120.x},
	journal = {International Statistical Review},
	keywords = {Factor analysis, indeterminacies, least squares estimation, matrix fitting problems, constrained optimization, principal component analysis, rotation},
	number = {3},
	pages = {363-382},
	title = {Simultaneous Parameter Estimation in Exploratory Factor Analysis: An Expository Review},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00120.x},
	volume = {78},
	year = {2010},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00120.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1751-5823.2010.00120.x}}
@misc{Ghojogh+2022,
      title={Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey}, 
      author={Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
      year={2022},
      eprint={2101.00734},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2101.00734}, 
}

@article{Kohonen1982,
	abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
	author = {Kohonen, Teuvo},
	date = {1982/01/01},
	date-added = {2024-08-14 14:28:55 +0900},
	date-modified = {2024-08-14 14:28:55 +0900},
	doi = {10.1007/BF00337288},
	id = {Kohonen1982},
	isbn = {1432-0770},
	journal = {Biological Cybernetics},
	number = {1},
	pages = {59--69},
	title = {Self-organized formation of topologically correct feature maps},
	url = {https://doi.org/10.1007/BF00337288},
	volume = {43},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF00337288}}

@article{Kohonen2013,
	abstract = {The self-organizing map (SOM) is an automatic data-analysis method. It is widely applied to clustering problems and data exploration in industry, finance, natural sciences, and linguistics. The most extensive applications, exemplified in this paper, can be found in the management of massive textual databases and in bioinformatics. The SOM is related to the classical vector quantization (VQ), which is used extensively in digital signal processing and transmission. Like in VQ, the SOM represents a distribution of input data items using a finite set of models. In the SOM, however, these models are automatically associated with the nodes of a regular (usually two-dimensional) grid in an orderly fashion such that more similar models become automatically associated with nodes that are adjacent in the grid, whereas less similar models are situated farther away from each other in the grid. This organization, a kind of similarity diagram of the models, makes it possible to obtain an insight into the topographic relationships of data, especially of high-dimensional data items. If the data items belong to certain predetermined classes, the models (and the nodes) can be calibrated according to these classes. An unknown input item is then classified according to that node, the model of which is most similar with it in some metric used in the construction of the SOM. A new finding introduced in this paper is that an input item can even more accurately be represented by a linear mixture of a few best-matching models. This becomes possible by a least-squares fitting procedure where the coefficients in the linear mixture of models are constrained to nonnegative values.},
	author = {Teuvo Kohonen},
	doi = {https://doi.org/10.1016/j.neunet.2012.09.018},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Self-organizing map, SOM, Data analysis, Brain map, Similarity, Vector quantization},
	note = {Twenty-fifth Anniversay Commemorative Issue},
	pages = {52-65},
	title = {Essentials of the self-organizing map},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	volume = {37},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2012.09.018}}
@article{Baum+1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239727},
 author = {Leonard E. Baum and Ted Petrie and George Soules and Norman Weiss},
 journal = {The Annals of Mathematical Statistics},
 number = {1},
 pages = {164--171},
 publisher = {Institute of Mathematical Statistics},
 title = {A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains},
 urldate = {2024-08-14},
 volume = {41},
 year = {1970}
}
@unpublished{Murphy-Linderman2022,
	author = {Kevin Murphy and Scott Linderman},
	year   = {2022},
	title  = {State Space Models: A Modern Approach},
	url    = {https://github.com/probml/ssm-book}
}

@article{Hsu+2012,
	abstract = {Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.},
	author = {Daniel Hsu and Sham M. Kakade and Tong Zhang},
	doi = {https://doi.org/10.1016/j.jcss.2011.12.025},
	issn = {0022-0000},
	journal = {Journal of Computer and System Sciences},
	keywords = {Hidden Markov Models, Latent variable models, Observable operator models, Time series, Spectral algorithm, Singular value decomposition, Learning probability distributions, Unsupervised learning},
	note = {JCSS Special Issue: Cloud Computing 2011},
	number = {5},
	pages = {1460-1480},
	title = {A spectral algorithm for learning Hidden Markov Models},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
	volume = {78},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcss.2011.12.025}}

@InProceedings{Anandkumar+2012,
  title = 	 {A Method of Moments for Mixture Models and Hidden Markov Models},
  author = 	 {Anandkumar, Animashree and Hsu, Daniel and Kakade, Sham M.},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {33.1--33.34},
  year = 	 {2012},
  editor = 	 {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/anandkumar12/anandkumar12.pdf},
  url = 	 {https://proceedings.mlr.press/v23/anandkumar12.html},
  abstract = 	 {Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (\emphe.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient \emphmethod of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment.}
}

@inproceedings{Anandkumar+2015,
	abstract = {This note is a short version of that in [1]. It is intended as a survey for the 2015 Algorithmic Learning Theory (ALT) conference.},
	address = {Cham},
	author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	booktitle = {Algorithmic Learning Theory},
	editor = {Chaudhuri, Kamalika and GENTILE, CLAUDIO and Zilles, Sandra},
	isbn = {978-3-319-24486-0},
	pages = {19--38},
	publisher = {Springer International Publishing},
	title = {Tensor Decompositions for Learning Latent Variable Models (A Survey for ALT)},
	year = {2015}}

@InProceedings{Obermeyer+2019,
  title = 	 {Tensor Variable Elimination for Plated Factor Graphs},
  author =       {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Pradhan, Neeraj and Chiu, Justin and Rush, Alexander and Goodman, Noah},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4871--4880},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/obermeyer19a/obermeyer19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/obermeyer19a.html},
  abstract = 	 {A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graphical models. To exploit efficient tensor algebra in graphs with plates of variables, we generalize undirected factor graphs to plated factor graphs and variable elimination to a tensor variable elimination algorithm that operates directly on plated factor graphs. Moreover, we generalize complexity bounds based on treewidth and characterize the class of plated factor graphs for which inference is tractable. As an application, we integrate tensor variable elimination into the Pyro probabilistic programming language to enable exact inference in discrete latent variable models with repeated structure. We validate our methods with experiments on both directed and undirected graphical models, including applications to polyphonic music modeling, animal movement modeling, and latent sentiment analysis.}
}

@article{Scott2002,
	author = {Steven L Scott},
	doi = {10.1198/016214502753479464},
	eprint = {https://doi.org/10.1198/016214502753479464},
	journal = {Journal of the American Statistical Association},
	number = {457},
	pages = {337--351},
	publisher = {Taylor \& Francis},
	title = {Bayesian Methods for Hidden Markov Models},
	url = {https://doi.org/10.1198/016214502753479464},
	volume = {97},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1198/016214502753479464}}

@inproceedings{Gu+2020,
	author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1474--1487},
	publisher = {Curran Associates, Inc.},
	title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf}}

@InProceedings{Goel+2022,
  title = 	 {It’s Raw! {A}udio Generation with State-Space Models},
  author =       {Goel, Karan and Gu, Albert and Donahue, Chris and Re, Christopher},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {7616--7633},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/goel22a/goel22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/goel22a.html},
  abstract = 	 {Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2{\texttimes} better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3{\texttimes} fewer parameters}
}
@unpublished{Oudot2016,
	author = {Steve Oudot},
	year   = {2016},
	title  = {Reeb Graph and Mapper},
	url    = {https://geometrica.saclay.inria.fr/team/Steve.Oudot/courses/EMA/Slides_Reeb_Mapper.pdf}
}
@unpublished{Schnider2024,
	author = {Patrick Schnider},
	year   = {2024},
	title  = {Introduction to Topological Data Analysi},
	url    = {https://ti.inf.ethz.ch/ew/courses/TDA24/index.html}
}
@inproceedings{Tang+2016,
author = {Tang, Jian and Liu, Jingzhou and Zhang, Ming and Mei, Qiaozhu},
title = {Visualizing Large-scale and High-dimensional Data},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883041},
doi = {10.1145/2872427.2883041},
abstract = {We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {287–297},
numpages = {11},
keywords = {visualization, high-dimensional data, big data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}
@inproceedings{DasGupta-Freud2008,
author = {Dasgupta, Sanjoy and Freund, Yoav},
title = {Random projection trees and low dimensional manifolds},
year = {2008},
isbn = {9781605580470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1374376.1374452},
doi = {10.1145/1374376.1374452},
abstract = {We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure.},
booktitle = {Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing},
pages = {537–546},
numpages = {10},
keywords = {curse of dimension, k-d tree, manifold, random projection},
location = {Victoria, British Columbia, Canada},
series = {STOC '08}
}

@article{Saul2020,
	abstract = {Latent variable models (LVMs) are powerful tools for discovering hidden structure in data. Canonical LVMs include factor analysis, which explains the correlation of a large number of observed variables in terms of a smaller number of unobserved ones, and Gaussian mixture models, which reveal clusters of data arising from an underlying multimodal distribution. In this paper, we describe a conceptually simple and equally effective LVM for nonlinear dimensionality reduction (NLDR), where the goal is to discover faithful, neighborhood-preserving embeddings of high-dimensional data. Tools for NLDR can help researchers across all areas of science and engineering to better understand and visualize their data. Our approach elevates NLDR into the family of problems that can be studied by especially tractable LVMs. We propose a latent variable model to discover faithful low-dimensional representations of high-dimensional data. The model computes a low-dimensional embedding that aims to preserve neighborhood relationships encoded by a sparse graph. The model both leverages and extends current leading approaches to this problem. Like t-distributed Stochastic Neighborhood Embedding, the model can produce two- and three-dimensional embeddings for visualization, but it can also learn higher-dimensional embeddings for other uses. Like LargeVis and Uniform Manifold Approximation and Projection, the model produces embeddings by balancing two goals---pulling nearby examples closer together and pushing distant examples further apart. Unlike these approaches, however, the latent variables in our model provide additional structure that can be exploited for learning. We derive an Expectation--Maximization procedure with closed-form updates that monotonically improve the model's likelihood: In this procedure, embeddings are iteratively adapted by solving sparse, diagonally dominant systems of linear equations that arise from a discrete graph Laplacian. For large problems, we also develop an approximate coarse-graining procedure that avoids the need for negative sampling of nonadjacent nodes in the graph. We demonstrate the model's effectiveness on datasets of images and text.},
	author = {Lawrence K. Saul},
	doi = {10.1073/pnas.1916012117},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1916012117},
	journal = {Proceedings of the National Academy of Sciences},
	number = {27},
	pages = {15403-15408},
	title = {A tractable latent variable model for nonlinear dimensionality reduction},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1916012117},
	volume = {117},
	year = {2020},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1916012117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1916012117}}

@article{Tutte1963,
	author = {Tutte, W. T.},
	doi = {https://doi.org/10.1112/plms/s3-13.1.743},
	eprint = {https://londmathsoc.onlinelibrary.wiley.com/doi/pdf/10.1112/plms/s3-13.1.743},
	journal = {Proceedings of the London Mathematical Society},
	number = {1},
	pages = {743-767},
	title = {{How to Draw a Graph}},
	url = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-13.1.743},
	volume = {s3-13},
	year = {1963},
	bdsk-url-1 = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-13.1.743},
	bdsk-url-2 = {https://doi.org/10.1112/plms/s3-13.1.743}}
@article{Eades1984,
	author          = {Peter Eades},
	year            = {1984},
	title           = {{A Heuristic for Graph Drawing}},
	journal         = {Congress Numerantium},
	volume          = {42},
	number          = {11},
	pages           = {149-160},
	url             = {https://www.cs.ubc.ca/~will/536E/papers/Eades1984.pdf}
}

@article{Kamada-Kawai1989,
	author = {Tomihisa Kamada and Satoru Kawai},
	doi = {https://doi.org/10.1016/0020-0190(89)90102-6},
	issn = {0020-0190},
	journal = {Information Processing Letters},
	keywords = {Graph, network structure, layout, drawing algorithm},
	number = {1},
	pages = {7-15},
	title = {An algorithm for drawing general undirected graphs},
	url = {https://www.sciencedirect.com/science/article/pii/0020019089901026},
	volume = {31},
	year = {1989},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0020019089901026},
	bdsk-url-2 = {https://doi.org/10.1016/0020-0190(89)90102-6}}
@ARTICLE{Fisk+1967,
  author={Fisk, C.J. and Caskey, D.L. and West, L.E.},
  journal={Proceedings of the IEEE}, 
  title={ACCEL: Automated circuit card etching layout}, 
  year={1967},
  volume={55},
  number={11},
  pages={1971-1982},
  keywords={Etching;Printed circuits;Assembly;Insulation;Joining processes;Libraries;Production systems;Drilling;Connectors;Contacts},
  doi={10.1109/PROC.1967.6027}}
@ARTICLE{Quinn-Breuer1979,
  author={Quinn, N. and Breuer, M.},
  journal={IEEE Transactions on Circuits and Systems}, 
  title={A forced directed component placement procedure for printed circuit boards}, 
  year={1979},
  volume={26},
  number={6},
  pages={377-388},
  keywords={Printed circuits;Equations;Wire;Personal communication networks;DH-HEMTs;Environmental management;Ceramics;Routing;Length measurement},
  doi={10.1109/TCS.1979.1084652}}

@article{Lee-Seung1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	date = {1999/10/01},
	date-added = {2024-08-14 16:57:13 +0900},
	date-modified = {2024-08-14 16:57:13 +0900},
	doi = {10.1038/44565},
	id = {Lee1999},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6755},
	pages = {788--791},
	title = {Learning the parts of objects by non-negative matrix factorization},
	url = {https://doi.org/10.1038/44565},
	volume = {401},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1038/44565}}
@inbook{Howard-Matheson1981,
	author         = {R. A. Howard and J. E. Matheson},
	chapter        = {Influence Diagrams},
	editor         = {R. A. Howard and J. E. Matheson},
	pages          = {},
	publisher      = {Strategic Decision Group},
	title          = {Readings on the Principles and Applications of Decision Analysis},
	year           = {1981}
}
@INPROCEEDINGS{Gori+2005,
  author={Gori, M. and Monfardini, G. and Scarselli, F.},
  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, 
  title={A new model for learning in graph domains}, 
  year={2005},
  volume={2},
  number={},
  pages={729-734 vol. 2},
  keywords={Neural networks;Focusing;Application software;Machine learning;Recurrent neural networks;Encoding;Data structures;Machine learning algorithms;Tree graphs;Software engineering},
  doi={10.1109/IJCNN.2005.1555942}}
@ARTICLE{Scarselli+2009,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}

@InProceedings{Gilmer+2017,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
@article{Battahlia+2018,title	= {Relational inductive biases, deep learning, and graph networks},author	= {Peter Battaglia and Jessica Blake Chandler Hamrick and Victor Bapst and Alvaro Sanchez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andy Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Jayne Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},year	= {2018},URL	= {https://arxiv.org/pdf/1806.01261.pdf},journal	= {arXiv}}

@inproceedings{Bruna+2014,
	author          = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
	year            = {2014},
	title           = {Spectral Networks and Locally Connected Networks on Graphs},
	booktitle       = {International Conference on Learning Representation},
	volume          = {},
	pages           = {},
	url             = {https://openreview.net/forum?id=DQNsQf-UsoDBa}
}
@inproceedings{Kipf-Welling2017,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}

@inproceedings{Hamilton+2017,
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Inductive Representation Learning on Large Graphs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf}}
@inproceedings{Velickovic+2018,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}
@INPROCEEDINGS{Masci+2015,
  author={Masci, Jonathan and Boscaini, Davide and Bronstein, Michael M. and Vandergheynst, Pierre},
  booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)}, 
  title={Geodesic Convolutional Neural Networks on Riemannian Manifolds}, 
  year={2015},
  volume={},
  number={},
  pages={832-840},
  keywords={Shape;Manifolds;Heating;Eigenvalues and eigenfunctions;Kernel;Neural networks;Geometry},
  doi={10.1109/ICCVW.2015.112}}

@inproceedings{Boscaini+2016,
	author = {Boscaini, Davide and Masci, Jonathan and Rodol\`{a}, Emanuele and Bronstein, Michael},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning shape correspondence with anisotropic convolutional neural networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf}}
@INPROCEEDINGS{Monti+2017,
  author={Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs}, 
  year={2017},
  volume={},
  number={},
  pages={5425-5434},
  keywords={Manifolds;Machine learning;Convolution;Laplace equations;Three-dimensional displays;Shape;Computational modeling},
  doi={10.1109/CVPR.2017.576}}

@inproceedings{Chami+2019,
	author = {Chami, Ines and Ying, Zhitao and R\'{e}, Christopher and Leskovec, Jure},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Hyperbolic Graph Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf}}

@inproceedings{Liu+2019,
	author = {Liu, Qi and Nickel, Maximilian and Kiela, Douwe},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Hyperbolic Graph Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf}}
@article{Bell-Sejnowski1995,
    author = {Bell, Anthony J. and Sejnowski, Terrence J.},
    title = "{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}",
    journal = {Neural Computation},
    volume = {7},
    number = {6},
    pages = {1129-1159},
    year = {1995},
    month = {11},
    abstract = "{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1995.7.6.1129},
    url = {https://doi.org/10.1162/neco.1995.7.6.1129},
    eprint = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
}
@unpublished{Yao2011,
	author = {Yuan Yao},
	year   = {2011},
	title  = {Mathematics of Data -- Laplacian, Diffusion, and Hessian LLE},
	note            = {Lecture 10},
	url    = {https://www.math.pku.edu.cn/teachers/yaoy/Spring2011/}
}

@article{Jordan-Kinderlehrer-Otto1998,
	abstract = { The Fokker--Planck equation, or forward Kolmogorov equation, describes the evolution of the probability density for a stochastic process associated with an Ito stochastic differential equation. It pertains to a wide variety of time-dependent systems in which randomness plays a role. In this paper, we are concerned with Fokker--Planck equations for which the drift term is given by the gradient of a potential. For a broad class of potentials, we construct a time discrete, iterative variational scheme whose solutions converge to the solution of the Fokker--Planck equation. The major novelty of this iterative scheme is that the time-step is governed by the Wasserstein metric on probability measures. This formulation enables us to reveal an appealing, and previously unexplored, relationship between the Fokker--Planck equation and the associated free energy functional. Namely, we demonstrate that the dynamics may be regarded as a gradient flux, or a steepest descent, for the free energy with respect to the Wasserstein metric. },
	author = {Jordan, Richard and Kinderlehrer, David and Otto, Felix},
	doi = {10.1137/S0036141096303359},
	eprint = {https://doi.org/10.1137/S0036141096303359},
	journal = {SIAM Journal on Mathematical Analysis},
	number = {1},
	pages = {1-17},
	title = {The Variational Formulation of the Fokker--Planck Equation},
	url = {https://doi.org/10.1137/S0036141096303359},
	volume = {29},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1137/S0036141096303359}}
@unpublished{Hairer2018,
    author = {Martin Hairer},
    note   = {Lecture Note},
    title  = {Ergodic Properties of Markov Processes},
    year   = {2018},
    url    = {https://www.hairer.org/notes/Markov.pdf}
}

@article{Zhang-Zha2004,
	abstract = { We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each data point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in two-dimensional/three-dimensional (2D/3D) Euclidean spaces and in higher-dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements. },
	author = {Zhang, Zhenyue and Zha, Hongyuan},
	doi = {10.1137/S1064827502419154},
	eprint = {https://doi.org/10.1137/S1064827502419154},
	journal = {SIAM Journal on Scientific Computing},
	number = {1},
	pages = {313-338},
	title = {Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment},
	url = {https://doi.org/10.1137/S1064827502419154},
	volume = {26},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1137/S1064827502419154}}

@article{Donoho-Grimes2003,
	author = {David L. Donoho and Carrie Grimes},
	doi = {10.1073/pnas.1031596100},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1031596100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {10},
	pages = {5591-5596},
	title = {Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
	volume = {100},
	year = {2003},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1031596100}}

@article{Moon+2019,
	abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
	author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and Elzen, Antonia van den and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
	date = {2019/12/01},
	date-added = {2024-08-15 14:48:06 +0900},
	date-modified = {2024-08-15 14:48:06 +0900},
	doi = {10.1038/s41587-019-0336-3},
	id = {Moon2019},
	isbn = {1546-1696},
	journal = {Nature Biotechnology},
	number = {12},
	pages = {1482--1492},
	title = {Visualizing structure and transitions in high-dimensional biological data},
	url = {https://doi.org/10.1038/s41587-019-0336-3},
	volume = {37},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41587-019-0336-3}}

@article{Szubert+2019,
	abstract = {Single-cell technologies offer an unprecedented opportunity to effectively characterize cellular heterogeneity in health and disease. Nevertheless, visualisation and interpretation of these multi-dimensional datasets remains a challenge. We present a novel framework, ivis, for dimensionality reduction of single-cell expression data. ivis utilizes a siamese neural network architecture that is trained using a novel triplet loss function. Results on simulated and real datasets demonstrate that ivis preserves global data structures in a low-dimensional space, adds new data points to existing embeddings using a parametric mapping function, and scales linearly to hundreds of thousands of cells. ivis is made publicly available through Python and R interfaces on https://github.com/beringresearch/ivis.},
	author = {Szubert, Benjamin and Cole, Jennifer E. and Monaco, Claudia and Drozdov, Ignat},
	date = {2019/06/20},
	date-added = {2024-08-16 20:22:42 +0900},
	date-modified = {2024-08-16 20:22:42 +0900},
	doi = {10.1038/s41598-019-45301-0},
	id = {Szubert2019},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {8914},
	title = {Structure-preserving visualisation of high dimensional single-cell datasets},
	url = {https://doi.org/10.1038/s41598-019-45301-0},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-019-45301-0}}
@misc{Campbell+2024,
      title={Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design}, 
      author={Andrew Campbell and Jason Yim and Regina Barzilay and Tom Rainforth and Tommi Jaakkola},
      year={2024},
      eprint={2402.04997},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2402.04997}, 
}
@article{Stimper+2023, doi = {10.21105/joss.05361}, url = {https://doi.org/10.21105/joss.05361}, year = {2023}, publisher = {The Open Journal}, volume = {8}, number = {86}, pages = {5361}, author = {Vincent Stimper and David Liu and Andrew Campbell and Vincent Berenz and Lukas Ryll and Bernhard Schölkopf and José Miguel Hernández-Lobato}, title = {normflows: A PyTorch Package for Normalizing Flows}, journal = {Journal of Open Source Software} } 

@InProceedings{Germain+2015,
  title = 	 {{MADE: Masked Autoencoder for Distribution Estimation}},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}
@book{Pontryagin+1962,
	author         = {L. S. Pontryagin and V. G. Boltyanskii and R. V. Gamkrelidze and E. F. Mishchenko},
	year           = {1962},
	title          = {The Mathematical Theory of Optimal Processes},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {https://doi.org/10.1201/9780203749319},
	doi            = {},
	publisher      = {John Wiley \& Sons}
}

@article{Lettermann+2024,
	abstract = {Building a representative model of a complex dynamical system from empirical evidence remains a highly challenging problem. Classically, these models are described by systems of differential equations that depend on parameters that need to be optimized by comparison with data. In this tutorial, we introduce the most common multi-parameter estimation techniques, highlighting their successes and limitations. We demonstrate how to use the adjoint method, which allows efficient handling of large systems with many unknown parameters, and present prototypical examples across several fields of physics. Our primary objective is to provide a practical introduction to adjoint optimization, catering for a broad audience of scientists and engineers.},
	author = {Lettermann, Leon and Jurado, Alejandro and Betz, Timo and W{\"o}rg{\"o}tter, Florentin and Herzog, Sebastian},
	date = {2024/04/15},
	date-added = {2024-08-20 13:44:11 +0900},
	date-modified = {2024-08-20 13:44:11 +0900},
	doi = {10.1038/s42005-024-01606-9},
	id = {Lettermann2024},
	isbn = {2399-3650},
	journal = {Communications Physics},
	number = {1},
	pages = {128},
	title = {Tutorial: a beginner's guide to building a representative model of dynamical systems using the adjoint method},
	url = {https://doi.org/10.1038/s42005-024-01606-9},
	volume = {7},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42005-024-01606-9}}
@article{sanchez-lengeling+2021,
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  title = {A Gentle Introduction to Graph Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/gnn-intro},
  doi = {10.23915/distill.00033}
}
@misc{Adams+2018,
      title={Estimating the Spectral Density of Large Implicit Matrices}, 
      author={Ryan P. Adams and Jeffrey Pennington and Matthew J. Johnson and Jamie Smith and Yaniv Ovadia and Brian Patton and James Saunderson},
      year={2018},
      eprint={1802.03451},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03451}, 
}
@article{Avron-Toledo2011,
author = {Avron, Haim and Toledo, Sivan},
title = {Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/1944345.1944349},
doi = {10.1145/1944345.1944349},
abstract = {We analyze the convergence of randomized trace estimators. Starting at 1989, several algorithms have been proposed for estimating the trace of a matrix by 1/MΣi=1M ziT Azi, where the zi are random vectors; different estimators use different distributions for the zis, all of which lead to E(1/MΣi=1M ziT Azi) = trace(A). These algorithms are useful in applications in which there is no explicit representation of A but rather an efficient method compute zTAz given z. Existing results only analyze the variance of the different estimators. In contrast, we analyze the number of samples M required to guarantee that with probability at least 1-δ, the relative error in the estimate is at most ϵ. We argue that such bounds are much more useful in applications than the variance. We found that these bounds rank the estimators differently than the variance; this suggests that minimum-variance estimators may not be the best.We also make two additional contributions to this area. The first is a specialized bound for projection matrices, whose trace (rank) needs to be computed in electronic structure calculations. The second is a new estimator that uses less randomness than all the existing estimators.},
journal = {J. ACM},
month = {apr},
articleno = {8},
numpages = {34},
keywords = {implicit linear operators, Trace estimation}
}
@inbook{Meyer+2021,
author = {Raphael A. Meyer and Cameron Musco and Christopher Musco and David P. Woodruff},
title = {Hutch++: Optimal Stochastic Trace Estimation},
booktitle = {2021 Symposium on Simplicity in Algorithms (SOSA)},
chapter = {},
pages = {142-155},
doi = {10.1137/1.9781611976496.16},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976496.16},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976496.16},
}
@inproceedings{Zhang-Chen2023,
title={Fast Sampling of Diffusion Models with Exponential Integrator},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Loek7hfb46P}
}
@article{McCann1997,
title = {A Convexity Principle for Interacting Gases},
journal = {Advances in Mathematics},
volume = {128},
number = {1},
pages = {153-179},
year = {1997},
issn = {0001-8708},
doi = {https://doi.org/10.1006/aima.1997.1634},
url = {https://www.sciencedirect.com/science/article/pii/S0001870897916340},
author = {Robert J. McCann},
abstract = {A new set of inequalities is introduced, based on a novel but natural interpolation between Borel probability measures onRd. Using these estimates in lieu of convexity or rearrangement inequalities, the existence and uniqueness problems are solved for a family of attracting gas models. In these models, the gas interacts with itself through a force which increases with distance and is governed by an equation of stateP=P(ϱ) relating pressure to density.P(ϱ)/ϱ>(d−1)/dis assumed non-decreasing for ad-dimensional gas. By showing that the internal and potential energies for the system are convex functions of the interpolation parameter, an energy minimizing state—unique up to translation—is proven to exist. The concavity established for ¶ρt¶−p/dqas a function oft∈[0,1] generalizes the Brunn–Minkowski inequality from sets to measures.}
}
@Article{Maoutsa+2020,
AUTHOR = {Maoutsa, Dimitra and Reich, Sebastian and Opper, Manfred},
TITLE = {Interacting Particle Solutions of Fokker–Planck Equations Through Gradient–Log–Density Estimation},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {802},
URL = {https://www.mdpi.com/1099-4300/22/8/802},
PubMedID = {33286573},
ISSN = {1099-4300},
ABSTRACT = {Fokker–Planck equations are extensively employed in various scientific fields as they characterise the behaviour of stochastic systems at the level of probability density functions. Although broadly used, they allow for analytical treatment only in limited settings, and often it is inevitable to resort to numerical solutions. Here, we develop a computational approach for simulating the time evolution of Fokker–Planck solutions in terms of a mean field limit of an interacting particle system. The interactions between particles are determined by the gradient of the logarithm of the particle density, approximated here by a novel statistical estimator. The performance of our method shows promising results, with more accurate and less fluctuating statistics compared to direct stochastic simulations of comparable particle number. Taken together, our framework allows for effortless and reliable particle-based simulations of Fokker–Planck equations in low and moderate dimensions. The proposed gradient–log–density estimator is also of independent interest, for example, in the context of optimal control.},
DOI = {10.3390/e22080802}
}
@inproceedings{Heitz+2023,
author = {Heitz, Eric and Belcour, Laurent and Chambon, Thomas},
title = {Iterative α -(de)Blending: a&nbsp;Minimalist&nbsp;Deterministic&nbsp;Diffusion&nbsp;Model},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591540},
doi = {10.1145/3588432.3591540},
abstract = {We derive a minimalist but powerful deterministic denoising-diffusion model. While denoising diffusion has shown great success in many domains, its underlying theory remains largely inaccessible to non-expert users. Indeed, an understanding of graduate-level concepts such as Langevin dynamics or score matching appears to be required to grasp how it works. We propose an alternative approach that requires no more than undergrad calculus and probability. We consider two densities and observe what happens when random samples from these densities are blended (linearly interpolated). We show that iteratively blending and deblending samples produces random paths between the two densities that converge toward a deterministic mapping. This mapping can be evaluated with a neural network trained to deblend samples. We obtain a model that behaves like deterministic denoising diffusion: it iteratively maps samples from one density (e.g., Gaussian noise) to another (e.g., cat images). However, compared to the state-of-the-art alternative, our model is simpler to derive, simpler to implement, more numerically stable, achieves higher quality results in our experiments, and has interesting connections to computer graphics.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {34},
numpages = {8},
keywords = {diffusion models, mapping, sampling},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{Porter-Duff1984,
author = {Porter, Thomas and Duff, Tom},
title = {Compositing digital images},
year = {1984},
isbn = {0897911385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800031.808606},
doi = {10.1145/800031.808606},
abstract = {Most computer graphics pictures have been computed all at once, so that the rendering program takes care of all computations relating to the overlap of objects. There are several applications, however, where elements must be rendered separately, relying on compositing techniques for the anti-aliased accumulation of the full image. This paper presents the case for four-channel pictures, demonstrating that a matte component can be computed similarly to the color channels. The paper discusses guidelines for the generation of elements and the arithmetic for their arbitrary compositing.},
booktitle = {Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {253–259},
numpages = {7},
keywords = {Compositing, Graphics systems, Matte algebra, Matte channel, Visible surface algorithms},
series = {SIGGRAPH '84}
}

@misc{Oulhaj+2024,
      title={Differentiable Mapper For Topological Optimization Of Data Representation}, 
      author={Ziyad Oulhaj and Mathieu Carrière and Bertrand Michel},
      year={2024},
      eprint={2402.12854},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12854}, 
}
@misc{Oulhaj+2024DeepMapper,
      title={Deep Mapper Graph and its Application to Visualize Plausible Pathways on High-Dimensional Distribution with Small Time-Complexity}, 
      author={Ziyad Oulhaj and Yoshiyuki Ishii and Kento Ohga and Kimihiro Yamazaki and Mutsuyo Wada and Yuhei Umeda and Takashi Kato and Yuichiro Wada and Hiroaki Kurihara},
      year={2024},
      eprint={2402.19177},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/2402.19177}, 
}
@misc{Esser+2024,
      title={Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach},
      year={2024},
      eprint={2403.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.03206}, 
}
@inproceedings{Kingma-Gao2023,
 author = {Kingma, Diederik and Gao, Ruiqi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {65484--65516},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ce79fbf9baef726645bc2337abb0ade2-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}
@misc{Heng+2022,
      title={{Simulating Diffusion Bridges with Score Matching}}, 
      author={Jeremy Heng and Valentin De Bortoli and Arnaud Doucet and James Thornton},
      year={2022},
      eprint={2111.07243},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2111.07243}, 
}
@article{Tong+2024,
title={{Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport}},
author={Alexander Tong and Kilian FATRAS and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=CD9Snc73AW},
note={Expert Certification}
}
@article{Schrödinger1932,
author = {Schrödinger, E.},
journal = {Annales de l'institut Henri Poincaré},
keywords = {quantum theory},
language = {fre},
number = {4},
pages = {269-310},
publisher = {INSTITUT HENRI POINCARÉ ET LES PRESSES UNIVERSITAIRES DE FRANCE},
title = {Sur la théorie relativiste de l'électron et l'interprétation de la mécanique quantique},
url = {http://eudml.org/doc/78968},
volume = {2},
year = {1932},
}

@InProceedings{Pooladian+2023,
  title = 	 {Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  author =       {Pooladian, Aram-Alexandre and Ben-Hamu, Heli and Domingo-Enrich, Carles and Amos, Brandon and Lipman, Yaron and Chen, Ricky T. Q.},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28100--28127},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/pooladian23a/pooladian23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/pooladian23a.html},
  abstract = 	 {Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with low cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a completely simulation-free manner with a simple minimization objective. We show that our proposed methods improve sample consistency on downsampled ImageNet data sets, and lead to better low-cost sample generation.}
}
@misc{Isobe+2024,
      title={Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation}, 
      author={Noboru Isobe and Masanori Koyama and Jinzhe Zhang and Kohei Hayashi and Kenji Fukumizu},
      year={2024},
      eprint={2402.18839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18839}, 
}
@misc{Fatras+2021,
      title={Minibatch optimal transport distances; analysis and applications}, 
      author={Kilian Fatras and Younes Zine and Szymon Majewski and Rémi Flamary and Rémi Gribonval and Nicolas Courty},
      year={2021},
      eprint={2101.01792},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2101.01792}, 
}
@Inbook{Brenier2003,
author="Brenier, Yann",
title="Extended Monge-Kantorovich Theory",
bookTitle="Optimal Transportation and Applications: Lectures given at the C.I.M.E. Summer School, held in Martina Franca, Italy, September 2-8, 2001",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="91--121",
abstract="1 Abstract2 Generalized geodesics and the Monge-Kantorovich theory2.1 Generalized geodesics2.2 Extension to probability measures2.3 A decomposition result2.4 Relativistic MKT2.5 A relativistic heat equation2.6 Laplace's equation and Moser's lemma revisited3 Generalized Harmonic functions3.1 Classical harmonic functions3.2 Open problems4 Multiphasic MKT5 Generalized extremal surfaces5.1 MKT revisited as a subset of generalized surface theory5.2 Degenerate quadratic cost functions6 Generalized extremal surfaces in {\$}<math display='block'><mrow><msup><mi>{\&}{\#}x211D;</mi><mn>5</mn></msup></mrow></math>{\$}{\$}{\backslash}mathbb{\{}R{\}}^5{\$}and Electrodynamics6.1 Recovery of the Maxwell equations6.2 Derivation of a set of nonlinear Maxwell equations6.3 An Euler-Maxwell-type systemReferences",
isbn="978-3-540-44857-0",
doi="10.1007/978-3-540-44857-0_4",
url="https://doi.org/10.1007/978-3-540-44857-0_4"
}


@InProceedings{Kerrigan+2024,
  title = 	 {Functional Flow Matching},
  author =       {Kerrigan, Gavin and Migliorini, Giosue and Smyth, Padhraic},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3934--3942},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/kerrigan24a/kerrigan24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/kerrigan24a.html},
  abstract = 	 {We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.}
}
@article{Lavenant2019,
title = {Harmonic mappings valued in the Wasserstein space},
journal = {Journal of Functional Analysis},
volume = {277},
number = {3},
pages = {688-785},
year = {2019},
issn = {0022-1236},
doi = {https://doi.org/10.1016/j.jfa.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022123619301478},
author = {Hugo Lavenant},
}
@article{Lavenant+2024,
author = {Hugo Lavenant and Stephen Zhang and Young-Heon Kim and Geoffrey Schiebinger},
title = {{Toward a mathematical theory of trajectory inference}},
volume = {34},
journal = {The Annals of Applied Probability},
number = {1A},
publisher = {Institute of Mathematical Statistics},
pages = {428 -- 500},
keywords = {Convex optimization, developmental biology, Optimal transport, single-cell RNA-sequencing, Stochastic processes, Trajectory inference},
year = {2024},
doi = {10.1214/23-AAP1969},
URL = {https://doi.org/10.1214/23-AAP1969}
}

@InProceedings{Hashimoto+2016,
  title = 	 {Learning Population-Level Diffusions with Generative RNNs},
  author = 	 {Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2417--2426},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hashimoto16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/hashimoto16.html},
  abstract = 	 {We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the ‘epigenetic landscape’ from existing biological assays.}
}

@misc{Dao+2023,
      title={Flow Matching in Latent Space}, 
      author={Quan Dao and Hao Phung and Binh Nguyen and Anh Tran},
      year={2023},
      eprint={2307.08698},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.08698}, 
}
@misc{Zheng+2023GuidedFlow,
      title={Guided Flows for Generative Modeling and Decision Making}, 
      author={Qinqing Zheng and Matt Le and Neta Shaul and Yaron Lipman and Aditya Grover and Ricky T. Q. Chen},
      year={2023},
      eprint={2311.13443},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.13443}, 
}
@inproceedings{Chen-Lipman2024,
title={Flow Matching on General Geometries},
author={Ricky T. Q. Chen and Yaron Lipman},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=g7ohDlTITL}
}
@misc{Habermann+2024,
      title={Amortized Bayesian Multilevel Models}, 
      author={Daniel Habermann and Marvin Schmitt and Lars Kühmichel and Andreas Bulling and Stefan T. Radev and Paul-Christian Bürkner},
      year={2024},
      eprint={2408.13230},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.13230}, 
}
@ARTICLE{Radev+2022,
  author={Radev, Stefan T. and Mertens, Ulf K. and Voss, Andreas and Ardizzone, Lynton and Köthe, Ullrich},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks}, 
  year={2022},
  volume={33},
  number={4},
  pages={1452-1466},
  keywords={Data models;Bayes methods;Biological system modeling;Neural networks;Training;Numerical models;Estimation;Bayesian inference;computational and artificial intelligence;machine learning;neural networks;statistical learning},
  doi={10.1109/TNNLS.2020.3042395}}
@article{Cranmer+2020,
author = {Kyle Cranmer  and Johann Brehmer  and Gilles Louppe },
title = {The frontier of simulation-based inference},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30055-30062},
year = {2020},
doi = {10.1073/pnas.1912789117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1912789117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1912789117},
abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.}}
@inproceedings{Koshizuka-Sato2023,
title={Neural Lagrangian Schr{\textbackslash}''\{o\}dinger Bridge: Diffusion Modeling for Population Dynamics},
author={Takeshi Koshizuka and Issei Sato},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=d3QNWD_pcFv}
}
@article{Efron2011,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/23239562},
 abstract = {We suppose that the statistician observes some large number of estimates zi, each with its own unobserved expectation parameter μi. The largest few of the zi's are likely to substantially overestimate their corresponding μi's, this being an example of selection bias, or regression to the mean. Tweedie's formula, first reported by Robbins in 1956, offers a simple empirical Bayes approach for correcting selection bias. This article investigates its merits and limitations. In addition to the methodology, Tweedie's formula raises more general questions concerning empirical Bayes theory, discussed here as "relevance" and "empirical Bayes information." There is a close connection between applications of the formula and James—Stein estimation.},
 author = {Bradley Efron},
 journal = {Journal of the American Statistical Association},
 number = {496},
 pages = {1602--1614},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Tweedie's Formula and Selection Bias},
 urldate = {2024-08-27},
 volume = {106},
 year = {2011}
}
@misc{Fjelde+2024,
  title   = "An Introduction to Flow Matching",
  author  = "Fjelde, Tor and Mathieu, Emile and Dutordoir, Vincent",
  journal = "https://mlg.eng.cam.ac.uk/blog/",
  year    = "2024",
  month   = "January",
  url     = "https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html"
}
@misc{Chen+2023SB,
  title={Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis}, 
  author={Zehua Chen and Guande He and Kaiwen Zheng and Xu Tan and Jun Zhu},
  year={2023},
  url             = {https://bridge-tts.github.io/},
}
@unpublished{Dieleman2023,
	author = {Sander Dieleman},
	year   = {2023},
	title  = {Perspectives on diffusion},
	url    = {https://sander.ai/2023/07/20/perspectives.html},
	doi    = {}
}
@unpublished{Yuan2024,
	author = {Chenyang Yuan},
	year   = {2024},
	title  = {Diffusion Models from Scratch, from a New Theoretical Perspective},
	url    = {https://www.chenyang.co/diffusion.html},
	doi    = {}
}
@misc{Nakkiran+2024,
      title={Step-by-Step Diffusion: An Elementary Tutorial}, 
      author={Preetum Nakkiran and Arwen Bradley and Hattie Zhou and Madhu Advani},
      year={2024},
      eprint={2406.08929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.08929}, 
}
@unpublished{Duan2023,
	author = {Tony Duan},
	year   = {2023},
	title  = {Diffusion Models from Scratch},
	url    = {https://www.tonyduan.com/diffusion/index.html},
	doi    = {}
}
@inproceedings{Das2024,
  author = {Das, Ayan},
  title = {Building Diffusion Model's theory from ground up},
  abstract = {Diffusion Models, a new generative model family, have taken the world by storm after the seminal paper by Ho et al. [2020]. While diffusion models are often described as a probabilistic Markov Chains, their underlying principle is based on the decade-old theory of Stochastic Differential Equations (SDE), as found out later by Song et al. [2021]. In this article, we will go back and revisit the 'fundamental ingredients' behind the SDE formulation and show how the idea can be 'shaped' to get to the modern form of Score-based Diffusion Models. We'll start from the very definition of the 'score', how it was used in the context of generative modeling, how we achieve the necessary theoretical guarantees and how the critical design choices were made to finally arrive at the more 'principled' framework of Score-based Diffusion. Throughout this article, we provide several intuitive illustrations for ease of understanding.},
  booktitle = {ICLR Blogposts 2024},
  year = {2024},
  date = {May 7, 2024},
  note = {https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/},
  url  = {https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/}
}

@InProceedings{Bunne+2022,
  title = 	 { Proximal Optimal Transport Modeling of Population Dynamics },
  author =       {Bunne, Charlotte and Papaxanthos, Laetitia and Krause, Andreas and Cuturi, Marco},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6511--6528},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/bunne22a/bunne22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/bunne22a.html},
  abstract = 	 { We propose a new approach to model the collective dynamics of a population of particles evolving with time. As is often the case in challenging scientific applications, notably single-cell genomics, measuring features for these particles requires destroying them. As a result, the population can only be monitored with periodic snapshots, obtained by sampling a few particles that are sacrificed in exchange for measurements. Given only access to these snapshots, can we reconstruct likely individual trajectories for all other particles? We propose to model these trajectories as collective realizations of a causal Jordan-Kinderlehrer-Otto (JKO) flow of measures: The JKO scheme posits that the new configuration taken by a population at time t+1 is one that trades off an improvement, in the sense that it decreases an energy, while remaining close (in Wasserstein distance) to the previous configuration observed at t. In order to learn such an energy using only snapshots, we propose JKOnet, a neural architecture that computes (in end-to-end differentiable fashion) the JKO flow given a parametric energy and initial configuration of points. We demonstrate the good performance and robustness of the JKOnet fitting procedure, compared to a more direct forward method. }
}
@article{Schiebinger+2019,
title = {Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming},
journal = {Cell},
volume = {176},
number = {4},
pages = {928-943.e22},
year = {2019},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S009286741930039X},
author = {Geoffrey Schiebinger and Jian Shu and Marcin Tabaka and Brian Cleary and Vidya Subramanian and Aryeh Solomon and Joshua Gould and Siyan Liu and Stacie Lin and Peter Berube and Lia Lee and Jenny Chen and Justin Brumbaugh and Philippe Rigollet and Konrad Hochedlinger and Rudolf Jaenisch and Aviv Regev and Eric S. Lander},
keywords = {optimal-transport, reprogramming, scRNA-seq, trajectories, ancestors, descendants, development, regulation, paracrine interactions, iPSCs},
abstract = {Summary
Understanding the molecular programs that guide differentiation during development is a major challenge. Here, we introduce Waddington-OT, an approach for studying developmental time courses to infer ancestor-descendant fates and model the regulatory programs that underlie them. We apply the method to reconstruct the landscape of reprogramming from 315,000 single-cell RNA sequencing (scRNA-seq) profiles, collected at half-day intervals across 18 days. The results reveal a wider range of developmental programs than previously characterized. Cells gradually adopt either a terminal stromal state or a mesenchymal-to-epithelial transition state. The latter gives rise to populations related to pluripotent, extra-embryonic, and neural cells, with each harboring multiple finer subpopulations. The analysis predicts transcription factors and paracrine signals that affect fates and experiments validate that the TF Obox6 and the cytokine GDF9 enhance reprogramming efficiency. Our approach sheds light on the process and outcome of reprogramming and provides a framework applicable to diverse temporal processes in biology.}
}

@InProceedings{Tong+2020,
  title = 	 {{T}rajectory{N}et: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics},
  author =       {Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9526--9536},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tong20a/tong20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tong20a.html},
  abstract = 	 {It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present \emph{TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.}
}

@InProceedings{Neklyudov+2023,
  title = 	 {Action Matching: Learning Stochastic Dynamics from Samples},
  author =       {Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {25858--25889},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/neklyudov23a/neklyudov23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/neklyudov23a.html},
  abstract = 	 {Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation and destruction of probability mass. Finally, we showcase applications of Action Matching by achieving competitive performance in a diverse set of experiments from biology, physics, and generative modeling.}
}

@article{Eldan2013,
	abstract = {We consider the isoperimetric inequality on the class of high-dimensional isotropic convex bodies. We establish quantitative connections between two well-known open problems related to this inequality, namely, the thin shell conjecture, and the conjecture by Kannan, Lov{\'a}sz, and Simonovits, showing that the corresponding optimal bounds are equivalent up to logarithmic factors. In particular we prove that, up to logarithmic factors, the minimal possible ratio between surface area and volume is attained on ellipsoids. We also show that a positive answer to the thin shell conjecture would imply an optimal dependence on the dimension in a certain formulation of the Brunn--Minkowski inequality. Our results rely on the construction of a stochastic localization scheme for log-concave measures.},
	author = {Eldan, Ronen},
	date = {2013/04/01},
	date-added = {2024-08-28 20:57:55 +0900},
	date-modified = {2024-08-28 20:57:55 +0900},
	doi = {10.1007/s00039-013-0214-y},
	id = {Eldan2013},
	isbn = {1420-8970},
	journal = {Geometric and Functional Analysis},
	number = {2},
	pages = {532--569},
	title = {{Thin Shell Implies Spectral Gap Up to Polylog via a Stochastic Localization Scheme}},
	url = {https://doi.org/10.1007/s00039-013-0214-y},
	volume = {23},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s00039-013-0214-y}}
@misc{Montanari2023,
      title={Sampling, Diffusions, and Stochastic Localization}, 
      author={Andrea Montanari},
      year={2023},
      eprint={2305.10690},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.10690}, 
}
@INPROCEEDINGS{Alaoui+2022,
  author={Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
  booktitle={2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)}, 
  title={Sampling from the Sherrington-Kirkpatrick Gibbs measure via algorithmic stochastic localization}, 
  year={2022},
  volume={},
  number={},
  pages={323-334},
  keywords={Measurement;Location awareness;Couplings;Temperature distribution;Stochastic processes;Approximation algorithms;Sampling methods},
  doi={10.1109/FOCS54457.2022.00038}}
@misc{Benton+2024,
      title={Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization}, 
      author={Joe Benton and Valentin De Bortoli and Arnaud Doucet and George Deligiannidis},
      year={2024},
      eprint={2308.03686},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2308.03686}, 
}
@INPROCEEDINGS{Zhu-Mumford1998,
  author={Song Chun Zhu and Mumford, D.},
  booktitle={Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)}, 
  title={GRADE: Gibbs reaction and diffusion equations}, 
  year={1998},
  volume={},
  number={},
  pages={847-854},
  keywords={Application software;Partial differential equations;Pattern formation;Nonlinear equations;Computer vision;Image processing;Markov random fields;Minimax techniques;Entropy;Differential equations},
  doi={10.1109/ICCV.1998.710816}}

@article{Zhu+1998,
	abstract = {This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework.},
	author = {Zhu, Song Chun and Wu, Yingnian and Mumford, David},
	date = {1998/03/01},
	date-added = {2024-09-02 13:15:46 +0900},
	date-modified = {2024-09-02 13:15:46 +0900},
	doi = {10.1023/A:1007925832420},
	id = {Zhu1998},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {2},
	pages = {107--126},
	title = {{Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling}},
	url = {https://doi.org/10.1023/A:1007925832420},
	volume = {27},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1007925832420}}
@article{Shibue-Komaki2020,
    doi = {10.1371/journal.pcbi.1007650},
    author = {Shibue, Ryohei AND Komaki, Fumiyasu},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deconvolution of calcium imaging data using marked point processes},
    year = {2020},
    month = {03},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pcbi.1007650},
    pages = {1-25},
    number = {3},
}

@article{Shibue-Komaki2017,
	abstract = {Neural decoding is a framework for reconstructing external stimuli from spike trains recorded by various neural recordings. Kloosterman et al. proposed a new decoding method using marked point processes (Kloosterman F, Layton SP, Chen Z, Wilson MA. J Neurophysiol 111: 217?227, 2014). This method does not require spike sorting and thereby improves decoding accuracy dramatically. In this method, they used kernel density estimation to estimate intensity functions of marked point processes. However, the use of kernel density estimation causes problems such as low decoding accuracy and high computational costs. To overcome these problems, we propose a new decoding method using infinite mixture models to estimate intensity. The proposed method improves decoding performance in terms of accuracy and computational speed. We apply the proposed method to simulation and experimental data to verify its performance. NEW \& NOTEWORTHY We propose a new neural decoding method using infinite mixture models and nonparametric Bayesian statistics. The proposed method improves decoding performance in terms of accuracy and computation speed. We have successfully applied the proposed method to position decoding from spike trains recorded in a rat hippocampus.},
	annote = {doi: 10.1152/jn.00818.2016},
	author = {Shibue, Ryohei and Komaki, Fumiyasu},
	date = {2017/11/01},
	date-added = {2024-09-02 13:56:48 +0900},
	date-modified = {2024-09-02 13:56:48 +0900},
	doi = {10.1152/jn.00818.2016},
	isbn = {0022-3077},
	journal = {Journal of Neurophysiology},
	journal1 = {Journal of Neurophysiology},
	month = {2024/09/01},
	n2 = {Neural decoding is a framework for reconstructing external stimuli from spike trains recorded by various neural recordings. Kloosterman et al. proposed a new decoding method using marked point processes (Kloosterman F, Layton SP, Chen Z, Wilson MA. J Neurophysiol 111: 217?227, 2014). This method does not require spike sorting and thereby improves decoding accuracy dramatically. In this method, they used kernel density estimation to estimate intensity functions of marked point processes. However, the use of kernel density estimation causes problems such as low decoding accuracy and high computational costs. To overcome these problems, we propose a new decoding method using infinite mixture models to estimate intensity. The proposed method improves decoding performance in terms of accuracy and computational speed. We apply the proposed method to simulation and experimental data to verify its performance. NEW \& NOTEWORTHY We propose a new neural decoding method using infinite mixture models and nonparametric Bayesian statistics. The proposed method improves decoding performance in terms of accuracy and computation speed. We have successfully applied the proposed method to position decoding from spike trains recorded in a rat hippocampus.},
	number = {5},
	pages = {2902--2913},
	publisher = {American Physiological Society},
	title = {Firing rate estimation using infinite mixture models and its application to neural decoding},
	type = {doi: 10.1152/jn.00818.2016},
	url = {https://doi.org/10.1152/jn.00818.2016},
	volume = {118},
	year = {2017},
	year1 = {2017},
	bdsk-url-1 = {https://doi.org/10.1152/jn.00818.2016}}
@book{Goldstine1980,
	author = {Herman H. Goldstine},
	year = {1980},
	title = {A History of the Calculus of Variations from the 17th through the 19th Century},
	series = {Studies in the History of Mathematics and Physical Sciences},
	volume = {5},
	edition = {},
	url = {https://doi.org/10.1007/978-1-4613-8106-8},
	doi = {10.1007/978-1-4613-8106-8},
	publisher = {Springer New York}
}
@article{Hamilton1834,
	author = {W. R. Hamilton},
	year = {1834},
	title = {On a General Method in Dynamics; by which the Study of the Motions of all free Systems of attracting or repelling Points is reduced to the Search and Differentiation of one central Relation, or characteristic Function.},
	journal = {Philosophical Transactions of the Royal Society},
	volume = {124},
	number = {},
	pages = {247-308},
	url = {https://www.jstor.org/stable/108066},
	doi = {}
}
@inproceedings{中根美知代2000,
	author = {中根美知代},
	year = {2000},
	title = {物理学から数学へ : Hamilton-Jacobi 理論の誕生 (数学史の研究)},
	booktitle = {数理解析研究所講究録},
	volume = {1130},
	pages = {58-71},
	url = {http://hdl.handle.net/2433/63679},
	doi = {}
}
@article{Fermat1657,
	author = {Pierre de Fermat},
	year = {1657},
	title = {Marin Cureau de la Chambre, La Lumière (Chez Iacqves D'Allin, Paris, 1657)},
	journal = {Personal correspondence},
	volume = {},
	number = {},
	pages = {},
	url = {https://gallica.bnf.fr/ark:/12148/bpt6k94859n.pdf},
	doi = {}
}
@book{Euler1744,
	author = {Leonhard Euler},
	year = {1744},
	title = {ethodus Inveniendi Lineas Curvas Maximi Minimive Proprietate Gaudentes sive Solutio Problematis Isoperimetrici Latissimo Sensu Accepti},
	series = {},
	volume = {},
	edition = {},
	url = {https://archive.org/details/methodusinvenie00eule},
	doi = {},
	publisher = {Lausannæ ; Genevæ : Apud Marcum-Michaelem Bousquet & Socios}
}
@article{Bernoulli1969,
	author = {John Bernoulli},
	year = {1696},
	title = {Poblema novum ad cujus solution em mathematici invitantur},
	journal = {Acta Eruditorum},
	volume = {1},
	number = {},
	pages = {269},
	url = {},
	doi = {}
}
@book{Lagrange1788,
	author = {Joseph-Louis Lagrange},
	year = {1788},
	title = {Mécanique Analytique},
	series = {},
	volume = {},
	edition = {},
	url = {https://gallica.bnf.fr/ark:/12148/bpt6k862625},
	doi = {},
	publisher = {}
}
@unpublished{Hanc2017,
	author = {Jozef Hanc},
	year = {2017},
	title = {The Original Euler's Calculus-of-Variations Method: Key to Lagrangian Mechanics for Beginners},
	url = {https://www.eftaylor.com/pub/HancEulerEJP.pdf},
	doi = {}
}
@book{Monge1781,
	author = {Gaspard Monge},
	year = {1781},
	title = {Mémoire sur la théorie des déblais et des remblais},
	series = {},
	volume = {},
	edition = {},
	url = {},
	doi = {},
	publisher = {Imprimerie Royale}
}

@article{Kantorovich1942,
    author          = {L. V. Kantorovich},
    year            = {1942},
    title           = {On the Translocation of Masses},
    language        = {Russian},
    journal         = {Doklady Akademii Nauk SSSR},
    volume          = {37},
    number          = {7-8},
    pages           = {227-229},
    url             = {https://link.springer.com/article/10.1007/s10958-006-0049-2}
}
@article{Kantorovich1940,
    author          = {L. V. Kantorovich},
    year            = {1940},
    title           = {On an Effective Method of Solving Certain Classes of Extremal Problems},
    language        = {Russian},
    journal         = {Doklady Akademii Nauk SSSR},
    volume          = {28},
	pages = {212-215},
}
@article{Kantorovich1948,
    author          = {L. V. Kantorovich},
    year            = {1948},
    title           = {On a Problem of Monge},
    language        = {Russian},
    journal         = {Uspekhi Matematicheskikh Nauk},
    volume          = {3},
    number          = {2},
    pages           = {225-226},
    url             = {https://doi.org/10.1007/s10958-006-0050-9}
}

@article{Vershik2013,
	author = {Vershik, A.  M. },
	date = {2013/12/01},
	date-added = {2024-09-04 13:28:11 +0900},
	date-modified = {2024-09-04 13:28:11 +0900},
	doi = {10.1007/s00283-013-9380-x},
	id = {Vershik2013},
	isbn = {1866-7414},
	journal = {The Mathematical Intelligencer},
	number = {4},
	pages = {1--9},
	title = {Long History of the Monge-Kantorovich Transportation Problem},
	url = {https://doi.org/10.1007/s00283-013-9380-x},
	volume = {35},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s00283-013-9380-x}}
@Inbook{Ambrosio2024,
author="Ambrosio, Luigi
and Quarteroni, Alfio",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="Talking about Optimal Transport",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--19",
abstract="In this session, we have the pleasure of hosting Luigi Ambrosio, a professor at the Scuola Normale Superiore in Pisa, Italy, as our guest. Professor Ambrosio, who recently co-authored the new textbook, Lectures on Optimal Transport, with Elia Bru{\'e} and Daniele Semola, engages in a lively conversation with Alfio Quarteroni, a professor at Politecnico di Milano.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_1",
url="https://doi.org/10.1007/978-3-031-51685-6_1"
}
@incollection{Levi2014,
	author = {Mark Levi},
	booktitle = {SIAM News},
	publisher = {SIAM},
	title = {Quick! Find a Solution to the Brachistochrone Problem},
	year = {2014}
}
@Inbook{Ambrosio2003,
author="Ambrosio, Luigi",
title="Lecture Notes on Optimal Transport Problems",
bookTitle="Mathematical Aspects of Evolving Interfaces: Lectures given at the C.I.M.-C.I.M.E. joint Euro-Summer School held in Madeira, Funchal, Portugal, July 3-9, 2000",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--52",
abstract="1 Some elementary examples2 Optimal transport plans: existence and regularity3 The one dimensional case4 The ODE version of the optimal transport problem5 The PDE version of the optimal transport problem and the p-laplacian approximation6 Existence of optimal transport maps7 Regularity and uniqueness of the transport density8 The Bouchitt{\'e}-Buttazzo mass optimization problem9 Appendix: some measure theoretic resultsReferences",
isbn="978-3-540-39189-0",
doi="10.1007/978-3-540-39189-0_1",
url="https://doi.org/10.1007/978-3-540-39189-0_1"
}
@book{Evans-Gangbo1999,
	author = {L. C. Evans and W. Gangbo},
	year = {1999},
	title = {Differential Equations Methods for the Monge-Kantorovich Mass Transfer Problem},
	series = {Memoirs of the American Mathematical Society},
	volume = {137},
	number = {653},
	edition = {},
	url = {https://doi.org/10.1090/memo/0653},
	doi = {},
	publisher = {American Mathematical Society}
}
@unpublished{Figalli2023,
	author = {Alessio Figalli},
	year = {2023},
	title = {An Introduction to Optimal Transport and Wasserstein Gradient Flows},
	url = {https://people.math.ethz.ch/~afigalli/lecture-notes},
	note = {Lecture Note},
	doi = {}
}
@book{佐藤竜馬2023,
	author = {佐藤竜馬},
	year = {2023},
	title = {最適輸送の理論とアルゴリズム},
	series = {機械学習プロフェッショナルシリーズ},
	volume = {},
	edition = {},
	url = {https://www.kspub.co.jp/book/detail/5305140.html},
	doi = {},
	publisher = {講談社サイエンティフィック}
}
@Inbook{Figalli-Ambrosio2024,
author="Figalli, Alessio
and Ambrosio, Luigi",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="Optimal Transport, Fields Medals and beyond",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="21--38",
abstract="Welcome to the Springer Math Podcast. This month, we're delighted to host Alessio Figalli, the Director of the Institute for Mathematical Research at ETH Zurich, Switzerland. A distinguished academic, Professor Figalli completed his PhD at the Scuola Normale Superiore of Pisa, Italy, and at the Ecole Normale Superieure of Lyon, France. His research has taken him across France, the United States, and Switzerland. His notable contributions to the theory of optimal transport have earned him numerous accolades, including the prestigious Fields Medal in 2018, and the European Mathematical Society Prize in 2012.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_2",
url="https://doi.org/10.1007/978-3-031-51685-6_2"
}
@Inbook{Gigli-DeLellis2024,
author="Gigli, Nicola
and De Lellis, Camillo",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="From moving masses to bending spaces: an excursion in metric geometry",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="39--58",
abstract="Welcome to the Springer Math Podcast. In this month's podcast, Camillo De Lellis, a researcher at the Institute for Advanced Study in Princeton, converses with Nicola Gigli from the Scuola Internazionale Superiore di Studi Avanzati in Trieste, Italy. They delve into Nicola Gigli's personal journey in and out of mathematics, discussing his path to the topics of his research and his enthusiasm for them. In this conversation, they also explore the connection between the concepts of optimal transport and the curvature of space, a discovery that has given rise to a flourishing research field at the intersection of multiple areas of mathematics. Originally aired by the Springer Nature webinar series, this interview has been specifically adapted for the podcastformat.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_3",
url="https://doi.org/10.1007/978-3-031-51685-6_3"
}
@article{久保川達也2006,
  title={線形混合モデルと小地域の推定},
  author={久保川達也},
  journal={応用統計学},
  volume={35},
  number={3},
  pages={139-161},
  year={2006},
  doi={10.5023/jappstat.35.139}
}
@article{Kubokawa2000,
  title={ESTIMATION OF VARIANCE AND COVARIANCE COMPONENTS IN ELLIPTICALLY CONTOURED DISTRIBUTIONS},
  author={Tatsuya Kubokawa},
  journal={JOURNAL OF THE JAPAN STATISTICAL SOCIETY},
  volume={30},
  number={2},
  pages={143-176},
  year={2000},
  doi={10.14490/jjss1995.30.143}
}
@article{Battese+1988,
author = {George E. Battese, Rachel M. Harter and Wayne A. Fuller},
title = {An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data},
journal = {Journal of the American Statistical Association},
volume = {83},
number = {401},
pages = {28--36},
year = {1988},
publisher = {ASA Website},
doi = {10.1080/01621459.1988.10478561},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478561
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478561
    

}

}
@article{Gelman2014,
author = {Andrew Gelman},
title = {{How Bayesian Analysis Cracked the Red-State, Blue-State Problem}},
volume = {29},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {26 -- 35},
keywords = {Multilevel regression and poststratification (MRP), political science, sample surveys, sparse data, voting},
year = {2014},
doi = {10.1214/13-STS458},
URL = {https://doi.org/10.1214/13-STS458}
}
@article{Efron-Morris1975,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2285814},
 abstract = {In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution having uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein's rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson's chi-square test with results from a computer simulation. In each of these examples, the mean square error of these rules is less than half that of the sample mean.},
 author = {Bradley Efron and Carl Morris},
 journal = {Journal of the American Statistical Association},
 number = {350},
 pages = {311--319},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Data Analysis Using Stein's Estimator and its Generalizations},
 urldate = {2024-09-11},
 volume = {70},
 year = {1975}
}
@article{Kubokawa-Srivastava1999,
author = {T. Kubokawa and M. S. Srivastava},
title = {{Improved nonnegative estimation of multivariate components of variance}},
volume = {27},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2008 -- 2032},
keywords = {minimax and unbiased estimators, random effects model, restricted maximum likelihood estimator, Stein loss},
year = {1999},
doi = {10.1214/aos/1017939248},
URL = {https://doi.org/10.1214/aos/1017939248}
}
@article{丹後俊郎1988,
  title={死亡指標の経験的ベイズ推定量について},
  author={丹後俊郎},
  journal={応用統計学},
  volume={17},
  number={2},
  pages={81-96},
  year={1988},
  doi={10.5023/jappstat.17.81}
}
@article{Clayton-Kaldor1987,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532003},
 abstract = {There have been many attempts in recent years to map incidence and mortality from diseases such as cancer. Such maps usually display either relative rates in each district, as measured by a standardized mortality ratio (SMR) or some similar index, or the statistical significance level for a test of the difference between the rates in that district and elsewhere. Neither of these approaches is fully satisfactory and we propose a new approach using empirical Bayes estimation. The resulting estimators represent a weighted compromise between the SMR, the overall mean relative rate, and a local mean of the relative rate in nearby areas. The compromise solution depends on the reliability of each individual SMR and on estimates of the overall amount of dispersion of relative rates over different districts.},
 author = {David Clayton and John Kaldor},
 journal = {Biometrics},
 number = {3},
 pages = {671--681},
 publisher = {[Wiley, International Biometric Society]},
 title = {Empirical Bayes Estimates of Age-Standardized Relative Risks for Use in Disease Mapping},
 urldate = {2024-09-11},
 volume = {43},
 year = {1987}
}
@article{西川正子2008,
  title={生存時間解析における競合リスクモデル},
  author={西川正子},
  journal={計量生物学},
  volume={29},
  number={2},
  pages={141-170},
  year={2008},
  doi={10.5691/jjb.29.141}
}
@article{Kaplan-Meier1958,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2281868},
 author = {E. L. Kaplan and Paul Meier},
 journal = {Journal of the American Statistical Association},
 number = {282},
 pages = {457--481},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Nonparametric Estimation from Incomplete Observations},
 urldate = {2024-09-12},
 volume = {53},
 year = {1958}
}
@misc{Hardcastle+2024,
      title={Averaging polyhazard models using Piecewise deterministic Monte Carlo with applications to data with long-term survivors}, 
      author={Luke Hardcastle and Samuel Livingstone and Gianluca Baio},
      year={2024},
      eprint={2406.14182},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2406.14182}, 
}
@article{森満2016,
  title={専門医に必要な統計の知識と研究デザイン},
  author={森満},
  journal={日本耳鼻咽喉科学会会報},
  volume={119},
  number={7},
  pages={989-992},
  year={2016},
  doi={10.3950/jibiinkoka.119.989}
}
@article{Cutler-Ederer1958,
title = {Maximum utilization of the life table method in analyzing survival},
journal = {Journal of Chronic Diseases},
volume = {8},
number = {6},
pages = {699-712},
year = {1958},
issn = {0021-9681},
doi = {https://doi.org/10.1016/0021-9681(58)90126-7},
url = {https://www.sciencedirect.com/science/article/pii/0021968158901267},
author = {Sidney J. Cutler and Fred Ederer},
abstract = {We have illustrated the life table method for computing survival rates with 5-year survival data for cancer patients, emphasizing the advantage gained by including survival information on cases which entered the series too late to have had the opportunity to survive a full 5 years. The advantage is measured in terms of reduction in standard error of the survival rate. For the five series of patients in this paper, the reduction in standard error ranged from one-third to two-thirds.}
}
@techreport{Latimer2011,
	author = {N. Latimer},
	institution = {National Institute for Health and Care Excellence},
	title = {NICE DSU Technical Support Document 14: Undertaking Survival Analysis for Economic Evaluations alongside Clinical Trials--Estrapolation with Patient-level Data},
	year = {2011},
	url = {https://www.sheffield.ac.uk/nice-dsu/tsds/survival-analysis},
}
@article{Berger-Sun1993,
author = {James O. Berger and Dongchu Sun},
title = {Bayesian Analysis for the Poly-Weibull Distribution},
journal = {Journal of the American Statistical Association},
volume = {88},
number = {424},
pages = {1412--1418},
year = {1993},
publisher = {ASA Website},
doi = {10.1080/01621459.1993.10476426},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476426
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476426
    

}

}
@article{Louzada-Neto1999,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2533756},
 abstract = {We propose a polyhazard model to deal with lifetime data associated with latent competing risks. The causes of failure are assumed unobserved and affecting individuals independently. The general framework allows a broad class of hazard models that includes the most common hazard-based models. The model accommodates bathtub and multimodal hazards, keeping enough flexibility for common lifetime data that cannot be accommodated by usual hazard-based models. Maximum likelihood estimation is discussed, and parametric simulation is used for hypothesis testing.},
 author = {Francisco Louzada-Neto},
 journal = {Biometrics},
 number = {4},
 pages = {1281--1285},
 publisher = {[Wiley, International Biometric Society]},
 title = {Polyhazard Models for Lifetime Data},
 urldate = {2024-09-12},
 volume = {55},
 year = {1999}
}






@article{Latimer2013,
author = {Nicholas R. Latimer},
title ={Survival Analysis for Economic Evaluations Alongside Clinical Trials—Extrapolation with Patient-Level Data: Inconsistencies, Limitations, and a Practical Guide},

journal = {Medical Decision Making},
volume = {33},
number = {6},
pages = {743-754},
year = {2013},
doi = {10.1177/0272989X12472398},
    note ={PMID: 23341049},

URL = { 
    
        https://doi.org/10.1177/0272989X12472398
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0272989X12472398
    
    

},
}
@article{齋藤-室谷2023,
  title={マルチステートモデルの理論とがん臨床研究への応用},
  author={齋藤哲雄 and 室谷健太},
  journal={日本統計学会誌},
  volume={52},
  number={2},
  pages={221-267},
  year={2023},
  doi={10.11329/jjssj.52.221}
}
@article{齋藤-室谷2024,
  title={Competing Risks and Multistate Modelsin Oncology Clinical Trials},
  author={Tetsuo Saito and Kenta Murotani},
  journal={Japanese Journal of Biometrics},
  volume={45},
  number={1},
  pages={37-65},
  year={2024},
  doi={10.5691/jjb.45.37}
}
@article{Negrin+2017,
author = {Miguel A. Negrín and Julian Nam and Andrew H. Briggs},
title ={Bayesian Solutions for Handling Uncertainty in Survival Extrapolation},

journal = {Medical Decision Making},
volume = {37},
number = {4},
pages = {367-376},
year = {2017},
doi = {10.1177/0272989X16650669},
    note ={PMID: 27281336},

URL = { 
    
        https://doi.org/10.1177/0272989X16650669
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0272989X16650669
    
    

}
,
}
@article{Demiris+2015,
author = {Nikolaos Demiris and David Lunn and Linda D Sharples},
title ={Survival extrapolation using the poly-Weibull model},

journal = {Statistical Methods in Medical Research},
volume = {24},
number = {2},
pages = {287-301},
year = {2015},
doi = {10.1177/0962280211419645},
    note ={PMID: 21937472},

URL = { 
    
        https://doi.org/10.1177/0962280211419645
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0962280211419645
    
    

},
}
@article{Benaglia+2015,
author = {Benaglia, Tatiana and Jackson, Christopher H. and Sharples, Linda D.},
title = {Survival extrapolation in the presence of cause specific hazards},
journal = {Statistics in Medicine},
volume = {34},
number = {5},
pages = {796-811},
keywords = {survival analysis, survival extrapolation, poly-weibull, polyhazard, cause specific hazards},
doi = {https://doi.org/10.1002/sim.6375},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6375},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6375},
abstract = {Health economic evaluations require estimates of expected survival from patients receiving different interventions, often over a lifetime. However, data on the patients of interest are typically only available for a much shorter follow-up time, from randomised trials or cohorts. Previous work showed how to use general population mortality to improve extrapolations of the short-term data, assuming a constant additive or multiplicative effect on the hazards for all-cause mortality for study patients relative to the general population. A more plausible assumption may be a constant effect on the hazard for the specific cause of death targeted by the treatments. To address this problem, we use independent parametric survival models for cause-specific mortality among the general population. Because causes of death are unobserved for the patients of interest, a polyhazard model is used to express their all-cause mortality as a sum of latent cause-specific hazards. Assuming proportional cause-specific hazards between the general and study populations then allows us to extrapolate mortality of the patients of interest to the long term. A Bayesian framework is used to jointly model all sources of data. By simulation, we show that ignoring cause-specific hazards leads to biased estimates of mean survival when the proportion of deaths due to the cause of interest changes through time. The methods are applied to an evaluation of implantable cardioverter defibrillators for the prevention of sudden cardiac death among patients with cardiac arrhythmia. After accounting for cause-specific mortality, substantial differences are seen in estimates of life years gained from implantable cardioverter defibrillators. © 2014 The Authors Statistics in Medicine Published by John Wiley \& Sons Ltd.},
year = {2015}
}
@article{Mitchell-Beauchamp1988,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2290129},
 abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
 author = {T. J. Mitchell and J. J. Beauchamp},
 journal = {Journal of the American Statistical Association},
 number = {404},
 pages = {1023--1032},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Variable Selection in Linear Regression},
 urldate = {2024-09-12},
 volume = {83},
 year = {1988}
}
@article{Ley-Steel2009,
author = {Ley, Eduardo and Steel, Mark F.J.},
title = {On the effect of prior assumptions in Bayesian model averaging with applications to growth regression},
journal = {Journal of Applied Econometrics},
volume = {24},
number = {4},
pages = {651-674},
doi = {https://doi.org/10.1002/jae.1057},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.1057},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.1057},
abstract = {Abstract We consider the problem of variable selection in linear regression models. Bayesian model averaging has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. We examine the effect of a variety of prior assumptions on the inference concerning model size, posterior inclusion probabilities of regressors and on predictive performance. We illustrate these issues in the context of cross-country growth regressions using three datasets with 41–67 potential drivers of growth and 72–93 observations. Finally, we recommend priors for use in this and related contexts. Copyright © 2009 John Wiley \& Sons, Ltd.},
year = {2009}
}
@article{Polson-Scott2012,
author = {Nicholas G. Polson and James G. Scott},
title = {{On the Half-Cauchy Prior for a Global Scale Parameter}},
volume = {7},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {887 -- 902},
keywords = {hierarchical models, normal scale mixtures, shrinkage},
year = {2012},
doi = {10.1214/12-BA730},
URL = {https://doi.org/10.1214/12-BA730}
}
@article{Gelman2006,
author = {Andrew Gelman},
title = {{Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)}},
volume = {1},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {515 -- 534},
keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-$t$ distribution, half-$t$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2006},
doi = {10.1214/06-BA117A},
URL = {https://doi.org/10.1214/06-BA117A}
}
@article{Jasra+2005,
author = {A. Jasra and C. C. Holmes and D. A. Stephens},
title = {{Markov Chain Monte Carlo Methods and the Label Switching Problem in Bayesian Mixture Modeling}},
volume = {20},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 67},
keywords = {Bayesian statistics, Identifiability, label switching, MCMC, mixture modeling, sensitivity analysis},
year = {2005},
doi = {10.1214/088342305000000016},
URL = {https://doi.org/10.1214/088342305000000016}
}
@article{山口一大2022,
  title={項目反応理論モデルのパラメタ推定法の展開},
  author={山口 一大},
  journal={日本テスト学会誌},
  volume={18},
  number={1},
  pages={103-131},
  year={2022},
  doi={10.24690/jart.18.1_103}
}
@phdthesis{Wilson1984,
	author = {Mark R. Wilson},
	school = {University of Chicago},
	title = {A Psychometric Model of Hierarchical Development},
	year = {1984},

}

@article{Embretson1984,
	abstract = {The purpose of the current paper is to propose a general multicomponent latent trait model (GLTM) for response processes. The proposed model combines the linear logistic latent trait (LLTM) with the multicomponent latent trait model (MLTM). As with both LLTM and MLTM, the general multicomponent latent trait model can be used to (1) test hypotheses about the theoretical variables that underlie response difficulty and (2) estimate parameters that describe test items by basic substantive properties. However, GLTM contains both component outcomes and complexity factors in a single model and may be applied to data that neither LLTM nor MLTM can handle. Joint maximum likelihood estimators are presented for the parameters of GLTM and an application to cognitive test items is described.},
	author = {Embretson (Whitely), Susan},
	date = {1984/06/01},
	date-added = {2024-09-17 13:20:02 +0900},
	date-modified = {2024-09-17 13:20:02 +0900},
	doi = {10.1007/BF02294171},
	id = {Embretson (Whitely)1984},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {175--186},
	title = {A general latent trait model for response processes},
	url = {https://doi.org/10.1007/BF02294171},
	volume = {49},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1007/BF02294171}}
@misc{yuimadocs2024,
	author = {The YUIMA Project Team},
	title = {Package `yuima`},
	year = {2024},
	url = {https://cran.r-project.org/web/packages/yuima/index.html},
}