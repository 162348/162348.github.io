@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
@inproceedings{Zhao+2024,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
	  booksubtitle    = {International Conference on Machine Learning},
      url={https://arxiv.org/abs/2404.17546}, 
}
@article{Teh+2003,
author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
title = {Energy-based models for sparse overcomplete representations},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1235–1260},
numpages = {26},
url = {https://www.jmlr.org/papers/v4/teh03a.html}
}

@InProceedings{Xie+2016,
  title = 	 {A Theory of Generative ConvNet},
  author = 	 {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2635--2644},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/xiec16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/xiec16.html},
  abstract = 	 {We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.}
}
@inproceedings{He+2019,
      title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders}, 
      author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
      year={2019},
      booksubtitle    = {International Conference on Learning Representations},
      url={https://arxiv.org/abs/1901.05534}, 
}
@inproceedings{Hinton-Teh2001,
author = {Hinton, Geoffrey E. and Teh, Yee-Whye},
title = {Discovering multiple constraints that are frequently approximately satisfied},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {227–234},
numpages = {8},
location = {Seattle, Washington},
series = {UAI'01},
url             = {https://dl.acm.org/doi/abs/10.5555/2074022.2074051},
}
@misc{Finn+2016,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03852}, 
}
@inproceedings{Swersku+2011,
author = {Swersky, Kevin and Ranzato, Marc'Aurelio and Buchman, David and Marlin, Benjamin M. and Freitas, Nandode},
title = {On autoencoders and score matching for energy based models},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider estimation methods for the class of continuous-data energy based models (EBMs). Our main result shows that estimating the parameters of an EBM using score matching when the conditional distribution over the visible units is Gaussian corresponds to training a particular form of regularized autoencoder. We show how different Gaussian EBMs lead to different autoencoder architectures, providing deep links between these two families of models. We compare the score matching estimator for the mPoT model, a particular Gaussian EBM, to several other training methods on a variety of tasks including image denoising and unsupervised feature extraction. We show that the regularization function induced by score matching leads to superior classification performance relative to a standard autoencoder. We also show that score matching yields classification results that are indistinguishable from better-known stochastic approximation maximum likelihood estimators.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {1201–1208},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104633},
}

@inproceedings{Koster-Hyvarinen2007,
	abstract = {Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Hyv{\"a}rinen, Aapo},
	booktitle = {Artificial Neural Networks -- ICANN 2007},
	editor = {de S{\'a}, Joaquim Marques and Alexandre, Lu{\'\i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
	isbn = {978-3-540-74695-9},
	pages = {798--807},
	publisher = {Springer Berlin Heidelberg},
	title = {A Two-Layer ICA-Like Model Estimated by Score Matching},
	year = {2007}}

@inproceedings{Koster+2009,
	abstract = {Markov Random Field (MRF) models with potentials learned from the data have recently received attention for learning the low-level structure of natural images. A MRF provides a principled model for whole images, unlike ICA, which can in practice be estimated for small patches only. However, learning the filters in an MRF paradigm has been problematic in the past since it required computationally expensive Monte Carlo methods. Here, we show how MRF potentials can be estimated using Score Matching (SM). With this estimation method we can learn filters of size 12 {\texttimes}12 pixels, considerably larger than traditional ''hand-crafted'' MRF potentials. We analyze the tuning properties of the filters in comparison to ICA filters, and show that the optimal MRF potentials are similar to the filters from an overcomplete ICA model.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Lindgren, Jussi T. and Hyv{\"a}rinen, Aapo},
	booktitle = {Independent Component Analysis and Signal Separation},
	editor = {Adali, T{\"u}lay and Jutten, Christian and Romano, Jo{\~a}o Marcos Travassos and Barros, Allan Kardec},
	isbn = {978-3-642-00599-2},
	pages = {515--522},
	publisher = {Springer Berlin Heidelberg},
	title = {Estimating Markov Random Field Potentials for Natural Images},
	year = {2009}}

@InProceedings{Gutmann-Hyvarinen2010,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract = 	 {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}

@InProceedings{Perpinan-Hinton2005,
  title = 	 {On Contrastive Divergence Learning},
  author =       {Carreira-Perpi{\~n}\'an, Miguel \'A. and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {33--40},
  year = 	 {2005},
  editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
  volume = 	 {R5},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--08 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf},
  url = 	 {https://proceedings.mlr.press/r5/carreira-perpinan05a.html},
  note =         {Reissued by PMLR on 30 March 2021.}
}

@inproceedings{Kingma-LeCun2010,
	author = {Durk P Kingma and Yann LeCun},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
	publisher = {Curran Associates, Inc.},
	title = {Regularized estimation of image statistics by Score Matching},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},
	volume = {23},
	year = {2010},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf}}
@ARTICLE{Drucker-LeCun1992,
  author={Drucker, H. and Le Cun, Y.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Improving generalization performance using double backpropagation}, 
  year={1992},
  volume={3},
  number={6},
  pages={991-997},
  keywords={Testing;Backpropagation algorithms;Jacobian matrices;Neurons;Signal to noise ratio;Neural networks},
  doi={10.1109/72.165600}}
@inproceedings{Song+2019,
      title={{Sliced Score Matching: A Scalable Approach to Density and Score Estimation}},
      author={Yang Song and Sahaj Garg and Jiaxin Shi and Stefano Ermon},
      year={2019},
      booksubtitle    = {Uncertainty in Artificial Intelligence},
      url={https://arxiv.org/abs/1905.07088}, 
}
@ARTICLE{Hyvarinen2007,
  author={Hyvarinen, Aapo},
  journal={IEEE Transactions on Neural Networks}, 
  title={Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables}, 
  year={2007},
  volume={18},
  number={5},
  pages={1529-1531},
  keywords={Samarium;Statistical analysis;Probability density function;Parameter estimation;Monte Carlo methods;Computer science;Information technology;Normalization constant;partition function;statistical estimation},
  doi={10.1109/TNN.2007.895819}}
@inproceedings{Lyu2009,
author = {Lyu, Siwei},
title = {Interpretation and generalization of score matching},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {359–366},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@inproceedings{Gutmann-Hirayama2011,
author = {Gutmann, Michael U. and Hirayama, Jun-ichiro},
title = {Bregman divergence as general framework to estimate unnormalized statistical models},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in un-supervised learning.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {283–290},
numpages = {8},
location = {Barcelona, Spain},
series = {UAI'11}
}

@InProceedings{Chwialkowski+2016,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}

@InProceedings{Liu+2016,
  title = 	 {A Kernelized Stein Discrepancy for Goodness-of-fit Tests},
  author = 	 {Liu, Qiang and Lee, Jason and Jordan, Michael},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {276--284},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/liub16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/liub16.html},
  abstract = 	 {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.}
}
@misc{McAllester2023,
      title={On the Mathematics of Diffusion Models}, 
      author={David McAllester},
      year={2023},
      eprint={2301.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.11108}, 
}
@article{Yang+2023,
author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626235},
doi = {10.1145/3626235},
abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github:},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {39},
keywords = {stochastic differential equations, score-based generative models, diffusion models, Generative models}
}
@unpublished{Kreis+2022,
    author = {Karsten Kreis and Ruiqi Gao and Arash Vahdat},
    year   = {2022},
    title  = {Denoising Diffusion-based Generative Modeling: Foundations and Applications},
    url    = {https://cvpr2022-tutorial-diffusion-models.github.io/},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@unpublished{Song+2023,
    author = {Jiaming Song and Chenlin Meng and Arash Vahdat},
    year   = {2023},
    title  = {Denoising Diffusion Models: A Generative Learning Big Bang},
    url    = {https://cvpr.thecvf.com/virtual/2023/tutorial/18546},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@INPROCEEDINGS{Choi+2022,
  author={Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Perception Prioritized Training of Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={11462-11471},
  keywords={Training;Visualization;Computational modeling;Noise reduction;Data models;Image restoration;Pattern recognition;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01118}}
@inproceedings{Kingma+2021,
author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
title = {Variational diffusion models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1660},
numpages = {12},
series = {NIPS '21},
url             = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
}
@inproceedings{Salimans-Ho2021,
title={Should {EBM}s model the energy or the score?},
author={Tim Salimans and Jonathan Ho},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
url={https://openreview.net/forum?id=9AS-TF2jRNb}
}

@article{Anderson1982,
	abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.},
	author = {Brian D.O. Anderson},
	doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	number = {3},
	pages = {313-326},
	title = {Reverse-time diffusion equation models},
	url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	volume = {12},
	year = {1982},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	bdsk-url-2 = {https://doi.org/10.1016/0304-4149(82)90051-5}}

@article{Haussmann-Pardoux1986,
	author = {U. G. Haussmann and E. Pardoux},
	doi = {10.1214/aop/1176992362},
	journal = {The Annals of Probability},
	keywords = {diffusion process, Kolmogorov equation, Markov process, Martingale problem, Time reversal},
	number = {4},
	pages = {1188 -- 1205},
	publisher = {Institute of Mathematical Statistics},
	title = {{Time Reversal of Diffusions}},
	url = {https://doi.org/10.1214/aop/1176992362},
	volume = {14},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176992362}}
@inproceedings{Karras+2022,
title={Elucidating the Design Space of Diffusion-Based Generative Models},
author={Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=k7FuTOWMOc7}
}
@ARTICLE {Croitoru+2023,
author = {F. Croitoru and V. Hondru and R. Ionescu and M. Shah},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Diffusion Models in Vision: A Survey},
year = {2023},
volume = {45},
number = {09},
issn = {1939-3539},
pages = {10850-10869},
abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
keywords = {computational modeling;mathematical models;noise reduction;data models;computer vision;training;task analysis},
doi = {10.1109/TPAMI.2023.3261988},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2023.3261988},
}

@ARTICLE{Cao+2024,
  author={Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Generative Diffusion Models}, 
  year={2024},
  volume={36},
  number={7},
  pages={2814-2830},
  keywords={Mathematical models;Kernel;Computational modeling;Training;Surveys;Noise reduction;Markov processes;Diffusion model;deep generative model;diffusion algorithm;diffusion applications},
  doi={10.1109/TKDE.2024.3361474}}
@inproceedings{JiamingSong+2021,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}
@inproceedings{Gao+2021,
title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P Kingma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v_1Soh8QUNc}
}
@inproceedings{Xiao+2021,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JprM0p-q0Co}
}
@inproceedings{Salimans-Ho2022,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}
@inproceedings{Vahdat+2021,
	author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {11287--11302},
	publisher = {Curran Associates, Inc.},
	title = {Score-based Generative Modeling in Latent Space},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf}}

@article{Pandey+2022,
title={Diffuse{VAE}: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents},
author={Kushagra Pandey and Avideep Mukherjee and Piyush Rai and Abhishek Kumar},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ygoNPRiLxw},
note={}
}

@InProceedings{Nichol-Dhariwal2021,
  title = 	 {Improved Denoising Diffusion Probabilistic Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}
@inproceedings{Ho-Salimans2021,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}
@article{Ho+2022,
  author  = {Jonathan Ho and Chitwan Saharia and William Chan and David J. Fleet and Mohammad Norouzi and Tim Salimans},
  title   = {Cascaded Diffusion Models for High Fidelity Image Generation},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {47},
  pages   = {1--33},
  url     = {http://jmlr.org/papers/v23/21-0635.html}
}
@misc{Saharia+2022SIGGRAPH,
title={Palette: Image-to-Image Diffusion Models},
author={Chitwan Saharia and William Chan and Huiwen Chang and Chris A. Lee and Jonathan Ho and Tim Salimans and David J. Fleet and Mohammad Norouzi},
year={2022},
url={https://openreview.net/forum?id=FPGs276lUeq}
}

@inproceedings{Austin+2021,
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17981--17993},
	publisher = {Curran Associates, Inc.},
	title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf}}
@InProceedings{Chang+2022,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@article{Heng+2024,
	author = {Jeremy Heng and Valentin De Bortoli and Arnaud Doucet},
	doi = {10.1214/23-STS908},
	journal = {Statistical Science},
	keywords = {Optimal transport, Schr{\"o}dinger bridge, score matching, Stochastic differential equation, Time reversal},
	number = {1},
	pages = {90 -- 99},
	publisher = {Institute of Mathematical Statistics},
	title = {{Diffusion Schr{\"o}dinger Bridges for Bayesian Computation}},
	url = {https://doi.org/10.1214/23-STS908},
	volume = {39},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1214/23-STS908}}
@inproceedings{Chung+2023,
title={Diffusion Posterior Sampling for General Noisy Inverse Problems},
author={Hyungjin Chung and Jeongsol Kim and Michael Thompson Mccann and Marc Louis Klasky and Jong Chul Ye},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=OnD9zGAGT0k}
}

@InProceedings{Song+2023,
  title = 	 {Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation},
  author =       {Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32483--32498},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23k/song23k.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23k.html},
  abstract = 	 {We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion (LGD), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a Monte Carlo method that uses multiple samples from a suitable distribution to reduce bias. Our method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and controllable motion synthesis. Notably, we show how our method can be applied to control a pretrained motion diffusion model to follow certain paths and avoid obstacles that are proven challenging to prior methods.}
}

@InProceedings{Shi+2022,
  title = 	 {Conditional simulation using diffusion {S}chr{ö}dinger bridges},
  author =       {Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1792--1802},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/shi22a/shi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/shi22a.html},
  abstract = 	 {Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr{ö}dinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schrödinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.}
}

@inproceedings{DeBortoli+2021,
	author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17695--17709},
	publisher = {Curran Associates, Inc.},
	title = {Diffusion Schr\"{o}dinger Bridge with Applications to Score-Based Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf}}

@InProceedings{Kurras2015,
  title = 	 {{Symmetric Iterative Proportional Fitting}},
  author = 	 {Kurras, Sven},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {526--534},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/kurras15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/kurras15.html},
  abstract = 	 {Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix W*. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W=W’ and r=c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of W* in terms of expansion properties of the undirected weighted graph represented by W. Finally, we show how our results contribute to recent work in machine learning.}
}
@article{Sinkhorn1967,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2314570},
 author = {Richard Sinkhorn},
 journal = {The American Mathematical Monthly},
 number = {4},
 pages = {402--405},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Diagonal Equivalence to Matrices with Prescribed Row and Column Sums},
 urldate = {2024-08-04},
 volume = {74},
 year = {1967}
}

@article{Sinkhorn-Knopp1967,
	author = {Richard Sinkhorn and Paul Knopp},
	journal = {Pacific Journal of Mathematics},
	number = {2},
	pages = {343 -- 348},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{Concerning Nonnegative Matrices and Doubly Stochastic Matrices}},
	volume = {21},
	year = {1967},
  url             = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-21/issue-2/Concerning-nonnegative-matrices-and-doubly-stochastic-matrices/pjm/1102992505.full},
}

@article{Fortet1940,
    author          = {Robert Fortet},
    year            = {1940},
    title           = {{Résolution d'un système d'équations de M. Schrödinger}},
    journal         = {Journal de Mathématiques Pures et Appliquées, Series 9},
    volume          = {19},
    number          = {1-4},
    pages           = {83-105},
    url             = {http://www.numdam.org/item/JMPA_1940_9_19_1-4_83_0/}
}
@article{Deming-Stephan1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235722},
 author = {W. Edwards Deming and Frederick F. Stephan},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {427--444},
 publisher = {Institute of Mathematical Statistics},
 title = {On a Least Squares Adjustment of a Sampled Frequency Table When the Expected Marginal Totals are Known},
 urldate = {2024-08-04},
 volume = {11},
 year = {1940}
}
@article{Kullback1968,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239692},
 author = {S. Kullback},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {1236--1243},
 publisher = {Institute of Mathematical Statistics},
 title = {Probability Densities with Given Marginals},
 urldate = {2024-08-04},
 volume = {39},
 year = {1968}
}
@article{Ireland-Kullback1968,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334462},
 abstract = {In its simplest formulation the problem considered is to estimate the cell probabilities pij of an r × c contingency table for which the marginal probabilities $p_{i\ldot}$ and $p_{\ldot j}$ are known and fixed, so as to minimize ΣΣpijln (pij/πij), where πij are the corresponding entries in a given contingency table. An iterative procedure is given for determining the estimates and it is shown that the estimates are BAN, and that the iterative procedure is convergent. A summary of results for a four-way contingency table is given. An illustrative example is given.},
 author = {C. T. Ireland and S. Kullback},
 journal = {Biometrika},
 number = {1},
 pages = {179--188},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Contingency Tables with Given Marginals},
 urldate = {2024-08-04},
 volume = {55},
 year = {1968}
}
@inproceedings{Vargas-Grathwohl-Doucet2023,
title={{Denoising Diffusion Samplers}},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@inproceedings{Follmer1985,
	abstract = {We introduce an entropy technique which allows to treat some infinite-dimensional extensions of the classical duality equations for the time reversal of diffusion processes.},
	address = {Berlin, Heidelberg},
	author = {F{\"o}llmer, H.},
	booktitle = {Stochastic Differential Systems Filtering and Control},
	editor = {Metivier, M. and Pardoux, E.},
	isbn = {978-3-540-39253-8},
	pages = {156--163},
	publisher = {Springer Berlin Heidelberg},
	title = {An entropy approach to the time reversal of diffusion processes},
	year = {1985}}

@InProceedings{Barr+2020,
  title = 	 {Quantum Ground States from Reinforcement Learning},
  author =       {Barr, Ariel and Gispen, Willem and Lamacraft, Austen},
  booktitle = 	 {Proceedings of The First Mathematical and Scientific Machine Learning Conference},
  pages = 	 {635--653},
  year = 	 {2020},
  editor = 	 {Lu, Jianfeng and Ward, Rachel},
  volume = 	 {107},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v107/barr20a/barr20a.pdf},
  url = 	 {https://proceedings.mlr.press/v107/barr20a.html},
  abstract = 	 {  Finding the ground state of a quantum mechanical system can be formulated as an optimal control problem. In this formulation, the drift of the optimally controlled process is chosen to match the distribution of paths in the Feynman–Kac (FK) representation of the solution of the imaginary time Schrödinger equation. This provides a variational principle that can be used for reinforcement learning of a neural representation of the drift. Our approach is a drop-in replacement for path integral Monte Carlo, learning an optimal importance sampler for the FK trajectories. We demonstrate the applicability of our approach to several problems of one-, two-, and many-particle physics.}
}
@inproceedings{Zhang+2021,
title={Sampling via Controlled Stochastic Dynamical Systems},
author={Benjamin Zhang and Tuhin Sahai and Youssef Marzouk},
booktitle={I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop},
year={2021},
url={https://openreview.net/forum?id=dHruzYDH719}
}
@article{DeBortoli2022,
title={Convergence of denoising diffusion models under the manifold hypothesis},
author={Valentin De Bortoli},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=MhK5aXo3gB},
note={Expert Certification}
}
@misc{Montanari-Wu2023,
      title={Posterior Sampling from the Spiked Models via Diffusion Processes}, 
      author={Andrea Montanari and Yuchen Wu},
      year={2023},
      eprint={2304.11449},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2304.11449}, 
}


@article{Crooks1998,
	abstract = {An equality has recently been shown relating the free energy difference between two equilibrium ensembles of a system and an ensemble average of the work required to switch between these two configurations. In the present paper it is shown that this result can be derived under the assumption that the system's dynamics is Markovian and microscopically reversible.},
	author = {Crooks, Gavin E. },
	date = {1998/03/01},
	date-added = {2024-08-05 13:48:11 +0900},
	date-modified = {2024-08-05 13:48:11 +0900},
	doi = {10.1023/A:1023208217925},
	id = {Crooks1998},
	isbn = {1572-9613},
	journal = {Journal of Statistical Physics},
	number = {5},
	pages = {1481--1487},
	title = {{Nonequilibrium Measurements of Free Energy Differences for Microscopically Reversible Markovian Systems}},
	url = {https://doi.org/10.1023/A:1023208217925},
	volume = {90},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1023208217925}}
@article{Jarzynski1997Equality,
  title = {{Nonequilibrium Equality for Free Energy Differences}},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. Lett.},
  volume = {78},
  issue = {14},
  pages = {2690--2693},
  numpages = {0},
  year = {1997},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.78.2690},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.78.2690}
}
@article{Jarzynski1997MasterEquation,
  title = {Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. E},
  volume = {56},
  issue = {5},
  pages = {5018--5035},
  numpages = {0},
  year = {1997},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.56.5018},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.56.5018}
}
@inproceedings{Doucet+2022,
title={Score-Based Diffusion meets Annealed Importance Sampling},
author={Arnaud Doucet and Will Sussman Grathwohl and Alexander G. D. G. Matthews and Heiko Strathmann},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=9cU2iW3bz0}
}

@inproceedings{Wu+2020,
	author = {Wu, Hao and K\"{o}hler, Jonas and Noe, Frank},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {5933--5944},
	publisher = {Curran Associates, Inc.},
	title = {{Stochastic Normalizing Flows}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf}}

@InProceedings{Thin+2021,
  title = 	 {Monte Carlo Variational Auto-Encoders},
  author =       {Thin, Achille and Kotelevskii, Nikita and Doucet, Arnaud and Durmus, Alain and Moulines, Eric and Panov, Maxim},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10247--10257},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/thin21a/thin21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/thin21a.html},
  abstract = 	 {Variational auto-encoders (VAE) are popular deep latent variable models which are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter ELBO and hence better variational approximations, it has been proposed to use importance sampling to get a lower variance estimate of the evidence. However, importance sampling is known to perform poorly in high dimensions. While it has been suggested many times in the literature to use more sophisticated algorithms such as Annealed Importance Sampling (AIS) and its Sequential Importance Sampling (SIS) extensions, the potential benefits brought by these advanced techniques have never been realized for VAE: the AIS estimate cannot be easily differentiated, while SIS requires the specification of carefully chosen backward Markov kernels. In this paper, we address both issues and demonstrate the performance of the resulting Monte Carlo VAEs on a variety of applications.}
}
@article{Tang+2024,
    author          = {Weipin Tang and Yuhang Wu and Xunyu Zhou},
    year            = {2024},
    title           = {{Discrete-Time Simulated Annealing: A Convergence Analysis via the Eyring-Kramers Law}},
    journal         = {Numerical Algebra, Control and Optimization},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://doi.org/10.3934/naco.2024015}
}

@article{Fournier-Tardif2021,
	abstract = {Using a localization procedure and the result of Holley-Kusuoka-Stroock [8] in the torus, we widely weaken the usual growth assumptions concerning the success of the continuous-time simulated annealing in Rd. Our only assumption is the existence of an invariant probability measure for a sufficiently low temperature. We also prove, in an appendix, a non-explosion criterion for a class of time-inhomogeneous diffusions.},
	author = {Nicolas Fournier and Camille Tardif},
	doi = {https://doi.org/10.1016/j.jfa.2021.109086},
	issn = {0022-1236},
	journal = {Journal of Functional Analysis},
	keywords = {Simulated annealing, Time-inhomogeneous diffusion processes, Large time behavior, Non-explosion},
	number = {5},
	pages = {109086},
	title = {On the simulated annealing in Rd},
	url = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	volume = {281},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	bdsk-url-2 = {https://doi.org/10.1016/j.jfa.2021.109086}}

@inproceedings{Chen+2018,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Ordinary Differential Equations},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf}}
@inproceedings{Grathwohl+2019,
title={Scalable Reversible Generative Models with Free-form Continuous Dynamics},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJxgknCcK7},
}
@article{Gortler+2019,
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  title = {A Visual Exploration of Gaussian Processes},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  doi = {10.23915/distill.00017}
}
@article{Tipping2001,
    author          = {Michael E. Tipping},
    year            = {2001},
    title           = {Sparse Bayesian Learning and the Relevance Vector Machine},
    journal         = {Journal of Machine Learning Research},
    volume          = {1},
    number          = {},
    pages           = {211-244},
    url             = {https://www.jmlr.org/papers/v1/tipping01a.html}
}
@INPROCEEDINGS{Loeliger+2016,
  author={Loeliger, Hans-Andrea and Bruderer, Lukas and Malmberg, Hampus and Wadehn, Federico and Zalmai, Nour},
  booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
  title={On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  keywords={Computational modeling;Covariance matrices;Message passing;Smoothing methods;Maximum likelihood estimation;Kalman filters},
  doi={10.1109/ITA.2016.7888168}}
@phdthesis{Gibbs1997,
    author      = {M. N. Gibbs},
    school      = {Cambridge University},
    title       = {Bayesian Gaussian Process Regression and Classification},
    year        = {1997},
    url             = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b5a0c62c8d7cf51137bfb079947b8393c00ed169},
}

@InProceedings{Heinonen+2016,
  title = 	 {Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo},
  author = 	 {Heinonen, Markus and Mannerström, Henrik and Rousu, Juho and Kaski, Samuel and Lähdesmäki, Harri},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {732--740},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/heinonen16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/heinonen16.html},
  abstract = 	 {We present a novel approach for non-stationary Gaussian process regression (GPR), where the three key parameters – noise variance, signal variance and lengthscale – can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. For inferring the full posterior distribution we use Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with the gradients. The MAP solution can also be learned with gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the non-stationary GPR is shown to give major improvement when modeling realistic input-dependent dynamics.}
}
@mastersthesis{Krige1951,
    author  = {D. G. Krige},
    school  = {University of the Witwatersrand, Faculty of Engineering},
    title   = {A Statistical Approach to Some Mine Valuation and Allied Problems on the Witwatersrand},
    year    = {1951},
    url             = {http://hdl.handle.net/10539/17975},
}

@inproceedings{Remes+2017,
	author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Non-Stationary Spectral Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf}}

@article{Kriege+2020,
	abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.},
	author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
	date = {2020/01/14},
	date-added = {2024-08-08 14:27:51 +0900},
	date-modified = {2024-08-08 14:27:51 +0900},
	doi = {10.1007/s41109-019-0195-3},
	id = {Kriege2020},
	isbn = {2364-8228},
	journal = {Applied Network Science},
	number = {1},
	pages = {6},
	title = {A survey on graph kernels},
	url = {https://doi.org/10.1007/s41109-019-0195-3},
	volume = {5},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s41109-019-0195-3}}
@article{Lodhi+2002,
    author          = {Huma Lodhi and Craig Saunders and John Shawe-Taylor and Nello Cristianini and Chris Watkins},
    year            = {2002},
    title           = {Text Classification using String Kernels},
    journal         = {Journal of Machine Learning Research},
    volume          = {2},
    number          = {},
    pages           = {419-444},
    url             = {https://www.jmlr.org/papers/v2/lodhi02a.html}
}

@inproceedings{Borgwardt+2006,
	author = {Borgwardt, Karsten and Schraudolph, Nicol and Vishwanathan, S.v.n.},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
	publisher = {MIT Press},
	title = {Fast Computation of Graph Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf},
	volume = {19},
	year = {2006},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf}}
@article{Shervashidze+2011,
  author  = {Nino Shervashidze and Pascal Schweitzer and Erik Jan van Leeuwen and Kurt Mehlhorn and Karsten M. Borgwardt},
  title   = {Weisfeiler-Lehman Graph Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {77},
  pages   = {2539-2561},
  url     = {http://jmlr.org/papers/v12/shervashidze11a.html}
}

@InProceedings{Wilson-Adams2013,
  title = 	 {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  author = 	 {Wilson, Andrew and Adams, Ryan},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1067--1075},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wilson13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wilson13.html},
  abstract = 	 {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}
@inproceedings{Sutherland-Schneider2015,
author = {Sutherland, Danica J. and Schneider, Jeff},
title = {On the error of random fourier features},
year = {2015},
isbn = {9780996643108},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Kernel methods give powerful, flexible, and theoretically grounded approaches to solving many problems in machine learning. The standard approach, however, requires pairwise evaluations of a kernel function, which can lead to scalability issues for very large datasets. Rahimi and Recht (2007) suggested a popular approach to handling this problem, known as random Fourier features. The quality of this approximation, however, is not well understood. We improve the uniform error bound of that paper, as well as giving novel understandings of the embedding's variance, approximation error, and use in some machine learning methods. We also point out that surprisingly, of the two main variants of those features, the more widely used is strictly higher-variance for the Gaussian kernel and has worse bounds.},
booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
pages = {862–871},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {UAI'15}
}


@inproceedings{Rahimi-Recht2007,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Random Features for Large-Scale Kernel Machines},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}}

@inproceedings{Rahimi-Recht2008,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf}}

@inproceedings{Yu+2016,
	author = {Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Orthogonal Random Features},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf}}
@article{Kimeldorf-Wahba1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239347},
 author = {George S. Kimeldorf and Grace Wahba},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {495--502},
 publisher = {Institute of Mathematical Statistics},
 title = {A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines},
 urldate = {2024-08-08},
 volume = {41},
 year = {1970}
}

@inproceedings{Scholkopf+2001,
	abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
	address = {Berlin, Heidelberg},
	author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
	booktitle = {Computational Learning Theory},
	editor = {Helmbold, David and Williamson, Bob},
	isbn = {978-3-540-44581-4},
	pages = {416--426},
	publisher = {Springer Berlin Heidelberg},
	title = {A Generalized Representer Theorem},
	year = {2001}}
@article{Wilkinson+2023,
  author  = {William J. Wilkinson and Simo Särkkä and Arno Solin},
  title   = {Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {83},
  pages   = {1--50},
  url     = {http://jmlr.org/papers/v24/21-1298.html}
}
@inproceedings{Wenzel+2019,
    author          = {Florian Wenzel and Théo Galy-Fajou and Christan Donner and Marius Kolft and Manfred Opper},
    year            = {2019},
    title           = {Efficient Gaussian Process Classification Using Pólya-Gamma Data Augmentation},
    booktitle       = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume          = {33},
    pages           = {},
    url             = {},
    doi             = {10.1609/aaai.v33i01.33015417},
}

@InProceedings{Galy-Fajou2020,
  title = 	 {Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models},
  author =       {Galy-Fajou, Theo and Wenzel, Florian and Opper, Manfred},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3025--3035},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/galy-fajou20a/galy-fajou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/galy-fajou20a.html},
  abstract = 	 {We propose automated augmented conjugate inference, a new inference method for non-conjugate Gaussian processes (GP) models.Our method automatically constructs an auxiliary variable augmentation that renders the GP model conditionally conjugate. Building on the conjugate structure of the augmented model, we develop two inference methods. First, a fast and scalable stochastic variational inference method that uses efficient block coordinate ascent updates, which are computed in closed form. Second, an asymptotically correct Gibbs sampler that is useful for small datasets.Our experiments show that our method is up two orders of magnitude faster and more robust than existing state-of-the-art black-box methods.}
}
@ARTICLE{Liu+2020,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  keywords={Kernel;Scalability;Sparse representation;Complexity theory;Computational modeling;Ground penetrating radar;Predictive models;Big data;Gaussian process regression (GPR);local approximations;scalability;sparse approximations},
  doi={10.1109/TNNLS.2019.2957109}}

@InProceedings{Wilson+2020,
  title = 	 {Efficiently sampling functions from {G}aussian process posteriors},
  author =       {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10292--10302},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wilson20a/wilson20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/wilson20a.html},
  abstract = 	 {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes’ statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.}
}
@INPROCEEDINGS{Hartikainen-Sarkka2010,
  author={Hartikainen, Jouni and Särkkä, Simo},
  booktitle={2010 IEEE International Workshop on Machine Learning for Signal Processing}, 
  title={Kalman filtering and smoothing solutions to temporal Gaussian process regression models}, 
  year={2010},
  volume={},
  number={},
  pages={379-384},
  keywords={Kalman filters;Approximation methods;Mathematical model;Markov processes;Computational modeling;Gaussian processes;Equations},
  doi={10.1109/MLSP.2010.5589113}}

@inproceedings{Snelson-Ghahramani2005,
	author = {Snelson, Edward and Ghahramani, Zoubin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Sparse Gaussian Processes using Pseudo-inputs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf}}

@inproceedings{Wilson+2014,
	author = {Wilson, Andrew G and Gilboa, Elad and Nehorai, Arye and Cunningham, John P},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Fast Kernel Learning for Multidimensional Pattern Extrapolation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf}}

@inproceedings{Salakhutdinov-Hinton2007,
	author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf}}

@InProceedings{Ober+2021,
  title = 	 {The promises and pitfalls of deep kernel learning},
  author =       {Ober, Sebastian W. and Rasmussen, Carl E. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1206--1216},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/ober21a/ober21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/ober21a.html},
  abstract = 	 {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.}
}
@inproceedings{Novak+2019,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}
@misc{Yang2020,
      title={Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation}, 
      author={Greg Yang},
      year={2020},
      eprint={1902.04760},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1902.04760}, 
}

@InProceedings{Damianou-Lawrence2013,
  title = 	 {Deep {G}aussian Processes},
  author = 	 {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {207--215},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/damianou13a.pdf},
  url = 	 {https://proceedings.mlr.press/v31/damianou13a.html},
  abstract = 	 {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@inproceedings{Jacot+2018,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}}

@InProceedings{Woodworth+2020,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
  abstract = 	 { A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e.&nbsp;when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by \citet{chizat2018note}, we show how the \textbf{\textit{scale of the initialization}} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-$D$ linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the \emph{width}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
}

@InProceedings{Yang-Hu2021,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}

@inproceedings{Chizat+2019,
	author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On Lazy Training in Differentiable Programming},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}}
@ARTICLE{Sarkka+2013,
  author={Särkkä, Simo and Solin, Arno and Hartikainen, Jouni},
  journal={IEEE Signal Processing Magazine}, 
  title={Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering}, 
  year={2013},
  volume={30},
  number={4},
  pages={51-61},
  keywords={Machine learning;Learning systems;Gaussian processes;Bayes methods;Parametric statistics;Linear regression analysis;Kalman filters;Smoothing methods;Spatiotemporal phenomena;Kernel},
  doi={10.1109/MSP.2013.2246292}}

@InProceedings{Adam+2020,
  title = 	 {Doubly Sparse Variational Gaussian Processes},
  author =       {Adam, Vincent and Eleftheriadis, Stefanos and Artemev, Artem and Durrande, Nicolas and Hensman, James},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2874--2884},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/adam20a/adam20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/adam20a.html},
  abstract = 	 {The use of Gaussian process models is typically limited to datasets with a few tens of thousands of observations due to their complexity and memory footprint.The two most commonly used methods to overcome this limitation are 1) the variational sparse approximation which relies on inducing points and 2) the state-space equivalent formulation of Gaussian processes which can be seen as exploiting some sparsity in the precision matrix.In this work, we propose to take the best of both worlds: we show that the inducing point framework is still valid for state space models and that it can bring further computational and memory savings. Furthermore, we provide the natural gradient formulation for the proposed variational parameterisation.Finally, this work makes it possible to use the state-space formulation inside deep Gaussian process models as illustrated in one of the experiments. }
}
@article{Jona-Lasinio+2012,
 ISSN = {19326157, 19417330},
 URL = {http://www.jstor.org/stable/41713483},
 abstract = {Directional data arise in various contexts such as oceanography (wave directions) and meteorology (wind directions), as well as with measurements on a periodic scale (weekdays, hours, etc.). Our contribution is to introduce a model-based approach to handle periodic data in the case of measurements taken at spatial locations, anticipating structured dependence between these measurements. We formulate a wrapped Gaussian spatial process model for this setting, induced from a customary linear Gaussian process. We build a hierarchical model to handle this situation and show that the fitting of such a model is possible using standard Markov chain Monte Carlo methods. Our approach enables spatial interpolation (and can accommodate measurement error). We illustrate with a set of wave direction data from the Adriatic coast of Italy, generated through a complex computer model.},
 author = {Giovanna Jona-Lasinio and Alan Gelfand and Mattia Jona-Lasinio},
 journal = {The Annals of Applied Statistics},
 number = {4},
 pages = {1478--1498},
 publisher = {Institute of Mathematical Statistics},
 title = {SPATIAL ANALYSIS OF WAVE DIRECTION DATA USING WRAPPED GAUSSIAN PROCESSES},
 urldate = {2024-08-08},
 volume = {6},
 year = {2012}
}
@ARTICLE{Jacobs+1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@article{Emerson+2023,
	abstract = {Where do firms innovate? Mapping their locations and directions in technological space is challenging due to its high dimensionality. We propose a new method to characterize firms' inventive activities via topological data analysis (TDA) that represents high-dimensional data in a shape graph. Applying this method to 333 major firms' patents in 1976--2005 reveals hitherto undocumented industry dynamics: some firms remain undifferentiated; others develop unique portfolios. Firms with unique trajectories, which we define and measure graph-theoretically as ``flares'' in the Mapper graph, tend to perform better. This association is statistically and economically significant, and continues to hold after we control for portfolio size, firm survivorship, and industry classification.},
	author = {Emerson G. Escolar and Yasuaki Hiraoka and Mitsuru Igami and Yasin Ozcan},
	doi = {https://doi.org/10.1016/j.respol.2023.104821},
	issn = {0048-7333},
	journal = {Research Policy},
	keywords = {Innovation, Mapper, Patents, R&D, Topological data analysis},
	number = {8},
	pages = {104821},
	title = {Mapping firms' locations in technological space: A topological analysis of patent statistics},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	volume = {52},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	bdsk-url-2 = {https://doi.org/10.1016/j.respol.2023.104821}}
@inproceedings{Singh+2007
,
booktitle = {Eurographics Symposium on Point-Based Graphics
},
editor = {M. Botsch and R. Pajarola and B. Chen and M. Zwicker
},
title = {{Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition
}},
author = {Singh, Gurjeet and 
Memoli, Facundo and 
Carlsson, Gunnar
},
year = {2007
},
publisher = {The Eurographics Association
},
ISSN = {1811-7813
},
ISBN = {978-3-905673-51-7
},
DOI = {/10.2312/SPBG/SPBG07/091-100
}
}
@phdthesis{Roberts1963,
    author      = {Lawrence G. Roberts},
    school      = {Massachusetts Institute of Technology},
    title       = {Machine Perception of Three-Dimensional Solids},
    year        = {1963},
    url             = {http://hdl.handle.net/1721.1/11589},
}

@article{Lee-Mumford2003,
	abstract = {Traditional views of visual processing suggest that early visual neurons in areas V1 and V2 are static spatiotemporal filters that extract local features from a visual scene. The extracted information is then channeled through a feedforward chain of modules in successively higher visual areas for further analysis. Recent electrophysiological recordings from early visual neurons in awake behaving monkeys reveal that there are many levels of complexity in the information processing of the early visual cortex, as seen in the long-latency responses of its neurons. These new findings suggest that activity in the early visual cortex is tightly coupled and highly interactive with the rest of the visual system. They lead us to propose a new theoretical setting based on the mathematical framework of hierarchical Bayesian inference for reasoning about the visual system. In this framework, the recurrent feedforward/feedback loops in the cortex serve to integrate top-down contextual priors and bottom-up observations so as to implement concurrent probabilistic inference along the visual hierarchy. We suggest that the algorithms of particle filtering and Bayesian-belief propagation might model these interactive cortical computations. We review some recent neurophysiological evidences that support the plausibility of these ideas.},
	author = {Tai Sing Lee and David Mumford},
	doi = {10.1364/JOSAA.20.001434},
	journal = {J. Opt. Soc. Am. A},
	keywords = {Vision modeling ; Edge detection; Information processing; Machine vision; Neural networks; Physiology; Spatial resolution},
	month = {Jul},
	number = {7},
	pages = {1434--1448},
	publisher = {Optica Publishing Group},
	title = {Hierarchical Bayesian inference in the visual cortex},
	url = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	volume = {20},
	year = {2003},
	bdsk-url-1 = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	bdsk-url-2 = {https://doi.org/10.1364/JOSAA.20.001434}}

@article{Lake+2015,
	abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms---for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	author = {Brenden M. Lake and Ruslan Salakhutdinov and Joshua B. Tenenbaum},
	doi = {10.1126/science.aab3050},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aab3050},
	journal = {Science},
	number = {6266},
	pages = {1332-1338},
	title = {Human-level concept learning through probabilistic program induction},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	volume = {350},
	year = {2015},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	bdsk-url-2 = {https://doi.org/10.1126/science.aab3050}}
@inproceedings{Donahue+2017,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Kr{\"a}henb{\"u}hl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJtNZAFgg}
}
@inproceedings{Bao+2022,
title={{BE}iT: {BERT} Pre-Training of Image Transformers},
author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{Kivva+2021,
	author = {Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {18087--18101},
	publisher = {Curran Associates, Inc.},
	title = {Learning latent causal graphs via mixture oracles},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf}}
@inproceedings{Kivva+2022,
title={Identifiability of deep generative models under mixture priors without auxiliary information},
author={Bohdan Kivva and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022},
url={https://openreview.net/forum?id=UeG3kt_Ebg2}
}

@InProceedings{Lopez+2024,
  title = 	 {Toward the Identifiability of Comparative Deep Generative Models},
  author =       {Lopez, Romain and Huetter, Jan-Christian and Hajiramezanali, Ehsan and Pritchard, Jonathan K and Regev, Aviv},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {868--912},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/lopez24a/lopez24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/lopez24a.html},
  abstract = 	 {Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice.  Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network).  We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.}
}
@article{Locatello+2020,
  author  = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Raetsch and Sylvain Gelly and Bernhard Sch{{\"o}}lkopf and Olivier Bachem},
  title   = {A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {209},
  pages   = {1--62},
  url     = {http://jmlr.org/papers/v21/19-976.html}
}
@inproceedings{Ding+2021,
title={Cc{\{}GAN{\}}: Continuous Conditional Generative Adversarial Networks for Image Generation},
author={Xin Ding and Yongwei Wang and Zuheng Xu and William J Welch and Z. Jane Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PrzjugOsDeE}
}
@inproceedings{Hoogeboom+2021,
title={Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=6nbpPqUCIi7}
}
@misc{Simo2024,
  author={Simo Ryu},
  title={Minimal Implementation of a D3PM (Structured Denoising Diffusion Models in Discrete State-Spaces), in pytorch},
  year={2024},
  url             = {https://github.com/cloneofsimo/d3pm},
}

@inproceedings{Kingma+2014,
	author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {{Semi-supervised Learning with Deep Generative Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf}}
@inproceedings{Chen+2023,
title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},
author={Ting Chen and Ruixiang ZHANG and Geoffrey Hinton},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3itjR9QxFw}
}

@article{Watson+2023,
	abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods1--9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence--structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
	date = {2023/08/01},
	date-added = {2024-08-10 21:39:03 +0900},
	date-modified = {2024-08-10 21:39:03 +0900},
	doi = {10.1038/s41586-023-06415-8},
	id = {Watson2023},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7976},
	pages = {1089--1100},
	title = {De novo design of protein structure and function with RFdiffusion},
	url = {https://doi.org/10.1038/s41586-023-06415-8},
	volume = {620},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-023-06415-8}}
@article{Abramson+2024,
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	date = {2024/06/01},
	date-added = {2024-08-10 21:52:55 +0900},
	date-modified = {2024-08-10 21:52:55 +0900},
	doi = {10.1038/s41586-024-07487-w},
	id = {Abramson2024},
	isbn = {1476-4687},
	journal = {Nature},
	number = {8016},
	pages = {493--500},
	title = {Accurate structure prediction of biomolecular interactions with AlphaFold 3},
	url = {https://doi.org/10.1038/s41586-024-07487-w},
	volume = {630},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-024-07487-w}}
@article{Krishna+2024,
	abstract = {Deep-learning methods have revolutionized protein structure prediction and design but are presently limited to protein-only systems. We describe RoseTTAFold All-Atom (RFAA), which combines a residue-based representation of amino acids and DNA bases with an atomic representation of all other groups to model assemblies that contain proteins, nucleic acids, small molecules, metals, and covalent modifications, given their sequences and chemical structures. By fine-tuning on denoising tasks, we developed RFdiffusion All-Atom (RFdiffusionAA), which builds protein structures around small molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we designed and experimentally validated, through crystallography and binding measurements, proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and the light-harvesting molecule bilin. Advances in machine learning have made protein structure prediction and design much more accurate and accessible in recent years, but these tools have generally been limited to polypeptide chains. However, ligands such as small molecules, metal ions, and nucleic acids are crucial components of most proteins, both in terms of structure and biological function. Krishna et al. present a next-generation protein structure prediction and design tool, RoseTTAFold All-Atom, that can accept a wide range of ligands and covalent amino acid modifications. The authors demonstrate superior performance on protein-ligand structure prediction relative to other tools, even in the absence of an input experimental structure. They also perform de novo design of proteins to bind cofactors and small molecules and experimentally validate these designs. ---Michael A. Funk},
	author = {Rohith Krishna and Jue Wang and Woody Ahern and Pascal Sturmfels and Preetham Venkatesh and Indrek Kalvet and Gyu Rie Lee and Felix S. Morey-Burrows and Ivan Anishchenko and Ian R. Humphreys and Ryan McHugh and Dionne Vafeados and Xinting Li and George A. Sutherland and Andrew Hitchcock and C. Neil Hunter and Alex Kang and Evans Brackenbrough and Asim K. Bera and Minkyung Baek and Frank DiMaio and David Baker},
	doi = {10.1126/science.adl2528},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.adl2528},
	journal = {Science},
	number = {6693},
	pages = {eadl2528},
	title = {Generalized biomolecular modeling and design with RoseTTAFold All-Atom},
	url = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	volume = {384},
	year = {2024},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	bdsk-url-2 = {https://doi.org/10.1126/science.adl2528}}

@article{Zheng+2024,
	abstract = {Advances in deep learning have greatly improved structure prediction of molecules. However, many macroscopic observations that are important for real-world applications are not functions of a single molecular structure but rather determined from the equilibrium distribution of structures. Conventional methods for obtaining these distributions, such as molecular dynamics simulation, are computationally expensive and often intractable. Here we introduce a deep learning framework, called Distributional Graphormer (DiG), in an attempt to predict the equilibrium distribution of molecular systems. Inspired by the annealing process in thermodynamics, DiG uses deep neural networks to transform a simple distribution towards the equilibrium distribution, conditioned on a descriptor of a molecular system such as a chemical graph or a protein sequence. This framework enables the efficient generation of diverse conformations and provides estimations of state densities, orders of magnitude faster than conventional methods. We demonstrate applications of DiG on several molecular tasks, including protein conformation sampling, ligand structure sampling, catalyst--adsorbate sampling and property-guided structure generation. DiG presents a substantial advancement in methodology for statistically understanding molecular systems, opening up new research opportunities in the molecular sciences.},
	author = {Zheng, Shuxin and He, Jiyan and Liu, Chang and Shi, Yu and Lu, Ziheng and Feng, Weitao and Ju, Fusong and Wang, Jiaxi and Zhu, Jianwei and Min, Yaosen and Zhang, He and Tang, Shidi and Hao, Hongxia and Jin, Peiran and Chen, Chi and No{\'e}, Frank and Liu, Haiguang and Liu, Tie-Yan},
	date = {2024/05/01},
	date-added = {2024-08-10 22:01:13 +0900},
	date-modified = {2024-08-10 22:01:13 +0900},
	doi = {10.1038/s42256-024-00837-3},
	id = {Zheng2024},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {5},
	pages = {558--567},
	title = {Predicting equilibrium distributions for molecular systems with deep learning},
	url = {https://doi.org/10.1038/s42256-024-00837-3},
	volume = {6},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-024-00837-3}}
@unpublished{Duvenaud2014,
    author = {David Kristjanson Duvenaud},
    year   = {2014},
    title  = {The Kernel Cookbook: Advice on Covariance functions},
    url    = {https://www.cs.toronto.edu/~duvenaud/cookbook/}
}
@ARTICLE{Linsker1988,
  author={Linsker, R.},
  journal={Computer}, 
  title={{Self-Organization in a Perceptual Network}}, 
  year={1988},
  volume={21},
  number={3},
  pages={105-117},
  keywords={Intelligent networks;Biological information theory;Circuits;Biology computing;Animal structures;Neuroscience;Genetics;System testing;Neural networks;Constraint theory},
  doi={10.1109/2.36}}

@article{Torgenson1952,
	abstract = {Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.},
	author = {Torgerson, Warren S. },
	date = {1952/12/01},
	date-added = {2024-08-10 23:19:09 +0900},
	date-modified = {2024-08-10 23:19:09 +0900},
	doi = {10.1007/BF02288916},
	id = {Torgerson1952},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {401--419},
	title = {Multidimensional scaling: I. Theory and method},
	url = {https://doi.org/10.1007/BF02288916},
	volume = {17},
	year = {1952},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288916}}

@article{Kruskal1964,
	abstract = {Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.},
	author = {Kruskal, J.  B. },
	date = {1964/03/01},
	date-added = {2024-08-10 23:20:08 +0900},
	date-modified = {2024-08-10 23:20:08 +0900},
	doi = {10.1007/BF02289565},
	id = {Kruskal1964},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {1--27},
	title = {Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
	url = {https://doi.org/10.1007/BF02289565},
	volume = {29},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289565}}
@article{Poole-Rosenthal1985,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2111172},
 abstract = {A general nonlinear logit model is used to analyze political choice data. The model assumes probabilistic voting based on a spatial utility function. The parameters of the utility function and the spatial coordinates of the choices and the choosers can all be estimated on the basis of observed choices. Ordinary Guttman scaling is a degenerate case of this model. Estimation of the model is implemented in the NOMINATE program for one dimensional analysis of two alternative choices with no nonvoting. The robustness and face validity of the program outputs are evaluated on the basis of roll call voting data for the U.S. House and Senate.},
 author = {Keith T. Poole and Howard Rosenthal},
 journal = {American Journal of Political Science},
 number = {2},
 pages = {357--384},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {A Spatial Model for Legislative Roll Call Analysis},
 urldate = {2024-08-10},
 volume = {29},
 year = {1985}
}

@article{岡田謙介-加藤淳子2016,
	author = {岡田謙介 and 加藤淳子},
	doi = {10.2333/jbhmk.43.155},
	journal = {行動計量学},
	number = {2},
	pages = {155-166},
	title = {政治学における空間分析と認知空間},
	volume = {43},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.43.155}}
@book{Enelow-Hinich1984,
    author         = {James M. Enelow and Melvin J. Hinich},
    year           = {1984},
    title          = {The Spatial Theory of Voting: An Introduction},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.cambridge.org/us/universitypress/subjects/politics-international-relations/political-theory/spatial-theory-voting-introduction?format=PB&isbn=9780521275156},
    publisher      = {Cambridge University Press}
}

@article{Eckart-Young1936,
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
	author = {Eckart, Carl and Young, Gale},
	date = {1936/09/01},
	date-added = {2024-08-11 11:46:20 +0900},
	date-modified = {2024-08-11 11:46:20 +0900},
	doi = {10.1007/BF02288367},
	id = {Eckart1936},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {211--218},
	title = {The approximation of one matrix by another of lower rank},
	url = {https://doi.org/10.1007/BF02288367},
	volume = {1},
	year = {1936},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288367}}
@inbook{Easterling1987,
    author         = {D. V. Easterling},
    chapter        = {Political Scicnce: Using the Generalized Euclidian Model to Study Ideological Shifts in the U.S. Senate },
    editor         = {Robert M. Hamer and Forrest W. Young},
    pages          = {219-256},
    publisher      = {Psychology Press},
    title          = {Multidimensional Scaling: History, Theory, and Applications},
    year           = {1987},
    url             = {https://doi.org/10.4324/9780203767719},
}
@article{Coombs1950,
    author          = {C. H. Coombs},
    year            = {1950},
    title           = {{Psychological Scaling without a Unit of Measurement}},
    journal         = {Psychological Review},
    volume          = {57},
    number          = {3},
    pages           = {145-158},
    url             = {https://psycnet.apa.org/doi/10.1037/h0060984}
}

@article{足立浩平2000,
	author = {足立浩平},
	doi = {10.2333/jbhmk.27.12},
	journal = {行動計量学},
	number = {1},
	pages = {12-23},
	title = {計量多次元展開法の変量モデル},
	volume = {27},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.27.12}}
@book{MacRae1958,
    author         = {Duncan MacRae},
    year           = {1958},
    title          = {Dimensions of Congressional Voting: A Statistical Study of the House of Representatives in the Eighty-first Congress},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {University of California Press}
}
@article{Poole+2011,
 title={Scaling Roll Call Votes with wnominate in R},
 volume={42},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v042i14},
 doi={10.18637/jss.v042.i14},
 abstract={This paper presents a software package designed to estimate Poole and Rosenthal W-NOMINATE scores in R. The package uses a logistic regression model to analyze political choice data, usually (though not exclusively) from a legislative setting. In contrast to other scaling methods, W-NOMINATE explicitly assumes probabilistic voting based on a spatial utility function, where the parameters of the utility function and the spatial coordinates of the legislators and the votes can all be estimated on the basis of observed voting behavior. Building on software written by Poole in Fortran, the new &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R facilitates easier data input and manipulation, generates bootstrapped standard errors, and includes a new suite of graphics functions to display the results. We demonstrate the functionality of this package by conducting a natural experiment using roll calls -- an experiment which is greatly simplified by the data manipulation capabilities of the &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R.},
 number={14},
 journal={Journal of Statistical Software},
 author={Poole, Keith and Lewis, Jeffrey B. and Lo, James and Carroll, Royce},
 year={2011},
 pages={1–21}
}
@article{Lee2001,
	abstract = {Multidimensional scaling models of stimulus domains are widely used as a representational basis for cognitive modeling. These representations associate stimuli with points in a coordinate space that has some predetermined number of dimensions. Although the choice of dimensionality can significantly influence cognitive modeling, it is often made on the basis of unsatisfactory heuristics. To address this problem, a Bayesian approach to dimensionality determination, based on the Bayesian Information Criterion (BIC), is developed using a probabilistic formulation of multidimensional scaling. The BIC approach formalizes the trade-off between data-fit and model complexity implicit in the problem of dimensionality determination and allows for the explicit introduction of information regarding data precision. Monte Carlo simulations are presented that indicate, by using this approach, the determined dimensionality is likely to be accurate if either a significant number of stimuli are considered or a reasonable estimate of precision is available. The approach is demonstrated using an established data set involving the judged pairwise similarities between a set of geometric stimuli.},
	author = {Michael D. Lee},
	doi = {https://doi.org/10.1006/jmps.1999.1300},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	number = {1},
	pages = {149-166},
	title = {Determining the Dimensionality of Multidimensional Scaling Representations for Cognitive Modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	volume = {45},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	bdsk-url-2 = {https://doi.org/10.1006/jmps.1999.1300}}
@article{Baker-Poole2013,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/23359696},
 abstract = {In this article, we show how to apply Bayesian methods to noisy ratio scale distances for both the classical similarities problem as well as the unfolding problem. Bayesian methods produce essentially the same point estimates as the classical methods, but are superior in that they provide more accurate measures of uncertainty in the data. Identification is nontrivial for this class of problems because a configuration of points that reproduces the distances is identified only up to a choice of origin, angles of rotation, and sign flips on the dimensions. We prove that fixing the origin and rotation is sufficient to identify a configuration in the sense that the corresponding maxima/minima are inflection points with full-rank Hessians. However, an unavoidable result is multiple posterior distributions that are mirror images of one another. This poses a problem for Markov chain Monte Carlo (MCMC) methods. The approach we take is to find the optimal solution using standard optimizers. The configuration of points from the optimizers is then used to isolate a single Bayesian posterior that can then be easily analyzed with standard MCMC methods.},
 author = {Ryan Bakker and Keith T. Poole},
 journal = {Political Analysis},
 number = {1},
 pages = {125--140},
 publisher = {Oxford University Press},
 title = {Bayesian Metric Multidimensional Scaling},
 urldate = {2024-08-10},
 volume = {21},
 year = {2013}
}
@inproceedings{Lim+2024,
    author          = {Johan Lim and Sooahn Shin and Jong Hee Park},
    year            = {2024},
    title           = {$\ell^1$-Based Bayesian Ideal Point Model for Multidimensional Politics},
    booktitle       = {ISI World Statistics Congress},
    volume          = {64},
    pages           = {},
    url             = {https://www.isi-next.org/abstracts/submission/1310/view/}
}
@article{Jackman2001,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791646},
 abstract = {Vote-specific parameters are often by-products of roll call analysis, the primary goal being the measurement of legislators' ideal points. But these vote-specific parameters are more important in higher-dimensional settings: prior restrictions on vote parameters help identify the model, and researchers often have prior beliefs about the nature of the dimensions underlying the proposal space. Bayesian methods provide a straightforward and rigorous way for incorporating these prior beliefs into roll call analysis. I demonstrate this by exploiting the close connections among roll call analysis, item—response models, and "full-information" factor analysis. Vote-specific discrimination parameters are equivalent to factor loadings, and as in factor analysis, they (1) enable researchers to discern the substantive content of the recovered dimensions, (2) can be used for assessing dimensionality and model checking, and (3) are an obvious vehicle for introducing and testing researchers' prior beliefs about the dimensions. Bayesian simulation facilitates these uses of discrimination parameters, by simplifying estimation and inference for the massive number of parameters generated by roll call analysis.},
 author = {Simon Jackman},
 journal = {Political Analysis},
 number = {3},
 pages = {227--241},
 publisher = {[Oxford University Press, Society for Political Methodology]},
 title = {Multidimensional Analysis of Roll Call Data via Bayesian Simulation: Identification, Estimation, Inference, and Model Checking},
 urldate = {2024-08-11},
 volume = {9},
 year = {2001}
}
@techreport{deLeeuw2005,
    author          = {Jan {de Leeuw}},
    year            = {2005},
    title           = {Applications of Convex Analysis to Multidimensional Scaling},
    institution = {UCLA: Department of Statistics},
    url             = {https://escholarship.org/uc/item/7wg0k7xq},
}
@ARTICLE{Sammon1969,
  author={Sammon, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Nonlinear Mapping for Data Structure Analysis}, 
  year={1969},
  volume={C-18},
  number={5},
  pages={401-409},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric, pattern recognition, statistics.},
  doi={10.1109/T-C.1969.222678}}

@article{Tenenbaum+2000,
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 106 optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	author = {Joshua B. Tenenbaum and Vin de Silva and John C. Langford},
	doi = {10.1126/science.290.5500.2319},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
	journal = {Science},
	number = {5500},
	pages = {2319-2323},
	title = {{A Global Geometric Framework for Nonlinear Dimensionality Reduction}},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2319}}

@article{Balasubramanian-Schwartz2002,
	author = {Mukund Balasubramanian and Eric L. Schwartz},
	doi = {10.1126/science.295.5552.7a},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.295.5552.7a},
	journal = {Science},
	number = {5552},
	pages = {7-7},
	title = {The Isomap Algorithm and Topological Stability},
	url = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	volume = {295},
	year = {2002},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	bdsk-url-2 = {https://doi.org/10.1126/science.295.5552.7a}}

@article{Choi-Choi2007,
	abstract = {Isomap is one of widely used low-dimensional embedding methods, where geodesic distances on a weighted graph are incorporated with the classical scaling (metric multidimensional scaling). In this paper we pay our attention to two critical issues that were not considered in Isomap, such as: (1) generalization property (projection property); (2) topological stability. Then we present a robust kernel Isomap method, armed with such two properties. We present a method which relates the Isomap to Mercer kernel machines, so that the generalization property naturally emerges, through kernel principal component analysis. For topological stability, we investigate the network flow in a graph, providing a method for eliminating critical outliers. The useful behavior of the robust kernel Isomap is confirmed through numerical experiments with several data sets.},
	author = {Heeyoul Choi and Seungjin Choi},
	doi = {https://doi.org/10.1016/j.patcog.2006.04.025},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Isomap, Kernel PCA, Manifold learning, Multidimensional scaling (MDS), Nonlinear dimensionality reduction},
	number = {3},
	pages = {853-862},
	title = {Robust kernel Isomap},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	volume = {40},
	year = {2007},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2006.04.025}}
@ARTICLE{Scholkopf+1998,
  author={Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
  journal={Neural Computation}, 
  title={Nonlinear Component Analysis as a Kernel Eigenvalue Problem}, 
  year={1998},
  volume={10},
  number={5},
  pages={1299-1319},
  keywords={},
  doi={10.1162/089976698300017467}}
@inproceedings{Weinberger+2004,
author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
title = {Learning a kernel matrix for nonlinear dimensionality reduction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015345},
doi = {10.1145/1015330.1015345},
abstract = {We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {106},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{Roweis-Saul2000,
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	author = {Sam T. Roweis and Lawrence K. Saul},
	doi = {10.1126/science.290.5500.2323},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2323},
	journal = {Science},
	number = {5500},
	pages = {2323-2326},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2323}}

@inproceedings{Mikhali-Partha2001,
	author = {Belkin, Mikhail and Niyogi, Partha},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf}}
@book{Chung1997,
    author         = {Fan R. K. Chung},
    year           = {1997},
    title          = {Spectral Graph Theory},
    series         = {CBMS Regional Conference Series in Mathematics},
    volume         = {92},
    edition        = {},
    url            = {https://doi.org/10.1090/cbms/092},
    publisher      = {American Mathematical Society}
}

@inproceedings{Hinton-Roweis2002,
	author = {Hinton, Geoffrey E and Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Becker and S. Thrun and K. Obermayer},
	publisher = {MIT Press},
	title = {Stochastic Neighbor Embedding},
	url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
	volume = {15},
	year = {2002},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf}}
@article{Maaten-Hinton2008,
  author  = {Laurens {van der Maaten} and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@article{Coifman+2005,
	author = {R. R. Coifman and S. Lafon and A. B. Lee and M. Maggioni and B. Nadler and F. Warner and S. W. Zucker},
	doi = {10.1073/pnas.0500334102},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0500334102},
	journal = {Proceedings of the National Academy of Sciences},
	number = {21},
	pages = {7426-7431},
	title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	volume = {102},
	year = {2005},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0500334102}}
