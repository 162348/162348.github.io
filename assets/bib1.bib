@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
@inproceedings{Zhao+2024,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
	  booksubtitle    = {International Conference on Machine Learning},
      url={https://arxiv.org/abs/2404.17546}, 
}
@article{Teh+2003,
author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
title = {Energy-based models for sparse overcomplete representations},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1235–1260},
numpages = {26},
url = {https://www.jmlr.org/papers/v4/teh03a.html}
}


@article{Teh+2006,
	author = {Yee Whye Teh, Michael I Jordan, Matthew J Beal and David M Blei},
	doi = {10.1198/016214506000000302},
	eprint = {https://doi.org/10.1198/016214506000000302},
	journal = {Journal of the American Statistical Association},
	number = {476},
	pages = {1566--1581},
	publisher = {Taylor \& Francis},
	title = {Hierarchical Dirichlet Processes},
	url = {https://doi.org/10.1198/016214506000000302},
	volume = {101},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/016214506000000302}}

@inproceedings{Wallach+2009,
author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
title = {Evaluation methods for topic models},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553515},
doi = {10.1145/1553374.1553515},
abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1105–1112},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@InProceedings{Xie+2016,
  title = 	 {A Theory of Generative ConvNet},
  author = 	 {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2635--2644},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/xiec16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/xiec16.html},
  abstract = 	 {We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.}
}
@inproceedings{He+2019,
      title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders}, 
      author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
      year={2019},
      booksubtitle    = {International Conference on Learning Representations},
      url={https://arxiv.org/abs/1901.05534}, 
}
@inproceedings{Hinton-Teh2001,
author = {Hinton, Geoffrey E. and Teh, Yee-Whye},
title = {Discovering multiple constraints that are frequently approximately satisfied},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {227–234},
numpages = {8},
location = {Seattle, Washington},
series = {UAI'01},
url             = {https://dl.acm.org/doi/abs/10.5555/2074022.2074051},
}
@misc{Finn+2016,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03852}, 
}
@inproceedings{Swersku+2011,
author = {Swersky, Kevin and Ranzato, Marc'Aurelio and Buchman, David and Marlin, Benjamin M. and Freitas, Nandode},
title = {On autoencoders and score matching for energy based models},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider estimation methods for the class of continuous-data energy based models (EBMs). Our main result shows that estimating the parameters of an EBM using score matching when the conditional distribution over the visible units is Gaussian corresponds to training a particular form of regularized autoencoder. We show how different Gaussian EBMs lead to different autoencoder architectures, providing deep links between these two families of models. We compare the score matching estimator for the mPoT model, a particular Gaussian EBM, to several other training methods on a variety of tasks including image denoising and unsupervised feature extraction. We show that the regularization function induced by score matching leads to superior classification performance relative to a standard autoencoder. We also show that score matching yields classification results that are indistinguishable from better-known stochastic approximation maximum likelihood estimators.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {1201–1208},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104633},
}

@inproceedings{Koster-Hyvarinen2007,
	abstract = {Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Hyv{\"a}rinen, Aapo},
	booktitle = {Artificial Neural Networks -- ICANN 2007},
	editor = {de S{\'a}, Joaquim Marques and Alexandre, Lu{\'\i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
	isbn = {978-3-540-74695-9},
	pages = {798--807},
	publisher = {Springer Berlin Heidelberg},
	title = {A Two-Layer ICA-Like Model Estimated by Score Matching},
	year = {2007}}

@inproceedings{Koster+2009,
	abstract = {Markov Random Field (MRF) models with potentials learned from the data have recently received attention for learning the low-level structure of natural images. A MRF provides a principled model for whole images, unlike ICA, which can in practice be estimated for small patches only. However, learning the filters in an MRF paradigm has been problematic in the past since it required computationally expensive Monte Carlo methods. Here, we show how MRF potentials can be estimated using Score Matching (SM). With this estimation method we can learn filters of size 12 {\texttimes}12 pixels, considerably larger than traditional ''hand-crafted'' MRF potentials. We analyze the tuning properties of the filters in comparison to ICA filters, and show that the optimal MRF potentials are similar to the filters from an overcomplete ICA model.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Lindgren, Jussi T. and Hyv{\"a}rinen, Aapo},
	booktitle = {Independent Component Analysis and Signal Separation},
	editor = {Adali, T{\"u}lay and Jutten, Christian and Romano, Jo{\~a}o Marcos Travassos and Barros, Allan Kardec},
	isbn = {978-3-642-00599-2},
	pages = {515--522},
	publisher = {Springer Berlin Heidelberg},
	title = {Estimating Markov Random Field Potentials for Natural Images},
	year = {2009}}

@InProceedings{Gutmann-Hyvarinen2010,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract = 	 {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}

@InProceedings{Perpinan-Hinton2005,
  title = 	 {On Contrastive Divergence Learning},
  author =       {Carreira-Perpi{\~n}\'an, Miguel \'A. and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {33--40},
  year = 	 {2005},
  editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
  volume = 	 {R5},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--08 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf},
  url = 	 {https://proceedings.mlr.press/r5/carreira-perpinan05a.html},
  note =         {Reissued by PMLR on 30 March 2021.}
}

@inproceedings{Kingma-LeCun2010,
	author = {Durk P Kingma and Yann LeCun},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
	publisher = {Curran Associates, Inc.},
	title = {Regularized estimation of image statistics by Score Matching},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},
	volume = {23},
	year = {2010},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf}}
@ARTICLE{Drucker-LeCun1992,
  author={Drucker, H. and Le Cun, Y.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Improving generalization performance using double backpropagation}, 
  year={1992},
  volume={3},
  number={6},
  pages={991-997},
  keywords={Testing;Backpropagation algorithms;Jacobian matrices;Neurons;Signal to noise ratio;Neural networks},
  doi={10.1109/72.165600}}
@inproceedings{Song+2019,
      title={{Sliced Score Matching: A Scalable Approach to Density and Score Estimation}},
      author={Yang Song and Sahaj Garg and Jiaxin Shi and Stefano Ermon},
      year={2019},
      booksubtitle    = {Uncertainty in Artificial Intelligence},
      url={https://arxiv.org/abs/1905.07088}, 
}
@ARTICLE{Hyvarinen2007,
  author={Hyvarinen, Aapo},
  journal={IEEE Transactions on Neural Networks}, 
  title={Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables}, 
  year={2007},
  volume={18},
  number={5},
  pages={1529-1531},
  keywords={Samarium;Statistical analysis;Probability density function;Parameter estimation;Monte Carlo methods;Computer science;Information technology;Normalization constant;partition function;statistical estimation},
  doi={10.1109/TNN.2007.895819}}
@inproceedings{Lyu2009,
author = {Lyu, Siwei},
title = {Interpretation and generalization of score matching},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {359–366},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@inproceedings{Gutmann-Hirayama2011,
author = {Gutmann, Michael U. and Hirayama, Jun-ichiro},
title = {Bregman divergence as general framework to estimate unnormalized statistical models},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in un-supervised learning.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {283–290},
numpages = {8},
location = {Barcelona, Spain},
series = {UAI'11}
}

@InProceedings{Chwialkowski+2016,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}

@InProceedings{Liu+2016,
  title = 	 {A Kernelized Stein Discrepancy for Goodness-of-fit Tests},
  author = 	 {Liu, Qiang and Lee, Jason and Jordan, Michael},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {276--284},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/liub16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/liub16.html},
  abstract = 	 {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.}
}
@misc{McAllester2023,
      title={On the Mathematics of Diffusion Models}, 
      author={David McAllester},
      year={2023},
      eprint={2301.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.11108}, 
}
@article{Yang+2023,
author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626235},
doi = {10.1145/3626235},
abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github:},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {39},
keywords = {stochastic differential equations, score-based generative models, diffusion models, Generative models}
}
@unpublished{Kreis+2022,
    author = {Karsten Kreis and Ruiqi Gao and Arash Vahdat},
    year   = {2022},
    title  = {Denoising Diffusion-based Generative Modeling: Foundations and Applications},
    url    = {https://cvpr2022-tutorial-diffusion-models.github.io/},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@unpublished{Song+2023Tutorial,
    author = {Jiaming Song and Chenlin Meng and Arash Vahdat},
    year   = {2023},
    title  = {Denoising Diffusion Models: A Generative Learning Big Bang},
    url    = {https://cvpr.thecvf.com/virtual/2023/tutorial/18546},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@INPROCEEDINGS{Choi+2022,
  author={Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Perception Prioritized Training of Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={11462-11471},
  keywords={Training;Visualization;Computational modeling;Noise reduction;Data models;Image restoration;Pattern recognition;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01118}}
@inproceedings{Kingma+2021,
author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
title = {Variational diffusion models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1660},
numpages = {12},
series = {NIPS '21},
url             = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
}
@inproceedings{Salimans-Ho2021,
title={Should {EBM}s model the energy or the score?},
author={Tim Salimans and Jonathan Ho},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
url={https://openreview.net/forum?id=9AS-TF2jRNb}
}

@article{Anderson1982,
	abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.},
	author = {Brian D.O. Anderson},
	doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	number = {3},
	pages = {313-326},
	title = {Reverse-time diffusion equation models},
	url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	volume = {12},
	year = {1982},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	bdsk-url-2 = {https://doi.org/10.1016/0304-4149(82)90051-5}}

@article{Haussmann-Pardoux1986,
	author = {U. G. Haussmann and E. Pardoux},
	doi = {10.1214/aop/1176992362},
	journal = {The Annals of Probability},
	keywords = {diffusion process, Kolmogorov equation, Markov process, Martingale problem, Time reversal},
	number = {4},
	pages = {1188 -- 1205},
	publisher = {Institute of Mathematical Statistics},
	title = {{Time Reversal of Diffusions}},
	url = {https://doi.org/10.1214/aop/1176992362},
	volume = {14},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176992362}}
@inproceedings{Karras+2022,
title={Elucidating the Design Space of Diffusion-Based Generative Models},
author={Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=k7FuTOWMOc7}
}
@ARTICLE {Croitoru+2023,
author = {F. Croitoru and V. Hondru and R. Ionescu and M. Shah},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Diffusion Models in Vision: A Survey},
year = {2023},
volume = {45},
number = {09},
issn = {1939-3539},
pages = {10850-10869},
abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
keywords = {computational modeling;mathematical models;noise reduction;data models;computer vision;training;task analysis},
doi = {10.1109/TPAMI.2023.3261988},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2023.3261988},
}

@ARTICLE{Cao+2024,
  author={Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Generative Diffusion Models}, 
  year={2024},
  volume={36},
  number={7},
  pages={2814-2830},
  keywords={Mathematical models;Kernel;Computational modeling;Training;Surveys;Noise reduction;Markov processes;Diffusion model;deep generative model;diffusion algorithm;diffusion applications},
  doi={10.1109/TKDE.2024.3361474}}
@inproceedings{JiamingSong+2021,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}
@inproceedings{Gao+2021,
title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P Kingma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v_1Soh8QUNc}
}
@inproceedings{Xiao+2021,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JprM0p-q0Co}
}
@inproceedings{Salimans-Ho2022,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}
@inproceedings{Vahdat+2021,
	author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {11287--11302},
	publisher = {Curran Associates, Inc.},
	title = {{Score-based Generative Modeling in Latent Space}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf}}

@article{Pandey+2022,
title={Diffuse{VAE}: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents},
author={Kushagra Pandey and Avideep Mukherjee and Piyush Rai and Abhishek Kumar},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ygoNPRiLxw},
note={}
}

@InProceedings{Nichol-Dhariwal2021,
  title = 	 {{Improved Denoising Diffusion Probabilistic Models}},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}
@inproceedings{Ho-Salimans2021,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}
@article{Ho+2022,
  author  = {Jonathan Ho and Chitwan Saharia and William Chan and David J. Fleet and Mohammad Norouzi and Tim Salimans},
  title   = {Cascaded Diffusion Models for High Fidelity Image Generation},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {47},
  pages   = {1--33},
  url     = {http://jmlr.org/papers/v23/21-0635.html}
}
@misc{Saharia+2022SIGGRAPH,
title={{Palette: Image-to-Image Diffusion Models}},
author={Chitwan Saharia and William Chan and Huiwen Chang and Chris A. Lee and Jonathan Ho and Tim Salimans and David J. Fleet and Mohammad Norouzi},
year={2022},
url={https://openreview.net/forum?id=FPGs276lUeq}
}
@ARTICLE{Saharia+2023,
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Image Super-Resolution via Iterative Refinement}, 
  year={2023},
  volume={45},
  number={4},
  pages={4713-4726},
  keywords={Noise reduction;Superresolution;Task analysis;Iterative methods;Data models;Faces;Diffusion processes;Image super-resolution;diffusion models;deep generative models},
  doi={10.1109/TPAMI.2022.3204461}}

@inproceedings{Austin+2021,
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17981--17993},
	publisher = {Curran Associates, Inc.},
	title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf}}
@InProceedings{Chang+2022,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@article{Heng+2024,
	author = {Jeremy Heng and Valentin De Bortoli and Arnaud Doucet},
	doi = {10.1214/23-STS908},
	journal = {Statistical Science},
	keywords = {Optimal transport, Schr{\"o}dinger bridge, score matching, Stochastic differential equation, Time reversal},
	number = {1},
	pages = {90 -- 99},
	publisher = {Institute of Mathematical Statistics},
	title = {{Diffusion Schr{\"o}dinger Bridges for Bayesian Computation}},
	url = {https://doi.org/10.1214/23-STS908},
	volume = {39},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1214/23-STS908}}
@inproceedings{Chung+2023,
title={Diffusion Posterior Sampling for General Noisy Inverse Problems},
author={Hyungjin Chung and Jeongsol Kim and Michael Thompson Mccann and Marc Louis Klasky and Jong Chul Ye},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=OnD9zGAGT0k}
}

@InProceedings{Song+2023,
  title = 	 {Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation},
  author =       {Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32483--32498},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23k/song23k.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23k.html},
  abstract = 	 {We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion (LGD), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a Monte Carlo method that uses multiple samples from a suitable distribution to reduce bias. Our method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and controllable motion synthesis. Notably, we show how our method can be applied to control a pretrained motion diffusion model to follow certain paths and avoid obstacles that are proven challenging to prior methods.}
}

@InProceedings{Shi+2022,
  title = 	 {Conditional simulation using diffusion {S}chr{ö}dinger bridges},
  author =       {Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1792--1802},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/shi22a/shi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/shi22a.html},
  abstract = 	 {Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr{ö}dinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schrödinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.}
}

@inproceedings{DeBortoli+2021,
	author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17695--17709},
	publisher = {Curran Associates, Inc.},
	title = {Diffusion Schr\"{o}dinger Bridge with Applications to Score-Based Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf}}

@InProceedings{Kurras2015,
  title = 	 {{Symmetric Iterative Proportional Fitting}},
  author = 	 {Kurras, Sven},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {526--534},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/kurras15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/kurras15.html},
  abstract = 	 {Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix W*. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W=W’ and r=c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of W* in terms of expansion properties of the undirected weighted graph represented by W. Finally, we show how our results contribute to recent work in machine learning.}
}
@article{Sinkhorn1967,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2314570},
 author = {Richard Sinkhorn},
 journal = {The American Mathematical Monthly},
 number = {4},
 pages = {402--405},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Diagonal Equivalence to Matrices with Prescribed Row and Column Sums},
 urldate = {2024-08-04},
 volume = {74},
 year = {1967}
}

@article{Sinkhorn-Knopp1967,
	author = {Richard Sinkhorn and Paul Knopp},
	journal = {Pacific Journal of Mathematics},
	number = {2},
	pages = {343 -- 348},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{Concerning Nonnegative Matrices and Doubly Stochastic Matrices}},
	volume = {21},
	year = {1967},
  url             = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-21/issue-2/Concerning-nonnegative-matrices-and-doubly-stochastic-matrices/pjm/1102992505.full},
}

@article{Fortet1940,
    author          = {Robert Fortet},
    year            = {1940},
    title           = {{Résolution d'un système d'équations de M. Schrödinger}},
    journal         = {Journal de Mathématiques Pures et Appliquées, Series 9},
    volume          = {19},
    number          = {1-4},
    pages           = {83-105},
    url             = {http://www.numdam.org/item/JMPA_1940_9_19_1-4_83_0/}
}
@article{Deming-Stephan1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235722},
 author = {W. Edwards Deming and Frederick F. Stephan},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {427--444},
 publisher = {Institute of Mathematical Statistics},
 title = {On a Least Squares Adjustment of a Sampled Frequency Table When the Expected Marginal Totals are Known},
 urldate = {2024-08-04},
 volume = {11},
 year = {1940}
}
@article{Kullback1968,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239692},
 author = {S. Kullback},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {1236--1243},
 publisher = {Institute of Mathematical Statistics},
 title = {Probability Densities with Given Marginals},
 urldate = {2024-08-04},
 volume = {39},
 year = {1968}
}
@article{Ireland-Kullback1968,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334462},
 abstract = {In its simplest formulation the problem considered is to estimate the cell probabilities pij of an r × c contingency table for which the marginal probabilities $p_{i\ldot}$ and $p_{\ldot j}$ are known and fixed, so as to minimize ΣΣpijln (pij/πij), where πij are the corresponding entries in a given contingency table. An iterative procedure is given for determining the estimates and it is shown that the estimates are BAN, and that the iterative procedure is convergent. A summary of results for a four-way contingency table is given. An illustrative example is given.},
 author = {C. T. Ireland and S. Kullback},
 journal = {Biometrika},
 number = {1},
 pages = {179--188},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Contingency Tables with Given Marginals},
 urldate = {2024-08-04},
 volume = {55},
 year = {1968}
}
@inproceedings{Vargas-Grathwohl-Doucet2023,
title={{Denoising Diffusion Samplers}},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@inproceedings{Follmer1985,
	abstract = {We introduce an entropy technique which allows to treat some infinite-dimensional extensions of the classical duality equations for the time reversal of diffusion processes.},
	address = {Berlin, Heidelberg},
	author = {F{\"o}llmer, H.},
	booktitle = {Stochastic Differential Systems Filtering and Control},
	editor = {Metivier, M. and Pardoux, E.},
	isbn = {978-3-540-39253-8},
	pages = {156--163},
	publisher = {Springer Berlin Heidelberg},
	title = {An entropy approach to the time reversal of diffusion processes},
	year = {1985}}

@InProceedings{Barr+2020,
  title = 	 {Quantum Ground States from Reinforcement Learning},
  author =       {Barr, Ariel and Gispen, Willem and Lamacraft, Austen},
  booktitle = 	 {Proceedings of The First Mathematical and Scientific Machine Learning Conference},
  pages = 	 {635--653},
  year = 	 {2020},
  editor = 	 {Lu, Jianfeng and Ward, Rachel},
  volume = 	 {107},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v107/barr20a/barr20a.pdf},
  url = 	 {https://proceedings.mlr.press/v107/barr20a.html},
  abstract = 	 {  Finding the ground state of a quantum mechanical system can be formulated as an optimal control problem. In this formulation, the drift of the optimally controlled process is chosen to match the distribution of paths in the Feynman–Kac (FK) representation of the solution of the imaginary time Schrödinger equation. This provides a variational principle that can be used for reinforcement learning of a neural representation of the drift. Our approach is a drop-in replacement for path integral Monte Carlo, learning an optimal importance sampler for the FK trajectories. We demonstrate the applicability of our approach to several problems of one-, two-, and many-particle physics.}
}
@inproceedings{Zhang+2021,
title={Sampling via Controlled Stochastic Dynamical Systems},
author={Benjamin Zhang and Tuhin Sahai and Youssef Marzouk},
booktitle={I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop},
year={2021},
url={https://openreview.net/forum?id=dHruzYDH719}
}
@article{DeBortoli2022,
title={Convergence of denoising diffusion models under the manifold hypothesis},
author={Valentin De Bortoli},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=MhK5aXo3gB},
note={Expert Certification}
}
@misc{Montanari-Wu2024,
      title={Posterior Sampling from the Spiked Models via Diffusion Processes}, 
      author={Andrea Montanari and Yuchen Wu},
      year={2023},
      eprint={2304.11449},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2304.11449}, 
}


@article{Crooks1998,
	abstract = {An equality has recently been shown relating the free energy difference between two equilibrium ensembles of a system and an ensemble average of the work required to switch between these two configurations. In the present paper it is shown that this result can be derived under the assumption that the system's dynamics is Markovian and microscopically reversible.},
	author = {Crooks, Gavin E. },
	date = {1998/03/01},
	date-added = {2024-08-05 13:48:11 +0900},
	date-modified = {2024-08-05 13:48:11 +0900},
	doi = {10.1023/A:1023208217925},
	id = {Crooks1998},
	isbn = {1572-9613},
	journal = {Journal of Statistical Physics},
	number = {5},
	pages = {1481--1487},
	title = {{Nonequilibrium Measurements of Free Energy Differences for Microscopically Reversible Markovian Systems}},
	url = {https://doi.org/10.1023/A:1023208217925},
	volume = {90},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1023208217925}}
@article{Jarzynski1997Equality,
  title = {{Nonequilibrium Equality for Free Energy Differences}},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. Lett.},
  volume = {78},
  issue = {14},
  pages = {2690--2693},
  numpages = {0},
  year = {1997},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.78.2690},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.78.2690}
}
@article{Jarzynski1997MasterEquation,
  title = {Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. E},
  volume = {56},
  issue = {5},
  pages = {5018--5035},
  numpages = {0},
  year = {1997},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.56.5018},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.56.5018}
}
@inproceedings{Doucet+2022,
title={Score-Based Diffusion meets Annealed Importance Sampling},
author={Arnaud Doucet and Will Sussman Grathwohl and Alexander G. D. G. Matthews and Heiko Strathmann},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=9cU2iW3bz0}
}

@inproceedings{Wu+2020,
	author = {Wu, Hao and K\"{o}hler, Jonas and Noe, Frank},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {5933--5944},
	publisher = {Curran Associates, Inc.},
	title = {{Stochastic Normalizing Flows}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf}}

@InProceedings{Thin+2021,
  title = 	 {Monte Carlo Variational Auto-Encoders},
  author =       {Thin, Achille and Kotelevskii, Nikita and Doucet, Arnaud and Durmus, Alain and Moulines, Eric and Panov, Maxim},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10247--10257},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/thin21a/thin21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/thin21a.html},
  abstract = 	 {Variational auto-encoders (VAE) are popular deep latent variable models which are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter ELBO and hence better variational approximations, it has been proposed to use importance sampling to get a lower variance estimate of the evidence. However, importance sampling is known to perform poorly in high dimensions. While it has been suggested many times in the literature to use more sophisticated algorithms such as Annealed Importance Sampling (AIS) and its Sequential Importance Sampling (SIS) extensions, the potential benefits brought by these advanced techniques have never been realized for VAE: the AIS estimate cannot be easily differentiated, while SIS requires the specification of carefully chosen backward Markov kernels. In this paper, we address both issues and demonstrate the performance of the resulting Monte Carlo VAEs on a variety of applications.}
}
@article{Tang+2024,
    author          = {Weipin Tang and Yuhang Wu and Xunyu Zhou},
    year            = {2024},
    title           = {{Discrete-Time Simulated Annealing: A Convergence Analysis via the Eyring-Kramers Law}},
    journal         = {Numerical Algebra, Control and Optimization},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://doi.org/10.3934/naco.2024015}
}

@article{Fournier-Tardif2021,
	abstract = {Using a localization procedure and the result of Holley-Kusuoka-Stroock [8] in the torus, we widely weaken the usual growth assumptions concerning the success of the continuous-time simulated annealing in Rd. Our only assumption is the existence of an invariant probability measure for a sufficiently low temperature. We also prove, in an appendix, a non-explosion criterion for a class of time-inhomogeneous diffusions.},
	author = {Nicolas Fournier and Camille Tardif},
	doi = {https://doi.org/10.1016/j.jfa.2021.109086},
	issn = {0022-1236},
	journal = {Journal of Functional Analysis},
	keywords = {Simulated annealing, Time-inhomogeneous diffusion processes, Large time behavior, Non-explosion},
	number = {5},
	pages = {109086},
	title = {On the simulated annealing in Rd},
	url = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	volume = {281},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	bdsk-url-2 = {https://doi.org/10.1016/j.jfa.2021.109086}}

@inproceedings{Chen+2018,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Ordinary Differential Equations},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf}}
@inproceedings{Grathwohl+2019,
title={{Scalable Reversible Generative Models with Free-form Continuous Dynamics}},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJxgknCcK7},
}
@article{Gortler+2019,
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  title = {A Visual Exploration of Gaussian Processes},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  doi = {10.23915/distill.00017}
}
@article{Tipping2001,
    author          = {Michael E. Tipping},
    year            = {2001},
    title           = {Sparse Bayesian Learning and the Relevance Vector Machine},
    journal         = {Journal of Machine Learning Research},
    volume          = {1},
    number          = {},
    pages           = {211-244},
    url             = {https://www.jmlr.org/papers/v1/tipping01a.html}
}
@INPROCEEDINGS{Loeliger+2016,
  author={Loeliger, Hans-Andrea and Bruderer, Lukas and Malmberg, Hampus and Wadehn, Federico and Zalmai, Nour},
  booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
  title={On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  keywords={Computational modeling;Covariance matrices;Message passing;Smoothing methods;Maximum likelihood estimation;Kalman filters},
  doi={10.1109/ITA.2016.7888168}}
@phdthesis{Gibbs1997,
    author      = {M. N. Gibbs},
    school      = {Cambridge University},
    title       = {Bayesian Gaussian Process Regression and Classification},
    year        = {1997},
    url             = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b5a0c62c8d7cf51137bfb079947b8393c00ed169},
}

@InProceedings{Heinonen+2016,
  title = 	 {Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo},
  author = 	 {Heinonen, Markus and Mannerström, Henrik and Rousu, Juho and Kaski, Samuel and Lähdesmäki, Harri},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {732--740},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/heinonen16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/heinonen16.html},
  abstract = 	 {We present a novel approach for non-stationary Gaussian process regression (GPR), where the three key parameters – noise variance, signal variance and lengthscale – can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. For inferring the full posterior distribution we use Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with the gradients. The MAP solution can also be learned with gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the non-stationary GPR is shown to give major improvement when modeling realistic input-dependent dynamics.}
}
@mastersthesis{Krige1951,
    author  = {D. G. Krige},
    school  = {University of the Witwatersrand, Faculty of Engineering},
    title   = {A Statistical Approach to Some Mine Valuation and Allied Problems on the Witwatersrand},
    year    = {1951},
    url             = {http://hdl.handle.net/10539/17975},
}

@inproceedings{Remes+2017,
	author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Non-Stationary Spectral Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf}}

@article{Kriege+2020,
	abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.},
	author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
	date = {2020/01/14},
	date-added = {2024-08-08 14:27:51 +0900},
	date-modified = {2024-08-08 14:27:51 +0900},
	doi = {10.1007/s41109-019-0195-3},
	id = {Kriege2020},
	isbn = {2364-8228},
	journal = {Applied Network Science},
	number = {1},
	pages = {6},
	title = {A survey on graph kernels},
	url = {https://doi.org/10.1007/s41109-019-0195-3},
	volume = {5},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s41109-019-0195-3}}
@article{Lodhi+2002,
    author          = {Huma Lodhi and Craig Saunders and John Shawe-Taylor and Nello Cristianini and Chris Watkins},
    year            = {2002},
    title           = {Text Classification using String Kernels},
    journal         = {Journal of Machine Learning Research},
    volume          = {2},
    number          = {},
    pages           = {419-444},
    url             = {https://www.jmlr.org/papers/v2/lodhi02a.html}
}

@inproceedings{Borgwardt+2006,
	author = {Borgwardt, Karsten and Schraudolph, Nicol and Vishwanathan, S.v.n.},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
	publisher = {MIT Press},
	title = {Fast Computation of Graph Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf},
	volume = {19},
	year = {2006},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf}}
@article{Shervashidze+2011,
  author  = {Nino Shervashidze and Pascal Schweitzer and Erik Jan van Leeuwen and Kurt Mehlhorn and Karsten M. Borgwardt},
  title   = {Weisfeiler-Lehman Graph Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {77},
  pages   = {2539-2561},
  url     = {http://jmlr.org/papers/v12/shervashidze11a.html}
}

@InProceedings{Wilson-Adams2013,
  title = 	 {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  author = 	 {Wilson, Andrew and Adams, Ryan},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1067--1075},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wilson13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wilson13.html},
  abstract = 	 {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}
@inproceedings{Sutherland-Schneider2015,
author = {Sutherland, Danica J. and Schneider, Jeff},
title = {On the error of random fourier features},
year = {2015},
isbn = {9780996643108},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Kernel methods give powerful, flexible, and theoretically grounded approaches to solving many problems in machine learning. The standard approach, however, requires pairwise evaluations of a kernel function, which can lead to scalability issues for very large datasets. Rahimi and Recht (2007) suggested a popular approach to handling this problem, known as random Fourier features. The quality of this approximation, however, is not well understood. We improve the uniform error bound of that paper, as well as giving novel understandings of the embedding's variance, approximation error, and use in some machine learning methods. We also point out that surprisingly, of the two main variants of those features, the more widely used is strictly higher-variance for the Gaussian kernel and has worse bounds.},
booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
pages = {862–871},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {UAI'15}
}


@inproceedings{Rahimi-Recht2007,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Random Features for Large-Scale Kernel Machines},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}}

@inproceedings{Rahimi-Recht2008,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf}}

@inproceedings{Yu+2016,
	author = {Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Orthogonal Random Features},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf}}
@article{Kimeldorf-Wahba1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239347},
 author = {George S. Kimeldorf and Grace Wahba},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {495--502},
 publisher = {Institute of Mathematical Statistics},
 title = {A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines},
 urldate = {2024-08-08},
 volume = {41},
 year = {1970}
}

@inproceedings{Scholkopf+2001,
	abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
	address = {Berlin, Heidelberg},
	author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
	booktitle = {Computational Learning Theory},
	editor = {Helmbold, David and Williamson, Bob},
	isbn = {978-3-540-44581-4},
	pages = {416--426},
	publisher = {Springer Berlin Heidelberg},
	title = {A Generalized Representer Theorem},
	year = {2001}}
@article{Wilkinson+2023,
  author  = {William J. Wilkinson and Simo Särkkä and Arno Solin},
  title   = {Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {83},
  pages   = {1--50},
  url     = {http://jmlr.org/papers/v24/21-1298.html}
}
@inproceedings{Wenzel+2019,
    author          = {Florian Wenzel and Théo Galy-Fajou and Christan Donner and Marius Kolft and Manfred Opper},
    year            = {2019},
    title           = {Efficient Gaussian Process Classification Using Pólya-Gamma Data Augmentation},
    booktitle       = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume          = {33},
    pages           = {},
    url             = {},
    doi             = {10.1609/aaai.v33i01.33015417},
}

@InProceedings{Galy-Fajou2020,
  title = 	 {Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models},
  author =       {Galy-Fajou, Theo and Wenzel, Florian and Opper, Manfred},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3025--3035},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/galy-fajou20a/galy-fajou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/galy-fajou20a.html},
  abstract = 	 {We propose automated augmented conjugate inference, a new inference method for non-conjugate Gaussian processes (GP) models.Our method automatically constructs an auxiliary variable augmentation that renders the GP model conditionally conjugate. Building on the conjugate structure of the augmented model, we develop two inference methods. First, a fast and scalable stochastic variational inference method that uses efficient block coordinate ascent updates, which are computed in closed form. Second, an asymptotically correct Gibbs sampler that is useful for small datasets.Our experiments show that our method is up two orders of magnitude faster and more robust than existing state-of-the-art black-box methods.}
}
@ARTICLE{Liu+2020,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  keywords={Kernel;Scalability;Sparse representation;Complexity theory;Computational modeling;Ground penetrating radar;Predictive models;Big data;Gaussian process regression (GPR);local approximations;scalability;sparse approximations},
  doi={10.1109/TNNLS.2019.2957109}}

@InProceedings{Wilson+2020,
  title = 	 {Efficiently sampling functions from {G}aussian process posteriors},
  author =       {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10292--10302},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wilson20a/wilson20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/wilson20a.html},
  abstract = 	 {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes’ statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.}
}
@INPROCEEDINGS{Hartikainen-Sarkka2010,
  author={Hartikainen, Jouni and Särkkä, Simo},
  booktitle={2010 IEEE International Workshop on Machine Learning for Signal Processing}, 
  title={Kalman filtering and smoothing solutions to temporal Gaussian process regression models}, 
  year={2010},
  volume={},
  number={},
  pages={379-384},
  keywords={Kalman filters;Approximation methods;Mathematical model;Markov processes;Computational modeling;Gaussian processes;Equations},
  doi={10.1109/MLSP.2010.5589113}}

@inproceedings{Snelson-Ghahramani2005,
	author = {Snelson, Edward and Ghahramani, Zoubin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Sparse Gaussian Processes using Pseudo-inputs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf}}

@inproceedings{Wilson+2014,
	author = {Wilson, Andrew G and Gilboa, Elad and Nehorai, Arye and Cunningham, John P},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Fast Kernel Learning for Multidimensional Pattern Extrapolation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf}}

@inproceedings{Salakhutdinov-Hinton2007,
	author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf}}

@InProceedings{Ober+2021,
  title = 	 {The promises and pitfalls of deep kernel learning},
  author =       {Ober, Sebastian W. and Rasmussen, Carl E. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1206--1216},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/ober21a/ober21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/ober21a.html},
  abstract = 	 {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.}
}
@inproceedings{Novak+2019,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}
@misc{Yang2020,
      title={Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation}, 
      author={Greg Yang},
      year={2020},
      eprint={1902.04760},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1902.04760}, 
}

@InProceedings{Damianou-Lawrence2013,
  title = 	 {Deep {G}aussian Processes},
  author = 	 {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {207--215},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/damianou13a.pdf},
  url = 	 {https://proceedings.mlr.press/v31/damianou13a.html},
  abstract = 	 {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@article{Carvalho+2010,
    author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
    title = "{The horseshoe estimator for sparse signals}",
    journal = {Biometrika},
    volume = {97},
    number = {2},
    pages = {465-480},
    year = {2010},
    month = {04},
    abstract = "{This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator’s advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator’s tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asq017},
    url = {https://doi.org/10.1093/biomet/asq017},
    eprint = {https://academic.oup.com/biomet/article-pdf/97/2/465/584621/asq017.pdf},
}


@inproceedings{Jacot+2018,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}}

@InProceedings{Woodworth+2020,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
  abstract = 	 { A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e.&nbsp;when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by \citet{chizat2018note}, we show how the \textbf{\textit{scale of the initialization}} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-$D$ linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the \emph{width}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
}

@InProceedings{Yang-Hu2021,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}

@inproceedings{Chizat+2019,
	author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On Lazy Training in Differentiable Programming},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}}
@ARTICLE{Sarkka+2013,
  author={Särkkä, Simo and Solin, Arno and Hartikainen, Jouni},
  journal={IEEE Signal Processing Magazine}, 
  title={Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering}, 
  year={2013},
  volume={30},
  number={4},
  pages={51-61},
  keywords={Machine learning;Learning systems;Gaussian processes;Bayes methods;Parametric statistics;Linear regression analysis;Kalman filters;Smoothing methods;Spatiotemporal phenomena;Kernel},
  doi={10.1109/MSP.2013.2246292}}

@InProceedings{Adam+2020,
  title = 	 {Doubly Sparse Variational Gaussian Processes},
  author =       {Adam, Vincent and Eleftheriadis, Stefanos and Artemev, Artem and Durrande, Nicolas and Hensman, James},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2874--2884},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/adam20a/adam20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/adam20a.html},
  abstract = 	 {The use of Gaussian process models is typically limited to datasets with a few tens of thousands of observations due to their complexity and memory footprint.The two most commonly used methods to overcome this limitation are 1) the variational sparse approximation which relies on inducing points and 2) the state-space equivalent formulation of Gaussian processes which can be seen as exploiting some sparsity in the precision matrix.In this work, we propose to take the best of both worlds: we show that the inducing point framework is still valid for state space models and that it can bring further computational and memory savings. Furthermore, we provide the natural gradient formulation for the proposed variational parameterisation.Finally, this work makes it possible to use the state-space formulation inside deep Gaussian process models as illustrated in one of the experiments. }
}
@article{Jona-Lasinio+2012,
 ISSN = {19326157, 19417330},
 URL = {http://www.jstor.org/stable/41713483},
 abstract = {Directional data arise in various contexts such as oceanography (wave directions) and meteorology (wind directions), as well as with measurements on a periodic scale (weekdays, hours, etc.). Our contribution is to introduce a model-based approach to handle periodic data in the case of measurements taken at spatial locations, anticipating structured dependence between these measurements. We formulate a wrapped Gaussian spatial process model for this setting, induced from a customary linear Gaussian process. We build a hierarchical model to handle this situation and show that the fitting of such a model is possible using standard Markov chain Monte Carlo methods. Our approach enables spatial interpolation (and can accommodate measurement error). We illustrate with a set of wave direction data from the Adriatic coast of Italy, generated through a complex computer model.},
 author = {Giovanna Jona-Lasinio and Alan Gelfand and Mattia Jona-Lasinio},
 journal = {The Annals of Applied Statistics},
 number = {4},
 pages = {1478--1498},
 publisher = {Institute of Mathematical Statistics},
 title = {SPATIAL ANALYSIS OF WAVE DIRECTION DATA USING WRAPPED GAUSSIAN PROCESSES},
 urldate = {2024-08-08},
 volume = {6},
 year = {2012}
}
@ARTICLE{Jacobs+1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@article{Emerson+2023,
	abstract = {Where do firms innovate? Mapping their locations and directions in technological space is challenging due to its high dimensionality. We propose a new method to characterize firms' inventive activities via topological data analysis (TDA) that represents high-dimensional data in a shape graph. Applying this method to 333 major firms' patents in 1976--2005 reveals hitherto undocumented industry dynamics: some firms remain undifferentiated; others develop unique portfolios. Firms with unique trajectories, which we define and measure graph-theoretically as ``flares'' in the Mapper graph, tend to perform better. This association is statistically and economically significant, and continues to hold after we control for portfolio size, firm survivorship, and industry classification.},
	author = {Emerson G. Escolar and Yasuaki Hiraoka and Mitsuru Igami and Yasin Ozcan},
	doi = {https://doi.org/10.1016/j.respol.2023.104821},
	issn = {0048-7333},
	journal = {Research Policy},
	keywords = {Innovation, Mapper, Patents, R&D, Topological data analysis},
	number = {8},
	pages = {104821},
	title = {Mapping firms' locations in technological space: A topological analysis of patent statistics},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	volume = {52},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	bdsk-url-2 = {https://doi.org/10.1016/j.respol.2023.104821}}
@inproceedings{Singh+2007
,
booktitle = {Eurographics Symposium on Point-Based Graphics
},
editor = {M. Botsch and R. Pajarola and B. Chen and M. Zwicker
},
title = {{Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition
}},
author = {Singh, Gurjeet and 
Memoli, Facundo and 
Carlsson, Gunnar
},
year = {2007
},
publisher = {The Eurographics Association
},
ISSN = {1811-7813
},
ISBN = {978-3-905673-51-7
},
DOI = {/10.2312/SPBG/SPBG07/091-100
}
}
@phdthesis{Roberts1963,
    author      = {Lawrence G. Roberts},
    school      = {Massachusetts Institute of Technology},
    title       = {Machine Perception of Three-Dimensional Solids},
    year        = {1963},
    url             = {http://hdl.handle.net/1721.1/11589},
}

@article{Lee-Mumford2003,
	abstract = {Traditional views of visual processing suggest that early visual neurons in areas V1 and V2 are static spatiotemporal filters that extract local features from a visual scene. The extracted information is then channeled through a feedforward chain of modules in successively higher visual areas for further analysis. Recent electrophysiological recordings from early visual neurons in awake behaving monkeys reveal that there are many levels of complexity in the information processing of the early visual cortex, as seen in the long-latency responses of its neurons. These new findings suggest that activity in the early visual cortex is tightly coupled and highly interactive with the rest of the visual system. They lead us to propose a new theoretical setting based on the mathematical framework of hierarchical Bayesian inference for reasoning about the visual system. In this framework, the recurrent feedforward/feedback loops in the cortex serve to integrate top-down contextual priors and bottom-up observations so as to implement concurrent probabilistic inference along the visual hierarchy. We suggest that the algorithms of particle filtering and Bayesian-belief propagation might model these interactive cortical computations. We review some recent neurophysiological evidences that support the plausibility of these ideas.},
	author = {Tai Sing Lee and David Mumford},
	doi = {10.1364/JOSAA.20.001434},
	journal = {J. Opt. Soc. Am. A},
	keywords = {Vision modeling ; Edge detection; Information processing; Machine vision; Neural networks; Physiology; Spatial resolution},
	month = {Jul},
	number = {7},
	pages = {1434--1448},
	publisher = {Optica Publishing Group},
	title = {Hierarchical Bayesian inference in the visual cortex},
	url = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	volume = {20},
	year = {2003},
	bdsk-url-1 = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	bdsk-url-2 = {https://doi.org/10.1364/JOSAA.20.001434}}

@article{Lake+2015,
	abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms---for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	author = {Brenden M. Lake and Ruslan Salakhutdinov and Joshua B. Tenenbaum},
	doi = {10.1126/science.aab3050},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aab3050},
	journal = {Science},
	number = {6266},
	pages = {1332-1338},
	title = {Human-level concept learning through probabilistic program induction},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	volume = {350},
	year = {2015},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	bdsk-url-2 = {https://doi.org/10.1126/science.aab3050}}
@inproceedings{Donahue+2017,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Kr{\"a}henb{\"u}hl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJtNZAFgg}
}
@inproceedings{Bao+2022,
title={{BE}iT: {BERT} Pre-Training of Image Transformers},
author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{Kivva+2021,
	author = {Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {18087--18101},
	publisher = {Curran Associates, Inc.},
	title = {Learning latent causal graphs via mixture oracles},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf}}
@inproceedings{Kivva+2022,
title={Identifiability of deep generative models under mixture priors without auxiliary information},
author={Bohdan Kivva and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022},
url={https://openreview.net/forum?id=UeG3kt_Ebg2}
}

@InProceedings{Lopez+2024,
  title = 	 {Toward the Identifiability of Comparative Deep Generative Models},
  author =       {Lopez, Romain and Huetter, Jan-Christian and Hajiramezanali, Ehsan and Pritchard, Jonathan K and Regev, Aviv},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {868--912},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/lopez24a/lopez24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/lopez24a.html},
  abstract = 	 {Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice.  Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network).  We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.}
}
@article{Locatello+2020,
  author  = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Raetsch and Sylvain Gelly and Bernhard Sch{{\"o}}lkopf and Olivier Bachem},
  title   = {A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {209},
  pages   = {1--62},
  url     = {http://jmlr.org/papers/v21/19-976.html}
}
@inproceedings{Ding+2021,
title={Cc{\{}GAN{\}}: Continuous Conditional Generative Adversarial Networks for Image Generation},
author={Xin Ding and Yongwei Wang and Zuheng Xu and William J Welch and Z. Jane Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PrzjugOsDeE}
}
@inproceedings{Hoogeboom+2021,
title={Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=6nbpPqUCIi7}
}
@misc{Simo2024,
  author={Simo Ryu},
  title={Minimal Implementation of a D3PM (Structured Denoising Diffusion Models in Discrete State-Spaces), in pytorch},
  year={2024},
  url             = {https://github.com/cloneofsimo/d3pm},
}

@inproceedings{Kingma+2014,
	author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {{Semi-supervised Learning with Deep Generative Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf}}
@inproceedings{Chen+2023AnalogBits,
title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},
author={Ting Chen and Ruixiang ZHANG and Geoffrey Hinton},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3itjR9QxFw}
}

@article{Watson+2023,
	abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods1--9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence--structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
	date = {2023/08/01},
	date-added = {2024-08-10 21:39:03 +0900},
	date-modified = {2024-08-10 21:39:03 +0900},
	doi = {10.1038/s41586-023-06415-8},
	id = {Watson2023},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7976},
	pages = {1089--1100},
	title = {De novo design of protein structure and function with RFdiffusion},
	url = {https://doi.org/10.1038/s41586-023-06415-8},
	volume = {620},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-023-06415-8}}
@article{Abramson+2024,
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	date = {2024/06/01},
	date-added = {2024-08-10 21:52:55 +0900},
	date-modified = {2024-08-10 21:52:55 +0900},
	doi = {10.1038/s41586-024-07487-w},
	id = {Abramson2024},
	isbn = {1476-4687},
	journal = {Nature},
	number = {8016},
	pages = {493--500},
	title = {Accurate structure prediction of biomolecular interactions with AlphaFold 3},
	url = {https://doi.org/10.1038/s41586-024-07487-w},
	volume = {630},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-024-07487-w}}
@article{Krishna+2024,
	abstract = {Deep-learning methods have revolutionized protein structure prediction and design but are presently limited to protein-only systems. We describe RoseTTAFold All-Atom (RFAA), which combines a residue-based representation of amino acids and DNA bases with an atomic representation of all other groups to model assemblies that contain proteins, nucleic acids, small molecules, metals, and covalent modifications, given their sequences and chemical structures. By fine-tuning on denoising tasks, we developed RFdiffusion All-Atom (RFdiffusionAA), which builds protein structures around small molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we designed and experimentally validated, through crystallography and binding measurements, proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and the light-harvesting molecule bilin. Advances in machine learning have made protein structure prediction and design much more accurate and accessible in recent years, but these tools have generally been limited to polypeptide chains. However, ligands such as small molecules, metal ions, and nucleic acids are crucial components of most proteins, both in terms of structure and biological function. Krishna et al. present a next-generation protein structure prediction and design tool, RoseTTAFold All-Atom, that can accept a wide range of ligands and covalent amino acid modifications. The authors demonstrate superior performance on protein-ligand structure prediction relative to other tools, even in the absence of an input experimental structure. They also perform de novo design of proteins to bind cofactors and small molecules and experimentally validate these designs. ---Michael A. Funk},
	author = {Rohith Krishna and Jue Wang and Woody Ahern and Pascal Sturmfels and Preetham Venkatesh and Indrek Kalvet and Gyu Rie Lee and Felix S. Morey-Burrows and Ivan Anishchenko and Ian R. Humphreys and Ryan McHugh and Dionne Vafeados and Xinting Li and George A. Sutherland and Andrew Hitchcock and C. Neil Hunter and Alex Kang and Evans Brackenbrough and Asim K. Bera and Minkyung Baek and Frank DiMaio and David Baker},
	doi = {10.1126/science.adl2528},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.adl2528},
	journal = {Science},
	number = {6693},
	pages = {eadl2528},
	title = {Generalized biomolecular modeling and design with RoseTTAFold All-Atom},
	url = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	volume = {384},
	year = {2024},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	bdsk-url-2 = {https://doi.org/10.1126/science.adl2528}}

@article{Zheng+2024,
	abstract = {Advances in deep learning have greatly improved structure prediction of molecules. However, many macroscopic observations that are important for real-world applications are not functions of a single molecular structure but rather determined from the equilibrium distribution of structures. Conventional methods for obtaining these distributions, such as molecular dynamics simulation, are computationally expensive and often intractable. Here we introduce a deep learning framework, called Distributional Graphormer (DiG), in an attempt to predict the equilibrium distribution of molecular systems. Inspired by the annealing process in thermodynamics, DiG uses deep neural networks to transform a simple distribution towards the equilibrium distribution, conditioned on a descriptor of a molecular system such as a chemical graph or a protein sequence. This framework enables the efficient generation of diverse conformations and provides estimations of state densities, orders of magnitude faster than conventional methods. We demonstrate applications of DiG on several molecular tasks, including protein conformation sampling, ligand structure sampling, catalyst--adsorbate sampling and property-guided structure generation. DiG presents a substantial advancement in methodology for statistically understanding molecular systems, opening up new research opportunities in the molecular sciences.},
	author = {Zheng, Shuxin and He, Jiyan and Liu, Chang and Shi, Yu and Lu, Ziheng and Feng, Weitao and Ju, Fusong and Wang, Jiaxi and Zhu, Jianwei and Min, Yaosen and Zhang, He and Tang, Shidi and Hao, Hongxia and Jin, Peiran and Chen, Chi and No{\'e}, Frank and Liu, Haiguang and Liu, Tie-Yan},
	date = {2024/05/01},
	date-added = {2024-08-10 22:01:13 +0900},
	date-modified = {2024-08-10 22:01:13 +0900},
	doi = {10.1038/s42256-024-00837-3},
	id = {Zheng2024},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {5},
	pages = {558--567},
	title = {Predicting equilibrium distributions for molecular systems with deep learning},
	url = {https://doi.org/10.1038/s42256-024-00837-3},
	volume = {6},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-024-00837-3}}
@unpublished{Duvenaud2014,
    author = {David Kristjanson Duvenaud},
    year   = {2014},
    title  = {The Kernel Cookbook: Advice on Covariance functions},
    url    = {https://www.cs.toronto.edu/~duvenaud/cookbook/}
}
@ARTICLE{Linsker1988,
  author={Linsker, R.},
  journal={Computer}, 
  title={{Self-Organization in a Perceptual Network}}, 
  year={1988},
  volume={21},
  number={3},
  pages={105-117},
  keywords={Intelligent networks;Biological information theory;Circuits;Biology computing;Animal structures;Neuroscience;Genetics;System testing;Neural networks;Constraint theory},
  doi={10.1109/2.36}}

@article{Torgenson1952,
	abstract = {Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.},
	author = {Torgerson, Warren S. },
	date = {1952/12/01},
	date-added = {2024-08-10 23:19:09 +0900},
	date-modified = {2024-08-10 23:19:09 +0900},
	doi = {10.1007/BF02288916},
	id = {Torgerson1952},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {401--419},
	title = {Multidimensional scaling: I. Theory and method},
	url = {https://doi.org/10.1007/BF02288916},
	volume = {17},
	year = {1952},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288916}}

@article{Kruskal1964,
	abstract = {Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.},
	author = {Kruskal, J.  B. },
	date = {1964/03/01},
	date-added = {2024-08-10 23:20:08 +0900},
	date-modified = {2024-08-10 23:20:08 +0900},
	doi = {10.1007/BF02289565},
	id = {Kruskal1964},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {1--27},
	title = {Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
	url = {https://doi.org/10.1007/BF02289565},
	volume = {29},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289565}}
@article{Poole-Rosenthal1985,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2111172},
 abstract = {A general nonlinear logit model is used to analyze political choice data. The model assumes probabilistic voting based on a spatial utility function. The parameters of the utility function and the spatial coordinates of the choices and the choosers can all be estimated on the basis of observed choices. Ordinary Guttman scaling is a degenerate case of this model. Estimation of the model is implemented in the NOMINATE program for one dimensional analysis of two alternative choices with no nonvoting. The robustness and face validity of the program outputs are evaluated on the basis of roll call voting data for the U.S. House and Senate.},
 author = {Keith T. Poole and Howard Rosenthal},
 journal = {American Journal of Political Science},
 number = {2},
 pages = {357--384},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {A Spatial Model for Legislative Roll Call Analysis},
 urldate = {2024-08-10},
 volume = {29},
 year = {1985}
}

@article{岡田謙介-加藤淳子2016,
	author = {岡田謙介 and 加藤淳子},
	doi = {10.2333/jbhmk.43.155},
	journal = {行動計量学},
	number = {2},
	pages = {155-166},
	title = {政治学における空間分析と認知空間},
	volume = {43},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.43.155}}
@book{Enelow-Hinich1984,
    author         = {James M. Enelow and Melvin J. Hinich},
    year           = {1984},
    title          = {The Spatial Theory of Voting: An Introduction},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.cambridge.org/us/universitypress/subjects/politics-international-relations/political-theory/spatial-theory-voting-introduction?format=PB&isbn=9780521275156},
    publisher      = {Cambridge University Press}
}

@article{Eckart-Young1936,
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
	author = {Eckart, Carl and Young, Gale},
	date = {1936/09/01},
	date-added = {2024-08-11 11:46:20 +0900},
	date-modified = {2024-08-11 11:46:20 +0900},
	doi = {10.1007/BF02288367},
	id = {Eckart1936},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {211--218},
	title = {The approximation of one matrix by another of lower rank},
	url = {https://doi.org/10.1007/BF02288367},
	volume = {1},
	year = {1936},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288367}}
@inbook{Easterling1987,
    author         = {D. V. Easterling},
    chapter        = {Political Scicnce: Using the Generalized Euclidian Model to Study Ideological Shifts in the U.S. Senate },
    editor         = {Robert M. Hamer and Forrest W. Young},
    pages          = {219-256},
    publisher      = {Psychology Press},
    title          = {Multidimensional Scaling: History, Theory, and Applications},
    year           = {1987},
    url             = {https://doi.org/10.4324/9780203767719},
}
@article{Coombs1950,
    author          = {C. H. Coombs},
    year            = {1950},
    title           = {{Psychological Scaling without a Unit of Measurement}},
    journal         = {Psychological Review},
    volume          = {57},
    number          = {3},
    pages           = {145-158},
    url             = {https://psycnet.apa.org/doi/10.1037/h0060984}
}

@article{足立浩平2000,
	author = {足立浩平},
	doi = {10.2333/jbhmk.27.12},
	journal = {行動計量学},
	number = {1},
	pages = {12-23},
	title = {計量多次元展開法の変量モデル},
	volume = {27},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.27.12}}
@book{MacRae1958,
    author         = {Duncan MacRae},
    year           = {1958},
    title          = {Dimensions of Congressional Voting: A Statistical Study of the House of Representatives in the Eighty-first Congress},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {University of California Press}
}
@article{Poole+2011,
 title={Scaling Roll Call Votes with wnominate in R},
 volume={42},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v042i14},
 doi={10.18637/jss.v042.i14},
 abstract={This paper presents a software package designed to estimate Poole and Rosenthal W-NOMINATE scores in R. The package uses a logistic regression model to analyze political choice data, usually (though not exclusively) from a legislative setting. In contrast to other scaling methods, W-NOMINATE explicitly assumes probabilistic voting based on a spatial utility function, where the parameters of the utility function and the spatial coordinates of the legislators and the votes can all be estimated on the basis of observed voting behavior. Building on software written by Poole in Fortran, the new &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R facilitates easier data input and manipulation, generates bootstrapped standard errors, and includes a new suite of graphics functions to display the results. We demonstrate the functionality of this package by conducting a natural experiment using roll calls -- an experiment which is greatly simplified by the data manipulation capabilities of the &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R.},
 number={14},
 journal={Journal of Statistical Software},
 author={Poole, Keith T. and Lewis, Jeffrey B. and Lo, James and Carroll, Royce},
 year={2011},
 pages={1–21}
}
@article{Lee2001,
	abstract = {Multidimensional scaling models of stimulus domains are widely used as a representational basis for cognitive modeling. These representations associate stimuli with points in a coordinate space that has some predetermined number of dimensions. Although the choice of dimensionality can significantly influence cognitive modeling, it is often made on the basis of unsatisfactory heuristics. To address this problem, a Bayesian approach to dimensionality determination, based on the Bayesian Information Criterion (BIC), is developed using a probabilistic formulation of multidimensional scaling. The BIC approach formalizes the trade-off between data-fit and model complexity implicit in the problem of dimensionality determination and allows for the explicit introduction of information regarding data precision. Monte Carlo simulations are presented that indicate, by using this approach, the determined dimensionality is likely to be accurate if either a significant number of stimuli are considered or a reasonable estimate of precision is available. The approach is demonstrated using an established data set involving the judged pairwise similarities between a set of geometric stimuli.},
	author = {Michael D. Lee},
	doi = {https://doi.org/10.1006/jmps.1999.1300},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	number = {1},
	pages = {149-166},
	title = {Determining the Dimensionality of Multidimensional Scaling Representations for Cognitive Modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	volume = {45},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	bdsk-url-2 = {https://doi.org/10.1006/jmps.1999.1300}}
@article{Baker-Poole2013,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/23359696},
 abstract = {In this article, we show how to apply Bayesian methods to noisy ratio scale distances for both the classical similarities problem as well as the unfolding problem. Bayesian methods produce essentially the same point estimates as the classical methods, but are superior in that they provide more accurate measures of uncertainty in the data. Identification is nontrivial for this class of problems because a configuration of points that reproduces the distances is identified only up to a choice of origin, angles of rotation, and sign flips on the dimensions. We prove that fixing the origin and rotation is sufficient to identify a configuration in the sense that the corresponding maxima/minima are inflection points with full-rank Hessians. However, an unavoidable result is multiple posterior distributions that are mirror images of one another. This poses a problem for Markov chain Monte Carlo (MCMC) methods. The approach we take is to find the optimal solution using standard optimizers. The configuration of points from the optimizers is then used to isolate a single Bayesian posterior that can then be easily analyzed with standard MCMC methods.},
 author = {Ryan Bakker and Keith T. Poole},
 journal = {Political Analysis},
 number = {1},
 pages = {125--140},
 publisher = {Oxford University Press},
 title = {Bayesian Metric Multidimensional Scaling},
 urldate = {2024-08-10},
 volume = {21},
 year = {2013}
}
@inproceedings{Lim+2024,
    author          = {Johan Lim and Sooahn Shin and Jong Hee Park},
    year            = {2024},
    title           = {$\ell^1$-Based Bayesian Ideal Point Model for Multidimensional Politics},
    booktitle       = {ISI World Statistics Congress},
    volume          = {64},
    pages           = {},
    url             = {https://www.isi-next.org/abstracts/submission/1310/view/}
}
@article{Jackman2001,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791646},
 abstract = {Vote-specific parameters are often by-products of roll call analysis, the primary goal being the measurement of legislators' ideal points. But these vote-specific parameters are more important in higher-dimensional settings: prior restrictions on vote parameters help identify the model, and researchers often have prior beliefs about the nature of the dimensions underlying the proposal space. Bayesian methods provide a straightforward and rigorous way for incorporating these prior beliefs into roll call analysis. I demonstrate this by exploiting the close connections among roll call analysis, item—response models, and "full-information" factor analysis. Vote-specific discrimination parameters are equivalent to factor loadings, and as in factor analysis, they (1) enable researchers to discern the substantive content of the recovered dimensions, (2) can be used for assessing dimensionality and model checking, and (3) are an obvious vehicle for introducing and testing researchers' prior beliefs about the dimensions. Bayesian simulation facilitates these uses of discrimination parameters, by simplifying estimation and inference for the massive number of parameters generated by roll call analysis.},
 author = {Simon Jackman},
 journal = {Political Analysis},
 number = {3},
 pages = {227--241},
 publisher = {[Oxford University Press, Society for Political Methodology]},
 title = {Multidimensional Analysis of Roll Call Data via Bayesian Simulation: Identification, Estimation, Inference, and Model Checking},
 urldate = {2024-08-11},
 volume = {9},
 year = {2001}
}
@techreport{deLeeuw1977,
    author          = {Jan {de Leeuw}},
    year            = {1977},
    title           = {Applications of Convex Analysis to Multidimensional Scaling},
    institution = {UCLA: Department of Statistics},
    url             = {https://escholarship.org/uc/item/7wg0k7xq},
}
@ARTICLE{Sammon1969,
  author={Sammon, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Nonlinear Mapping for Data Structure Analysis}, 
  year={1969},
  volume={C-18},
  number={5},
  pages={401-409},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric, pattern recognition, statistics.},
  doi={10.1109/T-C.1969.222678}}

@article{Tenenbaum+2000,
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 106 optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	author = {Joshua B. Tenenbaum and Vin de Silva and John C. Langford},
	doi = {10.1126/science.290.5500.2319},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
	journal = {Science},
	number = {5500},
	pages = {2319-2323},
	title = {{A Global Geometric Framework for Nonlinear Dimensionality Reduction}},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2319}}

@article{Balasubramanian-Schwartz2002,
	author = {Mukund Balasubramanian and Eric L. Schwartz},
	doi = {10.1126/science.295.5552.7a},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.295.5552.7a},
	journal = {Science},
	number = {5552},
	pages = {7-7},
	title = {The Isomap Algorithm and Topological Stability},
	url = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	volume = {295},
	year = {2002},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	bdsk-url-2 = {https://doi.org/10.1126/science.295.5552.7a}}

@article{Choi-Choi2007,
	abstract = {Isomap is one of widely used low-dimensional embedding methods, where geodesic distances on a weighted graph are incorporated with the classical scaling (metric multidimensional scaling). In this paper we pay our attention to two critical issues that were not considered in Isomap, such as: (1) generalization property (projection property); (2) topological stability. Then we present a robust kernel Isomap method, armed with such two properties. We present a method which relates the Isomap to Mercer kernel machines, so that the generalization property naturally emerges, through kernel principal component analysis. For topological stability, we investigate the network flow in a graph, providing a method for eliminating critical outliers. The useful behavior of the robust kernel Isomap is confirmed through numerical experiments with several data sets.},
	author = {Heeyoul Choi and Seungjin Choi},
	doi = {https://doi.org/10.1016/j.patcog.2006.04.025},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Isomap, Kernel PCA, Manifold learning, Multidimensional scaling (MDS), Nonlinear dimensionality reduction},
	number = {3},
	pages = {853-862},
	title = {Robust kernel Isomap},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	volume = {40},
	year = {2007},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2006.04.025}}
@ARTICLE{Scholkopf+1998,
  author={Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
  journal={Neural Computation}, 
  title={Nonlinear Component Analysis as a Kernel Eigenvalue Problem}, 
  year={1998},
  volume={10},
  number={5},
  pages={1299-1319},
  keywords={},
  doi={10.1162/089976698300017467}}
@inproceedings{Weinberger+2004,
author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
title = {Learning a kernel matrix for nonlinear dimensionality reduction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015345},
doi = {10.1145/1015330.1015345},
abstract = {We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {106},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{Roweis-Saul2000,
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	author = {Sam T. Roweis and Lawrence K. Saul},
	doi = {10.1126/science.290.5500.2323},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2323},
	journal = {Science},
	number = {5500},
	pages = {2323-2326},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2323}}

@inproceedings{Mikhali-Partha2001,
	author = {Belkin, Mikhail and Niyogi, Partha},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf}}
@book{Chung1997,
    author         = {Fan R. K. Chung},
    year           = {1997},
    title          = {Spectral Graph Theory},
    series         = {CBMS Regional Conference Series in Mathematics},
    volume         = {92},
    edition        = {},
    url            = {https://doi.org/10.1090/cbms/092},
    publisher      = {American Mathematical Society}
}

@inproceedings{Hinton-Roweis2002,
	author = {Hinton, Geoffrey E and Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Becker and S. Thrun and K. Obermayer},
	publisher = {MIT Press},
	title = {Stochastic Neighbor Embedding},
	url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
	volume = {15},
	year = {2002},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf}}
@article{Maaten-Hinton2008,
  author  = {Laurens {van der Maaten} and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@article{Coifman+2005,
	author = {R. R. Coifman and S. Lafon and A. B. Lee and M. Maggioni and B. Nadler and F. Warner and S. W. Zucker},
	doi = {10.1073/pnas.0500334102},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0500334102},
	journal = {Proceedings of the National Academy of Sciences},
	number = {21},
	pages = {7426-7431},
	title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	volume = {102},
	year = {2005},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0500334102}}
@inproceedings{Carreira-Perpinan2010,
author = {Carreira-Perpi\~{n}an, Miguel \'{A}.},
title = {The elastic embedding algorithm for dimensionality reduction},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a new dimensionality reduction method, the elastic embedding (EE), that optimises an intuitive, nonlinear objective function of the low-dimensional coordinates of the data. The method reveals a fundamental relation betwen a spectral method, Laplacian eigenmaps, and a nonlinear method, stochastic neighbour embedding; and shows that EE can be seen as learning both the coordinates and the affinities between data points. We give a homotopy method to train EE, characterise the critical value of the homotopy parameter, and study the method's behaviour. For a fixed homotopy parameter, we give a globally convergent iterative algorithm that is very effective and requires no user parameters. Finally, we give an extension to out-of-sample points. In standard datasets, EE obtains results as good or better than those of SNE, but more efficiently and robustly.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {167–174},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}
@inproceedings{Wu-Fischer2020,
title={Phase Transitions for the Information Bottleneck in Representation Learning},
author={Tailin Wu and Ian Fischer},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJloElBYvB}
}
@article{Wattemnerg+2016,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}
@article{vanderMaaten2014,
  author  = {Laurens van der Maaten},
  title   = {Accelerating t-SNE using Tree-Based Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {93},
  pages   = {3221--3245},
  url     = {http://jmlr.org/papers/v15/vandermaaten14a.html}
}
@article{McInnes+2018, doi = {10.21105/joss.00861}, url = {https://doi.org/10.21105/joss.00861}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {29}, pages = {861}, author = {Leland McInnes and John Healy and Nathaniel Saul and Lukas Großberger}, title = {UMAP: Uniform Manifold Approximation and Projection}, journal = {Journal of Open Source Software} } 

@inproceedings{Weinberger+2005,
	author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf}}
@article{Weinberger-Saul2009,
  author  = {Kilian Q. Weinberger and Lawrence K. Saul},
  title   = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  number  = {9},
  pages   = {207-244},
  url     = {http://jmlr.org/papers/v10/weinberger09a.html}
}

@inproceedings{Goldberger+2004,
	author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	publisher = {MIT Press},
	title = {Neighbourhood Components Analysis},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf}}

@inproceedings{Musgrave+2020,
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best. Code is available at github.com/KevinMusgrave/powerful-benchmarker.},
	address = {Cham},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58595-2},
	pages = {681--699},
	publisher = {Springer International Publishing},
	title = {A Metric Learning Reality Check},
	year = {2020}}
@article{Chopra+2005,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Sumit Chopra and Raia Hadsell and Yann LeCun},
  journal={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  year={2005},
  volume={1},
  pages={539-546},
  url={https://doi.org/10.1109/CVPR.2005.202}
}
@InProceedings{Schroff+2015,
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
title = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015},
url             = {https://openaccess.thecvf.com/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
}

@inproceedings{Sohn2016,
	author = {Sohn, Kihyuk},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf}}
@INPROCEEDINGS{Movshovitz-Attias+2017,
author = {Y. Movshovitz-Attias and A. Toshev and T. K. Leung and S. Ioffe and S. Singh},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
title = {No Fuss Distance Metric Learning Using Proxies},
year = {2017},
volume = {},
issn = {2380-7504},
pages = {360-368},
abstract = {We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship - an anchor point x is similar to a set of positive points Y , and dissimilar to a set of negative points Z, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.},
keywords = {training;measurement;computer vision;optimization;convergence;fasteners;training data},
doi = {10.1109/ICCV.2017.47},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.47},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@InProceedings{Qian+2019,
author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
title = {SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019},
url             = {https://openaccess.thecvf.com/content_ICCV_2019/html/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.html},
}

@article{Nadaraya1964,
	abstract = { A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly. },
	author = {Nadaraya, E. A.},
	doi = {10.1137/1109020},
	eprint = {https://doi.org/10.1137/1109020},
	journal = {Theory of Probability \& Its Applications},
	number = {1},
	pages = {141-142},
	title = {On Estimating Regression},
	url = {https://doi.org/10.1137/1109020},
	volume = {9},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1137/1109020}}
@article{Watson1964,
 ISSN = {0581572X},
 URL = {http://www.jstor.org/stable/25049340},
 abstract = {Few would deny that the most powerful statistical tool is graph paper. When however there are many observations (and/or many variables) graphical procedures become tedious. It seems to the author that the most characteristic problem for statisticians at the moment is the development of methods for analyzing the data poured out by electronic observing systems. The present paper gives a simple computer method for obtaining a "graph" from a large number of observations.},
 author = {Geoffrey S. Watson},
 journal = {Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)},
 number = {4},
 pages = {359--372},
 publisher = {Springer},
 title = {Smooth Regression Analysis},
 urldate = {2024-08-11},
 volume = {26},
 year = {1964}
}

@article{Cleveland-Devlin1988,
	author = {William S. Cleveland and Susan J. Devlin},
	doi = {10.1080/01621459.1988.10478639},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478639},
	journal = {Journal of the American Statistical Association},
	number = {403},
	pages = {596--610},
	publisher = {Taylor \& Francis},
	title = {Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478639},
	volume = {83},
	year = {1988},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478639},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1988.10478639}}

@article{Cleveland1979,
	author = {William S. Cleveland},
	doi = {10.1080/01621459.1979.10481038},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1979.10481038},
	journal = {Journal of the American Statistical Association},
	number = {368},
	pages = {829--836},
	publisher = {Taylor \& Francis},
	title = {Robust Locally Weighted Regression and Smoothing Scatterplots},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	volume = {74},
	year = {1979},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1979.10481038}}

@article{Savitzky-Golay1964,
	annote = {doi: 10.1021/ac60214a047},
	author = {Savitzky, Abraham. and Golay, M. J. E.},
	date = {1964/07/01},
	date-added = {2024-08-12 00:30:38 +0900},
	date-modified = {2024-08-12 00:30:38 +0900},
	doi = {10.1021/ac60214a047},
	isbn = {0003-2700},
	journal = {Analytical Chemistry},
	journal1 = {Analytical Chemistry},
	journal2 = {Anal. Chem.},
	month = {07},
	number = {8},
	pages = {1627--1639},
	publisher = {American Chemical Society},
	title = {Smoothing and Differentiation of Data by Simplified Least Squares Procedures.},
	type = {doi: 10.1021/ac60214a047},
	url = {https://doi.org/10.1021/ac60214a047},
	volume = {36},
	year = {1964},
	year1 = {1964},
	bdsk-url-1 = {https://doi.org/10.1021/ac60214a047}}

@inproceedings{Chaudhuri-DasGupta2014,
	author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Rates of Convergence for Nearest Neighbor Classification},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf}}

@article{Gonzalez1985,
	abstract = {The problem of clustering a set of points so as to minimize the maximum intercluster distance is studied. An O(kn) approximation algorithm, where n is the number of points and k is the number of clusters, that guarantees solutions with an objective function value within two times the optimal solution value is presented. This approximation algorithm succeeds as long as the set of points satisfies the triangular inequality. We also show that our approximation algorithm is best possible, with respect to the approximation bound, if P ≠ NP.},
	author = {Teofilo F. Gonzalez},
	doi = {https://doi.org/10.1016/0304-3975(85)90224-5},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Algorithms, clustering, NP-completeness, approximation algorithms, minimizing the maximum intercluster distance},
	pages = {293-306},
	title = {Clustering to minimize the maximum intercluster distance},
	url = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
	volume = {38},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
	bdsk-url-2 = {https://doi.org/10.1016/0304-3975(85)90224-5}}
@inproceedings{David-Sergei2007,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {k-means++: the advantages of careful seeding},
year = {2007},
isbn = {9780898716245},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
location = {New Orleans, Louisiana},
series = {SODA '07}
}
@inproceedings{Kaufmann-Rousseeuw1987,
    author          = {L. Kaufmann and P. Rousseeuw},
    year            = {1987},
    title           = {{Clustering by Means of Medoids}},
    booktitle       = {Proceedings of Statistical Data Analysis Based on the L1 Norm Conference},
    volume          = {},
    pages           = {405-416},
    url             = {}
}

@article{Park-Jun2009,
	abstract = {This paper proposes a new algorithm for K-medoids clustering which runs like the K-means algorithm and tests several methods for selecting initial medoids. The proposed algorithm calculates the distance matrix once and uses it for finding new medoids at every iterative step. To evaluate the proposed algorithm, we use some real and artificial data sets and compare with the results of other algorithms in terms of the adjusted Rand index. Experimental results show that the proposed algorithm takes a significantly reduced time in computation with comparable performance against the partitioning around medoids.},
	author = {Hae-Sang Park and Chi-Hyuck Jun},
	doi = {https://doi.org/10.1016/j.eswa.2008.01.039},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Clustering, K-means, K-medoids, Rand index},
	number = {2, Part 2},
	pages = {3336-3341},
	title = {A simple and fast algorithm for K-medoids clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S095741740800081X},
	volume = {36},
	year = {2009},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741740800081X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2008.01.039}}

@inproceedings{Mnih-Kavukcuoglu2013,
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
	volume = {26},
	year = {2013},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf}}

@InProceedings{Chen+2020SimCLR,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@inproceedings{Tian+2020,
	abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog'' can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.},
	address = {Cham},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58621-8},
	pages = {776--794},
	publisher = {Springer International Publishing},
	title = {Contrastive Multiview Coding},
	year = {2020}}
@misc{Faghri+2018,
      title={VSE++: Improving Visual-Semantic Embeddings with Hard Negatives}, 
      author={Fartash Faghri and David J. Fleet and Jamie Ryan Kiros and Sanja Fidler},
      year={2018},
      eprint={1707.05612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.05612}, 
}

@inproceedings{Tian+2020WhatMakes,
	author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {6827--6839},
	publisher = {Curran Associates, Inc.},
	title = {What Makes for Good Views for Contrastive Learning?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf}}

@inproceedings{Khosla+2020,
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {18661--18673},
	publisher = {Curran Associates, Inc.},
	title = {Supervised Contrastive Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf}}
@INPROCEEDINGS{Caron+2021,
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Emerging Properties in Self-Supervised Vision Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={9630-9640},
  keywords={Training;Image segmentation;Computer vision;Semantics;Layout;Image retrieval;Computer architecture;Representation learning;Recognition and classification;Transfer/Low-shot/Semi/Unsupervised Learning},
  doi={10.1109/ICCV48922.2021.00951}}

@inproceedings{Grill+2020,
	author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {21271--21284},
	publisher = {Curran Associates, Inc.},
	title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf}}

@InProceedings{Zbontar+2021,
  title = 	 {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author =       {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12310--12320},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zbontar21a.html},
  abstract = 	 {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.}
}

@inproceedings{Gretton+2007,
	author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch\"{o}lkopf, Bernhard and Smola, Alex},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {A Kernel Statistical Test of Independence},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf}}

@InProceedings{Roeder+2021,
  title = 	 {On Linear Identifiability of Learned Representations},
  author =       {Roeder, Geoffrey and Metz, Luke and Kingma, Durk},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9030--9039},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/roeder21a/roeder21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/roeder21a.html},
  abstract = 	 {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions lack identifiability in parameter space, because they are over-parameterized by design. In this paper, building on recent advances in nonlinear Independent Components Analysis, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.}
}

@inproceedings{Halva+2021,
	author = {H\"{a}lv\"{a}, Hermanni and Le Corff, Sylvain and Leh\'{e}ricy, Luc and So, Jonathan and Zhu, Yongjie and Gassiat, Elisabeth and Hyvarinen, Aapo},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {1624--1633},
	publisher = {Curran Associates, Inc.},
	title = {Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf}}
@article{Bell-Sejnowski1995,
    author = {Bell, Anthony J. and Sejnowski, Terrence J.},
    title = "{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}",
    journal = {Neural Computation},
    volume = {7},
    number = {6},
    pages = {1129-1159},
    year = {1995},
    month = {11},
    abstract = "{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1995.7.6.1129},
    url = {https://doi.org/10.1162/neco.1995.7.6.1129},
    eprint = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
}
@incollection{Barlow1961,
    author = {Barlow, H. B.},
    isbn = {9780262518420},
    title = "{Possible Principles Underlying the Transformations of Sensory Messages}",
    booktitle = "{Sensory Communication}",
    publisher = {The MIT Press},
    year = {1961},
    month = {09},
    abstract = "{This chapter is an attempt to formulate ideas about the operations performed by physiological mechanisms, and not merely a discussion of the physiological mechanisms of sensory pathways. It presents three hypotheses regarding the purpose of sensory relays. The first one is the “password” hypothesis, which posits that, since animals respond specifically to specific stimuli, their sensory pathways must possess mechanisms for detecting such stimuli and discriminating between them. The second hypothesis is the fashionable one that relays act as control points at which the flow of information is modulated according to the requirements of other parts of the nervous system. Finally, the third hypothesis theorizes that reduction of redundancy is an important principle guiding the organization of sensory messages and is carried out at relays in the sensory pathways.}",
    doi = {10.7551/mitpress/9780262518420.003.0013},
    url = {https://doi.org/10.7551/mitpress/9780262518420.003.0013},
    eprint = {https://academic.oup.com/mit-press-scholarship-online/book/0/chapter/180090664/chapter-ag-pdf/44697172/book\_20714\_section\_180090664.ag.pdf},
}

@article{Barlow1972,
	abstract = { The problem discussed is the relationship between the firing of single neurons in sensory pathways and subjectively experienced sensations. The conclusions are formulated as the following five dogmas:To understand nervous function one needs to look at interactions at a cellular level, rather than either a more macroscopic or microscopic level, because behaviour depends upon the organized pattern of these intercellular interactions.The sensory system is organized to achieve as complete a representation of the sensory stimulus as possible with the minimum number of active neurons.Trigger features of sensory neurons are matched to redundant patterns of stimulation by experience as well as by developmental processes.Perception corresponds to the activity of a small selection from the very numerous high-level neurons, each of which corresponds to a pattern of external events of the order of complexity of the events symbolized by a word.High impulse frequency in such neurons corresponds to high certainty that the trigger feature is present.The development of the concepts leading up to these speculative dogmas, their experimental basis, and some of their limitations are discussed. },
	author = {H B Barlow},
	doi = {10.1068/p010371},
	eprint = {https://doi.org/10.1068/p010371},
	journal = {Perception},
	note = {PMID: 4377168},
	number = {4},
	pages = {371-394},
	title = {Single Units and Sensation: A Neuron Doctrine for Perceptual Psychology?},
	url = {https://doi.org/10.1068/p010371},
	volume = {1},
	year = {1972},
	bdsk-url-1 = {https://doi.org/10.1068/p010371}}

@article{島崎秀昭2019,
	author = {島崎秀昭},
	doi = {10.3902/jnns.26.72},
	journal = {日本神経回路学会誌},
	number = {3},
	pages = {72-98},
	title = {ベイズ統計と熱力学から見る生物の学習と認識のダイナミクス},
	volume = {26},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3902/jnns.26.72}}

@article{Laughlin1981,
	author = {Simon Laughlin},
	doi = {doi:10.1515/znc-1981-9-1040},
	journal = {Zeitschrift f{\"u}r Naturforschung C},
	lastchecked = {2024-08-12},
	number = {9-10},
	pages = {910--912},
	url = {https://doi.org/10.1515/znc-1981-9-1040},
	volume = {36},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1515/znc-1981-9-1040},
	title     = {A Simple Coding Procedure Enhances a Neuron's Information Capacity},
	}

@article{Brenner+2000,
	abstract = {Adaptation is a widespread phenomenon in nervous systems, providing flexibility to function under varying external conditions. Here, we relate an adaptive property of a sensory system directly to its function as a carrier of information about input signals. We show that the input/output relation of a sensory system in a dynamic environment changes with the statistical properties of the environment. Specifically, when the dynamic range of inputs changes, the input/output relation rescales so as to match the dynamic range of responses to that of the inputs. We give direct evidence that the scaling of the input/output relation is set to maximize information transmission for each distribution of signals. This adaptive behavior should be particularly useful in dealing with the intermittent statistics of natural signals.},
	author = {Naama Brenner and William Bialek and Rob {de Ruyter van Steveninck}},
	doi = {https://doi.org/10.1016/S0896-6273(00)81205-2},
	issn = {0896-6273},
	journal = {Neuron},
	number = {3},
	pages = {695-702},
	title = {Adaptive Rescaling Maximizes Information Transmission},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627300812052},
	volume = {26},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0896627300812052},
	bdsk-url-2 = {https://doi.org/10.1016/S0896-6273(00)81205-2}}
@book{Doya+2006,
    author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh P.N.},
    title = "{Bayesian Brain: Probabilistic Approaches to Neural Coding }",
    publisher = {The MIT Press},
    year = {2006},
    month = {12},
    abstract = "{Experimental and theoretical neuroscientists use Bayesian approaches to analyze the brain mechanisms of perception, decision-making, and motor control.A Bayesian approach can contribute to an understanding of the brain on multiple levels, by giving normative predictions about how an ideal sensory system should combine prior knowledge and observation, by providing mechanistic interpretation of the dynamic functioning of the brain circuit, and by suggesting optimal ways of deciphering experimental data. Bayesian Brain brings together contributions from both experimental and theoretical neuroscientists that examine the brain mechanisms of perception, decision making, and motor control according to the concepts of Bayesian estimation.After an overview of the mathematical concepts, including Bayes' theorem, that are basic to understanding the approaches discussed, contributors discuss how Bayesian concepts can be used for interpretation of such neurobiological data as neural spikes and functional brain imaging. Next, contributors examine the modeling of sensory processing, including the neural coding of information about the outside world. Finally, contributors explore dynamic processes for proper behaviors, including the mathematics of the speed and accuracy of perceptual decisions and neural models of belief propagation.}",
    isbn = {9780262294188},
    doi = {10.7551/mitpress/9780262042383.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262042383.001.0001},
}
@INPROCEEDINGS{Romaszko+2017,
  author={Romaszko, Lukasz and Williams, Christopher K. I. and Moreno, Pol and Kohli, Pushmeet},
  booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Vision-as-Inverse-Graphics: Obtaining a Rich 3D Explanation of a Scene from a Single Image}, 
  year={2017},
  volume={},
  number={},
  pages={940-948},
  keywords={Cameras;Probabilistic logic;Lighting;Detectors;Object detection;Graphics;Transforms},
  doi={10.1109/ICCVW.2017.115}}
@INPROCEEDINGS{Tishby-Zaslavsky2015,
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  keywords={Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training},
  doi={10.1109/ITW.2015.7133169}}
@misc{Tishby+2000,
      title={The information bottleneck method}, 
      author={Naftali Tishby and Fernando C. Pereira and William Bialek},
      year={2000},
      eprint={physics/0004057},
      archivePrefix={arXiv},
      primaryClass={physics.data-an},
      url={https://arxiv.org/abs/physics/0004057}, 
}
@misc{Andrieu+2024,
      title={Gradient-free optimization via integration}, 
      author={Christophe Andrieu and Nicolas Chopin and Ettore Fincato and Mathieu Gerber},
      year={2024},
      eprint={2408.00888},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2408.00888}, 
}

@book{足立-山本2024,
	author         = {足立浩平 and 山本倫生},
	year           = {2024},
	title          = {主成分分析と因子分析―特異値分解を出発点として―},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {https://www.kyoritsu-pub.co.jp/book/b10085699.html},
	publisher      = {共立出版}
}

@book{Thurstone1947,
	author         = {L. L. Thurstone},
	year           = {1947},
	title          = {Multiple Factor Analysis},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {},
	publisher      = {University of Chicago Press}
}

@article{Stewart1993,
	abstract = { This paper surveys the contributions of five mathematicians---Eugenio Beltrami (1835--1899), Camille Jordan (1838--1921), James Joseph Sylvester (1814--1897), Erhard Schmidt (1876--1959), and Hermann Weyl (1885--1955)---who were responsible for establishing the existence of the singular value decomposition and developing its theory. },
	author = {Stewart, G. W.},
	doi = {10.1137/1035134},
	eprint = {https://doi.org/10.1137/1035134},
	journal = {SIAM Review},
	number = {4},
	pages = {551-566},
	title = {On the Early History of the Singular Value Decomposition},
	url = {https://doi.org/10.1137/1035134},
	volume = {35},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1137/1035134}}
@article{Sylvester1889,
	author          = {James Joseph Sylvester},
	year            = {1889},
	title           = {On The Reduction of a Bilinear Quantic of the $n$-th Order to the Form of a Sum of $n$ Products by a Double Orthogonal Substitution},
	journal         = {The Messenger of Mathematics},
	volume          = {18},
	number          = {},
	pages           = {42-46},
	url             = {https://books.google.co.jp/books?id=cxRKAQAAMAAJ&pg=PA1&hl=ja&source=gbs_toc_r&cad=2#v=onepage&q&f=false}
}
@article{Beltrami1873,
	author          = {E. Beltrami},
	year            = {1873},
	title           = {Sulle Funzioni Bilineari},
	journal         = {Giornale di Matematiche ad Uso degli Studenti Delle Universita},
	volume          = {11},
	number          = {},
	pages           = {98-106},
	url             = {https://gallica.bnf.fr/ark:/12148/bpt6k99434d/f442}
}
@article{Moore1920,
	author          = {E. H. Moore},
	year            = {1920},
	title           = {On the Reciprocal of the General Algebraic Matrix},
	journal         = {Bulletin of the American Mathematical Society},
	volume          = {26},
	number          = {9},
	pages           = {394-395},
	url             = {https://doi.org/10.1090%2FS0002-9904-1920-03322-7},
	note            = {In The fourteenth western meeting of the American Mathematical Society},
}
@article{Penrose1955, title={A generalized inverse for matrices}, volume={51}, DOI={10.1017/S0305004100030401}, number={3}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Penrose, R.}, year={1955}, pages={406–413}}

@article{Halko+2011,
	abstract = { Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m \times n\$ matrix. (i) For a dense input matrix, randomized algorithms require \$\bigO(mn \log(k))\$ floating-point operations (flops) in contrast to \$ \bigO(mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \$\bigO(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. },
	author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
	doi = {10.1137/090771806},
	eprint = {https://doi.org/10.1137/090771806},
	journal = {SIAM Review},
	number = {2},
	pages = {217-288},
	title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
	url = {https://doi.org/10.1137/090771806},
	volume = {53},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1137/090771806}}
@techreport{Murray+2023,
    Author= {Murray, Riley and Demmel, James and Mahoney, Michael W. and Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman Asif and Grigori, Laura and Luszczek, Piotr and Derezinski, Michal and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui and Dongarra, Jack},
    Title= {Randomized Numerical Linear Algebra: A Perspective on the Field With an Eye to Software},
    Year= {2023},
    Month= {Feb},
    Url= {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-19.html},
    Number= {UCB/EECS-2023-19},
}
@article{Drineas+2016,
author = {Drineas, Petros and Mahoney, Michael W.},
title = {RandNLA: randomized numerical linear algebra},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2842602},
doi = {10.1145/2842602},
abstract = {Randomization offers new benefits for large-scale linear algebra computations.},
journal = {Commun. ACM},
month = {may},
pages = {80–90},
numpages = {11}
}

@inproceedings{Roweis1997,
	author = {Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Jordan and M. Kearns and S. Solla},
	publisher = {MIT Press},
	title = {EM Algorithms for PCA and SPCA},
	url = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf},
	volume = {10},
	year = {1997},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf}}
@article{Gabriel1971,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334381},
 abstract = {Any matrix of rank two can be displayed as a biplot which consists of a vector for each row and a vector for each column, chosen so that any element of the matrix is exactly the inner product of the vectors corresponding to its row and to its column. If a matrix is of higher rank, one may display it approximately by a biplot of a matrix of rank two which approximates the original matrix. The biplot provides a useful tool of data analysis and allows the visual appraisal of the structure of large data matrices. It is especially revealing in principal component analysis, where the biplot can show inter-unit distances and indicate clustering of units as well as display variances and correlations of the variables.},
 author = {K. R. Gabriel},
 journal = {Biometrika},
 number = {3},
 pages = {453--467},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Biplot Graphic Display of Matrices with Application to Principal Component Analysis},
 urldate = {2024-08-12},
 volume = {58},
 year = {1971}
}
@article{Gower2004,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/20441132},
 abstract = {A simple geometry allows the main properties of matrix approximations used in biplot displays to be developed. It establishes orthogonal components of an analysis of variance, from which different contributions to approximations may be assessed. Particular attention is paid to approximations that share the same singular vectors, in which case the solution space is a convex cone. Two- and three-dimensional approximations are examined in detail and then the geometry is interpreted for different forms of the matrix being approximated.},
 author = {J. C. Gower},
 journal = {Biometrika},
 number = {3},
 pages = {705--714},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Geometry of Biplot Scaling},
 urldate = {2024-08-12},
 volume = {91},
 year = {2004}
}
@article{Spearman1904,
    author          = {Spearman, Charles},
    title           = {'General intelligence,' objectively determined and measured},
    year            = {1904},
    journal         = {The American Journal of Psychology},
    volume          = {15},
    number          = {2},
    pages           = {201-293},
	url             = {https://psycnet.apa.org/doi/10.2307/1412107},
}
@book{豊田秀樹1992,
	author         = {豊田秀樹},
	year           = {1992},
	title          = {SASによる共分散構造分析 },
	series         = {ＳＡＳで学ぶ統計的データ解析},
	volume         = {3},
	edition        = {},
	url            = {https://www.utp.or.jp/book/b302422.html},
	publisher      = {東京大学出版会}
}

@article{Muthen2002,
	abstract = {This article gives an overview of statistical analysis with latent variables. Using traditional structural equation modeling as a starting point, it shows how the idea of latent variables captures a wide variety of statistical concepts, including random effects, missing data, sources of variation in hierarchical data, finite mixtures. latent classes, and clusters. These latent variable applications go beyond the traditional latent variable useage in psychometrics with its focus on measurement error and hypothetical constructs measured by multiple indicators. The article argues for the value of integrating statistical and psychometric modeling ideas. Different applications are discussed in a unifying framework that brings together in one general model such different analysis types as factor models, growth curve models, multilevel models, latent class models and discrete-time survival models. Several possible combinations and extensions of these models are made clear due to the unifying framework.},
	author = {Muth{\'e}n, Bengt O. },
	date = {2002/01/01},
	date-added = {2024-08-13 12:06:06 +0900},
	date-modified = {2024-08-13 12:06:06 +0900},
	doi = {10.2333/bhmk.29.81},
	id = {Muth{\'e}n2002},
	isbn = {1349-6964},
	journal = {Behaviormetrika},
	number = {1},
	pages = {81--117},
	title = {{Beyond SEM: General Latent Variable Modeling}},
	url = {https://doi.org/10.2333/bhmk.29.81},
	volume = {29},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.2333/bhmk.29.81}}

@article{狩野裕2002,
	author = {狩野裕},
	doi = {10.2333/jbhmk.29.138},
	journal = {行動計量学},
	number = {2},
	pages = {138-159},
	title = {構造方程式モデリングは，因子分析，分散分析，パス解析の すべてにとって代わるのか？},
	volume = {29},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.29.138}}

@article{狩野裕2019,
	author = {狩野裕},
	doi = {10.11329/jjssj.48.199},
	journal = {日本統計学会誌},
	number = {2},
	pages = {199-214},
	title = {欠測データ解析のmissとmyth},
	volume = {48},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.11329/jjssj.48.199}}


@phdthesis{Socan2003,
	author      = {G. Socan},
	school      = {Rijksuniversiteit Groningen },
	title       = {The Incremental Value of Rank Factor Analysis},
	year        = {2003}
}

@article{足立浩平+2019,
	author = {足立浩平 and 伊藤真道 and 宇野光平},
	doi = {10.20551/jscswabun.32.1_61},
	journal = {計算機統計学},
	number = {1},
	pages = {61-77},
	title = {行列分解に基づく因子分析とその新展開},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.20551/jscswabun.32.1_61}}
@article{Lawley1942, title={XIV.—Further Investigations in Factor Estimation}, volume={61}, DOI={10.1017/S0080454100006178}, number={2}, journal={Proceedings of the Royal Society of Edinburgh. Section A. Mathematical and Physical Sciences}, author={Lawley, D. N.}, year={1942}, pages={176–185}}
@inproceedings{Anderson-Rubin1956,
	author          = {T. W. Anderson and Herman Rubin},
	year            = {1956},
	title           = {Statistical Inference in Factor Analysis},
	booktitle       = {Proceedings of the Thrid Berkeley Symposium on Mathematical Statistics and Probability},
	volume          = {5},
	pages           = {111-150},
	url             = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Statistical-Inference-in-Factor-Analysis/bsmsp/1200511860}
}

@article{Harman-Jones1966,
	abstract = {This paper is addressed to the classical problem of estimating factor loadings under the condition that the sum of squares of off-diagonal residuals be minimized. Communalities consistent with this criterion are produced as a by-product. The experimental work included several alternative algorithms before a highly efficient method was developed. The final procedure is illustrated with a numerical example. Some relationships of minres to principal-factor analysis and maximum-likelihood factor estimates are discussed, and several unresolved problems are pointed out.},
	author = {Harman, Harry H. and Jones, Wayne H.},
	date = {1966/09/01},
	date-added = {2024-08-13 13:15:59 +0900},
	date-modified = {2024-08-13 13:15:59 +0900},
	doi = {10.1007/BF02289468},
	id = {Harman1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {351--368},
	title = {Factor analysis by minimizing residuals (minres)},
	url = {https://doi.org/10.1007/BF02289468},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289468}}

@article{Harman-Fukuda1966,
	abstract = {In the course of developing the minres method of factor analysis the troublesome situation of communalities greater than one arose. This problem---referred to as the generalized Heywood case---is resolved in this paper by means of a process of minimizing the sum of squares of off-diagonal residuals. The resulting solution is superior to the otherwise very efficient original minres method without requiring additional computing time.},
	author = {Harman, Harry H. and Fukuda, Yoichiro},
	date = {1966/12/01},
	date-added = {2024-08-13 13:19:47 +0900},
	date-modified = {2024-08-13 13:19:47 +0900},
	doi = {10.1007/BF02289525},
	id = {Harman1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {563--571},
	title = {Resolution of the heywood case in the minres solution},
	url = {https://doi.org/10.1007/BF02289525},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289525}}

@article{Rubin-Thayer1982,
	abstract = {The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq ×q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation.},
	author = {Rubin, Donald B. and Thayer, Dorothy T.},
	date = {1982/03/01},
	date-added = {2024-08-13 13:21:17 +0900},
	date-modified = {2024-08-13 13:21:17 +0900},
	doi = {10.1007/BF02293851},
	id = {Rubin1982},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {69--76},
	title = {EM algorithms for ML factor analysis},
	url = {https://doi.org/10.1007/BF02293851},
	volume = {47},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293851}}
@Inbook{Jöreskog1988,
author="J{\"o}reskog, Karl G.",
editor="Nesselroade, John R.
and Cattell, Raymond B.",
title="Analysis of Covariance Structures",
bookTitle="Handbook of Multivariate Experimental Psychology",
year="1988",
publisher="Springer US",
address="Boston, MA",
pages="207--230",
abstract="Analysis of covariance structures is the common term for a number of techniques for analyzing multivariate data in order to detect and assess latent (unobserved) sources of variation and covariation in the observed measurements. The techniques of covariance structure analysis are general and flexible in that they can handle many types of covariance structures useful especially in the behavioral and social sciences. Although these techniques can be used for exploratory analysis, they have been most successfully applied to confirmatory analysis where the type of covariance structure is specified in advance. A covariance structure of a specified kind may arise because of a specified substantive theory or hypothesis, a given classificatory design for the measures, known experimental conditions, or because of results from previous studies based on extensive data. Sometimes the observed variables are ordered through time, as in longitudinal studies, or according to linear or circular patterns, as in Guttman's (1954) simplex and circumplex models, or according to a given causal scheme, as in path analysis.",
isbn="978-1-4613-0893-5",
doi="10.1007/978-1-4613-0893-5_5",
url="https://doi.org/10.1007/978-1-4613-0893-5_5"
}


@article{Bock-Bargmann1966,
	abstract = {A general method is presented for estimating variance components when the experimental design has one random way of classification and a possibly unbalanced fixed classification. The procedure operates on a sample covariance matrix in which the fixed classes play the role of variables and the random classes correspond to observations. Cases are considered which assume (i) homogeneous and (ii) nonhomogeneous error variance, and (iii) arbitrary scale factors in the measurements and homogeneous error variance. The results include maximum-likelihood estimations of the variance components and scale factors, likelihood-ratio tests of the goodness-of-fit of the model assumed for the design, and large-sample variances and covariances of the estimates. Applications to mental test data are presented. In these applications the subjects constitute the random dimension of the design, and a classification of the mental tests according to objective features of format or content constitute the fixed dimensions.},
	author = {Bock, R. Darrell and Bargmann, Rolf E.},
	date = {1966/12/01},
	date-added = {2024-08-13 13:32:44 +0900},
	date-modified = {2024-08-13 13:32:44 +0900},
	doi = {10.1007/BF02289521},
	id = {Bock1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {507--534},
	title = {Analysis of covariance structures},
	url = {https://doi.org/10.1007/BF02289521},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289521}}
@article{Joreskog1970,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334833},
 abstract = {It is assumed that observations on a set of variables have a multivariate normal distribution with a general parametric form of the mean vector and the variance-covariance matrix. Any parameter of the model may be fixed, free or constrained to be equal to other parameters. The free and constrained parameters are estimated by maximum likelihood. A wide range of models is obtained from the general model by imposing various specifications on the parametric structure of the general model. Examples are given of areas and problems, especially in the behavioural sciences, where the method may be useful.},
 author = {K. G. Jöreskog},
 journal = {Biometrika},
 number = {2},
 pages = {239--251},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A General Method for Analysis of Covariance Structures},
 urldate = {2024-08-13},
 volume = {57},
 year = {1970}
}

@article{Joreskog1978,
	abstract = {A general approach to the analysis of covariance structures is considered, in which the variances and covariances or correlations of the observed variables are directly expressed in terms of the parameters of interest. The statistical problems of identification, estimation and testing of such covariance or correlation structures are discussed.},
	author = {J{\"o}reskog, Karl G. },
	date = {1978/12/01},
	date-added = {2024-08-13 13:34:21 +0900},
	date-modified = {2024-08-13 13:34:21 +0900},
	doi = {10.1007/BF02293808},
	id = {J{\"o}reskog1978},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--477},
	title = {Structural analysis of covariance and correlation matrices},
	url = {https://doi.org/10.1007/BF02293808},
	volume = {43},
	year = {1978},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293808}}
@techreport{Fornell1985,
	author      = {Claes Fornell},
	institution = {Business, Stephen M. Ross School, University of Michigan},
	title       = {A Second generation of multivariate analysis: classification of methods and implications for marketing research},
	year        = {1985},
	url             = {https://hdl.handle.net/2027.42/35621},
}

@article{豊田秀樹1991,
	author = {豊田秀樹},
	doi = {10.5926/jjep1953.39.4_467},
	journal = {教育心理学研究},
	number = {4},
	pages = {467-478},
	title = {共分散構造分析の下位モデルとその適用例},
	volume = {39},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.5926/jjep1953.39.4_467}}
@inbook{Joreskog-Wold1982,
	author         = {K. G. Jöreskog and H. Wold},
	chapter        = {The ML and PLS Techniques for Modeling with Latent Variables: Historical and Comparative Aspects},
	editor         = {},
	pages          = {263-270},
	publisher      = {North-Holland},
	title          = {Systems under Indirect Observation: Causality, Structure, Prediction},
	year           = {1982}
}

@article{星野崇宏+2005,
	author = {星野崇宏 and 岡田謙介 and 前田忠彦},
	doi = {10.2333/jbhmk.32.209},
	journal = {行動計量学},
	number = {2},
	pages = {209-235},
	title = {構造方程式モデリングにおける適合度指標とモデル改善について : 展望とシミュレーション研究による新たな知見},
	volume = {32},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.32.209}}

@article{Joreskog1967a,
	abstract = {A new computational method for the maximum likelihood solution in factor analysis is presented. This method takes into account the fact that the likelihood function may not have a maximum in a point of the parameter space where all unique variances are positive. Instead, the maximum may be attained on the boundary of the parameter space where one or more of the unique variances are zero. It is demonstrated that suchimproper (Heywood) solutions occur more often than is usually expected. A general procedure to deal with such improper solutions is proposed. The proposed methods are illustrated using two small sets of empirical data, and results obtained from the analyses of many other sets of data are reported. These analyses verify that the new computational method converges rapidly and that the maximum likelihood solution can be determined very accurately. A by-product obtained by the method is a large sample estimate of the variance-covariance matrix of the estimated unique variances. This can be used to set up approximate confidence intervals for communalities and unique variances.},
	author = {J{\"o}reskog, K.  G. },
	date = {1967/12/01},
	date-added = {2024-08-13 14:29:19 +0900},
	date-modified = {2024-08-13 14:29:19 +0900},
	doi = {10.1007/BF02289658},
	id = {J{\"o}reskog1967},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--482},
	title = {Some contributions to maximum likelihood factor analysis},
	url = {https://doi.org/10.1007/BF02289658},
	volume = {32},
	year = {1967},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289658}}
@techreport{Joreskog1966,
	author      = {Karl G. Jöreskog},
	institution = {ETS},
	title       = {UMLFA: A Computer Program for Unrestricted Maximum Likelihood Factor Analysis},
	year        = {1966},
	url             = {https://www.ets.org/research/policy_research_reports/publications/report/1966/iazh.html},
}
@book{Grimm-Yarnold2016,
    author         = {Grimm, Laurence G. and Yarnold, Paul R.},
    title          = {研究論文を読み解くための多変量解析入門 応用篇},
    year           = {2016},
    publisher      = {北大路書房},
    series         = {},
    volume         = {},
    month          = {9},
    edition        = {},
    howpublished   = {Reading and Understanding MORE Multivariate Statistics (2020) の翻訳書}
}

@article{Sorbom1974,
	abstract = {A statistical model is developed for the study of similarities and differences in factor structure between several groups. The model assumes that the observed variables satisfy a factor analysis model in each group. A method of data analysis is presented which, in contrast to earlier work, makes use of information in the observed means as well as the observed variances and covariances to estimate the parameters in each group, i.e. factor means, factor loadings, factor variances and covariances and unique variances. Usually the units of measurement in the observed variables have no intrinsic meaning and therefore it is only meaningful to compare the relative magnitudes of the parameters for the different groups. The method estimates the parameters for all groups simultaneously and can take into account a priori information about factorial invariance of various degrees.},
	author = {S{\"o}rbom, Dag},
	doi = {https://doi.org/10.1111/j.2044-8317.1974.tb00543.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1974.tb00543.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {2},
	pages = {229-239},
	title = {{A General Method for Studying Differences in Factor Means and Factor Structure between Groups}},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1974.tb00543.x},
	volume = {27},
	year = {1974},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1974.tb00543.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.1974.tb00543.x}}
@article{清水和秋1994,
	author          = {清水和秋},
	year            = {1994},
	title           = {JöreskogとSörbomによるコンピュータ・プログラムと構造方程式モデル},
	journal         = {関西大学社会学部紀要},
	volume          = {25},
	number          = {3},
	pages           = {1-41},
	url             = {http://hdl.handle.net/10112/13345}
}
@article{清水和秋1989,
	author          = {清水和秋},
	year            = {1989},
	title           = {検証的因子分析，LISRELそしてRAMの概要},
	journal         = {関西大学社会学部紀要},
	volume          = {20},
	number          = {2},
	pages           = {61-86},
	url             = {http://hdl.handle.net/10112/13348}
}

@article{McArdle1984,
	author = {J. Jack McArdle},
	doi = {10.1080/00273171.1984.9676927},
	eprint = {https://doi.org/10.1080/00273171.1984.9676927},
	journal = {Multivariate Behavioral Research},
	note = {PMID: 26781893},
	number = {2-3},
	pages = {245--267},
	publisher = {Routledge},
	title = {On the Madness in His Method: R. B. Cattell's Contributions to Structural Equation Modeling},
	url = {https://doi.org/10.1080/00273171.1984.9676927},
	volume = {19},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1080/00273171.1984.9676927}}

@article{Bentler1980,
	author = {Bentler, P M},
	doi = {https://doi.org/10.1146/annurev.ps.31.020180.002223},
	issn = {1545-2085},
	journal = {Annual Review of Psychology},
	number = {Volume 31, 1980},
	pages = {419-456},
	publisher = {Annual Reviews},
	title = {Multivariate Analysis with Latent Variables: Causal Modeling},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.ps.31.020180.002223},
	volume = {31},
	year = {1980},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev.ps.31.020180.002223},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.ps.31.020180.002223}}
@article{白倉幸男1984,
	author          = {白倉幸男},
	year            = {1984},
	title           = {多重指標線形構造モデルとその応用 : 研究ノート },
	journal         = {大阪大学人間科学部紀要},
	volume          = {10},
	number          = {},
	pages           = {25-45},
	url             = {https://doi.org/10.18910/4301}
}
@techreport{Ghahramani-Hinton1996,
	author      = {Zoubin Ghahramani and Geoffrey E. Hinton},
	institution = {Department of Computer Science, University of Toronto},
	title       = {The EM Algorithm for Mixtures of Factor Analyzers},
	year        = {1996},
	url             = {https://www.cs.toronto.edu/~hinton/absps/tr96-1.html},
}

@article{Bernaards-Jennrich2003,
	abstract = {A loading matrix has perfect simple structure if each row has at most one nonzero element. It is shown that if there is an orthogonal rotation of an initial loading matrix that has perfect simple structure, then orthomax rotation with 0 ≤γ≤1 of the initial loading matrix will produce the perfect simple structure. In particular, varimax and quartimax will produce rotations with perfect simple structure whenever they exist.},
	author = {Bernaards, Coen A. and Jennrich, Robert I.},
	date = {2003/12/01},
	date-added = {2024-08-13 15:52:15 +0900},
	date-modified = {2024-08-13 15:52:15 +0900},
	doi = {10.1007/BF02295613},
	id = {Bernaards2003},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {585--588},
	title = {Orthomax rotation and perfect simple structure},
	url = {https://doi.org/10.1007/BF02295613},
	volume = {68},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1007/BF02295613}}
@article{Zou+2006,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/27594179},
 abstract = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.},
 author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {265--286},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Sparse Principal Component Analysis},
 urldate = {2024-08-13},
 volume = {15},
 year = {2006}
}

@article{Jolliffe+2003,
	author = {Ian T Jolliffe, Nickolay T Trendafilov and Mudassir Uddin},
	doi = {10.1198/1061860032148},
	eprint = {https://doi.org/10.1198/1061860032148},
	journal = {Journal of Computational and Graphical Statistics},
	number = {3},
	pages = {531--547},
	publisher = {Taylor \& Francis},
	title = {A Modified Principal Component Technique Based on the LASSO},
	url = {https://doi.org/10.1198/1061860032148},
	volume = {12},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1198/1061860032148}}

@inproceedings{Bishop1998,
	author = {Bishop, Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Kearns and S. Solla and D. Cohn},
	publisher = {MIT Press},
	title = {Bayesian PCA},
	url = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf},
	volume = {11},
	year = {1998},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf}}
@article{Rattray+2009,
doi = {10.1088/1742-6596/197/1/012002},
url = {https://dx.doi.org/10.1088/1742-6596/197/1/012002},
year = {2009},
month = {dec},
publisher = {},
volume = {197},
number = {1},
pages = {012002},
author = {Magnus Rattray and  Oliver Stegle and  Kevin Sharp and  John Winn},
title = {Inference algorithms and learning theory for Bayesian sparse factor analysis},
journal = {Journal of Physics: Conference Series},
abstract = {Bayesian sparse factor analysis has many applications; for example, it has been applied to the problem of inferring a sparse regulatory network from gene expression data. We describe a number of inference algorithms for Bayesian sparse factor analysis using a slab and spike mixture prior. These include well-established Markov chain Monte Carlo (MCMC) and variational Bayes (VB) algorithms as well as a novel hybrid of VB and Expectation Propagation (EP). For the case of a single latent factor we derive a theory for learning performance using the replica method. We compare the MCMC and VB/EP algorithm results with simulated data to the theoretical prediction. The results for MCMC agree closely with the theory as expected. Results for VB/EP are slightly sub-optimal but show that the new algorithm is effective for sparse inference. In large-scale problems MCMC is infeasible due to computational limitations and the VB/EP algorithm then provides a very useful computationally efficient alternative.}
}

@inproceedings{Archambeau-Bach2008,
	author = {Archambeau, C\'{e}dric and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Sparse probabilistic projections},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf}}

@inproceedings{Collins+2001,
	author = {Collins, Michael and Dasgupta, S. and Schapire, Robert E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {A Generalization of Principal Components Analysis to the Exponential Family},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf}}

@article{Joreskog-Lawley1968,
	abstract = {Until recently the main difficulty in the use of maximum-likelihood estimation in factor analysis has been the lack of satisfactory methods of obtaining numerical solutions. This defect has now been remedied, and this paper describes new rapid methods of finding maximum-likelihood estimates.},
	author = {J{\"o}reskog, K. G. and Lawley, D. N.},
	doi = {https://doi.org/10.1111/j.2044-8317.1968.tb00399.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1968.tb00399.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {1},
	pages = {85-96},
	title = {New methods in maximum likelihood factor analysis},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1968.tb00399.x},
	volume = {21},
	year = {1968},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1968.tb00399.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.1968.tb00399.x}}

@article{Joreskog-vanThillo1972,
	author = {J{\H o}reskog, Karl G. and van Thiilo, Marielle},
	doi = {https://doi.org/10.1002/j.2333-8504.1972.tb00827.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1972.tb00827.x},
	journal = {ETS Research Bulletin Series},
	number = {2},
	pages = {i-71},
	title = {{LISREL: A General Computer Program for Estimating a Linear Structural Equation System Involving Multiple Indicators of Unmeasured Variables}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1972.tb00827.x},
	volume = {1972},
	year = {1972},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1972.tb00827.x},
	bdsk-url-2 = {https://doi.org/10.1002/j.2333-8504.1972.tb00827.x}}
@inproceedings{Yu+2006,
author = {Yu, Shipeng and Yu, Kai and Tresp, Volker and Kriegel, Hans-Peter and Wu, Mingrui},
title = {Supervised probabilistic principal component analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150454},
doi = {10.1145/1150402.1150454},
abstract = {Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {464–473},
numpages = {10},
keywords = {supervised projection, semi-supervised projection, principal component analysis, dimensionality reduction},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@article{Gustafsson2001,
	annote = {doi: 10.1021/ci0003909},
	author = {Gustafsson, Mats G. },
	date = {2001/03/01},
	date-added = {2024-08-13 16:43:51 +0900},
	date-modified = {2024-08-13 16:43:51 +0900},
	doi = {10.1021/ci0003909},
	isbn = {0095-2338},
	journal = {Journal of Chemical Information and Computer Sciences},
	journal1 = {Journal of Chemical Information and Computer Sciences},
	journal2 = {J. Chem. Inf. Comput. Sci.},
	month = {03},
	number = {2},
	pages = {288--294},
	publisher = {American Chemical Society},
	title = {A Probabilistic Derivation of the Partial Least-Squares Algorithm},
	type = {doi: 10.1021/ci0003909},
	url = {https://doi.org/10.1021/ci0003909},
	volume = {41},
	year = {2001},
	year1 = {2001},
	bdsk-url-1 = {https://doi.org/10.1021/ci0003909}}
@techreport{Bach-Jordan2005,
	author      = {Francis R. Bach and Michael I. Jordan},
	institution = {University of California, Berkeley},
	title       = {A Probabilistic Interpretation of Canonical Correlation Analysis},
	year        = {2005},
	url             = {https://statistics.berkeley.edu/tech-reports/688},
}

@article{Nounou+2002,
	abstract = {Abstract Large quantities of measured data are being routinely collected in various industries and used for extracting linear models for tasks such as process control, fault diagnosis, and process monitoring. Existing linear modeling methods, however, do not fully utilize all the information contained in the measurements. A new approach for linear process modeling makes maximum use of available process data and process knowledge. Bayesian latent-variable regression (BLVR) permits extraction and incorporation of knowledge about the statistical behavior of measurements in developing linear process models. Furthermore, BLVR can handle noise in inputs and outputs, collinear variables, and incorporate prior knowledge about regression parameters and measured variables. The model is usually more accurate than those of existing methods, including OLS, PCR, and PLS. BLVR considers a univariate output and assumes the underlying variables and noise to be Gaussian, but it can be used for multivariate outputs and other distributions. An empirical Bayes approach is developed to extract the prior information from historical data or maximum-likelihood solution of available data. Examples of steady-state, dynamic and inferential modeling demonstrate the superior accuracy of BLVR over existing methods even when the assumptions of Gaussian distributions are violated. The relationship between BLVR and existing methods and opportunities for future work based on this framework are also discussed.},
	author = {Nounou, Mohamed N. and Bakshi, Bhavik R. and Goel, Prem K. and Shen, Xiaotong},
	doi = {https://doi.org/10.1002/aic.690480818},
	eprint = {https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690480818},
	journal = {AIChE Journal},
	number = {8},
	pages = {1775-1793},
	title = {Process modeling by Bayesian latent variable regression},
	url = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690480818},
	volume = {48},
	year = {2002},
	bdsk-url-1 = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690480818},
	bdsk-url-2 = {https://doi.org/10.1002/aic.690480818}}
@article{Horst1961,
author = {Horst, Paul},
title = {Generalized canonical correlations and their applications to experimental data},
journal = {Journal of Clinical Psychology},
volume = {17},
number = {4},
pages = {331-347},
doi = {https://doi.org/10.1002/1097-4679(196110)17:4<331::AID-JCLP2270170402>3.0.CO;2-D},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28196110%2917%3A4%3C331%3A%3AAID-JCLP2270170402%3E3.0.CO%3B2-D},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28196110%2917%3A4%3C331%3A%3AAID-JCLP2270170402%3E3.0.CO%3B2-D},
year = {1961}
}

@inproceedings{Klami+2010,
author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
title = {Bayesian exponential family projections for coupled data sources},
year = {2010},
isbn = {9780974903965},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Exponential family extensions of principal component analysis (EPCA) have received a considerable amount of attention in recent years, demonstrating the growing need for basic modeling tools that do not assume the squared loss or Gaussian distribution. We extend the EPCA model toolbox by presenting the first exponential family multi-view learning methods of the partial least squares and canonical correlation analysis, based on a unified representation of EPCA as matrix factorization of the natural parameters of exponential family. The models are based on a new family of priors that are generally usable for all such factorizations. We also introduce new inference strategies, and demonstrate how the methods outperform earlier ones when the Gaussianity assumption does not hold.},
booktitle = {Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
pages = {286–293},
numpages = {8},
location = {Catalina Island, CA},
series = {UAI'10}
}
@misc{Wang+2017,
title={Deep Variational Canonical Correlation Analysis},
author={Weiran Wang and Xinchen Yan and Honglak Lee and Karen Livescu},
year={2017},
url={https://openreview.net/forum?id=H1Heentlx}
}
@misc{Suzuki+2017,
title={Joint Multimodal Learning with Deep Generative Models},
author={Masahiro Suzuki and Kotaro Nakayama and Yutaka Matsuo},
year={2017},
url={https://openreview.net/forum?id=Hk8rlUqge}
}
@inproceedings{Sun+2009,
author = {Sun, Liang and Ji, Shuiwang and Yu, Shipeng and Ye, Jieping},
title = {On the equivalence between canonical correlation analysis and orthonormalized partial least squares},
year = {2009},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Canonical correlation analysis (CCA) and partial least squares (PLS) are well-known techniques for feature extraction from two sets of multi-dimensional variables. The fundamental difference between CCA and PLS is that CCA maximizes the correlation while PLS maximizes the covariance. Although both CCA and PLS have been applied successfully in various applications, the intrinsic relationship between them remains unclear. In this paper, we attempt to address this issue by showing the equivalence relationship between CCA and orthonormalized partial least squares (OPLS), a variant of PLS. We further extend the equivalence relationship to the case when regularization is employed for both sets of variables. In addition, we show that the CCA projection for one set of variables is independent of the regularization on the other set of variables. We have performed experimental studies using both synthetic and real data sets and our results confirm the established equivalence relationship. The presented analysis provides novel insights into the connection between these two existing algorithms as well as the effect of the regularization.},
booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence},
pages = {1230–1235},
numpages = {6},
location = {Pasadena, California, USA},
series = {IJCAI'09}
}

@InProceedings{Hoffman2017,
  title = 	 {Learning Deep Latent {G}aussian Models with {M}arkov Chain {M}onte {C}arlo},
  author =       {Matthew D. Hoffman},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1510--1519},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/hoffman17a.html},
  abstract = 	 {Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC’s additional computational overhead proves to be significant, but not prohibitive.}
}
@inproceedings{Canny2004,
author = {Canny, John},
title = {GaP: a factor model for discrete data},
year = {2004},
isbn = {1581138814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008992.1009016},
doi = {10.1145/1008992.1009016},
abstract = {We present a probabilistic model for a document corpus that combines many of the desirable features of previous models. The model is called "GaP" for Gamma-Poisson, the distributions of the first and last random variable. GaP is a factor model, that is it gives an approximate factorization of the document-term matrix into a product of matrices Λ and X. These factors have strictly non-negative terms. GaP is a generative probabilistic model that assigns finite probabilities to documents in a corpus. It can be computed with an efficient and simple EM recurrence. For a suitable choice of parameters, the GaP factorization maximizes independence between the factors. So it can be used as an independent-component algorithm adapted to document data. The form of the GaP model is empirically as well as analytically motivated. It gives very accurate results as a probabilistic model (measured via perplexity) and as a retrieval model. The GaP model projects documents and terms into a low-dimensional space of "themes," and models texts as "passages" of terms on the same theme.},
booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {122–129},
numpages = {8},
keywords = {probabilistic models, latent semantic analysis, EM algorithm},
location = {Sheffield, United Kingdom},
series = {SIGIR '04}
}

@article{Paatero-Tapper1994,
	abstract = {Abstract A new variant `PMF' of factor analysis is described. It is assumed that X is a matrix of observed data and σ is the known matrix of standard deviations of elements of X. Both X and σ are of dimensions n × m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n × p, F is the unknown right hand factor matrix (loadings) of dimensions p × m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by σ is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
	author = {Paatero, Pentti and Tapper, Unto},
	doi = {https://doi.org/10.1002/env.3170050203},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.3170050203},
	journal = {Environmetrics},
	keywords = {Factor analysis, Principal component analysis, Weighted least squares, Alternating regression, Error estimates, Scaling, Repetitive measurements},
	number = {2},
	pages = {111-126},
	title = {Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	volume = {5},
	year = {1994},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	bdsk-url-2 = {https://doi.org/10.1002/env.3170050203}}

@inproceedings{Buntine-Jakulin2006,
	abstract = {This article presents a unified theory for analysis of components in discrete data, and compares the methods with techniques such as independent component analysis, non-negative matrix factorisation and latent Dirichlet allocation. The main families of algorithms discussed are a variational approximation, Gibbs sampling, and Rao-Blackwellised Gibbs sampling. Applications are presented for voting records from the United States Senate for 2003, and for the Reuters-21578 newswire collection.},
	address = {Berlin, Heidelberg},
	author = {Buntine, Wray and Jakulin, Aleks},
	booktitle = {Subspace, Latent Structure and Feature Selection},
	editor = {Saunders, Craig and Grobelnik, Marko and Gunn, Steve and Shawe-Taylor, John},
	isbn = {978-3-540-34138-3},
	pages = {1--33},
	publisher = {Springer Berlin Heidelberg},
	title = {Discrete Component Analysis},
	year = {2006}}

@article{Bhattacharya-Dunson2012,
	author = {Anirban Bhattacharya and David B. Dunson},
	doi = {10.1080/01621459.2011.646934},
	eprint = {https://doi.org/10.1080/01621459.2011.646934},
	journal = {Journal of the American Statistical Association},
	note = {PMID: 23908561},
	number = {497},
	pages = {362--377},
	publisher = {Taylor \& Francis},
	title = {Simplex Factor Models for Multivariate Unordered Categorical Data},
	url = {https://doi.org/10.1080/01621459.2011.646934},
	volume = {107},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2011.646934}}

@article{Erosheva+2004,
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	author = {Elena Erosheva and Stephen Fienberg and John Lafferty},
	doi = {10.1073/pnas.0307760101},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0307760101},
	journal = {Proceedings of the National Academy of Sciences},
	number = {suppl\_1},
	pages = {5220-5227},
	title = {Mixed-membership models of scientific publications},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0307760101},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0307760101},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0307760101}}
@article{Pritchard+2000,
    author = {Pritchard, Jonathan K and Stephens, Matthew and Donnelly, Peter},
    title = "{Inference of Population Structure Using Multilocus Genotype Data}",
    journal = {Genetics},
    volume = {155},
    number = {2},
    pages = {945-959},
    year = {2000},
    month = {06},
    abstract = "{We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci—e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/~pritch/home.html.}",
    issn = {1943-2631},
    doi = {10.1093/genetics/155.2.945},
    url = {https://doi.org/10.1093/genetics/155.2.945},
    eprint = {https://academic.oup.com/genetics/article-pdf/155/2/945/42030266/genetics0945.pdf},
}


@inproceedings{Marlin2003,
	author = {Marlin, Benjamin M},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
	publisher = {MIT Press},
	title = {Modeling User Rating Profiles For Collaborative Filtering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf},
	volume = {16},
	year = {2003},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf}}


@article{Hyvarinen-Oja2000,
	abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
	author = {A. Hyv{\"a}rinen and E. Oja},
	doi = {https://doi.org/10.1016/S0893-6080(00)00026-5},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Independent component analysis, Projection pursuit, Blind signal separation, Source separation, Factor analysis, Representation},
	number = {4},
	pages = {411-430},
	title = {Independent component analysis: algorithms and applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	volume = {13},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	bdsk-url-2 = {https://doi.org/10.1016/S0893-6080(00)00026-5}}
@ARTICLE{Friedman-Tukey1974,
  author={Friedman, J.H. and Tukey, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Projection Pursuit Algorithm for Exploratory Data Analysis}, 
  year={1974},
  volume={C-23},
  number={9},
  pages={881-890},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric pattern recognition, statistics.},
  doi={10.1109/T-C.1974.224051}}

@inproceedings{Ricahrdson-Weiss2018,
	author = {Richardson, Eitan and Weiss, Yair},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On GANs and GMMs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf}}
@inproceedings{Zong+2018,
title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},
author={Bo Zong and Qi Song and Martin Renqiang Min and Wei Cheng and Cristian Lumezanu and Daeki Cho and Haifeng Chen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJJLHbb0-},
}
@inproceedings{Paisley-Carin2009,
author = {Paisley, John and Carin, Lawrence},
title = {Nonparametric factor analysis with beta process priors},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553474},
doi = {10.1145/1553374.1553474},
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {777–784},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}
@misc{Chen+2024,
      title={DiJiang: Efficient Large Language Models through Compact Kernelization}, 
      author={Hanting Chen and Zhicheng Liu and Xutao Wang and Yuchuan Tian and Yunhe Wang},
      year={2024},
      eprint={2403.19928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19928}, 
}
@INPROCEEDINGS{Zoran-Weiss2011,
  author={Zoran, Daniel and Weiss, Yair},
  booktitle={2011 International Conference on Computer Vision}, 
  title={From learning models of natural image patches to whole image restoration}, 
  year={2011},
  volume={},
  number={},
  pages={479-486},
  keywords={Image restoration;Noise reduction;Equations;Noise measurement;Image reconstruction;Mathematical model;Estimation},
  doi={10.1109/ICCV.2011.6126278}}
@ARTICLE{Papyam-Elad2016,
  author={Papyan, Vardan and Elad, Michael},
  journal={IEEE Transactions on Image Processing}, 
  title={Multi-Scale Patch-Based Image Restoration}, 
  year={2016},
  volume={25},
  number={1},
  pages={249-261},
  keywords={Noise reduction;Image restoration;Mathematical model;Closed-form solutions;Approximation methods;Context awareness;Image restoration;expected patch log likelihood (EPLL);Gaussian mixture model;multi-scale;denoising;deblurring;super-resolution;Image restoration;expected patch log likelihood (EPLL);Gaussian mixture model;multi-scale;denoising;deblurring;super-resolution},
  doi={10.1109/TIP.2015.2499698}}
@article{Deerwester+1990,
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
title = {Indexing by latent semantic analysis},
journal = {Journal of the American Society for Information Science},
volume = {41},
number = {6},
pages = {391-407},
doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
year = {1990}
}

@inproceedings{Hofmann1999,
author = {Hofmann, Thomas},
title = {Probabilistic latent semantic indexing},
year = {1999},
isbn = {1581130961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/312624.312649},
doi = {10.1145/312624.312649},
booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {50–57},
numpages = {8},
location = {Berkeley, California, USA},
series = {SIGIR '99}
}
@article{Blei+2003,
	author          = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	year            = {2003},
	title           = {{Latent Dirichlet Allocation}},
	journal         = {Journal of Machine Learning Research},
	volume          = {3},
	number          = {},
	pages           = {993-1022},
	url             = {https://www.jmlr.org/papers/v3/blei03a.html}
}
@article{Blei2012,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = {apr},
pages = {77–84},
numpages = {8}
}

@article{Church-Gale1991,
	abstract = {This paper describes a new program, CORRECT, which takes words rejected by the Unix{\textregistered}SPELL program, proposes a list of candidate corrections, and sorts them by probability score. The probability scores are the novel contribution of this work. They are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in recognition applications, especially speech recognition (Jelinek, 1985), one can often recover the intended correction,c, from a typo,t, by finding the correctionc that maximizesPr(c) Pr(t/c). The first factor,Pr(c), is a prior model of word probabilities; the second factor,Pr(t/c), is a model of the noisy channel that accounts for spelling transformations on letter sequences (insertions, deletions, substitutions and reversals). Both sets of probabilities were estimated using data collected from the Associated Press (AP) newswire over 1988 and 1989 as a training set. The AP generates about 1 million words and 500 typos per week.},
	author = {Church, Kenneth W. and Gale, William A.},
	date = {1991/12/01},
	date-added = {2024-08-13 22:12:23 +0900},
	date-modified = {2024-08-13 22:12:23 +0900},
	doi = {10.1007/BF01889984},
	id = {Church1991},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {2},
	pages = {93--103},
	title = {Probability scoring for spelling correction},
	url = {https://doi.org/10.1007/BF01889984},
	volume = {1},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1007/BF01889984}}

@InProceedings{Arova+2013,
  title = 	 {A Practical Algorithm for Topic Modeling with Provable Guarantees},
  author = 	 {Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {280--288},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/arora13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/arora13.html},
  abstract = 	 {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.}
}

@article{Griffith-Steyvers2004,
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \&amp; Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	author = {Thomas L. Griffiths and Mark Steyvers},
	doi = {10.1073/pnas.0307752101},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0307752101},
	journal = {Proceedings of the National Academy of Sciences},
	number = {suppl\_1},
	pages = {5228-5235},
	title = {Finding scientific topics},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0307752101}}
@inproceedings{Blei+2006,
author = {Blei, David M. and Lafferty, John D.},
title = {Dynamic topic models},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143859},
doi = {10.1145/1143844.1143859},
abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {113–120},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@inproceedings{Griffiths+2004,
	author = {Griffiths, Thomas and Steyvers, Mark and Blei, David and Tenenbaum, Joshua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	publisher = {MIT Press},
	title = {Integrating Topics and Syntax},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf}}
@inproceedings{Dieng+2017,
title={Topic{RNN}: A Recurrent Neural Network with Long-Range Semantic Dependency},
author={Adji B. Dieng and Chong Wang and Jianfeng Gao and John Paisley},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJbbOLcex}
}
@article{Zou-Hastie2005,
    author = {Zou, Hui and Hastie, Trevor},
    title = "{Regularization and Variable Selection Via the Elastic Net}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {67},
    number = {2},
    pages = {301-320},
    year = {2005},
    month = {03},
    abstract = "{We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.}",
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2005.00503.x},
    url = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/67/2/301/49795094/jrsssb\_67\_2\_301.pdf},
}
@book{豊田秀樹2009,
    author         = {豊田秀樹},
    title          = {共分散構造分析［実践編］},
    year           = {2009},
    publisher      = {朝倉書店},
    series         = {統計ライブラリー},
    volume         = {},
    month          = {10},
    edition        = {},
    howpublished   = {}
}
@book{豊田秀樹2007,
    author         = {豊田秀樹},
    title          = {共分散構造分析［理論編］},
    year           = {2007},
    publisher      = {朝倉書店},
    series         = {統計ライブラリー},
    volume         = {},
    month          = {10},
    edition        = {},
    howpublished   = {}
}

@book{Herbert-Simon57-ModelsOfMan,
    author         = {Herbert Simon},
    title          = {Models of man; social and rational.},
    year           = {1957},
    publisher      = {Wiley},
    series         = {},
    volume         = {},
    month          = {},
    edition        = {},
    howpublished   = {}
}

@article{岩瀬-中山2016,
	author          = {岩瀬智亮 and 中山英樹},
	year            = {2016},
	title           = {深層一般化正準相関分析},
	journal         = {情報処理学会第78回全国大会講演論文集},
	volume          = {2016},
	number          = {1},
	pages           = {183-184},
	url             = {http://id.nii.ac.jp/1001/00162588/}
}

@InProceedings{Andrew+2013,
  title = 	 {Deep Canonical Correlation Analysis},
  author = 	 {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1247--1255},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/andrew13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/andrew13.html},
  abstract = 	 {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}
}

@article{赤穂昭太郎2013,
	author = {赤穂昭太郎},
	doi = {10.3902/jnns.20.62},
	journal = {日本神経回路学会誌},
	number = {2},
	pages = {62-72},
	title = {正準相関分析入門},
	volume = {20},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.3902/jnns.20.62}}

@article{Sei-Yano2024,
	author = {Tomonari Sei and Keisuke Yano},
	doi = {10.3150/23-BEJ1687},
	journal = {Bernoulli},
	keywords = {conditional inference, copula, earthquake data, Graphical model, mixed-domain, Monte Carlo method},
	number = {4},
	pages = {2623 -- 2643},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	title = {{Minimum information dependence modeling}},
	url = {https://doi.org/10.3150/23-BEJ1687},
	volume = {30},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.3150/23-BEJ1687}}

@article{清智也2021,
	author = {清智也},
	doi = {10.11329/jjssj.51.75},
	journal = {日本統計学会誌},
	number = {1},
	pages = {75-99},
	title = {最小情報コピュラとその周辺},
	volume = {51},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.11329/jjssj.51.75}}

@article{Bedford+2016,
	abstract = {Many applications of risk analysis require us to jointly model multiple uncertain quantities. Bayesian networks and copulas are two common approaches to modeling joint uncertainties with probability distributions. This article focuses on new methodologies for copulas by developing work of Cooke, Bedford, Kurowica, and others on vines as a way of constructing higher dimensional distributions that do not suffer from some of the restrictions of alternatives such as the multivariate Gaussian copula. The article provides a fundamental approximation result, demonstrating that we can approximate any density as closely as we like using vines. It further operationalizes this result by showing how minimum information copulas can be used to provide parametric classes of copulas that have such good levels of approximation. We extend previous approaches using vines by considering nonconstant conditional dependencies, which are particularly relevant in financial risk modeling. We discuss how such models may be quantified, in terms of expert judgment or by fitting data, and illustrate the approach by modeling two financial data sets.},
	author = {Bedford, Tim and Daneshkhah, Alireza and Wilson, Kevin J.},
	doi = {https://doi.org/10.1111/risa.12471},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/risa.12471},
	journal = {Risk Analysis},
	keywords = {Copula, entropy, information, risk modeling, vine},
	number = {4},
	pages = {792-815},
	title = {Approximate Uncertainty Modeling in Risk Analysis with Vine Copulas},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12471},
	volume = {36},
	year = {2016},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12471},
	bdsk-url-2 = {https://doi.org/10.1111/risa.12471}}
@article{江口真透1999,
	author          = {江口真透},
	year            = {1999},
	title           = {概パラメトリック推測 － 柔らかなモデルの構築 －},
	journal         = {統計数理},
	volume          = {47},
	number          = {1},
	pages           = {29-48},
	url             = {http://hdl.handle.net/10787/295}
}

@article{飽戸弘1966,
	author = {飽戸弘},
	doi = {10.4992/jjpsy.37.204},
	journal = {心理学研究},
	number = {4},
	pages = {204-218},
	title = {政治的態度の構造に関する研究 I},
	volume = {37},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.4992/jjpsy.37.204}}

@article{Young-Hausholder1938,
	abstract = {Necessary and sufficient conditions are given for a set of numbers to be the mutual distances of a set of real points in Euclidean space, and matrices are found whose ranks determine the dimension of the smallest Euclidean space containing such points. Methods are indicated for determining the configuration of these points, and for approximating to them by points in a space of lower dimensionality.},
	author = {Young, Gale and Householder, A. S.},
	date = {1938/03/01},
	date-added = {2024-08-14 13:45:15 +0900},
	date-modified = {2024-08-14 13:45:15 +0900},
	doi = {10.1007/BF02287916},
	id = {Young1938},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {19--22},
	title = {Discussion of a set of points in terms of their mutual distances},
	url = {https://doi.org/10.1007/BF02287916},
	volume = {3},
	year = {1938},
	bdsk-url-1 = {https://doi.org/10.1007/BF02287916}}

@article{Unkel-Trendafilov2010,
	abstract = {Summary The classical exploratory factor analysis (EFA) finds estimates for the factor loadings matrix and the matrix of unique factor variances which give the best fit to the sample correlation matrix with respect to some goodness-of-fit criterion. Common factor scores can be obtained as a function of these estimates and the data. Alternatively to the classical EFA, the EFA model can be fitted directly to the data which yields factor loadings and common factor scores simultaneously. Recently, new algorithms were introduced for the simultaneous least squares estimation of all EFA model unknowns. The new methods are based on the numerical procedure for singular value decomposition of matrices and work equally well when the number of variables exceeds the number of observations. This paper provides an account that is intended as an expository review of methods for simultaneous parameter estimation in EFA. The methods are illustrated on Harman's five socio-economic variables data and a high-dimensional data set from genome research.},
	author = {Unkel, Steffen and Trendafilov, Nickolay T.},
	doi = {https://doi.org/10.1111/j.1751-5823.2010.00120.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2010.00120.x},
	journal = {International Statistical Review},
	keywords = {Factor analysis, indeterminacies, least squares estimation, matrix fitting problems, constrained optimization, principal component analysis, rotation},
	number = {3},
	pages = {363-382},
	title = {Simultaneous Parameter Estimation in Exploratory Factor Analysis: An Expository Review},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00120.x},
	volume = {78},
	year = {2010},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00120.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1751-5823.2010.00120.x}}
@misc{Ghojogh+2022,
      title={Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey}, 
      author={Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
      year={2022},
      eprint={2101.00734},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2101.00734}, 
}

@article{Kohonen1982,
	abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
	author = {Kohonen, Teuvo},
	date = {1982/01/01},
	date-added = {2024-08-14 14:28:55 +0900},
	date-modified = {2024-08-14 14:28:55 +0900},
	doi = {10.1007/BF00337288},
	id = {Kohonen1982},
	isbn = {1432-0770},
	journal = {Biological Cybernetics},
	number = {1},
	pages = {59--69},
	title = {Self-organized formation of topologically correct feature maps},
	url = {https://doi.org/10.1007/BF00337288},
	volume = {43},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF00337288}}

@article{Kohonen2013,
	abstract = {The self-organizing map (SOM) is an automatic data-analysis method. It is widely applied to clustering problems and data exploration in industry, finance, natural sciences, and linguistics. The most extensive applications, exemplified in this paper, can be found in the management of massive textual databases and in bioinformatics. The SOM is related to the classical vector quantization (VQ), which is used extensively in digital signal processing and transmission. Like in VQ, the SOM represents a distribution of input data items using a finite set of models. In the SOM, however, these models are automatically associated with the nodes of a regular (usually two-dimensional) grid in an orderly fashion such that more similar models become automatically associated with nodes that are adjacent in the grid, whereas less similar models are situated farther away from each other in the grid. This organization, a kind of similarity diagram of the models, makes it possible to obtain an insight into the topographic relationships of data, especially of high-dimensional data items. If the data items belong to certain predetermined classes, the models (and the nodes) can be calibrated according to these classes. An unknown input item is then classified according to that node, the model of which is most similar with it in some metric used in the construction of the SOM. A new finding introduced in this paper is that an input item can even more accurately be represented by a linear mixture of a few best-matching models. This becomes possible by a least-squares fitting procedure where the coefficients in the linear mixture of models are constrained to nonnegative values.},
	author = {Teuvo Kohonen},
	doi = {https://doi.org/10.1016/j.neunet.2012.09.018},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Self-organizing map, SOM, Data analysis, Brain map, Similarity, Vector quantization},
	note = {Twenty-fifth Anniversay Commemorative Issue},
	pages = {52-65},
	title = {Essentials of the self-organizing map},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	volume = {37},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2012.09.018}}
@article{Baum+1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239727},
 author = {Leonard E. Baum and Ted Petrie and George Soules and Norman Weiss},
 journal = {The Annals of Mathematical Statistics},
 number = {1},
 pages = {164--171},
 publisher = {Institute of Mathematical Statistics},
 title = {A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains},
 urldate = {2024-08-14},
 volume = {41},
 year = {1970}
}
@unpublished{Murphy-Linderman2022,
	author = {Kevin Murphy and Scott Linderman},
	year   = {2022},
	title  = {State Space Models: A Modern Approach},
	url    = {https://github.com/probml/ssm-book}
}

@article{Hsu+2012,
	abstract = {Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.},
	author = {Daniel Hsu and Sham M. Kakade and Tong Zhang},
	doi = {https://doi.org/10.1016/j.jcss.2011.12.025},
	issn = {0022-0000},
	journal = {Journal of Computer and System Sciences},
	keywords = {Hidden Markov Models, Latent variable models, Observable operator models, Time series, Spectral algorithm, Singular value decomposition, Learning probability distributions, Unsupervised learning},
	note = {JCSS Special Issue: Cloud Computing 2011},
	number = {5},
	pages = {1460-1480},
	title = {A spectral algorithm for learning Hidden Markov Models},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
	volume = {78},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcss.2011.12.025}}

@InProceedings{Anandkumar+2012,
  title = 	 {A Method of Moments for Mixture Models and Hidden Markov Models},
  author = 	 {Anandkumar, Animashree and Hsu, Daniel and Kakade, Sham M.},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {33.1--33.34},
  year = 	 {2012},
  editor = 	 {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/anandkumar12/anandkumar12.pdf},
  url = 	 {https://proceedings.mlr.press/v23/anandkumar12.html},
  abstract = 	 {Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (\emphe.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient \emphmethod of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment.}
}

@inproceedings{Anandkumar+2015,
	abstract = {This note is a short version of that in [1]. It is intended as a survey for the 2015 Algorithmic Learning Theory (ALT) conference.},
	address = {Cham},
	author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	booktitle = {Algorithmic Learning Theory},
	editor = {Chaudhuri, Kamalika and GENTILE, CLAUDIO and Zilles, Sandra},
	isbn = {978-3-319-24486-0},
	pages = {19--38},
	publisher = {Springer International Publishing},
	title = {Tensor Decompositions for Learning Latent Variable Models (A Survey for ALT)},
	year = {2015}}

@InProceedings{Obermeyer+2019,
  title = 	 {Tensor Variable Elimination for Plated Factor Graphs},
  author =       {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Pradhan, Neeraj and Chiu, Justin and Rush, Alexander and Goodman, Noah},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4871--4880},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/obermeyer19a/obermeyer19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/obermeyer19a.html},
  abstract = 	 {A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graphical models. To exploit efficient tensor algebra in graphs with plates of variables, we generalize undirected factor graphs to plated factor graphs and variable elimination to a tensor variable elimination algorithm that operates directly on plated factor graphs. Moreover, we generalize complexity bounds based on treewidth and characterize the class of plated factor graphs for which inference is tractable. As an application, we integrate tensor variable elimination into the Pyro probabilistic programming language to enable exact inference in discrete latent variable models with repeated structure. We validate our methods with experiments on both directed and undirected graphical models, including applications to polyphonic music modeling, animal movement modeling, and latent sentiment analysis.}
}

@article{Scott2002,
	author = {Steven L Scott},
	doi = {10.1198/016214502753479464},
	eprint = {https://doi.org/10.1198/016214502753479464},
	journal = {Journal of the American Statistical Association},
	number = {457},
	pages = {337--351},
	publisher = {Taylor \& Francis},
	title = {Bayesian Methods for Hidden Markov Models},
	url = {https://doi.org/10.1198/016214502753479464},
	volume = {97},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1198/016214502753479464}}

@inproceedings{Gu+2020,
	author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1474--1487},
	publisher = {Curran Associates, Inc.},
	title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf}}

@InProceedings{Goel+2022,
  title = 	 {It’s Raw! {A}udio Generation with State-Space Models},
  author =       {Goel, Karan and Gu, Albert and Donahue, Chris and Re, Christopher},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {7616--7633},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/goel22a/goel22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/goel22a.html},
  abstract = 	 {Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2{\texttimes} better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3{\texttimes} fewer parameters}
}
@unpublished{Oudot2016,
	author = {Steve Oudot},
	year   = {2016},
	title  = {Reeb Graph and Mapper},
	url    = {https://geometrica.saclay.inria.fr/team/Steve.Oudot/courses/EMA/Slides_Reeb_Mapper.pdf}
}
@unpublished{Schnider2024,
	author = {Patrick Schnider},
	year   = {2024},
	title  = {Introduction to Topological Data Analysi},
	url    = {https://ti.inf.ethz.ch/ew/courses/TDA24/index.html}
}
@inproceedings{Tang+2016,
author = {Tang, Jian and Liu, Jingzhou and Zhang, Ming and Mei, Qiaozhu},
title = {Visualizing Large-scale and High-dimensional Data},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883041},
doi = {10.1145/2872427.2883041},
abstract = {We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {287–297},
numpages = {11},
keywords = {visualization, high-dimensional data, big data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}
@inproceedings{DasGupta-Freud2008,
author = {Dasgupta, Sanjoy and Freund, Yoav},
title = {Random projection trees and low dimensional manifolds},
year = {2008},
isbn = {9781605580470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1374376.1374452},
doi = {10.1145/1374376.1374452},
abstract = {We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure.},
booktitle = {Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing},
pages = {537–546},
numpages = {10},
keywords = {curse of dimension, k-d tree, manifold, random projection},
location = {Victoria, British Columbia, Canada},
series = {STOC '08}
}

@article{Saul2020,
	abstract = {Latent variable models (LVMs) are powerful tools for discovering hidden structure in data. Canonical LVMs include factor analysis, which explains the correlation of a large number of observed variables in terms of a smaller number of unobserved ones, and Gaussian mixture models, which reveal clusters of data arising from an underlying multimodal distribution. In this paper, we describe a conceptually simple and equally effective LVM for nonlinear dimensionality reduction (NLDR), where the goal is to discover faithful, neighborhood-preserving embeddings of high-dimensional data. Tools for NLDR can help researchers across all areas of science and engineering to better understand and visualize their data. Our approach elevates NLDR into the family of problems that can be studied by especially tractable LVMs. We propose a latent variable model to discover faithful low-dimensional representations of high-dimensional data. The model computes a low-dimensional embedding that aims to preserve neighborhood relationships encoded by a sparse graph. The model both leverages and extends current leading approaches to this problem. Like t-distributed Stochastic Neighborhood Embedding, the model can produce two- and three-dimensional embeddings for visualization, but it can also learn higher-dimensional embeddings for other uses. Like LargeVis and Uniform Manifold Approximation and Projection, the model produces embeddings by balancing two goals---pulling nearby examples closer together and pushing distant examples further apart. Unlike these approaches, however, the latent variables in our model provide additional structure that can be exploited for learning. We derive an Expectation--Maximization procedure with closed-form updates that monotonically improve the model's likelihood: In this procedure, embeddings are iteratively adapted by solving sparse, diagonally dominant systems of linear equations that arise from a discrete graph Laplacian. For large problems, we also develop an approximate coarse-graining procedure that avoids the need for negative sampling of nonadjacent nodes in the graph. We demonstrate the model's effectiveness on datasets of images and text.},
	author = {Lawrence K. Saul},
	doi = {10.1073/pnas.1916012117},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1916012117},
	journal = {Proceedings of the National Academy of Sciences},
	number = {27},
	pages = {15403-15408},
	title = {A tractable latent variable model for nonlinear dimensionality reduction},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1916012117},
	volume = {117},
	year = {2020},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1916012117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1916012117}}

@article{Tutte1963,
	author = {Tutte, W. T.},
	doi = {https://doi.org/10.1112/plms/s3-13.1.743},
	eprint = {https://londmathsoc.onlinelibrary.wiley.com/doi/pdf/10.1112/plms/s3-13.1.743},
	journal = {Proceedings of the London Mathematical Society},
	number = {1},
	pages = {743-767},
	title = {{How to Draw a Graph}},
	url = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-13.1.743},
	volume = {s3-13},
	year = {1963},
	bdsk-url-1 = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-13.1.743},
	bdsk-url-2 = {https://doi.org/10.1112/plms/s3-13.1.743}}
@article{Eades1984,
	author          = {Peter Eades},
	year            = {1984},
	title           = {{A Heuristic for Graph Drawing}},
	journal         = {Congress Numerantium},
	volume          = {42},
	number          = {11},
	pages           = {149-160},
	url             = {https://www.cs.ubc.ca/~will/536E/papers/Eades1984.pdf}
}

@article{Kamada-Kawai1989,
	author = {Tomihisa Kamada and Satoru Kawai},
	doi = {https://doi.org/10.1016/0020-0190(89)90102-6},
	issn = {0020-0190},
	journal = {Information Processing Letters},
	keywords = {Graph, network structure, layout, drawing algorithm},
	number = {1},
	pages = {7-15},
	title = {An algorithm for drawing general undirected graphs},
	url = {https://www.sciencedirect.com/science/article/pii/0020019089901026},
	volume = {31},
	year = {1989},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0020019089901026},
	bdsk-url-2 = {https://doi.org/10.1016/0020-0190(89)90102-6}}
@ARTICLE{Fisk+1967,
  author={Fisk, C.J. and Caskey, D.L. and West, L.E.},
  journal={Proceedings of the IEEE}, 
  title={ACCEL: Automated circuit card etching layout}, 
  year={1967},
  volume={55},
  number={11},
  pages={1971-1982},
  keywords={Etching;Printed circuits;Assembly;Insulation;Joining processes;Libraries;Production systems;Drilling;Connectors;Contacts},
  doi={10.1109/PROC.1967.6027}}
@ARTICLE{Quinn-Breuer1979,
  author={Quinn, N. and Breuer, M.},
  journal={IEEE Transactions on Circuits and Systems}, 
  title={A forced directed component placement procedure for printed circuit boards}, 
  year={1979},
  volume={26},
  number={6},
  pages={377-388},
  keywords={Printed circuits;Equations;Wire;Personal communication networks;DH-HEMTs;Environmental management;Ceramics;Routing;Length measurement},
  doi={10.1109/TCS.1979.1084652}}

@article{Lee-Seung1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	date = {1999/10/01},
	date-added = {2024-08-14 16:57:13 +0900},
	date-modified = {2024-08-14 16:57:13 +0900},
	doi = {10.1038/44565},
	id = {Lee1999},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6755},
	pages = {788--791},
	title = {Learning the parts of objects by non-negative matrix factorization},
	url = {https://doi.org/10.1038/44565},
	volume = {401},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1038/44565}}
@inbook{Howard-Matheson1981,
	author         = {R. A. Howard and J. E. Matheson},
	chapter        = {Influence Diagrams},
	editor         = {R. A. Howard and J. E. Matheson},
	pages          = {},
	publisher      = {Strategic Decision Group},
	title          = {Readings on the Principles and Applications of Decision Analysis},
	year           = {1981}
}
@INPROCEEDINGS{Gori+2005,
  author={Gori, M. and Monfardini, G. and Scarselli, F.},
  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, 
  title={A new model for learning in graph domains}, 
  year={2005},
  volume={2},
  number={},
  pages={729-734 vol. 2},
  keywords={Neural networks;Focusing;Application software;Machine learning;Recurrent neural networks;Encoding;Data structures;Machine learning algorithms;Tree graphs;Software engineering},
  doi={10.1109/IJCNN.2005.1555942}}
@ARTICLE{Scarselli+2009,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}

@InProceedings{Gilmer+2017,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
@article{Battahlia+2018,title	= {Relational inductive biases, deep learning, and graph networks},author	= {Peter Battaglia and Jessica Blake Chandler Hamrick and Victor Bapst and Alvaro Sanchez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andy Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Jayne Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},year	= {2018},URL	= {https://arxiv.org/pdf/1806.01261.pdf},journal	= {arXiv}}

@inproceedings{Bruna+2014,
	author          = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
	year            = {2014},
	title           = {Spectral Networks and Locally Connected Networks on Graphs},
	booktitle       = {International Conference on Learning Representation},
	volume          = {},
	pages           = {},
	url             = {https://openreview.net/forum?id=DQNsQf-UsoDBa}
}
@inproceedings{Kipf-Welling2017,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}

@inproceedings{Hamilton+2017,
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Inductive Representation Learning on Large Graphs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf}}
@inproceedings{Velickovic+2018,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}
@INPROCEEDINGS{Masci+2015,
  author={Masci, Jonathan and Boscaini, Davide and Bronstein, Michael M. and Vandergheynst, Pierre},
  booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)}, 
  title={Geodesic Convolutional Neural Networks on Riemannian Manifolds}, 
  year={2015},
  volume={},
  number={},
  pages={832-840},
  keywords={Shape;Manifolds;Heating;Eigenvalues and eigenfunctions;Kernel;Neural networks;Geometry},
  doi={10.1109/ICCVW.2015.112}}

@inproceedings{Boscaini+2016,
	author = {Boscaini, Davide and Masci, Jonathan and Rodol\`{a}, Emanuele and Bronstein, Michael},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning shape correspondence with anisotropic convolutional neural networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf}}
@INPROCEEDINGS{Monti+2017,
  author={Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs}, 
  year={2017},
  volume={},
  number={},
  pages={5425-5434},
  keywords={Manifolds;Machine learning;Convolution;Laplace equations;Three-dimensional displays;Shape;Computational modeling},
  doi={10.1109/CVPR.2017.576}}

@inproceedings{Chami+2019,
	author = {Chami, Ines and Ying, Zhitao and R\'{e}, Christopher and Leskovec, Jure},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Hyperbolic Graph Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf}}

@inproceedings{Liu+2019,
	author = {Liu, Qi and Nickel, Maximilian and Kiela, Douwe},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Hyperbolic Graph Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf}}
@article{Bell-Sejnowski1995,
    author = {Bell, Anthony J. and Sejnowski, Terrence J.},
    title = "{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}",
    journal = {Neural Computation},
    volume = {7},
    number = {6},
    pages = {1129-1159},
    year = {1995},
    month = {11},
    abstract = "{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1995.7.6.1129},
    url = {https://doi.org/10.1162/neco.1995.7.6.1129},
    eprint = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
}
@unpublished{Yao2011,
	author = {Yuan Yao},
	year   = {2011},
	title  = {Mathematics of Data -- Laplacian, Diffusion, and Hessian LLE},
	note            = {Lecture 10},
	url    = {https://www.math.pku.edu.cn/teachers/yaoy/Spring2011/}
}

@article{Jordan-Kinderlehrer-Otto1998,
	abstract = { The Fokker--Planck equation, or forward Kolmogorov equation, describes the evolution of the probability density for a stochastic process associated with an Ito stochastic differential equation. It pertains to a wide variety of time-dependent systems in which randomness plays a role. In this paper, we are concerned with Fokker--Planck equations for which the drift term is given by the gradient of a potential. For a broad class of potentials, we construct a time discrete, iterative variational scheme whose solutions converge to the solution of the Fokker--Planck equation. The major novelty of this iterative scheme is that the time-step is governed by the Wasserstein metric on probability measures. This formulation enables us to reveal an appealing, and previously unexplored, relationship between the Fokker--Planck equation and the associated free energy functional. Namely, we demonstrate that the dynamics may be regarded as a gradient flux, or a steepest descent, for the free energy with respect to the Wasserstein metric. },
	author = {Jordan, Richard and Kinderlehrer, David and Otto, Felix},
	doi = {10.1137/S0036141096303359},
	eprint = {https://doi.org/10.1137/S0036141096303359},
	journal = {SIAM Journal on Mathematical Analysis},
	number = {1},
	pages = {1-17},
	title = {The Variational Formulation of the Fokker--Planck Equation},
	url = {https://doi.org/10.1137/S0036141096303359},
	volume = {29},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1137/S0036141096303359}}
@unpublished{Hairer2018,
    author = {Martin Hairer},
    note   = {Lecture Note},
    title  = {Ergodic Properties of Markov Processes},
    year   = {2018},
    url    = {https://www.hairer.org/notes/Markov.pdf}
}

@article{Zhang-Zha2004,
	abstract = { We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each data point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in two-dimensional/three-dimensional (2D/3D) Euclidean spaces and in higher-dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements. },
	author = {Zhang, Zhenyue and Zha, Hongyuan},
	doi = {10.1137/S1064827502419154},
	eprint = {https://doi.org/10.1137/S1064827502419154},
	journal = {SIAM Journal on Scientific Computing},
	number = {1},
	pages = {313-338},
	title = {Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment},
	url = {https://doi.org/10.1137/S1064827502419154},
	volume = {26},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1137/S1064827502419154}}

@article{Donoho-Grimes2003,
	author = {David L. Donoho and Carrie Grimes},
	doi = {10.1073/pnas.1031596100},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1031596100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {10},
	pages = {5591-5596},
	title = {Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
	volume = {100},
	year = {2003},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1031596100}}

@article{Moon+2019,
	abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
	author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and Elzen, Antonia van den and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
	date = {2019/12/01},
	date-added = {2024-08-15 14:48:06 +0900},
	date-modified = {2024-08-15 14:48:06 +0900},
	doi = {10.1038/s41587-019-0336-3},
	id = {Moon2019},
	isbn = {1546-1696},
	journal = {Nature Biotechnology},
	number = {12},
	pages = {1482--1492},
	title = {Visualizing structure and transitions in high-dimensional biological data},
	url = {https://doi.org/10.1038/s41587-019-0336-3},
	volume = {37},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41587-019-0336-3}}

@article{Szubert+2019,
	abstract = {Single-cell technologies offer an unprecedented opportunity to effectively characterize cellular heterogeneity in health and disease. Nevertheless, visualisation and interpretation of these multi-dimensional datasets remains a challenge. We present a novel framework, ivis, for dimensionality reduction of single-cell expression data. ivis utilizes a siamese neural network architecture that is trained using a novel triplet loss function. Results on simulated and real datasets demonstrate that ivis preserves global data structures in a low-dimensional space, adds new data points to existing embeddings using a parametric mapping function, and scales linearly to hundreds of thousands of cells. ivis is made publicly available through Python and R interfaces on https://github.com/beringresearch/ivis.},
	author = {Szubert, Benjamin and Cole, Jennifer E. and Monaco, Claudia and Drozdov, Ignat},
	date = {2019/06/20},
	date-added = {2024-08-16 20:22:42 +0900},
	date-modified = {2024-08-16 20:22:42 +0900},
	doi = {10.1038/s41598-019-45301-0},
	id = {Szubert2019},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {8914},
	title = {Structure-preserving visualisation of high dimensional single-cell datasets},
	url = {https://doi.org/10.1038/s41598-019-45301-0},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-019-45301-0}}
@misc{Campbell+2024,
      title={Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design}, 
      author={Andrew Campbell and Jason Yim and Regina Barzilay and Tom Rainforth and Tommi Jaakkola},
      year={2024},
      eprint={2402.04997},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2402.04997}, 
}
@article{Stimper+2023, doi = {10.21105/joss.05361}, url = {https://doi.org/10.21105/joss.05361}, year = {2023}, publisher = {The Open Journal}, volume = {8}, number = {86}, pages = {5361}, author = {Vincent Stimper and David Liu and Andrew Campbell and Vincent Berenz and Lukas Ryll and Bernhard Schölkopf and José Miguel Hernández-Lobato}, title = {normflows: A PyTorch Package for Normalizing Flows}, journal = {Journal of Open Source Software} } 

@InProceedings{Germain+2015,
  title = 	 {{MADE: Masked Autoencoder for Distribution Estimation}},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}
@book{Pontryagin+1962,
	author         = {L. S. Pontryagin and V. G. Boltyanskii and R. V. Gamkrelidze and E. F. Mishchenko},
	year           = {1962},
	title          = {The Mathematical Theory of Optimal Processes},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {https://doi.org/10.1201/9780203749319},
	doi            = {},
	publisher      = {John Wiley \& Sons}
}

@article{Lettermann+2024,
	abstract = {Building a representative model of a complex dynamical system from empirical evidence remains a highly challenging problem. Classically, these models are described by systems of differential equations that depend on parameters that need to be optimized by comparison with data. In this tutorial, we introduce the most common multi-parameter estimation techniques, highlighting their successes and limitations. We demonstrate how to use the adjoint method, which allows efficient handling of large systems with many unknown parameters, and present prototypical examples across several fields of physics. Our primary objective is to provide a practical introduction to adjoint optimization, catering for a broad audience of scientists and engineers.},
	author = {Lettermann, Leon and Jurado, Alejandro and Betz, Timo and W{\"o}rg{\"o}tter, Florentin and Herzog, Sebastian},
	date = {2024/04/15},
	date-added = {2024-08-20 13:44:11 +0900},
	date-modified = {2024-08-20 13:44:11 +0900},
	doi = {10.1038/s42005-024-01606-9},
	id = {Lettermann2024},
	isbn = {2399-3650},
	journal = {Communications Physics},
	number = {1},
	pages = {128},
	title = {Tutorial: a beginner's guide to building a representative model of dynamical systems using the adjoint method},
	url = {https://doi.org/10.1038/s42005-024-01606-9},
	volume = {7},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42005-024-01606-9}}
@article{sanchez-lengeling+2021,
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  title = {A Gentle Introduction to Graph Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/gnn-intro},
  doi = {10.23915/distill.00033}
}
@misc{Adams+2018,
      title={Estimating the Spectral Density of Large Implicit Matrices}, 
      author={Ryan P. Adams and Jeffrey Pennington and Matthew J. Johnson and Jamie Smith and Yaniv Ovadia and Brian Patton and James Saunderson},
      year={2018},
      eprint={1802.03451},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03451}, 
}
@article{Avron-Toledo2011,
author = {Avron, Haim and Toledo, Sivan},
title = {Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/1944345.1944349},
doi = {10.1145/1944345.1944349},
abstract = {We analyze the convergence of randomized trace estimators. Starting at 1989, several algorithms have been proposed for estimating the trace of a matrix by 1/MΣi=1M ziT Azi, where the zi are random vectors; different estimators use different distributions for the zis, all of which lead to E(1/MΣi=1M ziT Azi) = trace(A). These algorithms are useful in applications in which there is no explicit representation of A but rather an efficient method compute zTAz given z. Existing results only analyze the variance of the different estimators. In contrast, we analyze the number of samples M required to guarantee that with probability at least 1-δ, the relative error in the estimate is at most ϵ. We argue that such bounds are much more useful in applications than the variance. We found that these bounds rank the estimators differently than the variance; this suggests that minimum-variance estimators may not be the best.We also make two additional contributions to this area. The first is a specialized bound for projection matrices, whose trace (rank) needs to be computed in electronic structure calculations. The second is a new estimator that uses less randomness than all the existing estimators.},
journal = {J. ACM},
month = {apr},
articleno = {8},
numpages = {34},
keywords = {implicit linear operators, Trace estimation}
}
@inbook{Meyer+2021,
author = {Raphael A. Meyer and Cameron Musco and Christopher Musco and David P. Woodruff},
title = {Hutch++: Optimal Stochastic Trace Estimation},
booktitle = {2021 Symposium on Simplicity in Algorithms (SOSA)},
chapter = {},
pages = {142-155},
doi = {10.1137/1.9781611976496.16},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976496.16},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976496.16},
}
@inproceedings{Zhang-Chen2023,
title={Fast Sampling of Diffusion Models with Exponential Integrator},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Loek7hfb46P}
}
@article{McCann1997,
title = {A Convexity Principle for Interacting Gases},
journal = {Advances in Mathematics},
volume = {128},
number = {1},
pages = {153-179},
year = {1997},
issn = {0001-8708},
doi = {https://doi.org/10.1006/aima.1997.1634},
url = {https://www.sciencedirect.com/science/article/pii/S0001870897916340},
author = {Robert J. McCann},
abstract = {A new set of inequalities is introduced, based on a novel but natural interpolation between Borel probability measures onRd. Using these estimates in lieu of convexity or rearrangement inequalities, the existence and uniqueness problems are solved for a family of attracting gas models. In these models, the gas interacts with itself through a force which increases with distance and is governed by an equation of stateP=P(ϱ) relating pressure to density.P(ϱ)/ϱ>(d−1)/dis assumed non-decreasing for ad-dimensional gas. By showing that the internal and potential energies for the system are convex functions of the interpolation parameter, an energy minimizing state—unique up to translation—is proven to exist. The concavity established for ¶ρt¶−p/dqas a function oft∈[0,1] generalizes the Brunn–Minkowski inequality from sets to measures.}
}
@Article{Maoutsa+2020,
AUTHOR = {Maoutsa, Dimitra and Reich, Sebastian and Opper, Manfred},
TITLE = {Interacting Particle Solutions of Fokker–Planck Equations Through Gradient–Log–Density Estimation},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {802},
URL = {https://www.mdpi.com/1099-4300/22/8/802},
PubMedID = {33286573},
ISSN = {1099-4300},
ABSTRACT = {Fokker–Planck equations are extensively employed in various scientific fields as they characterise the behaviour of stochastic systems at the level of probability density functions. Although broadly used, they allow for analytical treatment only in limited settings, and often it is inevitable to resort to numerical solutions. Here, we develop a computational approach for simulating the time evolution of Fokker–Planck solutions in terms of a mean field limit of an interacting particle system. The interactions between particles are determined by the gradient of the logarithm of the particle density, approximated here by a novel statistical estimator. The performance of our method shows promising results, with more accurate and less fluctuating statistics compared to direct stochastic simulations of comparable particle number. Taken together, our framework allows for effortless and reliable particle-based simulations of Fokker–Planck equations in low and moderate dimensions. The proposed gradient–log–density estimator is also of independent interest, for example, in the context of optimal control.},
DOI = {10.3390/e22080802}
}
@inproceedings{Heitz+2023,
author = {Heitz, Eric and Belcour, Laurent and Chambon, Thomas},
title = {Iterative α -(de)Blending: a&nbsp;Minimalist&nbsp;Deterministic&nbsp;Diffusion&nbsp;Model},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591540},
doi = {10.1145/3588432.3591540},
abstract = {We derive a minimalist but powerful deterministic denoising-diffusion model. While denoising diffusion has shown great success in many domains, its underlying theory remains largely inaccessible to non-expert users. Indeed, an understanding of graduate-level concepts such as Langevin dynamics or score matching appears to be required to grasp how it works. We propose an alternative approach that requires no more than undergrad calculus and probability. We consider two densities and observe what happens when random samples from these densities are blended (linearly interpolated). We show that iteratively blending and deblending samples produces random paths between the two densities that converge toward a deterministic mapping. This mapping can be evaluated with a neural network trained to deblend samples. We obtain a model that behaves like deterministic denoising diffusion: it iteratively maps samples from one density (e.g., Gaussian noise) to another (e.g., cat images). However, compared to the state-of-the-art alternative, our model is simpler to derive, simpler to implement, more numerically stable, achieves higher quality results in our experiments, and has interesting connections to computer graphics.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {34},
numpages = {8},
keywords = {diffusion models, mapping, sampling},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{Porter-Duff1984,
author = {Porter, Thomas and Duff, Tom},
title = {Compositing digital images},
year = {1984},
isbn = {0897911385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800031.808606},
doi = {10.1145/800031.808606},
abstract = {Most computer graphics pictures have been computed all at once, so that the rendering program takes care of all computations relating to the overlap of objects. There are several applications, however, where elements must be rendered separately, relying on compositing techniques for the anti-aliased accumulation of the full image. This paper presents the case for four-channel pictures, demonstrating that a matte component can be computed similarly to the color channels. The paper discusses guidelines for the generation of elements and the arithmetic for their arbitrary compositing.},
booktitle = {Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {253–259},
numpages = {7},
keywords = {Compositing, Graphics systems, Matte algebra, Matte channel, Visible surface algorithms},
series = {SIGGRAPH '84}
}

@misc{Oulhaj+2024,
      title={Differentiable Mapper For Topological Optimization Of Data Representation}, 
      author={Ziyad Oulhaj and Mathieu Carrière and Bertrand Michel},
      year={2024},
      eprint={2402.12854},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12854}, 
}
@misc{Oulhaj+2024DeepMapper,
      title={Deep Mapper Graph and its Application to Visualize Plausible Pathways on High-Dimensional Distribution with Small Time-Complexity}, 
      author={Ziyad Oulhaj and Yoshiyuki Ishii and Kento Ohga and Kimihiro Yamazaki and Mutsuyo Wada and Yuhei Umeda and Takashi Kato and Yuichiro Wada and Hiroaki Kurihara},
      year={2024},
      eprint={2402.19177},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/2402.19177}, 
}
@misc{Esser+2024,
      title={Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach},
      year={2024},
      eprint={2403.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.03206}, 
}
@inproceedings{Kingma-Gao2023,
 author = {Kingma, Diederik and Gao, Ruiqi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {65484--65516},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ce79fbf9baef726645bc2337abb0ade2-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}
@misc{Heng+2022,
      title={{Simulating Diffusion Bridges with Score Matching}}, 
      author={Jeremy Heng and Valentin De Bortoli and Arnaud Doucet and James Thornton},
      year={2022},
      eprint={2111.07243},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2111.07243}, 
}
@article{Tong+2024,
title={{Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport}},
author={Alexander Tong and Kilian FATRAS and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=CD9Snc73AW},
note={Expert Certification}
}
@article{Schrodinger1932,
author = {Schrödinger, E.},
journal = {Annales de l'institut Henri Poincaré},
keywords = {quantum theory},
language = {fre},
number = {4},
pages = {269-310},
publisher = {INSTITUT HENRI POINCARÉ ET LES PRESSES UNIVERSITAIRES DE FRANCE},
title = {Sur la théorie relativiste de l'électron et l'interprétation de la mécanique quantique},
url = {http://eudml.org/doc/78968},
volume = {2},
year = {1932},
}

@InProceedings{Pooladian+2023,
  title = 	 {Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  author =       {Pooladian, Aram-Alexandre and Ben-Hamu, Heli and Domingo-Enrich, Carles and Amos, Brandon and Lipman, Yaron and Chen, Ricky T. Q.},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28100--28127},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/pooladian23a/pooladian23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/pooladian23a.html},
  abstract = 	 {Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with low cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a completely simulation-free manner with a simple minimization objective. We show that our proposed methods improve sample consistency on downsampled ImageNet data sets, and lead to better low-cost sample generation.}
}
@misc{Isobe+2024,
      title={Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation}, 
      author={Noboru Isobe and Masanori Koyama and Jinzhe Zhang and Kohei Hayashi and Kenji Fukumizu},
      year={2024},
      eprint={2402.18839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18839}, 
}
@misc{Fatras+2021,
      title={Minibatch optimal transport distances; analysis and applications}, 
      author={Kilian Fatras and Younes Zine and Szymon Majewski and Rémi Flamary and Rémi Gribonval and Nicolas Courty},
      year={2021},
      eprint={2101.01792},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2101.01792}, 
}
@Inbook{Brenier2003,
author="Brenier, Yann",
title="Extended Monge-Kantorovich Theory",
bookTitle="Optimal Transportation and Applications: Lectures given at the C.I.M.E. Summer School, held in Martina Franca, Italy, September 2-8, 2001",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="91--121",
abstract="1 Abstract2 Generalized geodesics and the Monge-Kantorovich theory2.1 Generalized geodesics2.2 Extension to probability measures2.3 A decomposition result2.4 Relativistic MKT2.5 A relativistic heat equation2.6 Laplace's equation and Moser's lemma revisited3 Generalized Harmonic functions3.1 Classical harmonic functions3.2 Open problems4 Multiphasic MKT5 Generalized extremal surfaces5.1 MKT revisited as a subset of generalized surface theory5.2 Degenerate quadratic cost functions6 Generalized extremal surfaces in {\$}<math display='block'><mrow><msup><mi>{\&}{\#}x211D;</mi><mn>5</mn></msup></mrow></math>{\$}{\$}{\backslash}mathbb{\{}R{\}}^5{\$}and Electrodynamics6.1 Recovery of the Maxwell equations6.2 Derivation of a set of nonlinear Maxwell equations6.3 An Euler-Maxwell-type systemReferences",
isbn="978-3-540-44857-0",
doi="10.1007/978-3-540-44857-0_4",
url="https://doi.org/10.1007/978-3-540-44857-0_4"
}


@InProceedings{Kerrigan+2024,
  title = 	 {Functional Flow Matching},
  author =       {Kerrigan, Gavin and Migliorini, Giosue and Smyth, Padhraic},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3934--3942},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/kerrigan24a/kerrigan24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/kerrigan24a.html},
  abstract = 	 {We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.}
}
@article{Lavenant2019,
title = {Harmonic mappings valued in the Wasserstein space},
journal = {Journal of Functional Analysis},
volume = {277},
number = {3},
pages = {688-785},
year = {2019},
issn = {0022-1236},
doi = {https://doi.org/10.1016/j.jfa.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022123619301478},
author = {Hugo Lavenant},
}
@article{Lavenant+2024,
author = {Hugo Lavenant and Stephen Zhang and Young-Heon Kim and Geoffrey Schiebinger},
title = {{Toward a mathematical theory of trajectory inference}},
volume = {34},
journal = {The Annals of Applied Probability},
number = {1A},
publisher = {Institute of Mathematical Statistics},
pages = {428 -- 500},
keywords = {Convex optimization, developmental biology, Optimal transport, single-cell RNA-sequencing, Stochastic processes, Trajectory inference},
year = {2024},
doi = {10.1214/23-AAP1969},
URL = {https://doi.org/10.1214/23-AAP1969}
}

@InProceedings{Hashimoto+2016,
  title = 	 {Learning Population-Level Diffusions with Generative RNNs},
  author = 	 {Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2417--2426},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hashimoto16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/hashimoto16.html},
  abstract = 	 {We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the ‘epigenetic landscape’ from existing biological assays.}
}

@misc{Dao+2023,
      title={Flow Matching in Latent Space}, 
      author={Quan Dao and Hao Phung and Binh Nguyen and Anh Tran},
      year={2023},
      eprint={2307.08698},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.08698}, 
}
@misc{Zheng+2023GuidedFlow,
      title={Guided Flows for Generative Modeling and Decision Making}, 
      author={Qinqing Zheng and Matt Le and Neta Shaul and Yaron Lipman and Aditya Grover and Ricky T. Q. Chen},
      year={2023},
      eprint={2311.13443},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.13443}, 
}
@inproceedings{Chen-Lipman2024,
title={Flow Matching on General Geometries},
author={Ricky T. Q. Chen and Yaron Lipman},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=g7ohDlTITL}
}
@misc{Habermann+2024,
      title={Amortized Bayesian Multilevel Models}, 
      author={Daniel Habermann and Marvin Schmitt and Lars Kühmichel and Andreas Bulling and Stefan T. Radev and Paul-Christian Bürkner},
      year={2024},
      eprint={2408.13230},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.13230}, 
}
@ARTICLE{Radev+2022,
  author={Radev, Stefan T. and Mertens, Ulf K. and Voss, Andreas and Ardizzone, Lynton and Köthe, Ullrich},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks}, 
  year={2022},
  volume={33},
  number={4},
  pages={1452-1466},
  keywords={Data models;Bayes methods;Biological system modeling;Neural networks;Training;Numerical models;Estimation;Bayesian inference;computational and artificial intelligence;machine learning;neural networks;statistical learning},
  doi={10.1109/TNNLS.2020.3042395}}
@article{Cranmer+2020,
author = {Kyle Cranmer  and Johann Brehmer  and Gilles Louppe },
title = {The frontier of simulation-based inference},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30055-30062},
year = {2020},
doi = {10.1073/pnas.1912789117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1912789117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1912789117},
abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.}}
@inproceedings{Koshizuka-Sato2023,
title={Neural Lagrangian Schr{\textbackslash}''\{o\}dinger Bridge: Diffusion Modeling for Population Dynamics},
author={Takeshi Koshizuka and Issei Sato},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=d3QNWD_pcFv}
}
@article{Efron2011,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/23239562},
 abstract = {We suppose that the statistician observes some large number of estimates zi, each with its own unobserved expectation parameter μi. The largest few of the zi's are likely to substantially overestimate their corresponding μi's, this being an example of selection bias, or regression to the mean. Tweedie's formula, first reported by Robbins in 1956, offers a simple empirical Bayes approach for correcting selection bias. This article investigates its merits and limitations. In addition to the methodology, Tweedie's formula raises more general questions concerning empirical Bayes theory, discussed here as "relevance" and "empirical Bayes information." There is a close connection between applications of the formula and James—Stein estimation.},
 author = {Bradley Efron},
 journal = {Journal of the American Statistical Association},
 number = {496},
 pages = {1602--1614},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Tweedie's Formula and Selection Bias},
 urldate = {2024-08-27},
 volume = {106},
 year = {2011}
}
@misc{Fjelde+2024,
  title   = "An Introduction to Flow Matching",
  author  = "Fjelde, Tor and Mathieu, Emile and Dutordoir, Vincent",
  journal = "https://mlg.eng.cam.ac.uk/blog/",
  year    = "2024",
  month   = "January",
  url     = "https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html"
}
@misc{Chen+2023SB,
  title={Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis}, 
  author={Zehua Chen and Guande He and Kaiwen Zheng and Xu Tan and Jun Zhu},
  year={2023},
  url             = {https://bridge-tts.github.io/},
}
@unpublished{Dieleman2023,
	author = {Sander Dieleman},
	year   = {2023},
	title  = {Perspectives on diffusion},
	url    = {https://sander.ai/2023/07/20/perspectives.html},
	doi    = {}
}
@unpublished{Yuan2024,
	author = {Chenyang Yuan},
	year   = {2024},
	title  = {Diffusion Models from Scratch, from a New Theoretical Perspective},
	url    = {https://www.chenyang.co/diffusion.html},
	doi    = {}
}
@misc{Nakkiran+2024,
      title={Step-by-Step Diffusion: An Elementary Tutorial}, 
      author={Preetum Nakkiran and Arwen Bradley and Hattie Zhou and Madhu Advani},
      year={2024},
      eprint={2406.08929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.08929}, 
}
@unpublished{Duan2023,
	author = {Tony Duan},
	year   = {2023},
	title  = {Diffusion Models from Scratch},
	url    = {https://www.tonyduan.com/diffusion/index.html},
	doi    = {}
}
@inproceedings{Das2024,
  author = {Das, Ayan},
  title = {Building Diffusion Model's theory from ground up},
  abstract = {Diffusion Models, a new generative model family, have taken the world by storm after the seminal paper by Ho et al. [2020]. While diffusion models are often described as a probabilistic Markov Chains, their underlying principle is based on the decade-old theory of Stochastic Differential Equations (SDE), as found out later by Song et al. [2021]. In this article, we will go back and revisit the 'fundamental ingredients' behind the SDE formulation and show how the idea can be 'shaped' to get to the modern form of Score-based Diffusion Models. We'll start from the very definition of the 'score', how it was used in the context of generative modeling, how we achieve the necessary theoretical guarantees and how the critical design choices were made to finally arrive at the more 'principled' framework of Score-based Diffusion. Throughout this article, we provide several intuitive illustrations for ease of understanding.},
  booktitle = {ICLR Blogposts 2024},
  year = {2024},
  date = {May 7, 2024},
  note = {https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/},
  url  = {https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/}
}

@InProceedings{Bunne+2022,
  title = 	 { Proximal Optimal Transport Modeling of Population Dynamics },
  author =       {Bunne, Charlotte and Papaxanthos, Laetitia and Krause, Andreas and Cuturi, Marco},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6511--6528},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/bunne22a/bunne22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/bunne22a.html},
  abstract = 	 { We propose a new approach to model the collective dynamics of a population of particles evolving with time. As is often the case in challenging scientific applications, notably single-cell genomics, measuring features for these particles requires destroying them. As a result, the population can only be monitored with periodic snapshots, obtained by sampling a few particles that are sacrificed in exchange for measurements. Given only access to these snapshots, can we reconstruct likely individual trajectories for all other particles? We propose to model these trajectories as collective realizations of a causal Jordan-Kinderlehrer-Otto (JKO) flow of measures: The JKO scheme posits that the new configuration taken by a population at time t+1 is one that trades off an improvement, in the sense that it decreases an energy, while remaining close (in Wasserstein distance) to the previous configuration observed at t. In order to learn such an energy using only snapshots, we propose JKOnet, a neural architecture that computes (in end-to-end differentiable fashion) the JKO flow given a parametric energy and initial configuration of points. We demonstrate the good performance and robustness of the JKOnet fitting procedure, compared to a more direct forward method. }
}
@article{Schiebinger+2019,
title = {Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming},
journal = {Cell},
volume = {176},
number = {4},
pages = {928-943.e22},
year = {2019},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S009286741930039X},
author = {Geoffrey Schiebinger and Jian Shu and Marcin Tabaka and Brian Cleary and Vidya Subramanian and Aryeh Solomon and Joshua Gould and Siyan Liu and Stacie Lin and Peter Berube and Lia Lee and Jenny Chen and Justin Brumbaugh and Philippe Rigollet and Konrad Hochedlinger and Rudolf Jaenisch and Aviv Regev and Eric S. Lander},
keywords = {optimal-transport, reprogramming, scRNA-seq, trajectories, ancestors, descendants, development, regulation, paracrine interactions, iPSCs},
abstract = {Summary
Understanding the molecular programs that guide differentiation during development is a major challenge. Here, we introduce Waddington-OT, an approach for studying developmental time courses to infer ancestor-descendant fates and model the regulatory programs that underlie them. We apply the method to reconstruct the landscape of reprogramming from 315,000 single-cell RNA sequencing (scRNA-seq) profiles, collected at half-day intervals across 18 days. The results reveal a wider range of developmental programs than previously characterized. Cells gradually adopt either a terminal stromal state or a mesenchymal-to-epithelial transition state. The latter gives rise to populations related to pluripotent, extra-embryonic, and neural cells, with each harboring multiple finer subpopulations. The analysis predicts transcription factors and paracrine signals that affect fates and experiments validate that the TF Obox6 and the cytokine GDF9 enhance reprogramming efficiency. Our approach sheds light on the process and outcome of reprogramming and provides a framework applicable to diverse temporal processes in biology.}
}

@InProceedings{Tong+2020,
  title = 	 {{T}rajectory{N}et: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics},
  author =       {Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9526--9536},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tong20a/tong20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tong20a.html},
  abstract = 	 {It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present \emph{TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.}
}

@InProceedings{Neklyudov+2023,
  title = 	 {Action Matching: Learning Stochastic Dynamics from Samples},
  author =       {Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {25858--25889},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/neklyudov23a/neklyudov23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/neklyudov23a.html},
  abstract = 	 {Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation and destruction of probability mass. Finally, we showcase applications of Action Matching by achieving competitive performance in a diverse set of experiments from biology, physics, and generative modeling.}
}

@article{Eldan2013,
	abstract = {We consider the isoperimetric inequality on the class of high-dimensional isotropic convex bodies. We establish quantitative connections between two well-known open problems related to this inequality, namely, the thin shell conjecture, and the conjecture by Kannan, Lov{\'a}sz, and Simonovits, showing that the corresponding optimal bounds are equivalent up to logarithmic factors. In particular we prove that, up to logarithmic factors, the minimal possible ratio between surface area and volume is attained on ellipsoids. We also show that a positive answer to the thin shell conjecture would imply an optimal dependence on the dimension in a certain formulation of the Brunn--Minkowski inequality. Our results rely on the construction of a stochastic localization scheme for log-concave measures.},
	author = {Eldan, Ronen},
	date = {2013/04/01},
	date-added = {2024-08-28 20:57:55 +0900},
	date-modified = {2024-08-28 20:57:55 +0900},
	doi = {10.1007/s00039-013-0214-y},
	id = {Eldan2013},
	isbn = {1420-8970},
	journal = {Geometric and Functional Analysis},
	number = {2},
	pages = {532--569},
	title = {{Thin Shell Implies Spectral Gap Up to Polylog via a Stochastic Localization Scheme}},
	url = {https://doi.org/10.1007/s00039-013-0214-y},
	volume = {23},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s00039-013-0214-y}}
@misc{Montanari2023,
      title={Sampling, Diffusions, and Stochastic Localization}, 
      author={Andrea Montanari},
      year={2023},
      eprint={2305.10690},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.10690}, 
}
@INPROCEEDINGS{Alaoui+2022,
  author={Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
  booktitle={2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)}, 
  title={Sampling from the Sherrington-Kirkpatrick Gibbs measure via algorithmic stochastic localization}, 
  year={2022},
  volume={},
  number={},
  pages={323-334},
  keywords={Measurement;Location awareness;Couplings;Temperature distribution;Stochastic processes;Approximation algorithms;Sampling methods},
  doi={10.1109/FOCS54457.2022.00038}}
@misc{Benton+2024,
      title={Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization}, 
      author={Joe Benton and Valentin De Bortoli and Arnaud Doucet and George Deligiannidis},
      year={2024},
      eprint={2308.03686},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2308.03686}, 
}
@article{Benton+2024Denoising,
    author = {Benton, Joe and Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
    title = "{From denoising diffusions to denoising Markov models}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {86},
    number = {2},
    pages = {286-301},
    year = {2024},
    month = {01},
    abstract = "{Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalizing this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.}",
    issn = {1369-7412},
    doi = {10.1093/jrsssb/qkae005},
    url = {https://doi.org/10.1093/jrsssb/qkae005},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/86/2/286/57219053/qkae005.pdf},
}
@INPROCEEDINGS{Zhu-Mumford1998,
  author={Song Chun Zhu and Mumford, D.},
  booktitle={Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)}, 
  title={GRADE: Gibbs reaction and diffusion equations}, 
  year={1998},
  volume={},
  number={},
  pages={847-854},
  keywords={Application software;Partial differential equations;Pattern formation;Nonlinear equations;Computer vision;Image processing;Markov random fields;Minimax techniques;Entropy;Differential equations},
  doi={10.1109/ICCV.1998.710816}}

@article{Zhu+1998,
	abstract = {This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework.},
	author = {Zhu, Song Chun and Wu, Yingnian and Mumford, David},
	date = {1998/03/01},
	date-added = {2024-09-02 13:15:46 +0900},
	date-modified = {2024-09-02 13:15:46 +0900},
	doi = {10.1023/A:1007925832420},
	id = {Zhu1998},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {2},
	pages = {107--126},
	title = {{Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling}},
	url = {https://doi.org/10.1023/A:1007925832420},
	volume = {27},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1007925832420}}
@article{Shibue-Komaki2020,
    doi = {10.1371/journal.pcbi.1007650},
    author = {Shibue, Ryohei AND Komaki, Fumiyasu},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deconvolution of calcium imaging data using marked point processes},
    year = {2020},
    month = {03},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pcbi.1007650},
    pages = {1-25},
    number = {3},
}

@article{Shibue-Komaki2017,
	abstract = {Neural decoding is a framework for reconstructing external stimuli from spike trains recorded by various neural recordings. Kloosterman et al. proposed a new decoding method using marked point processes (Kloosterman F, Layton SP, Chen Z, Wilson MA. J Neurophysiol 111: 217?227, 2014). This method does not require spike sorting and thereby improves decoding accuracy dramatically. In this method, they used kernel density estimation to estimate intensity functions of marked point processes. However, the use of kernel density estimation causes problems such as low decoding accuracy and high computational costs. To overcome these problems, we propose a new decoding method using infinite mixture models to estimate intensity. The proposed method improves decoding performance in terms of accuracy and computational speed. We apply the proposed method to simulation and experimental data to verify its performance. NEW \& NOTEWORTHY We propose a new neural decoding method using infinite mixture models and nonparametric Bayesian statistics. The proposed method improves decoding performance in terms of accuracy and computation speed. We have successfully applied the proposed method to position decoding from spike trains recorded in a rat hippocampus.},
	annote = {doi: 10.1152/jn.00818.2016},
	author = {Shibue, Ryohei and Komaki, Fumiyasu},
	date = {2017/11/01},
	date-added = {2024-09-02 13:56:48 +0900},
	date-modified = {2024-09-02 13:56:48 +0900},
	doi = {10.1152/jn.00818.2016},
	isbn = {0022-3077},
	journal = {Journal of Neurophysiology},
	journal1 = {Journal of Neurophysiology},
	month = {2024/09/01},
	n2 = {Neural decoding is a framework for reconstructing external stimuli from spike trains recorded by various neural recordings. Kloosterman et al. proposed a new decoding method using marked point processes (Kloosterman F, Layton SP, Chen Z, Wilson MA. J Neurophysiol 111: 217?227, 2014). This method does not require spike sorting and thereby improves decoding accuracy dramatically. In this method, they used kernel density estimation to estimate intensity functions of marked point processes. However, the use of kernel density estimation causes problems such as low decoding accuracy and high computational costs. To overcome these problems, we propose a new decoding method using infinite mixture models to estimate intensity. The proposed method improves decoding performance in terms of accuracy and computational speed. We apply the proposed method to simulation and experimental data to verify its performance. NEW \& NOTEWORTHY We propose a new neural decoding method using infinite mixture models and nonparametric Bayesian statistics. The proposed method improves decoding performance in terms of accuracy and computation speed. We have successfully applied the proposed method to position decoding from spike trains recorded in a rat hippocampus.},
	number = {5},
	pages = {2902--2913},
	publisher = {American Physiological Society},
	title = {Firing rate estimation using infinite mixture models and its application to neural decoding},
	type = {doi: 10.1152/jn.00818.2016},
	url = {https://doi.org/10.1152/jn.00818.2016},
	volume = {118},
	year = {2017},
	year1 = {2017},
	bdsk-url-1 = {https://doi.org/10.1152/jn.00818.2016}}
@book{Goldstine1980,
	author = {Herman H. Goldstine},
	year = {1980},
	title = {A History of the Calculus of Variations from the 17th through the 19th Century},
	series = {Studies in the History of Mathematics and Physical Sciences},
	volume = {5},
	edition = {},
	url = {https://doi.org/10.1007/978-1-4613-8106-8},
	doi = {10.1007/978-1-4613-8106-8},
	publisher = {Springer New York}
}
@article{Hamilton1834,
	author = {W. R. Hamilton},
	year = {1834},
	title = {On a General Method in Dynamics; by which the Study of the Motions of all free Systems of attracting or repelling Points is reduced to the Search and Differentiation of one central Relation, or characteristic Function.},
	journal = {Philosophical Transactions of the Royal Society},
	volume = {124},
	number = {},
	pages = {247-308},
	url = {https://www.jstor.org/stable/108066},
	doi = {}
}
@inproceedings{中根美知代2000,
	author = {中根美知代},
	year = {2000},
	title = {物理学から数学へ : Hamilton-Jacobi 理論の誕生 (数学史の研究)},
	booktitle = {数理解析研究所講究録},
	volume = {1130},
	pages = {58-71},
	url = {http://hdl.handle.net/2433/63679},
	doi = {}
}
@article{Fermat1657,
	author = {Pierre de Fermat},
	year = {1657},
	title = {Marin Cureau de la Chambre, La Lumière (Chez Iacqves D'Allin, Paris, 1657)},
	journal = {Personal correspondence},
	volume = {},
	number = {},
	pages = {},
	url = {https://gallica.bnf.fr/ark:/12148/bpt6k94859n.pdf},
	doi = {}
}
@book{Euler1744,
	author = {Leonhard Euler},
	year = {1744},
	title = {ethodus Inveniendi Lineas Curvas Maximi Minimive Proprietate Gaudentes sive Solutio Problematis Isoperimetrici Latissimo Sensu Accepti},
	series = {},
	volume = {},
	edition = {},
	url = {https://archive.org/details/methodusinvenie00eule},
	doi = {},
	publisher = {Lausannæ ; Genevæ : Apud Marcum-Michaelem Bousquet & Socios}
}
@article{Bernoulli1969,
	author = {John Bernoulli},
	year = {1696},
	title = {Poblema novum ad cujus solution em mathematici invitantur},
	journal = {Acta Eruditorum},
	volume = {1},
	number = {},
	pages = {269},
	url = {},
	doi = {}
}
@book{Lagrange1788,
	author = {Joseph-Louis Lagrange},
	year = {1788},
	title = {Mécanique Analytique},
	series = {},
	volume = {},
	edition = {},
	url = {https://gallica.bnf.fr/ark:/12148/bpt6k862625},
	doi = {},
	publisher = {}
}
@unpublished{Hanc2017,
	author = {Jozef Hanc},
	year = {2017},
	title = {The Original Euler's Calculus-of-Variations Method: Key to Lagrangian Mechanics for Beginners},
	url = {https://www.eftaylor.com/pub/HancEulerEJP.pdf},
	doi = {}
}
@book{Monge1781,
	author = {Gaspard Monge},
	year = {1781},
	title = {Mémoire sur la théorie des déblais et des remblais},
	series = {},
	volume = {},
	edition = {},
	url = {},
	doi = {},
	publisher = {Imprimerie Royale}
}

@article{Kantorovich1942,
    author          = {L. V. Kantorovich},
    year            = {1942},
    title           = {On the Translocation of Masses},
    language        = {Russian},
    journal         = {Doklady Akademii Nauk SSSR},
    volume          = {37},
    number          = {7-8},
    pages           = {227-229},
    url             = {https://link.springer.com/article/10.1007/s10958-006-0049-2}
}
@article{Kantorovich1940,
    author          = {L. V. Kantorovich},
    year            = {1940},
    title           = {On an Effective Method of Solving Certain Classes of Extremal Problems},
    language        = {Russian},
    journal         = {Doklady Akademii Nauk SSSR},
    volume          = {28},
	pages = {212-215},
}
@article{Kantorovich1948,
    author          = {L. V. Kantorovich},
    year            = {1948},
    title           = {On a Problem of Monge},
    language        = {Russian},
    journal         = {Uspekhi Matematicheskikh Nauk},
    volume          = {3},
    number          = {2},
    pages           = {225-226},
    url             = {https://doi.org/10.1007/s10958-006-0050-9}
}

@article{Vershik2013,
	author = {Vershik, A.  M. },
	date = {2013/12/01},
	date-added = {2024-09-04 13:28:11 +0900},
	date-modified = {2024-09-04 13:28:11 +0900},
	doi = {10.1007/s00283-013-9380-x},
	id = {Vershik2013},
	isbn = {1866-7414},
	journal = {The Mathematical Intelligencer},
	number = {4},
	pages = {1--9},
	title = {Long History of the Monge-Kantorovich Transportation Problem},
	url = {https://doi.org/10.1007/s00283-013-9380-x},
	volume = {35},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s00283-013-9380-x}}
@Inbook{Ambrosio2024,
author="Ambrosio, Luigi
and Quarteroni, Alfio",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="Talking about Optimal Transport",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--19",
abstract="In this session, we have the pleasure of hosting Luigi Ambrosio, a professor at the Scuola Normale Superiore in Pisa, Italy, as our guest. Professor Ambrosio, who recently co-authored the new textbook, Lectures on Optimal Transport, with Elia Bru{\'e} and Daniele Semola, engages in a lively conversation with Alfio Quarteroni, a professor at Politecnico di Milano.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_1",
url="https://doi.org/10.1007/978-3-031-51685-6_1"
}
@incollection{Levi2014,
	author = {Mark Levi},
	booktitle = {SIAM News},
	publisher = {SIAM},
	title = {Quick! Find a Solution to the Brachistochrone Problem},
	year = {2014}
}
@Inbook{Ambrosio2003,
author="Ambrosio, Luigi",
title="Lecture Notes on Optimal Transport Problems",
bookTitle="Mathematical Aspects of Evolving Interfaces: Lectures given at the C.I.M.-C.I.M.E. joint Euro-Summer School held in Madeira, Funchal, Portugal, July 3-9, 2000",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--52",
abstract="1 Some elementary examples2 Optimal transport plans: existence and regularity3 The one dimensional case4 The ODE version of the optimal transport problem5 The PDE version of the optimal transport problem and the p-laplacian approximation6 Existence of optimal transport maps7 Regularity and uniqueness of the transport density8 The Bouchitt{\'e}-Buttazzo mass optimization problem9 Appendix: some measure theoretic resultsReferences",
isbn="978-3-540-39189-0",
doi="10.1007/978-3-540-39189-0_1",
url="https://doi.org/10.1007/978-3-540-39189-0_1"
}
@book{Evans-Gangbo1999,
	author = {L. C. Evans and W. Gangbo},
	year = {1999},
	title = {Differential Equations Methods for the Monge-Kantorovich Mass Transfer Problem},
	series = {Memoirs of the American Mathematical Society},
	volume = {137},
	number = {653},
	edition = {},
	url = {https://doi.org/10.1090/memo/0653},
	doi = {},
	publisher = {American Mathematical Society}
}
@unpublished{Figalli2023,
	author = {Alessio Figalli},
	year = {2023},
	title = {An Introduction to Optimal Transport and Wasserstein Gradient Flows},
	url = {https://people.math.ethz.ch/~afigalli/lecture-notes},
	note = {Lecture Note},
	doi = {}
}
@book{佐藤竜馬2023,
	author = {佐藤竜馬},
	year = {2023},
	title = {最適輸送の理論とアルゴリズム},
	series = {機械学習プロフェッショナルシリーズ},
	volume = {},
	edition = {},
	url = {https://www.kspub.co.jp/book/detail/5305140.html},
	doi = {},
	publisher = {講談社サイエンティフィック}
}
@Inbook{Figalli-Ambrosio2024,
author="Figalli, Alessio
and Ambrosio, Luigi",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="Optimal Transport, Fields Medals and beyond",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="21--38",
abstract="Welcome to the Springer Math Podcast. This month, we're delighted to host Alessio Figalli, the Director of the Institute for Mathematical Research at ETH Zurich, Switzerland. A distinguished academic, Professor Figalli completed his PhD at the Scuola Normale Superiore of Pisa, Italy, and at the Ecole Normale Superieure of Lyon, France. His research has taken him across France, the United States, and Switzerland. His notable contributions to the theory of optimal transport have earned him numerous accolades, including the prestigious Fields Medal in 2018, and the European Mathematical Society Prize in 2012.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_2",
url="https://doi.org/10.1007/978-3-031-51685-6_2"
}
@Inbook{Gigli-DeLellis2024,
author="Gigli, Nicola
and De Lellis, Camillo",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="From moving masses to bending spaces: an excursion in metric geometry",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="39--58",
abstract="Welcome to the Springer Math Podcast. In this month's podcast, Camillo De Lellis, a researcher at the Institute for Advanced Study in Princeton, converses with Nicola Gigli from the Scuola Internazionale Superiore di Studi Avanzati in Trieste, Italy. They delve into Nicola Gigli's personal journey in and out of mathematics, discussing his path to the topics of his research and his enthusiasm for them. In this conversation, they also explore the connection between the concepts of optimal transport and the curvature of space, a discovery that has given rise to a flourishing research field at the intersection of multiple areas of mathematics. Originally aired by the Springer Nature webinar series, this interview has been specifically adapted for the podcastformat.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_3",
url="https://doi.org/10.1007/978-3-031-51685-6_3"
}
@article{久保川達也2006,
  title={線形混合モデルと小地域の推定},
  author={久保川達也},
  journal={応用統計学},
  volume={35},
  number={3},
  pages={139-161},
  year={2006},
  doi={10.5023/jappstat.35.139}
}
@article{Kubokawa2000,
  title={ESTIMATION OF VARIANCE AND COVARIANCE COMPONENTS IN ELLIPTICALLY CONTOURED DISTRIBUTIONS},
  author={Tatsuya Kubokawa},
  journal={JOURNAL OF THE JAPAN STATISTICAL SOCIETY},
  volume={30},
  number={2},
  pages={143-176},
  year={2000},
  doi={10.14490/jjss1995.30.143}
}
@article{Battese+1988,
author = {George E. Battese, Rachel M. Harter and Wayne A. Fuller},
title = {An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data},
journal = {Journal of the American Statistical Association},
volume = {83},
number = {401},
pages = {28--36},
year = {1988},
publisher = {ASA Website},
doi = {10.1080/01621459.1988.10478561},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478561
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478561
    

}

}
@article{Gelman2014,
author = {Andrew Gelman},
title = {{How Bayesian Analysis Cracked the Red-State, Blue-State Problem}},
volume = {29},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {26 -- 35},
keywords = {Multilevel regression and poststratification (MRP), political science, sample surveys, sparse data, voting},
year = {2014},
doi = {10.1214/13-STS458},
URL = {https://doi.org/10.1214/13-STS458}
}
@article{Efron-Morris1975,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2285814},
 abstract = {In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution having uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein's rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson's chi-square test with results from a computer simulation. In each of these examples, the mean square error of these rules is less than half that of the sample mean.},
 author = {Bradley Efron and Carl Morris},
 journal = {Journal of the American Statistical Association},
 number = {350},
 pages = {311--319},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Data Analysis Using Stein's Estimator and its Generalizations},
 urldate = {2024-09-11},
 volume = {70},
 year = {1975}
}
@article{Kubokawa-Srivastava1999,
author = {T. Kubokawa and M. S. Srivastava},
title = {{Improved nonnegative estimation of multivariate components of variance}},
volume = {27},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2008 -- 2032},
keywords = {minimax and unbiased estimators, random effects model, restricted maximum likelihood estimator, Stein loss},
year = {1999},
doi = {10.1214/aos/1017939248},
URL = {https://doi.org/10.1214/aos/1017939248}
}
@article{丹後俊郎1988,
  title={死亡指標の経験的ベイズ推定量について},
  author={丹後俊郎},
  journal={応用統計学},
  volume={17},
  number={2},
  pages={81-96},
  year={1988},
  doi={10.5023/jappstat.17.81}
}
@article{Clayton-Kaldor1987,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532003},
 abstract = {There have been many attempts in recent years to map incidence and mortality from diseases such as cancer. Such maps usually display either relative rates in each district, as measured by a standardized mortality ratio (SMR) or some similar index, or the statistical significance level for a test of the difference between the rates in that district and elsewhere. Neither of these approaches is fully satisfactory and we propose a new approach using empirical Bayes estimation. The resulting estimators represent a weighted compromise between the SMR, the overall mean relative rate, and a local mean of the relative rate in nearby areas. The compromise solution depends on the reliability of each individual SMR and on estimates of the overall amount of dispersion of relative rates over different districts.},
 author = {David Clayton and John Kaldor},
 journal = {Biometrics},
 number = {3},
 pages = {671--681},
 publisher = {[Wiley, International Biometric Society]},
 title = {Empirical Bayes Estimates of Age-Standardized Relative Risks for Use in Disease Mapping},
 urldate = {2024-09-11},
 volume = {43},
 year = {1987}
}
@article{西川正子2008,
  title={生存時間解析における競合リスクモデル},
  author={西川正子},
  journal={計量生物学},
  volume={29},
  number={2},
  pages={141-170},
  year={2008},
  doi={10.5691/jjb.29.141}
}
@article{Kaplan-Meier1958,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2281868},
 author = {E. L. Kaplan and Paul Meier},
 journal = {Journal of the American Statistical Association},
 number = {282},
 pages = {457--481},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Nonparametric Estimation from Incomplete Observations},
 urldate = {2024-09-12},
 volume = {53},
 year = {1958}
}
@misc{Hardcastle+2024,
      title={Averaging polyhazard models using Piecewise deterministic Monte Carlo with applications to data with long-term survivors}, 
      author={Luke Hardcastle and Samuel Livingstone and Gianluca Baio},
      year={2024},
      eprint={2406.14182},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2406.14182}, 
}
@article{森満2016,
  title={専門医に必要な統計の知識と研究デザイン},
  author={森満},
  journal={日本耳鼻咽喉科学会会報},
  volume={119},
  number={7},
  pages={989-992},
  year={2016},
  doi={10.3950/jibiinkoka.119.989}
}
@article{Cutler-Ederer1958,
title = {Maximum utilization of the life table method in analyzing survival},
journal = {Journal of Chronic Diseases},
volume = {8},
number = {6},
pages = {699-712},
year = {1958},
issn = {0021-9681},
doi = {https://doi.org/10.1016/0021-9681(58)90126-7},
url = {https://www.sciencedirect.com/science/article/pii/0021968158901267},
author = {Sidney J. Cutler and Fred Ederer},
abstract = {We have illustrated the life table method for computing survival rates with 5-year survival data for cancer patients, emphasizing the advantage gained by including survival information on cases which entered the series too late to have had the opportunity to survive a full 5 years. The advantage is measured in terms of reduction in standard error of the survival rate. For the five series of patients in this paper, the reduction in standard error ranged from one-third to two-thirds.}
}
@techreport{Latimer2011,
	author = {N. Latimer},
	institution = {National Institute for Health and Care Excellence},
	title = {NICE DSU Technical Support Document 14: Undertaking Survival Analysis for Economic Evaluations alongside Clinical Trials--Estrapolation with Patient-level Data},
	year = {2011},
	url = {https://www.sheffield.ac.uk/nice-dsu/tsds/survival-analysis},
}
@article{Berger-Sun1993,
author = {James O. Berger and Dongchu Sun},
title = {Bayesian Analysis for the Poly-Weibull Distribution},
journal = {Journal of the American Statistical Association},
volume = {88},
number = {424},
pages = {1412--1418},
year = {1993},
publisher = {ASA Website},
doi = {10.1080/01621459.1993.10476426},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476426
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476426
    

}

}
@article{Louzada-Neto1999,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2533756},
 abstract = {We propose a polyhazard model to deal with lifetime data associated with latent competing risks. The causes of failure are assumed unobserved and affecting individuals independently. The general framework allows a broad class of hazard models that includes the most common hazard-based models. The model accommodates bathtub and multimodal hazards, keeping enough flexibility for common lifetime data that cannot be accommodated by usual hazard-based models. Maximum likelihood estimation is discussed, and parametric simulation is used for hypothesis testing.},
 author = {Francisco Louzada-Neto},
 journal = {Biometrics},
 number = {4},
 pages = {1281--1285},
 publisher = {[Wiley, International Biometric Society]},
 title = {Polyhazard Models for Lifetime Data},
 urldate = {2024-09-12},
 volume = {55},
 year = {1999}
}






@article{Latimer2013,
author = {Nicholas R. Latimer},
title ={Survival Analysis for Economic Evaluations Alongside Clinical Trials—Extrapolation with Patient-Level Data: Inconsistencies, Limitations, and a Practical Guide},

journal = {Medical Decision Making},
volume = {33},
number = {6},
pages = {743-754},
year = {2013},
doi = {10.1177/0272989X12472398},
    note ={PMID: 23341049},

URL = { 
    
        https://doi.org/10.1177/0272989X12472398
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0272989X12472398
    
    

},
}
@article{齋藤-室谷2023,
  title={マルチステートモデルの理論とがん臨床研究への応用},
  author={齋藤哲雄 and 室谷健太},
  journal={日本統計学会誌},
  volume={52},
  number={2},
  pages={221-267},
  year={2023},
  doi={10.11329/jjssj.52.221}
}
@article{齋藤-室谷2024,
  title={Competing Risks and Multistate Modelsin Oncology Clinical Trials},
  author={Tetsuo Saito and Kenta Murotani},
  journal={Japanese Journal of Biometrics},
  volume={45},
  number={1},
  pages={37-65},
  year={2024},
  doi={10.5691/jjb.45.37}
}
@article{Negrin+2017,
author = {Miguel A. Negrín and Julian Nam and Andrew H. Briggs},
title ={Bayesian Solutions for Handling Uncertainty in Survival Extrapolation},

journal = {Medical Decision Making},
volume = {37},
number = {4},
pages = {367-376},
year = {2017},
doi = {10.1177/0272989X16650669},
    note ={PMID: 27281336},

URL = { 
    
        https://doi.org/10.1177/0272989X16650669
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0272989X16650669
    
    

}
,
}
@article{Demiris+2015,
author = {Nikolaos Demiris and David Lunn and Linda D Sharples},
title ={Survival extrapolation using the poly-Weibull model},

journal = {Statistical Methods in Medical Research},
volume = {24},
number = {2},
pages = {287-301},
year = {2015},
doi = {10.1177/0962280211419645},
    note ={PMID: 21937472},

URL = { 
    
        https://doi.org/10.1177/0962280211419645
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0962280211419645
    
    

},
}
@article{Benaglia+2015,
author = {Benaglia, Tatiana and Jackson, Christopher H. and Sharples, Linda D.},
title = {Survival extrapolation in the presence of cause specific hazards},
journal = {Statistics in Medicine},
volume = {34},
number = {5},
pages = {796-811},
keywords = {survival analysis, survival extrapolation, poly-weibull, polyhazard, cause specific hazards},
doi = {https://doi.org/10.1002/sim.6375},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6375},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6375},
abstract = {Health economic evaluations require estimates of expected survival from patients receiving different interventions, often over a lifetime. However, data on the patients of interest are typically only available for a much shorter follow-up time, from randomised trials or cohorts. Previous work showed how to use general population mortality to improve extrapolations of the short-term data, assuming a constant additive or multiplicative effect on the hazards for all-cause mortality for study patients relative to the general population. A more plausible assumption may be a constant effect on the hazard for the specific cause of death targeted by the treatments. To address this problem, we use independent parametric survival models for cause-specific mortality among the general population. Because causes of death are unobserved for the patients of interest, a polyhazard model is used to express their all-cause mortality as a sum of latent cause-specific hazards. Assuming proportional cause-specific hazards between the general and study populations then allows us to extrapolate mortality of the patients of interest to the long term. A Bayesian framework is used to jointly model all sources of data. By simulation, we show that ignoring cause-specific hazards leads to biased estimates of mean survival when the proportion of deaths due to the cause of interest changes through time. The methods are applied to an evaluation of implantable cardioverter defibrillators for the prevention of sudden cardiac death among patients with cardiac arrhythmia. After accounting for cause-specific mortality, substantial differences are seen in estimates of life years gained from implantable cardioverter defibrillators. © 2014 The Authors Statistics in Medicine Published by John Wiley \& Sons Ltd.},
year = {2015}
}
@article{Mitchell-Beauchamp1988,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2290129},
 abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
 author = {T. J. Mitchell and J. J. Beauchamp},
 journal = {Journal of the American Statistical Association},
 number = {404},
 pages = {1023--1032},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Variable Selection in Linear Regression},
 urldate = {2024-09-12},
 volume = {83},
 year = {1988}
}
@article{Ley-Steel2009,
author = {Ley, Eduardo and Steel, Mark F.J.},
title = {On the effect of prior assumptions in Bayesian model averaging with applications to growth regression},
journal = {Journal of Applied Econometrics},
volume = {24},
number = {4},
pages = {651-674},
doi = {https://doi.org/10.1002/jae.1057},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.1057},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.1057},
abstract = {Abstract We consider the problem of variable selection in linear regression models. Bayesian model averaging has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. We examine the effect of a variety of prior assumptions on the inference concerning model size, posterior inclusion probabilities of regressors and on predictive performance. We illustrate these issues in the context of cross-country growth regressions using three datasets with 41–67 potential drivers of growth and 72–93 observations. Finally, we recommend priors for use in this and related contexts. Copyright © 2009 John Wiley \& Sons, Ltd.},
year = {2009}
}
@article{Polson-Scott2012,
author = {Nicholas G. Polson and James G. Scott},
title = {{On the Half-Cauchy Prior for a Global Scale Parameter}},
volume = {7},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {887 -- 902},
keywords = {hierarchical models, normal scale mixtures, shrinkage},
year = {2012},
doi = {10.1214/12-BA730},
URL = {https://doi.org/10.1214/12-BA730}
}
@article{Gelman2006,
author = {Andrew Gelman},
title = {{Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)}},
volume = {1},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {515 -- 534},
keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-$t$ distribution, half-$t$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2006},
doi = {10.1214/06-BA117A},
URL = {https://doi.org/10.1214/06-BA117A}
}
@article{Jasra+2005,
author = {A. Jasra and C. C. Holmes and D. A. Stephens},
title = {{Markov Chain Monte Carlo Methods and the Label Switching Problem in Bayesian Mixture Modeling}},
volume = {20},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 67},
keywords = {Bayesian statistics, Identifiability, label switching, MCMC, mixture modeling, sensitivity analysis},
year = {2005},
doi = {10.1214/088342305000000016},
URL = {https://doi.org/10.1214/088342305000000016}
}
@article{山口一大2022,
  title={項目反応理論モデルのパラメタ推定法の展開},
  author={山口一大},
  journal={日本テスト学会誌},
  volume={18},
  number={1},
  pages={103-131},
  year={2022},
  doi={10.24690/jart.18.1_103}
}
@phdthesis{Wilson1984,
	author = {Mark R. Wilson},
	school = {University of Chicago},
	title = {A Psychometric Model of Hierarchical Development},
	year = {1984},

}

@article{Embretson1984,
	abstract = {The purpose of the current paper is to propose a general multicomponent latent trait model (GLTM) for response processes. The proposed model combines the linear logistic latent trait (LLTM) with the multicomponent latent trait model (MLTM). As with both LLTM and MLTM, the general multicomponent latent trait model can be used to (1) test hypotheses about the theoretical variables that underlie response difficulty and (2) estimate parameters that describe test items by basic substantive properties. However, GLTM contains both component outcomes and complexity factors in a single model and may be applied to data that neither LLTM nor MLTM can handle. Joint maximum likelihood estimators are presented for the parameters of GLTM and an application to cognitive test items is described.},
	author = {Embretson (Whitely), Susan},
	date = {1984/06/01},
	date-added = {2024-09-17 13:20:02 +0900},
	date-modified = {2024-09-17 13:20:02 +0900},
	doi = {10.1007/BF02294171},
	id = {Embretson (Whitely)1984},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {175--186},
	title = {A general latent trait model for response processes},
	url = {https://doi.org/10.1007/BF02294171},
	volume = {49},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1007/BF02294171}}
@misc{yuimadocs2024,
	author = {The YUIMA Project Team},
	title = {Package `yuima`},
	year = {2024},
	url = {https://cran.r-project.org/web/packages/yuima/index.html},
}
@article{Cox+1985,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1911242},
 abstract = {This paper uses an intertemporal general equilibrium asset pricing model to study the term structure of interest rates. In this model, anticipations, risk aversion, investment alternatives, and preferences about the timing of consumption all play a role in determining bond prices. Many of the factors traditionally mentioned as influencing the term structure are thus included in a way which is fully consistent with maximizing behavior and rational expectations. The model leads to specific formulas for bond prices which are well suited for empirical testing.},
 author = {John C. Cox and Jonathan E. Ingersoll and Stephen A. Ross},
 journal = {Econometrica},
 number = {2},
 pages = {385--407},
 publisher = {[Wiley, Econometric Society]},
 title = {A Theory of the Term Structure of Interest Rates},
 urldate = {2024-09-17},
 volume = {53},
 year = {1985}
}
@article{Hayashi-Yoshida2005,
author = {Takaki Hayashi and Nakahiro Yoshida},
title = {{On covariance estimation of non-synchronously observed diffusion processes}},
volume = {11},
journal = {Bernoulli},
number = {2},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {359 -- 379},
keywords = {Diffusions, Discrete-time observations, high-frequency data, mathematical finance, non-synchronous trading, Quadratic Variation, realized volatility},
year = {2005},
doi = {10.3150/bj/1116340299},
URL = {https://doi.org/10.3150/bj/1116340299}
}
@article{Gelman+2015,
author = {Andrew Gelman and Daniel Lee and Jiqiang Guo},
title ={Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization},

journal = {Journal of Educational and Behavioral Statistics},
volume = {40},
number = {5},
pages = {530-543},
year = {2015},
doi = {10.3102/1076998615606113},

URL = { 
    
        https://doi.org/10.3102/1076998615606113
    
    

},
eprint = { 
    
        https://doi.org/10.3102/1076998615606113
    
    

}
,
}

@article{Lunn+2000,
	abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.},
	author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
	date = {2000/10/01},
	date-added = {2024-09-20 13:45:47 +0900},
	date-modified = {2024-09-20 13:45:47 +0900},
	doi = {10.1023/A:1008929526011},
	id = {Lunn2000},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {4},
	pages = {325--337},
	title = {WinBUGS - A Bayesian modelling framework: Concepts, structure, and extensibility},
	url = {https://doi.org/10.1023/A:1008929526011},
	volume = {10},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1023/A:1008929526011}}
@article{Green1995,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2337340},
 abstract = {Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.},
 author = {Peter J. Green},
 journal = {Biometrika},
 number = {4},
 pages = {711--732},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination},
 urldate = {2024-09-22},
 volume = {82},
 year = {1995}
}

@article{McShane+2019,
	author = {Blakeley B. McShane, David Gal, Andrew Gelman, Christian Robert and Jennifer L. Tackett},
	doi = {10.1080/00031305.2018.1527253},
	eprint = {https://doi.org/10.1080/00031305.2018.1527253},
	journal = {The American Statistician},
	number = {sup1},
	pages = {235--245},
	publisher = {ASA Website},
	title = {Abandon Statistical Significance},
	url = {https://doi.org/10.1080/00031305.2018.1527253},
	volume = {73},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1080/00031305.2018.1527253}}

@article{Gelman-Stern2006,
	author = {Andrew Gelman and Hal Stern},
	doi = {10.1198/000313006X152649},
	eprint = {https://doi.org/10.1198/000313006X152649},
	journal = {The American Statistician},
	number = {4},
	pages = {328--331},
	publisher = {ASA Website},
	title = {The Difference Between ``Significant'' and ``Not Significant'' is not Itself Statistically Significant},
	url = {https://doi.org/10.1198/000313006X152649},
	volume = {60},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/000313006X152649}}

@article{Wagenmakers+2016,
	abstract = { The practical advantages of Bayesian inference are demonstrated here through two concrete examples. In the first example, we wish to learn about a criminal's IQ: a problem of parameter estimation. In the second example, we wish to quantify and track support in favor of the null hypothesis that Adam Sandler movies are profitable regardless of their quality: a problem of hypothesis testing. The Bayesian approach unifies both problems within a coherent predictive framework, in which parameters and models that predict the data successfully receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more informative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology. },
	author = {Eric-Jan Wagenmakers and Richard D. Morey and Michael D. Lee},
	doi = {10.1177/0963721416643289},
	eprint = {https://doi.org/10.1177/0963721416643289},
	journal = {Current Directions in Psychological Science},
	number = {3},
	pages = {169-176},
	title = {Bayesian Benefits for the Pragmatic Researcher},
	url = {https://doi.org/10.1177/0963721416643289},
	volume = {25},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1177/0963721416643289}}

@article{Dienes-Mclatchie2018,
	abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in motivating researchers to conclude that H1 is supported better than H0, and the other way round, that H0 is better supported than H1. The next four, however, show that the methods will also often disagree. In these cases, the aim of the paper will be to motivate the sensible evidential conclusion, and then see which approach matches those intuitions. Specifically, it is shown that a high-powered non-significant result is consistent with no evidence for H0 over H1 worth mentioning, which a Bayes factor can show, and, conversely, that a low-powered non-significant result is consistent with substantial evidence for H0 over H1, again indicated by Bayesian analyses. The fourth study illustrates that a high-powered significant result may not amount to any evidence for H1 over H0, matching the Bayesian conclusion. Finally, the fifth study illustrates that different theories can be evidentially supported to different degrees by the same data; a fact that P-values cannot reflect but Bayes factors can. It is argued that appropriate conclusions match the Bayesian inferences, but not those based on significance testing, where they disagree.},
	author = {Dienes, Zoltan and Mclatchie, Neil},
	date = {2018/02/01},
	date-added = {2024-09-23 15:12:32 +0900},
	date-modified = {2024-09-23 15:12:32 +0900},
	doi = {10.3758/s13423-017-1266-z},
	id = {Dienes2018},
	isbn = {1531-5320},
	journal = {Psychonomic Bulletin \& Review},
	number = {1},
	pages = {207--218},
	title = {Four reasons to prefer Bayesian analyses over significance testing},
	url = {https://doi.org/10.3758/s13423-017-1266-z},
	volume = {25},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3758/s13423-017-1266-z}}

@article{vandenBergh+2020,
  author = {Don van den Bergh, Johnny van Doorn, Maarten Marsman, Tim Draws, Erik-Jan van Kesteren, Koen Derks, Fabian Dablander, Quentin F. Gronau, Šimon Kucharský, Akash R. Komarlu Narendra Gupta, Alexandra Sarafoglou, Jan G. Voelkel, Angelika Stefan, Alexander Ly, Max Hinne, Dora Matzke and Eric-Jan Wagenmakers},
  year = {2020},
  title = {{A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP}},
  journal = {L’Année psychologique},
  volume = {120},
  number = {},
  pages = {73-96},
  url = {https://doi.org/10.3917/anpsy1.201.0073}
}

@article{Rouder+2016,
	abstract = {Analysis of variance (ANOVA), the workhorse analysis of experimental designs, consists of F-tests of main effects and interactions. Yet, testing, including traditional ANOVA, has been recently critiqued on a number of theoretical and practical grounds. In light of these critiques, model comparison and model selection serve as an attractive alternative. Model comparison differs from testing in that one can support a null or nested model vis-a-vis a more general alternative by penalizing more flexible models. We argue this ability to support more simple models allows for more nuanced theoretical conclusions than provided by traditional ANOVA F-tests. We provide a model comparison strategy and show how ANOVA models may be reparameterized to better address substantive questions in data analysis.},
	author = {Rouder, Jeffrey N. and Engelhardt, Christopher R. and McCabe, Simon and Morey, Richard D.},
	date = {2016/12/01},
	date-added = {2024-09-23 17:04:20 +0900},
	date-modified = {2024-09-23 17:04:20 +0900},
	doi = {10.3758/s13423-016-1026-5},
	id = {Rouder2016},
	isbn = {1531-5320},
	journal = {Psychonomic Bulletin \& Review},
	number = {6},
	pages = {1779--1786},
	title = {Model comparison in ANOVA},
	url = {https://doi.org/10.3758/s13423-016-1026-5},
	volume = {23},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.3758/s13423-016-1026-5}}
@article{Shapiro-Wilk1965,
    author = {Shapiro, S. S. and Wilk, M. B.},
    title = "{An analysis of variance test for normality (complete samples)†}",
    journal = {Biometrika},
    volume = {52},
    number = {3-4},
    pages = {591-611},
    year = {1965},
    month = {12},
    issn = {0006-3444},
    doi = {10.1093/biomet/52.3-4.591},
    url = {https://doi.org/10.1093/biomet/52.3-4.591},
    eprint = {https://academic.oup.com/biomet/article-pdf/52/3-4/591/962907/52-3-4-591.pdf},
}

@article{Brown-Forsythe1974,
	author = {Morton B. Brown and Alan B. Forsythe},
	doi = {10.1080/01621459.1974.10482955},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1974.10482955},
	journal = {Journal of the American Statistical Association},
	number = {346},
	pages = {364--367},
	publisher = {ASA Website},
	title = {Robust Tests for the Equality of Variances},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10482955},
	volume = {69},
	year = {1974},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10482955},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1974.10482955}}
@article{Mauchly1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235878},
 author = {John W. Mauchly},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {204--209},
 publisher = {Institute of Mathematical Statistics},
 title = {Significance Test for Sphericity of a Normal n-Variate Distribution},
 urldate = {2024-09-23},
 volume = {11},
 year = {1940}
}

@article{Tijmstra2018,
	abstract = {This article explores whether the null hypothesis significance testing (NHST) framework provides a sufficient basis for the evaluation of statistical model assumptions. It is argued that while NHST-based tests can provide some degree of confirmation for the model assumption that is evaluated---formulated as the null hypothesis---these tests do not inform us of the degree of support that the data provide for the null hypothesis and to what extent the null hypothesis should be considered to be plausible after having taken the data into account. Addressing the prior plausibility of the model assumption is unavoidable if the goal is to determine how plausible it is that the model assumption holds. Without assessing the prior plausibility of the model assumptions, it remains fully uncertain whether the model of interest gives an adequate description of the data and thus whether it can be considered valid for the application at hand. Although addressing the prior plausibility is difficult, ignoring the prior plausibility is not an option if we want to claim that the inferences of our statistical model can be relied upon.},
	author = {Tijmstra, Jesper},
	date = {2018/04/01},
	date-added = {2024-09-23 17:19:55 +0900},
	date-modified = {2024-09-23 17:19:55 +0900},
	doi = {10.3758/s13423-018-1447-4},
	id = {Tijmstra2018},
	isbn = {1531-5320},
	journal = {Psychonomic Bulletin \& Review},
	number = {2},
	pages = {548--559},
	title = {Why checking model assumptions using null hypothesis significance tests does not suffice: A plea for plausibility},
	url = {https://doi.org/10.3758/s13423-018-1447-4},
	volume = {25},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3758/s13423-018-1447-4}}

@article{Rouder+2012,
	abstract = {Bayes factors have been advocated as superior to p-values for assessing statistical evidence in data. Despite the advantages of Bayes factors and the drawbacks of p-values, inference by p-values is still nearly ubiquitous. One impediment to the adoption of Bayes factors is a lack of practical development, particularly a lack of ready-to-use formulas and algorithms. In this paper, we discuss and expand a set of default Bayes factor tests for ANOVA designs. These tests are based on multivariate generalizations of Cauchy priors on standardized effects, and have the desirable properties of being invariant with respect to linear transformations of measurement units. Moreover, these Bayes factors are computationally convenient, and straightforward sampling algorithms are provided. We cover models with fixed, random, and mixed effects, including random interactions, and do so for within-subject, between-subject, and mixed designs. We extend the discussion to regression models with continuous covariates. We also discuss how these Bayes factors may be applied in nonlinear settings, and show how they are useful in differentiating between the power law and the exponential law of skill acquisition. In sum, the current development makes the computation of Bayes factors straightforward for the vast majority of designs in experimental psychology.},
	author = {Jeffrey N. Rouder and Richard D. Morey and Paul L. Speckman and Jordan M. Province},
	doi = {https://doi.org/10.1016/j.jmp.2012.08.001},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	keywords = {Bayes factor, Model selection, Bayesian statistics, Linear models},
	number = {5},
	pages = {356-374},
	title = {Default Bayes factors for ANOVA designs},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249612000806},
	volume = {56},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249612000806},
	bdsk-url-2 = {https://doi.org/10.1016/j.jmp.2012.08.001}}

@article{Gelman-Shalizi2013,
	abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
	author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
	doi = {https://doi.org/10.1111/j.2044-8317.2011.02037.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.2011.02037.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {1},
	pages = {8-38},
	title = {Philosophy and the practice of Bayesian statistics},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x},
	volume = {66},
	year = {2013},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.2011.02037.x}}
@misc{Gelman+2020,
      title={Bayesian Workflow}, 
      author={Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian Bürkner and Martin Modrák},
      year={2020},
      eprint={2011.01808},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2011.01808}, 
}
@book{Jeffreys1961,
  author = {Harold Jeffreys},
  year = {1961},
  title = {Theory of Probability},
  series = {},
  volume = {},
  edition = {3},
  url = {https://doi.org/10.1093/oso/9780198503682.001.0001},
  publisher = {Oxford University Press}
}

@article{Robert+2009,
	author = {Christian P. Robert and Nicolas Chopin and Judith Rousseau},
	doi = {10.1214/09-STS284},
	journal = {Statistical Science},
	keywords = {Bayes factor, Bayesian foundations, goodness of fit, Jeffreys's prior, Kullback divergence, noninformative prior, P-values, tests, σ-finite measure},
	number = {2},
	pages = {141 -- 172},
	publisher = {Institute of Mathematical Statistics},
	title = {{Harold Jeffreys's Theory of Probability Revisited}},
	url = {https://doi.org/10.1214/09-STS284},
	volume = {24},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1214/09-STS284}}
@article{Bayarri-Darcia-Donato2007,
    author = {Bayarri, M.J. and García-Donato, Gonzalo},
    title = "{Extending conventional priors for testing general hypotheses in linear models}",
    journal = {Biometrika},
    volume = {94},
    number = {1},
    pages = {135-152},
    year = {2007},
    month = {03},
    abstract = "{We consider that observations come from a general normal linear model and that it is desirable to test a simplifying null hypothesis about the parameters. We approach this problem from an objective Bayesian, model-selection perspective. Crucial ingredients for this approach are ‘proper objective priors’ to be used for deriving the Bayes factors. Jeffreys-Zellner-Siow priors have good properties for testing null hypotheses defined by specific values of the parameters in full-rank linear models. We extend these priors to deal with general hypotheses in general linear models, not necessarily of full rank. The resulting priors, which we call ‘conventional priors’, are expressed as a generalization of recently introduced ‘partially informative distributions’. The corresponding Bayes factors are fully automatic, easily computed and very reasonable. The methodology is illustrated for the change-point problem and the equality of treatments effects problem. We compare the conventional priors derived for these problems with other objective Bayesian proposals like the intrinsic priors. It is concluded that both priors behave similarly although interesting subtle differences arise. We adapt the conventional priors to deal with nonnested model selection as well as multiple-model comparison. Finally, we briefly address a generalization of conventional priors to nonnormal scenarios.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asm014},
    url = {https://doi.org/10.1093/biomet/asm014},
    eprint = {https://academic.oup.com/biomet/article-pdf/94/1/135/617595/asm014.pdf},
}

@article{Zellner-Siow1980,
	abstract = {Bayesian posterior odds ratios for frequently encountered hypotheses about parameters of the normal linear multiple regression model are derived and discussed. For the particular prior distributions utilized, it is found that the posterior odds ratios can be well approximated by functions that are monotonic in usual sampling theoryF statistics. Some implications of this finding and the relation of our work to the pioneering work of Jeffreys and others are considered. Tabulations of odds ratios are provided and discussed.},
	author = {Zellner, A. and Siow, A.},
	date = {1980/02/01},
	date-added = {2024-09-23 19:22:46 +0900},
	date-modified = {2024-09-23 19:22:46 +0900},
	doi = {10.1007/BF02888369},
	id = {Zellner1980},
	isbn = {0041-0241},
	journal = {Trabajos de Estadistica Y de Investigacion Operativa},
	number = {1},
	pages = {585--603},
	title = {Posterior odds ratios for selected regression hypotheses},
	url = {https://doi.org/10.1007/BF02888369},
	volume = {31},
	year = {1980},
	bdsk-url-1 = {https://doi.org/10.1007/BF02888369}}

@article{Wetzels+2011,
	abstract = { Statistical inference in psychology has traditionally relied heavily on p-value significance testing. This approach to drawing conclusions from data, however, has been widely criticized, and two types of remedies have been advocated. The first proposal is to supplement p values with complementary measures of evidence, such as effect sizes. The second is to replace inference with Bayesian measures of evidence, such as the Bayes factor. The authors provide a practical comparison of p values, effect sizes, and default Bayes factors as measures of statistical evidence, using 855 recently published t tests in psychology. The comparison yields two main results. First, although p values and default Bayes factors almost always agree about what hypothesis is better supported by the data, the measures often disagree about the strength of this support; for 70\% of the data sets for which the p value falls between .01 and .05, the default Bayes factor indicates that the evidence is only anecdotal. Second, effect sizes can provide additional evidence to p values and default Bayes factors. The authors conclude that the Bayesian approach is comparatively prudent, preventing researchers from overestimating the evidence in favor of an effect. },
	author = {Ruud Wetzels and Dora Matzke and Michael D. Lee and Jeffrey N. Rouder and Geoffrey J. Iverson and Eric-Jan Wagenmakers},
	doi = {10.1177/1745691611406923},
	eprint = {https://doi.org/10.1177/1745691611406923},
	journal = {Perspectives on Psychological Science},
	note = {PMID: 26168519},
	number = {3},
	pages = {291-298},
	title = {Statistical Evidence in Experimental Psychology: An Empirical Comparison Using 855 t Tests},
	url = {https://doi.org/10.1177/1745691611406923},
	volume = {6},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1177/1745691611406923}}

@article{Gelman2005,
	author = {Andrew Gelman},
	doi = {10.1214/009053604000001048},
	journal = {The Annals of Statistics},
	keywords = {ANOVA, Bayesian inference, fixed effects, hierarchical model, Linear regression, multilevel model, random effects, variance components},
	number = {1},
	pages = {1 -- 53},
	publisher = {Institute of Mathematical Statistics},
	title = {{Analysis of variance---why it is more important than ever}},
	url = {https://doi.org/10.1214/009053604000001048},
	volume = {33},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1214/009053604000001048}}

@inbook{Tweney2014,
	abstract = {Abstract First used by behavioral scientists in the 1930s, use of Analysis of Variance (ANOVA) grew quickly after World War II, and exactly paralleled incorporation of statistical significance testing in general. Detailed consideration of this history suggests several reasons for the pattern of slow growth, followed by rapid institutionalization. In particular, ANOVA stood duty as a warrant of scientific legitimacy among behavioral scientists, a fact that may also be relevant to understanding recent critiques by psychologists of its overuse and misuse.},
	author = {Tweney, Ryan D.},
	booktitle = {Wiley StatsRef: Statistics Reference Online},
	doi = {https://doi.org/10.1002/9781118445112.stat06304},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06304},
	isbn = {9781118445112},
	keywords = {ANOVA, analysis of variance, Fisher, significance testing, history},
	publisher = {John Wiley & Sons, Ltd},
	title = {History of Analysis of Variance},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06304},
	year = {2014},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06304},
	bdsk-url-2 = {https://doi.org/10.1002/9781118445112.stat06304}}
@article{Kelter2022,
  author = {Kelter, Riko},
  title = {bayesanova: An R package for Bayesian Inference in the Analysis of Variance via Markov Chain Monte Carlo in Gaussian Mixture Models},
  journal = {The R Journal},
  year = {2022},
  note = {https://rjournal.github.io/},
  volume = {14},
  issue = {1},
  issn = {2073-4859},
  pages = {54-78}
}

@article{Robert2016,
	abstract = {This note is a discussion commenting on the paper by Ly et al. on ``Harold Jeffreys's Default Bayes Factor Hypothesis Tests: Explanation, Extension, and Application in Psychology'' and on the perceived shortcomings of the classical Bayesian approach to testing, while reporting on an alternative approach advanced by Kamary et al. (2014) as a solution to this quintessential inference problem.},
	author = {Christian P. Robert},
	doi = {https://doi.org/10.1016/j.jmp.2015.08.002},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	keywords = {Testing of hypotheses, Bayesian inference, Bayes factor, Evidence, Decision theory, Loss function, Consistency, Mixtures of distributions},
	note = {Bayes Factors for Testing Hypotheses in Psychological Research: Practical Relevance and New Developments},
	pages = {33-37},
	title = {The expected demise of the Bayes factor},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249615000504},
	volume = {72},
	year = {2016},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249615000504},
	bdsk-url-2 = {https://doi.org/10.1016/j.jmp.2015.08.002}}
@misc{Kamary+2018,
      title={Testing hypotheses via a mixture estimation model}, 
      author={Kaniav Kamary and Kerrie Mengersen and Christian P. Robert and Judith Rousseau},
      year={2018},
      eprint={1412.2044},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1412.2044}, 
}

@article{Kruschke2018,
	abstract = { This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors. },
	author = {John K. Kruschke},
	doi = {10.1177/2515245918771304},
	eprint = {https://doi.org/10.1177/2515245918771304},
	journal = {Advances in Methods and Practices in Psychological Science},
	number = {2},
	pages = {270-280},
	title = {Rejecting or Accepting Parameter Values in Bayesian Estimation},
	url = {https://doi.org/10.1177/2515245918771304},
	volume = {1},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1177/2515245918771304}}

@article{Kruskal-Wallis1952,
	author = {William H. Kruskal and W. Allen Wallis},
	doi = {10.1080/01621459.1952.10483441},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483441},
	journal = {Journal of the American Statistical Association},
	number = {260},
	pages = {583--621},
	publisher = {ASA Website},
	title = {Use of Ranks in One-Criterion Variance Analysis},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
	volume = {47},
	year = {1952},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1952.10483441}}

@article{vandenBergh+2023,
	abstract = { Analysis of variance (ANOVA) is widely used to assess the influence of one or more experimental (or quasi-experimental) manipulations on a continuous outcome. Traditionally, ANOVA is carried out in a frequentist manner using p values, but a Bayesian alternative has been proposed. Assuming that the proposed Bayesian ANOVA is closely modeled after its frequentist counterpart, one may be surprised to find that the two can yield very different conclusions when the design involves multiple repeated-measures factors. We illustrate such a discrepancy with a real data set from a two-factorial within-subject experiment. For this data set, the results of a frequentist and Bayesian ANOVA are in a disagreement about which main effect accounts for the variance in the data. The reason for this disagreement is that frequentist and the proposed Bayesian ANOVA use different model specifications. As currently implemented, the proposed Bayesian ANOVA assumes that there are no individual differences in the magnitude of effects. We suspect that this assumption is neither obvious to nor desired by most analysts because it is untenable in most applications. We argue here that the Bayesian ANOVA should be revised to allow for individual differences. As a default, we suggest the standard frequentist model specification but discuss a recently proposed alternative and provide guidance on how to choose the appropriate model specification. We end by discussing the implications of the revised model specification for previously published results of Bayesian ANOVAs. },
	author = {Don van den Bergh and Eric-Jan Wagenmakers and Frederik Aust},
	doi = {10.1177/25152459231168024},
	eprint = {https://doi.org/10.1177/25152459231168024},
	journal = {Advances in Methods and Practices in Psychological Science},
	number = {2},
	pages = {25152459231168024},
	title = {Bayesian Repeated-Measures Analysis of Variance: An Updated Methodology Implemented in JASP},
	url = {https://doi.org/10.1177/25152459231168024},
	volume = {6},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1177/25152459231168024}}

@article{Spiegelhalter+1994,
	abstract = {SUMMARY Statistical issues in conducting randomized trials include the choice of a sample size, whether to stop a trial early and the appropriate analysis and interpretation of the trial results. At each of these stages, evidence external to the trial is useful, but generally such evidence is introduced in an unstructured and informal manner. We argue that a Bayesian approach allows a formal basis for using external evidence and in addition provides a rational way for dealing with issues such as the ethics of randomization, trials to show treatment equivalence, the monitoring of accumulating data and the prediction of the consequences of continuing a study. The motivation for using this methodology is practical rather than ideological.},
	author = {Spiegelhalter, David J. and Freedman, Laurence S. and Parmar, Mahesh K. B.},
	doi = {https://doi.org/10.2307/2983527},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2983527},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	keywords = {clinical trials, ethics, power, prediction, prior distribution, range of equivalence, shrinkage, stopping rules, subjective probabilities},
	number = {3},
	pages = {357-387},
	title = {Bayesian Approaches to Randomized Trials},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2983527},
	volume = {157},
	year = {1994},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2983527},
	bdsk-url-2 = {https://doi.org/10.2307/2983527}}
@article{Freedman+1984,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2530902},
 abstract = {A method is described of eliciting a `range of equivalence', i.e. a range of differences between two treatments over which a group of clinical trial participants would have no clear preference for either treatment. This range of equivalence is then incorporated into a formal stopping rule for the trial using an extension of the group sequential design. Tables for the implementation of the design are presented. The method is discussed in the context of other sequential-trial designs.},
 author = {L. S. Freedman and D. Lowe and P. Macaskill},
 journal = {Biometrics},
 number = {3},
 pages = {575--586},
 publisher = {International Biometric Society},
 title = {Stopping Rules for Clinical Trials Incorporating Clinical Opinion},
 urldate = {2024-09-24},
 volume = {40},
 year = {1984}
}
@book{Cohen1988,
  author = {Jacob Cohen},
  year = {1988},
  title = {Statistical Power Analysis for the Behavioral Sciences },
  series = {},
  volume = {},
  edition = {2},
  url = {https://doi.org/10.4324/9780203771587},
  publisher = {Routledge}
}
@misc{Kim2024,
      title={Statistics in Survey Sampling}, 
      author={Jae Kwang Kim},
      year={2024},
      eprint={2401.07625},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2401.07625}, 
}
@article{Rubin1976,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2335739},
 abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
 author = {Donald B. Rubin},
 journal = {Biometrika},
 number = {3},
 pages = {581--592},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Inference and Missing Data},
 urldate = {2024-09-24},
 volume = {63},
 year = {1976}
}

@article{Horvitz-Thompson1952,
	author = {D. G. Horvitz and D. J. Thompson},
	doi = {10.1080/01621459.1952.10483446},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446},
	journal = {Journal of the American Statistical Association},
	number = {260},
	pages = {663--685},
	publisher = {ASA Website},
	title = {A Generalization of Sampling Without Replacement from a Finite Universe},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446},
	volume = {47},
	year = {1952},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1952.10483446}}
@article{Sen1953,
  author = {A. R. Sen},
  year = {1953},
  title = {On the estimate of the variance in sampling with varying probabilities},
  journal = {Journal of the Indian Society of Agricultural Statistics},
  volume = {5},
  number = {},
  pages = {119-127},
  url = {}
}

@article{Yates-Grundy1953,
	abstract = {SUMMARY In selection with probability proportional to size x from within strata without replacement, the usual method of selection gives rise to bias in the estimate of the total of a variate y derived by weighting the units by weights proportional to 1/x. By means of numerical examples it is shown that the amount of this bias is usually quite trivial. If, however, unbiased estimates are required, the true total probabilities of selection of the different units can be calculated easily for samples of 2, and with considerably more labour for samples of 3. The bias in the ordinary formula for the estimation of error is also investigated, and the formula is shown to be reasonably accurate. Horvitz and Thompson have given an unbiased estimator of the error variance, but this is shown to be inefficient and a new unbiased estimator is given. A method of revising the size measures so that with the usual method of selection the true total probabilities of selection are proportional to the original size measures is given for samples of 2. Horvitz and Thompson's solution of this problem does not appear to give satisfactory approximations in the cases met with in practice. The selection of successive members of a sample with arbitrary sets of probabilities chosen solely so that the total probabilities shall be proportional to the original size measures, which has been advocated in various quarters, is criticized.},
	author = {Yates, F. and Grundy, P. M.},
	doi = {https://doi.org/10.1111/j.2517-6161.1953.tb00140.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1953.tb00140.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {2},
	pages = {253-261},
	title = {Selection Without Replacement from Within Strata with Probability Proportional to Size},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1953.tb00140.x},
	volume = {15},
	year = {1953},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1953.tb00140.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1953.tb00140.x}}

@book{Sarndal+1992,
  author = {Carl-Erik Särndal and Bengt Swensson and Jan Wretman},
  year = {1992},
  title = {Model Assisted Survey Sampling},
  series = {},
  volume = {},
  edition = {},
  url = {https://link.springer.com/book/9780387406206},
  publisher = {Springer New York}
}

@article{Deville-Sarndal1992,
	author = {Jean-Claude Deville and Carl-Erik S{\"a}rndal},
	doi = {10.1080/01621459.1992.10475217},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1992.10475217},
	journal = {Journal of the American Statistical Association},
	number = {418},
	pages = {376--382},
	publisher = {ASA Website},
	title = {Calibration Estimators in Survey Sampling},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475217},
	volume = {87},
	year = {1992},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475217},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1992.10475217}}
@article{Gelman-Little1997,
  author = {Andrew Gelman and Thomas Little},
  year = {1997},
  title = {Poststratification into Many Categories using Hierarchical Logistic Regression},
  journal = {Survey Methodology},
  volume = {},
  number = {1997002},
  pages = {},
  url = {https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X19970023616}
}
@article{Rubin1978,
 ISSN = {00905364, 21688966},
 URL = {http://www.jstor.org/stable/2958688},
 abstract = {Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabilistic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. Moreover, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical randomized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.},
 author = {Donald B. Rubin},
 journal = {The Annals of Statistics},
 number = {1},
 pages = {34--58},
 publisher = {Institute of Mathematical Statistics},
 title = {Bayesian Inference for Causal Effects: The Role of Randomization},
 urldate = {2024-09-24},
 volume = {6},
 year = {1978}
}
@article{Rubin1978MI,
  author = {Donald B. Rubin},
  year = {1978},
  title = {Multiple Imputations in Sample Surveys - A Phenomenological Bayesian Approach to Nonresponse},
  journal = {Proceedings of the Survey Research Methods Section, ASA},
  volume = {},
  number = {},
  pages = {20-28},
  url = {http://www.asasrms.org/Proceedings/y1978f.html}
}
@article{Aitken1936, title={IV.—On Least Squares and Linear Combination of Observations}, volume={55}, DOI={10.1017/S0370164600014346}, journal={Proceedings of the Royal Society of Edinburgh}, author={Aitken, A. C.}, year={1936}, pages={42–48}}
@article{Zieschang1990,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2289595},
 abstract = {The widely used Principal Person method of weighting households in federal government surveys uses external post-Censal information on population to improve survey sample weights by a form of poststratification. While the Principal Person Methodology can be viewed as part of a procedure to adjust for nonresponse and undercoverage, it is not oriented for efficiently incorporating ancillary information or combining information from multiple surveys into survey estimates of subdomain totals. In this article a generalized least squares adjustment algorithm is shown to incorporate ancillary information in a way that, in principle, reduces the design variance of estimated survey totals. The flexibility of the method is exploited in an application to the Consumer Expenditure Survey that makes use of its "weighting control" and "composition" features.},
 author = {Kimberly D. Zieschang},
 journal = {Journal of the American Statistical Association},
 number = {412},
 pages = {986--1001},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Sample Weighting Methods and Estimation of Totals in the Consumer Expenditure Survey},
 urldate = {2024-09-24},
 volume = {85},
 year = {1990}
}

@article{Deville-Sarndal1993,
	author = {Jean-Claude Deville, Carl-Erik S{\"a}rndal and Olivier Sautory},
	doi = {10.1080/01621459.1993.10476369},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476369},
	journal = {Journal of the American Statistical Association},
	number = {423},
	pages = {1013--1020},
	publisher = {ASA Website},
	title = {Generalized Raking Procedures in Survey Sampling},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476369},
	volume = {88},
	year = {1993},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476369},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1993.10476369}}

@article{Firth-Bennett1998,
	abstract = {In the estimation of a population mean or total from a random sample, certain methods based on linear models are known to be automatically design consistent, regardless of how well the underlying model describes the population. A sufficient condition is identified for this type of robustness to model failure; the condition, which we call `internal bias calibration', relates to the combination of a model and the method used to fit it. Included among the internally bias-calibrated models, in addition to the aforementioned linear models, are certain canonical link generalized linear models and nonparametric regressions constructed from them by a particular style of local likelihood fitting. Other models can often be made robust by using a suboptimal fitting method. Thus the class of model-based, but design consistent, analyses is enlarged to include more realistic models for certain types of survey variable such as binary indicators and counts. Particular applications discussed are the estimation of the size of a population subdomain, as arises in tax auditing for example, and the estimation of a bootstrap tail probability.},
	author = {Firth, D. and Bennett, K. E.},
	doi = {https://doi.org/10.1111/1467-9868.00105},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00105},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {Auditing, Bias calibration, Bootstrap acceleration, Control variate, Finite population, Generalized linear model, Importance sampling, Instrumental variable, Local likelihood, Logistic regression, Smoothing, Spline, Stratification, Survey sampling},
	number = {1},
	pages = {3-21},
	title = {Robust models in probability sampling},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00105},
	volume = {60},
	year = {1998},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00105},
	bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00105}}
@article{Godambe-Joshi1965,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2239112},
 author = {V. P. Godambe and V. M. Joshi},
 journal = {The Annals of Mathematical Statistics},
 number = {6},
 pages = {1707--1722},
 publisher = {Institute of Mathematical Statistics},
 title = {Admissibility and Bayes Estimation in Sampling Finite Populations. I},
 urldate = {2024-09-24},
 volume = {36},
 year = {1965}
}

@article{Isaki-Fuller1982,
	author = {Cary T. Isaki and Wayne A. Fuller},
	doi = {10.1080/01621459.1982.10477770},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1982.10477770},
	journal = {Journal of the American Statistical Association},
	number = {377},
	pages = {89--96},
	publisher = {ASA Website},
	title = {Survey Design under the Regression Superpopulation Model},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1982.10477770},
	volume = {77},
	year = {1982},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1982.10477770},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1982.10477770}}

@article{Wu-Sitter2001,
	author = {Changbao Wu and Randy R Sitter},
	doi = {10.1198/016214501750333054},
	eprint = {https://doi.org/10.1198/016214501750333054},
	journal = {Journal of the American Statistical Association},
	number = {453},
	pages = {185--193},
	publisher = {ASA Website},
	title = {A Model-Calibration Approach to Using Complete Auxiliary Information From Survey Data},
	url = {https://doi.org/10.1198/016214501750333054},
	volume = {96},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1198/016214501750333054}}
@article{Kim-Haziza2014,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/26432548},
 abstract = {Statistical inference with missing data requires assumptions about the population or about the response probability. Doubly robust (DR) estimators use both relationships to estimate the parameters of interest, so that they are consistent even when one of the models is misspecified. In this paper, we propose a method of computing propensity scores that leads to DR estimation. In addition, we discuss DR variance estimation so that the resulting inference is doubly robust. Some asymptotic properties are discussed. Results from two limited simulation studies are also presented.},
 author = {Jae Kwang Kim and David Haziza},
 journal = {Statistica Sinica},
 number = {1},
 pages = {375--394},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {DOUBLY ROBUST INFERENCE WITH MISSING DATA IN SURVEY SAMPLING},
 urldate = {2024-09-24},
 volume = {24},
 year = {2014}
}
@article{Berg+2016,
    author = {Berg, Emily and Kim, Jae-Kwang and Skinner, Chris},
    title = "{Imputation Under Informative Sampling}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {4},
    number = {4},
    pages = {436-462},
    year = {2016},
    month = {12},
    abstract = "{Imputed values in surveys are often generated under the assumption that the sampling mechanism is non-informative (or ignorable) and the study variable is missing at random (MAR). When the sampling design is informative, the assumption of MAR in the population does not necessarily imply MAR in the sample. In this case, the classical method of imputation using a model fitted to the sample data does not in general lead to unbiased estimation. To overcome this problem, we consider alternative approaches to imputation assuming MAR in the population. We compare the alternative imputation procedures through simulation and an application to estimation of mean erosion using data from the Conservation Effects Assessment Project.}",
    issn = {2325-0984},
    doi = {10.1093/jssam/smw032},
    url = {https://doi.org/10.1093/jssam/smw032},
    eprint = {https://academic.oup.com/jssam/article-pdf/4/4/436/8721804/smw032.pdf},
}
@article{Sugden-Smith1984,
    author = {Sugden, R. A. and Smith, T. M. F.},
    title = "{Ignorable and informative designs in survey sampling inference}",
    journal = {Biometrika},
    volume = {71},
    number = {3},
    pages = {495-506},
    year = {1984},
    month = {12},
    abstract = "{The role of the sample selection mechanism in a model-based approach to finite population inference is examined. When the data analyst has only partial information on the sample design then a design which is ignorable when known fully may become informative. Conditions under which partially known designs can be ignored are established and examined for some standard designs. The results are illustrated by an example used by Scott (1977).}",
    issn = {0006-3444},
    doi = {10.1093/biomet/71.3.495},
    url = {https://doi.org/10.1093/biomet/71.3.495},
    eprint = {https://academic.oup.com/biomet/article-pdf/71/3/495/718101/71-3-495.pdf},
}

@article{Little1992,
	author = {Roderick J. A. Little},
	doi = {10.1080/01621459.1992.10476282},
	eprint = {https://doi.org/10.1080/01621459.1992.10476282},
	journal = {Journal of the American Statistical Association},
	number = {420},
	pages = {1227--1237},
	publisher = {ASA Website},
	title = {Regression with Missing X's: A Review},
	url = {https://doi.org/10.1080/01621459.1992.10476282},
	volume = {87},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.1992.10476282}}
@article{Enders+2001,
author = {Craig K. Enders and Deborah L. Bandalos},
title = {The Relative Performance of Full Information Maximum Likelihood Estimation for Missing Data in Structural Equation Models},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
volume = {8},
number = {3},
pages = {430--457},
year = {2001},
publisher = {Routledge},
doi = {10.1207/S15328007SEM0803\_5},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1207/S15328007SEM0803_5
    

}}


@article{Kalton-Kish1984,
	author = {Graham Kalton and Leslie Kish},
	doi = {10.1080/03610928408828805},
	eprint = {https://doi.org/10.1080/03610928408828805},
	journal = {Communications in Statistics - Theory and Methods},
	number = {16},
	pages = {1919--1939},
	publisher = {Taylor \& Francis},
	title = {Some efficient random imputation methods},
	url = {https://doi.org/10.1080/03610928408828805},
	volume = {13},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1080/03610928408828805}}
@article{Kim-Fuller2004,
    author = {Kim, Jae Kwang and Fuller, Wayne},
    title = "{Fractional hot deck imputation}",
    journal = {Biometrika},
    volume = {91},
    number = {3},
    pages = {559-578},
    year = {2004},
    month = {09},
    abstract = "{To compensate for item nonresponse, hot deck imputation procedures replace missing values with values that occur in the sample. Fractional hot deck imputation replaces each missing observation with a set of imputed values and assigns a weight to each imputed value. Under the model in which observations in an imputation cell are independently and identically distributed, fractional hot deck imputation is shown to be an effective imputation procedure. A consistent replication variance estimation procedure for estimators computed with fractional imputation is suggested. Simulations show that fractional imputation and the suggested variance estimator are superior to multiple imputation estimators in general, and much superior to multiple imputation for estimating the variance of a domain mean.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/91.3.559},
    url = {https://doi.org/10.1093/biomet/91.3.559},
    eprint = {https://academic.oup.com/biomet/article-pdf/91/3/559/623932/910559.pdf},
}
@article{Enders+2020,
  author = {C. K. Enders and H. Du and B. T. Keller},
  year = {2020},
  title = {{A Model-based Imputation Procedure for Multilevel Regression Models with Random Coefficients, Interaction Effects, and Nonlinear Terms}},
  journal = {Psychological Methods},
  volume = {25},
  number = {1},
  pages = {88-112},
  url = {https://psycnet.apa.org/doi/10.1037/met0000228}
}


@book{vanBuuren2018,
 abstract = {},
 address = {Boca Raton, FL.},
 author = {van Buuren, Stef},
 editor = {},
 keywords = {},
 pages = {},
 publisher = {CRC Press},
 title = {{Flexible Imputation of Missing Data}},
 volume = {},
 edition = {2},
 year = {2018},
 url = {https://stefvanbuuren.name/fimd/},
}

@article{Royston-White2011,
 title={Multiple Imputation by Chained Equations (MICE): Implementation in Stata},
 volume={45},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v045i04},
 doi={10.18637/jss.v045.i04},
 abstract={Missing data are a common occurrence in real datasets. For epidemiological and prognostic factors studies in medicine, multiple imputation is becoming the standard route to estimating models with missing covariate data under a missing-at-random assumption. We describe &amp;lt;b&amp;gt;ice&amp;lt;/b&amp;gt;, an implementation in Stata of the MICE approach to multiple imputation. Real data from an observational study in ovarian cancer are used to illustrate the most important of the many options available with &amp;lt;b&amp;gt;ice&amp;lt;/b&amp;gt;. We remark briefly on the new database architecture and procedures for multiple imputation introduced in releases 11 and 12 of Stata.},
 number={4},
 journal={Journal of Statistical Software},
 author={Royston, Patrick and White, Ian R.},
 year={2011},
 pages={1–20}
}

@article{vanBuuren+2006,
	author = {Stef Van Buuren, J. P.L. Brand, C. G.M. Groothuis-Oudshoorn and D. B. Rubin},
	doi = {10.1080/10629360600810434},
	eprint = {https://doi.org/10.1080/10629360600810434},
	journal = {Journal of Statistical Computation and Simulation},
	number = {12},
	pages = {1049--1064},
	publisher = {Taylor \& Francis},
	title = {Fully conditional specification in multivariate imputation},
	url = {https://doi.org/10.1080/10629360600810434},
	volume = {76},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1080/10629360600810434}}
@article{vanBuuren-Groothuis-Oudshoorn2011,
 title={mice: Multivariate Imputation by Chained Equations in R},
 volume={45},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v045i03},
 doi={10.18637/jss.v045.i03},
 abstract={The R package &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt;, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
 number={3},
 journal={Journal of Statistical Software},
 author={van Buuren, Stef and Groothuis-Oudshoorn, Karin},
 year={2011},
 pages={1–67}
}

@article{Rezvan+2015,
	abstract = {Missing data are common in medical research, which can lead to a loss in statistical power and potentially biased results if not handled appropriately. Multiple imputation (MI) is a statistical method, widely adopted in practice, for dealing with missing data. Many academic journals now emphasise the importance of reporting information regarding missing data and proposed guidelines for documenting the application of MI have been published. This review evaluated the reporting of missing data, the application of MI including the details provided regarding the imputation model, and the frequency of sensitivity analyses within the MI framework in medical research articles.},
	author = {Hayati Rezvan, Panteha and Lee, Katherine J. and Simpson, Julie A.},
	date = {2015/04/07},
	date-added = {2024-09-25 15:49:14 +0900},
	date-modified = {2024-09-25 15:49:14 +0900},
	doi = {10.1186/s12874-015-0022-1},
	id = {Hayati Rezvan2015},
	isbn = {1471-2288},
	journal = {BMC Medical Research Methodology},
	number = {1},
	pages = {30},
	title = {The rise of multiple imputation: a review of the reporting and implementation of the method in medical research},
	url = {https://doi.org/10.1186/s12874-015-0022-1},
	volume = {15},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1186/s12874-015-0022-1}}

@article{野間久史2017,
	author = {野間 久史},
	doi = {10.5023/jappstat.46.67},
	journal = {応用統計学},
	number = {2},
	pages = {67-86},
	title = {連鎖方程式による多重代入法},
	volume = {46},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.5023/jappstat.46.67}}

@article{Meng1994,
	author = {Xiao-Li Meng},
	doi = {10.1214/ss/1177010269},
	journal = {Statistical Science},
	keywords = {Congeniality, importance sampling, incomplete data, missing data, nonresponse, Normalizing constants, public-use data file, Randomization, self-efficiency},
	number = {4},
	pages = {538 -- 558},
	publisher = {Institute of Mathematical Statistics},
	title = {{Multiple-Imputation Inferences with Uncongenial Sources of Input}},
	url = {https://doi.org/10.1214/ss/1177010269},
	volume = {9},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1214/ss/1177010269}}
@article{逸見昌之2014,
  author = {逸見昌之},
  year = {2014},
  title = {欠測データに対するセミパラメトリックな解析法――その理論的背景について――},
  journal = {統計数理},
  volume = {62},
  number = {1},
  pages = {103-122},
  url = {https://www.ism.ac.jp/editsec/toukei/pdf/62-1-103.pdf}
}
@article{Baker+2013,
    author = {Baker, Reg and Brick, J. Michael and Bates, Nancy A. and Battaglia, Mike and Couper, Mick P. and Dever, Jill A. and Gile, Krista J. and Tourangeau, Roger},
    title = "{Summary Report of the AAPOR Task Force on Non-probability Sampling}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {1},
    number = {2},
    pages = {90-143},
    year = {2013},
    month = {09},
    issn = {2325-0984},
    doi = {10.1093/jssam/smt008},
    url = {https://doi.org/10.1093/jssam/smt008},
    eprint = {https://academic.oup.com/jssam/article-pdf/1/2/90/2774724/smt008.pdf},
}
@techreport{AAOR2013,
  author = {AAOR},
  institution = {American Association for Public Opinion Research},
  title = {Report of the AAPOR Task Force on Non-Probability Sampling},
  year = {2013}
}

@article{Morikawa-Kim2021,
	author = {Kosuke Morikawa and Jae Kwang Kim},
	doi = {10.1214/21-AOS2070},
	journal = {The Annals of Statistics},
	keywords = {Estimating functions, Identification, incomplete data, not missing at random (NMAR), semiparametric efficient estimation},
	number = {5},
	pages = {2991 -- 3014},
	publisher = {Institute of Mathematical Statistics},
	title = {{Semiparametric optimal estimation with nonignorable nonresponse data}},
	url = {https://doi.org/10.1214/21-AOS2070},
	volume = {49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/21-AOS2070}}
@article{Elliott-Valliant2017,
 ISSN = {08834237, 21688745},
 URL = {http://www.jstor.org/stable/26408228},
 abstract = {Although selecting a probability sample has been the standard for decades when making inferences from a sample to a finite population, incentives are increasing to use nonprobability samples. In a world of "big data", large amounts of data are available that are faster and easier to collect than are probability samples. Design-based inference, in which the distribution for inference is generated by the random mechanism used by the sampler, cannot be used for nonprobability samples. One alternative is quasi-randomization in which pseudo-inclusion probabilities are estimated based on covariates available for samples and nonsample units. Another is superpopulation modeling for the analytic variables collected on the sample units in which the model is used to predict values for the nonsample units. We discuss the pros and cons of each approach.},
 author = {Michael R. Elliott and Richard Valliant},
 journal = {Statistical Science},
 number = {2},
 pages = {249--264},
 publisher = {Institute of Mathematical Statistics},
 title = {Inference for Nonprobability Samples},
 urldate = {2024-09-25},
 volume = {32},
 year = {2017}
}
@article{Dorie+2019,
 ISSN = {08834237, 21688745},
 URL = {https://www.jstor.org/stable/26771031},
 abstract = {Statisticians have made great progress in creating methods that reduce our reliance on parametric assumptions. However, this explosion in research has resulted in a breadth of inferential strategies that both create opportunities for more reliable inference as well as complicate the choices that an applied researcher has to make and defend. Relatedly, researchers advocating for new methods typically compare their method to at best 2 or 3 other causal inference strategies and test using simulations that may or may not be designed to equally tease out flaws in all the competing methods. The causal inference data analysis challenge, "Is Your SATT Where It's At?", launched as part of the 2016 Atlantic Causal Inference Conference, sought to make progress with respect to both of these issues. The researchers creating the data testing grounds were distinct from the researchers submitting methods whose efficacy would be evaluated. Results from 30 competitors across the two versions of the competition (black-box algorithms and do-it-yourself analyses) are presented along with post-hoc analyses that reveal information about the characteristics of causal inference strategies and settings that affect performance. The most consistent conclusion was that methods that flexibly model the response surface perform better overall than methods that fail to do so. Finally new methods are proposed that combine features of several of the top-performing submitted methods.},
 author = {Vincent Dorie and Jennifer Hill and Uri Shalit and Marc Scott and Dan Cervone},
 journal = {Statistical Science},
 number = {1},
 pages = {pp. 43--68},
 publisher = {Institute of Mathematical Statistics},
 title = {Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition},
 urldate = {2024-09-25},
 volume = {34},
 year = {2019}
}

@article{Kim-Tam2021,
	abstract = {Summary The statistical challenges in using big data for making valid statistical inference in the finite population have been well documented in literature. These challenges are due primarily to statistical bias arising from under-coverage in the big data source to represent the population of interest and measurement errors in the variables available in the data set. By stratifying the population into a big data stratum and a missing data stratum, we can estimate the missing data stratum by using a fully responding probability sample and hence the population as a whole by using a data integration estimator. By expressing the data integration estimator as a regression estimator, we can handle measurement errors in the variables in big data and also in the probability sample. We also propose a fully nonparametric classification method for identifying the overlapping units and develop a bias-corrected data integration estimator under misclassification errors. Finally, we develop a two-step regression data integration estimator to deal with measurement errors in the probability sample. An advantage of the approach advocated in this paper is that we do not have to make unrealistic missing-at-random assumptions for the methods to work. The proposed method is applied to the real data example using 2015--2016 Australian Agricultural Census data.},
	author = {Kim, Jae-Kwang and Tam, Siu-Ming},
	doi = {https://doi.org/10.1111/insr.12434},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12434},
	journal = {International Statistical Review},
	keywords = {Calibration weighting, Measurement error, Non-response, Regression estimation, Selection bias},
	number = {2},
	pages = {382-401},
	title = {Data Integration by Combining Big Data and Survey Sample Data for Finite Population Inference},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12434},
	volume = {89},
	year = {2021},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12434},
	bdsk-url-2 = {https://doi.org/10.1111/insr.12434}}
@article{key,
  
  journal = {Proceedings of Social Statistics Section},
  volume = {},
  number = {},
  pages = {203-206},
  url = {}
}
@inproceedings{Hartley1962,
  author = {H. O. Hartley},
  year = {1962},
  title = {Multiple Frame Surveys},
  booktitle = {Proceedings of Social Statistics Section},
  volume = {},
  pages = {203-206},
  url = {}
}

@article{Skinner-Rao1996,
	author = {C. J. Skinner and J. N. K. Rao},
	doi = {10.1080/01621459.1996.10476695},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476695},
	journal = {Journal of the American Statistical Association},
	number = {433},
	pages = {349--356},
	publisher = {ASA Website},
	title = {Estimation in Dual Frame Surveys with Complex Designs},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476695},
	volume = {91},
	year = {1996},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476695},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1996.10476695}}

@article{Meng2018,
	author = {Xiao-Li Meng},
	doi = {10.1214/18-AOAS1161SF},
	journal = {The Annals of Applied Statistics},
	keywords = {Bias-variance tradeoff, data confidentiality and privacy, data defect correlation, data defect index (d.d.i.), data quality-quantity tradeoff, Euler identity, Monte Carlo and Quasi Monte Carlo (MCQMC), non-response bias},
	number = {2},
	pages = {685 -- 726},
	publisher = {Institute of Mathematical Statistics},
	title = {{Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election}},
	url = {https://doi.org/10.1214/18-AOAS1161SF},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1214/18-AOAS1161SF}}

@techreport{Athey+2019,
  author = {Susan Athey and Raj Chetty and Guido W. Imbens and Hyunseung Kang},
  institution = {National Bureau of Economic Research},
  title = {The Surrogate Index: Combining Short-Term Proxies to Estimate Long-Term Treatment Effects More Rapidly and Precisely},
  year = {2019},
  doi = {10.3386/w26463},
}
@misc{Athey+2020,
      title={Combining Experimental and Observational Data to Estimate Treatment Effects on Long Term Outcomes}, 
      author={Susan Athey and Raj Chetty and Guido Imbens},
      year={2020},
      eprint={2006.09676},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2006.09676}, 
}
@misc{Park-Sasaki2024,
      title={The Informativeness of Combined Experimental and Observational Data under Dynamic Selection}, 
      author={Yechan Park and Yuya Sasaki},
      year={2024},
      eprint={2403.16177},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      url={https://arxiv.org/abs/2403.16177}, 
}

@article{Lohr-Raghunathan2017,
	author = {Sharon L. Lohr and Trivellore E. Raghunathan},
	doi = {10.1214/16-STS584},
	journal = {Statistical Science},
	keywords = {hierarchical models, imputation, multiple frame survey, probability sample, record linkage, small area estimation},
	number = {2},
	pages = {293 -- 312},
	publisher = {Institute of Mathematical Statistics},
	title = {{Combining Survey Data with Other Data Sources}},
	url = {https://doi.org/10.1214/16-STS584},
	volume = {32},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/16-STS584}}
@article{Hand2018,
    author = {Hand, David J.},
    title = "{Statistical Challenges of Administrative and Transaction Data}",
    journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
    volume = {181},
    number = {3},
    pages = {555-605},
    year = {2018},
    month = {02},
    abstract = "{Administrative data are becoming increasingly important. They are typically the side effect of some operational exercise and are often seen as having significant advantages over alternative sources of data. Although it is true that such data have merits, statisticians should approach the analysis of such data with the same cautious and critical eye as they approach the analysis of data from any other source. The paper identifies some statistical challenges, with the aim of stimulating debate about and improving the analysis of administrative data, and encouraging methodology researchers to explore some of the important statistical problems which arise with such data.}",
    issn = {0964-1998},
    doi = {10.1111/rssa.12315},
    url = {https://doi.org/10.1111/rssa.12315},
    eprint = {https://academic.oup.com/jrsssa/article-pdf/181/3/555/49449402/jrsssa\_181\_3\_555.pdf},
}

@article{Rao2021,
	abstract = {Survey samplers have long been using probability samples from one or more sources in conjunction with census and administrative data to make valid and efficient inferences on finite population parameters. This topic has received a lot of attention more recently in the context of data from non-probability samples such as transaction data, web surveys and social media data. In this paper, I will provide a brief overview of probability sampling methods first and then discuss some recent methods, based on models for the non-probability samples, which could lead to useful inferences from a non-probability sample by itself or when combined with a probability sample. I will also explain how big data may be used as predictors in small area estimation, a topic of current interest because of the growing demand for reliable local area statistics.},
	author = {Rao, J.  N.  K. },
	date = {2021/05/01},
	date-added = {2024-09-27 13:37:43 +0900},
	date-modified = {2024-09-27 13:37:43 +0900},
	doi = {10.1007/s13571-020-00227-w},
	id = {Rao2021},
	isbn = {0976-8394},
	journal = {Sankhya B},
	number = {1},
	pages = {242--272},
	title = {On Making Valid Inferences by Integrating Data from Surveys and Other Sources},
	url = {https://doi.org/10.1007/s13571-020-00227-w},
	volume = {83},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s13571-020-00227-w}}

@article{Golini-Righi2024,
	abstract = {This paper introduces the pseudo-calibration estimators, a novel method that integrates a non-probability sample of big size with a probability sample, assuming both samples contain relevant information for estimating the population parameter. The proposed estimators share a structural similarity with the adjusted projection estimators and the difference estimators but they adopt a different inferential approach and informative setup. The pseudo-calibration estimators can be employed when the target variable is observed in the probability sample and, in the non-probability sample, it is observed correctly, observed with error, or predicted. This paper also introduces an original application of the jackknife-type method for variance estimation. A simulation study shows that the proposed estimators are robust and efficient compared to the regression data integration estimators that use the same informative setup. Finally, a further evaluation using real data is carried out.},
	author = {Golini, Natalia and Righi, Paolo},
	date = {2024/04/01},
	date-added = {2024-09-27 13:38:28 +0900},
	date-modified = {2024-09-27 13:38:28 +0900},
	doi = {10.1007/s10260-023-00740-y},
	id = {Golini2024},
	isbn = {1613-981X},
	journal = {Statistical Methods \& Applications},
	number = {2},
	pages = {555--580},
	title = {Integrating probability and big non-probability samples data to produce Official Statistics},
	url = {https://doi.org/10.1007/s10260-023-00740-y},
	volume = {33},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s10260-023-00740-y}}
@article{Salvatore2024,
    author = {Salvatore, Camilla and Biffignandi, Silvia and Sakshaug, Joseph W and Wiśniowski, Arkadiusz and Struminskaya, Bella},
    title = "{Bayesian Integration of Probability and Nonprobability Samples for Logistic Regression}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {12},
    number = {2},
    pages = {458-492},
    year = {2024},
    month = {11},
    abstract = "{Probability sample (PS) surveys are considered the gold standard for population-based inference but face many challenges due to decreasing response rates, relatively small sample sizes, and increasing costs. In contrast, the use of nonprobability sample (NPS) surveys has increased significantly due to their convenience, large sample sizes, and relatively low costs, but they are susceptible to large selection biases and unknown selection mechanisms. Integrating both sample types in a way that exploits their strengths and overcomes their weaknesses is an ongoing area of methodological research. We build on previous work by proposing a method of supplementing PSs with NPSs to improve analytic inference for logistic regression coefficients and potentially reduce survey costs. Specifically, we use a Bayesian framework for inference. Inference relies on a probability survey with a small sample size, and through the prior structure we incorporate supplementary auxiliary information from a less-expensive (but potentially biased) NPS survey fielded in parallel. The performance of several strongly informative priors constructed from the NPS information is evaluated through a simulation study and real-data application. Overall, the proposed priors reduce the mean-squared error (MSE) of regression coefficients or, in the worst case, perform similarly to a weakly informative (baseline) prior that does not utilize any nonprobability information. Potential cost savings (of up to 68 percent) are evident compared to a probability-only sampling design with the same MSE for different informative priors under different sample sizes and cost scenarios. The algorithm, detailed results, and interactive cost analysis are provided through a Shiny web app as guidance for survey practitioners.}",
    issn = {2325-0992},
    doi = {10.1093/jssam/smad041},
    url = {https://doi.org/10.1093/jssam/smad041},
    eprint = {https://academic.oup.com/jssam/article-pdf/12/2/458/57170654/smad041.pdf},
}

@article{Angelopoulos2023,
	abstract = {Prediction-powered inference is a framework for performing valid statistical inference when an experimental dataset is supplemented with predictions from a machine-learning system. The framework yields simple algorithms for computing provably valid confidence intervals for quantities such as means, quantiles, and linear and logistic regression coefficients without making any assumptions about the machine-learning algorithm that supplies the predictions. Furthermore, more accurate predictions translate to smaller confidence intervals. Prediction-powered inference could enable researchers to draw valid and more data-efficient conclusions using machine learning. The benefits of prediction-powered inference were demonstrated with datasets from proteomics, astronomy, genomics, remote sensing, census analysis, and ecology. Over the past decade, there has been rapid progress in the development of large-scale machine learning (ML) systems that provide predictions related to various scientific phenomena. Unfortunately, the standard statistical approaches used to calculate confidence intervals and P values from gold standard data lose their statistical validity for ML-derived data. Angelopoulos et al. introduced ``prediction-powered inference,'' a standardized protocol for constructing valid confidence intervals and P values that enables the power and scale of ML systems to be used as predictors while ensuring responsible and reliable scientific inferences. The method has been demonstrated on a broad range of real datasets and offers a promising statistical approach for using ML to derive scientific conclusions responsibly. ---Yury Suleymanov A statistical protocol for valid scientific discovery using machine learning is presented.},
	author = {Anastasios N. Angelopoulos and Stephen Bates and Clara Fannjiang and Michael I. Jordan and Tijana Zrnic},
	doi = {10.1126/science.adi6000},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.adi6000},
	journal = {Science},
	number = {6671},
	pages = {669-674},
	title = {Prediction-powered inference},
	url = {https://www.science.org/doi/abs/10.1126/science.adi6000},
	volume = {382},
	year = {2023},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.adi6000},
	bdsk-url-2 = {https://doi.org/10.1126/science.adi6000}}
@article{Robbins+2020,
    author = {Robbins, Michael W and Ghosh-Dastidar, Bonnie and Ramchand, Rajeev},
    title = "{Blending Probability and Nonprobability Samples with Applications to a Survey of Military Caregivers}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {9},
    number = {5},
    pages = {1114-1145},
    year = {2020},
    month = {12},
    abstract = "{Probability samples are the preferred method for providing inferences that are generalizable to a larger population. However, in many cases, this approach is unlikely to yield a sample size large enough to produce precise inferences. Our goal here is to improve the efficiency of inferences from a probability sample by combining (or blending) it with a nonprobability sample, which is (by itself) potentially fraught with selection biases that would compromise the generalizability of results. We develop novel methods of statistical weighting that may be used for this purpose. Specifically, we make a distinction between weights that can be used to make the two samples representative of the population individually (disjoint blending) and those that make only the combined sample representative (simultaneous blending). Our focus is on weights constructed using propensity scores, but consideration is also given to calibration weighting. We include simulation studies that, among other illustrations, show the gain in precision provided by the convenience sample is lower in circumstances where the outcome is strongly related to the auxiliary variables used to align the samples. Motivating the exposition is a survey of military caregivers; our interest is focused on unpaid caregivers of wounded, ill, or injured US servicemembers and veterans who served following September 11, 2001. Our work serves not only to illustrate the proper execution of blending but also to caution the reader with respect to its dangers, as invoking a nonprobability sample may not yield substantial improvements in precision when assumptions are valid and may induce biases in the event that they are not.}",
    issn = {2325-0984},
    doi = {10.1093/jssam/smaa037},
    url = {https://doi.org/10.1093/jssam/smaa037},
    eprint = {https://academic.oup.com/jssam/article-pdf/9/5/1114/41727173/smaa037.pdf},
}
@article{Beaumont-Rao2021,
  author = {Jean-Francois Beaumont and J. N. K. Rao},
  year = {2021},
  title = {{Pitfalls of Making Inference from Non-probability Samples: Can Data Integration through Probability Samples Provide Remedies?}},
  journal = {The Survey Statistician},
  volume = {83},
  number = {},
  pages = {11-22},
  url = {https://isi-iass.org/home/wp-content/uploads/Survey_Statistician_2021_January_N83_02.pdf}
}
@article{Kim+2021,
    author = {Kim, Jae Kwang and Park, Seho and Chen, Yilin and Wu, Changbao},
    title = "{Combining Non-Probability and Probability Survey Samples Through Mass Imputation}",
    journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
    volume = {184},
    number = {3},
    pages = {941-963},
    year = {2021},
    month = {07},
    abstract = "{Analysis of non-probability survey samples requires auxiliary information at the population level. Such information may also be obtained from an existing probability survey sample from the same finite population. Mass imputation has been used in practice for combining non-probability and probability survey samples and making inferences on the parameters of interest using the information collected only in the non-probability sample for the study variables. Under the assumption that the conditional mean function from the non-probability sample can be transported to the probability sample, we establish the consistency of the mass imputation estimator and derive its asymptotic variance formula. Variance estimators are developed using either linearization or bootstrap. Finite sample performances of the mass imputation estimator are investigated through simulation studies. We also address important practical issues of the method through the analysis of a real-world non-probability survey sample collected by the Pew Research Centre.}",
    issn = {0964-1998},
    doi = {10.1111/rssa.12696},
    url = {https://doi.org/10.1111/rssa.12696},
    eprint = {https://academic.oup.com/jrsssa/article-pdf/184/3/941/49411783/jrsssa\_184\_3\_941.pdf},
}
@misc{Beresovsky+2024,
      title={Review of Quasi-Randomization Approaches for Estimation from Non-probability Samples}, 
      author={Vladislav Beresovsky and Julie Gershunskaya and Terrance D. Savitsky},
      year={2024},
      eprint={2312.05383},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/2312.05383}, 
}
@article{Elliot2009,
	author = {Elliot, Michael R},
	journal = {Survey Practice},
	doi = {10.29115/SP-2009-0025},
	number = {6},
	year = {2009},
	month = {sep 1},
	title = {Combining {Data} from {Probability} and {Non}- {Probability} {Samples} {Using} {Pseudo}-{Weights}},
	volume = {2},
}


@article{Shimodaira2000,
	abstract = {A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is effective in cases such as sample surveys or design of experiments, where the observed covariate follows a different distribution than that in the whole population. Under misspecification of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estimation of sample surveys. The optimality is defined by the expected Kullback--Leibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct specification of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed.},
	author = {Hidetoshi Shimodaira},
	doi = {https://doi.org/10.1016/S0378-3758(00)00115-4},
	issn = {0378-3758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Akaike information criterion, Design of experiments, Importance sampling, Kullback--Leibler divergence, Misspecification, Sample surveys, Weighted least squares},
	number = {2},
	pages = {227-244},
	title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
	volume = {90},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
	bdsk-url-2 = {https://doi.org/10.1016/S0378-3758(00)00115-4}}
@article{Imai-Ratkovic2014,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/24772753},
 abstract = {The propensity score plays a central role in a variety of causal inference settings. In particular, matching and weighting methods based on the estimated propensity score have become increasingly common in the analysis of observational data. Despite their popularity and theoretical appeal, the main practical difficulty of these methods is that the propensity score must be estimated. Researchers have found that slight misspecification of the propensity score model can result in substantial bias of estimated treatment effects. We introduce covariate balancing propensity score (CBPS) methodology, which models treatment assignment while optimizing the covariate balance. The CBPS exploits the dual characteristics of the propensity score as a covariate balancing score and the conditional probability of treatment assignment. The estimation of the CBPS is done within the generalized method-of-moments or empirical likelihood framework. We find that the CBPS dramatically improves the poor empirical performance of propensity score matching and weighting methods reported in the literature. We also show that the CBPS can be extended to other important settings, including the estimation of the generalized propensity score for non-binary treatments and the generalization of experimental estimates to a target population. Open source software is available for implementing the methods proposed.},
 author = {Kosuke Imai and Marc Ratkovic},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {1},
 pages = {243--263},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Covariate balancing propensity score},
 urldate = {2024-09-27},
 volume = {76},
 year = {2014}
}
@article{Deng-Wu1987,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2289466},
 abstract = {The regression estimator and the ratio estimator are commonly used in survey practice. In the past more attention has been given to the ratio estimator because of its computational ease and applicability for general sampling designs. The ratio estimator is appropriate for populations whose regression line passes close to the origin. If the intercept of the regression line is significantly nonzero, however, it is much less efficient than the regression estimator (Deng 1984). In general, apart from n-2 terms, the mean squared error (MSE) of the former is bigger than that of the latter (Cochran 1977, p. 196). Given the present computing capacity, the computational advantage of the ratio estimator should be less of a concern and the regression estimator will gain wider popularity. The main purpose of this article is to provide a theoretical and empirical comparison of several variance estimators for the regression estimator in simple random sampling without replacement. The companion problem for the ratio estimator has been studied in the literature (see Wu and Deng 1983). Under comparison are several design-based and model-based estimators and a new class of estimators. Their second-order expressions and biases are derived and compared. Empirical results on the biases and MSE's of the variance estimators and the conditional and unconditional coverage probabilities of their associated t intervals are obtained. They lend support to the theoretical results and suggest questions for further investigation. Our empirical and theoretical study provides a guide to the use of these estimators in practice.},
 author = {Lih-Yuan Deng and C. F. J. Wu},
 journal = {Journal of the American Statistical Association},
 number = {398},
 pages = {568--576},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Estimation of Variance of the Regression Estimator},
 urldate = {2024-09-27},
 volume = {82},
 year = {1987}
}
@book{川口-澤田2024,
    author = {川口康平 and 澤田真行},
    year = {2024},
    title = {因果推論の計量経済学},
    series = {},
    volume = {},
    edition = {},
    url = {https://github.com/keisemi/EconometriciansGuide_CausalInference},
    doi = {},
    publisher = {日本評論社}
}
@book{Hernan-Robins2020,
    author = {M. A. Hernán and James M. Robins},
    year = {2020},
    title = {{Causal Inference: What If}},
    series = {},
    volume = {},
    edition = {https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/},
    doi = {},
    publisher = {Boca Raton: Chapman & Hall/CRC}
}
@book{Morgan-Winship2014,
    author = {Stephen L. Morgan and Christopher Winship},
    year = {2014},
    title = {{Counterfactuals and Causal Inference}},
    series = {Analytical Methods for Social Research},
    volume = {},
    edition = {2},
    url = {https://doi.org/10.1017/CBO9781107587991},
    doi = {},
    publisher = {Cambridge University Press}
}
@book{Rosenbaum2023,
    author = {Paul R. Rosenbaum},
    year = {2023},
    title = {Causal Inference},
    series = {MIT Press Essential Knowledge Series},
    volume = {},
    edition = {},
    url = {https://mitpress.mit.edu/9780262545198/causal-inference/},
    doi = {},
    publisher = {MIT Press}
}
@article{Pearl2015, title={TRYGVE HAAVELMO AND THE EMERGENCE OF CAUSAL CALCULUS}, volume={31}, DOI={10.1017/S0266466614000231}, number={1}, journal={Econometric Theory}, author={Pearl, Judea}, year={2015}, pages={152–179}}

@article{Haavelmo1943,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1905714},
 author = {Trygve Haavelmo},
 journal = {Econometrica},
 number = {1},
 pages = {1--12},
 publisher = {[Wiley, Econometric Society]},
 title = {The Statistical Implications of a System of Simultaneous Equations},
 urldate = {2024-09-28},
 volume = {11},
 year = {1943}
}
@article{Leamer1983,
 ISSN = {00028282},
 URL = {http://www.jstor.org/stable/1803924},
 author = {Edward E. Leamer},
 journal = {The American Economic Review},
 number = {1},
 pages = {31--43},
 publisher = {American Economic Association},
 title = {Let's Take the Con Out of Econometrics},
 urldate = {2024-09-28},
 volume = {73},
 year = {1983}
}

@article{Bongers+2021,
	author = {Stephan Bongers and Patrick Forr{\'e} and Jonas Peters and Joris M. Mooij},
	date = {2021/10/1},
	date-added = {2024-09-28 17:56:02 +0900},
	date-modified = {2024-09-28 17:56:02 +0900},
	doi = {10.1214/21-AOS2064},
	journal = {The Annals of Statistics},
	journal1 = {The Annals of Statistics},
	journal2 = {The Annals of Statistics},
	month = {10},
	number = {5},
	pages = {2885--2915 },
	title = {Foundations of structural causal models with cycles and latent variables},
	url = {https://doi.org/10.1214/21-AOS2064},
	volume = {49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/21-AOS2064}}
@article{Rubin1980,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2287653},
 author = {Donald B. Rubin},
 journal = {Journal of the American Statistical Association},
 number = {371},
 pages = {591--593},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment},
 urldate = {2024-09-28},
 volume = {75},
 year = {1980}
}

@article{Runge2023,
	abstract = {Many research questions in Earth and environmental sciences are inherently causal, requiring robust analyses to establish whether and how changes in one variable cause changes in another. Causal inference provides the theoretical foundations to use data and qualitative domain knowledge to quantitatively answer these questions, complementing statistics and machine learning techniques. However, there is still a broad language gap between the methodological and domain science communities. In this Technical Review, we explain the use of causal inference frameworks with a focus on the challenges of time series data. Domain-adapted explanations, method guidance and practical case studies provide an accessible summary of methods for causal discovery and causal effect estimation. Examples from climate and biogeosciences illustrate typical challenges, such as contemporaneous causation, hidden confounding and non-stationarity, and some strategies to address these challenges. Integrating causal thinking into data-driven science will facilitate process understanding and more robust machine learning and statistical models for Earth and environmental sciences, enabling the tackling of many open problems with relevant environmental, economic and societal implications.},
	author = {Runge, Jakob and Gerhardus, Andreas and Varando, Gherardo and Eyring, Veronika and Camps-Valls, Gustau},
	date = {2023/07/01},
	date-added = {2024-09-28 18:33:14 +0900},
	date-modified = {2024-09-28 18:33:14 +0900},
	doi = {10.1038/s43017-023-00431-y},
	id = {Runge2023},
	isbn = {2662-138X},
	journal = {Nature Reviews Earth \& Environment},
	number = {7},
	pages = {487--505},
	title = {Causal inference for time series},
	url = {https://doi.org/10.1038/s43017-023-00431-y},
	volume = {4},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s43017-023-00431-y}}

@article{Cui-Athey2022,
	abstract = {Causal inference has recently attracted substantial attention in the machine learning and artificial intelligence community. It is usually positioned as a distinct strand of research that can broaden the scope of machine learning from predictive modelling to intervention and decision-making. In this Perspective, however, we argue that ideas from causality can also be used to improve the stronghold of machine learning, predictive modelling, if predictive stability, explainability and fairness are important. With the aim of bridging the gap between the tradition of precise modelling in causal inference and black-box approaches from machine learning, stable learning is proposed and developed as a source of common ground. This Perspective clarifies a source of risk for machine learning models and discusses the benefits of bringing causality into learning. We identify the fundamental problems addressed by stable learning, as well as the latest progress from both causal inference and learning perspectives, and we discuss relationships with explainability and fairness problems.},
	author = {Cui, Peng and Athey, Susan},
	date = {2022/02/01},
	date-added = {2024-09-28 19:20:10 +0900},
	date-modified = {2024-09-28 19:20:10 +0900},
	doi = {10.1038/s42256-022-00445-z},
	id = {Cui2022},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {2},
	pages = {110--115},
	title = {Stable learning establishes some common ground between causal inference and machine learning},
	url = {https://doi.org/10.1038/s42256-022-00445-z},
	volume = {4},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-022-00445-z}}
@article{LaLonde1986,
 ISSN = {00028282},
 URL = {http://www.jstor.org/stable/1806062},
 abstract = {This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric procedures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.},
 author = {Robert J. LaLonde},
 journal = {The American Economic Review},
 number = {4},
 pages = {604--620},
 publisher = {American Economic Association},
 title = {Evaluating the Econometric Evaluations of Training Programs with Experimental Data},
 urldate = {2024-09-28},
 volume = {76},
 year = {1986}
}
@article{Heckman2010,
 ISSN = {00220515},
 URL = {http://www.jstor.org/stable/20778729},
 abstract = {This paper compares the structural approach to economic policy analysis with the program evaluation approach. It offers a third way to do policy analysis that combines the best features of both approaches. I illustrate the value of this alternative approach by making the implicit economics of LATE explicit, thereby extending the interpretability and range of policy questions that LATE can answer.},
 author = {James J. Heckman},
 journal = {Journal of Economic Literature},
 number = {2},
 pages = {356--398},
 publisher = {American Economic Association},
 title = {Building Bridges Between Structural and Program Evaluation Approaches to Evaluating Policy},
 urldate = {2024-09-28},
 volume = {48},
 year = {2010}
}

@article{黒木学2016,
	author = {黒木 学},
	doi = {10.20742/pbsj.44.0_124},
	journal = {日本行動計量学会大会抄録集},
	pages = {124-125},
	title = {R-6-2 構造的因果モデルから潜在反応モデルへ},
	volume = {44},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.20742/pbsj.44.0_124}}

@article{Lucas1976,
	author = {Robert E. Lucas},
	doi = {https://doi.org/10.1016/S0167-2231(76)80003-6},
	issn = {0167-2231},
	journal = {Carnegie-Rochester Conference Series on Public Policy},
	pages = {19-46},
	title = {Econometric policy evaluation: A critique},
	url = {https://www.sciencedirect.com/science/article/pii/S0167223176800036},
	volume = {1},
	year = {1976},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167223176800036},
	bdsk-url-2 = {https://doi.org/10.1016/S0167-2231(76)80003-6}}
@book{Ding2024,
  author = {Peng Ding},
  year = {2024},
  title = {{A First Course in Causal Inference}},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1201/9781003484080},
  publisher = {Chapman and Hall/CRC}
}
@book{Huber2023,
  author = {Martin Huber},
  year = {2023},
  title = {{Causal Analysis: Impact Evaluation and Causal Machine Learning with Applications in R}},
  series = {},
  volume = {},
  edition = {},
  url = {https://mitpress.mit.edu/9780262545914/causal-analysis/},
  publisher = {MIT Press}
}

@article{Brand+2023,
	abstract = {This article reviews recent advances in causal inference relevant to sociology. We focus on a selective subset of contributions aligning with four broad topics: causal effect identification and estimation in general, causal effect heterogeneity, causal effect mediation, and temporal and spatial interference. We describe how machine learning, as an estimation strategy, can be effectively combined with causal inference, which has been traditionally concerned with identification. The incorporation of machine learning in causal inference enables researchers to better address potential biases in estimating causal effects and uncover heterogeneous causal effects. Uncovering sources of effect heterogeneity is key for generalizing to populations beyond those under study. While sociology has long emphasized the importance of causal mechanisms, historical and life-cycle variation, and social contexts involving network interactions, recent conceptual and computational advances facilitate more principled estimation of causal effects under these settings. We encourage sociologists to incorporate these insights into their empirical research.},
	author = {Brand, Jennie E. and Zhou, Xiang and Xie, Yu},
	doi = {https://doi.org/10.1146/annurev-soc-030420-015345},
	issn = {1545-2115},
	journal = {Annual Review of Sociology},
	keywords = {external validity},
	number = {Volume 49, 2023},
	pages = {81-110},
	publisher = {Annual Reviews},
	title = {Recent Developments in Causal Inference and Machine Learning},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-soc-030420-015345},
	volume = {49},
	year = {2023},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev-soc-030420-015345},
	bdsk-url-2 = {https://doi.org/10.1146/annurev-soc-030420-015345}}
@article{Fuller-Kim2005,
  author = {Wayne A. Fuller and Jae-Kwang Kim},
  year = {2005},
  title = {Hot Deck Imputation for the Response Model},
  journal = {Survey Methodology},
  volume = {31},
  number = {2},
  pages = {139-149},
  url = {https://www150.statcan.gc.ca/n1/pub/12-001-x/2005002/article/9041-eng.pdf}
}
@misc{Kwon+2024,
      title={Debiased calibration estimation using generalized entropy in survey sampling}, 
      author={Yonghyun Kwon and Jae Kwang Kim and Yumou Qiu},
      year={2024},
      eprint={2404.01076},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2404.01076}, 
}
@book{Hayashi2000,
  author = {Fumio Hayashi},
  year = {2000},
  title = {Econometrics},
  series = {},
  volume = {},
  edition = {},
  url = {https://press.princeton.edu/books/ebook/9781400823833/econometrics-1},
  publisher = {Princeton University Press}
}
@article{Pfanzagl1969,
	abstract = {The concept of minimum contrast (m.c.) estimates used in this paper covers maximum likelihood (m.l.) estimates as a special case. Section 1 contains sufficient conditions for the existence of measurable m.c. estimates and for their consistency.},
	author = {Pfanzagl, J. },
	date = {1969/12/01},
	date-added = {2024-09-29 14:40:43 +0900},
	date-modified = {2024-09-29 14:40:43 +0900},
	doi = {10.1007/BF02613654},
	id = {Pfanzagl1969},
	isbn = {1435-926X},
	journal = {Metrika},
	number = {1},
	pages = {249--272},
	title = {On the measurability and consistency of minimum contrast estimates},
	url = {https://doi.org/10.1007/BF02613654},
	volume = {14},
	year = {1969},
	bdsk-url-1 = {https://doi.org/10.1007/BF02613654}}

@article{Huber1964,
	author = {Peter J. Huber},
	doi = {10.1214/aoms/1177703732},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	pages = {73 -- 101},
	publisher = {Institute of Mathematical Statistics},
	title = {{Robust Estimation of a Location Parameter}},
	url = {https://doi.org/10.1214/aoms/1177703732},
	volume = {35},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1214/aoms/1177703732}}
@book{Huber1981,
  author = {Peter J. Huber},
  year = {1981},
  title = {Robust Statistics},
  series = {Wiley Series in Probability and Statistics},
  volume = {},
  edition = {},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0471725250},
  publisher = {John Wiley & Sons}
}
@phdthesis{LeCam1952,
  author = {Lucien Marie Le{\ }Cam},
  school = {UC Berkeley},
  title = {On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayes' Estimates},
  year = {1952},
  url = {https://math.berkeley.edu/publications/some-asymptotic-properties-maximum-likelihood-estimates-and-related-bayes-estimates},
}
@article{Godambe1997,
 ISSN = {07492170},
 URL = {http://www.jstor.org/stable/4356005},
 abstract = {The development of the modern theory of estimating functions is traced from its inception. It is shown that this development has brought about a synthesis of the two historically important methodologies of estimation namely, the 'least squares' and the 'maximum likelihood'.},
 author = {V. P. Godambe},
 journal = {Lecture Notes-Monograph Series},
 pages = {5--15},
 publisher = {Institute of Mathematical Statistics},
 title = {Estimating Functions: A Synthesis of Least Squares and Maximum Likelihood Methods},
 urldate = {2024-09-29},
 volume = {32},
 year = {1997}
}
@article{Owen1988,
    author = {Art B Owen},
    title = "{Empirical likelihood ratio confidence intervals for a single functional}",
    journal = {Biometrika},
    volume = {75},
    number = {2},
    pages = {237-249},
    year = {1988},
    month = {06},
    abstract = "{The empirical distribution function based on a sample is well known to be the maximum likelihood estimate of the distribution from which the sample was taken. In this paper the likelihood function for distributions is used to define a likelihood ratio function for distributions. It is shown that this empirical likelihood ratio function can be used to construct confidence intervals for the sample mean, for a class of M-estimates that includes quantiles, and for differentiable statistical functionals. The results are nonpara-metric extensions of Wilks's (1938) theorem for parametric likelihood ratios. The intervals are illustrated on some real data and compared in a simulation to some bootstrap confidence intervals and to intervals based on Student's t statistic. A hybrid method that uses the bootstrap to determine critical values of the likelihood ratio is introduced.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/75.2.237},
    url = {https://doi.org/10.1093/biomet/75.2.237},
    eprint = {https://academic.oup.com/biomet/article-pdf/75/2/237/827908/75-2-237.pdf},
}
@article{Wedderburn1974,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334725},
 abstract = {To define a likelihood we have to specify the form of distribution of the observations, but to define a quasi-likelihood function we need only specify a relation between the mean and variance of the observations and the quasi-likelihood can then be used for estimation. For a one-parameter exponential family the log likelihood is the same as the quasi-likelihood and it follows that assuming a one-parameter exponential family is the weakest sort of distributional assumption that can be made. The Gauss-Newton method for calculating nonlinear least squares estimates generalizes easily to deal with maximum quasi-likelihood estimates, and a rearrangement of this produces a generalization of the method described by Nelder & Wedderburn (1972).},
 author = {R. W. M. Wedderburn},
 journal = {Biometrika},
 number = {3},
 pages = {439--447},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Quasi-Likelihood Functions, Generalized Linear Models, and the Gauss-Newton Method},
 urldate = {2024-09-29},
 volume = {61},
 year = {1974}
}

@article{Qin-Lawless1994,
	author = {Jin Qin and Jerry Lawless},
	doi = {10.1214/aos/1176325370},
	journal = {The Annals of Statistics},
	keywords = {Asymptotic efficiency, Auxiliary information, empirical likelihood, estimating equations, parametric likelihood, semiparametric models, testing hypotheses, Wilks' theorem},
	number = {1},
	pages = {300 -- 325},
	publisher = {Institute of Mathematical Statistics},
	title = {{Empirical Likelihood and General Estimating Equations}},
	url = {https://doi.org/10.1214/aos/1176325370},
	volume = {22},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176325370}}

@article{Robins-Greenland1992,
	author = {Robins, James M. and Greenland, Sander},
	date-added = {2024-09-29 19:47:26 +0900},
	date-modified = {2024-09-29 19:47:26 +0900},
	id = {00001648-199203000-00013},
	isbn = {1044-3983},
	journal = {Epidemiology},
	keywords = {causality; causal modeling; epidemiologic methods; risk},
	n2 = {We consider the problem of separating the direct effects of an exposure from effects relayed through an intermediate variable (indirect effects). We show that adjustment for the intermediate variable, which is the most common method of estimating direct effects, can be biased. We also show that, even in a randomized crossover trial of exposure, direct and indirect effects cannot be separated without special assumptions; in other words, direct and indirect effects are not separately identifiable when only exposure is randomized. If the exposure and intermediate never interact to cause disease and if intermediate effects can be controlled, that is, blocked by a suitable intervention, then a trial randomizing both exposure and the intervention can separate direct from indirect effects. Nonetheless, the estimation must be carried out using the G-computation algorithm. Conventional adjustment methods remain biased. When exposure and the intermediate interact to cause disease, direct and indirect effects will not be separable even in a trial in which both the exposure and the intervention blocking intermediate effects are randomly assigned. Nonetheless, in such a trial, one can still estimate the fraction of exposure-induced disease that could be prevented by control of the intermediate. Even in the absence of an intervention blocking the intermediate effect, the fraction of exposure-induced disease that could be prevented by control of the intermediate can be estimated with the G-computation algorithm if data are obtained on additional confounding variables. (Epidemiology 1992;3:143--155)  {\copyright} Lippincott-Raven Publishers.},
	number = {2},
	title = {Identifiability and Exchangeability for Direct and Indirect Effects},
	url = {https://journals.lww.com/epidem/fulltext/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx},
	volume = {3},
	year = {1992},
	bdsk-url-1 = {https://journals.lww.com/epidem/fulltext/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx}}
@article{Nguyen+2021,
  author = {T. Q. Nguyen and I. Schmid and E. A. Stuart},
  year = {2021},
  title = {Clarifying causal mediation analysis for the applied researcher: Defining effects based on what we want to learn},
  journal = {Psychological Methods},
  volume = {26},
  number = {2},
  pages = {255-271},
  url = {https://doi.org/10.1037/met0000299}
}

@inbook{Pearl2012,
	abstract = {Summary This chapter contains sections titled: Mediation: Direct and indirect effects The mediation formula: A simple solution to a thorny problem Relation to other methods Conclusions Acknowledgments References},
	author = {Pearl, Judea},
	booktitle = {Causality},
	chapter = {12},
	doi = {https://doi.org/10.1002/9781119945710.ch12},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119945710.ch12},
	isbn = {9781119945710},
	keywords = {mediation formula, causal pathways in nonlinear models, direct effects, in legal disputes over hiring race/sex, and the discrimination, structural equation models, for analyzing path-specific effects, controlled direct effect, in linear systems, to path coefficient of link, identification of direct effects, not `ignorability' or `sequential ignorability', hiring context, path-specific effects in policy making, mediation effects in nonparametric, nonlinear models, mediation effects in linear, logistic, and probit models, analytical `sensitivity analysis' in statistics for parameter estimation, mediation formulas, causal pathway assessment in the sciences},
	pages = {151-179},
	publisher = {John Wiley & Sons, Ltd},
	title = {The Mediation Formula: A Guide to the Assessment of Causal Pathways in Nonlinear Models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119945710.ch12},
	year = {2012},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119945710.ch12},
	bdsk-url-2 = {https://doi.org/10.1002/9781119945710.ch12}}

@article{Fujii+2022,
	author = {Ryosuke Fujii, Shuntaro Sato, Yoshiki Tsuboi, Andres Cardenas and Koji Suzuki},
	doi = {10.1080/15592294.2021.1959736},
	eprint = {https://doi.org/10.1080/15592294.2021.1959736},
	journal = {Epigenetics},
	note = {PMID: 34384035},
	number = {7},
	pages = {759--785},
	publisher = {Taylor \& Francis},
	title = {DNA methylation as a mediator of associations between the environment and chronic diseases: A scoping review on application of mediation analysis},
	url = {https://doi.org/10.1080/15592294.2021.1959736},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1080/15592294.2021.1959736}}

@article{Robins1986,
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
	author = {James Robins},
	doi = {https://doi.org/10.1016/0270-0255(86)90088-6},
	issn = {0270-0255},
	journal = {Mathematical Modelling},
	number = {9},
	pages = {1393-1512},
	title = {A new approach to causal inference in mortality studies with a sustained exposure period---application to control of the healthy worker survivor effect},
	url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	volume = {7},
	year = {1986},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	bdsk-url-2 = {https://doi.org/10.1016/0270-0255(86)90088-6}}
@article{Robins+1992,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532304},
 abstract = {In order to estimate the causal effects of one or more exposures or treatments on an outcome of interest, one has to account for the effect of "confounding factors" which both covary with the exposures or treatments and are independent predictors of the outcome. In this paper we present regression methods which, in contrast to standard methods, adjust for the confounding effect of multiple continuous or discrete covariates by modelling the conditional expectation of the exposures or treatments given the confounders. In the special case of a univariate dichotomous exposure or treatment, this conditional expectation is identical to what Rosenbaum and Rubin have called the propensity score. They have also proposed methods to estimate causal effects by modelling the propensity score. Our methods generalize those of Rosenbaum and Rubin in several ways. First, our approach straightforwardly allows for multivariate exposures or treatments, each of which may be continuous, ordinal, or discrete. Second, even in the case of a single dichotomous exposure, our approach does not require subclassification or matching on the propensity score so that the potential for "residual confounding," i.e., bias, due to incomplete matching is avoided. Third, our approach allows a rather general formalization of the idea that it is better to use the "estimated propensity score" than the true propensity score even when the true score is known. The additional power of our approach derives from the fact that we assume the causal effects of the exposures or treatments can be described by the parametric component of a semiparametric regression model. To illustrate our methods, we reanalyze the effect of current cigarette smoking on the level of forced expiratory volume in one second in a cohort of 2,713 adult white males. We compare the results with those obtained using standard methods.},
 author = {James M. Robins and Steven D. Mark and Whitney K. Newey},
 journal = {Biometrics},
 number = {2},
 pages = {479--495},
 publisher = {International Biometric Society},
 title = {Estimating Exposure Effects by Modelling the Expectation of Exposure Conditional on Confounders},
 urldate = {2024-09-29},
 volume = {48},
 year = {1992}
}

@article{Robins+2000,
	author = {Robins, James M. and Hern{\'a}n, Miguel {\'A}ngel and Brumback, Babette},
	date-added = {2024-09-29 20:22:32 +0900},
	date-modified = {2024-09-29 20:22:32 +0900},
	id = {00001648-200009000-00011},
	isbn = {1044-3983},
	journal = {Epidemiology},
	keywords = {causality; counterfactuals; epidemiologic methods; longitudinal data; structural models; confounding; intermediate variables},
	n2 = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
	number = {5},
	title = {Marginal Structural Models and Causal Inference in Epidemiology},
	url = {https://journals.lww.com/epidem/fulltext/2000/09000/marginal_structural_models_and_causal_inference_in.11.aspx},
	volume = {11},
	year = {2000},
	bdsk-url-1 = {https://journals.lww.com/epidem/fulltext/2000/09000/marginal_structural_models_and_causal_inference_in.11.aspx}}

@inproceedings{Robins2000,
	abstract = {Robins (1993, 1994, 1997, 1998ab) has developed a set of causal or counterfactual models, the structural nested models (SNMs). This paper describes an alternative new class of causal models --- the (non-nested) marginal structural models (MSMs). We will then describe a class of semiparametric estimators for the parameters of these new models under a sequential randomization (i.e., ignorability) assumption. We then compare the strengths and weaknesses of MSMs versus SNMs for causal inference from complex longitudinal data with time-dependent treatments and confounders. Our results provide an extension to continuous treatments of propensity score estimators of an average treatment effect.},
	address = {New York, NY},
	author = {Robins, James M.},
	booktitle = {Statistical Models in Epidemiology, the Environment, and Clinical Trials},
	editor = {Halloran, M. Elizabeth and Berry, Donald},
	isbn = {978-1-4612-1284-3},
	pages = {95--133},
	publisher = {Springer New York},
	title = {Marginal Structural Models versus Structural nested Models as Tools for Causal inference},
	year = {2000}}

@article{Vamsteelandt-Joffe2014,
	author = {Stijn Vansteelandt and Marshall Joffe},
	doi = {10.1214/14-STS493},
	journal = {Statistical Science},
	keywords = {causal effect, confounding, direct effect, instrumental variable, mediation, time-varying confounding},
	number = {4},
	pages = {707 -- 731},
	publisher = {Institute of Mathematical Statistics},
	title = {{Structural Nested Models and G-estimation: The Partially Realized Promise}},
	url = {https://doi.org/10.1214/14-STS493},
	volume = {29},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1214/14-STS493}}
@article{Baron-Kenny1986,
  author = {R. M. Baron and D. A. Kenny},
  year = {1986},
  title = {The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations},
  journal = {Journal of Personality and Social Psychology},
  volume = {51},
  number = {6},
  pages = {1173-1182},
  url = {https://doi.org/10.1037/0022-3514.51.6.1173}
}
@article{Alwin-Hauser1975,
 ISSN = {00031224},
 URL = {http://www.jstor.org/stable/2094445},
 abstract = {This paper is about the logic of interpreting recursive causal theories in sociology. We review the distinction between associations and effects and discuss the decomposition of effects into direct and indirect components. We then describe a general method for decomposing effects into their components by the systematic application of ordinary least squares regression. The method involves successive computation of reduced-form equations, beginning with an equation containing only exogenous variables, then computing equations which add intervening variables in sequence from cause to effect. This generates all the information required to decompose effects into their various direct and indirect parts. This method is a substitute for the often more cumbersome computation of indirect effects from the structural coefficients (direct effects) of the causal model. Finally, we present a way of summarizing this information in tabular form and illustrate the procedures using an empirical example.},
 author = {Duane F. Alwin and Robert M. Hauser},
 journal = {American Sociological Review},
 number = {1},
 pages = {37--47},
 publisher = {[American Sociological Association, Sage Publications, Inc.]},
 title = {The Decomposition of Effects in Path Analysis},
 urldate = {2024-09-29},
 volume = {40},
 year = {1975}
}
@article{Cheng1994,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2291203},
 abstract = {This article considers a distribution-free estimation procedure for a basic pattern of missing data that often arises from the well-known double sampling in survey methodology. Without parametric modeling of the missing mechanism or the joint distribution, kernel regression estimators are used to estimate mean functionals through empirical estimation of the missing pattern. A generalization of the method of Cheng and Wei is verified under the assumption of missing at random. Asymptotic distributions are derived for estimating the mean of the incomplete data and for estimating the mean treatment difference in a nonrandomized observational study. The nonparametric method is compared with a naive pairwise deletion method and a linear regression method via the asymptotic relative efficiencies and a simulation study. The comparison shows that the proposed nonparametric estimators attain reliable performances in general.},
 author = {Philip E. Cheng},
 journal = {Journal of the American Statistical Association},
 number = {425},
 pages = {81--87},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Nonparametric Estimation of Mean Functionals with Data Missing at Random},
 urldate = {2024-09-29},
 volume = {89},
 year = {1994}
}
@article{Bertrand+2004,
    author = {Bertrand, Marianne and Duflo, Esther and Mullainathan, Sendhil},
    title = "{How Much Should We Trust Differences-In-Differences Estimates?*}",
    journal = {The Quarterly Journal of Economics},
    volume = {119},
    number = {1},
    pages = {249-275},
    year = {2004},
    month = {02},
    abstract = "{Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are inconsistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its “effect” as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an “effect” significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the autocorrelation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a “pre”- and “post”-period and explicitly takes into account the effective sample size works well even for small numbers of states.}",
    issn = {0033-5533},
    doi = {10.1162/003355304772839588},
    url = {https://doi.org/10.1162/003355304772839588},
    eprint = {https://academic.oup.com/qje/article-pdf/119/1/249/5304584/119-1-249.pdf},
}
@article{Poole-Rosenthal1991,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2111445},
 abstract = {Congressional roll call voting has been highly structured for most of U.S. history. The structure is revealed by a dynamic, spatial analysis of the entire roll call voting record from 1789 to 1985. The space is characterized by a predominant major dimension with, at times, a significant, but less important second dimension. In the modern era, spatial positions are very stable. This stability is such that, under certain conditions, short run forecasting of roll call votes is possible. Since the end of World War II, changes in congressional voting patterns have occurred almost entirely through the process of replacement of retiring or defeated legislators with new members. Politically, selection is far more important than adaptation.},
 author = {Keith T. Poole and Howard Rosenthal},
 journal = {American Journal of Political Science},
 number = {1},
 pages = {228--278},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {Patterns of Congressional Voting},
 urldate = {2024-09-29},
 volume = {35},
 year = {1991}
}
@book{浅古泰史2016,
  author = {浅古泰史},
  year = {2016},
  title = {政治の数理分析入門},
  series = {},
  volume = {},
  edition = {},
  url = {http://www.yasushiasako.com/book.htm},
  publisher = {木鐸社}
}

@article{稗田健志2015,
	author = {稗田健志},
	doi = {10.7218/nenpouseijigaku.66.1_13},
	journal = {年報政治学},
	number = {1},
	pages = {1_13-1_36},
	title = {政治理論と実証研究をつなぐ環},
	volume = {66},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.7218/nenpouseijigaku.66.1_13}}

@article{細野助博1981,
	author = {細野助博},
	doi = {10.11228/pcs1981.1981.65},
	journal = {公共選択の研究},
	number = {1},
	pages = {65-73},
	title = {政治競争モデル設計の試み},
	volume = {1981},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.11228/pcs1981.1981.65}}
@article{Davis+1970,
 ISSN = {00030554, 15375943},
 URL = {http://www.jstor.org/stable/1953842},
 author = {Otto A. Davis and Melvin J. Hinich and Peter C. Ordeshook},
 journal = {The American Political Science Review},
 number = {2},
 pages = {426--448},
 publisher = {[American Political Science Association, Cambridge University Press]},
 title = {An Expository Development of a Mathematical Model of the Electoral Process},
 urldate = {2024-09-30},
 volume = {64},
 year = {1970}
}

@article{Converse2006,
	author = {Philip E. Converse},
	doi = {10.1080/08913810608443650},
	eprint = {https://doi.org/10.1080/08913810608443650},
	journal = {Critical Review},
	number = {1-3},
	pages = {1--74},
	publisher = {Routledge},
	title = {The nature of belief systems in mass publics (1964)},
	url = {https://doi.org/10.1080/08913810608443650},
	volume = {18},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1080/08913810608443650}}
@article{Hinich-Pollard1981,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2110856},
 abstract = {In his unidimensional model of electoral competition, Downs argues that voters use party ideology as an informational short cut for forecasting the policies that a party will pursue if elected. Parties are perceived by voters as points on an ideological axis. In the Davis-Hinich multidimensional model, on the other hand, the axes are real issues, and the principal actors are politicians who are modeled as points in the multi-issue space. This paper reformulates spatial voting theory in terms of a model that connects what we call predictive dimensions with political issues that are salient during a given election campaign. This model is both a synthesis and an extension of the Downs and Davis-Hinich spatial models. We obtain a median voter result for one predictive dimension that is similar to the Downs result but with important differences. We also obtain results showing the electoral advantage of incumbency and the tendency for incremental change when there is a great deal of heterogeneity in voter perceptions about the candidates.},
 author = {Melvin J. Hinich and Walker Pollard},
 journal = {American Journal of Political Science},
 number = {2},
 pages = {323--341},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {A New Approach to the Spatial Theory of Electoral Competition},
 urldate = {2024-10-01},
 volume = {25},
 year = {1981}
}
@Article{Imai+2016,
  author={Imai, Kosuke and Lo, James and Olmsted, Jonathan},
  title={{Fast Estimation of Ideal Points with Massive Data}},
  journal={American Political Science Review},
  year=2016,
  volume={110},
  number={4},
  pages={631-656},
  month={November},
  keywords={},
  doi={},
  abstract={Estimation of ideological positions among voters, legislators, and other actors is central to many subfields of political science. Recent applications include large data sets of various types including roll calls, surveys, and textual and social media data. To overcome the resulting computational challenges, we propose fast estimation methods for ideal points with massive data. We derive the expectation-maximization (EM) algorithms to estimate the standard ideal point model with binary, ordinal, and continuous outcome variables. We then extend this methodology to dynamic and hierarchical ideal point models by developing variational EM algorithms for approximate inference. We demonstrate the computational efficiency and scalability of our methodology through a variety of real and simulated data. In cases where a standard Markov chain Monte Carlo algorithm would require several days to compute ideal points, the proposed algorithm can produce essentially identical estimates within minutes. Open-source software is available for implementing the proposed methods.},
  url={https://ideas.repec.org/a/cup/apsrev/v110y2016i04p631-656_00.html}
}
@article{Heckman-Snyder1997,
 ISSN = {07416261},
 URL = {http://www.jstor.org/stable/3087459},
 abstract = {We formulate and estimate a rigorously justified linear probability model of binary choices over alternatives characterized by unobserved attributes. We apply the model to estimate preferences of congressmen as expressed in their votes on bills. The effective dimension of the attribute space characterizing votes is larger than what has been estimated in recent influential studies of congressional voting by Poole and Rosenthal. Congressmen vote on more than ideology. Issue-specific attributes are an important determinant of congressional voting patterns. The estimated dimension is too large for the median voter model to describe congressional voting.},
 author = {James J. Heckman and James M. Snyder},
 journal = {The RAND Journal of Economics},
 pages = {S142--S189},
 publisher = {[RAND Corporation, Wiley]},
 title = {Linear Probability Models of the Demand for Attributes with an Empirical Application to Estimating the Preferences of Legislators},
 urldate = {2024-10-01},
 volume = {28},
 year = {1997}
}
@InCollection{McFadden1976,
  author={Daniel L. McFadden},
  title={{Quantal Choice Analysis: A Survey}},
  booktitle={{Annals of Economic and Social Measurement, Volume 5, number 4}},
  publisher={National Bureau of Economic Research, Inc},
  year={1976},
  month={},
  volume={},
  number={},
  series={NBER Chapters},
  edition={},
  chapter={},
  pages={363-390},
  doi={},
  keywords={},
  abstract={No abstract is available for this item.},
  url={https://ideas.repec.org/h/nbr/nberch/10488.html}
}
@Article{Zeileis+2008,
  title = {Regression Models for Count Data in {R}},
  author = {Achim Zeileis and Christian Kleiber and Simon Jackman},
  journal = {Journal of Statistical Software},
  year = {2008},
  volume = {27},
  number = {8},
  url = {https://www.jstatsoft.org/v27/i08/},
}
@article{Clinton-Meirowitz2003, title={Integrating Voting Theory and Roll Call Analysis: A Framework}, volume={11}, DOI={10.1093/pan/mpg023}, number={4}, journal={Political Analysis}, author={Clinton, Joshua D. and Meirowitz, Adam}, year={2017}, pages={381–396}}
@article{Clinton-Meirowitz2001,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791647},
 abstract = {Existing preference estimation procedures do not incorporate the full structure of the spatial model of voting, as they fail to use the sequential natural of the agenda. In the maximum likelihood framework, the consequences of this omission may be far-reaching. First, information useful for the identification of the model is neglected. Specifically, information that identifies the proposal locations is ignored. Second, the dimensionality of the policy space may be incorrectly estimated. Third, preference and proposal location estimates are incorrect and difficult to interpret in terms of the spatial model. We also show that the Bayesian simulation approach to ideal point estimation (Clinton et al. 2000; Jackman 2000) may be improved through the use of information about the legislative agenda. This point is illustrated by comparing several preference estimators of the first U.S. House (1789–1791).},
 author = {Joshua D. Clinton and Adam Meirowitz},
 journal = {Political Analysis},
 number = {3},
 pages = {242--259},
 publisher = {[Oxford University Press, Society for Political Methodology]},
 title = {Agenda Constrained Legislator Ideal Points and the Spatial Voting Model},
 urldate = {2024-10-01},
 volume = {9},
 year = {2001}
}


@article{加藤拓巳2021,
	author = {加藤拓巳},
	doi = {10.7222/marketing.2021.001},
	journal = {マーケティングジャーナル},
	number = {3},
	pages = {78-88},
	title = {選択における文脈効果の出現要因とその方向性},
	volume = {40},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.7222/marketing.2021.001}}
@article{Simonson1989,
 ISSN = {00935301, 15375277},
 URL = {http://www.jstor.org/stable/2489315},
 abstract = {Building on previous research, this article proposes that choice behavior under preference uncertainty may be easier to explain by assuming that consumers select the alternative supported by the best reasons. This approach provides an explanation for the so-called attraction effect and leads to the prediction of a compromise effect. Consistent with the hypotheses, the results indicate that (1) brands tend to gain share when they become compromise alternatives in a choice set; (2) attraction and compromise effects tend to be stronger among subjects who expect to justify their decisions to others; and (3) selections of dominating and compromise brands are associated with more elaborate and difficult decisions.},
 author = {Itamar Simonson},
 journal = {Journal of Consumer Research},
 number = {2},
 pages = {158--174},
 publisher = {Oxford University Press},
 title = {Choice Based on Reasons: The Case of Attraction and Compromise Effects},
 urldate = {2024-10-01},
 volume = {16},
 year = {1989}
}
@book{Embretson-Reise2000,
  author = {Susan E. Embretson and Steven P. Reise},
  year = {2000},
  title = {Item Response Theory for Psychologists},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.4324/9781410605269},
  publisher = {Psychology Press}
}
@book{Lord+1968,
  author = {F. M. Lord and M. R. Novick and A. Birnbaum},
  year = {1968},
  title = {{Statistical Theories of Mental Test Scores}},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {Addison-Wesley}
}

@article{Bock-Aitkin1981,
	abstract = {Maximum likelihood estimation of item parameters in the marginal distribution, integrating over the distribution of ability, becomes practical when computing procedures based on an EM algorithm are used. By characterizing the ability distribution empirically, arbitrary assumptions about its form are avoided. The Em procedure is shown to apply to general item-response models lacking simple sufficient statistics for ability. This includes models with more than one latent dimension.},
	author = {Bock, R. Darrell and Aitkin, Murray},
	date = {1981/12/01},
	date-added = {2024-10-01 16:33:15 +0900},
	date-modified = {2024-10-01 16:33:15 +0900},
	doi = {10.1007/BF02293801},
	id = {Bock1981},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--459},
	title = {Marginal maximum likelihood estimation of item parameters: Application of an EM algorithm},
	url = {https://doi.org/10.1007/BF02293801},
	volume = {46},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293801}}
@book{Rasch1960,
  author = {Georg W. Rasch},
  year = {1960},
  title = {Studies in Mathematical Psychology: I. Probabilistic Models for Some Intelligence and Attainment Tests},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {Nielsen & Lydiche}
}

@article{Fischer1973,
	abstract = {The present paper consists of a theoretical and an empirical part: First Rasch's test model for items with two answer categories is considered under the assumption of linear constraints on the item parameters (`linear logistic model'). It is shown that this model is appropriate for the analysis of subject areas in instructional research if the subject area comprises tasks or items which are solved by the pupil by combination of a certain number of cognitive operations or rules. An empirical investigation was made which showed that the psychological complexity of problems in elementary differential calculus, as taught in secondary school mathematics, can be approximately explained through the assumption of seven psychologically meaningful operations. The psychological contribution of this analysis does not lie in a mere statistical description of item difficulties, but rather in the testing of hypotheses as to which steps (operations) in solving a problem are to be viewed as psychological units. It was seen, for instance, that differentiation of a polynomial is to be considered a single operation psychologically, which is mastered and correctly combined with the other operations or not, and that the complexity of a task is primarily determined by the combination of different operations and is not increased significantly when the same operation occurs repeatedly within the problem.},
	author = {Gerhard H. Fischer},
	doi = {https://doi.org/10.1016/0001-6918(73)90003-6},
	issn = {0001-6918},
	journal = {Acta Psychologica},
	number = {6},
	pages = {359-374},
	title = {The linear logistic test model as an instrument in educational research},
	url = {https://www.sciencedirect.com/science/article/pii/0001691873900036},
	volume = {37},
	year = {1973},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0001691873900036},
	bdsk-url-2 = {https://doi.org/10.1016/0001-6918(73)90003-6}}
@book{Fox2010,
  author = {Jean-Paul Fox},
  year = {2010},
  title = {{Bayesian Item Response Modeling}},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1007/978-1-4419-0742-4},
  publisher = {Springer New York}
}

@article{Bock-Lieberman1970,
	abstract = {A method of estimating the parameters of the normal ogive model for dichotomously scored item-responses by maximum likelihood is demonstrated. Although the procedure requires numerical integration in order to evaluate the likelihood equations, a computer implemented Newton-Raphson solution is shown to be straightforward in other respects. Empirical tests of the procedure show that the resulting estimates are very similar to those based on a conventional analysis of item ``difficulties''and first factor loadings obtained from the matrix of tetrachoric correlation coefficients. Problems of testing the fit of the model, and of obtaining invariant parameters are discussed.},
	author = {Bock, R. Darrell and Lieberman, Marcus},
	date = {1970/06/01},
	date-added = {2024-10-01 19:21:24 +0900},
	date-modified = {2024-10-01 19:21:24 +0900},
	doi = {10.1007/BF02291262},
	id = {Darrell Bock1970},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {179--197},
	title = {Fitting a response model forn dichotomously scored items},
	url = {https://doi.org/10.1007/BF02291262},
	volume = {35},
	year = {1970},
	bdsk-url-1 = {https://doi.org/10.1007/BF02291262}}
@book{Hambleton+1991,
  author = {Hambleton, R. K., Swaminathan, H., & Rogers, H. J.},
  year = {1991},
  title = {Fundamentals of item response theory},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {Sage Publications}
}

@article{前川眞一2023,
	author = {前川眞一},
	doi = {10.24690/jart.19.1_35},
	journal = {日本テスト学会誌},
	number = {1},
	pages = {35-58},
	title = {項目反応理論におけるモデル変換},
	volume = {19},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.24690/jart.19.1_35}}

@article{Masters1982,
	abstract = {A unidimensional latent trait model for responses scored in two or more ordered categories is developed. This ``Partial Credit''model is a member of the family of latent trait models which share the property of parameter separability and so permit ``specifically objective''comparisons of persons and items. The model can be viewed as an extension of Andrich's Rating Scale model to situations in which ordered response alternatives are free to vary in number and structure from item to item. The difference between the parameters in this model and the ``category boundaries''in Samejima's Graded Response model is demonstrated. An unconditional maximum likelihood procedure for estimating the model parameters is developed.},
	author = {Masters, Geoff N. },
	date = {1982/06/01},
	date-added = {2024-10-01 19:43:34 +0900},
	date-modified = {2024-10-01 19:43:34 +0900},
	doi = {10.1007/BF02296272},
	id = {Masters1982},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {149--174},
	title = {A rasch model for partial credit scoring},
	url = {https://doi.org/10.1007/BF02296272},
	volume = {47},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF02296272}}

@article{Muraki1992,
	abstract = {ABSTRACT The Partial Credit model with a varying slope parameter has been developed, and it is called the Generalized Partial Credit model. The item step parameter of this model is decomposed to a location and a threshold parameter, following Andrich's Rating Scale formulation. The EM algorithm for estimating the model parameters was derived. The performance of this generalized model is compared with a Rasch family of polytomous item response models based on both simulated and real data. Simulated data were generated and then analyzed by the various polytomous item response models. The results obtained demonstrate that the rating formulation of the Generalized Partial Credit model is quite adaptable to the analysis of polytomous item responses. The real data used in this study consisted of NAEP Mathematics data which was made up of both dichotomous and polytomous item types. The Partial Credit model was applied to this data using both constant and varying slope parameters. The Generalized Partial Credit model, which provides for varying slope parameters, yielded better fit to data than the Partial Credit model without such a provision. Index terms: item response model polytomous item response model the Partial Credit model the Rating Scale model the Nominal Response model NAEP},
	author = {Muraki, Eiji},
	doi = {https://doi.org/10.1002/j.2333-8504.1992.tb01436.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1992.tb01436.x},
	journal = {ETS Research Report Series},
	number = {1},
	pages = {i-30},
	title = {{A Generalized Partial Credit Model: Application of an EM Algorithm}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1992.tb01436.x},
	volume = {1992},
	year = {1992},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1992.tb01436.x},
	bdsk-url-2 = {https://doi.org/10.1002/j.2333-8504.1992.tb01436.x}}

@inbook{Samejima1997,
	abstract = {The graded response model represents a family of mathematical models that deals with ordered polytomous categories. These ordered categories include rating such as letter grading, A, B, C, D, and F, used in the evaluation of students' performance; strongly disagree, disagree, agree, and strongly agree, used in attitude surveys; or partial credit given in accordance with an examinee's degree of attainment in solving a problem.},
	address = {New York, NY},
	author = {Samejima, Fumiko},
	booktitle = {Handbook of Modern Item Response Theory},
	doi = {10.1007/978-1-4757-2691-6_5},
	editor = {van der Linden, Wim J. and Hambleton, Ronald K.},
	isbn = {978-1-4757-2691-6},
	pages = {85--100},
	publisher = {Springer New York},
	title = {Graded Response Model},
	url = {https://doi.org/10.1007/978-1-4757-2691-6_5},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4757-2691-6_5}}
@inproceedings{井澤廣行2008,
  author = {井澤廣行},
  year = {2008},
  title = {Raschの理論と理念、及び、WrightのRasch測定展開},
  booktitle = {},
  volume = {20},
  number = {2},
  pages = {},
  url = {https://ryuka.repo.nii.ac.jp/records/176}
}

@article{Wright-Panchapakesan1969,
	author = {Benjamin Wright and Nargis Panchapakesan},
	doi = {10.1177/001316446902900102},
	eprint = {https://doi.org/10.1177/001316446902900102},
	journal = {Educational and Psychological Measurement},
	number = {1},
	pages = {23-48},
	title = {A Procedure for Sample-Free Item Analysis},
	url = {https://doi.org/10.1177/001316446902900102},
	volume = {29},
	year = {1969},
	bdsk-url-1 = {https://doi.org/10.1177/001316446902900102}}

@article{Kruschke2011,
	abstract = { Psychologists have been trained to do data analysis by asking whether null values can be rejected. Is the difference between groups nonzero? Is choice accuracy not at chance level? These questions have been traditionally addressed by null hypothesis significance testing (NHST). NHST has deep problems that are solved by Bayesian data analysis. As psychologists transition to Bayesian data analysis, it is natural to ask how Bayesian analysis assesses null values. The article explains and evaluates two different Bayesian approaches. One method involves Bayesian model comparison (and uses Bayes factors). The second method involves Bayesian parameter estimation and assesses whether the null value falls among the most credible values. Which method to use depends on the specific question that the analyst wants to answer, but typically the estimation approach (not using Bayes factors) provides richer information than the model comparison approach. },
	author = {John K. Kruschke},
	doi = {10.1177/1745691611406925},
	eprint = {https://doi.org/10.1177/1745691611406925},
	journal = {Perspectives on Psychological Science},
	note = {PMID: 26168520},
	number = {3},
	pages = {299-312},
	title = {Bayesian Assessment of Null Values Via Parameter Estimation and Model Comparison},
	url = {https://doi.org/10.1177/1745691611406925},
	volume = {6},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1177/1745691611406925}}
@misc{ONeill-Kypraios2016,
      title={Bayesian model choice via mixture distributions with application to epidemics and population process models}, 
      author={Philip D. O'Neill and Theodore Kypraios},
      year={2016},
      eprint={1411.7888},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1411.7888}, 
}
@article{Jackman2000, title={Estimation and Inference Are Missing Data Problems: Unifying Social Science Statistics via Bayesian Simulation}, volume={8}, DOI={10.1093/oxfordjournals.pan.a029818}, number={4}, journal={Political Analysis}, author={Jackman, Simon}, year={2000}, pages={307–332}}

@article{Bliss1934,
	author = {C. I. Bliss},
	doi = {10.1126/science.79.2037.38},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.79.2037.38},
	journal = {Science},
	number = {2037},
	pages = {38-39},
	title = {The Method of Probits},
	url = {https://www.science.org/doi/abs/10.1126/science.79.2037.38},
	volume = {79},
	year = {1934},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.79.2037.38},
	bdsk-url-2 = {https://doi.org/10.1126/science.79.2037.38}}
@article{Klein-Spady1993,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/2951556},
 abstract = {This paper proposes an estimator for discrete choice models that makes no assumption concerning the functional form of the choice probability function, where this function can be characterized by an index. The estimator is shown to be consistent, asymptotically normally distributed, and to achieve the semiparametric efficiency bound. Monte-Carlo evidence indicates that there may be only modest efficiency losses relative to maximum likelihood estimation when the distribution of the disturbances is known, and that the small-sample behavior of the estimator in other cases is good.},
 author = {Roger W. Klein and Richard H. Spady},
 journal = {Econometrica},
 number = {2},
 pages = {387--421},
 publisher = {[Wiley, Econometric Society]},
 title = {An Efficient Semiparametric Estimator for Binary Response Models},
 urldate = {2024-10-01},
 volume = {61},
 year = {1993}
}
@article{Albert1992,
 ISSN = {03629791},
 URL = {http://www.jstor.org/stable/1165149},
 abstract = {The problem of estimating item parameters from a two-parameter normal ogive model is considered. Gibbs sampling (Gelfand & Smith, 1990) is used to simulate draws from the joint posterior distribution of the ability and item parameters. This method gives marginal posterior density estimates for any parameter of interest; these density estimates can be used to judge the accuracy of normal approximations based on maximum likelihood estimates. This simulation technique is illustrated using data from a mathematics placement exam.},
 author = {James H. Albert},
 journal = {Journal of Educational Statistics},
 number = {3},
 pages = {251--269},
 publisher = {[Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
 title = {Bayesian Estimation of Normal Ogive Item Response Curves Using Gibbs Sampling},
 urldate = {2024-10-01},
 volume = {17},
 year = {1992}
}
@article{Bafumi+2005, title={Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation}, volume={13}, DOI={10.1093/pan/mpi010}, number={2}, journal={Political Analysis}, author={Bafumi, Joseph and Gelman, Andrew and Park, David K. and Kaplan, Noah}, year={2005}, pages={171–187}}
@article{Lewis-Poole2004, title={Measuring Bias and Uncertainty in Ideal Point Estimates via the Parametric Bootstrap}, volume={12}, DOI={10.1093/pan/mph015}, number={2}, journal={Political Analysis}, author={Lewis, Jeffrey B. and Poole, Keith T.}, year={2004}, pages={105–127}}
@article{Carroll+2009,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791974},
 abstract = {DW-NOMINATE scores for the U.S. Congress are widely used measures of legislators' ideological locations over time. These scores have been used in a large number of studies in political science and closely related fields. In this paper, we extend the work of Lewis and Poole (2004) on the parametric bootstrap to DW-NOMINATE and obtain standard errors for the legislator ideal points. These standard errors are in the range of 1%–4% of the range of DW-NOMINATE coordinates.},
 author = {Royce Carroll and Jeffrey B. Lewis and James Lo and Keith T. Poole and Howard Rosenthal},
 journal = {Political Analysis},
 number = {3},
 pages = {261--275},
 publisher = {[Cambridge University Press, Oxford University Press, Society for Political Methodology]},
 title = {Measuring Bias and Uncertainty in DW-NOMINATE Ideal Point Estimates via the Parametric Bootstrap},
 urldate = {2024-10-01},
 volume = {17},
 year = {2009}
}
@article{Martin+2011,
  author = {Andrew D. Martin and Kevin M. Quinn and Jong Hee Park},
  year = {2011},
  title = {{MCMCpack: Markov chain Monte Carlo in R}},
  journal = {Journal of Statistical Software},
  volume = {42},
  number = {9},
  pages = {1-21},
  url = {https://doi.org/10.18637/jss.v042.i09}
}
@book{Arnold2018,
  author = {Jeffrey B. Arnold},
  year = {2018},
  title = {{Simon Jackman’s Bayesian Model Examples in Stan}},
  series = {},
  volume = {},
  edition = {},
  url = {https://jrnold.github.io/bugs-examples-in-stan/},
  publisher = {}
}

@article{坂本-柴山2017,
	author = {坂本佑太朗 and 柴山直},
	doi = {10.32146/bdajcs.6.31},
	journal = {データ分析の理論と応用},
	number = {1},
	pages = {31-45},
	title = {学力テストの下位領域に関する多次元IRT分析},
	volume = {6},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.32146/bdajcs.6.31}}

@article{宇佐美+2018,
	author = {宇佐美慧 and 荘島宏二郎 and 光永悠彦 and 登藤直弥},
	doi = {10.20587/pamjaep.60.0_24},
	journal = {日本教育心理学会総会発表論文集},
	pages = {24-25},
	title = {項目反応理論（IRT）の考え方と実践},
	volume = {60},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.20587/pamjaep.60.0_24},
  note = {スライドも公開されている: http://usami-lab.com/教育心理学会チュートリアル2018資料.pdf},
  }
@book{樋口知之*2011,
  author = {樋口知之 and 上野玄太 and 中野慎也 and 中村和幸 and 吉田亮},
  year = {2011},
  title = {データ同化入門―次世代のシミュレーション技術},
  series = {予測と発見の科学},
  volume = {6},
  edition = {},
  url = {https://www.asakura.co.jp/detail.php?book_code=12786},
  publisher = {朝倉書店}
}
@article{Hoerl-Kennard1970,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267351},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2024-10-02},
 volume = {12},
 year = {1970}
}
@incollection{Nickl2017,
  author = {Richard Nickl},
  booktitle = {Bernoulli News},
  publisher = {Bernoulli Society},
  title = {{On Bayesian Inference for Some Statistical Inverse Problems with Partial Differential Equations}},
  year = {2017},
  volume = {24},
  number = {2},
  url = {https://www.bernoullisociety.org/files/BernoulliNews2017-2.pdf},
}

@article{岡本-本間2002,
	author = {岡本良夫 and 本間生夫},
	doi = {10.1541/ieejeiss1987.122.9_1417},
	journal = {電気学会論文誌. C, 電子・情報・システム部門誌},
	number = {9},
	pages = {1417-1425},
	title = {脳波とその逆問題解析},
	volume = {122},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1541/ieejeiss1987.122.9_1417}}

@article{Ter-Pogossian1975,
	abstract = { An apparatus was developed for obtaining emission transaxial images of sections of organs containing positron-emitting radiopharmaceuticals. The detection system is a hexagonal array of 24 NaI (Tl) detectors connected to coincidence circuits to achieve the ``electronic'' collimation of annihilation photons. The image is formed by a computer-applied algorithm which provides quantitative reconstruction of the distribution of activity. Computer simulations, phantom and animal studies show that this approach is capable of providing images of better contrast and resolution than are obtained with scintillation cameras. Advantages of positron vs. single photon reconstruction tomography are discussed. },
	author = {Ter-Pogossian, Michel M. and Phelps, Michael E. and Hoffman, Edward J. and Mullani, Nizar A.},
	doi = {10.1148/114.1.89},
	eprint = {https://doi.org/10.1148/114.1.89},
	journal = {Radiology},
	note = {PMID: 1208874},
	number = {1},
	pages = {89-98},
	title = {A Positron-Emission Transaxial Tomograph for Nuclear Imaging (PETT)},
	url = {https://doi.org/10.1148/114.1.89},
	volume = {114},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1148/114.1.89}}

@book{Nakamura-Potthast2015,
	abstract = {This book provides a comprehensive introduction to the techniques, tools and methods for
        inverse problems and data assimilation, and is written at the interface between mathematics
        and applications for students, researchers and developers in mathematics, physics,
        engineering, acoustics, electromagnetics, meteorology, biology, environmental and other
        applied sciences. Basic analytic questions and tools are introduced, as well as a wide
        variety of concepts, methods and approaches to formulate and solve inverse problems. OCTAVE
        /MATLAB codes are included, which serve as a first step towards simulations and more
        sophisticated inversion or data assimilation algorithms.},
	author = {Nakamura, Gen and Potthast, Roland},
	doi = {10.1088/978-0-7503-1218-9},
	isbn = {978-0-7503-1218-9},
	publisher = {IOP Publishing},
	series = {2053-2563},
	title = {Inverse Modeling: An introduction to the theory and methods of inverse problems and data assimilation},
	url = {https://dx.doi.org/10.1088/978-0-7503-1218-9},
	year = {2015},
	bdsk-url-1 = {https://dx.doi.org/10.1088/978-0-7503-1218-9}}

@article{vanLeeuwen+2019,
	abstract = {Particle filters contain the promise of fully nonlinear data assimilation. They have been applied in numerous science areas, including the geosciences, but their application to high-dimensional geoscience systems has been limited due to their inefficiency in high-dimensional systems in standard settings. However, huge progress has been made, and this limitation is disappearing fast due to recent developments in proposal densities, the use of ideas from (optimal) transportation, the use of localization and intelligent adaptive resampling strategies. Furthermore, powerful hybrids between particle filters and ensemble Kalman filters and variational methods have been developed. We present a state-of-the-art discussion of present efforts of developing particle filters for high-dimensional nonlinear geoscience state-estimation problems, with an emphasis on atmospheric and oceanic applications, including many new ideas, derivations and unifications, highlighting hidden connections, including pseudo-code, and generating a valuable tool and guide for the community. Initial experiments show that particle filters can be competitive with present-day methods for numerical weather prediction, suggesting that they will become mainstream soon.},
	author = {van Leeuwen, Peter Jan and K{\"u}nsch, Hans R. and Nerger, Lars and Potthast, Roland and Reich, Sebastian},
	doi = {https://doi.org/10.1002/qj.3551},
	eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3551},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	keywords = {hybrids, localization, nonlinear data assimilation, particle filters, proposal densities},
	number = {723},
	pages = {2335-2365},
	title = {Particle filters for high-dimensional geoscience applications: A review},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3551},
	volume = {145},
	year = {2019},
	bdsk-url-1 = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3551},
	bdsk-url-2 = {https://doi.org/10.1002/qj.3551}}

@article{Kac1966,
	author = {Mark Kac},
	doi = {10.1080/00029890.1966.11970915},
	eprint = {https://doi.org/10.1080/00029890.1966.11970915},
	journal = {The American Mathematical Monthly},
	number = {4P2},
	pages = {1--23},
	publisher = {Taylor \& Francis},
	title = {Can One Hear the Shape of a Drum?},
	url = {https://doi.org/10.1080/00029890.1966.11970915},
	volume = {73},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1080/00029890.1966.11970915}}

@article{佐々木1958,
	author = {Y. Sasaki},
	doi = {10.2151/jmsj1923.36.3_77},
	journal = {氣象集誌. 第2輯},
	number = {3},
	pages = {77-88},
	title = {An ObJective Analysis Based on the Variational Method},
	volume = {36},
	year = {1958},
	bdsk-url-1 = {https://doi.org/10.2151/jmsj1923.36.3_77}}

@article{浅井冨雄1967,
	author = {浅井冨雄},
	doi = {10.2151/jmsj1965.45.3_251},
	journal = {Journal of the Meteorological Society of Japan. Ser. II},
	number = {3},
	pages = {251-260},
	title = {細胞状積雲対流の特性について},
	volume = {45},
	year = {1967},
	bdsk-url-1 = {https://doi.org/10.2151/jmsj1965.45.3_251}}

@article{Yano+2018,
	address = {Boston MA, USA},
	author = {Jun-Ichi Yano and Micha{\l} Z. Ziemia{\'n}ski and Mike Cullen and Piet Termonia and Jeanette Onvlee and Lisa Bengtsson and Alberto Carrassi and Richard Davy and Anna Deluca and Suzanne L. Gray and V{\'\i}ctor Homar and Martin K{\"o}hler and Simon Krichak and Silas Michaelides and Vaughan T. J. Phillips and Pedro M. M. Soares and Andrzej A. Wyszogrodzki},
	doi = {10.1175/BAMS-D-17-0125.1},
	journal = {Bulletin of the American Meteorological Society},
	number = {4},
	pages = {699 - 710},
	publisher = {American Meteorological Society},
	title = {Scientific Challenges of Convective-Scale Numerical Weather Prediction},
	url = {https://journals.ametsoc.org/view/journals/bams/99/4/bams-d-17-0125.1.xml},
	volume = {99},
	year = {2018},
	bdsk-url-1 = {https://journals.ametsoc.org/view/journals/bams/99/4/bams-d-17-0125.1.xml},
	bdsk-url-2 = {https://doi.org/10.1175/BAMS-D-17-0125.1}}
@incollection{Lorenz1995,
  author = {Edward N. Lorenz},
  booktitle = {Seminar on Predictability, 4-8 September 1995},
  publisher = {ECMWF},
  title = {Predictability: A Problem Partly Solved},
  year = {1995},
  url = {https://www.ecmwf.int/en/elibrary/75462-predictability-problem-partly-solved},
}

@software{Milan2021,
	author = {Milan K},
	doi = {10.5281/zenodo.5121430},
	month = jul,
	publisher = {Zenodo},
	title = {milankl/Lorenz96.jl: v0.3.0},
	url = {https://doi.org/10.5281/zenodo.5121430},
	version = {v0.3.0},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.5121430}}
@book{Balwada+2023,
  author = {Balwada, D., Abernathey, R., Acharya, S., Adcroft, A., Brener, J., Balaji, V. and Zanna, L.},
  year = {2023},
  title = {{Learning Machine Learning with Lorenz-96}},
  series = {},
  volume = {},
  edition = {},
  url     = {https://m2lines.github.io/L96_demo/intro.html},
  publisher = {Authorea Preprints}
}
@phdthesis{vanKekem2018,
  author = {Dirk Leendert van{/ }Kekem},
  school = {University of Groningen},
  title = {Dynamics of the Lorenz-96 model: Bifurcations, symmetries and waves},
  year = {2018},
  url  = {https://research.rug.nl/en/publications/dynamics-of-the-lorenz-96-model-bifurcations-symmetries-and-waves}
}
@articleInfo{Kerin-Engler2022,
title = {On the Lorenz '96 model and some generalizations},
journal = {Discrete and Continuous Dynamical Systems - B},
volume = {27},
number = {2},
pages = {769-797},
year = {2022},
issn = {1531-3492},
doi = {10.3934/dcdsb.2021064},
url = {https://www.aimsciences.org/article/id/a8ebee41-67f0-484c-abc6-99061059ace6},
author = {John Kerin and Hans Engler},
keywords = {Lorenz-96 model, equivariant dynamical system, Hopf bifurcation, normal form, Neimark-Sacker bifurcation}
}

@article{Kantas+2014,
	abstract = { We consider the inverse problem of estimating the initial condition of a partial differential equation, which is observed only through noisy measurements at discrete time intervals. In particular, we focus on the case where Eulerian measurements are obtained from the time and space evolving vector field, whose evolution obeys the two-dimensional Navier--Stokes equations defined on a torus. This context is particularly relevant to the area of numerical weather forecasting and data assimilation. We will adopt a Bayesian formulation resulting from a particular regularization that ensures the problem is well posed. In the context of Monte Carlo--based inference, it is a challenging task to obtain samples from the resulting high-dimensional posterior on the initial condition. In real data assimilation applications it is common for computational methods to invoke the use of heuristics and Gaussian approximations. As a result, the resulting inferences are biased and not well justified in the presence of nonlinear dynamics and observations. On the other hand, Monte Carlo methods can be used to assimilate data in a principled manner, but they are often perceived as inefficient in this context due to the high dimensionality of the problem. In this work we will propose a generic sequential Monte Carlo (SMC) sampling approach for high-dimensional inverse problems that overcomes these difficulties. The method builds upon ``state of the art'' Markov chain Monte Carlo (MCMC) techniques, which are currently considered as benchmarks for evaluating data assimilation algorithms used in practice. SMC samplers can improve in terms of efficiency, as they possess greater flexibility and one can include steps like sequential tempering, adaptation, and parallelization with a relatively low number of extra computations. We will illustrate this using numerical examples, where our proposed SMC approach can achieve the same accuracy as MCMC but in a much more efficient manner. },
	author = {Kantas, Nikolas and Beskos, Alexandros and Jasra, Ajay},
	doi = {10.1137/130930364},
	eprint = {https://doi.org/10.1137/130930364},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	number = {1},
	pages = {464-489},
	title = {Sequential Monte Carlo Methods for High-Dimensional Inverse Problems: A Case Study for the Navier--Stokes Equations},
	url = {https://doi.org/10.1137/130930364},
	volume = {2},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1137/130930364}}

@article{Chorin1967,
	author = {Alexandre Joel Chorin},
	journal = {Bulletin of the American Mathematical Society},
	number = {6},
	pages = {928 -- 931},
	publisher = {American Mathematical Society},
	title = {{The numerical solution of the Navier-Stokes equations for an incompressible fluid}},
	volume = {73},
	year = {1967},
  url = {https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society-new-series/volume-73/issue-6/The-numerical-solution-of-the-Navier-Stokes-equations-for-an/bams/1183529112.full},
  }
@article{Chorin1968,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2004575},
 abstract = {A finite-difference method for solving the time-dependent Navier Stokes equations for an incompressible fluid is introduced. This method uses the primitive variables, i.e. the velocities and the pressure, and is equally applicable to problems in two and three space dimensions. Test problems are solved, and an application to a three-dimensional convection problem is presented.},
 author = {Alexandre Joel Chorin},
 journal = {Mathematics of Computation},
 number = {104},
 pages = {745--762},
 publisher = {American Mathematical Society},
 title = {Numerical Solution of the Navier-Stokes Equations},
 urldate = {2024-10-05},
 volume = {22},
 year = {1968}
}

@article{Lorenz1963,
	address = {Boston MA, USA},
	author = {Edward N. Lorenz},
	doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
	journal = {Journal of Atmospheric Sciences},
	number = {2},
	pages = {130 - 141},
	publisher = {American Meteorological Society},
	title = {Deterministic Nonperiodic Flow},
	url = {https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml},
	volume = {20},
	year = {1963},
	bdsk-url-1 = {https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml},
	bdsk-url-2 = {https://doi.org/10.1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2}}
@book{Temam1995,
  author = {Roger Meyer Temam},
  year = {1995},
  title = {Navier–Stokes Equations and Nonlinear Functional Analysis},
  series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
  volume = {},
  edition = {2},
  doi = {10.1137/1.9781611970050},
  publisher = {Society for Industrial and Applied Mathematics}
}
@book{Tsai2018,
  author = {Tai-Peng Tsai},
  year = {2018},
  title = {Lectures on Navier-Stokes Equations},
  series = {Graduate Studies in Mathematics},
  volume = {192},
  edition = {},
  doi = {10.1090/gsm/192},
  publisher = {American Mathematical Society}
}

@article{Cox-Matthews2002,
	abstract = {We develop a class of numerical methods for stiff systems, based on the method of exponential time differencing. We describe schemes with second- and higher-order accuracy, introduce new Runge--Kutta versions of these schemes, and extend the method to show how it may be applied to systems whose linear part is nondiagonal. We test the method against other common schemes, including integrating factor and linearly implicit methods, and show how it is more accurate in a number of applications. We apply the method to both dissipative and dispersive partial differential equations, after illustrating its behavior using forced ordinary differential equations with stiff linear parts.},
	author = {S.M. Cox and P.C. Matthews},
	doi = {https://doi.org/10.1006/jcph.2002.6995},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	number = {2},
	pages = {430-455},
	title = {Exponential Time Differencing for Stiff Systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999102969950},
	volume = {176},
	year = {2002},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999102969950},
	bdsk-url-2 = {https://doi.org/10.1006/jcph.2002.6995}}

@article{Sanderse-Koren2012,
	abstract = {This paper investigates the temporal accuracy of the velocity and pressure when explicit Runge--Kutta methods are applied to the incompressible Navier--Stokes equations. It is shown that, at least up to and including fourth order, the velocity attains the classical order of accuracy without further constraints. However, in case of a time-dependent gradient operator, which can appear in case of time-varying meshes, additional order conditions need to be satisfied to ensure the correct order of accuracy. Furthermore, the pressure is only first-order accurate unless additional order conditions are satisfied. Two new methods that lead to a second-order accurate pressure are proposed, which are applicable to a certain class of three- and four-stage methods. A special case appears when the boundary conditions for the continuity equation are independent of time, since in that case the pressure can be computed to the same accuracy as the velocity field, without additional cost. Relevant computations of decaying vortices and of an actuator disk in a time-dependent inflow support the analysis and the proposed methods.},
	author = {B. Sanderse and B. Koren},
	doi = {https://doi.org/10.1016/j.jcp.2011.11.028},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	keywords = {Differential--algebraic equations, Incompressible Navier--Stokes equations, Temporal accuracy, Time integration, Runge--Kutta method, Moving meshes},
	number = {8},
	pages = {3041-3063},
	title = {Accuracy analysis of explicit Runge--Kutta methods applied to the incompressible Navier--Stokes equations},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999111006838},
	volume = {231},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999111006838},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcp.2011.11.028}}

@article{Tsitouras2011,
	abstract = {Among the most popular methods for the solution of the Initial Value Problem are the Runge--Kutta pairs of orders 5 and 4. These methods can be derived solving a system of nonlinear equations for its coefficients. To achieve this, we usually admit various simplifying assumptions. The most common of them are the so-called row simplifying assumptions. Here we neglect them and present an algorithm for the construction of Runge--Kutta pairs of orders 5 and 4 based only in the first column simplifying assumption. The result is a pair that outperforms other known pairs in the bibliography when tested to the standard set of problems of DETEST. A cost free fourth order formula is also derived for handling dense output.},
	author = {Ch. Tsitouras},
	doi = {https://doi.org/10.1016/j.camwa.2011.06.002},
	issn = {0898-1221},
	journal = {Computers & Mathematics with Applications},
	keywords = {Runge--Kutta, Truncation error, Non-linear algebraic systems, Free parameters, Dense output},
	number = {2},
	pages = {770-775},
	title = {Runge--Kutta pairs of order 5(4) satisfying only the first column simplifying assumption},
	url = {https://www.sciencedirect.com/science/article/pii/S0898122111004706},
	volume = {62},
	year = {2011},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0898122111004706},
	bdsk-url-2 = {https://doi.org/10.1016/j.camwa.2011.06.002}}
@article{Schrodinger1931,
  author = {E. Schrödinger},
  year = {1931},
  title = {Über die Umkehrung der Naturgesetze},
  journal = {Sitzungsberichte der preussischen Akademie der Wissenschaften, physikalische mathematische Klasse},
  volume = {8},
  number = {9},
  pages = {144-153},
  url = {}
}

@article{Chetrite+2021,
	abstract = {We present an English translation of Erwin Schr{\"o}dinger's paper on ``On the Reversal of the Laws of Nature`'. In this paper, Schr{\"o}dinger analyses the idea of time reversal of a diffusion process. Schr{\"o}dinger's paper acted as a prominent source of inspiration for the works of Bernstein on reciprocal processes and of Kolmogorov on time reversal properties of Markov processes and detailed balance. The ideas outlined by Schr{\"o}dinger also inspired the development of probabilistic interpretations of quantum mechanics by F{\'e}nyes, Nelson and others as well as the notion of ``Euclidean Quantum Mechanics''as probabilistic analogue of quantization. In the second part of the paper, Schr{\"o}dinger discusses the relation between time reversal and statistical laws of physics. We emphasize in our commentary the relevance of Schr{\"o}dinger's intuitions for contemporary developments in statistical nano-physics.},
	author = {Chetrite, Rapha{\"e}l and Muratore-Ginanneschi, Paolo and Schwieger, Kay},
	date = {2021/11/22},
	date-added = {2024-10-06 14:58:11 +0900},
	date-modified = {2024-10-06 14:58:11 +0900},
	doi = {10.1140/epjh/s13129-021-00032-7},
	id = {Chetrite2021},
	isbn = {2102-6467},
	journal = {The European Physical Journal H},
	number = {1},
	pages = {28},
	title = {E. Schr{\"o}dinger's 1931 paper ``On the Reversal of the Laws of Nature''{$[$}``{\"U}ber die Umkehrung der Naturgesetze'', Sitzungsberichte der preussischen Akademie der Wissenschaften, physikalisch-mathematische Klasse, 8 N9 144--153{$]$}},
	url = {https://doi.org/10.1140/epjh/s13129-021-00032-7},
	volume = {46},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1140/epjh/s13129-021-00032-7}}

@InProceedings{Sharrock+2024,
  title = 	 {Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models},
  author =       {Sharrock, Louis and Simons, Jack and Liu, Song and Beaumont, Mark},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {44565--44602},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/sharrock24a/sharrock24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/sharrock24a.html},
  abstract = 	 {We introduce Sequential Neural Posterior Score Estimation (SNPSE), a score-based method for Bayesian inference in simulator-based models. Our method, inspired by the remarkable success of score-based methods in generative modelling, leverages conditional score-based diffusion models to generate samples from the posterior distribution of interest. The model is trained using an objective function which directly estimates the score of the posterior. We embed the model into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We also introduce several alternative sequential approaches, and discuss their relative merits. We then validate our method, as well as its amortised, non-sequential, variant on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE).}
}

@article{Neal2003,
	author = {Radford M. Neal},
	doi = {10.1214/aos/1056562461},
	journal = {The Annals of Statistics},
	keywords = {Adaptive methods, auxiliary variables, dynamical methods, Gibbs sampling, Markov chain Monte Carlo, Metropolis algorithm, overrelaxation},
	number = {3},
	pages = {705 -- 767},
	publisher = {Institute of Mathematical Statistics},
	title = {{Slice sampling}},
	url = {https://doi.org/10.1214/aos/1056562461},
	volume = {31},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1056562461}}
@articleInfo{Leonard2014,
title = {A survey of the Schrödinger problem and some of its  connections with optimal transport},
journal = {Discrete and Continuous Dynamical Systems},
volume = {34},
number = {4},
pages = {1533-1574},
year = {2014},
issn = {1078-0947},
doi = {10.3934/dcds.2014.34.1533},
url = {https://www.aimsciences.org/article/id/d5bcf817-901d-4104-b7da-eade7847c53e},
author = {Christian Léonard},
keywords = {Schrödinger problem, optimal transport, displacement interpolations, Markov measures, relative entropy, large deviations}
}

@article{Eldan+2020,
	author = {Ronen Eldan and Joseph Lehec and Yair Shenfeld},
	doi = {10.1214/19-AIHP1038},
	journal = {Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et Statistiques},
	keywords = {Quantitative functional inequalities, Stochastic methods},
	number = {3},
	pages = {2253 -- 2269},
	publisher = {Institut Henri Poincar{\'e}},
	title = {{Stability of the logarithmic Sobolev inequality via the F{\"o}llmer process}},
	url = {https://doi.org/10.1214/19-AIHP1038},
	volume = {56},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1214/19-AIHP1038}}

@article{Jamison1975,
	author = {Jamison, Benton},
	date = {1975/12/01},
	date-added = {2024-10-06 21:47:06 +0900},
	date-modified = {2024-10-06 21:47:06 +0900},
	doi = {10.1007/BF00535844},
	id = {Jamison1975},
	isbn = {1432-2064},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	number = {4},
	pages = {323--331},
	title = {The Markov processes of Schr{\"o}dinger},
	url = {https://doi.org/10.1007/BF00535844},
	volume = {32},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1007/BF00535844}}

@article{DaiPra1991,
	abstract = {The problem of forcing a nondegenerate diffusion process to a given final configuration is considered. Using the logarithmic transformation approach developed by Fleming, it is shown that the perturbation of the drift suggested by Jamison solves an optimal stochastic control problem. Such perturbation happens to have minimum energy between all controls that ``bring''the diffusion to the desired final distribution. A special property of the change of measure on the path-space that corresponds to the aforesaid perturbation of the drift is also shown.},
	author = {Dai{\ }Pra, Paolo},
	date = {1991/01/01},
	date-added = {2024-10-06 21:48:11 +0900},
	date-modified = {2024-10-06 21:48:11 +0900},
	doi = {10.1007/BF01442404},
	id = {Dai Pra1991},
	isbn = {1432-0606},
	journal = {Applied Mathematics and Optimization},
	number = {1},
	pages = {313--329},
	title = {A stochastic control approach to reciprocal diffusion processes},
	url = {https://doi.org/10.1007/BF01442404},
	volume = {23},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1007/BF01442404}}
@misc{Huang+2021SF,
      title={{Schr{\"o}dinger-F{\"o}llmer Sampler: Sampling without Ergodicity}}, 
      author={Jian Huang and Yuling Jiao and Lican Kang and Xu Liao and Jin Liu and Yanyan Liu},
      year={2021},
      eprint={2106.10880},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2106.10880}, 
}
@misc{Jiao+2021,
      title={Convergence Analysis of Schr{\"o}dinger-F{\"o}llmer Sampler without Convexity}, 
      author={Yuling Jiao and Lican Kang and Yanyan Liu and Youzhou Zhou},
      year={2021},
      eprint={2107.04766},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2107.04766}, 
}

@article{Chopin-Ridgway2017,
	author = {Nicolas Chopin and James Ridgway},
	doi = {10.1214/16-STS581},
	journal = {Statistical Science},
	keywords = {Bayesian computation, expectation propagation, Markov chain Monte Carlo, sequential Monte Carlo, variational inference},
	number = {1},
	pages = {64 -- 87},
	publisher = {Institute of Mathematical Statistics},
	title = {{Leave Pima Indians Alone: Binary Regression as a Benchmark for Bayesian Computation}},
	url = {https://doi.org/10.1214/16-STS581},
	volume = {32},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/16-STS581}}

@article{Breiman2001DM,
	author = {Leo Breiman},
	doi = {10.1214/ss/1009213726},
	journal = {Statistical Science},
	number = {3},
	pages = {199 -- 231},
	publisher = {Institute of Mathematical Statistics},
	title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
	url = {https://doi.org/10.1214/ss/1009213726},
	volume = {16},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1214/ss/1009213726}}
@article{Firth1993,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336755},
 abstract = {It is shown how, in regular parametric problems, the first-order term is removed from the asymptotic bias of maximum likelihood estimates by a suitable modification of the score function. In exponential families with canonical parameterization the effect is to penalize the likelihood by the Jeffreys invariant prior. In binomial logistic models, Poisson log linear models and certain other generalized linear models, the Jeffreys prior penalty function can be imposed in standard regression software using a scheme of iterative adjustments to the data.},
 author = {David Firth},
 journal = {Biometrika},
 number = {1},
 pages = {27--38},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Bias Reduction of Maximum Likelihood Estimates},
 urldate = {2024-10-07},
 volume = {80},
 year = {1993}
}

@article{Gelman+2008,
	author = {Andrew Gelman and Aleks Jakulin and Maria Grazia Pittau and Yu-Sung Su},
	doi = {10.1214/08-AOAS191},
	journal = {The Annals of Applied Statistics},
	keywords = {Bayesian inference, generalized linear model, hierarchical model, least squares, Linear regression, logistic regression, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	number = {4},
	pages = {1360 -- 1383},
	publisher = {Institute of Mathematical Statistics},
	title = {{A weakly informative default prior distribution for logistic and other regression models}},
	url = {https://doi.org/10.1214/08-AOAS191},
	volume = {2},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1214/08-AOAS191}}
