@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
@inproceedings{Zhao+2024,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
	  booksubtitle    = {International Conference on Machine Learning},
      url={https://arxiv.org/abs/2404.17546}, 
}
@article{Teh+2003,
author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
title = {Energy-based models for sparse overcomplete representations},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1235–1260},
numpages = {26},
url = {https://www.jmlr.org/papers/v4/teh03a.html}
}

@InProceedings{Xie+2016,
  title = 	 {A Theory of Generative ConvNet},
  author = 	 {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2635--2644},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/xiec16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/xiec16.html},
  abstract = 	 {We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.}
}
@inproceedings{He+2019,
      title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders}, 
      author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
      year={2019},
      booksubtitle    = {International Conference on Learning Representations},
      url={https://arxiv.org/abs/1901.05534}, 
}
@inproceedings{Hinton-Teh2001,
author = {Hinton, Geoffrey E. and Teh, Yee-Whye},
title = {Discovering multiple constraints that are frequently approximately satisfied},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {227–234},
numpages = {8},
location = {Seattle, Washington},
series = {UAI'01},
url             = {https://dl.acm.org/doi/abs/10.5555/2074022.2074051},
}
@misc{Finn+2016,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03852}, 
}
@inproceedings{Swersku+2011,
author = {Swersky, Kevin and Ranzato, Marc'Aurelio and Buchman, David and Marlin, Benjamin M. and Freitas, Nandode},
title = {On autoencoders and score matching for energy based models},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider estimation methods for the class of continuous-data energy based models (EBMs). Our main result shows that estimating the parameters of an EBM using score matching when the conditional distribution over the visible units is Gaussian corresponds to training a particular form of regularized autoencoder. We show how different Gaussian EBMs lead to different autoencoder architectures, providing deep links between these two families of models. We compare the score matching estimator for the mPoT model, a particular Gaussian EBM, to several other training methods on a variety of tasks including image denoising and unsupervised feature extraction. We show that the regularization function induced by score matching leads to superior classification performance relative to a standard autoencoder. We also show that score matching yields classification results that are indistinguishable from better-known stochastic approximation maximum likelihood estimators.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {1201–1208},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104633},
}

@inproceedings{Koster-Hyvarinen2007,
	abstract = {Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Hyv{\"a}rinen, Aapo},
	booktitle = {Artificial Neural Networks -- ICANN 2007},
	editor = {de S{\'a}, Joaquim Marques and Alexandre, Lu{\'\i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
	isbn = {978-3-540-74695-9},
	pages = {798--807},
	publisher = {Springer Berlin Heidelberg},
	title = {A Two-Layer ICA-Like Model Estimated by Score Matching},
	year = {2007}}

@inproceedings{Koster+2009,
	abstract = {Markov Random Field (MRF) models with potentials learned from the data have recently received attention for learning the low-level structure of natural images. A MRF provides a principled model for whole images, unlike ICA, which can in practice be estimated for small patches only. However, learning the filters in an MRF paradigm has been problematic in the past since it required computationally expensive Monte Carlo methods. Here, we show how MRF potentials can be estimated using Score Matching (SM). With this estimation method we can learn filters of size 12 {\texttimes}12 pixels, considerably larger than traditional ''hand-crafted'' MRF potentials. We analyze the tuning properties of the filters in comparison to ICA filters, and show that the optimal MRF potentials are similar to the filters from an overcomplete ICA model.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Lindgren, Jussi T. and Hyv{\"a}rinen, Aapo},
	booktitle = {Independent Component Analysis and Signal Separation},
	editor = {Adali, T{\"u}lay and Jutten, Christian and Romano, Jo{\~a}o Marcos Travassos and Barros, Allan Kardec},
	isbn = {978-3-642-00599-2},
	pages = {515--522},
	publisher = {Springer Berlin Heidelberg},
	title = {Estimating Markov Random Field Potentials for Natural Images},
	year = {2009}}

@InProceedings{Gutmann-Hyvarinen2010,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract = 	 {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}

@InProceedings{Perpinan-Hinton2005,
  title = 	 {On Contrastive Divergence Learning},
  author =       {Carreira-Perpi{\~n}\'an, Miguel \'A. and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {33--40},
  year = 	 {2005},
  editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
  volume = 	 {R5},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--08 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf},
  url = 	 {https://proceedings.mlr.press/r5/carreira-perpinan05a.html},
  note =         {Reissued by PMLR on 30 March 2021.}
}

@inproceedings{Kingma-LeCun2010,
	author = {Durk P Kingma and Yann LeCun},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
	publisher = {Curran Associates, Inc.},
	title = {Regularized estimation of image statistics by Score Matching},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},
	volume = {23},
	year = {2010},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf}}
@ARTICLE{Drucker-LeCun1992,
  author={Drucker, H. and Le Cun, Y.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Improving generalization performance using double backpropagation}, 
  year={1992},
  volume={3},
  number={6},
  pages={991-997},
  keywords={Testing;Backpropagation algorithms;Jacobian matrices;Neurons;Signal to noise ratio;Neural networks},
  doi={10.1109/72.165600}}
@inproceedings{Song+2019,
      title={{Sliced Score Matching: A Scalable Approach to Density and Score Estimation}},
      author={Yang Song and Sahaj Garg and Jiaxin Shi and Stefano Ermon},
      year={2019},
      booksubtitle    = {Uncertainty in Artificial Intelligence},
      url={https://arxiv.org/abs/1905.07088}, 
}
@ARTICLE{Hyvarinen2007,
  author={Hyvarinen, Aapo},
  journal={IEEE Transactions on Neural Networks}, 
  title={Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables}, 
  year={2007},
  volume={18},
  number={5},
  pages={1529-1531},
  keywords={Samarium;Statistical analysis;Probability density function;Parameter estimation;Monte Carlo methods;Computer science;Information technology;Normalization constant;partition function;statistical estimation},
  doi={10.1109/TNN.2007.895819}}
@inproceedings{Lyu2009,
author = {Lyu, Siwei},
title = {Interpretation and generalization of score matching},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {359–366},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@inproceedings{Gutmann-Hirayama2011,
author = {Gutmann, Michael U. and Hirayama, Jun-ichiro},
title = {Bregman divergence as general framework to estimate unnormalized statistical models},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in un-supervised learning.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {283–290},
numpages = {8},
location = {Barcelona, Spain},
series = {UAI'11}
}

@InProceedings{Chwialkowski+2016,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}

@InProceedings{Liu+2016,
  title = 	 {A Kernelized Stein Discrepancy for Goodness-of-fit Tests},
  author = 	 {Liu, Qiang and Lee, Jason and Jordan, Michael},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {276--284},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/liub16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/liub16.html},
  abstract = 	 {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.}
}
@misc{McAllester2023,
      title={On the Mathematics of Diffusion Models}, 
      author={David McAllester},
      year={2023},
      eprint={2301.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.11108}, 
}
@article{Yang+2023,
author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626235},
doi = {10.1145/3626235},
abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github:},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {39},
keywords = {stochastic differential equations, score-based generative models, diffusion models, Generative models}
}
@unpublished{Kreis+2022,
    author = {Karsten Kreis and Ruiqi Gao and Arash Vahdat},
    year   = {2022},
    title  = {Denoising Diffusion-based Generative Modeling: Foundations and Applications},
    url    = {https://cvpr2022-tutorial-diffusion-models.github.io/},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@unpublished{Song+2023,
    author = {Jiaming Song and Chenlin Meng and Arash Vahdat},
    year   = {2023},
    title  = {Denoising Diffusion Models: A Generative Learning Big Bang},
    url    = {https://cvpr.thecvf.com/virtual/2023/tutorial/18546},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@INPROCEEDINGS{Choi+2022,
  author={Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Perception Prioritized Training of Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={11462-11471},
  keywords={Training;Visualization;Computational modeling;Noise reduction;Data models;Image restoration;Pattern recognition;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01118}}
@inproceedings{Kingma+2021,
author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
title = {Variational diffusion models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1660},
numpages = {12},
series = {NIPS '21},
url             = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
}
@inproceedings{Salimans-Ho2021,
title={Should {EBM}s model the energy or the score?},
author={Tim Salimans and Jonathan Ho},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
url={https://openreview.net/forum?id=9AS-TF2jRNb}
}

@article{Anderson1982,
	abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.},
	author = {Brian D.O. Anderson},
	doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	number = {3},
	pages = {313-326},
	title = {Reverse-time diffusion equation models},
	url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	volume = {12},
	year = {1982},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	bdsk-url-2 = {https://doi.org/10.1016/0304-4149(82)90051-5}}

@article{Haussmann-Pardoux1986,
	author = {U. G. Haussmann and E. Pardoux},
	doi = {10.1214/aop/1176992362},
	journal = {The Annals of Probability},
	keywords = {diffusion process, Kolmogorov equation, Markov process, Martingale problem, Time reversal},
	number = {4},
	pages = {1188 -- 1205},
	publisher = {Institute of Mathematical Statistics},
	title = {{Time Reversal of Diffusions}},
	url = {https://doi.org/10.1214/aop/1176992362},
	volume = {14},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176992362}}
@inproceedings{Karras+2022,
title={Elucidating the Design Space of Diffusion-Based Generative Models},
author={Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=k7FuTOWMOc7}
}
@ARTICLE {Croitoru+2023,
author = {F. Croitoru and V. Hondru and R. Ionescu and M. Shah},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Diffusion Models in Vision: A Survey},
year = {2023},
volume = {45},
number = {09},
issn = {1939-3539},
pages = {10850-10869},
abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
keywords = {computational modeling;mathematical models;noise reduction;data models;computer vision;training;task analysis},
doi = {10.1109/TPAMI.2023.3261988},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2023.3261988},
}

@ARTICLE{Cao+2024,
  author={Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Generative Diffusion Models}, 
  year={2024},
  volume={36},
  number={7},
  pages={2814-2830},
  keywords={Mathematical models;Kernel;Computational modeling;Training;Surveys;Noise reduction;Markov processes;Diffusion model;deep generative model;diffusion algorithm;diffusion applications},
  doi={10.1109/TKDE.2024.3361474}}
@inproceedings{JiamingSong+2021,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}
@inproceedings{Gao+2021,
title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P Kingma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v_1Soh8QUNc}
}
@inproceedings{Xiao+2021,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JprM0p-q0Co}
}
@inproceedings{Salimans-Ho2022,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}
@inproceedings{Vahdat+2021,
	author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {11287--11302},
	publisher = {Curran Associates, Inc.},
	title = {Score-based Generative Modeling in Latent Space},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf}}

@article{Pandey+2022,
title={Diffuse{VAE}: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents},
author={Kushagra Pandey and Avideep Mukherjee and Piyush Rai and Abhishek Kumar},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ygoNPRiLxw},
note={}
}

@InProceedings{Nichol-Dhariwal2021,
  title = 	 {Improved Denoising Diffusion Probabilistic Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}
@inproceedings{Ho-Salimans2021,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}
@article{Ho+2022,
  author  = {Jonathan Ho and Chitwan Saharia and William Chan and David J. Fleet and Mohammad Norouzi and Tim Salimans},
  title   = {Cascaded Diffusion Models for High Fidelity Image Generation},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {47},
  pages   = {1--33},
  url     = {http://jmlr.org/papers/v23/21-0635.html}
}
@misc{Saharia+2022SIGGRAPH,
title={Palette: Image-to-Image Diffusion Models},
author={Chitwan Saharia and William Chan and Huiwen Chang and Chris A. Lee and Jonathan Ho and Tim Salimans and David J. Fleet and Mohammad Norouzi},
year={2022},
url={https://openreview.net/forum?id=FPGs276lUeq}
}

@inproceedings{Austin+2021,
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17981--17993},
	publisher = {Curran Associates, Inc.},
	title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf}}
@InProceedings{Chang+2022,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@article{Heng+2024,
	author = {Jeremy Heng and Valentin De Bortoli and Arnaud Doucet},
	doi = {10.1214/23-STS908},
	journal = {Statistical Science},
	keywords = {Optimal transport, Schr{\"o}dinger bridge, score matching, Stochastic differential equation, Time reversal},
	number = {1},
	pages = {90 -- 99},
	publisher = {Institute of Mathematical Statistics},
	title = {{Diffusion Schr{\"o}dinger Bridges for Bayesian Computation}},
	url = {https://doi.org/10.1214/23-STS908},
	volume = {39},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1214/23-STS908}}
@inproceedings{Chung+2023,
title={Diffusion Posterior Sampling for General Noisy Inverse Problems},
author={Hyungjin Chung and Jeongsol Kim and Michael Thompson Mccann and Marc Louis Klasky and Jong Chul Ye},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=OnD9zGAGT0k}
}

@InProceedings{Song+2023,
  title = 	 {Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation},
  author =       {Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32483--32498},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23k/song23k.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23k.html},
  abstract = 	 {We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion (LGD), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a Monte Carlo method that uses multiple samples from a suitable distribution to reduce bias. Our method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and controllable motion synthesis. Notably, we show how our method can be applied to control a pretrained motion diffusion model to follow certain paths and avoid obstacles that are proven challenging to prior methods.}
}

@InProceedings{Shi+2022,
  title = 	 {Conditional simulation using diffusion {S}chr{ö}dinger bridges},
  author =       {Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1792--1802},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/shi22a/shi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/shi22a.html},
  abstract = 	 {Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr{ö}dinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schrödinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.}
}

@inproceedings{DeBortoli+2021,
	author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17695--17709},
	publisher = {Curran Associates, Inc.},
	title = {Diffusion Schr\"{o}dinger Bridge with Applications to Score-Based Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf}}

@InProceedings{Kurras2015,
  title = 	 {{Symmetric Iterative Proportional Fitting}},
  author = 	 {Kurras, Sven},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {526--534},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/kurras15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/kurras15.html},
  abstract = 	 {Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix W*. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W=W’ and r=c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of W* in terms of expansion properties of the undirected weighted graph represented by W. Finally, we show how our results contribute to recent work in machine learning.}
}
@article{Sinkhorn1967,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2314570},
 author = {Richard Sinkhorn},
 journal = {The American Mathematical Monthly},
 number = {4},
 pages = {402--405},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Diagonal Equivalence to Matrices with Prescribed Row and Column Sums},
 urldate = {2024-08-04},
 volume = {74},
 year = {1967}
}

@article{Sinkhorn-Knopp1967,
	author = {Richard Sinkhorn and Paul Knopp},
	journal = {Pacific Journal of Mathematics},
	number = {2},
	pages = {343 -- 348},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{Concerning Nonnegative Matrices and Doubly Stochastic Matrices}},
	volume = {21},
	year = {1967},
  url             = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-21/issue-2/Concerning-nonnegative-matrices-and-doubly-stochastic-matrices/pjm/1102992505.full},
}

@article{Fortet1940,
    author          = {Robert Fortet},
    year            = {1940},
    title           = {{Résolution d'un système d'équations de M. Schrödinger}},
    journal         = {Journal de Mathématiques Pures et Appliquées, Series 9},
    volume          = {19},
    number          = {1-4},
    pages           = {83-105},
    url             = {http://www.numdam.org/item/JMPA_1940_9_19_1-4_83_0/}
}
@article{Deming-Stephan1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235722},
 author = {W. Edwards Deming and Frederick F. Stephan},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {427--444},
 publisher = {Institute of Mathematical Statistics},
 title = {On a Least Squares Adjustment of a Sampled Frequency Table When the Expected Marginal Totals are Known},
 urldate = {2024-08-04},
 volume = {11},
 year = {1940}
}
@article{Kullback1968,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239692},
 author = {S. Kullback},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {1236--1243},
 publisher = {Institute of Mathematical Statistics},
 title = {Probability Densities with Given Marginals},
 urldate = {2024-08-04},
 volume = {39},
 year = {1968}
}
@article{Ireland-Kullback1968,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334462},
 abstract = {In its simplest formulation the problem considered is to estimate the cell probabilities pij of an r × c contingency table for which the marginal probabilities $p_{i\ldot}$ and $p_{\ldot j}$ are known and fixed, so as to minimize ΣΣpijln (pij/πij), where πij are the corresponding entries in a given contingency table. An iterative procedure is given for determining the estimates and it is shown that the estimates are BAN, and that the iterative procedure is convergent. A summary of results for a four-way contingency table is given. An illustrative example is given.},
 author = {C. T. Ireland and S. Kullback},
 journal = {Biometrika},
 number = {1},
 pages = {179--188},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Contingency Tables with Given Marginals},
 urldate = {2024-08-04},
 volume = {55},
 year = {1968}
}
@inproceedings{Vargas-Grathwohl-Doucet2023,
title={{Denoising Diffusion Samplers}},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@inproceedings{Follmer1985,
	abstract = {We introduce an entropy technique which allows to treat some infinite-dimensional extensions of the classical duality equations for the time reversal of diffusion processes.},
	address = {Berlin, Heidelberg},
	author = {F{\"o}llmer, H.},
	booktitle = {Stochastic Differential Systems Filtering and Control},
	editor = {Metivier, M. and Pardoux, E.},
	isbn = {978-3-540-39253-8},
	pages = {156--163},
	publisher = {Springer Berlin Heidelberg},
	title = {An entropy approach to the time reversal of diffusion processes},
	year = {1985}}

@InProceedings{Barr+2020,
  title = 	 {Quantum Ground States from Reinforcement Learning},
  author =       {Barr, Ariel and Gispen, Willem and Lamacraft, Austen},
  booktitle = 	 {Proceedings of The First Mathematical and Scientific Machine Learning Conference},
  pages = 	 {635--653},
  year = 	 {2020},
  editor = 	 {Lu, Jianfeng and Ward, Rachel},
  volume = 	 {107},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v107/barr20a/barr20a.pdf},
  url = 	 {https://proceedings.mlr.press/v107/barr20a.html},
  abstract = 	 {  Finding the ground state of a quantum mechanical system can be formulated as an optimal control problem. In this formulation, the drift of the optimally controlled process is chosen to match the distribution of paths in the Feynman–Kac (FK) representation of the solution of the imaginary time Schrödinger equation. This provides a variational principle that can be used for reinforcement learning of a neural representation of the drift. Our approach is a drop-in replacement for path integral Monte Carlo, learning an optimal importance sampler for the FK trajectories. We demonstrate the applicability of our approach to several problems of one-, two-, and many-particle physics.}
}
@inproceedings{Zhang+2021,
title={Sampling via Controlled Stochastic Dynamical Systems},
author={Benjamin Zhang and Tuhin Sahai and Youssef Marzouk},
booktitle={I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop},
year={2021},
url={https://openreview.net/forum?id=dHruzYDH719}
}
@article{DeBortoli2022,
title={Convergence of denoising diffusion models under the manifold hypothesis},
author={Valentin De Bortoli},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=MhK5aXo3gB},
note={Expert Certification}
}