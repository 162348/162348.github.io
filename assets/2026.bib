@article{Murphy-Winkler1984,
author = {Allan H. Murphy and Robert L. Winkler},
title = {Probability Forecasting in Meteorology},
journal = {Journal of the American Statistical Association},
volume = {79},
number = {387},
pages = {489--500},
year = {1984},
publisher = {Taylor \& Francis},
doi = {10.1080/01621459.1984.10478075},


URL = { 
    
        https://doi.org/10.1080/01621459.1984.10478075
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01621459.1984.10478075
    
    

}

}
@article{Waghmare-Ziegel2025,
   author = "Waghmare, Kartik and Ziegel, Johanna",
   title = "Proper Scoring Rules for Estimation and Forecast Evaluation",
   journal = "Annual Review of Statistics and Its Application",
   issn = "2326-8298",
   year = "2025",
   publisher = "Annual Reviews",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-042424-050626",
   doi = "https://doi.org/10.1146/annurev-statistics-042424-050626",
   abstract = "Proper scoring rules have been a subject of growing interest in recent years, not only as tools for evaluation of probabilistic forecasts but also as methods for estimating probability distributions. In this article, we review the mathematical foundations of proper scoring rules, including general characterization results and important families of scoring rules. We discuss their role in statistics and machine learning for estimation and forecast evaluation. Furthermore, we comment on interesting developments of their usage in applications."
}
@article{Brier1950,
      author = "Glenn W. Brier",
      title = "Verification of Forecasts Expressed in Terms of Probability",
      journal = "Monthly Weather Review",
      year = "1950",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "78",
      number = "1",
      doi = "10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2",
      pages=      "1 - 3",
      url = "https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml"
}
@article{McCarthy1956,
author = {John McCarthy},
title = {Measures of the Value of Information},
journal = {Proceedings of the National Academy of Sciences},
volume = {42},
number = {9},
pages = {654-655},
year = {1956},
doi = {10.1073/pnas.42.9.654},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.42.9.654},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.42.9.654}}
@article{Savage1971,
author = {Leonard J. Savage},
title = {Elicitation of Personal Probabilities and Expectations},
journal = {Journal of the American Statistical Association},
volume = {66},
number = {336},
pages = {783--801},
year = {1971},
publisher = {Taylor \& Francis},
doi = {10.1080/01621459.1971.10482346},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482346
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1971.10482346
    

}

}

@article{Good1952,
author = {Good, I. J.},
title = {Rational Decisions},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {14},
number = {1},
pages = {107-114},
doi = {https://doi.org/10.1111/j.2517-6161.1952.tb00104.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1952.tb00104.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1952.tb00104.x},
abstract = {Summary This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of imformation being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
year = {1952}
}
@article{Gneiting-Raftery2007,
author = {Tilmann Gneiting and Adrian E Raftery},
title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
journal = {Journal of the American Statistical Association},
volume = {102},
number = {477},
pages = {359--378},
year = {2007},
publisher = {Taylor \& Francis},
doi = {10.1198/016214506000001437},


URL = { 
    
        https://doi.org/10.1198/016214506000001437
    
    

},
eprint = { 
    
        https://doi.org/10.1198/016214506000001437
    
    

}

}
@article{Winkler-Murphy1968,
      author = "Robert L.  Winkler and Allan H.  Murphy",
      title = "“Good” Probability Assessors",
      journal = "Journal of Applied Meteorology and Climatology",
      year = "1968",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "7",
      number = "5",
      doi = "10.1175/1520-0450(1968)007<0751:PA>2.0.CO;2",
      pages=      "751 - 758",
      url = "https://journals.ametsoc.org/view/journals/apme/7/5/1520-0450_1968_007_0751_pa_2_0_co_2.xml"
}
 @InProceedings{Lin+2017,
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
title = {Focal Loss for Dense Object Detection},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}
@article{Dawid+2016,
author = {Dawid, A. Philip and Musio, Monica and Ventura, Laura},
title = {Minimum Scoring Rule Inference},
journal = {Scandinavian Journal of Statistics},
volume = {43},
number = {1},
pages = {123-138},
keywords = {B-robustness, Bregman estimate, composite score, Godambe information, M-estimator, pseudo-likelihood, Tsallis score, unbiased estimating equation},
doi = {https://doi.org/10.1111/sjos.12168},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12168},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/sjos.12168},
abstract = {Abstract Proper scoring rules are devices for encouraging honest assessment of probability distributions. Just like log-likelihood, which is a special case, a proper scoring rule can be applied to supply an unbiased estimating equation for any statistical model, and the theory of such equations can be applied to understand the properties of the associated estimator. In this paper, we discuss some novel applications of scoring rules to parametric inference. In particular, we focus on scoring rule test statistics, and we propose suitable adjustments to allow reference to the usual asymptotic chi-squared distribution. We further explore robustness and interval estimation properties, by both theory and simulations.},
year = {2016}
}
@inproceedings{Li+2020,
 author = {Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21002--21012},
 publisher = {Curran Associates, Inc.},
 title = {Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection},
 url = {https://proceedings.neurips.cc/paper/2020/hash/f0bda020d2470f2e74990a07a607ebd9-Abstract.html},
 volume = {33},
 year = {2020}
}
@InProceedings{Charoenphakdee+2021,
    author    = {Charoenphakdee, Nontawat and Vongkulbhisal, Jayakorn and Chairatanakul, Nuttapong and Sugiyama, Masashi},
    title     = {On Focal Loss for Class-Posterior Probability Estimation: A Theoretical Perspective},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {5202-5211},
    url = {https://arxiv.org/abs/2008.13367},
}
@InProceedings{Bao-Charoenphakdee2025,
  title = 	 {Calm Composite Losses: Being Improper Yet Proper Composite},
  author =       {Bao, Han and Charoenphakdee, Nontawat},
  booktitle = 	 {Proceedings of The 28th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2800--2808},
  year = 	 {2025},
  editor = 	 {Li, Yingzhen and Mandt, Stephan and Agrawal, Shipra and Khan, Emtiyaz},
  volume = 	 {258},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--05 May},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v258/main/assets/bao25b/bao25b.pdf},
  url = 	 {https://proceedings.mlr.press/v258/bao25b.html},
  abstract = 	 {Strict proper losses are fundamental loss functions inducing classifiers capable of estimating class probabilities. While practitioners have devised many loss functions, their properness is often unverified. In this paper, we identify several losses as improper, calling into question the validity of class probability estimates derived from their simplex-projected outputs. Nevertheless, we show that these losses are strictly proper composite with appropriate link functions, allowing predictions to be mapped into true class probabilities. We invent the calmness condition, which we prove suffices to identify that a loss has a strictly proper composite representation, and provide the general form of the inverse link. To further understand proper composite losses, we explore proper composite losses through the framework of property elicitation, revealing a connection between inverse link functions and Bregman projections. Numerical simulations are provided to demonstrate the behavior of proper composite losses and the effectiveness of the inverse link function.}
}
@InProceedings{Zhang+2021-Focal,
    author    = {Zhang, Haoyang and Wang, Ying and Dayoub, Feras and Sunderhauf, Niko},
    title     = {VarifocalNet: An IoU-Aware Dense Object Detector},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8514-8523},
    url = {https://arxiv.org/abs/2008.13367},
}
@misc{George+2025,
      title={Denoising Score Matching with Random Features: Insights on Diffusion Models from Precise Learning Curves}, 
      author={Anand Jerry George and Rodrigo Veiga and Nicolas Macris},
      year={2025},
      eprint={2502.00336},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.00336}, 
}
@article{Peche2019,
author = {S. P{\'e}ch{\'e}},
title = {{A note on the Pennington-Worah distribution}},
volume = {24},
journal = {Electronic Communications in Probability},
number = {none},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1 -- 7},
keywords = {machine learning, random matrices},
year = {2019},
doi = {10.1214/19-ECP262},
URL = {https://doi.org/10.1214/19-ECP262}
}
@inproceedings{Bonnaire+2025,
title={Why Diffusion Models Don{\textquoteright}t Memorize:  The Role of Implicit Dynamical Regularization in Training},
author={Tony Bonnaire and Rapha{\"e}l Urfin and Giulio Biroli and Marc Mezard},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=BSZqpqgqM0}
}
@book{Anderson+2011,
    author = {Greg W. Anderson and Alice Guionnet and Ofer Zeitouni},
    year = {2011},
    title = {An Introduction to Random Matrices},
    series = {Cambridge Studies in Advanced Mathematics},
    volume = {118},
    edition = {},
    url = {https://doi.org/10.1017/CBO9780511801334},
    publisher = {Cambridge University Press}
}
@book{Magnus-Neudecker2019,
    author = {Jan R. Magnus and Heinz Neudecker},
    year = {2019},
    title = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
    series = {Wiley Series in Probability and Statistics},
    volume = {},
    edition = {3},
    url = {https://doi.org/10.1002/9781119541219},
    doi = {10.1002/9781119541219},
    publisher = {John Wiley \& Sons}
}
@article{Marčenko-Pastur1967,
doi = {10.1070/SM1967v001n04ABEH001994},
url = {https://doi.org/10.1070/SM1967v001n04ABEH001994},
year = {1967},
month = {apr},
publisher = {},
volume = {1},
number = {4},
pages = {457},
author = {V A Marčenko and L A Pastur},
title = {DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES},
journal = {Mathematics of the USSR-Sbornik},
abstract = {}
}

@article{Biroli+2024,
	abstract = {We study generative diffusion models in the regime where both the data dimension and the sample size are large, and the score function is trained optimally. Using statistical physics methods, we identify three distinct dynamical regimes during the generative diffusion process. The generative dynamics, starting from pure noise, first encounters a speciation transition, where the broad structure of the data emerges, akin to symmetry breaking in phase transitions. This is followed by a collapse phase, where the dynamics is attracted to a specific training point through a mechanism similar to condensation in a glass phase. The speciation time can be obtained from a spectral analysis of the data's correlation matrix, while the collapse time relates to an excess entropy measure, and reveals the existence of a curse of dimensionality for diffusion models. These theoretical findings are supported by analytical solutions for Gaussian mixtures and confirmed by numerical experiments on real datasets.},
	author = {Biroli, Giulio and Bonnaire, Tony and de Bortoli, Valentin and M{\'e}zard, Marc},
	date = {2024/11/17},
	date-added = {2026-01-25 17:56:53 +0900},
	date-modified = {2026-01-25 17:56:53 +0900},
	doi = {10.1038/s41467-024-54281-3},
	id = {Biroli2024},
	isbn = {2041-1723},
	journal = {Nature Communications},
	number = {1},
	pages = {9957},
	title = {Dynamical regimes of diffusion models},
	url = {https://doi.org/10.1038/s41467-024-54281-3},
	volume = {15},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41467-024-54281-3}}
@InProceedings{Rahaman+2019,
  title = 	 {On the Spectral Bias of Neural Networks},
  author =       {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5301--5310},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rahaman19a.html},
  abstract = 	 {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.}
}
