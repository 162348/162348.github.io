---
title: "Gauss 過程を用いた統計解析２"
subtitle: 理論編
author: "司馬博文"
date: 2/11/2024
date-modified: 8/8/2024
image: Images/Gibbs.svg
categories: [Bayesian, Kernel, Process]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: Gauss 過程は関数に対するノンパラメトリックモデルである．正確には，関数空間上の共役確率分布を定めるため，Gauss 過程を用いて回帰関数に関する効率的な Bayes 推論が可能になる．ニューラルネットワークも，例えば１層で全結合のものは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価になる．
---

{{< include ../../../_preamble.qmd >}}

[![実践編も参照（画像をタップでリンクを開く）](../../../docs/posts/2024/Kernels/GP_files/figure-html/cell-10-output-1.png)](GP.qmd)

## 導入

> ガウス過程はベイズ統計の立場から見たカーネル法ということができます．[@持橋-大羽2019]

### Gauss 過程とは

Gauss 過程 $\{X_t\}_{t\in T}\subset\L(\Om;\R^d)$ とは，各変数 $X_t$ が正規分布を持つ確率過程である．転置
$$
X_-:\om\mapsto\Map(T;\R^d)
$$
を考えれば，これはランダムに定まる添え字集合 $T$ 上の関数と見れる．

それゆえ，Gauss 過程は，関数空間 $\Map(T;\R^d)$ 上の確率分布を定める際に使うことが出来る．

### クリギング (Kriging)

Gauss 過程回帰の歴史的端緒の１つは，空間統計学における [**クリギング**](https://ja.wikipedia.org/wiki/クリギング) [@Krige1951] である．

これは空間上の各地で取られているデータから，データのない地点におけるデータを内挿する方法である．（[@Krige1951] は特に鉱山評価において用いている）．

現代的な言葉で言えば，これは $T=\R^2$ 上での Gauss 過程回帰を用いた統計推測である．

### Gauss 過程回帰

尤度も Gauss である場合，事後分布も Gauss 過程になるため，正確な計算が可能である．

一方で一般の尤度の場合は，変分推論 [@Wilkinson+2023] が行われる．

タスクが分類である場合は，Pólya-Gamma 分布に基づくデータ拡張に基づく変分推論 [@Wenzel+2019], [@Galy-Fajou2020] も用いられる．

### ニューラルネットワークとの対応

独立同分布な事前分布の下で，１層の全結合ニューラルネットワークは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価である [@Neal1996]．

したがって，[@Williams1996] などの方法で対応する Gauss 過程が特定できれば，当該のニューラルネットワークと等価な Bayes 推論が可能になる．

Gauss 過程との同様の対応は，多層のニューラルネットワークの間にもつけられている [@Lee+2018]．この際の Gauss 過程のカーネルは NN-GP 核と呼ばれており，CNN [@Novak+2019] や一般化 (Tensor Program [@Yang2020]) など，各方面で特徴づけが進んでいる．

しかしニューラルネットワークは隠れ層において適応的に適切な基底関数を学習できる一方で，Gauss 過程回帰は実行する前に適切な正定値カーネルを選ぶ必要がある．

## 正定値カーネル

Gauss 過程は，平均関数と共分散関数＝正定値カーネルを定めるごとに定まる．

Gauss 過程回帰を実行するには適切な事前 Gauss 過程を選ぶ必要があり，そのためには適切な正定値カーネルを選ぶ必要がある．

### 定常カーネル

分布が時間変化に対して不変である場合，Gauss 過程の共分散関数
$$
\C(s,t):=\E\SQuare{(X_s-\E[X_s])(X_t-\E[X_t])}
$$
は $\abs{s-t}$ のみの関数である．

#### 動径基底関数核 {#sec-rbf-kernel}

::: {.callout-tip title="定義 (Radial Basis Function kernel / Squared Exponential kernel)^[前者は [@持橋-大羽2019 p.68]，後者は [@Rasmussen-Williams2006 p.14] の用語．[@Murphy2023] では両方が併記されている．Gaussian kernel とも呼ばれる．]" icon="false"}
動径基底関数カーネルとは，
$$
K(r;\ell):=\exp\paren{-\frac{r^2}{2\ell^2}}
$$
で定まるカーネルである．`GPy` での実装は[こちら](https://gpy.readthedocs.io/en/deploy/GPy.kern.src.html#GPy.kern.src.rbf.RBF)．
:::

#### 関連度自動決定核 (ARD: Autonatic Relevance Determination) {#sec-ARD-kernel}

::: {.callout-tip title="定義 (Autonatic Relevance Determination)^[[@MacKay1994], [@Neal1996 p.16] なども参照．]" icon="false"}
動径基底関数カーネルの Euclid ノルムを Mahalanobis ノルムに変更したもの
$$
K(r;\Sigma,\sigma^2)=\sigma^2\exp\paren{-\frac{r^\top\Sigma^{-1}r}{2}}
$$
を関連度自動決定カーネルという．
:::

そもそも関連度自動決定 [@MacKay1994], [@Neal1996 p.16] またはスパースベイズ学習 [@Tipping2001] とは，ニューラルネットワークの最初のレイヤーの荷重をスパースにするために分散不定の正規分布を事前分布として導入する，という技法である [@Loeliger+2016]．

一般に出力をスパースにするためのフレームワークとしても活用され，ARD 核はその最たる例である．

#### Matérn 核

ARD 核は軟化性能を持つため，見本道は無限回微分可能になってしまう．

これが不適な状況下では，Matérn 核
$$
K(r;\nu,\ell)=\frac{2^{1-\nu}}{\Gamma(\nu)}\paren{\frac{\sqrt{2\nu}r}{\ell}}^\nu K_\nu\paren{\frac{\sqrt{2\nu}r}{\ell}}
$$
などが用いられることがある．ただし，$K_\nu$ は修正 Bessel 関数とする．

$\nu$ は滑らか度を決定し，見本道は $\floor{\nu}$ 階微分可能になる．$\nu\to\infty$ の極限で RBF 核に収束する．

$\nu=1/2$ の場合が OU 過程に当たる．

#### スペクトル核

任意の（定常な）正定値関数は，ある関数 $p$ に関して
$$
K(r)=\int_{\R^d}p(\om)e^{i\om^\top r}\,d\om
$$ {#eq-spectral-decomposition}
と表せる．この $p$ は **スペクトル密度** という．

$K$ が RBF 核であるとき，$p$ もそうなる：
$$
p(\om)=\sqrt{2\pi\ell^2}\exp\Paren{-2\pi^2\om^2\ell^2}.
$$

この対応を用いて，スペクトル密度 $p$ をデザインすることで，様々な正定値カーネルを得ることが出来る．

例えば spectral mixture kernel [@Wilson-Adams2013] では，スケール母数と位置母数とについて RBF 核の混合を考えることで，新たな正定値カーネルを構成する．

### 非定常カーネル

環境統計学などにおいて，空間相関の仕方が時間的に変化していくという設定がよくある．

このような時間方向での法則変化を取り入れるものが非定常カーネルである．

#### 多項式核

$$
K(x,y)=(x^\top y+c)^M
$$
は非斉次項 $c$ を持つ，$M$ 次の多項式核と呼ばれる．

#### Gibbs 核

Gibbs 核 [@Gibbs1997] は，ハイパーパラメータ $\sigma,\ell$ を入力に依存するようにした RBF 核である：
$$
K(x,y)=\sigma(x)\sigma(y)\sqrt{\frac{2\ell(x)\ell(y)}{\ell(x)^2+\ell(y)^2}}\exp\paren{-\frac{\abs{x-y}^2}{\ell(x)^2+\ell(y)^2}}.
$$

このようにすることで，$\sigma,\ell$ を別の Gauss 過程でモデリングし，階層モデルを考えることもできる [@Heinonen+2016]．

#### スペクトル核 [@Remes+2017]

正定値核は Fourier 変換を通じて，スペクトル密度によって指定することもできる（Bochner の定理）．

この手法は，非定常核に対しても [@Remes+2017] が拡張している．

### 位相空間上の正定値核

#### 離散構造上の核

文章上の string kernel [@Lodhi+2002] やグラフ上の graph kernel [@Kriege+2020] も考えられている．

[@Borgwardt+2006] は random walk kernel を提案しており，$\R^d$ へ埋め込まれるようなものの計算量は $O(n^3d)$ である．さらに効率の良いカーネルとして Weisfeiler-Lehman カーネル [@Shervashidze+2011] もある．

#### $S^1$ 値の核

$S^1\simeq\cointerval{0,\2pi}$ 上の確率分布は，方向データとして，海洋学における波の方向，気象学における風向のモデリングに応用を持つ．

全射 $\pi:\R\epi S^1$ に従って，$\R$-値の Gauss 過程を，方向データ値の Gauss 過程に押し出すことが出来る [@Jona-Lasinio+2012]．これに伴い，$\R$-値の核を $S^1$-値に押し出すこともできる．

$\pi$ による Gauss 分布の押し出し $\pi_*\rN_1(\mu,\sigma^2)$ は [wrapped normal distribution](https://en.wikipedia.org/wiki/Wrapped_normal_distribution) と呼ばれている．これに対応し，この Gauss 過程は wrapped Gaussian process と呼ばれている [@Jona-Lasinio+2012]．

### 核の Monte Carlo 近似

#### カーネルの近似

以上，種々のカーネル関数を紹介してきたが，これらはデータに関して効率的に計算される必要がある．

特に潜在空間上での Gram 行列の逆行列または Cholesky 分解を計算する $O(n^3)$ の複雑性が難点である [@Liu+2020]．

このデータ数 $n$ に関してスケールしない点が従来カーネル法の難点とされてきたが，これはランダムなカーネル関数を用いた Monte Carlo 近似によって高速化できる．$m$ 個のランダムに選択された基底関数を用いれば，Monte Carlo 誤差を許して計算量は $O(nm+m^3)$ にまで圧縮できる．

#### Random Fourier Features

正定値核のスペクトル表現 ([-@eq-spectral-decomposition]) を通じて，核の値 $K(x,y)$ を Monte Carlo 近似をすることが出来る．

例えば $K$ が RBF 核であるとき，$p$ は正規密度になるから，Gauss 確率変数からのサンプリングを通じてこれを実現できる：
$$
K(x,y)\approx\phi(x)^\top\phi(y),\qquad \phi(x):=\sqrt{\frac{1}{D}}\vctr{\sin(Z^\top x)}{\cos(Z^\top x)},Z=(z_{ij}),z_{ij}\iidsim\rN(0,\sigma^{-2}).
$$

これは核の値 $K(x,y)$ を，逆に（ランダムに定まる）特徴ベクトル $\phi(x),\phi(y)$ の値を通じて計算しているため，Random Fourier Features [@Rahimi-Recht2007], [@Sutherland-Schneider2015]，または Random Kitchen Sinks [@Rahimi-Recht2008] と呼ばれる．

$Z$ の行を互いに直交するように取ることで，Monte Carlo 推定の精度が上がる．これを orthogonal random features [@Yu+2016] と呼ぶ．

## 推論法

### Expectation Propagation 法

> EP might not converge in some cases since quadrature is used. [`GPML` 4.2 Documentation](http://gaussianprocess.org/gpml/code/matlab/doc/)

### FITC (Fully Independent Training Conditional) [@Snelson-Ghahramani2005]

複数のデータを１つのデータに要約し，有効的なデータ数を減らす方法を inducing point または pseudo-input と呼ぶ．

### MCMC

MCMC は唯一ブラックボックスとして用いることが出来ない推論手法である．また，勾配ベースの周辺尤度最適化も MCMC では不可能である．

> Inference by MCMC sampling is the only inference method that cannot be used as a black box. Also gradient-based marginal likelihood optimisation is not possible with MCMC. Please see usageSampling for a toy example illustrating the usage of the implemented samplers. [`GPML` 4.2 Documentation](http://gaussianprocess.org/gpml/code/matlab/doc/)

この関門が乗り越えられたならば，Gauss 過程による機械学習の応用は大きく進展するだろう．

$S^1$-値の Gauss 過程は，データ拡張に基づく MCMC により推論できる [@Jona-Lasinio+2012]．

また，関数空間からの事後分布サンプリングを高速化することも考えられている [@Wilson+2020]．

### Kalman フィルター

事前分布として設定した Gauss 過程を，線型 Gauss な状態空間モデルとして解釈することで，Gauss 過程回帰を Kalman フィルタリングによって解く [@Hartikainen-Sarkka2010] ことも考えられている．

この方法は特に，時系列データに対して，Gauss 過程回帰を通じて外挿をするタスクにおいて考えられている [@Sarkka+2013], [@Adam+2020]．

## カーネルの学習

Gauss 過程を用いて，ニューラルネットワークのような外挿汎化性を獲得するためには，適切な正定値カーネルをデータから適応的に学習できるようになる必要がある [@Wilson-Adams2013], [@Wilson+2014]．

実はこのカーネル学習はニューラルネットワークの学習と類似していく．実際，NN と GP が相補的な役割を果たすことが最も良い実践を生むかもしれない．

### Deep Kernel Learning (DKL) [@Wilson+2016] {#sec-DKL}

$$
K(x,y)=\exp\paren{-\frac{1}{2\sigma^2}\abs{h_\theta(x)-h_\theta(y)}^2}
$$
というように，深層学習と Gauss 過程回帰を組み合わせる発想は [@Salakhutdinov-Hinton2007] から存在した．

この方法は，ニューラルネットワークの最後の一層を Gauss 過程に取り替えることに相当する．

しかし [@Ober+2021] によると，通常のニューラルネット以上に深刻な過適応を見せやすく，完全にベイズによるアプローチを採る方法を議論している．

### Deep Gaussian Process (DGS) [@Damianou-Lawrence2013]

GP をスタックして深層な階層モデルにするという発想であるが，学習が極めて困難になる．

### Neural Tangent Kernel (NTK) [@Jacot+2018]

ある一定の条件の下では，訓練中／訓練後の DNN が，学習率 $\eta\to0$，幅無限大の極限でどのようなカーネルを持った GP に対応するかを導くことができる．

しかし，NTK の仮定は特に「初期値からほとんどパラメータ値は変化しない (lazy training) [@Chizat+2019]」というものも含意しており，これが実際と乖離している [@Woodworth+2020]．

実際，幅無限大の極限で，元々の NTK の理論では特徴学習をしないということが示せてしまう．しかし，これはパラメータの変換を通じて修正することができ，NTK 理論の射程はさらに広がっている [@Yang-Hu2021]．^[[@Yang-Hu2021] は幅無限大極限において特徴学習が起こるための十分条件も与えている．]

## 文献紹介 {.appendix}

[@Gortler+2019] というサイトに注目．

[@Liu+2020] は GP のスケーラビリティに関するサーベイである．