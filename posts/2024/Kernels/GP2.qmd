---
title: "Gauss 過程を用いた統計解析２"
subtitle: 理論編
author: "司馬 博文"
date: 2/11/2024
categories: [Kernel, Math Notes]
toc: true
number-sections: true
code-block-bg: true
code-fold: false
code-annotations: select
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，Gauss 過程を用いた統計解析を紹介する．
---

> ガウス過程はベイズ統計の立場から見たカーネル法ということができます．[@持橋-大羽2019]

{{< include ../../../_preamble.qmd >}}

## 導入

独立同分布な事前分布の下で，１層の全結合ニューラルネットワークは，隠れ素子数が無限になる極限で Gauss 過程回帰と等価である [@Neal1996]．

したがって，[@Williams1996] などの方法で対応する Gauss 過程が特定できれば，当該のニューラルネットワークと等価な Bayes 推論が可能になる．

Gauss 過程との同様の対応は，多層のニューラルネットワークの間にもつけられている [@Lee+2018]．この論文では，ニューラルネットワークの性能を凌駕することが観察されている．

しかし現状でニューラルネットワークが Gauss 過程を越えるのは，推論手法である誤差逆伝播法 [@Rumelhart+1986] においてであろう．

## Gauss 過程回帰の理論

### カーネル

#### 動径基底関数カーネル {#sec-rbf-kernel}

動径基底関数カーネル (Radial Basis Function kernel) [@持橋-大羽2019 p.68] または Squared Exponential kernel [@Rasmussen-Williams2006 p.14] は
$$
k(r):=\sigma^2\exp\paren{-\frac{r^2}{2}}
$$
で定まるカーネルである．`GPy` での実装は[こちら](https://gpy.readthedocs.io/en/deploy/GPy.kern.src.html#GPy.kern.src.rbf.RBF)．

#### 定数カーネル

### 推論手法

#### Expectation Propagation 法

> EP might not converge in some cases since quadrature is used. [`GPML` 4.2 Documentation](http://gaussianprocess.org/gpml/code/matlab/doc/)

#### FITC (Fully Independent Training Conditional) 推論

#### MCMC

MCMC は唯一ブラックボックスとして用いることが出来ない推論手法である．また，勾配ベースの周辺尤度最適化も MCMC では不可能である．

> Inference by MCMC sampling is the only inference method that cannot be used as a black box. Also gradient-based marginal likelihood optimisation is not possible with MCMC. Please see usageSampling for a toy example illustrating the usage of the implemented samplers. [`GPML` 4.2 Documentation](http://gaussianprocess.org/gpml/code/matlab/doc/)

この関門が乗り越えられたならば，Gauss 過程による機械学習の応用は大きく進展するだろう．