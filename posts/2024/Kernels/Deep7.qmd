---
title: "数学者のための深層学習７"
subtitle: 確率的最適化
author: "司馬 博文"
date: 2/16/2024
categories: [Kernel, Math Notes]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 深層学習の学習における確率最適化アルゴリズムに関して概説する．
---

{{< include ../../../_preamble.qmd >}}

非斉次 Markov 過程とも見れる？ [@Robert-Casella2004 p.162]．

## 導入

確率的最適化は，はじめは統計学の文脈で [@Robbins-Monro1951] によってオンラインの最尤推定を題材に考察された．

これを一般化する形で [@Kiefer-Wolfowitz1952] は [**確率的勾配降下法**](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95) (SGD) を提案した．

SGD を拡張し，適応的に学習率を調整する手法としては，Adagrad [@Duchi+2011] や RMSprop [@Tieleman-Hinton2012] が提案された．