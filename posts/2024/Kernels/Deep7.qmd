---
title: "数学者のための深層学習７"
subtitle: エネルギーベースモデル
author: "司馬 博文"
date: 3/30/2024
categories: [Deep]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 確率分布をエネルギーの言葉でモデリングする方法である．
---

{{< include ../../../_preamble.qmd >}}

[@Gao+2020]

エネルギーベースのモデル (EBM: Energy-based Model) とは，
$$
p(z)\propt e^{-H(z)}
$$
の形で与えられるノンパラメトリックモデルで，訓練データ $\{(x_i,y_i)\}_{i=1}^n$ に対して最も低いエネルギーを割り当てるエネルギー関数 $H$ を探す (energy minimization) ことで確率分布 $p(x)$ を推定する手法である [@LeCun+2007]．

この形の分布族を **正準分布** または **Gibbs 分布** [@Koller-Friedman2009 p.108], [@Friedli-Velenik2017 p.25]，または **Boltzmann 分布** [@Kim-Bengio2016], [@Mezard-Montanari2009 p.23], [@Chewi2024] ともいう．^[ということもあるようであるが，物理の用語では $e^{H(z)}$ を Boltzmann 因子と呼ぶのみであるようである [@田崎晴明2008 p.107]．[@Liu2004 p.7] ではどちらも掲載している．正準集団は，NVT 一定集団ともいう．]

この推定を，うまく損失関数を設定することで，最適化手法によって解くことが [@LeCun+2007] では考えられている．データの分布との KL 乖離度を，勾配降下法によって最小化することによって学習することも多いが，この場合 $p$ からのサンプリングを必要とするため，[GAN](Deep3.qmd) にヒントを得た敵対的な学習も考えられている [@Kim-Bengio2016], [@Gao+2020]．

回帰や分類などの古典的なタスクだけでなく，ほとんどの確率的モデルもこの手続きから理解することができ，この場合は EBM が非確率的な／最適化ベースの推論手法を提供するフレームワークとして働くことになる [@LeCun+2007 p.192]．

また EBM は，入力 $x$ と出力 $y$ の整合性 $k(x,y)$ を，エネルギーの言葉で与えているモデルであると見ることもできる．^[分野によっては，エネルギー関数 $H$ を，contrast function, value functions, NLL (Negative Log-Likelihood) functions などとも呼ぶとしている [@LeCun+2007 p.193]．]

