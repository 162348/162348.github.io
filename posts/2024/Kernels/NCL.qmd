---
title: "表現学習とノイズ対照学習"
author: "司馬 博文"
date: 7/29/2024
date-modified: 8/10/2024
categories: [Deep]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 深層潜在モデルを敵対的に学習させる手法である **NCL (Noise-Contrastive Learning)** について述べる．この手法は，表現学習や非線型独立成分分析に応用を持つ．識別性を保った深層潜在モデルを学習しようとする方法は，因果的表現学習とも呼ばれている．
---

{{< include ../../../assets/_preamble.qmd >}}

## 表現学習と非線型独立成分分析

### 表現学習とは何か？

::: {.callout-tip appearance="simple" icon="false"}

1. 教師あり学習による表現学習
2. 生成による表現学習
3. 自己教師あり表現学習
4. ノイズ対照による表現学習
5. 独立成分分析による表現学習

:::

極めて高精度な分類器が完成してすぐのころ，分類タスクが極めて上手なニューラルネットワークは他の下流タスクでも良い成績が観察され，最初に考えられた方法が１であった．一方でこのスキームではすぐにドメインシフトと転移学習が問題になった．

これを克服するのが２の方法である．高精度なデータを生成できる深層潜在模型が学習された場合，その潜在変数は現実の何らかの表象になっているだろう，というアイデアは analysis-by-synthesis [@Roberts1963], [@Lee-Mumford2003] とも呼ばれている．

この方法は，文字のストローク（トメ，ハネ）が集まった構造に注目するなど，データの生成過程がある程度明らかなものでは特に性能が良い [@Lake+2015]．

[$\beta$-VAE](Deep4.qmd#sec-beta-VAE) [@Higgins+2017] や BiGAN [@Donahue+2017] はその例であるが，ImageNet などの大規模データに対する分類や分割のタスクで十分な性能はまだ見られていない．

### 生成から雑音除去へ

２よりも高い性能を誇るのが３である．生成のためには大変多くの特徴量が必要であるが，下流タスクに重要なのはその一部のみに限る．このような場合，[Denoising AE](Deep4.qmd#sec-denoising-autoencoder) [@Vincent+2008] のように「データにノイズを印加してこれを戻すのに必要な知識は何か？」を問うことが極めて普遍的な力を持つ．

雑音除去と同様に，表現学習に極めて有効なタスクがマスク除去 [@Devlin+2019] である．これは画像領域にも応用されている：BEiT [@Bao+2022]．**masked autoencoder** [@He+2022] が現在の state of the art である．

### ノイズ対照学習による表現学習

ノイズ対照学習に基づいた方法が第４勢力として登場してきている．例えば CPC (Contrastive Predictive Coding) [@Oord+2019] は，言語，音声，画像，３次元空間での強化学習など，多くの領域で有力な代替を提供するようである．

この方法では，雑音やマスク除去とは違った方法で，予測問題として表現学習を解く．^[予測符号化 ([predictive coding](https://en.wikipedia.org/wiki/Predictive_coding)) [@Elias1955] は従来からデータ圧縮の原理であると同時に，認知科学において，脳のメンタルモデルとしても有名である [@Rao-Ballard1999]．また word2vec [@Mikolov2013] から始まり，BART や GPT などの言語モデルはこの方法による表現学習を行っている．] その結果雑音・マスク除去と違い，ある程度どのようなデータを「似ている」とするかの制御が効く美点がある．これを **距離学習** ともいう．^[[@Murphy2023 p.1056] 第32.3.4.2節も参照．]

#### CPC [@Oord+2019]

![Contrastive Predictive Coding [@Oord+2019]](Images/contrastive_repr4.jpeg)

まずエンコーダー $z_t=g_{\text{enc}}(x_t)$ を作る．続いて，自己回帰モデル $g_{\text{ar}}$ を用いて $z_{1:t}$ を要約して予測しようとする．

この段階で潜在表現 $c_t=g_{\text{ar}}(z_{1:t})$ が作られることを期待するのであるが，直接 $p(x|c)$ を予測しようとしてしまうと，必ずしも有用な潜在表現 $c$ が得られるとは限らない．

そこで，距離 $k$ だけ離れたデータ $x_{t+k}$ の尤度比
$$
f_k(x_{t+k},c_t)\propt\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}
$$
を，
$$
f_k(x_{t+k},c_t)=\exp\paren{z_{t+k}^\top W_kc_t}
$$
の形で予測しようとし，この荷重 $W_k$ の推定を考える．

これは，表現学習においては予測 $p(x|c)$ が至上命題であるわけではなく，$x$ と $c$ の相互情報量が近ければ十分であるために用意された，表現学習のための代理目標である（**InfoMax** ともいう）．^[相互情報量は$$I(x;c)=\sum p(x,c)\log\frac{p(x|c)}{p(x)}$$ と表される．密度比の推定が成功していれば，相互情報量は殆ど変わらない．]

このモデルに対しては，GAN 様の敵対的生成であるノイズ対照学習の損失を用いることができる．[@Oord+2019] では，エンコーダとして残差接続を持つ strided convolutional layer が，自己回帰モデルとして GRU (Gated Recurrent Unit) [@Cho+2014] という RNN の変種が使われている．

こうして推定された $(z_t,c_t)$ は，$x_{1:t}$ までのヒストリを見た要約が欲しい場合は $c_t$ を，そうでない場合は $z_t$ を，データ $x_t$ の潜在表現として使える．

#### CLIP [@Radford+2019]

ノイズ対照学習に基づくアプローチの美点は，別のモーダリティを持つデータを統合しやすい点にある．

![画像に対する種々のノイズ対照学習法がどのようなノイズと対照させるか [@Murphy2023 p.1055]](Images/CLIP.png)

### 独立成分分析による表現学習

表現学習の１つの目標である **disentangle** とは，要因ごとにデータ内の変動を説明して分離することをいう．これを達成するには，データやモデルに追加の仮定を課すことが必要な場合が多い点が困難さを増している [@Locatello+2020]．

この disentangle は独立成分分析の目標と重なる部分が大きい．

いや，独立成分分析が目指すように，現実に何らかの意味で則した方法でデータの潜在表現を得ることが，表現学習で最も好ましい，あるべき disentanglement と言うべきかもしれない [@Khemakhem+2020]．^[The advantage of the new framework over typical deep latent-variable models used with VAEs is that we actually recover the original latents, thus providing principled disentanglement. [@Khemakhem+2020] Section 6．] これを **因果的表現学習** (causal representation learning) ともいう．^[[@Murphy2023 p.1060] 33.4.1節も参照．]

VAE などの深層生成モデル，ノイズ対照学習，独立成分分析などはいずれも，多層の階層モデルを学習するという点では共通している．

### 深層潜在モデルの識別可能性 {#sec-identifiability}

仮に追加に観測されている変数 $u$ が存在して，事前分布 $p_\theta(z|u)$ は $z$ 上で積の形に分解し，指数型分布族に属するとする．すなわち，潜在変数は $U$ で条件づければ互いに独立であるとする．この仮定が識別可能性の鍵となる [@Hyvarinen+2019]．

$u$ はタイムスタンプや前時点での観測，信頼できないラベルなどがありえる [@Hyvarinen-Morioka2016]．

観測 $X$ と潜在変数 $Z$ に対して，$\theta=(f,T,\lambda)$ をパラメータとして
$$
p_\theta(x,z|u)=p_f(x|z)p_{T,\lambda}(z|u),
$$
$$
X=f(Z)+\ep,\qquad \ep\sim p_\ep(\ep).
$$
という形のモデルは，$p_{T,\lambda}$ が十分統計量 $T$ とパラメータ $\lambda$ を持つ指数型分布族である限り，いくつかの正則性条件を満たせば識別可能になる：

::: {.callout-tip title="[@Khemakhem+2020 定理１]" icon="false"}

次の４条件が成り立つ場合，パラメータ $\theta$ は，ある線型変換 $A$ に対して
$$
T\circ f^{-1}=A\circ \wt{T}\circ\wt{f}^{-1}+c
$$
の違いを除いて識別可能である：

1. $p_\ep$ の特性関数は殆ど至る所零にならない．
2. $f$ は単射である．
3. 十分統計量 $\{T_{i,j}\}_{i\in[n],j\in[k]}$ は殆ど至る所可微分で，任意の測度正集合上に線型独立な関数を定める．
4. ある点 $u^0,\cdots,u^{nk}$ が存在して，行列 $(\lambda(u^1)\;\cdots\;\lambda(u^{nk}))-(\lambda(u^0)\;\cdots\;\lambda(u^0))$ は可逆：

:::

加えて，モデルが真の分布を含む場合，変分下界の最大化は上述の線型変換 $A$ の違いを除いて $\theta$ の一致推定に成功する．

### 非線型独立成分分析

非線型独立成分分析は，ある独立な成分からなる潜在変数
$$
p(z)=\prod_{i=1}^dp_i(z_i)
$$
に対して，観測がこの非線型変換 $x=f(z)$ であると仮定し，データ生成過程を特定しようとする営みである．

これは上のモデルの $\ep=0$ とした場合に他ならない．

つまるところ，従来からの深層生成モデリングのうち，統計的に特別な意味を持つものが非線型独立成分分析と捉えることもできるはずである．すなわち，生成モデルと非線型独立成分分析は，モデルの骨子自体は共通で，その適用目的が違うに過ぎない（[この稿](../Samplers/Sampling.qmd#sec-sampling-as-synthesis) も参照）．

ただし，統計モデルと見る以上は識別可能性が肝要である．しかし近年の ICA は，識別可能性を緩めた形 [-@sec-identifiability] で得ることに成功しており，これにより深層生成モデルとの同一視が進むことになる [@Hyvarinen+2019], [@Khemakhem+2020]．

これにより，VAE などの深層生成モデルをより統計的に意味のあるものとすることができる．上述の定理により識別可能性を確保した VAE を iVAE (identifiable VAE) [@Khemakhem+2020] と呼ぶ．

また逆の方向には，非線型 ICA モデルを変分ベイズや確率的勾配降下法により推定することができる．

### VAE の識別可能性

VAE の事前分布が特定の混合分布の形を持つならば，補助変数 $u$ が存在しない場合でも，VAE は識別可能な因果グラフを与えるという [@Kivva+2021], [@Kivva+2022], [@Lopez+2024]．

## 参考文献 {.appendix}

[@Hyvarinen-Morioka2016], [@Hyvarinen-Morioka2017], [@Hyvarinen+2019] は深層潜在モデルが識別可能になるための条件を示した非線型 ICA の論文である．

