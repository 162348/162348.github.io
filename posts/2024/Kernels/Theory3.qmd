---
title: "統計的学習理論３"
subtitle: "構造的リスク最小化"
author: "司馬 博文"
date: 3/2/2024
categories: [Kernel, Math Notes]
toc: true
image: Theory.png
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: 統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
---

{{< include ../../../_preamble.qmd >}}

## 正則化と汎化の関係

### 導入

定理 @thm-1 の証明からも判る通り，推定誤差と近似誤差のトレードオフが存在する．

これを踏まえて，機械学習モデルの根本的な設計思想として [構造リスク最小化](https://en.wikipedia.org/wiki/Structural_risk_minimization) が提案された [@Vapnik-Chervonenkis1974]．これは近似誤差をある一定以下に抑え，その中で仮設空間の複雑さをなるべく落とす，という設計方針である．

一方で，仮設集合 $\cH$ を小さくする代わりに，アルゴリズム $A:(\cX\times\cY)^n\to\cH$ が探索する範囲を小さいものにし，実質的な仮設空間のサイズを抑えることも考えられる．これを **正則化** という．

この方向の研究の源流は，[@Bousquet-Elisseeff2002] らの **安定性** の理論であった．

これは「実質的な仮設空間」という考え方を導入することで，@sec-1 の機械学習モデルの予測精度理論の精緻化も生んだ．

### 枠組み：アルゴリズムに目を向ける

リスクを
$$
R(A,S):=\E[l(A(S)(X),YT)]
$$
経験リスクと
$$
\wh{R}(A,S_n):=\frac{1}{n}\sum_{i=1}^nl(A(S_n)(x_i),y_i)
$$
として，アルゴリズム $A:(\cX\times\cY)^n\to\cH$ の関数とみる．^[[@Bousquet-Elisseeff2002 p.502]．]

この場合，仮設空間 $\cH$ 上の一様な評価は，そもそも目指さない．

### 安定性

::: {.callout-tip icon="false"}
## 
::: {#def-stability}
## 安定性^[[@Bousquet-Elisseeff2002 p.503]．]

アルゴリズム $A:(\cX\times\cY)^n\to\cH$ が，損失関数 $l$ に関して **$\beta\in(0,1)$-安定** であるとは，任意の $S\subset(\cX\times\cY)^n$ に対して，
$$
\begin{align*}
    &\max_{i\in[n]}\E\SQuare{\ABs{l(A(S)(x_i),y_i)\\
    &\qquad-l(A(S\setminus\{z_i\})(x_i),y_i)}}\le\beta
\end{align*}
$$
が成り立つことをいう．
:::
:::

すなわち，学習データを１つ減らしたときの損失の変化が，ある一定以下であることをいう．

これは感度分析的な考え方であるが，実は正則化により，アルゴリズムは安定的な挙動をするようになり，安定性が汎化誤差の上界を与える！

### 主結果

::: {.callout-tip icon="false"}
##
::: {#thm-2}
## 安定なアルゴリズムに対する汎化バウンド^[[@Bousquet-Elisseeff2002 p.507] Theorem 12．]

$A$ を $\beta_1$-安定で，損失関数 $l$ は上界 $M>0$ を持つとする．このとき，$1-\delta$ の確率で
$$
R(A,S)\le\wh{R}(A,S_n)+2\beta+(4n\beta+M)\sqrt{\frac{\log1/\delta}{2n}}.
$$
:::
:::

### アルゴリズムの安定性

一方で，アルゴリズムの安定性を示すことは難しく，通常 admissibility と Bregman divergence を通じて議論されるようである．^[[@Bousquet-Elisseeff2002] 第５節．]

