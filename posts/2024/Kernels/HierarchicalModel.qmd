---
title: "階層模型再論"
author: "司馬 博文"
date: 8/12/2024
date-modified: 8/12/2024
image: Images/PCA.svg
categories: [Statistics]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．
listing: 
    -   id: kernel-listing
        type: grid
        sort: false
        contents:
            - "NCL.qmd"
            - "Manifold.qmd"
            - "../Samplers/Sampling.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
# shift-heading-level-by: -1
---

### 関連ページ {.unnumbered .unlisted}

::: {#kernel-listing}
:::


{{< include ../../../assets/_preamble.qmd >}}

## はじめに

潜在変数模型とはどうやらとんでもなく広い射程を持った対象であるようである．

::: {.callout-tip appearance="simple" icon="false" title="潜在変数モデルとは……"}

1. 心理学，経済学をはじめとして多くの分野で中心的に扱われてきたモデルである（[構造方程式モデル](https://ja.wikipedia.org/wiki/共分散構造分析)，因子分析，同時方程式モデル，[Probabilistic Graphical Model](../Computation/PGM1.qmd) など）．
2. ベイズ統計学では **階層モデル** (hierarchical model) として極めて重要な役割を果たす．
3. 生成モデリング（[VAE](Deep4.qmd), [EBM](../Samplers/EBM.qmd), [Diffusion](../Samplers/Diffusion.qmd), [GAN](Deep3.qmd)）も，観測変数上の周辺分布がデータ分布に近づくように潜在変数模型を学習する方法である．
4. 表現学習や独立成分分析だけでなく，脳も潜在変数模型に基いてメンタルモデルを構成しているという仮説もある（[InfoMax に関する稿](NCL.qmd#sec-InfoMax)も参照）．
5. 情報理論において，通信路は潜在変数模型としてモデリングできる．この方向には，潜在変数模型は数学的には確率空間の圏上の図式であるとして研究されている [@Perrone2024]．

:::

このように種々の文脈で登場する潜在変数模型であるが，[**それぞれの文脈において「潜在変数」の果たす役割は全く違う**]{.underline}．

しかし，数学的には全く同じ枠組みで記述できる．従って，そのように扱うことは一定の価値を持つだろう．

本稿では問題とする潜在変数モデルを線型かつ１層に固定し，それぞれの文脈での「使い方の違い」に注目することを目指す．

## 主成分分析 (PCA) {#sec-PCA}

### はじめに

主成分分析では，$p$ 次元のデータ $\{x_i\}_{i=1}^n\subset\R^p$ の各成分を，より少数の潜在変数を持った１層の線型 Gauss 模型

![](Images/PCA.svg)

で説明しようとする．

最も，このような潜在変数モデルとしての見方とベイズ推定による一般化は probabilistic PCA [@Tipping-Bishop1999] / SPCA (Sensible PCA) [@Roweis1997] として初めて自覚された見方である．

$Z^1,\cdots,Z^r$ 上の事前分布がデルタ分布に縮退している場合が古典的な PCA である [@Roweis1997]．

いずれの場合も追加の過程なくしてモデルは識別可能性がなく，後続タスクに応じて種々の制約を追加することで所望の解を得る，という動的な使い方がなされる．

以降，$X\in\L(\Om;\R^p),Z\in\L(\Om;\R^r)$ を確率変数，
$$
\b{X}=(x_i^j)\in M_{n,p}(\R),\b{Z}=(z_i^j)\in M_{n,r}(\R)
$$
を行列として，probabilistic PCA の見方を先に提示し，その特別な場合として種々の特殊化手法を見る．

### 概要

PCA ではデータ行列を
$$
\b{X}:=\vctrr{x_1^\top}{\vdots}{x_n^\top}\in M_{n,p}(\R)
$$
で定めたとき，データ次元 $p$ より小さい数の成分 $r$ で説明しようとする：
$$
\b{X}\approx\b{Z}C^\top,\qquad\b{Z}:=\vctrr{z_1^\top}{\vdots}{z_n^\top}\in M_{n,r}(\R),C\in M_{p,r}(\R).
$$

::: {.callout-tip appearance="simple" icon="false"}

* 古典的には，$z_{ij}$ を（主成分）**得点** (score)，$c_{ij}$ を **負荷量** (loading) ともいう [@足立-山本2024]．
* 機械学習では $Z^1,\cdots,Z^r$ を **潜在因子**，$W\in M_{pr}(\R)$ を **荷重** (weight) ともいう [@Murphy2022]．

:::

この問題は $\b{X}$ の [特異値分解](../FunctionalAnalysis/SVD.qmd) (SVD) $\b{X}=U\Sigma V^\top$ により解ける：
$$
\b{Z}=U\Sigma_{1:r}^\al A=\b{X}(\underbrace{V\Sigma^{\al-1}_{1:r}A}_{=:W}),\qquad C:=V\Sigma_{1:r}^{1-\al}(A^{-1})^\top.
$$
ただし，$\al\in\R,A\in\GL_p(\R)$ は任意である．この解は，特異値分解の性質により，残差を Hilbert-Schmidt ノルムの意味で最小にする：
$$
\min_C\norm{\b{X}-\b{Z}C^\top}_\HS=\min_C\frac{1}{n}\sum_{i=1}^n\abs{x_i-Cz_i}^2=\sigma_{r+1}
$$ {#eq-PCA-objective}
この目的関数は復元誤差とも理解できる．ただし，$\sigma_{r+1}$ は行列 $\b{Z}C$ の第 $r+1$ 特異値である．

### 主成分分散最大化

荷重行列 $W$ が $W^\top W=I_r$ を満たすという制約条件を追加すると，目的関数 ([-@eq-PCA-objective]) は潜在変数の分散を最大にすることと等価になる：
$$
\argmin_{W}\norm{\b{X}-\b{Z}W}_\HS=\argmin_W\Tr((\b{X}W)^\top\b{X}W).
$$ {#eq-PCA-objective2}

すなわち，$\b{Z}=\b{X}W$ の変動が差大になるようにすれば良い．

そのためには，確率変数 $X$ のデータ行列 $\b{X}$ から計算した経験共分散行列 $S\in M_{p}(\R)_+$ の固有ベクトルのうち，対応する固有値が大きいものから $w_1,\cdots,w_r$ として荷重行列とすれば良い：
$$
W:=(w_1\;\cdots\;w_r).
$$

実はこれは解の１つに過ぎず，$W$ に右から直交行列を乗じて「回転」させたものは全て解になる．上の解は追加の条件 $Z^\top Z=I_r$ を課すことで特定される．

### 計算上の注意

各次元に関する長さのスケールを揃えるために，PCA を始める前にデータを正規化しておくか，または共分散行列 $S$ の代わりに，相関行列を用いるべきである．

また，実際に最適化や相関行列の固有値分解をすることはなく，基本的に SVD の方が $O(np^2)+O(p^3)$ と高速である．

さらに次元 $p$ が高い場合は，**確率的 SVD** [@Halko+2011], [@Drineas+2016] を用いてさらに $O(nr^2)+(r^3)$ まで削減できる．このような手法は確率的数値解析と呼ばれる [@Murray+2023]．

### 線型射影による次元縮約 {#sec-dimension-reduction}

$W^\top W=I_r$ の仮定の下で，PCA の目的関数 ([-@eq-PCA-objective]) は，潜在変数の分散最大化 ([-@eq-PCA-objective2]) と見れるのだった．

これは同じ仮定の下で，データ変数 $X$ の最小誤差の線型射影を求める問題とも見れる：
$$
\argmin_W\norm{\b{X}-\b{Z}W}_\HS=\argmin_W\norm{\b{X}-\b{X}WW^\top}_\HS.
$$

なお，一般の行列 $A$ について $P_A=A(A^{-1}A)^+A^\top$ は $\Im A$ 上の直交射影になる．$A$ が直交行列であるとき，$P_A=AA^\top$ が成り立つ．

### 因子分析志向の主成分分析

因子分析では，$Z^1,\cdots,Z^r$ を対等な因子と見て，それぞれのデータへの影響を調べたい．このような場合は，
$$
\frac{1}{n}\b{Z}^\top\b{Z}=I_r
$$
が自然な制約になる．この際の解は，直交行列 $T\in O_r(\R)$ の違いを除いて，
$$
\b{Z}=\sqrt{n}UT,\qquad C=\frac{1}{\sqrt{n}}V\Sigma_{1:r}T,\qquad W=\sqrt{n}V\Sigma_{1:r}^{-1}T,
$$
まで確定する．

しばしば，追加の仮定
$$
C^\top C=\diag(\rho_{1:r}),\qquad \rho_1\ge\cdots\ge\rho_r\ge0
$$
を課して得られる一意な解
$$
\b{Z}=\sqrt{n}U,\qquad C=\frac{1}{\sqrt{n}}V\Sigma_{1:r},\qquad W=\sqrt{n}V\Sigma_{1:r}^{-1},
$$
を **初期解** と呼び，これを「回転」させることで他の解が探索され，所望の分解を探す．

因子分析では [@Thurstone1947] 以来，種々の回転法とアルゴリズムが蓄積している [@足立-山本2024]．一般にこの文脈では，[@Thurstone1947] にいう「単純構造」を達成した，解釈が容易な因子をドメイン知識に基づいて構成することを目指す．この「単純構造」とは，現代でいう一種の disentangled factor と理解できる．

## 因子分析 (FA)

### はじめに

主成分分析が「低階数近似」ならば，因子分析は「高階数近似」というべきである [@足立浩平2023]．

![](Images/FA.svg)

より正確には，因子分析は，観測の各次元 $X^1,\cdots,X^p$ ごとに「独自因子」$Z^1,\cdots,Z^p$ を想定しつつ，全観測に共通する「共通因子」$F^1,\cdots,F^r$ をどのように抽出できるかを考える，という志向性を持つ：

![](Images/FA2.svg)

この意味では，FA は独自因子 $U^1,\cdots,U^p$ を追加した PCA とも理解できる．

歴史的には [@Spearman1904] が古典テスト理論の文脈で $r=1$ の因子分析を，[@Thurstone1947] が一般の $1\le r<p$ の場合の因子分析を「回転」の手法と共に導入した．

さらに興味深いことに，FA では PCA をはじめとした多くの多変量分析手法と違い，[@Lawley1942], [@Anderson-Rubin1956] らにより，初期から確率的な扱いが発展した手法である [@足立-山本2024]．

### 概要

FA では $\b{Z}=(\b{F}\;\b{U})\in M_{n,r+p}(\R)$ の分解に基づき，
$$
\b{X}\approx\b{F}A^\top+\b{U}\Psi^{1/2},\qquad A\in M_{r,p}(\R),\Psi=\diag(\psi_1,\cdots,\psi_p)\in M_p(\R),
$$
によってデータ行列 $\b{X}\in M_{n,p}(\R)$ を説明しようとする．^[[@足立-山本2024] によると，この行列分解による定式化は Henk A. K. Kiers によるもので，初出は同大学からの博士論文 [@Socan2003] が最初ではないか，とのこと．この見方を MDFA (Matrix Decomposition Factor Analysis) と呼ぶ．[@足立浩平+2019] も参照．]

PCA よりさらに識別可能性は絶望的であるが，FA では潜在変数の解釈可能性担保のため，次の仮定を課す：
$$
\b{1}_n^\top\b{F}=\b{0}_r,\qquad \b{1}_n^\top\b{U}=\b{0}_p,
$$
$$
\b{F}^\top\b{F}=n\b{I}_r,\qquad \b{U}^\top\b{U}=n\b{I}_p,\qquad\b{F}^\top\b{U}=O.
$$
すなわち，推定される確率変数 $F,U$ が標準化されていて互いに無相関であるように誘導する．

また，$\b{U}$ の経験分散が $\Psi$ になることに注意．

::: {.callout-tip appearance="simple" icon="false"}

* 古典的には，$f_{ij}$ を共通因子，$\psi_j$ を独自因子の **得点** (score)，$a_{ij}$ を **負荷量** (loading) ともいう [@足立-山本2024]．
* 機械学習では $Z^1,\cdots,Z^r$ を **潜在因子**，$W\in M_{pr}(\R)$ を **荷重** (weight) ともいう [@Murphy2022]．

:::

この問題は，$C:=(A\;\Psi^{1/2})$ と定めると，PCA と同じ問題 ([-@eq-PCA-objective]) に帰着される：
$$
\min_C\norm{\b{X}-\b{Z}C^\top}_\HS.
$$

これはやはり特異値分解により解くことができる [@DeLeeuw04-SimultaneousEstimationOfEFA]．

解は直交行列による回転を除いても，やはり一意に定まらないようである．

### 確率的アプローチ

ここで，
$$
U:=\vctrr{U^1}{\vdots}{U^p}\in\L(\Om;\R^p),\qquad F:=\vctrr{F^1}{\vdots}{F^r}\in\L(\Om;\R^r),
$$
を確率変数とすると，
$$
X\approx AF+\Psi^{1/2}U
$$ {#eq-probabilistic-FA}
によって $X$ に確率モデルが誘導されることになる．

$U,F$ に正規性の仮定をおけば，このモデルは EM アルゴリズムなどを用いて最尤推定できる [@Rubin-Thayer1982]．このような最尤推定のアプローチは [@Lawley1942] から考えられていた．

一方で，$X$ の経験分散 $S$ を，式 ([-@eq-probabilistic-FA]) の右辺の共分散
$$
\Sigma:=AA^\top+\Psi
$$
となるべく近づけるように学習する方法もある．

例えば [@Harman-Jones1966], [@Harman-Fukuda1966] では，Hilbert-Schmidt ノルム $\norm{S-\Sigma}_\HS$ の最小化することで解を探索する方法が考慮された．

このように，データの共分散行列を低階数近似するアプローチは **共分散構造分析** [@Bock-Bargmann1966] ともいう．

さらに，確率論的なアプローチは一般の構造方程式モデル (SEM, 次節 [-@sec-SEM] 参照) へと発展 [@Joreskog1970], [@Sorbom1974], [@Joreskog1978] し，現状，共分散構造分析は SEM の特別な場合と解される．^[ただし，[@星野崇宏+2005] は SEM をより一般的とし，共分散構造分析とは観測変数が連続な場合の下位モデルである，と解している．]

## 構造方程式モデリング (SEM) {#sec-SEM}

### はじめに

[@Joreskog1969] は因子分析モデルを潜在変数モデルとして，事前情報を取り入れるなど柔軟に用いた．特に，仮説検証のための使い方を提案し，自身の手法を **検証的因子分析** (Confirmatory FA) と呼び，それ以前の手法に **探索的因子分析** というレトロニムを与えた．^[[@豊田秀樹1992] では CFA を確認的因子分析と呼んでいる．[@豊田秀樹1991] では，古典テスト理論を確認的因子分析の下位モデルとして紹介している．]

最終的に，共分散構造に基づいた非線型数値最適化手法を推論エンジンとして多くの潜在変数モデルが統一的に推定できる上に，潜在変数モデルをうまく設計することで因果推論・高次の因子分析・分散分析など従来考慮されなかった新たなタスクにも適用可能という見方に到達した [@Joreskog1978], [@Bentler1980]．^[[@狩野裕2002] は SEM の射程と得意・不得意を分析している．]

これを **共分散構造分析** または **構造方程式モデリング** という．^[現代ではコンピュータの力により，新たに「生成」というタスクが加わったと思うと，感慨深い．] 心理学の文脈では，潜在変数のことを **構成概念** (construct) と呼んでいた [@清水和秋1989], [@豊田秀樹1992]．

SEM の名前の下に，行動計量学において，多くの既存の多変量解析法（因子分析，パス解析，二段階抽出モデル，潜在構造分析，項目反応モデルな）はいずれも潜在変数モデルと特殊な形だと解釈できることが自覚された [@McArdle1984], [@Muthen2002]．

このことから，SEM は第二世代の多変量解析 [@Fornell1985] とも評される．^[多変量解析の高級言語とか形容することもあるという．構造方程式モデリングについては，[@豊田秀樹1991], [@狩野裕2002] も参照．]

また，そもそも Jöreskog は因子分析を研究していた時期 [@Joreskog1966] [@Joreskog1967a] から，数値的な解法とコンピュータプログラムの開発にも重点を置いていた．SEM も，コンピュータプログラム LISREL (LInear Structural RELationships) の存在が，広い分野の人口に膾炙した要因として大きい [@清水和秋1989], [@Grimm-Yarnold2016]．

構造方程式モデルがどのように因子分析，因果分析，共分散構造分析を統合し，LISREL プログラムと共に発展していたかが，[@清水和秋1994] に大変わかりやすくまとまっている

### 部分最小自乗モデル (PLS)

PLS (Partial Least Square) モデル [@Joreskog-Wold1982] では，次のような潜在変数モデルを用いて，２つの構成概念間の因果関係を評価しようとする [@豊田秀樹1991]：

![](Images/PLS.svg)

パス図において，潜在変数から観測変数に矢印が伸びている場合，これは影響的指標と呼ばれ，観測のモデルと解され，誤差が入ることが想定される [@豊田秀樹1991]．^[また，パス図では観測変数は四角で囲むべきであるが，ここでは省略した．] 逆の矢印は形成的指標という．

したがって，潜在変数から構成概念への矢印が全て影響的であった場合，これは（探索的）因子分析と等価になる．

これは社会学において **多重指標分析** と呼ばれていたモデルに相当し [@白倉幸男1984] [@清水和秋1989]，経済学において **同時方程式モデル** と呼ばれていたモデルに相当する [@Bentler1980]．このように，SEM の名と LISREL プログラムの下で，多くの社会科学分野で使われていたモデルが，形式的にはほとんど等価であるという了解が形成されていった．

### 正準相関分析

正準相関分析においては，２つの構成概念の間は相関関係で結び，すべての観測は形成的な影響を及ぼすとする（観測誤差は想定しない） [@豊田秀樹1991]：

![](Images/CCA.svg)

## 混合モデル

混合モデルは SEM の別の選択肢としても使える．また，ランダム効果要因を明示的にモデルに組み込む意味で，一般線型モデルの確率論的な拡張と考えることもできる [@狩野裕2002]．^[ただし，SEM は共分散構造，混合モデルは平均構造に分析の焦点がある，という志向の違いもある．[@狩野裕2002] も参照．]

## 終わりに {.appendix}

ここでは，歴史を感じる引用をいくつか紹介したい．

> 心理測定学 (psychometrics) における因子分析，計量経済学 (econometrics) における同時方程式モデル (simultaneous equation models), そして生物測定学 (biometrics) におけるパス解析 (path analysis) を，共分散構造分析の下に統一化することが可能となった契機は，潜在変数 (latent variables) の概念である [@Bentler1980]．[@清水和秋1989]

そして，異分野横断の知見交流が進んだ契機の一つは，LISREL プログラムの存在であった．[@清水和秋1994] では，ETS での安定した研究環境が LISREL の継続的な保守を可能にして最終的には WINDOWS 上でも安定して提供され，これを用いることを通じて異分野を巻き込みながら構造方程式モデリングが発展していった様子が詳細に解説されている．LISREL はバージョン VI まである．

> 紹介した文献からもわかるように，この分野は最近になってやっと日本では注目されてようになってきた。 このように日本へのこの方法論の導入が遅れた理由の一つはソフト流通の問題にあると筆者は考えている。青木 (1988) や土田 (1988) が述べているように， LISREL は大型計算機の場合， アメリカ産のコンビュータでしかサポートしてくれないとのことである。[@清水和秋1989]

そして現代はというと，計算機統計学と機械学習が先行しており，種々の科学への応用とそれぞれ固有の課題への特殊化が遅れている状態だと言うべきではないだろうか？

[ベイズ機械学習](https://puniupa.github.io/posts/2024/AI/BAI.html) や [位相的機械学習](https://puniupa.github.io/posts/2024/AI/TDL.html) をはじめとした丁寧なモデリングが，分野横断の相互理解の試みを促進してくれるのではないかと，筆者は意気込んでいる．