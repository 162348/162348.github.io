---
title: "階層モデル再論"
subtitle: "多変量解析から機械学習へ"
author: "司馬博文"
date: 8/12/2024
date-modified: 8/14/2024
image: Images/MM.svg
categories: [Statistics, Kernel, Probability, Bayesian]
biblio-title: 参考文献
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．
listing: 
    -   id: kernel-listing
        type: grid
        sort: false
        contents:
            - "NCL.qmd"
            - "Manifold.qmd"
            - "Deep.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
# shift-heading-level-by: -1
---


{{< include ../../../assets/_preamble.qmd >}}

## はじめに {.unnumbered}

潜在変数模型とはどうやらとんでもなく広い射程を持った対象であるようである．

::: {.callout-tip appearance="simple" icon="false" title="潜在変数モデルとは……"}

1. 心理学，経済学をはじめとして多くの分野で中心的に扱われてきたモデルである（[構造方程式モデル](https://ja.wikipedia.org/wiki/共分散構造分析)，因子分析，同時方程式モデル，[Probabilistic Graphical Model](../Computation/PGM1.qmd) など）．
2. ベイズ統計学では **階層モデル** (hierarchical model) として極めて重要な役割を果たす．
3. 生成モデリング（[VAE](Deep4.qmd), [EBM](../Samplers/EBM.qmd), [Diffusion](../Samplers/Diffusion.qmd), [GAN](Deep3.qmd)）も，観測変数上の周辺分布がデータ分布に近づくように潜在変数模型を学習する方法である．
4. 表現学習や独立成分分析だけでなく，脳も潜在変数模型に基いてメンタルモデルを構成しているという仮説もある（[InfoMax に関する稿](NCL.qmd#sec-InfoMax)も参照）．
5. 情報理論において，通信路は潜在変数模型としてモデリングできる．この方向には，潜在変数模型は数学的には確率空間の圏上の図式であるとして研究されている [@Perrone2024]．

:::

このように種々の文脈で登場する潜在変数模型であるが，[**それぞれの文脈において「潜在変数」の果たす役割は全く違う**]{.underline}．

しかし，数学的には全く同じ枠組みで記述できる．従って，そのように扱うことは一定の価値を持つだろう．

実際，近年になり，これから本稿で解説するように，潜在変数モデルの観点から心理学，経済学，環境科学，遺伝学，信号処理，逆問題，社会学，政治科学，マーケティング分野で独自に発展した手法が，特定の手法の特別な場合と見れるという理解が進み，手法の交流と知見の交換が進んでいる．

## 本稿の目的 {.unnumbered}

本稿では主成分分析，因子分析，構造方程式モデリング，混合モデル，独立成分分析を，[潜在変数モデルとして解釈し，図式で理解する]{.underline}．

確率変数を丸つきの大文字で表し，$X^i,Y^i$ は観測変数，$Z^i$ は潜在変数を表す．矢印は [確率核](../Probability/Kernel.qmd) を表す．

![混合モデル（第 [-@sec-MM] 節）](Images/MM.svg)

種々の **多変量解析法** を（ベイズ）階層モデルとして統一的に理解すると同時に，それぞれの文脈での「使い方の違い」に注目することを目指す．

## 主成分分析 (PCA) {#sec-PCA}

### はじめに

主成分分析では，$p$ 次元のデータ $\{x_i\}_{i=1}^n\subset\R^p$ の各成分を，より少数の潜在変数を持った１層の線型 Gauss 模型

![](Images/PCA.svg)

で説明しようとする．^[図を見やすくするために，$X^1\to X^{p-1}$ や $X^2\to X^p$ などは省略している．]

歴史的に主成分分析は，おろした垂線の足の二乗距離和の意味でコストが最小になるような線型射影を求める問題 [@Pearson01-PCA] として最初に登場し，値の分散が最大となるような線型射影を求める問題 [@Hotelling33-PCA] として PCA の名前がつき，心理学分野，特に psychometrika で取り上げられて大きく発展した．

このような潜在変数モデルとしての見方は probabilistic PCA [@Tipping-Bishop1999] / SPCA (Sensible PCA) [@Roweis1997] として，[因子分析](#sec-FA)から逆輸入する形で初めて自覚された見方である（第 [-@sec-PPCA] 節も参照）．

確率的な見地から見れば，正規性を仮定した変数 $Z^1,\cdots,Z^r$ の事前分布が互いに独立なデルタ分布に縮退している場合が古典的な PCA である [@Roweis1997]．

いずれの場合も追加の過程なくしてモデルは識別可能性がなく，後続タスクに応じて種々の制約を追加することで所望の解を得る，という動的な使い方がなされる．

以降，$X\in\L(\Om;\R^p),Z\in\L(\Om;\R^r)$ を確率変数，
$$
\b{X}=(x_i^j)\in M_{n,p}(\R),\b{Z}=(z_i^j)\in M_{n,r}(\R)
$$
を行列することに注意．

### 概要

PCA ではデータ行列を
$$
\b{X}:=\vctrr{x_1^\top}{\vdots}{x_n^\top}\in M_{n,p}(\R)
$$
で定めたとき，データ次元 $p$ より小さい数の成分 $r$ で説明しようとする：
$$
\b{X}\approx\b{Z}C^\top,\qquad\b{Z}:=\vctrr{z_1^\top}{\vdots}{z_n^\top}\in M_{n,r}(\R),C\in M_{p,r}(\R).
$$

::: {.callout-tip appearance="simple" icon="false"}

* 古典的には，$z_{ij}$ を（主成分）**得点** (score)，$Z^i$ を **合成変量**，$c_{ij}$ を **負荷量** (loading) ともいう [@足立-山本2024]．
* 機械学習では $Z^1,\cdots,Z^r$ を **潜在因子**，$W\in M_{pr}(\R)$ を **荷重** (weight) ともいう [@Murphy2022]．

:::

この問題は $\b{X}$ の [特異値分解](../FunctionalAnalysis/SVD.qmd) (SVD) $\b{X}=U\Sigma V^\top$ により解ける：
$$
\b{Z}=U\Sigma_{1:r}^\al A=\b{X}(\underbrace{V\Sigma^{\al-1}_{1:r}A}_{=:W}),\qquad C:=V\Sigma_{1:r}^{1-\al}(A^{-1})^\top.
$$
ただし，$\al\in\R,A\in\GL_p(\R)$ は任意である．この解は，特異値分解の性質により，残差を Hilbert-Schmidt ノルムの意味で最小にする：
$$
\min_C\norm{\b{X}-\b{Z}C^\top}_\HS=\min_C\frac{1}{n}\sum_{i=1}^n\abs{x_i-Cz_i}^2=\sigma_{r+1}
$$ {#eq-PCA-objective}
この目的関数は復元誤差とも理解できる．ただし，$\sigma_{r+1}$ は行列 $\b{Z}C$ の第 $r+1$ 特異値である．

### 主成分分散最大化

荷重行列 $W$ が $W^\top W=I_r$ を満たすという制約条件を追加すると，目的関数 ([-@eq-PCA-objective]) は潜在変数の分散を最大にすることと等価になる：
$$
\argmin_{W}\norm{\b{X}-\b{Z}W}_\HS=\argmin_W\Tr((\b{X}W)^\top\b{X}W).
$$ {#eq-PCA-objective2}

すなわち，$\b{Z}=\b{X}W$ の変動が差大になるようにすれば良い．

そのためには，確率変数 $X$ のデータ行列 $\b{X}$ から計算した経験共分散行列 $S\in M_{p}(\R)_+$ の固有ベクトルのうち，対応する固有値が大きいものから $w_1,\cdots,w_r$ として荷重行列とすれば良い：
$$
W:=(w_1\;\cdots\;w_r).
$$

実はこれは解の１つに過ぎず，$W$ に右から直交行列を乗じて「回転」させたものは全て解になる．上の解は追加の条件 $Z^\top Z=I_r$ を課すことで特定される．

### 計算上の注意

各次元に関する長さのスケールを揃えるために，PCA を始める前にデータを正規化しておくか，または共分散行列 $S$ の代わりに，相関行列を用いるべきである．

また，実際に最適化や相関行列の固有値分解をすることはなく，基本的に SVD の方が $O(np^2)+O(p^3)$ と高速である [@Unkel-Trendafilov2010]．

さらに次元 $p$ が高い場合は，**確率的 SVD** [@Halko+2011], [@Drineas+2016] を用いてさらに $O(nr^2)+(r^3)$ まで削減できる．このような手法は確率的数値解析と呼ばれる [@Murray+2023]．

### 線型射影による次元縮約 {#sec-dimension-reduction}

$W^\top W=I_r$ の仮定の下で，PCA の目的関数 ([-@eq-PCA-objective]) は，潜在変数の分散最大化 ([-@eq-PCA-objective2]) と見れるのだった．

これは同じ仮定の下で，データ変数 $X$ の最小誤差の線型射影を求める問題とも見れる：
$$
\argmin_W\norm{\b{X}-\b{Z}W}_\HS=\argmin_W\norm{\b{X}-\b{X}WW^\top}_\HS.
$$

なお，一般の行列 $A$ について $P_A=A(A^{-1}A)^+A^\top$ は $\Im A$ 上の直交射影になる．$A$ が直交行列であるとき，$P_A=AA^\top$ が成り立つ．

### 因子分析志向の主成分分析

因子分析では，$Z^1,\cdots,Z^r$ を対等な因子と見て，それぞれのデータへの影響を調べたい．このような場合は，
$$
\frac{1}{n}\b{Z}^\top\b{Z}=I_r
$$
が自然な制約になる．この際の解は，直交行列 $T\in O_r(\R)$ の違いを除いて，
$$
\b{Z}=\sqrt{n}UT,\qquad C=\frac{1}{\sqrt{n}}V\Sigma_{1:r}T,\qquad W=\sqrt{n}V\Sigma_{1:r}^{-1}T,
$$
まで確定する．

しばしば，追加の仮定
$$
C^\top C=\diag(\rho_{1:r}),\qquad \rho_1\ge\cdots\ge\rho_r\ge0
$$
を課して得られる一意な解
$$
\b{Z}=\sqrt{n}U,\qquad C=\frac{1}{\sqrt{n}}V\Sigma_{1:r},\qquad W=\sqrt{n}V\Sigma_{1:r}^{-1},
$$
を **初期解** と呼び，これを「回転」させることで他の解が探索され，所望の分解を探す．

因子分析では [@Thurstone1947] 以来，種々の回転法とアルゴリズムが蓄積している [@足立-山本2024]．一般にこの文脈では，[@Thurstone1947] にいう「単純構造」を達成した，解釈が容易な因子をドメイン知識に基づいて構成することを目指す．この「単純構造」とは，現代でいう一種の disentangled factor と理解できる．

## 因子分析 (FA) {#sec-FA}

### はじめに

主成分分析が「低階数近似」ならば，因子分析は「高階数近似」というべきである [@足立浩平2023]．

![](Images/FA.svg)

より正確には，因子分析は，観測の各次元 $X^1,\cdots,X^p$ ごとに「独自因子」$Z^1,\cdots,Z^p$ を想定しつつ，全観測に共通する「共通因子」$F^1,\cdots,F^r$ をどのように抽出できるかを考える，という志向性を持つ：

![](Images/FA2.svg)

この意味では，FA は独自因子 $U^1,\cdots,U^p$ を追加した PCA とも理解できる．

歴史的には [@Spearman1904] が古典テスト理論の文脈で $r=1$ の因子分析を，[@Thurstone1947] が一般の $1\le r<p$ の場合の因子分析を「回転」の手法と共に導入した．

さらに興味深いことに，FA では PCA をはじめとした多くの多変量分析手法と違い，[@Lawley1942], [@Anderson-Rubin1956] らにより，初期から確率的な扱いが発展した手法である [@足立-山本2024]．

FA に倣う形で，PCA にも確率論的なアプローチが導入された [@Tipping-Bishop1999], [@Roweis1997]．

### 概要

FA では $\b{Z}=(\b{F}\;\b{U})\in M_{n,r+p}(\R)$ の分解に基づき，
$$
\b{X}\approx\b{F}A^\top+\b{U}\Psi^{1/2},\qquad A\in M_{r,p}(\R),\Psi=\diag(\psi_1,\cdots,\psi_p)\in M_p(\R),
$$
によってデータ行列 $\b{X}\in M_{n,p}(\R)$ を説明しようとする．^[[@足立-山本2024], [@足立浩平2023] によると，この行列分解による定式化は Henk A. K. Kiers によるもので，初出は同大学からの博士論文 [@Socan2003] が最初ではないか，とのこと．この見方を MDFA (Matrix Decomposition Factor Analysis) と呼ぶ．[@足立浩平+2019] も参照．]

PCA よりさらに識別可能性は絶望的であるが，FA では潜在変数の解釈可能性担保のため，次の仮定を課す：
$$
\b{1}_n^\top\b{F}=\b{0}_r,\qquad \b{1}_n^\top\b{U}=\b{0}_p,
$$
$$
\b{F}^\top\b{F}=n\b{I}_r,\qquad \b{U}^\top\b{U}=n\b{I}_p,\qquad\b{F}^\top\b{U}=O.
$$
すなわち，推定される確率変数 $F,U$ が標準化されていて互いに無相関であるように誘導する．

また，$\b{U}$ の経験分散が $\Psi$ になることに注意．

::: {.callout-tip appearance="simple" icon="false"}

* 古典的には，$f_{ij}$ を共通因子，$\psi_j$ を独自因子の **得点** (score)，$a_{ij}$ を **負荷量** (loading) ともいう [@足立-山本2024]．
* 機械学習では $Z^1,\cdots,Z^r$ を **潜在因子**，$W\in M_{pr}(\R)$ を **荷重** (weight) ともいう [@Murphy2022]．

:::

この問題は，$C:=(A\;\Psi^{1/2})$ と定めると，PCA と同じ問題 ([-@eq-PCA-objective]) に帰着される：
$$
\min_C\norm{\b{X}-\b{Z}C^\top}_\HS.
$$

これはやはり特異値分解により解くことができる [@DeLeeuw04-SimultaneousEstimationOfEFA]．

解は直交行列による回転を除いても，やはり一意に定まらないようである．

### 確率的アプローチ

ここで，
$$
U:=\vctrr{U^1}{\vdots}{U^p}\in\L(\Om;\R^p),\qquad F:=\vctrr{F^1}{\vdots}{F^r}\in\L(\Om;\R^r),
$$
を確率変数とすると，
$$
X\approx AF+\Psi^{1/2}U
$$ {#eq-probabilistic-FA}
によって $X$ に確率モデルが誘導されることになる．

#### 正規性の仮定 {#sec-PPCA}

$U,F$ に正規性の仮定をおけば，このモデルは EM アルゴリズムなどを用いて最尤推定できる [@Rubin-Thayer1982], [@Ghahramani-Hinton1996]．このような最尤推定のアプローチは [@Lawley1942] から考えられていた．

この見方が PCA にも応用された．追加の仮定
$$
A^\top A=I_{r},\qquad \Psi=\sigma^2I_p,
$$
の下での FA への確率論的アプローチを probabilistic PCA [@Tipping-Bishop1999] / SPCA (Sensible PCA) [@Roweis1997] という．

$\sigma\to0$ の極限で古典的 PCA が回復される．

#### 共分散構造分析

一方で，分布の仮定は課さず，$X$ の経験分散 $S$ を，式 ([-@eq-probabilistic-FA]) の右辺の共分散
$$
\Sigma:=AA^\top+\Psi
$$
となるべく近づけるように学習する方法もある．

例えば [@Harman-Jones1966], [@Harman-Fukuda1966] では，Hilbert-Schmidt ノルム $\norm{S-\Sigma}_\HS$ の最小化することで解を探索する方法が考慮された．

このように，データの共分散行列を低階数近似するアプローチは **共分散構造分析** [@Bock-Bargmann1966] ともいう．

さらに，確率論的なアプローチは一般の構造方程式モデル (SEM, 次節 [-@sec-SEM] 参照) へと発展 [@Joreskog70], [@Sorbom1974], [@Joreskog1978] し，現状，共分散構造分析は SEM の特別な場合と解される．^[ただし，[@星野崇宏+2005] は SEM をより一般的とし，共分散構造分析とは観測変数が連続な場合の下位モデルである，と解している．]

### スパース推定

FA のモデルは識別可能とは程遠く，解釈可能性が重要である．[@Thurstone1947] は因子付加行列が「単純構造」を持つことを一つの指標としたが，現代的にはスパース推定の言葉で与えられた **完全単純構造** [@Bernaards-Jennrich2003] を仮定することが増えてきた．

**スパース PCA** [@Zou+2006], [@Jolliffe+2003] では，従来の SVD + 回転ではなく，LASSO 様の $L^1$-正則化項によって，解釈可能な因子付加行列を得ようとする．最終的に得られる目的関数は elastic net [@Zou-Hastie2005] 様になる．

等価だが，自動関連度決定 (ARD) を用いた **Bayesian PCA** [@Bishop1998], [@Archambeau-Bach2008] や spike-and-slab [@Rattray+2009] など，スパース性を促す事前分布を用いることもできる．

### その他の事前分布 {#sec-other-priors-FA}

非正規な事前分布（特に Laplace 分布やロジスティック分布などの裾の重いもの）を用いることで，モデルが識別可能性を回復することがある．

このように，[一般の設定で潜在変数モデルが識別可能になるための条件が，非線型独立分析の分野で提案されている](NCL.qmd#sec-identifiability) [@Khemakhem+2020]．

#### Gamma 分布

また，Gamma 事前分布は非負かつスパースな表現を促進し，カウントデータとよく用いられる [@Canny2004]．

これは環境科学分野の Positive Matrix Factorization [@Paatero-Tapper1994] や信号処理分野の Nonnegative Matrix Factorization (NMF) [@Lee-Seung1999] の，確率論的な一般化と見れる [@Buntine-Jakulin2006]．

#### Dirichlet 分布

また，Dirichlet 事前分布を用いることで，潜在変数 $Z\in\L(\Om;\R^r)$ に
$$
\sum_{i=1}^rZ^i=1
$$
が課されるため，「各次元への依存度」のような意味づけが可能になる．これは政治学における空間分析において，「どの立場への傾倒が強いか」を推定することにも用いられる [@Buntine-Jakulin2006]．

このモデルは multinomial PCA [@Buntine-Jakulin2006] の他に，遺伝学で admixture [@Pritchard+2000]，simplex factor analysis [@Bhattacharya-Dunson2012], 科学出版で mixed-membership model [@Erosheva+2004]，マーケティングで user rating profile model [@Marlin2003] など，種々の分野で独立に提案されている．

### 非線型化

FA の一般化の方向性として，正規性の緩和の他に，線型性の緩和があり得る．

MCMC による推論 [@Hoffman2017] をすることも，または指数型分布 [@Collins+2001] への拡張や，VAE による非線型化を通じて変分推論をすることも考えられる．

[自己符号化器](Deep4.qmd#sec-AE) は，まさに非線型な潜在変数モデルに対する最尤推定を行っており，４層以上のニューラルネットワークを用いることで PCA を非線型化して一般化することができる．^[[@江口真透1999] 第３節に，PCA をニューラルネットワークにより近似的に実行する方法が紹介されている．[@Ghojogh+2022] はサーベイを与えている．]

また，カーネル法と Gauss 過程により非線型化することもできる [@Lawrence2005]．

### 混合モデリング {#sec-MixFA}

複数の線型 Gauss 因子分析モデルの重ね合わせとみなす **mixture of factor analysers** [@Ghahramani-Hinton1996] も単純ながら表現が高く，EM アルゴリズムや SGD [@Ricahrdson-Weiss2018], [@Zong+2018] によって推定できる．

[@Ricahrdson-Weiss2018] では生成モデルとしての性能も GAN と劣らないこと，VAE や GAN などの生成モデルよりも分布へのフィッティングが良いことを報告している．

さらにこのアプローチはノンパラメトリックベイズ法につながる．この方法では，例えば [@Paisley-Carin2009] では Beta 過程事前分布をおき，Gibbs サンプラーで推論することで，混合数 $K$ も同時に自動で決定できる．

## 構造方程式モデリング (SEM) {#sec-SEM}

### はじめに

[@Joreskog1969] は因子分析モデルを潜在変数モデルとして，事前情報を取り入れるなど柔軟に用いた．

特に，データを（現代でいう）訓練データと検証データに分けて，因子分析により推定された潜在変数間の関数関係を検定するための方法を提案し [@Joreskog-Lawley1968]，これを **検証的因子分析** (Confirmatory FA) と呼び，それ以前の手法に **探索的因子分析** というレトロニムを与えた．^[[@豊田秀樹1992] では CFA を確認的因子分析と呼んでいる．[@豊田秀樹1991] では，古典テスト理論を確認的因子分析の下位モデルとして紹介している．また，このような因果関係の確認的方法は，社会学における [@Herbert-Simon57-ModelsOfMan] の基準などが知られていた．]

最終的に，潜在変数同士により一般的な関数関係も考慮したものなど多くの潜在変数モデルが，共分散構造に基づいた非線型数値最適化を推論エンジンとして統一的に推定できることに辿り着いた．^[[@Joreskog70] は具体的なモデルを例に取り，彼の検証的因果分析が，パス解析 [@Wright1918], [@Wright1921] のように因果分析に応用できることを示した結果だと言える [@Asher83-Causal]．この観点から，パス解析は「検証的因果推論」と表現することもできる [@統計科学のフロンティア5 p.73]．]

このことに加えて，潜在変数間の関数関係に適切な仮定をおくことで，因果推論・高次の因子分析・分散分析など従来考慮されなかった新たなタスクにも適用可能であることも了解された [@Joreskog1978], [@Bentler1980]．^[[@狩野裕2002] は SEM の射程と得意・不得意を分析している．]

現代では特徴抽出，生成，表現学習にも用いられていると思うと感慨である．

これを **共分散構造分析** または **構造方程式モデリング** (SEM: Structural Equation Modeling) という．^[現代ではコンピュータの力により，新たに「生成」「表現学習」というタスクが加わったと思うと，感慨深い．] 心理学の文脈では，潜在変数のことを **構成概念** (construct) と呼び，潜在変数間は無関係とした従来の因果分析モデルを **測定方程式** と呼ぶ．^[[@清水和秋1989], [@豊田秀樹1992], [@統計科学のフロンティア5 p.82] も参照．]

### 部分最小自乗モデル (PLS) {#sec-PLS}

PLS (Partial Least Square) モデル [@Joreskog-Wold1982], [@Gustafsson2001] では，次のような潜在変数モデルを用いて，２つの構成概念間の因果関係を評価しようとする [@豊田秀樹1991]：

![](Images/PLS.svg)

なお，パス図において，潜在変数から観測変数に矢印が伸びている場合，これは影響的指標と呼ばれ，観測のモデルと解され，誤差が入ることが想定される [@豊田秀樹1991]．^[また，パス図では観測変数は四角で囲むべきであるが，ここでは省略した．] 逆の矢印は形成的指標という．

すなわち，PLS では，$X^1,X^2,X^3,\cdots$ には，$Z^1,Z^2,\cdots$ とは独立な独自因子が作用していると仮定されている．

このような仮定は，$Y^1,Y^2,\cdots$ を被説明変数として，教師あり PCA [@Yu+2006] に有用である．

というのも，被説明変数のうち必ずしも $Y^1,Y^2,\cdots$ に関係する要素が全てとは限らないために，$Z^1,Z^2$ の間で間接的に回帰分析を行いたい場合に自然な設定である [@Nounou+2002]．

### 構造方程式モデリングの発展

PLS において，潜在変数から構成概念への矢印が全て影響的であった場合，これは潜在因子の間に関係が仮定されていることを除いて，（探索的）因子分析と等価になる．

一般に，SEM は，潜在変数同士の関数関係も考慮した因子分析モデルだと理解できる．

このようなモデルは，社会学において **多重指標分析** と呼ばれていたモデルに相当し [@白倉幸男1984] [@清水和秋1989]，経済学において **同時方程式モデル** と呼ばれていたモデルに相当する [@Bentler1980]．^[同時方程式は潜在変数を持たない模型で，経済学におけるパス解析の継承と見れる [@豊田秀樹2007]．特に Keynes 経済学におけるマクロな経済計画の発想で，[Cowles 委員会](https://ja.wikipedia.org/wiki/コウルズ財団) により 1940 年代から 1950 年代にかけて盛んに研究された．]

加えて，心理学・行動計量学においても，多くの既存の多変量解析法（因子分析，パス解析，二段階抽出モデル，潜在構造分析，項目反応モデルな）はいずれも SEM の特殊な形だと解釈できることが自覚された [@McArdle1984], [@Muthen2002]．

こうして SEM の名と LISREL プログラムの下で，多くの社会科学分野で使われていたモデルが，形式的にはほとんど等価であるという了解が形成されていった．

このことから，SEM は第二世代の多変量解析 [@Fornell1985] とも評される．^[多変量解析の高級言語とか形容することもあるという．構造方程式モデリングについては，[@豊田秀樹1991], [@狩野裕2002] も参照．]

### 計算統計学という要素

構造方程式モデリングが普及した理由の一つとして，計算機統計学の発展とうまく合流した点が見逃せない．

そもそも Jöreskog は，因子分析を研究していた時期 [@Joreskog1966] [@Joreskog1967a] から，数値的な解法とコンピュータプログラムの開発にも重点を置いていた．特に，因子分析モデルを，[DFP 法](https://ja.wikipedia.org/wiki/DFP法) に基づいて数値的に最尤推定する方法を提案した [@Joreskog1967a]．

SEM も，コンピュータプログラム LISREL (LInear Structural RELationships) [@Joreskog-vanThillo1972] の存在が，広い分野の人口に膾炙した要因として大きい [@清水和秋1989], [@Grimm-Yarnold2016]．

構造方程式モデルがどのように因子分析，因果分析，共分散構造分析を統合し，LISREL プログラムと共に発展していたかは，[@清水和秋1994] に大変わかりやすくまとまっている

### 正準相関分析 (CCA) {#sec-CCA}

正準相関分析 [@Hotelling36] においては，２つの構成概念の間は相関関係で結び，すべての観測は形成的な影響を及ぼすとする（観測誤差は想定しない） [@豊田秀樹1991]：

![](Images/CCA.svg)

このモデルでは $X^1,X^2,X^3$ とその潜在要因 $Z^1$，$X^4,X^5$ とその潜在要因 $Z^2$ とを完全に対等に扱い，その間の関係を理解しようとする．

例えばマルチモーダル学習において，$X,Y$ が類似したタスクに関するデータという場合に応用がある [@岩瀬-中山2016]．また，PLS と共に特徴抽出にも用いられる [@Sun+2009]．

複数の標本に対して同時に実行する主成分分析ともみなせるが，別々に PCA を実行した場合と違い「共通要因」を抽出することに志向がある [@赤穂昭太郎2013]．

なお，正準相関分析が，このような確率論的解釈ができることは [@Bach-Jordan2005] で自覚されたことである．

この潜在変数モデルとしての観点から，$Z^3,Z^4,\cdots$ がある GCCA (Generalized CCA) [@Horst1961]，指数分布族の場合 [@Klami+2010]，ニューラルネットワークにより非線型にした DCCA [@Andrew+2013]，さらに変分推論する場合 [@Wang+2017], [@Suzuki+2017] に拡張されている．

質的データをダミーベクトルに変換して（一般化）正準相関分析を行う，質的データの解析法を **対応分析** (correspondence analysis) または **数量化第III類** ともいう．^[オランダ学派を中心に等質性分析とも呼ぶ．]

## 混合モデル (MM) {#sec-MM}

### はじめに

混合モデルは，次のようなたいへん基本的な設定であるが，第 [-@sec-MixFA] 節で見たように，例えば因子分析モデルと組み合わせることで極めて豊かな表現力を持つ．

![](Images/MM.svg){fig-align="center"}

混合モデルは SEM の別の選択肢としても使える．また，ランダム効果要因を明示的にモデルに組み込む意味で，一般線型モデルの確率論的な拡張と考えることもできる [@狩野裕2002]．^[ただし，SEM は共分散構造，混合モデルは平均構造に分析の焦点がある，という志向の違いもある．[@狩野裕2002] も参照．]

### 正規混合モデル (GMM)

$Z\in\L(\Om;[K])$ は
$$
[K]=\{1,\cdots,K\}
$$
に値を取る離散確率変数で，確率核 $Z\to X$ が
$$
p(x|z=k)\,dx=\rN_p(\mu_k,\Sigma_k)
$$
と表せる場合，$X$ に課される仮定を **正規混合モデル** (GMM: Gaussian Mixture Model) という．

$Z\sim\rU([K]),\Sigma_k=I$ の場合，これは [$K$-平均クラスタリングに等価](../Computation/VI2.qmd#sec-EM-and-K-means) なモデルとなる．

これは SGD により訓練をすることで，生成のタスクにおいても GAN に匹敵する性能も持つ [@Ricahrdson-Weiss2018]．

また，デノイジングや deblurring, inpainting, super-resolution などの画像逆問題は，巨大な GMM の潜在変数の推定として理解できる [@Zoran-Weiss2011], [@Papyam-Elad2016]．

### 正規スケール混合モデル (GSM)

Gaussian scale mixture モデルとは，
$$
p(x|z)\,dx=\rN_p(0,\sigma_0^2z)
$$
で定まる階層モデルである．

このモデルは，$Z$ の分布により，種々の（特に裾の重い）分布を表せる：

::: {.callout-tip appearance="simple" icon="false"}

1. $Z\sim\Ber(\pi)$ のときを spike and slab 分布という：
    $$
    p(x)\,dx=\pi\rN(0,\sigma_0^2)+(1-\pi)\delta_0.
    $$
2. $Z\sim\rC(1)_+$ のとき，**馬蹄分布** [@Carvalho+2010] という．^[$\rC(\sigma)_+$ は Cauchy 分布 $\rC(0,\sigma)$ を $\R_+$ 上に制限したものである．truncated Cauchy または half-Cauchy という．]
:::

### 潜在 Dirichlet 配分 (LDA)

#### はじめに

文書の埋め込み・数値表現を得るために，単語 $i\in[M]$ が文書 $j\in[N]$ に現れた回数をカウントした行列 $\b{C}\in M_{MN}(\N)$ を通じた主成分分析が用いることも考えられる．

これを **潜在意味索引** (LSI: Latent Semantic Indexing) [@Deerwester+1990] と呼ぶ．得られた低次元埋め込みを文書検索 (document retrieval) などに用いることもできる．

$\b{C}$ の列も単語とし，帯幅 $h>0$ を決めて，$h$ 文字以内に単語 $i,j\in[M]$ が共起した回数を $C_{ij}$ とすると，全く同様の手続きが，単語の埋め込みに応用できる．これを [**潜在意味解析**](https://ja.wikipedia.org/wiki/潜在意味解析) (LSA: Latent Semantic Analysis) [@Deerwester+1990] と呼ぶ．

#### 確率的潜在意味索引 (PLSI) {#sec-PLSI}

[@Hofmann1999] による pLSI または aspect model は LSI を確率モデル，特に混合モデルとして解釈し直したものである．

単語数よりも少ない数の **トピック** $Z$ というものがあり，これが単語を決めている，というモデルを想定した．

![](Images/PLSI.svg){fig-align="center"}

このモデルを通じて，トピック $Z$ の分布（あるいは，現代的には $\Theta$ の値）を「文書」の特徴量とする，というアイデアである．

#### Dirichlet 事前分布の追加 {#sec-LDA}

変数 $\Theta$ に Dirichlet 事前分布を追加し，完全なベイズの見方を提示したのが Latent Dirichlet Allocation [@Blei+2003] である．

$\Theta$ を文書，$Z$ トピック，$W$ をトピックごとの語彙デッキとする．

![](Images/LDA.svg)

最終的に，トピック $Z$ とその人の語彙 $W$ が合わさって，単語 $X$ が観測されるというモデルが考えられている．

#### 確率的トピックモデル

自然言語処理において，単語分布のモデリングの潜在変数は **トピック** と呼ばれて，これを確率的にモデリングする手法は PTM (Probabilistic Topic Model) [@Blei2012] と呼ばれている．

「トピック」は短い文章の中でも激しく移り変わることが知られている [@Church-Gale1991]．

そのため，LDA では，$\Theta$ の事前分布と $W$ の事前分布は，
$$
\Dirichlet(\al\b{1}),\qquad\al>0,
$$
という形で，極めて小さい $\al>0$ を設定し，特定のトピックがどの文書に現れるかは極めてスパースになるようにモデリングをする．

#### 推論

LDA の推論手法には変分推論 [@Blei+2003] や Gibbs サンプリング [@Griffith-Steyvers2004]，そしてスペクトルに基づく方法 [@Arova+2013] がある．

トピック数の決定には，尤度を [焼なまし重点サンプリング](../../Surveys/SMCSamplers.qmd#sec-AIS) で計算する方法 [@Wallach+2009] の他，ノンパラメトリックベイズ法も用いられる [@Teh+2006]．

#### 時系列化

単語の並びは明らかな方向性があり，対照的なモデリングはこの消息を取り逃がしていると考えられる．

そこで，トピックの移り変わりを捉えるモデルとして dynamic topic model [@Blei+2006] がある．これは Kalman 平滑化と変分推論を組み合わせている様である．

また単語の時系列構造を捉えるために，LDA に隠れ Markov モデルを組み合わせた LDA-HMM [@Griffiths+2004] が提案された．TopicRNN [@Dieng+2017] ではより長距離の相関を捉えるために，[RNN](Deep.qmd#sec-RNN2) と組み合わせている．

### 状態空間モデル (SSM) {#sec-SSM}

#### 概要

状態空間モデル (State Space Model) は，混合モデルの時系列化と捉えられる：

![](Images/SSM.svg)

潜在変数 $X_t$ が離散的である場合は特に **隠れ Markov モデル** (HMM: Hidden Markov Model) [@Baum-Petrie1966] と呼ばれる．

HMM に関しては早くから EM 様の推定手法 **Baum-Welch アルゴリズム** [@Baum-Eagon1967], [@Baum+1970] が提案されているが，データサイズが大きい場合は SGD が用いられる．Blocked Gibbs サンプラー [@Scott2002] や，潜在変数を消去して，周辺尤度に関してスペクトル法／テンソル分解 [@Hsu+2012], [@Anandkumar+2012], [@Anandkumar+2015], [@Obermeyer+2019] を実行するなどの代替手法がある．

#### 構造的状態系列モデル (S4) {#sec-S4}

S4 (Structured State Space Sequence) [@Gu+2022], [@Gu+2020], [@Goel+2022] とは，時系列を深層ニューラルネットワークの力でモデリングするために，線型 Gauss で単純な SMM を上下にスタックし深層にしたものである．各層は LSSL (Linear State Space Layer) と呼ばれる．

さらに長距離の依存性に耐えるために，S5 [@Smith+2023] や Mamba [@Gu-Dao2024] が提案されている．後者では，選択的に記憶を忘却できるような「選択」機構 (S6: Selective SSM) を導入している．

## 独立成分分析 (ICA) {#sec-ICA}

### はじめに

（線型）独立成分分析で用いるモデルは，PCA や FA のそれと全く変わらず，線型変換 $x_n=Az_n$ でデータを説明しようとする：

![](Images/ICA.svg)

ただし，潜在変数 $Z^1,\cdots,Z^r$ は互いに **独立** であるという「真の構造」が強く想定される場合に使われる．

加えて，モデルの **識別可能性** を重視する．このために，（独立）因子分析（第 [-@sec-other-priors-FA] 節）で考えたように，正規分布より裾の重い事前分布を導入することで，モデルの識別可能性を確約する．^[[@Hyvarinen-Oja2000] では，[@Bell-Sejnowski1995] のように測定誤差を考えない場合を ICA といい，誤差も入る一般の場合を IFA (Independent Factor Analysis) と呼び分けている．[@統計科学のフロンティア5 p.110] も参照．「これを回転の不定性という．因子分析はさまざまな考察によって，この不定性を解消しようとする．独立成分分析は，非正規性を仮定すれば，この不定性が消えることを示したものとも言える」[@統計科学のフロンティア5 p.13]．]

この意味で，確率モデルとしては PCA / FA に等価であるが，典型的な ICA の文脈では $Z^1,\cdots,Z^r$ は非正規確率変数であり，$A$ を生成荷重や混合行列，$A^{-1}$ を **認識荷重** (recognition weight) などという．

### 推定手法

最初に [音源分離](https://ja.wikipedia.org/wiki/ブラインド信号源分離) について適用された [@Bell-Sejnowski1995] では，$X$ と $Z$ の相互情報量の最大化が目指された．

最尤推定は EM アルゴリズムの他に近似 Newton 法で実行されることもあり，fast ICA [@Hyvarinen-Oja2000] と呼ばれる．

また古典的には，探索的データ解析で考案された **射影追跡** (PP: Projection Pursuit) [@Friedman-Tukey1974] みたく，学習される $Z$ の分布が Gauss からなるべく遠いように学習することも考えられた．

disentangled な表現を学習したい場面では，$Z$ の成分同士の相関が最小になるように学習される；
$$
\KL\paren{\P^Z,\bigotimes_{j=1}^r\P^{Z_j}}.
$$

最小情報コピュラに基づく方法も提案されている [@Bedford+2016], [@Sei-Yano2024]．

他にも表現学習や認知科学の文脈を踏襲して，[InfoMax](NCL.qmd#sec-InfoMax) やスパース符号化などの原則がある．

### 非線型化

[非線型独立成分分析は，表現学習の文脈でも研究されている](NCL.qmd)．

## おわりに {.unnumbered .unlisted}

現代の深層生成モデルは，いずれも非線型な潜在変数モデルであると理解できる．

その意味で，次の記事は全て，本稿の続きであり，本稿は現代の機械学習の壮大な序章としても理解できる．

::: {#kernel-listing}
:::

非線形性の他に本稿で扱わなかったものは深層モデルである．

だがそもそも，現代のニューラルネットワークが深層化したのは，単純で可微分なモジュール性を保ちながら表現力を高めるためのトリックであり，確率論的には本稿で扱ったモデルと等価であるはずである．

ニューラルネットワークの他にも，計算のために深層化したモデルを考える場面は多い．例えばアニーリングを用いた [SMC サンプラー](../../Surveys/SMCSamplers.qmd) は，グラフカルモデル $Z\to X$ の潜在変数 $Z$ の推定を，人工的に時系列構造を見出して状態空間モデル [-@sec-SSM] にあてはめてサンプリングしやすくする方法と言える．

しかし，[確率核は射をなす](../Probability/Kernel.qmd)のだから，全てのモデルは本質的には一層であるとみなすこともできるのである．

この見方をとった方が計算効率が上がるという例もある．例えば [@Chen+2024] では，トランスフォーマーの注意機構をランダム Fourier 特徴写像で近似し，[Monte Carlo 法によって元のモデルと等価な計算を安価に行っている](Kernel.qmd#sec-RFF)．

[ベイズ機械学習](https://puniupa.github.io/posts/2024/AI/BAI.html) や [位相的機械学習](https://puniupa.github.io/posts/2024/AI/TDL.html) をはじめとした，丁寧なモデルへの理解が，これからも手法への統一した視点からの理解と，応用分野を横断した相互理解を促進してくれるのではないかと，筆者は意気込んでいる．

## 扱ったモデル一覧 {.appendix .unnumbered}

::: {layout-ncol=2 layout-valign="bottom"}

![[PCA](#sec-PCA)](Images/PCA.svg)

![[FA](#sec-FA)](Images/FA.svg)

![[PLS](#sec-PLS)](Images/PLS.svg)

![[CCA](#sec-CCA)](Images/CCA.svg)

![[PLSI](#sec-PLSI)](Images/PLSI.svg)

![[LDA](#sec-LDA)](Images/LDA.svg)

![[SSM](#sec-SSM)](Images/SSM.svg)

![[ICA](#sec-ICA)](Images/ICA.svg)

:::

![[@Murphy2023 p.920] より，本稿で扱ったモデルのいくつかを含んだ，数式による一覧表．すでに図式による解説を受けた後だと，より見やすいだろう．$\operatorname{Cat}(c|\b{\pi})$ は確率ベクトル $\b{\pi}$ が定める質量関数を表す．](Images/HierarchicalModels.png)

## 付録 {.unnumbered .appendix}

ここでは，歴史を感じる引用をいくつか紹介したい．

> 心理測定学 (psychometrics) における因子分析，計量経済学 (econometrics) における同時方程式モデル (simultaneous equation models), そして生物測定学 (biometrics) におけるパス解析 (path analysis) を，共分散構造分析の下に統一化することが可能となった契機は，潜在変数 (latent variables) の概念である [@Bentler1980]．[@清水和秋1989]

そして，異分野横断の知見交流が進んだ契機の一つは，LISREL プログラムの存在であった．[@清水和秋1994] では，ETS での安定した研究環境が LISREL の継続的な保守を可能にして最終的には WINDOWS 上でも安定して提供され，これを用いることを通じて異分野を巻き込みながら構造方程式モデリングが発展していった様子が詳細に解説されている．LISREL はバージョン VI まである．

> 紹介した文献からもわかるように，この分野は最近になってやっと日本では注目されてようになってきた。 このように日本へのこの方法論の導入が遅れた理由の一つはソフト流通の問題にあると筆者は考えている。青木 (1988) や土田 (1988) が述べているように， LISREL は大型計算機の場合， アメリカ産のコンビュータでしかサポートしてくれないとのことである。[@清水和秋1989]

そして現代はというと，計算機統計学と機械学習が先行し（過ぎ）ていると思える．

もしその通りならば，種々の科学への応用とそれぞれ固有の課題への特殊化が，これからの未来を彩ってくれるのかもしれない．