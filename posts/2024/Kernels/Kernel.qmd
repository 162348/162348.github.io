---
title: "カーネル法の概観"
subtitle: "表現学習に向けて"
author: "司馬 博文"
date: 8/10/2024
date-modified: 8/11/2024
image: Images/Gibbs.svg
categories: [Kernel]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: カーネル法は，
listing: 
    -   id: kernel-listing
        type: grid
        sort: false
        contents:
            # - "SVM.qmd"
            - "GP.qmd"
            - "GP2.qmd"
            - "../../2023/KernelMethods/KernelMethods4Mathematicians.qmd"
            - "Kernel1.qmd"
            - "Manifold.qmd"
            - "NCL.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
---

### 関連ページ {.unnumbered .unlisted}

::: {#kernel-listing}
:::

{{< include ../../../assets/_preamble.qmd >}}

## 正定値カーネル

### はじめに

カーネル法は，カーネルの選択と構成が第一歩になる．例えば [Gauss 過程](GP2.qmd) は，平均関数と共分散関数＝正定値カーネルを定めるごとに定まる．

Gauss 過程回帰を実行するには適切な事前 Gauss 過程を選ぶ必要があり，そのためには適切な正定値カーネルを選ぶ必要がある．

::: {.callout-tip title="定義（正定値核関数）" icon="false"}

一般に **核** とは，可測関数 $E,F$ の間の写像 $K:E\to\cS(F)$ をいう．ただし，$\cS(F)$ は $F$ 上の符号付き測度全体の集合とする．

**核（関数）** とは，$F$ 上に自然な $\sigma$-有限測度 $\nu\in\cS(F)$ がある際に，次を満たす関数 $k:E\times F\to\R$ をいう：
$$
K(x,A)=\int_A k(x,y)\,d\nu(y).
$$

:::

### 定常カーネル

分布が時間変化に対して不変である場合，Gauss 過程の共分散関数
$$
\C(s,t):=\E\SQuare{(X_s-\E[X_s])(X_t-\E[X_t])}
$$
は $\abs{s-t}$ のみの関数である．

#### 動径基底関数核 {#sec-RBF-kernel}

::: {.callout-tip title="定義 (Radial Basis Function kernel / Squared Exponential kernel)^[前者は [@持橋-大羽2019 p.68]，後者は [@Rasmussen-Williams2006 p.14] の用語．[@Murphy2023] では両方が併記されている．Gaussian kernel とも呼ばれる．]" icon="false"}
動径基底関数カーネルとは，
$$
K(r;\ell):=\exp\paren{-\frac{r^2}{2\ell^2}}
$$
で定まるカーネルである．`GPy` での実装は[こちら](https://gpy.readthedocs.io/en/deploy/GPy.kern.src.html#GPy.kern.src.rbf.RBF)．
:::

#### 関連度自動決定核 (ARD: Autonatic Relevance Determination) {#sec-ARD-kernel}

::: {.callout-tip title="定義 (Autonatic Relevance Determination)^[[@MacKay1994], [@Neal1996 p.16] なども参照．]" icon="false"}
動径基底関数カーネルの Euclid ノルムを Mahalanobis ノルムに変更したもの
$$
K(r;\Sigma,\sigma^2)=\sigma^2\exp\paren{-\frac{r^\top\Sigma^{-1}r}{2}}
$$
を関連度自動決定カーネルという．
:::

そもそも関連度自動決定 [@MacKay1994], [@Neal1996 p.16] またはスパースベイズ学習 [@Tipping2001] とは，ニューラルネットワークの最初のレイヤーの荷重をスパースにするために分散不定の正規分布を事前分布として導入する，という技法である [@Loeliger+2016]．

一般に出力をスパースにするためのフレームワークとしても活用され，ARD 核はその最たる例である．

#### Matérn 核

ARD 核は軟化性能を持つため，見本道は無限回微分可能になってしまう．

これが不適な状況下では，Matérn 核
$$
K(r;\nu,\ell)=\frac{2^{1-\nu}}{\Gamma(\nu)}\paren{\frac{\sqrt{2\nu}r}{\ell}}^\nu K_\nu\paren{\frac{\sqrt{2\nu}r}{\ell}}
$$
などが用いられることがある．ただし，$K_\nu$ は修正 Bessel 関数とする．

$\nu$ は滑らか度を決定し，見本道は $\floor{\nu}$ 階微分可能になる．$\nu\to\infty$ の極限で RBF 核に収束する．

$\nu=1/2$ の場合が OU 過程に当たる．

#### スペクトル核

任意の（定常な）正定値関数は，ある関数 $p$ に関して
$$
K(r)=\int_{\R^d}p(\om)e^{i\om^\top r}\,d\om
$$ {#eq-spectral-decomposition}
と表せる．この $p$ は **スペクトル密度** という．

$K$ が RBF 核であるとき，$p$ もそうなる：
$$
p(\om)=\sqrt{2\pi\ell^2}\exp\Paren{-2\pi^2\om^2\ell^2}.
$$

この対応を用いて，スペクトル密度 $p$ をデザインすることで，様々な正定値カーネルを得ることが出来る．

例えば spectral mixture kernel [@Wilson-Adams2013] では，スケール母数と位置母数とについて RBF 核の混合を考えることで，新たな正定値カーネルを構成する．

### 非定常カーネル

環境統計学などにおいて，空間相関の仕方が時間的に変化していくという設定がよくある．

このような時間方向での法則変化を取り入れるものが非定常カーネルである．

#### 多項式核

$$
K(x,y)=(x^\top y+c)^M
$$
は非斉次項 $c$ を持つ，$M$ 次の多項式核と呼ばれる．

#### Gibbs 核

Gibbs 核 [@Gibbs1997] は，ハイパーパラメータ $\sigma,\ell$ を入力に依存するようにした RBF 核である：
$$
K(x,y)=\sigma(x)\sigma(y)\sqrt{\frac{2\ell(x)\ell(y)}{\ell(x)^2+\ell(y)^2}}\exp\paren{-\frac{\abs{x-y}^2}{\ell(x)^2+\ell(y)^2}}.
$$

このようにすることで，$\sigma,\ell$ を別の Gauss 過程でモデリングし，階層モデルを考えることもできる [@Heinonen+2016]．

#### スペクトル核 [@Remes+2017]

正定値核は Fourier 変換を通じて，スペクトル密度によって指定することもできる（Bochner の定理）．

この手法は，非定常核に対しても [@Remes+2017] が拡張している．

### 位相空間上の正定値核

#### 離散構造上の核

文章上の string kernel [@Lodhi+2002] やグラフ上の graph kernel [@Kriege+2020] も考えられている．

[@Borgwardt+2006] は random walk kernel を提案しており，$\R^d$ へ埋め込まれるようなものの計算量は $O(n^3d)$ である．さらに効率の良いカーネルとして Weisfeiler-Lehman カーネル [@Shervashidze+2011] もある．

#### $S^1$ 値の核

$S^1\simeq\cointerval{0,2\pi}$ 上の確率分布は，方向データとして，海洋学における波の方向，気象学における風向のモデリングに応用を持つ．

全射 $\pi:\R\epi S^1$ に従って，$\R$-値の Gauss 過程を，方向データ値の Gauss 過程に押し出すことが出来る [@Jona-Lasinio+2012]．これに伴い，$\R$-値の核を $S^1$-値に押し出すこともできる．

$\pi$ による Gauss 分布の押し出し $\pi_*\rN_1(\mu,\sigma^2)$ は [wrapped normal distribution](https://en.wikipedia.org/wiki/Wrapped_normal_distribution) と呼ばれている．これに対応し，この Gauss 過程は wrapped Gaussian process と呼ばれている [@Jona-Lasinio+2012]．

### 核の Monte Carlo 近似

#### カーネルの近似

以上，種々のカーネル関数を紹介してきたが，これらはデータに関して効率的に計算される必要がある．

特に潜在空間上での Gram 行列の逆行列または Cholesky 分解を計算する $O(n^3)$ の複雑性が難点である [@Liu+2020]．

このデータ数 $n$ に関してスケールしない点が従来カーネル法の難点とされてきたが，これはランダムなカーネル関数を用いた Monte Carlo 近似によって高速化できる．$m$ 個のランダムに選択された基底関数を用いれば，Monte Carlo 誤差を許して計算量は $O(nm+m^3)$ にまで圧縮できる．

#### Random Fourier Features

正定値核のスペクトル表現 ([-@eq-spectral-decomposition]) を通じて，核の値 $K(x,y)$ を Monte Carlo 近似をすることが出来る．

例えば $K$ が RBF 核であるとき，$p$ は正規密度になるから，Gauss 確率変数からのサンプリングを通じてこれを実現できる：
$$
K(x,y)\approx\phi(x)^\top\phi(y),\qquad \phi(x):=\sqrt{\frac{1}{D}}\vctr{\sin(Z^\top x)}{\cos(Z^\top x)},Z=(z_{ij}),z_{ij}\iidsim\rN(0,\sigma^{-2}).
$$

これは核の値 $K(x,y)$ を，逆に（ランダムに定まる）特徴ベクトル $\phi(x),\phi(y)$ の値を通じて計算しているため，Random Fourier Features [@Rahimi-Recht2007], [@Sutherland-Schneider2015]，または Random Kitchen Sinks [@Rahimi-Recht2008] と呼ばれる．

$Z$ の行を互いに直交するように取ることで，Monte Carlo 推定の精度が上がる．これを orthogonal random features [@Yu+2016] と呼ぶ．
