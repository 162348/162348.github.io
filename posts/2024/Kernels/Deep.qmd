---
title: "数学者のための深層学習（１）"
subtitle: 導入と歴史
author: "司馬 博文"
date: 2/11/2024
categories: [Kernel, Math Notes]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，深層学習の基礎と歴史を概観する．
---

{{< include ../../../_preamble.qmd >}}

## 歴史^[[@Schmidhuber2015], [@Bishop-Bishop2024] を参考．]

誤差逆伝播法をニューラルネットに使うことで，表現学習がなされることの発見 [@Rumelhart+1986] から，深層学習の分野は次の点で変化したという：

* 神経科学・生物学的なモチベーションから遊離し，より確率論的・統計学的なアプローチが主流になった [@Bishop-Bishop2024 p.19]．
* 多層のニューラルネットワークと高次元データに対する理論的な研究が加速した [@MacKay2003 p.535]．

しかし，畳み込みニューラルネットワーク (CNN) [@LeCun1998] を除いて，２層以上のニューラルネットワークの成功した応用例は殆どなかった．これは，[勾配消失](https://ja.wikipedia.org/wiki/%E5%8B%BE%E9%85%8D%E6%B6%88%E5%A4%B1%E5%95%8F%E9%A1%8C) により，どんなに多層なニューラルネットワークを構築しても，最後の２層程度しか意味のあるパラメータを学習できなかったためである．

そのために，多層のニューラルネットワークを適用したい複雑な問題に対しては，問題固有の特徴抽出技法が編み出され，一般的な解決法はなかった．

多くの研究者は，カーネル法や Gauss 過程に打開策を求めていた．

ImageNet [@Krizhevsky+2012] が大きなターニングポイントとなった．