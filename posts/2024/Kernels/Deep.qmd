---
title: "数学者のための深層学習概観"
subtitle: "歴史と導入"
author: "司馬博文"
date: 2/11/2024
date-modified: 7/29/2024
categories: [Deep, Survey, Economic Security]
image: Images/AE.png
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
abstract-title: 概要
abstract: 数学者のために，深層学習の基礎と歴史を概観する．
listing: 
    -   id: deep-listing
        type: grid
        contents:
            - "Deep2.qmd"
            - "Deep3.qmd"
            - "Deep4.qmd"
            - "../Samplers/Diffusion.qmd"
            - "../Samplers/NF.qmd"
            - "../Samplers/EBM.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
        # page-size: 3
        # filter-ui: true
        # sort-ui: true
---

::: {#deep-listing}
:::

{{< include ../../../_preamble.qmd >}}

## 歴史

深層学習とは，多層パーセプトロンを用いた機械学習の手法をいう．

<!-- 画像認識，自然言語処理，画像生成，タンパク質の構造予測など，多くの分野で成功を収めている． -->

### ニューラルネットワークの黎明

#### 脳の解明

[@McCulloch-Pitts1943] は脳の神経回路を命題論理の枠組みで扱い，ニューロンの論理素子としての機能を考察した（McCulloch-Pitts の形式ニューロンや閾素子と呼ばれる）．なお，パーセプトロンはシグモイド関数を活性化関数に用いた場合，Turing 完全である [@Siegelmann-Sontag1991]．

脳を模したモデルとして，当然学習能力をどうモデルに組み込むかが問題になる．これはシナプスの結合荷重の変化によるものだという仮設は古くからあったが，最も明確な形で表現したのが [@Hebb1949] であった．

具体的には Hebb 則は，２つの正の相関を持つ（＝共起しやすい）ニューロンの間の荷重は増強される，というものである．^[[@MacKay2003 p.506] など．] これは連想記憶のモデルとして提案された（第 [-@sec-Hopfield] 節も参照）．

#### パーセプトロン

パーセプトロンは，脳の記憶と認識のモデルとして，[@Rosenblatt1958] が心理学の学会誌に発表した．^[入力層と中間層と出力層の３層のみからなるものモデルを単純パーセプトロンと呼ぶが，それだけでなく，多層のものや，フィードバック結合のあるものも含む，一般的な形で考えていた．]

複雑過ぎるものに対する理論解析は進まなかった．

そのような中で [@Minsky-Papert1969] は線型分離不可能な問題に対しては線型単純パーセプトロンは解を見つけられないほど機能は劣り，かといって複雑なものは計算量が爆発するので，結局パーセプトロンは使い物にならないのではないかとの見方を示した [@甘利俊一1989 p.130]．

現状の深層学習の成功を見ている者からすれば，これだけのことでパーセプトロンの研究が下火になってしまったことは驚くべきことに感じる．

[@甘利俊一1989] はこの点を，次のように断罪している．

> ミンスキーらは，自分たちの立てた問題は正しく解いた．しかし問題の立て方を誤ったのである．[@甘利俊一1989 p.132]

#### 多層パーセプトロン

パーセプトロンの発明の時点で，深層学習のモデルとしてはすでに完成していた．ただ，[@Minsky-Papert1969] の指摘の通り，計算複雑性や学習アルゴリズムの問題が残り続けた．

多層のパーセプトロンでは中間層を学習させることが課題であった．これに対しては，活性化関数に可微分なものを用いて確率的勾配降下法によって学習させることで解決できること [@Amari1967] などは早い時期から議論されていた．この問題点は，局所解に囚われることであった．

多層のパーセプトロンに対して，局所解に囚われるなどの問題はあれど，極めて多くの場合で誤差逆伝播法が有効な学習法になることが広く周知されたのは，[@Rumelhart+1986], [@Rumelhart+1987] であり，これが深層学習の第二次ブームの着火剤となった．^[オートエンコーダーを導入し，中間層で表現学習がなされること（最も幅の狭い中間層の活性化を通じてコンパクトに表現する，など）と，モーメンタムが学習を加速することを示している [@Rumelhart+1987 p.330], [@Schmidhuber2015]．「バックプロパゲーションがこの流行の起爆剤となったとさえ言える」 [@甘利俊一1989 p.144]．]

すぐさま英語の発音を学習させた研究 [@Sejnowski-Rosenberg1987] の [デモ](https://youtu.be/gakJlr3GecE?si=JLNQjl2bXJomWu9z) も発表され，大きな波紋を呼び起こした．ここでも特徴的な内部表現が観察された．

局所解に囚われることが解決された訳ではない．技術的な問題点（特に，結局深層モデルの訓練はうまくいかないこと）はあれど，ニューラルネットワークは実用的に極めて簡単に使えるモデルであると知らしめたのである．

> In short, we believe that we have answered Minsky and Papert's challenge and _have_ found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced. [@Rumelhart+1987 p.361]

現在は，[ドロップアウト](../AI/BAI1_Dropout.qmd) や区分的線型な活性化関数 ReLU を用いるなどの工夫がなされている [@Goodfellow+2014]．

### 深層化の歴史^[[@Schmidhuber2015], [@Bishop-Bishop2024], [@人工知能学会2015] を参考．]

#### 深層化の障壁

誤差逆伝播法をニューラルネットに使うことで，表現学習がなされることの発見 [@Rumelhart+1986] から，深層学習の分野は次の点で変化したという：

* 神経科学・生物学的なモチベーションから遊離し，より確率論的・統計学的なアプローチが主流になった [@Bishop-Bishop2024 p.19]．
* 多層のニューラルネットワークと高次元データに対する理論的な研究が加速した [@MacKay2003 p.535]．

しかし，画像識別タスクに特化して結合構造を予め作り込むことで学習を容易にしてある畳み込みニューラルネットワーク (CNN) [@LeCun1998] などを除いて，２層以上のニューラルネットワークの成功した応用例は殆どなかった．これは，[勾配消失](https://ja.wikipedia.org/wiki/%E5%8B%BE%E9%85%8D%E6%B6%88%E5%A4%B1%E5%95%8F%E9%A1%8C) により，どんなに多層なニューラルネットワークを構築しても，最後の２層程度しか意味のあるパラメータを学習できなかったためである．

そのために，多層のニューラルネットワークでは表現学習は難しいという認識が広まり，複雑なタスクに対しては，問題固有の特徴抽出技法が編み出されるのみで，一般的な解決法はなかった．

３層のニューラルネットワークも，隠れ層の幅が無限大の極限で，任意の連続関数を近似できるという普遍近似定理という抽象的な慰め [@Hecht-Nielsen1989] があるのみであった．

そこで多くの研究者は，SVM [@Cortes-Vapnik1995] やカーネル法，Gauss 過程に現実的な打開策を求め始めたのである．これは冬の時代とも呼ばれる．

#### ネオコグニトロンと CNN {#sec-CNN0}

[@Hubel-Wiesel1959] は猫の視覚野には，単純細胞と複雑細胞の２種類の細胞があることを発見した．また，生後すぐの猫を一日交代で異なる片目を覆って成長した猫では，多くの視覚野の神経細胞は単眼性になる [@Hubel1967]．

これらの観察から，[@Malsburg1973] や [@Fukushima1975] は，特徴抽出細胞が自己形成するメカニズムを，自己組織化のキーワードの下で調べた．特に後者は **コグニトロン** というモデルを提案した．

[@Fukushima1980] の [ネオコグニトロン](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3) は初の深層モデルの例と言える．^[[@Schmidhuber2015 p.90] など．] これは，単純細胞層と複雑細胞層を交互に深く重ねたネットワークである．

しかし [福島](https://ja.wikipedia.org/wiki/%E7%A6%8F%E5%B3%B6%E9%82%A6%E5%BD%A6) は学習の問題を中心に扱った訳ではなかった．^[教師なしの競合学習を試みたのみであった [@人工知能学会2015 p.21]．] [@LeCun1998] の畳み込みニューラルネットワーク LeNet は，ネオコグニトロンを誤差逆伝播法により教師あり学習させたものと言える．

このモデルは後に AlexNet としてダントツの性能で世界を驚かせることになる．

画像データはピクセルを節としたグラフデータとみなすこともでき，CNN を一般化する形で GNN (Graph Neural Network) も提案されている [@Zhou+2020], [@Wu+2021], [@Velickovic2023]．

#### 自己符号化器 {#sec-AE}

[@Cottrell-Munro1988] は中間層の幅を小さくし，入力信号自身を教師信号として誤差逆伝播法により学習することで，隠れ層に入力の低次元表現が学習され，主成分分析に用いることが出来ることを報告した．

これは入力層と出力層の素子数を一致させ，出力が入力に近づくように訓練される．このようなモデルを **自己符号化器** (autoencoder) または 自己連想ネットワーク (auto-associative neural network) と呼ぶ．

内部表現を獲得するとされる中間層に対して，それより前半の層全体を符号化器，それ以降を復号化器と呼ぶ．^[三層の場合は，入力層を符号化器，出力層を復号化器とも言う．さらに中間層の幅が小さい場合，その形から hourgalss-type neural network とも呼ばれる [@人工知能学会2015 p.91]．]

[@Baldi-Hornik1989] は活性化関数がない３層の場合，自己符号化器を訓練させることは主成分分析を実行することに等価であることを示した．

さらに層を増やすことで，非線型な次元圧縮が可能であることが [@DeMers-Cottrell1992] で示された．この論文では，データの空間内の（非線型な）部分多様体の局所座標が学習されたり，人間の顔の写真のデータ圧縮が出来ることが実証されている．

自己符号化器はスタッキングが可能である [@Ballard1987]．このことが，次の節で述べる事前学習のアイデアに繋がる．

#### 自己符号化器を用いた事前学習 {#sec-pretraining-using-AE}

自己符号化器自体を符号圧縮と表現学習に用いることは，他の手法と比べて特別優れているという訳でもなかった．

しかし，深層ニューラルネットワークを層ごとに事前に教師なし学習をすることで，後の教師あり学習において勾配消失などの問題が回避できるというアイデア [@Hinton+2006Science], [@Hinton+2006] は画期的であり，**深層学習** (Deep Learning) という言葉が広まったのもこの頃であるという [@Schmidhuber2015 p.96]．

[@Bengio+2006] もこれを拡張し，この事前学習によって，素性の良い局所解の近くにパラメータの初期値が調整されるということを実験的に示している．

#### AlexNet

ImageNet データベース [@Deng+2009] を用いた判別コンテスト ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) で，ダントツで優勝した AlexNet [@Krizhevsky+2012] が大きなターニングポイントとなった．

これも [@LeCun1998] の CNN を基にした８層のモデルで，活性化関数には ReLU とドロップアウトによる正則化が用いられていた．学習も，NVIDIA 社の GPU を用いていた．

#### ResNet {#sec-ResNet}

20 層以上の多層ニューラルネットワークの学習の困難さを初めて解決したのが [@He+2016] の Residual Network (ResNet) であった．^[当時の CNN で深かったものには，19 層の CNN である VGGNet [@Simonyan-Zisserman2015] がある．]

このモデルは 152 層もあったが効率的に訓練することが可能で，2015 年の ILSVRC でダントツで優勝し，しかも初めて人間の誤答率 5% [@Dodge-Karam2017] を下回ったのである．

そのアイデアは，各層の入力 $x$ を出力に再度加算した形
$$
y=x+F(x)
$$
で各層をデザインし，現状の入力からの差分 $F$ のみを学習するとすることで勾配消失を回避する，というものであった．実際，この層の微分は
$$
\dd{y}{x}=1+F'(x)
$$
と表せる．

このテクニックは [トランスフォーマー](Deep2.qmd) [@Vaswani+2017] などのモデルでも用いられている．

[@Li+2018] によると，残差レイヤーの追加は誤差関数を滑らかにし，近しい入力に対しても勾配が非常に大きくなってしまうことが効率的な学習を阻害してしまう問題 (shattered gradient problem) [@Balduzzi+2017] を解決している，という．

## アーキテクチャ

### 導入

ニューラルネットワークは，内部に循環を持つかどうかで二分され，それぞれを Feedforward Network (FFN) と Feedback Network (FBN) と呼ぶ．^[[@MacKay2003 p.505]，[@Murphy2023 p.633] 16.3.1節など．]

FFN は **多層パーセプトロン** (MLP) ともいう．^[[@Murphy2023 p.632] 16.3.1節など．]

[Recurrent Neural Network](https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF) (RNN) [-@sec-RNN] や Hopfield ネットワーク [-@sec-Hopfield] は FBN の例である．

多層パーセプトロンは分類や回帰などの統計学的な用途に主に用いられるが，FBN は学習機械として以外に，生物の神経機能のモデルとしてや，組合せ最適化ソルバーとしても用いられる．^[[@MacKay2003 p.468] の導入も参照．]

<!-- ここでは特に **教師なしニューラルネットワーク** を取り上げる．^[[@MacKay2003 p.470] など．] -->

### CNN：畳み込みニューラルネットワーク

CNN はその畳み込み層に特徴付けられる画像に特化した FNN アーキテクチャである．

第 [-@sec-CNN] 節で詳しく扱う．

### AE：自己符号化器

![Example of Autoencoder from [@Murphy2023 p.635]](Images/AE.png)

自己符号化器または自己連想ニューラルネットワーク (auto-associative neural network) [-@sec-AE] は図のような砂時計型の，入力 $x$ と出力 $y$ の次元数が一致したアーキテクチャを持ち，
$$
\L(\theta)=\norm{y-x}^2_2
$$
などの入力の復元誤差を目的関数として訓練される．

### RNN：再帰的ニューラルネットワーク {#sec-RNN}

再帰的な層を持ち，自己回帰モデル [-@sec-AR] を実装する FBN の例である．

![Example of Recurrent Neural Network from [@Murphy2023 p.636]](Images/RNN.png)

第 [-@sec-RNN2] 節でも詳しく扱う．

### Transformer

RNN の最大の難点は，隠れ次元 $z_t$ が $x_{1:t}$ までの入力の要約になっており，$x_1$ などの最初の方の情報がどんどん薄れていく点にある．

そこで，時点 $t$ でも $x_1$ など以前の情報にワンステップでアクセスできるような機構である **注意機構** が考案された．これがエンコーダーのみのトランスフォーマーである．

デコーダーのみのトランスフォーマーは masked attenstion という技術を用いて $x_{1:t-1}$ のみで条件づけて $x_t$ を生成することができる．

一方で，入力全体で条件づけて，文章を生成することもでき，これが最初に [@Vaswani+2017] によって提案された，最も一般的な形の encoder-decoder トランスフォーマーである．

```{=html}
<div class="article-card-container">
    <div class="article-card">
        <a href="https://162348.github.io/posts/2024/Kernels/Deep2.html" target="_blank">
            <img src="https://162348.github.io/posts/2024/Kernels/Transformer.png" alt="Article Image" class="article-image">
            <div class="article-content">
                <h3 class="article-title">トランスフォーマー</h3>
                <p class="article-description">深層生成モデル１</p>
            </div>
        </a>
    </div>
</div>
```

### GNN：グラフニューラルネットワーク

従来の NN は辺の存在しない退化したグラフに対するものだとして，NN を一般のグラフデータに対して拡張することが近年考えられている．

```{=html}
<div class="article-card-container">
    <div class="article-card">
        <a href="https://162348.github.io/posts/2024/Kernels/GNN.html" target="_blank">
            <img src="https://raw.githubusercontent.com/162348/162348.github.io/main/thumbnail.svg" alt="Article Image" class="article-image">
            <div class="article-content">
                <h3 class="article-title">グラフニューラルネットワーク</h3>
                <p class="article-description"></p>
            </div>
        </a>
    </div>
</div>
```

### 非有向ネットワーク

#### Hopfield ネットワークとスピングラス {#sec-Hopfield}

計算機の記憶と生物の記憶の相違点のうち，大きなものには連想性 (associativity) がある．アドレスで整理されているのではなく，内容で整理されているのである．1970 年代には，神経の連想記憶機能のモデルとしてのニューラルネットワークが多数提案された．

それには，ここまで議論してきた階層型のネットワークと異なり，ノード同士は **相互結合** しているものも含まれる．連想記憶のモデルとしては，特に相互結合で，どちらの方向に関する重みも同じであるもの（**対称結合** ネットワーク）が多く，全結合の FBN である Hopfield network はその代表例である．^[[@MacKay2003 p.503] 第42章，[@人工知能学会2015 p.11] など．]

（連続変数の）Hopfield network の学習則は，Hebb 則 [@Hebb1949] に基づいて，各ニューロン $x_j\in[0,1]$ についてその入力
$$
a_i:=\sum_{j}w_{ij}x_j
$$
を計算し，$x_i\gets\tanh(a_i)$ と更新する．

実はこの学習則は必ず収束する．このことを，[@Hopfield1982] はスピングラスのモデルと関連付けて示したことから，特に統計物理学の文脈で Hopfield network の名前がついた．^[「このため，物理学者はこの種のモデルのことを Hopfield モデルと呼ぶが，この命名は適切とは思えない」[@甘利俊一1989 p.97] としている．]

> 神経回路網の解析，とくに連想記憶モデルの解析が，スピングラスを解析する方法を用いて実行できるのではないかという考えが出てきて，大量の物理学者が神経回路網に注目しだした．こうしたアイデアの火付け役がホップフィールドと言われる．[@甘利俊一1989 p.105]

対称結合のニューラルネットワークの学習則は，スピングラスと同様に，必ずポテンシャル関数が減少する方に動作するというのである．この連関を利用して，[@Hopfield-Tank1985] は Hopfield ネットワークをアナログ回路に実装し，巡回セールスマン問題を解くという，最適化問題ソルバーとして利用してみせた．

単体 Hopfield Network [@Burns-Fukai2023] という拡張もあり，パラメータ数は変わらずとも記憶容量が増える．

#### ボルツマンマシン [@Ackley+1985]

ボルツマンマシンは確率的 Hopfield ネットワークともいい，Hopfield ネットワークが統計物理のモデルとして近似するところの Gibbs 分布を，実際に持つ [Markov 確率場](../Computation/PGM2.qmd) の一種である．^["The popularity of the Boltzmann machine was primarily driven by its similarity to an activation model for neurons." [@Koller-Friedman2009 p.126]．]

これは，荷重を，確率 $\frac{1}{1+e^{-2a_i}}$ で $x_i=1$，そうでない場合は $x_i=-1$ と定めることで得られる．

#### 深層ボルツマンマシン [@Salakhutdinov-Hinton2009]

#### 深層信念ネットワーク [@Hinton+2006]

これは深層学習研究の皮切りになった確率的深層モデルである．

### Spiking Neural Network

実際の神経細胞は **発火** という離散的なイベントを発生させ，その頻度やタイミングも大きな役割を持っている．これを取り入れたモデルを Spiking Neural Network (SNN) [@Maass1997] と呼ぶ [@岡島義憲2020]．

SNN は現状の人工ニューラルネット (ANN: Artificial Neural Network) よりも，半導体上での計算を効率化することが出来る．そこで，SNN による深層学習が近年試みられている [@Tavanaei+2019]．

Microsoft の BitNet [@Wang+2023-BitNet] も計算効率性を目指すにあたってそのアイデアを等しくする．

### ベイズからの見方

#### ネットワークの正則化と汎化性能

良い汎化性能を得るためには，[偏倚と分散のトレードオフ](https://ja.wikipedia.org/wiki/%E5%81%8F%E3%82%8A%E3%81%A8%E5%88%86%E6%95%A3) を乗り越える必要がある．

データセットに対してモデルの自由度が高すぎると，分散は小さくなれど，過学習を起こしてしまう．すなわち，大きなバイアスが導入され，汎化性能が悪くなる．一方で，モデルの自由度が低すぎると，平均的には正しい予測ができても，分散が大きくて役に立たない．

大規模なデータセットを用いることは一つの解決である．自由度の高いモデルを用いても過学習が起こりにくくなるためである．^[[@Bishop-Bishop2024 p.254] など．]

#### 帰納バイアス

正則化とは，モデルの自由度を制限することで過学習を抑制することである．これは，「正しいモデルは十分に滑らかであるはずである」という帰納バイアスを導入することで，モデルの汎化性能を改善させていることに等しい．^[[@Bishop-Bishop2024 p.255] など．]

転移学習も一種の帰納バイアスの注入だと見れる．２つの異なるタスクの間に類似性が存在するという事前知識を注入することで，汎化性能を改善する手法だと思えるのである．

汎化性能の高いモデルを作るということは，人類が解きたいタスクに普遍的に共通する特徴を捉え，これを帰納バイアスの形でモデルに注入することに等しい．[No free lunch theorem](https://ja.wikipedia.org/wiki/%E3%83%8E%E3%83%BC%E3%83%95%E3%83%AA%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%81%E5%AE%9A%E7%90%86) [@Wolpert1996] から，^[[@Wolpert1996] は最適化の文脈での定理を示した．統計的機械学習の文脈での No free lunch theorem は，["Any classifier with finite sample error guarantees necessarily needs inductive bias: structural assumptions on either the function class or the sampling distribution."](https://twitter.com/aryehazan/status/1609423557796450304) と説明できる．[@Shalev-Shwartz-Ben-David2014 p.37] 定理5.1 も参照．] 一般に特定のタスクに対する性能向上は，他のタスクに対する性能低下を伴うものであることが予想されるが，例えば推定関数が十分滑らかであるというのは，人間の認識特性上，有意義な結果にはほとんど普遍的に必要な条件である．

#### ベイズ深層学習の美点

[@Gal-Ghahramani2016] などでは，

* データの数が少なくとも，有効なパラメータ推定が可能である．
* 過学習が起こりにくい．
* 不確実性の定量化が自動でなされる．

と説明される．

#### ベイズ深層学習の例

CNN は特に過学習しやすい上にサンプル効率性が悪い．その場合には，変分近似による Bayesian CNN は既存法と同等の性能を誇る [@Gal-Ghahramani2016]．

## 自己回帰モデル {#sec-AR}

### 導入

長さ $T$ のデータベクトルをモデリングするとき，
$$
p(x_{1:T})=\prod_{t=1}^Tp(x_t|x_{1:t-1})
$$
という分解を用いることができる．このようなアプローチを機械学習では **自己回帰モデル** という．^[[@Murphy2023 p.811] 22章など．]

### RNN {#sec-RNN2}

しかし，このようなモデリング法は $T$ が大きくなるにつれて，条件づける変数 $x_{1:t-1}$ が高次元になるため，モデリングが難しくなる．

かと言って，$\{x_t\}_{t=1}^T$ を Markov 連鎖とみなしてモデリングするわけにもいかない．

一つの解決法が **状態空間モデル / 隠れ Markov モデル** によるものである．$x_{1:t}$ はある状態変数 $z_t$ にある既知の確率法則で圧縮できると仮定し，これのみを持ち越す方法である．

特に，$X_{1:t-1}\to Z_t$ の対応が決定論的であるとき，これを多層パーセプトロンによってモデリングしたものを **再帰ニューラルネットワーク** という．

$x_t=f(z_{t-1},x_{t-1})$ の関係を学習した後は，$x_1$ から再帰的に $f$ に通すことでデータ $x_{1:T}$ を生成する．

### 機械学習の中で占める役割

AR モデルは尤度を効率的に扱えるため，計算や最適化が簡単である．実際，RNN や CNN などの自己回帰モデルが，歴史上最初に発達したアーキテクチャである．

一方で，データの生成が逐次的であるために生成が遅いことや，内部で潜在表現を学習するということがないために表現学習に使うことができない点が欠点と言える．^[[@Murphy2023 p.812] など．]

## CNN {#sec-CNN}

CNN はトランスフォーマーによりデータ間の関係を自動的に学習する枠組みが提案される前に，主に画像分野において，データの構造に関する事前知識をモデルに組み込んだ例として提案されたものである．

世界初の深層学習モデルによる席巻は，CNN により，画像認識の分野において達成された．

### Computer Vision という分野 {#sec-CV}

Computer Vision という問題の複雑性が，ニューラルネットワークのアーキテクチャの開発を後押しした歴史がある．

並行移動・拡大変換という2つの合同変換不変性に対して，画像の認識結果は不変であるべきである．

このような不変性，または **同変性** (equivariance) をモデルに取り入れる方法は大きく分けて４つある：

1. 誤差関数に正則化項を導入する^[tangent propagation [@Simard+1991] などがその例である．2の例とも見れる．]
2. **対称性を取り入れた潜在表現** を用いてその上で学習をする
3. 不変性を効率良く学習出来るように **データセットを拡張する**
4. 対称性を取り扱う構造をネットワークの**アーキテクチャに組み込む**

CNN は４番目のアプローチで歴史上最初に取られたものである．しかし，このどのアプローチも完全には不変性を取り入れることは出来ていないことも報告されている [@Azulay-Weiss2019]．

幾何学を種々の変換に対する不変性の研究と捉え直した Felix Klein の [Erlangen program](https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%AB%E3%83%A9%E3%83%B3%E3%82%B2%E3%83%B3%E3%83%BB%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0) に倣い，種々の深層モデルの帰納バイアスとアーキテクチャを，幾何学的な変換から導出してシステマティックに理解する試み Geometric Deep Learning [@Bronstein+2021] がある（第 [-@sec-GDL] 節）．

### 物体認識 CNN {#sec-obj-recog-CNN}

CNN は画像の特徴を，階層的に学習出来るように誘導するような構造を持っている．

多くの例では，畳み込み層とプーリング層が交互に繰り返され，最後に全結合層を持つような構造を持っている．

#### 局所的な特徴

最初の素子は，画像の局所的な一部のみを入力として取る．その範囲を **受容野** (receptive field) と呼ぶ．この素子の荷重を **フィルター** または **カーネル** という．

::: {.callout-caution icon="false" title="例：2次元の畳み込み層" collapse="true"}

2次元での幅 $2k+1$ の畳み込み層は，フィルター $\supp(\psi)\subset\{0,\pm1,\cdots,\pm k\}$ を用いて，
$$
f^{\text{out}}_{i,j}=\phi\paren{\sum_{a,b\in\bZ}\psi(i-a,j-b)f_{i,j}^{\text{in}}+\theta}
$$
と表せる．

:::

決まったカーネルに対して，この素子はカーネルの特徴にマッチした入力に対して，大きな出力を返す．

次に，このフィルターを畳み込むことで，画像内の異なる位置に存在する特徴を検出する．畳み込み層は，荷重を共有した疎結合層ということになる．

畳み込みを行うと，入力次元と出力次元が変わってしまうことがあるため，その場合は入力画像にパディングを施す．

出力次元を小さくして，畳み込み特徴写像で大きな次元削減を行いたい場合，strided convolution を用いる．

#### 並行移動不変性

畳み込みの結果が，特徴の位置の変化に対して不変になるようにする設計に，**プーリング層** または **ダウンサンプリング層** (down-sampling / sub-sampling) がある．

プーリングも，受容野を持った素子と畳み込みからなるが，畳み込みに学習されるべきパラメータはなく，確定的な関数と畳み込まれる．

代表的なプーリング関数には **最大プーリング** [@Zhou-Chellappa1988] や平均プーリング，$l^2$-プーリングなどがある．

プーリングは不変性の導入に加えて，畳み込み特徴のダウンサンプリングを行って，更なる次元削減を行う役割も果たす．

### 画像分割 CNN {#sec-image-segmentation-CNN}

画像分類では，１枚の画像に対して１つのクラスの対応づけたが，１つのピクセルに１つのクラスを対応づけることで，画像をクラスごとに分類することが考えられる．

#### Up-sampling

画像分類の問題では，最終的に全ピクセルから得た情報を１次元に圧縮することになる．一方で，十分な潜在表現を得たのちは up-sampling に転じる Encoder-Decoder 構造にすることで，最終的に元の画像サイズに戻しながら，画像分割問題を解くことが出来る [@Long+2015], [@Noh+2015], [@Badrinarayanan+2017]．

[@Badrinarayanan+2017] は max-unpooling などの up-sampling 層の設計を考慮したが，これに学習可能なパラメータを増やした transpose convolution / deconvolution も提案された．

Pooling 層を一切用いず，down-sampling も up-sampling も畳み込み層のみによって行われる場合，これを 全畳み込みネットワーク (fully convolutional network) という [@Long+2015]．

#### U-Net {#sec-U-net}

この encoder-decoder 構造は，分類に必要のない情報を自動的に削減し，モデルのサイズを小さくするのには効果的であるが，タスクによっては元の画像の情報量を保ちたい場合がある．

U-net [@Ronneberger+2015] は対応する down-sampling 層と up-sampling 層とを直接繋ぐ経路を追加することでこれを解決した．

#### Capsule Networks

プーリング層は並行移動不変性の概念を取り入れるが，回転や拡大などの変換に対する不変性を取り入れるにはデータ拡張に依るしかないのでは，CNN の学習はどうしても大規模なデータセットが必要になってしまう．

そこで，アーキテクチャによる解決を試みたのが，畳み込み層に加えて **カプセル層** を取り入れた Capsule Network [@Sabour+2017] である．

### Inpainting

[@Horita+2023]

### スタイル転移

CNN において，最初の方のレイヤーは局所的な特徴を捉えているが，後の方のレイヤーはスタイルなどの大域的な特徴を捉えている．

これを用いて，既存の画像の具体的な特徴を変えずに，他の画像からスタイルのみを転移する手法 **Neural Style Transfer** が提案された [@Gatys+2015], [@Gatys+2016]．

### 幾何学的深層学習 {#sec-GDL}

以上，種々のタスクに種々のアーキテクチャが存在することを見てきた．この状況は，19 世紀の幾何学と似ていると [@Bronstein+2021] はいう．

これらのアーキテクチャがどのような帰納バイアスを導入する役割を果たしているかを，不変性や同変性といった第一原理から理解する試みが **幾何学的深層学習** である．

> In this text, we make a modest attempt to apply the **Erlangen Programme mindset** to the domain of deep learning, with the ultimate goal of obtaining a systematisation of this field and ‘connecting the dots’. We call this geometrisation attempt ‘**Geometric Deep Learning**’, and true to the spirit of Felix Klein, propose to derive different inductive biases and network architectures implementing them from **first principles of symmetry and invariance**. [@Bronstein+2021 p.2]

化学・生物・物理はいずれも対象の対称性をしっかり扱う理論を持っている．これらの分野に深層学習が広く取り入れられつつある今，深層学習の分野も対称性を第一原理として整理される脱皮が待たれているのである．

#### 幾何学と解析学は双対である

[Mikhael Gromov](https://en.wikipedia.org/wiki/Mikhael_Gromov_(mathematician)) によると，空間 $X$ の解析学とは $X$ 上の関数の研究で，$X$ の幾何学とは $X$ への関数の研究である [@深谷賢治1997 p.11]．

[![Gromov による幾何と解析の解釈](Images/Gromov.png)](https://www.math.kyoto-u.ac.jp/~fukaya/shzuok.pdf)

同変性と共変性は，データ集合 $X$ と群作用 $\rho:G\times X\to X$ との組 $(X,\rho)$ を取り扱うから，確かに上述の定義に適っている．

#### 20 世紀の数学の方向

[@Gromov2001]

#### 共変性と同変性

::: {.callout-tip title="定義 (invariance, equivariance)^[[@福水健次2024], [@Bronstein+2021 pp.15-16] など．または [nLab](https://ncatlab.org/nlab/show/equivariant+structure) も参照．]" icon="false"}

$X,Y$ を集合，$G$ を群で $X,Y$ に左から作用するとする．^[このような集合 $X,Y$ を [$G$-集合](https://ncatlab.org/nlab/show/G-set) という．[同変性](https://ncatlab.org/nlab/show/equivariant) は $G$-集合の射と見れる．] 関数 $\varphi:X\to Y$ が

* $G$ に関して **不変** であるとは，任意の $g\in G$ と $x\in X$ について
$$
\varphi(g\cdot x)=\varphi(x)
$$
を満たすことをいう．
* $G$ に関して **同変** であるとは，任意の $g\in G$ と $x\in X$ について
$$
\varphi(g\cdot x)=g\cdot\varphi(x)
$$
を満たすことをいう．

:::

物体認識 @sec-obj-recog-CNN は不変的で，画像分割 @sec-image-segmentation-CNN は同変的な問題である．

ただし，不変性は，$G$ の $Y$ への作用が自明である場合の同変性と見れるため，同変性の方が一般的な概念であることに注意．

#### G-CNN [@Cohen-Welling2016]

一般の群変換に対して，これを帰納バイアスとして取り入れる CNN である Group CNN [@Cohen-Welling2016] が提案されており，医療画像解析での応用 [@Lafarge+2021] もなされている．

#### Steerable CNN [@Cohen-Welling2017]

Steerable CNN ではチャンネルの間での対称性を取り入れることができる．

[@Weiler+2018] により最先端の性能が発揮されている．