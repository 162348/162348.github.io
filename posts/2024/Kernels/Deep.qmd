---
title: "数学者のための深層学習１"
subtitle: 歴史と導入
author: "司馬博文"
date: 2/11/2024
date-modified: 2/14/2024
categories: [Kernel, Review]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
image: neocognitron.jpg
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，深層学習の基礎と歴史を概観する．
---

{{< include ../../../_preamble.qmd >}}

## 歴史

深層学習とは，多層パーセプトロンを用いた機械学習の手法をいう．

画像認識，自然言語処理，画像生成，タンパク質の構造予測など，多くの分野で成功を収めている．

### ニューラルネットワークとは？

#### 脳の解明

[@McCulloch-Pitts1943] は脳の神経回路を命題論理の枠組みで扱い，ニューロンの論理素子としての機能を考察した（McCulloch-Pitts の形式ニューロンや閾素子と呼ばれる）．なお，パーセプトロンはシグモイド関数を活性化関数に用いた場合，Turing 完全である [@Siegelmann-Sontag1991]．

脳を模したモデルとして，当然学習能力をどうモデルに組み込むかが問題になる．これはシナプスの結合荷重の変化によるものだという仮設は古くからあったが，最も明確な形で表現したのが [@Hebb1949] であった．

#### パーセプトロン

パーセプトロンは，脳の記憶と認識のモデルとして，[@Rosenblatt1958] が心理学の学会誌に発表した．^[入力層と中間層と出力層の３層のみからなるものモデルを単純パーセプトロンと呼ぶが，それだけでなく，多層のものや，フィードバック結合のあるものも含む，一般的な形で考えていた．]

複雑過ぎるものに対する理論解析は進まなかった．

そのような中で [@Minsky-Papert1969] は線型分離不可能な問題に対しては線型単純パーセプトロンは解を見つけられないほど機能は劣り，かといって複雑なものは計算量が爆発するので，結局パーセプトロンは使い物にならないのではないかとの見方を示した [@甘利俊一1989 p.130]．

現状の深層学習の成功を見ている者からすれば，これだけのことでパーセプトロンの研究が下火になってしまったことは驚くべきことに感じる．

[@甘利俊一1989] はこの点を，次のように断罪している．

> ミンスキーらは，自分たちの立てた問題は正しく解いた．しかし問題の立て方を誤ったのである．[@甘利俊一1989 p.132]

#### 多層パーセプトロン

パーセプトロンの発明の時点で，深層学習のモデルとしてはすでに完成していた．ただ，[@Minsky-Papert1969] の指摘の通り，計算複雑性や学習アルゴリズムの問題が残り続けた．

多層のパーセプトロンでは中間層を学習させることが課題であった．これに対しては，活性化関数に可微分なものを用いて確率的勾配降下法によって学習させることで解決できること [@Amari1967] などは早い時期から議論されていた．この問題点は，局所解に囚われることであった．

多層のパーセプトロンに対して，局所解に囚われるなどの問題はあれど，極めて多くの場合で誤差逆伝播法が有効な学習法になることが広く周知されたのは，[@Rumelhart+1986], [@Rumelhart+1987] であり，これが深層学習の第二次ブームの着火剤となった．^[オートエンコーダーを導入し，中間層で表現学習がなされること（最も幅の狭い中間層の活性化を通じてコンパクトに表現する，など）と，モーメンタムが学習を加速することを示している [@Rumelhart+1987 p.330], [@Schmidhuber2015]．「バックプロパゲーションがこの流行の起爆剤となったとさえ言える」 [@甘利俊一1989 p.144]．]

すぐさま英語の発音を学習させた研究 [@Sejnowski-Rosenberg1987] の [デモ](https://youtu.be/gakJlr3GecE?si=JLNQjl2bXJomWu9z) も発表され，大きな波紋を呼び起こした．ここでも特徴的な内部表現が観察された．

局所解に囚われることが解決された訳ではない．技術的な問題点（特に，結局深層モデルの訓練はうまくいかないこと）はあれど，ニューラルネットワークは実用的に極めて簡単に使えるモデルであると知らしめたのである．

> In short, we believe that we have answered Minsky and Papert's challenge and _have_ found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced. [@Rumelhart+1987 p.361]

現在は，[ドロップアウト](../AI/Dropout.qmd) や区分的線型な活性化関数 ReLU を用いるなどの工夫がなされている [@Goodfellow+2014]．

### 深層化の歴史^[[@Schmidhuber2015], [@Bishop-Bishop2024], [@人工知能学会2015] を参考．]

#### 深層化の障壁

誤差逆伝播法をニューラルネットに使うことで，表現学習がなされることの発見 [@Rumelhart+1986] から，深層学習の分野は次の点で変化したという：

* 神経科学・生物学的なモチベーションから遊離し，より確率論的・統計学的なアプローチが主流になった [@Bishop-Bishop2024 p.19]．
* 多層のニューラルネットワークと高次元データに対する理論的な研究が加速した [@MacKay2003 p.535]．

しかし，画像識別タスクに特化して結合構造を予め作り込むことで学習を容易にしてある畳み込みニューラルネットワーク (CNN) [@LeCun1998] などを除いて，２層以上のニューラルネットワークの成功した応用例は殆どなかった．これは，[勾配消失](https://ja.wikipedia.org/wiki/%E5%8B%BE%E9%85%8D%E6%B6%88%E5%A4%B1%E5%95%8F%E9%A1%8C) により，どんなに多層なニューラルネットワークを構築しても，最後の２層程度しか意味のあるパラメータを学習できなかったためである．

そのために，多層のニューラルネットワークでは表現学習は難しいという認識が広まり，複雑なタスクに対しては，問題固有の特徴抽出技法が編み出されるのみで，一般的な解決法はなかった．

３層のニューラルネットワークも，隠れ層の幅が無限大の極限で，任意の連続関数を近似できるという普遍近似定理という抽象的な慰め [@Hecht-Nielsen1989] があるのみであった．

そこで多くの研究者は，SVM [@Cortes-Vapnik1995] やカーネル法，Gauss 過程に現実的な打開策を求め始めたのである．これは冬の時代とも呼ばれる．

#### ネオコグニトロンと CNN {#sec-CNN}

[@Hubel-Wiesel1959] は猫の視覚野には，単純細胞と複雑細胞の２種類の細胞があることを発見した．また，生後すぐの猫を一日交代で異なる片目を覆って成長した猫では，多くの視覚野の神経細胞は単眼性になる [@Hubel1967]．

これらの観察から，[@Malsburg1973] や [@Fukushima1975] は，特徴抽出細胞が自己形成するメカニズムを，自己組織化のキーワードの下で調べた．特に後者は **コグニトロン** というモデルを提案した．

[@Fukushima1980] の [ネオコグニトロン](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3) は初の深層モデルの例と言える．^[[@Schmidhuber2015 p.90] など．] これは，単純細胞層と複雑細胞層を交互に深く重ねたネットワークである．

しかし [福島](https://ja.wikipedia.org/wiki/%E7%A6%8F%E5%B3%B6%E9%82%A6%E5%BD%A6) は学習の問題を中心に扱った訳ではなかった．^[教師なしの競合学習を試みたのみであった [@人工知能学会2015 p.21]．] [@LeCun1998] の畳み込みニューラルネットワーク LeNet は，ネオコグニトロンを誤差逆伝播法により教師あり学習させたものと言える．

このモデルは後に AlexNet としてダントツの性能で世界を驚かせることになる．

画像データはピクセルを節としたグラフデータとみなすこともでき，CNN を一般化する形で GNN (Graph Neural Network) も提案されている [@Zhou+2020], [@Wu+2021], [@Velickovic2023]．

#### 自己符号化器

[@Cottrell-Munro1988] は中間層の幅を小さくし，入力信号自身を教師信号として誤差逆伝播法により学習することで，隠れ層に入力の低次元表現が学習され，主成分分析に用いることが出来ることを報告した．

これは入力層と出力層の素子数を一致させ，出力が入力に近づくように訓練される．このようなモデルを **自己符号化器** (autoencoder) または 自己連想ネットワーク (auto-associative neural network) と呼ぶ．

内部表現を獲得するとされる中間層に対して，それより前半の層全体を符号化器，それ以降を複合化器と呼ぶ．^[三層の場合は，入力層を符号化器，出力層を複合化器とも言う．さらに中間層の幅が小さい場合，その形から hourgalss-type neural network とも呼ばれる [@人工知能学会2015 p.91]．]

[@Baldi-Hornik1989] は活性化関数がない３層の場合，自己符号化器を訓練させることは主成分分析を実行することに等価であることを示した．

さらに層を増やすことで，非線型な次元圧縮が可能であることが [@DeMers-Cottrell1992] で示された．この論文では，データの空間内の（非線型な）部分多様体の局所座標が学習されたり，人間の顔の写真のデータ圧縮が出来ることが実証されている．

自己符号化器はスタッキングが可能である [@Ballard1987]．このことが，次の節で述べる事前学習のアイデアに繋がる．

#### 自己符号化器を用いた事前学習 {#sec-pretraining-using-AE}

自己符号化器自体を符号圧縮に用いることは，他の手法と比べて特別優れているという訳でもなかった．

しかし，深層ニューラルネットワークを層ごとに事前に教師なし学習をすることで，後の教師あり学習において勾配消失などの問題が回避できるというアイデア [@Hinton+2006], [@Hinton+2006] は画期的であり，**深層学習** (Deep Learning) という言葉が広まったのもこの頃であるという [@Schmidhuber2015 p.96]．

[@Bengio+2006] もこれを拡張し，この事前学習によって，素性の良い局所解の近くにパラメータの初期値が調整されるということを実験的に示している．

#### AlexNet

ImageNet データベースを用いた判別コンテスト ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) で，ダントツで優勝した AlexNet [@Krizhevsky+2012] が大きなターニングポイントとなった．

これも [@LeCun1998] の CNN を基にした８層のモデルであった．学習も，NVIDIA 社の GPU を用いていた．

#### ResNet {#sec-ResNet}

多層ニューラルネットワークの学習の困難さを回避したモデルが [@He+2016] の Residual Network (ResNet) である．

このモデルは 152 層でも効率的に訓練され，2015 年の ILSVRC で優勝し，しかも初めて人間の誤答率を下回った．

そのアイデアは，各層の出力を入力に加算し，現状の入力からの差分のみを学習するとすることで勾配消失を回避する，というものであった．

このテクニックは [トランスフォーマー](Deep2.qmd) [@Vaswani+2017] などのモデルでも用いられている．

## 導入

ニューラルネットワークは，内部に循環を持つかどうかで二分され，それぞれを Feed-Forward Network (FFN) と [Recurrent Neural Network](https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF) (RNN) と呼ぶ．

### Hopfield ネットワーク

1970 年代に連想記憶のモデルが多数提案された．

その中でも，[@Hopfield1982] はスピングラスのモデルと関連付けたため，特に統計物理学の文脈では Hopfield network の名前がついた．^[「このため，物理学者はこの種のモデルのことを Hopfield モデルと呼ぶが，この命名は適切とは思えない」[@甘利俊一1989 p.97] としている．]

> 神経回路網の解析，とくに連想記憶モデルの解析が，スピングラスを解析する方法を用いて実行できるのではないかという考えが出てきて，大量の物理学者が神経回路網に注目しだした．こうしたアイデアの火付け役がホップフィールドと言われる．[@甘利俊一1989 p.105]

対称結合のニューラルネットワークは，スピングラスと同様に，ポテンシャル関数が減少する方に動作するというのである．

この連関を利用して，[@Hopfield-Tank1985] はニューラルネットワークをアナログ回路に実装し，巡回セールスマン問題を解いてみせた．

## CNN

### U-Net {#sec-U-net}

[@Ronneberger+2015]

