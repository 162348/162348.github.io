---
title: "数学者のための深層学習（３）"
subtitle: 生成モデル GAN
author: "司馬博文"
date: 2/11/2024
date-modified: 2/13/2024
categories: [Kernel, Math Notes]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，深層生成モデルの先駆けである GAN を概観する．
---

{{< include ../../../_preamble.qmd >}}

# GAN [@Goodfellow+2014]

![Samples from a GAN Taken from [@Goodfellow+2014 p.7] Figure 2](GAN.png)

## 導入

GAN 以前の深層生成モデルは，学習の難しさから，データ生成分布にパラメトリックな仮定をおき，その中で **最尤推定** を行うことが一般的であった．深層 Boltzmann マシン [@Salakhutdinov-Hinton2009] もその例である．

複雑なモデルで尤度を解析的に計算することは困難である．そのために，MCMC によるサンプリングによりこれを回避することを考え，その Markov 連鎖の遷移核を学習するという生成確率的ネットワーク (GSN: Generative Stochastic Network) などのアプローチ [@Bengio+2014] も提案されていた．

GAN (Generative Adversarial Network) は，このような中で [@Goodfellow+2014] によって提案された深層生成モデルである．GAN も尤度の評価を必要としないが，MCMC などのサンプリング手法も用いず，ただ誤差逆伝播法のみによって学習が可能である．

同時の深層学習は，ImageNet コンペティションにおいて大成功を収めた AlexNet [@Krizhevsky+2012] など，主に識別のタスクにおいて大きな成功を収めていたが，生成モデルにおいては芳しくなかった．

主な障壁は

1. 分布の近似が難しいこと
2. 区分的線型な活性化関数を用いても勾配を通じた学習が難しいこと

の２点であったが，GAN はこの２つの問題を回避すべく提案された．

生成モデル $G$ に対して，判別モデル $D$ を対置し，加えて $(G,D)$ をセットで誤差逆伝播法とドロップアウト法 [@Hinton+2012]（当時深層識別モデルを最も成功させていた学習法）により学習可能にしたのである．

## 枠組み

データの空間を $x\in\cX$ とし，潜在変数の値域 $\cZ$ とその上の確率測度 $P_z\in\cP(\cZ)$，そして深層ニューラルネットワークのパラメータ空間 $\Theta_g$ を用意して，生成モデルを写像 $G:\cZ\times\Theta_g\to\cX$ とする．

生成モデル $G$ は押し出しによりモデル $\{G(-,\theta_g)_*P_z\}_{\theta_g\in\Theta_g}$ を定める．

このモデルの密度（尤度）の評価を回避するために，これに判別モデル $D$ を対置する．これは，パラメータ $\theta_d\in\Theta_d$ を通じて学習される写像 $D:\cX\times\Theta_d\to[0,1]$ とし，あるデータ $x\in\cX$ を観測した際に，これが $G$ から生成されたものではなく，実際の訓練データである確率を $D(x)$ によって近似することを目指す．

この組 $(G,D)$ に対して，
$$
V(D,G):=\E[\log D(X)]+\E[\log(1-D(G(Z))]
$$
$$
X\sim P_{\text{data}},\quad Z\sim P_z
$$
を目的関数とし，
$$
\min_{G\in\Hom_\Mark(\cZ\times\cG_g,\cX)}\max_{D\in\L(\cX;[0,1])}V(D,G)
$$ {#eq-GAN}
を解く，ミニマックスゲームを考える．^[この基準にしたがって学習すると，$G$ が外れすぎている際，$\log(1-D(G(z)))$ が殆ど $0$ になり得る．そのような場合は，$\log D(G(z))$ の最大化を代わりに考えることで，学習が進むことがある [@Goodfellow+2014 p.3]．]

## 理論

$G$ と $D$ が表現するモデルが十分に大きいとき，すなわち $\Theta_g,\Theta_d$ が十分に大きく，殆どノンパラメトリックモデルであるとみなせる場合には，学習基準 @eq-GAN は真の生成分布 $P_{\text{data}}$ に収束するアルゴリズムを与える．

このことを示すには，$P_{\text{data}}$ が，@eq-GAN の大域的最適解であることを示せば良い．

::: {.callout-tip icon="false" title="定義 (Jensen-Shannon divergence)"}
確率測度 $P,Q\in\cP(\cX)$ に対して，

1. 
$$
\KL(P,Q):=\begin{cases}
\int_\cX\log\paren{\dd{P}{Q}}\,dP&P\ll Q,\\
\infty&\otherwise.
\end{cases}
$$
を [**Kullback-Leibler 乖離度**](https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%AB%E3%83%90%E3%83%83%E3%82%AF%E3%83%BB%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%BC%E6%83%85%E5%A0%B1%E9%87%8F) という．
2. 
$$
\JS(P,Q):=\KL\paren{P,\frac{P+Q}{2}}+\KL\paren{Q,\frac{P+Q}{2}}
$$
を [**Jensen-Shannon 乖離度**](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) という．

このとき，$\sqrt{\JS}$ は，任意の $\sigma$-有限測度 $\mu\in\cM(\cX)$ に関して，
$$
\cP_\mu(\cX):=\Brace{P\in\cP(\cX)\mid P\ll\mu}
$$
上に距離を定める．
:::

::: {.callout-note icon="false" collapse="true" title="証明"}
KL 乖離度は $P\ne Q\Rightarrow\KL(P,Q)>0$ を満たすが，対称性も三角不等式も満たさない．そもそも，$\R_+$-値とは限らず，$\infty$ を取り得る．

JS 乖離度は，
$$
P\ll\frac{P+Q}{2}
$$
であるから，$\cP(\cX)^2$ 上で常に $\R_+$-値であることに注意．

以降，$\sqrt{\JS}$ が距離であることを示す．

1. $P=Q$ のとき $\JS(P,Q)=0$ であり，$P\ne Q$ のとき，
$$
P\ne\frac{P+Q}{2}
$$
であるから，$\JS(P,Q)>0$ である．
2. 対称性も直ちに従う．
3. あとは三角不等式を示せば良いが，任意の $P,Q\in\cP_\mu(\cX)$ に関して，密度を
$$
p:=\dd{P}{\mu},\quad q:=\dd{Q}{\mu}
$$
と表すと，
$$
\sqrt{\JS(P,Q)}=\Norm{\sqrt{L(p,q)}}_{L^2(\mu)}
$$
であることより，次の補題と $\norm{-}_{L^2(\mu)}$ の三角不等式より従う．
:::

::: {.callout-tip icon="false" title="補題 [@Endres-Schindelin2003 p.1859]"}

非負実数 $p,q\in\R_+$ について，
$$
L(p,q):=p\log\frac{2p}{p+q}+q\log\frac{2q}{p+q}
$$
で定まる関数 $L:\R_+^2\to\R_+$ は，任意の $r\in\R_+$ について，
$$
\sqrt{L(p,q)}\le\sqrt{L(p,r)}+\sqrt{L(r,q)}
$$
を満たす．
:::

::: {.callout-note icon="false" collapse="true" title="証明"}

右辺を
$$
f(p,q,r):=\sqrt{L(p,r)}+\sqrt{L(r,q)}
$$
とおいて，$r$ に関する偏導関数の符号変化を調べる．
$$
\begin{align*}
    \pp{f}{r}&=\frac{1}{2\sqrt{L(p,r)}}\pp{L}{r}(p,r)+\frac{1}{2\sqrt{L(r,q)}}\pp{L}{r}(r,q)\\
    &=\frac{\log\frac{2r}{p+r}}{2\sqrt{L(p,r)}}+\frac{\log\frac{2r}{r+q}}{2\sqrt{L(r,q)}}\\
    &=\frac{1}{\sqrt{r}}\paren{\frac{\log\frac{2}{x+1}}{2\sqrt{L(x,1)}}+\frac{\log\frac{2}{\beta x+1}}{2\sqrt{L(\beta x,1)}}}.
\end{align*}
$$ {#eq-1}
ここで，$x:=\frac{p}{r},\beta x=\frac{q}{r}$ とおいた．
$$
p<q\quad\Leftrightarrow\quad\beta>1
$$
と仮定しても一般性は失われない．

そこで，$x\in(-1,\infty)\setminus\{1\}$ の関数
$$
\begin{align*}
    g(x)&:=\frac{\log\frac{2}{x+1}}{\sqrt{L(x,1)}}\\
    &=\frac{\log\frac{2}{x+1}}{\sqrt{x\log\frac{2x}{x+1}+\log\frac{2}{x+1}}}
\end{align*}
$$
の性質を調べる．

実は $g'>0\;\on \R_+\setminus\{1\}$ であり，
$$
\lim_{x\to0+}g(x)=\sqrt{\log 2}>0
$$
$$
\lim_{x\to\infty}g(x)=0
$$
$$
\lim_{x\to1\mp}g(x)=\pm1
$$
と併せると，$g((0,1))\subset(0,1)$，$g((1,\infty))\subset(-1,0)$ である．特に $\abs{g}<1$．

```{python}
#| output: false
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

# 関数の定義
def func(x):
    numerator = np.log(2 / (x + 1))
    denominator = np.sqrt(x * np.log(2 * x / (x + 1)) + np.log(2 / (x + 1)))
    return numerator / denominator

# xの範囲を定義
x0 = np.linspace(-0.95, -0.05, 100)
x1 = np.linspace(0.01, 10, 1000)
x2 = np.linspace(10, 100, 100)

# xの配列を結合
x = np.concatenate((x0, x1, x2))

# yの値を計算
y = func(x)
```

```{python}
#| echo: false
# プロット
plt.plot(x, y)
plt.title('Plot of the function g')
plt.grid(False)
plt.show()
```

これより， @eq-1 は $x=1,\beta$ と，その間で１回の計３回符号変化し，$x\to\infty$ の極限では負である．

よって，$f$ は $r$ の関数として，$r=p$ で極小値，$r\in(p,q)$ のどこかで極大値を取り，$r=q$ で再び極小値を取る．
$$
f(p,q,p)=f(p,q,q)=\sqrt{L(p,q)}
$$
であるから，結論を得る．

:::

::: {.callout-tip icon="false" title="命題"}
$P_0,P_1\in\cP(\cX)$ を確率測度で，それぞれ密度 $p_0,p_1$ を持つとする．$X_0\sim P_0,X_1\sim P_1$ とする．このとき，


1. 最大化問題
$$
L:=\sup_{D\in\L(\cX;[0,1])}\Paren{\E[\log D(X_0)]+\E[\log(1-D(X_1))]}
$$
はただ一つの解
$$
D^*(x)=\frac{p_0(x)}{p_0(x)+p_1(x)}
$$
を持つ．

2. $\JS(P_0,P_1)$ は $L$ と定数の差を除いて一致する．
:::

::: {.callout-note icon="false" collapse="true" title="証明"}
1. 目的関数は
$$
\begin{align*}
    &\E[\log D(X_0)]+\E[\log(1-D(X_1))]\\
    =&\int_\cX\log D\cdot p_0\,d\mu+\int_\cX\log(1-D)\cdot p_1\,d\mu\\
    =&\int_\cX\Paren{p_0\log D+p_1\log(1-D)}\,d\mu
\end{align*}
$$
と変形できる．いま，任意の $a,b\in\ocinterval{0,1}$ に関して，
$$
f(t):=a\log t+b\log(1-t)\quad(t\in(0,1))
$$
は $t=\frac{a}{a+b}$ 上で最大値を取る．$a,b$ のどちらか一方のみが $0$ である場合も含めてこの主張は成り立つ．よって，
$$
D(x)=\frac{p_0(x)}{p_0(x)+p_1(x)}
$$
が目的関数を最大化することが判る．
2. １より，$L$ の上限 $\sup$ は達成されることがわかった：
$$
\begin{align*}
    L&=\E[\log D^*(X_0)]+\E[\log(1-D^*(X_1))]\\
    &=\int_\cX\paren{p_0\log\frac{p_0}{p_0+p_1}+p_1\log\frac{p_1}{p_0+p_1}}\,d\mu\\
    &=\int_\cX\Paren{p_0\log\frac{2p_0}{p_0+p_1}+p_1\log\frac{2p_1}{p_0+p_1}-p_0\log2-p_1\log2}\,d\mu\\
    &=-2\log2+\JS(P_0,P_1).
\end{align*}
$$
:::

これより，訓練基準 @eq-GAN はただ一つの大域的な最適解を持ち，これは $P_{\text{data}}=G_*P_z$ かつ $D^*=\frac{1}{2}$ のときに最小値 $-2\log2$ を取るということが判る．

## アルゴリズムとその収束

組 $(G,D)$ を勾配降下法により同時に学習するには，

1. 判別器 $D$ の最大化ステップ
   1. ミニバッチ $\{z^i\}_{i=1}^m$ と $\{x^i\}_{i=1}^m$ をそれぞれ $P_z$ と $P_{\text{data}}$ からサンプリングする．
   2. 確率的勾配
$$
D_{\theta_d}\frac{1}{m}\sum_{i=1}^m\paren{\log D(x^i)+\log(1-D(G(z^i)))}
$$
    の増加方向にパラメータ $\theta_d$ を更新する．
2. 生成モデル $G$ の最小化ステップ
   1. ミニパッチ $\{z^i\}_{i=1}^m$ を $P_z$ からサンプリングする．
   2. 確率的勾配
$$
D_{\theta_g}\sum_{i=1}^m\log\Paren{1-D(G(z^i))}
$$
    の減少方向にパラメータ $\theta_g$ を更新する．

というアルゴリズムを実行すれば良い．[@Goodfellow+2014 p.] の数値実験ではモーメンタム法 [@Rumelhart+1987 p.330] が用いられている．

::: {.callout-tip icon="false" title="命題 [@Goodfellow+2014 p.5]"}

このアルゴリズムは，次の３条件が成り立つならば，$G_*P_z$ は $P_{\text{data}}$ に収束する：

1. モデル $G,D$ の表現力が十分大きい．
2. 判別器 $D$ の最大化ステップにおいて，必ず $\max_{D\in\L(\cX;[0,1])}V(D,G)$ が達成される．
3. 生成モデル $G$ の最大化ステップにおいても，必ず $V(D,G)$ が改善される．
:::

実際は，$G$ はパラメトリックモデル $\{G_*P_z(\theta,-)\}_{\theta\in\Theta_g}$ であるから，その分の誤差は残ることになる．

また，$D$ が最適化されていない状況で $G$ が学習されすぎると，多くの $z\in\cZ$ の値を $D$ が不得意な判別点 $x\in\cX$ に対応させすぎてしまうことがあり得る．

$P_{\text{data}}$ が強い多峰性を持つ場合でも効率よく学習することができる．これは同じ確率分布からのサンプリング手法として，MCMC にはない美点になり得る [@Goodfellow+2014 p.6]．

## 補遺：Jensen-Shannon 乖離度のその他の性質

### 情報理論からの導入

乖離度としての Jensen-Shannon 乖離度は [@Lin1991] で最初に導入されたようである．

が，その以前から，
$$
\JS(P,Q)=2H\paren{\frac{P+Q}{2}}-H(P)-H(Q)
$$
という関係を通じて，[@Rao1982 p.25] などは右辺を Jensen 差分 (difference) と呼んでいたようである．[@Rao1987 p.222] は，$H$ が Shannon のエントロピーではなくとも，有用な性質を持つことを情報幾何学の立場から議論している．

### JS 乖離度が定める距離

$$
\Paren{\JS(P,Q)}^\al
$$
が $\al=\frac{1}{2}$ において距離をなすことを示したが，実は一般の $\al\in\ocinterval{0,1/2}$ に関して距離をなす [@Osan+2018]．

### 変分問題としての特徴付け

::: {.callout-tip icon="false" title="命題 [@Nielsen2021 p.6]"}

任意の $P,Q\in\cP_\mu(\cX)$ について，

$$
\JS(P,Q)=\min_{R\in\cP_\mu(\cX)}\Brace{\KL(P,R)+\KL(Q,R)}
$$
:::

### 有界な距離である

::: {.callout-tip icon="false" title="命題 [@Endres-Schindelin2003 p.1859]"}

$\JS:\cP_\mu(\cX)^2\to\R_+$ は最大値 $\sqrt{2\log 2}$ を持つ．
:::

### $\chi^2$-距離に漸近する [@Endres-Schindelin2003 p.1859]

### $f$-乖離度の例である

$f$-乖離度の考え方は [@Renyi1961 p.561] で導入された．他，[@Csiszar1963], [@Morimoto1963], [@Ali-Silvey1966] なども独立に導入している．

::: {.callout-tip icon="false" title="定義 ($f$-divergence)"}

$P\ll Q$ とする．凸関数 $f:\R_+\to\R$ に対して，

$$
D_f(P,Q):=\int_\cX f\paren{\dd{P}{Q}}\,dQ
$$
を **$f$-乖離度** という．

:::

KL-乖離度は
$$
f(x)=x\log x
$$
について，JS-乖離度は
$$
f(x)=x\log\frac{2x}{x+1}+\log\frac{2}{x+1}
$$
についての $f$-乖離度である．

全変動ノルムも
$$
f(x)=\abs{x-1}
$$
に関する $f$-乖離度である．


