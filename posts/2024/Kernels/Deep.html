<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="司馬博文">

<title>数学者のための深層学習概観 – Hirofumi Shiba</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/Shiba2.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-5082c1b048a9243e4e9bba654cb201bf.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-45b2177c850e8bd7118844e84da28edb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-deep-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-title','listing-image','listing-date','listing-subtitle',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-deep-listing'] = new List('listing-deep-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-lst-GNN .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-title','listing-image','listing-date','listing-subtitle',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-lst-GNN'] = new List('listing-lst-GNN', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-lst-Deep2 .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-title','listing-image','listing-date','listing-subtitle',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-lst-Deep2'] = new List('listing-lst-Deep2', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Zen+Kurenaido&amp;display=swap" rel="stylesheet">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&amp;display=swap" rel="stylesheet">

<style>
  h1, .title, .description, .subtitle {
    font-family: "Zen Kurenaido", sans-serif !important;
  }
</style>

<!-- <style>
  .menu-text {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
  .navbar-title {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
</style> -->

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../assets/styles.css">
<meta property="og:title" content="数学者のための深層学習概観 – Hirofumi Shiba">
<meta property="og:description" content="数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．">
<meta property="og:image" content="https://162348.github.io/posts/2024/Kernels/Images/AE.png">
<meta property="og:site_name" content="Hirofumi Shiba">
<meta property="og:image:height" content="385">
<meta property="og:image:width" content="577">
<meta name="twitter:title" content="数学者のための深層学習概観 – Hirofumi Shiba">
<meta name="twitter:description" content="数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．">
<meta name="twitter:image" content="https://162348.github.io/posts/2024/Kernels/Images/AE.png">
<meta name="twitter:creator" content="@ano2math5">
<meta name="twitter:image-height" content="385">
<meta name="twitter:image-width" content="577">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Notes</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-notes">    
        <li>
    <a class="dropdown-item" href="../../../static/English.html">
 <span class="dropdown-text">English Notes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../blog.html">
 <span class="dropdown-text">ノート (Japanese)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Materials.html"> 
<span class="menu-text">Materials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/162348/162348.github.io/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">数学者のための深層学習概観</h1>
            <p class="subtitle lead">歴史と導入</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Deep</div>
                <div class="quarto-category">Survey</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>司馬博文 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">2/11/2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">7/29/2024</p>
      </div>
    </div>
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">概要</div>
      数学者のために，深層学習の基礎と歴史を概観する．ニューラルネットワークの成功は，極めて単純な関数族を表現する可微分な層を深く重ねていくことで，関数としての高い表現力を得ながら，自動微分により効率的に数値的な最尤推定を実行可能にした，計算機時代最強のモデリング技法の１つである．関数近似能力，適切な初期値設定を見つける表現学習技法，そこからの確率的最適化など，種々の要素が成功に必要不可欠であったために，その成功の理由は極めて込み入っている．ここでは少しでもその成功の理由に近づくことを目標に，深層学習の発展の歴史を概観する．
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目次</h2>
   
  <ul>
  <li><a href="#歴史" id="toc-歴史" class="nav-link active" data-scroll-target="#歴史"><span class="header-section-number">1</span> 歴史</a>
  <ul class="collapse">
  <li><a href="#ニューラルネットワークの黎明" id="toc-ニューラルネットワークの黎明" class="nav-link" data-scroll-target="#ニューラルネットワークの黎明"><span class="header-section-number">1.1</span> ニューラルネットワークの黎明</a></li>
  <li><a href="#深層化の歴史" id="toc-深層化の歴史" class="nav-link" data-scroll-target="#深層化の歴史"><span class="header-section-number">1.2</span> 深層化の歴史</a></li>
  </ul></li>
  <li><a href="#アーキテクチャ" id="toc-アーキテクチャ" class="nav-link" data-scroll-target="#アーキテクチャ"><span class="header-section-number">2</span> アーキテクチャ</a>
  <ul class="collapse">
  <li><a href="#はじめに" id="toc-はじめに" class="nav-link" data-scroll-target="#はじめに"><span class="header-section-number">2.1</span> はじめに</a></li>
  <li><a href="#cnn畳み込みニューラルネットワーク" id="toc-cnn畳み込みニューラルネットワーク" class="nav-link" data-scroll-target="#cnn畳み込みニューラルネットワーク"><span class="header-section-number">2.2</span> CNN：畳み込みニューラルネットワーク</a></li>
  <li><a href="#ae自己符号化器" id="toc-ae自己符号化器" class="nav-link" data-scroll-target="#ae自己符号化器"><span class="header-section-number">2.3</span> AE：自己符号化器</a></li>
  <li><a href="#sec-RNN" id="toc-sec-RNN" class="nav-link" data-scroll-target="#sec-RNN"><span class="header-section-number">2.4</span> RNN：再帰的ニューラルネットワーク</a></li>
  <li><a href="#transformer" id="toc-transformer" class="nav-link" data-scroll-target="#transformer"><span class="header-section-number">2.5</span> Transformer</a></li>
  <li><a href="#gnnグラフニューラルネットワーク" id="toc-gnnグラフニューラルネットワーク" class="nav-link" data-scroll-target="#gnnグラフニューラルネットワーク"><span class="header-section-number">2.6</span> GNN：グラフニューラルネットワーク</a></li>
  <li><a href="#非有向ネットワーク" id="toc-非有向ネットワーク" class="nav-link" data-scroll-target="#非有向ネットワーク"><span class="header-section-number">2.7</span> 非有向ネットワーク</a></li>
  <li><a href="#spiking-neural-network" id="toc-spiking-neural-network" class="nav-link" data-scroll-target="#spiking-neural-network"><span class="header-section-number">2.8</span> Spiking Neural Network</a></li>
  <li><a href="#ベイズからの見方" id="toc-ベイズからの見方" class="nav-link" data-scroll-target="#ベイズからの見方"><span class="header-section-number">2.9</span> ベイズからの見方</a></li>
  </ul></li>
  <li><a href="#sec-AR" id="toc-sec-AR" class="nav-link" data-scroll-target="#sec-AR"><span class="header-section-number">3</span> 自己回帰モデル</a>
  <ul class="collapse">
  <li><a href="#はじめに-1" id="toc-はじめに-1" class="nav-link" data-scroll-target="#はじめに-1"><span class="header-section-number">3.1</span> はじめに</a></li>
  <li><a href="#sec-RNN2" id="toc-sec-RNN2" class="nav-link" data-scroll-target="#sec-RNN2"><span class="header-section-number">3.2</span> RNN</a></li>
  <li><a href="#機械学習の中で占める役割" id="toc-機械学習の中で占める役割" class="nav-link" data-scroll-target="#機械学習の中で占める役割"><span class="header-section-number">3.3</span> 機械学習の中で占める役割</a></li>
  </ul></li>
  <li><a href="#sec-CNN" id="toc-sec-CNN" class="nav-link" data-scroll-target="#sec-CNN"><span class="header-section-number">4</span> CNN</a>
  <ul class="collapse">
  <li><a href="#sec-CV" id="toc-sec-CV" class="nav-link" data-scroll-target="#sec-CV"><span class="header-section-number">4.1</span> Computer Vision という分野</a></li>
  <li><a href="#sec-obj-recog-CNN" id="toc-sec-obj-recog-CNN" class="nav-link" data-scroll-target="#sec-obj-recog-CNN"><span class="header-section-number">4.2</span> 物体認識 CNN</a></li>
  <li><a href="#sec-image-segmentation-CNN" id="toc-sec-image-segmentation-CNN" class="nav-link" data-scroll-target="#sec-image-segmentation-CNN"><span class="header-section-number">4.3</span> 画像分割 CNN</a></li>
  <li><a href="#inpainting" id="toc-inpainting" class="nav-link" data-scroll-target="#inpainting"><span class="header-section-number">4.4</span> Inpainting</a></li>
  <li><a href="#スタイル転移" id="toc-スタイル転移" class="nav-link" data-scroll-target="#スタイル転移"><span class="header-section-number">4.5</span> スタイル転移</a></li>
  <li><a href="#sec-GDL" id="toc-sec-GDL" class="nav-link" data-scroll-target="#sec-GDL"><span class="header-section-number">4.6</span> 幾何学的深層学習</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="関連ページ" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="関連ページ">関連ページ</h3>
<div id="listing-deep-listing" class="quarto-listing quarto-listing-container-grid">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="RGVlcCUyQ0FJ" data-listing-date-sort="1708387200000" data-listing-file-modified-sort="1757604503919" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="6" data-listing-word-count-sort="1121">
<a href="../../../posts/2024/Kernels/Deep2.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Kernels/Transformer.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
トランスフォーマー
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル１
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-20
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="1" data-categories="RGVlcCUyQ1NhbXBsaW5n" data-listing-date-sort="1707609600000" data-listing-file-modified-sort="1757604503919" data-listing-date-modified-sort="1707782400000" data-listing-reading-time-sort="3" data-listing-word-count-sort="558">
<a href="../../../posts/2024/Kernels/Deep3.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top"><img src="GAN.png" style="height: 150px;"  class="thumbnail-image card-img"/></p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
GAN：敵対的生成ネットワーク
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル２
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-11
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="2" data-categories="RGVlcCUyQ1NhbXBsaW5n" data-listing-date-sort="1708214400000" data-listing-file-modified-sort="1757604503919" data-listing-date-modified-sort="1722211200000" data-listing-reading-time-sort="5" data-listing-word-count-sort="980">
<a href="../../../posts/2024/Kernels/Deep4.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top"><img src="VAE_files/figure-html/fig-reconstruction-output-1.png" style="height: 150px;"  class="thumbnail-image card-img"/></p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
VAE：変分自己符号化器
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル３
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-18
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="3" data-categories="RGVlcCUyQ1NhbXBsaW5n" data-listing-date-sort="1707868800000" data-listing-file-modified-sort="1757604505387" data-listing-date-modified-sort="1738022400000" data-listing-reading-time-sort="3" data-listing-word-count-sort="520">
<a href="../../../posts/2024/Samplers/NF.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Samplers/Files/NF/NSF_result.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
正規化流
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル４
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-14
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="4" data-categories="RGVlcCUyQ05hdHVyZSUyQ1NhbXBsaW5n" data-listing-date-sort="1711756800000" data-listing-file-modified-sort="1757604505230" data-listing-date-modified-sort="1722470400000" data-listing-reading-time-sort="5" data-listing-word-count-sort="880">
<a href="../../../posts/2024/Samplers/EBM.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Samplers/Files/NCL/thumb_checkerboard.gif" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
エネルギーベースモデル
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル５
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-03-30
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="5" data-categories="RGVlcCUyQ1Byb2Nlc3MlMkNTYW1wbGluZw==" data-listing-date-sort="1707868800000" data-listing-file-modified-sort="1757604505230" data-listing-date-modified-sort="1724371200000" data-listing-reading-time-sort="5" data-listing-word-count-sort="826">
<a href="../../../posts/2024/Samplers/Diffusion.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Samplers/Files/DDPM_outputs.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
拡散模型
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル６
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-14
</div>
</div>
</div>
</div></a>
</div>
</div>
<div class="listing-no-matching d-none">No matching items</div>
</div>
<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
<p>$$</p>
<p>$$</p>
</div>
</section>
<section id="歴史" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="歴史"><span class="header-section-number">1</span> 歴史</h2>
<p>深層学習とは，多層パーセプトロンを用いた機械学習の手法をいう．</p>
<!-- 画像認識，自然言語処理，画像生成，タンパク質の構造予測など，多くの分野で成功を収めている． -->
<section id="ニューラルネットワークの黎明" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="ニューラルネットワークの黎明"><span class="header-section-number">1.1</span> ニューラルネットワークの黎明</h3>
<section id="脳の解明" class="level4" data-number="1.1.1">
<h4 data-number="1.1.1" class="anchored" data-anchor-id="脳の解明"><span class="header-section-number">1.1.1</span> 脳の解明</h4>
<p><span class="citation" data-cites="McCulloch-Pitts1943">(<a href="#ref-McCulloch-Pitts1943" role="doc-biblioref">McCulloch and Pitts, 1943</a>)</span> は脳の神経回路を命題論理の枠組みで扱い，ニューロンの論理素子としての機能を考察した（McCulloch-Pitts の形式ニューロンや閾素子と呼ばれる）．なお，パーセプトロンはシグモイド関数を活性化関数に用いた場合，Turing 完全である <span class="citation" data-cites="Siegelmann-Sontag1991">(<a href="#ref-Siegelmann-Sontag1991" role="doc-biblioref">Siegelmann and Sontag, 1991</a>)</span>．</p>
<p>脳を模したモデルとして，当然学習能力をどうモデルに組み込むかが問題になる．これはシナプスの結合荷重の変化によるものだという仮設は古くからあったが，最も明確な形で表現したのが <span class="citation" data-cites="Hebb1949">(<a href="#ref-Hebb1949" role="doc-biblioref">Hebb, 1949</a>)</span> であった．</p>
<p>具体的には Hebb 則は，２つの正の相関を持つ（＝共起しやすい）ニューロンの間の荷重は増強される，というものである．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> これは連想記憶のモデルとして提案された（第 <a href="#sec-Hopfield" class="quarto-xref">2.7.1</a> 節も参照）．</p>
</section>
<section id="パーセプトロン" class="level4" data-number="1.1.2">
<h4 data-number="1.1.2" class="anchored" data-anchor-id="パーセプトロン"><span class="header-section-number">1.1.2</span> パーセプトロン</h4>
<p>パーセプトロンは，脳の記憶と認識のモデルとして，<span class="citation" data-cites="Rosenblatt1958">(<a href="#ref-Rosenblatt1958" role="doc-biblioref">Rosenblatt, 1958</a>)</span> が心理学の学会誌に発表した．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>複雑過ぎるものに対する理論解析は進まなかった．</p>
<p>そのような中で <span class="citation" data-cites="Minsky-Papert1969">(<a href="#ref-Minsky-Papert1969" role="doc-biblioref">Minsky and Papert, 1969</a>)</span> は線型分離不可能な問題に対しては線型単純パーセプトロンは解を見つけられないほど機能は劣り，かといって複雑なものは計算量が爆発するので，結局パーセプトロンは使い物にならないのではないかとの見方を示した <span class="citation" data-cites="甘利俊一1989">(<a href="#ref-甘利俊一1989" role="doc-biblioref">甘利俊一, 1989, p. 130</a>)</span>．</p>
<p>現状の深層学習の成功を見ている者からすれば，これだけのことでパーセプトロンの研究が下火になってしまったことは驚くべきことに感じる．</p>
<p><span class="citation" data-cites="甘利俊一1989">(<a href="#ref-甘利俊一1989" role="doc-biblioref">甘利俊一, 1989</a>)</span> はこの点を，次のように断罪している．</p>
<blockquote class="blockquote">
<p>ミンスキーらは，自分たちの立てた問題は正しく解いた．しかし問題の立て方を誤ったのである．<span class="citation" data-cites="甘利俊一1989">(<a href="#ref-甘利俊一1989" role="doc-biblioref">甘利俊一, 1989, p. 132</a>)</span></p>
</blockquote>
</section>
<section id="多層パーセプトロン" class="level4" data-number="1.1.3">
<h4 data-number="1.1.3" class="anchored" data-anchor-id="多層パーセプトロン"><span class="header-section-number">1.1.3</span> 多層パーセプトロン</h4>
<p>パーセプトロンの発明の時点で，深層学習のモデルとしてはすでに完成していた．ただ，<span class="citation" data-cites="Minsky-Papert1969">(<a href="#ref-Minsky-Papert1969" role="doc-biblioref">Minsky and Papert, 1969</a>)</span> の指摘の通り，計算複雑性や学習アルゴリズムの問題が残り続けた．</p>
<p>多層のパーセプトロンでは中間層を学習させることが課題であった．これに対しては，活性化関数に可微分なものを用いて確率的勾配降下法によって学習させることで解決できること <span class="citation" data-cites="Amari1967">(<a href="#ref-Amari1967" role="doc-biblioref">Amari, 1967</a>)</span> などは早い時期から議論されていた．この問題点は，局所解に囚われることであった．</p>
<p>多層のパーセプトロンに対して，局所解に囚われるなどの問題はあれど，極めて多くの場合で誤差逆伝播法が有効な学習法になることが広く周知されたのは，<span class="citation" data-cites="Rumelhart+1986">(<a href="#ref-Rumelhart+1986" role="doc-biblioref">Rumelhart et al., 1986</a>)</span>, <span class="citation" data-cites="Rumelhart+1987">(<a href="#ref-Rumelhart+1987" role="doc-biblioref">Rumelhart et al., 1987</a>)</span> であり，これが深層学習の第二次ブームの着火剤となった．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>すぐさま英語の発音を学習させた研究 <span class="citation" data-cites="Sejnowski-Rosenberg1987">(<a href="#ref-Sejnowski-Rosenberg1987" role="doc-biblioref">Sejnowski and Rosenberg, 1987</a>)</span> の <a href="https://youtu.be/gakJlr3GecE?si=JLNQjl2bXJomWu9z">デモ</a> も発表され，大きな波紋を呼び起こした．ここでも特徴的な内部表現が観察された．</p>
<p>局所解に囚われることが解決された訳ではない．技術的な問題点（特に，結局深層モデルの訓練はうまくいかないこと）はあれど，ニューラルネットワークは実用的に極めて簡単に使えるモデルであると知らしめたのである．</p>
<blockquote class="blockquote">
<p>In short, we believe that we have answered Minsky and Papert’s challenge and <em>have</em> found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced. <span class="citation" data-cites="Rumelhart+1987">(<a href="#ref-Rumelhart+1987" role="doc-biblioref">Rumelhart et al., 1987, p. 361</a>)</span></p>
</blockquote>
<p>現在は，<a href="../../../posts/2024/AI/BAI1_Dropout.html">ドロップアウト</a> や区分的線型な活性化関数 ReLU を用いるなどの工夫がなされている <span class="citation" data-cites="Goodfellow+2014">(<a href="#ref-Goodfellow+2014" role="doc-biblioref">Goodfellow et al., 2014</a>)</span>．</p>
</section>
</section>
<section id="深層化の歴史" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="深層化の歴史"><span class="header-section-number">1.2</span> 深層化の歴史<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></h3>
<section id="深層化の障壁" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="深層化の障壁"><span class="header-section-number">1.2.1</span> 深層化の障壁</h4>
<p>誤差逆伝播法をニューラルネットに使うことで，表現学習がなされることの発見 <span class="citation" data-cites="Rumelhart+1986">(<a href="#ref-Rumelhart+1986" role="doc-biblioref">Rumelhart et al., 1986</a>)</span> から，深層学習の分野は次の点で変化したという：</p>
<ul>
<li>神経科学・生物学的なモチベーションから遊離し，より確率論的・統計学的なアプローチが主流になった <span class="citation" data-cites="Bishop-Bishop2024">(<a href="#ref-Bishop-Bishop2024" role="doc-biblioref">Bishop and Bishop, 2024, p. 19</a>)</span>．</li>
<li>多層のニューラルネットワークと高次元データに対する理論的な研究が加速した <span class="citation" data-cites="MacKay2003">(<a href="#ref-MacKay2003" role="doc-biblioref">MacKay, 2003, p. 535</a>)</span>．</li>
</ul>
<p>しかし，画像識別タスクに特化して結合構造を予め作り込むことで学習を容易にしてある畳み込みニューラルネットワーク (CNN) <span class="citation" data-cites="LeCun1998">(<a href="#ref-LeCun1998" role="doc-biblioref">LeCun et al., 1998</a>)</span> などを除いて，２層以上のニューラルネットワークの成功した応用例は殆どなかった．これは，<a href="https://ja.wikipedia.org/wiki/%E5%8B%BE%E9%85%8D%E6%B6%88%E5%A4%B1%E5%95%8F%E9%A1%8C">勾配消失</a> により，どんなに多層なニューラルネットワークを構築しても，最後の２層程度しか意味のあるパラメータを学習できなかったためである．</p>
<p>そのために，多層のニューラルネットワークでは表現学習は難しいという認識が広まり，複雑なタスクに対しては，問題固有の特徴抽出技法が編み出されるのみで，一般的な解決法はなかった．</p>
<p>３層のニューラルネットワークも，隠れ層の幅が無限大の極限で，任意の連続関数を近似できるという普遍近似定理という抽象的な慰め <span class="citation" data-cites="Hecht-Nielsen1989">(<a href="#ref-Hecht-Nielsen1989" role="doc-biblioref">Hecht-Nielsen, 1989</a>)</span> があるのみであった．</p>
<p>そこで多くの研究者は，SVM <span class="citation" data-cites="Cortes-Vapnik1995">(<a href="#ref-Cortes-Vapnik1995" role="doc-biblioref">Cortes and Vapnik, 1995</a>)</span> やカーネル法，Gauss 過程に現実的な打開策を求め始めたのである．これは冬の時代とも呼ばれる．</p>
</section>
<section id="sec-CNN0" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="sec-CNN0"><span class="header-section-number">1.2.2</span> ネオコグニトロンと CNN</h4>
<p><span class="citation" data-cites="Hubel-Wiesel1959">(<a href="#ref-Hubel-Wiesel1959" role="doc-biblioref">Wiesel and Hubel, 1959</a>)</span> は猫の視覚野には，単純細胞と複雑細胞の２種類の細胞があることを発見した．また，生後すぐの猫を一日交代で異なる片目を覆って成長した猫では，多くの視覚野の神経細胞は単眼性になる <span class="citation" data-cites="Hubel1967">(<a href="#ref-Hubel1967" role="doc-biblioref">Hubel, 1967</a>)</span>．</p>
<p>これらの観察から，<span class="citation" data-cites="Malsburg1973">(<a href="#ref-Malsburg1973" role="doc-biblioref">von&nbsp;der&nbsp;Malsburg, 1973</a>)</span> や <span class="citation" data-cites="Fukushima1975">(<a href="#ref-Fukushima1975" role="doc-biblioref">Fukushima, 1975</a>)</span> は，特徴抽出細胞が自己形成するメカニズムを，自己組織化のキーワードの下で調べた．特に後者は <strong>コグニトロン</strong> というモデルを提案した．</p>
<p><span class="citation" data-cites="Fukushima1980">(<a href="#ref-Fukushima1980" role="doc-biblioref">Fukushima, 1980</a>)</span> の <a href="https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3">ネオコグニトロン</a> は初の深層モデルの例と言える．<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> これは，単純細胞層と複雑細胞層を交互に深く重ねたネットワークである．</p>
<p>しかし <a href="https://ja.wikipedia.org/wiki/%E7%A6%8F%E5%B3%B6%E9%82%A6%E5%BD%A6">福島</a> は学習の問題を中心に扱った訳ではなかった．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="citation" data-cites="LeCun1998">(<a href="#ref-LeCun1998" role="doc-biblioref">LeCun et al., 1998</a>)</span> の畳み込みニューラルネットワーク LeNet は，ネオコグニトロンを誤差逆伝播法により教師あり学習させたものと言える．</p>
<p>このモデルは後に AlexNet としてダントツの性能で世界を驚かせることになる．</p>
<p>画像データはピクセルを節としたグラフデータとみなすこともでき，CNN を一般化する形で GNN (Graph Neural Network) も提案されている <span class="citation" data-cites="Zhou+2020">(<a href="#ref-Zhou+2020" role="doc-biblioref">J. Zhou et al., 2020</a>)</span>, <span class="citation" data-cites="Wu+2021">(<a href="#ref-Wu+2021" role="doc-biblioref">Wu et al., 2021</a>)</span>, <span class="citation" data-cites="Velickovic2023">(<a href="#ref-Velickovic2023" role="doc-biblioref">Veličković, 2023</a>)</span>．</p>
</section>
<section id="sec-AE" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="sec-AE"><span class="header-section-number">1.2.3</span> 自己符号化器</h4>
<p><span class="citation" data-cites="Cottrell-Munro1988">(<a href="#ref-Cottrell-Munro1988" role="doc-biblioref">Cottrell and Munro, 1988</a>)</span> は中間層の幅を小さくし，入力信号自身を教師信号として誤差逆伝播法により学習することで，隠れ層に入力の低次元表現が学習され，主成分分析に用いることが出来ることを報告した．</p>
<p>これは入力層と出力層の素子数を一致させ，出力が入力に近づくように訓練される．このようなモデルを <strong>自己符号化器</strong> (autoencoder) または 自己連想ネットワーク (auto-associative neural network) と呼ぶ．</p>
<p>内部表現を獲得するとされる中間層に対して，それより前半の層全体を符号化器，それ以降を復号化器と呼ぶ．<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p><span class="citation" data-cites="Baldi-Hornik1989">(<a href="#ref-Baldi-Hornik1989" role="doc-biblioref">Baldi and Hornik, 1989</a>)</span> は活性化関数がない３層の場合，自己符号化器を訓練させることは主成分分析を実行することに等価であることを示した．</p>
<p>さらに層を増やすことで，非線型な次元圧縮が可能であることが <span class="citation" data-cites="DeMers-Cottrell1992">(<a href="#ref-DeMers-Cottrell1992" role="doc-biblioref">DeMers and Cottrell, 1992</a>)</span> で示された．この論文では，データの空間内の（非線型な）部分多様体の局所座標が学習されたり，人間の顔の写真のデータ圧縮が出来ることが実証されている．</p>
<p>自己符号化器はスタッキングが可能である <span class="citation" data-cites="Ballard1987">(<a href="#ref-Ballard1987" role="doc-biblioref">Ballard, 1987</a>)</span>．このことが，次の節で述べる事前学習のアイデアに繋がる．</p>
</section>
<section id="sec-pretraining-using-AE" class="level4" data-number="1.2.4">
<h4 data-number="1.2.4" class="anchored" data-anchor-id="sec-pretraining-using-AE"><span class="header-section-number">1.2.4</span> 自己符号化器を用いた事前学習 <span class="citation" data-cites="Bengio+2006">(<a href="#ref-Bengio+2006" role="doc-biblioref">Bengio et al., 2006</a>)</span></h4>
<p>自己符号化器自体を符号圧縮と表現学習に用いることは，他の手法と比べて特別優れているという訳でもなかった．</p>
<p>しかし，深層ニューラルネットワークを <strong>層ごとに事前に教師なし学習をする</strong> ことで，後の教師あり学習において勾配消失などの問題が回避できるというアイデア <span class="citation" data-cites="Hinton-Salakhutdinov2006">(<a href="#ref-Hinton-Salakhutdinov2006" role="doc-biblioref">Hinton and Salakhutdinov, 2006</a>)</span>, <span class="citation" data-cites="Hinton+2006">(<a href="#ref-Hinton+2006" role="doc-biblioref">Hinton et al., 2006</a>)</span> は画期的であり，<strong>深層学習</strong> (Deep Learning) という言葉が広まったのもこの頃であるという <span class="citation" data-cites="Schmidhuber2015">(<a href="#ref-Schmidhuber2015" role="doc-biblioref">Schmidhuber, 2015, p. 96</a>)</span>．<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>入力に近い方から，１層ごとに，前層の出力を入力として自己符号化器と見て訓練するのである <span class="citation" data-cites="Bengio+2006">(<a href="#ref-Bengio+2006" role="doc-biblioref">Bengio et al., 2006</a>)</span>．<span class="citation" data-cites="Larochelle+2007">(<a href="#ref-Larochelle+2007" role="doc-biblioref">Larochelle et al., 2007</a>)</span> はこの事前学習によって，素性の良い局所解の近くにパラメータの初期値が調整されるということを実験的に示している．</p>
<p>この発見は深層模型の訓練を可能にするという大きなブレイクスルーを，教師なしの表現学習と教師あり学習による調整とに問題を分離して解くことによって成し遂げたと言える．</p>
<p>また，単純な自己符号化器の代わりに，ノイズが加えられたデータを入力しこれを復元するようにに学習する <a href="../../../posts/2024/Kernels/Deep4.html#sec-denoising-autoencoder">denoising autoencoder</a> を用いた方が，深層モデルにより良い初期値を与える潜在表現を獲得できることが報告されている <span class="citation" data-cites="Vincent+2008">(<a href="#ref-Vincent+2008" role="doc-biblioref">Vincent et al., 2008</a>)</span>．<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
</section>
<section id="alexnet-krizhevsky2012" class="level4" data-number="1.2.5">
<h4 data-number="1.2.5" class="anchored" data-anchor-id="alexnet-krizhevsky2012"><span class="header-section-number">1.2.5</span> AlexNet <span class="citation" data-cites="Krizhevsky+2012">(<a href="#ref-Krizhevsky+2012" role="doc-biblioref">Krizhevsky et al., 2012</a>)</span></h4>
<p>ImageNet データベース <span class="citation" data-cites="Deng+2009">(<a href="#ref-Deng+2009" role="doc-biblioref">Deng et al., 2009</a>)</span> を用いた判別コンテスト ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) で，ダントツで優勝した AlexNet <span class="citation" data-cites="Krizhevsky+2012">(<a href="#ref-Krizhevsky+2012" role="doc-biblioref">Krizhevsky et al., 2012</a>)</span> が大きなターニングポイントとなった．</p>
<p>これも <span class="citation" data-cites="LeCun1998">(<a href="#ref-LeCun1998" role="doc-biblioref">LeCun et al., 1998</a>)</span> の CNN を基にした８層のモデルで，活性化関数には ReLU とドロップアウトによる正則化が用いられていた．学習も，NVIDIA 社の GPU を用いていた．</p>
</section>
<section id="sec-ResNet" class="level4" data-number="1.2.6">
<h4 data-number="1.2.6" class="anchored" data-anchor-id="sec-ResNet"><span class="header-section-number">1.2.6</span> ResNet <span class="citation" data-cites="He+2016">(<a href="#ref-He+2016" role="doc-biblioref">He et al., 2016</a>)</span></h4>
<p>20 層以上の多層ニューラルネットワークの学習の困難さを初めて解決したのが <span class="citation" data-cites="He+2016">(<a href="#ref-He+2016" role="doc-biblioref">He et al., 2016</a>)</span> の Residual Network (ResNet) であった．<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>このモデルは 152 層もあったが効率的に訓練することが可能で，2015 年の ILSVRC でダントツで優勝し，しかも初めて人間の誤答率 5% <span class="citation" data-cites="Dodge-Karam2017">(<a href="#ref-Dodge-Karam2017" role="doc-biblioref">Dodge and Karam, 2017</a>)</span> を下回ったのである．</p>
<p>そのアイデアは，各層の入力 <span class="math inline">\(x\)</span> を出力に再度加算した形 <span class="math display">\[
y=x+F(x)
\]</span> で各層をデザインし，現状の入力からの差分 <span class="math inline">\(F\)</span> のみを学習するとすることで勾配消失を回避する，というものであった．実際，この層の微分は <span class="math display">\[
\frac{d y}{d x}=1+F'(x)
\]</span> と表せる．</p>
<p>このテクニックは <a href="../../../posts/2024/Kernels/Deep2.html">トランスフォーマー</a> <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> などのモデルでも用いられている．</p>
<p><span class="citation" data-cites="Li+2018">(<a href="#ref-Li+2018" role="doc-biblioref">Li et al., 2018</a>)</span> によると，残差レイヤーの追加は誤差関数を滑らかにし，近しい入力に対しても勾配が非常に大きくなってしまうことが効率的な学習を阻害してしまう問題 (shattered gradient problem) <span class="citation" data-cites="Balduzzi+2017">(<a href="#ref-Balduzzi+2017" role="doc-biblioref">Balduzzi et al., 2017</a>)</span> を解決している，という．</p>
</section>
</section>
</section>
<section id="アーキテクチャ" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="アーキテクチャ"><span class="header-section-number">2</span> アーキテクチャ</h2>
<section id="はじめに" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="はじめに"><span class="header-section-number">2.1</span> はじめに</h3>
<p>ニューラルネットワークは，内部に循環を持つかどうかで二分され，それぞれを Feedforward Network (FFN) と Feedback Network (FBN) と呼ぶ．<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>FFN は <strong>多層パーセプトロン</strong> (MLP) ともいう．<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p><a href="https://ja.wikipedia.org/wiki/%E5%9B%9E%E5%B8%B0%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF">Recurrent Neural Network</a> (RNN) <a href="#sec-RNN" class="quarto-xref">2.4</a> や Hopfield ネットワーク <a href="#sec-Hopfield" class="quarto-xref">2.7.1</a> は FBN の例である．</p>
<p>多層パーセプトロンは分類や回帰などの統計学的な用途に主に用いられるが，FBN は学習機械として以外に，生物の神経機能のモデルとしてや，組合せ最適化ソルバーとしても用いられる．<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<!-- ここでは特に **教師なしニューラルネットワーク** を取り上げる．^[[@MacKay2003 p.470] など．] -->
</section>
<section id="cnn畳み込みニューラルネットワーク" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="cnn畳み込みニューラルネットワーク"><span class="header-section-number">2.2</span> CNN：畳み込みニューラルネットワーク</h3>
<p>CNN はその畳み込み層に特徴付けられる画像に特化した FNN アーキテクチャである．</p>
<p>第 <a href="#sec-CNN" class="quarto-xref">4</a> 節で詳しく扱う．</p>
</section>
<section id="ae自己符号化器" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="ae自己符号化器"><span class="header-section-number">2.3</span> AE：自己符号化器</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/AE.png" class="img-fluid figure-img"></p>
<figcaption>Example of Autoencoder from <span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 635</a>)</span></figcaption>
</figure>
</div>
<p>自己符号化器または自己連想ニューラルネットワーク (auto-associative neural network) <a href="#sec-AE" class="quarto-xref">1.2.3</a> は図のような砂時計型の，入力 <span class="math inline">\(x\)</span> と出力 <span class="math inline">\(y\)</span> の次元数が一致したアーキテクチャを持ち， <span class="math display">\[
\mathcal{L}(\theta)=\|y-x\|^2_2
\]</span> などの入力の復元誤差を目的関数として訓練される．</p>
</section>
<section id="sec-RNN" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="sec-RNN"><span class="header-section-number">2.4</span> RNN：再帰的ニューラルネットワーク</h3>
<p>再帰的な層を持ち，自己回帰モデル <a href="#sec-AR" class="quarto-xref">3</a> を実装する FBN の例である．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/RNN.png" class="img-fluid figure-img"></p>
<figcaption>Example of Recurrent Neural Network from <span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 636</a>)</span></figcaption>
</figure>
</div>
<p>第 <a href="#sec-RNN2" class="quarto-xref">3.2</a> 節でも詳しく扱う．</p>
</section>
<section id="transformer" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="transformer"><span class="header-section-number">2.5</span> Transformer</h3>
<p>RNN の最大の難点は，隠れ次元 <span class="math inline">\(z_t\)</span> が <span class="math inline">\(x_{1:t}\)</span> までの入力の要約になっており，<span class="math inline">\(x_1\)</span> などの最初の方の情報がどんどん薄れていく点にある．</p>
<p>そこで，時点 <span class="math inline">\(t\)</span> でも <span class="math inline">\(x_1\)</span> など以前の情報にワンステップでアクセスできるような機構である <strong>注意機構</strong> が考案された．これがエンコーダーのみのトランスフォーマーである．</p>
<p>デコーダーのみのトランスフォーマーは masked attenstion という技術を用いて <span class="math inline">\(x_{1:t-1}\)</span> のみで条件づけて <span class="math inline">\(x_t\)</span> を生成することができる．</p>
<p>一方で，入力全体で条件づけて，文章を生成することもでき，これが最初に <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> によって提案された，最も一般的な形の encoder-decoder トランスフォーマーである．</p>
<div id="listing-lst-Deep2" class="listing quarto-float quarto-figure quarto-figure-left quarto-listing quarto-listing-container-grid anchored">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="RGVlcCUyQ0FJ" data-listing-date-sort="1708387200000" data-listing-file-modified-sort="1757604503919" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="6" data-listing-word-count-sort="1121">
<a href="../../../posts/2024/Kernels/Deep2.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Kernels/Transformer.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
トランスフォーマー
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル１
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-20
</div>
</div>
</div>
</div></a>
</div>
</div>
<div class="listing-no-matching d-none">No matching items</div>
</div>
</section>
<section id="gnnグラフニューラルネットワーク" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="gnnグラフニューラルネットワーク"><span class="header-section-number">2.6</span> GNN：グラフニューラルネットワーク</h3>
<p>従来の NN は辺の存在しない退化したグラフに対するものだとして，NN を一般のグラフデータに対して拡張することが近年考えられている．</p>
<div id="listing-lst-GNN" class="listing quarto-float quarto-figure quarto-figure-left quarto-listing quarto-listing-container-grid anchored">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="RGVlcA==" data-listing-date-sort="1709769600000" data-listing-file-modified-sort="1757604503921" data-listing-date-modified-sort="1723593600000" data-listing-reading-time-sort="1" data-listing-word-count-sort="93">
<a href="../../../posts/2024/Kernels/GNN.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Kernels/Images/GNN_Distill.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
グラフニューラルネットワーク
</h5>
<div class="card-subtitle listing-subtitle">
位相的データ解析の旗手
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-03-07
</div>
</div>
</div>
</div></a>
</div>
</div>
<div class="listing-no-matching d-none">No matching items</div>
</div>
</section>
<section id="非有向ネットワーク" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="非有向ネットワーク"><span class="header-section-number">2.7</span> 非有向ネットワーク</h3>
<p>無向グラフが定めるニューラルネットワークは，<a href="../../../posts/2024/Samplers/EBM.html">エネルギーベースモデル</a> (EBM: Energy-Based Model) で主に用いられる．</p>
<section id="sec-Hopfield" class="level4" data-number="2.7.1">
<h4 data-number="2.7.1" class="anchored" data-anchor-id="sec-Hopfield"><span class="header-section-number">2.7.1</span> Hopfield ネットワークとスピングラス</h4>
<p>計算機の記憶と生物の記憶の相違点のうち，大きなものには連想性 (associativity) がある．アドレスで整理されているのではなく，内容で整理されているのである．1970 年代には，神経の連想記憶機能のモデルとしてのニューラルネットワークが多数提案された．</p>
<p>それには，ここまで議論してきた階層型のネットワークと異なり，ノード同士は <strong>相互結合</strong> しているものも含まれる．連想記憶のモデルとしては，特に相互結合で，どちらの方向に関する重みも同じであるもの（<strong>対称結合</strong> ネットワーク）が多く，全結合の FBN である Hopfield network はその代表例である．<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<p>（連続変数の）Hopfield network の学習則は，Hebb 則 <span class="citation" data-cites="Hebb1949">(<a href="#ref-Hebb1949" role="doc-biblioref">Hebb, 1949</a>)</span> に基づいて，各ニューロン <span class="math inline">\(x_j\in[0,1]\)</span> についてその入力 <span class="math display">\[
a_i:=\sum_{j}w_{ij}x_j
\]</span> を計算し，<span class="math inline">\(x_i\gets\tanh(a_i)\)</span> と更新する．</p>
<p>実はこの学習則は必ず収束する．このことを，<span class="citation" data-cites="Hopfield1982">(<a href="#ref-Hopfield1982" role="doc-biblioref">Hopfield, 1982</a>)</span> はスピングラスのモデルと関連付けて示したことから，特に統計物理学の文脈で Hopfield network の名前がついた．<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<blockquote class="blockquote">
<p>神経回路網の解析，とくに連想記憶モデルの解析が，スピングラスを解析する方法を用いて実行できるのではないかという考えが出てきて，大量の物理学者が神経回路網に注目しだした．こうしたアイデアの火付け役がホップフィールドと言われる．<span class="citation" data-cites="甘利俊一1989">(<a href="#ref-甘利俊一1989" role="doc-biblioref">甘利俊一, 1989, p. 105</a>)</span></p>
</blockquote>
<p>対称結合のニューラルネットワークの学習則は，スピングラスと同様に，必ずポテンシャル関数が減少する方に動作するというのである．この連関を利用して，<span class="citation" data-cites="Hopfield-Tank1985">(<a href="#ref-Hopfield-Tank1985" role="doc-biblioref">Hopfield and Tank, 1985</a>)</span> は Hopfield ネットワークをアナログ回路に実装し，巡回セールスマン問題を解くという，最適化問題ソルバーとして利用してみせた．</p>
<p>単体 Hopfield Network <span class="citation" data-cites="Burns-Fukai2023">(<a href="#ref-Burns-Fukai2023" role="doc-biblioref">Burns and Fukai, 2023</a>)</span> という拡張もあり，パラメータ数は変わらずとも記憶容量が増える．</p>
</section>
<section id="ボルツマンマシン-ackley1985" class="level4" data-number="2.7.2">
<h4 data-number="2.7.2" class="anchored" data-anchor-id="ボルツマンマシン-ackley1985"><span class="header-section-number">2.7.2</span> ボルツマンマシン <span class="citation" data-cites="Ackley+1985">(<a href="#ref-Ackley+1985" role="doc-biblioref">Ackley et al., 1985</a>)</span></h4>
<p>ボルツマンマシンは確率的 Hopfield ネットワークともいい，Hopfield ネットワークが統計物理のモデルとして近似するところの Gibbs 分布を，実際に持つ <a href="../../../posts/2024/Computation/PGM2.html">Markov 確率場</a> の一種である．<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<p>これは，荷重を，確率 <span class="math inline">\(\frac{1}{1+e^{-2a_i}}\)</span> で <span class="math inline">\(x_i=1\)</span>，そうでない場合は <span class="math inline">\(x_i=-1\)</span> と定めることで得られる．</p>
</section>
<section id="深層ボルツマンマシン-salakhutdinov-hinton2009" class="level4" data-number="2.7.3">
<h4 data-number="2.7.3" class="anchored" data-anchor-id="深層ボルツマンマシン-salakhutdinov-hinton2009"><span class="header-section-number">2.7.3</span> 深層ボルツマンマシン <span class="citation" data-cites="Salakhutdinov-Hinton2009">(<a href="#ref-Salakhutdinov-Hinton2009" role="doc-biblioref">Salakhutdinov and Hinton, 2009</a>)</span></h4>
</section>
<section id="sec-DBN" class="level4" data-number="2.7.4">
<h4 data-number="2.7.4" class="anchored" data-anchor-id="sec-DBN"><span class="header-section-number">2.7.4</span> 深層信念ネットワーク <span class="citation" data-cites="Hinton+2006">(<a href="#ref-Hinton+2006" role="doc-biblioref">Hinton et al., 2006</a>)</span></h4>
<p>これは深層学習研究の皮切りになった確率的深層モデルである．</p>
</section>
</section>
<section id="spiking-neural-network" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="spiking-neural-network"><span class="header-section-number">2.8</span> Spiking Neural Network</h3>
<p>実際の神経細胞は <strong>発火</strong> という離散的なイベントを発生させ，その頻度やタイミングも大きな役割を持っている．これを取り入れたモデルを Spiking Neural Network (SNN) <span class="citation" data-cites="Maass1997">(<a href="#ref-Maass1997" role="doc-biblioref">Maass, 1997</a>)</span> と呼ぶ <span class="citation" data-cites="岡島義憲2020">(<a href="#ref-岡島義憲2020" role="doc-biblioref">岡島義憲, 2020</a>)</span>．</p>
<p>SNN は現状の人工ニューラルネット (ANN: Artificial Neural Network) よりも，半導体上での計算を効率化することが出来る．そこで，SNN による深層学習が近年試みられている <span class="citation" data-cites="Tavanaei+2019">(<a href="#ref-Tavanaei+2019" role="doc-biblioref">Tavanaei et al., 2019</a>)</span>．</p>
<p>Microsoft の BitNet <span class="citation" data-cites="Wang+2023-BitNet">(<a href="#ref-Wang+2023-BitNet" role="doc-biblioref">Wang et al., 2023</a>)</span> も計算効率性を目指すにあたってそのアイデアを等しくする．</p>
</section>
<section id="ベイズからの見方" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="ベイズからの見方"><span class="header-section-number">2.9</span> ベイズからの見方</h3>
<section id="ネットワークの正則化と汎化性能" class="level4" data-number="2.9.1">
<h4 data-number="2.9.1" class="anchored" data-anchor-id="ネットワークの正則化と汎化性能"><span class="header-section-number">2.9.1</span> ネットワークの正則化と汎化性能</h4>
<p>良い汎化性能を得るためには，<a href="https://ja.wikipedia.org/wiki/%E5%81%8F%E3%82%8A%E3%81%A8%E5%88%86%E6%95%A3">偏倚と分散のトレードオフ</a> を乗り越える必要がある．</p>
<p>データセットに対してモデルの自由度が高すぎると，分散は小さくなれど，過学習を起こしてしまう．すなわち，大きなバイアスが導入され，汎化性能が悪くなる．一方で，モデルの自由度が低すぎると，平均的には正しい予測ができても，分散が大きくて役に立たない．</p>
<p>大規模なデータセットを用いることは一つの解決である．自由度の高いモデルを用いても過学習が起こりにくくなるためである．<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
</section>
<section id="帰納バイアス" class="level4" data-number="2.9.2">
<h4 data-number="2.9.2" class="anchored" data-anchor-id="帰納バイアス"><span class="header-section-number">2.9.2</span> 帰納バイアス</h4>
<p>正則化とは，モデルの自由度を制限することで過学習を抑制することである．これは，「正しいモデルは十分に滑らかであるはずである」という帰納バイアスを導入することで，モデルの汎化性能を改善させていることに等しい．<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<p>転移学習も一種の帰納バイアスの注入だと見れる．２つの異なるタスクの間に類似性が存在するという事前知識を注入することで，汎化性能を改善する手法だと思えるのである．</p>
<p>汎化性能の高いモデルを作るということは，人類が解きたいタスクに普遍的に共通する特徴を捉え，これを帰納バイアスの形でモデルに注入することに等しい．<a href="https://ja.wikipedia.org/wiki/%E3%83%8E%E3%83%BC%E3%83%95%E3%83%AA%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%81%E5%AE%9A%E7%90%86">No free lunch theorem</a> <span class="citation" data-cites="Wolpert1996">(<a href="#ref-Wolpert1996" role="doc-biblioref">Wolpert, 1996</a>)</span> から，<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> 一般に特定のタスクに対する性能向上は，他のタスクに対する性能低下を伴うものであることが予想されるが，例えば推定関数が十分滑らかであるというのは，人間の認識特性上，有意義な結果にはほとんど普遍的に必要な条件である．</p>
</section>
<section id="ベイズ深層学習の美点" class="level4" data-number="2.9.3">
<h4 data-number="2.9.3" class="anchored" data-anchor-id="ベイズ深層学習の美点"><span class="header-section-number">2.9.3</span> ベイズ深層学習の美点</h4>
<p><span class="citation" data-cites="Gal-Ghahramani2016">(<a href="#ref-Gal-Ghahramani2016" role="doc-biblioref">Gal and Ghahramani, 2016</a>)</span> などでは，</p>
<ul>
<li>データの数が少なくとも，有効なパラメータ推定が可能である．</li>
<li>過学習が起こりにくい．</li>
<li>不確実性の定量化が自動でなされる．</li>
</ul>
<p>と説明される．</p>
</section>
<section id="ベイズ深層学習の例" class="level4" data-number="2.9.4">
<h4 data-number="2.9.4" class="anchored" data-anchor-id="ベイズ深層学習の例"><span class="header-section-number">2.9.4</span> ベイズ深層学習の例</h4>
<p>CNN は特に過学習しやすい上にサンプル効率性が悪い．その場合には，変分近似による Bayesian CNN は既存法と同等の性能を誇る <span class="citation" data-cites="Gal-Ghahramani2016">(<a href="#ref-Gal-Ghahramani2016" role="doc-biblioref">Gal and Ghahramani, 2016</a>)</span>．</p>
</section>
</section>
</section>
<section id="sec-AR" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-AR"><span class="header-section-number">3</span> 自己回帰モデル</h2>
<section id="はじめに-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="はじめに-1"><span class="header-section-number">3.1</span> はじめに</h3>
<p>長さ <span class="math inline">\(T\)</span> のデータベクトルをモデリングするとき， <span class="math display">\[
p(x_{1:T})=\prod_{t=1}^Tp(x_t|x_{1:t-1})
\]</span> という分解を用いることができる．このようなアプローチを機械学習では <strong>自己回帰モデル</strong> という．<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
</section>
<section id="sec-RNN2" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-RNN2"><span class="header-section-number">3.2</span> RNN</h3>
<p>しかし，このようなモデリング法は <span class="math inline">\(T\)</span> が大きくなるにつれて，条件づける変数 <span class="math inline">\(x_{1:t-1}\)</span> が高次元になるため，モデリングが難しくなる．</p>
<p>かと言って，<span class="math inline">\(\{x_t\}_{t=1}^T\)</span> を Markov 連鎖とみなしてモデリングするわけにもいかない．</p>
<p>一つの解決法が <strong>状態空間モデル / 隠れ Markov モデル</strong> によるものである．<span class="math inline">\(x_{1:t}\)</span> はある状態変数 <span class="math inline">\(z_t\)</span> にある既知の確率法則で圧縮できると仮定し，これのみを持ち越す方法である．</p>
<p>特に，<span class="math inline">\(X_{1:t-1}\to Z_t\)</span> の対応が決定論的であるとき，これを多層パーセプトロンによってモデリングしたものを <strong>再帰ニューラルネットワーク</strong> という．</p>
<p><span class="math inline">\(x_t=f(z_{t-1},x_{t-1})\)</span> の関係を学習した後は，<span class="math inline">\(x_1\)</span> から再帰的に <span class="math inline">\(f\)</span> に通すことでデータ <span class="math inline">\(x_{1:T}\)</span> を生成する．</p>
</section>
<section id="機械学習の中で占める役割" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="機械学習の中で占める役割"><span class="header-section-number">3.3</span> 機械学習の中で占める役割</h3>
<p>AR モデルは尤度を効率的に扱えるため，計算や最適化が簡単である．実際，RNN や CNN などの自己回帰モデルが，歴史上最初に発達したアーキテクチャである．</p>
<p>一方で，データの生成が逐次的であるために生成が遅いことや，内部で潜在表現を学習するということがないために表現学習に使うことができない点が欠点と言える．<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
</section>
</section>
<section id="sec-CNN" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-CNN"><span class="header-section-number">4</span> CNN</h2>
<p>CNN はトランスフォーマーによりデータ間の関係を自動的に学習する枠組みが提案される前に，主に画像分野において，データの構造に関する事前知識をモデルに組み込んだ例として提案されたものである．</p>
<p>世界初の深層学習モデルによる席巻は，CNN により，画像認識の分野において達成された．</p>
<section id="sec-CV" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="sec-CV"><span class="header-section-number">4.1</span> Computer Vision という分野</h3>
<p>Computer Vision という問題の複雑性が，ニューラルネットワークのアーキテクチャの開発を後押しした歴史がある．</p>
<p>並行移動・拡大変換という2つの合同変換不変性に対して，画像の認識結果は不変であるべきである．</p>
<p>このような不変性，または <strong>同変性</strong> (equivariance) をモデルに取り入れる方法は大きく分けて４つある：</p>
<ol type="1">
<li>誤差関数に正則化項を導入する<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></li>
<li><strong>対称性を取り入れた潜在表現</strong> を用いてその上で学習をする</li>
<li>不変性を効率良く学習出来るように <strong>データセットを拡張する</strong></li>
<li>対称性を取り扱う構造をネットワークの<strong>アーキテクチャに組み込む</strong></li>
</ol>
<p>CNN は４番目のアプローチで歴史上最初に取られたものである．しかし，このどのアプローチも完全には不変性を取り入れることは出来ていないことも報告されている <span class="citation" data-cites="Azulay-Weiss2019">(<a href="#ref-Azulay-Weiss2019" role="doc-biblioref">Azulay and Weiss, 2019</a>)</span>．</p>
<p>幾何学を種々の変換に対する不変性の研究と捉え直した Felix Klein の <a href="https://ja.wikipedia.org/wiki/%E3%82%A8%E3%83%AB%E3%83%A9%E3%83%B3%E3%82%B2%E3%83%B3%E3%83%BB%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0">Erlangen program</a> に倣い，種々の深層モデルの帰納バイアスとアーキテクチャを，幾何学的な変換から導出してシステマティックに理解する試み Geometric Deep Learning <span class="citation" data-cites="Bronstein+2021">(<a href="#ref-Bronstein+2021" role="doc-biblioref">Bronstein et al., 2021</a>)</span> がある（第 <a href="#sec-GDL" class="quarto-xref">4.6</a> 節）．</p>
</section>
<section id="sec-obj-recog-CNN" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="sec-obj-recog-CNN"><span class="header-section-number">4.2</span> 物体認識 CNN</h3>
<p>CNN は画像の特徴を，階層的に学習出来るように誘導するような構造を持っている．</p>
<p>多くの例では，畳み込み層とプーリング層が交互に繰り返され，最後に全結合層を持つような構造を持っている．</p>
<section id="局所的な特徴" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="局所的な特徴"><span class="header-section-number">4.2.1</span> 局所的な特徴</h4>
<p>最初の素子は，画像の局所的な一部のみを入力として取る．その範囲を <strong>受容野</strong> (receptive field) と呼ぶ．この素子の荷重を <strong>フィルター</strong> または <strong>カーネル</strong> という．</p>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="例：2次元の畳み込み層">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
例：2次元の畳み込み層
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>2次元での幅 <span class="math inline">\(2k+1\)</span> の畳み込み層は，フィルター <span class="math inline">\(\mathrm{supp}\;(\psi)\subset\{0,\pm1,\cdots,\pm k\}\)</span> を用いて， <span class="math display">\[
f^{\text{out}}_{i,j}=\phi\left(\sum_{a,b\in\mathbb{Z}}\psi(i-a,j-b)f_{i,j}^{\text{in}}+\theta\right)
\]</span> と表せる．</p>
</div>
</div>
</div>
<p>決まったカーネルに対して，この素子はカーネルの特徴にマッチした入力に対して，大きな出力を返す．</p>
<p>次に，このフィルターを畳み込むことで，画像内の異なる位置に存在する特徴を検出する．畳み込み層は，荷重を共有した疎結合層ということになる．</p>
<p>畳み込みを行うと，入力次元と出力次元が変わってしまうことがあるため，その場合は入力画像にパディングを施す．</p>
<p>出力次元を小さくして，畳み込み特徴写像で大きな次元削減を行いたい場合，strided convolution を用いる．</p>
</section>
<section id="並行移動不変性" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="並行移動不変性"><span class="header-section-number">4.2.2</span> 並行移動不変性</h4>
<p>畳み込みの結果が，特徴の位置の変化に対して不変になるようにする設計に，<strong>プーリング層</strong> または <strong>ダウンサンプリング層</strong> (down-sampling / sub-sampling) がある．</p>
<p>プーリングも，受容野を持った素子と畳み込みからなるが，畳み込みに学習されるべきパラメータはなく，確定的な関数と畳み込まれる．</p>
<p>代表的なプーリング関数には <strong>最大プーリング</strong> <span class="citation" data-cites="Zhou-Chellappa1988">(<a href="#ref-Zhou-Chellappa1988" role="doc-biblioref">Y. Zhou and Chellappa, 1988</a>)</span> や平均プーリング，<span class="math inline">\(l^2\)</span>-プーリングなどがある．</p>
<p>プーリングは不変性の導入に加えて，畳み込み特徴のダウンサンプリングを行って，更なる次元削減を行う役割も果たす．</p>
</section>
</section>
<section id="sec-image-segmentation-CNN" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="sec-image-segmentation-CNN"><span class="header-section-number">4.3</span> 画像分割 CNN</h3>
<p>画像分類では，１枚の画像に対して１つのクラスの対応づけたが，１つのピクセルに１つのクラスを対応づけることで，画像をクラスごとに分類することが考えられる．</p>
<section id="up-sampling" class="level4" data-number="4.3.1">
<h4 data-number="4.3.1" class="anchored" data-anchor-id="up-sampling"><span class="header-section-number">4.3.1</span> Up-sampling</h4>
<p>画像分類の問題では，最終的に全ピクセルから得た情報を１次元に圧縮することになる．一方で，十分な潜在表現を得たのちは up-sampling に転じる Encoder-Decoder 構造にすることで，最終的に元の画像サイズに戻しながら，画像分割問題を解くことが出来る <span class="citation" data-cites="Long+2015">(<a href="#ref-Long+2015" role="doc-biblioref">Long et al., 2015</a>)</span>, <span class="citation" data-cites="Noh+2015">(<a href="#ref-Noh+2015" role="doc-biblioref">Noh et al., 2015</a>)</span>, <span class="citation" data-cites="Badrinarayanan+2017">(<a href="#ref-Badrinarayanan+2017" role="doc-biblioref">Badrinarayanan et al., 2017</a>)</span>．</p>
<p><span class="citation" data-cites="Badrinarayanan+2017">(<a href="#ref-Badrinarayanan+2017" role="doc-biblioref">Badrinarayanan et al., 2017</a>)</span> は max-unpooling などの up-sampling 層の設計を考慮したが，これに学習可能なパラメータを増やした transpose convolution / deconvolution も提案された．</p>
<p>Pooling 層を一切用いず，down-sampling も up-sampling も畳み込み層のみによって行われる場合，これを 全畳み込みネットワーク (fully convolutional network) という <span class="citation" data-cites="Long+2015">(<a href="#ref-Long+2015" role="doc-biblioref">Long et al., 2015</a>)</span>．</p>
</section>
<section id="sec-U-net" class="level4" data-number="4.3.2">
<h4 data-number="4.3.2" class="anchored" data-anchor-id="sec-U-net"><span class="header-section-number">4.3.2</span> U-Net</h4>
<p>この encoder-decoder 構造は，分類に必要のない情報を自動的に削減し，モデルのサイズを小さくするのには効果的であるが，タスクによっては元の画像の情報量を保ちたい場合がある．</p>
<p>U-net <span class="citation" data-cites="Ronneberger+2015">(<a href="#ref-Ronneberger+2015" role="doc-biblioref">Ronneberger et al., 2015</a>)</span> は対応する down-sampling 層と up-sampling 層とを直接繋ぐ経路を追加することでこれを解決した．</p>
</section>
<section id="capsule-networks" class="level4" data-number="4.3.3">
<h4 data-number="4.3.3" class="anchored" data-anchor-id="capsule-networks"><span class="header-section-number">4.3.3</span> Capsule Networks</h4>
<p>プーリング層は並行移動不変性の概念を取り入れるが，回転や拡大などの変換に対する不変性を取り入れるにはデータ拡張に依るしかないのでは，CNN の学習はどうしても大規模なデータセットが必要になってしまう．</p>
<p>そこで，アーキテクチャによる解決を試みたのが，畳み込み層に加えて <strong>カプセル層</strong> を取り入れた Capsule Network <span class="citation" data-cites="Sabour+2017">(<a href="#ref-Sabour+2017" role="doc-biblioref">Sabour et al., 2017</a>)</span> である．</p>
</section>
</section>
<section id="inpainting" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="inpainting"><span class="header-section-number">4.4</span> Inpainting</h3>
<p><span class="citation" data-cites="Horita+2023">(<a href="#ref-Horita+2023" role="doc-biblioref">Horita et al., 2023</a>)</span></p>
</section>
<section id="スタイル転移" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="スタイル転移"><span class="header-section-number">4.5</span> スタイル転移</h3>
<p>CNN において，最初の方のレイヤーは局所的な特徴を捉えているが，後の方のレイヤーはスタイルなどの大域的な特徴を捉えている．</p>
<p>これを用いて，既存の画像の具体的な特徴を変えずに，他の画像からスタイルのみを転移する手法 <strong>Neural Style Transfer</strong> が提案された <span class="citation" data-cites="Gatys+2015">(<a href="#ref-Gatys+2015" role="doc-biblioref">Gatys et al., 2015</a>)</span>, <span class="citation" data-cites="Gatys+2016">(<a href="#ref-Gatys+2016" role="doc-biblioref">Gatys et al., 2016</a>)</span>．</p>
</section>
<section id="sec-GDL" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="sec-GDL"><span class="header-section-number">4.6</span> 幾何学的深層学習</h3>
<p>以上，種々のタスクに種々のアーキテクチャが存在することを見てきた．この状況は，19 世紀の幾何学と似ていると <span class="citation" data-cites="Bronstein+2021">(<a href="#ref-Bronstein+2021" role="doc-biblioref">Bronstein et al., 2021</a>)</span> はいう．</p>
<p>これらのアーキテクチャがどのような帰納バイアスを導入する役割を果たしているかを，不変性や同変性といった第一原理から理解する試みが <strong>幾何学的深層学習</strong> である．</p>
<blockquote class="blockquote">
<p>In this text, we make a modest attempt to apply the <strong>Erlangen Programme mindset</strong> to the domain of deep learning, with the ultimate goal of obtaining a systematisation of this field and ‘connecting the dots’. We call this geometrisation attempt ‘<strong>Geometric Deep Learning</strong>’, and true to the spirit of Felix Klein, propose to derive different inductive biases and network architectures implementing them from <strong>first principles of symmetry and invariance</strong>. <span class="citation" data-cites="Bronstein+2021">(<a href="#ref-Bronstein+2021" role="doc-biblioref">Bronstein et al., 2021, p. 2</a>)</span></p>
</blockquote>
<p>化学・生物・物理はいずれも対象の対称性をしっかり扱う理論を持っている．これらの分野に深層学習が広く取り入れられつつある今，深層学習の分野も対称性を第一原理として整理される脱皮が待たれているのである．</p>
<section id="幾何学と解析学は双対である" class="level4" data-number="4.6.1">
<h4 data-number="4.6.1" class="anchored" data-anchor-id="幾何学と解析学は双対である"><span class="header-section-number">4.6.1</span> 幾何学と解析学は双対である</h4>
<p><a href="https://en.wikipedia.org/wiki/Mikhael_Gromov_(mathematician)">Mikhael Gromov</a> によると，空間 <span class="math inline">\(X\)</span> の解析学とは <span class="math inline">\(X\)</span> 上の関数の研究で，<span class="math inline">\(X\)</span> の幾何学とは <span class="math inline">\(X\)</span> への関数の研究である <span class="citation" data-cites="深谷賢治1997">(<a href="#ref-深谷賢治1997" role="doc-biblioref">深谷賢治, 1997, p. 11</a>)</span>．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://www.math.kyoto-u.ac.jp/~fukaya/shzuok.pdf"><img src="Images/Gromov.png" class="img-fluid figure-img"></a></p>
<figcaption>Gromov による幾何と解析の解釈</figcaption>
</figure>
</div>
<p>同変性と共変性は，データ集合 <span class="math inline">\(X\)</span> と群作用 <span class="math inline">\(\rho:G\times X\to X\)</span> との組 <span class="math inline">\((X,\rho)\)</span> を取り扱うから，確かに上述の定義に適っている．</p>
</section>
<section id="世紀の数学の方向" class="level4" data-number="4.6.2">
<h4 data-number="4.6.2" class="anchored" data-anchor-id="世紀の数学の方向"><span class="header-section-number">4.6.2</span> 20 世紀の数学の方向</h4>
<p><span class="citation" data-cites="Gromov2001">(<a href="#ref-Gromov2001" role="doc-biblioref">Gromov, 2001</a>)</span></p>
</section>
<section id="共変性と同変性" class="level4" data-number="4.6.3">
<h4 data-number="4.6.3" class="anchored" data-anchor-id="共変性と同変性"><span class="header-section-number">4.6.3</span> 共変性と同変性</h4>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="定義 (invariance, equivariance)^[[@福水健次2024], [@Bronstein+2021 pp.15-16] など．または [nLab](https://ncatlab.org/nlab/show/equivariant+structure) も参照．]">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
定義 (invariance, equivariance)<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(X,Y\)</span> を集合，<span class="math inline">\(G\)</span> を群で <span class="math inline">\(X,Y\)</span> に左から作用するとする．<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> 関数 <span class="math inline">\(\varphi:X\to Y\)</span> が</p>
<ul>
<li><span class="math inline">\(G\)</span> に関して <strong>不変</strong> であるとは，任意の <span class="math inline">\(g\in G\)</span> と <span class="math inline">\(x\in X\)</span> について <span class="math display">\[
\varphi(g\cdot x)=\varphi(x)
\]</span> を満たすことをいう．</li>
<li><span class="math inline">\(G\)</span> に関して <strong>同変</strong> であるとは，任意の <span class="math inline">\(g\in G\)</span> と <span class="math inline">\(x\in X\)</span> について <span class="math display">\[
\varphi(g\cdot x)=g\cdot\varphi(x)
\]</span> を満たすことをいう．</li>
</ul>
</div>
</div>
<p>物体認識 <a href="#sec-obj-recog-CNN" class="quarto-xref">Section&nbsp;4.2</a> は不変的で，画像分割 <a href="#sec-image-segmentation-CNN" class="quarto-xref">Section&nbsp;4.3</a> は同変的な問題である．</p>
<p>ただし，不変性は，<span class="math inline">\(G\)</span> の <span class="math inline">\(Y\)</span> への作用が自明である場合の同変性と見れるため，同変性の方が一般的な概念であることに注意．</p>
</section>
<section id="g-cnn-cohen-welling2016" class="level4" data-number="4.6.4">
<h4 data-number="4.6.4" class="anchored" data-anchor-id="g-cnn-cohen-welling2016"><span class="header-section-number">4.6.4</span> G-CNN <span class="citation" data-cites="Cohen-Welling2016">(<a href="#ref-Cohen-Welling2016" role="doc-biblioref">T. Cohen and Welling, 2016</a>)</span></h4>
<p>一般の群変換に対して，これを帰納バイアスとして取り入れる CNN である Group CNN <span class="citation" data-cites="Cohen-Welling2016">(<a href="#ref-Cohen-Welling2016" role="doc-biblioref">T. Cohen and Welling, 2016</a>)</span> が提案されており，医療画像解析での応用 <span class="citation" data-cites="Lafarge+2021">(<a href="#ref-Lafarge+2021" role="doc-biblioref">Lafarge et al., 2021</a>)</span> もなされている．</p>
</section>
<section id="steerable-cnn-cohen-welling2017" class="level4" data-number="4.6.5">
<h4 data-number="4.6.5" class="anchored" data-anchor-id="steerable-cnn-cohen-welling2017"><span class="header-section-number">4.6.5</span> Steerable CNN <span class="citation" data-cites="Cohen-Welling2017">(<a href="#ref-Cohen-Welling2017" role="doc-biblioref">T. S. Cohen and Welling, 2017</a>)</span></h4>
<p>Steerable CNN ではチャンネルの間での対称性を取り入れることができる．</p>
<p><span class="citation" data-cites="Weiler+2018">(<a href="#ref-Weiler+2018" role="doc-biblioref">Weiler et al., 2018</a>)</span> により最先端の性能が発揮されている．</p>
</section>
</section>
</section>



<div id="quarto-appendix" class="default"><section id="終わりに" class="level2 appendix" data-number="5"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">5</span> 終わりに</h2><div class="quarto-appendix-contents">

<p>もはやニューラルネットワークは，layered differentiable model <span class="citation" data-cites="Oord+2019">(<a href="#ref-Oord+2019" role="doc-biblioref">Oord et al., 2019</a>)</span> と呼んだ方がその数学的な存在をよく表すだろうと思う．</p>




</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Ackley+1985" class="csl-entry" role="listitem">
Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). <a href="https://doi.org/10.1016/S0364-0213(85)80012-4">A learning algorithm for boltzmann machines</a>. <em>Cognitive Science</em>, <em>9</em>(1), 147–169.
</div>
<div id="ref-Amari1967" class="csl-entry" role="listitem">
Amari, S. (1967). <a href="https://ieeexplore.ieee.org/document/4039068">A theory of adaptive pattern classifiers</a>. <em>IEEE Transactions on Electronic Computers</em>, <em>EC-16</em>(3), 299–307.
</div>
<div id="ref-Azulay-Weiss2019" class="csl-entry" role="listitem">
Azulay, A., and Weiss, Y. (2019). <a href="http://jmlr.org/papers/v20/19-519.html">Why do deep convolutional networks generalize so poorly to small image transformations?</a> <em>Journal of Machine Learning Research</em>, <em>20</em>(184), 1–25.
</div>
<div id="ref-Badrinarayanan+2017" class="csl-entry" role="listitem">
Badrinarayanan, V., Kendall, A., and Cipolla, R. (2017). <a href="https://doi.org/10.1109/TPAMI.2016.2644615">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</a>. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>39</em>(12), 2481–2495.
</div>
<div id="ref-Baldi-Hornik1989" class="csl-entry" role="listitem">
Baldi, P., and Hornik, K. (1989). <a href="https://www.sciencedirect.com/science/article/pii/0893608089900142">Neural networks and principal component analysis: Learning from examples without local minima</a>. <em>Neural Networks</em>, <em>2</em>(1), 53–58.
</div>
<div id="ref-Balduzzi+2017" class="csl-entry" role="listitem">
Balduzzi, D., Frean, M., Leary, L., Lewis, J. P., Ma, K. W.-D., and McWilliams, B. (2017). <a href="https://proceedings.mlr.press/v70/balduzzi17b.html">The shattered gradients problem: If resnets are the answer, then what is the question?</a> In D. Precup and Y. W. Teh, editors, <em>Proceedings of the 34th international conference on machine learning</em>,Vol. 70, pages 342–350. PMLR.
</div>
<div id="ref-Ballard1987" class="csl-entry" role="listitem">
Ballard, D. H. (1987). <a href="https://dl.acm.org/doi/10.5555/1863696.1863746">Modular learning in neural networks</a>. <em>Proceedings of the Sixth National Conference on Artificial Intelligence</em>, <em>1</em>, 279–284.
</div>
<div id="ref-Bengio+2006" class="csl-entry" role="listitem">
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2006). <a href="https://papers.nips.cc/paper_files/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html">Greedy layer-wise training of deep networks</a>. In <em>Advances in neural information processing systems</em>,Vol. 19, pages 153–160.
</div>
<div id="ref-Bishop-Bishop2024" class="csl-entry" role="listitem">
Bishop, C. M., and Bishop, H. (2024). <em><a href="https://link.springer.com/book/10.1007/978-3-031-45468-4">Deep learning: Foundations and concepts</a></em>. Springer Cham.
</div>
<div id="ref-Bronstein+2021" class="csl-entry" role="listitem">
Bronstein, M. M., Bruna, J., Cohen, T., and Veličković, P. (2021). <a href="https://arxiv.org/abs/2104.13478">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</a>.
</div>
<div id="ref-Burns-Fukai2023" class="csl-entry" role="listitem">
Burns, T. F., and Fukai, T. (2023). <a href="https://openreview.net/forum?id=_QLsH8gatwx">Simplicial hopfield networks</a>. In <em>The eleventh international conference on learning representations</em>.
</div>
<div id="ref-Cohen-Welling2017" class="csl-entry" role="listitem">
Cohen, T. S., and Welling, M. (2017). <a href="https://openreview.net/forum?id=rJQKYt5ll">Steerable <span>CNN</span>s</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Cohen-Welling2016" class="csl-entry" role="listitem">
Cohen, T., and Welling, M. (2016). <a href="https://proceedings.mlr.press/v48/cohenc16.html">Group equivariant convolutional networks</a>. In M. F. Balcan and K. Q. Weinberger, editors, <em>Proceedings of the 33rd international conference on machine learning</em>,Vol. 48, pages 2990–2999. New York, New York, USA: PMLR.
</div>
<div id="ref-Cortes-Vapnik1995" class="csl-entry" role="listitem">
Cortes, C., and Vapnik, V. (1995). Support-vector networks. <em>Machine Learning</em>, <em>20</em>, 273–297.
</div>
<div id="ref-Cottrell-Munro1988" class="csl-entry" role="listitem">
Cottrell, G. W., and Munro, P. (1988). <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1001/1/Principal-Components-Analysis-Of-Images-Via-Back-Propagation/10.1117/12.969060.short">Principal component analysis of images via back propagation</a>. In <em>Proceedings of SPIE visual communications and image processings</em>,Vol. 1001, pages 1070–1076.
</div>
<div id="ref-DeMers-Cottrell1992" class="csl-entry" role="listitem">
DeMers, D., and Cottrell, G. (1992). <a href="https://proceedings.neurips.cc/paper/1992/hash/cdc0d6e63aa8e41c89689f54970bb35f-Abstract.html">Non-linear dimensionality reduction</a>. In <em>Advances in neural information processing systems</em>,Vol. 5, pages 580–587.
</div>
<div id="ref-Deng+2009" class="csl-entry" role="listitem">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). <a href="https://doi.org/10.1109/CVPR.2009.5206848">ImageNet: A large-scale hierarchical image database</a>. In <em>2009 IEEE conference on computer vision and pattern recognition</em>, pages 248–255.
</div>
<div id="ref-Dodge-Karam2017" class="csl-entry" role="listitem">
Dodge, S., and Karam, L. (2017). <a href="https://arxiv.org/abs/1705.02498">A study and comparison of human and deep learning recognition performance under visual distortions</a>.
</div>
<div id="ref-Fukushima1975" class="csl-entry" role="listitem">
Fukushima, K. (1975). <a href="">Cognitron: A self-organizing multilayered neural network</a>. <em>Biological Cybernetics</em>, <em>20</em>, 121–136.
</div>
<div id="ref-Fukushima1980" class="csl-entry" role="listitem">
Fukushima, K. (1980). <a href="https://link.springer.com/article/10.1007/BF00344251">Neural network model for a mechanism of pattern recognition unaffected by shift in position–neocognitron</a>. <em>Biological Cybernetics</em>, <em>36</em>, 193–202.
</div>
<div id="ref-Gal-Ghahramani2016" class="csl-entry" role="listitem">
Gal, Y., and Ghahramani, Z. (2016). <a href="https://openreview.net/forum?id=3QxqXoJEyfp7y9wltP11">Bayesian convolutional neural networks with bernoulli approximate variational inference</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Gatys+2015" class="csl-entry" role="listitem">
Gatys, L. A., Ecker, A. S., and Bethge, M. (2015). <a href="https://arxiv.org/abs/1508.06576">A neural algorithm of artistic style</a>.
</div>
<div id="ref-Gatys+2016" class="csl-entry" role="listitem">
Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). <a href="https://doi.org/10.1109/CVPR.2016.265">Image style transfer using convolutional neural networks</a>. In <em>2016 IEEE conference on computer vision and pattern recognition (CVPR)</em>, pages 2414–2423.
</div>
<div id="ref-Goodfellow+2014" class="csl-entry" role="listitem">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). <a href="https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html">Generative adversarial nets</a>. In <em>Advances in neural information processing systems</em>,Vol. 27, pages 1–9.
</div>
<div id="ref-Gromov2001" class="csl-entry" role="listitem">
Gromov, M. (2001). <a href="https://doi.org/10.1007/978-3-642-56478-9_26">Possible trends in mathematics in the coming decades</a>. In B. Engquist and W. Schmid, editors, <em>Mathematics unlimited — 2001 and beyond</em>, pages 525–527. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-He+2016" class="csl-entry" role="listitem">
He, K., Zhang, X., Ren, S., and Sun, J. (2016). <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Deep residual learning for image recognition</a>. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770–778.
</div>
<div id="ref-Hebb1949" class="csl-entry" role="listitem">
Hebb, D. O. (1949). <em><a href="https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb">The organization of behavior: A neuropsychological theory</a></em>. John Wiley &amp; Sons, Chapman; Hall.
</div>
<div id="ref-Hecht-Nielsen1989" class="csl-entry" role="listitem">
Hecht-Nielsen, R. (1989). <a href="https://ieeexplore.ieee.org/document/118638">Theory of the backpropagation neural network</a>. In <em>International 1989 joint conference on neural networks</em>.
</div>
<div id="ref-Hinton+2006" class="csl-entry" role="listitem">
Hinton, G. E., Esindero, S., and Teh, Y.-W. (2006). <a href="https://direct.mit.edu/neco/article/18/7/1527/7065/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets">A fast learning algorithm for deep belief nets</a>. <em>Naural Computation</em>, <em>18</em>(7), 1527–1554.
</div>
<div id="ref-Hinton-Salakhutdinov2006" class="csl-entry" role="listitem">
Hinton, G. E., and Salakhutdinov, R. R. (2006). <a href="https://www.science.org/doi/10.1126/science.1127647">Reducing the dimensionality of data with neural networks</a>. <em>Science</em>, <em>313</em>(5786), 504–507.
</div>
<div id="ref-Hopfield1982" class="csl-entry" role="listitem">
Hopfield, J. J. (1982). <a href="https://www.pnas.org/doi/10.1073/pnas.79.8.2554">Neural networks and physical systems with emergent collective computational abilities</a>. <em>Proceedings of the National Academy of Science</em>, <em>79</em>(8), 2554–2558.
</div>
<div id="ref-Hopfield-Tank1985" class="csl-entry" role="listitem">
Hopfield, J. J., and Tank, D. W. (1985). <a href="https://link.springer.com/article/10.1007/BF00339943">"Neural" computation of decisions in optimization problems</a>. <em>Biological Cybernetics</em>, <em>52</em>, 141–152.
</div>
<div id="ref-Horita+2023" class="csl-entry" role="listitem">
Horita, D., Yang, J., Chen, D., Koyama, Y., Aizawa, K., and Sebe, N. (2023). <a href="https://arxiv.org/abs/2211.10437">A structure-guided diffusion model for large-hole image completion</a>.
</div>
<div id="ref-Hubel1967" class="csl-entry" role="listitem">
Hubel, D. H. (1967). <a href="">Effects of distortion of sensory input on the visual system of kittens</a>. <em>The Physiologist</em>, <em>10</em>, 17–45.
</div>
<div id="ref-Koller-Friedman2009" class="csl-entry" role="listitem">
Koller, D., and Friedman, N. (2009). <em><a href="https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/">Probabilistic graphical models</a></em>. MIT Press.
</div>
<div id="ref-Krizhevsky+2012" class="csl-entry" role="listitem">
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). <a href="https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">ImageNet classification with deep convolutional neural networks</a>. In <em>Advances in neural information processing systems</em>,Vol. 25.
</div>
<div id="ref-Lafarge+2021" class="csl-entry" role="listitem">
Lafarge, M. W., Bekkers, E. J., Pluim, J. P. W., Duits, R., and Veta, M. (2021). <a href="https://doi.org/10.1016/j.media.2020.101849">Roto-translation equivariant convolutional networks: Application to histopathology image analysis</a>. <em>Medical Image Analysis</em>, <em>68</em>, 101849.
</div>
<div id="ref-Larochelle+2007" class="csl-entry" role="listitem">
Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. (2007). <a href="https://doi.org/10.1145/1273496.1273556">An empirical evaluation of deep architectures on problems with many factors of variation</a>. In <em>Proceedings of the 24th international conference on machine learning</em>, pages 473–480. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-LeCun1998" class="csl-entry" role="listitem">
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). <a href="https://ieeexplore.ieee.org/document/726791">Gradient-based learning applied to document recognition</a>. <em>Proceedings of the IEEE</em>, <em>86</em>(11), 2278–2324.
</div>
<div id="ref-Li+2018" class="csl-entry" role="listitem">
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf">Visualizing the loss landscape of neural nets</a>. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, <em>Advances in neural information processing systems</em>,Vol. 31. Curran Associates, Inc.
</div>
<div id="ref-Long+2015" class="csl-entry" role="listitem">
Long, J., Shelhamer, E., and Darrell, T. (2015). <a href="https://doi.org/10.1109/CVPR.2015.7298965">Fully convolutional networks for semantic segmentation</a>. In <em>2015 IEEE conference on computer vision and pattern recognition (CVPR)</em>, pages 3431–3440. Los Alamitos, CA, USA: IEEE Computer Society.
</div>
<div id="ref-Maass1997" class="csl-entry" role="listitem">
Maass, W. (1997). <a href="https://doi.org/10.1016/S0893-6080(97)00011-7">Networks of spiking neurons: The third generation of neural network models</a>. <em>Neural Networks</em>, <em>10</em>(9), 1659–1671.
</div>
<div id="ref-MacKay2003" class="csl-entry" role="listitem">
MacKay, D. J. C. (2003). <em><a href="https://www.cambridge.org/gb/universitypress/subjects/computer-science/pattern-recognition-and-machine-learning/information-theory-inference-and-learning-algorithms?format=HB&amp;isbn=9780521642989">Information theory, inference and learning algorithms</a></em>. Cambridge University Press.
</div>
<div id="ref-McCulloch-Pitts1943" class="csl-entry" role="listitem">
McCulloch, W., and Pitts, W. (1943). <a href="https://link.springer.com/article/10.1007/BF02478259">A logical calculus of the ideas immanent in nervous activity</a>. <em>Bulletin of Mathematical Biophysics</em>, <em>7</em>, 115–133.
</div>
<div id="ref-Minsky-Papert1969" class="csl-entry" role="listitem">
Minsky, M., and Papert, S. A. (1969). <em><a href="https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational">Perceptrons: An introduction to computational geometry</a></em>. The MIT Press.
</div>
<div id="ref-Murphy2023" class="csl-entry" role="listitem">
Murphy, K. P. (2023). <em><a href="http://probml.github.io/book2">Probabilistic machine learning: Advanced topics</a></em>. MIT Press.
</div>
<div id="ref-Noh+2015" class="csl-entry" role="listitem">
Noh, H., Hong, S., and Han, B. (2015). <a href="https://doi.org/10.1109/ICCV.2015.178">Learning deconvolution network for semantic segmentation</a>. In <em>2015 IEEE international conference on computer vision (ICCV)</em>, pages 1520–1528. Los Alamitos, CA, USA: IEEE Computer Society.
</div>
<div id="ref-Oord+2019" class="csl-entry" role="listitem">
Oord, A. van den, Li, Y., and Vinyals, O. (2019). <a href="https://arxiv.org/abs/1807.03748">Representation learning with contrastive predictive coding</a>.
</div>
<div id="ref-Ronneberger+2015" class="csl-entry" role="listitem">
Ronneberger, O., Fischer, P., and Brox, T. (2015). <a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28">U-net: Convolutional networks for biomedical image segmentation</a>. In N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, editors, <em>Medical image computing and computer-assisted intervention – MICCAI 2015</em>, pages 234–241. Cham: Springer International Publishing.
</div>
<div id="ref-Rosenblatt1958" class="csl-entry" role="listitem">
Rosenblatt, F. (1958). <a href="https://psycnet.apa.org/record/1959-09865-001">The perceptron: A probabilistic model for information storage and organization in the brain</a>. <em>Psychological Review</em>, <em>65</em>(6), 386–408.
</div>
<div id="ref-Rumelhart+1986" class="csl-entry" role="listitem">
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). <a href="https://www.nature.com/articles/323533a0">Learning representations by back-propagating errors</a>. <em>Nature</em>, <em>323</em>, 533–536.
</div>
<div id="ref-Rumelhart+1987" class="csl-entry" role="listitem">
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1987). <a href="https://mitpress.mit.edu/9780262680530/parallel-distributed-processing/">Parallel distributed processing: Explorations in the microstructure of cognition: foundations</a>. In D. E. Rumelhart and J. L. McClelland, editors, pages 318–362. MIT Press.
</div>
<div id="ref-Sabour+2017" class="csl-entry" role="listitem">
Sabour, S., Frosst, N., and Hinton, G. E. (2017). <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf">Dynamic routing between capsules</a>. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, <em>Advances in neural information processing systems</em>,Vol. 30. Curran Associates, Inc.
</div>
<div id="ref-Salakhutdinov-Hinton2009" class="csl-entry" role="listitem">
Salakhutdinov, R., and Hinton, G. (2009). <a href="https://proceedings.mlr.press/v5/salakhutdinov09a.html">Deep boltzmann machines</a>. In <em>Proceedings of the twelth international conference on artificial intelligence and statistics</em>,Vol. 5, pages 448–455.
</div>
<div id="ref-Schmidhuber2015" class="csl-entry" role="listitem">
Schmidhuber, J. (2015). <a href="https://www.sciencedirect.com/science/article/pii/S0893608014002135">Deep learning in neural networks: An overview</a>. <em>Neural Networks</em>, <em>61</em>, 85–117.
</div>
<div id="ref-Sejnowski-Rosenberg1987" class="csl-entry" role="listitem">
Sejnowski, T. J., and Rosenberg, C. R. (1987). <a href="https://www.complex-systems.com/abstracts/v01_i01_a10/">Parallel networks that learn to pronounce english text</a>. <em>Complex Systems</em>, <em>1</em>(1), 145–168.
</div>
<div id="ref-Shalev-Shwartz-Ben-David2014" class="csl-entry" role="listitem">
Shalev-Shwartz, S., and Ben-David, S. (2014). <em><a href="https://doi.org/10.1017/CBO9781107298019">Understanding machine learning: From theory to algorithms</a></em>. Cambridge University Press.
</div>
<div id="ref-Siegelmann-Sontag1991" class="csl-entry" role="listitem">
Siegelmann, H. T., and Sontag, E. D. (1991). <a href="https://www.sciencedirect.com/science/article/pii/089396599190080F">Turing computability with neural nets</a>. <em>Applied Mathematics Letters</em>, <em>4</em>(6), 77–80.
</div>
<div id="ref-Simard+1991" class="csl-entry" role="listitem">
Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1991). <a href="https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf">Tangent prop - a formalism for specifying selected invariances in an adaptive network</a>. In J. Moody, S. Hanson, and R. P. Lippmann, editors, <em>Advances in neural information processing systems</em>,Vol. 4. Morgan-Kaufmann.
</div>
<div id="ref-Simonyan-Zisserman2015" class="csl-entry" role="listitem">
Simonyan, K., and Zisserman, A. (2015). <a href="https://ora.ox.ac.uk/objects/uuid:60713f18-a6d1-4d97-8f45-b60ad8aebbce">Very deep convolutional networks for large-scale image recognition</a>. In <em>International conference on learning representations</em>,Vol. 3, pages 1–14.
</div>
<div id="ref-Tavanaei+2019" class="csl-entry" role="listitem">
Tavanaei, A., Ghodrati, M., Kheradpisheh, S. R., Masquelier, T., and Maida, A. (2019). <a href="https://doi.org/10.1016/j.neunet.2018.12.002">Deep learning in spiking neural networks</a>. <em>Neural Networks</em>, <em>111</em>, 47–63.
</div>
<div id="ref-Vaswani+2017" class="csl-entry" role="listitem">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Attention is all you need</a>. In <em>Advances in neural information processing systems</em>,Vol. 30.
</div>
<div id="ref-Velickovic2023" class="csl-entry" role="listitem">
Veličković, P. (2023). <a href="https://www.sciencedirect.com/science/article/pii/S0959440X2300012X">Everything is connected: Graph neural networks</a>. <em>Current Opinion in Structural Biology</em>, <em>79</em>, 102538.
</div>
<div id="ref-Vincent+2008" class="csl-entry" role="listitem">
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). <a href="https://doi.org/10.1145/1390156.1390294">Extracting and composing robust features with denoising autoencoders</a>. In <em>Proceedings of the 25th international conference on machine learning</em>, pages 1096–1103. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Malsburg1973" class="csl-entry" role="listitem">
von&nbsp;der&nbsp;Malsburg, C. (1973). <a href="">Self-organization of orientation sensitive cells in the striate cortex</a>. <em>Kybernetik</em>, <em>14</em>, 85–100.
</div>
<div id="ref-Wang+2023-BitNet" class="csl-entry" role="listitem">
Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., … Wei, F. (2023). <a href="https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/">BitNet: Scaling 1-bit transformers for large language models</a>. arXiv.
</div>
<div id="ref-Weiler+2018" class="csl-entry" role="listitem">
Weiler, M., Hamprecht, F. A., and Storath, M. (2018). Learning steerable filters for rotation equivariant CNNs. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</em>.
</div>
<div id="ref-Hubel-Wiesel1959" class="csl-entry" role="listitem">
Wiesel, D. H., and Hubel, T. N. (1959). <a href="https://physoc.onlinelibrary.wiley.com/doi/10.1113/jphysiol.1959.sp006308">Receptive fields of single neurones in the cat’s striate cortex</a>. <em>Journal of Physiology</em>, <em>148</em>(3), 574–591.
</div>
<div id="ref-Wolpert1996" class="csl-entry" role="listitem">
Wolpert, D. H. (1996). <a href="https://doi.org/10.1162/neco.1996.8.7.1341">The lack of a priori distinctions between learning algorithms</a>. <em>Neural Computation</em>, <em>8</em>(7), 1341–130.
</div>
<div id="ref-Wu+2021" class="csl-entry" role="listitem">
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Yu, P. S. (2021). <a href="https://doi.org/10.1109/tnnls.2020.2978386">A comprehensive survey on graph neural networks</a>. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, <em>32</em>(1), 4–24.
</div>
<div id="ref-Zhou+2020" class="csl-entry" role="listitem">
Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z., … Sun, M. (2020). <a href="https://www.sciencedirect.com/science/article/pii/S2666651021000012">Graph neural networks: A review of methods and applications</a>. <em>AI Open</em>, <em>1</em>, 57–81.
</div>
<div id="ref-Zhou-Chellappa1988" class="csl-entry" role="listitem">
Zhou, Y., and Chellappa, R. (1988). <a href="https://ieeexplore.ieee.org/document/23914">Computation of optic flow using a neural network</a>. In <em>IEEE 1988 international conference on neural networks</em>.
</div>
<div id="ref-岡島義憲2020" class="csl-entry" role="listitem">
岡島義憲. (2020). <a href="https://doi.org/10.11517/jsaisigtwo.2020.AGI-015_08">Spiking neural network 技術の現状と課題に関する考察</a>. <em>人工知能学会第二種研究会資料</em>, <em>2020</em>(AGI-015), 08.
</div>
<div id="ref-深谷賢治1997" class="csl-entry" role="listitem">
深谷賢治. (1997). <em><a href="https://www.math.kyoto-u.ac.jp/~fukaya/">「位相的場の理論」 集中講義ノート</a></em>.
</div>
<div id="ref-甘利俊一1989" class="csl-entry" role="listitem">
甘利俊一. (1989). <em><a href="https://www.utp.or.jp/book/b305707.html">神経回路網モデルとコネクショニズム</a></em>,Vol. 22. 東京大学出版会.
</div>
<div id="ref-福水健次2024" class="csl-entry" role="listitem">
福水健次. (2024). <em><a href="">Machine learning with group theory</a></em>.
</div>
<div id="ref-人工知能学会2015" class="csl-entry" role="listitem">
麻生英樹, 安田宗樹, 前田新一, 岡野原大輔, 岡谷貴之, 久保陽太郎, and ボレガラダヌシカ. (2015). <em><a href="https://www.kindaikagaku.co.jp/book_list/detail/9784764904873/">深層学習</a></em>. 近代科学社.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="MacKay2003">(<a href="#ref-MacKay2003" role="doc-biblioref">MacKay, 2003, p. 506</a>)</span> など．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>入力層と中間層と出力層の３層のみからなるものモデルを単純パーセプトロンと呼ぶが，それだけでなく，多層のものや，フィードバック結合のあるものも含む，一般的な形で考えていた．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>オートエンコーダーを導入し，中間層で表現学習がなされること（最も幅の狭い中間層の活性化を通じてコンパクトに表現する，など）と，モーメンタムが学習を加速することを示している <span class="citation" data-cites="Rumelhart+1987">(<a href="#ref-Rumelhart+1987" role="doc-biblioref">Rumelhart et al., 1987, p. 330</a>)</span>, <span class="citation" data-cites="Schmidhuber2015">(<a href="#ref-Schmidhuber2015" role="doc-biblioref">Schmidhuber, 2015</a>)</span>．「バックプロパゲーションがこの流行の起爆剤となったとさえ言える」 <span class="citation" data-cites="甘利俊一1989">(<a href="#ref-甘利俊一1989" role="doc-biblioref">甘利俊一, 1989, p. 144</a>)</span>．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="Schmidhuber2015">(<a href="#ref-Schmidhuber2015" role="doc-biblioref">Schmidhuber, 2015</a>)</span>, <span class="citation" data-cites="Bishop-Bishop2024">(<a href="#ref-Bishop-Bishop2024" role="doc-biblioref">Bishop and Bishop, 2024</a>)</span>, <span class="citation" data-cites="人工知能学会2015">(<a href="#ref-人工知能学会2015" role="doc-biblioref">麻生英樹 et al., 2015</a>)</span> を参考．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="Schmidhuber2015">(<a href="#ref-Schmidhuber2015" role="doc-biblioref">Schmidhuber, 2015, p. 90</a>)</span> など．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>教師なしの競合学習を試みたのみであった <span class="citation" data-cites="人工知能学会2015">(<a href="#ref-人工知能学会2015" role="doc-biblioref">麻生英樹 et al., 2015, p. 21</a>)</span>．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>三層の場合は，入力層を符号化器，出力層を復号化器とも言う．さらに中間層の幅が小さい場合，その形から hourgalss-type neural network とも呼ばれる <span class="citation" data-cites="人工知能学会2015">(<a href="#ref-人工知能学会2015" role="doc-biblioref">麻生英樹 et al., 2015, p. 91</a>)</span>．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Hinton のこの２つの論文は，深層信念ネットワーク <a href="#sec-DBN" class="quarto-xref">2.7.4</a> において実証されていた．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>さらに，Hinton らが示した成功は，深層信念ネットワークは制約付き Boltzmann マシン (RBM) として各層が事前訓練されたが，これが denoising autoencoder と似たノイズへのロバスト性を示すために起こったものではないかと予想している <span class="citation" data-cites="Vincent+2008">(<a href="#ref-Vincent+2008" role="doc-biblioref">Vincent et al., 2008</a>)</span> 第６節．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>当時の CNN で深かったものには，19 層の CNN である VGGNet <span class="citation" data-cites="Simonyan-Zisserman2015">(<a href="#ref-Simonyan-Zisserman2015" role="doc-biblioref">Simonyan and Zisserman, 2015</a>)</span> がある．<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><span class="citation" data-cites="MacKay2003">(<a href="#ref-MacKay2003" role="doc-biblioref">MacKay, 2003, p. 505</a>)</span>，<span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 633</a>)</span> 16.3.1節など．<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p><span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 632</a>)</span> 16.3.1節など．<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p><span class="citation" data-cites="MacKay2003">(<a href="#ref-MacKay2003" role="doc-biblioref">MacKay, 2003, p. 468</a>)</span> の導入も参照．<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p><span class="citation" data-cites="MacKay2003">(<a href="#ref-MacKay2003" role="doc-biblioref">MacKay, 2003, p. 503</a>)</span> 第42章，<span class="citation" data-cites="人工知能学会2015">(<a href="#ref-人工知能学会2015" role="doc-biblioref">麻生英樹 et al., 2015, p. 11</a>)</span> など．<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>「このため，物理学者はこの種のモデルのことを Hopfield モデルと呼ぶが，この命名は適切とは思えない」<span class="citation" data-cites="甘利俊一1989">(<a href="#ref-甘利俊一1989" role="doc-biblioref">甘利俊一, 1989, p. 97</a>)</span> としている．<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>“The popularity of the Boltzmann machine was primarily driven by its similarity to an activation model for neurons.” <span class="citation" data-cites="Koller-Friedman2009">(<a href="#ref-Koller-Friedman2009" role="doc-biblioref">Koller and Friedman, 2009, p. 126</a>)</span>．<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><span class="citation" data-cites="Bishop-Bishop2024">(<a href="#ref-Bishop-Bishop2024" role="doc-biblioref">Bishop and Bishop, 2024, p. 254</a>)</span> など．<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><span class="citation" data-cites="Bishop-Bishop2024">(<a href="#ref-Bishop-Bishop2024" role="doc-biblioref">Bishop and Bishop, 2024, p. 255</a>)</span> など．<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><span class="citation" data-cites="Wolpert1996">(<a href="#ref-Wolpert1996" role="doc-biblioref">Wolpert, 1996</a>)</span> は最適化の文脈での定理を示した．統計的機械学習の文脈での No free lunch theorem は，<a href="https://twitter.com/aryehazan/status/1609423557796450304">“Any classifier with finite sample error guarantees necessarily needs inductive bias: structural assumptions on either the function class or the sampling distribution.”</a> と説明できる．<span class="citation" data-cites="Shalev-Shwartz-Ben-David2014">(<a href="#ref-Shalev-Shwartz-Ben-David2014" role="doc-biblioref">Shalev-Shwartz and Ben-David, 2014, p. 37</a>)</span> 定理5.1 も参照．<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p><span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 811</a>)</span> 22章など．<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p><span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 812</a>)</span> など．<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>tangent propagation <span class="citation" data-cites="Simard+1991">(<a href="#ref-Simard+1991" role="doc-biblioref">Simard et al., 1991</a>)</span> などがその例である．2の例とも見れる．<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p><span class="citation" data-cites="福水健次2024">(<a href="#ref-福水健次2024" role="doc-biblioref">福水健次, 2024</a>)</span>, <span class="citation" data-cites="Bronstein+2021">(<a href="#ref-Bronstein+2021" role="doc-biblioref">Bronstein et al., 2021, pp. 15–16</a>)</span> など．または <a href="https://ncatlab.org/nlab/show/equivariant+structure">nLab</a> も参照．<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>このような集合 <span class="math inline">\(X,Y\)</span> を <a href="https://ncatlab.org/nlab/show/G-set"><span class="math inline">\(G\)</span>-集合</a> という．<a href="https://ncatlab.org/nlab/show/equivariant">同変性</a> は <span class="math inline">\(G\)</span>-集合の射と見れる．<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/162348\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            trigger: 'click',
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            positionFixed: true,
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "162348/162348.github.io";
    script.dataset.repoId = "R_kgDOKlfKYQ";
    script.dataset.category = "Announcements";
    script.dataset.categoryId = "DIC_kwDOKlfKYc4CgDmb";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://162348.github.io/">
<p>Hirofumi Shiba</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/162348/162348.github.io/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ano2math5">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:shiba.hirofumi@ism.ac.jp">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>