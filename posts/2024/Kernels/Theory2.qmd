---
title: "統計的学習理論２"
subtitle: "PAC-Bayes"
author: "司馬 博文"
date: 3/2/2024
categories: [Kernel, Math Notes]
toc: true
image: Theory.png
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: 統計的機械学習には，「汎化」に価値を置く，独特の決定理論的な枠組みが存在する．特に，現状では経験リスク最小化と正則化とを組み合わせた「構造的リスク最小化」が最もよく見られる．この枠組みから，各手法の優越を評価することとなる．
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
---

{{< include ../../../_preamble.qmd >}}

## PAC-Bayes

通常の機械学習の枠組みでは，仮設集合 $\cH\subset\L(\cX;\cY)$ を固定し，この中で最適な推定量 $\ov{h}\in\cH$ を探すことに集中する．

一方で，PAC-Bayes では，仮設集合 $\cH$ 上の確率分布を学習し，最終的に 投票 (vote) などの確率的な操作によって決めることを考え，これにも対応する理論を構築する．^[[@Alquier2021] Introduction より．]

これは，従来の PAC 学習は最悪評価をしていたとして相対化するような試みであり，より実際の機械学習アルゴリズムへの応用可能性が意識されている．

これは [@Shawe-Taylor-Williamson1997] によって創始され， [@McAllester1999] によって最初の定理が示された．[@Seeger2002], [@Catoni2007] も金字塔であり，後者は情報統計力学との関連を推し進めている．

### 枠組み

データにより決まる確率測度
$$
\wh{\rho}:(\cX\times\cY)^n\to\cP(\cH)
$$
を考え，推定量をランダムに $\wt{h}\sim\wh{\rho}$ とサンプリングする．これを **ランダム推定量** (randomized estimator) という．

例えば $\cY=2$ においては，Gibbs 判別器と呼ばれる．^[[@Scholkopf-Smola2002 p.381] 定義12.23．]

また，最終的な推定量を
$$
h_{\wh{\rho}}:=\wh{\rho}(h)
$$
と決定しても良い．これを **集合推定量** (aggregated predictor) という．

これらのリスク $R(\wh{h}),R(h_{\wh{\rho}})$ を調べるのが PAC-Bayes である．

### KL-乖離度

すると，$\log M$ の項に KL-乖離度が現れる．

::: {.callout-tip icon="false"}
## 
::: {#def-KL-divergence}
## Kullback-Leibler divergence

$\mu,\nu\in\cP(\cH)$ の **Kullback-Leibler 乖離度** とは，
$$
\KL(\mu|\nu):=\begin{cases}
\int_\cH\log\paren{\dd{\mu}{\nu}(\theta)}\mu(d\theta)&\mu\ll\nu,\\
\infty&\otherwise.
\end{cases}
$$
をいう．
:::
:::