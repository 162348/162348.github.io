<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="司馬博文">

<title>階層モデル再論 – Hirofumi Shiba</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/profile.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-23b8e6f3f748bc5bdbba311d511a5b21.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-cbbf670b64ffb0f4f82a1bdb363d1e3a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-kernel-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-title','listing-image','listing-date','listing-subtitle',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-kernel-listing'] = new List('listing-kernel-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Zen+Kurenaido&amp;display=swap" rel="stylesheet">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&amp;display=swap" rel="stylesheet">

<style>
  .navbar-title, .menu-text {
      font-family: "Zen Kurenaido", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
  h1, .title, .description, .subtitle {
    font-family: "Zen Kurenaido", sans-serif !important;
  }
</style>

<!-- <style>
  .menu-text {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
  .navbar-title {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
</style> -->

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../assets/styles.css">
<meta property="og:title" content="階層モデル再論 – Hirofumi Shiba">
<meta property="og:description" content="本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．">
<meta property="og:image" content="https://162348.github.io/posts/2024/Kernels/Images/MM.svg">
<meta property="og:site_name" content="Hirofumi Shiba">
<meta name="twitter:title" content="階層モデル再論 – Hirofumi Shiba">
<meta name="twitter:description" content="本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．">
<meta name="twitter:image" content="https://162348.github.io/posts/2024/Kernels/Images/MM.svg">
<meta name="twitter:creator" content="@ano2math5">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../static/English.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Sessions.html"> 
<span class="menu-text">Sessions</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">ノート</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Japanese.html"> 
<span class="menu-text">自己紹介</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/162348/162348.github.io/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">階層モデル再論</h1>
            <p class="subtitle lead">多変量解析から機械学習へ</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Statistics</div>
                <div class="quarto-category">Kernel</div>
                <div class="quarto-category">Probability</div>
                <div class="quarto-category">Bayesian</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>司馬博文 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">8/12/2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">8/14/2024</p>
      </div>
    </div>
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">概要</div>
      本稿では，線型かつ１層の潜在変数モデルに議論を限り，機械学習と統計学と種々の応用分野での潜在変数モデル／階層モデルの議論を統一的に扱う．
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目次</h2>
   
  <ul>
  <li><a href="#はじめに" id="toc-はじめに" class="nav-link active" data-scroll-target="#はじめに">はじめに</a></li>
  <li><a href="#本稿の目的" id="toc-本稿の目的" class="nav-link" data-scroll-target="#本稿の目的">本稿の目的</a></li>
  <li><a href="#sec-PCA" id="toc-sec-PCA" class="nav-link" data-scroll-target="#sec-PCA"><span class="header-section-number">1</span> 主成分分析 (PCA)</a>
  <ul class="collapse">
  <li><a href="#はじめに-1" id="toc-はじめに-1" class="nav-link" data-scroll-target="#はじめに-1"><span class="header-section-number">1.1</span> はじめに</a></li>
  <li><a href="#概要" id="toc-概要" class="nav-link" data-scroll-target="#概要"><span class="header-section-number">1.2</span> 概要</a></li>
  <li><a href="#主成分分散最大化" id="toc-主成分分散最大化" class="nav-link" data-scroll-target="#主成分分散最大化"><span class="header-section-number">1.3</span> 主成分分散最大化</a></li>
  <li><a href="#計算上の注意" id="toc-計算上の注意" class="nav-link" data-scroll-target="#計算上の注意"><span class="header-section-number">1.4</span> 計算上の注意</a></li>
  <li><a href="#sec-dimension-reduction" id="toc-sec-dimension-reduction" class="nav-link" data-scroll-target="#sec-dimension-reduction"><span class="header-section-number">1.5</span> 線型射影による次元縮約</a></li>
  <li><a href="#因子分析志向の主成分分析" id="toc-因子分析志向の主成分分析" class="nav-link" data-scroll-target="#因子分析志向の主成分分析"><span class="header-section-number">1.6</span> 因子分析志向の主成分分析</a></li>
  </ul></li>
  <li><a href="#sec-FA" id="toc-sec-FA" class="nav-link" data-scroll-target="#sec-FA"><span class="header-section-number">2</span> 因子分析 (FA)</a>
  <ul class="collapse">
  <li><a href="#はじめに-2" id="toc-はじめに-2" class="nav-link" data-scroll-target="#はじめに-2"><span class="header-section-number">2.1</span> はじめに</a></li>
  <li><a href="#概要-1" id="toc-概要-1" class="nav-link" data-scroll-target="#概要-1"><span class="header-section-number">2.2</span> 概要</a></li>
  <li><a href="#確率的アプローチ" id="toc-確率的アプローチ" class="nav-link" data-scroll-target="#確率的アプローチ"><span class="header-section-number">2.3</span> 確率的アプローチ</a></li>
  <li><a href="#スパース推定" id="toc-スパース推定" class="nav-link" data-scroll-target="#スパース推定"><span class="header-section-number">2.4</span> スパース推定</a></li>
  <li><a href="#sec-other-priors-FA" id="toc-sec-other-priors-FA" class="nav-link" data-scroll-target="#sec-other-priors-FA"><span class="header-section-number">2.5</span> その他の事前分布</a></li>
  <li><a href="#非線型化" id="toc-非線型化" class="nav-link" data-scroll-target="#非線型化"><span class="header-section-number">2.6</span> 非線型化</a></li>
  <li><a href="#sec-MixFA" id="toc-sec-MixFA" class="nav-link" data-scroll-target="#sec-MixFA"><span class="header-section-number">2.7</span> 混合モデリング</a></li>
  </ul></li>
  <li><a href="#sec-SEM" id="toc-sec-SEM" class="nav-link" data-scroll-target="#sec-SEM"><span class="header-section-number">3</span> 構造方程式モデリング (SEM)</a>
  <ul class="collapse">
  <li><a href="#はじめに-3" id="toc-はじめに-3" class="nav-link" data-scroll-target="#はじめに-3"><span class="header-section-number">3.1</span> はじめに</a></li>
  <li><a href="#sec-PLS" id="toc-sec-PLS" class="nav-link" data-scroll-target="#sec-PLS"><span class="header-section-number">3.2</span> 部分最小自乗モデル (PLS)</a></li>
  <li><a href="#構造方程式モデリングの発展" id="toc-構造方程式モデリングの発展" class="nav-link" data-scroll-target="#構造方程式モデリングの発展"><span class="header-section-number">3.3</span> 構造方程式モデリングの発展</a></li>
  <li><a href="#計算統計学という要素" id="toc-計算統計学という要素" class="nav-link" data-scroll-target="#計算統計学という要素"><span class="header-section-number">3.4</span> 計算統計学という要素</a></li>
  <li><a href="#sec-CCA" id="toc-sec-CCA" class="nav-link" data-scroll-target="#sec-CCA"><span class="header-section-number">3.5</span> 正準相関分析 (CCA)</a></li>
  </ul></li>
  <li><a href="#sec-MM" id="toc-sec-MM" class="nav-link" data-scroll-target="#sec-MM"><span class="header-section-number">4</span> 混合モデル (MM)</a>
  <ul class="collapse">
  <li><a href="#はじめに-4" id="toc-はじめに-4" class="nav-link" data-scroll-target="#はじめに-4"><span class="header-section-number">4.1</span> はじめに</a></li>
  <li><a href="#正規混合モデル-gmm" id="toc-正規混合モデル-gmm" class="nav-link" data-scroll-target="#正規混合モデル-gmm"><span class="header-section-number">4.2</span> 正規混合モデル (GMM)</a></li>
  <li><a href="#正規スケール混合モデル-gsm" id="toc-正規スケール混合モデル-gsm" class="nav-link" data-scroll-target="#正規スケール混合モデル-gsm"><span class="header-section-number">4.3</span> 正規スケール混合モデル (GSM)</a></li>
  <li><a href="#潜在-dirichlet-配分-lda" id="toc-潜在-dirichlet-配分-lda" class="nav-link" data-scroll-target="#潜在-dirichlet-配分-lda"><span class="header-section-number">4.4</span> 潜在 Dirichlet 配分 (LDA)</a></li>
  <li><a href="#sec-SSM" id="toc-sec-SSM" class="nav-link" data-scroll-target="#sec-SSM"><span class="header-section-number">4.5</span> 状態空間モデル (SSM)</a></li>
  </ul></li>
  <li><a href="#sec-ICA" id="toc-sec-ICA" class="nav-link" data-scroll-target="#sec-ICA"><span class="header-section-number">5</span> 独立成分分析 (ICA)</a>
  <ul class="collapse">
  <li><a href="#はじめに-6" id="toc-はじめに-6" class="nav-link" data-scroll-target="#はじめに-6"><span class="header-section-number">5.1</span> はじめに</a></li>
  <li><a href="#推定手法" id="toc-推定手法" class="nav-link" data-scroll-target="#推定手法"><span class="header-section-number">5.2</span> 推定手法</a></li>
  <li><a href="#非線型化-1" id="toc-非線型化-1" class="nav-link" data-scroll-target="#非線型化-1"><span class="header-section-number">5.3</span> 非線型化</a></li>
  </ul></li>
  
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
<p>$$</p>
<p>$$</p>
</div>
<section id="はじめに" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="はじめに">はじめに</h2>
<p>潜在変数模型とはどうやらとんでもなく広い射程を持った対象であるようである．</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled" title="潜在変数モデルとは……">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
潜在変数モデルとは……
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>心理学，経済学をはじめとして多くの分野で中心的に扱われてきたモデルである（<a href="https://ja.wikipedia.org/wiki/共分散構造分析">構造方程式モデル</a>，因子分析，項目応答モデル，同時方程式モデルなど）．</li>
<li>ベイズ統計学では <strong>階層モデル</strong> (hierarchical model) として極めて重要な役割を果たす．</li>
<li>生成モデリング（<a href="../../../posts/2024/Kernels/Deep4.html">VAE</a>, <a href="../../../posts/2024/Samplers/EBM.html">EBM</a>, <a href="../../../posts/2024/Samplers/Diffusion.html">Diffusion</a>, <a href="../../../posts/2024/Kernels/Deep3.html">GAN</a>, <a href="../../../posts/2024/Computation/PGM1.html">Probabilistic Graphical Model</a>）も，観測変数上の周辺分布がデータ分布に近づくように潜在変数模型を学習する方法である．</li>
<li>認知科学において，脳も潜在変数模型に基いてメンタルモデルを構成しているという仮説もある（<a href="../../../posts/2024/Kernels/NCL.html#sec-InfoMax">InfoMax に関する稿</a>も参照）．表現学習や独立成分分析の指導原理になっている側面がある．</li>
<li>情報理論において，通信路の組み合わせは潜在変数模型としてモデリングできる．さらには，潜在変数模型は数学的には確率空間の圏上の図式であるとして研究されている <span class="citation" data-cites="Perrone2024">(<a href="#ref-Perrone2024" role="doc-biblioref">Perrone, 2024</a>)</span>．</li>
</ol>
</div>
</div>
<p>このように種々の文脈で登場する潜在変数模型であるが，<u><strong>それぞれの文脈において「潜在変数」の果たす役割は全く違う</strong></u>．</p>
<p>しかし，数学的には全く同じ枠組みで記述できる．従って，そのように扱うことは一定の価値を持つだろう．</p>
<p>実際，近年になり，これから本稿で解説するように，潜在変数モデルの観点から心理学，経済学，環境科学，遺伝学，信号処理，逆問題，社会学，政治科学，マーケティング分野で独自に発展した手法が，特定の手法の特別な場合と見れるという理解が進み，手法の交流と知見の交換が進んでいる．</p>
</section>
<section id="本稿の目的" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="本稿の目的">本稿の目的</h2>
<p>本稿では主成分分析，因子分析，構造方程式モデリング，混合モデル，独立成分分析を，<u>潜在変数モデルとして解釈し，図式で理解する</u>．</p>
<p>確率変数を丸つきの大文字で表し，<span class="math inline">\(X^i,Y^i\)</span> は観測変数，<span class="math inline">\(Z^i\)</span> は潜在変数を表す．矢印は <a href="../../../posts/2024/Probability/Kernel.html">確率核</a> を表す．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/MM.svg" class="img-fluid figure-img"></p>
<figcaption>混合モデル（第 <a href="#sec-MM" class="quarto-xref">4</a> 節）</figcaption>
</figure>
</div>
<p>種々の <strong>多変量解析法</strong> を（ベイズ）階層モデルとして統一的に理解すると同時に，それぞれの文脈での「使い方の違い」に注目することを目指す．</p>
</section>
<section id="sec-PCA" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="sec-PCA"><span class="header-section-number">1</span> 主成分分析 (PCA)</h2>
<section id="はじめに-1" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="はじめに-1"><span class="header-section-number">1.1</span> はじめに</h3>
<p>主成分分析では，<span class="math inline">\(p\)</span> 次元のデータ <span class="math inline">\(\{x_i\}_{i=1}^n\subset\mathbb{R}^p\)</span> の各成分を，より少数の潜在変数を持った１層の線型 Gauss 模型</p>
<p><img src="Images/PCA.svg" class="img-fluid"></p>
<p>で説明しようとする．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>歴史的に主成分分析は，おろした垂線の足の二乗距離和の意味でコストが最小になるような線型射影を求める問題 <span class="citation" data-cites="Pearson01-PCA">(<a href="#ref-Pearson01-PCA" role="doc-biblioref">Pearson, 1901</a>)</span> として最初に登場し，値の分散が最大となるような線型射影を求める問題 <span class="citation" data-cites="Hotelling33-PCA">(<a href="#ref-Hotelling33-PCA" role="doc-biblioref">Hotelling, 1933</a>)</span> として PCA の名前がつき，心理学分野，特に psychometrika で取り上げられて大きく発展した．</p>
<p>このような潜在変数モデルとしての見方は probabilistic PCA <span class="citation" data-cites="Tipping-Bishop1999">(<a href="#ref-Tipping-Bishop1999" role="doc-biblioref">Tipping and Bishop, 1999</a>)</span> / SPCA (Sensible PCA) <span class="citation" data-cites="Roweis1997">(<a href="#ref-Roweis1997" role="doc-biblioref">Roweis, 1997</a>)</span> として，<a href="#sec-FA">因子分析</a>から逆輸入する形で初めて自覚された見方である（第 <a href="#sec-PPCA" class="quarto-xref">2.3.1</a> 節も参照）．</p>
<p>確率的な見地から見れば，正規性を仮定した変数 <span class="math inline">\(Z^1,\cdots,Z^r\)</span> の事前分布が互いに独立なデルタ分布に縮退している場合が古典的な PCA である <span class="citation" data-cites="Roweis1997">(<a href="#ref-Roweis1997" role="doc-biblioref">Roweis, 1997</a>)</span>．</p>
<p>いずれの場合も追加の過程なくしてモデルは識別可能性がなく，後続タスクに応じて種々の制約を追加することで所望の解を得る，という動的な使い方がなされる．</p>
<p>以降，<span class="math inline">\(X\in\mathcal{L}(\Omega;\mathbb{R}^p),Z\in\mathcal{L}(\Omega;\mathbb{R}^r)\)</span> を確率変数， <span class="math display">\[
\boldsymbol{X}=(x_i^j)\in M_{n,p}(\mathbb{R}),\boldsymbol{Z}=(z_i^j)\in M_{n,r}(\mathbb{R})
\]</span> を行列することに注意．</p>
</section>
<section id="概要" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="概要"><span class="header-section-number">1.2</span> 概要</h3>
<p>PCA ではデータ行列を <span class="math display">\[
\boldsymbol{X}:=\begin{pmatrix}x_1^\top\\\vdots\\x_n^\top\end{pmatrix}\in M_{n,p}(\mathbb{R})
\]</span> で定めたとき，データ次元 <span class="math inline">\(p\)</span> より小さい数の成分 <span class="math inline">\(r\)</span> で説明しようとする： <span class="math display">\[
\boldsymbol{X}\approx\boldsymbol{Z}C^\top,\qquad\boldsymbol{Z}:=\begin{pmatrix}z_1^\top\\\vdots\\z_n^\top\end{pmatrix}\in M_{n,r}(\mathbb{R}),C\in M_{p,r}(\mathbb{R}).
\]</span></p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li>古典的には，<span class="math inline">\(z_{ij}\)</span> を（主成分）<strong>得点</strong> (score)，<span class="math inline">\(Z^i\)</span> を <strong>合成変量</strong>，<span class="math inline">\(c_{ij}\)</span> を <strong>負荷量</strong> (loading) ともいう <span class="citation" data-cites="足立-山本2024">(<a href="#ref-足立-山本2024" role="doc-biblioref">足立浩平 and 山本倫生, 2024</a>)</span>．</li>
<li>機械学習では <span class="math inline">\(Z^1,\cdots,Z^r\)</span> を <strong>潜在因子</strong>，<span class="math inline">\(W\in M_{pr}(\mathbb{R})\)</span> を <strong>荷重</strong> (weight) ともいう <span class="citation" data-cites="Murphy2022">(<a href="#ref-Murphy2022" role="doc-biblioref">Murphy, 2022</a>)</span>．</li>
</ul>
</div>
</div>
</div>
<p>この問題は <span class="math inline">\(\boldsymbol{X}\)</span> の <a href="../../../posts/2024/FunctionalAnalysis/SVD.html">特異値分解</a> (SVD) <span class="math inline">\(\boldsymbol{X}=U\Sigma V^\top\)</span> により解ける： <span class="math display">\[
\boldsymbol{Z}=U\Sigma_{1:r}^\alpha A=\boldsymbol{X}(\underbrace{V\Sigma^{\alpha-1}_{1:r}A}_{=:W}),\qquad C:=V\Sigma_{1:r}^{1-\alpha}(A^{-1})^\top.
\]</span> ただし，<span class="math inline">\(\alpha\in\mathbb{R},A\in\mathrm{GL}_p(\mathbb{R})\)</span> は任意である．この解は，特異値分解の性質により，残差を Hilbert-Schmidt ノルムの意味で最小にする： <span id="eq-PCA-objective"><span class="math display">\[
\min_C\|\boldsymbol{X}-\boldsymbol{Z}C^\top\|_\mathrm{HS}=\min_C\frac{1}{n}\sum_{i=1}^n\lvert x_i-Cz_i\rvert^2=\sigma_{r+1}
\tag{1}\]</span></span> この目的関数は復元誤差とも理解できる．ただし，<span class="math inline">\(\sigma_{r+1}\)</span> は行列 <span class="math inline">\(\boldsymbol{Z}C\)</span> の第 <span class="math inline">\(r+1\)</span> 特異値である．</p>
</section>
<section id="主成分分散最大化" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="主成分分散最大化"><span class="header-section-number">1.3</span> 主成分分散最大化</h3>
<p>荷重行列 <span class="math inline">\(W\)</span> が <span class="math inline">\(W^\top W=I_r\)</span> を満たすという制約条件を追加すると，目的関数 (<a href="#eq-PCA-objective" class="quarto-xref">1</a>) は潜在変数の分散を最大にすることと等価になる： <span id="eq-PCA-objective2"><span class="math display">\[
\operatorname*{argmin}_{W}\|\boldsymbol{X}-\boldsymbol{Z}W\|_\mathrm{HS}=\operatorname*{argmin}_W\operatorname{Tr}((\boldsymbol{X}W)^\top\boldsymbol{X}W).
\tag{2}\]</span></span></p>
<p>すなわち，<span class="math inline">\(\boldsymbol{Z}=\boldsymbol{X}W\)</span> の変動が差大になるようにすれば良い．</p>
<p>そのためには，確率変数 <span class="math inline">\(X\)</span> のデータ行列 <span class="math inline">\(\boldsymbol{X}\)</span> から計算した経験共分散行列 <span class="math inline">\(S\in M_{p}(\mathbb{R})_+\)</span> の固有ベクトルのうち，対応する固有値が大きいものから <span class="math inline">\(w_1,\cdots,w_r\)</span> として荷重行列とすれば良い： <span class="math display">\[
W:=(w_1\;\cdots\;w_r).
\]</span></p>
<p>実はこれは解の１つに過ぎず，<span class="math inline">\(W\)</span> に右から直交行列を乗じて「回転」させたものは全て解になる．上の解は追加の条件 <span class="math inline">\(Z^\top Z=I_r\)</span> を課すことで特定される．</p>
</section>
<section id="計算上の注意" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="計算上の注意"><span class="header-section-number">1.4</span> 計算上の注意</h3>
<p>各次元に関する長さのスケールを揃えるために，PCA を始める前にデータを正規化しておくか，または共分散行列 <span class="math inline">\(S\)</span> の代わりに，相関行列を用いるべきである．</p>
<p>また，実際に最適化や相関行列の固有値分解をすることはなく，基本的に SVD の方が <span class="math inline">\(O(np^2)+O(p^3)\)</span> と高速である <span class="citation" data-cites="Unkel-Trendafilov2010">(<a href="#ref-Unkel-Trendafilov2010" role="doc-biblioref">Unkel and Trendafilov, 2010</a>)</span>．</p>
<p>さらに次元 <span class="math inline">\(p\)</span> が高い場合は，<strong>確率的 SVD</strong> <span class="citation" data-cites="Halko+2011">(<a href="#ref-Halko+2011" role="doc-biblioref">Halko et al., 2011</a>)</span>, <span class="citation" data-cites="Drineas+2016">(<a href="#ref-Drineas+2016" role="doc-biblioref">Drineas and Mahoney, 2016</a>)</span> を用いてさらに <span class="math inline">\(O(nr^2)+(r^3)\)</span> まで削減できる．このような手法は確率的数値解析と呼ばれる <span class="citation" data-cites="Murray+2023">(<a href="#ref-Murray+2023" role="doc-biblioref">Murray et al., 2023</a>)</span>．</p>
</section>
<section id="sec-dimension-reduction" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="sec-dimension-reduction"><span class="header-section-number">1.5</span> 線型射影による次元縮約</h3>
<p><span class="math inline">\(W^\top W=I_r\)</span> の仮定の下で，PCA の目的関数 (<a href="#eq-PCA-objective" class="quarto-xref">1</a>) は，潜在変数の分散最大化 (<a href="#eq-PCA-objective2" class="quarto-xref">2</a>) と見れるのだった．</p>
<p>これは同じ仮定の下で，データ変数 <span class="math inline">\(X\)</span> の最小誤差の線型射影を求める問題とも見れる： <span class="math display">\[
\operatorname*{argmin}_W\|\boldsymbol{X}-\boldsymbol{Z}W\|_\mathrm{HS}=\operatorname*{argmin}_W\|\boldsymbol{X}-\boldsymbol{X}WW^\top\|_\mathrm{HS}.
\]</span></p>
<p>なお，一般の行列 <span class="math inline">\(A\)</span> について <span class="math inline">\(P_A=A(A^{-1}A)^+A^\top\)</span> は <span class="math inline">\(\mathrm{Im}\,A\)</span> 上の直交射影になる．<span class="math inline">\(A\)</span> が直交行列であるとき，<span class="math inline">\(P_A=AA^\top\)</span> が成り立つ．</p>
</section>
<section id="因子分析志向の主成分分析" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="因子分析志向の主成分分析"><span class="header-section-number">1.6</span> 因子分析志向の主成分分析</h3>
<p>因子分析では，<span class="math inline">\(Z^1,\cdots,Z^r\)</span> を対等な因子と見て，それぞれのデータへの影響を調べたい．このような場合は， <span class="math display">\[
\frac{1}{n}\boldsymbol{Z}^\top\boldsymbol{Z}=I_r
\]</span> が自然な制約になる．この際の解は，直交行列 <span class="math inline">\(T\in O_r(\mathbb{R})\)</span> の違いを除いて， <span class="math display">\[
\boldsymbol{Z}=\sqrt{n}UT,\qquad C=\frac{1}{\sqrt{n}}V\Sigma_{1:r}T,\qquad W=\sqrt{n}V\Sigma_{1:r}^{-1}T,
\]</span> まで確定する．</p>
<p>しばしば，追加の仮定 <span class="math display">\[
C^\top C=\mathrm{diag}(\rho_{1:r}),\qquad \rho_1\ge\cdots\ge\rho_r\ge0
\]</span> を課して得られる一意な解 <span class="math display">\[
\boldsymbol{Z}=\sqrt{n}U,\qquad C=\frac{1}{\sqrt{n}}V\Sigma_{1:r},\qquad W=\sqrt{n}V\Sigma_{1:r}^{-1},
\]</span> を <strong>初期解</strong> と呼び，これを「回転」させることで他の解が探索され，所望の分解を探す．</p>
<p>因子分析では <span class="citation" data-cites="Thurstone1947">(<a href="#ref-Thurstone1947" role="doc-biblioref">Thurstone, 1947</a>)</span> 以来，種々の回転法とアルゴリズムが蓄積している <span class="citation" data-cites="足立-山本2024">(<a href="#ref-足立-山本2024" role="doc-biblioref">足立浩平 and 山本倫生, 2024</a>)</span>．一般にこの文脈では，<span class="citation" data-cites="Thurstone1947">(<a href="#ref-Thurstone1947" role="doc-biblioref">Thurstone, 1947</a>)</span> にいう「単純構造」を達成した，解釈が容易な因子をドメイン知識に基づいて構成することを目指す．この「単純構造」とは，現代でいう一種の disentangled factor と理解できる．</p>
</section>
</section>
<section id="sec-FA" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-FA"><span class="header-section-number">2</span> 因子分析 (FA)</h2>
<section id="はじめに-2" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="はじめに-2"><span class="header-section-number">2.1</span> はじめに</h3>
<p>主成分分析が「低階数近似」ならば，因子分析は「高階数近似」というべきである <span class="citation" data-cites="足立浩平2023">(<a href="#ref-足立浩平2023" role="doc-biblioref">足立浩平, 2023</a>)</span>．</p>
<p><img src="Images/FA.svg" class="img-fluid"></p>
<p>より正確には，因子分析は，観測の各次元 <span class="math inline">\(X^1,\cdots,X^p\)</span> ごとに「独自因子」<span class="math inline">\(Z^1,\cdots,Z^p\)</span> を想定しつつ，全観測に共通する「共通因子」<span class="math inline">\(F^1,\cdots,F^r\)</span> をどのように抽出できるかを考える，という志向性を持つ：</p>
<p><img src="Images/FA2.svg" class="img-fluid"></p>
<p>この意味では，FA は独自因子 <span class="math inline">\(U^1,\cdots,U^p\)</span> を追加した PCA とも理解できる．</p>
<p>歴史的には <span class="citation" data-cites="Spearman1904">(<a href="#ref-Spearman1904" role="doc-biblioref">Spearman, 1904</a>)</span> が古典テスト理論の文脈で <span class="math inline">\(r=1\)</span> の因子分析を，<span class="citation" data-cites="Thurstone1947">(<a href="#ref-Thurstone1947" role="doc-biblioref">Thurstone, 1947</a>)</span> が一般の <span class="math inline">\(1\le r&lt;p\)</span> の場合の因子分析を「回転」の手法と共に導入した．</p>
<p>さらに興味深いことに，FA では PCA をはじめとした多くの多変量分析手法と違い，<span class="citation" data-cites="Lawley1942">(<a href="#ref-Lawley1942" role="doc-biblioref">Lawley, 1942</a>)</span>, <span class="citation" data-cites="Anderson-Rubin1956">(<a href="#ref-Anderson-Rubin1956" role="doc-biblioref">Anderson and Rubin, 1956</a>)</span> らにより，初期から確率的な扱いが発展した手法である <span class="citation" data-cites="足立-山本2024">(<a href="#ref-足立-山本2024" role="doc-biblioref">足立浩平 and 山本倫生, 2024</a>)</span>．</p>
<p>FA に倣う形で，PCA にも確率論的なアプローチが導入された <span class="citation" data-cites="Tipping-Bishop1999">(<a href="#ref-Tipping-Bishop1999" role="doc-biblioref">Tipping and Bishop, 1999</a>)</span>, <span class="citation" data-cites="Roweis1997">(<a href="#ref-Roweis1997" role="doc-biblioref">Roweis, 1997</a>)</span>．</p>
</section>
<section id="概要-1" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="概要-1"><span class="header-section-number">2.2</span> 概要</h3>
<p>FA では <span class="math inline">\(\boldsymbol{Z}=(\boldsymbol{F}\;\boldsymbol{U})\in M_{n,r+p}(\mathbb{R})\)</span> の分解に基づき， <span class="math display">\[
\boldsymbol{X}\approx\boldsymbol{F}A^\top+\boldsymbol{U}\Psi^{1/2},\qquad A\in M_{r,p}(\mathbb{R}),\Psi=\mathrm{diag}(\psi_1,\cdots,\psi_p)\in M_p(\mathbb{R}),
\]</span> によってデータ行列 <span class="math inline">\(\boldsymbol{X}\in M_{n,p}(\mathbb{R})\)</span> を説明しようとする．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>PCA よりさらに識別可能性は絶望的であるが，FA では潜在変数の解釈可能性担保のため，次の仮定を課す： <span class="math display">\[
\boldsymbol{1}_n^\top\boldsymbol{F}=\boldsymbol{0}_r,\qquad \boldsymbol{1}_n^\top\boldsymbol{U}=\boldsymbol{0}_p,
\]</span> <span class="math display">\[
\boldsymbol{F}^\top\boldsymbol{F}=n\boldsymbol{I}_r,\qquad \boldsymbol{U}^\top\boldsymbol{U}=n\boldsymbol{I}_p,\qquad\boldsymbol{F}^\top\boldsymbol{U}=O.
\]</span> すなわち，推定される確率変数 <span class="math inline">\(F,U\)</span> が標準化されていて互いに無相関であるように誘導する．</p>
<p>また，<span class="math inline">\(\boldsymbol{U}\)</span> の経験分散が <span class="math inline">\(\Psi\)</span> になることに注意．</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li>古典的には，<span class="math inline">\(f_{ij}\)</span> を共通因子，<span class="math inline">\(\psi_j\)</span> を独自因子の <strong>得点</strong> (score)，<span class="math inline">\(a_{ij}\)</span> を <strong>負荷量</strong> (loading) ともいう <span class="citation" data-cites="足立-山本2024">(<a href="#ref-足立-山本2024" role="doc-biblioref">足立浩平 and 山本倫生, 2024</a>)</span>．</li>
<li>機械学習では <span class="math inline">\(Z^1,\cdots,Z^r\)</span> を <strong>潜在因子</strong>，<span class="math inline">\(W\in M_{pr}(\mathbb{R})\)</span> を <strong>荷重</strong> (weight) ともいう <span class="citation" data-cites="Murphy2022">(<a href="#ref-Murphy2022" role="doc-biblioref">Murphy, 2022</a>)</span>．</li>
</ul>
</div>
</div>
</div>
<p>この問題は，<span class="math inline">\(C:=(A\;\Psi^{1/2})\)</span> と定めると，PCA と同じ問題 (<a href="#eq-PCA-objective" class="quarto-xref">1</a>) に帰着される： <span class="math display">\[
\min_C\|\boldsymbol{X}-\boldsymbol{Z}C^\top\|_\mathrm{HS}.
\]</span></p>
<p>これはやはり特異値分解により解くことができる <span class="citation" data-cites="DeLeeuw04-SimultaneousEstimationOfEFA">(<a href="#ref-DeLeeuw04-SimultaneousEstimationOfEFA" role="doc-biblioref">De Leeuw, 2004</a>)</span>．</p>
<p>解は直交行列による回転を除いても，やはり一意に定まらないようである．</p>
</section>
<section id="確率的アプローチ" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="確率的アプローチ"><span class="header-section-number">2.3</span> 確率的アプローチ</h3>
<p>ここで， <span class="math display">\[
U:=\begin{pmatrix}U^1\\\vdots\\U^p\end{pmatrix}\in\mathcal{L}(\Omega;\mathbb{R}^p),\qquad F:=\begin{pmatrix}F^1\\\vdots\\F^r\end{pmatrix}\in\mathcal{L}(\Omega;\mathbb{R}^r),
\]</span> を確率変数とすると， <span id="eq-probabilistic-FA"><span class="math display">\[
X\approx AF+\Psi^{1/2}U
\tag{3}\]</span></span> によって <span class="math inline">\(X\)</span> に確率モデルが誘導されることになる．</p>
<section id="sec-PPCA" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="sec-PPCA"><span class="header-section-number">2.3.1</span> 正規性の仮定</h4>
<p><span class="math inline">\(U,F\)</span> に正規性の仮定をおけば，このモデルは EM アルゴリズムなどを用いて最尤推定できる <span class="citation" data-cites="Rubin-Thayer1982">(<a href="#ref-Rubin-Thayer1982" role="doc-biblioref">Rubin and Thayer, 1982</a>)</span>, <span class="citation" data-cites="Ghahramani-Hinton1996">(<a href="#ref-Ghahramani-Hinton1996" role="doc-biblioref">Ghahramani and Hinton, 1996</a>)</span>．このような最尤推定のアプローチは <span class="citation" data-cites="Lawley1942">(<a href="#ref-Lawley1942" role="doc-biblioref">Lawley, 1942</a>)</span> から考えられていた．</p>
<p>この見方が PCA にも応用された．追加の仮定 <span class="math display">\[
A^\top A=I_{r},\qquad \Psi=\sigma^2I_p,
\]</span> の下での FA への確率論的アプローチを probabilistic PCA <span class="citation" data-cites="Tipping-Bishop1999">(<a href="#ref-Tipping-Bishop1999" role="doc-biblioref">Tipping and Bishop, 1999</a>)</span> / SPCA (Sensible PCA) <span class="citation" data-cites="Roweis1997">(<a href="#ref-Roweis1997" role="doc-biblioref">Roweis, 1997</a>)</span> という．</p>
<p><span class="math inline">\(\sigma\to0\)</span> の極限で古典的 PCA が回復される．</p>
</section>
<section id="共分散構造分析" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="共分散構造分析"><span class="header-section-number">2.3.2</span> 共分散構造分析</h4>
<p>一方で，分布の仮定は課さず，<span class="math inline">\(X\)</span> の経験分散 <span class="math inline">\(S\)</span> を，式 (<a href="#eq-probabilistic-FA" class="quarto-xref">3</a>) の右辺の共分散 <span class="math display">\[
\Sigma:=AA^\top+\Psi
\]</span> となるべく近づけるように学習する方法もある．</p>
<p>例えば <span class="citation" data-cites="Harman-Jones1966">(<a href="#ref-Harman-Jones1966" role="doc-biblioref">Harman and Jones, 1966</a>)</span>, <span class="citation" data-cites="Harman-Fukuda1966">(<a href="#ref-Harman-Fukuda1966" role="doc-biblioref">Harman and Fukuda, 1966</a>)</span> では，Hilbert-Schmidt ノルム <span class="math inline">\(\|S-\Sigma\|_\mathrm{HS}\)</span> の最小化することで解を探索する方法が考慮された．</p>
<p>このように，データの共分散行列を低階数近似するアプローチは <strong>共分散構造分析</strong> <span class="citation" data-cites="Bock-Bargmann1966">(<a href="#ref-Bock-Bargmann1966" role="doc-biblioref">Bock and Bargmann, 1966</a>)</span> ともいう．</p>
<p>さらに，確率論的なアプローチは一般の構造方程式モデル (SEM, 次節 <a href="#sec-SEM" class="quarto-xref">3</a> 参照) へと発展 <span class="citation" data-cites="Joreskog70">(<a href="#ref-Joreskog70" role="doc-biblioref">Karl Gustav Jöreskog, 1970</a>)</span>, <span class="citation" data-cites="Sorbom1974">(<a href="#ref-Sorbom1974" role="doc-biblioref">Sörbom, 1974</a>)</span>, <span class="citation" data-cites="Joreskog1978">(<a href="#ref-Joreskog1978" role="doc-biblioref">Karl G. Jöreskog, 1978</a>)</span> し，現状，共分散構造分析は SEM の特別な場合と解される．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
</section>
</section>
<section id="スパース推定" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="スパース推定"><span class="header-section-number">2.4</span> スパース推定</h3>
<p>FA のモデルは識別可能とは程遠く，解釈可能性が重要である．<span class="citation" data-cites="Thurstone1947">(<a href="#ref-Thurstone1947" role="doc-biblioref">Thurstone, 1947</a>)</span> は因子付加行列が「単純構造」を持つことを一つの指標としたが，現代的にはスパース推定の言葉で与えられた <strong>完全単純構造</strong> <span class="citation" data-cites="Bernaards-Jennrich2003">(<a href="#ref-Bernaards-Jennrich2003" role="doc-biblioref">Bernaards and Jennrich, 2003</a>)</span> を仮定することが増えてきた．</p>
<p><strong>スパース PCA</strong> <span class="citation" data-cites="Zou+2006">(<a href="#ref-Zou+2006" role="doc-biblioref">Zou et al., 2006</a>)</span>, <span class="citation" data-cites="Jolliffe+2003">(<a href="#ref-Jolliffe+2003" role="doc-biblioref">Ian T Jolliffe and Uddin, 2003</a>)</span> では，従来の SVD + 回転ではなく，LASSO 様の <span class="math inline">\(L^1\)</span>-正則化項によって，解釈可能な因子付加行列を得ようとする．最終的に得られる目的関数は elastic net <span class="citation" data-cites="Zou-Hastie2005">(<a href="#ref-Zou-Hastie2005" role="doc-biblioref">Zou and Hastie, 2005</a>)</span> 様になる．</p>
<p>等価だが，自動関連度決定 (ARD) を用いた <strong>Bayesian PCA</strong> <span class="citation" data-cites="Bishop1998">(<a href="#ref-Bishop1998" role="doc-biblioref">Bishop, 1998</a>)</span>, <span class="citation" data-cites="Archambeau-Bach2008">(<a href="#ref-Archambeau-Bach2008" role="doc-biblioref">Archambeau and Bach, 2008</a>)</span> や spike-and-slab <span class="citation" data-cites="Rattray+2009">(<a href="#ref-Rattray+2009" role="doc-biblioref">Rattray et al., 2009</a>)</span> など，スパース性を促す事前分布を用いることもできる．</p>
</section>
<section id="sec-other-priors-FA" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="sec-other-priors-FA"><span class="header-section-number">2.5</span> その他の事前分布</h3>
<p>非正規な事前分布（特に Laplace 分布やロジスティック分布などの裾の重いもの）を用いることで，モデルが識別可能性を回復することがある．</p>
<p>このように，<a href="../../../posts/2024/Kernels/NCL.html#sec-identifiability">一般の設定で潜在変数モデルが識別可能になるための条件が，非線型独立分析の分野で提案されている</a> <span class="citation" data-cites="Khemakhem+2020">(<a href="#ref-Khemakhem+2020" role="doc-biblioref">Khemakhem et al., 2020</a>)</span>．</p>
<section id="gamma-分布" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="gamma-分布"><span class="header-section-number">2.5.1</span> Gamma 分布</h4>
<p>また，Gamma 事前分布は非負かつスパースな表現を促進し，カウントデータとよく用いられる <span class="citation" data-cites="Canny2004">(<a href="#ref-Canny2004" role="doc-biblioref">Canny, 2004</a>)</span>．</p>
<p>これは環境科学分野の Positive Matrix Factorization <span class="citation" data-cites="Paatero-Tapper1994">(<a href="#ref-Paatero-Tapper1994" role="doc-biblioref">Paatero and Tapper, 1994</a>)</span> や信号処理分野の Nonnegative Matrix Factorization (NMF) <span class="citation" data-cites="Lee-Seung1999">(<a href="#ref-Lee-Seung1999" role="doc-biblioref">Lee and Seung, 1999</a>)</span> の，確率論的な一般化と見れる <span class="citation" data-cites="Buntine-Jakulin2006">(<a href="#ref-Buntine-Jakulin2006" role="doc-biblioref">Buntine and Jakulin, 2006</a>)</span>．</p>
</section>
<section id="dirichlet-分布" class="level4" data-number="2.5.2">
<h4 data-number="2.5.2" class="anchored" data-anchor-id="dirichlet-分布"><span class="header-section-number">2.5.2</span> Dirichlet 分布</h4>
<p>また，Dirichlet 事前分布を用いることで，潜在変数 <span class="math inline">\(Z\in\mathcal{L}(\Omega;\mathbb{R}^r)\)</span> に <span class="math display">\[
\sum_{i=1}^rZ^i=1
\]</span> が課されるため，「各次元への依存度」のような意味づけが可能になる．これは政治学における空間分析において，「どの立場への傾倒が強いか」を推定することにも用いられる <span class="citation" data-cites="Buntine-Jakulin2006">(<a href="#ref-Buntine-Jakulin2006" role="doc-biblioref">Buntine and Jakulin, 2006</a>)</span>．</p>
<p>このモデルは multinomial PCA <span class="citation" data-cites="Buntine-Jakulin2006">(<a href="#ref-Buntine-Jakulin2006" role="doc-biblioref">Buntine and Jakulin, 2006</a>)</span> の他に，遺伝学で admixture <span class="citation" data-cites="Pritchard+2000">(<a href="#ref-Pritchard+2000" role="doc-biblioref">Pritchard et al., 2000</a>)</span>，simplex factor analysis <span class="citation" data-cites="Bhattacharya-Dunson2012">(<a href="#ref-Bhattacharya-Dunson2012" role="doc-biblioref">Bhattacharya and Dunson, 2012</a>)</span>, 科学出版で mixed-membership model <span class="citation" data-cites="Erosheva+2004">(<a href="#ref-Erosheva+2004" role="doc-biblioref">Erosheva et al., 2004</a>)</span>，マーケティングで user rating profile model <span class="citation" data-cites="Marlin2003">(<a href="#ref-Marlin2003" role="doc-biblioref">Marlin, 2003</a>)</span> など，種々の分野で独立に提案されている．</p>
</section>
</section>
<section id="非線型化" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="非線型化"><span class="header-section-number">2.6</span> 非線型化</h3>
<p>FA の一般化の方向性として，正規性の緩和の他に，線型性の緩和があり得る．</p>
<p>MCMC による推論 <span class="citation" data-cites="Hoffman2017">(<a href="#ref-Hoffman2017" role="doc-biblioref">Hoffman, 2017</a>)</span> をすることも，または指数型分布 <span class="citation" data-cites="Collins+2001">(<a href="#ref-Collins+2001" role="doc-biblioref">Collins et al., 2001</a>)</span> への拡張や，VAE による非線型化を通じて変分推論をすることも考えられる．</p>
<p><a href="../../../posts/2024/Kernels/Deep4.html#sec-AE">自己符号化器</a> は，まさに非線型な潜在変数モデルに対する最尤推定を行っており，４層以上のニューラルネットワークを用いることで PCA を非線型化して一般化することができる．<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>また，カーネル法と Gauss 過程により非線型化することもできる <span class="citation" data-cites="Lawrence2005">(<a href="#ref-Lawrence2005" role="doc-biblioref">Lawrence, 2005</a>)</span>．</p>
</section>
<section id="sec-MixFA" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="sec-MixFA"><span class="header-section-number">2.7</span> 混合モデリング</h3>
<p>複数の線型 Gauss 因子分析モデルの重ね合わせとみなす <strong>mixture of factor analysers</strong> <span class="citation" data-cites="Ghahramani-Hinton1996">(<a href="#ref-Ghahramani-Hinton1996" role="doc-biblioref">Ghahramani and Hinton, 1996</a>)</span> も単純ながら表現が高く，EM アルゴリズムや SGD <span class="citation" data-cites="Ricahrdson-Weiss2018">(<a href="#ref-Ricahrdson-Weiss2018" role="doc-biblioref">Richardson and Weiss, 2018</a>)</span>, <span class="citation" data-cites="Zong+2018">(<a href="#ref-Zong+2018" role="doc-biblioref">Zong et al., 2018</a>)</span> によって推定できる．</p>
<p><span class="citation" data-cites="Ricahrdson-Weiss2018">(<a href="#ref-Ricahrdson-Weiss2018" role="doc-biblioref">Richardson and Weiss, 2018</a>)</span> では生成モデルとしての性能も GAN と劣らないこと，VAE や GAN などの生成モデルよりも分布へのフィッティングが良いことを報告している．</p>
<p>さらにこのアプローチはノンパラメトリックベイズ法につながる．この方法では，例えば <span class="citation" data-cites="Paisley-Carin2009">(<a href="#ref-Paisley-Carin2009" role="doc-biblioref">Paisley and Carin, 2009</a>)</span> では Beta 過程事前分布をおき，Gibbs サンプラーで推論することで，混合数 <span class="math inline">\(K\)</span> も同時に自動で決定できる．</p>
</section>
</section>
<section id="sec-SEM" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-SEM"><span class="header-section-number">3</span> 構造方程式モデリング (SEM)</h2>
<section id="はじめに-3" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="はじめに-3"><span class="header-section-number">3.1</span> はじめに</h3>
<p><span class="citation" data-cites="Joreskog1969">(<a href="#ref-Joreskog1969" role="doc-biblioref">K. G. Jöreskog, 1969</a>)</span> は因子分析モデルを潜在変数モデルとして，事前情報を取り入れるなど柔軟に用いた．</p>
<p>特に，データを（現代でいう）訓練データと検証データに分けて，因子分析により推定された潜在変数間の関数関係を検定するための方法を提案し <span class="citation" data-cites="Joreskog-Lawley1968">(<a href="#ref-Joreskog-Lawley1968" role="doc-biblioref">K. G. Jöreskog and Lawley, 1968</a>)</span>，これを <strong>検証的因子分析</strong> (Confirmatory FA) と呼び，それ以前の手法に <strong>探索的因子分析</strong> というレトロニムを与えた．<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>最終的に，潜在変数同士により一般的な関数関係も考慮したものなど多くの潜在変数モデルが，共分散構造に基づいた非線型数値最適化を推論エンジンとして統一的に推定できることに辿り着いた．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>このことに加えて，潜在変数間の関数関係に適切な仮定をおくことで，因果推論・高次の因子分析・分散分析など従来考慮されなかった新たなタスクにも適用可能であることも了解された <span class="citation" data-cites="Joreskog1978">(<a href="#ref-Joreskog1978" role="doc-biblioref">Karl G. Jöreskog, 1978</a>)</span>, <span class="citation" data-cites="Bentler1980">(<a href="#ref-Bentler1980" role="doc-biblioref">Bentler, 1980</a>)</span>．<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>現代では特徴抽出，生成，表現学習にも用いられていると思うと感慨である．</p>
<p>これを <strong>共分散構造分析</strong> または <strong>構造方程式モデリング</strong> (SEM: Structural Equation Modeling) という．<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> 心理学の文脈では，潜在変数のことを <strong>構成概念</strong> (construct) と呼び，潜在変数間は無関係とした従来の因果分析モデルを <strong>測定方程式</strong> と呼ぶ．<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
</section>
<section id="sec-PLS" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-PLS"><span class="header-section-number">3.2</span> 部分最小自乗モデル (PLS)</h3>
<p>PLS (Partial Least Square) モデル <span class="citation" data-cites="Joreskog-Wold1982">(<a href="#ref-Joreskog-Wold1982" role="doc-biblioref">K. G. Jöreskog and Wold, 1982</a>)</span>, <span class="citation" data-cites="Gustafsson2001">(<a href="#ref-Gustafsson2001" role="doc-biblioref">Gustafsson, 2001</a>)</span> では，次のような潜在変数モデルを用いて，２つの構成概念間の因果関係を評価しようとする <span class="citation" data-cites="豊田秀樹1991">(<a href="#ref-豊田秀樹1991" role="doc-biblioref">豊田秀樹, 1991</a>)</span>：</p>
<p><img src="Images/PLS.svg" class="img-fluid"></p>
<p>なお，パス図において，潜在変数から観測変数に矢印が伸びている場合，これは影響的指標と呼ばれ，観測のモデルと解され，誤差が入ることが想定される <span class="citation" data-cites="豊田秀樹1991">(<a href="#ref-豊田秀樹1991" role="doc-biblioref">豊田秀樹, 1991</a>)</span>．<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> 逆の矢印は形成的指標という．</p>
<p>すなわち，PLS では，<span class="math inline">\(X^1,X^2,X^3,\cdots\)</span> には，<span class="math inline">\(Z^1,Z^2,\cdots\)</span> とは独立な独自因子が作用していると仮定されている．</p>
<p>このような仮定は，<span class="math inline">\(Y^1,Y^2,\cdots\)</span> を被説明変数として，教師あり PCA <span class="citation" data-cites="Yu+2006">(<a href="#ref-Yu+2006" role="doc-biblioref">Yu et al., 2006</a>)</span> に有用である．</p>
<p>というのも，被説明変数のうち必ずしも <span class="math inline">\(Y^1,Y^2,\cdots\)</span> に関係する要素が全てとは限らないために，<span class="math inline">\(Z^1,Z^2\)</span> の間で間接的に回帰分析を行いたい場合に自然な設定である <span class="citation" data-cites="Nounou+2002">(<a href="#ref-Nounou+2002" role="doc-biblioref">Nounou et al., 2002</a>)</span>．</p>
</section>
<section id="構造方程式モデリングの発展" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="構造方程式モデリングの発展"><span class="header-section-number">3.3</span> 構造方程式モデリングの発展</h3>
<p>PLS において，潜在変数から構成概念への矢印が全て影響的であった場合，これは潜在因子の間に関係が仮定されていることを除いて，（探索的）因子分析と等価になる．</p>
<p>一般に，SEM は，潜在変数同士の関数関係も考慮した因子分析モデルだと理解できる．</p>
<p>このようなモデルは，社会学において <strong>多重指標分析</strong> と呼ばれていたモデルに相当し <span class="citation" data-cites="白倉幸男1984">(<a href="#ref-白倉幸男1984" role="doc-biblioref">白倉幸男, 1984</a>)</span> <span class="citation" data-cites="清水和秋1989">(<a href="#ref-清水和秋1989" role="doc-biblioref">清水和秋, 1989</a>)</span>，経済学において <strong>同時方程式モデル</strong> と呼ばれていたモデルに相当する <span class="citation" data-cites="Bentler1980">(<a href="#ref-Bentler1980" role="doc-biblioref">Bentler, 1980</a>)</span>．<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>加えて，心理学・行動計量学においても，多くの既存の多変量解析法（因子分析，パス解析，二段階抽出モデル，潜在構造分析，項目反応モデルなど）はいずれも SEM の特殊な形だと解釈できることが自覚された <span class="citation" data-cites="McArdle1984">(<a href="#ref-McArdle1984" role="doc-biblioref">McArdle, 1984</a>)</span>, <span class="citation" data-cites="Muthen2002">(<a href="#ref-Muthen2002" role="doc-biblioref">Muthén, 2002</a>)</span>．<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p>こうして SEM の名と LISREL プログラムの下で，多くの社会科学分野で使われていたモデルが，形式的にはほとんど等価であるという了解が形成されていった．</p>
<p>このことから，SEM は第二世代の多変量解析 <span class="citation" data-cites="Fornell1985">(<a href="#ref-Fornell1985" role="doc-biblioref">Fornell, 1985</a>)</span> とも評される．<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
</section>
<section id="計算統計学という要素" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="計算統計学という要素"><span class="header-section-number">3.4</span> 計算統計学という要素</h3>
<p>構造方程式モデリングが普及した理由の一つとして，計算機統計学の発展とうまく合流した点が見逃せない．</p>
<p>そもそも Jöreskog は，因子分析を研究していた時期 <span class="citation" data-cites="Joreskog1966">(<a href="#ref-Joreskog1966" role="doc-biblioref">Karl G. Jöreskog, 1966</a>)</span> <span class="citation" data-cites="Joreskog1967a">(<a href="#ref-Joreskog1967a" role="doc-biblioref">K. G. Jöreskog, 1967</a>)</span> から，数値的な解法とコンピュータプログラムの開発にも重点を置いていた．特に，因子分析モデルを，<a href="https://ja.wikipedia.org/wiki/DFP法">DFP 法</a> に基づいて数値的に最尤推定する方法を提案した <span class="citation" data-cites="Joreskog1967a">(<a href="#ref-Joreskog1967a" role="doc-biblioref">K. G. Jöreskog, 1967</a>)</span>．</p>
<p>SEM も，コンピュータプログラム LISREL (LInear Structural RELationships) <span class="citation" data-cites="Joreskog-vanThillo1972">(<a href="#ref-Joreskog-vanThillo1972" role="doc-biblioref">Jőreskog and Thiilo, 1972</a>)</span> の存在が，広い分野の人口に膾炙した要因として大きい <span class="citation" data-cites="清水和秋1989">(<a href="#ref-清水和秋1989" role="doc-biblioref">清水和秋, 1989</a>)</span>, <span class="citation" data-cites="Grimm-Yarnold2016">(<a href="#ref-Grimm-Yarnold2016" role="doc-biblioref">Grimm and Yarnold, 2016</a>)</span>．</p>
<p>構造方程式モデルがどのように因子分析，因果分析，共分散構造分析を統合し，LISREL プログラムと共に発展していたかは，<span class="citation" data-cites="清水和秋1994">(<a href="#ref-清水和秋1994" role="doc-biblioref">清水和秋, 1994</a>)</span> に大変わかりやすくまとまっている</p>
</section>
<section id="sec-CCA" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-CCA"><span class="header-section-number">3.5</span> 正準相関分析 (CCA)</h3>
<p>正準相関分析 <span class="citation" data-cites="Hotelling36">(<a href="#ref-Hotelling36" role="doc-biblioref">Hotelling, 1936</a>)</span> においては，２つの構成概念の間は相関関係で結び，すべての観測は形成的な影響を及ぼすとする（観測誤差は想定しない） <span class="citation" data-cites="豊田秀樹1991">(<a href="#ref-豊田秀樹1991" role="doc-biblioref">豊田秀樹, 1991</a>)</span>：</p>
<p><img src="Images/CCA.svg" class="img-fluid"></p>
<p>このモデルでは <span class="math inline">\(X^1,X^2,X^3\)</span> とその潜在要因 <span class="math inline">\(Z^1\)</span>，<span class="math inline">\(X^4,X^5\)</span> とその潜在要因 <span class="math inline">\(Z^2\)</span> とを完全に対等に扱い，その間の関係を理解しようとする．</p>
<p>例えばマルチモーダル学習において，<span class="math inline">\(X,Y\)</span> が類似したタスクに関するデータという場合に応用がある <span class="citation" data-cites="岩瀬-中山2016">(<a href="#ref-岩瀬-中山2016" role="doc-biblioref">岩瀬智亮 and 中山英樹, 2016</a>)</span>．また，PLS と共に特徴抽出にも用いられる <span class="citation" data-cites="Sun+2009">(<a href="#ref-Sun+2009" role="doc-biblioref">Sun et al., 2009</a>)</span>．</p>
<p>複数の標本に対して同時に実行する主成分分析ともみなせるが，別々に PCA を実行した場合と違い「共通要因」を抽出することに志向がある <span class="citation" data-cites="赤穂昭太郎2013">(<a href="#ref-赤穂昭太郎2013" role="doc-biblioref">赤穂昭太郎, 2013</a>)</span>．</p>
<p>なお，正準相関分析が，このような確率論的解釈ができることは <span class="citation" data-cites="Bach-Jordan2005">(<a href="#ref-Bach-Jordan2005" role="doc-biblioref">Bach and Jordan, 2005</a>)</span> で自覚されたことである．</p>
<p>この潜在変数モデルとしての観点から，<span class="math inline">\(Z^3,Z^4,\cdots\)</span> がある GCCA (Generalized CCA) <span class="citation" data-cites="Horst1961">(<a href="#ref-Horst1961" role="doc-biblioref">Horst, 1961</a>)</span>，指数分布族の場合 <span class="citation" data-cites="Klami+2010">(<a href="#ref-Klami+2010" role="doc-biblioref">Klami et al., 2010</a>)</span>，ニューラルネットワークにより非線型にした DCCA <span class="citation" data-cites="Andrew+2013">(<a href="#ref-Andrew+2013" role="doc-biblioref">Andrew et al., 2013</a>)</span>，さらに変分推論する場合 <span class="citation" data-cites="Wang+2017">(<a href="#ref-Wang+2017" role="doc-biblioref">Wang et al., 2017</a>)</span>, <span class="citation" data-cites="Suzuki+2017">(<a href="#ref-Suzuki+2017" role="doc-biblioref">Suzuki et al., 2017</a>)</span> に拡張されている．</p>
<p>質的データをダミーベクトルに変換して（一般化）正準相関分析を行う，質的データの解析法を <strong>対応分析</strong> (correspondence analysis) または <strong>数量化第III類</strong> ともいう．<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
</section>
</section>
<section id="sec-MM" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-MM"><span class="header-section-number">4</span> 混合モデル (MM)</h2>
<section id="はじめに-4" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="はじめに-4"><span class="header-section-number">4.1</span> はじめに</h3>
<p>混合モデルは，次のようなたいへん基本的な設定であるが，第 <a href="#sec-MixFA" class="quarto-xref">2.7</a> 節で見たように，例えば因子分析モデルと組み合わせることで極めて豊かな表現力を持つ．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/MM.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>混合モデルは SEM の別の選択肢としても使える．また，ランダム効果要因を明示的にモデルに組み込む意味で，一般線型モデルの確率論的な拡張と考えることもできる <span class="citation" data-cites="狩野裕2002">(<a href="#ref-狩野裕2002" role="doc-biblioref">狩野裕, 2002</a>)</span>．<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
</section>
<section id="正規混合モデル-gmm" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="正規混合モデル-gmm"><span class="header-section-number">4.2</span> 正規混合モデル (GMM)</h3>
<p><span class="math inline">\(Z\in\mathcal{L}(\Omega;[K])\)</span> は <span class="math display">\[
[K]=\{1,\cdots,K\}
\]</span> に値を取る離散確率変数で，確率核 <span class="math inline">\(Z\to X\)</span> が <span class="math display">\[
p(x|z=k)\,dx=\mathrm{N}_p(\mu_k,\Sigma_k)
\]</span> と表せる場合，<span class="math inline">\(X\)</span> に課される仮定を <strong>正規混合モデル</strong> (GMM: Gaussian Mixture Model) という．</p>
<p><span class="math inline">\(Z\sim\mathrm{U}([K]),\Sigma_k=I\)</span> の場合，これは <a href="../../../posts/2024/Computation/VI2.html#sec-EM-and-K-means"><span class="math inline">\(K\)</span>-平均クラスタリングに等価</a> なモデルとなる．</p>
<p>これは SGD により訓練をすることで，生成のタスクにおいても GAN に匹敵する性能も持つ <span class="citation" data-cites="Ricahrdson-Weiss2018">(<a href="#ref-Ricahrdson-Weiss2018" role="doc-biblioref">Richardson and Weiss, 2018</a>)</span>．</p>
<p>また，デノイジングや deblurring, inpainting, super-resolution などの画像逆問題は，巨大な GMM の潜在変数の推定として理解できる <span class="citation" data-cites="Zoran-Weiss2011">(<a href="#ref-Zoran-Weiss2011" role="doc-biblioref">Zoran and Weiss, 2011</a>)</span>, <span class="citation" data-cites="Papyam-Elad2016">(<a href="#ref-Papyam-Elad2016" role="doc-biblioref">Papyan and Elad, 2016</a>)</span>．</p>
</section>
<section id="正規スケール混合モデル-gsm" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="正規スケール混合モデル-gsm"><span class="header-section-number">4.3</span> 正規スケール混合モデル (GSM)</h3>
<p>Gaussian scale mixture モデルとは， <span class="math display">\[
p(x|z)\,dx=\mathrm{N}_p(0,\sigma_0^2z)
\]</span> で定まる階層モデルである．</p>
<p>このモデルは，<span class="math inline">\(Z\)</span> の分布により，種々の（特に裾の重い）分布を表せる：</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<ol type="1">
<li><span class="math inline">\(Z\sim\mathrm{Ber}(\pi)\)</span> のときを spike and slab 分布という： <span class="math display">\[
p(x)\,dx=\pi\mathrm{N}(0,\sigma_0^2)+(1-\pi)\delta_0.
\]</span></li>
<li><span class="math inline">\(Z\sim\mathrm{C}(1)_+\)</span> のとき，<strong>馬蹄分布</strong> <span class="citation" data-cites="Carvalho+2009">(<a href="#ref-Carvalho+2009" role="doc-biblioref">Carvalho et al., 2009</a>)</span>, <span class="citation" data-cites="Carvalho+2010">(<a href="#ref-Carvalho+2010" role="doc-biblioref">Carvalho et al., 2010</a>)</span> という．<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></li>
</ol>
</div>
</div>
</div>
</section>
<section id="潜在-dirichlet-配分-lda" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="潜在-dirichlet-配分-lda"><span class="header-section-number">4.4</span> 潜在 Dirichlet 配分 (LDA)</h3>
<section id="はじめに-5" class="level4" data-number="4.4.1">
<h4 data-number="4.4.1" class="anchored" data-anchor-id="はじめに-5"><span class="header-section-number">4.4.1</span> はじめに</h4>
<p>文書の埋め込み・数値表現を得るために，単語 <span class="math inline">\(i\in[M]\)</span> が文書 <span class="math inline">\(j\in[N]\)</span> に現れた回数をカウントした行列 <span class="math inline">\(\boldsymbol{C}\in M_{MN}(\mathbb{N})\)</span> を通じた主成分分析が用いることも考えられる．</p>
<p>これを <strong>潜在意味索引</strong> (LSI: Latent Semantic Indexing) <span class="citation" data-cites="Deerwester+1990">(<a href="#ref-Deerwester+1990" role="doc-biblioref">Deerwester et al., 1990</a>)</span> と呼ぶ．得られた低次元埋め込みを文書検索 (document retrieval) などに用いることもできる．</p>
<p><span class="math inline">\(\boldsymbol{C}\)</span> の列も単語とし，帯幅 <span class="math inline">\(h&gt;0\)</span> を決めて，<span class="math inline">\(h\)</span> 文字以内に単語 <span class="math inline">\(i,j\in[M]\)</span> が共起した回数を <span class="math inline">\(C_{ij}\)</span> とすると，全く同様の手続きが，単語の埋め込みに応用できる．これを <a href="https://ja.wikipedia.org/wiki/潜在意味解析"><strong>潜在意味解析</strong></a> (LSA: Latent Semantic Analysis) <span class="citation" data-cites="Deerwester+1990">(<a href="#ref-Deerwester+1990" role="doc-biblioref">Deerwester et al., 1990</a>)</span> と呼ぶ．</p>
</section>
<section id="sec-PLSI" class="level4" data-number="4.4.2">
<h4 data-number="4.4.2" class="anchored" data-anchor-id="sec-PLSI"><span class="header-section-number">4.4.2</span> 確率的潜在意味索引 (PLSI)</h4>
<p><span class="citation" data-cites="Hofmann1999">(<a href="#ref-Hofmann1999" role="doc-biblioref">Hofmann, 1999</a>)</span> による pLSI または aspect model は LSI を確率モデル，特に混合モデルとして解釈し直したものである．</p>
<p>単語数よりも少ない数の <strong>トピック</strong> <span class="math inline">\(Z\)</span> というものがあり，これが単語を決めている，というモデルを想定した．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/PLSI.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>このモデルを通じて，トピック <span class="math inline">\(Z\)</span> の分布（あるいは，現代的には <span class="math inline">\(\Theta\)</span> の値）を「文書」の特徴量とする，というアイデアである．</p>
</section>
<section id="sec-LDA" class="level4" data-number="4.4.3">
<h4 data-number="4.4.3" class="anchored" data-anchor-id="sec-LDA"><span class="header-section-number">4.4.3</span> Dirichlet 事前分布の追加</h4>
<p>変数 <span class="math inline">\(\Theta\)</span> に Dirichlet 事前分布を追加し，完全なベイズの見方を提示したのが Latent Dirichlet Allocation <span class="citation" data-cites="Blei+2003">(<a href="#ref-Blei+2003" role="doc-biblioref">Blei et al., 2003</a>)</span> である．</p>
<p><span class="math inline">\(\Theta\)</span> を文書，<span class="math inline">\(Z\)</span> トピック，<span class="math inline">\(W\)</span> をトピックごとの語彙デッキとする．</p>
<p><img src="Images/LDA.svg" class="img-fluid"></p>
<p>最終的に，トピック <span class="math inline">\(Z\)</span> とその人の語彙 <span class="math inline">\(W\)</span> が合わさって，単語 <span class="math inline">\(X\)</span> が観測されるというモデルが考えられている．</p>
</section>
<section id="確率的トピックモデル" class="level4" data-number="4.4.4">
<h4 data-number="4.4.4" class="anchored" data-anchor-id="確率的トピックモデル"><span class="header-section-number">4.4.4</span> 確率的トピックモデル</h4>
<p>自然言語処理において，単語分布のモデリングの潜在変数は <strong>トピック</strong> と呼ばれて，これを確率的にモデリングする手法は PTM (Probabilistic Topic Model) <span class="citation" data-cites="Blei2012">(<a href="#ref-Blei2012" role="doc-biblioref">Blei, 2012</a>)</span> と呼ばれている．</p>
<p>「トピック」は短い文章の中でも激しく移り変わることが知られている <span class="citation" data-cites="Church-Gale1991">(<a href="#ref-Church-Gale1991" role="doc-biblioref">Church and Gale, 1991</a>)</span>．</p>
<p>そのため，LDA では，<span class="math inline">\(\Theta\)</span> の事前分布と <span class="math inline">\(W\)</span> の事前分布は， <span class="math display">\[
\mathrm{Dirichlet}(\alpha\boldsymbol{1}),\qquad\alpha&gt;0,
\]</span> という形で，極めて小さい <span class="math inline">\(\alpha&gt;0\)</span> を設定し，特定のトピックがどの文書に現れるかは極めてスパースになるようにモデリングをする．</p>
</section>
<section id="推論" class="level4" data-number="4.4.5">
<h4 data-number="4.4.5" class="anchored" data-anchor-id="推論"><span class="header-section-number">4.4.5</span> 推論</h4>
<p>LDA の推論手法には変分推論 <span class="citation" data-cites="Blei+2003">(<a href="#ref-Blei+2003" role="doc-biblioref">Blei et al., 2003</a>)</span> や Gibbs サンプリング <span class="citation" data-cites="Griffith-Steyvers2004">(<a href="#ref-Griffith-Steyvers2004" role="doc-biblioref">T. L. Griffiths and Steyvers, 2004</a>)</span>，そしてスペクトルに基づく方法 <span class="citation" data-cites="Arova+2013">(<a href="#ref-Arova+2013" role="doc-biblioref">Arora et al., 2013</a>)</span> がある．</p>
<p>トピック数の決定には，尤度を <a href="../../../posts/Surveys/SMCSamplers.html#sec-AIS">焼なまし重点サンプリング</a> で計算する方法 <span class="citation" data-cites="Wallach+2009">(<a href="#ref-Wallach+2009" role="doc-biblioref">Wallach et al., 2009</a>)</span> の他，ノンパラメトリックベイズ法も用いられる <span class="citation" data-cites="Teh+2006">(<a href="#ref-Teh+2006" role="doc-biblioref">Yee Whye Teh and Blei, 2006</a>)</span>．</p>
</section>
<section id="時系列化" class="level4" data-number="4.4.6">
<h4 data-number="4.4.6" class="anchored" data-anchor-id="時系列化"><span class="header-section-number">4.4.6</span> 時系列化</h4>
<p>単語の並びは明らかな方向性があり，対照的なモデリングはこの消息を取り逃がしていると考えられる．</p>
<p>そこで，トピックの移り変わりを捉えるモデルとして dynamic topic model <span class="citation" data-cites="Blei+2006">(<a href="#ref-Blei+2006" role="doc-biblioref">Blei and Lafferty, 2006</a>)</span> がある．これは Kalman 平滑化と変分推論を組み合わせている様である．</p>
<p>また単語の時系列構造を捉えるために，LDA に隠れ Markov モデルを組み合わせた LDA-HMM <span class="citation" data-cites="Griffiths+2004">(<a href="#ref-Griffiths+2004" role="doc-biblioref">T. Griffiths et al., 2004</a>)</span> が提案された．TopicRNN <span class="citation" data-cites="Dieng+2017">(<a href="#ref-Dieng+2017" role="doc-biblioref">Dieng et al., 2017</a>)</span> ではより長距離の相関を捉えるために，<a href="../../../posts/2024/Kernels/Deep.html#sec-RNN2">RNN</a> と組み合わせている．</p>
</section>
</section>
<section id="sec-SSM" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="sec-SSM"><span class="header-section-number">4.5</span> 状態空間モデル (SSM)</h3>
<section id="概要-2" class="level4" data-number="4.5.1">
<h4 data-number="4.5.1" class="anchored" data-anchor-id="概要-2"><span class="header-section-number">4.5.1</span> 概要</h4>
<p>状態空間モデル (State Space Model) は，混合モデルの時系列化と捉えられる：</p>
<p><img src="Images/SSM.svg" class="img-fluid"></p>
<p>潜在変数 <span class="math inline">\(X_t\)</span> が離散的である場合は特に <strong>隠れ Markov モデル</strong> (HMM: Hidden Markov Model) <span class="citation" data-cites="Baum-Petrie1966">(<a href="#ref-Baum-Petrie1966" role="doc-biblioref">Baum and Petrie, 1966</a>)</span> と呼ばれる．</p>
<p>HMM に関しては早くから EM 様の推定手法 <strong>Baum-Welch アルゴリズム</strong> <span class="citation" data-cites="Baum-Eagon1967">(<a href="#ref-Baum-Eagon1967" role="doc-biblioref">Baum and Eagon, 1967</a>)</span>, <span class="citation" data-cites="Baum+1970">(<a href="#ref-Baum+1970" role="doc-biblioref">Baum et al., 1970</a>)</span> が提案されているが，データサイズが大きい場合は SGD が用いられる．Blocked Gibbs サンプラー <span class="citation" data-cites="Scott2002">(<a href="#ref-Scott2002" role="doc-biblioref">Scott, 2002</a>)</span> や，潜在変数を消去して，周辺尤度に関してスペクトル法／テンソル分解 <span class="citation" data-cites="Hsu+2012">(<a href="#ref-Hsu+2012" role="doc-biblioref">Hsu et al., 2012</a>)</span>, <span class="citation" data-cites="Anandkumar+2012">(<a href="#ref-Anandkumar+2012" role="doc-biblioref">Animashree Anandkumar et al., 2012</a>)</span>, <span class="citation" data-cites="Anandkumar+2015">(<a href="#ref-Anandkumar+2015" role="doc-biblioref">Anima Anandkumar et al., 2015</a>)</span>, <span class="citation" data-cites="Obermeyer+2019">(<a href="#ref-Obermeyer+2019" role="doc-biblioref">Obermeyer et al., 2019</a>)</span> を実行するなどの代替手法がある．</p>
</section>
<section id="sec-S4" class="level4" data-number="4.5.2">
<h4 data-number="4.5.2" class="anchored" data-anchor-id="sec-S4"><span class="header-section-number">4.5.2</span> 構造的状態系列モデル (S4)</h4>
<p>S4 (Structured State Space Sequence) <span class="citation" data-cites="Gu+2022">(<a href="#ref-Gu+2022" role="doc-biblioref">Gu et al., 2022</a>)</span>, <span class="citation" data-cites="Gu+2020">(<a href="#ref-Gu+2020" role="doc-biblioref">Gu et al., 2020</a>)</span>, <span class="citation" data-cites="Goel+2022">(<a href="#ref-Goel+2022" role="doc-biblioref">Goel et al., 2022</a>)</span> とは，時系列を深層ニューラルネットワークの力でモデリングするために，線型 Gauss で単純な SMM を上下にスタックし深層にしたものである．各層は LSSL (Linear State Space Layer) と呼ばれる．</p>
<p>さらに長距離の依存性に耐えるために，S5 <span class="citation" data-cites="Smith+2023">(<a href="#ref-Smith+2023" role="doc-biblioref">Smith et al., 2023</a>)</span> や Mamba <span class="citation" data-cites="Gu-Dao2024">(<a href="#ref-Gu-Dao2024" role="doc-biblioref">Gu and Dao, 2024</a>)</span> が提案されている．後者では，選択的に記憶を忘却できるような「選択」機構 (S6: Selective SSM) を導入している．</p>
</section>
</section>
</section>
<section id="sec-ICA" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-ICA"><span class="header-section-number">5</span> 独立成分分析 (ICA)</h2>
<section id="はじめに-6" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="はじめに-6"><span class="header-section-number">5.1</span> はじめに</h3>
<p>（線型）独立成分分析で用いるモデルは，PCA や FA のそれと全く変わらず，線型変換 <span class="math inline">\(x_n=Az_n\)</span> でデータを説明しようとする：</p>
<p><img src="Images/ICA.svg" class="img-fluid"></p>
<p>ただし，潜在変数 <span class="math inline">\(Z^1,\cdots,Z^r\)</span> は互いに <strong>独立</strong> であるという「真の構造」が強く想定される場合に使われる．</p>
<p>加えて，モデルの <strong>識別可能性</strong> を重視する．このために，（独立）因子分析（第 <a href="#sec-other-priors-FA" class="quarto-xref">2.5</a> 節）で考えたように，正規分布より裾の重い事前分布を導入することで，モデルの識別可能性を確約する．<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<p>この意味で，確率モデルとしては PCA / FA に等価であるが，典型的な ICA の文脈では <span class="math inline">\(Z^1,\cdots,Z^r\)</span> は非正規確率変数であり，<span class="math inline">\(A\)</span> を生成荷重や混合行列，<span class="math inline">\(A^{-1}\)</span> を <strong>認識荷重</strong> (recognition weight) などという．</p>
</section>
<section id="推定手法" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="推定手法"><span class="header-section-number">5.2</span> 推定手法</h3>
<p>最初に <a href="https://ja.wikipedia.org/wiki/ブラインド信号源分離">音源分離</a> について適用された <span class="citation" data-cites="Bell-Sejnowski1995">(<a href="#ref-Bell-Sejnowski1995" role="doc-biblioref">Bell and Sejnowski, 1995</a>)</span> では，<span class="math inline">\(X\)</span> と <span class="math inline">\(Z\)</span> の相互情報量の最大化が目指された．</p>
<p>最尤推定は EM アルゴリズムの他に近似 Newton 法で実行されることもあり，fast ICA <span class="citation" data-cites="Hyvarinen-Oja2000">(<a href="#ref-Hyvarinen-Oja2000" role="doc-biblioref">Hyvärinen and Oja, 2000</a>)</span> と呼ばれる．</p>
<p>また古典的には，探索的データ解析で考案された <strong>射影追跡</strong> (PP: Projection Pursuit) <span class="citation" data-cites="Friedman-Tukey1974">(<a href="#ref-Friedman-Tukey1974" role="doc-biblioref">Friedman and Tukey, 1974</a>)</span> みたく，学習される <span class="math inline">\(Z\)</span> の分布が Gauss からなるべく遠いように学習することも考えられた．</p>
<p>disentangled な表現を学習したい場面では，<span class="math inline">\(Z\)</span> の成分同士の相関が最小になるように学習される； <span class="math display">\[
\operatorname{KL}\left(\operatorname{P}^Z,\bigotimes_{j=1}^r\operatorname{P}^{Z_j}\right).
\]</span></p>
<p>最小情報コピュラに基づく方法も提案されている <span class="citation" data-cites="Bedford+2016">(<a href="#ref-Bedford+2016" role="doc-biblioref">Bedford et al., 2016</a>)</span>, <span class="citation" data-cites="Sei-Yano2024">(<a href="#ref-Sei-Yano2024" role="doc-biblioref">Sei and Yano, 2024</a>)</span>．</p>
<p>他にも表現学習や認知科学の文脈を踏襲して，<a href="../../../posts/2024/Kernels/NCL.html#sec-InfoMax">InfoMax</a> やスパース符号化などの原則がある．</p>
</section>
<section id="非線型化-1" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="非線型化-1"><span class="header-section-number">5.3</span> 非線型化</h3>
<p><a href="../../../posts/2024/Kernels/NCL.html">非線型独立成分分析は，表現学習の文脈でも研究されている</a>．</p>
</section>
</section>
<section id="おわりに" class="level2 unnumbered unlisted">
<h2 class="unnumbered unlisted anchored" data-anchor-id="おわりに">おわりに</h2>
<p>現代の深層生成モデルは，いずれも非線型な潜在変数モデルであると理解できる．</p>
<p>その意味で，次の記事は全て，本稿の続きであり，本稿は現代の機械学習の壮大な序章としても理解できる．</p>
<div id="listing-kernel-listing" class="quarto-listing quarto-listing-container-grid">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="RGVlcA==" data-listing-date-sort="1722211200000" data-listing-file-modified-sort="1736498021745" data-listing-date-modified-sort="1723420800000" data-listing-reading-time-sort="2" data-listing-word-count-sort="385">
<a href="../../../posts/2024/Kernels/NCL.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Kernels/Images/contrastive_repr4.jpeg" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
表現学習と非線型独立成分分析
</h5>
<div class="card-subtitle listing-subtitle">
「データ理解」に向けた深層潜在変数モデル
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-07-29
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="1" data-categories="RGVlcCUyQ05hdHVyZSUyQ1N0YXRpc3RpY3MlMkNHZW9tZXRyeQ==" data-listing-date-sort="1722297600000" data-listing-file-modified-sort="1736498021745" data-listing-date-modified-sort="1723680000000" data-listing-reading-time-sort="3" data-listing-word-count-sort="597">
<a href="../../../posts/2024/Kernels/Manifold.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Kernels/Images/UMAPvSNE.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
非線型な次元縮約法の概観
</h5>
<div class="card-subtitle listing-subtitle">
最古にして最難のタスクと多様体学習
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-07-30
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="2" data-categories="RGVlcCUyQ1N1cnZleQ==" data-listing-date-sort="1707609600000" data-listing-file-modified-sort="1736498021724" data-listing-date-modified-sort="1722211200000" data-listing-reading-time-sort="5" data-listing-word-count-sort="883">
<a href="../../../posts/2024/Kernels/Deep.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Kernels/Images/AE.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
数学者のための深層学習概観
</h5>
<div class="card-subtitle listing-subtitle">
歴史と導入
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-11
</div>
</div>
</div>
</div>
</a>
</div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div>
<p>非線形性の他に本稿で扱わなかったものは深層モデルである．</p>
<p>だがそもそも，現代のニューラルネットワークが深層化したのは，単純で可微分なモジュール性を保ちながら表現力を高めるためのトリックであり，確率論的には本稿で扱ったモデルと等価であるはずである．</p>
<p>ニューラルネットワークの他にも，計算のために深層化したモデルを考える場面は多い．例えばアニーリングを用いた <a href="../../../posts/Surveys/SMCSamplers.html">SMC サンプラー</a> は，グラフカルモデル <span class="math inline">\(Z\to X\)</span> の潜在変数 <span class="math inline">\(Z\)</span> の推定を，人工的に時系列構造を見出して状態空間モデル <a href="#sec-SSM" class="quarto-xref">4.5</a> にあてはめてサンプリングしやすくする方法と言える．</p>
<p>しかし，<a href="../../../posts/2024/Probability/Kernel.html">確率核は射をなす</a>のだから，全てのモデルは本質的には一層であるとみなすこともできるのである．</p>
<p>この見方をとった方が計算効率が上がるという例もある．例えば <span class="citation" data-cites="Chen+2024">(<a href="#ref-Chen+2024" role="doc-biblioref">Chen et al., 2024</a>)</span> では，トランスフォーマーの注意機構をランダム Fourier 特徴写像で近似し，<a href="../../../posts/2024/Kernels/Kernel.html#sec-RFF">Monte Carlo 法によって元のモデルと等価な計算を安価に行っている</a>．</p>
<p><a href="https://puniupa.github.io/posts/2024/AI/BAI.html">ベイズ機械学習</a> や <a href="https://puniupa.github.io/posts/2024/AI/TDL.html">位相的機械学習</a> をはじめとした，丁寧なモデルへの理解が，これからも手法への統一した視点からの理解と，応用分野を横断した相互理解を促進してくれるのではないかと，筆者は意気込んでいる．</p>
</section>




<div id="quarto-appendix" class="default"><section id="扱ったモデル一覧" class="level2 appendix unnumbered"><h2 class="anchored quarto-appendix-heading">扱ったモデル一覧</h2><div class="quarto-appendix-contents">

<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/PCA.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-PCA">PCA</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/FA.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-FA">FA</a></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/PLS.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-PLS">PLS</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/CCA.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-CCA">CCA</a></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/PLSI.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-PLSI">PLSI</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/LDA.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-LDA">LDA</a></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/SSM.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-SSM">SSM</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/ICA.svg" class="img-fluid figure-img"></p>
<figcaption><a href="#sec-ICA">ICA</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/HierarchicalModels.png" class="img-fluid figure-img"></p>
<figcaption><span class="citation" data-cites="Murphy2023">(<a href="#ref-Murphy2023" role="doc-biblioref">Murphy, 2023, p. 920</a>)</span> より，本稿で扱ったモデルのいくつかを含んだ，数式による一覧表．すでに図式による解説を受けた後だと，より見やすいだろう．<span class="math inline">\(\operatorname{Cat}(c|\boldsymbol{\pi})\)</span> は確率ベクトル <span class="math inline">\(\boldsymbol{\pi}\)</span> が定める質量関数を表す．</figcaption>
</figure>
</div>
</div></section><section id="付録" class="level2 unnumbered appendix"><h2 class="anchored quarto-appendix-heading">付録</h2><div class="quarto-appendix-contents">

<p>ここでは，歴史を感じる引用をいくつか紹介したい．</p>
<blockquote class="blockquote">
<p>心理測定学 (psychometrics) における因子分析，計量経済学 (econometrics) における同時方程式モデル (simultaneous equation models), そして生物測定学 (biometrics) におけるパス解析 (path analysis) を，共分散構造分析の下に統一化することが可能となった契機は，潜在変数 (latent variables) の概念である <span class="citation" data-cites="Bentler1980">(<a href="#ref-Bentler1980" role="doc-biblioref">Bentler, 1980</a>)</span>．<span class="citation" data-cites="清水和秋1989">(<a href="#ref-清水和秋1989" role="doc-biblioref">清水和秋, 1989</a>)</span></p>
</blockquote>
<p>そして，異分野横断の知見交流が進んだ契機の一つは，LISREL プログラムの存在であった．<span class="citation" data-cites="清水和秋1994">(<a href="#ref-清水和秋1994" role="doc-biblioref">清水和秋, 1994</a>)</span> では，ETS での安定した研究環境が LISREL の継続的な保守を可能にして最終的には WINDOWS 上でも安定して提供され，これを用いることを通じて異分野を巻き込みながら構造方程式モデリングが発展していった様子が詳細に解説されている．LISREL はバージョン VI まである．</p>
<blockquote class="blockquote">
<p>紹介した文献からもわかるように，この分野は最近になってやっと日本では注目されてようになってきた。 このように日本へのこの方法論の導入が遅れた理由の一つはソフト流通の問題にあると筆者は考えている。青木 (1988) や土田 (1988) が述べているように， LISREL は大型計算機の場合， アメリカ産のコンビュータでしかサポートしてくれないとのことである。<span class="citation" data-cites="清水和秋1989">(<a href="#ref-清水和秋1989" role="doc-biblioref">清水和秋, 1989</a>)</span></p>
</blockquote>
<p>そして現代はというと，計算機統計学と機械学習が先行し（過ぎ）ていると思える．</p>
<p>もしその通りならば，種々の科学への応用とそれぞれ固有の課題への特殊化が，これからの未来を彩ってくれるのかもしれない．</p>




</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Anandkumar+2015" class="csl-entry" role="listitem">
Anandkumar, Anima, Ge, R., Hsu, D., Kakade, S. M., and Telgarsky, M. (2015). Tensor decompositions for learning latent variable models (a survey for ALT). In K. Chaudhuri, C. GENTILE, and S. Zilles, editors, <em>Algorithmic learning theory</em>, pages 19–38. Cham: Springer International Publishing.
</div>
<div id="ref-Anandkumar+2012" class="csl-entry" role="listitem">
Anandkumar, Animashree, Hsu, D., and Kakade, S. M. (2012). <a href="https://proceedings.mlr.press/v23/anandkumar12.html">A method of moments for mixture models and hidden markov models</a>. In S. Mannor, N. Srebro, and R. C. Williamson, editors, <em>Proceedings of the 25th annual conference on learning theory</em>,Vol. 23, pages 33.1–33.34. Edinburgh, Scotland: PMLR.
</div>
<div id="ref-Anderson-Rubin1956" class="csl-entry" role="listitem">
Anderson, T. W., and Rubin, H. (1956). <a href="https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Statistical-Inference-in-Factor-Analysis/bsmsp/1200511860">Statistical inference in factor analysis</a>. In <em>Proceedings of the thrid berkeley symposium on mathematical statistics and probability</em>,Vol. 5, pages 111–150.
</div>
<div id="ref-Andrew+2013" class="csl-entry" role="listitem">
Andrew, G., Arora, R., Bilmes, J., and Livescu, K. (2013). <a href="https://proceedings.mlr.press/v28/andrew13.html">Deep canonical correlation analysis</a>. In S. Dasgupta and D. McAllester, editors, <em>Proceedings of the 30th international conference on machine learning</em>,Vol. 28, pages 1247–1255. Atlanta, Georgia, USA: PMLR.
</div>
<div id="ref-Archambeau-Bach2008" class="csl-entry" role="listitem">
Archambeau, C., and Bach, F. (2008). <a href="https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf">Sparse probabilistic projections</a>. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, <em>Advances in neural information processing systems</em>,Vol. 21. Curran Associates, Inc.
</div>
<div id="ref-Arova+2013" class="csl-entry" role="listitem">
Arora, S., Ge, R., Halpern, Y., Mimno, D., Moitra, A., Sontag, D., … Zhu, M. (2013). <a href="https://proceedings.mlr.press/v28/arora13.html">A practical algorithm for topic modeling with provable guarantees</a>. In S. Dasgupta and D. McAllester, editors, <em>Proceedings of the 30th international conference on machine learning</em>,Vol. 28, pages 280–288. Atlanta, Georgia, USA: PMLR.
</div>
<div id="ref-Asher83-Causal" class="csl-entry" role="listitem">
Asher, H. B. (1983). <em>Causal modelling</em>,Vol. 3. 和訳は心理学者広瀬弘忠による『因果分析法』（朝倉書店，1980）; SAGE Publications, Inc.
</div>
<div id="ref-Bach-Jordan2005" class="csl-entry" role="listitem">
Bach, F. R., and Jordan, M. I. (2005). <em>A probabilistic interpretation of canonical correlation analysis</em>. University of California, Berkeley. Retrieved from <a href="https://statistics.berkeley.edu/tech-reports/688">https://statistics.berkeley.edu/tech-reports/688</a>
</div>
<div id="ref-Baum-Eagon1967" class="csl-entry" role="listitem">
Baum, L. E., and Eagon, J. A. (1967). An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology. <em>Bulletin of the American Mathematical Society</em>, <em>73</em>(3), 360–363.
</div>
<div id="ref-Baum-Petrie1966" class="csl-entry" role="listitem">
Baum, L. E., and Petrie, T. (1966). Statistical inference for probabilistic functions of finite state markov chains. <em>The Annals of Mathematical Statistics</em>, <em>37</em>(6), 1554–1563.
</div>
<div id="ref-Baum+1970" class="csl-entry" role="listitem">
Baum, L. E., Petrie, T., Soules, G., and Weiss, N. (1970). <a href="http://www.jstor.org/stable/2239727">A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains</a>. <em>The Annals of Mathematical Statistics</em>, <em>41</em>(1), 164–171.
</div>
<div id="ref-Bedford+2016" class="csl-entry" role="listitem">
Bedford, T., Daneshkhah, A., and Wilson, K. J. (2016). <a href="https://doi.org/10.1111/risa.12471">Approximate uncertainty modeling in risk analysis with vine copulas</a>. <em>Risk Analysis</em>, <em>36</em>(4), 792–815.
</div>
<div id="ref-Bell-Sejnowski1995" class="csl-entry" role="listitem">
Bell, A. J., and Sejnowski, T. J. (1995). <a href="https://doi.org/10.1162/neco.1995.7.6.1129"><span class="nocase">An Information-Maximization Approach to Blind Separation and Blind Deconvolution</span></a>. <em>Neural Computation</em>, <em>7</em>(6), 1129–1159.
</div>
<div id="ref-Bentler1980" class="csl-entry" role="listitem">
Bentler, P. M. (1980). <a href="https://doi.org/10.1146/annurev.ps.31.020180.002223">Multivariate analysis with latent variables: Causal modeling</a>. <em>Annual Review of Psychology</em>, <em>31</em>(Volume 31, 1980), 419–456. Journal Article.
</div>
<div id="ref-Bernaards-Jennrich2003" class="csl-entry" role="listitem">
Bernaards, C. A., and Jennrich, R. I. (2003). <a href="https://doi.org/10.1007/BF02295613">Orthomax rotation and perfect simple structure</a>. <em>Psychometrika</em>, <em>68</em>(4), 585–588.
</div>
<div id="ref-Bhattacharya-Dunson2012" class="csl-entry" role="listitem">
Bhattacharya, A., and Dunson, D. B. (2012). <a href="https://doi.org/10.1080/01621459.2011.646934">Simplex factor models for multivariate unordered categorical data</a>. <em>Journal of the American Statistical Association</em>, <em>107</em>(497), 362–377.
</div>
<div id="ref-Bishop1998" class="csl-entry" role="listitem">
Bishop, C. (1998). <a href="https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf">Bayesian PCA</a>. In M. Kearns, S. Solla, and D. Cohn, editors, <em>Advances in neural information processing systems</em>,Vol. 11. MIT Press.
</div>
<div id="ref-Blei2012" class="csl-entry" role="listitem">
Blei, D. M. (2012). <a href="https://doi.org/10.1145/2133806.2133826">Probabilistic topic models</a>. <em>Commun. ACM</em>, <em>55</em>(4), 77–84.
</div>
<div id="ref-Blei+2006" class="csl-entry" role="listitem">
Blei, D. M., and Lafferty, J. D. (2006). <a href="https://doi.org/10.1145/1143844.1143859">Dynamic topic models</a>. In <em>Proceedings of the 23rd international conference on machine learning</em>, pages 113–120. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Blei+2003" class="csl-entry" role="listitem">
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). <a href="https://www.jmlr.org/papers/v3/blei03a.html"><span>Latent Dirichlet Allocation</span></a>. <em>Journal of Machine Learning Research</em>, <em>3</em>, 993–1022.
</div>
<div id="ref-Bock-Bargmann1966" class="csl-entry" role="listitem">
Bock, R. D., and Bargmann, R. E. (1966). <a href="https://doi.org/10.1007/BF02289521">Analysis of covariance structures</a>. <em>Psychometrika</em>, <em>31</em>(4), 507–534.
</div>
<div id="ref-Buntine-Jakulin2006" class="csl-entry" role="listitem">
Buntine, W., and Jakulin, A. (2006). Discrete component analysis. In C. Saunders, M. Grobelnik, S. Gunn, and J. Shawe-Taylor, editors, <em>Subspace, latent structure and feature selection</em>, pages 1–33. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-Canny2004" class="csl-entry" role="listitem">
Canny, J. (2004). <a href="https://doi.org/10.1145/1008992.1009016">GaP: A factor model for discrete data</a>. In <em>Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval</em>, pages 122–129. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Carvalho+2009" class="csl-entry" role="listitem">
Carvalho, C. M., Polson, N. G., and Scott, J. G. (2009). <a href="https://proceedings.mlr.press/v5/carvalho09a.html">Handling sparsity via the horseshoe</a>. In D. van Dyk and M. Welling, editors, <em>Proceedings of the twelfth international conference on artificial intelligence and statistics</em>,Vol. 5, pages 73–80. Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA: PMLR.
</div>
<div id="ref-Carvalho+2010" class="csl-entry" role="listitem">
Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010). <a href="https://doi.org/10.1093/biomet/asq017"><span class="nocase">The horseshoe estimator for sparse signals</span></a>. <em>Biometrika</em>, <em>97</em>(2), 465–480.
</div>
<div id="ref-Chen+2024" class="csl-entry" role="listitem">
Chen, H., Liu, Z., Wang, X., Tian, Y., and Wang, Y. (2024). <a href="https://arxiv.org/abs/2403.19928">DiJiang: Efficient large language models through compact kernelization</a>.
</div>
<div id="ref-Church-Gale1991" class="csl-entry" role="listitem">
Church, K. W., and Gale, W. A. (1991). <a href="https://doi.org/10.1007/BF01889984">Probability scoring for spelling correction</a>. <em>Statistics and Computing</em>, <em>1</em>(2), 93–103.
</div>
<div id="ref-Collins+2001" class="csl-entry" role="listitem">
Collins, M., Dasgupta, S., and Schapire, R. E. (2001). <a href="https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf">A generalization of principal components analysis to the exponential family</a>. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, <em>Advances in neural information processing systems</em>,Vol. 14. MIT Press.
</div>
<div id="ref-DeLeeuw04-SimultaneousEstimationOfEFA" class="csl-entry" role="listitem">
De Leeuw, J. (2004). <a href="https://doi.org/10.1007/978-1-4020-1958-6_7">Least squares optimal scaling of partially observed linear systems</a>. In <em>Recent developments on structural equation models</em>. Springer Dordrecht.
</div>
<div id="ref-Deerwester+1990" class="csl-entry" role="listitem">
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). <a href="https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9">Indexing by latent semantic analysis</a>. <em>Journal of the American Society for Information Science</em>, <em>41</em>(6), 391–407.
</div>
<div id="ref-Dieng+2017" class="csl-entry" role="listitem">
Dieng, A. B., Wang, C., Gao, J., and Paisley, J. (2017). <a href="https://openreview.net/forum?id=rJbbOLcex">Topic<span>RNN</span>: A recurrent neural network with long-range semantic dependency</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Drineas+2016" class="csl-entry" role="listitem">
Drineas, P., and Mahoney, M. W. (2016). <a href="https://doi.org/10.1145/2842602">RandNLA: Randomized numerical linear algebra</a>. <em>Commun. ACM</em>, <em>59</em>(6), 80–90.
</div>
<div id="ref-Erosheva+2004" class="csl-entry" role="listitem">
Erosheva, E., Fienberg, S., and Lafferty, J. (2004). <a href="https://doi.org/10.1073/pnas.0307760101">Mixed-membership models of scientific publications</a>. <em>Proceedings of the National Academy of Sciences</em>, <em>101</em>(suppl_1), 5220–5227.
</div>
<div id="ref-Fornell1985" class="csl-entry" role="listitem">
Fornell, C. (1985). <em>A second generation of multivariate analysis: Classification of methods and implications for marketing research</em>. Business, Stephen M. Ross School, University of Michigan. Retrieved from <a href="https://hdl.handle.net/2027.42/35621">https://hdl.handle.net/2027.42/35621</a>
</div>
<div id="ref-Friedman-Tukey1974" class="csl-entry" role="listitem">
Friedman, J. H., and Tukey, J. W. (1974). <a href="https://doi.org/10.1109/T-C.1974.224051">A projection pursuit algorithm for exploratory data analysis</a>. <em>IEEE Transactions on Computers</em>, <em>C-23</em>(9), 881–890.
</div>
<div id="ref-Ghahramani-Hinton1996" class="csl-entry" role="listitem">
Ghahramani, Z., and Hinton, G. E. (1996). <em>The EM algorithm for mixtures of factor analyzers</em>. Department of Computer Science, University of Toronto. Retrieved from <a href="https://www.cs.toronto.edu/~hinton/absps/tr96-1.html">https://www.cs.toronto.edu/~hinton/absps/tr96-1.html</a>
</div>
<div id="ref-Ghojogh+2022" class="csl-entry" role="listitem">
Ghojogh, B., Ghodsi, A., Karray, F., and Crowley, M. (2022). <a href="https://arxiv.org/abs/2101.00734">Factor analysis, probabilistic principal component analysis, variational inference, and variational autoencoder: Tutorial and survey</a>.
</div>
<div id="ref-Goel+2022" class="csl-entry" role="listitem">
Goel, K., Gu, A., Donahue, C., and Re, C. (2022). <a href="https://proceedings.mlr.press/v162/goel22a.html">It’s raw! <span>A</span>udio generation with state-space models</a>. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, <em>Proceedings of the 39th international conference on machine learning</em>,Vol. 162, pages 7616–7633. PMLR.
</div>
<div id="ref-Griffith-Steyvers2004" class="csl-entry" role="listitem">
Griffiths, T. L., and Steyvers, M. (2004). <a href="https://doi.org/10.1073/pnas.0307752101">Finding scientific topics</a>. <em>Proceedings of the National Academy of Sciences</em>, <em>101</em>(suppl_1), 5228–5235.
</div>
<div id="ref-Griffiths+2004" class="csl-entry" role="listitem">
Griffiths, T., Steyvers, M., Blei, D., and Tenenbaum, J. (2004). <a href="https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf">Integrating topics and syntax</a>. In L. Saul, Y. Weiss, and L. Bottou, editors, <em>Advances in neural information processing systems</em>,Vol. 17. MIT Press.
</div>
<div id="ref-Grimm-Yarnold2016" class="csl-entry" role="listitem">
Grimm, L. G., and Yarnold, P. R. (2016). <em>研究論文を読み解くための多変量解析入門 応用篇</em>. Reading and Understanding MORE Multivariate Statistics (2020) の翻訳書; 北大路書房.
</div>
<div id="ref-Gu-Dao2024" class="csl-entry" role="listitem">
Gu, A., and Dao, T. (2024). <a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-time sequence modeling with selective state spaces</a>.
</div>
<div id="ref-Gu+2020" class="csl-entry" role="listitem">
Gu, A., Dao, T., Ermon, S., Rudra, A., and Ré, C. (2020). <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf">HiPPO: Recurrent memory with optimal polynomial projections</a>. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, <em>Advances in neural information processing systems</em>,Vol. 33, pages 1474–1487. Curran Associates, Inc.
</div>
<div id="ref-Gu+2022" class="csl-entry" role="listitem">
Gu, A., Goel, K., and Re, C. (2022). <a href="https://openreview.net/forum?id=uYLFoz1vlAC">Efficiently modeling long sequences with structured state spaces</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Gustafsson2001" class="csl-entry" role="listitem">
Gustafsson, M. G. (2001). <a href="https://doi.org/10.1021/ci0003909">A probabilistic derivation of the partial least-squares algorithm</a>. <em>Journal of Chemical Information and Computer Sciences</em>, <em>41</em>(2), 288–294. doi: 10.1021/ci0003909.
</div>
<div id="ref-Halko+2011" class="csl-entry" role="listitem">
Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). <a href="https://doi.org/10.1137/090771806">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</a>. <em>SIAM Review</em>, <em>53</em>(2), 217–288.
</div>
<div id="ref-Harman-Fukuda1966" class="csl-entry" role="listitem">
Harman, H. H., and Fukuda, Y. (1966). <a href="https://doi.org/10.1007/BF02289525">Resolution of the heywood case in the minres solution</a>. <em>Psychometrika</em>, <em>31</em>(4), 563–571.
</div>
<div id="ref-Harman-Jones1966" class="csl-entry" role="listitem">
Harman, H. H., and Jones, W. H. (1966). <a href="https://doi.org/10.1007/BF02289468">Factor analysis by minimizing residuals (minres)</a>. <em>Psychometrika</em>, <em>31</em>(3), 351–368.
</div>
<div id="ref-Hoffman2017" class="csl-entry" role="listitem">
Hoffman, M. D. (2017). <a href="https://proceedings.mlr.press/v70/hoffman17a.html">Learning deep latent <span>G</span>aussian models with <span>M</span>arkov chain <span>M</span>onte <span>C</span>arlo</a>. In D. Precup and Y. W. Teh, editors, <em>Proceedings of the 34th international conference on machine learning</em>,Vol. 70, pages 1510–1519. PMLR.
</div>
<div id="ref-Hofmann1999" class="csl-entry" role="listitem">
Hofmann, T. (1999). <a href="https://doi.org/10.1145/312624.312649">Probabilistic latent semantic indexing</a>. In <em>Proceedings of the 22nd annual international ACM SIGIR conference on research and development in information retrieval</em>, pages 50–57. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Horst1961" class="csl-entry" role="listitem">
Horst, P. (1961). <a href="https://doi.org/10.1002/1097-4679(196110)17:4<331::AID-JCLP2270170402>3.0.CO;2-D">Generalized canonical correlations and their applications to experimental data</a>. <em>Journal of Clinical Psychology</em>, <em>17</em>(4), 331–347.
</div>
<div id="ref-Hotelling33-PCA" class="csl-entry" role="listitem">
Hotelling, H. (1933). <a href="https://psycnet.apa.org/doi/10.1037/h0071325">Analysis of a complex of statistical variables into principal components</a>. <em>Journal of Educational Psychology</em>, <em>24</em>(6), 417–441.
</div>
<div id="ref-Hotelling36" class="csl-entry" role="listitem">
Hotelling, H. (1936). <a href="https://www.jstor.org/stable/2333955">Relations between two sets of variates</a>. <em>Biometrika</em>, <em>27</em>(3/4), 321–377.
</div>
<div id="ref-Hsu+2012" class="csl-entry" role="listitem">
Hsu, D., Kakade, S. M., and Zhang, T. (2012). <a href="https://doi.org/10.1016/j.jcss.2011.12.025">A spectral algorithm for learning hidden markov models</a>. <em>Journal of Computer and System Sciences</em>, <em>78</em>(5), 1460–1480.
</div>
<div id="ref-Hyvarinen-Oja2000" class="csl-entry" role="listitem">
Hyvärinen, A., and Oja, E. (2000). <a href="https://doi.org/10.1016/S0893-6080(00)00026-5">Independent component analysis: Algorithms and applications</a>. <em>Neural Networks</em>, <em>13</em>(4), 411–430.
</div>
<div id="ref-Jolliffe+2003" class="csl-entry" role="listitem">
Ian T Jolliffe, N. T. T., and Uddin, M. (2003). <a href="https://doi.org/10.1198/1061860032148">A modified principal component technique based on the LASSO</a>. <em>Journal of Computational and Graphical Statistics</em>, <em>12</em>(3), 531–547.
</div>
<div id="ref-Joreskog1966" class="csl-entry" role="listitem">
Jöreskog, Karl G. (1966). <em>UMLFA: A computer program for unrestricted maximum likelihood factor analysis</em>. ETS. Retrieved from <a href="https://www.ets.org/research/policy_research_reports/publications/report/1966/iazh.html">https://www.ets.org/research/policy_research_reports/publications/report/1966/iazh.html</a>
</div>
<div id="ref-Joreskog1967a" class="csl-entry" role="listitem">
Jöreskog, K. G. (1967). <a href="https://doi.org/10.1007/BF02289658">Some contributions to maximum likelihood factor analysis</a>. <em>Psychometrika</em>, <em>32</em>(4), 443–482.
</div>
<div id="ref-Joreskog1969" class="csl-entry" role="listitem">
Jöreskog, K. G. (1969). <a href="https://doi.org/10.1007/BF02289343">A general approach to confirmatory maximum likelihood factor analysis</a>. <em>Psychometrika</em>, <em>34</em>(2), 183–202.
</div>
<div id="ref-Joreskog70" class="csl-entry" role="listitem">
Jöreskog, Karl Gustav. (1970). <a href="https://www.jstor.org/stable/2334833">A general method for analysis of covariance structures</a>. <em>Biometrika</em>, <em>57</em>(2), 239–251.
</div>
<div id="ref-Joreskog1978" class="csl-entry" role="listitem">
Jöreskog, Karl G. (1978). <a href="https://doi.org/10.1007/BF02293808">Structural analysis of covariance and correlation matrices</a>. <em>Psychometrika</em>, <em>43</em>(4), 443–477.
</div>
<div id="ref-Joreskog-Lawley1968" class="csl-entry" role="listitem">
Jöreskog, K. G., and Lawley, D. N. (1968). <a href="https://doi.org/10.1111/j.2044-8317.1968.tb00399.x">New methods in maximum likelihood factor analysis</a>. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>21</em>(1), 85–96.
</div>
<div id="ref-Joreskog-Wold1982" class="csl-entry" role="listitem">
Jöreskog, K. G., and Wold, H. (1982). Systems under indirect observation: Causality, structure, prediction. In, pages 263–270. North-Holland.
</div>
<div id="ref-Joreskog-vanThillo1972" class="csl-entry" role="listitem">
Jőreskog, K. G., and Thiilo, M. van. (1972). <a href="https://doi.org/10.1002/j.2333-8504.1972.tb00827.x"><span class="nocase">LISREL: A General Computer Program for Estimating a Linear Structural Equation System Involving Multiple Indicators of Unmeasured Variables</span></a>. <em>ETS Research Bulletin Series</em>, <em>1972</em>(2), i–71.
</div>
<div id="ref-Khemakhem+2020" class="csl-entry" role="listitem">
Khemakhem, I., Kingma, D., Monti, R., and Hyvarinen, A. (2020). <a href="https://proceedings.mlr.press/v108/khemakhem20a.html">Variational autoencoders and nonlinear ICA: A unifying framework</a>. In S. Chiappa and R. Calandra, editors, <em>Proceedings of the twenty third international conference on artificial intelligence and statistics</em>,Vol. 108, pages 2207–2217. PMLR.
</div>
<div id="ref-Klami+2010" class="csl-entry" role="listitem">
Klami, A., Virtanen, S., and Kaski, S. (2010). Bayesian exponential family projections for coupled data sources. In <em>Proceedings of the twenty-sixth conference on uncertainty in artificial intelligence</em>, pages 286–293. Arlington, Virginia, USA: AUAI Press.
</div>
<div id="ref-Lawley1942" class="csl-entry" role="listitem">
Lawley, D. N. (1942). <a href="https://doi.org/10.1017/S0080454100006178">XIV.—further investigations in factor estimation</a>. <em>Proceedings of the Royal Society of Edinburgh. Section A. Mathematical and Physical Sciences</em>, <em>61</em>(2), 176–185.
</div>
<div id="ref-Lawrence2005" class="csl-entry" role="listitem">
Lawrence, N. (2005). <a href="http://jmlr.org/papers/v6/lawrence05a.html">Probabilistic non-linear principal component analysis with gaussian process latent variable models</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(60), 1783–1816.
</div>
<div id="ref-Lee-Seung1999" class="csl-entry" role="listitem">
Lee, D. D., and Seung, H. S. (1999). <a href="https://doi.org/10.1038/44565">Learning the parts of objects by non-negative matrix factorization</a>. <em>Nature</em>, <em>401</em>(6755), 788–791.
</div>
<div id="ref-Marlin2003" class="csl-entry" role="listitem">
Marlin, B. M. (2003). <a href="https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf">Modeling user rating profiles for collaborative filtering</a>. In S. Thrun, L. Saul, and B. Schölkopf, editors, <em>Advances in neural information processing systems</em>,Vol. 16. MIT Press.
</div>
<div id="ref-McArdle1984" class="csl-entry" role="listitem">
McArdle, J. J. (1984). <a href="https://doi.org/10.1080/00273171.1984.9676927">On the madness in his method: R. B. Cattell’s contributions to structural equation modeling</a>. <em>Multivariate Behavioral Research</em>, <em>19</em>(2-3), 245–267.
</div>
<div id="ref-Murphy2022" class="csl-entry" role="listitem">
Murphy, K. P. (2022). <em><a href="https://probml.github.io/pml-book/book1.html">Probabilistic machine learning: An introduction</a></em>. MIT Press.
</div>
<div id="ref-Murphy2023" class="csl-entry" role="listitem">
Murphy, K. P. (2023). <em><a href="http://probml.github.io/book2">Probabilistic machine learning: Advanced topics</a></em>. MIT Press.
</div>
<div id="ref-Murray+2023" class="csl-entry" role="listitem">
Murray, R., Demmel, J., Mahoney, M. W., Erichson, N. B., Melnichenko, M., Malik, O. A., … Dongarra, J. (2023). <em>Randomized numerical linear algebra: A perspective on the field with an eye to software</em> (No. UCB/EECS-2023-19). Retrieved from <a href="http://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-19.html">http://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-19.html</a>
</div>
<div id="ref-Muthen2002" class="csl-entry" role="listitem">
Muthén, B. O. (2002). <a href="https://doi.org/10.2333/bhmk.29.81"><span>Beyond SEM: General Latent Variable Modeling</span></a>. <em>Behaviormetrika</em>, <em>29</em>(1), 81–117.
</div>
<div id="ref-Nounou+2002" class="csl-entry" role="listitem">
Nounou, M. N., Bakshi, B. R., Goel, P. K., and Shen, X. (2002). <a href="https://doi.org/10.1002/aic.690480818">Process modeling by bayesian latent variable regression</a>. <em>AIChE Journal</em>, <em>48</em>(8), 1775–1793.
</div>
<div id="ref-Obermeyer+2019" class="csl-entry" role="listitem">
Obermeyer, F., Bingham, E., Jankowiak, M., Pradhan, N., Chiu, J., Rush, A., and Goodman, N. (2019). <a href="https://proceedings.mlr.press/v97/obermeyer19a.html">Tensor variable elimination for plated factor graphs</a>. In K. Chaudhuri and R. Salakhutdinov, editors, <em>Proceedings of the 36th international conference on machine learning</em>,Vol. 97, pages 4871–4880. PMLR.
</div>
<div id="ref-Paatero-Tapper1994" class="csl-entry" role="listitem">
Paatero, P., and Tapper, U. (1994). <a href="https://doi.org/10.1002/env.3170050203">Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values</a>. <em>Environmetrics</em>, <em>5</em>(2), 111–126.
</div>
<div id="ref-Paisley-Carin2009" class="csl-entry" role="listitem">
Paisley, J., and Carin, L. (2009). <a href="https://doi.org/10.1145/1553374.1553474">Nonparametric factor analysis with beta process priors</a>. In <em>Proceedings of the 26th annual international conference on machine learning</em>, pages 777–784. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Papyam-Elad2016" class="csl-entry" role="listitem">
Papyan, V., and Elad, M. (2016). <a href="https://doi.org/10.1109/TIP.2015.2499698">Multi-scale patch-based image restoration</a>. <em>IEEE Transactions on Image Processing</em>, <em>25</em>(1), 249–261.
</div>
<div id="ref-Pearson01-PCA" class="csl-entry" role="listitem">
Pearson, K. (1901). <a href="https://www.tandfonline.com/doi/abs/10.1080/14786440109462720">On lines and planes of closest fit to systems of points in space</a>. <em>Philosophical Magazine</em>, <em>2</em>(11), 559–572.
</div>
<div id="ref-Perrone2024" class="csl-entry" role="listitem">
Perrone, P. (2024). <a href="https://doi.org/10.1109/TIT.2023.3328825"><span class="nocase">Markov Categories and Entropy</span></a>. <em>IEEE Transactions on Information Theory</em>, <em>70</em>(3), 1671–1692.
</div>
<div id="ref-Pritchard+2000" class="csl-entry" role="listitem">
Pritchard, J. K., Stephens, M., and Donnelly, P. (2000). <a href="https://doi.org/10.1093/genetics/155.2.945"><span class="nocase">Inference of Population Structure Using Multilocus Genotype Data</span></a>. <em>Genetics</em>, <em>155</em>(2), 945–959.
</div>
<div id="ref-Rattray+2009" class="csl-entry" role="listitem">
Rattray, M., Stegle, O., Sharp, K., and Winn, J. (2009). <a href="https://doi.org/10.1088/1742-6596/197/1/012002">Inference algorithms and learning theory for bayesian sparse factor analysis</a>. <em>Journal of Physics: Conference Series</em>, <em>197</em>(1), 012002.
</div>
<div id="ref-Ricahrdson-Weiss2018" class="csl-entry" role="listitem">
Richardson, E., and Weiss, Y. (2018). <a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf">On GANs and GMMs</a>. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, <em>Advances in neural information processing systems</em>,Vol. 31. Curran Associates, Inc.
</div>
<div id="ref-Roweis1997" class="csl-entry" role="listitem">
Roweis, S. (1997). <a href="https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf">EM algorithms for PCA and SPCA</a>. In M. Jordan, M. Kearns, and S. Solla, editors, <em>Advances in neural information processing systems</em>,Vol. 10. MIT Press.
</div>
<div id="ref-Rubin-Thayer1982" class="csl-entry" role="listitem">
Rubin, D. B., and Thayer, D. T. (1982). <a href="https://doi.org/10.1007/BF02293851">EM algorithms for ML factor analysis</a>. <em>Psychometrika</em>, <em>47</em>(1), 69–76.
</div>
<div id="ref-Scott2002" class="csl-entry" role="listitem">
Scott, S. L. (2002). <a href="https://doi.org/10.1198/016214502753479464">Bayesian methods for hidden markov models</a>. <em>Journal of the American Statistical Association</em>, <em>97</em>(457), 337–351.
</div>
<div id="ref-Sei-Yano2024" class="csl-entry" role="listitem">
Sei, T., and Yano, K. (2024). <a href="https://doi.org/10.3150/23-BEJ1687"><span class="nocase">Minimum information dependence modeling</span></a>. <em>Bernoulli</em>, <em>30</em>(4), 2623–2643.
</div>
<div id="ref-Herbert-Simon57-ModelsOfMan" class="csl-entry" role="listitem">
Simon, H. (1957). <em>Models of man; social and rational.</em> Wiley.
</div>
<div id="ref-Smith+2023" class="csl-entry" role="listitem">
Smith, J. T. H., Warrington, A., and Linderman, S. (2023). <a href="https://openreview.net/forum?id=Ai8Hw3AXqks">Simplified state space layers for sequence modeling</a>. In <em>The eleventh international conference on learning representations</em>.
</div>
<div id="ref-Socan2003" class="csl-entry" role="listitem">
Socan, G. (2003). <em>The incremental value of rank factor analysis</em> (PhD thesis). Rijksuniversiteit Groningen.
</div>
<div id="ref-Sorbom1974" class="csl-entry" role="listitem">
Sörbom, D. (1974). <a href="https://doi.org/10.1111/j.2044-8317.1974.tb00543.x"><span class="nocase">A General Method for Studying Differences in Factor Means and Factor Structure between Groups</span></a>. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>27</em>(2), 229–239.
</div>
<div id="ref-Spearman1904" class="csl-entry" role="listitem">
Spearman, C. (1904). <a href="https://psycnet.apa.org/doi/10.2307/1412107">’General intelligence,’ objectively determined and measured</a>. <em>The American Journal of Psychology</em>, <em>15</em>(2), 201–293.
</div>
<div id="ref-Sun+2009" class="csl-entry" role="listitem">
Sun, L., Ji, S., Yu, S., and Ye, J. (2009). On the equivalence between canonical correlation analysis and orthonormalized partial least squares. In <em>Proceedings of the 21st international joint conference on artificial intelligence</em>, pages 1230–1235. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
</div>
<div id="ref-Suzuki+2017" class="csl-entry" role="listitem">
Suzuki, M., Nakayama, K., and Matsuo, Y. (2017). <a href="https://openreview.net/forum?id=Hk8rlUqge">Joint multimodal learning with deep generative models</a>.
</div>
<div id="ref-Thurstone1947" class="csl-entry" role="listitem">
Thurstone, L. L. (1947). <em><a href="">Multiple factor analysis</a></em>. University of Chicago Press.
</div>
<div id="ref-Tipping-Bishop1999" class="csl-entry" role="listitem">
Tipping, M. E., and Bishop, C. M. (1999). <a href="https://www.jstor.org/stable/2680726">Probabilistic principle component analysis</a>. <em>Journal of the Royal Statistical Society. Series B (Statistical Methodology)</em>, <em>61</em>(3), 611–622.
</div>
<div id="ref-Unkel-Trendafilov2010" class="csl-entry" role="listitem">
Unkel, S., and Trendafilov, N. T. (2010). <a href="https://doi.org/10.1111/j.1751-5823.2010.00120.x">Simultaneous parameter estimation in exploratory factor analysis: An expository review</a>. <em>International Statistical Review</em>, <em>78</em>(3), 363–382.
</div>
<div id="ref-Wallach+2009" class="csl-entry" role="listitem">
Wallach, H. M., Murray, I., Salakhutdinov, R., and Mimno, D. (2009). <a href="https://doi.org/10.1145/1553374.1553515">Evaluation methods for topic models</a>. In <em>Proceedings of the 26th annual international conference on machine learning</em>, pages 1105–1112. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Wang+2017" class="csl-entry" role="listitem">
Wang, W., Yan, X., Lee, H., and Livescu, K. (2017). <a href="https://openreview.net/forum?id=H1Heentlx">Deep variational canonical correlation analysis</a>.
</div>
<div id="ref-Wright1918" class="csl-entry" role="listitem">
Wright, S. (1918). <a href="https://academic.oup.com/genetics/article/3/4/367/5934526">On the nature of size factors</a>. <em>Genetics</em>, <em>3</em>(4), 367.
</div>
<div id="ref-Wright1921" class="csl-entry" role="listitem">
Wright, S. (1921). Correlation and causation. <em>Journal of Agricultural Reserach</em>, <em>20</em>, 557–585.
</div>
<div id="ref-Teh+2006" class="csl-entry" role="listitem">
Yee Whye Teh, M. J. B., Michael I Jordan, and Blei, D. M. (2006). <a href="https://doi.org/10.1198/016214506000000302">Hierarchical dirichlet processes</a>. <em>Journal of the American Statistical Association</em>, <em>101</em>(476), 1566–1581.
</div>
<div id="ref-Yu+2006" class="csl-entry" role="listitem">
Yu, S., Yu, K., Tresp, V., Kriegel, H.-P., and Wu, M. (2006). <a href="https://doi.org/10.1145/1150402.1150454">Supervised probabilistic principal component analysis</a>. In <em>Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining</em>, pages 464–473. New York, NY, USA: Association for Computing Machinery.
</div>
<div id="ref-Zong+2018" class="csl-entry" role="listitem">
Zong, B., Song, Q., Min, M. R., Cheng, W., Lumezanu, C., Cho, D., and Chen, H. (2018). <a href="https://openreview.net/forum?id=BJJLHbb0-">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Zoran-Weiss2011" class="csl-entry" role="listitem">
Zoran, D., and Weiss, Y. (2011). <a href="https://doi.org/10.1109/ICCV.2011.6126278">From learning models of natural image patches to whole image restoration</a>. In <em>2011 international conference on computer vision</em>, pages 479–486.
</div>
<div id="ref-Zou-Hastie2005" class="csl-entry" role="listitem">
Zou, H., and Hastie, T. (2005). <a href="https://doi.org/10.1111/j.1467-9868.2005.00503.x"><span class="nocase">Regularization and Variable Selection Via the Elastic Net</span></a>. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em>, <em>67</em>(2), 301–320.
</div>
<div id="ref-Zou+2006" class="csl-entry" role="listitem">
Zou, H., Hastie, T., and Tibshirani, R. (2006). <a href="http://www.jstor.org/stable/27594179">Sparse principal component analysis</a>. <em>Journal of Computational and Graphical Statistics</em>, <em>15</em>(2), 265–286.
</div>
<div id="ref-岩瀬-中山2016" class="csl-entry" role="listitem">
岩瀬智亮, and 中山英樹. (2016). <a href="http://id.nii.ac.jp/1001/00162588/">深層一般化正準相関分析</a>. <em>情報処理学会第78回全国大会講演論文集</em>, <em>2016</em>(1), 183–184.
</div>
<div id="ref-星野崇宏+2005" class="csl-entry" role="listitem">
星野崇宏, 岡田謙介, and 前田忠彦. (2005). <a href="https://doi.org/10.2333/jbhmk.32.209">構造方程式モデリングにおける適合度指標とモデル改善について : 展望とシミュレーション研究による新たな知見</a>. <em>行動計量学</em>, <em>32</em>(2), 209–235.
</div>
<div id="ref-江口真透1999" class="csl-entry" role="listitem">
江口真透. (1999). <a href="http://hdl.handle.net/10787/295">概パラメトリック推測 － 柔らかなモデルの構築 －</a>. <em>統計数理</em>, <em>47</em>(1), 29–48.
</div>
<div id="ref-清水和秋1989" class="csl-entry" role="listitem">
清水和秋. (1989). <a href="http://hdl.handle.net/10112/13348">検証的因子分析，LISRELそしてRAMの概要</a>. <em>関西大学社会学部紀要</em>, <em>20</em>(2), 61–86.
</div>
<div id="ref-清水和秋1994" class="csl-entry" role="listitem">
清水和秋. (1994). <a href="http://hdl.handle.net/10112/13345">JöreskogとSörbomによるコンピュータ・プログラムと構造方程式モデル</a>. <em>関西大学社会学部紀要</em>, <em>25</em>(3), 1–41.
</div>
<div id="ref-狩野裕2002" class="csl-entry" role="listitem">
狩野裕. (2002). <a href="https://doi.org/10.2333/jbhmk.29.138">構造方程式モデリングは，因子分析，分散分析，パス解析の すべてにとって代わるのか？</a>. <em>行動計量学</em>, <em>29</em>(2), 138–159.
</div>
<div id="ref-統計科学のフロンティア5" class="csl-entry" role="listitem">
甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫. (2002). <em><a href="https://www.iwanami.co.jp/book/b260371.html">多変量解析の展開：隠れた構造と因果を推理する</a></em>,Vol. 5. 岩波書店.
</div>
<div id="ref-白倉幸男1984" class="csl-entry" role="listitem">
白倉幸男. (1984). <a href="https://doi.org/10.18910/4301">多重指標線形構造モデルとその応用 : 研究ノート</a>. <em>大阪大学人間科学部紀要</em>, <em>10</em>, 25–45.
</div>
<div id="ref-豊田秀樹1991" class="csl-entry" role="listitem">
豊田秀樹. (1991). <a href="https://doi.org/10.5926/jjep1953.39.4_467">共分散構造分析の下位モデルとその適用例</a>. <em>教育心理学研究</em>, <em>39</em>(4), 467–478.
</div>
<div id="ref-豊田秀樹1992" class="csl-entry" role="listitem">
豊田秀樹. (1992). <em><a href="https://www.utp.or.jp/book/b302422.html">SASによる共分散構造分析</a></em>,Vol. 3. 東京大学出版会.
</div>
<div id="ref-豊田秀樹2007" class="csl-entry" role="listitem">
豊田秀樹. (2007). <em>共分散構造分析［理論編］</em>. 朝倉書店.
</div>
<div id="ref-赤穂昭太郎2013" class="csl-entry" role="listitem">
赤穂昭太郎. (2013). <a href="https://doi.org/10.3902/jnns.20.62">正準相関分析入門</a>. <em>日本神経回路学会誌</em>, <em>20</em>(2), 62–72.
</div>
<div id="ref-足立浩平2023" class="csl-entry" role="listitem">
足立浩平. (2023). 50歳を超えてから始めた因子分析. <em>日本行動計量学会報</em>, <em>177</em>.
</div>
<div id="ref-足立浩平+2019" class="csl-entry" role="listitem">
足立浩平, 伊藤真道, and 宇野光平. (2019). <a href="https://doi.org/10.20551/jscswabun.32.1_61">行列分解に基づく因子分析とその新展開</a>. <em>計算機統計学</em>, <em>32</em>(1), 61–77.
</div>
<div id="ref-足立-山本2024" class="csl-entry" role="listitem">
足立浩平, and 山本倫生. (2024). <em><a href="https://www.kyoritsu-pub.co.jp/book/b10085699.html">主成分分析と因子分析―特異値分解を出発点として―</a></em>. 共立出版.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>図を見やすくするために，<span class="math inline">\(X^1\to X^{p-1}\)</span> や <span class="math inline">\(X^2\to X^p\)</span> などは省略している．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="citation" data-cites="足立-山本2024">(<a href="#ref-足立-山本2024" role="doc-biblioref">足立浩平 and 山本倫生, 2024</a>)</span>, <span class="citation" data-cites="足立浩平2023">(<a href="#ref-足立浩平2023" role="doc-biblioref">足立浩平, 2023</a>)</span> によると，この行列分解による定式化は Henk A. K. Kiers によるもので，初出は同大学からの博士論文 <span class="citation" data-cites="Socan2003">(<a href="#ref-Socan2003" role="doc-biblioref">Socan, 2003</a>)</span> が最初ではないか，とのこと．この見方を MDFA (Matrix Decomposition Factor Analysis) と呼ぶ．<span class="citation" data-cites="足立浩平+2019">(<a href="#ref-足立浩平+2019" role="doc-biblioref">足立浩平 et al., 2019</a>)</span> も参照．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>ただし，<span class="citation" data-cites="星野崇宏+2005">(<a href="#ref-星野崇宏+2005" role="doc-biblioref">星野崇宏 et al., 2005</a>)</span> は SEM をより一般的とし，共分散構造分析とは観測変数が連続な場合の下位モデルである，と解している．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="江口真透1999">(<a href="#ref-江口真透1999" role="doc-biblioref">江口真透, 1999</a>)</span> 第３節に，PCA をニューラルネットワークにより近似的に実行する方法が紹介されている．<span class="citation" data-cites="Ghojogh+2022">(<a href="#ref-Ghojogh+2022" role="doc-biblioref">Ghojogh et al., 2022</a>)</span> はサーベイを与えている．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="豊田秀樹1992">(<a href="#ref-豊田秀樹1992" role="doc-biblioref">豊田秀樹, 1992</a>)</span> では CFA を確認的因子分析と呼んでいる．<span class="citation" data-cites="豊田秀樹1991">(<a href="#ref-豊田秀樹1991" role="doc-biblioref">豊田秀樹, 1991</a>)</span> では，古典テスト理論を確認的因子分析の下位モデルとして紹介している．また，このような因果関係の確認的方法は，社会学における <span class="citation" data-cites="Herbert-Simon57-ModelsOfMan">(<a href="#ref-Herbert-Simon57-ModelsOfMan" role="doc-biblioref">Simon, 1957</a>)</span> の基準などが知られていた．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><span class="citation" data-cites="Joreskog70">(<a href="#ref-Joreskog70" role="doc-biblioref">Karl Gustav Jöreskog, 1970</a>)</span> は具体的なモデルを例に取り，彼の検証的因果分析が，パス解析 <span class="citation" data-cites="Wright1918">(<a href="#ref-Wright1918" role="doc-biblioref">Wright, 1918</a>)</span>, <span class="citation" data-cites="Wright1921">(<a href="#ref-Wright1921" role="doc-biblioref">Wright, 1921</a>)</span> のように因果分析に応用できることを示した結果だと言える <span class="citation" data-cites="Asher83-Causal">(<a href="#ref-Asher83-Causal" role="doc-biblioref">Asher, 1983</a>)</span>．この観点から，パス解析は「検証的因果推論」と表現することもできる <span class="citation" data-cites="統計科学のフロンティア5">(<a href="#ref-統計科学のフロンティア5" role="doc-biblioref">甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 73</a>)</span>．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="citation" data-cites="狩野裕2002">(<a href="#ref-狩野裕2002" role="doc-biblioref">狩野裕, 2002</a>)</span> は SEM の射程と得意・不得意を分析している．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>現代ではコンピュータの力により，新たに「生成」「表現学習」というタスクが加わったと思うと，感慨深い．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><span class="citation" data-cites="清水和秋1989">(<a href="#ref-清水和秋1989" role="doc-biblioref">清水和秋, 1989</a>)</span>, <span class="citation" data-cites="豊田秀樹1992">(<a href="#ref-豊田秀樹1992" role="doc-biblioref">豊田秀樹, 1992</a>)</span>, <span class="citation" data-cites="統計科学のフロンティア5">(<a href="#ref-統計科学のフロンティア5" role="doc-biblioref">甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 82</a>)</span> も参照．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>また，パス図では観測変数は四角で囲むべきであるが，ここでは省略した．<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>同時方程式は潜在変数を持たない模型で，経済学におけるパス解析の継承と見れる <span class="citation" data-cites="豊田秀樹2007">(<a href="#ref-豊田秀樹2007" role="doc-biblioref">豊田秀樹, 2007</a>)</span>．特に Keynes 経済学におけるマクロな経済計画の発想で，<a href="https://ja.wikipedia.org/wiki/コウルズ財団">Cowles 委員会</a> により 1940 年代から 1950 年代にかけて盛んに研究された．<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>「従来から存在するがやや標準的でない分析方法がSEMの枠組みで実行できることも指摘しておきたい．たとえば，三相データの分析モデルである PARAFAC，行動遺伝学における ACE モデル，イプサティブデータの分析，潜在曲線モデル，潜在構造分析などの離散潜在変数のモデル，項目反応モデルなどである．加えて，SEM で実行できる新しいモデル，たとえば，多変量二段抽出モデル，平均に特色をもたせる三相データの分析モデルや因子分析と分散分析の統合モデルなどがある．」<span class="citation" data-cites="狩野裕2002">(<a href="#ref-狩野裕2002" role="doc-biblioref">狩野裕, 2002, p. 139</a>)</span>．<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>多変量解析の高級言語とか形容することもあるという．構造方程式モデリングについては，<span class="citation" data-cites="豊田秀樹1991">(<a href="#ref-豊田秀樹1991" role="doc-biblioref">豊田秀樹, 1991</a>)</span>, <span class="citation" data-cites="狩野裕2002">(<a href="#ref-狩野裕2002" role="doc-biblioref">狩野裕, 2002</a>)</span> も参照．<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>オランダ学派を中心に等質性分析とも呼ぶ．<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>ただし，SEM は共分散構造，混合モデルは平均構造に分析の焦点がある，という志向の違いもある．<span class="citation" data-cites="狩野裕2002">(<a href="#ref-狩野裕2002" role="doc-biblioref">狩野裕, 2002</a>)</span> も参照．<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p><span class="math inline">\(\mathrm{C}(\sigma)_+\)</span> は Cauchy 分布 <span class="math inline">\(\mathrm{C}(0,\sigma)\)</span> を <span class="math inline">\(\mathbb{R}_+\)</span> 上に制限したものである．truncated Cauchy または half-Cauchy という．<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p><span class="citation" data-cites="Hyvarinen-Oja2000">(<a href="#ref-Hyvarinen-Oja2000" role="doc-biblioref">Hyvärinen and Oja, 2000</a>)</span> では，<span class="citation" data-cites="Bell-Sejnowski1995">(<a href="#ref-Bell-Sejnowski1995" role="doc-biblioref">Bell and Sejnowski, 1995</a>)</span> のように測定誤差を考えない場合を ICA といい，誤差も入る一般の場合を IFA (Independent Factor Analysis) と呼び分けている．<span class="citation" data-cites="統計科学のフロンティア5">(<a href="#ref-統計科学のフロンティア5" role="doc-biblioref">甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 110</a>)</span> も参照．「これを回転の不定性という．因子分析はさまざまな考察によって，この不定性を解消しようとする．独立成分分析は，非正規性を仮定すれば，この不定性が消えることを示したものとも言える」<span class="citation" data-cites="統計科学のフロンティア5">(<a href="#ref-統計科学のフロンティア5" role="doc-biblioref">甘利俊一，狩野裕，佐藤俊哉，松山裕，竹内啓，石黒真木夫, 2002, p. 13</a>)</span>．<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/162348\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          trigger: 'click',
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          positionFixed: true,
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="162348/162348.github.io" data-repo-id="R_kgDOKlfKYQ" data-category="Announcements" data-category-id="DIC_kwDOKlfKYc4CgDmb" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://162348.github.io/">
<p>Hirofumi Shiba</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/162348/162348.github.io/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ano2math5">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:shiba.hirofumi@ism.ac.jp">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>