---
title: "非線型な次元縮約法の概観"
subtitle: "最古にして最難のタスクと多様体学習"
author: "司馬博文"
date: 7/30/2024
date-modified: 8/15/2024
image: Images/UMAPvSNE.png
categories: [Deep, Nature, Statistics, Geometry]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
    - ../Stat/IdealPoint.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．
shift-heading-level-by: -1
listing: 
    -   id: kernel-listing
        type: grid
        sort: false
        contents:
            - "HierarchicalModel.qmd"
            - "NCL.qmd"
            - "Kernel.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
---

## 関連ページ {.unnumbered .unlisted}

::: {#kernel-listing}
:::


{{< include ../../../assets/_preamble.qmd >}}

## はじめに

多様体学習とは，非線型な次元削減手法のことをいう．

### 多様体仮説

「多様体」の名前の由来は，「高次元データは低次元の部分多様体としての構造を持つ」という **多様体仮説** [@Fefferman+2016] である．^[[@本武陽一2017] も注目．]

特に多様体学習と呼ばれる際は，知識発見やデータ可視化を重視する志向がある．

一方で自己符号化器などによる表現学習では，種々の下流タスクに有用な表現を得るための分布外汎化が重視される，と言えるだろう．

### 距離学習との関係

[近年，対照学習による表現学習が注目されている](NCL.qmd#sec-NCL4RL)．このアプローチには，目的関数にサンプル間の類似度に関する事前情報を含めやすいという美点がある．

このような表現学習法は **距離学習** (metric learning) とも呼ばれている．

多くの多様体学習手法や，$\R^d$ への埋め込み手法は，なんらかの意味でサンプル間の類似度を保存する埋め込みを求めている [@Agrawal+2021]．

この意味で，「距離学習」というキーワードは，表現学習と多様体学習の交差点を意味していると言えるだろう．

### 例：細胞間の類似度

現代のシークエンサー NGS (Next Generation Sequencer) では，単一の細胞が保持している mRNA の全体 scRNA-seq (single-cell RNA sequencing) を調べることができ，このような場合は極めて高次元のデータが大量に得られることになる．

例えば COVID-19 重症患者の末梢免疫の状態を調べるために末梢血単核細胞 PBMC^[(Peripheral Blood Mononuclear Cell)] から scRNA-seq を行った例 [@Wilk+2020] では，全部で $n=44,721$ の細胞のデータが，$p=26361$ 次元のスパースなベクトルを扱っている．^[なお，[@Wilk+2020] では最初の 50 の主成分がプロットされている．]

## 多次元尺度法 (MDS)

### 概要

多次元尺度法 (MDS: Multi-Dimensional Scaling) [@Torgenson1952], [@Kruskal1964] は，元のデータの「（非）類似度」を保存したままの低次元表現を探索する手法群である．

後続の手法はいずれも高次元データ $\{y_i\}_{i=1}^n\subset\R^p$ を所与とするが，MDS は本質的には類似度行列 $D\in M_n(\R)$ が与えられていればよく，解析対象は必ずしも $\R^p$-値のデータである必要はない．

類似度行列 $D$ が与えられたのちは，埋め込み $\{z_i\}_{i=1}^n\subset\R^d$ の要素間の類似度が $D$ に近くなるように埋め込みを学習する．

### PCA としての MDS

特にデータ $\{x_i\}_{i=1}^n\subset\R^p$ 間の Euclid 距離を $D$ とし，埋め込み $\{z_i\}_{i=1}^n\subset\R^d$ の間の経験共分散が $D$ に Hilbert-Schmidt ノルムに関して最小化することを [@Torgenson1952] は考えた．

これは [[@Eckart-Young1936] の定理](../FunctionalAnalysis/SVD.qmd) を通じて [PCA](HierarchicalModel.qmd#sec-PCA) に一致する，線型な次元縮約法となる．

[@Torgenson1952] のアプローチは **計量**多次元尺度法と呼ばれ，一般のデータに適用可能な [@Kruskal1964] のアプローチは**非計量**多次元尺度法と対照される．^[[@岡田謙介-加藤淳子2016] 参照．大変入門に良い日本語文献である．計量的多次元尺度法は **主座標分析** (PCoA: Principal Coordinate Analysis) [@Young-Hausholder1938] とも呼ばれる．]

### [@Kruskal1964] Stress

前述の通り，$D$ は Euclid 距離に基づくとは限らないし，そもそもデータはベクトルなど構造化されているものでなくても良い．

このような一般的な場面で $\{z_i\}_{i=1}^n\subset\R^d$ 間の類似度の $D$ との「近さ」を図る尺度として，Kruskal の Stress-1 [@Kruskal1964] がよく用いられる：
$$
\L(E):=\sqrt{\frac{\sum_{i_1<i_2}\paren{z_{i_1i_2}-d_{i_1i_2}}^2}{\sum_{i_1<i_2}d_{i_1i_2}^2}}.
$$

勾配法を用いた最適化ベースの推論手法が使えるが，この目的関数に関しては SMACOF (Scaling by Majorizing a Complementary Function) [@deLeeuw1977] という凸最適化アルゴリズムが提案されており，[`scikit-learn` にも実装されている](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.smacof.html)．

### [@Sammon1969] Stress

[@Sammon1969] は探索的データ解析の文脈から，データの「構造」をよく保つために，$z_{i_1i_2}$ の値が小さい場合は小さな変化も重視してくれるようにスケーリングを調整する新たなストレス関数を提案した：

$$
\frac{1}{\sum_{i_1<i_2}z_{i_1i_2}}\sum_{i_1<i_2}\frac{\paren{z_{i_1i_2}-d_{i_1i_2}}^2}{z_{i_1i_2}}.
$$

係数 $\frac{1}{\sum_{i_1<i_2}z_{i_1i_2}}$ の存在は，勾配法による推論を効率的にする．

### 多次元展開法 (MDU) {#sec-MDU}

多次元展開法 (MDU: Multi-Dimensional Unfolding) は，個人の選考順位データに対処するために [@Coombs1950] が提唱した，多次元尺度法 (MDS) の拡張である [@足立浩平2000]．

MDU ではさらに一般的な行列 $D$ を取ることができる．というのも，MDS では暗黙のうちに行と列は同一物であり，$D$ の対角成分は全て一定であるが，MDU では行と列で別の対象を取ることができる．

例えば，個体 $i$ が項目 $j$ をどれくらい好むか？を $D$ と取り，行と列を同一の平面上に [バイプロット](https://en.wikipedia.org/wiki/Biplot) [@Gabriel1971], [@Gower2004] する．

### [理想点推定](../Stat/IdealPoint.qmd)

特に政治科学の分野で用いられる多次元展開手法であり，$\R^d\;(d\le3)$ に埋め込むことが考えられる．

その際は $\R^d$ 上への観測に至るまでの階層モデル（潜在変数モデル）を立てて，全体を MCMC により推定する方法が，[@Martin-Quinn2002] 以来中心的である．

[@Imai2016]，[@三輪洋文2017] は変分 EM アルゴリズムにより推定している．

### 力指向グラフ展開 (FDL / SE)

[力学モデルによるグラフ描画法](https://ja.wikipedia.org/wiki/力学モデル_(グラフ描画アルゴリズム)) (Force-directed layout / Spring Embedder) [@Tutte1963], [@Eades1984], [@Kamada-Kawai1989] は，グラフの頂点を質点，辺をバネに見立てて，グラフを $\R^2$ 上に展開する方法である．

超大規模集積回路 (VLSI) の設計問題と両輪で発展してきた [@Fisk+1967], [@Quinn-Breuer1979]．

このグラフ埋め込み法はポテンシャルエネルギーをストレスと見立てた MDS とみなせる．

## Isomap {#sec-Isomap}

MDS 法の難点の１つに，データがある部分多様体上に存在する場合，その部分多様体上の測地距離を尊重できないという点がある．

このデータの部分多様体構造を尊重した測地距離をグラフにより捉え，MDS を実行する手法が Isomap [@Tenenbaum+2000] である．

::: {.callout-tip appearance="simple" icon="false" title="Isomap：データの多様体構造を尊重した MDS"}

1. データを頂点とした $K$-近傍グラフを構成する．
2. 測地距離を，このグラフ上の最短距離として近似する．
3. こうして得た近似測地距離を用いて MDS を行う．

:::

ステップ２においては Dijkstra 法を用いることができ，高速な計算が可能である．

しかし Isomap はデータの摂動に極めて弱いことが topological instability として知られている [@Balasubramanian-Schwartz2002]．

この安定性をカーネル法に外注する Robust Kernel Isomap [@Choi-Choi2007] も提案されている．

## 最大分散展開 (MVU)

### [カーネル PCA](../../2023/KernelMethods/KernelMethods4Mathematicians.qmd#sec-KPCA) (kPCA) [@Scholkopf+1998]

カーネル法の見地からは，従来の PCA は線型な核を用いた場合のカーネル主成分分析だったと相対化される．

しかし，少なくとも RBF カーネルを用いた場合は [@Weinberger+2004]，次元縮約の代わりにより高次元な空間に埋め込みがちである．

### 半正定値埋め込み

カーネル PCA を次元縮約のために用いたものが **半正定値埋め込み** (semidefinite embedding) または **最大分散展開** (MVU: Maximum Vairance Unfolding) [@Weinberger+2004] である．

これは，カーネル PCA による埋め込みの中でも，元データ $y$ と埋め込み $z$ の間で
$$
\norm{z_i-z_j}_2=\norm{y_i-y_j}_2,\qquad(i,j)\in G
$$
を $K$-近傍 $G$ に関して満たすような埋め込みの中で，
$$
\max_{z\in\R^d}\sum_{i,j}\norm{z_i-z_j}_2^2
$$
を最大にするものを求めることを考える．

幸い，これを満たすカーネル関数は半正定値計画によって解くことができ，このカーネル関数によるカーネル PCA 法が MVU である．

## 局所線型埋め込み (LLE)

### はじめに

ここまでの手法は畢竟，類似度行列 $Y$ に関して，kPCA は特徴空間上でのスペクトル分解，Isomap はデータのなす $K$-近傍グラフ上でのスペクトル分解を考えている．

これに対して，スパースなスペクトル分解を用いることで，データの局所的構造がさらに尊重できることが，**局所線型埋め込み** (LLE: Local Linear Embedding) [@Roweis-Saul2000] として提案された．

この方法は Isomap よりデータの摂動に関して頑健であることが知られている．

### アルゴリズム

この方法では，データ多様体の接空間に注目する．

まず，各点をその $K$-近傍点の線型結合で表す方法を，次のように学習する：
$$
\wh{W}=\min_{W}\sum_{i=1}^n\paren{x_i-\sum_{j=1}^nw_{ij}x_j}^2,\qquad\subjectto\begin{cases}w_{ij}=0&x_i,x_j\,\text{は}\,K\text{-近傍でない}\\\sum_{j=1}^Nw_{ij}=1&\text{任意の}\,i\in[N]\,\text{について}\end{cases}
$$
こうして得た $\wh{W}$ はスパース行列になる．この $W$ を通じて，局所構造を保った低次元埋め込みを構成する：
$$
\wh{Z}=\argmin_Z\sum_{i=1}^n\Norm{z_i-\sum_{j=1}^n\wh{w}_{ij}z_j}_2^2.
$$

この最適化問題は，$I_n-W$ の [特異値分解](../FunctionalAnalysis/SVD.qmd) に帰着する．

### Hessian 埋め込み (HE)

Hessian Eigenmaps [@Donoho-Grimes2003] は微分幾何学的見方を推し進め，Hessian からの情報を取り入れた局所線型埋め込みの変種である．

### 接空間配置 (TSA)

Tangent Space Alignment [@Zhang-Zha2004] はより明示的に接空間の構造に注目する．

## スペクトル埋め込み (SE / LE) {#sec-LE}



Laplacian Eigenmaps または Spectral Embedding [@Mikhali-Partha2001] は，データ点の $K$-近傍グラフ $(Y,E)$ 上で
$$
\L(Z):=\sum_{(i,j)\in E}W_{ij}\norm{z_i-z_j}_2^2,\qquad W_{ij}:=\exp\paren{-\frac{\norm{y_i-y_j}_2^2}{2\sigma^2}}
$$
を最小化する埋め込み $Z$ を求める方法である．

この目的関数 $\L(Z)$ は実は，$K$-近傍グラフ $(Y,E)$ の Laplacian 行列 $L$ に関して
$$
\L(Z)=2\Tr(Z^\top LZ)
$$
とも表せる．

この量は，$Z$ をグラフ上の関数と見た際の Dirichlet 汎函数になっており，グラフ上の関数としての「滑らかさ」の尺度となっている．

最も滑らかであるような $Z$ を学習することで，データの局所構造を最も反映した埋め込みが得られる，というアイデアである．

## $t$-分布型確率的近傍埋め込み (t-SNE)

### 概要

SNE [@Hinton-Roweis2002] では，$K$-近傍グラフを用いた [Isomap](#sec-Isomap) を，ハードな帰属からソフトな帰属へ，確率分布を用いて軟化する．

t-SNE [@Maaten-Hinton2008] では SNE が Gauss 核を用いていたところを Laplace 核（$t$-分布）を用いることで，より分散した表現を得ることを目指す．

### 確率的近傍埋め込み [SNE, @Hinton-Roweis2002] {#sec-SNE}

ハイパーパラメータ $\sigma_i$ を残して，
$$
p_{j|i}:=\frac{\exp\paren{-\frac{\abs{x_i-x_j}^2}{2\sigma_i^2}}}{\sum_{k\neq i}\exp\paren{-\frac{\abs{x_i-x_k}^2}{2\sigma_i^2}}}
$$
と定める．

この $(p_{j|i})$ と，潜在空間における帰属確率
$$
q_{j|i}:=\frac{e^{-\abs{z_i-z_j}^2}}{\sum_{k\neq i}e^{-\abs{z_i-z_k}^2}}
$$
と一致するように埋め込みを学習する．すなわち，訓練目標は
$$
\L:=\sum_{i=1}^n\KL(p_{-|i},q_{-|i})=\sum_{i,j=1}^np_{j|i}\log\frac{p_{j|i}}{q_{j|i}}
$$
と定める．

この目的関数は凸ではなく，SGD で訓練可能であるが特殊なノイズスケジュールなどのテクニックがある．^[どうやら相転移境界が近いため，アニーリングが必要？[@Wu-Fischer2020]．[@Murphy2022 p.700] 20.4.10.1節も参照．]

SNE は，ある事前分布を課した[スペクトル埋め込み](#sec-LE)とも見れる [@Carreira-Perpinan2010]．

### 目的関数の対称化

$p_{i|j},q_{i|j}$ ではなく，結合分布 $p_{ij},q_{ij}$ を用いることで，目的関数を対称化することができる．

このとき，目的関数の勾配は次のようになる：
$$
\nabla_{z_i}\L(Z)=2\sum_{j=1}^n(z_j-z_i)(p_{ij}-q_{ij}).
$$

### $t$-分布の導入

t-SNE [@Maaten-Hinton2008] は $p_{j|i}$ の定義に Gauss 核を用いていたところを，Cauchy 分布の密度（Poisson 核）に置き換えたものである：
$$
q_{ij}=\frac{\frac{1}{1+\abs{z_i-z_j}^2}}{\sum_{k<l}\frac{1}{1+\abs{z_k-z_l}^2}}.
$$

このことにより，元々遠かったデータ点を引き寄せ過ぎてしまうこと (crowding problem) を回避できる．

勾配は
$$
\nabla_{z_i}\L(Z)=4\sum_{j=1}^n(z_j-z_i)(p_{ij}-q_{ij})\frac{1}{1+\abs{z_i-z_j}^2}
$$
で与えられ，対称化された SNE に，$z_i,z_j$ の距離に従って調整する因子が追加された形になっている．

### 実装

t-SNE は $O(n^2)$ であるが，埋め込みの次元数が $d=2$ などの低次元であるとき，これを $O(n\log n)$ にまで加速する実装が知られている [@vanderMaaten2014]．

### 敏捷な埋め込み (EE)

t-SNE はハイパーパラメータ $\sigma_i^2$ の設定に敏感である上に訓練が局所解にトラップされるなど不安定で，またデータ内のノイズに弱いことが知られている [@Wattemnerg+2016]．

**敏捷な埋め込み** (EN: Elastic Net) [@Carreira-Perpinan2010] という，より安定的でノイズに頑健な手法が目的関数を修正することで得られている．

### LargeVis

LargeVis [@Tang+2016] は t-SNE の計算量を軽減させるために，$K$-近傍グラフの計算に近似手法である **ランダム射影木** [@DasGupta-Freud2008] を導入する．

### UMAP

UMAP (Uniform Manifold Approximation and Projection) [@McInnes+2018] は t-SNE より高速で，大域的構造をより尊重する手法として提案された．

[![UMAP and t-SNE applied to the Fashion MNIST dataset; tap to visit https://pair-code.github.io/understanding-umap/](Images/UMAPvSNE.png)](https://pair-code.github.io/understanding-umap/)

### モデルベース手法

t-SNE, VargeVis, UMAP はいずれも確率を導入しているが，完全にモデルベースの発想をしているわけではない．

[@Saul2020] はこれらの手法を [潜在変数モデル](HierarchicalModel.qmd) として定式化し，データサイズに合わせて EM アルゴリズムをはじめとした推定手法を議論している．

## 拡散写像

### はじめに

**拡散埋め込み** (Diffusion Map) [@Coifman+2005] では，データ上に乱歩を定めることでデータ多様体の局所構造を捉える．

この方法は Isomap [-@sec-Isomap] でグラフを用いて測地距離を近似したよりも，頑健なアルゴリズムを与える．

### PHATE

PHATE (Heat Diffusion for Affinity-based Transition Embedding) [@Moon+2019] は DEMaP (Denoised Embedding Manifold Preservation) という情報理論に基づく距離を定義し，データの局所構造を捉える．

## 自己組織化写像 (SOM)

Self-organizing Map [@Kohonen1982], [@Kohonen2013] は，ニューラルネットワークのダイナミクスを利用した，クラスタリングベースの次元縮約法である．

### IVIS

IVIS [@Szubert+2019] は [三つ子損失を取り入れたシャムネットワーク](Kernel.qmd#sec-triplet-loss) による距離学習に基づく次元縮約法である．

## Mapper

Mapper [@Singh+2007] は，距離空間 $E$ 上のデータ $\{x_i\}\subset E$ を，関数 $f:E\to\R$ が定める分解を用いてグラフとして図示する方法である．他手法とは違い，出力が $\R^d$ への埋め込みではなく，グラフである点に注意．

::: {.callout-tip appearance="simple" icon="false" title="Mapper アルゴリズム^[このアルゴリズムは Morse 理論における概念である Reeb グラフの拡張と見れる．[@Oudot2016] のスライドや [@Schnider2024] の講義資料も参照．]"}

1. 値の空間 $\R$ 上の開被覆 $(I_i)$ の引き戻し $\cU:=(f^{-1}(I_i))$ として誘導されるグラフを，連結成分のみからなるように細分して $\cV=(U_i)_{i\in I}$ を得る．
2. $\cV$ の [神経複体](https://en.wikipedia.org/wiki/Nerve_complex)
    $$
    N(\cV):=\Brace{J\fsub I\,\middle|\,\bigcap_{j\in J}U_j\ne\emptyset}
    $$
    またはその近傍グラフを出力とする．

:::

[@Emerson+2023] では特許のデータを用い，各企業を技術空間 $\R^{430}$ 内に埋め込んだ後，mapper によりグラフ化したところ，企業の独自戦略が可視化できるという：

{{< video https://www.youtube.com/embed/0LQpJiecCvw?si=I-6R3dn8EAAG8xs1 >}}

## 終わりに {.appendix}

結局機械学習が日々の統計的営みに与えた最大のインパクトは，ニューラルネットワークや生成モデル自体というより，高精度な非線型表現学習であると言えるかもしれない．

例えば [@Hoffmann+2019] は [IAF](../Samplers/NF.qmd#sec-NF-Bayes) [@Kingma+2016] を用いて目標分布を学習し，学習された密度 $q$ で変換後の分布から MCMC サンプリングをすることで効率がはるかに改善することを報告した．実際，フローによる変換を受けた後は対象分布は正規分布に近くなることから，MCMC サンプリングを減速させる要因の多くが消滅していることが期待される．

多くの他の統計的な困難も，良い表現学習された空間上（あるいはカーネル空間上）で実行することで回避することができるということになるかもしれない．これが最初に SVM が機械学習に与えた希望の形であったし，ニューラルネットワークに対する飽くなき理論的興味の源泉でもある．データの視覚化や探索的データ解析の意味では，よき潜在表現を必要としているのは人間も然りである．

## 文献 {.appendix}

[@Yao2011] は LLE, Laplacian Eigenmaps, カーネル PCA, Diffusion Maps の関連性を扱っている講義スライドである．

[@Wattemnerg+2016] はインタラクティブに t-SNE の性質を理解できるウェブページである．[Understanding UMAP](https://pair-code.github.io/understanding-umap/) も同様にして，UMAP と t-SNE の比較を与えている．

[@Murphy2022] 第20章は次元縮約という題で，PCA, FA から自己符号化器，多様体学習を解説している．[@Burges2010] が同じテーマのしばらく前のサーベイである．

多様体学習に関しては [@本武陽一2017] も注目．

PCA は Kosambi-Karhunen-Loéve 変換ともいう [@Bishop-Bishop2024 p.497]．PCA からとにかくスペクトルとの関連が深い．