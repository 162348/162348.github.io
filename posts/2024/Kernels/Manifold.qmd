---
title: "多様体学習"
author: "司馬 博文"
date: 7/30/2024
date-modified: 7/30/2024
categories: [Deep]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 生成・表現学習と深い関係にあるタスクに，次元削減と多様体学習がある．一般に表現学習はパラメトリックであるとするならば，次元削減と多様体学習はノンパラメトリックな表現と視覚化の学習が目標である．
---

{{< include ../../../_preamble.qmd >}}

結局機械学習が日々の統計的営みに与えた最大のインパクトは，ニューラルネットワークや生成モデル自体というより，高精度な非線型表現学習であると言えるかもしれない．^[例えば [@Hoffmann+2019] は IAF を用いて目標分布を学習し，学習された密度 $q$ で変換後の分布から MCMC サンプリングをすることで効率がはるかに改善することを報告した．実際，フローによる変換を受けた後は対象分布は正規分布に近くなることから，MCMC サンプリングを減速させる要因の多くが消滅していることが期待される．多くの他の統計的な困難も，良い表現学習された空間上（あるいはカーネル空間上）で実行することで回避することができるということになるかもしれない．データの視覚化や探索的データ解析の意味では，人間も然りである．]

## 主成分分析 (PCA)

### 導入

$K$-平均法は混合モデルの最尤推定アルゴリズムとみなせたように，主成分分析が一般の線型 Gauss 潜在変数モデルにおける最尤推定とみなせる．

一方で，一般の非線型な潜在変数モデルに対する最尤推定とは [自己符号化器](Deep4.qmd#sec-AE) に他ならない．すなわち，４層以上のニューラルネットワークを用いることで，PCA を非線型化して一般化することができる．

自己符号化器については次稿で扱う：

```{=html}
<div class="article-card-container">
    <div class="article-card">
        <a href="https://162348.github.io/posts/2024/Kernels/Deep4.html" target="_blank">
            <img src="https://162348.github.io/posts/2024/Kernels/VAE_files/figure-html/fig-reconstruction-output-1.png" alt="Article Image" class="article-image">
            <div class="article-content">
                <h3 class="article-title">VAE：変分自己符号化器</h3>
                <p class="article-description">深層生成モデル３</p>
            </div>
        </a>
    </div>
</div>
```

### 他の多変量解析法との関係

**主成分分析** (PCA: Principal Component Analysis) も，$K$-平均法と同様に，次元削減，（非可逆）データ圧縮，特徴抽出，データ可視化の用途に用いられる．^[Kosambi-Karhunen-Loéve 変換ともいう [@Bishop-Bishop2024 p.497]．]

主成分分析は，値の分散が最大となるような線型射影を求める問題 [@Hotelling33-PCA] とも，また（おろした垂線の足の二乗距離和の意味で）コストが最小になるような線型射影を求める問題 [@Pearson01-PCA] とも見ることができ，歴史的には広い分野で研究されてきた．

また，複数のデータに対して，同時に主成分分析を行う場合，これを **正準相関分析** (canonical correlation analysis) [@Hotelling36] ともいう．

一方で潜在変数を導入し，「データ＝共通因子部＋独立因子部＋誤差」という分解を得ることを目標とする場合，これを **因子分析** (factor analysis) という [@足立浩平23-因子分析]．^[すなわち，主成分分析が低階数近似で，因子分析が高階数近似になっている，という説明が１つありえる．]

いずれも確率的モデリングとは無縁に見えるが，これらはいずれも線型 Gauss な潜在変数モデルの特殊化と見れる [@Tipping-Bishop1999]．^[[@Bishop-Bishop2024] 第16章．] 従って，EM アルゴリズムによる効率的な推定を初めとした，Bayes 学習，ニューラルネットワークなど，グラフィカルモデルとして様々な拡張が可能である．

## 多様体学習

### 導入

**非線型な次元削減** を行うというタスクは，「高次元データは低次元の部分多様体としての構造を持つ」という **多様体仮説** [@Fefferman+2016] を作業仮設として，**多様体学習** の名前でも呼ばれる．^[[@本武陽一2017] も注目．]

特に多様体学習と呼ばれる際は，知識発見やデータ可視化を重視する．自己符号化器の文脈とは違い，表現学習としての用途を考えるときでも，分布外汎化が重視されるとは限らないという志向の違いがある [@Agrawal+2021]．

### 例

現代のシークエンサー NGS (Next Generation Sequencer) では，単一の細胞が保持している mRNA の全体 scRNA-seq (single-cell RNA sequencing) を調べることができ，このような場合は極めて高次元のデータが大量に得られることになる．

例えば COVID-19 重症患者の末梢免疫の状態を調べるために末梢血単核細胞 PBMC (Peripheral Blood Mononuclear Cell) から scRNA-seq を行った例 [@Wilk+2020] では，全部で $n=44,721$ の細胞のデータが，$p=26361$ 次元のスパースなベクトルで表現されているものから，多様体学習が必要である．なお，[@Wilk+2020] では最初の 50 の主成分がプロットされている．

[@Agrawal+2021] では MDE (Minimum-Distortion Embedding) の観点からこの例を論じている．

### 多次元尺度構成法 (MDS)

多次元尺度構成法 (MDS: Multi-Dimensional Scaling) は，元のデータの「類似度」を保存したままの低次元表現を探索する手法群である．

特に古典的な MDS 法は PCA に一致する．

### Isomap

### カーネル PCA

### MVU (Maximum Vairance Unfolding)

### LLE (Local Linear Embedding)

### Laplacian Eigenmaps

### t-SNE

### 拡散写像 (Diffusion Map)

## 文献 {.appendix}

[@Murphy2022] 第20章は次元削減という題で，PCA, FA から自己符号化器，多様体学習を解説している．[@Burges2010] が同じテーマのしばらく前のサーベイである．

[@Tipping-Bishop1999] の著者である [@Bishop-Bishop2024] 第16章も読みたい．

[@本武陽一2017] も注目．