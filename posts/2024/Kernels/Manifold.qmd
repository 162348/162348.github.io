---
title: "次元縮約法の概観"
subtitle: "最古にして最難のタスクと多様体学習"
author: "司馬 博文"
date: 7/30/2024
date-modified: 8/10/2024
categories: [Deep, Nature, Statistics]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
    - ../Stat/IdealPoint.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 生成・表現学習と深い関係にあるタスクに，次元縮約がある．非線型な次元縮約法は多様体学習の名前の下でも研究されている．表現学習とも関連が深いが，一般に表現学習はパラメトリックであるとするならば，次元縮約ではノンパラメトリックな表現と視覚化の学習が目標である．
---

{{< include ../../../assets/_preamble.qmd >}}

## 主成分分析と因子分析 (PCA and FA)

### はじめに

[$K$-平均法は特定の形を持った混合モデルの最尤推定アルゴリズムとみなせた](../Computation/VI.qmd)ように，主成分分析は線型 Gauss 潜在変数モデルにおける最尤推定の特殊形ともみなせる．

主成分分析をはじめとした線型な次元縮約法は，本稿で扱う手法の中でほとんど唯一正確に解けるものであり，より一般の次元縮約問題／多様体学習の問題は近似的にしか解けず，効率的な推論アルゴリズムを構成することが重要なテーマになっている [@Agrawal+2021]．

例えば [自己符号化器](Deep4.qmd#sec-AE) は，まさに非線型な潜在変数モデルに対する最尤推定を行っており，４層以上のニューラルネットワークを用いることで PCA を非線型化して一般化することができる．

### 他の多変量解析法との関係

**主成分分析** (PCA: Principal Component Analysis) も，$K$-平均法と同様に，次元縮約，（非可逆）データ圧縮，特徴抽出，データ可視化の用途に用いられる．^[Kosambi-Karhunen-Loéve 変換ともいう [@Bishop-Bishop2024 p.497]．]

主成分分析は，おろした垂線の足の二乗距離和の意味でコストが最小になるような線型射影を求める問題 [@Pearson01-PCA] として最初に登場し，値の分散が最大となるような線型射影を求める問題 [@Hotelling33-PCA] として心理学分野でも取り上げられて大きく発展した．

また，複数のデータに対して，同時に主成分分析を行う場合，これを **正準相関分析** (canonical correlation analysis) [@Hotelling36] ともいう．

一方で潜在変数を導入し，「データ＝共通因子部＋独立因子部＋誤差」という分解を得ることを目標とする場合，これを **因子分析** (factor analysis) という [@足立浩平23-因子分析]．^[すなわち，主成分分析が低階数近似で，因子分析が高階数近似になっている，という説明が１つありえる．]

いずれも確率的モデリングとは無縁に見えるが，これらはいずれも線型 Gauss な潜在変数モデルの特殊化と見れる [@Tipping-Bishop1999]．^[[@Bishop-Bishop2024] 第16章．] 従って，EM アルゴリズムによる効率的な推定を初めとした，Bayes 学習，ニューラルネットワークなど，グラフィカルモデルとして様々な拡張が可能である．

## 非線型次元縮約／多様体学習

### はじめに

多様体学習とは，非線型な次元削減手法のことをいう．

#### 多様体仮説

「多様体」の名前の由来は，「高次元データは低次元の部分多様体としての構造を持つ」という **多様体仮説** [@Fefferman+2016] である．^[[@本武陽一2017] も注目．]

特に多様体学習と呼ばれる際は，知識発見やデータ可視化を重視する志向がある．

一方で自己符号化器などによる表現学習では，種々の下流タスクに有用な表現を得るための分布外汎化が重視される，と言えるだろう．

#### 距離学習との関係

[近年，対照学習による表現学習が注目されている](NCL.qmd#sec-NCL4RL)．このアプローチには，目的関数にサンプル間の類似度に関する事前情報を含めやすいという美点がある．

このような表現学習法は **距離学習** (metric learning) とも呼ばれている．

多くの多様体学習手法や，$\R^d$ への埋め込み手法は，なんらかの意味でサンプル間の類似度を保存する埋め込みを求めている [@Agrawal+2021]．

この意味で，「距離学習」というキーワードは，表現学習と多様体学習の交差点を意味していると言えるだろう．

#### 例：細胞間の類似度

現代のシークエンサー NGS (Next Generation Sequencer) では，単一の細胞が保持している mRNA の全体 scRNA-seq (single-cell RNA sequencing) を調べることができ，このような場合は極めて高次元のデータが大量に得られることになる．

例えば COVID-19 重症患者の末梢免疫の状態を調べるために末梢血単核細胞 PBMC^[(Peripheral Blood Mononuclear Cell)] から scRNA-seq を行った例 [@Wilk+2020] では，全部で $n=44,721$ の細胞のデータが，$p=26361$ 次元のスパースなベクトルを扱っている．^[なお，[@Wilk+2020] では最初の 50 の主成分がプロットされている．]

### 多次元尺度法 (MDS)

#### 概要

多次元尺度法 (MDS: Multi-Dimensional Scaling) [@Torgenson1952], [@Kruskal1964] は，元のデータの「（非）類似度」を保存したままの低次元表現を探索する手法群であり，基礎心理学における研究 [@Eckart-Young1936] に端を発する．^[[@Torgenson1952] のアプローチは，**計量**多次元尺度法と呼ばれ，[@Kruskal1964] のアプローチは**非計量**多次元尺度法という．[@岡田謙介-加藤淳子2016] は大変入門に良い日本語文献である．]

特に $\ell_2$-距離を用いた [@Torgenson1952] の古典的な MDS 法は PCA に一致することもある．

$I$ 個の対象の非類似度を表す観測行列 $Y\in M_I(\R)$ に対して，$J<I$ 次元の行列 $X\in M_J(\R)$ を，
$$
d(i_1,i_2):=\norm{x_{i_1-}-x_{i_2-}}_p
$$
を要素とする距離行列 $D=(d(i_1,i_2))_{i_1,i_2=1}^I$ が $Y$ に「近く」なるように学習する方法をいう．

#### [@Kruskal1964] Stress

$Y,D$ の「近さ」を図る尺度として，Kruskal の Stress-1 [@Kruskal1964] がよく用いられる：^[[@岡田謙介-加藤淳子2016] を参考にした．]
$$
\sqrt{\frac{\sum_{i_1<i_2}\paren{y_{i_1i_2}-d(i_1,i_2)}^2}{\sum_{i_1<i_2}d(i_1,i_2)^2}}.
$$

勾配法を用いた最適化ベースの推論手法も提案されている [@deLeeuw2005]．

#### [@Sammon1969] Stress

[@Sammon1969] は探索的データ解析の文脈から，データの「構造」をよく保つために，$y_{i_1i_2}$ の値が小さい場合は小さな変化も重視してくれるように，新たなストレス関数を提案した：

$$
\frac{1}{\sum_{i_1<i_2}y_{i_1i_2}}\sum_{i_1<i_2}\frac{\paren{y_{i_1i_2}-d(i_1,i_2)}^2}{y_{i_1i_2}}.
$$

係数 $\frac{1}{\sum_{i_1<i_2}y_{i_1i_2}}$ の存在は，勾配法による推論を効率的にする．

#### 多次元展開法 (MDU)

多次元展開法 (MDU: Multi-Dimensional Unfolding) は，個人の選考順位データに対処するために [@Coombs1950] が提唱した，多次元尺度法 (MDS) の拡張である [@足立浩平2000]．

MDS では類似性行列 $Y$ が与えられるところが，行と列に互いに異なる対象が並び，その間の親近性が与えられる．すなわち，各行と各列を与える対象を同一視し，同じ座標に埋め込む場合の MDU が，MDS であったと言える．

### Isomap [@Tenenbaum+2000]

MDS 法の難点の１つに，データがある部分多様体上に存在する場合，その部分多様体上の測地距離を尊重できないという点がある．

この測地距離を，グラフにより捉える手法が Isomap [@Tenenbaum+2000] である．

::: {.callout-tip appearance="simple" icon="false"}

1. データを頂点とした $K$-最近傍グラフを構成する．
2. 測地距離を，このグラフ上の最短距離として近似する．
3. こうして得た近似測地距離を用いて MDS を行う．

:::

ステップ２においては Dijkstra 法を用いることができ，高速な計算が可能である．

しかし Isomap はデータの摂動に極めて弱いことが topological instability として知られている [@Balasubramanian-Schwartz2002]．

この安定性をカーネル法に外注する Robust Kernel Isomap [@Choi-Choi2007] も提案されている．

### 最大分散展開 [MVU, @Weinberger+2004]

#### [カーネル PCA](../../2023/KernelMethods/KernelMethods4Mathematicians.qmd#sec-KPCA) (kPCA) [@Scholkopf+1998]

カーネル法の見地からは，従来の PCA は線型な核を用いた場合のカーネル主成分分析だったと相対化される．

しかし，少なくとも RBF カーネルを用いた場合は [@Weinberger+2004]，次元縮約の代わりにより高次元な空間に埋め込みがちである．

#### 半正定値埋め込み

カーネル PCA を次元縮約のために用いたものが **半正定値埋め込み** (semidefinite embedding) または **最大分散展開** (MVU: Maximum Vairance Unfolding) [@Weinberger+2004] である．

これは，カーネル PCA による埋め込みの中でも，元データ $y$ と埋め込み $z$ の間で
$$
\norm{z_i-z_j}_2=\norm{y_i-y_j}_2,\qquad(i,j)\in G
$$
を $K$-最近傍 $G$ に関して満たすような埋め込みの中で，
$$
\max_{z\in\R^d}\sum_{i,j}\norm{z_i-z_j}_2^2
$$
を最大にするものを求めることを考える．

幸い，これを満たすカーネル関数は半正定値計画によって解くことができ，このカーネル関数によるカーネル PCA 法が MVU である．

### 局所線型埋め込み [LLE, @Roweis-Saul2000]

ここまで手法は畢竟，類似度行列 $Y$ に関して，kPCA は特徴空間上でのスペクトル分解，Isomap はデータのなす $K$-最近傍グラフ上でのスペクトル分解を考えている．

これに対して，スパースなスペクトル分解を用いることで，データの局所的構造がさらに尊重できることが，**局所線型埋め込み** (LLE: Local Linear Embedding) [@Roweis-Saul2000] として提案された．

この方法では，データ多様体の接空間に注目する．

まず，各点をその $K$-最近傍点の線型結合で表す：
$$
\wh{W}=\min_{W}\sum_{i=1}^n\paren{x_i-\sum_{j=1}^nw_{ij}x_j}^2,\qquad\subjectto\begin{cases}w_{ij}=0&x_i,x_j\,\text{は}\,K\text{-近傍でない}\\\sum_{j=1}^Nw_{ij}=1&\text{任意の}\,i\in[N]\,\text{について}\end{cases}
$$
そして，この局所構造を保った低次元埋め込みを構成する：
$$
\wh{Z}=\argmin_Z\sum_{i=1}^n\Norm{z_i-\sum_{j=1}^n\wh{w}_{ij}z_j}_2^2.
$$

この方法は Isomap より頑健であることが知られている．

### スペクトル埋め込み [@Mikhali-Partha2001]

Laplacian Eigenmaps または spectral embedding [@Mikhali-Partha2001] は，データ点の $K$-最近傍グラフ $(Y,E)$ 上で
$$
\L(Z):=\sum_{(i,j)\in E}W_{ij}\norm{z_i-z_j}_2^2,\qquad W_{ij}:=\exp\paren{-\frac{\norm{y_i-y_j}_2^2}{2\sigma^2}}
$$
を最小化する埋め込み $Z$ を求める方法である．

この目的関数 $\L(Z)$ は実は，$K$-最近傍グラフ $(Y,E)$ の Laplacian 行列 $L$ に関して
$$
\L(Z)=2\Tr(Z^\top LZ)
$$
を満たすようになっている．

この量は，$Z$ をグラフ上の関数と見た際の Dirichlet 汎函数になっており，グラフ上の関数としての「滑らかさ」の尺度となっている．

最も滑らかであるような $Z$ を学習することで，データの局所構造を最も反映した埋め込みが得られる，というアイデアである．

### $t$-分布型確率的近傍埋め込み [t-SNE, @Maaten-Hinton2008]

#### 確率的近傍埋め込み [SNE, @Hinton-Roweis2002]

### 拡散埋め込み [@Coifman+2005]

## 埋め込み

### はじめに

一般に埋め込みは，本稿で取り扱った次元縮約法よりも広いタスクである．

いわば，高次元データの低次元空間への埋め込みが次元縮約法であったのである．

一方で，埋め込みの用途は次元縮約に限らない．機械学習，特に生成モデリング分野で埋め込みは表現学習とともに不可欠な要素となっている．同時に，種々の科学分野では，複雑な対象を簡単で低次元の空間に埋め込むことは基本的な研究課題となっている．

このような，必ずしも次元縮約を目標とはしない埋め込み手法を章を変えて引き続き記述する．[@Agrawal+2021] も参照．

### [理想点推定](../Stat/IdealPoint.qmd)

特に政治科学の分野で用いられる手法であり，$\R^d\;(d\le3)$ に埋め込むことが考えられる．

その際は $\R^d$ 上への観測に至るまでの階層モデル（潜在変数モデル）を立てて，全体を MCMC により推定する方法が，[@Martin-Quinn2002] 以来中心的である．

[@Imai2016]，[@三輪洋文2017] は変分 EM アルゴリズムにより推定している．

### Mapper [@Singh+2007]

[@Emerson+2023] では特許のデータを用い，各企業を技術空間 $\R^{430}$ 内に埋め込んだ後，mapper [@Singh+2007] によりグラフ化したところ，企業の独自戦略が可視化されたという．

{{< video https://www.youtube.com/embed/0LQpJiecCvw?si=I-6R3dn8EAAG8xs1 >}}

## 終わりに {.appendix}

結局機械学習が日々の統計的営みに与えた最大のインパクトは，ニューラルネットワークや生成モデル自体というより，高精度な非線型表現学習であると言えるかもしれない．

例えば [@Hoffmann+2019] は [IAF](../Samplers/NF.qmd#sec-NF-Bayes) [@Kingma+2016] を用いて目標分布を学習し，学習された密度 $q$ で変換後の分布から MCMC サンプリングをすることで効率がはるかに改善することを報告した．実際，フローによる変換を受けた後は対象分布は正規分布に近くなることから，MCMC サンプリングを減速させる要因の多くが消滅していることが期待される．

多くの他の統計的な困難も，良い表現学習された空間上（あるいはカーネル空間上）で実行することで回避することができるということになるかもしれない．これが最初に SVM が機械学習に与えた希望の形であったし，ニューラルネットワークに対する飽くなき理論的興味の源泉でもある．データの視覚化や探索的データ解析の意味では，よき潜在表現を必要としているのは人間も然りである．

## 文献 {.appendix}

[@Murphy2022] 第20章は次元縮約という題で，PCA, FA から自己符号化器，多様体学習を解説している．[@Burges2010] が同じテーマのしばらく前のサーベイである．

[@Tipping-Bishop1999] の著者である [@Bishop-Bishop2024] 第16章も読みたい．

[@本武陽一2017] も注目．