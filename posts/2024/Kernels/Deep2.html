<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="司馬 博文">
<meta name="dcterms.date" content="2024-02-11">

<title>Hirofumi Shiba - 数学者のための深層学習２</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../profile.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Hirofumi Shiba - 数学者のための深層学習２">
<meta property="og:description" content="A Blog by a Bayesian Computation Researcher">
<meta property="og:site-name" content="Hirofumi Shiba">
<meta name="twitter:title" content="Hirofumi Shiba - 数学者のための深層学習２">
<meta name="twitter:description" content="数学者のために，深層学習モデルの例として，トランスフォーマーを概説する．">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../recent.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Sessions.html" rel="" target="">
 <span class="menu-text">Sessions</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Japanese.html" rel="" target="">
 <span class="menu-text">日本語</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">数学者のための深層学習２</h1>
            <p class="subtitle lead">トランスフォーマー</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Kernel</div>
                <div class="quarto-category">Math Notes</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>司馬 博文 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="abstract-title">概要</div>
      数学者のために，深層学習モデルの例として，トランスフォーマーを概説する．
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#トランスフォーマー" id="toc-トランスフォーマー" class="nav-link active" data-scroll-target="#トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#名前の由来と背景" id="toc-名前の由来と背景" class="nav-link" data-scroll-target="#名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</a></li>
  <li><a href="#注意機構" id="toc-注意機構" class="nav-link" data-scroll-target="#注意機構"><span class="header-section-number">1.2</span> 注意機構</a></li>
  <li><a href="#トランスフォーマーの全体" id="toc-トランスフォーマーの全体" class="nav-link" data-scroll-target="#トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</a></li>
  </ul></li>
  <li><a href="#言語トランスフォーマー" id="toc-言語トランスフォーマー" class="nav-link" data-scroll-target="#言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#言語の取り扱い" id="toc-言語の取り扱い" class="nav-link" data-scroll-target="#言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</a></li>
  <li><a href="#従来の言語モデル" id="toc-従来の言語モデル" class="nav-link" data-scroll-target="#従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</a></li>
  <li><a href="#トランスフォーマーによる言語モデル" id="toc-トランスフォーマーによる言語モデル" class="nav-link" data-scroll-target="#トランスフォーマーによる言語モデル"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデル</a></li>
  </ul></li>
  <li><a href="#大規模言語モデル" id="toc-大規模言語モデル" class="nav-link" data-scroll-target="#大規模言語モデル"><span class="header-section-number">3</span> 大規模言語モデル</a>
  <ul class="collapse">
  <li><a href="#名前の由来と背景-1" id="toc-名前の由来と背景-1" class="nav-link" data-scroll-target="#名前の由来と背景-1"><span class="header-section-number">3.1</span> 名前の由来と背景</a></li>
  <li><a href="#sec-foundation-model" id="toc-sec-foundation-model" class="nav-link" data-scroll-target="#sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</a></li>
  <li><a href="#プロンプトエンジニアリング" id="toc-プロンプトエンジニアリング" class="nav-link" data-scroll-target="#プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</a></li>
  <li><a href="#rag" id="toc-rag" class="nav-link" data-scroll-target="#rag"><span class="header-section-number">3.4</span> RAG</a></li>
  </ul></li>
  <li><a href="#sec-multimodal-transformer" id="toc-sec-multimodal-transformer" class="nav-link" data-scroll-target="#sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#画像トランスフォーマー-vit" id="toc-画像トランスフォーマー-vit" class="nav-link" data-scroll-target="#画像トランスフォーマー-vit"><span class="header-section-number">4.1</span> 画像トランスフォーマー (ViT)</a></li>
  <li><a href="#画像生成トランスフォーマー" id="toc-画像生成トランスフォーマー" class="nav-link" data-scroll-target="#画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</a></li>
  <li><a href="#text-to-image-トランスフォーマー" id="toc-text-to-image-トランスフォーマー" class="nav-link" data-scroll-target="#text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</a></li>
  <li><a href="#image-to-text-トランスフォーマー" id="toc-image-to-text-トランスフォーマー" class="nav-link" data-scroll-target="#image-to-text-トランスフォーマー"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</a></li>
  <li><a href="#動画トランスフォーマー" id="toc-動画トランスフォーマー" class="nav-link" data-scroll-target="#動画トランスフォーマー"><span class="header-section-number">4.5</span> 動画トランスフォーマー</a></li>
  <li><a href="#世界モデルとしてのトランスフォーマー" id="toc-世界モデルとしてのトランスフォーマー" class="nav-link" data-scroll-target="#世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</a></li>
  <li><a href="#音声トランスフォーマー" id="toc-音声トランスフォーマー" class="nav-link" data-scroll-target="#音声トランスフォーマー"><span class="header-section-number">4.7</span> 音声トランスフォーマー</a></li>
  <li><a href="#text-to-speech-トランスフォーマー" id="toc-text-to-speech-トランスフォーマー" class="nav-link" data-scroll-target="#text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</a></li>
  </ul></li>
  <li><a href="#近年の動向" id="toc-近年の動向" class="nav-link" data-scroll-target="#近年の動向"><span class="header-section-number">5</span> 近年の動向</a>
  <ul class="collapse">
  <li><a href="#llama-の一般公開とその影響" id="toc-llama-の一般公開とその影響" class="nav-link" data-scroll-target="#llama-の一般公開とその影響"><span class="header-section-number">5.1</span> LLaMA の一般公開とその影響</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="hidden">
$$
<p>%%% 演算子 </p>
%%% 線型代数学
<p>%%% 複素解析学 %%% 集合と位相 </p>
<p>%%% 形式言語理論 %%% Graph Theory </p>
%%% 多様体 %%% 代数 %%% 代数的位相幾何学 %%% 微分幾何学
%%% 函数解析
%%% 積分論
<p>%%% Fourier解析 %%% 数値解析 </p>
%%% 確率論
<p>%%% 情報理論 %%% 量子論 %%% 最適化 %%% 数理ファイナンス </p>
<p>%%% 偏微分方程式 %%% 常微分方程式 %%% 統計力学 %%% 解析力学 </p>
%%% 統計的因果推論 %%% 応用統計学 %%% 数理統計
<p>%%% 計量経済学 </p>
%%% 無限次元統計模型の理論
<p>%%% Banach Lattices </p>
<p>%%% 圏 %代数の圏 %Metric space &amp; Contraction maps %確率空間とMarkov核の圏 %Sober space &amp; continuous map %Category of open subsets %Category of sheave %Category of presheave, PSh(C)=[C^op,set]のこと %Convergence spaceの圏 %一様空間と一様連続写像の圏 %フレームとフレームの射 %その反対圏 %滑らかな多様体の圏 %Quiverの圏 </p>
%%% SMC
%%% 括弧類
<p>%%% 予約語 </p>
<p>%%% 略記 </p>
<p>%%% 矢印類 $$</p>
</div>
<section id="トランスフォーマー" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</h2>
<section id="名前の由来と背景" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</h3>
<p>トランスフォーマーは，注意 (attension) <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> という機構を通じて，時系列データの効率的な内部表現を獲得することの出来るモデルである．これは，内部表現ベクトルを，次元を変えずにより良いものに「変換する」というところから名前が付けられている．</p>
<p>初めは自然言語処理（特に機械翻訳）の文脈で導入されたが，画像，動画でも抜群の性能を発揮する上に，これらを組み合わせることもできる（第 <a href="#sec-multimodal-transformer">4</a> 節）．</p>
<p>さらに，トランスフォーマーはアーキテクチャとしてシンプルであり，大規模なデータセットで大規模なモデルを訓練することが出来るスケーラビリティが魅力である．これより，一度訓練した大規模モデルを種々の下流タスクに応用することが可能になり，基盤モデルという新たな存在を生み出した（第 <a href="#sec-foundation-model">3.2</a> 節）．</p>
<p>このことが，理論的に優れているが複雑なモデルを開発することよりも，スケーラブルでシンプルなモデルの方が，これを大規模にすることで実用的に高い性能を達成しやすいという共通了解を生みつつある．</p>
</section>
<section id="注意機構" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="注意機構"><span class="header-section-number">1.2</span> 注意機構</h3>
<p>注意機構は，元々機械翻訳に用いられていたエンコーダー・デコーダー型の RNN の性能を向上させる機構として提案された <span class="citation" data-cites="Bahdanau+2015">(<a href="#ref-Bahdanau+2015" role="doc-biblioref">Bahdanau et al., 2015</a>)</span>．その後，<span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> の <em>Attention is All You Need</em> とは，注意機構のみが重要で，RNN としての構造（や画像では畳み込みの構造）を排してシンプルにした方が更に性能が向上する，という報告である．</p>
<p>時系列データの解析では，そして自然言語処理ではとりわけ，文脈というものが重要であり，同じデータでも文脈が違えば全く違う意味を持つということがよくある．</p>
<section id="枠組み" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="枠組み"><span class="header-section-number">1.2.1</span> 枠組み</h4>
<p>トランスフォーマーに入力する系列を <span class="math inline">\(\{x^n\}_{n=1}^N\subset\mathbb{R}^D\)</span> で表す．各 <span class="math inline">\(x^n\)</span> を <strong>トークン</strong> (token) という．画像では <strong>パッチ</strong> (patch) ともいう．</p>
<p><span class="math inline">\(X:=(x^n)_{n=1}^N\in M_{ND}(\mathbb{R})\)</span> とも表す．</p>
</section>
<section id="自己注意機構のプロトタイプ" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="自己注意機構のプロトタイプ"><span class="header-section-number">1.2.2</span> 自己注意機構のプロトタイプ</h4>
<p>自己注意機構とは，<span class="math inline">\(Y=AX\)</span> によって定まる <span class="math inline">\(M_{ND}(\mathbb{R})\)</span> 上の線型変換 <span class="math inline">\(X\mapsto Y\)</span> のことである： <span id="eq-1"><span class="math display">\[
y^n=\sum_{m=1}^N a^n_mx^m,
\tag{1}\]</span></span> <span id="eq-2"><span class="math display">\[
a^n_m=\frac{e^{(x^n)^\top x^m}}{\sum_{k=1}^Ne^{(x^n)^\top x^k}}.
\tag{2}\]</span></span> ここで，<span class="math inline">\(A=(a^n_m)_{n,m\in[N]}\in M_N(\mathbb{R})\)</span> は確率行列をなし，その成分を <strong>注意荷重</strong> (attention weight) という．</p>
<p>この変換において，同じ <span class="math inline">\(x^m\)</span> の値を，３回別々の意味で使われていることに注意する．</p>
<ul>
<li><a href="#eq-1">式&nbsp;1</a> における <span class="math inline">\(x^m\)</span> は，新たな表現 <span class="math inline">\(y^n\)</span> を作るためのプロトタイプにような働きをしている．これを <strong>値</strong> (value) という．</li>
<li><a href="#eq-2">式&nbsp;2</a> において，内積が用いられており，<span class="math inline">\(x^n\)</span> と <span class="math inline">\(x^m\)</span> の類似度が測られている．
<ul>
<li><span class="math inline">\(x^m\)</span> を，<span class="math inline">\(x^m\)</span> が提供出来る情報を要約した量としての働きをし，<strong>鍵</strong> (key) という．</li>
<li><span class="math inline">\(x^n\)</span> は，<span class="math inline">\(x^n\)</span> と関連すべき情報を要求する役割を果たし，<strong>クエリ</strong> (query) という．</li>
</ul></li>
<li>最終的に，鍵とクエリの類似度・マッチ度を，<a href="https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0">ソフトマックス関数</a> を通じて確率分布として表現し，バリューの空間 <span class="math inline">\(\{x^m\}_{m=1}^N\)</span> 上の確率質量関数 <span class="math inline">\(\{a^n_m\}_{m=1}^N\)</span> を得ている．これに関して積分することで，鍵 <span class="math inline">\(y^n\)</span> を得る．</li>
</ul>
</section>
<section id="実際の自己注意機構" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="実際の自己注意機構"><span class="header-section-number">1.2.3</span> 実際の自己注意機構</h4>
<p>３つの別々の役割を果たしている以上，それぞれ固有の表現を持っていても良いはずである．そこで，値，鍵，クエリに，それぞれにニューラルネットワーク <span class="math inline">\(W_{(\Lambda)}\in M_{DD_{(\Lambda)}}(\mathbb{R})\;(\Lambda\in\{V,K,Q\})\)</span> を与えて固有の表現 <span class="math display">\[
x_{(\Lambda)}^n:=XW_{(\Lambda)}
\]</span> を持たせ，この <span class="math inline">\(W_{(\Lambda)}\)</span> を誤差逆伝播法により同時に学習することとする．</p>
<p>こうして得るのが，内積による自己注意機構 (dot-product self-attention mechanism) である．このとき，<span class="math inline">\(D_{(K)}=D_{(Q)}\)</span> は必要だが，<span class="math inline">\(y^n\in\mathbb{R}^{D_{(V)}}\)</span> は，元の次元 <span class="math inline">\(D\)</span> と異なっても良いことに注意．</p>
<p>最後に，ソフトマックス関数の適用において，勾配消失を回避するために，次元 <span class="math inline">\(D_{(K)}\)</span> に応じたスケーリングを介して <span class="math display">\[
a^n_m=\frac{e^{\frac{\left(x^n_{(Q)}\right)^\top x^m_{(K)}}{\sqrt{D_K}}}}{\sum_{k=1}^Ne^{\frac{\left(x^n_{(Q)}\right)^\top x^k_{(K)}}{\sqrt{D_K}}}}
\]</span> とする．これを最終的な <strong>自己注意機構</strong> (scaled dot-product self-attention mechanism) という．</p>
</section>
</section>
<section id="トランスフォーマーの全体" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</h3>
<section id="多頭注意" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="多頭注意"><span class="header-section-number">1.3.1</span> 多頭注意</h4>
<p>以上の自己注意機構を１単位として，これを複数独立に訓練し，最終的にはこれらの線型結合を採用する仕組みを <strong>多頭注意</strong> (multi-head attention) という．</p>
<p>これにより，種々の文脈をより頑健に読み取ることが出来るようである．</p>
</section>
<section id="残差結合と正規化" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="残差結合と正規化"><span class="header-section-number">1.3.2</span> 残差結合と正規化</h4>
<p>更に勾配消失を回避するために，<a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">残差結合</a> を導入し，訓練の高速化のために正規化 <span class="citation" data-cites="Ba+2016">(<a href="#ref-Ba+2016" role="doc-biblioref">Ba et al., 2016</a>)</span> が導入される．</p>
<p>そして，モデルを大規模化していくには，この「多頭注意＋残差結合と正規化」のブロックを積み重ねる．</p>
<p>注意機構は線型性が高いため，多頭注意の層の間に，通常の多層パーセプトロンもスタックして，ネットワークの表現能力を保つ工夫もされる．</p>
</section>
<section id="正規化" class="level4" data-number="1.3.3">
<h4 data-number="1.3.3" class="anchored" data-anchor-id="正規化"><span class="header-section-number">1.3.3</span> 正規化</h4>
<p>レイヤー正則化 (layer normalization) <span class="citation" data-cites="Ba+2016">(<a href="#ref-Ba+2016" role="doc-biblioref">Ba et al., 2016</a>)</span> は，バッチ正規化 (batch normalization) <span class="citation" data-cites="Ioffe-Szegedy2015">(<a href="#ref-Ioffe-Szegedy2015" role="doc-biblioref">Ioffe &amp; Szegedy, 2015</a>)</span> が RNN にも適するようにした修正として提案された．</p>
<p>バッチ正規化は，ニューラルネットワークの内部層の学習が，手前の層のパラメータが時事刻々と変化するために安定した学習が出来ないという <strong>内部共変量シフト</strong> (internal covariate shift) にあると突き止め，これをモデルアーキテクチャに正規化層を取り入れることで解決したものである．</p>
<p>正規化層は，ニューラルネットワークへの入力を，平均が零で分散が <span class="math inline">\(1\)</span> になるように変換する．元々，ニューラルネットワークの入力を正規化してから学習させることで学習が効率化されることは知られていた <span class="citation" data-cites="LeCun+2012">(<a href="#ref-LeCun+2012" role="doc-biblioref">LeCun et al., 2012</a>)</span> が，バッチ正規化は，これをバッチごとに，かつ，モデルの内部にも取り込んだものである．</p>
<p>バッチ正規化は精度の上昇と訓練の加速をもたらす．これはバッチ正規化により大きな学習率で訓練しても活性化が発散せず，これにより訓練時間の短縮と，局所解に囚われにくく汎化性能の向上がもたらされているようである <span class="citation" data-cites="Bjorck+2018">(<a href="#ref-Bjorck+2018" role="doc-biblioref">Bjorck et al., 2018</a>)</span>．</p>
</section>
</section>
</section>
<section id="言語トランスフォーマー" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</h2>
<p>言語モデルをニューラルネットワークによって作ることは早くから試みられていた <span class="citation" data-cites="Bengio+2000">(<a href="#ref-Bengio+2000" role="doc-biblioref">Bengio et al., 2000</a>)</span>．</p>
<p>トランスフォーマーの登場まで，これには RNN が主に用いられていた．しかし，RNN は長い系列に対しては勾配消失が起こりやすく，また，並列化が難しいという問題があった．</p>
<section id="言語の取り扱い" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</h3>
<section id="単語の分散表現" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="単語の分散表現"><span class="header-section-number">2.1.1</span> 単語の分散表現</h4>
<p>言語をそのまま扱うのではなく，トークン <span class="math inline">\(x^n\in\mathbb{R}^D\)</span> の形に符号化する必要がある．</p>
<p>言語には他にも改行や数式，コンピューターコードがあるが，まずは単語の表現を考える．</p>
<p>単語を Euclid 空間内に埋め込んだものを <strong>分散表現</strong> (distributed representation) という．これを２層のニューラルネットワークで行う技術が <code>word2vec</code> である <span class="citation" data-cites="Mikolov2013">(<a href="#ref-Mikolov2013" role="doc-biblioref">Mikolov et al., 2013</a>)</span>．</p>
<p>その訓練法には２つあり，窓の幅を <span class="math inline">\(M=5\)</span> などとすると，</p>
<ul>
<li>CBOW (Continuous Bag of Words)：前後 <span class="math inline">\(M\)</span> 語のみを見せて，中央の語を予測する．</li>
<li>Continuous Skip-gram：中央の語を見せて，前後 <span class="math inline">\(M\)</span> 語を予測する．</li>
</ul>
<p>という，いずれも教師なしの方法によって学習される．</p>
</section>
<section id="トークン化" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="トークン化"><span class="header-section-number">2.1.2</span> トークン化</h4>
<p>バイトペア符号化 (BPE: Byte Pair Encoding) <span class="citation" data-cites="Sennrich+2016">(<a href="#ref-Sennrich+2016" role="doc-biblioref">Sennrich et al., 2016</a>)</span> は，データ圧縮の手法であるが，単語に限らず種々のデータを含んだ文字列を符号化するのにも用いられる．</p>
</section>
<section id="位置情報符号化" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="位置情報符号化"><span class="header-section-number">2.1.3</span> 位置情報符号化</h4>
<p>トランスフォーマーはそのままではトークンの順番を考慮しないため，トークンの順番の情報も符号化時に含める必要がある．これを <strong>位置情報符号化</strong> (positional encoding) という <span class="citation" data-cites="Dufter+2021">(<a href="#ref-Dufter+2021" role="doc-biblioref">Dufter et al., 2021</a>)</span>．</p>
<p>このようにして，位置情報はトランスフォーマーのモデル構造を修正して組み込むのではなく，符号化の段階で組み込み，トランスフォーマーはそのまま使うのである．</p>
<p>これは，位置情報をトークンと同じ空間に埋め込んだ表現 <span class="math inline">\(r^n\)</span> を学習し， <span class="math display">\[
\widetilde{x}^n:=x^n+r^n
\]</span> を新たな符号とする．</p>
</section>
</section>
<section id="従来の言語モデル" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</h3>
<p>文章をトークン列 <span class="math inline">\(\{x^n\}_{n=1}^N\subset\mathbb{R}^D\)</span> に置き換えたあとに，この上の結合分布 <span class="math inline">\(p(x^1,\cdots,x^N)\)</span> をモデリングすることが，<strong>言語モデル</strong> の目標である．</p>
<section id="n-gram" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="n-gram"><span class="header-section-number">2.2.1</span> <span class="math inline">\(n\)</span>-gram</h4>
<p><span class="math inline">\(L\ge1\)</span> とし，<span class="math inline">\(x_n=0\;(n\le0)\)</span> として， <span class="math display">\[
p(x_1,\cdots,x_N)=\prod_{n=1}^Np_{\theta_n}(x_n|x_{n-L},\cdots,x_{n-1})
\]</span> という形で <span class="math inline">\(p\)</span> をモデリングする，自己回帰性を加味したモデルは古くから使われる．</p>
<p>これは <span class="math inline">\(L\)</span>-gram モデルと呼ばれるが，文章の長さ <span class="math inline">\(N\)</span> が大きくなると，必要なパラメータ <span class="math inline">\(\theta_n\)</span> の数が増加する．</p>
<p>これに対処する方法としては，<a href="../../../posts/2023/Surveys/SSM.html">隠れ Markov モデル</a> を用いることが考えられる．</p>
</section>
<section id="rnn" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="rnn"><span class="header-section-number">2.2.2</span> RNN</h4>
<p>言語モデルをニューラルネットワークによって表現することが，<span class="citation" data-cites="Mikolov+2010">(<a href="#ref-Mikolov+2010" role="doc-biblioref">Mikolov et al., 2010</a>)</span> によって試みられた．</p>
<p>これは通常のニューラルネットワーク (FFN: Feed-Forward Network と呼ばれる) に出力の一部を次の入力に使うという回帰的な流れを追加することで，隠れ Markov モデルのように次に持ち越される内部状態を持つことを可能にしたモデルである．</p>
<p>しかしこれは学習が困難である．誤差の逆伝播を時間に対しても逆方向に繰り返す必要がある (Backpropagation through time) ので，長い系列に対しては逆伝播しなければいけない距離が長く，勾配消失・爆発が起こりやすい．これは長期的な依存関係を学習しにくいということももたらす．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> また，並列化も難しい．</p>
<p>これに対処するために，モデルの構造を変えて過去の情報を流用しやすくする方法も種々提案された．LSTM (Long short-term memory) <span class="citation" data-cites="Hochreiter-Schmidhuber1997">(<a href="#ref-Hochreiter-Schmidhuber1997" role="doc-biblioref">Hochreiter &amp; Schmidhuber, 1997</a>)</span> や GRU (Gated Recurrent Unit) <span class="citation" data-cites="Cho+2014">(<a href="#ref-Cho+2014" role="doc-biblioref">Cho et al., 2014</a>)</span> などがその例である．</p>
</section>
</section>
<section id="トランスフォーマーによる言語モデル" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="トランスフォーマーによる言語モデル"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデル</h3>
<ol type="1">
<li>BERT (Bidirectional Encoder Representations from Transformers) <span class="citation" data-cites="Devlin+2019">(<a href="#ref-Devlin+2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> は，双方向エンコーダーである．</li>
<li>GPT (Generative Pre-trained Transformer) <span class="citation" data-cites="Radford+2018">(<a href="#ref-Radford+2018" role="doc-biblioref">Radford et al., 2018</a>)</span> は，単方向デコーダーである．</li>
<li>BART (Bidirectional and Auto-Regressive Transformer) <span class="citation" data-cites="Lewis+2020-BART">(<a href="#ref-Lewis+2020-BART" role="doc-biblioref">M. Lewis et al., 2020</a>)</span> は，双方向エンコーダーと単方向デコーダーの両方を持つ．</li>
</ol>
<section id="デコーダーのみの言語モデル" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="デコーダーのみの言語モデル"><span class="header-section-number">2.3.1</span> デコーダーのみの言語モデル</h4>
<p>GPT などの生成モデルは，デコーダー部分のトランスフォーマーの機能を主に用いている．</p>
<p>これはまず，</p>
<ol type="1">
<li>トークン列 <span class="math inline">\((x^n)_{n=1}^{N-1}\)</span> を入力し，条件付き分布 <span class="math inline">\(p(x^N|x_1,\cdots,x^{N-1})\)</span> を得る．</li>
<li>分布 <span class="math inline">\(p(x^N|x_1,\cdots,x^{N-1})\)</span> からサンプリングをする．</li>
</ol>
<p>の２段階で行われる．こうして <span class="math inline">\((x^n)_{n=1}^N\)</span> を得たら，次は <span class="math inline">\(x^{N+1}\)</span> を生成し，文章が終わるまでこれを続けることで，最終的な生成を完遂する．</p>
<section id="条件付き分布の表現" class="level5" data-number="2.3.1.1">
<h5 data-number="2.3.1.1" class="anchored" data-anchor-id="条件付き分布の表現"><span class="header-section-number">2.3.1.1</span> 条件付き分布の表現</h5>
<p>大規模なデータセットの上で，文章を途中まで読み，次のトークンを推測する，という自己教師あり学習を行うことで，トークン上の条件付き分布を学習する．</p>
<p>この際に，先のトークンの情報は使わないように，注意機構を工夫 (masked / causal attention) して訓練する．</p>
</section>
<section id="条件付き分布からのサンプリング" class="level5" data-number="2.3.1.2">
<h5 data-number="2.3.1.2" class="anchored" data-anchor-id="条件付き分布からのサンプリング"><span class="header-section-number">2.3.1.2</span> 条件付き分布からのサンプリング</h5>
<p>仮に最も確率の高いトークンを毎回選択する場合，出力は決定論的であり，同じ表現を繰り返すことが多くみられる．</p>
<p>実は，より人間らしい表現は，確率の低いトークンもかなら頻繁に採用される <span class="citation" data-cites="Holtzman+2020">(<a href="#ref-Holtzman+2020" role="doc-biblioref">Holtzman et al., 2020</a>)</span>．</p>
<p>かと言って，純粋なサンプリングをしたのでは，文章全体から見て意味をなさない場合も多い．</p>
<p>これを解決したのが top-<span class="math inline">\(p\)</span> sampling / nucleus sampling <span class="citation" data-cites="Holtzman+2020">(<a href="#ref-Holtzman+2020" role="doc-biblioref">Holtzman et al., 2020</a>)</span> である．</p>
<p><a href="https://github.com/openai/gpt-2-output-dataset/issues/5">GPT-2 にも実装されている</a> ようである．</p>
</section>
</section>
<section id="エンコーダーのみの言語モデル" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="エンコーダーのみの言語モデル"><span class="header-section-number">2.3.2</span> エンコーダーのみの言語モデル</h4>
<p>BERT (bidirectional encoder representations from transformers) <span class="citation" data-cites="Devlin+2019">(<a href="#ref-Devlin+2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> などの言語理解モデルは，エンコーダ部分のトランスフォーマーの機能を主に用いている．その結果，生成は出来ない．</p>
<p>訓練は，データセットから単語を確率的に脱落させ，これを補完するように訓練する．結果として，文章の前後両方 (bidirectional) の文脈を考慮するようになるのである．</p>
<p>実際に使う際は，例えば感情の判別などでは，文章の冒頭に <code>[class]</code> などの特殊なトークンを置き，これをエンコーダーに通してトークンが何に置き換わるかを見ることで，判別を実行することができる．</p>
</section>
<section id="エンコーダーデコーダーの言語モデル" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="エンコーダーデコーダーの言語モデル"><span class="header-section-number">2.3.3</span> エンコーダー・デコーダーの言語モデル</h4>
<p>トランスフォーマーは原論文 <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> では，エンコーダーとデコーダーがセットになったモデルとして提案された．</p>
<p>これは機械翻訳を念頭に置いていたため，RNN の構造を引き継いだ形で提案されたためである．この場合，次のようにしてモデルは使われる</p>
<ol type="1">
<li>入力 <span class="math inline">\(X\)</span> をエンコーダーに通し，内部表現 <span class="math inline">\(Z\)</span> を得る．</li>
<li>この内部表現 <span class="math inline">\(Z\)</span> を元に，デコードした結果 <span class="math inline">\(Y\)</span> を出力する．</li>
<li>唯一，<span class="math inline">\(Z\)</span> をデコーダーに渡す部分での注意機構層では，鍵と値としては <span class="math inline">\(Z\)</span> を使うが，クエリとしては <span class="math inline">\(Y\)</span> を使う．</li>
</ol>
<p>３の機構を <strong>エンコーダー・デコーダーの注意機構</strong> (encoder-decoder / corss attention mechanism) といい，これによって <span class="math inline">\(Z\)</span> と <span class="math inline">\(Y\)</span> のトークンの間の類似度をモデルに取り入れる．</p>
</section>
</section>
</section>
<section id="大規模言語モデル" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="大規模言語モデル"><span class="header-section-number">3</span> 大規模言語モデル</h2>
<section id="名前の由来と背景-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="名前の由来と背景-1"><span class="header-section-number">3.1</span> 名前の由来と背景</h3>
<p>自然言語処理にトランスフォーマーを応用した例は大きな成功を見ている．GPT <span class="citation" data-cites="Radford+2018">(<a href="#ref-Radford+2018" role="doc-biblioref">Radford et al., 2018</a>)</span>, GPT-2 <span class="citation" data-cites="Radford+2019">(<a href="#ref-Radford+2019" role="doc-biblioref">Radford et al., 2019</a>)</span>, GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span>, GPT-4 <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023</a>)</span> のシリーズはその代表であり，特に GPT-4 は AGI の実現に向けた重要な一歩とも評されている <span class="citation" data-cites="Bubeck+2023">(<a href="#ref-Bubeck+2023" role="doc-biblioref">Bubeck et al., 2023</a>)</span>．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>その成功は，言語モデルとして優れているという点よりもむしろ，並列化が可能であり GPU などの計算資源を効率的に使えるという点にあり，モデルの改良よりも計算資源の増強が最終的に大きな進歩をもたらすという側面が大きい，という認識が優勢になっている <span class="citation" data-cites="Sutton2019">(<a href="#ref-Sutton2019" role="doc-biblioref">R. Sutton, 2019</a>)</span>．これはスケーリング則として理論的にも理解が試みられている <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span>．</p>
<p>この観点から，トランスフォーマーを用いた事前学習済みの言語モデルが，種々のタスクをほとんど例示なし (few-shot / zero-shot) で解ける能力を創発する程度に大きい場合，その規模が意味を持つことを強調して，<strong>大規模言語モデル</strong> (LLM: Large Language Model) とも呼ぶ <span class="citation" data-cites="Zhao+2023">(<a href="#ref-Zhao+2023" role="doc-biblioref">Zhao et al., 2023</a>)</span>．</p>
</section>
<section id="sec-foundation-model" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</h3>
<p>GPT の P とは Pre-trained である．自己教師あり学習によって <a href="../../../posts/2024/Kernels/Deep.html#sec-pretraining-using-AE">事前学習</a> をしたあと，さらに教師あり学習によって微調整を行う．</p>
<p>この微調整は <strong>事後調整</strong> (fine-tune) と呼ばれ，転移学習の一種ともみなせる．この意味でも，さらに使い方の意味でも，種々の応用や下流タスク (downstream task) の基礎となるモデルであることと，そのものでは未完成であることとを強調して，<strong>基盤モデル</strong> (foundation model) とも呼ばれる <span class="citation" data-cites="Bommasani+2021">(<a href="#ref-Bommasani+2021" role="doc-biblioref">Bommasani et al., 2021</a>)</span>．</p>
<p>事後調整では，モデルの全体では規模が大きすぎるため，出力層の後に新しいニューラルネットを付加したり，最後の数層のみを追加で教師あり学習をしたりする方法が一般的である．または，LoRA (Low-Rank Adaptation) <span class="citation" data-cites="Hu+2021">(<a href="#ref-Hu+2021" role="doc-biblioref">E. J. Hu et al., 2021</a>)</span> では，トランスフォーマーの各層に新たな層を挿入し，これを学習する．</p>
<p>これは，事後調整に有効な内的次元は実際には小さく <span class="citation" data-cites="Aghajanyan+2021">(<a href="#ref-Aghajanyan+2021" role="doc-biblioref">Aghajanyan et al., 2021</a>)</span>，これに有効にアクセスし，効率的な事後調整を行うことが出来るという．</p>
<p>事後調整には，他にも，ChatGPT のようなサービスを展開するために必要なユーザー体験の改善を目的としたものも含まれる．GPT-4 では <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(<a href="#ref-Christiano+2017" role="doc-biblioref">Christiano et al., 2017</a>)</span> が用いられている <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023, p. 2</a>)</span>．</p>
</section>
<section id="プロンプトエンジニアリング" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</h3>
<p>基盤モデルは使い方によって大きく性能が変わる．特に，prompt engineering <span class="citation" data-cites="Liu+2023">(<a href="#ref-Liu+2023" role="doc-biblioref">Liu et al., 2023</a>)</span> は，プロンプトの送り方によって性能がどう変わるかを調べる新たな分野である．</p>
<p>その結果，プロンプト内で新たなタスクを定義するだけで，これが解けてしまうこともわかっており，これを few-shot learning という．これも大規模言語モデルの大きな特徴である．</p>
</section>
<section id="rag" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="rag"><span class="header-section-number">3.4</span> RAG</h3>
<p>LLM は世界に関する正確な知識も持っており，知識ベースとしての利用も期待されている <span class="citation" data-cites="Petroni+2019">(<a href="#ref-Petroni+2019" role="doc-biblioref">Petroni et al., 2019</a>)</span>．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> しかし，知識を正確に・信頼出来る形で引き出すことが難しい．特に，出典を示すことや，最新の知識のアップデートなどが難題として待っている．</p>
<p>そこで，LLM に（自由に外部情報を探索できるという意味で）ノンパラメトリックな知識ベースを接続することで解決するのが RAG (Retrieval-Augmented Generation) モデル <span class="citation" data-cites="Lewis+2020">(<a href="#ref-Lewis+2020" role="doc-biblioref">P. Lewis et al., 2020</a>)</span> である．</p>
<p>DPR (Dense Passage Retriever) <span class="citation" data-cites="Karpukhin+2020">(<a href="#ref-Karpukhin+2020" role="doc-biblioref">Karpukhin et al., 2020</a>)</span> は文書を密に符号化する手法を開発し，これを用いて文書検索をすることで Q&amp;A タスクを効率的に解く手法を提案した．このような文書の符号化器は <strong>検索器</strong> (retriever) と呼ばれる．</p>
<p><span class="citation" data-cites="Lewis+2020">(<a href="#ref-Lewis+2020" role="doc-biblioref">P. Lewis et al., 2020</a>)</span> はこの検索器を BART <span class="citation" data-cites="Lewis+2020-BART">(<a href="#ref-Lewis+2020-BART" role="doc-biblioref">M. Lewis et al., 2020</a>)</span> に接続した．</p>
<p>Meta での研究 <span class="citation" data-cites="Yasunaga+2023">(<a href="#ref-Yasunaga+2023" role="doc-biblioref">Yasunaga et al., 2023</a>)</span> はこの検索器を Text-to-Image トランスフォーマー である CM3 <span class="citation" data-cites="Aghajanyan+2022">(<a href="#ref-Aghajanyan+2022" role="doc-biblioref">Aghajanyan et al., 2022</a>)</span> と結合することで，初めて言語と画像の両方を扱える RAG モデル RA-CM3 (retrieval-augmented CM3) を構成した．</p>
</section>
</section>
<section id="sec-multimodal-transformer" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</h2>
<p>トランスフォーマーは自然言語処理の文脈で開発されたが，画像や動画，音声 <span class="citation" data-cites="Radford+2023">(<a href="#ref-Radford+2023" role="doc-biblioref">Radford et al., 2023</a>)</span> にも適用されている．</p>
<p>動画はまだしも画像には，直感的には時系列構造がないように思えるが，トランスフォーマーはもはや汎用のニューラルネットワークアーキテクチャとして使用できることが解りつつある．</p>
<p>それぞれの応用分野で <strong>モデルの構造は殆ど差異がなく</strong>，トークン化の手法などに差異があるのみのように見受けられる．<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<section id="画像トランスフォーマー-vit" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="画像トランスフォーマー-vit"><span class="header-section-number">4.1</span> 画像トランスフォーマー (ViT)</h3>
<p>画像の分類問題を解くためのエンコーダ・トランスフォーマーは ViT (Vision Transformer) <span class="citation" data-cites="Dosovitskiy+2021">(<a href="#ref-Dosovitskiy+2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span> と呼ばれており，ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) では未だ <a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">ResNet</a> 系のモデルが優勢であった 2021 年に，これを超える性能を示した．</p>
<p>実はモデルは殆どトランスフォーマーそのままであり，肝要であったのは画像をトークン化である．ピクセルをそのまま用いるのではなく，ある程度大きなピクセルの集合である <strong>パッチ</strong> (patch) を用いることで，計算量を下げる．<span class="citation" data-cites="Dosovitskiy+2021">(<a href="#ref-Dosovitskiy+2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span> では <span class="math inline">\(16\times16\)</span> サイズなどが採用された．</p>
<p>一方で，画像を恣意的に系列化しているため，幾何学的な構造は１から学ぶ必要があり，最初からモデルに組み込まれている <a href="../../../posts/2024/Kernels/Deep.html#sec-CNN">CNN</a> よりは一般に多くの訓練データを必要とする．だが，これにより帰納バイアスが弱いということでもある．</p>
<p>トークン化に小規模な CNN を用いてデータ圧縮を行うこともある．</p>
</section>
<section id="画像生成トランスフォーマー" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</h3>
<p>一方でデコーダートランスフォーマーを用いて，画像の生成モデリングを行った最初の例は <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> である．</p>
<p>なお，自己回帰的な生成モデルを通じて画像の生成を試みることは，CNN <span class="citation" data-cites="vandenOord+2016b">(<a href="#ref-vandenOord+2016b" role="doc-biblioref">van&nbsp;den&nbsp;Oord, Kalchbrenner, Vinyals, et al., 2016</a>)</span> や RNN <span class="citation" data-cites="vandenOord+2016">(<a href="#ref-vandenOord+2016" role="doc-biblioref">van&nbsp;den&nbsp;Oord, Kalchbrenner, &amp; Kavukcuoglu, 2016</a>)</span> でも行われていた．</p>
<p>この際に判明したことには，画像の分類タスクでは連続表現が役に立っても，生成タスクでは高い解像度を持った画像の生成が難しく，離散表現が有効であることが知られている．しかしこれではデータ量が増えてしまうため，画像の <a href="../../../posts/2024/Computation/VI.html#sec-history">ベクトル量子化</a> が行われることが多い．</p>
<p>そこで，<a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> でも <span class="math inline">\(K\)</span>-平均法によるクラスタリングが行われており，さらに <a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いたデータ圧縮も行われている．</p>
<p><a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> では最終的に各ピクセルを one-hot 表現にまで落とし込み，これを GPT-2 モデル <span class="citation" data-cites="Radford+2019">(<a href="#ref-Radford+2019" role="doc-biblioref">Radford et al., 2019</a>)</span> につなげている．</p>
<p><a href="https://muse-model.github.io/">MUSE</a> <span class="citation" data-cites="Chang+2023">(<a href="#ref-Chang+2023" role="doc-biblioref">Chang et al., 2023</a>)</span> も，トランスフォーマーを用いた画像生成モデルの例である．</p>
</section>
<section id="text-to-image-トランスフォーマー" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</h3>
<p>GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span> と <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> とは殆ど同じモデルを用いている．これらを組み合わせて作られたのが <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(<a href="#ref-Ramesh+2021" role="doc-biblioref">Ramesh et al., 2021</a>)</span> である．</p>
<p>トークン化して仕舞えば，言語も画像も等価に扱えるというのである．Google の <a href="https://sites.research.google/parti/">Parti</a> <span class="citation" data-cites="Yu+2022">(<a href="#ref-Yu+2022" role="doc-biblioref">J. Yu et al., 2022</a>)</span> も同様のアプローチである．</p>
<p>Meta の CM3 <span class="citation" data-cites="Aghajanyan+2022">(<a href="#ref-Aghajanyan+2022" role="doc-biblioref">Aghajanyan et al., 2022</a>)</span> と <a href="https://ai.meta.com/blog/generative-ai-text-images-cm3leon/">CM3leon</a> <span class="citation" data-cites="Yu+2023">(<a href="#ref-Yu+2023" role="doc-biblioref">L. Yu et al., 2023</a>)</span> は画像と言語を両方含んだ HTML ドキュメントから学習している．</p>
<p>Google DeepMind の <a href="https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/">Flamingo</a> <span class="citation" data-cites="Alayrac+2022">(<a href="#ref-Alayrac+2022" role="doc-biblioref">Alayrac et al., 2022</a>)</span> は画像から言語を生成する．</p>
</section>
<section id="image-to-text-トランスフォーマー" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="image-to-text-トランスフォーマー"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/clip">CLIP</a> <span class="citation" data-cites="Radford+2021">(<a href="#ref-Radford+2021" role="doc-biblioref">Radford et al., 2021</a>)</span> は画像から概念を学習する視覚モデルである．</p>
</section>
<section id="動画トランスフォーマー" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="動画トランスフォーマー"><span class="header-section-number">4.5</span> 動画トランスフォーマー</h3>
<p>動画を画像の連続と見てトランスフォーマーを応用するアプローチは Latte (Latent Diffusion Transformer) <span class="citation" data-cites="Rakhimov+2020">(<a href="#ref-Rakhimov+2020" role="doc-biblioref">Rakhimov et al., 2020</a>)</span> に始まる．</p>
<p><a href="https://wilson1yan.github.io/videogpt/index.html">VideoGPT</a> <span class="citation" data-cites="Yan+2021">(<a href="#ref-Yan+2021" role="doc-biblioref">Yan et al., 2021</a>)</span> では動画を 3D の CNN でデータ圧縮，VQ-VAE で量子化して離散的な潜在表現を得た後，GPT と殆ど似たトランスフォーマーに通して学習する．</p>
<p><a href="https://wayve.ai/company/">Wayve</a> の <a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(<a href="#ref-Hu+2023" role="doc-biblioref">A. Hu et al., 2023</a>)</span> も同様の手法で動画を生成しているが，その動画を用いて自動運転の強化学習に応用する点が画期的である．</p>
</section>
<section id="世界モデルとしてのトランスフォーマー" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</h3>
<p>トランスフォーマーを世界モデルとして用いて，シミュレーションを行い動画を生成し，これをモデルベースの強化学習 <span class="citation" data-cites="Sutton-Barto2018">(<a href="#ref-Sutton-Barto2018" role="doc-biblioref">R. S. Sutton &amp; Barto, 2018</a>)</span> の材料とすることが広く提案されている．これは learning in imagination <span class="citation" data-cites="Racaniere+2017">(<a href="#ref-Racaniere+2017" role="doc-biblioref">Racanière et al., 2017</a>)</span> と呼ばれる．<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>IRIS (Imagination with auto-Regression over an Inner Speech) <span class="citation" data-cites="Micheli+2023">(<a href="#ref-Micheli+2023" role="doc-biblioref">Micheli et al., 2023</a>)</span> はこれに初めてトランスフォーマーを用いた世界モデルから動画生成をした．</p>
</section>
<section id="音声トランスフォーマー" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="音声トランスフォーマー"><span class="header-section-number">4.7</span> 音声トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/jukebox">Jukebox</a> <span class="citation" data-cites="Dhariwal+2020">(<a href="#ref-Dhariwal+2020" role="doc-biblioref">Dhariwal et al., 2020</a>)</span> は，<a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いて音声データを圧縮・量子化し，トランスフォーマーに通したものである．</p>
<p>このトランスフォーマーは <a href="https://openai.com/research/sparse-transformer">Sparse Transformer</a> <span class="citation" data-cites="Child+2019">(<a href="#ref-Child+2019" role="doc-biblioref">Child et al., 2019</a>)</span> という，注意機構の計算効率を改良したモデルを用いている．</p>
</section>
<section id="text-to-speech-トランスフォーマー" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</h3>
<p>Microsoft Research の <a href="https://www.microsoft.com/en-us/research/project/vall-e-x/">VALL-E</a> <span class="citation" data-cites="Wang+2023">(<a href="#ref-Wang+2023" role="doc-biblioref">Wang et al., 2023</a>)</span> は，音声データをベクトル量子化によって言語データと全く同等に扱うことで，トランスフォーマーを用いて音声生成を行っている．</p>
</section>
</section>
<section id="近年の動向" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="近年の動向"><span class="header-section-number">5</span> 近年の動向</h2>
<section id="llama-の一般公開とその影響" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="llama-の一般公開とその影響"><span class="header-section-number">5.1</span> LLaMA の一般公開とその影響</h3>
<p>Meta AI が <a href="https://about.fb.com/ja/news/2023/07/meta-and-microsoft-introduce-the-next-generation-of-llama/">7/18/2023</a> に LLM <a href="https://llama.meta.com/">LLaMA</a> <span class="citation" data-cites="Touvron+2023">(<a href="#ref-Touvron+2023" role="doc-biblioref">Touvron et al., 2023</a>)</span> を公開した．そして API を通じて利用する形ではなく，そのモデルのウェイトが公開されたため，Stanford 大学の <a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> など，モデルの改良と研究が促進されている．</p>
<p><a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> では Self-Instruct <span class="citation" data-cites="Wang+2023">(<a href="#ref-Wang+2023" role="doc-biblioref">Wang et al., 2023</a>)</span> による効率的な alignment 技術が採用されている．</p>
<p>産業界でも影響は大きい．<a href="https://elyza.ai/">ELYZA</a> は <a href="https://elyza.ai/news/2023/12/27/130%E5%84%84%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E5%95%86%E7%94%A8%E5%88%A9%E7%94%A8%E5%8F%AF%E8%83%BD%E3%81%AA%E6%97%A5%E6%9C%AC%E8%AA%9Ellmelyza-j">12/27/2023</a> に日本語に特化した LLM である ELYZA-japanese-Llama-2-13b を公開している．<a href="https://stockmark.co.jp/">Stockmark</a> も <a href="https://stockmark.co.jp/news/20231027">10/27/2023</a> に Stockmark-13b を公開している．</p>
<p>いずれも，開発費と開発時間が大幅に圧縮されたという．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>IBM は <a href="https://jp.newsroom.ibm.com/2023-09-12-Blog-Building-AI-for-business-IBM-Granite-foundation-models">9/12/2023</a> に LLM Granite を発表している．加えて，プラットフォーム <code>watsonx</code> も提供しており，その上で RAG など独自の事後調整を可能にしている．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-Aghajanyan+2021" class="csl-entry" role="listitem">
Aghajanyan, A., Gupta, S., &amp; Zettlemoyer, L. (2021). Intrinsic dimensionality explains the effectiveness of language model fine-tuning. <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</em>, <em>1</em>, 7319–7328. <a href="https://aclanthology.org/2021.acl-long.568/">https://aclanthology.org/2021.acl-long.568/</a>
</div>
<div id="ref-Aghajanyan+2022" class="csl-entry" role="listitem">
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., &amp; Zettlemoyer, L. (2022). <em>CM3: A causal masked multimodal model of the internet</em>. <a href="https://arxiv.org/abs/2201.07520">https://arxiv.org/abs/2201.07520</a>
</div>
<div id="ref-Alayrac+2022" class="csl-entry" role="listitem">
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., … Simonyan, K. (2022). <em>Flamingo: A visual language model for few-shot learning</em>. <a href="https://arxiv.org/abs/2204.14198">https://arxiv.org/abs/2204.14198</a>
</div>
<div id="ref-Ba+2016" class="csl-entry" role="listitem">
Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016). <em>Layer normalization</em>. <a href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>
</div>
<div id="ref-Bahdanau+2015" class="csl-entry" role="listitem">
Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). <em>Neural machine translation by jointly learning to align and translate</em>. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>
</div>
<div id="ref-Bengio+2000" class="csl-entry" role="listitem">
Bengio, Y., Ducharme, R., &amp; Vincent, P. (2000). A neural probabilistic language model. <em>Advances in Neural Information Processing Systems</em>, <em>13</em>. <a href="https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html">https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html</a>
</div>
<div id="ref-Bjorck+2018" class="csl-entry" role="listitem">
Bjorck, N., Gomes, C. P., Selman, B., &amp; Weinberger, K. Q. (2018). Understanding batch normalization. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>. <a href="https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html">https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html</a>
</div>
<div id="ref-Bommasani+2021" class="csl-entry" role="listitem">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., Arx, S. von, Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K. A., Davis, J., Demszky, D., … Liang, P. (2021). On the opportunities and risks of foundation models. <em>ArXiv</em>. <a href="https://crfm.stanford.edu/report.html">https://crfm.stanford.edu/report.html</a>
</div>
<div id="ref-Brown+2020" class="csl-entry" role="listitem">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., … Amodei, D. (2020). Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 1877–1901. <a href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>
</div>
<div id="ref-Bubeck+2023" class="csl-entry" role="listitem">
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., &amp; Zhang, Y. (2023). <em>Sparks of artificial general intelligence: Early experiments with GPT-4</em>. <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a>
</div>
<div id="ref-Chang+2023" class="csl-entry" role="listitem">
Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K. P., Freeman, W. T., Rubinstein, M., Li, Y., &amp; Krishnan, D. (2023). Muse: Text-to-image generation via masked generative transformers. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, &amp; J. Scarlett (Eds.), <em>Proceedings of the 40th international conference on machine learning</em> (Vol. 202, pp. 4055–4075). PMLR. <a href="https://proceedings.mlr.press/v202/chang23b.html">https://proceedings.mlr.press/v202/chang23b.html</a>
</div>
<div id="ref-Chen+2020" class="csl-entry" role="listitem">
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., &amp; Sutskever, I. (2020). Generative pretraining from pixels. <em>Proceedings of the 37th International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v119/chen20s.html">https://proceedings.mlr.press/v119/chen20s.html</a>
</div>
<div id="ref-Child+2019" class="csl-entry" role="listitem">
Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). <em>Generating long sequences with sparse transformers</em>. <a href="https://arxiv.org/abs/1904.10509">https://arxiv.org/abs/1904.10509</a>
</div>
<div id="ref-Cho+2014" class="csl-entry" role="listitem">
Cho, K., Merriënboer, B. van, Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using <span>RNN</span> encoder<span>–</span>decoder for statistical machine translation. In A. Moschitti, B. Pang, &amp; W. Daelemans (Eds.), <em>Proceedings of the 2014 conference on empirical methods in natural language processing (<span>EMNLP</span>)</em> (pp. 1724–1734). Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1179">https://doi.org/10.3115/v1/D14-1179</a>
</div>
<div id="ref-Chowdhery+2022" class="csl-entry" role="listitem">
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). <em>PaLM: Scaling language modeling with pathways</em>. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>
</div>
<div id="ref-Christiano+2017" class="csl-entry" role="listitem">
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html</a>
</div>
<div id="ref-Devlin+2019" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, <em>1</em>, 4171–4186. <a href="https://aclanthology.org/N19-1423/">https://aclanthology.org/N19-1423/</a>
</div>
<div id="ref-Dhariwal+2020" class="csl-entry" role="listitem">
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., &amp; Sutskever, I. (2020). <em>Jukebox: A generative model for music</em>. <a href="https://arxiv.org/abs/2005.00341">https://arxiv.org/abs/2005.00341</a>
</div>
<div id="ref-Dosovitskiy+2021" class="csl-entry" role="listitem">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp; Houlsby, N. (2021). An image is worth 16×16 words: Transformers for image recognition at scale. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a>
</div>
<div id="ref-Dufter+2021" class="csl-entry" role="listitem">
Dufter, P., Schmitt, M., &amp; Schütze, H. (2021). <em>Position information in transformers: An overview</em>. <a href="https://arxiv.org/abs/2102.11090">https://arxiv.org/abs/2102.11090</a>
</div>
<div id="ref-Fedus+2022" class="csl-entry" role="listitem">
Fedus, W., Zoph, B., &amp; Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. <em>The Journal of Machine Learning Research</em>, <em>23</em>(120), 1–39. <a href="https://jmlr.org/papers/v23/21-0998.html">https://jmlr.org/papers/v23/21-0998.html</a>
</div>
<div id="ref-Ha-Schmidthuber2018" class="csl-entry" role="listitem">
Ha, D., &amp; Schmidhuber, J. (2018). Recurrent world models facilitate policy evaluation. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>. <a href="https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html">https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html</a>
</div>
<div id="ref-Hafner+2021" class="csl-entry" role="listitem">
Hafner, D., Lillicrap, T. P., Norouzi, M., &amp; Ba, J. (2021). Mastering atari with discrete world models. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=0oabwyZbOu">https://openreview.net/forum?id=0oabwyZbOu</a>
</div>
<div id="ref-Hochreiter-Schmidhuber1997" class="csl-entry" role="listitem">
Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-time memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780. <a href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext</a>
</div>
<div id="ref-Holtzman+2020" class="csl-entry" role="listitem">
Holtzman, A., Buys, J., Du, L., Forbes, M., &amp; Choi, Y. (2020). The curious case of neural text degeneration. <em>International Conference on Learning Representation</em>. <a href="https://arxiv.org/abs/1904.09751">https://arxiv.org/abs/1904.09751</a>
</div>
<div id="ref-Hu+2023" class="csl-entry" role="listitem">
Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., &amp; Corrado, G. (2023). <em>GAIA-1: A generative world model for autonomous driving</em>. <a href="https://arxiv.org/abs/2309.17080">https://arxiv.org/abs/2309.17080</a>
</div>
<div id="ref-Hu+2021" class="csl-entry" role="listitem">
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). <em>LoRA: Low-rank adaptation of large language models</em>. <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>
</div>
<div id="ref-Ioffe-Szegedy2015" class="csl-entry" role="listitem">
Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. <em>Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37</em>, 448–456. <a href="https://dl.acm.org/doi/10.5555/3045118.3045167">https://dl.acm.org/doi/10.5555/3045118.3045167</a>
</div>
<div id="ref-Kaiser+2020" class="csl-entry" role="listitem">
Kaiser, Ł., Babaeizadeh, M., Miłos, P., Osiński, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., &amp; Michalewski, H. (2020). Model based reinforcement learning for atari. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=S1xCPJHtDB">https://openreview.net/forum?id=S1xCPJHtDB</a>
</div>
<div id="ref-Kaplan+2020" class="csl-entry" role="listitem">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020). <em>Scaling laws for neural language models</em>. <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a>
</div>
<div id="ref-Karpukhin+2020" class="csl-entry" role="listitem">
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., &amp; Yih, W. (2020). Dense passage retrieval for open-domain question answering. In B. Webber, T. Cohn, Y. He, &amp; Y. Liu (Eds.), <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 6769–6781). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.550">https://doi.org/10.18653/v1/2020.emnlp-main.550</a>
</div>
<div id="ref-LeCun+2012" class="csl-entry" role="listitem">
LeCun, Y. A., Bottou, L., Orr, G. B., &amp; Müller, K.-R. (2012). <em>Neural networks: Tricks of the trade</em> (G. Montavon, G. B. Orr, &amp; K.-R. Müller, Eds.; 2nd ed., pp. 9–48). Springer Berlin, Heidelberg. <a href="https://link.springer.com/book/10.1007/978-3-642-35289-8">https://link.springer.com/book/10.1007/978-3-642-35289-8</a>
</div>
<div id="ref-Lepikhin+2021" class="csl-entry" role="listitem">
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., &amp; Chen, Z. (2021). GShard: Scaling giant models with conditional computation and automatic sharding. <em>International Conference on Learning Representation</em>. <a href="https://openreview.net/forum?id=qrwe7XHTmYb">https://openreview.net/forum?id=qrwe7XHTmYb</a>
</div>
<div id="ref-Lewis+2020-BART" class="csl-entry" role="listitem">
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., &amp; Zettlemoyer, L. (2020). <span>BART</span>: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In D. Jurafsky, J. Chai, N. Schluter, &amp; J. Tetreault (Eds.), <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em> (pp. 7871–7880). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a>
</div>
<div id="ref-Lewis+2020" class="csl-entry" role="listitem">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 9459–9474. <a href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</a>
</div>
<div id="ref-Liu+2023" class="csl-entry" role="listitem">
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., &amp; Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. <em>ACM Computing Surveys</em>, <em>55</em>(9), 1–35. <a href="https://dl.acm.org/doi/full/10.1145/3560815">https://dl.acm.org/doi/full/10.1145/3560815</a>
</div>
<div id="ref-Marcus2020" class="csl-entry" role="listitem">
Marcus, G. (2020). <em>The next decade in AI: Four steps towards robust artificial intelligence</em>. <a href="https://arxiv.org/abs/2002.06177">https://arxiv.org/abs/2002.06177</a>
</div>
<div id="ref-Micheli+2023" class="csl-entry" role="listitem">
Micheli, V., Alonso, E., &amp; Fleuret, F. (2023). Transformers are sample-efficient world models. <em>International Conference on Learning Representation</em>. <a href="https://openreview.net/forum?id=vhFu1Acb0xb">https://openreview.net/forum?id=vhFu1Acb0xb</a>
</div>
<div id="ref-Mikolov2013" class="csl-entry" role="listitem">
Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Efficient estimation of word representations in vector space</em>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>
</div>
<div id="ref-Mikolov+2010" class="csl-entry" role="listitem">
Mikolov, T., Kopecky, J., Burget, L., C̆ernocky, J., &amp; Khudanpur, S. (2010). Recurrent neural network based language model. <em>Proceedings of Interspeech</em>. <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf</a>
</div>
<div id="ref-OpenAI2023" class="csl-entry" role="listitem">
OpenAI. (2023). <em>GPT-4 technical report</em>. <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a>
</div>
<div id="ref-Petroni+2019" class="csl-entry" role="listitem">
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., &amp; Miller, A. (2019). Language models as knowledge bases? In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em> (pp. 2463–2473). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1250">https://doi.org/10.18653/v1/D19-1250</a>
</div>
<div id="ref-Racaniere+2017" class="csl-entry" role="listitem">
Racanière, S., Weber, T., Reichert, D. P., Buesing, L., Guez, A., Rezende, D., Badia, A. P., Vinyals, O., Heess, N., Li, Y., Pascanu, R., Battaglia, P., Hassabis, D., Silver, D., &amp; Wierstra, D. (2017). Imagination-augmented agents for deep reinforcement learning. <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 5694–5705. <a href="https://dl.acm.org/doi/10.5555/3295222.3295320">https://dl.acm.org/doi/10.5555/3295222.3295320</a>
</div>
<div id="ref-Radford+2021" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8748–8763). PMLR. <a href="https://proceedings.mlr.press/v139/radford21a.html">https://proceedings.mlr.press/v139/radford21a.html</a>
</div>
<div id="ref-Radford+2023" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., &amp; Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. <em>Proceedings of the 40th International Conference on Machine Learning</em>. <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">https://dl.acm.org/doi/10.5555/3618408.3619590</a>
</div>
<div id="ref-Radford+2018" class="csl-entry" role="listitem">
Radford, A., narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <em>Improving language understanding with unsupervised learning</em>. OpenAI. <a href="https://openai.com/research/language-unsupervised">https://openai.com/research/language-unsupervised</a>
</div>
<div id="ref-Radford+2019" class="csl-entry" role="listitem">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). <em>Language models are unsupervised multitask learners</em>. <a href="https://github.com/openai/gpt-2?tab=readme-ov-file">https://github.com/openai/gpt-2?tab=readme-ov-file</a>
</div>
<div id="ref-Rae+2021" class="csl-entry" role="listitem">
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., Driessche, G. van den, Hendricks, L. A., Rauh, M., Huang, P.-S., … Irving, G. (2021). <em>Scaling language models: Methods, analysis &amp; insights from training gopher</em>. Google DeepMind. <a href="https://arxiv.org/abs/2112.11446">https://arxiv.org/abs/2112.11446</a>
</div>
<div id="ref-Raffel+2020" class="csl-entry" role="listitem">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. <em>J. Mach. Learn. Res.</em>, <em>21</em>(1).
</div>
<div id="ref-Rakhimov+2020" class="csl-entry" role="listitem">
Rakhimov, R., Volkhonskiy, D., Artemov, A., Zorin, D., &amp; Burnaev, E. (2020). <em>Latent video transformer</em>. <a href="https://arxiv.org/abs/2006.10704">https://arxiv.org/abs/2006.10704</a>
</div>
<div id="ref-Ramesh+2021" class="csl-entry" role="listitem">
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. (2021). Zero-shot text-to-image generation. In M. Meila &amp; T. Zhang (Eds.), <em>Proceedings of the 38th international conference on machine learning</em> (Vol. 139, pp. 8821–8831). PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>
</div>
<div id="ref-Roberts+2020" class="csl-entry" role="listitem">
Roberts, A., Raffel, C., &amp; Shazeer, N. (2020). How much knowledge can you pack into the parameters of a language model? In B. Webber, T. Cohn, Y. He, &amp; Y. Liu (Eds.), <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em> (pp. 5418–5426). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.437">https://doi.org/10.18653/v1/2020.emnlp-main.437</a>
</div>
<div id="ref-Sennrich+2016" class="csl-entry" role="listitem">
Sennrich, R., Haddow, B., &amp; Birch, A. (2016). Neural machine translation of rare words with subword units. <em>Proceedings of the 54th Annual Meetings of the Association for Computational Linguistics</em>, <em>1</em>, 1715–1725. <a href="https://aclanthology.org/P16-1162/">https://aclanthology.org/P16-1162/</a>
</div>
<div id="ref-Sutton2019" class="csl-entry" role="listitem">
Sutton, R. (2019). <em>The bitter lesson</em>. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>
</div>
<div id="ref-Sutton-Barto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (2nd ed.). MIT Press. <a href="https://mitpress.mit.edu/9780262352703/reinforcement-learning/">https://mitpress.mit.edu/9780262352703/reinforcement-learning/</a>
</div>
<div id="ref-Touvron+2023" class="csl-entry" role="listitem">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., &amp; Lample, G. (2023). <em>LLaMA: Open and efficient foundation language models</em>. <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a>
</div>
<div id="ref-vandenOord+2016" class="csl-entry" role="listitem">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., &amp; Kavukcuoglu, K. (2016). Pixel recurrent neural networks. <em>Proceedings of the 33rd International Conference on Machine Learning</em>. <a href="https://proceedings.mlr.press/v48/oord16.html">https://proceedings.mlr.press/v48/oord16.html</a>
</div>
<div id="ref-vandenOord+2016b" class="csl-entry" role="listitem">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., &amp; Kavukcuoglu, K. (2016). Conditional image generation with PixelCNN decoders. <em>Proceedings of the 30th International Conference on Neural Information Processing Systems</em>, 4797–4805. <a href="https://dl.acm.org/doi/10.5555/3157382.3157633">https://dl.acm.org/doi/10.5555/3157382.3157633</a>
</div>
<div id="ref-Vaswani+2017" class="csl-entry" role="listitem">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em>, <em>30</em>. <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>
</div>
<div id="ref-Wang+2023" class="csl-entry" role="listitem">
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., &amp; Wei, F. (2023). <em>Neural codec language models are zero-shot text to speech synthesizers</em>. <a href="https://arxiv.org/abs/2301.02111">https://arxiv.org/abs/2301.02111</a>
</div>
<div id="ref-Yan+2021" class="csl-entry" role="listitem">
Yan, W., Zhang, Y., Abbeel, P., &amp; Srinivas, A. (2021). <em>VideoGPT: Video generation using VQ-VAE and transformers</em>. <a href="https://arxiv.org/abs/2104.10157">https://arxiv.org/abs/2104.10157</a>
</div>
<div id="ref-Yasunaga+2023" class="csl-entry" role="listitem">
Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &amp; Yih, W.-T. (2023). Retrieval-augmented multimodal language modeling. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, &amp; J. Scarlett (Eds.), <em>Proceedings of the 40th international conference on machine learning</em> (Vol. 202, pp. 39755–39769). PMLR. <a href="https://proceedings.mlr.press/v202/yasunaga23a.html">https://proceedings.mlr.press/v202/yasunaga23a.html</a>
</div>
<div id="ref-Yu+2022" class="csl-entry" role="listitem">
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., &amp; Wu, Y. (2022). Scaling autoregressive models for content-rich text-to-image generation. <em>Transactions on Machine Learning Research</em>. <a href="https://openreview.net/forum?id=AFDcYJKhND">https://openreview.net/forum?id=AFDcYJKhND</a>
</div>
<div id="ref-Yu+2023" class="csl-entry" role="listitem">
Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., Ross, C., Polyak, A., Howes, R., Sharma, V., Xu, P., Tamoyan, H., Ashual, O., Singer, U., Li, S.-W., … Aghajanyan, A. (2023). <em>Scaling autoregressive multi-modal models: Pretraining and instruction tuning</em>. <a href="https://arxiv.org/abs/2309.02591">https://arxiv.org/abs/2309.02591</a>
</div>
<div id="ref-Zhao+2023" class="csl-entry" role="listitem">
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). <em>A survey of large language models</em>. <a href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>勾配の爆発に対しては gradient clipping などの対症療法が用いられる．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>他にも Google の <a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(<a href="#ref-Lepikhin+2021" role="doc-biblioref">Lepikhin et al., 2021</a>)</span>，<a href="https://japan.googleblog.com/2023/05/palm-2.html">PaML</a> <span class="citation" data-cites="Chowdhery+2022">(<a href="#ref-Chowdhery+2022" role="doc-biblioref">Chowdhery et al., 2022</a>)</span>，Google Brain の Switch Transformers <span class="citation" data-cites="Fedus+2022">(<a href="#ref-Fedus+2022" role="doc-biblioref">Fedus et al., 2022</a>)</span>，Google DeepMind の <a href="https://deepmind.google/discover/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval/">Gopher</a> <span class="citation" data-cites="Rae+2021">(<a href="#ref-Rae+2021" role="doc-biblioref">Rae et al., 2021</a>)</span> などがある．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>パラメトリックな知識ベースとしての利用については <span class="citation" data-cites="Raffel+2020">(<a href="#ref-Raffel+2020" role="doc-biblioref">Raffel et al., 2020</a>)</span>，<span class="citation" data-cites="Roberts+2020">(<a href="#ref-Roberts+2020" role="doc-biblioref">Roberts et al., 2020</a>)</span> など．一方で <span class="citation" data-cites="Marcus2020">(<a href="#ref-Marcus2020" role="doc-biblioref">Marcus, 2020</a>)</span> などは，hallucination などの欠点を補う形で，古典的な知識ベースと連結したハイブリット型での使用を提案している．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>モデルの比較は <span class="citation" data-cites="Raffel+2020">(<a href="#ref-Raffel+2020" role="doc-biblioref">Raffel et al., 2020</a>)</span> などが行っている．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="Ha-Schmidthuber2018">(<a href="#ref-Ha-Schmidthuber2018" role="doc-biblioref">Ha &amp; Schmidhuber, 2018</a>)</span> は RNN により世界モデルを構築している．<span class="citation" data-cites="Kaiser+2020">(<a href="#ref-Kaiser+2020" role="doc-biblioref">Kaiser et al., 2020</a>)</span> は動画から Atari を学習している．<span class="citation" data-cites="Hafner+2021">(<a href="#ref-Hafner+2021" role="doc-biblioref">Hafner et al., 2021</a>)</span> はさらに性能が良い．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://www.nikkei.com/article/DGXZQOUC268J80W3A221C2000000/">日経新聞 (2/19/2024)</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="quarto-dev/quarto-web" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>