<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="司馬博文">

<title>トランスフォーマー – Hirofumi Shiba</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/Shiba2.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-9849038e97c5862c14c4eb81893d019b.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-574e53edf4fce1fa3fc4689e835a41bb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Zen+Kurenaido&amp;display=swap" rel="stylesheet">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&amp;display=swap" rel="stylesheet">

<style>
  h1, .title, .description, .subtitle {
    font-family: "Zen Kurenaido", sans-serif !important;
  }
</style>

<!-- <style>
  .menu-text {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
  .navbar-title {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
</style> -->

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../assets/styles.css">
<meta property="og:title" content="トランスフォーマー – Hirofumi Shiba">
<meta property="og:description" content="2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．">
<meta property="og:image" content="https://162348.github.io/posts/2024/Kernels/Transformer.png">
<meta property="og:site_name" content="Hirofumi Shiba">
<meta property="og:image:height" content="582">
<meta property="og:image:width" content="391">
<meta name="twitter:title" content="トランスフォーマー – Hirofumi Shiba">
<meta name="twitter:description" content="2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．">
<meta name="twitter:image" content="https://162348.github.io/posts/2024/Kernels/Transformer.png">
<meta name="twitter:creator" content="@ano2math5">
<meta name="twitter:image-height" content="582">
<meta name="twitter:image-width" content="391">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Notes</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-notes">    
        <li>
    <a class="dropdown-item" href="../../../static/English.html">
 <span class="dropdown-text">English Notes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../blog.html">
 <span class="dropdown-text">ノート (Japanese)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Materials.html"> 
<span class="menu-text">Materials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/162348/162348.github.io/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">トランスフォーマー</h1>
            <p class="subtitle lead">深層生成モデル１</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Deep</div>
                <div class="quarto-category">AI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>司馬博文 </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">2/20/2024</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">概要</div>
      2023 年までの「基盤モデル」と呼ばれるような大規模な深層学習モデルは，ほとんど全て同一のアーキテクチャを持つ．これがトランスフォーマーである．その構造を，主に言語の分野に注目して概説する．最後に画像と動画の分野にも触れる．
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目次</h2>
   
  <ul>
  <li><a href="#トランスフォーマー" id="toc-トランスフォーマー" class="nav-link active" data-scroll-target="#トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#名前の由来と背景" id="toc-名前の由来と背景" class="nav-link" data-scroll-target="#名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</a></li>
  <li><a href="#注意機構" id="toc-注意機構" class="nav-link" data-scroll-target="#注意機構"><span class="header-section-number">1.2</span> 注意機構</a></li>
  <li><a href="#トランスフォーマーの全体" id="toc-トランスフォーマーの全体" class="nav-link" data-scroll-target="#トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</a></li>
  <li><a href="#なぜトランスフォーマーはうまく行くのか" id="toc-なぜトランスフォーマーはうまく行くのか" class="nav-link" data-scroll-target="#なぜトランスフォーマーはうまく行くのか"><span class="header-section-number">1.4</span> なぜトランスフォーマーはうまく行くのか？</a></li>
  </ul></li>
  <li><a href="#言語トランスフォーマー" id="toc-言語トランスフォーマー" class="nav-link" data-scroll-target="#言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#言語の取り扱い" id="toc-言語の取り扱い" class="nav-link" data-scroll-target="#言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</a></li>
  <li><a href="#従来の言語モデル" id="toc-従来の言語モデル" class="nav-link" data-scroll-target="#従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</a></li>
  <li><a href="#トランスフォーマーによる言語モデルとその訓練" id="toc-トランスフォーマーによる言語モデルとその訓練" class="nav-link" data-scroll-target="#トランスフォーマーによる言語モデルとその訓練"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデルとその訓練</a></li>
  <li><a href="#近年の進展" id="toc-近年の進展" class="nav-link" data-scroll-target="#近年の進展"><span class="header-section-number">2.4</span> 近年の進展</a></li>
  </ul></li>
  <li><a href="#sec-fine-tuning" id="toc-sec-fine-tuning" class="nav-link" data-scroll-target="#sec-fine-tuning"><span class="header-section-number">3</span> 基盤モデル</a>
  <ul class="collapse">
  <li><a href="#大規模言語モデル" id="toc-大規模言語モデル" class="nav-link" data-scroll-target="#大規模言語モデル"><span class="header-section-number">3.1</span> 大規模言語モデル</a></li>
  <li><a href="#sec-foundation-model" id="toc-sec-foundation-model" class="nav-link" data-scroll-target="#sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</a></li>
  <li><a href="#プロンプトエンジニアリング" id="toc-プロンプトエンジニアリング" class="nav-link" data-scroll-target="#プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</a></li>
  <li><a href="#rag" id="toc-rag" class="nav-link" data-scroll-target="#rag"><span class="header-section-number">3.4</span> RAG</a></li>
  <li><a href="#sec-alignment" id="toc-sec-alignment" class="nav-link" data-scroll-target="#sec-alignment"><span class="header-section-number">3.5</span> アラインメント</a></li>
  </ul></li>
  <li><a href="#sec-multimodal-transformer" id="toc-sec-multimodal-transformer" class="nav-link" data-scroll-target="#sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</a>
  <ul class="collapse">
  <li><a href="#sec-ViT" id="toc-sec-ViT" class="nav-link" data-scroll-target="#sec-ViT"><span class="header-section-number">4.1</span> 画像認識トランスフォーマー (ViT)</a></li>
  <li><a href="#画像生成トランスフォーマー" id="toc-画像生成トランスフォーマー" class="nav-link" data-scroll-target="#画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</a></li>
  <li><a href="#text-to-image-トランスフォーマー" id="toc-text-to-image-トランスフォーマー" class="nav-link" data-scroll-target="#text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</a></li>
  <li><a href="#image-to-text" id="toc-image-to-text" class="nav-link" data-scroll-target="#image-to-text"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</a></li>
  <li><a href="#動画生成トランスフォーマー" id="toc-動画生成トランスフォーマー" class="nav-link" data-scroll-target="#動画生成トランスフォーマー"><span class="header-section-number">4.5</span> 動画生成トランスフォーマー</a></li>
  <li><a href="#世界モデルとしてのトランスフォーマー" id="toc-世界モデルとしてのトランスフォーマー" class="nav-link" data-scroll-target="#世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</a></li>
  <li><a href="#音声生成トランスフォーマー" id="toc-音声生成トランスフォーマー" class="nav-link" data-scroll-target="#音声生成トランスフォーマー"><span class="header-section-number">4.7</span> 音声生成トランスフォーマー</a></li>
  <li><a href="#text-to-speech-トランスフォーマー" id="toc-text-to-speech-トランスフォーマー" class="nav-link" data-scroll-target="#text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</a></li>
  <li><a href="#speach-to-text-トランスフォーマー" id="toc-speach-to-text-トランスフォーマー" class="nav-link" data-scroll-target="#speach-to-text-トランスフォーマー"><span class="header-section-number">4.9</span> Speach to Text トランスフォーマー</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
<p>$$</p>
<p>$$</p>
</div>
<section id="トランスフォーマー" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="トランスフォーマー"><span class="header-section-number">1</span> トランスフォーマー</h2>
<section id="名前の由来と背景" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="名前の由来と背景"><span class="header-section-number">1.1</span> 名前の由来と背景</h3>
<p>トランスフォーマー <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> は，注意 (attension) という機構を通じて，時系列データの依存関係を効率的に学習することの出来るモデルである．この「変換器」という名前は，後述の内部表現ベクトル <span class="math inline">\(Y\)</span> を，入力 <span class="math inline">\(X\)</span> から次元を変えずにより良いものに「変換する」というところから名前が付けられている．</p>
<p>初めは自然言語処理（特に機械翻訳）の文脈で導入されたデコーダーとエンコーダーの組からなるモデルであるが，そのエンコーダー部分だけで言語，画像，動画などあらゆる系列データのモデリング全体で抜群の性能を発揮する上に，これら複数ドメインのデータを組み合わせてモデリングすることもできる（第 <a href="#sec-multimodal-transformer" class="quarto-xref">4</a> 節）．</p>
<p>さらに，トランスフォーマーはアーキテクチャとして（CNN や RNN などに比べると）シンプルであり，大規模なデータセットで大規模なモデルを訓練することが出来るスケーラビリティが魅力である．また，モデルの大きさに対して性能が単調に改善するというスケーリング則 <span class="citation" data-cites="Hestness+2017">(<a href="#ref-Hestness+2017" role="doc-biblioref">Hestness et al., 2017</a>)</span>, <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span> が成り立つことが示されており，大規模な資源を投下して大規模なモデルを作る経営判断も下しやすかった．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/SimplePowerLaws.png" class="img-fluid figure-img"></p>
<figcaption>Scaling Laws <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span></figcaption>
</figure>
</div>
<p>その後すぐに，一度大規模なモデルを訓練してしまえば，少しの修正を施すのみで種々の下流タスクに適用することが可能であることが発覚した．これを <strong>基盤モデル</strong> という（第 <a href="#sec-foundation-model" class="quarto-xref">3.2</a> 節）．</p>
</section>
<section id="注意機構" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="注意機構"><span class="header-section-number">1.2</span> 注意機構</h3>
<p>トランスフォーマーの核はその注意機構にある．とはいっても，注意機構自体はトランスフォーマー以前から存在した技術である．</p>
<p>元々機械翻訳に用いられていたエンコーダー・デコーダー型の RNN の性能を向上させる機構として提案された <span class="citation" data-cites="Bahdanau+2015">(<a href="#ref-Bahdanau+2015" role="doc-biblioref">Bahdanau et al., 2015</a>)</span>．その後，<span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> の <em>Attention is All You Need</em> とは，<strong>注意機構のみが重要で，RNN としての構造（や画像では畳み込みの構造）を排してシンプルにした方が更に性能が向上する</strong>，という報告である．</p>
<p>時系列データの解析では，そして自然言語処理ではとりわけ，文脈というものが重要である．しかし文脈は長期の依存関係になることもしばしばあり，従来の RNN ではこのモデリングに苦労していた (bottleneck problem)．</p>
<p>注意機構は，遠く離れた２つのトークンも直接に相互作用を持つアーキテクチャになっており，この点を抜本的に解決したものである．その結果，元の RNN のアーキテクチャも不要とするくらいのモデリング能力を，自然言語のみでなく，画像や動画に対しても示したのである．</p>
<p>注意機構は自己注意と交差注意に分けられる．</p>
<section id="枠組み" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="枠組み"><span class="header-section-number">1.2.1</span> 枠組み</h4>
<p>トランスフォーマーに入力する系列を <span class="math inline">\(\{x^n\}_{n=1}^N\subset\mathbb{R}^D\)</span> で表す．生のデータをそのままモデルに入れるわけではないので，別の言葉で呼び変える．</p>
<p>慣習として，特に言語データの場合は各 <span class="math inline">\(x^n\)</span> を <strong>トークン</strong> (token) という．画像では <strong>パッチ</strong> (patch) ともいう．</p>
<p>以降，<span class="math inline">\(X:=(x^n)_{n=1}^N\in M_{ND}(\mathbb{R})\)</span> とも表す．</p>
</section>
<section id="自己注意機構のプロトタイプ" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="自己注意機構のプロトタイプ"><span class="header-section-number">1.2.2</span> 自己注意機構のプロトタイプ</h4>
<p>自己注意機構とは，<span class="math inline">\(Y=AX\)</span> によって定まる <span class="math inline">\(M_{ND}(\mathbb{R})\)</span> 上の線型変換 <span class="math inline">\(X\mapsto Y\)</span> のことである： <span id="eq-1"><span class="math display">\[
y^n=\sum_{m=1}^N a^n_mx^m,
\tag{1}\]</span></span> <span id="eq-2"><span class="math display">\[
a^n_m=\frac{e^{(x^n)^\top x^m}}{\sum_{k=1}^Ne^{(x^n)^\top x^k}}.
\tag{2}\]</span></span> ここで，<span class="math inline">\(A=(a^n_m)_{n,m\in[N]}\in M_N(\mathbb{R})\)</span> は <a href="https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E8%A1%8C%E5%88%97">確率行列</a> をなし，その成分を <strong>注意荷重</strong> (attention weight) という．</p>
<p>この変換において，同じ <span class="math inline">\(x^m\)</span> の値を，３回別々の意味で使われていることに注意する：</p>
<ul>
<li><a href="#eq-1" class="quarto-xref">Equation&nbsp;1</a> における <span class="math inline">\(x^m\)</span> は，新たな表現 <span class="math inline">\(y^n\)</span> を作るためのプロトタイプにような働きをしている．これを <strong>値</strong> (value) という．</li>
<li><a href="#eq-2" class="quarto-xref">Equation&nbsp;2</a> において，内積が用いられており，<span class="math inline">\(x^n\)</span> と <span class="math inline">\(x^m\)</span> の類似度が測られている．
<ul>
<li><span class="math inline">\(x^m\)</span> を，<span class="math inline">\(x^m\)</span> が提供出来る情報を要約した量としての働きをし，<strong>鍵</strong> (key) という．</li>
<li><span class="math inline">\(x^n\)</span> は，<span class="math inline">\(x^n\)</span> と関連すべき情報を要求する役割を果たし，<strong>クエリ</strong> (query) という．</li>
</ul></li>
<li>最終的に，鍵とクエリの類似度・マッチ度を，<a href="https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0">ソフトマックス関数</a> を通じて確率分布として表現し，値の空間 <span class="math inline">\(\{x^m\}_{m=1}^N\)</span> 上の確率質量関数 <span class="math inline">\(\{a^n_m\}_{m=1}^N\)</span> を得ている．これに関して <strong>平均する</strong> ことで，鍵 <span class="math inline">\(y^n\)</span> を得る．</li>
</ul>
</section>
<section id="内積による自己注意機構" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="内積による自己注意機構"><span class="header-section-number">1.2.3</span> 内積による自己注意機構</h4>
<p>３つの別々の役割を果たしている以上，それぞれ固有の表現を持っていても良いはずである．そこで，値，鍵，クエリに，それぞれにニューラルネットワーク <span class="math inline">\(W_{(\Lambda)}\in M_{DD_{(\Lambda)}}(\mathbb{R})\;(\Lambda\in\{V,K,Q\})\)</span> を与えて固有の表現 <span class="math display">\[
x_{(\Lambda)}^n:=XW_{(\Lambda)}
\]</span> を持たせ，この <span class="math inline">\(W_{(\Lambda)}\)</span> を誤差逆伝播法により同時に学習することとする．</p>
<p>こうして得るのが，<strong>内積による自己注意機構</strong> (dot-product self-attention mechanism) である．このとき，<span class="math inline">\(D_{(K)}=D_{(Q)}\)</span> は必要だが，<span class="math inline">\(y^n\in\mathbb{R}^{D_{(V)}}\)</span> は，元の次元 <span class="math inline">\(D\)</span> と異なっても良いことに注意．</p>
<p>最後に，ソフトマックス関数の適用において，勾配消失を回避するために，次元 <span class="math inline">\(D_{(K)}\)</span> に応じたスケーリングを介して <span class="math display">\[
a^n_m=\frac{e^{\frac{\left(x^n_{(Q)}\right)^\top x^m_{(K)}}{\sqrt{D_K}}}}{\sum_{k=1}^Ne^{\frac{\left(x^n_{(Q)}\right)^\top x^k_{(K)}}{\sqrt{D_K}}}}
\]</span> とする．これを最終的な <strong>自己注意機構</strong> (scaled dot-product self-attention mechanism) という．</p>
</section>
<section id="交差注意" class="level4" data-number="1.2.4">
<h4 data-number="1.2.4" class="anchored" data-anchor-id="交差注意"><span class="header-section-number">1.2.4</span> 交差注意</h4>
<p>デコーダーとエンコーダーの接続部に用いられる <strong>交差注意</strong> (cross attention) については，ここでは触れない．</p>
</section>
<section id="マスキング" class="level4" data-number="1.2.5">
<h4 data-number="1.2.5" class="anchored" data-anchor-id="マスキング"><span class="header-section-number">1.2.5</span> マスキング</h4>
<p>実際に学習するとき，注意荷重 <span class="math inline">\(A\)</span> は上三角部分が <span class="math inline">\(-\infty\)</span> になったものを用いる．</p>
<p>これは，次のトークンを予測するにあたって，そのトークンより後のトークンを見ないようにするためである．</p>
</section>
</section>
<section id="トランスフォーマーの全体" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="トランスフォーマーの全体"><span class="header-section-number">1.3</span> トランスフォーマーの全体</h3>
<p>注意機構に加えて，次の３要素を含め，典型的には 20 から 24 層を成した深層ニューラルネットワークがトランスフォーマーの全てである．<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/ModalNet-21.png" class="img-fluid figure-img"></p>
<figcaption>Transformer Architecture <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span></figcaption>
</figure>
</div>
<section id="多頭注意" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="多頭注意"><span class="header-section-number">1.3.1</span> 多頭注意</h4>
<p>以上の自己注意機構を１単位として，これを複数独立に訓練し，最終的にはこれらの線型結合を採用する仕組みを <strong>多頭注意</strong> (multi-head attention) という．</p>
<p>これにより，種々の文脈をより頑健に読み取ることが出来るようである．</p>
</section>
<section id="残差結合と正規化" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="残差結合と正規化"><span class="header-section-number">1.3.2</span> 残差結合と正規化</h4>
<p>更に勾配消失を回避するために，<a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">残差結合</a> を導入し，訓練の高速化のために正規化 <span class="citation" data-cites="Ba+2016">(<a href="#ref-Ba+2016" role="doc-biblioref">Ba et al., 2016</a>)</span> が導入される．</p>
<p>そして，モデルを大規模化していくには，この「多頭注意＋残差結合と正規化」のブロックを積み重ねる．</p>
</section>
<section id="sec-FF" class="level4" data-number="1.3.3">
<h4 data-number="1.3.3" class="anchored" data-anchor-id="sec-FF"><span class="header-section-number">1.3.3</span> 多層パーセプトロン</h4>
<p>注意機構は線型性が高いため，多頭注意の層の間に，通常の Feedforward ネットワークもスタックして，ネットワークの表現能力を保つ工夫もされる．</p>
</section>
<section id="正規化レイヤーについての補足" class="level4" data-number="1.3.4">
<h4 data-number="1.3.4" class="anchored" data-anchor-id="正規化レイヤーについての補足"><span class="header-section-number">1.3.4</span> 正規化レイヤーについての補足</h4>
<p>レイヤー正則化 (layer normalization) <span class="citation" data-cites="Ba+2016">(<a href="#ref-Ba+2016" role="doc-biblioref">Ba et al., 2016</a>)</span> は，バッチ正規化 (batch normalization) <span class="citation" data-cites="Ioffe-Szegedy2015">(<a href="#ref-Ioffe-Szegedy2015" role="doc-biblioref">Ioffe and Szegedy, 2015</a>)</span> が RNN にも適するようにした修正として提案された．</p>
<p>バッチ正規化は，ニューラルネットワークの内部層の学習が，手前の層のパラメータが時事刻々と変化するために安定した学習が出来ないという <strong>内部共変量シフト</strong> (internal covariate shift) にあると突き止め，これをモデルアーキテクチャに正規化層を取り入れることで解決するものである．</p>
<p>正規化層は，ニューラルネットワークへの入力を，平均が零で分散が <span class="math inline">\(1\)</span> になるように変換する．元々，ニューラルネットワークの入力を正規化してから学習させることで学習が効率化されることは知られていた <span class="citation" data-cites="LeCun+2012">(<a href="#ref-LeCun+2012" role="doc-biblioref">LeCun et al., 2012</a>)</span> が，バッチ正規化は，これをバッチごとに，かつ，モデルの内部にも取り込んだものである．</p>
<p>バッチ正規化は精度の上昇と訓練の加速をもたらす．これはバッチ正規化により大きな学習率で訓練しても活性化が発散せず，これにより訓練時間の短縮と，局所解に囚われにくく汎化性能の向上がもたらされているようである <span class="citation" data-cites="Bjorck+2018">(<a href="#ref-Bjorck+2018" role="doc-biblioref">Bjorck et al., 2018</a>)</span>．</p>
</section>
</section>
<section id="なぜトランスフォーマーはうまく行くのか" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="なぜトランスフォーマーはうまく行くのか"><span class="header-section-number">1.4</span> なぜトランスフォーマーはうまく行くのか？</h3>
<p>注意機構は全体として線型変換になっている．これをカーネル法などを用いて非線型にする試みは多くあるが，これは成功していない．<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>その代わり，トランスフォーマーのパラメータ数のほとんどは FF 層（ <a href="#sec-FF" class="quarto-xref">Section&nbsp;1.3.3</a> ）によるものであり，この層が大きな表現能力を持っていることが，トランスフォーマーの性能を支えていると考えられている．<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><strong>注意機構は，遠く離れた２つのトークンを直接相互作用可能にすることに妙がある</strong>．実際，注意機構は，荷重行列を入力から学習するような，荷重平均プーリング (weighted mean pooling) ともみなせる．</p>
</section>
</section>
<section id="言語トランスフォーマー" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="言語トランスフォーマー"><span class="header-section-number">2</span> 言語トランスフォーマー</h2>
<p>トランスフォーマーの訓練は，後述するように事前学習と事後調整からなる．事後調整は <a href="#sec-fine-tuning" class="quarto-xref">Section&nbsp;3</a> で述べる．ここでは，事前学習を，言語を例に取って説明する．</p>
<p>トランスフォーマーの事前学習とは <a href="https://ja.wikipedia.org/wiki/%E3%83%AC%E3%83%88%E3%83%AD%E3%83%8B%E3%83%A0">レトロニム</a> であり，トークン（≒単語）上の確率分布をモデリングをすることに他ならない．</p>
<p>古典的には <span class="math inline">\(n\)</span>-gram <a href="#sec-n-gram" class="quarto-xref">Section&nbsp;2.2.1</a> などのモデルが用いられていたが，これをニューラルネットワークによって作ることはトランスフォーマー以前から試みられていた <span class="citation" data-cites="Bengio+2000">(<a href="#ref-Bengio+2000" role="doc-biblioref">Bengio et al., 2000</a>)</span>．</p>
<p>その後，トランスフォーマーの登場まで，これには RNN <a href="#sec-RNN" class="quarto-xref">Section&nbsp;2.2.2</a> が主に用いられていた．しかし，RNN は長い系列に対しては勾配消失とボトルネック問題が起こりやすく，また，訓練の並列化が難しいという問題があった．</p>
<section id="言語の取り扱い" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="言語の取り扱い"><span class="header-section-number">2.1</span> 言語の取り扱い</h3>
<section id="単語の分散表現" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="単語の分散表現"><span class="header-section-number">2.1.1</span> 単語の分散表現</h4>
<p>言語をそのまま扱うのではなく，トークン <span class="math inline">\(x^n\in\mathbb{R}^D\)</span> の形に符号化する必要がある．</p>
<p>言語には他にも改行や数式，コンピューターコードがあるが，まずは単語の表現を考える．</p>
<p>単語を Euclid 空間内に埋め込んだものを <strong>分散表現</strong> (distributed representation) という．これを２層のニューラルネットワークで行う技術が <code>word2vec</code> である <span class="citation" data-cites="Mikolov2013">(<a href="#ref-Mikolov2013" role="doc-biblioref">Tomas Mikolov, Chen, et al., 2013</a>)</span>, <span class="citation" data-cites="Mikolov2013b">(<a href="#ref-Mikolov2013b" role="doc-biblioref">Tomas Mikolov, Sutskever, et al., 2013</a>)</span>．</p>
<p>その訓練法には２つあり，窓の幅を <span class="math inline">\(M=5\)</span> などとすると，</p>
<ul>
<li>CBOW (Continuous Bag of Words)：前後 <span class="math inline">\(M\)</span> 語のみを見せて，中央の語を予測する．</li>
<li>Continuous Skip-gram：中央の語を見せて，前後 <span class="math inline">\(M\)</span> 語を予測する．</li>
</ul>
<p>という，いずれも教師なしの方法によって学習される．</p>
</section>
<section id="トークン化" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="トークン化"><span class="header-section-number">2.1.2</span> トークン化</h4>
<p>バイトペア符号化 (BPE: Byte Pair Encoding) <span class="citation" data-cites="Sennrich+2016">(<a href="#ref-Sennrich+2016" role="doc-biblioref">Sennrich et al., 2016</a>)</span> は，データ圧縮の手法であるが，単語に限らず種々のデータを含んだ文字列を符号化するのに用いられる．</p>
</section>
<section id="位置情報符号化" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="位置情報符号化"><span class="header-section-number">2.1.3</span> 位置情報符号化</h4>
<p>トランスフォーマーはそのままではトークンの順番を考慮しないため，トークンの順番の情報も符号化時に含める必要がある．これを <strong>位置情報符号化</strong> (positional encoding) という <span class="citation" data-cites="Dufter+2021">(<a href="#ref-Dufter+2021" role="doc-biblioref">Dufter et al., 2021</a>)</span>．</p>
<p>このようにして，位置情報はトランスフォーマーのモデル構造を修正して組み込むのではなく，符号化の段階で組み込み，トランスフォーマーはそのまま使うのである．</p>
<p>これは，位置情報をトークンと同じ空間に埋め込んだ表現 <span class="math inline">\(r^n\)</span> を学習し， <span class="math display">\[
\widetilde{x}^n:=x^n+r^n
\]</span> を新たな符号とする．<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
</section>
</section>
<section id="従来の言語モデル" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="従来の言語モデル"><span class="header-section-number">2.2</span> 従来の言語モデル</h3>
<p>文章をトークン列 <span class="math inline">\(\{x^n\}_{n=1}^N\subset\mathbb{R}^D\)</span> に置き換えたあとに，この上の結合分布 <span class="math inline">\(p(x^1,\cdots,x^N)\)</span> をモデリングすることが，<strong>言語モデル</strong> の目標である．</p>
<section id="sec-n-gram" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="sec-n-gram"><span class="header-section-number">2.2.1</span> <span class="math inline">\(n\)</span>-gram</h4>
<p><span class="math inline">\(n\ge1\)</span> とし，<span class="math inline">\(x_i=0\;(i\le0)\)</span> として， <span class="math display">\[
p(x_1,\cdots,x_N)=\prod_{i=1}^Np_{\theta_i}(x_i|x_{i-n},\cdots,x_{i-1})
\]</span> という形で <span class="math inline">\(p\)</span> をモデリングする．</p>
<p>これを <span class="math inline">\(n\)</span>-gram モデルと呼ぶが，文章の長さ <span class="math inline">\(N\)</span> が大きくなると，必要なパラメータ <span class="math inline">\(\theta_n\)</span> の数が増加する．</p>
<p>これに対処する方法としては，<a href="../../../posts/2023/Surveys/SSM.html">隠れ Markov モデル</a> を用いることが考えられる．</p>
<p>ニューラルネットワーク <span class="citation" data-cites="Bengio+2000">(<a href="#ref-Bengio+2000" role="doc-biblioref">Bengio et al., 2000</a>)</span> を用いることも出来る．しかし，依存の長さ <span class="math inline">\(n\ge1\)</span> が固定されていることはやはり問題である．</p>
</section>
<section id="sec-RNN" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="sec-RNN"><span class="header-section-number">2.2.2</span> RNN</h4>
<p>長さ制限のない長期的な依存関係を Feedback network によって表現することが，<span class="citation" data-cites="Mikolov+2010">(<a href="#ref-Mikolov+2010" role="doc-biblioref">T. Mikolov et al., 2010</a>)</span> によって試みられた．</p>
<p>これは通常のニューラルネットワーク (FFN: Feedforward Network と呼ばれる) に，出力の一部を次の入力に使うという回帰的な流れを追加することで，隠れ Markov モデルのように次に持ち越される内部状態を持つことを可能にしたモデルである．</p>
<p>しかしこれは学習が困難であることと，結局長期的な依存関係は効率的に学習されないという２つの問題があった．</p>
<p>誤差の逆伝播を時間に対しても逆方向に繰り返す必要がある (Backpropagation through time) ので，長い系列に対しては逆伝播しなければいけない距離が長く，勾配消失・爆発が起こりやすい．これは長期的な依存関係を学習しにくいということももたらす．<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> また，並列化も難しく，大規模なモデルの学習は難しい．</p>
<p>これに対処するために，モデルの構造を変えて過去の情報を流用しやすくする方法も種々提案された．LSTM (Long short-term memory) <span class="citation" data-cites="Hochreiter-Schmidhuber1997">(<a href="#ref-Hochreiter-Schmidhuber1997" role="doc-biblioref">Hochreiter and Schmidhuber, 1997</a>)</span> や GRU (Gated Recurrent Unit) <span class="citation" data-cites="Cho+2014">(<a href="#ref-Cho+2014" role="doc-biblioref">Cho et al., 2014</a>)</span> などがその例である．</p>
</section>
</section>
<section id="トランスフォーマーによる言語モデルとその訓練" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="トランスフォーマーによる言語モデルとその訓練"><span class="header-section-number">2.3</span> トランスフォーマーによる言語モデルとその訓練</h3>
<p>トランスフォーマーによる言語モデルの最大の美点は，自己教師あり学習による言語モデルの学習が可能である点である．これによりインターネットに蓄積していた大量のデータが利用可能になる．</p>
<p>パラメータを自己教師あり学習により初期化することで，言語モデルの性能が大幅に改善できることは <span class="citation" data-cites="Dai-Le2015">(<a href="#ref-Dai-Le2015" role="doc-biblioref">Dai and Le, 2015</a>)</span> が LSTM 入りの RRN における実験を通じて最初に指摘したようである．</p>
<p>ラベルデータを必要とするならば，これは本当の意味でスケーラブルではなかったであろう．</p>
<ol type="1">
<li>BERT (Bidirectional Encoder Representations from Transformers) <span class="citation" data-cites="Devlin+2019">(<a href="#ref-Devlin+2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> は，双方向エンコーダーである．</li>
<li>GPT (Generative Pre-trained Transformer) <span class="citation" data-cites="Radford+2018">(<a href="#ref-Radford+2018" role="doc-biblioref">Radford et al., 2018</a>)</span> は，単方向デコーダーである．</li>
<li>BART (Bidirectional and Auto-Regressive Transformer) <span class="citation" data-cites="Lewis+2020-BART">(<a href="#ref-Lewis+2020-BART" role="doc-biblioref">M. Lewis et al., 2020</a>)</span> は，双方向エンコーダーと単方向デコーダーの両方を持つ．</li>
</ol>
<section id="デコーダーのみの言語モデル" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="デコーダーのみの言語モデル"><span class="header-section-number">2.3.1</span> デコーダーのみの言語モデル</h4>
<p>GPT などの生成モデルは，デコーダー部分のトランスフォーマーの機能を主に用いている．</p>
<p>これはまず，</p>
<ol type="1">
<li>トークン列 <span class="math inline">\((x^n)_{n=1}^{N-1}\)</span> を入力し，条件付き分布 <span class="math inline">\(p(x^N|x_1,\cdots,x^{N-1})\)</span> を得る．</li>
<li>分布 <span class="math inline">\(p(x^N|x_1,\cdots,x^{N-1})\)</span> からサンプリングをする．</li>
</ol>
<p>の２段階で行われる．こうして <span class="math inline">\((x^n)_{n=1}^N\)</span> を得たら，次は <span class="math inline">\(x^{N+1}\)</span> を生成し，文章が終わるまでこれを続けることで，最終的な生成を完遂する．</p>
<section id="条件付き分布の表現" class="level5" data-number="2.3.1.1">
<h5 data-number="2.3.1.1" class="anchored" data-anchor-id="条件付き分布の表現"><span class="header-section-number">2.3.1.1</span> 条件付き分布の表現</h5>
<p>大規模なデータセットの上で，文章を途中まで読み，次のトークンを推測する，という自己教師あり学習を行うことで，トークン上の条件付き分布を学習する．</p>
<p>この際に，先のトークンの情報は使わないように，注意機構を工夫 (masked / causal attention) して訓練する．</p>
</section>
<section id="条件付き分布からのサンプリング" class="level5" data-number="2.3.1.2">
<h5 data-number="2.3.1.2" class="anchored" data-anchor-id="条件付き分布からのサンプリング"><span class="header-section-number">2.3.1.2</span> 条件付き分布からのサンプリング</h5>
<p>仮に最も確率の高いトークンを毎回選択する場合，出力は決定論的であり，同じ表現を繰り返すことが多くみられる．</p>
<p>実は，より人間らしい表現は，確率の低いトークンもかなら頻繁に採用される <span class="citation" data-cites="Holtzman+2020">(<a href="#ref-Holtzman+2020" role="doc-biblioref">Holtzman et al., 2020</a>)</span>．</p>
<p>かと言って，純粋なサンプリングをしたのでは，文章全体から見て意味をなさない場合も多い．</p>
<p>これを解決したのが top-<span class="math inline">\(p\)</span> sampling / nucleus sampling <span class="citation" data-cites="Holtzman+2020">(<a href="#ref-Holtzman+2020" role="doc-biblioref">Holtzman et al., 2020</a>)</span> である．</p>
<p><a href="https://github.com/openai/gpt-2-output-dataset/issues/5">GPT-2 にも実装されている</a> ようである．</p>
</section>
</section>
<section id="エンコーダーのみの言語モデル" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="エンコーダーのみの言語モデル"><span class="header-section-number">2.3.2</span> エンコーダーのみの言語モデル</h4>
<p>BERT (bidirectional encoder representations from transformers) <span class="citation" data-cites="Devlin+2019">(<a href="#ref-Devlin+2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> などの言語理解モデルは，エンコーダ部分のトランスフォーマーの機能を主に用いている．その結果，生成は出来ない．</p>
<p>訓練は，データセットから単語を確率的に脱落させ，これを補完するように訓練する．結果として，文章の前後両方 (bidirectional) の文脈を考慮するようになるのである．</p>
<p>実際に使う際は，例えば感情の判別などでは，文章の冒頭に <code>[class]</code> などの特殊なトークンを置き，これをエンコーダーに通してトークンが何に置き換わるかを見ることで，判別を実行することができる．</p>
</section>
<section id="エンコーダーデコーダーの言語モデル" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="エンコーダーデコーダーの言語モデル"><span class="header-section-number">2.3.3</span> エンコーダー・デコーダーの言語モデル</h4>
<p>トランスフォーマーは原論文 <span class="citation" data-cites="Vaswani+2017">(<a href="#ref-Vaswani+2017" role="doc-biblioref">Vaswani et al., 2017</a>)</span> では，エンコーダーとデコーダーがセットになったモデルとして提案された．</p>
<p>これは機械翻訳を念頭に置いていたため，RNN の構造を引き継いだ形で提案されたためである．この場合，次のようにしてモデルは使われる</p>
<ol type="1">
<li>入力 <span class="math inline">\(X\)</span> をエンコーダーに通し，内部表現 <span class="math inline">\(Z\)</span> を得る．</li>
<li>この内部表現 <span class="math inline">\(Z\)</span> を元に，デコードした結果 <span class="math inline">\(Y\)</span> を出力する．</li>
<li>唯一，<span class="math inline">\(Z\)</span> をデコーダーに渡す部分での注意機構層では，鍵と値としては <span class="math inline">\(Z\)</span> を使うが，クエリとしては <span class="math inline">\(Y\)</span> を使う．</li>
</ol>
<p>３の機構を <strong>エンコーダー・デコーダーの注意機構</strong> (encoder-decoder / corss attention mechanism) といい，これによって <span class="math inline">\(Z\)</span> と <span class="math inline">\(Y\)</span> のトークンの間の類似度をモデルに取り入れる．</p>
</section>
</section>
<section id="近年の進展" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="近年の進展"><span class="header-section-number">2.4</span> 近年の進展</h3>
<section id="sec-in-context-learning" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="sec-in-context-learning"><span class="header-section-number">2.4.1</span> 分布外データに対するロバスト性</h4>
<p>GPT-4 などの大規模言語モデルが，コンテクスト内学習 (in-context learning) が可能であることが，興味深い現象として解析されている．</p>
<p>この文脈内学習とは，<strong>パラメータをそのタスクに対して事後調整した訳でもないのに優れた性能を見せること</strong> を指す <span class="citation" data-cites="Garg+2022">(<a href="#ref-Garg+2022" role="doc-biblioref">Garg et al., 2022</a>)</span>．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/ff_1.png" class="img-fluid figure-img"></p>
<figcaption>An Example of In-context Learning <span class="citation" data-cites="Bubeck+2023">(<a href="#ref-Bubeck+2023" role="doc-biblioref">Bubeck et al., 2023</a>)</span></figcaption>
</figure>
</div>
<p>これの理論的な解明が進んでいる．</p>
<p>トランスフォーマーの注意機構をデータが通過する過程が，勾配降下を通じて局所モデルを学習する過程と等価になっている見方が指摘されている <span class="citation" data-cites="vanOswald+2023">(<a href="#ref-vanOswald+2023" role="doc-biblioref">von&nbsp;Oswald et al., 2023</a>)</span>．すなわち，トランスフォーマーを通過すること自体が，汎用的な学習そのものになっている <span class="citation" data-cites="Akyurek+2023">(<a href="#ref-Akyurek+2023" role="doc-biblioref">Akyürek et al., 2023</a>)</span>, <span class="citation" data-cites="Bai+2023">(<a href="#ref-Bai+2023" role="doc-biblioref">Bai et al., 2023</a>)</span>, <span class="citation" data-cites="Guo+2024">(<a href="#ref-Guo+2024" role="doc-biblioref">Guo et al., 2024</a>)</span>, <span class="citation" data-cites="Kim-Suzuki2024">(<a href="#ref-Kim-Suzuki2024" role="doc-biblioref">Kim and Suzuki, 2024</a>)</span>．</p>
</section>
<section id="sec-SSM" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="sec-SSM"><span class="header-section-number">2.4.2</span> 状態空間モデルによる依存構造モデリング</h4>
<p>トランスフォーマーの注意機構はデータ内の長期的な依存関係をモデリングできる点が革新的なのであった．</p>
<p>しかし，そのシークエンスの長さに対して計算複雑性が非線型に増加してしまう点を改良すべく，種々の代替的なアーキテクチャが試みられており，中でも状態空間モデルを中間層に用いるモデルが注目されている．</p>
<p>しかし，まだまだ長期的な依存関係の処理を苦手とするため，言語においては注意機構ほど性能が出ず，トランスフォーマーに比べて並列計算が難しいために実行速度が遅くなる，という問題がある．</p>
<p>しかし，これらは解決可能で，トランスフォーマーの性能を凌駕する可能性があるとされている <span class="citation" data-cites="Gu+2022">(<a href="#ref-Gu+2022" role="doc-biblioref">Gu et al., 2022</a>)</span>, <span class="citation" data-cites="Fu+2023">(<a href="#ref-Fu+2023" role="doc-biblioref">Fu et al., 2023</a>)</span>．まず，状態空間モデルのパラメータを入力から決めることで依存関係のモデリングを豊かにし（ <strong>選択的状態空間モデル</strong> (Selective SSM) <span class="citation" data-cites="Gu-Dao2024">(<a href="#ref-Gu-Dao2024" role="doc-biblioref">Gu and Dao, 2024</a>)</span> ），並列可能なアルゴリズムも提案されている．</p>
<p>実際に，選択的状態空間モデルを注意機構と多層パーセプトロン層の代わりに取り入れた Mamba <span class="citation" data-cites="Gu-Dao2024">(<a href="#ref-Gu-Dao2024" role="doc-biblioref">Gu and Dao, 2024</a>)</span> は同じサイズのトランスフォーマーの性能を凌駕する．</p>
</section>
</section>
</section>
<section id="sec-fine-tuning" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-fine-tuning"><span class="header-section-number">3</span> 基盤モデル</h2>
<p>大規模なトランスフォーマーを，インターネットに蓄積していた大量のデータを用いて訓練することにより得るモデルは，チャットボットや感情分析，要約など種々の下流タスクに少しの事後調整を施すだけで抜群の性能を発揮することが発見された．</p>
<p>これを <strong>基盤モデル</strong> という．</p>
<section id="大規模言語モデル" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="大規模言語モデル"><span class="header-section-number">3.1</span> 大規模言語モデル</h3>
<section id="名前の由来と背景-1" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="名前の由来と背景-1"><span class="header-section-number">3.1.1</span> 名前の由来と背景</h4>
<p>自然言語処理にトランスフォーマーを応用した例は大きな成功を見ている．GPT <span class="citation" data-cites="Radford+2018">(<a href="#ref-Radford+2018" role="doc-biblioref">Radford et al., 2018</a>)</span>, <a href="https://openai.com/research/gpt-2-1-5b-release">GPT-2</a> <span class="citation" data-cites="Radford+2019">(<a href="#ref-Radford+2019" role="doc-biblioref">Radford et al., 2019</a>)</span>, GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span>, <a href="https://openai.com/research/gpt-4">GPT-4</a> <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023b</a>)</span> のシリーズはその代表であり，特に GPT-4 はその文脈内学習能力（第 <a href="#sec-in-context-learning" class="quarto-xref">2.4.1</a> 節）の高さから AGI の実現に向けた重要な一歩とも評されている <span class="citation" data-cites="Bubeck+2023">(<a href="#ref-Bubeck+2023" role="doc-biblioref">Bubeck et al., 2023</a>)</span>．</p>
<p>その成功は，アーキテクチャとして優れているという点よりもむしろ，並列化が可能であり GPU などの計算資源を効率的に使える <span class="citation" data-cites="Weng-Brockman2022">(<a href="#ref-Weng-Brockman2022" role="doc-biblioref">Weng and Brockman, 2022</a>)</span> という点にあり，アーキテクチャの改良よりも計算資源の増強が最終的に大きな進歩をもたらすという側面が大きい，という認識が優勢になっている <span class="citation" data-cites="Sutton2019">(<a href="#ref-Sutton2019" role="doc-biblioref">R. Sutton, 2019</a>)</span>．これはスケーリング則として理論的にも理解が試みられている <span class="citation" data-cites="Kaplan+2020">(<a href="#ref-Kaplan+2020" role="doc-biblioref">Kaplan et al., 2020</a>)</span>．</p>
<p>この観点から，トランスフォーマーを用いた事前学習済みの言語モデルが，種々のタスクをほとんど例示なし (few-shot / zero-shot) で解ける能力を創発する程度に大きい場合，その規模が意味を持つことを強調して，<strong>大規模言語モデル</strong> (LLM: Large Language Model) とも呼ぶ <span class="citation" data-cites="Zhao+2023">(<a href="#ref-Zhao+2023" role="doc-biblioref">Zhao et al., 2023</a>)</span>．</p>
</section>
<section id="最適なモデルサイズ" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="最適なモデルサイズ"><span class="header-section-number">3.1.2</span> 最適なモデルサイズ</h4>
<p><strong>従来の LLM は，訓練データに対してモデルが大規模すぎる</strong> 可能性があることが <span class="citation" data-cites="Hoffmann+2022">(<a href="#ref-Hoffmann+2022" role="doc-biblioref">Hoffmann et al., 2022</a>)</span> で指摘された．</p>
<table class="table-striped table-hover caption-top table">
<caption>Size of LLMs</caption>
<colgroup>
<col style="width: 50%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">Training Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">LaMDA <span class="citation" data-cites="Thoppilan+2022">(<a href="#ref-Thoppilan+2022" role="doc-biblioref">Thoppilan et al., 2022</a>)</span></td>
<td style="text-align: center;">137B</td>
<td style="text-align: center;">168B</td>
</tr>
<tr class="even">
<td style="text-align: center;">GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span></td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Jurassic <span class="citation" data-cites="Lieber+2021">(<a href="#ref-Lieber+2021" role="doc-biblioref">Lieber et al., 2021</a>)</span></td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="even">
<td style="text-align: center;">Gopher <span class="citation" data-cites="Rae+2021">(<a href="#ref-Rae+2021" role="doc-biblioref">Rae et al., 2021</a>)</span></td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">300B</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Chinchilla</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">1.4T</td>
</tr>
</tbody>
</table>
<p>最適なパラメータ-学習データサイズの比を考慮して設計された Chinchilla <span class="citation" data-cites="Hoffmann+2022">(<a href="#ref-Hoffmann+2022" role="doc-biblioref">Hoffmann et al., 2022</a>)</span> は，モデルのサイズは最も小さいにも拘らず，種々の下流タスクに対して，他のモデルを凌駕することが <span class="citation" data-cites="Hoffmann+2022">(<a href="#ref-Hoffmann+2022" role="doc-biblioref">Hoffmann et al., 2022</a>)</span> で報告されている．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/model_sizes.png" class="img-fluid figure-img"></p>
<figcaption>Number of Training Tokens <a href="https://babylm.github.io/">BabyLM Challenge</a></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled" title="種々の LLM とマルチモーダル化">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
種々の LLM とマルチモーダル化
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Google の <a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(<a href="#ref-Lepikhin+2021" role="doc-biblioref">Lepikhin et al., 2021</a>)</span>，<a href="https://japan.googleblog.com/2023/05/palm-2.html">PaML</a> <span class="citation" data-cites="Chowdhery+2022">(<a href="#ref-Chowdhery+2022" role="doc-biblioref">Chowdhery et al., 2022</a>)</span>，M4 <span class="citation" data-cites="Aharoni+2019">(<a href="#ref-Aharoni+2019" role="doc-biblioref">Aharoni et al., 2019</a>)</span>，Google Brain の Switch Transformers <span class="citation" data-cites="Fedus+2022">(<a href="#ref-Fedus+2022" role="doc-biblioref">Fedus et al., 2022</a>)</span>，Google DeepMind の <a href="https://deepmind.google/discover/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval/">Gopher</a> <span class="citation" data-cites="Rae+2021">(<a href="#ref-Rae+2021" role="doc-biblioref">Rae et al., 2021</a>)</span> などがある．</p>
<p>最近のものでは，</p>
<ul>
<li>Google の <a href="https://blog.google/technology/ai/lamda/">LaMDA</a> <span class="citation" data-cites="Thoppilan+2022">(<a href="#ref-Thoppilan+2022" role="doc-biblioref">Thoppilan et al., 2022</a>)</span> は会話に特化した LLM である．</li>
<li>Google から 12/6/2023 に Gemini <span class="citation" data-cites="Geminiteam+2023">(<a href="#ref-Geminiteam+2023" role="doc-biblioref">Team et al., 2023</a>)</span> が発表され，<a href="https://japan.googleblog.com/2024/02/gemini-15.html">2/16/2024</a> には Gemini 1.5 が発表された．
<ul>
<li>これに伴い，Bard と Duet AI はいずれも Gemini に名称変更された．</li>
<li><a href="https://research.google/pubs/gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding/">GShard</a> <span class="citation" data-cites="Lepikhin+2021">(<a href="#ref-Lepikhin+2021" role="doc-biblioref">Lepikhin et al., 2021</a>)</span> 同様，トランスフォーマーに加えて，新しいアーキテクチャである Sparsely-Gated MoE <span class="citation" data-cites="Shazeer+2017">(<a href="#ref-Shazeer+2017" role="doc-biblioref">Shazeer et al., 2017</a>)</span> が用いられている．これはモデルのパラメータを分割し（それぞれを専門家 expert という），１つの入力にはその一部分しか使わないようにすることでメモリを節約し並列化を可能にする手法である．</li>
<li>文書から高精度にテキストを抽出する LMDX (Language Model-based Document Information Extraction and Localization) <span class="citation" data-cites="Perot+2023">(<a href="#ref-Perot+2023" role="doc-biblioref">Perot et al., 2023</a>)</span> も用いられている．</li>
</ul></li>
<li>OpenAI から 9/5/2023 に GPT-4V <span class="citation" data-cites="OpenAI2023-GPT4V">(<a href="#ref-OpenAI2023-GPT4V" role="doc-biblioref">OpenAI, 2023c</a>)</span> が発表され，ChatGPT にも実装された．
<ul>
<li>Microsoft の研究者も，GPT-4V の出来ること関する考察 <span class="citation" data-cites="Yang+2023-GPT-4V">(<a href="#ref-Yang+2023-GPT-4V" role="doc-biblioref">Yang et al., 2023</a>)</span> を発表している．</li>
</ul></li>
</ul>
</div>
</div>
</div>
</section>
<section id="訓練の並列化" class="level4" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="訓練の並列化"><span class="header-section-number">3.1.3</span> 訓練の並列化</h4>
<p>ここまで大規模なモデルだと，訓練時の GPU の並行計算を適切に計画することが肝心になる．</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/parallelism.png" class="img-fluid figure-img"></p>
<figcaption>four types of parallelism <span class="citation" data-cites="Weng-Brockman2022">(<a href="#ref-Weng-Brockman2022" role="doc-biblioref">Weng and Brockman, 2022</a>)</span></figcaption>
</figure>
</div>
<p><a href="https://ja.wikipedia.org/wiki/ByteDance">ByteDance</a> の MegaScale <span class="citation" data-cites="Jiang+2024">(<a href="#ref-Jiang+2024" role="doc-biblioref">Jiang et al., 2024</a>)</span> は，12,288 の GPU を用いながら，55.2 % の Model FLOPs Utilization を引き出した．</p>
<p>更なる LLM の訓練の効率化には，GPU による並列計算におけるボトルネックである <a href="https://ja.wikipedia.org/wiki/%E3%83%A1%E3%83%A2%E3%83%AA%E5%B8%AF%E5%9F%9F%E5%B9%85">メモリ帯域幅</a> を克服するために，分散型訓練手法を採用することが提案されている．</p>
</section>
</section>
<section id="sec-foundation-model" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-foundation-model"><span class="header-section-number">3.2</span> 「基盤モデル」と事後調整</h3>
<p>GPT の P とは Pre-trained である．自己教師あり学習によって <a href="../../../posts/2024/Kernels/Deep.html#sec-pretraining-using-AE">事前学習</a> をしたあと，その後のタスクに応じて，教師あり学習によって <strong>事後調整</strong> (fine-tune) を行う．<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>事後調整では，目的関数に KL 乖離度を入れるなどして，元のモデルから遠く離れすぎないように工夫されている．</p>
<p>事後調整を行う前の大規模言語モデルのことを，種々の応用や下流タスク (downstream task) の基礎となるモデルであることと，そのものでは未完成であることとを強調して，<strong>基盤モデル</strong> (foundation model) とも呼ばれる <span class="citation" data-cites="Bommasani+2021">(<a href="#ref-Bommasani+2021" role="doc-biblioref">Bommasani et al., 2021</a>)</span>．</p>
<p>事後調整では，モデルの全体では規模が大きすぎるため，出力層の後に新しいニューラルネットを付加したり，最後の数層のみを追加で教師あり学習をしたりする方法が一般的である．または，LoRA (Low-Rank Adaptation) <span class="citation" data-cites="Hu+2021">(<a href="#ref-Hu+2021" role="doc-biblioref">E. J. Hu et al., 2021</a>)</span> では，トランスフォーマーの各層に新たな層を挿入し，これを学習する．</p>
<p>これは，事後調整に有効な内的次元は実際には小さく <span class="citation" data-cites="Aghajanyan+2021">(<a href="#ref-Aghajanyan+2021" role="doc-biblioref">Aghajanyan et al., 2021</a>)</span>，これに有効にアクセスし，効率的な事後調整を行うことが出来るという．さらに <span class="citation" data-cites="Zhou+2023">(<a href="#ref-Zhou+2023" role="doc-biblioref">Zhou et al., 2023</a>)</span> によると，事後調整に必要なラベル付きデータは，量よりも質が重要であり，<a href="https://puniupa.github.io/posts/2024/AI/LLM.html#sec-LLaMA">LLaMA</a> に対しても多くて 1000 データで十分であるようである．</p>
<p>事後調整には，他にも，ChatGPT のようなサービスを展開するために必要なユーザー体験の改善を目的としたものも含まれる．これは <strong>アラインメント</strong> とも呼ばれ，強化学習が用いられることが多い．実際，GPT-4 では <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(<a href="#ref-Christiano+2017" role="doc-biblioref">Christiano et al., 2017</a>)</span> が用いられている <span class="citation" data-cites="OpenAI2023">(<a href="#ref-OpenAI2023" role="doc-biblioref">OpenAI, 2023b, p. 2</a>)</span>．</p>
<p>これについては第 <a href="#sec-alignment" class="quarto-xref">3.5</a> 節で改めて論じる．．</p>
</section>
<section id="プロンプトエンジニアリング" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="プロンプトエンジニアリング"><span class="header-section-number">3.3</span> プロンプトエンジニアリング</h3>
<p>基盤モデルには世界と人間に対する膨大な知識が含まれているが，使い方によって大きく性能が変わる．正しい条件付けを行うことで，内部に存在する知識をうまく引き出すことができる．これを大規模言語モデルでは <strong>prompt engineering</strong> <span class="citation" data-cites="Liu+2023-PPP">(<a href="#ref-Liu+2023-PPP" role="doc-biblioref">Liu et al., 2023</a>)</span> という．プロンプトの送り方によって性能がどう変わるかを調べる新たな分野である．</p>
<p>その結果，プロンプト内で新たなタスクを定義するだけで，またはいくつか例を与えるだけで，これが解けてしまうこともわかっており，これを zero-shot または few-shot learning という．</p>
</section>
<section id="rag" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="rag"><span class="header-section-number">3.4</span> RAG</h3>
<p>LLM は世界に関する正確な知識を持っており，知識ベースとしての利用も期待されている <span class="citation" data-cites="Petroni+2019">(<a href="#ref-Petroni+2019" role="doc-biblioref">Petroni et al., 2019</a>)</span>．<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> しかし，知識を正確に，そして信頼出来る形で引き出すことが難しいのであった．</p>
<p>特に，出典を示すことや，最新の知識のアップデートなどが難題として待っている．</p>
<p>そこで，LLM に（自由に外部情報を探索できるという意味で）ノンパラメトリックな知識ベースを接続することで解決するのが RAG (Retrieval-Augmented Generation) モデル <span class="citation" data-cites="Lewis+2020">(<a href="#ref-Lewis+2020" role="doc-biblioref">P. Lewis et al., 2020</a>)</span> である．</p>
<p>DPR (Dense Passage Retriever) <span class="citation" data-cites="Karpukhin+2020">(<a href="#ref-Karpukhin+2020" role="doc-biblioref">Karpukhin et al., 2020</a>)</span> は文書を密に符号化する手法を開発し，これを用いて文書検索をすることで Q&amp;A タスクを効率的に解く手法を提案した．このような文書の符号化器は <strong>検索器</strong> (retriever) と呼ばれる．</p>
<p>RAG <span class="citation" data-cites="Lewis+2020">(<a href="#ref-Lewis+2020" role="doc-biblioref">P. Lewis et al., 2020</a>)</span> はこの検索器を BART <span class="citation" data-cites="Lewis+2020-BART">(<a href="#ref-Lewis+2020-BART" role="doc-biblioref">M. Lewis et al., 2020</a>)</span> に接続した．</p>
<p>REALM (Retrieval-Augmented Language Model) <span class="citation" data-cites="Guu+2020">(<a href="#ref-Guu+2020" role="doc-biblioref">Guu et al., 2020</a>)</span> も同時期に提案されている．</p>
<p>Meta での研究 <span class="citation" data-cites="Yasunaga+2023">(<a href="#ref-Yasunaga+2023" role="doc-biblioref">Yasunaga et al., 2023</a>)</span> はこの検索器を Text-to-Image トランスフォーマー である CM3 <span class="citation" data-cites="Aghajanyan+2022">(<a href="#ref-Aghajanyan+2022" role="doc-biblioref">Aghajanyan et al., 2022</a>)</span> と結合することで，初めて言語と画像の両方を扱える RAG モデル RA-CM3 (retrieval-augmented CM3) を構成した．</p>
<p><a href="https://openai.com/research/webgpt">WebGPT</a> <span class="citation" data-cites="Nakano+2022">(<a href="#ref-Nakano+2022" role="doc-biblioref">Nakano et al., 2022</a>)</span> は，RAG や REAML が文書検索をしているところを，Web 検索を実行できるようにした GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span> の事後調整である．</p>
</section>
<section id="sec-alignment" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-alignment"><span class="header-section-number">3.5</span> アラインメント</h3>
<p>LLM などの機械学習モデルを訓練する際の目的関数は，そのままでは人間社会が要請するものとずれがあることが多い．これを修正するような試みを <strong>アラインメント</strong> (alignment) という．</p>
<blockquote class="blockquote">
<p>For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, <strong>these models are not aligned with their users</strong>. <span class="citation" data-cites="Ouyang+2022">(<a href="#ref-Ouyang+2022" role="doc-biblioref">Ouyang et al., 2022</a>)</span></p>
</blockquote>
<p>加えて，人間の選好は，0-1 損失関数で表現できるものではないことが多い．そこで，強化学習を用いることが考えられた．しかし，一々人間がフィードバックを与える方法はスケーラビリティに深刻な問題があるため，「人間の選好」をモデリングするニューラルネットワークを <strong>代理モデル</strong> (surrogate model) として構築することも考える．</p>
<p>その代表的な手法が <a href="https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">人間のフィードバックによる強化学習</a> (RLHF: Reinforcement Learning through Human Feedback) <span class="citation" data-cites="Christiano+2017">(<a href="#ref-Christiano+2017" role="doc-biblioref">Christiano et al., 2017</a>)</span> である．</p>
<p><a href="https://openai.com/research/instruction-following">InstructGPT</a> <span class="citation" data-cites="Ouyang+2022">(<a href="#ref-Ouyang+2022" role="doc-biblioref">Ouyang et al., 2022</a>)</span> は OpenAI API を通じて寄せられたフィードバックを用いて，<a href="https://openai.com/research/openai-baselines-ppo">PPO</a> (Proximal Policy Optimization) アルゴリズム <span class="citation" data-cites="Schulman+2017">(<a href="#ref-Schulman+2017" role="doc-biblioref">Schulman et al., 2017</a>)</span> による強化学習により事後調整をしたものである．</p>
<p>近接ポリシー最適化 (PPO: Proximal Policy Optimization) アルゴリズム <span class="citation" data-cites="Schulman+2017">(<a href="#ref-Schulman+2017" role="doc-biblioref">Schulman et al., 2017</a>)</span> は 信頼領域ポリシー最適化 (TRPO: Trust Region Policy Optimization) <span class="citation" data-cites="Schulman+2015">(<a href="#ref-Schulman+2015" role="doc-biblioref">Schulman et al., 2015</a>)</span> の洗練化として提案されたもので，現在の RLHF においても最も広く使われている手法である <span class="citation" data-cites="Zheng+2023">(<a href="#ref-Zheng+2023" role="doc-biblioref">Zheng et al., 2023</a>)</span>．</p>
<p>InstructGPT が ChatGPT の前身となっている．</p>
</section>
</section>
<section id="sec-multimodal-transformer" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-multimodal-transformer"><span class="header-section-number">4</span> 多相トランスフォーマー</h2>
<p>トランスフォーマーは自然言語処理の文脈で開発されたが，画像や動画，音声 <span class="citation" data-cites="Radford+2023">(<a href="#ref-Radford+2023" role="doc-biblioref">Radford et al., 2023a</a>)</span>，さらにはプログラミング言語 <span class="citation" data-cites="Chen+2021">(<a href="#ref-Chen+2021" role="doc-biblioref">Chen et al., 2021</a>)</span> にも適用されている．</p>
<p>動画はまだしも画像には，直感的には時系列構造がないように思えるが，トランスフォーマーはもはや汎用のニューラルネットワークアーキテクチャとして使用できることが解りつつある．</p>
<p>それぞれの応用分野で <strong>モデルの構造は殆ど差異がなく</strong>，トークン化の手法などに差異があるのみのように見受けられる．<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<section id="sec-ViT" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="sec-ViT"><span class="header-section-number">4.1</span> 画像認識トランスフォーマー (ViT)</h3>
<p>画像の分類問題を解くためのエンコーダ・トランスフォーマーは ViT (Vision Transformer) <span class="citation" data-cites="Dosovitskiy+2021">(<a href="#ref-Dosovitskiy+2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span> と呼ばれており，ILSVRC (the ImageNet Large Scale Visual Recognition Challenge) では未だ <a href="../../../posts/2024/Kernels/Deep.html#sec-ResNet">ResNet</a> 系のモデルが優勢であった 2021 年に，これを超える性能を示した．</p>
<p>実はモデルは殆どトランスフォーマーそのままであり，肝要であったのは画像をトークン化である．ピクセルをそのまま用いるのではなく，ある程度大きなピクセルの集合である <strong>パッチ</strong> (patch) を用いることで，計算量を下げる．<span class="citation" data-cites="Dosovitskiy+2021">(<a href="#ref-Dosovitskiy+2021" role="doc-biblioref">Dosovitskiy et al., 2021</a>)</span> では <span class="math inline">\(16\times16\)</span> サイズなどが採用された．</p>
<p>一方で，画像を恣意的に系列化しているため，幾何学的な構造は１から学ぶ必要があり，最初からモデルに組み込まれている <a href="../../../posts/2024/Kernels/Deep.html#sec-CNN">CNN</a> よりは一般に多くの訓練データを必要とする．だが，これにより帰納バイアスが弱いということでもある．<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>ViT はその後，動画も扱える ViViT <span class="citation" data-cites="Arnab+2021">(<a href="#ref-Arnab+2021" role="doc-biblioref">Arnab et al., 2021</a>)</span>, あらゆるアスペクト比に対応する NaViT (Native Resolution ViT) <span class="citation" data-cites="Dehghani+2023">(<a href="#ref-Dehghani+2023" role="doc-biblioref">Dehghani et al., 2023</a>)</span> などの拡張が続いた．</p>
<p>DeepMind の <a href="https://deepmind.google/discover/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation/">Perceiver</a> <span class="citation" data-cites="Jaegle+2021">(<a href="#ref-Jaegle+2021" role="doc-biblioref">Jaegle et al., 2021</a>)</span> は画像，動画，音声のいずれのメディアの分類問題にも対応可能な，非対称な注意機構を持つトランスフォーマーである．</p>
</section>
<section id="画像生成トランスフォーマー" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="画像生成トランスフォーマー"><span class="header-section-number">4.2</span> 画像生成トランスフォーマー</h3>
<p>一方でデコーダートランスフォーマーを用いて，画像の生成モデリングを行った最初の例は <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> である．</p>
<p>なお，自己回帰的な生成モデルを通じて画像の生成を試みることは，CNN <span class="citation" data-cites="vandenOord+2016b">(<a href="#ref-vandenOord+2016b" role="doc-biblioref">van&nbsp;den&nbsp;Oord, Kalchbrenner, Vinyals, et al., 2016</a>)</span> や RNN <span class="citation" data-cites="vandenOord+2016">(<a href="#ref-vandenOord+2016" role="doc-biblioref">van&nbsp;den&nbsp;Oord, Kalchbrenner, and Kavukcuoglu, 2016</a>)</span> でも行われていた．</p>
<p>この際に判明したことには，画像の分類タスクでは連続表現が役に立っても，生成タスクでは高い解像度を持った画像の生成が難しく，離散表現が有効であることが知られている．しかしこれではデータ量が増えてしまうため，画像の <a href="../../../posts/2024/Computation/VI.html#sec-history">ベクトル量子化</a> が行われることが多い．</p>
<p>そこで，<a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> でも <span class="math inline">\(K\)</span>-平均法によるクラスタリングが行われており，さらに <a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いたデータ圧縮も行われている．</p>
<p><a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> では最終的に各ピクセルを one-hot 表現にまで落とし込み，これを GPT-2 モデル <span class="citation" data-cites="Radford+2019">(<a href="#ref-Radford+2019" role="doc-biblioref">Radford et al., 2019</a>)</span> につなげている．</p>
<p><a href="https://muse-model.github.io/">MUSE</a> <span class="citation" data-cites="Chang+2023">(<a href="#ref-Chang+2023" role="doc-biblioref">Chang et al., 2023</a>)</span> も，トランスフォーマーを用いた画像生成モデルの例である．</p>
<section id="拡散モデルとの邂逅" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="拡散モデルとの邂逅"><span class="header-section-number">4.2.1</span> 拡散モデルとの邂逅</h4>
<p><a href="../../../posts/2024/Samplers/Diffusion.html#sec-idea">潜在拡散モデル</a> で <a href="../../../posts/2024/Kernels/Deep.html#sec-U-net">U-Net</a> <span class="citation" data-cites="Ronneberger+2015">(<a href="#ref-Ronneberger+2015" role="doc-biblioref">Ronneberger et al., 2015</a>)</span> を用いていたところをトランスフォーマーに置換した <strong>拡散トランスフォーマー</strong> (DiT: Diffusion Transformer) <span class="citation" data-cites="Peebles-Xie2023">(<a href="#ref-Peebles-Xie2023" role="doc-biblioref">Peebles and Xie, 2023</a>)</span> が発表された．</p>
<p>その後，確率的補間 によって DiT を改良した SiT (Scalable Interpolant Transformer) <span class="citation" data-cites="Ma+2024">(<a href="#ref-Ma+2024" role="doc-biblioref">Ma et al., 2024</a>)</span> が発表された．</p>
</section>
</section>
<section id="text-to-image-トランスフォーマー" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="text-to-image-トランスフォーマー"><span class="header-section-number">4.3</span> Text-to-Image トランスフォーマー</h3>
<p>この分野は <span class="citation" data-cites="Reed+2016">(<a href="#ref-Reed+2016" role="doc-biblioref">Reed et al., 2016</a>)</span> 以来，初めは GAN によるアプローチが試みられていた．</p>
<p>GPT-3 <span class="citation" data-cites="Brown+2020">(<a href="#ref-Brown+2020" role="doc-biblioref">Brown et al., 2020</a>)</span> と <a href="https://openai.com/research/image-gpt">ImageGPT</a> <span class="citation" data-cites="Chen+2020">(<a href="#ref-Chen+2020" role="doc-biblioref">Chen et al., 2020</a>)</span> とは殆ど同じモデルを用いている．これらを組み合わせたデコーダー型のトランスフォーマーが <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(<a href="#ref-Ramesh+2021" role="doc-biblioref">Ramesh et al., 2021</a>)</span> である．<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>トークン化して仕舞えば，言語も画像も等価に扱えるというのである．Google の <a href="https://sites.research.google/parti/">Parti</a> <span class="citation" data-cites="Yu+2022">(<a href="#ref-Yu+2022" role="doc-biblioref">J. Yu et al., 2022</a>)</span> も同様のアプローチである．</p>
<p>Meta の CM3 <span class="citation" data-cites="Aghajanyan+2022">(<a href="#ref-Aghajanyan+2022" role="doc-biblioref">Aghajanyan et al., 2022</a>)</span> と <a href="https://ai.meta.com/blog/generative-ai-text-images-cm3leon/">CM3leon</a> <span class="citation" data-cites="Yu+2023">(<a href="#ref-Yu+2023" role="doc-biblioref">L. Yu et al., 2023</a>)</span> は画像と言語を両方含んだ HTML ドキュメントから学習している．</p>
<p>Google DeepMind の <a href="https://deepmind.google/discover/blog/tackling-multiple-tasks-with-a-single-visual-language-model/">Flamingo</a> <span class="citation" data-cites="Alayrac+2022">(<a href="#ref-Alayrac+2022" role="doc-biblioref">Alayrac et al., 2022</a>)</span> は画像から言語を生成する．</p>
</section>
<section id="image-to-text" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="image-to-text"><span class="header-section-number">4.4</span> Image-to-Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/clip">CLIP</a> (Contrastive Language-Image Pre-training) <span class="citation" data-cites="Radford+2021">(<a href="#ref-Radford+2021" role="doc-biblioref">Radford et al., 2021</a>)</span> は画像の表現学習をする視覚モデルである．これは <a href="https://openai.com/research/dall-e">DALL-E</a> <span class="citation" data-cites="Ramesh+2021">(<a href="#ref-Ramesh+2021" role="doc-biblioref">Ramesh et al., 2021</a>)</span> と同時に開発された重要な構成要素である．</p>
<p>一方で <a href="https://openai.com/dall-e-2">DALL-E2</a> <span class="citation" data-cites="Ramesh+2022">(<a href="#ref-Ramesh+2022" role="doc-biblioref">Ramesh et al., 2022</a>)</span> では，CLIP により画像を潜在空間にエンコードし，拡散モデルによってデコードする．</p>
<p><a href="https://openai.com/research/dall-e-3-system-card">DALL-E3</a> <span class="citation" data-cites="OpenAI2023DallE3">(<a href="#ref-OpenAI2023DallE3" role="doc-biblioref">OpenAI, 2023a</a>)</span> もその改良である．</p>
</section>
<section id="動画生成トランスフォーマー" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="動画生成トランスフォーマー"><span class="header-section-number">4.5</span> 動画生成トランスフォーマー</h3>
<p>動画を画像の連続と見てトランスフォーマーを応用するアプローチは Latte (Latent Diffusion Transformer) <span class="citation" data-cites="Rakhimov+2020">(<a href="#ref-Rakhimov+2020" role="doc-biblioref">Rakhimov et al., 2020</a>)</span> に始まる．</p>
<p><a href="https://wilson1yan.github.io/videogpt/index.html">VideoGPT</a> <span class="citation" data-cites="Yan+2021">(<a href="#ref-Yan+2021" role="doc-biblioref">Yan et al., 2021</a>)</span> では動画を 3D の CNN でデータ圧縮，VQ-VAE で量子化して離散的な潜在表現を得た後，GPT と殆ど似たトランスフォーマーに通して学習する．</p>
<p><a href="https://wayve.ai/company/">Wayve</a> の <a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(<a href="#ref-Hu+2023" role="doc-biblioref">A. Hu et al., 2023</a>)</span> も同様の手法で動画を生成しているが，その動画を用いて自動運転の強化学習に応用する点が画期的である．</p>
<p>OpenAI は 2/15/2024 に <a href="https://openai.com/sora">Sora</a> <span class="citation" data-cites="Brooks+2024">(<a href="#ref-Brooks+2024" role="doc-biblioref">Brooks et al., 2024</a>)</span> を発表した．これも <a href="../../../posts/2024/Samplers/Diffusion.html#sec-idea">潜在拡散モデル</a> <span class="citation" data-cites="Rombach+2022">(<a href="#ref-Rombach+2022" role="doc-biblioref">Rombach et al., 2022</a>)</span> 同様，自己符号化器による動画の潜在表現を得た上でパッチに分割し，この上で拡散トランスフォーマー <span class="citation" data-cites="Peebles-Xie2023">(<a href="#ref-Peebles-Xie2023" role="doc-biblioref">Peebles and Xie, 2023</a>)</span> の学習を行う．</p>
</section>
<section id="世界モデルとしてのトランスフォーマー" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="世界モデルとしてのトランスフォーマー"><span class="header-section-number">4.6</span> 世界モデルとしてのトランスフォーマー</h3>
<p>トランスフォーマーを世界モデルとして用いて，シミュレーションを行い動画を生成し，これをモデルベースの強化学習 <span class="citation" data-cites="Sutton-Barto2018">(<a href="#ref-Sutton-Barto2018" role="doc-biblioref">R. S. Sutton and Barto, 2018</a>)</span> の材料とすることが広く提案されている．これは learning in imagination <span class="citation" data-cites="Racaniere+2017">(<a href="#ref-Racaniere+2017" role="doc-biblioref">Racanière et al., 2017</a>)</span> と呼ばれる．<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>IRIS (Imagination with auto-Regression over an Inner Speech) <span class="citation" data-cites="Micheli+2023">(<a href="#ref-Micheli+2023" role="doc-biblioref">Micheli et al., 2023</a>)</span> はこれに初めてトランスフォーマーを用いた世界モデルから動画生成をした．</p>
<p><a href="https://wayve.ai/thinking/introducing-gaia1/">GAIA-1</a> (Generative AI for Autonomy) <span class="citation" data-cites="Hu+2023">(<a href="#ref-Hu+2023" role="doc-biblioref">A. Hu et al., 2023</a>)</span> は自動運転に特化した世界モデルを，トランスフォーマーを用いて構築している．</p>
<p>他にも，動画生成を強化学習に応用する例としては，OpenAI による <a href="https://openai.com/research/vpt">VPT (Video Pre-Training)</a> <span class="citation" data-cites="Baker+2022">(<a href="#ref-Baker+2022" role="doc-biblioref">Baker et al., 2022</a>)</span> がある．</p>
</section>
<section id="音声生成トランスフォーマー" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="音声生成トランスフォーマー"><span class="header-section-number">4.7</span> 音声生成トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/jukebox">Jukebox</a> <span class="citation" data-cites="Dhariwal+2020">(<a href="#ref-Dhariwal+2020" role="doc-biblioref">Dhariwal et al., 2020</a>)</span> は，<a href="../../../posts/2024/Kernels/Deep4.html#sec-VQ-VAE">VQ-VAE</a> を用いて音声データを圧縮・量子化し，トランスフォーマーに通したものである．</p>
<p>このトランスフォーマーは <a href="https://openai.com/research/sparse-transformer">Sparse Transformer</a> <span class="citation" data-cites="Child+2019">(<a href="#ref-Child+2019" role="doc-biblioref">Child et al., 2019</a>)</span> という，注意機構の計算効率を改良したモデルを用いている．</p>
</section>
<section id="text-to-speech-トランスフォーマー" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="text-to-speech-トランスフォーマー"><span class="header-section-number">4.8</span> Text-to-Speech トランスフォーマー</h3>
<p>Microsoft Research の <a href="https://www.microsoft.com/en-us/research/project/vall-e-x/">VALL-E</a> <span class="citation" data-cites="Wang+2023-VALL-E">(<a href="#ref-Wang+2023-VALL-E" role="doc-biblioref">Wang et al., 2023</a>)</span> は，音声データをベクトル量子化によって言語データと全く同等に扱うことで，トランスフォーマーを用いて音声生成を行っている．</p>
</section>
<section id="speach-to-text-トランスフォーマー" class="level3" data-number="4.9">
<h3 data-number="4.9" class="anchored" data-anchor-id="speach-to-text-トランスフォーマー"><span class="header-section-number">4.9</span> Speach to Text トランスフォーマー</h3>
<p>OpenAI の <a href="https://openai.com/research/whisper">Whisper</a> <span class="citation" data-cites="Radford+2023-Whisper">(<a href="#ref-Radford+2023-Whisper" role="doc-biblioref">Radford et al., 2023b</a>)</span> は encoder-decoder 型のトランスフォーマーを用いている．</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Aghajanyan+2021" class="csl-entry" role="listitem">
Aghajanyan, A., Gupta, S., and Zettlemoyer, L. (2021). <a href="https://aclanthology.org/2021.acl-long.568/">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</a>. In <em>Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing</em>,Vol. 1, pages 7319–7328.
</div>
<div id="ref-Aghajanyan+2022" class="csl-entry" role="listitem">
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., … Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2201.07520">CM3: A causal masked multimodal model of the internet</a>.
</div>
<div id="ref-Aharoni+2019" class="csl-entry" role="listitem">
Aharoni, R., Johnson, M., and Firat, O. (2019). <a href="https://doi.org/10.18653/v1/N19-1388">Massively multilingual neural machine translation</a>. In J. Burstein, C. Doran, and T. Solorio, editors, <em>Proceedings of the 2019 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</em>, pages 3874–3884. Minneapolis, Minnesota: Association for Computational Linguistics.
</div>
<div id="ref-Akyurek+2023" class="csl-entry" role="listitem">
Akyürek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. (2023). <a href="https://openreview.net/forum?id=0g0X4H8yN4I">​​What learning algorithm is in-context learning? Investigations with linear models</a>. In <em>The eleventh international conference on learning representations</em>.
</div>
<div id="ref-Alayrac+2022" class="csl-entry" role="listitem">
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., … Simonyan, K. (2022). <a href="https://arxiv.org/abs/2204.14198">Flamingo: A visual language model for few-shot learning</a>.
</div>
<div id="ref-Arnab+2021" class="csl-entry" role="listitem">
Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., and Schmid, C. (2021). <a href="https://doi.org/10.1109/ICCV48922.2021.00676">ViViT: A video vision transformer</a>. In <em>2021 IEEE/CVF international conference on computer vision (ICCV)</em>, pages 6816–6826.
</div>
<div id="ref-Ba+2016" class="csl-entry" role="listitem">
Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). <em><a href="https://arxiv.org/abs/1607.06450">Layer normalization</a></em>.
</div>
<div id="ref-Bahdanau+2015" class="csl-entry" role="listitem">
Bahdanau, D., Cho, K., and Bengio, Y. (2015). <em><a href="https://arxiv.org/abs/1409.0473">Neural machine translation by jointly learning to align and translate</a></em>.
</div>
<div id="ref-Bai+2023" class="csl-entry" role="listitem">
Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. (2023). <a href="https://openreview.net/forum?id=vlCG5HKEkI">Transformers as statisticians: Provable in-context learning with in-context algorithm selection</a>. In <em>Workshop on efficient systems for foundation models @ ICML2023</em>.
</div>
<div id="ref-Baker+2022" class="csl-entry" role="listitem">
Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., … Clune, J. (2022). <a href="https://arxiv.org/abs/2206.11795">Video PreTraining (VPT): Learning to act by watching unlabeled online videos</a>.
</div>
<div id="ref-Bengio+2000" class="csl-entry" role="listitem">
Bengio, Y., Ducharme, R., and Vincent, P. (2000). <a href="https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html">A neural probabilistic language model</a>. In <em>Advances in neural information processing systems</em>,Vol. 13.
</div>
<div id="ref-Bjorck+2018" class="csl-entry" role="listitem">
Bjorck, N., Gomes, C. P., Selman, B., and Weinberger, K. Q. (2018). <a href="https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html">Understanding batch normalization</a>. In <em>Advances in neural information processing systems</em>,Vol. 31.
</div>
<div id="ref-Bommasani+2021" class="csl-entry" role="listitem">
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., Arx, S. von, … Liang, P. (2021). <a href="https://crfm.stanford.edu/report.html">On the opportunities and risks of foundation models</a>. <em>ArXiv</em>.
</div>
<div id="ref-Brooks+2024" class="csl-entry" role="listitem">
Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., … Ramesh, A. (2024). <em>Video generation models as world simulators</em>. OpenAI. Retrieved from <a href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a>
</div>
<div id="ref-Brown+2020" class="csl-entry" role="listitem">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., … Amodei, D. (2020). <a href="https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">Language models are few-shot learners</a>. In <em>Advances in neural information processing systems</em>,Vol. 33, pages 1877–1901.
</div>
<div id="ref-Bubeck+2023" class="csl-entry" role="listitem">
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., … Zhang, Y. (2023). <em><a href="https://arxiv.org/abs/2303.12712">Sparks of artificial general intelligence: Early experiments with GPT-4</a></em>.
</div>
<div id="ref-Chang+2023" class="csl-entry" role="listitem">
Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., … Krishnan, D. (2023). <a href="https://proceedings.mlr.press/v202/chang23b.html">Muse: Text-to-image generation via masked generative transformers</a>. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, <em>Proceedings of the 40th international conference on machine learning</em>,Vol. 202, pages 4055–4075. PMLR.
</div>
<div id="ref-Chen+2020" class="csl-entry" role="listitem">
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. (2020). <a href="https://proceedings.mlr.press/v119/chen20s.html">Generative pretraining from pixels</a>. In <em>Proceedings of the 37th international conference on machine learning</em>.
</div>
<div id="ref-Chen+2021" class="csl-entry" role="listitem">
Chen, M., Tworek, J., Jun, H., Yuan, Q., Oliveira Pinto, H. P. de, Kaplan, J., … Zaremba, W. (2021). <a href="https://arxiv.org/abs/2107.03374">Evaluating large language models trained on code</a>.
</div>
<div id="ref-Child+2019" class="csl-entry" role="listitem">
Child, R., Gray, S., Radford, A., and Sutskever, I. (2019). <a href="https://arxiv.org/abs/1904.10509">Generating long sequences with sparse transformers</a>.
</div>
<div id="ref-Cho+2014" class="csl-entry" role="listitem">
Cho, K., Merriënboer, B. van, Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). <a href="https://doi.org/10.3115/v1/D14-1179">Learning phrase representations using <span>RNN</span> encoder<span>–</span>decoder for statistical machine translation</a>. In A. Moschitti, B. Pang, and W. Daelemans, editors, <em>Proceedings of the 2014 conference on empirical methods in natural language processing (<span>EMNLP</span>)</em>, pages 1724–1734. Doha, Qatar: Association for Computational Linguistics.
</div>
<div id="ref-Chowdhery+2022" class="csl-entry" role="listitem">
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., … Fiedel, N. (2022). <a href="https://arxiv.org/abs/2204.02311">PaLM: Scaling language modeling with pathways</a>.
</div>
<div id="ref-Christiano+2017" class="csl-entry" role="listitem">
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). <a href="https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html">Deep reinforcement learning from human preferences</a>. In <em>Advances in neural information processing systems</em>,Vol. 30.
</div>
<div id="ref-Dai-Le2015" class="csl-entry" role="listitem">
Dai, A. M., and Le, Q. V. (2015). <a href="https://arxiv.org/abs/1511.01432">Semi-supervised sequence learning</a>.
</div>
<div id="ref-Dehghani+2023" class="csl-entry" role="listitem">
Dehghani, M., Mustafa, B., Djolonga, J., Heek, J., Minderer, M., Caron, M., … Houlsby, N. (2023). <a href="https://openreview.net/forum?id=VpGFHmI7e5">Patch n<span>’</span> pack: NaViT, a vision transformer for any aspect ratio and resolution</a>. In <em>Thirty-seventh conference on neural information processing systems</em>.
</div>
<div id="ref-Devlin+2019" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). <a href="https://aclanthology.org/N19-1423/">BERT: Pre-training of deep bidirectional transformers for language understanding</a>. In <em>Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies</em>,Vol. 1, pages 4171–4186.
</div>
<div id="ref-Dhariwal+2020" class="csl-entry" role="listitem">
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. (2020). <a href="https://arxiv.org/abs/2005.00341">Jukebox: A generative model for music</a>.
</div>
<div id="ref-Dosovitskiy+2021" class="csl-entry" role="listitem">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … Houlsby, N. (2021). <a href="https://openreview.net/forum?id=YicbFdNTTy">An image is worth 16×16 words: Transformers for image recognition at scale</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Dufter+2021" class="csl-entry" role="listitem">
Dufter, P., Schmitt, M., and Schütze, H. (2021). <em><a href="https://arxiv.org/abs/2102.11090">Position information in transformers: An overview</a></em>.
</div>
<div id="ref-Fedus+2022" class="csl-entry" role="listitem">
Fedus, W., Zoph, B., and Shazeer, N. (2022). <a href="https://jmlr.org/papers/v23/21-0998.html">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</a>. <em>The Journal of Machine Learning Research</em>, <em>23</em>(120), 1–39.
</div>
<div id="ref-Fu+2023" class="csl-entry" role="listitem">
Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. (2023). <a href="https://openreview.net/forum?id=COZDy0WYGg">Hungry hungry hippos: Towards language modeling with state space models</a>. In <em>The eleventh international conference on learning representations</em>.
</div>
<div id="ref-Garg+2022" class="csl-entry" role="listitem">
Garg, S., Tsipras, D., Liang, P., and Valiant, G. (2022). <a href="https://openreview.net/forum?id=flNZJ2eOet">What can transformers learn in-context? A case study of simple function classes</a>. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, <em>Advances in neural information processing systems</em>.
</div>
<div id="ref-Gu-Dao2024" class="csl-entry" role="listitem">
Gu, A., and Dao, T. (2024). <a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-time sequence modeling with selective state spaces</a>.
</div>
<div id="ref-Gu+2022" class="csl-entry" role="listitem">
Gu, A., Goel, K., and Re, C. (2022). <a href="https://openreview.net/forum?id=uYLFoz1vlAC">Efficiently modeling long sequences with structured state spaces</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Guo+2024" class="csl-entry" role="listitem">
Guo, T., Hu, W., Mei, S., Wang, H., Xiong, C., Savarese, S., and Bai, Y. (2024). <a href="https://openreview.net/forum?id=ikwEDva1JZ">How do transformers learn in-context beyond simple functions? A case study on learning with representations</a>. In <em>The twelfth international conference on learning representations</em>.
</div>
<div id="ref-Guu+2020" class="csl-entry" role="listitem">
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.-W. (2020). REALM: Retrieval-augmented language model pre-training. In <em>Proceedings of the 37th international conference on machine learning</em>. JMLR.org.
</div>
<div id="ref-Ha-Schmidthuber2018" class="csl-entry" role="listitem">
Ha, D., and Schmidhuber, J. (2018). <a href="https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html">Recurrent world models facilitate policy evaluation</a>. In <em>Advances in neural information processing systems</em>,Vol. 31.
</div>
<div id="ref-Hafner+2021" class="csl-entry" role="listitem">
Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. (2021). <a href="https://openreview.net/forum?id=0oabwyZbOu">Mastering atari with discrete world models</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Hashimoto2024" class="csl-entry" role="listitem">
Hashimoto, T. (2024). Large language models. Lecture at MLSS2024.
</div>
<div id="ref-Hestness+2017" class="csl-entry" role="listitem">
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., … Zhou, Y. (2017). <a href="https://arxiv.org/abs/1712.00409">Deep learning scaling is predictable, empirically</a>.
</div>
<div id="ref-Hochreiter-Schmidhuber1997" class="csl-entry" role="listitem">
Hochreiter, S., and Schmidhuber, J. (1997). <a href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">Long short-time memory</a>. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780.
</div>
<div id="ref-Hoffmann+2022" class="csl-entry" role="listitem">
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … Sifre, L. (2022). <a href="https://arxiv.org/abs/2203.15556">Training compute-optimal large language models</a>.
</div>
<div id="ref-Holtzman+2020" class="csl-entry" role="listitem">
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. (2020). <a href="https://arxiv.org/abs/1904.09751">The curious case of neural text degeneration</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Hu+2023" class="csl-entry" role="listitem">
Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., … Corrado, G. (2023). <a href="https://arxiv.org/abs/2309.17080">GAIA-1: A generative world model for autonomous driving</a>.
</div>
<div id="ref-Hu+2021" class="csl-entry" role="listitem">
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W. (2021). <em><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-rank adaptation of large language models</a></em>.
</div>
<div id="ref-Ioffe-Szegedy2015" class="csl-entry" role="listitem">
Ioffe, S., and Szegedy, C. (2015). <a href="https://dl.acm.org/doi/10.5555/3045118.3045167">Batch normalization: Accelerating deep network training by reducing internal covariate shift</a>. In <em>Proceedings of the 32nd international conference on international conference on machine learning - volume 37</em>, pages 448–456. Lille, France: JMLR.org.
</div>
<div id="ref-Jaegle+2021" class="csl-entry" role="listitem">
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. (2021). <a href="https://proceedings.mlr.press/v139/jaegle21a.html">Perceiver: General perception with iterative attention</a>. In M. Meila and T. Zhang, editors, <em>Proceedings of the 38th international conference on machine learning</em>,Vol. 139, pages 4651–4664. PMLR.
</div>
<div id="ref-Jiang+2024" class="csl-entry" role="listitem">
Jiang, Z., Lin, H., Zhong, Y., Huang, Q., Chen, Y., Zhang, Z., … Liu, X. (2024). <a href="https://arxiv.org/abs/2402.15627">MegaScale: Scaling large language model training to more than 10,000 GPUs</a>.
</div>
<div id="ref-Kaiser+2020" class="csl-entry" role="listitem">
Kaiser, Ł., Babaeizadeh, M., Miłos, P., Osiński, B., Campbell, R. H., Czechowski, K., … Michalewski, H. (2020). <a href="https://openreview.net/forum?id=S1xCPJHtDB">Model based reinforcement learning for atari</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Kaplan+2020" class="csl-entry" role="listitem">
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., … Amodei, D. (2020). <em><a href="https://arxiv.org/abs/2001.08361">Scaling laws for neural language models</a></em>.
</div>
<div id="ref-Karpukhin+2020" class="csl-entry" role="listitem">
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., … Yih, W. (2020). <a href="https://doi.org/10.18653/v1/2020.emnlp-main.550">Dense passage retrieval for open-domain question answering</a>. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em>, pages 6769–6781. Online: Association for Computational Linguistics.
</div>
<div id="ref-Kim-Suzuki2024" class="csl-entry" role="listitem">
Kim, J., and Suzuki, T. (2024). <a href="https://arxiv.org/abs/2402.01258">Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape</a>.
</div>
<div id="ref-LeCun+2012" class="csl-entry" role="listitem">
LeCun, Y. A., Bottou, L., Orr, G. B., and Müller, K.-R. (2012). <a href="https://link.springer.com/book/10.1007/978-3-642-35289-8">Neural networks: Tricks of the trade</a>. In G. Montavon, G. B. Orr, and K.-R. Müller, editors, pages 9–48. Springer Berlin, Heidelberg.
</div>
<div id="ref-Lepikhin+2021" class="csl-entry" role="listitem">
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., … Chen, Z. (2021). <a href="https://openreview.net/forum?id=qrwe7XHTmYb">GShard: Scaling giant models with conditional computation and automatic sharding</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Lewis+2020-BART" class="csl-entry" role="listitem">
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., … Zettlemoyer, L. (2020). <a href="https://doi.org/10.18653/v1/2020.acl-main.703"><span>BART</span>: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</a>. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, <em>Proceedings of the 58th annual meeting of the association for computational linguistics</em>, pages 7871–7880. Online: Association for Computational Linguistics.
</div>
<div id="ref-Lewis+2020" class="csl-entry" role="listitem">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., … Kiela, D. (2020). <a href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>. In <em>Advances in neural information processing systems</em>,Vol. 33, pages 9459–9474.
</div>
<div id="ref-Lieber+2021" class="csl-entry" role="listitem">
Lieber, O., Sharir, O., Lenz, B., and Shoham, Y. (2021). <a href="https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1">Jurrassic-1: Technical details and evaluation</a>. AI21.
</div>
<div id="ref-Liu+2023-PPP" class="csl-entry" role="listitem">
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023). <a href="https://dl.acm.org/doi/full/10.1145/3560815">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</a>. <em>ACM Computing Surveys</em>, <em>55</em>(9), 1–35.
</div>
<div id="ref-Ma+2024" class="csl-entry" role="listitem">
Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., and Xie, S. (2024). <a href="https://arxiv.org/abs/2401.08740">SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers</a>.
</div>
<div id="ref-Marcus2020" class="csl-entry" role="listitem">
Marcus, G. (2020). <a href="https://arxiv.org/abs/2002.06177">The next decade in AI: Four steps towards robust artificial intelligence</a>.
</div>
<div id="ref-Micheli+2023" class="csl-entry" role="listitem">
Micheli, V., Alonso, E., and Fleuret, F. (2023). <a href="https://openreview.net/forum?id=vhFu1Acb0xb">Transformers are sample-efficient world models</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Mikolov2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Chen, K., Corrado, G., and Dean, J. (2013). <em><a href="https://arxiv.org/abs/1301.3781">Efficient estimation of word representations in vector space</a></em>.
</div>
<div id="ref-Mikolov+2010" class="csl-entry" role="listitem">
Mikolov, T., Kopecky, J., Burget, L., C̆ernocky, J., and Khudanpur, S. (2010). <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">Recurrent neural network based language model</a>. In <em>Proceedings of interspeech</em>.
</div>
<div id="ref-Mikolov2013b" class="csl-entry" role="listitem">
Mikolov, Tomas, Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). <a href="https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed representations of words and phrases and their compositionality</a>. In C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, <em>Advances in neural information processing systems</em>,Vol. 26. Curran Associates, Inc.
</div>
<div id="ref-Nakano+2022" class="csl-entry" role="listitem">
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., … Schulman, J. (2022). <a href="https://arxiv.org/abs/2112.09332">WebGPT: Browser-assisted question-answering with human feedback</a>.
</div>
<div id="ref-OpenAI2023DallE3" class="csl-entry" role="listitem">
OpenAI. (2023a). <em>DALL-E3 system card</em>. OpenAI. Retrieved from <a href="https://openai.com/research/dall-e-3-system-card">https://openai.com/research/dall-e-3-system-card</a>
</div>
<div id="ref-OpenAI2023" class="csl-entry" role="listitem">
OpenAI. (2023b). <em><a href="https://arxiv.org/abs/2303.08774">GPT-4 technical report</a></em>.
</div>
<div id="ref-OpenAI2023-GPT4V" class="csl-entry" role="listitem">
OpenAI. (2023c). <em>GPT-4V(ision) system card</em>. OpenAI. Retrieved from <a href="https://openai.com/research/gpt-4v-system-card">https://openai.com/research/gpt-4v-system-card</a>
</div>
<div id="ref-Ouyang+2022" class="csl-entry" role="listitem">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., … Lowe, R. (2022). <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf">Training language models to follow instructions with human feedback</a>. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, <em>Advances in neural information processing systems</em>,Vol. 35, pages 27730–27744. Curran Associates, Inc.
</div>
<div id="ref-Peebles-Xie2023" class="csl-entry" role="listitem">
Peebles, W., and Xie, S. (2023). <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">Scalable diffusion models with transformers</a>. In <em>Proceedings of the IEEE/CVF international conference on computer vision (ICCV)</em>, pages 4195–4205.
</div>
<div id="ref-Perot+2023" class="csl-entry" role="listitem">
Perot, V., Kang, K., Luisier, F., Su, G., Sun, X., Boppana, R. S., … Hua, N. (2023). <a href="https://arxiv.org/abs/2309.10952">LMDX: Language model-based document information extraction and localization</a>.
</div>
<div id="ref-Petroni+2019" class="csl-entry" role="listitem">
Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. (2019). <a href="https://doi.org/10.18653/v1/D19-1250">Language models as knowledge bases?</a> In K. Inui, J. Jiang, V. Ng, and X. Wan, editors, <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em>, pages 2463–2473. Hong Kong, China: Association for Computational Linguistics.
</div>
<div id="ref-Racaniere+2017" class="csl-entry" role="listitem">
Racanière, S., Weber, T., Reichert, D. P., Buesing, L., Guez, A., Rezende, D., … Wierstra, D. (2017). <a href="https://dl.acm.org/doi/10.5555/3295222.3295320">Imagination-augmented agents for deep reinforcement learning</a>. In <em>Proceedings of the 31st international conference on neural information processing systems</em>, pages 5694–5705. Red Hook, NY, USA: Curran Associates Inc.
</div>
<div id="ref-Radford+2021" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … Sutskever, I. (2021). <a href="https://proceedings.mlr.press/v139/radford21a.html">Learning transferable visual models from natural language supervision</a>. In M. Meila and T. Zhang, editors, <em>Proceedings of the 38th international conference on machine learning</em>,Vol. 139, pages 8748–8763. PMLR.
</div>
<div id="ref-Radford+2023" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. (2023a). <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">Robust speech recognition via large-scale weak supervision</a>. In <em>Proceedings of the 40th international conference on machine learning</em>. Honolulu, Hawaii, USA: JMLR.org.
</div>
<div id="ref-Radford+2023-Whisper" class="csl-entry" role="listitem">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. (2023b). <a href="https://dl.acm.org/doi/10.5555/3618408.3619590">Robust speech recognition via large-scale weak supervision</a>. In <em>Proceedings of the 40th international conference on machine learning</em>. Honolulu, Hawaii, USA: JMLR.org.
</div>
<div id="ref-Radford+2018" class="csl-entry" role="listitem">
Radford, A., narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em>Improving language understanding with unsupervised learning</em>. OpenAI. Retrieved from <a href="https://openai.com/research/language-unsupervised">https://openai.com/research/language-unsupervised</a>
</div>
<div id="ref-Radford+2019" class="csl-entry" role="listitem">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). <em><a href="https://github.com/openai/gpt-2?tab=readme-ov-file">Language models are unsupervised multitask learners</a></em>.
</div>
<div id="ref-Rae+2021" class="csl-entry" role="listitem">
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., … Irving, G. (2021). <em>Scaling language models: Methods, analysis &amp; insights from training gopher</em>. Google DeepMind. Retrieved from <a href="https://arxiv.org/abs/2112.11446">https://arxiv.org/abs/2112.11446</a>
</div>
<div id="ref-Raffel+2020" class="csl-entry" role="listitem">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., … Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. <em>J. Mach. Learn. Res.</em>, <em>21</em>(1).
</div>
<div id="ref-Rakhimov+2020" class="csl-entry" role="listitem">
Rakhimov, R., Volkhonskiy, D., Artemov, A., Zorin, D., and Burnaev, E. (2020). <a href="https://arxiv.org/abs/2006.10704">Latent video transformer</a>.
</div>
<div id="ref-Ramesh+2022" class="csl-entry" role="listitem">
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). <a href="https://arxiv.org/abs/2204.06125">Hierarchical text-conditional image generation with CLIP latents</a>.
</div>
<div id="ref-Ramesh+2021" class="csl-entry" role="listitem">
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., … Sutskever, I. (2021). <a href="https://proceedings.mlr.press/v139/ramesh21a.html">Zero-shot text-to-image generation</a>. In M. Meila and T. Zhang, editors, <em>Proceedings of the 38th international conference on machine learning</em>,Vol. 139, pages 8821–8831. PMLR.
</div>
<div id="ref-Reed+2016" class="csl-entry" role="listitem">
Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., and Lee, H. (2016). <a href="https://proceedings.mlr.press/v48/reed16.html">Generative adversarial text to image synthesis</a>. In M. F. Balcan and K. Q. Weinberger, editors, <em>Proceedings of the 33rd international conference on machine learning</em>,Vol. 48, pages 1060–1069. New York, New York, USA: PMLR.
</div>
<div id="ref-Roberts+2020" class="csl-entry" role="listitem">
Roberts, A., Raffel, C., and Shazeer, N. (2020). <a href="https://doi.org/10.18653/v1/2020.emnlp-main.437">How much knowledge can you pack into the parameters of a language model?</a> In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, <em>Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)</em>, pages 5418–5426. Online: Association for Computational Linguistics.
</div>
<div id="ref-Rombach+2022" class="csl-entry" role="listitem">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">High-resolution image systhesis with latent diffusion models</a>. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)</em>, pages 10684–10695.
</div>
<div id="ref-Ronneberger+2015" class="csl-entry" role="listitem">
Ronneberger, O., Fischer, P., and Brox, T. (2015). <a href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28">U-net: Convolutional networks for biomedical image segmentation</a>. In N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, editors, <em>Medical image computing and computer-assisted intervention – MICCAI 2015</em>, pages 234–241. Cham: Springer International Publishing.
</div>
<div id="ref-Schulman+2015" class="csl-entry" role="listitem">
Schulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). <a href="https://arxiv.org/abs/1502.05477">Trust region policy optimization</a>.
</div>
<div id="ref-Schulman+2017" class="csl-entry" role="listitem">
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). <a href="https://arxiv.org/abs/1707.06347">Proximal policy optimization algorithms</a>.
</div>
<div id="ref-Sennrich+2016" class="csl-entry" role="listitem">
Sennrich, R., Haddow, B., and Birch, A. (2016). <a href="https://aclanthology.org/P16-1162/">Neural machine translation of rare words with subword units</a>. In <em>Proceedings of the 54th annual meetings of the association for computational linguistics</em>,Vol. 1, pages 1715–1725.
</div>
<div id="ref-Shazeer+2017" class="csl-entry" role="listitem">
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. (2017). <a href="https://openreview.net/forum?id=B1ckMDqlg">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</a>. In <em>International conference on learning representations</em>.
</div>
<div id="ref-Sutton2019" class="csl-entry" role="listitem">
Sutton, R. (2019). <em><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The bitter lesson</a></em>.
</div>
<div id="ref-Sutton-Barto2018" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (2018). <em><a href="https://mitpress.mit.edu/9780262352703/reinforcement-learning/">Reinforcement learning: An introduction</a></em>. MIT Press.
</div>
<div id="ref-Tamkin+2021" class="csl-entry" role="listitem">
Tamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021). <a href="https://arxiv.org/abs/2102.02503">Understanding the capabilities, limitations, and societal impact of large language models</a>.
</div>
<div id="ref-Geminiteam+2023" class="csl-entry" role="listitem">
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., … Vinyals, O. (2023). <a href="https://arxiv.org/abs/2312.11805">Gemini: A family of highly capable multimodal models</a>.
</div>
<div id="ref-Thoppilan+2022" class="csl-entry" role="listitem">
Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., … Le, Q. (2022). <a href="https://arxiv.org/abs/2201.08239">LaMDA: Language models for dialog applications</a>.
</div>
<div id="ref-vandenOord+2016" class="csl-entry" role="listitem">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. (2016). <a href="https://proceedings.mlr.press/v48/oord16.html">Pixel recurrent neural networks</a>. <em>Proceedings of the 33rd International Conference on Machine Learning</em>.
</div>
<div id="ref-vandenOord+2016b" class="csl-entry" role="listitem">
van&nbsp;den&nbsp;Oord, A., Kalchbrenner, N., Vinyals, O., Espeholt, L., Graves, A., and Kavukcuoglu, K. (2016). <a href="https://dl.acm.org/doi/10.5555/3157382.3157633">Conditional image generation with PixelCNN decoders</a>. In <em>Proceedings of the 30th international conference on neural information processing systems</em>, pages 4797–4805. Red Hook, NY, USA: Curran Associates Inc.
</div>
<div id="ref-Vaswani+2017" class="csl-entry" role="listitem">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). <a href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">Attention is all you need</a>. In <em>Advances in neural information processing systems</em>,Vol. 30.
</div>
<div id="ref-vanOswald+2023" class="csl-entry" role="listitem">
von&nbsp;Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. (2023). <a href="https://research.google/pubs/transformers-learn-in-context-by-gradient-descent/">Transformers learn in-context by gradient descent</a>. In, pages 35151–35174.
</div>
<div id="ref-Wang+2023-VALL-E" class="csl-entry" role="listitem">
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., … Wei, F. (2023). <a href="https://arxiv.org/abs/2301.02111">Neural codec language models are zero-shot text to speech synthesizers</a>.
</div>
<div id="ref-Weng-Brockman2022" class="csl-entry" role="listitem">
Weng, L., and Brockman, G. (2022). <em>Techniques for training large neural networks</em>. OpenAI. Retrieved from <a href="https://openai.com/research/techniques-for-training-large-neural-networks">https://openai.com/research/techniques-for-training-large-neural-networks</a>
</div>
<div id="ref-Yan+2021" class="csl-entry" role="listitem">
Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. (2021). <em><a href="https://arxiv.org/abs/2104.10157">VideoGPT: Video generation using VQ-VAE and transformers</a></em>.
</div>
<div id="ref-Yang+2023-GPT-4V" class="csl-entry" role="listitem">
Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. (2023). <a href="https://arxiv.org/abs/2309.17421">The dawn of LMMs: Preliminary explorations with GPT-4V(ision)</a>.
</div>
<div id="ref-Yasunaga+2023" class="csl-entry" role="listitem">
Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., … Yih, W.-T. (2023). <a href="https://proceedings.mlr.press/v202/yasunaga23a.html">Retrieval-augmented multimodal language modeling</a>. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, <em>Proceedings of the 40th international conference on machine learning</em>,Vol. 202, pages 39755–39769. PMLR.
</div>
<div id="ref-Yu+2022" class="csl-entry" role="listitem">
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., … Wu, Y. (2022). <a href="https://openreview.net/forum?id=AFDcYJKhND">Scaling autoregressive models for content-rich text-to-image generation</a>. <em>Transactions on Machine Learning Research</em>.
</div>
<div id="ref-Yu+2023" class="csl-entry" role="listitem">
Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., … Aghajanyan, A. (2023). <a href="https://arxiv.org/abs/2309.02591">Scaling autoregressive multi-modal models: Pretraining and instruction tuning</a>.
</div>
<div id="ref-Zhao+2023" class="csl-entry" role="listitem">
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., … Wen, J.-R. (2023). <em><a href="https://arxiv.org/abs/2303.18223">A survey of large language models</a></em>.
</div>
<div id="ref-Zheng+2023" class="csl-entry" role="listitem">
Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., … Huang, X. (2023). <a href="https://arxiv.org/abs/2307.04964">Secrets of RLHF in large language models part i: PPO</a>.
</div>
<div id="ref-Zhou+2023" class="csl-entry" role="listitem">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., … Levy, O. (2023). <a href="https://openreview.net/forum?id=KBMOKmX2he"><span>LIMA</span>: Less is more for alignment</a>. In <em>Thirty-seventh conference on neural information processing systems</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>層の数などの表現は <span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span> から．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span> で聞きました．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span> で聞きました．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>もちろん結合することも考えられているが，加算を行うことが現状の多数派であるようである <span class="citation" data-cites="Hashimoto2024">(<a href="#ref-Hashimoto2024" role="doc-biblioref">Hashimoto, 2024</a>)</span>．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>勾配の爆発に対しては gradient clipping などの対症療法が用いられる．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>後述のアラインメントも事後調整の一つであるが，これと区別して，<strong>教師ありの事後調整</strong> (supervised fine-tuning) とも呼ばれる．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>パラメトリックな知識ベースとしての利用については <span class="citation" data-cites="Raffel+2020">(<a href="#ref-Raffel+2020" role="doc-biblioref">Raffel et al., 2020</a>)</span>，<span class="citation" data-cites="Roberts+2020">(<a href="#ref-Roberts+2020" role="doc-biblioref">Roberts et al., 2020</a>)</span> など．一方で <span class="citation" data-cites="Marcus2020">(<a href="#ref-Marcus2020" role="doc-biblioref">Marcus, 2020</a>)</span> などは，hallucination などの欠点を補う形で，古典的な知識ベースと連結したハイブリット型での使用を提案している．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>モデルの比較は <span class="citation" data-cites="Raffel+2020">(<a href="#ref-Raffel+2020" role="doc-biblioref">Raffel et al., 2020</a>)</span> などが行っている．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>トークン化に小規模な CNN を用いてデータ圧縮を行うこともある．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>すなわち，DALL-E は GPT-3 のマルチモーダルな実装である <span class="citation" data-cites="Tamkin+2021">(<a href="#ref-Tamkin+2021" role="doc-biblioref">Tamkin et al., 2021, p. 4</a>)</span>．<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><span class="citation" data-cites="Ha-Schmidthuber2018">(<a href="#ref-Ha-Schmidthuber2018" role="doc-biblioref">Ha and Schmidhuber, 2018</a>)</span> は RNN により世界モデルを構築している．<span class="citation" data-cites="Kaiser+2020">(<a href="#ref-Kaiser+2020" role="doc-biblioref">Kaiser et al., 2020</a>)</span> は動画から Atari を学習している．<span class="citation" data-cites="Hafner+2021">(<a href="#ref-Hafner+2021" role="doc-biblioref">Hafner et al., 2021</a>)</span> はさらに性能が良い．<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/162348\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            trigger: 'click',
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            positionFixed: true,
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      if (authorPrefersDark) {
          [baseTheme, altTheme] = [altTheme, baseTheme];
      }
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "162348/162348.github.io";
    script.dataset.repoId = "R_kgDOKlfKYQ";
    script.dataset.category = "Announcements";
    script.dataset.categoryId = "DIC_kwDOKlfKYc4CgDmb";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://162348.github.io/">
<p>Hirofumi Shiba</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/162348/162348.github.io/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ano2math5">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:shiba.hirofumi@ism.ac.jp">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>