---
title: "数学者のための深層学習６"
subtitle: 正規化流
author: "司馬 博文"
date: 2/14/2024
categories: [Kernel, Math Notes]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，深層生成モデルの１つ正規化流を概観する．
---

{{< include ../../../_preamble.qmd >}}

[@Kobyzev+2021], [@Papamakarios+2021]．

## 導入

[GAN](Deep3.qmd)，[VAE](Deep4.qmd)，[拡散モデル](Deep5.qmd) など，深層生成モデルは，潜在空間 $\cZ$ 上の基底分布 $p_z$ を，パラメータ $w\in\cW$ を持つ深層ニューラルネットによる変換 $f:\cZ\times\cW\to\cX$ を通じて，$\{f(w)_*p_z\}_{w\in\cW}$ により $\cX$ 上の分布をモデリングする．

このモデル $\{f(w)_*p_z\}_{w\in\cW}$ の尤度は解析的に表示できない．そこで，GAN [@Goodfellow+2014] は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものなのであった．

正則化流では，[拡散モデル](Deep5.qmd) に似て，「逆変換」を利用することを考える．

$\{f_w\}\subset\L(\cZ,\cX)$ が可逆であるように設計する．$g_w:=f_w^{-1}$ と表すと，$p_x(-|w)$ は $p_z$ の $g_w$ による引き戻しの関係になっているから，[変数変換](../../2023/Probability/Beta-Gamma.qmd#sec-transform) を通じて，
$$
p_x(x|w)=p_z(g_w(x))\abs{\det J_{g_w}(x)}\;\as
$$
が成立する．

すると，
$$
\log p(x|w)=\log p_z(g_w(x))+\log\abs{\det J_{g_w}(x)}
$$
を通じて，パラメータの最尤推定が可能である．

従って，可逆なニューラルネットワーク $\{f_w\}\subset\L(\cZ,\cX)$ を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．

このとき，行列式 $\det:\GL_D(\R)\to\R^\times$ は群準同型であるから，$g_w$ のヤコビアンは，各層のヤコビアンの積として得られる．

この条件を課しても，深層ニューラルネットワーク $\{f_w\}$ の表現力は十分高いため，$p_z$ としては典型的に正規分布を用いる．従って，$g_w$ はデータを正規化する写像である．それ故，以上のようなモデルを **正規化流** (normalizing flow) という．

## カップリング流

## 自己回帰流

## 連続流

