---
title: "数学者のための深層学習６"
subtitle: 正規化流
author: "司馬 博文"
date: 2/14/2024
categories: [Deep]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．
---

{{< include ../../../_preamble.qmd >}}

[@Kobyzev+2021], [@Papamakarios+2021]．

## 導入

[GAN](Deep3.qmd)，[VAE](Deep4.qmd)，[拡散モデル](Deep5.qmd) など，深層生成モデルは，潜在空間 $\cZ$ 上の基底分布 $p_z$ を，パラメータ $w\in\cW$ を持つ深層ニューラルネットによる変換 $f:\cZ\times\cW\to\cX$ を通じて，押し出し $\{f(w)_*p_z\}_{w\in\cW}$ により $\cX$ 上の分布をモデリングする．

これらのモデル $\{f(w)_*p_z\}_{w\in\cW}$ の尤度は解析的に表示できない．そこで，[GAN](Deep3.qmd) [@Goodfellow+2014] は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものであり，[VAE](Deep4.qmd) [@Kingma-Welling2014] は変分下界を通じて尤度を近似するというものであった．

**正則化流** (normalizing flow / flow-based models) では，[拡散モデル](Deep5.qmd) に似て，「逆変換」を利用することを考える．

すなわち，$\{f_w\}\subset\L(\cZ,\cX)$ が可逆であるように設計するのである．逆関数を $g_w:=f_w^{-1}$ と表すと，$p_x(-|w)$ は $p_z$ の $g_w$ による引き戻しの関係になっているから，[変数変換](../../2023/Probability/Beta-Gamma.qmd#sec-transform) を通じて，
$$
p_x(x|w)=p_z(g_w(x))\abs{\det J_{g_w}(x)}\;\as
$$
が成立する．

すると，
$$
\log p(x|w)=\log p_z(g_w(x))+\log\abs{\det J_{g_w}(x)}
$$
を通じて，尤度の評価とパラメータの最尤推定が可能である．

従って，可逆なニューラルネットワーク $\{f_w\}\subset\L(\cZ,\cX)$ を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．

このとき，行列式 $\det:\GL_D(\R)\to\R^\times$ は群準同型であるから，$g_w$ のヤコビアンは，各層のヤコビアンの積として得られる．

この条件はたしかにモデルに仮定を置いている（$p_z$ は典型的に正規で，$f_w$ は可逆である）．しかしそれでも，深層ニューラルネットワーク $\{f_w\}$ の表現力は十分高いため，モデリングにも使うことは出来るだろうが，どちらかというと学習されたサンプラーのような立ち位置に理解しやすい [@Gao+2020]．

## カップリング流

## 自己回帰流

## 連続流

### フローマッチング

フローマッチング [@Lipman+2023], rectified flow [@Liu+2023-Flow]

## 確率的補間

[@Albergo-Vanden-Eijnden2023] により提案されたもので，SiT (Scalable Interpolant Transformer) [@Ma+2024] でも用いられている技術である．

[@Albergo+2023]

