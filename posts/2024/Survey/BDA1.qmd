---
title: "ベイズデータ解析５"
subtitle: "回帰モデルの概観"
author: "司馬博文"
date: 12/5/2024
categories: [Bayesian, Statistics]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: |
    ベイズ回帰分析のワークフローを概観する．一つの悲願として，階層モデルを構築して，パラメータをもはや残さず，尤度の推定に成功することがあることを紹介する．
    分散分析はこの階層化の際の鍵を握る考え方として，現代でも重要な位置付けを得ることになる．
    また多くの回帰分析ではデータを変換して線型関係の推定に集中する場合が多く，これを扱う数理モデルとして一般化線型モデルを紹介する．
image: Files/BayesianWorkflow.svg
listing: 
    -   id: lst-survey
        type: grid
        sort: false
        contents:
            - "../Computation/brms.qmd"
            - "BDA2.qmd"
            - "Survey1.qmd"
            - "Survey2.qmd"
            - "Survey3.qmd"
            - "Survey4.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
---

{{< include ../../../assets/_preamble.qmd >}}

## 関連記事 {.unnumbered .unlisted}

::: {#lst-survey}
:::

## 回帰分析の一般事項

### はじめに：正規線型回帰

回帰分析とは，２つの確率変数 $X,Y$ の実現と見られるデータが得られている際に，その条件付き期待値 $\E[Y|X]$ に対して，$x\in\cX$ に依存するモデル
$$
\cX\ni x\mapsto P_{\theta,x}\in\cP(\cY)
$$
を考えることをいう．例えば
$$
P_{\theta,x}=\rN(\beta^\top x,\sigma^2),\qquad\theta=(\beta,\sigma),
$$
とした場合を **正規線型回帰** (normal linear model) という．

### ベイズ回帰分析

しかしベイズ回帰分析では，条件付き期待値 $\E[Y|X]$ を推定するというより，本質的には結合変数 $(X,Y)$ の結合分布をモデリングしているとみなすべきである．

実際，ベイズ回帰分析では事前分布 $P_\varphi$ も併せて次の２つのモデルが想定される：
$$
Y|X\sim P_{\theta,x},\qquad X\sim P_\varphi.
$$

この結果，パラメータの空間 $(\theta,\varphi)$ 上からの確率核
$$
(\theta,\varphi)\mapsto P_{\theta,x}P_\varphi(dx)\in\cP(\cX\times\cY)
$$
が想定されることになる．これを **尤度** という．

尤度によりデータの空間 $\cX\times\cY$ 上の確率分布に関する問題を，パラメータの空間上に移送することが **（ベイズ）推論** である．

ベイズ回帰分析とは，尤度を $P_{\theta}(x,dy)P_\varphi(dx)$ と分解された形で与える場合のベイズモデリングに過ぎない．換言すれば，$Y|X$ の依存構造に焦点がある際に行うベイズ推論である．

$\Theta\indep\Phi$ の仮定を置いていることが特徴であるが，$(X,Y)$ の結合分布の想定が簡単になる点が，ここまで人口に膾炙している理由であると思われる．

### ベイズ回帰分析ワークフロー {#sec-Bayesian-workflow}

1. $(X,Y)$ の依存構造が単純（線型）になるような変数変換を行う（一般化線型モデルの利用を含む）．
2. $(\Theta,\Phi)$ の事前分布を設定する（初めは一様分布やデフォルトの無情報事前分布で良い）．
3. 事後分布を計算し，事後予測分布を見てデータが再現できているかを基にモデルを検証する．

その後，十分に階層化をして，パラメータの空間上の事前分布がほとんど情報を持たなくて良いようにする，完全ベイズ推論が一つの悲願とされる．^[従来は事前分布の経験ベイズ推定と呼ばれていた考え方である [@Gelman+2020 p.6]．]

モデルの挙動がもはや事前分布に依存しなくなった際，モデルの階層構造や尤度の構造が十分にデータを反映できていると思われるためである．^[一方で多くの頻度論的な手法は，無情報事前分布を仮定したベイズ推論とみなせる．そこでベイズの，有効な頻度論的モデルを探索するための方法としての美点が見えてくるのである．]

> 推定すべきものはモデルの尤度であってパラメターの値ではないというのが赤池氏の主張です．いいかえると，推定すべきは確率構造であってパラメターではないというのです．[@田邉國士2010]

この階層化と尤度の推定を探索的に実行できる点がベイズの真の美点だと筆者は考える．そして一度モデルの構造・尤度が明瞭化された際は，もはやベイズである必要はない場合が多い．^[この点については [@Gelman2014] も参照．大統領選における有権者の行動のモデリングを，ベイズ階層モデルに基づいて探索的に実行しており，"multilevel Bayesian modeling can be considered as an elaborate form of exploratory data analysis" と結論している．]

## 階層モデル

### はじめに

階層モデルは複雑なモデルを構築するための強力なツールであり，ベイズのワークフローにおいて基本的な要素になる．

層別抽出やクラスター抽出をはじめとして，多くの場合階層別に知識が存在し，これらを系統的に組み込んだ形でモデルを構築できる．

しかし同時に計算が困難になり，第一近似として正規性が仮定される場合が多い．

### ランダム効果モデル

標本を $J$ 個のグループにわけ，これへの所属を表す２値変数 $x_i\in\R^J$ を説明変数に追加するとする．

このような所属変数 $x_i$ の回帰係数 $\beta\in\R^J$ に対して階層モデルが考えられる場合が多い．

特に $\al\in\R,\sigma_\beta>0$ をスカラーとして
$$
\beta\sim\rN_J(\al\b{1},\sigma^2_\beta I_J),
$$
というモデルを想定した場合，$x_{ij}$ の係数 $\beta_j$ は各グループ $j\in[J]$ 固有の切片項であり，**変量効果** (random effect) と呼ばれる．

変量効果の追加は，同一グループ内の $y_i$ に相関を生じさせるが，グループが違う場合は相変わらず独立のままとする効果がある．

さらに $\beta_j$ の分散 $\sigma^2_{\beta_j}$ を $\infty$ とした場合，すなわち（もはや確率分布ではないが）一様分布を仮定した場合を **固定効果** (fixed effect) という．^[[@Bafumi-Gelman2007] では unmodeled varying intercept と呼んでいる．]

この２つは事前分布の違いしかない．２種類の事前分布を混在させた場合は **混合モデル** (mixed model) という [@BDA p.383]．

### 分散分析

ベイズのワークフローにおいて，複数の説明変数間の階層関係の特定が極めて重要である [-@sec-Bayesian-workflow]．

特に，膨大な説明変数の中から「因子」（性別・教育水準など）とその「水準」（女性・大学院生など）とを峻別することが重要であり，それぞれに独自の回帰係数 $\beta^{(m)}\sim\rN_{J_m}(\al_{m}\b{1}_{J_m},\sigma^2_{m}I_{J_m})$ を与えることが尤度の推定において大きな効果を持つ [@Gelman2005]．

この際の共役事前分布は逆 $\chi^2$-分布である [@BDA p.396]
$$
\sigma^2_{m}\sim\chi^{-2}(\mu_m,\sigma^2_{0m})
$$
であり，$\nu_m=-1,\sigma^2_{0m}=0$ とすることで一様事前分布を得る．

> Analysis of variance (Anova) represents a key idea in statistical modeling of complex data structures. [@BDA p.395]

### 階層分散分析

$M$-元配置の分散分析において，$\sigma_1^2,\cdots,\sigma_M^2$ の $M$ 個のみを考えるのではなく，
$$
\beta^{(m)}\sim\rN_{J_m}(\al_m\b{1}_{J_m},\diag(\sigma^2_{m1},\cdots,\sigma^2_{mJ_m})),\qquad m\in[M],
$$
として $\sum_{m=1}^M J_m$ 個の分散を考えることもできる．

この際，
$$
\operatorname{Cauchy}(0,A),\qquad A\sim U(\R_+),
$$
という形の半 Cauchy 分布が $\sigma^2_{mj}$ の事前分布として用いられる [@BDA p.399]．これは一部の水準にのみ大きな分散を認め，その他の水準にはほとんど分散への寄与がないという仮定を表している．

## 一般化線型モデル

### 線型 Gauss 性からの乖離

正規線型モデルから，次の２つの自由度を追加したモデルを **一般化線型モデル** という：

::: {.callout-tip appearance="simple" icon="false"}

1. リンク関数 $g$

  $$
  g\circ\E[Y|X]=\beta^\top X
  $$

2. （正規分布以外の）分布族 $P$

  $$
  Y|X\sim P(\E[Y|X],\phi)
  $$

:::

その結果質的データ解析にも応用可能な広いクラスのモデルを得る．

::: {.callout-caution title="カウントデータに対する Poisson モデル" icon="false"}

自然数値のデータに対して，$y_i\sim\Pois(\lambda_i)$ の $\lambda_i$ を，正準リンク関数 $g=\log$ を通じて
$$
\log\lambda_i=X_i^\top\beta
$$
とモデリングすることが考えられる．このモデルを **Poisson 回帰** ともいう．

:::

::: {.callout-caution title="成功回数データに対する二項モデル" icon="false"}

２値データや成功回数を表すデータの場合，$y_i\sim\Bin(n_i,\mu_i)$ の $\mu_i$ を $\P[Y_i=1|X_i]$ の形でモデリングすることを考えることができる．

この場合，正準リンク関数は
$$
g(\mu)=\log\frac{\mu}{1-\mu}
$$
という logit 変換で与えられる．より効率的な推論を促進するために probit リンクや，非対称性を導入する log-log リンク
$$
g(\mu)=\log(-\log\mu)
$$
が用いられることもある．

:::

### 指数分布族

なお正準リンクとは，Poisson 分布族や二項分布族を指数分布族とみなした際のリンク関数のことである．

例えば二項分布族 $\{\Bin(n,\mu)\}_{\mu\in(0,1)}$ は，計数測度 $\nu$ に対して，
$$
\dd{\Bin(n,\mu)}{\nu}(x)=\comb{n}{x}\exp\paren{x\log\frac{\mu}{1-\mu}+n\log(1-\mu)}
$$
と表せる．$g(\mu)$ を **自然な十分統計量** ともいう．

指数分布族と正準リンク関数を用いた一般化線型モデリングは，パラメトリック分布族の十分統計量を代理の応答変数として線型回帰を行なっているものとみなせる．

::: {.callout-important title="変換の意味" icon="false"}

このような数理統計学的な理由とは別に，$g(\mu)$ の意味を直接解釈することもできる．

* Poisson 回帰はオフセット $\log y_i$ を作ることで，
  $$
  \log\frac{\lambda_i}{y_i}=X_i^\top\beta
  $$
  と見ることもできる．$\lambda_i/y_i$ は観測カウント $y_i$ に対する平均 $\lambda_i$ の **率比** (rate ratio) と呼ばれ，$g(\lambda_i/y_i)$ は対数率比と解釈できる．
* $\frac{\mu}{1-\mu}$ という値は成功確率 $\mu$ に対するオッズ比と呼ばれ，$g(\mu)$ は **対数オッズ比** (log odds ratio) と呼ばれる．

:::

## 終わりに {.appendix}

[@BDA] 第14章で回帰分析，15章で階層モデルが議論されている．15.6, 15.7 章で Bayesian Anova が解説されている．