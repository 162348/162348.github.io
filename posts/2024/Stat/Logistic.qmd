---
title: "大規模な不均衡データに対するロジスティック回帰"
subtitle: "離散時間 MCMC から連続時間 MCMC へ"
author: "司馬 博文"
date: 7/12/2024
date-modified: 7/13/2024
categories: [Bayesian, Computation, Python, MCMC]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
abstract-title: 概要
abstract: ロジットモデルやプロビットモデルの事後分布からのサンプリングには，その混合構造を利用したデータ拡張による Gibbs サンプラーが考案されている．しかし，このような Gibbs サンプラーは不明な理由で極めて収束が遅くなることがよく見られ，そのうちの１つのパターンが **大規模な不均衡データ** である．この記事では，この現象がなぜ起こるかに関する考察と，代替手法として Zig-Zag サンプラーがうまくいくことをみる．
code-fold: false
execute:
    cache: true
---

{{< include ../../../_preamble.qmd >}}

## ロジスティック回帰

### ロジットモデルのベイズ推定

応答変数 $Y\in\L(\Om;2)$ は，説明変数 $X\in\L(\Om;\R^p)$ の関数であるとして，係数 $\xi\in\L(\Om;\R^p)$ をロジスティック回帰モデル
$$
\P[Y=1\mi X,\xi]=g^{-1}(X^\top\xi)=\frac{\exp(X^\top\xi)}{1+\exp(X^\top\xi)}
$$ {#eq-the-model}
を通じて定める．ただし，
$$
g(x):=\log\frac{x}{1-x}
$$
は [**ロジット関数**](https://ja.wikipedia.org/wiki/ロジット)，$g^{-1}$ は **ロジスティック関数** という．^[$x$ が確率を表すとき $\frac{x}{1-x}$ という量はオッズともいい，それゆえ $p$ のロジットは対数オッズ比ともいう．]

このパラメータ $\xi$ をベイス推定することを考える．即ち，データ $\{(y^i,x^i)\}_{i=1}^n\subset2\times\R^p$ と事前分布 $p_0(\xi)d\xi\in\cP(\R^p)$ を通じて，事後分布
$$
\pi(\xi)\propt p_0(\xi)\prod_{i=1}^n\frac{\exp(y^i(x^i)^\top\xi)}{1+\exp((x^i)^\top\xi)}=e^{-U(\xi)}
$$
を計算することを考える．ただし，
$$
U(\xi):=-\log p_0(\xi)-\sum_{i=1}^n\log\paren{\frac{\exp(y^i(x^i)^\top\xi)}{1+\exp((x^i)^\top\xi)}}
$$
と定めた．

### ロジットモデルの事後分布サンプラー

ロジットリンクによる変換が複雑であるため，ロジスティック回帰は（完全な）ベイズ推定を実行することが難しいモデルとして知られてきた．

一方で，リンク関数 $g$ を標準正規分布 $\rN(0,1)$ の分布関数の逆関数に取り替えた [プロビットモデル](https://ja.wikipedia.org/wiki/プロビット) は，Gaussian data augmentation [@Albert-Chib1993] と呼ばれるデータ拡張に基づく Gibbs サンプラーが早くから提案されており，これにより効率的なベイズ推論が可能となっていた．

プロビットモデルはロジットモデルに似ており，実用上はただ裾の重さが違うのみであると言って良い [@BDA p.407]．そのこともあり，プロビットモデルのベイズ推論は計量経済学や政治科学で広く使われている手法となったが，ロジットモデルのベイズ推論の応用は遅れた [@Polson+2013]．

しかし実は，ロジットモデルの事後分布 $\pi$ も正規分布の Pólya-Gamma 混合として表すことができ，データ拡張によって効率的な Gibbs サンプラーを構成することができる [@Polson+2013]．現在ではこのデータ拡張 Gibbs サンプラーが，標準的な事後分布サンプラーとなっている．

### Gibbs サンプラーの課題：不均衡データ

データもモデルも大規模になっていく現代では，このようなデータ拡張に基づく Gibbs サンプラーは，特定の条件が揃うと極めて収束が遅くなる場面が少なくないことが明らかになってきている．

そのうちの１つのパターンが大規模な **不均衡データ** [@Johndrow+2019]，すなわち，特定のラベルが極めて稀少なカテゴリカルデータである．このようなデータに対しては，プロビットモデルやロジットモデルに限らず，ほとんど全てのデータ拡張に基づく Gibbs サンプラーが低速化することが報告されている：

> We have found that this behavior occurs routinely, essentially regardless of the type and complexity of the statistical model, if the data are large and imbalanced. [@Johndrow+2019 p.1395]

### 実験：不均衡データでの収束鈍化

ここでは問題を簡単にし，カテゴリーが２つの場合，即ち２値のスパースデータ $y^i\in2=\{0,1\}$ の場合を考える．

この下で，
$$
\sum_{i=1}^n y^i\,\bigg|\,n\sim\Bin(n,g^{-1}(\theta)),\qquad\theta\sim\rN(0,B),
$$
すなわち，モデル ([-@eq-the-model]) において
$$
p=1,\qquad X=1,\qquad p_0(\xi)d\xi=\rN(a,B),
$$
とした，説明変数なしの切片項のみでの回帰分析の場合を考える．この場合，ポテンシャルは次のように表される：
$$
-U(\xi)=\xi\sum_{i=1}^ny^i-n\log(1+e^{\xi})-\frac{(\xi-a)^2}{2B}-\frac{1}{2}\log2\pi B.
$$

ここまで単純化した設定でも，前述の Gibbs サンプラーの収束鈍化が見られることを検証する．ここでは

```{julia}
#| output: false
(a,B) = (0,100.0)
```

そして
$$
\sum_{i=1}^ny^i=1
$$
を保ちながら $n\to\infty$ として実験するが，[@Johndrow+2019] では，
$$
\sum_{i=1}^ny^i\ll n
$$
である大規模不均衡データである限り，$(a,B)$ の値に依らず同様の結果が得られることが報告されている．

::: {.callout-important title="実装の詳細" collapse="true" icon="false"}

Metropolis-Hastings 法は，Turing Institute による Julia の `AdvancedMH.jl` パッケージなどを通じて実装することができる：

```{=html}
<div class="article-card-container">
    <div class="article-card">
        <a href="https://162348.github.io/posts/2024/Julia/MALAwithJulia.html" target="_blank">
            <img src="https://162348.github.io/posts/2024/Julia/AdaptiveMCMC.svg" alt="Article Image" class="article-image">
            <div class="article-content">
                <h3 class="article-title">Metropolis-Hastings サンプラー</h3>
                <p class="article-description">Julia と Turing エコシステムを用いて</p>
            </div>
        </a>
    </div>
</div>
```

```{julia}
using AdvancedMH
using Distributions
using MCMCChains
using ForwardDiff
using StructArrays
using LinearAlgebra
using LogDensityProblems
using LogDensityProblemsAD

# Define the components of a basic model.
struct LogTargetDensity_Logistic
    a::Float64
    B::Float64
    n::Int64
end

LogDensityProblems.logdensity(p::LogTargetDensity_Logistic, ξ) = -log(2π * p.B) - (ξ[1] - p.a)^2/(2 * p.B) + ξ[1] - p.n * log(1 + exp(ξ[1]))
LogDensityProblems.dimension(p::LogTargetDensity_Logistic) = 1
LogDensityProblems.capabilities(::Type{LogTargetDensity_Logistic}) = LogDensityProblems.LogDensityOrder{0}()

function MHSampler(n::Int64; discard_initial=30000)

    model_with_ad = LogDensityProblemsAD.ADgradient(Val(:ForwardDiff), LogTargetDensity_Logistic(a, B, n))

    spl = RWMH(MvNormal(zeros(1), I))

    chain = sample(model_with_ad, spl, 50000; chain_type=Chains, param_names=["ξ"])

    return chain
end

# ξ_vector = MHSampler(10000)
# plot(ξ_vector, title="Plot of \$\\xi\$ values", xlabel="Index", ylabel="ξ", legend=false, color="#78C2AD")
```

```{julia}
using DataFrames
using Plots

n_list = [10, 100, 1000, 10000]

elapsed_time_Metropolis = @elapsed begin
    chains = [MHSampler(n) for n in n_list]
end

autos = [DataFrame(autocor(chain, lags=1:100)) for chain in chains]

combined_df = vcat(autos..., source=:chain)

lag_columns = names(combined_df)[2:101]
lags = 1:100

p_Metropolis = plot(
    title = "Metropolis",
    xlabel = "Lag",
    ylabel = "Autocorrelation",
    legend = :topright,
    background_color = "#F0F1EB"
)

for (i, n) in zip(1:4, n_list)
    plot!(
        p_Metropolis,
        lags,
        Array(combined_df[i, lag_columns]),
        label = "n = $n",
        linewidth = 2
    )
end
```

```{julia}
using PolyaGammaSamplers

function PGSampler(n::Int64; discard_initial=30000, iter_number=50000, initial_ξ=0.0, B=100)

    λ = 1 - n/2

    ξ_list = [initial_ξ]
    ω_list = []

    while length(ξ_list) < iter_number
        ξ = ξ_list[end]
        ω_sampler = PolyaGammaPSWSampler(n, ξ)
        ω_new = rand(ω_sampler)
        push!(ω_list, ω_new)
        ξ_sampler = Normal((ω_new + B^(-1))^(-1) * λ, (ω_new + B^(-1))^(-1))
        ξ_new = rand(ξ_sampler)
        push!(ξ_list, ξ_new)
    end

    return Chains(ξ_list[discard_initial+1:end])
end
```

```{julia}
elapsed_time_PolyaGamma = @elapsed begin
    chains = [PGSampler(n) for n in n_list]
end
autos = [DataFrame(autocor(chain, lags=1:100)) for chain in chains]

combined_df = vcat(autos..., source=:chain)

lag_columns = names(combined_df)[2:101]
lags = 1:100

p_PolyaGamma = plot(
    title = "Pólya-Gamma",
    xlabel = "Lag",
    ylabel = "Autocorrelation",
    legend = (0.65, 0.35),
    background_color = "#F0F1EB"
)

for (i, n) in zip(1:4, n_list)
    plot!(
        p_PolyaGamma,
        lags,
        Array(combined_df[i, lag_columns]),
        label = "n = $n",
        linewidth = 2,
    )
end
```

```{julia}
println("Elapsed time: $elapsed_time_Metropolis seconds v.s. $elapsed_time_PolyaGamma seconds")
```

PG サンプラーは MH 法に比べ恐ろしいほどに時間がかかる．これは，`Turing` のパッケージの最適化が優秀であるのか，Pólya-Gamma サンプラーの宿命であるのか，引き続き調べる必要がある．

:::

```{julia}
#| code-fold: true
plot(p_Metropolis, p_PolyaGamma, layout=(1,2), background_color = "#F0F1EB")
```

<!-- ```{julia}
savefig("Logistic.svg")
``` -->
