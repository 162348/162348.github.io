---
title: "ベイズ統計学と統計物理学"
author: "司馬 博文"
date: 6/20/2024
categories: [Bayesian, Nature, Information]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: 繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．
jupyter: julia-1.10
format:
  html:
    code-fold: false
execute:
  cache: true
---

{{< include ../../../_preamble.qmd >}}

## Bayes 推定

誤り訂正符号の文脈で Bayes （点）推定を考える．

誤り訂正符号は，Bayes 推定が自然に選好される格好の題材である．

### 設定

情報源は無記憶で，確率分布 $p(x)dx$ に従うとし，通信路は，確率核 $p(y|x)dy$ に従うとする．

送信符号は１つの値 $x_*\sim p(x)dx$ であったとし，この単一の入力 $x_*$ を $n$ 回独立に観測する：
$$
y_1,\cdots,y_n\iidsim p(y|x_*)dy.
$$

この観測を経たあと，送信符号 $x_*$ はいったい何だったのかを推定することを考えると，極めて自然に Bayes 推定が選択される．

::: {.callout-caution title="純粋な解釈を持つ Bayes 推定" collapse="true" icon="false"}

まず，何の観測もない場合，$x_*$ の確率分布は $p(x)dx$ である（設定上）．

しかし，すでに観測 $y_1,\cdots,y_n$ を経ている．よってこの事象の上での $x_*$ の条件付き分布を計算すれば良いことになる：
\begin{align*}
    p(x|y_1,\cdots,y_n) &= \frac{p(\b{y}|x)p(x)}{p(\b{y})}\\
    &= \frac{\displaystyle p(y_1|x)\cdots p(y_n|x)p(x)}{\displaystyle\int_\R p(y_1|x)\cdots p(y_n|x)p(x)\,dx}
\end{align*}

こうして，誤り訂正符号の文脈では，Bayes 事前確率・事後確率が純粋に確率としての解釈を持つ．

:::

### 例 {#sec-example}

入力は $1/2$ の確率で $x=0$ だが，もう $1/2$ の確率で標準正規分布 $\rN(0,1)$ に従うとする：
$$
p(x)dx=\frac{1}{2}\delta_0(dx)+\frac{1}{2}\rN(0,1)(dx).
$$
通信路は加法的 Gauss 型であるとする：
$$
p(x,y)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(x-y)^2}{2}\right).
$$
では真値を $x_*=1$ として，$n=10$ 回の観測データを生成してみる：

```{julia}
data = 1 .+ randn(10)
print(data)
```

愚直な計算から，$p(\b{y})$ は次の積分で計算される：
\begin{align*}
    p(\b{y})&=\int_\R p(\b{y}|x)p(x)dx\\
    &=\int_\R \prod_{i=1}^n p(y_i|x)p(x)dx\\
    &=\frac{1}{2}\frac{1}{(2\pi)^{n+1}}\int_\R\exp\paren{-\frac{1}{2}\sum_{i=1}^n(y_i-x)^2+x^2}dx\\
    &\qquad+\frac{1}{2}\frac{1}{(2\pi)^n}\exp\paren{-\frac{1}{2}\sum_{i=1}^n(y_i-x)^2}dx.
\end{align*}

これを実際に計算した結果は [@Krzakala-Zdeborova2024 p.117] で与えられている．

ここでは，邪教のような計算を啓示するのではなく，Julia と Turing を通じて計算する方法を紹介する [@Storopoli2021]．

```{julia}
#| output: false
using Turing, Distributions

data = [0.99978688, 1.7956116, −0.43158072, 3.07234211, 1.11920946, 0.53248943, 0.80011329, −0.52783428, 0.40378413, −0.00223177]

prior = MixtureModel([Normal(0.0, 1.0), Normal(0.0, 0.005)], [0.5, 0.5])

@model function mixed_normal_model(data)
    μ ~ prior
    for i in 1:length(data)
        data[i] ~ Normal(μ, 1)
    end
end

# モデルのインスタンス化
model = mixed_normal_model(data)
```

```{julia}
#| output: false
chain = sample(model, NUTS(), 100000)
```

```{julia}
using MCMCChains, Plots, StatsPlots

plot(chain[:μ], seriestype = :density, xlabel = "x*", ylabel = "Density", title = "Posterior Distribution", color = "#80c4ac")

vline!([1], color = :pink, linewidth = 2, label = "x_* = 1")
```

::: {.callout-caution title="注" collapse="true" icon="false"}

$x_*=0$ に原子が存在する．

ここでは密度関数は滑らかに見えるが，実際には $x_*=0$ で不連続である．

これは，計算の都合上，$\delta_0$ を $\rN(0,0.005)$ で代用したためである．

:::

### MAP 推定量

::: {.callout-tip title="定義 (maximum a posterior estimator)" icon="false"}
$$
\wh{x}:=\argmax_{x\in\R}p(x|y_1,\cdots,y_n)
$$
で定まる推定量 $\R^n\to\R$ を **MAP 推定量** という．
:::

例 [-@sec-example] などをはじめ，ほとんどの場面では良い推定量を与え，多くの場合の最初のチョイスとして適しているかもしれない．

しかし，例 [-@sec-example] をさらに変形することで，次のような事後分布が得られる状況は容易に想像がつく：



## 統計物理からの視点