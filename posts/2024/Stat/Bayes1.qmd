---
title: "ベイズ統計学と統計物理学"
author: "司馬 博文"
date: 6/20/2024
categories: [Bayesian, Nature, Information]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: 繰り返し起こる事象の待ち時間をモデル化した Markov 連鎖・過程を例として，Markov 連鎖のエルゴード性に関連する概念を概観する．特に，収束レートと中心極限定理がいつ成り立つかを議論する．
format:
    html:
        code-fold: false
execute:
    cache: true
---

{{< include ../../../_preamble.qmd >}}

## Bayes 推定

誤り訂正符号の文脈で Bayes （点）推定を考える．

誤り訂正符号は，Bayes 推定が自然に選好される格好の題材である．

### 設定

情報源は無記憶で，確率分布 $p(x)dx$ に従うとし，通信路は，確率核 $p(y|x)dy$ に従うとする．

送信符号は１つの値 $x_*\sim p(x)dx$ であったとし，この単一の入力 $x_*$ を $n$ 回独立に観測する：
$$
y_1,\cdots,y_n\iidsim p(y|x_*)dy.
$$

この観測を経たあと，送信符号 $x_*$ はいったい何だったのかを推定することを考えると，極めて自然に Bayes 推定が選択される．

::: {.callout-caution title="純粋な解釈を持つ Bayes 推定" collapse="true" icon="false"}

まず，何の観測もない場合，$x_*$ の確率分布は $p(x)dx$ である（設定上）．

しかし，すでに観測 $y_1,\cdots,y_n$ を経ている．よってこの事象の上での $x_*$ の条件付き分布を計算すれば良いことになる：
$$
p(x|y_1,\cdots,y_n) = \frac{p(\b{y}|x)p(x)}{p(\b{y})}
$$
$$
= \frac{\displaystyle p(y_1|x)\cdots p(y_n|x)p(x)}{\displaystyle\int_\R p(y_1|x)\cdots p(y_n|x)p(x)\,dx}
$$ {#eq-bayes}

こうして，誤り訂正符号の文脈では，Bayes 事前確率・事後確率が純粋に確率としての解釈を持つ．

:::

### 例 {#sec-example}

入力は $1/2$ の確率で $x=0$ だが，もう $1/2$ の確率で標準正規分布 $\rN(0,1)$ に従うとする：
$$
p(x)dx=\frac{1}{2}\delta_0(dx)+\frac{1}{2}\rN(0,1)(dx).
$$
通信路は加法的 Gauss 型であるとする：
$$
p(x,y)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(x-y)^2}{2}\right).
$$
では真値を $x_*=1$ として，$n=10$ 回の観測データを生成してみる：

```{julia}
data = 1 .+ randn(10)
print(data)
```

愚直な計算から，$p(\b{y})$ は次の積分で計算される：
\begin{align*}
    p(\b{y})&=\int_\R p(\b{y}|x)p(x)dx\\
    &=\int_\R \prod_{i=1}^n p(y_i|x)p(x)dx\\
    &=\frac{1}{2}\frac{1}{(2\pi)^{n+1}}\int_\R\exp\paren{-\frac{1}{2}\sum_{i=1}^n(y_i-x)^2+x^2}dx\\
    &\qquad+\frac{1}{2}\frac{1}{(2\pi)^n}\exp\paren{-\frac{1}{2}\sum_{i=1}^n(y_i-x)^2}dx.
\end{align*}

これを実際に計算した結果は [@Krzakala-Zdeborova2024 p.117] で与えられている．

ここでは，邪教のような計算を啓示するのではなく，Julia と Turing を通じて計算する方法を紹介する [@Storopoli2021]．

```{julia}
#| output: false
using Turing, Distributions

data = [0.99978688, 1.7956116, −0.43158072, 3.07234211, 1.11920946, 0.53248943, 0.80011329, −0.52783428, 0.40378413, −0.00223177]

prior = MixtureModel([Normal(0.0, 1.0), Normal(0.0, 0.005)], [0.5, 0.5])

@model function mixed_normal_model(data)
    μ ~ prior
    for i in 1:length(data)
        data[i] ~ Normal(μ, 1)
    end
end

model = mixed_normal_model(data)
```

```{julia}
#| output: false
chain = sample(model, NUTS(), 100000)
```

```{julia}
using MCMCChains, Plots, StatsPlots

plot(chain[:μ], seriestype = :density, xlabel = "x*", ylabel = "Density", title = "Posterior Distribution", color = "#80c4ac")

vline!([1], color = :pink, linewidth = 2, label = "x_* = 1")
```

::: {.callout-caution title="注（$x_*=0$ でのアトムについて）" collapse="true" icon="false"}

$x_*=0$ に原子が存在する．

ここでは密度関数は滑らかに見えるが，実際には $x_*=0$ で不連続である．

これは，計算の都合上，$\delta_0$ を $\rN(0,0.005)$ で代用したためである．

:::

### MAP 推定量

::: {.callout-tip title="定義 (maximum a posterior estimator)" icon="false"}
$$
\wh{x}:=\argmax_{x\in\R}p(x|y_1,\cdots,y_n)
$$
で定まる推定量 $\R^n\to\R$ を **MAP 推定量** という．
:::

例 [-@sec-example] などをはじめ，ほとんどの場面では良い推定量を与え，多くの場合の最初のチョイスとして適しているかもしれない．

しかし，例 [-@sec-example] をさらに変形することで，次のような事後分布が得られる状況は容易に想像がつく：

```{julia}
#| code-fold: true
#| label: fig-1
function f(x)
    if -1.8 < x < -1.6
        return 2.5
    elseif 0 < x < 1.5
        return 1
    else 
        return 0
    end
end

plot(f, -2, 2, size=(600, 400), legend=false, color="#80c4ac")
```

### ベイズ最適推定量

@fig-1 の状況でも，たしかに１点のみを選ぶならば MAP 推定量で良いかもしれないが，$x\le0$ である確率は $x\ge0$ である確率よりも低いため，この意味では，$x\in[0,3/2]$ の範囲に推定量が収まっていた方が好ましいかもしれない．

推定量 $\wh{x}_n:\R^n\to\R$ を評価するには，何らかのアプリオリな **損失** の概念が必要である．これを損失関数 $L:\R^2\to\R$ という形で与える．

すると，損失の期待値が計算可能になり，これを **危険** という：
$$
R(\wh{x}_n):=\E[L(\wh{x}_n(\b{Y}),X)].
$$

このリスクを最小化する推定量を **ベイズ最適推定量**，その際のリスクを **ベイズリスク** という．^[[@Krzakala-Zdeborova2024 p.119]．]

#### $l^2$-ベイズ最適推定量

::: {.callout-tip title="命題" icon="false"}
$L(x,y):=\norm{x-y}_2$ と定める．このとき，**事後平均** がベイズリスクを達成する：
$$
\wh{x}(\b{y}):=\int_\R xp(x|\b{y})\,
dx.
$$
:::
::: {.callout-note title="証明" collapse="true" icon="false"}

:::

@fig-1 の場合では，事後平均は約 $0.1$ で，かろうじて正になる．

#### $l^1$-ベイズ最適推定量

::: {.callout-tip title="命題" icon="false"}
$L(x,y):=\abs{x-y}=\norm{x-y}_1$ と定める．このとき，**事後中央値** がベイズリスクを達成する．
:::
::: {.callout-note title="証明" collapse="true" icon="false"}

:::

@fig-1 の場合では，事後中央値は $0.5$ となる．大変頑健な推定だと言えるだろう．

#### 最適決定規則

今回の誤り訂正符号の文脈の目標は，$x_*$ の復元であることを思い出せば，今回の損失は $L(x,y)=\delta_0(x-y)$ ととり，復号が成功する確率を最大とする推定量が「最良」と言うべきであろう：
$$
R(\wh{x}_n)=\P[\wh{x}_n(\b{Y})=X]
$$

これは結局 MAP 推定量
$$
\wh{x}_n:=\argmax_{x\in\R}p(x|\b{y})
$$
で与えられることになる．

## 統計物理からの視点

### 事後分布をある物理系の平衡分布と見る

ベイズの公式 ([-@eq-bayes]) は
$$
p(x|\b{y})=\frac{e^{\log p(\b{y}|x)p(x)}}{p(\b{y})}
$$
$$
=\frac{e^{-H(x,\b{y})}}{Z(\b{y})},
$$
$$
H(x,\b{y}):=-\log p(\b{y}|x)-\log p(x),\quad Z(\b{y}):=p(y)
$$
と書き換えられる．

すなわち，Bayes 事後分布 $p(x|\b{y})$ は，$\R\times\R^n$ を配位空間に持ち，Hamiltonian $H(x,\b{y})$ を持つ正準集団の平衡分布と捉えることができる．

### もう一つの見方

今回，通信路は加法的に Gauss ノイズを加えるものとした．
$$
p(x,y)dy=\rN(x,\sigma)(dy)
$$
とした場合，次のように理解することもできる：
$$
p(x|\b{y})=\frac{e^{-H(x,\b{y})}}{Z(\b{y})},
$$
$$
H(x,\b{y}):=\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)-\log p(x),
$$
$$
Z(\b{y}):=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\frac{e^{-\frac{\abs{\b{y}}^2}{2\sigma^2}}}{p(\b{y})}.
$$

この場合，分配関数 $Z(\b{y})$ は，情報源 $p(x)$ と Gauss 型通信路 $p(y|x)$ で与えられた周辺モデル $p(y)$ と，$\rN(0,\sigma^2)$ との [**ベイズ因子**](https://ja.wikipedia.org/wiki/ベイズ因子) になっている．

::: {.callout-note title="式変形" collapse="true" icon="false"}
\begin{align*}
    p(x|\b{y})&=p(\b{y}|x)\frac{p(x)}{p(\b{y})}\\
    &=\frac{p(x)}{p(\b{y})}\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x-y_i)^2}\\
    &=\frac{p(x)}{p(\b{y})}\frac{e^{-\frac{1}{2\sigma^2}\sum_{i=1}^ny_i^2}}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)}\\
    &=\frac{p(x)e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)}}{Z(\b{y})}\\
    &=\frac{1}{Z(\b{y})}\exp\paren{\log p(x)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)}
\end{align*}
:::

さらに，このモデル $H$ における自由エントロピーは，$p(\b{y})$ と $\rN(0,\sigma^2)^{\otimes n}$ との間の KL 乖離度となっている：
$$
F_n:=\int_{\R^n}\log\frac{p(\b{y})}{q(\b{y})}p(\b{y})\,d\b{y}=\KL(p,q).
$$
ただし，$q$ は $\rN(0,\sigma^2)^{\otimes n}$ の密度とした．

::: {.callout-tip title="命題（その他の物理量）^[[@Krzakala-Zdeborova2024 p.122]．]" icon="false"}

この Hamiltonian $H$ を持つ系について，

1. エントロピー $H$ は次で与えられる：

    \begin{align*}
        H(Y)&:=-\int_{\R^n}(\log p(\b{y}))p(\b{y})\,d\b{y}\\
        &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\int_{\R^n}\abs{\b{y}}^2p(\b{y})\,d\b{y}-F_n.
    \end{align*}

2. 相互情報量 $I$ は次で与えられる：

    \begin{align*}
        I(X,Y)&=\KL(p(x,y),p(x)p(y))\\\\
        &=\frac{n}{2\sigma^2}\int_{\R}x^2p(x)\,dx.
    \end{align*}

特に，いずれも自由エネルギーの定数倍である

:::

::: {.callout-note title="証明" collapse="true" icon="false"}

1. の式変形は次のとおり：

\begin{align*}
    H(Y)&:=-\int_{\R^n}(\log p(\b{y}))p(\b{y})\,d\b{y}\\
    &=-\int_{\R^n}p(\b{y})d\b{y}\log q(\b{y})-F_n\\
    &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\int_{\R^n}\abs{\b{y}}^2p(\b{y})\,d\b{y}-F_n.
\end{align*}

2. 次の関係式を用いる：
$$
I(X,Y)=H(Y)-H(Y|X)
$$
これより，
\begin{align*}
    I(X,Y)&=H(Y)-\frac{n}{2}\log(2\pi e\sigma^2)
\end{align*}

:::