---
title: "ベイズ統計学と統計物理学"
subtitle: "スパース符号の復元を題材として"
author: "司馬 博文"
date: 6/20/2024
categories: [Bayesian, Nature, Information]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: ノイズ付きで観測された情報を復元するデノイジング問題を，ベイズ統計学と統計力学の２つの観点から考える．ベイズ統計モデルはスピングラスモデルと同一視することができる．このことからも，一般に事後分布からのサンプリングが困難になりえることがよくわかる．
format:
    html:
        code-fold: false
execute:
    cache: true
---

{{< include ../../../_preamble.qmd >}}

## Bayes 推定

誤り訂正符号の文脈で Bayes （点）推定を考える．

誤り訂正符号は，Bayes 推定が自然に選好される格好の題材である．

### 設定

情報源は無記憶で，確率分布 $p(x)dx$ に従うとし，通信路は，確率核 $p(y|x)dy$ に従うとする．

送信符号は１つの値 $x_*\sim p(x)dx$ であったとし，この単一の入力 $x_*$ を $n$ 回独立に観測する：
$$
y_1,\cdots,y_n\iidsim p(y|x_*)dy.
$$

この観測を経たあと，送信符号 $x_*$ はいったい何だったのかを推定することを考えると，極めて自然に Bayes 推定が選択される．

::: {.callout-caution title="純粋な解釈を持つ Bayes 推定" collapse="true" icon="false"}

まず，何の観測もない場合，$x_*$ の確率分布は $p(x)dx$ である（設定上）．

しかし，すでに観測 $y_1,\cdots,y_n$ を経ている．よってこの事象の上での $x_*$ の条件付き分布を計算すれば良いことになる：
$$
p(x|y_1,\cdots,y_n) = \frac{p(\b{y}|x)p(x)}{p(\b{y})}
$$
$$
= \frac{\displaystyle p(y_1|x)\cdots p(y_n|x)p(x)}{\displaystyle\int_\R p(y_1|x)\cdots p(y_n|x)p(x)\,dx}
$$ {#eq-bayes}

こうして，誤り訂正符号の文脈では，Bayes 事前確率・事後確率が純粋に確率としての解釈を持つ．

:::

### 例 {#sec-example}

入力は $1/2$ の確率で $x=0$ だが，もう $1/2$ の確率で標準正規分布 $\rN(0,1)$ に従うとする：
$$
p(x)dx=\frac{1}{2}\delta_0(dx)+\frac{1}{2}\rN(0,1)(dx).
$$
通信路は加法的 Gauss 型であるとする：
$$
p(x,y)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(x-y)^2}{2}\right).
$$
では真値を $x_*=1$ として，$n=10$ 回の観測データを生成してみる：

```{julia}
data = 1 .+ randn(10)
print(data)
```

愚直な計算から，$p(\b{y})$ は次の積分で計算される：
\begin{align*}
    p(\b{y})&=\int_\R p(\b{y}|x)p(x)dx\\
    &=\int_\R \prod_{i=1}^n p(y_i|x)p(x)dx\\
    &=\frac{1}{2}\frac{1}{(2\pi)^{n+1}}\int_\R\exp\paren{-\frac{1}{2}\sum_{i=1}^n(y_i-x)^2+x^2}dx\\
    &\qquad+\frac{1}{2}\frac{1}{(2\pi)^n}\exp\paren{-\frac{1}{2}\sum_{i=1}^n(y_i-x)^2}dx.
\end{align*}

これを実際に計算した結果は [@Krzakala-Zdeborova2024 p.117] で与えられている．

ここでは，邪教のような計算を啓示するのではなく，Julia と Turing を通じて計算する方法を紹介する [@Storopoli2021]．

```{julia}
#| output: false
using Turing, Distributions

data = [0.99978688, 1.7956116, −0.43158072, 3.07234211, 1.11920946, 0.53248943, 0.80011329, −0.52783428, 0.40378413, −0.00223177]

prior = MixtureModel([Normal(0.0, 1.0), Normal(0.0, 0.005)], [0.5, 0.5])

@model function mixed_normal_model(data)
    μ ~ prior
    for i in 1:length(data)
        data[i] ~ Normal(μ, 1)
    end
end

model = mixed_normal_model(data)
```

```{julia}
#| output: false
chain = sample(model, NUTS(), 100000)
```

```{julia}
using MCMCChains, Plots, StatsPlots

plot(chain[:μ], seriestype = :density, xlabel = "x*", ylabel = "Density", title = "Posterior Distribution", color = "#80c4ac")

vline!([1], color = :pink, linewidth = 2, label = "x_* = 1")
```

::: {.callout-caution title="注（$x_*=0$ でのアトムについて）" collapse="true" icon="false"}

$x_*=0$ に原子が存在する．

ここでは密度関数は滑らかに見えるが，実際には $x_*=0$ で不連続である．

これは，計算の都合上，$\delta_0$ を $\rN(0,0.005)$ で代用したためである．

:::

### MAP 推定量

::: {.callout-tip title="定義 (maximum a posterior estimator)" icon="false"}
$$
\wh{x}:=\argmax_{x\in\R}p(x|y_1,\cdots,y_n)
$$
で定まる推定量 $\R^n\to\R$ を **MAP 推定量** という．
:::

例 [-@sec-example] などをはじめ，ほとんどの場面では良い推定量を与え，多くの場合の最初のチョイスとして適しているかもしれない．

しかし，例 [-@sec-example] をさらに変形することで，次のような事後分布が得られる状況は容易に想像がつく：

```{julia}
#| code-fold: true
#| label: fig-1
function f(x)
    if -1.8 < x < -1.6
        return 2.5
    elseif 0 < x < 1.5
        return 1
    else 
        return 0
    end
end

plot(f, -2, 2, size=(600, 400), legend=false, color="#80c4ac")
```

### ベイズ最適推定量

@fig-1 の状況でも，たしかに１点のみを選ぶならば MAP 推定量で良いかもしれないが，$x\le0$ である確率は $x\ge0$ である確率よりも低いため，この意味では，$x\in[0,3/2]$ の範囲に推定量が収まっていた方が好ましいかもしれない．

推定量 $\wh{x}_n:\R^n\to\R$ を評価するには，何らかのアプリオリな **損失** の概念が必要である．これを損失関数 $L:\R^2\to\R$ という形で与える．

すると，損失の期待値が計算可能になり，これを **危険** という：
$$
R(\wh{x}_n):=\E[L(\wh{x}_n(\b{Y}),X)].
$$

このリスクを最小化する推定量を **ベイズ最適推定量**，その際のリスクを **ベイズリスク** という．^[[@Krzakala-Zdeborova2024 p.119]．]

#### $l^2$-ベイズ最適推定量 {#sec-MMSE}

::: {.callout-tip title="命題" icon="false"}
$L(x,y):=(x-y)^2$ と定める．このとき，**事後平均** がベイズリスクを達成する：
$$
\wh{x}(\b{y}):=\int_\R xp(x|\b{y})\,
dx.
$$
:::
::: {.callout-note title="証明" collapse="true" icon="false"}

:::

@fig-1 の場合では，事後平均は約 $0.1$ で，かろうじて正になる．

#### $l^1$-ベイズ最適推定量

::: {.callout-tip title="命題" icon="false"}
$L(x,y):=\abs{x-y}$ と定める．このとき，**事後中央値** がベイズリスクを達成する．
:::
::: {.callout-note title="証明" collapse="true" icon="false"}

:::

@fig-1 の場合では，事後中央値は $0.5$ となる．大変頑健な推定だと言えるだろう．

#### 最適決定規則

今回の誤り訂正符号の文脈の目標は，$x_*$ の復元であることを思い出せば，今回の損失は $L(x,y)=\delta_0(x-y)$ ととり，復号が成功する確率を最大とする推定量が「最良」と言うべきであろう：
$$
R(\wh{x}_n)=\P[\wh{x}_n(\b{Y})=X]
$$

これは結局 MAP 推定量
$$
\wh{x}_n:=\argmax_{x\in\R}p(x|\b{y})
$$
で与えられることになる．

## 統計物理からの視点

真のシグナル $x^*\in\R$ を，事後平均によって点推定する問題を考える（次節で一般次元のベクトル $\R^n$ のスパース推定を考える．

### 事後分布をある物理系の平衡分布と見る

ベイズの公式 ([-@eq-bayes]) は
$$
p(x|\b{y})=\frac{e^{\log p(\b{y}|x)p(x)}}{p(\b{y})}
$$
$$
=\frac{e^{-H(x,\b{y})}}{Z(\b{y})},
$$
$$
H(x,\b{y}):=-\log p(\b{y}|x)-\log p(x),\quad Z(\b{y}):=p(y)
$$
と書き換えられる．

すなわち，Bayes 事後分布 $p(x|\b{y})$ は，$\R\times\R^n$ を配位空間に持ち，Hamiltonian $H(x,\b{y})$ を持つ正準集団の平衡分布と捉えることができる．

### もう一つの見方 {#sec-identification}

今回，通信路は加法的に Gauss ノイズを加えるものとした．
$$
p(x,y)dy=\rN(x,\sigma)(dy)
$$
とした場合，次のように理解することもできる：
$$
p(x|\b{y})=\frac{e^{-H(x,\b{y})}}{Z(\b{y})},
$$
$$
H(x,\b{y}):=\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)-\log p(x),
$$
$$
Z(\b{y}):=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\frac{e^{-\frac{\abs{\b{y}}^2}{2\sigma^2}}}{p(\b{y})}.
$$

この場合，分配関数 $Z(\b{y})$ は，情報源 $p(x)$ と Gauss 型通信路 $p(y|x)$ で与えられた周辺モデル $p(y)$ と，$\rN(0,\sigma^2)$ との [**ベイズ因子**](https://ja.wikipedia.org/wiki/ベイズ因子) になっている．

::: {.callout-note title="式変形" collapse="true" icon="false"}
\begin{align*}
    p(x|\b{y})&=p(\b{y}|x)\frac{p(x)}{p(\b{y})}\\
    &=\frac{p(x)}{p(\b{y})}\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x-y_i)^2}\\
    &=\frac{p(x)}{p(\b{y})}\frac{e^{-\frac{1}{2\sigma^2}\sum_{i=1}^ny_i^2}}{(2\pi\sigma^2)^{\frac{n}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)}\\
    &=\frac{p(x)e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)}}{Z(\b{y})}\\
    &=\frac{1}{Z(\b{y})}\exp\paren{\log p(x)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x^2-2xy_i)}
\end{align*}
:::

さらに，このモデル $H$ における自由エントロピーは，$p(\b{y})$ と $\rN(0,\sigma^2)^{\otimes n}$ との間の KL 乖離度となっている：
$$
F_n:=\int_{\R^n}\log\frac{p(\b{y})}{q(\b{y})}p(\b{y})\,d\b{y}=\KL(p,q).
$$
ただし，$q$ は $\rN(0,\sigma^2)^{\otimes n}$ の密度とした．

::: {.callout-tip title="命題（その他の物理量）^[[@Krzakala-Zdeborova2024 p.122]．]" icon="false"}

この Hamiltonian $H$ を持つ系について，

1. エントロピー $H$ は次で与えられる：

    \begin{align*}
        H(Y)&:=-\int_{\R^n}(\log p(\b{y}))p(\b{y})\,d\b{y}\\
        &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\int_{\R^n}\abs{\b{y}}^2p(\b{y})\,d\b{y}-F_n.
    \end{align*}

2. 相互情報量 $I$ は次で与えられる：

    \begin{align*}
        I(X,Y)&:=\KL(p(x,y),p(x)p(y))\\\\
        &=\frac{n}{2\sigma^2}\int_{\R}x^2p(x)\,dx-F_n.
    \end{align*}

特に，いずれも自由エネルギーの定数倍である

:::

::: {.callout-note title="証明" collapse="true" icon="false"}

1. の式変形は次のとおり：
   \begin{align*}
       H(Y)&:=-\int_{\R^n}(\log p(\b{y}))p(\b{y})\,d\b{y}\\
       &=-\int_{\R^n}p(\b{y})d\b{y}\log q(\b{y})-F_n\\
       &=-\int_{\R^n}\Paren{-\frac{n}{2}\log(2\pi\sigma^2)-\frac{\abs{\b{y}}^2}{2\sigma^2}}p(\b{y})d\b{y}-F_n\\
       &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{n}{2\sigma^2}\int_{\R}y_i^2p(y_i)\,dy_i-F_n.
   \end{align*}
2. 次の関係式を用いる：
   $$
   I(X,Y)=H(Y)-H(Y|X)
   $$
   $H(Y)$ は 1. から判明しており，$H(Y|X)$ は次のように計算できる：

   \begin{align*}
    H(Y|X)&=-\int_{\R^{n+1}}\log p(\b{y}|x)p(\b{y}|x)p(x)\,dxd\b{y}\\
    &=-\int_{\R^{n+1}}\log\paren{\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\exp\paren{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-x)^2}}p(\b{y}|x)p(x)\,dxd\b{y}\\
    &=\int_{\R^{n+1}}\paren{\frac{n}{2}\log(2\pi\sigma^2)+\frac{\sum_{i=1}^n(x-y_i)^2}{2\sigma^2}}p(\b{y}|x)p(x)\,dxd\b{y}\\
    &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{1}{2\sigma^2}\int_\R\paren{\int_{\R^n}\sum_{i=1}^n(x-y_i)^2p(\b{y}|x)\,d\b{y}}\,p(x)dx\\
    &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{n\sigma^2}{2\sigma^2}\int_\R p(x)\,dx\\
    &=\frac{n}{2}\log(2\pi\sigma^2)+\frac{n}{2}.
   \end{align*}

これと，$y_i$ には $x_i$ とこれと独立な分散 $\sigma^2$ のノイズが加わって得られる値であることから，次のように計算できる：
\begin{align*}
    I(X,Y)&=H(Y)-\frac{n}{2}\log(2\pi\sigma^2)-\frac{n}{2}\\
    &=-F_n-\frac{n}{2}+\frac{n}{2\sigma^2}\int_\R y_i^2p(y_i)\,dy_i\\
    &=-F_n-\frac{n}{2}+\frac{n}{2\sigma^2}\paren{\sigma^2+\int_\R x^2p(x)\,dx}\\
    &=-F_n+\frac{n}{2\sigma^2}\int_\R x^2p(x)\,dx.
\end{align*}

:::

### Stein の補題

::: {.callout-tip title="命題 [@Stein1972]" icon="false"}

可積分な確率変数 $X\in\L^1(\Om)$ について，次は同値：

1. $X$ の分布は標準Gaussである：$X\sim\gamma_1$．
2. 任意の $f\in C^1_b(\R)$ について，
$$
\E[f'(X)]=\E[Xf(X)]<\infty.
$$

:::

::: {.callout-note title="証明" collapse="true" icon="false"}
* (1)$\Rightarrow$(2)

    $f,f'$ はいずれも有界としたから，
    $$
    \E[f'(X)],\E[f(X)]<\infty
    $$
    が成り立つ．

    $\gamma_1$ の密度 $\phi$ は $\phi'(x)=-x\phi(x)$ を満たすことに注意すれば，部分積分により，
    \begin{align*}
        \E[f'(X)]&=-\int_\R f(x)\phi'(x)dx\\
        &=\int_\R f(x)x\phi(x)dx=\E[f(X)X].
    \end{align*}

* (2)$\Rightarrow$(1)

    $X$ は可積分としたから，特性関数 $\varphi(u):=\E[e^{iuX}]$ は少なくとも $C^1(\R)$-級で，その微分は $\phi(x):=e^{iux}$ に関する仮定 $\E[\phi'(X)]=\E[X\phi(X)]$ を通じて
    \begin{align*}
        \varphi'(u)&=i\E[Xe^{iuX}]=-u\E[e^{iuX}]\\
        &=-u\varphi(u),\qquad u\in\R,
    \end{align*}
    と計算できる．
    
    この微分方程式は規格化条件 $\varphi(0)=1$ の下で一意な解 $\varphi(u)=e^{-\frac{u^2}{2}}$ を持つ．
:::

### 西森対称性

::: {.callout-tip title="命題 [@西森1980]^[[@Krzakala-Zdeborova2024 p.123] 定理13．]" icon="false"}

$P:\R\to\R^n$ を確率核，$X^*\in\L(\Om)$ は分布 $\nu\in\cP(\R)$ を持ち，$Y\in\L(\Om;\R^n)$ の分布は
$$
\mu(dy)=\int_\R\nu(dx)P(x,dy)
$$
で定まるとする．ここで，$Y$ で条件づけた $X^*$ の確率分布を $P^{X|Y}$ とする：
$$
\nu(dx)=\int_{\R^n}\mu(dy)P^{X|Y}(y,dx).
$$
$$
X^{(1)},\cdots,X^{(k)}\iidsim P^{X|Y}
$$
を独立な確率変数列とすると，次が成り立つ：
$$
\E\SQuare{f(Y,X^{(1)},\cdots,X^{(k)})}=\E\Square{f(Y,X^{(1)},\cdots,X^{(k-1)},X^*)}.
$$

:::

$X^{(1)},\cdots,X^{(k)}$ で表した確率変数の $P^{X|Y}$ に関する積分を Boltzmann 積分と呼び，観測を作り出す過程 $(X^*,Y)$ に関する積分を無秩序積分 (disorder expectation) という．

::: {.callout-note title="証明" icon="false" collapse="true"}

この設定の下で，$(X^*,Y)$ の結合分布が次の２通りで表せていることに注意：
$$
\nu(dx)P(x,dy)=\mu(dy)P^{X|Y}(y,dx).
$$
従って，
\begin{align*}
    &\quad\nu(dx)P(x,dy)P^{X|Y}(y,dx^{(1)},\cdots,dx^{(k)})\\
    &=\mu(dy)P^{X|Y}(y,dx)P^{X|Y}(y,dx^{(1)},\cdots,dx^{(k)})
\end{align*}
に関して $f(Y,X^{(1)},\cdots,X^{(k)})$ の期待値を取ると，次のように計算できる：
\begin{align*}
    &\quad\E\SQuare{f(Y,X^{(1)},\cdots,X^{(k)})}\\
    &=\int_{\R^{n+k}}f(y,x^{(1)},\cdots ,x^{(k-1)},x^{(k)})P^{X|Y}(y,dx^{(1)})\cdots P^{X|Y}(y,dx^{(k)})\mu(dy)\\
    &=\int_{\R^{n+k}}f(y,x^{(1)},\cdots,x^{(k-1)},x)P^{X|Y}(y,dx^{(1)})\cdots P^{X|Y}(y,dx^{(k-1)})\nu(dx)P(x,dy)\\
    &=\E\Square{f(Y,X^{(1)},\cdots,X^{(k-1)},X^*)}.
\end{align*}

確率核 $P$ にまつわる記法は次の記事も参照：
```{=html}
<div class="article-card-container">
    <div class="article-card">
        <a href="https://162348.github.io/static/Notations.html" target="_blank">
            <img src="https://162348.github.io/static/Notations.svg" alt="Article Image" class="article-image">
            <div class="article-content">
                <h3 class="article-title">数学記法一覧</h3>
                <p class="article-description">本サイトで用いる記法と規約</p>
            </div>
        </a>
    </div>
</div>
```
:::

::: {.callout-important title="物理的な解釈" collapse="true" icon="false"}

$P^{X|Y}$ に関する積分を $\brac{-}$ で表すことで，何をどのように物理的に解釈しているかが明確になる：
$$
\brac{X}=\E[X|Y].
$$

この見方を採用すると，期待値を
$$
\E\SQuare{f(Y,X^{(1)},\cdots,X^{(k)})}=\E^Y\Square{\Brac{f(Y,X^{(1)},\cdots,X^{(k)})}}
$$
と二段階で捉えていることになる．右辺の外側の期待値は単に $Y$ のみに関してとっていることになる．

第 [-@sec-identification] 節で考えたモデル $H$ における Boltzmann 分布が $P^{X|Y}$ となり，平均 $\brac{-}$ はこれに関する平均となる．

一方で，Hamiltonian $H$ にもランダム性が残っているのであり，これに関する平均が $(X^*,Y)$ に関する平均に当たる．

こうして，ベイズ統計モデルはスピングラス系（特にランダムエネルギーモデル [@Derrida1980]）と同一視できるようになる．

だが同時に，スピングラスのサンプリングを困難にする多谷構造も，ベイズ統計に輸入されるのである……．

スピングラスについては，次の記事も参照：

```{=html}
<div class="article-card-container">
    <div class="article-card">
        <a href="https://162348.github.io/posts/2024/Nature/StatisticalMechanics1.html" target="_blank">
            <img src="https://162348.github.io/posts/2024/Nature/SG.png" alt="Article Image" class="article-image">
            <div class="article-content">
                <h3 class="article-title">数学者のための統計力学１</h3>
                <p class="article-description">Ising 模型とスピングラス</p>
            </div>
        </a>
    </div>
</div>
```

:::

### 最小自乗誤差推定量 {#sec-MMSE-via-Nishimori}

第 [-@sec-MMSE] 節で扱った最小自乗誤差推定量の自乗誤差は次のように計算できる：

::: {.callout-tip title="命題（最小自乗誤差の表示）" icon="false"}

事後平均推定量の自乗誤差は次のように表せる：
\begin{align*}
    \DeclareMathOperator{\MMSE}{MMSE}
    \MMSE&:=\E\SQuare{\Paren{X-\E[X|Y]}^2}\\
    &=\E[X^2]-\E\SQuare{\E[X|Y]^2}.
\end{align*}

:::

::: {.callout-note title="証明" collapse="true" icon="false"}

\begin{align*}
    \E[\V[X|Y]]&=\E\SQuare{\Paren{\E[X|Y]-X}^2}\\
    &=\E\SQuare{\E[X|Y]^2-2X\E[X|Y]+X^2}\\
    &=\E\SQuare{\E[X|Y]^2}-2\E\SQuare{X\E[X|Y]}+\E[X^2]\\
    &=\E[X^2]-\E\SQuare{\E[X|Y]^2}.
\end{align*}

この変形では，繰り返し期待値の法則
\begin{align*}
    \E\SQuare{X\E[X|Y]}&=\E\SQuare{\SQuare{X\E[X|Y]\,\bigg|\,Y}}\\
    &=\E\SQuare{\E[X|Y]^2}
\end{align*}
を西森の対称性の代わりに用いたことになる．

換言すれば，西森の対称性を証明したのと同様の方法を本命題にも適用したため，直接命題の適用は回避したことになる．

:::

::: {.callout-tip title="命題（自由エネルギーによる特徴付け）^[[@Krzakala-Zdeborova2024 p.124] 定理14．]" icon="false"}

簡単のため，$n=1$ とする．このとき，$\beta:=\sigma^{-1}$ に関して，次の式が成り立つ：

1. 相互情報量 $I(X,Y)$ について，

\begin{align*}
    \pp{I(X,Y)}{\beta}&=\frac{\MMSE}{2}\\
    &=\frac{\E[X^2]-\E\SQuare{\E[X|Y]^2}}{2}
\end{align*}

2. 自由エネルギー $F_1$ について，
$$
\pp{F_1}{\beta}=\frac{\E\SQuare{\E[X|Y]^2}}{2}.
$$

:::

::: {.callout-note title="証明" collapse="true" icon="false"}



:::