---
title: "信念伝搬アルゴリズム"
subtitle: "変分平均場近似"
author: "司馬博文"
date: 7/26/2024
date-modified: 7/27/2024
image: posterior.svg
categories: [Bayesian, Nature, Computation]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
abstract-title: 概要
format:
    html:
        code-fold: false
execute:
    cache: true
abstract: |
    信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．コミュニティ抽出や圧縮センシングの問題はまさにこのクラスのモデルと対応し，信念伝搬法（または変分近似）によって効率的に解くことができる．
---

{{< include ../../../_preamble.qmd >}}

## 導入

### はじめに

MP は情報理論では [@Gallager1962] が，因果推論の文脈で [@Pearl1982] が独立に開発している．

信念伝搬法は変分手法とは違い，ある種のモデル（木やランダムグラフなど）では正確な推論が可能になる上に，一般に速い．^[[@Zdeborova-Krzakala2016 p.471]] 当然 Monte Carlo 法よりも速い．

そのため，変分推論の改良の源泉が，メッセージ伝搬を通じて探求されている [@Winn-Bishop2005]．

### ランダムグラフ上のスピン系の分配関数

頂点数 $N$，辺数 $E$ の Erdös-Renyi ランダムグラフ $\cG(N,E)$ 上のスピン $(\sigma_i)_{i\in[N]}$ と相互作用 $(\psi_{ij})_{(i,j)\in E}$ を考える：
$$
\psi_{ij}(\sigma_i,\sigma_j)=\exp\Paren{-\beta J_{ij}\sigma_i\sigma_j}.
$$

ランダムグラフでは平均的なループの長さは $\log N$ であり，局所的に木の構造を持つため，このことを利用した近似を使って信念伝搬を行う．

$Z_{i\to j}(\sigma_i)$ を，$i$ を根とする木から $j$ を根とする木を除去し，$i$ におけるスピンの値が $\sigma_i$ であると条件付けた場合の部分木上の分配関数，$Z_i(\sigma_i)$ を $i$ におけるスピンの値が $\sigma_i$ であると条件付けた場合の木全体の分配関数とすると，次の関係がある：
$$
Z_i(\sigma_i)=\prod_{j\in\partial i}\paren{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}.
$$

するとこのとき，
\begin{align*}
    \eta_i(\sigma_i):=\frac{Z_i(\sigma_i)}{\sum_{\sigma'}Z_i(\sigma')}&=\frac{1}{\sum_{\sigma'}Z_i(\sigma')}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}\\
    &=:\frac{1}{z_i}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}\eta_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}.
\end{align*}
は Boltzmann-Gibbs 分布の周辺分布になっている：
$$
m_i=\brac{\sigma_i}=\sum_{\sigma}\eta_i(\sigma)\sigma.
$$

このときの正規化定数は
\begin{align*}
    z_i&=\sum_{\sigma_i}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}\eta_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}
    =\sum_{\sigma_i}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}\frac{Z_{j\to i}(\sigma_j)}{\sum_{\sigma'}Z_{j\to i}(\sigma')}\psi_{ij}(\sigma_i,\sigma_j)}\\
    &=\sum_{\sigma_i}\frac{Z_i(\sigma_i)}{\prod_{j\in\partial i}\sum_{\sigma_j}Z_{j\to i}(\sigma_j)}=\frac{Z}{\prod_{j\in\partial i}\sum_{\sigma_j}Z_{j\to i}(\sigma_j)},\\
    z_{j\to i}&=\frac{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)}{\prod_{k\in\partial j\setminus i}\sum_{\sigma_k}Z_{k\to j}(\sigma_k)}.
\end{align*}

と表せる．

$z_i$ の分母には $Z$ が現れていることを用いると $Z$ は，任意の位置 $i\in[N]$ を起点として，次のような展開表示ができる：
\begin{align*}
    Z&=\sum_{\sigma_i}Z_i(\sigma_i)=z_i\prod_{j\in\partial i}\paren{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)}=z_i\prod_{j\in\partial i}\paren{z_{j\to i}\prod_{k\in\partial j\setminus i}\sum_{\sigma_k}Z_{k\to j}(\sigma_k)}.
\end{align*}
ここで，最右辺の $\sum_{\sigma_k}Z_{k\to j}(\sigma_k)$ は $z_{j\to i}$ を計算するのと同じ要領で計算されることになり，最終的に木の葉まで再帰的に計算することで，公式
$$
Z=z_i\prod_{j\in\partial i}\paren{z_{j\to i}\prod_{k\in\partial j\setminus i}z_{k\to j}\cdots}=z_i\prod_{j\in\partial i}\paren{\frac{z_j}{z_{ij}}\prod_{k\in\partial j\setminus i}\frac{z_k}{z_{jk}}\cdots}=\frac{\prod_{i\in[N]}z_i}{\prod_{(i,j)\in E}z_{ij}}.
$$

よって，この再帰的計算により，自由エネルギー
$$
F=-T\log Z=\sum_{i\in[N]}\Paren{-T\log z_i}-\sum_{(i,j)\in E}\Paren{-T\log z_{ij}}.
$$
も得られることになる．

### 無限レンジ極限での高温解の安定性

[@Thouless+1977] は TAP 方程式により，[@Sherrington-Kirkpatrick1975] 模型を解いた．

その際の結果が，同じく無限サイズのグラフである [Bethe 格子](https://ja.wikipedia.org/wiki/ベーテ格子) 上でならば，信念伝搬法により議論でき，有効 cavity 場が次のように与えられるという：
$$
\beta h_0=\sum_{i=1}^k\operatorname{atanh}\Paren{\tanh(\beta J_{ij})\tanh(\beta h_i)}+H.
$$

このモデルでもスピングラス相が出現することが，信念伝搬法が失敗する（局所不安定になる）こととして現れる．これはグラフの木構造を破壊するような長距離の相関が出現することによる．スピングラス相と常磁性相の境界は Almeida-Thouless 線 [@Thouless1986] という．

## 共同体抽出

### 導入

グラフの頂点をクラスタリングした際のクラスターに当たる概念を **共同体** (community) という．

最初におもつくような方法は，横断する辺の数が最小になるようなクラスター境界を決定する方法であるが，これは NP 困難である．

これを少し修正して，各辺に対して edge centrality を計算し，これを順に脱落させて edge centrality を計算しなおすというような反復を行う [Girvan-Newman アルゴリズム](https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm) [@Girvan-Newman2002] が最も有名なものである（引用数２万！）．

### modularity 最大化による方法

Girvan-Newman アルゴリズムの中では，分割の「良さ」の指標として modularity
$$
Q=\frac{1}{2m}\sum_{ij}\paren{A_{ij}-\frac{d_id_j}{2N}}\delta_{s_i}\delta_{s_j}
$$
を定義し，アルゴリズムの停止の指標としていた．

現在では，逆にこの modularity を最適化することでクラスタリングを達成する手法が主流の１つである．

辺を全て消去した状況から１つずつ追加していき，この modularity の値を最大化する階層的クラスタリングアルゴリズムとして [@Newman2004], [@Clauset+2004] が提案された．

最適化問題として定式化された以上，模擬アニーリングが用いられることもある [@Guimera+2004]．

### スピングラス対応

じきに [@Reichardt-Bornholdt2006] によって，共同体抽出の問題は Potts 模型の基底状態探索と等価であることが示された．

実際，各頂点に割り当てられた未知の Potts スピン $\sigma_i$ に対して，同じ状態を持つ頂点同士は繋がろうとし，違う頂点を持つ場合は結合は疎になる傾向があるという状況は
$$
H(\sigma)=-\sum_{i<j}J_{ij}\delta(\sigma_i,\sigma_j),
$$
$$
J_{ij}:=J\Paren{A_{ij}-\gamma p_{ij}}
$$
というハミルトニアンで表現できる．$A_{ij}$ は隣接行列 $A=(A_{ij})\in M_n(2)$ の成分，$p_{ij}$ は辺の数の期待値，$\gamma$ は推定されるクラスタ数を決定するハイパーパラメータ (resolution parameter とも表現するらしい) である．

こう捉えると，たしかに模擬アニーリングは１つの選択肢だ．

### 信念伝搬による復号

Potts モデルではないが，[@Hastings2006] は同様にスピングラスモデルと同一視をし，こちらでは信念伝搬により基底状態探索を行った．

一方で [@Newman-Leicht2007] は混合モデルとみなし，EM アルゴリズムによる方法を見出している．ここまで来ると確率的ブロックモデル [@Snijders-Nowicki1997] による方法に非常に似通っており，[@Decelle2011] はまさにこの見方をしている．

そもそも，EM アルゴリズムと西森ラインは深い関係がある．^[[@Krzakala+2015 p.23]，[ベイズ統計学とスピングラスの稿](Bayes2.qmd#系の温度はハイパーパラメータに対応する) も参照．] 実際，EM アルゴリズムも，確率的ブロックモデルにおけるクラスの割り当ても，$k$-平均法のクラスタ数も，相転移を起こす．背後に何か物理過程と対応がつくものが存在するのかもしれない．

**ブロックモデル** (blockmodel / image graph) とはグラフの分割に関するモデルで，クラスタ数 $q$，$[q]$ 上の確率分布（頂点数の分布）$\{n_\al\}_{\al\in[q]}$，グループ間に辺が存在する確率を表す行列 $(p_{ab})_{a,b\in[q]}$ のパラメータからなる．^[[@Krzakala+2015 p.21] 4.1節，[@Fortunato2010 p.124] 9.2節．]

### スペクトルを通じた方法

モジュラリティ最適化と対照的な手法として，スペクトルを用いた方法がある．

最も直接的には，[Laplacian 行列](https://ja.wikipedia.org/wiki/ラプラシアン行列) の固有空間分解を通じて頂点集合を別の潜在空間に埋め込み，そこで $\R^N$-クラスタリングを行うという [@Dpnath-Hoffman1973] 以来の方法である．この文献はグラフの分割を問題にしているが，[@Donetti-Munoz2004] はコミュニティ抽出に集中している．

こちらの方が数学的にグラフの構造を捉えられそうなものであるが，現状，物理学的な背景を持った最適化・ベイズ推論に基づいた手法の方が性能が良いようである．^[[@Krzakala+2015 p.29] "In fact previous methods like spectral methods are not so good as the algorithm proposed in this section."]

## 圧縮センシング

### データ圧縮との違い

[@Krzakala+2015 p.30] には大変魅力的な導入がなされている．

例えば JPEG 2000 ではデータ圧縮の技術が使われており，これは画像データが wavelet 基底表現では大変スパースなデータになることを用いている．

圧縮センシングは最初から観測がスパースであるとし，データの復元の代わりに観測がなんだったかを推定することを考える．この技術は MRI に応用される [@Lustig+2007] ことで有名である．

即ち，「データは正しい基底に関してはスパースになるはずである」という事前情報の下，
$$
\vec{y}=G\vec{s},\qquad G\in M_{MN}(\R),M<N,
$$
という計画行列 $G$ と低次元の観測のみを通じて，真の観測 $\vec{s}$ を，ある行列 $A$ に関して
$$
\vec{s}=A\vec{x}
$$
として得られる $\vec{x}$ はスパースになるという追加情報を通じて推定することが，圧縮センシングの問題になる．

### スパースなデータの少数観測からの復元

$(A,\vec{x})$ の組について最適化を行い，最もスパースな $\vec{x}$ を決定するという問題
$$
\argmin_{\vec{x}}\norm{\vec{y}-F\vec{x}}_{\ell_0}
$$
は NP-困難であるが，統計力学の方法により $M/N=:\rho$ を一定にした比例的高次元極限 $N\to\infty$ に関する漸近論が得られる．

結論として，$\vec{s}$ の長さ $N$ に対して，実際は
$$
M=O\paren{N^{1/4}\log^{5/2}N}
$$
の固定された (nonadaptive) 観測があれば十分な復元が可能であるという．

具体的には，特定の条件の下では，（適切な基底に関する）スパースな $l^p$-展開係数が大きい順に $n$ 個わかれば，$l^2$-誤差が　$O(n^{1/2-1/p})$ のオーダーに従う [@Pinkus1985]．

そして，$M=O\paren{n\log N}$ の観測があれば，この $n$ 個の重要な係数が十分な精度で復元できる，というカラクリである．そのためのアルゴリズムには Basis Persuit [@Chen+1998] という凸最適化を通じた極めて効率的なアルゴリズムが利用可能である．

### $\ell_p$-ノルム最適化の方法

代わりに
$$
\argmin_{\vec{x}}\norm{\vec{y}-F\vec{x}}_{\ell_p}
$$
という最適化問題を考えるとこれは凸最適化問題であるため，突然多項式時間で解ける問題になる．

$p=1$ と取ることが実用上スパースな結果をうまく出してくれるようであるが，観測数の比 $\rho$ は真の限界より大きく取る必要がある．

### スピングラス対応

観測モデルが線型 Gauss であるとするとき，ランダムに観測されたスパース符号の復元の問題は，事前分布 $p(x)=(1-\rho)\delta(x)+\rho\phi(x)$ が定める事後分布
$$
p(x|F,y)=\frac{1}{Z}\prod_{i=1}^N\Paren{(1-\rho)\delta(x_i)+\rho\phi(x_i)}\prod_{\mu=1}^M\frac{1}{\sqrt{2\pi\Delta_\mu}}\exp\paren{-\frac{\abs{y_\mu-F_{\mu-}\cdot x}^2}{2\Delta_\mu}}
$$
を通じた Bayes 推論として定式化できる．するとこれは負の対数尤度
$$
H(x)=-\sum_{i=1}^N\log\Paren{(1-\rho)\delta(x_i)+\rho\phi(x_i)}+\sum_{\mu=1}^M\frac{1}{2\Delta_\mu}\abs{y_\mu-F_{\mu-}\cdot x}^2
$$
が定める Boltzmann 分布と見ることができるが，これは長距離無秩序を持つスピングラス系の Hamiltonian となっており，この系の基底状態の特定として問題が捉え直せる．

### 平均場変分ベイズによる復号（一般論）

一般の事後分布が $p(x|y)\propt p(y|x)p(x)$ の形で与えられているとし，分布 $q$ に関するこの系の自由エネルギーは
$$
\L=[\log p(y|x)p(x)]_q-\int_\Om q(x)\log q(x)\,dx=[-E(y,x)]_q-\int_\Om q(x)\log q(x)\,dx
$$ {#eq-L}
と表せる．ただし，$E:=-\log p(y|x)p(x)$ は内部エネルギーとした．

すると，この $\L$ を最小化する $q$ が平衡分布となるわけである．これで自由エネルギーに関する変分原理に問題が定式化し直された．

平均場近似とは，この $q$ に関して
$$
q(x)=\prod_{i=1}^Nq_i(x_i)
$$
という積の形を仮定した場合の表現
$$
\L_{\mathrm{MF}}=[-E(y,x)]_q-\sum_{i=1}^N\int q_i(x_i)\log q_i(x_i)\,dx
$$
に関して最小化を考えることをいう．

実はこの場合の $\L_{\mathrm{MF}}$ は KL-乖離度となっており，停留条件は
$$
q_i(x_i)=\frac{e^{[-E(y,x)]_q}}{Z_i}
$$
で得られ，これは平均場方程式と呼ばれるものである．

このことから，$q$ が一般の分布族の場合でも，KL 乖離度を最小化することによって推論をするという変分推論の指針が示唆される．

### 平均場変分ベイズによる復号（圧縮センシングの場合）

この場合，$R_i,\Sigma^2_i$ を用いれば
$$
Q_i(x_i)=\frac{1}{Z_i}p(x_i)\frac{1}{\sqrt{2\pi\Sigma_i^2}}e^{-\frac{(x_i-R)^2}{2\Sigma_i^2}}
$$
というように，事前分布と Gauss 分布の積の形を持ち，$\Sigma_i^2,R_i$ の値も順次決定される．

するとその結果を用いれば逐次的に計算することができる．実際，これを
$$
\sum_{\mu}F^2_{\mu i}=1
$$
の仮定の下で行ったものが，Iterative Threshouding [@Maleki-Donoho2010] である．

### 信念伝搬による復号

式 ([-@eq-L]) は
$$
\L=[\log p(y|x)]_q-\KL(q,p)
$$
とも表せる．第一項は人工的な分布 $q$ に関する平均であるから計算できるものとすると，$\KL(q,p)$ も，平均場近似の下では，各 $x_i$ は独立と仮定しているから各項ごとに計算できる：
$$
\KL(q_i,p_i)=-\log\wt{Z}_i-\frac{c_i+(a_i-R_i)^2}{2\Sigma^2_i}.
$$
右辺はノルム，$q_i$ による $x_i$ の平均と分散によって計算できる．

実際，メッセージ
$$
m_{\mu\to i}(x_i)=\frac{1}{Z^{\mu\to i}}\int\prod_{j\ne i}dx_je^{-\frac{1}{2\Delta_\mu}\paren{\sum_{j\ne i}F_{\mu j}x_j+F_{\mu i}x_i-y_\mu}^2}\prod_{j\ne i}m_{j\to\mu}(x_j)
$$
$$
m_{i\to\mu}(x_i)=\frac{1}{Z^{i\to\mu}}\Paren{(1-\rho)\delta(x_i)+\rho\phi(x_i)}\prod_{\gamma\ne\mu}m_{\gamma\to i}(x_i)
$$
の伝播により計算できるところを，$N\to\infty$ の極限では，密度 $m_{\mu\to i},m_{i\to\mu}$ そのものではなく平均 $a_{i\to\mu}$ と分散 $v_{i\to\mu}$，その局所的な近似 $a_i,v_i$（「信念」）を伝えるのみで近似計算が可能である．

その結果，とりわけ圧縮センシングの問題に関しては，正確性とスピードの面で極めて効率的な信念伝搬法が得られる [@Kschischang+2001], [@Yedidia+2003]．アルゴリズムが停止したと判断されたのち，$a_i$ によって真の信号 $x_i^*$ の推定とし，$v_i$ は推定の不確実性を表す．$N\to\infty$ の極限において一致性を持つ．

BP はあるクラスの分布（混合 Gauss など）に従う観測 $x$ について，理論的な復号限界ピッタリまで復元可能である．

### 近似メッセージ伝搬 (AMP)

この信念伝搬方では $2M\times N$ のメッセージが送信されるが，ある仮定の下では $N+M$ のメッセージで十分である [@Donoho+2009]．

これは物理的には TAP 方程式 [@Thouless+1977] に基づく消息であり，$N\to\infty$ の極限で信念伝搬法に一致する．

その後，AMP は必ずしもスパースな観測に限らずとも，一般の設定における復号（特に $M$-推定）においても有用であることが報告されている [@Donoho-Montanari2016]．

## 参考文献 {.appendix}

### [@Krzakala+2015] {.appendix}

Lecture note なので歩み寄りやすい．Bayes 推論とスピングラス系の対応，これを解くための信念伝搬法について非常に良い入門になる．

第四章でグラフのコミュニティ抽出，第五章で圧縮センシングについて扱っている．クラスタリングはとかく物理的な背景が深いことが，相転移に関する詳細な解析を通じてよくわかる．クラスタリング面白すぎる．$k$-means，EM アルゴリズム，Gibbs サンプラーはいずれもクラスタリングに使用可能で，いずれも物理を背景に持ち，拡散過程を極限に持ち，それぞれに相転移があるのかもしれない．



### [@Zdeborova-Krzakala2016] {.appendix}

> In physics the origins of these methods trace back to the Bethe approximation [@Bethe1935], the work of Peierls [@Peierls+1936] and the Thouless–Anderson–Palmer (TAP) equations [@Thouless+1977]. Interestingly, these equations can be seen as “improved” variational mean-ﬁeld equations, and this is at the roots of many perturbation approaches (such as [@Plefka1982], [@Georges-Yedidia1991] or [@Kikuchi1951]). [@Zdeborova-Krzakala2016 p.471]

### [@Fortunato2010] 共同体抽出のレビュー {.appendix}

グラフによるモデリングとその共同体抽出が，社会学・疫学・生化学で活躍している様子がよくわかり，大変なモチベーションが上がるようなレビューである．

[@村田剛志2009] では community detection は **コミュニティ抽出** としている．

### [@Donoho2016] 圧縮センシングのレビュー {.appendix}

荘厳すぎる完成された理論の感じよ．