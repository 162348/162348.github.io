---
title: "信念伝搬アルゴリズム"
subtitle: "変分平均場近似"
author: "司馬 博文"
date: 7/26/2024
image: posterior.svg
categories: [Bayesian, Nature, Computation]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
abstract-title: 概要
format:
    html:
        code-fold: false
execute:
    cache: true
abstract: |
    信念伝搬法 (BP: Belief Propagation) はランダムグラフや木の上で定義されたスピン系の熱平均を計算するアルゴリズムであり，Monte Carlo 法より高速な代替となる．変分手法と違い，前述のクラスのモデルでは正確な推論が可能になる上に，一般のグラフ上でも良い近似を与え，また一般により速いアルゴリズムを与える．
---

{{< include ../../../_preamble.qmd >}}

## 導入

### はじめに

MP は情報理論では [@Gallager1962] が，因果推論の文脈で [@Pearl1982] が独立に開発している．

信念伝搬法は変分手法とは違い，ある種のモデル（木やランダムグラフなど）では正確な推論が可能になる上に，一般に速い．^[[@Zdeborova-Krzakala2016 p.471]] 当然 Monte Carlo 法よりも速い．

そのため，変分推論の改良の源泉が，メッセージ伝搬を通じて探求されている [@Winn-Bishop2005]．

### ランダムグラフ上のスピン系の分配関数

頂点数 $N$，辺数 $E$ の Erdös-Renyi ランダムグラフ $\cG(N,E)$ 上のスピン $(\sigma_i)_{i\in[N]}$ と相互作用 $(\psi_{ij})_{(i,j)\in E}$ を考える：
$$
\psi_{ij}(\sigma_i,\sigma_j)=\exp\Paren{-\beta J_{ij}\sigma_i\sigma_j}.
$$

ランダムグラフでは平均的なループの長さは $\log N$ であり，局所的に木の構造を持つため，このことを利用した近似を使って信念伝搬を行う．

$Z_{i\to j}(\sigma_i)$ を，$i$ を根とする木から $j$ を根とする木を除去し，$i$ におけるスピンの値が $\sigma_i$ であると条件付けた場合の部分木上の分配関数，$Z_i(\sigma_i)$ を $i$ におけるスピンの値が $\sigma_i$ であると条件付けた場合の木全体の分配関数とすると，次の関係がある：
$$
Z_i(\sigma_i)=\prod_{j\in\partial i}\paren{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}.
$$

するとこのとき，
\begin{align*}
    \eta_i(\sigma_i):=\frac{Z_i(\sigma_i)}{\sum_{\sigma'}Z_i(\sigma')}&=\frac{1}{\sum_{\sigma'}Z_i(\sigma')}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}\\
    &=:\frac{1}{z_i}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}\eta_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}.
\end{align*}
は Boltzmann-Gibbs 分布の周辺分布になっている：
$$
m_i=\brac{\sigma_i}=\sum_{\sigma}\eta_i(\sigma)\sigma.
$$

このときの正規化定数は
\begin{align*}
    z_i&=\sum_{\sigma_i}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}\eta_{j\to i}(\sigma_j)\psi_{ij}(\sigma_i,\sigma_j)}
    =\sum_{\sigma_i}\prod_{j\in\partial i}\paren{\sum_{\sigma_j}\frac{Z_{j\to i}(\sigma_j)}{\sum_{\sigma'}Z_{j\to i}(\sigma')}\psi_{ij}(\sigma_i,\sigma_j)}\\
    &=\sum_{\sigma_i}\frac{Z_i(\sigma_i)}{\prod_{j\in\partial i}\sum_{\sigma_j}Z_{j\to i}(\sigma_j)}=\frac{Z}{\prod_{j\in\partial i}\sum_{\sigma_j}Z_{j\to i}(\sigma_j)},\\
    z_{j\to i}&=\frac{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)}{\prod_{k\in\partial j\setminus i}\sum_{\sigma_k}Z_{k\to j}(\sigma_k)}.
\end{align*}

と表せる．

$z_i$ の分母には $Z$ が現れていることを用いると $Z$ は，任意の位置 $i\in[N]$ を起点として，次のような展開表示ができる：
\begin{align*}
    Z&=\sum_{\sigma_i}Z_i(\sigma_i)=z_i\prod_{j\in\partial i}\paren{\sum_{\sigma_j}Z_{j\to i}(\sigma_j)}=z_i\prod_{j\in\partial i}\paren{z_{j\to i}\prod_{k\in\partial j\setminus i}\sum_{\sigma_k}Z_{k\to j}(\sigma_k)}.
\end{align*}
ここで，最右辺の $\sum_{\sigma_k}Z_{k\to j}(\sigma_k)$ は $z_{j\to i}$ を計算するのと同じ要領で計算されることになり，最終的に木の葉まで再帰的に計算することで，公式
$$
Z=z_i\prod_{j\in\partial i}\paren{z_{j\to i}\prod_{k\in\partial j\setminus i}z_{k\to j}\cdots}=z_i\prod_{j\in\partial i}\paren{\frac{z_j}{z_{ij}}\prod_{k\in\partial j\setminus i}\frac{z_k}{z_{jk}}\cdots}=\frac{\prod_{i\in[N]}z_i}{\prod_{(i,j)\in E}z_{ij}}.
$$

よって，この再帰的計算により，自由エネルギー
$$
F=-T\log Z=\sum_{i\in[N]}\Paren{-T\log z_i}-\sum_{(i,j)\in E}\Paren{-T\log z_{ij}}.
$$
も得られることになる．

### 無限レンジ極限での高温解の安定性

[@Thouless+1977] は TAP 方程式により，[@Sherrington-Kirkpatrick1975] 模型を解いた．

その際の結果が，同じく無限サイズのグラフである [Bethe 格子](https://ja.wikipedia.org/wiki/ベーテ格子) 上でならば，信念伝搬法により議論でき，有効 cavity 場が次のように与えられるという：
$$
\beta h_0=\sum_{i=1}^k\atanh\Paren{\tanh(\beta J_{ij})\tanh(\beta h_i)}+H.
$$

このモデルでもスピングラス相が出現することが，信念伝搬法が失敗する（局所不安定になる）こととして現れる．これはグラフの木構造を破壊するような長距離の相関が出現することによる．スピングラス相と常磁性相の境界は Almeida-Thouless 線 [@Thouless1986] という．

## 共同体抽出

### 導入

グラフの頂点をクラスタリングした際のクラスターに当たる概念を **共同体** (community) という．

最初におもつくような方法は，横断する辺の数が最小になるようなクラスター境界を決定する方法であるが，これは NP 困難である．

これを少し修正して，各辺に対して edge centrality を計算し，これを順に脱落させて edge centrality を計算しなおすというような反復を行う [Girvan-Newman アルゴリズム](https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm) [@Girvan-Newman2002] が最も有名なものである（引用数２万！）．

### modularity 最大化による方法

Girvan-Newman アルゴリズムの中では，分割の「良さ」の指標として modularity
$$
Q=\frac{1}{2m}\sum_{ij}\paren{A_{ij}-\frac{d_id_j}{2N}}\delta_{s_i}\delta_{s_j}
$$
を定義し，アルゴリズムの停止の指標としていた．

現在では，逆にこの modularity を最適化することでクラスタリングを達成する手法が主流の１つである．

辺を全て消去した状況から１つずつ追加していき，この modularity の値を最大化する階層的クラスタリングアルゴリズムとして [@Newman2004], [@Clauset+2004] が提案された．

最適化問題として定式化された以上，模擬アニーリングが用いられることもある [@Guimera+2004]．

### スピングラス対応

じきに [@Reichardt-Bornholdt2006] によって，共同体抽出の問題は Potts 模型の基底状態探索と等価であることが示された．

実際，各頂点に割り当てられた未知の Potts スピン $\sigma_i$ に対して，同じ状態を持つ頂点同士は繋がろうとし，違う頂点を持つ場合は結合は疎になる傾向があるという状況は
$$
H(\sigma)=-\sum_{i<j}J_{ij}\delta(\sigma_i,\sigma_j),
$$
$$
J_{ij}:=J\Paren{A_{ij}-\gamma p_{ij}}
$$
というハミルトニアンで表現できる．$A_{ij}$ は隣接行列 $A=(A_{ij})\in M_n(2)$ の成分，$p_{ij}$ は辺の数の期待値，$\gamma$ は推定されるクラスタ数を決定するハイパーパラメータ (resolution parameter とも表現するらしい) である．

こう捉えると，たしかに模擬アニーリングは１つの選択肢だ．

Potts モデルではないが，[@Hastings2006] は同様にスピングラスモデルと同一視をし，こちらでは信念伝搬により基底状態探索を行った．

一方で [@Newman-Leicht2007] は混合モデルとみなし，EM アルゴリズムによる方法を見出している．ここまで来ると確率的ブロックモデル [@Snijders-Nowicki1997] による方法に非常に似通っており，[@Decelle2011] はまさにこの見方をしている．

そもそも，EM アルゴリズムと西森ラインは深い関係がある．^[[@Krzakala+2015 p.23]，[ベイズ統計学とスピングラスの稿](Bayes2.qmd#系の温度はハイパーパラメータに対応する) も参照．] 実際，EM アルゴリズムも，確率的ブロックモデルにおけるクラスの割り当ても，$k$-平均法のクラスタ数も，相転移を起こす．背後に何か物理過程と対応がつくものが存在するのかもしれない．

**ブロックモデル** (blockmodel / image graph) とはグラフの分割に関するモデルで，クラスタ数 $q$，$[q]$ 上の確率分布（頂点数の分布）$\{n_\al\}_{\al\in[q]}$，グループ間に辺が存在する確率を表す行列 $(p_{ab})_{a,b\in[q]}$ のパラメータからなる．^[[@Krzakala+2015 p.21] 4.1節，[@Fortunato2010 p.124] 9.2節．]

### スペクトルを通じた方法

モジュラリティ最適化と対照的な手法として，スペクトルを用いた方法がある．

最も直接的には，[Laplacian 行列](https://ja.wikipedia.org/wiki/ラプラシアン行列) の固有空間分解を通じて頂点集合を別の潜在空間に埋め込み，そこで $\R^N$-クラスタリングを行うという [@Dpnath-Hoffman1973] 以来の方法である．この文献はグラフの分割を問題にしているが，[@Donetti-Munoz2004] はコミュニティ抽出に集中している．

こちらの方が数学的にグラフの構造を捉えられそうなものであるが，現状，物理学的な背景を持った最適化・ベイズ推論に基づいた手法の方が性能が良いようである．^[[@Krzakala+2015 p.29] "In fact previous methods like spectral methods are not so good as the algorithm proposed in this section."]

## 圧縮センシング

### データ圧縮との違い

[@Krzakala+2015 p.30] には大変魅力的な導入がなされている．

例えば JPEG 2000 ではデータ圧縮の技術が使われており，これは画像データが wavelet 基底表現では大変スパースなデータになることを用いている．

圧縮センシングは最初から観測がスパースであるとし，データの復元の代わりに観測がなんだったかを推定することを考える．

即ち，「データは正しい基底に関してはスパースになるはずである」という事前情報の下，
$$
\vec{y}=G\vec{s},\qquad G\in M_{MN}(\R),M<N,
$$
という計画行列 $G$ と低次元の観測のみを通じて，真の観測 $\vec{s}$ を，ある行列 $A$ に関して
$$
\vec{s}=A\vec{x}
$$
として得られる $\vec{x}$ はスパースになるという追加情報を通じて推定することが，圧縮センシングの問題になる．



## 参考文献 {.appendix}

### [@Krzakala+2015] {.appendix}

Lecture note なので歩み寄りやすい．Bayes 推論とスピングラス系の対応，これを解くための信念伝搬法について非常に良い入門になる．

第四章でグラフのコミュニティ抽出，第五章で圧縮センシングについて扱っている．クラスタリングはとかく物理的な背景が深いことが，相転移に関する詳細な解析を通じてよくわかる．クラスタリング面白すぎる．$k$-means，EM アルゴリズム，Gibbs サンプラーはいずれもクラスタリングに使用可能で，いずれも物理を背景に持ち，拡散過程を極限に持ち，それぞれに相転移があるのかもしれない．



### [@Zdeborova-Krzakala2016] {.appendix}

> In physics the origins of these methods trace back to the Bethe approximation [@Bethe1935], the work of Peierls [@Peierls+1936] and the Thouless–Anderson–Palmer (TAP) equations [@Thouless+1977]. Interestingly, these equations can be seen as “improved” variational mean-ﬁeld equations, and this is at the roots of many perturbation approaches (such as [@Plefka1982], [@Georges-Yedidia1991] or [@Kikuchi1951]). [@Zdeborova-Krzakala2016 p.471]

### [@Fortunato2010] 共同体抽出のレビュー {.appendix}

グラフによるモデリングとその共同体抽出が，社会学・疫学・生化学で活躍している様子がよくわかり，大変なモチベーションが上がるようなレビューである．

[@村田剛志2009] では community detection は **コミュニティ抽出** としている．