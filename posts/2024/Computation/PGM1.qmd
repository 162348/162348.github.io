---
title: "数学者のための確率的グラフィカルモデル１"
subtitle: ベイジアンネットワークとマルコフネットワーク
author: "司馬博文"
date: 12/20/2023
date-modified: 6/27/2024
categories: [Bayesian, Computation]
toc: true
image: PGM1.png
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: PGM (Probabilistic Graphical Modelling) で用いられる代表的なモデル３つ（ベイジアンネットワーク，マルコフネットワーク，ファクターグラフ）を定義し，その性質を抽象的に説明する．これらは，複雑な高次元分布の分解を，計算機に理解可能な形で与える技法である．マルコフネットワークの形で与えられる分布に対しては，たとえ高次元であろうとも，MCMC によって効率的なサンプリングが可能である．
---

{{< include ../../../assets/_preamble.qmd >}}

## 歴史と導入

### 例

::: {.callout-tip appearance="simple" icon="false" title="ベイジアンネットワークの例^[[@Taroni+2014 p.35]，[@Sucar2021 p.x], [@Clark2018] [Graphical Models](https://m-clark.github.io/sem/graphical-models.html)．[@Jordan+1999 p.191] は 3.2節で Neural Networks as Graphical Models を扱っている．]"}

* [階層モデル](../Kernels/HierarchicalModel.qmd) や構造方程式モデル，[状態空間モデル](../../2023/Surveys/SSM.qmd)
* ニューラルネットワーク [@Rumelhart+1987]
* [決定木](../../2023/数理法務/%E6%B3%95%E5%BE%8B%E5%AE%B6%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E7%B5%B1%E8%A8%88%E6%95%B0%E7%90%863.qmd) や influence diagram [@Howard-Matheson1981]
* システム工学におけるブロック線図
* 構造的因果モデル [@Pearl16-Primer]^[この文脈では，ベイジアンネットワークのことは DAG とも，汎函数因果モデル [@Scholkopf2022] とも呼ぶ．[@Murphy2023 p.211] 4.7節．例えば，医療診断では，複数の症状や検査結果，医学的指標との関連・相関・因果に関する知識を Bayesian Network （@sec-BN） で表現する．]

:::

また，ベイジアンネットワークは，[Markov 圏](../../2023/Probability/MarkovCategory.qmd) 上の図式のうち，特定のグラフ理論的な条件 [-@sec-DAG] を満たすものと見れる．

::: {.callout-tip appearance="simple" icon="false" title="マルコフネットワークの例^[[@Mezard-Montanari2009 p.177] 9.1.2 節に，ファクターグラフの例が挙げられている．]"}

* （２次元以下の）[Ising 模型](PGM2.qmd#sec-Ising-model) や [Potts 模型](../Nature/StatisticalMechanics1.qmd#sec-Potts) はマルコフネットワークの例である．
* 画像データはマルコフネットワークとみなされ，デノイジングなどに用いられる．

:::

ファクターグラフは，マルコフネットワークと，その上に局所的に定義されたポテンシャルに関する情報との組である．

以上，グラフを利用したモデリング法は全て，確率的グラフィカルモデルとも呼ばれる．

### はじめに

モデルに変数が多く含まれるほど，モデリングの作業は難しくなっていく．

その中でも，最も基本的な事前知識は「どの変数の間に関係があり，どの変数は互いに独立か」というタイプのものであり，変数間のグラフを描くことで特にわかりやすくなる．

**グラフィカルモデル** とは，このような変数間の依存性・独立性を表現したグラフに，特定の分布族を対応させる数学的枠組みである．^[[@Koller-Friedman2009 p.3] 1.2.1，[@Murphy2023 p.143] 第4章．[@Balgi+2024] "As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships."]

### 諸科学での知識表現の歴史

多くの科学分野において，「知識表現」の「知識」とは，特に因果関係に関する知識のことを指すようである．これを捉えるために，グラフを用いることは自然な発想であり，計算機の登場以前にも，純粋に人間が理解を深めるための用途に，歴史上極めて早い時期から用いられていた．

:::{.callout-caution icon="false" collapse="true" title="グラフ表現の歴史^[[@Koller-Friedman2009 pp.12-14] 1.4節 など．]"}

高次元分布において，成分間の独立性をグラフを用いて表現しようという発想は，その計算機との親和性が見つかる前に，種々の科学分野で試みられていた．

* [@Gibbs1902] が統計力学の文脈で，相関粒子系の分布をグラフで表現した．
* [@Wright1918] は骨格測定のデータを用いた因子分析で，（遺伝的な意味での）依存関係を，パス図と呼ばれる有向グラフを用いて表した．
* [@Wold1954] とその教え子との [@Joreskog-Wold1981]，さらに [@Blalock1971] が社会学において，因果をグラフを用いて表す因子分析法を **構造方程式モデル** (SEM: Structural Equation Model) の名前の下に普及させた．^[一般に，SEM は [@Joreskog70] が発祥と見られており，潜在変数モデルにもパス解析を拡張したもの，と説明される [@Clark2018]．]
* 経済学では特に **操作変数法** として独自の発展を遂げている．
* その後 [@Wold-Strotz60] は [@Pearl09-Causality] などの do-calculus に繋がっている．これはパス解析や構造方程式モデルのノンパラメトリックな拡張とも見れる．^[[@黒木学-小林史明2012] など．]
* 統計学でも [@Bartlett1935] が分割表分析において変数同士の相関の研究をしたが，界隈が本格的に受け入れたのはやっと 1960 年代以降である．

:::

### 人工知能分野での確率的モデリングの採用 {#sec-probabilistic-methods}

人工知能分野が確率的手法を採用したのは，エキスパートシステムの構築が志向された 1960 年代であった．^[[@Koller-Friedman2009 pp.12-14] 1.4節．]

医療診断や油源探索における専門家に匹敵する判断力を持つアルゴリズムを構築する途上で，不確実性の度合いの定量化が必要となり，naive Bayes model （第 [-@sec-naive-Bayes] 節）と呼ばれる確率的モデルが採用された．特に [@Dombal+1972] は限られた分野であるが人間を凌駕する診断正答率を示した．

だがこの確率的アプローチは，主にその計算複雑性から 1970 年代では冬の時代を経験することとなり，エキスパートシステムも production rule framework や [ファジー論理](https://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A1%E3%82%B8%E3%82%A3%E8%AB%96%E7%90%86) [@Zadeh1989] など，確率論に代わって他のアーキテクチャが試みられるようになっていった．

::: {.callout-important title="Bayesian network とエキスパートシステム" collapse="true" icon="false"}

グラフィカルモデリングは初め，自立して推論・意思決定を行うエキスパート・システムの構築のために人工知能分野で用いられ始めた．^[[@Koller-Friedman2009 p.1] 1.1 Motivation．これはこの分野が不確実性を定量的に扱う必要があり，それ故確率的モデリングを必要としたためである．一般に，特定のタスクに特化しながら，汎用性も持つエキスパートシステムを構築するためには，[宣言型の知識表現](https://ja.wikipedia.org/wiki/%E5%AE%A3%E8%A8%80%E5%9E%8B%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0) が良い接近として用いられる [@Koller-Friedman2009 p.1] 1.1 Motivation．declarative representation の他に model-based approach ともいう．これは対象となるシステムの構造に関する知識を，計算機が理解可能な形で表現するモデルベースな接近であり，「知識」と「推論」という異なるタスクを分離する点に妙がある．]

ベイズ流のアプローチでは，事前分布を定める上で，系に対する客観的な事前知識と主観的な事前知識とを分けることが重要であるが，グラフィカルモデルや階層モデリングはこの分離を自然に行うことができる [@Robert2007 pp.457-458]．

世界に対する知識には不確実性がつきものであり，実世界応用ではこの「不確実性」を反映したモデリング手法が有効であることが多い．そこで，近年の機械学習へのベイズ流アプローチでも，グラフィカルモデルは盛んに応用されている（第 [-@sec-probabilistic-methods] 節）．^[[@Koller-Friedman2009 p.2] 1.1 Motivation．Probabilistic models allow us to make this fact (= many systems cannot be specified deterministically.) explicit, and therefore often provide a model which is more faithful to reality.]

> I have approximate answers and possible beliefs with different degrees of uncertainty about different things, but I am not absolutely sure of anything. -- [Richard Feynman](https://www.youtube.com/watch?v=P-Qdl6Gbx0k)

加えてグラフという表現方法は，人間にとって視覚的にわかりやすいだけでなく，周辺化，極値計算，条件付き確率の計算を高速化するという点で計算機的にも利点がある．^[[@Theodoridis2020 p.772] なども参照．用いるアルゴリズムの計算複雑性も，グラフ理論の言葉で記述できることが多い [@Wainwright-Jordan2008 p.4]．]

:::

### Bayesian Network と確率的モデリングの登場

これを打開したのが

::: {.callout-important appearance="simple" icon="false"}

1. [@Pearl88-IntelligentSystem] による Bayesian network framework と，[@Lauritzen-Spiegelhalter1988] による効率的な推論手法という理論的発展．
2. [@Heckerman+1992], [@Heckerman-Nathwani1992] が Bayesian network を病理学標本に応用して大きな成功を挙げたこと．

:::

の２つである．

これにより，確率的グラフィカルモデル，また一般に確率的アプローチが広く受け入れられるようになった．

### Markov Network の登場と MCMC の普及

MCMC が真に統計界隈に輸入されるきっかけとなった [@Gelfand-Smith1990] は Gibbs サンプラーに関するものであった．[@Geman-Geman1984] による Gibbs サンプラーの提案も，画像分析，広く空間統計学においてなされたものであった．

統計物理学における Ising モデルが，空間統計学において Markov random field として広く受け入れられ [@Besag1974]，これにより統計界隈に階層モデルと Gibbs サンプラーが広く受け入れられるようになったのである．

特に，Markov random field が，Gibbs 分布の局所的な条件付き分布からの特徴付けを与える（Hammersley-Clifford の定理[-@sec-separation-in-markov-network]）という認識を導き，このことが MCMC を一般の Bayes 計算手法たらしめたと強調している [@Besag-Green1993]．^[その最重要文献として，[@Grenander1983]，特に画像分析への Bayesian アプローチを取り扱った 4-6 章を挙げている．Gibbs サンプラーの語を導入したのは [@Geman-Geman1984] であるが，すでに [@Grenander1983] において極めて重要な Bayes 計算手法として扱われていた．]

> it is no coincidence that the original concept and the early development of MCMC in Bayesian inference should take place exclusivelyin the spatial statistics literature. [@Besag-Green1993 p.26]

<!-- ### 確率的グラフィカルモデリングの美点

確率的グラフィカルモデリングの美点は，人間（エキスパート）と計算機の協業を促進する共通言語としての働きが出来る点である．

1. 人間と計算機の双方にとって解釈しやすい **表現** である．
2. 確率的グラフィカルモデルで表現できる分布のクラスと，効率的に Bayes **推論** が可能な分布のクラスとが一致する．^[[@Koller-Friedman2009 pp.5-6] 1.2.2節．高次元分布が成分間に依存を持つことと，その依存を用いてコンパクトに低次元で表現可能であることとは殆ど等価な事実である．]
3. 人間と計算機の双方がモデリングに参加できる．後者によるモデリングは，**学習** とも呼ばれることになる．^[[@Koller-Friedman2009 p.6] 1.2.2節．"Probabilistic graphical models support a data-driven approach to model construction that is very eﬀective in practice."]

さらに，高次元分布 $P$ の成分間の依存関係を効率よく捉える手法であるため，その背後にあるグラフが判れば，グラフの分離性（ @sec-separation-in-markov-network ）を判定するだけで，$P$ の独立性の情報を得ることが出来る．

他にも，グラフの構造を用いて，$P$ を効率的に表現し，本質的な次元を大幅に落として計算を効率化することもできる． -->

## Bayesian Network {#sec-BN}

### 例：ナイーブ Bayes モデル {#sec-naive-Bayes}

[naive Bayes model](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) は Idiot Bayes model とも呼ばれる Bayesian Network の簡単な例である．

これは **クラス** と呼ばれる離散潜在変数 $C\in\{c^1,\cdots,c^k\}$ を持つ次のようなモデルである．

![naive Bayes model](naiveBayes.svg)

この際，グラフィカルモデルに共通する用語を確認する．

* クラスの実現値 $c^i$ を **インスタンス** と呼ぶ．
* 潜在変数の実現値が確定することを，**観測** の他に **インスタンス化** ともいう．
* インスタンス化されたときに取る値は **エビデンス** とも呼ばれる．

観測値 $X_1,\cdots,X_n$ は **特徴** (features) と呼ばれ，これは互いに辺で結ばれていないため，クラスを与えた下で互いに条件付き独立であるとする：
$$
(X_i\indep\b{X}_{-i}\mid C)\;(i\in[n]),
$$
$$
\b{X}_{-i}:=(X_{1:i-1},X_{i+1:n}).
$$

こうして得る階層モデルを **naive Bayes model** という．^[[@Bishop2006 p.46] などでも紹介されている．] その結合密度は
$$
p(c,x_1,\cdots,x_n)=p(c)\prod_{i=1}^np(x_i|c)
$$
と表せる．

### DAG {#sec-DAG}

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.57]，[@Mezard-Montanari2009 p.269]．] （Bayesian Network structure）"}

確率変数 $\b{X}:=(X_1,\cdots,X_n)$ に関して，成分の全体 $\cX:=\{\cX_1,\cdots,\cX_n\}$ を節集合とした [**有向非循環グラフ**](https://ja.wikipedia.org/wiki/%E6%9C%89%E5%90%91%E9%9D%9E%E5%B7%A1%E5%9B%9E%E3%82%B0%E3%83%A9%E3%83%95) (directed acyclic graph, DAG) $\cG=(\cX,\cE)$ を **Bayesian Network 構造** といい，これが親ノードから子ノードへの確率核 $P_i:\pi(\cX_i)\to\cX_i$ を通じて定める
$$
\prod_{\cX_i\in\cX}P_i
$$
という形の分布の全体を **Bayesian Network** または **有向グラフィカルモデル** という．

:::

Bayesian network は belief network とも呼ばれる．^[[@須山敦志2019 p.4], [@Li2009 p.48]．[Wikipedia](https://en.wikipedia.org/wiki/Bayesian_network) も参照．] 決定分析で用いられる [influence diagram](https://en.wikipedia.org/wiki/Influence_diagram) / decision network はその一般化である．

:::{.callout-caution icon="false" collapse="true" title="記法（親ノード，子孫ノード，非子孫ノード）"}


DAG $\cG$ において，

* 節 $\cX_i$ からその親節の全体への対応を
$$
\cX_i\mapsto\pi(\cX_i)
$$
で表す．
* 節 $\cX_i$ からその子節の全体への対応を添字について表現したものを
$$
\cX_i\mapsto\des(\cX_i)
$$
で表す．
* 次の対応を **非子孫ノード** という：
$$
\nd(\cX_i):=\cX\setminus(\{\cX_i\}\cup\des(\cX_i)).
$$

「子孫ノードである」という関係は DAG 上に順序を定める．

:::

### DAG が表現する局所独立構造

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.57]] （Directed Local Markov Independence）"}

DAG $\cG$ が表現する条件付き独立性の全体を $\cI(\cG)$ で表す．そのうち，特に
$$
X_i\indep (X_j)_{j\in\nd(i)}\mid (X_j)_{j\in\pi(i)}
$$
という形をしたものを **局所独立性** といい，その（論理式の）全体を $\cI_l(\cG)$ で表す．

:::

Bayesian Network が視覚的表現・記号論で，その表現する所の局所依存性が意味論であると言える．

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.60]] （Independence Assertions）"}

$P\in\cP(\cX)$ をノードの集合 $\cX=\{X_1,\cdots,X_n\}$ 上の確率分布とする．$(X_i)_{i=1}^n\sim P$ に関して成立する条件付き独立性の主張
$$
(X_i)_{i\in I}\indep(X_j)_{j\in J}\mid (X_k)_{k\in K}
$$
$$
I\sqcup J\sqcup K\subset[n]
$$
の（論理式の）全体を **$P$ が含意する条件付き独立性** といい， $\cI(P)$ で表す．

:::

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.60]] （$I$-Map）"}

$\cI$ を確率変数 $(X_1,\cdots,X_n)$ の成分間の条件付き独立性に関する論理式の全体，$\cK$ を DAG とする．$\cK$ が $\cI$ の **$I$-map** であるとは，
$$
\cI_l(\cK)\subset\cI
$$
を満たすことをいう．すなわち，DAG $\cK$ が表す局所的な独立性関係が $\cI$ に含まれていることをいう．

:::

### Bayesian Network の特徴付け

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.62]] （factorize, chain rule, local probabilistic model, Bayesian Network）"}

$\cG$ を確率変数 $(X_1,\cdots,X_n)$ に関する Bayesian Network 構造とする．

1. 分布 $P\in\cP(\cX)$ が $\cG$ に従って **分解する** とは，$(X_1,\cdots,X_n)\sim P$ と仮定したとき，次が成り立つことをいう：
$$
\L[X_1,\cdots,X_n]=\bigotimes^n_{i=1}\L[X_i|(X_j)_{j\in\pi(i)}].
$$
2. この式を Bayesian Network $\cG$ の **連鎖律** といい，右辺の因子 $\L[X_i|(X_j)_{j\in\pi(i)}]$ の全体を 条件付き確率分布族 または **局所モデル** という．
3. Bayesian Network 構造 $\cG$ とこれに沿って分解する分布 $P\in\cP(\cX)$ との組 $(\cG,P)$ を，**Bayesian Network** または **有向確率モデル** という．

:::

:::{.callout-tip icon="false" collapse="false" title="命題^[[@Koller-Friedman2009 p.62] 定理3.1，定理3.2 p.63．[@Howard-Matheson1984] による．] （Bayesian Network の特徴付け）"}

$\cG$ を確率変数 $(X_1,\cdots,X_n)$ に関する Bayesian Network 構造，$P\in\cP(\cX)$ を確率分布とする．このとき，次は同値：

1. $\cG$ が $\cI(P)$ の $I$-map である．
2. $P$ は $\cG$ に従って分解する．

:::

### ３節グラフの分離性

節が３つ $X,Y,Z$ の場合の DAG は大別して３通り存在する．この場合で「分離性」の概念を説明する．

３つの成分 $(X,Y,Z)$ が依存関係にある状態で，$Z$ が観測された（インスタンス化された）とする．

その場合に，$X,Y$ 間の因果関係がどう変化するか？を考える．元々因果関係があったところから，^[これを trail が active である，ともいう．[@Koller-Friedman2009 p.71]．] これが解消されるとき，$X,Y$ は $Z$ を介して **$d$-分離** であるという．^[この語は directed separation の略であり [@Koller-Friedman2009 p.71]，和語では **有向分離** ともいう．]

:::{.callout-caution icon="false" collapse="true" title="逐次結合の場合"}

次のような逐次結合の場合，節 $X,Y$ は，節 $Z$ がインスタンス化されたとき **$d$-分離** である，という．
```{dot}
//| label: fig-CausalTrail
//| fig-cap: "逐次結合 (Causal Trail)"
//| fig-width: 1
digraph CausalTrail {
    X -> Z;
    Z -> Y;
}
```
$X$ を勉強量，$Z$ を素点，$Y$ を GPA とするとき，$Z$ が観測されたならば，もはや勉強量は GPA に影響を与えない．ただし，相関は存在するだろうが．
:::

:::{.callout-caution icon="false" collapse="true" title="分岐結合の場合"}

次のような分岐結合の場合，節 $X,Y$ は，節 $Z$ がインスタンス化されたとき **$d$-分離** である，という．
```{dot}
//| label: fig-CommonCause
//| fig-cap: "分岐結合 (Common Cause)"
//| fig-width: 3
digraph CausalTrail {
    Z -> X;
    Z -> Y;
}
```
:::

:::{.callout-caution icon="false" collapse="true" title="合流結合の場合"}

次のような合流結合の場合，節 $X,Y$ は，節 $Z$ またはその子孫節がインスタンス化されなければ，節 $Z$ を介して **$d$-分離** である，という．
```{dot}
//| label: fig-CommonEffect
//| fig-cap: "合流結合 (Common Effect)"
//| fig-width: 3
digraph CausalTrail {
    X -> Z;
    Y -> Z;
}
```
この構造は $v$-構造ともいう．^[[@Koller-Friedman2009 p.71]] この場合，$Z$ が観測されたならば，$X,Y$ は因果関係を持つようになる．

$Z$ が事象の有無で，$X,Y$ のいずれかが起こった時に $Z$ も起こるとしよう．いま $Z$ が起こったこと $Z=1$ が判明したとすると，$X,Y$ のいずれか一方も起こっている必要がある．従って，$X=0$ は $Y=1$ を要請するという因果関係が生じる．
:::

### 一般の DAG の分離性

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 pp.71-72] 定義3.6, 3.7．] （active, $d$-Separated, Directed Global Markov Independencies）"}

$\cG$ を Bayesian Network 構造，$\b{Z}\subset\cX$ を観測された節とする．

1. 非有向道 $X_1\rightleftharpoons\cdots\rightleftharpoons X_n$ が $\b{Z}$ の下でも **active** であるとは， 次の２条件を満たすことをいう：
   1. $\{X_i\}_{i=1}^n\cap\b{Z}=\emptyset$．
   2. 任意の無向道内の合流結合 $X_{i-1}\rightarrow X_i\leftarrow X_{i+1}$ について，$X_i$ またはその子孫に $\b{Z}$ の元が存在する．
2. $\b{X}\sqcup\b{Y}\sqcup\b{Z}\subset\cX$ を節の集合とする．$\b{X},\b{Y}$ が $\b{Z}$ に関して **$d$-分離** であるとは，任意の $X\in\b{X}$ と $Y\in\b{Y}$ と，$X,Y$ を結ぶ無向道が，$\b{Z}$ の下で active でないことをいう．このことを $\dsep_\cG(\b{X};\b{Y}|\b{Z})$ と表す．^[$I(\b{X},\b{Y}|\b{Z})_\cG$ と表すこともある．]
3. $\cG$ 内の $d$-分離な組 $(\b{X},\b{Y},\b{Z})$ が表す条件付き独立性の条件式の全体を
$$
\cI(\cG):=\Brace{(\b{X}\indep\b{Y}|\b{Z})\mid\dsep_\cG(\b{X};\b{Y}|\b{Z})}.
$$
この元を **大域的独立性** ともいう．
:::

局所依存性（ @sec-DAG ）は $d$-分離性の特別な場合であり，$\cI_l(\cG)\subset\cI(\cG)$ である．

### 例

![$d$-分離になるのはいつか？](Files/separation.svg){width=50%}

この Bayesian Network 構造は，いつ $d$-分離になり，いつ $d$-分離ではないか？

:::{.callout-caution icon="false" collapse="true" title="答え"}

* いずれも観測されない場合は $d$-分離である．
* $Z$ が観測された場合，$A,B$ のいずれかも観測されていれば，やはり $d$-分離である．
:::

### 分離性の特徴付け {#sec-characterization-of-d-separation}

:::{.callout-tip icon="false" collapse="false" title="命題^[[@Koller-Friedman2009 pp.72-73] 定理3.3, 3.5．] （$d$-分離性の特徴付け）"}

$\cG$ を Bayesian Network 構造，$P\in\cP(\cX)$ を確率分布とする．

1. $P$ が $\cG$ に沿って分解するならば，$\cI(\cG)\subset\cI(P)$．
2. $\cH$ に沿って分解する殆ど全ての $P\in\cP(\cX)$ に関して，上の逆も成り立ち，特に等号が成立する．

:::

$\cG$ が定める分布族について，殆ど全ての分布が共通して持つ条件付き独立性の構造を，$\cG$ から読み取れる $d$-分離性によって発見できるということになる．

さらには，分布 $P$ の独立性の情報を知りたい場合，この背後にあるグラフ $\cG$ を探し出して，$d$-分離性を調べれば良い，ということでもであるのである．^[[@Koller-Friedman2009 p.78] 3.4節 の内容．]

### $I$-同値性

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.76] 定義3.9．] （$I$-Equivalence）"}

２つの Bayesian Network 構造 $\cG,\cG'$ が **$I$-同値** であるとは，$\cI(\cG)=\cI(\cG')$ が成り立つことをいう．
:::

$I$ は写像であるから，この関係は確かに Bayesian Network 構造の全体（果てには有向グラフの全体）に同値関係を定める．

:::{.callout-tip icon="false" collapse="false" title="命題^[[@Koller-Friedman2009 p.77] 定理3.7．] （$I$-同値性の十分条件）"}

２つの Bayesian Network 構造 $\cG,\cG'$ が

1. 同じスケルトンを持ち，^[有向グラフの **スケルトン** とは，同じ辺を持つ無向グラフのことである．]
2. 同じ $v$-構造を持つ

ならば，$I$-同値である．
:::

有向グラフ $\cG=(\cX,\cE)$ の辺 $(X,Y)\in\cE$ が **被覆されている** とは，
$$
\pi(Y)=\pi(X)\cup\{X\}
$$
を満たすことをいう．

合流結合 $X\rightarrow Z\leftarrow Y$ において，辺 $X\to Z$ は被覆されていない．

:::{.callout-tip icon="false" collapse="false" title="命題^[[@Koller-Friedman2009 p.77] 定理3.8．] （$I$-同値性の特徴付け）"}

２つの Bayesian Network 構造 $\cG,\cG'$ について，次は同値：

1. $\cG,\cG'$ は $I$-同値である．
2. $\cG$ に $I$-同値なグラフの列 $\cG=\cG_0,\cdots,\cG_m=\cG'$ であって，隣り合うグラフ $\cG_i,\cG_{i+1}\;(i\in m)$ 同士は，被覆されている辺の向きの反転しか違わないものが存在する．
:::

## Markov Network

### グラフ理論の準備

$A$ を集合とする．
$$
[A]^k:=\Brace{B\in P(A)\mid\# B=k}
$$
とする．無向グラフとは集合 $V$ と $E\subset[V]^2$ の組 $G:=(V,E)$ のことをいう．^[[@Diestel2017 pp.1-2] 参照．]

**Markov Network 構造** とは，任意の無向グラフをいう．

２つの節 $x,y\in V$ が **隣接する** (adjacent / neighbours) とは，$\{x,y\}\in E$ が成り立つことをいう．

無向グラフ $G$ が **完備** (complete) であるとは，任意の $x,y\in V$ について $\{x,y\}\in E$ が成り立つことをいう．このとき，頂点集合 $V$ は **クリーク** (clique) であるという．位数 $n$ の完備グラフは $K^n$ で表される．^[[@Diestel2017 p.3] 参照．]

$K^r\subset G$ を満たす最大の数
$$
\om(G):=\Brace{r\in\N\mid K^r\subset G}
$$
を **クリーク数** といい，グラフの不変量となる．^[[@Diestel2017 p.135] 参照．]

[**弦グラフ**](https://ja.wikipedia.org/wiki/%E5%BC%A6%E3%82%B0%E3%83%A9%E3%83%95) (chordal / triangulated graph) とは，任意の長さ４以上のサイクルが弦を持つグラフを言う．^[すなわち，三角形以外の [誘導部分グラフ](https://ja.wikipedia.org/wiki/%E8%AA%98%E5%B0%8E%E9%83%A8%E5%88%86%E3%82%B0%E3%83%A9%E3%83%95) を部分グラフに持たないグラフをいう．[@Diestel2017 p.135] 参照．] 弦グラフが，Bayesian Network と Markov Network の双方により表現可能であるグラフのクラスに一致する．

### Markov Network と Markov Random Field

マルコフネットワークは，２次元のマルコフ確率場に等価である．^[[@Sucar2021 p.94] も参照．[@Li2009 p.47] は，pairwise なマルコフ確率場もマルコフネットワークと見れることを指摘している．pairwise とは非零なポテンシャルを持つクリークが二点集合になるマルコフ確率場をいう．]

後者は [Ising モデル](PGM2.qmd#sec-Ising-model) の一般化である．^[[@Kindermann-Snell1980 p.1]]

### はじめに

Markov Network は相互作用に自然な双方向性がない場合でもモデリングを可能とする．

例えば，集合 $\{A,B,C,D\}$ 上の条件付き独立関係
$$
\cI:=\Brace{\substack{A\indep C|(B,D),\\B\indep D|(A,C)}}
$$
に関して，$\cI(\cG)=\cI$ を満たす Bayesian Network 構造 $\cG$ は存在しない．

一方で，分岐結合と合流結合とを区別できないため，因果性のような方向を持った依存関係は表現できない．

Markov Network では，節の間に自然な順序構造がないため，分布の表示が難しくなり，より純粋にグラフの分解に頼ることになる．それゆえ，データからの構造学習も遥かに難しくなる．^[[@Koller-Friedman2009 p.106] 4.2節．]

Bayesian Network では条件付き確率密度のみで十分だったところを，これを一般化する概念である factor と呼ばれる概念によって達成する．

条件付き確率密度 $p(x_1,\cdots,x_m|y_1,\cdots,y_k)$ とは，形式的には，積空間 $\prod_{i=1}^m\Im(X_i)\times\prod_{j=1}^k\Im(Y_j)$ 上の（正規化された）関数である．一般に，確率変数の値域の積上の（正規化されているとは限らない）関数を **ファクター** と言う．

### ファクター

確率変数の組 $\b{X}=(X_1,\cdots,X_n)$ 上の **ファクター** とは，ある部分集合 $\{n_1,\cdots,n_D\}\subset[n]$ に対して，関数 $(X_{n_1},\cdots,X_{n_D})$ の値域上に定義された関数
$$
\phi:\prod_{i=1}^D\Im(X_{n_i})\to\R
$$
を言う．この定義域を **スコープ** と言う．^[[@Koller-Friedman2009 p.104] 定義4.1．]

定義域 $a,b\subset[n]$ がかぶる２つのファクター $\phi_1,\phi_2,a\cap b\ne\emptyset$ が存在する場合，これらを接続して，$\prod_{i\in a\cup b}\Im(X_i)$ 上に定義された新たなファクターを作ることが出来る：^[ただし，$\phi_1(X_a)$ とは $\phi_1((X_i)_{i\in a})$ の略とした．]
$$
\phi_1\times\phi_2(X_{a\cup b}):=\phi_1(X_a)\phi_2(X_b).
$$

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.108] 定義4.3．] （Gibbs distribution, factorization）"}

1. 離散確率変数の組 $\b{X}=(X_1,\cdots,X_n)$ とその上のファクター
$$
\Phi:=(\phi_1(\b{D}_1),\cdots,\phi_m(\b{D}_m))
$$
$$
\b{D}_j\subset\{X_i\}_{i=1}^n\quad(j\in[m])
$$
とが定める $\prod_{i=1}^n\Im(X_i)$ 上の [**Gibbs 分布**](https://en.wikipedia.org/wiki/Gibbs_measure) とは，密度
$$
p_\Phi(\b{x})=\frac{1}{Z}\prod_{j=1}^m\phi_j(\b{D}_j)
$$
が定める分布をいう．ここで $Z$ は正規化定数であり，歴史的には **分配関数** と言う．^[[@Koller-Friedman2009 p.105] によると，当初統計物理学の分野の Markov 確率場の概念でこの用語が用いられたことが始まりとなっている．]
2. Gibbs 分布 $p_\Phi$ が Markov network $\cH=(\{X_i\}_{i=1}^n,\cE)$ 上で **分解する** とは，任意の $\cD_j\subset\{X_i\}_{i=1}^n\;(j\in[m])$ が $\cH$ のクリークであることをいう．このとき，各ファクター $\phi_1,\cdots,\phi_m$ を **clique potential** という．

:::

### Markov Network の分離性 {#sec-separation-in-markov-network}

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 pp.114-115] 定義4.8, 9．] （Global Markov Independence）"}

$\cH$ を Markov network 構造とする．

1. 道 $X_1\rightleftharpoons\cdots\rightleftharpoons X_n$ が $\b{Z}\subset\{X_i\}_{i=1}^n$ が観測された下でも **active** であるとは，$\{X_i\}_{i=1}^n\cap\b{Z}=\emptyset$ を満たすことをいう．
2. 節集合 $\b{X},\b{Y},\b{Z}$ について，$\b{Z}$ が $\b{X},\b{Y}$ を **分離** するとは，任意の $X\in\b{X}$ と $Y\in\b{Y}$ と，$X,Y$ を結ぶ道が，$\b{Z}$ の下で active でないことをいう．このことを $\sep_\cH(\b{X};\b{Y}|\b{Z})$ と表す．
* $\cH$ 内の分離的な組 $(\b{X},\b{Y},\b{Z})$ が表す条件付き独立性の条件式の全体を
$$
\cI(\cH):=\Brace{(\b{X}\indep\b{Y}|\b{Z})\mid\sep_\cH(\b{X};\b{Y}|\b{Z})}
$$
で表す．この元を **大域的独立性** ともいう．
:::

:::{.callout-tip icon="false" collapse="false" title="定理^[[@Koller-Friedman2009 pp.116-117] 定理4.1，定理4.2．] [@Hammersley-Clifford1971]"}

$P\in\cP(\cX)$ をノードの集合 $\cX=\{X_1,\cdots,X_n\}$ 上の確率分布，$\cH$ を $\cX$ 上の Markov network 構造とする．このとき 1. $\Rightarrow$ 2. が成り立ち，$P$ が $\cX$ 全域を台に持つとき次は同値：

1. $\cH$ は $P$ の $I$-map である：$\cI(\cH)\subset\cI(P)$．
2. $P$ は $\cH$ に従って分解する Gibbs 分布である．

:::

[@Besag1974] はこの定理に別証明を付し，植物生態学における空間統計モデルに応用している．^[[@Li2009] の Rama Chellappa による foreword に "A big impetus to theoretical and practical considerations of 2D spatial interaction models, of which MRF’s form a subclass, was given by the seminal works of Julian Besag." とある．"Labeling is also a natural representation for the study of MRF’s (Besag 1974)." は [@Li2009 p.3]．]

Markov 確率場の結合分布を，条件付き分布の系から得ることは困難であるが，結局結合分布も Gibbs 分布になることが [@Hammersley-Clifford1971] の定理からわかるので，Gibbs 分布を通じて計算することができる．

この「条件付き分布から結合分布が復元できる」という知見が Gibbs sampling の基礎となった．^[[@Robert-Casella2011] [@Li2009 p.1] も参照．] また統計的画像解析の基礎ともなった [@Grenander1983]．

また [@Geman-Geman1984] は，Markov 確率場でモデリングをし，その最大事後確率 MAP (Maximum a Posteriori) を目的関数として最適化を行う，という MAP-MRF アプローチを創始した [@Li2009 p.2]．

さらに統計計算法の進展により，画像の低レイヤーな特徴を表現する（画像修復，物体発見など）だけでなく，高レイヤーな特徴（物体認識やマッチングなど）をも扱えることがわかっている [@Gidas1989], [@Li1991]．

:::{.callout-tip icon="false" collapse="false" title="命題^[[@Koller-Friedman2009 p.117] 定理4.3．] （分離性の特徴付け）"}

$\cH$ を Markov network 構造，$\{X\}\sqcup\{Y\}\sqcup\b{Z}\subset\cX$ を節の集合とする．このとき，次が成り立つ：

1. $\cH$ 内で $X,Y$ は $\b{Z}$ によって分離されないならば，ある $\cH$ に沿って分解する分布 $P\in\cP(\cX)$ について，$X\indep Y|\b{Z}$ が成り立つ．
2. $\cH$ に沿って分解する殆ど全ての $P\in\cP(\cX)$ に関して，$\cI(\cH)=\cI(P)$ が成り立つ．
:::

Beysian Network （ @sec-characterization-of-d-separation ）の場合と違い，1. の主張が，$\cH$ に沿って分解する全ての分布 $P\in\cP(\cX)$ に関して成り立つとは限らない．

しかし，殆ど全ての $\cH$ に沿って分解する分布 $P\in\cP(\cX)$ に関して成り立つ条件付き独立性は，グラフの構造から読み取れる．

### 局所依存性

Bayesian Network の $d$-分離性に対応する分離性の概念を導入し，大域的独立性の概念を定義した．

しかし，Bayesian Network の場合では有向グラフとしての構造からすぐに読み取れた局所依存性の概念は，Markov Network の場合では，グラフの構造からは読み取れない．

そして２通りの定義が考え得る．局所依存性は，大域的依存性のサブセットであることに注意．そして，台を全体 $\cX$ に持つ分布については，大域的依存性も含めて３つの定義は全て同値である．^[これは台が縮退している場合は，自明な（決定論的な）独立性が生じてしまうためである．]

## Factor Graph {#sec-Factor-Graph}

Markov network は Gibbs 分布の依存性を十分に表現できているわけではなかった（第 [-@sec-separation-in-markov-network] 節）．これは特に，クリーク間の大小関係を把握できていないことに因る．

### 定義

:::{.callout-tip icon="false" collapse="false" title="定義^[[@Koller-Friedman2009 p.123] 4.4.1.1，[@Mezard-Montanari2009 p.175] 9.1.1 節．] （Factor Graph）"}

Markov newtork から，ファクターを表す節を（四角形で囲うなどして区別した形で）追加し，ファクターをそのスコープに入る変数と隣接するようにし，一方で変数を表す（元々の）節とファクターを表す節とが隣接しないように修正した [２部グラフ](https://ja.wikipedia.org/wiki/2%E9%83%A8%E3%82%B0%E3%83%A9%E3%83%95) $\F$ を [**因子グラフ**](https://en.wikipedia.org/wiki/Factor_graph) という．

分布 $P\in\cP(\cX)$ が $\F$ に関して **分解する** とは，$\F$ が定める確率変数の組とその上のファクターが定める Gibbs 分布であることをいう．

:::

## 終わりに {.appendix}

MCMC，特に Gibbs サンプラーは，Markov network の形で与えられる局所的な情報を利用したダイナミクスを持つ．

それ故，デザインから，大域的な探索が不得手であると言える．