---
title: "変分推論２"
subtitle: "EM アルゴリズム"
author: "司馬 博文"
date: 2/10/2024
categories: [Computation, Python]
toc: true
number-sections: true
code-block-bg: true
code-overflow: wrap
code-fold: true
code-annotations: select
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムに注目する．
---

{{< include ../../../_preamble.qmd >}}

::: {.callout-important icon="false" title="要約"}

EM アルゴリズムとは，目的関数が
$$
h(x)=\E[H(x,Z)]
$$
と表せる場合に対する MM アルゴリズムである．欠測モデルや混合モデルにおいて，最尤推定量を求める用途等の応用がある．このような状況では，（対数）尤度に関して下からの評価を与える代理関数で，特に振る舞いのようもの（$Q$-関数）が見つかるため（ @eq-3 ），代わりにこれを逐次的に最大化することが出来るのである．この手続きは変分 Bayes 法の特別な場合にあたる．

:::

[@Blei+2017]

## 最尤推定

クラスタリングを一度さっぱり忘れて，最尤推定を思い出してみる．

::: {.callout-tip icon="false" title="定義^[[@Cramer1946 p.498] によると，[@Fisher1912] が初出であるが，以前に Gauss がその特別な形を用いていた．また，[@Cramer1946 p.499] での定義はこことは違い，尤度関数の停留点（＝尤度方程式の解）と定義している．] [@Fisher1912]"}

$\{P_\theta\}_{\theta\in\Theta}\subset\cP(\cX)$ を統計モデルで，ある共通の $\sigma$-有限測度 $\mu\in\cP(\cX)$ に関して密度 $\{p_\theta\}_{\theta\in\Theta}$ を持つとする．

独立な観測 $X_1,\cdots,X_n$ の [**最尤推定量**](https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A) とは，モデルの対数尤度
$$
\log p_\theta
$$
を通じて定まる次の目的関数 $\ell_n:\Theta\to(-\infty,0)$ を最大化するような [$M$-推定量](https://en.wikipedia.org/wiki/M-estimator) をいう：
$$
\ell_n(\theta;X_1,\cdots,X_n):=\sum_{i=1}^n\log p_\theta(X_i),\qquad\theta\in\Theta.
$$

:::

すなわち，最尤推定量とは最適化問題の解として定式化されるのである．

最大値点であるということは，停留点である必要があるから，微分が零になるという条件を通じて解析的に求まることもある．この $Z$-推定量としての特徴付けは [@Cramer1946 p.498] による．

また，計算機的な方法では，[Iterative Propertional Fittting](https://en.wikipedia.org/wiki/Iterative_proportional_fitting) や勾配降下法を用いることも考えられる．

最尤推定量が解析的に求まらない場面は，代表的には欠測モデルなどがある．この場合には，モデルの構造を利用して最尤推定量を求めるための MM アルゴリズム [@Sun+2016] の例がある．これが [**EM アルゴリズム**](https://ja.wikipedia.org/wiki/EM%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0) [@Dempster+1977] である．

現在でも，その他の MM アルゴリズムが，種々の最適化問題に対する "problem-driven algorithm" であり続けている [@Wu-Lange2010]．

## EM アルゴリズム（Gauss 有限混合モデル）

EM アルゴリズムは，混合モデルに対する最尤推定アルゴリズムである．一般に，目的関数が
$$
h(x)=\E[H(x,Z)]
$$
と表せる場合に対する MM アルゴリズムである．^[[@Robert-Casella2004 p.174] 式(5.8)．]

### 欠測データと混合モデル

欠測データ (incomplete data) とは，２つの確率変数 $(Z,X)$ について次の図式が成り立つ際の，$Z$ を潜在変数として，$X$ からの観測とみなせるデータをいう [@Dempster+1977 p.1]：

![Missing Data Model / Latent State Model / Completed Model for $X$](incomplete.png){#fig-incomplete}

$Z$ もパラメータと見た場合，これは潜在変数 $Z$ を持つモデルともみなせる．また，$X$ に対する混合モデルともみなせる．

[隠れ Markov モデル](../../2023/Surveys/SSM.qmd) はこの良い例である．^[特に，状態空間モデルの文脈では，EM アルゴリズムは Baum-Welch アルゴリズムとも呼ばれる．]

このように，$X$ の分布を，潜在変数 $Z$ を追加して理解することを，モデルの **完備化** (completion) または **脱周辺化** (demarginalization)，またはデータの拡張 (data augmentation) ともいう．^[それぞれ，[@Robert2007 p.330]，[@Robert-Casella2004 p.176]，[@Hastie-Tibshirani-Friedman2009 p.276]，]

これは，
$$
p(x|\theta)=\int_{\cZ}p(x,z|\theta)\,dz
$$ {#eq-2}
という形の尤度を持つモデルである．^[[@Robert-Casella2004 p.174] 式(5.7)．$p$ を完備化された尤度 (completed likelihood) ともいう．]

### EM アルゴリズム {#sec-EM}

値域 $\cZ$ を持つ潜在変数 $Z$ とパラメータ $\theta\in\Theta$ に関して @eq-2 で表せる尤度関数 $p(x|\theta)$ に関して，Jensen の不等式より，任意の $x,\theta$ で添字づけられた確率密度関数 $q:\cZ\to\R_+$^[正確には確率核 $Q:\cX\times\Theta\to\cZ$．] とパラメータ $\theta\in\Theta$ について次の評価が成り立つ：

$$
\begin{align*}
    \log p(x|\theta)&=\log\int_{\cZ}p(x,z|\theta)\,dz\\
    &\ge\int_\cZ q(z|x,\varphi)\log\frac{p(x,z|\theta)}{q(z|x,\varphi)}\,dz\\
    &=:F(q,\theta).
\end{align*}
$$ {#eq-3}

この事実に基づき，$F$ を代理関数として，これを２つの変数 $q,\theta$ について交互に最大化するという手続きを，**EM アルゴリズム** という．この $F$ は多く $Q$ とも表され，$Q$-関数ともいう．

1. $E$-ステップ：$F$ を $q$ について最大化する．
$$
\begin{align*}
    F(q,\theta)&=\int_\cZ q(z|x,\varphi)\log\frac{p(z|x,\theta)p(x|\theta)}{q(z|x,\varphi)}\,dz\\
    &=\log p(x|\theta)-\KL(q_\varphi,p_\theta).
\end{align*}
$$
より，$q(z|x,\varphi)=p(z|x,\theta)$ で最大化される．^[この $p(z|x,\theta)$ は観測 $x$ の下での，潜在変数 $z$ の条件付き分布である．この $E$-ステップは，単に $q=p$ を代入して $Q(\theta)$ を計算する，とも説明される．しかし，このように双方を最大化ステップと見る変分法的な見方が出来るのである [@Wainwright-Jordan2008 pp.153-154], [@Neal-Hinton1998], [@Hastie-Tibshirani-Friedman2009 p.277]．]
1. $M$-ステップ：$F$ を $\theta$ について最大化する．
$$
\begin{align*}
    F(q,\theta)&=\int_{\cZ}q(z|x,\varphi)\log p(x,z|\theta)\,dz\\
    &\qquad-\int_\cZ q(z|x,\varphi)\log q(z|x,\varphi)\,dz\\
    &=\underbrace{(q_\varphi dz\,|\log p_\theta)}_{=:Q(\theta|\varphi,x)}+H(q_\varphi)
\end{align*}
$$
より，$Q$ の停留点で最大化される．^[これは典型的には微分を伴う．これを回避し，「最大化」とは限らないが，「現状よりは大きくする」アルゴリズムを用いた場合，これを GEM (Generalized EM) ともいう [@Hastie-Tibshirani-Friedman2009 p.277]．]

総じて，EM アルゴリズムは $p,q$ の KL 乖離度を逐次的に最小化している．

### EM アルゴリズムの有効性

::: {.callout-tip icon="false" title="命題：尤度は単調減少する [@Dempster+1977]^[[@Robert-Casella2004 p.177] 定理5.15，[@Robert2007 p.334] 演習6.52．]"}

$\{\wh{\theta}_{(j)}\}\subset\Theta$ を EM アルゴリズムの $M$-ステップでの出力列とする．このとき，

$$
L(\wh{\theta}_{(j+1)}|x)\ge L(\wh{\theta}_{(j)}|x).
$$
等号成立は
$$
Q(\wh{\theta}_{(j+1)}|\wh{\theta}_{(j)},x)=Q(\wh{\theta}_{(j)}|\wh{\theta}_{(j)},x)
$$
の場合のみ．

:::

::: {.callout-note icon="false" collapse="true" title="証明"}

:::

::: {.callout-tip icon="false" title="命題：局所解への収束 [@Boyles1983]-[@Wu1983]^[[@Robert-Casella2004 p.178] 定理5.16．]"}

$$
Q(\theta|\theta_0,x):=\int_\cZ p(z|\theta,x)\log p(\theta|x,z)\,dz
$$
は $\theta,\theta_0\in\Theta$ について連続であるとする．このとき，EM アルゴリズムの出力 $\{\wh{\theta}_(j)\}$ は尤度 $p(\theta|x)$ の停留点に単調に収束する．

:::

::: {.callout-note icon="false" collapse="true" title="証明"}

:::

よって，EM アルゴリズムは局所解には収束する．

しかし，常に尤度が単調増加するという性質上，局所解に囚われてしまった場合，そこから逃れることはないことになる．

大域解に収束することを保証したい場合は，異なる初期値で複数回 EM アルゴリズムを実行するか，擬似除冷 (simulated annealing)^[この用語は [@甘利俊一1989 p.141] の **模擬除冷** の表現に触発された．] などの別の手法を用いることを考える必要がある [@Finch+1989]．


## EM アルゴリズムの実装

負担率にもモデルを置いた場合，$K$-平均アルゴリズムは EM アルゴリズムになる．

EM アルゴリズムは一般に多峰性に弱いことをここで示す．^[[@Robert-Casella2004] も参照．]

### Guass 有限混合モデル

ここでは，以下の，有限な混合モデルで，さらに混合される分布は正規であるものを考える：

::: {.callout-tip icon="false" title="定義 (Gaussian finite mixture model)"}

集合 $[K]$ 上に値を取る隠れ変数 $Z$ の確率質量関数を $(p_k)_{k=1}^K$ とする．
$$
p(x;(\mu_k),\sigma,(p_k))=\sum_{k=1}^K p_k\phi(x;\mu_k,\sigma)
$$ {#eq-1}
として定まるモデル $(p_{(\mu_k),\sigma,(p_k)})$ を **Gauss 有限混合モデル** という．

ただし，$\phi(x$; $\mu$, $\sigma)$ は $\rN(\mu,\sigma^2)$ の密度とした．

:::

@eq-1 は $(X,Z)$ 上の結合分布の族を表しており，そのパラメータは $\theta:=((\mu_k),\sigma,(p_k))$ である．

尤度は，@eq-2 の $\cZ=[K]$，
$$
\begin{align*}
    p(x|\theta)&=\sum_{k=1}^K \P[Z=k]\phi(x|\mu_k,\sigma)\\
    p(x,z|\theta)&=\phi(x|\mu_k,\sigma)
\end{align*}
$$
に当たる場合である．

## Monte Carlo 法による解決

混合モデルにおける Bayes 推論が Gibbs サンプラーによって実行できる [@Robert1996] 上に，初期値依存性や局所解へのトラップが懸念されるという EM アルゴリズムの問題点を，MCMC はいずれも持ち合わせていない．

さらに，混合数 $K$ に関する検定も構成できる [@Mengersen-Robert1996]．

### Gibbs サンプリング

高次元な確率変数 $(U_1,\cdots,U_K)$ のシミュレーションを行いたい場合，直接行うのではなく，条件付き分布 $p(U_k|U_{-k})$ からのサンプリングを繰り返すことでこれを行うことが出来る．^[$U_{-k}:=U_{1:(k-1),(k+1):K}$ とした．]

1. 任意の初期値 $U_1^{(0)},\cdots,U_K^{(0)}$ を与える．
2. 各 $k\in[K]$ について，
$$
U_k^{(t)}\sim p(U_k|U_{-k}^{(t-1)})
$$
をサンプリングする．
1. 十分時間が経過した際，アルゴリズムの出力 $(U^{(t)}_1,\cdots,U^{(t)}_K)$ は $(U_1,\cdots,U_K)$ と同分布になる．

### 確率的 EM アルゴリズム

Gibbs サンプリングアルゴリズムは，EM アルゴリズム[-@sec-EM]の変形とみなせる：

1. $E$-ステップ：EM アルゴリズムでは
$$
Q(\theta|\vartheta,x):=(p_\vartheta dz\,|\log p_\theta)
$$
を評価するところであったが，Gibbs サンプリングでは，$p(z|x,\theta)$ のサンプリングを行う．
2. $M$-ステップ：EM アルゴリズムでは
$$
\argmax_{\vartheta\in\Theta}Q(\vartheta|\theta,x)=(p_\theta dz\,|\log p_\vartheta)
$$
を求めるところであったが，Gibbs サンプリングでは，$p(\theta|z,x)$ のサンプリングを行う．

これは $E$-ステップでの $Q$ 関数の評価が困難であるとき，$p(z|x,\theta)$ からのサンプリングでこれを回避できるという美点もある．

この美点のみを用いて，Gibbs サンプリングを用いないまでも，$E$-ステップのみをサンプリング法で代替した EM 法を MCEM (Monte Carlo EM) という [@Wei-Tanner1990], [@Wei-Tanner1990+]．

