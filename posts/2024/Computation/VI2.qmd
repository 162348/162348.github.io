---
title: "変分推論（２）EM アルゴリズム"
author: "司馬 博文"
date: 2/10/2024
categories: [Computation, Python]
toc: true
number-sections: true
code-block-bg: true
code-overflow: wrap
code-fold: true
code-annotations: select
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムに注目する．
---

{{< include ../../../_preamble.qmd >}}

::: {.callout-important icon="false" title="要約"}

EM アルゴリズムとは，部分的に観測されたグラフィカルモデルにおいて，最尤推定量を求めるためのアルゴリズムである．このようなモデルでは，（対数）尤度に関して下からの評価を与える代理関数で，特に振る舞いのようものが見つかるため（ @eq-3 ），代わりにこれを逐次的に最大化することが出来る．この手続きは変分 Bayes 法の特別な場合にあたる．

:::

[@Blei+2017]

## 最尤推定

クラスタリングを一度さっぱり忘れて，最尤推定を思い出してみる．

::: {.callout-tip icon="false"}
## 定義^[[@Cramer1946 p.498] によると，[@Fisher1912] が初出であるが，以前に Gauss がその特別な形を用いていた．また，[@Cramer1946 p.499] での定義はこことは違い，尤度関数の停留点（＝尤度方程式の解）と定義している．] [@Fisher1912]

$\{P_\theta\}_{\theta\in\Theta}\subset\cP(\cX)$ を統計モデルで，ある共通の $\sigma$-有限測度 $\mu\in\cP(\cX)$ に関して密度 $\{p_\theta\}_{\theta\in\Theta}$ を持つとする．

観測 $X_1,\cdots,X_n$ の [**最尤推定量**](https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A) とは，モデルの対数尤度
$$
\log p_\theta
$$
を通じて定まる次の目的関数 $\ell_n:\Theta\to(-\infty,0)$ を最大化するような [$M$-推定量](https://en.wikipedia.org/wiki/M-estimator) をいう：
$$
\ell_n(\theta;X_1,\cdots,X_n):=\sum_{i=1}^n\log p_\theta(X_i),\qquad\theta\in\Theta.
$$

:::

すなわち，最尤推定量とは最適化問題の解として定式化されるのである．

しかし最大値点であるということは，停留点である必要があるから，微分が零になるという条件を通じて解析的に求まることもある．

最尤推定量が解析的に求まらない場合は，代表的には欠測モデルなどがある．この場合にも最尤推定量を求めるための，MM アルゴリズム [@Sun+2016] の特殊な場合とも見れるソルバーが，[**EM アルゴリズム**](https://ja.wikipedia.org/wiki/EM%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0) [@Dempster+1977] である．

現在でも，その他の MM アルゴリズムが，種々の最適化問題に対する "problem-driven algorithm" であり続けている [@Wu-Lange2010]．

## EM アルゴリズム（Gauss 有限混合モデル）

欠測データ (incomplete data) とは，２つの確率変数 $(Z,X)$ について次の図式が成り立つ際の，$Z$ を潜在変数として，$X$ からの観測とみなせるデータをいう [@Dempster+1977 p.1]：

![Hidden / Latent State Model](incomplete.png){#fig-incomplete}

$X$ もパラメータと見た場合，これは潜在変数を持つモデルともみなせる．その簡単な例に混合モデルがある．ここでは，以下の有限混合モデルで，さらに混合される分布は正規であるものとする：

::: {.callout-tip icon="false"}
## 定義 (Gaussian finite mixture model)

集合 $[K]$ 上に値を取る隠れ変数 $Z$ の確率質量関数を $(p_k)_{k=1}^K$ とする．
$$
p(x;(\mu_k),\sigma,(p_k))=\sum_{k=1}^K p_k\phi(x;\mu_k,\sigma)
$$ {#eq-1}
として定まるモデル $(p_{(\mu_k),\sigma,(p_k)})$ を **Gauss 有限混合モデル** という．

ただし，$\phi(x$; $\mu$, $\sigma)$ は $\rN(\mu,\sigma^2)$ の密度とした．

:::

@eq-1 は $(X,Z)$ 上の結合分布の族を表しており，そのパラメータは $\theta:=((\mu_k),\sigma,(p_k))$ である．以降， @fig-incomplete に沿って，
$$
\begin{align*}
    p(x|\theta)&=\int_{[K]}p(x,z;\theta)\P^Z(dz)\\
    &=\sum_{k=1}^K \P[Z=k]\phi(x;\mu_k,\sigma)\\
    p(x,z;\theta)&:=\phi(x;\mu_k,\sigma)\\
\end{align*}
$$ {#eq-2}
と理解する．

### EM アルゴリズム

値域 $\cZ$ を持つ潜在変数 $Z$ とパラメータ $\theta\in\Theta$ に関して @eq-2 で表せる尤度関数 $p(x|\theta)$ に関して，Jensen の不等式より，任意の $x,\theta$ で添字づけられた確率密度関数 $q:\cZ\to\R_+$^[正確には確率核 $Q:\cX\times\Theta\to\cZ$．] とパラメータ $\theta\in\Theta$ について次の評価が成り立つ：

$$
\begin{align*}
    \log p(x|\theta)&=\log\int_{\cZ}p(x,z;\theta)\P^Z(dz)\\
    &\ge\log\int_\cZ q(z|x,\varphi)\log\frac{p(x,z|\theta)}{q(z|x,\varphi)}\\
    &=:F(q,\theta).
\end{align*}
$$ {#eq-3}

この事実に基づき，$F$ を代理関数として，これを２つの変数 $q,\theta$ について交互に最大化するという手続きを，**EM アルゴリズム** という．