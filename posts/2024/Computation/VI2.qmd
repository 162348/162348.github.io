---
title: "変分推論（２）"
subtitle: "EM アルゴリズム"
author: "司馬 博文"
date: 2/10/2024
categories: [Computation, Python]
toc: true
number-sections: true
code-block-bg: true
code-overflow: wrap
code-fold: true
code-annotations: select
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムに注目する．
---

{{< include ../../../_preamble.qmd >}}

::: {.callout-important icon="false" title="要約"}

EM アルゴリズムとは，目的関数が
$$
h(x)=\E[H(x,Z)]
$$
と表せる場合に対する MM アルゴリズムである．欠測モデルや混合モデルにおいて，最尤推定量を求める用途等の応用がある．このような状況では，（対数）尤度に関して下からの評価を与える代理関数で，特に振る舞いのようもの（$Q$-関数）が見つかるため（ @eq-3 ），代わりにこれを逐次的に最大化することが出来るのである．この手続きは変分 Bayes 法の特別な場合にあたる．

:::

[@Blei+2017]

## 最尤推定

クラスタリングを一度さっぱり忘れて，最尤推定を思い出してみる．

::: {.callout-tip icon="false" title="定義^[[@Cramer1946 p.498] によると，[@Fisher1912] が初出であるが，以前に Gauss がその特別な形を用いていた．また，[@Cramer1946 p.499] での定義はこことは違い，尤度関数の停留点（＝尤度方程式の解）と定義している．] [@Fisher1912]"}

$\{P_\theta\}_{\theta\in\Theta}\subset\cP(\cX)$ を統計モデルで，ある共通の $\sigma$-有限測度 $\mu\in\cP(\cX)$ に関して密度 $\{p_\theta\}_{\theta\in\Theta}$ を持つとする．

独立な観測 $X_1,\cdots,X_n$ の [**最尤推定量**](https://ja.wikipedia.org/wiki/%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A) とは，モデルの対数尤度
$$
\log p_\theta
$$
を通じて定まる次の目的関数 $\ell_n:\Theta\to(-\infty,0)$ を最大化するような [$M$-推定量](https://en.wikipedia.org/wiki/M-estimator) をいう：
$$
\ell_n(\theta;X_1,\cdots,X_n):=\sum_{i=1}^n\log p_\theta(X_i),\qquad\theta\in\Theta.
$$

:::

すなわち，最尤推定量とは最適化問題の解として定式化されるのである．

最大値点であるということは，停留点である必要があるから，微分が零になるという条件を通じて解析的に求まることもある．この $Z$-推定量としての特徴付けは [@Cramer1946 p.498] による．

また，計算機的な方法では，[Iterative Propertional Fittting](https://en.wikipedia.org/wiki/Iterative_proportional_fitting) や勾配降下法を用いることも考えられる．

最尤推定量が解析的に求まらない場面は，代表的には欠測モデルなどがある．この場合には，モデルの構造を利用して最尤推定量を求めるための MM アルゴリズム [@Sun+2016] の例がある．これが [**EM アルゴリズム**](https://ja.wikipedia.org/wiki/EM%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0) [@Dempster+1977] である．

現在でも，その他の MM アルゴリズムが，種々の最適化問題に対する "problem-driven algorithm" であり続けている [@Wu-Lange2010]．

## EM アルゴリズム（Gauss 有限混合モデル）

EM アルゴリズムは，混合モデルに対する最尤推定アルゴリズムである．一般に，目的関数が
$$
h(x)=\E[H(x,Z)]
$$
と表せる場合に対する MM アルゴリズムである．^[[@Robert-Casella2004 p.174] 式(5.8)．]

### 欠測データと混合モデル

欠測データ (incomplete data) とは，２つの確率変数 $(Z,X)$ について次の図式が成り立つ際の，$Z$ を潜在変数として，$X$ からの観測とみなせるデータをいう [@Dempster+1977 p.1]：

![Missing Data Model / Latent State Model / Completed Model for $X$](incomplete.png){#fig-incomplete}

$Z$ もパラメータと見た場合，これは潜在変数 $Z$ を持つモデルともみなせる．また，$X$ に対する混合モデルともみなせる．

このように，$X$ の分布を，潜在変数 $Z$ を追加して理解することを，モデルの **完備化** (completion) または **脱周辺化** (demarginalization) ともいう．^[[@Robert2007 p.330]，[@Robert-Casella2004 p.176]．]

これは，
$$
p(x|\theta)=\int_{\cZ}p(x,z|\theta)\,dz
$$ {#eq-2}
という形の尤度を持つモデルである．^[[@Robert-Casella2004 p.174] 式(5.7)．$p$ を完備化された尤度 (completed likelihood) ともいう．]

### EM アルゴリズム

値域 $\cZ$ を持つ潜在変数 $Z$ とパラメータ $\theta\in\Theta$ に関して @eq-2 で表せる尤度関数 $p(x|\theta)$ に関して，Jensen の不等式より，任意の $x,\theta$ で添字づけられた確率密度関数 $q:\cZ\to\R_+$^[正確には確率核 $Q:\cX\times\Theta\to\cZ$．] とパラメータ $\theta\in\Theta$ について次の評価が成り立つ：

$$
\begin{align*}
    \log p(x|\theta)&=\log\int_{\cZ}p(x,z|\theta)\,dz\\
    &\ge\int_\cZ q(z|x,\varphi)\log\frac{p(x,z|\theta)}{q(z|x,\varphi)}\,dz\\
    &=:F(q,\theta).
\end{align*}
$$ {#eq-3}

この事実に基づき，$F$ を代理関数として，これを２つの変数 $q,\theta$ について交互に最大化するという手続きを，**EM アルゴリズム** という．この $F$ は多く $Q$ とも表され，$Q$-関数ともいう．

1. $E$-ステップ：$F$ を $q$ について最大化する．
$$
\begin{align*}
    F(q,\theta)&=\int_\cZ q(z|x,\varphi)\log\frac{p(z|x,\theta)p(x|\theta)}{q(z|x,\varphi)}\,dz\\
    &=\log p(x|\theta)-\KL(q_\varphi,p_\theta).
\end{align*}
$$
より，$q(z|x,\varphi)=p(z|x,\theta)$ で最大化される．^[この $p(z|x,\theta)$ は観測 $x$ の下での，潜在変数 $z$ の条件付き分布である．通常，$q=p$ を取るから，この $E$-ステップは，単に $Q(\theta)$ を計算する，というステップになる．しかし，このように双方を最大化ステップと見る変分法的な見方が出来るのである [@Wainwright-Jordan2008 pp.153-154], [@Neal-Hinton1998]．@eq-5 も参照．]
2. $M$-ステップ：$F$ を $\theta$ について最大化する．
$$
\begin{align*}
    F(q,\theta)&=\int_{\cZ}q(z|x,\varphi)\log p(x,z|\theta)\,dz\\
    &\qquad-\int_\cZ q(z|x,\varphi)\log q(z|x,\varphi)\,dz\\
    &=\underbrace{(q_\varphi|\log p_\theta)}_{=:Q(\theta)}+H(q_\varphi)
\end{align*}
$$
より，$Q$ の停留点で最大化される．

総じて，EM アルゴリズムは $p,q$ の KL 乖離度を逐次的に最小化している．

### EM アルゴリズムの有効性

::: {.callout-tip icon="false" title="命題：尤度は単調減少する [@Dempster+1977]^[[@Robert-Casella2004 p.177] 定理5.15，[@Robert2007 p.334] 演習6.52．]"}

$\{\wh{\theta}_{(j)}\}\subset\Theta$ を EM アルゴリズムの $M$-ステップでの出力列とする．このとき，

$$
L(\wh{\theta}_{(j+1)}|x)\ge L(\wh{\theta}_{(j)}|x).
$$
等号成立は
$$
Q(\wh{\theta}_{(j+1)}|\wh{\theta}_{(j)},x)=Q(\wh{\theta}_{(j)}|\wh{\theta}_{(j)},x)
$$
の場合のみ．

:::

::: {.callout-note icon="false" collapse="true" title="証明"}

:::

::: {.callout-tip icon="false" title="命題：局所解への収束 [@Boyles1983]-[@Wu1983]^[[@Robert-Casella2004 p.178] 定理5.16．]"}

$$
Q(\theta|\theta_0,x):=\int_\cZ p(z|\theta,x)\log p(\theta|x,z)\,dz
$$
は $\theta,\theta_0\in\Theta$ について連続であるとする．このとき，EM アルゴリズムの出力 $\{\wh{\theta}_(j)\}$ は尤度 $p(\theta|x)$ の停留点に単調に収束する．

:::

::: {.callout-note icon="false" collapse="true" title="証明"}

:::

よって，EM アルゴリズムは局所解には収束する．

しかし，常に尤度が単調増加するという性質上，局所解に囚われてしまった場合，そこから逃れることはないことになる．

大域解に収束することを保証したい場合は，異なる初期値で複数回 EM アルゴリズムを実行するか，擬似アニーリングなどの別の手法を用いることを考える必要がある [@Finch+1989]．


## EM アルゴリズムの実装

### Guass 有限混合モデル

ここでは，以下の，有限な混合モデルで，さらに混合される分布は正規であるものを考える：

::: {.callout-tip icon="false" title="定義 (Gaussian finite mixture model)"}

集合 $[K]$ 上に値を取る隠れ変数 $Z$ の確率質量関数を $(p_k)_{k=1}^K$ とする．
$$
p(x;(\mu_k),\sigma,(p_k))=\sum_{k=1}^K p_k\phi(x;\mu_k,\sigma)
$$ {#eq-1}
として定まるモデル $(p_{(\mu_k),\sigma,(p_k)})$ を **Gauss 有限混合モデル** という．

ただし，$\phi(x$; $\mu$, $\sigma)$ は $\rN(\mu,\sigma^2)$ の密度とした．

:::

@eq-1 は $(X,Z)$ 上の結合分布の族を表しており，そのパラメータは $\theta:=((\mu_k),\sigma,(p_k))$ である．

尤度は，@eq-2 の $\cZ=[K]$，
$$
\begin{align*}
    p(x|\theta)&=\sum_{k=1}^K \P[Z=k]\phi(x|\mu_k,\sigma)\\
    p(x,z|\theta)&=\phi(x|\mu_k,\sigma)
\end{align*}
$$
に当たる場合である．

