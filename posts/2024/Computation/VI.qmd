---
title: "変分推論（１）EM アルゴリズム"
author: "Draft Draft"
date: 2/3/2024
categories: [Computation, Math Notes, Python]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
highlight-style: github
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，変分推論の基本的な考え方を説明する．今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムを扱う．さらにその特殊な場合である $K$-平均法から説明を始める．
---

{{< include ../../../_preamble.qmd >}}

[@Blei+2017]

## $K$-平均アルゴリズム

まずは $K$-平均アルゴリズムによるクラスタリングを，Python により実演しながら，このアルゴリズムの考え方と問題点をみる．次の章で，$K$-平均アルゴリズムの統計的な一般化である EM アルゴリズムを説明し，その問題点の数理的な理解を目指す．

### 用いるデータ

次のような２次元のデータを考える．

```{python}
#| echo: false
import pandas as pd
data = pd.read_csv('mixture1.dat', delimiter="\t" , header=None)
data_2d = data.iloc[:, 1:3].to_numpy()
print(data)
```

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

markers = data.iloc[:, 0]
x = data.iloc[:, 1]
y = data.iloc[:, 2]

plt.figure(figsize=(4, 3))
for marker in np.unique(markers):
    plt.scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}',
                marker=f'${marker}$') 

plt.title('Scatter Plot of the Data')
plt.xlabel('x') 
plt.ylabel('y')
plt.show()
```

### ハード $K$-平均法

head $K$-means はデータ $\{x^{(n)}\}_{n=1}^N\subset\R^I$ とクラスタ数 $K\in\N^+$，そして初期値 $(m^{(k)})_{k=1}^K\in(\R^I)^K$ がパラメータである．soft $K$-means アルゴリズムはさらに硬度パラメータ $\beta\in\R_+$ を持つ．

特に `numpy` の提供する行列積を利用して，これを Python により実装した例を以下に示す．ソフト $K$-平均法の実装と対比できるように，負担率を通じた実装を行う．

```{python}
#| echo: false
import math

def d(x,y):
    """
    ２次元での Euclid 距離を計算する関数

    Parameter:
    - x,y: (2,)-numpy.ndarray
    """
    return math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)

def normalize(x):
    """
    ラベル番号データを昇順にする関数

    Parameter:
    - x: (N,)-numpy.ndarray
    """
    labels = [x[0]]
    for i in x:
        if i not in labels:
            labels.append(i)

    conditions = [x == labels[i] for i in range(len(labels))]
    choices = [i + 1 for i in range(len(labels))]

    return np.select(conditions, choices)

def accuracy(ans, pred):
    """
    正解値と予測値を比べ，正答数と正解率を print する関数

    Parameters:
    - ans: (N,)-numpy.ndarray 正解値
    - pred: (N,)-numpy.ndarray 予測値
    """
    num_correct = np.sum(ans == pred)
    accuracy = num_correct / len(ans)
    return num_correct, accuracy
```

```{python}
#| lst-label: lst-hard-k-means
#| lst-cap: ハード K-平均法の実装
def hkmeans_2d(data, K, init, max_iter=100):
    """
    ２次元データに対するハード K-平均法の実装．

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値

    Returns:
    - clusters: (N,)-numpy.ndarray クラスター番号
    """
    N = data.shape[0]  # データ数
    I = data.shape[1]  # 次元数 今回は２
    m = init  # クラスター中心の初期化．2×K行列．
    r = np.zeros((K, N))  # 負担率．K×N行列．

    for _ in range(max_iter):
        # Assignment Step
        for i in range(N):
            distances = np.array([d(data[i], m[:,j]) for j in range(K)]) # (N,)-numpy.ndarray になる．d は Euclid 距離として定義済み．
            k_hat = np.argmin(distances)  # 最小距離のクラスター番号
            r[k_hat,i] = 1
        
        # Update Step
        new_m = np.zeros_like(m)
        numerator = np.dot(r, data)  # (K,2)-numpy.ndarray
        denominator = np.sum(r, axis=1)  # 各クラスターの負担率の和
        for k in range(K):
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = m[:,k]

        # 終了条件の確認
        if np.allclose(m, new_m):
            break
        m = new_m
    
    return np.argmax(r, axis=0)
```

#### 正解率の観察

まずは簡単に初期値
$$
\vctr{1}{1},\quad\vctr{2}{2},\quad\vctr{3}{3},
$$
を与えてみると，次の通りの結果を得る：

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[0,0],[1,1],[2,2]]).T)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
#| label: fig-1
#| fig-cap: ハード K-平均法によるクラスタリングの結果
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[1,1],[2,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

赤い点が初期値である．**ハード $K$-平均アルゴリズムは初期値に敏感である** ことがよく分かる．

別の初期値を与えてみる．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[0,0],[2,4],[4,2]]).T)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
#| label: fig-2
#| fig-cap: ハード K-平均法によるクラスタリングの結果
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[2,4],[4,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

これはひどい．

#### リトライ

そこで，答えに近いように，
$$
\vctr{-1}{-1},\quad\vctr{1}{-2},\quad\vctr{2.5}{2},
$$
を初期値として与えてみて，正答率の変化を観察する．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[-1,-1],[1,-2],[2.5,2]]).T)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[-1,-1],[1,-2],[2.5,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

#### リトライ２

惜しい！！ １箇所だけ明らかに間違えている！何とかこれを消せないか！？
左下の赤い丸をより下からスタートさせることで，青のクラスターに干渉しないことを目指す：
$$
\vctr{-1}{-1}\mapsto\vctr{-1}{-2}
$$
に変更．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[-1,-2],[1,-2],[2.5,2]]).T)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[-1,-2],[1,-2],[2.5,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')
axs[0].scatter(0.274, -2.433, facecolors='none', edgecolors='red', s=100)

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

あ，この最後のやつ（赤丸で囲った点）は無理な気がする．しかもミスは２つでした．

#### リトライ３度目の正直

ええい！もうヤツの隣から初めてやる！
$$
\vctr{-1}{-2}\mapsto\vctr{0}{-2}
$$
に変更．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[0,-2],[1,-2],[2.5,2]]).T)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,-2],[1,-2],[2.5,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

遂に正答率 100% ！！！

全くアホらしい．このような初期値の目視での調整は，正解を知っているから出来ることである上に，何よりデータの次元が上がった場合には殆ど不可能である．

### ソフト $K$-平均法

負担率の計算 $r_{kn}\gets\delta_{\argmax_{i\in[k]}d(m_i,x_n)}(k)$ を，
$$
\sigma(z;e)_i:=\frac{e^{z_i}}{\sum_{j=1}^Ke^{e_j}}\quad(i\in[K])
$$
で定まる [**ソフトマックス関数**](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0) $\sigma:\R^K\to(0,1)^K$ を用いて，「軟化」する．

ここでは，$\beta\ge0$ として，
$$
\sigma(z;e^{-\beta})_i=\frac{e^{-\beta z_i}}{\sum_{j=1}^Ke^{-\beta e_j}}
$$
の形で用い，$\argmax$ の代わりに
$$
\begin{align*}
    r_{kn}&\gets\sigma(d(-,x_n)\circ m;e^{-\beta})_k\\
    &=\frac{e^{-\beta d(m_k,x_n)}}{\sum_{j=1}^K e^{-\beta d(m_j,x_n)}}
\end{align*}
$$
とする．

$\beta$ は硬度 (stiffness) または逆温度と呼ぶ．$\beta=0$のとき，温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる$\beta\to\infty$の極限が hard $K$-means アルゴリズムに相当する．

```{.python #lst-soft-k-means lst-cap="ソフト K-平均法の実装（負担率計算の部分のみが違う）"}
for i in range(N):
        distances = np.array([d(data[i], m[:,j]) for j in range(K)]) # (N,)-numpy.ndarray
        denominator_ = np.sum(np.exp(-beta * distances))  # 分母
        r[:,i] = np.exp(-beta * distances) / denominator_
```

```{python}
#| echo: false
def skmeans_2d(data, K, init, beta, max_iter=100):
    """
    ２次元データに対するソフト K-平均法の実装．

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値
    - beta: float 硬度パラメータ

    Returns:
    - clusters: (N,)-numpy.ndarray クラスター番号
    """
    N = data.shape[0]  # データ数
    I = data.shape[1]  # 次元数 今回は２
    m = init  # クラスター中心の初期化．2×K行列．
    r = np.zeros((K, N))  # 負担率．K×N行列．

    for _ in range(max_iter):
        # Assignment Step
        for i in range(N):
            distances = np.array([d(data[i], m[:,j]) for j in range(K)]) # (N,)-numpy.ndarray
            denominator_ = np.sum(np.exp(-beta * distances))  # 分母
            r[:,i] = np.exp(- beta * distances) / denominator_
        
        # Update Step
        new_m = np.zeros_like(m)
        numerator = np.dot(r, data)  # (K,2)-numpy.ndarray
        denominator = np.sum(r, axis=1)  # 各クラスターの負担率の和
        for k in range(K):
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = m[:,k]

        # 終了条件の確認
        if np.allclose(m, new_m):
            break
        m = new_m
    
    return np.argmax(r, axis=0)
```

#### 正解率の観察

逆温度を $\beta=1$ としてみる．@fig-1 と全く同様な初期値
$$
\vctr{1}{1},\quad\vctr{2}{2},\quad\vctr{3}{3},
$$
を与えてみると，次の通りの結果を得る：

```{python}
#| echo: false
pred = skmeans_2d(data_2d, 3, np.array([[0,0],[1,1],[2,2]]).T, 1)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[1,1],[2,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

正解率 70% であったところから少し改善している．

@fig-2 で与えた初期値も与えてみる．

```{python}
#| echo: false
pred = skmeans_2d(data_2d, 3, np.array([[0,0],[2,4],[4,2]]).T, 1)
ans = data.iloc[:, 0]
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}
#| echo: false
markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[2,4],[4,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Marker {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()
```

この場合はハード $K$-平均法では 47.8% であったから，大きく改善されていることが判る．

## EM アルゴリズム