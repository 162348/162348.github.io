---
title: "変分推論（１）EM アルゴリズム"
author: "Draft Draft"
date: 2/3/2024
categories: [Computation, Math Notes, Python]
toc: true
number-sections: true
code-block-bg: true
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，変分推論の基本的な考え方を説明する．今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムを扱う．さらにその特殊な場合である $K$-平均法から説明を始める．
---

{{< include ../../../_preamble.qmd >}}

[@Blei+2017]

## $K$-平均アルゴリズム

まずは $K$-平均アルゴリズムによるクラスタリングを，Python により実演しながら，このアルゴリズムの考え方と問題点をみる．次の章で，$K$-平均アルゴリズムの統計的な一般化である EM アルゴリズムを説明し，その問題点の数理的な理解を目指す．

### 用いるデータ

次のような２次元のデータを考える．

```{python}
#| echo: false
import pandas as pd
data = pd.read_csv('mixture1.dat', delimiter="\t" , header=None)
data_2d = data.iloc[:, 1:3].to_numpy()
print(data)
```

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

markers = data.iloc[:, 0]
x = data.iloc[:, 1]
y = data.iloc[:, 2]

plt.figure(figsize=(4, 3))
for marker in np.unique(markers):
    plt.scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$') 

plt.title('Scatter Plot of the Data')
plt.xlabel('x') 
plt.ylabel('y')
plt.show()
```

### ハード $K$-平均法

head $K$-means はデータ $\{x^{(n)}\}_{n=1}^N\subset\R^I$ とクラスタ数 $K\in\N^+$，そして初期値 $(m^{(k)})_{k=1}^K\in(\R^I)^K$ がパラメータである．soft $K$-means アルゴリズムはさらに硬度パラメータ $\beta\in\R_+$ を持つ．

特に `numpy` の提供する行列積を利用して，これを Python により実装した例を以下に示す．ソフト $K$-平均法の実装と対比できるように，負担率を通じた実装を行う．

```{python}
#| echo: false
import math

def d(x,y):
    """
    ２次元での Euclid 距離を計算する関数

    Parameter:
    - x,y: (2,)-numpy.ndarray
    """
    return math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)

def normalize(x):
    """
    ラベル番号データを昇順にする関数

    Parameter:
    - x: (N,)-numpy.ndarray
    """
    labels = [x[0]]
    for i in x:
        if i not in labels:
            labels.append(i)

    conditions = [x == labels[i] for i in range(len(labels))]
    choices = [i + 1 for i in range(len(labels))]

    return np.select(conditions, choices)

def accuracy(ans, pred):
    """
    正解値と予測値を比べ，正答数と正解率を print する関数

    Parameters:
    - ans: (N,)-numpy.ndarray 正解値
    - pred: (N,)-numpy.ndarray 予測値
    """
    num_correct = np.sum(ans == pred)
    accur = num_correct / len(ans)
    return num_correct, accur
```

```{python}
#| lst-label: lst-hard-k-means
#| lst-cap: ハード K-平均法の実装
def hkmeans_2d(data, K, init, max_iter=100):
    """
    ２次元データに対するハード K-平均法の実装．

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値

    Returns:
    - clusters: (N,)-numpy.ndarray クラスター番号
    """
    N = data.shape[0]  # データ数
    I = data.shape[1]  # 次元数 今回は２
    m = init  # クラスター中心の初期化．2×K行列．
    r = np.zeros((K, N))  # 負担率．K×N行列．

    for _ in range(max_iter):
        # Assignment Step
        for i in range(N):
            distances = np.array([d(data[i], m[:,j]) for j in range(K)]) # (N,)-numpy.ndarray になる．d は Euclid 距離として定義済み．
            k_hat = np.argmin(distances)  # 最小距離のクラスター番号
            r[k_hat,i] = 1
        
        # Update Step
        new_m = np.zeros_like(m)
        numerator = np.dot(r, data)  # (K,2)-numpy.ndarray
        denominator = np.sum(r, axis=1)  # 各クラスターの負担率の和
        for k in range(K):
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = m[:,k]

        # 終了条件の確認
        if np.allclose(m, new_m):
            break
        m = new_m
    
    return np.argmax(r, axis=0)
```

#### 正解率の観察

まずは簡単に初期値
$$
\vctr{1}{1},\quad\vctr{2}{2},\quad\vctr{3}{3},
$$
を与えてみると，次の通りの結果を得る：

```{python}
#| echo: false
#| label: fig-1
#| fig-cap: ハード K-平均法によるクラスタリングの結果
pred = hkmeans_2d(data_2d, 3, np.array([[0,0],[1,1],[2,2]]).T)
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[1,1],[2,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

赤い点が初期値である．**ハード $K$-平均アルゴリズムは初期値に敏感である** ことがよく分かる．

別の初期値を与えてみる．

```{python}
#| echo: false
#| label: fig-2
#| fig-cap: ハード K-平均法によるクラスタリングの結果
pred = hkmeans_2d(data_2d, 3, np.array([[0,0],[2,4],[4,2]]).T)
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[2,4],[4,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

これはひどい．

#### リトライ

そこで，答えに近いように，
$$
\vctr{-1}{-1},\quad\vctr{1}{-2},\quad\vctr{2.5}{2},
$$
を初期値として与えてみて，正答率の変化を観察する．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[-1,-1],[1,-2],[2.5,2]]).T)
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[-1,-1],[1,-2],[2.5,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

#### リトライ２

惜しい！！ １箇所だけ明らかに間違えている！何とかこれを消せないか！？
左下の赤い丸をより下からスタートさせることで，青のクラスターに干渉しないことを目指す：
$$
\vctr{-1}{-1}\mapsto\vctr{-1}{-2}
$$
に変更．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[-1,-2],[1,-2],[2.5,2]]).T)
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[-1,-2],[1,-2],[2.5,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')
axs[0].scatter(0.274, -2.433, facecolors='none', edgecolors='red', s=100)

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

あ，この最後のやつ（赤丸で囲った点）は無理な気がする．しかもミスは２つでした．

#### リトライ３度目の正直

ええい！もうヤツの隣から初めてやる！
$$
\vctr{-1}{-2}\mapsto\vctr{0}{-2}
$$
に変更．

```{python}
#| echo: false
pred = hkmeans_2d(data_2d, 3, np.array([[0,-2],[1,-2],[2.5,2]]).T)
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,-2],[1,-2],[2.5,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of hard K-means')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

遂に正答率 100% ！！！

全くアホらしい．このような初期値の目視での調整は，正解を知っているから出来ることである上に，何よりデータの次元が上がった場合には殆ど不可能である．

### ソフト $K$-平均法

負担率の計算 $r_{kn}\gets\delta_{\argmax_{i\in[k]}d(m_i,x_n)}(k)$ を，
$$
\sigma(z;e)_i:=\frac{e^{z_i}}{\sum_{j=1}^Ke^{e_j}}\quad(i\in[K])
$$
で定まる [**ソフトマックス関数**](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0) $\sigma:\R^K\to(0,1)^K$ を用いて，「軟化」する．

ここでは，$\beta\ge0$ として，
$$
\sigma(z;e^{-\beta})_i=\frac{e^{-\beta z_i}}{\sum_{j=1}^Ke^{-\beta e_j}}
$$
の形で用い，$\argmax$ の代わりに
$$
\begin{align*}
    r_{kn}&\gets\sigma(d(-,x_n)\circ m;e^{-\beta})_k\\
    &=\frac{e^{-\beta d(m_k,x_n)}}{\sum_{j=1}^K e^{-\beta d(m_j,x_n)}}
\end{align*}
$$
とする．

$\beta$ は硬度 (stiffness) または逆温度と呼ぶ．$\beta=0$ のとき，温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる $\beta\to\infty$ の極限が hard $K$-means アルゴリズムに相当する．

```{.python #lst-soft-k-means lst-cap="ソフト K-平均法の実装（負担率計算の部分のみが違う）"}
for i in range(N):
        distances = np.array([d(data[i], m[:,j]) for j in range(K)]) # (N,)-numpy.ndarray
        denominator_ = np.sum(np.exp(-beta * distances))  # 分母
        r[:,i] = np.exp(-beta * distances) / denominator_
```

```{python}
#| echo: false
def skmeans_2d(data, K, init, beta, max_iter=100):
    """
    ２次元データに対するソフト K-平均法の実装．

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値
    - beta: float 硬度パラメータ

    Returns:
    - clusters: (N,)-numpy.ndarray クラスター番号
    """
    N = data.shape[0]  # データ数
    I = data.shape[1]  # 次元数 今回は２
    m = init  # クラスター中心の初期化．2×K行列．
    r = np.zeros((K, N))  # 負担率．K×N行列．

    for _ in range(max_iter):
        # Assignment Step
        for i in range(N):
            distances = np.array([d(data[i], m[:,j]) for j in range(K)]) # (N,)-numpy.ndarray
            denominator_ = np.sum(np.exp(-beta * distances))  # 分母
            r[:,i] = np.exp(- beta * distances) / denominator_
        
        # Update Step
        new_m = np.zeros_like(m)
        numerator = np.dot(r, data)  # (K,2)-numpy.ndarray
        denominator = np.sum(r, axis=1)  # 各クラスターの負担率の和
        for k in range(K):
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = m[:,k]

        # 終了条件の確認
        if np.allclose(m, new_m):
            break
        m = new_m
    
    return np.argmax(r, axis=0), r
```

```{python}
#| echo: false

class kmeans_2d:
    """
    ２次元データに対するソフト K-平均法のイテレータクラスとしての実装．

    Usage:
        kmeans = kmeans_2d(data, K, init, beta)
        kmeans.run()

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値
    - beta: float 硬度パラメータ
    """
    def __init__(self, data, K, init, beta, max_iter=100):
        self.data = data
        self.K = K
        self.init = init
        self.beta = beta
        self.max_iter = max_iter
        self.N = data.shape[0]  # データ数
        self.I = data.shape[1]  # 次元数 今回は２
        self.m = init  # クラスター中心の初期化．2×K行列．
        self.r = np.zeros((K, self.N))  # 負担率．K×N行列．
        self.history = [init.copy()] # クラスター中心の履歴．2×K行列．
    
    def soft_assigment(self):
        """負担率の更新"""
        for i in range(self.N):
            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray
            denominator_ = np.sum(np.exp(-self.beta * distances))  # 分母
            self.r[:,i] = np.exp(- self.beta * distances) / denominator_
    
    def update(self):
        """クラスター中心の更新"""
        new_m = np.zeros_like(self.m)
        numerator = np.dot(self.r, self.data)  # (K,2)-numpy.ndarray
        denominator = np.sum(self.r, axis=1)  # 各クラスターの負担率の和
        for k in range(self.K):
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = self.m[:,k]
        self.m = new_m

    def fetch_cluster(self):
        return np.argmax(self.r, axis=0)
    
    def fetch_history(self):
        return np.stack(self.history, axis=0)

    def run_soft(self):
        for _ in range(self.max_iter):
            self.soft_assigment()
            self.update()
            self.history.append(self.m.copy())
            if np.allclose(self.history[-1], self.history[-2]):
                break
```

```{python}
result = kmeans_2d(data_2d, 3, np.array([[-3,-2],[4,2],[-2,2]]).T, 1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

```{python}

plt.figure(figsize=(8, 6))

# 各点に対して軌跡をプロット
for i in range(history.shape[2]):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    plt.plot(x_coords, y_coords, '-o', label=f'Point {i+1}')

# グラフの装飾
plt.title('Temporal Changes of Three Points in 2D Space')
plt.xlabel('X Coordinate')
plt.ylabel('Y Coordinate')
plt.legend()
plt.grid(True)
plt.show()
```

#### 正解率の観察

逆温度を $\beta=1$ としてみる．@fig-1 と全く同様な初期値
$$
\vctr{0}{0},\quad\vctr{1}{1},\quad\vctr{2}{2},
$$
を与えてみると，次の通りの結果を得る：

```{python}
#| echo: false
#| fig-cap: ソフト K-平均法（$\beta=1$）によるクラスタリングの結果．赤い丸が初期値で，その後のクラスター中心 CoC (Center of Cluster) の推移が表現されている．
result = kmeans_2d(data_2d, 3, np.array([[0,0],[1,1],[2,2]]).T, 1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[1,1],[2,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

rearrange = [2,1,0]
for i in rearrange:  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, '-o', label=f'CoC {3-i}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

正解率 70% であったところから少し改善している．

@fig-2 で与えた初期値も与えてみる．

```{python}
#| echo: false
#| label: fig-3
#| fig-cap: ソフト K-平均法（$\beta=1$）によるクラスタリングの結果
result = kmeans_2d(data_2d, 3, np.array([[0,0],[2,4],[4,2]]).T, 1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[2,4],[4,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

rearrange = [2,1,0]
for i in rearrange:  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, '-o', label=f'CoC {3-i}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

この場合はハード $K$-平均法では 47.8% であったから，大きく改善されていることが判る．図 @fig-2 との差は，

1. クラスター１がより大きな範囲を占めているため，正解率の上昇に寄与している．
2. 図 @fig-2 ではクラスター３が海峡を渡りクラスター２を侵食していたが，それがみられない．

の主に２点が観察されるが，これは $\beta=1$ による「軟化」の結果，$(2,4)^\top$ からスタートしたクラスター中心がしっかりと手前の大陸を制圧することに成功するためであると考えられる．

#### 硬度パラメータの変化

逆温度を $\beta=0.1$ としてみる．

```{python}
#| echo: false
result = kmeans_2d(data_2d, 3, np.array([[0,0],[2,4],[4,2]]).T, 0.1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[2,4],[4,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

rearrange = [2,1,0]
for i in rearrange:  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, '-o', label=f'CoC {3-i}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

全てが同じクラスターに属する結果となってしまった．温度が大変に高い状態に相当し，全てが乱雑で，３つのクラスターが一様・公平に負担率を持つようになった．そのため，関数 `argmax` が全ての $k$ に対して同じ値を返してしまっているのである．

次に，逆温度を $\beta=10$ としてみる．

```{python}
#| echo: false
result = kmeans_2d(data_2d, 3, np.array([[0,0],[2,4],[4,2]]).T, 10)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット
initial_points = np.array([[0,0],[2,4],[4,2]])

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x[markers_pred == marker], y[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

rearrange = [2,1,0]
for i in rearrange:  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, '-o', label=f'CoC {3-i}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers):
    axs[1].scatter(x[markers == marker], y[markers == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].set_title('Answer')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
print(f'正解数: {num_correct}')
print(f'正解率: {accr*100:.1f} %')
```

正答率が $\beta=1$ の場合 @fig-3 から下がっているのは（それでも @fig-2 の47.8% よりは少し高い），クラスター２の制圧範囲が広くなっているためである．^[なお，$\beta=100$ と設定しても結果は変わらなかった．]

しかしこれは，右上の島を仮に２つのクラスターに分けるならば，より理想的な分け方になっているとも捉えられる．そこで，左下の島を２つに分けさせるために，初期値を変更してみる．

#### $\beta=1$ vs. $\beta=10$

```{python}
#| echo: false
#| fig-cap: ソフト K-平均法によるクラスタリングの結果の比較（$\beta=1$ vs. $\beta=10$）．
initial_points = np.array([[0,0],[-1,-2],[-2,-3]])
result1 = kmeans_2d(data_2d, 3, initial_points.T, 1)
result2 = kmeans_2d(data_2d, 3, initial_points.T, 10)
result1.run_soft()
result2.run_soft()
pred1 = result1.fetch_cluster()
pred2 = result2.fetch_cluster()
history1 = result1.fetch_history()
history2 = result2.fetch_history()
ans = data.iloc[:, 0]

markers_pred1 = normalize(pred1)
markers_pred2 = normalize(pred2)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x[markers_pred1 == marker], y[markers_pred1 == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of soft K-means with beta=1')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x[markers_pred2 == marker], y[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[1].set_title('Result of soft K-means with beta=10')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize(pred1))
num_correct2, accr2 = accuracy(ans, normalize(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}')
print(f'正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %')
```

```{python}
#| echo: false
#| fig-cap: ソフト K-平均法によるクラスタリングの結果の比較（$\beta=1$ vs. $\beta=10$）．
initial_points = np.array([[0,-3],[-1,-2],[-2,-3]])
pred1, r = skmeans_2d(data_2d, 3, initial_points.T, 1)
pred2, r = skmeans_2d(data_2d, 3, initial_points.T, 10)
ans = data.iloc[:, 0]

markers_pred1 = normalize(pred1)
markers_pred2 = normalize(pred2)
fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x[markers_pred1 == marker], y[markers_pred1 == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[0].set_title('Result of soft K-means with beta=1')
axs[0].set_xlabel('x')
axs[0].set_ylabel('y')
axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x[markers_pred2 == marker], y[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$')  # markerパラメータによって形状を指定

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], color='red')

axs[1].set_title('Result of soft K-means with beta=10')
axs[1].set_xlabel('x')
axs[1].set_ylabel('y')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize(pred1))
num_correct2, accr2 = accuracy(ans, normalize(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}')
print(f'正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %')
```

## EM アルゴリズム