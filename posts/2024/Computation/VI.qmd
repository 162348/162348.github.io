---
title: "変分推論１"
subtitle: "K-平均アルゴリズム"
author: "司馬博文"
date: 2/3/2024
date-modified: 2/17/2024
categories: [Computation, Python]
toc: true
number-sections: true
code-block-bg: true
code-overflow: wrap
code-fold: true
code-annotations: select
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
    fig-prefix: 図
    fig-title: 図
abstract-title: 概要
abstract: 数学者のために，変分推論の基本的な考え方を説明するシリーズであるが，今回は変分 Bayes アルゴリズムの特殊な場合である EM アルゴリズムの，さらにその特殊な場合である $K$-平均法の説明から始める．
---

{{< include ../../../_preamble.qmd >}}

本稿では，[$K$-平均アルゴリズム](https://ja.wikipedia.org/wiki/K%E5%B9%B3%E5%9D%87%E6%B3%95) [@Lloyd1982] によるクラスタリングの考え方と問題点を，Python による実演を通じてみる．次の [稿](VI2.qmd) で，$K$-平均アルゴリズムの統計モデリングの観点からの一般化である [EM アルゴリズム](https://ja.wikipedia.org/wiki/EM%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0) を説明し，その問題点の数理的な理解を目指す．

より図が見やすい PDF 版は [こちら](report1.pdf)．

## 用いるデータ {#sec-data}

次のような２次元のデータ（＋最左列に教師データ付き）を考える．

```{python}
#| echo: false
import pandas as pd
data1 = pd.read_csv('mixture1.dat', delimiter="\t" , header=None)
data1_2d = data1.iloc[:, 1:3].to_numpy()
print(data1)
```

```{python}
#| echo: false
import matplotlib.pyplot as plt
import numpy as np

markers1 = data1.iloc[:, 0]
x1 = data1.iloc[:, 1]
y1 = data1.iloc[:, 2]

plt.figure(figsize=(3.5, 3))
for marker in np.unique(markers1):
    plt.scatter(x1[markers1 == marker], y1[markers1 == marker], label=f'Cluster {marker}', marker=f'${marker}$') 

plt.title('Scatter Plot of the Data')
plt.show()
```

## ハード $K$-平均法 {#sec-hard-k-means}

### アルゴリズムの説明

head $K$-means algorithm はデータ $\{x^{(n)}\}_{n=1}^N\subset\R^I$ とクラスタ数 $K\in\N^+$，そして初期クラスター中心 $(m^{(k)})_{k=1}^K\in(\R^I)^K$ の３組をパラメータに持つ．

soft $K$-means algorithm はさらに硬度パラメータ $\beta\in\R_+$ を持つ．

特に `numpy` の提供する行列積を利用して，これを Python により実装した例を以下に示す．

ソフト $K$-平均法の実装と対比できるように，負担率を通じた実装を意識した例である．

アノテーションを付してあるので，該当箇所（右端の丸囲み数字）をクリックすることで適宜解説が読めるようになっている．

```{python}
#| echo: false
import math

def d(x,y):
    """
    ２次元での Euclid 距離を計算する関数

    Parameter:
    - x,y: (2,)-numpy.ndarray
    """
    return math.sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)

def normalize(x):
    """
    ラベル番号データを昇順にする関数

    Parameter:
    - x: (N,)-numpy.ndarray
    """
    labels = [x[0]]
    for i in x:
        if i not in labels:
            labels.append(i)

    conditions = [x == labels[i] for i in range(len(labels))]
    choices = [i + 1 for i in range(len(labels))]

    return np.select(conditions, choices)

def normalize_abnormal(x):
    """
    ラベル番号データを昇順にする関数

    Parameter:
    - x: (N,)-numpy.ndarray
    """
    labels = [x[0]]
    for i in x:
        if i not in labels:
            labels.append(i)

    conditions = [x == labels[i] for i in range(len(labels))]
    choices = [1,3,2]

    return np.select(conditions, choices)

def accuracy(ans, pred):
    """
    正解値と予測値を比べ，正答数と正解率を print する関数

    Parameters:
    - ans: (N,)-numpy.ndarray 正解値
    - pred: (N,)-numpy.ndarray 予測値
    """
    num_correct = np.sum(ans == pred)
    accur = num_correct / len(ans)
    return num_correct, accur
```

```{.python}
def hkmeans_2d(data, K, init, max_iter=100):
    """
    ２次元データに対するハード K-平均法の実装例．

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値

    Returns:
    - clusters: (N,)-numpy.ndarray クラスター番号
    """

    N = data.shape[0]  # <1>
    I = data.shape[1]  # <2>
    m = init  # <3>
    r = np.zeros((K, N), dtype=float)  # <4>

    for _ in range(max_iter):
        # Assignment Step
        for i in range(N):
            distances = np.array([d(data[i], m[:,k]) for k in range(K)]) # <5>
            k_hat = np.argmin(distances)  # <6>
            r[:,i] = 0  # <7>
            r[k_hat,i] = 1
        
        # Update Step
        new_m = np.zeros_like(m, dtype=float) # <8>
        numerator = np.dot(r, data)  # <9>
        denominator = np.sum(r, axis=1) # <10>
        for k in range(K): # <11>
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = m[:,k]

        if np.allclose(m, new_m): # <12>
            break
        m = new_m
    
    return np.argmax(r, axis=0)  # <13>
```

1. データ数を取得している．
2. データの次元を取得している．今回はすべて２次元データを用いる．
3. クラスター中心に引数として受け取った初期値を代入. $2×K$-行列であることに注意．
4. 負担率を $K×N$-行列として格納している．その理由は後ほど行列積を通じた計算を行うためである．`dtype=float` の理由は後述．
5. この `distances` 変数は `(K,)-numpy.ndarray` になる．すなわち，第 $k$ 成分が，第 $k$ クラスター中心との距離となっているようなベクトルである．ただし，`d` は Euclid 距離を計算する関数として定義済みとした．
6. 距離が最小となるクラスター番号 $\hat{k}:=[\argmin_{k\in[K]}d(m_k,x_i)]$ を，$i\in[N]$ 番目のデータについて求める．
7. $\hat{k}$ に基づいて負担率を更新するが，ループ内で前回の結果をリセットする必要があることに注意．
8. ここで `dtype=float` と指定しないと，初め引数 `init` が整数のみで構成されていた場合に，Python の自動型付機能が `int` 型だと判定し，クラスター中心 `m` の値が整数に限られてしまう．すると，アルゴリズムがすぐに手頃な格子点に収束してしまう．
9. `numpy` の行列積を計算する関数 `np.dot` を使用している．更新式
$$
m^{(k)}\gets\frac{\sum_{n=1}^Nr^{(n)}_kx^{(n)}}{\sum_{n=1}^Nr^{(n)}_k}
$$
の分子を行列積と見たのである．
10. 分母 (denominator) は $(K,N)$-行列 `r` の行和として得られる．
11. ゼロによる除算が起こらないように場合わけをしている．
12. クラスター中心がもはや変わらない場合はアルゴリズムを終了する．
13. 負担率の最も大きいクラスター番号を返す．今回は `hat_k` の列をそのまま返せば良いが，soft $K$-means アルゴリズムにも通じる形で実装した．

::: {.callout-caution icon="false" title="注：実際に用いる実装" collapse="true"}

ただし，本記事の背後では次の実装を用いる．

クラスター中心の推移のヒストリーを保存して図示に利用したり，負担率 `r` の中身を見たりすることが出来るようにするため，assignment step と update step とに分けてクラスメソッドとして実装し，`run` メソッドでそれらを呼び出すようにしている．これに `fetch_cluster` と `fetch_history` メソッドを加えることで，クラスター番号とクラスター中心の推移を取得することが出来る．フィールド `.r` から（最終的な）負担率を見ることもできる．

```{python}
#| code-fold: true
class kmeans_2d:
    """
    ２次元データに対するソフト K-平均法の実装．

    Usage:
        kmeans = kmeans_2d(data, K, init, beta)
        kmeans.run()

    Parameters:
    - data: (N,2)-numpy.ndarray
    - K: int クラスター数
    - init: (2,K)-numpy.ndarray 初期値
    - beta: float 硬度パラメータ
    """

    def __init__(self, data, K, init, beta, max_iter=100):
        self.data = np.array(data, dtype=float)
        self.K = K
        self.init = np.array(init, dtype=float)
        self.beta = float(beta)
        self.max_iter = max_iter
        self.N = data.shape[0]  # データ数
        self.I = data.shape[1]  # 次元数 今回は２
        self.m = init  # クラスター中心の初期化．2×K行列．
        self.r = np.zeros((K, self.N), dtype=float)  # 負担率．K×N行列．
        self.history = [init.copy()] # クラスター中心の履歴．2×K行列．
    
    def soft_assigment(self):
        """soft K-means の場合の負担率の更新"""
        for i in range(self.N):
            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray
            denominator_ = np.sum(np.exp(-self.beta * distances))  # 分母
            self.r[:,i] = np.exp(- self.beta * distances) / denominator_

    def hard_assigment(self):
        """hard K-means の場合の負担率の更新"""
        for i in range(self.N):
            distances = np.array([d(self.data[i], self.m[:,j]) for j in range(self.K)]) # (N,)-numpy.ndarray
            k_hat = np.argmin(distances)  # 最小距離のクラスター番号
            self.r[:,i] = 0  # 前のループの結果をリセット
            self.r[k_hat,i] = 1
    
    def update(self):
        """クラスター中心の更新"""
        new_m = np.zeros_like(self.m, dtype=float) # ここで float にしないと，クラスター中心が整数に限られてしまう．
        numerator = np.dot(self.r, self.data)  # (K,2)-numpy.ndarray
        denominator = np.sum(self.r, axis=1)  # 各クラスターの負担率の和
        for k in range(self.K):
            if denominator[k] > 0:
                new_m[:,k] = numerator[k] / denominator[k]
            else:
                new_m[:,k] = self.m[:,k]
        self.m = new_m

    def fetch_cluster(self):
        """最終的なクラスター番号を格納した (N,)-array を返す"""
        return np.argmax(self.r, axis=0)
    
    def fetch_history(self):
        """クラスター中心の履歴を格納したリストを，３次元の np.array に変換して返す"""
        return np.stack(self.history, axis=0)

    def run_soft(self):
        """soft K-means アルゴリズムの実行"""
        for _ in range(self.max_iter):
            self.soft_assigment()
            self.update()
            self.history.append(self.m.copy())
            if np.allclose(self.history[-1], self.history[-2]):
                break
    
    def run_hard(self):
        """hard K-means アルゴリズムの実行"""
        for _ in range(self.max_iter):
            self.hard_assigment()
            self.update()
            self.history.append(self.m.copy())
            if np.allclose(self.history[-1], self.history[-2]):
                break
```

なお，この実装は $\beta\ge500$ などの場合にオーバーフローが起こることに注意．これへの対処は `logsumexp` の使用などが考えられる．

:::

### 正解率の観察

次の２つの初期値を与えてみる．
$$
m_1:=\vctr{4}{0},\quad m_2:=\vctr{1}{4},\quad m_3=\vctr{-1}{1},
$$
と，$m_2,m_3$ は変えずに $m_1$ の $y$-座標を $1$ だけ下げたもの
$$
m_1':=\vctr{4}{-1}
$$
とを初期値として与えてみる．

```{python}
#| echo: false
#| label: fig-1
#| fig-cap: ハード K-平均法によるクラスタリングの結果．初期値は $(m_1,m_2,m_3)=\paren{\vctr{4}{0},\vctr{1}{4},\vctr{-1}{1}}$．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．
initial_points = np.array([[-1,1],[1,4],[4,0]])

result = kmeans_2d(data1_2d, 3, initial_points.T, 0.1)
result.run_hard()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [2,1,0]
default = [0,1,2]
markerstyle = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markerstyle[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of hard K-means')

axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers1):
    axs[1].scatter(x1[markers1 == marker], y1[markers1 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

axs[1].set_title('Answer')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
iteration = history.shape[0]
print(f'正解数: {num_correct}     正解率: {accr*100:.1f} %     反復数: {iteration} 回')
```

別の初期値を与えてみる（右下の点 $m_1$ を $1$ だけ下に下げただけ）：
$$
\vctr{4}{0}=m_1\mapsto m_1':=\vctr{4}{-1}
$$

```{python}
#| echo: false
#| label: fig-2
#| fig-cap: ハード K-平均法によるクラスタリングの結果．初期値は $(m_1',m_2,m_3)=\paren{\vctr{4}{-1},\vctr{1}{4},\vctr{-1}{1}}$．

initial_points = np.array([[-1,1],[1,4],[4,-1]])

result = kmeans_2d(data1_2d, 3, initial_points.T, 0.1)
result.run_hard()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markerstyle = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markerstyle[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of hard K-means')

axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers1):
    axs[1].scatter(x1[markers1 == marker], y1[markers1 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

axs[1].set_title('Answer')

axs[1].legend()

plt.tight_layout()
plt.show()

center_2 = history[-1, :, :]

num_correct, accr = accuracy(ans, normalize(pred))
iteration = history.shape[0]
print(f'正解数: {num_correct}     正解率: {accr*100:.1f} %     反復数: {iteration} 回')
```

結果が全く変わり，$(m_1',m_2,m_3)$ を与えた方が，大きく正解に近づいている．具体的には，右下の初期値 $m_1$ は右上の島に行くが，$m_1'$ は左下の島に行ってくれる．

**ハード $K$-平均アルゴリズムは初期値に敏感である** ことがよく分かる．

### 正解率を上げる試み

直前の結果ではクラスター２と３の境界線で４つのミスを犯しており，これを修正できないか試したい．

そこで，答えに近いように，
$$
m_1\gets\vctr{2.5}{2},\;\; m_2\gets\vctr{-1}{-1},\;\; m_3\gets\vctr{1}{-2},
$$
を初期値として与えてみて，正答率の変化を観察する．

```{python}
#| echo: false
initial_points = np.array([[-1,-1],[1,-2],[2.5,2]])
result = kmeans_2d(data1_2d, 3, initial_points.T, 0.1)
result.run_hard()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [2,0,1]
default = [0,1,2]
markerstyle = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markerstyle[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')

axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers1):
    axs[1].scatter(x1[markers1 == marker], y1[markers1 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

axs[1].set_title('Answer')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
iteration = history.shape[0]
print(f'正解数: {num_correct}     正解率: {accr*100:.1f} %     反復数: {iteration} 回')

center1 = history[-1, :, :]
```

もはや初期値から殆ど動いていないが，目標のクラスター３に分類された３つの点が，相変わらず３のままであり，加えてクラスター２の中心がこれらから逃げているようにも見えるので，クラスター２の初期値をよりクラスター３に近いように誘導し，クラスター３の中心をより右側から開始する：

$$
m_2:\vctr{-1}{-1}\mapsto\vctr{0}{-2}\;\; m_3:\vctr{1}{-2}\mapsto\vctr{2}{-2}
$$


```{python}
#| echo: false
initial_points = np.array([[0,-2],[2,-2],[2.5,2]])
result = kmeans_2d(data1_2d, 3, initial_points.T, 0.1)
result.run_hard()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [2,0,1]
default = [0,1,2]
markerstyle = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markerstyle[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')

axs[0].legend()

# markerを使用したプロット
for marker in np.unique(markers1):
    axs[1].scatter(x1[markers1 == marker], y1[markers1 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

axs[1].set_title('Answer')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct, accr = accuracy(ans, normalize(pred))
iteration = history.shape[0]
print(f'正解数: {num_correct}     正解率: {accr*100:.1f} %     反復数: {iteration} 回')

center2 = history[-1, :, :]
```

こんなに誘導をしても，正しく分類してくれない．

実は，以上２つの初期値では，最終的に３つのクラスター中心は同じ値に収束している．よって，これ以上どのように初期値を変更しても，正答率は上がらないシナリオが考えられる．

以上の観察から，ハード $K$-平均法はある種の **局所解に収束する** ようなアルゴリズムであると考えられる．

## ソフト $K$-平均法

### アルゴリズムの説明

ハード $K$-平均法では，負担率
$$
r_{kn}\gets\delta_{k}(\argmax_{i\in[k]}d(m_i,x_n))
$$
は $0,1$ のいずれかの値しか取らなかった．この振る舞いを，
$$
\sigma(z;e)_i:=\frac{e^{z_i}}{\sum_{j=1}^Ke^{e_j}}\quad(i\in[K])
$$
で定まる [**ソフトマックス関数**](https://ja.wikipedia.org/wiki/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0) $\sigma:\R^K\to(0,1)^K$ を用いて，「軟化」する．

ここでは，$\beta\ge0$ として，
$$
\sigma(z;e^{-\beta})_i=\frac{e^{-\beta z_i}}{\sum_{j=1}^Ke^{-\beta e_j}}
$$
の形で用い，$\argmax$ の代わりに
$$
\begin{align*}
    r_{kn}&\gets\sigma(d(-,x_n)\circ m;e^{-\beta})_k\\
    &=\frac{e^{-\beta d(m_k,x_n)}}{\sum_{j=1}^K e^{-\beta d(m_j,x_n)}}
\end{align*}
$$
とする．

$\beta$ は **硬度 (stiffness)** または逆温度と呼ぶ．^[stiffness の用語は [@MacKay2003 p.289] から．] $\beta=0$ のときは温度が無限大の場合にあたり，常に負担率は一様になる．絶対零度に当たる $\beta\to\infty$ の極限が hard $K$-means アルゴリズムに相当する．

実装は例えば hard $K$-means アルゴリズム（ @sec-hard-k-means ）から，負担率計算の部分のみを変更すれば良い：

```{.python}
for i in range(N):
        distances = np.array([d(data[i], m[:,k]) for k in range(K)]) # <1>
        denominator_ = np.sum(np.exp(-beta * distances))  # <2>
        r[:,i] = np.exp(-beta * distances) / denominator_ # <3>
```
1. データ $x_i$ とクラスター中心 $(m_k)_{k=1}^K$ との距離を計算し，ベクトル $(d(x_n,m_k))_{k=1}^K$ を `distances` に格納している．
2. 負担率の計算
$$
r_{ik}=\frac{\exp(-\beta d(m_k,x_i))}{\sum_{j=1}^K\exp(-\beta d(m_j,x_i))}
$$
を２段階に分けて行なっており，分母を先に計算して変数 `denominator_` に格納している．
3. すでに計算してある分母 `denominator_` を用いてデータ $x_i$ の負担率 $(r_{ki})_{k=1}^K$ を計算し，$(K,N)$-行列 `r` の各列に格納している．

### 正解率の観察

逆温度をひとまず $\beta=1$ としてみる．@fig-1 と全く同様な初期値
$$
m_1:=\vctr{4}{0},\quad m_2:=\vctr{1}{4},\quad m_3=\vctr{-1}{1},
$$
を与えてみると，次の通りの結果を得る：

```{python}
#| echo: false
#| label: fig-3
#| fig-cap: 左がソフト K-平均法（$\beta=1$），右がハード K-平均法によるクラスタリングの結果（図２の左と全く同じもの）．初期値は $(m_1,m_2,m_3)=\paren{\vctr{4}{0},\vctr{1}{4},\vctr{-1}{1}}$．赤丸で囲まれている点がクラスター中心 (CoC / Center of Cluster) の初期値で，その後の移動が図示されている．
initial_points = np.array([[-1,1],[1,4],[4,0]])
result = kmeans_2d(data1_2d, 3, initial_points.T, 1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)

result2 = kmeans_2d(data1_2d, 3, initial_points.T, 1)
result2.run_hard()
pred2 = result2.fetch_cluster()
history2 = result2.fetch_history()

markers_pred2 = normalize(pred2)

fig, axs = plt.subplots(1, 2, figsize=(10, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [2,1,0]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')

axs[0].legend()

for marker in np.unique(markers_pred2):
    axs[1].scatter(x1[markers_pred2 == marker], y1[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [2,1,0]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of hard K-means')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize(pred))
iteration = history.shape[0]
num_correct2, accr2 = accuracy(ans, normalize(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}     正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %     反復数: {history.shape[0]} 回 vs. {history2.shape[0]} 回')
```


正解率 43.3% であったところから少し改善している．さらに，反復数が９回であったところから，劇的に増えている（65回）．

また，右上の２つのクラスター中心の収束先は，微妙にずれているが **ほとんど一致している** 点も注目に値する．

::: {.callout-note icon="false" title="参考：最終的なクラスター中心の座標" collapse="true"}
```{.python}
centers = history[-1, :, :]
df = pd.DataFrame(centers, columns=['Cluster1', 'Cluster2', 'Cluster3'])
print(df)
```
```{python}
#| echo: false
center3 = history[-1, :, :]
df = pd.DataFrame(center3, columns=['Cluster1', 'Cluster2', 'Cluster3'])
df = df[['Cluster3', 'Cluster2', 'Cluster1']]
df.index = ['x', 'y']
df.columns = ['Cluster1', 'Cluster2', 'Cluster3']
print(df)
```
:::

@fig-2 で与えた初期値 $(m_1',m_2,m_3)$ も与えてみる．

```{python}
#| echo: false
#| label: fig-4
#| fig-cap: ソフト K-平均法（$\beta=1$）によるクラスタリングの結果，右がハード K-平均法によるクラスタリングの結果（図２の右と全く同じもの）．初期値は $(m_1',m_2,m_3)=\paren{\vctr{4}{-1},\vctr{1}{4},\vctr{-1}{1}}$．
initial_points = np.array([[-1,1],[1,4],[4,-1]])
result = kmeans_2d(data1_2d, 3, initial_points.T, 1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize_abnormal(pred)

result2 = kmeans_2d(data1_2d, 3, initial_points.T, 1)
result2.run_hard()
pred2 = result2.fetch_cluster()
history2 = result2.fetch_history()

markers_pred2 = normalize(pred2)

fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1')

axs[0].legend()

for marker in np.unique(markers_pred2):
    axs[1].scatter(x1[markers_pred2 == marker], y1[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title("Result of hard K-means")

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize_abnormal(pred))
iteration = history.shape[0]
num_correct2, accr2 = accuracy(ans, normalize(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}     正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %     反復数: {history.shape[0]} 回 vs. {history2.shape[0]} 回')
```

正答率は 94.4% からやはり少し改善しており，反復数が７回から大きく増えている．

結果はやはり @fig-3 とは大きく異なっており，ハード $K$-平均法で観察された初期値鋭敏性が，変わらず残っている．

加えてこの場合も @fig-3 のクラスター１と２と同様に，クラスター２と３の中心がほぼ一致している．

::: {.callout-note icon="false" title="参考：最終的なクラスター中心の座標" collapse="true"}
```{python}
#| echo: false
center4 = history[-1, :, :]
df = pd.DataFrame(center4, columns=['Cluster1', 'Cluster2', 'Cluster3'])
df = df[['Cluster2', 'Cluster1', 'Cluster3']]
df.index = ['x', 'y']
df.columns = ['Cluster1', 'Cluster2', 'Cluster3']
print(df)
```
:::

$\beta=1$ の場合のソフト $K$-平均法は，この例では **クラスター中心が融合する傾向にある** ようである．

一般に，$\beta$ が小さく，温度が大きいほど，エネルギーランドスケープに極小点が少なくなり，クラスターは同じ場所へ収束しやすくなると予想される．

### 硬度パラメータに依る挙動の変化

初期値を直前で用いた
$$
m_1\gets\vctr{4}{-1},\quad m_2\gets\vctr{1}{4},\quad m_3\gets\vctr{-1}{1},
$$
で固定とし，さらに温度を上げて，逆温度を $\beta=0.1$ としてみる．

```{python}
#| echo: false
#| label: fig-5
#| fig-cap: ソフト K-平均法（左$\beta=0.1$，右$\beta=1$）によるクラスタリングの結果．初期値は $(m_1',m_2,m_3)=\paren{\vctr{4}{-1},\vctr{1}{4},\vctr{-1}{1}}$．
initial_points = np.array([[-1,1],[1,4],[4,-1]])
result = kmeans_2d(data1_2d, 3, initial_points.T, 0.1)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)

result2 = kmeans_2d(data1_2d, 3, initial_points.T, 1)
result2.run_soft()
pred2 = result2.fetch_cluster()
history2 = result2.fetch_history()

markers_pred2 = normalize_abnormal(pred2)

fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=0.1')

axs[0].legend()

for marker in np.unique(markers_pred2):
    axs[1].scatter(x1[markers_pred2 == marker], y1[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of soft K-means with beta=1')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize(pred))
iteration = history.shape[0]
num_correct2, accr2 = accuracy(ans, normalize_abnormal(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}     正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %     反復数: {history.shape[0]} 回 vs. {history2.shape[0]} 回')
```

反復数は減少し，全てがほとんど同じクラスターに属する結果となってしまった．

::: {.callout-note icon="false" title="参考：最終的なクラスター中心の座標" collapse="true"}
```{python}
#| echo: false
center5 = history[-1, :, :]
df = pd.DataFrame(center5, columns=['Cluster1', 'Cluster2', 'Cluster3'])
df = df[['Cluster2', 'Cluster1', 'Cluster3']]
df.index = ['x', 'y']
df.columns = ['Cluster1', 'Cluster2', 'Cluster3']
print(df)
```
:::

３つのクラスター中心の座標が小数点以下５桁の精度で一致してしまっている．

温度が大変に高い状態では，全てが乱雑で，３つのクラスターが一様・公平に負担率を持つようになった．そのため，第一歩からほとんど全体の中心へと移動し，反復数が減る．

加えて，関数 `argmax` が全てのデータ $x_n$ に対してほとんど $k\in[K]$ を返してしまっているのである．

次に，温度を少し下げて，逆温度を $\beta=10$ としてみる．

```{python}
#| echo: false
#| label: fig-6
#| fig-cap: ソフト K-平均法（左$\beta=10$，右$\beta=1$）によるクラスタリングの結果．初期値は $(m_1',m_2,m_3)=\paren{\vctr{4}{-1},\vctr{1}{4},\vctr{-1}{1}}$．
initial_points = np.array([[-1,1],[1,4],[4,-1]])
result = kmeans_2d(data1_2d, 3, initial_points.T, 10)
result.run_soft()
pred = result.fetch_cluster()
history = result.fetch_history()
ans = data1.iloc[:, 0]

markers_pred = normalize(pred)

result2 = kmeans_2d(data1_2d, 3, initial_points.T, 1)
result2.run_soft()
pred2 = result2.fetch_cluster()
history2 = result2.fetch_history()

markers_pred2 = normalize_abnormal(pred2)

fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred):
    axs[0].scatter(x1[markers_pred == marker], y1[markers_pred == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=10')

axs[0].legend()

for marker in np.unique(markers_pred2):
    axs[1].scatter(x1[markers_pred2 == marker], y1[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of soft K-means with beta=1')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize(pred))
iteration = history.shape[0]
num_correct2, accr2 = accuracy(ans, normalize_abnormal(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}     正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %     反復数: {history.shape[0]} 回 vs. {history2.shape[0]} 回')
```

初めて soft $K$-means アルゴリズムを用いた場合で，３つのクラスター中心がはっきりと別れた．反復回数は，$\beta=1$ の場合と比べればやはり落ち着いている．

しかし，正解率は head $K$-means の場合（ @fig-2 など）と全く同じである．実は，最終的なクラスター中心も @fig-2 の最終的なクラスター中心とほとんど同じになっている．

::: {.callout-note icon="false" title="参考：最終的なクラスター中心の座標" collapse="true"}

**今回のソフト $K$-平均法の最終的なクラスター中心**

```{python}
#| echo: false
center6 = history[-1, :, :]
df1 = pd.DataFrame(center6, columns=['Cluster1', 'Cluster2', 'Cluster3'])
df1 = df1[['Cluster2', 'Cluster3', 'Cluster1']]
df1.index = ['x', 'y']
df1.columns = ['Cluster1', 'Cluster2', 'Cluster3']
print(df1)
```

@fig-2 のハード $K$-平均法の最終的なクラスター中心

```{python}
#| echo: false
df2 = pd.DataFrame(center_2, columns=['Cluster1', 'Cluster2', 'Cluster3'])
df2 = df2[['Cluster2', 'Cluster3', 'Cluster1']]
df2.index = ['x', 'y']
df2.columns = ['Cluster1', 'Cluster2', 'Cluster3']
print(df2)
```
:::

以上より，ソフト $K$-平均法は温度を上げるほどクラスター数が少なくなり，温度を下げるほどクラスター数は上がり，**十分に温度を下げるとハード $K$-平均法に挙動が似通う**．

### 最適な硬度の選択

$\beta=1$ ではクラスターが２つに縮退し，$\beta=10$ では hard $K$-means アルゴリズムの結果とほとんど変わらなくなった．そこで，この中間の値での挙動の変化を調べる．

```{python}
#| echo: false
#| label: fig-7
#| fig-cap: ソフト K-平均法によるクラスタリングの結果の比較（$\beta=1.1$ vs. $\beta=1.2$）．
initial_points = np.array([[-1,1],[1,4],[4,-1]])
result1 = kmeans_2d(data1_2d, 3, initial_points.T, 1.1)
result2 = kmeans_2d(data1_2d, 3, initial_points.T, 1.2)
result1.run_soft()
result2.run_soft()
pred1 = result1.fetch_cluster()
pred2 = result2.fetch_cluster()
history1 = result1.fetch_history()
history2 = result2.fetch_history()
ans = data1.iloc[:, 0]

markers_pred1 = normalize_abnormal(pred1)
markers_pred2 = normalize_abnormal(pred2)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x1[markers_pred1 == marker], y1[markers_pred1 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i in rearrange:  # 点の数だけ繰り返し
    x_coords = history1[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history1[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[2-i], label=f'CoC {3-i}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1.1')

axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x1[markers_pred2 == marker], y1[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

for i in rearrange:  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[2-i], label=f'CoC {3-i}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of soft K-means with beta=1.2')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize_abnormal(pred1))
num_correct2, accr2 = accuracy(ans, normalize_abnormal(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}')
print(f'正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %')
```

```{python}
#| echo: false
#| label: fig-8
#| fig-cap: ソフト K-平均法によるクラスタリングの結果の比較（$\beta=1.5$ vs. $\beta=1.8$）．
initial_points = np.array([[-1,1],[1,4],[4,-1]])
result1 = kmeans_2d(data1_2d, 3, initial_points.T, 1.5)
result2 = kmeans_2d(data1_2d, 3, initial_points.T, 1.8)
result1.run_soft()
result2.run_soft()
pred1 = result1.fetch_cluster()
pred2 = result2.fetch_cluster()
history1 = result1.fetch_history()
history2 = result2.fetch_history()
ans = data1.iloc[:, 0]

markers_pred1 = normalize_abnormal(pred1)
markers_pred2 = normalize(pred2)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x1[markers_pred1 == marker], y1[markers_pred1 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
default = [0,1,2]
markers = ['o', '^', 's']  # マーカーの形（丸、三角、正方形）
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history1[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history1[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1.5')

axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x1[markers_pred2 == marker], y1[markers_pred2 == marker], label=f'Cluster {marker}', marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,2]
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of soft K-means with beta=1.8')

axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize_abnormal(pred1))
num_correct2, accr2 = accuracy(ans, normalize(pred2))
print(f'正解数: {num_correct1} vs. {num_correct2}')
print(f'正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %')
```

クラスター２と３の中心が，温度の低下と共に徐々に離れていくことが観察できる．やはり，温度が高い場合はクラスター中心が合流・融合してしまいやすいが，冷却することでクラスター数は大きい状態で安定する．

## 別のデータセットを使った場合

今まで使っていたデータ[-@sec-data]はクラスターがオーバーラップしていたため，いわば優しいデータであった．ここからはよりデータ生成過程が複雑なデータを用いて，ソフト $K$-平均法の挙動を観察する．

### データの概観 {#sec-data2}

今度は，次の４クラスのデータを用いる．

```{python}
#| echo: false

data2_unsorted = pd.read_csv('mixture2.dat', delimiter="\t" , header=None)
data2 = data2_unsorted.sort_values(by=0, ascending=True)
data2_2d = data2.iloc[:, 1:3].to_numpy()

markers2 = data2.iloc[:, 0]
x2 = data2.iloc[:, 1]
y2 = data2.iloc[:, 2]

plt.figure(figsize=(3.5, 3))
for marker in np.unique(markers2):
    plt.scatter(x2[markers2 == marker], y2[markers2 == marker], label=f'Cluster {marker}', marker=f'${marker}$') 

plt.title('Scatter Plot of the Data')
plt.show()
```

実は，これは４つの Gauss 分布から生成されたデータである．

### 結果一覧

```{python}
#| echo: false
initial_points = np.array([[1,6],[6,-1],[1,-8],[-4,-1]])
result1 = kmeans_2d(data2_2d, 4, initial_points.T, 0.5)
result2 = kmeans_2d(data2_2d, 4, initial_points.T, 1.0)
result1.run_soft()
result2.run_soft()
pred1 = result1.fetch_cluster()
pred2 = result2.fetch_cluster()
history1 = result1.fetch_history()
history2 = result2.fetch_history()
ans = data2.iloc[:, 0]

markers_pred1 = normalize(pred1)
markers_pred2 = normalize(pred2)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x2[markers_pred1 == marker], y2[markers_pred1 == marker], marker=f'${marker}$', alpha=0.3),  # markerパラメータによって形状を指定

rearrange = [0,3,2,1]
default = [0,1,2,3]
markers = ['o', '^', 's', '*']
colors = ['blue', 'blue', 'orange', 'orange']
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history1[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history1[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, color=colors[j], linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=0.5')
axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x2[markers_pred2 == marker], y2[markers_pred2 == marker], marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,3,2]
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history1[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history1[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of soft K-means with beta=1.0')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct1, accr1 = accuracy(ans, normalize(pred1))
num_correct2, accr2 = accuracy(ans, normalize(pred2))
iteration1 = history1.shape[0]
iteration2 = history2.shape[0]
```

```{python}
#| echo: false
result1 = kmeans_2d(data2_2d, 4, initial_points.T, 1.5)
result2 = kmeans_2d(data2_2d, 4, initial_points.T, 2)
result1.run_soft()
result2.run_soft()
pred1 = result1.fetch_cluster()
pred2 = result2.fetch_cluster()
history1 = result1.fetch_history()
history2 = result2.fetch_history()
ans = data2.iloc[:, 0]

markers_pred1 = normalize(pred1)
markers_pred2 = normalize(pred2)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x2[markers_pred1 == marker], y2[markers_pred1 == marker], marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,3,2]
default = [0,1,2,3]
markers = ['o', '^', 's', '*']
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history1[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history1[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=1.5')
axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x2[markers_pred2 == marker], y2[markers_pred2 == marker], marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of soft K-means with beta=2.0')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct3, accr3 = accuracy(ans, normalize(pred1))
num_correct4, accr4 = accuracy(ans, normalize(pred2))
iteration3 = history1.shape[0]
iteration4 = history2.shape[0]
```

```{python}
#| echo: false
result1 = kmeans_2d(data2_2d, 4, initial_points.T, 10)
result2 = kmeans_2d(data2_2d, 4, initial_points.T, 10)
result1.run_soft()
result2.run_hard()
pred1 = result1.fetch_cluster()
pred2 = result2.fetch_cluster()
history1 = result1.fetch_history()
history2 = result2.fetch_history()
ans = data2.iloc[:, 0]

markers_pred1 = normalize(pred1)
markers_pred2 = normalize(pred2)
fig, axs = plt.subplots(1, 2, figsize=(7, 4))  # 1行2列のサブプロット

# marker_predを使用したプロット
for marker in np.unique(markers_pred1):
    axs[0].scatter(x2[markers_pred1 == marker], y2[markers_pred1 == marker], marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

rearrange = [1,0,3,2]
default = [0,1,2,3]
markers = ['o', '^', 's', '*']
for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history1[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history1[:, 1, i]  # i番目の点の全時点でのy座標
    axs[0].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[0].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[0].set_title('Result of soft K-means with beta=10')
axs[0].legend()


# marker_predを使用したプロット
for marker in np.unique(markers_pred2):
    axs[1].scatter(x2[markers_pred2 == marker], y2[markers_pred2 == marker], marker=f'${marker}$', alpha=0.3)  # markerパラメータによって形状を指定

for i,j in zip(rearrange, default):  # 点の数だけ繰り返し
    x_coords = history2[:, 0, i]  # i番目の点の全時点でのx座標
    y_coords = history2[:, 1, i]  # i番目の点の全時点でのy座標
    axs[1].plot(x_coords, y_coords, linestyle='-', marker=markers[j], label=f'CoC {j+1}', zorder=1)

axs[1].scatter(initial_points[:, 0], initial_points[:, 1], facecolors='none', edgecolors='red', s=100, zorder=2)

axs[1].set_title('Result of hard K-means')
axs[1].legend()

plt.tight_layout()
plt.show()

num_correct5, accr5 = accuracy(ans, normalize(pred1))
num_correct6, accr6 = accuracy(ans, normalize(pred2))
iteration5 = history1.shape[0]
iteration6 = history2.shape[0]
```

```{python}
#| echo: false
print(f'正解数: {num_correct1} vs. {num_correct2}     正解率: {accr1*100:.1f} % vs. {accr2*100:.1f} %     反復数: {iteration1} 回 vs. {iteration2} 回')
print(f'正解数: {num_correct3} vs. {num_correct4}     正解率: {accr3*100:.1f} % vs. {accr4*100:.1f} %     反復数: {iteration3} 回 vs. {iteration4} 回')
print(f'正解数: {num_correct5} vs. {num_correct6}     正解率: {accr5*100:.1f} % vs. {accr6*100:.1f} %     反復数: {iteration5} 回 vs. {iteration6} 回')
```


## $K$-平均法のまとめ

### アルゴリズムの性質

::: {.callout-note icon="false" title="結論"}
* データ [-@sec-data] に対して，（初期値 $(m'_1,m_2,m_3)$ で）ソフト $K$-平均法を適用すると，
  * $\beta\ge2$ の場合で結果はハード $K$-平均法と変わらなくなる．
  * $\beta=1$ の場合で結果はクラスターがほとんど２つになり，$\beta\le0.5$ では計算機上では実際に２つになってしまう．
  * 正答率は $1\le\beta\le1.1$ で最大であった．
  * $\beta$ を大きくするほど，反復回数は減少していった．
* データ [-@sec-data2] に対しても，以上の４点について同様の傾向が確認できた．
:::

こうしてソフト $K$-平均法とハード $K$-平均法の性質は分かった．主に

1. 初期値依存性
2. クラスタ数 $K$ の選択法

の問題が未解決であり，恣意性が残る．

誰がどう使ってもうまくいくようなアルゴリズムであると言うことは出来ない．

### $K$-平均法の応用

ソフト $K$-平均法は（非可逆）データ圧縮にも用いられる．元論文 [@Lloyd1982] もその文脈である．クラスター中心での画像の値と，それ以外では帰属先のクラスター番号のみを保存すれば良いというのである．このアプローチを **ベクトル量子化** (vector quantization) という．^[[@MacKay2003 p.284]，[@Bishop2006 p.429]．クラスター中心は **符号表ベクトル** または 代表ベクトル (code-book vector) という．]