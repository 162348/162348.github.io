---
title: "新時代の MCMC を迎えるために"
subtitle: "連続時間アルゴリズムへの進化"
author: 
    - name: "司馬 博文"
      orcid: 0009-0007-8251-1224
      affiliation:
        - name: 総合研究大学院大学先端学術院（統計科学コース）
          url: https://www.ism.ac.jp/senkou/
          group: ５年一貫博士課程
date: 5/24/2024
keywords:
    - マルコフ連鎖モンテカルロ法（MCMC）
    - 連続時間 MCMC
    - ハミルトニアンモンテカルロ法（HMC）
    - "区分的確定的マルコフ過程（PDMP: piecewise deterministic Markov process）"
categories: [MCMC, Simulation, Poster]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: 物質科学を震源地とする MCMC のイノベーションが，統計力学と統計学の分野に波及して来ています．その結果，ここ 10 年で急激に MCMC 手法の革新が起こりました．従来 MCMC が離散時間ベースだったところが，イベントベースかつ連続時間ベースなものにとって替わられようとしているのです．これら連続時間 MCMC は従来法を超えるのか？どのような場面で使えるのか？この新たな手法を正しく受け止めるために，現状の MCMC への理解をまとめます．
copyright: "Copyright Hirofumi Shiba 2024. All Rights Reserved"
---

{{< include ../../../_preamble.qmd >}}

[![「新時代の MCMC を迎えるために」統数研オープンハウス（タップで PDF 閲覧）](../../../static/Posters/ISM-OH2024.jpg){width=200}](../../../static/Posters/ISM-OH2024.pdf)

::: {.callout-tip appearance="minimal"}
以下は，5月24日 10:30~12:30 に行われた [2024年度統数研オープンハウス](../../../static/Sessions.qmd#sec-ISM-openhouse2024) ポスターセッションに於て発表された上付のポスターに関する解説記事です．
:::

現状，HMC (Hamiltonian Monte Carlo) [@Duane+1987] というなんと約 40 年前の MCMC 手法が，Stan などの確率的プログラミング言語のデフォルト MCMC 手法として採用されています．^[Hamiltonian Monte Carlo の名称は [@Neal2011-HMC] からで，元々は Hybrid Monte Carlo と呼ばれていました．分子動力学法 (Molecular Dynamics) と MCMC のハイブリッド，という意味でした．]

この手法はもともと "別の HMC" たる混合モンテカルロ (HMC: hybrid Monte Carlo) [@Duane+1987] が量子力学系のシミュレーションに特化した MCMC であったところを，一般の統計モデルに適用可能な形式に翻訳する形で提案されたものでした．

ということで，HMC は，オリジナルの MCMC がそうであったように，物理学の方から着想された効率的な MCMC 手法であったのです．

**HMC が，提案から 40 年目を迎える前に，更なる効率的な手法によって代替されようとしています**．

そのきっかけ [@Peters-deWith2012] も，やはり，物理学（今回は物質科学）からの着想でした．

## MCMC とは何か？

MCMC とは，確率変数をシミュレーションする際に用いられる汎用的アルゴリズムです．

一様分布や正規分布などの名前がついた分布ではない場合，どのようにすればその分布に従う確率変数をシミュレーションできるのかは極めて難しい問題です．

実際，MCMC では空間を時々刻々と移動するマルコフ連鎖をうまく構成し，その軌跡がちょうど確率変数のシミュレーションになっていると聞いても，なぜそのような回りくどい方法を使うのか？本当にうまくいくのか？疑問が絶えないでしょう．

ですが，MCMC が発明された経緯である物理学の問題から見てみると，実は極めて自然に思えてくるかもしれません．

### 着想経緯

::: {.callout-caution title="よりみち：どうして MCMC が必要だったのか？" collapse="true"}

[@Metropolis+1953] では，温度 $T$ 一定の条件下で，$N$ 粒子系をシミュレートし，任意の物理量 $F$ に対してその相空間上の平均
$$
\brac{F}=\frac{\int Fe^{-\frac{E}{kT}}dp}{\int e^{-\frac{E}{kT}}dp}
$$
を効率的に計算するアルゴリズムが提案された．これが現在では Metropolis 法と呼ばれている．^[統計学界隈では [@Hastings1970] を入れて，Metropolis-Hastings 法とも呼ばれる．]

[@Metropolis+1953] では $N$ が数百になる場合を考えており，当然愚直な数値積分は（現代の計算機でも）実行可能ではない．そこで Monte Carlo 法を考えることになるが，当時 Monte Carlo 法といえば，一様乱数を用いた計算法の全般を指し，それを用いると $\brac{F}$ を重点サンプリング推定量
$$
\wh{F}=\frac{\sum_{n=1}^NF(\om)e^{-\frac{E(\om)}{kT}}}{\sum_{n=1}^Ne^{-\frac{E(\om)}{kT}}}
$$
で推定することを指した．^[ただし，配置 $\om\in\Om$ は空間内にランダム（一様）に粒子 $N$ 個を配置することで生成することとする．]

しかしこれでは，配置 $\om\in\Om$ を完全に一様に生成するため，高エネルギーな配置も生成しやすく，そのようなサンプルは $\brac{F}$ の推定にあまり寄与しない．

これを低減することが出来れば Monte Carlo 法の更なる効率改善に繋がる．こうして，Gibbs 分布 $\frac{1}{Z}e^{-\frac{E}{kT}}$ からの直接的サンプリングを実行することが考えられた．

:::

[@Metropolis+1953] では，Boltzmann-Gibbs 分布 $\frac{1}{Z}e^{-\frac{E}{kT}}$ から直接サンプリングする方法が探求されました．

ここでは簡単のため，１粒子が次のようなポテンシャル
$$
U(x)=\frac{x^2}{2}+\frac{x^4}{4}
$$
に従って運動する場合を考えましょう：

![ポテンシャル $U$ のプロット](Files/potential.svg)

このポテンシャルに関する Boltzmann-Gibbs 分布は次のような形になります：

![ポテンシャル $U$ が定める Botlzmann-Gibbs 分布のプロット](Files/Gibbs.svg)

<!--
```
\begin{tikzpicture}
\begin{axis}[
    axis lines = middle,
    axis line style={->},
    xlabel = $x$,
    ylabel = {$e^{-\beta U(x)}$},
    xlabel style={at={(ticklabel* cs:1)}, anchor=north west},
    ylabel style={at={(ticklabel* cs:1)}, anchor=south west},
    xmin=-1.2, xmax=1.2,
    ymin=0, ymax=1,
    xtick distance=1,
    ytick=\empty,
    grid=both,
    grid style={gray!30, dashed},
    minor tick num=1,
    width=10cm,
    height=8cm,
    legend pos=north east,
    legend cell align=left,
    legend style={fill=white, fill opacity=0.8, draw opacity=1, text opacity=1, font=\small},
]
\addplot[domain=-2:2, samples=100, smooth, thick, SaddleBrown] {exp(-x^2 - x^4)};
\addlegendentry{$y = e^{-\frac{U(x)}{kT}}$}
\end{axis}
\end{tikzpicture}
```
-->

低エネルギー状態が現れやすく，エネルギーが上がるにつれて急激に現れにくくなることがわかります．

```{python}
import numpy as np

def U(x):
    return x**2/2 + x**4/4

def pi(x):
    return np.exp(-U(x))

def metropolis(num_samples, initial_state, verbose=False):
    samples = [initial_state]
    current_state = initial_state

    accept = []

    for _ in range(num_samples - 1):
        proposed_state = current_state + np.random.uniform(-1,1)
        acceptance_ratio = pi(proposed_state) / pi(current_state)
        if np.random.rand() < acceptance_ratio:
            current_state = proposed_state
            accept.append(True)
        samples.append(current_state)

    if verbose:
      rate = len(accept) / num_samples
      print(f'acceptance rate : {rate}')

    return np.array(samples)

# サンプル数と初期状態を固定
num_samples = 10000
initial_state = 0.0
```

```{python}
#| echo: false
ISMblue = "#2f579c"
SaddleBrown = "#8b4513"
```

```{python}
import matplotlib.pyplot as plt

samples_MH = metropolis(num_samples, initial_state)
plt.acorr(samples_MH - np.mean(samples_MH), maxlags=10, color=SaddleBrown)
plt.xlim(-0.5, 10.5)
#plt.ylim(-0.5,0.85)
# グラフの装飾
plt.title('Metropolis-Hastings', color=SaddleBrown)
plt.tick_params(axis='x', colors=ISMblue)
plt.tick_params(axis='y', colors=ISMblue)
plt.xlabel('Lag', color=ISMblue)
plt.ylabel('Autocorrelation', color=ISMblue)
#plt.savefig('MH_auto.svg', format='svg')
plt.show()
```

```{python}
plt.figure()
plt.plot(samples_MH[0:50], range(50), color=SaddleBrown)
plt.ylim(-0.5, 49.5)
#plt.xlim(-1.2,1.2)
# グラフの装飾
plt.title('Metropolis-Hastings', color=SaddleBrown)
plt.tick_params(axis='x', colors=ISMblue)
plt.tick_params(axis='y', colors=ISMblue)
plt.xlabel('X-axis', color=ISMblue)
plt.ylabel('Step', color=ISMblue)
#plt.savefig('MH_traj.svg', format='svg')
plt.show()
```

### 統計学への応用

こうして MCMC が発明されれば，すぐにイノベーションとして理解されたかというとそうではありませんでした．

この Metropolis の手法が極めて賢いシミュレーション手法であることは一目瞭然でも，一般の確率分布からのサンプリングに使える汎用アルゴリズムになっているという抽象的な観点が得られるまでには時間を要しました．これを成し遂げたのが [@Hastings1970] でした．^[While [@Metropolis+1953] proposed the use of MCMC sampling to compute particular integrals in statistical mechanics, **it was the Hastings paper that elevated the concept to a general one, and introduced it to the broader statistics community**. [@Martin+2023-history p.7] 3.5節．]

さらにこの結果も見過ごされました．真にMCMC 法一般を統計学界隈に広め，ベイズ統計学の興隆につながったのは [@Gelfand-Smith1990] だと言われます．^[[@Martin+2023-history p.8] 4節，[@Robert-Casella2011 p.102]．]

### リフティング

```{python}
def lifted_metropolis(num_samples, initial_state, verbose=False):
    samples = [initial_state]
    current_state = initial_state
    lifting_variable = 1
    accept = []

    for _ in range(num_samples - 1):
        delta = np.random.uniform(0,1)
        proposed_state = current_state + lifting_variable * delta
        acceptance_ratio = pi(proposed_state) / pi(current_state)

        if np.random.rand() < acceptance_ratio:
            current_state = proposed_state
            accept.append(True)
        else:
            lifting_variable = (-1) * lifting_variable

        samples.append(current_state)
    
    if verbose:
      rate = len(accept) / num_samples
      print(f'acceptance rate : {rate}')

    return np.array(samples)
```

```{python}
#| echo: false
samples_LMH = lifted_metropolis(num_samples, initial_state)
plt.acorr(samples_LMH - np.mean(samples_LMH), maxlags=10, color=SaddleBrown)
plt.xlim(-0.5, 10.5)
#plt.ylim(-0.5,0.85)
# グラフの装飾
plt.title('Lifted Metropolis-Hastings', color=SaddleBrown)
plt.tick_params(axis='x', colors=ISMblue)
plt.tick_params(axis='y', colors=ISMblue)
plt.xlabel('Lag', color=ISMblue)
plt.ylabel('Autocorrelation', color=ISMblue)
#plt.savefig('LMH_auto.svg', format='svg')
plt.show()
```

```{python}
#| echo: false
plt.figure()
plt.plot(samples_LMH[0:50], range(50), color=SaddleBrown)
plt.ylim(-0.5, 49.5)
#plt.xlim(-1.2,1.2)
# グラフの装飾
plt.title('Lifted Metropolis-Hastings', color=SaddleBrown)
plt.tick_params(axis='x', colors=ISMblue)
plt.tick_params(axis='y', colors=ISMblue)
plt.xlabel('X-axis', color=ISMblue)
plt.ylabel('Step', color=ISMblue)
#plt.savefig('LMH_traj.svg', format='svg')
plt.show()
```

## 新たな MCMC

こうして MCMC は物理学者から物質科学者，そして統計学者から機械学習家まで，多くの人が幅広く用いる手法になりました．

その結果，多くの同一の手法が違う名前で呼ばれていることも多く，現状の最先端ではどのようなことが起こっているのか見極めるのが困難です．

ここでは，上述のすべての分野に渡って共通して起こりつつある大きな地殻変動を紹介します．キーワードは **連続時間 MCMC** です．^[[@Fearnhead+2018-PDMC] から取った用語です．コンピュータシミュレーションである以上，結局は離散化するのですが，粒子の動きは（従来の Metropolis-Hastings 法のような）Markov 連鎖であるというより，連続時間確率過程のような動きをする手法群であることには間違いありません．]

### 連続時間 MCMC

```{python}
import math

def zigzag(num_samples, initial_state):
    samples = [initial_state]
    current_state = initial_state
    lifting_variable = 1
    t = 0

    while t < num_samples:
        state_event = lifting_variable * np.sqrt(-1 + np.sqrt( 1 - 4 * np.log(np.random.rand()) ))
        t_event = t + np.abs(state_event - current_state)
        for _ in range(math.ceil(t),math.ceil(t_event)):
            samples.append(current_state + lifting_variable * (t_event - t))
        current_state = state_event
        lifting_variable = (-1) * lifting_variable
        t = t_event

    return np.array(samples)
```

```{python}
#| echo: false
samples_zigzag = zigzag(num_samples, initial_state)
plt.acorr(samples_zigzag - np.mean(samples_zigzag), maxlags=10, color=SaddleBrown)
plt.xlim(-0.5, 10.5)
#plt.ylim(-0.5,0.85)
# グラフの装飾
plt.title('Zig-Zag sampler', color=SaddleBrown)
plt.tick_params(axis='x', colors=ISMblue)
plt.tick_params(axis='y', colors=ISMblue)
plt.xlabel('Lag', color=ISMblue)
plt.ylabel('Autocorrelation', color=ISMblue)
#plt.savefig('zigzag_auto.svg', format='svg')
plt.show()
```

```{python}
#| echo: false
plt.figure()
plt.plot(samples_zigzag[0:50], range(50), color=SaddleBrown)
plt.ylim(-0.5, 49.5)
#plt.xlim(-1.2,1.2)
# グラフの装飾
plt.title('Zig-Zag sampler', color=SaddleBrown)
plt.tick_params(axis='x', colors=ISMblue)
plt.tick_params(axis='y', colors=ISMblue)
plt.xlabel('X-axis', color=ISMblue)
plt.ylabel('Step', color=ISMblue)
#plt.savefig('zigzag_traj.svg', format='svg')
plt.show()
```

### 連続時間 MCMC の美点



### 筆者の目標：新時代のサンプラーの開発

情報通信機器の発達によりデータが複雑で大規模化する現代では，モデルも同様に大規模で複雑化していく必要があります．OpenAI の ChatGPT や Sora，Anthropic の Claude-3 などの **基盤モデル** はその象徴と言えるでしょう．

筆者は，その中で **新時代の MCMC** の開発を目標としています．

高次元空間上の複雑な分布からも効率的にサンプリングできる MCMC 手法が開発された際には，多くの人が自分のノートパソコンで気軽にできるベイズ統計分析の幅が大きく広がることでしょう．

それこそ，ニューラルネットワークの表現力をフルに活用するだけでなく，ベイズ手法の強みも併せて，小規模データでも鮮やかな分析が簡単に出来るようになるかもしれません．

そのような世界線こそ，AI 技術の民主化と呼ぶにふさわしい，来るべき未来だと筆者は信じています．

また，基盤モデルの Bayes 的な理解を進めることも，実は壮大ながらも，筆者の最終的な目標の一つであります．

## 終わりに {.appendix}

本ポスター執筆のきっかけは，[MLSS でのポスター発表](../Particles/PF.qmd) で連続時間 MCMC のことが機械学習の界隈では全く知られていないことを知ったことと，情報統計力学の研究集会に出席したことでした．

統計界隈では PDMP や連続時間 MCMC と呼ばれる手法は，物理学界隈では lifting や event-based simulation，rejection-free と呼ばれる手法群の１つとして活発に研究されていました．

全く同じ問題を解こうとしているのに，用語法が全く異なるのです！

２つの分野の相互理解と知見の交換が進むことを目指し，これからも研究していきたいと考えます．