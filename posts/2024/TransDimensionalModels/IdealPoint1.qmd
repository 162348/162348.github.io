---
title: "理想点解析のハンズオン"
subtitle: "`pscl`, `MCMCpack`, `emIRT` パッケージ"
author: "司馬 博文"
date: 10/2/2024
date-modified: 12/13/2024
categories: [Bayesian, Statistics, MCMC, R]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
    - IdealPoint.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: |
    理想点解析とは，政治学において国会議員のイデオロギーを定量化・視覚化する方法論である．この手法は多くの側面を持ち，[多次元展開法](../Kernels/Manifold.qmd#sec-MDU) (MDU: Multidimensional Unfolding) であると同時に項目反応モデルでもある．ここでは既存のパッケージを用いて理想点解析を行う方法を紹介する．
image: Images/IdealPoint.png
code-fold: false
execute:
    cache: true
listing: 
    -   id: lst-embedding
        type: grid
        contents:
            - "IdealPoint.qmd"
            - "IdealPoint2.qmd"
            - "../Survey/BDA3.qmd"
        date-format: iso
        fields: [title,image,date,subtitle,categories]
    -   id: lst-embedding
        type: grid
        grid-columns: 1
        contents:
            - "../Survey/BayesGLMM.qmd"
        date-format: iso
        fields: [title,image,date,subtitle,categories]
---

{{< include ../../../assets/_preamble.qmd >}}

## 関連記事 {.unnumbered .unlisted}

::: {#lst-embedding}
:::

本稿では実際に理想点モデルの推定を，[Martin-Quinn](http://mqscores.wustl.edu/replication.php) により公開されている連邦最高裁判所の 1937 年から 2022 年までのデータを（`MCMCpack` パッケージを通じて）用いて行う．

```{r}
#| output: false
#| echo: false
library(brms)
library(knitr)
library(gridExtra)
library(ggplot2)
```

```{r}
library(MCMCpack)
data(Rehnquist)  # MCMCpackに含まれる U.S. Supreme Court（連邦最高裁）のデータ
kable(head(Rehnquist))
```

このデータは保守的な判断をする場合が $y_i=1$，リベラルな判断をする場合が $y_i=0$ の２値データとなっている．


## 理想点モデルとは何か？

### `MCMCpack` パッケージ

はじめに，理想点モデルではどのようなことができるかをみるために，`MCMCpack` パッケージを通じて理想点推定を簡単に実行する方法を見る．

理想点モデルでは識別性が一つの論点になる（第 [-@sec-identification] 節）が，ここでは簡単に，Stevens 判事と Thomas 判事の位置を固定する方法を用いてみよう．

`MCMCpack` パッケージでは，時系列理想点モデルの推定に [`MCMCdynamicIRT1d()`](https://github.com/cran/MCMCpack/blob/master/man/MCMCdynamicIRT1d.Rd) 関数が用意されている．

```{r}
#| output: false
# 初期値の設定
theta.start <- rep(0, 9)  # 9人の裁判官の初期値
theta.start[2] <- -3      # Stevens裁判官の初期値
theta.start[7] <- 2       # Thomas裁判官の初期値

# MCMCの実行
out <- MCMCdynamicIRT1d(
    t(Rehnquist[,1:9]),           # データ行列（転置して裁判官×案件の形に）
    item.time.map=Rehnquist$time, # 各案件の時期情報
    theta.start=theta.start,      # 初期値
    mcmc=2000,                   # MCMCの反復回数
    burnin=2000,                 # バーンイン期間
    thin=5,                       # 間引き数
    verbose=500,                  # 進捗表示間隔
    tau2.start=rep(0.1, 9),      # τ²の初期値
    e0=0, E0=1,                  # θの事前分布パラメータ
    a0=0, A0=1,                  # αの事前分布パラメータ
    b0=0, B0=1,                  # βの事前分布パラメータ
    c0=-1, d0=-1,               # τ²の事前分布パラメータ
    store.item=FALSE,            # アイテムパラメータを保存しない
    theta.constraints=list(Stevens="-", Thomas="+")  # 識別制約
)

theta_cols <- grep("theta", colnames(out), value=TRUE)
theta_mcmc <- out[, theta_cols]

# library(coda)
# summary(theta_mcmc)  # codaのsummary関数で要約
plot(theta_mcmc)
```

![](Images/plot.png){width=70% fig-align=center}

### 理想点の推定

出力は各最高裁判事の理想点の事後分布である．

```{r}
theta_means <- colMeans(theta_mcmc)
pattern <- "t11"
col_names <- colnames(theta_mcmc)
selected_cols <- grep(pattern, col_names)  # 正規表現にマッチする列のインデックスを取得
selected_theta_mcmc <- theta_mcmc[, selected_cols]

quantiles_2_5 <- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.025))
quantiles_97_5 <- apply(selected_theta_mcmc, 2, function(x) quantile(x, 0.975))

ggplot(data.frame(
  legislator = colnames(Rehnquist)[1:9],
  mean = unname(theta_means[selected_cols]),
  lower = unname(quantiles_2_5),
  upper = unname(quantiles_97_5)
), aes(x = mean, y = legislator)) + 
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Estimated Ideal Points (11th term)", x = "Ideal Point", y = "Legislator")
```

### 時系列化

[@Martin-Quinn2002] ではこの理想点の時系列的な変化を調べた．

```r
time_points <- unique(Rehnquist$time)
n_subjects <- 9  # 裁判官の数

# 各裁判官の軌跡をプロット
plot(time_points, theta_means[1:length(time_points)], 
     type="l", ylim=range(theta_means),
     xlab="Time", ylab="Ideal Point",
     main="Estimated Ideal Points Over Time")

# 各裁判官を異なる色で追加
colors <- rainbow(n_subjects)
colors[9] <- "blue"
for(i in c(1,8,9)) {
    lines(time_points, 
          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],
          col=colors[i],
          lwd=3)
}

# 凡例を追加
legend("topright", 
       legend=unique(colnames(Rehnquist)[c(1,8,9)]),  # 裁判官の名前
       col=colors[c(1,8,9)], 
       lty=1)
```

![](Images/plot3.png){width=70% fig-align=center}

たしかに [William Rehnquist](https://en.wikipedia.org/wiki/William_Rehnquist) は共和党，[Ruth Bader Ginsburg](https://en.wikipedia.org/wiki/Ruth_Bader_Ginsburg) と [Stephen Breyer](https://en.wikipedia.org/wiki/Stephen_Breyer) は民主党である．

```{r}
#| eval: false
#| echo: false
# プロットの基本設定
plot(time_points, theta_means[1:length(time_points)], 
     type="l", 
     ylim=range(theta_means),
     xlab="Time", 
     ylab="Ideal Point",
     main="Estimated Ideal Points Over Time",
     lwd=2)  # ← 基本の線の太さを2に設定

# 各裁判官の軌跡をプロット
colors <- rainbow(n_subjects)
for(i in 1:n_subjects) {
    lines(time_points, 
          theta_means[((i-1)*length(time_points)+1):(i*length(time_points))],
          col=colors[i],
          lwd=2)  # ← 各線の太さを2に設定
}

# 凡例を追加（凡例の線も太くする）
legend("topright", 
       legend=unique(colnames(Rehnquist)[1:9]),
       col=colors, 
       lty=1,
       lwd=2)  # ← 凡例の線の太さも2に設定
```

![](Images/plot2.png){width=70% fig-align=center}

$0$ の上に位置している [Anthony Kennedy](https://ja.wikipedia.org/wiki/アンソニー・ケネディ) や [Sandra Day O'Connor](https://en.wikipedia.org/wiki/Sandra_Day_O%27Connor) はほとんど中道的だが，やや保守党寄りである． [Antonin Scalia](https://en.wikipedia.org/wiki/Antonin_Scalia) は特に保守的な立場であることが知られている．

$0$ よりも下に位置するもう一人は [David Souter](https://en.wikipedia.org/wiki/David_Souter) であるが，彼はもともと保守系と目されていたが，後年リベラルな傾向を示したとされる．^["Souter was nominated to the Supreme Court without a significant "paper trail" but was expected to be a conservative justice. Within a few years of his appointment, Souter moved towards the ideological center. He eventually came to vote reliably with the Court's liberal wing." [Wikipedia](https://en.wikipedia.org/wiki/David_Souter) より引用．]

## ２母数ロジットモデルの推定

### はじめに

`MCMCpack` パッケージで理想点推定の出力がつかめたいま，より詳しくモデルを見ていく．

本節では `rstan` パッケージを用いて，項目反応モデルとして具体的な手順を踏んで推定してみる．

```{r}
#| output: false
library(tidyverse)
df <- Rehnquist %>%
  # データを長形式に変換
  pivot_longer(cols = -c(term, time), names_to = "name", values_to = "y") %>%
  # ケース ID を追加
  mutate(case = (row_number() - 1) %/% 9 + 1)
```

### １母数モデル（`brms` パッケージ）

まずは最も簡単な項目反応モデルとして，$g$ を logit リンクとして，
$$
g(\P[Y_{ij}=1])=\al_0+\al_j-x_i
$$
というモデルを推定することを考えよう．

`brms` パッケージを用いれば，他の R パッケージと同様のインターフェイスで推定を行うことができる．

```{r}
#| output: false
library(brms)
formula <- bf(
  y ~ 1 + (1 | case) + (1 | name)
)
fit_1PL <- brm(
  formula,
  data = df,
  family = brmsfamily("bernoulli", link = "logit"),
  chains = 4, cores = 4
)
```

```{r}
summary(fit_1PL)
```

簡単なモデルであるが切片項の ESS が低く，すでに暗雲が立ち込めている．

```{r}
plot(fit_1PL)
```

ここには変動係数（我々の欲しい潜在変数）はパラメータとみなされておらず，推定値が表示されないので次のようにしてプロットする必要がある：

```{r}
ranef_legislator <- ranef(fit_1PL)$name
posterior_means <- ranef_legislator[,1,"Intercept"]
lower_bounds <- ranef_legislator[,3,"Intercept"]
upper_bounds <- ranef_legislator[,4,"Intercept"]
plot_legislator <- data.frame(
  legislator = rownames(ranef_legislator),
  mean = posterior_means,
  lower = lower_bounds,
  upper = upper_bounds
)
p_1PL <- ggplot(plot_legislator, aes(x = mean, y = legislator)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "1PL Model",
       x = "Posterior Estimate",
       y = "Legislator")
p_1PL
```

Thomas や Scalia，そして Stevens が極端であることはとらえているが，Stevens や Ginsburg らリベラルな判事は左側に来て欲しいのであった．

誘導が成功しておらず，片方の峯からサンプリングしてしまっている．

```{r}
prior_summary(fit_1PL)
```

### １母数モデル（`rstan` パッケージ）

`brms` パッケージ内で生成される Stan コードを参考にして，自分で Stan コードを書いて推定することもできる．

::: {.callout-caution title="Stan コードの出力" collapse="true" icon="false"}

```r
stancode(fit_1PL)
```

```stan
// generated with brms 2.21.0
functions {
}
data {
  int<lower=1> N;  // total number of observations
  array[N] int Y;  // response variable
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  array[N] int<lower=1> J_1;  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  // data for group-level effects of ID 2
  int<lower=1> N_2;  // number of grouping levels
  int<lower=1> M_2;  // number of coefficients per level
  array[N] int<lower=1> J_2;  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_2_1;
  int prior_only;  // should the likelihood be ignored?
}
transformed data {
}
parameters {
  real Intercept;  // temporary intercept for centered predictors
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  array[M_1] vector[N_1] z_1;  // standardized group-level effects
  vector<lower=0>[M_2] sd_2;  // group-level standard deviations
  array[M_2] vector[N_2] z_2;  // standardized group-level effects
}
transformed parameters {
  vector[N_1] r_1_1;  // actual group-level effects
  vector[N_2] r_2_1;  // actual group-level effects
  real lprior = 0;  // prior contributions to the log posterior
  r_1_1 = (sd_1[1] * (z_1[1]));
  r_2_1 = (sd_2[1] * (z_2[1]));
  lprior += student_t_lpdf(Intercept | 3, 0, 2.5);
  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
}
model {
  // likelihood including constants
  if (!prior_only) {
    // initialize linear predictor term
    vector[N] mu = rep_vector(0.0, N);
    mu += Intercept;
    for (n in 1:N) {
      // add more terms to the linear predictor
      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n];
    }
    target += bernoulli_logit_lpmf(Y | mu);
  }
  // priors including constants
  target += lprior;
  target += std_normal_lpdf(z_1[1]);
  target += std_normal_lpdf(z_2[1]);
}
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept;
}
```

:::

```r
library(rstan)
stan_code <- "
data {
  int<lower=1> n;  // data size: n = N * J - #(NA responses)
  int<lower=1> N;  // number of judges
  int<lower=1> J;  // number of cases

  array[n] int<lower=0, upper=1> Y;  // response variable
  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]
  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]
}
parameters {
  vector[N] X;  // ideal points
  vector[J] alpha_zero;  // intercepts
  vector[J] alpha;  // item effects
}
transformed parameters {
  real lprior = 0;

  lprior += student_t_lpdf(alpha_zero | 3, 0, 2.5);
  lprior += std_normal_lpdf(alpha);
  lprior += std_normal_lpdf(X);
}
model {
  vector[n] mu = rep_vector(0, n);
  for (k in 1:n) {
    mu[k] = alpha_zero[j[k]] + alpha[j[k]] - X[i[k]];
  }
  target += bernoulli_logit_lpmf(Y | mu);
  target += lprior;
}
"
case_number <- as.integer(nrow(df) / 9)
indicator_i <- rep(1:9, times = case_number)
indicator_j <- rep(1:case_number, each = 9)
df$i <- indicator_i
df$j <- indicator_j

df_NA <- df %>% filter(!is.na(y))

data <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)
fit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)
```

```r
x_samples <- extract(fit, pars = "X")$X

plot_dataframe <- data.frame(
  legislator = colnames(Rehnquist)[1:9],
  mean = apply(x_samples, 2, mean),
  lower = apply(x_samples, 2, quantile, probs = 0.025),
  upper = apply(x_samples, 2, quantile, probs = 0.975)
)
ggplot(plot_dataframe, aes(x = mean, y = legislator)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Mean", y = "Legislator", title = "1PL Model (RStan)")

```

![](Images/stan1.png){width=70% fig-align=center}

係数 $\al_j,x_i$ の事前分布を正規分布にしているため，事後分散が短くなっている．

#### ２母数モデル

[@Bafumi+2005] など，多くの理想点モデルでは２母数ロジットモデルが用いられる：
$$
g(x):=\operatorname{logit}(x)=\log\frac{x}{1-x},
$$
$$
g(\mu_{i,j})=\al_j+\beta_j x_i=\beta_j\Paren{\wt{\al}_j-x_i}.
$$
この際 $x_i$ は $i$ 番目の判事の **理想点** といい，$\al_j,\beta_j$ は $j$ 番目の事件の性質を表すパラメータである．

ものによっては判事の立場が関係ない事件もあるため，$\beta_j$ が用意されている．

基本的にこの識別パラメータが正になるように調整したいが，明示的にそうすることはしない．

次節で説明する方法により，理想点 $x_i$ が大きい場合は保守的な判断を下しやすいものと解釈できるように設計することができる（$x_i$ を数直線上にプロットした際に，リベラルな場合に左に，保守的な場合に右に来るようにする）が，ここではストレートに実装してみよう．

```r
library(rstan)
stan_code <- "
data {
  int<lower=1> n;  // n = N * J - #(NA responses)
  int<lower=1> N;  // number of judges
  int<lower=1> J;  // number of cases

  array[n] int<lower=0, upper=1> Y;  // response variable
  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]
  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]
}
parameters {
  vector[N] X;  // ideal points
  vector[J] alpha;  // item effects
  vector[J] beta;  // item discremination
}
transformed parameters {
  real lprior = 0;

  lprior += std_normal_lpdf(alpha);
  lprior += std_normal_lpdf(beta);
  lprior += std_normal_lpdf(X);
}
model {
  vector[n] mu = rep_vector(0, n);
  for (k in 1:n) {
    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];
  }
  target += bernoulli_logit_lpmf(Y | mu);
  target += lprior;
}
"
case_number <- as.integer(nrow(df) / 9)
indicator_i <- rep(1:9, times = case_number)
indicator_j <- rep(1:case_number, each = 9)
df$i <- indicator_i
df$j <- indicator_j

df_NA <- df %>% filter(!is.na(y))

data <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)
fit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)
```

```r
x_samples <- extract(fit, pars = "X")$X

plot_dataframe <- data.frame(
  legislator = colnames(Rehnquist)[1:9],
  mean = apply(x_samples, 2, mean),
  lower = apply(x_samples, 2, quantile, probs = 0.025),
  upper = apply(x_samples, 2, quantile, probs = 0.975)
)
ggplot(plot_dataframe, aes(x = mean, y = legislator)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Mean", y = "Legislator", title = "Ideal Points of Rehnquist")

```

![](Images/stan2.png){width=70% fig-align=center}

なぜだか不思議な結果になってしまった．どうしてだろうか？

<!-- 

```r
formula_2PL <- bf(
  y ~ 0 + (1 + name | case)
)
fit_2PL <- update(fit_1PL, formula = formula_2PL, cores = 4, iter = 3000)
```

```r
summary(fit_2PL)
```

    Family: bernoulli 
      Links: mu = logit 
    Formula: y ~ (1 | case) + (case | name) - 1 
      Data: df (Number of observations: 4343) 
      Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
            total post-warmup draws = 4000

    Multilevel Hyperparameters:
    ~case (Number of levels: 485) 
                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
    sd(Intercept)     1.05      0.06     0.93     1.18 1.00     1800     2643

    ~name (Number of levels: 9) 
                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
    sd(Intercept)           1.59      0.44     0.96     2.70 1.00      767     1456
    sd(case)                0.00      0.00     0.00     0.00 1.00     1144     2343
    cor(Intercept,case)    -0.06      0.48    -0.87     0.87 1.00     4220     2337

    Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
    and Tail_ESS are effective sample size measures, and Rhat is the potential
    scale reduction factor on split chains (at convergence, Rhat = 1).


```r
plot(fit_2PL)
```

![](Images/plot_2PL.png){width=70% fig-align=center}

```r
ranef_legislator <- ranef(fit_2PL)$name
posterior_means <- ranef_legislator[,1,"Intercept"]
lower_bounds <- ranef_legislator[,3,"Intercept"]
upper_bounds <- ranef_legislator[,4,"Intercept"]
plot_legislator <- data.frame(
  legislator = rownames(ranef_legislator),
  mean = posterior_means,
  lower = lower_bounds,
  upper = upper_bounds
)
p_2PL <- ggplot(plot_legislator, aes(x = mean, y = legislator)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "2PL Model",
       x = "Posterior Estimate",
       y = "Legislator")
grid.arrange(p_1PL, p_2PL, nrow = 1)
```

![](Images/grid_arrange.png)

よりモデルの不確実性が減って，$0$ の周りに縮小されたことがわかる．これは一般の項目反応モデルで見られる：

::: {#lst-embedding}
:::

左派が右に表示されてしまっていることは変わらない．
-->

### 識別可能性 {#sec-identification}

実は２母数ロジットモデルでは $\wt{\al}_j,x_i$ に同じ数を足した場合と $\beta_j$ と $(\wt{\al}_j,x_i)$ に同じ数を乗じた／除した場合，全く等価なモデルが得られる．すなわちスケールを定める必要がある．

ベイズ推定では，次のような事前分布と階層構造を置くことでこの問題を回避できる：
$$
x_i\iidsim\rN(0,1),\qquad \al_j=\mu_\al+\ep_\al,\qquad\ep_\al\iidsim\rN(0,\sigma^2_\al),
$$
$$
\beta_j=\mu_\beta+\ep_\beta,\qquad\ep_\beta\iidsim\rN(0,\sigma^2_\beta).
$$
<!-- 
```r
library(rstan)
stan_code <- "
data {
  int<lower=1> n;  // n = N * J - #(NA responses)
  int<lower=1> N;  // number of judges
  int<lower=1> J;  // number of cases

  array[n] int<lower=0, upper=1> Y;  // response variable
  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]
  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]
}
parameters {
  vector[N] X;  // ideal points
  vector[J] alpha_zero;  // item effects
  vector[J] beta_zero;  // item discremination

  real<lower=0> sigma_al;  // sd of alpha
  real<lower=0> sigma_beta;  // sd of beta

  vector[J] alpha;  // real values of alpha
  vector[J] beta;  // real values of beta
}
transformed parameters {
  real lprior = 0;

  lprior += student_t_lpdf(alpha_zero | 3, 0, 2.5);
  lprior += student_t_lpdf(beta_zero | 3, 0, 2.5);
  lprior += student_t_lpdf(sigma_al | 3, 0, 2.5);
  lprior += student_t_lpdf(sigma_beta | 3, 0, 2.5);
  lprior += std_normal_lpdf(X);
}
model {
  alpha ~ normal(alpha_zero, sigma_al);
  beta ~ normal(beta_zero, sigma_beta);
  vector[n] mu = rep_vector(0, n);
  for (k in 1:n) {
    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];
  }
  target += bernoulli_logit_lpmf(Y | mu);
  target += lprior;
}
"
case_number <- as.integer(nrow(df) / 9)
indicator_i <- rep(1:9, times = case_number)
indicator_j <- rep(1:case_number, each = 9)
df$i <- indicator_i
df$j <- indicator_j

df_NA <- df %>% filter(!is.na(y))

data <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, i = df_NA$i, j = df_NA$j)
fit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)
```

```r
x_samples <- extract(fit, pars = "X")$X

plot_dataframe <- data.frame(
  legislator = colnames(Rehnquist)[1:9],
  mean = apply(x_samples, 2, mean),
  lower = apply(x_samples, 2, quantile, probs = 0.025),
  upper = apply(x_samples, 2, quantile, probs = 0.975)
)
ggplot(plot_dataframe, aes(x = mean, y = legislator)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Mean", y = "Legislator", title = "Ideal Points of Rehnquist")
```
-->


![](Images/stan3.png){width=70% fig-align=center}

これにより少しは傾向が見られるようになったが，それでも事後分散が大きい．なぜだろうか？

最後に $\beta_j$ の符号の問題があるためである．$\beta_j$ を $-1$ 倍させることで $\wt{\al}_j,x_i$ の役割を $-1$ 倍させることができる．このまま推定すると事後分布は $0$ に関して対称な形を持つことになる．

$\beta_j$ の符号を制約したり，特定の判事の $x_i$ を固定して参照点とするなどの方法があるかもしれないが，ここでは [2.2.3 節 @Bafumi+2005 p.178] に倣って，階層モデルの方法により，構造的なやり方でモデルに情報を伝える．

というのも，理想点 $x_i$ に次の階層構造を入れるのである：
$$
x_i=\delta+\gamma z_i+\ep_i\qquad\ep_i\iidsim\rN(0,1).
$$

$z_i$ は当該判事を示した大統領の所属政党を表す２値変数で，共和党ならば $z_i=1$ とする．そして $\gamma$ に $\R_+$ 上に台を持つ事前分布を置く．

::: {.callout-important appearance="simple" icon="false" title="[@Bafumi+2005] による理想点モデルの階層化"}

このように共変量を適切な階層に追加することは，モデルに自然な形で正則化情報を伝えることに繋がり，モデルの識別やより現実的な推定値の獲得に繋がる．

:::


### 階層ベイズ推定

#### 指名大統領の政党属性

続いて [-@sec-identification] で検討した，[@Bafumi+2005] による階層ベイズモデルにより緩やかに情報を伝えることで識別可能性を保つ方法を検討する（第 [-@sec-Bafumi] 節）．

```{r}
df <- df %>%
  mutate(
    nominator = case_when(
      name %in% c("Rehnquist", "Stevens") ~ "Nixon",
      name %in% c("O.Connor", "Scalia", "Kennedy") ~ "Reagan",
      name %in% c("Souter", "Thomas") ~ "Bush",
      name %in% c("Breyer", "Ginsburg") ~ "Clinton"
    )
  )
df$x <- ifelse(
  df$nominator %in% c("Nixon", "Reagan", "Bush", "Trump"),
  1, -1)
```

#### 階層２母数モデル {#sec-Bafumi}

`x` の情報を階層的に伝えるには，もはや `brms` パッケージでは実行できないようである．

::: {.callout-caution title="Stan コードの出力" collapse="true" icon="false"}

`brms` パッケージでは [`stancode()`](https://paulbuerkner.com/brms/reference/stancode.html) 関数を用いて Stan コードを出力できる．

```r
stancode(fit_2PL)
```

結果は以下の通りになる：

```stan
// generated with brms 2.21.0
functions {
 /* compute correlated group-level effects
  * Args:
  *   z: matrix of unscaled group-level effects
  *   SD: vector of standard deviation parameters
  *   L: cholesky factor correlation matrix
  * Returns:
  *   matrix of scaled group-level effects
  */
  matrix scale_r_cor(matrix z, vector SD, matrix L) {
    // r is stored in another dimension order than z
    return transpose(diag_pre_multiply(SD, L) * z);
  }
}
data {
  int<lower=1> N;  // total number of observations
  array[N] int Y;  // response variable
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  array[N] int<lower=1> J_1;  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  // data for group-level effects of ID 2
  int<lower=1> N_2;  // number of grouping levels
  int<lower=1> M_2;  // number of coefficients per level
  array[N] int<lower=1> J_2;  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_2_1;
  vector[N] Z_2_2;
  int<lower=1> NC_2;  // number of group-level correlations
  int prior_only;  // should the likelihood be ignored?
}
transformed data {
}
parameters {
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  array[M_1] vector[N_1] z_1;  // standardized group-level effects
  vector<lower=0>[M_2] sd_2;  // group-level standard deviations
  matrix[M_2, N_2] z_2;  // standardized group-level effects
  cholesky_factor_corr[M_2] L_2;  // cholesky factor of correlation matrix
}
transformed parameters {
  vector[N_1] r_1_1;  // actual group-level effects
  matrix[N_2, M_2] r_2;  // actual group-level effects
  // using vectors speeds up indexing in loops
  vector[N_2] r_2_1;
  vector[N_2] r_2_2;
  real lprior = 0;  // prior contributions to the log posterior
  r_1_1 = (sd_1[1] * (z_1[1]));
  // compute actual group-level effects
  r_2 = scale_r_cor(z_2, sd_2, L_2);
  r_2_1 = r_2[, 1];
  r_2_2 = r_2[, 2];
  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)
    - 1 * student_t_lccdf(0 | 3, 0, 2.5);
  lprior += student_t_lpdf(sd_2 | 3, 0, 2.5)
    - 2 * student_t_lccdf(0 | 3, 0, 2.5);
  lprior += lkj_corr_cholesky_lpdf(L_2 | 1);
}
model {
  // likelihood including constants
  if (!prior_only) {
    // initialize linear predictor term
    vector[N] mu = rep_vector(0.0, N);
    for (n in 1:N) {
      // add more terms to the linear predictor
      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];
    }
    target += bernoulli_logit_lpmf(Y | mu);
  }
  // priors including constants
  target += lprior;
  target += std_normal_lpdf(z_1[1]);
  target += std_normal_lpdf(to_vector(z_2));
}
generated quantities {
  // compute group-level correlations
  corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);
  vector<lower=-1,upper=1>[NC_2] cor_2;
  // extract upper diagonal of correlation matrix
  for (k in 1:M_2) {
    for (j in 1:(k - 1)) {
      cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];
    }
  }
}
```

`function` ブロックでは `scale_r_cor()` 関数が定義されている．標準化された項目変数 $(\al_j,\beta_j)$ を `z` として，その各次元の標準偏差を含む２次元ベクトルを `sd_2`，Cholesky 因子を `L_2` として，行列積 `sd_2*L2` にベクトル `z` を掛けて返している．これは $(\al_j,\beta_j)$ を３つの要素（対角行列・Cholesky 因子・標準化されたベクトル）に因数分解して計算していることに起因する．

`data` ブロックでデータのコーディングを定めている．`Y` は $NJ$ 行列である．`case` が `ID2` で `name` が `ID1` に対応する．判事 $i\in[n]$ は `N_1` 人，件数 $j\in[J]$ は `N_2` 件．このデータから `M_1=1` 次元の理想点を持った `M_2=2` パラメータモデルを推定する．`Z_1_1` が判事を表す標示変数で，項目を表す標示変数は `Z_2_1` と `Z_2_2` である．

`parameter` ブロックでは標準化された理想点 `z_1` と項目パラメータ `z_2`，それぞれの標準偏差 `sd_1`, `sd_2` を宣言している．`z_1` だけ `N_1` ベクトル，`z_2` は `N_2` 行 `M_2` 列の行列であることに注意．

`transformed_parameters` で真のスケールに戻す．`r_1_1=(sd_1[1]*(z_1[1]))` は理想点 $x_1$ にあたり，`r_2=scale_r_cor(z_2,sd_2,L_2)` は項目パラメータ $(\al_j,\beta_j)$ にあたる．その後 `r_2_1=r_2[,1]` と `r_2_2=r_2[,2]` でそれぞれ $\al_j$ と $\beta_j$ に分解している．最後に `sd_1`, `sd_2` に t-分布，`L_2` に Cholesky 分布を事前分布として定義している．

`model` で尤度を定義している．`mu` はこのブロックでしか使われない線型予測子の格納変数である．
```stan
mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n];
```
により $\al_j+\beta_j x_i$ が計算されている．$j$ は `J_2[n]`, $i$ は `J_1[n]` で表されている．最後に `bernoulli_logit_lpmf(Y | mu)` で尤度が定義される．

`generated quantities` ブロックでは相関行列 `cor_2` を計算している．

:::

```r
library(rstan)
stan_code <- "
data {
  int<lower=1> n;  // n = N * J - #(NA responses)
  int<lower=1> N;  // number of judges
  int<lower=1> J;  // number of cases

  array[n] int<lower=0, upper=1> Y;  // response variable
  vector[N] Z;  // covariates for judges
  array[n] int<lower=1, upper=N> i;  // indicator for judges i in [N]
  array[n] int<lower=1, upper=J> j;  // indicator for cases j in [J]
}
parameters {
  vector[N] X;  // ideal points for judges
  vector[J] alpha;
  vector[J] beta;

  real delta;
  real<lower=0> gamma;
}
transformed parameters {
  real lprior = 0;

  lprior += std_normal_lpdf(delta);
  lprior += std_normal_lpdf(gamma);
  lprior += std_normal_lpdf(alpha);
  lprior += std_normal_lpdf(beta);
  lprior += std_normal_lpdf(X);
}
model {
  X ~ normal(delta + Z * gamma, 1);

  vector[n] mu = rep_vector(0, n);
  for (k in 1:n) {
    mu[k] = alpha[j[k]] + beta[j[k]] * X[i[k]];
  }
  target += bernoulli_logit_lpmf(Y | mu);
  target += lprior;
}
"
case_number <- as.integer(nrow(df) / 9)
indicator_i <- rep(1:9, times = case_number)
indicator_j <- rep(1:case_number, each = 9)
df$i <- indicator_i
df$j <- indicator_j

df_NA <- df %>% filter(!is.na(y))

data <- list(Y = df_NA$y, n = nrow(df_NA), N = 9, J = case_number, Z = df_NA$x[1:9], i = df_NA$i, j = df_NA$j)
fit <- stan(model_code = stan_code, data = data, chains = 4, cores = 4, verbose = TRUE, iter = 4000, warmup = 3000)
```

`ESS` が低く，`R-hat` が大きく，さらに `maximum-treedepth` を越して発散したものがあるという．`X[1]` のサンプルを見てみると $10^7$ というオーダーが出現していた．そこから `X` に事前分布を置き忘れていたことに気づいた．

```r
x_samples <- extract(fit, pars = "X")$X

plot_dataframe <- data.frame(
  legislator = colnames(Rehnquist)[1:9],
  mean = apply(x_samples, 2, mean),
  lower = apply(x_samples, 2, quantile, probs = 0.025),
  upper = apply(x_samples, 2, quantile, probs = 0.975)
)
ggplot(plot_dataframe, aes(x = mean, y = legislator)) +
  geom_point() +
  geom_errorbar(aes(xmin = lower, xmax = upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Mean", y = "Legislator", title = "Ideal Points of Rehnquist")

```

## 理想点解析パッケージ一覧

本節では次の３つのパッケージを紹介する：

::: {.callout-tip appearance="simple" icon="false"}

* `pscl` [@Zeileis+2008]：[GitHub](https://github.com/atahk/pscl), [CRAN](https://cran.r-project.org/web/packages/pscl/index.html)．[@Arnold2018] も参照．
* `MCMCpack` [@Martin+2011]：[GitHub](https://github.com/cran/MCMCpack), [CRAN](https://cran.r-project.org/web/packages/MCMCpack/index.html)
* `emIRT` [@Imai+2016]：[GitHub](https://github.com/kosukeimai/emIRT), [CRAN](https://cran.r-project.org/web/packages/emIRT/index.html)

:::

### `pscl` パッケージ

```r
install.packages("pscl")
```

#### `voteview` データ

このパッケージでは，Keith T. Poole と Howard Rosenthal が 1995 年から運営しているサイト [`voteview.com`](https://voteview.com/) のデータを利用するための関数 `readKH()` が提供されている．

例えば連邦議会 (U.S. Congress) 117 議会期 (Congress) 2021.1.3-2023.1.3 の上院 (Senate) の点呼投票データを読み込むには以下のようにする：^[１つの議会期 (Congress) は２つの会期 (Session)，第１会期と第２会期から構成される．]

```{r}
#| output: false
library(pscl)
s117 <- readKH("https://voteview.com/static/data/out/votes/S117_votes.ord",
                desc="117th U.S. Senate")
```

`s117` は `rollcall` オブジェクト，８つのフィールドを持った配列である．

`s117$votes` データは $n=104$ 議員の計 $m=949$ 回の投票からなる $10$-値の行列である．

```{r}
summary(s117)
```

#### 点呼投票データ

点呼投票データとは $n\times m$ の行列で，そのエントリーは２値変数である（今回は $1$ か $6$）．

しかし実際には種々の欠測により，$0,7,9$ も使われる．

これをヒートマップで可視化してみる．

```{r}
#| output: false
#| code-fold: false
library(tidyverse)

votes_df <- as.data.frame(s117$votes[1:15, 1:15]) %>% rownames_to_column("Legislator")  # 投票データをデータフレームに変換し、行名を列として追加

votes_long <- votes_df %>% pivot_longer(cols = -Legislator, names_to = "Vote", values_to = "value")  # データを長形式に変換
```
```{r}
ggplot(votes_long, aes(x = Vote, y = Legislator, fill = value)) + geom_tile() + scale_fill_gradient(low = "white", high = "red") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = "Votes", y = "Legislators", title = "Voting Patterns")  # ヒートマップを作成
```

#### 政党毎の賛成率

政党でソートし，賛成率を最初の 15 法案についてプロットしたものは次の通り：

```{r}
#| output: false
#| code-fold: true
library(dplyr)

# 政党ごとの賛成票の割合を計算
party_votes <- s117$votes %>%
  as.data.frame() %>%
  mutate(party = s117$legis.data$party) %>%
  group_by(party) %>%
  summarise(across(everything(), ~mean(. == 1, na.rm = TRUE)))

# データを長形式に変換
party_votes_long <- party_votes %>% pivot_longer(cols = -party, names_to = "Vote", values_to = "value")

# DとRのデータのみを抽出
party_votes_d <- party_votes_long %>% filter(party == "D")
party_votes_r <- party_votes_long %>% filter(party == "R")

# Democrats (D) のデータのみをプロット
ggplot(party_votes_d, aes(x = as.numeric(gsub("Vote ", "", Vote)), y = value)) +
  geom_line(color = "blue") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(x = "Votes", y = "Proportion of Yea votes",
       title = "Proportion of Yea votes for Democrats")
```
```{r}
# Democrats (D) と Republicans (R) のデータを同じプロットに追加
ggplot() +
  geom_line(data = party_votes_d[1:15,], aes(x = as.numeric(gsub("Vote ", "", Vote)), y = value, color = "Democrat"), linewidth = 0.5) +
  geom_line(data = party_votes_r[1:15,], aes(x = as.numeric(gsub("Vote ", "", Vote)), y = value, color = "Republican"), linewidth = 0.5) +
  scale_color_manual(values = c("Democrat" = "blue", "Republican" = "red")) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(x = "Votes", y = "Proportion of Yea votes", color = "Party",
       title = "Proportion of Yea votes by Party")
```

民主党の 0-1 がはっきりした投票行動が見られる．

```{r}
s109 <- readKH("https://voteview.com/static/data/out/votes/S109_votes.ord",
                desc="109th U.S. Senate")
```

#### ベイズ推定

`pscl` パッケージでは，`rollcall` オブジェクトに対して `ideal()` 関数を用いてデータ拡張に基づく Gibbs サンプラーを通じた理想点解析を行うことができる．

[`ideal()` 関数のマニュアル](https://github.com/atahk/pscl/blob/master/man/ideal.Rd) に記載された例では `maxiter=260E3, burnin=10E3, thin=100` での実行が例示されているが，ここでは簡単に実行してみる．

```{r}
#| output: false
n <- dim(s117$legis.data)[1]
x0 <- rep(0,n)
x0[s117$legis.data$party=="D"] <- -1
x0[s117$legis.data$party=="R"] <- 1

library(tictoc)
tic("ideal() fitting")

id1 <- ideal(s117,
             d=1,
             startvals=list(x=x0),
             normalize=TRUE,
             store.item=TRUE,
             maxiter=10000,  # MCMCの反復回数
             burnin=5000,
             thin=50,  # 間引き間隔
             verbose=TRUE)
toc()
```

`ideal() fitting: 43.938 sec elapsed` であった．

```{r}
plot(id1)
```

[`plot.ideal()` 関数のマニュアル](https://github.com/atahk/pscl/blob/master/man/plot.ideal.Rd) にある通り，`shoALLNames = FALSE` がデフォルトになっている．

```{r}
#| output: false
summary(id1)  # 全議員の正確な推定値が見れる．
```

もっとも保守的な議員として Trump，５番目にリベラルな議員として Biden の名前がみえる．Harris は中道である．

### `MCMCpack` パッケージ

#### ロジットモデルの推定

```{r}
#| output: false
library(MCMCpack)
# データの生成
x1 <- rnorm(1000)  # 説明変数1
x2 <- rnorm(1000)  # 説明変数2
Xdata <- cbind(1, x1, x2)  # デザイン行列

# 真のパラメータ
true_beta <- c(0.5, -1, 1)

# 応答変数の生成
p <- exp(Xdata %*% true_beta) / (1 + exp(Xdata %*% true_beta))
y <- rbinom(1000, 1, p)

# MCMClogitでサンプリング
posterior <- MCMClogit(y ~ x1 + x2,    # モデル式
                      burnin = 1000,    # バーンイン期間
                      mcmc = 10000,     # MCMCの反復回数
                      thin = 1,         # 間引き数
                      verbose = 1000)   # 進捗表示間隔
```

```{r}
# 結果の確認
summary(posterior)
plot(posterior)
```

#### 変化点解析

[@Chib1998] に基づく変化点モデルのベイズ推定の関数 `MCMCpoissonChange()` も実装されている．詳しくは [@Martin+2011] 第4節参照．

### `emIRT` パッケージ

```r
install.packages("emIRT")
```

このパッケージには備え付けの 80-110 議会期の上院における点呼投票データ `dwnom` がある．

このデータに対して，階層モデルを用いた理想点解析を行う関数 `hierIRT()` がある．

```{r}
#| eval: false
library(emIRT)
data(dwnom)

## This takes about 10 minutes to run on 8 threads
## You may need to reduce threads depending on what your machine can support
lout <- hierIRT(.data = dwnom$data.in,
                    .starts = dwnom$cur,
                    .priors = dwnom$priors,
                    .control = {list(
                    threads = 8,
                    verbose = TRUE,
                    thresh = 1e-4,
				    maxit=200,
				    checkfreq=1
                        )})

## Bind ideal point estimates back to legislator data
final <- cbind(dwnom$legis, idealpt.hier=lout$means$x_implied)

## These are estimates from DW-NOMINATE as given on the Voteview example
## From file "SL80110C21.DAT"
nomres <- dwnom$nomres

## Merge the DW-NOMINATE estimates to model results by legislator ID
## Check correlation between hierIRT() and DW-NOMINATE scores
res <- merge(final, nomres, by=c("senate","id"),all.x=TRUE,all.y=FALSE)
cor(res$idealpt.hier, res$dwnom1d)
```

