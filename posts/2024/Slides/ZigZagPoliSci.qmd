---
title: "Zig-Zag Sampler"
subtitle: "A MCMC Game-Changer"
author:
  - name: "Hirofumi Shiba"
    orcid: 0009-0007-8251-1224
    affiliations: 
      - name: "Institute of Statistical Mathematics"
      - name: "the University of Tokyo"
        url: https://esrp.rcast.u-tokyo.ac.jp/experts/hirofumi-shiba/?lang=en
date: "9/10/2024"
categories: [Slide, MCMC]
image: zigzag_fps14_WhiteBackground.gif
toc-title: "Table of Contents"
format:
  html: default
  revealjs:
    output-file: ZigZagPoliSci_Slides.html
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ../../../assets/profile.jpg
    css: ../../../assets/slides.css
    footer: |
      [Hirofumi Shiba](https://162348.github.io/posts/2024/Slides/ZigZagPoliSci.html)
    scrollable: true
    # smaller: true
    controls: true
    controls-layout: bottom-right
    self-contained-math: true
    shift-heading-level-by: -1
    toc: true
    toc-depth: 1
    toc-title: "Today's Menu"
    number-sections: true
    theme: serif
    show-slide-number: all
    include-in-header: ../../../assets/include-in-header.html
    tbl-cap-location: bottom
    margin: 0.05
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
csl: ../../../assets/apalike.csl
description: |
  Slides are available <a href="https://162348.github.io/posts/2024/Slides/ZigZagPoliSci_Slides.html" style="text-decoration: underline;">here</a>.
comment: false
code-fold: false
execute:
    cache: true
html-math-method: katex
abstract-title: Abstract
abstract: |
  Zig-Zag sampler, known as an instance of 'Continuous-time MCMC', is a cutting-edge sampling method that exhibits scalability and state-of-the-art performance on high-dimensional models including logistic models etc. This talk includes a brief introduction to the Zig-Zag sampler and the two important properties, irreversibility of its dynamics and thinning of Poisson point processes, followed by a few numerical experiments on logistic models with large and unbalanced data.
citation: 
  type: speech
  container-title: University of Seoul, Gwanak (관악) campus, South Korea
license: "CC BY"
format-links:
  - text: pdf
    href: Files/ZigZagPoliSci.pdf
    icon: file-pdf
---

## The Zig-Zag Sampler: What Is It? {#sec-Zig-Zag}

A continuous-time variant of MCMC algorithms

![Trajectory for Zig-Zag Sampler. Please attribute Hirofumi Shiba. <i class="fa-brands fa-creative-commons"></i> <i class="fa-brands fa-creative-commons-by"></i>](zigzag_fps14.gif)

{{< include ../../../assets/_preamble.qmd >}}

### Keywords: PDMP (1/2)

[PDMP]{.underline} (Piecewise Deterministic^[Mostly [deterministic]{.color-unite} with the exception of random jumps happens at random times] Markov Process^[[Continuous-time]{.color-unite}, instead of discrete-time processes]) [@Davis1984]

1. Mostly [deterministic]{.color-unite} with the exception of random jumps happens at random times
2. [Continuous-time]{.color-unite}, instead of discrete-time processes

{{< fa arrow-right >}} Plays a [complementary role]{.color-unite} to SDEs / Diffusions

| Property | PDMP | SDE |
|:------:|:------:|:------:|
| Exactly simulatable? | [{{< fa check >}}]{.color-red} | [{{< fa xmark >}}]{.color-blue} |
| Subject to discretization errors? | [{{< fa xmark >}}]{.color-blue} | [{{< fa check >}}]{.color-red} |
| Driving noise | Poisson | Gauss |

: {.hover .responsive-sm tbl-colwidths="[60,20,20]"}

::: {.callout-note appearance="simple" title="History of PDMP Applications"}

1. First applications: control theory, operations research, etc. [@Davis1993]
2. Second applications: Monte Carlo simulation in material sciences [@Peters-deWith2012]
3. Third applications: Bayesian statistics [@Bouchard-Cote+2018-BPS]

:::

### Keywords: PDMP (2/2)

* We will concentrate on Zig-Zag sampler [@Bierkens+2019]
* Other PDMPs: Bouncy sampler [@Bouchard-Cote+2018-BPS] , Boomerang sampler [@Bierkens+2020]

![The most famous three PDMPs. Animated by [@Grazzi2020]](pdmps.gif)

### Menu

::: {.callout-tip appearance="simple" icon="false" title="What We've Learned"}

The new algorithm 'Zig-Zag Sampler' is based on comtinuous-time process called [PDMP]{.color-unite}.

:::

::: {.callout-tip appearance="simple" icon="false" title="What We'll Learn in the Rest of this @sec-Zig-Zag"}

We will review 3 instances of the standard (discrete-time) MCMC algorithm: [MH]{.color-unite}, [Lifted MH]{.color-unite}, and [MALA]{.color-unite}.

1. Review: [MH]{.color-unite} (Metropolis-Hastings) algorithm
2. Review: [Lifted MH]{.color-unite}, A method bridging [MH]{.color-unite} and Zig-Zag
3. Comparison: [MH]{.color-unite} vs. [Lifted MH]{.color-unite} vs. Zig-Zag
4. Review: [MALA]{.color-unite} (Metropolis Adjusted Langevin Algorithm)
5. Comparison: Zig-Zag vs. [MALA]{.color-unite}

:::

### Review: Metropolis-Hastings (1/2)

::: {.callout-tip appearance="simple" icon="false" title="[@Metropolis+1953]-[@Hastings1970]"}

Input: Target distribution $p$, (symmetric) proposal distribution $q$

1. Draw a $X_t\sim q(-|X_{t-1})$
2. Compute
  $$
  \alpha(X_{t-1}, X_t) = \frac{p(X_t)}{p(X_{t-1})}
  $$
3. Draw a uniform random number $U\sim\rU([0,1])$.
4. If $\al(X_{t-1},X_t)\le U$, then $X_t\gets X_{t-1}$. Do nothing otherwise.
5. Return to Step 1.

:::

::: aside

MH algorithm works even without $p$'s normalizing constant. Hence, its ubiquity.

:::

### Review: Metropolis-Hastings (2/2)

::: {.small-letter}

Alternative View: MH is a generic procedure to turn a [simple $q$-Markov chain]{.color-unite} into a [Markov chain converging to $p$]{.color-unite}.

:::

::: {.callout-tip appearance="simple" icon="false" title="The Choise of Proposal $q$"}

* [Random Walk Metropolis]{.underline} [@Metropolis+1953]: Uniform / Gaussian
  $$
  q(y|x) = q(y-x) \in\left\{ \dd{\rU([0,1])}{\lambda}(y-x),\dd{\rN(0,\Sigma)}{\lambda}(y-x)\right\}
  $$
* [Hybrid / Hamiltonian Monte Carlo]{.underline} [@Duane+1987]: Hamiltonian dynamics
  $$
  q(y|x) = \delta_{x + \ep \rho},\qquad\ep>0,\;\rho\;\text{: momentum defined via Hamiltonian}
  $$
* [Metropolis-adjusted Langevin algorithm]{.underline} (MALA) [@Besag1994]: Langevin diffusion
  $$
  q(-|X_t):=\text{ the transition probability of } X_t \text{ where } dX_t=\nabla\log p(X_t)\,dt+\sqrt{2\beta^{-1}}dB_t.
  $$

:::

<!-- ![](Files/arrow-right.svg){.absolute right=-250 top=0 width="1000" height="100" style="transform: rotate(90deg);" .color-unite} -->

### Problem: [Reversibility]{.color-unite}

[Reversibility]{.color-unite} (a.k.a detailed balance):
$$
p(x)q(x|y)=p(y)q(y|x).
$$
In words:
$$
\text{Probability}[\text{Going}\;x\to y]=\text{Probability}[\text{Going}\;y\to x].
$$
{{< fa arrow-right >}} Harder to explore the entire space

{{< fa arrow-right >}} Slow mixing of [MH]{.color-unite}

::: aside

From the beginning of 21th century, many efforts have been made to make [MH]{.color-unite} [irreversible]{.color-unite}.

:::

### Lifting (1/3)

::: {.small-letter}

[Lifting]{.color-unite}: A method to make MH's dynamics [irreversible]{.color-unite}

How?: By adding an auxiliary variable $\sigma\in\{\pm1\}$, called [momentum]{.color-unite}

:::

::: {.callout-tip appearance="simple" icon="false" title="Lifted MH [@Turitsyn+2011]"}

Input: Target $p$, [two]{.color-unite} proposals $q^{(+1)},q^{(-1)}$, and [momentum]{.color-unite} $\sigma\in\{\pm1\}$

1. Draw $X_t$ from $q^{(\sigma)}$
2. Do a MH step
3. If accepted, go back to Step 1.
4. If rejected, [flip the momentum]{.color-unite} and go back to Step 1.

:::

### Lifting (2/3)

::: {layout="[50,50]" layout-valign="top" style="margin: 0px; !important"}

::: {#first-column}

![](Files/lifting_illustration.svg)

![](Files/potential.svg)

:::

::: {#second-column}

::: {style="margin-bottom: 50px; !important"}

$q^{(+1)}$: Only propose $\rightarrow$ moves

$q^{(-1)}$: Only propose $\leftarrow$ moves

:::

{{< fa arrow-right >}} Once going uphill, it continues to go uphill.

{{< fa arrow-right >}} This is [irreversible]{.color-unite}, since

\begin{align*}
  &\text{Probability}[x\to y]\\
  &\qquad\ne\text{Probability}[y\to x].
\end{align*}

:::

:::

### Lifting (3/3)

Reversible dynamic of MH has '[irreversified]{.color-unite .artificial-align}'

::: {layout-ncol=3}

![MH](Files/MH_traj.svg)

![[Lifted MH]{.color-unite}](Files/LMH_traj.svg)

::: {#third-column}

::: {.callout-caution appearance="simple" title="Caution"}

**Scale is different** in the vertical axis!

:::

::: small-letter

[Lifted MH]{.color-unite} successfully explores the edges of the target distribution.

:::

:::

:::

::: aside

*Irreversibility actually improves the efficiency of MCMC, as we observe in two slides later.

:::

### Comparison: MH vs. LMH vs. Zig-Zag (1/2)

::: {layout-ncol=3}
![MH](Files/MH_traj.svg)

![Lifted MH](Files/LMH_traj.svg)

![Zig-Zag](Files/zigzag_traj.svg)
:::

Zig-Zag corresponds to the [limiting case of lifted MH]{.color-unite} as the step size of proposal $q$ goes to zero, as we'll learn later.

{{< fa arrow-right >}} Zig-Zag has a maximum [irreversibility]{.color-unite}.

### Comparison: MH vs. LMH vs. Zig-Zag (2/2)

[Irreversibility]{.color-unite} actually improves the efficiency of MCMC.

Faster decay of **autocorrelation** $\rho_t\approx\Corr[X_0,X_t]$ implies

1. faster mixing of MCMC
2. lower variance of Monte Carlo estimates

::: {layout-ncol=3}
![](Files/MH_auto.svg)

![](Files/LMH_auto.svg)

![](Files/zigzag_auto.svg)

![MH](Files/MH_traj.svg)

![Lifted MH](Files/LMH_traj.svg)

![Zig-Zag](Files/zigzag_traj.svg)

:::

### Review: MALA

::: {.callout-tip appearance="simple" icon="false"}

**Langevin diffusion**: A diffusion process defined by the following SDE:

$$
dX_t=\nabla\log p(X_t)\,dt+\sqrt{2\beta^{-1}}dB_t.
$$

**Langevin diffusion** itself converges to the target distribution $p$ in the sense that ^[under fairly general conditions on $p$.]
$$
\norm{p_t-p}_{L^1}\to0,\qquad t\to\infty.
$$

:::

Two MCMC algorithms derived from **Langevin diffusion**:

::: small-letter

[ULA (Unadjusted Langevin Algorithm)]{.underline}<br>
$\quad$ Use the discretization of $(X_t)$. [Discretization errors accumulate]{.color-unite}.

[MALA (Metropolis Adjusted Langevin Algorithm)]{.underline}<br>
$\quad$ Use ULA as a proposal in MH, erasing the errors by MH steps.

:::

### Comparison: Zig-Zag vs. MALA (1/3)

How fast do they go back to high-probability regions? ^[The target here is the standard Cauchy distribution $\rC(0,1)$, equivalent to $\rt(1)$ distribution. Its heavy tails hinder the convergence of MCMC.]

::: {layout-ncol=2 style="margin: 0px; !important"}
![Zig-Zag](../Process/ZigZag_1D.svg)

![MALA](../Process/MALA_1D.svg)
:::

[Irreversibility]{.color-unite} of Zig-Zag accelerates its convergence.

### Comparison: Zig-Zag vs. MALA (2/3)

::: {layout="[40,60]" layout-valign="center"}

::: {#first-column}

![MALA trajectory](../Process/MALA_1D.svg)

:::

::: {#second-column}

::: {.callout-caution appearance="simple" title="Caution: Fake Continuity"}

The left plot looks continuous, but **it actually is not**.

:::

:::

:::

MH, including MALA, is actually a [discrete-time process]{.underline}.

The plot is obtained by [connecting the points]{.underline} by line segments.

### Comparison: Zig-Zag vs. MALA (3/3)

Monte Carlo estimation is also done differently:

::: {layout="[30,70]" layout-valign="center" style="margin: 0px; !important"}

::: {#first-column}

![](../Process/MALA_1D.svg)

:::

::: {#second-column}

[MALA]{.underline} outputs $(X_n)_{n\in[N]}$ defines

$$
\frac{1}{N}\sum_{n=1}^Nf(X_n)\xrightarrow{N\to\infty}\int_{\R^d} f(x)p(x)\,dx.
$$

:::

:::

::: {layout="[30,70]" layout-valign="top"}

::: {#first-column}

![](../Process/ZigZag_1D.svg)

:::

::: {#second-column}

[Zig-Zag]{.underline} outputs $(X_t)_{t\in[0,T]}$ defines

$$
\int^T_0f(X_t)\,dt\xrightarrow{T\to\infty}\int_{\R^d} f(x)p(x)\,dx.
$$

:::

:::

### Recap of @sec-Zig-Zag

* Zig-Zag Sampler's trajectory is a [PDMP]{.underline}.
* [PDMP]{.underline}, by design, has maximum [irreversibility]{.color-unite}.
* [Irreversibility]{.color-unite} leads to faster convergence of Zig-Zag in comparisons against [MH]{.color-unite}, [Lifted MH]{.color-unite}, and especially [MALA]{.color-unite}.

::: {layout-ncol=3}
![](Files/MH_auto.svg)

![](Files/LMH_auto.svg)

![](Files/zigzag_auto.svg)

![MH](Files/MH_traj.svg)

![Lifted MH](Files/LMH_traj.svg)

![Zig-Zag](Files/zigzag_traj.svg)

:::

## The Algorithm: How to Use It? {#sec-Algorithm}

Fast and exact simulation of continuous trajectory.

### Review: MH vs. LMH vs. Zig-Zag (1/2)

As we've learned before, Zig-Zag corresponds to the [limiting case of lifted MH]{.color-unite} as the step size of proposal $q$ goes to zero.

::: {layout-ncol=3}
![MH](Files/MH_traj.svg)

![Lifted MH](Files/LMH_traj.svg)

![Zig-Zag](Files/zigzag_traj.svg)
:::

### Review: MH vs. LMH vs. Zig-Zag (2/2)

'[Limiting case of lifted MH]{.color-unite}' means that we only simulate [**where we should flip the momentum**]{.underline} $\sigma\in\{\pm1\}$ in Lifted MH.

::: {layout-ncol=3}
![MH](Files/MH_traj.svg)

![Lifted MH](Files/LMH_traj.svg)

![Zig-Zag](Files/zigzag_traj.svg)
:::

### Algorithm (1/2)

'[Limiting case of lifted MH]{.color-unite}' means that we only simulate [**where we should flip the momentum**]{.underline} $\sigma\in\{\pm1\}$ in Lifted MH.

::: {.callout-tip appearance="simple" icon="false" title="[1d ^[Multidimensional extension is straightforward, but we won't cover it today.] Zig Zag sampler @Bierkens+2019]"}

**Input**: Gradient $\nabla\log p$ of log target density $p$

For $n\in\{1,2,\cdots,N\}$:

1. Simulate an first arrival time $T_n$ of a [Poisson point process]{.color-unite} (described in the next slide)
2. Linearly interpolate until time $T_n$:
    $$
    X_t = X_{T_{n-1}} + \sigma(t-T_{n-1}),\qquad t\in[T_{n-1},T_n].
    $$
3. Go back to Step 1 with the momentum $\sigma\in\{\pm1\}$ flipped

:::

### Algorithm (2/2)

::: {.callout-tip title="[Fundamental Property of Zig-Zag Sampler (1d) @Bierkens+2019]" icon="false"}

Let $U(x):=-\log p(x)$. Simluating a [Poisson point process]{.color-unite} with a rate function
$$
\lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x)
$$
ensures the Zig-Zag sampler converges to the target $p$, where $\gamma$ is an arbitrary non-negative function.

:::

Its ergodicity is ensured as long as there exists $c,C>0$ such that^[With some regularity conditions on $U$. [See @Bierkens-Roberts-Zigg2019].]
$$
p(x)\le C\abs{x}^{-c}.
$$


### Core of the Algorithm

Given a rate function
$$
\lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x)
$$
how to simulate a corresponding [Poisson point process]{.color-unite}?

::: {.callout-tip appearance="simple" icon="false" title="What We'll Learn in the Rest of this @sec-Algorithm"}

1. What is [Poisson Point Process]{.color-unite}?
2. How to Simulate It?
3. Core Technique: [Poisson Thinning]{.color-blue}

**Take Away: Zig-Zag sampling reduces to [Poisson Thinning]{.color-blue}**.

:::

### Simulating [Poisson Point Process]{.color-unite} (1/2)

::: {.callout-tip title="What is a [Poisson Point Process]{.color-unite} with rate $\lambda$?" icon="false"}

The number of points in $[0,t]$ follows a Poisson distribution with mean $\int^t_0\lambda(x_s,\sigma_s)\,ds$:
$$
N([0,t])\sim\Pois\paren{M(t)},\qquad M(t):=\int^t_0\lambda(x_s,\sigma_s)\,ds.
$$
We want to know when the first point $T_1$ falls on $\cointerval{0,\infty}$.

:::

::: {layout="[30,70]" layout-valign="center" style="margin: 0px; !important"}

::: {#first-column}

```{julia}
#| echo: false
using Distributions
using Plots

function simulate_poisson_process(λ, T)
    # ポイントの数を生成
    N = rand(Poisson(λ * T))
    
    # ポイントの位置を生成
    points = sort(rand(Uniform(0, T), N))
    
    return points
end

# パラメータ設定
λ = 1.0  # 強度パラメータ
T = 10.0  # 時間間隔

# シミュレーション実行
points = simulate_poisson_process(λ, T)
cumulative_counts = collect(1:length(points))

# プロット
p = plot(title="Poisson Point Process with \$\\lambda\\equiv1\$", xlabel="Time", ylabel="\$y\$", legend=false
, background_color="#F0F1EB", titlefontcolor=colorant"#E95420", size=(400, 300))
# 累積数のプロット
plot!(points, cumulative_counts, color="#0096FF", linewidth=2, linetype=:steppost)
# 点のプロット
scatter!(points, zeros(length(points)), color="#E95420", markersize=5)

display(p)
```

:::

::: {#second-column}

When $\displaystyle\lambda(x,\sigma)\equiv c\;(\text{constant})$,

* [blue line]{.color-blue}: [Poisson Process]{.color-blue}

* [red dots]{.color-unite}: [Poisson Point Process]{.color-unite}

satisfying $\displaystyle\textcolor{#0096FF}{N_t}=\textcolor{#E95420}{N([0,t])}\sim\Pois(ct)$.

:::

:::

<!--
### Simulating [Poisson Point Process]{.color-unite} (2/3)

::: {.callout-tip title="Proposition (Simulation of Poisson Point Process)" icon="false"}

The first arrival time $T_1$ of a [Poisson Point Process]{.color-unite} with rate $\lambda$ can be simulated by
$$
T_1\deq M^{-1}(E),\qquad E\sim\Exp(1),M(t):=\int^t_0\lambda(x_s,\sigma_s)\,ds,
$$
where $\Exp(1)$ denotes the exponential distribution with parameter $1$.

:::

Reminder: Poisson Process with $\lambda\equiv c\in\R$ can be simulated as
$$
T_1\sim\Exp(c).
$$
In addition, $T_{n}-T_{n-1}\sim\Exp(c)$ for all $n\in\{2,3,\cdots\}$.
-->

### Simulating [Poisson Point Process]{.color-unite} (2/2)

::: {.callout-tip title="Proposition (Simulation of Poisson Point Process)" icon="false"}

The first arrival time $T_1$ of a [Poisson Point Process]{.color-unite} with rate $\lambda$ can be simulated by
$$
T_1\deq M^{-1}(E),\qquad E\sim\Exp(1),M(t):=\int^t_0\lambda(x_s,\sigma_s)\,ds,
$$
where $\Exp(1)$ denotes the exponential distribution with parameter $1$.

:::

Since $\displaystyle\lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x)$, $M$ can be quite complicated.

{{< fa arrow-right >}} Inverting $M$ can be impossible.

{{< fa arrow-right >}} We need more general techniques: [Poisson Thinning]{.color-blue}.

<!-- 
1d: $x\in\R,\sigma\in\{\pm1\}$
$$
\lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x)
$$

Multidimensional: $x\in\R^d,\sigma\in\{\pm1\}^d$
$$
\lambda_i(x,\sigma):=\Paren{\sigma\cdot \nabla U_i(x)}_++\;\gamma_i(x,\sigma_{-i})
$$
-->

### [Poisson Thinning]{.color-blue} (1/2)

::: {.callout-tip title="[@Lewis-Shedler1979]" icon="false"}

To obtain the first arrival time $T_1$ of a [Poisson Point Process]{.color-unite} with rate $\lambda$,

1. Find a bound $M$ that satisfies
$$
m(t):=\int^t_0\lambda(x_s,\sigma_s)\,ds\le M(t).
$$
2. Simulate a point $T$ from the [Poisson Point Process]{.color-unite} with intensity $M$.
3. Accept $T$ with probability $\frac{m(T)}{M(T)}$.

:::

::: small-letter

* $m(t)$: Defined via $\displaystyle\lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x)$.

* $M(t)$: Simple upper bound $m\le M$, such that $M^{-1}$ is analytically tractable.

:::

### [Poisson Thinning]{.color-blue} (2/2)

In order to simulate a [Poisson Point Process]{.color-unite} with rate
$$
\lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x),
$$
we find a [invertible upper bound $M$]{.color-blue} that satisfies
$$
\int^t_0\lambda(x_s,\sigma_s)\,ds=m(t)\le\textcolor{#0096FF}{M}(t).
$$
for all possible Zig-Zag trajectories $\{(x_s,\sigma_s)\}_{s\in[0,T]}$.

### Recap of @sec-Algorithm

1. Continuous-time MCMC, based on [PDMP]{.underline}, has an entirely different algorithm and strategy.
2. To simulate [PDMP]{.underline} is to simulate [Poisson Point Process]{.color-unite}.
3. The core technology to simulate [Poisson Point Process]{.color-unite} is [Poisson Thinning]{.color-blue}.
4. [Poisson Thinning]{.color-blue} is about finding an [upper bound $M$]{.color-blue}, with tractable inverse $M^{-1}$; Typically a polynomial function.
5. The [upper bound $M$]{.color-blue} has to be given on a case-by-case basis.

## Proof of Concept: How Good Is It? {#sec-ProofOfConcept}

Quick demonstration of the state-of-the-art performance on a toy example.

### Review: The 3 Steps of Zig-Zag Sampling

Given a target $p$,

1. Calculate the negative log-likelihood $U(x):=-\log p(x)$
2. Fix a refresh rate $\gamma(x)$ and compute the rate function
  $$
  \lambda(x,\sigma):=\Paren{\sigma U'(x)}_++\;\gamma(x).
  $$
3. Find an [invertible upper bound $M$]{.color-blue} that satisfies
  $$
  \int^t_0\lambda(x_s,\sigma_s)\,ds=:m(t)\le\textcolor{#0096FF}{M}(t).
  $$

### Model: 1d Gaussian Mean Reconstruction

::: {layout="[53,50]" layout-valign="top" style="margin: 0px; !important"}

::: {#first-column}

::: {.callout-tip appearance="simple" icon="false" title="Setting"}

* [Data]{.underline}: $y_1,\cdots,y_n\in\R$ aquired by
  $$
  y_i\iidsim\rN(x_0,\sigma^2),\qquad i\in[n],
  $$
  with $\sigma>0$ known, $x_0\in\R$ unknown.

* [Prior]{.underline}: $\rN(0,\rho^2)$ with known $\rho>0$.

* [Goal]{.underline}: Sampling from the posterior
  $$
  p(x)\propt\paren{\prod_{i=1}^n\phi(x|y_i,\sigma^2)}\phi(x|0,\rho^2),
  $$
  where $\phi(x|y,\sigma^2)$ is the $\rN(y,\sigma^2)$ density.

:::

:::

::: {#second-column}

The negative log-likelihood:
\begin{align*}
  U(x)&=-\log p(x)\\
  &=\frac{x^2}{2\rho^2}+\frac{1}{2\sigma^2}\sum_{i=1}^n(x-y_i)^2+\const,\\
  U'(x)&=\frac{x}{\rho^2}+\frac{1}{\sigma^2}\sum_{i=1}^n(x-y_i),\\
  U''(x)&=\frac{1}{\rho^2}+\frac{n}{\sigma^2}.
\end{align*}

:::

:::

### Menu

In the rest of this @sec-ProofOfConcept, we'll learn:

1. Even a [simple Zig-Zag Sampler with $\gamma\equiv0$]{.color-minty} surpasses [MALA]{.color-julia-blue}.
2. Incorporating sub-sampling, [Zig-Zag with Control Variates]{.color-unite} further improves the efficiency.

![](Files/MeanOfGaussian_addedMALA.svg){fig-align="center"}

### [Simple Zig-Zag Sampler with $\gamma\equiv0$]{.color-minty} (1/2)

Fixing $\gamma\equiv0$, we obtain [the upper bound $M$]{.color-blue}
\begin{align*}
  m(t)&=\int^t_0\lambda(x_s,\sigma_s)\,ds=\int^t_0\Paren{\sigma U'(x_s)}_+\,ds\\
  &\le\paren{\frac{\sigma x}{\rho^2}+\frac{\sigma}{\sigma^2}\sum_{i=1}^n(x-y_i)+t\paren{\frac{1}{\rho^2}+\frac{n}{\sigma^2}}}_+\\
  &=:(a+bt)_+=\textcolor{#0096FF}{M}(t),
\end{align*}

where
$$
a=\frac{\sigma x}{\rho^2}+\frac{\sigma}{\sigma^2}\sum_{i=1}^n(x-y_i),\quad b=\frac{1}{\rho^2}+\frac{n}{\sigma^2}.
$$

### Result: 1d Gaussian Mean Reconstruction

We generated 100 samples from $\rN(x_0,\sigma^2)$ with $x_0=1$.

![](Files/MeanOfGaussian_addedMALA.svg){fig-align="center"}

### MSE per Epoch: The Vertical Axis

MSE (Mean Squared Error) of $\{X_i\}_{i=1}^n$ is defined as
$$
\frac{1}{n}\sum_{i=1}^n(X_i-x_0)^2.
$$
Epoch: Unit computational cost.

::: {.callout-tip appearance="simple" icon="false" title="The following is considered as one epoch:"}

* One evaluation of a likelihood ratio
$$
\frac{p(X_{n+1})}{p(X_n)}.
$$
* One evaluation of a [Poisson Point Process]{.color-unite}.

:::

### Good News!

Case-by-case construction of an [upper bound $M$]{.color-blue} is too complicated / demanding.

Therefore, we are trying to [automate]{.color-unite} the whole procedure.

::: {.callout-tip appearance="simple" icon="false" title="Automatic Zig-Zag"}

1. Automatic Zig-Zag [@Corbella2022]
2. Concave-Convex PDMP [@Sutton-Fearnhead2023]
3. NuZZ (numerical Zig-Zag) [@Pagani+2023]

:::


## References {.unnumbered .unlisted}

::: {layout="[40,60]" layout-valign="top"}

::: {#first-column}

![Slides and codes are available here](Files/QR_ZigZagPoliSci.svg)

:::

::: {#second-column}

::: {#refs}
:::

:::

:::

## Appendix: Scalability by Subsampling {.unnumbered .unlisted visibility="uncounted"}

Construction of [ZZ-CV]{.color-unite} ([Zig-Zag with Control Variates]{.color-unite}).

### Review: 1d Gaussian Mean Reconstruction

$U'$ has an alternative form:

\begin{align*}
  U'(x)&=\frac{x}{\rho^2}+\frac{1}{\sigma^2}\sum_{i=1}^n(x-y_i)=:\frac{1}{n}\sum_{i=1}^nU'_i(x),
\end{align*}
where
$$
U'_i(x)=\frac{x}{\rho^2}+\frac{n}{\sigma^2}(x-y_i).
$$

{{< fa arrow-right >}} We only need one sample $y_i$ to evaluate $U'_i$.

### Randomized Rate Function

::: small-letter

Instead of
$$
\lambda_{\textcolor{#78C2AD}{\text{ZZ}}}(x,\sigma)=\Paren{\sigma U'(x)}_+
$$
we use
$$
\lambda_{\textcolor{#E95420}{\text{ZZ-CV}}}(x,\sigma)=\Paren{\sigma U'_I(x)}_+,\qquad I\sim\rU([n]).
$$
Then, the latter is an unbiased estimator of the former:
$$
\E_{I\sim\rU([n])}\SQuare{\lambda_{\textcolor{#E95420}{\text{ZZ-CV}}}(x,\sigma)}=\lambda_{\textcolor{#78C2AD}{\text{ZZ}}}(x,\sigma).
$$

:::

### Last Step: [Poisson Thinning]{.color-blue}

Find an [invertible upper bound $M$]{.color-blue} that satisfies
$$
\int^t_0\lambda_{\textcolor{#E95420}{\text{ZZ-CV}}}(x_s,\sigma_s)\,ds=:m_I(t)\le\textcolor{#0096FF}{M}(t),\qquad I\sim\rU([n]).
$$
It is harder to bound $\lambda_{\textcolor{#E95420}{\text{ZZ-CV}}}$, since it is now an estimator (random function).

### [Upper Bound $M$]{.color-blue} with [Control Variates]{.color-unite}

::: {layout="[53,50]" layout-valign="center" style="margin: 0px; !important"}

::: {#first-column}

::: {.callout-tip appearance="simple" icon="false" title="Preprocessing (once and for all)"}

1. Find
  $$
  x_*:=\argmin_{x\in\R}U(x)
  $$
2. Compute
  $$
  U'(x_*)=\frac{x_*}{\rho^2}+\frac{1}{\sigma^2}\sum_{i=1}^n(x_*-y_i).
  $$

:::

:::

::: {#second-column}

Then, with a re-parameterization of $m_i$,
$$
m_i(t)\le M(t):=a+bt,
$$



:::

:::

where
$$
a=(\sigma U'(x_*))_++\norm{U'}_\Lip\norm{x-x_*}_p,\qquad b:=\norm{U'}_\Lip.
$$
And $m_i$ is redefined as
$$
m_i(t)=U'(x_*)+U'_i(x)-U'_i(x_*).
$$

### Subsampling with Control Variates

Zig-Zag sampler with the random rate function
$$
\lambda_{\textcolor{#E95420}{\text{ZZ-CV}}}(x,\sigma)=\Paren{\sigma U'_I(x)}_+,\qquad I\sim\rU([n]).
$$
and the upper bound
$$
M(t)=a+bt
$$
is called [Zig-Zag with Control Variates]{.color-unite} [@Bierkens+2019].

### [Zig-Zag with Control Variates]{.color-unite}

1. has $O(1)$ efficiency as the sample size $n$ grows.^[As long as the preprocessing step is properly done.]
2. is exact (no bias).

![](Files/MeanOfGaussian_addedMALA.svg){fig-align="center"}

### Scalability (1/3) {visibility="uncounted"}

There are currently two main approaches to scaling up MCMC for large data.

1. [Devide-and-conquer]{.underline}

    Devide the data into smaller **chunks** and run MCMC on each **chunk**.

2. [Subsampling]{.underline}

    Use a subsampling estimate of the likelihood, which does not require the entire data.

### Scalability (2/3) by Devide-and-conquer {visibility="uncounted"}

Devide the data into smaller chunks and run MCMC on each chunk.

| Unbiased? | Method | Reference |
|:-:|:------:|:------:|
| [{{< fa xmark >}}]{.color-blue} | WASP | [[@Srivastava+2015]]{.footnote-letter} |
| [{{< fa xmark >}}]{.color-blue} | Consensus Monte Carlo | [[@Scott+2016]]{.footnote-letter} |
| [{{< fa check >}}]{.color-red} | Monte Carlo Fusion | [[@Dai+2019]]{.footnote-letter} |

: {.hover .responsive-sm tbl-colwidths="[20,50,35]"}

### Scalability (3/3) by Subsampling {visibility="uncounted"}

Use a subsampling estimate of the likelihood, which does not require the entire data.

| Unbiased? | Method | Reference |
|:-:|:------:|:------:|
| [{{< fa xmark >}}]{.color-blue} | Stochastic Gadient MCMC | [[@Welling-Teh2011]]{.footnote-letter} |
| [{{< fa check >}}]{.color-red} | Zig-Zag with Subsampling | [[@Bierkens+2019]]{.footnote-letter} |
| [{{< fa xmark >}}]{.color-blue} | Stochastic Gradient PDMP | [[@Fearnhead+2024]]{.footnote-letter} |

: {.hover .responsive-sm tbl-colwidths="[20,50,35]"}

<!-- ## Appendix: Application to Imbalanced Logistic Regression {.unnumbered .unlisted visibility="uncounted"}

Scalability and state-of-the-art performance on high-dimensional models, with the example of logistic models. -->