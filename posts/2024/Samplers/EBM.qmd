---
title: "エネルギーベースモデル"
subtitle: "深層生成モデル５"
author: "司馬 博文"
date: 3/30/2024
date-modified: 7/30/2024
categories: [Deep, Nature, Sampling]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
abstract-title: 概要
abstract: 確率分布を統計物理の言葉（エネルギー，分配関数など）でモデリングする方法論である．
---

{{< include ../../../_preamble.qmd >}}

## エネルギーベースモデル

### 導入

エネルギーベースのモデル (EBM: Energy-based Model) とは，密度が
$$
p(x,z)\propt e^{-H(x,z)}
$$
の形で与えられるモデルをいう．^[この形の分布族を **正準分布** または **Gibbs 分布** [@Koller-Friedman2009 p.108], [@Friedli-Velenik2017 p.25]，または **Boltzmann 分布** [@Kim-Bengio2016], [@Mezard-Montanari2009 p.23], [@Chewi2024] ともいう．物理の用語では $e^{H(z)}$ を Boltzmann 因子と呼ぶのみであるようである [@田崎晴明2008 p.107]．[@Liu2004 p.7] ではどちらも掲載している．正準集団は，NVT 一定集団ともいう．]

[VAE](../Kernels/Deep4.qmd) や [正則化流](NF.qmd) などの生成モデルや，[トランスフォーマー](../Kernels/Deep2.qmd) などの自己回帰モデルも EBM とみなせる上に，Markov 確率場などの無向グラフィカルモデルで定義される分布族や，正規化定数を除いて定義された確率分布族も，自然に EBM とみなせる．^[だが，正規化定数 $Z$ が評価できないという前提で EBM の理論は進むため，有向グラフで定義される VAE などのモデルを EBM とみる積極的な理由はない．]

これは統計物理学的な系との対応が意識されているように，入力 $z$ と出力 $x$ の整合性 $k(x,z)$ をエネルギー $H(x,z)$ の言葉で与えているモデルであると見ることができ，このエネルギー $H$ をニューラルネットワークによってモデル化するのである．

すると EBM の最尤推定とは，訓練データ $\{x_i\}_{i=1}^n$ に対して最も低いエネルギーを割り当てるエネルギー関数 $H_\theta$ を探すという基底状態探索の問題に対応する [@LeCun+2007]．

これで EBM の概要は終わりであるが，このような極めて一般的な設定で有用な一般論が得られることは驚くべきことである．

<!-- 回帰や分類などの古典的なタスクだけでなく，ほとんどの確率的モデルもこの手続きから理解することができ，この場合は EBM が非確率的な／最適化ベースの推論手法を提供するフレームワークとして働くことになる [@LeCun+2007 p.192]． -->

### エネルギー

仮に，２つの学習済みモデル $p_1(x),p_2(x)$ が EBM の形で得られており，それぞれのエネルギーが $H_1,H_2$ で与えられるとする．この際，$H:=H_1+H_2$ をエネルギーにもつ EBM は $p\propt p_1p_2$ であり，このモデルは $p_1$ でも $p_2$ でも高確率であるような $x$ を高く評価することになる．

このようにして得るモデル $p$ を **積スパートモデル** (PoE: Product of Experts) [@Hinton2002] という．^[MoE (Mixture of Expert) と並んで使う用語である．] [@Hinton2002] は引き続き，対照分離度 (contrast divergence) [-@sec-CD] による訓練法を提案している．

例えば [@Murphy2023 p.840] では，タンパク質構造の生成モデルを作りたいとして，$p_1$ を「常温で安定であるタンパク質」の生成モデル，$p_2$ を「COVID-19 のスパイクタンパク質に結合するタンパク質」の生成モデルとして説明している．

従ってこの $H$ は，データの好ましさを表すパラメータと考えられ，他モデルへの移転にも使えることが期待される．$H$ は，contrast function, value function, 負の対数尤度などとも呼ぶ [@LeCun+2007 p.193]．

### 最尤推定

EBM は，データの分布との KL 乖離度，または等価なことだが対数尤度の期待値
$$
\E[\log p_\theta]
$$
を最小化することによって学習することが考えられる [@LeCun+2007]．

しかし尤度の評価は正規化定数 $Z_\theta:=\int e^{-H_\theta(x)}\,dx$ の評価が必要であるため，一般の設定では実行できないが，勾配
$$
\nabla_\theta\log p_\theta(x)=-\nabla_\theta H_\theta(x)-\nabla_\theta\log Z_\theta
$$
は近似できる．$\nabla_\theta H_\theta(x)$ はニューラルネットワークの自動微分で計算することができ，２項目は
$$
\nabla_\theta\log Z_\theta=-(p_\theta|\nabla_\theta H_\theta)
$$
の関係を用いて，$p_\theta$ からのサンプルを用いた Monte Carlo 推定量で評価できる．

総じて，確率的勾配降下法によって学習できる．

### 対照分離度 (CD) {#sec-CD}

[@Hinton2002] は Gibbs サンプリングが可能な潜在変数を持つ EBM モデルである制限付き Boltzmann マシン（Markov 確率場の例）について，対照分離度を平衡分布 $p_\infty$ を持つ Gibbs サンプラー $\{p_t\}$ を用いて
$$\newcommand{\CD}{\operatorname{CD}}
\CD_T:=\KL(p_0,p_\infty)-\KL(p_T,p_\infty)
$$
と定めた．ただし，$p_0$ はデータ $\{x_i\}_{i=1}^n$ の分布とする．

すなわち，データ点から初めて，MCMC を $T$ ステップ実行することに相当する行為を，$\CD_T$ の最小化として捉え直し，確率的勾配降下法によって学習する．

データ点を取り替えるごとに $p_0$ を取り替えるのではなく，$p_0$ を以前の MCMC の終わり値から定めた場合の CD の変形を **PCD (Persistent Contrastive Divergence)** [@Tieleman2008], [@Tieleman-Hinton2009] という．

更なる改良をすることで，GAN に匹敵する性能と，分布の峰を正確に再現できるという GAN にはない美点を獲得できるという [@Du-Mordatch2019]．

一方で，MCMC を途中で止めていることが，学習にどのような影響を与えるかは不明である [@Nijkaml+2019]．

### GAN による敵対的学習

尤度 $p_\theta$ の評価を迂回するため，[GAN](../Kernels/Deep3.qmd) にヒントを得た敵対的な学習も考えられている [@Kim-Bengio2016], [@Gao+2020]．



## 文献 {.appendix}

[`Awesome-EBM`](https://github.com/yataobian/awesome-ebm) レポジトリは，種々の EBM のリストを与えている．

[@Gao+2020]