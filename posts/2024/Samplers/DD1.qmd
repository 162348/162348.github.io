---
title: "拡散過程の時間反転"
subtitle: "雑音除去拡散モデルから確率的局所化まで"
author: "司馬博文"
date: 8/26/2024
date-modified: 8/28/2024
categories: [Process, Sampling]
image: Files/DSM.svg
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 拡散過程の時間反転を考えると，Hyvärinen スコアがドリフト項に現れる．デノイジングスコアマッチングの目的関数は，これを利用してデータ分布のスコアが推定されるように設計されている．Tweedie の式がこれを正当化するが，この式を用いたサンプリング手法には確率的局所化というものもある．
listing: 
    -   id: lst-listing
        type: grid
        sort: false
        contents:
            - "Diffusion.qmd"
            - "SB1.qmd"
            - "EBM.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
---

{{< include ../../../assets/_preamble.qmd >}}

## 命題

::: {.callout-tip title="[命題 @Haussmann-Pardoux1986]" icon="false"}

Brown 運動 $\{B_t\}\subset\L(\Om;\R^d)$ と可測関数 $b:[0,1]\times\R^d\to\R^d,\sigma:[0,1]\times\R^d\to M_d(\R)$ に関して，
$$
dX_t=b_t(X_t)\,dt+\sigma_t(X_t)\,dB_t,\qquad t\in[0,1],
$$
を Markov 過程とする．さらに次の３条件を仮定する：

* $b_t,\sigma_t$ は $\R^d$ 上局所 Lipschitz 連続で，線型増大条件を満たす：
    $$
    \abs{b_t(x)}+\abs{\sigma_t(x)}\le K(1+\abs{x}),\qquad x\in\R^d,K>0.
    $$
* $X_0$ は密度 $p_0$ をもち，ある $\lambda<0$ について次を満たす：
    $$
    p_0\in L^2((1+\abs{x}^2)^\lambda\,dx).
    $$
* $a:=\sigma\sigma^\top$ は一様に正定値である
    $$
    a_t(x)\ge\al I_d
    $$
    であるか，$\al^{ij}_{x_ix_j}\in L^\infty((0,1)\times\R^d)$ である．

このとき，時間反転 $\ov{X}_t:=X_{1-t}$ の分布は次の SDE の解である：
$$
d\ov{X}_t=\ov{b}_t(\ov{X}_t)\,dt+\ov{\sigma}_t(\ov{X}_t)\,d\ov{B}_t,\qquad t\in[0,1].
$$
ただし，$(B_t)$ も Brown 運動で，
$$
\ov{b}^i_t(x)=-b_{1-t}^i(x)+\sum_{j=1}^d\frac{\Paren{a^{ij}_{1-t}(x)p_{1-t}(x)}_{x_j}}{p_{1-t}(x)},
$$
$$
\ov{a}^{ij}_t(x)=a^{ij}_{1-t}(x),\qquad\ov{\sigma}^{ij}_t(x)=\sigma^{ij}_{1-t}(x).
$$

:::

::: {.callout-tip title="系" icon="false"}

前の命題の３条件を満たす，ドリフト係数 $\sigma$ が $x\in\R^d$ に依らない SDE
$$
dX_t=b_t(X_t)\,dt+\sigma_t\,dB_t,\qquad t\in[0,1],
$$
を考える．この $(X)_{t\in[0,1]}$ の時間反転は，$a_t:=\sigma_t\sigma^\top_t$ に関して
$$
d\ov{X}_t=\Paren{-b_{1-t}(\ov{X}_t)+a_{1-t}\nabla_x\log p_{1-t}(\ov{X}_t)}\,dt+\sigma_{1-t}\,d\ov{B}_t,\qquad\ov{X}_0=X_1,
$$
と分布同等になる．

:::

SGM [@Song-Ermon2019], [@Song+2021ICLR] は，
$$
b_t(x)=-x,\qquad\sigma_t=\sqrt{2},
$$ {#eq-OU}
と設定し，$(X_t)$ を OU 過程とした．これは $\rN_d(0,I_d)$ に全変動距離・Wasserstein 距離・$\chi^2$-距離で[指数収束する](../Process/Langevin.qmd)．

従って，この時間反転を $\rN_d(0,I_d)$ からスタートさせることで，データ分布 $p_0$ からのサンプリングが可能になる．

しかしこのアイデアを実行するためには，スコア $\nabla_x\log p_{1-t}(X_t)$ の項を何らかの方法で推定する方法が必要である．

## スコアマッチングへの応用

### はじめに

[Denoising Score Matching](../Samplers/EBM.qmd#sec-DSM) [@Vincent2011] を初めとして，[Generalized Flow Matching](../Samplers/NF3.qmd#sec-GFM) [@Isobe+2024] や Functional Flow Matching [@Kerrigan+2024] は，次のような目的関数を持っている：
$$
\L(\theta)=\E\SQuare{\ABs{s_\theta(\wt{X})-\frac{X-\wt{X}}{\sigma^2}}^2},\qquad X\sim p_0,\wt{X}\sim p_0*\rN(0,\sigma^2I_d).
$$ {#eq-DSM-loss}

これはデータ $X\sim p_0$ と，それに独立な Gauss ノイズを印加したもの $\wt{X}$ との差分を目標としてスコアネットワーク $s_\theta$ を学習している．

### デノイジング過程としての見方

データにノイズを印加する過程は，$b_t=0,\sigma_t=I_d$ とした SDE
$$
dX_t=dB_t,\qquad t\in[0,1],X_0\sim p_0,
$$
で $t=0$ から $t=\sigma^2$ まで輸送することにあたる：
$$
X_\sigma^2=X_0+(B_{\sigma^2}-B_0)\deq\wt{X}.
$$
この時間反転は
$$
d\ov{X}_t=\nabla_x\log p_{1-t}(\ov{X}_t)\,dt+d\ov{B}_t,\qquad\ov{X}_0=X_1,
$$
と分布同等になる．

この時間反転過程 $(\ov{X}_t)$ は $\ov{X}_{1-\sigma^2}=\wt{X}$ を $\ov{X}_1=X$ まで運ぶが，この際に $\sigma\ll1$ ならば次の関係を示唆する：
\begin{align*}
    X&\deq\wt{X}+\int^1_{1-\sigma^2}\nabla_x\log p_{1-t}(\ov{X}_t)\,dt+\ep\\
    &\approx\wt{X}+\sigma^2\nabla_x\log p_0(X)+\ep,\qquad\ep\sim\rN(0,\sigma^2I_d).
\end{align*}
$\sigma\to0$ の極限で次の等号が成り立つ：
$$
\lim_{\sigma\to0}\frac{X-\wt{X}}{\sigma^2}\deq\nabla_x\log p_0(X).
$$

### Tweedie の式 {#sec-Tweedie-formula}

実は同様の式は，$\sigma\to0$ の極限で漸近的にではなく正の $\sigma^2>0$ に関しても，次の意味で成り立つ：

::: {.callout-tip title="命題" icon="false"}

$X\sim p_0,\wt{X}\sim p_0*\rN(0,\sigma^2I_d)=:\wt{p}_0$ とする．このとき，次が成り立つ：
$$
\E[X|\wt{X}=\wt{x}]=\wt{x}+\sigma^2\nabla_{\wt{x}}\log \wt{p}_0(\wt{x}).
$$

:::
::: {.callout-note title="証明" icon="false" collapse="true"}

一般に，$\phi_\sigma$ を $\rN_d(0,\sigma^2I_d)$ の密度関数とすると，
$$
\E[X|\wt{X}=\wt{x}]=\int_{\R^d}xp_0(x)\phi_\sigma(\wt{x}-x)\,dx
$$
より，
$$
\E[X|\wt{X}=\wt{x}]-\wt{x}=\int_{\R^d}(x-\wt{x})p_0(x)\phi_\sigma(\wt{x}-x)\,dx.
$$

一方で，
\begin{align*}
    &\qquad\nabla_{\wt{x}}\log\paren{\int_{\R^d}p_0(x)\phi_\sigma(\wt{x}-x)\,dx}\\
    &=\frac{\int_{\R^d}p_0(x)\nabla_{\wt{x}}\phi_\sigma(\wt{x}-x)\,dx}{\int_{\R^d}p_0(x)\phi_\sigma(\wt{x}-x)\,dx}\\
    &=\int_{\R^d}p_0(x)\phi_\sigma(\wt{x}-x)\frac{x-\wt{x}}{\sigma^2}\,dx\\
    &=\frac{\E[X|\wt{X}=\wt{x}]-\wt{x}}{\sigma^2}.
\end{align*}

:::

すなわち，$\wt{X}$ から $X$ を不偏推定しようとすることで，スコア $\nabla_{\wt{x}}\log\wt{p}(\wt{x})$ を学習することができるのである．

ただし，学習されるスコアは，データ分布 $p_0$ のものではなく，ノイズ分布 $\wt{p}_0$ のものであることに注意．

これが，デノイジングスコアマッチングの目的関数 ([-@eq-DSM-loss]) の背後にある動機付けである．

## 確率的局所化

### OU 過程による SGM

OU 過程の例 ([-@eq-OU]) に戻ろう．OU 過程
$$
dX_t=-X_t\,dt+\sqrt{2}\,dB_t
$$
の時間反転は次と分布同等である：
$$
d\ov{X}_t=\Paren{\ov{X}_t+2\nabla_x\log p_{1-t}(\ov{X}_t)}\,dt+\sqrt{2}\,d\ov{B}_t,\qquad\ov{X}_0=X_1.
$$ {#eq-OU-reverse}

OU 過程 $(X_t)$ は
$$
X_t\deq e^{-t}X_0+\sqrt{1-e^{-2t}}\ep,\qquad X_0\sim p_0,\ep\sim\rN_d(0,I_d)
$$
という遷移半群を持っているため，$\L[X_t]=\L[e^{-t}X_0]*\rN_d(0,1-e^{-2t})$ であることから，Tweedie の式 [-@sec-Tweedie-formula] より
$$
\nabla_x\log p_t(x_t)=\frac{\E[e^{-t}X_0|X_t=x_t]-x_t}{1-e^{-2t}}
$$
を得る．従ってこのスコアを式 ([-@eq-OU-reverse]) に代入し，
$$
m_t(x_t):=\E[X_0|tX_0+\sqrt{t}\ep=x_t],\qquad X_0\sim p_0,\ep\sim\rN_d(0,I_d),
$$
とおき，
$$
\tau(t)=\frac{1}{e^{2t}-1}
$$
の変数変換を施すと OU 過程の時間反転 ([-@eq-OU-reverse]) は次のように書き直せる：
$$
d\ov{Y}_\tau=\Paren{-\frac{1+\tau}{\tau(1+\tau)}\ov{Y}_\tau+\frac{1}{\sqrt{\tau(1+\tau)}}m_\tau\paren{\sqrt{\tau(1+\tau)}\ov{Y}_\tau}}\,d\tau+\frac{1}{\sqrt{\tau(1+\tau)}}\,d\ov{B}_\tau.
$$ {#eq-OU-reverse-2}

これが [denoising diffusion](../Samplers/SB1.qmd) である．

### もう一つのサンプリング法

**確率的局所化** (stochastic localization) は初め，[@Eldan2013] が高次元空間内の等方的な凸体上での等周不等式を示すために構成した半マルチンゲールが基になっている．

確率的局所化においては，$p_0$ からのあるサンプル $x_0$ に対して，その **観測過程** $(Y_t)$ と呼ばれる $x_0$ のノイズ付きの観測を考える．ただし，$Y_t$ は時間が進むごとに $x_0$ に関する情報量が増えるとする．^[例えば，$x_*,Y_{t_2},Y_{t_1}$ が長さ３の Markov 連鎖をなす，などの意味で．]

例えば
$$
Y_t=tx_0+B_t,\qquad t\in\R_+,
$$
という場合である．$B_t$ というノイズは印加されているが，$x_0$ というメッセージの内容がどんどん大きくなるため，Signal-to-noise 比は増大していく．

この場合については，$p_0$ が有限な二次の積率を持つならば，
$$
dY_\tau=m_\tau(Y_\tau)\,d\tau+dB'_\tau
$$
という SDE の解と分布同等である [@Liptser-Shiryaev2001-Statistics]．

これは式 ([-@eq-OU-reverse-2]) で与えた OU 過程の時間反転 $(\ov{Y}_\tau)$ に関して，$Y_\tau=\sqrt{\tau(1+\tau)}\ov{Y}_\tau$ の関係を持つ．

### 確率的局所化

実は $(Y_t)$ は，
$$
\mu_t:=\L[X_0|Y_t]
$$
として定まる $\P(\R^d)$-値の確率過程 $\{\mu_t\}\subset\L(\Om;\cP(\R^d))$ について，次の性質を持つ：
$$
\mu_0=\L[X_0]=p_0\,d\ell_d,\qquad\mu_t\Rightarrow\delta_{x_0}\qquad t\to\infty,
$$

実は上述のサンプリング法は，このような $p_0\,d\ell_d$ から $\delta_{X_0}\;(X_0\sim p_0)$ への確率過程が与えられるごとに構成できる．

実際，最も簡単には，重心
$$
M_t:=\int_{\R^d}x\,\mu_t(dx)
$$
を計算すれば，$M_t$ は $t\to\infty$ の極限で $\L[X_0]$ に収束する．

これが **確率的局所化** である．

確率的局所化に基づいたサンプラーは [@Alaoui+2022] により [Sherrington-Kirkpatrick 模型](../Nature/StatisticalMechanics1.qmd#sec-SK-model) の Gibbs 分布からのサンプリングに適用され，[@Montanari-Wu2024] でさらにベイズ統計への応用のために拡張されている．

また，最良の雑音除去拡散モデルの収束証明は確率的局所化に基づいた証明によって与えられている [@Benton+2024]．

## 関連ページ {.unnumbered .unlisted}

::: {#lst-listing}
:::

## 文献紹介 {.appendix}

[@Anderson1982] では Fokker-Planck 方程式の解に対する条件の言葉で時間反転命題を与えている．また，時間反転も，元の Brown 運動 $B_t$ と独立な Brown 運動 $\ov{B}_t$ に関する SDE ではなく，その時間反転 $\wt{B}_t:=B_{1-t}$ に関する SDE で与えている．

[Aleandre Thiéry のブログ記事](https://alexxthiery.github.io/posts/reverse_and_tweedie/reverse_and_tweedie.html#ref-efron2011tweedie) や [鈴木大慈氏のスライド](https://drive.google.com/file/d/1ipWPVNBpFy5GlQqXtSbbhB0gAJUld-yd/view)，[Montanari の講義資料](https://metaphor.ethz.ch/x/2024/fs/401-4634-24L/) も参照．

Tweedie の式は [@Robbins1956] によって命名されている．[@Efron2011] では選択バイアスが存在する状況における経験ベイズ法に応用している．

確率的局所化については [@Montanari2023] を参考にした．