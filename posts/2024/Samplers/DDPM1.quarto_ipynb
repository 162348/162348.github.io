{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Neural Network 訓練の加速\"\n",
        "subtitle: \"PyTorch について調べたこと\"\n",
        "author: \"司馬博文\"\n",
        "date: 8/6/2024\n",
        "date-modified: 8/10/2024\n",
        "image: Files/fig-generation3-output-1.png\n",
        "categories: [Deep, Python]\n",
        "bibliography: \n",
        "    - ../../../assets/mathematics.bib\n",
        "    - ../../../assets/bib.bib\n",
        "    - ../../../assets/bib1.bib\n",
        "csl: ../../../assets/apalike.csl\n",
        "abstract-title: 概要\n",
        "abstract: 前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックを調べた．筆者のローカルマシンは M2 Mac mini であるため，CUDA がなく，皮層的な内容に終始している．Apple Silicon 上では，小さなモデルであっても MPS (Metal Performance Shaders) を用いることで５倍以上の高速化が可能であった．\n",
        "code-fold: false\n",
        "execute:\n",
        "    eval: false\n",
        "---\n",
        "\n",
        "::: {.hidden}\n",
        "\n",
        "::: {.content-visible when-format=\"html\"}\n",
        "\n",
        "A Blog Entry on Bayesian Computation by an Applied Mathematician\n",
        "\n",
        "$$\n",
        "\n",
        "\\renewcommand{\\P}{\\operatorname{P}}\\newcommand{\\E}{\\operatorname{E}}\n",
        "\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\F}{\\mathcal{F}}\n",
        "\\newcommand{\\abs}[1]{\\lvert#1\\rvert}\\newcommand{\\Abs}[1]{\\left|#1\\right|}\\newcommand{\\ABs}[1]{\\biggl|#1\\biggr|}\\newcommand{\\norm}[1]{\\|#1\\|}\\newcommand{\\Norm}[1]{\\left\\|#1\\right\\|}\\newcommand{\\NOrm}[1]{\\biggl\\|#1\\biggr\\|}\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\\newcommand{\\BRace}[1]{\\biggl\\{#1\\biggr\\}}\\newcommand{\\paren}[1]{\\left(#1\\right)}\\newcommand{\\Paren}[1]{\\biggr(#1\\biggl)}\\newcommand{\\brac}[1]{\\langle#1\\rangle}\\newcommand{\\Brac}[1]{\\left\\langle#1\\right\\rangle}\\newcommand{\\BRac}[1]{\\biggl\\langle#1\\biggr\\rangle}\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\\newcommand{\\Square}[1]{\\left[#1\\right]}\\newcommand{\\SQuare}[1]{\\biggl[#1\\biggr]}\\newcommand{\\rN}{\\mathrm{N}}\\newcommand{\\ov}[1]{\\overline{#1}}\\newcommand{\\un}[1]{\\underline{#1}}\\newcommand{\\wt}[1]{\\widetilde{#1}}\\newcommand{\\wh}[1]{\\widehat{#1}}\\newcommand{\\pp}[2]{\\frac{\\partial #1}{\\partial #2}}\\newcommand{\\ppp}[3]{\\frac{\\partial #1}{\\partial #2\\partial #3}}\\newcommand{\\dd}[2]{\\frac{d #1}{d #2}}\\newcommand{\\floor}[1]{\\lfloor#1\\rfloor}\\newcommand{\\Floor}[1]{\\left\\lfloor#1\\right\\rfloor}\\newcommand{\\ceil}[1]{\\lceil#1\\rceil}\\newcommand{\\ocinterval}[1]{(#1]}\\newcommand{\\cointerval}[1]{[#1)}\\newcommand{\\COinterval}[1]{\\left[#1\\right)}\\newcommand{\\iso}{\\overset{\\sim}{\\to}}\n",
        "\n",
        "\n",
        "\n",
        "\\newcommand{\\y}{\\b{y}}\\newcommand{\\mi}{\\,|\\,}\\newcommand{\\Mark}{\\mathrm{Mark}}\n",
        "\\newcommand{\\argmax}{\\operatorname*{argmax}}\\newcommand{\\argmin}{\\operatorname*{argmin}}\n",
        "\n",
        "\\newcommand{\\pr}{\\mathrm{pr}}\\newcommand{\\Conv}{\\operatorname{Conv}}\\newcommand{\\cU}{\\mathcal{U}}\n",
        "\\newcommand{\\Map}{\\mathrm{Map}}\\newcommand{\\dom}{\\mathrm{Dom}\\;}\\newcommand{\\cod}{\\mathrm{Cod}\\;}\\newcommand{\\supp}{\\mathrm{supp}\\;}\n",
        "\\newcommand{\\grad}{\\operatorname{grad}}\\newcommand{\\rot}{\\operatorname{rot}}\\renewcommand{\\div}{\\operatorname{div}}\\newcommand{\\tr}{\\operatorname{tr}}\\newcommand{\\Tr}{\\operatorname{Tr}}\\newcommand{\\KL}{\\operatorname{KL}}\\newcommand{\\JS}{\\operatorname{JS}}\\newcommand{\\ESS}{\\operatorname{ESS}}\\newcommand{\\MSE}{\\operatorname{MSE}}\\newcommand{\\erf}{\\operatorname{erf}}\\newcommand{\\arctanh}{\\operatorname{arctanh}}\\newcommand{\\pl}{\\operatorname{pl}}\\newcommand{\\minimize}{\\operatorname{minimize}}\\newcommand{\\subjectto}{\\operatorname{subject to}}\\newcommand{\\sinc}{\\operatorname{sinc}}\\newcommand{\\Ent}{\\operatorname{Ent}}\\newcommand{\\Polya}{\\operatorname{Polya}}\\newcommand{\\Exp}{\\operatorname{Exp}}\\newcommand{\\codim}{\\operatorname{codim}}\\newcommand{\\sgn}{\\operatorname{sgn}}\\newcommand{\\rank}{\\operatorname{rank}}\n",
        "\n",
        "\\newcommand{\\vctr}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\vctrr}[3]{\\begin{pmatrix}#1\\\\#2\\\\#3\\end{pmatrix}}\\newcommand{\\mtrx}[4]{\\begin{pmatrix}#1&#2\\\\#3&#4\\end{pmatrix}}\\newcommand{\\smtrx}[4]{\\paren{\\begin{smallmatrix}#1&#2\\\\#3&#4\\end{smallmatrix}}}\\newcommand{\\Ker}{\\mathrm{Ker}\\;}\\newcommand{\\Coker}{\\mathrm{Coker}\\;}\\newcommand{\\Coim}{\\mathrm{Coim}\\;}\\newcommand{\\lcm}{\\mathrm{lcm}}\\newcommand{\\GL}{\\mathrm{GL}}\\newcommand{\\SL}{\\mathrm{SL}}\\newcommand{\\alt}{\\mathrm{alt}}\n",
        "\n",
        "\\renewcommand{\\Re}{\\mathrm{Re}\\;}\\renewcommand{\\Im}{\\mathrm{Im}\\,}\\newcommand{\\Gal}{\\mathrm{Gal}}\\newcommand{\\PGL}{\\mathrm{PGL}}\\newcommand{\\PSL}{\\mathrm{PSL}}\\newcommand{\\Log}{\\mathrm{Log}\\,}\\newcommand{\\Res}{\\mathrm{Res}\\,}\\newcommand{\\on}{\\mathrm{on}\\;}\\newcommand{\\hatC}{\\widehat{\\C}}\\newcommand{\\hatR}{\\hat{\\R}}\\newcommand{\\PV}{\\mathrm{P.V.}}\\newcommand{\\diam}{\\mathrm{diam}}\\newcommand{\\Area}{\\mathrm{Area}}\\newcommand{\\Lap}{\\Laplace}\\newcommand{\\f}{\\mathbf{f}}\\newcommand{\\cR}{\\mathcal{R}}\\newcommand{\\const}{\\mathrm{const.}}\\newcommand{\\Om}{\\Omega}\\newcommand{\\Cinf}{C^\\infty}\\newcommand{\\ep}{\\epsilon}\\newcommand{\\dist}{\\mathrm{dist}}\\newcommand{\\opart}{\\o{\\partial}}\\newcommand{\\Length}{\\mathrm{Length}}\n",
        "\n",
        "\\newcommand{\\cA}{\\mathcal{A}}\\newcommand{\\cO}{\\mathcal{O}}\\newcommand{\\cW}{\\mathcal{W}}\\renewcommand{\\O}{\\mathcal{O}}\\renewcommand{\\S}{\\mathcal{S}}\\newcommand{\\U}{\\mathcal{U}}\\newcommand{\\V}{\\mathrm{V}}\\newcommand{\\N}{\\mathbb{N}}\\newcommand{\\bN}{\\mathbb{N}}\\newcommand{\\C}{\\mathrm{C}}\\newcommand{\\bC}{\\mathbb{C}}\\newcommand{\\Z}{\\mathcal{Z}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\bQ}{\\mathbb{Q}}\\newcommand{\\TV}{\\mathrm{TV}}\\newcommand{\\ORD}{\\mathrm{ORD}}\\newcommand{\\Card}{\\mathrm{Card}\\,}\\newcommand{\\Top}{\\mathrm{Top}}\\newcommand{\\Disc}{\\mathrm{Disc}}\\newcommand{\\Codisc}{\\mathrm{Codisc}}\\newcommand{\\CoDisc}{\\mathrm{CoDisc}}\\newcommand{\\Ult}{\\mathrm{Ult}}\\newcommand{\\ord}{\\mathrm{ord}}\\newcommand{\\bS}{\\mathbb{S}}\\newcommand{\\PConn}{\\mathrm{PConn}}\\newcommand{\\mult}{\\mathrm{mult}}\\newcommand{\\inv}{\\mathrm{inv}}\n",
        "\n",
        "\\newcommand{\\Der}{\\mathrm{Der}}\\newcommand{\\osub}{\\overset{\\mathrm{open}}{\\subset}}\\newcommand{\\osup}{\\overset{\\mathrm{open}}{\\supset}}\\newcommand{\\al}{\\alpha}\\newcommand{\\K}{\\mathbb{K}}\\newcommand{\\Sp}{\\mathrm{Sp}}\\newcommand{\\g}{\\mathfrak{g}}\\newcommand{\\h}{\\mathfrak{h}}\\newcommand{\\Imm}{\\mathrm{Imm}}\\newcommand{\\Imb}{\\mathrm{Imb}}\\newcommand{\\Gr}{\\mathrm{Gr}}\n",
        "\n",
        "\\newcommand{\\Ad}{\\mathrm{Ad}}\\newcommand{\\finsupp}{\\mathrm{fin\\;supp}}\\newcommand{\\SO}{\\mathrm{SO}}\\newcommand{\\SU}{\\mathrm{SU}}\\newcommand{\\acts}{\\curvearrowright}\\newcommand{\\mono}{\\hookrightarrow}\\newcommand{\\epi}{\\twoheadrightarrow}\\newcommand{\\Stab}{\\mathrm{Stab}}\\newcommand{\\nor}{\\mathrm{nor}}\\newcommand{\\T}{\\mathbb{T}}\\newcommand{\\Aff}{\\mathrm{Aff}}\\newcommand{\\rsup}{\\triangleright}\\newcommand{\\subgrp}{\\overset{\\mathrm{subgrp}}{\\subset}}\\newcommand{\\Ext}{\\mathrm{Ext}}\\newcommand{\\sbs}{\\subset}\\newcommand{\\sps}{\\supset}\\newcommand{\\In}{\\mathrm{in}\\;}\\newcommand{\\Tor}{\\mathrm{Tor}}\\newcommand{\\p}{\\b{p}}\\newcommand{\\q}{\\mathfrak{q}}\\newcommand{\\m}{\\mathfrak{m}}\\newcommand{\\cS}{\\mathcal{S}}\\newcommand{\\Frac}{\\mathrm{Frac}\\,}\\newcommand{\\Spec}{\\mathrm{Spec}\\,}\\newcommand{\\bA}{\\mathbb{A}}\\newcommand{\\Sym}{\\mathrm{Sym}}\\newcommand{\\Ann}{\\mathrm{Ann}}\\newcommand{\\Her}{\\mathrm{Her}}\\newcommand{\\Bil}{\\mathrm{Bil}}\\newcommand{\\Ses}{\\mathrm{Ses}}\\newcommand{\\FVS}{\\mathrm{FVS}}\n",
        "\n",
        "\\newcommand{\\Ho}{\\mathrm{Ho}}\\newcommand{\\CW}{\\mathrm{CW}}\\newcommand{\\lc}{\\mathrm{lc}}\\newcommand{\\cg}{\\mathrm{cg}}\\newcommand{\\Fib}{\\mathrm{Fib}}\\newcommand{\\Cyl}{\\mathrm{Cyl}}\\newcommand{\\Ch}{\\mathrm{Ch}}\n",
        "\\newcommand{\\rP}{\\mathrm{P}}\\newcommand{\\rE}{\\mathrm{E}}\\newcommand{\\e}{\\b{e}}\\renewcommand{\\k}{\\b{k}}\\newcommand{\\Christ}[2]{\\begin{Bmatrix}#1\\\\#2\\end{Bmatrix}}\\renewcommand{\\Vec}[1]{\\overrightarrow{\\mathrm{#1}}}\\newcommand{\\hen}[1]{\\mathrm{#1}}\\renewcommand{\\b}[1]{\\boldsymbol{#1}}\n",
        "\n",
        "\\newcommand{\\Inc}{\\mathrm{Inc}}\\newcommand{\\aInc}{\\mathrm{aInc}}\\newcommand{\\HS}{\\mathrm{HS}}\\newcommand{\\loc}{\\mathrm{loc}}\\newcommand{\\Lh}{\\mathrm{L.h.}}\\newcommand{\\Epi}{\\mathrm{Epi}}\\newcommand{\\slim}{\\mathrm{slim}}\\newcommand{\\Ban}{\\mathrm{Ban}}\\newcommand{\\Hilb}{\\mathrm{Hilb}}\\newcommand{\\Ex}{\\mathrm{Ex}}\\newcommand{\\Co}{\\mathrm{Co}}\\newcommand{\\sa}{\\mathrm{sa}}\\newcommand{\\nnorm}[1]{{\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert}}\\newcommand{\\dvol}{\\mathrm{dvol}}\\newcommand{\\Sconv}{\\mathrm{Sconv}}\\newcommand{\\I}{\\mathcal{I}}\\newcommand{\\nonunital}{\\mathrm{nu}}\\newcommand{\\cpt}{\\mathrm{cpt}}\\newcommand{\\lcpt}{\\mathrm{lcpt}}\\newcommand{\\com}{\\mathrm{com}}\\newcommand{\\Haus}{\\mathrm{Haus}}\\newcommand{\\proper}{\\mathrm{proper}}\\newcommand{\\infinity}{\\mathrm{inf}}\\newcommand{\\TVS}{\\mathrm{TVS}}\\newcommand{\\ess}{\\mathrm{ess}}\\newcommand{\\ext}{\\mathrm{ext}}\\newcommand{\\Index}{\\mathrm{Index}\\;}\\newcommand{\\SSR}{\\mathrm{SSR}}\\newcommand{\\vs}{\\mathrm{vs.}}\\newcommand{\\fM}{\\mathfrak{M}}\\newcommand{\\EDM}{\\mathrm{EDM}}\\newcommand{\\Tw}{\\mathrm{Tw}}\\newcommand{\\fC}{\\mathfrak{C}}\\newcommand{\\bn}{\\boldsymbol{n}}\\newcommand{\\br}{\\boldsymbol{r}}\\newcommand{\\Lam}{\\Lambda}\\newcommand{\\lam}{\\lambda}\\newcommand{\\one}{\\mathbf{1}}\\newcommand{\\dae}{\\text{-a.e.}}\\newcommand{\\das}{\\text{-a.s.}}\\newcommand{\\td}{\\text{-}}\\newcommand{\\RM}{\\mathrm{RM}}\\newcommand{\\BV}{\\mathrm{BV}}\\newcommand{\\normal}{\\mathrm{normal}}\\newcommand{\\lub}{\\mathrm{lub}\\;}\\newcommand{\\Graph}{\\mathrm{Graph}}\\newcommand{\\Ascent}{\\mathrm{Ascent}}\\newcommand{\\Descent}{\\mathrm{Descent}}\\newcommand{\\BIL}{\\mathrm{BIL}}\\newcommand{\\fL}{\\mathfrak{L}}\\newcommand{\\De}{\\Delta}\n",
        "\n",
        "\\newcommand{\\calA}{\\mathcal{A}}\\newcommand{\\calB}{\\mathcal{B}}\\newcommand{\\D}{\\mathcal{D}}\\newcommand{\\Y}{\\mathcal{Y}}\\newcommand{\\calC}{\\mathcal{C}}\\renewcommand{\\ae}{\\mathrm{a.e.}\\;}\\newcommand{\\cZ}{\\mathcal{Z}}\\newcommand{\\fF}{\\mathfrak{F}}\\newcommand{\\fI}{\\mathfrak{I}}\\newcommand{\\rV}{\\mathrm{V}}\\newcommand{\\cE}{\\mathcal{E}}\\newcommand{\\sMap}{\\sigma\\textrm{-}\\mathrm{Map}}\\newcommand{\\cC}{\\mathcal{C}}\\newcommand{\\comp}{\\complement}\\newcommand{\\J}{\\mathcal{J}}\\newcommand{\\sumN}[1]{\\sum_{#1\\in\\N}}\\newcommand{\\cupN}[1]{\\cup_{#1\\in\\N}}\\newcommand{\\capN}[1]{\\cap_{#1\\in\\N}}\\newcommand{\\Sum}[1]{\\sum_{#1=1}^\\infty}\\newcommand{\\sumn}{\\sum_{n=1}^\\infty}\\newcommand{\\summ}{\\sum_{m=1}^\\infty}\\newcommand{\\sumk}{\\sum_{k=1}^\\infty}\\newcommand{\\sumi}{\\sum_{i=1}^\\infty}\\newcommand{\\sumj}{\\sum_{j=1}^\\infty}\\newcommand{\\cupn}{\\cup_{n=1}^\\infty}\\newcommand{\\capn}{\\cap_{n=1}^\\infty}\\newcommand{\\cupk}{\\cup_{k=1}^\\infty}\\newcommand{\\cupi}{\\cup_{i=1}^\\infty}\\newcommand{\\cupj}{\\cup_{j=1}^\\infty}\\newcommand{\\limn}{\\lim_{n\\to\\infty}}\\renewcommand{\\L}{\\mathcal{L}}\\newcommand{\\cL}{\\mathcal{L}}\\newcommand{\\Cl}{\\mathrm{Cl}}\\newcommand{\\cN}{\\mathcal{N}}\\newcommand{\\Ae}{\\textrm{-a.e.}\\;}\\renewcommand{\\csub}{\\overset{\\textrm{closed}}{\\subset}}\\renewcommand{\\csup}{\\overset{\\textrm{closed}}{\\supset}}\\newcommand{\\wB}{\\wt{B}}\\newcommand{\\cG}{\\mathcal{G}}\\newcommand{\\Lip}{\\mathrm{Lip}}\\newcommand{\\AC}{\\mathrm{AC}}\\newcommand{\\Mol}{\\mathrm{Mol}}\n",
        "\n",
        "\\newcommand{\\Pe}{\\mathrm{Pe}}\\newcommand{\\wR}{\\wh{\\mathbb{\\R}}}\\newcommand*{\\Laplace}{\\mathop{}\\!\\mathbin\\bigtriangleup}\\newcommand*{\\DAlambert}{\\mathop{}\\!\\mathbin\\Box}\\newcommand{\\bT}{\\mathbb{T}}\\newcommand{\\dx}{\\dslash x}\\newcommand{\\dt}{\\dslash t}\\newcommand{\\ds}{\\dslash s}\n",
        "\n",
        "\\newcommand{\\round}{\\mathrm{round}}\\newcommand{\\cond}{\\mathrm{cond}}\\newcommand{\\diag}{\\mathrm{diag}}\n",
        "\\newcommand{\\Adj}{\\mathrm{Adj}}\\newcommand{\\Pf}{\\mathrm{Pf}}\\newcommand{\\Sg}{\\mathrm{Sg}}\n",
        "\n",
        "\n",
        "\\newcommand{\\aseq}{\\overset{\\text{a.s.}}{=}}\\newcommand{\\deq}{\\overset{\\text{d}}{=}}\\newcommand{\\cV}{\\mathcal{V}}\\newcommand{\\FM}{\\mathrm{FM}}\\newcommand{\\KR}{\\mathrm{KR}}\\newcommand{\\rba}{\\mathrm{rba}}\\newcommand{\\rca}{\\mathrm{rca}}\\newcommand{\\Prob}{\\mathrm{Prob}}\\newcommand{\\X}{\\mathcal{X}}\\newcommand{\\Meas}{\\mathrm{Meas}}\\newcommand{\\as}{\\;\\text{a.s.}}\\newcommand{\\io}{\\;\\mathrm{i.o.}}\\newcommand{\\fe}{\\;\\text{f.e.}}\\newcommand{\\bF}{\\mathbb{F}}\\newcommand{\\W}{\\mathcal{W}}\\newcommand{\\Pois}{\\mathrm{Pois}}\\newcommand{\\iid}{\\text{i.i.d.}}\\newcommand{\\wconv}{\\rightsquigarrow}\\newcommand{\\Var}{\\mathrm{Var}}\\newcommand{\\xrightarrown}{\\xrightarrow{n\\to\\infty}}\\newcommand{\\au}{\\mathrm{au}}\\newcommand{\\cT}{\\mathcal{T}}\\newcommand{\\wto}{\\overset{\\text{w}}{\\to}}\\newcommand{\\dto}{\\overset{\\text{d}}{\\to}}\\newcommand{\\sto}{\\overset{\\text{s}}{\\to}}\\newcommand{\\pto}{\\overset{\\text{p}}{\\to}}\\newcommand{\\mto}{\\overset{\\text{m}}{\\to}}\\newcommand{\\vto}{\\overset{v}{\\to}}\\newcommand{\\Cont}{\\mathrm{Cont}}\\newcommand{\\stably}{\\mathrm{stably}}\\newcommand{\\Np}{\\mathbb{N}^+}\\newcommand{\\oM}{\\overline{\\mathcal{M}}}\\newcommand{\\fP}{\\mathfrak{P}}\\newcommand{\\sign}{\\mathrm{sign}}\n",
        "\\newcommand{\\Borel}{\\mathrm{Borel}}\\newcommand{\\Mid}{\\,|\\,}\\newcommand{\\middleMid}{\\;\\middle|\\;}\\newcommand{\\CP}{\\mathrm{CP}}\\newcommand{\\bD}{\\mathbb{D}}\\newcommand{\\bL}{\\mathbb{L}}\\newcommand{\\fW}{\\mathfrak{W}}\\newcommand{\\DL}{\\mathcal{D}\\mathcal{L}}\\renewcommand{\\r}[1]{\\mathrm{#1}}\\newcommand{\\rC}{\\mathrm{C}}\\newcommand{\\qqquad}{\\qquad\\quad}\n",
        "\n",
        "\\newcommand{\\bit}{\\mathrm{bit}}\n",
        "\n",
        "\\newcommand{\\err}{\\mathrm{err}}\n",
        "\n",
        "\\newcommand{\\varparallel}{\\mathbin{\\!/\\mkern-5mu/\\!}}\\newcommand{\\Ri}{\\mathrm{Ri}}\\newcommand{\\Cone}{\\mathrm{Cone}}\\newcommand{\\Int}{\\mathrm{Int}}\n",
        "\n",
        "\\newcommand{\\pre}{\\mathrm{pre}}\\newcommand{\\om}{\\omega}\n",
        "\n",
        "\n",
        "\\newcommand{\\del}{\\partial}\n",
        "\\newcommand{\\LHS}{\\mathrm{LHS}}\\newcommand{\\RHS}{\\mathrm{RHS}}\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\\newcommand{\\interior}{\\mathrm{in}\\;}\\newcommand{\\SH}{\\mathrm{SH}}\\renewcommand{\\v}{\\boldsymbol{\\nu}}\\newcommand{\\n}{\\mathbf{n}}\\newcommand{\\ssub}{\\Subset}\\newcommand{\\curl}{\\mathrm{curl}}\n",
        "\n",
        "\\newcommand{\\Ei}{\\mathrm{Ei}}\\newcommand{\\sn}{\\mathrm{sn}}\\newcommand{\\wgamma}{\\widetilde{\\gamma}}\n",
        "\n",
        "\\newcommand{\\Ens}{\\mathrm{Ens}}\n",
        "\n",
        "\\newcommand{\\cl}{\\mathrm{cl}}\\newcommand{\\x}{\\boldsymbol{x}}\n",
        "\n",
        "\\newcommand{\\Do}{\\mathrm{Do}}\\newcommand{\\IV}{\\mathrm{IV}}\n",
        "\n",
        "\\newcommand{\\AIC}{\\mathrm{AIC}}\\newcommand{\\mrl}{\\mathrm{mrl}}\\newcommand{\\dotx}{\\dot{x}}\\newcommand{\\UMV}{\\mathrm{UMV}}\\newcommand{\\BLU}{\\mathrm{BLU}}\n",
        "\n",
        "\\newcommand{\\comb}[2]{\\begin{pmatrix}#1\\\\#2\\end{pmatrix}}\\newcommand{\\bP}{\\mathbb{P}}\\newcommand{\\compsub}{\\overset{\\textrm{cpt}}{\\subset}}\\newcommand{\\lip}{\\textrm{lip}}\\newcommand{\\BL}{\\mathrm{BL}}\\newcommand{\\G}{\\mathbb{G}}\\newcommand{\\NB}{\\mathrm{NB}}\\newcommand{\\oR}{\\ov{\\R}}\\newcommand{\\liminfn}{\\liminf_{n\\to\\infty}}\\newcommand{\\limsupn}{\\limsup_{n\\to\\infty}}\\newcommand{\\esssup}{\\mathrm{ess.sup}}\\newcommand{\\asto}{\\xrightarrow{\\as}}\\newcommand{\\Cov}{\\mathrm{Cov}}\\newcommand{\\cQ}{\\mathcal{Q}}\\newcommand{\\VC}{\\mathrm{VC}}\\newcommand{\\mb}{\\mathrm{mb}}\\newcommand{\\Avar}{\\mathrm{Avar}}\\newcommand{\\bB}{\\mathbb{B}}\\newcommand{\\bW}{\\mathbb{W}}\\newcommand{\\sd}{\\mathrm{sd}}\\newcommand{\\w}[1]{\\widehat{#1}}\\newcommand{\\bZ}{\\mathbb{Z}}\\newcommand{\\Bernoulli}{\\mathrm{Ber}}\\newcommand{\\Ber}{\\mathrm{Ber}}\\newcommand{\\Mult}{\\mathrm{Mult}}\\newcommand{\\BPois}{\\mathrm{BPois}}\\newcommand{\\fraks}{\\mathfrak{s}}\\newcommand{\\frakk}{\\mathfrak{k}}\\newcommand{\\IF}{\\mathrm{IF}}\\newcommand{\\bX}{\\boldsymbol{X}}\\newcommand{\\bx}{\\boldsymbol{x}}\\newcommand{\\indep}{\\perp\\!\\!\\!\\perp}\\newcommand{\\IG}{\\mathrm{IG}}\\newcommand{\\Levy}{\\mathrm{Levy}}\\newcommand{\\MP}{\\mathrm{MP}}\\newcommand{\\Hermite}{\\mathrm{Hermite}}\\newcommand{\\Skellam}{\\mathrm{Skellam}}\\newcommand{\\Dirichlet}{\\mathrm{Dirichlet}}\\renewcommand{\\Beta}{\\operatorname{Beta}}\\newcommand{\\bE}{\\mathbb{E}}\\newcommand{\\bG}{\\mathbb{G}}\\newcommand{\\MISE}{\\mathrm{MISE}}\\newcommand{\\logit}{\\mathtt{logit}}\\newcommand{\\expit}{\\mathtt{expit}}\\newcommand{\\cK}{\\mathcal{K}}\\newcommand{\\dl}{\\dot{l}}\\newcommand{\\dotp}{\\dot{p}}\\newcommand{\\wl}{\\wt{l}}\\newcommand{\\Gauss}{\\mathrm{Gauss}}\\newcommand{\\fA}{\\mathfrak{A}}\\newcommand{\\under}{\\mathrm{under}\\;}\\newcommand{\\whtheta}{\\wh{\\theta}}\\newcommand{\\Em}{\\mathrm{Em}}\\newcommand{\\ztheta}{{\\theta_0}}\n",
        "\\newcommand{\\rO}{\\mathrm{O}}\\newcommand{\\Bin}{\\mathrm{Bin}}\\newcommand{\\rW}{\\mathrm{W}}\\newcommand{\\rG}{\\mathrm{G}}\\newcommand{\\rB}{\\mathrm{B}}\\newcommand{\\rU}{\\mathrm{U}}\\newcommand{\\HG}{\\mathrm{HG}}\\newcommand{\\GAMMA}{\\mathrm{Gamma}}\\newcommand{\\Cauchy}{\\mathrm{Cauchy}}\\newcommand{\\rt}{\\mathrm{t}}\\newcommand{\\rF}{\\mathrm{F}}\n",
        "\\newcommand{\\FE}{\\mathrm{FE}}\\newcommand{\\bV}{\\boldsymbol{V}}\\newcommand{\\GLS}{\\mathrm{GLS}}\\newcommand{\\be}{\\boldsymbol{e}}\\newcommand{\\POOL}{\\mathrm{POOL}}\\newcommand{\\GMM}{\\mathrm{GMM}}\\newcommand{\\MM}{\\mathrm{MM}}\\newcommand{\\SSIV}{\\mathrm{SSIV}}\\newcommand{\\JIV}{\\mathrm{JIV}}\\newcommand{\\AR}{\\mathrm{AR}}\\newcommand{\\ILS}{\\mathrm{ILS}}\\newcommand{\\SLS}{\\mathrm{SLS}}\\newcommand{\\LIML}{\\mathrm{LIML}}\n",
        "\n",
        "\\newcommand{\\Rad}{\\mathrm{Rad}}\\newcommand{\\bY}{\\boldsymbol{Y}}\\newcommand{\\pone}{{(1)}}\\newcommand{\\ptwo}{{(2)}}\\newcommand{\\ps}[1]{{(#1)}}\\newcommand{\\fsub}{\\overset{\\text{finite}}{\\subset}}\n",
        "\n",
        "\n",
        "\\newcommand{\\varlim}{\\varprojlim}\\newcommand{\\Hom}{\\mathrm{Hom}}\\newcommand{\\Iso}{\\mathrm{Iso}}\\newcommand{\\Mor}{\\mathrm{Mor}}\\newcommand{\\Isom}{\\mathrm{Isom}}\\newcommand{\\Aut}{\\mathrm{Aut}}\\newcommand{\\End}{\\mathrm{End}}\\newcommand{\\op}{\\mathrm{op}}\\newcommand{\\ev}{\\mathrm{ev}}\\newcommand{\\Ob}{\\mathrm{Ob}}\\newcommand{\\Ar}{\\mathrm{Ar}}\\newcommand{\\Arr}{\\mathrm{Arr}}\\newcommand{\\Set}{\\mathrm{Set}}\\newcommand{\\Grp}{\\mathrm{Grp}}\\newcommand{\\Cat}{\\mathrm{Cat}}\\newcommand{\\Mon}{\\mathrm{Mon}}\\newcommand{\\Ring}{\\mathrm{Ring}}\\newcommand{\\CRing}{\\mathrm{CRing}}\\newcommand{\\Ab}{\\mathrm{Ab}}\\newcommand{\\Pos}{\\mathrm{Pos}}\\newcommand{\\Vect}{\\mathrm{Vect}}\\newcommand{\\FinVect}{\\mathrm{FinVect}}\\newcommand{\\FinSet}{\\mathrm{FinSet}}\\newcommand{\\FinMeas}{\\mathrm{FinMeas}}\\newcommand{\\OmegaAlg}{\\Omega\\text{-}\\mathrm{Alg}}\\newcommand{\\OmegaEAlg}{(\\Omega,E)\\text{-}\\mathrm{Alg}}\\newcommand{\\Fun}{\\mathrm{Fun}}\\newcommand{\\Func}{\\mathrm{Func}}\n",
        "\n",
        "\\newcommand{\\Stoch}{\\mathrm{Stoch}}\\newcommand{\\FinStoch}{\\mathrm{FinStoch}}\\newcommand{\\Copy}{\\mathrm{copy}}\\newcommand{\\Delete}{\\mathrm{delete}}\n",
        "\\newcommand{\\Bool}{\\mathrm{Bool}}\\newcommand{\\CABool}{\\mathrm{CABool}}\\newcommand{\\CompBoolAlg}{\\mathrm{CompBoolAlg}}\\newcommand{\\BoolAlg}{\\mathrm{BoolAlg}}\\newcommand{\\BoolRng}{\\mathrm{BoolRng}}\\newcommand{\\HeytAlg}{\\mathrm{HeytAlg}}\\newcommand{\\CompHeytAlg}{\\mathrm{CompHeytAlg}}\\newcommand{\\Lat}{\\mathrm{Lat}}\\newcommand{\\CompLat}{\\mathrm{CompLat}}\\newcommand{\\SemiLat}{\\mathrm{SemiLat}}\\newcommand{\\Stone}{\\mathrm{Stone}}\\newcommand{\\Mfd}{\\mathrm{Mfd}}\\newcommand{\\LieAlg}{\\mathrm{LieAlg}}\n",
        "\\newcommand{\\Op}{\\mathrm{Op}}\n",
        "\\newcommand{\\Sh}{\\mathrm{Sh}}\n",
        "\\newcommand{\\Diff}{\\mathrm{Diff}}\n",
        "\\newcommand{\\B}{\\mathcal{B}}\\newcommand{\\cB}{\\mathcal{B}}\\newcommand{\\Span}{\\mathrm{Span}}\\newcommand{\\Corr}{\\mathrm{Corr}}\\newcommand{\\Decat}{\\mathrm{Decat}}\\newcommand{\\Rep}{\\mathrm{Rep}}\\newcommand{\\Grpd}{\\mathrm{Grpd}}\\newcommand{\\sSet}{\\mathrm{sSet}}\\newcommand{\\Mod}{\\mathrm{Mod}}\\newcommand{\\SmoothMnf}{\\mathrm{SmoothMnf}}\\newcommand{\\coker}{\\mathrm{coker}}\\newcommand{\\Ord}{\\mathrm{Ord}}\\newcommand{\\eq}{\\mathrm{eq}}\\newcommand{\\coeq}{\\mathrm{coeq}}\\newcommand{\\act}{\\mathrm{act}}\n",
        "\n",
        "\\newcommand{\\apf}{\\mathrm{apf}}\\newcommand{\\opt}{\\mathrm{opt}}\\newcommand{\\IS}{\\mathrm{IS}}\\newcommand{\\IR}{\\mathrm{IR}}\\newcommand{\\iidsim}{\\overset{\\text{i.i.d.}}{\\sim}}\\newcommand{\\propt}{\\,\\propto\\,}\\newcommand{\\bM}{\\mathbb{M}}\\newcommand{\\cX}{\\mathcal{X}}\\newcommand{\\cY}{\\mathcal{Y}}\\newcommand{\\cP}{\\mathcal{P}}\\newcommand{\\ola}[1]{\\overleftarrow{#1}}\n",
        "\n",
        "\\renewcommand{\\iff}{\\;\\mathrm{iff}\\;}\n",
        "\\newcommand{\\False}{\\mathrm{False}}\\newcommand{\\True}{\\mathrm{True}}\n",
        "\\newcommand{\\otherwise}{\\mathrm{otherwise}}\n",
        "\\newcommand{\\suchthat}{\\;\\mathrm{s.t.}\\;}\n",
        "\n",
        "\\newcommand{\\cM}{\\mathcal{M}}\\newcommand{\\M}{\\mathbb{M}}\\newcommand{\\cF}{\\mathcal{F}}\\newcommand{\\cD}{\\mathcal{D}}\\newcommand{\\fX}{\\mathfrak{X}}\\newcommand{\\fY}{\\mathfrak{Y}}\\newcommand{\\fZ}{\\mathfrak{Z}}\\renewcommand{\\H}{\\mathcal{H}}\\newcommand{\\cH}{\\mathcal{H}}\\newcommand{\\fH}{\\mathfrak{H}}\\newcommand{\\bH}{\\mathbb{H}}\\newcommand{\\id}{\\mathrm{id}}\\newcommand{\\A}{\\mathcal{A}}\n",
        "\\newcommand{\\lmd}{\\lambda}\n",
        "\\newcommand{\\Lmd}{\\Lambda}\n",
        "\\newcommand{\\cI}{\\mathcal{I}}\n",
        "\n",
        "\\newcommand{\\Lrarrow}{\\;\\;\\Leftrightarrow\\;\\;}\n",
        "\\DeclareMathOperator{\\des}{des}\n",
        "\\DeclareMathOperator{\\nd}{nd}\n",
        "\\DeclareMathOperator{\\dsep}{d-sep}\n",
        "\\DeclareMathOperator{\\sep}{sep}\n",
        "\\newcommand{\\rLL}{\\mathrm{LL}}\\newcommand{\\HT}{\\mathrm{HT}}\\newcommand{\\PS}{\\mathrm{PS}}\\newcommand{\\rI}{\\mathrm{I}}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "![[前稿：拡散模型の実装――PyTorch によるハンズオン](DDPM.qmd)](../../../docs/posts/2024/Samplers/DDPM_files/figure-html/fig-encoding-output-1.png){width=\"70%\"}\n",
        "\n",
        "## 問題点と改善したいこと\n",
        "\n",
        "データセットの読み込みの段階において，次のコードがある：\n",
        "\n",
        "```python\n",
        "kwargs = {'num_workers': 5, 'pin_memory': True, 'prefetch_factor': 2}\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, **kwargs)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False,  **kwargs)\n",
        "```\n",
        "\n",
        "これはデータセット（今回は`MNIST`）を読み込み，iterable 型としての仕様を可能にするためのコードである．\n",
        "\n",
        "上述の通りのコードだとエポック 18 で `RuntimeError: Shared memory manager connection has timed out` を得たが，`num_workers=0` とするとエラーが発生しなかった．\n",
        "\n",
        "しかし，`num_workers=0` （デフォルト設定）とすると，デフォルトの単一プロセス処理が実行されるため，並列による高速化の恩恵を受けられない．その結果，１エポック 12 分以上なので，40 時間以上をかける必要が出てきた（寝てる間もディスプレイをオフにするだけでスリープさせず，回し続ける）．\n",
        "\n",
        "::: {.callout-important appearance=\"simple\" icon=\"false\" title=\"今回の目標\"}\n",
        "\n",
        "うまく並列処理をするようなコードに書き直すことで，ローカル環境でも１日以内で実行できるようにしたい．\n",
        "\n",
        ":::\n",
        "\n",
        "## `DataLoader` の引数について\n",
        "\n",
        "[`DataLoader` メソッドのドキュメント](https://pytorch.org/docs/stable/data.html) を参照すると，\n",
        "\n",
        "### `num_workers`\n",
        "\n",
        "は正整数に設定されると，その数だけ並列に動く 'worker' が起動され，マルチプロセス処理が実行される．\n",
        "\n",
        "しかし，子プロセスも同等のメモリを占めるため，値が大きすぎるとランタイムエラーが発生する（[issue #13246](https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662) 参照）．\n",
        "\n",
        "さらに，この際の並列処理は Python の `multiprocessing` パッケージによるもので，Windows と MacOS では（Unix 系のような `fork()` ではなく） `spawn()` が呼ばれる．これは別のインタープリターを開始するため，コードの大部分を `if __name__ == \"__main__\":` で囲まない限り，同じコードを何回も実行することとなり，ランタイムエラーが出現することとなる．\n",
        "\n",
        "### `pin_memeory`\n",
        "\n",
        "しかし，CUDA 上のテンソルオブジェクトを並列処理で共有することは非推奨であり，その際は自動メモリ固定 (automatic memory pinning) を行う必要がある．\n",
        "\n",
        "pinned memory とは page-locked メモリとも呼ばれ，通常の pageable メモリより転送速度が速いという．\n",
        "\n",
        "さて，paging とはなんだろうか？（一旦後回し）\n",
        "\n",
        "### `prefetch_factor`\n",
        "\n",
        "は各 `worker` が取ってきてストックしておくバッチの数である．\n",
        "\n",
        "すなわち，`num_workers * prefetch_factor` だけデータをメモリに読み込んでおくことになる．\n",
        "\n",
        "## 高速化法\n",
        "\n",
        "### Google Colab の利用\n",
        "\n",
        "結局この方法でトレーニングをし，[前稿](DDPM.qmd) を完成させたのであった．\n",
        "\n",
        "![A100（8/6/2024 時点）](Files/A100.png)\n",
        "\n",
        "A100 が税込 1,494,000 円であったが，これを利用すると１エポック 22 秒で実行できた．\n",
        "\n",
        "### `torch.nn.DataParallel` の使用\n",
        "\n",
        "自分のローカルマシンは CUDA がないため利用できないが，ある場合は `PyTorch` のモジュールで並列処理が可能である．^[[PyTorchでGPUを並列で使えるようにするtorch.nn.DataParallelのメモ](https://qiita.com/m__k/items/87b3b1da15f35321ecf5) などを参照した．]\n",
        "\n",
        "## `mps` で本当に高速になっているのか？\n",
        "\n",
        "アップルは [Metal](https://developer.apple.com/jp/metal/) という計算 API を提供しており，これが Apple Silicon で利用できる．\n"
      ],
      "id": "75e0573b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "DEVICE = torch.device(\"mps\")\n",
        "train_batch_size = 128\n",
        "epochs = 1"
      ],
      "id": "86998176",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "とし，１エポックにかかる時間を比較する．その他の設定は前節と同様．\n"
      ],
      "id": "d7411a0b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "\n",
        "import math\n",
        "\n",
        "dataset_path = '~/hirofumi/datasets'\n",
        "\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True}\n",
        "\n",
        "dataset = 'MNIST'\n",
        "img_size = (32, 32, 3)   if dataset == \"CIFAR10\" else (28, 28, 1) # (width, height, channels)\n",
        "\n",
        "timestep_embedding_dim = 256\n",
        "n_layers = 8\n",
        "hidden_dim = 256\n",
        "n_timesteps = 1000\n",
        "beta_minmax=[1e-4, 2e-2]\n",
        "\n",
        "inference_batch_size = 64\n",
        "lr = 5e-5\n",
        "\n",
        "seed = 2024\n",
        "\n",
        "hidden_dims = [hidden_dim for _ in range(n_layers)]\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    multiprocessing.set_start_method('spawn')\n",
        "\n",
        "    if dataset == 'CIFAR10':\n",
        "        train_dataset = CIFAR10(dataset_path, transform=transform, train=True, download=True)\n",
        "        test_dataset  = CIFAR10(dataset_path, transform=transform, train=False, download=True)\n",
        "    else:\n",
        "        train_dataset = MNIST(dataset_path, transform=transform, train=True, download=True)\n",
        "        test_dataset  = MNIST(dataset_path, transform=transform, train=False, download=True)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, **kwargs)\n",
        "    test_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False,  **kwargs)\n",
        "    \n",
        "    class SinusoidalPosEmb(nn.Module):\n",
        "        def __init__(self, dim):\n",
        "            super().__init__()\n",
        "            self.dim = dim\n",
        "\n",
        "        def forward(self, x):\n",
        "            device = x.device\n",
        "            half_dim = self.dim // 2\n",
        "            emb = math.log(10000) / (half_dim - 1)\n",
        "            emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "            emb = x[:, None] * emb[None, :]\n",
        "            emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "            return emb\n",
        "\n",
        "    class ConvBlock(nn.Conv2d):\n",
        "        \"\"\"\n",
        "            Conv2D Block\n",
        "                Args:\n",
        "                    x: (N, C_in, H, W)\n",
        "                Returns:\n",
        "                    y: (N, C_out, H, W)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, in_channels, out_channels, kernel_size, activation_fn=None, drop_rate=0.,\n",
        "                        stride=1, padding='same', dilation=1, groups=1, bias=True, gn=False, gn_groups=8):\n",
        "            \n",
        "            if padding == 'same':\n",
        "                padding = kernel_size // 2 * dilation\n",
        "\n",
        "            super(ConvBlock, self).__init__(in_channels, out_channels, kernel_size,\n",
        "                                                stride=stride, padding=padding, dilation=dilation,\n",
        "                                                groups=groups, bias=bias)\n",
        "\n",
        "            self.activation_fn = nn.SiLU() if activation_fn else None\n",
        "            self.group_norm = nn.GroupNorm(gn_groups, out_channels) if gn else None\n",
        "            \n",
        "        def forward(self, x, time_embedding=None, residual=False):\n",
        "            \n",
        "            if residual:\n",
        "                # in the paper, diffusion timestep embedding was only applied to residual blocks of U-Net\n",
        "                x = x + time_embedding\n",
        "                y = x\n",
        "                x = super(ConvBlock, self).forward(x)\n",
        "                y = y + x\n",
        "            else:\n",
        "                y = super(ConvBlock, self).forward(x)\n",
        "            y = self.group_norm(y) if self.group_norm is not None else y\n",
        "            y = self.activation_fn(y) if self.activation_fn is not None else y\n",
        "            \n",
        "            return y\n",
        "        \n",
        "    class Denoiser(nn.Module):\n",
        "        \n",
        "        def __init__(self, image_resolution, hidden_dims=[256, 256], diffusion_time_embedding_dim = 256, n_times=1000):\n",
        "            super(Denoiser, self).__init__()\n",
        "            \n",
        "            _, _, img_C = image_resolution\n",
        "            \n",
        "            self.time_embedding = SinusoidalPosEmb(diffusion_time_embedding_dim)\n",
        "            \n",
        "            self.in_project = ConvBlock(img_C, hidden_dims[0], kernel_size=7)\n",
        "            \n",
        "            self.time_project = nn.Sequential(\n",
        "                                    ConvBlock(diffusion_time_embedding_dim, hidden_dims[0], kernel_size=1, activation_fn=True),\n",
        "                                    ConvBlock(hidden_dims[0], hidden_dims[0], kernel_size=1))\n",
        "            \n",
        "            self.convs = nn.ModuleList([ConvBlock(in_channels=hidden_dims[0], out_channels=hidden_dims[0], kernel_size=3)])\n",
        "            \n",
        "            for idx in range(1, len(hidden_dims)):\n",
        "                self.convs.append(ConvBlock(hidden_dims[idx-1], hidden_dims[idx], kernel_size=3, dilation=3**((idx-1)//2),\n",
        "                                                        activation_fn=True, gn=True, gn_groups=8))                                \n",
        "                                \n",
        "            self.out_project = ConvBlock(hidden_dims[-1], out_channels=img_C, kernel_size=3)\n",
        "            \n",
        "            \n",
        "        def forward(self, perturbed_x, diffusion_timestep):\n",
        "            y = perturbed_x\n",
        "            \n",
        "            diffusion_embedding = self.time_embedding(diffusion_timestep)\n",
        "            diffusion_embedding = self.time_project(diffusion_embedding.unsqueeze(-1).unsqueeze(-2))\n",
        "            \n",
        "            y = self.in_project(y)\n",
        "            \n",
        "            for i in range(len(self.convs)):\n",
        "                y = self.convs[i](y, diffusion_embedding, residual = True)\n",
        "                \n",
        "            y = self.out_project(y)\n",
        "                \n",
        "            return y\n",
        "        \n",
        "    model = Denoiser(image_resolution=img_size,\n",
        "                    hidden_dims=hidden_dims, \n",
        "                    diffusion_time_embedding_dim=timestep_embedding_dim, \n",
        "                    n_times=n_timesteps).to(DEVICE)\n",
        "\n",
        "    class Diffusion(nn.Module):\n",
        "        def __init__(self, model, image_resolution=[32, 32, 3], n_times=1000, beta_minmax=[1e-4, 2e-2], device='cuda'):\n",
        "        \n",
        "            super(Diffusion, self).__init__()\n",
        "        \n",
        "            self.n_times = n_times\n",
        "            self.img_H, self.img_W, self.img_C = image_resolution\n",
        "\n",
        "            self.model = model\n",
        "            \n",
        "            # define linear variance schedule(betas)\n",
        "            beta_1, beta_T = beta_minmax\n",
        "            betas = torch.linspace(start=beta_1, end=beta_T, steps=n_times).to(device) # follows DDPM paper\n",
        "            self.sqrt_betas = torch.sqrt(betas)\n",
        "                                        \n",
        "            # define alpha for forward diffusion kernel\n",
        "            self.alphas = 1 - betas\n",
        "            self.sqrt_alphas = torch.sqrt(self.alphas)\n",
        "            alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
        "            self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n",
        "            self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n",
        "            \n",
        "            self.device = device\n",
        "        \n",
        "        def extract(self, a, t, x_shape):\n",
        "            \"\"\"\n",
        "                from lucidrains' implementation\n",
        "                    https://github.com/lucidrains/denoising-diffusion-pytorch/blob/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L376\n",
        "            \"\"\"\n",
        "            b, *_ = t.shape\n",
        "            out = a.gather(-1, t)\n",
        "            return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "        \n",
        "        def scale_to_minus_one_to_one(self, x):\n",
        "            # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n",
        "            return x * 2 - 1\n",
        "        \n",
        "        def reverse_scale_to_zero_to_one(self, x):\n",
        "            return (x + 1) * 0.5\n",
        "        \n",
        "        def make_noisy(self, x_zeros, t): \n",
        "            # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n",
        "            epsilon = torch.randn_like(x_zeros).to(self.device)\n",
        "            \n",
        "            sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars, t, x_zeros.shape)\n",
        "            sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, t, x_zeros.shape)\n",
        "            \n",
        "            # Let's make noisy sample!: i.e., Forward process with fixed variance schedule\n",
        "            #      i.e., sqrt(alpha_bar_t) * x_zero + sqrt(1-alpha_bar_t) * epsilon\n",
        "            noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n",
        "        \n",
        "            return noisy_sample.detach(), epsilon\n",
        "        \n",
        "        \n",
        "        def forward(self, x_zeros):\n",
        "            x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n",
        "            \n",
        "            B, _, _, _ = x_zeros.shape\n",
        "            \n",
        "            # (1) randomly choose diffusion time-step\n",
        "            t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(self.device)\n",
        "            \n",
        "            # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n",
        "            perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n",
        "            \n",
        "            # (3) predict epsilon(noise) given perturbed data at diffusion-timestep t.\n",
        "            pred_epsilon = self.model(perturbed_images, t)\n",
        "            \n",
        "            return perturbed_images, epsilon, pred_epsilon\n",
        "        \n",
        "        \n",
        "        def denoise_at_t(self, x_t, timestep, t):\n",
        "            B, _, _, _ = x_t.shape\n",
        "            if t > 1:\n",
        "                z = torch.randn_like(x_t).to(self.device)\n",
        "            else:\n",
        "                z = torch.zeros_like(x_t).to(self.device)\n",
        "            \n",
        "            # at inference, we use predicted noise(epsilon) to restore perturbed data sample.\n",
        "            epsilon_pred = self.model(x_t, timestep)\n",
        "            \n",
        "            alpha = self.extract(self.alphas, timestep, x_t.shape)\n",
        "            sqrt_alpha = self.extract(self.sqrt_alphas, timestep, x_t.shape)\n",
        "            sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars, timestep, x_t.shape)\n",
        "            sqrt_beta = self.extract(self.sqrt_betas, timestep, x_t.shape)\n",
        "            \n",
        "            # denoise at time t, utilizing predicted noise\n",
        "            x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n",
        "            \n",
        "            return x_t_minus_1.clamp(-1., 1)\n",
        "                    \n",
        "        def sample(self, N):\n",
        "            # start from random noise vector, x_0 (for simplicity, x_T declared as x_t instead of x_T)\n",
        "            x_t = torch.randn((N, self.img_C, self.img_H, self.img_W)).to(self.device)\n",
        "            \n",
        "            # autoregressively denoise from x_T to x_0\n",
        "            #     i.e., generate image from noise, x_T\n",
        "            for t in range(self.n_times-1, -1, -1):\n",
        "                timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(self.device)\n",
        "                x_t = self.denoise_at_t(x_t, timestep, t)\n",
        "            \n",
        "            # denormalize x_0 into 0 ~ 1 ranged values.\n",
        "            x_0 = self.reverse_scale_to_zero_to_one(x_t)\n",
        "            \n",
        "            return x_0\n",
        "        \n",
        "        \n",
        "    diffusion = Diffusion(model, image_resolution=img_size, n_times=n_timesteps, \n",
        "                        beta_minmax=beta_minmax, device=DEVICE).to(DEVICE)\n",
        "\n",
        "    optimizer = Adam(diffusion.parameters(), lr=lr)\n",
        "    denoising_loss = nn.MSELoss()"
      ],
      "id": "b0ea7bd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Start training DDPMs...\")\n",
        "model.train()\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    noise_prediction_loss = 0\n",
        "    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "        \n",
        "        noisy_input, epsilon, pred_epsilon = diffusion(x)\n",
        "        loss = denoising_loss(pred_epsilon, epsilon)\n",
        "        \n",
        "        noise_prediction_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n",
        "    \n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"Finish!! Total time: \", total_time)"
      ],
      "id": "4be2b5cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "12:58 であった．一方で，CPU でも訓練してみる．\n"
      ],
      "id": "46499ff4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "model = Denoiser(image_resolution=img_size,\n",
        "                    hidden_dims=hidden_dims, \n",
        "                    diffusion_time_embedding_dim=timestep_embedding_dim, \n",
        "                    n_times=n_timesteps).to(DEVICE)\n",
        "diffusion = Diffusion(model, image_resolution=img_size, n_times=n_timesteps, beta_minmax=beta_minmax, device=DEVICE).to(DEVICE)\n",
        "\n",
        "print(\"Start training DDPMs...\")\n",
        "model.train()\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    noise_prediction_loss = 0\n",
        "    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.to(DEVICE)\n",
        "        \n",
        "        noisy_input, epsilon, pred_epsilon = diffusion(x)\n",
        "        loss = denoising_loss(pred_epsilon, epsilon)\n",
        "        \n",
        "        noise_prediction_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tDenoising Loss: \", noise_prediction_loss / batch_idx)\n",
        "    \n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"Finish!! Total time: \", total_time)"
      ],
      "id": "480e8ad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "１時間越え！\n",
        "\n",
        "## 終わりに {.appendix}\n",
        "\n",
        "あまりに時間がかかるので，本記事は `eval: false` としておく．"
      ],
      "id": "60244895"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/hirofumi48/Library/Jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}