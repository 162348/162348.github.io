---
title: "ニューラル常微分方程式"
subtitle: "シミュレーションなしの拡散モデルとしての連続正規化流"
author: "司馬博文"
date: 2/14/2024
date-modified: 8/28/2024
categories: [Deep, Sampling, P(X)]
image: Files/linear/output.gif
bibliography: 
    - ../../../assets/2023.bib
    - ../../../assets/2024.bib
    - ../../../assets/2025.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: Gauss 分布からデータ分布までの変換を，可逆なニューラルネットワークでモデリングする正規化流は，ODE に基づいて設計することもできる．この方法は Neural ODE や連続な正規化流 (CNF) ともいう．しかし，連続なフローを学習するのに，MLE では大変なコストがかかる．実は２つの分布を繋ぐ経路を学習する問題は尤度とは何の関係もなく，Flow Matching により直接的かつ効率的に学習できる．現在の最先端の画像・動画生成モデルは，この Flow Matching の技術に拠っている．
listing: 
    -   id: flow-listing
        type: grid
        sort: false
        contents:
            - "NF.qmd"
            - "NF2.qmd"
            - "NF4.qmd"
            - "Diffusion.qmd"
            - "NF3.qmd"
            - "EBM1.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
---

{{< include ../../../assets/_preamble.qmd >}}

### 関連ページ {.unnumbered .unlisted}

::: {#flow-listing}
:::

## ニューラル常微分方程式 (NODE) {#sec-CNF}

### ベクトル場のモデリング

ベクトル場 $F:\R^d\times[0,T]\to\R^d$ に関して ODE
$$
\dd{x_t}{t}=F(x_t,t)
$$ {#eq-ODE}
を満たす曲線 $(x_t)$ を [**積分曲線**](https://ja.wikipedia.org/wiki/積分曲線) という．

CNF (Continuous Normalizing Flow) では，$(x_t)$ はデータ分布とノイズ分布を結ぶダイナミクスとする．すなわち，フロー $(f_t)$ によりノイズからのデータの生成を目指す．

この積分曲線をモデリングするために，ベクトル場 $F$ をニューラルネットによってモデリングするが，CNF では離散化誤差を入れずに，連続なままモデリングする方法を考える．

<!-- これは拡散過程が，$\{X_t\}$ をモデリングするために，その分布の軌道 $\{\L[X_t]\}$ をモデリングするのと対照的である． -->

### Neural ODE

$F$ が得られたならば，Euler の方法により積分曲線 $(x_t)$ を数値計算できる：
$$
x_{t+\ep}=x_t+\ep F(x_t,t),\qquad\ep>0.
$$

この式の形から，$F$ が定める ODE を $\ep F(x_t,t)$ が定める $T/\ep$ 層の残差ネットワークによってモデリングすることもあり得たが，それではタイムステップ $\ep>0$ を自由に設定することができない．

連続時間アプローチではこの出力 $x_T$ を得る手続きは，完全にネットワーク外の ODE ソルバーに任せてしまう．一方で，$\ep>0$ を自由に取れるように，連続なままダイナミクスをモデリングする．これが **NODE (Neural ODE)** [@Chen+2018] である．

従って NODE ではその強みを活かし，$\ep>0$ を必ずしも等間隔ではなく，適応的な設定が追究される．

### 訓練

$F(x_t,t)$ を何度も使う NODE のスキームは，$x_0,x_T$ のみに依存した損失関数に関する誤差逆伝播法に向いていない．

[@Chen+2018] では，最適制御の分野で知られていた [@Pontryagin+1962] の **随伴感度法** (adjoint sensitivity method) を用いた誤差逆伝播法の連続時間への拡張を提案している．

時刻 $t$ での損失 $L_t(x_t)$ はパス $(x_t)$ の全体に依存する汎函数であるとする：
$$
L_t(x_{t}):=L\paren{x_0+\int^t_0F_\theta(x_t,t)\,dt}.
$$

$\dd{L_t}{\theta}$ を計算するためには，まずは次の **随伴**（状態） を考える：
$$
a(t):=\dd{L_t(x_t)}{x_t}.
$$

出力 $x_T$ が得られているとするならば，この随伴は [@Pontryagin+1962] の定理から次の ODE を後ろ向きに解けば良いため，実際に $x_t,L_t$ を計算して微分する必要はない：
$$
\dd{a(t)}{t}=-a(t)^\top\pp{F_\theta(x_t,t)}{x_t}.
$$ {#eq-ODE-for-adjoint}

この ODE にも $x_t$ の項が表れているが，ODE ([-@eq-ODE]) と同時に解けば良い．こうして $a(t)$ を得たのちは，

$$
\dd{L_t}{\theta}=-\int^t_0a(s)^\top\pp{F_\theta(x_s,s)}{\theta}\,ds
$$ {#eq-ODE-for-gradient}
によって最終的な勾配を得る．

::: {.callout-tip appearance="simple" icon="false" title="勾配の計算法"}

1. 誤差逆伝播により $D_xF_\theta,D_\theta F_\theta$ を得る．
2. ODE ([-@eq-ODE]) と ([-@eq-ODE-for-adjoint]) を解いて随伴 $a(t)$ を得る．
3. 勾配の計算 ([-@eq-ODE-for-gradient]) により勾配 $\dd{L_t}{\theta}$ を得る．

:::

実際には，([-@eq-ODE]), ([-@eq-ODE-for-adjoint]), ([-@eq-ODE-for-gradient]) は同時に1つの ODE ソルバーへの関数呼び出しで解くことができる．

### Jacobian の計算

NODE を連続な正則化流として用いるためには，損失 $L$ に尤度 $p_T$ を登場させる必要がある：
$$
\log p_t(x_t)=\log p(x_0)-\log\abs{\det J_{f_t}(x_t)}.
$$

そして尤度の評価のためにはフロー $(f_t)$ の Jacobian $J_{f_t}(x_t)$ が必要である．

[残差ネットワークによる正規化流](NF.qmd#sec-residual-flow) においては，[Hutchinson の跡推定量](../Probability/Trace.qmd) を用いたり，残差接続の関数形を単純にして Jacobian を解析的に計算可能にしたりという方法で，Jacobian の計算 $O(d^3)$ を効率化していた．

NODE では，Jacobian $J_{f_t}(x_t)$ は
$$
\dd{\log p_t(x_t)}{t}=-\dd{\log\abs{\det J_{f_t}(x_t)}}{t}=-\Tr\Paren{J_{F_t}(x_t)}
$$
を利用することで，$J_{F_t}$ の跡から得ることができる [@Chen+2018 定理1]：
$$
\log p_t(x_t)=\log p(x_0)-\int^t_0\Tr\Paren{J_{F_s}(x_s)}\,ds
$$

$\det J_{f_t}(x_t)$ の行列評価が $O(d^3)$ であるところを，$\Tr(J_{F_t}(x_t))$ の計算は $O(d^2)$ で済む．

こうして，勾配 $D_\theta L_t$ と Jacobian $J_{f_t}$ の計算が，いずれも $F_\theta$ の微分係数が定める ODE の数値解を求めることに帰着される．

### Hutchinson の跡推定量による更なる軽量化

FFJORD (Free-Form Jacobian of Reversible Dynamics) [@Grathwohl+2019] では，$\Tr(J_{F_t}(x_t))$ の計算に [Hutchinson の跡推定量](../Probability/Trace.qmd) を用いる：
$$
\log p_t(x_t)=\log p(x_0)-\E\Square{\int^t_0\ep^\top J_{F_s}(x_s)\ep\,ds}.
$$

これにより最終的に $O(d)$ の計算量が達成される．

![[@Grathwohl+2019 p.5]](Files/FFJORD.png)

## フローマッチング (FM)

### はじめに

拡散模型をスコアマッチングと見ることでさらに効率的な訓練が可能になったように，NODE を **フローマッチング** (FM: Flow Matching) [@Lipman+2023] と見ることでよりスケーラブルな代替訓練方法が与えられる．

FM ではベクトル場 $F_\theta(x_t,t)$ を直接目標 $F(x_t,t)$ に向けて回帰する．この目標 $F(x_t,t)$ はノイズ分布とデータ分布を結ぶ輸送を定めるように決定される．

直接的な目標 $F(x_t,t)$ を特定することで，随伴感度法と ODE ソルバーを通じた連続な誤差逆伝播よりも直接的な訓練目標 ([-@eq-CFM-objective]) が得られ，効率的でスケーラブルな訓練手法が得られる．

[拡散過程](Diffusion.qmd) も，与えられた SDE と等価な輸送を行う ODE [@Song+2021ICLR], [@Maoutsa+2020] を通じてベクトル場のモデリングに議論を帰着できるから，FM により Neural ODE を効率的に訓練できたら，拡散模型のサンプリングの遅さの問題も解決できる．しかし，フローマッチングの美点はそれにとどまらない．

始点と終点がノイズ分布とデータ分布である限り，輸送はそもそも拡散過程に基づいたものである必要はない．フローマッチングでは，$F(x_t,t)$ をそのように選ぶことで，最適輸送経路に沿った輸送を行うベクトル場を直接学習することを考えることもできる [@Lipman+2023]．

当時，[Denoising Score Matching](EBM.qmd#sec-DSM) が使える拡散模型と違い，CNF にはスケーラブルな訓練手法が存在しなかった．この拡散模型の加速の問題を，CNF の訓練の加速の問題と同時に解いたのが FM [@Lipman+2023] である．

その発想の鍵は，$\cP(E)$ 上の経路のみに注目することで，SDE と ODE を「輸送 $(p_t)$ を定めるフロー $(\phi_t)$ を構成する道具」として相対化することであった．同様のアイデアは，同時期に確率的補間 [@Albergo-Vanden-Eijnden2023] と rectified flow [@Liu+2023-Flow] と２つ提出されている．

### フローマッチング (FM) {#sec-FM}

Flow Matching [@Lipman+2023] のアイデアは，ノイズ分布とデータ分布を結ぶ輸送 $p_t:=(f_t)_*p_0$ を定めるベクトル場 $F(x_t,t)$ に関して，
$$
\ov{\L}(\theta):=\abs{F_\theta(X_\tau,\tau)-F(X_\tau,\tau)}^2,\qquad \tau\sim\rU([0,T]),X_\tau\sim p_t(x_\tau)
$$
を目的関数とするというものである．

すなわち，ベクトル場ネットワーク $F_\theta$ を準備し，目的の $F$ に向かって最小二乗法を用いて訓練する．

アイデアは大変シンプルであり，拡散模型とスコアマッチングのアナロジーに沿うものであるが，条件を満たす $F$ は複数ある上，データ分布 $p_T$ が不明であるために $F$ の解析的な表示も不明である．

これは Monte Carlo 法により解決できる．

### 条件付きフローマッチング (CFM) {#sec-CFM}

まずデータ分布 $p_T$ を，経験分布
$$
q(y)\,dy=\frac{1}{n}\sum_{i=1}^n\delta_{x_i}
$$
と Gauss 核の畳み込みで近似する：
$$
p_T(x)=\int_{\R^d} p_T(x|y)q(y)\,dy,\qquad p_T(-|y)=\rN_d(y,\sigma^2I_d).
$$

すると，各データ点 $y\in\{x_i\}_{i=1}^n$ に関して輸送 $(p_t(x|y)\,dx)_t$ を
$$
p_T(-|y)=\rN_d(y,\sigma^2I_d),\qquad p_0(-|y)=\rN(0,I_d),
$$ {#eq-CPP}
を満たすように学習すれば，全体としてノイズ分布をデータ分布に輸送する道 $(p_t)$ が得られる．

これを，$p_T(-|y)$ を生成する **条件付きベクトル場** (Conditional Vector Field) $F_t(-|y)$ として学習し，最終的に次のように混合することで所望のベクトル場 $F_t$ を得る [定理1 @Lipman+2023]：
$$
F_t(x)=\int_{\R^d}F_t(x|y)\frac{p_t(x|y)q(y)}{p_t(x)}\,dy.
$$

このことに基づき，次の代理目標が得られる：
$$
\L(\theta):=\abs{F_\theta(X_\tau,\tau)-F_t(X_\tau|Y)}^2,\qquad \tau\sim\rU([0,T]),Y\sim q(y)\,dy,X_\tau\sim p_\tau(x|y)\,dx.
$$ {#eq-CFM-objective}

実は，単に代理目標となっているだけでなく，$\ov{\L}$ と $\L$ の $\theta$ に関する勾配は一致する [定理2 @Lipman+2023]．

条件付き変数 $y$ が離散的でない場合もこれは成り立つ [定理3.1 @Tong+2024]．さらに，$p_0(-)$ が Gauss でない場合にも CFM は拡張できる [@Tong+2024]．

### 架橋の選択 {#sec-CFM-model}

最後に，条件付き確率の ([-@eq-CPP]) を満たす輸送 $(p_t(-|y))$ を，どのようにパラメータ付けして学習するかを考える．

式 ([-@eq-CPP]) の始点と終点が Gauss 分布であることを見れば，Gauss 空間内での輸送
$$
p_t(x|y)\,dx=\rN\Paren{\mu_t(y),\sigma_t(y)^2I_d},\qquad t\in[0,T],
$$
が候補に上がる．ただし，$\sigma_t$ は単調減少とし，
$$
(\mu_0(y),\mu_T(y))=(0,y),\qquad(\sigma_0(y),\sigma_T(y))=(1,\sigma_\min).
$$

$p_0(x|y)\,dx=\rN_d(0,I_d)$ をこのような分布に押し出す写像で最も簡単なものは
$$
\psi_t(x):=\sigma_t(y)x+\mu_t(y)
$$ {#eq-FM-model}
である．したがって，フロー $(\psi_t)$ を定めるベクトル場
$$
F_t(x|y):=\frac{\sigma_t'(y)}{\sigma_t(y)}\Paren{x-\mu_t(y)}+\mu_t'(y)
$$ {#eq-FM-VF}
を目標として学習される．これが CFM (Conditional Flow Matching) [@Lipman+2023] である．

::: {.callout-caution title="フローマッチングの例としての拡散模型" collapse="true" icon="false"}

[拡散模型は２種の SDE でデータ分布をノイズ分布に還元しているとみれる](Diffusion.qmd#sec-forward-diffusion)．

DDPM が対応する分散保存過程の逆は，輸送
$$
p_t(x|y)\,dx=\rN(y,\sigma^2_{T-t}I_d)
$$
を定める．これは
$$
\mu_t(y)=y,\qquad\sigma_t(y)=\sigma_{T-t},
$$
の場合に当たる．

SGM が対応する分散爆発過程の逆は，輸送
$$
p_t(x|y)\,dx=\rN\Paren{\al_{T-t}y,(1-\al_{T-t}^2)I_d}
$$
を定める．これは
$$
\mu_t(y)=\al_{T-t}y,\qquad\sigma_t(y)=\sqrt{1-\al_{T-t}^2},
$$
の場合に当たる．

この見地から，拡散模型も，フローマッチングによりより効率的に訓練することができる．

:::

::: {.callout-caution title="最適輸送になる場合" collapse="true" icon="false"}

$\mu_t,\sigma_t$ を線型関数
$$
\mu_t(y)=ty,\qquad\sigma_t(y)=1-(1-\sigma_\min)t,
$$
に設定する．

この場合に対応するフロー
$$
\psi_t(x)=\Paren{1-(1-\sigma_\min)t}x+ty
$$
は，最適輸送になっている [@McCann1997 p.159]．

:::

### モデルから輸送へ {#sec-transport-problem}

FM の貢献は，所望のフロー $(p_t(x|y))$ に対して，ベクトル場 $F_t(x|y)$ をどのように定めれば良いかの必要条件を与えたところにもあるが，何より生成モデリングの問題を，モデリングから輸送へ中心を据え変えた点が大きい．

VAE と DDPM は確率モデルとして考案されたが，**生成モデリングのために確率モデルを考える必要はなかったのである**．^[生成モデルとしての VAE の学習された潜在変数が，何らかの現実を意味していると仮定して解釈を試みることはないだろう．]

重要なのは，「事前分布 $p_0$ をどのように $p_1$ に移すか」という輸送の問題だけであり，我々はずっと確率モデルと尤度という偽の目標に囚われていたのである．

GAN や VAE, NF は最尤推定が目標であった．拡散模型において，確率モデルの最尤推定の見方と輸送計画のスコア場を通じた学習としての見方の２つが出揃ったが，SGM はまだ輸送の問題を SDE の言葉で暗に捉えているのみであった．

フローとその $\cP(E)$ 上への押し出しが生成モデリングの本体であることがやっと明らかになったのは，拡散モデルと連続時間正規化流との関係が自覚されてからようやくのことである．^[フローをナイーブに確率モデルとしてみると無限層のニューラルネットワークと見る．これを打開した NODE のアイデアが，尤度原理という蒙昧の打開に必要であったのかもしれない．「拡散過程はサンプリングが遅い」というのは，この発想の転換の最後の離陸の段階であったのだろう．これは拡散過程の正確なシミュレーションが困難である一方で，区分確定的過程のシミュレーションが容易であることに対応する．]

::: {.callout-tip title="輸送問題^[[@Liu+2023-Flow] と [@Albergo-Vanden-Eijnden2023], [@Heitz+2023] の問題設定に従った．]" icon="false"}

任意の２つの分布 $p_0,p_1\in\cP(\R^d)$ から，これを結ぶ輸送 $(p_t)_{t\in[0,1]}$ を定めるフロー $\phi_t:\R^d\to\R^d$
$$
(\phi_1)_*p_0=p_1
$$
を学習することを考える．^[[@Liu+2023-Flow] はこのような輸送の問題として，GAN や VAE をはじめとした生成モデリングと，ドメイン転移の問題をみた．]

:::

### ODE により輸送問題を解く

FM, Rectified Flow, 確率的補間はいずれも，ODE を通じて輸送問題を解く．

所望の輸送 $(p_t)$ があった場合，これを定めるフローを生成するベクトル場 $F_t$ は次のように特定される：

::: {.callout-tip title="指針^[[@Albergo-Vanden-Eijnden2023 p.2] など．正確なステートメントは，[@Ambrosio+2008 p.183] 定理8.3.1 など参照．]" icon="false"}

$(p_t)$ はベクトル場 $F_t$ に関する次の ODE を満たすとする：
$$
\pp{p_t}{t}+\nabla\cdot(F_tp_t)=0.
$$
このとき，ベクトル場 $F_t$ が定めるフロー $(\phi_t)$
$$
\pp{\phi_t(x)}{t}=F_t(\phi_t(x))
$$
は，輸送 $(p_t)$ を定める：
$$
p_t=(\phi_t)_*p_0.
$$

:::

この ODE を **連続方程式** という．この連続方程式を解くベクトル場 $F_t$ を学習することで，所望の輸送が学習できる．

### FM 再論

FM では各データ点 $y\in\{x_i\}_{i=1}^n$ に関する条件付きベクトル場 $F_t(x|y)$ を $y$ に関して積分したもの $F_t(x)$ が解の１つであることを特定した（[@Lipman+2023] 定理１，第 [-@sec-CFM] 節）．

この $F_t(x|y)$ を CFM 目的関数 ([-@eq-CFM-objective]) の最適化により学習することで $F_t(x)$，果てには $(\phi_t)$ を学習することを目指した．

$F_t(x|y)$ の具体的な形に関してはほとんど留保しており，([-@eq-FM-VF]) の形の条件付きベクトル場が使えるとし，例を２つ挙げたのみである（第 [-@sec-CFM-model] 節）．

CFM は訓練可能な代理目標を定める非常に有用な方法であるが，条件付きベクトル場 $F_t(x|y)$ を最適輸送に学習したからといって，最終的なベクトル場 $F_t(x)$ が最適輸送を定めるとは限らない．これは条件付け変数 $y$ を工夫して得る手法 OT-CFM [@Tong+2024] により乗り越えられる．

以降，FM の例（とみなせる手法）を３つ見る：

::: {.callout-important appearance="simple" icon="false"}

* Rectified Flow [@Liu+2023-Flow] は，ベクトル場でないダイナミクスを対象に FM 様の学習する．FM の理論から逸脱するが，その有用性は最適輸送の枠組みに支えられる．^[さらに，$\sigma\to0$ の極限をとっており，CFM がまだ確率的であるのに対して，Rectified Flow では $p_t(-|y)$ は完全に Delta 分布になる．]
* IADB [@Heitz+2023] は FM 様の目的関数の他に，等価な非確率的アルゴリズムも提案している．学習される軌道は DDIM [@JiamingSong+2021] のものと一致する．
* 確率的補間 [@Albergo-Vanden-Eijnden2023] では，ベクトル場ではなく，輸送計画 $I_t$ を直接回帰するための，等価な二次の目的関数を提案している．^[さらに，Rectified Flow が直線を考えているのに対して，$I_t(x_0,x_1):=\cos(\pi t/2)x_0+\sin(\pi t/2)x_1$ という回転様の補間を考えている．]

:::

### Rectified Flow

Rectified Flow [@Liu+2023-Flow] はこの最適輸送を定めるベクトル場 $F_\theta(x_t,t)$ を，なるべく直線で行けるところまで行くような，区分線形なダイナミクスとして学習することを目指す．

これは，$p_0,p_1$ からのサンプリングが可能な状況下では，目的関数
$$
\L(\theta)=\abs{(X_1-X_0)-F_\theta(X_\tau,\tau)}^2,
$$ {#eq-RF-objective}
$$
\tau\sim\rU([0,1]),X_0\sim p_0,X_1\sim p_1,X_\tau:=\tau X_1+(1-\tau)X_0,
$$
を最小化するように学習することで達成できるという．

これは任意のサンプル $X_0,X_1$ を線型に繋ぐようなダイナミクスを目標として誤差を最小化しているが，このようなダイナミクスは ODE が定めるものではない．実際，簡単に交差してしまい，ODE の解の一意性に違反する．

従って recrified flow の有用性は実証的に認められなければならないが，[@Liu+2023-Flow] は FID と recall に関する SOTA を CIFAR-10 で達成している．

これは，rectified flow は繰り返すことができることによる．目的関数 ([-@eq-RF-objective]) を最小化するベクトル場 $F(x,t)$ について，
$$
dZ_t:=F(Z_t,t)\,dt,\qquad Z_0\sim p_0,
$$
は $Z_1\sim p_1$ を満たす．次に $(Z_0,Z_1)$ に関して再び rectified flow を適用して得るダイナミクスは，元の $F$ よりも直線的で [定理D.7 @Liu+2023-Flow p.26]，輸送コストが落ちたものになる [定理D.5 @Liu+2023-Flow p.24]．

![[@Liu+2023-Flow p.6] より．](Files/reflows.png)

$X_t=tX_1+(1-t)X_0$ という直線的なダイナミクスを変更して，
$$
X_t=\al_tX_1+\beta_tX_0,\qquad\al_1=\beta_0=1,\al_0=\beta_1=0,
$$
という非線型なダイナミクスを考えた場合，これは [probabilistic flow ODE](Diffusion.qmd#sec-probability-flow-ODE) [@Song+2021ICLR] や [DDIM](Diffusion.qmd#sec-DDIM) [@JiamingSong+2021] に等価になる．

Rectified Flow は Stable Diffusion 3 のアーキテクチャ [@Esser+2024] に採用されており，従来の拡散モデルの方法より画像生成用途に優れていると結論付けている．^[DDPM [@Ho+2020] と同様，正確な訓練目標ではなく，困難なデノイジングでの成功を強調する uniform reweighting した訓練目標を用いている点に注意．]

### 繰り返し $\al$-ブレンディング (IADB)

IADB (Iterative $\al$-(de)Blending) [@Heitz+2023] は，どうやら直線にはならないようであるが，効率的な決定論的ダイナミクス $(\phi_t)$ が，次の逐次サンプリングによって得られることを報告している：

::: {.callout-tip appearance="simple" icon="false"}

$N\in\N$ を繰り返し数として，$n\in[N]$ に関して $\al_n:=n/N$ として次をループする：

1. blending
    $$X_{\al_n}^{(n)}:=(1-\al_n)X_0^{(n)}+\al_n X_1^{(n)}.$$
2. deblending
    $$X_\al^{(n)}=(1-\al_n)X_0^{(n+1)}+\al_n X_1^{(n+1)}$$
    を満たすように
    $$
    X_0^{(n+1)}\sim p_0,\qquad X_1^{(n+1)}\sim p_1,
    $$
    を取り直す．
3. $$X_{\al_{n+1}}^{(n+1)}:=(1-\al_{n+1})X_0^{(n+1)}+\al_{n+1}X_1^{(n+1)}$$
    として繰り返す．

:::

$N\to\infty$ の極限で，$(X_{\al_n})_{n\in[N]}$ はある確定的なダイナミクスに収束する [定理 @Heitz+2023 p.3]．

![[Fig.6 @Heitz+2023 p.3] より．](Files/IADB.png)

同時に，ニューラルネットワークによりこの軌道を訓練するための，Rectified Flow 様の目的関数 ([-@eq-RF-objective]) も導入している．

実は，この結果学習される軌道は，[DDIM](Diffusion.qmd#sec-DDIM) [@JiamingSong+2021] のものと一致するため，拡散モデルの決定論的な代替として機能する．

[$\al$-ブレンド](https://ja.wikipedia.org/wiki/アルファブレンド) [@Porter-Duff1984] の名前の通り，コンピュータグラフィクスへの応用も意識して議論されている．

### 確率的補間

[@Albergo-Vanden-Eijnden2023] により提案されたもので，SiT (Scalable Interpolant Transformer) [@Ma+2024] でも用いられている技術である．

この方法では $p_0,p_1$ からのサンプル $x_0,x_1$ に対する決定論的輸送 $(I_t(x_0,x_1))$ に注目し，ベクトル場 $F_t(x)$ を
$$
\argmin\L(F):=\argmin\E\SQuare{\ABs{F_T(I_T(X_0,X_1))-2\partial_tI_T(X_0,X_1)\cdot F_T(I_T(X_0,X_1))}}
$$
のただ一つの解として探索する．この解が輸送 $(p_t)$ を定めることが，連続方程式を通じて [命題1 @Albergo-Vanden-Eijnden2023 p.4] で示されている2．

この方法では，目標 $p_1$ と学習された輸送 $(\phi_1)_*p_0$ との誤差を 2-Wasserstein 距離で測ることもできる．

また，学習されたダイナミクスは，ある Langevin 過程の時間変換に等しくなる [命題4 @Albergo-Vanden-Eijnden2023 p.7]．

### 軌道推定

さて，輸送問題 [-@sec-transport-problem] は，確率過程 $(X_t)_{t\in[0,1]}$ を，２つの時点 $X_0,X_1$ に関する観測から推定する問題とも等価である．

こうみると，拡散模型とフローマッチングの違いは，２つの $X_0,X_1$ を繋ぐダイナミクス $(X_t)_{t\in[0,1]}$ を，どのような帰納バイアスの下で推定するかの違いに他ならない．

すると拡散模型は，これを Langevin 拡散により内挿する問題，そして FM をはじめとして Rectified Flow や $\al$-ブレンディングは，これを線型などの単純なダイナミクスにより内挿する問題として理解できる．

一般に，確率過程 $(X_t)_{t\in[0,1]}$ の独立なコピー $X^1,X^2,\cdots,X^n$ から，ランダムな時点 $t\in[0,1]$ での観測
$$
X^1_{t_1},X^2_{t_2},\cdots,X^n_{t_n},\qquad\{t_i\}_{i=1}^n\subset[0,1],
$$
をもとに，$(X_t)$ を推定する問題を **軌道推定** (trajectory inference) [@Lavenant+2024] という．^[[@Hashimoto+2016], [@Koshizuka-Sato2023] などは scRNA-seq データへの応用を念頭に population dynamics と呼んでいる．古典的な横断面データ (cross-sectional data) の設定に似ている．]

**作用マッチング** (Action Matching) [@Neklyudov+2023] では，生成モデリングを軌道推定の問題として解く．

ただし，軌道はある作用 $s_t^*:\R^d\to\R$ に関する勾配ベクトル場 $\nabla_xs_t^*$ が定めるフローであると仮定した下で，この場を
$$
\wt{\L}(\theta)=\frac{1}{2}\E\SQuare{\ABs{\nabla s_T(X_T)-\nabla s^*_T(X_T)}},\qquad T\sim\rU([0,1]),X_T\sim p_T,
$$
の最小化によって学習することを考える．

この $\wt{\L}$ は真の場 $s^*$ を含むが，次の目的関数と定数を除いて一致するため，次の目的関数により訓練可能である：
$$
\L(\theta):=\E\SQuare{s_0(X_0)-s_1(X_1)+\frac{1}{2}\abs{\nabla s_T(X_T)}^2+\partial_ts_T(x_T)}.
$$

一方で Neural Lagrangian Schrödinger 橋 [@Koshizuka-Sato2023] では，同様にラグランジアンの言葉で帰納バイアスを導入しながら，拡散過程のダイナミクスを学習する．

## 文献紹介 {.appendix}

[@Lettermann+2024] は NODE に触れつつ，随伴感度法を用いた複雑系のモデリングとパラメータ推定の方法を解説したチュートリアルである．

[@Albergo+2023] は確率的補間の観点をさらに推し進め，CNF と Diffusion モデルを統一的な観点から提示している．

Cambridge MLG による An Introduction to Flow Matching の web ページ [@Fjelde+2024] も参照．

軌道推定の見方は新しいようで古い．はじめ TrajectoryNet [@Tong+2020] という CNF 手法は軌道推定に用いられており，のちに OT-CFM として生成モデリングにも使えることが自覚されたのである．

TrajectoryNet では，OT によりより直線的な軌道が学習されるような帰納バイアスを導入することが主眼であった．