---
title: "正規化流"
subtitle: "深層生成モデル５"
author: "司馬 博文"
date: 2/14/2024
categories: [Deep, Sampling]
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apalike.csl
abstract-title: 概要
abstract: 確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．
---

{{< include ../../../_preamble.qmd >}}

[@Kobyzev+2021], [@Papamakarios+2021]．

## 正則化流 (NF)

### 原理

[GAN](../Kernels/Deep3.qmd)，[VAE](../Kernels/Deep4.qmd)，[拡散モデル](Diffusion.qmd) など，深層生成モデルは，潜在空間 $\cZ$ 上の基底分布 $p(z)dz$ を，パラメータ $w\in\cW$ を持つ深層ニューラルネットによる変換 $f_w:\cZ\times\cW\to\cX$ を通じて，押し出し
$$
p_w(x):=(f_w)_*p(x)
$$
により $\cX$ 上の分布をモデリングする．

このモデル $\{p_w\}_{w\in\cW}$ の尤度は解析的に表示できない．そこで，[GAN](../Kernels/Deep3.qmd) [@Goodfellow+2014] は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものであり，[VAE](../Kernels/Deep4.qmd) [@Kingma-Welling2014] は変分下界を通じて尤度を近似するというものであった．

**正則化流** (normalizing flow / flow-based models) では，[拡散モデル](Diffusion.qmd) に似て，「逆変換」を利用することを考える．

すなわち，$\{f_w\}\subset\L(\cZ,\cX)$ が可逆であるように設計するのである．逆関数を $g_w:=f_w^{-1}$ と表すと，$p_w(x)dx$ は $p(z)dz$ の $g_w$ による引き戻しの関係になっているから，[変数変換](../../2023/Probability/Beta-Gamma.qmd#sec-transform) を通じて，
$$
p_w(x)=p(g_w(x))\abs{\det J_{g_w}(x)}\;\ae
$$
が成立する．

すると，
$$
\log p_w(x)=\log p(g_w(x))+\log\abs{\det J_{g_w}(x)}
$$
を通じて，尤度の評価とパラメータの最尤推定が可能である．

### 実装

上述のアイデアは，$f$ が表現力が高く数値的に可微分なモデルで表現できれば最も強力な形で実装できるが，これがまさにニューラルネットワークである．

従って，可逆なニューラルネットワーク $\{f_w\}\subset\L(\cZ,\cX)$ を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．^[この考え方は，VAE ではデコーダーをエンコーダーを用いていたものを，１つの可逆な NN で済ましているようにみなせる．]

このとき，行列式 $\det:\GL_n(\R)\to\R^\times$ は群準同型であるから，$g_w$ のヤコビアンは，各層のヤコビアンの積として得られ，その対数は
$$
\log\Abs{\det J_{g_w}(x)}=\sum_{i=1}^n\log\Abs{\det J_{g_i}(z_i)},\qquad z_i:=f_i\circ\cdots\circ f_1(z),
$$
と得られる．

この条件はたしかにモデルに仮定を置いている（$p(z)dz$ は典型的に正規で，$f_w$ は可逆である）．しかしそれでも，深層ニューラルネットワーク $\{f_w\}$ の表現力は十分高いため，多様な密度のモデリングにも使うことが出来る [@Papamakarios+2021]．

また一方で，変分推論における $E$-ステップ（変分分布について期待値を取るステップ）などにおいて，複雑な分布からのサンプラーとしても用いられる [@Gao+2020]．

### カップリング流

ある可微分同相 $f_\theta:\R^m\to\R^m$ と任意の関数 $\Theta:\R^{n-m}\to\R^{n-m}$ について，
$$
(z^{(1)},z^{(2)})\mapsto (f_{\Theta(z^{(2)})}(z^{(1)}),z^{(2)})
$$
で定まる変換を **カップリング層** という．

この逆変換は
$$
(x^{(1)},x^{(2)})\mapsto (f^{-1}_{\Theta(x^{(2)})}(x^{(1)}),x^{(2)})
$$
で与えられる．

加えて，後半の成分 $(-)^{(2)}$ には変換を施していないので，カップリング層の Jacobian は $f$ の Jacobian に一致する．

$f_\theta$ は各成分が可逆になるように設計することで $f_\theta^{-1}$ が計算しやすくされることが多い．

### 自己回帰流

入力 $z\in\R^n$ を形式上時系列と見做し，ある可微分関数 $f_\theta:\R\to\R$ と任意の関数列 $\Theta_i$ について，
$$
x_i=f_{\Theta_i(x_{1:i-1})}(z_i)
$$
と再帰的に定義していく変換 $z\mapsto x$ を **自己回帰流** という．

この逆は
$$
z_i=f_{\Theta_i(x_{1:i-1})}^{-1}(x_i)
$$
で与えられる．

自己回帰流の Jacobi 行列は上三角行列になるので，Jacobian は効率的に計算できる．

#### MAF

$\Theta_i$ は典型的に単一の自己回帰型のニューラルネットワークを用いてモデリングする．その際，$\Theta_i$ が $x_{1:i-1}$ のみに依存するようにマスクをする  MAF (Masked Autoregressive Flow) [@Papamakarios+2017]．

[@Papamakarios+2017] では $f$ は affine 関数としている．

#### 逆自己回帰流

元から $f^{-1}$ の方をモデリングする方法を IAF (Inverse Autoregressive Flow) [@Kingma+2016] という．順方向のものが密度評価が速いのに比べ，IAF はサンプリングに使われる．

Parallel WaveNet [@Oord+2018] では，WaveNet モデル $p_t$ からのサンプリングを加速させる方法として IAF $p_s$ を用いた．

$p_t$ の密度評価は高速であったため，$\KL(p_s,p_t)$ の値は，$p_t$ の評価と IAF $p_s$ からのサンプリングによって効率的に計算できる．

この KL 乖離度を最適化することで $p_t$ のモデルを $p_s$ に移すことで，サンプルの質を保ちながらサンプリングを加速することに成功した．

### 残差フロー

残差接続
$$
u\mapsto u+F(u)
$$
において，残差ブロック $F$ が縮小写像になるようにすれば，全体として可逆な層となる．

iResNet (Invertible Residual Networks) [@Behrmann+2019] では，これを CNN において実装した．

#### Jacobian を推定する方法

これは極めて一般的な層を定めてしまい，逆変換が不明であるが，Jacobian は
$$
\log\abs{\det(I+J_F)}=\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k}\Tr(J_F^k)
$$
を通じて，$\Tr(J_F^k)$ が Hutchinson の跡推定量により，その無限和が Russian-roulette 推定量 [@Hutchinson1990], [@Skilling1989] により不偏推定できる [@Chen+2019]．

#### Jacobian が計算可能な場合

$J_F$ のランクが低い場合は，$I+J_F$ の形の行列の Jacobian は効率的に計算できる．

Planar flow [@Rezebde0Niganed2015] では隠れ素子数１の残差接続層を用いたものである．

Sylvester flow [@vandenBerg+2019] も．

## 連続流

### フローマッチング

フローマッチング [@Lipman+2023], rectified flow [@Liu+2023-Flow]

## 確率的補間

[@Albergo-Vanden-Eijnden2023] により提案されたもので，SiT (Scalable Interpolant Transformer) [@Ma+2024] でも用いられている技術である．

[@Albergo+2023]

## 文献 {.appendix}

Janosh Riebesell のリポジトリ [`awesome-normalizing-flows`](https://github.com/janosh/awesome-normalizing-flows) に実装がまとめられている．

[@Murphy2023] 第23章，