---
title: "正規化流"
subtitle: "深層生成モデル４"
author: "司馬 博文"
image: Files/NF2.png
date: 2/14/2024
date-modified: 8/6/2024
categories: [Deep, Sampling]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 確率分布を Gauss 潜在変数の非線型な押し出しとしてモデリングする．この押し出しを深層ニューラルネットワークでモデリングすれば，豊かな表現力が得られる．加えて，このニューラルネットワークを可逆に設計すれば，このモデルの尤度も評価することが出来る．
listing: 
    -   id: flow-listing
        type: grid
        sort: false
        contents:
            - "NF1.qmd"
            - "NF2.qmd"
            - "Diffusion.qmd"
        date-format: iso
        fields: [title,image,date,subtitle]
---

{{< include ../../../assets/_preamble.qmd >}}

::: {#flow-listing}
:::

## 正則化流 (NF) [@Tabak-Vanden-Eijnden2010]

### 原理

[GAN](../Kernels/Deep3.qmd)，[VAE](../Kernels/Deep4.qmd)，[拡散モデル](Diffusion.qmd) など，深層生成モデルは，潜在空間 $\cZ$ 上の基底分布 $p(z)dz$ を，パラメータ $w\in\cW$ を持つ深層ニューラルネットによる変換 $f_w:\cZ\times\cW\to\cX$ を通じて，押し出し
$$
p_w(x):=(f_w)_*p(x)
$$
により $\cX$ 上の分布をモデリングする．

このモデル $\{p_w\}_{w\in\cW}$ の尤度は解析的に表示できない．そこで，[GAN](../Kernels/Deep3.qmd) [@Goodfellow+2014] は敵対的な学習規則を用いれば，尤度の評価を回避できるというアイデアに基づくものであり，[VAE](../Kernels/Deep4.qmd) [@Kingma-Welling2014] は変分下界を通じて尤度を近似するというものであった．

**正則化流** (normalizing flow / flow-based models) では，[拡散モデル](Diffusion.qmd) に似て，「逆変換」を利用することを考える．

すなわち，$\{f_w\}\subset\L(\cZ,\cX)$ が可逆であるように設計するのである．逆関数を $g_w:=f_w^{-1}$ と表すと，$p_w(x)dx$ は $p(z)dz$ の $g_w$ による引き戻しの関係になっているから，[変数変換](../../2023/Probability/Beta-Gamma.qmd#sec-transform) を通じて，
$$
p_w(x)=p(g_w(x))\abs{\det J_{g_w}(x)}\;\ae
$$
が成立する．

すると，
$$
\log p_w(x)=\log p(g_w(x))+\log\abs{\det J_{g_w}(x)}
$$
を通じて，尤度の評価とパラメータの最尤推定が可能である．

### 実装

上述のアイデアは，$f$ が表現力が高く数値的に可微分なモデルで表現できれば最も強力な形で実装できるが，これがまさにニューラルネットワークである．

従って，可逆なニューラルネットワーク $\{f_w\}\subset\L(\cZ,\cX)$ を設計することを考える．これは，各層が可逆な変換を定めるようにすることが必要十分である．^[この考え方は，VAE ではデコーダーをエンコーダーを用いていたものを，１つの可逆な NN で済ましているようにみなせる．]

このとき，行列式 $\det:\GL_n(\R)\to\R^\times$ は群準同型であるから，$g_w$ のヤコビアンは，各層のヤコビアンの積として得られ，その対数は
$$
\log\Abs{\det J_{g_w}(x)}=\sum_{i=1}^n\log\Abs{\det J_{g_i}(z_i)},\qquad z_i:=f_i\circ\cdots\circ f_1(z),
$$
と得られる．

この条件はたしかにモデルに仮定を置いている（$p(z)dz$ は典型的に正規で，$f_w$ は可逆である）．しかしそれでも，深層ニューラルネットワーク $\{f_w\}$ の表現力は十分高いため，多様な密度のモデリングにも使うことが出来る [@Papamakarios+2021]．

また一方で，変分推論における $E$-ステップ（変分分布について期待値を取るステップ）などにおいて，複雑な分布からのサンプラーとしても用いられる [@Gao+2020]．

### 応用

#### ベイズ推定

変分推論において，変分事後分布のモデリングにフローが使えることを提唱し，フローベースのアプローチを有名にしたのが [@Rezebde0Niganed2015] であった．

このような変分ベイズ推論，特にベイズ深層学習に向けたフローベースモデルが盛んに提案されている：Householder flow [@Tomczak-Welling2017] は VAE の改良, IAF [@Kingma+2016], 乗法的正則化流 [@Louizos-Welling2017], Sylvester flow [@vandenBerg+2019] など．

#### ベイズ計算

Neural Importance Sampling [@Muller+2019] とは，困難な分布からの重点サンプリングの際に，提案分布を正則化流で近似する方法である．

Boltzmann Generator [@Noe+2019] は名前の通り，多体系の平衡分布から正則化流でサンプリングをするという手法である．

[@Hoffmann+2019] は IAF を用いて目標分布を学習し，学習された密度 $q$ で変換後の分布から MCMC サンプリングをすることで効率がはるかに改善することを報告した．実際，フローによる変換を受けた後は対象分布は正規分布に近くなることから，MCMC サンプリングを減速させる要因の多くが消滅していることが期待される．

#### 密度推定

目標の分布 $p_w$ を Guass 分布 $p$ からの写像 $(f_w)_*p$ として捉える発想は，まずなんといっても密度推定に用いられた [@Chen-Gopinath2000]．

この $f_w$ のモデリングにニューラルネットワークを用いるという発想は [@Rippel-Adams2013] の Deep Density Model 以来であるようだ．

その後同様の発想は非線型独立成分分析 [@Dinh+2015]，引き続き密度推定 [@Dinh+2017] に用いられた．

現在は MAF [-@sec-MAF] の性能が圧倒的である．

#### 表現学習

通常の VAE などは，あくまで $p(x|z)$ を学習する形をとるが，正則化流を用いて結合分布 $p(x,z)$ を学習することで，双方を対等にモデリングすることができる．

これを flow-based **hybrid model** [@Nalisnick+2019] という．これは予測と生成のタスクでも良い性能を見せるが，分布外検知などの応用も示唆している．

異常検知 [@Zhang+2020Detection]，不確実性定量化 [@Charpentier+2020] のような種々の下流タスクに用いられた場合は，VAE など NN を２つ用いる手法よりもモデルが軽量で，順方向での１度の使用で足りるなどの美点があるという．

#### 生成

画像生成への応用が多い：GLOW [@Kingma-Dhariwal2018] ([OpenAI release](https://openai.com/index/glow/)), 残差フロー [@Chen+2019] など．

動画に対する応用も提案されている：VideoFlow [@Kumar+2020]．

言語に対する応用もある [@Tran+2019]．言語は離散データであることが難点であるが，潜在空間上でフローを使うことも提案されている [@Ziegler-Rush2019]．

#### 蒸留

純粋な生成モデリングの他に，IAF [-@sec-IAF] は [@Oord+2018] において音声生成モデルの蒸留に用いられた．

WaveFLOW [@Prenger+2018], FloWaveNet [@Kim+2019] などもカップリング層を取り入れて WaveNet の高速化に成功している．

#### SBI

モデルの尤度は隠されており，入力 $\theta$ に対して，$p(x|\theta)$ からのサンプルのみが利用可能であるとする．このような状況でベイズ推論を行う問題を **simulation-based inference (SBI)** という．これはデータの分布のサンプルから学習して，似たようなデータを増やすという生成モデルのタスクに似ている．

この際，任意の分布 $p(\theta)$ に対して，結合分布
$$
p(x,\theta)=p(x|\theta)p(\theta)
$$
を，シミュレータから得られるサンプルのみから，フローベースモデルにより学習してしまうことで，ベイズ推論が可能になる [@Papamakarios+2019]．

この方法はさらに，事後分布推定に特化することで，SMC-ABC などの従来の近似ベイズ計算技法の性能を超えていくようである [@Greenberg+2019]．

## 離散流 (DNF)

### カップリング流

ある可微分同相 $f_\theta:\R^m\to\R^m$ と任意の関数 $\Theta:\R^{n-m}\to\R^{n-m}$ について，
$$
(z^{(1)},z^{(2)})\mapsto (f_{\Theta(z^{(2)})}(z^{(1)}),z^{(2)})
$$
で定まる変換を **カップリング層** という．

この逆変換は
$$
(x^{(1)},x^{(2)})\mapsto (f^{-1}_{\Theta(x^{(2)})}(x^{(1)}),x^{(2)})
$$
で与えられる．

加えて，後半の成分 $(-)^{(2)}$ には変換を施していないので，カップリング層の Jacobian は $f$ の Jacobian に一致する．

$f_\theta$ は各成分が可逆になるように設計することで $f_\theta^{-1}$ が計算しやすくされることが多い．

### 自己回帰流

入力 $z\in\R^n$ を形式上時系列と見做し，ある可微分関数 $f_\theta:\R\to\R$ と任意の関数列 $\Theta_i$ について，
$$
x_i=f_{\Theta_i(x_{1:i-1})}(z_i)
$$
と再帰的に定義していく変換 $z\mapsto x$ を **自己回帰流** という．

この逆は
$$
z_i=f_{\Theta_i(x_{1:i-1})}^{-1}(x_i)
$$
で与えられる．

自己回帰流の Jacobi 行列は上三角行列になるので，Jacobian は効率的に計算できる．

#### マスク付き自己回帰流 (MAF) {#sec-MAF}

$\Theta_i$ は典型的に単一の自己回帰型のニューラルネットワークを用いてモデリングする．その際，$\Theta_i$ が $x_{1:i-1}$ のみに依存するようにマスクをする  MAF (Masked Autoregressive Flow) [@Papamakarios+2017]．

[@Papamakarios+2017] では $f$ は affine 関数としている．

#### 逆自己回帰流 (IAF) {#sec-IAF}

元から $f^{-1}$ の方をモデリングする方法を IAF (Inverse Autoregressive Flow) [@Kingma+2016] という．順方向のものが密度評価が速いのに比べ，IAF はサンプリングに使われる．

Parallel WaveNet [@Oord+2018] では，WaveNet モデル $p_t$ からのサンプリングを加速させる方法として IAF $p_s$ を用いた．

$p_t$ の密度評価は高速であったため，$\KL(p_s,p_t)$ の値は，$p_t$ の評価と IAF $p_s$ からのサンプリングによって効率的に計算できる．

この KL 乖離度を最適化することで $p_t$ のモデルを $p_s$ に移すことで，サンプルの質を保ちながらサンプリングを加速することに成功した．

### 残差フロー

残差接続
$$
u\mapsto u+F(u)
$$
において，残差ブロック $F$ が縮小写像になるようにすれば，全体として可逆な層となる．

iResNet (Invertible Residual Networks) [@Behrmann+2019] では，これを CNN において実装した．

#### Jacobian を推定する方法 {#sec-Hutchinson}

これは極めて一般的な層を定めてしまい，逆変換が不明であるが，Jacobian は
$$
\log\abs{\det(I+J_F)}=\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k}\Tr(J_F^k)
$$
を通じて，$\Tr(J_F^k)$ が Hutchinson の跡推定量により，その無限和が Russian-roulette 推定量 [@Hutchinson1990], [@Skilling1989] により不偏推定できる [@Chen+2019]．

#### Jacobian が計算可能な場合

$J_F$ のランクが低い場合は，$I+J_F$ の形の行列の Jacobian は効率的に計算できる．

Planar flow [@Rezebde0Niganed2015] では隠れ素子数１の残差接続層を用いたものである．

Sylvester flow [@vandenBerg+2019] も．

## 文献 {.appendix}

Janosh Riebesell のリポジトリ [`awesome-normalizing-flows`](https://github.com/janosh/awesome-normalizing-flows) に実装がまとめられている．

[@Murphy2023] 第23章は入門に良い．詳細はサーベイ [@Kobyzev+2021], [@Papamakarios+2021] に譲られている．

[@Chen-Gopinath2000] の時点では同様のアイデアは **Gaussianization** と呼ばれていた．

[@Tabak-Vanden-Eijnden2010], [@Tabak-Turner2013] が正則化流の提案と命名を行った．