---
title: "拡散模型の訓練の加速"
subtitle: "PyTorch によるハンズオン２"
author: "司馬 博文"
date: 8/6/2024
date-modified: 8/7/2024
categories: [Deep, Sampling, Python]
bibliography: 
    - ../../../assets/mathematics.bib
    - ../../../assets/bib.bib
    - ../../../assets/bib1.bib
csl: ../../../assets/apalike.csl
abstract-title: 概要
abstract: 前稿で DDPM の実装を紹介したが，実際にローカルのマシンで訓練をしてみると２日かかる．これを加速するためのテクニックと，スパコンの仕様方法を検討する．
code-fold: false
# execute:
#     eval: false
---

{{< include ../../../_preamble.qmd >}}

![[前稿：拡散模型の実装――PyTorch によるハンズオン](DDPM.qmd)](../../../docs/posts/2024/Samplers/DDPM_files/figure-html/fig-encoding-output-1.png){width="70%"}

## 問題点と改善したいこと

データセットの読み込みの段階において，次のコードがある：

```python
kwargs = {'num_workers': 5, 'pin_memory': True, 'prefetch_factor': 2}
train_loader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True, **kwargs)
test_loader  = DataLoader(dataset=test_dataset,  batch_size=inference_batch_size, shuffle=False,  **kwargs)
```

これはデータセット（今回は`MNIST`）を読み込み，iterable 型としての仕様を可能にするためのコードである．

上述の通りのコードだとエポック 18 で `RuntimeError: Shared memory manager connection has timed out` を得たが，`num_workers=0` とするとエラーが発生しなかった．

しかし，`num_workers=0` （デフォルト設定）とすると，デフォルトの単一プロセス処理が実行されるため，並列による高速化の恩恵を受けられない．その結果，１エポック 12 分以上なので，40 時間以上をかける必要が出てきた（寝てる間もディスプレイをオフにするだけでスリープさせず，回し続ける）．

::: {.callout-important appearance="simple" icon="false" title="今回の目標"}

うまく並列処理をするようなコードに書き直すことで，ローカル環境でも１日以内で実行できるようにしたい．これが難しい場合は，統数研のスパコンを利用してでも良いから，並列処理ができるようになってみたい．

:::

## `DataLoader` の引数について

[`DataLoader` メソッドのドキュメント](https://pytorch.org/docs/stable/data.html) を参照すると，

### `num_workers`

は正整数に設定されると，その数だけ並列に動く 'worker' が起動され，マルチプロセス処理が実行される．

しかし，子プロセスも同等のメモリを占めるため，値が大きすぎるとランタイムエラーが発生する（[issue #13246](https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662) 参照）．

さらに，この際の並列処理は Python の `multiprocessing` パッケージによるもので，Windows と MacOS では（Unix 系のような `fork()` ではなく） `spawn()` が呼ばれる．これは別のインタープリターを開始するため，コードの大部分を `if __name__ == "__main__":` で囲まない限り，同じコードを何回も実行することとなり，ランタイムエラーが出現することとなる．

### `pin_memeory`

しかし，CUDA 上のテンソルオブジェクトを並列処理で共有することは非推奨であり，その際は自動メモリ固定 (automatic memory pinning) を行う必要がある．

pinned memory とは page-locked メモリとも呼ばれ，通常の pageable メモリより転送速度が速いという．

さて，paging とはなんだろうか？（一旦後回し）

### `prefetch_factor`

は各 `worker` が取ってきてストックしておくバッチの数である．

すなわち，`num_workers * prefetch_factor` だけデータをメモリに読み込んでおくことになる．