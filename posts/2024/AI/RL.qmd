---
title: "強化学習"
author: "司馬 博文"
date: 2/6/2024
categories: [Control]
toc: true
number-sections: true
code-block-bg: true
code-block-border-left: "#5AB5BA"
code-overflow: wrap
code-fold: true
bibliography: 
    - ../../../mathematics.bib
    - ../../../bib.bib
csl: ../../../apa.csl
abstract-title: 概要
abstract: 強化学習の考え方を数学的に理解する．
crossref:
    sec-prefix: 節
    eq-prefix: 式
    def-prefix: 定義
    def-title: 定義
    thm-prefix: 定理
    thm-title: 定理
---

{{< include ../../../_preamble.qmd >}}

## 導入

強化学習は機械学習の３分類の１つとして「他の２類型（教師あり・教師なし学習）とは大きく文脈が違う」という注記と共によく紹介される．それもその通り．強化学習は，エージェントが世界の中でどのように行動するかを，環境との相互作用を通じて自律的に学ぶ，という人工知能分野の問題設定から生じた学問である [@Sutton-Barto2018]．

強化学習の設定にはいくつか特徴がある．

* 試行錯誤の中で学ぶこと：教師あり学習のように，報酬を最大化するように学んでいくが，学習データというものはなく，試行錯誤の中で学ぶ必要がある．
* 報酬は遅れてくるものもあること：行動の結果がすぐに報酬として返ってくるわけではなく，以前の行動が未来の報酬に影響を与えることがあり，それを踏まえて学習をすることが求められる．

すると，強化学習は専ら動的計画法の議論が中心となる．

強化学習は，部分的に観測されている Markov 決定過程の最適制御として理解される．

在庫管理 [@VanRoy+1997]，動的なチャンネルの割り当て [@Singh-Bertsekas1996]，エレベータ制御 [@Crites-Barto1998]，テーブルゲーム [@Silver+2018]，気候変動対策 [@Rolnick+2022] などにも用いられている．

今後も，[人間のフィードバックによる強化学習](https://ja.wikipedia.org/wiki/%E4%BA%BA%E9%96%93%E3%81%AE%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92) (RLHF: Reinforcement Learning through Human Feedback) [@Christiano+2017] や [GNN](../Kernels/GNN.qmd) のトレーニングなど，他の機械学習手法と組み合わせることでより大きな AI システムを作るにあたって，強化学習は必要不可欠な立場を占めていくだろう．

### 歴史

## Markov 決定過程

## $Q$-学習

強化学習の最も標準的かつ古典的なアルゴリズムが，$Q$-学習 [@Watkins1989], [@Watkins-Dayan1992] と，その派生アルゴリズムである SARSA である [@Powell2011 p.122]．

### TD 学習

推移確率が未知である場合，これをシミュレーションによって推定することができる．この設定では **時間差分学習** (Temporal Difference Learning) とも呼ばれる [@Sutton1988]．

