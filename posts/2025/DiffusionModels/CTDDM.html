<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hirofumi Shiba">

<title>Masked Diffusion Models – Hirofumi Shiba</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../assets/Shiba2.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-c10b30eb894da51f146a98bd36b89e1d.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-4a4176c2a757821146007d2a6478b1b5.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-c10b30eb894da51f146a98bd36b89e1d.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-a11df776b066826d16c4a98fa21b98a3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-a11df776b066826d16c4a98fa21b98a3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-a11df776b066826d16c4a98fa21b98a3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "H"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-diffusion-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-title','listing-image','listing-date','listing-subtitle',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-diffusion-listing'] = new List('listing-diffusion-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-36GX2G6GLL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-36GX2G6GLL', { 'anonymize_ip': true});
</script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Zen+Kurenaido&amp;display=swap" rel="stylesheet">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=BIZ+UDPGothic&amp;display=swap" rel="stylesheet">

<style>
  h1, .title, .description, .subtitle {
    font-family: "Zen Kurenaido", sans-serif !important;
  }
</style>

<!-- <style>
  .menu-text {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
  .navbar-title {
      font-family: "Gill Sans", sans-serif !important;
      font-weight: 400;
      font-style: normal;
  }
</style> -->

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../assets/styles.css">
<meta property="og:title" content="Masked Diffusion Models – Hirofumi Shiba">
<meta property="og:description" content="Masked diffusion models are conceptually based upon an absorbing forward process and its reverse denoising process. However, their roles are intricately intertwined, in that all three aspects of training, sampling, and modeling are involved. To develop our understanding, we give two toy examples, 1d and 2d, without training a neural network, to showcase how absorbing processes behave. We identify core questions which should be investigated to expand our understanding.">
<meta property="og:image" content="https://162348.github.io/posts/2025/DiffusionModels/CTDDM_files/figure-html/cell-9-output-2.png">
<meta property="og:site_name" content="Hirofumi Shiba">
<meta property="og:image:height" content="556">
<meta property="og:image:width" content="1132">
<meta name="twitter:title" content="Masked Diffusion Models – Hirofumi Shiba">
<meta name="twitter:description" content="Masked diffusion models are conceptually based upon an absorbing forward process and its reverse denoising process. However, their roles are intricately intertwined, in that all three aspects of training, sampling, and modeling are involved. To develop our understanding, we give two toy examples, 1d and 2d, without training a neural network, to showcase how absorbing processes behave. We identify core questions which should be investigated to expand our understanding.">
<meta name="twitter:image" content="https://162348.github.io/posts/2025/DiffusionModels/CTDDM_files/figure-html/cell-9-output-2.png">
<meta name="twitter:creator" content="@ano2math5">
<meta name="twitter:image-height" content="556">
<meta name="twitter:image-width" content="1132">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Hirofumi Shiba</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Notes</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-notes">    
        <li>
    <a class="dropdown-item" href="../../../static/English.html">
 <span class="dropdown-text">English Notes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../blog.html">
 <span class="dropdown-text">ノート (Japanese)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Materials.html"> 
<span class="menu-text">Materials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/Software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../static/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/162348/162348.github.io/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Masked Diffusion Models</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
            <p class="subtitle lead">A New Light to Discrete Data Modeling</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Denoising Model</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hirofumi Shiba </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">9/14/2025</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">概要</div>
      <p>Masked diffusion models are conceptually based upon an absorbing forward process and its reverse denoising process. However, their roles are intricately intertwined, in that all three aspects of training, sampling, and modeling are involved. To develop our understanding, we give two toy examples, 1d and 2d, without training a neural network, to showcase how absorbing processes behave. We identify core questions which should be investigated to expand our understanding.</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目次</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#a-time-agnostic-learning-framework" id="toc-a-time-agnostic-learning-framework" class="nav-link" data-scroll-target="#a-time-agnostic-learning-framework"><span class="header-section-number">1.1</span> A Time-Agnostic Learning Framework</a></li>
  <li><a href="#choice-of-alpha_t" id="toc-choice-of-alpha_t" class="nav-link" data-scroll-target="#choice-of-alpha_t"><span class="header-section-number">1.2</span> Choice of <span class="math inline">\(\alpha_t\)</span></a></li>
  <li><a href="#sec-Gibbs-sampler-take" id="toc-sec-Gibbs-sampler-take" class="nav-link" data-scroll-target="#sec-Gibbs-sampler-take"><span class="header-section-number">1.3</span> A Gibbs Sampler Take</a></li>
  <li><a href="#sec-Intermediate-States" id="toc-sec-Intermediate-States" class="nav-link" data-scroll-target="#sec-Intermediate-States"><span class="header-section-number">1.4</span> Intermediate States</a></li>
  <li><a href="#state-dependent-rate" id="toc-state-dependent-rate" class="nav-link" data-scroll-target="#state-dependent-rate"><span class="header-section-number">1.5</span> State-Dependent Rate</a></li>
  </ul></li>
  <li><a href="#sec-Demo" id="toc-sec-Demo" class="nav-link" data-scroll-target="#sec-Demo"><span class="header-section-number">2</span> Demo</a>
  <ul class="collapse">
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup"><span class="header-section-number">2.1</span> Setup</a></li>
  <li><a href="#the-backward-transition-kernel" id="toc-the-backward-transition-kernel" class="nav-link" data-scroll-target="#the-backward-transition-kernel"><span class="header-section-number">2.2</span> The Backward Transition Kernel</a></li>
  <li><a href="#honest-sampling" id="toc-honest-sampling" class="nav-link" data-scroll-target="#honest-sampling"><span class="header-section-number">2.3</span> Honest Sampling</a></li>
  <li><a href="#sec-exp" id="toc-sec-exp" class="nav-link" data-scroll-target="#sec-exp"><span class="header-section-number">2.4</span> Choice of <span class="math inline">\(\alpha_t\)</span></a></li>
  </ul></li>
  <li><a href="#sec-2d-example" id="toc-sec-2d-example" class="nav-link" data-scroll-target="#sec-2d-example"><span class="header-section-number">3</span> 2D Example</a>
  <ul class="collapse">
  <li><a href="#setup-1" id="toc-setup-1" class="nav-link" data-scroll-target="#setup-1"><span class="header-section-number">3.1</span> Setup</a></li>
  <li><a href="#sec-exact-kernel-2d" id="toc-sec-exact-kernel-2d" class="nav-link" data-scroll-target="#sec-exact-kernel-2d"><span class="header-section-number">3.2</span> The Backward Transition Kernel</a></li>
  <li><a href="#honest-sampling-1" id="toc-honest-sampling-1" class="nav-link" data-scroll-target="#honest-sampling-1"><span class="header-section-number">3.3</span> Honest Sampling</a></li>
  <li><a href="#coordinate-wise-sampling" id="toc-coordinate-wise-sampling" class="nav-link" data-scroll-target="#coordinate-wise-sampling"><span class="header-section-number">3.4</span> Coordinate-wise Sampling</a></li>
  <li><a href="#sec-Predictor-Corrector" id="toc-sec-Predictor-Corrector" class="nav-link" data-scroll-target="#sec-Predictor-Corrector"><span class="header-section-number">3.5</span> Corrector Sampling</a></li>
  <li><a href="#sec-Chao" id="toc-sec-Chao" class="nav-link" data-scroll-target="#sec-Chao"><span class="header-section-number">3.6</span> Discussion</a></li>
  </ul></li>
  <li><a href="#future-works" id="toc-future-works" class="nav-link" data-scroll-target="#future-works"><span class="header-section-number">4</span> Future Works</a>
  <ul class="collapse">
  <li><a href="#a-reinforcenment-learning-take" id="toc-a-reinforcenment-learning-take" class="nav-link" data-scroll-target="#a-reinforcenment-learning-take"><span class="header-section-number">4.1</span> A Reinforcenment Learning Take</a></li>
  <li><a href="#a-scaling-analysis" id="toc-a-scaling-analysis" class="nav-link" data-scroll-target="#a-scaling-analysis"><span class="header-section-number">4.2</span> A Scaling Analysis</a></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks"><span class="header-section-number">4.3</span> Concluding Remarks</a></li>
  </ul></li>
  
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="hidden">
<p>A Blog Entry on Bayesian Computation by an Applied Mathematician</p>
<p>$$</p>
<p>$$</p>
</div>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<section id="a-time-agnostic-learning-framework" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="a-time-agnostic-learning-framework"><span class="header-section-number">1.1</span> A Time-Agnostic Learning Framework</h3>
<p>The absorbing process, a.k.a. masked diffusion, has a unique characteristic as a forward process in a discrete denoising diffusion model; it offers a time-agnostic <em>learning to unmask</em> training framework.</p>
<p>When the state space is <span class="math inline">\(E^d\)</span> where <span class="math inline">\(E=\{0,\cdots,K-1\}\)</span> is finite, a current practice is to learn a neural network <span class="math inline">\(p_\theta\)</span> based on a loss given by <span id="eq-L"><span class="math display">\[
\mathcal{L}(\theta):=\int^1_0\frac{\dot{\alpha}_t}{1-\alpha_t}\operatorname{E}\left[\sum_{i=1}^d\log p_\theta(X_0^i|X_t)\right]\,dt,
\tag{1}\]</span></span> where <span class="math inline">\(\alpha_t\)</span> is a <em>noising schedule</em>, determining the convergence speed of the forward process.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>The expectation in (<a href="#eq-L" class="quarto-xref">1</a>) is exactly a cross-entropy loss. Therefore, the loss (<a href="#eq-L" class="quarto-xref">1</a>) can be understood as a weighted cross-entropy loss, weighted by the noising schedule <span class="math inline">\(\alpha_t\)</span>.</p>
<p>Note that <span class="math inline">\(p_\theta\)</span> predicts the true state <span class="math inline">\(x_0\)</span>, based on the current state <span class="math inline">\(x_t\)</span>, some components of which might be masked. Hence we called this framework <em>learning to unmask</em>.</p>
<p>Note also that <span class="math inline">\(p_\theta\)</span> doesn’t take <span class="math inline">\(t\)</span> as an argument <span class="citation" data-cites="Shi+2024">(<a href="#ref-Shi+2024" role="doc-biblioref">Shi et al., 2024</a>)</span>, <span class="citation" data-cites="Ou+2025">(<a href="#ref-Ou+2025" role="doc-biblioref">Ou et al., 2025</a>)</span>, which we call the <em>time-agnostic</em> property, following <span class="citation" data-cites="Zheng+2025">(<a href="#ref-Zheng+2025" role="doc-biblioref">Zheng et al., 2025</a>)</span>.</p>
</section>
<section id="choice-of-alpha_t" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="choice-of-alpha_t"><span class="header-section-number">1.2</span> Choice of <span class="math inline">\(\alpha_t\)</span></h3>
<p>This ‘learning to unmask’ task might be very hard when <span class="math inline">\(t\)</span> is near <span class="math inline">\(1\)</span>, since most of the <span class="math inline">\(x_t^i\)</span>’s are still masked.</p>
<p>For instance, if we choose a linear schedule <span class="math display">\[
\alpha_t=1-t\qquad(t\in[0,1])
\]</span> the scaler <span class="math inline">\(\frac{\dot{\alpha}_t}{1-\alpha_t}=-t^{-1}\)</span> before the expectation in (<a href="#eq-L" class="quarto-xref">1</a>) puts less weight on large <span class="math inline">\(t\approx1\)</span>, while puts much more weight on small <span class="math inline">\(t\approx0\)</span>, where most of the <span class="math inline">\(x_t^i\)</span>’s should be already unmasked.</p>
<p>Hence, selecting the schedule <span class="math inline">\(\alpha_t\)</span> to make learning easier can be very effective, for example by reducing the variance of gradient estimator in a SGD algorithm. Actually, this is a part of the technique how <span class="citation" data-cites="Arriola+2025">(<a href="#ref-Arriola+2025" role="doc-biblioref">Arriola et al., 2025</a>)</span> achieved their remarkable empirical results.</p>
<p>This flexibility of <span class="math inline">\(\alpha_t\)</span> is why the loss (<a href="#eq-L" class="quarto-xref">1</a>) is considered as a potential competitor against the current dominant autoregressive models. In fact, one work <span class="citation" data-cites="Chao+2025">(<a href="#ref-Chao+2025" role="doc-biblioref">Chao et al., 2025</a>)</span>, still under review, claimed their masked diffusion model surpassed the autoregressive model on the task of language modeling, achieving an evaluation perplexity of 15.36 on the OpenWebText dataset.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>We briefly discuss their trick and related promising techniques to improve the model, before programming toy examples in <a href="#sec-Demo" class="quarto-xref">Section&nbsp;2</a> and <a href="#sec-2d-example" class="quarto-xref">Section&nbsp;3</a> to deepen our understanding in absorbing forward process.</p>
</section>
<section id="sec-Gibbs-sampler-take" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="sec-Gibbs-sampler-take"><span class="header-section-number">1.3</span> A Gibbs Sampler Take</h3>
<p>One problem about the loss (<a href="#eq-L" class="quarto-xref">1</a>) is that <span class="math inline">\(p_\theta\)</span> predicts the unmasked complete sequence in a product form: <span class="math display">\[
p_\theta(x_0|x_t)=\prod_{i=1}^d p_\theta(x_0^i|x_t).
\]</span></p>
<p>This should cause no problem when we unmask one component at a time, since it will be a form of ancestral sampling based on disintegration property.</p>
<p>However, when unmasking two or more components simultaneously, for example when the number of steps is less than <span class="math inline">\(d\)</span>, the product form assumption will simply introduce a bias, as the data distribution is by no means of product form on <span class="math inline">\(E^d\)</span>.</p>
<p>Here, to recover correctness asymptotically, analogy with Gibbs sampling becomes very important.</p>
<p>For example, predictor-corrector technique can be readily employed to mitigate this bias, as discussed in <span class="citation" data-cites="Lezama+2023">(<a href="#ref-Lezama+2023" role="doc-biblioref">Lezama et al., 2023</a>)</span>, <span class="citation" data-cites="Zhao+2024">(<a href="#ref-Zhao+2024" role="doc-biblioref">S. Zhao et al., 2024</a>)</span>, <span class="citation" data-cites="Zhao+2024Unified">(<a href="#ref-Zhao+2024Unified" role="doc-biblioref">L. Zhao et al., 2024</a>)</span>, <span class="citation" data-cites="Gat+2024">(<a href="#ref-Gat+2024" role="doc-biblioref">Gat et al., 2024</a>)</span>, <span class="citation" data-cites="Wang+2025">(<a href="#ref-Wang+2025" role="doc-biblioref">Wang et al., 2025</a>)</span>.</p>
<p>We demonstrate this strategy in <a href="#sec-Predictor-Corrector" class="quarto-xref">Section&nbsp;3.5</a>.</p>
</section>
<section id="sec-Intermediate-States" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="sec-Intermediate-States"><span class="header-section-number">1.4</span> Intermediate States</h3>
<p>As we mentioned earlier, unmasking can be a very hard task, as closely investigated in <span class="citation" data-cites="Kim+2025">(<a href="#ref-Kim+2025" role="doc-biblioref">Kim et al., 2025, sec. 3</a>)</span>.</p>
<p>To alleviate this problem, <span class="citation" data-cites="Chao+2025">(<a href="#ref-Chao+2025" role="doc-biblioref">Chao et al., 2025</a>)</span> introduced intermediate states by re-encoding the token in a base-<span class="math inline">\(2\)</span> encoding, such as <span class="math display">\[
5\mapsto 101.
\]</span> The right-hand side needs three steps to be completely unmasked, while the left-hand side only needs one jump.</p>
<p>Therefore, unmasking can be easier to learn, compared to the original token encoding.</p>
<p>However, this is not the only advantage of intermediate states. <span class="citation" data-cites="Chao+2025">(<a href="#ref-Chao+2025" role="doc-biblioref">Chao et al., 2025</a>)</span> were able to construct a full predictor <span class="math inline">\(p_\theta\)</span> without the product form assumption on each token.</p>
<p>This approach might act as a block Gibbs sampler and make the convergence faster, as we will discuss later in <a href="#sec-Chao" class="quarto-xref">Section&nbsp;3.6</a>.</p>
</section>
<section id="state-dependent-rate" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="state-dependent-rate"><span class="header-section-number">1.5</span> State-Dependent Rate</h3>
<p>As a function of <span class="math inline">\(t\mapsto\alpha_t\)</span>, different choices for <span class="math inline">\(\alpha_t\)</span> seem to make little impact on the total performance of the model, as we observe in our toy example in <a href="#sec-exp" class="quarto-xref">Section&nbsp;2.4</a>.</p>
<p>However, if we allow <span class="math inline">\(\alpha_t\)</span> to depend on the state as in <span class="citation" data-cites="Shi+2024">(<a href="#ref-Shi+2024" role="doc-biblioref">Shi et al., 2024, sec. 6</a>)</span>, I believe the masked diffusion model will start to show its real potential over currently dominant autoregressive framework.</p>
<p>A problem arises when one tries to learn <span class="math inline">\(\alpha_t\)</span> at the same time, for example, by including a corresponding term into the loss (<a href="#eq-L" class="quarto-xref">1</a>). This will lead to amplified variances of the gradient estimates and unstable training, as reported in <span class="citation" data-cites="Shi+2024">(<a href="#ref-Shi+2024" role="doc-biblioref">Shi et al., 2024</a>)</span> and <span class="citation" data-cites="Arriola+2025">(<a href="#ref-Arriola+2025" role="doc-biblioref">Arriola et al., 2025</a>)</span>.</p>
<p>The idea of exploiting state-dependent rate is already very common in sampling time <span class="citation" data-cites="Peng+2025">(<a href="#ref-Peng+2025" role="doc-biblioref">Peng et al., 2025</a>)</span>, <span class="citation" data-cites="Liu+2025">(<a href="#ref-Liu+2025" role="doc-biblioref">Liu et al., 2025</a>)</span>, <span class="citation" data-cites="Kim+2025">(<a href="#ref-Kim+2025" role="doc-biblioref">Kim et al., 2025</a>)</span>, <span class="citation" data-cites="Rout+2025">(<a href="#ref-Rout+2025" role="doc-biblioref">Rout et al., 2025</a>)</span>, determining which token to unmask next during the backward sampling, a bit like Monte Carlo tree search in reinforcement learning.</p>
</section>
</section>
<section id="sec-Demo" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-Demo"><span class="header-section-number">2</span> Demo</h2>
<p>To demonstrate what an absorbing process does, we carry out generation from a toy data distribution <span class="math inline">\(\pi_{\text{data}}\)</span> on <span class="math inline">\(5=\{0,1,2,3,4\}\)</span>, by running an exact reverse kernel of the absorbing (masked) forward process.</p>
<p>Therefore, no neural network training will be involved. A 2d example, which is much more interesting, in <a href="#sec-2d-example" class="quarto-xref">Section&nbsp;3</a> will basically process in parallel.</p>
<section id="setup" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">2.1</span> Setup</h3>
<div id="be87021b" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="95c4c33d" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>p_data <span class="op">=</span> np.array([<span class="fl">0.40</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.10</span>, <span class="fl">0.02</span>], dtype<span class="op">=</span><span class="bu">float</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We will represent the MASK as <span class="math inline">\(-1\)</span>. The state space is then <span class="math inline">\(E:=5 \cup \{-1\}\)</span>.</p>
<div id="ece93547" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>MASK <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>An important design choice in the forward process is the noising schedule <span class="math inline">\(\alpha_t\)</span>, which can be interpreted as <em>survival probability</em> and satisfy the following relatioship with the jump intensity <span class="math inline">\(\beta_t\)</span>: <span class="math display">\[
\alpha_t=\exp\left(-\int^t_0\beta_s\,ds\right),\qquad t\in[0,1].
\]</span></p>
<p>Let us keep it simple and set <span class="math inline">\(\alpha_t=t\)</span>. To achive this, we need to set <span class="math display">\[
\beta_t=\frac{1}{1-t},\qquad t\in[0,1),
\]</span> which is clearly diverging as <span class="math inline">\(t\to1\)</span>. This is to ensure the process to converge in finite time.</p>
<div id="c865e199" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># number of steps</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.00</span>, <span class="fl">0.00</span>, T<span class="op">+</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="the-backward-transition-kernel" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="the-backward-transition-kernel"><span class="header-section-number">2.2</span> The Backward Transition Kernel</h3>
<p>In this setting, the backward transition kernel <span class="math inline">\(p(x_{t-1} | x_t)\)</span> satisfies <span class="math display">\[
\operatorname{P}[X_{t-1}=-1|X_t=-1]=\frac{1 - \alpha_{t-1}}{1 - \alpha_t}.
\]</span> In the other cases, the unmasked values <span class="math inline">\(x_{t-1}\)</span> should be determined according to <span class="math inline">\(\pi_{\text{data}}\)</span>, which is unavailable in a real setting, of course.</p>
<div id="ece6987d" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])  <span class="co"># length T</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="71030694" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Start from x_T = MASK for all samples, apply the exact reverse transitions down to t=0.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns x_0 samples in 5 = {0,1,...,4}.</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    x_t <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    hist <span class="op">=</span> np.empty((T<span class="op">+</span><span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    hist[<span class="dv">0</span>] <span class="op">=</span> x_t.copy()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        idx_mask <span class="op">=</span> np.where(x_t <span class="op">==</span> MASK)[<span class="dv">0</span>]  <span class="co"># masked indices</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_mask.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            u <span class="op">=</span> rng.random(idx_mask.size)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            unmask_now <span class="op">=</span> idx_mask[u <span class="op">&lt;</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]]  <span class="co"># indices that are going to be unmasked</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> unmask_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                cats <span class="op">=</span> rng.choice(<span class="dv">5</span>, size<span class="op">=</span>unmask_now.size, p<span class="op">=</span>p_data)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                x_t[unmask_now] <span class="op">=</span> cats</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        hist[T<span class="op">-</span>t<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> x_t.copy()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># but numerically we ensure no MASK remains:</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x_t <span class="op">!=</span> MASK), <span class="st">"Some samples remained MASK at t=0, which should not happen."</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_t, hist</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="a-note-on-alternative-sampling-strategies" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="a-note-on-alternative-sampling-strategies"><span class="header-section-number">2.2.1</span> A Note on Alternative Sampling Strategies</h4>
<p>Note that we need not to obey this exact backward transition kernel to sample from the data distribution.</p>
<p>For example, remasking <span class="citation" data-cites="Lezama+2023">(<a href="#ref-Lezama+2023" role="doc-biblioref">Lezama et al., 2023</a>)</span>, <span class="citation" data-cites="Zhao+2024">(<a href="#ref-Zhao+2024" role="doc-biblioref">S. Zhao et al., 2024</a>)</span>, <span class="citation" data-cites="Gat+2024">(<a href="#ref-Gat+2024" role="doc-biblioref">Gat et al., 2024</a>)</span>, <span class="citation" data-cites="Wang+2025">(<a href="#ref-Wang+2025" role="doc-biblioref">Wang et al., 2025</a>)</span>, a form of predictor-corrector sampling, can be incorporated to improve sample quality, mitigating numerical errors, as we will see in <a href="#sec-Predictor-Corrector" class="quarto-xref">Section&nbsp;3.5</a>.</p>
<p>Recently, sampling time path planning <span class="citation" data-cites="Peng+2025">(<a href="#ref-Peng+2025" role="doc-biblioref">Peng et al., 2025</a>)</span>, <span class="citation" data-cites="Liu+2025">(<a href="#ref-Liu+2025" role="doc-biblioref">Liu et al., 2025</a>)</span>, <span class="citation" data-cites="Kim+2025">(<a href="#ref-Kim+2025" role="doc-biblioref">Kim et al., 2025</a>)</span>, <span class="citation" data-cites="Rout+2025">(<a href="#ref-Rout+2025" role="doc-biblioref">Rout et al., 2025</a>)</span> has been proposed to improve sample quality and model log-likelihood, which lay out of the scope of this post.</p>
</section>
</section>
<section id="honest-sampling" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="honest-sampling"><span class="header-section-number">2.3</span> Honest Sampling</h3>
<div id="61b923bb" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100_000</span>  <span class="co"># size of sample to get</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>x0_samples, hist <span class="op">=</span> reverse_sample(N, p_unmask)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We first make sure our implementation is correct by checking the empirical distribution of the samples generated agrees with the true distribution.</p>
<div id="6e9e46e0" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.bincount(x0_samples, minlength<span class="op">=</span><span class="dv">5</span>).astype(<span class="bu">float</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>p_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Toy data marginal p_data:"</span>, p_data.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Empirical p after reverse sampling:"</span>, p_emp.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------- Bar chart: p_data vs empirical ----------</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.arange(<span class="dv">5</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">3</span>))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.bar(xs <span class="op">-</span> width<span class="op">/</span><span class="dv">2</span>, p_data, width<span class="op">=</span>width, label<span class="op">=</span><span class="st">"true p_data"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.bar(xs <span class="op">+</span> width<span class="op">/</span><span class="dv">2</span>, p_emp, width<span class="op">=</span>width, label<span class="op">=</span><span class="st">"empirical (reverse)"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Reverse samples match the data marginal"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"category id"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"probability"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Toy data marginal p_data: [0.4  0.3  0.18 0.1  0.02]
Empirical p after reverse sampling: [0.3979 0.3002 0.1828 0.0992 0.0198]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-9-output-2.png" width="566" height="278" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Perfect! Making sure everything is working, we plot 1000 sample paths from the reverse process.</p>
<div id="6f0d93db" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n_samples_to_plot <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1000</span>, hist.shape[<span class="dv">1</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples_to_plot):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(hist.shape[<span class="dv">0</span>]), hist[:, i], alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time step'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'State'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Sample trajectories (first </span><span class="sc">{</span>n_samples_to_plot<span class="sc">}</span><span class="ss"> samples)'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-10-output-1.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see a relatively equal number of jumps per step:</p>
<div id="1c649d70" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>jump_counts <span class="op">=</span> np.zeros(T)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    jump_counts[i] <span class="op">=</span> <span class="bu">sum</span>(hist[i] <span class="op">!=</span> hist[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(jump_counts)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[ 9916. 10017. 10001. 10143. 10062. 10195.  9815. 10005.  9859.  9987.]</code></pre>
</div>
</div>
<p>This is because we set <span class="math inline">\(\alpha_t=1-t\)</span> to be linear.</p>
<div id="8840ee4b" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------- Plot schedule α_t (survival probability) ----------</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">3</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(T<span class="op">+</span><span class="dv">1</span>), alpha, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Survival probability </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-12-output-1.png" width="470" height="277" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-exp" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="sec-exp"><span class="header-section-number">2.4</span> Choice of <span class="math inline">\(\alpha_t\)</span></h3>
<p><span class="math inline">\(\alpha_t\)</span> controls the convergence rate of the forward process.</p>
<p>We change <span class="math inline">\(\alpha_t\)</span> to see the impact on the sampling accuracy. (There should be no influence as long as the exact backward kernel is used.)</p>
<p>Let us change <span class="math inline">\(\alpha_t\)</span> to be an exponential schedule:</p>
<div id="4c7bea6e" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>alpha_exp <span class="op">=</span> np.exp(np.linspace(<span class="fl">0.00</span>, <span class="op">-</span><span class="fl">10.00</span>, T<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>p_unmask_exp <span class="op">=</span> (alpha_exp[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha_exp[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha_exp[<span class="dv">1</span>:])</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">3</span>))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(T<span class="op">+</span><span class="dv">1</span>), alpha_exp, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Survival probability </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-13-output-1.png" width="470" height="277" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In this way, most of the unmasking events should occur in the very last step of the reverse process.</p>
<div id="dc31a589" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>x0_exp, hist_exp <span class="op">=</span> reverse_sample(N, p_unmask_exp)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c9a9a740" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>n_samples_to_plot <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1000</span>, hist_exp.shape[<span class="dv">1</span>])</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples_to_plot):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(hist_exp.shape[<span class="dv">0</span>]), hist_exp[:, i], alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time step'</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'State'</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Sample trajectories (first </span><span class="sc">{</span>n_samples_to_plot<span class="sc">}</span><span class="ss"> samples)'</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-15-output-1.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see many jumps happen in the latter half.</p>
<p>Practically speaking, this is certainly not what we want.</p>
<p>We spend almost half of the computational time (up to the 6th step) in simulating the phantom jumps which just do not happen. The same concern was raised by <span class="citation" data-cites="Chao+2025">(<a href="#ref-Chao+2025" role="doc-biblioref">Chao et al., 2025</a>)</span>.</p>
<p>However, the accuracy is same, as the exact kernel is used to simulate, if the computational cost might be different.</p>
<div id="e0983a05" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_l1_kl(x0_samples, split <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> np.array_split(x0_samples, split)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> np.array([np.bincount(chunk, minlength<span class="op">=</span><span class="dv">5</span>).astype(<span class="bu">float</span>) <span class="cf">for</span> chunk <span class="kw">in</span> chunks])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    p_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    l1 <span class="op">=</span> np.<span class="bu">abs</span>(p_emp <span class="op">-</span> p_data).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    l1_var <span class="op">=</span> np.<span class="bu">abs</span>(p_emp <span class="op">-</span> p_data).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).var()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    kl <span class="op">=</span> (np.where(p_emp <span class="op">&gt;</span> <span class="dv">0</span>, p_emp <span class="op">*</span> np.log(p_emp <span class="op">/</span> p_data), <span class="dv">0</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    kl_var <span class="op">=</span> (np.where(p_emp <span class="op">&gt;</span> <span class="dv">0</span>, p_emp <span class="op">*</span> np.log(p_emp <span class="op">/</span> p_data), <span class="dv">0</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).var()</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> l1, l1_var, kl, kl_var</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>l1, l1_var, kl, kl_var <span class="op">=</span> calc_l1_kl(x0_samples)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Schedule: L1 distance:"</span>, <span class="bu">round</span>(l1, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(l1_var, <span class="dv">6</span>), <span class="st">"   KL(p_emp || p_data):"</span>, <span class="bu">round</span>(kl, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(kl_var, <span class="dv">6</span>))</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>l1_exp, l1_exp_var, kl_exp, kl_exp_var <span class="op">=</span> calc_l1_kl(x0_exp)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Exponential Schedule: L1 distance:"</span>, <span class="bu">round</span>(l1_exp, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(l1_exp_var, <span class="dv">6</span>), <span class="st">"   KL(p_emp || p_data):"</span>, <span class="bu">round</span>(kl_exp, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(kl_exp_var, <span class="dv">6</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Linear Schedule: L1 distance: 0.0147  ±  3e-05    KL(p_emp || p_data): 0.000196  ±  0.0
Exponential Schedule: L1 distance: 0.0148  ±  4.9e-05    KL(p_emp || p_data): 0.000232  ±  0.0</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-2d-example" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-2d-example"><span class="header-section-number">3</span> 2D Example</h2>
<section id="setup-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="setup-1"><span class="header-section-number">3.1</span> Setup</h3>
<p>We consider a highly correlated distribution, whose support is degenerated on the diagonal element on <span class="math inline">\(5^2\)</span>.</p>
<div id="486e4536" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>MASK <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Base marginal for a single site</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>p_single <span class="op">=</span> np.array([<span class="fl">0.40</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.10</span>, <span class="fl">0.02</span>], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>p_single <span class="op">/=</span> p_single.<span class="bu">sum</span>()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Build correlated joint with same-parity constraint</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">2</span>) <span class="op">==</span> (j <span class="op">%</span> <span class="dv">2</span>):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            W[i, j] <span class="op">=</span> p_single[i] <span class="op">*</span> p_single[j]</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>pi_joint <span class="op">=</span> W <span class="op">/</span> W.<span class="bu">sum</span>()</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>pi_x <span class="op">=</span> pi_joint.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>pi_y <span class="op">=</span> pi_joint.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditionals</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>cond_x_given_y <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)  <span class="co"># [j, i]</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>cond_y_given_x <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)  <span class="co"># [i, j]</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    col <span class="op">=</span> pi_joint[:, j]<span class="op">;</span> s <span class="op">=</span> col.<span class="bu">sum</span>()</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> s <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        cond_x_given_y[j, :] <span class="op">=</span> col <span class="op">/</span> s</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    row <span class="op">=</span> pi_joint[i, :]<span class="op">;</span> s <span class="op">=</span> row.<span class="bu">sum</span>()</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> s <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        cond_y_given_x[i, :] <span class="op">=</span> row <span class="op">/</span> s</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Heatmap</span></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax1.imshow(pi_joint, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Y'</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'X'</span>)</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Joint Probability Distribution (Heatmap)'</span>)</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>ax1.set_xticks(<span class="bu">range</span>(K))</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>ax1.set_yticks(<span class="bu">range</span>(K))</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Value annotation</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>        ax1.text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>ax1)</span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 3D bar plot</span></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(K)</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.arange(K)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> pi_joint</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>ax2.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), </span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>         <span class="fl">0.8</span>, <span class="fl">0.8</span>, Z.ravel(), alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Y'</span>)</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'X'</span>)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>ax2.set_zlabel(<span class="st">'Probability'</span>)</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Joint Probability Distribution (3D)'</span>)</span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>ax2.set_xticks(x)</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>ax2.set_yticks(y)</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-17-output-1.png" width="717" height="334" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-exact-kernel-2d" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-exact-kernel-2d"><span class="header-section-number">3.2</span> The Backward Transition Kernel</h3>
<p>We will first consider, again, linear schedule:</p>
<div id="d440566f" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="9cfadcb0" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(T<span class="op">+</span><span class="dv">1</span>), alpha, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Survival probability </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-19-output-1.png" width="470" height="277" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The code for backward sampling is basically the same, except for the number of <code>if</code> branch is now four, rather than just one.</p>
<div id="2cb71a29" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code (definition of reverse_sample_pairs)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample_pairs(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray, T: <span class="bu">int</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    hist1 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist1[<span class="dv">0</span>] <span class="op">=</span> x1</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    hist2 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist2[<span class="dv">0</span>] <span class="op">=</span> x2</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> pi_joint.ravel()</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>        hist1[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x1<span class="op">;</span> hist2[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x2</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x1 <span class="op">!=</span> MASK) <span class="kw">and</span> np.<span class="bu">all</span>(x2 <span class="op">!=</span> MASK)</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([x1, x2], axis<span class="op">=</span><span class="dv">1</span>), hist1, hist2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
</section>
<section id="honest-sampling-1" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="honest-sampling-1"><span class="header-section-number">3.3</span> Honest Sampling</h3>
<p>Again, using the exact backward kernel, we are able to reproduce the true joint distribution.</p>
<div id="dc739d61" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_pairs(N, p_unmask, T)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-21-output-1.png" width="736" height="321" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="885d3f91" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> l1_kl(pairs, split<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> np.array_split(pairs, split)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    l1, kl <span class="op">=</span> [], []</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> chunk <span class="kw">in</span> chunks:</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a, b <span class="kw">in</span> chunk:</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>            counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> <span class="fl">1e-12</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        l1.append(np.<span class="bu">abs</span>(pi_emp <span class="op">-</span> pi_joint).<span class="bu">sum</span>())</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        nz <span class="op">=</span> (pi_emp <span class="op">&gt;</span> <span class="dv">0</span>) <span class="op">&amp;</span> (pi_joint <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        kl.append((pi_emp[nz] <span class="op">*</span> np.log((pi_emp[nz] <span class="op">+</span> eps) <span class="op">/</span> pi_joint[nz])).<span class="bu">sum</span>())</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> l1, kl</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L1 distance: 0.024162  ±  4.6e-05    KL(emp || true): 7.286359e-04 ± 7.462192e-08</code></pre>
</div>
</div>
<p>Note that since the survival rate <span class="math inline">\(\alpha_t\)</span> decreases linearly, the number of newly unmasked coordinates per step will decrease exponentially.</p>
<div id="d1b0eb2a" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>new_unmasks_per_step <span class="op">=</span> []</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    changed1 <span class="op">=</span> (h1[t] <span class="op">==</span> MASK) <span class="op">&amp;</span> (h1[t<span class="op">+</span><span class="dv">1</span>] <span class="op">!=</span> MASK)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    changed2 <span class="op">=</span> (h2[t] <span class="op">==</span> MASK) <span class="op">&amp;</span> (h2[t<span class="op">+</span><span class="dv">1</span>] <span class="op">!=</span> MASK)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    new_unmasks_per_step.append(changed1.<span class="bu">sum</span>() <span class="op">+</span> changed2.<span class="bu">sum</span>())</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, T<span class="op">+</span><span class="dv">1</span>), new_unmasks_per_step, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Newly unmasked coordinates per step"</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"reverse step (t→t-1)"</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"#coords"</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-23-output-1.png" width="566" height="278" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="larger-step-size" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="larger-step-size"><span class="header-section-number">3.3.1</span> Larger Step Size</h4>
<p>What if we employ a large step size?</p>
<p>Actually, the result doesn’t change. Moreover, the accuracy is higher, since it is equivalent to direct sampling from <span class="math inline">\(\pi_{\text{data}}\)</span>.</p>
<div id="1276aa86" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Number of steps</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f0096506" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_pairs(N, p_unmask, T)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-25-output-1.png" width="736" height="321" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="1647e893" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L1 distance: 0.021245  ±  5e-05    KL(emp || true): 6.510835e-04 ± 1.258177e-07</code></pre>
</div>
</div>
</section>
</section>
<section id="coordinate-wise-sampling" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="coordinate-wise-sampling"><span class="header-section-number">3.4</span> Coordinate-wise Sampling</h3>
<p>The joint distribution would be unavailable, even if the learning based on the loss (<a href="#eq-L" class="quarto-xref">1</a>) were perfectly done, because of the product form assumption on the neural network predictor <span class="math inline">\(p_\theta\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>We mock this situation by replacing the joint distribution in the exact kernel, programmed in <a href="#sec-exact-kernel-2d" class="quarto-xref">Section&nbsp;3.2</a>, with the product of its marginals.</p>
<div id="05bbf1c8" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code (definition of reverse_sample_incorrect)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample_incorrect(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray, T: <span class="bu">int</span>):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    hist1 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist1[<span class="dv">0</span>] <span class="op">=</span> x1</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    hist2 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist2[<span class="dv">0</span>] <span class="op">=</span> x2</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> np.outer(pi_x, pi_y).ravel()</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb31-41"><a href="#cb31-41" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb31-42"><a href="#cb31-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-43"><a href="#cb31-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb31-44"><a href="#cb31-44" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb31-45"><a href="#cb31-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-46"><a href="#cb31-46" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb31-47"><a href="#cb31-47" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb31-48"><a href="#cb31-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb31-49"><a href="#cb31-49" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb31-50"><a href="#cb31-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb31-51"><a href="#cb31-51" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb31-52"><a href="#cb31-52" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb31-53"><a href="#cb31-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-54"><a href="#cb31-54" aria-hidden="true" tabindex="-1"></a>        hist1[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x1<span class="op">;</span> hist2[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x2</span>
<span id="cb31-55"><a href="#cb31-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-56"><a href="#cb31-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x1 <span class="op">!=</span> MASK) <span class="kw">and</span> np.<span class="bu">all</span>(x2 <span class="op">!=</span> MASK)</span>
<span id="cb31-57"><a href="#cb31-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([x1, x2], axis<span class="op">=</span><span class="dv">1</span>), hist1, hist2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="a43ce07c" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of steps</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_incorrect(N, p_unmask, T)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-28-output-1.png" width="736" height="321" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Then, this time the result is not so good.</p>
<div id="4a3901ca" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L1 distance: 0.089135  ±  1.2e-05    KL(emp || true): -4.100716e-02 ± 6.515707e-06</code></pre>
</div>
</div>
<p>There is a small deviation from the exact kernel, where <span class="math inline">\(\ell^1\)</span> distance was <span class="math inline">\(0.024\pm0.00005\)</span>.</p>
<p>We can easily see, in the empirical distribution, some cells are assinged with positive mass, although the true probability is <span class="math inline">\(0\)</span>.</p>
<p>This effect becomes smaller when the number of steps <code>T</code> is large. For example, setting <code>T=1000</code> gives us with almost same accuracy as the exact kernel (although we don’t show it here for brevity).</p>
<section id="larger-step-size-1" class="level4" data-number="3.4.1">
<h4 data-number="3.4.1" class="anchored" data-anchor-id="larger-step-size-1"><span class="header-section-number">3.4.1</span> Larger Step Size</h4>
<p>The situation gets worse when the step size is large, for example <code>T=1</code>.</p>
<div id="29032feb" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Number of steps</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_incorrect(N, p_unmask, T)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-30-output-1.png" width="740" height="321" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="aadd5a6c" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L1 distance: 0.85124  ±  4.4e-05    KL(emp || true): -2.884951e-01 ± 1.114368e-05</code></pre>
</div>
</div>
<p>The error is now significant, because the incorrect product kernel is used every time, as we set <span class="math inline">\(T=1\)</span> meaning unmasking in just one step!</p>
<p>One way to fix this is to set <code>T</code> as large as possible, making sure no more than one jump occurs simultaneously.</p>
</section>
</section>
<section id="sec-Predictor-Corrector" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-Predictor-Corrector"><span class="header-section-number">3.5</span> Corrector Sampling</h3>
<p>Another remedy is to utilise Gibbs sampling ideas to correct the bias, again at the expense of computational cost.</p>
<p>To do this, we must identify a Markov kernel that keeps the marginal distribution of <span class="math inline">\(X_t\)</span> invariant for every timestep <span class="math inline">\(t\in[0,1]\)</span>.</p>
<p>In our case, let us simply add a <em>re-masking</em> step, only to those pairs <span class="math inline">\((x_t^1,x_t^2)\)</span>’s which are completely unmasked.</p>
<p>As there is some probability of having been unmasked simultaneously, re-masking only one of them, this time <span class="math inline">\(x_t^1\)</span>, will allow us a second chance to arrive at a correct pair <span class="math inline">\((x_t^{1,\text{corrected}},x_t^2)\)</span>.</p>
<div id="891ea461" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Code (definition of reverse_sample_correct)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample_corrector(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray, T: <span class="bu">int</span>):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    hist1 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist1[<span class="dv">0</span>] <span class="op">=</span> x1</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    hist2 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist2[<span class="dv">0</span>] <span class="op">=</span> x2</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> np.outer(pi_x, pi_y).ravel()</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># corrector step</span></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> p  <span class="co"># masking probability</span></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">!=</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK)</span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> q</span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx[will]</span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>                x1[idx_now] <span class="op">=</span> MASK</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predictor step</span></span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> np.outer(pi_x, pi_y).ravel()</span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb38-77"><a href="#cb38-77" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb38-78"><a href="#cb38-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-87"><a href="#cb38-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb38-88"><a href="#cb38-88" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb38-89"><a href="#cb38-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-90"><a href="#cb38-90" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb38-91"><a href="#cb38-91" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb38-92"><a href="#cb38-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-93"><a href="#cb38-93" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb38-94"><a href="#cb38-94" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb38-95"><a href="#cb38-95" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb38-96"><a href="#cb38-96" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb38-97"><a href="#cb38-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-98"><a href="#cb38-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb38-99"><a href="#cb38-99" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb38-100"><a href="#cb38-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-101"><a href="#cb38-101" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb38-102"><a href="#cb38-102" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb38-103"><a href="#cb38-103" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb38-104"><a href="#cb38-104" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb38-105"><a href="#cb38-105" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb38-106"><a href="#cb38-106" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb38-107"><a href="#cb38-107" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb38-108"><a href="#cb38-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-109"><a href="#cb38-109" aria-hidden="true" tabindex="-1"></a>        hist1[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x1<span class="op">;</span> hist2[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x2</span>
<span id="cb38-110"><a href="#cb38-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-111"><a href="#cb38-111" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x1 <span class="op">!=</span> MASK) <span class="kw">and</span> np.<span class="bu">all</span>(x2 <span class="op">!=</span> MASK)</span>
<span id="cb38-112"><a href="#cb38-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([x1, x2], axis<span class="op">=</span><span class="dv">1</span>), hist1, hist2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div id="4f3c8d7c" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of steps</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_corrector(N, p_unmask, T)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="CTDDM_files/figure-html/cell-33-output-1.png" width="736" height="321" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="00bf15e1" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Code (tap me)</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs, <span class="dv">10</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L1 distance: 0.022518  ±  2e-05    KL(emp || true): 4.694469e-05 ± 8.500292e-08</code></pre>
</div>
</div>
<p>We see a small improvement from the <span class="math inline">\(\ell^1\)</span> distance of <span class="math inline">\(0.0241\)</span>. The difference is significant in terms of our variance of an order <span class="math inline">\(O(10^{-5})\)</span> culculated from <span class="math inline">\(10\)</span> repeated experiments.</p>
<p>This is the idea behind the predictor-corrector technique discussed, for example, in <span class="citation" data-cites="Gat+2024">(<a href="#ref-Gat+2024" role="doc-biblioref">Gat et al., 2024</a>)</span>, <span class="citation" data-cites="Zhao+2024">(<a href="#ref-Zhao+2024" role="doc-biblioref">S. Zhao et al., 2024</a>)</span>, <span class="citation" data-cites="Zhao+2024Unified">(<a href="#ref-Zhao+2024Unified" role="doc-biblioref">L. Zhao et al., 2024</a>)</span>.</p>
<p>We only included one corrector step per reverse step, although more correction will increase accuracy.</p>
<p>This can be regarded as a form of inference time compute scaling <span class="citation" data-cites="Wang+2025">(<a href="#ref-Wang+2025" role="doc-biblioref">Wang et al., 2025</a>)</span>, although it is a fairly bad idea to wait for the predictor-corrector steps to converge.</p>
</section>
<section id="sec-Chao" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="sec-Chao"><span class="header-section-number">3.6</span> Discussion</h3>
<p>Let us come back to the technique employed in <span class="citation" data-cites="Chao+2025">(<a href="#ref-Chao+2025" role="doc-biblioref">Chao et al., 2025</a>)</span>, which we mentioned in <a href="#sec-Intermediate-States" class="quarto-xref">Section&nbsp;1.4</a>.</p>
<p>As we have seen, the number of steps <span class="math inline">\(T\)</span> has to be kept larger than <span class="math inline">\(d\)</span>, to make sure no more than one jump occurs simultaneously.</p>
<p>However, this increases the computational cost, as more time steps must be spent in simulating phantom jumps.</p>
<p>One thing we can do is to fill this blank steps with a informed ‘half-jump’, by introducing a sub-structure in each state <span class="math inline">\(x\in E\)</span>.</p>
<p>This half-jump is a bit more robust than a direct unmasking. Therefore we are able to spend the time up to <span class="math inline">\(T\)</span> more meaningfully.</p>
<p>Thus, this strategy can be view as another way to mitigate the bias introduced by the incorrect backward kernel.</p>
</section>
</section>
<section id="future-works" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="future-works"><span class="header-section-number">4</span> Future Works</h2>
<p>Since the absorbing process is favoured only because of its time-agnostic property, its ability should be explained separately with the properties of the process.</p>
<section id="a-reinforcenment-learning-take" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="a-reinforcenment-learning-take"><span class="header-section-number">4.1</span> A Reinforcenment Learning Take</h3>
<p>The inference step and the sampling step should be decoupled, at least conceptually.</p>
<p>To tune the forward process noising schedule <span class="math inline">\(\alpha_t(x)\)</span>, a reinforcement learning framework will be employed, I believe in the near future.</p>
<p>This is a variant of meta-learning and, in this way, the unmasking network <span class="math inline">\(p_\theta\)</span> will be able to efficiently learn the correct dependence structure in the data domain.</p>
<p>For example, in language modeling, there is a natural sequential structure, which is partly why autoregressive modeling has been dominant in this domain. However, by learning <span class="math inline">\(\alpha_t\)</span> in masking process, a much more efficent factorization over texts can be aquired in a data-driven way.</p>
<p>I even think this <span class="math inline">\(\alpha_t\)</span> can play an important role just as a word embedding does currently.</p>
<p>In the sampling step, a sampling time path planning will greatly enhance sample quality, just as Monte Carlo tree search does in reinforcement learning.</p>
<p>As a conclusion, the flexible framework of masked diffusion models will enable a marriage with reinforcement learning and meta learning, which will be a way to overcome the autoregressive modeling framework, because the latter imposes unnecessary sequential inductive bias into the model.</p>
</section>
<section id="a-scaling-analysis" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="a-scaling-analysis"><span class="header-section-number">4.2</span> A Scaling Analysis</h3>
<p>There are two papers, namely <span class="citation" data-cites="Santos+2023">(<a href="#ref-Santos+2023" role="doc-biblioref">Santos et al., 2023</a>)</span> and <span class="citation" data-cites="Winkler+2024">(<a href="#ref-Winkler+2024" role="doc-biblioref">Winkler et al., 2024</a>)</span>, which carry out a scaling analysis as <span class="math inline">\(K\to\infty\)</span>, in order to bridge the gap between discrete and continuous state spaces.</p>
<p>In <span class="citation" data-cites="Santos+2023">(<a href="#ref-Santos+2023" role="doc-biblioref">Santos et al., 2023, sec. 2.4</a>)</span> and its Appendix C, it is proved that a fairly large class of discrete processes will converge to Ito processes, as <span class="math inline">\(K\to\infty\)</span>.</p>
<p>Their discussion based on a formal argument. They proves that the Kramers-Moyal expansion of the Chapman-Kolmogorov equation of the discrete process converges to that of a Ito process.</p>
<p>In other words, they didn’t identify the direct correspondence, for example deriving the exact expression of limiting SDEs.</p>
<p><span class="citation" data-cites="Winkler+2024">(<a href="#ref-Winkler+2024" role="doc-biblioref">Winkler et al., 2024</a>)</span> builds on their analysis, identifying an OU process on <span class="math inline">\(\mathbb{R}^d\)</span> corresponds to a Ehrenfest process.</p>
<p>This line of research has a lot to do with thermodynamics, and might provide insights into whether images should be modeled discretely or continuously.</p>
<p>Also, the limit <span class="math inline">\(d\to\infty\)</span> has yet to be explored.</p>
</section>
<section id="concluding-remarks" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="concluding-remarks"><span class="header-section-number">4.3</span> Concluding Remarks</h3>
<p>I believe that masked diffusion modeling can be viewed as a form of Gibbs sampling, with a learned transition kernel.</p>
<p>Many current practices are based on (uniformly) random scan Gibbs, while the autoregressive models are a fixed scan Gibbs.</p>
<p>Most recent improvements are based upon ideas from reinforcement learning and meta learning, where an optimal order to unmask components is pursued.</p>
<p>This point of view might not be only an abstract nonsense. I actually believe that this point of view will be fruitful in the future.</p>
</section>
</section>



<div id="quarto-appendix" class="default"><section id="関連記事" class="level2 appendix" data-number="5"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">5</span> 関連記事</h2><div class="quarto-appendix-contents">

<div id="listing-diffusion-listing" class="quarto-listing quarto-listing-container-grid">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="RGVlcCUyQ1NhbXBsaW5nJTJDTmF0dXJl" data-listing-date-sort="1723161600000" data-listing-file-modified-sort="1758544511567" data-listing-date-modified-sort="1751068800000" data-listing-reading-time-sort="3" data-listing-word-count-sort="490">
<a href="../../../posts/2025/DiffusionModels/DiscreteDiffusion.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Samplers/Files/best.gif" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
離散空間上の拡散確率モデル
</h5>
<div class="card-subtitle listing-subtitle">
位相構造を取り入れた次世代の構造生成へ
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-08-09
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="1" data-categories="RGVlcCUyQ1NhbXBsaW5nJTJDUHl0aG9u" data-listing-date-sort="1722556800000" data-listing-file-modified-sort="1758544509955" data-listing-date-modified-sort="1722816000000" data-listing-reading-time-sort="15" data-listing-word-count-sort="2810">
<a href="../../../posts/2024/Samplers/DDPM.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top"><img src="../../2024/Samplers/DDPM_files/figure-html/fig-encoding-output-1.png" style="height: 150px;"  class="thumbnail-image card-img"/></p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
拡散模型の実装
</h5>
<div class="card-subtitle listing-subtitle">
<code>PyTorch</code>によるハンズオン
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-08-02
</div>
</div>
</div>
</div></a>
</div>
<div class="g-col-1" data-index="2" data-categories="RGVlcCUyQ1Byb2Nlc3MlMkNTYW1wbGluZw==" data-listing-date-sort="1707868800000" data-listing-file-modified-sort="1758544509956" data-listing-date-modified-sort="1724371200000" data-listing-reading-time-sort="5" data-listing-word-count-sort="826">
<a href="../../../posts/2024/Samplers/Diffusion.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../../posts/2024/Samplers/Files/DDPM_outputs.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
拡散模型
</h5>
<div class="card-subtitle listing-subtitle">
深層生成モデル６
</div>
<div class="card-attribution card-text-small end">
<div class="listing-date">
2024-02-14
</div>
</div>
</div>
</div></a>
</div>
</div>
<div class="listing-no-matching d-none">No matching items</div>
</div>



<!-- -->


</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Arriola+2025" class="csl-entry" role="listitem">
Arriola, M., Sahoo, S. S., Gokaslan, A., Yang, Z., Qi, Z., Han, J., … Kuleshov, V. (2025). <a href="https://openreview.net/forum?id=tyEyYT267x">Block diffusion: Interpolating between autoregressive and diffusion language models</a>. In <em>The thirteenth international conference on learning representations</em>.
</div>
<div id="ref-Campbell+2022" class="csl-entry" role="listitem">
Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. (2022). <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b5b528767aa35f5b1a60fe0aaeca0563-Paper-Conference.pdf"><span class="nocase">A Continuous Time Framework for Discrete Denoising Models</span></a>. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, <em>Advances in neural information processing systems</em>,Vol. 35, pages 28266–28279. Curran Associates, Inc.
</div>
<div id="ref-Campbell+2024" class="csl-entry" role="listitem">
Campbell, A., Yim, J., Barzilay, R., Rainforth, T., and Jaakkola, T. (2024). <a href="https://arxiv.org/abs/2402.04997"><span class="nocase">Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design</span></a>.
</div>
<div id="ref-Chao+2025" class="csl-entry" role="listitem">
Chao, C.-H., Sun, W.-F., Liang, H., Lee, C.-Y., and Krishnan, R. G. (2025). <a href="https://arxiv.org/abs/2505.18495">Beyond masked and unmasked: Discrete diffusion models via partial masking</a>.
</div>
<div id="ref-Gat+2024" class="csl-entry" role="listitem">
Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T. Q., Synnaeve, G., … Lipman, Y. (2024). <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/f0d629a734b56a642701bba7bc8bb3ed-Paper-Conference.pdf">Discrete flow matching</a>. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, <em>Advances in neural information processing systems</em>,Vol. 37, pages 133345–133385. Curran Associates, Inc.
</div>
<div id="ref-Kim+2025" class="csl-entry" role="listitem">
Kim, J., Shah, K., Kontonis, V., Kakade, S. M., and Chen, S. (2025). <a href="https://openreview.net/forum?id=DjJmre5IkP">Train for the worst, plan for the best: Understanding token ordering in masked diffusions</a>. In <em>Forty-second international conference on machine learning</em>.
</div>
<div id="ref-Lezama+2023" class="csl-entry" role="listitem">
Lezama, J., Salimans, T., Jiang, L., Chang, H., Ho, J., and Essa, I. (2023). <a href="https://openreview.net/forum?id=VM8batVBWvg">Discrete predictor-corrector diffusion models for image synthesis</a>. In <em>The eleventh international conference on learning representations</em>.
</div>
<div id="ref-Liu+2025" class="csl-entry" role="listitem">
Liu, S., Nam, J., Campbell, A., Stark, H., Xu, Y., Jaakkola, T., and Gomez-Bombarelli, R. (2025). <a href="https://openreview.net/forum?id=MJNywBdSDy">Think while you generate: Discrete diffusion with planned denoising</a>. In <em>The thirteenth international conference on learning representations</em>.
</div>
<div id="ref-Ou+2025" class="csl-entry" role="listitem">
Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. (2025). <a href="https://openreview.net/forum?id=sMyXP8Tanm">Your absorbing discrete diffusion secretly models the conditional distributions of clean data</a>. In <em>The thirteenth international conference on learning representations</em>.
</div>
<div id="ref-Peng+2025" class="csl-entry" role="listitem">
Peng, F. Z., Bezemek, Z., Patel, S., Rector-Brooks, J., Yao, S., Tong, A., and Chatterjee, P. (2025). <a href="https://openreview.net/forum?id=fFuVPKpSt0">Path planning for masked diffusion models with applications to biological sequence generation</a>. In <em>ICLR 2025 workshop on deep generative model in machine learning: Theory, principle and efficacy</em>.
</div>
<div id="ref-Rout+2025" class="csl-entry" role="listitem">
Rout, L., Caramanis, C., and Shakkottai, S. (2025). <a href="https://arxiv.org/abs/2505.18456">Anchored diffusion language model</a>.
</div>
<div id="ref-Santos+2023" class="csl-entry" role="listitem">
Santos, J. E., Fox, Z. R., Lubbers, N., and Lin, Y. T. (2023). <a href="https://proceedings.mlr.press/v202/santos23a.html">Blackout diffusion: Generative diffusion models in discrete-state spaces</a>. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, <em>Proceedings of the 40th international conference on machine learning</em>,Vol. 202, pages 9034–9059. PMLR.
</div>
<div id="ref-Shaul+2025" class="csl-entry" role="listitem">
Shaul, N., Gat, I., Havasi, M., Severo, D., Sriram, A., Holderrieth, P., … Chen, R. T. Q. (2025). <a href="https://openreview.net/forum?id=tcvMzR2NrP">Flow matching with general discrete paths: A kinetic-optimal perspective</a>. In <em>The thirteenth international conference on learning representations</em>.
</div>
<div id="ref-Shi+2024" class="csl-entry" role="listitem">
Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. (2024). <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/bad233b9849f019aead5e5cc60cef70f-Paper-Conference.pdf">Simplified and generalized masked diffusion for discrete data</a>. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, <em>Advances in neural information processing systems</em>,Vol. 37, pages 103131–103167. Curran Associates, Inc.
</div>
<div id="ref-Wang+2025" class="csl-entry" role="listitem">
Wang, G., Schiff, Y., Sahoo, S. S., and Kuleshov, V. (2025). <a href="https://openreview.net/forum?id=xNwZ8kDC7T">Remasking discrete diffusion models with inference-time scaling</a>. In <em>ICLR 2025 workshop on deep generative model in machine learning: Theory, principle and efficacy</em>.
</div>
<div id="ref-Winkler+2024" class="csl-entry" role="listitem">
Winkler, L., Richter, L., and Opper, M. (2024). <a href="https://openreview.net/forum?id=8GYclcxQXB">Bridging discrete and continuous state spaces: Exploring the ehrenfest process in time-continuous diffusion models</a>. In <em>Forty-first international conference on machine learning</em>.
</div>
<div id="ref-Zhao+2024Unified" class="csl-entry" role="listitem">
Zhao, L., Ding, X., Yu, L., and Akoglu, L. (2024). <a href="https://arxiv.org/abs/2402.03701">Unified discrete diffusion for categorical data</a>.
</div>
<div id="ref-Zhao+2024" class="csl-entry" role="listitem">
Zhao, S., Brekelmans, R., Makhzani, A., and Grosse, R. (2024). <a href="https://arxiv.org/abs/2404.17546">Probabilistic inference in language models via twisted sequential monte carlo</a>. In.
</div>
<div id="ref-Zheng+2025" class="csl-entry" role="listitem">
Zheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and Zhang, Q. (2025). <a href="https://openreview.net/forum?id=CTC7CmirNr">Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling</a>. In <em>The thirteenth international conference on learning representations</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Discrete Flow Matching <span class="citation" data-cites="Campbell+2024">(<a href="#ref-Campbell+2024" role="doc-biblioref">Campbell et al., 2024</a>)</span>, <span class="citation" data-cites="Gat+2024">(<a href="#ref-Gat+2024" role="doc-biblioref">Gat et al., 2024</a>)</span>, <span class="citation" data-cites="Shaul+2025">(<a href="#ref-Shaul+2025" role="doc-biblioref">Shaul et al., 2025</a>)</span> and simplified Masked diffusion <span class="citation" data-cites="Shi+2024">(<a href="#ref-Shi+2024" role="doc-biblioref">Shi et al., 2024</a>)</span>, <span class="citation" data-cites="Ou+2025">(<a href="#ref-Ou+2025" role="doc-biblioref">Ou et al., 2025</a>)</span>, <span class="citation" data-cites="Zheng+2025">(<a href="#ref-Zheng+2025" role="doc-biblioref">Zheng et al., 2025</a>)</span> are different frameworks with different ranges, but both lead to the same training objective (<a href="#eq-L" class="quarto-xref">1</a>), when applied to the forward masking process.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In the context of language modeling, the perplexity is defined as <span class="math inline">\(2^{l}\)</span> where <span class="math inline">\(l\)</span> is the average log-likelihood of the test set.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Of course, the exact sampling would have been available, for exmple, if we learned the backward intensity as <span class="citation" data-cites="Campbell+2022">(<a href="#ref-Campbell+2022" role="doc-biblioref">Campbell et al., 2022</a>)</span>. However, these methods have been marginalized due to suboptimal performance.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/162348\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            trigger: 'click',
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            positionFixed: true,
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<script>
  function loadGiscus() {
    // Function to get the theme based on body class
    const getTheme = () => {
      let baseTheme = document.getElementById('giscus-base-theme').value;
      let altTheme = document.getElementById('giscus-alt-theme').value;
      return document.body.classList.contains('quarto-dark') ? altTheme : baseTheme;
    };
    const script = document.createElement("script");
    script.src = "https://giscus.app/client.js";
    script.async = true;
    script.dataset.repo = "162348/162348.github.io";
    script.dataset.repoId = "R_kgDOKlfKYQ";
    script.dataset.category = "Announcements";
    script.dataset.categoryId = "DIC_kwDOKlfKYc4CgDmb";
    script.dataset.mapping = "pathname";
    script.dataset.reactionsEnabled = "1";
    script.dataset.emitMetadata = "0";
    script.dataset.inputPosition = "top";
    script.dataset.theme = getTheme();
    script.dataset.lang = "en";
    script.crossOrigin = "anonymous";
    // Append the script to the desired div instead of at the end of the body
    document.getElementById("quarto-content").appendChild(script);
  }
  loadGiscus();
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb42" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Masked Diffusion Models"</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "A New Light to Discrete Data Modeling"</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Hirofumi Shiba"</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 9/15/2025</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co"># image: ../../2024/Samplers/Files/best.gif</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Denoising Model]</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> </span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - ../../../assets/2023.bib</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - ../../../assets/2024.bib</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - ../../../assets/2025.bib</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="an">csl:</span><span class="co"> ../../../assets/apalike.csl</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co">  Masked diffusion models are conceptually based upon an absorbing forward process and its reverse denoising process. However, their roles are intricately intertwined, in that all three aspects of training, sampling, and modeling are involved.</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co">  To develop our understanding, we give two toy examples, 1d and 2d, without training a neural network, to showcase how absorbing processes behave.</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co">  We identify core questions which should be investigated to expand our understanding.</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="an">listing:</span><span class="co"> </span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co">    -   id: diffusion-listing</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="co">        type: grid</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="co">        sort: false</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="co">        contents:</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="co">            - "DiscreteDiffusion.qmd"</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a><span class="co">            - "../../2024/Samplers/DDPM.qmd"</span></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="co">            - "../../2024/Samplers/Diffusion.qmd"</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="co">        date-format: iso</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a><span class="co">        fields: [title,image,date,subtitle]</span></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a><span class="co">    code-summary: Code (tap me)</span></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a><span class="co">  cache: false</span></span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../assets/_preamble.qmd &gt;}}</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Time-Agnostic Learning Framework</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>The absorbing process, a.k.a. masked diffusion, has a unique characteristic as a forward process in a discrete denoising diffusion model; it offers a time-agnostic *learning to unmask* training framework.</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a>When the state space is $E^d$ where $E=<span class="sc">\{</span>0,\cdots,K-1<span class="sc">\}</span>$ is finite, a current practice is to learn a neural network $p_\theta$ based on a loss given by</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>\cL(\theta):=\int^1_0\frac{\dot{\alpha}_t}{1-\alpha_t}\E\Square{\sum_{i=1}^d\log p_\theta(X_0^i|X_t)}\,dt,</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>$$ {#eq-L}</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>where $\alpha_t$ is a *noising schedule*, determining the convergence speed of the forward process.^<span class="co">[</span><span class="ot">Discrete Flow Matching [@Campbell+2024], [@Gat+2024], [@Shaul+2025] and simplified Masked diffusion [@Shi+2024], [@Ou+2025], [@Zheng+2025] are different frameworks with different ranges, but both lead to the same training objective ([-@eq-L]), when applied to the forward masking process.</span><span class="co">]</span></span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>The expectation in (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>) is exactly a cross-entropy loss. Therefore, the loss (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>) can be understood as a weighted cross-entropy loss, weighted by the noising schedule $\alpha_t$.</span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>Note that $p_\theta$ predicts the true state $x_0$, based on the current state $x_t$, some components of which might be masked. Hence we called this framework *learning to unmask*.</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>Note also that $p_\theta$ doesn't take $t$ as an argument <span class="co">[</span><span class="ot">@Shi+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Ou+2025</span><span class="co">]</span>, which we call the *time-agnostic* property, following <span class="co">[</span><span class="ot">@Zheng+2025</span><span class="co">]</span>.</span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choice of $\alpha_t$</span></span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>This 'learning to unmask' task might be very hard when $t$ is near $1$, since most of the $x_t^i$'s are still masked. </span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>For instance, if we choose a linear schedule</span>
<span id="cb42-61"><a href="#cb42-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-62"><a href="#cb42-62" aria-hidden="true" tabindex="-1"></a>\al_t=1-t\qquad(t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>)</span>
<span id="cb42-63"><a href="#cb42-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-64"><a href="#cb42-64" aria-hidden="true" tabindex="-1"></a>the scaler $\frac{\dot{\al}_t}{1-\al_t}=-t^{-1}$ before the expectation in (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>) puts less weight on large $t\approx1$, while puts much more weight on small $t\approx0$, where most of the $x_t^i$'s should be already unmasked.</span>
<span id="cb42-65"><a href="#cb42-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-66"><a href="#cb42-66" aria-hidden="true" tabindex="-1"></a>Hence, selecting the schedule $\al_t$ to make learning easier can be very effective, for example by reducing the variance of gradient estimator in a SGD algorithm. Actually, this is a part of the technique how <span class="co">[</span><span class="ot">@Arriola+2025</span><span class="co">]</span> achieved their remarkable empirical results.</span>
<span id="cb42-67"><a href="#cb42-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-68"><a href="#cb42-68" aria-hidden="true" tabindex="-1"></a>This flexibility of $\al_t$ is why the loss (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>) is considered as a potential competitor against the current dominant autoregressive models. In fact, one work <span class="co">[</span><span class="ot">@Chao+2025</span><span class="co">]</span>, still under review, claimed their masked diffusion model surpassed the autoregressive model on the task of language modeling, achieving an evaluation perplexity of 15.36 on the OpenWebText dataset.^<span class="co">[</span><span class="ot">In the context of language modeling, the perplexity is defined as $2^{l}$ where $l$ is the average log-likelihood of the test set.</span><span class="co">]</span></span>
<span id="cb42-69"><a href="#cb42-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-70"><a href="#cb42-70" aria-hidden="true" tabindex="-1"></a>We briefly discuss their trick and related promising techniques to improve the model, before programming toy examples in @sec-Demo and @sec-2d-example to deepen our understanding in absorbing forward process.</span>
<span id="cb42-71"><a href="#cb42-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-72"><a href="#cb42-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Gibbs Sampler Take {#sec-Gibbs-sampler-take}</span></span>
<span id="cb42-73"><a href="#cb42-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-74"><a href="#cb42-74" aria-hidden="true" tabindex="-1"></a>One problem about the loss (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>) is that $p_\theta$ predicts the unmasked complete sequence in a product form:^<span class="co">[</span><span class="ot">This has been a problem from [@Campbell+2022].</span><span class="co">]</span></span>
<span id="cb42-75"><a href="#cb42-75" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-76"><a href="#cb42-76" aria-hidden="true" tabindex="-1"></a>p_\theta(x_0|x_t)=\prod_{i=1}^d p_\theta(x_0^i|x_t).</span>
<span id="cb42-77"><a href="#cb42-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-78"><a href="#cb42-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-79"><a href="#cb42-79" aria-hidden="true" tabindex="-1"></a>This should cause no problem when we unmask one component at a time, since it will be a form of ancestral sampling based on disintegration property.</span>
<span id="cb42-80"><a href="#cb42-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-81"><a href="#cb42-81" aria-hidden="true" tabindex="-1"></a>However, when unmasking two or more components simultaneously, for example when the number of steps is less than $d$, the product form assumption will simply introduce a bias, as the data distribution is by no means of product form on $E^d$.</span>
<span id="cb42-82"><a href="#cb42-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-83"><a href="#cb42-83" aria-hidden="true" tabindex="-1"></a>Here, to recover correctness asymptotically, analogy with Gibbs sampling becomes very important.</span>
<span id="cb42-84"><a href="#cb42-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-85"><a href="#cb42-85" aria-hidden="true" tabindex="-1"></a>For example, predictor-corrector technique can be readily employed to mitigate this bias, as discussed in <span class="co">[</span><span class="ot">@Lezama+2023</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Zhao+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Zhao+2024Unified</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Gat+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Wang+2025</span><span class="co">]</span>.</span>
<span id="cb42-86"><a href="#cb42-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-87"><a href="#cb42-87" aria-hidden="true" tabindex="-1"></a>We demonstrate this strategy in @sec-Predictor-Corrector.</span>
<span id="cb42-88"><a href="#cb42-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-89"><a href="#cb42-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### Intermediate States {#sec-Intermediate-States}</span></span>
<span id="cb42-90"><a href="#cb42-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-91"><a href="#cb42-91" aria-hidden="true" tabindex="-1"></a>As we mentioned earlier, unmasking can be a very hard task, as closely investigated in <span class="co">[</span><span class="ot">@Kim+2025 Section 3</span><span class="co">]</span>.</span>
<span id="cb42-92"><a href="#cb42-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-93"><a href="#cb42-93" aria-hidden="true" tabindex="-1"></a>To alleviate this problem, <span class="co">[</span><span class="ot">@Chao+2025</span><span class="co">]</span> introduced intermediate states by re-encoding the token in a base-$2$ encoding, such as</span>
<span id="cb42-94"><a href="#cb42-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-95"><a href="#cb42-95" aria-hidden="true" tabindex="-1"></a>5\mapsto 101.</span>
<span id="cb42-96"><a href="#cb42-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-97"><a href="#cb42-97" aria-hidden="true" tabindex="-1"></a>The right-hand side needs three steps to be completely unmasked, while the left-hand side only needs one jump.</span>
<span id="cb42-98"><a href="#cb42-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-99"><a href="#cb42-99" aria-hidden="true" tabindex="-1"></a>Therefore, unmasking can be easier to learn, compared to the original token encoding.</span>
<span id="cb42-100"><a href="#cb42-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-101"><a href="#cb42-101" aria-hidden="true" tabindex="-1"></a>However, this is not the only advantage of intermediate states. <span class="co">[</span><span class="ot">@Chao+2025</span><span class="co">]</span> were able to construct a full predictor $p_\theta$ without the product form assumption on each token.</span>
<span id="cb42-102"><a href="#cb42-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-103"><a href="#cb42-103" aria-hidden="true" tabindex="-1"></a>This approach might act as a block Gibbs sampler and make the convergence faster, as we will discuss later in @sec-Chao.</span>
<span id="cb42-104"><a href="#cb42-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-105"><a href="#cb42-105" aria-hidden="true" tabindex="-1"></a><span class="fu">### State-Dependent Rate</span></span>
<span id="cb42-106"><a href="#cb42-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-107"><a href="#cb42-107" aria-hidden="true" tabindex="-1"></a>As a function of $t\mapsto\alpha_t$, different choices for $\al_t$ seem to make little impact on the total performance of the model, as we observe in our toy example in @sec-exp.</span>
<span id="cb42-108"><a href="#cb42-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-109"><a href="#cb42-109" aria-hidden="true" tabindex="-1"></a>However, if we allow $\al_t$ to depend on the state as in <span class="co">[</span><span class="ot">@Shi+2024 Section 6</span><span class="co">]</span>, I believe the masked diffusion model will start to show its real potential over currently dominant autoregressive framework.</span>
<span id="cb42-110"><a href="#cb42-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-111"><a href="#cb42-111" aria-hidden="true" tabindex="-1"></a>A problem arises when one tries to learn $\al_t$ at the same time, for example, by including a corresponding term into the loss (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>). This will lead to amplified variances of the gradient estimates and unstable training, as reported in <span class="co">[</span><span class="ot">@Shi+2024</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@Arriola+2025</span><span class="co">]</span>.</span>
<span id="cb42-112"><a href="#cb42-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-113"><a href="#cb42-113" aria-hidden="true" tabindex="-1"></a>The idea of exploiting state-dependent rate is already very common in sampling time <span class="co">[</span><span class="ot">@Peng+2025</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Liu+2025</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Kim+2025</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Rout+2025</span><span class="co">]</span>, determining which token to unmask next during the backward sampling, a bit like Monte Carlo tree search in reinforcement learning.</span>
<span id="cb42-114"><a href="#cb42-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-115"><a href="#cb42-115" aria-hidden="true" tabindex="-1"></a><span class="fu">## Demo {#sec-Demo}</span></span>
<span id="cb42-116"><a href="#cb42-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-117"><a href="#cb42-117" aria-hidden="true" tabindex="-1"></a>To demonstrate what an absorbing process does, we carry out generation from a toy data distribution $\pi_{\text{data}}$ on $5=<span class="sc">\{</span>0,1,2,3,4<span class="sc">\}</span>$, by running an exact reverse kernel of the absorbing (masked) forward process.</span>
<span id="cb42-118"><a href="#cb42-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-119"><a href="#cb42-119" aria-hidden="true" tabindex="-1"></a>Therefore, no neural network training will be involved. A 2d example, which is much more interesting, in @sec-2d-example will basically process in parallel.</span>
<span id="cb42-120"><a href="#cb42-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-121"><a href="#cb42-121" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setup</span></span>
<span id="cb42-122"><a href="#cb42-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-125"><a href="#cb42-125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-126"><a href="#cb42-126" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-127"><a href="#cb42-127" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb42-128"><a href="#cb42-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-129"><a href="#cb42-129" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb42-130"><a href="#cb42-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-131"><a href="#cb42-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-134"><a href="#cb42-134" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-135"><a href="#cb42-135" aria-hidden="true" tabindex="-1"></a>p_data <span class="op">=</span> np.array([<span class="fl">0.40</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.10</span>, <span class="fl">0.02</span>], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-136"><a href="#cb42-136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-137"><a href="#cb42-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-138"><a href="#cb42-138" aria-hidden="true" tabindex="-1"></a>We will represent the MASK as $-1$. The state space is then $E:=5 \cup <span class="sc">\{</span>-1<span class="sc">\}</span>$.</span>
<span id="cb42-139"><a href="#cb42-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-142"><a href="#cb42-142" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-143"><a href="#cb42-143" aria-hidden="true" tabindex="-1"></a>MASK <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb42-144"><a href="#cb42-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-145"><a href="#cb42-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-146"><a href="#cb42-146" aria-hidden="true" tabindex="-1"></a>An important design choice in the forward process is the noising schedule $\alpha_t$, which can be interpreted as *survival probability* and satisfy the following relatioship with the jump intensity $\beta_t$:</span>
<span id="cb42-147"><a href="#cb42-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-148"><a href="#cb42-148" aria-hidden="true" tabindex="-1"></a>\alpha_t=\exp\paren{-\int^t_0\beta_s\,ds},\qquad t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>.</span>
<span id="cb42-149"><a href="#cb42-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-150"><a href="#cb42-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-151"><a href="#cb42-151" aria-hidden="true" tabindex="-1"></a>Let us keep it simple and set $\alpha_t=t$. To achive this, we need to set</span>
<span id="cb42-152"><a href="#cb42-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-153"><a href="#cb42-153" aria-hidden="true" tabindex="-1"></a>\beta_t=\frac{1}{1-t},\qquad t\in[0,1),</span>
<span id="cb42-154"><a href="#cb42-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-155"><a href="#cb42-155" aria-hidden="true" tabindex="-1"></a>which is clearly diverging as $t\to1$. This is to ensure the process to converge in finite time.</span>
<span id="cb42-156"><a href="#cb42-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-159"><a href="#cb42-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-160"><a href="#cb42-160" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># number of steps</span></span>
<span id="cb42-161"><a href="#cb42-161" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.00</span>, <span class="fl">0.00</span>, T<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb42-162"><a href="#cb42-162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-163"><a href="#cb42-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-164"><a href="#cb42-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Backward Transition Kernel</span></span>
<span id="cb42-165"><a href="#cb42-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-166"><a href="#cb42-166" aria-hidden="true" tabindex="-1"></a>In this setting, the backward transition kernel $p(x_{t-1} | x_t)$ satisfies</span>
<span id="cb42-167"><a href="#cb42-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-168"><a href="#cb42-168" aria-hidden="true" tabindex="-1"></a>\P<span class="co">[</span><span class="ot">X_{t-1}=-1|X_t=-1</span><span class="co">]</span>=\frac{1 - \alpha_{t-1}}{1 - \alpha_t}.</span>
<span id="cb42-169"><a href="#cb42-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-170"><a href="#cb42-170" aria-hidden="true" tabindex="-1"></a>In the other cases, the unmasked values $x_{t-1}$ should be determined according to $\pi_{\text{data}}$, which is unavailable in a real setting, of course.</span>
<span id="cb42-171"><a href="#cb42-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-174"><a href="#cb42-174" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-175"><a href="#cb42-175" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])  <span class="co"># length T</span></span>
<span id="cb42-176"><a href="#cb42-176" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-177"><a href="#cb42-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-180"><a href="#cb42-180" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-181"><a href="#cb42-181" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray):</span>
<span id="cb42-182"><a href="#cb42-182" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb42-183"><a href="#cb42-183" aria-hidden="true" tabindex="-1"></a><span class="co">    Start from x_T = MASK for all samples, apply the exact reverse transitions down to t=0.</span></span>
<span id="cb42-184"><a href="#cb42-184" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns x_0 samples in 5 = {0,1,...,4}.</span></span>
<span id="cb42-185"><a href="#cb42-185" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb42-186"><a href="#cb42-186" aria-hidden="true" tabindex="-1"></a>    x_t <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-187"><a href="#cb42-187" aria-hidden="true" tabindex="-1"></a>    hist <span class="op">=</span> np.empty((T<span class="op">+</span><span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-188"><a href="#cb42-188" aria-hidden="true" tabindex="-1"></a>    hist[<span class="dv">0</span>] <span class="op">=</span> x_t.copy()</span>
<span id="cb42-189"><a href="#cb42-189" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb42-190"><a href="#cb42-190" aria-hidden="true" tabindex="-1"></a>        idx_mask <span class="op">=</span> np.where(x_t <span class="op">==</span> MASK)[<span class="dv">0</span>]  <span class="co"># masked indices</span></span>
<span id="cb42-191"><a href="#cb42-191" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_mask.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-192"><a href="#cb42-192" aria-hidden="true" tabindex="-1"></a>            u <span class="op">=</span> rng.random(idx_mask.size)</span>
<span id="cb42-193"><a href="#cb42-193" aria-hidden="true" tabindex="-1"></a>            unmask_now <span class="op">=</span> idx_mask[u <span class="op">&lt;</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]]  <span class="co"># indices that are going to be unmasked</span></span>
<span id="cb42-194"><a href="#cb42-194" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> unmask_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-195"><a href="#cb42-195" aria-hidden="true" tabindex="-1"></a>                cats <span class="op">=</span> rng.choice(<span class="dv">5</span>, size<span class="op">=</span>unmask_now.size, p<span class="op">=</span>p_data)</span>
<span id="cb42-196"><a href="#cb42-196" aria-hidden="true" tabindex="-1"></a>                x_t[unmask_now] <span class="op">=</span> cats</span>
<span id="cb42-197"><a href="#cb42-197" aria-hidden="true" tabindex="-1"></a>        hist[T<span class="op">-</span>t<span class="op">+</span><span class="dv">1</span>] <span class="op">=</span> x_t.copy()</span>
<span id="cb42-198"><a href="#cb42-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-199"><a href="#cb42-199" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At t=0, all remaining MASKs (if any) must have already unmasked earlier with probability 1,</span></span>
<span id="cb42-200"><a href="#cb42-200" aria-hidden="true" tabindex="-1"></a>    <span class="co"># but numerically we ensure no MASK remains:</span></span>
<span id="cb42-201"><a href="#cb42-201" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x_t <span class="op">!=</span> MASK), <span class="st">"Some samples remained MASK at t=0, which should not happen."</span></span>
<span id="cb42-202"><a href="#cb42-202" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_t, hist</span>
<span id="cb42-203"><a href="#cb42-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-204"><a href="#cb42-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-205"><a href="#cb42-205" aria-hidden="true" tabindex="-1"></a><span class="fu">#### A Note on Alternative Sampling Strategies</span></span>
<span id="cb42-206"><a href="#cb42-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-207"><a href="#cb42-207" aria-hidden="true" tabindex="-1"></a>Note that we need not to obey this exact backward transition kernel to sample from the data distribution.</span>
<span id="cb42-208"><a href="#cb42-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-209"><a href="#cb42-209" aria-hidden="true" tabindex="-1"></a>For example, remasking <span class="co">[</span><span class="ot">@Lezama+2023</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Zhao+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Gat+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Wang+2025</span><span class="co">]</span>, a form of predictor-corrector sampling, can be incorporated to improve sample quality, mitigating numerical errors, as we will see in @sec-Predictor-Corrector.</span>
<span id="cb42-210"><a href="#cb42-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-211"><a href="#cb42-211" aria-hidden="true" tabindex="-1"></a>Recently, sampling time path planning <span class="co">[</span><span class="ot">@Peng+2025</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Liu+2025</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Kim+2025</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Rout+2025</span><span class="co">]</span> has been proposed to improve sample quality and model log-likelihood, which lay out of the scope of this post.</span>
<span id="cb42-212"><a href="#cb42-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-213"><a href="#cb42-213" aria-hidden="true" tabindex="-1"></a><span class="fu">### Honest Sampling</span></span>
<span id="cb42-214"><a href="#cb42-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-217"><a href="#cb42-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-218"><a href="#cb42-218" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100_000</span>  <span class="co"># size of sample to get</span></span>
<span id="cb42-219"><a href="#cb42-219" aria-hidden="true" tabindex="-1"></a>x0_samples, hist <span class="op">=</span> reverse_sample(N, p_unmask)</span>
<span id="cb42-220"><a href="#cb42-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-221"><a href="#cb42-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-222"><a href="#cb42-222" aria-hidden="true" tabindex="-1"></a>We first make sure our implementation is correct by checking the empirical distribution of the samples generated agrees with the true distribution.</span>
<span id="cb42-223"><a href="#cb42-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-226"><a href="#cb42-226" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-227"><a href="#cb42-227" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-228"><a href="#cb42-228" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.bincount(x0_samples, minlength<span class="op">=</span><span class="dv">5</span>).astype(<span class="bu">float</span>)</span>
<span id="cb42-229"><a href="#cb42-229" aria-hidden="true" tabindex="-1"></a>p_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-230"><a href="#cb42-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-231"><a href="#cb42-231" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Toy data marginal p_data:"</span>, p_data.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb42-232"><a href="#cb42-232" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Empirical p after reverse sampling:"</span>, p_emp.<span class="bu">round</span>(<span class="dv">4</span>))</span>
<span id="cb42-233"><a href="#cb42-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-234"><a href="#cb42-234" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------- Bar chart: p_data vs empirical ----------</span></span>
<span id="cb42-235"><a href="#cb42-235" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.arange(<span class="dv">5</span>)</span>
<span id="cb42-236"><a href="#cb42-236" aria-hidden="true" tabindex="-1"></a>width <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb42-237"><a href="#cb42-237" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">3</span>))</span>
<span id="cb42-238"><a href="#cb42-238" aria-hidden="true" tabindex="-1"></a>plt.bar(xs <span class="op">-</span> width<span class="op">/</span><span class="dv">2</span>, p_data, width<span class="op">=</span>width, label<span class="op">=</span><span class="st">"true p_data"</span>)</span>
<span id="cb42-239"><a href="#cb42-239" aria-hidden="true" tabindex="-1"></a>plt.bar(xs <span class="op">+</span> width<span class="op">/</span><span class="dv">2</span>, p_emp, width<span class="op">=</span>width, label<span class="op">=</span><span class="st">"empirical (reverse)"</span>)</span>
<span id="cb42-240"><a href="#cb42-240" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Reverse samples match the data marginal"</span>)</span>
<span id="cb42-241"><a href="#cb42-241" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"category id"</span>)</span>
<span id="cb42-242"><a href="#cb42-242" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"probability"</span>)</span>
<span id="cb42-243"><a href="#cb42-243" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb42-244"><a href="#cb42-244" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-245"><a href="#cb42-245" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-246"><a href="#cb42-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-247"><a href="#cb42-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-248"><a href="#cb42-248" aria-hidden="true" tabindex="-1"></a>Perfect! Making sure everything is working, we plot 1000 sample paths from the reverse process.</span>
<span id="cb42-249"><a href="#cb42-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-252"><a href="#cb42-252" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-253"><a href="#cb42-253" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-254"><a href="#cb42-254" aria-hidden="true" tabindex="-1"></a>n_samples_to_plot <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1000</span>, hist.shape[<span class="dv">1</span>])</span>
<span id="cb42-255"><a href="#cb42-255" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb42-256"><a href="#cb42-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-257"><a href="#cb42-257" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples_to_plot):</span>
<span id="cb42-258"><a href="#cb42-258" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(hist.shape[<span class="dv">0</span>]), hist[:, i], alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb42-259"><a href="#cb42-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-260"><a href="#cb42-260" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time step'</span>)</span>
<span id="cb42-261"><a href="#cb42-261" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'State'</span>)</span>
<span id="cb42-262"><a href="#cb42-262" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Sample trajectories (first </span><span class="sc">{</span>n_samples_to_plot<span class="sc">}</span><span class="ss"> samples)'</span>)</span>
<span id="cb42-263"><a href="#cb42-263" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb42-264"><a href="#cb42-264" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-265"><a href="#cb42-265" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-266"><a href="#cb42-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-267"><a href="#cb42-267" aria-hidden="true" tabindex="-1"></a>We see a relatively equal number of jumps per step:</span>
<span id="cb42-268"><a href="#cb42-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-271"><a href="#cb42-271" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-272"><a href="#cb42-272" aria-hidden="true" tabindex="-1"></a>jump_counts <span class="op">=</span> np.zeros(T)</span>
<span id="cb42-273"><a href="#cb42-273" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb42-274"><a href="#cb42-274" aria-hidden="true" tabindex="-1"></a>    jump_counts[i] <span class="op">=</span> <span class="bu">sum</span>(hist[i] <span class="op">!=</span> hist[i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb42-275"><a href="#cb42-275" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(jump_counts)</span>
<span id="cb42-276"><a href="#cb42-276" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-277"><a href="#cb42-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-278"><a href="#cb42-278" aria-hidden="true" tabindex="-1"></a>This is because we set $\alpha_t=1-t$ to be linear.</span>
<span id="cb42-279"><a href="#cb42-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-282"><a href="#cb42-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-283"><a href="#cb42-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-284"><a href="#cb42-284" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------- Plot schedule α_t (survival probability) ----------</span></span>
<span id="cb42-285"><a href="#cb42-285" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">3</span>))</span>
<span id="cb42-286"><a href="#cb42-286" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(T<span class="op">+</span><span class="dv">1</span>), alpha, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb42-287"><a href="#cb42-287" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Survival probability </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb42-288"><a href="#cb42-288" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb42-289"><a href="#cb42-289" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb42-290"><a href="#cb42-290" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-291"><a href="#cb42-291" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-292"><a href="#cb42-292" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-293"><a href="#cb42-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-294"><a href="#cb42-294" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choice of $\alpha_t$ {#sec-exp}</span></span>
<span id="cb42-295"><a href="#cb42-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-296"><a href="#cb42-296" aria-hidden="true" tabindex="-1"></a>$\alpha_t$ controls the convergence rate of the forward process.</span>
<span id="cb42-297"><a href="#cb42-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-298"><a href="#cb42-298" aria-hidden="true" tabindex="-1"></a>We change $\al_t$ to see the impact on the sampling accuracy. (There should be no influence as long as the exact backward kernel is used.)</span>
<span id="cb42-299"><a href="#cb42-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-300"><a href="#cb42-300" aria-hidden="true" tabindex="-1"></a>Let us change $\al_t$ to be an exponential schedule:</span>
<span id="cb42-301"><a href="#cb42-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-304"><a href="#cb42-304" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-305"><a href="#cb42-305" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-306"><a href="#cb42-306" aria-hidden="true" tabindex="-1"></a>alpha_exp <span class="op">=</span> np.exp(np.linspace(<span class="fl">0.00</span>, <span class="op">-</span><span class="fl">10.00</span>, T<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb42-307"><a href="#cb42-307" aria-hidden="true" tabindex="-1"></a>p_unmask_exp <span class="op">=</span> (alpha_exp[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha_exp[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha_exp[<span class="dv">1</span>:])</span>
<span id="cb42-308"><a href="#cb42-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-309"><a href="#cb42-309" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">3</span>))</span>
<span id="cb42-310"><a href="#cb42-310" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(T<span class="op">+</span><span class="dv">1</span>), alpha_exp, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb42-311"><a href="#cb42-311" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Survival probability </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb42-312"><a href="#cb42-312" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb42-313"><a href="#cb42-313" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb42-314"><a href="#cb42-314" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-315"><a href="#cb42-315" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-316"><a href="#cb42-316" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-317"><a href="#cb42-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-318"><a href="#cb42-318" aria-hidden="true" tabindex="-1"></a>In this way, most of the unmasking events should occur in the very last step of the reverse process.</span>
<span id="cb42-319"><a href="#cb42-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-322"><a href="#cb42-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-323"><a href="#cb42-323" aria-hidden="true" tabindex="-1"></a>x0_exp, hist_exp <span class="op">=</span> reverse_sample(N, p_unmask_exp)</span>
<span id="cb42-324"><a href="#cb42-324" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-325"><a href="#cb42-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-328"><a href="#cb42-328" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-329"><a href="#cb42-329" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-330"><a href="#cb42-330" aria-hidden="true" tabindex="-1"></a>n_samples_to_plot <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1000</span>, hist_exp.shape[<span class="dv">1</span>])</span>
<span id="cb42-331"><a href="#cb42-331" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb42-332"><a href="#cb42-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-333"><a href="#cb42-333" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples_to_plot):</span>
<span id="cb42-334"><a href="#cb42-334" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(hist_exp.shape[<span class="dv">0</span>]), hist_exp[:, i], alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb42-335"><a href="#cb42-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-336"><a href="#cb42-336" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Time step'</span>)</span>
<span id="cb42-337"><a href="#cb42-337" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'State'</span>)</span>
<span id="cb42-338"><a href="#cb42-338" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Sample trajectories (first </span><span class="sc">{</span>n_samples_to_plot<span class="sc">}</span><span class="ss"> samples)'</span>)</span>
<span id="cb42-339"><a href="#cb42-339" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb42-340"><a href="#cb42-340" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-341"><a href="#cb42-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-342"><a href="#cb42-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-343"><a href="#cb42-343" aria-hidden="true" tabindex="-1"></a>We see many jumps happen in the latter half.</span>
<span id="cb42-344"><a href="#cb42-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-345"><a href="#cb42-345" aria-hidden="true" tabindex="-1"></a>Practically speaking, this is certainly not what we want.</span>
<span id="cb42-346"><a href="#cb42-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-347"><a href="#cb42-347" aria-hidden="true" tabindex="-1"></a>We spend almost half of the computational time (up to the 6th step) in simulating the phantom jumps which just do not happen. The same concern was raised by <span class="co">[</span><span class="ot">@Chao+2025</span><span class="co">]</span>.</span>
<span id="cb42-348"><a href="#cb42-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-349"><a href="#cb42-349" aria-hidden="true" tabindex="-1"></a>However, the accuracy is same, as the exact kernel is used to simulate, if the computational cost might be different.</span>
<span id="cb42-350"><a href="#cb42-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-353"><a href="#cb42-353" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-354"><a href="#cb42-354" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-355"><a href="#cb42-355" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_l1_kl(x0_samples, split <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb42-356"><a href="#cb42-356" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> np.array_split(x0_samples, split)</span>
<span id="cb42-357"><a href="#cb42-357" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> np.array([np.bincount(chunk, minlength<span class="op">=</span><span class="dv">5</span>).astype(<span class="bu">float</span>) <span class="cf">for</span> chunk <span class="kw">in</span> chunks])</span>
<span id="cb42-358"><a href="#cb42-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-359"><a href="#cb42-359" aria-hidden="true" tabindex="-1"></a>    p_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb42-360"><a href="#cb42-360" aria-hidden="true" tabindex="-1"></a>    l1 <span class="op">=</span> np.<span class="bu">abs</span>(p_emp <span class="op">-</span> p_data).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span>
<span id="cb42-361"><a href="#cb42-361" aria-hidden="true" tabindex="-1"></a>    l1_var <span class="op">=</span> np.<span class="bu">abs</span>(p_emp <span class="op">-</span> p_data).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).var()</span>
<span id="cb42-362"><a href="#cb42-362" aria-hidden="true" tabindex="-1"></a>    kl <span class="op">=</span> (np.where(p_emp <span class="op">&gt;</span> <span class="dv">0</span>, p_emp <span class="op">*</span> np.log(p_emp <span class="op">/</span> p_data), <span class="dv">0</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).mean()</span>
<span id="cb42-363"><a href="#cb42-363" aria-hidden="true" tabindex="-1"></a>    kl_var <span class="op">=</span> (np.where(p_emp <span class="op">&gt;</span> <span class="dv">0</span>, p_emp <span class="op">*</span> np.log(p_emp <span class="op">/</span> p_data), <span class="dv">0</span>)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>).var()</span>
<span id="cb42-364"><a href="#cb42-364" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> l1, l1_var, kl, kl_var</span>
<span id="cb42-365"><a href="#cb42-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-366"><a href="#cb42-366" aria-hidden="true" tabindex="-1"></a>l1, l1_var, kl, kl_var <span class="op">=</span> calc_l1_kl(x0_samples)</span>
<span id="cb42-367"><a href="#cb42-367" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Linear Schedule: L1 distance:"</span>, <span class="bu">round</span>(l1, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(l1_var, <span class="dv">6</span>), <span class="st">"   KL(p_emp || p_data):"</span>, <span class="bu">round</span>(kl, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(kl_var, <span class="dv">6</span>))</span>
<span id="cb42-368"><a href="#cb42-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-369"><a href="#cb42-369" aria-hidden="true" tabindex="-1"></a>l1_exp, l1_exp_var, kl_exp, kl_exp_var <span class="op">=</span> calc_l1_kl(x0_exp)</span>
<span id="cb42-370"><a href="#cb42-370" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Exponential Schedule: L1 distance:"</span>, <span class="bu">round</span>(l1_exp, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(l1_exp_var, <span class="dv">6</span>), <span class="st">"   KL(p_emp || p_data):"</span>, <span class="bu">round</span>(kl_exp, <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(kl_exp_var, <span class="dv">6</span>))</span>
<span id="cb42-371"><a href="#cb42-371" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-372"><a href="#cb42-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-373"><a href="#cb42-373" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2D Example {#sec-2d-example}</span></span>
<span id="cb42-374"><a href="#cb42-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-375"><a href="#cb42-375" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setup</span></span>
<span id="cb42-376"><a href="#cb42-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-377"><a href="#cb42-377" aria-hidden="true" tabindex="-1"></a>We consider a highly correlated distribution, whose support is degenerated on the diagonal element on $5^2$.</span>
<span id="cb42-378"><a href="#cb42-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-381"><a href="#cb42-381" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-382"><a href="#cb42-382" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-383"><a href="#cb42-383" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb42-384"><a href="#cb42-384" aria-hidden="true" tabindex="-1"></a>MASK <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb42-385"><a href="#cb42-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-386"><a href="#cb42-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Base marginal for a single site</span></span>
<span id="cb42-387"><a href="#cb42-387" aria-hidden="true" tabindex="-1"></a>p_single <span class="op">=</span> np.array([<span class="fl">0.40</span>, <span class="fl">0.30</span>, <span class="fl">0.18</span>, <span class="fl">0.10</span>, <span class="fl">0.02</span>], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-388"><a href="#cb42-388" aria-hidden="true" tabindex="-1"></a>p_single <span class="op">/=</span> p_single.<span class="bu">sum</span>()</span>
<span id="cb42-389"><a href="#cb42-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-390"><a href="#cb42-390" aria-hidden="true" tabindex="-1"></a><span class="co"># Build correlated joint with same-parity constraint</span></span>
<span id="cb42-391"><a href="#cb42-391" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-392"><a href="#cb42-392" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-393"><a href="#cb42-393" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-394"><a href="#cb42-394" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">2</span>) <span class="op">==</span> (j <span class="op">%</span> <span class="dv">2</span>):</span>
<span id="cb42-395"><a href="#cb42-395" aria-hidden="true" tabindex="-1"></a>            W[i, j] <span class="op">=</span> p_single[i] <span class="op">*</span> p_single[j]</span>
<span id="cb42-396"><a href="#cb42-396" aria-hidden="true" tabindex="-1"></a>pi_joint <span class="op">=</span> W <span class="op">/</span> W.<span class="bu">sum</span>()</span>
<span id="cb42-397"><a href="#cb42-397" aria-hidden="true" tabindex="-1"></a>pi_x <span class="op">=</span> pi_joint.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-398"><a href="#cb42-398" aria-hidden="true" tabindex="-1"></a>pi_y <span class="op">=</span> pi_joint.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-399"><a href="#cb42-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-400"><a href="#cb42-400" aria-hidden="true" tabindex="-1"></a><span class="co"># Conditionals</span></span>
<span id="cb42-401"><a href="#cb42-401" aria-hidden="true" tabindex="-1"></a>cond_x_given_y <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)  <span class="co"># [j, i]</span></span>
<span id="cb42-402"><a href="#cb42-402" aria-hidden="true" tabindex="-1"></a>cond_y_given_x <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)  <span class="co"># [i, j]</span></span>
<span id="cb42-403"><a href="#cb42-403" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-404"><a href="#cb42-404" aria-hidden="true" tabindex="-1"></a>    col <span class="op">=</span> pi_joint[:, j]<span class="op">;</span> s <span class="op">=</span> col.<span class="bu">sum</span>()</span>
<span id="cb42-405"><a href="#cb42-405" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> s <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-406"><a href="#cb42-406" aria-hidden="true" tabindex="-1"></a>        cond_x_given_y[j, :] <span class="op">=</span> col <span class="op">/</span> s</span>
<span id="cb42-407"><a href="#cb42-407" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-408"><a href="#cb42-408" aria-hidden="true" tabindex="-1"></a>    row <span class="op">=</span> pi_joint[i, :]<span class="op">;</span> s <span class="op">=</span> row.<span class="bu">sum</span>()</span>
<span id="cb42-409"><a href="#cb42-409" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> s <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-410"><a href="#cb42-410" aria-hidden="true" tabindex="-1"></a>        cond_y_given_x[i, :] <span class="op">=</span> row <span class="op">/</span> s</span>
<span id="cb42-411"><a href="#cb42-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-412"><a href="#cb42-412" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb42-413"><a href="#cb42-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-414"><a href="#cb42-414" aria-hidden="true" tabindex="-1"></a><span class="co"># Heatmap</span></span>
<span id="cb42-415"><a href="#cb42-415" aria-hidden="true" tabindex="-1"></a>ax1 <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb42-416"><a href="#cb42-416" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax1.imshow(pi_joint, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>)</span>
<span id="cb42-417"><a href="#cb42-417" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Y'</span>)</span>
<span id="cb42-418"><a href="#cb42-418" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'X'</span>)</span>
<span id="cb42-419"><a href="#cb42-419" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Joint Probability Distribution (Heatmap)'</span>)</span>
<span id="cb42-420"><a href="#cb42-420" aria-hidden="true" tabindex="-1"></a>ax1.set_xticks(<span class="bu">range</span>(K))</span>
<span id="cb42-421"><a href="#cb42-421" aria-hidden="true" tabindex="-1"></a>ax1.set_yticks(<span class="bu">range</span>(K))</span>
<span id="cb42-422"><a href="#cb42-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-423"><a href="#cb42-423" aria-hidden="true" tabindex="-1"></a><span class="co"># Value annotation</span></span>
<span id="cb42-424"><a href="#cb42-424" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-425"><a href="#cb42-425" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-426"><a href="#cb42-426" aria-hidden="true" tabindex="-1"></a>        ax1.text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-427"><a href="#cb42-427" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-428"><a href="#cb42-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-429"><a href="#cb42-429" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>ax1)</span>
<span id="cb42-430"><a href="#cb42-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-431"><a href="#cb42-431" aria-hidden="true" tabindex="-1"></a><span class="co"># 3D bar plot</span></span>
<span id="cb42-432"><a href="#cb42-432" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb42-433"><a href="#cb42-433" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(K)</span>
<span id="cb42-434"><a href="#cb42-434" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.arange(K)</span>
<span id="cb42-435"><a href="#cb42-435" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb42-436"><a href="#cb42-436" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> pi_joint</span>
<span id="cb42-437"><a href="#cb42-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-438"><a href="#cb42-438" aria-hidden="true" tabindex="-1"></a>ax2.bar3d(X.ravel(), Y.ravel(), np.zeros_like(Z.ravel()), </span>
<span id="cb42-439"><a href="#cb42-439" aria-hidden="true" tabindex="-1"></a>         <span class="fl">0.8</span>, <span class="fl">0.8</span>, Z.ravel(), alpha<span class="op">=</span><span class="fl">0.8</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb42-440"><a href="#cb42-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-441"><a href="#cb42-441" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Y'</span>)</span>
<span id="cb42-442"><a href="#cb42-442" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'X'</span>)</span>
<span id="cb42-443"><a href="#cb42-443" aria-hidden="true" tabindex="-1"></a>ax2.set_zlabel(<span class="st">'Probability'</span>)</span>
<span id="cb42-444"><a href="#cb42-444" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Joint Probability Distribution (3D)'</span>)</span>
<span id="cb42-445"><a href="#cb42-445" aria-hidden="true" tabindex="-1"></a>ax2.set_xticks(x)</span>
<span id="cb42-446"><a href="#cb42-446" aria-hidden="true" tabindex="-1"></a>ax2.set_yticks(y)</span>
<span id="cb42-447"><a href="#cb42-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-448"><a href="#cb42-448" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-449"><a href="#cb42-449" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-450"><a href="#cb42-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-451"><a href="#cb42-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-452"><a href="#cb42-452" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Backward Transition Kernel {#sec-exact-kernel-2d}</span></span>
<span id="cb42-453"><a href="#cb42-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-454"><a href="#cb42-454" aria-hidden="true" tabindex="-1"></a>We will first consider, again, linear schedule:</span>
<span id="cb42-455"><a href="#cb42-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-458"><a href="#cb42-458" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-459"><a href="#cb42-459" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb42-460"><a href="#cb42-460" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb42-461"><a href="#cb42-461" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb42-462"><a href="#cb42-462" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb42-463"><a href="#cb42-463" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-464"><a href="#cb42-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-467"><a href="#cb42-467" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-468"><a href="#cb42-468" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-469"><a href="#cb42-469" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb42-470"><a href="#cb42-470" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(T<span class="op">+</span><span class="dv">1</span>), alpha, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb42-471"><a href="#cb42-471" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="vs">r"Survival probability </span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb42-472"><a href="#cb42-472" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"t"</span>)</span>
<span id="cb42-473"><a href="#cb42-473" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\a</span><span class="vs">lpha_t</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb42-474"><a href="#cb42-474" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-475"><a href="#cb42-475" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-476"><a href="#cb42-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-477"><a href="#cb42-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-478"><a href="#cb42-478" aria-hidden="true" tabindex="-1"></a>The code for backward sampling is basically the same, except for the number of <span class="in">`if`</span> branch is now four, rather than just one.</span>
<span id="cb42-479"><a href="#cb42-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-482"><a href="#cb42-482" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-483"><a href="#cb42-483" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-484"><a href="#cb42-484" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: Code (definition of reverse_sample_pairs)</span></span>
<span id="cb42-485"><a href="#cb42-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-486"><a href="#cb42-486" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample_pairs(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray, T: <span class="bu">int</span>):</span>
<span id="cb42-487"><a href="#cb42-487" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-488"><a href="#cb42-488" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-489"><a href="#cb42-489" aria-hidden="true" tabindex="-1"></a>    hist1 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist1[<span class="dv">0</span>] <span class="op">=</span> x1</span>
<span id="cb42-490"><a href="#cb42-490" aria-hidden="true" tabindex="-1"></a>    hist2 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist2[<span class="dv">0</span>] <span class="op">=</span> x2</span>
<span id="cb42-491"><a href="#cb42-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-492"><a href="#cb42-492" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb42-493"><a href="#cb42-493" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-494"><a href="#cb42-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-495"><a href="#cb42-495" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb42-496"><a href="#cb42-496" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb42-497"><a href="#cb42-497" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb42-498"><a href="#cb42-498" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-499"><a href="#cb42-499" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-500"><a href="#cb42-500" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-501"><a href="#cb42-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-502"><a href="#cb42-502" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb42-503"><a href="#cb42-503" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-504"><a href="#cb42-504" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> pi_joint.ravel()</span>
<span id="cb42-505"><a href="#cb42-505" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb42-506"><a href="#cb42-506" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb42-507"><a href="#cb42-507" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb42-508"><a href="#cb42-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-509"><a href="#cb42-509" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb42-510"><a href="#cb42-510" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-511"><a href="#cb42-511" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb42-512"><a href="#cb42-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-513"><a href="#cb42-513" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb42-514"><a href="#cb42-514" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-515"><a href="#cb42-515" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb42-516"><a href="#cb42-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-517"><a href="#cb42-517" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb42-518"><a href="#cb42-518" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-519"><a href="#cb42-519" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-520"><a href="#cb42-520" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb42-521"><a href="#cb42-521" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb42-522"><a href="#cb42-522" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-523"><a href="#cb42-523" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb42-524"><a href="#cb42-524" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb42-525"><a href="#cb42-525" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-526"><a href="#cb42-526" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb42-527"><a href="#cb42-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-528"><a href="#cb42-528" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb42-529"><a href="#cb42-529" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-530"><a href="#cb42-530" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-531"><a href="#cb42-531" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb42-532"><a href="#cb42-532" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb42-533"><a href="#cb42-533" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-534"><a href="#cb42-534" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb42-535"><a href="#cb42-535" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb42-536"><a href="#cb42-536" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-537"><a href="#cb42-537" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb42-538"><a href="#cb42-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-539"><a href="#cb42-539" aria-hidden="true" tabindex="-1"></a>        hist1[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x1<span class="op">;</span> hist2[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x2</span>
<span id="cb42-540"><a href="#cb42-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-541"><a href="#cb42-541" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x1 <span class="op">!=</span> MASK) <span class="kw">and</span> np.<span class="bu">all</span>(x2 <span class="op">!=</span> MASK)</span>
<span id="cb42-542"><a href="#cb42-542" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([x1, x2], axis<span class="op">=</span><span class="dv">1</span>), hist1, hist2</span>
<span id="cb42-543"><a href="#cb42-543" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-544"><a href="#cb42-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-545"><a href="#cb42-545" aria-hidden="true" tabindex="-1"></a><span class="fu">### Honest Sampling</span></span>
<span id="cb42-546"><a href="#cb42-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-547"><a href="#cb42-547" aria-hidden="true" tabindex="-1"></a>Again, using the exact backward kernel, we are able to reproduce the true joint distribution.</span>
<span id="cb42-548"><a href="#cb42-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-551"><a href="#cb42-551" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-552"><a href="#cb42-552" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-553"><a href="#cb42-553" aria-hidden="true" tabindex="-1"></a><span class="co"># Run</span></span>
<span id="cb42-554"><a href="#cb42-554" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb42-555"><a href="#cb42-555" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_pairs(N, p_unmask, T)</span>
<span id="cb42-556"><a href="#cb42-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-557"><a href="#cb42-557" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb42-558"><a href="#cb42-558" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-559"><a href="#cb42-559" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb42-560"><a href="#cb42-560" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb42-561"><a href="#cb42-561" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-562"><a href="#cb42-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-563"><a href="#cb42-563" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb42-564"><a href="#cb42-564" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-565"><a href="#cb42-565" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb42-566"><a href="#cb42-566" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-567"><a href="#cb42-567" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-568"><a href="#cb42-568" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-569"><a href="#cb42-569" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-570"><a href="#cb42-570" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-571"><a href="#cb42-571" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-572"><a href="#cb42-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-573"><a href="#cb42-573" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-574"><a href="#cb42-574" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb42-575"><a href="#cb42-575" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-576"><a href="#cb42-576" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-577"><a href="#cb42-577" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-578"><a href="#cb42-578" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-579"><a href="#cb42-579" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-580"><a href="#cb42-580" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-581"><a href="#cb42-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-582"><a href="#cb42-582" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-583"><a href="#cb42-583" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-584"><a href="#cb42-584" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-585"><a href="#cb42-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-586"><a href="#cb42-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-589"><a href="#cb42-589" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-590"><a href="#cb42-590" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-591"><a href="#cb42-591" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> l1_kl(pairs, split<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb42-592"><a href="#cb42-592" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> np.array_split(pairs, split)</span>
<span id="cb42-593"><a href="#cb42-593" aria-hidden="true" tabindex="-1"></a>    l1, kl <span class="op">=</span> [], []</span>
<span id="cb42-594"><a href="#cb42-594" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> chunk <span class="kw">in</span> chunks:</span>
<span id="cb42-595"><a href="#cb42-595" aria-hidden="true" tabindex="-1"></a>        counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-596"><a href="#cb42-596" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a, b <span class="kw">in</span> chunk:</span>
<span id="cb42-597"><a href="#cb42-597" aria-hidden="true" tabindex="-1"></a>            counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb42-598"><a href="#cb42-598" aria-hidden="true" tabindex="-1"></a>        pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-599"><a href="#cb42-599" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-600"><a href="#cb42-600" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> <span class="fl">1e-12</span></span>
<span id="cb42-601"><a href="#cb42-601" aria-hidden="true" tabindex="-1"></a>        l1.append(np.<span class="bu">abs</span>(pi_emp <span class="op">-</span> pi_joint).<span class="bu">sum</span>())</span>
<span id="cb42-602"><a href="#cb42-602" aria-hidden="true" tabindex="-1"></a>        nz <span class="op">=</span> (pi_emp <span class="op">&gt;</span> <span class="dv">0</span>) <span class="op">&amp;</span> (pi_joint <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb42-603"><a href="#cb42-603" aria-hidden="true" tabindex="-1"></a>        kl.append((pi_emp[nz] <span class="op">*</span> np.log((pi_emp[nz] <span class="op">+</span> eps) <span class="op">/</span> pi_joint[nz])).<span class="bu">sum</span>())</span>
<span id="cb42-604"><a href="#cb42-604" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> l1, kl</span>
<span id="cb42-605"><a href="#cb42-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-606"><a href="#cb42-606" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb42-607"><a href="#cb42-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-608"><a href="#cb42-608" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span>
<span id="cb42-609"><a href="#cb42-609" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-610"><a href="#cb42-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-611"><a href="#cb42-611" aria-hidden="true" tabindex="-1"></a>Note that since the survival rate $\al_t$ decreases linearly, the number of newly unmasked coordinates per step will decrease exponentially.</span>
<span id="cb42-612"><a href="#cb42-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-615"><a href="#cb42-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-616"><a href="#cb42-616" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-617"><a href="#cb42-617" aria-hidden="true" tabindex="-1"></a>new_unmasks_per_step <span class="op">=</span> []</span>
<span id="cb42-618"><a href="#cb42-618" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb42-619"><a href="#cb42-619" aria-hidden="true" tabindex="-1"></a>    changed1 <span class="op">=</span> (h1[t] <span class="op">==</span> MASK) <span class="op">&amp;</span> (h1[t<span class="op">+</span><span class="dv">1</span>] <span class="op">!=</span> MASK)</span>
<span id="cb42-620"><a href="#cb42-620" aria-hidden="true" tabindex="-1"></a>    changed2 <span class="op">=</span> (h2[t] <span class="op">==</span> MASK) <span class="op">&amp;</span> (h2[t<span class="op">+</span><span class="dv">1</span>] <span class="op">!=</span> MASK)</span>
<span id="cb42-621"><a href="#cb42-621" aria-hidden="true" tabindex="-1"></a>    new_unmasks_per_step.append(changed1.<span class="bu">sum</span>() <span class="op">+</span> changed2.<span class="bu">sum</span>())</span>
<span id="cb42-622"><a href="#cb42-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-623"><a href="#cb42-623" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb42-624"><a href="#cb42-624" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, T<span class="op">+</span><span class="dv">1</span>), new_unmasks_per_step, marker<span class="op">=</span><span class="st">"o"</span>)</span>
<span id="cb42-625"><a href="#cb42-625" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Newly unmasked coordinates per step"</span>)</span>
<span id="cb42-626"><a href="#cb42-626" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"reverse step (t→t-1)"</span>)</span>
<span id="cb42-627"><a href="#cb42-627" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"#coords"</span>)</span>
<span id="cb42-628"><a href="#cb42-628" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-629"><a href="#cb42-629" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-630"><a href="#cb42-630" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-631"><a href="#cb42-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-632"><a href="#cb42-632" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Larger Step Size</span></span>
<span id="cb42-633"><a href="#cb42-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-634"><a href="#cb42-634" aria-hidden="true" tabindex="-1"></a>What if we employ a large step size?</span>
<span id="cb42-635"><a href="#cb42-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-636"><a href="#cb42-636" aria-hidden="true" tabindex="-1"></a>Actually, the result doesn't change. Moreover, the accuracy is higher, since it is equivalent to direct sampling from $\pi_{\text{data}}$.</span>
<span id="cb42-637"><a href="#cb42-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-640"><a href="#cb42-640" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-641"><a href="#cb42-641" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Number of steps</span></span>
<span id="cb42-642"><a href="#cb42-642" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb42-643"><a href="#cb42-643" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb42-644"><a href="#cb42-644" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb42-645"><a href="#cb42-645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-646"><a href="#cb42-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-649"><a href="#cb42-649" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-650"><a href="#cb42-650" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-651"><a href="#cb42-651" aria-hidden="true" tabindex="-1"></a><span class="co"># Run</span></span>
<span id="cb42-652"><a href="#cb42-652" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb42-653"><a href="#cb42-653" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_pairs(N, p_unmask, T)</span>
<span id="cb42-654"><a href="#cb42-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-655"><a href="#cb42-655" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb42-656"><a href="#cb42-656" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-657"><a href="#cb42-657" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb42-658"><a href="#cb42-658" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb42-659"><a href="#cb42-659" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-660"><a href="#cb42-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-661"><a href="#cb42-661" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb42-662"><a href="#cb42-662" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-663"><a href="#cb42-663" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb42-664"><a href="#cb42-664" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-665"><a href="#cb42-665" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-666"><a href="#cb42-666" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-667"><a href="#cb42-667" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-668"><a href="#cb42-668" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-669"><a href="#cb42-669" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-670"><a href="#cb42-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-671"><a href="#cb42-671" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-672"><a href="#cb42-672" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb42-673"><a href="#cb42-673" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-674"><a href="#cb42-674" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-675"><a href="#cb42-675" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-676"><a href="#cb42-676" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-677"><a href="#cb42-677" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-678"><a href="#cb42-678" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-679"><a href="#cb42-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-680"><a href="#cb42-680" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-681"><a href="#cb42-681" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-682"><a href="#cb42-682" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-683"><a href="#cb42-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-684"><a href="#cb42-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-687"><a href="#cb42-687" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-688"><a href="#cb42-688" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-689"><a href="#cb42-689" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb42-690"><a href="#cb42-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-691"><a href="#cb42-691" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span>
<span id="cb42-692"><a href="#cb42-692" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-693"><a href="#cb42-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-694"><a href="#cb42-694" aria-hidden="true" tabindex="-1"></a><span class="fu">### Coordinate-wise Sampling</span></span>
<span id="cb42-695"><a href="#cb42-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-696"><a href="#cb42-696" aria-hidden="true" tabindex="-1"></a>The joint distribution would be unavailable, even if the learning based on the loss (<span class="co">[</span><span class="ot">-@eq-L</span><span class="co">]</span>) were perfectly done, because of the product form assumption on the neural network predictor $p_\theta$.^<span class="co">[</span><span class="ot">Of course, the exact sampling would have been available, for exmple, if we learned the backward intensity as [@Campbell+2022]. However, these methods have been marginalized due to suboptimal performance.</span><span class="co">]</span></span>
<span id="cb42-697"><a href="#cb42-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-698"><a href="#cb42-698" aria-hidden="true" tabindex="-1"></a>We mock this situation by replacing the joint distribution in the exact kernel, programmed in @sec-exact-kernel-2d, with the product of its marginals.</span>
<span id="cb42-699"><a href="#cb42-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-702"><a href="#cb42-702" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-703"><a href="#cb42-703" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-704"><a href="#cb42-704" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: Code (definition of reverse_sample_incorrect)</span></span>
<span id="cb42-705"><a href="#cb42-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-706"><a href="#cb42-706" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample_incorrect(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray, T: <span class="bu">int</span>):</span>
<span id="cb42-707"><a href="#cb42-707" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-708"><a href="#cb42-708" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-709"><a href="#cb42-709" aria-hidden="true" tabindex="-1"></a>    hist1 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist1[<span class="dv">0</span>] <span class="op">=</span> x1</span>
<span id="cb42-710"><a href="#cb42-710" aria-hidden="true" tabindex="-1"></a>    hist2 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist2[<span class="dv">0</span>] <span class="op">=</span> x2</span>
<span id="cb42-711"><a href="#cb42-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-712"><a href="#cb42-712" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb42-713"><a href="#cb42-713" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-714"><a href="#cb42-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-715"><a href="#cb42-715" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb42-716"><a href="#cb42-716" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb42-717"><a href="#cb42-717" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb42-718"><a href="#cb42-718" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-719"><a href="#cb42-719" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-720"><a href="#cb42-720" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-721"><a href="#cb42-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-722"><a href="#cb42-722" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb42-723"><a href="#cb42-723" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-724"><a href="#cb42-724" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> np.outer(pi_x, pi_y).ravel()</span>
<span id="cb42-725"><a href="#cb42-725" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb42-726"><a href="#cb42-726" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb42-727"><a href="#cb42-727" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb42-728"><a href="#cb42-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-729"><a href="#cb42-729" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb42-730"><a href="#cb42-730" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-731"><a href="#cb42-731" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb42-732"><a href="#cb42-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-733"><a href="#cb42-733" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb42-734"><a href="#cb42-734" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-735"><a href="#cb42-735" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb42-736"><a href="#cb42-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-737"><a href="#cb42-737" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb42-738"><a href="#cb42-738" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-739"><a href="#cb42-739" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-740"><a href="#cb42-740" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb42-741"><a href="#cb42-741" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb42-742"><a href="#cb42-742" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-743"><a href="#cb42-743" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb42-744"><a href="#cb42-744" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb42-745"><a href="#cb42-745" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-746"><a href="#cb42-746" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb42-747"><a href="#cb42-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-748"><a href="#cb42-748" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb42-749"><a href="#cb42-749" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-750"><a href="#cb42-750" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-751"><a href="#cb42-751" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb42-752"><a href="#cb42-752" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb42-753"><a href="#cb42-753" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-754"><a href="#cb42-754" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb42-755"><a href="#cb42-755" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb42-756"><a href="#cb42-756" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-757"><a href="#cb42-757" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb42-758"><a href="#cb42-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-759"><a href="#cb42-759" aria-hidden="true" tabindex="-1"></a>        hist1[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x1<span class="op">;</span> hist2[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x2</span>
<span id="cb42-760"><a href="#cb42-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-761"><a href="#cb42-761" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x1 <span class="op">!=</span> MASK) <span class="kw">and</span> np.<span class="bu">all</span>(x2 <span class="op">!=</span> MASK)</span>
<span id="cb42-762"><a href="#cb42-762" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([x1, x2], axis<span class="op">=</span><span class="dv">1</span>), hist1, hist2</span>
<span id="cb42-763"><a href="#cb42-763" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-764"><a href="#cb42-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-767"><a href="#cb42-767" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-768"><a href="#cb42-768" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-769"><a href="#cb42-769" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of steps</span></span>
<span id="cb42-770"><a href="#cb42-770" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb42-771"><a href="#cb42-771" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb42-772"><a href="#cb42-772" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb42-773"><a href="#cb42-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-774"><a href="#cb42-774" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_incorrect(N, p_unmask, T)</span>
<span id="cb42-775"><a href="#cb42-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-776"><a href="#cb42-776" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb42-777"><a href="#cb42-777" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-778"><a href="#cb42-778" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb42-779"><a href="#cb42-779" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb42-780"><a href="#cb42-780" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-781"><a href="#cb42-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-782"><a href="#cb42-782" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb42-783"><a href="#cb42-783" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-784"><a href="#cb42-784" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb42-785"><a href="#cb42-785" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-786"><a href="#cb42-786" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-787"><a href="#cb42-787" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-788"><a href="#cb42-788" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-789"><a href="#cb42-789" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-790"><a href="#cb42-790" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-791"><a href="#cb42-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-792"><a href="#cb42-792" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-793"><a href="#cb42-793" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb42-794"><a href="#cb42-794" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-795"><a href="#cb42-795" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-796"><a href="#cb42-796" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-797"><a href="#cb42-797" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-798"><a href="#cb42-798" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-799"><a href="#cb42-799" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-800"><a href="#cb42-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-801"><a href="#cb42-801" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-802"><a href="#cb42-802" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-803"><a href="#cb42-803" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-804"><a href="#cb42-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-805"><a href="#cb42-805" aria-hidden="true" tabindex="-1"></a>Then, this time the result is not so good.</span>
<span id="cb42-806"><a href="#cb42-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-809"><a href="#cb42-809" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-810"><a href="#cb42-810" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-811"><a href="#cb42-811" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb42-812"><a href="#cb42-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-813"><a href="#cb42-813" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span>
<span id="cb42-814"><a href="#cb42-814" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-815"><a href="#cb42-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-816"><a href="#cb42-816" aria-hidden="true" tabindex="-1"></a>There is a small deviation from the exact kernel, where $\ell^1$ distance was $0.024\pm0.00005$.</span>
<span id="cb42-817"><a href="#cb42-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-818"><a href="#cb42-818" aria-hidden="true" tabindex="-1"></a>We can easily see, in the empirical distribution, some cells are assinged with positive mass, although the true probability is $0$.</span>
<span id="cb42-819"><a href="#cb42-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-820"><a href="#cb42-820" aria-hidden="true" tabindex="-1"></a>This effect becomes smaller when the number of steps <span class="in">`T`</span> is large. For example, setting <span class="in">`T=1000`</span> gives us with almost same accuracy as the exact kernel (although we don't show it here for brevity).</span>
<span id="cb42-821"><a href="#cb42-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-822"><a href="#cb42-822" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Larger Step Size</span></span>
<span id="cb42-823"><a href="#cb42-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-824"><a href="#cb42-824" aria-hidden="true" tabindex="-1"></a>The situation gets worse when the step size is large, for example <span class="in">`T=1`</span>.</span>
<span id="cb42-825"><a href="#cb42-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-828"><a href="#cb42-828" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-829"><a href="#cb42-829" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-830"><a href="#cb42-830" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Number of steps</span></span>
<span id="cb42-831"><a href="#cb42-831" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb42-832"><a href="#cb42-832" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb42-833"><a href="#cb42-833" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb42-834"><a href="#cb42-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-835"><a href="#cb42-835" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_incorrect(N, p_unmask, T)</span>
<span id="cb42-836"><a href="#cb42-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-837"><a href="#cb42-837" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb42-838"><a href="#cb42-838" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-839"><a href="#cb42-839" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb42-840"><a href="#cb42-840" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb42-841"><a href="#cb42-841" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-842"><a href="#cb42-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-843"><a href="#cb42-843" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb42-844"><a href="#cb42-844" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-845"><a href="#cb42-845" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb42-846"><a href="#cb42-846" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-847"><a href="#cb42-847" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-848"><a href="#cb42-848" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-849"><a href="#cb42-849" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-850"><a href="#cb42-850" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-851"><a href="#cb42-851" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-852"><a href="#cb42-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-853"><a href="#cb42-853" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-854"><a href="#cb42-854" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb42-855"><a href="#cb42-855" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-856"><a href="#cb42-856" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-857"><a href="#cb42-857" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-858"><a href="#cb42-858" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-859"><a href="#cb42-859" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-860"><a href="#cb42-860" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-861"><a href="#cb42-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-862"><a href="#cb42-862" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-863"><a href="#cb42-863" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-864"><a href="#cb42-864" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-865"><a href="#cb42-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-868"><a href="#cb42-868" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-869"><a href="#cb42-869" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-870"><a href="#cb42-870" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs)</span>
<span id="cb42-871"><a href="#cb42-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-872"><a href="#cb42-872" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span>
<span id="cb42-873"><a href="#cb42-873" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-874"><a href="#cb42-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-875"><a href="#cb42-875" aria-hidden="true" tabindex="-1"></a>The error is now significant, because the incorrect product kernel is used every time, as we set $T=1$ meaning unmasking in just one step!</span>
<span id="cb42-876"><a href="#cb42-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-877"><a href="#cb42-877" aria-hidden="true" tabindex="-1"></a>One way to fix this is to set <span class="in">`T`</span> as large as possible, making sure no more than one jump occurs simultaneously.</span>
<span id="cb42-878"><a href="#cb42-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-879"><a href="#cb42-879" aria-hidden="true" tabindex="-1"></a><span class="fu">### Corrector Sampling {#sec-Predictor-Corrector}</span></span>
<span id="cb42-880"><a href="#cb42-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-881"><a href="#cb42-881" aria-hidden="true" tabindex="-1"></a>Another remedy is to utilise Gibbs sampling ideas to correct the bias, again at the expense of computational cost.</span>
<span id="cb42-882"><a href="#cb42-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-883"><a href="#cb42-883" aria-hidden="true" tabindex="-1"></a>To do this, we must identify a Markov kernel that keeps the marginal distribution of $X_t$ invariant for every timestep $t\in<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$.</span>
<span id="cb42-884"><a href="#cb42-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-885"><a href="#cb42-885" aria-hidden="true" tabindex="-1"></a>In our case, let us simply add a *re-masking* step, only to those pairs $(x_t^1,x_t^2)$'s which are completely unmasked.</span>
<span id="cb42-886"><a href="#cb42-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-887"><a href="#cb42-887" aria-hidden="true" tabindex="-1"></a>As there is some probability of having been unmasked simultaneously, re-masking only one of them, this time $x_t^1$, will allow us a second chance to arrive at a correct pair $(x_t^{1,\text{corrected}},x_t^2)$.</span>
<span id="cb42-888"><a href="#cb42-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-891"><a href="#cb42-891" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-892"><a href="#cb42-892" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-893"><a href="#cb42-893" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: Code (definition of reverse_sample_correct)</span></span>
<span id="cb42-894"><a href="#cb42-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-895"><a href="#cb42-895" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_sample_corrector(num_samples: <span class="bu">int</span>, p_unmask: np.ndarray, T: <span class="bu">int</span>):</span>
<span id="cb42-896"><a href="#cb42-896" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-897"><a href="#cb42-897" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> np.full(num_samples, MASK, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb42-898"><a href="#cb42-898" aria-hidden="true" tabindex="-1"></a>    hist1 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist1[<span class="dv">0</span>] <span class="op">=</span> x1</span>
<span id="cb42-899"><a href="#cb42-899" aria-hidden="true" tabindex="-1"></a>    hist2 <span class="op">=</span> np.empty((T <span class="op">+</span> <span class="dv">1</span>, num_samples), dtype<span class="op">=</span><span class="bu">int</span>)<span class="op">;</span> hist2[<span class="dv">0</span>] <span class="op">=</span> x2</span>
<span id="cb42-900"><a href="#cb42-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-901"><a href="#cb42-901" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb42-902"><a href="#cb42-902" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> p_unmask[t<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-903"><a href="#cb42-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-904"><a href="#cb42-904" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb42-905"><a href="#cb42-905" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb42-906"><a href="#cb42-906" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb42-907"><a href="#cb42-907" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-908"><a href="#cb42-908" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-909"><a href="#cb42-909" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-910"><a href="#cb42-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-911"><a href="#cb42-911" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb42-912"><a href="#cb42-912" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-913"><a href="#cb42-913" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> np.outer(pi_x, pi_y).ravel()</span>
<span id="cb42-914"><a href="#cb42-914" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb42-915"><a href="#cb42-915" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb42-916"><a href="#cb42-916" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb42-917"><a href="#cb42-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-918"><a href="#cb42-918" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb42-919"><a href="#cb42-919" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-920"><a href="#cb42-920" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb42-921"><a href="#cb42-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-922"><a href="#cb42-922" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb42-923"><a href="#cb42-923" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-924"><a href="#cb42-924" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb42-925"><a href="#cb42-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-926"><a href="#cb42-926" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb42-927"><a href="#cb42-927" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-928"><a href="#cb42-928" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-929"><a href="#cb42-929" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb42-930"><a href="#cb42-930" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb42-931"><a href="#cb42-931" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-932"><a href="#cb42-932" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb42-933"><a href="#cb42-933" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb42-934"><a href="#cb42-934" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-935"><a href="#cb42-935" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb42-936"><a href="#cb42-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-937"><a href="#cb42-937" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb42-938"><a href="#cb42-938" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-939"><a href="#cb42-939" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-940"><a href="#cb42-940" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb42-941"><a href="#cb42-941" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb42-942"><a href="#cb42-942" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-943"><a href="#cb42-943" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb42-944"><a href="#cb42-944" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb42-945"><a href="#cb42-945" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-946"><a href="#cb42-946" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb42-947"><a href="#cb42-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-948"><a href="#cb42-948" aria-hidden="true" tabindex="-1"></a>        <span class="co"># corrector step</span></span>
<span id="cb42-949"><a href="#cb42-949" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> p  <span class="co"># masking probability</span></span>
<span id="cb42-950"><a href="#cb42-950" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">!=</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK)</span>
<span id="cb42-951"><a href="#cb42-951" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb42-952"><a href="#cb42-952" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-953"><a href="#cb42-953" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> q</span>
<span id="cb42-954"><a href="#cb42-954" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx[will]</span>
<span id="cb42-955"><a href="#cb42-955" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-956"><a href="#cb42-956" aria-hidden="true" tabindex="-1"></a>                x1[idx_now] <span class="op">=</span> MASK</span>
<span id="cb42-957"><a href="#cb42-957" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predictor step</span></span>
<span id="cb42-958"><a href="#cb42-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-959"><a href="#cb42-959" aria-hidden="true" tabindex="-1"></a>        <span class="co"># both masked</span></span>
<span id="cb42-960"><a href="#cb42-960" aria-hidden="true" tabindex="-1"></a>        both <span class="op">=</span> (x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">==</span> MASK)</span>
<span id="cb42-961"><a href="#cb42-961" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(both)[<span class="dv">0</span>]</span>
<span id="cb42-962"><a href="#cb42-962" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-963"><a href="#cb42-963" aria-hidden="true" tabindex="-1"></a>            um1 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-964"><a href="#cb42-964" aria-hidden="true" tabindex="-1"></a>            um2 <span class="op">=</span> rng.random(idx.size) <span class="op">&lt;</span> p</span>
<span id="cb42-965"><a href="#cb42-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-966"><a href="#cb42-966" aria-hidden="true" tabindex="-1"></a>            idx_both <span class="op">=</span> idx[um1 <span class="op">&amp;</span> um2]</span>
<span id="cb42-967"><a href="#cb42-967" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_both.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-968"><a href="#cb42-968" aria-hidden="true" tabindex="-1"></a>                flat <span class="op">=</span> np.outer(pi_x, pi_y).ravel()</span>
<span id="cb42-969"><a href="#cb42-969" aria-hidden="true" tabindex="-1"></a>                choices <span class="op">=</span> rng.choice(K<span class="op">*</span>K, size<span class="op">=</span>idx_both.size, p<span class="op">=</span>flat)</span>
<span id="cb42-970"><a href="#cb42-970" aria-hidden="true" tabindex="-1"></a>                xs <span class="op">=</span> choices <span class="op">//</span> K<span class="op">;</span> ys <span class="op">=</span> choices <span class="op">%</span> K</span>
<span id="cb42-971"><a href="#cb42-971" aria-hidden="true" tabindex="-1"></a>                x1[idx_both] <span class="op">=</span> xs<span class="op">;</span> x2[idx_both] <span class="op">=</span> ys</span>
<span id="cb42-972"><a href="#cb42-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-973"><a href="#cb42-973" aria-hidden="true" tabindex="-1"></a>            idx_only1 <span class="op">=</span> idx[um1 <span class="op">&amp;</span> (<span class="op">~</span>um2)]</span>
<span id="cb42-974"><a href="#cb42-974" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-975"><a href="#cb42-975" aria-hidden="true" tabindex="-1"></a>                x1[idx_only1] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only1.size, p<span class="op">=</span>pi_x)</span>
<span id="cb42-976"><a href="#cb42-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-977"><a href="#cb42-977" aria-hidden="true" tabindex="-1"></a>            idx_only2 <span class="op">=</span> idx[(<span class="op">~</span>um1) <span class="op">&amp;</span> um2]</span>
<span id="cb42-978"><a href="#cb42-978" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_only2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-979"><a href="#cb42-979" aria-hidden="true" tabindex="-1"></a>                x2[idx_only2] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>idx_only2.size, p<span class="op">=</span>pi_y)</span>
<span id="cb42-980"><a href="#cb42-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-981"><a href="#cb42-981" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x1 masked, x2 revealed</span></span>
<span id="cb42-982"><a href="#cb42-982" aria-hidden="true" tabindex="-1"></a>        idx_b1 <span class="op">=</span> np.where((x1 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x2 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-983"><a href="#cb42-983" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b1.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-984"><a href="#cb42-984" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b1.size) <span class="op">&lt;</span> p</span>
<span id="cb42-985"><a href="#cb42-985" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b1[will]</span>
<span id="cb42-986"><a href="#cb42-986" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-987"><a href="#cb42-987" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> x2[idx_now]</span>
<span id="cb42-988"><a href="#cb42-988" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(y_vals):</span>
<span id="cb42-989"><a href="#cb42-989" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (y_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-990"><a href="#cb42-990" aria-hidden="true" tabindex="-1"></a>                    x1[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_x_given_y[val, :])</span>
<span id="cb42-991"><a href="#cb42-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-992"><a href="#cb42-992" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x2 masked, x1 revealed</span></span>
<span id="cb42-993"><a href="#cb42-993" aria-hidden="true" tabindex="-1"></a>        idx_b2 <span class="op">=</span> np.where((x2 <span class="op">==</span> MASK) <span class="op">&amp;</span> (x1 <span class="op">!=</span> MASK))[<span class="dv">0</span>]</span>
<span id="cb42-994"><a href="#cb42-994" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx_b2.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-995"><a href="#cb42-995" aria-hidden="true" tabindex="-1"></a>            will <span class="op">=</span> rng.random(idx_b2.size) <span class="op">&lt;</span> p</span>
<span id="cb42-996"><a href="#cb42-996" aria-hidden="true" tabindex="-1"></a>            idx_now <span class="op">=</span> idx_b2[will]</span>
<span id="cb42-997"><a href="#cb42-997" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> idx_now.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb42-998"><a href="#cb42-998" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> x1[idx_now]</span>
<span id="cb42-999"><a href="#cb42-999" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val <span class="kw">in</span> np.unique(x_vals):</span>
<span id="cb42-1000"><a href="#cb42-1000" aria-hidden="true" tabindex="-1"></a>                    m <span class="op">=</span> (x_vals <span class="op">==</span> val)<span class="op">;</span> n <span class="op">=</span> m.<span class="bu">sum</span>()</span>
<span id="cb42-1001"><a href="#cb42-1001" aria-hidden="true" tabindex="-1"></a>                    x2[idx_now[m]] <span class="op">=</span> rng.choice(K, size<span class="op">=</span>n, p<span class="op">=</span>cond_y_given_x[val, :])</span>
<span id="cb42-1002"><a href="#cb42-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1003"><a href="#cb42-1003" aria-hidden="true" tabindex="-1"></a>        hist1[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x1<span class="op">;</span> hist2[T <span class="op">-</span> t <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x2</span>
<span id="cb42-1004"><a href="#cb42-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1005"><a href="#cb42-1005" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>(x1 <span class="op">!=</span> MASK) <span class="kw">and</span> np.<span class="bu">all</span>(x2 <span class="op">!=</span> MASK)</span>
<span id="cb42-1006"><a href="#cb42-1006" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.stack([x1, x2], axis<span class="op">=</span><span class="dv">1</span>), hist1, hist2</span>
<span id="cb42-1007"><a href="#cb42-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-1008"><a href="#cb42-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1011"><a href="#cb42-1011" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-1012"><a href="#cb42-1012" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-1013"><a href="#cb42-1013" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of steps</span></span>
<span id="cb42-1014"><a href="#cb42-1014" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="fl">1.0</span>, <span class="fl">0.0</span>, T <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb42-1015"><a href="#cb42-1015" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> (alpha[:<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> alpha[<span class="dv">1</span>:]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> alpha[<span class="dv">1</span>:])</span>
<span id="cb42-1016"><a href="#cb42-1016" aria-hidden="true" tabindex="-1"></a>p_unmask <span class="op">=</span> np.clip(p_unmask, <span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb42-1017"><a href="#cb42-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1018"><a href="#cb42-1018" aria-hidden="true" tabindex="-1"></a>pairs, h1, h2 <span class="op">=</span> reverse_sample_corrector(N, p_unmask, T)</span>
<span id="cb42-1019"><a href="#cb42-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1020"><a href="#cb42-1020" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical joint</span></span>
<span id="cb42-1021"><a href="#cb42-1021" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.zeros((K, K), dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb42-1022"><a href="#cb42-1022" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a, b <span class="kw">in</span> pairs:</span>
<span id="cb42-1023"><a href="#cb42-1023" aria-hidden="true" tabindex="-1"></a>    counts[a, b] <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb42-1024"><a href="#cb42-1024" aria-hidden="true" tabindex="-1"></a>pi_emp <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb42-1025"><a href="#cb42-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1026"><a href="#cb42-1026" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.4</span>))</span>
<span id="cb42-1027"><a href="#cb42-1027" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> ax[<span class="dv">0</span>].imshow(pi_joint, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-1028"><a href="#cb42-1028" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"True joint π_data"</span>)</span>
<span id="cb42-1029"><a href="#cb42-1029" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">0</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-1030"><a href="#cb42-1030" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-1031"><a href="#cb42-1031" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-1032"><a href="#cb42-1032" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">0</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_joint[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-1033"><a href="#cb42-1033" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-1034"><a href="#cb42-1034" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im0, ax<span class="op">=</span>ax[<span class="dv">0</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-1035"><a href="#cb42-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1036"><a href="#cb42-1036" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> ax[<span class="dv">1</span>].imshow(pi_emp, origin<span class="op">=</span><span class="st">"lower"</span>, aspect<span class="op">=</span><span class="st">"equal"</span>)</span>
<span id="cb42-1037"><a href="#cb42-1037" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Empirical joint (reverse)"</span>)</span>
<span id="cb42-1038"><a href="#cb42-1038" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"x2"</span>)<span class="op">;</span> ax[<span class="dv">1</span>].set_ylabel(<span class="st">"x1"</span>)</span>
<span id="cb42-1039"><a href="#cb42-1039" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-1040"><a href="#cb42-1040" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb42-1041"><a href="#cb42-1041" aria-hidden="true" tabindex="-1"></a>        ax[<span class="dv">1</span>].text(j, i, <span class="ss">f'</span><span class="sc">{</span>pi_emp[i, j]<span class="sc">:.3f}</span><span class="ss">'</span>, </span>
<span id="cb42-1042"><a href="#cb42-1042" aria-hidden="true" tabindex="-1"></a>                ha<span class="op">=</span><span class="st">'center'</span>, va<span class="op">=</span><span class="st">'center'</span>, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-1043"><a href="#cb42-1043" aria-hidden="true" tabindex="-1"></a>fig.colorbar(im1, ax<span class="op">=</span>ax[<span class="dv">1</span>], fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb42-1044"><a href="#cb42-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1045"><a href="#cb42-1045" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb42-1046"><a href="#cb42-1046" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-1047"><a href="#cb42-1047" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-1048"><a href="#cb42-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1051"><a href="#cb42-1051" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-1052"><a href="#cb42-1052" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb42-1053"><a href="#cb42-1053" aria-hidden="true" tabindex="-1"></a>l1, kl <span class="op">=</span> l1_kl(pairs, <span class="dv">10</span>)</span>
<span id="cb42-1054"><a href="#cb42-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1055"><a href="#cb42-1055" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L1 distance:"</span>, <span class="bu">round</span>(np.mean(l1), <span class="dv">6</span>), <span class="st">" ± "</span>, <span class="bu">round</span>(np.var(l1), <span class="dv">6</span>), <span class="st">"   KL(emp || true):"</span>, <span class="ss">f"</span><span class="sc">{</span>np<span class="sc">.</span>mean(kl)<span class="sc">:.6e}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>var(kl)<span class="sc">:.6e}</span><span class="ss">"</span>)</span>
<span id="cb42-1056"><a href="#cb42-1056" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-1057"><a href="#cb42-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1058"><a href="#cb42-1058" aria-hidden="true" tabindex="-1"></a>We see a small improvement from the $\ell^1$ distance of $0.0241$. The difference is significant in terms of our variance of an order $O(10^{-5})$ culculated from $10$ repeated experiments.</span>
<span id="cb42-1059"><a href="#cb42-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1060"><a href="#cb42-1060" aria-hidden="true" tabindex="-1"></a>This is the idea behind the predictor-corrector technique discussed, for example, in <span class="co">[</span><span class="ot">@Gat+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Zhao+2024</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@Zhao+2024Unified</span><span class="co">]</span>.</span>
<span id="cb42-1061"><a href="#cb42-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1062"><a href="#cb42-1062" aria-hidden="true" tabindex="-1"></a>We only included one corrector step per reverse step, although more correction will increase accuracy.</span>
<span id="cb42-1063"><a href="#cb42-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1064"><a href="#cb42-1064" aria-hidden="true" tabindex="-1"></a>This can be regarded as a form of inference time compute scaling <span class="co">[</span><span class="ot">@Wang+2025</span><span class="co">]</span>, although it is a fairly bad idea to wait for the predictor-corrector steps to converge.</span>
<span id="cb42-1065"><a href="#cb42-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1066"><a href="#cb42-1066" aria-hidden="true" tabindex="-1"></a><span class="fu">### Discussion {#sec-Chao}</span></span>
<span id="cb42-1067"><a href="#cb42-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1068"><a href="#cb42-1068" aria-hidden="true" tabindex="-1"></a>Let us come back to the technique employed in <span class="co">[</span><span class="ot">@Chao+2025</span><span class="co">]</span>, which we mentioned in @sec-Intermediate-States.</span>
<span id="cb42-1069"><a href="#cb42-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1070"><a href="#cb42-1070" aria-hidden="true" tabindex="-1"></a>As we have seen, the number of steps $T$ has to be kept larger than $d$, to make sure no more than one jump occurs simultaneously.</span>
<span id="cb42-1071"><a href="#cb42-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1072"><a href="#cb42-1072" aria-hidden="true" tabindex="-1"></a>However, this increases the computational cost, as more time steps must be spent in simulating phantom jumps.</span>
<span id="cb42-1073"><a href="#cb42-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1074"><a href="#cb42-1074" aria-hidden="true" tabindex="-1"></a>One thing we can do is to fill this blank steps with an informed 'half-jump,' by introducing a sub-structure in each state $x\in E$.</span>
<span id="cb42-1075"><a href="#cb42-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1076"><a href="#cb42-1076" aria-hidden="true" tabindex="-1"></a>This half-jump is a bit more robust than a direct unmasking. Therefore we are able to spend the time up to $T$ more meaningfully.</span>
<span id="cb42-1077"><a href="#cb42-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1078"><a href="#cb42-1078" aria-hidden="true" tabindex="-1"></a>Thus, this strategy can be view as another way to mitigate the bias introduced by the incorrect backward kernel.</span>
<span id="cb42-1079"><a href="#cb42-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1080"><a href="#cb42-1080" aria-hidden="true" tabindex="-1"></a><span class="fu">## Future Works</span></span>
<span id="cb42-1081"><a href="#cb42-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1082"><a href="#cb42-1082" aria-hidden="true" tabindex="-1"></a>Since the absorbing process is favoured only because of its time-agnostic property, its ability should be explained separately with the properties of the process.</span>
<span id="cb42-1083"><a href="#cb42-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1084"><a href="#cb42-1084" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Reinforcenment Learning Take</span></span>
<span id="cb42-1085"><a href="#cb42-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1086"><a href="#cb42-1086" aria-hidden="true" tabindex="-1"></a>The inference step and the sampling step should be decoupled, at least conceptually.</span>
<span id="cb42-1087"><a href="#cb42-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1088"><a href="#cb42-1088" aria-hidden="true" tabindex="-1"></a>To tune the forward process noising schedule $\alpha_t(x)$, a reinforcement learning framework will be employed, I believe in the near future.</span>
<span id="cb42-1089"><a href="#cb42-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1090"><a href="#cb42-1090" aria-hidden="true" tabindex="-1"></a>This is a variant of meta-learning and, in this way, the unmasking network $p_\theta$ will be able to efficiently learn the correct dependence structure in the data domain.</span>
<span id="cb42-1091"><a href="#cb42-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1092"><a href="#cb42-1092" aria-hidden="true" tabindex="-1"></a>For example, in language modeling, there is a natural sequential structure, which is partly why autoregressive modeling has been dominant in this domain. However, by learning $\alpha_t$ in masking process, a much more efficent factorization over texts can be aquired in a data-driven way.</span>
<span id="cb42-1093"><a href="#cb42-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1094"><a href="#cb42-1094" aria-hidden="true" tabindex="-1"></a>I even think this $\alpha_t$ can play an important role just as a word embedding does currently.</span>
<span id="cb42-1095"><a href="#cb42-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1096"><a href="#cb42-1096" aria-hidden="true" tabindex="-1"></a>In the sampling step, a sampling time path planning will greatly enhance sample quality, just as Monte Carlo tree search does in reinforcement learning.</span>
<span id="cb42-1097"><a href="#cb42-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1098"><a href="#cb42-1098" aria-hidden="true" tabindex="-1"></a>As a conclusion, the flexible framework of masked diffusion models will enable a marriage with reinforcement learning and meta learning, which will be a way to overcome the autoregressive modeling framework, because the latter imposes unnecessary sequential inductive bias into the model.</span>
<span id="cb42-1099"><a href="#cb42-1099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1100"><a href="#cb42-1100" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Scaling Analysis</span></span>
<span id="cb42-1101"><a href="#cb42-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1102"><a href="#cb42-1102" aria-hidden="true" tabindex="-1"></a>There are two papers, namely <span class="co">[</span><span class="ot">@Santos+2023</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@Winkler+2024</span><span class="co">]</span>, which carry out a scaling analysis as $K\to\infty$, in order to bridge the gap between discrete and continuous state spaces.</span>
<span id="cb42-1103"><a href="#cb42-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1104"><a href="#cb42-1104" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@Santos+2023 Section 2.4</span><span class="co">]</span> and its Appendix C, it is proved that a fairly large class of discrete processes will converge to Ito processes, as $K\to\infty$.</span>
<span id="cb42-1105"><a href="#cb42-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1106"><a href="#cb42-1106" aria-hidden="true" tabindex="-1"></a>Their discussion based on a formal argument. They proves that the Kramers-Moyal expansion of the Chapman-Kolmogorov equation of the discrete process converges to that of a Ito process.</span>
<span id="cb42-1107"><a href="#cb42-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1108"><a href="#cb42-1108" aria-hidden="true" tabindex="-1"></a>In other words, they didn't identify the direct correspondence, for example deriving the exact expression of limiting SDEs.</span>
<span id="cb42-1109"><a href="#cb42-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1110"><a href="#cb42-1110" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Winkler+2024</span><span class="co">]</span> builds on their analysis, identifying an OU process on $\R^d$ corresponds to a Ehrenfest process.</span>
<span id="cb42-1111"><a href="#cb42-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1112"><a href="#cb42-1112" aria-hidden="true" tabindex="-1"></a>They basicly build on the result of <span class="co">[</span><span class="ot">@Ushio+2006 Theorem 4.1</span><span class="co">]</span> and draw a correspondence between the concrete score matching approach of <span class="co">[</span><span class="ot">@Meng+2022</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@Lou+2024</span><span class="co">]</span> and the score-based diffusion modeling approach on the continuous state space $\R^d$.</span>
<span id="cb42-1113"><a href="#cb42-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1114"><a href="#cb42-1114" aria-hidden="true" tabindex="-1"></a>Mathematically, however, I must note that, in <span class="co">[</span><span class="ot">@Winkler+2024</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@Ushio+2006</span><span class="co">]</span>, only the convergence of the one-dimensional marginal distributions is proved, not the distribution of the process itself.</span>
<span id="cb42-1115"><a href="#cb42-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1116"><a href="#cb42-1116" aria-hidden="true" tabindex="-1"></a>This line of research has a lot to do with thermodynamics, and might provide insights into whether images should be modeled discretely or continuously, as remarked in <span class="co">[</span><span class="ot">@Winkler+2024 Remark 3.7</span><span class="co">]</span>.</span>
<span id="cb42-1117"><a href="#cb42-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1118"><a href="#cb42-1118" aria-hidden="true" tabindex="-1"></a>Also, the limit $d\to\infty$ has yet to be explored.</span>
<span id="cb42-1119"><a href="#cb42-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1120"><a href="#cb42-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Numerical Discretisation Scheme</span></span>
<span id="cb42-1121"><a href="#cb42-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1122"><a href="#cb42-1122" aria-hidden="true" tabindex="-1"></a>Accelerating sampling from a continuous-space diffusion model is a issue.</span>
<span id="cb42-1123"><a href="#cb42-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1124"><a href="#cb42-1124" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@Duffield+2025</span><span class="co">]</span> tackles this issue from a hardware perspective.</span>
<span id="cb42-1125"><a href="#cb42-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1126"><a href="#cb42-1126" aria-hidden="true" tabindex="-1"></a>However, as remarked in <span class="co">[</span><span class="ot">@Winkler+2024 Remark 3.7</span><span class="co">]</span>, transfer learning between continuous and discrete state spaces might lead to a sampling scheme with increased precision.</span>
<span id="cb42-1127"><a href="#cb42-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1128"><a href="#cb42-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">### Concluding Remarks</span></span>
<span id="cb42-1129"><a href="#cb42-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1130"><a href="#cb42-1130" aria-hidden="true" tabindex="-1"></a>I believe that masked diffusion modeling can be viewed as a form of Gibbs sampling, with a learned transition kernel.</span>
<span id="cb42-1131"><a href="#cb42-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1132"><a href="#cb42-1132" aria-hidden="true" tabindex="-1"></a>Many current practices are based on (uniformly) random scan Gibbs, while the autoregressive models are a fixed scan Gibbs.</span>
<span id="cb42-1133"><a href="#cb42-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1134"><a href="#cb42-1134" aria-hidden="true" tabindex="-1"></a>Most recent improvements are based upon ideas from reinforcement learning and meta learning, where an optimal order to unmask components is pursued.</span>
<span id="cb42-1135"><a href="#cb42-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1136"><a href="#cb42-1136" aria-hidden="true" tabindex="-1"></a>This point of view might not be only an abstract nonsense. I actually believe that this point of view will be fruitful in the future.</span>
<span id="cb42-1137"><a href="#cb42-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1138"><a href="#cb42-1138" aria-hidden="true" tabindex="-1"></a><span class="fu">## 関連記事 {.appendix}</span></span>
<span id="cb42-1139"><a href="#cb42-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-1140"><a href="#cb42-1140" aria-hidden="true" tabindex="-1"></a>::: {#diffusion-listing}</span>
<span id="cb42-1141"><a href="#cb42-1141" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://162348.github.io/">
<p>Hirofumi Shiba</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/162348/162348.github.io/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ano2math5">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:shiba.hirofumi@ism.ac.jp">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../../blog.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>