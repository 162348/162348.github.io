---
title: "拡散モデルでの動的暗黙正則化"
subtitle: "Bonnaire, Urfin, Biroli & Mézard (2025, NeurIPS)<br>Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training"
author: "司馬博文"
date: "1/26/2026"
categories: [Diffusion, Nonresearch]
image: ./Bonnaire2025/Summary1.png
format:
  # html: default
  revealjs: 
    output-file: Bonnaire2025Slides.html
    footer: |
      [Why Diffusion Models Don’t Memorize](Bonnaire2025.qmd)
    toc-depth: 1
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ../../../assets/profile.jpg
    css: ../../../assets/slides.css
    scrollable: true
    smaller: true
    controls: true
    controls-layout: bottom-right
    self-contained-math: true
    shift-heading-level-by: -1
    toc: true
    toc-title: 目次
    number-sections: true
    theme: serif
    show-slide-number: all
    include-in-header: ../../../assets/forSlides.html
    tbl-cap-location: bottom
    margin: 0.05
    comments: false
comment: false
code-fold: false
execute:
    cache: true
html-math-method: katex
abstract-title: 概要
abstract: |
  スライド版は[[こちら]{.underline}](Bonnaire2025Slides.html)．
revealjs-plugins:
  - pointer  # shortcut q (https://github.com/quarto-ext/pointer)
---

## はじめに

[-@sec-DMs]. Diffusion Model とは？

[-@sec-Biroli]. 先行研究 [@Biroli+2024]

[-@sec-summary]. 本論文の要旨

{{< include ../../../assets/_preamble.qmd >}}

### Score-based Diffusion Models {#sec-DMs}

$$
\text{Forward}\quad dX_t=-X_t\,dt+dB_t,\quad X_0\sim P_x,
$$
$$
\text{Reverse}\quad dX'_t=-\Paren{X_t'+2s_t'(X_t')}\,dt+dB'_t.
$$

ただし $s_t'(x):=\nabla_x\log p_t'(x)$ は $(X_t')$ の周辺密度 $p_t'$ の Hyvärinen スコア．

::: {.callout-tip title="[Tweedie's Formula @Efron2011] の系" icon="false"}

$$
\E\Square{\frac{e^{-t}X_x-X_{s+t}}{\Delta_t}\,\middle|\,X_{s+t}=x}=\nabla_x\log p_{s+t}(x).
$$

:::

$\Delta_t:=\sqrt{1-e^{-2t}}$ とすると $\xi:=\frac{X_{s+t}-e^{-t}X_s}{\sqrt{\Delta_t}}\sim\rN(0,1)$．

$s\to\infty$ を考えて，次の損失関数が発想される：
$$
\mathcal{L}_t(\theta;\{x^\nu\}_{\nu=1}^n)=\frac{1}{n}\sum_{\nu=1}^n\E_\xi\Square{\norm{\sqrt{\Delta_t}s_\theta(x_t^\nu(\xi))+\xi}^2}.
$$

::: aside
ただし，$x_t^\nu(\xi):=e^{-t}x^\nu+\sqrt{\Delta_t}\xi$．
:::

### Generation from the Empirical Score {#sec-Biroli}

::: {.callout-important icon="false" title="Memorization Regime = 過学習 [@Biroli+2024]"}

経験リスク最小化問題

$$
\mathcal{L}_t(\theta;\{x^\nu\}_{\nu=1}^n)=\frac{1}{n}\sum_{\nu=1}^n\E_\xi\Square{\norm{\sqrt{\Delta_t}s_\theta(x_t^\nu(\xi))+\xi}^2}.
$$

を解き切ったらうまくいかない．理論的には $\mathbb{P}_n$ からのサンプルを再現してしまう．

→ 実際には経験リスク最小化を解き切っていないはず．

:::

* Early stopping により汎化を達成しているのでは？

* 不完全なスコア学習＋サンプリングでの数値誤差で汎化している？？？

::: aside
$n=O(e^d)$ で大きくすると memorization が起こる時刻は $\tau_C\to0$ に近づく [@Biroli+2024]：
$$
\tau_C=\frac{1}{2}\log\paren{1+\frac{\sigma^2}{n^{\frac{2}{d}}-1}}
$$
:::

### 本論文の要旨：Implicit Dynamical Regularization {#sec-summary}

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
![](Bonnaire2025/Summary1.png){width=100% fig-align="center"}
:::

::: {.column width="50%"}
![](Bonnaire2025/Summary_diagram_tau_mem.png){width=100% fig-align="center"}
:::

::::

* たしかに解き切っていない．汎化のためには early stopping が必須．
* $n<p$ じゃないと implicit dynamical regularization は起きない．
* $p<n$ のとき，訓練ダイナミクスにスケール分離が見られる
  * 汎化が始まる時間は $\tau_{\text{gen}}=O(1)$ だが，過学習が始まる時間は $\tau_{\text{mem}}=O(n)$

    → $n$ が大きいほど early stopping の sweet spot が広い．

## 実験パート (Section 2)

理論パートではスコア学習に特化して解析するので，

$$
\text{スコア学習での汎化}\,\approx\,\text{Generative Model としての汎化}
$$

を確認する必要がある ([-@sec-experiment-n])．

＋スケーリングの検証 ([-@sec-experiment-p]) ＋相図の検証 ([-@sec-experiment-p-n])

### 設定

* **データ**：cropped, grayscaled, & downsampled CelebA, $d=32^2$

![](Bonnaire2025/SM_Images_CelebA.jpeg){width=10 fig-align="center"}

* **スコア学習に使う NN**：U-Net, 時刻 $t$ は sinusoidal position embedding
* **損失関数**：DDPM loss．ここまで全て [@Ho+2020] の設定に従う．
* **訓練**：SGD, momentum $\beta=0.95$, $\eta=0.01$, $B=n\land512$

$n\in\{128,\cdots,32768\}$, $p\in\{1/4,1,2,4,9,16\}\times10^6$ を順に動かしていく

### データサイズ $n$ を変える {#sec-experiment-n}

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
![横軸：訓練 step 数 $\tau$，縦軸：Fréchet Inception Distance（実線），記憶データ点割合 %（点線）．](Bonnaire2025/N1.png){fig-align="center" width="70%"}
:::

::: {.column width="50%"}
![スコア学習誤差のプロット<br>訓練誤差（実線），テスト誤差（点線）．全て $t=0.01$．](Bonnaire2025/N2.png){fig-align="center" width="70%"}
:::

* $n$ が増えるほど汎化ギャップ $\mathcal{L}_{\text{gen}}=\mathcal{L}_{\text{test}}-\mathcal{L}_{\text{train}}$ が小さくなる．
* $\mathcal{L}_{\text{gen}}>0$ が始まって，少し遅れて $f_{\text{mem}}>0$ が始まる．が，スケーリングは同じ．

::::

### 2.2 データサイズ $n$ を変える {.unnumbered .unlisted visibility="uncounted"}

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
![フルバッチ $B=n$ を取った場合](Bonnaire2025/CelebA_B=n.png){fig-align="center" width="70%"}

[それでも同じスケーリングが見られる]{.small-letter}

[→ 「モデルにサンプルを見せた回数」に依らない現象]{.tiny-letter}
:::

::: {.column width="50%"}
![SGD with momentum ではなく Adam を使った場合](Bonnaire2025/CelebA_Adam.png){fig-align="center" width="70%"}

[$\tau_{\text{gen}},\tau_{\text{mem}}$ が全体的に小さい方向へスライド]{.small-letter}

[主要なスケーリングは変わらない]{.small-letter}
:::

<!-- [横軸：訓練 step 数 $\tau$，縦軸：Fréchet Inception Distance（実線），記憶データ点割合 %（点線）．]{.small-letter} -->

::::

### モデルサイズ $p$ を変える {#sec-experiment-p}

:::: {.columns layout-valign="middle"}

::: {.column width="50%" layout-valign="middle"}
![](Bonnaire2025/p1.png){fig-align="center" width="100%"}
:::

::: {.column width="50%"}

::: {.callout-tip title="U-Net チャンネル数を増やす" icon="false"}

* Base channel width $W$
  
  $W\in\{8,16,32,48,64\}$

  → $p\in\{1/4,1,2,4,9,16\}\times10^6$

:::

FID が下がり始める時刻：
$$
\tau_{\text{gen}}W=O(1)
$$

訓練データに酷似した生成が始まる時刻：
$$
\tau_{\text{mem}}W=O(n)
$$

:::

::::

### $(p,n)$ 相図 {#sec-experiment-p-n}

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
![間に挟まれた相では，訓練時間に２つのスケールが現れる．](Bonnaire2025/p2.png){fig-align="center" width="100%"}
:::

::: {.column width="50%"}

::: {.callout-important title="Memorization Regime" icon="false"}

$n$ が小さすぎると，FID が下がり始める瞬間 $\tau_{\text{gen}}$ に memorization も起こる
$$
\tau_{\text{gen}}\approx\tau_{\text{mem}}\quad \textcolor{#EEC1C0}{p\gg n}
$$

:::

::: {.callout-tip title="Architectural Regularization" icon="false"}

$$
n>n^*(p)\approx p\qquad(n,p\to\infty)
$$
を超えると，$\tau\to\infty$ でも memorization は起こらない．

:::

:::

::::

## 理論パート (Section 3)

ランダム行列の経験スペクトル分布の極限に対するスケーリング解析

* ランダム行列のスペクトル分布解析への帰着 ([-@sec-reduction])
* ランダム行列の Stieltjes Transform Method ([-@sec-Stieltjes])
* **主定理**：平均レゾルベントの満たす自己無撞着方程式 ([-@sec-main-theorem])
* **主補題**：Gaussian Equivalence Principle ([-@sec-GEP])
* 証明方針２つ：Replica Method v. Free Probability ([-@sec-proof])
* **主な系**：Two-Bulk 構造 ([-@sec-two-bulk])
* 結論・ダイナミクスへの示唆：Separation of Timescales ([-@sec-dynamics])

### 設定

* **データ**: $\R^d\ni x^\nu\iidsim P_x\;(\nu=1,\cdots,n)$. あとで $d\to\infty$ の極限を考える．
* **スコア学習に使うNN**：Random Feature $W\in\R^{p\times d}$ with i.i.d. Gaussian
  $$
  s_A(x):=\frac{A}{\sqrt{p}}\sigma^{\otimes p}\paren{\frac{Wx}{\sqrt{d}}}\qquad A\in\R^{d\times p},
  $$
* **損失関数**: 固定した時刻 $t>0$ での DSM loss
  $$
  \mathcal{L}_t(A;\{x^\nu\}_{\nu=1}^n):=\frac{1}{dn}\sum_{\nu=1}^n\E\Square{\norm{\sqrt{\Delta_t s_A(x_t^\nu(\xi))}+\xi}^2}.
  $$
* **訓練**: GD の連続時間極限
  $$
  \dot{A}_\tau=-d^2\nabla_A\mathcal{L}_t(A_\tau)=-2\Delta_t\frac{d}{p}AU_\tau-\frac{2d\sqrt{\Delta_t}}{\sqrt{p}}V_\tau^\top,\qquad\tau\ge0.
  $$

### GD の連続時間極限

DSM loss での勾配降下法：
<!-- $$\mathcal{L}_t(\theta,\{x^\nu\}^n_{\nu=1})=\frac{1}{d}\frac{1}{n}\sum_{\nu=1}^n\operatorname{E}_\xi\SQuare{\|\sqrt{\Delta_t}s_\theta(X^\nu_t)+\xi\|^2\,\bigg|\,X^\nu_0=\xi}$$ -->
$$
A^{(k+1)}\gets A^{(k)}-\eta\nabla_A\mathcal{L}_t(A^{(k)}),\qquad k=0,1,2,\cdots
$$
$$
\mathcal{L}_t(A)=\frac{1}{d}\paren{\frac{\Delta_t}{p}\Tr(A^\top AU)+\frac{2\sqrt{\Delta_t}}{\sqrt{p}}\Tr(AV)+d}
$$

時間変換 $\tau(k):=k\eta/d^2$ を施して，$\eta\to0$ の極限を取ると
$$
\dot{A}_\tau=-d^2\nabla_A\mathcal{L}_t(A_\tau)=-2\Delta_t\frac{d}{p}A_\tau U-\frac{2d\sqrt{\Delta_t}}{\sqrt{p}}V^\top,\qquad\tau\ge0,
$$
$$%:=\frac{1}{n}\sum_{\nu=1}^n\E[y^\nu (y^\nu)^\top]
U:=\frac{1}{n}\sum_{\nu=1}^n\operatorname{E}_\xi\left[\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}^\top\right],
$$
$$
V:=\frac{1}{n}\sum_{\nu=1}^n\operatorname{E}_\xi\Square{\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}\xi^\top}.
$$

### $U$ のスペクトルの分布への帰着 {#sec-reduction}

::: {.callout-important appearance="simple" icon="false"}
訓練ダイナミクス $(A_\tau)_{\tau\ge0}$ のスケール分離は，$U$ のスペクトルの分布のスケール分離からくる．
特に，典型的な緩和時間は $\Delta_tU/\psi_p$ の固有値の逆数が与える．
:::

$$
\dot{\textcolor{#E95420}{A}}_{\textcolor{#E95420}{\tau}}=-d^2\nabla_A\mathcal{L}_t(\textcolor{#E95420}{A_\tau})=-2\Delta_t\frac{d}{p}\textcolor{#E95420}{A_\tau} U-\frac{2d\sqrt{\Delta_t}}{\sqrt{p}}V^\top,\qquad\tau\ge0,
$$
は行列 $A\in\R^{d\times p}$ に関する線型 ODE．$U\in\GL_p(\R)$ なら Duhamel の公式から
$$
\frac{\textcolor{#E95420}{A_\tau}}{\sqrt{p}}=-\frac{1}{\sqrt{\Delta_t}}V^\top U^{-1}+\paren{\frac{1}{\sqrt{\Delta_t}}V^\top U^{-1}+\frac{A_0}{\sqrt{p}}}\exp\paren{-\frac{2\Delta_t}{\psi_p}U\textcolor{#E95420}{\tau}},
$$
$$
\psi_p:=\frac{p}{d},\qquad\psi_n:=\frac{n}{d}.
$$

::: aside
$A_0=0$ と仮定する．

[@George+2025] は解 $-V^\top U^{-1}/\sqrt{\Delta_t}$ に注目して，別の道具を通じてスペクトル分布を調べる．
:::

### Stieltjes Transform Method (1/2) {#sec-Stieltjes}

$U$ の経験スペクトル分布 $\mu_U$ の Stieltjes transform は，resolvent $R(z):=(U-zI_p)^{-1}$ の平均固有値が与える：
$$
q(z):=\frac{\Tr(R(z))}{p}=\frac{1}{p}\sum_{i=1}^pR(z)_{ii}=\int_{-\infty}^\infty\frac{1}{\lambda-z}\mu_U(d\lambda)\quad(z\in\bC\setminus\R).
$$

::: {.callout-tip title="[IX.2.2 The Spectral Theorem @Conway2007]" icon="false"}

$U$ を Hilbert 空間 $H$ 上の正規作用素とする．このとき，ただ一つのスペクトル測度 $E:\Sp(U)\to B(H)$ が存在して，
$$
U=\int_{\Sp(U)} \lambda E(d\lambda).
$$

:::

::: aside
Resolvent $R(z)$ は $\Sp(U)$ 上の有界 Borel 関数 $f(\lambda)=(z-\lambda)^{-1}$ の $E$ に関する積分．
:::

### 3.4 Stieltjes Transform Method (1/2) {.unnumbered .unlisted visibility="uncounted"}

$U$ の経験スペクトル分布 $\mu_U$ の Stieltjes transform は，resolvent $R(z):=(U-zI_p)^{-1}$ の平均固有値が与える：
$$
q(z):=\frac{\Tr(R(z))}{p}=\frac{1}{p}\sum_{i=1}^pR(z)_{ii}=\int_{-\infty}^\infty\frac{1}{\lambda-z}\mu_U(d\lambda)\quad(z\in\bC\setminus\R).
$$

Resolvent $R(z)$ は $f(\lambda)=(z-\lambda)^{-1}$ の $E$ に関する積分：
$$
R(z)=\frac{1}{U-z}=\int_{\Sp(U)}\frac{1}{\lambda-z}E(d\lambda).
$$
$$
\therefore\qquad R(z)_{ii}=(e_i|R(z)e_i)=\int_{-\infty}^\infty\frac{1}{\lambda-z}\mu_{e_i}(d\lambda).
$$
$$
\mu_U=\frac{1}{p}\sum_{i=1}^p\mu_{e_i},\qquad \mu_{e_i}(A):=(e_i|E(A)e_i)\quad(A\in \mathcal{B}(\R)).
$$


### Stieltjes Transform Method (2/2)

$q(z)=\Tr(R(z))/p$ を求めれば，$\mu_U$ が復元できる：

::: {.callout-tip title="[定理2.4.3 @Anderson+2011]" icon="false"}
$\mu$ を $\R$ 上の確率測度，
$$
S_\mu(z):=\int_{-\infty}^\infty\frac{\mu(dx)}{x-z},\qquad z\in\bC\setminus\R
$$
をStieltjes transform とする．任意の開区間 $I\subset\R$ について，$\mu$ が $\partial I$ 上に atom を持たないならば
$$
\mu(I)=\lim_{\ep\to0}\int_I\frac{\Im S_\mu(\lambda+i\epsilon)}{\pi}\,d\lambda.
$$
:::

### 3.5 Stieltjes Transform Method (2/2) {.unnumbered .unlisted visibility="uncounted"}

【証明】 任意の $\lambda\in\R$ について，

\begin{align*}
    \Im\paren{\frac{S_\mu(\lambda+i\ep)}{\pi}}&=\frac{S_\mu(\lambda+i\ep)-S_\mu(\lambda-i\ep)}{2\pi i}\\
    &=\frac{1}{2\pi i}\int_\R\paren{\frac{1}{x-(\lambda+i\ep)}-\frac{1}{x-(\lambda-i\ep)}}\,\mu(dx)\\
    &=\frac{1}{2\pi i}\int_\R\frac{2i\ep}{x^2-2\lambda x+(\lambda^2+\ep^2)}\mu(dx)\\
    &=\int_\R\frac{1}{\pi}\frac{\ep}{(x-\lambda)^2+\ep^2}\mu(dx),\qquad\lambda\in\R.
  \end{align*}

この右辺は実は $X\sim\mu,C_\ep\sim\operatorname{Cauchy}(0,\epsilon)$ について $X+C_\ep$ の分布の $\lambda$ での確率密度である．$X+C_\ep\Rightarrow X$ から結論が従う．

::: aside
複素解析の [Sokhotski-Plemelj の公式](https://en.wikipedia.org/wiki/Sokhotski%E2%80%93Plemelj_theorem) の系ともみれる．
:::

### 主定理：平均レゾルベント $q(z)$ の極限の特徴付け {#sec-main-theorem}

$q$ を記述するための補助的な量（$R(z)$ と $W,\Sigma^{-1}W$ との overlap）を導入：

$$
\scriptstyle
r(z):=\frac{1}{p}\Tr(\Sigma^{1/2}W^\top(U-zI_p)^{-1}W\Sigma^{1/2}),\quad
s(z):=\frac{1}{p}\Tr(W^\top(U-zI_p)^{-1}W),\quad z\in\bC
$$

::: {.callout-tip title="[定理3.1 @Bonnaire+2025]" icon="false"}

$q(z)=\Tr(R(z))/p$ はランダム行列極限 $d,p,n\to\infty$ で大数の法則が性質するとする．このとき，後述の仮定の下で，$q$ は次の鞍点方程式の解が与える：

$$
\begin{align*}
\scriptstyle
\wh{s}(q)&\scriptstyle=b^2_t\psi_p+\frac{1}{q},\quad\hat{r}(r,q)=\frac{\psi_pa_t^2e^{-2t}}{1+\frac{a_t^2e^{-2t}\psi_p}{\psi_n}r+\frac{\psi_pv_t^2}{\psi_n}q},\quad
s(z)=\int\frac{1}{\hat{s}(q)+\lambda\hat{r}(r,q)}\rho_\Sigma(d\lambda),\\
\scriptstyle r(z)&\scriptstyle=\int\frac{\lambda}{\hat{s}(q)+\lambda\hat{r}(r,q)}\,\rho_\Sigma(d\lambda),\quad\psi_p(s_t^2-z)+\frac{\psi_pv_t^2}{\scriptscriptstyle 1+\frac{a_t^2e^{-2t}\psi_p}{\psi_n}r+\frac{\psi_pv_t^2}{\psi_n}q}+\frac{1-\psi_p}{q}-\frac{s}{q^2}=0.
\end{align*}
$$

:::

::: aside
$\sigma=\id_{\R},P_x=N_d(0,I_d)$ のとき，$r=s$ で $s,\hat{s}$ を消去すると [@Marčenko-Pastur1967] の自己無撞着方程式を得る：
$$
q(z)=\frac{1}{-z+\frac{\psi_p}{1+q(z)}}.
$$
:::

### Gaussian Equivalence Principle {#sec-GEP}

::: {.callout-tip title="経験スペクトル極限の構造定理 [定理1.4 @Peche2019]" icon="false"}

ランダム行列 $X\in\R^{d\times n},W\in\R^{p\times d}$ の成分はそれぞれ i.i.d. で，指数より軽い裾を持ち，$(0,1)$ に正規化されているとする．
$f:\R\to\R$ が Schwartz 急減少関数ならば，
次の２つの行列 $M,M'$ の経験スペクトル分布は漸近同等：
$$
M:=\frac{1}{d}f^{\otimes(p\times n)}\paren{\frac{WX}{\sqrt{d}}}f^{\otimes(p\times n)}\paren{\frac{WX}{\sqrt{d}}}^\top
$$
$$
\scriptstyle
M':=\frac{1}{n}\paren{\sqrt{\theta_2(f)}\frac{WX}{\sqrt{d}}+\sqrt{\theta_1(f)-\theta_2(f)}Z}\paren{\sqrt{\theta_2(f)}\frac{WX}{\sqrt{d}}+\sqrt{\theta_1(f)-\theta_2(f)}Z}^\top
$$
ただし，$Z\in\R^{p\times n}$ は Gaussian random matrix で，
$$
\theta_1(f)=\E[f(Z_{11})^2],\quad\theta_2(f)=\E[f'(Z_{11})]^2=\E[Z_{11}f(Z_{11})]^2.
$$

:::

::: aside
$\sqrt{\theta_2(f)}$ は $f(Z_{11})$ を $Z_{11}$ で線型回帰した際の係数の絶対値，$\theta_1(f)-\theta_2(f)$ は残差と見れる．
:::

### 3.7 Gaussian Equivalence Principle {.unnumbered .unlisted visibility="uncounted"}

::: {.callout-tip title="[補題C.1 @Bonnaire+2025]" icon="false"}

1. $\sigma\in L^2_0(\gamma),\;\gamma:=N_1(0,1)$ である．
2. データ分布 $P_x$ は sub-Gaussian．
3. $P_x$ の共分散行列 $\Sigma^d:=\E[XX^\top]$ は $d\to\infty$ の極限で有界な固有値を持ち，経験スペクトル分布の極限は密度 $\rho_\Sigma$ を持つ絶対連続分布になる．

上の３条件の下で，次の２つの行列 $U,U'$ の経験スペクトル分布は漸近同等：

$$
\begin{align*}
  \scriptstyle
  U&\scriptstyle:=\frac{1}{n}\sum_{\nu=1}^n\operatorname{E}_\xi\left[\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}^\top\right]\\
  \scriptstyle U'&\scriptstyle:=\frac{GG^\top}{n}+b_t^2\frac{WW^\top}{d}+s_t^2I_p,\qquad G:=e^{-t}a_t\frac{W}{\sqrt{d}}X'+v_t\Omega
\end{align*}
$$

$X'$ は $X$ の独立 copy，$\Omega$ は独立な Gaussian random matrix．

:::

### 3.7 Gaussian Equivalence Principle {.unnumbered .unlisted visibility="uncounted"}

$$
\begin{align*}
  U&:=\frac{1}{n}\sum_{\nu=1}^n\operatorname{E}_\xi\left[\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}\sigma^{\otimes p}\paren{\frac{Wx^\nu_t(\xi)}{\sqrt{d}}}^\top\right]\\
  U'&:=\frac{GG^\top}{n}+b_t^2\frac{WW^\top}{d}+s_t^2I_p,\qquad G:=e^{-t}a_t\frac{W}{\sqrt{d}}X'+v_t\Omega
\end{align*}
$$

$\sigma^2_x:=\Tr(\Sigma)/d,\;f_t(\xi,\eta):=\sigma(e^{-t}\sigma_x\eta+\sqrt{\Delta_t}\xi)\;\xi,\eta,\zeta\sim\rN_1(0,1)$ とすると
$$
\begin{align*}
  b_t&:=\E[\xi f_t(\xi,\eta)],\qquad a_t:=\E\Square{\frac{\eta}{e^{-t}\sigma_x} f_t(\xi,\eta)},\\
  v_t^2&:=\E[f_t(\xi,\eta)f_t(\xi,\zeta)]-a_t^2e^{-2t}\sigma^2_x,\\
  s^2_t&:=\E[\sigma(\Gamma_t\eta)^2]-a_t^2e^{-2t}\sigma^2_x-v_t^2-b_t^2,\qquad\Gamma_t:=e^{-2t}\sigma^2_x+\Delta_t.
\end{align*}
$$

### $q(z)=\Tr(R(z))/p$ の極限の計算 {#sec-proof}

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
::: {.callout-important icon="false" style="text-align: left;" title="Replica Trick [[@Bonnaire+2025]]{.tiny-letter}"}

$\frac{d\det(X)}{dX}\scriptstyle=\Tr(X^{-1})\det(X)$ より，^[[@Magnus-Neudecker2019 p.201]]
$$
\begin{align*}
  q(z)%&=-\frac{1}{p}\pp{}{z}\log\det(U-zI_p)\\
  &=\frac{2}{p}\pp{}{z}\log\paren{\det(U-zI_p)}^{-\frac{1}{2}}\\
  &=2\pp{}{z}\lim_{p\to\infty}\frac{\E[\log Z(z)]}{p}\\
  Z(z)&:=\frac{1}{(2\pi)^{\frac{p}{2}}}\int_{\R^p}e^{-\frac{\phi^\top(U-zI_p)\phi}{2}}\,d\phi
\end{align*}
$$
次の関係を用いて計算する：
$$
\log Z=\lim_{n\to\infty}\frac{Z^n-1}{n}.
$$


:::
:::

::: {.column width="50%"}
::: {.callout-note icon="false" style="text-align: left;" title="Linearization [[@George+2025]]{.tiny-letter}"}

$U=FF^\top/n$ と表せたとき，
$$
\begin{align*}
  L(z)&:=\begin{pmatrix}
   -z_pI&F^\top/\sqrt{n}\\
   F/\sqrt{n}&I_n
   \end{pmatrix}\\
   &\scriptstyle=B_0\otimes I+E_{12}\otimes X+E_{21}\otimes X^*
\end{align*}
$$
という大きい行列 $L(z)^{-1}$ の一部
$$
(L(z)^{-1})_{11}=-(U+zI_p)^{-1}
$$
に $R(z)$ が入る．この**直線束 (linear pencil) 表現**の $\Tr(L(z)^{-1})$ の計算が自由確率論 (free CLT) で出来る．

:::
:::

::::

<!--
### 証明

::: {.callout-tip title="[定理3.1 @Bonnaire+2025]" icon="false" style="margin-bottom: 0em; !important"}

$q(z)=\Tr(R(z))/p$ の極限は，次の鞍点方程式の解が与える：

$$
\begin{align*}
\scriptstyle
\wh{s}(q)&\scriptstyle=b^2_t\psi_p+\frac{1}{q},\quad\hat{r}(r,q)=\frac{\psi_pa_t^2e^{-2t}}{1+\frac{a_t^2e^{-2t}\psi_p}{\psi_n}r+\frac{\psi_pv_t^2}{\psi_n}q},\quad
s(z)=\int\frac{1}{\hat{s}(q)+\lambda\hat{r}(r,q)}\rho_\Sigma(d\lambda),\\
\scriptstyle r(z)&\scriptstyle=\int\frac{\lambda}{\hat{s}(q)+\lambda\hat{r}(r,q)}\,\rho_\Sigma(d\lambda),\quad\psi_p(s_t^2-z)+\frac{\psi_pv_t^2}{\scriptscriptstyle 1+\frac{a_t^2e^{-2t}\psi_p}{\psi_n}r+\frac{\psi_pv_t^2}{\psi_n}q}+\frac{1-\psi_p}{q}-\frac{s}{q^2}=0.
\end{align*}
$$

:::

行列微分則 $\frac{d\det(X)}{dX}=\Tr(X^{-1})\det(X)$ [@Magnus-Neudecker2019 p.201] から，
$$
q(z)=-\frac{1}{p}\pp{}{z}\log\det(U-zI_p)=\frac{2}{p}\pp{}{z}\log\paren{\det(U-zI_p)}^{-\frac{1}{2}}
$$
-->

### 主な系：スペクトルの two-bulk 構造 {#sec-two-bulk}

::: {.callout-tip title="[定理3.2 @Bonnaire+2025]" icon="false" style="margin-bottom: 0em; !important"}

$U$ のランダム行列極限での極限固有値分布 $\rho$ は次の表示を持つ：^[平均 $\wt{U}:=\E[U]$ の経験スペクトル分布の極限を $\mu_{\wt{U}}$ とした．]

1. 過剰パラメータ領域：$\psi_p>\psi_n\gg1$ のとき

  $$
  \rho(\lambda)=\paren{1-\frac{1+\psi_n}{\psi_p}}\delta_{\{s_t^2\}}(\lambda)+\frac{\psi_n}{\psi_p}\rho_1(\lambda)+\frac{1}{\psi_p}\mu_{\wt{U}}(\lambda).
  $$

2. 不足パラメータ領域：$\psi_n>\psi_p\gg1$ のとき

  $$
  \rho(\lambda)=\paren{1-\frac{1}{\psi_p}}\rho_1(\lambda)+\frac{1}{\psi_p}\mu_{\wt{U}}(\lambda).
  $$

ただし，$\rho_1$ とは……（続く）

:::

証明は $\psi_n,\psi_p\to\infty$ の近似の下で固定点方程式を解くことによる．

### 3.9 主な系：スペクトルの two-bulk 構造 {.unnumbered .unlisted visibility="uncounted"}

::: {.callout-tip title="過剰パラメータ領域：$\psi_p>\psi_n\gg1$ のとき" icon="false" style="margin-bottom: 0em; !important"}

$$
\rho(\lambda)=\paren{1-\frac{1+\psi_n}{\psi_p}}\delta_{\{s_t^2\}}(\lambda)+\frac{\psi_n}{\psi_p}\rho_1(\lambda)+\frac{1}{\psi_p}\mu_{\wt{U}}(\lambda).
$$

* $s_t^2$ に属する固有ベクトルは $\Ker(A_\tau)$ に入り，訓練／テスト誤差の値を変えない．
* $\rho_1$ は atom を持たず，
  $$
  \supp(\rho_1)=\Square{s_t^2+v_t^2\paren{1-\sqrt{\psi_p/\psi_n}}^2,s_t^2+v_t^2\paren{1+\sqrt{\psi_p/\psi_n}}^2}
  $$
* $\mu_{\wt{U}}$ は atom を持つかもしれないが，絶対連続部分は
  $$
  \inf\supp(\mu_{\wt{U}})=O(\psi_p)\qquad(\psi_n,\psi_p\to\infty).
  $$

:::

特に，$\psi_p/\psi_n\to\infty$ も仮定すると，$\sup\supp(\rho_1)=O(\psi_p/\psi_n)$．

### 3.9 主な系：スペクトルの two-bulk 構造 {.unnumbered .unlisted visibility="uncounted"}

::: {.callout-tip title="過剰パラメータ領域：$\psi_n,\psi_p/\psi_n\to\infty$ のとき" icon="false" style="margin-bottom: 0em; !important"}

$$
\rho(\lambda)=\paren{1-\frac{1+\psi_n}{\psi_p}}\delta_{\{s_t^2\}}(\lambda)+\frac{\psi_n}{\psi_p}\rho_1(\lambda)+\frac{1}{\psi_p}\mu_{\wt{U}}(\lambda).
$$
$$
\sup\supp(\textcolor{#928EC3}{\rho_1})=O(\psi_p/\psi_n),\qquad\inf\supp(\textcolor{#E69252}{\mu_{\wt{U}}})=O(\psi_p/\psi_n\cdot\psi_n).
$$

:::

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
![$d=10^2,\psi_p=64,\psi_n=8,t=0.01,\rho_\Sigma=\delta_1$ の場合．](Bonnaire2025/Spectrum.png){fig-align="center" width="70%"}
:::

::: {.column width="50%"}
![$\rho_\Sigma=(\delta_{1/2}+\delta_{3/2})/2$ の場合．](Bonnaire2025/Spectrum2.png){fig-align="center" width="70%"}
:::

::::

$\rho_1$ は $\sigma^2_x:=\Tr(\Sigma)/d$ を通じてしかデータに依存しないので，２つの図で変わらない．

### 過剰パラメータ領域での訓練ダイナミクス {#sec-dynamics}

$$
\tau_{\text{gen}}:=\frac{\psi_p}{\Delta_t}\inf\supp(\textcolor{#E69252}{\mu_{\wt{U}}})=O(1/\Delta_t).
$$
$$
\tau_{\text{mem}}:=\frac{\psi_p}{\Delta_t}\inf\supp(\textcolor{#928EC3}{\rho_1})=O(\psi_n/\Delta_t).
$$
$t>0$ が大きく，$n$ が大きいほど $\tau_{\text{mem}}$ が大きい．

:::: {.columns style="text-align: center;"}

::: {.column width="50%"}
![スコア誤差 $\psi_n=4,8,16,32,t=0.1$．](Bonnaire2025/Error_score_draft.png){fig-align="center" width="70%"}

$$
\mathcal{E}_{\text{score}}:=\frac{1}{d}\operatorname{E}_x\SQuare{\|s_A(x)-\nabla\log P_x\|^2}
$$
:::

::: {.column width="50%"}
![訓練誤差（実線）とテスト誤差（点線）](Bonnaire2025/test_train_inset_draft.png){fig-align="center" width="70%"}

$\tau_{\text{gen}}$ までの速いスケールでは，汎化ギャプ $\mathcal{L}_{\text{gen}}=\mathcal{L}_{\text{test}}-\mathcal{L}_{\text{train}}$ が消える．
:::

::::

### 補足：経験誤差最小化解 $\tau\to\infty$ の汎化誤差

実は収束先も $n$ が大きいほど汎化ギャップ $\mathcal{L}_{\text{gen}}=\mathcal{L}_{\text{test}}-\mathcal{L}_{\text{train}}$ は低くなる [@George+2025]．

ただし，パラメータ不足領域 $\psi_p<\psi_n$ に侵入した場合は，時間スケールの分離は見られず，architectural regularization により $\mathcal{L}_{\text{gen}}=\mathcal{L}_{\text{test}}-\mathcal{L}_{\text{train}}$ はさらに下がる．

![$t=0.01$．縦の点線は $\psi_n=\psi_p$ [Figure 5 @George+2025]．$\tau\to\infty$ の場合に対応．](Bonnaire2025/George.png)

## まとめ

### 学習ダイナミクスのスケール分離

1. 学習ダイナミクスのスケール分離を，勾配流の緩和時間
  $$
  \tau^{-1}=\psi_p\lambda_{\text{min}}/\Delta_t
  $$
  の解析から示した．
2. $U$ の固有値分布 $\rho(d\lambda)$ が次を満たす $d\to\infty$ 極限でスケール分離する：
  $$
  \psi_p:=\frac{p}{d}\to\infty,\quad\psi_n:=\frac{n}{d}\to\infty,\quad\frac{\psi_p}{\psi_n}\to\infty.
  $$
3. 速いスケールは真のデータ分布のみに依存し，$\mathcal{L}_{\text{gen}}=\mathcal{L}_{\text{test}}-\mathcal{L}_{\text{train}}$ が消える
4. 遅いスケールは $\tau_{\text{mem}}=O(n)$ のオーダーで，データ分布の $\sigma_x^2:=\Tr(\Sigma)/d$ のみに依存する．
5. $n$ も同時に大きくしていくと，経験リスク最小解 $\tau\to\infty$ での $\mathcal{L}_{\text{gen}}$ は下がる

### なぜ学習ダイナミクスにスケール分離が見られるのか？

::: {.callout-important title="なぜスケール分離が起こるのか？" icon="false"}

$\tau_{\text{mem}}=O(n)$ というのは，NN が経験スコアの高周波成分を学習するのにかかる時間が $O(n)$ でスケールするのが見えている？

:::

* NN の spectral bias [@Rahaman+2019]：NN が関数を学習する際，低周波成分 → 高周波成分の順に学んでいく
  * → early stopping と併せると「低周波成分だけを学んでいることが汎化の主な理由である」という仮説がたつ．
* 拡散モデルは訓練データに依らず，高周波成分を縮小して学習する [@Kadkhodaie+2024]^[[@Kadkhodaie+2024] は denoiser を直接 MSE で学習していることに注意．]

### 附言

* GD ダイナミクスだけ？ SGD, Adam でも普遍的に観察されるし，説明したい．
* conditional generation でも成り立つように思われ，その場合 $n,p,d$ 以外の変数が出てくる？
* $P_x$ の裾が重い場合はどうなる？部分多様体上に台を持つ場合は？

## 参考文献 {.unnumbered .unlisted visibility="uncounted"}