---
title: "拡散モデルでの動的暗黙正則化"
subtitle: "Bonnaire, Urfin, Biroli & Mézard (2025, NeurIPS)<br>Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training"
author: "司馬 博文"
date: "1/26/2026"
categories: [Diffusion]
# image: ../../../static/Materials/総研大.png
format:
  # html: default
  revealjs: 
    output-file: Bonnaire2025Slides.html
    footer: |
      [Why Diffusion Models Don’t Memorize](Bonnaire2025.qmd)
    toc-depth: 1
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ../../../assets/profile.jpg
    css: ../../../assets/slides.css
    scrollable: true
    smaller: true
    controls: true
    controls-layout: bottom-right
    self-contained-math: true
    shift-heading-level-by: -1
    toc: true
    toc-title: 目次
    number-sections: true
    theme: serif
    show-slide-number: all
    include-in-header: ../../../assets/forSlides.html
    tbl-cap-location: bottom
    margin: 0.05
    comments: false
comment: false
code-fold: false
execute:
    cache: true
html-math-method: katex
abstract-title: 概要
abstract: |
  スライド版は[[こちら]{.underline}](Bonnaire2025Slides.html)．
---

## 概観

{{< include ../../../assets/_preamble.qmd >}}

## 実験パート (Section 2)

### 設定

* **データ**：cropped & grayscaled CelebA, $d=32^2$, $n\in\{128,\cdots,32768\}$

![](Bonnaire2025/SM_Images_CelebA.jpeg){width=10 fig-align="center"}

* **スコア学習に使う NN**：U-Net, 時刻 $t$ は sinusoidal position embedding
* **損失関数**：DDPM loss．ここまで全て [@Ho+2020] の設定に従う．
* **訓練**：SGD, momentum $\beta=0.95$, fixed $\eta=0.01$, batch size $B=n\land512$

## 理論パート (Section 3)

### 設定

* **データ**: $x^\nu\in\R^d\;(\nu=1,\cdots,n)$. あとで $d\to\infty$ の極限を考える．
* **スコア学習に使うNN**：Random Feature $W\in\R^{p\times d}$ with i.i.d. Gaussian
  $$
  s_A(x):=\frac{A}{\sqrt{p}}\sigma^{\otimes p}\paren{\frac{Wx}{\sqrt{d}}}\qquad A\in\R^{d\times p},
  $$
* **損失関数**: 固定した時刻 $t>0$ での DSM loss
  $$
  \mathcal{L}_t(A;\{x^\nu\}_{\nu=1}^n):=\frac{1}{dn}\sum_{\nu=1}^n\E\Square{\norm{\sqrt{\Delta_t s_A(x_t^\nu)}+Z}^2}.
  $$
* **訓練**: GD による
  $$
  \dot{A}_\tau=-d^2\nabla_A\mathcal{L}_t(A_\tau)=-2\Delta_t\frac{d}{p}AU_\tau-\frac{2d\sqrt{\Delta_t}}{\sqrt{p}}V_\tau^\top,\qquad\tau\ge0.
  $$

### GD の連続時間極限



### 鍵となる reduction argument

訓練ダイナミクスのスケール分離は，スペクトル分布のスケール分離からくる．

## まとめ

### Open Question


::: {.callout-important appearance="simple" icon="false"}

empirical score と population score の違いは，生成の最終段階において，関数的には近いかもしれないが，スペクトル的には高周波成分がひらすら noisy になる．しかし NN の spectral bias により，低周波成分だけを学んでいることが，汎化の主な理由なのではないか？

:::

data dependent な高周波成分の学習時間が $O(n)$ のスケールでかかるのも独立で興味に値する新たな発見になっている．

## 参考文献