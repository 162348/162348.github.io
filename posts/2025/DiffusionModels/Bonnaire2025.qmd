---
title: "拡散モデルでの動的暗黙正則化"
subtitle: "Bonnaire, Urfin, Biroli & Mézard (2025, NeurIPS)<br>Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training"
author: "司馬 博文"
date: "1/26/2026"
categories: [Diffusion]
# image: ../../../static/Materials/総研大.png
format:
  # html: default
  revealjs: 
    output-file: Bonnaire2025Slides.html
    footer: |
      [Why Diffusion Models Don’t Memorize](Bonnaire2025.qmd)
    toc-depth: 1
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ../../../assets/profile.jpg
    css: ../../../assets/slides.css
    scrollable: true
    smaller: true
    controls: true
    controls-layout: bottom-right
    self-contained-math: true
    shift-heading-level-by: -1
    toc: true
    toc-title: 目次
    number-sections: true
    theme: serif
    show-slide-number: all
    include-in-header: ../../../assets/forSlides.html
    tbl-cap-location: bottom
    margin: 0.05
    comments: false
comment: false
code-fold: false
execute:
    cache: true
html-math-method: katex
abstract-title: 概要
abstract: |
  スライド版は[[こちら]{.underline}](Bonnaire2025Slides.html)．
---

## 概観 (Section 1)



## 実験パート (Section 2)

### 設定

* **スコア学習に使う NN**：U-Net, 時刻 $t$ は sinusoidal position embedding
* **損失関数**：DDPM loss．ここまで全て [@Ho+2020] の設定に従う．
* **データ**：cropped & grayscaled CelebA, $d=32^2$, $n\in\{128,\cdots,32768\}$

![](Bonnaire2025/SM_Images_CelebA.jpeg){width=10 fig-align="center"}

* **訓練**：SGD, momentum $\beta=0.95$, fixed $\eta=0.01$, batch size $B=n\land512$

## 理論パート (Section 3)

## まとめ

### Open Question


::: {.callout-important appearance="simple" icon="false"}

empirical score と population score の違いは，生成の最終段階において，関数的には近いかもしれないが，スペクトル的には高周波成分がひらすら noisy になる．しかし NN の spectral bias により，低周波成分だけを学んでいることが，汎化の主な理由なのではないか？

:::

data dependent な高周波成分の学習時間が $O(n)$ のスケールでかかるのも独立で興味に値する新たな発見になっている．

## 参考文献