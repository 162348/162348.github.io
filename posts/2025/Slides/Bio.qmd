---
title: "Drawing Parallels between Statistics and Nature"
author:
  - name: "Hirofumi Shiba"
    affiliations: 
      - name: "D3, Institute of Statistical Mathematics"
date: "9/2/2025"
categories: [Slide, PDMP, Nonresearch]
image: Bio/Bio/Bio.002.png
format:
  html: default
  revealjs: 
    output-file: Bio_Slides.html
    footer: |
      [司馬博文](Bio.qmd)
    toc-depth: 1
    slide-number: false
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ../../../assets/profile.jpg
    css: ../../../assets/slides.css
    scrollable: true
    smaller: true
    controls: true
    controls-layout: bottom-right
    self-contained-math: true
    shift-heading-level-by: -1
    toc: false
    toc-title: Contents
    number-sections: false
    theme: serif
    show-slide-number: all
    include-in-header: ../../../assets/forSlides.html
    tbl-cap-location: bottom
    margin: 0.05
    comments: false
include-in-header: 
            - text: |
                <style>
                .title, .subtitle, .quarto-title-author-name, .quarto-title-affiliation, .date, h1, h2, h3, h4, h5, h6, .navbar-title, .menu-text, .reveal-full-page, a, li, p, div {
                  font-family: "Helvetica Neue", sans-serif !important;
                }
                </style>
description: |
  These slides are designed to introduce myself in 5 minutes, especially to those who are not familiar with statistics or machine learning.
  Slides can be viewed in slide mode [[here]{.underline}](Bio_Slides.html).
comment: false
bibliography: 
    - ../../../assets/2023.bib
    - ../../../assets/2024.bib
    - ../../../assets/2025.bib
csl: ../../../assets/apalike.csl
code-fold: false
execute:
    cache: true
html-math-method: katex
abstract-title: 概要
abstract: |
  
slide:
  event-title: SOKENDAI 研究交流セミナー
  place: 御茶ノ水ソラシティ
  time: 14:00-16:00
---

## [Keywords in My Research]{.small-letter}

![](Bio/Bio/Bio.001.png){fig-align="center"}

::: {.content-visible when-format="html" unless-format="revealjs"}

This slide contains some of my research keywords. The four words in black letters, Monte Carlo Simulation, Bayesian Inference, Generative Modeling, Uncertainty Quantification, are the keywords of my research.

The four fields in colors are the fields that my research is related to.

:::

## [Distribution: Key of New Science]{.small-letter}

![](Bio/Bio/Bio.002.png){fig-align="center"}

::: {.content-visible when-format="html" unless-format="revealjs"}

One thing that all the keywords and fields have in common is that they use the notion of *probability distribution* as a key concept.

In the slide, I show four examples where the notion of distribution is used as a key building block,

* **Diffusion Model**, where a transformation between known and a unknown distribution is seeked,
* **Boltzmann-Shannon entropy**, which is one of the most important concepts to understand different distributions, identified in the field of statistical mechanics, but found a new application in the field of information theory,
* **Boltmann-Gibbs distribution**, which is a distribution that is used to describe the equilibrium state of a system, but found a new interpretation in the field of Bayesian inference and machine learning,
* **Uncertainty Quantification**, which is a characteristic of Bayesian approach in statistics, but recently rediscovered as a key quality of successful methods in the field of machine learning.

:::

## [A Computational Reinterpretation]{.small-letter}

![](Bio/Bio/Bio.003.png){fig-align="center"}

::: {.content-visible when-format="html" unless-format="revealjs"}

Most of the traditional approaches, including conjugate Bayesian models and density estimation, concentrate on aquiring the function value $p(x)$ of the probability density function.

However, after the rise of the computer, 'learn to generate from $p$' approach has been found to be more powerful and practical, especially through the development of a class of Monte Carlo methods called Markov Chain Monte Carlo (MCMC).

The mere fact that these two approach (learn the function $p$, and learn to generate) are indeed different is already surprising, which is only realized recently by the invention of **Generative Modeling**.

:::

## [Development in Monte Carlo Methods]{.small-letter}

:::: {.columns style="text-align: center;"}
::: {.column width="33%"}
![[Markov Chain]{.large-letter}](../../2024/Slides/PDMPs/RWMH.gif)
:::

::: {.column width="33%"}
![[Diffusion]{.large-letter .color-blue}](../../2024/Slides/PDMPs/Langevin.gif)
:::

::: {.column width="33%"}
![[Jump Process]{.large-letter .color-unite}](../../2024/Slides/PDMPs/ZigZag_SlantedGauss2D_longer.gif)
:::

::::

::: {.content-visible when-format="html" unless-format="revealjs"}

Starting from this slide, I will talk about my research more specifically, which is about MCMC methods mentioned above.

In this slide, there are three stochastic processes with distinct characteristics. They are found and utilized in the MCMC methods, from the left to the right, that is, Markov Chain (discrete time) to Diffusion Process (continuous time), and the last one is a Jump Process (still in continuous time).

:::

### What's Wrong with [Diffusion]{.color-blue}? {auto-animate=true}

:::: {.columns}
::: {.column width="40%"}
![](../../2024/Slides/PDMPs/Langevin.gif)
:::

::: {.column width="60%"}
::: {.callout-tip title="Equilibrium ≒ Reversibility" icon="false"}
[Langevin Diffusion]{.color-blue} represents a particle in a medium.

[E.g.]{.color-minty} *A sugar particle in a coffee*
:::

* Nature is not necessarily efficient.

  [E.g.]{.color-minty} *Would you wait until the sugar dissolves? To have a cup of coffee?*

* It's difficult to simulate.

:::
::::

::: {.content-visible when-format="html" unless-format="revealjs"}

The Langevin diffusion constitutes one of the most effective methods in MCMC's history, but it is not free of shortcomings.

There are sereval reasons technically, but from the high level, the choice of the process itself is not optimal.

An intuition behind this poor performance is captured by the notion of **reversibility**. The reversibility of the process typically implies slow exploration of the space, as the process tends to go back to where it came from.

:::

### What's New in [PDMP]{.color-unite}? {auto-animate=true}

:::: {.columns}
::: {.column width="40%"}
![](../../2024/Slides/PDMPs/ZigZag_SlantedGauss2D.gif)
:::

::: {.column width="60%"}
::: {.callout-tip title="Irreversibility & Acceleration" icon="false"}
* [**Ballistic motion**]{.color-unite}, up until a turn

  [E.g.]{.color-minty} *Stirring coffee with a spoon*
* No artificial symmetry (e.g. detailed balance)

  [{{< fa arrow-right >}}]{.color-minty} Fast convergence & reduced computational cost
:::

All with a new strategy of simulation, which seems to be very efficient (ongoing research)

:::
::::

::: {.content-visible when-format="html" unless-format="revealjs"}

Therefore, a new class of processes, called Piecewise Deterministic Markov Process (PDMP), has been proposed as an alternative to the Langevin diffusion.

As can be easily seen, PDMPs are characterized by the dominating deterministic move, which goes straight through the space until the point where it changes the direction randomly.

:::

## [PDMP]{.color-unite} Package

[P]{.color-unite}iecewise [D]{.color-unite}eterministic [M]{.color-unite}arkov [P]{.color-unite}rocess

:::: {.columns}
::: {.column width="50%"}
Python
![](../../2024/Slides/PDMPs/pdmp_jax.png)
```zsh
pip install pdmp-jax
```
:::

::: {.column width="50%"}
Julia
![](../../2024/Slides/PDMPs/PDMPFlux.png)
```julia
] add PDMPFlux
```
:::
::::

::: {.content-visible when-format="html" unless-format="revealjs"}

My research mainly concerns the theoretical aspects of PDMPs. The most obvious question is whether PDMPs are really efficient seen as a Monte Carlo algorithm.

I aim to conduct convergence analysis and complexity analysis of PDMPs to answer this question, and further provide insights into PDMP and related statistical computational algorithms.

Alongside theoretical inquiries, I have developed a package to simulate PDMPs for sampling applications, because there was not a fixed implementation that works for general targets and general PDMPs.

This package will accelerate my research and comparison between different implementations, which may be one of the most interests practically.

:::

## [Bringing Science Back]{.small-letter}

![](Bio/Bio/Bio.004.png){fig-align="center"}

::: {.content-visible when-format="html" unless-format="revealjs"}

This line of research regarding PDMPs might occupy the green area indicated in the slide, bridging Statistical Physics with (Bayesian) Statistics, and a part of AI and Machine Learning.

However, the notion of reversibility has recently found to be importance in generative modeling as well.

Thus, the Stochastic Process approach I take and the notion of probability distribution are expected to have more and more important roles in the four fields I mentioned above.

The recent development in AI and Machine Learning is sometimes said to be 'launched bottom up', which means that the success of the methods are not necessarily based on the theoretical insights, but rather on the empirical success.

My aim is to bring science back to the field of machine learning and statistics, and to provide a solid foundation for the future collaborative development.

:::