---
title: "漸近論の歴史"
author: "司馬 博文"
date: 10/10/2025
categories: [Statistics]
# image: Images/Cochran.svg
bibliography: 
    - ../../../assets/2023.bib
    - ../../../assets/2024.bib
    - ../../../assets/2025.bib
csl: ../../../assets/apalike.csl
---

{{< include ../../../assets/_preamble.qmd >}}

$T$ を時間パラメータの有向集合，$(\Om,\cF,(\cF_t)_{t\in T},\P)$ をフィルトレーション付きの標本空間とする．
この上にパラメータづけられた確率分布の族 $\{P_\theta\}_{\theta\in\Theta}\subset\cP(\Om)$ を考える．

$(\Theta,d)$ はコンパクト距離空間とし，$H:=C(\Theta;\R^p)$ を一様ノルムに関して Hilbert 空間と見る．

## 最小コントラスト推定量

::: {.callout-tip title="定義（コントラスト関数） [@Pfanzagl1969]" icon="false"}

可測関数 $C:\Om\to H$ が次を満たすとき，
これを **コントラスト関数** という：^[[@Pfanzagl1969 p.250 定義1.1] が [@Huber1967] を拡張する形で導入した用語である．]
$$
E_\theta[C(\theta)]<E_\theta[C(\theta')],\qquad(\theta\ne\theta').
$$

:::

::: {.callout-warning title="別名 (Extremum Estimator)" collapse="true" icon="false"}

最小コントラスト推定量のように，特定の標本依存関数 $\wh{Q}_n(\theta)$ を最小化して得る推定量 $\wh{\theta}_n$ を extremum estimator ということもある [@Manski1975], [@Amemiya1985], [@Newey-McFadden1994]．

この場合の一致性証明も全く同様に与えられ [@Amemiya1973 補題3], [@Newey-McFadden1994 定理2.1]，本質的に同じ枠組みであると言うことができる．

最小コントラスト推定量の用語は [@Pfanzagl1969] で初めて導入され，その後もヨーロッパ系，分野で言えば確率過程の統計推測において広く用いられている印象がある．

:::

このような（これよりも少しだけ広い意味での）識別可能性を持った関数 $C$ と，これに収束する列 $C_t$ をうまく設計することにより，$\P\in\{P_\theta\}$ を要請せずとも，目標の値 $\theta^*\in\Theta$ に収束する推定量の列 $\wh{\theta}_t$ を構成することができる：

::: {.callout-important title="定理（漸近最小コントラスト推定量の一致性）" icon="false"}

$C_t:\Om\to H$ を $\cF_t$-可測関数の列とし，$\Theta'\subset\Theta$ を集合とする．$C_t$ に確率収束極限 $C$ があり，$\wh{\theta}_t:\Om\to\Theta$ が次を満たすならば，$d(\wh{\theta}_t,\Theta')\pto0.$ が成り立つ：

1. $C$ に関する識別可能性条件：任意の $\eta>0$ に関して，
    $$
    \inf_{d(\theta,\Theta')\ge\eta}C(\theta)-\inf C(\Theta')>0.
    $$

2. $C_t$ に関する収束条件：任意の $\epsilon>0$ に対して，
    $$
    \P\Square{C_t(\wh{\theta}_t)-\inf C_t(\Theta)>\ep}\to0.
    $$

:::

::: {.callout-warning title="例（対数尤度と KL 乖離度）" appearance="simple" icon="false"}

対数尤度はコントラスト関数になる：
$$
C(-;\theta)=-\log\dd{P_\theta}{P_{\theta^*}}(-).
$$
このとき，
$$
E_{\theta^*}[C(\theta)]=\KL(P_{\theta^*},P_\theta).
$$
$\{P_\theta\}$ に真のデータ生成分布 $\P$ が含まれていない場合でも，定理は有効であり，何かしらの収束極限 $\Theta'\subset\Theta$ は持つ．

一般のコントラスト関数に関して，誤特定の場合でも，この点 $\Theta'$ は特定の距離を真のモデル $\P$ との間で最小化した点であり，大偏差原理様の結果が成り立つことが [@Golubev-Spokoiny2009] で議論されている．

:::

::: {.callout-warning title="例（一般化モーメント法 [@Hansen82-GMM]）" appearance="simple" icon="false"}

「モーメント関数」とは $\operatorname{E}[g(X,\theta_0)]=0$ を満たすベクトル値関数 $g$ をいう．これに対して，
$$
C_n(\theta)=-G_n^\top\widehat{W}G_n,\qquad G_n:=\frac{1}{n}\sum_{i=1}^ng(z_i,\theta)
$$
と，半正定値行列 $\widehat{W}$ を通じて $0$ に近づけていこうとする手続きを GMM (Generalized Method of Moments) という．

この形の目的関数が最も自然に現れるのが [**操作変数法**](../../2024/Stat/Regression.qmd#操作変数) であり，
$$
G_n:=\frac{1}{n}\sum_{i=1}^nz_i(y_i-x_i^\top\beta)
$$
と与えた場合に当たる．

この枠組みでの最適な荷重 $\wh{W}$ の取り方が，[@Newey-McFadden1994 p.2170 Section 5.4] などで議論されている．例えば最小二乗法は
$$
G_n:=\sum_{i=1}^n(y_i-h(x_i,\theta))^2
$$
と取った場合の GMM に当たるが，この際の荷重 $\wh{W}$ の各成分は誤差の条件付き分散 $\V[\ep^2|X=x]$ の逆行列が最適な選択になる．この推定量は GLS (Generalized Least Squares) とも呼ばれる．

:::


### 最小距離推定量

### $M$-推定量

## $Z$-推定量

::: {.callout-important title="定理（$Z$-推定量の漸近分布）" icon="false"}

$\varphi_t:\Om\to H$ を $\cF_t$-可測関数の列とし，$\theta^*\in\Theta^\circ$ の近傍で $C^1$-級，$(\wh{\theta}_t)$ を $(\cF_t)$-適合過程とする．次を満たすならば，
$$
c_t^{-1}(\wh{\theta}_t-\theta^*)=\Gamma^{-1}_{\theta^*}\Delta_T(\theta^*)+o_p(1).
$$
が成り立つ．

1. 一致性：$\wh{\theta}_t\pto\theta^*$．
2. 漸近的零点である：ある $0$ に収束する正数列 $b_t$ について，
    $$
    \Delta_t(\wh{\theta}_t):=b_t\varphi_t(\wh{\theta}_t)\pto0.
    $$
3. 中心極限定理：ある確率変数 $\Delta_{\theta^*}$ が存在して，
    $$
    \Delta_t(\theta^*)\dto\Delta_{\theta^*}.
    $$
4. 局所一様大数の法則：ある $0$ に収束する正数列 $c_t$ と，確率変数 $\Gamma_{\theta^*}$ について，
    $$
    \sup_{\abs{\theta-\theta^*}\le\eta}\abs{\Gamma_t(\theta)-\Gamma_{\theta^*}}\pto0\qquad(\eta,t)\to(0,\infty),
    $$
    $$
    \Gamma_t:=-b_t(\partial_\theta\varphi_t)c_t.
    $$

:::

::: {.callout-warning title="例（最尤推定量 [@Fisher25]）" appearance="simple" icon="false"}

最尤推定量は，真値が $\theta^*\in\Theta^\circ$ を満たし，尤度が $C^1$-級であるとき，
$$
0=\frac{1}{n}\sum_{i=1}^n\psi_i(z_i;\theta),\quad\psi_i(z;\theta):=\partial_\theta\log f(z|\theta)
$$
という推定方程式の解と理解できる．平均値の定理より，ある $\theta'$ が $\theta$ と $\theta^*$ の間に存在して，^[$\theta':\Om\to\Theta$ の可測性に注意は必要．]
$$
0=\frac{1}{n}\sum_{i=1}^n\psi_i(z_i;\theta^*)+\paren{\frac{1}{n}\sum_{i=1}^n\partial_\theta\psi(z_i;\theta')}(\theta-\theta^*)
$$
と表せ，中心極限定理のオーダーの項と大数の法則のオーダーの項との積と見れる：
$$
\sqrt{n}(\wh{\theta}_n-\theta^*)=-\frac{1}{\sqrt{n}}\sum_{i=1}^n\psi(z_i;\theta^*)\paren{\frac{1}{n}\sum_{i=1}^n\partial_\theta\psi(z_i;\theta')}^{-1}.
$$ {#eq-ae}
２つ目の因子に関する大数の法則は，$\theta^*$ に十分近い $\theta'$ のとり得る値について一様に起こる必要があることに注意．

最終的に，第一項である経験スコアの極限分布 $N(0,J)$ と第二項の収束先である対数尤度の Hessian $H^{-1}$ について，Slutzky の補題から極限分布は $N(0,H^{-1}JH^{-1})$ というサンドイッチ型の分散を持つ．最尤推定法に限っては $H=-J$ が成り立つことに注意．

一般に推定関数 $\psi(z;\theta)$ はデータ $z$ の推定量への漸近的な影響を，式 ([-@eq-ae]) の意味で定量化していると見れ，式 ([-@eq-ae]) のような１次近似を与える関数 $\psi$ は **影響関数** とも呼ばれる．

:::

::: {.callout-warning title="例（一般化推定方程式 [@Liang-Zeger1986]）" appearance="simple" icon="false"}

一般化推定方程式法とは，一般化線型モデル $\mu(\beta)$ に対して，
$$
U(\beta):=\sum_{i=1}^N\pp{\mu_i}{\beta}V_i^{-1}(Y_i-\mu_i(\beta))
$$
が定める $Z$-推定量をいう．

$U=0$ という条件は，$V_i$ を荷重として，ほとんど GLS の目的関数が導く１階の最適性条件に一致する．

:::
